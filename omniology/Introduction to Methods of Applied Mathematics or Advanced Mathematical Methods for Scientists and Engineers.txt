Introduction to Methods of Applied Mathematics
or
Advanced Mathematical Methods for Scientists and Engineers
Sean Mauch
April 8, 2002

Contents
Anti-Copyright
xxiii
Preface
xxiv
0.1
Advice to Teachers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxiv
0.2
Acknowledgments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxiv
0.3
Warnings and Disclaimers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxv
0.4
Suggested Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxvi
0.5
About the Title
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxvi
I
Algebra
1
1
Sets and Functions
2
1.1
Sets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Single Valued Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.3
Inverses and Multi-Valued Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.4
Transforming Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.6
Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
1.7
Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
i

2
Vectors
22
2.1
Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.1.1
Scalars and Vectors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.1.2
The Kronecker Delta and Einstein Summation Convention . . . . . . . . . . . . . . . . . . . .
25
2.1.3
The Dot and Cross Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.2
Sets of Vectors in n Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.4
Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
2.5
Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
II
Calculus
46
3
Diﬀerential Calculus
47
3.1
Limits of Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.2
Continuous Functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.3
The Derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.4
Implicit Diﬀerentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
3.5
Maxima and Minima . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
3.6
Mean Value Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
3.6.1
Application: Using Taylor’s Theorem to Approximate Functions. . . . . . . . . . . . . . . . . .
66
3.6.2
Application: Finite Diﬀerence Schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.7
L’Hospital’s Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
3.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
3.9
Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
3.10 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4
Integral Calculus
111
4.1
The Indeﬁnite Integral . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
4.2
The Deﬁnite Integral . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
ii

4.2.1
Deﬁnition
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
4.2.2
Properties
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
4.3
The Fundamental Theorem of Integral Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.4
Techniques of Integration
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
4.4.1
Partial Fractions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
4.5
Improper Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
4.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
4.7
Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
4.8
Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
5
Vector Calculus
147
5.1
Vector Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
5.2
Gradient, Divergence and Curl . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
5.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
5.4
Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
5.5
Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
III
Functions of a Complex Variable
170
6
Complex Numbers
171
6.1
Complex Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
6.2
The Complex Plane
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
6.3
Polar Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
6.4
Arithmetic and Vectors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
6.5
Integer Exponents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
6.6
Rational Exponents
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
6.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
6.8
Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
6.9
Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
iii

7
Functions of a Complex Variable
228
7.1
Curves and Regions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
7.2
The Point at Inﬁnity and the Stereographic Projection . . . . . . . . . . . . . . . . . . . . . . . . . . 231
7.3
Cartesian and Modulus-Argument Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
7.4
Graphing Functions of a Complex Variable
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
7.5
Trigonometric Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
7.6
Inverse Trigonometric Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
7.7
Riemann Surfaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
7.8
Branch Points
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
7.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
7.10 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
7.11 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
8
Analytic Functions
346
8.1
Complex Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
8.2
Cauchy-Riemann Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
8.3
Harmonic Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
8.4
Singularities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
8.4.1
Categorization of Singularities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
8.4.2
Isolated and Non-Isolated Singularities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
8.5
Application: Potential Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
8.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374
8.7
Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380
8.8
Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
9
Analytic Continuation
419
9.1
Analytic Continuation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
9.2
Analytic Continuation of Sums
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
9.3
Analytic Functions Deﬁned in Terms of Real Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 424
9.3.1
Polar Coordinates
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
iv

9.3.2
Analytic Functions Deﬁned in Terms of Their Real or Imaginary Parts . . . . . . . . . . . . . . 432
9.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
9.5
Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
9.6
Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
10 Contour Integration and the Cauchy-Goursat Theorem
444
10.1 Line Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444
10.2 Contour Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
10.2.1 Maximum Modulus Integral Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
10.3 The Cauchy-Goursat Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450
10.4 Contour Deformation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
10.5 Morera’s Theorem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
10.6 Indeﬁnite Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
10.7 Fundamental Theorem of Calculus via Primitives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
10.7.1 Line Integrals and Primitives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
10.7.2 Contour Integrals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
10.8 Fundamental Theorem of Calculus via Complex Calculus . . . . . . . . . . . . . . . . . . . . . . . . . 457
10.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460
10.10Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
10.11Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
11 Cauchy’s Integral Formula
475
11.1 Cauchy’s Integral Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476
11.2 The Argument Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483
11.3 Rouche’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484
11.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487
11.5 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491
11.6 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
v

12 Series and Convergence
508
12.1 Series of Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508
12.1.1 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508
12.1.2 Special Series
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
12.1.3 Convergence Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
12.2 Uniform Convergence
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 519
12.2.1 Tests for Uniform Convergence
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 520
12.2.2 Uniform Convergence and Continuous Functions. . . . . . . . . . . . . . . . . . . . . . . . . . 522
12.3 Uniformly Convergent Power Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
12.4 Integration and Diﬀerentiation of Power Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530
12.5 Taylor Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533
12.5.1 Newton’s Binomial Formula. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
12.6 Laurent Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538
12.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543
12.8 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558
12.9 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567
13 The Residue Theorem
614
13.1 The Residue Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 614
13.2 Cauchy Principal Value for Real Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 622
13.2.1 The Cauchy Principal Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 622
13.3 Cauchy Principal Value for Contour Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 627
13.4 Integrals on the Real Axis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 631
13.5 Fourier Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635
13.6 Fourier Cosine and Sine Integrals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 637
13.7 Contour Integration and Branch Cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 640
13.8 Exploiting Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643
13.8.1 Wedge Contours . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643
13.8.2 Box Contours
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 646
13.9 Deﬁnite Integrals Involving Sine and Cosine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 647
vi

13.10Inﬁnite Sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650
13.11Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 655
13.12Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 669
13.13Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 675
IV
Ordinary Diﬀerential Equations
761
14 First Order Diﬀerential Equations
762
14.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 762
14.2 One Parameter Families of Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 764
14.3 Exact Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 766
14.3.1 Separable Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 771
14.3.2 Homogeneous Coeﬃcient Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 773
14.4 The First Order, Linear Diﬀerential Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 777
14.4.1 Homogeneous Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 777
14.4.2 Inhomogeneous Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 779
14.4.3 Variation of Parameters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 782
14.5 Initial Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 782
14.5.1 Piecewise Continuous Coeﬃcients and Inhomogeneities . . . . . . . . . . . . . . . . . . . . . . 783
14.6 Well-Posed Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 788
14.7 Equations in the Complex Plane . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 791
14.7.1 Ordinary Points
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 791
14.7.2 Regular Singular Points
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 794
14.7.3 Irregular Singular Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 799
14.7.4 The Point at Inﬁnity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 801
14.8 Additional Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 804
14.9 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 807
14.10Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 810
vii

15 First Order Linear Systems of Diﬀerential Equations
831
15.1 Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 831
15.2 Using Eigenvalues and Eigenvectors to ﬁnd Homogeneous Solutions . . . . . . . . . . . . . . . . . . . 832
15.3 Matrices and Jordan Canonical Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 837
15.4 Using the Matrix Exponential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 844
15.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 850
15.6 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 855
15.7 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 857
16 Theory of Linear Ordinary Diﬀerential Equations
885
16.1 Exact Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 885
16.2 Nature of Solutions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 886
16.3 Transformation to a First Order System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 889
16.4 The Wronskian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 890
16.4.1 Derivative of a Determinant.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 890
16.4.2 The Wronskian of a Set of Functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 891
16.4.3 The Wronskian of the Solutions to a Diﬀerential Equation . . . . . . . . . . . . . . . . . . . . 893
16.5 Well-Posed Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 896
16.6 The Fundamental Set of Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 898
16.7 Adjoint Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 900
16.8 Additional Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 904
16.9 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 905
16.10Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 906
17 Techniques for Linear Diﬀerential Equations
911
17.1 Constant Coeﬃcient Equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 911
17.1.1 Second Order Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 912
17.1.2 Higher Order Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 916
17.1.3 Real-Valued Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 917
17.2 Euler Equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 921
viii

17.2.1 Real-Valued Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 923
17.3 Exact Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 926
17.4 Equations Without Explicit Dependence on y . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 927
17.5 Reduction of Order . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 928
17.6 *Reduction of Order and the Adjoint Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 929
17.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 932
17.8 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 938
17.9 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 941
18 Techniques for Nonlinear Diﬀerential Equations
965
18.1 Bernoulli Equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 965
18.2 Riccati Equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 967
18.3 Exchanging the Dependent and Independent Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 971
18.4 Autonomous Equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 973
18.5 *Equidimensional-in-x Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 976
18.6 *Equidimensional-in-y Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 978
18.7 *Scale-Invariant Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 981
18.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 982
18.9 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 985
18.10Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 987
19 Transformations and Canonical Forms
999
19.1 The Constant Coeﬃcient Equation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 999
19.2 Normal Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1002
19.2.1 Second Order Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1002
19.2.2 Higher Order Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1003
19.3 Transformations of the Independent Variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1005
19.3.1 Transformation to the form u” + a(x) u = 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . 1005
19.3.2 Transformation to a Constant Coeﬃcient Equation . . . . . . . . . . . . . . . . . . . . . . . . 1006
19.4 Integral Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1008
ix

19.4.1 Initial Value Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1008
19.4.2 Boundary Value Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1010
19.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1013
19.6 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1015
19.7 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1016
20 The Dirac Delta Function
1022
20.1 Derivative of the Heaviside Function
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1022
20.2 The Delta Function as a Limit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1024
20.3 Higher Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1026
20.4 Non-Rectangular Coordinate Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1027
20.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1029
20.6 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1031
20.7 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1033
21 Inhomogeneous Diﬀerential Equations
1040
21.1 Particular Solutions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1040
21.2 Method of Undetermined Coeﬃcients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1042
21.3 Variation of Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1046
21.3.1 Second Order Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1046
21.3.2 Higher Order Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1049
21.4 Piecewise Continuous Coeﬃcients and Inhomogeneities . . . . . . . . . . . . . . . . . . . . . . . . . . 1052
21.5 Inhomogeneous Boundary Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1055
21.5.1 Eliminating Inhomogeneous Boundary Conditions . . . . . . . . . . . . . . . . . . . . . . . . . 1055
21.5.2 Separating Inhomogeneous Equations and Inhomogeneous Boundary Conditions . . . . . . . . . 1057
21.5.3 Existence of Solutions of Problems with Inhomogeneous Boundary Conditions . . . . . . . . . . 1058
21.6 Green Functions for First Order Equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1060
21.7 Green Functions for Second Order Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1063
21.7.1 Green Functions for Sturm-Liouville Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 1073
21.7.2 Initial Value Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1076
x

21.7.3 Problems with Unmixed Boundary Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . 1078
21.7.4 Problems with Mixed Boundary Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1081
21.8 Green Functions for Higher Order Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1085
21.9 Fredholm Alternative Theorem
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1090
21.10Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1098
21.11Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1104
21.12Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1107
22 Diﬀerence Equations
1145
22.1 Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1145
22.2 Exact Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1147
22.3 Homogeneous First Order . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1148
22.4 Inhomogeneous First Order
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1150
22.5 Homogeneous Constant Coeﬃcient Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1153
22.6 Reduction of Order . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1156
22.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1158
22.8 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1159
22.9 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1160
23 Series Solutions of Diﬀerential Equations
1163
23.1 Ordinary Points
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1163
23.1.1 Taylor Series Expansion for a Second Order Diﬀerential Equation
. . . . . . . . . . . . . . . . 1167
23.2 Regular Singular Points of Second Order Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1177
23.2.1 Indicial Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1180
23.2.2 The Case: Double Root . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1182
23.2.3 The Case: Roots Diﬀer by an Integer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1186
23.3 Irregular Singular Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1196
23.4 The Point at Inﬁnity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1196
23.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1199
23.6 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1204
xi

23.7 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1205
24 Asymptotic Expansions
1228
24.1 Asymptotic Relations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1228
24.2 Leading Order Behavior of Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1232
24.3 Integration by Parts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1241
24.4 Asymptotic Series
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1248
24.5 Asymptotic Expansions of Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1249
24.5.1 The Parabolic Cylinder Equation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1249
25 Hilbert Spaces
1255
25.1 Linear Spaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1255
25.2 Inner Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1257
25.3 Norms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1258
25.4 Linear Independence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1260
25.5 Orthogonality
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1260
25.6 Gramm-Schmidt Orthogonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1261
25.7 Orthonormal Function Expansion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1263
25.8 Sets Of Functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1265
25.9 Least Squares Fit to a Function and Completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1272
25.10Closure Relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1275
25.11Linear Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1280
25.12Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1281
25.13Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1282
25.14Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1283
26 Self Adjoint Linear Operators
1285
26.1 Adjoint Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1285
26.2 Self-Adjoint Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1286
26.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1289
xii

26.4 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1290
26.5 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1291
27 Self-Adjoint Boundary Value Problems
1292
27.1 Summary of Adjoint Operators
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1292
27.2 Formally Self-Adjoint Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1293
27.3 Self-Adjoint Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1296
27.4 Self-Adjoint Eigenvalue Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1296
27.5 Inhomogeneous Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1301
27.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1304
27.7 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1305
27.8 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1306
28 Fourier Series
1308
28.1 An Eigenvalue Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1308
28.2 Fourier Series. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1311
28.3 Least Squares Fit
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1315
28.4 Fourier Series for Functions Deﬁned on Arbitrary Ranges . . . . . . . . . . . . . . . . . . . . . . . . . 1319
28.5 Fourier Cosine Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1322
28.6 Fourier Sine Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1323
28.7 Complex Fourier Series and Parseval’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1324
28.8 Behavior of Fourier Coeﬃcients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1327
28.9 Gibb’s Phenomenon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1336
28.10Integrating and Diﬀerentiating Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1336
28.11Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1341
28.12Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1349
28.13Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1351
29 Regular Sturm-Liouville Problems
1398
29.1 Derivation of the Sturm-Liouville Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1398
xiii

29.2 Properties of Regular Sturm-Liouville Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1400
29.3 Solving Diﬀerential Equations With Eigenfunction Expansions . . . . . . . . . . . . . . . . . . . . . . 1411
29.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1417
29.5 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1421
29.6 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1423
30 Integrals and Convergence
1448
30.1 Uniform Convergence of Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1448
30.2 The Riemann-Lebesgue Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1449
30.3 Cauchy Principal Value
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1450
30.3.1 Integrals on an Inﬁnite Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1450
30.3.2 Singular Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1451
31 The Laplace Transform
1453
31.1 The Laplace Transform
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1453
31.2 The Inverse Laplace Transform
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1455
31.2.1
ˆf(s) with Poles
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1458
31.2.2
ˆf(s) with Branch Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1463
31.2.3 Asymptotic Behavior of ˆf(s)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1466
31.3 Properties of the Laplace Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1468
31.4 Constant Coeﬃcient Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1471
31.5 Systems of Constant Coeﬃcient Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . 1473
31.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1476
31.7 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1483
31.8 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1486
32 The Fourier Transform
1518
32.1 Derivation from a Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1518
32.2 The Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1520
32.2.1 A Word of Caution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1523
xiv

32.3 Evaluating Fourier Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1524
32.3.1 Integrals that Converge
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1524
32.3.2 Cauchy Principal Value and Integrals that are Not Absolutely Convergent. . . . . . . . . . . . . 1527
32.3.3 Analytic Continuation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1529
32.4 Properties of the Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1531
32.4.1 Closure Relation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1531
32.4.2 Fourier Transform of a Derivative. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1532
32.4.3 Fourier Convolution Theorem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1534
32.4.4 Parseval’s Theorem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1537
32.4.5 Shift Property. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1539
32.4.6 Fourier Transform of x f(x). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1539
32.5 Solving Diﬀerential Equations with the Fourier Transform
. . . . . . . . . . . . . . . . . . . . . . . . 1540
32.6 The Fourier Cosine and Sine Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1542
32.6.1 The Fourier Cosine Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1542
32.6.2 The Fourier Sine Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1543
32.7 Properties of the Fourier Cosine and Sine Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . 1544
32.7.1 Transforms of Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1544
32.7.2 Convolution Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1546
32.7.3 Cosine and Sine Transform in Terms of the Fourier Transform . . . . . . . . . . . . . . . . . . 1548
32.8 Solving Diﬀerential Equations with the Fourier Cosine and Sine Transforms . . . . . . . . . . . . . . . 1549
32.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1551
32.10Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1558
32.11Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1561
33 The Gamma Function
1585
33.1 Euler’s Formula
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1585
33.2 Hankel’s Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1587
33.3 Gauss’ Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1589
33.4 Weierstrass’ Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1591
33.5 Stirling’s Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1593
xv

33.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1598
33.7 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1599
33.8 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1600
34 Bessel Functions
1602
34.1 Bessel’s Equation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1602
34.2 Frobeneius Series Solution about z = 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1603
34.2.1 Behavior at Inﬁnity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1606
34.3 Bessel Functions of the First Kind . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1608
34.3.1 The Bessel Function Satisﬁes Bessel’s Equation . . . . . . . . . . . . . . . . . . . . . . . . . . 1609
34.3.2 Series Expansion of the Bessel Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1610
34.3.3 Bessel Functions of Non-Integer Order
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1613
34.3.4 Recursion Formulas
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1616
34.3.5 Bessel Functions of Half-Integer Order
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1619
34.4 Neumann Expansions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1620
34.5 Bessel Functions of the Second Kind
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1624
34.6 Hankel Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1626
34.7 The Modiﬁed Bessel Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1626
34.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1630
34.9 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1635
34.10Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1637
V
Partial Diﬀerential Equations
1660
35 Transforming Equations
1661
35.1 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1662
35.2 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1663
35.3 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1664
xvi

36 Classiﬁcation of Partial Diﬀerential Equations
1665
36.1 Classiﬁcation of Second Order Quasi-Linear Equations . . . . . . . . . . . . . . . . . . . . . . . . . . 1665
36.1.1 Hyperbolic Equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1666
36.1.2 Parabolic equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1671
36.1.3 Elliptic Equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1672
36.2 Equilibrium Solutions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1674
36.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1676
36.4 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1677
36.5 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1678
37 Separation of Variables
1684
37.1 Eigensolutions of Homogeneous Equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1684
37.2 Homogeneous Equations with Homogeneous Boundary Conditions . . . . . . . . . . . . . . . . . . . . 1684
37.3 Time-Independent Sources and Boundary Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . 1686
37.4 Inhomogeneous Equations with Homogeneous Boundary Conditions . . . . . . . . . . . . . . . . . . . 1689
37.5 Inhomogeneous Boundary Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1690
37.6 The Wave Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1693
37.7 General Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1696
37.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1698
37.9 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1714
37.10Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1719
38 Finite Transforms
1801
38.1 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1805
38.2 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1806
38.3 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1807
39 The Diﬀusion Equation
1811
39.1 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1812
39.2 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1814
xvii

39.3 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1815
40 Laplace’s Equation
1821
40.1 Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1821
40.2 Fundamental Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1821
40.2.1 Two Dimensional Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1822
40.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1823
40.4 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1826
40.5 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1827
41 Waves
1839
41.1 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1840
41.2 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1846
41.3 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1848
42 Similarity Methods
1868
42.1 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1873
42.2 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1874
42.3 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1875
43 Method of Characteristics
1878
43.1 First Order Linear Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1878
43.2 First Order Quasi-Linear Equations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1879
43.3 The Method of Characteristics and the Wave Equation . . . . . . . . . . . . . . . . . . . . . . . . . . 1881
43.4 The Wave Equation for an Inﬁnite Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1882
43.5 The Wave Equation for a Semi-Inﬁnite Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1883
43.6 The Wave Equation for a Finite Domain
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1885
43.7 Envelopes of Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1886
43.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1889
43.9 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1891
xviii

43.10Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1892
44 Transform Methods
1899
44.1 Fourier Transform for Partial Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1899
44.2 The Fourier Sine Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1901
44.3 Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1901
44.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1903
44.5 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1907
44.6 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1909
45 Green Functions
1931
45.1 Inhomogeneous Equations and Homogeneous Boundary Conditions
. . . . . . . . . . . . . . . . . . . 1931
45.2 Homogeneous Equations and Inhomogeneous Boundary Conditions
. . . . . . . . . . . . . . . . . . . 1932
45.3 Eigenfunction Expansions for Elliptic Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1934
45.4 The Method of Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1939
45.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1941
45.6 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1952
45.7 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1955
46 Conformal Mapping
2015
46.1 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2016
46.2 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2019
46.3 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2020
47 Non-Cartesian Coordinates
2032
47.1 Spherical Coordinates
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2032
47.2 Laplace’s Equation in a Disk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2033
47.3 Laplace’s Equation in an Annulus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2036
xix

VI
Calculus of Variations
2040
48 Calculus of Variations
2041
48.1 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2042
48.2 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2056
48.3 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2060
VII
Nonlinear Diﬀerential Equations
2147
49 Nonlinear Ordinary Diﬀerential Equations
2148
49.1 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2149
49.2 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2154
49.3 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2155
50 Nonlinear Partial Diﬀerential Equations
2177
50.1 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2178
50.2 Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2181
50.3 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2182
VIII
Appendices
2201
A Greek Letters
2202
B Notation
2204
C Formulas from Complex Variables
2206
D Table of Derivatives
2209
xx

E
Table of Integrals
2213
F
Deﬁnite Integrals
2217
G Table of Sums
2219
H Table of Taylor Series
2222
I
Table of Laplace Transforms
2225
I.1
Properties of Laplace Transforms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2225
I.2
Table of Laplace Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2227
J
Table of Fourier Transforms
2231
K Table of Fourier Transforms in n Dimensions
2234
L
Table of Fourier Cosine Transforms
2235
M Table of Fourier Sine Transforms
2237
N Table of Wronskians
2239
O Sturm-Liouville Eigenvalue Problems
2241
P Green Functions for Ordinary Diﬀerential Equations
2243
Q Trigonometric Identities
2246
Q.1 Circular Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2246
Q.2 Hyperbolic Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2248
R Bessel Functions
2251
R.1
Deﬁnite Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2251
xxi

S
Formulas from Linear Algebra
2252
T Vector Analysis
2253
U Partial Fractions
2255
V Finite Math
2259
W Probability
2260
W.1 Independent Events
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2260
W.2 Playing the Odds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2261
X Economics
2262
Y Glossary
2263
xxii

Anti-Copyright
Anti-Copyright @ 1995-2001 by Mauch Publishing Company, un-Incorporated.
No rights reserved. Any part of this publication may be reproduced, stored in a retrieval system, transmitted or
desecrated without permission.
xxiii

Preface
During the summer before my ﬁnal undergraduate year at Caltech I set out to write a math text unlike any other,
namely, one written by me. In that respect I have succeeded beautifully. Unfortunately, the text is neither complete nor
polished. I have a “Warnings and Disclaimers” section below that is a little amusing, and an appendix on probability
that I feel concisesly captures the essence of the subject. However, all the material in between is in some stage of
development. I am currently working to improve and expand this text.
This text is freely available from my web set. Currently I’m at http://www.its.caltech.edu/˜sean.
I post new
versions a couple of times a year.
0.1
Advice to Teachers
If you have something worth saying, write it down.
0.2
Acknowledgments
I would like to thank Professor Saﬀman for advising me on this project and the Caltech SURF program for providing
the funding for me to write the ﬁrst edition of this book.
xxiv

0.3
Warnings and Disclaimers
• This book is a work in progress. It contains quite a few mistakes and typos. I would greatly appreciate your
constructive criticism. You can reach me at ‘sean@its.caltech.edu’.
• Reading this book impairs your ability to drive a car or operate machinery.
• This book has been found to cause drowsiness in laboratory animals.
• This book contains twenty-three times the US RDA of ﬁber.
• Caution: FLAMMABLE - Do not read while smoking or near a ﬁre.
• If infection, rash, or irritation develops, discontinue use and consult a physician.
• Warning: For external use only. Use only as directed. Intentional misuse by deliberately concentrating contents
can be harmful or fatal. KEEP OUT OF REACH OF CHILDREN.
• In the unlikely event of a water landing do not use this book as a ﬂotation device.
• The material in this text is ﬁction; any resemblance to real theorems, living or dead, is purely coincidental.
• This is by far the most amusing section of this book.
• Finding the typos and mistakes in this book is left as an exercise for the reader. (Eye ewes a spelling chequer
from thyme too thyme, sew their should knot bee two many misspellings. Though I ain’t so sure the grammar’s
too good.)
• The theorems and methods in this text are subject to change without notice.
• This is a chain book. If you do not make seven copies and distribute them to your friends within ten days of
obtaining this text you will suﬀer great misfortune and other nastiness.
• The surgeon general has determined that excessive studying is detrimental to your social life.
xxv

• This text has been buﬀered for your protection and ribbed for your pleasure.
• Stop reading this rubbish and get back to work!
0.4
Suggested Use
This text is well suited to the student, professional or lay-person. It makes a superb gift. This text has a boquet that
is light and fruity, with some earthy undertones. It is ideal with dinner or as an apertif. Bon apetit!
0.5
About the Title
The title is only making light of naming conventions in the sciences and is not an insult to engineers. If you want to
learn about some mathematical subject, look for books with “Introduction” or “Elementary” in the title. If it is an
“Intermediate” text it will be incomprehensible. If it is “Advanced” then not only will it be incomprehensible, it will
have low production qualities, i.e. a crappy typewriter font, no graphics and no examples. There is an exception to this
rule: When the title also contains the word “Scientists” or “Engineers” the advanced book may be quite suitable for
actually learning the material.
xxvi

Part I
Algebra
1

Chapter 1
Sets and Functions
1.1
Sets
Deﬁnition.
A set is a collection of objects. We call the objects, elements. A set is denoted by listing the elements
between braces.
For example: {e, ı, π, 1}.
We use ellipses to indicate patterns.
The set of positive integers is
{1, 2, 3, . . .}. We also denote a sets with the notation {x|conditions on x} for sets that are more easily described than
enumerated. This is read as “the set of elements x such that x satisﬁes . . . ”. x ∈S is the notation for “x is an
element of the set S.” To express the opposite we have x ̸∈S for “x is not an element of the set S.”
Examples.
We have notations for denoting some of the commonly encountered sets.
• ∅= {} is the empty set, the set containing no elements.
• Z = {. . . , −1, 0, 1 . . .} is the set of integers. (Z is for “Zahlen”, the German word for “number”.)
• Q = {p/q|p, q ∈Z, q ̸= 0} is the set of rational numbers. (Q is for quotient.)
• R = {x|x = a1a2 · · · an.b1b2 · · · } is the set of real numbers, i.e. the set of numbers with decimal expansions. 1
1Guess what R is for.
2

• C = {a + ıb|a, b ∈R, ı2 = −1} is the set of complex numbers. ı is the square root of −1. (If you haven’t seen
complex numbers before, don’t dismay. We’ll cover them later.)
• Z+, Q+ and R+ are the sets of positive integers, rationals and reals, respectively. For example, Z+ = {1, 2, 3, . . .}.
• Z0+, Q0+ and R0+ are the sets of non-negative integers, rationals and reals, respectively. For example, Z0+ =
{0, 1, 2, . . .}.
• (a . . . b) denotes an open interval on the real axis. (a . . . b) ≡{x|x ∈R, a < x < b}
• We use brackets to denote the closed interval. [a . . . b] ≡{x|x ∈R, a ≤x ≤b}
The cardinality or order of a set S is denoted |S|.
For ﬁnite sets, the cardinality is the number of elements in the
set. The Cartesian product of two sets is the set of ordered pairs:
X × Y ≡{(x, y)|x ∈X, y ∈Y }.
The Cartesian product of n sets is the set of ordered n-tuples:
X1 × X2 × · · · × Xn ≡{(x1, x2, . . . , xn)|x1 ∈X1, x2 ∈X2, . . . , xn ∈Xn}.
Equality.
Two sets S and T are equal if each element of S is an element of T and vice versa. This is denoted,
S = T. Inequality is S ̸= T, of course. S is a subset of T, S ⊆T, if every element of S is an element of T. S is a
proper subset of T, S ⊂T, if S ⊆T and S ̸= T. For example: The empty set is a subset of every set, ∅⊆S. The
rational numbers are a proper subset of the real numbers, Q ⊂R.
Operations.
The union of two sets, S ∪T, is the set whose elements are in either of the two sets. The union of n
sets,
∪n
j=1Sj ≡S1 ∪S2 ∪· · · ∪Sn
is the set whose elements are in any of the sets Sj. The intersection of two sets, S ∩T, is the set whose elements are
in both of the two sets. In other words, the intersection of two sets in the set of elements that the two sets have in
common. The intersection of n sets,
∩n
j=1Sj ≡S1 ∩S2 ∩· · · ∩Sn
3

is the set whose elements are in all of the sets Sj. If two sets have no elements in common, S ∩T = ∅, then the sets
are disjoint. If T ⊆S, then the diﬀerence between S and T, S \ T, is the set of elements in S which are not in T.
S \ T ≡{x|x ∈S, x ̸∈T}
The diﬀerence of sets is also denoted S −T.
Properties.
The following properties are easily veriﬁed from the above deﬁnitions.
• S ∪∅= S, S ∩∅= ∅, S \ ∅= S, S \ S = ∅.
• Commutative. S ∪T = T ∪S, S ∩T = T ∩S.
• Associative. (S ∪T) ∪U = S ∪(T ∪U) = S ∪T ∪U, (S ∩T) ∩U = S ∩(T ∩U) = S ∩T ∩U.
• Distributive. S ∪(T ∩U) = (S ∪T) ∩(S ∪U), S ∩(T ∪U) = (S ∩T) ∪(S ∩U).
1.2
Single Valued Functions
Single-Valued Functions.
A single-valued function or single-valued mapping is a mapping of the elements x ∈X
into elements y ∈Y . This is expressed as f : X →Y or X
f→Y . If such a function is well-deﬁned, then for each
x ∈X there exists a unique element of y such that f(x) = y. The set X is the domain of the function, Y is the
codomain, (not to be confused with the range, which we introduce shortly).
To denote the value of a function on a
particular element we can use any of the notations: f(x) = y, f : x 7→y or simply x 7→y. f is the identity map on
X if f(x) = x for all x ∈X.
Let f : X →Y . The range or image of f is
f(X) = {y|y = f(x) for some x ∈X}.
The range is a subset of the codomain. For each Z ⊆Y , the inverse image of Z is deﬁned:
f −1(Z) ≡{x ∈X|f(x) = z for some z ∈Z}.
4

Examples.
• Finite polynomials and the exponential function are examples of single valued functions which map real numbers
to real numbers.
• The greatest integer function, ⌊·⌋, is a mapping from R to Z. ⌊x⌋in the greatest integer less than or equal to x.
Likewise, the least integer function, ⌈x⌉, is the least integer greater than or equal to x.
The -jectives.
A function is injective if for each x1 ̸= x2, f(x1) ̸= f(x2). In other words, for each x in the domain
there is a unique y = f(x) in the range. f is surjective if for each y in the codomain, there is an x such that y = f(x).
If a function is both injective and surjective, then it is bijective. A bijective function is also called a one-to-one mapping.
Examples.
• The exponential function y = ex is a bijective function, (one-to-one mapping), that maps R to R+. (R is the set
of real numbers; R+ is the set of positive real numbers.)
• f(x) = x2 is a bijection from R+ to R+. f is not injective from R to R+. For each positive y in the range, there
are two values of x such that y = x2.
• f(x) = sin x is not injective from R to [−1..1]. For each y ∈[−1, 1] there exists an inﬁnite number of values of
x such that y = sin x.
1.3
Inverses and Multi-Valued Functions
If y = f(x), then we can write x = f −1(y) where f −1 is the inverse of f. If y = f(x) is a one-to-one function, then
f −1(y) is also a one-to-one function. In this case, x = f −1(f(x)) = f(f −1(x)) for values of x where both f(x) and
f −1(x) are deﬁned. For example log x, which maps R+ to R is the inverse of ex. x = elog x = log(ex) for all x ∈R+.
(Note the x ∈R+ ensures that log x is deﬁned.)
5

Injective
Surjective
Bijective
Figure 1.1: Depictions of Injective, Surjective and Bijective Functions
If y = f(x) is a many-to-one function, then x = f −1(y) is a one-to-many function. f −1(y) is a multi-valued function.
We have x = f(f −1(x)) for values of x where f −1(x) is deﬁned, however x ̸= f −1(f(x)). There are diagrams showing
one-to-one, many-to-one and one-to-many functions in Figure 1.2.
range
domain
range
domain
range
domain
one-to-one
many-to-one
one-to-many
Figure 1.2: Diagrams of One-To-One, Many-To-One and One-To-Many Functions
Example 1.3.1 y = x2, a many-to-one function has the inverse x = y1/2. For each positive y, there are two values of
x such that x = y1/2. y = x2 and y = x1/2 are graphed in Figure 1.3.
6

Figure 1.3: y = x2 and y = x1/2
We say that there are two branches of y = x1/2: the positive and the negative branch. We denote the positive
branch as y = √x; the negative branch is y = −√x. We call √x the principal branch of x1/2. Note that √x is a
one-to-one function. Finally, x = (x1/2)2 since (±√x)2 = x, but x ̸= (x2)1/2 since (x2)1/2 = ±x. y = √x is graphed
in Figure 1.4.
Figure 1.4: y = √x
Now consider the many-to-one function y = sin x. The inverse is x = arcsin y. For each y ∈[−1, 1] there are an
inﬁnite number of values x such that x = arcsin y. In Figure 1.5 is a graph of y = sin x and a graph of a few branches
of y = arcsin x.
Example 1.3.2 arcsin x has an inﬁnite number of branches. We will denote the principal branch by Arcsin x which
maps [−1, 1] to

−π
2, π
2

. Note that x = sin(arcsin x), but x ̸= arcsin(sin x). y = Arcsin x in Figure 1.6.
7

Figure 1.5: y = sin x and y = arcsin x
Figure 1.6: y = Arcsin x
Example 1.3.3 Consider 11/3. Since x3 is a one-to-one function, x1/3 is a single-valued function. (See Figure 1.7.)
11/3 = 1.
Figure 1.7: y = x3 and y = x1/3
8

Example 1.3.4 Consider arccos(1/2). cos x and a few branches of arccos x are graphed in Figure 1.8. cos x = 1/2
Figure 1.8: y = cos x and y = arccos x
has the two solutions x = ±π/3 in the range x ∈[−π, π]. Since cos(x + π) = −cos x,
arccos(1/2) = {±π/3 + nπ}.
1.4
Transforming Equations
We must take care in applying functions to equations. It is always safe to apply a one-to-one function to an equation,
(provided it is deﬁned for that domain). For example, we can apply y = x3 or y = ex to the equation x = 1. The
equations x3 = 1 and ex = e have the unique solution x = 1.
If we apply a many-to-one function to an equation, we may introduce spurious solutions. Applying y = x2 and
y = sin x to the equation x = π
2 results in x2 = π2
4 and sin x = 1. The former equation has the two solutions x = ±π
2;
the latter has the inﬁnite number of solutions x = π
2 + 2nπ, n ∈Z.
We do not generally apply a one-to-many function to both sides of an equation as this rarely is useful. Consider the
equation
sin2 x = 1.
9

Applying the function f(x) = x1/2 to the equation would not get us anywhere
(sin2 x)1/2 = 11/2.
Since (sin2 x)1/2 ̸= sin x, we cannot simplify the left side of the equation. Instead we could use the deﬁnition of
f(x) = x1/2 as the inverse of the x2 function to obtain
sin x = 11/2 = ±1.
Then we could use the deﬁnition of arcsin as the inverse of sin to get
x = arcsin(±1).
x = arcsin(1) has the solutions x = π/2 + 2nπ and x = arcsin(−1) has the solutions x = −π/2 + 2nπ. Thus
x = π
2 + nπ,
n ∈Z.
Note that we cannot just apply arcsin to both sides of the equation as arcsin(sin x) ̸= x.
10

1.5
Exercises
Exercise 1.1
The area of a circle is directly proportional to the square of its diameter. What is the constant of proportionality?
Hint, Solution
Exercise 1.2
Consider the equation
x + 1
y −2 = x2 −1
y2 −4.
1. Why might one think that this is the equation of a line?
2. Graph the solutions of the equation to demonstrate that it is not the equation of a line.
Hint, Solution
Exercise 1.3
Consider the function of a real variable,
f(x) =
1
x2 + 2.
What is the domain and range of the function?
Hint, Solution
Exercise 1.4
The temperature measured in degrees Celsius 2 is linearly related to the temperature measured in degrees Fahrenheit 3.
Water freezes at 0◦C = 32◦F and boils at 100◦C = 212◦F. Write the temperature in degrees Celsius as a function
of degrees Fahrenheit.
2 Originally, it was called degrees Centigrade. centi because there are 100 degrees between the two calibration points. It is now
called degrees Celsius in honor of the inventor.
3 The Fahrenheit scale, named for Daniel Fahrenheit, was originally calibrated with the freezing point of salt-saturated water to
be 0◦. Later, the calibration points became the freezing point of water, 32◦, and body temperature, 96◦. With this method, there are
64 divisions between the calibration points. Finally, the upper calibration point was changed to the boiling point of water at 212◦.
This gave 180 divisions, (the number of degrees in a half circle), between the two calibration points.
11

Hint, Solution
Exercise 1.5
Consider the function graphed in Figure 1.9. Sketch graphs of f(−x), f(x + 3), f(3 −x) + 2, and f −1(x). You may
use the blank grids in Figure 1.10.
Figure 1.9: Graph of the function.
Hint, Solution
Exercise 1.6
A culture of bacteria grows at the rate of 10% per minute. At 6:00 pm there are 1 billion bacteria. How many bacteria
are there at 7:00 pm? How many were there at 3:00 pm?
Hint, Solution
Exercise 1.7
The graph in Figure 1.11 shows an even function f(x) = p(x)/q(x) where p(x) and q(x) are rational quadratic
polynomials. Give possible formulas for p(x) and q(x).
Hint, Solution
12

Figure 1.10: Blank grids.
Exercise 1.8
Find a polynomial of degree 100 which is zero only at x = −2, 1, π and is non-negative.
Hint, Solution
Exercise 1.9
Hint, Solution
13

1
2
1
2
2
4
6
8
10
1
2
Figure 1.11: Plots of f(x) = p(x)/q(x).
Exercise 1.10
Hint, Solution
Exercise 1.11
Hint, Solution
Exercise 1.12
Hint, Solution
Exercise 1.13
Hint, Solution
Exercise 1.14
Hint, Solution
Exercise 1.15
Hint, Solution
Exercise 1.16
Hint, Solution
14

1.6
Hints
Hint 1.1
area = constant × diameter2.
Hint 1.2
A pair (x, y) is a solution of the equation if it make the equation an identity.
Hint 1.3
The domain is the subset of R on which the function is deﬁned.
Hint 1.4
Find the slope and x-intercept of the line.
Hint 1.5
The inverse of the function is the reﬂection of the function across the line y = x.
Hint 1.6
The formula for geometric growth/decay is x(t) = x0rt, where r is the rate.
Hint 1.7
Note that p(x) and q(x) appear as a ratio, they are determined only up to a multiplicative constant. We may take the
leading coeﬃcient of q(x) to be unity.
f(x) = p(x)
q(x) = ax2 + bx + c
x2 + βx + χ
Use the properties of the function to solve for the unknown parameters.
Hint 1.8
Write the polynomial in factored form.
15

1.7
Solutions
Solution 1.1
area = π × radius2
area = π
4 × diameter2
The constant of proportionality is π
4.
Solution 1.2
1. If we multiply the equation by y2 −4 and divide by x + 1, we obtain the equation of a line.
y + 2 = x −1
2. We factor the quadratics on the right side of the equation.
x + 1
y −2 = (x + 1)(x −1)
(y −2)(y + 2).
We note that one or both sides of the equation are undeﬁned at y = ±2 because of division by zero. There are
no solutions for these two values of y and we assume from this point that y ̸= ±2. We multiply by (y −2)(y +2).
(x + 1)(y + 2) = (x + 1)(x −1)
For x = −1, the equation becomes the identity 0 = 0. Now we consider x ̸= −1. We divide by x + 1 to obtain
the equation of a line.
y + 2 = x −1
y = x −3
Now we collect the solutions we have found.
{(−1, y) : y ̸= ±2} ∪{(x, x −3) : x ̸= 1, 5}
The solutions are depicted in Figure /refﬁg not a line.
16

-6
-4
-2
2
4
6
-6
-4
-2
2
4
6
Figure 1.12: The solutions of x+1
y−2 = x2−1
y2−4.
Solution 1.3
The denominator is nonzero for all x ∈R. Since we don’t have any division by zero problems, the domain of the
function is R. For x ∈R,
0 <
1
x2 + 2 ≤2.
Consider
y =
1
x2 + 2.
(1.1)
For any y ∈(0 . . . 1/2], there is at least one value of x that satisﬁes Equation 1.1.
x2 + 2 = 1
y
x = ±
r1
y −2
Thus the range of the function is (0 . . . 1/2]
17

Solution 1.4
Let c denote degrees Celsius and f denote degrees Fahrenheit. The line passes through the points (f, c) = (32, 0) and
(f, c) = (212, 100). The x-intercept is f = 32. We calculate the slope of the line.
slope = 100 −0
212 −32 = 100
180 = 5
9
The relationship between fahrenheit and celcius is
c = 5
9(f −32).
Solution 1.5
We plot the various transformations of f(x).
Solution 1.6
The formula for geometric growth/decay is x(t) = x0rt, where r is the rate. Let t = 0 coincide with 6:00 pm. We
determine x0.
x(0) = 109 = x0
11
10
0
= x0
x0 = 109
At 7:00 pm the number of bacteria is
109
11
10
60
= 1160
1051 ≈3.04 × 1011
At 3:00 pm the number of bacteria was
109
11
10
−180
= 10189
11180 ≈35.4
18

Figure 1.13: Graphs of f(−x), f(x + 3), f(3 −x) + 2, and f −1(x).
Solution 1.7
We write p(x) and q(x) as general quadratic polynomials.
f(x) = p(x)
q(x) = ax2 + bx + c
αx2 + βx + χ
We will use the properties of the function to solve for the unknown parameters.
19

Note that p(x) and q(x) appear as a ratio, they are determined only up to a multiplicative constant. We may take
the leading coeﬃcient of q(x) to be unity.
f(x) = p(x)
q(x) = ax2 + bx + c
x2 + βx + χ
f(x) has a second order zero at x = 0. This means that p(x) has a second order zero there and that χ ̸= 0.
f(x) =
ax2
x2 + βx + χ
We note that f(x) →2 as x →∞. This determines the parameter a.
lim
x→∞f(x) = lim
x→∞
ax2
x2 + βx + χ
= lim
x→∞
2ax
2x + β
= lim
x→∞
2a
2
= a
f(x) =
2x2
x2 + βx + χ
Now we use the fact that f(x) is even to conclude that q(x) is even and thus β = 0.
f(x) =
2x2
x2 + χ
Finally, we use that f(1) = 1 to determine χ.
f(x) =
2x2
x2 + 1
20

Solution 1.8
Consider the polynomial
p(x) = (x + 2)40(x −1)30(x −π)30.
It is of degree 100. Since the factors only vanish at x = −2, 1, π, p(x) only vanishes there. Since factors are non-
negative, the polynomial is non-negative.
21

Chapter 2
Vectors
2.1
Vectors
2.1.1
Scalars and Vectors
A vector is a quantity having both a magnitude and a direction. Examples of vector quantities are velocity, force
and position. One can represent a vector in n-dimensional space with an arrow whose initial point is at the origin,
(Figure 2.1). The magnitude is the length of the vector. Typographically, variables representing vectors are often
written in capital letters, bold face or with a vector over-line, A, a,⃗a. The magnitude of a vector is denoted |a|.
A scalar has only a magnitude. Examples of scalar quantities are mass, time and speed.
Vector Algebra.
Two vectors are equal if they have the same magnitude and direction. The negative of a vector,
denoted −a, is a vector of the same magnitude as a but in the opposite direction. We add two vectors a and b by
placing the tail of b at the head of a and deﬁning a + b to be the vector with tail at the origin and head at the head
of b. (See Figure 2.2.)
The diﬀerence, a −b, is deﬁned as the sum of a and the negative of b, a + (−b). The result of multiplying a by
a scalar α is a vector of magnitude |α| |a| with the same/opposite direction if α is positive/negative. (See Figure 2.2.)
22

x
z
y
Figure 2.1: Graphical Representation of a Vector in Three Dimensions
a+b
a
b
-a
a
2a
Figure 2.2: Vector Arithmetic
Here are the properties of adding vectors and multiplying them by a scalar.
They are evident from geometric
considerations.
a + b = b + a
αa = aα
commutative laws
(a + b) + c = a + (b + c)
α(βa) = (αβ)a
associative laws
α(a + b) = αa + αb
(α + β)a = αa + βa
distributive laws
23

Zero and Unit Vectors.
The additive identity element for vectors is the zero vector or null vector. This is a vector
of magnitude zero which is denoted as 0. A unit vector is a vector of magnitude one. If a is nonzero then a/|a| is a
unit vector in the direction of a. Unit vectors are often denoted with a caret over-line, ˆn.
Rectangular Unit Vectors.
In n dimensional Cartesian space, Rn, the unit vectors in the directions of the
coordinates axes are e1, . . . en. These are called the rectangular unit vectors. To cut down on subscripts, the unit
vectors in three dimensional space are often denoted with i, j and k. (Figure 2.3).
x
z
y
j
k
i
Figure 2.3: Rectangular Unit Vectors
Components of a Vector.
Consider a vector a with tail at the origin and head having the Cartesian coordinates
(a1, . . . , an). We can represent this vector as the sum of n rectangular component vectors, a = a1e1 + · · · + anen.
(See Figure 2.4.) Another notation for the vector a is ⟨a1, . . . , an⟩. By the Pythagorean theorem, the magnitude of
the vector a is |a| =
p
a2
1 + · · · + a2
n.
24

x
z
y
a
a
a
1
3
i
k
j
a2
Figure 2.4: Components of a Vector
2.1.2
The Kronecker Delta and Einstein Summation Convention
The Kronecker Delta tensor is deﬁned
δij =
(
1
if i = j,
0
if i ̸= j.
This notation will be useful in our work with vectors.
Consider writing a vector in terms of its rectangular components. Instead of using ellipses: a = a1e1 +· · ·+anen, we
could write the expression as a sum: a = Pn
i=1 aiei. We can shorten this notation by leaving out the sum: a = aiei,
where it is understood that whenever an index is repeated in a term we sum over that index from 1 to n. This is the
Einstein summation convention. A repeated index is called a summation index or a dummy index. Other indices can
take any value from 1 to n and are called free indices.
25

Example 2.1.1 Consider the matrix equation: A · x = b. We can write out the matrix and vectors explicitly.



a11
· · ·
a1n
...
...
...
an1
· · ·
ann






x1...
xn


=



b1...
bn



This takes much less space when we use the summation convention.
aijxj = bi
Here j is a summation index and i is a free index.
2.1.3
The Dot and Cross Product
Dot Product.
The dot product or scalar product of two vectors is deﬁned,
a · b ≡|a||b| cos θ,
where θ is the angle from a to b. From this deﬁnition one can derive the following properties:
• a · b = b · a, commutative.
• α(a · b) = (αa) · b = a · (αb), associativity of scalar multiplication.
• a · (b + c) = a · b + a · c, distributive.
• eiej = δij. In three dimension, this is
i · i = j · j = k · k = 1,
i · j = j · k = k · i = 0.
• a · b = aibi ≡a1b1 + · · · + anbn, dot product in terms of rectangular components.
• If a · b = 0 then either a and b are orthogonal, (perpendicular), or one of a and b are zero.
26

The Angle Between Two Vectors.
We can use the dot product to ﬁnd the angle between two vectors, a and
b. From the deﬁnition of the dot product,
a · b = |a||b| cos θ.
If the vectors are nonzero, then
θ = arccos
 a · b
|a||b|

.
Example 2.1.2 What is the angle between i and i + j?
θ = arccos
i · (i + j)
|i||i + j|

= arccos
 1
√
2

= π
4 .
Parametric Equation of a Line.
Consider a line that passes through the point a and is parallel to the vector t,
(tangent). A parametric equation of the line is
x = a + ut,
u ∈R.
Implicit Equation of a Line.
Consider a line that passes through the point a and is normal, (orthogonal, per-
pendicular), to the vector n. All the lines that are normal to n have the property that x · n is a constant, where x is
any point on the line. (See Figure 2.5.) x · n = 0 is the line that is normal to n and passes through the origin. The
line that is normal to n and passes through the point a is
x · n = a · n.
27

=0
=1
= a n
n
a
=-1
x n
x n
x n
x n
Figure 2.5: Equation for a Line
The normal to a line determines an orientation of the line. The normal points in the direction that is above the
line. A point b is (above/on/below) the line if (b −a) · n is (positive/zero/negative). The signed distance of a point
b from the line x · n = a · n is
(b −a) · n
|n|.
Implicit Equation of a Hyperplane.
A hyperplane in Rn is an n −1 dimensional “sheet” which passes through
a given point and is normal to a given direction. In R3 we call this a plane. Consider a hyperplane that passes through
the point a and is normal to the vector n. All the hyperplanes that are normal to n have the property that x · n is a
constant, where x is any point in the hyperplane. x · n = 0 is the hyperplane that is normal to n and passes through
the origin. The hyperplane that is normal to n and passes through the point a is
x · n = a · n.
The normal determines an orientation of the hyperplane. The normal points in the direction that is above the
hyperplane. A point b is (above/on/below) the hyperplane if (b −a) · n is (positive/zero/negative). The signed
28

distance of a point b from the hyperplane x · n = a · n is
(b −a) · n
|n|.
Right and Left-Handed Coordinate Systems.
Consider a rectangular coordinate system in two dimensions.
Angles are measured from the positive x axis in the direction of the positive y axis. There are two ways of labeling the
axes. (See Figure 2.6.) In one the angle increases in the counterclockwise direction and in the other the angle increases
in the clockwise direction. The former is the familiar Cartesian coordinate system.
x
y
x
y
θ
θ
Figure 2.6: There are Two Ways of Labeling the Axes in Two Dimensions.
There are also two ways of labeling the axes in a three-dimensional rectangular coordinate system. These are called
right-handed and left-handed coordinate systems. See Figure 2.7. Any other labelling of the axes could be rotated into
one of these conﬁgurations. The right-handed system is the one that is used by default. If you put your right thumb in
the direction of the z axis in a right-handed coordinate system, then your ﬁngers curl in the direction from the x axis
to the y axis.
Cross Product.
The cross product or vector product is deﬁned,
a × b = |a||b| sin θ n,
where θ is the angle from a to b and n is a unit vector that is orthogonal to a and b and in the direction such that a,
b and n form a right-handed system.
29

x
z
y
j
i
k
z
k
j
i
y
x
Figure 2.7: Right and Left Handed Coordinate Systems
You can visualize the direction of a × b by applying the right hand rule. Curl the ﬁngers of your right hand in the
direction from a to b. Your thumb points in the direction of a × b. Warning: Unless you are a lefty, get in the habit
of putting down your pencil before applying the right hand rule.
The dot and cross products behave a little diﬀerently. First note that unlike the dot product, the cross product is not
commutative. The magnitudes of a × b and b × a are the same, but their directions are opposite. (See Figure 2.8.)
a
b
b  a
a  b
Figure 2.8: The Cross Product is Anti-Commutative.
30

Let
a × b = |a||b| sin θ n
and
b × a = |b||a| sin φ m.
The angle from a to b is the same as the angle from b to a. Since {a, b, n} and {b, a, m} are right-handed systems,
m points in the opposite direction as n. Since a × b = −b × a we say that the cross product is anti-commutative.
Next we note that since
|a × b| = |a||b| sin θ,
the magnitude of a × b is the area of the parallelogram deﬁned by the two vectors. (See Figure 2.9.) The area of the
triangle deﬁned by two vectors is then 1
2|a × b|.
b
sin
b
b
a
θ
a
Figure 2.9: The Parallelogram and the Triangle Deﬁned by Two Vectors
From the deﬁnition of the cross product, one can derive the following properties:
• a × b = −b × a, anti-commutative.
• α(a × b) = (αa) × b = a × (αb), associativity of scalar multiplication.
• a × (b + c) = a × b + a × c, distributive.
• (a × b) × c ̸= a × (b × c). The cross product is not associative.
• i × i = j × j = k × k = 0.
31

• i × j = k, j × k = i, k × i = j.
•
a × b = (a2b3 −a3b2)i + (a3b1 −a1b3)j + (a1b2 −a2b1)k =

i
j
k
a1
a2
a3
b1
b2
b3

,
cross product in terms of rectangular components.
• If a · b = 0 then either a and b are parallel or one of a or b is zero.
Scalar Triple Product.
Consider the volume of the parallelopiped deﬁned by three vectors. (See Figure 2.10.)
The area of the base is ||b||c| sin θ|, where θ is the angle between b and c. The height is |a| cos φ, where φ is the angle
between b × c and a. Thus the volume of the parallelopiped is |a||b||c| sin θ cos φ.
φ
θ
b  c
a
b
c
Figure 2.10: The Parallelopiped Deﬁned by Three Vectors
Note that
|a · (b × c)| = |a · (|b||c| sin θ n)|
= ||a||b||c| sin θ cos φ| .
32

Thus |a · (b × c)| is the volume of the parallelopiped. a·(b×c) is the volume or the negative of the volume depending
on whether {a, b, c} is a right or left-handed system.
Note that parentheses are unnecessary in a · b × c. There is only one way to interpret the expression. If you did the
dot product ﬁrst then you would be left with the cross product of a scalar and a vector which is meaningless. a · b × c
is called the scalar triple product.
Plane Deﬁned by Three Points.
Three points which are not collinear deﬁne a plane. Consider a plane that
passes through the three points a, b and c. One way of expressing that the point x lies in the plane is that the vectors
x −a, b −a and c −a are coplanar. (See Figure 2.11.) If the vectors are coplanar, then the parallelopiped deﬁned by
these three vectors will have zero volume. We can express this in an equation using the scalar triple product,
(x −a) · (b −a) × (c −a) = 0.
b
c
x
a
Figure 2.11: Three Points Deﬁne a Plane.
2.2
Sets of Vectors in n Dimensions
Orthogonality.
Consider two n-dimensional vectors
x = (x1, x2, . . . , xn),
y = (y1, y2, . . . , yn).
33

The inner product of these vectors can be deﬁned
⟨x|y⟩≡x · y =
n
X
i=1
xiyi.
The vectors are orthogonal if x · y = 0. The norm of a vector is the length of the vector generalized to n dimensions.
∥x∥= √x · x
Consider a set of vectors
{x1, x2, . . . , xm}.
If each pair of vectors in the set is orthogonal, then the set is orthogonal.
xi · xj = 0
if i ̸= j
If in addition each vector in the set has norm 1, then the set is orthonormal.
xi · xj = δij =
(
1
if i = j
0
if i ̸= j
Here δij is known as the Kronecker delta function.
Completeness.
A set of n, n-dimensional vectors
{x1, x2, . . . , xn}
is complete if any n-dimensional vector can be written as a linear combination of the vectors in the set. That is, any
vector y can be written
y =
n
X
i=1
cixi.
34

Taking the inner product of each side of this equation with xm,
y · xm =
 n
X
i=1
cixi
!
· xm
=
n
X
i=1
cixi · xm
= cmxm · xm
cm = y · xm
∥xm∥2
Thus y has the expansion
y =
n
X
i=1
y · xi
∥xi∥2xi.
If in addition the set is orthonormal, then
y =
n
X
i=1
(y · xi)xi.
35

2.3
Exercises
The Dot and Cross Product
Exercise 2.1
Prove the distributive law for the dot product,
a · (b + c) = a · b + a · c.
Exercise 2.2
Prove that
a · b = aibi ≡a1b1 + · · · + anbn.
Exercise 2.3
What is the angle between the vectors i + j and i + 3j?
Exercise 2.4
Prove the distributive law for the cross product,
a × (b + c) = a × b + a × b.
Exercise 2.5
Show that
a × b =

i
j
k
a1
a2
a3
b1
b2
b3

Exercise 2.6
What is the area of the quadrilateral with vertices at (1, 1), (4, 2), (3, 7) and (2, 3)?
Exercise 2.7
What is the volume of the tetrahedron with vertices at (1, 1, 0), (3, 2, 1), (2, 4, 1) and (1, 2, 5)?
36

Exercise 2.8
What is the equation of the plane that passes through the points (1, 2, 3), (2, 3, 1) and (3, 1, 2)? What is the distance
from the point (2, 3, 5) to the plane?
37

2.4
Hints
The Dot and Cross Product
Hint 2.1
First prove the distributive law when the ﬁrst vector is of unit length,
n · (b + c) = n · b + n · c.
Then all the quantities in the equation are projections onto the unit vector n and you can use geometry.
Hint 2.2
First prove that the dot product of a rectangular unit vector with itself is one and the dot product of two distinct
rectangular unit vectors is zero. Then write a and b in rectangular components and use the distributive law.
Hint 2.3
Use a · b = |a||b| cos θ.
Hint 2.4
First consider the case that both b and c are orthogonal to a. Prove the distributive law in this case from geometric
considerations.
Next consider two arbitrary vectors a and b. We can write b = b⊥+ b∥where b⊥is orthogonal to a and b∥is
parallel to a. Show that
a × b = a × b⊥.
Finally prove the distributive law for arbitrary b and c.
Hint 2.5
Write the vectors in their rectangular components and use,
i × j = k,
j × k = i,
k × i = j,
and,
i × i = j × j = k × k = 0.
38

Hint 2.6
The quadrilateral is composed of two triangles. The area of a triangle deﬁned by the two vectors a and b is 1
2|a · b|.
Hint 2.7
Justify that the area of a tetrahedron determined by three vectors is one sixth the area of the parallelogram determined
by those three vectors. The area of a parallelogram determined by three vectors is the magnitude of the scalar triple
product of the vectors: a · b × c.
Hint 2.8
The equation of a line that is orthogonal to a and passes through the point b is a · x = a · b. The distance of a point
c from the plane is
(c −b) · a
|a|

39

2.5
Solutions
The Dot and Cross Product
Solution 2.1
First we prove the distributive law when the ﬁrst vector is of unit length, i.e.,
n · (b + c) = n · b + n · c.
(2.1)
From Figure 2.12 we see that the projection of the vector b + c onto n is equal to the sum of the projections b · n and
c · n.
b
c
n b
n c
b+c
n
n (b+c)
Figure 2.12: The Distributive Law for the Dot Product
Now we extend the result to the case when the ﬁrst vector has arbitrary length. We deﬁne a = |a|n and multiply
Equation 2.1 by the scalar, |a|.
|a|n · (b + c) = |a|n · b + |a|n · c
a · (b + c) = a · b + a · c.
Solution 2.2
First note that
ei · ei = |ei||ei| cos(0) = 1.
40

Then note that that dot product of any two distinct rectangular unit vectors is zero because they are orthogonal. Now
we write a and b in terms of their rectangular components and use the distributive law.
a · b = aiei · bjej
= aibjei · ej
= aibjδij
= aibi
Solution 2.3
Since a · b = |a||b| cos θ, we have
θ = arccos
 a · b
|a||b|

when a and b are nonzero.
θ = arccos
(i + j) · (i + 3j)
|i + j||i + 3j|

= arccos

4
√
2
√
10

= arccos
 
2
√
5
5
!
≈0.463648
Solution 2.4
First consider the case that both b and c are orthogonal to a. b + c is the diagonal of the parallelogram deﬁned by
b and c, (see Figure 2.13). Since a is orthogonal to each of these vectors, taking the cross product of a with these
vectors has the eﬀect of rotating the vectors through π/2 radians about a and multiplying their length by |a|. Note
that a × (b + c) is the diagonal of the parallelogram deﬁned by a × b and a × c. Thus we see that the distributive law
holds when a is orthogonal to both b and c,
a × (b + c) = a × b + a × c.
Now consider two arbitrary vectors a and b. We can write b = b⊥+ b∥where b⊥is orthogonal to a and b∥is
parallel to a, (see Figure 2.14).
By the deﬁnition of the cross product,
a × b = |a||b| sin θ n.
41

b
c
b+c
a  c
a
a  b
a  (b+c)
Figure 2.13: The Distributive Law for the Cross Product
a
b
b
θ
b
Figure 2.14: The Vector b Written as a Sum of Components Orthogonal and Parallel to a
Note that
|b⊥| = |b| sin θ,
42

and that a × b⊥is a vector in the same direction as a × b. Thus we see that
a × b = |a||b| sin θ n
= |a|(sin θ|b|)n
= |a||b⊥|n
= |a||b⊥| sin(π/2)n
a × b = a × b⊥.
Now we are prepared to prove the distributive law for arbitrary b and c.
a × (b + c) = a × (b⊥+ b∥+ c⊥+ c∥)
= a × ((b + c)⊥+ (b + c)∥)
= a × ((b + c)⊥)
= a × b⊥+ a × c⊥
= a × b + a × c
a × (b + c) = a × b + a × c
Solution 2.5
We know that
i × j = k,
j × k = i,
k × i = j,
and that
i × i = j × j = k × k = 0.
Now we write a and b in terms of their rectangular components and use the distributive law to expand the cross
product.
a × b = (a1i + a2j + a3k) × (b1i + b2j + b3k)
= a1i × (b1i + b2j + b3k) + a2j × (b1i + b2j + b3k) + a3k × (b1i + b2j + b3k)
= a1b2k + a1b3(−j) + a2b1(−k) + a2b3i + a3b1j + a3b2(−i)
= (a2b3 −a3b2)i −(a1b3 −a3b1)j + (a1b2 −a2b1)k
43

Next we evaluate the determinant.

i
j
k
a1
a2
a3
b1
b2
b3

= i

a2
a3
b2
b3
 −j

a1
a3
b1
b3
 + k

a1
a2
b1
b2

= (a2b3 −a3b2)i −(a1b3 −a3b1)j + (a1b2 −a2b1)k
Thus we see that,
a × b =

i
j
k
a1
a2
a3
b1
b2
b3

Solution 2.6
The area area of the quadrilateral is the area of two triangles. The ﬁrst triangle is deﬁned by the vector from (1, 1) to
(4, 2) and the vector from (1, 1) to (2, 3). The second triangle is deﬁned by the vector from (3, 7) to (4, 2) and the
vector from (3, 7) to (2, 3). (See Figure 2.15.) The area of a triangle deﬁned by the two vectors a and b is 1
2|a · b|.
The area of the quadrilateral is then,
1
2|(3i + j) · (i + 2j)| + 1
2|(i −5j) · (−i −4j)| = 1
2(5) + 1
2(19) = 12.
Solution 2.7
The tetrahedron is determined by the three vectors with tail at (1, 1, 0) and heads at (3, 2, 1), (2, 4, 1) and (1, 2, 5).
These are ⟨2, 1, 1⟩, ⟨1, 3, 1⟩and ⟨0, 1, 5⟩.
The area of the tetrahedron is one sixth the area of the parallelogram
determined by these vectors. (This is because the area of a pyramid is 1
3(base)(height). The base of the tetrahedron is
half the area of the parallelogram and the heights are the same.
1
2
1
3 = 1
6 ) Thus the area of a tetrahedron determined
by three vectors is 1
6|a · b × c|. The area of the tetrahedron is
1
6 |⟨2, 1, 1⟩· ⟨1, 3, 1⟩× ⟨1, 2, 5⟩| = 1
6 |⟨2, 1, 1⟩· ⟨13, −4, −1⟩| = 7
2
44

x
y
(3,7)
(4,2)
(2,3)
(1,1)
Figure 2.15: Quadrilateral
Solution 2.8
The two vectors with tails at (1, 2, 3) and heads at (2, 3, 1) and (3, 1, 2) are parallel to the plane. Taking the cross
product of these two vectors gives us a vector that is orthogonal to the plane.
⟨1, 1, −2⟩× ⟨2, −1, −1⟩= ⟨−3, −3, −3⟩
We see that the plane is orthogonal to the vector ⟨1, 1, 1⟩and passes through the point (1, 2, 3). The equation of the
plane is
⟨1, 1, 1⟩· ⟨x, y, z⟩= ⟨1, 1, 1⟩· ⟨1, 2, 3⟩,
x + y + z = 6.
Consider the vector with tail at (1, 2, 3) and head at (2, 3, 5). The magnitude of the dot product of this vector with
the unit normal vector gives the distance from the plane.
⟨1, 1, 2⟩· ⟨1, 1, 1⟩
|⟨1, 1, 1⟩|
 = 4
√
3 = 4
√
3
3
45

Part II
Calculus
46

Chapter 3
Diﬀerential Calculus
3.1
Limits of Functions
Deﬁnition of a Limit.
If the value of the function y(x) gets arbitrarily close to ψ as x approaches the point ξ,
then we say that the limit of the function as x approaches ξ is equal to ψ. This is written:
lim
x→ξ y(x) = ψ
To make the notion of “arbitrarily close” precise: for any ϵ > 0 there exists a δ > 0 such that |y(x) −ψ| < ϵ for all
0 < |x −ξ| < δ. That is, there is an interval surrounding the point x = ξ for which the function is within ϵ of ψ. See
Figure 3.1. Note that the interval surrounding x = ξ is a deleted neighborhood, that is it does not contain the point
x = ξ. Thus the value function at x = ξ need not be equal to ψ for the limit to exist. Indeed the function need not
even be deﬁned at x = ξ.
To prove that a function has a limit at a point ξ we ﬁrst bound |y(x) −ψ| in terms of δ for values of x satisfying
0 < |x −ξ| < δ. Denote this upper bound by u(δ). Then for an arbitrary ϵ > 0, we determine a δ > 0 such that the
the upper bound u(δ) and hence |y(x) −ψ| is less than ϵ.
47

x
y
η+ε
η−ε
ξ−δ
ξ+δ
Figure 3.1: The δ neighborhood of x = ξ such that |y(x) −ψ| < ϵ.
Example 3.1.1 Show that
lim
x→1 x2 = 1.
Consider any ϵ > 0. We need to show that there exists a δ > 0 such that |x2 −1| < ϵ for all |x −1| < δ. First we
obtain a bound on |x2 −1|.
|x2 −1| = |(x −1)(x + 1)|
= |x −1||x + 1|
< δ|x + 1|
= δ|(x −1) + 2|
< δ(δ + 2)
Now we choose a positive δ such that,
δ(δ + 2) = ϵ.
We see that
δ =
√
1 + ϵ −1,
is positive and satisﬁes the criterion that |x2 −1| < ϵ for all 0 < |x −1| < δ. Thus the limit exists.
48

Note that the value of the function y(ξ) need not be equal to limx→ξ y(x). This is illustrated in Example 3.1.2.
Example 3.1.2 Consider the function
y(x) =
(
1
for x ∈Z,
0
for x ̸∈Z.
For what values of ξ does limx→ξ y(x) exist?
First consider ξ ̸∈Z. Then there exists an open neighborhood a < ξ < b around ξ such that y(x) is identically zero
for x ∈(a, b). Then trivially, limx→ξ y(x) = 0.
Now consider ξ ∈Z. Consider any ϵ > 0. Then if |x −ξ| < 1 then |y(x) −0| = 0 < ϵ. Thus we see that
limx→ξ y(x) = 0.
Thus, regardless of the value of ξ, limx→ξ y(x) = 0.
Left and Right Limits.
With the notation limx→ξ+ y(x) we denote the right limit of y(x). This is the limit as x
approaches ξ from above. Mathematically: limx→ξ+ exists if for any ϵ > 0 there exists a δ > 0 such that |y(x)−ψ| < ϵ
for all 0 < ξ −x < δ. The left limit limx→ξ−y(x) is deﬁned analogously.
Example 3.1.3 Consider the function, sin x
|x| , deﬁned for x ̸= 0. (See Figure 3.2.) The left and right limits exist as x
approaches zero.
lim
x→0+
sin x
|x| = 1,
lim
x→0−
sin x
|x| = −1
However the limit,
lim
x→0
sin x
|x| ,
does not exist.
Properties of Limits.
Let limx→ξ u(x) and limx→ξ v(x) exist.
• limx→ξ (au(x) + bv(x)) = a limx→ξ u(x) + b limx→ξ v(x).
49

Figure 3.2: Plot of sin(x)/|x|.
• limx→ξ (u(x)v(x)) = (limx→ξ u(x)) (limx→ξ v(x)).
• limx→ξ

u(x)
v(x)

= limx→ξ u(x)
limx→ξ v(x) if limx→ξ v(x) ̸= 0.
Example 3.1.4 Prove that if limx→ξ u(x) = µ and limx→ξ v(x) = ν exist then
lim
x→ξ (u(x)v(x)) =

lim
x→ξ u(x)
 
lim
x→ξ v(x)

.
Assume that µ and ν are nonzero. (The cases where one or both are zero are similar and simpler.)
|u(x)v(x) −µν| = |uv −(u + µ −u)ν|
= |u(v −ν) + (u −µ)ν|
= |u||v −ν| + |u −µ||ν|
A suﬃcient condition for |u(x)v(x) −µν| < ϵ is
|u −µ| <
ϵ
2|ν|
and
|v −ν| <
ϵ
2

|µ| +
ϵ
2|ν|
.
Since the two right sides of the inequalities are positive, there exists δ1 > 0 and δ2 > 0 such that the ﬁrst inequality is
satisﬁed for all |x −ξ| < δ1 and the second inequality is satisﬁed for all |x −ξ| < δ2. By choosing δ to be the smaller
50

of δ1 and δ2 we see that
|u(x)v(x) −µν| < ϵ for all |x −ξ| < δ.
Thus
lim
x→ξ (u(x)v(x)) =

lim
x→ξ u(x)
 
lim
x→ξ v(x)

= µν.
Result 3.1.1 Deﬁnition of a Limit. The statement:
lim
x→ξ y(x) = ψ
means that y(x) gets arbitrarily close to ψ as x approaches ξ. For any ϵ > 0 there exists a
δ > 0 such that |y(x) −ψ| < ϵ for all x in the neighborhood 0 < |x −ξ| < δ. The left and
right limits,
lim
x→ξ−y(x) = ψ
and
lim
x→ξ+ y(x) = ψ
denote the limiting value as x approaches ξ respectively from below and above. The neigh-
borhoods are respectively −δ < x −ξ < 0 and 0 < x −ξ < δ.
Properties of Limits. Let limx→ξ u(x) and limx→ξ v(x) exist.
• limx→ξ (au(x) + bv(x)) = a limx→ξ u(x) + b limx→ξ v(x).
• limx→ξ (u(x)v(x)) = (limx→ξ u(x)) (limx→ξ v(x)).
• limx→ξ

u(x)
v(x)

= limx→ξ u(x)
limx→ξ v(x) if limx→ξ v(x) ̸= 0.
51

3.2
Continuous Functions
Deﬁnition of Continuity.
A function y(x) is said to be continuous at x = ξ if the value of the function is
equal to its limit, that is, limx→ξ y(x) = y(ξ). Note that this one condition is actually the three conditions: y(ξ) is
deﬁned, limx→ξ y(x) exists and limx→ξ y(x) = y(ξ). A function is continuous if it is continuous at each point in its
domain. A function is continuous on the closed interval [a, b] if the function is continuous for each point x ∈(a, b) and
limx→a+ y(x) = y(a) and limx→b−y(x) = y(b).
Discontinuous Functions.
If a function is not continuous at a point it is called discontinuous at that point. If
limx→ξ y(x) exists but is not equal to y(ξ), then the function has a removable discontinuity. It is thus named because
we could deﬁne a continuous function
z(x) =
(
y(x)
for x ̸= ξ,
limx→ξ y(x)
for x = ξ,
to remove the discontinuity. If both the left and right limit of a function at a point exist, but are not equal, then the
function has a jump discontinuity at that point. If either the left or right limit of a function does not exist, then the
function is said to have an inﬁnite discontinuity at that point.
Example 3.2.1
sin x
x
has a removable discontinuity at x = 0. The Heaviside function,
H(x) =





0
for x < 0,
1/2
for x = 0,
1
for x > 0,
has a jump discontinuity at x = 0.
1
x has an inﬁnite discontinuity at x = 0. See Figure 3.3.
Properties of Continuous Functions.
52

Figure 3.3: A Removable discontinuity, a Jump Discontinuity and an Inﬁnite Discontinuity
Arithmetic. If u(x) and v(x) are continuous at x = ξ then u(x) ± v(x) and u(x)v(x) are continuous at x = ξ.
u(x)
v(x)
is continuous at x = ξ if v(ξ) ̸= 0.
Function Composition. If u(x) is continuous at x = ξ and v(x) is continuous at x = µ = u(ξ) then u(v(x)) is
continuous at x = ξ. The composition of continuous functions is a continuous function.
Boundedness. A function which is continuous on a closed interval is bounded in that closed interval.
Nonzero in a Neighborhood. If y(ξ) ̸= 0 then there exists a neighborhood (ξ −ϵ, ξ + ϵ), ϵ > 0 of the point ξ such
that y(x) ̸= 0 for x ∈(ξ −ϵ, ξ + ϵ).
Intermediate Value Theorem. Let u(x) be continuous on [a, b]. If u(a) ≤µ ≤u(b) then there exists ξ ∈[a, b] such
that u(ξ) = µ. This is known as the intermediate value theorem. A corollary of this is that if u(a) and u(b) are
of opposite sign then u(x) has at least one zero on the interval (a, b).
Maxima and Minima. If u(x) is continuous on [a, b] then u(x) has a maximum and a minimum on [a, b]. That is, there
is at least one point ξ ∈[a, b] such that u(ξ) ≥u(x) for all x ∈[a, b] and there is at least one point ψ ∈[a, b]
such that u(ψ) ≤u(x) for all x ∈[a, b].
Piecewise Continuous Functions.
A function is piecewise continuous on an interval if the function is bounded on
the interval and the interval can be divided into a ﬁnite number of intervals on each of which the function is continuous.
53

For example, the greatest integer function, ⌊x⌋, is piecewise continuous. (⌊x⌋is deﬁned to the the greatest integer less
than or equal to x.) See Figure 3.4 for graphs of two piecewise continuous functions.
Figure 3.4: Piecewise Continuous Functions
Uniform Continuity.
Consider a function f(x) that is continuous on an interval. This means that for any point ξ
in the interval and any positive ϵ there exists a δ > 0 such that |f(x) −f(ξ)| < ϵ for all 0 < |x −ξ| < δ. In general,
this value of δ depends on both ξ and ϵ. If δ can be chosen so it is a function of ϵ alone and independent of ξ then
the function is said to be uniformly continuous on the interval. A suﬃcient condition for uniform continuity is that the
function is continuous on a closed interval.
3.3
The Derivative
Consider a function y(x) on the interval (x . . . x + ∆x) for some ∆x > 0. We deﬁne the increment ∆y = y(x + ∆x) −
y(x). The average rate of change, (average velocity), of the function on the interval is ∆y
∆x. The average rate of change
is the slope of the secant line that passes through the points (x, y(x)) and (x + ∆x, y(x + ∆x)). See Figure 3.5.
If the slope of the secant line has a limit as ∆x approaches zero then we call this slope the derivative or instantaneous
rate of change of the function at the point x. We denote the derivative by dy
dx, which is a nice notation as the derivative
is the limit of ∆y
∆x as ∆x →0.
dy
dx ≡lim
∆x→0
y(x + ∆x) −y(x)
∆x
.
54

y
x
∆y
∆x
Figure 3.5: The increments ∆x and ∆y.
∆x may approach zero from below or above. It is common to denote the derivative dy
dx by
d
dxy, y′(x), y′ or Dy.
A function is said to be diﬀerentiable at a point if the derivative exists there. Note that diﬀerentiability implies
continuity, but not vice versa.
Example 3.3.1 Consider the derivative of y(x) = x2 at the point x = 1.
y′(1) ≡lim
∆x→0
y(1 + ∆x) −y(1)
∆x
= lim
∆x→0
(1 + ∆x)2 −1
∆x
= lim
∆x→0(2 + ∆x)
= 2
Figure 3.6 shows the secant lines approaching the tangent line as ∆x approaches zero from above and below.
55

0.5
1
1.5
2
0.5
1
1.5
2
2.5
3
3.5
4
0.5
1
1.5
2
0.5
1
1.5
2
2.5
3
3.5
4
Figure 3.6: Secant lines and the tangent to x2 at x = 1.
Example 3.3.2 We can compute the derivative of y(x) = x2 at an arbitrary point x.
d
dx

x2
= lim
∆x→0
(x + ∆x)2 −x2
∆x
= lim
∆x→0(2x + ∆x)
= 2x
56

Properties.
Let u(x) and v(x) be diﬀerentiable.
Let a and b be constants.
Some fundamental properties of
derivatives are:
d
dx(au + bv) = adu
dx + bdv
dx
Linearity
d
dx(uv) = du
dxv + udv
dx
Product Rule
d
dx
u
v

= v du
dx −u dv
dx
v2
Quotient Rule
d
dx(ua) = aua−1du
dx
Power Rule
d
dx(u(v(x))) = du
dv
dv
dx = u′(v(x))v′(x)
Chain Rule
These can be proved by using the deﬁnition of diﬀerentiation.
Example 3.3.3 Prove the quotient rule for derivatives.
d
dx
u
v

= lim
∆x→0
u(x+∆x)
v(x+∆x) −u(x)
v(x)
∆x
= lim
∆x→0
u(x + ∆x)v(x) −u(x)v(x + ∆x)
∆xv(x)v(x + ∆x)
= lim
∆x→0
u(x + ∆x)v(x) −u(x)v(x) −u(x)v(x + ∆x) + u(x)v(x)
∆xv(x)v(x)
= lim
∆x→0
(u(x + ∆x) −u(x))v(x) −u(x)(v(x + ∆x) −v(x))
∆xv2(x)
= lim∆x→0
u(x+∆x)−u(x)
∆x
v(x) −u(x) lim∆x→0
v(x+∆x)−v(x)
∆x
v2(x)
= v du
dx −u dv
dx
v2
57

Trigonometric Functions.
Some derivatives of trigonometric functions are:
d
dx sin x = cos x
d
dx arcsin x =
1
(1 −x2)1/2
d
dx cos x = −sin x
d
dx arccos x = −
1
(1 −x2)1/2
d
dx tan x =
1
cos2 x
d
dx arctan x =
1
1 + x2
d
dx ex = ex
d
dx ln x = 1
x
d
dx sinh x = cosh x
d
dx arcsinh x =
1
(x2 + 1)1/2
d
dx cosh x = sinh x
d
dx arccosh x =
1
(x2 −1)1/2
d
dx tanh x =
1
cosh2 x
d
dx arctanh x =
1
1 −x2
Example 3.3.4 We can evaluate the derivative of xx by using the identity ab = eb ln a.
d
dxxx = d
dx ex ln x
= ex ln x d
dx(x ln x)
= xx(1 · ln x + x1
x)
= xx(1 + ln x)
58

Inverse Functions.
If we have a function y(x), we can consider x as a function of y, x(y).
For example, if
y(x) = 8x3 then x(y) = 2 3√y; if y(x) = x+2
x+1 then x(y) = 2−y
y−1. The derivative of an inverse function is
d
dyx(y) = 1
dy
dx
.
Example 3.3.5 The inverse function of y(x) = ex is x(y) = ln y. We can obtain the derivative of the logarithm from
the derivative of the exponential. The derivative of the exponential is
dy
dx = ex .
Thus the derivative of the logarithm is
d
dy ln y = d
dyx(y) = 1
dy
dx
= 1
ex = 1
y.
3.4
Implicit Diﬀerentiation
An explicitly deﬁned function has the form y = f(x). A implicitly deﬁned function has the form f(x, y) = 0. A few
examples of implicit functions are x2 + y2 −1 = 0 and x + y + sin(xy) = 0. Often it is not possible to write an implicit
equation in explicit form. This is true of the latter example above. One can calculate the derivative of y(x) in terms
of x and y even when y(x) is deﬁned by an implicit equation.
Example 3.4.1 Consider the implicit equation
x2 −xy −y2 = 1.
This implicit equation can be solved for the dependent variable.
y(x) = 1
2

−x ±
√
5x2 −4

.
59

We can diﬀerentiate this expression to obtain
y′ = 1
2

−1 ±
5x
√
5x2 −4

.
One can obtain the same result without ﬁrst solving for y. If we diﬀerentiate the implicit equation, we obtain
2x −y −xdy
dx −2y dy
dx = 0.
We can solve this equation for dy
dx.
dy
dx = 2x −y
x + 2y
We can diﬀerentiate this expression to obtain the second derivative of y.
d2y
dx2 = (x + 2y)(2 −y′) −(2x −y)(1 + 2y′)
(x + 2y)2
= 5(y −xy′)
(x + 2y)2
Substitute in the expression for y′.
= −10(x2 −xy −y2)
(x + 2y)2
Use the original implicit equation.
= −
10
(x + 2y)2
60

3.5
Maxima and Minima
A diﬀerentiable function is increasing where f ′(x) > 0, decreasing where f ′(x) < 0 and stationary where f ′(x) = 0.
A function f(x) has a relative maxima at a point x = ξ if there exists a neighborhood around ξ such that f(x) ≤f(ξ)
for x ∈(x −δ, x + δ), δ > 0. The relative minima is deﬁned analogously. Note that this deﬁnition does not require
that the function be diﬀerentiable, or even continuous. We refer to relative maxima and minima collectively are relative
extrema.
Relative Extrema and Stationary Points.
If f(x) is diﬀerentiable and f(ξ) is a relative extrema then x = ξ
is a stationary point, f ′(ξ) = 0. We can prove this using left and right limits. Assume that f(ξ) is a relative maxima.
Then there is a neighborhood (x −δ, x + δ), δ > 0 for which f(x) ≤f(ξ). Since f(x) is diﬀerentiable the derivative
at x = ξ,
f ′(ξ) = lim
∆x→0
f(ξ + ∆x) −f(ξ)
∆x
,
exists. This in turn means that the left and right limits exist and are equal. Since f(x) ≤f(ξ) for ξ −δ < x < ξ the
left limit is non-positive,
f ′(ξ) =
lim
∆x→0−
f(ξ + ∆x) −f(ξ)
∆x
≤0.
Since f(x) ≤f(ξ) for ξ < x < ξ + δ the right limit is nonnegative,
f ′(ξ) =
lim
∆x→0+
f(ξ + ∆x) −f(ξ)
∆x
≥0.
Thus we have 0 ≤f ′(ξ) ≤0 which implies that f ′(ξ) = 0.
It is not true that all stationary points are relative extrema. That is, f ′(ξ) = 0 does not imply that x = ξ is an
extrema. Consider the function f(x) = x3. x = 0 is a stationary point since f ′(x) = x2, f ′(0) = 0. However, x = 0 is
neither a relative maxima nor a relative minima.
It is also not true that all relative extrema are stationary points. Consider the function f(x) = |x|. The point x = 0
is a relative minima, but the derivative at that point is undeﬁned.
61

First Derivative Test.
Let f(x) be diﬀerentiable and f ′(ξ) = 0.
• If f ′(x) changes sign from positive to negative as we pass through x = ξ then the point is a relative maxima.
• If f ′(x) changes sign from negative to positive as we pass through x = ξ then the point is a relative minima.
• If f ′(x) is not identically zero in a neighborhood of x = ξ and it does not change sign as we pass through the
point then x = ξ is not a relative extrema.
Example 3.5.1 Consider y = x2 and the point x = 0. The function is diﬀerentiable. The derivative, y′ = 2x, vanishes
at x = 0. Since y′(x) is negative for x < 0 and positive for x > 0, the point x = 0 is a relative minima. See Figure 3.7.
Example 3.5.2 Consider y = cos x and the point x = 0. The function is diﬀerentiable. The derivative, y′ = −sin x
is positive for −π < x < 0 and negative for 0 < x < π. Since the sign of y′ goes from positive to negative, x = 0 is a
relative maxima. See Figure 3.7.
Example 3.5.3 Consider y = x3 and the point x = 0. The function is diﬀerentiable. The derivative, y′ = 3x2 is
positive for x < 0 and positive for 0 < x. Since y′ is not identically zero and the sign of y′ does not change, x = 0 is
not a relative extrema. See Figure 3.7.
Figure 3.7: Graphs of x2, cos x and x3.
62

Concavity.
If the portion of a curve in some neighborhood of a point lies above the tangent line through that point,
the curve is said to be concave upward. If it lies below the tangent it is concave downward. If a function is twice
diﬀerentiable then f ′′(x) > 0 where it is concave upward and f ′′(x) < 0 where it is concave downward. Note that
f ′′(x) > 0 is a suﬃcient, but not a necessary condition for a curve to be concave upward at a point. A curve may be
concave upward at a point where the second derivative vanishes. A point where the curve changes concavity is called
a point of inﬂection. At such a point the second derivative vanishes, f ′′(x) = 0. For twice continuously diﬀerentiable
functions, f ′′(x) = 0 is a necessary but not a suﬃcient condition for an inﬂection point. The second derivative may
vanish at places which are not inﬂection points. See Figure 3.8.
Figure 3.8: Concave Upward, Concave Downward and an Inﬂection Point.
Second Derivative Test.
Let f(x) be twice diﬀerentiable and let x = ξ be a stationary point, f ′(ξ) = 0.
• If f ′′(ξ) < 0 then the point is a relative maxima.
• If f ′′(ξ) > 0 then the point is a relative minima.
• If f ′′(ξ) = 0 then the test fails.
Example 3.5.4 Consider the function f(x) = cos x and the point x = 0.
The derivatives of the function are
f ′(x) = −sin x, f ′′(x) = −cos x. The point x = 0 is a stationary point, f ′(0) = −sin(0) = 0. Since the second
derivative is negative there, f ′′(0) = −cos(0) = −1, the point is a relative maxima.
63

Example 3.5.5 Consider the function f(x) = x4 and the point x = 0. The derivatives of the function are f ′(x) = 4x3,
f ′′(x) = 12x2. The point x = 0 is a stationary point. Since the second derivative also vanishes at that point the
second derivative test fails. One must use the ﬁrst derivative test to determine that x = 0 is a relative minima.
3.6
Mean Value Theorems
Rolle’s Theorem.
If f(x) is continuous in [a, b], diﬀerentiable in (a, b) and f(a) = f(b) = 0 then there exists a
point ξ ∈(a, b) such that f ′(ξ) = 0. See Figure 3.9.
Figure 3.9: Rolle’s Theorem.
To prove this we consider two cases. First we have the trivial case that f(x) ≡0. If f(x) is not identically zero
then continuity implies that it must have a nonzero relative maxima or minima in (a, b). Let x = ξ be one of these
relative extrema. Since f(x) is diﬀerentiable, x = ξ must be a stationary point, f ′(ξ) = 0.
Theorem of the Mean.
If f(x) is continuous in [a, b] and diﬀerentiable in (a, b) then there exists a point x = ξ
such that
f ′(ξ) = f(b) −f(a)
b −a
.
That is, there is a point where the instantaneous velocity is equal to the average velocity on the interval.
64

Figure 3.10: Theorem of the Mean.
We prove this theorem by applying Rolle’s theorem. Consider the new function
g(x) = f(x) −f(a) −f(b) −f(a)
b −a
(x −a)
Note that g(a) = g(b) = 0, so it satisﬁes the conditions of Rolle’s theorem. There is a point x = ξ such that g′(ξ) = 0.
We diﬀerentiate the expression for g(x) and substitute in x = ξ to obtain the result.
g′(x) = f ′(x) −f(b) −f(a)
b −a
g′(ξ) = f ′(ξ) −f(b) −f(a)
b −a
= 0
f ′(ξ) = f(b) −f(a)
b −a
Generalized Theorem of the Mean.
If f(x) and g(x) are continuous in [a, b] and diﬀerentiable in (a, b), then
there exists a point x = ξ such that
f ′(ξ)
g′(ξ) = f(b) −f(a)
g(b) −g(a) .
We have assumed that g(a) ̸= g(b) so that the denominator does not vanish and that f ′(x) and g′(x) are not
simultaneously zero which would produce an indeterminate form.
Note that this theorem reduces to the regular
theorem of the mean when g(x) = x. The proof of the theorem is similar to that for the theorem of the mean.
65

Taylor’s Theorem of the Mean.
If f(x) is n + 1 times continuously diﬀerentiable in (a, b) then there exists a
point x = ξ ∈(a, b) such that
f(b) = f(a) + (b −a)f ′(a) + (b −a)2
2!
f ′′(a) + · · · + (b −a)n
n!
f (n)(a) + (b −a)n+1
(n + 1)! f (n+1)(ξ).
(3.1)
For the case n = 0, the formula is
f(b) = f(a) + (b −a)f ′(ξ),
which is just a rearrangement of the terms in the theorem of the mean,
f ′(ξ) = f(b) −f(a)
b −a
.
3.6.1
Application: Using Taylor’s Theorem to Approximate Functions.
One can use Taylor’s theorem to approximate functions with polynomials. Consider an inﬁnitely diﬀerentiable function
f(x) and a point x = a. Substituting x for b into Equation 3.1 we obtain,
f(x) = f(a) + (x −a)f ′(a) + (x −a)2
2!
f ′′(a) + · · · + (x −a)n
n!
f (n)(a) + (x −a)n+1
(n + 1)! f (n+1)(ξ).
If the last term in the sum is small then we can approximate our function with an nth order polynomial.
f(x) ≈f(a) + (x −a)f ′(a) + (x −a)2
2!
f ′′(a) + · · · + (x −a)n
n!
f (n)(a)
The last term in Equation 3.6.1 is called the remainder or the error term,
Rn = (x −a)n+1
(n + 1)! f (n+1)(ξ).
66

Since the function is inﬁnitely diﬀerentiable, f (n+1)(ξ) exists and is bounded. Therefore we note that the error must
vanish as x →0 because of the (x −a)n+1 factor. We therefore suspect that our approximation would be a good one
if x is close to a. Also note that n! eventually grows faster than (x −a)n,
lim
n→∞
(x −a)n
n!
= 0.
So if the derivative term, f (n+1)(ξ), does not grow to quickly, the error for a certain value of x will get smaller with
increasing n and the polynomial will become a better approximation of the function. (It is also possible that the
derivative factor grows very quickly and the approximation gets worse with increasing n.)
Example 3.6.1 Consider the function f(x) = ex. We want a polynomial approximation of this function near the point
x = 0. Since the derivative of ex is ex, the value of all the derivatives at x = 0 is f (n)(0) = e0 = 1. Taylor’s theorem
thus states that
ex = 1 + x + x2
2! + x3
3! + · · · + xn
n! +
xn+1
(n + 1)! eξ,
for some ξ ∈(0, x). The ﬁrst few polynomial approximations of the exponent about the point x = 0 are
f1(x) = 1
f2(x) = 1 + x
f3(x) = 1 + x + x2
2
f4(x) = 1 + x + x2
2 + x3
6
The four approximations are graphed in Figure 3.11.
Note that for the range of x we are looking at, the approximations become more accurate as the number of terms
increases.
67

-1 -0.5
0.5
1
0.5
1
1.5
2
2.5
-1 -0.5
0.5
1
0.5
1
1.5
2
2.5
-1 -0.5
0.5
1
0.5
1
1.5
2
2.5
-1 -0.5
0.5
1
0.5
1
1.5
2
2.5
Figure 3.11: Four Finite Taylor Series Approximations of ex
Example 3.6.2 Consider the function f(x) = cos x. We want a polynomial approximation of this function near the
point x = 0. The ﬁrst few derivatives of f are
f(x) = cos x
f ′(x) = −sin x
f ′′(x) = −cos x
f ′′′(x) = sin x
f (4)(x) = cos x
It’s easy to pick out the pattern here,
f (n)(x) =
(
(−1)n/2 cos x
for even n,
(−1)(n+1)/2 sin x
for odd n.
Since cos(0) = 1 and sin(0) = 0 the n-term approximation of the cosine is,
cos x = 1 −x2
2! + x4
4! −x6
6! + · · · + (−1)2(n−1)
x2(n−1)
(2(n −1))! + x2n
(2n)! cos ξ.
Here are graphs of the one, two, three and four term approximations.
68

-3 -2 -1
1
2
3
-1
-0.5
0.5
1
-3 -2 -1
1
2
3
-1
-0.5
0.5
1
-3 -2 -1
1
2
3
-1
-0.5
0.5
1
-3 -2 -1
1
2
3
-1
-0.5
0.5
1
Figure 3.12: Taylor Series Approximations of cos x
Note that for the range of x we are looking at, the approximations become more accurate as the number of terms
increases. Consider the ten term approximation of the cosine about x = 0,
cos x = 1 −x2
2! + x4
4! −· · · −x18
18! + x20
20! cos ξ.
Note that for any value of ξ, | cos ξ| ≤1. Therefore the absolute value of the error term satisﬁes,
|R| =

x20
20! cos ξ
 ≤|x|20
20! .
x20/20! is plotted in Figure 3.13.
Note that the error is very small for x < 6, fairly small but non-negligible for x ≈7 and large for x > 8. The ten
term approximation of the cosine, plotted below, behaves just we would predict.
The error is very small until it becomes non-negligible at x ≈7 and large at x ≈8.
Example 3.6.3 Consider the function f(x) = ln x. We want a polynomial approximation of this function near the
69

2
4
6
8
10
0.2
0.4
0.6
0.8
1
Figure 3.13: Plot of x20/20!.
-10
-5
5
10
-2
-1.5
-1
-0.5
0.5
1
Figure 3.14: Ten Term Taylor Series Approximation of cos x
point x = 1. The ﬁrst few derivatives of f are
f(x) = ln x
f ′(x) = 1
x
f ′′(x) = −1
x2
f ′′′(x) = 2
x3
f (4)(x) = −3
x4
70

The derivatives evaluated at x = 1 are
f(0) = 0,
f (n)(0) = (−1)n−1(n −1)!, for n ≥1.
By Taylor’s theorem of the mean we have,
ln x = (x −1) −(x −1)2
2
+ (x −1)3
3
−(x −1)4
4
+ · · · + (−1)n−1(x −1)n
n
+ (−1)n(x −1)n+1
n + 1
1
ξn+1.
Below are plots of the 2, 4, 10 and 50 term approximations.
0.5 1 1.5 2 2.5 3
-6
-5
-4
-3
-2
-1
12
0.5 1 1.5 2 2.5 3
-6
-5
-4
-3
-2
-1
12
0.5 1 1.5 2 2.5 3
-6
-5
-4
-3
-2
-1
12
0.5 1 1.5 2 2.5
-6
-5
-4
-3
-2
-1
12
Figure 3.15: The 2, 4, 10 and 50 Term Approximations of ln x
Note that the approximation gets better on the interval (0, 2) and worse outside this interval as the number of terms
increases. The Taylor series converges to ln x only on this interval.
3.6.2
Application: Finite Diﬀerence Schemes
Example 3.6.4 Suppose you sample a function at the discrete points n∆x, n ∈Z. In Figure 3.16 we sample the
function f(x) = sin x on the interval [−4, 4] with ∆x = 1/4 and plot the data points.
We wish to approximate the derivative of the function on the grid points using only the value of the function on
those discrete points. From the deﬁnition of the derivative, one is lead to the formula
f ′(x) ≈f(x + ∆x) −f(x)
∆x
.
(3.2)
71

-4
-2
2
4
-1
-0.5
0.5
1
Figure 3.16: Sampling of sin x
Taylor’s theorem states that
f(x + ∆x) = f(x) + ∆xf ′(x) + ∆x2
2 f ′′(ξ).
Substituting this expression into our formula for approximating the derivative we obtain
f(x + ∆x) −f(x)
∆x
= f(x) + ∆xf ′(x) + ∆x2
2 f ′′(ξ) −f(x)
∆x
= f ′(x) + ∆x
2 f ′′(ξ).
Thus we see that the error in our approximation of the ﬁrst derivative is ∆x
2 f ′′(ξ). Since the error has a linear factor
of ∆x, we call this a ﬁrst order accurate method. Equation 3.2 is called the forward diﬀerence scheme for calculating
the ﬁrst derivative. Figure 3.17 shows a plot of the value of this scheme for the function f(x) = sin x and ∆x = 1/4.
The ﬁrst derivative of the function f ′(x) = cos x is shown for comparison.
Another scheme for approximating the ﬁrst derivative is the centered diﬀerence scheme,
f ′(x) ≈f(x + ∆x) −f(x −∆x)
2∆x
.
72

-4
-2
2
4
-1
-0.5
0.5
1
Figure 3.17: The Forward Diﬀerence Scheme Approximation of the Derivative
Expanding the numerator using Taylor’s theorem,
f(x + ∆x) −f(x −∆x)
2∆x
= f(x) + ∆xf ′(x) + ∆x2
2 f ′′(x) + ∆x3
6 f ′′′(ξ) −f(x) + ∆xf ′(x) −∆x2
2 f ′′(x) + ∆x3
6 f ′′′(ψ)
2∆x
= f ′(x) + ∆x2
12 (f ′′′(ξ) + f ′′′(ψ)).
The error in the approximation is quadratic in ∆x. Therefore this is a second order accurate scheme. Below is a plot
of the derivative of the function and the value of this scheme for the function f(x) = sin x and ∆x = 1/4.
Notice how the centered diﬀerence scheme gives a better approximation of the derivative than the forward diﬀerence
scheme.
3.7
L’Hospital’s Rule
Some singularities are easy to diagnose. Consider the function cos x
x
at the point x = 0. The function evaluates
to 1
0 and is thus discontinuous at that point. Since the numerator and denominator are continuous functions and the
73

-4
-2
2
4
-1
-0.5
0.5
1
Figure 3.18: Centered Diﬀerence Scheme Approximation of the Derivative
denominator vanishes while the numerator does not, the left and right limits as x →0 do not exist. Thus the function
has an inﬁnite discontinuity at the point x = 0. More generally, a function which is composed of continuous functions
and evaluates to a
0 at a point where a ̸= 0 must have an inﬁnite discontinuity there.
Other singularities require more analysis to diagnose. Consider the functions sin x
x , sin x
|x| and
sin x
1−cos x at the point x = 0.
All three functions evaluate to 0
0 at that point, but have diﬀerent kinds of singularities. The ﬁrst has a removable
discontinuity, the second has a ﬁnite discontinuity and the third has an inﬁnite discontinuity. See Figure 3.19.
Figure 3.19: The functions sin x
x , sin x
|x| and
sin x
1−cos x.
74

An expression that evaluates to 0
0, ∞
∞, 0·∞, ∞−∞, 1∞, 00 or ∞0 is called an indeterminate. A function f(x) which
is indeterminate at the point x = ξ is singular at that point. The singularity may be a removable discontinuity, a ﬁnite
discontinuity or an inﬁnite discontinuity depending on the behavior of the function around that point. If limx→ξ f(x)
exists, then the function has a removable discontinuity. If the limit does not exist, but the left and right limits do exist,
then the function has a ﬁnite discontinuity. If either the left or right limit does not exist then the function has an inﬁnite
discontinuity.
L’Hospital’s Rule.
Let f(x) and g(x) be diﬀerentiable and f(ξ) = g(ξ) = 0. Further, let g(x) be nonzero in a
deleted neighborhood of x = ξ, (g(x) ̸= 0 for x ∈0 < |x −ξ| < δ). Then
lim
x→ξ
f(x)
g(x) = lim
x→ξ
f ′(x)
g′(x).
To prove this, we note that f(ξ) = g(ξ) = 0 and apply the generalized theorem of the mean. Note that
f(x)
g(x) = f(x) −f(ξ)
g(x) −g(ξ) = f ′(ψ)
g′(ψ)
for some ψ between ξ and x. Thus
lim
x→ξ
f(x)
g(x) = lim
ψ→ξ
f ′(ψ)
g′(ψ) = lim
x→ξ
f ′(x)
g′(x)
provided that the limits exist.
L’Hospital’s Rule is also applicable when both functions tend to inﬁnity instead of zero or when the limit point, ξ,
is at inﬁnity. It is also valid for one-sided limits.
L’Hospital’s rule is directly applicable to the indeterminate forms 0
0 and ∞
∞.
Example 3.7.1 Consider the three functions sin x
x , sin x
|x| and
sin x
1−cos x at the point x = 0.
lim
x→0
sin x
x
= lim
x→0
cos x
1
= 1
75

Thus sin x
x
has a removable discontinuity at x = 0.
lim
x→0+
sin x
|x| = lim
x→0+
sin x
x
= 1
lim
x→0−
sin x
|x| = lim
x→0−
sin x
−x = −1
Thus sin x
|x| has a ﬁnite discontinuity at x = 0.
lim
x→0
sin x
1 −cos x = lim
x→0
cos x
sin x = 1
0 = ∞
Thus
sin x
1−cos x has an inﬁnite discontinuity at x = 0.
Example 3.7.2 Let a and d be nonzero.
lim
x→∞
ax2 + bx + c
dx2 + ex + f = lim
x→∞
2ax + b
2dx + e
= lim
x→∞
2a
2d
= a
d
Example 3.7.3 Consider
lim
x→0
cos x −1
x sin x .
This limit is an indeterminate of the form 0
0. Applying L’Hospital’s rule we see that limit is equal to
lim
x→0
−sin x
x cos x + sin x.
76

This limit is again an indeterminate of the form 0
0. We apply L’Hospital’s rule again.
lim
x→0
−cos x
−x sin x + 2 cos x = −1
2
Thus the value of the original limit is −1
2. We could also obtain this result by expanding the functions in Taylor series.
lim
x→0
cos x −1
x sin x
= lim
x→0

1 −x2
2 + x4
24 −· · ·

−1
x
 x −x3
6 + x5
120 −· · ·

= lim
x→0
−x2
2 + x4
24 −· · ·
x2 −x4
6 + x6
120 −· · ·
= lim
x→0
−1
2 + x2
24 −· · ·
1 −x2
6 + x4
120 −· · ·
= −1
2
We can apply L’Hospital’s Rule to the indeterminate forms 0 · ∞and ∞−∞by rewriting the expression in a
diﬀerent form, (perhaps putting the expression over a common denominator). If at ﬁrst you don’t succeed, try, try
again. You may have to apply L’Hospital’s rule several times to evaluate a limit.
Example 3.7.4
lim
x→0

cot x −1
x

= lim
x→0
x cos x −sin x
x sin x
= lim
x→0
cos x −x sin x −cos x
sin x + x cos x
= lim
x→0
−x sin x
sin x + x cos x
= lim
x→0
−x cos x −sin x
cos x + cos x −x sin x
= 0
77

You can apply L’Hospital’s rule to the indeterminate forms 1∞, 00 or ∞0 by taking the logarithm of the expression.
Example 3.7.5 Consider the limit,
lim
x→0 xx,
which gives us the indeterminate form 00. The logarithm of the expression is
ln(xx) = x ln x.
As x →0 we now have the indeterminate form 0 · ∞. By rewriting the expression, we can apply L’Hospital’s rule.
lim
x→0
ln x
1/x = lim
x→0
1/x
−1/x2
= lim
x→0(−x)
= 0
Thus the original limit is
lim
x→0 xx = e0 = 1.
78

3.8
Exercises
Limits and Continuity
Exercise 3.1
Does
lim
x→0 sin
1
x

exist?
Hint, Solution
Exercise 3.2
Does
lim
x→0 x sin
1
x

exist?
Hint, Solution
Exercise 3.3
Is the function sin(1/x) continuous in the open interval (0, 1)? Is there a value of a such that the function deﬁned by
f(x) =
(
sin(1/x)
for x ̸= 0,
a
for x = 0
is continuous on the closed interval [0, 1]?
Hint, Solution
Exercise 3.4
Is the function sin(1/x) uniformly continuous in the open interval (0, 1)?
Hint, Solution
79

Exercise 3.5
Are the functions √x and 1
x uniformly continuous on the interval (0, 1)?
Hint, Solution
Exercise 3.6
Prove that a function which is continuous on a closed interval is uniformly continuous on that interval.
Hint, Solution
Exercise 3.7
Prove or disprove each of the following.
1. If limn→∞an = L then limn→∞a2
n = L2.
2. If limn→∞a2
n = L2 then limn→∞an = L.
3. If an > 0 for all n > 200, and limn→∞an = L, then L > 0.
4. If f : R 7→R is continuous and limx→∞f(x) = L, then for n ∈Z, limn→∞f(n) = L.
5. If f : R 7→R is continuous and limn→∞f(n) = L, then for x ∈R, limx→∞f(x) = L.
Hint, Solution
Deﬁnition of Diﬀerentiation
Exercise 3.8 (mathematica/calculus/diﬀerential/deﬁnition.nb)
Use the deﬁnition of diﬀerentiation to prove the following identities where f(x) and g(x) are diﬀerentiable functions
and n is a positive integer.
1.
d
dx(xn) = nxn−1,
(I suggest that you use Newton’s binomial formula.)
2.
d
dx(f(x)g(x)) = f dg
dx + g df
dx
3.
d
dx(sin x) = cos x. (You’ll need to use some trig identities.)
80

4.
d
dx(f(g(x))) = f ′(g(x))g′(x)
Hint, Solution
Exercise 3.9
Use the deﬁnition of diﬀerentiation to determine if the following functions diﬀerentiable at x = 0.
1. f(x) = x|x|
2. f(x) =
p
1 + |x|
Hint, Solution
Rules of Diﬀerentiation
Exercise 3.10 (mathematica/calculus/diﬀerential/rules.nb)
Find the ﬁrst derivatives of the following:
a. x sin(cos x)
b. f(cos(g(x)))
c.
1
f(ln x)
d. xxx
e. |x| sin |x|
Hint, Solution
Exercise 3.11 (mathematica/calculus/diﬀerential/rules.nb)
Using
d
dx sin x = cos x
and
d
dx tan x =
1
cos2 x
ﬁnd the derivatives of arcsin x and arctan x.
Hint, Solution
81

Implicit Diﬀerentiation
Exercise 3.12 (mathematica/calculus/diﬀerential/implicit.nb)
Find y′(x), given that x2 + y2 = 1. What is y′(1/2)?
Hint, Solution
Exercise 3.13 (mathematica/calculus/diﬀerential/implicit.nb)
Find y′(x) and y′′(x), given that x2 −xy + y2 = 3.
Hint, Solution
Maxima and Minima
Exercise 3.14 (mathematica/calculus/diﬀerential/maxima.nb)
Identify any maxima and minima of the following functions.
a. f(x) = x(12 −2x)2.
b. f(x) = (x −2)2/3.
Hint, Solution
Exercise 3.15 (mathematica/calculus/diﬀerential/maxima.nb)
A cylindrical container with a circular base and an open top is to hold 64 cm3. Find its dimensions so that the surface
area of the cup is a minimum.
Hint, Solution
Mean Value Theorems
Exercise 3.16
Prove the generalized theorem of the mean. If f(x) and g(x) are continuous in [a, b] and diﬀerentiable in (a, b), then
there exists a point x = ξ such that
f ′(ξ)
g′(ξ) = f(b) −f(a)
g(b) −g(a) .
82

Assume that g(a) ̸= g(b) so that the denominator does not vanish and that f ′(x) and g′(x) are not simultaneously
zero which would produce an indeterminate form.
Hint, Solution
Exercise 3.17 (mathematica/calculus/diﬀerential/taylor.nb)
Find a polynomial approximation of sin x on the interval [−1, 1] that has a maximum error of
1
1000. Don’t use any more
terms that you need to. Prove the error bound. Use your polynomial to approximate sin 1.
Hint, Solution
Exercise 3.18 (mathematica/calculus/diﬀerential/taylor.nb)
You use the formula f(x+∆x)−2f(x)+f(x−∆x)
∆x2
to approximate f ′′(x). What is the error in this approximation?
Hint, Solution
Exercise 3.19
The formulas f(x+∆x)−f(x)
∆x
and f(x+∆x)−f(x−∆x)
2∆x
are ﬁrst and second order accurate schemes for approximating the ﬁrst
derivative f ′(x). Find a couple other schemes that have successively higher orders of accuracy. Would these higher
order schemes actually give a better approximation of f ′(x)? Remember that ∆x is small, but not inﬁnitesimal.
Hint, Solution
L’Hospital’s Rule
Exercise 3.20 (mathematica/calculus/diﬀerential/lhospitals.nb)
Evaluate the following limits.
a. limx→0
x−sin x
x3
b. limx→0
 csc x −1
x

c. limx→+∞
 1 + 1
x
x
d. limx→0
 csc2 x −1
x2

. (First evaluate using L’Hospital’s rule then using a Taylor series expansion. You will ﬁnd
that the latter method is more convenient.)
83

Hint, Solution
Exercise 3.21 (mathematica/calculus/diﬀerential/lhospitals.nb)
Evaluate the following limits,
lim
x→∞xa/x,
lim
x→∞

1 + a
x
bx
,
where a and b are constants.
Hint, Solution
84

3.9
Hints
Limits and Continuity
Hint 3.1
Apply the ϵ, δ deﬁnition of a limit.
Hint 3.2
Set y = 1/x. Consider limy→∞.
Hint 3.3
The composition of continuous functions is continuous. Apply the deﬁnition of continuity and look at the point x = 0.
Hint 3.4
Note that for x1 =
1
(n−1/2)π and x2 =
1
(n+1/2)π where n ∈Z we have | sin(1/x1) −sin(1/x2)| = 2.
Hint 3.5
Note that the function
√
x + δ −√x is a decreasing function of x and an increasing function of δ for positive x and
δ. Bound this function for ﬁxed δ.
Consider any positive δ and ϵ. For what values of x is
1
x −
1
x + δ > ϵ.
Hint 3.6
Let the function f(x) be continuous on a closed interval. Consider the function
e(x, δ) =
sup
|ξ−x|<δ
|f(ξ) −f(x)|.
Bound e(x, δ) with a function of δ alone.
Hint 3.7
CONTINUE
85

1. If limn→∞an = L then limn→∞a2
n = L2.
2. If limn→∞a2
n = L2 then limn→∞an = L.
3. If an > 0 for all n > 200, and limn→∞an = L, then L > 0.
4. If f : R 7→R is continuous and limx→∞f(x) = L, then for n ∈Z, limn→∞f(n) = L.
5. If f : R 7→R is continuous and limn→∞f(n) = L, then for x ∈R, limx→∞f(x) = L.
Deﬁnition of Diﬀerentiation
Hint 3.8
a. Newton’s binomial formula is
(a + b)n =
n
X
k=0
n
k

an−kbk = an + an−1b + n(n −1)
2
an−2b2 + · · · + nabn−1 + bn.
Recall that the binomial coeﬃcient is
n
k

=
n!
(n −k)!k!.
b. Note that
d
dx(f(x)g(x)) = lim
∆x→0
f(x + ∆x)g(x + ∆x) −f(x)g(x)
∆x

and
g(x)f ′(x) + f(x)g′(x) = g(x) lim
∆x→0
f(x + ∆x) −f(x)
∆x

+ f(x) lim
∆x→0
g(x + ∆x) −g(x)
∆x

.
Fill in the blank.
c. First prove that
lim
θ→0
sin θ
θ
= 1.
86

and
lim
θ→0
cos θ −1
θ

= 0.
d. Let u = g(x). Consider a nonzero increment ∆x, which induces the increments ∆u and ∆f. By deﬁnition,
∆f = f(u + ∆u) −f(u),
∆u = g(x + ∆x) −g(x),
and ∆f, ∆u →0 as ∆x →0. If ∆u ̸= 0 then we have
ϵ = ∆f
∆u −df
du →0
as
∆u →0.
If ∆u = 0 for some values of ∆x then ∆f also vanishes and we deﬁne ϵ = 0 for theses values. In either case,
∆y = df
du∆u + ϵ∆u.
Continue from here.
Hint 3.9
Rules of Diﬀerentiation
Hint 3.10
a. Use the product rule and the chain rule.
b. Use the chain rule.
c. Use the quotient rule and the chain rule.
d. Use the identity ab = eb ln a.
e. For x > 0, the expression is x sin x; for x < 0, the expression is (−x) sin(−x) = x sin x. Do both cases.
87

Hint 3.11
Use that x′(y) = 1/y′(x) and the identities cos x = (1 −sin2 x)1/2 and cos(arctan x) =
1
(1+x2)1/2.
Implicit Diﬀerentiation
Hint 3.12
Diﬀerentiating the equation
x2 + [y(x)]2 = 1
yields
2x + 2y(x)y′(x) = 0.
Solve this equation for y′(x) and write y(x) in terms of x.
Hint 3.13
Diﬀerentiate the equation and solve for y′(x) in terms of x and y(x). Diﬀerentiate the expression for y′(x) to obtain
y′′(x). You’ll use that
x2 −xy(x) + [y(x)]2 = 3
Maxima and Minima
Hint 3.14
a. Use the second derivative test.
b. The function is not diﬀerentiable at the point x = 2 so you can’t use a derivative test at that point.
Hint 3.15
Let r be the radius and h the height of the cylinder. The volume of the cup is πr2h = 64. The radius and height are
related by h =
64
πr2. The surface area of the cup is f(r) = πr2 + 2πrh = πr2 + 128
r . Use the second derivative test to
ﬁnd the minimum of f(r).
Mean Value Theorems
88

Hint 3.16
The proof is analogous to the proof of the theorem of the mean.
Hint 3.17
The ﬁrst few terms in the Taylor series of sin(x) about x = 0 are
sin(x) = x −x3
6 + x5
120 −
x7
5040 +
x9
362880 + · · · .
When determining the error, use the fact that | cos x0| ≤1 and |xn| ≤1 for x ∈[−1, 1].
Hint 3.18
The terms in the approximation have the Taylor series,
f(x + ∆x) = f(x) + ∆xf ′(x) + ∆x2
2 f ′′(x) + ∆x3
6 f ′′′(x) + ∆x4
24 f ′′′′(x1),
f(x −∆x) = f(x) −∆xf ′(x) + ∆x2
2 f ′′(x) −∆x3
6 f ′′′(x) + ∆x4
24 f ′′′′(x2),
where x ≤x1 ≤x + ∆x and x −∆x ≤x2 ≤x.
Hint 3.19
L’Hospital’s Rule
Hint 3.20
a. Apply L’Hospital’s rule three times.
b. You can write the expression as
x −sin x
x sin x .
c. Find the limit of the logarithm of the expression.
89

d. It takes four successive applications of L’Hospital’s rule to evaluate the limit.
For the Taylor series expansion method,
csc2 x −1
x2 = x2 −sin2 x
x2 sin2 x
= x2 −(x −x3/6 + O(x5))2
x2(x + O(x3))2
Hint 3.21
To evaluate the limits use the identity ab = eb ln a and then apply L’Hospital’s rule.
90

3.10
Solutions
Limits and Continuity
Solution 3.1
Note that in any open neighborhood of zero, (−δ, δ), the function sin(1/x) takes on all values in the interval [−1, 1].
Thus if we choose a positive ϵ such that ϵ < 1 then there is no value of ψ for which | sin(1/x) −ψ| < ϵ for all
x ∈(−ϵ, ϵ). Thus the limit does not exist.
Solution 3.2
We make the change of variables y = 1/x and consider y →∞. We use that sin(y) is bounded.
lim
x→0 x sin
1
x

= lim
y→∞
1
y sin(y) = 0
Solution 3.3
Since 1
x is continuous in the interval (0, 1) and the function sin(x) is continuous everywhere, the composition sin(1/x)
is continuous in the interval (0, 1).
Since limx→0 sin(1/x) does not exist, there is no way of deﬁning sin(1/x) at x = 0 to produce a function that is
continuous in [0, 1].
Solution 3.4
Note that for x1 =
1
(n−1/2)π and x2 =
1
(n+1/2)π where n ∈Z we have | sin(1/x1) −sin(1/x2)| = 2. Thus for any
0 < ϵ < 2 there is no value of δ > 0 such that | sin(1/x1) −sin(1/x2)| < ϵ for all x1, x2 ∈(0, 1) and |x1 −x2| < δ.
Thus sin(1/x) is not uniformly continuous in the open interval (0, 1).
Solution 3.5
First consider the function √x. Note that the function
√
x + δ −√x is a decreasing function of x and an increasing
function of δ for positive x and δ. Thus for any ﬁxed δ, the maximum value of
√
x + δ −√x is bounded by
√
δ.
Therefore on the interval (0, 1), a suﬃcient condition for |√x −√ξ| < ϵ is |x −ξ| < ϵ2. The function √x is uniformly
continuous on the interval (0, 1).
91

Consider any positive δ and ϵ. Note that
1
x −
1
x + δ > ϵ
for
x < 1
2
 r
δ2 + 4δ
ϵ −δ
!
.
Thus there is no value of δ such that

1
x −1
ξ
 < ϵ
for all |x −ξ| < δ. The function 1
x is not uniformly continuous on the interval (0, 1).
Solution 3.6
Let the function f(x) be continuous on a closed interval. Consider the function
e(x, δ) =
sup
|ξ−x|<δ
|f(ξ) −f(x)|.
Since f(x) is continuous, e(x, δ) is a continuous function of x on the same closed interval. Since continuous functions
on closed intervals are bounded, there is a continuous, increasing function ϵ(δ) satisfying,
e(x, δ) ≤ϵ(δ),
for all x in the closed interval.
Since ϵ(δ) is continuous and increasing, it has an inverse δ(ϵ).
Now note that
|f(x) −f(ξ)| < ϵ for all x and ξ in the closed interval satisfying |x −ξ| < δ(ϵ). Thus the function is uniformly
continuous in the closed interval.
Solution 3.7
1. The statement
lim
n→∞an = L
is equivalent to
∀ϵ > 0∃Ns.t.n > N ⇒|an −L| < ϵ.
92

We want to show that
∀δ > 0∃Ms.t.m > M ⇒|a2
n −L2| < δ.
Suppose that |an −L| < ϵ. We obtain an upper bound on |a2
n −L2|.
|a2
n −L2| = |an −L||an + L| < ϵ(|2L| + ϵ)
Now we choose a value of ϵ such that |a2
n −L2| < δ
ϵ(|2L| + ϵ) = δ
ϵ =
√
L2 + δ −|L|
Consider any ﬁxed δ > 0. We see that since
forϵ =
√
L2 + δ −|L|, ∃Ns.t.n > N ⇒|an −L| < ϵ
implies that
n > N ⇒|a2
n −L2| < δ.
Therefore
∀δ > 0∃Ms.t.m > M ⇒|a2
n −L2| < δ.
We conclude that limn→∞a2
n = L2.
2. limn→∞a2
n = L2 does not imply that limn→∞an = L. Consider an = −1. In this case limn→∞a2
n = 1 and
limn→∞an = −1.
3. If an > 0 for all n > 200, and limn→∞an = L, then L is not necessarily positive. Consider an = 1/n, which
satisﬁes the two constraints.
lim
n→∞
1
n = 0
93

4. The statement limx→∞f(x) = L is equivalent to
∀ϵ > 0∃Xs.t.x > X ⇒|f(x) −L| < ϵ.
This implies that for n > ⌈X⌉, |f(n) −L| < ϵ.
∀ϵ > 0∃Ns.t.n > N ⇒|f(n) −L| < ϵ
lim
n→∞f(n) = L
5. If f : R 7→R is continuous and limn→∞f(n) = L, then for x ∈R, it is not necessarily true that limx→∞f(x) = L.
Consider f(x) = sin(πx).
lim
n→∞sin(πn) = lim
n→∞0 = 0
limx→∞sin(πx) does not exist.
Deﬁnition of Diﬀerentiation
Solution 3.8
a.
d
dx(xn) = lim
∆x→0
(x + ∆x)n −xn
∆x

= lim
∆x→0



xn + nxn−1∆x + n(n−1)
2
xn−2∆x2 + · · · + ∆xn
−xn
∆x


= lim
∆x→0

nxn−1 + n(n −1)
2
xn−2∆x + · · · + ∆xn−1

= nxn−1
d
dx(xn) = nxn−1
94

b.
d
dx(f(x)g(x)) = lim
∆x→0
f(x + ∆x)g(x + ∆x) −f(x)g(x)
∆x

= lim
∆x→0
[f(x + ∆x)g(x + ∆x) −f(x)g(x + ∆x)] + [f(x)g(x + ∆x) −f(x)g(x)]
∆x

= lim
∆x→0 [g(x + ∆x)] lim
∆x→0
f(x + ∆x) −f(x)
∆x

+ f(x) lim
∆x→0
g(x + ∆x) −g(x)
∆x

= g(x)f ′(x) + f(x)g′(x)
d
dx(f(x)g(x)) = f(x)g′(x) + f ′(x)g(x)
c. Consider a right triangle with hypotenuse of length 1 in the ﬁrst quadrant of the plane. Label the vertices A, B,
C, in clockwise order, starting with the vertex at the origin. The angle of A is θ. The length of a circular arc of
radius cos θ that connects C to the hypotenuse is θ cos θ. The length of the side BC is sin θ. The length of a
circular arc of radius 1 that connects B to the x axis is θ. (See Figure 3.20.)
Considering the length of these three curves gives us the inequality:
θ cos θ ≤sin θ ≤θ.
Dividing by θ,
cos θ ≤sin θ
θ
≤1.
Taking the limit as θ →0 gives us
lim
θ→0
sin θ
θ
= 1.
95

B
θ
sin
A
C
θ
θ
θ
cos
θ
Figure 3.20:
One more little tidbit we’ll need to know is
lim
θ→0
cos θ −1
θ

= lim
θ→0
cos θ −1
θ
cos θ + 1
cos θ + 1

= lim
θ→0
 cos2 θ −1
θ(cos θ + 1)

= lim
θ→0

−sin2 θ
θ(cos θ + 1)

= lim
θ→0
−sin θ
θ

lim
θ→0

sin θ
(cos θ + 1)

= (−1)
0
2

= 0.
96

Now we’re ready to ﬁnd the derivative of sin x.
d
dx(sin x) = lim
∆x→0
sin(x + ∆x) −sin x
∆x

= lim
∆x→0
cos x sin ∆x + sin x cos ∆x −sin x
∆x

= cos x lim
∆x→0
sin ∆x
∆x

+ sin x lim
∆x→0
cos ∆x −1
∆x

= cos x
d
dx(sin x) = cos x
d. Let u = g(x). Consider a nonzero increment ∆x, which induces the increments ∆u and ∆f. By deﬁnition,
∆f = f(u + ∆u) −f(u),
∆u = g(x + ∆x) −g(x),
and ∆f, ∆u →0 as ∆x →0. If ∆u ̸= 0 then we have
ϵ = ∆f
∆u −df
du →0
as
∆u →0.
If ∆u = 0 for some values of ∆x then ∆f also vanishes and we deﬁne ϵ = 0 for theses values. In either case,
∆y = df
du∆u + ϵ∆u.
97

We divide this equation by ∆x and take the limit as ∆x →0.
df
dx = lim
∆x→0
∆f
∆x
= lim
∆x→0
df
du
∆u
∆x + ϵ∆u
∆x

=
df
du
 
lim
∆x→0
∆f
∆x

+

lim
∆x→0 ϵ
 
lim
∆x→0
∆u
∆x

= df
du
du
dx + (0)
du
dx

= df
du
du
dx
Thus we see that
d
dx(f(g(x))) = f ′(g(x))g′(x).
Solution 3.9
1.
f ′(0) = lim ϵ →0ϵ|ϵ| −0
ϵ
= lim ϵ →0|ϵ|
= 0
The function is diﬀerentiable at x = 0.
98

2.
f ′(0) = lim ϵ →0
p
1 + |ϵ| −1
ϵ
= lim ϵ →0
1
2(1 + |ϵ|)−1/2 sign(ϵ)
1
= lim ϵ →01
2 sign(ϵ)
Since the limit does not exist, the function is not diﬀerentiable at x = 0.
Rules of Diﬀerentiation
Solution 3.10
a.
d
dx[x sin(cos x)] = d
dx[x] sin(cos x) + x d
dx[sin(cos x)]
= sin(cos x) + x cos(cos x) d
dx[cos x]
= sin(cos x) −x cos(cos x) sin x
d
dx[x sin(cos x)] = sin(cos x) −x cos(cos x) sin x
b.
d
dx[f(cos(g(x)))] = f ′(cos(g(x))) d
dx[cos(g(x))]
= −f ′(cos(g(x))) sin(g(x)) d
dx[g(x)]
= −f ′(cos(g(x))) sin(g(x))g′(x)
99

d
dx[f(cos(g(x)))] = −f ′(cos(g(x))) sin(g(x))g′(x)
c.
d
dx

1
f(ln x)

= −
d
dx[f(ln x)]
[f(ln x)]2
= −f ′(ln x) d
dx[ln x]
[f(ln x)]2
= −f ′(ln x)
x[f(ln x)]2
d
dx

1
f(ln x)

= −f ′(ln x)
x[f(ln x)]2
d. First we write the expression in terms exponentials and logarithms,
xxx = xexp(x ln x) = exp(exp(x ln x) ln x).
Then we diﬀerentiate using the chain rule and the product rule.
d
dx exp(exp(x ln x) ln x) = exp(exp(x ln x) ln x) d
dx(exp(x ln x) ln x)
= xxx 
exp(x ln x) d
dx(x ln x) ln x + exp(x ln x)1
x

= xxx 
xx(ln x + x1
x) ln x + x−1 exp(x ln x)

= xxx  xx(ln x + 1) ln x + x−1xx
= xxx+x  x−1 + ln x + ln2 x

d
dxxxx = xxx+x  x−1 + ln x + ln2 x

100

e. For x > 0, the expression is x sin x; for x < 0, the expression is (−x) sin(−x) = x sin x. Thus we see that
|x| sin |x| = x sin x.
The ﬁrst derivative of this is
sin x + x cos x.
d
dx(|x| sin |x|) = sin x + x cos x
Solution 3.11
Let y(x) = sin x. Then y′(x) = cos x.
d
dy arcsin y =
1
y′(x)
=
1
cos x
=
1
(1 −sin2 x)1/2
=
1
(1 −y2)1/2
d
dx arcsin x =
1
(1 −x2)1/2
101

Let y(x) = tan x. Then y′(x) = 1/ cos2 x.
d
dy arctan y =
1
y′(x)
= cos2 x
= cos2(arctan y)
=

1
(1 + y2)1/2

=
1
1 + y2
d
dx arctan x =
1
1 + x2
Implicit Diﬀerentiation
Solution 3.12
Diﬀerentiating the equation
x2 + [y(x)]2 = 1
yields
2x + 2y(x)y′(x) = 0.
We can solve this equation for y′(x).
y′(x) = −x
y(x)
To ﬁnd y′(1/2) we need to ﬁnd y(x) in terms of x.
y(x) = ±
√
1 −x2
Thus y′(x) is
y′(x) = ±
x
√
1 −x2.
102

y′(1/2) can have the two values:
y′
1
2

= ± 1
√
3.
Solution 3.13
Diﬀerentiating the equation
x2 −xy(x) + [y(x)]2 = 3
yields
2x −y(x) −xy′(x) + 2y(x)y′(x) = 0.
Solving this equation for y′(x)
y′(x) = y(x) −2x
2y(x) −x.
Now we diﬀerentiate y′(x) to get y′′(x).
y′′(x) = (y′(x) −2)(2y(x) −x) −(y(x) −2x)(2y′(x) −1)
(2y(x) −x)2
,
y′′(x) = 3xy′(x) −y(x)
(2y(x) −x)2 ,
y′′(x) = 3
x y(x)−2x
2y(x)−x −y(x)
(2y(x) −x)2 ,
y′′(x) = 3x(y(x) −2x) −y(x)(2y(x) −x)
(2y(x) −x)3
,
y′′(x) = −6x2 −xy(x) + [y(x)]2
(2y(x) −x)3
,
y′′(x) =
−18
(2y(x) −x)3,
103

Maxima and Minima
Solution 3.14
a.
f ′(x) = (12 −2x)2 + 2x(12 −2x)(−2)
= 4(x −6)2 + 8x(x −6)
= 12(x −2)(x −6)
There are critical points at x = 2 and x = 6.
f ′′(x) = 12(x −2) + 12(x −6) = 24(x −4)
Since f ′′(2) = −48 < 0, x = 2 is a local maximum. Since f ′′(6) = 48 > 0, x = 6 is a local minimum.
b.
f ′(x) = 2
3(x −2)−1/3
The ﬁrst derivative exists and is nonzero for x ̸= 2. At x = 2, the derivative does not exist and thus x = 2 is a
critical point. For x < 2, f ′(x) < 0 and for x > 2, f ′(x) > 0. x = 2 is a local minimum.
Solution 3.15
Let r be the radius and h the height of the cylinder. The volume of the cup is πr2h = 64. The radius and height are
related by h =
64
πr2. The surface area of the cup is f(r) = πr2 + 2πrh = πr2 + 128
r . The ﬁrst derivative of the surface
area is f ′(r) = 2πr −128
r2 . Finding the zeros of f ′(r),
2πr −128
r2 = 0,
2πr3 −128 = 0,
r =
4
3√π.
104

The second derivative of the surface area is f ′′(r) = 2π + 256
r3 . Since f ′′( 4
3√π) = 6π, r =
4
3√π is a local minimum of
f(r). Since this is the only critical point for r > 0, it must be a global minimum.
The cup has a radius of
4
3√π cm and a height of
4
3√π.
Mean Value Theorems
Solution 3.16
We deﬁne the function
h(x) = f(x) −f(a) −f(b) −f(a)
g(b) −g(a) (g(x) −g(a)).
Note that h(x) is diﬀerentiable and that h(a) = h(b) = 0. Thus h(x) satisﬁes the conditions of Rolle’s theorem and
there exists a point ξ ∈(a, b) such that
h′(ξ) = f ′(ξ) −f(b) −f(a)
g(b) −g(a) g′(ξ) = 0,
f ′(ξ)
g′(ξ) = f(b) −f(a)
g(b) −g(a) .
Solution 3.17
The ﬁrst few terms in the Taylor series of sin(x) about x = 0 are
sin(x) = x −x3
6 + x5
120 −
x7
5040 +
x9
362880 + · · · .
The seventh derivative of sin x is −cos x. Thus we have that
sin(x) = x −x3
6 + x5
120 −cos x0
5040 x7,
where 0 ≤x0 ≤x. Since we are considering x ∈[−1, 1] and −1 ≤cos(x0) ≤1, the approximation
sin x ≈x −x3
6 + x5
120
105

has a maximum error of
1
5040 ≈0.000198. Using this polynomial to approximate sin(1),
1 −13
6 + 15
120 ≈0.841667.
To see that this has the required accuracy,
sin(1) ≈0.841471.
Solution 3.18
Expanding the terms in the approximation in Taylor series,
f(x + ∆x) = f(x) + ∆xf ′(x) + ∆x2
2 f ′′(x) + ∆x3
6 f ′′′(x) + ∆x4
24 f ′′′′(x1),
f(x −∆x) = f(x) −∆xf ′(x) + ∆x2
2 f ′′(x) −∆x3
6 f ′′′(x) + ∆x4
24 f ′′′′(x2),
where x ≤x1 ≤x + ∆x and x −∆x ≤x2 ≤x. Substituting the expansions into the formula,
f(x + ∆x) −2f(x) + f(x −∆x)
∆x2
= f ′′(x) + ∆x2
24 [f ′′′′(x1) + f ′′′′(x2)].
Thus the error in the approximation is
∆x2
24 [f ′′′′(x1) + f ′′′′(x2)].
Solution 3.19
L’Hospital’s Rule
106

Solution 3.20
a.
lim
x→0
x −sin x
x3

= lim
x→0
1 −cos x
3x2

= lim
x→0
sin x
6x

= lim
x→0
hcos x
6
i
= 1
6
lim
x→0
x −sin x
x3

= 1
6
b.
lim
x→0

csc x −1
x

= lim
x→0

1
sin x −1
x

= lim
x→0
x −sin x
x sin x

= lim
x→0

1 −cos x
x cos x + sin x

= lim
x→0

sin x
−x sin x + cos x + cos x

= 0
2
= 0
lim
x→0

csc x −1
x

= 0
107

c.
ln

lim
x→+∞

1 + 1
x
x
= lim
x→+∞

ln

1 + 1
x
x
= lim
x→+∞

x ln

1 + 1
x

= lim
x→+∞
"
ln
 1 + 1
x

1/x
#
= lim
x→+∞
" 1 + 1
x
−1  −1
x2

−1/x2
#
= lim
x→+∞
"
1 + 1
x
−1#
= 1
Thus we have
lim
x→+∞

1 + 1
x
x
= e.
108

d. It takes four successive applications of L’Hospital’s rule to evaluate the limit.
lim
x→0

csc2 x −1
x2

= lim
x→0
x2 −sin2 x
x2 sin2 x
= lim
x→0
2x −2 cos x sin x
2x2 cos x sin x + 2x sin2 x
= lim
x→0
2 −2 cos2 x + 2 sin2 x
2x2 cos2 x + 8x cos x sin x + 2 sin2 x −2x2 sin2 x
= lim
x→0
8 cos x sin x
12x cos2 x + 12 cos x sin x −8x2 cos x sin x −12x sin2 x
= lim
x→0
8 cos2 x −8 sin2 x
24 cos2 x −8x2 cos2 x −64x cos x sin x −24 sin2 x + 8x2 sin2 x
= 1
3
It is easier to use a Taylor series expansion.
lim
x→0

csc2 x −1
x2

= lim
x→0
x2 −sin2 x
x2 sin2 x
= lim
x→0
x2 −(x −x3/6 + O(x5))2
x2(x + O(x3))2
= lim
x→0
x2 −(x2 −x4/3 + O(x6))
x4 + O(x6)
= lim
x→0
1
3 + O(x2)

= 1
3
109

Solution 3.21
To evaluate the ﬁrst limit, we use the identity ab = eb ln a and then apply L’Hospital’s rule.
lim
x→∞xa/x = lim
x→∞e
a ln x
x
= exp

lim
x→∞
a ln x
x

= exp

lim
x→∞
a/x
1

= e0
lim
x→∞xa/x = 1
We use the same method to evaluate the second limit.
lim
x→∞

1 + a
x
bx
= lim
x→∞exp

bx ln

1 + a
x

= exp

lim
x→∞bx ln

1 + a
x

= exp

lim
x→∞bln(1 + a/x)
1/x

= exp

lim
x→∞b
−a/x2
1+a/x
−1/x2


= exp

lim
x→∞b
a
1 + a/x

lim
x→∞

1 + a
x
bx
= eab
110

Chapter 4
Integral Calculus
4.1
The Indeﬁnite Integral
The opposite of a derivative is the anti-derivative or the indeﬁnite integral. The indeﬁnite integral of a function f(x)
is denoted,
Z
f(x) dx.
It is deﬁned by the property that
d
dx
Z
f(x) dx = f(x).
While a function f(x) has a unique derivative if it is diﬀerentiable, it has an inﬁnite number of indeﬁnite integrals, each
of which diﬀer by an additive constant.
Zero Slope Implies a Constant Function.
If the value of a function’s derivative is identically zero, df
dx = 0,
then the function is a constant, f(x) = c. To prove this, we assume that there exists a non-constant diﬀerentiable
function whose derivative is zero and obtain a contradiction. Let f(x) be such a function. Since f(x) is non-constant,
there exist points a and b such that f(a) ̸= f(b). By the Mean Value Theorem of diﬀerential calculus, there exists a
111

point ξ ∈(a, b) such that
f ′(ξ) = f(b) −f(a)
b −a
̸= 0,
which contradicts that the derivative is everywhere zero.
Indeﬁnite Integrals Diﬀer by an Additive Constant.
Suppose that F(x) and G(x) are indeﬁnite integrals
of f(x). Then we have
d
dx(F(x) −G(x)) = F ′(x) −G′(x) = f(x) −f(x) = 0.
Thus we see that F(x) −G(x) = c and the two indeﬁnite integrals must diﬀer by a constant. For example, we have
R
sin x dx = −cos x + c. While every function that can be expressed in terms of elementary functions, (the exponent,
logarithm, trigonometric functions, etc.), has a derivative that can be written explicitly in terms of elementary functions,
the same is not true of integrals. For example,
R
sin(sin x) dx cannot be written explicitly in terms of elementary
functions.
Properties.
Since the derivative is linear, so is the indeﬁnite integral. That is,
Z
(af(x) + bg(x)) dx = a
Z
f(x) dx + b
Z
g(x) dx.
For each derivative identity there is a corresponding integral identity. Consider the power law identity,
d
dx(f(x))a =
a(f(x))a−1f ′(x). The corresponding integral identity is
Z
(f(x))af ′(x) dx = (f(x))a+1
a + 1
+ c,
a ̸= −1,
where we require that a ̸= −1 to avoid division by zero. From the derivative of a logarithm,
d
dx ln(f(x)) = f′(x)
f(x) , we
obtain,
Z f ′(x)
f(x) dx = ln |f(x)| + c.
112

Figure 4.1: Plot of ln |x| and 1/x.
Note the absolute value signs. This is because
d
dx ln |x| = 1
x for x ̸= 0. In Figure 4.1 is a plot of ln |x| and 1
x to reinforce
this.
Example 4.1.1 Consider
I =
Z
x
(x2 + 1)2 dx.
We evaluate the integral by choosing u = x2 + 1, du = 2x dx.
I = 1
2
Z
2x
(x2 + 1)2 dx
= 1
2
Z du
u2
= 1
2
−1
u
= −
1
2(x2 + 1).
Example 4.1.2 Consider
I =
Z
tan x dx =
Z sin x
cos x dx.
113

By choosing f(x) = cos x, f ′(x) = −sin x, we see that the integral is
I = −
Z −sin x
cos x dx = −ln | cos x| + c.
Change of Variable.
The diﬀerential of a function g(x) is dg = g′(x) dx.
Thus one might suspect that for
ξ = g(x),
Z
f(ξ) dξ =
Z
f(g(x))g′(x) dx,
(4.1)
since dξ = dg = g′(x) dx. This turns out to be true. To prove it we will appeal to the the chain rule for diﬀerentiation.
Let ξ be a function of x. The chain rule is
d
dxf(ξ) = f ′(ξ)ξ′(x),
d
dxf(ξ) = df
dξ
dξ
dx.
We can also write this as
df
dξ = dx
dξ
df
dx,
or in operator notation,
d
dξ = dx
dξ
d
dx.
Now we’re ready to start. The derivative of the left side of Equation 4.1 is
d
dξ
Z
f(ξ) dξ = f(ξ).
114

Next we diﬀerentiate the right side,
d
dξ
Z
f(g(x))g′(x) dx = dx
dξ
d
dx
Z
f(g(x))g′(x) dx
= dx
dξ f(g(x))g′(x)
= dx
dg f(g(x))dg
dx
= f(g(x))
= f(ξ)
to see that it is in fact an identity for ξ = g(x).
Example 4.1.3 Consider
Z
x sin(x2) dx.
We choose ξ = x2, dξ = 2xdx to evaluate the integral.
Z
x sin(x2) dx = 1
2
Z
sin(x2)2x dx
= 1
2
Z
sin ξ dξ
= 1
2(−cos ξ) + c
= −1
2 cos(x2) + c
115

Integration by Parts.
The product rule for diﬀerentiation gives us an identity called integration by parts. We start
with the product rule and then integrate both sides of the equation.
d
dx(u(x)v(x)) = u′(x)v(x) + u(x)v′(x)
Z
(u′(x)v(x) + u(x)v′(x)) dx = u(x)v(x) + c
Z
u′(x)v(x) dx +
Z
u(x)v′(x)) dx = u(x)v(x)
Z
u(x)v′(x)) dx = u(x)v(x) −
Z
v(x)u′(x) dx
The theorem is most often written in the form
Z
u dv = uv −
Z
v du.
So what is the usefulness of this? Well, it may happen for some integrals and a good choice of u and v that the integral
on the right is easier to evaluate than the integral on the left.
Example 4.1.4 Consider
R
x ex dx. If we choose u = x, dv = ex dx then integration by parts yields
Z
x ex dx = x ex −
Z
ex dx = (x −1) ex .
Now notice what happens when we choose u = ex, dv = x dx.
Z
x ex dx = 1
2x2 ex −
Z 1
2x2 ex dx
The integral gets harder instead of easier.
When applying integration by parts, one must choose u and dv wisely. As general rules of thumb:
116

• Pick u so that u′ is simpler than u.
• Pick dv so that v is not more complicated, (hopefully simpler), than dv.
Also note that you may have to apply integration by parts several times to evaluate some integrals.
4.2
The Deﬁnite Integral
4.2.1
Deﬁnition
The area bounded by the x axis, the vertical lines x = a and x = b and the function f(x) is denoted with a deﬁnite
integral,
Z b
a
f(x) dx.
The area is signed, that is, if f(x) is negative, then the area is negative. We measure the area with a divide-and-conquer
strategy. First partition the interval (a, b) with a = x0 < x1 < · · · < xn−1 < xn = b. Note that the area under the
curve on the subinterval is approximately the area of a rectangle of base ∆xi = xi+1 −xi and height f(ξi), where
ξi ∈[xi, xi+1]. If we add up the areas of the rectangles, we get an approximation of the area under the curve. See
Figure 4.2
Z b
a
f(x) dx ≈
n−1
X
i=0
f(ξi)∆xi
As the ∆xi’s get smaller, we expect the approximation of the area to get better. Let ∆x = max0≤i≤n−1 ∆xi. We
deﬁne the deﬁnite integral as the sum of the areas of the rectangles in the limit that ∆x →0.
Z b
a
f(x) dx = lim
∆x→0
n−1
X
i=0
f(ξi)∆xi
The integral is deﬁned when the limit exists. This is known as the Riemann integral of f(x). f(x) is called the
integrand.
117

a
x
x x
x
x
x
∆
1
2
3
i
n-2
n-1 b
f(   )
ξ1
Figure 4.2: Divide-and-Conquer Strategy for Approximating a Deﬁnite Integral.
4.2.2
Properties
Linearity and the Basics.
Because summation is a linear operator, that is
n−1
X
i=0
(cfi + dgi) = c
n−1
X
i=0
fi + d
n−1
X
i=0
gi,
deﬁnite integrals are linear,
Z b
a
(cf(x) + dg(x)) dx = c
Z b
a
f(x) dx + d
Z b
a
g(x) dx.
One can also divide the range of integration.
Z b
a
f(x) dx =
Z c
a
f(x) dx +
Z b
c
f(x) dx
118

We assume that each of the above integrals exist. If a ≤b, and we integrate from b to a, then each of the ∆xi will be
negative. From this observation, it is clear that
Z b
a
f(x) dx = −
Z a
b
f(x) dx.
If we integrate any function from a point a to that same point a, then all the ∆xi are zero and
Z a
a
f(x) dx = 0.
Bounding the Integral.
Recall that if fi ≤gi, then
n−1
X
i=0
fi ≤
n−1
X
i=0
gi.
Let m = minx∈[a,b] f(x) and M = maxx∈[a,b] f(x). Then
(b −a)m =
n−1
X
i=0
m∆xi ≤
n−1
X
i=0
f(ξi)∆xi ≤
n−1
X
i=0
M∆xi = (b −a)M
implies that
(b −a)m ≤
Z b
a
f(x) dx ≤(b −a)M.
Since

n−1
X
i=0
fi
 ≤
n−1
X
i=0
|fi|,
we have

Z b
a
f(x) dx
 ≤
Z b
a
|f(x)| dx.
119

Mean Value Theorem of Integral Calculus.
Let f(x) be continuous. We know from above that
(b −a)m ≤
Z b
a
f(x) dx ≤(b −a)M.
Therefore there exists a constant c ∈[m, M] satisfying
Z b
a
f(x) dx = (b −a)c.
Since f(x) is continuous, there is a point ξ ∈[a, b] such that f(ξ) = c. Thus we see that
Z b
a
f(x) dx = (b −a)f(ξ),
for some ξ ∈[a, b].
4.3
The Fundamental Theorem of Integral Calculus
Deﬁnite Integrals with Variable Limits of Integration.
Consider a to be a constant and x variable, then
the function F(x) deﬁned by
F(x) =
Z x
a
f(t) dt
(4.2)
120

is an anti-derivative of f(x), that is F ′(x) = f(x). To show this we apply the deﬁnition of diﬀerentiation and the
integral mean value theorem.
F ′(x) = lim
∆x→0
F(x + ∆x) −F(x)
∆x
= lim
∆x→0
R x+∆x
a
f(t) dt −
R x
a f(t) dt
∆x
= lim
∆x→0
R x+∆x
x
f(t) dt
∆x
= lim
∆x→0
f(ξ)∆x
∆x
,
ξ ∈[x, x + ∆x]
= f(x)
The Fundamental Theorem of Integral Calculus.
Let F(x) be any anti-derivative of f(x). Noting that all
anti-derivatives of f(x) diﬀer by a constant and replacing x by b in Equation 4.2, we see that there exists a constant c
such that
Z b
a
f(x) dx = F(b) + c.
Now to ﬁnd the constant. By plugging in b = a,
Z a
a
f(x) dx = F(a) + c = 0,
we see that c = −F(a). This gives us a result known as the Fundamental Theorem of Integral Calculus.
Z b
a
f(x) dx = F(b) −F(a).
We introduce the notation
[F(x)]b
a ≡F(b) −F(a).
121

Example 4.3.1
Z π
0
sin x dx = [−cos x]π
0 = −cos(π) + cos(0) = 2
4.4
Techniques of Integration
4.4.1
Partial Fractions
A proper rational function
p(x)
q(x) =
p(x)
(x −a)nr(x)
Can be written in the form
p(x)
(x −α)nr(x) =

a0
(x −α)n +
a1
(x −α)n−1 + · · · + an−1
x −α

+ (· · · )
where the ak’s are constants and the last ellipses represents the partial fractions expansion of the roots of r(x). The
coeﬃcients are
ak = 1
k!
dk
dxk
p(x)
r(x)
 
x=α
.
Example 4.4.1 Consider the partial fraction expansion of
1 + x + x2
(x −1)3 .
The expansion has the form
a0
(x −1)3 +
a1
(x −1)2 +
a2
x −1.
122

The coeﬃcients are
a0 = 1
0!(1 + x + x2)|x=1 = 3,
a1 = 1
1!
d
dx(1 + x + x2)|x=1 = (1 + 2x)|x=1 = 3,
a2 = 1
2!
d2
dx2(1 + x + x2)|x=1 = 1
2(2)|x=1 = 1.
Thus we have
1 + x + x2
(x −1)3
=
3
(x −1)3 +
3
(x −1)2 +
1
x −1.
Example 4.4.2 Suppose we want to evaluate
Z 1 + x + x2
(x −1)3
dx.
If we expand the integrand in a partial fraction expansion, then the integral becomes easy.
Z 1 + x + x2
(x −1)3
dx. =
Z 
3
(x −1)3 +
3
(x −1)2 +
1
x −1

dx
= −
3
2(x −1)2 −
3
(x −1) + ln(x −1)
Example 4.4.3 Consider the partial fraction expansion of
1 + x + x2
x2(x −1)2 .
The expansion has the form
a0
x2 + a1
x +
b0
(x −1)2 +
b1
x −1.
123

The coeﬃcients are
a0 = 1
0!
1 + x + x2
(x −1)2
 
x=0
= 1,
a1 = 1
1!
d
dx
1 + x + x2
(x −1)2
 
x=0
=
 1 + 2x
(x −1)2 −2(1 + x + x2)
(x −1)3
 
x=0
= 3,
b0 = 1
0!
1 + x + x2
x2
 
x=1
= 3,
b1 = 1
1!
d
dx
1 + x + x2
x2
 
x=1
=
1 + 2x
x2
−2(1 + x + x2)
x3
 
x=1
= −3,
Thus we have
1 + x + x2
x2(x −1)2 = 1
x2 + 3
x +
3
(x −1)2 −
3
x −1.
If the rational function has real coeﬃcients and the denominator has complex roots, then you can reduce the work
in ﬁnding the partial fraction expansion with the following trick: Let α and α be complex conjugate pairs of roots of
the denominator.
p(x)
(x −α)n(x −α)nr(x) =

a0
(x −α)n +
a1
(x −α)n−1 + · · · + an−1
x −α

+

a0
(x −α)n +
a1
(x −α)n−1 + · · · + an−1
x −α

+ (· · · )
Thus we don’t have to calculate the coeﬃcients for the root at α. We just take the complex conjugate of the coeﬃcients
for α.
Example 4.4.4 Consider the partial fraction expansion of
1 + x
x2 + 1.
124

The expansion has the form
a0
x −i +
a0
x + i
The coeﬃcients are
a0 = 1
0!
1 + x
x + i
 
x=i
= 1
2(1 −i),
a0 = 1
2(1 −i) = 1
2(1 + i)
Thus we have
1 + x
x2 + 1 =
1 −i
2(x −i) +
1 + i
2(x + i).
4.5
Improper Integrals
If the range of integration is inﬁnite or f(x) is discontinuous at some points then
R b
a f(x) dx is called an improper
integral.
Discontinuous Functions.
If f(x) is continuous on the interval a ≤x ≤b except at the point x = c where
a < c < b then
Z b
a
f(x) dx = lim
δ→0+
Z c−δ
a
f(x) dx + lim
ϵ→0+
Z b
c+ϵ
f(x) dx
provided that both limits exist.
Example 4.5.1 Consider the integral of ln x on the interval [0, 1]. Since the logarithm has a singularity at x = 0, this
125

is an improper integral. We write the integral in terms of a limit and evaluate the limit with L’Hospital’s rule.
Z 1
0
ln x dx = lim
δ→0
Z 1
δ
ln x dx
= lim
δ→0[x ln x −x]1
δ
= 1 ln(1) −1 −lim
δ→0(δ ln δ −δ)
= −1 −lim
δ→0(δ ln δ)
= −1 −lim
δ→0
ln δ
1/δ

= −1 −lim
δ→0
 1/δ
−1/δ2

= −1
Example 4.5.2 Consider the integral of xa on the range [0, 1]. If a < 0 then there is a singularity at x = 0. First
assume that a ̸= −1.
Z 1
0
xa dx = lim
δ→0+
 xa+1
a + 1
1
δ
=
1
a + 1 −lim
δ→0+
δa+1
a + 1
This limit exists only for a > −1. Now consider the case that a = −1.
Z 1
0
x−1 dx = lim
δ→0+ [ln x]1
δ
= ln(0) −lim
δ→0+ ln δ
126

This limit does not exist. We obtain the result,
Z 1
0
xa dx =
1
a + 1,
for a > −1.
Inﬁnite Limits of Integration.
If the range of integration is inﬁnite, say [a, ∞) then we deﬁne the integral as
Z ∞
a
f(x) dx = lim
α→∞
Z α
a
f(x) dx,
provided that the limit exists. If the range of integration is (−∞, ∞) then
Z ∞
−∞
f(x) dx =
lim
α→−∞
Z a
α
f(x) dx + lim
β→+∞
Z β
a
f(x) dx.
Example 4.5.3
Z ∞
1
ln x
x2 dx =
Z ∞
1
ln x
 d
dx
−1
x

dx
=

ln x−1
x
∞
1
−
Z ∞
1
−1
x
1
x dx
= lim
x→+∞

−ln x
x

−
1
x
∞
1
= lim
x→+∞

−1/x
1

−lim
x→∞
1
x + 1
= 1
127

Example 4.5.4 Consider the integral of xa on [1, ∞). First assume that a ̸= −1.
Z ∞
1
xa dx = lim
β→+∞
 xa+1
a + 1
β
1
= lim
β→+∞
βa+1
a + 1 −
1
a + 1
The limit exists for β < −1. Now consider the case a = −1.
Z ∞
1
x−1 dx = lim
β→+∞[ln x]β
1
= lim
β→+∞ln β −
1
a + 1
This limit does not exist. Thus we have
Z ∞
1
xa dx = −
1
a + 1,
for a < −1.
128

4.6
Exercises
Fundamental Integration Formulas
Exercise 4.1 (mathematica/calculus/integral/fundamental.nb)
Evaluate
R
(2x + 3)10 dx.
Hint, Solution
Exercise 4.2 (mathematica/calculus/integral/fundamental.nb)
Evaluate
R (ln x)2
x
dx.
Hint, Solution
Exercise 4.3 (mathematica/calculus/integral/fundamental.nb)
Evaluate
R
x
√
x2 + 3 dx.
Hint, Solution
Exercise 4.4 (mathematica/calculus/integral/fundamental.nb)
Evaluate
R cos x
sin x dx.
Hint, Solution
Exercise 4.5 (mathematica/calculus/integral/fundamental.nb)
Evaluate
R
x2
x3−5 dx.
Hint, Solution
Integration by Parts
Exercise 4.6 (mathematica/calculus/integral/parts.nb)
Evaluate
R
x sin x dx.
Hint, Solution
Exercise 4.7 (mathematica/calculus/integral/parts.nb)
Evaluate
R
x3 e2x dx.
129

Hint, Solution
Partial Fractions
Exercise 4.8 (mathematica/calculus/integral/partial.nb)
Evaluate
R
1
x2−4 dx.
Hint, Solution
Exercise 4.9 (mathematica/calculus/integral/partial.nb)
Evaluate
R
x+1
x3+x2−6x dx.
Hint, Solution
Deﬁnite Integrals
Exercise 4.10 (mathematica/calculus/integral/deﬁnite.nb)
Use the result
Z b
a
f(x) dx = lim
N→∞
N−1
X
n=0
f(xn)∆x
where ∆x = b−a
N and xn = a + n∆x, to show that
Z 1
0
x dx = 1
2.
Hint, Solution
Exercise 4.11 (mathematica/calculus/integral/deﬁnite.nb)
Evaluate the following integral using integration by parts and the Pythagorean identity.
R π
0 sin2 x dx
Hint, Solution
130

Exercise 4.12 (mathematica/calculus/integral/deﬁnite.nb)
Prove that
d
dx
Z f(x)
g(x)
h(ξ) dξ = h(f(x))f ′(x) −h(g(x))g′(x).
(Don’t use the limit deﬁnition of diﬀerentiation, use the Fundamental Theorem of Integral Calculus.)
Hint, Solution
Exercise 4.13 (mathematica/calculus/integral/deﬁnite.nb)
Let An be the area between the curves x and xn on the interval [0 . . . 1]. What is limn→∞An? Explain this result
geometrically.
Hint, Solution
Improper Integrals
Exercise 4.14 (mathematica/calculus/integral/improper.nb)
Evaluate
R 4
0
1
(x−1)2 dx.
Hint, Solution
Exercise 4.15 (mathematica/calculus/integral/improper.nb)
Evaluate
R 1
0
1
√x dx.
Hint, Solution
Exercise 4.16 (mathematica/calculus/integral/improper.nb)
Evaluate
R ∞
0
1
x2+4 dx.
Hint, Solution
Taylor Series
131

Exercise 4.17 (mathematica/calculus/integral/taylor.nb)
a. Show that
f(x) = f(0) +
Z x
0
f ′(x −ξ) dξ.
b. From the above identity show that
f(x) = f(0) + xf ′(0) +
Z x
0
ξf ′′(x −ξ) dξ.
c. Using induction, show that
f(x) = f(0) + xf ′(0) + 1
2x2f ′′(0) + · · · + 1
n!xnf (n)(0) +
Z x
0
1
n!ξnf (n+1)(x −ξ) dξ.
Hint, Solution
Exercise 4.18
Find a function f(x) whose arc length from 0 to x is 2x.
Hint, Solution
Exercise 4.19
Consider a curve C, bounded by −1 and 1, on the interval (−1 . . . 1). Can the length of C be unbounded? What if we
change to the closed interval [−1 . . . 1]?
Hint, Solution
132

4.7
Hints
Fundamental Integration Formulas
Hint 4.1
Make the change of variables u = 2x + 3.
Hint 4.2
Make the change of variables u = ln x.
Hint 4.3
Make the change of variables u = x2 + 3.
Hint 4.4
Make the change of variables u = sin x.
Hint 4.5
Make the change of variables u = x3 −5.
Integration by Parts
Hint 4.6
Let u = x, and dv = sin x dx.
Hint 4.7
Perform integration by parts three successive times. For the ﬁrst one let u = x3 and dv = e2x dx.
Partial Fractions
133

Hint 4.8
Expanding the integrand in partial fractions,
1
x2 −4 =
1
(x −2)(x + 2) =
a
(x −2) +
b
(x + 2)
1 = a(x + 2) + b(x −2)
Set x = 2 and x = −2 to solve for a and b.
Hint 4.9
Expanding the integral in partial fractions,
x + 1
x3 + x2 −6x =
x + 1
x(x −2)(x + 3) = a
x +
b
x −2 +
c
x + 3
x + 1 = a(x −2)(x + 3) + bx(x + 3) + cx(x −2)
Set x = 0, x = 2 and x = −3 to solve for a, b and c.
Deﬁnite Integrals
Hint 4.10
Z 1
0
x dx = lim
N→∞
N−1
X
n=0
xn∆x
= lim
N→∞
N−1
X
n=0
(n∆x)∆x
Hint 4.11
Let u = sin x and dv = sin x dx. Integration by parts will give you an equation for
R π
0 sin2 x dx.
134

Hint 4.12
Let H′(x) = h(x) and evaluate the integral in terms of H(x).
Hint 4.13
CONTINUE
Improper Integrals
Hint 4.14
Z 4
0
1
(x −1)2 dx = lim
δ→0+
Z 1−δ
0
1
(x −1)2 dx + lim
ϵ→0+
Z 4
1+ϵ
1
(x −1)2 dx
Hint 4.15
Z 1
0
1
√x dx = lim
ϵ→0+
Z 1
ϵ
1
√x dx
Hint 4.16
Z
1
x2 + a2 dx = 1
a arctan
x
a

Taylor Series
Hint 4.17
a. Evaluate the integral.
b. Use integration by parts to evaluate the integral.
c. Use integration by parts with u = f (n+1)(x −ξ) and dv = 1
n!ξn.
135

Hint 4.18
The arc length from 0 to x is
Z x
0
p
1 + (f ′(ξ))2 dξ
(4.3)
First show that the arc length of f(x) from a to b is 2(b −a). Then conclude that the integrand in Equation 4.3 must
everywhere be 2.
Hint 4.19
CONTINUE
136

4.8
Solutions
Fundamental Integration Formulas
Solution 4.1
Z
(2x + 3)10 dx
Let u = 2x + 3, g(u) = x = u−3
2 , g′(u) = 1
2.
Z
(2x + 3)10 dx =
Z
u101
2 du
= u11
11
1
2
= (2x + 3)11
22
Solution 4.2
Z (ln x)2
x
dx =
Z
(ln x)2d(ln x)
dx
dx
= (ln x)3
3
137

Solution 4.3
Z
x
√
x2 + 3 dx =
Z √
x2 + 31
2
d(x2)
dx
dx
= 1
2
(x2 + 3)3/2
3/2
= (x2 + 3)3/2
3
Solution 4.4
Z cos x
sin x dx =
Z
1
sin x
d(sin x)
dx
dx
= ln | sin x|
Solution 4.5
Z
x2
x3 −5 dx =
Z
1
x3 −5
1
3
d(x3)
dx
dx
= 1
3 ln |x3 −5|
Integration by Parts
Solution 4.6
Let u = x, and dv = sin x dx. Then du = dx and v = −cos x.
Z
x sin x dx = −x cos x +
Z
cos x dx
= −x cos x + sin x + C
138

Solution 4.7
Let u = x3 and dv = e2x dx. Then du = 3x2 dx and v = 1
2 e2x.
Z
x3 e2x dx = 1
2x3 e2x −3
2
Z
x2 e2x dx
Let u = x2 and dv = e2x dx. Then du = 2x dx and v = 1
2 e2x.
Z
x3 e2x dx = 1
2x3 e2x −3
2
1
2x2 e2x −
Z
x e2x dx

Z
x3 e2x dx = 1
2x3 e2x −3
4x2 e2x +3
2
Z
x e2x dx
Let u = x and dv = e2x dx. Then du = dx and v = 1
2 e2x.
Z
x3 e2x dx = 1
2x3 e2x −3
4x2 e2x +3
2
1
2x e2x −1
2
Z
e2x dx

Z
x3 e2x dx = 1
2x3 e2x −3
4x2 e2x +3
4x e2x −3
8 e2x +C
Partial Fractions
Solution 4.8
Expanding the integrand in partial fractions,
1
x2 −4 =
1
(x −2)(x + 2) =
A
(x −2) +
B
(x + 2)
1 = A(x + 2) + B(x −2)
139

Setting x = 2 yields A = 1
4. Setting x = −2 yields B = −1
4. Now we can do the integral.
Z
1
x2 −4 dx =
Z 
1
4(x −2) −
1
4(x + 2)

dx
= 1
4 ln |x −2| −1
4 ln |x + 2| + C
= 1
4

x −2
x + 2
 + C
Solution 4.9
Expanding the integral in partial fractions,
x + 1
x3 + x2 −6x =
x + 1
x(x −2)(x + 3) = A
x +
B
x −2 +
C
x + 3
x + 1 = A(x −2)(x + 3) + Bx(x + 3) + Cx(x −2)
Setting x = 0 yields A = −1
6. Setting x = 2 yields B =
3
10. Setting x = −3 yields C = −2
15.
Z
x + 1
x3 + x2 −6x dx =
Z 
−1
6x +
3
10(x −2) −
2
15(x + 3)

dx
= −1
6 ln |x| + 3
10 ln |x −2| −2
15 ln |x + 3| + C
= ln
|x −2|3/10
|x|1/6|x + 3|2/15 + C
Deﬁnite Integrals
140

Solution 4.10
Z 1
0
x dx = lim
N→∞
N−1
X
n=0
xn∆x
= lim
N→∞
N−1
X
n=0
(n∆x)∆x
= lim
N→∞∆x2
N−1
X
n=0
n
= lim
N→∞∆x2N(N −1)
2
= lim
N→∞
N(N −1)
2N 2
= 1
2
Solution 4.11
Let u = sin x and dv = sin x dx. Then du = cos x dx and v = −cos x.
Z π
0
sin2 x dx =

−sin x cos x
π
0 +
Z π
0
cos2 x dx
=
Z π
0
cos2 x dx
=
Z π
0
(1 −sin2 x) dx
= π −
Z π
0
sin2 x dx
2
Z π
0
sin2 x dx = π
141

Z π
0
sin2 x dx = π
2
Solution 4.12
Let H′(x) = h(x).
d
dx
Z f(x)
g(x)
h(ξ) dξ = d
dx (H(f(x)) −H(g(x)))
= H′(f(x))f ′(x) −H′(g(x))g′(x)
= h(f(x))f ′(x) −h(g(x))g′(x)
Solution 4.13
First we compute the area for positive integer n.
An =
Z 1
0
(x −xn) dx =
x2
2 −xn+1
n + 1
1
0
= 1
2 −
1
n + 1
Then we consider the area in the limit as n →∞.
lim
n→∞An = lim
n→∞
1
2 −
1
n + 1

= 1
2
In Figure 4.3 we plot the functions x1, x2, x4, x8, . . . , x1024. In the limit as n →∞, xn on the interval [0 . . . 1] tends to
the function
(
0
0 ≤x < 1
1
x = 1
Thus the area tends to the area of the right triangle with unit base and height.
Improper Integrals
142

0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 4.3: Plots of x1, x2, x4, x8, . . . , x1024.
Solution 4.14
Z 4
0
1
(x −1)2 dx = lim
δ→0+
Z 1−δ
0
1
(x −1)2 dx + lim
ϵ→0+
Z 4
1+ϵ
1
(x −1)2 dx
= lim
δ→0+

−
1
x −1
1−δ
0
+ lim
ϵ→0+

−
1
x −1
4
1+ϵ
= lim
δ→0+
1
δ −1

+ lim
ϵ→0+

−1
3 + 1
ϵ

= ∞+ ∞
The integral diverges.
143

Solution 4.15
Z 1
0
1
√x dx = lim
ϵ→0+
Z 1
ϵ
1
√x dx
= lim
ϵ→0+

2√x
1
ϵ
= lim
ϵ→0+ 2(1 −√ϵ)
= 2
Solution 4.16
Z ∞
0
1
x2 + 4 dx = lim
α→∞
Z α
0
1
x2 + 4 dx
= lim
α→∞
1
2 arctan
x
2
α
0
= 1
2
π
2 −0

= π
4
Taylor Series
Solution 4.17
1.
f(0) +
Z x
0
f ′(x −ξ) dξ = f(0) + [−f(x −ξ)]x
0
= f(0) −f(0) + f(x)
= f(x)
144

2.
f(0) + xf ′(0) +
Z x
0
ξf ′′(x −ξ) dξ = f(0) + xf ′(0) + [−ξf ′(x −ξ)]x
0 −
Z x
0
−f ′(x −ξ) dξ
= f(0) + xf ′(0) −xf ′(0) −[f(x −ξ)]x
0
= f(0) −f(0) + f(x)
= f(x)
3. Above we showed that the hypothesis holds for n = 0 and n = 1. Assume that it holds for some n = m ≥0.
f(x) = f(0) + xf ′(0) + 1
2x2f ′′(0) + · · · + 1
n!xnf (n)(0) +
Z x
0
1
n!ξnf (n+1)(x −ξ) dξ
= f(0) + xf ′(0) + 1
2x2f ′′(0) + · · · + 1
n!xnf (n)(0) +

1
(n + 1)!ξn+1f (n+1)(x −ξ)
x
0
−
Z x
0
−
1
(n + 1)!ξn+1f (n+2)(x −ξ) dξ
= f(0) + xf ′(0) + 1
2x2f ′′(0) + · · · + 1
n!xnf (n)(0) +
1
(n + 1)!xn+1f (n+1)(0)
+
Z x
0
1
(n + 1)!ξn+1f (n+2)(x −ξ) dξ
This shows that the hypothesis holds for n = m + 1. By induction, the hypothesis hold for all n ≥0.
Solution 4.18
First note that the arc length from a to b is 2(b −a).
Z b
a
p
1 + (f ′(x))2 dx =
Z b
0
p
1 + (f ′(x))2 dx −
Z a
0
p
1 + (f ′(x))2 dx = 2b −2a
Since a and b are arbitrary, we conclude that the integrand must everywhere be 2.
p
1 + (f ′(x))2 = 2
f ′(x) = ±
√
3
145

f(x) is a continuous, piecewise diﬀerentiable function which satisﬁes f ′(x) = ±
√
3 at the points where it is diﬀerentiable.
One example is
f(x) =
√
3x
Solution 4.19
CONTINUE
146

Chapter 5
Vector Calculus
5.1
Vector Functions
Vector-valued Functions.
A vector-valued function, r(t), is a mapping r : R 7→Rn that assigns a vector to each
value of t.
r(t) = r1(t)e1 + · · · + rn(t)en.
An example of a vector-valued function is the position of an object in space as a function of time. The function is
continous at a point t = τ if
lim
t→τ r(t) = r(τ).
This occurs if and only if the component functions are continuous. The function is diﬀerentiable if
dr
dt ≡lim
∆t→0
r(t + ∆t) −r(t)
∆t
exists. This occurs if and only if the component functions are diﬀerentiable.
If r(t) represents the position of a particle at time t, then the velocity and acceleration of the particle are
dr
dt
and
d2r
dt2 ,
147

respectively. The speed of the particle is |r′(t)|.
Diﬀerentiation Formulas.
Let f(t) and g(t) be vector functions and a(t) be a scalar function. By writing out
components you can verify the diﬀerentiation formulas:
d
dt(f · g) = f ′ · g + f · g′
d
dt(f × g) = f ′ × g + f × g′
d
dt(af) = a′f + af ′
5.2
Gradient, Divergence and Curl
Scalar and Vector Fields.
A scalar ﬁeld is a function of position u(x) that assigns a scalar to each point in space.
A function that gives the temperature of a material is an example of a scalar ﬁeld. In two dimensions, you can graph a
scalar ﬁeld as a surface plot, (Figure 5.1), with the vertical axis for the value of the function.
A vector ﬁeld is a function of position u(x) that assigns a vector to each point in space. Examples of vectors ﬁelds
are functions that give the acceleration due to gravity or the velocity of a ﬂuid. You can graph a vector ﬁeld in two or
three dimension by drawing vectors at regularly spaced points. (See Figure 5.1 for a vector ﬁeld in two dimensions.)
Partial Derivatives of Scalar Fields.
Consider a scalar ﬁeld u(x). The partial derivative of u with respect to
xk is the derivative of u in which xk is considered to be a variable and the remaining arguments are considered to be
parameters. The partial derivative is denoted
∂
∂xk u(x),
∂u
∂xk or uxk and is deﬁned
∂u
∂xk
≡lim
∆x→0
u(x1, . . . , xk + ∆x, . . . , xn) −u(x1, . . . , xk, . . . , xn)
∆x
.
Partial derivatives have the same diﬀerentiation formulas as ordinary derivatives.
148

0
2
4
6
0
2
4
6
-1
-0.5
0
0.5
1
0
2
4
6
Figure 5.1: A Scalar Field and a Vector Field
Consider a scalar ﬁeld in R3, u(x, y, z). Higher derivatives of u are denoted:
uxx ≡∂2u
∂x2 ≡∂
∂x
∂u
∂x,
uxy ≡∂2u
∂x∂y ≡∂
∂x
∂u
∂y ,
uxxyz ≡
∂4u
∂x2∂y∂z ≡∂2
∂x2
∂
∂y
∂u
∂z .
149

If uxy and uyx are continuous, then
∂2u
∂x∂y = ∂2u
∂y∂x.
This is referred to as the equality of mixed partial derivatives.
Partial Derivatives of Vector Fields.
Consider a vector ﬁeld u(x). The partial derivative of u with respect to
xk is denoted
∂
∂xk u(x),
∂u
∂xk or uxk and is deﬁned
∂u
∂xk
≡lim
∆x→0
u(x1, . . . , xk + ∆x, . . . , xn) −u(x1, . . . , xk, . . . , xn)
∆x
.
Partial derivatives of vector ﬁelds have the same diﬀerentiation formulas as ordinary derivatives.
Gradient.
We introduce the vector diﬀerential operator,
∇≡
∂
∂x1
e1 + · · · +
∂
∂xn
en,
which is known as del or nabla. In R3 it is
∇≡∂
∂xi + ∂
∂yj + ∂
∂zk.
Let u(x) be a diﬀerential scalar ﬁeld. The gradient of u is,
∇u ≡∂u
∂x1
e1 + · · · + ∂u
∂xn
en,
Directional Derivative.
Suppose you are standing on some terrain.
The slope of the ground in a particular
direction is the directional derivative of the elevation in that direction. Consider a diﬀerentiable scalar ﬁeld, u(x). The
150

derivative of the function in the direction of the unit vector a is the rate of change of the function in that direction.
Thus the directional derivative, Dau, is deﬁned:
Dau(x) = lim
ϵ→0
u(x + ϵa) −u(x)
ϵ
= lim
ϵ→0
u(x1 + ϵa1, . . . , xn + ϵan) −u(x1, . . . , xn)
ϵ
= lim
ϵ→0
(u(x) + ϵa1ux1(x) + · · · + ϵanuxn(x) + O(ϵ2)) −u(x)
ϵ
= a1ux1(x) + · · · + anuxn(x)
Dau(x) = ∇u(x) · a.
Tangent to a Surface.
The gradient, ∇f, is orthogonal to the surface f(x) = 0. Consider a point ξ on the
surface. Let the diﬀerential dr = dx1e1 + · · · dxnen lie in the tangent plane at ξ. Then
df = ∂f
∂x1
dx1 + · · · + ∂f
∂xn
dxn = 0
since f(x) = 0 on the surface. Then
∇f · dr =
 ∂f
∂x1
e1 + · · · + ∂f
∂xn
en

· (dx1e1 + · · · + dxnen)
= ∂f
∂x1
dx1 + · · · + ∂f
∂xn
dxn
= 0
Thus ∇f is orthogonal to the tangent plane and hence to the surface.
Example 5.2.1 Consider the paraboloid, x2 + y2 −z = 0. We want to ﬁnd the tangent plane to the surface at the
point (1, 1, 2). The gradient is
∇f = 2xi + 2yj −k.
151

At the point (1, 1, 2) this is
∇f(1, 1, 2) = 2i + 2j −k.
We know a point on the tangent plane, (1, 1, 2), and the normal, ∇f(1, 1, 2). The equation of the plane is
∇f(1, 1, 2) · (x, y, z) = ∇f(1, 1, 2) · (1, 1, 2)
2x + 2y −z = 2
The gradient of the function f(x) = 0, ∇f(x), is in the direction of the maximum directional derivative. The
magnitude of the gradient, |∇f(x)|, is the value of the directional derivative in that direction. To derive this, note that
Daf = ∇f · a = |∇f| cos θ,
where θ is the angle between ∇f and a. Daf is maximum when θ = 0, i.e. when a is the same direction as ∇f. In
this direction, Daf = |∇f|. To use the elevation example, ∇f points in the uphill direction and |∇f| is the uphill
slope.
Example 5.2.2 Suppose that the two surfaces f(x) = 0 and g(x) = 0 intersect at the point x = ξ. What is the angle
between their tangent planes at that point? First we note that the angle between the tangent planes is by deﬁnition the
angle between their normals. These normals are in the direction of ∇f(ξ) and ∇g(ξ). (We assume these are nonzero.)
The angle, θ, between the tangent planes to the surfaces is
θ = arccos
 ∇f(ξ) · ∇g(ξ)
|∇f(ξ)| |∇g(ξ)|

.
Example 5.2.3 Let u be the distance from the origin:
u(x) = √x · x = √xixi.
In three dimensions, this is
u(x, y, z) =
p
x2 + y2 + z2.
152

The gradient of u, ∇(x), is a unit vector in the direction of x. The gradient is:
∇u(x) =

x1
√x · x, . . .
xn
√x · x

=
xiei
√xjxj
.
In three dimensions, we have
∇u(x, y, z) =
*
x
p
x2 + y2 + z2,
y
p
x2 + y2 + z2,
z
p
x2 + y2 + z2
+
.
This is a unit vector because the sum of the squared components sums to unity.
∇u · ∇u =
xiei
√xjxj
· xkek
√xlxl
xixi
xjxj
= 1
Figure 5.2 shows a plot of the vector ﬁeld of ∇u in two dimensions.
Example 5.2.4 Consider an ellipse. An implicit equation of an ellipse is
x2
a2 + y2
b2 = 1.
We can also express an ellipse as u(x, y) + v(x, y) = c where u and v are the distance from the two foci. That is, an
ellipse is the set of points such that the sum of the distances from the two foci is a constant. Let n = ∇(u + v). This
is a vector which is orthogonal to the ellipse when evaluated on the surface. Let t be a unit tangent to the surface.
Since n and t are orthogonal,
n · t = 0
(∇u + ∇v) · t = 0
∇u · t = ∇v · (−t).
153

Figure 5.2: The gradient of the distance from the origin.
u
v θ
θ
n
t
v
u
-t
θ
θ
Figure 5.3: An ellipse and rays from the foci.
Since these are unit vectors, the angle between ∇u and t is equal to the angle between ∇v and −t. In other words:
If we draw rays from the foci to a point on the ellipse, the rays make equal angles with the ellipse. If the ellipse were
154

a reﬂective surface, a wave starting at one focus would be reﬂected from the ellipse and travel to the other focus. See
Figure 6.4. This result also holds for ellipsoids, u(x, y, z) + v(x, y, z) = c.
We see that an ellipsoidal dish could be used to collect spherical waves, (waves emanating from a point). If the
dish is shaped so that the source of the waves is located at one foci and a collector is placed at the second, then any
wave starting at the source and reﬂecting oﬀthe dish will travel to the collector. See Figure 5.4.
Figure 5.4: An elliptical dish.
155

5.3
Exercises
Vector Functions
Exercise 5.1
Consider the parametric curve
r = cos
 t
2

i + sin
 t
2

j.
Calculate dr
dt and d2r
dt2 . Plot the position and some velocity and acceleration vectors.
Hint, Solution
Exercise 5.2
Let r(t) be the position of an object moving with constant speed. Show that the acceleration of the object is orthogonal
to the velocity of the object.
Hint, Solution
Vector Fields
Exercise 5.3
Consider the paraboloid x2 + y2 −z = 0. What is the angle between the two tangent planes that touch the surface at
(1, 1, 2) and (1, −1, 2)? What are the equations of the tangent planes at these points?
Hint, Solution
Exercise 5.4
Consider the paraboloid x2 + y2 −z = 0. What is the point on the paraboloid that is closest to (1, 0, 0)?
Hint, Solution
Exercise 5.5
Consider the region R deﬁned by x2 + xy + y2 ≤9. What is the volume of the solid obtained by rotating R about the
y axis?
Is this the same as the volume of the solid obtained by rotating R about the x axis? Give geometric and algebraic
explanations of this.
156

Hint, Solution
Exercise 5.6
Two cylinders of unit radius intersect at right angles as shown in Figure 5.5. What is the volume of the solid enclosed
by the cylinders?
Figure 5.5: Two cylinders intersecting.
Hint, Solution
Exercise 5.7
Consider the curve f(x) = 1/x on the interval [1 . . . ∞). Let S be the solid obtained by rotating f(x) about the x
axis. (See Figure 5.6.) Show that the length of f(x) and the lateral area of S are inﬁnite. Find the volume of S. 1
Hint, Solution
Exercise 5.8
Suppose that a deposit of oil looks like a cone in the ground as illustrated in Figure 5.7. Suppose that the oil has a
1You could ﬁll S with a ﬁnite amount of paint, but it would take an inﬁnite amount of paint to cover its surface.
157

1
2
3
4
5 -1
0
1-1
0
1
1
2
3
4
-1
0
Figure 5.6: The rotation of 1/x about the x axis.
density of 800kg/m3 and it’s vertical depth is 12m. How much work2 would it take to get the oil to the surface.
Hint, Solution
Exercise 5.9
Find the area and volume of a sphere of radius R by integrating in spherical coordinates.
Hint, Solution
2 Recall that work = force × distance and force = mass × acceleration.
158

32 m
12 m
12 m
ground
surface
Figure 5.7: The oil deposit.
5.4
Hints
Vector Functions
Hint 5.1
Plot the velocity and acceleration vectors at regular intervals along the path of motion.
Hint 5.2
If r(t) has constant speed, then |r′(t)| = c. The condition that the acceleration is orthogonal to the velocity can be
stated mathematically in terms of the dot product, r′′(t) · r′(t) = 0. Write the condition of constant speed in terms of
a dot product and go from there.
Vector Fields
Hint 5.3
The angle between two planes is the angle between the vectors orthogonal to the planes. The angle between the two
159

vectors is
θ = arccos
 ⟨2, 2, −1⟩· ⟨2, −2, −1⟩
|⟨2, 2, −1⟩||⟨2, −2, −1⟩|

The equation of a line orthogonal to a and passing through the point b is a · x = a · b.
Hint 5.4
Since the paraboloid is a diﬀerentiable surface, the normal to the surface at the closest point will be parallel to the
vector from the closest point to (1, 0, 0). We can express this using the gradient and the cross product. If (x, y, z) is
the closest point on the paraboloid, then a vector orthogonal to the surface there is ∇f = ⟨2x, 2y, −1⟩. The vector
from the surface to the point (1, 0, 0) is ⟨1 −x, −y, −z⟩. These two vectors are parallel if their cross product is zero.
Hint 5.5
CONTINUE
Hint 5.6
CONTINUE
Hint 5.7
CONTINUE
Hint 5.8
Start with the formula for the work required to move the oil to the surface. Integrate over the mass of the oil.
Work =
Z
(acceleration) (distance) d(mass)
Here (distance) is the distance of the diﬀerential of mass from the surface. The acceleration is that of gravity, g.
Hint 5.9
CONTINUE
160

5.5
Solutions
Vector Functions
Solution 5.1
The velocity is
r′ = −1
2 sin
 t
2

i + 1
2 cos
 t
2

j.
The acceleration is
r′ = −1
4 cos
 t
2

i −1
4 sin
 t
2

j.
See Figure 5.8 for plots of position, velocity and acceleration.
Figure 5.8: A Graph of Position and Velocity and of Position and Acceleration
Solution 5.2
If r(t) has constant speed, then |r′(t)| = c. The condition that the acceleration is orthogonal to the velocity can be
stated mathematically in terms of the dot product, r′′(t) · r′(t) = 0. Note that we can write the condition of constant
161

speed in terms of a dot product,
p
r′(t) · r′(t) = c,
r′(t) · r′(t) = c2.
Diﬀerentiating this equation yields,
r′′(t) · r′(t) + r′(t) · r′′(t) = 0
r′′(t) · r′(t) = 0.
This shows that the acceleration is orthogonal to the velocity.
Vector Fields
Solution 5.3
The gradient, which is orthogonal to the surface when evaluated there is ∇f = 2xi+2yj−k. 2i+2j−k and 2i−2j−k
are orthogonal to the paraboloid, (and hence the tangent planes), at the points (1, 1, 2) and (1, −1, 2), respectively.
The angle between the tangent planes is the angle between the vectors orthogonal to the planes. The angle between
the two vectors is
θ = arccos
 ⟨2, 2, −1⟩· ⟨2, −2, −1⟩
|⟨2, 2, −1⟩||⟨2, −2, −1⟩|

θ = arccos
1
9

≈1.45946.
Recall that the equation of a line orthogonal to a and passing through the point b is a · x = a · b. The equations of
the tangent planes are
⟨2, ±2, −1⟩· ⟨x, y, z⟩= ⟨2, ±2, −1⟩· ⟨1, ±1, 2⟩,
2x ± 2y −z = 2.
The paraboloid and the tangent planes are shown in Figure 5.9.
162

-1
0
1
-1
0
1
0
2
4
Figure 5.9: Paraboloid and Two Tangent Planes
Solution 5.4
Since the paraboloid is a diﬀerentiable surface, the normal to the surface at the closest point will be parallel to the
vector from the closest point to (1, 0, 0). We can express this using the gradient and the cross product. If (x, y, z) is
the closest point on the paraboloid, then a vector orthogonal to the surface there is ∇f = ⟨2x, 2y, −1⟩. The vector
from the surface to the point (1, 0, 0) is ⟨1 −x, −y, −z⟩. These two vectors are parallel if their cross product is zero,
⟨2x, 2y, −1⟩× ⟨1 −x, −y, −z⟩= ⟨−y −2yz, −1 + x + 2xz, −2y⟩= 0.
This gives us the three equations,
−y −2yz = 0,
−1 + x + 2xz = 0,
−2y = 0.
The third equation requires that y = 0. The ﬁrst equation then becomes trivial and we are left with the second equation,
−1 + x + 2xz = 0.
Substituting z = x2 + y2 into this equation yields,
2x3 + x −1 = 0.
163

The only real valued solution of this polynomial is
x = 6−2/3  9 +
√
87
2/3 −6−1/3
 9 +
√
87
1/3
≈0.589755.
Thus the closest point to (1, 0, 0) on the paraboloid is

6−2/3  9 +
√
87
2/3 −6−1/3
 9 +
√
87
1/3
, 0,
 
6−2/3  9 +
√
87
2/3 −6−1/3
 9 +
√
87
1/3
!2
≈(0.589755, 0, 0.34781).
The closest point is shown graphically in Figure 5.10.
-1 -0.5
0
0.5
1-1
-0.5
0
0.5
1
0
0.5
1
1.5
2
-1 -0.5
0
0.5
Figure 5.10: Paraboloid, Tangent Plane and Line Connecting (1, 0, 0) to Closest Point
164

Solution 5.5
We consider the region R deﬁned by x2 + xy + y2 ≤9. The boundary of the region is an ellipse. (See Figure 5.11 for
the ellipse and the solid obtained by rotating the region.) Note that in rotating the region about the y axis, only the
-3
-2
-1
1
2
3
-3
-2
-1
1
2
3
-2
0
2
-2
0
2
-2
0
2
-2
0
2
-2
0
2
Figure 5.11: The curve x2 + xy + y2 = 9.
portions in the second and fourth quadrants make a contribution. Since the solid is symmetric across the xz plane, we
will ﬁnd the volume of the top half and then double this to get the volume of the whole solid. Now we consider rotating
the region in the second quadrant about the y axis. In the equation for the ellipse, x2 + xy + y2 = 9, we solve for x.
x = 1
2

−y ±
√
3
p
12 −y2

In the second quadrant, the curve (−y −
√
3
p
12 −y2)/2 is deﬁned on y ∈[0 . . .
√
12] and the curve (−y −
√
3
p
12 −y2)/2 is deﬁned on y ∈[3 . . .
√
12].
(See Figure 5.12.)
We ﬁnd the volume obtained by rotating the
165

-3.5
-3
-2.5
-2
-1.5
-1
-0.5
0.5
1
1.5
2
2.5
3
3.5
Figure 5.12: (−y −
√
3
p
12 −y2)/2 in red and (−y +
√
3
p
12 −y2)/2 in green.
ﬁrst curve and subtract the volume from rotating the second curve.
V = 2


Z √
12
0
π
 
−y −
√
3
p
12 −y2
2
!2
dy −
Z √
12
3
π
 
−y +
√
3
p
12 −y2
2
!2
dy


V = π
2
 Z √
12
0

y +
√
3
p
12 −y2
2
dy −
Z √
12
3

−y +
√
3
p
12 −y2
2
dy
!
V = π
2
 Z √
12
0

−2y2 +
√
12y
p
12 −y2 + 36

dy −
Z √
12
3

−2y2 −
√
12y
p
12 −y2 + 36

dy
!
V = π
2
 
−2
3y3 −2
√
3
 12 −y23/2 + 36y
√
12
0
−

−2
3y3 + 2
√
3
 12 −y23/2 + 36y
√
12
3
!
V = 72π
166

Now consider the volume of the solid obtained by rotating R about the x axis? This as the same as the volume of
the solid obtained by rotating R about the y axis. Geometrically we know this because R is symmetric about the line
y = x.
Now we justify it algebraically. Consider the phrase: Rotate the region x2 + xy + y2 ≤9 about the x axis. We
formally swap x and y to obtain: Rotate the region y2 + yx + x2 ≤9 about the y axis. Which is the original problem.
Solution 5.6
We ﬁnd of the volume of the intersecting cylinders by summing the volumes of the two cylinders and then subracting the
volume of their intersection. The volume of each of the cylinders is 2π. The intersection is shown in Figure 5.13. If we
slice this solid along the plane z = const we have a square with side length 2
√
1 −z2. The volume of the intersection
of the cylinders is
Z 1
−1
4
 1 −z2
dz.
We compute the volume of the intersecting cylinders.
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
-1
-0.5
0
0.5
Figure 5.13: The intersection of the two cylinders.
167

V = 2(2π) −2
Z 1
0
4
 1 −z2
dz
V = 4π −16
3
Solution 5.7
The length of f(x) is
L =
Z ∞
1
p
1 + 1/x2 dx.
Since
p
1 + 1/x2 > 1/x, the integral diverges. The length is inﬁnite.
We ﬁnd the area of S by integrating the length of circles.
A =
Z ∞
1
2π
x dx
This integral also diverges. The area is inﬁnite.
Finally we ﬁnd the volume of S by integrating the area of disks.
V =
Z ∞
1
π
x2 dx =
h
−π
x
i∞
1 = π
Solution 5.8
First we write the formula for the work required to move the oil to the surface. We integrate over the mass of the oil.
Work =
Z
(acceleration) (distance) d(mass)
Here (distance) is the distance of the diﬀerential of mass from the surface. The acceleration is that of gravity, g. The
diﬀerential of mass can be represented an a diﬀerential of volume time the density of the oil, 800 kg/m3.
Work =
Z
800g(distance) d(volume)
168

We place the coordinate axis so that z = 0 coincides with the bottom of the cone. The oil lies between z = 0 and
z = 12. The cross sectional area of the oil deposit at a ﬁxed depth is πz2. Thus the diﬀerential of volume is π z2 dz.
This oil must me raised a distance of 24 −z.
W =
Z 12
0
800 g (24 −z) π z2 dz
W = 6912000gπ
W ≈2.13 × 108 kg m2
s2
Solution 5.9
The Jacobian in spherical coordinates is r2 sin φ.
area =
Z 2π
0
Z π
0
R2 sin φ dφ dθ
= 2πR2
Z π
0
sin φ dφ
= 2πR2[−cos φ]π
0
area = 4πR2
volume =
Z R
0
Z 2π
0
Z π
0
r2 sin φ dφ dθ dr
= 2π
Z R
0
Z π
0
r2 sin φ dφ dr
= 2π
r3
3
R
0
[−cos φ]π
0
volume = 4
3πR3
169

Part III
Functions of a Complex Variable
170

Chapter 6
Complex Numbers
I’m sorry. You have reached an imaginary number. Please rotate your phone 90 degrees and dial again.
-Message on answering machine of Cathy Vargas.
6.1
Complex Numbers
Shortcomings of Real Numbers.
When you started algebra, you learned that the quadratic equation: x2+2ax+
b = 0 has either two, one or no solutions. For example:
• x2 −3x + 2 = 0 has the two solutions x = 1 and x = 2.
• For x2 −2x + 1 = 0, x = 1 is a solution of multiplicity two.
• x2 + 1 = 0 has no solutions.
171

This is a little unsatisfactory. We can formally solve the general quadratic equation.
x2 + 2ax + b = 0
(x + a)2 = a2 −b
x = −a ±
√
a2 −b
However, the solutions are deﬁned only when the discriminant, a2 −b is positive. This is because the square root
function, √x, is a bijection from R0+ to R0+. (See Figure 6.1.)
Figure 6.1: y = √x
A New Mathematical Constant.
We cannot solve x2 = −1 because √−1 is not deﬁned. To overcome this
apparent shortcoming of the real number system, we create a new symbolic constant √−1. Note that we can express
the square root of any negative real number in terms of √−1: √−r = √−1√r. Now we can express the solutions of
x2 = −1 as x = √−1 and x = −√−1. These satisfy the equation since
 √−1
2 = −1 and
 −√−1
2 = −1.
Euler’s Notation.
Euler introduced the notation of using the letter i to denote √−1.
We will use the symbol
ı, an i without a dot, to denote √−1. This helps us distinguish it from i used as a variable or index.1 We call any
1 Electrical engineering types prefer to use or j to denote √−1.
172

number of the form ıb, b ∈R, a pure imaginary number.2
We call numbers of the form a + ıb, where a, b ∈R,
complex numbers3
The Quadratic.
Now we return to the quadratic with real coeﬃcients, x2 + 2ax + b = 0. It has the solutions
x = −a ±
√
a2 −b. The solutions are real-valued only if a2 −b ≥0. If not, then we can deﬁne solutions as complex
numbers. If the discriminant is negative, we write x = −a ± ı
√
b −a2. Thus every quadratic polynomial with real
coeﬃcients has exactly two solutions, counting multiplicities. The fundamental theorem of algebra states that an nth
degree polynomial with complex coeﬃcients has n, not necessarily distinct, complex roots. We will prove this result
later using the theory of functions of a complex variable.
Component Operations.
Consider the complex number z = x + ıy, (x, y ∈R). The real part of z is ℜ(z) = x;
the imaginary part of z is ℑ(z) = y. Two complex numbers, z1 = x1 + ıy1 and z2 = x2 + ıy2, are equal if and only if
x1 = x2 and y1 = y2. The complex conjugate4 of z = x + ıy is z ≡x −ıy. The notation z∗≡x −ıy is also used.
Field Properties.
The set of complex numbers, C, form a ﬁeld. That essentially means that we can do arithmetic
with complex numbers. We treat ı as a symbolic constant with the property that ı2 = −1. The ﬁeld of complex
numbers satisfy the following properties: (Let z, z1, z2, z3 ∈C.)
1. Closure under addition and multiplication.
z1 + z2 = (x1 + ıy1) + (x2 + ıy2)
= (x1 + x2) + ı (y1 + y2) ∈C
z1z2 = (x1 + ıy1) (x2 + ıy2)
= (x1x2 −y1y2) + ı (x1y2 + x2y1) ∈C
2 “Imaginary” is an unfortunate term. Real numbers are artiﬁcial; constructs of the mind. Real numbers are no more real than
imaginary numbers.
3 Here complex means “composed of two or more parts”, not “hard to separate, analyze, or solve”. Those who disagree have a
complex number complex.
4 Conjugate: having features in common but opposite or inverse in some particular.
173

2. Commutativity of addition and multiplication. z1 + z2 = z2 + z1. z1z2 = z2z1.
3. Associativity of addition and multiplication. (z1 + z2) + z3 = z1 + (z2 + z3). (z1z2) z3 = z1 (z2z3).
4. Distributive law. z1 (z2 + z3) = z1z2 + z1z3.
5. Identity with respect to addition and multiplication. z + 0 = z. z(1) = z.
6. Inverse with respect to addition. z + (−z) = (x + ıy) + (−x −ıy) = 0.
7. Inverse with respect to multiplication for nonzero numbers. zz−1 = 1, where
z−1 = 1
z =
1
x + ıy = x −ıy
x2 + y2 =
x
x2 + y2 −ı
y
x2 + y2
Properties of the Complex Conjugate.
Using the ﬁeld properties of complex numbers, we can derive the
following properties of the complex conjugate, z = x −ıy.
1. (z) = z,
2. z + ζ = z + ζ,
3. zζ = zζ,
4.
z
ζ

= z
ζ .
6.2
The Complex Plane
Complex Plane.
We can denote a complex number z = x + ıy as an ordered pair of real numbers (x, y). Thus we
can represent a complex number as a point in R2 where the ﬁrst component is the real part and the second component
is the imaginary part of z. This is called the complex plane or the Argand diagram. (See Figure 6.2.) A complex
number written as z = x + ıy is said to be in Cartesian form, or a + ıb form.
174

Im(z)
Re(z)
r
(x,y)
θ
Figure 6.2: The Complex Plane
Recall that there are two ways of describing a point in the complex plane: an ordered pair of coordinates (x, y) that
give the horizontal and vertical oﬀset from the origin or the distance r from the origin and the angle θ from the positive
horizontal axis. The angle θ is not unique. It is only determined up to an additive integer multiple of 2π.
Modulus.
The magnitude or modulus of a complex number is the
distance of the point from the origin. It is
deﬁned as |z| = |x + ıy| =
p
x2 + y2. Note that zz = (x + ıy)(x −ıy) = x2 + y2 = |z|2. The modulus has the
following properties.
1. |z1z2| = |z1| |z2|
2.

z1
z2
 = |z1|
|z2| for z2 ̸= 0.
3. |z1 + z2| ≤|z1| + |z2|
4. |z1 + z2| ≥||z1| −|z2||
We could prove the ﬁrst two properties by expanding in x + ıy form, but it would be fairly messy. The proofs will
become simple after polar form has been introduced. The second two properties follow from the triangle inequalities in
175

geometry. This will become apparent after the relationship between complex numbers and vectors is introduced. One
can show that
|z1z2 · · · zn| = |z1| |z2| · · · |zn|
and
|z1 + z2 + · · · + zn| ≤|z1| + |z2| + · · · + |zn|
with proof by induction.
Argument.
The argument of a complex number is the angle that the vector with tail at the origin and head at
z = x + ıy makes with the positive x-axis. The argument is denoted arg(z). Note that the argument is deﬁned for all
nonzero numbers and is only determined up to an additive integer multiple of 2π. That is, the argument of a complex
number is the set of values: {θ + 2πn | n ∈Z}. The principal argument of a complex number is that angle in the set
arg(z) which lies in the range (−π, π]. The principal argument is denoted Arg(z). We prove the following identities in
Exercise 6.10.
arg(zζ) = arg(z) + arg(ζ)
Arg(zζ) ̸= Arg(z) + Arg(ζ)
arg
 z2
= arg(z) + arg(z) ̸= 2 arg(z)
Example 6.2.1 Consider the equation |z −1 −ı| = 2. The set of points satisfying this equation is a circle of radius
2 and center at 1 + ı in the complex plane. You can see this by noting that |z −1 −ı| is the distance from the point
(1, 1). (See Figure 6.3.)
Another way to derive this is to substitute z = x + ıy into the equation.
|x + ıy −1 −ı| = 2
p
(x −1)2 + (y −1)2 = 2
(x −1)2 + (y −1)2 = 4
This is the analytic geometry equation for a circle of radius 2 centered about (1, 1).
176

-1
1
2
3
-1
1
2
3
Figure 6.3: Solution of |z −1 −ı| = 2
Example 6.2.2 Consider the curve described by
|z| + |z −2| = 4.
Note that |z| is the distance from the origin in the complex plane and |z −2| is the distance from z = 2. The equation
is
(distance from (0, 0)) + (distance from (2, 0)) = 4.
From geometry, we know that this is an ellipse with foci at (0, 0) and (2, 0), major axis 2, and minor axis
√
3. (See
Figure 6.4.)
177

-1
1
2
3
-2
-1
1
2
Figure 6.4: Solution of |z| + |z −2| = 4
We can use the substitution z = x + ıy to get the equation in algebraic form.
|z| + |z −2| = 4
|x + ıy| + |x + ıy −2| = 4
p
x2 + y2 +
p
(x −2)2 + y2 = 4
x2 + y2 = 16 −8
p
(x −2)2 + y2 + x2 −4x + 4 + y2
x −5 = −2
p
(x −2)2 + y2
x2 −10x + 25 = 4x2 −16x + 16 + 4y2
1
4(x −1)2 + 1
3y2 = 1
Thus we have the standard form for an equation describing an ellipse.
178

6.3
Polar Form
Polar Form.
A complex number written in Cartesian form, z = x + ıy, can be converted polar form, z = r(cos θ +
ı sin θ), using trigonometry. Here r = |z| is the modulus and θ = arctan(x, y) is the argument of z. The argument is
the angle between the x axis and the vector with its head at (x, y). (See Figure 6.5.) Note that θ is not unique. If
z = r(cos θ + ı sin θ) then z = r(cos(θ + 2nπ) + ı sin(θ + 2nπ)) for any n ∈Z.
Re(  )
r
Im(  )
(x,y)
r
z
θ
sinθ
z
θ
cos
r
Figure 6.5: Polar Form
The Arctangent.
Note that arctan(x, y) is not the same thing as the old arctangent that you learned about in
trigonometry arctan(x, y) is sensitive to the quadrant of the point (x, y), while arctan
  y
x

is not. For example,
arctan(1, 1) = π
4 + 2nπ
and
arctan(−1, −1) = −3π
4
+ 2nπ,
whereas
arctan
−1
−1

= arctan
1
1

= arctan(1).
Euler’s Formula.
Euler’s formula, eıθ = cos θ+ı sin θ,5 allows us to write the polar form more compactly. Expressing
the polar form in terms of the exponential function of imaginary argument makes arithmetic with complex numbers
5 See Exercise 6.17 for justiﬁcation of Euler’s formula.
179

much more convenient.
z = r(cos θ + ı sin θ) = r eıθ
The exponential of an imaginary argument has all the nice properties that we know from studying functions of a real
variable, like eıa eıb = eı(a+b). Later on we will introduce the exponential of a complex number.
Using Euler’s Formula, we can express the cosine and sine in terms of the exponential.
eıθ + e−ıθ
2
= (cos(θ) + ı sin(θ)) + (cos(−θ) + ı sin(−θ))
2
= cos(θ)
eıθ −e−ıθ
ı2
= (cos(θ) + ı sin(θ)) −(cos(−θ) + ı sin(−θ))
ı2
= sin(θ)
Arithmetic With Complex Numbers.
Note that it is convenient to add complex numbers in Cartesian form.
(x1 + ıy1) + (x2 + ıy2) = (x1 + x2) + ı (y1 + y2)
However, it is diﬃcult to multiply or divide them in Cartesian form.
(x1 + ıy1) (x2 + ıy2) = (x1x2 −y1y2) + ı (x1y2 + x2y1)
x1 + ıy1
x2 + ıy2
= (x1 + ıy1) (x2 −ıy2)
(x2 + ıy2) (x2 −ıy2) = x1x2 + y1y2
x2
2 + y2
2
+ ıx2y1 −x1y2
x2
2 + y2
2
On the other hand, it is diﬃcult to add complex numbers in polar form.
r1 eıθ1 +r2 eıθ2 = r1 (cos θ1 + ı sin θ1) + r2 (cos θ2 + ı sin θ2)
= r1 cos θ1 + r2 cos θ2 + ı (r1 sin θ1 + r2 sin θ2)
=
q
(r1 cos θ1 + r2 cos θ2)2 + (r1 sin θ1 + r2 sin θ2)2
× eı arctan(r1 cos θ1+r2 cos θ2,r1 sin θ1+r2 sin θ2)
=
q
r2
1 + r2
2 + 2 cos (θ1 −θ2) eı arctan(r1 cos θ1+r2 cos θ2,r1 sin θ1+r2 sin θ2)
180

However, it is convenient to multiply and divide them in polar form.
r1 eıθ1 r2 eıθ2 = r1r2 eı(θ1+θ2)
r1 eıθ1
r2 eıθ2 = r1
r2
eı(θ1−θ2)
Keeping this in mind will make working with complex numbers a shade or two less grungy.
Result 6.3.1 Euler’s formula is
eıθ = cos θ + ı sin θ.
We can write the cosine and sine in terms of the exponential.
cos(θ) = eıθ + e−ıθ
2
,
sin(θ) = eıθ −e−ıθ
ı2
To change between Cartesian and polar form, use the identities
r eıθ = r cos θ + ır sin θ,
x + ıy =
p
x2 + y2 eı arctan(x,y) .
Cartesian form is convenient for addition. Polar form is convenient for multiplication and
division.
Example 6.3.1 We write 5 + ı7 in polar form.
5 + ı7 =
√
74 eı arctan(5,7)
181

We write 2 eıπ/6 in Cartesian form.
2 eıπ/6 = 2 cos
π
6

+ 2ı sin
π
6

=
√
3 + ı
Example 6.3.2 We will prove the trigonometric identity
cos4 θ = 1
8 cos(4θ) + 1
2 cos(2θ) + 3
8.
We start by writing the cosine in terms of the exponential.
cos4 θ =
eıθ + e−ıθ
2
4
= 1
16
 eı4θ +4 eı2θ +6 + 4 e−ı2θ + e−ı4θ
= 1
8
eı4θ + e−ı4θ
2

+ 1
2
eı2θ + e−ı2θ
2

+ 3
8
= 1
8 cos(4θ) + 1
2 cos(2θ) + 3
8
By the deﬁnition of exponentiation, we have eınθ =
 eıθn We apply Euler’s formula to obtain a result which is useful
in deriving trigonometric identities.
cos(nθ) + ı sin(nθ) = (cos(θ) + ı sin(θ))n
Result 6.3.2 DeMoivre’s Theorem.a
cos(nθ) + ı sin(nθ) = (cos(θ) + ı sin(θ))n
aIt’s amazing what passes for a theorem these days. I would think that this would be a corollary at most.
182

Example 6.3.3 We will express cos(5θ) in terms of cos θ and sin(5θ) in terms of sin θ. We start with DeMoivre’s
theorem.
eı5θ =
 eıθ5
cos(5θ) + ı sin(5θ) = (cos θ + ı sin θ)5
=
5
0

cos5 θ + ı
5
1

cos4 θ sin θ −
5
2

cos3 θ sin2 θ −ı
5
3

cos2 θ sin3 θ
+
5
4

cos θ sin4 θ + ı
5
5

sin5 θ
=
 cos5 θ −10 cos3 θ sin2 θ + 5 cos θ sin4 θ

+ ı
 5 cos4 θ sin θ −10 cos2 θ sin3 θ + sin5 θ

Then we equate the real and imaginary parts.
cos(5θ) = cos5 θ −10 cos3 θ sin2 θ + 5 cos θ sin4 θ
sin(5θ) = 5 cos4 θ sin θ −10 cos2 θ sin3 θ + sin5 θ
Finally we use the Pythagorean identity, cos2 θ + sin2 θ = 1.
cos(5θ) = cos5 θ −10 cos3 θ
 1 −cos2 θ

+ 5 cos θ
 1 −cos2 θ
2
cos(5θ) = 16 cos5 θ −20 cos3 θ + 5 cos θ
sin(5θ) = 5
 1 −sin2 θ
2 sin θ −10
 1 −sin2 θ

sin3 θ + sin5 θ
sin(5θ) = 16 sin5 θ −20 sin3 θ + 5 sin θ
6.4
Arithmetic and Vectors
Addition.
We can represent the complex number z = x + ıy = r eıθ as a vector in Cartesian space with tail at the
origin and head at (x, y), or equivalently, the vector of length r and angle θ. With the vector representation, we can
add complex numbers by connecting the tail of one vector to the head of the other. The vector z + ζ is the diagonal
of the parallelogram deﬁned by z and ζ. (See Figure 6.6.)
183

Negation.
The negative of z = x + ıy is −z = −x −ıy. In polar form we have z = r eıθ and −z = r eı(θ+π), (more
generally, z = r eı(θ+(2n+1)π), n ∈Z. In terms of vectors, −z has the same magnitude but opposite direction as z. (See
Figure 6.6.)
Multiplication.
The product of z = r eıθ and ζ = ρ eıφ is zζ = rρ eı(θ+φ). The length of the vector zζ is the
product of the lengths of z and ζ. The angle of zζ is the sum of the angles of z and ζ. (See Figure 6.6.)
Note that arg(zζ) = arg(z) + arg(ζ). Each of these arguments has an inﬁnite number of values. If we write out
the multi-valuedness explicitly, we have
{θ + φ + 2πn : n ∈Z} = {θ + 2πn : n ∈Z} + {φ + 2πn : n ∈Z}
The same is not true of the principal argument. In general, Arg(zζ) ̸= Arg(z) + Arg(ζ). Consider the case z = ζ =
eı3π/4. Then Arg(z) = Arg(ζ) = 3π/4, however, Arg(zζ) = −π/2.
Im(  )
z
ζ
ξ
z+  =(x+  )+i(y+  )
η
ζ=ξ+ η
i
z=x+iy
Re(  )
z
Re(  )
=re
Im(  )z
z=x+iy
iθ
z
-z=-x-iy
=rei(  +  )
θ π
ζ=(xξ−yη)+i(xη+yξ)
Re(  )
Im(  )z
=r  e
ρ i(  +  )
θ φ
z
ζ=ξ+ η=ρ
i
eiφ
z=x+iy=reiθ
z
Figure 6.6: Addition, Negation and Multiplication
Multiplicative Inverse.
Assume that z is nonzero. The multiplicative inverse of z = r eıθ is 1
z = 1
r e−ıθ. The
length of 1
z is the multiplicative inverse of the length of z. The angle of 1
z is the negative of the angle of z. (See
Figure 6.7.)
184

Division.
Assume that ζ is nonzero. The quotient of z = r eıθ and ζ = ρ eıφ is z
ζ = r
ρ eı(θ−φ). The length of the
vector z
ζ is the quotient of the lengths of z and ζ. The angle of z
ζ is the diﬀerence of the angles of z and ζ. (See
Figure 6.7.)
Complex Conjugate.
The complex conjugate of z = x + ıy = r eıθ is z = x −ıy = r e−ıθ. z is the mirror image
of z, reﬂected across the x axis. In other words, z has the same magnitude as z and the angle of z is the negative of
the angle of z. (See Figure 6.7.)
=−e
Re(  )
Im(  )
-
z
z=reiθ
z
1z
1r
-i θ
=- e
Re(  )
Im(  )
ζ=ρ
z=re
-
z
eiφ
iθ
z
z
ζ
ρ
i
r
(θ−φ)
Re(  )
Im(  )
z=x+iy=re
z=x-iy=re
z
iθ
z
-
-i θ
Figure 6.7: Multiplicative Inverse, Division and Complex Conjugate
6.5
Integer Exponents
Consider the product (a + b)n, n ∈Z. If we know arctan(a, b) then it will be most convenient to expand the product
working in polar form. If not, we can write n in base 2 to eﬃciently do the multiplications.
185

Example 6.5.1 Suppose that we want to write
 √
3 + ı
20 in Cartesian form.6 We can do the multiplication directly.
Note that 20 is 10100 in base 2. That is, 20 = 24 + 22. We ﬁrst calculate the powers of the form
 √
3 + ı
2n
by
successive squaring.
√
3 + ı
2
= 2 + ı2
√
3
√
3 + ı
4
= −8 + ı8
√
3
√
3 + ı
8
= −128 −ı128
√
3
√
3 + ı
16
= −32768 + ı32768
√
3
Next we multiply
 √
3 + ı
4 and
 √
3 + ı
16 to obtain the answer.
√
3 + ı
20
=

−32768 + ı32768
√
3
 
−8 + ı8
√
3

= −524288 −ı524288
√
3
Since we know that arctan
 √
3, 1

= π/6, it is easiest to do this problem by ﬁrst changing to modulus-argument
form.
√
3 + ı
20
=
 r√
3
2
+ 12 eı arctan(
√
3,1)
!20
=
 2 eıπ/620
= 220 eı4π/3
= 1048576
 
−1
2 −ı
√
3
2
!
= −524288 −ı524288
√
3
6No, I have no idea why we would want to do that. Just humor me. If you pretend that you’re interested, I’ll do the same. Believe
me, expressing your real feelings here isn’t going to do anyone any good.
186

Example 6.5.2 Consider (5 + ı7)11. We will do the exponentiation in polar form and write the result in Cartesian
form.
(5 + ı7)11 =
√
74 eı arctan(5,7)11
= 745√
74(cos(11 arctan(5, 7)) + ı sin(11 arctan(5, 7)))
= 2219006624
√
74 cos(11 arctan(5, 7)) + ı2219006624
√
74 sin(11 arctan(5, 7))
The result is correct, but not very satisfying. This expression could be simpliﬁed. You could evaluate the trigonometric
functions with some fairly messy trigonometric identities. This would take much more work than directly multiplying
(5 + ı7)11.
6.6
Rational Exponents
In this section we consider complex numbers with rational exponents, zp/q, where p/q is a rational number. First we
consider unity raised to the 1/n power. We deﬁne 11/n as the set of numbers {z} such that zn = 1.
11/n = {z | zn = 1}
We can ﬁnd these values by writing z in modulus-argument form.
zn = 1
rn eınθ = 1
rn = 1
nθ = 0
mod 2π
r = 1
θ = 2πk for k ∈Z
11/n =
eı2πk/n | k ∈Z
	
There are only n distinct values as a result of the 2π periodicity of eıθ. eı2π = eı0.
11/n =
eı2πk/n | k = 0, . . . , n −1
	
187

These values are equally spaced points on the unit circle in the complex plane.
Example 6.6.1 11/6 has the 6 values,
eı0, eıπ/3, eı2π/3, eıπ, eı4π/3, eı5π/3	
.
In Cartesian form this is
(
1, 1 + ı
√
3
2
, −1 + ı
√
3
2
, −1, −1 −ı
√
3
2
, 1 −ı
√
3
2
)
.
The sixth roots of unity are plotted in Figure 6.8.
-1
1
-1
1
Figure 6.8: The Sixth Roots of Unity.
The nth roots of the complex number c = α eıβ are the set of numbers z = r eıθ such that
zn = c = α eıβ
rn eınθ = α eıβ
r =
n√α
nθ = β
mod 2π
r =
n√α
θ = (β + 2πk)/n for k = 0, . . . , n −1.
188

Thus
c1/n =

n√α eı(β+2πk)/n | k = 0, . . . , n −1
	
=
n
np
|c| eı(Arg(c)+2πk)/n | k = 0, . . . , n −1
o
Principal Roots.
The principal nth root is denoted
n√z ≡
n√z eı Arg(z)/n .
Thus the principal root has the property
−π/n < Arg
 n√z

≤π/n.
This is consistent with the notation from functions of a real variable:
n√x denotes the positive nth root of a positive
real number. We adopt the convention that z1/n denotes the nth roots of z, which is a set of n numbers and
n√z is
the principal nth root of z, which is a single number. The nth roots of z are the principal nth root of z times the nth
roots of unity.
z1/n =

n√r eı(Arg(z)+2πk)/n | k = 0, . . . , n −1
	
z1/n =

n√z eı2πk/n | k = 0, . . . , n −1
	
z1/n =
n√z11/n
Rational Exponents.
We interpret zp/q to mean z(p/q). That is, we ﬁrst simplify the exponent, i.e. reduce the
fraction, before carrying out the exponentiation. Therefore z2/4 = z1/2 and z10/5 = z2. If p/q is a reduced fraction, (p
and q are relatively prime, in other words, they have no common factors), then
zp/q ≡(zp)1/q .
Thus zp/q is a set of q values. Note that for an un-reduced fraction r/s,
(zr)1/s ̸=
 z1/sr .
The former expression is a set of s values while the latter is a set of no more that s values. For instance, (12)1/2 =
11/2 = ±1 and
 11/22 = (±1)2 = 1.
189

Example 6.6.2 Consider 21/5, (1 + ı)1/3 and (2 + ı)5/6.
21/5 =
5√
2 eı2πk/5,
for k = 0, 1, 2, 3, 4
(1 + ı)1/3 =
√
2 eıπ/41/3
=
6√
2 eıπ/12 eı2πk/3,
for k = 0, 1, 2
(2 + ı)5/6 =
√
5 eı Arctan(2,1)5/6
=
√
55 eı5 Arctan(2,1)1/6
=
12√
55 eı 5
6 Arctan(2,1) eıπk/3,
for k = 0, 1, 2, 3, 4, 5
Example 6.6.3 We ﬁnd the roots of z5 + 4.
(−4)1/5 = (4 eıπ)1/5
=
5√
4 eıπ(1+2k)/5,
for k = 0, 1, 2, 3, 4
190

6.7
Exercises
Complex Numbers
Exercise 6.1
If z = x + ıy, write the following in the form a + ıb:
1. (1 + ı2)7
2.
1
zz
3.
ız + z
(3 + ı)9
Hint, Solution
Exercise 6.2
Verify that:
1. 1 + ı2
3 −ı4 + 2 −ı
ı5
= −2
5
2. (1 −ı)4 = −4
Hint, Solution
Exercise 6.3
Write the following complex numbers in the form a + ıb.
1.

1 + ı
√
3
−10
2. (11 + ı4)2
Hint, Solution
191

Exercise 6.4
Write the following complex numbers in the form a + ıb
1.

2 + ı
ı6 −(1 −ı2)
2
2. (1 −ı)7
Hint, Solution
Exercise 6.5
If z = x + ıy, write the following in the form u(x, y) + ıv(x, y).
1.
z
z

2. z + ı2
2 −ız
Hint, Solution
Exercise 6.6
Quaternions are sometimes used as a generalization of complex numbers. A quaternion u may be deﬁned as
u = u0 + ıu1 + u2 + ku3
where u0, u1, u2 and u3 are real numbers and ı, and k are objects which satisfy
ı2 = 2 = k2 = −1,
ı= k,
ı = −k
and the usual associative and distributive laws. Show that for any quaternions u, w there exists a quaternion v such
that
uv = w
except for the case u0 = u1 = u2 = u3.
Hint, Solution
192

Exercise 6.7
Let α ̸= 0, β ̸= 0 be two complex numbers. Show that α = tβ for some real number t (i.e. the vectors deﬁned by α
and β are parallel) if and only if ℑ
 αβ

= 0.
Hint, Solution
The Complex Plane
Exercise 6.8
Find and depict all values of
1. (1 + ı)1/3
2. ı1/4
Identify the principal root.
Hint, Solution
Exercise 6.9
Sketch the regions of the complex plane:
1. |ℜ(z)| + 2|ℑ(z)| ≤1
2. 1 ≤|z −ı| ≤2
3. |z −ı| ≤|z + ı|
Hint, Solution
Exercise 6.10
Prove the following identities.
1. arg(zζ) = arg(z) + arg(ζ)
2. Arg(zζ) ̸= Arg(z) + Arg(ζ)
193

3. arg (z2) = arg(z) + arg(z) ̸= 2 arg(z)
Hint, Solution
Exercise 6.11
Show, both by geometric and algebraic arguments, that for complex numbers z1 and z2 the inequalities
||z1| −|z2|| ≤|z1 + z2| ≤|z1| + |z2|
hold.
Hint, Solution
Exercise 6.12
Find all the values of
1. (−1)−3/4
2. 81/6
and show them graphically.
Hint, Solution
Exercise 6.13
Find all values of
1. (−1)−1/4
2. 161/8
and show them graphically.
Hint, Solution
Exercise 6.14
Sketch the regions or curves described by
194

1. 1 < |z −ı2| < 2
2. |ℜ(z)| + 5|ℑ(z)| = 1
3. |z −ı| = |z + ı|
Hint, Solution
Exercise 6.15
Sketch the regions or curves described by
1. |z −1 + ı| ≤1
2. ℜ(z) −ℑ(z) = 5
3. |z −ı| + |z + ı| = 1
Hint, Solution
Exercise 6.16
Solve the equation
| eıθ −1| = 2
for θ (0 ≤θ ≤π) and verify the solution geometrically.
Hint, Solution
Polar Form
Exercise 6.17
Show that Euler’s formula, eıθ = cos θ + ı sin θ, is formally consistent with the standard Taylor series expansions for the
real functions ex, cos x and sin x. Consider the Taylor series of ex about x = 0 to be the deﬁnition of the exponential
function for complex argument.
Hint, Solution
195

Exercise 6.18
Use de Moivre’s formula to derive the trigonometric identity
cos(3θ) = cos3(θ) −3 cos(θ) sin2(θ).
Hint, Solution
Exercise 6.19
Establish the formula
1 + z + z2 + · · · + zn = 1 −zn+1
1 −z
,
(z ̸= 1),
for the sum of a ﬁnite geometric series; then derive the formulas
1. 1 + cos(θ) + cos(2θ) + · · · + cos(nθ) = 1
2 + sin((n + 1/2))
2 sin(θ/2)
2. sin(θ) + sin(2θ) + · · · + sin(nθ) = 1
2 cot θ
2 −cos((n + 1/2))
2 sin(θ/2)
where 0 < θ < 2π.
Hint, Solution
Arithmetic and Vectors
Exercise 6.20
Prove |z1z2| = |z1||z2| and
 z1
z2
 = |z1|
|z2| using polar form.
Hint, Solution
Exercise 6.21
Prove that
|z + ζ|2 + |z −ζ|2 = 2
 |z|2 + |ζ|2
.
Interpret this geometrically.
Hint, Solution
196

Integer Exponents
Exercise 6.22
Write (1 + ı)10 in Cartesian form with the following two methods:
1. Just do the multiplication. If it takes you more than four multiplications, you suck.
2. Do the multiplication in polar form.
Hint, Solution
Rational Exponents
Exercise 6.23
Show that each of the numbers z = −a + (a2 −b)1/2 satisﬁes the equation z2 + 2az + b = 0.
Hint, Solution
197

6.8
Hints
Complex Numbers
Hint 6.1
Hint 6.2
Hint 6.3
Hint 6.4
Hint 6.5
Hint 6.6
Hint 6.7
The Complex Plane
Hint 6.8
Hint 6.9
198

Hint 6.10
Write the multivaluedness explicitly.
Hint 6.11
Consider a triangle with vertices at 0, z1 and z1 + z2.
Hint 6.12
Hint 6.13
Hint 6.14
Hint 6.15
Hint 6.16
Polar Form
Hint 6.17
Find the Taylor series of eıθ, cos θ and sin θ. Note that ı2n = (−1)n.
Hint 6.18
Hint 6.19
Arithmetic and Vectors
199

Hint 6.20
| eıθ | = 1.
Hint 6.21
Consider the parallelogram deﬁned by z and ζ.
Integer Exponents
Hint 6.22
For the ﬁrst part,
(1 + ı)10 =
 (1 + ı)222
(1 + ı)2.
Rational Exponents
Hint 6.23
Substitite the numbers into the equation.
200

6.9
Solutions
Complex Numbers
Solution 6.1
1. We can do the exponentiation by directly multiplying.
(1 + ı2)7 = (1 + ı2)(1 + ı2)2(1 + ı2)4
= (1 + ı2)(−3 + ı4)(−3 + ı4)2
= (11 −ı2)(−7 −ı24)
= 29 + ı278
We can also do the problem using De Moivre’s Theorem.
(1 + ı2)7 =
√
5 eı arctan(1,2)7
= 125
√
5 eı7 arctan(1,2)
= 125
√
5 cos(7 arctan(1, 2)) + ı125
√
5 sin(7 arctan(1, 2))
2.
1
zz =
1
(x −ıy)2
=
1
(x −ıy)2
(x + ıy)2
(x + ıy)2
= (x + ıy)2
(x2 + y2)2
=
x2 −y2
(x2 + y2)2 + ı
2xy
(x2 + y2)2
201

3. We can evaluate the expression using De Moivre’s Theorem.
ız + z
(3 + ı)9 = (−y + ıx + x −ıy)(3 + ı)−9
= (1 + ı)(x −y)
√
10 eı arctan(3,1)−9
= (1 + ı)(x −y)
1
10000
√
10
e−ı9 arctan(3,1)
= (1 + ı)(x −y)
10000
√
10
(cos(9 arctan(3, 1)) −ı sin(9 arctan(3, 1)))
=
(x −y)
10000
√
10 (cos(9 arctan(3, 1)) + sin(9 arctan(3, 1)))
+ ı (x −y)
10000
√
10 (cos(9 arctan(3, 1)) −sin(9 arctan(3, 1)))
202

We can also do this problem by directly multiplying but it’s a little grungy.
ız + z
(3 + ı)9 = (−y + ıx + x −ıy)(3 −ı)9
109
=
(1 + ı)(x −y)(3 −ı)

((3 −ı)2)22
109
= (1 + ı)(x −y)(3 −ı)
 (8 −ı6)22
109
= (1 + ı)(x −y)(3 −ı)(28 −ı96)2
109
= (1 + ı)(x −y)(3 −ı)(−8432 −ı5376)
109
= (x −y)(−22976 −ı38368)
109
= 359(y −x)
15625000 + ı1199(y −x)
31250000
Solution 6.2
1.
1 + ı2
3 −ı4 + 2 −ı
ı5
= 1 + ı2
3 −ı4
3 + ı4
3 + ı4 + 2 −ı
ı5
−ı
−ı
= −5 + ı10
25
+ −1 −ı2
5
= −2
5
2.
(1 −ı)4 = (−ı2)2 = −4
203

Solution 6.3
1. First we do the multiplication in Cartesian form.

1 + ı
√
3
−10
=

1 + ı
√
3
2 
1 + ı
√
3
8−1
=

−2 + ı2
√
3
 
−2 + ı2
√
3
4−1
=

−2 + ı2
√
3
 
−8 −ı8
√
3
2−1
=

−2 + ı2
√
3
 
−128 + ı128
√
3
−1
=

−512 −ı512
√
3
−1
=
1
512
−1
1 + ı
√
3
=
1
512
−1
1 + ı
√
3
1 −ı
√
3
1 −ı
√
3
= −
1
2048 + ı
√
3
2048
204

Now we do the multiplication in modulus-argument, (polar), form.

1 + ı
√
3
−10
=
 2 eıπ/3−10
= 2−10 e−ı10π/3
=
1
1024

cos

−10π
3

+ ı sin

−10π
3

=
1
1024

cos
4π
3

−ı sin
4π
3

=
1
1024
 
−1
2 + ı
√
3
2
!
= −
1
2048 + ı
√
3
2048
2.
(11 + ı4)2 = 105 + ı88
Solution 6.4
1.

2 + ı
ı6 −(1 −ı2)
2
=
 2 + ı
−1 + ı8
2
=
3 + ı4
−63 −ı16
=
3 + ı4
−63 −ı16
−63 + ı16
−63 + ı16
= −253
4225 −ı 204
4225
205

2.
(1 −ı)7 =
 (1 −ı)22 (1 −ı)2(1 −ı)
= (−ı2)2(−ı2)(1 −ı)
= (−4)(−2 −ı2)
= 8 + ı8
Solution 6.5
1.
z
z

=
x + ıy
x + ıy

=
x −ıy
x + ıy

= x + ıy
x −ıy
= x + ıy
x −ıy
x + ıy
x + ıy
= x2 −y2
x2 + y2 + ı
2xy
x2 + y2
206

2.
z + ı2
2 −ız = x + ıy + ı2
2 −ı(x −ıy)
= x + ı(y + 2)
2 −y −ıx
= x + ı(y + 2)
2 −y −ıx
2 −y + ıx
2 −y + ıx
= x(2 −y) −(y + 2)x
(2 −y)2 + x2
+ ıx2 + (y + 2)(2 −y)
(2 −y)2 + x2
=
−2xy
(2 −y)2 + x2 + ı 4 + x2 −y2
(2 −y)2 + x2
Solution 6.6
Method 1. We expand the equation uv = w in its components.
uv = w
(u0 + ıu1 + u2 + ku3) (v0 + ıv1 + v2 + kv3) = w0 + ıw1 + w2 + kw3
(u0v0 −u1v1 −u2v2 −u3v3) + ı (u1v0 + u0v1 −u3v2 + u2v3) + (u2v0 + u3v1 + u0v2 −u1v3)
+ k (u3v0 −u2v1 + u1v2 + u0v3) = w0 + ıw1 + w2 + kw3
We can write this as a matrix equation.




u0
−u1
−u2
−u3
u1
u0
−u3
u2
u2
u3
u0
−u1
u3
−u2
u1
u0








v0
v1
v2
v3



=




w0
w1
w2
w3




This linear system of equations has a unique solution for v if and only if the determinant of the matrix is nonzero. The
determinant of the matrix is (u2
0 + u2
1 + u2
2 + u2
3)2. This is zero if and only if u0 = u1 = u2 = u3 = 0. Thus there
207

exists a unique v such that uv = w if u is nonzero. This v is
v =
 (u0w0 + u1w1 + u2w2 + u3w3) + ı (−u1w0 + u0w1 + u3w2 −u2w3) + (−u2w0 −u3w1 + u0w2 + u1w3)
+ k (−u3w0 + u2w1 −u1w2 + u0w3)

/
 u2
0 + u2
1 + u2
2 + u2
3

Method 2. Note that uu is a real number.
uu = (u0 −ıu1 −u2 −ku3) (u0 + ıu1 + u2 + ku3)
=
 u2
0 + u2
1 + u2
2 + u2
3

+ ı (u0u1 −u1u0 −u2u3 + u3u2)
+ (u0u2 + u1u3 −u2u0 −u3u1) + k (u0u3 −u1u2 + u2u1 −u3u0)
=
 u2
0 + u2
1 + u2
2 + u2
3

uu = 0 only if u = 0. We solve for v by multiplying by the conjugate of u and dividing by uu.
uv = w
uuv = uw
v = uw
uu
v = (u0 −ıu1 −u2 −ku3) (w0 + ıw1 + w2 + kw3)
u2
0 + u2
1 + u2
2 + u2
3
v =
 (u0w0 + u1w1 + u2w2 + u3w3) + ı (−u1w0 + u0w1 + u3w2 −u2w3) + (−u2w0 −u3w1 + u0w2 + u1w3)
+ k (−u3w0 + u2w1 −u1w2 + u0w3)

/
 u2
0 + u2
1 + u2
2 + u2
3

Solution 6.7
If α = tβ, then αβ = t|β|2, which is a real number. Hence ℑ
 αβ

= 0.
Now assume that ℑ
 αβ

= 0. This implies that αβ = r for some r ∈R. We multiply by β and simplify.
α|β|2 = rβ
α =
r
|β|2β
By taking t =
r
|β|2 We see that α = tβ for some real number t.
208

The Complex Plane
Solution 6.8
1.
(1 + ı)1/3 =
√
2 eıπ/41/3
=
6√
2 eıπ/12 11/3
=
6√
2 eıπ/12 eı2πk/3,
k = 0, 1, 2
=
n
6√
2 eıπ/12,
6√
2 eı3π/4,
6√
2 eı17π/12o
The principal root is
3√
1 + ı =
6√
2 eıπ/12 .
The roots are depicted in Figure 6.9.
-1
1
-1
1
Figure 6.9: (1 + ı)1/3
209

2.
ı1/4 =
 eıπ/21/4
= eıπ/8 11/4
= eıπ/8 eı2πk/4,
k = 0, 1, 2, 3
=
eıπ/8, eı5π/8, eı9π/8, eı13π/8	
The principal root is
4√ı = eıπ/8 .
The roots are depicted in Figure 6.10.
-1
1
-1
1
Figure 6.10: ı1/4
Solution 6.9
1.
|ℜ(z)| + 2|ℑ(z)| ≤1
|x| + 2|y| ≤1
210

In the ﬁrst quadrant, this is the triangle below the line y = (1−x)/2. We reﬂect this triangle across the coordinate
axes to obtain triangles in the other quadrants. Explicitly, we have the set of points: {z = x + ıy | −1 ≤x ≤
1 ∧|y| ≤(1 −|x|)/2}. See Figure 6.11.
-1
1
-1
1
Figure 6.11: |ℜ(z)| + 2|ℑ(z)| ≤1
2. |z −ı| is the distance from the point ı in the complex plane. Thus 1 < |z −ı| < 2 is an annulus centered at
z = ı between the radii 1 and 2. See Figure 6.12.
3. The points which are closer to z = ı than z = −ı are those points in the upper half plane. See Figure 6.13.
Solution 6.10
Let z = r eıθ and ζ = ρ eıϑ.
1.
arg(zζ) = arg(z) + arg(ζ)
arg
 rρ eı(θ+ϑ)
= {θ + 2πm} + {ϑ + 2πn}
{θ + ϑ + 2πk} = {θ + ϑ + 2πm}
211

-3 -2 -1
1
2
3
-2
-1
1
2
3
4
Figure 6.12: 1 < |z −ı| < 2
-1
1
-1
1
Figure 6.13: The upper half plane.
2.
Arg(zζ) ̸= Arg(z) + Arg(ζ)
212

Consider z = ζ = −1. Arg(z) = Arg(ζ) = π, however Arg(zζ) = Arg(1) = 0. The identity becomes 0 ̸= 2π.
3.
arg
 z2
= arg(z) + arg(z) ̸= 2 arg(z)
arg
 r2 eı2θ
= {θ + 2πk} + {θ + 2πm} ̸= 2{θ + 2πn}
{2θ + 2πk} = {2θ + 2πm} ̸= {2θ + 4πn}
Solution 6.11
Consider a triangle in the complex plane with vertices at 0, z1 and z1 + z2. (See Figure 6.14.)
z
|z |
|z  |
|z +z  |
1
1
1
2
2
z +z
1
2
Figure 6.14: Triangle Inequality
The lengths of the sides of the triangle are |z1|, |z2| and |z1 + z2| The second inequality shows that one side of the
triangle must be less than or equal to the sum of the other two sides.
|z1 + z2| ≤|z1| + |z2|
The ﬁrst inequality shows that the length of one side of the triangle must be greater than or equal to the diﬀerence in
the length of the other two sides.
|z1 + z2| ≥||z1| −|z2||
213

Now we prove the inequalities algebraically. We will reduce the inequality to an identity. Let z1 = r1 eıθ1, z2 = r2 eıθ2.
||z1| −|z2|| ≤|z1 + z2| ≤|z1| + |z2|
|r1 −r2| ≤|r1 eıθ1 +r2 eıθ2 | ≤r1 + r2
(r1 −r2)2 ≤
 r1 eıθ1 +r2 eıθ2  r1 e−ıθ1 +r2 e−ıθ2
≤(r1 + r2)2
r2
1 + r2
2 −2r1r2 ≤r2
1 + r2
2 + r1r2 eı(θ1−θ2) +r1r2 eı(−θ1+θ2) ≤r2
1 + r2
2 + 2r1r2
−2r1r2 ≤2r1r2 cos (θ1 −θ2) ≤2r1r2
−1 ≤cos (θ1 −θ2) ≤1
Solution 6.12
1.
(−1)−3/4 =
 (−1)−31/4
= (−1)1/4
= (eıπ)1/4
= eıπ/4 11/4
= eıπ/4 eıkπ/2,
k = 0, 1, 2, 3
=
eıπ/4, eı3π/4, eı5π/4, eı7π/4	
=
1 + ı
√
2 , −1 + ı
√
2
, −1 −ı
√
2
, 1 −ı
√
2

See Figure 6.15.
214

-1
1
-1
1
Figure 6.15: (−1)−3/4
2.
81/6 =
6√
811/6
=
√
2 eıkπ/3,
k = 0, 1, 2, 3, 4, 5
=
n√
2,
√
2 eıπ/3,
√
2 eı2π/3,
√
2 eıπ,
√
2 eı4π/3,
√
2 eı5π/3o
=
(
√
2, 1 + ı
√
3
√
2
, −1 + ı
√
3
√
2
, −
√
2, −1 −ı
√
3
√
2
, 1 −ı
√
3
√
2
)
See Figure 6.16.
215

-2
-1
1
2
-2
-1
1
2
Figure 6.16: 81/6
Solution 6.13
1.
(−1)−1/4 = ((−1)−1)1/4
= (−1)1/4
= (eıπ)1/4
= eıπ/4 11/4
= eıπ/4 eıkπ/2,
k = 0, 1, 2, 3
=
eıπ/4, eı3π/4, eı5π/4, eı7π/4	
=
1 + ı
√
2 , −1 + ı
√
2
, −1 −ı
√
2
, 1 −ı
√
2

See Figure 6.17.
216

-1
1
-1
1
Figure 6.17: (−1)−1/4
2.
161/8 =
8√
1611/8
=
√
2 eıkπ/4,
k = 0, 1, 2, 3, 4, 5, 6, 7
=
n√
2,
√
2 eıπ/4,
√
2 eıπ/2,
√
2 eı3π/4,
√
2 eıπ,
√
2 eı5π/4,
√
2 eı3π/2,
√
2 eı7π/4o
=
n√
2, 1 + ı, ı
√
2, −1 + ı, −
√
2, −1 −ı, −ı
√
2, 1 −ı
o
See Figure 6.18.
Solution 6.14
1. |z −ı2| is the distance from the point ı2 in the complex plane. Thus 1 < |z −ı2| < 2 is an annulus. See
Figure 6.19.
217

-1
1
-1
1
Figure 6.18: 16−1/8
-3 -2 -1
1
2
3
-1
1
2
3
4
5
Figure 6.19: 1 < |z −ı2| < 2
218

2.
|ℜ(z)| + 5|ℑ(z)| = 1
|x| + 5|y| = 1
In the ﬁrst quadrant this is the line y = (1 −x)/5. We reﬂect this line segment across the coordinate axes to
obtain line segments in the other quadrants. Explicitly, we have the set of points: {z = x + ıy | −1 < x <
1 ∧y = ±(1 −|x|)/5}. See Figure 6.20.
-1
1
-0.4
-0.2
0.2
0.4
Figure 6.20: |ℜ(z)| + 5|ℑ(z)| = 1
3. The set of points equidistant from ı and −ı is the real axis. See Figure 6.21.
Solution 6.15
1. |z −1 + ı| is the distance from the point (1 −ı). Thus |z −1 + ı| ≤1 is the disk of unit radius centered at
(1 −ı). See Figure 6.22.
219

-1
1
-1
1
Figure 6.21: |z −ı| = |z + ı|
-1
1
2
3
-3
-2
-1
1
Figure 6.22: |z −1 + ı| < 1
220

2.
ℜ(z) −ℑ(z) = 5
x −y = 5
y = x −5
See Figure 6.23.
-10
-5
5
10
-15
-10
-5
5
Figure 6.23: ℜ(z) −ℑ(z) = 5
3. Since |z −ı| + |z + ı| ≥2, there are no solutions of |z −ı| + |z + ı| = 1.
221

Solution 6.16
| eıθ −1| = 2
 eıθ −1
  e−ıθ −1

= 4
1 −eıθ −e−ıθ +1 = 4
−2 cos(θ) = 2
θ = π
eıθ | 0 ≤θ ≤π
	
is a unit semi-circle in the upper half of the complex plane from 1 to −1. The only point on this
semi-circle that is a distance 2 from the point 1 is the point −1, which corresponds to θ = π.
Polar Form
Solution 6.17
We recall the Taylor series expansion of ex about x = 0.
ex =
∞
X
n=0
xn
n! .
We take this as the deﬁnition of the exponential function for complex argument.
eıθ =
∞
X
n=0
(ıθ)n
n!
=
∞
X
n=0
ın
n!θn
=
∞
X
n=0
(−1)n
(2n)! θ2n + ı
∞
X
n=0
(−1)n
(2n + 1)!θ2n+1
222

We compare this expression to the Taylor series for the sine and cosine.
cos θ =
∞
X
n=0
(−1)n
(2n)! θ2n,
sin θ =
∞
X
n=0
(−1)n
(2n + 1)!θ2n+1,
Thus eıθ and cos θ + ı sin θ have the same Taylor series expansions about θ = 0.
eıθ = cos θ + ı sin θ
Solution 6.18
cos(3θ) + ı sin(3θ) = (cos(θ) + ı sin(θ))3
cos(3θ) + ı sin(3θ) = cos3(θ) + ı3 cos2(θ) sin(θ) −3 cos(θ) sin2(θ) −ı sin3(θ)
We equate the real parts of the equation.
cos(3θ) = cos3(θ) −3 cos(θ) sin2(θ)
Solution 6.19
Deﬁne the partial sum,
Sn(z) =
n
X
k=0
zk.
Now consider (1 −z)Sn(z).
(1 −z)Sn(z) = (1 −z)
n
X
k=0
zk
(1 −z)Sn(z) =
n
X
k=0
zk −
n+1
X
k=1
zk
(1 −z)Sn(z) = 1 −zn+1
223

We divide by 1 −z. Note that 1 −z is nonzero.
Sn(z) = 1 −zn+1
1 −z
1 + z + z2 + · · · + zn = 1 −zn+1
1 −z
,
(z ̸= 1)
Now consider z = eıθ where 0 < θ < 2π so that z is not unity.
n
X
k=0
 eıθk = 1 −
 eıθn+1
1 −eıθ
n
X
k=0
eıkθ = 1 −eı(n+1)θ
1 −eıθ
In order to get sin(θ/2) in the denominator, we multiply top and bottom by e−ıθ/2.
n
X
k=0
(cos(kθ) + ı sin(kθ)) = e−ıθ/2 −eı(n+1/2)θ
e−ıθ/2 −eıθ/2
n
X
k=0
cos(kθ) + ı
n
X
k=0
sin(kθ) = cos(θ/2) −ı sin(θ/2) −cos((n + 1/2)θ) −ı sin((n + 1/2)θ)
−2ı sin(θ/2)
n
X
k=0
cos(kθ) + ı
n
X
k=1
sin(kθ) = 1
2 + sin((n + 1/2)θ)
sin(θ/2)
+ ı
1
2 cot(θ/2) −cos((n + 1/2)θ)
sin(θ/2)

1. We take the real and imaginary part of this to obtain the identities.
n
X
k=0
cos(kθ) = 1
2 + sin((n + 1/2)θ)
2 sin(θ/2)
224

2.
n
X
k=1
sin(kθ) = 1
2 cot(θ/2) −cos((n + 1/2)θ)
2 sin(θ/2)
Arithmetic and Vectors
Solution 6.20
|z1z2| = |r1 eıθ1 r2 eıθ2 |
= |r1r2 eı(θ1+θ2) |
= |r1r2|
= |r1||r2|
= |z1||z2|

z1
z2
 =

r1 eıθ1
r2 eıθ2

=

r1
r2
eı(θ1−θ2)

=

r1
r2

= |r1|
|r2|
= |z1|
|z2|
225

Solution 6.21
|z + ζ|2 + |z −ζ|2 = (z + ζ)
 z + ζ

+ (z −ζ)
 z −ζ

= zz + zζ + ζz + ζζ + zz −zζ −ζz + ζζ
= 2
 |z|2 + |ζ|2
Consider the parallelogram deﬁned by the vectors z and ζ. The lengths of the sides are z and ζ and the lengths of
the diagonals are z + ζ and z −ζ. We know from geometry that the sum of the squared lengths of the diagonals of a
parallelogram is equal to the sum of the squared lengths of the four sides. (See Figure 6.24.)
z+
z-
z
ζ
ζ
ζ
Figure 6.24: The parallelogram deﬁned by z and ζ.
Integer Exponents
226

Solution 6.22
1.
(1 + ı)10 =
 (1 + ı)222
(1 + ı)2
=
 (ı2)22 (ı2)
= (−4)2 (ı2)
= 16(ı2)
= ı32
2.
(1 + ı)10 =
√
2 eıπ/410
=
√
2
10
eı10π/4
= 32 eıπ/2
= ı32
Rational Exponents
Solution 6.23
We substitite the numbers into the equation to obtain an identity.
z2 + 2az + b = 0

−a +
 a2 −b
1/22
+ 2a

−a +
 a2 −b
1/2
+ b = 0
a2 −2a
 a2 −b
1/2 + a2 −b −2a2 + 2a
 a2 −b
1/2 + b = 0
0 = 0
227

Chapter 7
Functions of a Complex Variable
If brute force isn’t working, you’re not using enough of it.
-Tim Mauch
In this chapter we introduce the algebra of functions of a complex variable. We will cover the trigonometric and
inverse trigonometric functions. The properties of trigonometric functions carry over directly from real-variable theory.
However, because of multi-valuedness, the inverse trigonometric functions are signiﬁcantly trickier than their real-variable
counterparts.
7.1
Curves and Regions
In this section we introduce curves and regions in the complex plane. This material is necessary for the study of
branch points in this chapter and later for contour integration.
Curves.
Consider two continuous functions, x(t) and y(t), deﬁned on the interval t ∈[t0 . . . t1]. The set of points
in the complex plane
{z(t) = x(t) + ıy(t) | t ∈[t0 . . . t1]}
228

deﬁnes a continuous curve or simply a curve.
If the endpoints coincide, z (t0) = z (t1), it is a closed curve. (We
assume that t0 ̸= t1.) If the curve does not intersect itself, then it is said to be a simple curve.
If x(t) and y(t) have continuous derivatives and the derivatives do not both vanish at any point1 , then it is a
smooth curve. This essentially means that the curve does not have any corners or other nastiness.
A continuous curve which is composed of a ﬁnite number of smooth curves is called a piecewise smooth curve. We
will use the word contour as a synonym for a piecewise smooth curve.
See Figure 7.1 for a smooth curve, a piecewise smooth curve, a simple closed curve and a non-simple closed curve.
(a)
(b)
(c)
(d)
Figure 7.1: (a) Smooth Curve, (b) Piecewise Smooth Curve, (c) Simple Closed Curve, (d) Non-Simple Closed
Curve
Regions.
A region R is connected if any two points in R can be connected by a curve which lies entirely in R. A
region is simply-connected if every closed curve in R can be continuously shrunk to a point without leaving R. A region
which is not simply-connected is said to be multiply-connected region. Another way of deﬁning simply-connected is
that a path connecting two points in R can be continuously deformed into any other path that connects those points.
Figure 7.2 shows a simply-connected region with two paths which can be continuously deformed into one another and
a multiply-connected region with paths which cannot be deformed into one another.
Jordan Curve Theorem.
A continuous, simple, closed curve is known as a Jordan curve.
The Jordan Curve
Theorem, which seems intuitively obvious but is diﬃcult to prove, states that a Jordan curve divides the plane into
1Why is it necessary that the derivatives do not both vanish?
229

Figure 7.2: Simply-connected and multiply-connected regions.
a simply-connected, bounded region and an unbounded region. These two regions are called the interior and exterior
regions, respectively. The two regions share the curve as a boundary. Points in the interior are said to be inside the
curve; points in the exterior are said to be outside the curve.
Traversal of a Contour.
Consider a Jordan curve. If you traverse the curve in the positive direction, then the
inside is to your left. If you traverse
the curve in the opposite direction, then the outside will be to your left and you
will go around the curve in the negative direction. For circles, the positive direction is the counter-clockwise direction.
The positive direction is consistent with the way angles are measured in a right-handed coordinate system, i.e. for a
circle centered on the origin, the positive direction is the direction of increasing angle. For an oriented contour C, we
denote the contour with opposite orientation as −C.
Boundary of a Region.
Consider a simply-connected region. The boundary of the region is traversed in the positive
direction if the region is to the left as you walk along the contour. For multiply-connected regions, the boundary may
be a set of contours. In this case the boundary is traversed in the positive direction if each of the contours is traversed
in the positive direction. When we refer to the boundary of a region we will assume it is given the positive orientation.
In Figure 7.3 the boundaries of three regions are traversed in the positive direction.
230

Figure 7.3: Traversing the boundary in the positive direction.
Two Interpretations of a Curve.
Consider a simple closed curve as depicted in Figure 7.4a. By giving it an
orientation, we can make a contour that either encloses the bounded domain Figure 7.4b or the unbounded domain
Figure 7.4c. Thus a curve has two interpretations. It can be thought of as enclosing either the points which are “inside”
or the points which are “outside”.2
7.2
The Point at Inﬁnity and the Stereographic Projection
Complex Inﬁnity.
In real variables, there are only two ways to get to inﬁnity. We can either go up or down the
number line. Thus signed inﬁnity makes sense. By going up or down we respectively approach +∞and −∞. In the
complex plane there are an inﬁnite number of ways to approach inﬁnity. We stand at the origin, point ourselves in any
direction and go straight. We could walk along the positive real axis and approach inﬁnity via positive real numbers.
We could walk along the positive imaginary axis and approach inﬁnity via pure imaginary numbers. We could generalize
the real variable notion of signed inﬁnity to a complex variable notion of directional inﬁnity, but this will not be useful
2 A farmer wanted to know the most eﬃcient way to build a pen to enclose his sheep, so he consulted an engineer, a physicist
and a mathematician. The engineer suggested that he build a circular pen to get the maximum area for any given perimeter. The
physicist suggested that he build a fence at inﬁnity and then shrink it to ﬁt the sheep. The mathematician constructed a little fence
around himself and then deﬁned himself to be outside.
231

(a)
(b)
(c)
Figure 7.4: Two interpretations of a curve.
for our purposes. Instead, we introduce complex inﬁnity or the point at inﬁnity as the limit of going inﬁnitely far along
any direction in the complex plane. The complex plane together with the point at inﬁnity form the extended complex
plane.
Stereographic Projection.
We can visualize the point at inﬁnity with the stereographic projection. We place a
unit sphere on top of the complex plane so that the south pole of the sphere is at the origin. Consider a line passing
through the north pole and a point z = x + ıy in the complex plane. In the stereographic projection, the point point z
is mapped to the point where the line intersects the sphere. (See Figure 7.5.) Each point z = x + ıy in the complex
plane is mapped to a unique point (X, Y, Z) on the sphere.
X =
4x
|z|2 + 4,
Y =
4y
|z|2 + 4,
Z =
2|z|2
|z|2 + 4
The origin is mapped to the south pole. The point at inﬁnity, |z| = ∞, is mapped to the north pole.
In the stereographic projection circles in the complex plane are mapped to circles on the unit sphere. Figure 7.6
shows circles along the real and imaginary axes under the mapping.
Lines in the complex plane are also mapped to circles on the unit sphere. Figure 7.7 shows lines emanating from
the origin under the mapping.
232

x
y
Figure 7.5: The stereographic projection.
7.3
Cartesian and Modulus-Argument Form
We can write a function of a complex variable z as a function of x and y or as a function of r and θ with the substitutions
z = x + ıy and z = r eıθ, respectively. Then we can separate the real and imaginary components or write the function
in modulus-argument form,
f(z) = u(x, y) + ıv(x, y),
or
f(z) = u(r, θ) + ıv(r, θ),
f(z) = ρ(x, y) eıφ(x,y),
or
f(z) = ρ(r, θ) eıφ(r,θ) .
Example 7.3.1 Consider the functions f(z) = z, f(z) = z3 and f(z) =
1
1−z. We write the functions in terms of x
233

Figure 7.6: The stereographic projection of circles.
and y and separate them into their real and imaginary components.
f(z) = z
= x + ıy
f(z) = z3
= (x + ıy)3
= x3 + ıx2y −xy2 −ıy3
=
 x3 −xy2
+ ı
 x2y −y3
234

Figure 7.7: The stereographic projection of lines.
f(z) =
1
1 −z
=
1
1 −x −ıy
=
1
1 −x −ıy
1 −x + ıy
1 −x + ıy
=
1 −x
(1 −x)2 + y2 + ı
y
(1 −x)2 + y2
Example 7.3.2 Consider the functions f(z) = z, f(z) = z3 and f(z) =
1
1−z. We write the functions in terms of r
235

and θ and write them in modulus-argument form.
f(z) = z
= r eıθ
f(z) = z3
=
 r eıθ3
= r3 eı3θ
f(z) =
1
1 −z
=
1
1 −r eıθ
=
1
1 −r eıθ
1
1 −r e−ıθ
=
1 −r e−ıθ
1 −r eıθ −r e−ıθ +r2
= 1 −r cos θ + ır sin θ
1 −2r cos θ + r2
Note that the denominator is real and non-negative.
=
1
1 −2r cos θ + r2|1 −r cos θ + ır sin θ| eı arctan(1−r cos θ,r sin θ)
=
1
1 −2r cos θ + r2
q
(1 −r cos θ)2 + r2 sin2 θ eı arctan(1−r cos θ,r sin θ)
=
1
1 −2r cos θ + r2
p
1 −2r cos θ + r2 cos2 θ + r2 sin2 θ eı arctan(1−r cos θ,r sin θ)
=
1
√
1 −2r cos θ + r2 eı arctan(1−r cos θ,r sin θ)
236

7.4
Graphing Functions of a Complex Variable
We cannot directly graph functions of a complex variable as they are mappings from R2 to R2. To do so would require
four dimensions. However, we can can use a surface plot to graph the real part, the imaginary part, the modulus or the
argument of a function of a complex variable. Each of these are scalar ﬁelds, mappings from R2 to R.
Example 7.4.1 Consider the identity function, f(z) = z. In Cartesian coordinates and Cartesian form, the function
is f(z) = x + ıy. The real and imaginary components are u(x, y) = x and v(x, y) = y. (See Figure 7.8.) In modulus
-2 -1
0
1
2
x
-2
-1
0
1
2
y
-2
-1012
2 -1
0
1
x
-2 -1
0
1
2
x
-2
-1
0
1
2
y
-2
-1012
2 -1
0
1
x
Figure 7.8: The real and imaginary parts of f(z) = z = x + ıy
argument form the function is
f(z) = z = r eıθ =
p
x2 + y2 eı arctan(x,y) .
The modulus of f(z) is a single-valued function which is the distance from the origin. The argument of f(z) is a multi-
valued function. Recall that arctan(x, y) has an inﬁnite number of values each of which diﬀer by an integer multiple
of 2π. A few branches of arg(f(z)) are plotted in Figure 7.9. The modulus and principal argument of f(z) = z are
plotted in Figure 7.10.
Example 7.4.2 Consider the function f(z) = z2. In Cartesian coordinates and separated into its real and imaginary
237

-2-1 0
1
2
x
-2-10 1 2
y
-5
0
5
-2-1 0
1
x
2-10 1
y
Figure 7.9: A Few Branches of arg(z)
-2 -1
0
1
2
x
-2
-1
0
1
2
y
012
2 -1
0
1
x
-2-1 0
1
2
x
-2
-1
0
12
y
-202
2-1 0
1
x
Figure 7.10: Plots of |z| and Arg(z)
components the function is
f(z) = z2 = (x + ıy)2 =
 x2 −y2
+ ı2xy.
238

Figure 7.11 shows surface plots of the real and imaginary parts of z2. The magnitude of z2 is
-2
-1
0
1
2
x
-2
-1
0
1
2
y
-4
-2
0
2
4
2
-1
0
1
x
-2
-1
0
1
2
x
-2
-1
0
1
2
y
-5
0
5
2
-1
0
1
x
Figure 7.11: Plots of ℜ(z2) and ℑ(z2)
|z2| =
p
z2z2 = zz = (x + ıy)(x −ıy) = x2 + y2.
Note that
z2 =
 r eıθ2 = r2 eı2θ .
In Figure 7.12 are plots of |z2| and a branch of arg (z2).
7.5
Trigonometric Functions
The Exponential Function.
Consider the exponential function ez. We can use Euler’s formula to write ez = ex+ıy
in terms of its real and imaginary parts.
ez = ex+ıy = ex eıy = ex cos y + ı ex sin y
239

-2
-1
0
1
2
x
-2
-1
0
1
2
y
0
2
4
6
8
2
-1
0
1
x
-2
-1
0
1
2
x
-2
-1
0
1
2
y
-5
0
5
-2
-1
0
1
x
Figure 7.12: Plots of |z2| and a branch of arg (z2)
From this we see that the exponential function is ı2π periodic: ez+ı2π = ez, and ıπ odd periodic: ez+ıπ = −ez.
Figure 7.13 has surface plots of the real and imaginary parts of ez which show this periodicity.
-2
0
2
x
-5
0
5
y
-20
-10
0
10
20
-2
0
2
x
-2
0
2
x
-5
0
5
y
-20
-10
0
10
20
-2
0
2
x
Figure 7.13: Plots of ℜ(ez) and ℑ(ez)
240

The modulus of ez is a function of x alone.
|ez| =
ex+ıy = ex
The argument of ez is a function of y alone.
arg (ez) = arg
 ex+ıy
= {y + 2πn | n ∈Z}
In Figure 7.14 are plots of | ez | and a branch of arg (ez).
-2
0
2
x
-5
0
5
y
05
10
15
20
-2
0
2
x
-2
0
2
x
-5
0
5
y
-5
0
5
-2
0
2
x
Figure 7.14: Plots of | ez | and a branch of arg (ez)
Example 7.5.1 Show that the transformation w = ez maps the inﬁnite strip, −∞< x < ∞, 0 < y < π, onto the
upper half-plane.
Method 1. Consider the line z = x + ıc, −∞< x < ∞. Under the transformation, this is mapped to
w = ex+ıc = eıc ex,
−∞< x < ∞.
This is a ray from the origin to inﬁnity in the direction of eıc. Thus we see that z = x is mapped to the positive, real
w axis, z = x + ıπ is mapped to the negative, real axis, and z = x + ıc, 0 < c < π is mapped to a ray with angle c in
the upper half-plane. Thus the strip is mapped to the upper half-plane. See Figure 7.15.
241

-3 -2 -1
1
2
3
1
2
3
-3 -2 -1
1
2
3
1
2
3
Figure 7.15: ez maps horizontal lines to rays.
Method 2. Consider the line z = c + ıy, 0 < y < π. Under the transformation, this is mapped to
w = ec+ıy + ec eıy,
0 < y < π.
This is a semi-circle in the upper half-plane of radius ec. As c →−∞, the radius goes to zero. As c →∞, the radius
goes to inﬁnity. Thus the strip is mapped to the upper half-plane. See Figure 7.16.
-1
1
1
2
3
-3 -2 -1
1
2
3
1
2
3
Figure 7.16: ez maps vertical lines to circular arcs.
242

The Sine and Cosine.
We can write the sine and cosine in terms of the exponential function.
eız + e−ız
2
= cos(z) + ı sin(z) + cos(−z) + ı sin(−z)
2
= cos(z) + ı sin(z) + cos(z) −ı sin(z)
2
= cos z
eız −e−ız
ı2
= cos(z) + ı sin(z) −cos(−z) −ı sin(−z)
2
= cos(z) + ı sin(z) −cos(z) + ı sin(z)
2
= sin z
We separate the sine and cosine into their real and imaginary parts.
cos z = cos x cosh y −ı sin x sinh y
sin z = sin x cosh y + ı cos x sinh y
For ﬁxed y, the sine and cosine are oscillatory in x. The amplitude of the oscillations grows with increasing |y|. See
Figure 7.17 and Figure 7.18 for plots of the real and imaginary parts of the cosine and sine, respectively. Figure 7.19
shows the modulus of the cosine and the sine.
The Hyperbolic Sine and Cosine.
The hyperbolic sine and cosine have the familiar deﬁnitions in terms of the
exponential function. Thus not surprisingly, we can write the sine in terms of the hyperbolic sine and write the cosine
in terms of the hyperbolic cosine. Below is a collection of trigonometric identities.
243

-2
0
2
x
-2
-1
0
1
2
y
-5
-2.5
0
2.5
5
-2
0
2
x
-2
0
2
x
-2
-1
0
1
2
y
-5
-2.5
0
2.5
5
-2
0
2
x
Figure 7.17: Plots of ℜ(cos(z)) and ℑ(cos(z))
-2
0
2
x
-2
-1
0
1
2
y
-5
-2.5
0
2.5
5
-2
0
2
x
-2
0
2
x
-2
-1
0
1
2
y
-5
-2.5
0
2.5
5
-2
0
2
x
Figure 7.18: Plots of ℜ(sin(z)) and ℑ(sin(z))
Result 7.5.1
ez = ex(cos y + ı sin y)
cos z = eız + e−ız
2
sin z = eız −e−ız
ı2
cos z = cos x cosh y −ı sin x sinh y
sin z = sin x cosh y + ı cos x sinh y
cosh z = ez + e−z
2
sinh z = ez −e−z
2
cosh z = cosh x cos y + ı sinh x sin y
sinh z = sinh x cos y + ı cosh x sin y
sin(ız) = ı sinh z
sinh(ız) = ı sin z
cos(ız) = cosh z
cosh(ız) = cos z
log z = ln |z| + ı arg(z) = ln |z| + ı Arg(z) + ı2πn,
n ∈Z
244

-2
0
2
x
-2
-1
0
1
2
y
2
4
-2
0
2
x
-2
0
2
x
-2
-1
0
1
2
y
0
2
4
-2
0
2
x
Figure 7.19: Plots of | cos(z)| and | sin(z)|
7.6
Inverse Trigonometric Functions
The Logarithm.
The logarithm, log(z), is deﬁned as the inverse of the exponential function ez. The exponential
function is many-to-one and thus has a multi-valued inverse. From what we know of many-to-one functions, we conclude
that
elog z = z,
but
log (ez) ̸= z.
This is because elog z is single-valued but log (ez) is not. Because ez is ı2π periodic, the logarithm of a number is a set
of numbers which diﬀer by integer multiples of ı2π. For instance, eı2πn = 1 so that log(1) = {ı2πn : n ∈Z}. The
logarithmic function has an inﬁnite number of branches. The value of the function on the branches diﬀers by integer
multiples of ı2π. It has singularities at zero and inﬁnity. | log(z)| →∞as either z →0 or z →∞.
We will derive the formula for the complex variable logarithm. For now, let ln(x) denote the real variable logarithm
that is deﬁned for positive real numbers. Consider w = log z. This means that ew = z. We write w = u + ıv in
Cartesian form and z = r eıθ in polar form.
eu+ıv = r eıθ
245

We equate the modulus and argument of this expression.
eu = r
v = θ + 2πn
u = ln r
v = θ + 2πn
With log z = u + ıv, we have a formula for the logarithm.
log z = ln |z| + ı arg(z)
If we write out the multi-valuedness of the argument function we note that this has the form that we expected.
log z = ln |z| + ı(Arg(z) + 2πn),
n ∈Z
We check that our formula is correct by showing that elog z = z
elog z = eln |z|+ı arg(z) = eln r+ıθ+ı2πn = r eıθ = z
Note again that log (ez) ̸= z.
log (ez) = ln | ez | + ı arg (ez) = ln (ex) + ı arg
 ex+ıy
= x + ı(y + 2πn) = z + ı2nπ ̸= z
The real part of the logarithm is the single-valued ln r; the imaginary part is the multi-valued arg(z). We deﬁne the
principal branch of the logarithm Log z to be the branch that satisﬁes −π < ℑ(Log z) ≤π. For positive, real numbers
the principal branch, Log x is real-valued. We can write Log z in terms of the principal argument, Arg z.
Log z = ln |z| + ı Arg(z)
See Figure 7.20 for plots of the real and imaginary part of Log z.
The Form: ab.
Consider ab where a and b are complex and a is nonzero. We deﬁne this expression in terms of the
exponential and the logarithm as
ab = eb log a .
246

-2
-1
0
1
2
x
-2
-1
0
1
2
y
-2
-1
0
1
2
-1
0
1
x
-2
-1
0
1
2
x
-2
-1
0
1
2
y
-2
0
2
-2
-1
0
1
x
Figure 7.20: Plots of ℜ(Log z) and ℑ(Log z).
Note that the multi-valuedness of the logarithm may make ab multi-valued. First consider the case that the exponent
is an integer.
am = em log a = em(Log a+ı2nπ) = em Log a eı2mnπ = em Log a
Thus we see that am has a single value where m is an integer.
Now consider the case that the exponent is a rational number. Let p/q be a rational number in reduced form.
ap/q = e
p
q log a = e
p
q (Log a+ı2nπ) = e
p
q Log a eı2npπ/q .
This expression has q distinct values as
eı2npπ/q = eı2mpπ/q
if and only if
n = m
mod q.
Finally consider the case that the exponent b is an irrational number.
ab = eb log a = eb(Log a+ı2nπ) = eb Log a eı2bnπ
247

Note that eı2bnπ and eı2bmπ are equal if and only if ı2bnπ and ı2bmπ diﬀer by an integer multiple of ı2π, which means
that bn and bm diﬀer by an integer. This occurs only when n = m. Thus eı2bnπ has a distinct value for each diﬀerent
integer n. We conclude that ab has an inﬁnite number of values.
You may have noticed something a little ﬁshy. If b is not an integer and a is any non-zero complex number, then
ab is multi-valued. Then why have we been treating eb as single-valued, when it is merely the case a = e? The answer
is that in the realm of functions of a complex variable, ez is an abuse of notation. We write ez when we mean exp(z),
the single-valued exponential function. Thus when we write ez we do not mean “the number e raised to the z power”,
we mean “the exponential function of z”. We denote the former scenario as (e)z, which is multi-valued.
Logarithmic Identities.
Back in high school trigonometry when you thought that the logarithm was only deﬁned
for positive real numbers you learned the identity log xa = a log x. This identity doesn’t hold when the logarithm is
deﬁned for nonzero complex numbers. Consider the logarithm of za.
log za = Log za + ı2πn
a log z = a(Log z + ı2πn) = a Log z + ı2aπn
Note that
log za ̸= a log z
Furthermore, since
Log za = ln |za| + ı Arg (za) ,
a Log z = a ln |z| + ıa Arg(z)
and Arg (za) is not necessarily the same as a Arg(z) we see that
Log za ̸= a Log z.
Consider the logarithm of a product.
log(ab) = ln |ab| + ı arg(ab)
= ln |a| + ln |b| + ı arg(a) + ı arg(b)
= log a + log b
248

There is not an analogous identity for the principal branch of the logarithm since Arg(ab) is not in general the same as
Arg(a) + Arg(b).
Using log(ab) = log(a) + log(b) we can deduce that log (an) = Pn
k=1 log a = n log a, where n is a positive
integer. This result is simple, straightforward and wrong. I have led you down the merry path to damnation.3 In fact,
log (a2) ̸= 2 log a. Just write the multi-valuedness explicitly,
log
 a2
= Log
 a2
+ ı2nπ,
2 log a = 2(Log a + ı2nπ) = 2 Log a + ı4nπ.
You can verify that
log
1
a

= −log a.
We can use this and the product identity to expand the logarithm of a quotient.
log
a
b

= log a −log b
For general values of a, log za ̸= a log z. However, for some values of a, equality holds. We already know that a = 1
and a = −1 work. To determine if equality holds for other values of a, we explicitly write the multi-valuedness.
log za = log
 ea log z
= a log z + ı2πk,
k ∈Z
a log z = a ln |z| + ıa Arg z + ıa2πm,
m ∈Z
We see that log za = a log z if and only if
{am | m ∈Z} = {am + k | k, m ∈Z}.
The sets are equal if and only if a = 1/n, n ∈Z±. Thus we have the identity:
log
 z1/n
= 1
n log z,
n ∈Z±
3 Don’t feel bad if you fell for it. The logarithm is a tricky bastard.
249

Result 7.6.1 Logarithmic Identities.
ab = eb log a
elog z = eLog z = z
log(ab) = log a + log b
log(1/a) = −log a
log(a/b) = log a −log b
log

z1/n
= 1
n log z,
n ∈Z±
Logarithmic Inequalities.
Log(uv) ̸= Log(u) + Log(v)
log za ̸= a log z
Log za ̸= a Log z
log ez ̸= z
Example 7.6.1 Consider 1π. We apply the deﬁnition ab = eb log a.
1π = eπ log(1)
= eπ(ln(1)+ı2nπ)
= eı2nπ2
Thus we see that 1π has an inﬁnite number of values, all of which lie on the unit circle |z| = 1 in the complex plane.
However, the set 1π is not equal to the set |z| = 1. There are points in the latter which are not in the former. This is
analogous to the fact that the rational numbers are dense in the real numbers, but are a subset of the real numbers.
250

Example 7.6.2 We ﬁnd the zeros of sin z.
sin z = eız −e−ız
ı2
= 0
eız = e−ız
eı2z = 1
2z
mod 2π = 0
z = nπ,
n ∈Z
Equivalently, we could use the identity
sin z = sin x cosh y + ı cos x sinh y = 0.
This becomes the two equations (for the real and imaginary parts)
sin x cosh y = 0
and
cos x sinh y = 0.
Since cosh is real-valued and positive for real argument, the ﬁrst equation dictates that x = nπ, n ∈Z.
Since
cos(nπ) = (−1)n for n ∈Z, the second equation implies that sinh y = 0. For real argument, sinh y is only zero at
y = 0. Thus the zeros are
z = nπ,
n ∈Z
Example 7.6.3 Since we can express sin z in terms of the exponential function, one would expect that we could express
251

the sin−1 z in terms of the logarithm.
w = sin−1 z
z = sin w
z = eıw −e−ıw
ı2
eı2w −ı2z eıw −1 = 0
eıw = ız ±
√
1 −z2
w = −ı log

ız ±
√
1 −z2

Thus we see how the multi-valued sin−1 is related to the logarithm.
sin−1 z = −ı log

ız ±
√
1 −z2

Example 7.6.4 Consider the equation sin3 z = 1.
sin3 z = 1
sin z = 11/3
eız −e−ız
ı2
= 11/3
eız −ı2(1)1/3 −e−ız = 0
eı2z −ı2(1)1/3 eız −1 = 0
eız = ı2(1)1/3 ±
p
−4(1)2/3 + 4
2
eız = ı(1)1/3 ±
q
1 −(1)2/3
z = −ı log

ı(1)1/3 ±
p
1 −12/3

252

Note that there are three sources of multi-valuedness in the expression for z. The two values of the square root are
shown explicitly. There are three cube roots of unity. Finally, the logarithm has an inﬁnite number of branches. To
show this multi-valuedness explicitly, we could write
z = −ı Log

ı eı2mπ/3 ±
p
1 −eı4mπ/3

+ 2πn,
m = 0, 1, 2,
n = . . . , −1, 0, 1, . . .
Example 7.6.5 Consider the harmless looking equation, ız = 1.
Before we start with the algebra, note that the right side of the equation is a single number. ız is single-valued only
when z is an integer. Thus we know that if there are solutions for z, they are integers. We now proceed to solve the
equation.
ız = 1
 eıπ/2z = 1
Use the fact that z is an integer.
eıπz/2 = 1
ıπz/2 = ı2nπ,
for some n ∈Z
z = 4n,
n ∈Z
Here is a diﬀerent approach. We write down the multi-valued form of ız. We solve the equation by requiring that
all the values of ız are 1.
ız = 1
ez log ı = 1
z log ı = ı2πn,
for some n ∈Z
z

ıπ
2 + ı2πm

= ı2πn,
∀m ∈Z,
for some n ∈Z
ıπ
2 z + ı2πmz = ı2πn,
∀m ∈Z,
for some n ∈Z
253

The only solutions that satisfy the above equation are
z = 4k,
k ∈Z.
Now let’s consider a slightly diﬀerent problem: 1 ∈ız. For what values of z does ız have 1 as one of its values.
1 ∈ız
1 ∈ez log ı
1 ∈{ez(ıπ/2+ı2πn) | n ∈Z}
z(ıπ/2 + ı2πn) = ı2πm,
m, n ∈Z
z =
4m
1 + 4n,
m, n ∈Z
There are an inﬁnite set of rational numbers for which ız has 1 as one of its values. For example,
ı4/5 = 11/5 =

1, eı2π/5, eı4π/5, eı6π/5, eı8π/5	
7.7
Riemann Surfaces
Consider the mapping w = log(z). Each nonzero point in the z-plane is mapped to an inﬁnite number of points in
the w plane.
w = {ln |z| + ı arg(z)} = {ln |z| + ı(Arg(z) + 2πn) | n ∈Z}
This multi-valuedness makes it hard to work with the logarithm. We would like to select one of the branches of the
logarithm. One way of doing this is to decompose the z-plane into an inﬁnite number of sheets. The sheets lie above
one another and are labeled with the integers, n ∈Z. (See Figure 7.21.) We label the point z on the nth sheet as
(z, n). Now each point (z, n) maps to a single point in the w-plane. For instance, we can make the zeroth sheet map
to the principal branch of the logarithm. This would give us the following mapping.
log(z, n) = Log z + ı2πn
254

-2
-1
0
1
2
Figure 7.21: The z-plane decomposed into ﬂat sheets.
This is a nice idea, but it has some problems. The mappings are not continuous. Consider the mapping on the
zeroth sheet. As we approach the negative real axis from above z is mapped to ln |z| + ıπ as we approach from below
it is mapped to ln |z| −ıπ. (Recall Figure 7.20.) The mapping is not continuous across the negative real axis.
Let’s go back to the regular z-plane for a moment. We start at the point z = 1 and selecting the branch of the
logarithm that maps to zero. (log(1) = ı2πn). We make the logarithm vary continuously as we walk around the origin
once in the positive direction and return to the point z = 1. Since the argument of z has increased by 2π, the value
of the logarithm has changed to ı2π. If we walk around the origin again we will have log(1) = ı4π. Our ﬂat sheet
decomposition of the z-plane does not reﬂect this property. We need a decomposition with a geometry that makes the
mapping continuous and connects the various branches of the logarithm.
Drawing inspiration from the plot of arg(z), Figure 7.9, we decompose the z-plane into an inﬁnite corkscrew with
axis at the origin. (See Figure 7.22.) We deﬁne the mapping so that the logarithm varies continuously on this surface.
Consider a point z on one of the sheets. The value of the logarithm at that same point on sheet directly above it is
ı2π more than the original value. We call this surface, the Riemann surface for the logarithm. The mapping from the
Riemann surface to the w-plane is continuous and one-to-one.
255

Figure 7.22: The Riemann surface for the logarithm.
7.8
Branch Points
Example 7.8.1 Consider the function z1/2.
For each value of z, there are two values of z1/2.
We write z1/2 in
modulus-argument and Cartesian form.
z1/2 =
p
|z| eı arg(z)/2
z1/2 =
p
|z| cos(arg(z)/2) + ı
p
|z| sin(arg(z)/2)
Figure 7.23 shows the real and imaginary parts of z1/2 from three diﬀerent viewpoints. The second and third views are
looking down the x axis and y axis, respectively. Consider ℜ
 z1/2
. This is a double layered sheet which intersects
itself on the negative real axis. (ℑ(z1/2) has a similar structure, but intersects itself on the positive real axis.) Let’s
start at a point on the positive real axis on the lower sheet. If we walk around the origin once and return to the positive
real axis, we will be on the upper sheet. If we do this again, we will return to the lower sheet.
Suppose we are at a point in the complex plane. We pick one of the two values of z1/2. If the function varies
continuously as we walk around the origin and back to our starting point, the value of z1/2 will have changed. We will
be on the other branch. Because walking around the point z = 0 takes us to a diﬀerent branch of the function, we
256

refer to z = 0 as a branch point.
-2
-1
0
1
2
x
-2
-1
0
1
2
y
-1
0
1
2
-2
-1
0
1
2
x
-2
-1
0
1
2
y
-1
0
1
2
-2
-1012
x
-2
-1
0
1
2
y
-1
0
1
-2
-1012
x
-2
-1
0
1
2
y
-1
0
1
-2
-1
0
1
2
x
-2
-1
0
1
2
y
-1
0
1
-2
-1
0
1
x
-2
-1
0
1
2
x
-2
-1
0
1
2
y
-1
0
1
-2
-1
0
1
x
Figure 7.23: Plots of ℜ
 z1/2
(left) and ℑ
 z1/2
(right) from three viewpoints.
257

Now consider the modulus-argument form of z1/2:
z1/2 =
p
|z| eı arg(z)/2 .
Figure 7.24 shows the modulus and the principal argument of z1/2. We see that each time we walk around the origin,
the argument of z1/2 changes by π. This means that the value of the function changes by the factor eıπ = −1, i.e.
the function changes sign. If we walk around the origin twice, the argument changes by 2π, so that the value of the
function does not change, eı2π = 1.
-2-1 0 1
2
x
-2
-1
0
12
y
0
0.51
2-1 0 1
x
-2-1 0
1
2
x
-2
-1
0
12
y
-202
2-1 0
1
x
Figure 7.24: Plots of |z1/2| and Arg
 z1/2
.
z1/2 is a continuous function except at z = 0. Suppose we start at z = 1 = eı0 and the function value (eı0)1/2 = 1.
If we follow the ﬁrst path in Figure 7.25, the argument of z varies from up to about π
4, down to about −π
4 and back
to 0. The value of the function is still (eı0)1/2.
Now suppose we follow a circular path around the origin in the positive, counter-clockwise, direction. (See the
258

Re(z)
Im(z)
Re(z)
Im(z)
Figure 7.25: A path that does not encircle the origin and a path around the origin
second path in Figure 7.25.) The argument of z increases by 2π. The value of the function at half turns on the path is
 eı01/2 = 1,
(eıπ)1/2 = eıπ/2 = ı,
 eı2π1/2 = eıπ = −1
As we return to the point z = 1, the argument of the function has changed by π and the value of the function has
changed from 1 to −1. If we were to walk along the circular path again, the argument of z would increase by another
2π. The argument of the function would increase by another π and the value of the function would return to 1.
 eı4π1/2 = eı2π = 1
In general, any time we walk around the origin, the value of z1/2 changes by the factor −1. We call z = 0 a branch
point. If we want a single-valued square root, we need something to prevent us from walking around the origin. We
achieve this by introducing a branch cut. Suppose we have the complex plane drawn on an inﬁnite sheet of paper.
With a scissors we cut the paper from the origin to −∞along the real axis. Then if we start at z = eı0, and draw a
continuous line without leaving the paper, the argument of z will always be in the range −π < arg z < π. This means
259

that −π
2 < arg
 z1/2
< π
2. No matter what path we follow in this cut plane, z = 1 has argument zero and (1)1/2 = 1.
By never crossing the negative real axis, we have constructed a single valued branch of the square root function. We
call the cut along the negative real axis a branch cut.
Example 7.8.2 Consider the logarithmic function log z. For each value of z, there are an inﬁnite number of values of
log z. We write log z in Cartesian form.
log z = ln |z| + ı arg z
Figure 7.26 shows the real and imaginary parts of the logarithm. The real part is single-valued. The imaginary part is
multi-valued and has an inﬁnite number of branches. The values of the logarithm form an inﬁnite-layered sheet. If we
start on one of the sheets and walk around the origin once in the positive direction, then the value of the logarithm
increases by ı2π and we move to the next branch. z = 0 is a branch point of the logarithm.
-2 -1
0
1
2
x
-2
-1
0
1
2
y
-2
-1
0
1
2 -1
0
1
x
-2 -1
0
1
2
x
-2
-1
0
1
2
y
-505
-2 -1
0
1
x
Figure 7.26: Plots of ℜ(log z) and a portion of ℑ(log z).
The logarithm is a continuous function except at z = 0. Suppose we start at z = 1 = eı0 and the function value
log (eı0) = ln(1) + ı0 = 0. If we follow the ﬁrst path in Figure 7.25, the argument of z and thus the imaginary part of
the logarithm varies from up to about π
4, down to about −π
4 and back to 0. The value of the logarithm is still 0.
260

Now suppose we follow a circular path around the origin in the positive direction. (See the second path in Fig-
ure 7.25.) The argument of z increases by 2π. The value of the logarithm at half turns on the path is
log
 eı0
= 0,
log (eıπ) = ıπ,
log
 eı2π
= ı2π
As we return to the point z = 1, the value of the logarithm has changed by ı2π. If we were to walk along the circular
path again, the argument of z would increase by another 2π and the value of the logarithm would increase by another
ı2π.
Result 7.8.1 A point z0 is a branch point of a function f(z) if the function changes value
when you walk around the point on any path that encloses no singularities other than the one
at z = z0.
Branch Points at Inﬁnity : Mapping Inﬁnity to the Origin.
Up to this point we have considered only
branch points in the ﬁnite plane. Now we consider the possibility of a branch point at inﬁnity. As a ﬁrst method of
approaching this problem we map the point at inﬁnity to the origin with the transformation ζ = 1/z and examine the
point ζ = 0.
Example 7.8.3 Again consider the function z1/2.
Mapping the point at inﬁnity to the origin, we have f(ζ) =
(1/ζ)1/2 = ζ−1/2. For each value of ζ, there are two values of ζ−1/2. We write ζ−1/2 in modulus-argument form.
ζ−1/2 =
1
p
|ζ|
e−ı arg(ζ)/2
Like z1/2, ζ−1/2 has a double-layered sheet of values. Figure 7.27 shows the modulus and the principal argument of
ζ−1/2. We see that each time we walk around the origin, the argument of ζ−1/2 changes by −π. This means that the
261

-2 -1
0
1
2
x
-2
-1
0
1
2
y
1
1.52
2.53
2 -1
0
1
x
-2 -1
0
1
2
x
-2
-1
0
1
2
y
-2
0
2
-2 -1
0
1
x
Figure 7.27: Plots of |ζ−1/2| and Arg
 ζ−1/2
.
value of the function changes by the factor e−ıπ = −1, i.e. the function changes sign. If we walk around the origin
twice, the argument changes by −2π, so that the value of the function does not change, e−ı2π = 1.
Since ζ−1/2 has a branch point at zero, we conclude that z1/2 has a branch point at inﬁnity.
Example 7.8.4 Again consider the logarithmic function log z. Mapping the point at inﬁnity to the origin, we have
f(ζ) = log(1/ζ) = −log(ζ). From Example 7.8.2 we known that −log(ζ) has a branch point at ζ = 0. Thus log z
has a branch point at inﬁnity.
Branch Points at Inﬁnity : Paths Around Inﬁnity.
We can also check for a branch point at inﬁnity by
following a path that encloses the point at inﬁnity and no other singularities. Just draw a simple closed curve that
separates the complex plane into a bounded component that contains all the singularities of the function in the ﬁnite
plane. Then, depending on orientation, the curve is a contour enclosing all the ﬁnite singularities, or the point at inﬁnity
and no other singularities.
262

Example 7.8.5 Once again consider the function z1/2. We know that the function changes value on a curve that goes
once around the origin. Such a curve can be considered to be either a path around the origin or a path around inﬁnity.
In either case the path encloses one singularity. There are branch points at the origin and at inﬁnity. Now consider a
curve that does not go around the origin. Such a curve can be considered to be either a path around neither of the
branch points or both of them. Thus we see that z1/2 does not change value when we follow a path that encloses
neither or both of its branch points.
Example 7.8.6 Consider f(z) = (z2 −1)1/2. We factor the function.
f(z) = (z −1)1/2(z + 1)1/2
There are branch points at z = ±1. Now consider the point at inﬁnity.
f
 ζ−1
=
 ζ−2 −1
1/2 = ±ζ−1  1 −ζ21/2
Since f (ζ−1) does not have a branch point at ζ = 0, f(z) does not have a branch point at inﬁnity. We could reach
the same conclusion by considering a path around inﬁnity. Consider a path that circles the branch points at z = ±1
once in the positive direction. Such a path circles the point at inﬁnity once in the negative direction. In traversing this
path, the value of f(z) is multiplied by the factor (eı2π)1/2 (eı2π)1/2 = eı2π = 1. Thus the value of the function does
not change. There is no branch point at inﬁnity.
Diagnosing Branch Points.
We have the deﬁnition of a branch point, but we do not have a convenient criterion
for determining if a particular function has a branch point. We have seen that log z and zα for non-integer α have
branch points at zero and inﬁnity. The inverse trigonometric functions like the arcsine also have branch points, but they
can be written in terms of the logarithm and the square root. In fact all the elementary functions with branch points
can be written in terms of the functions log z and zα. Furthermore, note that the multi-valuedness of zα comes from
the logarithm, zα = eα log z. This gives us a way of quickly determining if and where a function may have branch points.
Result 7.8.2 Let f(z) be a single-valued function. Then log(f(z)) and (f(z))α may have
branch points only where f(z) is zero or singular.
263

Example 7.8.7 Consider the functions,
1. (z2)1/2
2.
 z1/22
3.
 z1/23
Are they multi-valued? Do they have branch points?
1.
 z21/2 = ±
√
z2 = ±z
Because of the (·)1/2, the function is multi-valued. The only possible branch points are at zero and inﬁnity. If

(eı0)21/2
= 1, then

(eı2π)21/2
= (eı4π)1/2 = eı2π = 1. Thus we see that the function does not change
value when we walk around the origin. We can also consider this to be a path around inﬁnity. This function is
multi-valued, but has no branch points.
2.
 z1/22 =
 ±√z
2 = z
This function is single-valued.
3.
 z1/23 =
 ±√z
3 = ±
 √z
3
This function is multi-valued.
We consider the possible branch point at z = 0.
If

(e0)1/23
= 1, then

(eı2π)1/23
= (eıπ)3 = eı3π = −1. Since the function changes value when we walk around the origin, it has a
branch point at z = 0. Since this is also a path around inﬁnity, there is a branch point there.
264

Example 7.8.8 Consider the function f(z) = log
 1
z−1

. Since
1
z−1 is only zero at inﬁnity and its only singularity is at
z = 1, the only possibilities for branch points are at z = 1 and z = ∞. Since
log

1
z −1

= −log(z −1)
and log w has branch points at zero and inﬁnity, we see that f(z) has branch points at z = 1 and z = ∞.
Example 7.8.9 Consider the functions,
1. elog z
2. log ez.
Are they multi-valued? Do they have branch points?
1.
elog z = exp(Log z + ı2πn) = eLog z eı2πn = z
This function is single-valued.
2.
log ez = Log ez +ı2πn = z + ı2πm
This function is multi-valued. It may have branch points only where ez is zero or inﬁnite. This only occurs at
z = ∞. Thus there are no branch points in the ﬁnite plane. The function does not change when traversing a
simple closed path. Since this path can be considered to enclose inﬁnity, there is no branch point at inﬁnity.
Consider (f(z))α where f(z) is single-valued and f(z) has either a zero or a singularity at z = z0. (f(z))α may
have a branch point at z = z0. If f(z) is not a power of z, then it may be diﬃcult to tell if (f(z))α changes value when
we walk around z0. Factor f(z) into f(z) = g(z)h(z) where h(z) is nonzero and ﬁnite at z0. Then g(z) captures the
important behavior of f(z) at the z0. g(z) tells us how fast f(z) vanishes or blows up. Since (f(z))α = (g(z))α(h(z))α
265

and (h(z))α does not have a branch point at z0, (f(z))α has a branch point at z0 if and only if (g(z))α has a branch
point there.
Similarly, we can decompose
log(f(z)) = log(g(z)h(z)) = log(g(z)) + log(h(z))
to see that log(f(z)) has a branch point at z0 if and only if log(g(z)) has a branch point there.
Result 7.8.3 Consider a single-valued function f(z) that has either a zero or a singularity at
z = z0. Let f(z) = g(z)h(z) where h(z) is nonzero and ﬁnite. (f(z))α has a branch point
at z = z0 if and only if (g(z))α has a branch point there. log(f(z)) has a branch point at
z = z0 if and only if log(g(z)) has a branch point there.
Example 7.8.10 Consider the functions,
1. sin z1/2
2. (sin z)1/2
3. z1/2 sin z1/2
4. (sin z2)1/2
Find the branch points and the number of branches.
1.
sin z1/2 = sin
 ±√z

= ± sin √z
sin z1/2 is multi-valued. It has two branches. There may be branch points at zero and inﬁnity. Consider the unit
circle which is a path around the origin or inﬁnity. If sin

(eı0)1/2
= sin(1), then sin

(eı2π)1/2
= sin (eıπ) =
sin(−1) = −sin(1). There are branch points at the origin and inﬁnity.
266

2.
(sin z)1/2 = ±
√
sin z
The function is multi-valued with two branches. The sine vanishes at z = nπ and is singular at inﬁnity. There
could be branch points at these locations. Consider the point z = nπ. We can write
sin z = (z −nπ) sin z
z −nπ
Note that
sin z
z−nπ is nonzero and has a removable singularity at z = nπ.
lim
z→nπ
sin z
z −nπ = lim
z→nπ
cos z
1
= (−1)n
Since (z −nπ)1/2 has a branch point at z = nπ, (sin z)1/2 has branch points at z = nπ.
Since the branch points at z = nπ go all the way out to inﬁnity. It is not possible to make a path that encloses
inﬁnity and no other singularities. The point at inﬁnity is a non-isolated singularity. A point can be a branch
point only if it is an isolated singularity.
3.
z1/2 sin z1/2 = ±√z sin
 ±√z

= ±√z
 ± sin √z

= √z sin √z
The function is single-valued. Thus there could be no branch points.
4.
 sin z21/2 = ±
√
sin z2
This function is multi-valued. Since sin z2 = 0 at z = (nπ)1/2, there may be branch points there. First consider
the point z = 0. We can write
sin z2 = z2sin z2
z2
267

where sin (z2) /z2 is nonzero and has a removable singularity at z = 0.
lim
z→0
sin z2
z2
= lim
z→0
2z cos z2
2z
= 1.
Since (z2)1/2 does not have a branch point at z = 0, (sin z2)1/2 does not have a branch point there either.
Now consider the point z = √nπ.
sin z2 =
 z −√nπ

sin z2
z −√nπ
sin (z2) / (z −√nπ) in nonzero and has a removable singularity at z = √nπ.
lim
z→√nπ
sin z2
z −√nπ =
lim
z→√nπ
2z cos z2
1
= 2√nπ(−1)n
Since (z −√nπ)
1/2 has a branch point at z = √nπ, (sin z2)1/2 also has a branch point there.
Thus we see that (sin z2)1/2 has branch points at z = (nπ)1/2 for n ∈Z \ {0}. This is the set of numbers:
{±√π, ±
√
2π, . . . , ±ı√π, ±ı
√
2π, . . .}. The point at inﬁnity is a non-isolated singularity.
Example 7.8.11 Find the branch points of
f(z) =
 z3 −z
1/3 .
Introduce branch cuts. If f(2) =
3√
6 then what is f(−2)?
We expand f(z).
f(z) = z1/3(z −1)1/3(z + 1)1/3.
There are branch points at z = −1, 0, 1. We consider the point at inﬁnity.
f
1
ζ

=
1
ζ
1/3 1
ζ −1
1/3 1
ζ + 1
1/3
= 1
ζ (1 −ζ)1/3 (1 + ζ)1/3
268

Since f(1/ζ) does not have a branch point at ζ = 0, f(z) does not have a branch point at inﬁnity. Consider the three
possible branch cuts in Figure 7.28.
Figure 7.28: Three Possible Branch Cuts for f(z) = (z3 −z)1/3
The ﬁrst and the third branch cuts will make the function single valued, the second will not. It is clear that the ﬁrst
set makes the function single valued since it is not possible to walk around any of the branch points.
The second set of branch cuts would allow you to walk around the branch points at z = ±1. If you walked around
these two once in the positive direction, the value of the function would change by the factor eı4π/3.
The third set of branch cuts would allow you to walk around all three branch points together. You can verify that
if you walk around the three branch points, the value of the function will not change (eı6π/3 = eı2π = 1).
Suppose we introduce the third set of branch cuts and are on the branch with f(2) =
3√
6.
f(2) =
 2 eı01/3  1 eı01/3  3 eı01/3 =
3√
6
The value of f(−2) is
f(−2) = (2 eıπ)1/3 (3 eıπ)1/3 (1 eıπ)1/3
=
3√
2 eıπ/3
3√
3 eıπ/3
3√
1 eıπ/3
=
3√
6 eıπ
= −
3√
6.
269

Example 7.8.12 Find the branch points and number of branches for
f(z) = zz2.
zz2 = exp
 z2 log z

There may be branch points at the origin and inﬁnity due to the logarithm. Consider walking around a circle of radius
r centered at the origin in the positive direction. Since the logarithm changes by ı2π, the value of f(z) changes by the
factor eı2πr2. There are branch points at the origin and inﬁnity. The function has an inﬁnite number of branches.
Example 7.8.13 Construct a branch of
f(z) =
 z2 + 1
1/3
such that
f(0) = 1
2

−1 + ı
√
3

.
First we factor f(z).
f(z) = (z −ı)1/3(z + ı)1/3
There are branch points at z = ±ı. Figure 7.29 shows one way to introduce branch cuts.
Since it is not possible to walk around any branch point, these cuts make the function single valued. We introduce
the coordinates:
z −ı = ρ eıφ,
z + ı = r eıθ .
f(z) =
 ρ eıφ1/3  r eıθ1/3
=
3√ρr eı(φ+θ)/3
The condition
f(0) = 1
2

−1 + ı
√
3

= eı(2π/3+2πn)
270

θ
r
φ
ρ
Figure 7.29: Branch Cuts for f(z) = (z2 + 1)1/3
can be stated
3√
1 eı(φ+θ)/3 = eı(2π/3+2πn)
φ + θ = 2π + 6πn
The angles must be deﬁned to satisfy this relation. One choice is
π
2 < φ < 5π
2 ,
−π
2 < θ < 3π
2 .
Principal Branches.
We construct the principal branch of the logarithm by putting a branch cut on the negative
real axis choose z = r eıθ, θ ∈(−π, π). Thus the principal branch of the logarithm is
Log z = ln r + ıθ,
−π < θ < π.
Note that the if x is a negative real number, (and thus lies on the branch cut), then Log x is undeﬁned.
The principal branch of zα is
zα = eα Log z .
271

Note that there is a branch cut on the negative real axis.
−απ < arg
 eα Log z
< απ
The principal branch of the z1/2 is denoted √z. The principal branch of z1/n is denoted
n√z.
Example 7.8.14 Construct
√
1 −z2, the principal branch of (1 −z2)1/2.
First note that since (1 −z2)1/2 = (1 −z)1/2(1 + z)1/2 there are branch points at z = 1 and z = −1. The
principal branch of the square root has a branch cut on the negative real axis. 1 −z2 is a negative real number for
z ∈(−∞. . . −1) ∪(1 . . . ∞). Thus we put branch cuts on (−∞. . . −1] and [1 . . . ∞).
272

7.9
Exercises
Cartesian and Modulus-Argument Form
Exercise 7.1
Find the image of the strip 2 < x < 3 under the mapping w = f(z) = z2. Does the image constitute a domain?
Hint, Solution
Exercise 7.2
For a given real number φ, 0 ≤φ < 2π, ﬁnd the image of the sector 0 ≤arg(z) < φ under the transformation w = z4.
How large should φ be so that the w plane is covered exactly once?
Hint, Solution
Trigonometric Functions
Exercise 7.3
In Cartesian coordinates, z = x + ıy, write sin(z) in Cartesian and modulus-argument form.
Hint, Solution
Exercise 7.4
Show that ez is nonzero for all ﬁnite z.
Hint, Solution
Exercise 7.5
Show that
ez2 ≤e|z|2 .
When does equality hold?
Hint, Solution
Exercise 7.6
Solve coth(z) = 1.
Hint, Solution
273

Exercise 7.7
Solve 2 ∈2z. That is, for what values of z is 2 one of the values of 2z? Derive this result then verify your answer by
evaluating 2z for the solutions that your ﬁnd.
Hint, Solution
Exercise 7.8
Solve 1 ∈1z. That is, for what values of z is 1 one of the values of 1z? Derive this result then verify your answer by
evaluating 1z for the solutions that your ﬁnd.
Hint, Solution
Logarithmic Identities
Exercise 7.9
Show that if ℜ(z1) > 0 and ℜ(z2) > 0 then
Log(z1z2) = Log(z1) + Log(z2)
and illustrate that this relationship does not hold in general.
Hint, Solution
Exercise 7.10
Find the fallacy in the following arguments:
1. log(−1) = log
  1
−1

= log(1) −log(−1) = −log(−1), therefore, log(−1) = 0.
2. 1 = 11/2 = ((−1)(−1))1/2 = (−1)1/2(−1)1/2 = ıı = −1, therefore, 1 = −1.
Hint, Solution
Exercise 7.11
Write the following expressions in modulus-argument or Cartesian form. Denote any multi-valuedness explicitly.
22/5,
31+ı,
√
3 −ı
1/4
,
1ı/4.
Hint, Solution
274

Exercise 7.12
Solve cos z = 69.
Hint, Solution
Exercise 7.13
Solve cot z = ı47.
Hint, Solution
Exercise 7.14
Determine all values of
1. log(−ı)
2. (−ı)−ı
3. 3π
4. log(log(ı))
and plot them in the complex plane.
Hint, Solution
Exercise 7.15
Evaluate and plot the following in the complex plane:
1. (cosh(ıπ))ı2
2. log

1
1 + ı

3. arctan(ı3)
Hint, Solution
275

Exercise 7.16
Determine all values of ıı and log ((1 + ı)ıπ) and plot them in the complex plane.
Hint, Solution
Exercise 7.17
Find all z for which
1. ez = ı
2. cos z = sin z
3. tan2 z = −1
Hint, Solution
Exercise 7.18
Prove the following identities and identify the branch points of the functions in the extended complex plane.
1. arctan(z) = ı
2 log
ı + z
ı −z

2. arctanh(z) = 1
2 log
1 + z
1 −z

3. arccosh(z) = log

z +
 z2 −1
1/2
Hint, Solution
Branch Points and Branch Cuts
Exercise 7.19
Identify the branch points of the function
f(z) = log
z(z + 1)
z −1

276

and introduce appropriate branch cuts to ensure that the function is single-valued.
Hint, Solution
Exercise 7.20
Identify all the branch points of the function
w = f(z) =
 z3 + z2 −6z
1/2
in the extended complex plane. Give a polar description of f(z) and specify branch cuts so that your choice of angles
gives a single-valued function that is continuous at z = −1 with f(−1) = −
√
6. Sketch the branch cuts in the
stereographic projection.
Hint, Solution
Exercise 7.21
Consider the mapping w = f(z) = z1/3 and the inverse mapping z = g(w) = w3.
1. Describe the multiple-valuedness of f(z).
2. Describe a region of the w-plane that g(w) maps one-to-one to the whole z-plane.
3. Describe and attempt to draw a Riemann surface on which f(z) is single-valued and to which g(w) maps one-
to-one. Comment on the misleading nature of your picture.
4. Identify the branch points of f(z) and introduce a branch cut to make f(z) single-valued.
Hint, Solution
Exercise 7.22
Determine the branch points of the function
f(z) =
 z3 −1
1/2 .
Construct cuts and deﬁne a branch so that z = 0 and z = −1 do not lie on a cut, and such that f(0) = −ı. What is
f(−1) for this branch?
Hint, Solution
277

Exercise 7.23
Determine the branch points of the function
w(z) = ((z −1)(z −6)(z + 2))1/2
Construct cuts and deﬁne a branch so that z = 4 does not lie on a cut, and such that w = ı6 when z = 4.
Hint, Solution
Exercise 7.24
Give the number of branches and locations of the branch points for the functions
1. cos
 z1/2
2. (z + ı)−z
Hint, Solution
Exercise 7.25
Find the branch points of the following functions in the extended complex plane, (the complex plane including the point
at inﬁnity).
1.
 z2 + 1
1/2
2.
 z3 −z
1/2
3. log
 z2 −1

4. log
z + 1
z −1

Introduce branch cuts to make the functions single valued.
Hint, Solution
278

Exercise 7.26
Find all branch points and introduce cuts to make the following functions single-valued: For the ﬁrst function, choose
cuts so that there is no cut within the disk |z| < 2.
1. f(z) =
 z3 + 8
1/2
2. f(z) = log
 
5 +
z + 1
z −1
1/2!
3. f(z) = (z + ı3)1/2
Hint, Solution
Exercise 7.27
Let f(z) have branch points at z = 0 and z = ±ı, but nowhere else in the extended complex plane. How does the
value and argument of f(z) change while traversing the contour in Figure 7.30? Does the branch cut in Figure 7.30
make the function single-valued?
Figure 7.30: Contour Around the Branch Points and Branch Cut.
Hint, Solution
279

Exercise 7.28
Let f(z) be analytic except for no more than a countably inﬁnite number of singularities. Suppose that f(z) has only
one branch point in the ﬁnite complex plane. Does f(z) have a branch point at inﬁnity? Now suppose that f(z) has
two or more branch points in the ﬁnite complex plane. Does f(z) have a branch point at inﬁnity?
Hint, Solution
Exercise 7.29
Find all branch points of (z4 + 1)1/4 in the extended complex plane. Which of the branch cuts in Figure 7.31 make the
function single-valued.
Figure 7.31: Four Candidate Sets of Branch Cuts for (z4 + 1)1/4
Hint, Solution
Exercise 7.30
Find the branch points of
f(z) =

z
z2 + 1
1/3
in the extended complex plane. Introduce branch cuts that make the function single-valued and such that the function
is deﬁned on the positive real axis. Deﬁne a branch such that f(1) = 1/
3√
2. Write down an explicit formula for the
value of the branch. What is f(1 + ı)? What is the value of f(z) on either side of the branch cuts?
Hint, Solution
280

Exercise 7.31
Find all branch points of
f(z) = ((z −1)(z −2)(z −3))1/2
in the extended complex plane. Which of the branch cuts in Figure 7.32 will make the function single-valued. Using
the ﬁrst set of branch cuts in this ﬁgure deﬁne a branch on which f(0) = ı
√
6. Write out an explicit formula for the
value of the function on this branch.
Figure 7.32: Four Candidate Sets of Branch Cuts for ((z −1)(z −2)(z −3))1/2
Hint, Solution
Exercise 7.32
Determine the branch points of the function
w =
  z2 −2

(z + 2)
1/3 .
Construct and deﬁne a branch so that the resulting cut is one line of ﬁnite extent and w(2) = 2. What is w(−3) for
this branch? What are the limiting values of w on either side of the branch cut?
Hint, Solution
281

Exercise 7.33
Construct the principal branch of arccos(z). (Arccos(z) has the property that if x ∈[−1, 1] then Arccos(x) ∈[0, π].
In particular, Arccos(0) = π
2).
Hint, Solution
Exercise 7.34
Find the branch points of
 z1/2 −1
1/2 in the ﬁnite complex plane.
Introduce branch cuts to make the function
single-valued.
Hint, Solution
Exercise 7.35
For the linkage illustrated in Figure 7.33, use complex variables to outline a scheme for expressing the angular position,
velocity and acceleration of arm c in terms of those of arm a. (You needn’t work out the equations.)
θ
φ
a
b
c
l
Figure 7.33: A linkage
Hint, Solution
Exercise 7.36
Find the image of the strip |ℜ(z)| < 1 and of the strip 1 < ℑ(z) < 2 under the transformations:
1. w = 2z2
282

2. w = z+1
z−1
Hint, Solution
Exercise 7.37
Locate and classify all the singularities of the following functions:
1. (z + 1)1/2
z + 2
2. cos

1
1 + z

3.
1
(1 −ez)2
In each case discuss the possibility of a singularity at the point ∞.
Hint, Solution
Exercise 7.38
Describe how the mapping w = sinh(z) transforms the inﬁnite strip −∞< x < ∞, 0 < y < π into the w-plane. Find
cuts in the w-plane which make the mapping continuous both ways. What are the images of the lines (a) y = π/4; (b)
x = 1?
Hint, Solution
283

7.10
Hints
Cartesian and Modulus-Argument Form
Hint 7.1
Hint 7.2
Trigonometric Functions
Hint 7.3
Recall that sin(z) = 1
ı2 (eız −e−ız). Use Result 6.3.1 to convert between Cartesian and modulus-argument form.
Hint 7.4
Write ez in polar form.
Hint 7.5
The exponential is an increasing function for real variables.
Hint 7.6
Write the hyperbolic cotangent in terms of exponentials.
Hint 7.7
Write out the multi-valuedness of 2z. There is a doubly-inﬁnite set of solutions to this problem.
Hint 7.8
Write out the multi-valuedness of 1z.
Logarithmic Identities
284

Hint 7.9
Hint 7.10
Write out the multi-valuedness of the expressions.
Hint 7.11
Do the exponentiations in polar form.
Hint 7.12
Write the cosine in terms of exponentials. Multiply by eız to get a quadratic equation for eız.
Hint 7.13
Write the cotangent in terms of exponentials. Get a quadratic equation for eız.
Hint 7.14
Hint 7.15
Hint 7.16
ıı has an inﬁnite number of real, positive values.
ıı = eı log ı.
log ((1 + ı)ıπ) has a doubly inﬁnite set of values.
log ((1 + ı)ıπ) = log(exp(ıπ log(1 + ı))).
Hint 7.17
Hint 7.18
Branch Points and Branch Cuts
285

Hint 7.19
Hint 7.20
Hint 7.21
Hint 7.22
Hint 7.23
Hint 7.24
Hint 7.25
1. (z2 + 1)1/2 = (z −ı)1/2(z + ı)1/2
2. (z3 −z)1/2 = z1/2(z −1)1/2(z + 1)1/2
3. log (z2 −1) = log(z −1) + log(z + 1)
4. log
  z+1
z−1

= log(z + 1) −log(z −1)
Hint 7.26
Hint 7.27
Reverse the orientation of the contour so that it encircles inﬁnity and does not contain any branch points.
286

Hint 7.28
Consider a contour that encircles all the branch points in the ﬁnite complex plane. Reverse the orientation of the
contour so that it contains the point at inﬁnity and does not contain any branch points in the ﬁnite complex plane.
Hint 7.29
Factor the polynomial. The argument of z1/4 changes by π/2 on a contour that goes around the origin once in the
positive direction.
Hint 7.30
Hint 7.31
To deﬁne the branch, deﬁne angles from each of the branch points in the ﬁnite complex plane.
Hint 7.32
Hint 7.33
Hint 7.34
Hint 7.35
Hint 7.36
Hint 7.37
287

Hint 7.38
288

7.11
Solutions
Cartesian and Modulus-Argument Form
Solution 7.1
Let w = u + ıv. We consider the strip 2 < x < 3 as composed of vertical lines. Consider the vertical line: z = c + ıy,
y ∈R for constant c. We ﬁnd the image of this line under the mapping.
w = (c + ıy)2
w = c2 −y2 + ı2cy
u = c2 −y2,
v = 2cy
This is a parabola that opens to the left. We can parameterize the curve in terms of v.
u = c2 −1
4c2v2,
v ∈R
The boundaries of the region, x = 2 and x = 3, are respectively mapped to the parabolas:
u = 4 −1
16v2,
v ∈R
and
u = 9 −1
36v2,
v ∈R
We write the image of the mapping in set notation.

w = u + ıv : v ∈R and 4 −1
16v2 < u < 9 −1
36v2

.
See Figure 7.34 for depictions of the strip and its image under the mapping. The mapping is one-to-one. Since the
image of the strip is open and connected, it is a domain.
Solution 7.2
We write the mapping w = z4 in polar coordinates.
w = z4 =
 r eıθ4 = r4 eı4θ
289

-1
1
2
3
4
5
-3
-2
-1
1
2
3
-5
5
10
15
-10
-5
5
10
Figure 7.34: The domain 2 < x < 3 and its image under the mapping w = z2.
Thus we see that
w : {r eıθ | r ≥0, 0 ≤θ < φ} →{r4 eı4θ | r ≥0, 0 ≤θ < φ} = {r eıθ | r ≥0, 0 ≤θ < 4φ}.
We can state this in terms of the argument.
w : {z | 0 ≤arg(z) < φ} →{z | 0 ≤arg(z) < 4φ}
If φ = π/2, the sector will be mapped exactly to the whole complex plane.
Trigonometric Functions
290

Solution 7.3
sin z = 1
ı2
 eız −e−ız
= 1
ı2
 e−y+ıx −ey−ıx
= 1
ı2
 e−y(cos x + ı sin x) −ey(cos x −ı sin x)

= 1
2
 e−y(sin x −ı cos x) + ey(sin x + ı cos x)

= sin x cosh y + ı cos x sinh y
sin z =
q
sin2 x cosh2 y + cos2 x sinh2 y exp(ı arctan(sin x cosh y, cos x sinh y))
=
q
cosh2 y −cos2 x exp(ı arctan(sin x cosh y, cos x sinh y))
=
r
1
2 (cosh(2y) −cos(2x)) exp(ı arctan(sin x cosh y, cos x sinh y))
Solution 7.4
In order that ez be zero, the modulus, ex must be zero. Since ex has no ﬁnite solutions, ez = 0 has no ﬁnite solutions.
Solution 7.5
We write the expressions in terms of Cartesian coordinates.
ez2 =
e(x+ıy)2
=
ex2−y2+ı2xy
= ex2−y2
291

e|z|2 = e|x+ıy|2 = ex2+y2
The exponential function is an increasing function for real variables. Since x2 −y2 ≤x2 + y2, ex2−y2 ≤ex2+y2.
ez2 ≤e|z|2
Equality holds only when y = 0.
Solution 7.6
coth(z) = 1
(ez + e−z) /2
(ez −e−z) /2 = 1
ez + e−z = ez −e−z
e−z = 0
There are no solutions.
Solution 7.7
We write out the multi-valuedness of 2z.
2 ∈2z
eln 2 ∈ez log(2)
eln 2 ∈{ez(ln(2)+ı2πn) | n ∈Z}
ln 2 ∈z{ln 2 + ı2πn + ı2πm | m, n ∈Z}
z =
ln(2) + ı2πm
ln(2) + ı2πn | m, n ∈Z

We verify this solution. Consider m and n to be ﬁxed integers. We express the multi-valuedness in terms of k.
2(ln(2)+ı2πm)/(ln(2)+ı2πn) = e(ln(2)+ı2πm)/(ln(2)+ı2πn) log(2)
= e(ln(2)+ı2πm)/(ln(2)+ı2πn)(ln(2)+ı2πk)
292

For k = n, this has the value, eln(2)+ı2πm = eln(2) = 2.
Solution 7.8
We write out the multi-valuedness of 1z.
1 ∈1z
1 ∈ez log(1)
1 ∈{eız2πn | n ∈Z}
The element corresponding to n = 0 is e0 = 1. Thus 1 ∈1z has the solutions,
z ∈C.
That is, z may be any complex number. We verify this solution.
1z = ez log(1) = eız2πn
For n = 0, this has the value 1.
Logarithmic Identities
Solution 7.9
We write the relationship in terms of the natural logarithm and the principal argument.
Log(z1z2) = Log(z1) + Log(z2)
ln |z1z2| + ı Arg(z1z2) = ln |z1| + ı Arg(z1) + ln |z2| + ı Arg(z2)
Arg(z1z2) = Arg(z1) + Arg(z2)
ℜ(zk) > 0 implies that Arg(zk) ∈(−π/2 . . . π/2). Thus Arg(z1) + Arg(z2) ∈(−π . . . π). In this case the relationship
holds.
The relationship does not hold in general because Arg(z1) + Arg(z2) is not necessarily in the interval (−π . . . π].
Consider z1 = z2 = −1.
Arg((−1)(−1)) = Arg(1) = 0,
Arg(−1) + Arg(−1) = 2π
Log((−1)(−1)) = Log(1) = 0,
Log(−1) + Log(−1) = ı2π
293

Solution 7.10
1. The algebraic manipulations are ﬁne. We write out the multi-valuedness of the logarithms.
log(−1) = log
 1
−1

= log(1) −log(−1) = −log(−1)
{ıπ + ı2πn : n ∈Z} = {ıπ + ı2πn : n ∈Z}
= {ı2πn : n ∈Z} −{ıπ + ı2πn : n ∈Z} = {−ıπ −ı2πn : n ∈Z}
Thus log(−1) = −log(−1). However this does not imply that log(−1) = 0. This is because the logarithm is a
set-valued function log(−1) = −log(−1) is really saying:
{ıπ + ı2πn : n ∈Z} = {−ıπ −ı2πn : n ∈Z}
2. We consider
1 = 11/2 = ((−1)(−1))1/2 = (−1)1/2(−1)1/2 = ıı = −1.
There are three multi-valued expressions above.
11/2 = ±1
((−1)(−1))1/2 = ±1
(−1)1/2(−1)1/2 = (±ı)(±ı) = ±1
Thus we see that the ﬁrst and fourth equalities are incorrect.
1 ̸= 11/2,
(−1)1/2(−1)1/2 ̸= ıı
Solution 7.11
22/5 = 41/5
=
5√
411/5
=
5√
4 eı2nπ/5,
n = 0, 1, 2, 3, 4
294

31+ı = e(1+ı) log 3
= e(1+ı)(ln 3+ı2πn)
= eln 3−2πn eı(ln 3+2πn),
n ∈Z
√
3 −ı
1/4
=
 2 e−ıπ/61/4
=
4√
2 e−ıπ/24 11/4
=
4√
2 eı(πn/2−π/24),
n = 0, 1, 2, 3
1ı/4 = e(ı/4) log 1
= e(ı/4)(ı2πn)
= e−πn/2,
n ∈Z
295

Solution 7.12
cos z = 69
eız + e−ız
2
= 69
eı2z −138 eız +1 = 0
eız = 1
2

138 ±
√
1382 −4

z = −ı log

69 ± 2
√
1190

z = −ı

ln

69 ± 2
√
1190

+ ı2πn

z = 2πn −ı ln

69 ± 2
√
1190

,
n ∈Z
296

Solution 7.13
cot z = ı47
(eız + e−ız) /2
(eız −e−ız) /(ı2) = ı47
eız + e−ız = 47
 eız −e−ız
46 eı2z −48 = 0
ı2z = log 24
23
z = −ı
2 log 24
23
z = −ı
2

ln 24
23 + ı2πn

,
n ∈Z
z = πn −ı
2 ln 24
23,
n ∈Z
Solution 7.14
1.
log(−ı) = ln | −ı| + ı arg(−ı)
= ln(1) + ı

−π
2 + 2πn

,
n ∈Z
log(−ı) = −ıπ
2 + ı2πn,
n ∈Z
These are equally spaced points in the imaginary axis. See Figure 7.35.
2.
(−ı)−ı = e−ı log(−ı)
= e−ı(−ıπ/2+ı2πn),
n ∈Z
297

-1
1
-10
10
Figure 7.35: log(−ı)
(−ı)−ı = e−π/2+2πn,
n ∈Z
These are points on the positive real axis with an accumulation point at the origin. See Figure 7.36.
1
-1
1
Figure 7.36: (−ı)−ı
3.
3π = eπ log(3)
= eπ(ln(3)+ı arg(3))
298

3π = eπ(ln(3)+ı2πn),
n ∈Z
These points all lie on the circle of radius |eπ| centered about the origin in the complex plane. See Figure 7.37.
-10
-5
5
10
-10
-5
5
10
Figure 7.37: 3π
4.
log(log(ı)) = log

ı
π
2 + 2πm

,
m ∈Z
= ln
π
2 + 2πm
 + ı Arg

ı
π
2 + 2πm

+ ı2πn,
m, n ∈Z
= ln
π
2 + 2πm
 + ı sign(1 + 4m)π
2 + ı2πn,
m, n ∈Z
These points all lie in the right half-plane. See Figure 7.38.
299

1
2
3
4
5
-20
-10
10
20
Figure 7.38: log(log(ı))
Solution 7.15
1.
(cosh(ıπ))ı2 =
eıπ + e−ıπ
2
ı2
= (−1)ı2
= eı2 log(−1)
= eı2(ln(1)+ıπ+ı2πn),
n ∈Z
= e−2π(1+2n),
n ∈Z
These are points on the positive real axis with an accumulation point at the origin. See Figure 7.39.
300

1000
-1
1
Figure 7.39: The values of (cosh(ıπ))ı2.
2.
log

1
1 + ı

= −log(1 + ı)
= −log
√
2 eıπ/4
= −1
2 ln(2) −log
 eıπ/4
= −1
2 ln(2) −ıπ/4 + ı2πn,
n ∈Z
These are points on a vertical line in the complex plane. See Figure 7.40.
301

-1
1
-10
10
Figure 7.40: The values of log
  1
1+ı

.
3.
arctan(ı3) = 1
ı2 log
ı −ı3
ı + ı3

= 1
ı2 log

−1
2

= 1
ı2

ln
1
2

+ ıπ + ı2πn

,
n ∈Z
= π
2 + πn + ı
2 ln(2)
These are points on a horizontal line in the complex plane. See Figure 7.41.
302

-5
5
-1
1
Figure 7.41: The values of arctan(ı3).
Solution 7.16
ıı = eı log(ı)
= eı(ln |ı|+ı Arg(ı)+ı2πn),
n ∈Z
= eı(ıπ/2+ı2πn),
n ∈Z
= e−π(1/2+2n),
n ∈Z
These are points on the positive real axis. There is an accumulation point at z = 0. See Figure 7.42.
log ((1 + ı)ıπ) = log
 eıπ log(1+ı)
= ıπ log(1 + ı) + ı2πn,
n ∈Z
= ıπ (ln |1 + ı| + ı Arg(1 + ı) + ı2πm) + ı2πn,
m, n ∈Z
= ıπ
1
2 ln 2 + ıπ
4 + ı2πm

+ ı2πn,
m, n ∈Z
= −π2
1
4 + 2m

+ ıπ
1
2 ln 2 + 2n

,
m, n ∈Z
303

25
50
75
100
-1
1
Figure 7.42: ıı
See Figure 7.43 for a plot.
-40
-20
20
-10
-5
5
10
Figure 7.43: log ((1 + ı)ıπ)
304

Solution 7.17
1.
ez = ı
z = log ı
z = ln |ı| + ı arg(ı)
z = ln(1) + ı
π
2 + 2πn

,
n ∈Z
z = ıπ
2 + ı2πn,
n ∈Z
2. We can solve the equation by writing the cosine and sine in terms of the exponential.
cos z = sin z
eız + e−ız
2
= eız −e−ız
ı2
(1 + ı) eız = (−1 + ı) e−ız
eı2z = −1 + ı
1 + ı
eı2z = ı
ı2z = log(ı)
ı2z = ıπ
2 + ı2πn,
n ∈Z
z = π
4 + πn,
n ∈Z
305

3.
tan2 z = −1
sin2 z = −cos2 z
cos z = ±ı sin z
eız + e−ız
2
= ±ıeız −e−ız
ı2
e−ız = −e−ız
or
eız = −eız
e−ız = 0
or
eız = 0
ey−ıx = 0
or
e−y+ıx = 0
ey = 0
or
e−y = 0
z = ∅
There are no solutions for ﬁnite z.
306

Solution 7.18
1.
w = arctan(z)
z = tan(w)
z = sin(w)
cos(w)
z = (eıw −e−ıw) /(ı2)
(eıw + e−ıw) /2
z eıw +z e−ıw = −ı eıw +ı e−ıw
(ı + z) eı2w = (ı −z)
eıw =
ı −z
ı + z
1/2
w = −ı log
ı −z
ı + z
1/2
arctan(z) = ı
2 log
ı + z
ı −z

We identify the branch points of the arctangent.
arctan(z) = ı
2 (log(ı + z) −log(ı −z))
There are branch points at z = ±ı due to the logarithm terms. We examine the point at inﬁnity with the change
of variables ζ = 1/z.
arctan(1/ζ) = ı
2 log
ı + 1/ζ
ı −1/ζ

arctan(1/ζ) = ı
2 log
ıζ + 1
ıζ −1

307

As ζ →0, the argument of the logarithm term tends to −1 The logarithm does not have a branch point at that
point. Since arctan(1/ζ) does not have a branch point at ζ = 0, arctan(z) does not have a branch point at
inﬁnity.
2.
w = arctanh(z)
z = tanh(w)
z = sinh(w)
cosh(w)
z = (ew −e−w) /2
(ew + e−w) /2
z ew +z e−w = ew −e−w
(z −1) e2w = −z −1
ew =
−z −1
z −1
1/2
w = log
z + 1
1 −z
1/2
arctanh(z) = 1
2 log
1 + z
1 −z

We identify the branch points of the hyperbolic arctangent.
arctanh(z) = 1
2 (log(1 + z) −log(1 −z))
There are branch points at z = ±1 due to the logarithm terms. We examine the point at inﬁnity with the change
308

of variables ζ = 1/z.
arctanh(1/ζ) = 1
2 log
1 + 1/ζ
1 −1/ζ

arctanh(1/ζ) = 1
2 log
ζ + 1
ζ −1

As ζ →0, the argument of the logarithm term tends to −1 The logarithm does not have a branch point at that
point. Since arctanh(1/ζ) does not have a branch point at ζ = 0, arctanh(z) does not have a branch point at
inﬁnity.
3.
w = arccosh(z)
z = cosh(w)
z = ew + e−w
2
e2w −2z ew +1 = 0
ew = z +
 z2 −1
1/2
w = log

z +
 z2 −1
1/2
arccosh(z) = log

z +
 z2 −1
1/2
We identify the branch points of the hyperbolic arc-cosine.
arccosh(z) = log
 z + (z −1)1/2(z + 1)1/2
First we consider branch points due to the square root. There are branch points at z = ±1 due to the square
root terms. If we walk around the singularity at z = 1 and no other singularities, the (z2 −1)1/2 term changes
309

sign. This will change the value of arccosh(z). The same is true for the point z = −1. The point at inﬁnity is
not a branch point for (z2 −1)1/2. We factor the expression to verify this.
 z2 −1
1/2 =
 z21/2  1 −z−21/2
(z2)1/2 does not have a branch point at inﬁnity. It is multi-valued, but it has no branch points. (1 −z−2)1/2 does
not have a branch point at inﬁnity, The argument of the square root function tends to unity there. In summary,
there are branch points at z = ±1 due to the square root. If we walk around either one of the these branch
points. the square root term will change value. If we walk around both of these points, the square root term will
not change value.
Now we consider branch points due to logarithm.
There may be branch points where the argument of the
logarithm vanishes or tends to inﬁnity. We see if the argument of the logarithm vanishes.
z +
 z2 −1
1/2 = 0
z2 = z2 −1
z + (z2 −1)1/2 is non-zero and ﬁnite everywhere in the complex plane. The only possibility for a branch point
in the logarithm term is the point at inﬁnity. We see if the argument of z + (z2 −1)1/2 changes when we walk
around inﬁnity but no other singularity. We consider a circular path with center at the origin and radius greater
than unity. We can either say that this path encloses the two branch points at z = ±1 and no other singularities
or we can say that this path encloses the point at inﬁnity and no other singularities. We examine the value of
the argument of the logarithm on this path.
z +
 z2 −1
1/2 = z +
 z21/2  1 −z−21/2
Neither (z2)1/2 nor (1 −z−2)1/2 changes value as we walk the path. Thus we can use the principal branch of the
square root in the expression.
z +
 z2 −1
1/2 = z ± z
√
1 −z−2 = z

1 ±
√
1 −z−2

310

First consider the “+” branch.
z

1 +
√
1 −z−2

As we walk the path around inﬁnity, the argument of z changes by 2π while the argument of
 1 +
√
1 −z−2
does not change. Thus the argument of z + (z2 −1)1/2 changes by 2π when we go around inﬁnity. This makes
the value of the logarithm change by ı2π. There is a branch point at inﬁnity.
First consider the “−” branch.
z

1 −
√
1 −z−2

= z

1 −

1 −1
2z−2 + O
 z−4
= z
1
2z−2 + O
 z−4
= 1
2z−1  1 + O
 z−2
As we walk the path around inﬁnity, the argument of z−1 changes by −2π while the argument of (1 + O (z−2))
does not change. Thus the argument of z + (z2 −1)1/2 changes by −2π when we go around inﬁnity. This makes
the value of the logarithm change by −ı2π. Again we conclude that there is a branch point at inﬁnity.
For the sole purpose of overkill, let’s repeat the above analysis from a geometric viewpoint. Again we consider
the possibility of a branch point at inﬁnity due to the logarithm. We walk along the circle shown in the ﬁrst plot
of Figure 7.44. Traversing this path, we go around inﬁnity, but no other singularities. We consider the mapping
w = z +(z2 −1)1/2. Depending on the branch of the square root, the circle is mapped to one one of the contours
shown in the second plot. For each branch, the argument of w changes by ±2π as we traverse the circle in the
z-plane. Therefore the value of arccosh(z) = log

z + (z2 −1)1/2
changes by ±ı2π as we traverse the circle.
We again conclude that there is a branch point at inﬁnity due to the logarithm.
To summarize: There are branch points at z = ±1 due to the square root and a branch point at inﬁnity due to
the logarithm.
Branch Points and Branch Cuts
311

-1
1
-1
1
-1
1
-1
1
Figure 7.44: The mapping of a circle under w = z + (z2 −1)1/2.
Solution 7.19
We expand the function to diagnose the branch points in the ﬁnite complex plane.
f(z) = log
z(z + 1)
z −1

= log(z) + log(z + 1) −log(z −1)
The are branch points at z = −1, 0, 1. Now we examine the point at inﬁnity. We make the change of variables z = 1/ζ.
f
1
ζ

= log
(1/ζ)(1/ζ + 1)
(1/ζ −1)

= log
1
ζ
(1 + ζ
1 −ζ

= log(1 + ζ) −log(1 −ζ) −log(ζ)
log(ζ) has a branch point at ζ = 0. The other terms do not have branch points there. Since f(1/ζ) has a branch point
at ζ = 0 f(z) has a branch point at inﬁnity.
Note that in walking around either z = −1 or z = 0 once in the positive direction, the argument of z(z +1)/(z −1)
changes by 2π. In walking around z = 1, the argument of z(z + 1)/(z −1) changes by −2π. This argument does not
312

change if we walk around both z = 0 and z = 1. Thus we put a branch cut between z = 0 and z = 1. Next be put
a branch cut between z = −1 and the point at inﬁnity. This prevents us from walking around either of these branch
points. These two branch cuts separate the branches of the function. See Figure 7.45
-3
-2
-1
1
2
Figure 7.45: Branch cuts for log

z(z+1)
z−1

Solution 7.20
First we factor the function.
f(z) = (z(z + 3)(z −2))1/2 = z1/2(z + 3)1/2(z −2)1/2
There are branch points at z = −3, 0, 2. Now we examine the point at inﬁnity.
f
1
ζ

=
1
ζ
1
ζ + 3
 1
ζ −2
1/2
= ζ−3/2((1 + 3ζ)(1 −2ζ))1/2
Since ζ−3/2 has a branch point at ζ = 0 and the rest of the terms are analytic there, f(z) has a branch point at inﬁnity.
Consider the set of branch cuts in Figure 7.46. These cuts do not permit us to walk around any single branch point.
We can only walk around none or all of the branch points, (which is the same thing). The cuts can be used to deﬁne
a single-valued branch of the function.
313

-4
-2
2
4
-3
-2
-1
1
2
3
Figure 7.46: Branch Cuts for (z3 + z2 −6z)1/2
Now to deﬁne the branch. We make a choice of angles.
z + 3 = r1 eıθ1,
−π < θ1 < π
z = r2 eıθ2,
−π
2 < θ2 < 3π
2
z −2 = r3 eıθ3,
0 < θ3 < 2π
The function is
f(z) =
 r1 eıθ1 r2 eıθ2 r3 eıθ31/2 = √r1r2r3 eı(θ1+θ2+θ3)/2 .
We evaluate the function at z = −1.
f(−1) =
p
(2)(1)(3) eı(0+π+π)/2 = −
√
6
We see that our choice of angles gives us the desired branch.
The stereographic projection is the projection from the complex plane onto a unit sphere with south pole at the
314

origin. The point z = x + ıy is mapped to the point (X, Y, Z) on the sphere with
X =
4x
|z|2 + 4,
Y =
4y
|z|2 + 4,
Z =
2|z|2
|z|2 + 4.
Figure 7.47 ﬁrst shows the branch cuts and their stereographic projections and then shows the stereographic projections
alone.
-4
0
4
-4
0
4
0
2
-4
0
4
-1
0
1
-1
0 1
0
1
2
1
0
1
0
Figure 7.47: Branch cuts for (z3 + z2 −6z)1/2 and their stereographic projections.
Solution 7.21
1. For each value of z, f(z) = z1/3 has three values.
f(z) = z1/3 =
3√z eık2π/3,
k = 0, 1, 2
2.
g(w) = w3 = |w|3 eı3 arg(w)
315

Any sector of the w plane of angle 2π/3 maps one-to-one to the whole z-plane.
g :

r eıθ | r ≥0, θ0 ≤θ < θ0 + 2π/3
	
7→

r3 eı3θ | r ≥0, θ0 ≤θ < θ0 + 2π/3
	
g :

r eıθ | r ≥0, θ0 ≤θ < θ0 + 2π/3
	
7→

r eıθ | r ≥0, 3θ0 ≤θ < 3θ0 + 2π
	
g :

r eıθ | r ≥0, θ0 ≤θ < θ0 + 2π/3
	
7→C
See Figure 7.48 to see how g(w) maps the sector 0 ≤θ < 2π/3.
3. See Figure 7.49 for a depiction of the Riemann surface for f(z) = z1/3. We show two views of the surface and a
curve that traces the edge of the shown portion of the surface. The depiction is misleading because the surface
is not self-intersecting. We would need four dimensions to properly visualize the this Riemann surface.
4. f(z) = z1/3 has branch points at z = 0 and z = ∞. Any branch cut which connects these two points would
prevent us from walking around the points singly and would thus separate the branches of the function. For
example, we could put a branch cut on the negative real axis. Deﬁning the angle −π < θ < π for the mapping
f
 r eıθ
=
3√r eıθ/3
deﬁnes a single-valued branch of the function.
Solution 7.22
The cube roots of 1 are

1, eı2π/3, eı4π/3	
=
(
1, −1 + ı
√
3
2
, −1 −ı
√
3
2
)
.
We factor the polynomial.
 z3 −1
1/2 = (z −1)1/2
 
z + 1 −ı
√
3
2
!1/2  
z + 1 + ı
√
3
2
!1/2
There are branch points at each of the cube roots of unity.
z =
(
1, −1 + ı
√
3
2
, −1 −ı
√
3
2
)
316

Figure 7.48: The mapping g(w) = w3 maps the sector 0 ≤θ < 2π/3 one-to-one to the whole z-plane.
Now we examine the point at inﬁnity. We make the change of variables z = 1/ζ.
f(1/ζ) =
 1/ζ3 −1
1/2 = ζ−3/2  1 −ζ31/2
ζ−3/2 has a branch point at ζ = 0, while (1 −ζ3)1/2 is not singular there. Since f(1/ζ) has a branch point at ζ = 0,
f(z) has a branch point at inﬁnity.
317

Figure 7.49: Riemann surface for f(z) = z1/3.
There are several ways of introducing branch cuts to separate the branches of the function. The easiest approach is
to put a branch cut from each of the three branch points in the ﬁnite complex plane out to the branch point at inﬁnity.
See Figure 7.50a. Clearly this makes the function single valued as it is impossible to walk around any of the branch
points. Another approach is to have a branch cut from one of the branch points in the ﬁnite plane to the branch point
at inﬁnity and a branch cut connecting the remaining two branch points. See Figure 7.50bcd. Note that in walking
around any one of the ﬁnite branch points, (in the positive direction), the argument of the function changes by π. This
means that the value of the function changes by eıπ, which is to say the value of the function changes sign. In walking
around any two of the ﬁnite branch points, (again in the positive direction), the argument of the function changes by
2π. This means that the value of the function changes by eı2π, which is to say that the value of the function does not
change. This demonstrates that the latter branch cut approach makes the function single-valued.
Now we construct a branch. We will use the branch cuts in Figure 7.50a. We introduce variables to measure radii
and angles from the three ﬁnite branch points.
z −1 = r1 eıθ1,
0 < θ1 < 2π
z + 1 −ı
√
3
2
= r2 eıθ2,
−2π
3 < θ2 < π
3
z + 1 + ı
√
3
2
= r3 eıθ3,
−π
3 < θ3 < 2π
3
318

a
b
c
d
Figure 7.50: (z3 −1)1/2
We compute f(0) to see if it has the desired value.
f(z) = √r1r2r3 eı(θ1+θ2+θ3)/2
f(0) = eı(π−π/3+π/3)/2 = ı
Since it does not have the desired value, we change the range of θ1.
z −1 = r1 eıθ1,
2π < θ1 < 4π
f(0) now has the desired value.
f(0) = eı(3π−π/3+π/3)/2 = −ı
We compute f(−1).
f(−1) =
√
2 eı(3π−2π/3+2π/3)/2 = −ı
√
2
Solution 7.23
First we factor the function.
w(z) = ((z + 2)(z −1)(z −6))1/2 = (z + 2)1/2(z −1)1/2(z −6)1/2
There are branch points at z = −2, 1, 6. Now we examine the point at inﬁnity.
w
1
ζ

=
1
ζ + 2
 1
ζ −1
 1
ζ −6
1/2
= ζ−3/2

1 + 2
ζ
 
1 −1
ζ
 
1 −6
ζ
1/2
319

Since ζ−3/2 has a branch point at ζ = 0 and the rest of the terms are analytic there, w(z) has a branch point at inﬁnity.
Consider the set of branch cuts in Figure 7.51. These cuts let us walk around the branch points at z = −2 and
z = 1 together or if we change our perspective, we would be walking around the branch points at z = 6 and z = ∞
together. Consider a contour in this cut plane that encircles the branch points at z = −2 and z = 1. Since the
argument of (z −z0)1/2 changes by π when we walk around z0, the argument of w(z) changes by 2π when we traverse
the contour. Thus the value of the function does not change and it is a valid set of branch cuts.
Figure 7.51: Branch Cuts for ((z + 2)(z −1)(z −6))1/2
Now to deﬁne the branch. We make a choice of angles.
z + 2 = r1 eıθ1,
θ1 = θ2 for z ∈(1 . . . 6),
z −1 = r2 eıθ2,
θ2 = θ1 for z ∈(1 . . . 6),
z −6 = r3 eıθ3,
0 < θ3 < 2π
The function is
w(z) =
 r1 eıθ1 r2 eıθ2 r3 eıθ31/2 = √r1r2r3 eı(θ1+θ2+θ3)/2 .
We evaluate the function at z = 4.
w(4) =
p
(6)(3)(2) eı(2πn+2πn+π)/2 = ı6
We see that our choice of angles gives us the desired branch.
320

Solution 7.24
1.
cos
 z1/2
= cos
 ±√z

= cos
 √z

This is a single-valued function. There are no branch points.
2.
(z + ı)−z = e−z log(z+ı)
= e−z(ln |z+ı|+ı Arg(z+ı)+ı2πn),
n ∈Z
There is a branch point at z = −ı. There are an inﬁnite number of branches.
Solution 7.25
1.
f(z) =
 z2 + 1
1/2 = (z + ı)1/2(z −ı)1/2
We see that there are branch points at z = ±ı. To examine the point at inﬁnity, we substitute z = 1/ζ and
examine the point ζ = 0.
 1
ζ
2
+ 1
!1/2
=
1
(ζ2)1/2
 1 + ζ21/2
Since there is no branch point at ζ = 0, f(z) has no branch point at inﬁnity.
A branch cut connecting z = ±ı would make the function single-valued. We could also accomplish this with two
branch cuts starting z = ±ı and going to inﬁnity.
2.
f(z) =
 z3 −z
1/2 = z1/2(z −1)1/2(z + 1)1/2
There are branch points at z = −1, 0, 1. Now we consider the point at inﬁnity.
f
1
ζ

=
 1
ζ
3
−1
ζ
!1/2
= ζ−3/2  1 −ζ21/2
321

There is a branch point at inﬁnity.
One can make the function single-valued with three branch cuts that start at z = −1, 0, 1 and each go to inﬁnity.
We can also make the function single-valued with a branch cut that connects two of the points z = −1, 0, 1 and
another branch cut that starts at the remaining point and goes to inﬁnity.
3.
f(z) = log
 z2 −1

= log(z −1) + log(z + 1)
There are branch points at z = ±1.
f
1
ζ

= log
 1
ζ2 −1

= log
 ζ−2
+ log
 1 −ζ2
log (ζ−2) has a branch point at ζ = 0.
log
 ζ−2
= ln
ζ−2 + ı arg
 ζ−2
= ln
ζ−2 −ı2 arg(ζ)
Every time we walk around the point ζ = 0 in the positive direction, the value of the function changes by −ı4π.
f(z) has a branch point at inﬁnity.
We can make the function single-valued by introducing two branch cuts that start at z = ±1 and each go to
inﬁnity.
4.
f(z) = log
z + 1
z −1

= log(z + 1) −log(z −1)
There are branch points at z = ±1.
f
1
ζ

= log
1/ζ + 1
1/ζ −1

= log
1 + ζ
1 −ζ

There is no branch point at ζ = 0. f(z) has no branch point at inﬁnity.
322

We can make the function single-valued by introducing two branch cuts that start at z = ±1 and each go to
inﬁnity. We can also make the function single-valued with a branch cut that connects the points z = ±1. This is
because log(z + 1) and −log(z −1) change by ı2π and −ı2π, respectively, when you walk around their branch
points once in the positive direction.
Solution 7.26
1. The cube roots of −8 are

−2, −2 eı2π/3, −2 eı4π/3	
=
n
−2, 1 + ı
√
3, 1 −ı
√
3
o
.
Thus we can write
 z3 + 8
1/2 = (z + 2)1/2 
z −1 −ı
√
3
1/2 
z −1 + ı
√
3
1/2
.
There are three branch points on the circle of radius 2.
z =
n
−2, 1 + ı
√
3, 1 −ı
√
3
o
.
We examine the point at inﬁnity.
f(1/ζ) =
 1/ζ3 + 8
1/2 = ζ−3/2  1 + 8ζ31/2
Since f(1/ζ) has a branch point at ζ = 0, f(z) has a branch point at inﬁnity.
There are several ways of introducing branch cuts outside of the disk |z| < 2 to separate the branches of the
function. The easiest approach is to put a branch cut from each of the three branch points in the ﬁnite complex
plane out to the branch point at inﬁnity. See Figure 7.52a. Clearly this makes the function single valued as it
is impossible to walk around any of the branch points. Another approach is to have a branch cut from one of
the branch points in the ﬁnite plane to the branch point at inﬁnity and a branch cut connecting the remaining
two branch points. See Figure 7.52bcd. Note that in walking around any one of the ﬁnite branch points, (in
the positive direction), the argument of the function changes by π. This means that the value of the function
changes by eıπ, which is to say the value of the function changes sign. In walking around any two of the ﬁnite
323

a
b
c
d
Figure 7.52: (z3 + 8)1/2
branch points, (again in the positive direction), the argument of the function changes by 2π. This means that
the value of the function changes by eı2π, which is to say that the value of the function does not change. This
demonstrates that the latter branch cut approach makes the function single-valued.
2.
f(z) = log
 
5 +
z + 1
z −1
1/2!
First we deal with the function
g(z) =
z + 1
z −1
1/2
Note that it has branch points at z = ±1. Consider the point at inﬁnity.
g(1/ζ) =
1/ζ + 1
1/ζ −1
1/2
=
1 + ζ
1 −ζ
1/2
Since g(1/ζ) has no branch point at ζ = 0, g(z) has no branch point at inﬁnity. This means that if we walk
around both of the branch points at z = ±1, the function does not change value. We can verify this with another
method: When we walk around the point z = −1 once in the positive direction, the argument of z + 1 changes
324

by 2π, the argument of (z + 1)1/2 changes by π and thus the value of (z + 1)1/2 changes by eıπ = −1. When we
walk around the point z = 1 once in the positive direction, the argument of z −1 changes by 2π, the argument
of (z −1)−1/2 changes by −π and thus the value of (z −1)−1/2 changes by e−ıπ = −1. f(z) has branch points
at z = ±1. When we walk around both points z = ±1 once in the positive direction, the value of
  z+1
z−1
1/2 does
not change. Thus we can make the function single-valued with a branch cut which enables us to walk around
either none or both of these branch points. We put a branch cut from −1 to 1 on the real axis.
f(z) has branch points where
5 +
z + 1
z −1
1/2
is either zero or inﬁnite. The only place in the extended complex plane where the expression becomes inﬁnite is
at z = 1. Now we look for the zeros.
5 +
z + 1
z −1
1/2
= 0
z + 1
z −1
1/2
= −5
z + 1
z −1 = 25
z + 1 = 25z −25
z = 13
12
Note that
13/12 + 1
13/12 −1
1/2
= 251/2 = ±5.
On one branch, (which we call the positive branch), of the function g(z) the quantity
5 +
z + 1
z −1
1/2
325

is always nonzero. On the other (negative) branch of the function, this quantity has a zero at z = 13/12.
The logarithm introduces branch points at z = 1 on both the positive and negative branch of g(z). It introduces
a branch point at z = 13/12 on the negative branch of g(z). To determine if additional branch cuts are needed
to separate the branches, we consider
w = 5 +
z + 1
z −1
1/2
and see where the branch cut between ±1 gets mapped to in the w plane. We rewrite the mapping.
w = 5 +

1 +
2
z −1
1/2
The mapping is the following sequence of simple transformations:
(a) z 7→z −1
(b) z 7→1
z
(c) z 7→2z
(d) z 7→z + 1
(e) z 7→z1/2
(f) z 7→z + 5
We show these transformations graphically below.
-1
1
z 7→z −1
-2
0
z 7→1
z
-1/2
z 7→2z
-1
z 7→z + 1
326

0
z 7→z1/2
z 7→z + 5
For the positive branch of g(z), the branch cut is mapped to the line x = 5 and the z plane is mapped to the
half-plane x > 5. log(w) has branch points at w = 0 and w = ∞. It is possible to walk around only one of these
points in the half-plane x > 5. Thus no additional branch cuts are needed in the positive sheet of g(z).
For the negative branch of g(z), the branch cut is mapped to the line x = 5 and the z plane is mapped to the
half-plane x < 5. It is possible to walk around either w = 0 or w = ∞alone in this half-plane. Thus we need an
additional branch cut. On the negative sheet of g(z), we put a branch cut beteen z = 1 and z = 13/12. This
puts a branch cut between w = ∞and w = 0 and thus separates the branches of the logarithm.
Figure 7.53 shows the branch cuts in the positive and negative sheets of g(z).
Im(z)
Re(z)
g(13/12)=-5
Im(z)
Re(z)
g(13/12)=5
Figure 7.53: The branch cuts for f(z) = log

5 +
  z+1
z−1
1/2
.
3. The function f(z) = (z +ı3)1/2 has a branch point at z = −ı3. The function is made single-valued by connecting
this point and the point at inﬁnity with a branch cut.
Solution 7.27
Note that the curve with opposite orientation goes around inﬁnity in the positive direction and does not enclose any
branch points. Thus the value of the function does not change when traversing the curve, (with either orientation, of
327

course). This means that the argument of the function must change my an integer multiple of 2π. Since the branch
cut only allows us to encircle all three or none of the branch points, it makes the function single valued.
Solution 7.28
We suppose that f(z) has only one branch point in the ﬁnite complex plane. Consider any contour that encircles this
branch point in the positive direction. f(z) changes value if we traverse the contour. If we reverse the orientation of
the contour, then it encircles inﬁnity in the positive direction, but contains no branch points in the ﬁnite complex plane.
Since the function changes value when we traverse the contour, we conclude that the point at inﬁnity must be a branch
point. If f(z) has only a single branch point in the ﬁnite complex plane then it must have a branch point at inﬁnity.
If f(z) has two or more branch points in the ﬁnite complex plane then it may or may not have a branch point at
inﬁnity. This is because the value of the function may or may not change on a contour that encircles all the branch
points in the ﬁnite complex plane.
Solution 7.29
First we factor the function,
f(z) =
 z4 + 1
1/4 =

z −1 + ı
√
2
1/4 
z −−1 + ı
√
2
1/4 
z −−1 −ı
√
2
1/4 
z −1 −ı
√
2
1/4
.
There are branch points at z = ±1±ı
√
2 . We make the substitution z = 1/ζ to examine the point at inﬁnity.
f
1
ζ

=
 1
ζ4 + 1
1/4
=
1
(ζ4)1/4
 1 + ζ41/4
 ζ1/44 has a removable singularity at the point ζ = 0, but no branch point there. Thus (z4 + 1)1/4 has no branch
point at inﬁnity.
Note that the argument of (z4 −z0)1/4 changes by π/2 on a contour that goes around the point z0 once in the
positive direction. The argument of (z4 + 1)1/4 changes by nπ/2 on a contour that goes around n of its branch points.
328

Thus any set of branch cuts that permit you to walk around only one, two or three of the branch points will not make
the function single valued. A set of branch cuts that permit us to walk around only zero or all four of the branch points
will make the function single-valued. Thus we see that the ﬁrst two sets of branch cuts in Figure 7.31 will make the
function single-valued, while the remaining two will not.
Consider the contour in Figure ??. There are two ways to see that the function does not change value while
traversing the contour. The ﬁrst is to note that each of the branch points makes the argument of the function increase
by π/2. Thus the argument of (z4 + 1)1/4 changes by 4(π/2) = 2π on the contour. This means that the value of the
function changes by the factor eı2π = 1. If we change the orientation of the contour, then it is a contour that encircles
inﬁnity once in the positive direction. There are no branch points inside the this contour with opposite orientation.
(Recall that the inside of a contour lies to your left as you walk around it.) Since there are no branch points inside this
contour, the function cannot change value as we traverse it.
Solution 7.30
f(z) =

z
z2 + 1
1/3
= z1/3(z −ı)−1/3(z + ı)−1/3
There are branch points at z = 0, ±ı.
f
1
ζ

=

1/ζ
(1/ζ)2 + 1
1/3
=
ζ1/3
(1 + ζ2)1/3
There is a branch point at ζ = 0. f(z) has a branch point at inﬁnity.
We introduce branch cuts from z = 0 to inﬁnity on the negative real axis, from z = ı to inﬁnity on the positive
imaginary axis and from z = −ı to inﬁnity on the negative imaginary axis. As we cannot walk around any of the branch
points, this makes the function single-valued.
We deﬁne a branch by deﬁning angles from the branch points. Let
z = r eıθ
−π < θ < π,
(z −ı) = s eıφ
−3π/2 < φ < π/2,
(z + ı) = t eıψ
−π/2 < ψ < 3π/2.
329

With
f(z) = z1/3(z −ı)−1/3(z + ı)−1/3
=
3√r eıθ/3 1
3√s e−ıφ/3 1
3√
t
e−ıψ/3
=
3
r r
st eı(θ−φ−ψ)/3
we have an explicit formula for computing the value of the function for this branch. Now we compute f(1) to see if we
chose the correct ranges for the angles. (If not, we’ll just change one of them.)
f(1) =
3
s
1
√
2
√
2
eı(0−π/4−(−π/4))/3 =
1
3√
2
We made the right choice for the angles. Now to compute f(1 + ı).
f(1 + ı) =
3
s √
2
1
√
5
eı(π/4−0−Arctan(2))/3 =
6
r
2
5 eı(π/4−Arctan(2))/3
Consider the value of the function above and below the branch cut on the negative real axis. Above the branch cut the
function is
f(−x + ı0) =
3
r
x
√
x2 + 1
√
x2 + 1
eı(π−φ−ψ)/3
Note that φ = −ψ so that
f(−x + ı0) =
3
r
x
x2 + 1 eıπ/3 =
3
r
x
x2 + 1
1 + ı
√
3
2
.
Below the branch cut θ = −π and
f(−x −ı0) =
3
r
x
x2 + 1 eı(−π)/3 =
3
r
x
x2 + 1
1 −ı
√
3
2
.
330

For the branch cut along the positive imaginary axis,
f(ıy + 0) =
3
r
y
(y −1)(y + 1) eı(π/2−π/2−π/2)/3
=
3
r
y
(y −1)(y + 1) e−ıπ/6
=
3
r
y
(y −1)(y + 1)
√
3 −ı
2
,
f(ıy −0) =
3
r
y
(y −1)(y + 1) eı(π/2−(−3π/2)−π/2)/3
=
3
r
y
(y −1)(y + 1) eıπ/2
= ı 3
r
y
(y −1)(y + 1).
For the branch cut along the negative imaginary axis,
f(−ıy + 0) =
3
r
y
(y + 1)(y −1) eı(−π/2−(−π/2)−(−π/2))/3
=
3
r
y
(y + 1)(y −1) eıπ/6
=
3
r
y
(y + 1)(y −1)
√
3 + ı
2
,
331

f(−ıy −0) =
3
r
y
(y + 1)(y −1) eı(−π/2−(−π/2)−(3π/2))/3
=
3
r
y
(y + 1)(y −1) e−ıπ/2
= −ı 3
r
y
(y + 1)(y −1).
Solution 7.31
First we factor the function.
f(z) = ((z −1)(z −2)(z −3))1/2 = (z −1)1/2(z −2)1/2(z −3)1/2
There are branch points at z = 1, 2, 3. Now we examine the point at inﬁnity.
f
1
ζ

=
1
ζ −1
 1
ζ −2
 1
ζ −3
1/2
= ζ−3/2

1 −1
ζ
 
1 −2
ζ
 
1 −3
ζ
1/2
Since ζ−3/2 has a branch point at ζ = 0 and the rest of the terms are analytic there, f(z) has a branch point at inﬁnity.
The ﬁrst two sets of branch cuts in Figure 7.32 do not permit us to walk around any of the branch points, including
the point at inﬁnity, and thus make the function single-valued. The third set of branch cuts lets us walk around the
branch points at z = 1 and z = 2 together or if we change our perspective, we would be walking around the branch
points at z = 3 and z = ∞together. Consider a contour in this cut plane that encircles the branch points at z = 1
and z = 2. Since the argument of (z −z0)1/2 changes by π when we walk around z0, the argument of f(z) changes by
2π when we traverse the contour. Thus the value of the function does not change and it is a valid set of branch cuts.
Clearly the fourth set of branch cuts does not make the function single-valued as there are contours that encircle the
branch point at inﬁnity and no other branch points. The other way to see this is to note that the argument of f(z)
changes by 3π as we traverse a contour that goes around the branch points at z = 1, 2, 3 once in the positive direction.
Now to deﬁne the branch. We make the preliminary choice of angles,
z −1 = r1 eıθ1,
0 < θ1 < 2π,
z −2 = r2 eıθ2,
0 < θ2 < 2π,
z −3 = r3 eıθ3,
0 < θ3 < 2π.
332

The function is
f(z) =
 r1 eıθ1 r2 eıθ2 r3 eıθ31/2 = √r1r2r3 eı(θ1+θ2+θ3)/2 .
The value of the function at the origin is
f(0) =
√
6 eı(3π)/2 = −ı
√
6,
which is not what we wanted. We will change range of one of the angles to get the desired result.
z −1 = r1 eıθ1,
0 < θ1 < 2π,
z −2 = r2 eıθ2,
0 < θ2 < 2π,
z −3 = r3 eıθ3,
2π < θ3 < 4π.
f(0) =
√
6 eı(5π)/2 = ı
√
6,
Solution 7.32
w =
  z2 −2

(z + 2)
1/3 
z +
√
2
1/3 
z −
√
2
1/3
(z + 2)1/3
There are branch points at z = ±
√
2 and z = −2. If we walk around any one of the branch points once in the positive
direction, the argument of w changes by 2π/3 and thus the value of the function changes by eı2π/3. If we walk around
all three branch points then the argument of w changes by 3 × 2π/3 = 2π. The value of the function is unchanged as
eı2π = 1. Thus the branch cut on the real axis from −2 to
√
2 makes the function single-valued.
Now we deﬁne a branch. Let
z −
√
2 = a eıα,
z +
√
2 = b eıβ,
z + 2 = c eıγ .
We constrain the angles as follows: On the positive real axis, α = β = γ. See Figure 7.54.
333

α
β
γ
a
c
b
Re(z)
Im(z)
Figure 7.54: A branch of ((z2 −2) (z + 2))1/3.
Now we determine w(2).
w(2) =

2 −
√
2
1/3 
2 +
√
2
1/3
(2 + 2)1/3
=
3q
2 −
√
2 eı0
3q
2 +
√
2 eı0
3√
4 eı0
=
3√
2
3√
4
= 2.
Note that we didn’t have to choose the angle from each of the branch points as zero. Choosing any integer multiple
of 2π would give us the same result.
w(−3) =

−3 −
√
2
1/3 
−3 +
√
2
1/3
(−3 + 2)1/3
=
3q
3 +
√
2 eıπ/3
3q
3 −
√
2 eıπ/3
3√
1 eıπ/3
=
3√
7 eıπ
= −
3√
7
334

The value of the function is
w =
3√
abc eı(α+β+γ)/3 .
Consider the interval
 −
√
2 . . .
√
2

. As we approach the branch cut from above, the function has the value,
w =
3√
abc eıπ/3 =
3
r√
2 −x
 
x +
√
2

(x + 2) eıπ/3 .
As we approach the branch cut from below, the function has the value,
w =
3√
abc e−ıπ/3 =
3
r√
2 −x
 
x +
√
2

(x + 2) e−ıπ/3 .
Consider the interval
 −2 . . . −
√
2

. As we approach the branch cut from above, the function has the value,
w =
3√
abc eı2π/3 =
3
r√
2 −x
 
−x −
√
2

(x + 2) eı2π/3 .
As we approach the branch cut from below, the function has the value,
w =
3√
abc e−ı2π/3 =
3
r√
2 −x
 
−x −
√
2

(x + 2) e−ı2π/3 .
Solution 7.33
Arccos(x) is shown in Figure 7.55 for real variables in the range [−1 . . . 1].
First we write arccos(z) in terms of log(z). If cos(w) = z, then w = arccos(z).
cos(w) = z
eıw + e−ıw
2
= z
(eıw)2 −2z eıw +1 = 0
eıw = z +
 z2 −1
1/2
w = −ı log

z +
 z2 −1
1/2
335

-1
-0.5
0.5
1
0.5
1
1.5
2
2.5
3
Figure 7.55: The Principal Branch of the arc cosine, Arccos(x).
Thus we have
arccos(z) = −ı log

z +
 z2 −1
1/2
.
Since Arccos(0) = π
2, we must ﬁnd the branch such that
−ı log

0 +
 02 −1
1/2
= 0
−ı log
 (−1)1/2
= 0.
Since
−ı log(ı) = −ı

ıπ
2 + ı2πn

= π
2 + 2πn
and
−ı log(−ı) = −ı

−ıπ
2 + ı2πn

= −π
2 + 2πn
we must choose the branch of the square root such that (−1)1/2 = ı and the branch of the logarithm such that
log(ı) = ı π
2.
First we construct the branch of the square root.
 z2 −1
1/2 = (z + 1)1/2(z −1)1/2
336

We see that there are branch points at z = −1 and z = 1. In particular we want the Arccos to be deﬁned for z = x,
x ∈[−1 . . . 1]. Hence we introduce branch cuts on the lines −∞< x ≤−1 and 1 ≤x < ∞. Deﬁne the local
coordinates
z + 1 = r eıθ,
z −1 = ρ eıφ .
With the given branch cuts, the angles have the possible ranges
{θ} = {. . . , (−π . . . π), (π . . . 3π), . . .},
{φ} = {. . . , (0 . . . 2π), (2π . . . 4π), . . .}.
Now we choose ranges for θ and φ and see if we get the desired branch. If not, we choose a diﬀerent range for one of
the angles. First we choose the ranges
θ ∈(−π . . . π),
φ ∈(0 . . . 2π).
If we substitute in z = 0 we get
 02 −1
1/2 =
 1 eı01/2 (1 eıπ)1/2 = eı0 eıπ/2 = ı
Thus we see that this choice of angles gives us the desired branch.
θ=π
θ=−π
φ=0
φ=2π
Figure 7.56: Branch Cuts and Angles for (z2 −1)1/2
Now we go back to the expression
arccos(z) = −ı log

z +
 z2 −1
1/2
.
337

We have already seen that there are branch points at z = −1 and z = 1 because of (z2 −1)1/2. Now we must
determine if the logarithm introduces additional branch points. The only possibilities for branch points are where the
argument of the logarithm is zero.
z +
 z2 −1
1/2 = 0
z2 = z2 −1
0 = −1
We see that the argument of the logarithm is nonzero and thus there are no additional branch points. Introduce the
variable, w = z + (z2 −1)1/2. What is the image of the branch cuts in the w plane? We parameterize the branch cut
connecting z = 1 and z = +∞with z = r + 1, r ∈[0 . . . ∞).
w = r + 1 +
 (r + 1)2 −1
1/2
= r + 1 ±
p
r(r + 2)
= r

1 ± r
p
1 + 2/r

+ 1
r

1 +
p
1 + 2/r

+ 1 is the interval [1 . . . ∞); r

1 −
p
1 + 2/r

+ 1 is the interval (0 . . . 1]. Thus we see that this
branch cut is mapped to the interval (0 . . . ∞) in the w plane. Similarly, we could show that the branch cut (−∞. . .−1]
in the z plane is mapped to (−∞. . . 0) in the w plane. In the w plane there is a branch cut along the real w axis
from −∞to ∞. Thus cut makes the logarithm single-valued. For the branch of the square root that we chose, all the
points in the z plane get mapped to the upper half of the w plane.
With the branch cuts we have introduced so far and the chosen branch of the square root we have
arccos(0) = −ı log

0 +
 02 −1
1/2
= −ı log ı
= −ı

ıπ
2 + ı2πn

= π
2 + 2πn
338

Choosing the n = 0 branch of the logarithm will give us Arccos(z). We see that we can write
Arccos(z) = −ı Log

z +
 z2 −1
1/2
.
Solution 7.34
We consider the function f(z) =
 z1/2 −1
1/2. First note that z1/2 has a branch point at z = 0. We place a branch
cut on the negative real axis to make it single valued. f(z) will have a branch point where z1/2 −1 = 0. This occurs
at z = 1 on the branch of z1/2 on which 11/2 = 1. (11/2 has the value 1 on one branch of z1/2 and −1 on the other
branch.) For this branch we introduce a branch cut connecting z = 1 with the point at inﬁnity. (See Figure 7.57.)
1   =1
1   =-1
1/2
1/2
Figure 7.57: Branch Cuts for
 z1/2 −1
1/2
Solution 7.35
The distance between the end of rod a and the end of rod c is b. In the complex plane, these points are a eıθ and
l + c eıφ, respectively. We write this out mathematically.
l + c eıφ −a eıθ = b
 l + c eıφ −a eıθ  l + c e−ıφ −a e−ıθ
= b2
l2 + cl e−ıφ −al e−ıθ +cl eıφ +c2 −ac eı(φ−θ) −al eıθ −ac eı(θ−φ) +a2 = b2
cl cos φ −ac cos(φ −θ) −al cos θ = 1
2
 b2 −a2 −c2 −l2
339

This equation relates the two angular positions.
One could diﬀerentiate the equation to relate the velocities and
accelerations.
Solution 7.36
1. Let w = u + ıv. First we do the strip: |ℜ(z)| < 1. Consider the vertical line: z = c + ıy, y ∈R. This line is
mapped to
w = 2(c + ıy)2
w = 2c2 −2y2 + ı4cy
u = 2c2 −2y2,
v = 4cy
This is a parabola that opens to the left. For the case c = 0 it is the negative u axis. We can parametrize the
curve in terms of v.
u = 2c2 −1
8c2v2,
v ∈R
The boundaries of the region are both mapped to the parabolas:
u = 2 −1
8v2,
v ∈R.
The image of the mapping is

w = u + ıv : v ∈R and u < 2 −1
8v2

.
Note that the mapping is two-to-one.
Now we do the strip 1 < ℑ(z) < 2. Consider the horizontal line: z = x + ıc, x ∈R. This line is mapped to
w = 2(x + ıc)2
w = 2x2 −2c2 + ı4cx
u = 2x2 −2c2,
v = 4cx
340

This is a parabola that opens upward. We can parametrize the curve in terms of v.
u = 1
8c2v2 −2c2,
v ∈R
The boundary ℑ(z) = 1 is mapped to
u = 1
8v2 −2,
v ∈R.
The boundary ℑ(z) = 2 is mapped to
u = 1
32v2 −8,
v ∈R
The image of the mapping is

w = u + ıv : v ∈R and 1
32v2 −8 < u < 1
8v2 −2

.
2. We write the transformation as
z + 1
z −1 = 1 +
2
z −1.
Thus we see that the transformation is the sequence:
(a) translation by −1
(b) inversion
(c) magniﬁcation by 2
(d) translation by 1
Consider the strip |ℜ(z)| < 1. The translation by −1 maps this to −2 < ℜ(z) < 0. Now we do the inversion.
The left edge, ℜ(z) = 0, is mapped to itself. The right edge, ℜ(z) = −2, is mapped to the circle |z+1/4| = 1/4.
Thus the current image is the left half plane minus a circle:
ℜ(z) < 0
and
z + 1
4
 > 1
4.
341

The magniﬁcation by 2 yields
ℜ(z) < 0
and
z + 1
2
 > 1
2.
The ﬁnal step is a translation by 1.
ℜ(z) < 1
and
z −1
2
 > 1
2.
Now consider the strip 1 < ℑ(z) < 2. The translation by −1 does not change the domain. Now we do the
inversion. The bottom edge, ℑ(z) = 1, is mapped to the circle |z + ı/2| = 1/2. The top edge, ℑ(z) = 2, is
mapped to the circle |z + ı/4| = 1/4. Thus the current image is the region between two circles:
z + ı
2
 < 1
2
and
z + ı
4
 > 1
4.
The magniﬁcation by 2 yields
|z + ı| < 1
and
z + ı
2
 > 1
2.
The ﬁnal step is a translation by 1.
|z −1 + ı| < 1
and
z −1 + ı
2
 > 1
2.
Solution 7.37
1. There is a simple pole at z = −2. The function has a branch point at z = −1. Since this is the only branch
point in the ﬁnite complex plane there is also a branch point at inﬁnity. We can verify this with the substitution
z = 1/ζ.
f
1
ζ

= (1/ζ + 1)1/2
1/ζ + 2
= ζ1/2(1 + ζ)1/2
1 + 2ζ
Since f(1/ζ) has a branch point at ζ = 0, f(z) has a branch point at inﬁnity.
342

2. cos z is an entire function with an essential singularity at inﬁnity. Thus f(z) has singularities only where 1/(1+z)
has singularities. 1/(1 + z) has a ﬁrst order pole at z = −1. It is analytic everywhere else, including the point at
inﬁnity. Thus we conclude that f(z) has an essential singularity at z = −1 and is analytic elsewhere. To explicitly
show that z = −1 is an essential singularity, we can ﬁnd the Laurent series expansion of f(z) about z = −1.
cos

1
1 + z

=
∞
X
n=0
(−1)n
(2n)! (z + 1)−2n
3. 1 −ez has simple zeros at z = ı2nπ, n ∈Z. Thus f(z) has second order poles at those points.
The point at inﬁnity is a non-isolated singularity. To justify this: Note that
f(z) =
1
(1 −ez)2
has second order poles at z = ı2nπ, n ∈Z. This means that f(1/ζ) has second order poles at ζ =
1
ı2nπ, n ∈Z.
These second order poles get arbitrarily close to ζ = 0. There is no deleted neighborhood around ζ = 0 in which
f(1/ζ) is analytic. Thus the point ζ = 0, (z = ∞), is a non-isolated singularity. There is no Laurent series
expansion about the point ζ = 0, (z = ∞).
The point at inﬁnity is neither a branch point nor a removable singularity. It is not a pole either. If it were, there
would be an n such that limz→∞z−nf(z) = const ̸= 0. Since z−nf(z) has second order poles in every deleted
neighborhood of inﬁnity, the above limit does not exist. Thus we conclude that the point at inﬁnity is an essential
singularity.
Solution 7.38
We write sinh z in Cartesian form.
w = sinh z = sinh x cos y + ı cosh x sin y = u + ıv
Consider the line segment x = c, y ∈(0 . . . π). Its image is
{sinh c cos y + ı cosh c sin y | y ∈(0 . . . π)}.
343

This is the parametric equation for the upper half of an ellipse. Also note that u and v satisfy the equation for an
ellipse.
u2
sinh2 c +
v2
cosh2 c = 1
The ellipse starts at the point (sinh(c), 0), passes through the point (0, cosh(c)) and ends at (−sinh(c), 0). As c varies
from zero to ∞or from zero to −∞, the semi-ellipses cover the upper half w plane. Thus the mapping is 2-to-1.
Consider the inﬁnite line y = c, x ∈(−∞. . . ∞).Its image is
{sinh x cos c + ı cosh x sin c | x ∈(−∞. . . ∞)}.
This is the parametric equation for the upper half of a hyperbola. Also note that u and v satisfy the equation for a
hyperbola.
−u2
cos2 c +
v2
sin2 c = 1
As c varies from 0 to π/2 or from π/2 to π, the semi-hyperbola cover the upper half w plane. Thus the mapping is
2-to-1.
We look for branch points of sinh−1 w.
w = sinh z
w = ez −e−z
2
e2z −2w ez −1 = 0
ez = w +
 w2 + 1
1/2
z = log
 w + (w −ı)1/2(w + ı)1/2
There are branch points at w = ±ı. Since w + (w2 + 1)1/2 is nonzero and ﬁnite in the ﬁnite complex plane, the
logarithm does not introduce any branch points in the ﬁnite plane. Thus the only branch point in the upper half w
plane is at w = ı. Any branch cut that connects w = ı with the boundary of ℑ(w) > 0 will separate the branches
under the inverse mapping.
344

Consider the line y = π/4. The image under the mapping is the upper half of the hyperbola
2u2 + 2v2 = 1.
Consider the segment x = 1.The image under the mapping is the upper half of the ellipse
u2
sinh2 1 +
v2
cosh2 1 = 1.
345

Chapter 8
Analytic Functions
Students need encouragement. So if a student gets an answer right, tell them it was a lucky guess. That way, they
develop a good, lucky feeling.1
-Jack Handey
8.1
Complex Derivatives
Functions of a Real Variable.
The derivative of a function of a real variable is
d
dxf(x) = lim
∆x→0
f(x + ∆x) −f(x)
∆x
.
If the limit exists then the function is diﬀerentiable at the point x. Note that ∆x can approach zero from above or
below. The limit cannot depend on the direction in which ∆x vanishes.
Consider f(x) = |x|. The function is not diﬀerentiable at x = 0 since
lim
∆x→0+
|0 + ∆x| −|0|
∆x
= 1
1Quote slightly modiﬁed.
346

and
lim
∆x→0−
|0 + ∆x| −|0|
∆x
= −1.
Analyticity.
The complex derivative, (or simply derivative if the context is clear), is deﬁned,
d
dzf(z) = lim
∆z→0
f(z + ∆z) −f(z)
∆z
.
The complex derivative exists if this limit exists. This means that the value of the limit is independent of the manner
in which ∆z →0. If the complex derivative exists at a point, then we say that the function is complex diﬀerentiable
there.
A function of a complex variable is analytic at a point z0 if the complex derivative exists in a neighborhood about
that point. The function is analytic in an open set if it has a complex derivative at each point in that set. Note that
complex diﬀerentiable has a diﬀerent meaning than analytic. Analyticity refers to the behavior of a function on an open
set. A function can be complex diﬀerentiable at isolated points, but the function would not be analytic at those points.
Analytic functions are also called regular or holomorphic. If a function is analytic everywhere in the ﬁnite complex
plane, it is called entire.
Example 8.1.1 Consider zn, n ∈Z+, Is the function diﬀerentiable? Is it analytic? What is the value of the derivative?
We determine diﬀerentiability by trying to diﬀerentiate the function. We use the limit deﬁnition of diﬀerentiation.
We will use Newton’s binomial formula to expand (z + ∆z)n.
d
dzzn = lim
∆z→0
(z + ∆z)n −zn
∆z
= lim
∆z→0

zn + nzn−1∆z + n(n−1)
2
zn−2∆z2 + · · · + ∆zn
−zn
∆z
= lim
∆z→0

nzn−1 + n(n −1)
2
zn−2∆z + · · · + ∆zn−1

= nzn−1
347

The derivative exists everywhere. The function is analytic in the whole complex plane so it is entire. The value of the
derivative is
d
dz = nzn−1.
Example 8.1.2 We will show that f(z) = z is not diﬀerentiable. Consider its derivative.
d
dzf(z) = lim
∆z→0
f(z + ∆z) −f(z)
∆z
.
d
dzz = lim
∆z→0
z + ∆z −z
∆z
= lim
∆z→0
∆z
∆z
First we take ∆z = ∆x and evaluate the limit.
lim
∆x→0
∆x
∆x = 1
Then we take ∆z = ı∆y.
lim
∆y→0
−ı∆y
ı∆y
= −1
Since the limit depends on the way that ∆z →0, the function is nowhere diﬀerentiable. Thus the function is not
analytic.
Complex Derivatives in Terms of Plane Coordinates.
Let z = ζ(ξ, ψ) be a system of coordinates in
the complex plane. (For example, we could have Cartesian coordinates z = ζ(x, y) = x + ıy or polar coordinates
z = ζ(r, θ) = r eıθ). Let f(z) = φ(ξ, ψ) be a complex-valued function. (For example we might have a function in the
form φ(x, y) = u(x, y) + ıv(x, y) or φ(r, θ) = R(r, θ) eıΘ(r,θ).) If f(z) = φ(ξ, ψ) is analytic, its complex derivative is
348

equal to the derivative in any direction. In particular, it is equal to the derivatives in the coordinate directions.
df
dz =
lim
∆ξ→0,∆ψ=0
f(z + ∆z) −f(z)
∆z
= lim
∆ξ→0
φ(ξ + ∆ξ, ψ) −φ(ξ, ψ)
∂ζ
∂ξ∆ξ
=
∂ζ
∂ξ
−1 ∂φ
∂ξ
df
dz =
lim
∆ξ=0,∆ψ→0
f(z + ∆z) −f(z)
∆z
= lim
∆ψ→0
φ(ξ, ψ + ∆ψ) −φ(ξ, ψ)
∂ζ
∂ψ∆ψ
=
 ∂ζ
∂ψ
−1 ∂φ
∂ψ
Example 8.1.3 Consider the Cartesian coordinates z = x + ıy. We write the complex derivative as derivatives in the
coordinate directions for f(z) = φ(x, y).
df
dz =
∂(x + ıy)
∂x
−1 ∂φ
∂x = ∂φ
∂x
df
dz =
∂(x + ıy)
∂y
−1 ∂φ
∂y = −ı∂φ
∂y
We write this in operator notation.
d
dz = ∂
∂x = −ı ∂
∂y.
Example 8.1.4 In Example 8.1.1 we showed that zn, n ∈Z+, is an entire function and that
d
dzzn = nzn−1. Now we
corroborate this by calculating the complex derivative in the Cartesian coordinate directions.
d
dzzn = ∂
∂x(x + ıy)n
= n(x + ıy)n−1
= nzn−1
349

d
dzzn = −ı ∂
∂y(x + ıy)n
= −ıın(x + ıy)n−1
= nzn−1
Complex Derivatives are Not the Same as Partial Derivatives
Recall from calculus that
f(x, y) = g(s, t)
→
∂f
∂x = ∂g
∂s
∂s
∂x + ∂g
∂t
∂t
∂x
Do not make the mistake of using a similar formula for functions of a complex variable. If f(z) = φ(x, y) then
df
dz ̸= ∂φ
∂x
∂x
∂z + ∂φ
∂y
∂y
∂z .
This is because the
d
dz operator means “The derivative in any direction in the complex plane.” Since f(z) is analytic,
f ′(z) is the same no matter in which direction we take the derivative.
Rules of Diﬀerentiation.
For an analytic function deﬁned in terms of z we can calculate the complex derivative
using all the usual rules of diﬀerentiation that we know from calculus like the product rule,
d
dzf(z)g(z) = f ′(z)g(z) + f(z)g′(z),
or the chain rule,
d
dzf(g(z)) = f ′(g(z))g′(z).
This is because the complex derivative derives its properties from properties of limits, just like its real variable counterpart.
350

Result 8.1.1 The complex derivative is,
d
dzf(z) = lim
∆z→0
f(z + ∆z) −f(z)
∆z
.
The complex derivative is deﬁned if the limit exists and is independent of the manner in which
∆z →0. A function is analytic at a point if the complex derivative exists in a neighborhood
of that point.
Let z = ζ(ξ, ψ) deﬁne coordinates in the complex plane. The complex derivative in the
coordinate directions is
d
dz =
∂ζ
∂ξ
−1 ∂
∂ξ =
 ∂ζ
∂ψ
−1 ∂
∂ψ.
In Cartesian coordinates, this is
d
dz = ∂
∂x = −ı ∂
∂y.
In polar coordinates, this is
d
dz = e−ıθ ∂
∂r = −ı
r e−ıθ ∂
∂θ
Since the complex derivative is deﬁned with the same limit formula as real derivatives, all the
rules from the calculus of functions of a real variable may be used to diﬀerentiate functions
of a complex variable.
Example 8.1.5 We have shown that zn, n ∈Z+, is an entire function. Now we corroborate that
d
dzzn = nzn−1 by
351

calculating the complex derivative in the polar coordinate directions.
d
dzzn = e−ıθ ∂
∂rrn eınθ
= e−ıθ nrn−1 eınθ
= nrn−1 eı(n−1)θ
= nzn−1
d
dzzn = −ı
r e−ıθ ∂
∂θrn eınθ
= −ı
r e−ıθ rnın eınθ
= nrn−1 eı(n−1)θ
= nzn−1
Analytic Functions can be Written in Terms of z.
Consider an analytic function expressed in terms of x and
y, φ(x, y). We can write φ as a function of z = x + ıy and z = x −ıy.
f (z, z) = φ
z + z
2
, z −z
ı2

We treat z and z as independent variables. We ﬁnd the partial derivatives with respect to these variables.
∂
∂z = ∂x
∂z
∂
∂x + ∂y
∂z
∂
∂y = 1
2
 ∂
∂x −ı ∂
∂y

∂
∂z = ∂x
∂z
∂
∂x + ∂y
∂z
∂
∂y = 1
2
 ∂
∂x + ı ∂
∂y

352

Since φ is analytic, the complex derivatives in the x and y directions are equal.
∂φ
∂x = −ı∂φ
∂y
The partial derivative of f (z, z) with respect to z is zero.
∂f
∂z = 1
2
∂φ
∂x + ı∂φ
∂y

= 0
Thus f (z, z) has no functional dependence on z, it can be written as a function of z alone.
If we were considering an analytic function expressed in polar coordinates φ(r, θ), then we could write it in Cartesian
coordinates with the substitutions:
r =
p
x2 + y2,
θ = arctan(x, y).
Thus we could write φ(r, θ) as a function of z alone.
Result 8.1.2 Any analytic function φ(x, y) or φ(r, θ) can be written as a function of z alone.
8.2
Cauchy-Riemann Equations
If we know that a function is analytic, then we have a convenient way of determining its complex derivative. We just
express the complex derivative in terms of the derivative in a coordinate direction. However, we don’t have a nice way
of determining if a function is analytic. The deﬁnition of complex derivative in terms of a limit is cumbersome to work
with. In this section we remedy this problem.
A necessary condition for analyticity.
Consider a function f(z) = φ(x, y). If f(z) is analytic, the complex
derivative is equal to the derivatives in the coordinate directions. We equate the derivatives in the x and y directions
to obtain the Cauchy-Riemann equations in Cartesian coordinates.
φx = −ıφy
(8.1)
353

This equation is a necessary condition for the analyticity of f(z).
Let φ(x, y) = u(x, y) + ıv(x, y) where u and v are real-valued functions. We equate the real and imaginary parts
of Equation 8.1 to obtain another form for the Cauchy-Riemann equations in Cartesian coordinates.
ux = vy,
uy = −vx.
Note that this is a necessary and not a suﬃcient condition for analyticity of f(z). That is, u and v may satisfy the
Cauchy-Riemann equations but f(z) may not be analytic. At this point, Cauchy-Riemann equations give us an easy
test for determining if a function is not analytic.
Example 8.2.1 In Example 8.1.2 we showed that z is not analytic using the deﬁnition of complex diﬀerentiation. Now
we obtain the same result using the Cauchy-Riemann equations.
z = x −ıy
ux = 1,
vy = −1
We see that the ﬁrst Cauchy-Riemann equation is not satisﬁed; the function is not analytic at any point.
A suﬃcient condition for analyticity.
A suﬃcient condition for f(z) = φ(x, y) to be analytic at a point
z0 = (x0, y0) is that the partial derivatives of φ(x, y) exist and are continuous in some neighborhood of z0 and satisfy
the Cauchy-Riemann equations there. If the partial derivatives of φ exist and are continuous then
φ(x + ∆x, y + ∆y) = φ(x, y) + ∆xφx(x, y) + ∆yφy(x, y) + o(∆x) + o(∆y).
354

Here the notation o(∆x) means “terms smaller than ∆x”. We calculate the derivative of f(z).
f ′(z) = lim
∆z→0
f(z + ∆z) −f(z)
∆z
=
lim
∆x,∆y→0
φ(x + ∆x, y + ∆y) −φ(x, y)
∆x + ı∆y
=
lim
∆x,∆y→0
φ(x, y) + ∆xφx(x, y) + ∆yφy(x, y) + o(∆x) + o(∆y) −φ(x, y)
∆x + ı∆y
=
lim
∆x,∆y→0
∆xφx(x, y) + ∆yφy(x, y) + o(∆x) + o(∆y)
∆x + ı∆y
Here we use the Cauchy-Riemann equations.
=
lim
∆x,∆y→0
(∆x + ı∆y)φx(x, y)
∆x + ı∆y
+
lim
∆x,∆y→0
o(∆x) + o(∆y)
∆x + ı∆y
= φx(x, y)
Thus we see that the derivative is well deﬁned.
Cauchy-Riemann Equations in General Coordinates
Let z = ζ(ξ, ψ) be a system of coordinates in the
complex plane. Let φ(ξ, ψ) be a function which we write in terms of these coordinates, A necessary condition for
analyticity of φ(ξ, ψ) is that the complex derivatives in the coordinate directions exist and are equal. Equating the
derivatives in the ξ and ψ directions gives us the Cauchy-Riemann equations.
∂ζ
∂ξ
−1 ∂φ
∂ξ =
 ∂ζ
∂ψ
−1 ∂φ
∂ψ
We could separate this into two equations by equating the real and imaginary parts or the modulus and argument.
355

Result 8.2.1 A necessary condition for analyticity of φ(ξ, ψ), where z = ζ(ξ, ψ), at z = z0
is that the Cauchy-Riemann equations are satisﬁed in a neighborhood of z = z0.
∂ζ
∂ξ
−1 ∂φ
∂ξ =
 ∂ζ
∂ψ
−1 ∂φ
∂ψ.
(We could equate the real and imaginary parts or the modulus and argument of this to obtain
two equations.) A suﬃcient condition for analyticity of f(z) is that the Cauchy-Riemann
equations hold and the ﬁrst partial derivatives of φ exist and are continuous in a neighborhood
of z = z0.
Below are the Cauchy-Riemann equations for various forms of f(z).
f(z) = φ(x, y),
φx = −ıφy
f(z) = u(x, y) + ıv(x, y),
ux = vy,
uy = −vx
f(z) = φ(r, θ),
φr = −ı
rφθ
f(z) = u(r, θ) + ıv(r, θ),
ur = 1
rvθ,
uθ = −rvr
f(z) = R(r, θ) eıΘ(r,θ),
Rr = R
r Θθ,
1
rRθ = −RΘr
f(z) = R(x, y) eıΘ(x,y),
Rx = RΘy,
Ry = −RΘx
Example 8.2.2 Consider the Cauchy-Riemann equations for f(z) = u(r, θ) + ıv(r, θ). From Exercise 8.3 we know
that the complex derivative in the polar coordinate directions is
d
dz = e−ıθ ∂
∂r = −ı
r e−ıθ ∂
∂θ.
356

From Result 8.2.1 we have the equation,
e−ıθ ∂
∂r[u + ıv] = −ı
r e−ıθ ∂
∂θ[u + ıv].
We multiply by eıθ and equate the real and imaginary components to obtain the Cauchy-Riemann equations.
ur = 1
rvθ,
uθ = −rvr
Example 8.2.3 Consider the exponential function.
ez = φ(x, y) = ex(cos y + ı sin(y))
We use the Cauchy-Riemann equations to show that the function is entire.
φx = −ıφy
ex(cos y + ı sin(y)) = −ı ex(−sin y + ı cos(y))
ex(cos y + ı sin(y)) = ex(cos y + ı sin(y))
Since the function satisﬁes the Cauchy-Riemann equations and the ﬁrst partial derivatives are continuous everywhere
in the ﬁnite complex plane, the exponential function is entire.
Now we ﬁnd the value of the complex derivative.
d
dz ez = ∂φ
∂x = ex(cos y + ı sin(y)) = ez
The diﬀerentiability of the exponential function implies the diﬀerentiability of the trigonometric functions, as they can
be written in terms of the exponential.
In Exercise 8.13 you can show that the logarithm log z is diﬀerentiable for z ̸= 0. This implies the diﬀerentiability
of zα and the inverse trigonometric functions as they can be written in terms of the logarithm.
357

Example 8.2.4 We compute the derivative of zz.
d
dz (zz) = d
dz ez log z
= (1 + log z) ez log z
= (1 + log z)zz
= zz + zz log z
8.3
Harmonic Functions
A function u is harmonic if its second partial derivatives exist, are continuous and satisfy Laplace’s equation ∆u = 0.2
(In Cartesian coordinates the Laplacian is ∆u ≡uxx + uyy.) If f(z) = u + ıv is an analytic function then u and v are
harmonic functions. To see why this is so, we start with the Cauchy-Riemann equations.
ux = vy,
uy = −vx
We diﬀerentiate the ﬁrst equation with respect to x and the second with respect to y. (We assume that u and v are
twice continuously diﬀerentiable. We will see later that they are inﬁnitely diﬀerentiable.)
uxx = vxy,
uyy = −vyx
Thus we see that u is harmonic.
∆u ≡uxx + uyy = vxy −vyx = 0
One can use the same method to show that ∆v = 0.
2 The capital Greek letter ∆is used to denote the Laplacian, like ∆u(x, y), and diﬀerentials, like ∆x.
358

If u is harmonic on some simply-connected domain, then there exists a harmonic function v such that f(z) = u + ıv
is analytic in the domain. v is called the harmonic conjugate of u. The harmonic conjugate is unique up to an additive
constant. To demonstrate this, let w be another harmonic conjugate of u. Both the pair u and v and the pair u and
w satisfy the Cauchy-Riemann equations.
ux = vy,
uy = −vx,
ux = wy,
uy = −wx
We take the diﬀerence of these equations.
vx −wx = 0,
vy −wy = 0
On a simply connected domain, the diﬀerence between v and w is thus a constant.
To prove the existence of the harmonic conjugate, we ﬁrst write v as an integral.
v(x, y) = v (x0, y0) +
Z (x,y)
(x0,y0)
vx dx + vy dy
On a simply connected domain, the integral is path independent and deﬁnes a unique v in terms of vx and vy. We use
the Cauchy-Riemann equations to write v in terms of ux and uy.
v(x, y) = v (x0, y0) +
Z (x,y)
(x0,y0)
−uy dx + ux dy
Changing the starting point (x0, y0) changes v by an additive constant. The harmonic conjugate of u to within an
additive constant is
v(x, y) =
Z
−uy dx + ux dy.
This proves the existence3 of the harmonic conjugate. This is not the formula one would use to construct the harmonic
conjugate of a u. One accomplishes this by solving the Cauchy-Riemann equations.
3 A mathematician returns to his oﬃce to ﬁnd that a cigarette tossed in the trash has started a small ﬁre. Being calm and a
quick thinker he notes that there is a ﬁre extinguisher by the window. He then closes the door and walks away because “the solution
exists.”
359

Result 8.3.1 If f(z) = u + ıv is an analytic function then u and v are harmonic functions.
That is, the Laplacians of u and v vanish ∆u = ∆v = 0. The Laplacian in Cartesian and
polar coordinates is
∆= ∂2
∂x2 + ∂2
∂y2,
∆= 1
r
∂
∂r

r ∂
∂r

+ 1
r2
∂2
∂θ2.
Given a harmonic function u in a simply connected domain, there exists a harmonic function
v, (unique up to an additive constant), such that f(z) = u + ıv is analytic in the domain.
One can construct v by solving the Cauchy-Riemann equations.
Example 8.3.1 Is x2 the real part of an analytic function?
The Laplacian of x2 is
∆[x2] = 2 + 0
x2 is not harmonic and thus is not the real part of an analytic function.
Example 8.3.2 Show that u = e−x(x sin y −y cos y) is harmonic.
∂u
∂x = e−x sin y −ex(x sin y −y cos y)
= e−x sin y −x e−x sin y + y e−x cos y
∂2u
∂x2 = −e−x sin y −e−x sin y + x e−x sin y −y e−x cos y
= −2 e−x sin y + x e−x sin y −y e−x cos y
∂u
∂y = e−x(x cos y −cos y + y sin y)
360

∂2u
∂y2 = e−x(−x sin y + sin y + y cos y + sin y)
= −x e−x sin y + 2 e−x sin y + y e−x cos y
Thus we see that ∂2u
∂x2 + ∂2u
∂y2 = 0 and u is harmonic.
Example 8.3.3 Consider u = cos x cosh y. This function is harmonic.
uxx + uyy = −cos x cosh y + cos x cosh y = 0
Thus it is the real part of an analytic function, f(z). We ﬁnd the harmonic conjugate, v, with the Cauchy-Riemann
equations. We integrate the ﬁrst Cauchy-Riemann equation.
vy = ux = −sin x cosh y
v = −sin x sinh y + a(x)
Here a(x) is a constant of integration. We substitute this into the second Cauchy-Riemann equation to determine a(x).
vx = −uy
−cos x sinh y + a′(x) = −cos x sinh y
a′(x) = 0
a(x) = c
Here c is a real constant. Thus the harmonic conjugate is
v = −sin x sinh y + c.
The analytic function is
f(z) = cos x cosh y −ı sin x sinh y + ıc
We recognize this as
f(z) = cos z + ıc.
361

Example 8.3.4 Here we consider an example that demonstrates the need for a simply connected domain. Consider
u = Log r in the multiply connected domain, r > 0. u is harmonic.
∆Log r = 1
r
∂
∂r

r ∂
∂r Log r

+ 1
r2
∂2
∂θ2 Log r = 0
We solve the Cauchy-Riemann equations to try to ﬁnd the harmonic conjugate.
ur = 1
rvθ,
uθ = −rvr
vr = 0,
vθ = 1
v = θ + c
We are able to solve for v, but it is multi-valued. Any single-valued branch of θ that we choose will not be continuous
on the domain. Thus there is no harmonic conjugate of u = Log r for the domain r > 0.
If we had instead considered the simply-connected domain r > 0, | arg(z)| < π then the harmonic conjugate would
be v = Arg(z) + c. The corresponding analytic function is f(z) = Log z + ıc.
Example 8.3.5 Consider u = x3 −3xy2 + x. This function is harmonic.
uxx + uyy = 6x −6x = 0
Thus it is the real part of an analytic function, f(z). We ﬁnd the harmonic conjugate, v, with the Cauchy-Riemann
equations. We integrate the ﬁrst Cauchy-Riemann equation.
vy = ux = 3x2 −3y2 + 1
v = 3x2y −y3 + y + a(x)
Here a(x) is a constant of integration. We substitute this into the second Cauchy-Riemann equation to determine a(x).
vx = −uy
6xy + a′(x) = 6xy
a′(x) = 0
a(x) = c
362

Here c is a real constant. The harmonic conjugate is
v = 3x2y −y3 + y + c.
The analytic function is
f(z) = x3 −3xy2 + x + ı
 3x2y −y3 + y

+ ıc
f(z) = x3 + ı3x2y −3xy2 −ıy2 + x + ıy + ıc
f(z) = z3 + z + ıc
8.4
Singularities
Any point at which a function is not analytic is called a singularity. In this section we will classify the diﬀerent ﬂavors
of singularities.
Result 8.4.1 Singularities. If a function is not analytic at a point, then that point is a
singular point or a singularity of the function.
8.4.1
Categorization of Singularities
Branch Points.
If f(z) has a branch point at z0, then we cannot deﬁne a branch of f(z) that is continuous in a
neighborhood of z0. Continuity is necessary for analyticity. Thus all branch points are singularities. Since function are
discontinuous across branch cuts, all points on a branch cut are singularities.
Example 8.4.1 Consider f(z) = z3/2. The origin and inﬁnity are branch points and are thus singularities of f(z). We
choose the branch g(z) =
√
z3. All the points on the negative real axis, including the origin, are singularities of g(z).
363

Removable Singularities.
Example 8.4.2 Consider
f(z) = sin z
z
.
This function is undeﬁned at z = 0 because f(0) is the indeterminate form 0/0. f(z) is analytic everywhere in the
ﬁnite complex plane except z = 0. Note that the limit as z →0 of f(z) exists.
lim
z→0
sin z
z
= lim
z→0
cos z
1
= 1
If we were to ﬁll in the hole in the deﬁnition of f(z), we could make it diﬀerentiable at z = 0. Consider the function
g(z) =
(
sin z
z
z ̸= 0,
1
z = 0.
We calculate the derivative at z = 0 to verify that g(z) is analytic there.
f ′(0) = lim
z→0
f(0) −f(z)
z
= lim
z→0
1 −sin(z)/z
z
= lim
z→0
z −sin(z)
z2
= lim
z→0
1 −cos(z)
2z
= lim
z→0
sin(z)
2
= 0
We call the point at z = 0 a removable singularity of sin(z)/z because we can remove the singularity by deﬁning the
value of the function to be its limiting value there.
364

Consider a function f(z) that is analytic in a deleted neighborhood of z = z0. If f(z) is not analytic at z0, but
limz→z0 f(z) exists, then the function has a removable singularity at z0. The function
g(z) =
(
f(z)
z ̸= z0
limz→z0 f(z)
z = z0
is analytic in a neighborhood of z = z0. We show this by calculating g′ (z0).
g′ (z0) = lim
z→z0
g (z0) −g(z)
z0 −z
= lim
z→z0
−g′(z)
−1
= lim
z→z0 f ′(z)
This limit exists because f(z) is analytic in a deleted neighborhood of z = z0.
Poles.
If a function f(z) behaves like c/ (z −z0)n near z = z0 then the function has an nth order pole at that point.
More mathematically we say
lim
z→z0 (z −z0)n f(z) = c ̸= 0.
We require the constant c to be nonzero so we know that it is not a pole of lower order. We can denote a removable
singularity as a pole of order zero.
Another way to say that a function has an nth order pole is that f(z) is not analytic at z = z0, but (z −z0)n f(z)
is either analytic or has a removable singularity at that point.
Example 8.4.3 1/ sin (z2) has a second order pole at z = 0 and ﬁrst order poles at z = (nπ)1/2, n ∈Z±.
lim
z→0
z2
sin (z2) = lim
z→0
2z
2z cos (z2)
= lim
z→0
2
2 cos (z2) −4z2 sin (z2)
= 1
365

lim
z→(nπ)1/2
z −(nπ)1/2
sin (z2)
=
lim
z→(nπ)1/2
1
2z cos (z2)
=
1
2(nπ)1/2(−1)n
Example 8.4.4 e1/z is singular at z = 0. The function is not analytic as limz→0 e1/z does not exist. We check if the
function has a pole of order n at z = 0.
lim
z→0 zn e1/z = lim
ζ→∞
eζ
ζn
= lim
ζ→∞
eζ
n!
Since the limit does not exist for any value of n, the singularity is not a pole. We could say that e1/z is more singular
than any power of 1/z.
Essential Singularities.
If a function f(z) is singular at z = z0, but the singularity is not a branch point, or a
pole, the the point is an essential singularity of the function.
The point at inﬁnity.
We can consider the point at inﬁnity z →∞by making the change of variables z = 1/ζ
and considering ζ →0. If f(1/ζ) is analytic at ζ = 0 then f(z) is analytic at inﬁnity. We have encountered branch
points at inﬁnity before (Section 7.8). Assume that f(z) is not analytic at inﬁnity. If limz→∞f(z) exists then f(z) has
a removable singularity at inﬁnity. If limz→∞f(z)/zn = c ̸= 0 then f(z) has an nth order pole at inﬁnity.
366

Result 8.4.2 Categorization of Singularities. Consider a function f(z) that has a singu-
larity at the point z = z0. Singularities come in four ﬂavors:
Branch Points. Branch points of multi-valued functions are singularities.
Removable Singularities. If limz→z0 f(z) exists, then z0 is a removable singularity. It
is thus named because the singularity could be removed and thus the function made
analytic at z0 by redeﬁning the value of f (z0).
Poles. If limz→z0 (z −z0)n f(z) = const ̸= 0 then f(z) has an nth order pole at z0.
Essential Singularities. Instead of deﬁning what an essential singularity is, we say what it
is not. If z0 neither a branch point, a removable singularity nor a pole, it is an essential
singularity.
A pole may be called a non-essential singularity. This is because multiplying the function by an integral power of
z −z0 will make the function analytic. Then an essential singularity is a point z0 such that there does not exist an n
such that (z −z0)n f(z) is analytic there.
8.4.2
Isolated and Non-Isolated Singularities
Result 8.4.3 Isolated and Non-Isolated Singularities. Suppose f(z) has a singularity at
z0. If there exists a deleted neighborhood of z0 containing no singularities then the point is
an isolated singularity. Otherwise it is a non-isolated singularity.
367

If you don’t like the abstract notion of a deleted neighborhood, you can work with a deleted circular neighborhood.
However, this will require the introduction of more math symbols and a Greek letter. z = z0 is an isolated singularity
if there exists a δ > 0 such that there are no singularities in 0 < |z −z0| < δ.
Example 8.4.5 We classify the singularities of f(z) = z/ sin z.
z has a simple zero at z = 0. sin z has simple zeros at z = nπ. Thus f(z) has a removable singularity at z = 0
and has ﬁrst order poles at z = nπ for n ∈Z±. We can corroborate this by taking limits.
lim
z→0 f(z) = lim
z→0
z
sin z = lim
z→0
1
cos z = 1
lim
z→nπ(z −nπ)f(z) = lim
z→nπ
(z −nπ)z
sin z
= lim
z→nπ
2z −nπ
cos z
=
nπ
(−1)n
̸= 0
Now to examine the behavior at inﬁnity. There is no neighborhood of inﬁnity that does not contain ﬁrst order
poles of f(z). (Another way of saying this is that there does not exist an R such that there are no singularities in
R < |z| < ∞.) Thus z = ∞is a non-isolated singularity.
We could also determine this by setting ζ = 1/z and examining the point ζ = 0. f(1/ζ) has ﬁrst order poles at
ζ = 1/(nπ) for n ∈Z \ {0}. These ﬁrst order poles come arbitrarily close to the point ζ = 0 There is no deleted
neighborhood of ζ = 0 which does not contain singularities. Thus ζ = 0, and hence z = ∞is a non-isolated singularity.
The point at inﬁnity is an essential singularity. It is certainly not a branch point or a removable singularity. It is not a
pole, because there is no n such that limz→∞z−nf(z) = const ̸= 0. z−nf(z) has ﬁrst order poles in any neighborhood
of inﬁnity, so this limit does not exist.
368

8.5
Application: Potential Flow
Example 8.5.1 We consider 2 dimensional uniform ﬂow in a given direction. The ﬂow corresponds to the complex
potential
Φ(z) = v0 e−ıθ0 z,
where v0 is the ﬂuid speed and θ0 is the direction. We ﬁnd the velocity potential φ and stream function ψ.
Φ(z) = φ + ıψ
φ = v0(cos(θ0)x + sin(θ0)y),
ψ = v0(−sin(θ0)x + cos(θ0)y)
These are plotted in Figure 8.1 for θ0 = π/6.
-1-0.5
0
0.5
1-1
-0.5
0
0.5
1
-1
0
1
1-0.5
0
0.5
-1-0.5
0
0.5
1-1
-0.5
0
0.5
1
-1
0
1
1-0.5
0
0.5
Figure 8.1: The velocity potential φ and stream function ψ for Φ(z) = v0 e−ıθ0 z.
Next we ﬁnd the stream lines, ψ = c.
v0(−sin(θ0)x + cos(θ0)y) = c
y =
c
v0 cos(θ0) + tan(θ0)x
369

-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
Figure 8.2: Streamlines for ψ = v0(−sin(θ0)x + cos(θ0)y).
Figure 8.2 shows how the streamlines go straight along the θ0 direction. Next we ﬁnd the velocity ﬁeld.
v = ∇φ
v = φxˆx + φyˆy
v = v0 cos(θ0)ˆx + v0 sin(θ0)ˆy
The velocity ﬁeld is shown in Figure 8.3.
Example 8.5.2 Steady, incompressible, inviscid, irrotational ﬂow is governed by the Laplace equation. We consider
ﬂow around an inﬁnite cylinder of radius a. Because the ﬂow does not vary along the axis of the cylinder, this is a
two-dimensional problem. The ﬂow corresponds to the complex potential
Φ(z) = v0

z + a2
z

.
370

Figure 8.3: Velocity ﬁeld and velocity direction ﬁeld for φ = v0(cos(θ0)x + sin(θ0)y).
We ﬁnd the velocity potential φ and stream function ψ.
Φ(z) = φ + ıψ
φ = v0

r + a2
r

cos θ,
ψ = v0

r −a2
r

sin θ
These are plotted in Figure 8.4.
Next we ﬁnd the stream lines, ψ = c.
v0

r −a2
r

sin θ = c
r = c ±
p
c2 + 4v0 sin2 θ
2v0 sin θ
Figure 8.5 shows how the streamlines go around the cylinder. Next we ﬁnd the velocity ﬁeld.
371

Figure 8.4: The velocity potential φ and stream function ψ for Φ(z) = v0

z + a2
z

.
Figure 8.5: Streamlines for ψ = v0

r −a2
r

sin θ.
v = ∇φ
v = φrˆr + φθ
r
ˆθ
v = v0

1 −a2
r2

cos θˆr −v0

1 + a2
r2

sin θˆθ
372

The velocity ﬁeld is shown in Figure 8.6.
Figure 8.6: Velocity ﬁeld and velocity direction ﬁeld for φ = v0

r + a2
r

cos θ.
373

8.6
Exercises
Complex Derivatives
Exercise 8.1
Consider two functions f(z) and g(z) analytic at z0 with f(z0) = g(z0) = 0 and g′(z0) ̸= 0.
1. Use the deﬁnition of the complex derivative to justify L’Hospital’s rule:
lim
z→z0
f(z)
g(z) = f ′(z0)
g′(z0)
2. Evaluate the limits
lim
z→ı
1 + z2
2 + 2z6,
lim
z→ıπ
sinh(z)
ez +1
Hint, Solution
Exercise 8.2
Show that if f(z) is analytic and φ(x, y) = f(z) is twice continuously diﬀerentiable then f ′(z) is analytic.
Hint, Solution
Exercise 8.3
Find the complex derivative in the coordinate directions for f(z) = φ(r, θ).
Hint, Solution
Exercise 8.4
Show that the following functions are nowhere analytic by checking where the derivative with respect to z exists.
1. sin x cosh y −ı cos x sinh y
2. x2 −y2 + x + ı(2xy −y)
Hint, Solution
374

Exercise 8.5
f(z) is analytic for all z, (|z| < ∞). f (z1 + z2) = f (z1) f (z2) for all z1 and z2. (This is known as a functional
equation). Prove that f(z) = exp (f ′(0)z).
Hint, Solution
Cauchy-Riemann Equations
Exercise 8.6
If f(z) is analytic in a domain and has a constant real part, a constant imaginary part, or a constant modulus, show
that f(z) is constant.
Hint, Solution
Exercise 8.7
Show that the function
f(z) =
(
e−z−4
for z ̸= 0,
0
for z = 0.
satisﬁes the Cauchy-Riemann equations everywhere, including at z = 0, but f(z) is not analytic at the origin.
Hint, Solution
Exercise 8.8
Find the Cauchy-Riemann equations for the following forms.
1. f(z) = R(r, θ) eıΘ(r,θ)
2. f(z) = R(x, y) eıΘ(x,y)
Hint, Solution
Exercise 8.9
1. Show that ez is not analytic.
2. f(z) is an analytic function of z. Show that f(z) = f (z) is also an analytic function of z.
Hint, Solution
375

Exercise 8.10
1. Determine all points z = x + ıy where the following functions are diﬀerentiable with respect to z:
(a) x3 + y3
(b)
x −1
(x −1)2 + y2 −ı
y
(x −1)2 + y2
2. Determine all points z where these functions are analytic.
3. Determine which of the following functions v(x, y) are the imaginary part of an analytic function u(x, y)+ıv(x, y).
For those that are, compute the real part u(x, y) and re-express the answer as an explicit function of z = x + ıy:
(a) x2 −y2
(b) 3x2y
Hint, Solution
Exercise 8.11
Let
f(z) =
(
x4/3y5/3+ıx5/3y4/3
x2+y2
for z ̸= 0,
0
for z = 0.
Show that the Cauchy-Riemann equations hold at z = 0, but that f is not diﬀerentiable at this point.
Hint, Solution
Exercise 8.12
Consider the complex function
f(z) = u + ıv =
(
x3(1+ı)−y3(1−ı)
x2+y2
for z ̸= 0,
0
for z = 0.
Show that the partial derivatives of u and v with respect to x and y exist at z = 0 and that ux = vy and uy = −vx
there: the Cauchy-Riemann equations are satisﬁed at z = 0. On the other hand, show that
lim
z→0
f(z)
z
376

does not exist, that is, f is not complex-diﬀerentiable at z = 0.
Hint, Solution
Exercise 8.13
Show that the logarithm log z is diﬀerentiable for z ̸= 0. Find the derivative of the logarithm.
Hint, Solution
Exercise 8.14
Show that the Cauchy-Riemann equations for the analytic function f(z) = u(r, θ) + ıv(r, θ) are
ur = vθ/r,
uθ = −rvr.
Hint, Solution
Exercise 8.15
w = u + ıv is an analytic function of z. φ(x, y) is an arbitrary smooth function of x and y. When expressed in terms
of u and v, φ(x, y) = Φ(u, v). Show that (w′ ̸= 0)
∂Φ
∂u −ı∂Φ
∂v =
dw
dz
−1 ∂φ
∂x −ı∂φ
∂y

.
Deduce
∂2Φ
∂u2 + ∂2Φ
∂v2 =

dw
dz

−2 ∂2φ
∂x2 + ∂2φ
∂y2

.
Hint, Solution
Exercise 8.16
Show that the functions deﬁned by f(z) = log |z|+ı arg(z) and f(z) =
p
|z| eı arg(z)/2 are analytic in the sector |z| > 0,
| arg(z)| < π. What are the corresponding derivatives df/dz?
Hint, Solution
Exercise 8.17
Show that the following functions are harmonic. For each one of them ﬁnd its harmonic conjugate and form the
corresponding holomorphic function.
377

1. u(x, y) = x Log(r) −y arctan(x, y) (r ̸= 0)
2. u(x, y) = arg(z) (| arg(z)| < π, r ̸= 0)
3. u(x, y) = rn cos(nθ)
4. u(x, y) = y/r2 (r ̸= 0)
Hint, Solution
Exercise 8.18
1. Use the Cauchy-Riemann equations to determine where the function
f(z) = (x −y)2 + ı2(x + y)
is diﬀerentiable and where it is analytic.
2. Evaluate the derivative of
f(z) = ex2−y2(cos(2xy) + ı sin(2xy))
and describe the domain of analyticity.
Hint, Solution
Exercise 8.19
Consider the function f(z) = u + ıv with real and imaginary parts expressed in terms of either x and y or r and θ.
1. Show that the Cauchy-Riemann equations
ux = vy,
uy = −vx
are satisﬁed and these partial derivatives are continuous at a point z if and only if the polar form of the Cauchy-
Riemann equations
ur = 1
rvθ,
1
ruθ = −vr
is satisﬁed and these partial derivatives are continuous there.
378

2. Show that it is easy to verify that Log z is analytic for r > 0 and −π < θ < π using the polar form of the
Cauchy-Riemann equations and that the value of the derivative is easily obtained from a polar diﬀerentiation
formula.
3. Show that in polar coordinates, Laplace’s equation becomes
φrr + 1
rφr + 1
r2φθθ = 0.
Hint, Solution
Exercise 8.20
Determine which of the following functions are the real parts of an analytic function.
1. u(x, y) = x3 −y3
2. u(x, y) = sinh x cos y + x
3. u(r, θ) = rn cos(nθ)
and ﬁnd f(z) for those that are.
Hint, Solution
Exercise 8.21
Consider steady, incompressible, inviscid, irrotational ﬂow governed by the Laplace equation. Determine the form of
the velocity potential and stream function contours for the complex potentials
1. Φ(z) = φ(x, y) + ıψ(x, y) = log z + ı log z
2. Φ(z) = log(z −1) + log(z + 1)
Plot and describe the features of the ﬂows you are considering.
Hint, Solution
379

8.7
Hints
Complex Derivatives
Hint 8.1
Hint 8.2
Start with the Cauchy-Riemann equation and then diﬀerentiate with respect to x.
Hint 8.3
Read Example 8.1.3 and use Result 8.1.1.
Hint 8.4
Use Result 8.1.1.
Hint 8.5
Take the logarithm of the equation to get a linear equation.
Cauchy-Riemann Equations
Hint 8.6
Hint 8.7
Hint 8.8
For the ﬁrst part use the result of Exercise 8.3.
Hint 8.9
Use the Cauchy-Riemann equations.
380

Hint 8.10
Hint 8.11
To evaluate ux(0, 0), etc.
use the deﬁnition of diﬀerentiation.
Try to ﬁnd f ′(z) with the deﬁnition of complex
diﬀerentiation. Consider ∆z = ∆r eıθ.
Hint 8.12
To evaluate ux(0, 0), etc.
use the deﬁnition of diﬀerentiation.
Try to ﬁnd f ′(z) with the deﬁnition of complex
diﬀerentiation. Consider ∆z = ∆r eıθ.
Hint 8.13
Hint 8.14
Hint 8.15
Hint 8.16
Hint 8.17
Hint 8.18
Hint 8.19
381

Hint 8.20
Hint 8.21
382

8.8
Solutions
Complex Derivatives
Solution 8.1
1. We consider L’Hospital’s rule.
lim
z→z0
f(z)
g(z) = f ′(z0)
g′(z0)
We start with the right side and show that it is equal to the left side. First we apply the deﬁnition of complex
diﬀerentiation.
f ′(z0)
g′(z0) = limϵ→0
f(z0+ϵ)−f(z0)
ϵ
limδ→0
g(z0+δ)−g(z0)
δ
= limϵ→0
f(z0+ϵ)
ϵ
limδ→0
g(z0+δ)
δ
Since both of the limits exist, we may take the limits with ϵ = δ.
f ′(z0)
g′(z0) = lim
ϵ→0
f(z0 + ϵ)
g(z0 + ϵ)
f ′(z0)
g′(z0) = lim
z→z0
f(z)
g(z)
This proves L’Hospital’s rule.
2.
lim
z→ı
1 + z2
2 + 2z6 =
 2z
12z5

z=ı
= 1
6
lim
z→ıπ
sinh(z)
ez +1 =
cosh(z)
ez

z=ıπ
= 1
383

Solution 8.2
We start with the Cauchy-Riemann equation and then diﬀerentiate with respect to x.
φx = −ıφy
φxx = −ıφyx
We interchange the order of diﬀerentiation.
(φx)x = −ı (φx)y
(f ′)x = −ı (f ′)y
Since f ′(z) satisﬁes the Cauchy-Riemann equation and its partial derivatives exist and are continuous, it is analytic.
Solution 8.3
We calculate the complex derivative in the coordinate directions.
df
dz =
 
∂
 r eıθ
∂r
!−1
∂φ
∂r = e−ıθ ∂φ
∂r ,
df
dz =
 
∂
 r eıθ
∂θ
!−1
∂φ
∂θ = −ı
r e−ıθ ∂φ
∂θ .
We can write this in operator notation.
d
dz = e−ıθ ∂
∂r = −ı
r e−ıθ ∂
∂θ
Solution 8.4
1. Consider f(x, y) = sin x cosh y −ı cos x sinh y. The derivatives in the x and y directions are
∂f
∂x = cos x cosh y + ı sin x sinh y
−ı∂f
∂y = −cos x cosh y −ı sin x sinh y
384

These derivatives exist and are everywhere continuous. We equate the expressions to get a set of two equations.
cos x cosh y = −cos x cosh y,
sin x sinh y = −sin x sinh y
cos x cosh y = 0,
sin x sinh y = 0

x = π
2 + nπ

and (x = mπ or y = 0)
The function may be diﬀerentiable only at the points
x = π
2 + nπ,
y = 0.
Thus the function is nowhere analytic.
2. Consider f(x, y) = x2 −y2 + x + ı(2xy −y). The derivatives in the x and y directions are
∂f
∂x = 2x + 1 + ı2y
−ı∂f
∂y = ı2y + 2x −1
These derivatives exist and are everywhere continuous. We equate the expressions to get a set of two equations.
2x + 1 = 2x −1,
2y = 2y.
Since this set of equations has no solutions, there are no points at which the function is diﬀerentiable. The
function is nowhere analytic.
Solution 8.5
f (z1 + z2) = f (z1) f (z2)
log (f (z1 + z2)) = log (f (z1)) + log (f (z2))
385

We deﬁne g(z) = log(f(z)).
g (z1 + z2) = g (z1) + g (z2)
This is a linear equation which has exactly the solutions:
g(z) = cz.
Thus f(z) has the solutions:
f(z) = ecz,
where c is any complex constant. We can write this constant in terms of f ′(0). We diﬀerentiate the original equation
with respect to z1 and then substitute z1 = 0.
f ′ (z1 + z2) = f ′ (z1) f (z2)
f ′ (z2) = f ′(0)f (z2)
f ′(z) = f ′(0)f(z)
We substitute in the form of the solution.
c ecz = f ′(0) ecz
c = f ′(0)
Thus we see that
f(z) = ef′(0)z .
Cauchy-Riemann Equations
Solution 8.6
Constant Real Part. First assume that f(z) has constant real part. We solve the Cauchy-Riemann equations to
determine the imaginary part.
ux = vy,
uy = −vx
vx = 0,
vy = 0
386

We integrate the ﬁrst equation to obtain v = a + g(y) where a is a constant and g(y) is an arbitrary function. Then
we substitute this into the second equation to determine g(y).
g′(y) = 0
g(y) = b
We see that the imaginary part of f(z) is a constant and conclude that f(z) is constant.
Constant Imaginary Part. Next assume that f(z) has constant imaginary part. We solve the Cauchy-Riemann
equations to determine the real part.
ux = vy,
uy = −vx
ux = 0,
uy = 0
We integrate the ﬁrst equation to obtain u = a + g(y) where a is a constant and g(y) is an arbitrary function. Then
we substitute this into the second equation to determine g(y).
g′(y) = 0
g(y) = b
We see that the real part of f(z) is a constant and conclude that f(z) is constant.
Constant Modulus. Finally assume that f(z) has constant modulus.
|f(z)| = constant
√
u2 + v2 = constant
u2 + v2 = constant
We diﬀerentiate this equation with respect to x and y.
2uux + 2vvx = 0,
2uuy + 2vvy = 0
ux
vx
uy
vy
 u
v

= 0
387

This system has non-trivial solutions for u and v only if the matrix is non-singular. (The trivial solution u = v = 0 is
the constant function f(z) = 0.) We set the determinant of the matrix to zero.
uxvy −uyvx = 0
We use the Cauchy-Riemann equations to write this in terms of ux and uy.
u2
x + u2
y = 0
ux = uy = 0
Since its partial derivatives vanish, u is a constant.
From the Cauchy-Riemann equations we see that the partial
derivatives of v vanish as well, so it is constant. We conclude that f(z) is a constant.
Constant Modulus. Here is another method for the constant modulus case. We solve the Cauchy-Riemann
equations in polar form to determine the argument of f(z) = R(x, y) eıΘ(x,y). Since the function has constant modulus
R, its partial derivatives vanish.
Rx = RΘy,
Ry = −RΘx
RΘy = 0,
RΘx = 0
The equations are satisﬁed for R = 0. For this case, f(z) = 0. We consider nonzero R.
Θy = 0,
Θx = 0
We see that the argument of f(z) is a constant and conclude that f(z) is constant.
Solution 8.7
First we verify that the Cauchy-Riemann equations are satisﬁed for z ̸= 0. Note that the form
fx = −ıfy
will be far more convenient than the form
ux = vy,
uy = −vx
388

for this problem.
fx = 4(x + ıy)−5 e−(x+ıy)−4
−ıfy = −ı4(x + ıy)−5ı e−(x+ıy)−4 = 4(x + ıy)−5 e−(x+ıy)−4
The Cauchy-Riemann equations are satisﬁed for z ̸= 0.
Now we consider the point z = 0.
fx(0, 0) = lim
∆x→0
f(∆x, 0) −f(0, 0)
∆x
= lim
∆x→0
e−∆x−4
∆x
= 0
−ıfy(0, 0) = −ı lim
∆y→0
f(0, ∆y) −f(0, 0)
∆y
= −ı lim
∆y→0
e−∆y−4
∆y
= 0
The Cauchy-Riemann equations are satisﬁed for z = 0.
f(z) is not analytic at the point z = 0. We show this by calculating the derivative.
f ′(0) = lim
∆z→0
f(∆z) −f(0)
∆z
= lim
∆z→0
f(∆z)
∆z
Let ∆z = ∆r eıθ, that is, we approach the origin at an angle of θ.
f ′(0) = lim
∆r→0
f
 ∆r eıθ
∆r eıθ
= lim
∆r→0
e−r−4 e−ı4θ
∆r eıθ
389

For most values of θ the limit does not exist. Consider θ = π/4.
f ′(0) = lim
∆r→0
er−4
∆r eıπ/4 = ∞
Because the limit does not exist, the function is not diﬀerentiable at z = 0. Recall that satisfying the Cauchy-Riemann
equations is a necessary, but not a suﬃcient condition for diﬀerentiability.
Solution 8.8
1. We ﬁnd the Cauchy-Riemann equations for
f(z) = R(r, θ) eıΘ(r,θ) .
From Exercise 8.3 we know that the complex derivative in the polar coordinate directions is
d
dz = e−ıθ ∂
∂r = −ı
r e−ıθ ∂
∂θ.
We equate the derivatives in the two directions.
e−ıθ ∂
∂r

R eıΘ
= −ı
r e−ıθ ∂
∂θ

R eıΘ
(Rr + ıRΘr) eıΘ = −ı
r (Rθ + ıRΘθ) eıΘ
We divide by eıΘ and equate the real and imaginary components to obtain the Cauchy-Riemann equations.
Rr = R
r Θθ,
1
rRθ = −RΘr
2. We ﬁnd the Cauchy-Riemann equations for
f(z) = R(x, y) eıΘ(x,y) .
390

We equate the derivatives in the x and y directions.
∂
∂x

R eıΘ
= −ı ∂
∂y

R eıΘ
(Rx + ıRΘy) eıΘ = −ı (Rx + ıRΘy) eıΘ
We divide by eıΘ and equate the real and imaginary components to obtain the Cauchy-Riemann equations.
Rx = RΘy,
Ry = −RΘx
Solution 8.9
1. A necessary condition for analyticity in an open set is that the Cauchy-Riemann equations are satisﬁed in that
set. We write ez in Cartesian form.
ez = ex−ıy = ex cos y −ı ex sin y.
Now we determine where u = ex cos y and v = −ex sin y satisfy the Cauchy-Riemann equations.
ux = vy,
uy = −vx
ex cos y = −ex cos y,
−ex sin y = ex sin y
cos y = 0,
sin y = 0
y = π
2 + πm,
y = πn
Thus we see that the Cauchy-Riemann equations are not satisﬁed anywhere. ez is nowhere analytic.
2. Since f(z) = u + ıv is analytic, u and v satisfy the Cauchy-Riemann equations and their ﬁrst partial derivatives
are continuous.
f(z) = f (z) = u(x, −y) + ıv(x, −y) = u(x, −y) −ıv(x, −y)
391

We deﬁne f(z) ≡µ(x, y) + ıν(x, y) = u(x, −y) −ıv(x, y). Now we see if µ and ν satisfy the Cauchy-Riemann
equations.
µx = νy,
µy = −νx
(u(x, −y))x = (−v(x, −y))y,
(u(x, −y))y = −(−v(x, −y))x
ux(x, −y) = vy(x, −y),
−uy(x, −y) = vx(x, −y)
ux = vy,
uy = −vx
Thus we see that the Cauchy-Riemann equations for µ and ν are satisﬁed if and only if the Cauchy-Riemann
equations for u and v are satisﬁed. The continuity of the ﬁrst partial derivatives of u and v implies the same of
µ and ν. Thus f(z) is analytic.
Solution 8.10
1. The necessary condition for a function f(z) = u + ıv to be diﬀerentiable at a point is that the Cauchy-Riemann
equations hold and the ﬁrst partial derivatives of u and v are continuous at that point.
(a)
f(z) = x3 + y3 + ı0
The Cauchy-Riemann equations are
ux = vy
and
uy = −vx
3x2 = 0
and
3y2 = 0
x = 0
and
y = 0
The ﬁrst partial derivatives are continuous. Thus we see that the function is diﬀerentiable only at the point
z = 0.
(b)
f(z) =
x −1
(x −1)2 + y2 −ı
y
(x −1)2 + y2
392

The Cauchy-Riemann equations are
ux = vy
and
uy = −vx
−(x −1)2 + y2
((x −1)2 + y2)2 = −(x −1)2 + y2
((x −1)2 + y2)2
and
2(x −1)y
((x −1)2 + y2)2 =
2(x −1)y
((x −1)2 + y2)2
The Cauchy-Riemann equations are each identities. The ﬁrst partial derivatives are continuous everywhere
except the point x = 1, y = 0. Thus the function is diﬀerentiable everywhere except z = 1.
2.
(a) The function is not diﬀerentiable in any open set. Thus the function is nowhere analytic.
(b) The function is diﬀerentiable everywhere except z = 1. Thus the function is analytic everywhere except
z = 1.
3.
(a) First we determine if the function is harmonic.
v = x2 −y2
vxx + vyy = 0
2 −2 = 0
The function is harmonic in the complex plane and this is the imaginary part of some analytic function. By
inspection, we see that this function is
ız2 + c = −2xy + c + ı
 x2 −y2
,
where c is a real constant. We can also ﬁnd the function by solving the Cauchy-Riemann equations.
ux = vy
and
uy = −vx
ux = −2y
and
uy = −2x
We integrate the ﬁrst equation.
u = −2xy + g(y)
393

Here g(y) is a function of integration. We substitute this into the second Cauchy-Riemann equation to
determine g(y).
uy = −2x
−2x + g′(y) = −2x
g′(y) = 0
g(y) = c
u = −2xy + c
f(z) = −2xy + c + ı
 x2 −y2
f(z) = ız2 + c
(b) First we determine if the function is harmonic.
v = 3x2y
vxx + vyy = 6y
The function is not harmonic. It is not the imaginary part of some analytic function.
Solution 8.11
We write the real and imaginary parts of f(z) = u + ıv.
u =
(
x4/3y5/3
x2+y2
for z ̸= 0,
0
for z = 0.
,
v =
(
x5/3y4/3
x2+y2
for z ̸= 0,
0
for z = 0.
The Cauchy-Riemann equations are
ux = vy,
uy = −vx.
394

We calculate the partial derivatives of u and v at the point x = y = 0 using the deﬁnition of diﬀerentiation.
ux(0, 0) = lim
∆x→0
u(∆x, 0) −u(0, 0)
∆x
= lim
∆x→0
0 −0
∆x = 0
vx(0, 0) = lim
∆x→0
v(∆x, 0) −v(0, 0)
∆x
= lim
∆x→0
0 −0
∆x = 0
uy(0, 0) = lim
∆y→0
u(0, ∆y) −u(0, 0)
∆y
= lim
∆y→0
0 −0
∆y
= 0
vy(0, 0) = lim
∆y→0
v(0, ∆y) −v(0, 0)
∆y
= lim
∆y→0
0 −0
∆y
= 0
Since ux(0, 0) = uy(0, 0) = vx(0, 0) = vy(0, 0) = 0 the Cauchy-Riemann equations are satisﬁed.
f(z) is not analytic at the point z = 0. We show this by calculating the derivative there.
f ′(0) = lim
∆z→0
f(∆z) −f(0)
∆z
= lim
∆z→0
f(∆z)
∆z
We let ∆z = ∆r eıθ, that is, we approach the origin at an angle of θ. Then x = ∆r cos θ and y = ∆r sin θ.
f ′(0) = lim
∆r→0
f
 ∆r eıθ
∆r eıθ
= lim
∆r→0
∆r4/3 cos4/3 θ∆r5/3 sin5/3 θ+ı∆r5/3 cos5/3 θ∆r4/3 sin4/3 θ
∆r2
∆r eıθ
= lim
∆r→0
cos4/3 θ sin5/3 θ + ı cos5/3 θ sin4/3 θ
eıθ
The value of the limit depends on θ and is not a constant.
Thus this limit does not exist.
The function is not
diﬀerentiable at z = 0.
Solution 8.12
u =
(
x3−y3
x2+y2
for z ̸= 0,
0
for z = 0. ,
v =
(
x3+y3
x2+y2
for z ̸= 0,
0
for z = 0.
395

The Cauchy-Riemann equations are
ux = vy,
uy = −vx.
The partial derivatives of u and v at the point x = y = 0 are,
ux(0, 0) = lim
∆x→0
u(∆x, 0) −u(0, 0)
∆x
= lim
∆x→0
∆x −0
∆x
= 1,
vx(0, 0) = lim
∆x→0
v(∆x, 0) −v(0, 0)
∆x
= lim
∆x→0
∆x −0
∆x
= 1,
uy(0, 0) = lim
∆y→0
u(0, ∆y) −u(0, 0)
∆y
= lim
∆y→0
−∆y −0
∆y
= −1,
vy(0, 0) = lim
∆y→0
v(0, ∆y) −v(0, 0)
∆y
= lim
∆y→0
∆y −0
∆y
= 1.
396

We see that the Cauchy-Riemann equations are satisﬁed at x = y = 0
f(z) is not analytic at the point z = 0. We show this by calculating the derivative.
f ′(0) = lim
∆z→0
f(∆z) −f(0)
∆z
= lim
∆z→0
f(∆z)
∆z
Let ∆z = ∆r eıθ, that is, we approach the origin at an angle of θ. Then x = ∆r cos θ and y = ∆r sin θ.
f ′(0) = lim
∆r→0
f
 ∆r eıθ
∆r eıθ
= lim
∆r→0
(1+ı)∆r3 cos3 θ−(1−ı)∆r3 sin3 θ
∆r2
∆r eıθ
= lim
∆r→0
(1 + ı) cos3 θ −(1 −ı) sin3 θ
eıθ
The value of the limit depends on θ and is not a constant.
Thus this limit does not exist.
The function is not
diﬀerentiable at z = 0.
Recall that satisfying the Cauchy-Riemann equations is a necessary, but not a suﬃcient
condition for diﬀerentiability.
Solution 8.13
We show that the logarithm log z = φ(r, θ) = Log r + ıθ satisﬁes the Cauchy-Riemann equations.
φr = −ı
rφθ
1
r = −ı
rı
1
r = 1
r
Since the logarithm satisﬁes the Cauchy-Riemann equations and the ﬁrst partial derivatives are continuous for z ̸= 0,
the logarithm is analytic for z ̸= 0.
397

Now we compute the derivative.
d
dz log z = e−ıθ ∂
∂r(Log r + ıθ)
= e−ıθ 1
r
= 1
z
Solution 8.14
The complex derivative in the coordinate directions is
d
dz = e−ıθ ∂
∂r = −ı
r e−ıθ ∂
∂θ.
We substitute f = u + ıv into this identity to obtain the Cauchy-Riemann equation in polar coordinates.
e−ıθ ∂f
∂r = −ı
r e−ıθ ∂f
∂θ
∂f
∂r = −ı
r
∂f
∂θ
ur + ıvr = −ı
r (uθ + ıvθ)
We equate the real and imaginary parts.
ur = 1
rvθ,
vr = −1
ruθ
ur = 1
rvθ,
uθ = −rvr
Solution 8.15
Since w is analytic, u and v satisfy the Cauchy-Riemann equations,
ux = vy
and
uy = −vx.
398

Using the chain rule we can write the derivatives with respect to x and y in terms of u and v.
∂
∂x = ux
∂
∂u + vx
∂
∂v
∂
∂y = uy
∂
∂u + vy
∂
∂v
Now we examine φx −ıφy.
φx −ıφy = uxΦu + vxΦv −ı (uyΦu + vyΦv)
φx −ıφy = (ux −ıuy) Φu + (vx −ıvy) Φv
φx −ıφy = (ux −ıuy) Φu −ı (vy + ıvx) Φv
We use the Cauchy-Riemann equations to write uy and vy in terms of ux and vx.
φx −ıφy = (ux + ıvx) Φu −ı (ux + ıvx) Φv
Recall that w′ = ux + ıvx = vy −ıuy.
φx −ıφy = dw
dz (Φu −ıΦv)
Thus we see that,
∂Φ
∂u −ı∂Φ
∂v =
dw
dz
−1 ∂φ
∂x −ı∂φ
∂y

.
We write this in operator notation.
∂
∂u −ı ∂
∂v =
dw
dz
−1  ∂
∂x −ı ∂
∂y

399

The complex conjugate of this relation is
∂
∂u + ı ∂
∂v =
dw
dz
−1  ∂
∂x + ı ∂
∂y

Now we apply both these operators to Φ = φ.
 ∂
∂u + ı ∂
∂v
  ∂
∂u −ı ∂
∂v

Φ =
dw
dz
−1  ∂
∂x + ı ∂
∂y
 dw
dz
−1  ∂
∂x −ı ∂
∂y

φ
 ∂2
∂u2 + ı ∂2
∂u∂v −ı ∂2
∂v∂u + ∂2
∂v2

Φ
=
dw
dz
−1 "  ∂
∂x + ı ∂
∂y
 dw
dz
−1!  ∂
∂x −ı ∂
∂y

+
dw
dz
−1  ∂
∂x + ı ∂
∂y
  ∂
∂x −ı ∂
∂y
#
φ
(w′)−1 is an analytic function. Recall that for analytic functions f, f ′ = fx = −ıfy. So that fx + ıfy = 0.
∂2Φ
∂u2 + ∂2Φ
∂v2 =
dw
dz
−1 "dw
dz
−1  ∂2
∂x2 + ∂2
∂y2
#
φ
∂2Φ
∂u2 + ∂2Φ
∂v2 =

dw
dz

−2 ∂2φ
∂x2 + ∂2φ
∂y2

Solution 8.16
1. We consider
f(z) = log |z| + ı arg(z) = log r + ıθ.
The Cauchy-Riemann equations in polar coordinates are
ur = 1
rvθ,
uθ = −rvr.
400

We calculate the derivatives.
ur = 1
r,
1
rvθ = 1
r
uθ = 0,
−rvr = 0
Since the Cauchy-Riemann equations are satisﬁed and the partial derivatives are continuous, f(z) is analytic in
|z| > 0, | arg(z)| < π. The complex derivative in terms of polar coordinates is
d
dz = e−ıθ ∂
∂r = −ı
r e−ıθ ∂
∂θ.
We use this to diﬀerentiate f(z).
df
dz = e−ıθ ∂
∂r[log r + ıθ] = e−ıθ 1
r = 1
z
2. Next we consider
f(z) =
p
|z| eı arg(z)/2 = √r eıθ/2 .
The Cauchy-Riemann equations for polar coordinates and the polar form f(z) = R(r, θ) eıΘ(r,θ) are
Rr = R
r Θθ,
1
rRθ = −RΘr.
We calculate the derivatives for R = √r, Θ = θ/2.
Rr =
1
2√r,
R
r Θθ =
1
2√r
1
rRθ = 0,
−RΘr = 0
Since the Cauchy-Riemann equations are satisﬁed and the partial derivatives are continuous, f(z) is analytic in
|z| > 0, | arg(z)| < π. The complex derivative in terms of polar coordinates is
d
dz = e−ıθ ∂
∂r = −ı
r e−ıθ ∂
∂θ.
401

We use this to diﬀerentiate f(z).
df
dz = e−ıθ ∂
∂r[√r eıθ/2] =
1
2 eıθ/2 √r =
1
2√z
Solution 8.17
1. We consider the function
u = x Log r −y arctan(x, y) = r cos θ Log r −rθ sin θ
We compute the Laplacian.
∆u = 1
r
∂
∂r

r∂u
∂r

+ 1
r2
∂2u
∂θ2
= 1
r
∂
∂r (cos θ(r + r Log r) −θ sin θ) + 1
r2 (r(θ sin θ −2 cos θ) −r cos θ Log r)
= 1
r(2 cos θ + cos θ Log r −θ sin θ) + 1
r(θ sin θ −2 cos θ −cos θ Log r)
= 0
The function u is harmonic. We ﬁnd the harmonic conjugate v by solving the Cauchy-Riemann equations.
vr = −1
ruθ,
vθ = rur
vr = sin θ(1 + Log r) + θ cos θ,
vθ = r (cos θ(1 + Log r) −θ sin θ)
We integrate the ﬁrst equation with respect to r to determine v to within the constant of integration g(θ).
v = r(sin θ Log r + θ cos θ) + g(θ)
We diﬀerentiate this expression with respect to θ.
vθ = r (cos θ(1 + Log r) −θ sin θ) + g′(θ)
402

We compare this to the second Cauchy-Riemann equation to see that g′(θ) = 0. Thus g(θ) = c. We have
determined the harmonic conjugate.
v = r(sin θ Log r + θ cos θ) + c
The corresponding analytic function is
f(z) = r cos θ Log r −rθ sin θ + ı(r sin θ Log r + rθ cos θ + c).
On the positive real axis, (θ = 0), the function has the value
f(z = r) = r Log r + ıc.
We use analytic continuation to determine the function in the complex plane.
f(z) = z log z + ıc
2. We consider the function
u = Arg(z) = θ.
We compute the Laplacian.
∆u = 1
r
∂
∂r

r∂u
∂r

+ 1
r2
∂2u
∂θ2 = 0
The function u is harmonic. We ﬁnd the harmonic conjugate v by solving the Cauchy-Riemann equations.
vr = −1
ruθ,
vθ = rur
vr = −1
r,
vθ = 0
We integrate the ﬁrst equation with respect to r to determine v to within the constant of integration g(θ).
v = −Log r + g(θ)
403

We diﬀerentiate this expression with respect to θ.
vθ = g′(θ)
We compare this to the second Cauchy-Riemann equation to see that g′(θ) = 0. Thus g(θ) = c. We have
determined the harmonic conjugate.
v = −Log r + c
The corresponding analytic function is
f(z) = θ −ı Log r + ıc
On the positive real axis, (θ = 0), the function has the value
f(z = r) = −ı Log r + ıc
We use analytic continuation to determine the function in the complex plane.
f(z) = −ı log z + ıc
3. We consider the function
u = rn cos(nθ)
We compute the Laplacian.
∆u = 1
r
∂
∂r

r∂u
∂r

+ 1
r2
∂2u
∂θ2
= 1
r
∂
∂r (nrn cos(nθ)) −n2rn−2 cos(nθ)
= n2rn−2 cos(nθ) −n2rn−2 cos(nθ)
= 0
404

The function u is harmonic. We ﬁnd the harmonic conjugate v by solving the Cauchy-Riemann equations.
vr = −1
ruθ,
vθ = rur
vr = nrn−1 sin(nθ),
vθ = nrn cos(nθ)
We integrate the ﬁrst equation with respect to r to determine v to within the constant of integration g(θ).
v = rn sin(nθ) + g(θ)
We diﬀerentiate this expression with respect to θ.
vθ = nrn cos(nθ) + g′(θ)
We compare this to the second Cauchy-Riemann equation to see that g′(θ) = 0. Thus g(θ) = c. We have
determined the harmonic conjugate.
v = rn sin(nθ) + c
The corresponding analytic function is
f(z) = rn cos(nθ) + ırn sin(nθ) + ıc
On the positive real axis, (θ = 0), the function has the value
f(z = r) = rn + ıc
We use analytic continuation to determine the function in the complex plane.
f(z) = zn
4. We consider the function
u = y
r2 = sin θ
r
405

We compute the Laplacian.
∆u = 1
r
∂
∂r

r∂u
∂r

+ 1
r2
∂2u
∂θ2
= 1
r
∂
∂r

−sin θ
r

−sin θ
r3
= sin θ
r3
−sin θ
r3
= 0
The function u is harmonic. We ﬁnd the harmonic conjugate v by solving the Cauchy-Riemann equations.
vr = −1
ruθ,
vθ = rur
vr = −cos θ
r2 ,
vθ = −sin θ
r
We integrate the ﬁrst equation with respect to r to determine v to within the constant of integration g(θ).
v = cos θ
r
+ g(θ)
We diﬀerentiate this expression with respect to θ.
vθ = −sin θ
r
+ g′(θ)
We compare this to the second Cauchy-Riemann equation to see that g′(θ) = 0. Thus g(θ) = c. We have
determined the harmonic conjugate.
v = cos θ
r
+ c
The corresponding analytic function is
f(z) = sin θ
r
+ ıcos θ
r
+ ıc
406

On the positive real axis, (θ = 0), the function has the value
f(z = r) = ı
r + ıc.
We use analytic continuation to determine the function in the complex plane.
f(z) = ı
z + ıc
Solution 8.18
1. We calculate the ﬁrst partial derivatives of u = (x −y)2 and v = 2(x + y).
ux = 2(x −y)
uy = 2(y −x)
vx = 2
vy = 2
We substitute these expressions into the Cauchy-Riemann equations.
ux = vy,
uy = −vx
2(x −y) = 2,
2(y −x) = −2
x −y = 1,
y −x = −1
y = x −1
Since the Cauchy-Riemann equation are satisﬁed along the line y = x−1 and the partial derivatives are continuous,
the function f(z) is diﬀerentiable there. Since the function is not diﬀerentiable in a neighborhood of any point,
it is nowhere analytic.
407

2. We calculate the ﬁrst partial derivatives of u and v.
ux = 2 ex2−y2(x cos(2xy) −y sin(2xy))
uy = −2 ex2−y2(y cos(2xy) + x sin(2xy))
vx = 2 ex2−y2(y cos(2xy) + x sin(2xy))
vy = 2 ex2−y2(x cos(2xy) −y sin(2xy))
Since the Cauchy-Riemann equations, ux = vy and uy = −vx, are satisﬁed everywhere and the partial derivatives
are continuous, f(z) is everywhere diﬀerentiable. Since f(z) is diﬀerentiable in a neighborhood of every point, it
is analytic in the complex plane. (f(z) is entire.)
Now to evaluate the derivative. The complex derivative is the derivative in any direction. We choose the x
direction.
f ′(z) = ux + ıvx
f ′(z) = 2 ex2−y2(x cos(2xy) −y sin(2xy)) + ı2 ex2−y2(y cos(2xy) + x sin(2xy))
f ′(z) = 2 ex2−y2((x + ıy) cos(2xy) + (−y + ıx) sin(2xy))
Finding the derivative is easier if we ﬁrst write f(z) in terms of the complex variable z and use complex diﬀeren-
tiation.
f(z) = ex2−y2(cos(2x, y) + ı sin(2xy))
f(z) = ex2−y2 eı2xy
f(z) = e(x+ıy)2
f(z) = ez2
f ′(z) = 2z ez2
408

Solution 8.19
1. Assume that the Cauchy-Riemann equations in Cartesian coordinates
ux = vy,
uy = −vx
are satisﬁed and these partial derivatives are continuous at a point z. We write the derivatives in polar coordinates
in terms of derivatives in Cartesian coordinates to verify the Cauchy-Riemann equations in polar coordinates. First
we calculate the derivatives.
x = r cos θ,
y = r sin θ
wr = ∂x
∂r wx + ∂y
∂r wy = cos θwx + sin θwy
wθ = ∂x
∂θ wx + ∂y
∂θwy = −r sin θwx + r cos θwy
Then we verify the Cauchy-Riemann equations in polar coordinates.
ur = cos θux + sin θuy
= cos θvy −sin θvx
= 1
rvθ
1
ruθ = −sin θux + cos θuy
= −sin θvy −cos θvx
= −vr
This proves that the Cauchy-Riemann equations in Cartesian coordinates hold only if the Cauchy-Riemann equa-
tions in polar coordinates hold. (Given that the partial derivatives are continuous.) Next we prove the converse.
Assume that the Cauchy-Riemann equations in polar coordinates
ur = 1
rvθ,
1
ruθ = −vr
409

are satisﬁed and these partial derivatives are continuous at a point z. We write the derivatives in Cartesian
coordinates in terms of derivatives in polar coordinates to verify the Cauchy-Riemann equations in Cartesian
coordinates. First we calculate the derivatives.
r =
p
x2 + y2,
θ = arctan(x, y)
wx = ∂r
∂xwr + ∂θ
∂xwθ = x
r wr −y
r2wθ
wy = ∂r
∂ywr + ∂θ
∂ywθ = y
rwr + x
r2wθ
Then we verify the Cauchy-Riemann equations in Cartesian coordinates.
ux = x
r ur −y
r2uθ
= x
r2vθ + y
rvr
= uy
uy = y
rur + x
r2uθ
= y
r2vθ −x
r vr
= −ux
This proves that the Cauchy-Riemann equations in polar coordinates hold only if the Cauchy-Riemann equations
in Cartesian coordinates hold. We have demonstrated the equivalence of the two forms.
2. We verify that log z is analytic for r > 0 and −π < θ < π using the polar form of the Cauchy-Riemann equations.
Log z = ln r + ıθ
ur = 1
rvθ,
1
ruθ = −vr
1
r = 1
r1,
1
r0 = −0
410

Since the Cauchy-Riemann equations are satisﬁed and the partial derivatives are continuous for r > 0, log z is
analytic there. We calculate the value of the derivative using the polar diﬀerentiation formulas.
d
dz Log z = e−ıθ ∂
∂r(ln r + ıθ) = e−ıθ 1
r = 1
z
d
dz Log z = −ı
z
∂
∂θ(ln r + ıθ) = −ı
z ı = 1
z
3. Let {xi} denote rectangular coordinates in two dimensions and let {ξi} be an orthogonal coordinate system .
The distance metric coeﬃcients hi are deﬁned
hi =
s∂x1
∂ξi
2
+
∂x2
∂ξi
2
.
The Laplacian is
∇2u =
1
h1h2
 ∂
∂ξ1
h2
h1
∂u
∂ξ1

+ ∂
∂ξ2
h1
h2
∂u
∂ξ2

.
First we calculate the distance metric coeﬃcients in polar coordinates.
hr =
s∂x
∂r
2
+
∂y
∂r
2
=
p
cos2 θ + sin2 θ = 1
hθ =
s∂x
∂θ
2
+
∂y
∂θ
2
=
p
r2 sin2 θ + r2 cos2 θ = r
Then we ﬁnd the Laplacian.
∇2φ = 1
r
 ∂
∂r(rφr) + ∂
∂θ
1
rφθ

In polar coordinates, Laplace’s equation is
φrr + 1
rφr + 1
r2φθθ = 0.
411

Solution 8.20
1. We compute the Laplacian of u(x, y) = x3 −y3.
∇2u = 6x −6y
Since u is not harmonic, it is not the real part of on analytic function.
2. We compute the Laplacian of u(x, y) = sinh x cos y + x.
∇2u = sinh x cos y −sinh x cos y = 0
Since u is harmonic, it is the real part of on analytic function. We determine v by solving the Cauchy-Riemann
equations.
vx = −uy,
vy = ux
vx = sinh x sin y,
vy = cosh x cos y + 1
We integrate the ﬁrst equation to determine v up to an arbitrary additive function of y.
v = cosh x sin y + g(y)
We substitute this into the second Cauchy-Riemann equation. This will determine v up to an additive constant.
vy = cosh x cos y + 1
cosh x cos y + g′(y) = cosh x cos y + 1
g′(y) = 1
g(y) = y + a
v = cosh x sin y + y + a
f(z) = sinh x cos y + x + ı(cosh x sin y + y + a)
Here a is a real constant. We write the function in terms of z.
f(z) = sinh z + z + ıa
412

3. We compute the Laplacian of u(r, θ) = rn cos(nθ).
∇2u = n(n −1)rn−2 cos(nθ) + nrn−2 cos(nθ) −n2rn−2 cos(nθ) = 0
Since u is harmonic, it is the real part of on analytic function. We determine v by solving the Cauchy-Riemann
equations.
vr = −1
ruθ,
vθ = rur
vr = nrn−1 sin(nθ),
vθ = nrn cos(nθ)
We integrate the ﬁrst equation to determine v up to an arbitrary additive function of θ.
v = rn sin(nθ) + g(θ)
We substitute this into the second Cauchy-Riemann equation. This will determine v up to an additive constant.
vθ = nrn cos(nθ)
nrn cos(nθ) + g′(θ) = nrn cos(nθ)
g′(θ) = 0
g(θ) = a
v = rn sin(nθ) + a
f(z) = rn cos(nθ) + ı(rn sin(nθ) + a)
Here a is a real constant. We write the function in terms of z.
f(z) = zn + ıa
Solution 8.21
1. We ﬁnd the velocity potential φ and stream function ψ.
Φ(z) = log z + ı log z
Φ(z) = ln r + ıθ + ı(ln r + ıθ)
φ = ln r −θ,
ψ = ln r + θ
413

Figure 8.7: The velocity potential φ and stream function ψ for Φ(z) = log z + ı log z.
A branch of these are plotted in Figure 8.7.
Next we ﬁnd the stream lines, ψ = c.
ln r + θ = c
r = ec−θ
These are spirals which go counter-clockwise as we follow them to the origin. See Figure 8.8. Next we ﬁnd the
414

Figure 8.8: Streamlines for ψ = ln r + θ.
velocity ﬁeld.
v = ∇φ
v = φrˆr + φθ
r
ˆθ
v = ˆr
r −
ˆθ
r
The velocity ﬁeld is shown in the ﬁrst plot of Figure 8.9. We see that the ﬂuid ﬂows out from the origin along
the spiral paths of the streamlines. The second plot shows the direction of the velocity ﬁeld.
2. We ﬁnd the velocity potential φ and stream function ψ.
Φ(z) = log(z −1) + log(z + 1)
Φ(z) = ln |z −1| + ı arg(z −1) + ln |z + 1| + ı arg(z + 1)
φ = ln |z2 −1|,
ψ = arg(z −1) + arg(z + 1)
415

Figure 8.9: Velocity ﬁeld and velocity direction ﬁeld for φ = ln r −θ.
-2 -1
0
1
2-2
-1
0
1
2
-1012
2 -1
0
1
-2 -1
0
1
2-2
-1
0
1
2
0
2
4
6
2 -1
0
1
Figure 8.10: The velocity potential φ and stream function ψ for Φ(z) = log(z −1) + log(z + 1).
The velocity potential and a branch of the stream function are plotted in Figure 8.10.
The stream lines, arg(z −1) + arg(z + 1) = c, are plotted in Figure 8.11.
416

-2
-1
0
1
2
-2
-1
0
1
2
Figure 8.11: Streamlines for ψ = arg(z −1) + arg(z + 1).
Next we ﬁnd the velocity ﬁeld.
v = ∇φ
v =
2x(x2 + y2 −1)
x4 + 2x2(y2 −1) + (y2 + 1)2 ˆx +
2y(x2 + y2 + 1)
x4 + 2x2(y2 −1) + (y2 + 1)2 ˆy
The velocity ﬁeld is shown in the ﬁrst plot of Figure 8.12. The ﬂuid is ﬂowing out of sources at z = ±1. The
second plot shows the direction of the velocity ﬁeld.
417

Figure 8.12: Velocity ﬁeld and velocity direction ﬁeld for φ = ln |z2 −1|.
418

Chapter 9
Analytic Continuation
For every complex problem, there is a solution that is simple, neat, and wrong.
- H. L. Mencken
9.1
Analytic Continuation
Suppose there is a function, f1(z) that is analytic in the domain D1 and another analytic function, f2(z) that is
analytic in the domain D2. (See Figure 9.1.)
If the two domains overlap and f1(z) = f2(z) in the overlap region D1 ∩D2, then f2(z) is called an analytic
continuation of f1(z). This is an appropriate name since f2(z) continues the deﬁnition of f1(z) outside of its original
domain of deﬁnition D1. We can deﬁne a function f(z) that is analytic in the union of the domains D1 ∪D2. On the
domain D1 we have f(z) = f1(z) and f(z) = f2(z) on D2. f1(z) and f2(z) are called function elements. There is an
analytic continuation even if the two domains only share an arc and not a two dimensional region.
With more overlapping domains D3, D4, . . . we could perhaps extend f1(z) to more of the complex plane. Sometimes
it is impossible to extend a function beyond the boundary of a domain. This is known as a natural boundary. If a
419

Im(z)
Re(z)
D
D
1
2
Figure 9.1: Overlapping Domains
function f1(z) is analytically continued to a domain Dn along two diﬀerent paths, (See Figure 9.2.), then the two
analytic continuations are identical as long as the paths do not enclose a branch point of the function. This is the
uniqueness theorem of analytic continuation.
D1
Dn
Figure 9.2: Two Paths of Analytic Continuation
Consider an analytic function f(z) deﬁned in the domain D. Suppose that f(z) = 0 on the arc AB, (see Figure 9.3.)
Then f(z) = 0 in all of D.
Consider a point ζ on AB. The Taylor series expansion of f(z) about the point z = ζ converges in a circle C at
420

D
B
ζ
C
A
Figure 9.3: Domain Containing Arc Along Which f(z) Vanishes
least up to the boundary of D. The derivative of f(z) at the point z = ζ is
f ′(ζ) = lim
∆z→0
f(ζ + ∆z) −f(ζ)
∆z
If ∆z is in the direction of the arc, then f ′(ζ) vanishes as well as all higher derivatives, f ′(ζ) = f ′′(ζ) = f ′′′(ζ) = · · · = 0.
Thus we see that f(z) = 0 inside C. By taking Taylor series expansions about points on AB or inside of C we see that
f(z) = 0 in D.
Result 9.1.1 Let f1(z) and f2(z) be analytic functions deﬁned in D. If f1(z) = f2(z) for
the points in a region or on an arc in D, then f1(z) = f2(z) for all points in D.
To prove Result 9.1.1, we deﬁne the analytic function g(z) = f1(z) −f2(z). Since g(z) vanishes in the region or
on the arc, then g(z) = 0 and hence f1(z) = f2(z) for all points in D.
421

Result 9.1.2 Consider analytic functions f1(z) and f2(z) deﬁned on the domains D1 and
D2, respectively. Suppose that D1 ∩D2 is a region or an arc and that f1(z) = f2(z) for all
z ∈D1 ∩D2. (See Figure 9.4.) Then the function
f(z) =
(
f1(z)
for z ∈D1,
f2(z)
for z ∈D2,
is analytic in D1 ∪D2.
D1
D2
D1
D2
Figure 9.4: Domains that Intersect in a Region or an Arc
Result 9.1.2 follows directly from Result 9.1.1.
9.2
Analytic Continuation of Sums
Example 9.2.1 Consider the function
f1(z) =
∞
X
n=0
zn.
The sum converges uniformly for D1 = |z| ≤r < 1. Since the derivative also converges in this domain, the function is
analytic there.
422

Im(z)
Re(z)
Im(z)
Re(z)
D2
D1
Figure 9.5: Domain of Convergence for P∞
n=0 zn.
Now consider the function
f2(z) =
1
1 −z.
This function is analytic everywhere except the point z = 1. On the domain D1,
f2(z) =
1
1 −z =
∞
X
n=0
zn = f1(z)
Analytic continuation tells us that there is a function that is analytic on the union of the two domains. Here, the
domain is the entire z plane except the point z = 1 and the function is
f(z) =
1
1 −z.
1
1−z is said to be an analytic continuation of P∞
n=0 zn.
423

9.3
Analytic Functions Deﬁned in Terms of Real Variables
Result 9.3.1 An analytic function, u(x, y) + ıv(x, y) can be written in terms of a function
of a complex variable, f(z) = u(x, y) + ıv(x, y).
Result 9.3.1 is proved in Exercise 9.1.
Example 9.3.1
f(z) = cosh y sin x (x ex cos y −y ex sin y) −cos x sinh y (y ex cos y + x ex sin y)
+ ı

cosh y sin x (y ex cos y + x ex sin y) + cos x sinh y (x ex cos y −y ex sin y)

is an analytic function. Express f(z) in terms of z.
On the real line, y = 0, f(z) is
f(z = x) = x ex sin x
(Recall that cos(0) = cosh(0) = 1 and sin(0) = sinh(0) = 0.)
The analytic continuation of f(z) into the complex plane is
f(z) = z ez sin z.
Alternatively, for x = 0 we have
f(z = ıy) = y sinh y(cos y −ı sin y).
The analytic continuation from the imaginary axis to the complex plane is
f(z) = −ız sinh(−ız)(cos(−ız) −ı sin(−ız))
= ız sinh(ız)(cos(ız) + ı sin(ız))
= z sin z ez .
424

Example 9.3.2 Consider u = e−x(x sin y −y cos y). Find v such that f(z) = u + ıv is analytic.
From the Cauchy-Riemann equations,
∂v
∂y = ∂u
∂x = e−x sin y −x e−x sin y + y e−x cos y
∂v
∂x = −∂u
∂y = e−x cos y −x e−x cos y −y e−x sin y
Integrate the ﬁrst equation with respect to y.
v = −e−x cos y + x e−x cos y + e−x(y sin y + cos y) + F(x)
= y e−x sin y + x e−x cos y + F(x)
F(x) is an arbitrary function of x. Substitute this expression for v into the equation for ∂v/∂x.
−y e−x sin y −x e−x cos y + e−x cos y + F ′(x) = −y e−x sin y −x e−x cos y + e−x cos y
Thus F ′(x) = 0 and F(x) = c.
v = e−x(y sin y + x cos y) + c
Example 9.3.3 Find f(z) in the previous example. (Up to the additive constant.)
Method 1
f(z) = u + ıv
= e−x(x sin y −y cos y) + ı e−x(y sin y + x cos y)
= e−x

x
eıy −e−ıy
ı2

−y
eıy + e−ıy
2

+ ı e−x

y
eıy −e−ıy
ı2

+ x
eıy + e−ıy
2

= ı(x + ıy) e−(x+ıy)
= ız e−z
425

Method 2
f(z) = f(x + ıy) = u(x, y) + ıv(x, y) is an analytic function.
On the real axis, y = 0, f(z) is
f(z = x) = u(x, 0) + ıv(x, 0)
= e−x(x sin 0 −0 cos 0) + ı e−x(0 sin 0 + x cos 0)
= ıx e−x
Suppose there is an analytic continuation of f(z) into the complex plane. If such a continuation, f(z), exists, then it
must be equal to f(z = x) on the real axis An obvious choice for the analytic continuation is
f(z) = u(z, 0) + ıv(z, 0)
since this is clearly equal to u(x, 0) + ıv(x, 0) when z is real. Thus we obtain
f(z) = ız e−z
Example 9.3.4 Consider f(z) = u(x, y) + ıv(x, y). Show that f ′(z) = ux(z, 0) −ıuy(z, 0).
f ′(z) = ux + ıvx
= ux −ıuy
f ′(z) is an analytic function. On the real axis, z = x, f ′(z) is
f ′(z = x) = ux(x, 0) −ıuy(x, 0)
Now f ′(z = x) is deﬁned on the real line. An analytic continuation of f ′(z = x) into the complex plane is
f ′(z) = ux(z, 0) −ıuy(z, 0).
426

Example 9.3.5 Again consider the problem of ﬁnding f(z) given that u(x, y) = e−x(x sin y −y cos y). Now we can
use the result of the previous example to do this problem.
ux(x, y) = ∂u
∂x = e−x sin y −x e−x sin y + y e−x cos y
uy(x, y) = ∂u
∂y = x e−x cos y + y e−x sin y −e−x cos y
f ′(z) = ux(z, 0) −ıuy(z, 0)
= 0 −ı
 z e−z −e−z
= ı
 −z e−z + e−z
Integration yields the result
f(z) = ız e−z +c
Example 9.3.6 Find f(z) given that
u(x, y) = cos x cosh2 y sin x + cos x sin x sinh2 y
v(x, y) = cos2 x cosh y sinh y −cosh y sin2 x sinh y
f(z) = u(x, y) + ıv(x, y) is an analytic function. On the real line, f(z) is
f(z = x) = u(x, 0) + ıv(x, 0)
= cos x cosh2 0 sin x + cos x sin x sinh2 0 + ı
 cos2 x cosh 0 sinh 0 −cosh 0 sin2 x sinh 0

= cos x sin x
Now we know the deﬁnition of f(z) on the real line. We would like to ﬁnd an analytic continuation of f(z) into the
complex plane. An obvious choice for f(z) is
f(z) = cos z sin z
427

Using trig identities we can write this as
f(z) = sin(2z)
2
.
Example 9.3.7 Find f(z) given only that
u(x, y) = cos x cosh2 y sin x + cos x sin x sinh2 y.
Recall that
f ′(z) = ux + ıvx
= ux −ıuy
Diﬀerentiating u(x, y),
ux = cos2 x cosh2 y −cosh2 y sin2 x + cos2 x sinh2 y −sin2 x sinh2 y
uy = 4 cos x cosh y sin x sinh y
f ′(z) is an analytic function. On the real axis, f ′(z) is
f ′(z = x) = cos2 x −sin2 x
Using trig identities we can write this as
f ′(z = x) = cos(2x)
Now we ﬁnd an analytic continuation of f ′(z = x) into the complex plane.
f ′(z) = cos(2z)
Integration yields the result
f(z) = sin(2z)
2
+ c
428

9.3.1
Polar Coordinates
Example 9.3.8 Is
u(r, θ) = r(log r cos θ −θ sin θ)
the real part of an analytic function?
The Laplacian in polar coordinates is
∆φ = 1
r
∂
∂r

r∂φ
∂r

+ 1
r2
∂2φ
∂θ2 .
We calculate the partial derivatives of u.
∂u
∂r = cos θ + log r cos θ −θ sin θ
r∂u
∂r = r cos θ + r log r cos θ −rθ sin θ
∂
∂r

r∂u
∂r

= 2 cos θ + log r cos θ −θ sin θ
1
r
∂
∂r

r∂u
∂r

= 1
r (2 cos θ + log r cos θ −θ sin θ)
∂u
∂θ = −r (θ cos θ + sin θ + log r sin θ)
∂2u
∂θ2 = r (−2 cos θ −log r cos θ + θ sin θ)
1
r2
∂2u
∂θ2 = 1
r (−2 cos θ −log r cos θ + θ sin θ)
From the above we see that
∆u = 1
r
∂
∂r

r∂u
∂r

+ 1
r2
∂2u
∂θ2 = 0.
Therefore u is harmonic and is the real part of some analytic function.
429

Example 9.3.9 Find an analytic function f(z) whose real part is
u(r, θ) = r (log r cos θ −θ sin θ) .
Let f(z) = u(r, θ) + ıv(r, θ). The Cauchy-Riemann equations are
ur = vθ
r ,
uθ = −rvr.
Using the partial derivatives in the above example, we obtain two partial diﬀerential equations for v(r, θ).
vr = −uθ
r = θ cos θ + sin θ + log r sin θ
vθ = rur = r (cos θ + log r cos θ −θ sin θ)
Integrating the equation for vθ yields
v = r (θ cos θ + log r sin θ) + F(r)
where F(r) is a constant of integration.
Substituting our expression for v into the equation for vr yields
θ cos θ + log r sin θ + sin θ + F ′(r) = θ cos θ + sin θ + log r sin θ
F ′(r) = 0
F(r) = const
Thus we see that
f(z) = u + ıv
= r (log r cos θ −θ sin θ) + ır (θ cos θ + log r sin θ) + const
f(z) is an analytic function. On the line θ = 0, f(z) is
f(z = r) = r(log r) + ır(0) + const
= r log r + const
430

The analytic continuation into the complex plane is
f(z) = z log z + const
Example 9.3.10 Find the formula in polar coordinates that is analogous to
f ′(z) = ux(z, 0) −ıuy(z, 0).
We know that
df
dz = e−ıθ ∂f
∂r .
If f(z) = u(r, θ) + ıv(r, θ) then
df
dz = e−ıθ (ur + ıvr)
From the Cauchy-Riemann equations, we have vr = −uθ/r.
df
dz = e−ıθ 
ur −ıuθ
r

f ′(z) is an analytic function. On the line θ = 0, f(z) is
f ′(z = r) = ur(r, 0) −ıuθ(r, 0)
r
The analytic continuation of f ′(z) into the complex plane is
f ′(z) = ur(z, 0) −ı
ruθ(z, 0).
431

Example 9.3.11 Find an analytic function f(z) whose real part is
u(r, θ) = r (log r cos θ −θ sin θ) .
ur(r, θ) = (log r cos θ −θ sin θ) + cos θ
uθ(r, θ) = r (−log r sin θ −sin θ −θ cos θ)
f ′(z) = ur(z, 0) −ı
ruθ(z, 0)
= log z + 1
Integrating f ′(z) yields
f(z) = z log z + ıc.
9.3.2
Analytic Functions Deﬁned in Terms of Their Real or Imaginary Parts
Consider an analytic function: f(z) = u(x, y) + ıv(x, y). We diﬀerentiate this expression.
f ′(z) = ux(x, y) + ıvx(x, y)
We apply the Cauchy-Riemann equation vx = −uy.
f ′(z) = ux(x, y) −ıuy(x, y).
(9.1)
Now consider the function of a complex variable, g(ζ):
g(ζ) = ux(x, ζ) −ıuy(x, ζ) = ux(x, ξ + ıψ) −ıuy(x, ξ + ıψ).
432

This function is analytic where f(ζ) is analytic. To show this we ﬁrst verify that the derivatives in the ξ and ψ directions
are equal.
∂
∂ξg(ζ) = uxy(x, ξ + ıψ) −ıuyy(x, ξ + ıψ)
−ı ∂
∂ψg(ζ) = −ı (ıuxy(x, ξ + ıψ) + uyy(x, ξ + ıψ)) = uxy(x, ξ + ıψ) −ıuyy(x, ξ + ıψ)
Since these partial derivatives are equal and continuous, g(ζ) is analytic. We evaluate the function g(ζ) at ζ = −ıx.
(Substitute y = −ıx into Equation 9.1.)
f ′(2x) = ux(x, −ıx) −ıuy(x, −ıx)
We make a change of variables to solve for f ′(x).
f ′(x) = ux
x
2, −ıx
2

−ıuy
x
2, −ıx
2

.
If the expression is non-singular, then this deﬁnes the analytic function, f ′(z), on the real axis. The analytic continuation
to the complex plane is
f ′(z) = ux
z
2, −ız
2

−ıuy
z
2, −ız
2

.
Note that
d
dz2u(z/2, −ız/2) = ux(z/2, −ız/2) −ıuy(z/2, −ız/2). We integrate the equation to obtain:
f(z) = 2u
z
2, −ız
2

+ c.
We know that the real part of an analytic function determines that function to within an additive constant. Assuming
that the above expression is non-singular, we have found a formula for writing an analytic function in terms of its real
part. With the same method, we can ﬁnd how to write an analytic function in terms of its imaginary part, v.
We can also derive formulas if u and v are expressed in polar coordinates:
f(z) = u(r, θ) + ıv(r, θ).
433

Result 9.3.2 If f(z) = u(x, y) + ıv(x, y) is analytic and the expressions are non-singular,
then
f(z) = 2u
z
2, −ız
2

+ const
(9.2)
f(z) = ı2v
z
2, −ız
2

+ const.
(9.3)
If f(z) = u(r, θ) + ıv(r, θ) is analytic and the expressions are non-singular, then
f(z) = 2u

z1/2, −ı
2 log z

+ const
(9.4)
f(z) = ı2v

z1/2, −ı
2 log z

+ const.
(9.5)
Example 9.3.12 Consider the problem of ﬁnding f(z) given that u(x, y) = e−x(x sin y −y cos y).
f(z) = 2u
z
2, −ız
2

= 2 e−z/2 z
2 sin

−ız
2

+ ız
2 cos

−ız
2

+ c
= ız e−z/2 
ı sin

ız
2

+ cos

−ız
2

+ c
= ız e−z/2  e−z/2
+ c
= ız e−z +c
Example 9.3.13 Consider
Log z = 1
2 Log
 x2 + y2
+ ı Arctan(x, y).
434

We try to construct the analytic function from it’s real part using Equation 9.2.
f(z) = 2u
z
2, −ız
2

+ c
= 21
2 Log
z
2
2
+

−ız
2
2
+ c
= Log(0) + c
We obtain a singular expression, so the method fails.
Example 9.3.14 Again consider the logarithm, this time written in terms of polar coordinates.
Log z = Log r + ıθ
We try to construct the analytic function from it’s real part using Equation 9.4.
f(z) = 2u

z1/2, −ı ı
2 log z

+ c
= 2 Log
 z1/2
+ c
= Log z + c
With this method we recover the analytic function.
435

9.4
Exercises
Exercise 9.1
Consider two functions, f(x, y) and g(x, y). They are said to be functionally dependent if there is a an h(g) such that
f(x, y) = h(g(x, y)).
f and g will be functionally dependent if and only if their Jacobian vanishes.
If f and g are functionally dependent, then the derivatives of f are
fx = h′(g)gx
fy = h′(g)gy.
Thus we have
∂(f, g)
∂(x, y) =

fx
fy
gx
gy
 = fxgy −fygx = h′(g)gxgy −h′(g)gygx = 0.
If the Jacobian of f and g vanishes, then
fxgy −fygx = 0.
This is a ﬁrst order partial diﬀerential equation for f that has the general solution
f(x, y) = h(g(x, y)).
Prove that an analytic function u(x, y) + ıv(x, y) can be written in terms of a function of a complex variable,
f(z) = u(x, y) + ıv(x, y).
Exercise 9.2
Which of the following functions are the real part of an analytic function? For those that are, ﬁnd the harmonic
conjugate, v(x, y), and ﬁnd the analytic function f(z) = u(x, y) + ıv(x, y) as a function of z.
1. x3 −3xy2 −2xy + y
2. ex sinh y
436

3. ex (sin x cos y cosh y −cos x sin y sinh y)
Exercise 9.3
For an analytic function, f(z) = u(r, θ) + ıv(r, θ) prove that under suitable restrictions:
f(z) = 2u

z1/2, −ı
2 log z

+ const.
437

9.5
Hints
Hint 9.1
Show that u(x, y) + ıv(x, y) is functionally dependent on x + ıy so that you can write f(z) = f(x + ıy) = u(x, y) +
ıv(x, y).
Hint 9.2
Hint 9.3
Check out the derivation of Equation 9.2.
438

9.6
Solutions
Solution 9.1
u(x, y) + ıv(x, y) is functionally dependent on z = x + ıy if and only if
∂(u + ıv, x + ıy)
∂(x, y)
= 0.
∂(u + ıv, x + ıy)
∂(x, y)
=

ux + ıvx
uy + ıvy
1
ı

= −vx −uy + ı (ux −vy)
Since u and v satisfy the Cauchy-Riemann equations, this vanishes.
= 0
Thus we see that u(x, y) + ıv(x, y) is functionally dependent on x + ıy so we can write
f(z) = f(x + ıy) = u(x, y) + ıv(x, y).
Solution 9.2
1. Consider u(x, y) = x3 −3xy2 −2xy + y. The Laplacian of this function is
∆u ≡uxx + uyy
= 6x −6x
= 0
Since the function is harmonic, it is the real part of an analytic function. Clearly the analytic function is of the
form,
az3 + bz2 + cz + ıd,
439

with a, b and c complex-valued constants and d a real constant. Substituting z = x + ıy and expanding products
yields,
a
 x3 + ı3x2y −3xy2 −ıy3
+ b
 x2 + ı2xy −y2
+ c(x + ıy) + ıd.
By inspection, we see that the analytic function is
f(z) = z3 + ız2 −ız + ıd.
The harmonic conjugate of u is the imaginary part of f(z),
v(x, y) = 3x2y −y3 + x2 −y2 −x + d.
We can also do this problem with analytic continuation. The derivatives of u are
ux = 3x2 −3y2 −2y,
uy = −6xy −2x + 1.
The derivative of f(z) is
f ′(z) = ux −ıuy = 3x2 −2y2 −2y + ı(6xy −2x + 1).
On the real axis we have
f ′(z = x) = 3x2 −ı2x + ı.
Using analytic continuation, we see that
f ′(z) = 3z2 −ı2z + ı.
Integration yields
f(z) = z3 −ız2 + ız + const
440

2. Consider u(x, y) = ex sinh y. The Laplacian of this function is
∆u = ex sinh y + ex sinh y
= 2 ex sinh y.
Since the function is not harmonic, it is not the real part of an analytic function.
3. Consider u(x, y) = ex(sin x cos y cosh y −cos x sin y sinh y). The Laplacian of the function is
∆u = ∂
∂x (ex (sin x cos y cosh y −cos x sin y sinh y + cos x cos y cosh y + sin x sin y sinh y))
+ ∂
∂y (ex (−sin x sin y cosh y −cos x cos y sinh y + sin x cos y sinh y −cos x sin y cosh y))
= 2 ex (cos x cos y cosh y + sin x sin y sinh y) −2 ex (cos x cos y cosh y + sin x sin y sinh y)
= 0.
Thus u is the real part of an analytic function. The derivative of the analytic function is
f ′(z) = ux + ıvx = ux −ıuy
From the derivatives of u we computed before, we have
f(z) = (ex (sin x cos y cosh y −cos x sin y sinh y + cos x cos y cosh y + sin x sin y sinh y))
−ı (ex (−sin x sin y cosh y −cos x cos y sinh y + sin x cos y sinh y −cos x sin y cosh y))
Along the real axis, f ′(z) has the value,
f ′(z = x) = ex(sin x + cos x).
By analytic continuation, f ′(z) is
f ′(z) = ez(sin z + cos z)
441

We obtain f(z) by integrating.
f(z) = ez sin z + const.
u is the real part of the analytic function
f(z) = ez sin z + ıc,
where c is a real constant. We ﬁnd the harmonic conjugate of u by taking the imaginary part of f.
f(z) = ex(cosy + ı sin y)(sin x cosh y + ı cos x sinh y) + ıc
v(x, y) = ex sin x sin y cosh y + cos x cos y sinh y + c
Solution 9.3
We consider the analytic function: f(z) = u(r, θ) + ıv(r, θ). Recall that the complex derivative in terms of polar
coordinates is
d
dz = e−ıθ ∂
∂r = −ı
r e−ıθ ∂
∂θ.
The Cauchy-Riemann equations are
ur = 1
rvθ,
vr = −1
ruθ.
We diﬀerentiate f(z) and use the partial derivative in r for the right side.
f ′(z) = e−ıθ (ur + ıvr)
We use the Cauchy-Riemann equations to right f ′(z) in terms of the derivatives of u.
f ′(z) = e−ıθ

ur −ı1
ruθ

(9.6)
Now consider the function of a complex variable, g(ζ):
g(ζ) = e−ıζ

ur(r, ζ) −ı1
ruθ(r, ζ)

= eψ−ıξ

ur(r, ξ + ıψ) −ı1
ruθ(r, ξ + ıψ)

442

This function is analytic where f(ζ) is analytic. It is a simple calculus exercise to show that the complex derivative in
the ξ direction,
∂
∂ξ, and the complex derivative in the ψ direction, −ı ∂
∂ψ, are equal. Since these partial derivatives are
equal and continuous, g(ζ) is analytic. We evaluate the function g(ζ) at ζ = −ı log r. (Substitute θ = −ı log r into
Equation 9.6.)
f ′  r eı(−ı log r)
= e−ı(−ı log r)

ur(r, −ı log r) −ı1
ruθ(r, −ı log r)

rf ′  r2
= ur(r, −ı log r) −ı1
ruθ(r, −ı log r)
If the expression is non-singular, then it deﬁnes the analytic function, f ′(z), on a curve. The analytic continuation to
the complex plane is
zf ′  z2
= ur(z, −ı log z) −ı1
zuθ(z, −ı log z).
We integrate to obtain an expression for f (z2).
1
2f
 z2
= u(z, −ı log z) + const
We make a change of variables and solve for f(z).
f(z) = 2u

z1/2, −ı
2 log z

+ const.
Assuming that the above expression is non-singular, we have found a formula for writing the analytic function in terms
of its real part, u(r, θ). With the same method, we can ﬁnd how to write an analytic function in terms of its imaginary
part, v(r, θ).
443

Chapter 10
Contour Integration and the Cauchy-Goursat
Theorem
Between two evils, I always pick the one I never tried before.
- Mae West
10.1
Line Integrals
In this section we will recall the deﬁnition of a line integral in the Cartesian plane. In the next section we will use
this to deﬁne the contour integral in the complex plane.
Limit Sum Deﬁnition.
First we develop a limit sum deﬁnition of a line integral. Consider a curve C in the Cartesian
plane joining the points (a0, b0) and (a1, b1). We partition the curve into n segments with the points (x0, y0), . . . , (xn, yn)
where the ﬁrst and last points are at the endpoints of the curve. We deﬁne the diﬀerences, ∆xk = xk+1 −xk and
∆yk = yk+1 −yk, and let (ξk, ψk) be points on the curve between (xk, yk) and (xk+1, yk+1). This is shown pictorially
in Figure 10.1.
444

(x  ,y )
0
0
(ξ  ,η )
0
0
(x  ,y )
1
1
(ξ  ,η )
1
1
(x  ,y )
2
2
(ξ  ,η )
2
2
(ξ     ,η    )
n-1
n-1
(x  ,y )
n
n
(x     ,y    )
n-1
n-1
y
x
Figure 10.1: A curve in the Cartesian plane.
Consider the sum
n−1
X
k=0
(P(ξk, ψk)∆xk + Q(ξk, ψk)∆yk) ,
where P and Q are continuous functions on the curve. (P and Q may be complex-valued.) In the limit as each of the
∆xk and ∆yk approach zero the value of the sum, (if the limit exists), is denoted by
Z
C
P(x, y) dx + Q(x, y) dy.
This is a line integral along the curve C. The value of the line integral depends on the functions P(x, y) and Q(x, y),
the endpoints of the curve and the curve C. We can also write a line integral in vector notation.
Z
C
f(x) · dx
Here x = (x, y) and f(x) = (P(x, y), Q(x, y)).
445

Evaluating Line Integrals with Parameterization.
Let the curve C be parametrized by x = x(t), y = y(t)
for t0 ≤t ≤t1. Then the diﬀerentials on the curve are dx = x′(t) dt and dy = y′(t) dt. Using the parameterization we
can evaluate a line integral in terms of a deﬁnite integral.
Z
C
P(x, y) dx + Q(x, y) dy =
Z t1
t0
 P(x(t), y(t))x′(t) + Q(x(t), y(t))y′(t)

dt
Example 10.1.1 Consider the line integral
Z
C
x2 dx + (x + y) dy,
where C is the semi-circle from (1, 0) to (−1, 0) in the upper half plane. We parameterize the curve with x = cos t,
y = sin t for 0 ≤t ≤π.
Z
C
x2 dx + (x + y) dy =
Z π
0
 cos2 t(−sin t) + (cos t + sin t) cos t

dt
= π
2 −2
3
10.2
Contour Integrals
Limit Sum Deﬁnition.
We develop a limit sum deﬁnition for contour integrals. It will be analogous to the deﬁnition
for line integrals except that the notation is cleaner in complex variables. Consider a contour C in the complex plane
joining the points c0 and c1. We partition the contour into n segments with the points z0, . . . , zn where the ﬁrst and
last points are at the endpoints of the contour. We deﬁne the diﬀerences ∆zk = zk+1 −zk and let ζk be points on the
contour between zk and zk+1. Consider the sum
n−1
X
k=0
f(ζk)∆zk,
446

where f is a continuous function on the contour. In the limit as each of the ∆zk approach zero the value of the sum,
(if the limit exists), is denoted by
Z
C
f(z) dz.
This is a contour integral along C.
We can write a contour integral in terms of a line integral. Let f(z) = φ(x, y). (φ : R2 7→C.)
Z
C
f(z) dz =
Z
C
φ(x, y)(dx + ı dy)
Z
C
f(z) dz =
Z
C
(φ(x, y) dx + ıφ(x, y) dy)
(10.1)
Further, we can write a contour integral in terms of two real-valued line integrals. Let f(z) = u(x, y) + ıv(x, y).
Z
C
f(z) dz =
Z
C
(u(x, y) + ıv(x, y))(dx + ı dy)
Z
C
f(z) dz =
Z
C
(u(x, y) dx −v(x, y) dy) + ı
Z
C
(v(x, y) dx + u(x, y) dy)
(10.2)
Evaluation.
Let the contour C be parametrized by z = z(t) for t0 ≤t ≤t1. Then the diﬀerential on the contour
is dz = z′(t) dt. Using the parameterization we can evaluate a contour integral in terms of a deﬁnite integral.
Z
C
f(z) dz =
Z t1
t0
f(z(t))z′(t) dt
Example 10.2.1 Let C be the positively oriented unit circle about the origin in the complex plane. Evaluate:
1.
R
C z dz
2.
R
C
1
z dz
447

3.
R
C
1
z |dz|
In each case we parameterize the contour and then do the integral.
1.
z = eıθ,
dz = ı eıθ dθ
Z
C
z dz =
Z 2π
0
eıθ ı eıθ dθ
=
1
2 eı2θ
2π
0
=
1
2 eı4π −1
2 eı0

= 0
2.
Z
C
1
z dz =
Z 2π
0
1
eıθ ı eıθ dθ = ı
Z 2π
0
dθ = ı2π
3.
|dz| =
ı eıθ dθ
 =
ı eıθ |dθ| = |dθ|
Since dθ is positive in this case, |dθ| = dθ.
Z
C
1
z |dz| =
Z 2π
0
1
eıθ dθ =

ı e−ıθ2π
0 = 0
448

10.2.1
Maximum Modulus Integral Bound
The absolute value of a real integral obeys the inequality

Z b
a
f(x) dx
 ≤
Z b
a
|f(x)| |dx| ≤(b −a) max
a≤x≤b |f(x)|.
Now we prove the analogous result for the modulus of a contour integral.

Z
C
f(z) dz
 =
 lim
∆z→0
n−1
X
k=0
f(ζk)∆zk

≤lim
∆z→0
n−1
X
k=0
|f(ζk)| |∆zk|
=
Z
C
|f(z)| |dz|
≤
Z
C

max
z∈C |f(z)|

|dz|
=

max
z∈C |f(z)|
 Z
C
|dz|
=

max
z∈C |f(z)|

× (length of C)
Result 10.2.1 Maximum Modulus Integral Bound.

Z
C
f(z) dz
 ≤
Z
C
|f(z)| |dz| ≤

max
z∈C |f(z)|

(length of C)
449

10.3
The Cauchy-Goursat Theorem
Let f(z) be analytic in a compact, closed, connected domain D. We consider the integral of f(z) on the boundary of
the domain.
Z
∂D
f(z) dz =
Z
∂D
ψ(x, y)(dx + ı dy) =
Z
∂D
ψ dx + ıψ dy
Recall Green’s Theorem.
Z
∂D
P dx + Q dy =
Z
D
(Qx −Py) dx dy
If we assume that f ′(z) is continuous, we can apply Green’s Theorem to the integral of f(z) on ∂D.
Z
∂D
f(z) dz =
Z
∂D
ψ dx + ıψ dy =
Z
D
(ıψx −ψy) dx dy
Since f(z) is analytic, it satisﬁes the Cauchy-Riemann equation ψx = −ıψy.
The integrand in the area integral,
ıψx −ψy, is zero. Thus the contour integral vanishes.
Z
∂D
f(z) dz = 0
This is known as Cauchy’s Theorem. The assumption that f ′(z) is continuous is not necessary, but it makes the
proof much simpler because we can use Green’s Theorem. If we remove this restriction the result is known as the
Cauchy-Goursat Theorem. The proof of this result is omitted.
Result 10.3.1 The Cauchy-Goursat Theorem. If f(z) is analytic in a compact, closed,
connected domain D then the integral of f(z) on the boundary of the domain vanishes.
I
∂D
f(z) dz =
X
k
I
Ck
f(z) dz = 0
Here the set of contours {Ck} make up the positively oriented boundary ∂D of the domain
D.
450

As a special case of the Cauchy-Goursat theorem we can consider a simply-connected region. For this the boundary
is a Jordan curve. We can state the theorem in terms of this curve instead of referring to the boundary.
Result 10.3.2 The Cauchy-Goursat Theorem for Jordan Curves. If f(z) is analytic
inside and on a simple, closed contour C, then
I
C
f(z) dz = 0
Example 10.3.1 Let C be the unit circle about the origin with positive orientation. In Example 10.2.1 we calculated
that
Z
C
z dz = 0
Now we can evaluate the integral without parameterizing the curve. We simply note that the integrand is analytic
inside and on the circle, which is simple and closed. By the Cauchy-Goursat Theorem, the integral vanishes.
We cannot apply the Cauchy-Goursat theorem to evaluate
Z
C
1
z dz = ı2π
as the integrand is not analytic at z = 0.
Example 10.3.2 Consider the domain D = {z | |z| > 1}. The boundary of the domain is the unit circle with negative
orientation. f(z) = 1/z is analytic on D and its boundary. However
R
∂D f(z) dz does not vanish and we cannot apply
the Cauchy-Goursat Theorem. This is because the domain is not compact.
451

10.4
Contour Deformation
Path Independence.
Consider a function f(z) that is analytic on a simply connected domain a contour C in that
domain with end points a and b. The contour integral
R
C f(z) dz is independent of the path connecting the end points
and can be denoted
R b
a f(z) dz. This result is a direct consequence of the Cauchy-Goursat Theorem. Let C1 and C2
be two diﬀerent paths connecting the points. Let −C2 denote the second contour with the opposite orientation. Let
C be the contour which is the union of C1 and −C2. By the Cauchy-Goursat theorem, the integral along this contour
vanishes.
Z
C
f(z) dz =
Z
C1
f(z) dz +
Z
−C2
f(z) dz = 0
This implies that the integrals along C1 and C2 are equal.
Z
C1
f(z) dz =
Z
C2
f(z) dz
Thus contour integrals on simply connected domains are independent of path. This result does not hold for multiply
connected domains.
Result 10.4.1 Path Independence. Let f(z) be analytic on a simply connected domain.
For points a and b in the domain, the contour integral,
Z b
a
f(z) dz
is independent of the path connecting the points.
Deforming Contours.
Consider two simple, closed, positively oriented contours, C1 and C2. Let C2 lie completely
within C1. If f(z) is analytic on and between C1 and C2 then the integrals of f(z) along C1 and C2 are equal.
Z
C1
f(z) dz =
Z
C2
f(z) dz
452

Again, this is a direct consequence of the Cauchy-Goursat Theorem. Let D be the domain on and between C1 and C2.
By the Cauchy-Goursat Theorem the integral along the boundary of D vanishes.
Z
C1
f(z) dz +
Z
−C2
f(z) dz = 0
Z
C1
f(z) dz =
Z
C2
f(z) dz
By following this line of reasoning, we see that we can deform a contour C without changing the value of
R
C f(z) dz
as long as we stay on the domain where f(z) is analytic.
Result 10.4.2 Contour Deformation. Let f(z) be analytic on a domain D. If a set of
closed contours {Cm} can be continuously deformed on the domain D to a set of contours
{Γn} then the integrals along {Cm} and {Γn} are equal.
Z
{Cm}
f(z) dz =
Z
{Γn}
f(z) dz
10.5
Morera’s Theorem.
The converse of the Cauchy-Goursat theorem is Morera’s Theorem. If the integrals of a continuous function f(z)
vanish along all possible simple, closed contours in a domain, then f(z) is analytic on that domain. To prove Morera’s
Theorem we will assume that ﬁrst partial derivatives of f(z) = u(x, y) + ıv(x, y) are continuous, although the result
can be derived without this restriction. Let the simple, closed contour C be the boundary of D which is contained in
453

the domain Ω.
I
C
f(z) dz =
I
C
(u + ıv)(dx + ı dy)
=
I
C
u dx −v dy + ı
I
C
v dx + u dy
=
Z
D
(−vx −uy) dx dy + ı
Z
D
(ux −vy) dx dy
= 0
Since the two integrands are continuous and vanish for all C in Ω, we conclude that the integrands are identically zero.
This implies that the Cauchy-Riemann equations,
ux = vy,
uy = −vx,
are satisﬁed. f(z) is analytic in Ω.
The converse of the Cauchy-Goursat theorem is Morera’s Theorem. If the integrals of a continuous function f(z)
vanish along all possible simple, closed contours in a domain, then f(z) is analytic on that domain. To prove Morera’s
Theorem we will assume that ﬁrst partial derivatives of f(z) = φ(x, y) are continuous, although the result can be
derived without this restriction. Let the simple, closed contour C be the boundary of D which is contained in the
domain Ω.
I
C
f(z) dz =
I
C
(φ dx + ıφ dy)
=
Z
D
(ıφx −φy) dx dy
= 0
Since the integrand, ıφx −φy is continuous and vanishes for all C in Ω, we conclude that the integrand is identically
zero. This implies that the Cauchy-Riemann equation,
φx = −ıφy,
454

is satisﬁed. We conclude that f(z) is analytic in Ω.
Result 10.5.1 Morera’s Theorem. If f(z) is continuous in a simply connected domain Ω
and
I
C
f(z) dz = 0
for all possible simple, closed contours C in the domain, then f(z) is analytic in Ω.
10.6
Indeﬁnite Integrals
Consider a function f(z) which is analytic in a domain D. An anti-derivative or indeﬁnite integral (or simply integral)
is a function F(z) which satisﬁes F ′(z) = f(z). This integral exists and is unique up to an additive constant. Note
that if the domain is not connected, then the additive constants in each connected component are independent. The
indeﬁnite integrals are denoted:
Z
f(z) dz = F(z) + c.
We will prove existence later by writing an indeﬁnite integral as a contour integral. We brieﬂy consider uniqueness
of the indeﬁnite integral here. Let F(z) and G(z) be integrals of f(z). Then F ′(z) −G′(z) = f(z) −f(z) = 0.
Although we do not prove it, it certainly makes sense that F(z) −G(z) is a constant on each connected component
of the domain. Indeﬁnite integrals are unique up to an additive constant.
Integrals of analytic functions have all the nice properties of integrals of functions of a real variable. All the formulas
from integral tables, including things like integration by parts, carry over directly.
455

10.7
Fundamental Theorem of Calculus via Primitives
10.7.1
Line Integrals and Primitives
Here we review some concepts from vector calculus.
Analagous to an integral in functions of a single variable is
a primitive in functions of several variables. Consider a function f(x). F(x) is an integral of f(x) if and only if
dF = f dx. Now we move to functions of x and y. Let P(x, y) and Q(x, y) be deﬁned on a simply connected domain.
A primitive Φ satisﬁes
dΦ = P dx + Q dy.
A necessary and suﬃcient condition for the existence of a primitive is that Py = Qx. The deﬁnite integral can be
evaluated in terms of the primitive.
Z (c,d)
(a,b)
P dx + Q dy = Φ(c, d) −Φ(a, b)
10.7.2
Contour Integrals
Now consider integral along the contour C of the function f(z) = φ(x, y).
Z
C
f(z) dz =
Z
C
(φ dx + ıφ dy)
A primitive Φ of φ dx + ıφ dy exists if and only if φy = ıφx. We recognize this as the Cauch-Riemann equation,
φx = −ıφy. Thus a primitive exists if and only if f(z) is analytic. If so, then
dΦ = φ dx + ıφ dy.
How do we ﬁnd the primitive Φ that satisﬁes Φx = φ and Φy = ıφ? Note that choosing Ψ(x, y) = F(z) where F(z)
is an anti-derivative of f(z), F ′(z) = f(z), does the trick. We express the complex derivative as partial derivatives in
the coordinate directions to show this.
F ′(z) = f(z) = ψ(x, y),
F ′(z) = Φx = −ıΦy
456

From this we see that Φx = φ and Φy = ıφ so Φ(x, y) = F(z) is a primitive. Since we can evaluate the line integral
of (φ dx + ıφ dy),
Z (c,d)
(a,b)
(φ dx + ıφ dy) = Φ(c, d) −Φ(a, b),
We can evaluate a deﬁnite integral of f in terms of its indeﬁnite integral, F.
Z b
a
f(z) dz = F(b) −F(a)
This is the Fundamental Theorem of Calculus for functions of a complex variable.
10.8
Fundamental Theorem of Calculus via Complex Calculus
Result 10.8.1 Constructing an Indeﬁnite Integral. If f(z) is analytic in a simply con-
nected domain D and a is a point in the domain, then
F(z) =
Z z
a
f(ζ) dζ
is analytic in D and is an indeﬁnite integral of f(z), (F ′(z) = f(z)).
Now we consider anti-derivatives and deﬁnite integrals without using vector calculus. From real variables we know
that we can construct an integral of f(x) with a deﬁnite integral.
F(x) =
Z x
a
f(ξ) dξ
Now we will prove the analogous property for functions of a complex variable.
F(z) =
Z z
a
f(ζ) dζ
457

Let f(z) be analytic in a simply connected domain D and let a be a point in the domain. To show that F(z) =
R z
a f(ζ) dζ
is an integral of f(z), we apply the limit deﬁnition of diﬀerentiation.
F ′(z) = lim
∆z→0
F(z + ∆z) −F(z)
∆z
= lim
∆z→0
1
∆z
Z z+∆z
a
f(ζ) dζ −
Z z
a
f(ζ) dζ

= lim
∆z→0
1
∆z
Z z+∆z
z
f(ζ) dζ
The integral is independent of path. We choose a straight line connecting z and z + ∆z. We add and subtract
∆zf(z) =
R z+∆z
z
f(z) dζ from the expression for F ′(z).
F ′(z) = lim
∆z→0
1
∆z

∆zf(z) +
Z z+∆z
z
(f(ζ) −f(z)) dζ

= f(z) + lim
∆z→0
1
∆z
Z z+∆z
z
(f(ζ) −f(z)) dζ
Since f(z) is analytic, it is certainly continuous. This means that
lim
ζ→z f(ζ) = 0.
The limit term vanishes as a result of this continuity.
lim
∆z→0

1
∆z
Z z+∆z
z
(f(ζ) −f(z)) dζ
 ≤lim
∆z→0
1
|∆z||∆z|
max
ζ∈[z...z+∆z] |f(ζ) −f(z)|
= lim
∆z→0
max
ζ∈[z...z+∆z] |f(ζ) −f(z)|
= 0
Thus F ′(z) = f(z).
458

This results demonstrates the existence of the indeﬁnite integral. We will use this to prove the Fundamental Theorem
of Calculus for functions of a complex variable.
Result 10.8.2 Fundamental Theorem of Calculus. If f(z) is analytic in a simply con-
nected domain D then
Z b
a
f(z) dz = F(b) −F(a)
where F(z) is any indeﬁnite integral of f(z).
From Result 10.8.1 we know that
Z b
a
f(z) dz = F(b) + c.
(Here we are considering b to be a variable.) The case b = a determines the constant.
Z a
a
f(z) dz = F(a) + c = 0
c = −F(a)
This proves the Fundamental Theorem of Calculus for functions of a complex variable.
Example 10.8.1 Consider the integral
Z
C
1
z −a dz
where C is any closed contour that goes around the point z = a once in the positive direction. We use the Fundamental
Theorem of Calculus to evaluate the integral. We start at a point on the contour z −a = r eıθ. When we traverse the
contour once in the positive direction we end at the point z −a = r eı(θ+2π).
Z
C
1
z −a dz = [log(z −a)]z−a=r eı(θ+2π)
z−a=r eıθ
= Log r + ı(θ + 2π) −(Log r + ıθ)
= ı2π
459

10.9
Exercises
Exercise 10.1
C is the arc corresponding to the unit semi-circle, |z| = 1, ℑ(z) ≥0, directed from z = −1 to z = 1. Evaluate
1.
Z
C
z2 dz
2.
Z
C
z2 dz
3.
Z
C
z2 |dz|
4.
Z
C
z2 |dz|
Hint, Solution
Exercise 10.2
Evaluate
Z ∞
−∞
e−(ax2+bx) dx,
where a, b ∈C and ℜ(a) > 0. Use the fact that
Z ∞
−∞
e−x2 dx = √π.
Hint, Solution
Exercise 10.3
Evaluate
2
Z ∞
0
e−ax2 cos(ωx) dx,
and
2
Z ∞
0
x e−ax2 sin(ωx)dx,
460

where ℜ(a) > 0 and ω ∈R.
Hint, Solution
Exercise 10.4
Use an admissible parameterization to evaluate
Z
C
(z −z0)n dz,
n ∈Z
for the following cases:
1. C is the circle |z −z0| = 1 traversed in the counterclockwise direction.
2. C is the circle |z −z0 −ı2| = 1 traversed in the counterclockwise direction.
3. z0 = 0, n = −1 and C is the closed contour deﬁned by the polar equation
r = 2 −sin2
θ
4

Is this result compatible with the results of part (a)?
Hint, Solution
Exercise 10.5
1. Use bounding arguments to show that
lim
R→∞
Z
CR
z + Log z
z3 + 1
dz = 0
where CR is the positive closed contour |z| = R.
2. Place a bound on

Z
C
Log z dz

where C is the arc of the circle |z| = 2 from −ı2 to ı2.
461

3. Deduce that

Z
C
z2 −1
z2 + 1 dz
 ≤πrR2 + 1
R2 −1
where C is a semicircle of radius R > 1 centered at the origin.
Hint, Solution
Exercise 10.6
Let C denote the entire positively oriented boundary of the half disk 0 ≤r ≤1, 0 ≤θ ≤π in the upper half plane.
Consider the branch
f(z) = √r eıθ/2,
−π
2 < θ < 3π
2
of the multi-valued function z1/2. Show by separate parametric evaluation of the semi-circle and the two radii constituting
the boundary that
Z
C
f(z) dz = 0.
Does the Cauchy-Goursat theorem apply here?
Hint, Solution
Exercise 10.7
Evaluate the following contour integrals using anti-derivatives and justify your approach for each.
1.
Z
C
 ız3 + z−3
dz,
where C is the line segment from z1 = 1 + ı to z2 = ı.
2.
Z
C
sin2 z cos z dz
where C is a right-handed spiral from z1 = π to z2 = ıπ.
462

3.
Z
C
zı dz = 1 + e−π
2
(1 −ı)
with
zı = eı Log z,
−π < Arg z < π.
C joins z1 = −1 and z2 = 1, lying above the real axis except at the end points. (Hint: redeﬁne zı so that it
remains unchanged above the real axis and is deﬁned continuously on the real axis.)
Hint, Solution
463

10.10
Hints
Hint 10.1
Hint 10.2
Let C be the parallelogram in the complex plane with corners at ±R and ±R + b/(2a). Consider the integral of e−az2
on this contour. Take the limit as R →∞.
Hint 10.3
Extend the range of integration to (−∞. . . ∞). Use eıωx = cos(ωx) + ı sin(ωx) and the result of Exercise 10.2.
Hint 10.4
Hint 10.5
Hint 10.6
Hint 10.7
464

10.11
Solutions
Solution 10.1
We parameterize the path with z = eıθ, with θ ranging from π to 0.
dz = ı eıθ dθ
|dz| = |ı eıθ dθ| = |dθ| = −dθ
1.
Z
C
z2 dz =
Z 0
π
eı2θ ı eıθ dθ
=
Z 0
π
ı eı3θ dθ
=
1
3 eı3θ
0
π
= 1
3
 eı0 −eı3π
= 1
3(1 −(−1))
= 2
3
465

2.
Z
C
|z2| dz =
Z 0
π
| eı2θ |ı eıθ dθ
=
Z 0
π
ı eıθ dθ
=
eıθ0
π
= 1 −(−1)
= 2
3.
Z
C
z2 |dz| =
Z 0
π
eı2θ |ı eıθ dθ|
=
Z 0
π
−eı2θ dθ
=
h ı
2 eı2θi0
π
= ı
2(1 −1)
= 0
4.
Z
C
|z2| |dz| =
Z 0
π
| eı2θ ||ı eıθ dθ|
=
Z 0
π
−dθ
= [−θ]0
π
= π
466

Solution 10.2
I =
Z ∞
−∞
e−(ax2+bx) dx
First we complete the square in the argument of the exponential.
I = eb2/(4a)
Z ∞
−∞
e−a(x+b/(2a))2 dx
Consider the parallelogram in the complex plane with corners at ±R and ±R + b/(2a). The integral of e−az2 on this
contour vanishes as it is an entire function. We relate the integral along one side of the parallelogram to the integrals
along the other three sides.
Z R+b/(2a)
−R+b/(2a)
e−az2 dz =
 Z −R
−R+b/(2a)
+
Z R
−R
+
Z R+b/(2a)
R
!
e−az2 dz.
The ﬁrst and third integrals on the right side vanish as R →∞because the integrand vanishes and the lengths of the
paths of integration are ﬁnite. Taking the limit as R →∞we have,
Z ∞+b/(2a)
−∞+b/(2a)
e−az2 dz ≡
Z ∞
−∞
e−a(x+b/(2a))2 dx =
Z ∞
−∞
e−ax2 dx.
Now we have
I = eb2/(4a)
Z ∞
−∞
e−ax2 dx.
We make the change of variables ξ = √ax.
I = eb2/(4a) 1
√a
Z ∞
−∞
e−ξ2 dξ
Z ∞
−∞
e−(ax2+bx) dx =
rπ
a eb2/(4a)
467

Solution 10.3
Consider
I = 2
Z ∞
0
e−ax2 cos(ωx) dx.
Since the integrand is an even function,
I =
Z ∞
−∞
e−ax2 cos(ωx) dx.
Since e−ax2 sin(ωx) is an odd function,
I =
Z ∞
−∞
e−ax2 eıωx dx.
We evaluate this integral with the result of Exercise 10.2.
2
Z ∞
0
e−ax2 cos(ωx) dx =
rπ
a e−ω2/(4a)
Consider
I = 2
Z ∞
0
x e−ax2 sin(ωx) dx.
Since the integrand is an even function,
I =
Z ∞
−∞
x e−ax2 sin(ωx) dx.
Since x e−ax2 cos(ωx) is an odd function,
I = −ı
Z ∞
−∞
x e−ax2 eıωx dx.
We add a dash of integration by parts to get rid of the x factor.
I = −ı

−1
2a e−ax2 eıωx
∞
−∞
+ ı
Z ∞
−∞

−1
2a e−ax2 ıω eıωx

dx
I = ω
2a
Z ∞
−∞
e−ax2 eıωx dx
468

2
Z ∞
0
x e−ax2 sin(ωx) dx = ω
2a
rπ
a e−ω2/(4a)
Solution 10.4
1. We parameterize the contour and do the integration.
z −z0 = eıθ,
θ ∈[0 . . . 2π)
Z
C
(z −z0)n dz =
Z 2π
0
eınθ ı eıθ dθ
=



h
eı(n+1)θ
n+1
i2π
0
for n ̸= −1
[ıθ]2π
0
for n = −1
=
(
0
for n ̸= −1
ı2π
for n = −1
2. We parameterize the contour and do the integration.
z −z0 = ı2 + eıθ,
θ ∈[0 . . . 2π)
Z
C
(z −z0)n dz =
Z 2π
0
 ı2 + eıθn ı eıθ dθ
=






(ı2+eıθ)
n+1
n+1
2π
0
for n ̸= −1

log
 ı2 + eıθ2π
0
for n = −1
= 0
3. We parameterize the contour and do the integration.
z = r eıθ,
r = 2 −sin2
θ
4

,
θ ∈[0 . . . 4π)
469

-1
1
-1
1
Figure 10.2: The contour: r = 2 −sin2   θ
4

.
The contour encircles the origin twice. See Figure 10.2.
Z
C
z−1 dz =
Z 4π
0
1
r(θ) eıθ (r′(θ) + ır(θ)) eıθ dθ
=
Z 4π
0
r′(θ)
r(θ) + ı

dθ
= [log(r(θ)) + ıθ]4π
0
Since r(θ) does not vanish, the argument of r(θ) does not change in traversing the contour and thus the
logarithmic term has the same value at the beginning and end of the path.
Z
C
z−1 dz = ı4π
This answer is twice what we found in part (a) because the contour goes around the origin twice.
470

Solution 10.5
1. We parameterize the contour with z = R eıθ and bound the modulus of the integral.

Z
CR
z + Log z
z3 + 1
dz
 ≤
Z
CR

z + Log z
z3 + 1
 |dz|
≤
Z 2π
0
R + ln R + π
R3 −1
R dθ
= 2πrR + ln R + π
R3 −1
The upper bound on the modulus on the integral vanishes as R →∞.
lim
R→∞2πrR + ln R + π
R3 −1
= 0
We conclude that the integral vanishes as R →∞.
lim
R→∞
Z
CR
z + Log z
z3 + 1
dz = 0
2. We parameterize the contour and bound the modulus of the integral.
z = 2 eıθ,
θ ∈[−π/2 . . . π/2]
471


Z
C
Log z dz
 ≤
Z
C
|Log z| |dz|
=
Z π/2
−π/2
| ln 2 + ıθ|2 dθ
≤2
Z π/2
−π/2
(ln 2 + |θ|) dθ
= 4
Z π/2
0
(ln 2 + θ) dθ
= π
2 (π + 4 ln 2)
3. We parameterize the contour and bound the modulus of the integral.
z = R eıθ,
θ ∈[θ0 . . . θ0 + π]

Z
C
z2 −1
z2 + 1 dz
 ≤
Z
C

z2 −1
z2 + 1
 |dz|
≤
Z θ0+π
θ0

R2 eı2θ −1
R2 eı2θ +1
 |R dθ|
≤R
Z θ0+π
θ0
R2 + 1
R2 −1 dθ
= πrR2 + 1
R2 −1
472

Solution 10.6
Z
C
f(z) dz =
Z 1
0
√r dr +
Z π
0
eıθ/2 ı eıθ dθ +
Z 0
1
ı√r (−dr)
= 2
3 +

−2
3 −ı2
3

+ ı2
3
= 0
The Cauchy-Goursat theorem does not apply because the function is not analytic at z = 0, a point on the boundary.
Solution 10.7
1.
Z
C
 ız3 + z−3
dz =
ız4
4 −1
2z2
ı
1+ı
= 1
2 + ı
In this example, the anti-derivative is single-valued.
2.
Z
C
sin2 z cos z dz =
sin3 z
3
ıπ
π
= 1
3
 sin3(ıπ) −sin3(π)

= −ısinh3(π)
3
Again the anti-derivative is single-valued.
473

3. We choose the branch of zı with −π/2 < arg(z) < 3π/2. This matches the principal value of zı above the real
axis and is deﬁned continuously on the path of integration.
Z
C
zı dz =
 z1+ı
1 + ı
eı0
eıπ
=
1 −ı
2
e(1+ı) log z
eı0
eıπ
= 1 −ı
2
 e0 −e(1+ı)ıπ
= 1 + e−π
2
(1 −ı)
474

Chapter 11
Cauchy’s Integral Formula
If I were founding a university I would begin with a smoking room; next a dormitory; and then a decent reading room
and a library. After that, if I still had more money that I couldn’t use, I would hire a professor and get some text books.
- Stephen Leacock
475

11.1
Cauchy’s Integral Formula
Result 11.1.1 Cauchy’s Integral Formula. If f(ζ) is analytic in a compact, closed, con-
nected domain D and z is a point in the interior of D then
f(z) =
1
ı2π
I
∂D
f(ζ)
ζ −z dζ =
1
ı2π
X
k
I
Ck
f(ζ)
ζ −z dζ.
(11.1)
Here the set of contours {Ck} make up the positively oriented boundary ∂D of the domain
D. More generally, we have
f (n)(z) = n!
ı2π
I
∂D
f(ζ)
(ζ −z)n+1 dζ = n!
ı2π
X
k
I
Ck
f(ζ)
(ζ −z)n+1 dζ.
(11.2)
Cauchy’s Formula shows that the value of f(z) and all its derivatives in a domain are determined by the value of
f(z) on the boundary of the domain. Consider the ﬁrst formula of the result, Equation 11.1. We deform the contour
to a circle of radius δ about the point ζ = z.
I
C
f(ζ)
ζ −z dζ =
I
Cδ
f(ζ)
ζ −z dζ
=
I
Cδ
f(z)
ζ −z dζ +
I
Cδ
f(ζ) −f(z)
ζ −z
dζ
We use the result of Example 10.8.1 to evaluate the ﬁrst integral.
I
C
f(ζ)
ζ −z dζ = ı2πf(z) +
I
Cδ
f(ζ) −f(z)
ζ −z
dζ
476

The remaining integral along Cδ vanishes as δ →0 because f(ζ) is continuous. We demonstrate this with the maximum
modulus integral bound. The length of the path of integration is 2πδ.
lim
δ→0

I
Cδ
f(ζ) −f(z)
ζ −z
dζ
 ≤lim
δ→0

(2πδ)1
δ max
|ζ−z|=δ |f(ζ) −f(z)|

≤lim
δ→0

2π max
|ζ−z|=δ |f(ζ) −f(z)|

= 0
This gives us the desired result.
f(z) =
1
ı2π
I
C
f(ζ)
ζ −z dζ
We derive the second formula, Equation
11.2, from the ﬁrst by diﬀerentiating with respect to z. Note that the
integral converges uniformly for z in any closed subset of the interior of C. Thus we can diﬀerentiate with respect to
z and interchange the order of diﬀerentiation and integration.
f (n)(z) =
1
ı2π
dn
dzn
I
C
f(ζ)
ζ −z dζ
=
1
ı2π
I
C
dn
dzn
f(ζ)
ζ −z dζ
= n!
ı2π
I
C
f(ζ)
(ζ −z)n+1 dζ
Example 11.1.1 Consider the following integrals where C is the positive contour on the unit circle. For the third
integral, the point z = −1 is removed from the contour.
1.
I
C
sin
 cos
 z5
dz
477

2.
I
C
1
(z −3)(3z −1) dz
3.
Z
C
√z dz
1. Since sin (cos (z5)) is an analytic function inside the unit circle,
I
C
sin
 cos
 z5
dz = 0
2.
1
(z−3)(3z−1) has singularities at z = 3 and z = 1/3. Since z = 3 is outside the contour, only the singularity at
z = 1/3 will contribute to the value of the integral. We will evaluate this integral using the Cauchy integral
formula.
I
C
1
(z −3)(3z −1) dz = ı2π

1
(1/3 −3)3

= −ıπ
4
3. Since the curve is not closed, we cannot apply the Cauchy integral formula. Note that √z is single-valued and
analytic in the complex plane with a branch cut on the negative real axis. Thus we use the Fundamental Theorem
of Calculus.
Z
C
√z dz =
2
3
√
z3
eıπ
e−ıπ
= 2
3
 eı3π/2 −e−ı3π/2
= 2
3(−ı −ı)
= −ı4
3
478

Cauchy’s Inequality.
Suppose the f(ζ) is analytic in the closed disk |ζ −z| ≤r. By Cauchy’s integral formula,
f (n)(z) = n!
ı2π
I
C
f(ζ)
(ζ −z)n+1 dζ,
where C is the circle of radius r centered about the point z. We use this to obtain an upper bound on the modulus of
f (n)(z).
f (n)(z)
 = n!
2π

I
C
f(ζ)
(ζ −z)n+1 dζ

≤n!
2π2πr max
|ζ−z|=r

f(ζ)
(ζ −z)n+1

= n!
rn max
|ζ−z|=r |f(ζ)|
Result 11.1.2 Cauchy’s Inequality. If f(ζ) is analytic in |ζ −z| ≤r then
f (n)(z)
 ≤n!M
rn
where |f(ζ)| ≤M for all |ζ −z| = r.
Liouville’s Theorem.
Consider a function f(z) that is analytic and bounded, (|f(z)| ≤M), in the complex plane.
From Cauchy’s inequality,
|f ′(z)| ≤M
r
for any positive r. By taking r →∞, we see that f ′(z) is identically zero for all z. Thus f(z) is a constant.
Result 11.1.3 Liouville’s Theorem. If f(z) is analytic and |f(z)| is bounded in the complex
plane then f(z) is a constant.
479

The Fundamental Theorem of Algebra.
We will prove that every polynomial of degree n ≥1 has exactly n
roots, counting multiplicities. First we demonstrate that each such polynomial has at least one root. Suppose that an
nth degree polynomial p(z) has no roots. Let the lower bound on the modulus of p(z) be 0 < m ≤|p(z)|. The function
f(z) = 1/p(z) is analytic, (f ′(z) = p′(z)/p2(z)), and bounded, (|f(z)| ≤1/m), in the extended complex plane. Using
Liouville’s theorem we conclude that f(z) and hence p(z) are constants, which yields a contradiction. Therefore every
such polynomial p(z) must have at least one root.
Now we show that we can factor the root out of the polynomial. Let
p(z) =
n
X
k=0
pkzk.
We note that
(zn −cn) = (z −c)
n−1
X
k=0
cn−1−kzk.
Suppose that the nth degree polynomial p(z) has a root at z = c.
p(z) = p(z) −p(c)
=
n
X
k=0
pkzk −
n
X
k=0
pkck
=
n
X
k=0
pk
 zk −ck
=
n
X
k=0
pk(z −c)
k−1
X
j=0
ck−1−jzj
= (z −c)q(z)
Here q(z) is a polynomial of degree n −1. By induction, we see that p(z) has exactly n roots.
480

Result 11.1.4 Fundamental Theorem of Algebra. Every polynomial of degree n ≥1 has
exactly n roots, counting multiplicities.
Gauss’ Mean Value Theorem.
Let f(ζ) be analytic in |ζ −z| ≤r. By Cauchy’s integral formula,
f(z) =
1
ı2π
I
C
f(ζ)
ζ −z dζ,
where C is the circle |ζ −z| = r. We parameterize the contour with ζ = z + r eıθ.
f(z) =
1
ı2π
Z 2π
0
f(z + r eıθ)
r eıθ
ır eıθ dθ
Writing this in the form,
f(z) =
1
2πr
Z 2π
0
f(z + r eıθ)r dθ,
we see that f(z) is the average value of f(ζ) on the circle of radius r about the point z.
Result 11.1.5 Gauss’ Average Value Theorem. If f(ζ) is analytic in |ζ −z| ≤r then
f(z) = 1
2π
Z 2π
0
f(z + r eıθ) dθ.
That is, f(z) is equal to its average value on a circle of radius r about the point z.
Extremum Modulus Theorem.
Let f(z) be analytic in closed, connected domain, D. The extreme values of the
modulus of the function must occur on the boundary. If |f(z)| has an interior extrema, then the function is a constant.
We will show this with proof by contradiction. Assume that |f(z)| has an interior maxima at the point z = c. This
481

means that there exists an neighborhood of the point z = c for which |f(z)| ≤|f(c)|. Choose an ϵ so that the set
|z −c| ≤ϵ lies inside this neighborhood. First we use Gauss’ mean value theorem.
f(c) = 1
2π
Z 2π
0
f
 c + ϵ eıθ
dθ
We get an upper bound on |f(c)| with the maximum modulus integral bound.
|f(c)| ≤1
2π
Z 2π
0
f
 c + ϵ eıθ dθ
Since z = c is a maxima of |f(z)| we can get a lower bound on |f(c)|.
|f(c)| ≥1
2π
Z 2π
0
f
 c + ϵ eıθ dθ
If |f(z)| < |f(c)| for any point on |z−c| = ϵ, then the continuity of f(z) implies that |f(z)| < |f(c)| in a neighborhood
of that point which would make the value of the integral of |f(z)| strictly less than |f(c)|. Thus we conclude that
|f(z)| = |f(c)| for all |z −c| = ϵ. Since we can repeat the above procedure for any circle of radius smaller than ϵ,
|f(z)| = |f(c)| for all |z −c| ≤ϵ, i.e. all the points in the disk of radius ϵ about z = c are also maxima. By recursively
repeating this procedure points in this disk, we see that |f(z)| = |f(c)| for all z ∈D. This implies that f(z) is a
constant in the domain. By reversing the inequalities in the above method we see that the minimum modulus of f(z)
must also occur on the boundary.
Result 11.1.6 Extremum Modulus Theorem. Let f(z) be analytic in a closed, connected
domain, D. The extreme values of the modulus of the function must occur on the boundary.
If |f(z)| has an interior extrema, then the function is a constant.
482

11.2
The Argument Theorem
Result 11.2.1 The Argument Theorem. Let f(z) be analytic inside and on C except for
isolated poles inside the contour. Let f(z) be nonzero on C.
1
ı2π
Z
C
f ′(z)
f(z) dz = N −P
Here N is the number of zeros and P the number of poles, counting multiplicities, of f(z)
inside C.
First we will simplify the problem and consider a function f(z) that has one zero or one pole. Let f(z) be analytic
and nonzero inside and on A except for a zero of order n at z = a. Then we can write f(z) = (z −a)ng(z) where g(z)
is analytic and nonzero inside and on A. The integral of f′(z)
f(z) along A is
1
ı2π
Z
A
f ′(z)
f(z) dz =
1
ı2π
Z
A
d
dz (log(f(z))) dz
=
1
ı2π
Z
A
d
dz (log((z −a)n) + log(g(z))) dz
=
1
ı2π
Z
A
d
dz (log((z −a)n)) dz
=
1
ı2π
Z
A
n
z −a dz
= n
483

Now let f(z) be analytic and nonzero inside and on B except for a pole of order p at z = b. Then we can write
f(z) =
g(z)
(z−b)p where g(z) is analytic and nonzero inside and on B. The integral of f′(z)
f(z) along B is
1
ı2π
Z
B
f ′(z)
f(z) dz =
1
ı2π
Z
B
d
dz (log(f(z))) dz
=
1
ı2π
Z
B
d
dz
 log((z −b)−p) + log(g(z))

dz
=
1
ı2π
Z
B
d
dz
 log((z −b)−p)+

dz
=
1
ı2π
Z
B
−p
z −b dz
= −p
Now consider a function f(z) that is analytic inside an on the contour C except for isolated poles at the points
b1, . . . , bp. Let f(z) be nonzero except at the isolated points a1, . . . , an. Let the contours Ak, k = 1, . . . , n, be simple,
positive contours which contain the zero at ak but no other poles or zeros of f(z). Likewise, let the contours Bk,
k = 1, . . . , p be simple, positive contours which contain the pole at bk but no other poles of zeros of f(z). (See
Figure 11.1.) By deforming the contour we obtain
Z
C
f ′(z)
f(z) dz =
n
X
j=1
Z
Aj
f ′(z)
f(z) dz +
p
X
k=1
Z
Bj
f ′(z)
f(z) dz.
From this we obtain Result 11.2.1.
11.3
Rouche’s Theorem
Result 11.3.1 Rouche’s Theorem. Let f(z) and g(z) be analytic inside and on a simple,
closed contour C. If |f(z)| > |g(z)| on C then f(z) and f(z) + g(z) have the same number
of zeros inside C and no zeros on C.
484

C
A1
B1
B3
B2
A2
Figure 11.1: Deforming the contour C.
First note that since |f(z)| > |g(z)| on C, f(z) is nonzero on C. The inequality implies that |f(z) + g(z)| > 0
on C so f(z) + g(z) has no zeros on C. We well count the number of zeros of f(z) and g(z) using the Argument
Theorem, (Result 11.2.1). The number of zeros N of f(z) inside the contour is
N =
1
ı2π
I
C
f ′(z)
f(z) dz.
Now consider the number of zeros M of f(z) + g(z). We introduce the function h(z) = g(z)/f(z).
M =
1
ı2π
I
C
f ′(z) + g′(z)
f(z) + g(z) dz
=
1
ı2π
I
C
f ′(z) + f ′(z)h(z) + f(z)h′(z)
f(z) + f(z)h(z)
dz
=
1
ı2π
I
C
f ′(z)
f(z) dz + 1
ı2π
I
C
h′(z)
1 + h(z) dz
= N + 1
ı2π [log(1 + h(z))]C
= N
485

(Note that since |h(z)| < 1 on C, ℜ(1 + h(z)) > 0 on C and the value of log(1 + h(z)) does not not change in
traversing the contour.) This demonstrates that f(z) and f(z) + g(z) have the same number of zeros inside C and
proves the result.
486

11.4
Exercises
Exercise 11.1
What is
(arg(sin z))

C
where C is the unit circle?
Exercise 11.2
Let C be the circle of radius 2 centered about the origin and oriented in the positive direction. Evaluate the following
integrals:
1.
H
C
sin z
z2+5 dz
2.
H
C
z
z2+1 dz
3.
H
C
z2+1
z
dz
Exercise 11.3
Let f(z) be analytic and bounded (i.e. |f(z)| < M) for |z| > R, but not necessarily analytic for |z| ≤R. Let the
points α and β lie inside the circle |z| = R. Evaluate
I
C
f(z)
(z −α)(z −β) dz
where C is any closed contour outside |z| = R, containing the circle |z| = R. [Hint: consider the circle at inﬁnity] Now
suppose that in addition f(z) is analytic everywhere. Deduce that f(α) = f(β).
Exercise 11.4
Using Rouche’s theorem show that all the roots of the equation p(z) = z6 −5z2 +10 = 0 lie in the annulus 1 < |z| < 2.
Exercise 11.5
Evaluate as a function of t
ω =
1
ı2π
I
C
ezt
z2(z2 + a2) dz,
487

where C is any positively oriented contour surrounding the circle |z| = a.
Exercise 11.6
Consider C1, (the positively oriented circle |z| = 4), and C2, (the positively oriented boundary of the square whose
sides lie along the lines x = ±1, y = ±1). Explain why
Z
C1
f(z) dz =
Z
C2
f(z) dz
for the functions
1. f(z) =
1
3z2 + 1
2. f(z) =
z
1 −ez
Exercise 11.7
Show that if f(z) is of the form
f(z) = αk
zk + αk−1
zk−1 + · · · + α1
z + g(z),
k ≥1
where g is analytic inside and on C, (the positive circle |z| = 1), then
Z
C
f(z) dz = ı2πα1.
Exercise 11.8
Show that if f(z) is analytic within and on a simple closed contour C and z0 is not on C then
Z
C
f ′(z)
z −z0
dz =
Z
C
f(z)
(z −z0)2 dz.
Note that z0 may be either inside or outside of C.
488

Exercise 11.9
If C is the positive circle z = eıθ show that for any real constant a,
Z
C
eaz
z dz = ı2π
and hence
Z π
0
ea cos θ cos(a sin θ) dθ = π.
Exercise 11.10
Use Cauchy-Goursat, the generalized Cauchy integral formula, and suitable extensions to multiply-connected domains
to evaluate the following integrals. Be sure to justify your approach in each case.
1.
Z
C
z
z3 −9 dz
where C is the positively oriented rectangle whose sides lie along x = ±5, y = ±3.
2.
Z
C
sin z
z2(z −4) dz,
where C is the positively oriented circle |z| = 2.
3.
Z
C
(z3 + z + ı) sin z
z4 + ız3
dz,
where C is the positively oriented circle |z| = π.
4.
Z
C
ezt
z2(z + 1) dz
where C is any positive simple closed contour surrounding |z| = 1.
489

Exercise 11.11
Use Liouville’s theorem to prove the following:
1. If f(z) is entire with ℜ(f(z)) ≤M for all z then f(z) is constant.
2. If f(z) is entire with |f (5)(z)| ≤M for all z then f(z) is a polynomial of degree at most ﬁve.
Exercise 11.12
Find all functions f(z) analytic in the domain D : |z| < R that satisfy f(0) = eı and |f(z)| ≤1 for all z in D.
Exercise 11.13
Let f(z) = P∞
k=0 k4   z
4
k and evaluate the following contour integrals, providing justiﬁcation in each case:
1.
Z
C
cos(ız)f(z) dz
C is the positive circle |z −1| = 1.
2.
Z
C
f(z)
z3 dz
C is the positive circle |z| = π.
490

11.5
Hints
Hint 11.1
Use the argument theorem.
Hint 11.2
Hint 11.3
To evaluate the integral, consider the circle at inﬁnity.
Hint 11.4
Hint 11.5
Hint 11.6
Hint 11.7
Hint 11.8
Hint 11.9
Hint 11.10
491

Hint 11.11
Hint 11.12
Hint 11.13
492

11.6
Solutions
Solution 11.1
Let f(z) be analytic inside and on the contour C. Let f(z) be nonzero on the contour. The argument theorem states
that
1
ı2π
Z
C
f ′(z)
f(z) dz = N −P,
where N is the number of zeros and P is the number of poles, (counting multiplicities), of f(z) inside C. The theorem
is aptly named, as
1
ı2π
Z
C
f ′(z)
f(z) dz =
1
ı2π [log(f(z))]C
=
1
ı2π [log |f(z)| + ı arg(f(z))]C
= 1
2π [arg(f(z))]C .
Thus we could write the argument theorem as
1
ı2π
Z
C
f ′(z)
f(z) dz = 1
2π [arg(f(z))]C = N −P.
Since sin z has a single zero and no poles inside the unit circle, we have
1
2π arg(sin(z))

C = 1 −0
arg(sin(z))

C = 2π
Solution 11.2
1. Since the integrand
sin z
z2+5 is analytic inside and on the contour, (the only singularities are at z = ±ı
√
5 and at
inﬁnity), the integral is zero by Cauchy’s Theorem.
493

2. First we expand the integrand in partial fractions.
z
z2 + 1 =
a
z −ı +
b
z + ı
a =
z
z + ı

z=ı
= 1
2,
b =
z
z −ı

z=−ı
= 1
2
Now we can do the integral with Cauchy’s formula.
Z
C
z
z2 + 1 dz =
Z
C
1/2
z −ı dz +
Z
C
1/2
z + ı dz
= 1
2ı2π + 1
2ı2π
= ı2π
3.
Z
C
z2 + 1
z
dz =
Z
C

z + 1
z

dz
=
Z
C
z dz +
Z
C
1
z dz
= 0 + ı2π
= ı2π
Solution 11.3
Let C be the circle of radius r, (r > R), centered at the origin. We get an upper bound on the integral with the
Maximum Modulus Integral Bound, (Result 10.2.1).

I
C
f(z)
(z −α)(z −β) dz
 ≤2πr max
|z|=r

f(z)
(z −α)(z −β)
 ≤2πr
M
(r −|α|)(r −|β|)
494

By taking the limit as r →∞we see that the modulus of the integral is bounded above by zero. Thus the integral
vanishes.
Now we assume that f(z) is analytic and evaluate the integral with Cauchy’s Integral Formula. (We assume that
α ̸= β.)
I
C
f(z)
(z −α)(z −β) dz = 0
I
C
f(z)
(z −α)(α −β) dz +
I
C
f(z)
(β −α)(z −β) dz = 0
ı2π f(α)
α −β + ı2π f(β)
β −α = 0
f(α) = f(β)
Solution 11.4
Consider the circle |z| = 2. On this circle:
|z6| = 64
| −5z2 + 10| ≤| −5z2| + |10| = 30
Since |z6| < | −5z2 + 10| on |z| = 2, p(z) has the same number of roots as z6 in |z| < 2. p(z) has 6 roots in |z| < 2.
Consider the circle |z| = 1. On this circle:
|10| = 10
|z6 −5z2| ≤|z6| + | −5z2| = 6
Since |z6 −5z2| < |10| on |z| = 1, p(z) has the same number of roots as 10 in |z| < 1. p(z) has no roots in |z| < 1.
On the unit circle,
|p(z)| ≥|10| −|z6| −|5z2| = 4.
Thus p(z) has no roots on the unit circle.
We conclude that p(z) has exactly 6 roots in 1 < |z| < 2.
495

Solution 11.5
We evaluate the integral with Cauchy’s Integral Formula.
ω =
1
ı2π
I
C
ezt
z2(z2 + a2) dz
ω =
1
ı2π
I
C
 ezt
a2z2 +
ı ezt
2a3(z −ıa) −
ı ezt
2a3(z + ıa)

dz
ω =
 d
dz
ezt
a2

z=0
+ ı eıat
2a3 −ı e−ıat
2a3
ω = t
a2 −sin(at)
a3
ω = at −sin(at)
a3
Solution 11.6
1. We factor the denominator of the integrand.
1
3z2 + 1 =
1
3(z −ı
√
3/3)(z + ı
√
3/3)
There are two ﬁrst order poles which could contribute to the value of an integral on a closed path. Both poles
lie inside both contours. See Figure 11.2. We see that C1 can be continuously deformed to C2 on the domain
where the integrand is analytic. Thus the integrals have the same value.
2. We consider the integrand
z
1 −ez .
Since ez = 1 has the solutions z = ı2πn for n ∈Z, the integrand has singularities at these points. There is a
removable singularity at z = 0 and ﬁrst order poles at z = ı2πn for n ∈Z \ {0}. Each contour contains only the
singularity at z = 0. See Figure 11.3. We see that C1 can be continuously deformed to C2 on the domain where
the integrand is analytic. Thus the integrals have the same value.
496

-4
-2
2
4
-4
-2
2
4
Figure 11.2: The contours and the singularities of
1
3z2+1.
Solution 11.7
First we write the integral of f(z) as a sum of integrals.
Z
C
f(z) dz =
Z
C
αk
zk + αk−1
zk−1 + · · · + α1
z + g(z)

dz
=
Z
C
αk
zk dz +
Z
C
αk−1
zk−1 dz + · · · +
Z
C
α1
z dz +
Z
C
g(z) dz
The integral of g(z) vanishes by the Cauchy-Goursat theorem. We evaluate the integral of α1/z with Cauchy’s integral
formula.
Z
C
α1
z dz = ı2πα1
497

-6
-4
-2
2
4
6
-6
-4
-2
2
4
6
Figure 11.3: The contours and the singularities of
z
1−ez .
We evaluate the remaining αn/zn terms with anti-derivatives. Each of these integrals vanish.
Z
C
f(z) dz =
Z
C
αk
zk dz +
Z
C
αk−1
zk−1 dz + · · · +
Z
C
α1
z dz +
Z
C
g(z) dz
=

−
αk
(k −1)zk−1

C
+ · · · +
h
−α2
z
i
C + ı2πα1
= ı2πα1
498

Solution 11.8
We evaluate the integrals with the Cauchy integral formula. (z0 is required to not be on C so the integrals exist.)
Z
C
f ′(z)
z −z0
dz =
(
ı2πf ′(z0)
if z0 is inside C
0
if z0 is outside C
Z
C
f(z)
(z −z0)2 dz =
(
ı2π
1! f ′(z0)
if z0 is inside C
0
if z0 is outside C
Thus we see that the integrals are equal.
Solution 11.9
First we evaluate the integral using the Cauchy Integral Formula.
Z
C
eaz
z dz = [eaz]z=0 = ı2π
Next we parameterize the path of integration. We use the periodicity of the cosine and sine to simplify the integral.
Z
C
eaz
z dz = ı2π
Z 2π
0
ea eıθ
eıθ ı eıθ dθ = ı2π
Z 2π
0
ea(cos θ+ı sin θ) dθ = 2π
Z 2π
0
ea cos θ(cos(sin θ) + ı sin(sin θ)) dθ = 2π
Z 2π
0
ea cos θ cos(sin θ) dθ = 2π
Z π
0
ea cos θ cos(sin θ) dθ = π
499

Solution 11.10
1. We factor the integrand to see that there are singularities at the cube roots of 9.
z
z3 −9 =
z
 z −
3√
9
  z −
3√
9 eı2π/3  z −
3√
9 e−ı2π/3
Let C1, C2 and C3 be contours around z =
3√
9, z =
3√
9 eı2π/3 and z =
3√
9 e−ı2π/3. See Figure 11.4. Let D be
the domain between C, C1 and C2, i.e. the boundary of D is the union of C, −C1 and −C2. Since the integrand
is analytic in D, the integral along the boundary of D vanishes.
Z
∂D
z
z3 −9 dz =
Z
C
z
z3 −9 dz +
Z
−C1
z
z3 −9 dz +
Z
−C2
z
z3 −9 dz +
Z
−C3
z
z3 −9 dz = 0
From this we see that the integral along C is equal to the sum of the integrals along C1, C2 and C3. (We could
also see this by deforming C onto C1, C2 and C3.)
Z
C
z
z3 −9 dz =
Z
C1
z
z3 −9 dz +
Z
C2
z
z3 −9 dz +
Z
C3
z
z3 −9 dz
500

We use the Cauchy Integral Formula to evaluate the integrals along C1, C2 and C2.
Z
C
z
z3 −9 dz =
Z
C1
z
 z −
3√
9
  z −
3√
9 eı2π/3  z −
3√
9 e−ı2π/3 dz
+
Z
C2
z
 z −
3√
9
  z −
3√
9 eı2π/3  z −
3√
9 e−ı2π/3 dz
+
Z
C3
z
 z −
3√
9
  z −
3√
9 eı2π/3  z −
3√
9 e−ı2π/3 dz
= ı2π
"
z
 z −
3√
9 eı2π/3  z −
3√
9 e−ı2π/3
#
z= 3√
9
+ ı2π
"
z
 z −
3√
9
  z −
3√
9 e−ı2π/3
#
z= 3√
9 eı2π/3
+ ı2π
"
z
 z −
3√
9
  z −
3√
9 eı2π/3
#
z= 3√
9 e−ı2π/3
= ı2π3−5/3  1 −eıπ/3 + eı2π/3
= 0
2. The integrand has singularities at z = 0 and z = 4. Only the singularity at z = 0 lies inside the contour. We use
the Cauchy Integral Formula to evaluate the integral.
Z
C
sin z
z2(z −4) dz = ı2π
 d
dz
sin z
z −4

z=0
= ı2π
 cos z
z −4 −
sin z
(z −4)2

z=0
= −ıπ
2
501

-6
-4
-2
2
4
6
-4
-3
-2
-1
1
2
3
4 C
C1
C2
C3
Figure 11.4: The contours for
z
z3−9.
3. We factor the integrand to see that there are singularities at z = 0 and z = −ı.
Z
C
(z3 + z + ı) sin z
z4 + ız3
dz =
Z
C
(z3 + z + ı) sin z
z3(z + ı)
dz
Let C1 and C2 be contours around z = 0 and z = −ı. See Figure 11.5. Let D be the domain between C, C1 and
C2, i.e. the boundary of D is the union of C, −C1 and −C2. Since the integrand is analytic in D, the integral
along the boundary of D vanishes.
Z
∂D
=
Z
C
+
Z
−C1
+
Z
−C2
= 0
From this we see that the integral along C is equal to the sum of the integrals along C1 and C2. (We could also
see this by deforming C onto C1 and C2.)
Z
C
=
Z
C1
+
Z
C2
502

We use the Cauchy Integral Formula to evaluate the integrals along C1 and C2.
Z
C
(z3 + z + ı) sin z
z4 + ız3
dz =
Z
C1
(z3 + z + ı) sin z
z3(z + ı)
dz +
Z
C2
(z3 + z + ı) sin z
z3(z + ı)
dz
= ı2π
(z3 + z + ı) sin z
z3

z=−ı
+ ı2π
2!
 d2
dz2
(z3 + z + ı) sin z
z + ı

z=0
= ı2π(−ı sinh(1)) + ıπ

2
3z2 + 1
z + ı
−z3 + z + ı
(z + ı)2

cos z
+
 6z
z + ı −2(3z2 + 1)
(z + ı)2
+ 2(z3 + z + ı)
(z + ı)3
−z3 + z + ı
z + ı

sin z

z=0
= 2π sinh(1)
4. We consider the integral
Z
C
ezt
z2(z + 1) dz.
There are singularities at z = 0 and z = −1.
Let C1 and C2 be contours around z = 0 and z = −1. See Figure 11.6. We deform C onto C1 and C2.
Z
C
=
Z
C1
+
Z
C2
503

-4
-3
-2
-1
1
2
3
4
-4
-3
-2
-1
1
2
3
4
C
C1
C2
Figure 11.5: The contours for (z3+z+ı) sin z
z4+ız3
.
We use the Cauchy Integral Formula to evaluate the integrals along C1 and C2.
Z
C
ezt
z2(z + 1) dz =
Z
C1
ezt
z2(z + 1) dz +
Z
C1
ezt
z2(z + 1) dz
= ı2π
ezt
z2

z=−1
+ ı2π
 d
dz
ezt
(z + 1)

z=0
= ı2π e−t +ı2π
 t ezt
(z + 1) −
ezt
(z + 1)2

z=0
= ı2π(e−t +t −1)
504

-2
-1
1
2
-2
-1
1
2
C
C1
C2
Figure 11.6: The contours for
ezt
z2(z+1).
Solution 11.11
Liouville’s Theorem states that if f(z) is analytic and bounded in the complex plane then f(z) is a constant.
1. Since f(z) is analytic, ef(z) is analytic. The modulus of ef(z) is bounded.
ef(z) = eℜ(f(z)) ≤eM
By Liouville’s Theorem we conclude that ef(z) is constant and hence f(z) is constant.
2. We know that f(z) is entire and |f (5)(z)| is bounded in the complex plane. Since f(z) is analytic, so is f (5)(z).
We apply Liouville’s Theorem to f (5)(z) to conclude that it is a constant. Then we integrate to determine the
form of f(z).
f(z) = c5z5 + c4z4 + c3z3 + c2z2 + c1z + c0
505

Here c5 is the value of f (5)(z) and c4 through c0 are constants of integration. We see that f(z) is a polynomial
of degree at most ﬁve.
Solution 11.12
For this problem we will use the Extremum Modulus Theorem: Let f(z) be analytic in a closed, connected domain, D.
The extreme values of the modulus of the function must occur on the boundary. If |f(z)| has an interior extrema, then
the function is a constant.
Since |f(z)| has an interior extrema, |f(0)| = | eı | = 1, we conclude that f(z) is a constant on D. Since we know
the value at z = 0, we know that f(z) = eı.
Solution 11.13
First we determine the radius of convergence of the series with the ratio test.
R = lim
k→∞

k4/4k
(k + 1)4/4k+1

= 4 lim
k→∞
k4
(k + 1)4
= 4 lim
k→∞
24
24
= 4
The series converges absolutely for |z| < 4.
1. Since the integrand is analytic inside and on the contour of integration, the integral vanishes by Cauchy’s Theorem.
506

2.
Z
C
f(z)
z3 dz =
Z
C
∞
X
k=0
k4 z
4
k 1
z3 dz
=
Z
C
∞
X
k=1
k4
4k zk−3 dz
=
Z
C
∞
X
k=−2
(k + 3)4
4k+3
zk dz
=
Z
C
1
4z2 dz +
Z
C
1
z dz +
Z
C
∞
X
k=0
(k + 3)4
4k+3
zk dz
We can parameterize the ﬁrst integral to show that it vanishes. The second integral has the value ı2π by the
Cauchy-Goursat Theorem. The third integral vanishes by Cauchy’s Theorem as the integrand is analytic inside
and on the contour.
Z
C
f(z)
z3 dz = ı2π
507

Chapter 12
Series and Convergence
You are not thinking. You are merely being logical.
- Neils Bohr
12.1
Series of Constants
12.1.1
Deﬁnitions
Convergence of Sequences.
The inﬁnite sequence {an}∞
n=0 ≡a0, a1, a2, . . . is said to converge if
lim
n→∞an = a
for some constant a. If the limit does not exist, then the sequence diverges. Recall the deﬁnition of the limit in the
above formula: For any ϵ > 0 there exists an N ∈Z such that |a −an| < ϵ for all n > N.
Example 12.1.1 The sequence {sin(n)} is divergent. The sequence is bounded above and below, but boundedness
does not imply convergence.
508

Cauchy Convergence Criterion.
Note that there is something a little ﬁshy about the above deﬁnition. We
should be able to say if a sequence converges without ﬁrst ﬁnding the constant to which it converges. We ﬁx this
problem with the Cauchy convergence criterion. A sequence {an} converges if and only if for any ϵ > 0 there exists an
N such that |an −am| < ϵ for all n, m > N. The Cauchy convergence criterion is equivalent to the deﬁnition we had
before. For some problems it is handier to use. Now we don’t need to know the limit of a sequence to show that it
converges.
Convergence of Series.
The series P∞
n=1 an converges if the sequence of partial sums, SN = PN−1
n=0 an, converges.
That is,
lim
N→∞SN = lim
N→∞
N−1
X
n=0
an = constant.
If the limit does not exist, then the series diverges. A necessary condition for the convergence of a series is that
lim
n→∞an = 0.
Otherwise the sequence of partial sums would not converge.
Example 12.1.2 The series P∞
n=0(−1)n = 1 −1 + 1 −1 + · · · is divergent because the sequence of partial sums,
{SN} = 1, 0, 1, 0, 1, 0, . . . is divergent.
Tail of a Series.
An inﬁnite series, P∞
n=0 an, converges or diverges with its tail. That is, for ﬁxed N, P∞
n=0 an
converges if and only if P∞
n=N an converges. This is because the sum of the ﬁrst N terms of a series is just a number.
Adding or subtracting a number to a series does not change its convergence.
Absolute Convergence.
The series P∞
n=0 an converges absolutely if P∞
n=0 |an| converges. Absolute convergence
implies convergence.
If a series is convergent, but not absolutely convergent, then it is said to be conditionally
convergent.
509

The terms of an absolutely convergent series can be rearranged in any order and the series will still converge to
the same sum. This is not true of conditionally convergent series. Rearranging the terms of a conditionally convergent
series may change the sum. In fact, the terms of a conditionally convergent series may be rearranged to obtain any
desired sum.
Example 12.1.3 The alternating harmonic series,
1 −1
2 + 1
3 −1
4 + · · · ,
converges, (Exercise 12.4). Since
1 + 1
2 + 1
3 + 1
4 + · · ·
diverges, (Exercise 12.5), the alternating harmonic series is not absolutely convergent. Thus the terms can be rearranged
to obtain any sum, (Exercise 12.6).
Finite Series and Residuals.
Consider the series f(z) = P∞
n=0 an(z). We will denote the sum of the ﬁrst N
terms in the series as
SN(z) =
N−1
X
n=0
an(z).
We will denote the residual after N terms as
RN(z) ≡f(z) −SN(z) =
∞
X
n=N
an(z).
12.1.2
Special Series
510

Geometric Series.
One of the most important series in mathematics is the geometric series, 1
∞
X
n=0
zn = 1 + z + z2 + z3 + · · · .
The series clearly diverges for |z| ≥1 since the terms do not vanish as n →∞. Consider the partial sum, SN(z) ≡
PN−1
n=0 zn, for |z| < 1.
(1 −z)SN(z) = (1 −z)
N−1
X
n=0
zn
=
N−1
X
n=0
zn −
N
X
n=1
zn
=
 1 + z + · · · + zN−1
−
 z + z2 + · · · + zN
= 1 −zN
N−1
X
n=0
zn = 1 −zN
1 −z →
1
1 −z
as N →∞.
The limit of the partial sums is
1
1−z.
∞
X
n=0
zn =
1
1 −z
for |z| < 1
Harmonic Series.
Another important series is the harmonic series,
∞
X
n=1
1
nα = 1 + 1
2α + 1
3α + · · · .
1 The series is so named because the terms grow or decay geometrically. Each term in the series is a constant times the previous
term.
511

The series is absolutely convergent for ℜ(α) > 1 and absolutely divergent for ℜ(α) ≤1, (see the Exercise 12.8). The
Riemann zeta function ζ(α) is deﬁned as the sum of the harmonic series.
ζ(α) =
∞
X
n=1
1
nα
The alternating harmonic series is
∞
X
n=1
(−1)n+1
nα
= 1 −1
2α + 1
3α −1
4α + · · · .
Again, the series is absolutely convergent for ℜ(α) > 1 and absolutely divergent for ℜ(α) ≤1.
12.1.3
Convergence Tests
The Comparison Test.
Result 12.1.1 The series of positive terms P an converges if there exists a convergent series
P bn such that an ≤bn for all n. Similarly, P an diverges if there exists a divergent series
P bn such that an ≥bn for all n.
Example 12.1.4 Consider the series
∞
X
n=1
1
2n2 .
We can rewrite this as
∞
X
n=1
n a perfect square
1
2n.
512

Then by comparing this series to the geometric series,
∞
X
n=1
1
2n = 1,
we see that it is convergent.
Integral Test.
Result 12.1.2 If the coeﬃcients an of a series P∞
n=0 an are monotonically decreasing and
can be extended to a monotonically decreasing function of the continuous variable x,
a(x) = an
for x ∈Z0+,
then the series converges or diverges with the integral
Z ∞
0
a(x) dx.
Example 12.1.5 Consider the series P∞
n=1
1
n2. Deﬁne the functions sl(x) and sr(x), (left and right),
sl(x) =
1
(⌈x⌉)2,
sr(x) =
1
(⌊x⌋)2.
Recall that ⌊x⌋is the greatest integer function, the greatest integer which is less than or equal to x. ⌈x⌉is the least
integer function, the least integer greater than or equal to x. We can express the series as integrals of these functions.
∞
X
n=1
1
n2 =
Z ∞
0
sl(x) dx =
Z ∞
1
sr(x) dx
513

In Figure 12.1 these functions are plotted against y = 1/x2. From the graph, it is clear that we can obtain a lower and
upper bound for the series.
Z ∞
1
1
x2 dx ≤
∞
X
n=1
1
n2 ≤1 +
Z ∞
1
1
x2 dx
1 ≤
∞
X
n=1
1
n2 ≤2
1
2
3
4
1
1
2
3
4
1
Figure 12.1: Upper and Lower bounds to P∞
n=1 1/n2.
In general, we have
Z ∞
m
a(x) dx ≤
∞
X
n=m
an ≤am +
Z ∞
m
a(x) dx.
Thus we see that the sum converges or diverges with the integral.
514

The Ratio Test.
Result 12.1.3 The series P an converges absolutely if
lim
n→∞

an+1
an
 < 1.
If the limit is greater than unity, then the series diverges. If the limit is unity, the test fails.
If the limit is greater than unity, then the terms are eventually increasing with n. Since the terms do not vanish,
the sum is divergent. If the limit is less than unity, then there exists some N such that

an+1
an
 ≤r < 1
for all n ≥N.
From this we can show that P∞
n=0 an is absolutely convergent by comparing it to the geometric series.
∞
X
n=N
|an| ≤|aN|
∞
X
n=0
rn
= |aN|
1
1 −r
Example 12.1.6 Consider the series,
∞
X
n=1
en
n!.
515

We apply the ratio test to test for absolute convergence.
lim
n→∞

an+1
an
 = lim
n→∞
en+1 n!
en(n + 1)!
= lim
n→∞
e
n + 1
= 0
The series is absolutely convergent.
Example 12.1.7 Consider the series,
∞
X
n=1
1
n2,
which we know to be absolutely convergent. We apply the ratio test.
lim
n→∞

an+1
an
 = lim
n→∞
1/(n + 1)2
1/n2
= lim
n→∞
n2
n2 + 2n + 1
= lim
n→∞
1
1 + 2/n + 1/n2
= 1
The test fails to predict the absolute convergence of the series.
516

The Root Test.
Result 12.1.4 The series P an converges absolutely if
lim
n→∞|an|1/n < 1.
If the limit is greater than unity, then the series diverges. If the limit is unity, the test fails.
More generally, we can test that
lim sup |an|1/n < 1.
If the limit is greater than unity, then the terms in the series do not vanish as n →∞. This implies that the sum
does not converge. If the limit is less than unity, then there exists some N such that
|an|1/n ≤r < 1
for all n ≥N.
We bound the tail of the series of |an|.
∞
X
n=N
|an| =
∞
X
n=N
 |an|1/nn
≤
∞
X
n=N
rn
=
rN
1 −r
P∞
n=0 an is absolutely convergent.
Example 12.1.8 Consider the series
∞
X
n=0
nabn,
517

where a and b are real constants. We use the root test to check for absolute convergence.
lim
n→∞|nabn|1/n < 1
|b| lim
n→∞na/n < 1
|b| exp

lim
n→∞
1 ln n
n

< 1
|b| e0 < 1
|b| < 1
Thus we see that the series converges absolutely for |b| < 1. Note that the value of a does not aﬀect the absolute
convergence.
Example 12.1.9 Consider the absolutely convergent series,
∞
X
n=1
1
n2.
We aply the root test.
lim
n→∞|an|1/n = lim
n→∞

1
n2

1/n
= lim
n→∞n−2/n
= lim
n→∞e−2
n ln n
= e0
= 1
It fails to predict the convergence of the series.
518

Raabe’s Test
Result 12.1.5 The series P an converges absolutely if
lim
n→∞n

1 −

an+1
an


> 1.
If the limit is less than unity, then the series diverges or converges conditionally. If the limit
is unity, the test fails.
Gauss’ Test
Result 12.1.6 Consider the series P an. If
an+1
an
= 1 −L
n + bn
n2
where bn is bounded then the series converges absolutely if L > 1. Otherwise the series
diverges or converges conditionally.
12.2
Uniform Convergence
Continuous Functions.
A function f(z) is continuous in a closed domain if, given any ϵ > 0, there exists a δ > 0
such that |f(z) −f(ζ)| < ϵ for all |z −ζ| < δ in the domain.
An equivalent deﬁnition is that f(z) is continuous in a closed domain if
lim
ζ→z f(ζ) = f(z)
519

for all z in the domain.
Convergence.
Consider a series in which the terms are functions of z, P∞
n=0 an(z). The series is convergent in a
domain if the series converges for each point z in the domain. We can then deﬁne the function f(z) = P∞
n=0 an(z).
We can state the convergence criterion as: For any given ϵ > 0 there exists a function N(z) such that
|f(z) −SN(z)(z)| =

f(z) −
N(z)−1
X
n=0
an(z)

< ϵ
for all z in the domain. Note that the rate of convergence, i.e. the number of terms, N(z) required for for the absolute
error to be less than ϵ, is a function of z.
Uniform Convergence.
Consider a series P∞
n=0 an(z) that is convergent in some domain. If the rate of convergence
is independent of z then the series is said to be uniformly convergent. Stating this a little more mathematically, the
series is uniformly convergent in the domain if for any given ϵ > 0 there exists an N, independent of z, such that
|f(z) −SN(z)| =
f(z) −
N
X
n=1
an(z)
 < ϵ
for all z in the domain.
12.2.1
Tests for Uniform Convergence
Weierstrass M-test.
The Weierstrass M-test is useful in determining if a series is uniformly convergent. The series
P∞
n=0 an(z) is uniformly and absolutely convergent in a domain if there exists a convergent series of positive terms
P∞
n=0 Mn such that |an(z)| ≤Mn for all z in the domain. This condition ﬁrst implies that the series is absolutely
convergent for all z in the domain. The condition |an(z)| ≤Mn also ensures that the rate of convergence is independent
of z, which is the criterion for uniform convergence.
Note that absolute convergence and uniform convergence are independent. A series of functions may be absolutely
convergent without being uniformly convergent or vice versa. The Weierstrass M-test is a suﬃcient but not a necessary
520

condition for uniform convergence. The Weierstrass M-test can succeed only if the series is uniformly and absolutely
convergent.
Example 12.2.1 The series
f(x) =
∞
X
n=1
sin x
n(n + 1)
is uniformly and absolutely convergent for all real x because |
sin x
n(n+1)| <
1
n2 and P∞
n=1
1
n2 converges.
Dirichlet Test.
Consider a sequence of monotone decreasing, positive constants cn with limit zero. If all the partial
sums of an(z) are bounded in some closed domain, that is

N
X
n=1
an(z)
 < constant
for all N, then P∞
n=1 cnan(z) is uniformly convergent in that closed domain. Note that the Dirichlet test does not
imply that the series is absolutely convergent.
Example 12.2.2 Consider the series,
∞
X
n=1
sin(nx)
n
.
We cannot use the Weierstrass M-test to determine if the series is uniformly convergent on an interval. While it is easy
to bound the terms with | sin(nx)/n| ≤1/n, the sum
∞
X
n=1
1
n
521

does not converge. Thus we will try the Dirichlet test. Consider the sum PN−1
n=1 sin(nx). This sum can be evaluated
in closed form. (See Exercise 12.9.)
N−1
X
n=1
sin(nx) =
(
0
for x = 2πk
cos(x/2)−cos((N−1/2)x)
2 sin(x/2)
for x ̸= 2πk
The partial sums have inﬁnite discontinuities at x = 2πk, k ∈Z. The partial sums are bounded on any closed interval
that does not contain an integer multiple of 2π. By the Dirichlet test, the sum P∞
n=1
sin(nx)
n
is uniformly convergent
on any such closed interval. The series may not be uniformly convergent in neighborhoods of x = 2kπ.
12.2.2
Uniform Convergence and Continuous Functions.
Consider a series f(z) = P∞
n=1 an(z) that is uniformly convergent in some domain and whose terms an(z) are continuous
functions. Since the series is uniformly convergent, for any given ϵ > 0 there exists an N such that |RN| < ϵ for all z
in the domain.
Since the ﬁnite sum SN is continuous, for that ϵ there exists a δ > 0 such that |SN(z) −SN(ζ)| < ϵ for all ζ in the
domain satisfying |z −ζ| < δ.
We combine these two results to show that f(z) is continuous.
|f(z) −f(ζ)| = |SN(z) + RN(z) −SN(ζ) −RN(ζ)|
≤|SN(z) −SN(ζ)| + |RN(z)| + |RN(ζ)|
< 3ϵ
for |z −ζ| < δ
Result 12.2.1 A uniformly convergent series of continuous terms represents a continuous
function.
Example 12.2.3 Again consider P∞
n=1
sin(nx)
n
. In Example 12.2.2 we showed that the convergence is uniform in any
closed interval that does not contain an integer multiple of 2π. In Figure 12.2 is a plot of the ﬁrst 10 and then 50 terms
522

in the series and ﬁnally the function to which the series converges. We see that the function has jump discontinuities
at x = 2kπ and is continuous on any closed interval not containing one of those points.
Figure 12.2: Ten, Fifty and all the Terms of P∞
n=1
sin(nx)
n
.
12.3
Uniformly Convergent Power Series
Power Series.
Power series are series of the form
∞
X
n=0
an(z −z0)n.
Domain of Convergence of a Power Series
Consider the series P∞
n=0 anzn. Let the series converge at some
point z0. Then |anzn
0 | is bounded by some constant A for all n, so
|anzn| = |anzn
0 |

z
z0

n
< A

z
z0

n
This comparison test shows that the series converges absolutely for all z satisfying |z| < |z0|.
523

Suppose that the series diverges at some point z1. Then the series could not converge for any |z| > |z1| since
this would imply convergence at z1. Thus there exists some circle in the z plane such that the power series converges
absolutely inside the circle and diverges outside the circle.
Result 12.3.1 The domain of convergence of a power series is a circle in the complex plane.
Radius of Convergence of Power Series.
Consider a power series
f(z) =
∞
X
n=0
anzn
Applying the ratio test, we see that the series converges if
lim
n→∞
|an+1zn+1|
|anzn|
< l
lim
n→∞
|an+1|
|an| |z| < 1
|z| < lim
n→∞
|an|
|an+1|
Result 12.3.2 Ratio formula. The radius of convergence of the power series
∞
X
n=0
anzn
is
R = lim
n→∞
|an|
|an+1|
when the limit exists.
524

Result 12.3.3 Cauchy-Hadamard formula. The radius of convergence of the power series:
∞
X
n=0
anzn
is
R =
1
lim sup
np
|an|
.
Absolute Convergence of Power Series.
Consider a power series
f(z) =
∞
X
n=0
anzn
that converges for z = z0. Let M be the value of the greatest term, anzn
0 . Consider any point z such that |z| < |z0|.
We can bound the residual of P∞
n=0 |anzn|,
RN(z) =
∞
X
n=N
|anzn|
=
∞
X
n=N

anzn
anzn
0
 |anzn
0 |
≤M
∞
X
n=N

z
z0

n
525

Since |z/z0| < 1, this is a convergent geometric series.
= M

z
z0

N
1
1 −|z/z0|
→0
as N →∞
Thus the power series is absolutely convergent for |z| < |z0|.
Result 12.3.4 If the power series P∞
n=0 anzn converges for z = z0, then the series converges
absolutely for |z| < |z0|.
Example 12.3.1 Find the radii of convergence of the following series.
1.
∞
X
n=1
nzn
2.
∞
X
n=1
n!zn
3.
∞
X
n=1
n!zn!
1. We apply the ratio test to determine the radius of convergence.
R = lim
n→∞

an
an+1
 = lim
n→∞
n
n + 1 = 1
The series converges absolutely for |z| < 1.
526

2. We apply the ratio test to the series.
R = lim
n→∞

n!
(n + 1)!

= lim
n→∞
1
n + 1
= 0
The series has a vanishing radius of convergence. It converges only for z = 0.
3. Again we apply the ration test to determine the radius of convergence.
lim
n→∞

(n + 1)!z(n+1)!
n!zn!
 < 1
lim
n→∞(n + 1)|z|(n+1)!−n! < 1
lim
n→∞(n + 1)|z|(n)n! < 1
lim
n→∞(ln(n + 1) + (n)n! ln |z|) < 0
ln |z| < lim
n→∞
−ln(n + 1)
(n)n!
ln |z| < 0
|z| < 1
The series converges absolutely for |z| < 1.
Alternatively we could determine the radius of convergence of the series with the comparison test.
∞
X
n=1
n!zn! ≤
∞
X
n=1
|nzn|
527

P∞
n=1 nzn has a radius of convergence of 1. Thus the series must have a radius of convergence of at least 1.
Note that if |z| > 1 then the terms in the series do not vanish as n →∞. Thus the series must diverge for all
|z| ≥1. Again we see that the radius of convergence is 1.
Uniform Convergence of Power Series.
Consider a power series P∞
n=0 anzn that converges in the disk |z| < r0.
The sum converges absolutely for z in the closed disk, |z| ≤r < r0. Since |anzn| ≤|anrn| and P∞
n=0 |anrn| converges,
the power series is uniformly convergent in |z| ≤r < r0.
Result 12.3.5 If the power series P∞
n=0 anzn converges for |z| < r0 then the series converges
uniformly for |z| ≤r < r0.
Example 12.3.2 Convergence and Uniform Convergence. Consider the series
log(1 −z) = −
∞
X
n=1
zn
n .
This series converges for |z| ≤1, z ̸= 1. Is the series uniformly convergent in this domain? The residual after N terms
RN is
RN(z) =
∞
X
n=N+1
zn
n .
We can get a lower bound on the absolute value of the residual for real, positive z.
|RN(x)| =
∞
X
n=N+1
xn
n
≤
Z ∞
N+1
xα
α dα
= −Ei((N + 1) ln x)
528

The exponential integral function, Ei(z), is deﬁned
Ei(z) = −
Z ∞
−z
e−t
t dt.
The exponential integral function is plotted in Figure 12.3. Since Ei(z) diverges as z →0, by choosing x suﬃciently
close to 1 the residual can be made arbitrarily large.
Thus this series is not uniformly convergent in the domain
|z| ≤1, z ̸= 1. The series is uniformly convergent for |z| ≤r < 1.
-4
-3
-2
-1
-2
-1.75
-1.5
-1.25
-1
-0.75
-0.5
-0.25
Figure 12.3: The Exponential Integral Function.
Analyticity.
Recall that a suﬃcient condition for the analyticity of a function f(z) in a domain is that
H
C f(z) dz = 0
for all simple, closed contours in the domain.
529

Consider a power series f(z) = P∞
n=0 anzn that is uniformly convergent in |z| ≤r. If C is any simple, closed
contour in the domain then
H
C f(z) dz exists. Expanding f(z) into a ﬁnite series and a residual,
I
C
f(z) dz =
I
C
(SN(z) + RN(z)) dz.
Since the series is uniformly convergent, for any given ϵ > 0 there exists an Nϵ such that |RNϵ| < ϵ for all z in |z| ≤r.
Let L be the length of the contour C. 
I
C
RNϵ(z) dz
 ≤Lϵ →0
as Nϵ →∞
I
C
f(z) dz = lim
N→∞
I
C
 N−1
X
n=0
anzn + RN(z)
!
dz
=
I
C
∞
X
n=0
anzn
=
∞
X
n=0
an
I
C
zn dz
= 0
Thus f(z) is analytic for |z| < r.
Result 12.3.6 A power series is analytic in its domain of uniform convergence.
12.4
Integration and Diﬀerentiation of Power Series
Consider a power series f(z) = P∞
n=0 anzn that is convergent in the disk |z| < r0. Let C be any contour of ﬁnite
length L lying entirely within the closed domain |z| ≤r < r0. The integral of f(z) along C is
Z
C
f(z) dz =
Z
C
(SN(z) + RN(z)) dz.
530

Since the series is uniformly convergent in the closed disk, for any given ϵ > 0, there exists an Nϵ such that
|RNϵ(z)| < ϵ
for all |z| ≤r.
We bound the absolute value of the integral of RNϵ(z).

Z
C
RNϵ(z) dz
 ≤
Z
C
|RNϵ(z)| dz
< ϵL
→0
as Nϵ →∞
Thus
Z
C
f(z) dz = lim
N→∞
Z
C
N
X
n=0
anzn dz
= lim
N→∞
N
X
n=0
an
Z
C
zn dz
=
∞
X
n=0
an
Z
C
zn dz
Result 12.4.1 If C is a contour lying in the domain of uniform convergence of the power
series P∞
n=0 anzn then
Z
C
∞
X
n=0
anzn dz =
∞
X
n=0
an
Z
C
zn dz.
In the domain of uniform convergence of a series we can interchange the order of summation and a limit process.
That is,
lim
z→z0
∞
X
n=0
an(z) =
∞
X
n=0
lim
z→z0 an(z).
531

We can do this because the rate of convergence does not depend on z. Since diﬀerentiation is a limit process,
d
dzf(z) = lim
h→0
f(z + h) −f(z)
h
,
we would expect that we could diﬀerentiate a uniformly convergent series.
Since we showed that a uniformly convergent power series is equal to an analytic function, we can diﬀerentiate a
power series in it’s domain of uniform convergence.
Result 12.4.2 Power series can be diﬀerentiated in their domain of uniform convergence.
d
dz
∞
X
n=0
anzn =
∞
X
n=0
(n + 1)an+1zn.
Example 12.4.1 Diﬀerentiating a Series. Consider the series from Example 12.3.2.
log(1 −z) = −
∞
X
n=1
zn
n
We diﬀerentiate this to obtain the geometric series.
−
1
1 −z = −
∞
X
n=1
zn−1
1
1 −z =
∞
X
n=0
zn
The geometric series is convergent for |z| < 1 and uniformly convergent for |z| ≤r < 1. Note that the domain of
convergence is diﬀerent than the series for log(1 −z). The geometric series does not converge for |z| = 1, z ̸= 1.
However, the domain of uniform convergence has remained the same.
532

12.5
Taylor Series
Result 12.5.1 Taylor’s Theorem. Let f(z) be a function that is single-valued and analytic
in |z −z0| < R. For all z in this open disk, f(z) has the convergent Taylor series
f(z) =
∞
X
n=0
f (n)(z0)
n!
(z −z0)n.
(12.1)
We can also write this as
f(z) =
∞
X
n=0
an(z −z0)n,
an = f (n)(z0)
n!
=
1
ı2π
I
C
f(z)
(z −z0)n+1 dz,
(12.2)
where C is a simple, positive, closed contour in 0 < |z −z0| < R that goes once around the
point z0.
Proof of Taylor’s Theorem.
Let’s see why Result 12.5.1 is true. Consider a function f(z) that is analytic in
|z| < R. (Considering z0 ̸= 0 is only trivially more general as we can introduce the change of variables ζ = z −z0.)
According to Cauchy’s Integral Formula, (Result ??),
f(z) =
1
ı2π
I
C
f(ζ)
ζ −z dζ,
(12.3)
where C is a positive, simple, closed contour in 0 < |ζ −z| < R that goes once around z. We take this contour to be
the circle about the origin of radius r where |z| < r < R. (See Figure 12.4.)
533

Im(z)
Re(z)
r
C
R
z
Figure 12.4: Graph of Domain of Convergence and Contour of Integration.
We expand
1
ζ−z in a geometric series,
1
ζ −z =
1/ζ
1 −z/ζ
= 1
ζ
∞
X
n=0
z
ζ
n
,
for |z| < |ζ|
=
∞
X
n=0
zn
ζn+1,
for |z| < |ζ|
We substitute this series into Equation 12.3.
f(z) =
1
ı2π
I
C
 ∞
X
n=0
f(ζ)zn
ζn+1
!
dζ
534

The series converges uniformly so we can interchange integration and summation.
=
∞
X
n=0
zn
ı2π
I
C
f(ζ)
ζn+1 dζ
Now we have derived Equation 12.2. To obtain Equation 12.1, we apply Cauchy’s Integral Formula.
=
∞
X
n=0
f (n)(0)
n!
zn
There is a table of some commonly encountered Taylor series in Appendix H.
Example 12.5.1 Consider the Taylor series expansion of 1/(1 −z) about z = 0. Previously, we showed that this
function is the sum of the geometric series P∞
n=0 zn and we used the ratio test to show that the series converged
absolutely for |z| < 1. Now we ﬁnd the series using Taylor’s theorem. Since the nearest singularity of the function is
at z = 1, the radius of convergence of the series is 1. The coeﬃcients in the series are
an = 1
n!
 dn
dzn
1
1 −z

z=0
= 1
n!

n!
(1 −z)n

z=0
= 1
Thus we have
1
1 −z =
∞
X
n=0
zn,
for |z| < 1.
535

12.5.1
Newton’s Binomial Formula.
Result 12.5.2 For all |z| < 1, a complex:
(1 + z)a = 1 +
a
1

z +
a
2

z2 +
a
3

z3 + · · ·
where
a
r

= a(a −1)(a −2) · · · (a −r + 1)
r!
.
If a is complex, then the expansion is of the principle branch of (1 + z)a. We deﬁne
r
0

= 1,
0
r

= 0,
for r ̸= 0,
0
0

= 1.
Example 12.5.2 Evaluate limn→∞(1 + 1/n)n.
First we expand (1 + 1/n)n using Newton’s binomial formula.
lim
n→∞

1 + 1
n
n
= lim
n→∞

1 +
n
1
1
n +
n
2
 1
n2 +
n
3
 1
n3 + · · ·

= lim
n→∞

1 + 1 + n(n −1)
2!n2
+ n(n −1)(n −2)
3!n3
+ · · ·

=

1 + 1 + 1
2! + 1
3! + · · ·

We recognize this as the Taylor series expansion of e1.
= e
536

We can also evaluate the limit using L’Hospital’s rule.
ln

lim
x→∞

1 + 1
x
x
= lim
x→∞ln

1 + 1
x
x
= lim
x→∞x ln

1 + 1
x

= lim
x→∞
ln(1 + 1/x)
1/x
= lim
x→∞
−1/x2
1+1/x
−1/x2
= 1
lim
x→∞

1 + 1
x
x
= e
Example 12.5.3 Find the Taylor series expansion of 1/(1 + z) about z = 0.
For |z| < 1,
1
1 + z = 1 +
−1
1

z +
−1
2

z2 +
−1
3

z3 + · · ·
= 1 + (−1)1z + (−1)2z2 + (−1)3z3 + · · ·
= 1 −z + z2 −z3 + · · ·
Example 12.5.4 Find the ﬁrst few terms in the Taylor series expansion of
1
√
z2 + 5z + 6
about the origin.
537

We factor the denominator and then apply Newton’s binomial formula.
1
√
z2 + 5z + 6 =
1
√z + 3
1
√z + 2
=
1
√
3
p
1 + z/3
1
√
2
p
1 + z/2
= 1
√
6

1 +
−1/2
1
z
3 +
−1/2
2
 z
3
2
+ · · ·
 
1 +
−1/2
1
z
2 +
−1/2
2
 z
2
2
+ · · ·

= 1
√
6

1 −z
6 + z2
24 + · · ·
 
1 −z
4 + 3z2
32 + · · ·

= 1
√
6

1 −5
12z + 17
96z2 + · · ·

12.6
Laurent Series
Result 12.6.1 Let f(z) be single-valued and analytic in the annulus R1 < |z −z0| < R2.
For points in the annulus, the function has the convergent Laurent series
f(z) =
∞
X
n=−∞
anzn,
where
an =
1
ı2π
I
C
f(z)
(z −z0)n+1 dz
and C is a positively oriented, closed contour around z0 lying in the annulus.
To derive this result, consider a function f(ζ) that is analytic in the annulus R1 < |ζ| < R2. Consider any point z
538

in the annulus. Let C1 be a circle of radius r1 with R1 < r1 < |z|. Let C2 be a circle of radius r2 with |z| < r2 < R2.
Let Cz be a circle around z, lying entirely between C1 and C2. (See Figure 12.5 for an illustration.)
Consider the integral of f(ζ)
ζ−z around the C2 contour. Since the the only singularities of f(ζ)
ζ−z occur at ζ = z and at
points outside the annulus,
I
C2
f(ζ)
ζ −z dζ =
I
Cz
f(ζ)
ζ −z dζ +
I
C1
f(ζ)
ζ −z dζ.
By Cauchy’s Integral Formula, the integral around Cz is
I
Cz
f(ζ)
ζ −z dζ = ı2πf(z).
This gives us an expression for f(z).
f(z) =
1
ı2π
I
C2
f(ζ)
ζ −z dζ −1
ı2π
I
C1
f(ζ)
ζ −z dζ
(12.4)
On the C2 contour, |z| < |ζ|. Thus
1
ζ −z =
1/ζ
1 −z/ζ
= 1
ζ
∞
X
n=0
z
ζ
n
,
for |z| < |ζ|
=
∞
X
n=0
zn
ζn+1,
for |z| < |ζ|
539

On the C1 contour, |ζ| < |z|. Thus
−
1
ζ −z =
1/z
1 −ζ/z
= 1
z
∞
X
n=0
ζ
z
n
,
for |ζ| < |z|
=
∞
X
n=0
ζn
zn+1,
for |ζ| < |z|
=
−1
X
n=−∞
zn
ζn+1,
for |ζ| < |z|
We substitute these geometric series into Equation 12.4.
f(z) =
1
ı2π
I
C2
 ∞
X
n=0
f(ζ)zn
ζn+1
!
dζ + 1
ı2π
I
C1
 
−1
X
n=−∞
f(ζ)zn
ζn+1
!
dζ
Since the sums converge uniformly, we can interchange the order of integration and summation.
f(z) =
1
ı2π
∞
X
n=0
I
C2
f(ζ)zn
ζn+1 dζ + 1
ı2π
−1
X
n=−∞
I
C1
f(ζ)zn
ζn+1 dζ
Since the only singularities of the integrands lie outside of the annulus, the C1 and C2 contours can be deformed to
any positive, closed contour C that lies in the annulus and encloses the origin. (See Figure 12.5.) Finally, we combine
the two integrals to obtain the desired result.
f(z) =
∞
X
n=−∞
1
ı2π
I
C
f(ζ)
ζn+1 dζ

zn
For the case of arbitrary z0, simply make the transformation z →z −z0.
540

Im(z)
Re(z)
R
R2
1
Im(z)
Re(z)
R
R2
1
C
r1
r2
z
C
C
C1
2
z
Figure 12.5: Contours for a Laurent Expansion in an Annulus.
Example 12.6.1 Find the Laurent series expansions of 1/(1 + z).
For |z| < 1,
1
1 + z = 1 +
−1
1

z +
−1
2

z2 +
−1
3

z3 + · · ·
= 1 + (−1)1z + (−1)2z2 + (−1)3z3 + · · ·
= 1 −z + z2 −z3 + · · ·
541

For |z| > 1,
1
1 + z =
1/z
1 + 1/z
= 1
z

1 +
−1
1

z−1 +
−1
2

z−2 + · · ·

= z−1 −z−2 + z−3 −· · ·
542

12.7
Exercises
Exercise 12.1
Answer the following questions true or false. Justify your answers.
1. There exists a convergent series whose terms do not converge to zero.
2. There exists a sequence which converges to both 1 and −1.
3. There exists a sequence {an} such that an > 1 for all n and limn→∞an = 1.
4. There exists a divergent geometric series whose terms converge.
5. There exists a sequence whose even terms are greater than 1, whose odd terms are less than 1 and that converges
to 1.
6. There exists a divergent series of non-negative terms, P∞
n=0 an, such that an < (1/2)n.
7. There exists a convergent sequence, {an}, such that limn→∞(an+1 −an) ̸= 0.
8. There exists a divergent sequence, {an}, such that limn→∞|an| = 2.
9. There exists divergent series, P an and P bn, such that P(an + bn) is convergent.
10. There exists 2 diﬀerent series of nonzero terms that have the same sum.
11. There exists a series of nonzero terms that converges to zero.
12. There exists a series with an inﬁnite number of non-real terms which converges to a real number.
13. There exists a convergent series P an with limn→∞|an+1/an| = 1.
14. There exists a divergent series P an with limn→∞|an+1/an| = 1.
15. There exists a convergent series P an with limn→∞
np
|an| = 1.
543

16. There exists a divergent series P an with limn→∞
np
|an| = 1.
17. There exists a convergent series of non-negative terms, P an, for which P a2
n diverges.
18. There exists a convergent series of non-negative terms, P an, for which P √an diverges.
19. There exists a convergent series, P an, for which P |an| diverges.
20. There exists a power series P an(z −z0)n which converges for z = 0 and z = 3 but diverges for z = 2.
21. There exists a power series P an(z −z0)n which converges for z = 0 and z = ı2 but diverges for z = 2.
Hint, Solution
Exercise 12.2
Determine if the following series converge.
1.
∞
X
n=2
1
n ln(n)
2.
∞
X
n=2
1
ln (nn)
3.
∞
X
n=2
ln
n√
ln n
4.
∞
X
n=10
1
n(ln n)(ln(ln n))
5.
∞
X
n=1
ln (2n)
ln (3n) + 1
544

6.
∞
X
n=0
1
ln(n + 20)
7.
∞
X
n=0
4n + 1
3n −2
8.
∞
X
n=0
(Logπ 2)n
9.
∞
X
n=2
n2 −1
n4 −1
10.
∞
X
n=2
n2
(ln n)n
11.
∞
X
n=2
(−1)n ln
1
n

12.
∞
X
n=2
(n!)2
(2n)!
13.
∞
X
n=2
3n + 4n + 5
5n −4n −3
14.
∞
X
n=2
n!
(ln n)n
15.
∞
X
n=2
en
ln(n!)
545

16.
∞
X
n=1
(n!)2
(n2)!
17.
∞
X
n=1
n8 + 4n4 + 8
3n9 −n5 + 9n
18.
∞
X
n=1
1
n −
1
n + 1

19.
∞
X
n=1
cos(nπ)
n
20.
∞
X
n=2
ln n
n11/10
Hint, Solution
Exercise 12.3
Determine the domain of convergence of the following series.
1.
∞
X
n=0
zn
(z + 3)n
2.
∞
X
n=2
Log z
ln n
3.
∞
X
n=1
z
n
546

4.
∞
X
n=1
(z + 2)2
n2
5.
∞
X
n=1
(z −e)n
nn
6.
∞
X
n=1
z2n
2nz
7.
∞
X
n=0
zn!
(n!)2
8.
∞
X
n=0
zln(n!)
n!
9.
∞
X
n=0
(z −π)2n+1nπ
n!
10.
∞
X
n=0
ln n
zn
Hint, Solution
Exercise 12.4 (mathematica/fcv/series/constants.nb)
Show that the alternating harmonic series,
∞
X
n=1
(−1)n+1
n
= 1 −1
2 + 1
3 −1
4 + · · · ,
is convergent.
Hint, Solution
547

Exercise 12.5 (mathematica/fcv/series/constants.nb)
Show that the series
∞
X
n=1
1
n
is divergent with the Cauchy convergence criterion.
Hint, Solution
Exercise 12.6
The alternating harmonic series has the sum:
∞
X
n=1
(−1)n
n
= ln(2).
Show that the terms in this series can be rearranged to sum to π.
Hint, Solution
Exercise 12.7 (mathematica/fcv/series/constants.nb)
Is the series,
∞
X
n=1
n!
nn,
convergent?
Hint, Solution
Exercise 12.8
Show that the harmonic series,
∞
X
n=1
1
nα = 1 + 1
2α + 1
3α + · · · ,
converges for α > 1 and diverges for α ≤1.
Hint, Solution
548

Exercise 12.9
Evaluate PN−1
n=1 sin(nx).
Hint, Solution
Exercise 12.10
Using the geometric series, show that
1
(1 −z)2 =
∞
X
n=0
(n + 1)zn,
for |z| < 1,
and
log(1 −z) = −
∞
X
n=1
zn
n ,
for |z| < 1.
Hint, Solution
Exercise 12.11
Find the Taylor series of
1
1+z2 about the z = 0. Determine the radius of convergence of the Taylor series from the
singularities of the function. Determine the radius of convergence with the ratio test.
Hint, Solution
Exercise 12.12
Use two methods to ﬁnd the Taylor series expansion of log(1+z) about z = 0 and determine the circle of convergence.
First directly apply Taylor’s theorem, then diﬀerentiate a geometric series.
Hint, Solution
Exercise 12.13
Find the Laurent series about z = 0 of 1/(z −ı) for |z| < 1 and |z| > 1.
Hint, Solution
549

Exercise 12.14
Evaluate
n
X
k=1
kzk
and
n
X
k=1
k2zk
for z ̸= 1.
Hint, Solution
Exercise 12.15
Find the circle of convergence of the following series.
1. z + (α −β)z2
2! + (α −β)(α −2β)z3
3! + (α −β)(α −2β)(α −3β)z4
4! + · · ·
2.
∞
X
n=1
n
2n(z −ı)n
3.
∞
X
n=1
nnzn
4.
∞
X
n=1
n!
nnzn
5.
∞
X
n=1
(3 + (−1)n)n zn
6.
∞
X
n=1
(n + αn) zn
(|α| > 1)
Hint, Solution
550

Exercise 12.16
Let f(z) = (1 + z)α be the branch for which f(0) = 1. Find its Taylor series expansion about z = 0. What is the
radius of convergence of the series? (α is an arbitrary complex number.)
Hint, Solution
Exercise 12.17
Obtain the Laurent expansion of
f(z) =
1
(z + 1)(z + 2)
centered on z = 0 for the three regions:
1. |z| < 1
2. 1 < |z| < 2
3. 2 < |z|
Hint, Solution
Exercise 12.18
By comparing the Laurent expansion of (z + 1/z)m, m ∈Z+, with the binomial expansion of this quantity, show that
Z 2π
0
(cos θ)m cos(nθ) dθ =
(
π
2m−1
 m
(m−n)/2

−m ≤n ≤m and m −n even
0
otherwise
Hint, Solution
Exercise 12.19
The function f(z) is analytic in the entire z-plane, including ∞, except at the point z = ı/2, where it has a simple
pole, and at z = 2, where it has a pole of order 2. In addition
I
|z|=1
f(z) dz = ı2π,
I
|z|=3
f(z) dz = 0,
I
|z|=3
(z −1)f(z) dz = 0.
551

Find f(z) and its complete Laurent expansion about z = 0.
Hint, Solution
Exercise 12.20
Let f(z) = P∞
k=1 k3   z
3
k. Compute each of the following, giving justiﬁcation in each case. The contours are circles of
radius one about the origin.
1.
Z
|z|=1
eız f(z) dz
2.
Z
|z|=1
f(z)
z4 dz
3.
Z
|z|=1
f(z) ez
z2
dz
Hint, Solution
Exercise 12.21
Find the Taylor series expansions about the point z = 1 for the following functions. What are the radii of convergence?
1. 1
z
2. Log z
3. 1
z2
4. zLogz −z
Hint, Solution
552

Exercise 12.22
Find the Taylor series expansion about the point z = 0 for ez. What is the radius of convergence? Use this to ﬁnd the
Taylor series expansions of cos z and sin z about z = 0.
Hint, Solution
Exercise 12.23
Find the Taylor series expansion about the point z = π for the cosine and sine.
Hint, Solution
Exercise 12.24
Sum the following series.
1.
∞
X
n=0
(ln 2)n
n!
2.
∞
X
n=0
(n + 1)(n + 2)
2n
3.
∞
X
n=0
(−1)n
n!
4.
∞
X
n=0
(−1)nπ2n+1
(2n + 1)!
5.
∞
X
n=0
(−1)nπ2n
(2n)!
6.
∞
X
n=0
(−π)n
(2n)!
Hint, Solution
553

Exercise 12.25
Show that if P an converges then limn→∞an = 0.
Hint, Solution
Exercise 12.26
Which of the following series converge? Find the sum of those that do.
1. 1
2 + 1
6 + 1
12 + 1
20 + · · ·
2. 1 + (−1) + 1 + (−1) + · · ·
3.
∞
X
n=1
1
2n−1
1
3n
1
5n+1
Hint, Solution
Exercise 12.27
Evaluate the following sum.
∞
X
k1=0
∞
X
k2=k1
· · ·
∞
X
kn=kn−1
1
2kn
Hint, Solution
Exercise 12.28
1. Find the ﬁrst three terms in the following Taylor series and state the convergence properties for the following.
(a) e−z around z0 = 0
(b) 1 + z
1 −z around z0 = ı
(c)
ez
z −1 around z0 = 0
554

It may be convenient to use the Cauchy product of two Taylor series.
2. Consider a function f(z) analytic for |z −z0| < R. Show that the series obtained by diﬀerentiating the Taylor
series for f(z) termwise is actually the Taylor series for f ′(z) and hence argue that this series converges uniformly
to f ′(z) for |z −z0| ≤ρ < R.
3. Find the Taylor series for
1
(1 −z)3
by appropriate diﬀerentiation of the geometric series and state the radius of convergence.
4. Consider the branch of f(z) = (z +1)ı corresponding to f(0) = 1. Find the Taylor series expansion about z0 = 0
and state the radius of convergence.
Hint, Solution
Exercise 12.29
Find the circle of convergence of the following series:
1.
∞
X
k=0
kzk
2.
∞
X
k=1
kkzk
3.
∞
X
k=1
k!
kk zk
4.
∞
X
k=0
(z + ı5)2k(k + 1)2
555

5.
∞
X
k=0
(k + 2k)zk
Hint, Solution
Exercise 12.30
1. Expand f(z) =
1
z(1−z) in Laurent series that converge in the following domains:
(a) 0 < |z| < 1
(b) |z| > 1
(c) |z + 1| > 2
2. Without determining the series, specify the region of convergence for a Laurent series representing f(z) =
1/(z4 + 4) in powers of z −1 that converges at z = ı.
Hint, Solution
Exercise 12.31
1. Classify all the singularities (removable, poles, isolated essential, branch points, non-isolated essential) of the
following functions in the extended complex plane
(a)
z
z2 + 1
(b)
1
sin z
(c) log
 1 + z2
(d) z sin(1/z)
(e)
tan−1(z)
z sinh2(πz)
2. Construct functions that have the following zeros or singularities:
556

(a) a simple zero at z = ı and an isolated essential singularity at z = 1.
(b) a removable singularity at z = 3, a pole of order 6 at z = −ı and an essential singularity at z∞.
Hint, Solution
557

12.8
Hints
Hint 12.1
CONTINUE
Hint 12.2
1.
∞
X
n=2
1
n ln(n)
Use the integral test.
2.
∞
X
n=2
1
ln (nn)
Simplify the summand.
3.
∞
X
n=2
ln
n√
ln n
Simplify the summand. Use the comparison test.
4.
∞
X
n=10
1
n(ln n)(ln(ln n))
Use the integral test.
5.
∞
X
n=1
ln (2n)
ln (3n) + 1
Show that the terms in the sum do not vanish as n →∞
558

6.
∞
X
n=0
1
ln(n + 20)
Shift the indices.
7.
∞
X
n=0
4n + 1
3n −2
Show that the terms in the sum do not vanish as n →∞
8.
∞
X
n=0
(Logπ 2)n
This is a geometric series.
9.
∞
X
n=2
n2 −1
n4 −1
Simplify the integrand. Use the comparison test.
10.
∞
X
n=2
n2
(ln n)n
Compare to a geometric series.
11.
∞
X
n=2
(−1)n ln
1
n

Group pairs of consecutive terms to obtain a series of positive terms.
559

12.
∞
X
n=2
(n!)2
(2n)!
Use the comparison test.
13.
∞
X
n=2
3n + 4n + 5
5n −4n −3
Use the root test.
14.
∞
X
n=2
n!
(ln n)n
Show that the terms do not vanish as n →∞.
15.
∞
X
n=2
en
ln(n!)
Show that the terms do not vanish as n →∞.
16.
∞
X
n=1
(n!)2
(n2)!
Apply the ratio test.
17.
∞
X
n=1
n8 + 4n4 + 8
3n9 −n5 + 9n
Use the comparison test.
560

18.
∞
X
n=1
1
n −
1
n + 1

Use the comparison test.
19.
∞
X
n=1
cos(nπ)
n
Simplify the integrand.
20.
∞
X
n=2
ln n
n11/10
Use the integral test.
Hint 12.3
1.
∞
X
n=0
zn
(z + 3)n
2.
∞
X
n=2
Log z
ln n
3.
∞
X
n=1
z
n
4.
∞
X
n=1
(z + 2)2
n2
5.
∞
X
n=1
(z −e)n
nn
561

6.
∞
X
n=1
z2n
2nz
7.
∞
X
n=0
zn!
(n!)2
8.
∞
X
n=0
zln(n!)
n!
9.
∞
X
n=0
(z −π)2n+1nπ
n!
10.
∞
X
n=0
ln n
zn
Hint 12.4
Group the terms.
1 −1
2 = 1
2
1
3 −1
4 = 1
12
1
5 −1
6 = 1
30
· · ·
Hint 12.5
Show that
|S2n −Sn| > 1
2.
562

Hint 12.6
The alternating harmonic series is conditionally convergent. Let {an} and {bn} be the positive and negative terms in
the sum, respectively, ordered in decreasing magnitude. Note that both P∞
n=1 an and P∞
n=1 bn are divergent. Devise a
method for alternately taking terms from {an} and {bn}.
Hint 12.7
Use the ratio test.
Hint 12.8
Use the integral test.
Hint 12.9
Note that sin(nx) = ℑ(eınx). This substitute will yield a ﬁnite geometric series.
Hint 12.10
Diﬀerentiate the geometric series. Integrate the geometric series.
Hint 12.11
The Taylor series is a geometric series.
Hint 12.12
Hint 12.13
Hint 12.14
Let Sn be the sum. Consider Sn −zSn. Use the ﬁnite geometric sum.
Hint 12.15
563

Hint 12.16
Hint 12.17
Hint 12.18
Hint 12.19
Hint 12.20
Hint 12.21
1.
1
z =
1
1 + (z −1)
The right side is the sum of a geometric series.
2. Integrate the series for 1/z.
3. Diﬀerentiate the series for 1/z.
4. Integrate the series for Log z.
Hint 12.22
Evaluate the derivatives of ez at z = 0. Use Taylor’s Theorem.
Write the cosine and sine in terms of the exponential function.
564

Hint 12.23
cos z = −cos(z −π)
sin z = −sin(z −π)
Hint 12.24
CONTINUE
Hint 12.25
If P an converges then
∀ϵ > 0 ∃N s.t. m, n > N ⇒|Sm −Sn| < ϵ.
Hint 12.26
1. The summand is a rational function. Find the ﬁrst few partial sums.
2.
3. This a geometric series.
Hint 12.27
CONTINUE
Hint 12.28
CONTINUE
Hint 12.29
CONTINUE
Hint 12.30
CONTINUE
565

Hint 12.31
CONTINUE
566

12.9
Solutions
Solution 12.1
CONTINUE
Solution 12.2
1.
∞
X
n=2
1
n ln(n)
Since this is a series of positive, monotone decreasing terms, the sum converges or diverges with the integral,
Z ∞
2
1
x ln x dx =
Z ∞
ln 2
1
ξ dξ
Since the integral diverges, the series also diverges.
2.
∞
X
n=2
1
ln (nn) =
∞
X
n=2
1
n ln(n)
The sum converges.
3.
∞
X
n=2
ln
n√
ln n =
∞
X
n=2
1
n ln(ln n) ≥
∞
X
n=2
1
n
The sum is divergent by the comparison test.
4.
∞
X
n=10
1
n(ln n)(ln(ln n))
567

Since this is a series of positive, monotone decreasing terms, the sum converges or diverges with the integral,
Z ∞
10
1
x ln x ln(ln x) dx =
Z ∞
ln(10)
1
y ln y dy =
Z ∞
ln(ln(10))
1
z dz
Since the integral diverges, the series also diverges.
5.
∞
X
n=1
ln (2n)
ln (3n) + 1 =
∞
X
n=1
n ln 2
n ln 3 + 1 =
∞
X
n=1
ln 2
ln 3 + 1/n
Since the terms in the sum do not vanish as n →∞, the series is divergent.
6.
∞
X
n=0
1
ln(n + 20) =
∞
X
n=20
1
ln n
The series diverges.
7.
∞
X
n=0
4n + 1
3n −2
Since the terms in the sum do not vanish as n →∞, the series is divergent.
8.
∞
X
n=0
(Logπ 2)n
This is a geometric series. Since | Logπ 2| < 1, the series converges.
9.
∞
X
n=2
n2 −1
n4 −1 =
∞
X
n=2
1
n2 + 1 <
∞
X
n=2
1
n2
The series converges by comparison to the harmonic series.
568

10.
∞
X
n=2
n2
(ln n)n =
∞
X
n=2
n2/n
ln n
n
Since n2/n →1 as n →∞, n2/n/ ln n →0 as n →∞. The series converges by comparison to a geometric
series.
11. We group pairs of consecutive terms to obtain a series of positive terms.
∞
X
n=2
(−1)n ln
1
n

=
∞
X
n=1

ln
 1
2n

−ln

1
2n + 1

=
∞
X
n=1
ln
2n + 1
2n

The series on the right side diverges because the terms do not vanish as n →∞.
12.
∞
X
n=2
(n!)2
(2n)! =
∞
X
n=2
(1)(2) · · · n
(n + 1)(n + 2) · · · (2n) <
∞
X
n=2
1
2n
The series converges by comparison with a geometric series.
13.
∞
X
n=2
3n + 4n + 5
5n −4n −3
We use the root test to check for convergence.
lim
n→∞|an|1/n = lim
n→∞

3n + 4n + 5
5n −4n −3

1/n
= lim
n→∞
4
5

(3/4)n + 1 + 5/4n
1 −(4/5)n −3/5n

1/n
= 4
5
< 1
569

We see that the series is absolutely convergent.
14. We will use the comparison test.
∞
X
n=2
n!
(ln n)n >
∞
X
n=2
(n/2)n/2
(ln n)n =
∞
X
n=2
 p
n/2
ln n
!n
Since the terms in the series on the right side do not vanish as n →∞, the series is divergent.
15. We will use the comparison test.
∞
X
n=2
en
ln(n!) >
∞
X
n=2
en
ln(nn) =
∞
X
n=2
en
n ln(n)
Since the terms in the series on the right side do not vanish as n →∞, the series is divergent.
16.
∞
X
n=1
(n!)2
(n2)!
We apply the ratio test.
lim
n→∞

an+1
an
 = lim
n→∞

((n + 1)!)2(n2)!
((n + 1)2)!(n!)2

= lim
n→∞

(n + 1)2
((n + 1)2 −n2)!

= lim
n→∞

(n + 1)2
(2n + 1)!

= 0
The series is convergent.
570

17.
∞
X
n=1
n8 + 4n4 + 8
3n9 −n5 + 9n =
∞
X
n=1
1
n
1 + 4n−4 + 8n−8
3 −n−4 + 9n−8
> 1
4
∞
X
n=1
1
n
We see that the series is divergent by comparison to the harmonic series.
18.
∞
X
n=1
1
n −
1
n + 1

=
∞
X
n=1
1
n2 + n <
∞
X
n=1
1
n2
The series converges by the comparison test.
19.
∞
X
n=1
cos(nπ)
n
=
∞
X
n=1
(−1)n
n
We recognize this as the alternating harmonic series, which is conditionally convergent.
20.
∞
X
n=2
ln n
n11/10
Since this is a series of positive, monotone decreasing terms, the sum converges or diverges with the integral,
Z ∞
2
ln x
x11/10 dx =
Z ∞
ln 2
y e−y/10 dy
Since the integral is convergent, so is the series.
571

Solution 12.3
1.
∞
X
n=0
zn
(z + 3)n
2.
∞
X
n=2
Log z
ln n
3.
∞
X
n=1
z
n
4.
∞
X
n=1
(z + 2)2
n2
5.
∞
X
n=1
(z −e)n
nn
6.
∞
X
n=1
z2n
2nz
7.
∞
X
n=0
zn!
(n!)2
8.
∞
X
n=0
zln(n!)
n!
9.
∞
X
n=0
(z −π)2n+1nπ
n!
10.
∞
X
n=0
ln n
zn
572

Solution 12.4
∞
X
n=1
(−1)n+1
n
=
∞
X
n=1

1
2n −1 −1
2n

=
∞
X
n=1
1
(2n −1)(2n)
<
∞
X
n=1
1
(2n −1)2
< 1
2
∞
X
n=1
1
n2
= π2
12
Thus the series is convergent.
Solution 12.5
Since
|S2n −Sn| =

2n−1
X
j=n
1
j

≥
2n−1
X
j=n
1
2n −1
=
n
2n −1
> 1
2
the series does not satisfy the Cauchy convergence criterion.
573

Solution 12.6
The alternating harmonic series is conditionally convergent. That is, the sum is convergent but not absolutely conver-
gent. Let {an} and {bn} be the positive and negative terms in the sum, respectively, ordered in decreasing magnitude.
Note that both P∞
n=1 an and P∞
n=1 bn are divergent. Otherwise the alternating harmonic series would be absolutely
convergent.
To sum the terms in the series to π we repeat the following two steps indeﬁnitely:
1. Take terms from {an} until the sum is greater than π.
2. Take terms from {bn} until the sum is less than π.
Each of these steps can always be accomplished because the sums, P∞
n=1 an and P∞
n=1 bn are both divergent. Hence the
tails of the series are divergent. No matter how many terms we take, the remaining terms in each series are divergent.
In each step a ﬁnite, nonzero number of terms from the respective series is taken. Thus all the terms will be used.
Since the terms in each series vanish as n →∞, the running sum converges to π.
Solution 12.7
Applying the ratio test,
lim
n→∞

an+1
an
 = lim
n→∞
(n + 1)!nn
n!(n + 1)(n+1)
= lim
n→∞
nn
(n + 1)n
= lim
n→∞

n
(n + 1)
n
= 1
e
< 1,
we see that the series is absolutely convergent.
574

Solution 12.8
The harmonic series,
∞
X
n=1
1
nα = 1 + 1
2α + 1
3α + · · · ,
converges or diverges absolutely with the integral,
Z ∞
1
1
|xα| dx =
Z ∞
1
1
xℜ(α) dx =
(
[ln x]∞
1
for ℜ(α) = 1,
h
x1−ℜ(α)
1−ℜ(α)
i∞
1
for ℜ(α) ̸= 1.
The integral converges only for ℜ(α) > 1. Thus the harmonic series converges absolutely for ℜ(α) > 1 and diverges
absolutely for ℜ(α) ≤1.
575

Solution 12.9
N−1
X
n=1
sin(nx) =
N−1
X
n=0
sin(nx)
=
N−1
X
n=0
ℑ(eınx)
= ℑ
 N−1
X
n=0
(eıx)n
!
=
(
ℑ(N)
for x = 2πk
ℑ
  1−eınx
1−eıx

for x ̸= 2πk
=
(
0
for x = 2πk
ℑ

e−ıx/2 −eı(N−1/2)x
e−ıx/2 −eıx/2

for x ̸= 2πk
=
(
0
for x = 2πk
ℑ

e−ıx/2 −eı(N−1/2)x
−ı2 sin(x/2)

for x ̸= 2πk
=
(
0
for x = 2πk
ℜ

e−ıx/2 −eı(N−1/2)x
2 sin(x/2)

for x ̸= 2πk
N−1
X
n=1
sin(nx) =
(
0
for x = 2πk
cos(x/2)−cos((N−1/2)x)
2 sin(x/2)
for x ̸= 2πk
576

Solution 12.10
The geometric series is
1
1 −z =
∞
X
n=0
zn.
This series is uniformly convergent in the domain, |z| ≤r < 1. Diﬀerentiating this equation yields,
1
(1 −z)2 =
∞
X
n=1
nzn−1
=
∞
X
n=0
(n + 1)zn
for |z| < 1.
Integrating the geometric series yields
−log(1 −z) =
∞
X
n=0
zn+1
n + 1
log(1 −z) = −
∞
X
n=1
zn
n ,
for |z| < 1.
Solution 12.11
1
1 + z2 =
∞
X
n=0
 −z2n =
∞
X
n=0
(−1)nz2n
The function
1
1+z2 =
1
(1−ız)(1+ız) has singularities at z = ±ı. Thus the radius of convergence is 1. Now we use the ratio
577

test to corroborate that the radius of convergence is 1.
lim
n→∞

an+1(z)
an(z)
 < 1
lim
n→∞

(−1)n+1z2(n+1)
(−1)nz2n
 < 1
lim
n→∞
z2 < 1
|z| < 1
Solution 12.12
Method 1.
log(1 + z) = [log(1 + z)]z=0 +
 d
dz log(1 + z)

z=0
z
1! +
 d2
dz2 log(1 + z)

z=0
z2
2! + · · ·
= 0 +

1
1 + z

z=0
z
1! +

−1
(1 + z)2

z=0
z2
2! +

2
(1 + z)3

z=0
z3
3! + · · ·
= z −z2
2 + z3
3 −z4
4 + · · ·
=
∞
X
n=1
(−1)n+1zn
n
Since the nearest singularity of log(1 + z) is at z = −1, the radius of convergence is 1.
Method 2. We know the geometric series converges for |z| < 1.
1
1 + z =
∞
X
n=0
(−1)nzn
We integrate this equation to get the series for log(1 + z) in the domain |z| < 1.
log(1 + z) =
∞
X
n=0
(−1)n zn+1
n + 1 =
∞
X
n=1
(−1)n+1zn
n
578

We calculate the radius of convergence with the ratio test.
R = lim
n→∞

an
an+1
 = lim
n→∞

−(n + 1)
n
 = 1
Thus the series converges absolutely for |z| < 1.
Solution 12.13
For |z| < 1:
1
z −ı =
ı
1 + ız
= ı
∞
X
n=0
(−ız)n
(Note that |z| < 1 ⇔| −ız| < 1.)
For |z| > 1:
1
z −ı = 1
z
1
(1 −ı/z)
(Note that |z| > 1 ⇔| −ı/z| < 1.)
= 1
z
∞
X
n=0
 ı
z
n
= 1
z
0
X
n=−∞
ı−nzn
=
0
X
n=−∞
(−ı)nzn−1
=
−1
X
n=−∞
(−ı)n+1zn
579

Solution 12.14
Let
Sn =
n
X
k=1
kzk.
Sn −zSn =
n
X
k=1
kzk −
n
X
k=1
kzk+1
=
n
X
k=1
kzk −
n+1
X
k=2
(k −1)zk
=
n
X
k=1
zk −nzn+1
= z −zn+1
1 −z
−nzn+1
n
X
k=1
kzk = z(1 −(n + 1)zn + nzn+1)
(1 −z)2
Let
Sn =
n
X
k=1
k2zk.
Sn −zSn =
n
X
k=1
(k2 −(k −1)2)zk −n2zn+1
= 2
n
X
k=1
kzk −
n
X
k=1
zk −n2zn+1
= 2z(1 −(n + 1)zn + nzn+1)
(1 −z)2
−z −zn+1
1 −z
−n2zn+1
580

n
X
k=1
k2zk = z(1 + z −zn(1 + z + n(n(z −1) −2)(z −1)))
(1 −z)3
Solution 12.15
1. We assume that β ̸= 0. We determine the radius of convergence with the ratio test.
R = lim
n→∞

an
an+1

= lim
n→∞

(α −β) · · · (α −(n −1)β)/n!
(α −β) · · · (α −nβ)/(n + 1)!

= lim
n→∞

n + 1
α −nβ

= 1
|β|
The series converges absolutely for |z| < 1/|β|.
2. By the ratio test formula, the radius of absolute convergence is
R = lim
n→∞

n/2n
(n + 1)/2n+1

= 2 lim
n→∞

n
n + 1

= 2
581

By the root test formula, the radius of absolute convergence is
R =
1
limn→∞
np
|n/2n|
=
2
limn→∞
n√n
= 2
The series converges absolutely for |z −ı| < 2.
3. We determine the radius of convergence with the Cauchy-Hadamard formula.
R =
1
lim sup
np
|an|
=
1
lim sup
np
|nn|
=
1
lim sup n
= 0
The series converges only for z = 0.
582

4. By the ratio test formula, the radius of absolute convergence is
R = lim
n→∞

n!/nn
(n + 1)!/(n + 1)n+1

= lim
n→∞

(n + 1)n
nn

= lim
n→∞
n + 1
n
n
= exp

lim
n→∞ln
n + 1
n
n
= exp

lim
n→∞n ln
n + 1
n

= exp

lim
n→∞
ln(n + 1) −ln(n)
1/n

= exp

lim
n→∞
1/(n + 1) −1/n
−1/n2

= exp

lim
n→∞
n
n + 1

= e1
The series converges absolutely in the circle, |z| < e.
5. By the Cauchy-Hadamard formula, the radius of absolute convergence is
R =
1
lim sup
np
| (3 + (−1)n)n |
=
1
lim sup (3 + (−1)n)
= 1
4
583

Thus the series converges absolutely for |z| < 1/4.
6. By the Cauchy-Hadamard formula, the radius of absolute convergence is
R =
1
lim sup
np
|n + αn|
=
1
lim sup |α| np
|1 + n/αn|
= 1
|α|
Thus the sum converges absolutely for |z| < 1/|α|.
Solution 12.16
The Taylor series expansion of f(z) about z = 0 is
f(z) =
∞
X
n=0
f (n)(0)
n!
zn.
The derivatives of f(z) are
f (n)(z) =
 n−1
Y
k=0
(α −k)
!
(1 + z)α−n.
Thus f (n)(0) is
f (n)(0) =
n−1
Y
k=0
(α −k).
If α = m is a non-negative integer, then only the ﬁrst m + 1 terms are nonzero. The Taylor series is a polynomial and
the series has an inﬁnite radius of convergence.
(1 + z)m =
m
X
n=0
Qn−1
k=0(α −k)
n!
zn
584

If α is not a non-negative integer, then all of the terms in the series are non-zero.
(1 + z)α =
∞
X
n=0
Qn−1
k=0(α −k)
n!
zn
The radius of convergence of the series is the distance to the nearest singularity of (1 + z)α. This occurs at z = −1.
Thus the series converges for |z| < 1. We can corroborate this with the ratio test. The radius of convergence is
R = lim
n→∞

 Qn−1
k=0(α −k)

/n!
(Qn
k=0(α −k)) /(n + 1)!
 = lim
n→∞

n + 1
α −n
 = 1.
If we use the binomial coeﬃcient, we can write the series in a compact form.
α
n

≡
Qn−1
k=0(α −k)
n!
(1 + z)α =
∞
X
n=0
α
n

zn
Solution 12.17
We expand the function in partial fractions.
f(z) =
1
(z + 1)(z + 2) =
1
z + 1 −
1
z + 2
The Taylor series about z = 0 for 1/(z + 1) is
1
1 + z =
1
1 −(−z)
=
∞
X
n=0
(−z)n,
for |z| < 1
=
∞
X
n=0
(−1)nzn,
for |z| < 1
585

The series about z = ∞for 1/(z + 1) is
1
1 + z =
1/z
1 + 1/z
= 1
z
∞
X
n=0
(−1/z)n,
for |1/z| < 1
=
∞
X
n=0
(−1)nz−n−1,
for |z| > 1
=
−1
X
n=−∞
(−1)n+1zn,
for |z| > 1
The Taylor series about z = 0 for 1/(z + 2) is
1
2 + z =
1/2
1 + z/2
= 1
2
∞
X
n=0
(−z/2)n,
for |z/2| < 1
=
∞
X
n=0
(−1)n
2n+1 zn,
for |z| < 2
586

The series about z = ∞for 1/(z + 2) is
1
2 + z =
1/z
1 + 2/z
= 1
z
∞
X
n=0
(−2/z)n,
for |2/z| < 1
=
∞
X
n=0
(−1)n2nz−n−1,
for |z| > 2
=
−1
X
n=−∞
(−1)n+1
2n+1
zn,
for |z| > 2
To ﬁnd the expansions in the three regions, we just choose the appropriate series.
1.
f(z) =
1
1 + z −
1
2 + z
=
∞
X
n=0
(−1)nzn −
∞
X
n=0
(−1)n
2n+1 zn,
for |z| < 1
=
∞
X
n=0
(−1)n

1 −
1
2n+1

zn,
for |z| < 1
f(z) =
∞
X
n=0
(−1)n2n+1 −1
2n+1
zn,
for |z| < 1
587

2.
f(z) =
1
1 + z −
1
2 + z
f(z) =
−1
X
n=−∞
(−1)n+1zn −
∞
X
n=0
(−1)n
2n+1 zn,
for 1 < |z| < 2
3.
f(z) =
1
1 + z −
1
2 + z
=
−1
X
n=−∞
(−1)n+1zn −
−1
X
n=−∞
(−1)n+1
2n+1
zn,
for 2 < |z|
f(z) =
−1
X
n=−∞
(−1)n+12n+1 −1
2n+1
zn,
for 2 < |z|
Solution 12.18
Laurent Series. We assume that m is a non-negative integer and that n is an integer. The Laurent series about the
point z = 0 of
f(z) =

z + 1
z
m
is
f(z) =
∞
X
n=−∞
anzn
where
an =
1
ı2π
I
C
f(z)
zn+1 dz
588

and C is a contour going around the origin once in the positive direction. We manipulate the coeﬃcient integral into
the desired form.
an =
1
ı2π
I
C
(z + 1/z)m
zn+1
dz
=
1
ı2π
Z 2π
0
(eıθ + e−ıθ)m
eı(n+1)θ
ı eıθ dθ
= 1
2π
Z 2π
0
2m cosm θ e−ınθ dθ
= 2m−1
π
Z 2π
0
cosm θ(cos(nθ) −ı sin(nθ)) dθ
Note that cosm θ is even and sin(nθ) is odd about θ = π.
= 2m−1
π
Z 2π
0
cosm θ cos(nθ) dθ
Binomial Series. Now we ﬁnd the binomial series expansion of f(z).

z + 1
z
m
=
m
X
n=0
m
n

zm−n
1
z
n
=
m
X
n=0
m
n

zm−2n
=
m
X
n=−m
m−n even

m
(m −n)/2

zn
589

The coeﬃcients in the series f(z) = P∞
n=−∞anzn are
an =
( m
(m−n)/2

−m ≤n ≤m and m −n even
0
otherwise
By equating the coeﬃcients found by the two methods, we evaluate the desired integral.
Z 2π
0
(cos θ)m cos(nθ) dθ =
(
π
2m−1
 m
(m−n)/2

−m ≤n ≤m and m −n even
0
otherwise
Solution 12.19
First we write f(z) in the form
f(z) =
g(z)
(z −ı/2)(z −2)2.
g(z) is an entire function which grows no faster that z3 at inﬁnity. By expanding g(z) in a Taylor series about the
origin, we see that it is a polynomial of degree no greater than 3.
f(z) = αz3 + βz2 + γz + δ
(z −ı/2)(z −2)2
Since f(z) is a rational function we expand it in partial fractions to obtain a form that is convenient to integrate.
f(z) =
a
z −ı/2 +
b
z −2 +
c
(z −2)2 + d
We use the value of the integrals of f(z) to determine the constants, a, b, c and d.
I
|z|=1

a
z −ı/2 +
b
z −2 +
c
(z −2)2 + d

dz = ı2π
ı2πa = ı2π
a = 1
590

I
|z|=3

1
z −ı/2 +
b
z −2 +
c
(z −2)2 + d

dz = 0
ı2π(1 + b) = 0
b = −1
Note that by applying the second constraint, we can change the third constraint to
I
|z|=3
zf(z) dz = 0.
I
|z|=3
z

1
z −ı/2 −
1
z −2 +
c
(z −2)2 + d

dz = 0
I
|z|=3
(z −ı/2) + ı/2
z −ı/2
−(z −2) + 2
z −2
+ c(z −2) + 2c
(z −2)2

dz = 0
ı2π
 ı
2 −2 + c

= 0
c = 2 −ı
2
Thus we see that the function is
f(z) =
1
z −ı/2 −
1
z −2 + 2 −ı/2
(z −2)2 + d,
where d is an arbitrary constant. We can also write the function in the form:
f(z) =
dz3 + 15 −ı8
4(z −ı/2)(z −2)2.
Complete Laurent Series. We ﬁnd the complete Laurent series about z = 0 for each of the terms in the partial
591

fraction expansion of f(z).
1
z −ı/2 =
ı2
1 + ı2z
= ı2
∞
X
n=0
(−ı2z)n,
for | −ı2z| < 1
= −
∞
X
n=0
(−ı2)n+1zn,
for |z| < 1/2
1
z −ı/2 =
1/z
1 −ı/(2z)
= 1
z
∞
X
n=0
 ı
2z
n
,
for |ı/(2z)| < 1
=
∞
X
n=0
 ı
2
n
z−n−1,
for |z| < 2
=
−1
X
n=−∞
 ı
2
−n−1
zn,
for |z| < 2
=
−1
X
n=−∞
(−ı2)n+1zn,
for |z| < 2
592

−
1
z −2 =
1/2
1 −z/2
= 1
2
∞
X
n=0
z
2
n
,
for |z/2| < 1
=
∞
X
n=0
zn
2n+1,
for |z| < 2
−
1
z −2 = −
1/z
1 −2/z
= −1
z
∞
X
n=0
2
z
n
,
for |2/z| < 1
= −
∞
X
n=0
2nz−n−1,
for |z| > 2
= −
−1
X
n=−∞
2−n−1zn,
for |z| > 2
593

2 −ı/2
(z −2)2 = (2 −ı/2)1
4(1 −z/2)−2
= 4 −ı
8
∞
X
n=0
−2
n
 
−z
2
n
,
for |z/2| < 1
= 4 −ı
8
∞
X
n=0
(−1)n(n + 1)(−1)n2−nzn,
for |z| < 2
= 4 −ı
8
∞
X
n=0
n + 1
2n
zn,
for |z| < 2
2 −ı/2
(z −2)2 = 2 −ı/2
z2

1 −2
z
−2
= 2 −ı/2
z2
∞
X
n=0
−2
n
 
−2
z
n
,
for |2/z| < 1
= (2 −ı/2)
∞
X
n=0
(−1)n(n + 1)(−1)n2nz−n−2,
for |z| > 2
= (2 −ı/2)
−2
X
n=−∞
(−n −1)2−n−2zn,
for |z| > 2
= −(2 −ı/2)
−2
X
n=−∞
n + 1
2n+2 zn,
for |z| > 2
We take the appropriate combination of these series to ﬁnd the Laurent series expansions in the regions: |z| < 1/2,
594

1/2 < |z| < 2 and 2 < |z|. For |z| < 1/2, we have
f(z) = −
∞
X
n=0
(−ı2)n+1zn +
∞
X
n=0
zn
2n+1 + 4 −ı
8
∞
X
n=0
n + 1
2n
zn + d
f(z) =
∞
X
n=0

−(−ı2)n+1 +
1
2n+1 + 4 −ı
8
n + 1
2n

zn + d
f(z) =
∞
X
n=0

−(−ı2)n+1 +
1
2n+1

1 + 4 −ı
4
(n + 1)

zn + d,
for |z| < 1/2
For 1/2 < |z| < 2, we have
f(z) =
−1
X
n=−∞
(−ı2)n+1zn +
∞
X
n=0
zn
2n+1 + 4 −ı
8
∞
X
n=0
n + 1
2n
zn + d
f(z) =
−1
X
n=−∞
(−ı2)n+1zn +
∞
X
n=0
 1
2n+1

1 + 4 −ı
4
(n + 1)

zn + d,
for 1/2 < |z| < 2
For 2 < |z|, we have
f(z) =
−1
X
n=−∞
(−ı2)n+1zn −
−1
X
n=−∞
2−n−1zn −(2 −ı/2)
−2
X
n=−∞
n + 1
2n+2 zn + d
f(z) =
−2
X
n=−∞

(−ı2)n+1 −
1
2n+1 (1 + (1 −ı/4)(n + 1))

zn + d,
for 2 < |z|
Solution 12.20
The radius of convergence of the series for f(z) is
R = lim
n→∞

k3/3k
(k + 1)3/3k+1
 = 3 lim
n→∞

k3
(k + 1)3
 = 3.
595

Thus f(z) is a function which is analytic inside the circle of radius 3.
1. The integrand is analytic. Thus by Cauchy’s theorem the value of the integral is zero.
I
|z|=1
eız f(z) dz = 0
2. We use Cauchy’s integral formula to evaluate the integral.
I
|z|=1
f(z)
z4 dz = ı2π
3! f (3)(0) = ı2π
3!
3!33
33
= ı2π
I
|z|=1
f(z)
z4 dz = ı2π
3. We use Cauchy’s integral formula to evaluate the integral.
I
|z|=1
f(z) ez
z2
dz = ı2π
1!
d
dz(f(z) ez)

z=0 = ı2π1!13
31
I
|z|=1
f(z) ez
z2
dz = ı2π
3
Solution 12.21
1. We ﬁnd the series for 1/z by writing it in terms of z −1 and using the geometric series.
1
z =
1
1 + (z −1)
1
z =
∞
X
n=0
(−1)n(z −1)n
for |z −1| < 1
596

Since the nearest singularity is at z = 0, the radius of convergence is 1. The series converges absolutely for
|z −1| < 1. We could also determine the radius of convergence with the Cauchy-Hadamard formula.
R =
1
lim sup
np
|an|
=
1
lim sup
np
|(−1)n|
= 1
2. We integrate 1/ζ from 1 to z for in the circle |z −1| < 1.
Z z
1
1
ζ dζ = [Log ζ]z
1 = Log z
The series we derived for 1/z is uniformly convergent for |z −1| ≤r < 1. We can integrate the series in this
domain.
Log z =
Z z
1
∞
X
n=0
(−1)n(ζ −1)n dζ
=
∞
X
n=0
(−1)n
Z z
1
(ζ −1)n dζ
=
∞
X
n=0
(−1)n(z −1)n+1
n + 1
Log z =
∞
X
n=1
(−1)n−1(z −1)n
n
for |z −1| < 1
597

3. The series we derived for 1/z is uniformly convergent for |z −1| ≤r < 1. We can diﬀerentiate the series in this
domain.
1
z2 = −d
dz
1
z
= −d
dz
∞
X
n=0
(−1)n(z −1)n
=
∞
X
n=1
(−1)n+1n(z −1)n−1
1
z2 =
∞
X
n=0
(−1)n(n + 1)(z −1)n
for |z −1| < 1
4. We integrate Log ζ from 1 to z for in the circle |z −1| < 1.
Z z
1
Log ζ dζ = [ζ Log ζ −ζ]z
1 = z Log z −z + 1
The series we derived for Log z is uniformly convergent for |z −1| ≤r < 1. We can integrate the series in this
domain.
z Log z −z = = −1 +
Z z
1
Log ζ dζ
= −1 +
Z z
1
∞
X
n=1
(−1)n−1(ζ −1)n
n
dζ
= −1 +
∞
X
n=1
(−1)n−1(z −1)n+1
n(n + 1)
z Log z −z = −1 +
∞
X
n=2
(−1)n(z −1)n
n(n −1)
for |z −1| < 1
598

Solution 12.22
We evaluate the derivatives of ez at z = 0. Then we use Taylor’s Theorem.
dn
dzn ez = ez
dn
dzn ez = ez

z=0
= 1
ez =
∞
X
n=0
zn
n!
Since the exponential function has no singularities in the ﬁnite complex plane, the radius of convergence is inﬁnite.
We ﬁnd the Taylor series for the cosine and sine by writing them in terms of the exponential function.
cos z = eız + e−ız
2
= 1
2
 ∞
X
n=0
(ız)n
n!
+
∞
X
n=0
(−ız)n
n!
!
=
∞
X
n=0
even n
(ız)n
n!
cos z =
∞
X
n=0
(−1)nz2n
(2n)!
599

sin z = eız −e−ız
ı2
= 1
ı2
 ∞
X
n=0
(ız)n
n!
−
∞
X
n=0
(−ız)n
n!
!
= −ı
∞
X
n=0
odd n
(ız)n
n!
sin z =
∞
X
n=0
(−1)nz2n+1
(2n + 1)!
Solution 12.23
cos z = −cos(z −π)
= −
∞
X
n=0
(−1)n(z −π)2n
(2n)!
=
∞
X
n=0
(−1)n+1(z −π)2n
(2n)!
sin z = −sin(z −π)
= −
∞
X
n=0
(−1)n(z −π)2n+1
(2n + 1)!
=
∞
X
n=0
(−1)n+1(z −π)2n+1
(2n + 1)!
600

Solution 12.24
CONTINUE
Solution 12.25
P an converges only if the partial sums, Sn, are a Cauchy sequence.
∀ϵ > 0 ∃N s.t. m, n > N ⇒|Sm −Sn| < ϵ,
In particular, we can consider m = n + 1.
∀ϵ > 0 ∃N s.t. n > N ⇒|Sn+1 −Sn| < ϵ
∀ϵ > 0 ∃N s.t. n > N ⇒|an+1| < ϵ
This means that limn→∞an = 0.
Solution 12.26
1.
∞
X
n=1
an = 1
2 + 1
6 + 1
12 + 1
20 + · · ·
We conjecture that the terms in the sum are rational functions of summation index. That is, an = 1/p(n) where
p(n) is a polynomial. We use divided diﬀerences to determine the order of the polynomial.
2
6
12
20
4
6
8
2
2
We see that the polynomial is second order. p(n) = an2 + bn + c. We solve for the coeﬃcients.
a + b + c = 2
4a + 2b + c = 6
9a + 3b + c = 12
601

p(n) = n2 + n
We examine the ﬁrst few partial sums.
S1 = 1
2
S2 = 2
3
S3 = 3
4
S4 = 4
5
We conjecture that Sn = n/(n+1). We prove this with induction. The base case is n = 1. S1 = 1/(1+1) = 1/2.
Now we assume the induction hypothesis and calculate Sn+1.
Sn+1 = Sn + an+1
=
n
n + 1 +
1
(n + 1)2 + (n + 1)
= n + 1
n + 2
This proves the induction hypothesis. We calculate the limit of the partial sums to evaluate the series.
∞
X
n=1
1
n2 + n = lim
n→∞
n
n + 1
∞
X
n=1
1
n2 + n = 1
2.
∞
X
n=0
(−1)n = 1 + (−1) + 1 + (−1) + · · ·
602

Since the terms in the series do not vanish as n →∞, the series is divergent.
3. We can directly sum this geometric series.
∞
X
n=1
1
2n−1
1
3n
1
5n+1 = 1
75
1
1 −1/30 =
2
145
CONTINUE
Solution 12.27
The innermost sum is a geometric series.
∞
X
kn=kn−1
1
2kn =
1
2kn−1
1
1 −1/2 = 21−kn−1
This gives us a relationship between n nested sums and n −1 nested sums.
∞
X
k1=0
∞
X
k2=k1
· · ·
∞
X
kn=kn−1
1
2kn = 2
∞
X
k1=0
∞
X
k2=k1
· · ·
∞
X
kn−1=kn−2
1
2kn−1
We evaluate the n nested sums by induction.
∞
X
k1=0
∞
X
k2=k1
· · ·
∞
X
kn=kn−1
1
2kn = 2n
Solution 12.28
1.
(a)
f(z) = e−z
f(0) = 1
f ′(0) = −1
f ′′(0) = 1
603

e−z = 1 −z + z2
2 + O
 z3
Since e−z is entire, the Taylor series converges in the complex plane.
(b)
f(z) = 1 + z
1 −z,
f(ı) = ı
f ′(z) =
2
(1 −z)2,
f ′(ı) = ı
f ′′(z) =
4
(1 −z)3,
f ′′(ı) = −1 + ı
1 + z
1 −z = ı + ı(z −ı) + −1 + ı
2
(z −ı)2 + O
 (z −ı)3
Since the nearest singularity, (at z = 1), is a distance of
√
2 from z0 = ı, the radius of convergence is
√
2.
The series converges absolutely for |z −ı| <
√
2.
(c)
ez
z −1 = −

1 + z + z2
2 + O
 z3  1 + z + z2 + O
 z3
= −1 −2z −5
2z2 + O
 z3
Since the nearest singularity, (at z = 1), is a distance of 1 from z0 = 0, the radius of convergence is 1. The
series converges absolutely for |z| < 1.
2. Since f(z) is analytic in |z −z0| < R, its Taylor series converges absolutely on this domain.
f(z) =
∞
X
n=0
f (n)(z0)zn
n!
604

The Taylor series converges uniformly on any closed sub-domain of |z −z0| < R. We consider the sub-domain
|z −z0| ≤ρ < R. On the domain of uniform convergence we can interchange diﬀerentiation and summation.
f ′(z) = d
dz
∞
X
n=0
f (n)(z0)zn
n!
f ′(z) =
∞
X
n=1
nf (n)(z0)zn−1
n!
f ′(z) =
∞
X
n=0
f (n+1)(z0)zn
n!
Note that this is the Taylor series that we could obtain directly for f ′(z). Since f(z) is analytic on |z −z0| < R
so is f ′(z).
f ′(z) =
∞
X
n=0
f (n+1)(z0)zn
n!
3.
1
(1 −z)3 = d2
dz2
1
2
1
1 −z
= 1
2
d2
dz2
∞
X
n=0
zn
= 1
2
∞
X
n=2
n(n −1)zn−2
= 1
2
∞
X
n=0
(n + 2)(n + 1)zn
The radius of convergence is 1, which is the distance to the nearest singularity at z = 1.
605

4. The Taylor series expansion of f(z) about z = 0 is
f(z) =
∞
X
n=0
f (n)(0)
n!
zn.
We compute the derivatives of f(z).
f (n)(z) =
 n−1
Y
k=0
(ı −k)
!
(1 + z)ı−n.
Now we determine the coeﬃcients in the series.
f (n)(0) =
n−1
Y
k=0
(ı −k)
(1 + z)ı =
∞
X
n=0
Qn−1
k=0(ı −k)
n!
zn
The radius of convergence of the series is the distance to the nearest singularity of (1 + z)ı. This occurs at
z = −1. Thus the series converges for |z| < 1. We can corroborate this with the ratio test. We compute the
radius of convergence.
R = lim
n→∞

 Qn−1
k=0(ı −k)

/n!
(Qn
k=0(ı −k)) /(n + 1)!
 = lim
n→∞

n + 1
ı −n
 = 1
If we use the binomial coeﬃcient,
α
n

≡
Qn−1
k=0(α −k)
n!
,
then we can write the series in a compact form.
(1 + z)ı =
∞
X
n=0
ı
n

zn
606

Solution 12.29
1.
∞
X
k=0
kzk
We determine the radius of convergence with the ratio formula.
R = lim
k→∞

k
k + 1

= lim
k→∞
1
1
= 1
The series converges absolutely for |z| < 1.
2.
∞
X
k=1
kkzk
We determine the radius of convergence with the Cauchy-Hadamard formula.
R =
1
lim sup
kp
|kk|
=
1
lim sup k
= 0
The series converges only for z = 0.
3.
∞
X
k=1
k!
kk zk
607

We determine the radius of convergence with the ratio formula.
R = lim
k→∞

k!/kk
(k + 1)!/(k + 1)(k+1)

= lim
k→∞
(k + 1)k
kk
= exp

lim
k→∞k ln
k + 1
k

= exp

lim
k→∞
ln(k + 1) −ln(k)
1/k

= exp

lim
k→∞
1/(k + 1) −1/k
−1/k2

= exp

lim
k→∞
k
k + 1

= exp(1)
= e
The series converges absolutely for |z| < e.
4.
∞
X
k=0
(z + ı5)2k(k + 1)2
608

We use the ratio formula to determine the domain of convergence.
lim
k→∞

(z + ı5)2(k+1)(k + 2)2
(z + ı5)2k(k + 1)2
 < 1
|z + ı5|2 lim
k→∞

(k + 2)2
(k + 1)2
 < 1
|z + ı5|2 lim
k→∞
2(k + 2)
2(k + 1) < 1
|z + ı5|2 lim
k→∞
2
2 < 1
|z + ı5|2 < 1
5.
∞
X
k=0
(k + 2k)zk
We determine the radius of convergence with the Cauchy-Hadamard formula.
R =
1
lim sup
kp
|k + 2k|
=
1
lim sup 2 kp
|1 + k/2k|
= 1
2
The series converges for |z| < 1/2.
609

Solution 12.30
1.
(a)
1
z(1 −z) = 1
z +
1
1 −z
= 1
z +
∞
X
n=0
zn,
for 0 < |z| < 1
= 1
z +
∞
X
n=−1
zn,
for 0 < |z| < 1
(b)
1
z(1 −z) = 1
z +
1
1 −z
= 1
z −1
z
1
1 −1/z
= 1
z −1
z
∞
X
n=0
1
z
n
,
for |z| > 1
= −1
z
∞
X
n=1
z−n,
for |z| > 1
= −
−∞
X
n=−2
zn,
for |z| > 1
610

(c)
1
z(1 −z) = 1
z +
1
1 −z
=
1
(z + 1) −1 +
1
2 −(z + 1)
=
1
(z + 1)
1
1 −1/(z + 1) −
1
(z + 1)
1
1 −2/(z + 1),
for |z + 1| > 1 and |z + 1| > 2
=
1
(z + 1)
∞
X
n=0
1
(z + 1)n −
1
(z + 1)
∞
X
n=0
2n
(z + 1)n,
for |z + 1| > 1 and |z + 1| > 2
=
1
(z + 1)
∞
X
n=0
1 −2n
(z + 1)n,
for |z + 1| > 2
=
∞
X
n=1
1 −2n
(z + 1)n+1,
for |z + 1| > 2
=
−∞
X
n=−2
 1 −2−n−1
(z + 1)n,
for |z + 1| > 2
2. First we factor the denominator of f(z) = 1/(z4 + 4).
z4 + 4 = (z −1 −ı)(z −1 + ı)(z + 1 −ı)(z + 1 + ı)
We look for an annulus about z = 1 containing the point z = ı where f(z) is analytic. The singularities at
z = 1 ± ı are a distance of 1 from z = 1; the singularities at z = −1 ± ı are at a distance of
√
5. Since f(z) is
analytic in the domain 1 < |z −1| <
√
5 there is a convergent Laurent series in that domain.
Solution 12.31
1.
(a) We factor the denominator to see that there are ﬁrst order poles at z = ±ı.
z
z2 + 1 =
z
(z −ı)(z + ı)
611

Since the function behaves like 1/z at inﬁnity, it is analytic there.
(b) The denominator of 1/ sin z has ﬁrst order zeros at z = nπ, n ∈Z. Thus the function has ﬁrst order poles
at these locations. Now we examine the point at inﬁnity with the change of variables z = 1/ζ.
1
sin z =
1
sin(1/ζ) =
ı2
eı/ζ −e−ı/ζ
We see that the point at inﬁnity is a singularity of the function. Since the denominator grows exponentially,
there is no multiplicative factor of ζn that will make the function analytic at ζ = 0. We conclude that the
point at inﬁnity is an essential singularity. Since there is no deleted neighborhood of the point at inﬁnity
that does contain ﬁrst order poles at the locations z = nπ, the point at inﬁnity is a non-isolated singularity.
(c)
log
 1 + z2
= log(z + ı) + log(z −ı)
There are branch points at z = ±ı. Since the argument of the logarithm is unbounded as z →∞there is
a branch point at inﬁnity as well. Branch points are non-isolated singularities.
(d)
z sin(1/z) = 1
2z
 eı/z + eı/z
The point z = 0 is a singularity. Since the function grows exponentially at z = 0. There is no multiplicative
factor of zn that will make the function analytic. Thus z = 0 is an essential singularity.
There are no other singularities in the ﬁnite complex plane. We examine the point at inﬁnity.
z sin
1
z

= 1
ζ sin ζ
The point at inﬁnity is a singularity.
We take the limit ζ →0 to demonstrate that it is a removable
singularity.
lim
ζ→0
sin ζ
ζ
= lim
ζ→0
cos ζ
1
= 1
612

(e)
tan−1(z)
z sinh2(πz) =
ı log
  ı+z
ı−z

2z sinh2(πz)
There are branch points at z = ±ı due to the logarithm. These are non-isolated singularities. Note that
sinh(z) has ﬁrst order zeros at z = ınπ, n ∈Z. The arctangent has a ﬁrst order zero at z = 0. Thus there
is a second order pole at z = 0. There are second order poles at z = ın, n ∈Z \ {0} due to the hyperbolic
sine. Since the hyperbolic sine has an essential singularity at inﬁnity, the function has an essential singularity
at inﬁnity as well. The point at inﬁnity is a non-isolated singularity because there is no neighborhood of
inﬁnity that does not contain second order poles.
2.
(a) (z −ı) e1/(z−1) has a simple zero at z = ı and an isolated essential singularity at z = 1.
(b)
sin(z −3)
(z −3)(z + ı)6
has a removable singularity at z = 3, a pole of order 6 at z = −ı and an essential singularity at z∞.
613

Chapter 13
The Residue Theorem
Man will occasionally stumble over the truth, but most of the time he will pick himself up and continue on.
- Winston Churchill
13.1
The Residue Theorem
We will ﬁnd that many integrals on closed contours may be evaluated in terms of the residues of a function. We ﬁrst
deﬁne residues and then prove the Residue Theorem.
614

Result 13.1.1 Residues. Let f(z) be single-valued an analytic in a deleted neighborhood
of z0. Then f(z) has the Laurent series expansion
f(z) =
∞
X
n=−∞
an(z −z0)n,
The residue of f(z) at z = z0 is the coeﬃcient of the
1
z−z0 term:
Res(f(z), z0) = a−1.
The residue at a branch point or non-isolated singularity is undeﬁned as the Laurent series
does not exist. If f(z) has a pole of order n at z = z0 then we can use the Residue Formula:
Res(f(z), z0) = lim
z→z0

1
(n −1)!
dn−1
dzn−1

(z −z0)nf(z)

.
See Exercise 13.4 for a proof of the Residue Formula.
Example 13.1.1 In Example 8.4.5 we showed that f(z) = z/ sin z has ﬁrst order poles at z = nπ, n ∈Z \ {0}. Now
615

we ﬁnd the residues at these isolated singularities.
Res
 z
sin z, z = nπ

= lim
z→nπ

(z −nπ)
z
sin z

= nπ lim
z→nπ
z −nπ
sin z
= nπ lim
z→nπ
1
cos z
= nπ
1
(−1)n
= (−1)nnπ
Residue Theorem.
We can evaluate many integrals in terms of the residues of a function. Suppose f(z) has only
one singularity, (at z = z0), inside the simple, closed, positively oriented contour C. f(z) has a convergent Laurent
series in some deleted disk about z0. We deform C to lie in the disk. See Figure 13.1. We now evaluate
R
C f(z) dz by
deforming the contour and using the Laurent series expansion of the function.
C
B
Figure 13.1: Deform the contour to lie in the deleted disk.
616

Z
C
f(z) dz =
Z
B
f(z) dz
=
Z
B
∞
X
n=−∞
an(z −z0)n dz
=
∞
X
n=−∞
n̸=−1
an
(z −z0)n+1
n + 1
r eı(θ+2π)
r eıθ
+ a−1 [log(z −z0)]r eı(θ+2π)
r eıθ
= a−1ı2π
Z
C
f(z) dz = ı2π Res(f(z), z0)
Now assume that f(z) has n singularities at {z1, . . . , zn}. We deform C to n contours C1, . . . , Cn which enclose the
singularities and lie in deleted disks about the singularities in which f(z) has convergent Laurent series. See Figure 13.2.
We evaluate
R
C f(z) dz by deforming the contour.
Z
C
f(z) dz =
n
X
k=1
Z
Ck
f(z) dz = ı2π
n
X
k=1
Res(f(z), zk)
Now instead let f(z) be analytic outside and on C except for isolated singularities at {ζn} in the domain outside C
and perhaps an isolated singularity at inﬁnity. Let a be any point in the interior of C. To evaluate
R
C f(z) dz we make
the change of variables ζ = 1/(z −a). This maps the contour C to C′. (Note that C′ is negatively oriented.) All
the points outside C are mapped to points inside C′ and vice versa. We can then evaluate the integral in terms of the
singularities inside C′.
617

C
C
C
C1
2
3
Figure 13.2: Deform the contour n contours which enclose the n singularities.
I
C
f(z) dz =
I
C′ f
1
ζ + a
 −1
ζ2 dζ
=
I
−C′
1
z2f
1
z + a

dz
= ı2π
X
n
Res
 1
z2f
1
z + a

,
1
ζn −a

+ ı2π Res
 1
z2f
1
z + a

, 0

.
618

a
C
C’
Figure 13.3: The change of variables ζ = 1/(z −a).
Result 13.1.2 Residue Theorem.
If f(z) is analytic in a compact, closed, connected
domain D except for isolated singularities at {zn} in the interior of D then
Z
∂D
f(z) dz =
X
k
I
Ck
f(z) dz = ı2π
X
n
Res(f(z), zn).
Here the set of contours {Ck} make up the positively oriented boundary ∂D of the domain
D. If the boundary of the domain is a single contour C then the formula simpliﬁes.
I
C
f(z) dz = ı2π
X
n
Res(f(z), zn)
If instead f(z) is analytic outside and on C except for isolated singularities at {ζn} in the
domain outside C and perhaps an isolated singularity at inﬁnity then
I
C
f(z) dz = ı2π
X
n
Res
 1
z2f
1
z + a

,
1
ζn −a

+ ı2π Res
 1
z2f
1
z + a

, 0

.
Here a is a any point in the interior of C.
619

Example 13.1.2 Consider
1
ı2π
Z
C
sin z
z(z −1) dz
where C is the positively oriented circle of radius 2 centered at the origin. Since the integrand is single-valued with
only isolated singularities, the Residue Theorem applies. The value of the integral is the sum of the residues from
singularities inside the contour.
The only places that the integrand could have singularities are z = 0 and z = 1. Since
lim
z→0
sin z
z
= lim
z→0
cos z
1
= 1,
there is a removable singularity at the point z = 0. There is no residue at this point.
Now we consider the point z = 1. Since sin(z)/z is analytic and nonzero at z = 1, that point is a ﬁrst order pole
of the integrand. The residue there is
Res

sin z
z(z −1), z = 1

= lim
z→1(z −1)
sin z
z(z −1) = sin(1).
There is only one singular point with a residue inside the path of integration. The residue at this point is sin(1).
Thus the value of the integral is
1
ı2π
Z
C
sin z
z(z −1) dz = sin(1)
Example 13.1.3 Evaluate the integral
Z
C
cot z coth z
z3
dz
where C is the unit circle about the origin in the positive direction.
The integrand is
cot z coth z
z3
= cos z cosh z
z3 sin z sinh z
620

sin z has zeros at nπ. sinh z has zeros at ınπ. Thus the only pole inside the contour of integration is at z = 0. Since
sin z and sinh z both have simple zeros at z = 0,
sin z = z + O(z3),
sinh z = z + O(z3)
the integrand has a pole of order 5 at the origin. The residue at z = 0 is
lim
z→0
1
4!
d4
dz4

z5cot z coth z
z3

= lim
z→0
1
4!
d4
dz4
 z2 cot z coth z

= 1
4! lim
z→0

24 cot(z) coth(z)csc(z)2 −32z coth(z)csc(z)4
−16z cos(2z) coth(z)csc(z)4 + 22z2 cot(z) coth(z)csc(z)4
+ 2z2 cos(3z) coth(z)csc(z)5 + 24 cot(z) coth(z)csch(z)2
+ 24csc(z)2csch(z)2 −48z cot(z)csc(z)2csch(z)2
−48z coth(z)csc(z)2csch(z)2 + 24z2 cot(z) coth(z)csc(z)2csch(z)2
+ 16z2csc(z)4csch(z)2 + 8z2 cos(2z)csc(z)4csch(z)2
−32z cot(z)csch(z)4 −16z cosh(2z) cot(z)csch(z)4
+ 22z2 cot(z) coth(z)csch(z)4 + 16z2csc(z)2csch(z)4
+ 8z2 cosh(2z)csc(z)2csch(z)4 + 2z2 cosh(3z) cot(z)csch(z)5

= 1
4!

−56
15

= −7
45
621

Since taking the fourth derivative of z2 cot z coth z really sucks, we would like a more elegant way of ﬁnding the
residue. We expand the functions in the integrand in Taylor series about the origin.
cos z cosh z
z3 sin z sinh z =

1 −z2
2 + z4
24 −· · ·
 
1 + z2
2 + z4
24 + · · ·

z3  z −z3
6 + z5
120 −· · ·
  z + z3
6 + z5
120 + · · ·

=
1 −z4
6 + · · ·
z3  z2 + z6   −1
36 + 1
60

+ · · ·

= 1
z5
1 −z4
6 + · · ·
1 −z4
90 + · · ·
= 1
z5

1 −z4
6 + · · ·
 
1 + z4
90 + · · ·

= 1
z5

1 −7
45z4 + · · ·

= 1
z5 −7
45
1
z + · · ·
Thus we see that the residue is −7
45. Now we can evaluate the integral.
Z
C
cot z coth z
z3
dz = −ı14
45π
13.2
Cauchy Principal Value for Real Integrals
13.2.1
The Cauchy Principal Value
First we recap improper integrals. If f(x) has a singularity at x0 ∈(a . . . b) then
Z b
a
f(x) dx ≡lim
ϵ→0+
Z x0−ϵ
a
f(x) dx + lim
δ→0+
Z b
x0+δ
f(x) dx.
622

For integrals on (−∞. . . ∞),
Z ∞
−∞
f(x) dx ≡
lim
a→−∞, b→∞
Z b
a
f(x) dx.
Example 13.2.1
R 1
−1
1
x dx is divergent. We show this with the deﬁnition of improper integrals.
Z 1
−1
1
x dx = lim
ϵ→0+
Z −ϵ
−1
1
x dx + lim
δ→0+
Z 1
δ
1
x dx
= lim
ϵ→0+ [ln |x|]−ϵ
−1 + lim
δ→0+ [ln |x|]1
δ
= lim
ϵ→0+ ln ϵ −lim
δ→0+ ln δ
The integral diverges because ϵ and δ approach zero independently.
Since 1/x is an odd function, it appears that the area under the curve is zero. Consider what would happen if ϵ and
δ were not independent. If they approached zero symmetrically, δ = ϵ, then the value of the integral would be zero.
lim
ϵ→0+
Z −ϵ
−1
+
Z 1
ϵ
 1
x dx = lim
ϵ→0+(ln ϵ −ln ϵ) = 0
We could make the integral have any value we pleased by choosing δ = cϵ. 1
lim
ϵ→0+
Z −ϵ
−1
+
Z 1
cϵ
 1
x dx = lim
ϵ→0+(ln ϵ −ln(cϵ)) = −ln c
We have seen it is reasonable that
Z 1
−1
1
x dx
has some meaning, and if we could evaluate the integral, the most reasonable value would be zero. The Cauchy principal
value provides us with a way of evaluating such integrals. If f(x) is continuous on (a, b) except at the point x0 ∈(a, b)
1This may remind you of conditionally convergent series. You can rearrange the terms to make the series sum to any number.
623

then the Cauchy principal value of the integral is deﬁned
−
Z b
a
f(x) dx = lim
ϵ→0+
Z x0−ϵ
a
f(x) dx +
Z b
x0+ϵ
f(x) dx

.
The Cauchy principal value is obtained by approaching the singularity symmetrically. The principal value of the integral
may exist when the integral diverges. If the integral exists, it is equal to the principal value of the integral.
The Cauchy principal value of
R 1
−1
1
x dx is deﬁned
−
Z 1
−1
1
x dx ≡lim
ϵ→0+
Z −ϵ
−1
1
x dx +
Z 1
ϵ
1
x dx

= lim
ϵ→0+
 [log |x|]−ϵ
−1 [log |x|]1
ϵ

= lim
ϵ→0+ (log | −ϵ| −log |ϵ|)
= 0.
(Another notation for the principal value of an integral is PV
R
f(x) dx.) Since the limits of integration approach zero
symmetrically, the two halves of the integral cancel. If the limits of integration approached zero independently, (the
deﬁnition of the integral), then the two halves would both diverge.
Example 13.2.2
R ∞
−∞
x
x2+1 dx is divergent. We show this with the deﬁnition of improper integrals.
Z ∞
−∞
x
x2 + 1 dx =
lim
a→−∞, b→∞
Z b
a
x
x2 + 1 dx
=
lim
a→−∞, b→∞
1
2 ln(x2 + 1)
b
a
= 1
2
lim
a→−∞, b→∞ln
b2 + 1
a2 + 1

624

The integral diverges because a and b approach inﬁnity independently. Now consider what would happen if a and b
were not independent. If they approached zero symmetrically, a = −b, then the value of the integral would be zero.
1
2 lim
b→∞ln
b2 + 1
b2 + 1

= 0
We could make the integral have any value we pleased by choosing a = −cb.
We can assign a meaning to divergent integrals of the form
R ∞
−∞f(x) dx with the Cauchy principal value. The
Cauchy principal value of the integral is deﬁned
−
Z ∞
−∞
f(x) dx = lim
a→∞
Z a
−a
f(x) dx.
The Cauchy principal value is obtained by approaching inﬁnity symmetrically.
The Cauchy principal value of
R ∞
−∞
x
x2+1 dx is deﬁned
−
Z ∞
−∞
x
x2 + 1 dx = lim
a→∞
Z a
−a
x
x2 + 1 dx
= lim
a→∞
1
2 ln
 x2 + 1
a
−a
= 0.
625

Result 13.2.1 Cauchy Principal Value. If f(x) is continuous on (a, b) except at the point
x0 ∈(a, b) then the integral of f(x) is deﬁned
Z b
a
f(x) dx = lim
ϵ→0+
Z x0−ϵ
a
f(x) dx + lim
δ→0+
Z b
x0+δ
f(x) dx.
The Cauchy principal value of the integral is deﬁned
−
Z b
a
f(x) dx = lim
ϵ→0+
Z x0−ϵ
a
f(x) dx +
Z b
x0+ϵ
f(x) dx

.
If f(x) is continuous on (−∞, ∞) then the integral of f(x) is deﬁned
Z ∞
−∞
f(x) dx =
lim
a→−∞, b→∞
Z b
a
f(x) dx.
The Cauchy principal value of the integral is deﬁned
−
Z ∞
−∞
f(x) dx = lim
a→∞
Z a
−a
f(x) dx.
The principal value of the integral may exist when the integral diverges. If the integral exists,
it is equal to the principal value of the integral.
Example 13.2.3 Clearly
R ∞
−∞x dx diverges, however the Cauchy principal value exists.
−
Z ∞
−∞
x dx = lim
a→∞
x2
2

−a
a = 0
626

In general, if f(x) is an odd function with no singularities on the ﬁnite real axis then
−
Z ∞
−∞
f(x) dx = 0.
13.3
Cauchy Principal Value for Contour Integrals
Example 13.3.1 Consider the integral
Z
Cr
1
z −1 dz,
where Cr is the positively oriented circle of radius r and center at the origin. From the residue theorem, we know that
the integral is
Z
Cr
1
z −1 dz =
(
0
for r < 1,
ı2π
for r > 1.
When r = 1, the integral diverges, as there is a ﬁrst order pole on the path of integration. However, the principal value
of the integral exists.
−
Z
Cr
1
z −1 dz = lim
ϵ→0+
Z 2π−ϵ
ϵ
1
eıθ −1ıeıθ dθ
= lim
ϵ→0+

log(eıθ −1)
2π−ϵ
ϵ
627

We choose the branch of the logarithm with a branch cut on the positive real axis and arg log z ∈(0, 2π).
= lim
ϵ→0+
 log
 eı(2π−ϵ) −1

−log (eıϵ −1)

= lim
ϵ→0+
 log
  1 −iϵ + O(ϵ2)

−1

−log
  1 + iϵ + O(ϵ2)

−1

= lim
ϵ→0+
 log
 −iϵ + O(ϵ2)

−log
 iϵ + O(ϵ2)

= lim
ϵ→0+
 Log
 ϵ + O(ϵ2)

+ ı arg
 −ıϵ + O(ϵ2)

−Log
 ϵ + O(ϵ2)

−ı arg
 ıϵ + O(ϵ2)

= ı3π
2 −ıπ
2
= ıπ
Thus we obtain
−
Z
Cr
1
z −1 dz =





0
for r < 1,
ıπ
for r = 1,
ı2π
for r > 1.
In the above example we evaluated the contour integral by parameterizing the contour. This approach is only
feasible when the integrand is simple. We would like to use the residue theorem to more easily evaluate the principal
value of the integral. But before we do that, we will need a preliminary result.
Result 13.3.1 Let f(z) have a ﬁrst order pole at z = z0 and let (z −z0)f(z) be analytic in
some neighborhood of z0. Let the contour Cϵ be a circular arc from z0 + ϵeıα to z0 + ϵeıβ.
(We assume that β > α and β −α < 2π.)
lim
ϵ→0+
Z
Cϵ
f(z) dz = ı(β −α) Res(f(z), z0)
The contour is shown in Figure 13.4. (See Exercise 13.9 for a proof of this result.)
628

β−α
Cε
z0
ε
Figure 13.4: The Cϵ Contour
Example 13.3.2 Consider
−
Z
C
1
z −1 dz
where C is the unit circle. Let Cp be the circular arc of radius 1 that starts and ends a distance of ϵ from z = 1. Let
Cϵ be the positive, circular arc of radius ϵ with center at z = 1 that joins the endpoints of Cp. Let Ci, be the union of
Cp and Cϵ. (Cp stands for Principal value Contour; Ci stands for Indented Contour.) Ci is an indented contour that
avoids the ﬁrst order pole at z = 1. Figure 13.5 shows the three contours.
C
C
p
ε
Figure 13.5: The Indented Contour.
629

Note that the principal value of the integral is
−
Z
C
1
z −1 dz = lim
ϵ→0+
Z
Cp
1
z −1 dz.
We can calculate the integral along Ci with the residue theorem.
Z
Ci
1
z −1 dz = 2πi
We can calculate the integral along Cϵ using Result 13.3.1. Note that as ϵ →0+, the contour becomes a semi-circle,
a circular arc of π radians.
lim
ϵ→0+
Z
Cϵ
1
z −1 dz = ıπ Res

1
z −1, 1

= ıπ
Now we can write the principal value of the integral along C in terms of the two known integrals.
−
Z
C
1
z −1 dz =
Z
Ci
1
z −1 dz −
Z
Cϵ
1
z −1 dz
= ı2π −ıπ
= ıπ
In the previous example, we formed an indented contour that included the ﬁrst order pole. You can show that if we
had indented the contour to exclude the pole, we would obtain the same result. (See Exercise 13.11.)
We can extend the residue theorem to principal values of integrals. (See Exercise 13.10.)
630

Result 13.3.2 Residue Theorem for Principal Values. Let f(z) be analytic inside and
on a simple, closed, positive contour C, except for isolated singularities at z1, . . . , zm inside
the contour and ﬁrst order poles at ζ1, . . . , ζn on the contour. Further, let the contour be C1
at the locations of these ﬁrst order poles. (i.e., the contour does not have a corner at any of
the ﬁrst order poles.) Then the principal value of the integral of f(z) along C is
−
Z
C
f(z) dz = ı2π
m
X
j=1
Res(f(z), zj) + ıπ
n
X
j=1
Res(f(z), ζj).
13.4
Integrals on the Real Axis
Example 13.4.1 We wish to evaluate the integral
Z ∞
−∞
1
x2 + 1 dx.
We can evaluate this integral directly using calculus.
Z ∞
−∞
1
x2 + 1 dx = [arctan x]∞
−∞
= π
Now we will evaluate the integral using contour integration. Let CR be the semicircular arc from R to −R in the upper
half plane. Let C be the union of CR and the interval [−R, R].
We can evaluate the integral along C with the residue theorem. The integrand has ﬁrst order poles at z = ±ı. For
631

R > 1, we have
Z
C
1
z2 + 1 dz = ı2π Res

1
z2 + 1, ı

= ı2π 1
ı2
= π.
Now we examine the integral along CR. We use the maximum modulus integral bound to show that the value of the
integral vanishes as R →∞.

Z
CR
1
z2 + 1 dz
 ≤πR max
z∈CR

1
z2 + 1

= πR
1
R2 −1
→0
as R →∞.
Now we are prepared to evaluate the original real integral.
Z
C
1
z2 + 1 dz = π
Z R
−R
1
x2 + 1 dx +
Z
CR
1
z2 + 1 dz = π
We take the limit as R →∞.
Z ∞
−∞
1
x2 + 1 dx = π
We would get the same result by closing the path of integration in the lower half plane. Note that in this case the
closed contour would be in the negative direction.
632

If you are really observant, you may have noticed that we did something a little funny in evaluating
Z ∞
−∞
1
x2 + 1 dx.
The deﬁnition of this improper integral is
Z ∞
−∞
1
x2 + 1 dx = lim
a→+∞
Z 0
−a
1
x2 + 1 dx+ = lim
b→+∞
Z b
0
1
x2 + 1 dx.
In the above example we instead computed
lim
R→+∞
Z R
−R
1
x2 + 1 dx.
Note that for some integrands, the former and latter are not the same. Consider the integral of
x
x2+1.
Z ∞
−∞
x
x2 + 1 dx = lim
a→+∞
Z 0
−a
x
x2 + 1 dx + lim
b→+∞
Z b
0
x
x2 + 1 dx
= lim
a→+∞
1
2 log |a2 + 1|

+ lim
b→+∞

−1
2 log |b2 + 1|

Note that the limits do not exist and hence the integral diverges. We get a diﬀerent result if the limits of integration
approach inﬁnity symmetrically.
lim
R→+∞
Z R
−R
x
x2 + 1 dx =
lim
R→+∞
1
2(log |R2 + 1| −log |R2 + 1|)

= 0
(Note that the integrand is an odd function, so the integral from −R to R is zero.) We call this the principal value of
the integral and denote it by writing “PV” in front of the integral sign or putting a dash through the integral.
PV
Z ∞
−∞
f(x) dx ≡−
Z ∞
−∞
f(x) dx ≡
lim
R→+∞
Z R
−R
f(x) dx
633

The principal value of an integral may exist when the integral diverges. If the integral does converge, then it is
equal to its principal value.
We can use the method of Example 13.4.1 to evaluate the principal value of integrals of functions that vanish fast
enough at inﬁnity.
Result 13.4.1 Let f(z) be analytic except for isolated singularities, with only ﬁrst order poles
on the real axis. Let CR be the semi-circle from R to −R in the upper half plane. If
lim
R→∞

R max
z∈CR |f(z)|

= 0
then
−
Z ∞
−∞
f(x) dx = ı2π
m
X
k=1
Res (f(z), zk) + ıπ
n
X
k=1
Res(f(z), xk)
where z1, . . . zm are the singularities of f(z) in the upper half plane and x1, . . . , xn are the
ﬁrst order poles on the real axis.
Now let CR be the semi-circle from R to −R in the lower half plane. If
lim
R→∞

R max
z∈CR |f(z)|

= 0
then
−
Z ∞
−∞
f(x) dx = −ı2π
m
X
k=1
Res (f(z), zk) −ıπ
n
X
k=1
Res(f(z), xk)
where z1, . . . zm are the singularities of f(z) in the lower half plane and x1, . . . , xn are the
ﬁrst order poles on the real axis.
634

This result is proved in Exercise 13.13. Of course we can use this result to evaluate the integrals of the form
Z ∞
0
f(z) dz,
where f(x) is an even function.
13.5
Fourier Integrals
In order to do Fourier transforms, which are useful in solving diﬀerential equations, it is necessary to be able to calculate
Fourier integrals. Fourier integrals have the form Z ∞
−∞
eıωx f(x) dx.
We evaluate these integrals by closing the path of integration in the lower or upper half plane and using techniques of
contour integration.
Consider the integral
Z π/2
0
e−R sin θ dθ.
Since 2θ/π ≤sin θ for 0 ≤θ ≤π/2,
e−R sin θ ≤e−R2θ/π
for 0 ≤θ ≤π/2
Z π/2
0
e−R sin θ dθ ≤
Z π/2
0
e−R2θ/π dθ
=
h
−π
2R e−R2θ/πiπ/2
0
= −π
2R(e−R −1)
≤π
2R
→0
as R →∞
635

We can use this to prove the following Result 13.5.1. (See Exercise 13.17.)
Result 13.5.1 Jordan’s Lemma.
Z π
0
e−R sin θ dθ < π
R.
Suppose that f(z) vanishes as |z| →∞. If ω is a (positive/negative) real number and CR is
a semi-circle of radius R in the (upper/lower) half plane then the integral
Z
CR
f(z) eıωz dz
vanishes as R →∞.
We can use Jordan’s Lemma and the Residue Theorem to evaluate many Fourier integrals. Consider
R ∞
−∞f(x) eıωx dx,
where ω is a positive real number. Let f(z) be analytic except for isolated singularities, with only ﬁrst order poles on
the real axis. Let C be the contour from −R to R on the real axis and then back to −R along a semi-circle in the
upper half plane. If R is large enough so that C encloses all the singularities of f(z) in the upper half plane then
Z
C
f(z) eıωz dz = ı2π
m
X
k=1
Res(f(z) eıωz, zk) + ıπ
n
X
k=1
Res(f(z) eıωz, xk)
where z1, . . . zm are the singularities of f(z) in the upper half plane and x1, . . . , xn are the ﬁrst order poles on the real
axis. If f(z) vanishes as |z| →∞then the integral on CR vanishes as R →∞by Jordan’s Lemma.
Z ∞
−∞
f(x) eıωx dx = ı2π
m
X
k=1
Res(f(z) eıωz, zk) + ıπ
n
X
k=1
Res(f(z) eıωz, xk)
For negative ω we close the path of integration in the lower half plane. Note that the contour is then in the negative
direction.
636

Result 13.5.2 Fourier Integrals. Let f(z) be analytic except for isolated singularities, with
only ﬁrst order poles on the real axis. Suppose that f(z) vanishes as |z| →∞. If ω is a
positive real number then
Z ∞
−∞
f(x) eıωx dx = ı2π
m
X
k=1
Res(f(z) eıωz, zk) + ıπ
n
X
k=1
Res(f(z) eıωz, xk)
where z1, . . . zm are the singularities of f(z) in the upper half plane and x1, . . . , xn are the
ﬁrst order poles on the real axis. If ω is a negative real number then
Z ∞
−∞
f(x) eıωx dx = −ı2π
m
X
k=1
Res(f(z) eıωz, zk) −ıπ
n
X
k=1
Res(f(z) eıωz, xk)
where z1, . . . zm are the singularities of f(z) in the lower half plane and x1, . . . , xn are the
ﬁrst order poles on the real axis.
13.6
Fourier Cosine and Sine Integrals
Fourier cosine and sine integrals have the form,
Z ∞
0
f(x) cos(ωx) dx
and
Z ∞
0
f(x) sin(ωx) dx.
If f(x) is even/odd then we can evaluate the cosine/sine integral with the method we developed for Fourier integrals.
637

Let f(z) be analytic except for isolated singularities, with only ﬁrst order poles on the real axis. Suppose that f(x)
is an even function and that f(z) vanishes as |z| →∞. We consider real ω > 0.
−
Z ∞
0
f(x) cos(ωx) dx = 1
2 −
Z ∞
−∞
f(x) cos(ωx) dx
Since f(x) sin(ωx) is an odd function,
1
2 −
Z ∞
−∞
f(x) sin(ωx) dx = 0.
Thus
−
Z ∞
0
f(x) cos(ωx) dx = 1
2 −
Z ∞
−∞
f(x) eıωx dx
Now we apply Result 13.5.2.
−
Z ∞
0
f(x) cos(ωx) dx = ıπ
m
X
k=1
Res(f(z) eıωz, zk) + ıπ
2
n
X
k=1
Res(f(z) eıωz, xk)
where z1, . . . zm are the singularities of f(z) in the upper half plane and x1, . . . , xn are the ﬁrst order poles on the real
axis.
If f(x) is an odd function, we note that f(x) cos(ωx) is an odd function to obtain the analogous result for Fourier
sine integrals.
638

Result 13.6.1 Fourier Cosine and Sine Integrals. Let f(z) be analytic except for isolated
singularities, with only ﬁrst order poles on the real axis. Suppose that f(x) is an even function
and that f(z) vanishes as |z| →∞. We consider real ω > 0.
−
Z ∞
0
f(x) cos(ωx) dx = ıπ
m
X
k=1
Res(f(z) eıωz, zk) + ıπ
2
n
X
k=1
Res(f(z) eıωz, xk)
where z1, . . . zm are the singularities of f(z) in the upper half plane and x1, . . . , xn are the
ﬁrst order poles on the real axis. If f(x) is an odd function then,
−
Z ∞
0
f(x) sin(ωx) dx = π
µ
X
k=1
Res(f(z) eıωz, ζk) + π
2
n
X
k=1
Res(f(z) eıωz, xk)
where ζ1, . . . ζµ are the singularities of f(z) in the lower half plane and x1, . . . , xn are the ﬁrst
order poles on the real axis.
Now suppose that f(x) is neither even nor odd. We can evaluate integrals of the form:
Z ∞
−∞
f(x) cos(ωx) dx
and
Z ∞
−∞
f(x) sin(ωx) dx
by writing them in terms of Fourier integrals
Z ∞
−∞
f(x) cos(ωx) dx = 1
2
Z ∞
−∞
f(x) eıωx dx + 1
2
Z ∞
−∞
f(x) e−ıωx dx
Z ∞
−∞
f(x) sin(ωx) dx = −ı
2
Z ∞
−∞
f(x) eıωx dx + ı
2
Z ∞
−∞
f(x) e−ıωx dx
639

13.7
Contour Integration and Branch Cuts
Example 13.7.1 Consider
Z ∞
0
x−a
x + 1 dx,
0 < a < 1,
where x−a denotes exp(−a ln(x)). We choose the branch of the function
f(z) = z−a
z + 1
|z| > 0, 0 < arg z < 2π
with a branch cut on the positive real axis.
Let Cϵ and CR denote the circular arcs of radius ϵ and R where ϵ < 1 < R. Cϵ is negatively oriented; CR is
positively oriented. Consider the closed contour C that is traced by a point moving from Cϵ to CR above the branch
cut, next around CR, then below the cut to Cϵ, and ﬁnally around Cϵ. (See Figure 13.11.)
ε
CR
C
Figure 13.6:
We write f(z) in polar coordinates.
f(z) = exp(−a log z)
z + 1
= exp(−a(log r + iθ))
r eıθ +1
640

We evaluate the function above, (z = r eı0), and below, (z = r eı2π), the branch cut.
f(r eı0) = exp[−a(log r + i0)]
r + 1
= r−a
r + 1
f(r eı2π) = exp[−a(log r + ı2π)]
r + 1
= r−a e−ı2aπ
r + 1
.
We use the residue theorem to evaluate the integral along C.
I
C
f(z) dz = ı2π Res(f(z), −1)
Z R
ϵ
r−a
r + 1dr +
Z
CR
f(z) dz −
Z R
ϵ
r−a e−ı2aπ
r + 1
dr +
Z
Cϵ
f(z) dz = ı2π Res(f(z), −1)
The residue is
Res(f(z), −1) = exp(−a log(−1)) = exp(−a(log 1 + ıπ)) = e−ıaπ .
We bound the integrals along Cϵ and CR with the maximum modulus integral bound.

Z
Cϵ
f(z) dz
 ≤2πϵ ϵ−a
1 −ϵ = 2π ϵ1−a
1 −ϵ

Z
CR
f(z) dz
 ≤2πR R−a
R −1 = 2π R1−a
R −1
Since 0 < a < 1, the values of the integrals tend to zero as ϵ →0 and R →∞. Thus we have
Z ∞
0
r−a
r + 1dr = ı2π
e−ıaπ
1 −e−ı2aπ
Z ∞
0
x−a
x + 1 dx =
π
sin aπ
641

Result 13.7.1 Integrals from Zero to Inﬁnity. Let f(z) be a single-valued analytic func-
tion with only isolated singularities and no singularities on the positive, real axis, [0, ∞). Let
a ̸∈Z. If the integrals exist then,
Z ∞
0
f(x) dx = −
n
X
k=1
Res (f(z) log z, zk) ,
Z ∞
0
xaf(x) dx =
ı2π
1 −eı2πa
n
X
k=1
Res (zaf(z), zk) ,
Z ∞
0
f(x) log x dx = −1
2
n
X
k=1
Res
 f(z) log2 z, zk

+ ıπ
n
X
k=1
Res (f(z) log z, zk) ,
Z ∞
0
xaf(x) log x dx =
ı2π
1 −eı2πa
n
X
k=1
Res (zaf(z) log z, zk)
+
π2a
sin2(πa)
n
X
k=1
Res (zaf(z), zk) ,
Z ∞
0
xaf(x) logm x dx = ∂m
∂am
 
ı2π
1 −eı2πa
n
X
k=1
Res (zaf(z), zk)
!
,
where z1, . . . , zn are the singularities of f(z) and there is a branch cut on the positive real
axis with 0 < arg(z) < 2π.
642

13.8
Exploiting Symmetry
We have already used symmetry of the integrand to evaluate certain integrals. For f(x) an even function we were
able to evaluate
R ∞
0 f(x) dx by extending the range of integration from −∞to ∞. For
Z ∞
0
xαf(x) dx
we put a branch cut on the positive real axis and noted that the value of the integrand below the branch cut is a
constant multiple of the value of the function above the branch cut. This enabled us to evaluate the real integral with
contour integration. In this section we will use other kinds of symmetry to evaluate integrals. We will discover that
periodicity of the integrand will produce this symmetry.
13.8.1
Wedge Contours
We note that zn = rn eınθ is periodic in θ with period 2π/n. The real and imaginary parts of zn are odd periodic
in θ with period π/n. This observation suggests that certain integrals on the positive real axis may be evaluated by
closing the path of integration with a wedge contour.
Example 13.8.1 Consider
Z ∞
0
1
1 + xn dx
643

where n ∈N, n ≥2. We can evaluate this integral using Result 13.7.1.
Z ∞
0
1
1 + xn dx = −
n−1
X
k=0
Res
 log z
1 + zn, eıπ(1+2k)/n

= −
n−1
X
k=0
lim
z→eıπ(1+2k)/n
(z −eıπ(1+2k)/n) log z
1 + zn

= −
n−1
X
k=0
lim
z→eıπ(1+2k)/n
log z + (z −eıπ(1+2k)/n)/z
nzn−1

= −
n−1
X
k=0
 ıπ(1 + 2k)/n
n eıπ(1+2k)(n−1)/n

= −
ıπ
n2 eıπ(n−1)/n
n−1
X
k=0
(1 + 2k) eı2πk/n
= ı2π eıπ/n
n2
n−1
X
k=1
k eı2πk/n
= ı2π eıπ/n
n2
n
eı2π/n −1
=
π
n sin(π/n)
This is a bit grungy. To ﬁnd a spiﬃer way to evaluate the integral we note that if we write the integrand as a function
of r and θ, it is periodic in θ with period 2π/n.
1
1 + zn =
1
1 + rn eınθ
The integrand along the rays θ = 2π/n, 4π/n, 6π/n, . . . has the same value as the integrand on the real axis. Consider
the contour C that is the boundary of the wedge 0 < r < R, 0 < θ < 2π/n. There is one singularity inside the
644

contour. We evaluate the residue there.
Res

1
1 + zn, eıπ/n

=
lim
z→eıπ/n
z −eıπ/n
1 + zn
=
lim
z→eıπ/n
1
nzn−1
= −eıπ/n
n
We evaluate the integral along C with the residue theorem.
Z
C
1
1 + zn dz = −ı2π eıπ/n
n
Let CR be the circular arc. The integral along CR vanishes as R →∞.

Z
CR
1
1 + zn dz
 ≤2πR
n
max
z∈CR

1
1 + zn

≤2πR
n
1
Rn −1
→0 as R →∞
We parametrize the contour to evaluate the desired integral.
Z ∞
0
1
1 + xn dx +
Z 0
∞
1
1 + xn eı2π/n dx = −ı2π eıπ/n
n
Z ∞
0
1
1 + xn dx =
−ı2π eıπ/n
n(1 −eı2π/n)
Z ∞
0
1
1 + xn dx =
π
n sin(π/n)
645

13.8.2
Box Contours
Recall that ez = ex+ıy is periodic in y with period 2π. This implies that the hyperbolic trigonometric functions
cosh z, sinh z and tanh z are periodic in y with period 2π and odd periodic in y with period π. We can exploit this
property to evaluate certain integrals on the real axis by closing the path of integration with a box contour.
Example 13.8.2 Consider the integral
Z ∞
−∞
1
cosh x dx =
h
ı log

tanh
ıπ
4 + x
2
i∞
−∞
= ı log(1) −ı log(−1)
= π.
We will evaluate this integral using contour integration. Note that
cosh(x + ıπ) = ex+ıπ + e−x−ıπ
2
= −cosh(x).
Consider the box contour C that is the boundary of the region −R < x < R, 0 < y < π. The only singularity of
the integrand inside the contour is a ﬁrst order pole at z = ıπ/2. We evaluate the integral along C with the residue
theorem.
I
C
1
cosh z dz = ı2π Res

1
cosh z, ıπ
2

= ı2π lim
z→ıπ/2
z −ıπ/2
cosh z
= ı2π lim
z→ıπ/2
1
sinh z
= 2π
646

The integrals along the sides of the box vanish as R →∞.

Z ±R+ıπ
±R
1
cosh z dz
 ≤π
max
z∈[±R...±R+ıπ]

1
cosh z

≤π max
y∈[0...π]

2
e±R+ıy + e∓R−ıy

=
2
eR −e−R
≤
π
sinh R
→0 as R →∞
The value of the integrand on the top of the box is the negative of its value on the bottom. We take the limit as
R →∞.
Z ∞
−∞
1
cosh x dx +
Z −∞
∞
1
−cosh x dx = 2π
Z ∞
−∞
1
cosh x dx = π
13.9
Deﬁnite Integrals Involving Sine and Cosine
Example 13.9.1 For real-valued a, evaluate the integral:
f(a) =
Z 2π
0
dθ
1 + a sin θ.
What is the value of the integral for complex-valued a.
647

Real-Valued a. For −1 < a < 1, the integrand is bounded, hence the integral exists. For |a| = 1, the integrand
has a second order pole on the path of integration. For |a| > 1 the integrand has two ﬁrst order poles on the path of
integration. The integral is divergent for these two cases. Thus we see that the integral exists for −1 < a < 1.
For a = 0, the value of the integral is 2π. Now consider a ̸= 0. We make the change of variables z = eıθ. The real
integral from θ = 0 to θ = 2π becomes a contour integral along the unit circle, |z| = 1. We write the sine, cosine and
the diﬀerential in terms of z.
sin θ = z −z−1
ı2
,
cos θ = z + z−1
2
,
dz = ı eıθ dθ,
dθ = dz
ız
We write f(a) as an integral along C, the positively oriented unit circle |z| = 1.
f(a) =
I
C
1/(ız)
1 + a(z −z−1)/(2ı) dz =
I
C
2/a
z2 + (ı2/a)z −1 dz
We factor the denominator of the integrand.
f(a) =
I
C
2/a
(z −z1)(z −z2) dz
z1 = ı
−1 +
√
1 −a2
a

,
z2 = ı
−1 −
√
1 −a2
a

Because |a| < 1, the second root is outside the unit circle.
|z2| = 1 +
√
1 −a2
|a|
> 1.
Since |z1z2| = 1, |z1| < 1. Thus the pole at z1 is inside the contour and the pole at z2 is outside. We evaluate the
648

contour integral with the residue theorem.
f(a) =
I
C
2/a
z2 + (ı2/a)z −1 dz
= ı2π
2/a
z1 −z2
= ı2π
1
ı
√
1 −a2
f(a) =
2π
√
1 −a2
Complex-Valued a. We note that the integral converges except for real-valued a satisfying |a| ≥1. On any closed
subset of C \ {a ∈R | |a| ≥1} the integral is uniformly convergent. Thus except for the values {a ∈R | |a| ≥1},
we can diﬀerentiate the integral with respect to a. f(a) is analytic in the complex plane except for the set of points
on the real axis: a ∈(−∞. . . −1] and a ∈[1 . . . ∞). The value of the analytic function f(a) on the real axis for the
interval (−1 . . . 1) is
f(a) =
2π
√
1 −a2.
By analytic continuation we see that the value of f(a) in the complex plane is the branch of the function
f(a) =
2π
(1 −a2)1/2
where f(a) is positive, real-valued for a ∈(−1 . . . 1) and there are branch cuts on the real axis on the intervals:
(−∞. . . −1] and [1 . . . ∞).
649

Result 13.9.1 For evaluating integrals of the form
Z a+2π
a
F(sin θ, cos θ) dθ
it may be useful to make the change of variables z = eıθ. This gives us a contour integral
along the unit circle about the origin. We can write the sine, cosine and diﬀerential in terms
of z.
sin θ = z −z−1
ı2
,
cos θ = z + z−1
2
,
dθ = dz
ız
13.10
Inﬁnite Sums
The function g(z) = π cot(πz) has simple poles at z = n ∈Z. The residues at these points are all unity.
Res(π cot(πz), n) = lim
z→n
π(z −n) cos(πz)
sin(πz)
= lim
z→n
π cos(πz) −π(z −n) sin(πz)
π cos(πz)
= 1
Let Cn be the square contour with corners at z = (n + 1/2)(±1 ± ı). Recall that
cos z = cos x cosh y −ı sin x sinh y
and
sin z = sin x cosh y + ı cos x sinh y.
650

First we bound the modulus of cot(z).
| cot(z)| =

cos x cosh y −ı sin x sinh y
sin x cosh y + ı cos x sinh y

=
s
cos2 x cosh2 y + sin2 x sinh2 y
sin2 x cosh2 y + cos2 x sinh2 y
≤
s
cosh2 y
sinh2 y
= | coth(y)|
The hyperbolic cotangent, coth(y), has a simple pole at y = 0 and tends to ±1 as y →±∞.
Along the top and bottom of Cn, (z = x ± ı(n + 1/2)), we bound the modulus of g(z) = π cot(πz).
|π cot(πz)| ≤π
 coth(π(n + 1/2))

Along the left and right sides of Cn, (z = ±(n + 1/2) + ıy), the modulus of the function is bounded by a constant.
|g(±(n + 1/2) + ıy)| =
πcos(π(n + 1/2)) cosh(πy) ∓ı sin(π(n + 1/2)) sinh(πy)
sin(π(n + 1/2)) cosh(πy) + ı cos(π(n + 1/2)) sinh(πy)

= |∓ıπ tanh(πy)|
≤π
Thus the modulus of π cot(πz) can be bounded by a constant M on Cn.
Let f(z) be analytic except for isolated singularities. Consider the integral,
I
Cn
π cot(πz)f(z) dz.
651

We use the maximum modulus integral bound.

I
Cn
π cot(πz)f(z) dz
 ≤(8n + 4)M max
z∈Cn |f(z)|
Note that if
lim
|z|→∞|zf(z)| = 0,
then
lim
n→∞
I
Cn
π cot(πz)f(z) dz = 0.
This implies that the sum of all residues of π cot(πz)f(z) is zero. Suppose further that f(z) is analytic at z = n ∈Z.
The residues of π cot(πz)f(z) at z = n are f(n). This means
∞
X
n=−∞
f(n) = −( sum of the residues of π cot(πz)f(z) at the poles of f(z) ).
Result 13.10.1 If
lim
|z|→∞|zf(z)| = 0,
then the sum of all the residues of π cot(πz)f(z) is zero. If in addition f(z) is analytic at
z = n ∈Z then
∞
X
n=−∞
f(n) = −( sum of the residues of π cot(πz)f(z) at the poles of f(z) ).
Example 13.10.1 Consider the sum
∞
X
n=−∞
1
(n + a)2,
a ̸∈Z.
652

By Result 13.10.1 with f(z) = 1/(z + a)2 we have
∞
X
n=−∞
1
(n + a)2 = −Res

π cot(πz)
1
(z + a)2, −a

= −π lim
z→−a
d
dz cot(πz)
= −π−π sin2(πz) −π cos2(πz)
sin2(πz)
.
∞
X
n=−∞
1
(n + a)2 =
π2
sin2(πa)
Example 13.10.2 Derive π/4 = 1 −1/3 + 1/5 −1/7 + 1/9 −· · · .
Consider the integral
In =
1
ı2π
Z
Cn
dw
w(w −z) sin w
where Cn is the square with corners at w = (n + 1/2)(±1 ± ı)π, n ∈Z+. With the substitution w = x + ıy,
| sin w|2 = sin2 x + sinh2 y,
we see that |1/ sin w| ≤1 on Cn. Thus In →0 as n →∞. We use the residue theorem and take the limit n →∞.
0 =
∞
X
n=1

(−1)n
nπ(nπ −z) +
(−1)n
nπ(nπ + z)

+
1
z sin z −1
z2
1
sin z = 1
z −2z
∞
X
n=1
(−1)n
n2π2 −z2
= 1
z −
∞
X
n=1
 (−1)n
nπ −z −(−1)n
nπ + z

653

We substitute z = π/2 into the above expression to obtain
π/4 = 1 −1/3 + 1/5 −1/7 + 1/9 −· · ·
654

13.11
Exercises
The Residue Theorem
Exercise 13.1
Evaluate the following closed contour integrals using Cauchy’s residue theorem.
1.
Z
C
dz
z2 −1,
where C is the contour parameterized by r = 2 cos(2θ), 0 ≤θ ≤2π.
2.
Z
C
eız
z2(z −2)(z + ı5) dz,
where C is the positive circle |z| = 3.
3.
Z
C
e1/z sin(1/z) dz,
where C is the positive circle |z| = 1.
Hint, Solution
Exercise 13.2
Derive Cauchy’s integral formula from Cauchy’s residue theorem.
Hint, Solution
Exercise 13.3
Calculate the residues of the following functions at each of the poles in the ﬁnite part of the plane.
1.
1
z4 −a4
2. sin z
z2
3.
1 + z2
z(z −1)2
4.
ez
z2 + a2
655

5. (1 −cos z)2
z7
Hint, Solution
Exercise 13.4
Let f(z) have a pole of order n at z = z0. Prove the Residue Formula:
Res(f(z), z0) = lim
z→z0

1
(n −1)!
dn−1
dzn−1 [(z −z0)nf(z)]

.
Hint, Solution
Exercise 13.5
Consider the function
f(z) =
z4
z2 + 1.
Classify the singularities of f(z) in the extended complex plane. Calculate the residue at each pole and at inﬁnity. Find
the Laurent series expansions and their domains of convergence about the points z = 0, z = ı and z = ∞.
Hint, Solution
Exercise 13.6
Let P(z) be a polynomial none of whose roots lie on the closed contour Γ. Show that
1
ı2π
Z P ′(z)
P(z) dz = number of roots of P(z) which lie inside Γ.
where the roots are counted according to their multiplicity.
Hint: From the fundamental theorem of algebra, it is always possible to factor P(z) in the form P(z) = (z−z1)(z−
z2) · · · (z −zn). Using this form of P(z) the integrand P ′(z)/P(z) reduces to a very simple expression.
Hint, Solution
656

Exercise 13.7
Find the value of
I
C
ez
(z −π) tan z dz
where C is the positively-oriented circle
1. |z| = 2
2. |z| = 4
Hint, Solution
Cauchy Principal Value for Real Integrals
Solution 13.1
Show that the integral
Z 1
−1
1
x dx.
is divergent. Evaluate the integral
Z 1
−1
1
x −ıα dx,
α ∈R, α ̸= 0.
Evaluate
lim
α→0+
Z 1
−1
1
x −ıα dx
and
lim
α→0−
Z 1
−1
1
x −ıα dx.
The integral exists for α arbitrarily close to zero, but diverges when α = 0. Plot the real and imaginary part of the
integrand. If one were to assign meaning to the integral for α = 0, what would the value of the integral be?
657

Exercise 13.8
Do the principal values of the following integrals exist?
1.
R 1
−1
1
x2 dx,
2.
R 1
−1
1
x3 dx,
3.
R 1
−1
f(x)
x3 dx.
Assume that f(x) is real analytic on the interval (−1, 1).
Hint, Solution
Cauchy Principal Value for Contour Integrals
Exercise 13.9
Let f(z) have a ﬁrst order pole at z = z0 and let (z −z0)f(z) be analytic in some neighborhood of z0. Let the contour
Cϵ be a circular arc from z0 + ϵeıα to z0 + ϵeıβ. (Assume that β > α and β −α < 2π.) Show that
lim
ϵ→0+
Z
Cϵ
f(z) dz = ı(β −α) Res(f(z), z0)
Hint, Solution
Exercise 13.10
Let f(z) be analytic inside and on a simple, closed, positive contour C, except for isolated singularities at z1, . . . , zm
inside the contour and ﬁrst order poles at ζ1, . . . , ζn on the contour. Further, let the contour be C1 at the locations of
these ﬁrst order poles. (i.e., the contour does not have a corner at any of the ﬁrst order poles.) Show that the principal
value of the integral of f(z) along C is
−
Z
C
f(z) dz = ı2π
m
X
j=1
Res(f(z), zj) + ıπ
n
X
j=1
Res(f(z), ζj).
Hint, Solution
658

Exercise 13.11
Let C be the unit circle. Evaluate
−
Z
C
1
z −1 dz
by indenting the contour to exclude the ﬁrst order pole at z = 1.
Hint, Solution
Integrals on the Real Axis
Exercise 13.12
Evaluate the following improper integrals.
1.
Z ∞
0
x2
(x2 + 1)(x2 + 4) dx = π
6
2.
Z ∞
−∞
dx
(x + b)2 + a2,
a > 0
Hint, Solution
Exercise 13.13
Prove Result 13.4.1.
Hint, Solution
Exercise 13.14
Evaluate
−
Z ∞
−∞
2x
x2 + x + 1.
Hint, Solution
Exercise 13.15
Use contour integration to evaluate the integrals
659

1.
Z ∞
−∞
dx
1 + x4,
2.
Z ∞
−∞
x2 dx
(1 + x2)2,
3.
Z ∞
−∞
cos(x)
1 + x2 dx.
Hint, Solution
Exercise 13.16
Evaluate by contour integration
Z ∞
0
x6
(x4 + 1)2 dx.
Hint, Solution
Fourier Integrals
Exercise 13.17
Suppose that f(z) vanishes as |z| →∞. If ω is a (positive / negative) real number and CR is a semi-circle of radius
R in the (upper / lower) half plane then show that the integral
Z
CR
f(z) eıωz dz
vanishes as R →∞.
Hint, Solution
Exercise 13.18
Evaluate by contour integration
Z ∞
−∞
cos 2x
x −ıπ dx.
660

Hint, Solution
Fourier Cosine and Sine Integrals
Exercise 13.19
Evaluate
Z ∞
−∞
sin x
x
dx.
Hint, Solution
Exercise 13.20
Evaluate
Z ∞
−∞
1 −cos x
x2
dx.
Hint, Solution
Exercise 13.21
Evaluate
Z ∞
0
sin(πx)
x(1 −x2) dx.
Hint, Solution
Contour Integration and Branch Cuts
Exercise 13.22
Evaluate the following integrals.
1.
Z ∞
0
ln2 x
1 + x2 dx = π3
8
2.
Z ∞
0
ln x
1 + x2 dx = 0
661

Hint, Solution
Exercise 13.23
By methods of contour integration ﬁnd
Z ∞
0
dx
x2 + 5x + 6
[ Recall the trick of considering
R
Γ f(z) log z dz with a suitably chosen contour Γ and branch for log z. ]
Hint, Solution
Exercise 13.24
Show that
Z ∞
0
xa
(x + 1)2 dx =
πa
sin(πa)
for −1 < ℜ(a) < 1.
From this derive that
Z ∞
0
log x
(x + 1)2 dx = 0,
Z ∞
0
log2 x
(x + 1)2 dx = π2
3 .
Hint, Solution
Exercise 13.25
Consider the integral
I(a) =
Z ∞
0
xa
1 + x2 dx.
1. For what values of a does the integral exist?
2. Evaluate the integral. Show that
I(a) =
π
2 cos(πa/2)
3. Deduce from your answer in part (b) the results
Z ∞
0
log x
1 + x2 dx = 0,
Z ∞
0
log2 x
1 + x2 dx = π3
8 .
662

You may assume that it is valid to diﬀerentiate under the integral sign.
Hint, Solution
Exercise 13.26
Let f(z) be a single-valued analytic function with only isolated singularities and no singularities on the positive real
axis, [0, ∞). Give suﬃcient conditions on f(x) for absolute convergence of the integral
Z ∞
0
xaf(x) dx.
Assume that a is not an integer. Evaluate the integral by considering the integral of zaf(z) on a suitable contour.
(Consider the branch of za on which 1a = 1.)
Hint, Solution
Exercise 13.27
Using the solution to Exercise 13.26, evaluate
Z ∞
0
xaf(x) log x dx,
and
Z ∞
0
xaf(x) logm x dx,
where m is a positive integer.
Hint, Solution
Exercise 13.28
Using the solution to Exercise 13.26, evaluate
Z ∞
0
f(x) dx,
i.e. examine a = 0. The solution will suggest a way to evaluate the integral with contour integration. Do the contour
integration to corroborate the value of
R ∞
0 f(x) dx.
Hint, Solution
663

Exercise 13.29
Let f(z) be an analytic function with only isolated singularities and no singularities on the positive real axis, [0, ∞).
Give suﬃcient conditions on f(x) for absolute convergence of the integral
Z ∞
0
f(x) log x dx
Evaluate the integral with contour integration.
Hint, Solution
Exercise 13.30
For what values of a does the following integral exist?
Z ∞
0
xa
1 + x4 dx.
Evaluate the integral. (Consider the branch of xa on which 1a = 1.)
Hint, Solution
Exercise 13.31
By considering the integral of f(z) = z1/2 log z/(z + 1)2 on a suitable contour, show that
Z ∞
0
x1/2 log x
(x + 1)2 dx = π,
Z ∞
0
x1/2
(x + 1)2 dx = π
2 .
Hint, Solution
Exploiting Symmetry
Exercise 13.32
Evaluate by contour integration, the principal value integral
I(a) = −
Z ∞
−∞
eax
ex −e−x dx
664

for a real and |a| < 1. [Hint: Consider the contour that is the boundary of the box, −R < x < R, 0 < y < π, but
indented around z = 0 and z = ıπ.
Hint, Solution
Exercise 13.33
Evaluate the following integrals.
1.
Z ∞
0
dx
(1 + x2)2,
2.
Z ∞
0
dx
1 + x3.
Hint, Solution
Exercise 13.34
Find the value of the integral I
I =
Z ∞
0
dx
1 + x6
by considering the contour integral
Z
Γ
dz
1 + z6
with an appropriately chosen contour Γ.
Hint, Solution
Exercise 13.35
Let C be the boundary of the sector 0 < r < R, 0 < θ < π/4. By integrating e−z2 on C and letting R →∞show
that
Z ∞
0
cos(x2) dx =
Z ∞
0
sin(x2) dx = 1
√
2
Z ∞
0
e−x2 dx.
Hint, Solution
665

Exercise 13.36
Evaluate
Z ∞
−∞
x
sinh x dx
using contour integration.
Hint, Solution
Exercise 13.37
Show that
Z ∞
−∞
eax
ex +1 dx =
π
sin(πa)
for 0 < a < 1.
Use this to derive that
Z ∞
−∞
cosh(bx)
cosh x dx =
π
cos(πb/2)
for −1 < b < 1.
Hint, Solution
Exercise 13.38
Using techniques of contour integration ﬁnd for real a and b:
F(a, b) =
Z π
0
dθ
(a + b cos θ)2
What are the restrictions on a and b if any? Can the result be applied for complex a, b? How?
Hint, Solution
Exercise 13.39
Show that
Z ∞
−∞
cos x
ex + e−x dx =
π
eπ/2 + e−π/2
[ Hint: Begin by considering the integral of eız /(ez + e−z) around a rectangle with vertices: ±R, ±R + ıπ.]
Hint, Solution
666

Deﬁnite Integrals Involving Sine and Cosine
Exercise 13.40
Evaluate the following real integrals.
1.
Z π
−π
dθ
1 + sin2 θ =
√
2π
2.
Z π/2
0
sin4 θ dθ
Hint, Solution
Exercise 13.41
Use contour integration to evaluate the integrals
1.
Z 2π
0
dθ
2 + sin(θ),
2.
Z π
−π
cos(nθ)
1 −2a cos(θ) + a2 dθ
for |a| < 1, n ∈Z0+.
Hint, Solution
Exercise 13.42
By integration around the unit circle, suitably indented, show that
−
Z π
0
cos(nθ)
cos θ −cos α dθ = πsin(nα)
sin α .
Hint, Solution
667

Exercise 13.43
Evaluate
Z 1
0
x2
(1 + x2)
√
1 −x2 dx.
Hint, Solution
Inﬁnite Sums
Exercise 13.44
Evaluate
∞
X
n=1
1
n4.
Hint, Solution
Exercise 13.45
Sum the following series using contour integration:
∞
X
n=−∞
1
n2 −α2
Hint, Solution
668

13.12
Hints
The Residue Theorem
Hint 13.1
Hint 13.2
Hint 13.3
Hint 13.4
Substitute the Laurent series into the formula and simplify.
Hint 13.5
Use that the sum of all residues of the function in the extended complex plane is zero in calculating the residue at
inﬁnity. To obtain the Laurent series expansion about z = ı, write the function as a proper rational function, (numerator
has a lower degree than the denominator) and expand in partial fractions.
Hint 13.6
Hint 13.7
Cauchy Principal Value for Real Integrals
Hint 13.8
669

Hint 13.9
For the third part, does the integrand have a term that behaves like 1/x2?
Cauchy Principal Value for Contour Integrals
Hint 13.10
Expand f(z) in a Laurent series. Only the ﬁrst term will make a contribution to the integral in the limit as ϵ →0+.
Hint 13.11
Use the result of Exercise 13.9.
Hint 13.12
Look at Example 13.3.2.
Integrals on the Real Axis
Hint 13.13
Hint 13.14
Close the path of integration in the upper or lower half plane with a semi-circle. Use the maximum modulus integral
bound, (Result 10.2.1), to show that the integral along the semi-circle vanishes.
Hint 13.15
Make the change of variables x = 1/ξ.
Hint 13.16
Use Result 13.4.1.
Hint 13.17
670

Fourier Integrals
Hint 13.18
Use
Z π
0
e−R sin θ dθ < π
R.
Hint 13.19
Fourier Cosine and Sine Integrals
Hint 13.20
Consider the integral of eıx
ıx .
Hint 13.21
Show that
Z ∞
−∞
1 −cos x
x2
dx = −
Z ∞
−∞
1 −eıx
x2
dx.
Hint 13.22
Show that
Z ∞
0
sin(πx)
x(1 −x2) dx = −ı
2 −
Z ∞
−∞
eıx
x(1 −x2) dx.
Contour Integration and Branch Cuts
Hint 13.23
Integrate a branch of log2 z/(1 + z2) along the boundary of the domain ϵ < r < R, 0 < θ < π.
Hint 13.24
671

Hint 13.25
Note that
Z 1
0
xa dx
converges for ℜ(a) > −1; and
Z ∞
1
xa dx
converges for ℜ(a) < 1.
Consider f(z) = za/(z + 1)2 with a branch cut along the positive real axis and the contour in Figure 13.11 in the
limit as ρ →0 and R →∞.
To derive the last two integrals, diﬀerentiate with respect to a.
Hint 13.26
Hint 13.27
Consider the integral of zaf(z) on the contour in Figure 13.11.
Hint 13.28
Diﬀerentiate with respect to a.
Hint 13.29
Take the limit as a →0. Use L’Hospital’s rule. To corroborate the result, consider the integral of f(z) log z on an
appropriate contour.
Hint 13.30
Consider the integral of f(z) log2 z on the contour in Figure 13.11.
672

Hint 13.31
Consider the integral of
f(z) =
za
1 + z4
on the boundary of the region ϵ < r < R, 0 < θ < π/2. Take the limits as ϵ →0 and R →∞.
Hint 13.32
Consider the branch of f(z) = z1/2 log z/(z + 1)2 with a branch cut on the positive real axis and 0 < arg z < 2π.
Integrate this function on the contour in Figure 13.11.
Exploiting Symmetry
Hint 13.33
Hint 13.34
For the second part, consider the integral along the boundary of the region, 0 < r < R, 0 < θ < 2π/3.
Hint 13.35
Hint 13.36
To show that the integral on the quarter-circle vanishes as R →∞establish the inequality,
cos 2θ ≥1 −4
πθ,
0 ≤θ ≤π
4 .
Hint 13.37
Consider the box contour C this is the boundary of the rectangle, −R ≤x ≤R, 0 ≤y ≤π. The value of the integral
is π2/2.
Hint 13.38
Consider the rectangular contour with corners at ±R and ±R + ı2π. Let R →∞.
673

Hint 13.39
Hint 13.40
Deﬁnite Integrals Involving Sine and Cosine
Hint 13.41
Hint 13.42
Hint 13.43
Hint 13.44
Make the changes of variables x = sin ξ and then z = eıξ.
Inﬁnite Sums
Hint 13.45
Use Result 13.10.1.
Hint 13.46
674

13.13
Solutions
The Residue Theorem
Solution 13.2
1. We consider
Z
C
dz
z2 −1
where C is the contour parameterized by r = 2 cos(2θ), 0 ≤θ ≤2π. (See Figure 13.7.) There are ﬁrst order
-2
-1
1
2
-2
-1
1
2
Figure 13.7: The contour r = 2 cos(2θ).
poles at z = ±1. We evaluate the integral with Cauchy’s residue theorem.
Z
C
dz
z2 −1 = ı2π

Res

1
z2 −1, z = 1

+ Res

1
z2 −1, z = −1

= ı2π

1
z + 1

z=1
+
1
z −1

z=−1

= 0
675

2. We consider the integral
Z
C
eız
z2(z −2)(z + ı5) dz,
where C is the positive circle |z| = 3. There is a second order pole at z = 0, and ﬁrst order poles at z = 2 and
z = −ı5. The poles at z = 0 and z = 2 lie inside the contour. We evaluate the integral with Cauchy’s residue
theorem.
Z
C
eız
z2(z −2)(z + ı5) dz = ı2π

Res

eız
z2(z −2)(z + ı5), z = 0

+ Res

eız
z2(z −2)(z + ı5), z = 2
 
= ı2π
 d
dz
eız
(z −2)(z + ı5)

z=0
+
eız
z2(z + ı5)

z=2

= ı2π
 d
dz
eız
(z −2)(z + ı5)

z=0
+
eız
z2(z + ı5)

z=2

= ı2π
 ı (z2 + (ı7 −2)z −5 −ı12) eız
(z −2)2(z + ı5)2

z=0
+
 1
58 −ı 5
116

eı2

= ı2π

−3
25 + ı
20 +
 1
58 −ı 5
116

eı2

= −π
10 + 5
58π cos 2 −1
29π sin 2 + ı

−6π
25 + 1
29π cos 2 + 5
58π sin 2

3. We consider the integral
Z
C
e1/z sin(1/z) dz
where C is the positive circle |z| = 1. There is an essential singularity at z = 0. We determine the residue there
676

by expanding the integrand in a Laurent series.
e1/z sin(1/z) =

1 + 1
z + O
 1
z2
 1
z + O
 1
z3

= 1
z + O
 1
z2

The residue at z = 0 is 1. We evaluate the integral with the residue theorem.
Z
C
e1/z sin(1/z) dz = ı2π
Solution 13.3
If f(ζ) is analytic in a compact, closed, connected domain D and z is a point in the interior of D then Cauchy’s integral
formula states
f (n)(z) = n!
ı2π
I
∂D
f(ζ)
(ζ −z)n+1 dζ.
To corroborate this, we evaluate the integral with Cauchy’s residue theorem. There is a pole of order n+1 at the point
ζ = z.
n!
ı2π
I
∂D
f(ζ)
(ζ −z)n+1 dζ. = n!
ı2π
ı2π
n!
dn
dζnf(ζ)

ζ=z
= f (n)(z)
Solution 13.4
1.
1
z4 −a4 =
1
(z −a)(z + a)(z −ıa)(z + ıa)
677

There are ﬁrst order poles at z = ±a and z = ±ıa. We calculate the residues there.
Res

1
z4 −a4, z = a

=
1
(z + a)(z −ıa)(z + ıa)

z=a
=
1
4a3
Res

1
z4 −a4, z = −a

=
1
(z −a)(z −ıa)(z + ıa)

z=−a
= −1
4a3
Res

1
z4 −a4, z = ıa

=
1
(z −a)(z + a)(z + ıa)

z=ıa
=
ı
4a3
Res

1
z4 −a4, z = −ıa

=
1
(z −a)(z + a)(z −ıa)

z=−ıa
= −ı
4a3
2.
sin z
z2
Since denominator has a second order zero at z = 0 and the numerator has a ﬁrst order zero there, the function
has a ﬁrst order pole at z = 0. We calculate the residue there.
Res
sin z
z2 , z = 0

= lim
z→0
sin z
z
= lim
z→0
cos z
1
= 1
3.
1 + z2
z(z −1)2
There is a ﬁrst order pole at z = 0 and a second order pole at z = 1.
Res
 1 + z2
z(z −1)2, z = 0

=
1 + z2
(z −1)2

z=0
= 1
678

Res
 1 + z2
z(z −1)2, z = 1

= d
dz
1 + z2
z

z=1
=

1 −1
z2

z=1
= 0
4. ez / (z2 + a2) has ﬁrst order poles at z = ±ıa. We calculate the residues there.
Res

ez
z2 + a2, z = ıa

=
ez
z + ıa

z=ıa
= −ı eıa
2a
Res

ez
z2 + a2, z = −ıa

=
ez
z −ıa

z=−ıa
= ı e−ıa
2a
5. Since 1 −cos z has a second order zero at z = 0, (1−cos z)2
z7
has a third order pole at that point. We ﬁnd the
residue by expanding the function in a Laurent series.
(1 −cos z)2
z7
= z−7

1 −

1 −z2
2 + z4
24 + O
 z62
= z−7
z2
2 −z4
24 + O
 z62
= z−7
z4
4 −z6
24 + O
 z8
=
1
4z3 −
1
24z + O(z)
The residue at z = 0 is −1/24.
679

Solution 13.5
Since f(z) has an isolated pole of order n at z = z0, it has a Laurent series that is convergent in a deleted neighborhood
about that point. We substitute this Laurent series into the Residue Formula to verify it.
Res(f(z), z0) = lim
z→z0

1
(n −1)!
dn−1
dzn−1 [(z −z0)nf(z)]

= lim
z→z0
 
1
(n −1)!
dn−1
dzn−1
"
(z −z0)n
∞
X
k=−n
ak(z −z0)k
#!
= lim
z→z0
 
1
(n −1)!
dn−1
dzn−1
" ∞
X
k=0
ak−n(z −z0)k
#!
= lim
z→z0
 
1
(n −1)!
∞
X
k=n−1
ak−n
k!
(k −n + 1)!(z −z0)k−n+1
!
= lim
z→z0
 
1
(n −1)!
∞
X
k=0
ak−1
(k + n −1)!
k!
(z −z0)k
!
=
1
(n −1)!a−1
(n −1)!
0!
= a−1
This proves the Residue Formula.
Solution 13.6
Classify Singularities.
f(z) =
z4
z2 + 1 =
z4
(z −ı)(z + ı).
There are ﬁrst order poles at z = ±ı. Since the function behaves like z2 at inﬁnity, there is a second order pole there.
680

To see this more slowly, we can make the substitution z = 1/ζ and examine the point ζ = 0.
f
1
ζ

=
ζ−4
ζ−2 + 1 =
1
ζ2 + ζ4 =
1
ζ2(1 + ζ2)
f(1/ζ) has a second order pole at ζ = 0, which implies that f(z) has a second order pole at inﬁnity.
Residues. The residues at z = ±ı are,
Res

z4
z2 + 1, ı

= lim
z→ı
z4
z + ı = −ı
2,
Res

z4
z2 + 1, −ı

= lim
z→−ı
z4
z −ı = ı
2.
The residue at inﬁnity is
Res(f(z), ∞) = Res
−1
ζ2 f
1
ζ

, ζ = 0

= Res
−1
ζ2
ζ−4
ζ−2 + 1, ζ = 0

= Res

−ζ−4
1 + ζ2, ζ = 0

Here we could use the residue formula, but it’s easier to ﬁnd the Laurent expansion.
= Res
 
−ζ−4
∞
X
n=0
(−1)nζ2n, ζ = 0
!
= 0
We could also calculate the residue at inﬁnity by recalling that the sum of all residues of this function in the extended
complex plane is zero.
−ı
2 + ı
2 + Res(f(z), ∞) = 0
681

Res(f(z), ∞) = 0
Laurent Series about z = 0. Since the nearest singularities are at z = ±ı, the Taylor series will converge in the
disk |z| < 1.
z4
z2 + 1 = z4
1
1 −(−z)2
= z4
∞
X
n=0
(−z2)n
= z4
∞
X
n=0
(−1)nz2n
=
∞
X
n=2
(−1)nz2n
This geometric series converges for | −z2| < 1, or |z| < 1. The series expansion of the function is
z4
z2 + 1 =
∞
X
n=2
(−1)nz2n
for |z| < 1
Laurent Series about z = ı. We expand f(z) in partial fractions. First we write the function as a proper rational
function, (i.e. the numerator has lower degree than the denominator). By polynomial division, we see that
f(z) = z2 −1 +
1
z2 + 1.
Now we expand the last term in partial fractions.
f(z) = z2 −1 + −ı/2
z −ı + ı/2
z + ı
682

Since the nearest singularity is at z = −ı, the Laurent series will converge in the annulus 0 < |z −ı| < 2.
z2 −1 = ((z −ı) + ı)2 −1
= (z −ı)2 + ı2(z −ı) −2
ı/2
z + ı =
ı/2
ı2 + (z −ı)
=
1/4
1 −ı(z −ı)/2
= 1
4
∞
X
n=0
ı(z −ı)
2
n
= 1
4
∞
X
n=0
ın
2n(z −ı)n
This geometric series converges for |ı(z −ı)/2| < 1, or |z −ı| < 2. The series expansion of f(z) is
f(z) = −ı/2
z −ı −2 + ı2(z −ı) + (z −ı)2 + 1
4
∞
X
n=0
ın
2n(z −ı)n.
z4
z2 + 1 = −ı/2
z −ı −2 + ı2(z −ı) + (z −ı)2 + 1
4
∞
X
n=0
ın
2n(z −ı)n
for |z −ı| < 2
Laurent Series about z = ∞. Since the nearest singularities are at z = ±ı, the Laurent series will converge in
683

the annulus 1 < |z| < ∞.
z4
z2 + 1 =
z2
1 + 1/z2
= z2
∞
X
n=0

−1
z2
n
=
0
X
n=−∞
(−1)nz2(n+1)
=
1
X
n=−∞
(−1)n+1z2n
This geometric series converges for | −1/z2| < 1, or |z| > 1. The series expansion of f(z) is
z4
z2 + 1 =
1
X
n=−∞
(−1)n+1z2n
for 1 < |z| < ∞
Solution 13.7
Method 1: Residue Theorem. We factor P(z). Let m be the number of roots, counting multiplicities, that lie
inside the contour Γ. We ﬁnd a simple expression for P ′(z)/P(z).
P(z) = c
n
Y
k=1
(z −zk)
P ′(z) = c
n
X
k=1
n
Y
j=1
j̸=k
(z −zj)
684

P ′(z)
P(z) =
c Pn
k=1
Qn
j=1
j̸=k(z −zj)
c Qn
k=1(z −zk)
=
n
X
k=1
Qn
j=1
j̸=k(z −zj)
Qn
j=1(z −zj)
=
n
X
k=1
1
z −zk
Now we do the integration using the residue theorem.
1
ı2π
Z
Γ
P ′(z)
P(z) dz =
1
ı2π
Z
Γ
n
X
k=1
1
z −zk
dz
=
n
X
k=1
1
ı2π
Z
Γ
1
z −zk
dz
=
X
zk inside Γ
1
ı2π
Z
Γ
1
z −zk
dz
=
X
zk inside Γ
1
= m
Method 2: Fundamental Theorem of Calculus. We factor the polynomial, P(z) = c Qn
k=1(z −zk). Let m be
685

the number of roots, counting multiplicities, that lie inside the contour Γ.
1
ı2π
Z
Γ
P ′(z)
P(z) dz =
1
ı2π [log P(z)]C
=
1
ı2π
"
log
n
Y
k=1
(z −zk)
#
C
=
1
ı2π
" n
X
k=1
log(z −zk)
#
C
The value of the logarithm changes by ı2π for the terms in which zk is inside the contour. Its value does not change
for the terms in which zk is outside the contour.
=
1
ı2π
"
X
zk inside Γ
log(z −zk)
#
C
=
1
ı2π
X
zk inside Γ
ı2π
= m
Solution 13.8
1.
I
C
ez
(z −π) tan z dz =
I
C
ez cos z
(z −π) sin z dz
The integrand has ﬁrst order poles at z = nπ, n ∈Z, n ̸= 1 and a double pole at z = π. The only pole inside
686

the contour occurs at z = 0. We evaluate the integral with the residue theorem.
I
C
ez cos z
(z −π) sin z dz = ı2π Res

ez cos z
(z −π) sin z, z = 0

= ı2π lim
z=0 z
ez cos z
(z −π) sin z
= −ı2 lim
z=0
z
sin z
= −ı2 lim
z=0
1
cos z
= −ı2
I
C
ez
(z −π) tan z dz = −ı2
2. The integrand has a ﬁrst order poles at z = 0, −π and a second order pole at z = π inside the contour. The
value of the integral is ı2π times the sum of the residues at these points. From the previous part we know that
residue at z = 0.
Res

ez cos z
(z −π) sin z, z = 0

= −1
π
We ﬁnd the residue at z = −π with the residue formula.
Res

ez cos z
(z −π) sin z, z = −π

= lim
z→−π(z + π)
ez cos z
(z −π) sin z
= e−π(−1)
−2π
lim
z→−π
z + π
sin z
= e−π
2π
lim
z→−π
1
cos z
= −e−π
2π
687

We ﬁnd the residue at z = π by ﬁnding the ﬁrst few terms in the Laurent series of the integrand.
ez cos z
(z −π) sin z = (eπ + eπ(z −π) + O ((z −π)2)) (1 + O ((z −π)2))
(z −π) (−(z −π) + O ((z −π)3))
= −eπ −eπ(z −π) + O ((z −π)2)
−(z −π)2 + O ((z −π)4)
=
eπ
(z−π)2 +
eπ
z−π + O(1)
1 + O ((z −π)2)
=

eπ
(z −π)2 +
eπ
z −π + O(1)
  1 + O
 (z −π)2
=
eπ
(z −π)2 +
eπ
z −π + O(1)
With this we see that
Res

ez cos z
(z −π) sin z, z = π

= eπ .
The integral is
I
C
ez cos z
(z −π) sin z dz = ı2π

Res

ez cos z
(z −π) sin z, z = −π

+ Res

ez cos z
(z −π) sin z, z = 0

+ Res

ez cos z
(z −π) sin z, z = π
 
= ı2π

−1
π −e−π
2π + eπ

I
C
ez
(z −π) tan z dz = ı
 2π eπ −2 −e−π
Cauchy Principal Value for Real Integrals
688

Solution 13.9
Consider the integral
Z 1
−1
1
x dx.
By the deﬁnition of improper integrals we have
Z 1
−1
1
x dx = lim
ϵ→0+
Z −ϵ
−1
1
x dx + lim
δ→0+
Z 1
δ
1
x dx
= lim
ϵ→0+ [log |x|]−ϵ
−1 + lim
δ→0+ [log |x|]1
δ
= lim
ϵ→0+ log ϵ −lim
δ→0+ log δ
This limit diverges. Thus the integral diverges.
Now consider the integral
Z 1
−1
1
x −ıα dx
where α ∈R, α ̸= 0. Since the integrand is bounded, the integral exists.
Z 1
−1
1
x −ıα dx =
Z 1
−1
x + ıα
x2 + α2 dx
=
Z 1
−1
ıα
x2 + α2 dx
= ı2
Z 1
0
α
x2 + α2 dx
= ı2
Z 1/α
0
1
ξ2 + 1 dξ
= ı2 [arctan ξ]1/α
0
= ı2 arctan
 1
α

689

Note that the integral exists for all nonzero real α and that
lim
α→0+
Z 1
−1
1
x −ıα dx = ıπ
and
lim
α→0−
Z 1
−1
1
x −ıα dx = −ıπ.
The integral exists for α arbitrarily close to zero, but diverges when α = 0. The real part of the integrand is an odd
function with two humps that get thinner and taller with decreasing α. The imaginary part of the integrand is an even
function with a hump that gets thinner and taller with decreasing α. (See Figure 13.8.)
ℜ

1
x −ıα

=
x
x2 + α2,
ℑ

1
x −ıα

=
α
x2 + α2
Figure 13.8: The real and imaginary part of the integrand for several values of α.
Note that
ℜ
Z 1
0
1
x −ıα dx →+∞as α →0+
690

and
ℜ
Z 0
−1
1
x −ıα dx →−∞as α →0−.
However,
lim
α→0 ℜ
Z 1
−1
1
x −ıα dx = 0
because the two integrals above cancel each other.
Now note that when α = 0, the integrand is real. Of course the integral doesn’t converge for this case, but if we
could assign some value to
Z 1
−1
1
x dx
it would be a real number. Since
lim
α→0
Z 1
−1
ℜ

1
x −ıα

dx = 0,
This number should be zero.
Solution 13.10
1.
−
Z 1
−1
1
x2 dx = lim
ϵ→0+
Z −ϵ
−1
1
x2 dx +
Z 1
ϵ
1
x2 dx

= lim
ϵ→0+
 
−1
x
−ϵ
−1
+

−1
x
1
ϵ
!
= lim
ϵ→0+
1
ϵ −1 −1 + 1
ϵ

The principal value of the integral does not exist.
691

2.
−
Z 1
−1
1
x3 dx = lim
ϵ→0+
Z −ϵ
−1
1
x3 dx +
Z 1
ϵ
1
x3 dx

= lim
ϵ→0+
 
−1
2x2
−ϵ
−1
+

−1
2x2
1
ϵ
!
= lim
ϵ→0+

−
1
2(−ϵ)2 +
1
2(−1)2 −
1
2(1)2 + 1
2ϵ2

= 0
3. Since f(x) is real analytic,
f(x) =
∞
X
n=1
fnxn
for x ∈(−1, 1).
We can rewrite the integrand as
f(x)
x3
= f0
x3 + f1
x2 + f2
x + f(x) −f0 −f1x −f2x2
x3
.
Note that the ﬁnal term is real analytic on (−1, 1). Thus the principal value of the integral exists if and only if
f2 = 0.
Cauchy Principal Value for Contour Integrals
Solution 13.11
We can write f(z) as
f(z) =
f0
z −z0
+ (z −z0)f(z) −f0
z −z0
.
Note that the second term is analytic in a neighborhood of z0. Thus it is bounded on the contour. Let Mϵ be the
692

maximum modulus of (z−z0)f(z)−f0
z−z0
on Cϵ. By using the maximum modulus integral bound, we have

Z
Cϵ
(z −z0)f(z) −f0
z −z0
dz
 ≤(β −α)ϵMϵ
→0
as ϵ →0+.
Thus we see that
lim
ϵ→0+
Z
Cϵ
f(z) dz lim
ϵ→0+
Z
Cϵ
f0
z −z0
dz.
We parameterize the path of integration with
z = z0 + ϵeıθ,
θ ∈(α, β).
Now we evaluate the integral.
lim
ϵ→0+
Z
Cϵ
f0
z −z0
dz = lim
ϵ→0+
Z β
α
f0
ϵeıθ ıϵeıθ dθ
= lim
ϵ→0+
Z β
α
ıf0 dθ
= ı(β −α)f0
≡ı(β −α) Res(f(z), z0)
This proves the result.
Solution 13.12
Let Ci be the contour that is indented with circular arcs or radius ϵ at each of the ﬁrst order poles on C so as to enclose
these poles. Let A1, . . . , An be these circular arcs of radius ϵ centered at the points ζ1, . . . , ζn. Let Cp be the contour,
(not necessarily connected), obtained by subtracting each of the Aj’s from Ci.
Since the curve is C1, (or continuously diﬀerentiable), at each of the ﬁrst order poles on C, the Aj’s becomes
semi-circles as ϵ →0+. Thus
Z
Aj
f(z) dz = ıπ Res(f(z), ζj)
for j = 1, . . . , n.
693

The principal value of the integral along C is
−
Z
C
f(z) dz = lim
ϵ→0+
Z
Cp
f(z) dz
= lim
ϵ→0+
 Z
Ci
f(z) dz −
n
X
j=1
Z
Aj
f(z) dz
!
= ı2π
 m
X
j=1
Res(f(z), zj) +
n
X
j=1
Res(f(z), ζj)
!
−ıπ
n
X
j=1
Res(f(z), ζj)
−
Z
C
f(z) dz = ı2π
m
X
j=1
Res(f(z), zj) + ıπ
n
X
j=1
Res(f(z), ζj).
Solution 13.13
Consider
−
Z
C
1
z −1 dz
where C is the unit circle. Let Cp be the circular arc of radius 1 that starts and ends a distance of ϵ from z = 1. Let
Cϵ be the negative, circular arc of radius ϵ with center at z = 1 that joins the endpoints of Cp. Let Ci, be the union of
Cp and Cϵ. (Cp stands for Principal value Contour; Ci stands for Indented Contour.) Ci is an indented contour that
avoids the ﬁrst order pole at z = 1. Figure 13.9 shows the three contours.
Figure 13.9: The Indented Contour.
Note that the principal value of the integral is
−
Z
C
1
z −1 dz = lim
ϵ→0+
Z
Cp
1
z −1 dz.
694

We can calculate the integral along Ci with Cauchy’s theorem. The integrand is analytic inside the contour.
Z
Ci
1
z −1 dz = 0
We can calculate the integral along Cϵ using Result 13.3.1. Note that as ϵ →0+, the contour becomes a semi-circle,
a circular arc of π radians in the negative direction.
lim
ϵ→0+
Z
Cϵ
1
z −1 dz = −ıπ Res

1
z −1, 1

= −ıπ
Now we can write the principal value of the integral along C in terms of the two known integrals.
−
Z
C
1
z −1 dz =
Z
Ci
1
z −1 dz −
Z
Cϵ
1
z −1 dz
= 0 −(−ıπ)
= ıπ
Integrals on the Real Axis
Solution 13.14
1. First we note that the integrand is an even function and extend the domain of integration.
Z ∞
0
x2
(x2 + 1)(x2 + 4) dx = 1
2
Z ∞
−∞
x2
(x2 + 1)(x2 + 4) dx
Next we close the path of integration in the upper half plane. Consider the integral along the boundary of the
695

domain 0 < r < R, 0 < θ < π.
1
2
Z
C
z2
(z2 + 1)(z2 + 4) dz = 1
2
Z
C
z2
(z −ı)(z + ı)(z −ı2)(z + ı2) dz
= ı2π1
2

Res

z2
(z2 + 1)(z2 + 4), z = ı

+ Res

z2
(z2 + 1)(z2 + 4), z = ı2
 
= ıπ

z2
(z + ı)(z2 + 4)

z=ı
+
z2
(z2 + 1)(z + ı2)

z=ı2

= ıπ
 ı
6 −ı
3

= π
6
Let CR be the circular arc portion of the contour.
R
C =
R R
−R +
R
CR. We show that the integral along CR vanishes
as R →∞with the maximum modulus bound.

Z
CR
z2
(z2 + 1)(z2 + 4) dz
 ≤πR max
z∈CR

z2
(z2 + 1)(z2 + 4)

= πR
R2
(R2 −1)(R2 −4)
→0 as R →∞
We take the limit as R →∞to evaluate the integral along the real axis.
lim
R→∞
1
2
Z R
−R
x2
(x2 + 1)(x2 + 4) dx = π
6
Z ∞
0
x2
(x2 + 1)(x2 + 4) dx = π
6
696

2. We close the path of integration in the upper half plane. Consider the integral along the boundary of the domain
0 < r < R, 0 < θ < π.
Z
C
dz
(z + b)2 + a2 =
Z
C
dz
(z + b −ıa)(z + b + ıa)
= ı2π Res

1
(z + b −ıa)(z + b + ıa), z = −b + ıa

= ı2π
1
z + b + ıa

z=−b+ıa
= π
a
Let CR be the circular arc portion of the contour.
R
C =
R R
−R +
R
CR. We show that the integral along CR vanishes
as R →∞with the maximum modulus bound.

Z
CR
dz
(z + b)2 + a2
 ≤πR max
z∈CR

1
(z + b)2 + a2

= πR
1
(R −b)2 + a2
→0 as R →∞
We take the limit as R →∞to evaluate the integral along the real axis.
lim
R→∞
Z R
−R
dx
(x + b)2 + a2 = π
a
Z ∞
−∞
dx
(x + b)2 + a2 = π
a
Solution 13.15
Let CR be the semicircular arc from R to −R in the upper half plane. Let C be the union of CR and the interval
697

[−R, R]. We can evaluate the principal value of the integral along C with Result 13.3.2.
−
Z
C
f(x) dx = ı2π
m
X
k=1
Res (f(z), zk) + ıπ
n
X
k=1
Res(f(z), xk)
We examine the integral along CR as R →∞.

Z
CR
f(z) dz
 ≤πR max
z∈CR |f(z)|
→0
as R →∞.
Now we are prepared to evaluate the real integral.
−
Z ∞
−∞
f(x) dx = lim
R→∞−
Z R
−R
f(x) dx
= lim
R→∞−
Z
C
f(z) dz
= ı2π
m
X
k=1
Res (f(z), zk) + ıπ
n
X
k=1
Res(f(z), xk)
If we close the path of integration in the lower half plane, the contour will be in the negative direction.
−
Z ∞
−∞
f(x) dx = −ı2π
m
X
k=1
Res (f(z), zk) −ıπ
n
X
k=1
Res(f(z), xk)
Solution 13.16
We consider
−
Z ∞
−∞
2x
x2 + x + 1 dx.
698

With the change of variables x = 1/ξ, this becomes
−
Z −∞
∞
2ξ−1
ξ−2 + ξ−1 + 1
−1
ξ2

dξ,
−
Z ∞
−∞
2ξ−1
ξ2 + ξ + 1 dξ
There are ﬁrst order poles at ξ = 0 and ξ = −1/2 ± ı
√
3/2. We close the path of integration in the upper half plane
with a semi-circle. Since the integrand decays like ξ−3 the integrand along the semi-circle vanishes as the radius tends
to inﬁnity. The value of the integral is thus
ıπ Res

2z−1
z2 + z + 1, z = 0

+ ı2π Res
 
2z−1
z2 + z + 1, z = −1
2 + ı
√
3
2
!
ıπ lim
z→0

2
z2 + z + 1

+ ı2π
lim
z→(−1+ı
√
3)/2

2z−1
z + (1 + ı
√
3)/2

−
Z ∞
−∞
2x
x2 + x + 1 dx = −2π
√
3
Solution 13.17
1. Consider
Z ∞
−∞
1
x4 + 1 dx.
The integrand
1
z4+1 is analytic on the real axis and has isolated singularities at the points z = {eıπ/4, eı3π/4, eı5π/4, eı7π/4}
Let CR be the semi-circle of radius R in the upper half plane. Since
lim
R→∞

R max
z∈CR

1
z4 + 1


= lim
R→∞

R
1
R4 −1

= 0,
699

we can apply Result 13.4.1.
Z ∞
−∞
1
x4 + 1 dx = ı2π

Res

1
z4 + 1, eıπ/4

+ Res

1
z4 + 1, eı3π/4

The appropriate residues are,
Res

1
z4 + 1, eıπ/4

=
lim
z→eıπ/4
z −eıπ/4
z4 + 1
=
lim
z→eıπ/4
1
4z3
= 1
4 e−ı3π/4
= −1 −ı
4
√
2 ,
Res

1
z4 + 1, eı3π/4

=
1
4(eı3π/4)3
= 1
4 e−ıπ/4
= 1 −ı
4
√
2 ,
We evaluate the integral with the residue theorem.
Z ∞
−∞
1
x4 + 1 dx = ı2π
−1 −ı
4
√
2
+ 1 −ı
4
√
2

Z ∞
−∞
1
x4 + 1 dx = π
√
2
700

2. Now consider
Z ∞
−∞
x2
(x2 + 1)2 dx.
The integrand is analytic on the real axis and has second order poles at z = ±ı. Since the integrand decays
suﬃciently fast at inﬁnity,
lim
R→∞

R max
z∈CR

z2
(z2 + 1)2


= lim
R→∞

R
R2
(R2 −1)2

= 0
we can apply Result 13.4.1.
Z ∞
−∞
x2
(x2 + 1)2 dx = ı2π Res

z2
(z2 + 1)2, z = ı

Res

z2
(z2 + 1)2, z = ı

= lim
z→ı
d
dz

(z −ı)2
z2
(z2 + 1)2

= lim
z→ı
d
dz

z2
(z + ı)2

= lim
z→ı
(z + ı)22z −z22(z + ı)
(z + ı)4

= −ı
4
Z ∞
−∞
x2
(x2 + 1)2 dx = π
2
3. Since
sin(x)
1 + x2
701

is an odd function,
Z ∞
−∞
cos(x)
1 + x2 dx =
Z ∞
−∞
eıx
1 + x2 dx
Since eız /(1 + z2) is analytic except for simple poles at z = ±ı and the integrand decays suﬃciently fast in the
upper half plane,
lim
R→∞

R max
z∈CR

eız
1 + z2


= lim
R→∞

R
1
R2 −1

= 0
we can apply Result 13.4.1.
Z ∞
−∞
eıx
1 + x2 dx = ı2π Res

eız
(z −ı)(z + ı), z = ı

= ı2πe−1
ı2
Z ∞
−∞
cos(x)
1 + x2 dx = π
e
Solution 13.18
Consider the function
f(z) =
z6
(z4 + 1)2.
The value of the function on the imaginary axis:
−y6
(y4 + 1)2
is a constant multiple of the value of the function on the real axis:
x6
(x4 + 1)2.
702

Thus to evaluate the real integral we consider the path of integration, C, which starts at the origin, follows the real
axis to R, follows a circular path to ıR and then follows the imaginary axis back down to the origin. f(z) has second
order poles at the fourth roots of −1: (±1 ± ı)/
√
2. Of these only (1 + ı)/
√
2 lies inside the path of integration. We
evaluate the contour integral with the Residue Theorem. For R > 1:
Z
C
z6
(z4 + 1)2 dz = ı2π Res

z6
(z4 + 1)2, z = eıπ/4

= ı2π
lim
z→eıπ/4
d
dz

(z −eıπ/4)2
z6
(z4 + 1)2

= ı2π
lim
z→eıπ/4
d
dz

z6
(z −eı3π/4)2(z −eı5π/4)2(z −eı7π/4)2

= ı2π
lim
z→eıπ/4

z6
(z −eı3π/4)2(z −eı5π/4)2(z −eı7π/4)2
6
z −
2
z −eı3π/4 −
2
z −eı5π/4 −
2
z −eı7π/4
 
= ı2π
−ı
(2)(ı4)(−2)
 
6
√
2
1 + ı −2
√
2 −2
√
2
2 + ı2 −
2
ı
√
2
!
= ı2π 3
32(1 −ı)
√
2
= 3π
8
√
2(1 + ı)
The integral along the circular part of the contour, CR, vanishes as R →∞. We demonstrate this with the maximum
703

modulus integral bound.

Z
CR
z6
(z4 + 1)2 dz
 ≤πR
4 max
z∈CR

z6
(z4 + 1)2

= πR
4
R6
(R4 −1)2
→0 as R →∞
Taking the limit R →∞, we have:
Z ∞
0
x6
(x4 + 1)2 dx +
Z 0
∞
(ıy)6
((ıy)4 + 1)2ı dy = 3π
8
√
2(1 + ı)
Z ∞
0
x6
(x4 + 1)2 dx + ı
Z ∞
0
y6
(y4 + 1)2 dy = 3π
8
√
2(1 + ı)
(1 + ı)
Z ∞
0
x6
(x4 + 1)2 dx = 3π
8
√
2(1 + ı)
Z ∞
0
x6
(x4 + 1)2 dx = 3π
8
√
2
Fourier Integrals
Solution 13.19
We know that
Z π
0
e−R sin θ dθ < π
R.
704

First take the case that ω is positive and the semi-circle is in the upper half plane.

Z
CR
f(z) eıωz dz
 ≤

Z
CR
eıωz dz
 max
z∈CR |f(z)|
≤
Z π
0
eıωR eıθ R eıθ dθ max
z∈CR |f(z)|
= R
Z π
0
e−ωR sin θ dθ max
z∈CR |f(z)|
< R π
ωR max
z∈CR |f(z)|
= π
ω max
z∈CR |f(z)|
→0
as R →∞
The procedure is almost the same for negative ω.
Solution 13.20
First we write the integral in terms of Fourier integrals.
Z ∞
−∞
cos 2x
x −ıπ dx =
Z ∞
−∞
eı2x
2(x −ıπ) dx +
Z ∞
−∞
e−ı2x
2(x −ıπ) dx
Note that
1
2(z−ıπ) vanishes as |z| →∞. We close the former Fourier integral in the upper half plane and the latter in
the lower half plane. There is a ﬁrst order pole at z = ıπ in the upper half plane.
Z ∞
−∞
eı2x
2(x −ıπ) dx = ı2π Res

eı2z
2(z −ıπ), z = ıπ

= ı2πe−2π
2
There are no singularities in the lower half plane.
Z ∞
−∞
e−ı2x
2(x −ıπ) dx = 0
705

Thus the value of the original real integral is
Z ∞
−∞
cos 2x
x −ıπ dx = ıπ e−2π
Fourier Cosine and Sine Integrals
Solution 13.21
We are considering the integral
Z ∞
−∞
sin x
x
dx.
The integrand is an entire function. So it doesn’t appear that the residue theorem would directly apply. Also the
integrand is unbounded as x →+ı∞and x →−ı∞, so closing the integral in the upper or lower half plane is not
directly applicable. In order to proceed, we must write the integrand in a diﬀerent form. Note that
−
Z ∞
−∞
cos x
x
dx = 0
since the integrand is odd and has only a ﬁrst order pole at x = 0. Thus
Z ∞
−∞
sin x
x
dx = −
Z ∞
−∞
eıx
ıx dx.
Let CR be the semicircular arc in the upper half plane from R to −R. Let C be the closed contour that is the union
of CR and the real interval [−R, R]. If we close the path of integration with a semicircular arc in the upper half plane,
we have
Z ∞
−∞
sin x
x
dx = lim
R→∞

−
Z
C
eız
ız dz −
Z
CR
eız
ız dz

,
provided that all the integrals exist.
The integral along CR vanishes as R →∞by Jordan’s lemma. By the residue theorem for principal values we have
−
Z eız
ız dz = ıπ Res
eız
ız , 0

= π.
706

Combining these results,
Z ∞
−∞
sin x
x
dx = π.
Solution 13.22
Note that (1 −cos x)/x2 has a removable singularity at x = 0. The integral decays like
1
x2 at inﬁnity, so the integral
exists. Since (sin x)/x2 is a odd function with a simple pole at x = 0, the principal value of its integral vanishes.
−
Z ∞
−∞
sin x
x2 dx = 0
Z ∞
−∞
1 −cos x
x2
dx = −
Z ∞
−∞
1 −cos x −ı sin x
x2
dx = −
Z ∞
−∞
1 −eıx
x2
dx
Let CR be the semi-circle of radius R in the upper half plane. Since
lim
R→∞

R max
z∈CR

1 −eız
z2


= lim
R→∞R 2
R2 = 0
the integral along CR vanishes as R →∞.
Z
CR
1 −eız
z2
dz →0 as R →∞
We can apply Result 13.4.1.
−
Z ∞
−∞
1 −eıx
x2
dx = ıπ Res
1 −eız
z2
, z = 0

= ıπ lim
z→0
1 −eız
z
= ıπ lim
z→0
−ı eız
1
Z ∞
−∞
1 −cos x
x2
dx = π
707

Solution 13.23
Consider
Z ∞
0
sin(πx)
x(1 −x2) dx.
Note that the integrand has removable singularities at the points x = 0, ±1 and is an even function.
Z ∞
0
sin(πx)
x(1 −x2) dx = 1
2
Z ∞
−∞
sin(πx)
x(1 −x2) dx.
Note that
cos(πx)
x(1 −x2) is an odd function with ﬁrst order poles at x = 0, ±1.
−
Z ∞
−∞
cos(πx)
x(1 −x2) dx = 0
Z ∞
0
sin(πx)
x(1 −x2) dx = −ı
2 −
Z ∞
−∞
eıπx
x(1 −x2) dx.
Let CR be the semi-circle of radius R in the upper half plane. Since
lim
R→∞

R max
z∈CR

eıπz
z(1 −z2)


= lim
R→∞R
1
R(R2 −1) = 0
the integral along CR vanishes as R →∞.
Z
CR
eıπz
z(1 −z2) dz →0 as R →∞
708

We can apply Result 13.4.1.
−ı
2 −
Z ∞
−∞
eıπx
x(1 −x2) dx = ıπ−ı
2

Res

eız
z(1 −z2), z = 0

+ Res

eız
z(1 −z2), z = 1

+ Res

eız
z(1 −z2), z = −1
 
= π
2

lim
z→0
eıπz
1 −z2 −lim
z→0
eıπz
z(1 + z) + lim
z→0
eıπz
z(1 −z)

= π
2

1 −−1
2 + −1
−2

Z ∞
0
sin(πx)
x(1 −x2) dx = π
Contour Integration and Branch Cuts
Solution 13.24
Let C be the boundary of the region ϵ < r < R, 0 < θ < π. Choose the branch of the logarithm with a branch cut
on the negative imaginary axis and the angle range −π/2 < θ < 3π/2. We consider the integral of log2 z/(1 + z2) on
this contour.
I
C
log2 z
1 + z2 dz = ı2π Res
 log2 z
1 + z2, z = ı

= ı2π lim
z→ı
log2 z
z + ı
= ı2π(ıπ/2)2
ı2
= −π3
4
709

Let CR be the semi-circle from R to −R in the upper half plane. We show that the integral along CR vanishes as
R →∞with the maximum modulus integral bound.

Z
CR
log2 z
1 + z2 dz
 ≤πR max
z∈CR

log2 z
1 + z2

≤πRln2 R + 2π ln R + π2
R2 −1
→0 as R →∞
Let Cϵ be the semi-circle from −ϵ to ϵ in the upper half plane. We show that the integral along Cϵ vanishes as ϵ →0
with the maximum modulus integral bound.

Z
Cϵ
log2 z
1 + z2 dz
 ≤πϵ max
z∈Cϵ

log2 z
1 + z2

≤πϵln2 ϵ −2π ln ϵ + π2
1 −ϵ2
→0 as ϵ →0
Now we take the limit as ϵ →0 and R →∞for the integral along C.
I
C
log2 z
1 + z2 dz = −π3
4
Z ∞
0
ln2 r
1 + r2 dr +
Z 0
∞
(ln r + ıπ)2
1 + r2
dr = −π3
4
2
Z ∞
0
ln2 x
1 + x2 dx + ı2π
Z ∞
0
ln x
1 + x2 dx = π2
Z ∞
0
1
1 + x2 dx −π3
4
(13.1)
We evaluate the integral of 1/(1 + x2) by extending the path of integration to (−∞. . . ∞) and closing the path of
integration in the upper half plane. Since
lim
R→∞

R max
z∈CR

1
1 + z2


≤lim
R→∞

R
1
R2 −1

= 0,
710

the integral of 1/(1 + z2) along CR vanishes as R →∞. We evaluate the integral with the Residue Theorem.
π2
Z ∞
0
1
1 + x2 dx = π2
2
Z ∞
−∞
1
1 + x2 dx
= π2
2 ı2π Res

1
1 + z2, z = ı

= ıπ3 lim
z→ı
1
z + ı
= π3
2
Now we return to Equation 13.1.
2
Z ∞
0
ln2 x
1 + x2 dx + ı2π
Z ∞
0
ln x
1 + x2 dx = π3
4
We equate the real and imaginary parts to solve for the desired integrals.
Z ∞
0
ln2 x
1 + x2 dx = π3
8
Z ∞
0
ln x
1 + x2 dx = 0
Solution 13.25
We consider the branch of the function
f(z) =
log z
z2 + 5z + 6
with a branch cut on the real axis and 0 < arg(z) < 2π.
Let Cϵ and CR denote the circles of radius ϵ and R where ϵ < 1 < R. Cϵ is negatively oriented; CR is positively
oriented. Consider the closed contour, C, that is traced by a point moving from ϵ to R above the branch cut, next
around CR back to R, then below the cut to ϵ, and ﬁnally around Cϵ back to ϵ. (See Figure 13.11.)
711

ε
CR
C
Figure 13.10: The path of integration.
We can evaluate the integral of f(z) along C with the residue theorem. For R > 3, there are ﬁrst order poles inside
the path of integration at z = −2 and z = −3.
Z
C
log z
z2 + 5z + 6 dz = ı2π

Res

log z
z2 + 5z + 6, z = −2

+ Res

log z
z2 + 5z + 6, z = −3

= ı2π

lim
z→−2
log z
z + 3 + lim
z→−3
log z
z + 2

= ı2π
log(−2)
1
+ log(−3)
−1

= ı2π (log(2) + ıπ −log(3) −ıπ)
= ı2π log
2
3

712

In the limit as ϵ →0, the integral along Cϵ vanishes. We demonstrate this with the maximum modulus theorem.

Z
Cϵ
log z
z2 + 5z + 6 dz
 ≤2πϵ max
z∈Cϵ

log z
z2 + 5z + 6

≤2πϵ 2π −log ϵ
6 −5ϵ −ϵ2
→0 as ϵ →0
In the limit as R →∞, the integral along CR vanishes. We again demonstrate this with the maximum modulus
theorem.

Z
CR
log z
z2 + 5z + 6 dz
 ≤2πR max
z∈CR

log z
z2 + 5z + 6

≤2πR log R + 2π
R2 −5R −6
→0 as R →∞
Taking the limit as ϵ →0 and R →∞, the integral along C is:
Z
C
log z
z2 + 5z + 6 dz =
Z ∞
0
log x
x2 + 5x + 6 dx +
Z 0
∞
log x + ı2π
x2 + 5x + 6 dx
= −ı2π
Z ∞
0
log x
x2 + 5x + 6 dx
Now we can evaluate the real integral.
−ı2π
Z ∞
0
log x
x2 + 5x + 6 dx = ı2π log
2
3

Z ∞
0
log x
x2 + 5x + 6 dx = log
3
2

713

Solution 13.26
We consider the integral
I(a) =
Z ∞
0
xa
(x + 1)2 dx.
To examine convergence, we split the domain of integration.
Z ∞
0
xa
(x + 1)2 dx =
Z 1
0
xa
(x + 1)2 dx +
Z ∞
1
xa
(x + 1)2 dx
First we work with the integral on (0 . . . 1).

Z 1
0
xa
(x + 1)2 dx
 ≤
Z 1
0

xa
(x + 1)2
 |dx|
=
Z 1
0
xℜ(a)
(x + 1)2 dx
≤
Z 1
0
xℜ(a) dx
This integral converges for ℜ(a) > −1.
Next we work with the integral on (1 . . . ∞).

Z ∞
1
xa
(x + 1)2 dx
 ≤
Z ∞
1

xa
(x + 1)2
 |dx|
=
Z ∞
1
xℜ(a)
(x + 1)2 dx
≤
Z ∞
1
xℜ(a)−2 dx
This integral converges for ℜ(a) < 1.
714

Thus we see that the integral deﬁning I(a) converges in the strip, −1 < ℜ(a) < 1. The integral converges uniformly
in any closed subset of this domain. Uniform convergence means that we can diﬀerentiate the integral with respect to
a and interchange the order of integration and diﬀerentiation.
I′(a) =
Z ∞
0
xa log x
(x + 1)2 dx
Thus we see that I(a) is analytic for −1 < ℜ(a) < 1.
For −1 < ℜ(a) < 1 and a ̸= 0, za is multi-valued. Consider the branch of the function f(z) = za/(z + 1)2 with a
branch cut on the positive real axis and 0 < arg(z) < 2π. We integrate along the contour in Figure 13.11.
The integral on Cϵ vanishes as ϵ →0. We show this with the maximum modulus integral bound. First we write za
in modulus-argument form, z = ϵ eıθ, where a = α + ıβ.
za = ea log z
= e(α+ıβ)(ln ϵ+ıθ)
= eα ln ϵ−βθ+ı(β ln ϵ+αθ)
= ϵα e−βθ eı(β log ϵ+αθ)
Now we bound the integral.

Z
Cϵ
za
(z + 1)2 dz
 ≤2πϵ max
z∈Cϵ

za
(z + 1)2

≤2πϵ ϵα e2π|β|
(1 −ϵ)2
→0 as ϵ →0
715

The integral on CR vanishes as R →∞.

Z
CR
za
(z + 1)2 dz
 ≤2πR max
z∈CR

za
(z + 1)2

≤2πR Rα e2π|β|
(R −1)2
→0 as R →∞
Above the branch cut, (z = r eı0), the integrand is
f(r eı0) =
ra
(r + 1)2.
Below the branch cut, (z = r eı2π), we have,
f(r eı2π) = eı2πa ra
(r + 1)2.
Now we use the residue theorem.
Z ∞
0
ra
(r + 1)2 dr +
Z 0
∞
eı2πa ra
(r + 1)2 dr = ı2π Res

za
(z + 1)2, −1

 1 −eı2πa Z ∞
0
ra
(r + 1)2 dr = ı2π lim
z→−1
d
dz(za)
Z ∞
0
ra
(r + 1)2 dr = ı2πa eıπ(a−1)
1 −eı2πa
Z ∞
0
ra
(r + 1)2 dr =
−ı2πa
e−ıπa −eıπa
Z ∞
0
xa
(x + 1)2 dx =
πa
sin(πa)
for −1 < ℜ(a) < 1, a ̸= 0
716

The right side has a removable singularity at a = 0. We use analytic continuation to extend the answer to a = 0.
I(a) =
Z ∞
0
xa
(x + 1)2 dx =
(
πa
sin(πa)
for −1 < ℜ(a) < 1, a ̸= 0
1
for a = 0
We can derive the last two integrals by diﬀerentiating this formula with respect to a and taking the limit a →0.
I′(a) =
Z ∞
0
xa log x
(x + 1)2 dx,
I′′(a) =
Z ∞
0
xa log2 x
(x + 1)2 dx
I′(0) =
Z ∞
0
log x
(x + 1)2 dx,
I′′(0) =
Z ∞
0
log2 x
(x + 1)2 dx
We can ﬁnd I′(0) and I′′(0) either by diﬀerentiating the expression for I(a) or by ﬁnding the ﬁrst few terms in the
Taylor series expansion of I(a) about a = 0. The latter approach is a little easier.
I(a) =
∞
X
n=0
I(n)(0)
n!
an
I(a) =
πa
sin(πa)
=
πa
πa −(πa)3/6 + O(a5)
=
1
1 −(πa)2/6 + O(a4)
= 1 + π2a2
6
+ O(a4)
I′(0) =
Z ∞
0
log x
(x + 1)2 dx = 0
I′′(0) =
Z ∞
0
log2 x
(x + 1)2 dx = π2
3
717

Solution 13.27
1. We consider the integral
I(a) =
Z ∞
0
xa
1 + x2 dx.
To examine convergence, we split the domain of integration.
Z ∞
0
xa
1 + x2 dx =
Z 1
0
xa
1 + x2 dx +
Z ∞
1
xa
1 + x2 dx
First we work with the integral on (0 . . . 1).

Z 1
0
xa
1 + x2 dx
 ≤
Z 1
0

xa
1 + x2
 |dx|
=
Z 1
0
xℜ(a)
1 + x2 dx
≤
Z 1
0
xℜ(a) dx
This integral converges for ℜ(a) > −1.
Next we work with the integral on (1 . . . ∞).

Z ∞
1
xa
1 + x2 dx
 ≤
Z ∞
1

xa
1 + x2
 |dx|
=
Z ∞
1
xℜ(a)
1 + x2 dx
≤
Z ∞
1
xℜ(a)−2 dx
This integral converges for ℜ(a) < 1.
718

Thus we see that the integral deﬁning I(a) converges in the strip, −1 < ℜ(a) < 1. The integral converges
uniformly in any closed subset of this domain. Uniform convergence means that we can diﬀerentiate the integral
with respect to a and interchange the order of integration and diﬀerentiation.
I′(a) =
Z ∞
0
xa log x
1 + x2 dx
Thus we see that I(a) is analytic for −1 < ℜ(a) < 1.
2. For −1 < ℜ(a) < 1 and a ̸= 0, za is multi-valued. Consider the branch of the function f(z) = za/(1 + z2) with
a branch cut on the positive real axis and 0 < arg(z) < 2π. We integrate along the contour in Figure 13.11.
ε
CR
C
Figure 13.11:
The integral on Cρ vanishes are ρ →0. We show this with the maximum modulus integral bound. First we write
za in modulus-argument form, where z = ρ eıθ and a = α + ıβ.
za = ea log z
= e(α+ıβ)(log ρ+ıθ)
= eα log ρ−βθ+ı(β log ρ+αθ)
= ρa e−βθ eı(β log ρ+αθ)
719

Now we bound the integral.

Z
Cρ
za
1 + z2 dz
 ≤2πρ max
z∈Cρ

za
1 + z2

≤2πρρα e2π|β|
1 −ρ2
→0 as ρ →0
The integral on CR vanishes as R →∞.

Z
CR
za
1 + z2 dz
 ≤2πR max
z∈CR

za
1 + z2

≤2πRRα e2π|β|
R2 −1
→0 as R →∞
Above the branch cut, (z = r eı0), the integrand is
f(r eı0) =
ra
1 + r2.
Below the branch cut, (z = r eı2π), we have,
f(r eı2π) = eı2πa ra
1 + r2 .
720

Now we use the residue theorem.
Z ∞
0
ra
1 + r2 dr +
Z 0
∞
eı2πa ra
1 + r2 dr = ı2π

Res

za
1 + z2, ı

+ Res

za
1 + z2, −ı

 1 −eı2πa Z ∞
0
xa
1 + x2 dx = ı2π

lim
z→ı
za
z + ı + lim
z→−ı
za
z −ı

 1 −eı2πa Z ∞
0
xa
1 + x2 dx = ı2π
eıaπ/2
ı2
+ eıa3π/2
−ı2

Z ∞
0
xa
1 + x2 dx = πeıaπ/2 −eıa3π/2
1 −eı2aπ
Z ∞
0
xa
1 + x2 dx = π
eıaπ/2(1 −eıaπ)
(1 + eıaπ)(1 −eıaπ)
Z ∞
0
xa
1 + x2 dx =
π
e−ıaπ/2 + eıaπ/2
Z ∞
0
xa
1 + x2 dx =
π
2 cos(πa/2)
for −1 < ℜ(a) < 1, a ̸= 0
We use analytic continuation to extend the answer to a = 0.
I(a) =
Z ∞
0
xa
1 + x2 dx =
π
2 cos(πa/2)
for −1 < ℜ(a) < 1
3. We can derive the last two integrals by diﬀerentiating this formula with respect to a and taking the limit a →0.
I′(a) =
Z ∞
0
xa log x
1 + x2 dx,
I′′(a) =
Z ∞
0
xa log2 x
1 + x2 dx
I′(0) =
Z ∞
0
log x
1 + x2 dx,
I′′(0) =
Z ∞
0
log2 x
1 + x2 dx
721

We can ﬁnd I′(0) and I′′(0) either by diﬀerentiating the expression for I(a) or by ﬁnding the ﬁrst few terms in
the Taylor series expansion of I(a) about a = 0. The latter approach is a little easier.
I(a) =
∞
X
n=0
I(n)(0)
n!
an
I(a) =
π
2 cos(πa/2)
= π
2
1
1 −(πa/2)2/2 + O(a4)
= π
2
 1 + (πa/2)2/2 + O(a4)

= π
2 + π3/8
2
a2 + O(a4)
I′(0) =
Z ∞
0
log x
1 + x2 dx = 0
I′′(0) =
Z ∞
0
log2 x
1 + x2 dx = π3
8
Solution 13.28
Convergence. If xaf(x) ≪xα as x →0 for some α > −1 then the integral
Z 1
0
xaf(x) dx
will converge absolutely. If xaf(x) ≪xβ as x →∞for some β < −1 then the integral
Z ∞
1
xaf(x)
722

will converge absolutely. These are suﬃcient conditions for the absolute convergence of
Z ∞
0
xaf(x) dx.
Contour Integration. We put a branch cut on the positive real axis and choose 0 < arg(z) < 2π. We consider
the integral of zaf(z) on the contour in Figure 13.11. Let the singularities of f(z) occur at z1, . . . , zn. By the residue
theorem,
Z
C
zaf(z) dz = ı2π
n
X
k=1
Res (zaf(z), zk) .
On the circle of radius ϵ, the integrand is o(ϵ−1). Since the length of Cϵ is 2πϵ, the integral on Cϵ vanishes as
ϵ →0. On the circle of radius R, the integrand is o(R−1). Since the length of CR is 2πR, the integral on CR vanishes
as R →∞.
The value of the integrand below the branch cut, z = x eı2π, is
f(x eı2π) = xa eı2πa f(x)
In the limit as ϵ →0 and R →∞we have
Z ∞
0
xaf(x) dx +
Z 0
−∞
xa eı2πa f(x) dx = ı2π
n
X
k=1
Res (zaf(z), zk) .
Z ∞
0
xaf(x) dx =
ı2π
1 −eı2πa
n
X
k=1
Res (zaf(z), zk) .
Solution 13.29
In the interval of uniform convergence of th integral, we can diﬀerentiate the formula
Z ∞
0
xaf(x) dx =
ı2π
1 −eı2πa
n
X
k=1
Res (zaf(z), zk) ,
723

with respect to a to obtain,
Z ∞
0
xaf(x) log x dx =
ı2π
1 −eı2πa
n
X
k=1
Res (zaf(z) log z, zk) , −4π2a eı2πa
(1 −eı2πa)2
n
X
k=1
Res (zaf(z), zk) .
Z ∞
0
xaf(x) log x dx =
ı2π
1 −eı2πa
n
X
k=1
Res (zaf(z) log z, zk) , +
π2a
sin2(πa)
n
X
k=1
Res (zaf(z), zk) ,
Diﬀerentiating the solution of Exercise 13.26 m times with respect to a yields
Z ∞
0
xaf(x) logm x dx = ∂m
∂am
 
ı2π
1 −eı2πa
n
X
k=1
Res (zaf(z), zk)
!
,
Solution 13.30
Taking the limit as a →0 ∈Z in the solution of Exercise 13.26 yields
Z ∞
0
f(x) dx = ı2π lim
a→0
Pn
k=1 Res (zaf(z), zk)
1 −eı2πa

The numerator vanishes because the sum of all residues of znf(z) is zero. Thus we can use L’Hospital’s rule.
Z ∞
0
f(x) dx = ı2π lim
a→0
Pn
k=1 Res (zaf(z) log z, zk)
−ı2π eı2πa

Z ∞
0
f(x) dx = −
n
X
k=1
Res (f(z) log z, zk)
This suggests that we could have derived the result directly by considering the integral of f(z) log z on the contour in
Figure 13.11. We put a branch cut on the positive real axis and choose the branch arg z = 0. Recall that we have
724

assumed that f(z) has only isolated singularities and no singularities on the positive real axis, [0, ∞). By the residue
theorem,
Z
C
f(z) log z dz = ı2π
n
X
k=1
Res (f(z) log z, z = zk) .
By assuming that f(z) ≪zα as z →0 where α > −1 the integral on Cϵ will vanish as ϵ →0. By assuming that
f(z) ≪zβ as z →∞where β < −1 the integral on CR will vanish as R →∞. The value of the integrand below the
branch cut, z = x eı2π is f(x)(log x + ı2π). Taking the limit as ϵ →0 and R →∞, we have
Z ∞
0
f(x) log x dx +
Z 0
∞
f(x)(log x + ı2π) dx = ı2π
n
X
k=1
Res (f(z) log z, zk) .
Thus we corroborate the result.
Z ∞
0
f(x) dx = −
n
X
k=1
Res (f(z) log z, zk)
Solution 13.31
Consider the integral of f(z) log2 z on the contour in Figure 13.11. We put a branch cut on the positive real axis and
choose the branch 0 < arg z < 2π. Let z1, . . . zn be the singularities of f(z). By the residue theorem,
Z
C
f(z) log2 z dz = ı2π
n
X
k=1
Res
 f(z) log2 z, zk

.
If f(z) ≪zα as z →0 for some α > −1 then the integral on Cϵ will vanish as ϵ →0. f(z) ≪zβ as z →∞for some
β < −1 then the integral on CR will vanish as R →∞. Below the branch cut the integrand is f(x)(log x + ı2π)2.
Thus we have
Z ∞
0
f(x) log2 x dx +
Z 0
∞
f(x)(log2 x + ı4π log x −4π2) dx = ı2π
n
X
k=1
Res
 f(z) log2 z, zk

.
−ı4π
Z ∞
0
f(x) log x dx + 4π2
Z ∞
0
f(x) dx = ı2π
n
X
k=1
Res
 f(z) log2 z, zk

.
725

Z ∞
0
f(x) log x dx = −1
2
n
X
k=1
Res
 f(z) log2 z, zk

+ ıπ
n
X
k=1
Res (f(z) log z, zk)
Solution 13.32
Convergence. We consider
Z ∞
0
xa
1 + x4 dx.
Since the integrand behaves like xa near x = 0 we must have ℜ(a) > −1. Since the integrand behaves like xa−4 at
inﬁnity we must have ℜ(a −4) < −1. The integral converges for −1 < ℜ(a) < 3.
Contour Integration. The function
f(z) =
za
1 + z4
has ﬁrst order poles at z = (±1 ± ı)/
√
2 and a branch point at z = 0. We could evaluate the real integral by putting
a branch cut on the positive real axis with 0 < arg(z) < 2π and integrating f(z) on the contour in Figure 13.12.
CR
Cε
Figure 13.12: Possible Path of Integration for f(z) =
za
1+z4
Integrating on this contour would work because the value of the integrand below the branch cut is a constant times
the value of the integrand above the branch cut. After demonstrating that the integrals along Cϵ and CR vanish in the
726

limits as ϵ →0 and R →∞we would see that the value of the integral is a constant times the sum of the residues at
the four poles. However, this is not the only, (and not the best), contour that can be used to evaluate the real integral.
Consider the value of the integral on the line arg(z) = θ.
f(r eıθ) =
ra eıaθ
1 + r4 eı4θ
If θ is a integer multiple of π/2 then the integrand is a constant multiple of
f(x) =
ra
1 + r4.
Thus any of the contours in Figure 13.13 can be used to evaluate the real integral. The only diﬀerence is how many
residues we have to calculate. Thus we choose the ﬁrst contour in Figure 13.13. We put a branch cut on the negative
real axis and choose the branch −π < arg(z) < π to satisfy f(1) = 1.
C
C
C
C
C
C
R
ε
ε
R
ε
R
Figure 13.13: Possible Paths of Integration for f(z) =
za
1+z4
We evaluate the integral along C with the Residue Theorem.
Z
C
za
1 + z4 dz = ı2π Res

za
1 + z4, z = 1 + ı
√
2

727

Let a = α + ıβ and z = r eıθ. Note that
|za| = |(r eıθ)α+ıβ| = rα e−βθ .
The integral on Cϵ vanishes as ϵ →0. We demonstrate this with the maximum modulus integral bound.

Z
Cϵ
za
1 + z4 dz
 ≤πϵ
2 max
z∈Cϵ

za
1 + z4

≤πϵ
2
ϵα eπ|β|/2
1 −ϵ4
→0
as ϵ →0
The integral on CR vanishes as R →∞.

Z
CR
za
1 + z4 dz
 ≤πR
2 max
z∈CR

za
1 + z4

≤πR
2
Rα eπ|β|/2
R4 −1
→0
as R →∞
The value of the integrand on the positive imaginary axis, z = x eıπ/2, is
(x eıπ/2)a
1 + (x eıπ/2)4 = xa eıπa/2
1 + x4 .
728

We take the limit as ϵ →0 and R →∞.
Z ∞
0
xa
1 + x4 dx +
Z 0
∞
xa eıπa/2
1 + x4 eıπ/2 dx = ı2π Res

za
1 + z4, eıπ/4

 1 −eıπ(a+1)/2 Z ∞
0
xa
1 + x4 dx = ı2π
lim
z→eıπ/4
za(z −eıπ/2)
1 + z4

Z ∞
0
xa
1 + x4 dx =
ı2π
1 −eıπ(a+1)/2
lim
z→eıπ/4
aza(z −eıπ/2) + za
4z3

Z ∞
0
xa
1 + x4 dx =
ı2π
1 −eıπ(a+1)/2
eıπa/4
4 eı3π/4
Z ∞
0
xa
1 + x4 dx =
−ıπ
2(e−ıπ(a+1)/4 −eıπ(a+1)/4)
Z ∞
0
xa
1 + x4 dx = π
4 csc
π(a + 1)
4

Solution 13.33
Consider the branch of f(z) = z1/2 log z/(z + 1)2 with a branch cut on the positive real axis and 0 < arg z < 2π. We
integrate this function on the contour in Figure 13.11.
We use the maximum modulus integral bound to show that the integral on Cρ vanishes as ρ →0.

Z
Cρ
z1/2 log z
(z + 1)2 dz
 ≤2πρ max
Cρ

z1/2 log z
(z + 1)2

= 2πρρ1/2(2π −log ρ)
(1 −ρ)2
→0 as ρ →0
729

The integral on CR vanishes as R →∞.

Z
CR
z1/2 log z
(z + 1)2 dz
 ≤2πR max
CR

z1/2 log z
(z + 1)2

= 2πRR1/2(log R + 2π)
(R −1)2
→0 as R →∞
Above the branch cut, (z = x eı0), the integrand is,
f(x eı0) = x1/2 log x
(x + 1)2 .
Below the branch cut, (z = x eı2π ), we have,
f(x eı2π) = −x1/2(log x + ıπ)
(x + 1)2
.
Taking the limit as ρ →0 and R →∞, the residue theorem gives us
Z ∞
0
x1/2 log x
(x + 1)2 dx +
Z 0
∞
−x1/2(log x + ı2π)
(x + 1)2
dx = ı2π Res
z1/2 log z
(z + 1)2 , −1

.
2
Z ∞
0
x1/2 log x
(x + 1)2 dx + ı2π
Z ∞
0
x1/2
(x + 1)2 dx = ı2π lim
z→−1
d
dz(z1/2 log z)
2
Z ∞
0
x1/2 log x
(x + 1)2 dx + ı2π
Z ∞
0
x1/2
(x + 1)2 dx = ı2π lim
z→−1
1
2z−1/2 log z + z1/21
z

2
Z ∞
0
x1/2 log x
(x + 1)2 dx + ı2π
Z ∞
0
x1/2
(x + 1)2 dx = ı2π
1
2(−ı)(ıπ) −ı

730

2
Z ∞
0
x1/2 log x
(x + 1)2 dx + ı2π
Z ∞
0
x1/2
(x + 1)2 dx = 2π + ıπ2
Equating real and imaginary parts,
Z ∞
0
x1/2 log x
(x + 1)2 dx = π,
Z ∞
0
x1/2
(x + 1)2 dx = π
2 .
Exploiting Symmetry
Solution 13.34
Convergence. The integrand,
eaz
ez −e−z =
eaz
2 sinh(z),
has ﬁrst order poles at z = ınπ, n ∈Z. To study convergence, we split the domain of integration.
Z ∞
−∞
=
Z −1
−∞
+
Z 1
−1
+
Z ∞
1
The principal value integral
−
Z 1
−1
eax
ex −e−x dx
exists for any a because the integrand has only a ﬁrst order pole on the path of integration.
Now consider the integral on (1 . . . ∞).

Z ∞
1
eax
ex −e−x dx
 =
Z ∞
1
e(a−1)x
1 −e−2x dx
≤
1
1 −e−2
Z ∞
1
e(a−1)x dx
This integral converges for a −1 < 0; a < 1.
731

Finally consider the integral on (−∞. . . −1).

Z −1
−∞
eax
ex −e−x dx
 =
Z −1
−∞
e(a+1)x
1 −e2x dx
≤
1
1 −e−2
Z −1
−∞
e(a+1)x dx
This integral converges for a + 1 > 0; a > −1.
Thus we see that the integral for I(a) converges for real a, |a| < 1.
Choice of Contour. Consider the contour C that is the boundary of the region: −R < x < R, 0 < y < π. The
integrand has no singularities inside the contour. There are ﬁrst order poles on the contour at z = 0 and z = ıπ. The
value of the integral along the contour is ıπ times the sum of these two residues.
The integrals along the vertical sides of the contour vanish as R →∞.

Z R+ıπ
R
eaz
ez −e−z dz
 ≤π
max
z∈(R...R+ıπ)

eaz
ez −e−z

≤π
eaR
eR −e−R
→0 as R →∞

Z −R+ıπ
−R
eaz
ez −e−z dz
 ≤π
max
z∈(−R...−R+ıπ)

eaz
ez −e−z

≤π
e−aR
e−R −eR
→0 as R →∞
732

Evaluating the Integral. We take the limit as R →∞and apply the residue theorem.
Z ∞
−∞
eax
ex −e−x dx +
Z −∞+ıπ
∞+ıπ
eaz
ez −e−z dz
= ıπ Res

eaz
ez −e−z , z = 0

+ ıπ Res

eaz
ez −e−z , z = ıπ

Z ∞
−∞
eax
ex −e−x dx +
Z −∞
∞
ea(x+ıπ
ex+ıπ −e−x−ıπ dz = ıπ lim
z→0
z eaz
2 sinh(z) + ıπ lim
z→ıπ
(z −ıπ) eaz
2 sinh(z)
(1 + eıaπ)
Z ∞
−∞
eax
ex −e−x dx = ıπ lim
z→0
eaz +az eaz
2 cosh(z) + ıπ lim
z→ıπ
eaz +a(z −ıπ) eaz
2 cosh(z)
(1 + eıaπ)
Z ∞
−∞
eax
ex −e−x dx = ıπ1
2 + ıπeıaπ
−2
Z ∞
−∞
eax
ex −e−x dx = ıπ(1 −eıaπ)
2(1 + eıaπ)
Z ∞
−∞
eax
ex −e−x dx = π
2
ı(e−ıaπ/2 −eıaπ/2)
eıaπ/2 + eıaπ/2
Z ∞
−∞
eax
ex −e−x dx = π
2 tan
aπ
2

Solution 13.35
1.
Z ∞
0
dx
(1 + x2)2 = 1
2
Z ∞
−∞
dx
(1 + x2)2
We apply Result 13.4.1 to the integral on the real axis. First we verify that the integrand vanishes fast enough
in the upper half plane.
lim
R→∞

R max
z∈CR

1
(1 + z2)2


= lim
R→∞

R
1
(R2 −1)2

= 0
733

Then we evaluate the integral with the residue theorem.
Z ∞
−∞
dx
(1 + x2)2 = ı2π Res

1
(1 + z2)2, z = ı

= ı2π Res

1
(z −ı)2(z + ı)2, z = ı

= ı2π lim
z→ı
d
dz
1
(z + ı)2
= ı2π lim
z→ı
−2
(z + ı)3
= π
2
Z ∞
0
dx
(1 + x2)2 = π
4
2. We wish to evaluate
Z ∞
0
dx
x3 + 1.
Let the contour C be the boundary of the region 0 < r < R, 0 < θ < 2π/3. We factor the denominator of the
integrand to see that the contour encloses the simple pole at eıπ/3 for R > 1.
z3 + 1 = (z −eıπ/3)(z + 1)(z −e−ıπ/3)
734

We calculate the residue at that point.
Res

1
z3 + 1, z = eıπ/3

=
lim
z→eıπ/3

(z −eıπ/3)
1
z3 + 1

=
lim
z→eıπ/3

1
(z + 1)(z −e−ıπ/3)

=
1
(eıπ/3 +1)(eıπ/3 −e−ıπ/3)
= −eıπ/3
3
We use the residue theorem to evaluate the integral.
I
C
dz
z3 + 1 = −ı2π eıπ/3
3
Let CR be the circular arc portion of the contour.
Z
C
dz
z3 + 1 =
Z R
0
dx
x3 + 1 +
Z
CR
dz
z3 + 1 −
Z R
0
eı2π/3 dx
x3 + 1
= (1 + e−ıπ/3)
Z R
0
dx
x3 + 1 +
Z
CR
dz
z3 + 1
We show that the integral along CR vanishes as R →∞with the maximum modulus integral bound.

Z
CR
dz
z3 + 1
 ≤2πR
3
1
R3 −1 →0
as R →∞
We take R →∞and solve for the desired integral.
 1 + e−ıπ/3 Z ∞
0
dx
x3 + 1 = −ı2π eıπ/3
3
Z ∞
0
dx
x3 + 1 = 2π
3
√
3
735

Solution 13.36
Method 1: Semi-Circle Contour. We wish to evaluate the integral
I =
Z ∞
0
dx
1 + x6.
We note that the integrand is an even function and express I as an integral over the whole real axis.
I = 1
2
Z ∞
−∞
dx
1 + x6
Now we will evaluate the integral using contour integration. We close the path of integration in the upper half plane.
Let ΓR be the semicircular arc from R to −R in the upper half plane. Let Γ be the union of ΓR and the interval
[−R, R]. (See Figure 13.14.)
Figure 13.14: The semi-circle contour.
We can evaluate the integral along Γ with the residue theorem. The integrand has ﬁrst order poles at z = eıπ(1+2k)/6,
736

k = 0, 1, 2, 3, 4, 5. Three of these poles are in the upper half plane. For R > 1, we have
Z
Γ
1
z6 + 1 dz = ı2π
2
X
k=0
Res

1
z6 + 1, eıπ(1+2k)/6

= ı2π
2
X
k=0
lim
z→eıπ(1+2k)/6
z −eıπ(1+2k)/6
z6 + 1
Since the numerator and denominator vanish, we apply L’Hospital’s rule.
= ı2π
2
X
k=0
lim
z→eıπ(1+2k)/6
1
6z5
= ıπ
3
2
X
k=0
e−ıπ5(1+2k)/6
= ıπ
3
 e−ıπ5/6 + e−ıπ15/6 + e−ıπ25/6
= ıπ
3
 e−ıπ5/6 + e−ıπ/2 + e−ıπ/6
= ıπ
3
 
−
√
3 −ı
2
−ı +
√
3 −ı
2
!
= 2π
3
Now we examine the integral along ΓR. We use the maximum modulus integral bound to show that the value of the
737

integral vanishes as R →∞.

Z
ΓR
1
z6 + 1 dz
 ≤πR max
z∈ΓR

1
z6 + 1

= πR
1
R6 −1
→0
as R →∞.
Now we are prepared to evaluate the original real integral.
Z
Γ
1
z6 + 1 dz = 2π
3
Z R
−R
1
x6 + 1 dx +
Z
ΓR
1
z6 + 1 dz = 2π
3
We take the limit as R →∞.
Z ∞
−∞
1
x6 + 1 dx = 2π
3
Z ∞
0
1
x6 + 1 dx = π
3
We would get the same result by closing the path of integration in the lower half plane. Note that in this case the
closed contour would be in the negative direction.
Method 2: Wedge Contour. Consider the contour Γ, which starts at the origin, goes to the point R along the
real axis, then to the point R eıπ/3 along a circle of radius R and then back to the origin along the ray θ = π/3. (See
Figure 13.15.)
We can evaluate the integral along Γ with the residue theorem. The integrand has one ﬁrst order pole inside the
738

Figure 13.15: The wedge contour.
contour at z = eıπ/6. For R > 1, we have
Z
Γ
1
z6 + 1 dz = ı2π Res

1
z6 + 1, eıπ/6

= ı2π
lim
z→eıπ/6
z −eıπ/6
z6 + 1
Since the numerator and denominator vanish, we apply L’Hospital’s rule.
= ı2π
lim
z→eıπ/6
1
6z5
= ıπ
3 e−ıπ5/6
= π
3 e−ıπ/3
Now we examine the integral along the circular arc, ΓR. We use the maximum modulus integral bound to show that
739

the value of the integral vanishes as R →∞.

Z
ΓR
1
z6 + 1 dz
 ≤πR
3 max
z∈ΓR

1
z6 + 1

= πR
3
1
R6 −1
→0
as R →∞.
Now we are prepared to evaluate the original real integral.
Z
Γ
1
z6 + 1 dz = π
3 e−ıπ/3
Z R
0
1
x6 + 1 dx +
Z
ΓR
1
z6 + 1 dz +
Z 0
R eıπ/3
1
z6 + 1 dz = π
3 e−ıπ/3
Z R
0
1
x6 + 1 dx +
Z
ΓR
1
z6 + 1 dz +
Z 0
R
1
x6 + 1 eıπ/3 dx = π
3 e−ıπ/3
We take the limit as R →∞.
 1 −eıπ/3 Z ∞
0
1
x6 + 1 dx = π
3 e−ıπ/3
Z ∞
0
1
x6 + 1 dx = π
3
e−ıπ/3
1 −eıπ/3
Z ∞
0
1
x6 + 1 dx = π
3
(1 −ı
√
3)/2
1 −(1 + ı
√
3)/2
Z ∞
0
1
x6 + 1 dx = π
3
Solution 13.37
First note that
cos(2θ) ≥1 −4
πθ,
0 ≤θ ≤π
4 .
740

These two functions are plotted in Figure 13.16. To prove this inequality analytically, note that the two functions are
equal at the endpoints of the interval and that cos(2θ) is concave downward on the interval,
d2
dθ2 cos(2θ) = −4 cos(2θ) ≤0
for 0 ≤θ ≤π
4 ,
while 1 −4θ/π is linear.
Figure 13.16: cos(2θ) and 1 −4
πθ
Let CR be the quarter circle of radius R from θ = 0 to θ = π/4. The integral along this contour vanishes as
R →∞.

Z
CR
e−z2 dz
 ≤
Z π/4
0
e−(R eıθ)2
Rı eıθ dθ
≤
Z π/4
0
R e−R2 cos(2θ) dθ
≤
Z π/4
0
R e−R2(1−4θ/π) dθ
=
h
R π
4R2 e−R2(1−4θ/π)iπ/4
0
= π
4R

1 −e−R2
→0 as R →∞
741

Let C be the boundary of the domain 0 < r < R, 0 < θ < π/4. Since the integrand is analytic inside C the integral
along C is zero. Taking the limit as R →∞, the integral from r = 0 to ∞along θ = 0 is equal to the integral from
r = 0 to ∞along θ = π/4.
Z ∞
0
e−x2 dx =
Z ∞
0
e−

1+ı
√
2 x
2 1 + ı
√
2 dx
Z ∞
0
e−x2 dx = 1 + ı
√
2
Z ∞
0
e−ıx2 dx
Z ∞
0
e−x2 dx = 1 + ı
√
2
Z ∞
0
 cos(x2) −ı sin(x2)

dx
Z ∞
0
e−x2 dx = 1
√
2
Z ∞
0
cos(x2) dx +
Z ∞
0
sin(x2) dx

+
ı
√
2
Z ∞
0
cos(x2) dx −
Z ∞
0
sin(x2) dx

We equate the imaginary part of this equation to see that the integrals of cos(x2) and sin(x2) are equal.
Z ∞
0
cos(x2) dx =
Z ∞
0
sin(x2) dx
The real part of the equation then gives us the desired identity.
Z ∞
0
cos(x2) dx =
Z ∞
0
sin(x2) dx = 1
√
2
Z ∞
0
e−x2 dx
Solution 13.38
Consider the box contour C that is the boundary of the rectangle −R ≤x ≤R, 0 ≤y ≤π. There is a removable
742

singularity at z = 0 and a ﬁrst order pole at z = ıπ. By the residue theorem,
−
Z
C
z
sinh z dz = ıπ Res

z
sinh z, ıπ

= ıπ lim
z→ıπ
z(z −ıπ)
sinh z
= ıπ lim
z→ıπ
2z −ıπ
cosh z
= π2
The integrals along the side of the box vanish as R →∞.

Z ±R+ıπ
±R
z
sinh z dz
 ≤π
max
z∈[±R,±R+ıπ]

z
sinh z

≤π R + π
sinh R
→0 as R →∞
The value of the integrand on the top of the box is
x + ıπ
sinh(x + ıπ) = −x + ıπ
sinh x .
Taking the limit as R →∞,
Z ∞
−∞
x
sinh x dx + −
Z ∞
−∞
−x + ıπ
sinh x dx = π2.
Note that
−
Z ∞
−∞
1
sinh x dx = 0
as there is a ﬁrst order pole at x = 0 and the integrand is odd.
Z ∞
−∞
x
sinh x dx = π2
2
743

Solution 13.39
First we evaluate
Z ∞
−∞
eax
ex +1 dx.
Consider the rectangular contour in the positive direction with corners at ±R and ±R + ı2π. With the maximum
modulus integral bound we see that the integrals on the vertical sides of the contour vanish as R →∞.

Z R+ı2π
R
eaz
ez +1 dz
 ≤2π eaR
eR −1 →0 as R →∞

Z −R
−R+ı2π
eaz
ez +1 dz
 ≤2π
e−aR
1 −e−R →0 as R →∞
In the limit as R tends to inﬁnity, the integral on the rectangular contour is the sum of the integrals along the top and
bottom sides.
Z
C
eaz
ez +1 dz =
Z ∞
−∞
eax
ex +1 dx +
Z −∞
∞
ea(x+ı2π)
ex+ı2π +1 dx
Z
C
eaz
ez +1 dz = (1 −e−ı2aπ)
Z ∞
−∞
eax
ex +1 dx
The only singularity of the integrand inside the contour is a ﬁrst order pole at z = ıπ. We use the residue theorem to
evaluate the integral.
Z
C
eaz
ez +1 dz = ı2π Res
 eaz
ez +1, ıπ

= ı2π lim
z→ıπ
(z −ıπ) eaz
ez +1
= ı2π lim
z→ıπ
a(z −ıπ) eaz + eaz
ez
= −ı2π eıaπ
744

We equate the two results for the value of the contour integral.
(1 −e−ı2aπ)
Z ∞
−∞
eax
ex +1 dx = −ı2π eıaπ
Z ∞
−∞
eax
ex +1 dx =
ı2π
eıaπ −e−ıaπ
Z ∞
−∞
eax
ex +1 dx =
π
sin(πa)
Now we derive the value of,
Z ∞
−∞
cosh(bx)
cosh x dx.
First make the change of variables x →2x in the previous result.
Z ∞
−∞
e2ax
e2x +12 dx =
π
sin(πa)
Z ∞
−∞
e(2a−1)x
ex + e−x dx =
π
sin(πa)
Now we set b = 2a −1.
Z ∞
−∞
ebx
cosh x dx =
π
sin(π(b + 1)/2) =
π
cos(πb/2)
for −1 < b < 1
Since the cosine is an even function, we also have,
Z ∞
−∞
e−bx
cosh x dx =
π
cos(πb/2)
for −1 < b < 1
Adding these two equations and dividing by 2 yields the desired result.
Z ∞
−∞
cosh(bx)
cosh x dx =
π
cos(πb/2)
for −1 < b < 1
745

Solution 13.40
Real-Valued Parameters. For b = 0, the integral has the value: π/a2. If b is nonzero, then we can write the integral
as
F(a, b) = 1
b2
Z π
0
dθ
(a/b + cos θ)2.
We deﬁne the new parameter c = a/b and the function,
G(c) = b2F(a, b) =
Z π
0
dθ
(c + cos θ)2.
If −1 ≤c ≤1 then the integrand has a double pole on the path of integration. The integral diverges. Otherwise
the integral exists. To evaluate the integral, we extend the range of integration to (0..2π) and make the change of
variables, z = eıθ to integrate along the unit circle in the complex plane.
G(c) = 1
2
Z 2π
0
dθ
(c + cos θ)2
For this change of variables, we have,
cos θ = z + z−1
2
,
dθ = dz
ız .
G(c) = 1
2
Z
C
dz/(ız)
(c + (z + z−1)/2)2
= −ı2
Z
C
z
(2cz + z2 + 1)2 dz
= −ı2
Z
C
z
(z + c +
√
c2 −1)2(z + c −
√
c2 −1)2 dz
746

If c > 1, then −c −
√
c2 −1 is outside the unit circle and −c +
√
c2 −1 is inside the unit circle. The integrand has
a second order pole inside the path of integration. We evaluate the integral with the residue theorem.
G(c) = −ı2ı2π Res

z
(z + c +
√
c2 −1)2(z + c −
√
c2 −1)2, z = −c +
√
c2 −1

= 4π
lim
z→−c+
√
c2−1
d
dz
z
(z + c +
√
c2 −1)2
= 4π
lim
z→−c+
√
c2−1

1
(z + c +
√
c2 −1)2 −
2z
(z + c +
√
c2 −1)3

= 4π
lim
z→−c+
√
c2−1
c +
√
c2 −1 −z
(z + c +
√
c2 −1)3
= 4π
2c
(2
√
c2 −1)3
=
πc
p
(c2 −1)3
747

If c < 1, then −c −
√
c2 −1 is inside the unit circle and −c +
√
c2 −1 is outside the unit circle.
G(c) = −ı2ı2π Res

z
(z + c +
√
c2 −1)2(z + c −
√
c2 −1)2, z = −c −
√
c2 −1

= 4π
lim
z→−c−
√
c2−1
d
dz
z
(z + c −
√
c2 −1)2
= 4π
lim
z→−c−
√
c2−1

1
(z + c −
√
c2 −1)2 −
2z
(z + c −
√
c2 −1)3

= 4π
lim
z→−c−
√
c2−1
c −
√
c2 −1 −z
(z + c −
√
c2 −1)3
= 4π
2c
(−2
√
c2 −1)3
= −
πc
p
(c2 −1)3
Thus we see that
G(c)







=
πc
√
(c2−1)3
for c > 1,
= −
πc
√
(c2−1)3
for c < 1,
is divergent
for −1 ≤c ≤1.
In terms of F(a, b), this is
F(a, b)







=
aπ
√
(a2−b2)3
for a/b > 1,
= −
aπ
√
(a2−b2)3
for a/b < 1,
is divergent
for −1 ≤a/b ≤1.
Complex-Valued Parameters. Consider
G(c) =
Z π
0
dθ
(c + cos θ)2,
748

for complex c. Except for real-valued c between −1 and 1, the integral converges uniformly. We can interchange
diﬀerentiation and integration. The derivative of G(c) is
G′(c) = d
dc
Z π
0
dθ
(c + cos θ)2
=
Z π
0
−2
(c + cos θ)3 dθ
Thus we see that G(c) is analytic in the complex plane with a cut on the real axis from −1 to 1. The value of the
function on the positive real axis for c > 1 is
G(c) =
πc
p
(c2 −1)3.
We use analytic continuation to determine G(c) for complex c. By inspection we see that G(c) is the branch of
πc
(c2 −1)3/2,
with a branch cut on the real axis from −1 to 1 and which is real-valued and positive for real c > 1. Using F(a, b) =
G(c)/b2 we can determine F for complex-valued a and b.
Solution 13.41
First note that
Z ∞
−∞
cos x
ex + e−x dx =
Z ∞
−∞
eıx
ex + e−x dx
since sin x/(ex + e−x) is an odd function. For the function
f(z) =
eız
ez + e−z
we have
f(x + ıπ) =
eıx−π
ex+ıπ + e−x−ıπ = −e−π
eıx
ex + e−x = −e−π f(x).
749

Thus we consider the integral
Z
C
eız
ez + e−z dz
where C is the box contour with corners at ±R and ±R + ıπ. We can evaluate this integral with the residue theorem.
We can write the integrand as
eız
2 cosh z.
We see that the integrand has ﬁrst order poles at z = ıπ(n + 1/2). The only pole inside the path of integration is at
z = ıπ/2.
Z
C
eız
ez + e−z dz = ı2π Res

eız
ez + e−z , z = ıπ
2

= ı2π lim
z→ıπ/2
(z −ıπ/2) eız
ez + e−z
= ı2π lim
z→ıπ/2
eız +ı(z −ıπ/2) eız
ez −e−z
= ı2π
e−π/2
eıπ/2 −e−ıπ/2
= π e−π/2
750

The integrals along the vertical sides of the box vanish as R →∞.

Z ±R+ıπ
±R
eız
ez + e−z dz
 ≤π
max
z∈[±R...±R+ıπ]

eız
ez + e−z

≤π max
y∈[0...π]

1
eR+ıy + e−R−ıy

≤π max
y∈[0...π]

1
eR + e−R−ı2y

= π
1
2 sinh R
→0 as R →∞
Taking the limit as R →∞, we have
Z ∞
−∞
eıx
ex + e−x dx +
Z −∞+ıπ
∞+ıπ
eız
ez + e−z dz = π e−π/2
(1 + e−π)
Z ∞
−∞
eıx
ex + e−x dx = π e−π/2
Z ∞
−∞
eıx
ex + e−x dx =
π
eπ/2 + e−π/2
Finally we have,
Z ∞
−∞
cos x
ex + e−x dx =
π
eπ/2 + e−π/2.
Deﬁnite Integrals Involving Sine and Cosine
Solution 13.42
1. To evaluate the integral we make the change of variables z = eıθ. The path of integration in the complex plane
751

is the positively oriented unit circle.
Z π
−π
dθ
1 + sin2 θ =
Z
C
1
1 −(z −z−1)2 /4
dz
ız
=
Z
C
ı4z
z4 −6z2 + 1 dz
=
Z
C
ı4z
 z −1 −
√
2
  z −1 +
√
2
  z + 1 −
√
2
  z + 1 +
√
2
 dz
There are ﬁrst order poles at z = ±1 ±
√
2. The poles at z = −1 +
√
2 and z = 1 −
√
2 are inside the path of
integration. We evaluate the integral with Cauchy’s Residue Formula.
Z
C
ı4z
z4 −6z2 + 1 dz = ı2π

Res

ı4z
z4 −6z2 + 1, z = −1 +
√
2

+ Res

ı4z
z4 −6z2 + 1, z = 1 −
√
2
 
= −8π
 
z
 z −1 −
√
2
  z −1 +
√
2
  z + 1 +
√
2


z=−1+
√
2
+
z
 z −1 −
√
2
  z + 1 −
√
2
  z + 1 +
√
2


z=1−
√
2
!
= −8π

−1
8
√
2 −
1
8
√
2

=
√
2π
2. First we use symmetry to expand the domain of integration.
Z π/2
0
sin4 θ dθ = 1
4
Z 2π
0
sin4 θ dθ
752

Next we make the change of variables z = eıθ. The path of integration in the complex plane is the positively
oriented unit circle. We evaluate the integral with the residue theorem.
1
4
Z 2π
0
sin4 θ dθ = 1
4
Z
C
1
16

z −1
z
4 dz
ız
= 1
64
Z
C
−ı(z2 −1)4
z5
dz
= −ı
64
Z
C

z3 −4z + 6
z −4
z3 + 1
z5

dz
= ı2π−ı
64 6
= 3π
16
Solution 13.43
1. Let C be the positively oriented unit circle about the origin. We parametrize this contour.
z = eıθ,
dz = ı eıθ dθ,
θ ∈(0 . . . 2π)
753

We write sin θ and the diﬀerential dθ in terms of z. Then we evaluate the integral with the Residue theorem.
Z 2π
0
1
2 + sin θ dθ =
I
C
1
2 + (z −1/z)/(ı2)
dz
ız
=
I
C
2
z2 + ı4z −1 dz
=
I
C
2
 z + ı
 2 +
√
3
  z + ı
 2 −
√
3
 dz
= ı2π Res

z + ı

2 +
√
3
 
z + ı

2 −
√
3

, z = ı

−2 +
√
3

= ı2π
2
ı2
√
3
= 2π
√
3
2. First consider the case a = 0.
Z π
−π
cos(nθ) dθ =
(
0
for n ∈Z+
2π
for n = 0
Now we consider |a| < 1, a ̸= 0. Since
sin(nθ)
1 −2a cos θ + a2
is an even function,
Z π
−π
cos(nθ)
1 −2a cos θ + a2 dθ =
Z π
−π
eınθ
1 −2a cos θ + a2 dθ
Let C be the positively oriented unit circle about the origin. We parametrize this contour.
z = eıθ,
dz = ı eıθ dθ,
θ ∈(−π . . . π)
754

We write the integrand and the diﬀerential dθ in terms of z. Then we evaluate the integral with the Residue
theorem.
Z π
−π
eınθ
1 −2a cos θ + a2 dθ =
I
C
zn
1 −a(z + 1/z) + a2
dz
ız
= −ı
I
C
zn
−az2 + (1 + a2)z −a dz
= ı
a
I
C
zn
z2 −(a + 1/a)z + 1 dz
= ı
a
I
C
zn
(z −a)(z −1/a) dz
= ı2π ı
a Res

zn
(z −a)(z −1/a), z = a

= −2π
a
an
a −1/a
= 2πan
1 −a2
We write the value of the integral for |a| < 1 and n ∈Z0+.
Z π
−π
cos(nθ)
1 −2a cos θ + a2 dθ =
(
2π
for a = 0, n = 0
2πan
1−a2
otherwise
Solution 13.44
Convergence. We consider the integral
I(α) = −
Z π
0
cos(nθ)
cos θ −cos α dθ = πsin(nα)
sin α .
We assume that α is real-valued. If α is an integer, then the integrand has a second order pole on the path of integration,
the principal value of the integral does not exist. If α is real, but not an integer, then the integrand has a ﬁrst order
pole on the path of integration. The integral diverges, but its principal value exists.
755

Contour Integration. We will evaluate the integral for real, non-integer α.
I(α) = −
Z π
0
cos(nθ)
cos θ −cos α dθ
= 1
2 −
Z 2π
0
cos(nθ)
cos θ −cos α dθ
= 1
2ℜ−
Z 2π
0
eınθ
cos θ −cos α dθ
We make the change of variables: z = eıθ.
I(α) = 1
2ℜ−
Z
C
zn
(z + 1/z)/2 −cos α
dz
ız
= ℜ−
Z
C
−ızn
(z −eıα)(z −e−ıα) dz
Now we use the residue theorem.
= ℜ

ıπ(−ı)

Res

zn
(z −eıα)(z −e−ıα), z = eıα

+ Res

zn
(z −eıα)(z −e−ıα), z = e−ıα
 
= πℜ

lim
z→eıα
zn
z −e−ıα +
lim
z→e−ıα
zn
z −eıα

= πℜ

eınα
eıα −e−ıα +
e−ınα
e−ıα −eıα

= πℜ
eınα −e−ınα
eıα −e−ıα

= πℜ
sin(nα)
sin(α)

756

I(α) = −
Z π
0
cos(nθ)
cos θ −cos α dθ = πsin(nα)
sin α .
Solution 13.45
Consider the integral
Z 1
0
x2
(1 + x2)
√
1 −x2 dx.
We make the change of variables x = sin ξ to obtain,
Z π/2
0
sin2 ξ
(1 + sin2 ξ)
p
1 −sin2 ξ
cos ξ dξ
Z π/2
0
sin2 ξ
1 + sin2 ξ dξ
Z π/2
0
1 −cos(2ξ)
3 −cos(2ξ) dξ
1
4
Z 2π
0
1 −cos ξ
3 −cos ξ dξ
Now we make the change of variables z = eıξ to obtain a contour integral on the unit circle.
1
4
Z
C
1 −(z + 1/z)/2
3 −(z + 1/z)/2
−ı
z

dz
−ı
4
Z
C
(z −1)2
z(z −3 + 2
√
2)(z −3 −2
√
2) dz
There are two ﬁrst order poles inside the contour. The value of the integral is
ı2π−ı
4

Res

(z −1)2
z(z −3 + 2
√
2)(z −3 −2
√
2), 0

+ Res

(z −1)2
z(z −3 + 2
√
2)(z −3 −2
√
2), z = 3 −2
√
2

757

π
2

lim
z→0

(z −1)2
(z −3 + 2
√
2)(z −3 −2
√
2)

+
lim
z→3−2
√
2

(z −1)2
z(z −3 −2
√
2)

.
Z 1
0
x2
(1 + x2)
√
1 −x2 dx = (2 −
√
2)π
4
Inﬁnite Sums
Solution 13.46
From Result 13.10.1 we see that the sum of the residues of π cot(πz)/z4 is zero. This function has simples poles at
nonzero integers z = n with residue 1/n4. There is a ﬁfth order pole at z = 0. Finding the residue with the formula
1
4! lim
z→0
d4
dz4(πz cot(πz))
would be a real pain. After doing the diﬀerentiation, we would have to apply L’Hospital’s rule multiple times. A better
way of ﬁnding the residue is with the Laurent series expansion of the function. Note that
1
sin(πz) =
1
πz −(πz)3/6 + (πz)5/120 −· · ·
= 1
πz
1
1 −(πz)2/6 + (πz)4/120 −· · ·
= 1
πz
 
1 +
π2
6 z2 −π4
120z4 + · · ·

+
π2
6 z2 −π4
120z4 + · · ·
2
+ · · ·
!
.
758

Now we ﬁnd the z−1 term in the Laurent series expansion of π cot(πz)/z4.
π cos(πz)
z4 sin(πz) = π
z4

1 −π2
2 z2 + π4
24z4 −· · ·
 1
πz
 
1 +
π2
6 z2 −π4
120z4 + · · ·

+
π2
6 z2 −π4
120z4 + · · ·
2
+ · · ·
!
= 1
z5

· · · +

−π4
120 + π4
36 −π4
12 + π4
24

z4 + · · ·

= · · · −π4
45
1
z + · · ·
Thus the residue at z = 0 is −π4/45. Summing the residues,
−1
X
n=−∞
1
n4 −π4
45 +
∞
X
n=1
1
n4 = 0.
∞
X
n=1
1
n4 = π4
90
Solution 13.47
For this problem we will use the following result: If
lim
|z|→∞|zf(z)| = 0,
then the sum of all the residues of π cot(πz)f(z) is zero. If in addition, f(z) is analytic at z = n ∈Z then
∞
X
n=−∞
f(n) = −( sum of the residues of π cot(πz)f(z) at the poles of f(z) ).
We assume that α is not an integer, otherwise the sum is not deﬁned. Consider f(z) = 1/(z2 −α2). Since
lim
|z|→∞
z
1
z2 −α2
 = 0,
759

and f(z) is analytic at z = n, n ∈Z, we have
∞
X
n=−∞
1
n2 −α2 = −( sum of the residues of π cot(πz)f(z) at the poles of f(z) ).
f(z) has ﬁrst order poles at z = ±α.
∞
X
n=−∞
1
n2 −α2 = −Res
π cot(πz)
z2 −α2 , z = α

−Res
π cot(πz)
z2 −α2 , z = −α

= −lim
z→α
π cot(πz)
z + α
−lim
z→−α
π cot(πz)
z −α
= −π cot(πα)
2α
−π cot(−πα)
−2α
∞
X
n=−∞
1
n2 −α2 = −π cot(πα)
α
760

Part IV
Ordinary Diﬀerential Equations
761

Chapter 14
First Order Diﬀerential Equations
Don’t show me your technique. Show me your heart.
-Tetsuyasu Uekuma
14.1
Notation
A diﬀerential equation is an equation involving a function, it’s derivatives, and independent variables. If there is only
one independent variable, then it is an ordinary diﬀerential equation. Identities such as
d
dx
 f 2(x)

= 2f(x)f ′(x),
and
dy
dx
dx
dy = 1
are not diﬀerential equations.
The order of a diﬀerential equation is the order of the highest derivative.
The following equations are ﬁrst, second
and third order, respectively.
• y′ = xy2
762

• y′′ + 3xy′ + 2y = x2
• y′′′ = y′′y
The degree of a diﬀerential equation is the highest power of the highest derivative in the equation. The following
equations are ﬁrst, second and third degree, respectively.
• y′ −3y2 = sin x
• (y′′)2 + 2x cos y = ex
• (y′)3 + y5 = 0
An equation is said to be linear if it is linear in the dependent variable.
• y′′ cos x + x2y = 0 is a linear diﬀerential equation.
• y′ + xy2 = 0 is a nonlinear diﬀerential equation.
A diﬀerential equation is homogeneous if it has no terms that are functions of the independent variable alone. Thus
an inhomogeneous equation is one in which there are terms that are functions of the independent variables alone.
• y′′ + xy + y = 0 is a homogeneous equation.
• y′ + y + x2 = 0 is an inhomogeneous equation.
A ﬁrst order diﬀerential equation may be written in terms of diﬀerentials. Recall that for the function y(x) the
diﬀerential dy is deﬁned dy = y′(x) dx. Thus the diﬀerential equations
y′ = x2y
and
y′ + xy2 = sin(x)
can be denoted:
dy = x2y dx
and
dy + xy2 dx = sin(x) dx.
763

A solution of a diﬀerential equation is a function which when substituted into the equation yields an identity. For
example, y = x ln |x| is a solution of
y′ −y
x = 1
and y = c ex is a solution of
y′′ −y = 0
for any value of the parameter c.
14.2
One Parameter Families of Functions
Consider the equation
F(x, y(x); c) = 0,
(14.1)
which implicitly deﬁnes a one-parameter family of functions y(x). (We assume that F has a non-trivial dependence on
y, that is Fy ̸= 0.) Diﬀerentiating this equation with respect to x yields
Fx + Fyy′ = 0.
This gives us two equations involving the independent variable x, the dependent variable y(x) and its derivative and
the parameter c. If we algebraically eliminate c between the two equations, the eliminant will be a ﬁrst order diﬀerential
equation for y(x). Thus we see that every equation of the form (14.1) deﬁnes a one-parameter family of functions y(x)
which satisfy a ﬁrst order diﬀerential equation. This y(x) is the primitive of the diﬀerential equation. Later we will
discuss why y(x) is the general solution of the diﬀerential equation.
Example 14.2.1 Consider the family of circles of radius c centered about the origin,
x2 + y2 = c2.
Diﬀerentiating this yields,
2x + 2yy′ = 0.
764

It is trivial to eliminate the parameter and obtain a diﬀerential equation for the family of circles.
x + yy′ = 0.
We can see the geometric meaning in this equation by writing it in the form
y′ = −x
y .
The slope of the tangent to a circle at a point is the negative of the cotangent of the angle.
Example 14.2.2 Consider the one-parameter family of functions,
y(x) = f(x) + cg(x),
where f(x) and g(x) are known functions. The derivative is
y′ = f ′ + cg′.
Eliminating the parameter yields
gy′ −g′y = gf ′ −g′f
y′ −g′
g y = f ′ −g′f
g .
Thus we see that y(x) = f(x) + cg(x) satisﬁes a ﬁrst order linear diﬀerential equation.
We know that every one-parameter family of functions satisﬁes a ﬁrst order diﬀerential equation. The converse is
true as well.
765

Result 14.2.1 Every ﬁrst order diﬀerential equation has a one-parameter family of solutions,
y(x), deﬁned by an equation of the form:
F(x, y(x); c) = 0.
This y(x) is called the general solution. If the equation is linear then the general solution
expresses the totality of solutions of the diﬀerential equation. If the equation is nonlinear,
there may be other special singular solutions, which do not depend on a parameter.
This is strictly an existence result. It does not say that the general solution of a ﬁrst order diﬀerential equation
can be determined by some method, it just says that it exists. There is no method for solving the general ﬁrst order
diﬀerential equation. However, there are some special forms that are soluble. We will devote the rest of this chapter to
studying these forms.
14.3
Exact Equations
Any ﬁrst order ordinary diﬀerential equation of the ﬁrst degree can be written as the total diﬀerential equation,
P(x, y) dx + Q(x, y) dy = 0.
If this equation can be integrated directly, that is if there is a primitive, u(x, y), such that
du = P dx + Q dy,
then this equation is called exact. The (implicit) solution of the diﬀerential equation is
u(x, y) = c,
766

where c is an arbitrary constant. Since the diﬀerential of a function, u(x, y), is
du ≡∂u
∂x dx + ∂u
∂y dy,
P and Q are the partial derivatives of u:
P(x, y) = ∂u
∂x,
Q(x, y) = ∂u
∂y .
In an alternate notation, the diﬀerential equation
P(x, y) + Q(x, y)dy
dx = 0,
(14.2)
is exact if there is a primitive u(x, y) such that
du
dx ≡∂u
∂x + ∂u
∂y
dy
dx = P(x, y) + Q(x, y)dy
dx.
The solution of the diﬀerential equation is u(x, y) = c.
Example 14.3.1
x + y dy
dx = 0
is an exact diﬀerential equation since
d
dx
1
2(x2 + y2)

= x + y dy
dx
The solution of the diﬀerential equation is
1
2(x2 + y2) = c.
767

Example 14.3.2 , Let f(x) and g(x) be known functions.
g(x)y′ + g′(x)y = f(x)
is an exact diﬀerential equation since
d
dx (g(x)y(x)) = gy′ + g′y.
The solution of the diﬀerential equation is
g(x)y(x) =
Z
f(x) dx + c
y(x) =
1
g(x)
Z
f(x) dx +
c
g(x).
A necessary condition for exactness.
The solution of the exact equation P + Qy′ = 0 is u = c where u is
the primitive of the equation, du
dx = P + Qy′. At present the only method we have for determining the primitive is
guessing. This is ﬁne for simple equations, but for more diﬃcult cases we would like a method more concrete than
divine inspiration. As a ﬁrst step toward this goal we determine a criterion for determining if an equation is exact.
Consider the exact equation,
P + Qy′ = 0,
with primitive u, where we assume that the functions P and Q are continuously diﬀerentiable. Since the mixed partial
derivatives of u are equal,
∂2u
∂x∂y = ∂2u
∂y∂x,
a necessary condition for exactness is
∂P
∂y = ∂Q
∂x .
768

A suﬃcient condition for exactness.
This necessary condition for exactness is also a suﬃcient condition. We
demonstrate this by deriving the general solution of (14.2). Assume that P + Qy′ = 0 is not necessarily exact, but
satisﬁes the condition Py = Qx. If the equation has a primitive,
du
dx ≡∂u
∂x + ∂u
∂y
dy
dx = P(x, y) + Q(x, y)dy
dx,
then it satisﬁes
∂u
∂x = P,
∂u
∂y = Q.
(14.3)
Integrating the ﬁrst equation of (14.3), we see that the primitive has the form
u(x, y) =
Z x
x0
P(ξ, y) dξ + f(y),
for some f(y). Now we substitute this form into the second equation of (14.3).
∂u
∂y = Q(x, y)
Z x
x0
Py(ξ, y) dξ + f ′(y) = Q(x, y)
Now we use the condition Py = Qx.
Z x
x0
Qx(ξ, y) dξ + f ′(y) = Q(x, y)
Q(x, y) −Q(x0, y) + f ′(y) = Q(x, y)
f ′(y) = Q(x0, y)
f(y) =
Z y
y0
Q(x0, ψ) dψ
769

Thus we see that
u =
Z x
x0
P(ξ, y) dξ +
Z y
y0
Q(x0, ψ) dψ
is a primitive of the derivative; the equation is exact. The solution of the diﬀerential equation is
Z x
x0
P(ξ, y) dξ +
Z y
y0
Q(x0, ψ) dψ = c.
Even though there are three arbitrary constants: x0, y0 and c, the solution is a one-parameter family. This is because
changing x0 or y0 only changes the left side by an additive constant.
Result 14.3.1 Any ﬁrst order diﬀerential equation of the ﬁrst degree can be written in the
form
P(x, y) + Q(x, y)dy
dx = 0.
This equation is exact if and only if
Py = Qx.
In this case the solution of the diﬀerential equation is given by
Z x
x0
P(ξ, y) dξ +
Z y
y0
Q(x0, ψ) dψ = c.
Exercise 14.1
Solve the following diﬀerential equations by inspection. That is, group terms into exact derivatives and then integrate.
f(x) and g(x) are known functions.
1.
y′(x)
y(x) = f(x)
2. yα(x)y′(x) = f(x)
770

3.
y′
cos x + y tan x
cos x = cos x
Hint, Solution
14.3.1
Separable Equations
Any diﬀerential equation that can written in the form
P(x) + Q(y)y′ = 0
is a separable equation, (because the dependent and independent variables are separated). We can obtain an implicit
solution by integrating with respect to x.
Z
P(x) dx +
Z
Q(y)dy
dx dx = c
Z
P(x) dx +
Z
Q(y) dy = c
Result 14.3.2 The general solution to the separable equation P(x) + Q(y)y′ = 0 is
Z
P(x) dx +
Z
Q(y) dy = c
Example 14.3.3 Consider the diﬀerential equation y′ = xy2. We separate the dependent and independent variables
771

and integrate to ﬁnd the solution.
dy
dx = xy2
y−2 dy = x dx
Z
y−2 dy =
Z
x dx + c
−y−1 = x2
2 + c
y = −
1
x2/2 + c
Example 14.3.4 The equation
y′ = y −y2,
is separable.
y′
y −y2 = 1
We expand in partial fractions and integrate.
1
y −
1
y −1

y′ = 1
ln |y| −ln |y −1| = x + c
772

We have an implicit equation for y(x). Now we solve for y(x).
ln

y
y −1
 = x + c

y
y −1
 = ex+c
y
y −1 = ± ex+c
y =
± ex+c
± ex+c −1
y =
ex+c
ex+c ±1
y =
1
1 ± ec−x
14.3.2
Homogeneous Coeﬃcient Equations
Homogeneous coeﬃcient, ﬁrst order diﬀerential equations form another class of soluble equations. We will ﬁnd that
a change of dependent variable will make such equations separable or we can determine an integrating factor that will
make such equations exact. First we deﬁne homogeneous functions.
Euler’s Theorem on Homogeneous Functions.
The function F(x, y) is homogeneous of degree n if
F(λx, λy) = λnF(x, y).
From this deﬁnition we see that
F(x, y) = xnF

1, y
x

.
773

(Just formally substitute 1/x for λ.) For example,
xy2,
x2y + 2y3
x + y
,
x cos(y/x)
are homogeneous functions of orders 3, 2 and 1, respectively.
Euler’s theorem for a homogeneous function of order n is:
xFx + yFy = nF.
To prove this, we deﬁne ξ = λx, ψ = λy. From the deﬁnition of homogeneous functions, we have
F(ξ, ψ) = λnF(x, y).
We diﬀerentiate this equation with respect to λ.
∂F(ξ, ψ)
∂ξ
∂ξ
∂λ + ∂F(ξ, ψ)
∂ψ
∂ψ
∂λ = nλn−1F(x, y)
xFξ + yFψ = nλn−1F(x, y)
Setting λ = 1, (and hence ξ = x, ψ = y), proves Euler’s theorem.
Result 14.3.3 Euler’s Theorem on Homogeneous Functions. If F(x, y) is a homoge-
neous function of degree n, then
xFx + yFy = nF.
Homogeneous Coeﬃcient Diﬀerential Equations.
If the coeﬃcient functions P(x, y) and Q(x, y) are homo-
geneous of degree n then the diﬀerential equation,
P(x, y) + Q(x, y)dy
dx = 0,
(14.4)
is called a homogeneous coeﬃcient equation. They are often referred to simply as homogeneous equations.
774

Transformation to a Separable Equation.
We can write the homogeneous equation in the form,
xnP

1, y
x

+ xnQ

1, y
x
 dy
dx = 0,
P

1, y
x

+ Q

1, y
x
 dy
dx = 0.
This suggests the change of dependent variable u(x) = y(x)
x .
P(1, u) + Q(1, u)

u + xdu
dx

= 0
This equation is separable.
P(1, u) + uQ(1, u) + xQ(1, u)du
dx = 0
1
x +
Q(1, u)
P(1, u) + uQ(1, u)
du
dx = 0
ln |x| +
Z
1
u + P(1, u)/Q(1, u) du = c
By substituting ln |c| for c, we can write this in a simpler form.
Z
1
u + P(1, u)/Q(1, u) du = ln
 c
x
 .
Integrating Factor.
One can show that
µ(x, y) =
1
xP(x, y) + yQ(x, y)
is an integrating factor for the Equation 14.4. The proof of this is left as an exercise for the reader. (See Exercise 14.2.)
775

Result 14.3.4 Homogeneous Coeﬃcient Diﬀerential Equations. If P(x, y) and Q(x, y)
are homogeneous functions of degree n, then the equation
P(x, y) + Q(x, y)dy
dx = 0
is made separable by the change of independent variable u(x) = y(x)
x . The solution is deter-
mined by
Z
1
u + P(1, u)/Q(1, u) du = ln
 c
x
 .
Alternatively, the homogeneous equation can be made exact with the integrating factor
µ(x, y) =
1
xP(x, y) + yQ(x, y).
Example 14.3.5 Consider the homogeneous coeﬃcient equation
x2 −y2 + xy dy
dx = 0.
The solution for u(x) = y(x)/x is determined by
Z
1
u + 1−u2
u
du = ln
 c
x

Z
u du = ln
 c
x

1
2u2 = ln
 c
x

u = ±
p
2 ln |c/x|
776

Thus the solution of the diﬀerential equation is
y = ±x
p
2 ln |c/x|
Exercise 14.2
Show that
µ(x, y) =
1
xP(x, y) + yQ(x, y)
is an integrating factor for the homogeneous equation,
P(x, y) + Q(x, y)dy
dx = 0.
Hint, Solution
Exercise 14.3 (mathematica/ode/ﬁrst order/exact.nb)
Find the general solution of the equation
dy
dt = 2y
t +
y
t
2
.
Hint, Solution
14.4
The First Order, Linear Diﬀerential Equation
14.4.1
Homogeneous Equations
The ﬁrst order, linear, homogeneous equation has the form
dy
dx + p(x)y = 0.
777

Note that if we can ﬁnd one solution, then any constant times that solution also satisﬁes the equation. If fact, all the
solutions of this equation diﬀer only by multiplicative constants. We can solve any equation of this type because it is
separable.
y′
y = −p(x)
ln |y| = −
Z
p(x) dx + c
y = ± e−
R
p(x) dx+c
y = c e−
R
p(x) dx
Result 14.4.1 First Order, Linear Homogeneous Diﬀerential Equations.
The ﬁrst
order, linear, homogeneous diﬀerential equation,
dy
dx + p(x)y = 0,
has the solution
y = c e−
R
p(x) dx .
(14.5)
The solutions diﬀer by multiplicative constants.
Example 14.4.1 Consider the equation
dy
dx + 1
xy = 0.
778

We use Equation 14.5 to determine the solution.
y(x) = c e−
R
1/x dx,
for x ̸= 0
y(x) = c e−ln |x|
y(x) = c
|x|
y(x) = c
x
14.4.2
Inhomogeneous Equations
The ﬁrst order, linear, inhomogeneous diﬀerential equation has the form
dy
dx + p(x)y = f(x).
(14.6)
This equation is not separable. Note that it is similar to the exact equation we solved in Example 14.3.2,
g(x)y′(x) + g′(x)y(x) = f(x).
To solve Equation 14.6, we multiply by an integrating factor. Multiplying a diﬀerential equation by its integrating factor
changes it to an exact equation. Multiplying Equation 14.6 by the function, I(x), yields,
I(x)dy
dx + p(x)I(x)y = f(x)I(x).
In order that I(x) be an integrating factor, it must satisfy
d
dxI(x) = p(x)I(x).
This is a ﬁrst order, linear, homogeneous equation with the solution
I(x) = c e
R
p(x) dx .
This is an integrating factor for any constant c. For simplicity we will choose c = 1.
779

To solve Equation 14.6 we multiply by the integrating factor and integrate. Let P(x) =
R
p(x) dx.
eP(x) dy
dx + p(x) eP(x) y = eP(x) f(x)
d
dx
 eP(x) y

= eP(x) f(x)
y = e−P(x)
Z
eP(x) f(x) dx + c e−P(x)
y ≡yp + c yh
Note that the general solution is the sum of a particular solution, yp, that satisﬁes y′ + p(x)y = f(x), and an arbitrary
constant times a homogeneous solution, yh, that satisﬁes y′ + p(x)y = 0.
Example 14.4.2 Consider the diﬀerential equation
y′ + 1
xy = x2,
x > 0.
First we ﬁnd the integrating factor.
I(x) = exp
Z 1
x dx

= eln x = x
We multiply by the integrating factor and integrate.
d
dx(xy) = x3
xy = 1
4x4 + c
y = 1
4x3 + c
x.
The particular and homogeneous solutions are
yp = 1
4x3
and
yh = 1
x.
780

Note that the general solution to the diﬀerential equation is a one-parameter family of functions. The general solution
is plotted in Figure 14.1 for various values of c.
-1
-0.5
0.5
1
-10
-7.5
-5
-2.5
2.5
5
7.5
10
Figure 14.1: Solutions to y′ + y/x = x2.
Exercise 14.4 (mathematica/ode/ﬁrst order/linear.nb)
Solve the diﬀerential equation
y′ −1
xy = xα,
x > 0.
Hint, Solution
781

14.4.3
Variation of Parameters.
We could also have found the particular solution with the method of variation of parameters. Although we can
solve ﬁrst order equations without this method, it will become important in the study of higher order inhomogeneous
equations. We begin by assuming that the particular solution has the form yp = u(x)yh(x) where u(x) is an unknown
function. We substitute this into the diﬀerential equation.
d
dxyp + p(x)yp = f(x)
d
dx(uyh) + p(x)uyh = f(x)
u′yh + u(y′
h + p(x)yh) = f(x)
Since yh is a homogeneous solution, y′
h + p(x)yh = 0.
u′ = f(x)
yh
u =
Z
f(x)
yh(x) dx
Recall that the homogeneous solution is yh = e−P(x).
u =
Z
eP(x) f(x) dx
Thus the particular solution is
yp = e−P(x)
Z
eP(x) f(x) dx.
14.5
Initial Conditions
In physical problems involving ﬁrst order diﬀerential equations, the solution satisﬁes both the diﬀerential equation
and a constraint which we call the initial condition. Consider a ﬁrst order linear diﬀerential equation subject to the
782

initial condition y(x0) = y0. The general solution is
y = yp + cyh = e−P(x)
Z
eP(x) f(x) dx + c e−P(x) .
For the moment, we will assume that this problem is well-posed. A problem is well-posed if there is a unique solution to
the diﬀerential equation that satisﬁes the constraint(s). Recall that
R eP(x) f(x) dx denotes any integral of eP(x) f(x).
For convenience, we choose
R x
x0 eP(ξ) f(ξ) dξ. The initial condition requires that
y(x0) = y0 = e−P(x0)
Z x0
x0
eP(ξ) f(ξ) dξ + c e−P(x0) = c e−P(x0) .
Thus c = y0 eP(x0). The solution subject to the initial condition is
y = e−P(x)
Z x
x0
eP(ξ) f(ξ) dξ + y0 eP(x0)−P(x) .
Example 14.5.1 Consider the problem
y′ + (cos x)y = x,
y(0) = 2.
From Result 14.5.1, the solution subject to the initial condition is
y = e−sin x
Z x
0
ξ esin ξ dξ + 2 e−sin x .
14.5.1
Piecewise Continuous Coeﬃcients and Inhomogeneities
If the coeﬃcient function p(x) and the inhomogeneous term f(x) in the ﬁrst order linear diﬀerential equation
dy
dx + p(x)y = f(x)
783

are continuous, then the solution is continuous and has a continuous ﬁrst derivative. To see this, we note that the
solution
y = e−P(x)
Z
eP(x) f(x) dx + c e−P(x)
is continuous since the integral of a piecewise continuous function is continuous. The ﬁrst derivative of the solution
can be found directly from the diﬀerential equation.
y′ = −p(x)y + f(x)
Since p(x), y, and f(x) are continuous, y′ is continuous.
If p(x) or f(x) is only piecewise continuous, then the solution will be continuous since the integral of a piecewise
continuous function is continuous. The ﬁrst derivative of the solution will be piecewise continuous.
Example 14.5.2 Consider the problem
y′ −y = H(x −1),
y(0) = 1,
where H(x) is the Heaviside function.
H(x) =
(
1
for x > 0,
0
for x < 0.
To solve this problem, we divide it into two equations on separate domains.
y′
1 −y1 = 0,
y1(0) = 1,
for x < 1
y′
2 −y2 = 1,
y2(1) = y1(1),
for x > 1
With the condition y2(1) = y1(1) on the second equation, we demand that the solution be continuous. The solution
to the ﬁrst equation is y = ex. The solution for the second equation is
y = ex
Z x
1
e−ξ dξ + e1 ex−1 = −1 + ex−1 + ex .
784

Thus the solution over the whole domain is
y =
(
ex
for x < 1,
(1 + e−1) ex −1
for x > 1.
The solution is graphed in Figure 14.2.
-1
-0.5
0.5
1
1.5
2
2
4
6
8
Figure 14.2: Solution to y′ −y = H(x −1).
Example 14.5.3 Consider the problem,
y′ + sign(x)y = 0,
y(1) = 1.
785

Recall that
sign x =





−1
for x < 0
0
for x = 0
1
for x > 0.
Since sign x is piecewise deﬁned, we solve the two problems,
y′
+ + y+ = 0,
y+(1) = 1,
for x > 0
y′
−−y−= 0,
y−(0) = y+(0),
for x < 0,
and deﬁne the solution, y, to be
y(x) =
(
y+(x),
for x ≥0,
y−(x),
for x ≤0.
The initial condition for y−demands that the solution be continuous.
Solving the two problems for positive and negative x, we obtain
y(x) =
(
e1−x,
for x > 0,
e1+x,
for x < 0.
This can be simpliﬁed to
y(x) = e1−|x| .
This solution is graphed in Figure 14.3.
786

-3
-2
-1
1
2
3
0.5
1
1.5
2
2.5
Figure 14.3: Solution to y′ + sign(x)y = 0.
Result 14.5.1 Existence, Uniqueness Theorem. Let p(x) and f(x) be piecewise contin-
uous on the interval [a, b] and let x0 ∈[a, b]. Consider the problem,
dy
dx + p(x)y = f(x),
y(x0) = y0.
The general solution of the diﬀerential equation is
y = e−P(x)
Z
eP(x) f(x) dx + c e−P(x) .
The unique, continuous solution of the diﬀerential equation subject to the initial condition is
y = e−P(x)
Z x
x0
eP(ξ) f(ξ) dξ + y0 eP(x0)−P(x),
where P(x) =
R
p(x) dx.
787

Exercise 14.5 (mathematica/ode/ﬁrst order/exact.nb)
Find the solutions of the following diﬀerential equations which satisfy the given initial conditions:
1. dy
dx + xy = x2n+1,
y(1) = 1,
n ∈Z
2. dy
dx −2xy = 1,
y(0) = 1
Hint, Solution
Exercise 14.6 (mathematica/ode/ﬁrst order/exact.nb)
Show that if α > 0 and λ > 0, then for any real β, every solution of
dy
dx + αy(x) = β e−λx
satisﬁes limx→+∞y(x) = 0. (The case α = λ requires special treatment.) Find the solution for β = λ = 1 which
satisﬁes y(0) = 1. Sketch this solution for 0 ≤x < ∞for several values of α. In particular, show what happens when
α →0 and α →∞.
Hint, Solution
14.6
Well-Posed Problems
Example 14.6.1 Consider the problem,
y′ −1
xy = 0,
y(0) = 1.
The general solution is y = cx. Applying the initial condition demands that 1 = c · 0, which cannot be satisﬁed. The
general solution for various values of c is plotted in Figure 14.4.
788

-1
-0.5
0.5
1
-0.6
-0.4
-0.2
0.2
0.4
0.6
Figure 14.4: Solutions to y′ −y/x = 0.
Example 14.6.2 Consider the problem
y′ −1
xy = −1
x,
y(0) = 1.
The general solution is
y = 1 + cx.
The initial condition is satisﬁed for any value of c so there are an inﬁnite number of solutions.
Example 14.6.3 Consider the problem
y′ + 1
xy = 0,
y(0) = 1.
789

The general solution is y = c
x. Depending on whether c is nonzero, the solution is either singular or zero at the origin
and cannot satisfy the initial condition.
The above problems in which there were either no solutions or an inﬁnite number of solutions are said to be ill-posed.
If there is a unique solution that satisﬁes the initial condition, the problem is said to be well-posed. We should have
suspected that we would run into trouble in the above examples as the initial condition was given at a singularity of
the coeﬃcient function, p(x) = 1/x.
Consider the problem,
y′ + p(x)y = f(x),
y(x0) = y0.
We assume that f(x) bounded in a neighborhood of x = x0. The diﬀerential equation has the general solution,
y = e−P(x)
Z
eP(x) f(x) dx + c e−P(x) .
If the homogeneous solution, e−P(x), is nonzero and ﬁnite at x = x0, then there is a unique value of c for which the
initial condition is satisﬁed. If the homogeneous solution vanishes at x = x0 then either the initial condition cannot be
satisﬁed or the initial condition is satisﬁed for all values of c. The homogeneous solution can vanish or be inﬁnite only
if P(x) →±∞as x →x0. This can occur only if the coeﬃcient function, p(x), is unbounded at that point.
Result 14.6.1 If the initial condition is given where the homogeneous solution to a ﬁrst
order, linear diﬀerential equation is zero or inﬁnite then the problem may be ill-posed. This
may occur only if the coeﬃcient function, p(x), is unbounded at that point.
790

14.7
Equations in the Complex Plane
14.7.1
Ordinary Points
Consider the ﬁrst order homogeneous equation
dw
dz + p(z)w = 0,
where p(z), a function of a complex variable, is analytic in some domain D. The integrating factor,
I(z) = exp
Z
p(z) dz

,
is an analytic function in that domain. As with the case of real variables, multiplying by the integrating factor and
integrating yields the solution,
w(z) = c exp

−
Z
p(z) dz

.
We see that the solution is analytic in D.
Example 14.7.1 It does not make sense to pose the equation
dw
dz + |z|w = 0.
For the solution to exist, w and hence w′(z) must be analytic. Since p(z) = |z| is not analytic anywhere in the complex
plane, the equation has no solution.
Any point at which p(z) is analytic is called an ordinary point of the diﬀerential equation. Since the solution is
analytic we can expand it in a Taylor series about an ordinary point. The radius of convergence of the series will be at
least the distance to the nearest singularity of p(z) in the complex plane.
791

Example 14.7.2 Consider the equation
dw
dz −
1
1 −zw = 0.
The general solution is w =
c
1−z. Expanding this solution about the origin,
w =
c
1 −z = c
∞
X
n=0
zn.
The radius of convergence of the series is,
R = lim
n→∞

an
an+1
 = 1,
which is the distance from the origin to the nearest singularity of p(z) =
1
1−z.
We do not need to solve the diﬀerential equation to ﬁnd the Taylor series expansion of the homogeneous solution.
We could substitute a general Taylor series expansion into the diﬀerential equation and solve for the coeﬃcients. Since
we can always solve ﬁrst order equations, this method is of limited usefulness. However, when we consider higher order
equations in which we cannot solve the equations exactly, this will become an important method.
Example 14.7.3 Again consider the equation
dw
dz −
1
1 −zw = 0.
Since we know that the solution has a Taylor series expansion about z = 0, we substitute w = P∞
n=0 anzn into the
792

diﬀerential equation.
(1 −z) d
dz
∞
X
n=0
anzn −
∞
X
n=0
anzn = 0
∞
X
n=1
nanzn−1 −
∞
X
n=1
nanzn −
∞
X
n=0
anzn = 0
∞
X
n=0
(n + 1)an+1zn −
∞
X
n=0
nanzn −
∞
X
n=0
anzn = 0
∞
X
n=0
((n + 1)an+1 −(n + 1)an) zn = 0.
Now we equate powers of z to zero. For zn, the equation is (n + 1)an+1 −(n + 1)an = 0, or an+1 = an. Thus we have
that an = a0 for all n ≥1. The solution is then
w = a0
∞
X
n=0
zn,
which is the result we obtained by expanding the solution in Example 14.7.2.
Result 14.7.1 Consider the equation
dw
dz + p(z)w = 0.
If p(z) is analytic at z = z0 then z0 is called an ordinary point of the diﬀerential equation. The
Taylor series expansion of the solution can be found by substituting w = P∞
n=0 an(z −z0)n
into the equation and equating powers of (z −z0). The radius of convergence of the series is
at least the distance to the nearest singularity of p(z) in the complex plane.
793

Exercise 14.7
Find the Taylor series expansion about the origin of the solution to
dw
dz +
1
1 −zw = 0
with the substitution w = P∞
n=0 anzn. What is the radius of convergence of the series? What is the distance to the
nearest singularity of
1
1−z?
Hint, Solution
14.7.2
Regular Singular Points
If the coeﬃcient function p(z) has a simple pole at z = z0 then z0 is a regular singular point of the ﬁrst order
diﬀerential equation.
Example 14.7.4 Consider the equation
dw
dz + α
z w = 0,
α ̸= 0.
This equation has a regular singular point at z = 0. The solution is w = cz−α. Depending on the value of α, the
solution can have three diﬀerent kinds of behavior.
α is a negative integer. The solution is analytic in the ﬁnite complex plane.
α is a positive integer The solution has a pole at the origin. w is analytic in the annulus, 0 < |z|.
α is not an integer. w has a branch point at z = 0. The solution is analytic in the cut annulus 0 < |z| < ∞,
θ0 < arg z < θ0 + 2π.
794

Consider the diﬀerential equation
dw
dz + p(z)w = 0,
where p(z) has a simple pole at the origin and is analytic in the annulus, 0 < |z| < r, for some positive r. Recall that
the solution is
w = c exp

−
Z
p(z) dz

= c exp

−
Z b0
z + p(z) −b0
z dz

= c exp

−b0 log z −
Z zp(z) −b0
z
dz

= cz−b0 exp

−
Z zp(z) −b0
z
dz

The exponential factor has a removable singularity at z = 0 and is analytic in |z| < r. We consider the following
cases for the z−b0 factor:
b0 is a negative integer. Since z−b0 is analytic at the origin, the solution to the diﬀerential equation is analytic in
the circle |z| < r.
b0 is a positive integer. The solution has a pole of order −b0 at the origin and is analytic in the annulus 0 < |z| < r.
b0 is not an integer. The solution has a branch point at the origin and thus is not single-valued. The solution is
analytic in the cut annulus 0 < |z| < r, θ0 < arg z < θ0 + 2π.
Since the exponential factor has a convergent Taylor series in |z| < r, the solution can be expanded in a series of
the form
w = z−b0
∞
X
n=0
anzn,
where a0 ̸= 0 and b0 = lim
z→0 z p(z).
795

In the case of a regular singular point at z = z0, the series is
w = (z −z0)−b0
∞
X
n=0
an(z −z0)n,
where a0 ̸= 0 and b0 = lim
z→z0(z −z0) p(z).
Series of this form are known as Frobenius series. Since we can write the solution as
w = c(z −z0)−b0 exp

−
Z 
p(z) −
b0
z −z0

dz

,
we see that the Frobenius expansion of the solution will have a radius of convergence at least the distance to the nearest
singularity of p(z).
Result 14.7.2 Consider the equation,
dw
dz + p(z)w = 0,
where p(z) has a simple pole at z = z0, p(z) is analytic in some annulus, 0 < |z −z0| < r,
and limz→z0(z −z0)p(z) = β. The solution to the diﬀerential equation has a Frobenius series
expansion of the form
w = (z −z0)−β
∞
X
n=0
an(z −z0)n,
a0 ̸= 0.
The radius of convergence of the expansion will be at least the distance to the nearest
singularity of p(z).
796

Example 14.7.5 We will ﬁnd the ﬁrst two nonzero terms in the series solution about z = 0 of the diﬀerential equation,
dw
dz +
1
sin zw = 0.
First we note that the coeﬃcient function has a simple pole at z = 0 and
lim
z→0
z
sin z = lim
z→0
1
cos z = 1.
Thus we look for a series solution of the form
w = z−1
∞
X
n=0
anzn,
a0 ̸= 0.
The nearest singularities of 1/ sin z in the complex plane are at z = ±π. Thus the radius of convergence of the series
will be at least π.
Substituting the ﬁrst three terms of the expansion into the diﬀerential equation,
d
dz(a0z−1 + a1 + a2z) +
1
sin z(a0z−1 + a1 + a2z) = O(z).
Recall that the Taylor expansion of sin z is sin z = z −1
6z3 + O(z5).

z −z3
6 + O(z5)

(−a0z−2 + a2) + (a0z−1 + a1 + a2z) = O(z2)
−a0z−1 +

a2 + a0
6

z + a0z−1 + a1 + a2z = O(z2)
a1 +

2a2 + a0
6

z = O(z2)
a0 is arbitrary. Equating powers of z,
z0 :
a1 = 0.
z1 :
2a2 + a0
6 = 0.
797

Thus the solution has the expansion,
w = a0

z−1 −z
12

+ O(z2).
In Figure 14.5 the exact solution is plotted in a solid line and the two term approximation is plotted in a dashed line.
The two term approximation is very good near the point x = 0.
1
2
3
4
5
6
-2
2
4
Figure 14.5: Plot of the Exact Solution and the Two Term Approximation.
Example 14.7.6 Find the ﬁrst two nonzero terms in the series expansion about z = 0 of the solution to
w′ −icos z
z
w = 0.
798

Since cos z
z
has a simple pole at z = 0 and limz→0 −i cos z = −i we see that the Frobenius series will have the form
w = zi
∞
X
n=0
anzn,
a0 ̸= 0.
Recall that cos z has the Taylor expansion P∞
n=0
(−1)nz2n
(2n)!
. Substituting the Frobenius expansion into the diﬀerential
equation yields
z
 
izi−1
∞
X
n=0
anzn + zi
∞
X
n=0
nanzn−1
!
−i
 ∞
X
n=0
(−1)nz2n
(2n)!
!  
zi
∞
X
n=0
anzn
!
= 0
∞
X
n=0
(n + i)anzn −i
 ∞
X
n=0
(−1)nz2n
(2n)!
!  ∞
X
n=0
anzn
!
= 0.
Equating powers of z,
z0 :
ia0 −ia0 = 0
→a0 is arbitrary
z1 :
(1 + i)a1 −ia1 = 0
→a1 = 0
z2 :
(2 + i)a2 −ia2 + i
2a0 = 0
→a2 = −i
4a0.
Thus the solution is
w = a0zi

1 −i
4z2 + O(z3)

.
14.7.3
Irregular Singular Points
If a point is not an ordinary point or a regular singular point then it is called an irregular singular point. The following
equations have irregular singular points at the origin.
799

• w′ + √zw = 0
• w′ −z−2w = 0
• w′ + exp(1/z)w = 0
Example 14.7.7 Consider the diﬀerential equation
dw
dz + αzβw = 0,
α ̸= 0,
β ̸= −1, 0, 1, 2, . . .
This equation has an irregular singular point at the origin. Solving this equation,
d
dz

exp
Z
αzβ dz

w

= 0
w = c exp

−
α
β + 1zβ+1

= c
∞
X
n=0
(−1)n
n!

α
β + 1
n
z(β+1)n.
If β is not an integer, then the solution has a branch point at the origin. If β is an integer, β < −1, then the solution
has an essential singularity at the origin. The solution cannot be expanded in a Frobenius series, w = zλ P∞
n=0 anzn.
Although we will not show it, this result holds for any irregular singular point of the diﬀerential equation. We cannot
approximate the solution near an irregular singular point using a Frobenius expansion.
Now would be a good time to summarize what we have discovered about solutions of ﬁrst order diﬀerential equations
in the complex plane.
800

Result 14.7.3 Consider the ﬁrst order diﬀerential equation
dw
dz + p(z)w = 0.
Ordinary Points If p(z) is analytic at z = z0 then z0 is an ordinary point of the diﬀerential
equation. The solution can be expanded in the Taylor series w = P∞
n=0 an(z −z0)n.
The radius of convergence of the series is at least the distance to the nearest singularity
of p(z) in the complex plane.
Regular Singular Points If p(z) has a simple pole at z = z0 and is analytic in some annulus
0 < |z −z0| < r then z0 is a regular singular point of the diﬀerential equation. The
solution at z0 will either be analytic, have a pole, or have a branch point. The solution
can be expanded in the Frobenius series w = (z −z0)−β P∞
n=0 an(z −z0)n where a0 ̸= 0
and β = limz→z0(z −z0)p(z). The radius of convergence of the Frobenius series will be
at least the distance to the nearest singularity of p(z).
Irregular Singular Points If the point z = z0 is not an ordinary point or a regular singular
point, then it is an irregular singular point of the diﬀerential equation. The solution
cannot be expanded in a Frobenius series about that point.
14.7.4
The Point at Inﬁnity
Now we consider the behavior of ﬁrst order linear diﬀerential equations at the point at inﬁnity. Recall from complex
variables that the complex plane together with the point at inﬁnity is called the extended complex plane. To study the
behavior of a function f(z) at inﬁnity, we make the transformation z = 1
ζ and study the behavior of f(1/ζ) at ζ = 0.
801

Example 14.7.8 Let’s examine the behavior of sin z at inﬁnity. We make the substitution z = 1/ζ and ﬁnd the
Laurent expansion about ζ = 0.
sin(1/ζ) =
∞
X
n=0
(−1)n
(2n + 1)! ζ(2n+1)
Since sin(1/ζ) has an essential singularity at ζ = 0, sin z has an essential singularity at inﬁnity.
We use the same approach if we want to examine the behavior at inﬁnity of a diﬀerential equation. Starting with
the ﬁrst order diﬀerential equation,
dw
dz + p(z)w = 0,
we make the substitution
z = 1
ζ ,
d
dz = −ζ2 d
dζ ,
w(z) = u(ζ)
to obtain
−ζ2du
dζ + p(1/ζ)u = 0
du
dζ −p(1/ζ)
ζ2
u = 0.
Result 14.7.4 The behavior at inﬁnity of
dw
dz + p(z)w = 0
is the same as the behavior at ζ = 0 of
du
dζ −p(1/ζ)
ζ2
u = 0.
802

Example 14.7.9 We classify the singular points of the equation
dw
dz +
1
z2 + 9w = 0.
We factor the denominator of the fraction to see that z = ı3 and z = −ı3 are regular singular points.
dw
dz +
1
(z −ı3)(z + ı3)w = 0
We make the transformation z = 1/ζ to examine the point at inﬁnity.
du
dζ −1
ζ2
1
(1/ζ)2 + 9u = 0
du
dζ −
1
9ζ2 + 1u = 0
Since the equation for u has a ordinary point at ζ = 0, z = ∞is a ordinary point of the equation for w.
803

14.8
Additional Exercises
Exact Equations
Exercise 14.8 (mathematica/ode/ﬁrst order/exact.nb)
Find the general solution y = y(x) of the equations
1. dy
dx = x2 + xy + y2
x2
,
2. (4y −3x) dx + (y −2x) dy = 0.
Hint, Solution
Exercise 14.9 (mathematica/ode/ﬁrst order/exact.nb)
Determine whether or not the following equations can be made exact. If so ﬁnd the corresponding general solution.
1. (3x2 −2xy + 2) dx + (6y2 −x2 + 3) dy = 0
2. dy
dx = −ax + by
bx + cy
Hint, Solution
Exercise 14.10 (mathematica/ode/ﬁrst order/exact.nb)
Find the solutions of the following diﬀerential equations which satisfy the given initial condition. In each case determine
the interval in which the solution is deﬁned.
1. dy
dx = (1 −2x)y2,
y(0) = −1/6.
2. x dx + y e−x dy = 0,
y(0) = 1.
Hint, Solution
804

Exercise 14.11
Are the following equations exact? If so, solve them.
1. (4y −x)y′ −(9x2 + y −1) = 0
2. (2x −2y)y′ + (2x + 4y) = 0.
Hint, Solution
Exercise 14.12 (mathematica/ode/ﬁrst order/exact.nb)
Find all functions f(t) such that the diﬀerential equation
y2 sin t + yf(t)dy
dt = 0
(14.7)
is exact. Solve the diﬀerential equation for these f(t).
Hint, Solution
The First Order, Linear Diﬀerential Equation
Exercise 14.13 (mathematica/ode/ﬁrst order/linear.nb)
Solve the diﬀerential equation
y′ +
y
sin x = 0.
Hint, Solution
Initial Conditions
Well-Posed Problems
Exercise 14.14
Find the solutions of
tdy
dt + Ay = 1 + t2,
t > 0
which are bounded at t = 0. Consider all (real) values of A.
Hint, Solution
805

Equations in the Complex Plane
Exercise 14.15
Classify the singular points of the following ﬁrst order diﬀerential equations, (include the point at inﬁnity).
1. w′ + sin z
z w = 0
2. w′ +
1
z−3w = 0
3. w′ + z1/2w = 0
Hint, Solution
Exercise 14.16
Consider the equation
w′ + z−2w = 0.
The point z = 0 is an irregular singular point of the diﬀerential equation. Thus we know that we cannot expand the
solution about z = 0 in a Frobenius series. Try substituting the series solution
w = zλ
∞
X
n=0
anzn,
a0 ̸= 0
into the diﬀerential equation anyway. What happens?
Hint, Solution
806

14.9
Hints
Hint 14.1
1.
d
dx ln |u| = 1
u
2.
d
dxuc = uc−1u′
Hint 14.2
Hint 14.3
The equation is homogeneous. Make the change of variables u = y/t.
Hint 14.4
Make sure you consider the case α = 0.
Hint 14.5
Hint 14.6
Hint 14.7
The radius of convergence of the series and the distance to the nearest singularity of
1
1−z are not the same.
Exact Equations
Hint 14.8
1.
2.
807

Hint 14.9
1. The equation is exact. Determine the primitive u by solving the equations ux = P, uy = Q.
2. The equation can be made exact.
Hint 14.10
1. This equation is separable. Integrate to get the general solution. Apply the initial condition to determine the
constant of integration.
2. Ditto. You will have to numerically solve an equation to determine where the solution is deﬁned.
Hint 14.11
Hint 14.12
The First Order, Linear Diﬀerential Equation
Hint 14.13
Look in the appendix for the integral of csc x.
Initial Conditions
Well-Posed Problems
Hint 14.14
Equations in the Complex Plane
Hint 14.15
808

Hint 14.16
Try to ﬁnd the value of λ by substituting the series into the diﬀerential equation and equating powers of z.
809

14.10
Solutions
Solution 14.1
1.
y′(x)
y(x) = f(x)
d
dx ln |y(x)| = f(x)
ln |y(x)| =
Z
f(x) dx + c
y(x) = ± e
R
f(x) dx+c
y(x) = c e
R
f(x) dx
2.
yα(x)y′(x) = f(x)
yα+1(x)
α + 1
=
Z
f(x) dx + c
y(x) =

(α + 1)
Z
f(x) dx + a
1/(α+1)
810

3.
y′
cos x + ytan x
cos x = cos x
d
dx

y
cos x

= cos x
y
cos x = sin x + c
y(x) = sin x cos x + c cos x
Solution 14.2
We consider the homogeneous equation,
P(x, y) + Q(x, y)dy
dx = 0.
That is, both P and Q are homogeneous of degree n. We hypothesize that multiplying by
µ(x, y) =
1
xP(x, y) + yQ(x, y)
will make the equation exact. To prove this we use the result that
M(x, y) + N(x, y)dy
dx = 0
is exact if and only if My = Nx.
My = ∂
∂y

P
xP + yQ

= Py(xP + yQ) −P(xPy + Q + yQy)
(xP + yQ)2
811

Nx = ∂
∂x

Q
xP + yQ

= Qx(xP + yQ) −Q(P + xPx + yQx)
(xP + yQ)2
My = Nx
Py(xP + yQ) −P(xPy + Q + yQy) = Qx(xP + yQ) −Q(P + xPx + yQx)
yPyQ −yPQy = xPQx −xPxQ
xPxQ + yPyQ = xPQx + yPQy
(xPx + yPy)Q = P(xQx + yQy)
With Euler’s theorem, this reduces to an identity.
nPQ = PnQ
Thus the equation is exact. µ(x, y) is an integrating factor for the homogeneous equation.
Solution 14.3
We note that this is a homogeneous diﬀerential equation. The coeﬃcient of dy/dt and the inhomogeneity are homo-
geneous of degree zero.
dy
dt = 2
y
t

+
y
t
2
.
We make the change of variables u = y/t to obtain a separable equation.
tu′ + u = 2u + u2
u′
u2 + u = 1
t
812

Now we integrate to solve for u.
u′
u(u + 1) = 1
t
u′
u −
u′
u + 1 = 1
t
ln |u| −ln |u + 1| = ln |t| + c
ln

u
u + 1
 = ln |ct|
u
u + 1 = ±ct
u
u + 1 = ct
u =
ct
1 −ct
u =
t
c −t
y =
t2
c −t
Solution 14.4
We consider
y′ −1
xy = xα,
x > 0.
First we ﬁnd the integrating factor.
I(x) = exp
Z
−1
x dx

= exp (−ln x) = 1
x.
813

We multiply by the integrating factor and integrate.
1
xy′ −1
x2y = xα−1
d
dx
1
xy

= xα−1
1
xy =
Z
xα−1 dx + c
y = x
Z
xα−1 dx + cx
y =
(
xα+1
α
+ cx
for α ̸= 0,
x ln x + cx
for α = 0.
Solution 14.5
1.
y′ + xy = x2n+1,
y(1) = 1,
n ∈Z
We ﬁnd the integrating factor.
I(x) = e
R
x dx = ex2/2
We multiply by the integrating factor and integrate. Since the initial condition is given at x = 1, we will take the
lower bound of integration to be that point.
d
dx

ex2/2 y

= x2n+1 ex2/2
y = e−x2/2
Z x
1
ξ2n+1 eξ2/2 dξ + c e−x2/2
We choose the constant of integration to satisfy the initial condition.
y = e−x2/2
Z x
1
ξ2n+1 eξ2/2 dξ + e(1−x2)/2
814

If n ≥0 then we can use integration by parts to write the integral as a sum of terms. If n < 0 we can write the
integral in terms of the exponential integral function. However, the integral form above is as nice as any other
and we leave the answer in that form.
2.
dy
dx −2xy(x) = 1,
y(0) = 1.
We determine the integrating factor and then integrate the equation.
I(x) = e
R
−2x dx = e−x2
d
dx

e−x2 y

= e−x2
y = ex2 Z x
0
e−ξ2 dξ + c ex2
We choose the constant of integration to satisfy the initial condition.
y = ex2 
1 +
Z x
0
e−ξ2 dξ

We can write the answer in terms of the Error function,
erf(x) ≡
2
√π
Z x
0
e−ξ2 dξ.
y = ex2 
1 +
√π
2 erf(x)

815

Solution 14.6
We determine the integrating factor and then integrate the equation.
I(x) = e
R
α dx = eαx
d
dx (eαx y) = β e(α−λ)x
y = β e−αx
Z
e(α−λ)x dx + c e−αx
First consider the case α ̸= λ.
y = β e−αx e(α−λ)x
α −λ + c e−αx
y =
β
α −λ e−λx +c e−αx
Clearly the solution vanishes as x →∞.
Next consider α = λ.
y = β e−αx x + c e−αx
y = (c + βx) e−αx
We use L’Hospital’s rule to show that the solution vanishes as x →∞.
lim
x→∞
c + βx
eαx
= lim
x→∞
β
α eαx = 0
For β = λ = 1, the solution is
y =
(
1
α−1 e−x +c e−αx
for α ̸= 1,
(c + x) e−x
for α = 1.
816

The solution which satisﬁes the initial condition is
y =
(
1
α−1 (e−x +(α −2) e−αx)
for α ̸= 1,
(1 + x) e−x
for α = 1.
In Figure 14.6 the solution is plotted for α = 1/16, 1/8, . . . , 16.
4
8
12
16
1
Figure 14.6: The Solution for a Range of α
Consider the solution in the limit as α →0.
lim
α→0 y(x) = lim
α→0
1
α −1
 e−x +(α −2) e−αx
= 2 −e−x
817

In the limit as α →∞we have,
lim
α→∞y(x) = lim
α→∞
1
α −1
 e−x +(α −2) e−αx
= lim
α→∞
α −2
α −1 e−αx
=
(
1
for x = 0,
0
for x > 0.
This behavior is shown in Figure 14.7. The ﬁrst graph plots the solutions for α = 1/128, 1/64, . . . , 1. The second
graph plots the solutions for α = 1, 2, . . . , 128.
1
2
3
4
1
1
2
3
4
1
Figure 14.7: The Solution as α →0 and α →∞
818

Solution 14.7
We substitute w = P∞
n=0 anzn into the equation dw
dz +
1
1−zw = 0.
d
dz
∞
X
n=0
anzn +
1
1 −z
∞
X
n=0
anzn = 0
(1 −z)
∞
X
n=1
nanzn−1 +
∞
X
n=0
anzn = 0
∞
X
n=0
(n + 1)an+1zn −
∞
X
n=0
nanzn +
∞
X
n=0
anzn = 0
∞
X
n=0
((n + 1)an+1 −(n −1)an) zn = 0
Equating powers of z to zero, we obtain the relation,
an+1 = n −1
n + 1an.
a0 is arbitrary. We can compute the rest of the coeﬃcients from the recurrence relation.
a1 = −1
1 a0 = −a0
a2 = 0
2a1 = 0
We see that the coeﬃcients are zero for n ≥2. Thus the Taylor series expansion, (and the exact solution), is
w = a0(1 −z).
The radius of convergence of the series in inﬁnite. The nearest singularity of
1
1−z is at z = 1. Thus we see the radius
of convergence can be greater than the distance to the nearest singularity of the coeﬃcient function, p(z).
819

Exact Equations
Solution 14.8
1.
dy
dx = x2 + xy + y2
x2
Since the right side is a homogeneous function of order zero, this is a homogeneous diﬀerential equation. We
make the change of variables u = y/x and then solve the diﬀerential equation for u.
xu′ + u = 1 + u + u2
du
1 + u2 = dx
x
arctan(u) = ln |x| + c
u = tan(ln(|cx|))
y = x tan(ln(|cx|))
2.
(4y −3x) dx + (y −2x) dy = 0
Since the coeﬃcients are homogeneous functions of order one, this is a homogeneous diﬀerential equation. We
820

make the change of variables u = y/x and then solve the diﬀerential equation for u.

4y
x −3

dx +
y
x −2

dy = 0
(4u −3) dx + (u −2)(u dx + x du) = 0
(u2 + 2u −3) dx + x(u −2) du = 0
dx
x +
u −2
(u + 3)(u −1) du = 0
dx
x +
 5/4
u + 3 −1/4
u −1

du = 0
ln(x) + 5
4 ln(u + 3) −1
4 ln(u −1) = c
x4(u + 3)5
u −1
= c
x4(y/x + 3)5
y/x −1
= c
(y + 3x)5
y −x
= c
Solution 14.9
1.
(3x2 −2xy + 2) dx + (6y2 −x2 + 3) dy = 0
We check if this form of the equation, P dx + Q dy = 0, is exact.
Py = −2x,
Qx = −2x
Since Py = Qx, the equation is exact. Now we ﬁnd the primitive u(x, y) which satisﬁes
du = (3x2 −2xy + 2) dx + (6y2 −x2 + 3) dy.
821

The primitive satisﬁes the partial diﬀerential equations
ux = P,
uy = Q.
(14.8)
We integrate the ﬁrst equation of 14.8 to determine u up to a function of integration.
ux = 3x2 −2xy + 2
u = x3 −x2y + 2x + f(y)
We substitute this into the second equation of 14.8 to determine the function of integration up to an additive
constant.
−x2 + f ′(y) = 6y2 −x2 + 3
f ′(y) = 6y2 + 3
f(y) = 2y3 + 3y
The solution of the diﬀerential equation is determined by the implicit equation u = c.
x3 −x2y + 2x + 2y3 + 3y = c
2.
dy
dx = −ax + by
bx + cy
(ax + by) dx + (bx + cy) dy = 0
We check if this form of the equation, P dx + Q dy = 0, is exact.
Py = b,
Qx = b
Since Py = Qx, the equation is exact. Now we ﬁnd the primitive u(x, y) which satisﬁes
du = (ax + by) dx + (bx + cy) dy
822

The primitive satisﬁes the partial diﬀerential equations
ux = P,
uy = Q.
(14.9)
We integrate the ﬁrst equation of 14.9 to determine u up to a function of integration.
ux = ax + by
u = 1
2ax2 + bxy + f(y)
We substitute this into the second equation of 14.9 to determine the function of integration up to an additive
constant.
bx + f ′(y) = bx + cy
f ′(y) = cy
f(y) = 1
2cy2
The solution of the diﬀerential equation is determined by the implicit equation u = d.
ax2 + 2bxy + cy2 = d
Solution 14.10
Note that since these equations are nonlinear, we cannot predict where the solutions will be deﬁned from the equation
alone.
1. This equation is separable. We integrate to get the general solution.
dy
dx = (1 −2x)y2
dy
y2 = (1 −2x) dx
−1
y = x −x2 + c
y =
1
x2 −x −c
823

Now we apply the initial condition.
y(0) = 1
−c = −1
6
y =
1
x2 −x −6
y =
1
(x + 2)(x −3)
The solution is deﬁned on the interval (−2 . . . 3).
2. This equation is separable. We integrate to get the general solution.
x dx + y e−x dy = 0
x ex dx + y dy = 0
(x −1) ex +1
2y2 = c
y =
p
2(c + (1 −x) ex)
We apply the initial condition to determine the constant of integration.
y(0) =
p
2(c + 1) = 1
c = −1
2
y =
p
2(1 −x) ex −1
The function 2(1 −x) ex −1 is plotted in Figure 14.8. We see that the argument of the square root in the
solution is non-negative only on an interval about the origin. Because 2(1 −x) ex −1 == 0 is a mixed algebraic
/ transcendental equation, we cannot solve it analytically. The solution of the diﬀerential equation is deﬁned on
the interval (−1.67835 . . . 0.768039).
824

-5 -4 -3 -2 -1
1
-3
-2
-1
1
Figure 14.8: The function 2(1 −x) ex −1.
Solution 14.11
1. We consider the diﬀerential equation,
(4y −x)y′ −(9x2 + y −1) = 0.
Py = ∂
∂y
 1 −y −9x2
= −1
Qx = ∂
∂x (4y −x) = −1
825

This equation is exact. It is simplest to solve the equation by rearranging terms to form exact derivatives.
4yy′ −xy′ −y + 1 −9x2 = 0
d
dx

2y2 −xy

+ 1 −9x2 = 0
2y2 −xy + x −3x3 + c = 0
y = 1
4

x ±
p
x2 −8(c + x −3x3)

2. We consider the diﬀerential equation,
(2x −2y)y′ + (2x + 4y) = 0.
Py = ∂
∂y (2x + 4y) = 4
Qx = ∂
∂x (2x −2y) = 2
Since Py ̸= Qx, this is not an exact equation.
Solution 14.12
Recall that the diﬀerential equation
P(x, y) + Q(x, y)y′ = 0
is exact if and only if Py = Qx. For Equation 14.7, this criterion is
2y sin t = yf ′(t)
f ′(t) = 2 sin t
f(t) = 2(a −cos t).
826

In this case, the diﬀerential equation is
y2 sin t + 2yy′(a −cos t) = 0.
We can integrate this exact equation by inspection.
d
dt
 y2(a −cos t)

= 0
y2(a −cos t) = c
y = ±
c
√a −cos t
The First Order, Linear Diﬀerential Equation
Solution 14.13
Consider the diﬀerential equation
y′ +
y
sin x = 0.
We use Equation 14.5 to determine the solution.
y = c e
R
−1/ sin x dx
y = c e−ln | tan(x/2)|
y = c
cot
x
2

y = c cot
x
2

Initial Conditions
Well-Posed Problems
Solution 14.14
First we write the diﬀerential equation in the standard form.
dy
dt + A
t y = 1
t + t,
t > 0
827

We determine the integrating factor.
I(t) = e
R
A/t dt = eA ln t = tA
We multiply the diﬀerential equation by the integrating factor and integrate.
dy
dt + A
t y = 1
t + t
d
dt
 tAy

= tA−1 + tA+1
tAy =





tA
A + tA+2
A+2 + c,
A ̸= 0, −2
ln t + 1
2t2 + c,
A = 0
−1
2t−2 + ln t + c,
A = −2
y =





1
A +
t2
A+2 + ct−A,
A ̸= −2
ln t + 1
2t2 + c,
A = 0
−1
2 + t2 ln t + ct2,
A = −2
For positive A, the solution is bounded at the origin only for c = 0. For A = 0, there are no bounded solutions. For
negative A, the solution is bounded there for any value of c and thus we have a one-parameter family of solutions.
In summary, the solutions which are bounded at the origin are:
y =





1
A +
t2
A+2,
A > 0
1
A +
t2
A+2 + ct−A,
A < 0, A ̸= −2
−1
2 + t2 ln t + ct2,
A = −2
Equations in the Complex Plane
Solution 14.15
828

1. Consider the equation w′ + sin z
z w = 0. The point z = 0 is the only point we need to examine in the ﬁnite plane.
Since sin z
z
has a removable singularity at z = 0, there are no singular points in the ﬁnite plane. The substitution
z = 1
ζ yields the equation
u′ −sin(1/ζ)
ζ
u = 0.
Since sin(1/ζ)
ζ
has an essential singularity at ζ = 0, the point at inﬁnity is an irregular singular point of the original
diﬀerential equation.
2. Consider the equation w′ +
1
z−3w = 0. Since
1
z−3 has a simple pole at z = 3, the diﬀerential equation has a
regular singular point there. Making the substitution z = 1/ζ, w(z) = u(ζ)
u′ −
1
ζ2(1/ζ −3)u = 0
u′ −
1
ζ(1 −3ζ)u = 0.
Since this equation has a simple pole at ζ = 0, the original equation has a regular singular point at inﬁnity.
3. Consider the equation w′ + z1/2w = 0. There is an irregular singular point at z = 0. With the substitution
z = 1/ζ, w(z) = u(ζ),
u′ −ζ−1/2
ζ2 u = 0
u′ −ζ−5/2u = 0.
We see that the point at inﬁnity is also an irregular singular point of the original diﬀerential equation.
Solution 14.16
We start with the equation
w′ + z−2w = 0.
829

Substituting w = zλ P∞
n=0 anzn, a0 ̸= 0 yields
d
dz
 
zλ
∞
X
n=0
anzn
!
+ z−2zλ
∞
X
n=0
anzn = 0
λzλ−1
∞
X
n=0
anzn + zλ
∞
X
n=1
nanzn−1 + zλ
∞
X
n=0
anzn−2 = 0
The lowest power of z in the expansion is zλ−2. The coeﬃcient of this term is a0. Equating powers of z demands that
a0 = 0 which contradicts our initial assumption that it was nonzero. Thus we cannot ﬁnd a λ such that the solution
can be expanded in the form,
w = zλ
∞
X
n=0
anzn,
a0 ̸= 0.
830

Chapter 15
First Order Linear Systems of Diﬀerential
Equations
We all agree that your theory is crazy, but is it crazy enough?
- Niels Bohr
15.1
Introduction
In this chapter we consider ﬁrst order linear systems of diﬀerential equations. That is, we consider equations of the
form,
x′(t) = Ax(t) + f(t),
x(t) =



x1(t)
...
xn(t)


,
A =





a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
...
an1
an2
. . .
ann




.
831

Initially we will consider the homogeneous problem, x′(t) = Ax(t). (Later we will ﬁnd particular solutions with variation
of parameters.) The best way to solve these equations is through the use of the matrix exponential. Unfortunately,
using the matrix exponential requires knowledge of the Jordan canonical form and matrix functions. Fortunately, we
can solve a certain class of problems using only the concepts of eigenvalues and eigenvectors of a matrix. We present
this simple method in the next section. In the following section we will take a detour into matrix theory to cover Jordan
canonical form and its applications. Then we will be able to solve the general case.
15.2
Using Eigenvalues and Eigenvectors to ﬁnd Homogeneous So-
lutions
If you have forgotten what eigenvalues and eigenvectors are and how to compute them, go ﬁnd a book on linear
algebra and spend a few minutes re-aquainting yourself with the rudimentary material.
Recall that the single diﬀerential equation x′(t) = Ax has the general solution x = c eAt. Maybe the system of
diﬀerential equations
x′(t) = Ax(t)
(15.1)
has similiar solutions. Perhaps it has a solution of the form x(t) = ξ eλt for some constant vector ξ and some value λ.
Let’s substitute this into the diﬀerential equation and see what happens.
x′(t) = Ax(t)
ξλ eλt = Aξ eλt
Aξ = λξ
We see that if λ is an eigenvalue of A with eigenvector ξ then x(t) = ξ eλt satisﬁes the diﬀerential equation. Since
the diﬀerential equation is linear, cξ eλt is a solution.
Suppose that the n×n matrix A has the eigenvalues {λk} with a complete set of linearly independent eigenvectors
{ξk}. Then each of ξk eλkt is a homogeneous solution of Equation 15.1. We note that each of these solutions is linearly
832

independent. Without any kind of justiﬁcation I will tell you that the general solution of the diﬀerential equation is a
linear combination of these n linearly independent solutions.
Result 15.2.1 Suppose that the n × n matrix A has the eigenvalues {λk} with a complete
set of linearly independent eigenvectors {ξk}. The system of diﬀerential equations,
x′(t) = Ax(t),
has the general solution,
x(t) =
n
X
k=1
ckξk eλkt
Example 15.2.1 (mathematica/ode/systems/systems.nb) Find the solution of the following initial value problem.
Describe the behavior of the solution as t →∞.
x′ = Ax ≡
−2
1
−5
4

x,
x(0) = x0 ≡
1
3

The matrix has the distinct eigenvalues λ1 = −1, λ2 = 3. The corresponding eigenvectors are
x1 =
1
1

,
x2 =
1
5

.
The general solution of the system of diﬀerential equations is
x = c1
1
1

e−t +c2
1
5

e3t .
833

We apply the initial condition to determine the constants.
1
1
1
5
 c1
c2

=
1
3

c1 = 1
2,
c2 = 1
2
The solution subject to the initial condition is
x = 1
2
1
1

e−t +1
2
1
5

e3t
For large t, the solution looks like
x ≈1
2
1
5

e3t .
Both coordinates tend to inﬁnity.
Figure 15.1 shows some homogeneous solutions in the phase plane.
Example 15.2.2 (mathematica/ode/systems/systems.nb) Find the solution of the following initial value problem.
Describe the behavior of the solution as t →∞.
x′ = Ax ≡


1
1
2
0
2
2
−1
1
3

x,
x(0) = x0 ≡


2
0
1


The matrix has the distinct eigenvalues λ1 = 1, λ2 = 2, λ3 = 3. The corresponding eigenvectors are
x1 =


0
−2
1

,
x2 =


1
1
0

,
x3 =


2
2
1

.
834

-10 -7.5
-5
-2.5
2.5
5
7.5
10
-10
-7.5
-5
-2.5
2.5
5
7.5
10
Figure 15.1: Homogeneous solutions in the phase plane.
The general solution of the system of diﬀerential equations is
x = c1


0
−2
1

et +c2


1
1
0

e2t +c3


2
2
1

e3t .
We apply the initial condition to determine the constants.


0
1
2
−2
1
2
1
0
1




c1
c2
c3

=


2
0
1


c1 = 1,
c2 = 2,
c3 = 0
835

The solution subject to the initial condition is
x =


0
−2
1

et +2


1
1
0

e2t .
As t →∞, all coordinates tend to inﬁnity.
Exercise 15.1 (mathematica/ode/systems/systems.nb)
Find the solution of the following initial value problem. Describe the behavior of the solution as t →∞.
x′ = Ax ≡
1
−5
1
−3

x,
x(0) = x0 ≡
1
1

Hint, Solution
Exercise 15.2 (mathematica/ode/systems/systems.nb)
Find the solution of the following initial value problem. Describe the behavior of the solution as t →∞.
x′ = Ax ≡


−3
0
2
1
−1
0
−2
−1
0

x,
x(0) = x0 ≡


1
0
0


Hint, Solution
Exercise 15.3
Use the matrix form of the method of variation of parameters to ﬁnd the general solution of
dx
dt =
4
−2
8
−4

x +
 t−3
−t−2

,
t > 0.
Hint, Solution
836

15.3
Matrices and Jordan Canonical Form
Functions of Square Matrices.
Consider a function f(x) with a Taylor series.
f(x) =
∞
X
n=0
f (n)(0)
n!
xn
We can deﬁne the function to take square matrices as arguments. The function of the square matrix A is deﬁned in
terms of the Taylor series.
f(A) =
∞
X
n=0
f (n)(0)
n!
An
(Note that this deﬁnition is usually not the most convenient method for computing a function of a matrix. Use the
Jordan canonical form for that.)
Eigenvalues and Eigenvectors.
Consider a square matrix A. A nonzero vector x is an eigenvector of the matrix
with eigenvalue λ if
Ax = λx.
Note that we can write this equation as
(A −λI)x = 0.
This equation has solutions for nonzero x if and only if A −λI is singular, (det(A −λI) = 0).
We deﬁne the
characteristic polynomial of the matrix χ(λ) as this determinant.
χ(λ) = det(A −λI)
The roots of the characteristic polynomial are the eigenvalues of the matrix. The eigenvectors of distinct eigenvalues
are linearly independent. Thus if a matrix has distinct eigenvalues, the eigenvectors form a basis.
If λ is a root of χ(λ) of multiplicity m then there are up to m linearly independent eigenvectors corresponding to
that eigenvalue. That is, it has from 1 to m eigenvectors.
837

Diagonalizing Matrices.
Consider an n×n matrix A that has a complete set of n linearly independent eigenvectors.
A may or may not have distinct eigenvalues. Consider the matrix S with eigenvectors as columns.
S =
 x1
x2
· · ·
xn

A is diagonalized by the similarity transformation:
Λ = S−1AS.
Λ is a diagonal matrix with the eigenvalues of A as the diagonal elements. Furthermore, the kth diagonal element is
λk, the eigenvalue corresponding to the the eigenvector, xk.
Generalized Eigenvectors.
A vector xk is a generalized eigenvector of rank k if
(A −λI)kxk = 0
but
(A −λI)k−1xk ̸= 0.
Eigenvectors are generalized eigenvectors of rank 1. An n×n matrix has n linearly independent generalized eigenvectors.
A chain of generalized eigenvectors generated by the rank m generalized eigenvector xm is the set: {x1, x2, . . . , xm},
where
xk = (A −λI)xk+1,
for
k = m −1, . . . , 1.
Computing Generalized Eigenvectors.
Let λ be an eigenvalue of multiplicity m. Let n be the smallest integer
such that
rank (nullspace ((A −λI)n)) = m.
Let Nk denote the number of eigenvalues of rank k. These have the value:
Nk = rank
 nullspace
 (A −λI)k
−rank
 nullspace
 (A −λI)k−1
.
One can compute the generalized eigenvectors of a matrix by looping through the following three steps until all the
the Nk are zero:
838

1. Select the largest k for which Nk is positive.
Find a generalized eigenvector xk of rank k which is linearly
independent of all the generalized eigenvectors found thus far.
2. From xk generate the chain of eigenvectors {x1, x2, . . . , xk}. Add this chain to the known generalized eigenvec-
tors.
3. Decrement each positive Nk by one.
Example 15.3.1 Consider the matrix
A =


1
1
1
2
1
−1
−3
2
4

.
The characteristic polynomial of the matrix is
χ(λ) =

1 −λ
1
1
2
1 −λ
−1
−3
2
4 −λ

= (1 −λ)2(4 −λ) + 3 + 4 + 3(1 −λ) −2(4 −λ) + 2(1 −λ)
= −(λ −2)3.
Thus we see that λ = 2 is an eigenvalue of multiplicity 3. A −2I is
A −2I =


−1
1
1
2
−1
−1
−3
2
2


The rank of the nullspace space of A −2I is less than 3.
(A −2I)2 =


0
0
0
−1
1
1
1
−1
−1


839

The rank of nullspace((A −2I)2) is less than 3 as well, so we have to take one more step.
(A −2I)3 =


0
0
0
0
0
0
0
0
0


The rank of nullspace((A −2I)3) is 3. Thus there are generalized eigenvectors of ranks 1, 2 and 3. The generalized
eigenvector of rank 3 satisﬁes:
(A −2I)3x3 = 0


0
0
0
0
0
0
0
0
0

x3 = 0
We choose the solution
x3 =


1
0
0

.
Now to compute the chain generated by x3.
x2 = (A −2I)x3 =


−1
2
−3


x1 = (A −2I)x2 =


0
−1
1


Thus a set of generalized eigenvectors corresponding to the eigenvalue λ = 2 are
x1 =


0
−1
1

,
x2 =


−1
2
−3

,
x3 =


1
0
0

.
840

Jordan Block.
A Jordan block is a square matrix which has the constant, λ, on the diagonal and ones on the ﬁrst
super-diagonal:










λ
1
0
· · ·
0
0
0
λ
1
· · ·
0
0
0
0
λ
...
0
0
...
...
...
...
...
...
0
0
0
...
λ
1
0
0
0
· · ·
0
λ










Jordan Canonical Form.
A matrix J is in Jordan canonical form if all the elements are zero except for Jordan
blocks Jk along the diagonal.
J =








J1
0
· · ·
0
0
0
J2
...
0
0
...
...
...
...
...
0
0
...
Jn−1
0
0
0
· · ·
0
Jn








The Jordan canonical form of a matrix is obtained with the similarity transformation:
J = S−1AS,
where S is the matrix of the generalized eigenvectors of A and the generalized eigenvectors are grouped in chains.
Example 15.3.2 Again consider the matrix
A =


1
1
1
2
1
−1
−3
2
4

.
841

Since λ = 2 is an eigenvalue of multiplicity 3, the Jordan canonical form of the matrix is
J =


2
1
0
0
2
1
0
0
2

.
In Example 15.3.1 we found the generalized eigenvectors of A. We deﬁne the matrix with generalized eigenvectors as
columns:
S =


0
−1
1
−1
2
0
1
−3
0

.
We can verify that J = S−1AS.
J = S−1AS
=


0
−3
−2
0
−1
−1
1
−1
−1




1
1
1
2
1
−1
−3
2
4




0
−1
1
−1
2
0
1
−3
0


=


2
1
0
0
2
1
0
0
2


Functions of Matrices in Jordan Canonical Form.
The function of an n × n Jordan block is the upper-
triangular matrix:
f(Jk) =











f(λ)
f′(λ)
1!
f′′(λ)
2!
· · ·
f(n−2)(λ)
(n−2)!
f(n−1)(λ)
(n−1)!
0
f(λ)
f′(λ)
1!
· · ·
f(n−3)(λ)
(n−3)!
f(n−2)(λ)
(n−2)!
0
0
f(λ)
...
f(n−4)(λ)
(n−4)!
f(n−3)(λ)
(n−3)!
...
...
...
...
...
...
0
0
0
...
f(λ)
f′(λ)
1!
0
0
0
· · ·
0
f(λ)











842

The function of a matrix in Jordan canonical form is
f(J) =








f(J1)
0
· · ·
0
0
0
f(J2)
...
0
0
...
...
...
...
...
0
0
...
f(Jn−1)
0
0
0
· · ·
0
f(Jn)








The Jordan canonical form of a matrix satisﬁes:
f(J) = S−1f(A)S,
where S is the matrix of the generalized eigenvectors of A. This gives us a convenient method for computing functions
of matrices.
Example 15.3.3 Consider the matrix exponential function eA for our old friend:
A =


1
1
1
2
1
−1
−3
2
4

.
In Example 15.3.2 we showed that the Jordan canonical form of the matrix is
J =


2
1
0
0
2
1
0
0
2

.
Since all the derivatives of eλ are just eλ, it is especially easy to compute eJ.
eJ =


e2
e2
e2 /2
0
e2
e2
0
0
e2


843

We ﬁnd eA with a similarity transformation of eJ. We use the matrix of generalized eigenvectors found in Example 15.3.2.
eA = S eJ S−1
eA =


0
−1
1
−1
2
0
1
−3
0




e2
e2
e2 /2
0
e2
e2
0
0
e2




0
−3
−2
0
−1
−1
1
−1
−1


eA =


0
2
2
3
1
−1
−5
3
5

e2
2
15.4
Using the Matrix Exponential
The homogeneous diﬀerential equation
x′(t) = Ax(t)
has the solution
x(t) = eAt c
where c is a vector of constants. The solution subject to the initial condition, x(t0) = x0 is
x(t) = eA(t−t0) x0.
The homogeneous diﬀerential equation
x′(t) = 1
t Ax(t)
has the solution
x(t) = tAc ≡eA Log t c,
where c is a vector of constants. The solution subject to the initial condition, x(t0) = x0 is
x(t) =
 t
t0
A
x0 ≡eA Log(t/t0) x0.
844

The inhomogeneous problem
x′(t) = Ax(t) + f(t),
x(t0) = x0
has the solution
x(t) = eA(t−t0) x0 + eAt
Z t
t0
e−Aτ f(τ) dτ.
Example 15.4.1 Consider the system
dx
dt =


1
1
1
2
1
−1
−3
2
4

x.
The general solution of the system of diﬀerential equations is
x(t) = eAt c.
In Example 15.3.3 we found eA. At is just a constant times A. The eigenvalues of At are {λkt} where {λk} are the
eigenvalues of A. The generalized eigenvectors of At are the same as those of A.
Consider eJt. The derivatives of f(λ) = eλt are f ′(λ) = t eλt and f ′′(λ) = t2 eλt. Thus we have
eJt =


e2t
t e2t
t2 e2t /2
0
e2t
t e2t
0
0
e2t


eJt =


1
t
t2/2
0
1
t
0
0
1

e2t
845

We ﬁnd eAt with a similarity transformation.
eAt = S eJt S−1
eAt =


0
−1
1
−1
2
0
1
−3
0




1
t
t2/2
0
1
t
0
0
1

e2t


0
−3
−2
0
−1
−1
1
−1
−1


eAt =


1 −t
t
t
2t −t2/2
1 −t + t2/2
−t + t2/2
−3t + t2/2
2t −t2/2
1 + 2t −t2/2

e2t
The solution of the system of diﬀerential equations is
x(t) =

c1


1 −t
2t −t2/2
−3t + t2/2

+ c2


t
1 −t + t2/2
2t −t2/2

+ c3


t
−t + t2/2
1 + 2t −t2/2



e2t
Example 15.4.2 Consider the Euler equation system
dx
dt = 1
t Ax ≡1
t
1
0
1
1

x.
The solution is x(t) = tAc. Note that A is almost in Jordan canonical form. It has a one on the sub-diagonal instead
of the super-diagonal. It is clear that a function of A is deﬁned
f(A) =
f(1)
0
f ′(1)
f(1)

.
The function f(λ) = tλ has the derivative f ′(λ) = tλ log t. Thus the solution of the system is
x(t) =

t
0
t log t
t
 c1
c2

= c1

t
t log t

+ c2
0
t

846

Example 15.4.3 Consider an inhomogeneous system of diﬀerential equations.
dx
dt = Ax + f(t) ≡
4
−2
8
−4

x +
 t−3
−t−2

,
t > 0.
The general solution is
x(t) = eAt c + eAt
Z
e−At f(t) dt.
First we ﬁnd homogeneous solutions. The characteristic equation for the matrix is
χ(λ) =

4 −λ
−2
8
−4 −λ
 = λ2 = 0
λ = 0 is an eigenvalue of multiplicity 2. Thus the Jordan canonical form of the matrix is
J =
0
1
0
0

.
Since rank(nullspace(A −0I)) = 1 there is only one eigenvector. A generalized eigenvector of rank 2 satisﬁes
(A −0I)2x2 = 0
0
0
0
0

x2 = 0
We choose
x2 =
1
0

Now we generate the chain from x2.
x1 = (A −0I)x2 =
4
8

847

We deﬁne the matrix of generalized eigenvectors S.
S =
4
1
8
0

The derivative of f(λ) = eλt is f ′(λ) = t eλt. Thus
eJt =
1
t
0
1

The homogeneous solution of the diﬀerential equation system is xh = eAt c where
eAt = S eJt S−1
eAt =
4
1
8
0

.
1
t
0
1
 0
1/8
1
−1/2

eAt =
1 + 4t
−2t
8t
1 −4t

The general solution of the inhomogeneous system of equations is
x(t) = eAt c + eAt
Z
e−At f(t) dt
x(t) =
1 + 4t
−2t
8t
1 −4t

c +
1 + 4t
−2t
8t
1 −4t
 Z 1 −4t
2t
−8t
1 + 4t
  t−3
−t−2

dt
x(t) = c1
1 + 4t
8t

+ c2
 −2t
1 −4t

+
2 −2 Log t + 6
t −
1
2t2
4 −4 Log t + 13
t

We can tidy up the answer a little bit. First we take linear combinations of the homogeneous solutions to obtain a
simpler form.
x(t) = c1
1
2

+ c2

2t
4t −1

+
2 −2 Log t + 6
t −
1
2t2
4 −4 Log t + 13
t

848

Then we subtract 2 times the ﬁrst homogeneous solution from the particular solution.
x(t) = c1
1
2

+ c2

2t
4t −1

+
−2 Log t + 6
t −
1
2t2
−4 Log t + 13
t

849

15.5
Exercises
Exercise 15.4 (mathematica/ode/systems/systems.nb)
Find the solution of the following initial value problem.
x′ = Ax ≡
−2
1
−5
4

x,
x(0) = x0 ≡
1
3

Hint, Solution
Exercise 15.5 (mathematica/ode/systems/systems.nb)
Find the solution of the following initial value problem.
x′ = Ax ≡


1
1
2
0
2
2
−1
1
3

x,
x(0) = x0 ≡


2
0
1


Hint, Solution
Exercise 15.6 (mathematica/ode/systems/systems.nb)
Find the solution of the following initial value problem. Describe the behavior of the solution as t →∞.
x′ = Ax ≡
1
−5
1
−3

x,
x(0) = x0 ≡
1
1

Hint, Solution
Exercise 15.7 (mathematica/ode/systems/systems.nb)
Find the solution of the following initial value problem. Describe the behavior of the solution as t →∞.
x′ = Ax ≡


−3
0
2
1
−1
0
−2
−1
0

x,
x(0) = x0 ≡


1
0
0


Hint, Solution
850

Exercise 15.8 (mathematica/ode/systems/systems.nb)
Find the solution of the following initial value problem. Describe the behavior of the solution as t →∞.
x′ = Ax ≡
1
−4
4
−7

x,
x(0) = x0 ≡
3
2

Hint, Solution
Exercise 15.9 (mathematica/ode/systems/systems.nb)
Find the solution of the following initial value problem. Describe the behavior of the solution as t →∞.
x′ = Ax ≡


−1
0
0
−4
1
0
3
6
2

x,
x(0) = x0 ≡


−1
2
−30


Hint, Solution
Exercise 15.10
1. Consider the system
x′ = Ax =


1
1
1
2
1
−1
−3
2
4

x.
(15.2)
(a) Show that λ = 2 is an eigenvalue of multiplicity 3 of the coeﬃcient matrix A, and that there is only one
corresponding eigenvector, namely
ξ(1) =


0
1
−1

.
(b) Using the information in part (i), write down one solution x(1)(t) of the system (15.2). There is no other
solution of a purely exponential form x = ξ eλt.
(c) To ﬁnd a second solution use the form x = ξt e2t +η e2t, and ﬁnd appropriate vectors ξ and η. This gives
a solution of the system (15.2) which is independent of the one obtained in part (ii).
851

(d) To ﬁnd a third linearly independent solution use the form x = ξ(t2/2) e2t +ηt e2t +ζ e2t. Show that ξ, η
and ζ satisfy the equations
(A −2I)ξ = 0,
(A −2I)η = ξ,
(A −2I)ζ = η.
The ﬁrst two equations can be taken to coincide with those obtained in part (iii). Solve the third equation,
and write down a third independent solution of the system (15.2).
2. Consider the system
x′ = Ax =


5
−3
−2
8
−5
−4
−4
3
3

x.
(15.3)
(a) Show that λ = 1 is an eigenvalue of multiplicity 3 of the coeﬃcient matrix A, and that there are only two
linearly independent eigenvectors, which we may take as
ξ(1) =


1
0
2

,
ξ(2) =


0
2
−3


Find two independent solutions of equation (15.3).
(b) To ﬁnd a third solution use the form x = ξt et +ηet; then show that ξ and η must satisfy
(A −I)ξ = 0,
(A −I)η = ξ.
Show that the most general solution of the ﬁrst of these equations is ξ = c1ξ1 + c2ξ2, where c1 and c2
are arbitrary constants. Show that, in order to solve the second of these equations it is necessary to take
c1 = c2. Obtain such a vector η, and use it to obtain a third independent solution of the system (15.3).
Hint, Solution
852

Exercise 15.11 (mathematica/ode/systems/systems.nb)
Consider the system of ODE’s
dx
dt = Ax,
x(0) = x0
where A is the constant 3 × 3 matrix
A =


1
1
1
2
1
−1
−8
−5
−3


1. Find the eigenvalues and associated eigenvectors of A. [HINT: notice that λ = −1 is a root of the characteristic
polynomial of A.]
2. Use the results from part (a) to construct eAt and therefore the solution to the initial value problem above.
3. Use the results of part (a) to ﬁnd the general solution to
dx
dt = 1
t Ax.
Hint, Solution
Exercise 15.12 (mathematica/ode/systems/systems.nb)
1. Find the general solution to
dx
dt = Ax
where
A =


2
0
1
0
2
0
0
1
3


2. Solve
dx
dt = Ax + g(t),
x(0) = 0
853

using A from part (a).
Hint, Solution
Exercise 15.13
Let A be an n × n matrix of constants. The system
dx
dt = 1
t Ax,
(15.4)
is analogous to the Euler equation.
1. Verify that when A is a 2 × 2 constant matrix, elimination of (15.4) yields a second order Euler diﬀerential
equation.
2. Now assume that A is an n × n matrix of constants. Show that this system, in analogy with the Euler equation
has solutions of the form x = atλ where a is a constant vector provided a and λ satisfy certain conditions.
3. Based on your experience with the treatment of multiple roots in the solution of constant coeﬃcient systems,
what form will the general solution of (15.4) take if λ is a multiple eigenvalue in the eigenvalue problem derived
in part (b)?
4. Verify your prediction by deriving the general solution for the system
dx
dt = 1
t
1
0
1
1

x.
Hint, Solution
854

15.6
Hints
Hint 15.1
Hint 15.2
Hint 15.3
Hint 15.4
Hint 15.5
Hint 15.6
Hint 15.7
Hint 15.8
Hint 15.9
Hint 15.10
855

Hint 15.11
Hint 15.12
Hint 15.13
856

15.7
Solutions
Solution 15.1
We consider an initial value problem.
x′ = Ax ≡
1
−5
1
−3

x,
x(0) = x0 ≡
1
1

The matrix has the distinct eigenvalues λ1 = −1 −ı, λ2 = −1 + ı. The corresponding eigenvectors are
x1 =
2 −ı
1

,
x2 =
2 + ı
1

.
The general solution of the system of diﬀerential equations is
x = c1
2 −ı
1

e(−1−ı)t +c2
2 + ı
1

e(−1+ı)t .
We can take the real and imaginary parts of either of these solution to obtain real-valued solutions.
2 + ı
1

e(−1+ı)t =
2 cos(t) −sin(t)
cos(t)

e−t +ı
cos(t) + 2 sin(t)
sin(t)

e−t
x = c1
2 cos(t) −sin(t)
cos(t)

e−t +c2
cos(t) + 2 sin(t)
sin(t)

e−t
We apply the initial condition to determine the constants.
2
1
1
0
 c1
c2

=
1
1

c1 = 1,
c2 = −1
The solution subject to the initial condition is
x =
cos(t) −3 sin(t)
cos(t) −sin(t)

e−t .
Plotted in the phase plane, the solution spirals in to the origin as t increases. Both coordinates tend to zero as t →∞.
857

Solution 15.2
We consider an initial value problem.
x′ = Ax ≡


−3
0
2
1
−1
0
−2
−1
0

x,
x(0) = x0 ≡


1
0
0


The matrix has the distinct eigenvalues λ1 = −2, λ2 = −1−ı
√
2, λ3 = −1+ı
√
2. The corresponding eigenvectors
are
x1 =


2
−2
1

,
x2 =


2 + ı
√
2
−1 + ı
√
2
3

,
x3 =


2 −ı
√
2
−1 −ı
√
2
3

.
The general solution of the system of diﬀerential equations is
x = c1


2
−2
1

e−2t +c2


2 + ı
√
2
−1 + ı
√
2
3

e(−1−ı
√
2)t +c3


2 −ı
√
2
−1 −ı
√
2
3

e(−1+ı
√
2)t .
We can take the real and imaginary parts of the second or third solution to obtain two real-valued solutions.


2 + ı
√
2
−1 + ı
√
2
3

e(−1−ı
√
2)t =


2 cos(
√
2t) +
√
2 sin(
√
2t)
−cos(
√
2t) +
√
2 sin(
√
2t)
3 cos(
√
2t)

e−t +ı


√
2 cos(
√
2t) −2 sin(
√
2t)
√
2 cos(
√
2t) + sin(
√
2t)
−3 sin(
√
2t)

e−t
x = c1


2
−2
1

e−2t +c2


2 cos(
√
2t) +
√
2 sin(
√
2t)
−cos(
√
2t) +
√
2 sin(
√
2t)
3 cos(
√
2t)

e−t +c3


√
2 cos(
√
2t) −2 sin(
√
2t)
√
2 cos(
√
2t) + sin(
√
2t)
−3 sin(
√
2t)

e−t
858

We apply the initial condition to determine the constants.


2
2
√
2
−2
−1
√
2
1
3
0




c1
c2
c3

=


1
0
0


c1 = 1
3,
c2 = −1
9,
c3 =
5
9
√
2
The solution subject to the initial condition is
x = 1
3


2
−2
1

e−2t +1
6


2 cos(
√
2t) −4
√
2 sin(
√
2t)
4 cos(
√
2t) +
√
2 sin(
√
2t)
−2 cos(
√
2t) −5
√
2 sin(
√
2t)

e−t .
As t →∞, all coordinates tend to inﬁnity. Plotted in the phase plane, the solution would spiral in to the origin.
Solution 15.3
Homogeneous Solution, Method 1. We designate the inhomogeneous system of diﬀerential equations
x′ = Ax + g(t).
First we ﬁnd homogeneous solutions. The characteristic equation for the matrix is
χ(λ) =

4 −λ
−2
8
−4 −λ
 = λ2 = 0
λ = 0 is an eigenvalue of multiplicity 2. The eigenvectors satisfy
4
−2
8
−4
 ξ1
ξ2

=
0
0

.
Thus we see that there is only one linearly independent eigenvector. We choose
ξ =
1
2

.
859

One homogeneous solution is then
x1 =
1
2

e0t =
1
2

.
We look for a second homogeneous solution of the form
x2 = ξt + η.
We substitute this into the homogeneous equation.
x′
2 = Ax2
ξ = A(ξt + η)
We see that ξ and η satisfy
Aξ = 0,
Aη = ξ.
We choose ξ to be the eigenvector that we found previously. The equation for η is then
4
−2
8
−4
 η1
η2

=
1
2

.
η is determined up to an additive multiple of ξ. We choose
η =

0
−1/2

.
Thus a second homogeneous solution is
x2 =
1
2

t +

0
−1/2

.
The general homogeneous solution of the system is
xh = c1
1
2

+ c2

t
2t −1/2

860

We can write this in matrix notation using the fundamental matrix Ψ(t).
xh = Ψ(t)c =
1
t
2
2t −1/2
 c1
c2

Homogeneous Solution, Method 2. The similarity transform C−1AC with
C =
1
0
2
−1/2

will convert the matrix
A =
4
−2
8
−4

to Jordan canonical form. We make the change of variables,
y =
1
0
2
−1/2

x.
The homogeneous system becomes
dy
dt =
1
0
4
−2
 4
−2
8
−4
 1
0
2
−1/2

y
y′
1
y′
2

=
0
1
0
0
 y1
y2

The equation for y2 is
y′
2 = 0.
y2 = c2
The equation for y1 becomes
y′
1 = c2.
y1 = c1 + c2t
861

The solution for y is then
y = c1
1
0

+ c2
t
1

.
We multiply this by C to obtain the homogeneous solution for x.
xh = c1
1
2

+ c2

t
2t −1/2

Inhomogeneous Solution. By the method of variation of parameters, a particular solution is
xp = Ψ(t)
Z
Ψ−1(t)g(t) dt.
xp =
1
t
2
2t −1/2
 Z 1 −4t
2t
4
−2
  t−3
−t−2

dt
xp =
1
t
2
2t −1/2
 Z −2t−1 −4t−2 + t−3
2t−2 + 4t−3

dt
xp =
1
t
2
2t −1/2
 −2 log t + 4t−1 −1
2t−2
−2t−1 −2t−2

xp =
−2 −2 log t + 2t−1 −1
2t−2
−4 −4 log t + 5t−1

By adding 2 times our ﬁrst homogeneous solution, we obtain
xp =
−2 log t + 2t−1 −1
2t−2
−4 log t + 5t−1

The general solution of the system of diﬀerential equations is
x = c1
1
2

+ c2

t
2t −1/2

+
−2 log t + 2t−1 −1
2t−2
−4 log t + 5t−1

862

Solution 15.4
We consider an initial value problem.
x′ = Ax ≡
−2
1
−5
4

x,
x(0) = x0 ≡
1
3

The Jordan canonical form of the matrix is
J =
−1
0
0
3

.
The solution of the initial value problem is x = eAt x0.
x = eAt x0
= S eJt S−1x0
=
1
1
1
5
 e−t
0
0
e3t
 1
4
 5
−1
−1
1
 1
3

= 1
2
 e−t + e3t
e−t +5 e3t

x = 1
2
1
1

e−t +1
2
1
5

e3t
Solution 15.5
We consider an initial value problem.
x′ = Ax ≡


1
1
2
0
2
2
−1
1
3

x,
x(0) = x0 ≡


2
0
1


The Jordan canonical form of the matrix is
J =


1
0
0
0
2
0
0
0
3

.
863

The solution of the initial value problem is x = eAt x0.
x = eAt x0
= S eJt S−1x0
=


0
1
2
−2
1
2
1
0
1




et
0
0
0
e2t
0
0
0
e3t

1
2


1
−1
0
4
−2
−4
−1
1
2




2
0
1


=


2 e2t
−2 et +2 e2t
et


x =


0
−2
1

et +


2
2
0

e2t .
Solution 15.6
We consider an initial value problem.
x′ = Ax ≡
1
−5
1
−3

x,
x(0) = x0 ≡
1
1

The Jordan canonical form of the matrix is
J =
−1 −ı
0
0
−1 + ı

.
864

The solution of the initial value problem is x = eAt x0.
x = eAt x0
= S eJt S−1x0
=
2 −ı
2 + ı
1
1
 e(−1−ı)t
0
0
e(−1+ı)t
 1
2
 ı
1 −ı2
−ı
1 + ı2
 1
1

=
(cos(t) −3 sin(t)) e−t
(cos(t) −sin(t)) e−t

x =
1
1

e−t cos(t) −
3
1

e−t sin(t)
Solution 15.7
We consider an initial value problem.
x′ = Ax ≡


−3
0
2
1
−1
0
−2
−1
0

x,
x(0) = x0 ≡


1
0
0


The Jordan canonical form of the matrix is
J =


−2
0
0
0
−1 −ı
√
2
0
0
0
−1 + ı
√
2

.
865

The solution of the initial value problem is x = eAt x0.
x = eAt x0
= S eJt S−1x0
= 1
3


6
2 + ı
√
2
2 −ı
√
2
−6
−1 + ı
√
2
−1 −ı
√
2
3
3
3




e−2t
0
0
0
e(−1−ı
√
2)t
0
0
0
e(−1+ı
√
2)t


1
6


2
−2
−2
−1 −ı5
√
2/2
1 −ı2
√
2
4 + ı
√
2
−1 + ı5
√
2/2
1 + ı2
√
2
4 −ı
√
2




1
0
0


x = 1
3


2
−2
1

e−2t +1
6


2 cos(
√
2t) −4
√
2 sin(
√
2t)
4 cos(
√
2t) +
√
2 sin(
√
2t)
−2 cos(
√
2t) −5
√
2 sin(
√
2t)

e−t .
Solution 15.8
We consider an initial value problem.
x′ = Ax ≡
1
−4
4
−7

x,
x(0) = x0 ≡
3
2

Method 1. Find Homogeneous Solutions. The matrix has the double eigenvalue λ1 = λ2 = −3. There is only
866

one corresponding eigenvector. We compute a chain of generalized eigenvectors.
(A + 3I)2x2 = 0
0x2 = 0
x2 =
1
0

(A + 3I)x2 = x1
x1 =
4
4

The general solution of the system of diﬀerential equations is
x = c1
1
1

e−3t +c2
4
4

t +
1
0

e−3t .
We apply the initial condition to determine the constants.
1
1
1
0
 c1
c2

=
3
2

c1 = 2,
c2 = 1
The solution subject to the initial condition is
x =
3 + 4t
2 + 4t

e−3t .
Both coordinates tend to zero as t →∞.
Method 2. Use the Exponential Matrix. The Jordan canonical form of the matrix is
J =
−3
1
0
−3

.
867

The solution of the initial value problem is x = eAt x0.
x = eAt x0
= S eJt S−1x0
=
1
1/4
1
0
 e−3t
t e−3t
0
e−3t
 0
1
4
−4
 3
2

x =
3 + 4t
2 + 4t

e−3t .
Solution 15.9
We consider an initial value problem.
x′ = Ax ≡


−1
0
0
−4
1
0
3
6
2

x,
x(0) = x0 ≡


−1
2
−30


Method 1. Find Homogeneous Solutions. The matrix has the distinct eigenvalues λ1 = −1, λ2 = 1, λ3 = 2.
The corresponding eigenvectors are
x1 =


−1
−2
5

,
x2 =


0
−1
6

,
x3 =


0
0
1

.
The general solution of the system of diﬀerential equations is
x = c1


−1
−2
5

e−t +c2


0
−1
6

et +c3


0
0
1

e2t .
868

We apply the initial condition to determine the constants.


−1
0
0
−2
−1
0
5
6
1




c1
c2
c3

=


−1
2
−30


c1 = 1,
c2 = −4,
c3 = −11
The solution subject to the initial condition is
x =


−1
−2
5

e−t −4


0
−1
6

et −11


0
0
1

e2t .
As t →∞, the ﬁrst coordinate vanishes, the second coordinate tends to ∞and the third coordinate tends to −∞
Method 2. Use the Exponential Matrix. The Jordan canonical form of the matrix is
J =


−1
0
0
0
1
0
0
0
2

.
The solution of the initial value problem is x = eAt x0.
x = eAt x0
= S eJt S−1x0
=


−1
0
0
−2
−1
0
5
6
1




e−t
0
0
0
et
0
0
0
e2t

1
2


−1
0
0
2
−1
0
−7
6
1




−1
2
−30


x =


−1
−2
5

e−t −4


0
−1
6

et −11


0
0
1

e2t .
869

Solution 15.10
1.
(a) We compute the eigenvalues of the matrix.
χ(λ) =

1 −λ
1
1
2
1 −λ
−1
−3
2
4 −λ

= −λ3 + 6λ2 −12λ + 8 = −(λ −2)3
λ = 2 is an eigenvalue of multiplicity 3. The rank of the null space of A −2I is 1. (The ﬁrst two rows are
linearly independent, but the third is a linear combination of the ﬁrst two.)
A −2I =


−1
1
1
2
−1
−1
−3
2
2


Thus there is only one eigenvector.


−1
1
1
2
−1
−1
−3
2
2




ξ1
ξ2
ξ3

= 0
ξ(1) =


0
1
−1


(b) One solution of the system of diﬀerential equations is
x(1) =


0
1
−1

e2t .
(c) We substitute the form x = ξt e2t +η e2t into the diﬀerential equation.
x′ = Ax
ξ e2t +2ξt e2t +2η e2t = Aξt e2t +Aη e2t
(A −2I)ξ = 0,
(A −2I)η = ξ
870

We already have a solution of the ﬁrst equation, we need the generalized eigenvector η. Note that η is only
determined up to a constant times ξ. Thus we look for the solution whose second component vanishes to
simplify the algebra.
(A −2I)η = ξ


−1
1
1
2
−1
−1
−3
2
2




η1
0
η3

=


0
1
−1


−η1 + η3 = 0,
2η1 −η3 = 1,
−3η1 + 2η3 = −1
η =


1
0
1


A second linearly independent solution is
x(2) =


0
1
−1

t e2t +


1
0
1

e2t .
(d) To ﬁnd a third solution we substutite the form x = ξ(t2/2) e2t +ηt e2t +ζ e2t into the diﬀerential equation.
x′ = Ax
2ξ(t2/2) e2t +(ξ + 2η)t e2t +(η + 2ζ) e2t = Aξ(t2/2) e2t +Aηt e2t +Aζ e2t
(A −2I)ξ = 0,
(A −2I)η = ξ,
(A −2I)ζ = η
We have already solved the ﬁrst two equations, we need the generalized eigenvector ζ. Note that ζ is only
determined up to a constant times ξ. Thus we look for the solution whose second component vanishes to
871

simplify the algebra.
(A −2I)ζ = η


−1
1
1
2
−1
−1
−3
2
2




ζ1
0
ζ3

=


1
0
1


−ζ1 + ζ3 = 1,
2ζ1 −ζ3 = 0,
−3ζ1 + 2ζ3 = 1
ζ =


1
0
2


A third linearly independent solution is
x(3) =


0
1
−1

(t2/2) e2t +


1
0
1

t e2t +


1
0
2

e2t
2.
(a) We compute the eigenvalues of the matrix.
χ(λ) =

5 −λ
−3
−2
8
−5 −λ
−4
−4
3
3 −λ

= −λ3 + 3λ2 −3λ + 1 = −(λ −1)3
λ = 1 is an eigenvalue of multiplicity 3. The rank of the null space of A −I is 2. (The second and third
rows are multiples of the ﬁrst.)
A −I =


4
−3
−2
8
−6
−4
−4
3
2


872

Thus there are two eigenvectors.


4
−3
−2
8
−6
−4
−4
3
2




ξ1
ξ2
ξ3

= 0
ξ(1) =


1
0
2

,
ξ(2) =


0
2
−3


Two linearly independent solutions of the diﬀerential equation are
x(1) =


1
0
2

et,
x(2) =


0
2
−3

et .
(b) We substitute the form x = ξt et +η et into the diﬀerential equation.
x′ = Ax
ξ et +ξt et +η et = Aξt et +Aη et
(A −I)ξ = 0,
(A −I)η = ξ
The general solution of the ﬁrst equation is a linear combination of the two solutions we found in the previous
part.
ξ = c1ξ1 + c2ξ2
Now we ﬁnd the generalized eigenvector, η. Note that η is only determined up to a linear combination of
ξ1 and ξ2. Thus we can take the ﬁrst two components of η to be zero.


4
−3
−2
8
−6
−4
−4
3
2




0
0
η3

= c1


1
0
2

+ c2


0
2
−3


−2η3 = c1,
−4η3 = 2c2,
2η3 = 2c1 −3c2
c1 = c2,
η3 = −c1
2
873

We see that we must take c1 = c2 in order to obtain a solution. We choose c1 = c2 = 2 A third linearly
independent solution of the diﬀerential equation is
x(3) =


2
4
−2

t et +


0
0
−1

et .
Solution 15.11
1. The characteristic polynomial of the matrix is
χ(λ) =

1 −λ
1
1
2
1 −λ
−1
−8
−5
−3 −λ

= (1 −λ)2(−3 −λ) + 8 −10 −5(1 −λ) −2(−3 −λ) −8(1 −λ)
= −λ3 −λ2 + 4λ + 4
= −(λ + 2)(λ + 1)(λ −2)
Thus we see that the eigenvalues are λ = −2, −1, 2. The eigenvectors ξ satisfy
(A −λI)ξ = 0.
For λ = −2, we have
(A + 2I)ξ = 0.


3
1
1
2
3
−1
−8
−5
−1




ξ1
ξ2
ξ3

=


0
0
0


If we take ξ3 = 1 then the ﬁrst two rows give us the system,
3
1
2
3
 ξ1
ξ2

=
−1
1

874

which has the solution ξ1 = −4/7, ξ2 = 5/7. For the ﬁrst eigenvector we choose:
ξ =


−4
5
7


For λ = −1, we have
(A + I)ξ = 0.


2
1
1
2
2
−1
−8
−5
−2




ξ1
ξ2
ξ3

=


0
0
0


If we take ξ3 = 1 then the ﬁrst two rows give us the system,
2
1
2
2
 ξ1
ξ2

=
−1
1

which has the solution ξ1 = −3/2, ξ2 = 2. For the second eigenvector we choose:
ξ =


−3
4
2


For λ = 2, we have
(A + I)ξ = 0.


−1
1
1
2
−1
−1
−8
−5
−5




ξ1
ξ2
ξ3

=


0
0
0


875

If we take ξ3 = 1 then the ﬁrst two rows give us the system,
−1
1
2
−1
 ξ1
ξ2

=
−1
1

which has the solution ξ1 = 0, ξ2 = −1. For the third eigenvector we choose:
ξ =


0
−1
1


In summary, the eigenvalues and eigenvectors are
λ = {−2, −1, 2},
ξ =





−4
5
7

,


−3
4
2

,


0
−1
1





2. The matrix is diagonalized with the similarity transformation
J = S−1AS,
where S is the matrix with eigenvectors as columns:
S =


−4
−3
0
5
4
−1
7
2
1


The matrix exponential, eAt is given by
eA = S eJ S−1.
eA =


−4
−3
0
5
4
−1
7
2
1




e−2t
0
0
0
e−t
0
0
0
e2t

1
12


6
3
3
−12
−4
−4
−18
−13
−1

.
876

eAt =


−2 e−2t +3 e−t
−e−2t + e−t
−e−2t + e−t
5 e−2t −8 e−t +3 et
2
15 e−2t −16 e−t +13 et
12
15 e−2t −16 e−t + et
12
7 e−2t −4 e−t −3 et
2
21 e−2t −8 e−t −13 et
12
21 e−2t −8 e−t −et
12


The solution of the initial value problem is eAt x0.
3. The general solution of the Euler equation is
c1


−4
5
7

t−2 + c2


−3
4
2

t−1 + c3


0
−1
1

t2.
We could also write the solution as
x = tAc ≡eA log t c,
Solution 15.12
1. The characteristic polynomial of the matrix is
χ(λ) =

2 −λ
0
1
0
2 −λ
0
0
1
3 −λ

= (2 −λ)2(3 −λ)
Thus we see that the eigenvalues are λ = 2, 2, 3. Consider
A −2I =


0
0
1
0
0
0
0
1
3

.
Since rank(nullspace(A −2I)) = 1 there is one eigenvector and one generalized eigenvector of rank two for
877

λ = 2. The generalized eigenvector of rank two satisﬁes
(A −2I)2ξ2 = 0


0
1
1
0
0
0
0
1
1

ξ2 = 0
We choose the solution
ξ2 =


0
−1
1

.
The eigenvector for λ = 2 is
ξ1 = (A −2I)ξ2 =


1
0
0

.
The eigenvector for λ = 3 satisﬁes
(A −3I)2ξ = 0


−1
0
1
0
−1
0
0
1
0

ξ = 0
We choose the solution
ξ =


1
0
1

.
The eigenvalues and generalized eigenvectors are
λ = {2, 2, 3},
ξ =





1
0
0

,


0
−1
1

,


1
0
1




.
878

The matrix of eigenvectors and its inverse is
S =


1
0
1
0
−1
0
0
1
1

,
S−1 =


1
−1
−1
0
−1
0
0
1
1

.
The Jordan canonical form of the matrix, which satisﬁes J = S−1AS is
J =


2
1
0
0
2
0
0
0
3


Recall that the function of a Jordan block is:
f








λ
1
0
0
0
λ
1
0
0
0
λ
1
0
0
0
λ







=





f(λ)
f′(λ)
1!
f′′(λ)
2!
f′′′(λ)
3!
0
f(λ)
f′(λ)
1!
f′′(λ)
2!
0
0
f(λ)
f′(λ)
1!
0
0
0
f(λ)




,
and that the function of a matrix in Jordan canonical form is
f








J1
0
0
0
0
J2
0
0
0
0
J3
0
0
0
0
J4







=




f(J1)
0
0
0
0
f(J2)
0
0
0
0
f(J3)
0
0
0
0
f(J4)



.
We want to compute eJt so we consider the function f(λ) = eλt, which has the derivative f ′(λ) = t eλt. Thus
we see that
eJt =


e2t
t e2t
0
0
e2t
0
0
0
e3t


879

The exponential matrix is
eAt = S eJt S−1,
eAt =


e2t
−(1 + t) e2t + e3t
−e2t + e3t
0
e2t
0
0
−e2t + e3t
e3t

.
The general solution of the homogeneous diﬀerential equation is
x = eAt C.
2. The solution of the inhomogeneous diﬀerential equation subject to the initial condition is
x = eAt 0 + eAt
Z t
0
e−Aτ g(τ) dτ
x = eAt
Z t
0
e−Aτ g(τ) dτ
Solution 15.13
1.
dx
dt = 1
t Ax
t
x′
1
x′
2

=
a
b
c
d
 x1
x2

The ﬁrst component of this equation is
tx′
1 = ax1 + bx2.
880

We diﬀerentiate and multiply by t to obtain a second order coupled equation for x1. We use (15.4) to eliminate
the dependence on x2.
t2x′′
1 + tx′
1 = atx′
1 + btx′
2
t2x′′
1 + (1 −a)tx′
1 = b(cx1 + dx2)
t2x′′
1 + (1 −a)tx′
1 −bcx1 = d(tx′
1 −ax1)
t2x′′
1 + (1 −a −d)tx′
1 + (ad −bc)x1 = 0
Thus we see that x1 satisﬁes a second order, Euler equation. By symmetry we see that x2 satisﬁes,
t2x′′
2 + (1 −b −c)tx′
2 + (bc −ad)x2 = 0.
2. We substitute x = atλ into (15.4).
λatλ−1 = 1
t Aatλ
Aa = λa
Thus we see that x = atλ is a solution if λ is an eigenvalue of A with eigenvector a.
3. Suppose that λ = α is an eigenvalue of multiplicity 2. If λ = α has two linearly independent eigenvectors, a and
b then atα and btα are linearly independent solutions. If λ = α has only one linearly independent eigenvector,
a, then atα is a solution. We look for a second solution of the form
x = ξtα log t + ηtα.
Substituting this into the diﬀerential equation yields
αξtα−1 log t + ξtα−1 + αηtα−1 = Aξtα−1 log t + Aηtα−1
We equate coeﬃcients of tα−1 log t and tα−1 to determine ξ and η.
(A −αI)ξ = 0,
(A −αI)η = ξ
881

These equations have solutions because λ = α has generalized eigenvectors of ﬁrst and second order.
Note that the change of independent variable τ = log t, y(τ) = x(t), will transform (15.4) into a constant
coeﬃcient system.
dy
dτ = Ay
Thus all the methods for solving constant coeﬃcient systems carry over directly to solving (15.4). In the case of
eigenvalues with multiplicity greater than one, we will have solutions of the form,
ξtα,
ξtα log t + ηtα,
ξtα (log t)2 + ηtα log t + ζtα,
. . . ,
analogous to the form of the solutions for a constant coeﬃcient system,
ξ eατ,
ξτ eατ +η eατ,
ξτ 2 eατ +ητ eατ +ζ eατ,
. . . .
4. Method 1. Now we consider
dx
dt = 1
t
1
0
1
1

x.
The characteristic polynomial of the matrix is
χ(λ) =

1 −λ
0
1
1 −λ
 = (1 −λ)2.
λ = 1 is an eigenvalue of multiplicity 2. The equation for the associated eigenvectors is
0
0
1
0
 ξ1
ξ2

=
0
0

.
There is only one linearly independent eigenvector, which we choose to be
a =
0
1

.
882

One solution of the diﬀerential equation is
x1 =
0
1

t.
We look for a second solution of the form
x2 = at log t + ηt.
η satisﬁes the equation
(A −I)η =
0
0
1
0

η =
0
1

.
The solution is determined only up to an additive multiple of a. We choose
η =
1
0

.
Thus a second linearly independent solution is
x2 =
0
1

t log t +
1
0

t.
The general solution of the diﬀerential equation is
x = c1
0
1

t + c2
0
1

t log t +
1
0

t

.
Method 2. Note that the matrix is lower triangular.
x′
1
x′
2

= 1
t
1
0
1
1
 x1
x2

(15.5)
We have an uncoupled equation for x1.
x′
1 = 1
t x1
x1 = c1t
883

By substituting the solution for x1 into (15.5), we obtain an uncoupled equation for x2.
x′
2 = 1
t (c1t + x2)
x′
2 −1
t x2 = c1
1
t x2
′
= c1
t
1
t x2 = c1 log t + c2
x2 = c1t log t + c2t
Thus the solution of the system is
x =

c1t
c1t log t + c2t

,
x = c1

t
t log t

+ c2
0
t

,
which is equivalent to the solution we obtained previously.
884

Chapter 16
Theory of Linear Ordinary Diﬀerential
Equations
A little partyin’ is good for the soul.
-Matt Metz
16.1
Exact Equations
Exercise 16.1
Determine a necessary condition for a second order linear diﬀerential equation to be exact.
Determine an equation for the integrating factor for a second order linear diﬀerential equation.
Hint, Solution
Exercise 16.2
Show that
y′′ + xy′ + y = 0
is exact. Find the solution.
885

Hint, Solution
16.2
Nature of Solutions
Result 16.2.1 Consider the nth order ordinary diﬀerential equation of the form
L[y] = dny
dxn + pn−1(x)dn−1y
dxn−1 + · · · + p1(x)dy
dx + p0(x)y = f(x).
(16.1)
If the coeﬃcient functions pn−1(x), . . . , p0(x) and the inhomogeneity f(x) are continuous on
some interval a < x < b then the diﬀerential equation subject to the conditions,
y(x0) = v0,
y′(x0) = v1,
. . .
y(n−1)(x0) = vn−1,
a < x0 < b,
has a unique solution on the interval.
Exercise 16.3
On what intervals do the following problems have unique solutions?
1. xy′′ + 3y = x
2. x(x −1)y′′ + 3xy′ + 4y = 2
3. ex y′′ + x2y′ + y = tan x
Hint, Solution
886

Linearity of the Operator.
The diﬀerential operator L is linear. To verify this,
L[cy] = dn
dxn(cy) + pn−1(x) dn−1
dxn−1(cy) + · · · + p1(x) d
dx(cy) + p0(x)(cy)
= c dn
dxny + cpn−1(x) dn−1
dxn−1y + · · · + cp1(x) d
dxy + cp0(x)y
= cL[y]
L[y1 + y2] = dn
dxn(y1 + y2) + pn−1(x) dn−1
dxn−1(y1 + y2) + · · · + p1(x) d
dx(y1 + y2) + p0(x)(y1 + y2)
= dn
dxn(y1) + pn−1(x) dn−1
dxn−1(y1) + · · · + p1(x) d
dx(y1) + p0(x)(y1)
+ dn
dxn(y2) + pn−1(x) dn−1
dxn−1(y2) + · · · + p1(x) d
dx(y2) + p0(x)(y2)
= L[y1] + L[y2].
Homogeneous Solutions.
The general homogeneous equation has the form
L[y] = dny
dxn + pn−1(x)dn−1y
dxn−1 + · · · + p1(x)dy
dx + p0(x)y = 0.
From the linearity of L, we see that if y1 and y2 are solutions to the homogeneous equation then c1y1 + c2y2 is also a
solution, (L[c1y1 + c2y2] = 0).
On any interval where the coeﬃcient functions are continuous, the nth order linear homogeneous equation has n
linearly independent solutions, y1, y2, . . . , yn. (We will study linear independence in Section 16.4.) The general solution
to the homogeneous problem is then
yh = c1y1 + c2y2 + · · · + cnyn.
Particular Solutions.
Any function, yp, that satisﬁes the inhomogeneous equation, L[yp] = f(x), is called a
particular solution or particular integral of the equation. Note that for linear diﬀerential equations the particular solution
is not unique. If yp is a particular solution then yp+yh is also a particular solution where yh is any homogeneous solution.
887

The general solution to the problem L[y] = f(x) is the sum of a particular solution and a linear combination of the
homogeneous solutions
y = yp + c1y1 + · · · + cnyn.
Example 16.2.1 Consider the diﬀerential equation
y′′ −y′ = 1.
You can verify that two homogeneous solutions are ex and 1. A particular solution is −x. Thus the general solution is
y = −x + c1 ex +c2.
Exercise 16.4
Suppose you are able to ﬁnd three linearly independent particular solutions u1(x), u2(x) and u3(x) of the second order
linear diﬀerential equation L[y] = f(x). What is the general solution?
Hint, Solution
Real-Valued Solutions.
If the coeﬃcient function and the inhomogeneity in Equation 16.1 are real-valued, then
the general solution can be written in terms of real-valued functions. Let y be any, homogeneous solution, (perhaps
complex-valued). By taking the complex conjugate of the equation L[y] = 0 we show that ¯y is a homogeneous solution
as well.
L[y] = 0
L[y] = 0
y(n) + pn−1y(n−1) + · · · + p0y = 0
¯y(n) + pn−1¯y(n−1) + · · · + p0¯y = 0
L [¯y] = 0
For the same reason, if yp is a particular solution, then yp is a particular solution as well.
888

Since the real and imaginary parts of a function y are linear combinations of y and ¯y,
ℜ(y) = y + ¯y
2
,
ℑ(y) = y −¯y
ı2
,
if y is a homogeneous solution then both ℜy and ℑ(y) are homogeneous solutions. Likewise, if yp is a particular solution
then ℜ(yp) is a particular solution.
L [ℜ(yp)] = L
yp + yp
2

= f
2 + f
2 = f
Thus we see that the homogeneous solution, the particular solution and the general solution of a linear diﬀerential
equation with real-valued coeﬃcients and inhomogeneity can be written in terms of real-valued functions.
Result 16.2.2 The diﬀerential equation
L[y] = dny
dxn + pn−1(x)dn−1y
dxn−1 + · · · + p1(x)dy
dx + p0(x)y = f(x)
with continuous coeﬃcients and inhomogeneity has a general solution of the form
y = yp + c1y1 + · · · + cnyn
where yp is a particular solution, L[yp] = f, and the yk are linearly independent homogeneous
solutions, L[yk] = 0. If the coeﬃcient functions and inhomogeneity are real-valued, then the
general solution can be written in terms of real-valued functions.
16.3
Transformation to a First Order System
Any linear diﬀerential equation can be put in the form of a system of ﬁrst order diﬀerential equations. Consider
y(n) + pn−1y(n−1) + · · · + p0y = f(x).
889

We introduce the functions,
y1 = y,
y2 = y′,
, . . . ,
yn = y(n−1).
The diﬀerential equation is equivalent to the system
y′
1 = y2
y′
2 = y3
... = ...
y′
n = f(x) −pn−1yn −· · · −p0y1.
The ﬁrst order system is more useful when numerically solving the diﬀerential equation.
Example 16.3.1 Consider the diﬀerential equation
y′′ + x2y′ + cos x y = sin x.
The corresponding system of ﬁrst order equations is
y′
1 = y2
y′
2 = sin x −x2y2 −cos x y1.
16.4
The Wronskian
16.4.1
Derivative of a Determinant.
Before investigating the Wronskian, we will need a preliminary result from matrix theory. Consider an n × n matrix A
whose elements aij(x) are functions of x. We will denote the determinant by ∆[A(x)]. We then have the following
theorem.
890

Result 16.4.1 Let aij(x), the elements of the matrix A, be diﬀerentiable functions of x.
Then
d
dx∆[A(x)] =
n
X
k=1
∆k[A(x)]
where ∆k[A(x)] is the determinant of the matrix A with the kth row replaced by the derivative
of the kth row.
Example 16.4.1 Consider the the matrix
A(x) =
 x
x2
x2
x4

The determinant is x5 −x4 thus the derivative of the determinant is 5x4 −4x3. To check the theorem,
d
dx∆[A(x)] = d
dx

x
x2
x2
x4

=

1
2x
x2
x4
 +

x
x2
2x
4x3

= x4 −2x3 + 4x4 −2x3
= 5x4 −4x3.
16.4.2
The Wronskian of a Set of Functions.
A set of functions {y1, y2, . . . , yn} is linearly dependent on an interval if there are constants c1, . . . , cn not all zero such
that
c1y1 + c2y2 + · · · + cnyn = 0
(16.2)
identically on the interval. The set is linearly independent if all of the constants must be zero to satisfy c1y1+· · · cnyn = 0
on the interval.
891

Consider a set of functions {y1, y2, . . . , yn} that are linearly dependent on a given interval and n −1 times diﬀer-
entiable. There are a set of constants, not all zero, that satisfy equation 16.2
Diﬀerentiating equation 16.2 n −1 times gives the equations,
c1y′
1 + c2y′
2 + · · · + cny′
n = 0
c1y′′
1 + c2y′′
2 + · · · + cny′′
n = 0
· · ·
c1y(n−1)
1
+ c2y(n−1)
2
+ · · · + cny(n−1)
n
= 0.
We could write the problem to ﬁnd the constants as







y1
y2
. . .
yn
y′
1
y′
2
. . .
y′
n
y′′
1
y′′
2
. . .
y′′
n
...
...
...
. . .
y(n−1)
1
y(n−1)
2
. . .
y(n−1)
n














c1
c2
c3...
cn







= 0
From linear algebra, we know that this equation has a solution for a nonzero constant vector only if the determinant of
the matrix is zero. Here we deﬁne the Wronskian ,W(x), of a set of functions.
W(x) =

y1
y2
. . .
yn
y′
1
y′
2
. . .
y′
n
...
...
...
. . .
y(n−1)
1
y(n−1)
2
. . .
y(n−1)
n

Thus if a set of functions is linearly dependent on an interval, then the Wronskian is identically zero on that interval.
Alternatively, if the Wronskian is identically zero, then the above matrix equation has a solution for a nonzero constant
vector. This implies that the the set of functions is linearly dependent.
Result 16.4.2 The Wronskian of a set of functions vanishes identically over an interval if
and only if the set of functions is linearly dependent on that interval. The Wronskian of a set
of linearly independent functions does not vanish except possibly at isolated points.
892

Example 16.4.2 Consider the set, {x, x2}. The Wronskian is
W(x) =

x
x2
1
2x

= 2x2 −x2
= x2.
Thus the functions are independent.
Example 16.4.3 Consider the set {sin x, cos x, eıx}. The Wronskian is
W(x) =

sin x
cos x
eıx
cos x
−sin x
ı eıx
−sin x
−cos x
−eıx

.
Since the last row is a constant multiple of the ﬁrst row, the determinant is zero. The functions are dependent. We
could also see this with the identity eıx = cos x + ı sin x.
16.4.3
The Wronskian of the Solutions to a Diﬀerential Equation
Consider the nth order linear homogeneous diﬀerential equation
y(n) + pn−1(x)y(n−1) + · · · + p0(x)y = 0.
Let {y1, y2, . . . , yn} be any set of n linearly independent solutions. Let Y (x) be the matrix such that W(x) = ∆[Y (x)].
Now let’s diﬀerentiate W(x).
W ′(x) = d
dx∆[Y (x)]
=
n
X
k=1
∆k[Y (x)]
893

We note that the all but the last term in this sum is zero. To see this, let’s take a look at the ﬁrst term.
∆1[Y (x)] =

y′
1
y′
2
· · ·
y′
n
y′
1
y′
2
· · ·
y′
n
...
...
...
...
y(n−1)
1
y(n−1)
2
· · ·
y(n−1)
n

The ﬁrst two rows in the matrix are identical. Since the rows are dependent, the determinant is zero.
The last term in the sum is
∆n[Y (x)] =

y1
y2
· · ·
yn
...
...
...
...
y(n−2)
1
y(n−2)
2
· · ·
y(n−2)
n
y(n)
1
y(n)
2
· · ·
y(n)
n

.
In the last row of this matrix we make the substitution y(n)
i
= −pn−1(x)y(n−1)
i
−· · · −p0(x)yi. Recalling that we
can add a multiple of a row to another without changing the determinant, we add p0(x) times the ﬁrst row, and p1(x)
times the second row, etc., to the last row. Thus we have the determinant,
W ′(x) =

y1
y2
· · ·
yn
...
...
...
...
y(n−2)
1
y(n−2)
2
· · ·
y(n−2)
n
−pn−1(x)y(n−1)
1
−pn−1(x)y(n−1)
2
· · ·
−pn−1(x)y(n−1)
n

= −pn−1(x)

y1
y2
· · ·
yn
...
...
...
...
y(n−2)
1
y(n−2)
2
· · ·
y(n−2)
n
y(n−1)
1
y(n−1)
2
· · ·
y(n−1)
n

= −pn−1(x)W(x)
Thus the Wronskian satisﬁes the ﬁrst order diﬀerential equation,
W ′(x) = −pn−1(x)W(x).
894

Solving this equation we get a result known as Abel’s formula.
W(x) = c exp

−
Z
pn−1(x) dx

Thus regardless of the particular set of solutions that we choose, we can compute their Wronskian up to a constant
factor.
Result 16.4.3 The Wronskian of any linearly independent set of solutions to the equation
y(n) + pn−1(x)y(n−1) + · · · + p0(x)y = 0
is, (up to a multiplicative constant), given by
W(x) = exp

−
Z
pn−1(x) dx

.
Example 16.4.4 Consider the diﬀerential equation
y′′ −3y′ + 2y = 0.
The Wronskian of the two independent solutions is
W(x) = c exp

−
Z
−3 dx

= c e3x .
For the choice of solutions {ex, e2x}, the Wronskian is
W(x) =

ex
e2x
ex
2 e2x
 = 2 e3x −e3x = e3x .
895

16.5
Well-Posed Problems
Consider the initial value problem for an nth order linear diﬀerential equation.
dny
dxn + pn−1(x)dn−1y
dxn−1 + · · · + p1(x)dy
dx + p0(x)y = f(x)
y(x0) = v1,
y′(x0) = v2,
. . . ,
y(n−1)(x0) = vn
Since the general solution to the diﬀerential equation is a linear combination of the n homogeneous solutions plus the
particular solution
y = yp + c1y1 + c2y2 + · · · + cnyn,
the problem to ﬁnd the constants ci can be written





y1(x0)
y2(x0)
. . .
yn(x0)
y′
1(x0)
y′
2(x0)
. . .
y′
n(x0)
...
...
...
. . .
y(n−1)
1
(x0)
y(n−1)
2
(x0)
. . .
y(n−1)
n
(x0)










c1
c2...
cn




+





yp(x0)
y′
p(x0)
...
y(n−1)
p
(x0)




=





v1
v2...
vn




.
From linear algebra we know that this system of equations has a unique solution only if the determinant of the matrix
is nonzero. Note that the determinant of the matrix is just the Wronskian evaluated at x0. Thus if the Wronskian
vanishes at x0, the initial value problem for the diﬀerential equation either has no solutions or inﬁnitely many solutions.
Such problems are said to be ill-posed. From Abel’s formula for the Wronskian
W(x) = exp

−
Z
pn−1(x) dx

,
we see that the only way the Wronskian can vanish is if the value of the integral goes to ∞.
Example 16.5.1 Consider the initial value problem
y′′ −2
xy′ + 2
x2y = 0,
y(0) = y′(0) = 1.
896

The Wronskian
W(x) = exp

−
Z
−2
x dx

= exp (2 log x) = x2
vanishes at x = 0. Thus this problem is not well-posed.
The general solution of the diﬀerential equation is
y = c1x + c2x2.
We see that the general solution cannot satisfy the initial conditions. If instead we had the initial conditions y(0) = 0,
y′(0) = 1, then there would be an inﬁnite number of solutions.
Example 16.5.2 Consider the initial value problem
y′′ −2
x2y = 0,
y(0) = y′(0) = 1.
The Wronskian
W(x) = exp

−
Z
0 dx

= 1
does not vanish anywhere. However, this problem is not well-posed.
The general solution,
y = c1x−1 + c2x2,
cannot satisfy the initial conditions. Thus we see that a non-vanishing Wronskian does not imply that the problem is
well-posed.
897

Result 16.5.1 Consider the initial value problem
dny
dxn + pn−1(x)dn−1y
dxn−1 + · · · + p1(x)dy
dx + p0(x)y = 0
y(x0) = v1,
y′(x0) = v2,
. . . ,
y(n−1)(x0) = vn.
If the Wronskian
W(x) = exp

−
Z
pn−1(x) dx

vanishes at x = x0 then the problem is ill-posed. The problem may be ill-posed even if the
Wronskian does not vanish.
16.6
The Fundamental Set of Solutions
Consider a set of linearly independent solutions {u1, u2, . . . , un} to an nth order linear homogeneous diﬀerential equation.
This is called the fundamental set of solutions at x0 if they satisfy the relations
u1(x0) = 1
u2(x0) = 0
. . .
un(x0) = 0
u′
1(x0) = 0
u′
2(x0) = 1
. . .
u′
n(x0) = 0
...
...
...
...
u(n−1)
1
(x0) = 0
u(n−1)
2
(x0) = 0
. . .
u(n−1)
n
(x0) = 1
Knowing the fundamental set of solutions is handy because it makes the task of solving an initial value problem
trivial. Say we are given the initial conditions,
y(x0) = v1,
y′(x0) = v2,
. . . ,
y(n−1)(x0) = vn.
If the ui’s are a fundamental set then the solution that satisﬁes these constraints is just
y = v1u1(x) + v2u2(x) + · · · + vnun(x).
898

Of course in general, a set of solutions is not the fundamental set. If the Wronskian of the solutions is nonzero and
ﬁnite we can generate a fundamental set of solutions that are linear combinations of our original set. Consider the case
of a second order equation Let {y1, y2} be two linearly independent solutions. We will generate the fundamental set of
solutions, {u1, u2}.
u1
u2

=
c11
c12
c21
c22
 y1
y2

For {u1, u2} to satisfy the relations that deﬁne a fundamental set, it must satisfy the matrix equation
u1(x0)
u′
1(x0)
u2(x0)
u′
2(x0)

=
c11
c12
c21
c22
 y1(x0)
y′
1(x0)
y2(x0)
y′
2(x0)

=
1
0
0
1

c11
c12
c21
c22

=
y1(x0)
y′
1(x0)
y2(x0)
y′
2(x0)
−1
If the Wronskian is non-zero and ﬁnite, we can solve for the constants, cij, and thus ﬁnd the fundamental set of
solutions. To generalize this result to an equation of order n, simply replace all the 2×2 matrices and vectors of length
2 with n × n matrices and vectors of length n. I presented the case of n = 2 simply to save having to write out all the
ellipses involved in the general case. (It also makes for easier reading.)
Example 16.6.1 Two linearly independent solutions to the diﬀerential equation y′′ +y = 0 are y1 = eıx and y2 = e−ıx.
y1(0)
y′
1(0)
y2(0)
y′
2(0)

=
1
ı
1
−i

To ﬁnd the fundamental set of solutions, {u1, u2}, at x = 0 we solve the equation
c11
c12
c21
c22

=
1
ı
1
−ı
−1
c11
c12
c21
c22

= 1
ı2
ı
ı
1
−1

899

The fundamental set is
u1 = eıx + e−ıx
2
,
u2 = eıx −e−ıx
ı2
.
Using trigonometric identities we can rewrite these as
u1 = cos x,
u2 = sin x.
Result 16.6.1 The fundamental set of solutions at x = x0, {u1, u2, . . . , un}, to an nth order
linear diﬀerential equation, satisfy the relations
u1(x0) = 1
u2(x0) = 0
. . .
un(x0) = 0
u′
1(x0) = 0
u′
2(x0) = 1
. . .
u′
n(x0) = 0
...
...
...
...
u(n−1)
1
(x0) = 0 u(n−1)
2
(x0) = 0 . . . u(n−1)
n
(x0) = 1.
If the Wronskian of the solutions is nonzero and ﬁnite at the point x0 then you can generate
the fundamental set of solutions from any linearly independent set of solutions.
Exercise 16.5
Two solutions of y′′ −y = 0 are ex and e−x. Show that the solutions are independent. Find the fundamental set of
solutions at x = 0.
Hint, Solution
16.7
Adjoint Equations
For the nth order linear diﬀerential operator
L[y] = pn
dny
dxn + pn−1
dn−1y
dxn−1 + · · · + p0y
900

(where the pj are complex-valued functions) we deﬁne the adjoint of L
L∗[y] = (−1)n dn
dxn(pny) + (−1)n−1 dn−1
dxn−1(pn−1y) + · · · + p0y.
Here f denotes the complex conjugate of f.
Example 16.7.1
L[y] = xy′′ + 1
xy′ + y
has the adjoint
L∗[y] = d2
dx2[xy] −d
dx
1
xy

+ y
= xy′′ + 2y′ −1
xy′ + 1
x2y + y
= xy′′ +

2 −1
x

y′ +

1 + 1
x2

y.
Taking the adjoint of L∗yields
L∗∗[y] = d2
dx2[xy] −d
dx

2 −1
x

y

+

1 + 1
x2

y
= xy′′ + 2y′ −

2 −1
x

y′ −
 1
x2

y +

1 + 1
x2

y
= xy′′ + 1
xy′ + y.
Thus by taking the adjoint of L∗, we obtain the original operator.
In general, L∗∗= L.
901

Consider L[y] = pny(n) + · · · + p0y. If each of the pk is k times continuously diﬀerentiable and u and v are n times
continuously diﬀerentiable on some interval, then on that interval
vL[u] −uL∗[v] = d
dxB[u, v]
where B[u, v], the bilinear concomitant, is the bilinear form
B[u, v] =
n
X
m=1
X
j+k=m−1
j≥0,k≥0
(−1)ju(k)(pmv)(j).
This equation is known as Lagrange’s identity. If L is a second order operator then
vL[u] −uL∗[v] = d
dx

up1v + u′p2v −u(p2v)′
= u′′p2v + u′p1v + u

−p2v′′ + (−2p′
2 + p1)v′ + (−p′′
2 + p′
1)v

.
Example 16.7.2 Verify Lagrange’s identity for the second order operator, L[y] = p2y′′ + p1y′ + p0y.
vL[u] −uL∗[v] = v(p2u′′ + p1u′ + p0u) −u
 d2
dx2(p2v) −d
dx(p1v) + p0v

= v(p2u′′ + p1u′ + p0u) −u(p2v′′ + (2p2′ −p1)v′ + (p2′′ −p1′ + p0)v)
= u′′p2v + u′p1v + u

−p2v′′ + (−2p′
2 + p1)v′ + (−p′′
2 + p′
1)v

.
We will not verify Lagrange’s identity for the general case.
Integrating Lagrange’s identity on its interval of validity gives us Green’s formula.
Z b
a

vL[u] −uL∗[v]

dx = B[u, v]

x=b −B[u, v]

x=a
902

Result 16.7.1 The adjoint of the operator
L[y] = pn
dny
dxn + pn−1
dn−1y
dxn−1 + · · · + p0y
is deﬁned
L∗[y] = (−1)n dn
dxn(pny) + (−1)n−1 dn−1
dxn−1(pn−1y) + · · · + p0y.
If each of the pk is k times continuously diﬀerentiable and u and v are n times continuously
diﬀerentiable, then Lagrange’s identity states
vL[y] −uL∗[v] = d
dxB[u, v] = d
dx
n
X
m=1
X
j+k=m−1
j≥0,k≥0
(−1)ju(k)(pmv)(j).
Integrating Lagrange’s identity on it’s domain of validity yields Green’s formula,
Z b
a

vL[u] −uL∗[v]

dx = B[u, v]

x=b −B[u, v]

x=a.
903

16.8
Additional Exercises
Exact Equations
Nature of Solutions
Transformation to a First Order System
The Wronskian
Well-Posed Problems
The Fundamental Set of Solutions
Adjoint Equations
Exercise 16.6
Find the adjoint of the Bessel equation of order ν,
x2y′′ + xy′ + (x2 −ν2)y = 0,
and the Legendre equation of order α,
(1 −x2)y′′ −2xy′ + α(α + 1)y = 0.
Hint, Solution
Exercise 16.7
Find the adjoint of
x2y′′ −xy′ + 3y = 0.
Hint, Solution
904

16.9
Hints
Hint 16.1
Hint 16.2
Hint 16.3
Hint 16.4
The diﬀerence of any two of the ui’s is a homogeneous solution.
Hint 16.5
Exact Equations
Nature of Solutions
Transformation to a First Order System
The Wronskian
Well-Posed Problems
The Fundamental Set of Solutions
Adjoint Equations
Hint 16.6
Hint 16.7
905

16.10
Solutions
Solution 16.1
The second order, linear, homogeneous diﬀerential equation is
P(x)y′′ + Q(x)y′ + R(x)y = 0.
(16.3)
The second order, linear, homogeneous, exact diﬀerential equation is
d
dx

P(x)dy
dx

+ d
dx [f(x)y] = 0.
(16.4)
P(x)y′′ + (P ′(x) + f(x)) y′ + f ′(x)y = 0
Equating the coeﬃcients of Equations 16.3 and 16.4 yields the set of equations,
P ′(x) + f(x) = Q(x),
f ′(x) = R(x).
We diﬀerentiate the ﬁrst equation and substitute in the expression for f ′(x) from the second equation to determine a
necessary condition for exactness.
P ′′(x) −Q′(x) + R(x) = 0
We multiply Equation 16.3 by the integrating factor µ(x) to obtain,
µ(x)P(x)y′′ + µ(x)Q(x)y′ + µ(x)R(x)y = 0.
(16.5)
The corresponding exact equation is of the form,
d
dx

µ(x)P(x)dy
dx

+ d
dx [f(x)y] = 0.
(16.6)
µ(x)P(x)y′′ + (µ′(x)P(x) + µ(x)P ′(x) + f(x)) y′ + f ′(x)y = 0
Equating the coeﬃcients of Equations 16.5 and 16.6 yields the set of equations,
µ′P + µP ′ + f = µQ,
f ′ = µR.
906

We diﬀerentiate the ﬁrst equation and substitute in the expression for f ′ from the second equation to ﬁnd a diﬀerential
equation for µ(x).
µ′′P + µ′P ′ + µ′P ′ + µP ′′ + µR = µ′Q + µQ′
Pµ′′ + (2P ′ −Q)µ′ + (P ′′ −Q′ + R)µ = 0
Solution 16.2
We consider the diﬀerential equation,
y′′ + xy′ + y = 0.
Since
(1)′′ −(x)′ + 1 = 0
we see that this is an exact equation. We rearrange terms to form exact derivatives and then integrate.
(y′)′ + (xy)′ = 0
y′ + xy = c
d
dx
h
ex2/2 y
i
= c ex2/2
y = c e−x2/2
Z
ex2/2 dx + d e−x2/2
Solution 16.3
Consider the initial value problem,
y′′ + p(x)y′ + q(x)y = f(x),
y(x0) = y0,
y′(x0) = y1.
If p(x), q(x) and f(x) are continuous on an interval (a . . . b) with x0 ∈(a . . . b), then the problem has a unique solution
on that interval.
907

1.
xy′′ + 3y = x
y′′ + 3
xy = 1
Unique solutions exist on the intervals (−∞. . . 0) and (0 . . . ∞).
2.
x(x −1)y′′ + 3xy′ + 4y = 2
y′′ +
3
x −1y′ +
4
x(x −1)y =
2
x(x −1)
Unique solutions exist on the intervals (−∞. . . 0), (0 . . . 1) and (1 . . . ∞).
3.
ex y′′ + x2y′ + y = tan x
y′′ + x2 e−x y′ + e−x y = e−x tan x
Unique solutions exist on the intervals

(2n−1)π
2
. . . (2n+1)π
2

for n ∈Z.
Solution 16.4
We know that the general solution is
y = yp + c1y1 + c2y2,
where yp is a particular solution and y1 and y2 are linearly independent homogeneous solutions.
Since yp can be
any particular solution, we choose yp = u1. Now we need to ﬁnd two homogeneous solutions. Since L[ui] = f(x),
L[u1−u2] = L[u2−u3] = 0. Finally, we note that since the ui’s are linearly independent, y1 = u1−u2 and y2 = u2−u3
are linearly independent. Thus the general solution is
y = u1 + c1(u1 −u2) + c2(u2 −u3).
908

Solution 16.5
The Wronskian of the solutions is
W(x) =

ex
e−x
ex
−e−x
 = −2.
Since the Wronskian is nonzero, the solutions are independent.
The fundamental set of solutions, {u1, u2}, is a linear combination of ex and e−x.
u1
u2

=
c11
c12
c21
c22
  ex
e−x

The coeﬃcients are
c11
c12
c21
c22

=
 e0
e0
e−0
−e−0
−1
=
1
1
1
−1
−1
= 1
−2
−1
−1
−1
1

= 1
2
1
1
1
−1

u1 = 1
2(ex + e−x),
u2 = 1
2(ex −e−x).
The fundamental set of solutions at x = 0 is
{cosh x, sinh x}.
Exact Equations
Nature of Solutions
Transformation to a First Order System
The Wronskian
909

Well-Posed Problems
The Fundamental Set of Solutions
Adjoint Equations
Solution 16.6
1. The Bessel equation of order ν is
x2y′′ + xy′ + (x2 −ν2)y = 0.
The adjoint equation is
x2µ′′ + (4x −x)µ′ + (2 −1 + x2 −ν2)µ = 0
x2µ′′ + 3xµ′ + (1 + x2 −ν2)µ = 0.
2. The Legendre equation of order α is
(1 −x2)y′′ −2xy′ + α(α + 1)y = 0
The adjoint equation is
(1 −x2)µ′′ + (−4x + 2x)µ′ + (−2 + 2 + α(α + 1))µ = 0
(1 −x2)µ′′ −2xµ′ + α(α + 1)µ = 0
Solution 16.7
The adjoint of
x2y′′ −xy′ + 3y = 0
is
d2
dx2(x2y) + d
dx(xy) + 3y = 0
(x2y′′ + 4xy′ + 2y) + (xy′ + y) + 3y = 0
x2y′′ + 5xy′ + 6y = 0.
910

Chapter 17
Techniques for Linear Diﬀerential Equations
My new goal in life is to take the meaningless drivel out of human interaction.
-Dave Ozenne
The nth order linear homogeneous diﬀerential equation has the form
y(n) + an−1(x)y(n−1) + · · · + a1(x)y′ + a0(x)y = 0.
In general it is not possible to solve second order and higher linear diﬀerential equations. In this chapter we will examine
equations that have special forms which allow us to either reduce the order of the equation or solve it.
17.1
Constant Coeﬃcient Equations
The nth order constant coeﬃcient diﬀerential equation has the form
y(n) + an−1y(n−1) + · · · + a1y′ + a0y = 0.
We will ﬁnd that solving a constant coeﬃcient diﬀerential equation is no more diﬃcult than ﬁnding the roots of a
polynomial.
911

17.1.1
Second Order Equations
Factoring the Diﬀerential Equation.
Consider the second order constant coeﬃcient diﬀerential equation
y′′ + 2ay′ + by = 0.
(17.1)
Just as we can factor the polynomial,
λ2 + 2aλ + b = (λ −α)(λ −β),
(17.2)
where
α = −a +
√
a2 −b
and
β = −a −
√
a2 −b,
we can factor the diﬀerential equation.
 d2
dx2 + 2a d
dx + b

y =
 d
dx −α
  d
dx −β

y
Once we have factored the diﬀerential equation, we can solve it by solving a series of of two ﬁrst order diﬀerential
equations. We set u =
  d
dx −β

y to obtain a ﬁrst order equation,
 d
dx −α

u = 0,
which has the solution
u = c1 eαx .
To ﬁnd the solution of Equation 17.1, we solve
 d
dx −β

y = u = c1 eαx .
We multiply by the integrating factor and integrate.
d
dx
 e−βx y

= c1 e(α−β)x
y = c1 eβx
Z
e(α−β)x dx + c2 eβx
912

We ﬁrst consider the case when α and β are distinct.
y = c1 eβx
1
α −β e(α−β)x +c2 eβx
We choose new constants to write the solution in a better form.
y = c1 eαx +c2 eβx
Now we consider the case α = β.
y = c1 eαx
Z
1 dx + c2 eαx
y = c1x eαx +c2 eαx .
The solution of Equation 17.1 is
y =
(
c1 eαx +c2 eβx,
α ̸= β,
c1 eαx +c2x eαx,
α = β.
Example 17.1.1 Consider the diﬀerential equation: y′′ + y = 0. We factor the equation.
 d
dx −ı
  d
dx + ı

y = 0
The general solution of the diﬀerential equation is
y = c1 eıx +c2 e−ıx .
Example 17.1.2 Consider the diﬀerential equation: y′′ = 0. We factor the equation.
 d
dx −0
  d
dx −0

y = 0
913

The general solution of the diﬀerential equation is
y = c1 e0x +c2x e0x
y = c1 + c2x.
Substituting the Form of the Solution into the Diﬀerential Equation.
Note that if we substitute y = eλx
into the diﬀerential equation 17.1, we will obtain the quadratic polynomial equation 17.2 for λ.
y′′ + 2ay′ + by = 0
λ2 eλx +2aλ eλx +b eλx = 0
λ2 + 2aλ + b = 0.
This gives us a superﬁcially diﬀerent method for solving constant coeﬃcient equations. We substitute y = eλx into
the diﬀerential equation. Let α and β be the roots of the quadratic in λ. If the roots are distinct, then the linearly
independent solutions are y1 = eαx and y2 = eβx. If the quadratic has a double root at λ = α, then the linearly
independent solutions are y1 = eαx and y2 = x eαx.
Example 17.1.3 Consider the equation
y′′ −3y′ + 2y = 0.
The substitution y = eλx yields
λ2 −3λ + 2 = (λ −1)(λ −2) = 0.
Thus the solutions are ex and e2x.
Example 17.1.4 Consider the equation
y′′ −2y′ + 4y = 0.
The substitution y = eλx yields
λ2 −2λ + 4 = (λ −2)2 = 0.
Thus the solutions are e2x and x e2x.
914

Result 17.1.1 Consider the second order constant coeﬃcient diﬀerential equation
y′′ + 2ay′ + by = 0.
We can factor the diﬀerential equation into the form,
 d
dx −α
  d
dx −β

y = 0,
which has the solution,
y =
(
c1 eαx +c2 eβx,
α ̸= β,
c1 eαx +c2x eαx,
α = β.
We can also determine α and β by substituting y = eλx into the diﬀerential equation and
factoring the polynomial in λ.
Shift Invariance.
Note that if u(x) is a solution of a constant coeﬃcient equation, then u(x + c) is also a solution.
This is useful in applying initial or boundary conditions.
Example 17.1.5 Consider the problem
y′′ −3y′ + 2y = 0,
y(0) = a,
y′(0) = b.
We know that the general solution is
y = c1 ex +c2 e2x .
Applying the initial conditions, we obtain the equations,
c1 + c2 = a,
c1 + 2c2 = b.
915

The solution is
y = (2a −b) ex +(b −a) e2x .
Now suppose we wish to solve the same diﬀerential equation with the boundary conditions y(1) = a and y′(1) = b. All
we have to do is shift the solution to the right.
y = (2a −b) ex−1 +(b −a) e2(x−1) .
17.1.2
Higher Order Equations
The constant coeﬃcient equation of order n has the form
L[y] = y(n) + an−1y(n−1) + · · · + a1y′ + a0y = 0.
(17.3)
The substitution y = eλx will transform this diﬀerential equation into an algebraic equation.
L[eλx] = λn eλx +an−1λn−1 eλx + · · · + a1λ eλx +a0 eλx = 0
 λn + an−1λn−1 + · · · + a1λ + a0
 eλx = 0
λn + an−1λn−1 + · · · + a1λ + a0 = 0
Assume that the roots of this equation, λ1, . . . , λn, are distinct. Then the n linearly independent solutions of Equa-
tion 17.3 are
eλ1x, . . . , eλnx .
If the roots of the algebraic equation are not distinct then we will not obtain all the solutions of the diﬀerential
equation. Suppose that λ1 = α is a double root. We substitute y = eλx into the diﬀerential equation.
L[eλx] = [(λ −α)2(λ −λ3) · · · (λ −λn)] eλx = 0
Setting λ = α will make the left side of the equation zero. Thus y = eαx is a solution. Now we diﬀerentiate both sides
of the equation with respect to λ and interchange the order of diﬀerentiation.
d
dλL[eλx] = L
 d
dλ eλx

= L

x eλx
916

Let p(λ) = (λ −λ3) · · · (λ −λn). We calculate L

x eλx
by applying L and then diﬀerentiating with respect to λ.
L

x eλx
= d
dλL[eλx]
= d
dλ[(λ −α)2(λ −λ3) · · · (λ −λn)] eλx
= d
dλ[(λ −α)2p(λ)] eλx
=

2(λ −α)p(λ) + (λ −α)2p′(λ) + (λ −α)2p(λ)x
 eλx
= (λ −α) [2p(λ) + (λ −α)p′(λ) + (λ −α)p(λ)x] eλx
Since setting λ = α will make this expression zero, L[x eαx] = 0, x eαx is a solution of Equation 17.3. You can verify
that eαx and x eαx are linearly independent. Now we have generated all of the solutions for the diﬀerential equation.
If λ = α is a root of multiplicity m then by repeatedly diﬀerentiating with respect to λ you can show that the
corresponding solutions are
eαx, x eαx, x2 eαx, . . . , xm−1 eαx .
Example 17.1.6 Consider the equation
y′′′ −3y′ + 2y = 0.
The substitution y = eλx yields
λ3 −3λ + 2 = (λ −1)2(λ + 2) = 0.
Thus the general solution is
y = c1 ex +c2x ex +c3 e−2x .
17.1.3
Real-Valued Solutions
If the coeﬃcients of the diﬀerential equation are real, then the solution can be written in terms of real-valued functions
(Result 16.2.2). For a real root λ = α of the polynomial in λ, the corresponding solution, y = eαx, is real-valued.
917

Now recall that the complex roots of a polynomial with real coeﬃcients occur in complex conjugate pairs. Assume
that α ± ıβ are roots of
λn + an−1λn−1 + · · · + a1λ + a0 = 0.
The corresponding solutions of the diﬀerential equation are e(α+ıβ)x and e(α−ıβ)x. Note that the linear combinations
e(α+ıβ)x + e(α−ıβ)x
2
= eαx cos(βx),
e(α+ıβ)x −e(α−ıβ)x
ı2
= eαx sin(βx),
are real-valued solutions of the diﬀerential equation. We could also obtain real-valued solution by taking the real and
imaginary parts of either e(α+ıβ)x or e(α−ıβ)x.
ℜ
 e(α+ıβ)x
= eαx cos(βx),
ℑ
 e(α+ıβ)x
= eαx sin(βx)
Example 17.1.7 Consider the equation
y′′ −2y′ + 2y = 0.
The substitution y = eλx yields
λ2 −2λ + 2 = (λ −1 −ı)(λ −1 + ı) = 0.
The linearly independent solutions are
e(1+ı)x,
and
e(1−ı)x .
We can write the general solution in terms of real functions.
y = c1 ex cos x + c2 ex sin x
Exercise 17.1
Find the general solution of
y′′ + 2ay′ + by = 0
for a, b ∈R. There are three distinct forms of the solution depending on the sign of a2 −b.
Hint, Solution
918

Exercise 17.2
Find the fundamental set of solutions of
y′′ + 2ay′ + by = 0
at the point x = 0, for a, b ∈R. Use the general solutions obtained in Exercise 17.1.
Hint, Solution
Result 17.1.2 . Consider the second order constant coeﬃcient equation
y′′ + 2ay′ + by = 0.
The general solution of this diﬀerential equation is
y =







e−ax 
c1 e
√
a2−b x +c2 e−
√
a2−b x
if a2 > b,
e−ax  c1 cos(
√
b −a2 x) + c2 sin(
√
b −a2 x)

if a2 < b,
e−ax(c1 + c2x)
if a2 = b.
The fundamental set of solutions at x = 0 is







n
e−ax 
cosh(
√
a2 −b x) +
a
√
a2−b sinh(
√
a2 −b x)

, e−ax
1
√
a2−b sinh(
√
a2 −b x)
o
if a2 > b,
n
e−ax 
cos(
√
b −a2 x) +
a
√
b−a2 sin(
√
b −a2 x)

, e−ax
1
√
b−a2 sin(
√
b −a2 x)
o
if a2 < b,
{(1 + ax) e−ax, x e−ax}
if a2 = b.
To obtain the fundamental set of solutions at the point x = ξ, substitute (x −ξ) for x in
the above solutions.
919

Result 17.1.3 Consider the nth order constant coeﬃcient equation
dny
dxn + an−1
dn−1y
dxn−1 + · · · + a1
dy
dx + a0y = 0.
Let the factorization of the algebraic equation obtained with the substitution y = eλx be
(λ −λ1)m1(λ −λ2)m2 · · · (λ −λp)mp = 0.
A set of linearly independent solutions is given by
{eλ1x, x eλ1x, . . . , xm1−1 eλ1x, . . . , eλpx, x eλpx, . . . , xmp−1 eλpx}.
If the coeﬃcients of the diﬀerential equation are real, then we can ﬁnd a real-valued set of
solutions.
Example 17.1.8 Consider the equation
d4y
dx4 + 2d2y
dx2 + y = 0.
The substitution y = eλx yields
λ4 + 2λ2 + 1 = (λ −i)2(λ + i)2 = 0.
Thus the linearly independent solutions are
eıx, x eıx, e−ıx and x e−ıx .
Noting that
eıx = cos(x) + ı sin(x),
we can write the general solution in terms of sines and cosines.
y = c1 cos x + c2 sin x + c3x cos x + c4x sin x
920

17.2
Euler Equations
Consider the equation
L[y] = x2 dy
dx + axdy
dx + by = 0,
x > 0.
Let’s say, for example, that y has units of distance and x has units of time. Note that each term in the diﬀerential
equation has the same dimension.
(time)2(distance)
(time)2
= (time)(distance)
(time)
= (distance)
Thus this is a second order Euler, or equidimensional equation. We know that the ﬁrst order Euler equation, xy′+ay = 0,
has the solution y = cxa. Thus for the second order equation we will try a solution of the form y = xλ. The substitution
y = xλ will transform the diﬀerential equation into an algebraic equation.
L[xλ] = x2 d2
dx2[xλ] + ax d
dx[xλ] + bxλ = 0
λ(λ −1)xλ + aλxλ + bxλ = 0
λ(λ −1) + aλ + b = 0
Factoring yields
(λ −λ1)(λ −λ2) = 0.
If the two roots, λ1 and λ2, are distinct then the general solution is
y = c1xλ1 + c2xλ2.
If the roots are not distinct, λ1 = λ2 = λ, then we only have the one solution, y = xλ. To generate the other solution
we use the same approach as for the constant coeﬃcient equation. We substitute y = xλ into the diﬀerential equation
921

and diﬀerentiate with respect to λ.
d
dλL[xλ] = L[ d
dλxλ]
= L[ln x xλ]
Note that
d
dλxλ = d
dλ eλ ln x = ln x eλ ln x = ln x xλ.
Now we apply L and then diﬀerentiate with respect to λ.
d
dλL[xλ] = d
dλ(λ −α)2xλ
= 2(λ −α)xλ + (λ −α)2 ln x xλ
Equating these two results,
L[ln x xλ] = 2(λ −α)xλ + (λ −α)2 ln x xλ.
Setting λ = α will make the right hand side zero. Thus y = ln x xα is a solution.
If you are in the mood for a little algebra you can show by repeatedly diﬀerentiating with respect to λ that if λ = α
is a root of multiplicity m in an nth order Euler equation then the associated solutions are
xα, ln x xα, (ln x)2xα, . . . , (ln x)m−1xα.
Example 17.2.1 Consider the Euler equation
xy′′ −y′ + y
x = 0.
The substitution y = xλ yields the algebraic equation
λ(λ −1) −λ + 1 = (λ −1)2 = 0.
Thus the general solution is
y = c1x + c2x ln x.
922

17.2.1
Real-Valued Solutions
If the coeﬃcients of the Euler equation are real, then the solution can be written in terms of functions that are real-valued
when x is real and positive, (Result 16.2.2). If α ± ıβ are the roots of
λ(λ −1) + aλ + b = 0
then the corresponding solutions of the Euler equation are
xα+ıβ
and
xα−ıβ.
We can rewrite these as
xα eıβ ln x
and
xα e−ıβ ln x .
Note that the linear combinations
xα eıβ ln x +xα e−ıβ ln x
2
= xα cos(β ln x),
and
xα eıβ ln x −xα e−ıβ ln x
ı2
= xα sin(β ln x),
are real-valued solutions when x is real and positive. Equivalently, we could take the real and imaginary parts of either
xα+ıβ or xα−ıβ.
ℜ
 xα eıβ ln x
= xα cos(β ln x),
ℑ
 xα eıβ ln x
= xα sin(β ln x)
923

Result 17.2.1 Consider the second order Euler equation
x2y′′ + (2a + 1)xy′ + by = 0.
The general solution of this diﬀerential equation is
y =







x−a 
c1x
√
a2−b + c2x−
√
a2−b
if a2 > b,
x−a  c1 cos
 √
b −a2 ln x

+ c2 sin
 √
b −a2 ln x

if a2 < b,
x−a (c1 + c2 ln x)
if a2 = b.
The fundamental set of solutions at x = ξ is
y =

























n 
x
ξ
−a 
cosh
√
a2 −b ln x
ξ

+
a
√
a2−b sinh
√
a2 −b ln x
ξ

,

x
ξ
−a
ξ
√
a2−b sinh
√
a2 −b ln x
ξ
 o
if a2 > b,
n 
x
ξ
−a 
cos
√
b −a2 ln x
ξ

+
a
√
b−a2 sin
√
b −a2 ln x
ξ

,

x
ξ
−a
ξ
√
b−a2 sin
√
b −a2 ln x
ξ
 o
if a2 < b,

x
ξ
−a 
1 + a ln x
ξ

,

x
ξ
−a
ξ ln x
ξ

if a2 = b.
Example 17.2.2 Consider the Euler equation
x2y′′ −3xy′ + 13y = 0.
The substitution y = xλ yields
λ(λ −1) −3λ + 13 = (λ −2 −ı3)(λ −2 + ı3) = 0.
924

The linearly independent solutions are

x2+ı3, x2−ı3	
.
We can put this in a more understandable form.
x2+ı3 = x2 eı3 ln x = x2 cos(3 ln x) + x2 sin(3 ln x)
We can write the general solution in terms of real-valued functions.
y = c1x2 cos(3 ln x) + c2x2 sin(3 ln x)
Result 17.2.2 Consider the nth order Euler equation
xndny
dxn + an−1xn−1dn−1y
dxn−1 + · · · + a1xdy
dx + a0y = 0.
Let the factorization of the algebraic equation obtained with the substitution y = xλ be
(λ −λ1)m1(λ −λ2)m2 · · · (λ −λp)mp = 0.
A set of linearly independent solutions is given by
{xλ1, ln x xλ1, . . . , (ln x)m1−1xλ1, . . . , xλp, ln x xλp, . . . , (ln x)mp−1xλp}.
If the coeﬃcients of the diﬀerential equation are real, then we can ﬁnd a set of solutions that
are real valued when x is real and positive.
925

17.3
Exact Equations
Exact equations have the form
d
dxF(x, y, y′, y′′, . . .) = f(x).
If you can write an equation in the form of an exact equation, you can integrate to reduce the order by one, (or solve
the equation for ﬁrst order). We will consider a few examples to illustrate the method.
Example 17.3.1 Consider the equation
y′′ + x2y′ + 2xy = 0.
We can rewrite this as
d
dx

y′ + x2y

= 0.
Integrating yields a ﬁrst order inhomogeneous equation.
y′ + x2y = c1
We multiply by the integrating factor I(x) = exp(
R
x2 dx) to make this an exact equation.
d
dx

ex3/3 y

= c1 ex3/3
ex3/3 y = c1
Z
ex3/3 dx + c2
y = c1 e−x3/3
Z
ex3/3 dx + c2 e−x3/3
926

Result 17.3.1 If you can write a diﬀerential equation in the form
d
dxF(x, y, y′, y′′, . . .) = f(x),
then you can integrate to reduce the order of the equation.
F(x, y, y′, y′′, . . .) =
Z
f(x) dx + c
17.4
Equations Without Explicit Dependence on y
Example 17.4.1 Consider the equation
y′′ + √xy′ = 0.
This is a second order equation for y, but note that it is a ﬁrst order equation for y′. We can solve directly for y′.
d
dx

exp
2
3x3/2

y′

= 0
y′ = c1 exp

−2
3x3/2

Now we just integrate to get the solution for y.
y = c1
Z
exp

−2
3x3/2

dx + c2
Result 17.4.1 If an nth order equation does not explicitly depend on y then you can consider
it as an equation of order n −1 for y′.
927

17.5
Reduction of Order
Consider the second order linear equation
L[y] ≡y′′ + p(x)y′ + q(x)y = f(x).
Suppose that we know one homogeneous solution y1. We make the substitution y = uy1 and use that L[y1] = 0.
L[uy1] = 0u′′y1 + 2u′y′
1 + uy′′
1 + p(u′y1 + uy′
1) + quy1 = 0
u′′y1 + u′(2y′
1 + py1) + u(y′′
1 + py′
1 + qy1) = 0
u′′y1 + u′(2y′
1 + py1) = 0
Thus we have reduced the problem to a ﬁrst order equation for u′. An analogous result holds for higher order equations.
Result 17.5.1 Consider the nth order linear diﬀerential equation
y(n) + pn−1(x)y(n−1) + · · · + p1(x)y′ + p0(x)y = f(x).
Let y1 be a solution of the homogeneous equation. The substitution y = uy1 will transform
the problem into an (n −1)th order equation for u′. For the second order problem
y′′ + p(x)y′ + q(x)y = f(x)
this reduced equation is
u′′y1 + u′(2y′
1 + py1) = f(x).
Example 17.5.1 Consider the equation
y′′ + xy′ −y = 0.
928

By inspection we see that y1 = x is a solution. We would like to ﬁnd another linearly independent solution. The
substitution y = xu yields
xu′′ + (2 + x2)u′ = 0
u′′ +
2
x + x

u′ = 0
The integrating factor is I(x) = exp(2 ln x + x2/2) = x2 exp(x2/2).
d
dx

x2 ex2/2 u′
= 0
u′ = c1x−2 e−x2/2
u = c1
Z
x−2 e−x2/2 dx + c2
y = c1x
Z
x−2 e−x2/2 dx + c2x
Thus we see that a second solution is
y2 = x
Z
x−2 e−x2/2 dx.
17.6
*Reduction of Order and the Adjoint Equation
Let L be the linear diﬀerential operator
L[y] = pn
dny
dxn + pn−1
dn−1y
dxn−1 + · · · + p0y,
where each pj is a j times continuously diﬀerentiable complex valued function. Recall that the adjoint of L is
L∗[y] = (−1)n dn
dxn(pny) + (−1)n−1 dn−1
dxn−1(pn−1y) + · · · + p0y.
929

If u and v are n times continuously diﬀerentiable, then Lagrange’s identity states
vL[u] −uL∗[v] = d
dxB[u, v],
where
B[u, v] =
n
X
m=1
X
j+k=m−1
j≥0,k≥0
(−1)ju(k)(pmv)(j).
For second order equations,
B[u, v] = up1v + u′p2v −u(p2v)′.
(See Section 16.7.)
If we can ﬁnd a solution to the homogeneous adjoint equation, L∗[y] = 0, then we can reduce the order of the
equation L[y] = f(x). Let ψ satisfy L∗[ψ] = 0. Substituting u = y, v = ψ into Lagrange’s identity yields
ψL[y] −yL∗[ψ] = d
dxB[y, ψ]
ψL[y] = d
dxB[y, ψ].
The equation L[y] = f(x) is equivalent to the equation
d
dxB[y, ψ] = ψf
B[y, ψ] =
Z
ψ(x)f(x) dx,
which is a linear equation in y of order n −1.
Example 17.6.1 Consider the equation
L[y] = y′′ −x2y′ −2xy = 0.
930

Method 1.
Note that this is an exact equation.
d
dx(y′ −x2y) = 0
y′ −x2y = c1
d
dx

e−x3/3 y

= c1 e−x3/3
y = c1 ex3/3
Z
e−x3/3 dx + c2 ex3/3
Method 2.
The adjoint equation is
L∗[y] = y′′ + x2y′ = 0.
By inspection we see that ψ = (constant) is a solution of the adjoint equation. To simplify the algebra we will choose
ψ = 1. Thus the equation L[y] = 0 is equivalent to
B[y, 1] = c1
y(−x2) + d
dx[y](1) −y d
dx[1] = c1
y′ −x2y = c1.
By using the adjoint equation to reduce the order we obtain the same solution as with Method 1.
931

17.7
Exercises
Constant Coeﬃcient Equations
Exercise 17.3 (mathematica/ode/techniques linear/constant.nb)
Find the solution of each one of the following initial value problems. Sketch the graph of the solution and describe its
behavior as t increases.
1. 6y′′ −5y′ + y = 0, y(0) = 4, y′(0) = 0
2. y′′ −2y′ + 5y = 0, y(π/2) = 0, y′(π/2) = 2
3. y′′ + 4y′ + 4y = 0, y(−1) = 2, y′(−1) = 1
Hint, Solution
Exercise 17.4 (mathematica/ode/techniques linear/constant.nb)
Substitute y = eλx to ﬁnd two linearly independent solutions to
y′′ −4y′ + 13y = 0.
that are real-valued when x is real-valued.
Hint, Solution
Exercise 17.5 (mathematica/ode/techniques linear/constant.nb)
Find the general solution to
y′′′ −y′′ + y′ −y = 0.
Write the solution in terms of functions that are real-valued when x is real-valued.
Hint, Solution
Exercise 17.6
Substitute y = eλx to ﬁnd the fundamental set of solutions at x = 0 for the equations:
1. y′′ + y = 0,
932

2. y′′ −y = 0,
3. y′′ = 0.
What are the fundamental sets of solutions at x = 1 for these equations.
Hint, Solution
Exercise 17.7
Consider a ball of mass m hanging by an ideal spring of spring constant k. The ball is suspended in a ﬂuid which
damps the motion. This resistance has a coeﬃcient of friction, µ. Find the diﬀerential equation for the displacement
of the mass from its equilibrium position by balancing forces. Denote this displacement by y(t). If the damping force
is weak, the mass will have a decaying, oscillatory motion. If the damping force is strong, the mass will not oscillate.
The displacement will decay to zero. The value of the damping which separates these two behaviors is called critical
damping.
Find the solution which satisﬁes the initial conditions y(0) = 0, y′(0) = 1. Use the solutions obtained in Exercise 17.2
or refer to Result 17.1.2.
Consider the case m = k = 1. Find the coeﬃcient of friction for which the displacement of the mass decays most
rapidly. Plot the displacement for strong, weak and critical damping.
Hint, Solution
Exercise 17.8
Show that y = c cos(x −φ) is the general solution of y′′ + y = 0 where c and φ are constants of integration. (It is not
suﬃcient to show that y = c cos(x −φ) satisﬁes the diﬀerential equation. y = 0 satisﬁes the diﬀerential equation, but
is is certainly not the general solution.) Find constants c and φ such that y = sin(x).
Is y = c cosh(x −φ) the general solution of y′′ −y = 0? Are there constants c and φ such that y = sinh(x)?
Hint, Solution
Exercise 17.9 (mathematica/ode/techniques linear/constant.nb)
Let y(t) be the solution of the initial-value problem
y′′ + 5y′ + 6y = 0;
y(0) = 1,
y′(0) = V.
For what values of V does y(t) remain nonnegative for all t > 0?
933

Hint, Solution
Exercise 17.10 (mathematica/ode/techniques linear/constant.nb)
Find two linearly independent solutions of
y′′ + sign(x)y = 0,
−∞< x < ∞.
where sign(x) = ±1 according as x is positive or negative. (The solution should be continuous and have a continuous
ﬁrst derivative.)
Hint, Solution
Euler Equations
Exercise 17.11
Find the general solution of
x2y′′ + xy′ + y = 0,
x > 0.
Hint, Solution
Exercise 17.12
Substitute y = xλ to ﬁnd the general solution of
x2y′′ −2xy + 2y = 0.
Hint, Solution
Exercise 17.13 (mathematica/ode/techniques linear/constant.nb)
Substitute y = xλ to ﬁnd the general solution of
xy′′′ + y′′ + 1
xy′ = 0.
Write the solution in terms of functions that are real-valued when x is real-valued and positive.
Hint, Solution
934

Exercise 17.14
Find the general solution of
x2y′′ + (2a + 1)xy′ + by = 0.
Hint, Solution
Exercise 17.15
Show that
y1 = eax,
y2 = lim
α→a
eαx −e−αx
α
are linearly indepedent solutions of
y′′ −a2y = 0
for all values of a. It is common to abuse notation and write the second solution as
y2 = eax −e−ax
a
where the limit is taken if a = 0. Likewise show that
y1 = xa,
y2 = xa −x−a
a
are linearly indepedent solutions of
x2y′′ + xy′ −a2y = 0
for all values of a.
Hint, Solution
Exercise 17.16 (mathematica/ode/techniques linear/constant.nb)
Find two linearly independent solutions (i.e., the general solution) of
(a) x2y′′ −2xy′ + 2y = 0,
(b) x2y′′ −2y = 0,
(c) x2y′′ −xy′ + y = 0.
Hint, Solution
935

Exact Equations
Exercise 17.17
Solve the diﬀerential equation
y′′ + y′ sin x + y cos x = 0.
Hint, Solution
Equations Without Explicit Dependence on y
Reduction of Order
Exercise 17.18
Consider
(1 −x2)y′′ −2xy′ + 2y = 0,
−1 < x < 1.
Verify that y = x is a solution. Find the general solution.
Hint, Solution
Exercise 17.19
Consider the diﬀerential equation
y′′ −x + 1
x
y′ + 1
xy = 0.
Since the coeﬃcients sum to zero, (1 −x+1
x + 1
x = 0), y = ex is a solution. Find another linearly independent solution.
Hint, Solution
Exercise 17.20
One solution of
(1 −2x)y′′ + 4xy′ −4y = 0
is y = x. Find the general solution.
Hint, Solution
936

Exercise 17.21
Find the general solution of
(x −1)y′′ −xy′ + y = 0,
given that one solution is y = ex. (you may assume x > 1)
Hint, Solution
*Reduction of Order and the Adjoint Equation
937

17.8
Hints
Hint 17.1
Substitute y = eλx into the diﬀerential equation.
Hint 17.2
The fundamental set of solutions is a linear combination of the homogeneous solutions.
Constant Coeﬃcient Equations
Hint 17.3
Hint 17.4
Hint 17.5
It is a constant coeﬃcient equation.
Hint 17.6
Use the fact that if u(x) is a solution of a constant coeﬃcient equation, then u(x + c) is also a solution.
Hint 17.7
The force on the mass due to the spring is −ky(t). The frictional force is −µy′(t).
Note that the initial conditions describe the second fundamental solution at t = 0.
Note that for large t, t eαt is much small than eβt if α < β. (Prove this.)
Hint 17.8
By deﬁnition, the general solution of a second order diﬀerential equation is a two parameter family of functions that
satisﬁes the diﬀerential equation. The trigonometric identities in Appendix Q may be useful.
938

Hint 17.9
Hint 17.10
Euler Equations
Hint 17.11
Hint 17.12
Hint 17.13
Hint 17.14
Substitute y = xλ into the diﬀerential equation. Consider the three cases: a2 > b, a2 < b and a2 = b.
Hint 17.15
Hint 17.16
Exact Equations
Hint 17.17
It is an exact equation.
Equations Without Explicit Dependence on y
939

Reduction of Order
Hint 17.18
Hint 17.19
Use reduction of order to ﬁnd the other solution.
Hint 17.20
Use reduction of order to ﬁnd the other solution.
Hint 17.21
*Reduction of Order and the Adjoint Equation
940

17.9
Solutions
Solution 17.1
We substitute y = eλx into the diﬀerential equation.
y′′ + 2ay′ + by = 0
λ2 + 2aλ + b = 0
λ = −a ±
√
a2 −b
If a2 > b then the two roots are distinct and real. The general solution is
y = c1 e(−a+
√
a2−b)x +c2 e(−a−
√
a2−b)x .
If a2 < b then the two roots are distinct and complex-valued. We can write them as
λ = −a ± ı
√
b −a2.
The general solution is
y = c1 e(−a+ı
√
b−a2)x +c2 e(−a−ı
√
b−a2)x .
By taking the sum and diﬀerence of the two linearly independent solutions above, we can write the general solution as
y = c1 e−ax cos
√
b −a2 x

+ c2 e−ax sin
√
b −a2 x

.
If a2 = b then the only root is λ = −a. The general solution in this case is then
y = c1 e−ax +c2x e−ax .
In summary, the general solution is
y =







e−ax 
c1 e
√
a2−b x +c2 e−
√
a2−b x
if a2 > b,
e−ax  c1 cos
 √
b −a2 x

+ c2 sin
 √
b −a2 x

if a2 < b,
e−ax(c1 + c2x)
if a2 = b.
941

Solution 17.2
First we note that the general solution can be written,
y =





e−ax  c1 cosh
 √
a2 −b x

+ c2 sinh
 √
a2 −b x

if a2 > b,
e−ax  c1 cos
 √
b −a2 x

+ c2 sin
 √
b −a2 x

if a2 < b,
e−ax(c1 + c2x)
if a2 = b.
We ﬁrst consider the case a2 > b. The derivative is
y′ = e−ax 
−ac1 +
√
a2 −b c2

cosh
√
a2 −b x

+

−ac2 +
√
a2 −b c1

sinh
√
a2 −b x

.
The conditions, y1(0) = 1 and y′
1(0) = 0, for the ﬁrst solution become,
c1 = 1,
−ac1 +
√
a2 −b c2 = 0,
c1 = 1,
c2 =
a
√
a2 −b.
The conditions, y2(0) = 0 and y′
2(0) = 1, for the second solution become,
c1 = 0,
−ac1 +
√
a2 −b c2 = 1,
c1 = 0,
c2 =
1
√
a2 −b.
The fundamental set of solutions is

e−ax

cosh
√
a2 −b x

+
a
√
a2 −b sinh
√
a2 −b x

, e−ax
1
√
a2 −b sinh
√
a2 −b x

.
Now consider the case a2 < b. The derivative is
y′ = e−ax 
−ac1 +
√
b −a2 c2

cos
√
b −a2 x

+

−ac2 −
√
b −a2 c1

sin
√
b −a2 x

.
942

Clearly, the fundamental set of solutions is

e−ax

cos
√
b −a2 x

+
a
√
b −a2 sin
√
b −a2 x

, e−ax
1
√
b −a2 sin
√
b −a2 x

.
Finally we consider the case a2 = b. The derivative is
y′ = e−ax(−ac1 + c2 + −ac2x).
The conditions, y1(0) = 1 and y′
1(0) = 0, for the ﬁrst solution become,
c1 = 1,
−ac1 + c2 = 0,
c1 = 1,
c2 = a.
The conditions, y2(0) = 0 and y′
2(0) = 1, for the second solution become,
c1 = 0,
−ac1 + c2 = 1,
c1 = 0,
c2 = 1.
The fundamental set of solutions is

(1 + ax) e−ax, x e−ax	
.
In summary, the fundamental set of solutions at x = 0 is







n
e−ax 
cosh
 √
a2 −b x

+
a
√
a2−b sinh
 √
a2 −b x

, e−ax
1
√
a2−b sinh
 √
a2 −b x
o
if a2 > b,
n
e−ax 
cos
 √
b −a2 x

+
a
√
b−a2 sin
 √
b −a2 x

, e−ax
1
√
b−a2 sin
 √
b −a2 x
o
if a2 < b,
{(1 + ax) e−ax, x e−ax}
if a2 = b.
Constant Coeﬃcient Equations
943

Solution 17.3
1. We consider the problem
6y′′ −5y′ + y = 0,
y(0) = 4,
y′(0) = 0.
We make the substitution y = eλx in the diﬀerential equation.
6λ2 −5λ + 1 = 0
(2λ −1)(3λ −1) = 0
λ =
1
3, 1
2

The general solution of the diﬀerential equation is
y = c1 et/3 +c2 et/2 .
We apply the initial conditions to determine the constants.
c1 + c2 = 4,
c1
3 + c2
2 = 0
c1 = 12,
c2 = −8
The solution subject to the initial conditions is
y = 12 et/3 −8 et/2 .
The solution is plotted in Figure 17.1. The solution tends to −∞as t →∞.
2. We consider the problem
y′′ −2y′ + 5y = 0,
y(π/2) = 0,
y′(π/2) = 2.
We make the substitution y = eλx in the diﬀerential equation.
λ2 −2λ + 5 = 0
λ = 1 ±
√
1 −5
λ = {1 + ı2, 1 −ı2}
944

1
2
3
4
5
-30
-25
-20
-15
-10
-5
Figure 17.1: The solution of 6y′′ −5y′ + y = 0, y(0) = 4, y′(0) = 0.
The general solution of the diﬀerential equation is
y = c1 et cos(2t) + c2 et sin(2t).
We apply the initial conditions to determine the constants.
y(π/2) = 0
⇒
−c1 eπ/2 = 0
⇒
c1 = 0
y′(π/2) = 2
⇒
−2c2 eπ/2 = 2
⇒
c2 = −e−π/2
The solution subject to the initial conditions is
y = −et−π/2 sin(2t).
The solution is plotted in Figure 17.2. The solution oscillates with an amplitude that tends to ∞as t →∞.
3. We consider the problem
y′′ + 4y′ + 4y = 0,
y(−1) = 2,
y′(−1) = 1.
945

3
4
5
6
-10
10
20
30
40
50
Figure 17.2: The solution of y′′ −2y′ + 5y = 0, y(π/2) = 0, y′(π/2) = 2.
We make the substitution y = eλx in the diﬀerential equation.
λ2 + 4λ + 4 = 0
(λ + 2)2 = 0
λ = −2
The general solution of the diﬀerential equation is
y = c1 e−2t +c2t e−2t .
We apply the initial conditions to determine the constants.
c1 e2 −c2 e2 = 2,
−2c1 e2 +3c2 e2 = 1
c1 = 7 e−2,
c2 = 5 e−2
The solution subject to the initial conditions is
y = (7 + 5t) e−2(t+1)
946

The solution is plotted in Figure 17.3. The solution vanishes as t →∞.
lim
t→∞(7 + 5t) e−2(t+1) = lim
t→∞
7 + 5t
e2(t+1) = lim
t→∞
5
2 e2(t+1) = 0
-1
1
2
3
4
5
0.5
1
1.5
2
Figure 17.3: The solution of y′′ + 4y′ + 4y = 0, y(−1) = 2, y′(−1) = 1.
Solution 17.4
y′′ −4y′ + 13y = 0.
With the substitution y = eλx we obtain
λ2 eλx −4λ eλx +13 eλx = 0
λ2 −4λ + 13 = 0
λ = 2 ± 3i.
947

Thus two linearly independent solutions are
e(2+3i)x,
and
e(2−3i)x .
Noting that
e(2+3i)x = e2x[cos(3x) + ı sin(3x)]
e(2−3i)x = e2x[cos(3x) −ı sin(3x)],
we can write the two linearly independent solutions
y1 = e2x cos(3x),
y2 = e2x sin(3x).
Solution 17.5
We note that
y′′′ −y′′ + y′ −y = 0
is a constant coeﬃcient equation. The substitution, y = eλx, yields
λ3 −λ2 + λ −1 = 0
(λ −1)(λ −i)(λ + i) = 0.
The corresponding solutions are ex, eıx, and e−ıx. We can write the general solution as
y = c1 ex +c2 cos x + c3 sin x.
Solution 17.6
We start with the equation y′′ + y = 0. We substitute y = eλx into the diﬀerential equation to obtain
λ2 + 1 = 0,
λ = ±i.
A linearly independent set of solutions is
{eıx, e−ıx}.
948

The fundamental set of solutions has the form
y1 = c1 eıx +c2 e−ıx,
y2 = c3 eıx +c4 e−ıx .
By applying the constraints
y1(0) = 1,
y′
1(0) = 0,
y2(0) = 0,
y′
2(0) = 1,
we obtain
y1 = eıx + e−ıx
2
= cos x,
y2 = eıx + e−ıx
ı2
= sin x.
Now consider the equation y′′ −y = 0. By substituting y = eλx we ﬁnd that a set of solutions is
{ex, e−x}.
By taking linear combinations of these we see that another set of solutions is
{cosh x, sinh x}.
Note that this is the fundamental set of solutions.
Next consider y′′ = 0. We can ﬁnd the solutions by substituting y = eλx or by integrating the equation twice. The
fundamental set of solutions as x = 0 is
{1, x}.
Note that if u(x) is a solution of a constant coeﬃcient diﬀerential equation, then u(x + c) is also a solution. Also
note that if u(x) satisﬁes y(0) = a, y′(0) = b, then u(x −x0) satisﬁes y(x0) = a, y′(x0) = b. Thus the fundamental
sets of solutions at x = 1 are
949

1. {cos(x −1), sin(x −1)},
2. {cosh(x −1), sinh(x −1)},
3. {1, x −1}.
Solution 17.7
Let y(t) denote the displacement of the mass from equilibrium. The forces on the mass are −ky(t) due to the spring
and −µy′(t) due to friction. We equate the external forces to my′′(t) to ﬁnd the diﬀerential equation of the motion.
my′′ = −ky −µy′
y′′ + µ
my′ + k
my = 0
The solution which satisﬁes the initial conditions y(0) = 0, y′(0) = 1 is
y(t) =









e−µt/(2m)
2m
√
µ2−4km sinh
p
µ2 −4km t/(2m)

if µ2 > km,
e−µt/(2m)
2m
√
4km−µ2 sin
p
4km −µ2 t/(2m)

if µ2 < km,
t e−µt/(2m)
if µ2 = km.
We respectively call these cases: strongly damped, weakly damped and critically damped. In the case that m = k = 1
the solution is
y(t) =









e−µt/2
2
√
µ2−4 sinh
p
µ2 −4 t/2

if µ > 2,
e−µt/2
2
√
4−µ2 sin
p
4 −µ2 t/2

if µ < 2,
t e−t
if µ = 2.
Note that when t is large, t e−t is much smaller than e−µt/2 for µ < 2. To prove this we examine the ratio of these
950

functions as t →∞.
lim
t→∞
t e−t
e−µt/2 = lim
t→∞
t
e(1−µ/2)t
= lim
t→∞
1
(1 −µ/2) e(1−µ)t
= 0
Using this result, we see that the critically damped solution decays faster than the weakly damped solution.
We can write the strongly damped solution as
e−µt/2
2
p
µ2 −4

e
√
µ2−4 t/2 −e−√
µ2−4 t/2
.
For large t, the dominant factor is e
√
µ2−4−µ

t/2. Note that for µ > 2,
p
µ2 −4 =
p
(µ + 2)(µ −2) > µ −2.
Therefore we have the bounds
−2 <
p
µ2 −4 −µ < 0.
This shows that the critically damped solution decays faster than the strongly damped solution. µ = 2 gives the fastest
decaying solution. Figure 17.4 shows the solution for µ = 4, µ = 1 and µ = 2.
Solution 17.8
Clearly y = c cos(x −φ) satisﬁes the diﬀerential equation y′′ + y = 0. Since it is a two-parameter family of functions,
it must be the general solution.
Using a trigonometric identity we can rewrite the solution as
y = c cos φ cos x + c sin φ sin x.
Setting this equal to sin x gives us the two equations
c cos φ = 0,
c sin φ = 1,
951

2
4
6
8
10
-0.1
0.1
0.2
0.3
0.4
0.5
Critical Dampi
Weak Damping
Strong Damping
Figure 17.4: Strongly, weakly and critically damped solutions.
which has the solutions c = 1, φ = (2n + 1/2)π, and c = −1, φ = (2n −1/2)π, for n ∈Z.
Clearly y = c cosh(x −φ) satisﬁes the diﬀerential equation y′′ −y = 0. Since it is a two-parameter family of
functions, it must be the general solution.
Using a trigonometric identity we can rewrite the solution as
y = c cosh φ cosh x + c sinh φ sinh x.
Setting this equal to sinh x gives us the two equations
c cosh φ = 0,
c sinh φ = 1,
which has the solutions c = −i, φ = ı(2n + 1/2)π, and c = i, φ = ı(2n −1/2)π, for n ∈Z.
952

Solution 17.9
We substitute y = eλt into the diﬀerential equation.
λ2 eλt +5λ eλt +6 eλt = 0
λ2 + 5λ + 6 = 0
(λ + 2)(λ + 3) = 0
The general solution of the diﬀerential equation is
y = c1 e−2t +c2 e−3t .
The initial conditions give us the constraints:
c1 + c2 = 1,
−2c1 −3c2 = V.
The solution subject to the initial conditions is
y = (3 + V ) e−2t −(2 + V ) e−3t .
This solution will be non-negative for t > 0 if V ≥−3.
Solution 17.10
For negative x, the diﬀerential equation is
y′′ −y = 0.
We substitute y = eλx into the diﬀerential equation to ﬁnd the solutions.
λ2 −1 = 0
λ = ±1
y =
ex, e−x	
953

We can take linear combinations to write the solutions in terms of the hyperbolic sine and cosine.
y = {cosh(x), sinh(x)}
For positive x, the diﬀerential equation is
y′′ + y = 0.
We substitute y = eλx into the diﬀerential equation to ﬁnd the solutions.
λ2 + 1 = 0
λ = ±ı
y =
eıx, e−ıx	
We can take linear combinations to write the solutions in terms of the sine and cosine.
y = {cos(x), sin(x)}
We will ﬁnd the fundamental set of solutions at x = 0. That is, we will ﬁnd a set of solutions, {y1, y2} that satisfy
the conditions:
y1(0) = 1
y′
1(0) = 0
y2(0) = 0
y′
2(0) = 1
Clearly these solutions are
y1 =
(
cosh(x)
x < 0
cos(x)
x ≥0
y2 =
(
sinh(x)
x < 0
sin(x)
x ≥0
Euler Equations
954

Solution 17.11
We consider an Euler equation,
x2y′′ + xy′ + y = 0,
x > 0.
We make the change of independent variable ξ = ln x, u(ξ) = y(x) to obtain
u′′ + u = 0.
We make the substitution u(ξ) = eλξ.
λ2 + 1 = 0
λ = ±i
A set of linearly independent solutions for u(ξ) is
{eıξ, e−ıξ}.
Since
cos ξ = eıξ + e−ıξ
2
and
sin ξ = eıξ −e−ıξ
ı2
,
another linearly independent set of solutions is
{cos ξ, sin ξ}.
The general solution for y(x) is
y(x) = c1 cos(ln x) + c2 sin(ln x).
Solution 17.12
Consider the diﬀerential equation
x2y′′ −2xy + 2y = 0.
With the substitution y = xλ this equation becomes
λ(λ −1) −2λ + 2 = 0
λ2 −3λ + 2 = 0
λ = 1, 2.
955

The general solution is then
y = c1x + c2x2.
Solution 17.13
We note that
xy′′′ + y′′ + 1
xy′ = 0
is an Euler equation. The substitution y = xλ yields
λ3 −3λ2 + 2λ + λ2 −λ + λ = 0
λ3 −2λ2 + 2λ = 0.
The three roots of this algebraic equation are
λ = 0,
λ = 1 + i,
λ = 1 −ı
The corresponding solutions to the diﬀerential equation are
y = x0
y = x1+ı
y = x1−ı
y = 1
y = x eı ln x
y = x e−ı ln x .
We can write the general solution as
y = c1 + c2x cos(ln x) + c3 sin(ln x).
Solution 17.14
We substitute y = xλ into the diﬀerential equation.
x2y′′ + (2a + 1)xy′ + by = 0
λ(λ −1) + (2a + 1)λ + b = 0
λ2 + 2aλ + b = 0
λ = −a ±
√
a2 −b
956

For a2 > b then the general solution is
y = c1x−a+
√
a2−b + c2x−a−
√
a2−b.
For a2 < b, then the general solution is
y = c1x−a+ı
√
b−a2 + c2x−a−ı
√
b−a2.
By taking the sum and diﬀerence of these solutions, we can write the general solution as
y = c1x−a cos
√
b −a2 ln x

+ c2x−a sin
√
b −a2 ln x

.
For a2 = b, the quadratic in lambda has a double root at λ = a. The general solution of the diﬀerential equation is
y = c1x−a + c2x−a ln x.
In summary, the general solution is:
y =







x−a 
c1x
√
a2−b + c2x−
√
a2−b
if a2 > b,
x−a  c1 cos
 √
b −a2 ln x

+ c2 sin
 √
b −a2 ln x

if a2 < b,
x−a (c1 + c2 ln x)
if a2 = b.
Solution 17.15
For a ̸= 0, two linearly independent solutions of
y′′ −a2y = 0
are
y1 = eax,
y2 = e−ax .
For a = 0, we have
y1 = e0x = 1,
y2 = x e0x = x.
957

In this case the solution are deﬁned by
y1 = [eax]a=0 ,
y2 =
 d
da eax

a=0
.
By the deﬁnition of diﬀerentiation, f ′(0) is
f ′(0) = lim
a→0
f(a) −f(−a)
2a
.
Thus the second solution in the case a = 0 is
y2 = lim
a→0
eax −e−ax
a
Consider the solutions
y1 = eax,
y2 = lim
α→a
eαx −e−αx
α
.
Clearly y1 is a solution for all a. For a ̸= 0, y2 is a linear combination of eax and e−ax and is thus a solution. Since the
coeﬃcient of e−ax in this linear combination is non-zero, it is linearly independent to y1. For a = 0, y2 is one half the
derivative of eax evaluated at a = 0. Thus it is a solution.
For a ̸= 0, two linearly independent solutions of
x2y′′ + xy′ −a2y = 0
are
y1 = xa,
y2 = x−a.
For a = 0, we have
y1 = [xa]a=0 = 1,
y2 =
 d
daxa

a=0
= ln x.
Consider the solutions
y1 = xa,
y2 = xa −x−a
a
Clearly y1 is a solution for all a. For a ̸= 0, y2 is a linear combination of xa and x−a and is thus a solution. For a = 0,
y2 is one half the derivative of xa evaluated at a = 0. Thus it is a solution.
958

Solution 17.16
1.
x2y′′ −2xy′ + 2y = 0
We substitute y = xλ into the diﬀerential equation.
λ(λ −1) −2λ + 2 = 0
λ2 −3λ + 2 = 0
(λ −1)(λ −2) = 0
y = c1x + c2x2
2.
x2y′′ −2y = 0
We substitute y = xλ into the diﬀerential equation.
λ(λ −1) −2 = 0
λ2 −λ −2 = 0
(λ + 1)(λ −2) = 0
y = c1
x + c2x2
3.
x2y′′ −xy′ + y = 0
We substitute y = xλ into the diﬀerential equation.
λ(λ −1) −λ + 1 = 0
λ2 −2λ + 1 = 0
(λ −1)2 = 0
959

Since there is a double root, the solution is:
y = c1x + c2x ln x.
Exact Equations
Solution 17.17
We note that
y′′ + y′ sin x + y cos x = 0
is an exact equation.
d
dx[y′ + y sin x] = 0
y′ + y sin x = c1
d
dx

y e−cos x
= c1 e−cos x
y = c1 ecos x
Z
e−cos x dx + c2 ecos x
Equations Without Explicit Dependence on y
Reduction of Order
Solution 17.18
(1 −x2)y′′ −2xy′ + 2y = 0,
−1 < x < 1
We substitute y = x into the diﬀerential equation to check that it is a solution.
(1 −x2)(0) −2x(1) + 2x = 0
960

We look for a second solution of the form y = xu. We substitute this into the diﬀerential equation and use the fact
that x is a solution.
(1 −x2)(xu′′ + 2u′) −2x(xu′ + u) + 2xu = 0
(1 −x2)(xu′′ + 2u′) −2x(xu′) = 0
(1 −x2)xu′′ + (2 −4x2)u′ = 0
u′′
u′ = 2 −4x2
x(x2 −1)
u′′
u′ = −2
x +
1
1 −x −
1
1 + x
ln(u′) = −2 ln(x) −ln(1 −x) −ln(1 + x) + const
ln(u′) = ln

c
x2(1 −x)(1 + x)

u′ =
c
x2(1 −x)(1 + x)
u′ = c
 1
x2 +
1
2(1 −x) +
1
2(1 + x)

u = c

−1
x −1
2 ln(1 −x) + 1
2 ln(1 + x)

+ const
u = c

−1
x + 1
2 ln
1 + x
1 −x

+ const
A second linearly independent solution is
y = −1 + x
2 ln
1 + x
1 −x

.
961

Solution 17.19
We are given that y = ex is a solution of
y′′ −x + 1
x
y′ + 1
xy = 0.
To ﬁnd another linearly independent solution, we will use reduction of order. Substituting
y = u ex
y′ = (u′ + u) ex
y′′ = (u′′ + 2u′ + u) ex
into the diﬀerential equation yields
u′′ + 2u′ + u −x + 1
x
(u′ + u) + 1
xu = 0.
u′′ + x −1
x
u′ = 0
d
dx

u′ exp
Z 
1 −1
x

dx

= 0
u′ ex−ln x = c1
u′ = c1x e−x
u = c1
Z
x e−x dx + c2
u = c1(x e−x + e−x) + c2
y = c1(x + 1) + c2 ex
Thus a second linearly independent solution is
y = x + 1.
962

Solution 17.20
We are given that y = x is a solution of
(1 −2x)y′′ + 4xy′ −4y = 0.
To ﬁnd another linearly independent solution, we will use reduction of order. Substituting
y = xu
y′ = xu′ + u
y′′ = xu′′ + 2u′
into the diﬀerential equation yields
(1 −2x)(xu′′ + 2u′) + 4x(xu′ + u) −4xu = 0,
(1 −2x)xu′′ + (4x2 −4x + 2)u′ = 0,
u′′
u′ = 4x2 −4x + 2
x(2x −1) ,
u′′
u′ = 2 −2
x +
2
2x −1,
ln(u′) = 2x −2 ln x + ln(2x −1) + const,
u′ = c1
2
x −1
x2

e2x,
u = c1
1
x e2x +c2,
y = c1 e2x +c2x.
Solution 17.21
One solution of
(x −1)y′′ −xy′ + y = 0,
963

is y1 = ex. We ﬁnd a second solution with reduction of order. We make the substitution y2 = u ex in the diﬀerential
equation. We determine u up to an additive constant.
(x −1)(u′′ + 2u′ + u) ex −x(u′ + u) ex +u ex = 0
(x −1)u′′ + (x −2)u′ = 0
u′′
u′ = −x −2
x −1 = −1 +
1
x −1
ln |u′| = −x + ln |x −1| + c
u′ = c(x −1) e−x
u = −cx e−x
The second solution of the diﬀerential equation is y2 = x.
*Reduction of Order and the Adjoint Equation
964

Chapter 18
Techniques for Nonlinear Diﬀerential
Equations
In mathematics you don’t understand things. You just get used to them.
- Johann von Neumann
18.1
Bernoulli Equations
Sometimes it is possible to solve a nonlinear equation by making a change of the dependent variable that converts it
into a linear equation. One of the most important such equations is the Bernoulli equation
dy
dt + p(t)y = q(t)yα,
α ̸= 1.
The change of dependent variable u = y1−α will yield a ﬁrst order linear equation for u which when solved will give us
an implicit solution for y. (See Exercise ??.)
965

Result 18.1.1 The Bernoulli equation y′ + p(t)y = q(t)yα, α ̸= 1 can be transformed to
the ﬁrst order linear equation
du
dt + (1 −α)p(t)u = (1 −α)q(t)
with the change of variables u = y1−α.
Example 18.1.1 Consider the Bernoulli equation
y′ = 2
xy + y2.
First we divide by y2.
y−2y′ = 2
xy−1 + 1
We make the change of variable u = y−1.
−u′ = 2
xu + 1
u′ + 2
xu = −1
966

The integrating factor is I(x) = exp(
R 2
x dx) = x2.
d
dx(x2u) = −x2
x2u = −1
3x3 + c
u = −1
3x + c
x2
y =

−1
3x + c
x2
−1
Thus the solution for y is
y =
3x2
c −x2.
18.2
Riccati Equations
Factoring Second Order Operators.
Consider the second order linear equation
L[y] =
 d2
dx2 + p(x) d
dx + q(x)

y = y′′ + p(x)y′ + q(x)y = f(x).
If we were able to factor the linear operator L into the form
L =
 d
dx + a(x)
  d
dx + b(x)

,
(18.1)
then we would be able to solve the diﬀerential equation. Factoring reduces the problem to a system of ﬁrst order
equations. We start with the factored equation
 d
dx + a(x)
  d
dx + b(x)

y = f(x).
967

We set u =
 d
dx + b(x)

y and solve the problem
 d
dx + a(x)

u = f(x).
Then to obtain the solution we solve
 d
dx + b(x)

y = u.
Example 18.2.1 Consider the equation
y′′ +

x −1
x

y′ +
 1
x2 −1

y = 0.
Let’s say by some insight or just random luck we are able to see that this equation can be factored into
 d
dx + x
  d
dx −1
x

y = 0.
We ﬁrst solve the equation
 d
dx + x

u = 0.
u′ + xu = 0
d
dx

ex2/2 u

= 0
u = c1 e−x2/2
968

Then we solve for y with the equation
 d
dx −1
x

y = u = c1 e−x2/2 .
y′ −1
xy = c1 e−x2/2
d
dx
 x−1y

= c1x−1 e−x2/2
y = c1x
Z
x−1 e−x2/2 dx + c2x
If we were able to solve for a and b in Equation 18.1 in terms of p and q then we would be able to solve any second
order diﬀerential equation. Equating the two operators,
d2
dx2 + p d
dx + q =
 d
dx + a
  d
dx + b

= d2
dx2 + (a + b) d
dx + (b′ + ab).
Thus we have the two equations
a + b = p,
and
b′ + ab = q.
Eliminating a,
b′ + (p −b)b = q
b′ = b2 −pb + q
Now we have a nonlinear equation for b that is no easier to solve than the original second order linear equation.
Riccati Equations.
Equations of the form
y′ = a(x)y2 + b(x)y + c(x)
969

are called Riccati equations. From the above derivation we see that for every second order diﬀerential equation there
is a corresponding Riccati equation. Now we will show that the converse is true.
We make the substitution
y = −u′
au,
y′ = −u′′
au + (u′)2
au2 + a′u′
a2u,
in the Riccati equation.
y′ = ay2 + by + c
−u′′
au + (u′)2
au2 + a′u′
a2u = a(u′)2
a2u2 −b u′
au + c
−u′′
au + a′u′
a2u + b u′
au −c = 0
u′′ −
a′
a + b

u′ + acu = 0
Now we have a second order linear equation for u.
Result 18.2.1 The substitution y = −u′
au transforms the Riccati equation
y′ = a(x)y2 + b(x)y + c(x)
into the second order linear equation
u′′ −
a′
a + b

u′ + acu = 0.
Example 18.2.2 Consider the Riccati equation
y′ = y2 + 1
xy + 1
x2.
970

With the substitution y = −u′
u we obtain
u′′ −1
xu′ + 1
x2u = 0.
This is an Euler equation. The substitution u = xλ yields
λ(λ −1) −λ + 1 = (λ −1)2 = 0.
Thus the general solution for u is
u = c1x + c2x log x.
Since y = −u′
u ,
y = −c1 + c2(1 + log x)
c1x + c2x log x
y = −1 + c(1 + log x)
x + cx log x
18.3
Exchanging the Dependent and Independent Variables
Some diﬀerential equations can be put in a more elementary form by exchanging the dependent and independent
variables. If the new equation can be solved, you will have an implicit solution for the initial equation. We will consider
a few examples to illustrate the method.
Example 18.3.1 Consider the equation
y′ =
1
y3 −xy2.
971

Instead of considering y to be a function of x, consider x to be a function of y. That is, x = x(y), x′ = dx
dy.
dy
dx =
1
y3 −xy2
dx
dy = y3 −xy2
x′ + y2x = y3
Now we have a ﬁrst order equation for x.
d
dy

ey3/3 x

= y3 ey3/3
x = e−y3/3
Z
y3 ey3/3 dy + c e−y3/3
Example 18.3.2 Consider the equation
y′ =
y
y2 + 2x.
Interchanging the dependent and independent variables yields
1
x′ =
y
y2 + 2x
x′ = y + 2x
y
x′ −2x
y = y
d
dy(y−2x) = y−1
y−2x = log y + c
x = y2 log y + cy2
972

Result 18.3.1 Some diﬀerential equations can be put in a simpler form by exchanging the
dependent and independent variables. Thus a diﬀerential equation for y(x) can be written as
an equation for x(y). Solving the equation for x(y) will give an implicit solution for y(x).
18.4
Autonomous Equations
Autonomous equations have no explicit dependence on x. The following are examples.
• y′′ + 3y′ −2y = 0
• y′′ = y + (y′)2
• y′′′ + y′′y = 0
The change of variables u(y) = y′ reduces an nth order autonomous equation in y to a non-autonomous equation
of order n −1 in u(y). Writing the derivatives of y in terms of u,
y′ = u(y)
y′′ = d
dxu(y)
= dy
dx
d
dyu(y)
= y′u′
= u′u
y′′′ = (u′′u + (u′)2)u.
Thus we see that the equation for u(y) will have an order of one less than the original equation.
Result 18.4.1 Consider an autonomous diﬀerential equation for y(x), (autonomous equa-
tions have no explicit dependence on x.) The change of variables u(y) = y′ reduces an nth
order autonomous equation in y to a non-autonomous equation of order n −1 in u(y).
973

Example 18.4.1 Consider the equation
y′′ = y + (y′)2.
With the substitution u(y) = y′, the equation becomes
u′u = y + u2
u′ = u + yu−1.
We recognize this as a Bernoulli equation. The substitution v = u2 yields
1
2v′ = v + y
v′ −2v = 2y
d
dy
 e−2y v

= 2y e−2y
v(y) = c1 e2y + e2y
Z
2y e−2y dy
v(y) = c1 e2y + e2y

−y e−2y +
Z
e−2y dy

v(y) = c1 e2y + e2y

−y e−2y −1
2 e−2y

v(y) = c1 e2y −y −1
2.
Now we solve for u.
u(y) =

c1 e2y −y −1
2
1/2
.
dy
dx =

c1 e2y −y −1
2
1/2
974

This equation is separable.
dx =
dy
 c1 e2y −y −1
2
1/2
x + c2 =
Z
1
 c1 e2y −y −1
2
1/2 dy
Thus we ﬁnally have arrived at an implicit solution for y(x).
Example 18.4.2 Consider the equation
y′′ + y3 = 0.
With the change of variables, u(y) = y′, the equation becomes
u′u + y3 = 0.
This equation is separable.
u du = −y3 dy
1
2u2 = −1
4y4 + c1
u =

2c1 −1
2y4
1/2
y′ =

2c1 −1
2y4
1/2
dy
(2c1 −1
2y4)1/2 = dx
975

Integrating gives us the implicit solution
Z
1
(2c1 −1
2y4)1/2 dy = x + c2.
18.5
*Equidimensional-in-x Equations
Diﬀerential equations that are invariant under the change of variables x = c ξ are said to be equidimensional-in-x. For
a familiar example from linear equations, we note that the Euler equation is equidimensional-in-x. Writing the new
derivatives under the change of variables,
x = c ξ,
d
dx = 1
c
d
dξ,
d2
dx2 = 1
c2
d2
dξ2,
. . . .
Example 18.5.1 Consider the Euler equation
y′′ + 2
xy′ + 3
x2y = 0.
Under the change of variables, x = c ξ, y(x) = u(ξ), this equation becomes
1
c2u′′ + 2
c ξ
1
cu′ +
3
c2 ξ2u = 0
u′′ + 2
ξ u′ + 3
ξ2u = 0.
Thus this equation is invariant under the change of variables x = c ξ.
976

Example 18.5.2 For a nonlinear example, consider the equation
y′′ y′ + y′′
x y + y′
x2 = 0.
With the change of variables x = c ξ, y(x) = u(ξ) the equation becomes
u′′
c2
u′
c +
u′′
c3 ξ u +
u′
c3 ξ2 = 0
u′′ u′ + u′′
ξ u + u′
ξ2 = 0.
We see that this equation is also equidimensional-in-x.
You may recall that the change of variables x = et reduces an Euler equation to a constant coeﬃcient equation. To
generalize this result to nonlinear equations we will see that the same change of variables reduces an equidimensional-in-x
equation to an autonomous equation.
Writing the derivatives with respect to x in terms of t,
x = et,
d
dx = dt
dx
d
dt = e−t d
dt
x d
dx = d
dt
x2 d2
dx2 = x d
dx

x d
dx

−x d
dx = d2
dt2 −d
dt.
Example 18.5.3 Consider the equation in Example 18.5.2
y′′ y′ + y′′
x y + y′
x2 = 0.
977

Applying the change of variables x = et, y(x) = u(t) yields an autonomous equation for u(t).
x2 y′′ x y′ + x2 y′′
y
+ x y′ = 0
(u′′ −u′)u′ + u′′ −u′
u
+ u′ = 0
Result 18.5.1 A diﬀerential equation that is invariant under the change of variables x = c ξ
is equidimensional-in-x. Such an equation can be reduced to autonomous equation of the
same order with the change of variables, x = et.
18.6
*Equidimensional-in-y Equations
A diﬀerential equation is said to be equidimensional-in-y if it is invariant under the change of variables y(x) = c v(x).
Note that all linear homogeneous equations are equidimensional-in-y.
Example 18.6.1 Consider the linear equation
y′′ + p(x)y′ + q(x)y = 0.
With the change of variables y(x) = cv(x) the equation becomes
cv′′ + p(x)cv′ + q(x)cv = 0
v′′ + p(x)v′ + q(x)v = 0
Thus we see that the equation is invariant under the change of variables.
978

Example 18.6.2 For a nonlinear example, consider the equation
y′′y + (y′)2 −y2 = 0.
Under the change of variables y(x) = cv(x) the equation becomes.
cv′′cv + (cv′)2 −(cv)2 = 0
v′′v + (v′)2 −v2 = 0.
Thus we see that this equation is also equidimensional-in-y.
The change of variables y(x) = eu(x) reduces an nth order equidimensional-in-y equation to an equation of order
n −1 for u′. Writing the derivatives of eu(x),
d
dx eu = u′ eu
d2
dx2 eu = (u′′ + (u′)2) eu
d3
dx3 eu = (u′′′ + 3u′′u′′ + (u′)3) eu .
Example 18.6.3 Consider the linear equation in Example 18.6.1
y′′ + p(x)y′ + q(x)y = 0.
Under the change of variables y(x) = eu(x) the equation becomes
(u′′ + (u′)2) eu +p(x)u′ eu +q(x) eu = 0
u′′ + (u′)2 + p(x)u′ + q(x) = 0.
Thus we have a Riccati equation for u′. This transformation might seem rather useless since linear equations are
usually easier to work with than nonlinear equations, but it is often useful in determining the asymptotic behavior of
the equation.
979

Example 18.6.4 From Example 18.6.2 we have the equation
y′′y + (y′)2 −y2 = 0.
The change of variables y(x) = eu(x) yields
(u′′ + (u′)2) eu eu +(u′ eu)2 −(eu)2 = 0
u′′ + 2(u′)2 −1 = 0
u′′ = −2(u′)2 + 1
Now we have a Riccati equation for u′. We make the substitution u′ = v′
2v.
v′′
2v −(v′)2
2v2 = −2(v′)2
4v2 + 1
v′′ −2v = 0
v = c1 e
√
2x +c2 e−
√
2x
u′ = 2
√
2c1 e
√
2x −c2 e−
√
2x
c1 e
√
2x +c2 e−
√
2x
u = 2
Z c1
√
2 e
√
2x −c2
√
2 e−
√
2x
c1 e
√
2x +c2 e−
√
2x
dx + c3
u = 2 log

c1 e
√
2x +c2 e−
√
2x
+ c3
y =

c1 e
√
2x +c2 e−
√
2x2
ec3
The constants are redundant, the general solution is
y =

c1 e
√
2x +c2 e−
√
2x2
980

Result 18.6.1 A diﬀerential equation is equidimensional-in-y if it is invariant under the
change of variables y(x) = cv(x). An nth order equidimensional-in-y equation can be re-
duced to an equation of order n −1 in u′ with the change of variables y(x) = eu(x).
18.7
*Scale-Invariant Equations
Result 18.7.1 An equation is scale invariant if it is invariant under the change of variables,
x = cξ, y(x) = cαv(ξ), for some value of α. A scale-invariant equation can be transformed
to an equidimensional-in-x equation with the change of variables, y(x) = xαu(x).
Example 18.7.1 Consider the equation
y′′ + x2y2 = 0.
Under the change of variables x = cξ, y(x) = cαv(ξ) this equation becomes
cα
c2 v′′(ξ) + c2x2c2αv2(ξ) = 0.
Equating powers of c in the two terms yields α = −4.
Introducing the change of variables y(x) = x−4u(x) yields
d2
dx2

x−4u(x)

+ x2(x−4u(x))2 = 0
x−4u′′ −8x−5u′ + 20x−6u + x−6u2 = 0
x2u′′ −8xu′ + 20u + u2 = 0.
We see that the equation for u is equidimensional-in-x.
981

18.8
Exercises
Exercise 18.1
1. Find the general solution and the singular solution of the Clairaut equation,
y = xp + p2.
2. Show that the singular solution is the envelope of the general solution.
Hint, Solution
Bernoulli Equations
Exercise 18.2 (mathematica/ode/techniques nonlinear/bernoulli.nb)
Consider the Bernoulli equation
dy
dt + p(t)y = q(t)yα.
1. Solve the Bernoulli equation for α = 1.
2. Show that for α ̸= 1 the substitution u = y1−α reduces Bernoulli’s equation to a linear equation.
3. Find the general solution to the following equations.
t2dy
dt + 2ty −y3 = 0, t > 0
(a)
dy
dx + 2xy + y2 = 0
(b)
Hint, Solution
982

Exercise 18.3
Consider a population, y. Let the birth rate of the population be proportional to y with constant of proportionality 1.
Let the death rate of the population be proportional to y2 with constant of proportionality 1/1000. Assume that the
population is large enough so that you can consider y to be continuous. What is the population as a function of time
if the initial population is y0?
Hint, Solution
Exercise 18.4
Show that the transformation u = y1−n reduces the equation to a linear ﬁrst order equation. Solve the equations
1. t2dy
dt + 2ty −y3 = 0
t > 0
2. dy
dt = (Γ cos t + T) y −y3, Γ and T are real constants. (From a ﬂuid ﬂow stability problem.)
Hint, Solution
Riccati Equations
Exercise 18.5
1. Consider the Ricatti equation,
dy
dx = a(x)y2 + b(x)y + c(x).
Substitute
y = yp(x) +
1
u(x)
into the Ricatti equation, where yp is some particular solution to obtain a ﬁrst order linear diﬀerential equation
for u.
2. Consider a Ricatti equation,
y′ = 1 + x2 −2xy + y2.
983

Verify that yp(x) = x is a particular solution. Make the substitution y = yp + 1/u to ﬁnd the general solution.
What would happen if you continued this method, taking the general solution for yp? Would you be able to ﬁnd
a more general solution?
3. The substitution
y = −u′
au
gives us the second order, linear, homogeneous diﬀerential equation,
u′′ −
a′
a + b

u′ + acu = 0.
The general solution for u has two constants of integration. However, the solution for y should only have one
constant of integration as it satisﬁes a ﬁrst order equation. Write y in terms of the solution for u and verify tha
y has only one constant of integration.
Hint, Solution
Exchanging the Dependent and Independent Variables
Exercise 18.6
Solve the diﬀerential equation
y′ =
√y
xy + y.
Hint, Solution
Autonomous Equations
*Equidimensional-in-x Equations
*Equidimensional-in-y Equations
*Scale-Invariant Equations
984

18.9
Hints
Hint 18.1
Bernoulli Equations
Hint 18.2
Hint 18.3
The diﬀerential equation governing the population is
dy
dt = y −
y2
1000,
y(0) = y0.
This is a Bernoulli equation.
Hint 18.4
Riccati Equations
Hint 18.5
Exchanging the Dependent and Independent Variables
Hint 18.6
Exchange the dependent and independent variables.
Autonomous Equations
*Equidimensional-in-x Equations
985

*Equidimensional-in-y Equations
*Scale-Invariant Equations
986

18.10
Solutions
Solution 18.1
We consider the Clairaut equation,
y = xp + p2.
(18.2)
1. We diﬀerentiate Equation 18.2 with respect to x to obtain a second order diﬀerential equation.
y′ = y′ + xy′′ + 2y′y′′
y′′(2y′ + x) = 0
Equating the ﬁrst or second factor to zero will lead us to two distinct solutions.
y′′ = 0
or
y′ = −x
2
If y′′ = 0 then y′ ≡p is a constant, (say y′ = c). From Equation 18.2 we see that the general solution is,
y(x) = cx + c2.
(18.3)
Recall that the general solution of a ﬁrst order diﬀerential equation has one constant of integration.
If y′ = −x/2 then y = −x2/4 + const. We determine the constant by substituting the expression into Equa-
tion 18.2.
−x2
4 + c = x

−x
2

+

−x
2
2
Thus we see that a singular solution of the Clairaut equation is
y(x) = −1
4x2.
(18.4)
Recall that a singular solution of a ﬁrst order nonlinear diﬀerential equation has no constant of integration.
987

2. Equating the general and singular solutions, y(x), and their derivatives, y′(x), gives us the system of equations,
cx + c2 = −1
4x2,
c = −1
2x.
Since the ﬁrst equation is satisﬁed for c = −x/2, we see that the solution y = cx + c2 is tangent to the solution
y = −x2/4 at the point (−2c, −|c|). The solution y = cx + c2 is plotted for c = . . . , −1/4, 0, 1/4, . . . in
Figure 18.1.
-4
-2
2
4
-4
-3
-2
-1
1
2
Figure 18.1: The Envelope of y = cx + c2.
The envelope of a one-parameter family F(x, y, c) = 0 is given by the system of equations,
F(x, y, c) = 0,
Fc(x, y, c) = 0.
For the family of solutions y = cx + c2 these equations are
y = cx + c2,
0 = x + 2c.
Substituting the solution of the second equation, c = −x/2, into the ﬁrst equation gives the envelope,
y =

−1
2x

x +

−1
2x
2
= −1
4x2.
Thus we see that the singular solution is the envelope of the general solution.
988

Bernoulli Equations
Solution 18.2
1.
dy
dt + p(t)y = q(t)y
dy
y = (q −p) dt
ln y =
Z
(q −p) dt + c
y = c e
R
(q−p) dt
2. We consider the Bernoulli equation,
dy
dt + p(t)y = q(t)yα,
α ̸= 1.
We divide by yα.
y−αy′ + p(t)y1−α = q(t)
This suggests the change of dependent variable u = y1−α, u′ = (1 −α)y−αy′.
1
1 −α
d
dty1−α + p(t)y1−α = q(t)
du
dt + (1 −α)p(t)u = (1 −α)q(t)
Thus we obtain a linear equation for u which when solved will give us an implicit solution for y.
3.
(a)
t2dy
dt + 2ty −y3 = 0,
t > 0
t2 y′
y3 + 2t 1
y2 = 1
989

We make the change of variables u = y−2.
−1
2t2u′ + 2tu = 1
u′ −4
t u = −2
t2
The integrating factor is
µ = e
R
(−4/t) dt = e−4 ln t = t−4.
We multiply by the integrating factor and integrate to obtain the solution.
d
dt
 t−4u

= −2t−6
u = 2
5t−1 + ct4
y−2 = 2
5t−1 + ct4
y = ±
1
q
2
5t−1 + ct4
y = ±
√
5t
√
2 + ct5
(b)
dy
dx + 2xy + y2 = 0
y′
y2 + 2x
y = −1
We make the change of variables u = y−1.
u′ −2xu = 1
990

The integrating factor is
µ = e
R
(−2x) dx = e−x2 .
We multiply by the integrating factor and integrate to obtain the solution.
d
dx

e−x2 u

= e−x2
u = ex2 Z
e−x2 dx + c ex2
y =
e−x2
R e−x2 dx + c
Solution 18.3
The diﬀerential equation governing the population is
dy
dt = y −
y2
1000,
y(0) = y0.
We recognize this as a Bernoulli equation. The substitution u(t) = 1/y(t) yields
−du
dt = u −
1
1000,
u(0) = 1
y0
.
u′ + u =
1
1000
u = 1
y0
e−t + e−t
1000
Z t
0
eτ dτ
u =
1
1000 +
 1
y0
−
1
1000

e−t
Solving for y(t),
y(t) =

1
1000 +
 1
y0
−
1
1000

e−t
−1
.
991

As a check, we see that as t →∞, y(t) →1000, which is an equilibrium solution of the diﬀerential equation.
dy
dt = 0 = y −
y2
1000
→
y = 1000.
Solution 18.4
1.
t2dy
dt + 2ty −y3 = 0
dy
dt + 2t−1y = t−2y3
We make the change of variables u(t) = y−2(t).
u′ −4t−1u = −2t−2
This gives us a ﬁrst order, linear equation. The integrating factor is
I(t) = e
R
−4t−1 dt = e−4 log t = t−4.
We multiply by the integrating factor and integrate.
d
dt
 t−4u

= −2t−6
t−4u = 2
5t−5 + c
u = 2
5t−1 + ct4
992

Finally we write the solution in terms of y(t).
y(t) = ±
1
q
2
5t−1 + ct4
y(t) = ±
√
5t
√
2 + ct5
2.
dy
dt −(Γ cos t + T) y = −y3
We make the change of variables u(t) = y−2(t).
u′ + 2 (Γ cos t + T) u = 2
This gives us a ﬁrst order, linear equation. The integrating factor is
I(t) = e
R
2(Γ cos t+T) dt = e2(Γ sin t+Tt)
We multiply by the integrating factor and integrate.
d
dt
 e2(Γ sin t+Tt) u

= 2 e2(Γ sin t+Tt)
u = 2 e−2(Γ sin t+Tt)
Z
e2(Γ sin t+Tt) dt + c

Finally we write the solution in terms of y(t).
y = ±
eΓ sin t+Tt
q
2
 R e2(Γ sin t+Tt) dt + c

993

Riccati Equations
Solution 18.5
We consider the Ricatti equation,
dy
dx = a(x)y2 + b(x)y + c(x).
(18.5)
1. We substitute
y = yp(x) +
1
u(x)
into the Ricatti equation, where yp is some particular solution.
y′
p −u′
u2 = +a(x)

y2
p + 2yp
u + 1
u2

+ b(x)

yp + 1
u

+ c(x)
−u′
u2 = b(x)1
u + a(x)

2yp
u + 1
u2

u′ = −(b + 2ayp) u −a
We obtain a ﬁrst order linear diﬀerential equation for u whose solution will contain one constant of integration.
2. We consider a Ricatti equation,
y′ = 1 + x2 −2xy + y2.
(18.6)
We verify that yp(x) = x is a solution.
1 = 1 + x2 −2xx + x2
Substituting y = yp + 1/u into Equation 18.6 yields,
u′ = −(−2x + 2x) u −1
u = −x + c
y = x +
1
c −x
994

What would happen if we continued this method? Since y = x +
1
c−x is a solution of the Ricatti equation we can
make the substitution,
y = x +
1
c −x +
1
u(x),
(18.7)
which will lead to a solution for y which has two constants of integration. Then we could repeat the process,
substituting the sum of that solution and 1/u(x) into the Ricatti equation to ﬁnd a solution with three constants
of integration. We know that the general solution of a ﬁrst order, ordinary diﬀerential equation has only one
constant of integration. Does this method for Ricatti equations violate this theorem? There’s only one way to
ﬁnd out. We substitute Equation 18.7 into the Ricatti equation.
u′ = −

−2x + 2

x +
1
c −x

u −1
u′ = −
2
c −xu −1
u′ +
2
c −xu = −1
The integrating factor is
I(x) = e2/(c−x) = e−2 log(c−x) =
1
(c −x)2.
Upon multiplying by the integrating factor, the equation becomes exact.
d
dx

1
(c −x)2u

= −
1
(c −x)2
u = (c −x)2 −1
c −x + b(c −x)2
u = x −c + b(c −x)2
Thus the Ricatti equation has the solution,
y = x +
1
c −x +
1
x −c + b(c −x)2.
995

It appears that we we have found a solution that has two constants of integration, but appearances can be
deceptive. We do a little algebraic simpliﬁcation of the solution.
y = x +
1
c −x +
1
(b(c −x) −1)(c −x)
y = x +
(b(c −x) −1) + 1
(b(c −x) −1)(c −x)
y = x +
b
b(c −x) −1
y = x +
1
(c −1/b) −x
This is actually a solution, (namely the solution we had before), with one constant of integration, (namely c−1/b).
Thus we see that repeated applications of the procedure will not produce more general solutions.
3. The substitution
y = −u′
au
gives us the second order, linear, homogeneous diﬀerential equation,
u′′ −
a′
a + b

u′ + acu = 0.
The solution to this linear equation is a linear combination of two homogeneous solutions, u1 and u2.
u = c1u1(x) + c2u2(x)
The solution of the Ricatti equation is then
y = −
c1u′
1(x) + c2u′
2(x)
a(x)(c1u1(x) + c2u2(x)).
996

Since we can divide the numerator and denominator by either c1 or c2, this answer has only one constant of
integration, (namely c1/c2 or c2/c1).
Exchanging the Dependent and Independent Variables
Solution 18.6
Exchanging the dependent and independent variables in the diﬀerential equation,
y′ =
√y
xy + y,
yields
x′(y) = y1/2x + y1/2.
997

This is a ﬁrst order diﬀerential equation for x(y).
x′ −y1/2x = y1/2
d
dy

x exp

−2y3/2
3

= y1/2 exp

−2y3/2
3

x exp

−2y3/2
3

= −exp

−2y3/2
3

+ c1
x = −1 + c1 exp
2y3/2
3

x + 1
c1
= exp
2y3/2
3

log
x + 1
c1

= 2
3y3/2
y =
3
2 log
x + 1
c1
2/3
y =

c + 3
2 log(x + 1)
2/3
Autonomous Equations
*Equidimensional-in-x Equations
*Equidimensional-in-y Equations
*Scale-Invariant Equations
998

Chapter 19
Transformations and Canonical Forms
Prize intensity more than extent. Excellence resides in quality not in quantity. The best is always few and rare -
abundance lowers value. Even among men, the giants are usually really dwarfs. Some reckon books by the thickness,
as if they were written to exercise the brawn more than the brain. Extent alone never rises above mediocrity; it is the
misfortune of universal geniuses that in attempting to be at home everywhere are so nowhere. Intensity gives eminence
and rises to the heroic in matters sublime.
-Balthasar Gracian
19.1
The Constant Coeﬃcient Equation
The solution of any second order linear homogeneous diﬀerential equation can be written in terms of the solutions to
either
y′′ = 0,
or
y′′ −y = 0
Consider the general equation
y′′ + ay′ + by = 0.
999

We can solve this diﬀerential equation by making the substitution y = eλx. This yields the algebraic equation
λ2 + aλ + b = 0.
λ = 1
2

−a ±
√
a2 −4b

There are two cases to consider. If a2 ̸= 4b then the solutions are
y1 = e(−a+
√
a2−4b)x/2,
y2 = e(−a−
√
a2−4b)x/2
If a2 = 4b then we have
y1 = e−ax/2,
y2 = x e−ax/2
Note that regardless of the values of a and b the solutions are of the form
y = e−ax/2 u(x)
We would like to write the solutions to the general diﬀerential equation in terms of the solutions to simpler diﬀerential
equations. We make the substitution
y = eλx u
The derivatives of y are
y′ = eλx(u′ + λu)
y′′ = eλx(u′′ + 2λu′ + λ2u)
Substituting these into the diﬀerential equation yields
u′′ + (2λ + a)u′ + (λ2 + aλ + b)u = 0
In order to get rid of the u′ term we choose
λ = −a
2.
The equation is then
u′′ +

b −a2
4

u = 0.
There are now two cases to consider.
1000

Case 1.
If b = a2/4 then the diﬀerential equation is
u′′ = 0
which has solutions 1 and x. The general solution for y is then
y = e−ax/2(c1 + c2x).
Case 2.
If b ̸= a2/4 then the diﬀerential equation is
u′′ −
a2
4 −b

u = 0.
We make the change variables
u(x) = v(ξ),
x = µξ.
The derivatives in terms of ξ are
d
dx = dξ
dx
d
dξ = 1
µ
d
dξ
d2
dx2 = 1
µ
d
dξ
1
µ
d
dξ = 1
µ2
d2
dξ2.
The diﬀerential equation for v is
1
µ2v′′ −
a2
4 −b

v = 0
v′′ −µ2
a2
4 −b

v = 0
We choose
µ =
a2
4 −b
−1/2
1001

to obtain
v′′ −v = 0
which has solutions e±ξ. The solution for y is
y = eλx  c1 ex/µ +c2 e−x/µ
y = e−ax/2 
c1 e
√
a2/4−b x +c2 e−√
a2/4−b x
19.2
Normal Form
19.2.1
Second Order Equations
Consider the second order equation
y′′ + p(x)y′ + q(x)y = 0.
(19.1)
Through a change of dependent variable, this equation can be transformed to
u′′ + I(x)y = 0.
This is known as the normal form of (19.1). The function I(x) is known as the invariant of the equation.
Now to ﬁnd the change of variables that will accomplish this transformation. We make the substitution y(x) =
a(x)u(x) in (19.1).
au′′ + 2a′u′ + a′′u + p(au′ + a′u) + qau = 0
u′′ +

2a′
a + p

u′ +
a′′
a + pa′
a + q

u = 0
To eliminate the u′ term, a(x) must satisfy
2a′
a + p = 0
a′ + 1
2pa = 0
1002

a = c exp

−1
2
Z
p(x) dx

.
For this choice of a, our diﬀerential equation for u becomes
u′′ +

q −p2
4 −p′
2

u = 0.
Two diﬀerential equations having the same normal form are called equivalent.
Result 19.2.1 The change of variables
y(x) = exp

−1
2
Z
p(x) dx

u(x)
transforms the diﬀerential equation
y′′ + p(x)y′ + q(x)y = 0
into its normal form
u′′ + I(x)u = 0
where the invariant of the equation, I(x), is
I(x) = q −p2
4 −p′
2 .
19.2.2
Higher Order Diﬀerential Equations
Consider the third order diﬀerential equation
y′′′ + p(x)y′′ + q(x)y′ + r(x)y = 0.
1003

We can eliminate the y′′ term. Making the change of dependent variable
y = u exp

−1
3
Z
p(x) dx

y′ =

u′ −1
3pu

exp

−1
3
Z
p(x) dx

y′′ =

u′′ −2
3pu′ + 1
9(p2 −3p′)u

exp

−1
3
Z
p(x) dx

y′′ =

u′′′ −pu′′ + 1
3(p2 −3p′)u′ + 1
27(9p′ −9p′′ −p3)u

exp

−1
3
Z
p(x) dx

yields the diﬀerential equation
u′′′ + 1
3(3q −3p′ −p2)u′ + 1
27(27r −9pq −9p′′ + 2p3)u = 0.
Result 19.2.2 The change of variables
y(x) = exp

−1
n
Z
pn−1(x) dx

u(x)
transforms the diﬀerential equation
y(n) + pn−1(x)y(n−1) + pn−2(x)y(n−2) + · · · + p0(x)y = 0
into the form
u(n) + an−2(x)u(n−2) + an−3(x)u(n−3) + · · · + a0(x)u = 0.
1004

19.3
Transformations of the Independent Variable
19.3.1
Transformation to the form u” + a(x) u = 0
Consider the second order linear diﬀerential equation
y′′ + p(x)y′ + q(x)y = 0.
We make the change of independent variable
ξ = f(x),
u(ξ) = y(x).
The derivatives in terms of ξ are
d
dx = dξ
dx
d
dξ = f ′ d
dξ
d2
dx2 = f ′ d
dξf ′ d
dξ = (f ′)2 d2
dξ2 + f ′′ d
dξ
The diﬀerential equation becomes
(f ′)2u′′ + f ′′u′ + pf ′u′ + qu = 0.
In order to eliminate the u′ term, f must satisfy
f ′′ + pf ′ = 0
f ′ = exp

−
Z
p(x) dx

f =
Z
exp

−
Z
p(x) dx

dx.
The diﬀerential equation for u is then
u′′ +
q
(f ′)2u = 0
1005

u′′(ξ) + q(x) exp

2
Z
p(x) dx

u(ξ) = 0.
Result 19.3.1 The change of variables
ξ =
Z
exp

−
Z
p(x) dx

dx,
u(ξ) = y(x)
transforms the diﬀerential equation
y′′ + p(x)y′ + q(x)y = 0
into
u′′(ξ) + q(x) exp

2
Z
p(x) dx

u(ξ) = 0.
19.3.2
Transformation to a Constant Coeﬃcient Equation
Consider the second order linear diﬀerential equation
y′′ + p(x)y′ + q(x)y = 0.
With the change of independent variable
ξ = f(x),
u(ξ) = y(x),
the diﬀerential equation becomes
(f ′)2u′′ + (f ′′ + pf ′)u′ + qu = 0.
For this to be a constant coeﬃcient equation we must have
(f ′)2 = c1q,
and
f ′′ + pf ′ = c2q,
1006

for some constants c1 and c2. Solving the ﬁrst condition,
f ′ = c√q,
f = c
Z p
q(x) dx.
The second constraint becomes
f ′′ + pf ′
q
= const
1
2cq−1/2q′ + pcq1/2
q
= const
q′ + 2pq
q3/2
= const.
Result 19.3.2 Consider the diﬀerential equation
y′′ + p(x)y′ + q(x)y = 0.
If the expression
q′ + 2pq
q3/2
is a constant then the change of variables
ξ = c
Z p
q(x) dx,
u(ξ) = y(x),
will yield a constant coeﬃcient diﬀerential equation. (Here c is an arbitrary constant.)
1007

19.4
Integral Equations
Volterra’s Equations.
Volterra’s integral equation of the ﬁrst kind has the form
Z x
a
N(x, ξ)f(ξ) dξ = f(x).
The Volterra equation of the second kind is
y(x) = f(x) + λ
Z x
a
N(x, ξ)y(ξ) dξ.
N(x, ξ) is known as the kernel of the equation.
Fredholm’s Equations.
Fredholm’s integral equations of the ﬁrst and second kinds are
Z b
a
N(x, ξ)f(ξ) dξ = f(x),
y(x) = f(x) + λ
Z b
a
N(x, ξ)y(ξ) dξ.
19.4.1
Initial Value Problems
Consider the initial value problem
y′′ + p(x)y′ + q(x)y = f(x),
y(a) = α,
y′(a) = β.
Integrating this equation twice yields
Z x
a
Z η
a
y′′(ξ) + p(ξ)y′(ξ) + q(ξ)y(ξ) dξ dη =
Z x
a
Z η
a
f(ξ) dξ dη
1008

Z x
a
(x −ξ)[y′′(ξ) + p(ξ)y′(ξ) + q(ξ)y(ξ)] dξ =
Z x
a
(x −ξ)f(ξ) dξ.
Now we use integration by parts.

(x −ξ)y′(ξ)
x
a −
Z x
a
−y′(ξ) dξ +

(x −ξ)p(ξ)y(ξ)
x
a −
Z x
a
[(x −ξ)p′(ξ) −p(ξ)]y(ξ) dξ
+
Z x
a
(x −ξ)q(ξ)y(ξ) dξ =
Z x
a
(x −ξ)f(ξ) dξ.
−(x −a)y′(a) + y(x) −y(a) −(x −a)p(a)y(a) −
Z x
a
[(x −ξ)p′(ξ) −p(ξ)]y(ξ) dξ
+
Z x
a
(x −ξ)q(ξ)y(ξ) dξ =
Z x
a
(x −ξ)f(ξ) dξ.
We obtain a Volterra integral equation of the second kind for y(x).
y(x) =
Z x
a
(x −ξ)f(ξ) dξ + (x −a)(αp(a) + β) + α +
Z x
a

(x −ξ)[p′(ξ) −q(ξ)] −p(ξ)
	
y(ξ) dξ.
Note that the initial conditions for the diﬀerential equation are “built into” the Volterra equation. Setting x = a in
the Volterra equation yields y(a) = α. Diﬀerentiating the Volterra equation,
y′(x) =
Z x
a
f(ξ) dξ + (αp(a) + β) −p(x)y(x) +
Z x
a
[p′(ξ) −q(ξ)] −p(ξ)y(ξ) dξ
and setting x = a yields
y′(a) = αp(a) + β −p(a)α = β.
(Recall from calculus that
d
dx
Z x
g(x, ξ) dξ = g(x, x) +
Z x ∂
∂x[g(x, ξ)] dξ.)
1009

Result 19.4.1 The initial value problem
y′′ + p(x)y′ + q(x)y = f(x),
y(a) = α,
y′(a) = β.
is equivalent to the Volterra equation of the second kind
y(x) = F(x) +
Z x
a
N(x, ξ)y(ξ) dξ
where
F(x) =
Z x
a
(x −ξ)f(ξ) dξ + (x −a)(αp(a) + β) + α
N(x, ξ) = (x −ξ)[p′(ξ) −q(ξ)] −p(ξ).
19.4.2
Boundary Value Problems
Consider the boundary value problem
y′′ = f(x),
y(a) = α,
y(b) = β.
(19.2)
To obtain a problem with homogeneous boundary conditions, we make the change of variable
y(x) = u(x) + α + β −α
b −a (x −a)
to obtain the problem
u′′ = f(x),
u(a) = u(b) = 0.
Now we will use Green’s functions to write the solution as an integral. First we solve the problem
G′′ = δ(x −ξ),
G(a|ξ) = G(b|ξ) = 0.
1010

The homogeneous solutions of the diﬀerential equation that satisfy the left and right boundary conditions are
c1(x −a)
and
c2(x −b).
Thus the Green’s function has the form
G(x|ξ) =
(
c1(x −a),
for x ≤ξ
c2(x −b),
for x ≥ξ
Imposing continuity of G(x|ξ) at x = ξ and a unit jump of G(x|ξ) at x = ξ, we obtain
G(x|ξ) =
(
(x−a)(ξ−b)
b−a
,
for x ≤ξ
(x−b)(ξ−a)
b−a
,
for x ≥ξ
Thus the solution of the (19.2) is
y(x) = α + β −α
b −a (x −a) +
Z b
a
G(x|ξ)f(ξ) dξ.
Now consider the boundary value problem
y′′ + p(x)y′ + q(x)y = 0,
y(a) = α,
y(b) = β.
From the above result we can see that the solution satisﬁes
y(x) = α + β −α
b −a (x −a) +
Z b
a
G(x|ξ)[f(ξ) −p(ξ)y′(ξ) −q(ξ)y(ξ)] dξ.
Using integration by parts, we can write
−
Z b
a
G(x|ξ)p(ξ)y′(ξ) dξ = −

G(x|ξ)p(ξ)y(ξ)
b
a +
Z b
a
∂G(x|ξ)
∂ξ
p(ξ) + G(x|ξ)p′(ξ)

y(ξ) dξ
=
Z b
a
∂G(x|ξ)
∂ξ
p(ξ) + G(x|ξ)p′(ξ)

y(ξ) dξ.
1011

Substituting this into our expression for y(x),
y(x) = α + β −α
b −a (x −a) +
Z b
a
G(x|ξ)f(ξ) dξ +
Z b
a
∂G(x|ξ)
∂ξ
p(ξ) + G(x|ξ)[p′(ξ) −q(ξ)]

y(ξ) dξ,
we obtain a Fredholm integral equation of the second kind.
Result 19.4.2 The boundary value problem
y′′ + p(x)y′ + q(x)y = f(x),
y(a) = α,
y(b) = β.
is equivalent to the Fredholm equation of the second kind
y(x) = F(x) +
Z b
a
N(x, ξ)y(ξ) dξ
where
F(x) = α + β −α
b −a (x −a) +
Z b
a
G(x|ξ)f(ξ) dξ,
N(x, ξ) =
Z b
a
H(x|ξ)y(ξ) dξ,
G(x|ξ) =
((x−a)(ξ−b)
b−a
,
for x ≤ξ
(x−b)(ξ−a)
b−a
,
for x ≥ξ,
H(x|ξ) =
((x−a)
b−a p(ξ) + (x−a)(ξ−b)
b−a
[p′(ξ) −q(ξ)]
for x ≤ξ
(x−b)
b−a p(ξ) + (x−b)(ξ−a)
b−a
[p′(ξ) −q(ξ)]
for x ≥ξ.
1012

19.5
Exercises
The Constant Coeﬃcient Equation
Normal Form
Exercise 19.1
Solve the diﬀerential equation
y′′ +

2 + 4
3x

y′ + 1
9
 24 + 12x + 4x2
y = 0.
Hint, Solution
Transformations of the Independent Variable
Integral Equations
Exercise 19.2
Show that the solution of the diﬀerential equation
y′′ + 2(a + bx)y′ + (c + dx + ex2)y = 0
can be written in terms of one of the following canonical forms:
v′′ + (ξ2 + A)v = 0
v′′ = ξv
v′′ + v = 0
v′′ = 0.
Hint, Solution
Exercise 19.3
Show that the solution of the diﬀerential equation
y′′ + 2

a + b
x

y′ +

c + d
x + e
x2

y = 0
1013

can be written in terms of one of the following canonical forms:
v′′ +

1 + A
ξ + B
ξ2

v = 0
v′′ +
1
ξ + A
ξ2

v = 0
v′′ + A
ξ2v = 0
Hint, Solution
Exercise 19.4
Show that the second order Euler equation
x2 d2y
d2x + a1xdy
dx + a0y = 0
can be transformed to a constant coeﬃcient equation.
Hint, Solution
Exercise 19.5
Solve Bessel’s equation of order 1/2,
y′′ + 1
xy′ +

1 −1
4x2

y = 0.
Hint, Solution
1014

19.6
Hints
The Constant Coeﬃcient Equation
Normal Form
Hint 19.1
Transform the equation to normal form.
Transformations of the Independent Variable
Integral Equations
Hint 19.2
Transform the equation to normal form and then apply the scale transformation x = λξ + µ.
Hint 19.3
Transform the equation to normal form and then apply the scale transformation x = λξ.
Hint 19.4
Make the change of variables x = et, y(x) = u(t). Write the derivatives with respect to x in terms of t.
x = et
dx = et dt
d
dx = e−t d
dt
x d
dx = d
dt
Hint 19.5
Transform the equation to normal form.
1015

19.7
Solutions
The Constant Coeﬃcient Equation
Normal Form
Solution 19.1
y′′ +

2 + 4
3x

y′ + 1
9
 24 + 12x + 4x2
y = 0
To transform the equation to normal form we make the substitution
y = exp

−1
2
Z 
2 + 4
3x

dx

u
= e−x−x2/3 u
The invariant of the equation is
I(x) = 1
9
 24 + 12x + 4x2
−1
4

2 + 4
3x
2
−1
2
d
dx

2 + 4
3x

= 1.
The normal form of the diﬀerential equation is then
u′′ + u = 0
which has the general solution
u = c1 cos x + c2 sin x
Thus the equation for y has the general solution
y = c1 e−x−x2/3 cos x + c2 e−x−x2/3 sin x.
1016

Transformations of the Independent Variable
Integral Equations
Solution 19.2
The substitution that will transform the equation to normal form is
y = exp

−1
2
Z
2(a + bx) dx

u
= e−ax−bx2/2 u.
The invariant of the equation is
I(x) = c + dx + ex2 −1
4(2(a + bx))2 −1
2
d
dx(2(a + bx))
= c −b −a2 + (d −2ab)x + (e −b2)x2
≡α + βx + γx2
The normal form of the diﬀerential equation is
u′′ + (α + βx + γx2)u = 0
We consider the following cases:
γ = 0.
β = 0.
α = 0. We immediately have the equation
u′′ = 0.
α ̸= 0. With the change of variables
v(ξ) = u(x),
x = α−1/2ξ,
we obtain
v′′ + v = 0.
1017

β ̸= 0. We have the equation
y′′ + (α + βx)y = 0.
The scale transformation x = λξ + µ yields
v′′ + λ2(α + β(λξ + µ))y = 0
v′′ = [βλ3ξ + λ2(βµ + α)]v.
Choosing
λ = (−β)−1/3,
µ = −α
β
yields the diﬀerential equation
v′′ = ξv.
γ ̸= 0. The scale transformation x = λξ + µ yields
v′′ + λ2[α + β(λξ + µ) + γ(λξ + µ)2]v = 0
v′′ + λ2[α + βµ + γµ2 + λ(β + 2γµ)ξ + λ2γξ2]v = 0.
Choosing
λ = γ−1/4,
µ = −β
2γ
yields the diﬀerential equation
v′′ + (ξ2 + A)v = 0
where
A = γ−1/2 −1
4βγ−3/2.
1018

Solution 19.3
The substitution that will transform the equation to normal form is
y = exp

−1
2
Z
2

a + b
x

dx

u
= x−b e−ax u.
The invariant of the equation is
I(x) = c + d
x + e
x2 −1
4

2

a + b
x
2
−1
2
d
dx

2

a + b
x

= c −ax + d −2ab
x
+ e + b −b2
x2
≡α + β
x + γ
x2.
The invariant form of the diﬀerential equation is
u′′ +

α + β
x + γ
x2

u = 0.
We consider the following cases:
α = 0.
β = 0. We immediately have the equation
u′′ + γ
x2u = 0.
β ̸= 0. We have the equation
u′′ +
β
x + γ
x2

u = 0.
1019

The scale transformation u(x) = v(ξ), x = λξ yields
v′′ +
βλ
ξ + γ
ξ2

u = 0.
Choosing λ = β−1, we obtain
v′′ +
1
ξ + γ
ξ2

u = 0.
α ̸= 0. The scale transformation x = λξ yields
v′′ +

αλ2 + βλ
ξ + γ
ξ2

v = 0.
Choosing λ = α−1/2, we obtain
v′′ +

1 + α−1/2β
ξ
+ γ
ξ2

v = 0.
Solution 19.4
We write the derivatives with respect to x in terms of t.
x = et
dx = et dt
d
dx = e−t d
dt
x d
dx = d
dt
Now we express x2 d2
dx2 in terms of t.
x2 d2
dx2 = x d
dx

x d
dx

−x d
dx = d2
dt2 −d
dt
1020

Thus under the change of variables, x = et, y(x) = u(t), the Euler equation becomes
u′′ −u′ + a1u′ + a0u = 0
u′′ + (a1 −1)u′ + a0u = 0.
Solution 19.5
The transformation
y = exp

−1
2
Z 1
x dx

= x−1/2u
will put the equation in normal form. The invariant is
I(x) =

1 −1
4x2

−1
4
 1
x2

−1
2
−1
x2 = 1.
Thus we have the diﬀerential equation
u′′ + u = 0,
with the solution
u = c1 cos x + c2 sin x.
The solution of Bessel’s equation of order 1/2 is
y = c1x−1/2 cos x + c2x−1/2 sin x.
1021

Chapter 20
The Dirac Delta Function
I do not know what I appear to the world; but to myself I seem to have been only like a boy playing on a seashore,
and diverting myself now and then by ﬁnding a smoother pebble or a prettier shell than ordinary, whilst the great ocean
of truth lay all undiscovered before me.
- Sir Issac Newton
20.1
Derivative of the Heaviside Function
The Heaviside function H(x) is deﬁned
H(x) =
(
0
for x < 0,
1
for x > 0.
The derivative of the Heaviside function is zero for x ̸= 0. At x = 0 the derivative is undeﬁned. We will represent
the derivative of the Heaviside function by the Dirac delta function, δ(x). The delta function is zero for x ̸= 0 and
inﬁnite at the point x = 0. Since the derivative of H(x) is undeﬁned, δ(x) is not a function in the conventional sense
of the word. One can derive the properties of the delta function rigorously, but the treatment in this text will be almost
entirely heuristic.
1022

The Dirac delta function is deﬁned by the properties
δ(x) =
(
0
for x ̸= 0,
∞
for x = 0,
and
Z ∞
−∞
δ(x) dx = 1.
The second property comes from the fact that δ(x) represents the derivative of H(x). The Dirac delta function is
conceptually pictured in Figure 20.1.
Figure 20.1: The Dirac Delta Function.
Let f(x) be a continuous function that vanishes at inﬁnity. Consider the integral
Z ∞
−∞
f(x)δ(x) dx.
1023

We use integration by parts to evaluate the integral.
Z ∞
−∞
f(x)δ(x) dx =

f(x)H(x)
∞
−∞−
Z ∞
−∞
f ′(x)H(x) dx
= −
Z ∞
0
f ′(x) dx
= [−f(x)]∞
0
= f(0)
We assumed that f(x) vanishes at inﬁnity in order to use integration by parts to evaluate the integral. However,
since the delta function is zero for x ̸= 0, the integrand is nonzero only at x = 0. Thus the behavior of the function
at inﬁnity should not aﬀect the value of the integral. Thus it is reasonable that f(0) =
R ∞
−∞f(x)δ(x) dx holds for all
continuous functions. By changing variables and noting that δ(x) is symmetric we can derive a more general formula.
f(0) =
Z ∞
−∞
f(ξ)δ(ξ) dξ
f(x) =
Z ∞
−∞
f(ξ + x)δ(ξ) dξ
f(x) =
Z ∞
−∞
f(ξ)δ(ξ −x) dξ
f(x) =
Z ∞
−∞
f(ξ)δ(x −ξ) dξ
This formula is very important in solving inhomogeneous diﬀerential equations.
20.2
The Delta Function as a Limit
Consider a function b(x, ϵ) deﬁned by
b(x, ϵ) =
(
0
for |x| > ϵ/2
1
ϵ
for |x| < ϵ/2.
1024

The graph of b(x, 1/10) is shown in Figure 20.2.
-1
1
5
10
Figure 20.2: Graph of b(x, 1/10).
The Dirac delta function δ(x) can be thought of as b(x, ϵ) in the limit as ϵ →0. Note that the delta function so
deﬁned satisﬁes the properties,
δ(x) =
(
0
for x ̸= 0
∞
for x = 0
and
Z ∞
−∞
δ(x) dx = 1
Delayed Limiting Process.
When the Dirac delta function appears inside an integral, we can think of the delta
function as a delayed limiting process.
Z ∞
−∞
f(x)δ(x) dx ≡lim
ϵ→0
Z ∞
−∞
f(x)b(x, ϵ) dx.
1025

Let f(x) be a continuous function and let F ′(x) = f(x). We compute the integral of f(x)δ(x).
Z ∞
−∞
f(x)δ(x) dx = lim
ϵ→0
1
ϵ
Z ϵ/2
−ϵ/2
f(x) dx
= lim
ϵ→0
1
ϵ[F(x)]ϵ/2
−ϵ/2
= lim
ϵ→0
F(ϵ/2) −F(−ϵ/2)
ϵ
= F ′(0)
= f(0)
20.3
Higher Dimensions
We can deﬁne a Dirac delta function in n-dimensional Cartesian space, δn(x), x ∈Rn. It is deﬁned by the following
two properties.
δn(x) = 0
for
x ̸= 0
Z
Rn δn(x) dx = 1
It is easy to verify, that the n-dimensional Dirac delta function can be written as a product of 1-dimensional Dirac delta
functions.
δn(x) =
n
Y
k=1
δ(xk)
1026

20.4
Non-Rectangular Coordinate Systems
We can derive Dirac delta functions in non-rectangular coordinate systems by making a change of variables in the
relation,
Z
Rn δn(x) dx = 1
Where the transformation is non-singular, one merely divides the Dirac delta function by the Jacobian of the transfor-
mation to the coordinate system.
Example 20.4.1 Consider the Dirac delta function in cylindrical coordinates, (r, θ, z). The Jacobian is J = r.
Z ∞
−∞
Z 2π
0
Z ∞
0
δ3 (x −x0) r dr dθ dz = 1
For r0 ̸= 0, the Dirac Delta function is
δ3 (x −x0) = 1
rδ (r −r0) δ (θ −θ0) δ (z −z0)
since it satisﬁes the two deﬁning properties.
1
rδ (r −r0) δ (θ −θ0) δ (z −z0) = 0
for
(r, θ, z) ̸= (r0, θ0, z0)
Z ∞
−∞
Z 2π
0
Z ∞
0
1
rδ (r −r0) δ (θ −θ0) δ (z −z0) r dr dθ dz
=
Z ∞
0
δ (r −r0) dr
Z 2π
0
δ (θ −θ0) dθ
Z ∞
−∞
δ (z −z0) dz = 1
For r0 = 0, we have
δ3 (x −x0) =
1
2πrδ (r) δ (z −z0)
1027

since this again satisﬁes the two deﬁning properties.
1
2πrδ (r) δ (z −z0) = 0
for
(r, z) ̸= (0, z0)
Z ∞
−∞
Z 2π
0
Z ∞
0
1
2πrδ (r) δ (z −z0) r dr dθ dz = 1
2π
Z ∞
0
δ (r) dr
Z 2π
0
dθ
Z ∞
−∞
δ (z −z0) dz = 1
1028

20.5
Exercises
Exercise 20.1
Let f(x) be a function that is continuous except for a jump discontinuity at x = 0. Using a delayed limiting process,
show that
f(0−) + f(0+)
2
=
Z ∞
−∞
f(x)δ(x) dx.
Hint, Solution
Exercise 20.2
Show that the Dirac delta function is symmetric.
δ(−x) = δ(x)
Hint, Solution
Exercise 20.3
Show that
δ(cx) = δ(x)
|c| .
Hint, Solution
Exercise 20.4
We will consider the Dirac delta function with a function as on argument, δ(y(x)). Assume that y(x) has simple zeros
at the points {xn}.
y(xn) = 0,
y′(xn) ̸= 0
Further assume that y(x) has no multiple zeros. (If y(x) has multiple zeros δ(y(x)) is not well-deﬁned in the same
sense that 1/0 is not well-deﬁned.) Prove that
δ(y(x)) =
X
n
δ(x −xn)
|y′(xn)| .
Hint, Solution
1029

Exercise 20.5
Justify the identity
Z ∞
−∞
f(x)δ(n)(x) dx = (−1)nf (n)(0)
From this show that
δ(n)(−x) = (−1)nδ(n)(x)
and
xδ(n)(x) = −nδ(n−1)(x).
Hint, Solution
Exercise 20.6
Consider x = (x1, . . . , xn) ∈Rn and the curvilinear coordinate system ξ = (ξ1, . . . , ξn). Show that
δ(x −a) = δ(ξ −α)
|J|
where a and α are corresponding points in the two coordinate systems and J is the Jacobian of the transformation
from x to ξ.
J ≡∂x
∂ξ
Hint, Solution
Exercise 20.7
Determine the Dirac delta function in spherical coordinates, (r, θ, φ).
x = r cos θ sin φ,
y = r sin θ sin φ,
z = r cos φ
Hint, Solution
1030

20.6
Hints
Hint 20.1
Hint 20.2
Verify that δ(−x) satisﬁes the two properties of the Dirac delta function.
Hint 20.3
Evaluate the integral,
Z ∞
−∞
f(x)δ(cx) dx,
by noting that the Dirac delta function is symmetric and making a change of variables.
Hint 20.4
Let the points {ξm} partition the interval (−∞. . . ∞) such that y′(x) is monotone on each interval (ξm . . . ξm+1).
Consider some such interval, (a . . . b) ≡(ξm . . . ξm+1). Show that
Z b
a
δ(y(x)) dx =
(R β
α
δ(y)
|y′(xn)| dy
if y(xn) = 0 for a < xn < b
0
otherwise
for α = min(y(a), y(b)) and β = max(y(a), y(b)). Now consider the integral on the interval (−∞. . . ∞) as the sum
of integrals on the intervals {(ξm . . . ξm+1)}.
Hint 20.5
Justify the identity,
Z ∞
−∞
f(x)δ(n)(x) dx = (−1)nf (n)(0),
with integration by parts.
1031

Hint 20.6
The Dirac delta function is deﬁned by the following two properties.
δ(x −a) = 0
for
x ̸= a
Z
Rn δ(x −a) dx = 1
Verify that δ(ξ −α)/|J| satisﬁes these properties in the ξ coordinate system.
Hint 20.7
Consider the special cases φ0 = 0, π and r0 = 0.
1032

20.7
Solutions
Solution 20.1
Let F ′(x) = f(x).
Z ∞
−∞
f(x)δ(x) dx = lim
ϵ→0
1
ϵ
Z ∞
−∞
f(x)b(x, ϵ) dx
= lim
ϵ→0
1
ϵ
 Z 0
−ϵ/2
f(x)b(x, ϵ) dx +
Z ϵ/2
0
f(x)b(x, ϵ) dx
!
= lim
ϵ→0
1
ϵ ((F(0) −F(−ϵ/2)) + (F(ϵ/2) −F(0)))
= lim
ϵ→0
1
2
F(0) −F(−ϵ/2)
ϵ/2
+ F(ϵ/2) −F(0)
ϵ/2

= F ′(0−) + F ′(0+)
2
= f(0−) + f(0+)
2
Solution 20.2
δ(−x) satisﬁes the two properties of the Dirac delta function.
δ(−x) = 0 for x ̸= 0
Z ∞
−∞
δ(−x) dx =
Z −∞
∞
δ(x) (−dx) =
Z ∞
−∞
δ(−x) dx = 1
Therefore δ(−x) = δ(x).
1033

Solution 20.3
We note the the Dirac delta function is symmetric and we make a change of variables to derive the identity.
Z ∞
−∞
δ(cx) dx =
Z ∞
−∞
δ(|c|x) dx
=
Z ∞
−∞
δ(x)
|c| dx
δ(cx) = δ(x)
|c|
Solution 20.4
Let the points {ξm} partition the interval (−∞. . . ∞) such that y′(x) is monotone on each interval (ξm . . . ξm+1).
Consider some such interval, (a . . . b) ≡(ξm . . . ξm+1). Note that y′(x) is either entirely positive or entirely negative in
the interval. First consider the case when it is positive. In this case y(a) < y(b).
Z b
a
δ(y(x)) dx =
Z y(b)
y(a)
δ(y)
dy
dx
−1
dy
=
Z y(b)
y(a)
δ(y)
y′(x) dy
=
(R y(b)
y(a)
δ(y)
y′(xn) dy
for y(xn) = 0 if y(a) < 0 < y(b)
0
otherwise
1034

Now consider the case that y′(x) is negative on the interval so y(a) > y(b).
Z b
a
δ(y(x)) dx =
Z y(b)
y(a)
δ(y)
dy
dx
−1
dy
=
Z y(b)
y(a)
δ(y)
y′(x) dy
=
Z y(a)
y(b)
δ(y)
−y′(x) dy
=
(R y(a)
y(b)
δ(y)
−y′(xn) dy
for y(xn) = 0 if y(b) < 0 < y(a)
0
otherwise
We conclude that
Z b
a
δ(y(x)) dx =
(R β
α
δ(y)
|y′(xn)| dy
if y(xn) = 0 for a < xn < b
0
otherwise
for α = min(y(a), y(b)) and β = max(y(a), y(b)).
1035

Now we turn to the integral of δ(y(x)) on (−∞. . . ∞). Let αm = min(y(ξm), y(ξm)) and βm = max(y(ξm), y(ξm)).
Z ∞
−∞
δ(y(x)) dx =
X
m
Z ξm+1
ξm
δ(y(x)) dx
=
X
m
xn∈(ξm...ξm+1)
Z ξm+1
ξm
δ(y(x)) dx
=
X
m
xn∈(ξm...ξm+1)
Z βm+1
αm
δ(y)
|y′(xn)| dy
=
X
n
Z ∞
−∞
δ(y)
|y′(xn)| dy
=
Z ∞
−∞
X
n
δ(y)
|y′(xn)| dy
δ(y(x)) =
X
n
δ(x −xn)
|y′(xn)|
Solution 20.5
To justify the identity,
Z ∞
−∞
f(x)δ(n)(x) dx = (−1)nf (n)(0),
1036

we will use integration by parts.
Z ∞
−∞
f(x)δ(n)(x) dx =

f(x)δ(n−1)(x)
∞
−∞−
Z ∞
−∞
f ′(x)δ(n−1)(x) dx
= −
Z ∞
−∞
f ′(x)δ(n−1)(x) dx
= (−1)n
Z ∞
−∞
f (n)(x)δ(x) dx
= (−1)nf (n)(0)
CONTINUE HERE
δ(n)(−x) = (−1)nδ(n)(x)
and
xδ(n)(x) = −nδ(n−1)(x).
Solution 20.6
The Dirac delta function is deﬁned by the following two properties.
δ(x −a) = 0
for
x ̸= a
Z
Rn δ(x −a) dx = 1
We verify that δ(ξ −α)/|J| satisﬁes these properties in the ξ coordinate system.
δ(ξ −α)
|J|
= δ(ξ1 −α1) · · · δ(ξn −αn)
|J|
= 0
for
ξ ̸= α
1037

Z δ(ξ −α)
|J|
|J| dξ =
Z
δ(ξ −α) dξ
=
Z
δ(ξ1 −α1) · · · δ(ξn −αn) dξ
=
Z
δ(ξ1 −α1) dξ1 · · ·
Z
δ(ξn −αn) dξn
= 1
We conclude that δ(ξ −α)/|J| is the Dirac delta function in the ξ coordinate system.
δ(x −a) = δ(ξ −α)
|J|
Solution 20.7
We consider the Dirac delta function in spherical coordinates, (r, θ, φ). The Jacobian is J = r2 sin(φ).
Z π
0
Z 2π
0
Z ∞
0
δ3 (x −x0) r2 sin(φ) dr dθ dφ = 1
For r0 ̸= 0, and φ0 ̸= 0, π, the Dirac Delta function is
δ3 (x −x0) =
1
r2 sin(φ)δ (r −r0) δ (θ −θ0) δ (φ −φ0)
since it satisﬁes the two deﬁning properties.
1
r2 sin(φ)δ (r −r0) δ (θ −θ0) δ (φ −φ0) = 0
for
(r, θ, φ) ̸= (r0, θ0, φ0)
Z π
0
Z 2π
0
Z ∞
0
1
r2 sin(φ)δ (r −r0) δ (θ −θ0) δ (φ −φ0) r2 sin(φ) dr dθ dφ
=
Z ∞
0
δ (r −r0) dr
Z 2π
0
δ (θ −θ0) dθ
Z π
0
δ (φ −φ0) dφ = 1
1038

For φ0 = 0 or φ0 = π, the Dirac delta function is
δ3 (x −x0) =
1
2πr2 sin(φ)δ (r −r0) δ (φ −φ0) .
We check that the value of the integral is unity.
Z π
0
Z 2π
0
Z ∞
0
1
2πr2 sin(φ)δ (r −r0) δ (φ −φ0) r2 sin(φ) dr dθ dφ
= 1
2π
Z ∞
0
δ (r −r0) dr
Z 2π
0
dθ
Z π
0
δ (φ −φ0) dφ = 1
For r0 = 0 the Dirac delta function is
δ3 (x) =
1
4πr2δ (r)
We verify that the value of the integral is unity.
Z π
0
Z 2π
0
Z ∞
0
1
4πr2δ (r −r0) r2 sin(φ) dr dθ dφ = 1
4π
Z ∞
0
δ (r) dr
Z 2π
0
dθ
Z π
0
sin(φ) dφ = 1
1039

Chapter 21
Inhomogeneous Diﬀerential Equations
Feelin’ stupid? I know I am!
-Homer Simpson
21.1
Particular Solutions
Consider the nth order linear homogeneous equation
L[y] ≡y(n) + pn−1(x)y(n−1) + · · · + p1(x)y′ + p0(x)y = 0.
Let {y1, y2, . . . , yn} be a set of linearly independent homogeneous solutions, L[yk] = 0. We know that the general
solution of the homogeneous equation is a linear combination of the homogeneous solutions.
yh =
n
X
k=1
ckyk(x)
Now consider the nth order linear inhomogeneous equation
L[y] ≡y(n) + pn−1(x)y(n−1) + · · · + p1(x)y′ + p0(x)y = f(x).
1040

Any function yp which satisﬁes this equation is called a particular solution of the diﬀerential equation. We want to
know the general solution of the inhomogeneous equation. Later in this chapter we will cover methods of constructing
this solution; now we consider the form of the solution.
Let yp be a particular solution. Note that yp + h is a particular solution if h satisﬁes the homogeneous equation.
L[yp + h] = L[yp] + L[h] = f + 0 = f
Therefore yp + yh satisﬁes the homogeneous equation. We show that this is the general solution of the inhomogeneous
equation. Let yp and ηp both be solutions of the inhomogeneous equation L[y] = f. The diﬀerence of yp and ηp is a
homogeneous solution.
L[yp −ηp] = L[yp] −L[ηp] = f −f = 0
yp and ηp diﬀer by a linear combination of the homogeneous solutions {yk}. Therefore the general solution of L[y] = f
is the sum of any particular solution yp and the general homogeneous solution yh.
yp + yh = yp(x) +
n
X
k=1
ckyk(x)
Result 21.1.1 The general solution of the nth order linear inhomogeneous equation L[y] =
f(x) is
y = yp + c1y1 + c2y2 + · · · + cnyn,
where yp is a particular solution, {y1, . . . , yn} is a set of linearly independent homogeneous
solutions, and the ck’s are arbitrary constants.
Example 21.1.1 The diﬀerential equation
y′′ + y = sin(2x)
has the two homogeneous solutions
y1 = cos x,
y2 = sin x,
1041

and a particular solution
yp = −1
3 sin(2x).
We can add any combination of the homogeneous solutions to yp and it will still be a particular solution. For example,
ηp = −1
3 sin(2x) −1
3 sin x
= −2
3 sin
3x
2

cos
x
2

is a particular solution.
21.2
Method of Undetermined Coeﬃcients
The ﬁrst method we present for computing particular solutions is the method of undetermined coeﬃcients. For some
simple diﬀerential equations, (primarily constant coeﬃcient equations), and some simple inhomogeneities we are able
to guess the form of a particular solution. This form will contain some unknown parameters. We substitute this form
into the diﬀerential equation to determine the parameters and thus determine a particular solution.
Later in this chapter we will present general methods which work for any linear diﬀerential equation and any
inhogeneity. Thus one might wonder why I would present a method that works only for some simple problems. (And
why it is called a “method” if it amounts to no more than guessing.) The answer is that guessing an answer is less
grungy than computing it with the formulas we will develop later. Also, the process of this guessing is not random,
there is rhyme and reason to it.
Consider an nth order constant coeﬃcient, inhomogeneous equation.
L[y] ≡y(n) + an−1y(n−1) + · · · + a1y′ + a0y = f(x)
If f(x) is one of a few simple forms, then we can guess the form of a particular solution. Below we enumerate some
cases.
1042

f = p(x). If f is an mth order polynomial, f(x) = pmxm + · · · + p1x + p0, then guess
yp = cmxm + · · · c1x + c0.
f = p(x) eax. If f is a polynomial times an exponential then guess
yp = (cmxm + · · · c1x + c0) eax .
f = p(x) eax cos (bx). If f is a cosine or sine times a polynomial and perhaps an exponential, f(x) = p(x) eax cos(bx)
or f(x) = p(x) eax sin(bx) then guess
yp = (cmxm + · · · c1x + c0) eax cos(bx) + (dmxm + · · · d1x + d0) eax sin(bx).
Likewise for hyperbolic sines and hyperbolic cosines.
Example 21.2.1 Consider
y′′ −2y′ + y = t2.
The homogeneous solutions are y1 = et and y2 = t et. We guess a particular solution of the form
yp = at2 + bt + c.
We substitute the expression into the diﬀerential equation and equate coeﬃcients of powers of t to determine the
parameters.
y′′
p −2y′
p + yp = t2
(2a) −2(2at + b) + (at2 + bt + c) = t2
(a −1)t2 + (b −4a)t + (2a −2b + c) = 0
a −1 = 0,
b −4a = 0,
2a −2b + c = 0
a = 1,
b = 4,
c = 6
A particular solution is
yp = t2 + 4t + 6.
1043

If the inhomogeneity is a sum of terms, L[y] = f ≡f1+· · ·+fk, you can solve the problems L[y] = f1, . . . , L[y] = fk
independently and then take the sum of the solutions as a particular solution of L[y] = f.
Example 21.2.2 Consider
L[y] ≡y′′ −2y′ + y = t2 + e2t .
(21.1)
The homogeneous solutions are y1 = et and y2 = t et. We already know a particular solution to L[y] = t2. We seek a
particular solution to L[y] = e2t. We guess a particular solution of the form
yp = a e2t .
We substitute the expression into the diﬀerential equation to determine the parameter.
y′′
p −2y′
p + yp = e2t
4ae2t −4a e2t +a e2t = e2t
a = 1
A particular solution of L[y] = e2t is yp = e2t. Thus a particular solution of Equation 21.1 is
yp = t2 + 4t + 6 + e2t .
The above guesses will not work if the inhomogeneity is a homogeneous solution. In this case, multiply the guess by
the lowest power of x such that the guess does not contain homogeneous solutions.
Example 21.2.3 Consider
L[y] ≡y′′ −2y′ + y = et .
The homogeneous solutions are y1 = et and y2 = t et. Guessing a particular solution of the form yp = a et would not
work because L[et] = 0. We guess a particular solution of the form
yp = at2 et
1044

We substitute the expression into the diﬀerential equation and equate coeﬃcients of like terms to determine the
parameters.
y′′
p −2y′
p + yp = et
(at2 + 4at + 2a) et −2(at2 + 2at) et +at2 et = et
2a et = et
a = 1
2
A particular solution is
yp = t2
2 et .
Example 21.2.4 Consider
y′′ + 1
xy′ + 1
x2y = x,
x > 0.
The homogeneous solutions are y1 = cos(ln x) and y2 = sin(ln x). We guess a particular solution of the form
yp = ax3
We substitute the expression into the diﬀerential equation and equate coeﬃcients of like terms to determine the
parameter.
y′′
p + 1
xy′
p + 1
x2yp = x
6ax + 3ax + ax = x
a = 1
10
A particular solution is
yp = x3
10.
1045

21.3
Variation of Parameters
In this section we present a method for computing a particular solution of an inhomogeneous equation given that we
know the homogeneous solutions. We will ﬁrst consider second order equations and then generalize the result for nth
order equations.
21.3.1
Second Order Diﬀerential Equations
Consider the second order inhomogeneous equation,
L[y] ≡y′′ + p(x)y′ + q(x)y = f(x),
on a < x < b.
We assume that the coeﬃcient functions in the diﬀerential equation are continuous on [a . . . b]. Let y1(x) and y2(x)
be two linearly independent solutions to the homogeneous equation. Since the Wronskian,
W(x) = exp

−
Z
p(x) dx

,
is non-vanishing, we know that these solutions exist. We seek a particular solution of the form,
yp = u1(x)y1 + u2(x)y2.
We compute the derivatives of yp.
y′
p = u′
1y1 + u1y′
1 + u′
2y2 + u2y′
2
y′′
p = u′′
1y1 + 2u′
1y′
1 + u1y′′
1 + u′′
2y2 + 2u′
2y′
2 + u2y′′
2
We substitute the expression for yp and its derivatives into the inhomogeneous equation and use the fact that y1 and
y2 are homogeneous solutions to simplify the equation.
u′′
1y1 + 2u′
1y′
1 + u1y′′
1 + u′′
2y2 + 2u′
2y′
2 + u2y′′
2 + p(u′
1y1 + u1y′
1 + u′
2y2 + u2y′
2) + q(u1y1 + u2y2) = f
u′′
1y1 + 2u′
1y′
1 + u′′
2y2 + 2u′
2y′
2 + p(u′
1y1 + u′
2y2) = f
1046

This is an ugly equation for u1 and u2, however, we have an ace up our sleeve. Since u1 and u2 are undetermined
functions of x, we are free to impose a constraint. We choose this constraint to simplify the algebra.
u′
1y1 + u′
2y2 = 0
This constraint simpliﬁes the derivatives of yp,
y′
p = u′
1y1 + u1y′
1 + u′
2y2 + u2y′
2
= u1y′
1 + u2y′
2
y′′
p = u′
1y′
1 + u1y′′
1 + u′
2y′
2 + u2y′′
2.
We substitute the new expressions for yp and its derivatives into the inhomogeneous diﬀerential equation to obtain a
much simpler equation than before.
u′
1y′
1 + u1y′′
1 + u′
2y′
2 + u2y′′
2 + p(u1y′
1 + u2y′
2) + q(u1y1 + u2y2) = f(x)
u′
1y′
1 + u′
2y′
2 + u1L[y1] + u2L[y2] = f(x)
u′
1y′
1 + u′
2y′
2 = f(x).
With the constraint, we have a system of linear equations for u′
1 and u′
2.
u′
1y1 + u′
2y2 = 0
u′
1y′
1 + u′
2y′
2 = f(x).
y1
y2
y′
1
y′
2
 u′
1
u′
2

=
0
f

We solve this system using Kramer’s rule. (See Appendix S.)
u′
1 = −f(x)y2
W(x)
u′
2 = f(x)y1
W(x)
1047

Here W(x) is the Wronskian.
W(x) =

y1
y2
y′
1
y′
2

We integrate to get u1 and u2. This gives us a particular solution.
yp = −y1
Z f(x)y2(x)
W(x)
dx + y2
Z f(x)y1(x)
W(x)
dx.
Result 21.3.1 Let y1 and y2 be linearly independent homogeneous solutions of
L[y] = y′′ + p(x)y′ + q(x)y = f(x).
A particular solution is
yp = −y1(x)
Z f(x)y2(x)
W(x)
dx + y2(x)
Z f(x)y1(x)
W(x)
dx,
where W(x) is the Wronskian of y1 and y2.
Example 21.3.1 Consider the equation,
y′′ + y = cos(2x).
The homogeneous solutions are y1 = cos x and y2 = sin x. We compute the Wronskian.
W(x) =

cos x
sin x
−sin x
cos x
 = cos2 x + sin2 x = 1
1048

We use variation of parameters to ﬁnd a particular solution.
yp = −cos(x)
Z
cos(2x) sin(x) dx + sin(x)
Z
cos(2x) cos(x) dx
= −1
2 cos(x)
Z  sin(3x) −sin(x)

dx + 1
2 sin(x)
Z  cos(3x) + cos(x)

dx
= −1
2 cos(x)

−1
3 cos(3x) + cos(x)

+ 1
2 sin(x)
1
3 sin(3x) + sin(x)

= 1
2
 sin2(x) −cos2(x)

+ 1
6
 cos(3x) cos(x) + sin(3x) sin(x)

= −1
2 cos(2x) + 1
6 cos(2x)
= −1
3 cos(2x)
The general solution of the inhomogeneous equation is
y = −1
3 cos(2x) + c1 cos(x) + c2 sin(x).
21.3.2
Higher Order Diﬀerential Equations
Consider the nth order inhomogeneous equation,
L[y] = y(n) + pn−1(x)y(n−1) + · · · + p1(x)y′ + p0(x)y = f(x),
on a < x < b.
We assume that the coeﬃcient functions in the diﬀerential equation are continuous on [a . . . b]. Let {y1, . . . , yn} be a
set of linearly independent solutions to the homogeneous equation. Since the Wronskian,
W(x) = exp

−
Z
pn−1(x) dx

,
1049

is non-vanishing, we know that these solutions exist. We seek a particular solution of the form
yp = u1y1 + u2y2 + · · · + unyn.
Since {u1, . . . , un} are undetermined functions of x, we are free to impose n−1 constraints. We choose these constraints
to simplify the algebra.
u′
1y1
+u′
2y2
+ · · ·+u′
nyn
=0
u′
1y′
1
+u′
2y′
2
+ · · ·+u′
ny′
n
=0
...
+ ...
+ ... + ...
=0
u′
1y(n−2)
1
+u′
2y(n−2)
2
+ · · ·+u′
ny(n−2)
n
=0
We diﬀerentiate the expression for yp, utilizing our constraints.
yp =u1y1 +u2y2 + · · ·+unyn
y′
p =u1y′
1 +u2y′
2 + · · ·+uny′
n
y′′
p =u1y′′
1 +u2y′′
2 + · · ·+uny′′
n
... = ...
+ ...
+ ... + ...
y(n)
p =u1y(n)
1 +u2y(n)
2 + · · ·+uny(n)
n
+ u′
1y(n−1)
1
+ u′
2y(n−1)
2
+ · · · + u′
ny(n−1)
n
We substitute yp and its derivatives into the inhomogeneous diﬀerential equation and use the fact that the yk are
homogeneous solutions.
u1y(n)
1
+ · · · + uny(n)
n
+ u′
1y(n−1)
1
+ · · · + u′
ny(n−1)
n
+ pn−1(u1y(n−1)
1
+ · · · + uny(n−1)
n
) + · · · + p0(u1y1 + · · · unyn) = f
u1L[y1] + u2L[y2] + · · · + unL[yn] + u′
1y(n−1)
1
+ u′
2y(n−1)
2
+ · · · + u′
ny(n−1)
n
= f
u′
1y(n−1)
1
+ u′
2y(n−1)
2
+ · · · + u′
ny(n−1)
n
= f.
1050

With the constraints, we have a system of linear equations for {u1, . . . , un}.





y1
y2
· · ·
yn
y′
1
y′
2
· · ·
y′
n
...
...
...
...
y(n−1)
1
y(n−1)
2
· · ·
y(n−1)
n










u′
1
u′
2...
u′
n




=





0
...
0
f




.
We solve this system using Kramer’s rule. (See Appendix S.)
u′
k = (−1)n+k+1W[y1, . . . , yk−1, yk+1, . . . , yn]
W[y1, y2, . . . , yn]
f,
for k = 1, . . . , n,
Here W is the Wronskian.
We integrating to obtain the uk’s.
uk = (−1)n+k+1
Z W[y1, . . . , yk−1, yk+1, . . . , yn](x)
W[y1, y2, . . . , yn](x)
f(x) dx,
for k = 1, . . . , n
Result 21.3.2 Let {y1, . . . , yn} be linearly independent homogeneous solutions of
L[y] = y(n) + pn−1(x)y(n−1) + · · · + p1(x)y′ + p0(x)y = f(x),
on a < x < b.
A particular solution is
yp = u1y1 + u2y2 + · · · + unyn.
where
uk = (−1)n+k+1
Z W[y1, . . . , yk−1, yk+1, . . . , yn](x)
W[y1, y2, . . . , yn](x)
f(x) dx,
for k = 1, . . . , n,
and W[y1, y2, . . . , yn](x) is the Wronskian of {y1(x), . . . , yn(x)}.
1051

21.4
Piecewise Continuous Coeﬃcients and Inhomogeneities
Example 21.4.1 Consider the problem
y′′ −y = e−α|x|,
y(±∞) = 0,
α > 0, α ̸= 1.
The homogeneous solutions of the diﬀerential equation are ex and e−x. We use variation of parameters to ﬁnd a
particular solution for x > 0.
yp = −ex
Z x e−ξ e−αξ
−2
dξ + e−x
Z x eξ e−αξ
−2
dξ
= 1
2 ex
Z x
e−(α+1)ξ dξ −1
2 e−x
Z x
e(1−α)ξ dξ
= −
1
2(α + 1) e−αx +
1
2(α −1) e−αx
=
e−αx
α2 −1,
for x > 0
A particular solution for x < 0 is
yp =
eαx
α2 −1,
for x < 0.
Thus a particular solution is
yp = e−α|x|
α2 −1.
The general solution is
y =
1
α2 −1 e−α|x| +c1 ex +c2 e−x .
Applying the boundary conditions, we see that c1 = c2 = 0. Apparently the solution is
y = e−α|x|
α2 −1.
1052

This function is plotted in Figure 21.1. This function satisﬁes the diﬀerential equation for positive and negative x. It
also satisﬁes the boundary conditions. However, this is NOT a solution to the diﬀerential equation. Since the diﬀerential
equation has no singular points and the inhomogeneous term is continuous, the solution must be twice continuously
diﬀerentiable. Since the derivative of e−α|x| /(α2 −1) has a jump discontinuity at x = 0, the second derivative does not
exist. Thus this function could not possibly be a solution to the diﬀerential equation. In the next example we examine
the right way to solve this problem.
-4
-2
2
4
0.05
0.1
0.15
0.2
0.25
0.3
-4
-2
2
4
-0.3
-0.25
-0.2
-0.15
-0.1
-0.05
Figure 21.1: The Incorrect and Correct Solution to the Diﬀerential Equation.
Example 21.4.2 Again consider
y′′ −y = e−α|x|,
y(±∞) = 0,
α > 0, α ̸= 1.
1053

Separating this into two problems for positive and negative x,
y′′
−−y−= eαx,
y−(−∞) = 0,
on −∞< x ≤0,
y′′
+ −y+ = e−αx,
y+(∞) = 0,
on 0 ≤x < ∞.
In order for the solution over the whole domain to be twice diﬀerentiable, the solution and it’s ﬁrst derivative must be
continuous. Thus we impose the additional boundary conditions
y−(0) = y+(0),
y′
−(0) = y′
+(0).
The solutions that satisfy the two diﬀerential equations and the boundary conditions at inﬁnity are
y−=
eαx
α2 −1 + c−ex,
y+ =
e−αx
α2 −1 + c+ e−x .
The two additional boundary conditions give us the equations
y−(0) = y+(0)
→
c−= c+
y′
−(0) = y′
+(0)
→
α
α2 −1 + c−= −
α
α2 −1 −c+.
We solve these two equations to determine c−and c+.
c−= c+ = −
α
α2 −1
Thus the solution over the whole domain is
y =
(
eαx −α ex
α2−1
for x < 0,
e−αx −α e−x
α2−1
for x > 0
y = e−α|x| −α e−|x|
α2 −1
.
This function is plotted in Figure 21.1. You can verify that this solution is twice continuously diﬀerentiable.
1054

21.5
Inhomogeneous Boundary Conditions
21.5.1
Eliminating Inhomogeneous Boundary Conditions
Consider the nth order equation
L[y] = f(x),
for a < x < b,
subject to the linear inhomogeneous boundary conditions
Bj[y] = γj,
for j = 1, . . . , n,
where the boundary conditions are of the form
B[y] ≡α0y(a) + α1y′(a) + · · · + yn−1y(n−1)(a) + β0y(b) + β1y′(b) + · · · + βn−1y(n−1)
Let g(x) be an n-times continuously diﬀerentiable function that satisﬁes the boundary conditions. Substituting y = u+g
into the diﬀerential equation and boundary conditions yields
L[u] = f(x) −L[g],
Bj[u] = bj −Bj[g] = 0
for j = 1, . . . , n.
Note that the problem for u has homogeneous boundary conditions. Thus a problem with inhomogeneous boundary
conditions can be reduced to one with homogeneous boundary conditions. This technique is of limited usefulness for
ordinary diﬀerential equations but is important for solving some partial diﬀerential equation problems.
Example 21.5.1 Consider the problem
y′′ + y = cos 2x,
y(0) = 1,
y(π) = 2.
g(x) = x
π + 1 satisﬁes the boundary conditions. Substituting y = u + g yields
u′′ + u = cos 2x −x
π −1,
y(0) = y(π) = 0.
1055

Example 21.5.2 Consider
y′′ + y = cos 2x,
y′(0) = y(π) = 1.
g(x) = sin x −cos x satisﬁes the inhomogeneous boundary conditions. Substituting y = u + sin x −cos x yields
u′′ + u = cos 2x,
u′(0) = u(π) = 0.
Note that since g(x) satisﬁes the homogeneous equation, the inhomogeneous term in the equation for u is the same as
that in the equation for y.
Example 21.5.3 Consider
y′′ + y = cos 2x,
y(0) = 2
3,
y(π) = −4
3.
g(x) = cos x −1
3 satisﬁes the boundary conditions. Substituting y = u + cos x −1
3 yields
u′′ + u = cos 2x + 1
3,
u(0) = u(π) = 0.
Result 21.5.1 The nth order diﬀerential equation with boundary conditions
L[y] = f(x),
Bj[y] = bj,
for j = 1, . . . , n
has the solution y = u + g where u satisﬁes
L[u] = f(x) −L[g],
Bj[u] = 0,
for j = 1, . . . , n
and g is any n-times continuously diﬀerentiable function that satisﬁes the inhomogeneous
boundary conditions.
1056

21.5.2
Separating Inhomogeneous Equations and Inhomogeneous Boundary Con-
ditions
Now consider a problem with inhomogeneous boundary conditions
L[y] = f(x),
B1[y] = γ1,
B2[y] = γ2.
In order to solve this problem, we solve the two problems
L[u] = f(x),
B1[u] = B2[u] = 0,
and
L[v] = 0,
B1[v] = γ1,
B2[v] = γ2.
The solution for the problem with an inhomogeneous equation and inhomogeneous boundary conditions will be the sum
of u and v. To verify this,
L[u + v] = L[u] + L[v] = f(x) + 0 = f(x),
Bi[u + v] = Bi[u] + Bi[v] = 0 + γi = γi.
This will be a useful technique when we develop Green functions.
Result 21.5.2 The solution to
L[y] = f(x),
B1[y] = γ1,
B2[y] = γ2,
is y = u + v where
L[u] = f(x),
B1[u] = 0,
B2[u] = 0,
and
L[v] = 0,
B1[v] = γ1,
B2[v] = γ2.
1057

21.5.3
Existence of Solutions of Problems with Inhomogeneous Boundary Con-
ditions
Consider the nth order homogeneous diﬀerential equation
L[y] = y(n) + pn−1y(n−1) + · · · + p1y′ + p0y = f(x),
for a < x < b,
subject to the n inhomogeneous boundary conditions
Bj[y] = γj,
for j = 1, . . . , n
where each boundary condition is of the form
B[y] ≡α0y(a) + α1y′(a) + · · · + αn−1y(n−1)(a) + β0y(b) + β1y′(b) + · · · + βn−1y(n−1)(b).
We assume that the coeﬃcients in the diﬀerential equation are continuous on [a, b]. Since the Wronskian of the solutions
of the diﬀerential equation,
W(x) = exp

−
Z
pn−1(x) dx

,
is non-vanishing on [a, b], there are n linearly independent solution on that range. Let {y1, . . . , yn} be a set of linearly
independent solutions of the homogeneous equation. From Result 21.3.2 we know that a particular solution yp exists.
The general solution of the diﬀerential equation is
y = yp + c1y1 + c2y2 + · · · + cnyn.
The n boundary conditions impose the matrix equation,





B1[y1]
B1[y2]
· · ·
B1[yn]
B2[y1]
B2[y2]
· · ·
B2[yn]
...
...
...
...
Bn[y1]
Bn[y2]
· · ·
Bn[yn]










c1
c2...
cn




=





γ1 −B1[yp]
γ2 −B2[yp]
...
γn −Bn[yp]





1058

This equation has a unique solution if and only if the equation





B1[y1]
B1[y2]
· · ·
B1[yn]
B2[y1]
B2[y2]
· · ·
B2[yn]
...
...
...
...
Bn[y1]
Bn[y2]
· · ·
Bn[yn]










c1
c2...
cn




=





0
0
...
0





has only the trivial solution. (This is the case if and only if the determinant of the matrix is nonzero.) Thus the problem
L[y] = y(n) + pn−1y(n−1) + · · · + p1y′ + p0y = f(x),
for a < x < b,
subject to the n inhomogeneous boundary conditions
Bj[y] = γj,
for j = 1, . . . , n,
has a unique solution if and only if the problem
L[y] = y(n) + pn−1y(n−1) + · · · + p1y′ + p0y = 0,
for a < x < b,
subject to the n homogeneous boundary conditions
Bj[y] = 0,
for j = 1, . . . , n,
has only the trivial solution.
1059

Result 21.5.3 The problem
L[y] = y(n) + pn−1y(n−1) + · · · + p1y′ + p0y = f(x),
for a < x < b,
subject to the n inhomogeneous boundary conditions
Bj[y] = γj,
for j = 1, . . . , n,
has a unique solution if and only if the problem
L[y] = y(n) + pn−1y(n−1) + · · · + p1y′ + p0y = 0,
for a < x < b,
subject to
Bj[y] = 0,
for j = 1, . . . , n,
has only the trivial solution.
21.6
Green Functions for First Order Equations
Consider the ﬁrst order inhomogeneous equation
L[y] ≡y′ + p(x)y = f(x),
for x > a,
(21.2)
subject to a homogeneous initial condition, B[y] ≡y(a) = 0.
The Green function G(x|ξ) is deﬁned as the solution to
L[G(x|ξ)] = δ(x −ξ)
subject to G(a|ξ) = 0.
We can represent the solution to the inhomogeneous problem in Equation 21.2 as an integral involving the Green
1060

function. To show that
y(x) =
Z ∞
a
G(x|ξ)f(ξ) dξ
is the solution, we apply the linear operator L to the integral. (Assume that the integral is uniformly convergent.)
L
Z ∞
a
G(x|ξ)f(ξ) dξ

=
Z ∞
a
L[G(x|ξ)]f(ξ) dξ
=
Z ∞
a
δ(x −ξ)f(ξ) dξ
= f(x)
The integral also satisﬁes the initial condition.
B
Z ∞
a
G(x|ξ)f(ξ) dξ

=
Z ∞
a
B[G(x|ξ)]f(ξ) dξ
=
Z ∞
a
(0)f(ξ) dξ
= 0
Now we consider the qualitiative behavior of the Green function.
For x ̸= ξ, the Green function is simply a
homogeneous solution of the diﬀerential equation, however at x = ξ we expect some singular behavior. G′(x|ξ) will
have a Dirac delta function type singularity. This means that G(x|ξ) will have a jump discontinuity at x = ξ. We
integrate the diﬀerential equation on the vanishing interval (ξ−. . . ξ+) to determine this jump.
G′ + p(x)G = δ(x −ξ)
G(ξ+|ξ) −G(ξ−|ξ) +
Z ξ+
ξ−p(x)G(x|ξ) dx = 1
G(ξ+|ξ) −G(ξ−|ξ) = 1
(21.3)
1061

The homogeneous solution of the diﬀerential equation is
yh = e−
R
p(x) dx
Since the Green function satisﬁes the homogeneous equation for x ̸= ξ, it will be a constant times this homogeneous
solution for x < ξ and x > ξ.
G(x|ξ) =
(
c1 e−
R
p(x) dx
a < x < ξ
c2 e−
R
p(x) dx
ξ < x
In order to satisfy the homogeneous initial condition G(a|ξ) = 0, the Green function must vanish on the interval
(a . . . ξ).
G(x|ξ) =
(
0
a < x < ξ
c e−
R
p(x) dx
ξ < x
The jump condition, (Equation 21.3), gives us the constraint G(ξ+|ξ) = 1.
This determines the constant in the
homogeneous solution for x > ξ.
G(x|ξ) =
(
0
a < x < ξ
e−
R x
ξ p(t) dt
ξ < x
We can use the Heaviside function to write the Green function without using a case statement.
G(x|ξ) = e−
R x
ξ p(t) dt H(x −ξ)
Clearly the Green function is of little value in solving the inhomogeneous diﬀerential equation in Equation 21.2, as
we can solve that problem directly. However, we will encounter ﬁrst order Green function problems in solving some
partial diﬀerential equations.
1062

Result 21.6.1 The ﬁrst order inhomogeneous diﬀerential equation with homogeneous initial
condition
L[y] ≡y′ + p(x)y = f(x),
for a < x,
y(a) = 0,
has the solution
y =
Z ∞
a
G(x|ξ)f(ξ) dξ,
where G(x|ξ) satisﬁes the equation
L[G(x|ξ)] = δ(x −ξ),
for a < x,
G(a|ξ) = 0.
The Green function is
G(x|ξ) = e−
R x
ξ p(t) dt H(x −ξ)
21.7
Green Functions for Second Order Equations
Consider the second order inhomogeneous equation
L[y] = y′′ + p(x)y′ + q(x)y = f(x),
for a < x < b,
(21.4)
subject to the homogeneous boundary conditions
B1[y] = B2[y] = 0.
The Green function G(x|ξ) is deﬁned as the solution to
L[G(x|ξ)] = δ(x −ξ)
subject to B1[G] = B2[G] = 0.
1063

The Green function is useful because you can represent the solution to the inhomogeneous problem in Equation 21.4
as an integral involving the Green function. To show that
y(x) =
Z b
a
G(x|ξ)f(ξ) dξ
is the solution, we apply the linear operator L to the integral. (Assume that the integral is uniformly convergent.)
L
Z b
a
G(x|ξ)f(ξ) dξ

=
Z b
a
L[G(x|ξ)]f(ξ) dξ
=
Z b
a
δ(x −ξ)f(ξ) dξ
= f(x)
The integral also satisﬁes the boundary conditions.
Bi
Z b
a
G(x|ξ)f(ξ) dξ

=
Z b
a
Bi[G(x|ξ)]f(ξ) dξ
=
Z b
a
[0]f(ξ) dξ
= 0
One of the advantages of using Green functions is that once you ﬁnd the Green function for a linear operator and
certain homogeneous boundary conditions,
L[G] = δ(x −ξ),
B1[G] = B2[G] = 0,
you can write the solution for any inhomogeneity, f(x).
L[f] = f(x),
B1[y] = B2[y] = 0
1064

You do not need to do any extra work to obtain the solution for a diﬀerent inhomogeneous term.
Qualitatively, what kind of behavior will the Green function for a second order diﬀerential equation have? Will it
have a delta function singularity; will it be continuous? To answer these questions we will ﬁrst look at the behavior of
integrals and derivatives of δ(x).
The integral of δ(x) is the Heaviside function, H(x).
H(x) =
Z x
−∞
δ(t) dt =
(
0
for x < 0
1
for x > 0
The integral of the Heaviside function is the ramp function, r(x).
r(x) =
Z x
−∞
H(t) dt =
(
0
for x < 0
x
for x > 0
The derivative of the delta function is zero for x ̸= 0. At x = 0 it goes from 0 up to +∞, down to −∞and then back
up to 0.
In Figure 21.2 we see conceptually the behavior of the ramp function, the Heaviside function, the delta function,
and the derivative of the delta function.
Figure 21.2: r(x), H(x), δ(x) and
d
dxδ(x)
1065

We write the diﬀerential equation for the Green function.
G′′(x|ξ) + p(x)G′(x|ξ) + q(x)G(x|ξ) = δ(x −ξ)
we see that only the G′′(x|ξ) term can have a delta function type singularity. If one of the other terms had a delta
function type singularity then G′′(x|ξ) would be more singular than a delta function and there would be nothing in the
right hand side of the equation to match this kind of singularity. Analogous to the progression from a delta function
to a Heaviside function to a ramp function, we see that G′(x|ξ) will have a jump discontinuity and G(x|ξ) will be
continuous.
Let y1 and y2 be two linearly independent solutions to the homogeneous equation, L[y] = 0. Since the Green
function satisﬁes the homogeneous equation for x ̸= ξ, it will be a linear combination of the homogeneous solutions.
G(x|ξ) =
(
c1y1 + c2y2
for x < ξ
d1y1 + d2y2
for x > ξ
We require that G(x|ξ) be continuous.
G(x|ξ)

x→ξ−= G(x|ξ)

x→ξ+
We can write this in terms of the homogeneous solutions.
c1y1(ξ) + c2y2(ξ) = d1y1(ξ) + d2y2(ξ)
We integrate L[G(x|ξ)] = δ(x −ξ) from ξ−to ξ+.
Z ξ+
ξ−[G′′(x|ξ) + p(x)G′(x|ξ) + q(x)G(x|ξ)] dx =
Z ξ+
ξ−δ(x −ξ) dx.
1066

Since G(x|ξ) is continuous and G′(x|ξ) has only a jump discontinuity two of the terms vanish.
Z ξ+
ξ−p(x)G′(x|ξ) dx = 0
and
Z ξ+
ξ−q(x)G(x|ξ) dx = 0
Z ξ+
ξ−G′′(x|ξ) dx =
Z ξ+
ξ−δ(x −ξ) dx

G′(x|ξ)
ξ+
ξ−=

H(x −ξ)
ξ+
ξ−
G′(ξ+|ξ) −G′(ξ−|ξ) = 1
We write this jump condition in terms of the homogeneous solutions.
d1y′
1(ξ) + d2y′
2(ξ) −c1y′
1(ξ) −c2y′
2(ξ) = 1
Combined with the two boundary conditions, this gives us a total of four equations to determine our four constants,
c1, c2, d1, and d2.
Result 21.7.1 The second order inhomogeneous diﬀerential equation with homogeneous
boundary conditions
L[y] = y′′ + p(x)y′ + q(x)y = f(x),
for a < x < b,
B1[y] = B2[y] = 0,
has the solution
y =
Z b
a
G(x|ξ)f(ξ) dξ,
where G(x|ξ) satisﬁes the equation
L[G(x|ξ)] = δ(x −ξ),
for a < x < b,
B1[G(x|ξ)] = B2[G(x|ξ)] = 0.
G(x|ξ) is continuous and G′(x|ξ) has a jump discontinuity of height 1 at x = ξ.
1067

Example 21.7.1 Solve the boundary value problem
y′′ = f(x),
y(0) = y(1) = 0,
using a Green function.
A pair of solutions to the homogeneous equation are y1 = 1 and y2 = x. First note that only the trivial solution
to the homogeneous equation satisﬁes the homogeneous boundary conditions. Thus there is a unique solution to this
problem.
The Green function satisﬁes
G′′(x|ξ) = δ(x −ξ),
G(0|ξ) = G(1|ξ) = 0.
The Green function has the form
G(x|ξ) =
(
c1 + c2x
for x < ξ
d1 + d2x
for x > ξ.
Applying the two boundary conditions, we see that c1 = 0 and d1 = −d2. The Green function now has the form
G(x|ξ) =
(
cx
for x < ξ
d(x −1)
for x > ξ.
Since the Green function must be continuous,
cξ = d(ξ −1)
→
d = c
ξ
ξ −1.
From the jump condition,
d
dxc
ξ
ξ −1(x −1)

x=ξ −d
dxcx

x=ξ = 1
c
ξ
ξ −1 −c = 1
c = ξ −1.
1068

Thus the Green function is
G(x|ξ) =
(
(ξ −1)x
for x < ξ
ξ(x −1)
for x > ξ.
The Green function is plotted in Figure 21.3 for various values of ξ. The solution to y′′ = f(x) is
y(x) =
Z 1
0
G(x|ξ)f(ξ) dξ
y(x) = (x −1)
Z x
0
ξf(ξ) dξ + x
Z 1
x
(ξ −1)f(ξ) dξ.
0.5
1
-0.3
-0.2
-0.1
0.1
0.5
1
-0.3
-0.2
-0.1
0.1
0.5
1
-0.3
-0.2
-0.1
0.1
0.5
1
-0.3
-0.2
-0.1
0.1
Figure 21.3: Plot of G(x|0.05),G(x|0.25),G(x|0.5) and G(x|0.75).
Example 21.7.2 Solve the boundary value problem
y′′ = f(x),
y(0) = 1,
y(1) = 2.
In Example 21.7.1 we saw that the solution to
u′′ = f(x),
u(0) = u(1) = 0
1069

is
u(x) = (x −1)
Z x
0
ξf(ξ) dξ + x
Z 1
x
(ξ −1)f(ξ) dξ.
Now we have to ﬁnd the solution to
v′′ = 0,
v(0) = 1,
u(1) = 2.
The general solution is
v = c1 + c2x.
Applying the boundary conditions yields
v = 1 + x.
Thus the solution for y is
y = 1 + x + (x −1)
Z x
0
ξf(ξ) dξ + x
Z 1
x
(ξ −1)f( xi) dξ.
Example 21.7.3 Consider
y′′ = x,
y(0) = y(1) = 0.
Method 1.
Integrating the diﬀerential equation twice yields
y = 1
6x3 + c1x + c2.
Applying the boundary conditions, we ﬁnd that the solution is
y = 1
6(x3 −x).
1070

Method 2.
Using the Green function to ﬁnd the solution,
y = (x −1)
Z x
0
ξ2 dξ + x
Z 1
x
(ξ −1)ξ dξ
= (x −1)1
3x3 + x
1
3 −1
2 −1
3x3 + 1
2x2

y = 1
6(x3 −x).
Example 21.7.4 Find the solution to the diﬀerential equation
y′′ −y = sin x,
that is bounded for all x.
The Green function for this problem satisﬁes
G′′(x|ξ) −G(x|ξ) = δ(x −ξ).
The homogeneous solutions are y1 = ex, and y2 = e−x. The Green function has the form
G(x|ξ) =
(
c1 ex +c2 e−x
for x < ξ
d1 ex +d2 e−x
for x > ξ.
Since the solution must be bounded for all x, the Green function must also be bounded. Thus c2 = d1 = 0. The Green
function now has the form
G(x|ξ) =
(
c ex
for x < ξ
d e−x
for x > ξ.
Requiring that G(x|ξ) be continuous gives us the condition
c eξ = d e−ξ
→
d = c e2ξ .
1071

G(x|ξ) has a jump discontinuity of height 1 at x = ξ.
d
dxc e2ξ e−x

x=ξ
−d
dxc ex

x=ξ
= 1
−c e2ξ e−ξ −c eξ = 1
c = −1
2 e−ξ
The Green function is then
G(x|ξ) =
(
−1
2 ex−ξ
for x < ξ
−1
2 e−x+ξ
for x > ξ
G(x|ξ) = −1
2 e−|x−ξ| .
A plot of G(x|0) is given in Figure 21.4. The solution to y′′ −y = sin x is
y(x) =
Z ∞
−∞
−1
2 e−|x−ξ| sin ξ dξ
= −1
2
Z x
−∞
sin ξ ex−ξ dξ +
Z ∞
x
sin ξ e−x+ξ dξ

= −1
2(−sin x + cos x
2
+ −sin x + cos x
2
)
y = 1
2 sin x.
1072

-4
-2
2
4
-0.6
-0.4
-0.2
0.2
0.4
0.6
Figure 21.4: Plot of G(x|0).
21.7.1
Green Functions for Sturm-Liouville Problems
Consider the problem
L[y] = (p(x)y′)′ + q(x)y = f(x),
subject to
B1[y] = α1y(a) + α2y′(a) = 0,
B2[y] = β1y(b) + β2y′(b) = 0.
This is known as a Sturm-Liouville problem. Equations of this type often occur when solving partial diﬀerential equations.
The Green function associated with this problem satisﬁes
L[G(x|ξ)] = δ(x −ξ),
B1[G(x|ξ)] = B2[G(x|ξ)] = 0.
Let y1 and y2 be two non-trivial homogeneous solutions that satisfy the left and right boundary conditions, respectively.
L[y1] = 0,
B1[y1] = 0,
L[y2] = 0,
B2[y2] = 0.
1073

The Green function satisﬁes the homogeneous equation for x ̸= ξ and satisﬁes the homogeneous boundary conditions.
Thus it must have the following form.
G(x|ξ) =
(
c1(ξ)y1(x)
for a ≤x ≤ξ,
c2(ξ)y2(x)
for ξ ≤x ≤b,
Here c1 and c2 are unknown functions of ξ.
The ﬁrst constraint on c1 and c2 comes from the continuity condition.
G(ξ−|ξ) = G(ξ+|ξ)
c1(ξ)y1(ξ) = c2(ξ)y2(ξ)
We write the inhomogeneous equation in the standard form.
G′′(x|ξ) + p′
p G′(x|ξ) + q
pG(x|ξ) = δ(x −ξ)
p
The second constraint on c1 and c2 comes from the jump condition.
G′(ξ+|ξ) −G′(ξ−|ξ) =
1
p(ξ)
c2(ξ)y′
2(ξ) −c1(ξ)y′
1(ξ) =
1
p(ξ)
Now we have a system of equations to determine c1 and c2.
c1(ξ)y1(ξ) −c2(ξ)y2(ξ) = 0
c1(ξ)y′
1(ξ) −c2(ξ)y′
2(ξ) = −1
p(ξ)
We solve this system with Kramer’s rule.
c1(ξ) = −
y2(ξ)
p(ξ)(−W(ξ)),
c2(ξ) = −
y1(ξ)
p(ξ)(−W(ξ))
1074

Here W(x) is the Wronskian of y1(x) and y2(x). The Green function is
G(x|ξ) =
(y1(x)y2(ξ)
p(ξ)W(ξ)
for a ≤x ≤ξ,
y2(x)y1(ξ)
p(ξ)W(ξ)
for ξ ≤x ≤b.
The solution of the Sturm-Liouville problem is
y =
Z b
a
G(x|ξ)f(ξ) dξ.
Result 21.7.2 The problem
L[y] = (p(x)y′)′ + q(x)y = f(x),
subject to
B1[y] = α1y(a) + α2y′(a) = 0,
B2[y] = β1y(b) + β2y′(b) = 0.
has the Green function
G(x|ξ) =
(y1(x)y2(ξ)
p(ξ)W(ξ)
for a ≤x ≤ξ,
y2(x)y1(ξ)
p(ξ)W(ξ)
for ξ ≤x ≤b,
where y1 and y2 are non-trivial homogeneous solutions that satisfy B1[y1] = B2[y2] = 0, and
W(x) is the Wronskian of y1 and y2.
Example 21.7.5 Consider the equation
y′′ −y = f(x),
y(0) = y(1) = 0.
A set of solutions to the homogeneous equation is {ex, e−x}. Equivalently, one could use the set {cosh x, sinh x}. Note
that sinh x satisﬁes the left boundary condition and sinh(x −1) satisﬁes the right boundary condition. The Wronskian
1075

of these two homogeneous solutions is
W(x) =

sinh x
sinh(x −1)
cosh x
cosh(x −1)

= sinh x cosh(x −1) −cosh x sinh(x −1)
= 1
2[sinh(2x −1) + sinh(1)] −1
2[sinh(2x −1) −sinh(1)]
= sinh(1).
The Green function for the problem is then
G(x|ξ) =
(sinh x sinh(ξ−1)
sinh(1)
for 0 ≤x ≤ξ
sinh(x−1) sinh ξ
sinh(1)
for ξ ≤x ≤1.
The solution to the problem is
y = sinh(x −1)
sinh(1)
Z x
0
sinh(ξ)f(ξ) dξ + sinh(x)
sinh(1)
Z 1
x
sinh(ξ −1)f(ξ) dξ.
21.7.2
Initial Value Problems
Consider
L[y] = y′′ + p(x)y′ + q(x)y = f(x),
for a < x < b,
subject the the initial conditions
y(a) = γ1,
y′(a) = γ2.
The solution is y = u + v where
u′′ + p(x)u′ + q(x)u = f(x),
u(a) = 0,
u′(a) = 0,
1076

and
v′′ + p(x)v′ + q(x)v = 0,
v(a) = γ1,
v′(a) = γ2.
Since the Wronskian
W(x) = c exp

−
Z
p(x) dx

is non-vanishing, the solutions of the diﬀerential equation for v are linearly independent. Thus there is a unique solution
for v that satisﬁes the initial conditions.
The Green function for u satisﬁes
G′′(x|ξ) + p(x)G′(x|ξ) + q(x)G(x|ξ) = δ(x −ξ),
G(a|ξ) = 0,
G′(a|ξ) = 0.
The continuity and jump conditions are
G(ξ−|ξ) = G(ξ+|ξ),
G′(ξ−|ξ) + 1 = G′(ξ+|ξ).
Let u1 and u2 be two linearly independent solutions of the diﬀerential equation. For x < ξ, G(x|ξ) is a linear combination
of these solutions. Since the Wronskian is non-vanishing, only the trivial solution satisﬁes the homogeneous initial
conditions. The Green function must be
G(x|ξ) =
(
0
for x < ξ
uξ(x)
for x > ξ,
where uξ(x) is the linear combination of u1 and u2 that satisﬁes
uξ(ξ) = 0,
u′
ξ(ξ) = 1.
Note that the non-vanishing Wronskian ensures a unique solution for uξ. We can write the Green function in the form
G(x|ξ) = H(x −ξ)uξ(x).
1077

This is known as the causal solution. The solution for u is
u =
Z b
a
G(x|ξ)f(ξ) dξ
=
Z b
a
H(x −ξ)uξ(x)f(ξ) dξ
=
Z x
a
uξ(x)f(ξ) dξ
Now we have the solution for y,
y = v +
Z x
a
uξ(x)f(ξ) dξ.
Result 21.7.3 The solution of the problem
y′′ + p(x)y′ + q(x)y = f(x),
y(a) = γ1,
y′(a) = γ2,
is
y = yh +
Z x
a
yξ(x)f(ξ) dξ
where yh is the combination of the homogeneous solutions of the equation that satisfy the
initial conditions and yξ(x) is the linear combination of homogeneous solutions that satisfy
yξ(ξ) = 0, y′
ξ(ξ) = 1.
21.7.3
Problems with Unmixed Boundary Conditions
Consider
L[y] = y′′ + p(x)y′ + q(x)y = f(x),
for a < x < b,
1078

subject the the unmixed boundary conditions
α1y(a) + α2y′(a) = γ1,
β1y(b) + β2y′(b) = γ2.
The solution is y = u + v where
u′′ + p(x)u′ + q(x)u = f(x),
α1u(a) + α2u′(a) = 0,
β1u(b) + β2u′(b) = 0,
and
v′′ + p(x)v′ + q(x)v = 0,
α1v(a) + α2v′(a) = γ1,
β1v(b) + β2v′(b) = γ2.
The problem for v may have no solution, a unique solution or an inﬁnite number of solutions. We consider only the
case that there is a unique solution for v. In this case the homogeneous equation subject to homogeneous boundary
conditions has only the trivial solution.
The Green function for u satisﬁes
G′′(x|ξ) + p(x)G′(x|ξ) + q(x)G(x|ξ) = δ(x −ξ),
α1G(a|ξ) + α2G′(a|ξ) = 0,
β1G(b|ξ) + β2G′(b|ξ) = 0.
The continuity and jump conditions are
G(ξ−|ξ) = G(ξ+|ξ),
G′(ξ−|ξ) + 1 = G′(ξ+|ξ).
Let u1 and u2 be two solutions of the homogeneous equation that satisfy the left and right boundary conditions,
respectively. The non-vanishing of the Wronskian ensures that these solutions exist. Let W(x) denote the Wronskian
of u1 and u2. Since the homogeneous equation with homogeneous boundary conditions has only the trivial solution,
W(x) is nonzero on [a, b]. The Green function has the form
G(x|ξ) =
(
c1u1
for x < ξ,
c2u2
for x > ξ.
1079

The continuity and jump conditions for Green function gives us the equations
c1u1(ξ) −c2u2(ξ) = 0
c1u′
1(ξ) −c2u′
2(ξ) = −1.
Using Kramer’s rule, the solution is
c1 = u2(ξ)
W(ξ),
c2 = u1(ξ)
W(ξ).
Thus the Green function is
G(x|ξ) =
(u1(x)u2(ξ)
W(ξ)
for x < ξ,
u1(ξ)u2(x)
W(ξ)
for x > ξ.
The solution for u is
u =
Z b
a
G(x|ξ)f(ξ) dξ.
Thus if there is a unique solution for v, the solution for y is
y = v +
Z b
a
G(x|ξ)f(ξ) dξ.
1080

Result 21.7.4 Consider the problem
y′′ + p(x)y′ + q(x)y = f(x),
α1y(a) + α2y′(a) = γ1,
β1y(b) + β2y′(b) = γ2.
If the homogeneous diﬀerential equation subject to the inhomogeneous boundary conditions
has the unique solution yh, then the problem has the unique solution
y = yh +
Z b
a
G(x|ξ)f(ξ) dξ
where
G(x|ξ) =
(u1(x)u2(ξ)
W(ξ)
for x < ξ,
u1(ξ)u2(x)
W(ξ)
for x > ξ,
u1 and u2 are solutions of the homogeneous diﬀerential equation that satisfy the left and right
boundary conditions, respectively, and W(x) is the Wronskian of u1 and u2.
21.7.4
Problems with Mixed Boundary Conditions
Consider
L[y] = y′′ + p(x)y′ + q(x)y = f(x),
for a < x < b,
subject the the mixed boundary conditions
B1[y] = α11y(a) + α12y′(a) + β11y(b) + β12y′(b) = γ1,
B2[y] = α21y(a) + α22y′(a) + β21y(b) + β22y′(b) = γ2.
1081

The solution is y = u + v where
u′′ + p(x)u′ + q(x)u = f(x),
B1[u] = 0,
B2[u] = 0,
and
v′′ + p(x)v′ + q(x)v = 0,
B1[v] = γ1,
B2[v] = γ2.
The problem for v may have no solution, a unique solution or an inﬁnite number of solutions. Again we consider
only the case that there is a unique solution for v. In this case the homogeneous equation subject to homogeneous
boundary conditions has only the trivial solution.
Let y1 and y2 be two solutions of the homogeneous equation that satisfy the boundary conditions B1[y1] = 0 and
B2[y2] = 0. Since the completely homogeneous problem has no solutions, we know that B1[y2] and B2[y1] are nonzero.
The solution for v has the form
v = c1y1 + c2y2.
Applying the two boundary conditions yields
v =
γ2
B2[y1]y1 +
γ1
B1[y2]y2.
The Green function for u satisﬁes
G′′(x|ξ) + p(x)G′(x|ξ) + q(x)G(x|ξ) = δ(x −ξ),
B1[G] = 0,
B2[G] = 0.
The continuity and jump conditions are
G(ξ−|ξ) = G(ξ+|ξ),
G′(ξ−|ξ) + 1 = G′(ξ+|ξ).
We write the Green function as the sum of the causal solution and the two homogeneous solutions
G(x|ξ) = H(x −ξ)yξ(x) + c1y1(x) + c2y2(x)
1082

With this form, the continuity and jump conditions are automatically satisﬁed. Applying the boundary conditions yields
B1[G] = B1[H(x −ξ)yξ] + c2B1[y2] = 0,
B2[G] = B2[H(x −ξ)yξ] + c1B2[y1] = 0,
B1[G] = β11yξ(b) + β12y′
ξ(b) + c2B1[y2] = 0,
B2[G] = β21yξ(b) + β22y′
ξ(b) + c1B2[y1] = 0,
G(x|ξ) = H(x −ξ)yξ(x) −β21yξ(b) + β22y′
ξ(b)
B2[y1]
y1(x) −β11yξ(b) + β12y′
ξ(b)
B1[y2]
y2(x).
Note that the Green function is well deﬁned since B2[y1] and B1[y2] are nonzero. The solution for u is
u =
Z b
a
G(x|ξ)f(ξ) dξ.
Thus if there is a unique solution for v, the solution for y is
y =
Z b
a
G(x|ξ)f(ξ) dξ +
γ2
B2[y1]y1 +
γ1
B1[y2]y2.
1083

Result 21.7.5 Consider the problem
y′′ + p(x)y′ + q(x)y = f(x),
B1[y] = α11y(a) + α12y′(a) + β11y(b) + β12y′(b) = γ1,
B2[y] = α21y(a) + α22y′(a) + β21y(b) + β22y′(b) = γ2.
If the homogeneous diﬀerential equation subject to the homogeneous boundary conditions
has no solution, then the problem has the unique solution
y =
Z b
a
G(x|ξ)f(ξ) dξ +
γ2
B2[y1]y1 +
γ1
B1[y2]y2,
where
G(x|ξ) = H(x −ξ)yξ(x) −
β21yξ(b) + β22y′
ξ(b)
B2[y1]
y1(x)
−
β11yξ(b) + β12y′
ξ(b)
B1[y2]
y2(x),
y1 and y2 are solutions of the homogeneous diﬀerential equation that satisfy the ﬁrst and sec-
ond boundary conditions, respectively, and yξ(x) is the solution of the homogeneous equation
that satisﬁes yξ(ξ) = 0, y′
ξ(ξ) = 1.
1084

21.8
Green Functions for Higher Order Problems
Consider the nth order diﬀerential equation
L[y] = y(n) + pn−1(x)y(n−1) + · · · + p1(x)y′ + p0y = f(x)
on a < x < b,
subject to the n independent boundary conditions
Bj[y] = γj
where the boundary conditions are of the form
B[y] ≡
n−1
X
k=0
αky(k)(a) +
n−1
X
k=0
βky(k)(b).
We assume that the coeﬃcient functions in the diﬀerential equation are continuous on [a, b]. The solution is y = u + v
where u and v satisfy
L[u] = f(x),
with
Bj[u] = 0,
and
L[v] = 0,
with
Bj[v] = γj
From Result 21.5.3, we know that if the completely homogeneous problem
L[w] = 0,
with
Bj[w] = 0,
has only the trivial solution, then the solution for y exists and is unique. We will construct this solution using Green
functions.
1085

First we consider the problem for v. Let {y1, . . . , yn} be a set of linearly independent solutions. The solution for v
has the form
v = c1y1 + · · · + cnyn
where the constants are determined by the matrix equation





B1[y1]
B1[y2]
· · ·
B1[yn]
B2[y1]
B2[y2]
· · ·
B2[yn]
...
...
...
...
Bn[y1]
Bn[y2]
· · ·
Bn[yn]










c1
c2...
cn




=





γ1
γ2...
γn




.
To solve the problem for u we consider the Green function satisfying
L[G(x|ξ)] = δ(x −ξ),
with
Bj[G] = 0.
Let yξ(x) be the linear combination of the homogeneous solutions that satisfy the conditions
yξ(ξ) = 0
y′
ξ(ξ) = 0
...
= ...
y(n−2)
ξ
(ξ) = 0
y(n−1)
ξ
(ξ) = 1.
The causal solution is then
yc(x) = H(x −ξ)yξ(x).
The Green function has the form
G(x|ξ) = H(x −ξ)yξ(x) + d1y1(x) + · · · + dnyn(x)
1086

The constants are determined by the matrix equation





B1[y1]
B1[y2]
· · ·
B1[yn]
B2[y1]
B2[y2]
· · ·
B2[yn]
...
...
...
...
Bn[y1]
Bn[y2]
· · ·
Bn[yn]










d1
d2...
dn




=





−B1[H(x −ξ)yξ(x)]
−B2[H(x −ξ)yξ(x)]
...
−Bn[H(x −ξ)yξ(x)]




.
The solution for u then is
u =
Z b
a
G(x|ξ)f(ξ) dξ.
Result 21.8.1 Consider the nth order diﬀerential equation
L[y] = y(n) + pn−1(x)y(n−1) + · · · + p1(x)y′ + p0y = f(x)
on a < x < b,
subject to the n independent boundary conditions
Bj[y] = γj
If the homogeneous diﬀerential equation subject to the homogeneous boundary conditions
has only the trivial solution, then the problem has the unique solution
y =
Z b
a
G(x|ξ)f(ξ) dξ + c1y1 + · · · cnyn
where
G(x|ξ) = H(x −ξ)yξ(x) + d1y1(x) + · · · + dnyn(x),
{y1, . . . , yn} is a set of solutions of the homogeneous diﬀerential equation, and the constants
cj and dj can be determined by solving sets of linear equations.
1087

Example 21.8.1 Consider the problem
y′′′ −y′′ + y′ −y = f(x),
y(0) = 1,
y′(0) = 2,
y(1) = 3.
The completely homogeneous associated problem is
w′′′ −w′′ + w′ −w = 0,
w(0) = w′(0) = w(1) = 0.
The solution of the diﬀerential equation is
w = c1 cos x + c2 sin x + c2 ex .
The boundary conditions give us the equation


1
0
1
0
1
1
cos 1
sin 1
e




c1
c2
c3

=


0
0
0

.
The determinant of the matrix is e −cos 1 −sin 1 ̸= 0. Thus the homogeneous problem has only the trivial solution
and the inhomogeneous problem has a unique solution.
We separate the inhomogeneous problem into the two problems
u′′′ −u′′ + u′ −u = f(x),
u(0) = u′(0) = u(1) = 0,
v′′′ −v′′ + v′ −v = 0,
v(0) = 1,
v′(0) = 2,
v(1) = 3,
First we solve the problem for v. The solution of the diﬀerential equation is
v = c1 cos x + c2 sin x + c2 ex .
The boundary conditions yields the equation


1
0
1
0
1
1
cos 1
sin 1
e




c1
c2
c3

=


1
2
3

.
1088

The solution for v is
v =
1
e −cos 1 −sin 1

(e + sin 1 −3) cos x + (2e −cos 1 −3) sin x + (3 −cos 1 −2 sin 1) ex 
.
Now we ﬁnd the Green function for the problem in u. The causal solution is
H(x −ξ)uξ(x) = H(x −ξ)1
2

(sin ξ −cos ξ) cos x −(sin ξ + cos ξ) sin ξ + e−ξ ex 
,
H(x −ξ)uξ(x) = 1
2H(x −ξ)
 ex−ξ −cos(x −ξ) −sin(x −ξ)

.
The Green function has the form
G(x|ξ) = H(x −ξ)uξ(x) + c1 cos x + c2 sin x + c3 ex .
The constants are determined by the three conditions

c1 cos x + c2 sin x + c3 ex 
x=0 = 0,
 ∂
∂x (c1 cos x + c2 sin x + c3 ex)

x=0
= 0,

uξ(x) + c1 cos x + c2 sin x + c3 ex 
x=1 = 0.
The Green function is
G(x|ξ) = 1
2H(x −ξ)
 ex−ξ −cos(x −ξ) −sin(x −ξ)

+ cos(1 −ξ) + sin(1 −ξ) −e1−ξ
2(cos 1 + sin 1 −e)

cos x + sin x −ex 
The solution for v is
v =
Z 1
0
G(x|ξ)f(ξ) dξ.
Thus the solution for y is
1089

y =
Z 1
0
G(x|ξ)f(ξ) dξ +
1
e −cos 1 −sin 1

(e + sin 1 −3) cos x
+ (2e −cos 1 −3) sin x + (3 −cos 1 −2 sin 1) ex 
.
21.9
Fredholm Alternative Theorem
Orthogonality.
Two real vectors, u and v are orthogonal if u · v = 0. Consider two functions, u(x) and v(x),
deﬁned in [a, b]. The dot product in vector space is analogous to the integral
Z b
a
u(x)v(x) dx
in function space. Thus two real functions are orthogonal if
Z b
a
u(x)v(x) dx = 0.
Consider the nth order linear inhomogeneous diﬀerential equation
L[y] = f(x)
on [a, b],
subject to the linear inhomogeneous boundary conditions
Bj[y] = 0,
for j = 1, 2, . . . n.
The Fredholm alternative theorem tells us if the problem has a unique solution, an inﬁnite number of solutions, or
no solution. Before presenting the theorem, we will consider a few motivating examples.
1090

No Nontrivial Homogeneous Solutions.
In the section on Green functions we showed that if the completely
homogeneous problem has only the trivial solution then the inhomogeneous problem has a unique solution.
Nontrivial Homogeneous Solutions Exist.
If there are nonzero solutions to the homogeneous problem L[y] = 0
that satisfy the homogeneous boundary conditions Bj[y] = 0 then the inhomogeneous problem L[y] = f(x) subject to
the same boundary conditions either has no solution or an inﬁnite number of solutions.
Suppose there is a particular solution yp that satisﬁes the boundary conditions. If there is a solution yh to the
homogeneous equation that satisﬁes the boundary conditions then there will be an inﬁnite number of solutions since
yp + cyh is also a particular solution.
The question now remains: Given that there are homogeneous solutions that satisfy the boundary conditions, how
do we know if a particular solution that satisﬁes the boundary conditions exists? Before we address this question we
will consider a few examples.
Example 21.9.1 Consider the problem
y′′ + y = cos x,
y(0) = y(π) = 0.
The two homogeneous solutions of the diﬀerential equation are
y1 = cos x,
and
y2 = sin x.
y2 = sin x satisﬁes the boundary conditions. Thus we know that there are either no solutions or an inﬁnite number of
1091

solutions. A particular solution is
yp = −cos x
Z cos x sin x
1
dx + sin x
Z cos2 x
1
dx
= −cos x
Z 1
2 sin(2x) dx + sin x
Z 1
2 + 1
2 cos(2x)

dx
= 1
4 cos x cos(2x) + sin x
1
2x + 1
4 sin(2x)

= 1
2x sin x + 1
4

cos x cos(2x) + sin x sin(2x)

= 1
2x sin x + 1
4 cos x
The general solution is
y = 1
2x sin x + c1 cos x + c2 sin x.
Applying the two boundary conditions yields
y = 1
2x sin x + c sin x.
Thus there are an inﬁnite number of solutions.
Example 21.9.2 Consider the diﬀerential equation
y′′ + y = sin x,
y(0) = y(π) = 0.
The general solution is
y = −1
2x cos x + c1 cos x + c2 sin x.
1092

Applying the boundary conditions,
y(0) = 0
→
c1 = 0
y(π) = 0
→
−1
2π cos(π) + c2 sin(π) = 0
→
π
2 = 0.
Since this equation has no solution, there are no solutions to the inhomogeneous problem.
In both of the above examples there is a homogeneous solution y = sin x that satisﬁes the boundary conditions.
In Example 21.9.1, the inhomogeneous term is cos x and there are an inﬁnite number of solutions. In Example 21.9.2,
the inhomogeneity is sin x and there are no solutions. In general, if the inhomogeneous term is orthogonal to all the
homogeneous solutions that satisfy the boundary conditions then there are an inﬁnite number of solutions. If not, there
are no inhomogeneous solutions.
1093

Result 21.9.1 Fredholm Alternative Theorem. Consider the nth order inhomogeneous
problem
L[y] = f(x)
on
[a, b]
subject to
Bj[y] = 0
for
j = 1, 2, . . . , n,
and the associated homogeneous problem
L[y] = 0
on
[a, b]
subject to
Bj[y] = 0
for
j = 1, 2, . . . , n.
If the homogeneous problem has only the trivial solution then the inhomogeneous problem has
a unique solution. If the homogeneous problem has m independent solutions, {y1, y2, . . . , ym},
then there are two possibilities:
• If f(x) is orthogonal to each of the homogeneous solutions then there are an inﬁnite
number of solutions of the form
y = yp +
m
X
j=1
cjyj.
• If f(x) is not orthogonal to each of the homogeneous solutions then there are no inho-
mogeneous solutions.
Example 21.9.3 Consider the problem
y′′ + y = cos 2x,
y(0) = 1,
y(π) = 2.
cos x and sin x are two linearly independent solutions to the homogeneous equation. sin x satisﬁes the homogeneous
boundary conditions. Thus there are either an inﬁnite number of solutions, or no solution.
1094

To transform this problem to one with homogeneous boundary conditions, we note that g(x) = x
π + 1 and make
the change of variables y = u + g to obtain
u′′ + u = cos 2x −x
π −1,
y(0) = 0,
y(π) = 0.
Since cos 2x −x
π −1 is not orthogonal to sin x, there is no solution to the inhomogeneous problem.
To check this, the general solution is
y = −1
3 cos 2x + c1 cos x + c2 sin x.
Applying the boundary conditions,
y(0) = 1
→
c1 = 4
3
y(π) = 2
→
−1
3 −4
3 = 2.
Thus we see that the right boundary condition cannot be satisﬁed.
Example 21.9.4 Consider
y′′ + y = cos 2x,
y′(0) = y(π) = 1.
There are no solutions to the homogeneous equation that satisfy the homogeneous boundary conditions. To check this,
note that all solutions of the homogeneous equation have the form uh = c1 cos x + c2 sin x.
u′
h(0) = 0
→
c2 = 0
uh(π) = 0
→
c1 = 0.
From the Fredholm Alternative Theorem we see that the inhomogeneous problem has a unique solution.
To ﬁnd the solution, start with
y = −1
3 cos 2x + c1 cos x + c2 sin x.
1095

y′(0) = 1
→
c2 = 1
y(π) = 1
→
−1
3 −c1 = 1
Thus the solution is
y = −1
3 cos 2x −4
3 cos x + sin x.
Example 21.9.5 Consider
y′′ + y = cos 2x,
y(0) = 2
3,
y(π) = −4
3.
cos x and sin x satisfy the homogeneous diﬀerential equation. sin x satisﬁes the homogeneous boundary conditions.
Since g(x) = cos x −1/3 satisﬁes the boundary conditions, the substitution y = u + g yields
u′′ + u = cos 2x + 1
3,
y(0) = 0,
y(π) = 0.
Now we check if sin x is orthogonal to cos 2x + 1
3.
Z π
0
sin x

cos 2x + 1
3

dx =
Z π
0
1
2 sin 3x −1
2 sin x + 1
3 sin x dx
=

−1
6 cos 3x + 1
6 cos x
π
0
= 0
Since sin x is orthogonal to the inhomogeneity, there are an inﬁnite number of solutions to the problem for u, (and
hence the problem for y).
As a check, then general solution for y is
y = −1
3 cos 2x + c1 cos x + c2 sin x.
1096

Applying the boundary conditions,
y(0) = 2
3
→
c1 = 1
y(π) = −4
3
→
−4
3 = −4
3.
Thus we see that c2 is arbitrary. There are an inﬁnite number of solutions of the form
y = −1
3 cos 2x + cos x + c sin x.
1097

21.10
Exercises
Undetermined Coeﬃcients
Exercise 21.1 (mathematica/ode/inhomogeneous/undetermined.nb)
Find the general solution of the following equations.
1. y′′ + 2y′ + 5y = 3 sin(2t)
2. 2y′′ + 3y′ + y = t2 + 3 sin(t)
Hint, Solution
Exercise 21.2 (mathematica/ode/inhomogeneous/undetermined.nb)
Find the solution of each one of the following initial value problems.
1. y′′ −2y′ + y = t et +4, y(0) = 1, y′(0) = 1
2. y′′ + 2y′ + 5y = 4 e−t cos(2t), y(0) = 1, y′(0) = 0
Hint, Solution
Variation of Parameters
Exercise 21.3 (mathematica/ode/inhomogeneous/variation.nb)
Use the method of variation of parameters to ﬁnd a particular solution of the given diﬀerential equation.
1. y′′ −5y′ + 6y = 2 et
2. y′′ + y = tan(t), 0 < t < π/2
3. y′′ −5y′ + 6y = g(t), for a given function g.
Hint, Solution
1098

Exercise 21.4 (mathematica/ode/inhomogeneous/variation.nb)
Solve
y′′(x) + y(x) = x,
y(0) = 1, y′(0) = 0.
Hint, Solution
Exercise 21.5 (mathematica/ode/inhomogeneous/variation.nb)
Solve
x2y′′(x) −xy′(x) + y(x) = x.
Hint, Solution
Exercise 21.6 (mathematica/ode/inhomogeneous/variation.nb)
1. Find the general solution of y′′ + y = ex.
2. Solve y′′ + λ2y = sin x, y(0) = y′(0) = 0. λ is an arbitrary real constant. Is there anything special about λ = 1?
Hint, Solution
Exercise 21.7 (mathematica/ode/inhomogeneous/variation.nb)
Consider the problem of solving the initial value problem
y′′ + y = g(t),
y(0) = 0,
y′(0) = 0.
1. Show that the general solution of y′′ + y = g(t) is
y(t) =

c1 −
Z t
a
g(τ) sin τ dτ

cos t +

c2 +
Z t
b
g(τ) cos τ dτ

sin t,
where c1 and c2 are arbitrary constants and a and b are any conveniently chosen points.
2. Using the result of part (a) show that the solution satisfying the initial conditions y(0) = 0 and y′(0) = 0 is given
by
y(t) =
Z t
0
g(τ) sin(t −τ) dτ.
1099

Notice that this equation gives a formula for computing the solution of the original initial value problem for any
given inhomogeneous term g(t). The integral is referred to as the convolution of g(t) with sin t.
3. Use the result of part (b) to solve the initial value problem,
y′′ + y = sin(λt),
y(0) = 0,
y′(0) = 0,
where λ is a real constant. How does the solution for λ = 1 diﬀer from that for λ ̸= 1? The λ = 1 case provides
an example of resonant forcing. Plot the solution for resonant and non-resonant forcing.
Hint, Solution
Exercise 21.8
Find the variation of parameters solution for the third order diﬀerential equation
y′′′ + p2(x)y′′ + p1(x)y′ + p0(x)y = f(x).
Hint, Solution
Green Functions
Exercise 21.9
Use a Green function to solve
y′′ = f(x),
y(−∞) = y′(−∞) = 0.
Verify the the solution satisﬁes the diﬀerential equation.
Hint, Solution
Exercise 21.10
Solve the initial value problem
y′′ + 1
xy′ −1
x2y = x2,
y(0) = 0,
y′(0) = 1.
First use variation of parameters, and then solve the problem with a Green function.
Hint, Solution
1100

Exercise 21.11
What are the continuity conditions at x = ξ for the Green function for the problem
y′′′ + p2(x)y′′ + p1(x)y′ + p0(x)y = f(x).
Hint, Solution
Exercise 21.12
Use variation of parameters and Green functions to solve
x2y′′ −2xy′ + 2y = e−x,
y(1) = 0,
y′(1) = 1.
Hint, Solution
Exercise 21.13
Find the Green function for
y′′ −y = f(x),
y′(0) = y(1) = 0.
Hint, Solution
Exercise 21.14
Find the Green function for
y′′ −y = f(x),
y(0) = y(∞) = 0.
Hint, Solution
Exercise 21.15
Find the Green function for each of the following:
a) xu′′ + u′ = f(x), u(0+) bounded, u(1) = 0.
b) u′′ −u = f(x), u(−a) = u(a) = 0.
c) u′′ −u = f(x), u(x) bounded as |x| →∞.
1101

d) Show that the Green function for (b) approaches that for (c) as a →∞.
Hint, Solution
Exercise 21.16
1. For what values of λ does the problem
y′′ + λy = f(x),
y(0) = y(π) = 0,
(21.5)
have a unique solution? Find the Green functions for these cases.
2. For what values of α does the problem
y′′ + 9y = 1 + αx,
y(0) = y(π) = 0,
have a solution? Find the solution.
3. For λ = n2, n ∈Z+ state in general the conditions on f in Equation 21.5 so that a solution will exist. What is
the appropriate modiﬁed Green function (in terms of eigenfunctions)?
Hint, Solution
Exercise 21.17
Show that the inhomogeneous boundary value problem:
Lu ≡(pu′)′ + qu = f(x),
a < x < b,
u(a) = α,
u(b) = β
has the solution:
u(x) =
Z b
a
g(x; ξ)f(ξ) dξ −αp(a)gξ(x; a) + βp(b)gξ(x; b).
Hint, Solution
Exercise 21.18
The Green function for
u′′ −k2u = f(x),
−∞< x < ∞
1102

subject to |u(±∞)| < ∞is
G(x; ξ) = −1
2k e−k|x−ξ| .
(We assume that k > 0.) Use the image method to ﬁnd the Green function for the same equation on the semi-inﬁnite
interval 0 < x < ∞satisfying the boundary conditions,
i)
u(0) = 0
|u(∞)| < ∞,
ii)
u′(0) = 0
|u(∞)| < ∞.
Express these results in simpliﬁed forms without absolute values.
Hint, Solution
Exercise 21.19
1. Determine the Green function for solving:
y′′ −a2y = f(x),
y(0) = y′(L) = 0.
2. Take the limit as L →∞to ﬁnd the Green function on (0, ∞) for the boundary conditions: y(0) = 0, y′(∞) = 0.
We assume here that a > 0. Use the limiting Green function to solve:
y′′ −a2y = e−x,
y(0) = 0,
y′(∞) = 0.
Check that your solution satisﬁes all the conditions of the problem.
Hint, Solution
1103

21.11
Hints
Undetermined Coeﬃcients
Hint 21.1
Hint 21.2
Variation of Parameters
Hint 21.3
Hint 21.4
Hint 21.5
Hint 21.6
Hint 21.7
Hint 21.8
Look for a particular solution of the form
yp = u1y1 + u2y2 + u3y3,
1104

where the yj’s are homogeneous solutions. Impose the constraints
u′
1y1 + u′
2y2 + u′
3y3 = 0
u′
1y′
1 + u′
2y′
2 + u′
3y′
3 = 0.
To avoid some messy algebra when solving for u′
j, use Kramer’s rule.
Green Functions
Hint 21.9
Hint 21.10
Hint 21.11
Hint 21.12
Hint 21.13
cosh(x) and sinh(x −1) are homogeneous solutions that satisfy the left and right boundary conditions, respectively.
Hint 21.14
sinh(x) and e−x are homogeneous solutions that satisfy the left and right boundary conditions, respectively.
Hint 21.15
The Green function for the diﬀerential equation
L[y] ≡d
dx(p(x)y′) + q(x)y = f(x),
1105

subject to unmixed, homogeneous boundary conditions is
G(x|ξ) = y1(x<)y2(x>)
p(ξ)W(ξ)
,
G(x|ξ) =
(y1(x)y2(ξ)
p(ξ)W(ξ)
for a ≤x ≤ξ,
y1(ξ)y2(x)
p(ξ)W(ξ)
for ξ ≤x ≤b,
where y1 and y2 are homogeneous solutions that satisfy the left and right boundary conditions, respectively.
Recall that if y(x) is a solution of a homogeneous, constant coeﬃcient diﬀerential equation then y(x + c) is also a
solution.
Hint 21.16
The problem has a Green function if and only if the inhomogeneous problem has a unique solution. The inhomogeneous
problem has a unique solution if and only if the homogeneous problem has only the trivial solution.
Hint 21.17
Show that gξ(x; a) and gξ(x; b) are solutions of the homogeneous diﬀerential equation. Determine the value of these
solutions at the boundary.
Hint 21.18
Hint 21.19
1106

21.12
Solutions
Undetermined Coeﬃcients
Solution 21.1
1. We consider
y′′ + 2y′ + 5y = 3 sin(2t).
We ﬁrst ﬁnd the homogeneous solution with the substitition y = eλt.
λ2 + 2λ + 5 = 0
λ = −1 ± 2i
The homogeneous solution is
yh = c1 e−t cos(2t) + c2 e−t sin(2t).
We guess a particular solution of the form
yp = a cos(2t) + b sin(2t).
We substitute this into the diﬀerential equation to determine the coeﬃcients.
y′′
p + 2y′
p + 5yp = 3 sin(2t)
−4a cos(2t) −4b sin(2t) −4a sin(2t) + 4b sin(2t) + 5a cos(2t) + 5b sin(2t) = −3 sin(2t)
(a + 4b) cos(2t) + (−3 −4a + b) sin(2t) = 0
a + 4b = 0,
−4a + b = 3
a = −12
17,
b = 3
17
A particular solution is
yp = 3
17(sin(2t) −4 cos(2t)).
1107

The general solution of the diﬀerential equation is
y = c1 e−t cos(2t) + c2 e−t sin(2t) + 3
17(sin(2t) −4 cos(2t)).
2. We consider
2y′′ + 3y′ + y = t2 + 3 sin(t)
We ﬁrst ﬁnd the homogeneous solution with the substitition y = eλt.
2λ2 + 3λ + 1 = 0
λ = {−1, −1/2}
The homogeneous solution is
yh = c1 e−t +c2 e−t/2 .
We guess a particular solution of the form
yp = at2 + bt + c + d cos(t) + e sin(t).
We substitute this into the diﬀerential equation to determine the coeﬃcients.
2y′′
p + 3y′
p + yp = t2 + 3 sin(t)
2(2a −d cos(t) −e sin(t)) + 3(2at + b −d sin(t) + e cos(t))
+ at2 + bt + c + d cos(t) + e sin(t) = t2 + 3 sin(t)
(a −1)t2 + (6a + b)t + (4a + 3b + c) + (−d + 3e) cos(t) −(3 + 3d + e) sin(t) = 0
a −1 = 0,
6a + b = 0,
4a + 3b + c = 0,
−d + 3e = 0,
3 + 3d + e = 0
a = 1,
b = −6,
c = 14,
d = −9
10,
e = −3
10
1108

A particular solution is
yp = t2 −6t + 14 −3
10(3 cos(t) + sin(t)).
The general solution of the diﬀerential equation is
y = c1 e−t +c2 e−t/2 +t2 −6t + 14 −3
10(3 cos(t) + sin(t)).
Solution 21.2
1. We consider the problem
y′′ −2y′ + y = t et +4,
y(0) = 1,
y′(0) = 1.
First we solve the homogeneous equation with the substitution y = eλt.
λ2 −2λ + 1 = 0
(λ −1)2 = 0
λ = 1
The homogeneous solution is
yh = c1 et +c2t et .
We guess a particular solution of the form
yp = at3 et +bt2 et +4.
We substitute this into the inhomogeneous diﬀerential equation to determine the coeﬃcients.
y′′
p −2y′
p + yp = t et +4
(a(t3 + 6t2 + 6t) + b(t2 + 4t + 2)) et −2(a(t2 + 3t) + b(t + 2)) et at3 et +bt2 et +4 = t et +4
(6a −1)t + 2b = 0
6a −1 = 0,
2b = 0
a = 1
6,
b = 0
1109

A particular solution is
yp = t3
6 et +4.
The general solution of the diﬀerential equation is
y = c1 et +c2t et +t3
6 et +4.
We use the initial conditions to determine the constants of integration.
y(0) = 1,
y′(0) = 1
c1 + 4 = 1,
c1 + c2 = 1
c1 = −3,
c2 = 4
The solution of the initial value problem is
y =
t3
6 + 4t −3

et +4.
2. We consider the problem
y′′ + 2y′ + 5y = 4 e−t cos(2t),
y(0) = 1,
y′(0) = 0.
First we solve the homogeneous equation with the substitution y = eλt.
λ2 + 2λ + 5 = 0
λ = −1 ±
√
1 −5
λ = −1 ± ı2
The homogeneous solution is
yh = c1 e−t cos(2t) + c2 e−t sin(2t).
1110

We guess a particular solution of the form
yp = t e−t(a cos(2t) + b sin(2t))
We substitute this into the inhomogeneous diﬀerential equation to determine the coeﬃcients.
y′′
p + 2y′
p + 5yp = 4 e−t cos(2t)
e−t((−(2 + 3t)a + 4(1 −t)b) cos(2t) + (4(t −1)a −(2 + 3t)b) sin(2t))
+ 2 e−t(((1 −t)a + 2tb) cos(2t) + (−2ta + (1 −t)b) sin(2t))
+ 5(e−t(ta cos(2t) + tb sin(2t))) = 4 e−t cos(2t)
4(b −1) cos(2t) −4a sin(2t) = 0
a = 0,
b = 1
A particular solution is
yp = t e−t sin(2t).
The general solution of the diﬀerential equation is
y = c1 e−t cos(2t) + c2 e−t sin(2t) + t e−t sin(2t).
We use the initial conditions to determine the constants of integration.
y(0) = 1,
y′(0) = 0
c1 = 1,
−c1 + 2c2 = 0
c1 = 1,
c2 = 1
2
The solution of the initial value problem is
y = 1
2 e−t (2 cos(2t) + (2t + 1) sin(2t)) .
1111

Variation of Parameters
Solution 21.3
1. We consider the equation
y′′ −5y′ + 6y = 2 et .
We ﬁnd homogeneous solutions with the substitution y = eλt.
λ2 −5λ + 6 = 0
λ = {2, 3}
The homogeneous solutions are
y1 = e2t,
y2 = e3t .
We compute the Wronskian of these solutions.
W(t) =

e2t
e3t
2 e2t
3 e3t
 = e5t
We ﬁnd a particular solution with variation of parameters.
yp = −e2t
Z 2 et e3t
e5t
dt + e3t
Z 2 et e2t
e5t
dt
= −2 e2t
Z
e−t dt + 2 e3t
Z
e−2t dt
= 2 et −et
yp = et
2. We consider the equation
y′′ + y = tan(t),
0 < t < π
2 .
1112

We ﬁnd homogeneous solutions with the substitution y = eλt.
λ2 + 1 = 0
λ = ±i
The homogeneous solutions are
y1 = cos(t),
y2 = sin(t).
We compute the Wronskian of these solutions.
W(t) =

cos(t)
sin(t)
−sin(t)
cos(t)
 = cos2(t) + sin2(t) = 1
We ﬁnd a particular solution with variation of parameters.
yp = −cos(t)
Z
tan(t) sin(t) dt + sin(t)
Z
tan(t) cos(t) dt
= −cos(t)
Z sin2(t)
cos(t) dt + sin(t)
Z
sin(t) dt
= cos(t)

ln
cos(t/2) −sin(t/2)
cos(t/2) + sin(t/2) + sin(t)

−sin(t) cos(t)
yp = cos(t) ln
cos(t/2) −sin(t/2)
cos(t/2) + sin(t/2)

3. We consider the equation
y′′ −5y′ + 6y = g(t).
The homogeneous solutions are
y1 = e2t,
y2 = e3t .
1113

The Wronskian of these solutions is W(t) = e5t. We ﬁnd a particular solution with variation of parameters.
yp = −e2t
Z g(t) e3t
e5t
dt + e3t
Z g(t) e2t
e5t
dt
yp = −e2t
Z
g(t) e−2t dt + e3t
Z
g(t) e−3t dt
Solution 21.4
Solve
y′′(x) + y(x) = x,
y(0) = 1, y′(0) = 0.
The solutions of the homogeneous equation are
y1(x) = cos x,
y2(x) = sin x.
The Wronskian of these solutions is
W[cos x, sin x] =

cos x
sin x
−sin x
cos x

= cos2 x + sin2 x
= 1.
The variation of parameters solution for the particular solution is
yp = −cos x
Z
x sin x dx + sin x
Z
x cos x dx
= −cos x

−x cos x +
Z
cos x dx

+ sin x

x sin x −
Z
sin x dx

= −cos x (−x cos x + sin x) + sin x (x sin x + cos x)
= x cos2 x −cos x sin x + x sin2 x + cos x sin x
= x
1114

The general solution of the diﬀerential equation is thus
y = c1 cos x + c2 sin x + x.
Applying the two initial conditions gives us the equations
c1 = 1,
c2 + 1 = 0.
The solution subject to the initial conditions is
y = cos x −sin x + x.
Solution 21.5
Solve
x2y′′(x) −xy′(x) + y(x) = x.
The homogeneous equation is
x2y′′(x) −xy′(x) + y(x) = 0.
Substituting y = xλ into the homogeneous diﬀerential equation yields
x2λ(λ −1)xλ−2 −xλxλ + xλ = 0
λ2 −2λ + 1 = 0
(λ −1)2 = 0
λ = 1.
The homogeneous solutions are
y1 = x,
y2 = x log x.
The Wronskian of the homogeneous solutions is
W[x, x log x] =

x
x log x
1
1 + log x

= x + x log x −x log x
= x.
1115

Writing the inhomogeneous equation in the standard form:
y′′(x) −1
xy′(x) + 1
x2y(x) = 1
x.
Using variation of parameters to ﬁnd the particular solution,
yp = −x
Z log x
x
dx + x log x
Z 1
x dx
= −x1
2 log2 x + x log x log x
= 1
2x log2 x.
Thus the general solution of the inhomogeneous diﬀerential equation is
y = c1x + c2x log x + 1
2x log2 x.
Solution 21.6
1. First we ﬁnd the homogeneous solutions. We substitute y = eλx into the homogeneous diﬀerential equation.
y′′ + y = 0
λ2 + 1 = 0
λ = ±ı
y =
eıx, e−ıx	
We can also write the solutions in terms of real-valued functions.
y = {cos x, sin x}
1116

The Wronskian of the homogeneous solutions is
W[cos x, sin x] =

cos x
sin x
−sin x
cos x
 = cos2 x + sin2 x = 1.
We obtain a particular solution with the variation of parameters formula.
yp = −cos x
Z
ex sin x dx + sin x
Z
ex cos x dx
yp = −cos x1
2 ex(sin x −cos x) + sin x1
2 ex(sin x + cos x)
yp = 1
2 ex
The general solution is the particular solution plus a linear combination of the homogeneous solutions.
y = 1
2 ex + cos x + sin x
2.
y′′ + λ2y = sin x,
y(0) = y′(0) = 0
Assume that λ is positive. First we ﬁnd the homogeneous solutions by substituting y = eαx into the homogeneous
diﬀerential equation.
y′′ + λ2y = 0
α2 + λ2 = 0
α = ±ıλ
y =
eıλx, e−ıλx	
y = {cos(λx), sin(λx)}
1117

The Wronskian of these homogeneous solution is
W[cos(λx), sin(λx)] =

cos(λx)
sin(λx)
−λ sin(λx)
λ cos(λx)
 = λ cos2(λx) + λ sin2(λx) = λ.
We obtain a particular solution with the variation of parameters formula.
yp = −cos(λx)
Z sin(λx) sin x
λ
dx + sin(λx)
Z cos(λx) sin x
λ
dx
We evaluate the integrals for λ ̸= 1.
yp = −cos(λx)cos(x) sin(λx) −λ sin x cos(λx)
λ(λ2 −1)
+ sin(λx)cos(x) cos(λx) + λ sin x sin(λx)
λ(λ2 −1)
yp = sin x
λ2 −1
The general solution for λ ̸= 1 is
y = sin x
λ2 −1 + c1 cos(λx) + c2 sin(λx).
The initial conditions give us the constraints:
c1 = 0,
1
λ2 −1 + λc2 = 0,
For λ ̸= 1, (non-resonant forcing), the solution subject to the initial conditions is
y = λ sin(x) −sin(λx)
λ(λ2 −1)
.
1118

Now consider the case λ = 1. We obtain a particular solution with the variation of parameters formula.
yp = −cos(x)
Z
sin2(x) dx + sin(x)
Z
cos(x) sin x dx
yp = −cos(x)1
2(x −cos(x) sin(x)) + sin(x)

−1
2 cos2(x)

yp = −1
2x cos(x)
The general solution for λ = 1 is
y = −1
2x cos(x) + c1 cos(x) + c2 sin(x).
The initial conditions give us the constraints:
c1 = 0
−1
2 + c2 = 0
For λ = 1, (resonant forcing), the solution subject to the initial conditions is
y = 1
2(sin(x) −x cos x).
Solution 21.7
1. A set of linearly independent, homogeneous solutions is {cos t, sin t}. The Wronskian of these solutions is
W(t) =

cos t
sin t
−sin t
cos t
 = cos2 t + sin2 t = 1.
We use variation of parameters to ﬁnd a particular solution.
yp = −cos t
Z
g(t) sin t dt + sin t
Z
g(t) cos t dt
1119

The general solution can be written in the form,
y(t) =

c1 −
Z t
a
g(τ) sin τ dτ

cos t +

c2 +
Z t
b
g(τ) cos τ dτ

sin t.
2. Since the initial conditions are given at t = 0 we choose the lower bounds of integration in the general solution
to be that point.
y =

c1 −
Z t
0
g(τ) sin τ dτ

cos t +

c2 +
Z t
0
g(τ) cos τ dτ

sin t
The initial condition y(0) = 0 gives the constraint, c1 = 0. The derivative of y(t) is then,
y′(t) = −g(t) sin t cos t +
Z t
0
g(τ) sin τ dτ sin t + g(t) cos t sin t +

c2 +
Z t
0
g(τ) cos τ dτ

cos t,
y′(t) =
Z t
0
g(τ) sin τ dτ sin t +

c2 +
Z t
0
g(τ) cos τ dτ

cos t.
The initial condition y′(0) = 0 gives the constraint c2 = 0. The solution subject to the initial conditions is
y =
Z t
0
g(τ)(sin t cos τ −cos t sin τ) dτ
y =
Z t
0
g(τ) sin(t −τ) dτ
3. The solution of the initial value problem
y′′ + y = sin(λt),
y(0) = 0,
y′(0) = 0,
is
y =
Z t
0
sin(λτ) sin(t −τ) dτ.
1120

For λ ̸= 1, this is
y = 1
2
Z t
0
 cos(t −τ −λτ) −cos(t −τ + λτ)

dτ
= 1
2

−sin(t −τ −λτ)
1 + λ
+ sin(t −τ + λτ)
1 −λ
t
0
= 1
2
sin(t) −sin(−λt)
1 + λ
+ −sin(t) + sin(λt)
1 −λ

y = −λ sin t
1 −λ2 + sin(λt)
1 −λ2 .
(21.6)
The solution is the sum of two periodic functions of period 2π and 2π/λ. This solution is plotted in Figure 21.5
on the interval t ∈[0, 16π] for the values λ = 1/4, 7/8, 5/2.
Figure 21.5: Non-resonant Forcing
1121

For λ = 1, we have
y = 1
2
Z t
0
 cos(t −2τ) −cos(tau)

dτ
= 1
2

−1
2 sin(t −2τ) −τ cos t
t
0
y = 1
2 (sin t −t cos t) .
(21.7)
The solution has both a periodic and a transient term. This solution is plotted in Figure 21.5 on the interval
t ∈[0, 16π].
Figure 21.6: Resonant Forcing
Note that we can derive (21.7) from (21.6) by taking the limit as λ →0.
lim
λ→1
sin(λt) −λ sin t
1 −λ2
= lim
λ→1
t cos(λt) −sin t
−2λ
= 1
2 (sin t −t cos t)
1122

Solution 21.8
Let y1, y2 and y3 be linearly independent homogeneous solutions to the diﬀerential equation
L[y] = y′′′ + p2y′′ + p1y′ + p0y = f(x).
We will look for a particular solution of the form
yp = u1y1 + u2y2 + u3y3.
Since the uj’s are undetermined functions, we are free to impose two constraints. We choose the constraints to simplify
the algebra.
u′
1y1 + u′
2y2 + u′
3y3 = 0
u′
1y′
1 + u′
2y′
2 + u′
3y′
3 = 0
Diﬀerentiating the expression for yp,
y′
p = u′
1y1 + u1y′
1 + u′
2y2 + u2y′
2 + u′
3y3 + u3y′
3
= u1y′
1 + u2y′
2 + u3y′
3
y′′
p = u′
1y′
1 + u1y′′
1 + u′
2y′
2 + u2y′′
2 + u′
3y′
3 + u3y′′
3
= u1y′′
1 + u2y′′
2 + u3y′′
3
y′′′
p = u′
1y′′
1 + u1y′′′
1 + u′
2y′′
2 + u2y′′′
2 + u′
3y′′
3 + u3y′′′
3
Substituting the expressions for yp and its derivatives into the diﬀerential equation,
u′
1y′′
1 + u1y′′′
1 + u′
2y′′
2 + u2y′′′
2 + u′
3y′′
3 + u3y′′′
3 + p2(u1y′′
1 + u2y′′
2 + u3y′′
3) + p1(u1y′
1 + u2y′
2 + u3y′
3)
+ p0(u1y1 + u2y2 + u3y3) = f(x)
u′
1y′′
1 + u′
2y′′
2 + u′
3y′′
3 + u1L[y1] + u2L[y2] + u3L[y3] = f(x)
u′
1y′′
1 + u′
2y′′
2 + u′
3y′′
3 = f(x).
1123

With the two constraints, we have the system of equations,
u′
1y1 + u′
2y2 + u′
3y3 = 0
u′
1y′
1 + u′
2y′
2 + u′
3y′
3 = 0
u′
1y′′
1 + u′
2y′′
2 + u′
3y′′
3 = f(x)
We solve for the u′
j using Kramer’s rule.
u′
1 = (y2y′
3 −y′
2y3)f(x)
W(x)
,
u′
2 = −(y1y′
3 −y′
1y3)f(x)
W(x)
,
u′
3 = (y1y′
2 −y′
1y2)f(x)
W(x)
Here W(x) is the Wronskian of {y1, y2, y3}. Integrating the expressions for u′
j, the particular solution is
yp = y1
Z (y2y′
3 −y′
2y3)f(x)
W(x)
dx + y2
Z (y3y′
1 −y′
3y1)f(x)
W(x)
dx + y3
Z (y1y′
2 −y′
1y2)f(x)
W(x)
dx.
Green Functions
Solution 21.9
We consider the Green function problem
G′′ = f(x),
G(−∞|ξ) = G′(−∞|ξ) = 0.
The homogeneous solution is y = c1 + c2x. The homogeneous solution that satisﬁes the boundary conditions is y = 0.
Thus the Green function has the form
G(x|ξ) =
(
0
x < ξ,
c1 + c2x
x > ξ.
The continuity and jump conditions are then
G(ξ+|ξ) = 0,
G′(ξ+|ξ) = 1.
1124

Thus the Green function is
G(x|ξ) =
(
0
x < ξ,
x −ξ
x > ξ = (x −ξ)H(x −ξ).
The solution of the problem
y′′ = f(x),
y(−∞) = y′(−∞) = 0.
is
y =
Z ∞
−∞
f(ξ)G(x|ξ) dξ
y =
Z ∞
−∞
f(ξ)(x −ξ)H(x −ξ) dξ
y =
Z x
−∞
f(ξ)(x −ξ) dξ
We diﬀerentiate this solution to verify that it satisﬁes the diﬀerential equation.
y′ = [f(ξ)(x −ξ)]ξ=x +
Z x
−∞
∂
∂x (f(ξ)(x −ξ)) dξ =
Z x
−∞
f(ξ) dξ
y′′ = [f(ξ)]ξ=x = f(x)
Solution 21.10
Since we are dealing with an Euler equation, we substitute y = xλ to ﬁnd the homogeneous solutions.
λ(λ −1) + λ −1 = 0
(λ −1)(λ + 1) = 0
y1 = x,
y2 = 1
x
1125

Variation of Parameters.
The Wronskian of the homogeneous solutions is
W(x) =

x
1/x
1
−1/x2
 = −1
x −1
x = −2
x.
A particular solution is
yp = −x
Z x2(1/x)
−2/x dx + 1
x
Z
x2x
−2/x dx
= −x
Z
−x2
2 dx + 1
x
Z
−x4
2 dx
= x4
6 −x4
10
= x4
15.
The general solution is
y = x4
15 + c1x + c2
1
x.
Applying the initial conditions,
y(0) = 0
→
c2 = 0
y′(0) = 0
→
c1 = 1.
Thus we have the solution
y = x4
15 + x.
1126

Green Function.
Since this problem has both an inhomogeneous term in the diﬀerential equation and inhomoge-
neous boundary conditions, we separate it into the two problems
u′′ + 1
xu′ −1
x2u = x2,
u(0) = u′(0) = 0,
v′′ + 1
xv′ −1
x2v = 0,
v(0) = 0, v′(0) = 1.
First we solve the inhomogeneous diﬀerential equation with the homogeneous boundary conditions.
The Green
function for this problem satisﬁes
L[G(x|ξ)] = δ(x −ξ),
G(0|ξ) = G′(0|ξ) = 0.
Since the Green function must satisfy the homogeneous boundary conditions, it has the form
G(x|ξ) =
(
0
for x < ξ
cx + d/x
for x > ξ.
From the continuity condition,
0 = cξ + d/ξ.
The jump condition yields
c −d/ξ2 = 1.
Solving these two equations, we obtain
G(x|ξ) =
(
0
for x < ξ
1
2x −ξ2
2x
for x > ξ
1127

Thus the solution is
u(x) =
Z ∞
0
G(x|ξ)ξ2 dξ
=
Z x
0
1
2x −ξ2
2x

ξ2 dξ
= 1
6x4 −1
10x4
= x4
15.
Now to solve the homogeneous diﬀerential equation with inhomogeneous boundary conditions. The general solution
for v is
v = cx + d/x.
Applying the two boundary conditions gives
v = x.
Thus the solution for y is
y = x + x4
15.
Solution 21.11
The Green function satisﬁes
G′′′(x|ξ) + p2(x)G′′(x|ξ) + p1(x)G′(x|ξ) + p0(x)G(x|ξ) = δ(x −ξ).
First note that only the G′′′(x|ξ) term can have a delta function singularity. If a lower derivative had a delta function
type singularity, then G′′′(x|ξ) would be more singular than a delta function and there would be no other term in the
equation to balance that behavior. Thus we see that G′′′(x|ξ) will have a delta function singularity; G′′(x|ξ) will have
a jump discontinuity; G′(x|ξ) will be continuous at x = ξ. Integrating the diﬀerential equation from ξ−to ξ+ yields
Z ξ+
ξ−G′′′(x|ξ) dx =
Z ξ+
ξ−δ(x −ξ) dx
1128

G′′(ξ+|ξ) −G′′(ξ−|ξ) = 1.
Thus we have the three continuity conditions:
G′′(ξ+|ξ) = G′′(ξ−|ξ) + 1
G′(ξ+|ξ) = G′(ξ−|ξ)
G(ξ+|ξ) = G(ξ−|ξ)
Solution 21.12
Variation of Parameters. Consider the problem
x2y′′ −2xy′ + 2y = e−x,
y(1) = 0,
y′(1) = 1.
Previously we showed that two homogeneous solutions are
y1 = x,
y2 = x2.
The Wronskian of these solutions is
W(x) =

x
x2
1
2x
 = 2x2 −x2 = x2.
In the variation of parameters formula, we will choose 1 as the lower bound of integration. (This will simplify the
algebra in applying the initial conditions.)
yp = −x
Z x
1
e−ξ ξ2
ξ4
dξ + x2
Z x
1
e−ξ ξ
ξ4
dξ
= −x
Z x
1
e−ξ
ξ2 dξ + x2
Z x
1
e−ξ
ξ3 dξ
= −x

e−1 −e−x
x −
Z x
1
e−ξ
ξ dξ

+ x2
e−x
2x −e−x
2x2 + 1
2
Z x
1
e−ξ
ξ dξ

= −x e−1 +1
2(1 + x) e−x +
x + x2
2
 Z x
1
e−ξ
ξ dξ
1129

If you wanted to, you could write the last integral in terms of exponential integral functions.
The general solution is
y = c1x + c2x2 −x e−1 +1
2(1 + x) e−x +

x + x2
2
 Z x
1
e−ξ
ξ dξ
Applying the boundary conditions,
y(1) = 0
→
c1 + c2 = 0
y′(1) = 1
→
c1 + 2c2 = 1,
we ﬁnd that c1 = −1, c2 = 1.
Thus the solution subject to the initial conditions is
y = −(1 + e−1)x + x2 + 1
2(1 + x) e−x +

x + x2
2
 Z x
1
e−ξ
ξ dξ
Green Functions. The solution to the problem is y = u + v where
u′′ −2
xu′ + 2
x2u = e−x
x2 ,
u(1) = 0,
u′(1) = 0,
and
v′′ −2
xv′ + 2
x2v = 0,
v(1) = 0,
v′(1) = 1.
The problem for v has the solution
v = −x + x2.
The Green function for u is
G(x|ξ) = H(x −ξ)uξ(x)
where
uξ(ξ) = 0,
and
u′
ξ(ξ) = 1.
1130

Thus the Green function is
G(x|ξ) = H(x −ξ)

−x + x2
ξ

.
The solution for u is then
u =
Z ∞
1
G(x|ξ)e−ξ
ξ2 dξ
=
Z x
1

−x + x2
ξ
 e−ξ
ξ2 dξ
= −x e−1 +1
2(1 + x) e−x +

x + x2
2
 Z x
1
e−ξ
ξ dξ.
Thus we ﬁnd the solution for y is
y = −(1 + e−1)x + x2 + 1
2(1 + x) e−x +

x + x2
2
 Z x
1
e−ξ
ξ dξ
Solution 21.13
The diﬀerential equation for the Green function is
G′′ −G = δ(x −ξ),
Gx(0|ξ) = G(1|ξ) = 0.
Note that cosh(x) and sinh(x −1) are homogeneous solutions that satisfy the left and right boundary conditions,
respectively. The Wronskian of these two solutions is
W(x) =

cosh(x)
sinh(x −1)
sinh(x)
cosh(x −1)

= cosh(x) cosh(x −1) −sinh(x) sinh(x −1)
= 1
4
  ex + e−x  ex−1 + e−x+1
−
 ex −e−x  ex−1 −e−x+1
= 1
2
 e1 + e−1
= cosh(1).
1131

The Green function for the problem is then
G(x|ξ) = cosh(x<) sinh(x> −1)
cosh(1)
,
G(x|ξ) =
(cosh(x) sinh(ξ−1)
cosh(1)
for 0 ≤x ≤ξ,
cosh(ξ) sinh(x−1)
cosh(1)
for ξ ≤x ≤1.
Solution 21.14
The diﬀerential equation for the Green function is
G′′ −G = δ(x −ξ),
G(0|ξ) = G(∞|ξ) = 0.
Note that sinh(x) and e−x are homogeneous solutions that satisfy the left and right boundary conditions, respectively.
The Wronskian of these two solutions is
W(x) =

sinh(x)
e−x
cosh(x)
−e−x

= −sinh(x) e−x −cosh(x) e−x
= −1
2
 ex −e−x e−x −1
2
 ex + e−x e−x
= −1
The Green function for the problem is then
G(x|ξ) = −sinh(x<) e−x>
G(x|ξ) =
(
−sinh(x) e−ξ
for 0 ≤x ≤ξ,
−sinh(ξ) e−x
for ξ ≤x ≤∞.
1132

Solution 21.15
a) The Green function problem is
xG′′(x|ξ) + G′(x|ξ) = δ(x −ξ),
G(0|ξ) bounded, G(1|ξ) = 0.
First we ﬁnd the homogeneous solutions of the diﬀerential equation.
xy′′ + y′ = 0
This is an exact equation.
d
dx[xy′] = 0
y′ = c1
x
y = c1 log x + c2
The homogeneous solutions y1 = 1 and y2 = log x satisfy the left and right boundary conditions, respectively.
The Wronskian of these solutions is
W(x) =

1
log x
0
1/x
 = 1
x.
The Green function is
G(x|ξ) = 1 · log x>
ξ(1/ξ) ,
G(x|ξ) = log x>.
b) The Green function problem is
G′′(x|ξ) −G(x|ξ) = δ(x −ξ),
G(−a|ξ) = G(a|ξ) = 0.
1133

{ex, e−x} and {cosh x, sinh x} are both linearly independent sets of homogeneous solutions. sinh(x + a) and
sinh(x −a) are homogeneous solutions that satisfy the left and right boundary conditions, respectively. The
Wronskian of these two solutions is,
W(x) =

sinh(x + a)
sinh(x −a)
cosh(x + a)
cosh(x −a)

= sinh(x + a) cosh(x −a) −sinh(x −a) cosh(x + a)
= sinh(2a)
The Green function is
G(x|ξ) = sinh(x< + a) sinh(x> −a)
sinh(2a)
.
c) The Green function problem is
G′′(x|ξ) −G(x|ξ) = δ(x −ξ),
G(x|ξ) bounded as |x| →∞.
ex and e−x are homogeneous solutions that satisfy the left and right boundary conditions, respectively. The
Wronskian of these solutions is
W(x) =

ex
e−x
ex
−e−x
 = −2.
The Green function is
G(x|ξ) = ex< e−x>
−2
,
G(x|ξ) = −1
2 ex<−x> .
d) The Green function from part (b) is,
G(x|ξ) = sinh(x< + a) sinh(x> −a)
sinh(2a)
.
1134

We take the limit as a →∞.
lim
a→∞
sinh(x< + a) sinh(x> −a)
sinh(2a)
= lim
a→∞
(ex<+a −e−x<−a) (ex>−a −e−x>+a)
2 (e2a −e−2a)
= lim
a→∞
−ex<−x> + ex<+x>−2a + e−x<−x>−2a −e−x<+x>−4a
2 −2 e−4a
= −ex<−x>
2
Thus we see that the solution from part (b) approaches the solution from part (c) as a →∞.
Solution 21.16
1. The problem,
y′′ + λy = f(x),
y(0) = y(π) = 0,
has a Green function if and only if it has a unique solution. This inhomogeneous problem has a unique solution
if and only if the homogeneous problem has only the trivial solution.
First consider the case λ = 0. We ﬁnd the general solution of the homogeneous diﬀerential equation.
y = c1 + c2x
Only the trivial solution satisﬁes the boundary conditions. The problem has a unique solution for λ = 0.
Now consider non-zero λ. We ﬁnd the general solution of the homogeneous diﬀerential equation.
y = c1 cos
√
λx

+ c2 sin
√
λx

.
The solution that satisﬁes the left boundary condition is
y = c sin
√
λx

.
1135

We apply the right boundary condition and ﬁnd nontrivial solutions.
sin
√
λπ

= 0
λ = n2,
n ∈Z+
Thus the problem has a unique solution for all complex λ except λ = n2, n ∈Z+.
Consider the case λ = 0. We ﬁnd solutions of the homogeneous equation that satisfy the left and right boundary
conditions, respectively.
y1 = x,
y2 = x −π.
We compute the Wronskian of these functions.
W(x) =

x
x −π
1
1
 = π.
The Green function for this case is
G(x|ξ) = x<(x> −π)
π
.
We consider the case λ ̸= n2, λ ̸= 0. We ﬁnd the solutions of the homogeneous equation that satisfy the left
and right boundary conditions, respectively.
y1 = sin
√
λx

,
y2 = sin
√
λ(x −π)

.
We compute the Wronskian of these functions.
W(x) =

sin
√
λx

sin
√
λ(x −π)

√
λ cos
√
λx

√
λ cos
√
λ(x −π)


=
√
λ sin
√
λπ

The Green function for this case is
G(x|ξ) =
sin
√
λx<

sin
√
λ(x> −π)

√
λ sin
√
λπ

.
1136

2. Now we consider the problem
y′′ + 9y = 1 + αx,
y(0) = y(π) = 0.
The homogeneous solutions of the problem are constant multiples of sin(3x). Thus for each value of α, the
problem either has no solution or an inﬁnite number of solutions. There will be an inﬁnite number of solutions if
the inhomogeneity 1 + αx is orthogonal to the homogeneous solution sin(3x) and no solution otherwise.
Z π
0
(1 + αx) sin(3x) dx = πα + 2
3
The problem has a solution only for α = −2/π. For this case the general solution of the inhomogeneous diﬀerential
equation is
y = 1
9

1 −2x
π

+ c1 cos(3x) + c2 sin(3x).
The one-parameter family of solutions that satisﬁes the boundary conditions is
y = 1
9

1 −2x
π −cos(3x)

+ c sin(3x).
3. For λ = n2, n ∈Z+, y = sin(nx) is a solution of the homogeneous equation that satisﬁes the boundary
conditions. Equation 21.5 has a (non-unique) solution only if f is orthogonal to sin(nx).
Z π
0
f(x) sin(nx) dx = 0
The modiﬁed Green function satisﬁes
G′′ + n2G = δ(x −ξ) −sin(nx) sin(nξ)
π/2
.
We expand G in a series of the eigenfunctions.
G(x|ξ) =
∞
X
k=1
gk sin(kx)
1137

We substitute the expansion into the diﬀerential equation to determine the coeﬃcients. This will not determine
gn. We choose gn = 0, which is one of the choices that will make the modiﬁed Green function symmetric in x
and ξ.
∞
X
k=1
gk
 n2 −k2
sin(kx) = 2
π
∞
X
k=1
k̸=n
sin(kx) sin(kξ)
G(x|ξ) = 2
π
∞
X
k=1
k̸=n
sin(kx) sin(kξ)
n2 −k2
The solution of the inhomogeneous problem is
y(x) =
Z π
0
f(ξ)G(x|ξ) dξ.
Solution 21.17
We separate the problem for u into the two problems:
Lv ≡(pv′)′ + qv = f(x),
a < x < b,
v(a) = 0,
v(b) = 0
Lw ≡(pw′)′ + qw = 0,
a < x < b,
w(a) = α,
w(b) = β
and note that the solution for u is u = v + w.
The problem for v has the solution,
v =
Z b
a
g(x; ξ)f(ξ) dξ,
with the Green function,
g(x; ξ) = v1(x<)v2(x>)
p(ξ)W(ξ)
≡
(v1(x)v2(ξ)
p(ξ)W(ξ)
for a ≤x ≤ξ,
v1(ξ)v2(x)
p(ξ)W(ξ)
for ξ ≤x ≤b.
Here v1 and v2 are homogeneous solutions that respectively satisfy the left and right homogeneous boundary conditions.
1138

Since g(x; ξ) is a solution of the homogeneous equation for x ̸= ξ, gξ(x; ξ) is a solution of the homogeneous
equation for x ̸= ξ. This is because for x ̸= ξ,
L
 ∂
∂ξg

= ∂
∂ξL[g] = ∂
∂ξδ(x −ξ) = 0.
If ξ is outside of the domain, (a, b), then g(x; ξ) and gξ(x; ξ) are homogeneous solutions on that domain. In particular
gξ(x; a) and gξ(x; b) are homogeneous solutions,
L [gξ(x; a)] = L [gξ(x; b)] = 0.
Now we use the deﬁnition of the Green function and v1(a) = v2(b) = 0 to determine simple expressions for these
homogeneous solutions.
gξ(x; a) = v′
1(a)v2(x)
p(a)W(a) −(p′(a)W(a) + p(a)W ′(a))v1(a)v2(x)
(p(a)W(a))2
= v′
1(a)v2(x)
p(a)W(a)
=
v′
1(a)v2(x)
p(a)(v1(a)v′
2(a) −v′
1(a)v2(a))
= −
v′
1(a)v2(x)
p(a)v′
1(a)v2(a)
= −
v2(x)
p(a)v2(a)
We note that this solution has the boundary values,
gξ(a; a) = −
v2(a)
p(a)v2(a) = −1
p(a),
gξ(b; a) = −
v2(b)
p(a)v2(a) = 0.
1139

We examine the second solution.
gξ(x; b) = v1(x)v′
2(b)
p(b)W(b) −(p′(b)W(b) + p(b)W ′(b))v1(x)v2(b)
(p(b)W(b))2
= v1(x)v′
2(b)
p(b)W(b)
=
v1(x)v′
2(b)
p(b)(v1(b)v′
2(b) −v′
1(b)v2(b))
=
v1(x)v′
2(b)
p(b)v1(b)v′
2(b)
=
v1(x)
p(b)v1(b)
This solution has the boundary values,
gξ(a; b) =
v1(a)
p(b)v1(b) = 0,
gξ(b; b) =
v1(b)
p(b)v1(b) =
1
p(b).
Thus we see that the solution of
Lw = (pw′)′ + qw = 0,
a < x < b,
w(a) = α,
w(b) = β,
is
w = −αp(a)gξ(x; a) + βp(b)gξ(x; b).
Therefore the solution of the problem for u is
u =
Z b
a
g(x; ξ)f(ξ) dξ −αp(a)gξ(x; a) + βp(b)gξ(x; b).
Solution 21.18
Figure 21.7 shows a plot of G(x; 1) and G(x; −1) for k = 1.
1140

-4
-2
2
4
-0.5
-0.4
-0.3
-0.2
-0.1
Figure 21.7: G(x; 1) and G(x; −1)
First we consider the boundary condition u(0) = 0. Note that the solution of
G′′ −k2G = δ(x −ξ) −δ(x + ξ), |G(±∞; ξ)| < ∞,
satisﬁes the condition G(0; ξ) = 0. Thus the Green function which satisﬁes G(0; ξ) = 0 is
G(x; ξ) = −1
2k e−k|x−ξ| + 1
2k e−k|x+ξ| .
Since x, ξ > 0 we can write this as
G(x; ξ) = −1
2k e−k|x−ξ| + 1
2k e−k(x+ξ)
=
(
−1
2k e−k(ξ−x) + 1
2k e−k(x+ξ),
for x < ξ
−1
2k e−k(x−ξ) + 1
2k e−k(x+ξ),
for ξ < x
=
(
−1
k e−kξ sinh(kx),
for x < ξ
−1
k e−kx sinh(kξ),
for ξ < x
1141

G(x; ξ) = −1
k e−kx> sinh(kx<)
Now consider the boundary condition u′(0) = 0. Note that the solution of
G′′ −k2G = δ(x −ξ) + δ(x + ξ),
|G(±∞; ξ)| < ∞,
satisﬁes the boundary condition G′(x; ξ) = 0. Thus the Green function is
G(x; ξ) = −1
2k e−k|x−ξ| −1
2k e−k|x+ξ| .
Since x, ξ > 0 we can write this as
G(x; ξ) = −1
2k e−k|x−ξ| −1
2k e−k(x+ξ)
=
(
−1
2k e−k(ξ−x) −1
2k e−k(x+ξ),
for x < ξ
−1
2k e−k(x−ξ) −1
2k e−k(x+ξ),
for ξ < x
=
(
−1
k e−kξ cosh(kx),
for x < ξ
−1
k e−kx cosh(kξ),
for ξ < x
G(x; ξ) = −1
k e−kx> cosh(kx<)
The Green functions which satisﬁes G(0; ξ) = 0 and G′(0; ξ) = 0 are shown in Figure 21.8.
Solution 21.19
1. The Green function satisﬁes
g′′ −a2g = δ(x −ξ),
g(0; ξ) = g′(L; ξ) = 0.
We can write the set of homogeneous solutions as
eax, e−ax	
or {cosh(ax), sinh(ax)} .
1142

1
2
3
4
5
-0.4
-0.3
-0.2
-0.1
1
2
3
4
5
-0.5
-0.4
-0.3
-0.2
-0.1
Figure 21.8: G(x; 1) and G(x; −1)
The solutions that respectively satisfy the left and right boundary conditions are
u1 = sinh(ax),
u2 = cosh(a(x −L)).
The Wronskian of these solutions is
W(x) =
 sinh(ax)
cosh(a(x −L))
a cosh(ax)
a sinh(a(x −L))

= −a cosh(aL).
Thus the Green function is
g(x; ξ) =
(
−sinh(ax) cosh(a(ξ−L))
a cosh(aL)
for x ≤ξ,
−sinh(aξ) cosh(a(x−L))
a cosh(aL)
for ξ ≤x. = −sinh(ax<) cosh(a(x> −L))
a cosh(aL)
.
2. We take the limit as L →∞.
g(x; ξ) = lim
L→∞−sinh(ax<) cosh(a(x> −L))
a cosh(aL)
= lim
L→∞−sinh(ax<)
a
cosh(ax>) cosh(aL) −sinh(ax>) sinh(aL)
cosh(aL)
= −sinh(ax<)
a
(cosh(ax>) −sinh(ax>))
1143

g(x; ξ) = −1
a sinh(ax<) e−ax>
The solution of
y′′ −a2y = e−x,
y(0) = y′(∞) = 0
is
y =
Z ∞
0
g(x; ξ) e−ξ dξ
= −1
a
Z ∞
0
sinh(ax<) e−ax> e−ξ dξ
= −1
a
Z x
0
sinh(aξ) e−ax e−ξ dξ +
Z ∞
x
sinh(ax) e−aξ e−ξ dξ

We ﬁrst consider the case that a ̸= 1.
= −1
a
 e−ax
a2 −1
 −a + e−x(a cosh(ax) + sinh(ax))

+
1
a + 1 e−(a+1)x sinh(ax)

= e−ax −e−x
a2 −1
For a = 1, we have
y = −
1
4 e −x
 −1 + 2x + e−2x
+ 1
2 e−2x sinh(x)

= −1
2x e−x .
Thus the solution of the problem is
y =
(
e−ax −e−x
a2−1
for a ̸= 1,
−1
2x e−x
for a = 1.
We note that this solution satisﬁes the diﬀerential equation and the boundary conditions.
1144

Chapter 22
Diﬀerence Equations
Televisions should have a dial to turn up the intelligence. There is a brightness knob, but it doesn’t work.
-?
22.1
Introduction
Example 22.1.1 Gambler’s ruin problem. Consider a gambler that initially has n dollars. He plays a game in which
he has a probability p of winning a dollar and q of losing a dollar. (Note that p + q = 1.) The gambler has decided
that if he attains N dollars he will stop playing the game. In this case we will say that he has succeeded. Of course
if he runs out of money before that happens, we will say that he is ruined. What is the probability of the gambler’s
ruin? Let us denote this probability by an. We know that if he has no money left, then his ruin is certain, so a0 = 1. If
he reaches N dollars he will quit the game, so that aN = 0. If he is somewhere in between ruin and success then the
probability of his ruin is equal to p times the probability of his ruin if he had n + 1 dollars plus q times the probability
of his ruin if he had n −1 dollars. Writing this in an equation,
an = pan+1 + qan−1
subject to
a0 = 1,
aN = 0.
1145

This is an example of a diﬀerence equation. You will learn how to solve this particular problem in the section on constant
coeﬃcient equations.
Consider the sequence a1, a2, a3, . . . Analogous to a derivative of a continuous function, we can deﬁne a discrete
derivative on the sequence
Dan = an+1 −an.
The second discrete derivative is then deﬁned as
D2an = D[an+1 −an] = an+2 −2an+1 + an.
The discrete integral of an is
n
X
i=n0
ai.
Corresponding to
Z β
α
df
dx dx = f(β) −f(α),
in the discrete realm we have
β−1
X
n=α
D[an] =
β−1
X
n=α
(an+1 −an) = aβ −aα.
Linear diﬀerence equations have the form
Dran + pr−1(n)Dr−1an + · · · + p1(n)Dan + p0(n)an = f(n).
From the deﬁnition of the discrete derivative an equivalent form is
an+r + qr−1(n)anr−1 + · · · + q1(n)an+1 + q0(n)an = f(n).
Besides being important in their own right, we will need to solve diﬀerence equations in order to develop series
solutions of diﬀerential equations.
Also, some methods of solving diﬀerential equations numerically are based on
approximating them with diﬀerence equations.
1146

There are many similarities between diﬀerential and diﬀerence equations. Like diﬀerential equations, an rth order
homogeneous diﬀerence equation has r linearly independent solutions. The general solution to the rth order inho-
mogeneous equation is the sum of the particular solution and an arbitrary linear combination of the homogeneous
solutions.
For an rth order diﬀerence equation, the initial condition is given by specifying the values of the ﬁrst r an’s.
Example 22.1.2 Consider the diﬀerence equation an−2 −an−1 −an = 0 subject to the initial condition a1 = a2 = 1.
Note that although we may not know a closed-form formula for the an we can calculate the an in order by substituting
into the diﬀerence equation. The ﬁrst few an are 1, 1, 2, 3, 5, 8, 13, 21, . . . We recognize this as the Fibonacci sequence.
22.2
Exact Equations
Consider the sequence a1, a2, . . .. Exact diﬀerence equations on this sequence have the form
D[F(an, an+1, . . . , n)] = g(n).
We can reduce the order of, (or solve for ﬁrst order), this equation by summing from 1 to n −1.
n−1
X
j=1
D[F(aj, aj+1, . . . , j)] =
n−1
X
j=1
g(j)
F(an, an+1, . . . , n) −F(a1, a2, . . . , 1) =
n−1
X
j=1
g(j)
F(an, an+1, . . . , n) =
n−1
X
j=1
g(j) + F(a1, a2, . . . , 1)
1147

Result 22.2.1 We can reduce the order of the exact diﬀerence equation
D[F(an, an+1, . . . , n)] = g(n),
for n ≥1
by summing both sides of the equation to obtain
F(an, an+1, . . . , n) =
n−1
X
j=1
g(j) + F(a1, a2, . . . , 1).
Example 22.2.1 Consider the diﬀerence equation, D[nan] = 1. Summing both sides of this equation
n−1
X
j=1
D[jaj] =
n−1
X
j=1
1
nan −a1 = n −1
an = n + a1 −1
n
.
22.3
Homogeneous First Order
Consider the homogeneous ﬁrst order diﬀerence equation
an+1 = p(n)an,
for n ≥1.
1148

We can directly solve for an.
an = an
an−1
an−1
an−2
an−2
· · · a1
a1
= a1
an
an−1
an−1
an−2
· · · a2
a1
= a1p(n −1)p(n −2) · · · p(1)
= a1
n−1
Y
j=1
p(j)
Alternatively, we could solve this equation by making it exact. Analogous to an integrating factor for diﬀerential
equations, we multiply the equation by the summing factor
S(n) =
" n
Y
j=1
p(j)
#−1
.
an+1 −p(n)an = 0
an+1
Qn
j=1 p(j) −
an
Qn−1
j=1 p(j)
= 0
D
"
an
Qn−1
j=1 p(j)
#
= 0
Now we sum from 1 to n −1.
an
Qn−1
j=1 p(j)
−a1 = 0
an = a1
n−1
Y
j=1
p(j)
1149

Result 22.3.1 The solution of the homogeneous ﬁrst order diﬀerence equation
an+1 = p(n)an,
for n ≥1,
is
an = a1
n−1
Y
j=1
p(j).
Example 22.3.1 Consider the equation an+1 = nan with the initial condition a1 = 1.
an = a1
n−1
Y
j=1
j = (1)(n −1)! = Γ(n)
Recall that Γ(z) is the generalization of the factorial function. For positive integral values of the argument, Γ(n) =
(n −1)!.
22.4
Inhomogeneous First Order
Consider the equation
an+1 = p(n)an + q(n)
for
n ≥1.
Multiplying by S(n) =
hQn
j=1 p(j)
i−1
yields
an+1
Qn
j=1 p(j) −
an
Qn−1
j=1 p(j)
=
q(n)
Qn
j=1 p(j).
The left hand side is a discrete derivative.
D
"
an
Qn−1
j=1 p(j)
#
=
q(n)
Qn
j=1 p(j)
1150

Summing both sides from 1 to n −1,
an
Qn−1
j=1 p(j)
−a1 =
n−1
X
k=1
"
q(k)
Qk
j=1 p(j)
#
an =
"n−1
Y
m=1
p(m)
# "n−1
X
k=1
"
q(k)
Qk
j=1 p(j)
#
+ a1
#
.
Result 22.4.1 The solution of the inhomogeneous ﬁrst order diﬀerence equation
an+1 = p(n)an + q(n)
for
n ≥1
is
an =
"n−1
Y
m=1
p(m)
# "n−1
X
k=1
"
q(k)
Qk
j=1 p(j)
#
+ a1
#
.
Example 22.4.1 Consider the equation an+1 = nan + 1 for n ≥1. The summing factor is
S(n) =
" n
Y
j=1
j
#−1
= 1
n!.
1151

Multiplying the diﬀerence equation by the summing factor,
an+1
n!
−
an
(n −1)! = 1
n!
D

an
(n −1)!

= 1
n!
an
(n −1)! −a1 =
n−1
X
k=1
1
k!
an = (n −1)!
"n−1
X
k=1
1
k! + a1
#
.
Example 22.4.2 Consider the equation
an+1 = λan + µ,
for n ≥0.
From the above result, (with the products and sums starting at zero instead of one), the solution is
a0 =
"n−1
Y
m=0
λ
# "n−1
X
k=0
"
µ
Qk
j=0 λ
#
+ a0
#
= λn
"n−1
X
k=0
h µ
λk+1
i
+ a0
#
= λn

µλ−n−1 −λ−1
λ−1 −1
+ a0

= λn

µλ−n −1
1 −λ
+ a0

= µ1 −λn
1 −λ + a0λn.
1152

22.5
Homogeneous Constant Coeﬃcient Equations
Homogeneous constant coeﬃcient equations have the form
an+N + pN−1an+N−1 + · · · + p1an+1 + p0an = 0.
The substitution an = rn yields
rN + pN−1rN−1 + · · · + p1r + p0 = 0
(r −r1)m1 · · · (r −rk)mk = 0.
If r1 is a distinct root then the associated linearly independent solution is rn
1. If r1 is a root of multiplicity m > 1
then the associated solutions are rn
1, nrn
1, n2rn
1, . . . , nm−1rn
1.
Result 22.5.1 Consider the homogeneous constant coeﬃcient diﬀerence equation
an+N + pN−1an+N−1 + · · · + p1an+1 + p0an = 0.
The substitution an = rn yields the equation
(r −r1)m1 · · · (r −rk)mk = 0.
A set of linearly independent solutions is
{rn
1, nrn
1, . . . , nm1−1rn
1, . . . , rn
k, nrn
k, . . . , nmk−1rn
k}.
Example 22.5.1 Consider the equation an+2 −3an+1 + 2an = 0 with the initial conditions a1 = 1 and a2 = 3. The
substitution an = rn yields
r2 −3r + 2 = (r −1)(r −2) = 0.
Thus the general solution is
an = c11n + c22n.
1153

The initial conditions give the two equations,
a1 = 1 = c1 + 2c2
a2 = 3 = c1 + 4c2
Since c1 = −1 and c2 = 1, the solution to the diﬀerence equation subject to the initial conditions is
an = 2n −1.
Example 22.5.2 Consider the gambler’s ruin problem that was introduced in Example 22.1.1. The equation for the
probability of the gambler’s ruin at n dollars is
an = pan+1 + qan−1
subject to
a0 = 1,
aN = 0.
We assume that 0 < p < 1. With the substitution an = rn we obtain
r = pr2 + q.
The roots of this equation are
r = 1 ± √1 −4pq
2p
= 1 ±
p
1 −4p(1 −p)
2p
= 1 ±
p
(1 −2p)2
2p
= 1 ± |1 −2p|
2p
.
We will consider the two cases p ̸= 1/2 and p = 1/2.
1154

p ̸= 1/2. If p < 1/2, the roots are
r = 1 ± (1 −2p)
2p
r1 = 1 −p
p
= q
p,
r2 = 1.
If p > 1/2 the roots are
r = 1 ± (2p −1)
2p
r1 = 1,
r2 = −p + 1
p
= q
p.
Thus the general solution for p ̸= 1/2 is
an = c1 + c2
q
p
n
.
The boundary condition a0 = 1 requires that c1 + c2 = 1. From the boundary condition aN = 0 we have
(1 −c2) + c2
q
p
N
= 0
c2 =
−1
−1 + (q/p)N
c2 =
pN
pN −qN .
Solving for c1,
c1 = 1 −
pN
pN −qN
c1 =
−qN
pN −qN .
1155

Thus we have
an =
−qN
pN −qN +
pN
pN −qN
q
p
n
.
p = 1/2. In this case, the two roots of the polynomial are both 1. The general solution is
an = c1 + c2n.
The left boundary condition demands that c1 = 1. From the right boundary condition we obtain
1 + c2N = 0
c2 = −1
N .
Thus the solution for this case is
an = 1 −n
N .
As a check that this formula makes sense, we see that for n = N/2 the probability of ruin is 1 −N/2
N = 1
2.
22.6
Reduction of Order
Consider the diﬀerence equation
(n + 1)(n + 2)an+2 −3(n + 1)an+1 + 2an = 0
for
n ≥0
(22.1)
We see that one solution to this equation is an = 1/n!. Analogous to the reduction of order for diﬀerential equations,
the substitution an = bn/n! will reduce the order of the diﬀerence equation.
(n + 1)(n + 2)bn+2
(n + 2)!
−3(n + 1)bn+1
(n + 1)!
+ 2bn
n! = 0
bn+2 −3bn+1 + 2bn = 0
(22.2)
1156

At ﬁrst glance it appears that we have not reduced the order of the equation, but writing it in terms of discrete
derivatives
D2bn −Dbn = 0
shows that we now have a ﬁrst order diﬀerence equation for Dbn. The substitution bn = rn in equation 22.2 yields the
algebraic equation
r2 −3r + 2 = (r −1)(r −2) = 0.
Thus the solutions are bn = 1 and bn = 2n. Only the bn = 2n solution will give us another linearly independent solution
for an. Thus the second solution for an is an = bn/n! = 2n/n!. The general solution to equation 22.1 is then
an = c1
1
n! + c2
2n
n! .
Result 22.6.1 Let an = sn be a homogeneous solution of a linear diﬀerence equation. The
substitution an = snbn will yield a diﬀerence equation for bn that is of order one less than the
equation for an.
1157

22.7
Exercises
Exercise 22.1
Find a formula for the nth term in the Fibonacci sequence 1, 1, 2, 3, 5, 8, 13, . . ..
Hint, Solution
Exercise 22.2
Solve the diﬀerence equation
an+2 = 2
nan,
a1 = a2 = 1.
Hint, Solution
1158

22.8
Hints
Hint 22.1
The diﬀerence equation corresponding to the Fibonacci sequence is
an+2 −an+1 −an = 0,
a1 = a2 = 1.
Hint 22.2
Consider this exercise as two ﬁrst order diﬀerence equations; one for the even terms, one for the odd terms.
1159

22.9
Solutions
Solution 22.1
We can describe the Fibonacci sequence with the diﬀerence equation
an+2 −an+1 −an = 0,
a1 = a2 = 1.
With the substitution an = rn we obtain the equation
r2 −r −1 = 0.
This equation has the two distinct roots
r1 = 1 +
√
5
2
,
r2 = 1 −
√
5
2
.
Thus the general solution is
an = c1
 
1 +
√
5
2
!n
+ c2
 
1 −
√
5
2
!n
.
From the initial conditions we have
c1r1+c2r2 = 1
c1r2
1+c2r2
2 = 1.
Solving for c2 in the ﬁrst equation,
c2 = 1
r2
(1 −c1r1).
We substitute this into the second equation.
c1r2
1 + 1
r2
(1 −c1r1)r2
2 = 1
c1(r2
1 −r1r2) = 1 −r2
1160

c1 =
1 −r2
r2
1 −r1r2
= 1 −1−
√
5
2
1+
√
5
2
√
5
=
1+
√
5
2
1+
√
5
2
√
5
= 1
√
5
Substitute this result into the equation for c2.
c2 = 1
r2

1 −1
√
5r1

=
2
1 −
√
5
 
1 −1
√
5
1 +
√
5
2
!
= −
2
1 −
√
5
 
1 −
√
5
2
√
5
!
= −1
√
5
Thus the nth term in the Fibonacci sequence has the formula
an = 1
√
5
 
1 +
√
5
2
!n
−1
√
5
 
1 −
√
5
2
!n
.
It is interesting to note that although the Fibonacci sequence is deﬁned in terms of integers, one cannot express the
formula form the nth element in terms of rational numbers.
1161

Solution 22.2
We can consider
an+2 = 2
nan,
a1 = a2 = 1
to be a ﬁrst order diﬀerence equation. First consider the odd terms.
a1 = 1
a3 = 2
1
a5 = 2
3
2
1
an =
2(n−1)/2
(n −2)(n −4) · · · (1)
For the even terms,
a2 = 1
a4 = 2
2
a6 = 2
4
2
2
an =
2(n−2)/2
(n −2)(n −4) · · · (2).
Thus
an =
(
2(n−1)/2
(n−2)(n−4)···(1)
for odd n
2(n−2)/2
(n−2)(n−4)···(2)
for even n.
1162

Chapter 23
Series Solutions of Diﬀerential Equations
Skill beats honesty any day.
-?
23.1
Ordinary Points
Big O and Little o Notation.
The notation O(zn) means “terms no bigger than zn.” This gives us a convenient
shorthand for manipulating series. For example,
sin z = z −z3
6 + O(z5)
1
1 −z = 1 + O(z)
The notation o(zn) means “terms smaller that zn.” For example,
cos z = 1 + o(1)
ez = 1 + z + o(z)
1163

Example 23.1.1 Consider the equation
w′′(z) −3w′(z) + 2w(z) = 0.
The general solution to this constant coeﬃcient equation is
w = c1 ez +c2 e2z .
The functions ez and e2z are analytic in the ﬁnite complex plane. Recall that a function is analytic at a point z0 if and
only if the function has a Taylor series about z0 with a nonzero radius of convergence. If we substitute the Taylor series
expansions about z = 0 of ez and e2z into the general solution, we obtain
w = c1
∞
X
n=0
zn
n! + c2
∞
X
n=0
2nzn
n! .
Thus we have a series solution of the diﬀerential equation.
Alternatively, we could try substituting a Taylor series into the diﬀerential equation and solving for the coeﬃcients.
Substituting w = P∞
n=0 anzn into the diﬀerential equation yields
d2
dz2
∞
X
n=0
anzn −3 d
dz
∞
X
n=0
anzn + 2
∞
X
n=0
anzn = 0
∞
X
n=2
n(n −1)anzn−2 −3
∞
X
n=1
nanzn−1 + 2
∞
X
n=0
anzn = 0
∞
X
n=0
(n + 2)(n + 1)an+2zn −3
∞
X
n=0
(n + 1)an+1zn + 2
∞
X
n=0
anzn = 0
∞
X
n=0
h
(n + 2)(n + 1)an+2 −3(n + 1)an+1 + 2an
i
zn = 0.
Equating powers of z, we obtain the diﬀerence equation
(n + 2)(n + 1)an+2 −3(n + 1)an+1 + 2an = 0,
n ≥0.
1164

We see that an = 1/n! is one solution since
(n + 2)(n + 1)
(n + 2)!
−3 n + 1
(n + 1)! + 2 1
n! = 1 −3 + 2
n!
= 0.
We use reduction of order for diﬀerence equations to ﬁnd the other solution. Substituting an = bn/n! into the diﬀerence
equation yields
(n + 2)(n + 1)
bn+2
(n + 2)! −3(n + 1)
bn+1
(n + 1)! + 2bn
n! = 0
bn+2 −3bn+1 + 2bn = 0.
At ﬁrst glance it appears that we have not reduced the order of the diﬀerence equation. However writing this equation
in terms of discrete derivatives,
D2bn −Dbn = 0
we see that this is a ﬁrst order diﬀerence equation for Dbn. Since this is a constant coeﬃcient diﬀerence equation we
substitute bn = rn into the equation to obtain an algebraic equation for r.
r2 −3r + 2 = (r −1)(r −2) = 0
Thus the two solutions are bn = 1nb0 and bn = 2nb0. Only bn = 2nb0 will give us a second independent solution for an.
Thus the two solutions for an are
an = a0
n!
and
an = 2na0
n! .
Thus we can write the general solution to the diﬀerential equation as
w = c1
∞
X
n=0
zn
n! + c2
∞
X
n=0
2nzn
n! .
We recognize these two sums as the Taylor expansions of ez and e2z. Thus we obtain the same result as we did solving
the diﬀerential equation directly.
1165

Of course it would be pretty silly to go through all the grunge involved in developing a series expansion of the solution
in a problem like Example 23.1.1 since we can solve the problem exactly. However if we could not solve a diﬀerential
equation, then having a Taylor series expansion of the solution about a point z0 would be useful in determining the
behavior of the solutions near that point.
For this method of substituting a Taylor series into the diﬀerential equation to be useful we have to know at what
points the solutions are analytic. Let’s say we were considering a second order diﬀerential equation whose solutions
were
w1 = 1
z,
and
w2 = log z.
Trying to ﬁnd a Taylor series expansion of the solutions about the point z = 0 would fail because the solutions are not
analytic at z = 0. This brings us to two important questions.
1. Can we tell if the solutions to a linear diﬀerential equation are analytic at a point without knowing the solutions?
2. If there are Taylor series expansions of the solutions to a diﬀerential equation, what are the radii of convergence
of the series?
In order to answer these questions, we will introduce the concept of an ordinary point. Consider the nth order linear
homogeneous equation
dnw
dzn + pn−1(z)dn−1w
dzn−1 + · · · + p1(z)dw
dz + p0(z)w = 0.
If each of the coeﬃcient functions pi(z) are analytic at z = z0 then z0 is an ordinary point of the diﬀerential equation.
For reasons of typography we will restrict our attention to second order equations and the point z0 = 0 for a while.
The generalization to an nth order equation will be apparent. Considering the point z0 ̸= 0 is only trivially more general
as we could introduce the transformation z −z0 →z to move the point to the origin.
In the chapter on ﬁrst order diﬀerential equations we showed that the solution is analytic at ordinary points. One
would guess that this remains true for higher order equations. Consider the second order equation
y′′ + p(z)y′ + q(z)y = 0,
where p and q are analytic at the origin.
p(z) =
∞
X
n=0
pnzn,
and
q(z) =
∞
X
n=0
qnzn
1166

Assume that one of the solutions is not analytic at the origin and behaves like zα at z = 0 where α ̸= 0, 1, 2, . . .. That
is, we can approximate the solution with w(z) = zα + o(zα). Let’s substitute w = zα + o(zα) into the diﬀerential
equation and look at the lowest power of z in each of the terms.

α(α −1)zα−2 + o(zα−2)

+

αzα−1 + o(zα−1)

∞
X
n=0
pnzn +

zα + o(zα)

∞
X
n=0
qnzn = 0.
We see that the solution could not possibly behave like zα, α ̸= 0, 1, 2, · · · because there is no term on the left to
cancel out the zα−2 term. The terms on the left side could not add to zero.
You could also check that a solution could not possibly behave like log z at the origin. Though we will not prove
it, if z0 is an ordinary point of a homogeneous diﬀerential equation, then all the solutions are analytic at the point z0.
Since the solution is analytic at z0 we can expand it in a Taylor series.
Now we are prepared to answer our second question. From complex variables, we know that the radius of convergence
of the Taylor series expansion of a function is the distance to the nearest singularity of that function. Since the solutions
to a diﬀerential equation are analytic at ordinary points of the equation, the series expansion about an ordinary point
will have a radius of convergence at least as large as the distance to the nearest singularity of the coeﬃcient functions.
Example 23.1.2 Consider the equation
w′′ +
1
cos zw′ + z2w = 0.
If we expand the solution to the diﬀerential equation in Taylor series about z = 0, the radius of convergence will be at
least π/2. This is because the coeﬃcient functions are analytic at the origin, and the nearest singularities of 1/ cos z
are at z = ±π/2.
23.1.1
Taylor Series Expansion for a Second Order Diﬀerential Equation
Consider the diﬀerential equation
w′′ + p(z)w′ + q(z)w = 0
1167

where p(z) and q(z) are analytic in some neighborhood of the origin.
p(z) =
∞
X
n=0
pnzn
and
q(z) =
∞
X
n=0
qnzn
We substitute a Taylor series and it’s derivatives
w =
∞
X
n=0
anzn
w′ =
∞
X
n=1
nznzn−1 =
∞
X
n=0
(n + 1)an+1zn
w′′ =
∞
X
n=2
n(n −1)anzn−2 =
∞
X
n=0
(n + 2)(n + 1)an+2zn
into the diﬀerential equation to obtain
∞
X
n=0
(n + 2)(n + 1)an+2zn +
 ∞
X
n=0
pnzn
!  ∞
X
n=0
(n + 1)an+1zn
!
+
 ∞
X
n=0
qnzn
!  ∞
X
n=0
anzn
!
= 0
∞
X
n=0
(n + 2)(n + 1)an+2zn +
∞
X
n=0
 
n
X
m=0
(m + 1)am+1pn−m
!
zn +
∞
X
n=0
 
n
X
m=0
amqn−m
!
zn = 0
∞
X
n=0
"
(n + 2)(n + 1)an+2 +
n
X
m=0
 (m + 1)am+1pn−m + amqn−m

#
zn = 0.
1168

Equating coeﬃcients of powers of z,
(n + 2)(n + 1)an+2 +
n
X
m=0
 (m + 1)am+1pn−m + amqn−m

= 0
for n ≥0.
We see that a0 and a1 are arbitrary and the rest of the coeﬃcients are determined by the recurrence relation
an+2 = −
1
(n + 1)(n + 2)
n
X
m=0
((m + 1)am+1pn−m + amqn−m)
for n ≥0.
Example 23.1.3 Consider the problem
y′′ +
1
cos xy′ + ex y = 0,
y(0) = y′(0) = 1.
Let’s expand the solution in a Taylor series about the origin.
y(x) =
∞
X
n=0
anxn
Since y(0) = a0 and y′(0) = a1, we see that a0 = a1 = 1. The Taylor expansions of the coeﬃcient functions are
1
cos x = 1 + O(x),
and
ex = 1 + O(x).
Now we can calculate a2 from the recurrence relation.
a2 = −1
1 · 2
0
X
m=0
((m + 1)am+1p0−m + amq0−m)
= −1
2(1 · 1 · 1 + 1 · 1)
= −1
1169

Thus the solution to the problem is
y(x) = 1 + x −x2 + O(x3).
In Figure 23.1 the numerical solution is plotted in a solid line and the sum of the ﬁrst three terms of the Taylor series
is plotted in a dashed line.
0.2
0.4
0.6
0.8
1
1.2
1.4
0.7
0.8
0.9
1.1
1.2
Figure 23.1: Plot of the Numerical Solution and the First Three Terms in the Taylor Series.
The general recurrence relation for the an’s is useful if you only want to calculate the ﬁrst few terms in the Taylor
expansion. However, for many problems substituting the Taylor series for the coeﬃcient functions into the diﬀerential
equation will enable you to ﬁnd a simpler form of the solution. We consider the following example to illustrate this
point.
1170

Example 23.1.4 Develop a series expansion of the solution to the initial value problem
w′′ +
1
(z2 + 1)w = 0,
w(0) = 1,
w′(0) = 0.
Solution using the General Recurrence Relation.
The coeﬃcient function has the Taylor expansion
1
1 + z2 =
∞
X
n=0
(−1)nz2n.
From the initial condition we obtain a0 = 1 and a1 = 0. Thus we see that the solution is
w =
∞
X
n=0
anzn,
where
an+2 = −
1
(n + 1)(n + 2)
n
X
m=0
amqn−m
and
qn =
(
0
for odd n
(−1)(n/2)
for even n.
Although this formula is ﬁne if you only want to calculate the ﬁrst few an’s, it is just a tad unwieldy to work with.
Let’s see if we can get a better expression for the solution.
Substitute the Taylor Series into the Diﬀerential Equation.
Substituting a Taylor series for w yields
d2
dz2
∞
X
n=0
anzn +
1
(z2 + 1)
∞
X
n=0
anzn = 0.
1171

Note that the algebra will be easier if we multiply by z2 + 1. The polynomial z2 + 1 has only two terms, but the Taylor
series for 1/(z2 + 1) has an inﬁnite number of terms.
(z2 + 1) d2
dz2
∞
X
n=0
anzn +
∞
X
n=0
anzn = 0
∞
X
n=2
n(n −1)anzn +
∞
X
n=2
n(n −1)anzn−2 +
∞
X
n=0
anzn = 0
∞
X
n=0
n(n −1)anzn +
∞
X
n=0
(n + 2)(n + 1)an+2zn +
∞
X
n=0
anzn = 0
∞
X
n=0
h
(n + 2)(n + 1)an+2 + n(n −1)an + an
i
zn = 0
Equating powers of z gives us the diﬀerence equation
an+2 = −
n2 −n + 1
(n + 2)(n + 1)an,
for n ≥0.
From the initial conditions we see that a0 = 1 and a1 = 0. All of the odd terms in the series will be zero. For the
even terms, it is easier to reformulate the problem with the change of variables bn = a2n. In terms of bn the diﬀerence
equation is
bn+1 = −(2n)2 −2n + 1
(2n + 2)(2n + 1)bn,
b0 = 1.
This is a ﬁrst order diﬀerence equation with the solution
bn =
n
Y
j=0

−
4j2 −2j + 1
(2j + 2)(2j + 1)

.
Thus we have that
an =
(Qn/2
j=0

−
4j2−2j+1
(2j+2)(2j+1)

for even n,
0
for odd n.
1172

Note that the nearest singularities of 1/(z2 +1) in the complex plane are at z = ±i. Thus the radius of convergence
must be at least 1. Applying the ratio test, the series converges for values of |z| such that
lim
n→∞

an+2zn+2
anzn
 < 1
lim
n→∞
−
n2 −n + 1
(n + 2)(n + 1)
 |z|2 < 1
|z|2 < 1.
The radius of convergence is 1.
The ﬁrst few terms in the Taylor expansion are
w = 1 −1
2z2 + 1
8z4 −13
240z6 + · · · .
In Figure 23.2 the plot of the ﬁrst two nonzero terms is shown in a short dashed line, the plot of the ﬁrst four
nonzero terms is shown in a long dashed line, and the numerical solution is shown in a solid line.
In general, if the coeﬃcient functions are rational functions, that is they are fractions of polynomials, multiplying
the equations by the quotient will reduce the algebra involved in ﬁnding the series solution.
Example 23.1.5 If we were going to ﬁnd the Taylor series expansion about z = 0 of the solution to
w′′ +
z
1 + zw′ +
1
1 −z2w = 0,
we would ﬁrst want to multiply the equation by 1 −z2 to obtain
(1 −z2)w′′ + z(1 −z)w′′ + w = 0.
1173

0.2
0.4
0.6
0.8
1
1.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Figure 23.2: Plot of the solution and approximations.
Example 23.1.6 Find the series expansions about z = 0 of the fundamental set of solutions for
w′′ + z2w = 0.
Recall that the fundamental set of solutions {w1, w2} satisfy
w1(0) = 1
w2(0) = 0
w′
1(0) = 0
w′
2(0) = 1.
Thus if
w1 =
∞
X
n=0
anzn
and
w2 =
∞
X
n=0
bnzn,
1174

then the coeﬃcients must satisfy
a0 = 1,
a1 = 0,
and
b0 = 0,
b1 = 1.
Substituting the Taylor expansion w = P∞
n=0 cnzn into the diﬀerential equation,
∞
X
n=2
n(n −1)cnzn−2 +
∞
X
n=0
cnzn+2 = 0
∞
X
n=0
(n + 2)(n + 1)cn+2zn +
∞
X
n=2
cn−2zn = 0
2c2 + 6c3z +
∞
X
n=2
h
(n + 2)(n + 1)cn+2 + cn−2
i
zn = 0
Equating coeﬃcients of powers of z,
z0 :
c2 = 0
z1 :
c3 = 0
zn :
(n + 2)(n + 1)cn+2 + cn−2 = 0,
for n ≥2
cn+4 = −
cn
(n + 4)(n + 3)
For our ﬁrst solution we have the diﬀerence equation
a0 = 1, a1 = 0, a2 = 0, a3 = 0,
an+4 = −
an
(n + 4)(n + 3).
For our second solution,
b0 = 0, b1 = 1, b2 = 0, b3 = 0,
bn+4 = −
bn
(n + 4)(n + 3).
1175

The ﬁrst few terms in the fundamental set of solutions are
w1 = 1 −1
12z4 +
1
672z8 −· · · ,
w2 = z −1
20z5 +
1
1440z9 −· · · .
In Figure 23.3 the ﬁve term approximation is graphed in a coarse dashed line, the ten term approximation is graphed
in a ﬁne dashed line, and the numerical solution of w1 is graphed in a solid line. The same is done for w2.
1
2
3
4
5
6
-1
-0.5
0.5
1
1.5
1
2
3
4
5
6
-1
-0.5
0.5
1
1.5
Figure 23.3: The graph of approximations and numerical solution of w1 and w2.
1176

Result 23.1.1 Consider the nth order linear homogeneous equation
dnw
dzn + pn−1(z)dn−1w
dzn−1 + · · · + p1(z)dw
dz + p0(z)w = 0.
If each of the coeﬃcient functions pi(z) are analytic at z = z0 then z0 is an ordinary point
of the diﬀerential equation. The solution is analytic in some region containing z0 and can
be expanded in a Taylor series. The radius of convergence of the series will be at least the
distance to the nearest singularity of the coeﬃcient functions in the complex plane.
23.2
Regular Singular Points of Second Order Equations
Consider the diﬀerential equation
w′′ + p(z)
z −z0
w′ +
q(z)
(z −z0)2w = 0.
If z = z0 is not an ordinary point but both p(z) and q(z) are analytic at z = z0 then z0 is a regular singular point of
the diﬀerential equation. The following equations have a regular singular point at z = 0.
• w′′ + 1
zw′ + z2w = 0
• w′′ +
1
sin zw′ −w = 0
• w′′ −zw′ +
1
z sin zw = 0
Concerning regular singular points of second order linear equations there is good news and bad news.
The Good News.
We will ﬁnd that with the use of the Frobenius method we can always ﬁnd series expansions of
two linearly independent solutions at a regular singular point. We will illustrate this theory with several examples.
1177

The Bad News.
Instead of a tidy little theory like we have for ordinary points, the solutions can be of several
diﬀerent forms. Also, for some of the problems the algebra can get pretty ugly.
Example 23.2.1 Consider the equation
w′′ + 3(1 + z)
16z2
w = 0.
We wish to ﬁnd series solutions about the point z = 0. First we try a Taylor series w = P∞
n=0 anzn. Substituting this
into the diﬀerential equation,
z2
∞
X
n=2
n(n −1)anzn−2 + 3
16(1 + z)
∞
X
n=0
anzn = 0
∞
X
n=0
n(n −1)anzn + 3
16
∞
X
n=0
anzn + 3
16
∞
X
n=1
an+1zn = 0.
Equating powers of z,
z0 :
a0 = 0
zn :

n(n −1) + 3
16

an + 3
16an+1 = 0
an+1 =
16
3 n(n −1) + 1

an.
This diﬀerence equation has the solution an = 0 for all n. Thus we have obtained only the trivial solution to the
diﬀerential equation. We must try an expansion of a more general form. We recall that for regular singular points of
ﬁrst order equations we can always ﬁnd a solution in the form of a Frobenius series w = zα P∞
n=0 anzn, a0 ̸= 0. We
1178

substitute this series into the diﬀerential equation.
z2
∞
X
n=0

α(α −1) + 2αn + n(n −1)

anzn+α−2 + 3
16(1 + z)zα
∞
X
n=0
anzn = 0
∞
X
n=0

α(α −1) + 2n + n(n −1)

anzn + 3
16
∞
X
n=0
anzn + 3
16
∞
X
n=1
an−1zn = 0
Equating the z0 term to zero yields the equation

α(α −1) + 3
16

a0 = 0.
Since we have assumed that a0 ̸= 0, the polynomial in α must be zero. The two roots of the polynomial are
α1 = 1 +
p
1 −3/4
2
= 3
4,
α2 = 1 −
p
1 −3/4
2
= 1
4.
Thus our two series solutions will be of the form
w1 = z3/4
∞
X
n=0
anzn,
w2 = z1/4
∞
X
n=0
bnzn.
Substituting the ﬁrst series into the diﬀerential equation,
∞
X
n=0

−3
16 + 2n + n(n −1) + 3
16

anzn + 3
16
∞
X
n=1
an−1zn = 0.
Equating powers of z, we see that a0 is arbitrary and
an = −
3
16n(n + 1)an−1
for n ≥1.
1179

This diﬀerence equation has the solution
an = a0
n
Y
j=1

−
3
16j(j + 1)

= a0

−3
16
n
n
Y
j=1
1
j(j + 1)
= a0

−3
16
n
1
n!(n + 1)!
for n ≥1.
Substituting the second series into the diﬀerential equation,
∞
X
n=0

−3
16 + 2n + n(n −1) + 3
16

bnzn + 3
16
∞
X
n=1
bn−1zn = 0.
We see that the diﬀerence equation for bn is the same as the equation for an. Thus we can write the general solution
to the diﬀerential equation as
w = c1z3/4
 
1 +
∞
X
n=1

−3
16
n
1
n!(n + 1)!zn
!
+ c2z1/4
 
1 +
∞
X
n=1

−3
16
n
1
n!(n + 1)!zn
!
 c1z3/4 + c2z1/4
 
1 +
∞
X
n=1

−3
16
n
1
n!(n + 1)!zn
!
.
23.2.1
Indicial Equation
Now let’s consider the general equation for a regular singular point at z = 0
w′′ + p(z)
z w′ + q(z)
z2 w = 0.
1180

Since p(z) and q(z) are analytic at z = 0 we can expand them in Taylor series.
p(z) =
∞
X
n=0
pnzn,
q(z) =
∞
X
n=0
qnzn
Substituting a Frobenius series w = zα P∞
n=0 anzn, a0 ̸= 0 and the Taylor series for p(z) and q(z) into the diﬀerential
equation yields
∞
X
n=0
h
(α + n)(α + n −1)
i
anzn +
 ∞
X
n=0
pnzn
!  ∞
X
n=0
(α + n)anzn
!
+
 ∞
X
n=0
qnzn
!  ∞
X
n=0
anzn
!
= 0
∞
X
n=0
h
(α + n)2 −(α + n) + p0(α + n) + q0
i
anzn
+
 ∞
X
n=1
pnzn
!  ∞
X
n=0
(α + n)anzn
!
+
 ∞
X
n=1
qnzn
!  ∞
X
n=0
anzn
!
= 0
∞
X
n=0
h
(α + n)2 + (p0 −1)(αn) + q0
i
anzn +
∞
X
n=1
 n−1
X
j=0
(α + j)ajpn−j
!
zn +
∞
X
n=1
 n−1
X
j=0
ajqn−j
!
zn = 0
Equating powers of z,
z0 :
h
α2 + (p0 −1)α + q0
i
a0 = 0
zn :
h
(α + n)2 + (p0 −1)(α + n) + q0
i
an = −
n−1
X
j=0
h
(α + j)pn−j + qn−j
i
aj.
Let
I(α) = α2 + (p0 −1)α + q0 = 0.
1181

This is known as the indicial equation. The indicial equation gives us the form of the solutions. The equation for
a0 is I(α)a0 = 0. Since we assumed that a0 is nonzero, I(α) = 0. Let the two roots of I(α) be α1 and α2 where
ℜ(α1) ≥ℜ(α2).
Rewriting the diﬀerence equation for an(α),
I(α + n)an(α) = −
n−1
X
j=0
h
(α + j)pn−j + qn−j
i
aj(α)
for n ≥1.
(23.1)
If the roots are distinct and do not diﬀer by an integer then we can use Equation 23.1 to solve for an(α1) and
an(α2), which will give us the two solutions
w1 = zα1
∞
X
n=0
an(α1)zn,
and
w2 = zα2
∞
X
n=0
an(α2)zn.
If the roots are not distinct, α1 = α2, we will only have one solution and will have to generate another. If the roots
diﬀer by an integer, α1 −α2 = N, there is one solution corresponding to α1, but when we try to solve Equation 23.1
for an(α2), we will encounter the equation
I(α2 + N)aN(α2) = I(α1)aN(α2) = 0 · aN(α2) = −
N−1
X
j=0
h
(α + n)pn−j + qn−j
i
aj(α2).
If the right side of the equation is nonzero, then aN(α2) is undeﬁned. On the other hand, if the right side is zero then
aN(α2) is arbitrary. The rest of this section is devoted to considering the cases α1 = α2 and α1 −α2 = N.
23.2.2
The Case: Double Root
Consider a second order equation L[w] = 0 with a regular singular point at z = 0. Suppose the indicial equation has a
double root.
I(α) = (α −α1)2 = 0
1182

One solution has the form
w1 = zα1
∞
X
n=0
anzn.
In order to ﬁnd the second solution, we will diﬀerentiate with respect to the parameter, α. Let an(α) satisfy Equa-
tion 23.1 Substituting the Frobenius expansion into the diﬀerential equation,
L
"
zα
∞
X
n=0
an(α)zn
#
= 0.
Setting α = α1 will make the left hand side of the equation zero. Diﬀerentiating this equation with respect to α,
∂
∂αL
"
zα
∞
X
n=0
an(α)zn
#
= 0.
Interchanging the order of diﬀerentiation,
L
"
log z zα
∞
X
n=0
an(α)zn + zα
∞
X
n=0
dan(α)
dα
zn
#
= 0.
Since setting α = α1 will make the left hand side of this equation zero, the second linearly independent solution is
w2 = log z zα1
∞
X
n=0
an(α1)zn + zα1
∞
X
n=0
dan(α)
dα

α=α1
zn
w2 = w1 log z + zα1
∞
X
n=0
a′
n(α1)zn.
1183

Example 23.2.2 Consider the diﬀerential equation
w′′ + 1 + z
4z2 w = 0.
There is a regular singular point at z = 0. The indicial equation is
α(α −1) + 1
4 =

α −1
2
2
= 0.
One solution will have the form
w1 = z1/2
∞
X
n=0
anzn,
a0 ̸= 0.
Substituting the Frobenius expansion
zα
∞
X
n=0
an(α)zn
into the diﬀerential equation yields
z2w′′ + 1
4(1 + z)w = 0
∞
X
n=0

α(α −1) + 2αn + n(n −1)

an(α)zn+α + 1
4
∞
X
n=0
an(α)zn+α + 1
4
∞
X
n=0
an(α)zn+α+1 = 0.
Divide by zα and adjust the summation indices.
∞
X
n=0
[α(α −1) + 2αn + n(n −1)] an(α)zn + 1
4
∞
X
n=0
an(α)zn + 1
4
∞
X
n=1
an−1(α)zn = 0

α(α −1)a0 + 1
4

a0 +
∞
X
n=1

α(α −1) + 2n + n(n −1) + 1
4

an(α) + 1
4an−1(α)

zn = 0
1184

Equating the coeﬃcient of z0 to zero yields I(α)a0 = 0. Equating the coeﬃcients of zn to zero yields the diﬀerence
equation

α(α −1) + 2n + n(n −1) + 1
4

an(α) + 1
4an−1(α) = 0
an(α) = −
n(n + 1)
4
+ α(α −1)
4
+ 1
16

an−1(α).
The ﬁrst few an’s are
a0,
−

α(α −1) + 9
16

a0,

α(α −1) + 25
16
 
α(α −1) + 9
16

a0, . . .
Setting α = 1/2, the coeﬃcients for the ﬁrst solution are
a0,
−5
16a0,
105
16 a0,
. . .
The second solution has the form
w2 = w1 log z + z1/2
∞
X
n=0
a′
n(1/2)zn.
Diﬀerentiating the an(α),
da0
dα = 0,
da1(α)
dα
= −(2α −1)a0,
da2(α)
dα
= (2α −1)

α(α −1) + 9
16

+

α(α −1) + 25
16

a0,
. . .
Setting α = 1/2 in this equation yields
a′
0 = 0,
a′
1(1/2) = 0,
a′
2(1/2) = 0,
. . .
Thus the second solution is
w2 = w1 log z.
1185

The ﬁrst few terms in the general solution are
(c1 + c2 log z)

1 −5
16z + 105
16 z2 −· · ·

.
23.2.3
The Case: Roots Diﬀer by an Integer
Consider the case in which the roots of the indicial equation α1 and α2 diﬀer by an integer. (α1 −α2 = N) Recall the
equation that determines an(α)
I(α + n)an =
h
(α + n)2 + (p0 −1)(α + n) + q0
i
an = −
n−1
X
j=0
h
(α + j)pn−j + qn−j
i
aj.
When α = α2 the equation for aN is
I(α2 + N)aN(α2) = 0 · aN(α2) = −
N−1
X
j=0
h
(α + j)pN−j + qN−j
i
aj.
If the right hand side of this equation is zero, then aN is arbitrary. There will be two solutions of the Frobenius form.
w1 = zα1
∞
X
n=0
an(α1)zn
and
w2 = zα2
∞
X
n=0
an(α2)zn.
If the right hand side of the equation is nonzero then aN(α2) will be undeﬁned. We will have to generate the second
solution. Let
w(z, α) = zα
∞
X
n=0
an(α)zn,
where an(α) satisﬁes the recurrence formula. Substituting this series into the diﬀerential equation yields
L[w(z, α)] = 0.
1186

We will multiply by (α −α2), diﬀerentiate this equation with respect to α and then set α = α2. This will generate a
linearly independent solution.
∂
∂αL[(α −α2)w(z, α)] = L
 ∂
∂α(α −α2)w(z, α)

= L
"
∂
∂α(α −α2)zα
∞
X
n=0
an(α)zn
#
= L
"
log z zα
∞
X
n=0
(α −α2)an(α)zn + zα
∞
X
n=0
d
dα[(α −α2)an(α)]zn
#
Setting α = α2 with make this expression zero, thus
log z zα
∞
X
n=0
lim
α→α2 {(α −α2)an(α)} zn + zα2
∞
X
n=0
lim
α→α2
 d
dα[(α −α2)an(α)]

zn
is a solution. Now let’s look at the ﬁrst term in this solution
log z zα
∞
X
n=0
lim
α→α2 {(α −α2)an(α)} zn.
The ﬁrst N terms in the sum will be zero. That is because a0, . . . , aN−1 are ﬁnite, so multiplying by (α −α2) and
taking the limit as α →α2 will make the coeﬃcients vanish. The equation for aN(α) is
I(α + N)aN(α) = −
N−1
X
j=0
h
(α + j)pN−j + qN−j
i
aj(α).
1187

Thus the coeﬃcient of the N th term is
lim
α→α2(α −α2)aN(α) = −lim
α→α2
"
(α −α2)
I(α + N)
N−1
X
j=0
h
(α + j)pN−j + qN−j
i
aj(α)
#
= −lim
α→α2
"
(α −α2)
(α + N −α1)(α + N −α2)
N−1
X
j=0
h
(α + j)pN−j + qN−j
i
aj(α)
#
Since α1 = α2 + N, limα→α2
α−α2
α+N−α1 = 1.
= −
1
(α1 −α2)
N−1
X
j=0
h
(α2 + j)pN−j + qN−j
i
aj(α2).
Using this you can show that the ﬁrst term in the solution can be written
d−1 log z w1,
where d−1 is a constant. Thus the second linearly independent solution is
w2 = d−1 log z w1 + zα2
∞
X
n=0
dnzn,
where
d−1 = −1
a0
1
(α1 −α2)
N−1
X
j=0
h
(α2 + j)pN−j + qN−j
i
aj(α2)
and
dn = lim
α→α2
 d
dα
h
(α −α2)an(α)
i
for n ≥0.
1188

Example 23.2.3 Consider the diﬀerential equation
w′′ +

1 −2
z

w′ + 2
z2w = 0.
The point z = 0 is a regular singular point. In order to ﬁnd series expansions of the solutions, we ﬁrst calculate the
indicial equation. We can write the coeﬃcient functions in the form
p(z)
z
= 1
z(−2 + z),
and
q(z)
z2
= 1
z2(2).
Thus the indicial equation is
α2 + (−2 −1)α + 2 = 0
(α −1)(α −2) = 0.
The First Solution.
The ﬁrst solution will have the Frobenius form
w1 = z2
∞
X
n=0
an(α1)zn.
Substituting a Frobenius series into the diﬀerential equation,
z2w′′ + (z2 −2z)w′ + 2w = 0
∞
X
n=0
(n + α)(n + α −1)zn+α + (z2 −2z)
∞
X
n=0
(n + α)zn+α−1 + 2
∞
X
n=0
anzn = 0
[α2 −3α + 2]a0 +
∞
X
n=1
h
(n + α)(n + α −1)an + (n + α −1)an−1 −2(n + α)an + 2an
i
zn = 0.
Equating powers of z,
h
(n + α)(n + α −1) −2(n + α) + 2
i
an = −(n + α −1)an−1
an = −
an−1
n + α −2.
1189

Setting α = α1 = 2, the recurrence relation becomes
an(α1) = −an−1(α1)
n
= a0
(−1)n
n!
.
The ﬁrst solution is
w1 = a0
∞
X
n=0
(−1)n
n!
zn = a0 e−z .
The Second Solution.
The equation for a1(α2) is
0 · a1(α2) = 2a0.
Since the right hand side of this equation is not zero, the second solution will have the form
w2 = d−1 log z w1 + zα2
∞
X
n=0
lim
α→α2
 d
dα[(α −α2)an(α)]

zn
First we will calculate d−1 as we deﬁned it previously.
d−1 = −1
a0
1
2 −1a0 = −1.
The expression for an(α) is
an(α) =
(−1)na0
(α + n −2)(α + n −1) · · · (α −1).
1190

The ﬁrst few an(α) are
a1(α) = −
a0
α −1
a2(α) =
a0
α(α −1)
a3(α) = −
a0
(α + 1)α(α −1).
We would like to calculate
dn = lim
α→1
 d
dα
h
(α −1)an(α)
i
.
1191

The ﬁrst few dn are
d0 = lim
α→1
 d
dα
h
(α −1)a0
i
= a0
d1 = lim
α→1
 d
dα

(α −1)

−
a0
α −1

= lim
α→1
 d
dα
h
−a0
i
= 0
d2 = lim
α→1
 d
dα

(α −1)

a0
α(α −1)

= lim
α→1
 d
dα
ha0
α
i
= −a0
d3 = lim
α→1
 d
dα

(α −1)

−
a0
(α + 1)α(α −1)

= lim
α→1
 d
dα

−
a0
(α + 1)α

= 3
4a0.
It will take a little work to ﬁnd the general expression for dn. We will need the following relations.
Γ(n) = (n −1)!,
Γ′(z) = Γ(z)ψ(z),
ψ(n) = −γ +
n−1
X
k=1
1
k.
1192

See the chapter on the Gamma function for explanations of these equations.
dn = lim
α→1
 d
dα

(α −1)
(−1)na0
(α + n −2)(α + n −1) · · · (α −1)

= lim
α→1
 d
dα

(−1)na0
(α + n −2)(α + n −1) · · · (α)

= lim
α→1
 d
dα
(−1)na0Γ(α)
Γ(α + n −1)

= (−1)na0 lim
α→1

Γ(α)ψ(α)
Γ(α + n −1) −Γ(α)ψ(α + n −1)
Γ(α + n −1)

= (−1)na0 lim
α→1
Γ(α)[ψ(α) −ψ(α + n −1)]
Γ(α + n −1)

= (−1)na0
ψ(1) −ψ(n)
(n −1)!
= (−1)n+1a0
(n −1)!
n−1
X
k=0
1
k
Thus the second solution is
w2 = −log z w1 + z
∞
X
n=0
 
(−1)n+1a0
(n −1)!
n−1
X
k=0
1
k
!
zn.
The general solution is
w = c1 e−z −c2 log z e−z +c2z
∞
X
n=0
 
(−1)n+1
(n −1)!
n−1
X
k=0
1
k
!
zn.
We see that even in problems that are chosen for their simplicity, the algebra involved in the Frobenius method can
be pretty involved.
1193

Example 23.2.4 Consider a series expansion about the origin of the equation
w′′ + 1 −z
z
w′ −1
z2w = 0.
The indicial equation is
α2 −1 = 0
α = ±1.
Substituting a Frobenius series into the diﬀerential equation,
z2
∞
X
n=0
(n + α)(n + α −1)anzn−2 + (z −z2)
∞
X
n=0
(n + α)anzn−1 −
∞
X
n=0
anzn = 0
∞
X
n=0
(n + α)(n + α −1)anzn +
∞
X
n=0
(n + α)anzn −
∞
X
n=1
(n + α −1)an−1zn −
∞
X
n=0
anzn = 0
h
α(α −1) + α −1
i
a0 +
∞
X
n=1
h
n + α)(n + α −1)an + (n + α −1)an −(n + α −1)an−1
i
zn = 0.
Equating powers of z to zero,
an(α) = an−1(α)
n + α + 1.
We know that the ﬁrst solution has the form
w1 = z
∞
X
n=0
anzn.
Setting α = 1 in the reccurence formula,
an = an−1
n + 2 =
2a0
(n + 2)!.
1194

Thus the ﬁrst solution is
w1 = z
∞
X
n=0
2a0
(n + 2)!zn
= 2a0
1
z
∞
X
n=0
zn+2
(n + 2)!
= 2a0
z
 ∞
X
n=0
zn
n! −1 −z
!
= 2a0
z (ez −1 −z).
Now to ﬁnd the second solution. Setting α = −1 in the reccurence formula,
an = an−1
n
= a0
n!.
We see that in this case there is no trouble in deﬁning a2(α2). The second solution is
w2 = a0
z
∞
X
n=0
zn
n! = a0
z ez .
Thus we see that the general solution is
w = c1
z (ez −1 −z) + c2
z ez
w = d1
z ez +d2

1 + 1
z

.
1195

23.3
Irregular Singular Points
If a point z0 of a diﬀerential equation is not ordinary or regular singular, then it is an irregular singular point. At least
one of the solutions at an irregular singular point will not be of the Frobenius form. We will examine how to obtain
series expansions about an irregular singular point in the chapter on asymptotic expansions.
23.4
The Point at Inﬁnity
If we want to determine the behavior of a function f(z) at inﬁnity, we can make the transformation ζ = 1/z and
examine the point ζ = 0.
Example 23.4.1 Consider the behavior of f(z) = sin z at inﬁnity. This is the same as considering the point ζ = 0 of
sin(1/ζ), which has the series expansion
sin
1
ζ

=
∞
X
n=0
(−1)n
(2n + 1)!ζ2n+1.
Thus we see that the point ζ = 0 is an essential singularity of sin(1/ζ). Hence sin z has an essential singularity at
z = ∞.
Example 23.4.2 Consider the behavior at inﬁnity of z e1/z. We make the transformation ζ = 1/z.
1
ζ eζ = 1
ζ
∞
X
n=0
ζn
n!
Thus z e1/z has a pole of order 1 at inﬁnity.
1196

In order to classify the point at inﬁnity of a diﬀerential equation in w(z), we apply the transformation ζ = 1/z,
u(ζ) = w(z). We write the derivatives with respect to z in terms of ζ.
z = 1
ζ
dz = −1
ζ2dζ
d
dz = −ζ2 d
dζ
d2
dz2 = −ζ2 d
dζ

−ζ2 d
dζ

= ζ4 d2
dζ2 + 2ζ3 d
dζ
Now we apply the transformation to the diﬀerential equation.
w′′ + p(z)w′ + q(z)w = 0
ζ4u′′ + 2ζ3u′ + p(1/ζ)(−ζ2)u′ + q(1/ζ)u = 0
u′′ +
2
ζ −p(1/ζ)
ζ2

u′ + q(1/ζ)
ζ4
u = 0
Example 23.4.3 Classify the singular points of the diﬀerential equation
w′′ + 1
zw′ + 2w = 0.
There is a regular singular point at z = 0. To examine the point at inﬁnity we make the transformation ζ = 1/z,
1197

u(ζ) = w(z).
u′′ +
2
ζ −1
ζ

u′ + 2
ζ4u = 0
u′′ + 1
ζ u′ + 2
ζ4u = 0
Thus we see that the diﬀerential equation for w(z) has an irregular singular point at inﬁnity.
1198

23.5
Exercises
Exercise 23.1 (mathematica/ode/series/series.nb)
f(x) satisﬁes the Hermite equation
d2f
dx2 −2xdf
dx + 2λf = 0.
Construct two linearly independent solutions of the equation as Taylor series about x = 0. For what values of x do the
series converge?
Show that for certain values of λ, called eigenvalues, one of the solutions is a polynomial, called an eigenfunction.
Calculate the ﬁrst four eigenfunctions H0(x), H1(x), H2(x), H3(x), ordered by degree.
Hint, Solution
Exercise 23.2
Consider the Legendre equation
(1 −x2)y′′ −2xy′ + α(α + 1)y = 0.
1. Find two linearly independent solutions in the form of power series about x = 0.
2. Compute the radius of convergence of the series. Explain why it is possible to predict the radius of convergence
without actually deriving the series.
3. Show that if α = 2n, with n an integer and n ≥0, the series for one of the solutions reduces to an even
polynomial of degree 2n.
4. Show that if α = 2n + 1, with n an integer and n ≥0, the series for one of the solutions reduces to an odd
polynomial of degree 2n + 1.
5. Show that the ﬁrst 4 polynomial solutions Pn(x) (known as Legendre polynomials) ordered by their degree and
normalized so that Pn(1) = 1 are
P0 = 1
P1 = x
P2 = 1
2(3x2 −1)
P4 = 1
2(5x3 −3x)
1199

6. Show that the Legendre equation can also be written as
((1 −x2)y′)′ = −α(α + 1)y.
Note that two Legendre polynomials Pn(x) and Pm(x) must satisfy this relation for α = n and α = m respectively.
By multiplying the ﬁrst relation by Pm(x) and the second by Pn(x) and integrating by parts show that Legendre
polynomials satisfy the orthogonality relation
Z 1
−1
Pn(x)Pm(x) dx = 0 if n ̸= m.
If n = m, it can be shown that the value of the integral is 2/(2n + 1). Verify this for the ﬁrst three polynomials
(but you needn’t prove it in general).
Hint, Solution
Exercise 23.3
Find the forms of two linearly independent series expansions about the point z = 0 for the diﬀerential equation
w′′ +
1
sin zw′ + 1 −z
z2 w = 0,
such that the series are real-valued on the positive real axis. Do not calculate the coeﬃcients in the expansions.
Hint, Solution
Exercise 23.4
Classify the singular points of the equation
w′′ +
w′
z −1 + 2w = 0.
Hint, Solution
1200

Exercise 23.5
Find the series expansions about z = 0 for
w′′ + 5
4zw′ + z −1
8z2 w = 0.
Hint, Solution
Exercise 23.6
Find the series expansions about z = 0 of the fundamental solutions of
w′′ + zw′ + w = 0.
Hint, Solution
Exercise 23.7
Find the series expansions about z = 0 of the two linearly independent solutions of
w′′ + 1
2zw′ + 1
zw = 0.
Hint, Solution
Exercise 23.8
Classify the singularity at inﬁnity of the diﬀerential equation
w′′ +
2
z + 3
z2

w′ + 1
z2w = 0.
Find the forms of the series solutions of the diﬀerential equation about inﬁnity that are real-valued when z is real-valued
and positive. Do not calculate the coeﬃcients in the expansions.
Hint, Solution
1201

Exercise 23.9
Consider the second order diﬀerential equation
xd2y
dx2 + (b −x)dy
dx −ay = 0,
where a, b are real constants.
1. Show that x = 0 is a regular singular point. Determine the location of any additional singular points and classify
them. Include the point at inﬁnity.
2. Compute the indicial equation for the point x = 0.
3. By solving an appropriate recursion relation, show that one solution has the form
y1(x) = 1 + ax
b + (a)2x2
(b)22! + · · · + (a)nxn
(b)nn! + · · ·
where the notation (a)n is deﬁned by
(a)n = a(a + 1)(a + 2) · · · (a + n −1),
(a)0 = 1.
Assume throughout this problem that b ̸= n where n is a non-negative integer.
4. Show that when a = −m, where m is a non-negative integer, that there are polynomial solutions to this equation.
Compute the radius of convergence of the series above when a ̸= −m. Verify that the result you get is in accord
with the Frobenius theory.
5. Show that if b = n + 1 where n = 0, 1, 2, . . ., then the second solution of this equation has logarithmic terms.
Indicate the form of the second solution in this case. You need not compute any coeﬃcients.
Hint, Solution
1202

Exercise 23.10
Consider the equation
xy′′ + 2xy′ + 6 ex y = 0.
Find the ﬁrst three non-zero terms in each of two linearly independent series solutions about x = 0.
Hint, Solution
1203

23.6
Hints
Hint 23.1
Hint 23.2
Hint 23.3
Hint 23.4
Hint 23.5
Hint 23.6
Hint 23.7
Hint 23.8
Hint 23.9
Hint 23.10
1204

23.7
Solutions
Solution 23.1
f(x) is a Taylor series about x = 0.
f(x) =
∞
X
n=0
anxn
f ′(x) =
∞
X
n=1
nanxn−1
=
∞
X
n=0
nanxn−1
f ′′(x) =
∞
X
n=2
n(n −1)anxn−2
=
∞
X
n=0
(n + 2)(n + 1)an+2xn
We substitute the Taylor series into the diﬀerential equation.
f ′′(x) −2xf ′(x) + 2λf = 0
∞
X
n=0
(n + 2)(n + 1)an+2xn −2
∞
X
n=0
nanxn + 2λ
∞
X
n=0
anxn
Equating coeﬃcients gives us a diﬀerence equation for an:
(n + 2)(n + 1)an+2 −2nan + 2λan = 0
an+2 = 2
n −λ
(n + 1)(n + 2)an.
1205

The ﬁrst two coeﬃcients, a0 and a1 are arbitrary. The remaining coeﬃcients are determined by the recurrence relation.
We will ﬁnd the fundamental set of solutions at x = 0. That is, for the ﬁrst solution we choose a0 = 1 and a1 = 0; for
the second solution we choose a0 = 0, a1 = 1. The diﬀerence equation for y1 is
an+2 = 2
n −λ
(n + 1)(n + 2)an,
a0 = 1,
a1 = 0,
which has the solution
a2n = 2n Qn
k=0(2(n −k) −λ)
(2n)!
,
a2n+1 = 0.
The diﬀerence equation for y2 is
an+2 = 2
n −λ
(n + 1)(n + 2)an,
a0 = 0,
a1 = 1,
which has the solution
a2n = 0,
a2n+1 = 2n Qn−1
k=0(2(n −k) −1 −λ)
(2n + 1)!
.
A set of linearly independent solutions, (in fact the fundamental set of solutions at x = 0), is
y1(x) =
∞
X
n=0
2n Qn
k=0(2(n −k) −λ)
(2n)!
x2n,
y2(x) =
∞
X
n=0
2n Qn−1
k=0(2(n −k) −1 −λ)
(2n + 1)!
x2n+1.
Since the coeﬃcient functions in the diﬀerential equation do not have any singularities in the ﬁnite complex plane, the
radius of convergence of the series is inﬁnite.
If λ = n is a positive even integer, then the ﬁrst solution, y1, is a polynomial of order n. If λ = n is a positive odd
integer, then the second solution, y2, is a polynomial of order n. For λ = 0, 1, 2, 3, we have
H0(x) = 1
H1(x) = x
H2(x) = 1 −2x2
H3(x) = x −2
3x3
1206

Solution 23.2
1. First we write the diﬀerential equation in the standard form.
 1 −x2
y′′ −2xy′ + α(α + 1)y = 0
(23.2)
y′′ −
2x
1 −x2y′ + α(α + 1)
1 −x2 y = 0.
(23.3)
Since the coeﬃcients of y′ and y are analytic in a neighborhood of x = 0, We can ﬁnd two Taylor series solutions
about that point. We ﬁnd the Taylor series for y and its derivatives.
y =
∞
X
n=0
anxn
y′ =
∞
X
n=1
nanxn−1
y′′ =
∞
X
n=2
(n −1)nanxn−2
=
∞
X
n=0
(n + 1)(n + 2)an+2xn
Here we used index shifting to explicitly write the two forms that we will need for y′′. Note that we can take the
lower bound of summation to be n = 0 for all above sums. The terms added by this operation are zero. We
substitute the Taylor series into Equation 23.2.
∞
X
n=0
(n + 1)(n + 2)an+2xn −
∞
X
n=0
(n −1)nanxn −2
∞
X
n=0
nanxn + α(α + 1)
∞
X
n=0
anxn = 0
∞
X
n=0

(n + 1)(n + 2)an+2 −
 (n −1)n + 2n −α(α + 1)

an

xn = 0
1207

We equate coeﬃcients of xn to obtain a recurrence relation.
(n + 1)(n + 2)an+2 = (n(n + 1) −α(α + 1))an
an+2 = n(n + 1) −α(α + 1)
(n + 1)(n + 2)
an,
n ≥0
We can solve this diﬀerence equation to determine the an’s. (a0 and a1 are arbitrary.)
an =















a0
n!
n−2
Y
k=0
even k
 k(k + 1) −α(α + 1)

,
even n,
a1
n!
n−2
Y
k=1
odd k
 k(k + 1) −α(α + 1)

,
odd n
We will ﬁnd the fundamental set of solutions at x = 0, that is the set {y1, y2} that satisﬁes
y1(0) = 1
y′
1(0) = 0
y2(0) = 0
y′
2(0) = 1.
For y1 we take a0 = 1 and a1 = 0; for y2 we take a0 = 0 and a1 = 1. The rest of the coeﬃcients are determined
from the recurrence relation.
y1 =
∞
X
n=0
even n


1
n!
n−2
Y
k=0
even k
 k(k + 1) −α(α + 1)



xn
y2 =
∞
X
n=1
odd n


1
n!
n−2
Y
k=1
odd k
 k(k + 1) −α(α + 1)



xn
1208

2. We determine the radius of convergence of the series solutions with the ratio test.
lim
n→∞

an+2xn+2
anxn
 < 1
lim
n→∞

n(n+1)−α(α+1)
(n+1)(n+2)
anxn+2
anxn
 < 1
lim
n→∞

n(n + 1) −α(α + 1)
(n + 1)(n + 2)

x2 < 1
x2 < 1
Thus we see that the radius of convergence of the series is 1. We knew that the radius of convergence would be
at least one, because the nearest singularities of the coeﬃcients of (23.3) occur at x = ±1, a distance of 1 from
the origin. This implies that the solutions of the equation are analytic in the unit circle about x = 0. The radius
of convergence of the Taylor series expansion of an analytic function is the distance to the nearest singularity.
3. If α = 2n then a2n+2 = 0 in our ﬁrst solution. From the recurrence relation, we see that all subsequent coeﬃcients
are also zero. The solution becomes an even polynomial.
y1 =
2n
X
m=0
even m


1
m!
m−2
Y
k=0
even k
 k(k + 1) −α(α + 1)



xm
4. If α = 2n + 1 then a2n+3 = 0 in our second solution. From the recurrence relation, we see that all subsequent
coeﬃcients are also zero. The solution becomes an odd polynomial.
y2 =
2n+1
X
m=1
odd m


1
m!
m−2
Y
k=1
odd k
 k(k + 1) −α(α + 1)



xm
1209

5. From our solutions above, the ﬁrst four polynomials are
1
x
1 −3x2
x −5
3x3
To obtain the Legendre polynomials we normalize these to have value unity at x = 1
P0 = 1
P1 = x
P2 = 1
2
 3x2 −1

P3 = 1
2
 5x3 −3x

These four Legendre polynomials are plotted in Figure 23.4.
Figure 23.4: The First Four Legendre Polynomials
1210

6. We note that the ﬁrst two terms in the Legendre equation form an exact derivative. Thus the Legendre equation
can also be written as
 (1 −x2)y′′ = −α(α + 1)y.
Pn and Pm are solutions of the Legendre equation.
 (1 −x2)P ′
n
′ = −n(n + 1)Pn,
 (1 −x2)P ′
m
′ = −m(m + 1)Pm
(23.4)
We multiply the ﬁrst relation of Equation 23.4 by Pm and integrate by parts.
 (1 −x2)P ′
n
′ Pm = −n(n + 1)PnPm
Z 1
−1
 (1 −x2)P ′
n
′ Pm dx = −n(n + 1)
Z 1
−1
PnPm dx
 (1 −x2)P ′
n

Pm
1
−1 −
Z 1
−1
(1 −x2)P ′
nP ′
m dx = −n(n + 1)
Z 1
−1
PnPm dx
Z 1
−1
(1 −x2)P ′
nP ′
m dx = n(n + 1)
Z 1
−1
PnPm dx
We multiply the secord relation of Equation 23.4 by Pn and integrate by parts. To obtain a diﬀerent expression
for
R 1
−1(1 −x2)P ′
mP ′
n dx.
Z 1
−1
(1 −x2)P ′
mP ′
n dx = m(m + 1)
Z 1
−1
PmPn dx
We equate the two expressions for
R 1
−1(1 −x2)P ′
mP ′
n dx. to obtain an orthogonality relation.
(n(n + 1) −m(m + 1))
Z 1
−1
PnPm dx = 0
Z 1
−1
Pn(x)Pm(x) dx = 0 if n ̸= m.
1211

We verify that for the ﬁrst four polynomials the value of the integral is 2/(2n + 1) for n = m.
Z 1
−1
P0(x)P0(x) dx =
Z 1
−1
1 dx = 2
Z 1
−1
P1(x)P1(x) dx =
Z 1
−1
x2 dx =
x3
3
1
−1
= 2
3
Z 1
−1
P2(x)P2(x) dx =
Z 1
−1
1
4
 9x4 −6x2 + 1

dx =
1
4
9x5
5 −2x3 + x
1
−1
= 2
5
Z 1
−1
P3(x)P3(x) dx =
Z 1
−1
1
4
 25x6 −30x4 + 9x2
dx =
1
4
25x7
7
−6x5 + 3x3
1
−1
= 2
7
Solution 23.3
The indicial equation for this problem is
α2 + 1 = 0.
Since the two roots α1 = i and α2 = −i are distinct and do not diﬀer by an integer, there are two solutions in the
Frobenius form.
w1 = zi
∞
X
n=0
anzn,
w1 = z−i
∞
X
n=0
bnzn
However, these series are not real-valued on the positive real axis. Recalling that
zi = ei log z = cos(log z) + i sin(log z),
and
z−i = e−i log z = cos(log z) −i sin(log z),
we can write a new set of solutions that are real-valued on the positive real axis as linear combinations of w1 and w2.
u1 = 1
2(w1 + w2),
u2 = 1
2i(w1 −w2)
u1 = cos(log z)
∞
X
n=0
cnzn,
u1 = sin(log z)
∞
X
n=0
dnzn
1212

Solution 23.4
Consider the equation w′′ + w′/(z −1) + 2w = 0.
We see that there is a regular singular point at z = 1. All other ﬁnite values of z are ordinary points of the equation.
To examine the point at inﬁnity we introduce the transformation z = 1/t, w(z) = u(t). Writing the derivatives with
respect to z in terms of t yields
d
dz = −t2 d
dt,
d2
dz2 = t4 d2
dt2 + 2t3 d
dt.
Substituting into the diﬀerential equation gives us
t4u′′ + 2t3u′ −
t2u′
1/t −1 + 2u = 0
u′′ +
2
t −
1
t(1 −t)

u′ + 2
t4u = 0.
Since t = 0 is an irregular singular point in the equation for u(t), z = ∞is an irregular singular point in the equation
for w(z).
Solution 23.5
Find the series expansions about z = 0 for
w′′ + 5
4zw′ + z −1
8z2 w = 0.
We see that z = 0 is a regular singular point of the equation. The indicial equation is
α2 + 1
4α −1
8 = 0

α + 1
2
 
α −1
4

= 0.
Since the roots are distinct and do not diﬀer by an integer, there will be two solutions in the Frobenius form.
w1 = z1/4
∞
X
n=0
an(α1)zn,
w2 = z−1/2
∞
X
n=0
an(α2)zn
1213

We multiply the diﬀerential equation by 8z2 to put it in a better form. Substituting a Frobenius series into the
diﬀerential equation,
8z2
∞
X
n=0
(n + α)(n + α −1)anzn+α−2 + 10z
∞
X
n=0
(n + α)anzn+α−1 + (z −1)
∞
X
n=0
anzn+α
8
∞
X
n=0
(n + α)(n + α −1)anzn + 10
∞
X
n=0
(n + α)anzn +
∞
X
n=1
an−1zn −
∞
X
n=0
anzn.
Equating coeﬃcients of powers of z,
[8(n + α)(n + α −1) + 10(n + α) −1] an = −an−1
an = −
an−1
8(n + α)2 + 2(n + α) −1.
The First Solution.
Setting α = 1/4 in the recurrence formula,
an(α1) = −
an−1
8(n + 1/4)2 + 2(n + 1/4) −1
an(α1) = −
an−1
2n(4n + 3).
Thus the ﬁrst solution is
w1 = z1/4
∞
X
n=0
an(α1)zn = a0z1/4

1 −1
14z +
1
616z2 + · · ·

.
The Second Solution.
Setting α = −1/2 in the recurrence formula,
an = −
an−1
8(n −1/2)2 + 2(n −1/2) −1
an = −
an−1
2n(4n −3)
1214

Thus the second linearly independent solution is
w2 = z−1/2
∞
X
n=0
an(α2)zn = a0z−1/2

1 −1
2z + 1
40z2 + · · ·

.
Solution 23.6
We consider the series solutions of,
w′′ + zw′ + w = 0.
We would like to ﬁnd the expansions of the fundamental set of solutions about z = 0. Since z = 0 is a regular
point, (the coeﬃcient functions are analytic there), we expand the solutions in Taylor series. Diﬀerentiating the series
expansions for w(z),
w =
∞
X
n=0
anzn
w′ =
∞
X
n=1
nanzn−1
w′′ =
∞
X
n=2
n(n −1)anzn−2
=
∞
X
n=0
(n + 2)(n + 1)an+2zn
We may take the lower limit of summation to be zero without changing the sums. Substituting these expressions into
the diﬀerential equation,
∞
X
n=0
(n + 2)(n + 1)an+2zn +
∞
X
n=0
nanzn +
∞
X
n=0
anzn = 0
∞
X
n=0
 (n + 2)(n + 1)an+2 + (n + 1)an

zn = 0.
1215

Equating the coeﬃcient of the zn term gives us
(n + 2)(n + 1)an+2 + (n + 1)an = 0,
n ≥0
an+2 = −an
n + 2,
n ≥0.
a0 and a1 are arbitrary. We determine the rest of the coeﬃcients from the recurrence relation. We consider the cases
for even and odd n separately.
a2n = −a2n−2
2n
=
a2n−4
(2n)(2n −2)
= (−1)n
a0
(2n)(2n −2) · · · 4 · 2
= (−1)n
a0
Qn
m=1 2m,
n ≥0
a2n+1 = −a2n−1
2n + 1
=
a2n−3
(2n + 1)(2n −1)
= (−1)n
a1
(2n + 1)(2n −1) · · · 5 · 3
= (−1)n
a1
Qn
m=1(2m + 1),
n ≥0
If {w1, w2} is the fundamental set of solutions, then the initial conditions demand that w1 = 1 + 0 · z + · · · and
w2 = 0 + z + · · · . We see that w1 will have only even powers of z and w2 will have only odd powers of z.
w1 =
∞
X
n=0
(−1)n
Qn
m=1 2mz2n,
w2 =
∞
X
n=0
(−1)n
Qn
m=1(2m + 1)z2n+1
1216

Since the coeﬃcient functions in the diﬀerential equation are entire, (analytic in the ﬁnite complex plane), the radius
of convergence of these series solutions is inﬁnite.
Solution 23.7
w′′ + 1
2zw′ + 1
zw = 0.
We can ﬁnd the indicial equation by substituting w = zα + O(zα+1) into the diﬀerential equation.
α(α −1)zα−2 + 1
2αzα−2 + zα−1 = O(zα−1)
Equating the coeﬃcient of the zα−2 term,
α(α −1) + 1
2α = 0
α = 0, 1
2.
Since the roots are distinct and do not diﬀer by an integer, the solutions are of the form
w1 =
∞
X
n=0
anzn,
w2 = z1/2
∞
X
n=0
bnzn.
1217

Diﬀerentiating the series for the ﬁrst solution,
w1 =
∞
X
n=0
anzn
w′
1 =
∞
X
n=1
nanzn−1
=
∞
X
n=0
(n + 1)an+1zn
w′′
1 =
∞
X
n=1
n(n + 1)an+1zn−1.
Substituting this series into the diﬀerential equation,
∞
X
n=1
n(n + 1)an+1zn−1 + 1
2z
∞
X
n=0
(n + 1)an+1zn + 1
z
∞
X
n=0
anzn = 0
∞
X
n=1

n(n + 1)an+1 + 1
2(n + 1)an+1 + an

zn−1 + 1
2za1 + 1
za0 = 0.
Equating powers of z,
z−1 : a1
2 + a0 = 0
→
a1 = −2a0
zn−1 :

n + 1
2

(n + 1)an+1 + an = 0
→
an+1 = −
an
(n + 1/2)(n + 1).
We can combine the above two equations for an.
an+1 = −
an
(n + 1/2)(n + 1),
for n ≥0
1218

Solving this diﬀerence equation for an,
an = a0
n−1
Y
j=0
−1
(j + 1/2)(j + 1)
an = a0
(−1)n
n!
n−1
Y
j=0
1
j + 1/2
Now let’s ﬁnd the second solution. Diﬀerentiating w2,
w′
2 =
∞
X
n=0
(n + 1/2)bnzn−1/2
w′′
2 =
∞
X
n=0
(n + 1/2)(n −1/2)bnzn−3/2.
Substituting these expansions into the diﬀerential equation,
∞
X
n=0
(n + 1/2)(n −1/2)bnzn−3/2 + 1
2
∞
X
n=0
(n + 1/2)bnzn−3/2 +
∞
X
n=1
bn−1zn−3/2 = 0.
Equating the coeﬃcient of the z−3/2 term,
1
2

−1
2

b0 + 1
2
1
2b0 = 0,
we see that b0 is arbitrary. Equating the other coeﬃcients of powers of z,
(n + 1/2)(n −1/2)bn + 1
2(n + 1/2)bn + bn−1 = 0
bn = −
bn−1
n(n + 1/2)
1219

Calculating the bn’s,
b1 = −b0
1 · 3
2
b2 =
b0
1 · 2 · 3
2 · 5
2
bn =
(−1)n2nb0
n! · 3 · 5 · · · (2n + 1)
Thus the second solution is
w2 = b0z1/2
∞
X
n=0
(−1)n2nzn
n! 3 · 5 · · · (2n + 1).
Solution 23.8
w′′ +
2
z + 3
z2

w′ + 1
z2w = 0.
In order to analyze the behavior at inﬁnity we make the change of variables t = 1/z, u(t) = w(z) and examine the
point t = 0. Writing the derivatives with respect to z in terms if t yields
z = 1
t
dz = −1
t2dt
d
dz = −t2 d
dt
d2
dz2 = −t2 d
dt

−t2 d
dt

= t4 d2
dt2 + 2t3 d
dt.
1220

The equation for u is then
t4u′′ + 2t3u′ + (2t + 3t2)(−t2)u′ + t2u = 0
u′′ + −3u′ + 1
t2u = 0
We see that t = 0 is a regular singular point. To ﬁnd the indicial equation, we substitute u = tα + O(tα+1) into the
diﬀerential equation.
α(α −1)tα−2 −3αtα−1 + tα−2 = O(tα−1)
Equating the coeﬃcients of the tα−2 terms,
α(α −1) + 1 = 0
α = 1 ± i
√
3
2
Since the roots of the indicial equation are distinct and do not diﬀer by an integer, a set of solutions has the form
(
t(1+i
√
3)/2
∞
X
n=0
antn,
t(1−i
√
3)/2
∞
X
n=0
bntn
)
.
Noting that
t(1+i
√
3)/2 = t1/2 exp
 
i
√
3
2
log t
!
,
and
t(1−i
√
3)/2 = t1/2 exp
 
−i
√
3
2
log t
!
.
We can take the sum and diﬀerence of the above solutions to obtain the form
u1 = t1/2 cos
 √
3
2 log t
!
∞
X
n=0
antn,
u1 = t1/2 sin
 √
3
2 log t
!
∞
X
n=0
bntn.
1221

Putting the answer in terms of z, we have the form of the two Frobenius expansions about inﬁnity.
w1 = z−1/2 cos
 √
3
2 log z
!
∞
X
n=0
an
zn,
w1 = z−1/2 sin
 √
3
2 log z
!
∞
X
n=0
bn
zn.
Solution 23.9
1. We write the equation in the standard form.
y′′ + b −x
x
y′ −a
xy = 0
Since b−x
x
has no worse than a ﬁrst order pole and a
x has no worse than a second order pole at x = 0, that is a
regular singular point. Since the coeﬃcient functions have no other singularities in the ﬁnite complex plane, all
the other points in the ﬁnite complex plane are regular points.
Now to examine the point at inﬁnity. We make the change of variables u(ξ) = y(x), ξ = 1/x.
y′ = dξ
dx
d
dξu = −1
x2u′ = −ξ2u′
y′′ = −ξ2 d
dξ

−ξ2 d
dξ

u = ξ4u′′ + 2ξ3u′
The diﬀerential equation becomes
xy′′ + (b −x)y′ −ay
1
ξ
 ξ4u′′ + 2ξ3u′
+

b −1
ξ
  −ξ2u′
−au = 0
ξ3u′′ +
 (2 −b)ξ2 + ξ

u′ −au = 0
u′′ +
2 −b
ξ
+ 1
ξ2

−a
ξ3u = 0
Since this equation has an irregular singular point at ξ = 0, the equation for y(x) has an irregular singular point
at inﬁnity.
1222

2. The coeﬃcient functions are
p(x) ≡1
x
∞
X
n=1
pnxn = 1
x(b −x),
q(x) ≡1
x2
∞
X
n=1
qnxn = 1
x2(0 −ax).
The indicial equation is
α2 + (p0 −1)α + q0 = 0
α2 + (b −1)α + 0 = 0
α(α + b −1) = 0.
3. Since one of the roots of the indicial equation is zero, and the other root is not a negative integer, one of the
1223

solutions of the diﬀerential equation is a Taylor series.
y1 =
∞
X
k=0
ckxk
y′
1 =
∞
X
k=1
kckxk−1
=
∞
X
k=0
(k + 1)ck+1xk
=
∞
X
k=0
kckxk−1
y′′
1 =
X
k=2
k(k −1)ckxk−2
=
∞
X
k=1
(k + 1)kck+1xk−1
=
∞
X
k=0
(k + 1)kck+1xk−1
We substitute the Taylor series into the diﬀerential equation.
xy′′ + (b −x)y′ −ay = 0
∞
X
k=0
(k + 1)kck+1xk + b
∞
X
k=0
(k + 1)ck+1xk −
∞
X
k=0
kckxk −a
∞
X
k=0
ckxk = 0
We equate coeﬃcients to determine a recurrence relation for the coeﬃcients.
(k + 1)kck+1 + b(k + 1)ck+1 −kck −ack = 0
ck+1 =
k + a
(k + 1)(k + b)ck
1224

For c0 = 1, the recurrence relation has the solution
ck = (a)kxk
(b)kk! .
Thus one solution is
y1(x) =
∞
X
k=0
(a)k
(b)kk!xk.
4. If a = −m, where m is a non-negative integer, then (a)k = 0 for k > m. This makes y1 a polynomial:
y1(x) =
m
X
k=0
(a)k
(b)kk!xk.
5. If b = n + 1, where n is a non-negative integer, the indicial equation is
α(α + n) = 0.
For the case n = 0, the indicial equation has a double root at zero. Thus the solutions have the form:
y1(x) =
m
X
k=0
(a)k
(b)kk!xk,
y2(x) = y1(x) log x +
∞
X
k=0
dkxk
For the case n > 0 the roots of the indicial equation diﬀer by an integer. The solutions have the form:
y1(x) =
m
X
k=0
(a)k
(b)kk!xk,
y2(x) = d−1y1(x) log x + x−n
∞
X
k=0
dkxk
The form of the solution for y2 can be substituted into the equation to determine the coeﬃcients dk.
1225

Solution 23.10
We write the equation in the standard form.
xy′′ + 2xy′ + 6 ex y = 0
y′′ + 2y′ + 6ex
x y = 0
We see that x = 0 is a regular singular point. The indicial equation is
α2 −α = 0
α = 0, 1.
The ﬁrst solution has the Frobenius form.
y1 = x + a2x2 + a3x3 + O(x4)
We substitute y1 into the diﬀerential equation and equate coeﬃcients of powers of x.
xy′′ + 2xy′ + 6 ex y = 0
x(2a2 + 6a3x + O(x2)) + 2x(1 + 2a2x + 3a3x2 + O(x3))
+ 6(1 + x + x2/2 + O(x3))(x + a2x2 + a3x3 + O(x4)) = 0
(2a2x + 6a3x2) + (2x + 4a2x2) + (6x + 6(1 + a2)x2) = O(x3) = 0
a2 = −4,
a3 = 17
3
y1 = x −4x2 + 17
3 x3 + O(x4)
Now we see if the second solution has the Frobenius form. There is no a1x term because y2 is only determined up to
an additive constant times y1.
y2 = 1 + O(x2)
1226

We substitute y2 into the diﬀerential equation and equate coeﬃcients of powers of x.
xy′′ + 2xy′ + 6 ex y = 0
O(x) + O(x) + 6(1 + O(x))(1 + O(x2)) = 0
6 = O(x)
The substitution y2 = 1 + O(x) has yielded a contradiction. Since the second solution is not of the Frobenius form, it
has the following form:
y2 = y1 ln(x) + a0 + a2x2 + O(x3)
The ﬁrst three terms in the solution are
y2 = a0 + x ln x −4x2 ln x + O(x2).
We calculate the derivatives of y2.
y′
2 = ln(x) + O(1)
y′′
2 = 1
x + O(ln(x))
We substitute y2 into the diﬀerential equation and equate coeﬃcients.
xy′′ + 2xy′ + 6 ex y = 0
(1 + O(x ln x)) + 2 (O(x ln x)) + 6 (a0 + O(x ln x)) = 0
1 + 6a0 = 0
y2 = −1
6 + x ln x −4x2 ln x + O(x2)
1227

Chapter 24
Asymptotic Expansions
The more you sweat in practice, the less you bleed in battle.
-Navy Seal Saying
24.1
Asymptotic Relations
The ≪and ∼symbols.
First we will introduce two new symbols used in asymptotic relations.
f(x) ≪g(x)
as x →x0,
is read, “f(x) is much smaller than g(x) as x tends to x0”. This means
lim
x→x0
f(x)
g(x) = 0.
The notation
f(x) ∼g(x)
as x →x0,
1228

is read “f(x) is asymptotic to g(x) as x tends to x0”; which means
lim
x→x0
f(x)
g(x) = 1.
A few simple examples are
• −ex ≫x
as x →+∞
• sin x ∼x
as x →0
• 1/x ≪1
as x →+∞
• e−1/x ≪x−n
as x →0+ for all n
An equivalent deﬁnition of f(x) ∼g(x) as x →x0 is
f(x) −g(x) ≪g(x)
as x →x0.
Note that it does not make sense to say that a function f(x) is asymptotic to zero. Using the above deﬁnition this
would imply
f(x) ≪0
as x →x0.
If you encounter an expression like f(x) + g(x) ∼0, take this to mean f(x) ∼−g(x).
The Big O and Little o Notation.
If |f(x)| ≤m|g(x)| for some constant m in some neighborhood of the point
x = x0, then we say that
f(x) = O(g(x))
as x →x0.
We read this as “f is big O of g as x goes to x0”. If g(x) does not vanish, an equivalent deﬁnition is that f(x)/g(x)
is bounded as x →x0.
If for any given positive δ there exists a neighborhood of x = x0 in which |f(x)| ≤δ|g(x)| then
f(x) = o(g(x))
as x →x0.
This is read, “f is little o of g as x goes to x0.”
For a few examples of the use of this notation,
1229

• e−x = o(x−n) as x →∞for any n.
• sin x = O(x) as x →0.
• cos x −1 = o(1) as x →0.
• log x = o(xα) as x →+∞for any positive α.
Operations on Asymptotic Relations.
You can perform the ordinary arithmetic operations on asymptotic rela-
tions. Addition, multiplication, and division are valid.
You can always integrate an asymptotic relation. Integration is a smoothing operation. However, it is necessary to
exercise some care.
Example 24.1.1 Consider
f ′(x) ∼1
x2
as x →∞.
This does not imply that
f(x) ∼−1
x
as x →∞.
We have forgotten the constant of integration. Integrating the asymptotic relation for f ′(x) yields
f(x) ∼−1
x + c
as x →∞.
If c is nonzero then
f(x) ∼c
as x →∞.
It is not always valid to diﬀerentiate an asymptotic relation.
Example 24.1.2 Consider f(x) = 1
x + 1
x2 sin(x3).
f(x) ∼1
x
as x →∞.
1230

Diﬀerentiating this relation yields
f ′(x) ∼−1
x2
as x →∞.
However, this is not true since
f ′(x) = −1
x2 −2
x3 sin(x3) + 2 cos(x3)
̸∼−1
x2
as x →∞.
The Controlling Factor.
The controlling factor is the most rapidly varying factor in an asymptotic relation.
Consider a function f(x) that is asymptotic to x2 ex as x goes to inﬁnity. The controlling factor is ex. For a few
examples of this,
• x log x has the controlling factor x as x →∞.
• x−2 e1/x has the controlling factor e1/x as x →0.
• x−1 sin x has the controlling factor sin x as x →∞.
The Leading Behavior.
Consider a function that is asymptotic to a sum of terms.
f(x) ∼a0(x) + a1(x) + a2(x) + · · · ,
as x →x0.
where
a0(x) ≫a1(x) ≫a2(x) ≫· · · ,
as x →x0.
The ﬁrst term in the sum is the leading order behavior. For a few examples,
• For sin x ∼x −x3/6 + x5/120 −· · · as x →0, the leading order behavior is x.
• For f(x) ∼ex(1 −1/x + 1/x2 −· · · ) as x →∞, the leading order behavior is ex.
1231

24.2
Leading Order Behavior of Diﬀerential Equations
It is often useful to know the leading order behavior of the solutions to a diﬀerential equation. If we are considering
a regular point or a regular singular point, the approach is straight forward. We simply use a Taylor expansion or the
Frobenius method. However, if we are considering an irregular singular point, we will have to be a little more creative.
Instead of an all encompassing theory like the Frobenius method which always gives us the solution, we will use a
heuristic approach that usually gives us the solution.
Example 24.2.1 Consider the Airy equation
y′′ = xy.
We 1 would like to know how the solutions of this equation behave as x →+∞. First we need to classify the point at
inﬁnity. The change of variables
x = 1
t ,
y(x) = u(t),
d
dx = −t2 d
dt,
d2
dx2 = t4 d2
dt2 + 2t3 d
dt
yields
t4u′′ + 2t3u′ = 1
t u
u′′ + 2
t u′ −1
t5u = 0.
Since the equation for u has an irregular singular point at zero, the equation for y has an irregular singular point at
inﬁnity.
1Using ”We” may be a bit presumptuous on my part. Even if you don’t particularly want to know how the solutions behave, I
urge you to just play along. This is an interesting section, I promise.
1232

The Controlling Factor.
Since the solutions at irregular singular points often have exponential behavior, we make
the substitution y = es(x) into the diﬀerential equation for y.
d2
dx2
 es 
= x es

s′′ + (s′)2 es = x es
s′′ + (s′)2 = x
The Dominant Balance.
Now we have a diﬀerential equation for s that appears harder to solve than our equation
for y. However, we did not introduce the substitution in order to obtain an equation that we could solve exactly. We
are looking for an equation that we can solve approximately in the limit as x →∞. If one of the terms in the equation
for s is much smaller that the other two as x →∞, then dropping that term and solving the simpler equation may give
us an approximate solution. If one of the terms in the equation for s is much smaller than the others then we say that
the remaining terms form a dominant balance in the limit as x →∞.
Assume that the s′′ term is much smaller that the others, s′′ ≪(s′)2, x as x →∞. This gives us
(s′)2 ∼x
s′ ∼±√x
s ∼±2
3x3/2
as x →∞.
Now let’s check our assumption that the s′′ term is small. Assuming that we can diﬀerentiate the asymptotic relation
s′ ∼±√x, we obtain s′′ ∼±1
2x−1/2 as x →∞.
s′′ ≪(s′)2, x
→
x−1/2 ≪x
as x →∞
Thus we see that the behavior we found for s is consistent with our assumption. The controlling factors for solutions
to the Airy equation are exp(±2
3x3/2) as x →∞.
1233

The Leading Order Behavior of the Decaying Solution.
Let’s ﬁnd the leading order behavior as x goes to
inﬁnity of the solution with the controlling factor exp(−2
3x3/2). We substitute
s(x) = −2
3x3/2 + t(x),
where t(x) ≪x3/2 as x →∞
into the diﬀerential equation for s.
s′′ + (s′)2 = x
−1
2x−1/2 + t′′ + (−x1/2 + t′)2 = x
t′′ + (t′)2 −2x1/2t′ −1
2x−1/2 = 0
Assume that we can diﬀerentiate t ≪x3/2 to obtain
t′ ≪x1/2,
t′′ ≪x−1/2
as x →∞.
Since t′′ ≪−1
2x−1/2 we drop the t′′ term. Also, t′ ≪x1/2 implies that (t′)2 ≪−2x1/2t′, so we drop the (t′)2 term.
This gives us
−2x1/2t′ −1
2x−1/2 ∼0
t′ ∼−1
4x−1
t ∼−1
4 log x + c
t ∼−1
4 log x
as x →∞.
Checking our assumptions about t,
t′ ≪x1/2
→
x−1 ≪x1/2
t′′ ≪x−1/2
→
x−2 ≪x−1/2
1234

we see that the behavior of t is consistent with our assumptions.
So far we have
y(x) ∼exp

−2
3x3/2 −1
4 log x + u(x)

as x →∞,
where u(x) ≪log x as x →∞. To continue, we substitute t(x) = −1
4 log x + u(x) into the diﬀerential equation for
t(x).
t′′ + (t′)2 −2x1/2t′ −1
2x−1/2 = 0
1
4x−2 + u′′ +

−1
4x−1 + u′
2
−2x1/2

−1
4x−1 + u′

−1
2x−1/2 = 0
u′′ + (u′)2 +

−1
2x−1 −2x1/2

u′ + 5
16x−2 = 0
Assume that we can diﬀerentiate the asymptotic relation for u to obtain
u′ ≪x−1,
u′′ ≪x−2
as x →∞.
We know that −1
2x−1u′ ≪−2x1/2u′. Using our assumptions,
u′′ ≪x−2
→
u′′ ≪5
16x−2
u′ ≪x−1
→
(u′)2 ≪5
16x−2.
Thus we obtain
−2x1/2u′ + 5
16x−2 ∼0
u′ ∼5
32x−5/2
u ∼−5
48x−3/2 + c
u ∼c
as x →∞.
1235

Since u = c + o(1), eu = ec +o(1). The behavior of y is
y ∼x−1/4 exp

−2
3x3/2

(ec +o(1))
as x →∞.
Thus the full leading order behavior of the decaying solution is
y ∼(const)x−1/4 exp

−2
3x3/2

as x →∞.
You can show that the leading behavior of the exponentially growing solution is
y ∼(const)x−1/4 exp
2
3x3/2

as x →∞.
Example 24.2.2 The Modiﬁed Bessel Equation. Consider the modiﬁed Bessel equation
x2y′′ + xy′ −(x2 + ν2)y = 0.
We would like to know how the solutions of this equation behave as x →+∞. First we need to classify the point at
inﬁnity. The change of variables x = 1
t, y(x) = u(t) yields
1
t2(t4u′′ + 2t3u′) + 1
t (−t2u′) −
 1
t2 + ν2

u = 0
u′′ + 1
t u′ −
 1
t4 + ν2
t2

u = 0
Since u(t) has an irregular singular point at t = 0, y(x) has an irregular singular point at inﬁnity.
1236

The Controlling Factor.
Since the solutions at irregular singular points often have exponential behavior, we make
the substitution y = es(x) into the diﬀerential equation for y.
x2(s′′ + (s′)2) es +xs′ es −(x2 + ν2) es = 0
s′′ + (s′)2 + 1
xs′ −(1 + ν2
x2) = 0
We make the assumption that s′′ ≪(s′)2 as x →∞and we know that ν2/x2 ≪1 as x →∞. Thus we drop these
two terms from the equation to obtain an approximate equation for s.
(s′)2 + 1
xs′ −1 ∼0
This is a quadratic equation for s′, so we can solve it exactly. However, let us try to simplify the equation even further.
Assume that as x goes to inﬁnity one of the three terms is much smaller that the other two. If this is the case, there
will be a balance between the two dominant terms and we can neglect the third. Let’s check the three possibilities.
1.
1 is small.
→
(s′)2 + 1
xs′ ∼0
→
s′ ∼−1
x, 0
1 ̸≪
1
x2, 0 as x →∞so this balance is inconsistent.
2.
1
xs′ is small.
→
(s′)2 −1 ∼0
→
s′ ∼±1
This balance is consistent as 1
x ≪1 as x →∞.
3.
(s′)2 is small.
→
1
xs′ −1 ∼0
→
s′ ∼x
This balance is not consistent as x2 ̸≪1 as x →∞.
1237

The only dominant balance that makes sense leads to s′ ∼±1 as x →∞. Integrating this relationship,
s ∼±x + c
∼±x
as x →∞.
Now let’s see if our assumption that we made to get the simpliﬁed equation for s is valid. Assuming that we can
diﬀerentiate s′ ∼±1, s′′ ≪(s′)2 becomes
d
dx

± 1 + o(1)

≪

± 1 + o(1)
2
0 + o(1/x) ≪1
Thus we see that the behavior we obtained for s is consistent with our initial assumption.
We have found two controlling factors, ex and e−x. This is a good sign as we know that there must be two linearly
independent solutions to the equation.
Leading Order Behavior.
Now let’s ﬁnd the full leading behavior of the solution with the controlling factor e−x.
In order to ﬁnd a better approximation for s, we substitute s(x) = −x + t(x), where t(x) ≪x as x →∞, into the
diﬀerential equation for s.
s′′ + (s′)2 + 1
xs′ −

1 + ν2
x2

= 0
t′′ + (−1 + t′)2 + 1
x(−1 + t′) −

1 + ν2
x2

= 0
t′′ + (t′)2 +
1
x −2

t′ −
1
x + ν2
x2

= 0
We know that 1
x ≪2 and ν2
x2 ≪1
x as x →∞. Dropping these terms from the equation yields
t′′ + (t′)2 −2t′ −1
x ∼0.
1238

Assuming that we can diﬀerentiate the asymptotic relation for t, we obtain t′ ≪1 and t′′ ≪1
x as x →∞. We can
drop t′′. Since t′ vanishes as x goes to inﬁnity, (t′)2 ≪t′. Thus we are left with
−2t′ −1
x ∼0,
as x →∞.
Integrating this relationship,
t ∼−1
2 log x + c
∼−1
2 log x
as x →∞.
Checking our assumptions about the behavior of t,
t′ ≪1
→
−1
2x ≪1
t′′ ≪1
x
→
1
2x2 ≪1
x
we see that the solution is consistent with our assumptions.
The leading order behavior to the solution with controlling factor e−x is
y(x) ∼exp

−x −1
2 log x + u(x)

= x−1/2 e−x+u(x)
as x →∞,
where u(x) ≪log x. We substitute t = −1
2 log x + u(x) into the diﬀerential equation for t in order to ﬁnd the
asymptotic behavior of u.
t′′ + (t′)2 +
1
x −2

t′ −
1
x + ν2
x2

= 0
1
2x2 + u′′ +

−1
2x + u′
2
+
1
x −2
 
−1
2x + u′

−
1
x + ν2
x2

= 0
u′′ + (u′)2 −2u′ + 1
4x2 −ν2
x2 = 0
1239

Assuming that we can diﬀerentiate the asymptotic relation for u, u′ ≪1
x and u′′ ≪
1
x2 as x →∞. Thus we see that
we can neglect the u′′ and (u′)2 terms.
−2u′ +
1
4 −ν2
 1
x2 ∼0
u′ ∼1
2
1
4 −ν2
 1
x2
u ∼1
2

ν2 −1
4
 1
x + c
u ∼c
as x →∞
Since u = c + o(1), we can expand eu as ec +o(1). Thus we can write the leading order behavior as
y ∼x−1/2 e−x(ec +o(1)).
Thus the full leading order behavior is
y ∼(const)x−1/2 e−x
as x →∞.
You can verify that the solution with the controlling factor ex has the leading order behavior
y ∼(const)x−1/2 ex
as x →∞.
Two linearly independent solutions to the modiﬁed Bessel equation are the modiﬁed Bessel functions, Iν(x) and
Kν(x). These functions have the asymptotic behavior
Iν(x) ∼
1
√
2πx
ex,
Kν(x) ∼
√π
√
2x
e−x
as x →∞.
In Figure 24.1 K0(x) is plotted in a solid line and
√π
√
2x e−x is plotted in a dashed line. We see that the leading order
behavior of the solution as x goes to inﬁnity gives a good approximation to the behavior even for fairly small values of
x.
1240

0
1
2
3
4
5
0.25
0.5
0.75
1
1.25
1.5
1.75
2
Figure 24.1: Plot of K0(x) and it’s leading order behavior.
24.3
Integration by Parts
Example 24.3.1 The complementary error function
erfc(x) =
2
√π
Z ∞
x
e−t2 dt
1241

is used in statistics for its relation to the normal probability distribution. We would like to ﬁnd an approximation to
erfc(x) for large x. Using integration by parts,
erfc(x) =
2
√π
Z ∞
x
−1
2t
 
−2t e−t2
dt
=
2
√π
−1
2t e−t2∞
x
−2
√π
Z ∞
x
1
2t−2 e−t2 dt
=
1
√πx−1 e−x2 −1
√π
Z ∞
x
t−2 e−t2 dt.
We examine the residual integral in this expression.
1
√π
Z ∞
x
t−2 e−t2 dt ≤−1
2√πx−3
Z ∞
x
−2t e−t2 dt
=
1
2√πx−3 e−x2 .
Thus we see that
1
√πx−1 e−x2 ≫
1
√π
Z ∞
x
t−2 e−t2 dt
as x →∞.
Therefore,
erfc(x) ∼
1
√πx−1 e−x2
as x →∞,
and we expect that
1
√πx−1 e−x2 would be a good approximation to erfc(x) for large x. In Figure 24.2 log(erfc(x)) is
graphed in a solid line and log

1
√πx−1 e−x2
is graphed in a dashed line. We see that this ﬁrst approximation to the
error function gives very good results even for moderate values of x. Table 24.1 gives the error in this ﬁrst approximation
for various values of x.
1242

0.5
1
1.5
2
2.5
3
-10
-8
-6
-4
-2
2
Figure 24.2: Logarithm of the Approximation to the Complementary Error Function.
If we continue integrating by parts, we might get a better approximation to the complementary error function.
erfc(x) =
1
√πx−1 e−x2 −1
√π
Z ∞
x
t−2 e−t2 dt
=
1
√πx−1 e−x2 −1
√π

−1
2t−3 e−t2∞
x
+ 1
√π
Z ∞
x
3
2t−4 e−t2 dt
=
1
√π e−x2 
x−1 −1
2x−3

+ 1
√π
Z ∞
x
3
2t−4 e−t2 dt
=
1
√π e−x2 
x−1 −1
2x−3

+ 1
√π

−3
4t−5 e−t2∞
x
−1
√π
Z ∞
x
15
4 t−6 e−t2 dt
=
1
√π e−x2 
x−1 −1
2x−3 + 3
4x−5

−1
√π
Z ∞
x
15
4 t−6 e−t2 dt
1243

x
erfc(x)
One Term Relative Error
Three Term Relative Error
1
0.157
0.3203
0.6497
2
0.00468
0.1044
0.0182
3
2.21 × 10−5
0.0507
0.0020
4
1.54 × 10−8
0.0296
3.9 · 10−4
5
1.54 × 10−12
0.0192
1.1 · 10−4
6
2.15 × 10−17
0.0135
3.7 · 10−5
7
4.18 × 10−23
0.0100
1.5 · 10−5
8
1.12 × 10−29
0.0077
6.9 · 10−6
9
4.14 × 10−37
0.0061
3.4 · 10−6
10
2.09 × 10−45
0.0049
1.8 · 10−6
Table 24.1:
The error in approximating erfc(x) with the ﬁrst three terms is given in Table 24.1. We see that for x ≥2 the three
terms give a much better approximation to erfc(x) than just the ﬁrst term.
At this point you might guess that you could continue this process indeﬁnitely. By repeated application of integration
by parts, you can obtain the series expansion
erfc(x) =
2
√π e−x2
∞
X
n=0
(−1)n(2n)!
n!(2x)2n+1 .
1244

This is a Taylor expansion about inﬁnity. Let’s ﬁnd the radius of convergence.
lim
n→∞

an+1(x)
an(x)
 < 1 →lim
n→∞

(−1)n+1(2(n + 1))!
(n + 1)!(2x)2(n+1)+1
n!(2x)2n+1
(−1)n(2n)!
 < 1
→lim
n→∞

(2n + 2)(2n + 1)
(n + 1)(2x)2
 < 1
→lim
n→∞

2(2n + 1)
(2x)2
 < 1
→

1
x
 = 0
Thus we see that our series diverges for all x. Our conventional mathematical sense would tell us that this series is
useless, however we will see that this series is very useful as an asymptotic expansion of erfc(x).
Say we are working with a convergent series expansion of some function f(x).
f(x) =
∞
X
n=0
an(x)
For ﬁxed x = x0,
f(x0) −
N
X
n=0
an(x0) →0
as N →∞.
For an asymptotic series we have a quite diﬀerent behavior. If g(x) is asymptotic to P∞
n=0 bn(x) as x →x0 then for
ﬁxed N,
g(x) −
N
X
0
bn(x) ≪bN(x)
as x →x0.
For the complementary error function,
For ﬁxed N, erfc(x) −2
√π e−x2
N
X
n=0
(−1)n(2n)!
n!(2x)2n+1 ≪x−2N−1
as x →∞.
1245

We say that the error function is asymptotic to the series as x goes to inﬁnity.
erfc(x) ∼
2
√π e−x2
∞
X
n=0
(−1)n(2n)!
n!(2x)2n+1
as x →∞
In Figure 24.3 the logarithm of the diﬀerence between the one term, ten term and twenty term approximations and
the complementary error function are graphed in coarse, medium, and ﬁne dashed lines, respectively.
1
2
3
4
5
6
-60
-40
-20
Figure 24.3: log(error in approximation)
*Optimal Asymptotic Series.
Of the three approximations, the one term is best for x ≲2, the ten term is
best for 2 ≲x ≲4, and the twenty term is best for 4 ≲x. This leads us to the concept of an optimal asymptotic
1246

approximation. An optimal asymptotic approximation contains the number of terms in the series that best approximates
the true behavior.
In Figure 24.4 we see a plot of the number of terms in the approximation versus the logarithm of the error for x = 3.
Thus we see that the optimal asymptotic approximation is the ﬁrst nine terms. After nine terms the error gets larger.
It was inevitable that the error would start to grow after some point as the series diverges for all x.
5
10
15
20
25
-18
-16
-14
-12
Figure 24.4: The logarithm of the error in using n terms.
A good rule of thumb for ﬁnding the optimal series is to ﬁnd the smallest term in the series and take all of the
terms up to but not including the smallest term as the optimal approximation. This makes sense, because the nth term
is an approximation of the error incurred by using the ﬁrst n −1 terms. In Figure 24.5 there is a plot of n versus the
logarithm of the nth term in the asymptotic expansion of erfc(3). We see that the tenth term is the smallest. Thus, in
this case, our rule of thumb predicts the actual optimal series.
1247

5
10
15
20
25
-16
-14
-12
Figure 24.5: The logarithm of the nth term in the expansion for x = 3.
24.4
Asymptotic Series
A function f(x) has an asymptotic series expansion about x = x0, P∞
n=0 an(x), if
f(x) −
N
X
n=0
an(x) ≪aN(x)
as x →x0
for all N.
An asymptotic series may be convergent or divergent. Most of the asymptotic series you encounter will be divergent.
If the series is convergent, then we have that
f(x) −
N
X
n=0
an(x) →0
as N →∞
for ﬁxed x.
1248

Let ϵn(x) be some set of gauge functions. The example that we are most familiar with is ϵn(x) = xn. If we say that
∞
X
n=0
anϵn(x) ∼
∞
X
n=0
bnϵn(x),
then this means that an = bn.
24.5
Asymptotic Expansions of Diﬀerential Equations
24.5.1
The Parabolic Cylinder Equation.
Controlling Factor.
Let us examine the behavior of the bounded solution of the parabolic cylinder equation as
x →+∞.
y′′ +

ν + 1
2 −1
4x2

y = 0
This equation has an irregular singular point at inﬁnity. With the substitution y = es, the equation becomes
s′′ + (s′)2 + ν + 1
2 −1
4x2 = 0.
We know that
ν + 1
2 ≪1
4x2
as x →+∞
so we drop this term from the equation. Let us make the assumption that
s′′ ≪(s′)2
as x →+∞.
1249

Thus we are left with the equation
(s′)2
∼
1
4x2
s′
∼
±1
2x
s
∼
±1
4x2 + c
s
∼
±1
4x2
as x →+∞
Now let’s check if our assumption is consistent. Substituting into s′′ ≪(s′)2 yields 1/2 ≪x2/4
as x →+∞which
is true. Since the equation for y is second order, we would expect that there are two diﬀerent behaviors as x →+∞.
This is conﬁrmed by the fact that we found two behaviors for s. s ∼−x2/4 corresponds to the solution that is bounded
at +∞. Thus the controlling factor of the leading behavior is e−x2/4.
Leading Order Behavior.
Now we attempt to get a better approximation to s.
We make the substitution
s = −1
4x2 + t(x) into the equation for s where t ≪x2 as x →+∞.
−1
2 + t′′ + 1
4x2 −xt′ + (t′)2 + ν + 1
2 −1
4x2 = 0
t′′ −xt′ + (t′)2 + ν = 0
Since t ≪x2, we assume that t′ ≪x and t′′ ≪1 as x →+∞. Note that this in only an assumption since it is not
always valid to diﬀerentiate an asymptotic relation. Thus (t′)2 ≪xt′ and t′′ ≪xt′ as x →+∞; we drop these terms
from the equation.
t′
∼
ν
x
t
∼
ν log x + c
t
∼
ν log x
as x →+∞
1250

Checking our assumptions for the derivatives of t,
t′ ≪x
→
1
x ≪x
t′′ ≪1
→
1
x2 ≪1,
we see that they were consistent.
Now we wish to reﬁne our approximation for t with the substitution t(x) =
ν log x + u(x). So far we have that
y ∼exp

−x2
4 + ν log x + u(x)

= xν exp

−x2
4 + u(x)

as x →+∞.
We can try and determine u(x) by substituting the expression t(x) = ν log x + u(x) into the equation for t.
−ν
x2 + u′′ −(ν + xu′) + ν2
x2 + 2ν
x u′ + (u′)2 + ν = 0
After suitable simpliﬁcation, this equation becomes
u′ ∼ν2 −ν
x3
as x →+∞
Integrating this asymptotic relation,
u ∼ν −ν2
2x2
+ c
as x →+∞.
Notice that ν−ν2
2x2 ≪c as x →+∞; thus this procedure fails to give us the behavior of u(x). Further reﬁnements to
our approximation for s go to a constant value as x →+∞. Thus we have that the leading behavior is
y ∼cxν exp

−x2
4

as x →+∞
1251

Asymptotic Expansion
Since we have factored oﬀthe singular behavior of y, we might expect that what is left
over is well behaved enough to be expanded in a Taylor series about inﬁnity. Let us assume that we can expand the
solution for y in the form
y(x) ∼xν exp

−x2
4

σ(x) = xν exp

−x2
4

∞
X
n=0
anx−nas x →+∞
where a0 = 1. Diﬀerentiating y = xν exp

−x2
4

σ(x),
y′ =

νxν−1 −1
2xν+1

e−x2/4 σ(x) + xν e−x2/4 σ′(x)
y′′ =

ν(ν −1)xν−2 −1
2νxν −1
2(ν + 1)xν + 1
4xν+2

e−x2/4 σ(x) + 2

νxν−1 −1
2xν+1

e−x2/4 σ′(x)
+ xν e−x2/4 σ′′(x).
Substituting this into the diﬀerential equation for y,

ν(ν −1)x−2 −(ν + 1
2) + 1
4x2

σ(x) + 2

νx−1 −1
2x

σ′(x) + σ′′(x) +

ν + 1
2 −1
4x2

σ(x) = 0
σ′′(x) + (2νx−1 −x)σ′(x) + ν(ν −1)x−2σ = 0
x2σ′′(x) + (2νx −x3)σ′(x) + ν(ν −1)σ(x) = 0.
Diﬀerentiating the expression for σ(x),
σ(x) =
∞
X
n=0
anx−n
σ′(x) =
∞
X
n=1
−nanx−n−1 =
∞
X
n=−1
−(n + 2)an+2x−n−3
σ′′(x) =
∞
X
n=1
n(n + 1)anx−n−2.
1252

Substituting this into the diﬀerential equation for σ(x),
∞
X
n=1
n(n + 1)anx−n + 2ν
∞
X
n=1
−nanx−n −
∞
X
n=−1
−(n + 2)an+2x−n + ν(ν −1)
∞
X
n=0
anx−n = 0.
Equating the coeﬃcient of x1 to zero yields
a1x = 0
→
a1 = 0.
Equating the coeﬃcient of x0,
2a2 + ν(ν −1)a0 = 0
→
a2 = −1
2ν(ν −1).
From the coeﬃcient of x−n for n > 0,
n(n + 1)an −2νnan + (n + 2)an+2 + ν(ν −1)an = 0
(n + 2)an+2 = −[n(n + 1) −2νn + ν(ν −1)]an
(n + 2)an+2 = −[n2 + n −2νn + ν(ν −1)]an
(n + 2)an+2 = −(n −ν)(n −ν + 1)an.
Thus the recursion formula for the an’s is
an+2 = −(n −ν)(n −ν + 1)
n + 2
an,
a0 = 1,
a1 = 0.
The ﬁrst few terms in σ(x) are
σ(x) ∼1 −ν(ν −1)
211!
x−2 + ν(ν −1)(ν −2)(ν −3)
222!
x−4 −· · ·
as x →+∞
If we check the radius of convergence of this series
lim
n→∞

an+2x−n−2
anx−n
 < 1
→
lim
n→∞
−(n −ν)(n −ν + 1)
n + 2
x−2
 < 1
→
1
x = 0
1253

we see that the radius of convergence is zero. Thus if ν ̸= 0, 1, 2, . . . our asymptotic expansion for y
y ∼xν e−x2/4

1 −ν(ν −1)
211!
x−2 + ν(ν −1)(ν −2)(ν −3)
222!
x−4 −· · ·

diverges for all x. However this solution is still very useful. If we only use a ﬁnite number of terms, we will get a very
good numerical approximation for large x.
In Figure 24.6 the one term, two term, and three term asymptotic approximations are shown in rough, medium, and
ﬁne dashing, respectively. The numerical solution is plotted in a solid line.
1
2
3
4
5
6
-2
2
4
6
Figure 24.6: Asymptotic Approximations to the Parabolic Cylinder Function.
1254

Chapter 25
Hilbert Spaces
An expert is a man who has made all the mistakes which can be made, in a narrow ﬁeld.
- Niels Bohr
WARNING: UNDER HEAVY CONSTRUCTION.
In this chapter we will introduce Hilbert spaces. We develop the two important examples: l2, the space of square
summable inﬁnite vectors and L2, the space of square integrable functions.
25.1
Linear Spaces
A linear space is a set of elements {x, y, z, . . .} that is closed under addition and scalar multiplication. By closed under
addition we mean: if x and y are elements, then z = x+y is an element. The addition is commutative and associative.
x + y = y + x
(x + y) + z = x + (y + z)
1255

Scalar multiplication is associative and distributive. Let a and b be scalars, a, b ∈C.
(ab)x = a(bx)
(a + b)x = ax + bx
a(x + y) = ax + ay
All the linear spaces that we will work with have additional properties: The zero element 0 is the additive identity.
x + 0 = x
Multiplication by the scalar 1 is the multiplicative identity.
1x = x
Each element x and the additive inverse, −x.
x + (−x) = 0
Consider a set of elements {x1, x2, . . .}. Let the ci be scalars. If
y = c1x1 + c2x2 + · · ·
then y is a linear combination of the xi. A set of elements {x1, x2, . . .} is linearly independent if the equation
c1x1 + c2x2 + · · · = 0
has only the trivial solution c1 = c2 = · · · = 0. Otherwise the set is linearly dependent.
Let {e1, e2, · · · } be a linearly independent set of elements. If every element x can be written as a linear combination
of the ei then the set {ei} is a basis for the space. The ei are called base elements.
x =
X
i
ciei
The set {ei} is also called a coordinate system. The scalars ci are the coordinates or components of x. If the set {ei}
is a basis, then we say that the set is complete.
1256

25.2
Inner Products
⟨x|y⟩is an inner product of two elements x and y if it satisﬁes the properties:
1. Conjugate-commutative.
⟨x|y⟩= ⟨x|y⟩
2. Linearity in the second argument.
⟨x|ay + bz⟩= a⟨x|y⟩+ b⟨x|y⟩
3. Positive deﬁnite.
⟨x|x⟩≥0
⟨x|x⟩= 0 if and only if x = 0
From these properties one can derive the properties:
1. Conjugate linearity in the ﬁrst argument.
⟨ax + by|z⟩= a⟨x|z⟩+ b⟨x|z⟩
2. Schwarz Inequality.
|⟨x|y⟩|2 ≤⟨x|x⟩⟨y|y⟩
One inner product of vectors is the Euclidean inner product.
⟨x|y⟩≡x · y =
n
X
i=0
xiyi.
One inner product of functions deﬁned on (a . . . b) is
⟨u|v⟩=
Z b
a
u(x)v(x) dx.
1257

If σ(x) is a positive-valued function, then we can deﬁne the inner product:
⟨u|σ|v⟩=
Z b
a
u(x)σ(x)v(x) dx.
This is called the inner product with respect to the weighting function σ(x). It is also denoted ⟨u|v⟩σ.
25.3
Norms
A norm is a real-valued function on a space which satisﬁes the following properties.
1. Positive.
∥x∥≥0
2. Deﬁnite.
∥x∥= 0 if and only if x = 0
3. Multiplication my a scalar, c ∈C.
∥cx∥= |c|∥x∥
4. Triangle inequality.
∥x + y∥≤∥x∥+ ∥y∥
Example 25.3.1 Consider a vector space, (ﬁnite or inﬁnite dimension), with elements x = (x1, x2, x3, . . .). Here are
some common norms.
• Norm generated by the inner product.
∥x∥=
p
⟨x|x⟩
1258

• The lp norm.
∥x∥p =
 ∞
X
k=1
|xk|p
!1/p
There are three common cases of the lp norm.
– Euclidian norm, or l2 norm.
∥x∥2 =
v
u
u
t
∞
X
k=1
|xk|2
– l1 norm.
∥x∥1 =
∞
X
k=1
|xk|
– l∞norm.
∥x∥∞= max
k
|xk|
Example 25.3.2 Consider a space of functions deﬁned on the interval (a . . . b). Here are some common norms.
• Norm generated by the inner product.
∥u∥=
p
⟨u|u⟩
• The Lp norm.
∥u∥p =
Z b
a
|u(x)|p dx
1/p
There are three common cases of the Lp norm.
– Euclidian norm, or L2 norm.
∥u∥2 =
sZ b
a
|u(x)|2 dx
1259

– L1 norm.
∥u∥1 =
Z b
a
|u(x)| dx
– L∞norm.
∥u∥∞= lim sup
x∈(a...b)
|u(x)|
Distance.
Using the norm, we can deﬁne the distance between elements u and v.
d(u, v) ≡∥u −v∥
Note that d(u, v) = 0 does not necessarily imply that u = v. CONTINUE.
25.4
Linear Independence.
25.5
Orthogonality
Orthogonality.
⟨φj|φk⟩= 0 if j ̸= k
Orthonormality.
⟨φj|φk⟩= δjk
Example 25.5.1 Inﬁnite vectors. ej has all zeros except for a 1 in the jth position.
ej = (0, 0, . . . 0, 1, 0, . . .)
1260

Example 25.5.2 L2 functions on (0 . . . 2π).
φj =
1
√
2π
eıjx,
j ∈Z
φ0 =
1
√
2π,
φ(1)
j
=
1
√π cos(jx),
φ(1)
j
=
1
√π sin(jx),
j ∈Z+
25.6
Gramm-Schmidt Orthogonalization
Let {ψ1(x), . . . , ψn(x)} be a set of linearly independent functions. Using the Gramm-Schmidt orthogonalization process
we can construct a set of orthogonal functions {φ1(x), . . . , φn(x)} that has the same span as the set of ψn’s with the
formulas
φ1 = ψ1
φ2 = ψ2 −⟨φ1|ψ2⟩
∥φ1∥2 φ1
φ3 = ψ3 −⟨φ1|ψ3⟩
∥φ1∥2 φ1 −⟨φ2|ψ3⟩
∥φ2∥2 φ2
· · ·
φn = ψn −
n−1
X
j=1
⟨φj|ψn⟩
∥φj∥2 φj.
You could verify that the φm are orthogonal with a proof by induction.
Example 25.6.1 Suppose we would like a polynomial approximation to cos(πx) in the domain [−1, 1]. One way to
do this is to ﬁnd the Taylor expansion of the function about x = 0. Up to terms of order x4, this is
cos(πx) = 1 −(πx)2
2
+ (πx)4
24
+ O(x6).
1261

In the ﬁrst graph of Figure 25.1 cos(πx) and this fourth degree polynomial are plotted. We see that the approximation
is very good near x = 0, but deteriorates as we move away from that point. This makes sense because the Taylor
expansion only makes use of information about the function’s behavior at the point x = 0.
As a second approach, we could ﬁnd the least squares ﬁt of a fourth degree polynomial to cos(πx). The set of
functions {1, x, x2, x3, x4} is independent, but not orthogonal in the interval [−1, 1]. Using Gramm-Schmidt orthogo-
nalization,
φ0 = 1
φ1 = x −⟨1|x⟩
⟨1|1⟩= x
φ2 = x2 −⟨1|x2⟩
⟨1|1⟩−⟨x|x2⟩
⟨x|x⟩x = x2 −1
3
φ3 = x3 −3
5x
φ4 = x4 −6
7x2 −3
35
A widely used set of functions in mathematics is the set of Legendre polynomials {P0(x), P1(x), . . .}. They diﬀer
from the φn’s that we generated only by constant factors. The ﬁrst few are
P0(x) = 1
P1(x) = x
P2(x) = 3x2 −1
2
P3(x) = 5x3 −3x
2
P4(x) = 35x4 −30x2 + 3
8
.
1262

Expanding cos(πx) in Legendre polynomials
cos(πx) ≈
4
X
n=0
cnPn(x),
and calculating the generalized Fourier coeﬃcients with the formula
cn = ⟨Pn| cos(πx)⟩
⟨Pn|Pn⟩
,
yields
cos(πx) ≈−15
π2P2(x) + 45(2π2 −21)
π4
P4(x)
= 105
8π4[(315 −30π2)x4 + (24π2 −270)x2 + (27 −2π2)]
The cosine and this polynomial are plotted in the second graph in Figure 25.1. The least squares ﬁt method uses
information about the function on the entire interval. We see that the least squares ﬁt does not give as good an ap-
proximation close to the point x = 0 as the Taylor expansion. However, the least squares ﬁt gives a good approximation
on the entire interval.
In order to expand a function in a Taylor series, the function must be analytic in some domain. One advantage of
using the method of least squares is that the function being approximated does not even have to be continuous.
25.7
Orthonormal Function Expansion
Let {φj} be an orthonormal set of functions on the interval (a, b). We expand a function f(x) in the φj.
f(x) =
X
j
cjφj
1263

-1
-0.5
0.5
1
-1
-0.5
0.5
1
-1
-0.5
0.5
1
-1
-0.5
0.5
1
Figure 25.1: Polynomial Approximations to cos(πx).
We choose the coeﬃcients to minimize the norm of the error.
f −
X
j
cjφj

2
=
*
f −
X
j
cjφj
f −
X
j
cjφj
+
= ∥f∥2 −
*
f

X
j
cjφj
+
−
*X
j
cjφj
f
+
+
*X
j
cjφj

X
j
cjφj
+
= ∥f∥2 +
X
j
|cj|2 −
X
j
cj⟨f|φj⟩−
X
j
cj⟨φj|f⟩
f −
X
j
cjφj

2
= ∥f∥2 +
X
j
|cj|2 −
X
j
cj⟨φj|f⟩−
X
j
cj⟨φj|f⟩
(25.1)
1264

To complete the square, we add the constant P
j⟨φj|f⟩⟨φj|f⟩. We see the values of cj which minimize
∥f∥2 +
X
j
|cj −⟨φj|f⟩|2 .
Clearly the unique minimum occurs for
cj = ⟨φj|f⟩.
We substitute this value for cj into the right side of Equation 25.1 and note that this quantity, the squared norm of the
error, is non-negative.
∥f∥2 +
X
j
|cj|2 −
X
j
|cj|2 −
X
j
|cj|2 ≥0
∥f∥2 ≥
X
j
|cj|2
This is known as Bessel’s Inequality. If the set of {φj} is complete then the norm of the error is zero and we obtain
Bessel’s Equality.
∥f∥2 =
X
j
|cj|2
25.8
Sets Of Functions
Orthogonality.
Consider two complex valued functions of a real variable φ1(x) and φ2(x) deﬁned on the interval
a ≤x ≤b. The inner product of the two functions is deﬁned
⟨φ1|φ2⟩=
Z b
a
φ1(x)φ2(x) dx.
The two functions are orthogonal if ⟨φ1|φ2⟩= 0. The L2 norm of a function is deﬁned ∥φ∥=
p
⟨φ|φ⟩.
1265

Let {φ1, φ2, φ3, . . .} be a set of complex valued functions. The set of functions is orthogonal if each pair of functions
is orthogonal. That is,
⟨φn|φm⟩= 0
if n ̸= m.
If in addition the norm of each function is 1, then the set is orthonormal. That is,
⟨φn|φm⟩= δnm =
(
1
if n = m
0
if n ̸= m.
Example 25.8.1 The set of functions
(r
2
π sin(x),
r
2
π sin(2x),
r
2
π sin(3x), . . .
)
is orthonormal on the interval [0, π]. To verify this,
*r
2
π sin(nx)

r
2
π sin(nx)
+
= 2
π
Z π
0
sin2(nx) dx
= 1
If n ̸= m then
*r
2
π sin(nx)

r
2
π sin(mx)
+
= 2
π
Z π
0
sin(nx) sin(mx) dx
= 1
π
Z π
0
(cos[(n −m)x] −cos[(n + m)x]) dx
= 0.
1266

Example 25.8.2 The set of functions
{. . . ,
1
√
2π
e−ıx,
1
√
2π,
1
√
2π
eıx,
1
√
2π
eı2x, . . .},
is orthonormal on the interval [−π, π]. To verify this,
*
1
√
2π
eınx

1
√
2π
eınx
+
= 1
2π
Z π
−π
e−ınx eınx dx
= 1
2π
Z π
−π
dx
= 1.
If n ̸= m then
*
1
√
2π
eınx

1
√
2π
eımx
+
= 1
2π
Z π
−π
e−ınx eımx dx
= 1
2π
Z π
−π
eı(m−n)x dx
= 0.
Orthogonal with Respect to a Weighting Function.
Let σ(x) be a real-valued, positive function on the
interval [a, b]. We introduce the notation
⟨φn|σ|φm⟩≡
Z b
a
φnσφm dx.
If the set of functions {φ1, φ2, φ3, . . .} satisfy
⟨φn|σ|φm⟩= 0
if n ̸= m
1267

then the functions are orthogonal with respect to the weighting function σ(x).
If the functions satisfy
⟨φn|σ|φm⟩= δnm
then the set is orthonormal with respect to σ(x).
Example 25.8.3 We know that the set of functions
(r
2
π sin(x),
r
2
π sin(2x),
r
2
π sin(3x), . . .
)
is orthonormal on the interval [0, π]. That is,
Z π
0
r
2
π sin(nx)
r
2
π sin(mx) dx = δnm.
If we make the change of variables x =
√
t in this integral, we obtain
Z π2
0
1
2
√
t
r
2
π sin(n
√
t)
r
2
π sin(m
√
t) dt = δnm.
Thus the set of functions
(r
1
π sin(
√
t),
r
1
π sin(2
√
t),
r
1
π sin(3
√
t), . . .
)
is orthonormal with respect to σ(t) =
1
2
√
t on the interval [0, π2].
Orthogonal Series.
Suppose that a function f(x) deﬁned on [a, b] can be written as a uniformly convergent sum
of functions that are orthogonal with respect to σ(x).
f(x) =
∞
X
n=1
cnφn(x)
1268

We can solve for the cn by taking the inner product of φm(x) and each side of the equation with respect to σ(x).
⟨φm|σ|f⟩=
*
φm
σ

∞
X
n=1
cnφn
+
⟨φm|σ|f⟩=
∞
X
n=1
cn⟨φm|σ|φn⟩
⟨φm|σ|f⟩= cm⟨φm|σ|φm⟩
cm = ⟨φm|σ|f⟩
⟨φm|σ|φm⟩
The cm are known as Generalized Fourier coeﬃcients. If the functions in the expansion are orthonormal, the formula
simpliﬁes to
cm = ⟨φm|σ|f⟩.
Example 25.8.4 The function f(x) = x(π −x) has a uniformly convergent series expansion in the domain [0, π] of
the form
x(π −x) =
∞
X
n=1
cn
r
2
π sin(nx).
1269

The Fourier coeﬃcients are
cn =
*r
2
π sin(nx)
x(π −x)
+
=
r
2
π
Z π
0
x(π −x) sin(nx) dx
=
r
2
π
2
n3(1 −(−1)n)
=
(q
2
π
4
n3
for odd n
0
for even n
Thus the expansion is
x(π −x) =
∞
X
n=1
oddn
8
πn3 sin(nx)
for x ∈[0, π].
In the ﬁrst graph of Figure 25.2 the ﬁrst term in the expansion is plotted in a dashed line and x(π −x) is plotted
in a solid line. The second graph shows the two term approximation.
Example 25.8.5 The set {. . . , 1/
√
2π e−ıx, 1/
√
2π, 1/
√
2π eıx, 1/
√
2π eı2x, . . .} is orthonormal on the interval [−π, π].
1270

1
2
3
1
2
1
2
3
1
2
Figure 25.2: Series Expansions of x(π −x).
f(x) = sign(x) has the expansion
sign(x) ∼
∞
X
n=−∞
*
1
√
2π
eınξ
 sign(ξ)
+
1
√
2π
eınx
= 1
2π
∞
X
n=−∞
Z π
−π
e−ınξ sign(ξ) dξ eınx
= 1
2π
∞
X
n=−∞
Z 0
−π
−e−ınξ dξ +
Z π
0
e−ınξ dξ

eınx
= 1
π
∞
X
n=−∞
1 −(−1)n
ın
eınx .
1271

In terms of real functions, this is
= 1
π
∞
X
n=−∞
1 −(−1)n
ın
(cos(nx) + ı sin(nx))
= 2
π
∞
X
n=1
1 −(−1)n
ın
sin(nx)
sign(x) ∼4
π
∞
X
n=1
oddn
1
n sin(nx).
25.9
Least Squares Fit to a Function and Completeness
Let {φ1, φ2, φ3, . . .} be a set of real, square integrable functions that are orthonormal with respect to the weighting
function σ(x) on the interval [a, b]. That is,
⟨φn|σ|φm⟩= δnm.
Let f(x) be some square integrable function deﬁned on the same interval. We would like to approximate the function
f(x) with a ﬁnite orthonormal series.
f(x) ≈
N
X
n=1
αnφn(x)
f(x) may or may not have a uniformly convergent expansion in the orthonormal functions.
We would like to choose the αn so that we get the best possible approximation to f(x). The most common measure
of how well a series approximates a function is the least squares measure. The error is deﬁned as the integral of the
weighting function times the square of the deviation.
E =
Z b
a
σ(x)
 
f(x) −
N
X
n=1
αnφn(x)
!2
dx
1272

The “best” ﬁt is found by choosing the αn that minimize E. Let cn be the Fourier coeﬃcients of f(x).
cn = ⟨φn|σ|f⟩
we expand the integral for E.
E(α) =
Z b
a
σ(x)
 
f(x) −
N
X
n=1
αnφn(x)
!2
dx
=

f −
N
X
n=1
αnφn
 σ
 f −
N
X
n=1
αnφn

= ⟨f|σ|f⟩−2

N
X
n=1
αnφn
σ
f

+

N
X
n=1
αnφn
σ

N
X
n=1
αnφn

= ⟨f|σ|f⟩−2
N
X
n=1
αn⟨φn|σ|f⟩+
N
X
n=1
N
X
m=1
αnαm⟨φn|σ|φm⟩
= ⟨f|σ|f⟩−2
N
X
n=1
αncn +
N
X
n=1
α2
n
= ⟨f|σ|f⟩+
N
X
n=1
(αn −cn)2 −
N
X
n=1
c2
n
Each term involving αn in non-negative and is minimized for αn = cn. The Fourier coeﬃcients give the least squares
approximation to a function. The least squares ﬁt to f(x) is thus
f(x) ≈
N
X
n=1
⟨φn|σ|f⟩φn(x).
1273

Result 25.9.1 If {φ1, φ2, φ3, . . .} is a set of real, square integrable functions that are orthog-
onal with respect to σ(x) then the least squares ﬁt of the ﬁrst N orthogonal functions to the
square integrable function f(x) is
f(x) ≈
N
X
n=1
⟨φn|σ|f⟩
⟨φn|σ|φn⟩φn(x).
If the set is orthonormal, this formula reduces to
f(x) ≈
N
X
n=1
⟨φn|σ|f⟩φn(x).
Since the error in the approximation E is a nonnegative number we can obtain on inequality on the sum of the
squared coeﬃcients.
E = ⟨f|σ|f⟩−
N
X
n=1
c2
n
N
X
n=1
c2
n ≤⟨f|σ|f⟩
This equation is known as Bessel’s Inequality. Since ⟨f|σ|f⟩is just a nonnegative number, independent of N, the
sum P∞
n=1 c2
n is convergent and cn →0 as n →∞
Convergence in the Mean.
If the error E goes to zero as N tends to inﬁnity
lim
N→∞
Z b
a
σ(x)
 
f(x) −
N
X
n=1
cnφn(x)
!2
dx = 0,
1274

then the sum converges in the mean to f(x) relative to the weighting function σ(x). This implies that
lim
N→∞
 
⟨f|σ|f⟩−
N
X
n=1
c2
n
!
= 0
∞
X
n=1
c2
n = ⟨f|σ|f⟩.
This is known as Parseval’s identity.
Completeness.
Consider a set of functions {φ1, φ2, φ3, . . .} that is orthogonal with respect to the weighting function
σ(x). If every function f(x) that is square integrable with respect to σ(x) has an orthogonal series expansion
f(x) ∼
∞
X
n=1
cnφn(x)
that converges in the mean to f(x), then the set is complete.
25.10
Closure Relation
Let {φ1, φ2, . . .} be an orthonormal, complete set on the domain [a, b]. For any square integrable function f(x) we can
write
f(x) ∼
∞
X
n=1
cnφn(x).
1275

Here the cn are the generalized Fourier coeﬃcients and the sum converges in the mean to f(x). Substituting the
expression for the Fourier coeﬃcients into the sum yields
f(x) ∼
∞
X
n=1
⟨φn|f⟩φn(x)
=
∞
X
n=1
Z b
a
φn(ξ)f(ξ) dξ

φn(x).
Since the sum is not necessarily uniformly convergent, we are not justiﬁed in exchanging the order of summation and
integration. . . but what the heck, let’s do it anyway.
=
Z b
a
 ∞
X
n=1
φn(ξ)f(ξ)φn(x)
!
dξ
=
Z b
a
 ∞
X
n=1
φn(ξ)φn(x)
!
f(ξ) dξ
The sum behaves like a Dirac delta function. Recall that δ(x −ξ) satisﬁes the equation
f(x) =
Z b
a
δ(x −ξ)f(ξ) dξ
for x ∈(a, b).
Thus we could say that the sum is a representation of δ(x −ξ). Note that a series representation of the delta function
could not be convergent, hence the necessity of throwing caution to the wind when we interchanged the summation
and integration in deriving the series. The closure relation for an orthonormal, complete set states
∞
X
n=1
φn(x)φn(ξ) ∼δ(x −ξ).
1276

Alternatively, you can derive the closure relation by computing the generalized Fourier coeﬃcients of the delta
function.
δ(x −ξ) ∼
∞
X
n=1
cnφn(x)
cn = ⟨φn|δ(x −ξ)⟩
=
Z b
a
φn(x)δ(x −ξ) dx
= φn(ξ)
δ(x −ξ) ∼
∞
X
n=1
φn(x)φn(ξ)
Result 25.10.1 If {φ1, φ2, . . .} is an orthogonal, complete set on the domain [a, b], then
∞
X
n=1
φn(x)φn(ξ)
∥φn∥2
∼δ(x −ξ).
If the set is orthonormal, then
∞
X
n=1
φn(x)φn(ξ) ∼δ(x −ξ).
Example 25.10.1 The integral of the Dirac delta function is the Heaviside function. On the interval x ∈(−π, π)
Z x
−π
δ(t) dt = H(x) =
(
1
for 0 < x < π
0
for −π < x < 0.
1277

Consider the orthonormal, complete set {. . . ,
1
√
2π e−ıx,
1
√
2π,
1
√
2π eıx, . . .} on the domain [−π, π]. The delta function
has the series
δ(t) ∼
∞
X
n=−∞
1
√
2π
eınt
1
√
2π
e−ın0 = 1
2π
∞
X
n=−∞
eınt .
We will ﬁnd the series expansion of the Heaviside function ﬁrst by expanding directly and then by integrating the
expansion for the delta function.
Finding the series expansion of H(x) directly.
The generalized Fourier coeﬃcients of H(x) are
c0 =
Z π
−π
1
√
2πH(x) dx
=
1
√
2π
Z π
0
dx
=
rπ
2
cn =
Z π
−π
1
√
2π
e−ınx H(x) dx
=
1
√
2π
Z π
0
e−ınx dx
= 1 −(−1)n
ın
√
2π
.
1278

Thus the Heaviside function has the expansion
H(x) ∼
rπ
2
1
√
2π +
∞
X
n=−∞
n̸=0
1 −(−1)n
ın
√
2π
1
√
2π
eınx
= 1
2 + 1
π
∞
X
n=1
1 −(−1)n
n
sin(nx)
H(x) ∼1
2 + 2
π
∞
X
n=1
oddn
1
n sin(nx).
Integrating the series for δ(t).
Z x
−π
δ(t) dt ∼1
2π
Z x
−π
∞
X
n=−∞
eınt dt
= 1
2π


(x + π) +
∞
X
n=−∞
n̸=0
 1
in eınt
x
−π



= 1
2π


(x + π) +
∞
X
n=−∞
n̸=0
1
ın
  eınx −(−1)n



= x
2π + 1
2 + 1
2π
∞
X
n=1
1
ın
  eınx −e−ınx −(−1)n + (−1)n
= x
2π + 1
2 + 1
π
∞
X
n=1
1
n sin(nx)
1279

Expanding
x
2π in the orthonormal set,
x
2π ∼
∞
X
n=−∞
cn
1
√
2π
eınx .
c0 =
Z π
−π
1
√
2π
x
2π dx = 0
cn =
Z π
−π
1
√
2π
e−ınx x
2π dx = ı(−1)n
n
√
2π
x
2π ∼
∞
X
n=−∞
n̸=0
ı(−1)n
n
√
2π
1
√
2π
eınx = −1
π
∞
X
n=1
(−1)n sin(nx)
Substituting the series for
x
2π into the expression for the integral of the delta function,
Z x
−π
δ(t) dt ∼1
2 + 1
π
∞
X
n=1
1 −(−1)n
n
sin(nx)
Z x
−π
δ(t) dt ∼1
2 + 2
π
∞
X
n=1
oddn
1
n sin(nx).
Thus we see that the series expansions of the Heaviside function and the integral of the delta function are the same.
25.11
Linear Operators
1280

25.12
Exercises
Exercise 25.1
1. Suppose {φk(x)}∞
k=0 is an orthogonal system on [a, b].
Show that any ﬁnite set of the φj(x) is a linearly
independent set on [a, b]. That is, if {φj1(x), φj2(x), . . . , φjn(x)} is the set and all the jν are distinct, then
a1φj1(x) + a2φj2(x) + · · · + anφjn(x) = 0
on
a ≤x ≤b
is true iﬀ: a1 = a2 = · · · = an = 0.
2. Show that the complex functions φk(x) ≡eıkπx/L, k = 0, 1, 2, . . . are orthogonal in the sense that
R L
−L φk(x)φ∗
n(x) dx =
0, for n ̸= k. Here φ∗
n(x) is the complex conjugate of φn(x).
Hint, Solution
1281

25.13
Hints
Hint 25.1
1282

25.14
Solutions
Solution 25.1
1.
a1φj1(x) + a2φj2(x) + · · · + anφjn(x) = 0
n
X
k=1
akφjk(x) = 0
We take the inner product with φjν for any ν = 1, . . . , n. (⟨φ, ψ⟩≡
R b
a φ(x)ψ∗(x) dx.)
* n
X
k=1
akφjk, φjν
+
= 0
We interchange the order of summation and integration.
n
X
k=1
ak ⟨φjk, φjν⟩= 0
⟨φjkφjν⟩= 0 for j ̸= ν.
aν ⟨φjνφjν⟩= 0
⟨φjνφjν⟩̸= 0.
aν = 0
Thus we see that a1 = a2 = · · · = an = 0.
1283

2. For k ̸= n, ⟨φk, φn⟩= 0.
⟨φk, φn⟩≡
Z L
−L
φk(x)φ∗
n(x) dx
=
Z L
−L
eıkπx/L e−ınπx/L dx
=
Z L
−L
eı(k−n)πx/L dx
=
 eı(k−n)πx/L
ı(k −n)π/L
L
−L
= eı(k−n)π −e−ı(k−n)π
ı(k −n)π/L
= 2L sin((k −n)π)
(k −n)π
= 0
1284

Chapter 26
Self Adjoint Linear Operators
26.1
Adjoint Operators
The adjoint of an operator, L∗, satisﬁes
⟨v|Lu⟩−⟨L∗v|u⟩= 0
for all elements u an v. This is known as Green’s Identity.
The adjoint of a matrix.
For vectors, one can represent linear operators L with matrix multiplication.
Lx ≡Ax
1285

Let B = A∗be the adjoint of the matrix A. We determine the adjoint of A from Green’s Identity.
⟨x|Ay⟩−⟨Bx|y⟩= 0
x · Ay = Bx · y
xTAy = Bx
Ty
xTAy = xTB
Ty
yTA
Tx = yTBxB = A
T
Thus we see that the adjoint of a matrix is the conjugate transpose of the matrix, A∗= A
T. The conjugate transpose
is also called the Hermitian transpose and is denoted AH.
The adjoint of a diﬀerential operator.
Consider a second order linear diﬀerential operator acting on C2 functions
deﬁned on (a . . . b) which satisfy certain boundary conditions.
Lu ≡p2(x)u′′ + p1(x)u′ + p0(x)u
26.2
Self-Adjoint Operators
Matrices.
A matrix is self-adjoint if it is equal to its conjugate transpose A = AH ≡A
T. Such matrices are called
Hermitian. For a Hermitian matrix H, Green’s identity is
⟨y|Hx⟩= ⟨Hy|x⟩
y · Hx = Hy · x
1286

The eigenvalues of a Hermitian matrix are real. Let x be an eigenvector with eigenvalue λ.
⟨x|Hx⟩= ⟨Hx|x⟩
⟨x|λx⟩−⟨λx|x⟩= 0
(λ −λ)⟨x|x⟩= 0
λ = λ
The eigenvectors corresponding to distinct eigenvalues are distinct. Let x and y be eigenvectors with distinct eigenvalues
λ and µ.
⟨y|Hx⟩= ⟨Hy|x⟩
⟨y|λx⟩−⟨µy|x⟩= 0
(λ −µ)⟨y|λx⟩= 0
(λ −µ)⟨y|x⟩= 0
⟨y|x⟩= 0
Furthermore, all Hermitian matrices are similar to a diagonal matrix and have a complete set of orthogonal eigenvectors.
Trigonometric Series.
Consider the problem
−y′′ = λy,
y(0) = y(2π),
y′(0) = y′(2π).
1287

We verify that the diﬀerential operator L = −d2
dx2 with periodic boundary conditions is self-adjoint.
⟨v|Lu⟩= ⟨v| −u′′⟩
= [−vu′]2π
0 −⟨v′| −u′⟩
= ⟨v′|u′⟩
=

v′u
2π
0 −⟨v′′|u⟩
= ⟨−v′′|u⟩
= ⟨Lv|u⟩
The eigenvalues and eigenfunctions of this problem are
λ0 = 0,
φ0 = 1
λn = n2,
φ(1)
n = cos(nx),
φ(2)
n = sin(nx),
n ∈Z+
1288

26.3
Exercises
1289

26.4
Hints
1290

26.5
Solutions
1291

Chapter 27
Self-Adjoint Boundary Value Problems
Seize the day and throttle it.
-Calvin
27.1
Summary of Adjoint Operators
The adjoint of the operator
L[y] = pn
dny
dxn + pn−1
dn−1y
dxn−1 + · · · + p0y,
is deﬁned
L∗[y] = (−1)n dn
dxn(pny) + (−1)n−1 dn−1
dxn−1(pn−1y) + · · · + p0y
If each of the pk is k times continuously diﬀerentiable and u and v are n times continuously diﬀerentiable on some
interval, then on that interval Lagrange’s identity states
vL[u] −uL∗[v] = d
dxB[u, v]
1292

where B[u, v] is the bilinear form
B[u, v] =
n
X
m=1
X
j+k=m−1
j≥0,k≥0
(−1)ju(k)(pmv)(j).
If L is a second order operator then
vL[u] −uL∗[v] = u′′p2v + u′p1v + u

−p2v′′ + (−2p′
2 + p1)v′ + (−p′′
2 + p′
1)v

.
Integrating Lagrange’s identity on its interval of validity gives us Green’s formula.
Z b
a

vL[u] −uL∗[v]

dx = ⟨v|L[u]⟩−⟨L∗[v]|u⟩= B[u, v]

x=b −B[u, v]

x=a
27.2
Formally Self-Adjoint Operators
Example 27.2.1 The linear operator
L[y] = x2y′′ + 2xy′ + 3y
has the adjoint operator
L∗[y] = d2
dx2(x2y) −d
dx(2xy) + 3y
= x2y′′ + 4xy′ + 2y −2xy′ −2y + 3y
= x2y′′ + 2xy′ + 3y.
In Example 27.2.1, the adjoint operator is the same as the operator. If L = L∗, the operator is said to be formally
self-adjoint.
1293

Most of the diﬀerential equations that we study in this book are second order, formally self-adjoint, with real-valued
coeﬃcient functions. Thus we wish to ﬁnd the general form of this operator. Consider the operator
L[y] = p2y′′ + p1y′ + p0y,
where the pj’s are real-valued functions. The adjoint operator then is
L∗[y] = d2
dx2(p2y) −d
dx(p1y) + p0y
= p2y′′ + 2p′
2y′ + p′′
2y −p1y′ −p′
1y + p0y
= p2y′′ + (2p′
2 −p1)y′ + (p′′
2 −p′
1 + p0)y.
Equating L and L∗yields the two equations,
2p′
2 −p1 = p1,
p′′
2 −p′
1 + p0 = p0
p′
2 = p1,
p′′
2 = p′
1.
Thus second order, formally self-adjoint operators with real-valued coeﬃcient functions have the form
L[y] = p2y′′ + p′
2y′ + p0y,
which is equivalent to the form
L[y] = d
dx(py′) + qy.
Any linear diﬀerential equation of the form
L[y] = y′′ + p1y′ + p0y = f(x),
where each pj is j times continuously diﬀerentiable and real-valued, can be written as a formally self adjoint equation.
We just multiply by the factor,
eP(x) = exp(
Z x
p1(ξ) dξ)
1294

to obtain
exp [P(x)] (y′′ + p1y′ + p0y) = exp [P(x)] f(x)
d
dx (exp [P(x)] y′) + exp [P(x)] p0y = exp [P(x)] f(x).
Example 27.2.2 Consider the equation
y′′ + 1
xy′ + y = 0.
Multiplying by the factor
exp
Z x 1
ξ dξ

= elog x = x
will make the equation formally self-adjoint.
xy′′ + y′ + xy = 0
d
dx(xy′) + xy = 0
Result 27.2.1 If L = L∗then the linear operator L is formally self-adjoint. Second order
formally self-adjoint operators have the form
L[y] = d
dx(py′) + qy.
Any diﬀerential equation of the form
L[y] = y′′ + p1y′ + p0y = f(x),
where each pj is j times continuously diﬀerentiable and real-valued, can be written as a
formally self adjoint equation by multiplying the equation by the factor exp(
R x p1(ξ) dξ).
1295

27.3
Self-Adjoint Problems
Consider the nth order formally self-adjoint equation L[y] = 0, on the domain a ≤x ≤b subject to the boundary
conditions, Bj[y] = 0 for j = 1, . . . , n. where the boundary conditions can be written
Bj[y] =
n
X
k=1
αjky(k−1)(a) + βjky(k−1)(b) = 0.
If the boundary conditions are such that Green’s formula reduces to
⟨v|L[u]⟩−⟨L[v]|u⟩= 0
then the problem is self-adjoint
Example 27.3.1 Consider the formally self-adjoint equation −y′′ = 0, subject to the boundary conditions y(0) =
y(π) = 0. Green’s formula is
⟨v| −u′′⟩−⟨−v′′|u⟩= [u′(−v) −u(−v)′]π
0
= [uv′ −u′v]π
0
= 0.
Thus this problem is self-adjoint.
27.4
Self-Adjoint Eigenvalue Problems
Associated with the self-adjoint problem
L[y] = 0,
subject to
Bj[y] = 0,
is the eigenvalue problem
L[y] = λy,
subject to
Bj[y] = 0.
1296

This is called a self-adjoint eigenvalue problem. The values of λ for which there exist nontrivial solutions to this problem
are called eigenvalues. The functions that satisfy the equation when λ is an eigenvalue are called eigenfunctions.
Example 27.4.1 Consider the self-adjoint eigenvalue problem
−y′′ = λy,
subject to
y(0) = y(π) = 0.
First consider the case λ = 0. The general solution is
y = c1 + c2x.
Only the trivial solution satisﬁes the boundary conditions. λ = 0 is not an eigenvalue. Now consider λ ̸= 0. The
general solution is
y = c1 cos
√
λx

+ c2 sin
√
λx

.
The solution that satisﬁes the left boundary condition is
y = c sin
√
λx

.
For non-trivial solutions, we must have
sin
√
λπ

= 0,
λ = n2,
n ∈N.
Thus the eigenvalues λn and eigenfunctions φn are
λn = n2,
φn = sin(nx),
for n = 1, 2, 3, . . .
Self-adjoint eigenvalue problems have a number a interesting properties. We will devote the rest of this section to
developing some of these properties.
1297

Real Eigenvalues.
The eigenvalues of a self-adjoint problem are real. Let λ be an eigenvalue with the eigenfunction
φ. Green’s formula states
⟨φ|L[φ]⟩−⟨L[φ]|φ⟩= 0
⟨φ|λφ⟩−⟨λφ|φ⟩= 0
(λ −λ)⟨φ|φ⟩= 0
Since φ ̸≡0, ⟨φ|φ⟩> 0. Thus λ = λ and λ is real.
Orthogonal Eigenfunctions.
The eigenfunctions corresponding to distinct eigenvalues are orthogonal. Let λn and
λm be distinct eigenvalues with the eigenfunctions φn and φm. Using Green’s formula,
⟨φn|L[φm]⟩−⟨L[φn]|φm⟩= 0
⟨φn|λmφm⟩−⟨λnφn|φm⟩= 0
(λm −λn)⟨φn|φm⟩= 0.
Since the eigenvalues are real,
(λm −λn)⟨φn|φm⟩= 0.
Since the two eigenvalues are distinct, ⟨φn|φm⟩= 0 and thus φn and φm are orthogonal.
*Enumerable Set of Eigenvalues.
The eigenvalues of a self-adjoint eigenvalue problem form an enumerable set
with no ﬁnite cluster point. Consider the problem
L[y] = λy on a ≤x ≤b,
subject to Bj[y] = 0.
Let {ψ1, ψ2, . . . , ψn} be a fundamental set of solutions at x = x0 for some a ≤x0 ≤b. That is,
ψ(k−1)
j
(x0) = δjk.
1298

The key to showing that the eigenvalues are enumerable, is that the ψj are entire functions of λ. That is, they are
analytic functions of λ for all ﬁnite λ. We will not prove this.
The boundary conditions are
Bj[y] =
n
X
k=1

αjky(k−1)(a) + βjky(k−1)(b)

= 0.
The eigenvalue problem has a solution for a given value of λ if y = Pn
k=1 ckψk satisﬁes the boundary conditions. That
is,
Bj
" n
X
k=1
ckψk
#
=
n
X
k=1
ckBj[ψk] = 0
for j = 1, . . . , n.
Deﬁne an n × n matrix M such that Mjk = Bk[ψj]. Then if ⃗c = (c1, c2, . . . , cn), the boundary conditions can be
written in terms of the matrix equation M⃗c = 0. This equation has a solution if and only if the determinant of the
matrix is zero. Since the ψj are entire functions of λ, ∆[M] is an entire function of λ. The eigenvalues are real, so
∆[M] has only real roots. Since ∆[M] is an entire function, (that is not identically zero), with only real roots, the
roots of ∆[M] can only cluster at inﬁnity. Thus the eigenvalues of a self-adjoint problem are enumerable and can only
cluster at inﬁnity.
An example of a function whose roots have a ﬁnite cluster point is sin(1/x). This function, (graphed in Figure 27.1),
is clearly not analytic at the cluster point x = 0.
Inﬁnite Number of Eigenvalues.
Though we will not show it, self-adjoint problems have an inﬁnite number of
eigenvalues. Thus the eigenfunctions form an inﬁnite orthogonal set.
Eigenvalues of Second Order Problems.
Consider the second order, self-adjoint eigenvalue problem
L[y] = (py′)′ + qy = λy,
on a ≤x ≤b,
subject to Bj[y] = 0.
1299

-1
1
Figure 27.1: Graph of sin(1/x).
Let λn be an eigenvalue with the eigenfunction φn.
⟨φn|L[φn]⟩= ⟨φn|λnφn⟩
⟨φn|(pφ′
n)′ + qφn⟩= λn⟨φn|φn⟩
Z b
a
φn(pφ′
n)′ dx + ⟨φn|q|φn⟩= λn⟨φn|φn⟩

φnpφ′
n
b
a −
Z b
a
φn
′pφ′
n dx + ⟨φn|q|φn⟩= λn⟨φn|φn⟩
λn = [pφnφ′
n]b
a −⟨φ′
n|p|φ′
n⟩+ ⟨φn|q|φn⟩
⟨φn|φn⟩
1300

Thus we can express each eigenvalue in terms of its eigenfunction. You might think that this formula is just a
shade less than worthless. When solving an eigenvalue problem you have to ﬁnd the eigenvalues before you determine
the eigenfunctions. Thus this formula could not be used to compute the eigenvalues. However, we can often use the
formula to obtain information about the eigenvalues before we solve a problem.
Example 27.4.2 Consider the self-adjoint eigenvalue problem
−y′′ = λy,
y(0) = y(π) = 0.
The eigenvalues are given by the formula
λn =

(−1)φφ′b
a −⟨φ′
n|(−1)|φ′
n⟩+ ⟨φn|0|φn⟩
⟨φn|φn⟩
= 0 + ⟨φ′
n|φ′
n⟩+ 0
⟨φn|φn⟩
.
We see that λn ≥0. If λn = 0 then ⟨φ′
n|φ′
n⟩= 0,which implies that φn = const. The only constant that satisﬁes the
boundary conditions is φn = 0 which is not an eigenfunction since it is the trivial solution. Thus the eigenvalues are
positive.
27.5
Inhomogeneous Equations
Let the problem,
L[y] = 0,
Bk[y] = 0,
be self-adjoint. If the inhomogeneous problem,
L[y] = f,
Bk[y] = 0,
has a solution, then we we can write this solution in terms of the eigenfunction of the associated eigenvalue problem,
L[y] = λy,
Bk[y] = 0.
1301

We denote the eigenvalues as λn and the eigenfunctions as φn for n ∈Z+. For the moment we assume that
λ = 0 is not an eigenvalue and that the eigenfunctions are real-valued. We expand the function f(x) in a series of the
eigenfunctions.
f(x) =
X
fnφn(x),
fn = ⟨φn|f⟩
∥φn∥
We expand the inhomogeneous solution in a series of eigenfunctions and substitute it into the diﬀerential equation.
L[y] = f
L
hX
ynφn(x)
i
=
X
fnφn(x)
X
λnynφn(x) =
X
fnφn(x)
yn = fn
λn
The inhomogeneous solution is
y(x) =
X ⟨φn|f⟩
λn∥φn∥φn(x).
(27.1)
As a special case we consider the Green function problem,
L[G] = δ(x −ξ),
Bk[G] = 0,
We expand the Dirac delta function in an eigenfunction series.
δ(x −ξ) =
X ⟨φn|δ⟩
∥φn∥φn(x) =
X φn(ξ)φn(x)
∥φn∥
The Green function is
G(x|ξ) =
X φn(ξ)φn(x)
λn∥φn∥
.
1302

We corroborate Equation 27.1 by solving the inhomogeneous equation in terms of the Green function.
y =
Z b
a
G(x|ξ)f(ξ) dξ
y =
Z b
a
X φn(ξ)φn(x)
λn∥φn∥
f(ξ) dξ
y =
X R b
a φn(ξ)f(ξ) dξ
λn∥φn∥
φn(x)
y =
X ⟨φn|f⟩
λn∥φn∥φn(x)
Example 27.5.1 Consider the Green function problem
G′′ + G = δ(x −ξ),
G(0|ξ) = G(1|ξ) = 0.
First we examine the associated eigenvalue problem.
φ′′ + φ = λφ,
φ(0) = φ(1) = 0
φ′′ + (1 −λ)φ = 0,
φ(0) = φ(1) = 0
λn = 1 −(nπ)2,
φn = sin(nπx),
n ∈Z+
We write the Green function as a series of the eigenfunctions.
G(x|ξ) = 2
∞
X
n=1
sin(nπξ) sin(nπx)
1 −(nπ)2
1303

27.6
Exercises
Exercise 27.1
Show that the operator adjoint to
Ly = y(n) + p1(z)y(n−1) + p2(z)y(n−2) + · · · + pn(z)y
is given by
My = (−1)nu(n) + (−1)n−1(p1(z)u)(n−1) + (−1)n−2(p2(z)u)(n−2) + · · · + pn(z)u.
Hint, Solution
1304

27.7
Hints
Hint 27.1
1305

27.8
Solutions
Solution 27.1
Consider u(x), v(x) ∈Cn. (Cn is the set of n times continuously diﬀerentiable functions). First we prove the preliminary
result
uv(n) −(−1)nu(n)v = d
dx
n−1
X
k=0
(−1)ku(k)v(n−k−1)
(27.2)
by simplifying the right side.
d
dx
n−1
X
k=0
(−1)ku(k)v(n−k−1) =
n−1
X
k=0
(−1)k  u(k)v(n−k) + u(k+1)v(n−k−1)
=
n−1
X
k=0
(−1)ku(k)v(n−k) −
n−1
X
k=0
(−1)k+1u(k+1)v(n−k−1)
=
n−1
X
k=0
(−1)ku(k)v(n−k) −
n
X
k=1
(−1)ku(k)v(n−k)
= (−1)0u(0)vn−0 −(−1)nu(n)v(n−n)
= uv(n) −(−1)nu(n)v
We deﬁne p0(x) = 1 so that we can write the operators in a nice form.
Ly =
n
X
m=0
pm(z)y(n−m),
Mu =
n
X
m=0
(−1)m(pm(z)u)(n−m)
Now we show that M is the adjoint to L.
uLy −yMu = u
n
X
m=0
pm(z)y(n−m) −y
n
X
m=0
(−1)m(pm(z)u)(n−m)
=
n
X
m=0
 upm(z)y(n−m) −(pm(z)u)(n−m)y

1306

We use Equation 27.2.
=
n
X
m=0
d
dz
n−m−1
X
k=0
(−1)k(upm(z))(k)y(n−m−k−1)
uLy −yMu = d
dz
n
X
m=0
n−m−1
X
k=0
(−1)k(upm(z))(k)y(n−m−k−1)
1307

Chapter 28
Fourier Series
Every time I close my eyes
The noise inside me ampliﬁes
I can’t escape
I relive every moment of the day
Every misstep I have made
Finds a way it can invade
My every thought
And this is why I ﬁnd myself awake
-Failure
-Tom Shear (Assemblage 23)
28.1
An Eigenvalue Problem.
A self adjoint eigenvalue problem.
Consider the eigenvalue problem
y′′ + λy = 0,
y(−π) = y(π),
y′(−π) = y′(π).
1308

We rewrite the equation so the eigenvalue is on the right side.
L[y] ≡−y′′ = λy
We demonstrate that this eigenvalue problem is self adjoint.
⟨v|L[u]⟩−⟨L[v]|u⟩= ⟨v| −u′′⟩−⟨−v′′|u⟩
= [−¯vu′]π
−π + ⟨v′|u′⟩−[−¯v′u]−ππ −⟨v′|u′⟩
= −v(π)u′(π) + v(−π)u′(−π) + v′(π)u(π) −v′(−π)u(−π)
= −v(π)u′(π) + v(π)u′(π) + v′(π)u(π) −v′(π)u(π)
= 0
Since Green’s Identity reduces to ⟨v|L[u]⟩−⟨L[v]|u⟩= 0, the problem is self adjoint. This means that the eigenvalues
are real and that eigenfunctions corresponding to distinct eigenvalues are orthogonal. We compute the Rayleigh quotient
for an eigenvalue λ with eigenfunction φ.
λ = −[¯φφ′]π
−π + ⟨φ′|φ′⟩
⟨φ|φ⟩
= −φ(π)φ′(π) + φ(−π)φ′(−π) + ⟨φ′|φ′⟩
⟨φ|φ⟩
= −φ(π)φ′(π) + φ(π)φ′(π) + ⟨φ′|φ′⟩
⟨φ|φ⟩
= ⟨φ′|φ′⟩
⟨φ|φ⟩
We see that the eigenvalues are non-negative.
Computing the eigenvalues and eigenfunctions.
Now we ﬁnd the eigenvalues and eigenfunctions. First we
consider the case λ = 0. The general solution of the diﬀerential equation is
y = c1 + c2x.
1309

The solution that satisﬁes the boundary conditions is y = const.
Now consider λ > 0. The general solution of the diﬀerential equation is
y = c1 cos
√
λx

+ c2 sin
√
λx

.
We apply the ﬁrst boundary condition.
y(−π) = y(π)
c1 cos

−
√
λπ

+ c2 sin

−
√
λπ

= c1 cos
√
λπ

+ c2 sin
√
λπ

c1 cos
√
λπ

−c2 sin
√
λπ

= c1 cos
√
λπ

+ c2 sin
√
λπ

c2 sin
√
λπ

= 0
Then we apply the second boundary condition.
y′(−π) = y′(π)
−c1
√
λ sin

−
√
λπ

+ c2
√
λ cos

−
√
λπ

= −c1
√
λ sin
√
λπ

+ c2
√
λ cos
√
λπ

c1 sin
√
λπ

+ c2 cos
√
λπ

= −c1 sin
√
λπ

+ c2 cos
√
λπ

c1 sin
√
λπ

= 0
To satisify the two boundary conditions either c1 = c2 = 0 or sin
√
λπ

= 0. The former yields the trivial solution.
The latter gives us the eigenvalues λn = n2, n ∈Z+. The corresponding solution is
yn = c1 cos(nx) + c2 sin(nx).
There are two eigenfunctions for each of the positive eigenvalues.
We choose the eigenvalues and eigenfunctions.
λ0 = 0,
φ0 = 1
2
λn = n2,
φ2n−1 = cos(nx),
φ2n = sin(nx),
for n = 1, 2, 3, . . .
1310

Orthogonality of Eigenfunctions.
We know that the eigenfunctions of distinct eigenvalues are orthogonal. In
addition, the two eigenfunctions of each positive eigenvalue are orthogonal.
Z π
−π
cos(nx) sin(nx) dx =
 1
2n sin2(nx)
π
−π
= 0
Thus the eigenfunctions { 1
2, cos(x), sin(x), cos(2x), sin(2x)} are an orthogonal set.
28.2
Fourier Series.
A series of the eigenfunctions
φ0 = 1
2,
φ(1)
n = cos(nx),
φ(2)
n = sin(nx),
for n ≥1
is
1
2a0 +
∞
X
n=1
 an cos(nx) + bn sin(nx)

.
This is known as a Fourier series. (We choose φ0 = 1
2 so all of the eigenfunctions have the same norm.) A fairly general
class of functions can be expanded in Fourier series. Let f(x) be a function deﬁned on −π < x < π. Assume that
f(x) can be expanded in a Fourier series
f(x) ∼1
2a0 +
∞
X
n=1
 an cos(nx) + bn sin(nx)

.
(28.1)
Here the “∼” means “has the Fourier series”. We have not said if the series converges yet. For now let’s assume that
the series converges uniformly so we can replace the ∼with an =.
1311

We integrate Equation 28.1 from −π to π to determine a0.
Z π
−π
f(x) dx = 1
2a0
Z π
−π
dx +
Z π
−π
∞
X
n=1
an cos(nx) + bn sin(nx) dx
Z π
−π
f(x) dx = πa0 +
∞
X
n=1

an
Z π
−π
cos(nx) dx + bn
Z π
−π
sin(nx) dx

Z π
−π
f(x) dx = πa0
a0 = 1
π
Z π
−π
f(x) dx
Multiplying by cos(mx) and integrating will enable us to solve for am.
Z π
−π
f(x) cos(mx) dx = 1
2a0
Z π
−π
cos(mx) dx
+
∞
X
n=1

an
Z π
−π
cos(nx) cos(mx) dx + bn
Z π
−π
sin(nx) cos(mx) dx

All but one of the terms on the right side vanishes due to the orthogonality of the eigenfunctions.
Z π
−π
f(x) cos(mx) dx = am
Z π
−π
cos(mx) cos(mx) dx
Z π
−π
f(x) cos(mx) dx = am
Z π
−π
1
2 + cos(2mx)

dx
Z π
−π
f(x) cos(mx) dx = πam
am = 1
π
Z π
−π
f(x) cos(mx) dx.
1312

Note that this formula is valid for m = 0, 1, 2, . . ..
Similarly, we can multiply by sin(mx) and integrate to solve for bm. The result is
bm = 1
π
Z π
−π
f(x) sin(mx) dx.
an and bn are called Fourier coeﬃcients.
Although we will not show it, Fourier series converge for a fairly general class of functions. Let f(x−) denote the
left limit of f(x) and f(x+) denote the right limit.
Example 28.2.1 For the function deﬁned
f(x) =
(
0
for x < 0,
x + 1
for x ≥0,
the left and right limits at x = 0 are
f(0−) = 0,
f(0+) = 1.
Result 28.2.1 Let f(x) be a 2π-periodic function for which
R π
−π |f(x)| dx exists. Deﬁne the
Fourier coeﬃcients
an = 1
π
Z π
−π
f(x) cos(nx) dx,
bn = 1
π
Z π
−π
f(x) sin(nx) dx.
If x is an interior point of an interval on which f(x) has limited total ﬂuctuation, then the
Fourier series of f(x)
a0
2 +
∞
X
n=1
 an cos(nx) + bn sin(nx)

,
converges to 1
2(f(x−) + f(x+)). If f is continuous at x, then the series converges to f(x).
1313

Periodic Extension of a Function.
Let g(x) be a function that is arbitrarily deﬁned on −π ≤x < π. The
Fourier series of g(x) will represent the periodic extension of g(x). The periodic extension, f(x), is deﬁned by the two
conditions:
f(x) = g(x)
for −π ≤x < π,
f(x + 2π) = f(x).
The periodic extension of g(x) = x2 is shown in Figure 28.1.
-5
5
10
-2
2
4
6
8
10
Figure 28.1: The Periodic Extension of g(x) = x2.
Limited Fluctuation.
A function that has limited total ﬂuctuation can be written f(x) = ψ+(x) −ψ−(x), where
ψ+ and ψ−are bounded, nondecreasing functions. An example of a function that does not have limited total ﬂuctuation
1314

is sin(1/x), whose ﬂuctuation is unlimited at the point x = 0.
Functions with Jump Discontinuities.
Let f(x) be a discontinuous function that has a convergent Fourier
series. Note that the series does not necessarily converge to f(x). Instead it converges to ˆf(x) = 1
2(f(x−) + f(x+)).
Example 28.2.2 Consider the function deﬁned by
f(x) =
(
−x
for −π ≤x < 0
π −2x
for 0 ≤x < π.
The Fourier series converges to the function deﬁned by
ˆf(x) =









0
for x = −π
−x
for −π < x < 0
π/2
for x = 0
π −2x
for 0 < x < π.
The function ˆf(x) is plotted in Figure 28.2.
28.3
Least Squares Fit
Approximating a function with a Fourier series.
Suppose we want to approximate a 2π-periodic function
f(x) with a ﬁnite Fourier series.
f(x) ≈a0
2 +
N
X
n=1
(an cos(nx) + bn sin(nx))
1315

-3
-2
-1
1
2
3
-3
-2
-1
1
2
3
Figure 28.2: Graph of ˆf(x).
Here the coeﬃcients are computed with the familiar formulas. Is this the best approximation to the function? That is,
is it possible to choose coeﬃcients αn and βn such that
f(x) ≈α0
2 +
N
X
n=1
(αn cos(nx) + βn sin(nx))
would give a better approximation?
Least squared error ﬁt.
The most common criterion for ﬁnding the best ﬁt to a function is the least squares ﬁt.
The best approximation to a function is deﬁned as the one that minimizes the integral of the square of the deviation.
1316

Thus if f(x) is to be approximated on the interval a ≤x ≤b by a series
f(x) ≈
N
X
n=1
cnφn(x),
(28.2)
the best approximation is found by choosing values of cn that minimize the error E.
E ≡
Z b
a
f(x) −
N
X
n=1
cnφn(x)

2
dx
Generalized Fourier coeﬃcients.
We consider the case that the φn are orthogonal. For simplicity, we also
assume that the φn are real-valued. Then most of the terms will vanish when we interchange the order of integration
and summation.
E =
Z b
a
 
f 2 −2f
N
X
n=1
cnφn +
N
X
n=1
cnφn
N
X
m=1
cmφm
!
dx
E =
Z b
a
f 2 dx −2
N
X
n=1
cn
Z b
a
fφn dx +
N
X
n=1
N
X
m=1
cncm
Z b
a
φnφm dx
E =
Z b
a
f 2 dx −2
N
X
n=1
cn
Z b
a
fφn dx +
N
X
n=1
c2
n
Z b
a
φ2
n dx
E =
Z b
a
f 2 dx +
N
X
n=1

c2
n
Z b
a
φ2
n dx −2cn
Z b
a
fφn dx

We complete the square for each term.
E =
Z b
a
f 2 dx +
N
X
n=1


Z b
a
φ2
n dx
 
cn −
R b
a fφn dx
R b
a φ2
n dx
!2
−
 R b
a fφn dx
R b
a φ2
n dx
!2

1317

Each term involving cn is non-negative, and is minimized for
cn =
R b
a fφn dx
R b
a φ2
n dx
.
(28.3)
We call these the generalized Fourier coeﬃcients.
For such a choice of the cn, the error is
E =
Z b
a
f 2 dx −
N
X
n=1
c2
n
Z b
a
φ2
n dx.
Since the error is non-negative, we have
Z b
a
f 2 dx ≥
N
X
n=1
c2
n
Z b
a
φ2
n dx.
This is known as Bessel’s Inequality. If the series in Equation 28.2 converges in the mean to f(x), lim N →∞E = 0,
then we have equality as N →∞.
Z b
a
f 2 dx =
∞
X
n=1
c2
n
Z b
a
φ2
n dx.
This is Parseval’s equality.
Fourier coeﬃcients.
Previously we showed that if the series,
f(x) = a0
2 +
∞
X
n=1
(an cos(nx) + bn sin(nx),
converges uniformly then the coeﬃcients in the series are the Fourier coeﬃcients,
an = 1
π
Z π
−π
f(x) cos(nx) dx,
bn = 1
π
Z π
−π
f(x) sin(nx) dx.
1318

Now we show that by choosing the coeﬃcients to minimize the squared error, we obtain the same result. We apply
Equation 28.3 to the Fourier eigenfunctions.
a0 =
R π
−π f 1
2 dx
R π
−π
1
4 dx = 1
π
Z π
−π
f(x) dx
an =
R π
−π f cos(nx) dx
R π
−π cos2(nx) dx = 1
π
Z π
−π
f(x) cos(nx) dx
bn =
R π
−π f sin(nx) dx
R π
−π sin2(nx) dx = 1
π
Z π
−π
f(x) sin(nx) dx
28.4
Fourier Series for Functions Deﬁned on Arbitrary Ranges
If f(x) is deﬁned on c −d ≤x < c + d and f(x + 2d) = f(x), then f(x) has a Fourier series of the form
f(x) ∼a0
2 +
∞
X
n=1
an cos
nπ(x + c)
d

+ bn sin
nπ(x + c)
d

.
Since
Z c+d
c−d
cos2
nπ(x + c)
d

dx =
Z c+d
c−d
sin2
nπ(x + c)
d

dx = d,
the Fourier coeﬃcients are given by the formulas
an = 1
d
Z c+d
c−d
f(x) cos
nπ(x + c)
d

dx
bn = 1
d
Z c+d
c−d
f(x) sin
nπ(x + c)
d

dx.
1319

Example 28.4.1 Consider the function deﬁned by
f(x) =





x + 1
for −1 ≤x < 0
x
for 0 ≤x < 1
3 −2x
for 1 ≤x < 2.
This function is graphed in Figure 28.3.
The Fourier series converges to ˆf(x) = (f(x−) + f(x+))/2,
ˆf(x) =















−1
2
for x = −1
x + 1
for −1 < x < 0
1
2
for x = 0
x
for 0 < x < 1
3 −2x
for 1 ≤x < 2.
ˆf(x) is also graphed in Figure 28.3.
The Fourier coeﬃcients are
an =
1
3/2
Z 2
−1
f(x) cos
2nπ(x + 1/2)
3

dx
= 2
3
Z 5/2
−1/2
f(x −1/2) cos
2nπx
3

dx
= 2
3
Z 1/2
−1/2
(x + 1/2) cos
2nπx
3

dx + 2
3
Z 3/2
1/2
(x −1/2) cos
2nπx
3

dx
+ 2
3
Z 5/2
3/2
(4 −2x) cos
2nπx
3

dx
= −
1
(nπ)2 sin
2nπ
3
 h
2(−1)nnπ + 9 sin
nπ
3
i
1320

-1
-0.5
0.5
1
1.5
2
-1
-0.5
0.5
1
-1
-0.5
0.5
1
1.5
2
-1
-0.5
0.5
1
Figure 28.3: A Function Deﬁned on the range −1 ≤x < 2 and the Function to which the Fourier Series Converges.
bn =
1
3/2
Z 2
−1
f(x) sin
2nπ(x + 1/2)
3

dx
= 2
3
Z 5/2
−1/2
f(x −1/2) sin
2nπx
3

dx
= 2
3
Z 1/2
−1/2
(x + 1/2) sin
2nπx
3

dx + 2
3
Z 3/2
1/2
(x −1/2) sin
2nπx
3

dx
+ 2
3
Z 5/2
3/2
(4 −2x) sin
2nπx
3

dx
= −
2
(nπ)2 sin2 nπ
3
 h
2(−1)nnπ + 4nπ cos
nπ
3

−3 sin
nπ
3
i
1321

28.5
Fourier Cosine Series
If f(x) is an even function, (f(−x) = f(x)), then there will not be any sine terms in the Fourier series for f(x). The
Fourier sine coeﬃcient is
bn = 1
π
Z π
−π
f(x) sin(nx) dx.
Since f(x) is an even function and sin(nx) is odd, f(x) sin(nx) is odd. bn is the integral of an odd function from −π
to π and is thus zero. We can rewrite the cosine coeﬃcients,
an = 1
π
Z π
−π
f(x) cos(nx) dx
= 2
π
Z π
0
f(x) cos(nx) dx.
Example 28.5.1 Consider the function deﬁned on [0, π) by
f(x) =
(
x
for 0 ≤x < π/2
π −x
for π/2 ≤x < π.
The Fourier cosine coeﬃcients for this function are
an = 2
π
Z π/2
0
x cos(nx) dx + 2
π
Z π
π/2
(π −x) cos(nx) dx
=
(
π
4
for n = 0,
8
πn2 cos
  nπ
2

sin2   nπ
4

for n ≥1.
In Figure 28.4 the even periodic extension of f(x) is plotted in a dashed line and the sum of the ﬁrst ﬁve nonzero terms
in the Fourier cosine series are plotted in a solid line.
1322

-3
-2
-1
1
2
3
0.25
0.5
0.75
1
1.25
1.5
Figure 28.4: Fourier Cosine Series.
28.6
Fourier Sine Series
If f(x) is an odd function, (f(−x) = −f(x)), then there will not be any cosine terms in the Fourier series. Since
f(x) cos(nx) is an odd function, the cosine coeﬃcients will be zero. Since f(x) sin(nx) is an even function,we can
rewrite the sine coeﬃcients
bn = 2
π
Z π
0
f(x) sin(nx) dx.
1323

Example 28.6.1 Consider the function deﬁned on [0, π) by
f(x) =
(
x
for 0 ≤x < π/2
π −x
for π/2 ≤x < π.
The Fourier sine coeﬃcients for this function are
bn = 2
π
Z π/2
0
x sin(nx) dx + 2
π
Z π
π/2
(π −x) sin(nx) dx
= 16
πn2 cos
nπ
4

sin3 nπ
4

In Figure 28.5 the odd periodic extension of f(x) is plotted in a dashed line and the sum of the ﬁrst ﬁve nonzero terms
in the Fourier sine series are plotted in a solid line.
28.7
Complex Fourier Series and Parseval’s Theorem
By writing sin(nx) and cos(nx) in terms of eınx and e−ınx we can obtain the complex form for a Fourier series.
a0
2 +
∞
X
n=1
 an cos(nx) + bn sin(nx)

= a0
2 +
∞
X
n=1

an
1
2(eınx + e−ınx) + bn
1
ı2(eınx −e−ınx)

= a0
2 +
∞
X
n=1
1
2(an −ıbn) eınx +1
2(an + ıbn) e−ınx

=
∞
X
n=−∞
cn eınx
where
cn =





1
2(an −ıbn)
for n ≥1
a0
2
for n = 0
1
2(a−n + ıb−n)
for n ≤−1.
1324

-3
-2
-1
1
2
3
-1.5
-1
-0.5
0.5
1
1.5
Figure 28.5: Fourier Sine Series.
The functions {. . . , e−ıx, 1, eıx, eı2x, . . .}, satisfy the relation
Z π
−π
eınx e−ımx dx =
Z π
−π
eı(n−m)x dx
=
(
2π
for n = m
0
for n ̸= m.
Starting with the complex form of the Fourier series of a function f(x),
f(x) ∼
∞
X
−∞
cn eınx,
1325

we multiply by e−ımx and integrate from −π to π to obtain
Z π
−π
f(x) e−ımx dx =
Z π
−π
∞
X
−∞
cn eınx e−ımx dx
cm = 1
2π
Z π
−π
f(x) e−ımx dx
If f(x) is real-valued then
c−m = 1
2π
Z π
−π
f(x) eımx dx = 1
2π
Z π
−π
f(x)(e−ımx) dx = cm
where ¯z denotes the complex conjugate of z.
Assume that f(x) has a uniformly convergent Fourier series.
Z π
−π
f 2(x) dx =
Z π
−π
 
∞
X
m=−∞
cm eımx
!  
∞
X
n=−∞
cn eınx
!
dx
= 2π
∞
X
n=−∞
cnc−n
= 2π
 
−1
X
n=−∞
1
4(a−n + ıb−n)(a−n −ıb−n)

+ a0
2
a0
2 +
∞
X
n=1
1
4(an −ıbn)(an + ıbn)
!
= 2π
 
a2
0
4 + 1
2
∞
X
n=1
(a2
n + b2
n)
!
This yields a result known as Parseval’s theorem which holds even when the Fourier series of f(x) is not uniformly
convergent.
1326

Result 28.7.1 Parseval’s Theorem.
If f(x) has the Fourier series
f(x) ∼a0
2 +
∞
X
n=1
(an cos(nx) + bn sin(nx)),
then
Z π
−π
f 2(x) dx = π
2a2
0 + π
∞
X
n=1
(a2
n + b2
n).
28.8
Behavior of Fourier Coeﬃcients
Before we jump hip-deep into the grunge involved in determining the behavior of the Fourier coeﬃcients, let’s take a
step back and get some perspective on what we should be looking for.
One of the important questions is whether the Fourier series converges uniformly. From Result 12.2.1 we know that
a uniformly convergent series represents a continuous function. Thus we know that the Fourier series of a discontinuous
function cannot be uniformly convergent. From Section 12.2 we know that a series is uniformly convergent if it can be
bounded by a series of positive terms. If the Fourier coeﬃcients, an and bn, are O(1/nα) where α > 1 then the series
can be bounded by (const) P∞
n=1 1/nα and will thus be uniformly convergent.
Let f(x) be a function that meets the conditions for having a Fourier series and in addition is bounded.
Let
(−π, p1), (p1, p2), (p2, p3), . . . , (pm, π) be a partition into a ﬁnite number of intervals of the domain, (−π, π) such that
on each interval f(x) and all it’s derivatives are continuous. Let f(p−) denote the left limit of f(p) and f(p+) denote
the right limit.
f(p−) = lim
ϵ→0+ f(p −ϵ),
f(p+) = lim
ϵ→0+ f(p + ϵ)
1327

Example 28.8.1 The function shown in Figure 28.6 would be partitioned into the intervals
(−2, −1), (−1, 0), (0, 1), (1, 2).
-2
-1
1
2
-1
-0.5
0.5
1
Figure 28.6: A Function that can be Partitioned.
Suppose f(x) has the Fourier series
f(x) ∼a0
2 +
∞
X
n=1
an cos(nx) + bn sin(nx).
1328

We can use the integral formula to ﬁnd the an’s.
an = 1
π
Z π
−π
f(x) cos(nx) dx
= 1
π
Z p1
−π
f(x) cos(nx) dx +
Z p2
p1
f(x) cos(nx) dx + · · · +
Z π
pm
f(x) cos(nx) dx

Using integration by parts,
= 1
nπ
h
f(x) sin(nx)
ip1
−π +
h
f(x) sin(nx)
ip2
p1 + · · · +
h
f(x) sin(nx)
iπ
pm

−1
nπ
Z p1
−π
f ′(x) sin(nx) dx +
Z p2
p1
f ′(x) sin(nx) dx +
Z π
pm
f ′(x) sin(nx) dx

= 1
nπ
n
f(p−
1 ) −f(p+
1 )

sin(np1) + · · · +

f(p−
m) −f(p+
m)

sin(npm)
o
−1
n
1
π
Z π
−π
f ′(x) sin(nx) dx
= 1
nAn −1
nb′
n
where
An = 1
π
m
X
j=1
sin(npj)

f(p−
j ) −f(p+
j )

and the b′
n are the sine coeﬃcients of f ′(x).
Since f(x) is bounded, An = O(1). Since f ′(x) is bounded,
b′
n = 1
π
Z π
−π
f ′(x) sin(nx) dx = O(1).
Thus an = O(1/n) as n →∞. (Actually, from the Riemann-Lebesgue Lemma, b′
n = O(1/n).)
1329

Now we repeat this analysis for the sine coeﬃcients.
bn = 1
π
Z π
−π
f(x) sin(nx) dx
= 1
π
Z p1
−π
f(x) sin(nx) dx +
Z p2
p1
f(x) sin(nx) dx + · · · +
Z π
pm
f(x) sin(nx) dx

= −1
nπ
n
f(x) cos(nx)
p1
−π +

f(x) cos(nx)
p2
p1 + · · · +

f(x) cos(nx)
π
pm
o
+ 1
nπ
Z p1
−π
f ′(x) cos(nx) dx +
Z p2
p1
f ′(x) cos(nx) dx +
Z π
pm
f ′(x) cos(nx) dx

= −1
nBn + 1
na′
n
where
Bn = (−1)n
π

f(−π) −f(π)

−1
π
m
X
j=1
cos(npj)

f(p−
j ) −f(p+
j )

and the a′
n are the cosine coeﬃcients of f ′(x).
Since f(x) and f ′(x) are bounded, Bn, a′
n = O(1) and thus bn = O(1/n) as n →∞.
With integration by parts on the Fourier coeﬃcients of f ′(x) we could ﬁnd that
a′
n = 1
nA′
n −1
nb′′
n
where A′
n = 1
π
Pm
j=1 sin(npj)[f ′(p−
j ) −f ′(p+
j )] and the b′′
n are the sine coeﬃcients of f ′′(x), and
b′
n = −1
nB′
n + 1
na′′
n
where B′
n = (−1)n
π
[f ′(−π) −f ′(π)] −1
π
Pm
j=1 cos(npj)[f ′(p−
j ) −f ′(p+
j )] and the a′′
n are the cosine coeﬃcients of f ′′(x).
1330

Now we can rewrite an and bn as
an = 1
nAn + 1
n2B′
n −1
n2a′′
n
bn = −1
nBn + 1
n2A′
n −1
n2b′′
n.
Continuing this process we could deﬁne A(j)
n
and B(j)
n
so that
an = 1
nAn + 1
n2B′
n −1
n3A′′
n −1
n4B′′′
n + · · ·
bn = −1
nBn + 1
n2A′
n + 1
n3B′′
n −1
n4A′′′
n −· · · .
For any bounded function, the Fourier coeﬃcients satisfy an, bn = O(1/n) as n →∞. If An and Bn are zero
then the Fourier coeﬃcients will be O(1/n2). A suﬃcient condition for this is that the periodic extension of f(x) is
continuous. We see that if the periodic extension of f ′(x) is continuous then A′
n and B′
n will be zero and the Fourier
coeﬃcients will be O(1/n3).
Result 28.8.1 Let f(x) be a bounded function for which there is a partition of the range
(−π, π) into a ﬁnite number of intervals such that f(x) and all it’s derivatives are continuous
on each of the intervals. If f(x) is not continuous then the Fourier coeﬃcients are O(1/n).
If f(x), f ′(x), . . . , f (k−2)(x) are continuous then the Fourier coeﬃcients are O(1/nk).
If the periodic extension of f(x) is continuous, then the Fourier coeﬃcients will be O(1/n2). The series P∞
n=1 |an cos(nx)
can be bounded by M P∞
n=1 1/n2 where M = max
n (|an| + |bn|). Thus the Fourier series converges to f(x) uniformly.
Result 28.8.2 If the periodic extension of f(x) is continuous then the Fourier series of f(x)
will converge uniformly for all x.
If the periodic extension of f(x) is not continuous, we have the following result.
1331

Result 28.8.3 If f(x) is continuous in the interval c < x < d, then the Fourier series is
uniformly convergent in the interval c + δ ≤x ≤d −δ for any δ > 0.
Example 28.8.2 Diﬀerent Rates of Convergence.
A Discontinuous Function.
Consider the function deﬁned by
f1(x) =
(
−1
for −1 < x < 0
1,
for 0 < x < 1.
This function has jump discontinuities, so we know that the Fourier coeﬃcients are O(1/n).
Since this function is odd, there will only be sine terms in it’s Fourier expansion. Furthermore, since the function is
symmetric about x = 1/2, there will be only odd sine terms. Computing these terms,
bn = 2
Z 1
0
sin(nπx) dx
= 2
−1
nπ cos(nπx)
1
0
= 2

−(−1)n
nπ
−−1
nπ

=
(
4
nπ
for odd n
0
for even n.
The function and the sum of the ﬁrst three terms in the expansion are plotted, in dashed and solid lines respectively,
in Figure 28.7.
Although the three term sum follows the general shape of the function, it is clearly not a good
approximation.
1332

-1
-0.5
0.5
1
-1
-0.5
0.5
1
-1
-0.5
0.5
1
-0.4
-0.2
0.2
0.4
Figure 28.7: Three Term Approximation for a Function with Jump Discontinuities and a Continuous Function.
A Continuous Function.
Consider the function deﬁned by
f2(x) =





−x −1
for −1 < x < −1/2
x
for −1/2 < x < 1/2
−x + 1
for 1/2 < x < 1.
1333

-1
-0.5
0.5
1
-0.2
-0.1
0.1
0.2
1
0.25
0.1
0
0.1
1
1
1
0.5
Figure 28.8: Three Term Approximation for a Function with Continuous First Derivative and Comparison of the
Rates of Convergence.
Since this function is continuous, the Fourier coeﬃcients will be O(1/n2). Also we see that there will only be odd sine
terms in the expansion.
bn =
Z −1/2
−1
(−x −1) sin(nπx) dx +
Z 1/2
−1/2
x sin(nπx) dx +
Z 1
1/2
(−x + 1) sin(nπx) dx
= 2
Z 1/2
0
x sin(nπx) dx + 2
Z 1
1/2
(1 −x) sin(nπx) dx
=
4
(nπ)2 sin(nπ/2)
=
(
4
(nπ)2(−1)(n−1)/2
for odd n
0
for even n.
1334

The function and the sum of the ﬁrst three terms in the expansion are plotted, in dashed and solid lines respectively,
in Figure 28.7. We see that the convergence is much better than for the function with jump discontinuities.
A Function with a Continuous First Derivative.
Consider the function deﬁned by
f3(x) =
(
x(1 + x)
for −1 < x < 0
x(1 −x)
for 0 < x < 1.
Since the periodic extension of this function is continuous and has a continuous ﬁrst derivative, the Fourier coeﬃcients
will be O(1/n3). We see that the Fourier expansion will contain only odd sine terms.
bn =
Z 0
−1
x(1 + x) sin(nπx) dx +
Z 1
0
x(1 −x) sin(nπx) dx
= 2
Z 1
0
x(1 −x) sin(nπx) dx
= 4(1 −(−1)n)
(nπ)3
=
(
4
(nπ)3
for odd n
0
for even n.
The function and the sum of the ﬁrst three terms in the expansion are plotted in Figure 28.8. We see that the ﬁrst
three terms give a very good approximation to the function. The plots of the function, (in a dashed line), and the three
term approximation, (in a solid line), are almost indistinguishable.
In Figure 28.8 the convergence of the of the ﬁrst three terms to f1(x), f2(x), and f3(x) are compared. In the last
graph we see a closeup of f3(x) and it’s Fourier expansion to show the error.
1335

28.9
Gibb’s Phenomenon
The Fourier expansion of
f(x) =
(
1
for 0 ≤x < 1
−1
for −1 ≤x < 0
is
f(x) ∼4
π
∞
X
n=1
1
n sin(nπx).
For any ﬁxed x, the series converges to 1
2(f(x−) + f(x+)). For any δ > 0, the convergence is uniform in the intervals
−1 + δ ≤x ≤−δ and δ ≤x ≤1 −δ. How will the nonuniform convergence at integral values of x aﬀect the Fourier
series? Finite Fourier series are plotted in Figure 28.9 for 5, 10, 50 and 100 terms. (The plot for 100 terms is closeup
of the behavior near x = 0.) Note that at each discontinuous point there is a series of overshoots and undershoots
that are pushed closer to the discontinuity by increasing the number of terms, but do not seem to decrease in height.
In fact, as the number of terms goes to inﬁnity, the height of the overshoots and undershoots does not vanish. This is
known as Gibb’s phenomenon.
28.10
Integrating and Diﬀerentiating Fourier Series
Integrating Fourier Series.
Since integration is a smoothing operation, any convergent Fourier series can be
integrated term by term to yield another convergent Fourier series.
Example 28.10.1 Consider the step function
f(x) =
(
π
for 0 ≤x < π
−π
for −π ≤x < 0.
1336

1
1
0.1
0.8
1.2
1
1
1
1
Figure 28.9:
Since this is an odd function, there are no cosine terms in the Fourier series.
bn = 2
π
Z π
0
π sin(nx) dx
= 2

−1
n cos(nx)
π
0
= 2
n(1 −(−1)n)
=
(
4
n
for odd n
0
for even n.
1337

f(x) ∼
∞
X
n=1
oddn
4
n sin nx
Integrating this relation,
Z x
−π
f(t) dt ∼
Z x
−π
∞
X
n=1
oddn
4
n sin(nt) dt
F(x) ∼
∞
X
n=1
oddn
4
n
Z x
−π
sin(nt) dt
=
∞
X
n=1
oddn
4
n

−1
n cos(nt)
x
−π
=
∞
X
n=1
oddn
4
n2(−cos(nx) + (−1)n)
= 4
∞
X
n=1
oddn
−1
n2 −4
∞
X
n=1
oddn
cos(nx)
n2
Since this series converges uniformly,
4
∞
X
n=1
oddn
−1
n2 −4
∞
X
n=1
oddn
cos(nx)
n2
= F(x) =
(
−x −π
for −π ≤x < 0
x −π
for 0 ≤x < π.
The value of the constant term is
4
∞
X
n=1
oddn
−1
n2 = 2
π
Z π
0
F(x) dx = −1
π.
1338

Thus
−1
π −4
∞
X
n=1
oddn
cos(nx)
n2
=
(
−x −π
for −π ≤x < 0
x −π
for 0 ≤x < π.
Diﬀerentiating Fourier Series.
Recall that in general, a series can only be diﬀerentiated if it is uniformly conver-
gent. The necessary and suﬃcient condition that a Fourier series be uniformly convergent is that the periodic extension
of the function is continuous.
Result 28.10.1 The Fourier series of a function f(x) can be diﬀerentiated only if the periodic
extension of f(x) is continuous.
Example 28.10.2 Consider the function deﬁned by
f(x) =
(
π
for 0 ≤x < π
−π
for −π ≤x < 0.
f(x) has the Fourier series
f(x) ∼
∞
X
n=1
oddn
4
n sin nx.
The function has a derivative except at the points x = nπ. Diﬀerentiating the Fourier series yields
f ′(x) ∼4
∞
X
n=1
oddn
cos(nx).
For x ̸= nπ, this implies
0 = 4
∞
X
n=1
oddn
cos(nx),
1339

which is false. The series does not converge. This is as we expected since the Fourier series for f(x) is not uniformly
convergent.
1340

28.11
Exercises
Exercise 28.1
1. Consider a 2π periodic function f(x) expressed as a Fourier series with partial sums
SN(x) = a0
2 +
N
X
n=1
an cos(nx) + bn sin(nt).
Assuming that the Fourier series converges in the mean, i.e.
lim
N→∞
Z π
−π
(f(x) −SN(x))2 dx = 0,
show
a2
0
2 +
∞
X
n=1
a2
n + b2
n = 1
π
Z π
−π
f(x)2 dx.
This is called Parseval’s equation.
2. Find the Fourier series for f(x) = x on −π ≤x < π (and repeating periodically). Use this to show
∞
X
n=1
1
n2 = π2
6 .
3. Similarly, by choosing appropriate functions f(x), use Parseval’s equation to determine
∞
X
n=1
1
n4
and
∞
X
n=1
1
n6.
Exercise 28.2
Consider the Fourier series of f(x) = x on −π ≤x < π as found above. Investigate the convergence at the points of
discontinuity.
1341

1. Let SN be the sum of the ﬁrst N terms in the Fourier series. Show that
dSN
dx = 1 −(−1)N cos
  N + 1
2

x

cos
  x
2

.
2. Now use this to show that
x −SN =
Z x
0
sin
  N + 1
2

(ξ −π)

sin
  ξ−π
2

dξ.
3. Finally investigate the maxima of this diﬀerence around x = π and provide an estimate (good to two decimal
places) of the overshoot in the limit N →∞.
Exercise 28.3
Consider the boundary value problem on the interval 0 < x < 1
y′′ + 2y = 1
y(0) = y(1) = 0.
1. Choose an appropriate periodic extension and ﬁnd a Fourier series solution.
2. Solve directly and ﬁnd the Fourier series of the solution (using the same extension). Compare the result to the
previous step and verify the series agree.
Exercise 28.4
Consider the boundary value problem on 0 < x < π
y′′ + 2y = sin x
y′(0) = y′(π) = 0.
1. Find a Fourier series solution.
2. Suppose the ODE is slightly modiﬁed: y′′ + 4y = sin x with the same boundary conditions. Attempt to ﬁnd a
Fourier series solution and discuss in as much detail as possible what goes wrong.
1342

Exercise 28.5
Find the Fourier cosine and sine series for f(x) = x2 on 0 ≤x < π. Are the series diﬀerentiable?
Exercise 28.6
Find the Fourier series of cosn(x).
Exercise 28.7
For what values of x does the Fourier series
π2
3 + 4
∞
X
n=1
(−1)n
n2
cos nx = x2
converge? What is the value of the above Fourier series for all x? From this relation show that
∞
X
n=1
1
n2 = π2
6
∞
X
n=1
(−1)n+1
n2
= π2
12
Exercise 28.8
1. Compute the Fourier sine series for the function
f(x) = cos x −1 + 2x
π ,
0 ≤x ≤π.
2. How fast do the Fourier coeﬃcients an where
f(x) =
∞
X
n=1
an sin nx
decrease with increasing n? Explain this rate of decrease.
1343

Exercise 28.9
Determine the cosine and sine series of
f(x) = x sin x,
(0 < x < π).
Estimate before doing the calculation the rate of decrease of Fourier coeﬃcients, an, bn, for large n.
Exercise 28.10
Determine the Fourier cosine series of the function
f(x) = cos(νx),
0 ≤x ≤π,
where ν is an arbitrary real number. From this series deduce the following identities for non-integer ν.
π
sin(πν) = 1
ν +
∞
X
n=1
(−1)n

1
ν −n +
1
ν + n

π cot(πν) = 1
ν +
∞
X
n=1

1
ν −n +
1
ν + n

Integrate the last formula from ν = 0 to ν = θ, (0 < θ < 1), to show that
sin(πθ)
πθ
=
∞
Y
n=1

1 −θ2
n2

.
Exercise 28.11
1. Show that
ln

cos
x
2

= −ln 2 −
∞
X
n=1
(−1)n
n
cos(nx),
−π < x < π.
Use properties of Fourier series to conclude that
ln
cos x
2
 = −ln 2 −
∞
X
n=1
(−1)n
n
cos(nx),
x ̸= (2k + 1)π, k ∈Z.
1344

Hint: use the identity
Log(1 −z) = −
∞
X
n=1
zn
n
for |z| ≤1, z ̸= 1.
2. From this series deduce that
Z π
0
ln

cos x
2

dx = −π ln 2.
3. Show that
1
2 ln

sin((x + ξ)/2)
sin((x −ξ)/2)
 =
∞
X
n=1
sin(nx) sin(nξ)
n
,
x ̸= ±ξ + 2kπ.
Exercise 28.12
Solve the problem
y′′ + αy = f(x),
y(a) = y(b) = 0,
with an eigenfunction expansion. Assume that α ̸= nπ/(b −a), n ∈N.
Exercise 28.13
Solve the problem
y′′ + αy = f(x),
y(a) = A,
y(b) = B,
with an eigenfunction expansion. Assume that α ̸= nπ/(b −a), n ∈N.
Exercise 28.14
Find the trigonometric series and the simple closed form expressions for A(r, x) and B(r, x) where z = r eıx and |r| < 1.
a)
A + ıB ≡
1
1 −z2 = 1 + z2 + z4 + · · ·
b)
A + ıB ≡log(1 + z) = z −1
2z2 + 1
3z3 −· · ·
Find An and Bn, and the trigonometric sum for them where:
c)
An + ıBn = 1 + z + z2 + · · · + zn.
1345

Exercise 28.15
1. Is the trigonometric system
{1, sin x, cos x, sin 2x, cos 2x, . . .}
orthogonal on the interval [0, π]? Is the system orthogonal on any interval of length π? Why, in each case?
2. Show that each of the systems
{1, cos x, cos 2x, . . .},
and
{sin x, sin 2x, . . .}
are orthogonal on [0, π]. Make them orthonormal too.
Exercise 28.16
Let SN(x) be the N th partial sum of the Fourier series for f(x) ≡|x| on −π < x < π. Find N such that |f(x) −
SN(x)| < 10−1 on |x| < π.
Exercise 28.17
The set {sin(nx)}∞
n=1 is orthogonal and complete on [0, π].
1. Find the Fourier sine series for f(x) ≡1 on 0 ≤x ≤π.
2. Find a convergent series for g(x) = x on 0 ≤x ≤π by integrating the series for part (a).
3. Apply Parseval’s relation to the series in (a) to ﬁnd:
∞
X
n=1
1
(2n −1)2
Check this result by evaluating the series in (b) at x = π.
Exercise 28.18
1. Show that the Fourier cosine series expansion on [0, π] of:
f(x) ≡





1,
0 ≤x < π
2,
1
2,
x = π
2,
0,
π
2 < x ≤π,
1346

is
S(x) = 1
2 + 2
π
∞
X
n=0
(−1)n
2n + 1 cos((2n + 1)x).
2. Show that the N th partial sum of the series in (a) is
SN(x) = 1
2 −1
π
Z x−π/2
0
sin((2(N + 1)t)
sin t
dt.
( Hint: Consider the diﬀerence of P2N+1
n=1 (eıy)n and PN
n=1(eı2y)n, where y = x −π/2.)
3. Show that dSN(x)/dx = 0 at x = xn =
nπ
2(N+1) for n = 0, 1, . . . , N, N + 2, . . . , 2N + 2.
4. Show that at x = xN, the maximum of SN(x) nearest to π/2 in (0, π/2) is
SN(xN) = 1
2 + 1
π
Z
πN
2(N+1)
0
sin(2(N + 1)t)
sin t
dt.
Clearly xN ↑π/2 as N →∞.
5. Show that also in this limit,
SN(xN) →1
2 + 1
π
Z π
0
sin t
t
dt ≈1.0895.
How does this compare with f(π/2 −0)? This overshoot is the Gibbs phenomenon that occurs at each disconti-
nuity. It is a manifestation of the non-uniform convergence of the Fourier series for f(x) on [0, π].
Exercise 28.19
Prove the Isoperimetric Inequality: L2 ≥4πA where L is the length of the perimeter and A the area of any piecewise
smooth plane ﬁgure.
Show that equality is attained only for the circle.
(Hints: The closed curve is represented
parametrically as
x = x(s),
y = y(s),
0 ≤s ≤L
1347

where s is the arclength. In terms of t = 2πs/L we have
dx
dt
2
+
dy
dt
2
=
 L
2π
2
.
Integrate this relation over [0, 2π]. The area is given by
A =
Z 2π
0
xdy
dt dt.
Express x(t) and y(t) as Fourier series and use the completeness and orthogonality relations to show that L2−4πA ≥0.)
Exercise 28.20
1. Find the Fourier sine series expansion and the Fourier cosine series expansion of
g(x) = x(1 −x), on 0 ≤x ≤1.
Which is better and why over the indicated interval?
2. Use these expansions to show that:
i)
∞
X
k=1
1
k2 = π2
6 ,
ii)
∞
X
k=1
(−1)k
k2
= −π2
12,
iii)
∞
X
k=1
(−1)k
(2k −1)2 = −π3
32.
Note: Some useful integration by parts formulas are:
Z
x sin(nx) = 1
n2 sin(nx) −x
n cos(nx);
Z
x cos(nx) = 1
n2 cos(nx) + x
n sin(nx)
Z
x2 sin(nx) = 2x
n2 sin(nx) −n2x2 −2
n3
cos(nx)
Z
x2 cos(nx) = 2x
n2 cos(nx) + n2x2 −2
n3
sin(nx)
1348

28.12
Hints
Hint 28.1
Hint 28.2
Hint 28.3
Hint 28.4
Hint 28.5
Hint 28.6
Expand
cosn(x) =
1
2(eıx + e−ıx)
n
Using Newton’s binomial formula.
Hint 28.7
Hint 28.8
Hint 28.9
1349

Hint 28.10
Hint 28.11
Hint 28.12
Hint 28.13
Hint 28.14
Hint 28.15
Hint 28.16
Hint 28.17
Hint 28.18
Hint 28.19
Hint 28.20
1350

28.13
Solutions
Solution 28.1
1. We start by assuming that the Fourier series converges in the mean.
Z π
−π
 
f(x) −a0
2 −
∞
X
n=1
(an cos(nx) + bn sin(nx))
!2
= 0
We interchange the order of integration and summation.
Z π
−π
(f(x))2 dx −a0
Z π
−π
f(x) dx −2
∞
X
n=1

an
Z π
−π
f(x) cos(nx) dx + bn
Z π
−π
f(x) sin(nx)

+ πa2
0
2
+ a0
∞
X
n=1
Z π
−π
(an cos(nx) + bn sin(nx)) dx
+
∞
X
n=1
∞
X
m=1
Z π
−π
(an cos(nx) + bn sin(nx))(am cos(mx) + bm sin(mx)) dx = 0
Most of the terms vanish because the eigenfunctions are orthogonal.
Z π
−π
(f(x))2 dx −a0
Z π
−π
f(x) dx −2
∞
X
n=1

an
Z π
−π
f(x) cos(nx) dx + bn
Z π
−π
f(x) sin(nx)

+ πa2
0
2
+
∞
X
n=1
Z π
−π
(a2
n cos2(nx) + b2
n sin2(nx)) dx = 0
1351

We use the deﬁnition of the Fourier coeﬃcients to evaluate the integrals in the last sum.
Z π
−π
(f(x))2 dx −πa2
0 −2π
∞
X
n=1
 a2
n + b2
n

+ πa2
0
2
+ π
∞
X
n=1
 a2
n + b2
n

= 0
a2
0
2 +
∞
X
n=1
 a2
n + b2
n

= 1
π
Z π
−π
f(x)2 dx
2. We determine the Fourier coeﬃcients for f(x) = x. Since f(x) is odd, all of the an are zero.
b0 = 1
π
Z π
−π
x sin(nx) dx
= 1
π

−1
nx cos(nx)
π
−π
+
Z π
−π
1
n cos(nx) dx
= 2(−1)n+1
n
The Fourier series is
x =
∞
X
n=1
2(−1)n+1
n
sin(nx)
for x ∈(−π . . . π).
We apply Parseval’s theorem for this series to ﬁnd the value of P∞
n=1
1
n2.
∞
X
n=1
4
n2 = 1
π
Z π
−π
x2 dx
∞
X
n=1
4
n2 = 2π2
3
∞
X
n=1
1
n2 = π2
6
1352

3. Consider f(x) = x2. Since the function is even, there are no sine terms in the Fourier series. The coeﬃcients in
the cosine series are
a0 = 2
π
Z π
0
x2 dx
= 2π2
3
an = 2
π
Z π
0
x2 cos(nx) dx
= 4(−1)n
n2
.
Thus the Fourier series is
x2 = π2
3 + 4
∞
X
n=1
(−1)n
n2
cos(nx)
for x ∈(−π . . . π).
We apply Parseval’s theorem for this series to ﬁnd the value of P∞
n=1
1
n4.
2π4
9
+ 16
∞
X
n=1
1
n4 = 1
π
Z π
−π
x4 dx
2π4
9
+ 16
∞
X
n=1
1
n4 = 2π4
5
∞
X
n=1
1
n4 = π4
90
1353

Now we integrate the series for f(x) = x2.
Z x
0

ξ2 −π2
3

dξ = 4
∞
X
n=1
(−1)n
n2
Z x
0
cos(nξ) dξ
x3
3 −π2
3 x = 4
∞
X
n=1
(−1)n
n3
sin(nx)
We apply Parseval’s theorem for this series to ﬁnd the value of P∞
n=1
1
n6.
16
∞
X
n=1
1
n6 = 1
π
Z π
−π
x3
3 −π2
3 x
2
dx
16
∞
X
n=1
1
n6 = 16π6
945
∞
X
n=1
1
n6 = π6
945
1354

Solution 28.2
1. We diﬀerentiate the partial sum of the Fourier series and evaluate the sum.
SN =
N
X
n=1
2(−1)n+1
n
sin(nx)
S′
N = 2
N
X
n=1
(−1)n+1 cos(nx)
S′
N = 2ℜ
 N
X
n=1
(−1)n+1 eınx
!
S′
N = 2ℜ
1 −(−1)N+2 eı(N+1)x
1 + eıx

S′
N = ℜ
1 + e−ıx −(−1)N eı(N+1)x −(−1)N eıNx
1 + cos(x)

S′
N = 1 −(−1)N cos((N + 1)x) + cos(Nx)
1 + cos(x)
S′
N = 1 −(−1)N cos
  N + 1
2

x

cos
  x
2

cos2   x
2

dSN
dx = 1 −(−1)N cos
  N + 1
2

x

cos
  x
2

1355

2. We integrate S′
N.
SN(x) −SN(0) = x −
Z x
0
(−1)N cos
  N + 1
2

ξ

cos
  ξ
2

dξ
x −SN =
Z x
0
sin
  N + 1
2

(ξ −π)

sin
  ξ−π
2

dξ
3. We ﬁnd the extrema of the overshoot E = x −SN with the ﬁrst derivative test.
E′ = sin
  N + 1
2

(x −π)

sin
  x−π
2

= 0
We look for extrema in the range (−π . . . π).

N + 1
2

(x −π) = −nπ
x = π

1 −
n
N + 1/2

,
n ∈[1 . . . 2N]
The closest of these extrema to x = π is
x = π

1 −
1
N + 1/2

Let E0 be the overshoot at this point. We approximate E0 for large N.
E0 =
Z π(1−1/(N+1/2))
0
sin
  N + 1
2

(ξ −π)

sin
  ξ−π
2

dξ
We shift the limits of integration.
E0 =
Z π
π/(N+1/2)
sin
  N + 1
2

ξ

sin
  ξ
2

dξ
1356

We add and subtract an integral over [0 . . . π/(N + 1/2)].
E0 =
Z π
0
sin
  N + 1
2

ξ

sin
  ξ
2

dξ −
Z π/(N+1/2)
0
sin
  N + 1
2

ξ

sin
  ξ
2

dξ
We can evaluate the ﬁrst integral with contour integration on the unit circle C.
Z π
0
sin
  N + 1
2

ξ

sin
  ξ
2

dξ =
Z π
0
sin ((2N + 1) ξ)
sin (ξ)
dξ
= 1
2
Z π
−π
sin ((2N + 1) ξ)
sin (ξ)
dξ
= 1
2 −
Z
C
ℑ
 z2N+1
(z −1/z)/(ı2)
dz
ız
= ℑ

−
Z
C
z2N+1
(z2 −1) dz

= ℑ

ıπ Res

z2N+1
(z + 1)(z −1), 1

+ ıπ Res

z2N+1
(z + 1)(z −1), −1

= πℜ
12N+1
2
+ (−1)2N+1
−2

= π
1357

We approximate the second integral.
Z π/(N+1/2)
0
sin
  N + 1
2

ξ

sin
  ξ
2

dξ =
2
2N + 1
Z π
0
sin(x)
sin
 x
2N+1
 dx
≈2
Z π
0
sin(x)
x
dx
= 2
Z π
0
1
x
∞
X
n=0
(−1)nx2n+1
(2n + 1)! dx
= 2
∞
X
n=0
Z π
0
(−1)nx2n
(2n + 1)! dx
= 2
∞
X
n=0
(−1)nπ2n+1
(2n + 1)(2n + 1)! dx
≈3.70387
In the limit as N →∞, the overshoot is
|π −3.70387| ≈0.56.
Solution 28.3
1. The eigenfunctions of the self-adjoint problem
−y′′ = λy,
y(0) = y(1) = 0,
are
φn = sin(nπx),
n ∈Z+
1358

We ﬁnd the series expansion of the inhomogeneity f(x) = 1.
1 =
∞
X
n=1
fn sin(nπx)
fn = 2
Z 1
0
sin(nπx) dx
fn = 2

−cos(nπx)
nπ
1
0
fn = 2
nπ(1 −(−1)n)
fn =
(
4
nπ
for odd n
0
for even n
We expand the solution in a series of the eigenfunctions.
y =
∞
X
n=1
an sin(nπx)
We substitute the series into the diﬀerential equation.
y′′ + 2y = 1
−
∞
X
n=1
anπ2n2 sin(nπx) + 2
∞
X
n=1
an sin(nπx) =
∞
X
n=1
odd n
4
nπ sin(nπx)
an =
(
4
nπ(2−π2n2)
for odd n
0
for even n
y =
∞
X
n=1
odd n
4
nπ(2 −π2n2) sin(nπx)
1359

2. Now we solve the boundary value problem directly.
y′′ + 2y = 1
y(0) = y(1) = 0
The general solution of the diﬀerential equation is
y = c1 cos
√
2x

+ c2 sin
√
2x

+ 1
2.
We apply the boundary conditions to ﬁnd the solution.
c1 + 1
2 = 0,
c1 cos
√
2

+ c2 sin
√
2

+ 1
2 = 0
c1 = −1
2,
c2 = cos
 √
2

−1
2 sin
 √
2

y = 1
2
 
1 −cos
√
2x

+ cos
 √
2

−1
sin
 √
2

sin
√
2x
!
We ﬁnd the Fourier sine series of the solution.
y =
∞
X
n=1
an sin(nπx)
an = 2
Z 1
0
y(x) sin(nπx) dx
an =
Z 1
0
 
1 −cos
√
2x

+ cos
 √
2

−1
sin
 √
2

sin
√
2x
!
sin(nπx) dx
an = 2(1 −(−1)2
nπ(2 −π2n2)
an =
(
4
nπ(2−π2n2)
for odd n
0
for even n
1360

We obtain the same series as in the ﬁrst part.
Solution 28.4
1. The eigenfunctions of the self-adjoint problem
−y′′ = λy,
y′(0) = y′(π) = 0,
are
φ0 = 1
2,
φn = cos(nx),
n ∈Z+
We ﬁnd the series expansion of the inhomogeneity f(x) = sin(x).
f(x) = f0
2 +
∞
X
n=1
fn cos(nx)
f0 = 2
π
Z π
0
sin(x) dx
f0 = 4
π
fn = 2
π
Z π
0
sin(x) cos(nx) dx
fn = 2(1 + (−1)n)
π(1 −n2)
fn =
(
4
π(1−n2)
for even n
0
for odd n
We expand the solution in a series of the eigenfunctions.
y = a0
2 +
∞
X
n=1
an cos(nx)
1361

We substitute the series into the diﬀerential equation.
y′′ + 2y = sin(x)
−
∞
X
n=1
ann2 cos(nx) + a0 + 2
∞
X
n=1
an cos(nx) = 2
π +
∞
X
n=2
even n
4
π(1 −n2) cos(nx)
y = 1
π +
∞
X
n=2
even n
4
π(1 −n2)(2 −n2) cos(nx)
2. We expand the solution in a series of the eigenfunctions.
y = a0
2 +
∞
X
n=1
an cos(nx)
We substitute the series into the diﬀerential equation.
y′′ + 4y = sin(x)
−
∞
X
n=1
ann2 cos(nx) + 2a0 + 4
∞
X
n=1
an cos(nx) = 2
π +
∞
X
n=2
even n
4
π(1 −n2) cos(nx)
It is not possible to solve for the a2 coeﬃcient. That equation is
(0)a2 = −4
3π.
This problem is to be expected, as this boundary value problem does not have a solution. The solution of the
diﬀerential equation is
y = c1 cos(2x) + c2 sin(2x) + 1
3 sin(x)
1362

The boundary conditions give us an inconsistent set of constraints.
y′(0) = 0,
y′(π) = 0
c2 + 1
3 = 0,
c2 −1
3 = 0
Thus the problem has no solution.
Solution 28.5
Cosine Series. The coeﬃcients in the cosine series are
a0 = 2
π
Z π
0
x2 dx
= 2π2
3
an = 2
π
Z π
0
x2 cos(nx) dx
= 4(−1)n
n2
.
Thus the Fourier cosine series is
f(x) = π2
3 +
∞
X
n=1
4(−1)n
n2
cos(nx).
In Figure 28.10 the even periodic extension of f(x) is plotted in a dashed line and the sum of the ﬁrst ﬁve terms in the
Fourier series is plotted in a solid line. Since the even periodic extension is continuous, the cosine series is diﬀerentiable.
1363

-3
-2
-1
1
2
3
2
4
6
8
10
-3
-2
-1
1
2
3
-10
-5
5
10
Figure 28.10: The Fourier Cosine and Sine Series of f(x) = x2.
Sine Series. The coeﬃcients in the sine series are
bn = 2
π
Z π
0
x2 sin(nx) dx
= −2(−1)nπ
n
−4(1 −(−1)n)
πn3
=
(
−2(−1)nπ
n
for even n
−2(−1)nπ
n
−
8
πn3
for odd n.
1364

Thus the Fourier sine series is
f(x) ∼−
∞
X
n=1
2(−1)nπ
n
+ 4(1 −(−1)n)
πn3

sin(nx).
In Figure 28.10 the odd periodic extension of f(x) and the sum of the ﬁrst ﬁve terms in the sine series are plotted.
Since the odd periodic extension of f(x) is not continuous, the series is not diﬀerentiable.
Solution 28.6
We could ﬁnd the expansion by integrating to ﬁnd the Fourier coeﬃcients, but it is easier to expand cosn(x) directly.
cosn(x) =
1
2(eıx + e−ıx)
n
= 1
2n
n
0

eınx +
n
1

eı(n−2)x + · · · +

n
n −1

e−ı(n−2)x +
n
n

e−ınx

If n is odd,
cosn(x) = 1
2n
"n
0

(eınx + e−ınx) +
n
1

(eı(n−2)x + e−ı(n−2)x) + · · ·
+

n
(n −1)/2

(eıx + e−ıx)
#
= 1
2n
n
0

2 cos(nx) +
n
1

2 cos((n −2)x) + · · · +

n
(n −1)/2

2 cos(x)

=
1
2n−1
(n−1)/2
X
m=0
n
m

cos((n −2m)x)
=
1
2n−1
n
X
k=1
odd k

n
(n −k)/2

cos(kx).
1365

If n is even,
cosn(x) = 1
2n
"n
0

(eınx + e−ınx) +
n
1

(eı(n−2)x + e−ı(n−2)x) + · · ·
+

n
n/2 −1

(eı2x + e−i2x) +
 n
n/2
#
= 1
2n
n
0

2 cos(nx) +
n
1

2 cos((n −2)x) + · · · +

n
n/2 −1

2 cos(2x) +
 n
n/2

= 1
2n
 n
n/2

+
1
2n−1
(n−2)/2
X
m=0
n
m

cos((n −2m)x)
= 1
2n
 n
n/2

+
1
2n−1
n
X
k=2
even k

n
(n −k)/2

cos(kx).
We may denote,
cosn(x) = a0
2
n
X
k=1
ak cos(kx),
where
ak = 1 + (−1)n−k
2
1
2n−1

n
(n −k)/2

.
1366

Solution 28.7
We expand f(x) in a cosine series. The coeﬃcients in the cosine series are
a0 = 2
π
Z π
0
x2 dx
= 2π2
3
an = 2
π
Z π
0
x2 cos(nx) dx
= 4(−1)n
n2
.
Thus the Fourier cosine series is
f(x) = π2
3 + 4
∞
X
n=1
(−1)n
n2
cos(nx).
The Fourier series converges to the even periodic extension of
f(x) = x2
for 0 < x < π,
which is
ˆf(x) =

x −2π
x + π
2π
2
.
(⌊·⌋denotes the ﬂoor or greatest integer function.) This periodic extension is a continuous function. Since x2 is an
even function, we have
π2
3 + 4
∞
X
n=1
(−1)n
n2
cos nx = x2
for −π ≤x ≤π.
1367

We substitute x = π into the Fourier series.
π2
3 + 4
∞
X
n=1
(−1)n
n2
cos(nπ) = π2
∞
X
n=1
1
n2 = π2
6
We substitute x = 0 into the Fourier series.
π2
3 + 4
∞
X
n=1
(−1)n
n2
= 0
∞
X
n=1
(−1)n+1
n2
= π2
12
Solution 28.8
1. We compute the Fourier sine coeﬃcients.
an = 2
π
Z π
0
f(x) sin(nx) dx
= 2
π
Z π
0

cos x −1 + 2x
π

sin(nx) dx
= 2(1 + (−1)n)
π(n3 −n)
an =
(
4
π(n3−n)
for even n
0
for odd n
1368

2. From our work in the previous part, we see that the Fourier coeﬃcients decay as 1/n3. The Fourier sine series
converges to the odd periodic extension of the function, ˆf(x). We can determine the rate of decay of the Fourier
coeﬃcients from the smoothness of ˆf(x). For −π < x < π, the odd periodic extension of f(x) is deﬁned
ˆf(x) =
(
f(x) = cos(x) −1 + 2x
π
0 ≤x < π,
−f(−x) = −cos(x) + 1 + 2x
π
−π ≤x < 0.
Since
ˆf(0+) = ˆf(0−) = 0
and
ˆf(π) = ˆf(−π) = 0
ˆf(x) is continuous, C0. Since
ˆf ′(0+) = ˆf ′(0−) = 2
π
and
ˆf ′(π) = ˆf ′(−π) = 2
π
ˆf(x) is continuously diﬀerentiable, C1. However, since
ˆf ′′(0+) = −1,
and
ˆf ′′(0−) = 1
ˆf(x) is not C2. Since ˆf(x) is C1 we know that the Fourier coeﬃcients decay as 1/n3.
Solution 28.9
Cosine Series. The even periodic extension of f(x) is a C0, continuous, function (See Figure 28.11. Thus the
coeﬃcients in the cosine series will decay as 1/n2. The Fourier cosine coeﬃcients are
a0 = 2
π
Z π
0
x sin x dx
= 2
a1 = 2
π
Z π
0
x sin x cos x dx
= −1
2
1369

an = 2
π
Z π
0
x sin x cos(nx) dx
= 2(−1)n+1
n2 −1 ,
for n ≥2
The Fourier cosine series is
ˆf(x) = 1 −1
2 cos x −2
∞
X
n=2
2(−1)n
n2 −1 cos(nx).
-5
5
1
Figure 28.11: The even periodic extension of x sin x.
Sine Series. The odd periodic extension of f(x) is a C1, continuously diﬀerentiable, function (See Figure 28.12.
Thus the coeﬃcients in the cosine series will decay as 1/n3. The Fourier sine coeﬃcients are
a1 = 1
π
Z π
0
x sin x sin x dx
= π
2
1370

an = 2
π
Z π
0
x sin x sin(nx) dx
= −4(1 + (−1)n)n
π(n2 −1)2
,
for n ≥2
The Fourier sine series is
ˆf(x) = π
2 sin x −4
π
∞
X
n=2
(1 + (−1)n)n
(n2 −1)2
cos(nx).
-5
5
1
Figure 28.12: The odd periodic extension of x sin x.
Solution 28.10
If ν = n is an integer, then the Fourier cosine series is the single term cos(|n|x). We assume that ν ̸= n.
We note that the even periodic extension of cos(νx) is C0 so that the series converges to cos(νx) for −π ≤x ≤π
1371

and the coeﬃcients decay as 1/n2. We compute the Fourier cosine coeﬃcients.
a0 = 2
π
Z π
0
cos(νx) dx
= 2 sin(πν)
πν
an = 2
π
Z π
0
cos(νx) cos(nx) dx
= (−1)n

1
ν −n +
1
ν + n

sin(πν)
The Fourier cosine series is
cos(νx) = sin(πν)
πν
+
∞
X
n=1
(−1)n

1
ν −n +
1
ν + n

sin(πν) cos(nx).
We substitute x = 0 into the Fourier cosine series.
1 = sin(πν)
πν
+
∞
X
n=1
(−1)n

1
ν −n +
1
ν + n

sin(πν)
π
sin πν = 1
ν +
∞
X
n=1
(−1)n

1
ν −n +
1
ν + n

Next we substitute x = π into the Fourier cosine series.
cos(νπ) = sin(πν)
πν
+
∞
X
n=1
(−1)n

1
ν −n +
1
ν + n

sin(πν)(−1)n
π cot πν = 1
ν +
∞
X
n=1

1
ν −n +
1
ν + n

1372

Note that neither cot(πν) nor 1/ν is integrable at ν = 0. We write the last formula so each side is integrable.
π cot πν −1
ν =
∞
X
n=1

1
ν −n +
1
ν + n

We integrate from ν = 0 to ν = θ < 1.

ln
sin(πν)
ν
θ
0
=
∞
X
n=1

[ln(n −ν)]θ
0 + [ln(n + ν)]θ
0

ln
sin(πθ)
θ

−ln π =
∞
X
n=1

ln
n −θ
n

+ ln
n + θ
n

ln
sin(πθ)
πθ

=
∞
X
n=1
ln

1 −θ2
n2

ln
sin(πθ)
πθ

= ln
 ∞
Y
n=1

1 −θ2
n2
!
sin(πθ)
πθ
=
∞
Y
n=1

1 −θ2
n2

Solution 28.11
1. We will consider the principal branch of the logarithm, −π < ℑ(Log z) ≤π. For −π < x < π, cos(x/2) is
positive so that ln(cos(x/2)) is well-deﬁned. At x = ±π, ln(cos(x/2)) is singular. However, the function is
integrable so it has a Fourier series which converges except at x = (2k + 1)π, k ∈Z.
ln

cos x
2

= ln
eıx/2 + e−ıx/2
2

= −ln 2 + ln
 e−ıx/2 (1 + eıx)

= −ln 2 −ıx
2 + Log (1 + eıx)
1373

Since | eıx | ≤1 and eıx ̸= −1 for ℑ(x) ≥0, x ̸= (2k + 1)π, we can expand the last term in a Taylor series in
that domain.
= −ln 2 −ıx
2 −
∞
X
n=1
(−1)n
n
(eıx)n
= −ln 2 −
∞
X
n=1
(−1)n
n
cos(nx) −ı
 
x
2 +
∞
X
n=1
(−1)n
n
sin(nx)
!
For −π < x < π, ln(cos(x/2)) is real-valued. We equate the real parts of the equation on this domain to obtain
the desired Fourier series.
ln

cos
x
2

= −ln 2 −
∞
X
n=1
(−1)n
n
cos(nx),
−π < x < π.
The domain of convergence for this series is ℑ(x) = 0, x ̸= (2k + 1)π. The Fourier series converges to the
periodic extension of the function.
ln
cos x
2
 = −ln 2 −
∞
X
n=1
(−1)n
n
cos(nx),
x ̸= (2k + 1)π, k ∈Z
2. Now we integrate the function from 0 to π.
Z π
0
ln

cos x
2

dx =
Z π
0
 
−ln 2 −
∞
X
n=1
(−1)n
n
cos(nx)
!
dx
= −π ln 2 −
∞
X
n=1
(−1)n
n
Z π
0
cos(nx) dx
= −π ln 2 −
∞
X
n=1
(−1)n
n
sin(nx)
n
π
0
1374

Z π
0
ln

cos
x
2

dx = −π ln 2
3. We expand the logorithm.
1
2 ln

sin((x + ξ)/2)
sin((x −ξ)/2)
 = 1
2 ln |sin((x + ξ)/2)| −1
2 ln |sin((x −ξ)/2)|
Consider the function ln | sin(y/2)|. Since sin(x) = cos(x −π/2), we can use the result of part (a) to obtain,
ln
sin
y
2
 = ln
cos
y −π
2

= −ln 2 −
∞
X
n=1
(−1)n
n
cos(n(y −π))
= −ln 2 −
∞
X
n=1
1
n cos(ny),
for y ̸= 2πk, k ∈Z.
We return to the original function:
1
2 ln

sin((x + ξ)/2)
sin((x −ξ)/2)
 = 1
2
 
−ln 2 −
∞
X
n=1
1
n cos(n(x + ξ)) + ln 2 +
∞
X
n=1
1
n cos(n(x −ξ))
!
,
for x ± ξ ̸= 2πk, k ∈Z.
1
2 ln

sin((x + ξ)/2)
sin((x −ξ)/2)
 =
∞
X
n=1
sin(nx) sin(nξ)
n
,
x ̸= ±ξ + 2kπ
Solution 28.12
The eigenfunction problem associated with this problem is
φ′′ + λ2φ = 0,
φ(a) = φ(b) = 0,
1375

which has the solutions,
λn =
nπ
b −a,
φn = sin
nπ(x −a)
b −a

,
n ∈N.
We expand the solution and the inhomogeneity in the eigenfunctions.
y(x) =
∞
X
n=1
yn sin
nπ(x −a)
b −a

f(x) =
∞
X
n=1
fn sin
nπ(x −a)
b −a

,
fn =
2
b −a
Z b
a
f(x) sin
nπ(x −a)
b −a

dx
Since the solution y(x) satisﬁes the same homogeneous boundary conditions as the eigenfunctions, we can diﬀerentiate
the series. We substitute the series expansions into the diﬀerential equation.
y′′ + αy = f(x)
∞
X
n=1
yn
 −λ2
n + α

sin (λnx) =
∞
X
n=1
fn sin (λnx)
yn =
fn
α −λ2
n
Thus the solution of the problem has the series representation,
y(x) =
∞
X
n=1
 α −λ2
n

sin
nπ(x −a)
b −a

.
Solution 28.13
The eigenfunction problem associated with this problem is
φ′′ + λ2φ = 0,
φ(a) = φ(b) = 0,
1376

which has the solutions,
λn =
nπ
b −a,
φn = sin
nπ(x −a)
b −a

,
n ∈N.
We expand the solution and the inhomogeneity in the eigenfunctions.
y(x) =
∞
X
n=1
yn sin
nπ(x −a)
b −a

f(x) =
∞
X
n=1
fn sin
nπ(x −a)
b −a

,
fn =
2
b −a
Z b
a
f(x) sin
nπ(x −a)
b −a

dx
Since the solution y(x) does not satisfy the same homogeneous boundary conditions as the eigenfunctions, we can
diﬀerentiate the series. We multiply the diﬀerential equation by an eigenfunction and integrate from a to b. We use
integration by parts to move derivatives from y to the eigenfunction.
y′′ + αy = f(x)
Z b
a
y′′(x) sin(λmx) dx + α
Z b
a
y(x) sin(λmx) dx =
Z b
a
f(x) sin(λmx) dx
[y′ sin(λmx)]b
a −
Z b
a
y′λm cos(λmx) dx + αb −a
2
ym = b −a
2
fm
−[yλm cos(λmx)]b
a −
Z b
a
yλ2
m sin(λmx) dx + αb −a
2
ym = b −a
2
fm
−Bλm(−1)m + Aλm(−1)m+1 −λ2
mym + αb −a
2
ym = b −a
2
fm
ym = fm + (−1)mλm(A + B)
α −λ2
m
Thus the solution of the problem has the series representation,
y(x) =
∞
X
n=1
fm + (−1)mλm(A + B)
α −λ2
m
sin
nπ(x −a)
b −a

.
1377

Solution 28.14
1.
A + ıB =
1
1 −z2
=
∞
X
n=0
z2n
=
∞
X
n=0
r2n eı2nx
=
∞
X
n=0
r2n cos(2nx) + ı
∞
X
n=1
r2n sin(2nx)
A =
∞
X
n=0
r2n cos(2nx),
B =
∞
X
n=1
r2n sin(2nx)
A + ıB =
1
1 −z2
=
1
1 −r2 eı2x
=
1
1 −r2 cos(2x) −ır2 sin(2x)
=
1 −r2 cos(2x) + ır2 sin(2x)
(1 −r2 cos(2x))2 + (r2 sin(2x))2
A =
1 −r2 cos(2x)
1 −2r2 cos(2x) + r4,
B =
r2 sin(2x)
1 −2r2 cos(2x) + r4
1378

2. We consider the principal branch of the logarithm.
A + ıB = log(1 + z)
=
∞
X
n=1
(−1)n+1
n
zn
=
∞
X
n=1
(−1)n+1
n
rn eınx
=
∞
X
n=1
(−1)n+1
n
rn cos(nx) + ı sin(nx)

A =
∞
X
n=1
(−1)n+1
n
rn cos(nx),
B =
∞
X
n=1
(−1)n+1
n
rn sin(nx)
A + ıB = log(1 + z)
= log (1 + r eıx)
= log (1 + r cos x + ır sin x)
= log |1 + r cos x + ır sin x| + ı arg (1 + r cos x + ır sin x)
= log
p
(1 + r cos x)2 + (r sin x)2 + ı arctan (1 + r cos x, r sin x)
A = 1
2 log
 1 + 2r cos x + r2
,
B = arctan (1 + r cos x, r sin x)
1379

3.
An + ıBn =
n
X
k=1
zk
= 1 −zn+1
1 −z
= 1 −rn+1 eı(n+1)x
1 −r eıx
= 1 −r e−ıx −rn+1 eı(n+1)x +rn+2 eınx
1 −2r cos x + r2
An = 1 −r cos x −rn+1 cos((n + 1)x) + rn+2 cos(nx)
1 −2r cos x + r2
Bn = r sin x −rn+1 sin((n + 1)x) + rn+2 sin(nx)
1 −2r cos x + r2
An + ıBn =
n
X
k=1
zk
=
n
X
k=1
rk eıkx
An =
n
X
k=1
rk cos(kx),
Bn =
n
X
k=1
rk sin(kx)
Solution 28.15
1.
Z π
0
1 · sin x dx = [−cos x]π
0 = 2
1380

Thus the system is not orthogonal on the interval [0, π]. Consider the interval [a, a + π].
Z a+π
a
1 · sin x dx = [−cos x]a+π
a
= 2 cos a
Z a+π
a
1 · cos x dx = [sin x]a+π
a
= −2 sin a
Since there is no value of a for which both cos a and sin a vanish, the system is not orthogonal for any interval
of length π.
2. First note that
Z π
0
cos nx dx = 0 for n ∈N.
If n ̸= m, n ≥1 and m ≥0 then
Z π
0
cos nx cos mx dx = 1
2
Z π
0
 cos((n −m)x) + cos((n + m)x)

dx = 0
Thus the set {1, cos x, cos 2x, . . .} is orthogonal on [0, π]. Since
Z π
0
dx = π
Z π
0
cos2(nx) dx = π
2 ,
the set
(r
1
π,
r
2
π cos x,
r
2
π cos 2x, . . .
)
is orthonormal on [0, π].
1381

If n ̸= m, n ≥1 and m ≥1 then
Z π
0
sin nx sin mx dx = 1
2
Z π
0
 cos((n −m)x) −cos((n + m)x)

dx = 0
Thus the set {sin x, sin 2x, . . .} is orthogonal on [0, π]. Since
Z π
0
sin2(nx) dx = π
2 ,
the set
(r
2
π sin x,
r
2
π sin 2x, . . .
)
is orthonormal on [0, π].
Solution 28.16
Since the periodic extension of |x| in [−π, π] is an even function its Fourier series is a cosine series. Because of the
anti-symmetry about x = π/2 we see that except for the constant term, there will only be odd cosine terms. Since the
periodic extension is a continuous function, but has a discontinuous ﬁrst derivative, the Fourier coeﬃcients will decay
as 1/n2.
|x| =
∞
X
n=0
an cos(nx),
for x ∈[−π, π]
a0 = 1
π
Z π
0
x dx = 1
π
x2
2
π
0
= π
2
1382

an = 2
π
Z π
0
x cos(nx) dx
= 2
π

xsin(nx)
n
π
0
−2
π
Z π
0
sin(nx)
n
dx
= −2
π
cos(nx)
n2
π
0
= −2
πn2(cos(nπ) −1)
= 2(1 −(−1)n)
πn2
|x| = π
2 + 4
π
∞
X
n=1
odd n
1
n2 cos(nx)
for x ∈[−π, π]
Deﬁne RN(x) = f(x) −SN(x). We seek an upper bound on |RN(x)|.
|RN(x)| =

4
π
∞
X
n=N+1
odd n
1
n2 cos(nx)

≤4
π
∞
X
n=N+1
odd n
1
n2
= 4
π
∞
X
n=1
odd n
1
n2 −4
π
N
X
n=1
odd n
1
n2
Since
∞
X
n=1
odd n
1
n2 = π2
8
1383

We can bound the error with,
|RN(x)| ≤π
2 −4
π
N
X
n=1
odd n
1
n2.
N = 7 is the smallest number for which our error bound is less than 10−1. N ≥7 is suﬃcient to make the error less
that 0.1.
|R7(x)| ≤π
2 −4
π

1 + 1
9 + 1
25 + 1
49

≈0.079
N ≥7 is also necessary because.
|RN(0)| = 4
π
∞
X
n=N+1
odd n
1
n2.
Solution 28.17
1.
1 ∼
∞
X
n=1
an sin(nx),
0 ≤x ≤π
Since the odd periodic extension of the function is discontinuous, the Fourier coeﬃcients will decay as 1/n.
Because of the symmetry about x = π/2, there will be only odd sine terms.
an = 2
π
Z π
0
1 · sin(nx) dx
= 2
nπ(−cos(nπ) + cos(0))
= 2
nπ(1 −(−1)n)
1 ∼4
π
∞
X
n=1
odd n
sin(nx)
n
1384

2. It’s always OK to integrate a Fourier series term by term. We integrate the series in part (a).
Z x
a
1 dx ∼4
π
∞
X
n=1
odd n
Z x
a
sin(nξ)
n
dx
x −a ∼4
π
∞
X
n=1
odd n
cos(na) −cos(nx)
n2
Since the series converges uniformly, we can replace the ∼with =.
x −a = 4
π
∞
X
n=1
odd n
cos(na)
n2
−4
π
∞
X
n=1
odd n
cos(nx)
n2
Now we have a Fourier cosine series. The ﬁrst sum on the right is the constant term. If we choose a = π/2 this
sum vanishes since cos(nπ/2) = 0 for odd integer n.
x = π
2 −4
π
∞
X
n=1
odd n
cos(nx)
n2
3. If f(x) has the Fourier series
f(x) ∼a0
2 +
∞
X
n=1
(an cos(nx) + bn sin(nx)),
then Parseval’s theorem states that
Z π
−π
f 2(x) dx = π
2 a2
0 + π
∞
X
n=1
(a2
n + b2
n).
1385

We apply this to the Fourier sine series from part (a).
Z π
−π
f 2(x) dx = π
∞
X
n=1
odd n
 4
πn
2
Z 0
−π
(−1)2 dx +
Z π
0
(1)2 dx = 16
π
∞
X
n=1
1
(2n −1)2
∞
X
n=1
1
(2n −1)2 = π2
8
We substitute x = π in the series from part (b) to corroborate the result.
x = π
2 −4
π
∞
X
n=1
cos((2n −1)x)
(2n −1)2
π = π
2 −4
π
∞
X
n=1
cos((2n −1)π)
(2n −1)2
∞
X
n=1
1
(2n −1)2 = π2
8
Solution 28.18
1.
f(x) ∼a0 +
∞
X
n=1
an cos(nx)
Since the periodic extension of the function is discontinuous, the Fourier coeﬃcients will decay like 1/n. Because
of the anti-symmetry about x = π/2, there will be only odd cosine terms.
a0 = 1
π
Z π
0
f(x) dx = 1
2
1386

an = 2
π
Z π
0
f(x) cos(nx) dx
= 2
π
Z π/2
0
cos(nx) dx
= 2
πn sin(nπ/2)
=
(
2
πn(−1)(n−1)/2,
for odd n
0
for even n
The Fourier cosine series of f(x) is
f(x) ∼1
2 + 2
π
∞
X
n=0
(−1)n
2n + 1 cos((2n + 1)x).
2. The N th partial sum is
SN(x) = 1
2 + 2
π
N
X
n=0
(−1)n
2n + 1 cos((2n + 1)x).
We wish to evaluate the sum from part (a). First we make the change of variables y = x −π/2 to get rid of the
1387

(−1)n factor.
∞
X
n=0
(−1)n
2n + 1 cos((2n + 1)x)
=
N
X
n=0
(−1)n
2n + 1 cos((2n + 1)(y + π/2))
=
N
X
n=0
(−1)n
2n + 1(−1)n+1 sin((2n + 1)y)
= −
N
X
n=0
1
2n + 1 sin((2n + 1)y)
1388

We write the summand as an integral and interchange the order of summation and integration to get rid of the
1/(2n + 1) factor.
= −
N
X
n=0
Z y
0
cos((2n + 1)t) dt
= −
Z y
0
N
X
n=0
cos((2n + 1)t) dt
= −
Z y
0
 2N+1
X
n=1
cos(nt) −
N
X
n=1
cos(2nt)
!
dt
= −
Z y
0
ℜ
 2N+1
X
n=1
eınt −
N
X
n=1
eı2nt
!
dt
= −
Z y
0
ℜ
eıt −eı(2N+2)t
1 −eıt
−eı2t −eı2(N+1)t
1 −eı2t

dt
= −
Z y
0
ℜ
(eıt −eı2(N+1)t)(1 −eı2t) −(eı2t −eı2(N+1)t)(1 −eıt)
(1 −eıt)(1 −eı2t)

dt
= −
Z y
0
ℜ
eıt −eı2t + eı(2N+4)t −eı(2N+3)t
(1 −eıt)(1 −eı2t)

dt
= −
Z y
0
ℜ
eıt −eı(2N+3)t
1 −eı2t

dt
= −
Z y
0
ℜ
eı(2N+2)t −1
eıt −e−ıt

dt
= −
Z y
0
ℜ
−ı eı2(N+1)t +ı
2 sin t

dt
= −1
2
Z y
0
sin(2(N + 1)t)
sin t
dt
= −1
2
Z x−π/2
0
sin(2(N + 1)t)
sin t
dt
1389

Now we have a tidy representation of the partial sum.
SN(x) = 1
2 −1
π
Z x−π/2
0
sin(2(N + 1)t)
sin t
dt
3. We solve dSN(x)
dx
= 0 to ﬁnd the relative extrema of SN(x).
S′
N(x) = 0
−1
π
sin(2(N + 1)(x −π/2))
sin(x −π/2)
= 0
(−1)N+1 sin(2(N + 1)x)
−cos(x)
= 0
sin(2(N + 1)x)
cos(x)
= 0
x = xn =
nπ
2(N + 1),
n = 0, 1, . . . , N, N + 2, . . . , 2N + 2
Note that xN+1 = π/2 is not a solution as the denominator vanishes there. The function has a removable
singularity at x = π/2 with limiting value (−1)N.
4.
SN(xN) = 1
2 −1
π
Z
πN
2(N+1) −π/2
0
sin(2(N + 1)t)
sin t
dt
We note that the integrand is even.
Z
πN
2(N+1) −π/2
0
=
Z −
π
2(N+1)
0
= −
Z
π
2(N+1)
0
SN(xN) = 1
2 + 1
π
Z
π
2(N+1)
0
sin(2(N + 1)t)
sin t
dt
1390

5. We make the change of variables 2(N + 1)t →t.
SN(xN) = 1
2 + 1
π
Z π
0
sin(t)
2(N + 1) sin(t/(2(N + 1))) dt
Note that
lim
ϵ→0
sin(ϵt)
ϵ
= lim
ϵ→0
t cos(ϵt)
1
= t
SN(xN) →1
2 + 1
π
Z π
0
sin(t)
t
dt ≈1.0895
as N →∞
This is not equal to the limiting value of f(x), f(π/2 −0) = 1.
Solution 28.19
With the parametrization in t, x(t) and y(t) are continuous functions on the range [0, 2π]. Since the curve is closed,
we have x(0) = x(2π) and y(0) = y(2π). This means that the periodic extensions of x(t) and y(t) are continuous
functions. Thus we can diﬀerentiate their Fourier series. First we deﬁne formal Fourier series for x(t) and y(t).
x(t) = a0
2 +
∞
X
n=1
 an cos(nt) + bn sin(nt)

y(t) = c0
2 +
∞
X
n=1
 cn cos(nt) + dn sin(nt)

x′(t) =
∞
X
n=1
 nbn cos(nt) −nan sin(nt)

y′(t) =
∞
X
n=1
 ndn cos(nt) −ncn sin(nt)

In this problem we will be dealing with integrals on [0, 2π] of products of Fourier series. We derive a general formula
1391

for later use.
Z 2π
0
xy dt =
Z 2π
0
 
a0
2 +
∞
X
n=1
 an cos(nt) + bn sin(nt)

!  
c0
2 +
∞
X
n=1
 cn cos(nt) + dn sin(nt)

!
dt
=
Z 2π
0
 
a0c0
4
+
∞
X
n=1
 ancn cos2(nt) + bndn sin2(nt)

!
dt
= π
 
1
2a0c0 +
∞
X
n=1
(ancn + bndn)
!
In the arclength parametrization we have
dx
ds
2
+
dy
ds
2
= 1.
In terms of t = 2πs/L this is
dx
dt
2
+
dy
dt
2
=
 L
2π
2
.
We integrate this identity on [0, 2π].
L2
2π =
Z 2π
0
 dx
dt
2
+
dy
dt
2!
dt
= π
 ∞
X
n=1
 (nbn)2 + (−nan)2
+
∞
X
n=1
 (ndn)2 + (−ncn)2
!
= π
∞
X
n=1
n2(a2
n + b2
n + c2
n + d2
n)
L2 = 2π2
∞
X
n=1
n2(a2
n + b2
n + c2
n + d2
n)
1392

We assume that the curve is parametrized so that the area is positive. (Reversing the orientation changes the sign
of the area as deﬁned above.) The area is
A =
Z 2π
0
xdy
dt dt
=
Z 2π
0
 
a0
2 +
∞
X
n=1
 an cos(nt) + bn sin(nt)

!  ∞
X
n=1
 ndn cos(nt) −ncn sin(nt)

!
dt
= π
∞
X
n=1
n(andn −bncn)
Now we ﬁnd an upper bound on the area. We will use the inequality |ab| ≤1
2|a2 + b2|, which follows from expanding
(a −b)2 ≥0.
A ≤π
2
∞
X
n=1
n
 a2
n + b2
n + c2
n + d2
n

≤π
2
∞
X
n=1
n2  a2
n + b2
n + c2
n + d2
n

We can express this in terms of the perimeter.
= L2
4π
L2 ≥4πA
Now we determine the curves for which L2 = 4πA. To do this we ﬁnd conditions for which A is equal to the upper
bound we obtained for it above. First note that
∞
X
n=1
n
 a2
n + b2
n + c2
n + d2
n

=
∞
X
n=1
n2  a2
n + b2
n + c2
n + d2
n

1393

implies that all the coeﬃcients except a0, c0, a1, b1, c1 and d1 are zero. The constraint,
π
∞
X
n=1
n(andn −bncn) = π
2
∞
X
n=1
n
 a2
n + b2
n + c2
n + d2
n

then becomes
a1d1 −b1c1 = a2
1 + b2
1 + c2
1 + d2
1.
This implies that d1 = a1 and c1 = −b1.
a0 and c0 are arbitrary.
Thus curves for which L2 = 4πA have the
parametrization
x(t) = a0
2 + a1 cos t + b1 sin t,
y(t) = c0
2 −b1 cos t + a1 sin t.
Note that

x(t) −a0
2
2
+

y(t) −c0
2
2
= a2
1 + b2
1.
The curve is a circle of radius
p
a2
1 + b2
1 and center (a0/2, c0/2).
Solution 28.20
1. The Fourier sine series has the form
x(1 −x) =
∞
X
n=1
an sin(nπx).
The norm of the eigenfunctions is
Z 1
0
sin2(nπx) dx = 1
2.
The coeﬃcients in the expansion are
an = 2
Z 1
0
x(1 −x) sin(nπx) dx
=
2
π3n3(2 −2 cos(nπ) −nπ sin(nπ))
=
4
π3n3(1 −(−1)n).
1394

Thus the Fourier sine series is
x(1 −x) = 8
π3
∞
X
n=1
odd n
sin(nπx)
n3
= 8
π3
∞
X
n=1
sin((2n −1)πx)
(2n −1)3
.
The Fourier cosine series has the form
x(1 −x) =
∞
X
n=0
an cos(nπx).
The norm of the eigenfunctions is
Z 1
0
12 dx = 1,
Z 1
0
cos2(nπx) dx = 1
2.
The coeﬃcients in the expansion are
a0 =
Z 1
0
x(1 −x) dx = 1
6,
an = 2
Z 1
0
x(1 −x) cos(nπx) dx
= −
2
π2n2 + 4 sin(nπ) −nπ cos(nπ)
π3n3
= −
2
π2n2(1 + (−1)n)
Thus the Fourier cosine series is
x(1 −x) = 1
6 −4
π2
∞
X
n=1
even n
cos(nπx)
n2
= 1
6 −1
π2
∞
X
n=1
cos(2nπx)
n2
.
1395

-1
-0.5
0.5
1
-0.2
-0.1
0.1
0.2
-1
-0.5
0.5
1
-0.2
-0.1
0.1
0.2
Figure 28.13: The odd and even periodic extension of x(1 −x), 0 ≤x ≤1.
The Fourier sine series converges to the odd periodic extension of the function.
Since this function is C1,
continuously diﬀerentiable, we know that the Fourier coeﬃcients must decay as 1/n3. The Fourier cosine series
converges to the even periodic extension of the function. Since this function is only C0, continuous, the Fourier
coeﬃcients must decay as 1/n2. The odd and even periodic extensions are shown in Figure 28.13. The sine series
is better because of the faster convergence of the series.
2.
(a) We substitute x = 0 into the cosine series.
0 = 1
6 −1
π2
∞
X
n=1
1
n2
∞
X
n=1
1
n2 = π2
6
1396

(b) We substitute x = 1/2 into the cosine series.
1
4 = 1
6 −1
π2
∞
X
n=1
cos(nπ)
n2
∞
X
n=1
(−1)n
n2
= −π2
12
(c) We substitute x = 1/2 into the sine series.
1
4 = 8
π3
∞
X
n=1
sin((2n −1)π/2)
(2n −1)3
∞
X
n=1
(−1)n
(2n −1)3 = −π3
32
1397

Chapter 29
Regular Sturm-Liouville Problems
I learned there are troubles
Of more than one kind.
Some come from ahead
And some come from behind.
But I’ve bought a big bat.
I’m all ready, you see.
Now my troubles are going
To have troubles with me!
-I Had Trouble in Getting to Solla Sollew
-Theodor S. Geisel, (Dr. Suess)
29.1
Derivation of the Sturm-Liouville Form
Consider the eigenvalue problem on the ﬁnite interval [a . . . b],
p2(x)y′′ + p1(x)y′ + p0(x)y = µy,
1398

subject to the homogeneous unmixed boundary conditions
α1y(a) + α2y′(a) = 0,
β1y(b) + β2y′(b) = 0.
Here the coeﬃcient functions pj are real and continuous and p2 > 0 on the interval [a . . . b]. (Note that if p2 were
negative we could multiply the equation by (−1) and replace µ by −µ.) The parameters αj and βj are real.
We would like to write this problem in a form that can be used to obtain qualitative information about the problem.
First we will write the operator in self-adjoint form. We divide by p2 since it is non-vanishing.
y′′ + p1
p2
y′ + p0
p2
y = µ
p2
y.
We multiply by an integrating factor.
I = exp
Z p1
p2
dx

≡eP(x)
eP(x)

y′′ + p1
p2
y′ + p0
p2
y

= eP(x) µ
p2
y
 eP(x) y′′ + eP(x) p0
p2
y = eP(x) µ
p2
y
For notational convenience, we deﬁne new coeﬃcient functions and parameters.
p = eP(x),
q = eP(x) p0
p2
,
σ = eP(x) 1
p2
,
λ = −µ.
Since the pj are continuous and p2 is positive, p, q, and σ are continuous. p and σ are positive functions. The problem
now has the form,
(py′)′ + qy + λσy = 0,
subject to the same boundary conditions,
α1y(a) + α2y′(a) = 0,
β1y(b) + β2y′(b) = 0.
1399

This is known as a Regular Sturm-Liouville problem. We will devote much of this chapter to studying the properties of
this problem. We will encounter many results that are analogous to the properties of self-adjoint eigenvalue problems.
Example 29.1.1
d
dx

ln xdy
dx

+ λxy = 0,
y(1) = y(2) = 0
is not a regular Sturm-Liouville problem since ln x vanishes at x = 1.
Result 29.1.1 Any eigenvalue problem of the form
p2y′′ + p1y′ + p0y = µy,
for a ≤x ≤b,
α1y(a) + α2y′(a) = 0,
β1y(b) + β2y′(b) = 0,
where the pj are real and continuous and p2 > 0 on [a, b], and the αj and βj are real can be
written in the form of a regular Sturm-Liouville problem,
(py′)′ + qy + λσy = 0,
on a ≤x ≤b,
α1y(a) + α2y′(a) = 0,
β1y(b) + β2y′(b) = 0.
29.2
Properties of Regular Sturm-Liouville Problems
Self-Adjoint.
Consider the Regular Sturm-Liouville equation.
L[y] ≡(py′)′ + qy = −λσy.
1400

We see that the operator is formally self-adjoint. Now we determine if the problem is self-adjoint.
⟨v|L[u]⟩−⟨L[v]|u⟩= ⟨v|(pu′)′ + qu⟩−⟨(pv′)′ + qv|u⟩
= [vpu′]b
a −⟨v′|pu′⟩+ ⟨v|qu⟩−[pv′u]b
a + ⟨pv′|u′⟩−⟨qv|u⟩
= [vpu′]b
a −[pv′u]b
a
= p(b)
 v(b)u′(b) −v′(b)u(b)

+ p(a)
 v(a)u′(a) −v′(a)u(a)

= p(b)

v(b)

−β1
β2

u(b) −

−β1
β2

v(b)u(b)

+ p(a)

v(a)

−α1
α2

u(a) −

−α1
α2

v(a)u(a)

= 0
Above we used the fact that the αi and βi are real.
α1
α2

=
α1
α2

,
β1
β2

=
β1
β2

Thus L[y] subject to the boundary conditions is self-adjoint.
Real Eigenvalues.
Let λ be an eigenvalue with the eigenfunction φ. We start with Green’s formula.
⟨φ|L[φ]⟩−⟨L[φ]|φ⟩= 0
⟨φ| −λσφ⟩−⟨−λσφ|φ⟩= 0
−λ⟨φ|σ|φ⟩+ λ⟨φ|σ|φ⟩= 0
(λ −λ)⟨φ|σ|φ⟩= 0
Since ⟨φ|σ|φ⟩> 0, λ −λ = 0. Thus the eigenvalues are real.
1401

Inﬁnite Number of Eigenvalues.
There are an inﬁnite of eigenvalues which have no ﬁnite cluster point. This
result is analogous to the result that we derived for self-adjoint eigenvalue problems. When we cover the Rayleigh
quotient, we will ﬁnd that there is a least eigenvalue. Since the eigenvalues are distinct and have no ﬁnite cluster point,
λn →∞as n →∞. Thus the eigenvalues form an ordered sequence,
λ1 < λ2 < λ3 < · · · .
Orthogonal Eigenfunctions.
Let λ and µ be two distinct eigenvalues with the eigenfunctions φ and ψ. Green’s
formula states
⟨ψ|L[φ]⟩−⟨L[ψ]|φ⟩= 0.
⟨ψ| −λσφ⟩−⟨−µσψ|φ⟩= 0
−λ⟨ψ|σ|φ⟩+ µ⟨ψ|σ|φ⟩= 0
(µ −λ)⟨ψ|σ|φ⟩= 0
Since the eigenvalues are distinct, ⟨ψ|σ|φ⟩= 0. Thus eigenfunctions corresponding to distinct eigenvalues are orthogonal
with respect to σ.
Unique Eigenfunctions.
Let λ be an eigenvalue. Suppose φ and ψ are two independent eigenfunctions corre-
sponding to λ.
L[φ] + λσφ = 0,
L[ψ] + λσψ = 0
We take the diﬀerence of ψ times the ﬁrst equation and φ times the second equation.
ψL[φ] −φL[ψ] = 0
ψ(pφ′)′ −φ(pψ′)′ = 0
(p(ψφ′ −ψ′φ))′ = 0
p(ψφ′ −ψ′φ) = const
In order to satisfy the boundary conditions, the constant must be zero.
p(ψφ′ −ψ′φ) = 0
1402

Since p > 0 the second factor vanishes.
ψφ′ −ψ′φ = 0
φ′
ψ −ψ′φ
ψ2 = 0
d
dx
φ
ψ

= 0
φ
ψ = const
φ and ψ are not independent. Thus each eigenvalue has a unique, (to within a multiplicative constant), eigenfunction.
Real Eigenfunctions.
If λ is an eigenvalue with eigenfunction φ, then
(pφ′)′ + qφ + λσφ = 0.
We take the complex conjugate of this equation.

pφ
′′
+ qφ + λσφ = 0.
Thus φ is also an eigenfunction corresponding to λ. Are φ and φ independent functions, or do they just diﬀer by a
multiplicative constant? (For example, eıx and e−ıx are independent functions, but ıx and −ıx are dependent.) From
our argument on unique eigenfunctions, we see that
φ = (const)φ.
Since φ and φ only diﬀer by a multiplicative constant, the eigenfunctions can be chosen so that they are real-valued
functions.
1403

Rayleigh’s Quotient.
Let λ be an eigenvalue with the eigenfunction φ.
⟨φ|L[φ]⟩= ⟨φ| −λσφ⟩
⟨φ|(pφ′)′ + qφ⟩= −λ⟨φ|σ|φ⟩

φpφ′b
a −⟨φ′|p|φ′⟩+ ⟨φ|q|φ⟩= −λ⟨φ|σ|φ⟩
λ = −

pφφ′b
a + ⟨φ′|p|φ′⟩−⟨φ|q|φ⟩
⟨φ|σ|φ⟩
This is known as Rayleigh’s quotient. It is useful for obtaining qualitative information about the eigenvalues.
Minimum Property of Rayleigh’s Quotient.
Note that since p, q, σ and φ are bounded functions, the Rayleigh
quotient is bounded below. Thus there is a least eigenvalue. If we restrict u to be a real continuous function that
satisﬁes the boundary conditions, then
λ1 = min
u
−[puu′]b
a + ⟨u′|p|u′⟩−⟨u|q|u⟩
⟨u|σ|u⟩
,
where λ1 is the least eigenvalue. This form allows us to get upper and lower bounds on λ1.
To derive this formula, we ﬁrst write it in terms of the operator L.
λ1 = min
u
−⟨u|L[u]⟩
⟨u|σ|u⟩
Since u is continuous and satisﬁes the boundary conditions, we can expand u in a series of the eigenfunctions.
−⟨u|L[u]⟩
⟨u|σ|u⟩= −

P∞
n=1 cnφn
L [P∞
m=1 cmφm]


P∞
n=1 cnφn
σ
 P∞
m=1 cmφm

= −

P∞
n=1 cnφn
 −P∞
m=1 cmλmσφm


P∞
n=1 cnφn
σ
 P∞
m=1 cmφm

1404

We assume that we can interchange summation and integration.
=
P∞
n=1
P∞
m=1 cncmλn⟨φm|σ|φn⟩
P∞
n=1
P∞
m=1 cncm⟨φm|σ|φn⟩
=
P∞
n=1 |cn|2λn⟨φn|σ|φn⟩
P∞
n=1 |cn|2⟨φn|σ|φn⟩
≤λ1
P∞
n=1 |cn|2⟨φn|σ|φn⟩
P∞
n=1 |cn|2⟨φn|σ|φn⟩
= λ1
We see that the minimum value of Rayleigh’s quotient is λ1. The minimum is attained when cn = 0 for all n ≥2, that
is, when u = c1φ1.
Completeness.
The set of the eigenfunctions of a regular Sturm-Liouville problem is complete. That is, any piecewise
continuous function deﬁned on [a, b] can be expanded in a series of the eigenfunctions,
f(x) ∼
∞
X
n=1
cnφn(x),
where the cn are the generalized Fourier coeﬃcients,
cn = ⟨φn|σ|f⟩
⟨φn|σ|φn⟩.
Here the sum is convergent in the mean. For any ﬁxed x, the sum converges to 1
2(f(x−)+f(x+)). If f(x) is continuous
and satisﬁes the boundary conditions, then the convergence is uniform.
1405

Result 29.2.1 Properties of regular Sturm-Liouville problems.
• The eigenvalues λ are real.
• There are an inﬁnite number of eigenvalues
λ1 < λ2 < λ3 < · · · .
There is a least eigenvalue λ1 but there is no greatest eigenvalue, (λn →∞as n →∞).
• For each eigenvalue, there is one unique, (to within a multiplicative constant), eigenfunc-
tion φn. The eigenfunctions can be chosen to be real-valued. (Assume the φn following
are real-valued.) The eigenfunction φn has exactly n −1 zeros in the open interval
a < x < b.
• The eigenfunctions are orthogonal with respect to the weighting function σ(x).
Z b
a
φn(x)φm(x)σ(x) dx = 0
if n ̸= m.
• The eigenfunctions are complete. Any piecewise continuous function f(x) deﬁned on
a ≤x ≤b can be expanded in a series of eigenfunctions
f(x) ∼
∞
X
n=1
cnφn(x),
where
cn =
R b
a f(x)φn(x)σ(x) dx
R b
a φ2n(x)σ(x) dx
.
The sum converges to 1
2(f(x−) + f(x+)).
• The eigenvalues can be related to the eigenfunctions with a formula known as the
Rayleigh quotient.
λn =
−pφn
dφn
dx

b
a +
Z b
a

p

dφn
dx
2
−qφ2
n

dx
R b
a φ2nσ dx
1406

Example 29.2.1 A simple example of a Sturm-Liouville problem is
d
dx
dy
dx

+ λy = 0,
y(0) = y(π) = 0.
Bounding The Least Eigenvalue.
The Rayleigh quotient for the ﬁrst eigenvalue is
λ1 =
R π
0 (φ′
1)2 dx
R π
0 φ2
1 dx .
Immediately we see that the eigenvalues are non-negative. If
R π
0 (φ′
1)2 dx = 0 then φ = (const). The only constant that
satisﬁes the boundary conditions is φ = 0. Since the trivial solution is not an eigenfunction, λ = 0 is not an eigenvalue.
Thus all the eigenvalues are positive.
Now we get an upper bound for the ﬁrst eigenvalue.
λ1 = min
u
R π
0 (u′)2 dx
R π
0 u2 dx
where u is continuous and satisﬁes the boundary conditions. We choose u = x(x −π) as a trial function.
λ1 ≤
R π
0 (u′)2 dx
R π
0 u2 dx
=
R π
0 (2x −π)2 dx
R π
0 (x2 −πx)2 dx
= π3/3
π5/30
= 10
π2
≈1.013
1407

Finding the Eigenvalues and Eigenfunctions.
We consider the cases of negative, zero, and positive eigenvalues
to check our results above.
λ < 0. The general solution is
y = c e
√
−λx +d e−
√
−λx .
The only solution that satisﬁes the boundary conditions is the trivial solution, y = 0. Thus there are no negative
eigenvalues.
λ = 0. The general solution is
y = c + dx.
Again only the trivial solution satisﬁes the boundary conditions, so λ = 0 is not an eigenvalue.
λ > 0. The general solution is
y = c cos(
√
λx) + d sin(
√
λx).
We apply the boundary conditions.
y(0) = 0
→
c = 0
y(π) = 0
→
d sin(
√
λπ) = 0
The nontrivial solutions are
√
λ = n = 1, 2, 3, . . .
y = d sin(nπ).
Thus the eigenvalues and eigenfunctions are
λn = n2,
φn = sin(nx),
for n = 1, 2, 3, . . .
We can verify that this example satisﬁes all the properties listed in Result 29.2.1. Note that there are an inﬁnite number
of eigenvalues. There is a least eigenvalue λ1 = 1 but there is no greatest eigenvalue. For each eigenvalue, there is one
eigenfunction. The nth eigenfunction sin(nx) has n −1 zeroes in the interval 0 < x < π.
1408

Since a series of the eigenfunctions is the familiar Fourier sine series, we know that the eigenfunctions are orthogonal
and complete. We check Rayleigh’s quotient.
λn =
−pφn
dφn
dx

π
0 +
Z π
0

p
  dφn
dx
2 −qφ2
n

dx
R π
0 φ2
nσ dx
=
−sin(nx)d(sin(nx))
dx

π
0 +
Z π
0

d(sin(nx))
dx
2
dx
R π
0 sin2(nx)dx
=
R π
0 n2 cos2(nx) dx
π/2
= n2
Example 29.2.2 Consider the eigenvalue problem
x2y′′ + xy′ + y = µy,
y(1) = y(2) = 0.
Since x2 > 0 on [1 . . . 2], we can write this problem in terms of a regular Sturm-Liouville eigenvalue problem. We divide
by x2.
y′′ + 1
xy′ + 1
x2(1 −µ)y = 0
We multiply by the integrating factor exp(
R 1
x dx) = exp(ln x) = x and make the substitution, λ = 1 −µ to obtain
the Sturm-Liouville form.
xy′′ + y′ + λ1
xy = 0
(xy′)′ + λ1
xy = 0
We see that the eigenfunctions will be orthogonal with respect to the weighting function σ = 1/x.
1409

The Rayleigh quotient is
λ = −

pφφ′b
a + ⟨φ′|x|φ′⟩
⟨φ| 1
x|φ⟩
= ⟨φ′|x|φ′⟩
⟨φ| 1
x|φ⟩.
If φ′ = 0, then only the trivial solution, φ = 0, satisﬁes the boundary conditions. Thus the eigenvalues λ are positive.
Returning to the original problem, we see that the eigenvalues, µ, satisfy µ < 1. Since this is an Euler equation, we
can ﬁnd solutions with the substitution y = xα.
α(α −1) + α + 1 −µ = 0
α2 + 1 −µ = 0
Note that µ < 1.
α = ±ı
p
1 −µ
The general solution is
y = c1xı√1−µ + c2x−ı√1−µ.
We know that the eigenfunctions can be written as real functions. We rewrite the solution.
y = c1 eı√1−µ ln x +c2 e−ı√1−µ ln x
An equivalent form is
y = c1 cos(
p
1 −µ ln x) + c2 sin(
p
1 −µ ln x).
We apply the boundary conditions.
y(1) = 0
→
c1 = 0
y(2) = 0
→
sin(
p
1 −µ ln 2) = 0
→
p
1 −µ ln 2 = nπ,
for n = 1, 2, . . .
1410

Thus the eigenvalues and eigenfunctions are
µn = 1 −
 nπ
ln 2
2
,
φn = sin

nπln x
ln 2

for n = 1, 2, . . .
29.3
Solving Diﬀerential Equations With Eigenfunction Expansions
Linear Algebra.
Consider the eigenvalue problem,
Ax = λx.
If the matrix A has a complete, orthonormal set of eigenvectors {ξk} with eigenvalues {λk} then we can represent any
vector as a linear combination of the eigenvectors.
y =
n
X
k=1
akξk,
ak = ξk · y
y =
n
X
k=1
(ξk · y) ξk
This property allows us to solve the inhomogeneous equation
Ax −µx = b.
(29.1)
Before we try to solve this equation, we should consider the existence/uniqueness of the solution.
If µ is not an
eigenvalue, then the range of L ≡A −µ is Rn. The problem has a unique solution. If µ is an eigenvalue, then the
null space of L is the span of the eigenvectors of µ. That is, if µ = λi, then nullspace(L) = span(ξi1, ξi2, . . . , ξim).
({ξi1, ξi2, . . . , ξim} are the eigenvalues of λi.) If b is orthogonal to nullspace(L) then Equation 29.1 has a solution,
but it is not unique. If y is a solution then we can add any linear combination of {ξij} to obtain another solution.
Thus the solutions have the form
x = y +
m
X
j=1
cjξij.
1411

If b is not orthogonal to nullspace(L) then Equation 29.1 has no solution.
Now we solve Equation 29.1. We assume that µ is not an eigenvalue. We expand the solution x and the inhomo-
geneity in the orthonormal eigenvectors.
x =
n
X
k=1
akξk,
b =
n
X
k=1
bkξk
We substitute the expansions into Equation 29.1.
A
n
X
k=1
akξk −µ
n
X
k=1
akξk =
n
X
k=1
bkξk
n
X
k=1
akλkξk −µ
n
X
k=1
akξk =
n
X
k=1
bkξk
ak =
bk
λk −µ
The solution is
x =
n
X
k=1
bk
λk −µξk.
Inhomogeneous Boundary Value Problems.
Consider the self-adjoint eigenvalue problem,
Ly = λy,
a < x < b,
B1[y] = B2[y] = 0.
If the problem has a complete, orthonormal set of eigenfunctions {φk} with eigenvalues {λk} then we can represent
any square-integrable function as a linear combination of the eigenfunctions.
f =
X
k
fkφk,
fk = ⟨φk|f⟩=
Z b
a
φk(x)f(x) dx
f =
X
k
⟨φk|f⟩φk
1412

This property allows us to solve the inhomogeneous diﬀerential equation
Ly −µy = f,
a < x < b,
(29.2)
B1[y] = B2[y] = 0.
Before we try to solve this equation, we should consider the existence/uniqueness of the solution.
If µ is not an
eigenvalue, then the range of L−µ is the space of square-integrable functions. The problem has a unique solution. If µ
is an eigenvalue, then the null space of L is the span of the eigenfunctions of µ. That is, if µ = λi, then nullspace(L) =
span(φi1, φi2, . . . , φim). ({φi1, φi2, . . . , φim} are the eigenvalues of λi.) If f is orthogonal to nullspace(L −µ) then
Equation 29.2 has a solution, but it is not unique. If u is a solution then we can add any linear combination of {φij}
to obtain another solution. Thus the solutions have the form
y = u +
m
X
j=1
cjφij.
If f is not orthogonal to nullspace(L −µ) then Equation 29.2 has no solution.
Now we solve Equation 29.2. We assume that µ is not an eigenvalue. We expand the solution y and the inhomo-
geneity in the orthonormal eigenfunctions.
y =
X
k
ykφk,
f =
X
k
fkφk
It would be handy if we could substitute the expansions into Equation 29.2. However, the expansion of a function is
not necessarily diﬀerentiable. Thus we demonstrate that since y is C2(a . . . b) and satisﬁes the boundary conditions
B1[y] = B2[y] = 0, we are justiﬁed in substituting it into the diﬀerential equation. In particular, we will show that
L[y] = L
"X
k
ykφk
#
=
X
k
ykL [φk] =
X
k
ykλkφk.
To do this we will use Green’s identity. If u and v are C2(a . . . b) and satisfy the boundary conditions B1[y] = B2[y] = 0
then
⟨u|L[v]⟩= ⟨L[u]|v⟩.
1413

First we assume that we can diﬀerentiate y term-by-term.
L[y] =
X
k
ykλkφk
Now we directly expand L[y] and show that we get the same result.
L[y] =
X
k
ckφk
ck = ⟨φk|L[y]⟩
= ⟨L[φk]|y⟩
= ⟨λkφk|y⟩
= λk⟨φk|y⟩
= λkyk
L[y] =
X
k
ykλφk
The series representation of y may not be diﬀerentiable, but we are justiﬁed in applying L term-by-term.
Now we substitute the expansions into Equation 29.2.
L
"X
k
ykφk
#
−µ
X
k
ykφk =
X
k
fkφk
X
k
λkykφk −µ
X
k
ykφk =
X
k
fkφk
yk =
fk
λk −µ
The solution is
y =
X
k
fk
λk −µφk
1414

Consider a second order, inhomogeneous problem.
L[y] = f(x),
B1[y] = b1,
B2[y] = b2
We will expand the solution in an orthogonal basis.
y =
X
n
anφn
We would like to substitute the series into the diﬀerential equation, but in general we are not allowed to diﬀerentiate
such series. To get around this, we use integration by parts to move derivatives from the solution y, to the φn.
Example 29.3.1 Consider the problem,
y′′ + αy = f(x),
y(0) = a,
y(π) = b,
where α ̸= n2, n ∈Z+. We expand the solution in a cosine series.
y(x) = y0
√π +
∞
X
n=1
yn
r
2
π cos(nx)
We also expand the inhomogeneous term.
f(x) = f0
√π +
∞
X
n=1
fn
r
2
π cos(nx)
We multiply the diﬀerential equation by the orthonormal functions and integrate over the interval. We neglect the
1415

special case φ0 = 1/√π for now.
Z π
0
r
2
π cos(nx)y′′ dx + α
Z π
0
r
2
π cos(nx)y dx =
Z π
0
r
2
πf(x) dx
"r
2
π cos(nx)y′(x)
#π
0
+
Z π
0
r
2
πn sin(nx)y′(x) dx + αyn = fn
r
2
π ((−1)ny′(π) −y′(0)) +
"r
2
πn sin(nx)y(x)
#π
0
−
Z π
0
r
2
πn2 cos(nx)y(x) dx + αyn = fn
r
2
π ((−1)ny′(π) −y′(0)) −n2yn + αyn = fn
Unfortunately we don’t know the values of y′(0) and y′(π).
CONTINUE HERE
1416

29.4
Exercises
Exercise 29.1
Find the eigenvalues and eigenfunctions of
y′′ + 2αy′ + λy = 0,
y(a) = y(b) = 0,
where a < b.
Write the problem in Sturm Liouville form. Verify that the eigenvalues and eigenfunctions satisfy the properties of
regular Sturm-Liouville problems. Find the coeﬃcients in the expansion of an arbitrary function f(x) in a series of the
eigenfunctions.
Hint, Solution
Exercise 29.2
Find the eigenvalues and eigenfunctions of the boundary value problem
y′′ +
λ
(x + 1)2y = 0
on the interval 1 ≤x ≤2 with boundary conditions y(1) = y(2) = 0. Discuss how the results satisfy the properties of
Sturm-Liouville problems.
Hint, Solution
Exercise 29.3
Find the eigenvalues and eigenfunctions of
y′′ + 2α + 1
x
y′ + λ
x2y = 0,
y(a) = y(b) = 0,
where 0 < a < b. Write the problem in Sturm Liouville form. Verify that the eigenvalues and eigenfunctions satisfy the
properties of regular Sturm-Liouville problems. Find the coeﬃcients in the expansion of an arbitrary function f(x) in a
series of the eigenfunctions.
Hint, Solution
1417

Exercise 29.4
Find the eigenvalues and eigenfunctions of
y′′ −y′ + λy = 0,
y(0) = y(1) = 0.
Find the coeﬃcients in the expansion of an arbitrary, f(x), in a series of the eigenfunctions.
Hint, Solution
Exercise 29.5
Consider
y′′ + y = f(x),
y(0) = 0,
y(1) + y′(1) = 0.
(29.3)
The associated eigenvalue problem is
y′′ + y = µy
y(0) = 0
y(1) + y′(1) = 0.
Find the eigenfunctions for this problem and the equation which the eigenvalues must satisfy.
To do this, consider the eigenvalues and eigenfunctions for,
y′′ + λy = 0,
y(0) = 0,
y(1) + y′(1) = 0.
Show that the transcendental equation for λ has inﬁnitely many roots λ1 < λ2 < λ3 < · · · . Find the limit of λn as
n →∞. How is this limit approached?
Give the general solution of Equation 29.3 in terms of the eigenfunctions.
Hint, Solution
Exercise 29.6
Consider
y′′ + y = f(x)
y(0) = 0
y(1) + y′(1) = 0.
Find the eigenfunctions for this problem and the equation which the eigenvalues satisfy. Give the general solution in
terms of these eigenfunctions.
Hint, Solution
1418

Exercise 29.7
Show that the eigenvalue problem,
y′′ + λy = 0,
y(0) = 0,
y′(0) −y(1) = 0,
(note the mixed boundary condition), has only one real eigenvalue. Find it and the corresponding eigenfunction. Show
that this problem is not self-adjoint. Thus the proof, valid for unmixed, homogeneous boundary conditions, that all
eigenvalues are real fails in this case.
Hint, Solution
Exercise 29.8
Determine the Rayleigh quotient, R[φ] for,
y′′ + 1
xy′ + λy = 0,
|y(0)| < ∞,
y(1) = 0.
Use the trial function φ = 1 −x in R[φ] to deduce that the smallest zero of J0(x), the Bessel function of the ﬁrst kind
and order zero, is less than
√
6.
Hint, Solution
Exercise 29.9
Discuss the eigenvalues of the equation
y′′ + λq(z)y = 0,
y(0) = y(π) = 0
where
q(z) =
(
a > 0,
0 ≤z ≤l
b > 0,
l < z ≤π.
This is an example that indicates that the results we obtained in class for eigenfunctions and eigenvalues with q(z)
continuous and bounded also hold if q(z) is simply integrable; that is
Z π
0
|q(z)| dz
1419

is ﬁnite.
Hint, Solution
Exercise 29.10
1. Find conditions on the smooth real functions p(x), q(x), r(x) and s(x) so that the eigenvalues, λ, of:
Lv ≡(p(x)v′′(x))′′ −(q(x)v′(x))′ + r(x)v(x) = λs(x)v(x),
a < x < b
v(a) = v′′(a) = 0
v′′(b) = 0,
p(b)v′′′(b) −q(b)v′(b) = 0
are positive. Prove the assertion.
2. Show that for any smooth p(x), q(x), r(x) and s(x) the eigenfunctions belonging to distinct eigenvalues are
orthogonal relative to the weight s(x). That is:
Z b
a
vm(x)vk(x)s(x) dx = 0 if λk ̸= λm.
3. Find the eigenvalues and eigenfunctions for:
d4φ
dx4 = λφ,
nφ(0) = φ′′(0) = 0,
φ(1) = φ′′(1) = 0.
Hint, Solution
1420

29.5
Hints
Hint 29.1
Hint 29.2
Hint 29.3
Hint 29.4
Write the problem in Sturm-Liouville form to show that the eigenfunctions are orthogonal with respect to the weighting
function σ = e−x.
Hint 29.5
Note that the solution is a regular Sturm-Liouville problem and thus the eigenvalues are real. Use the Rayleigh quotient
to show that there are only positive eigenvalues. Informally show that there are an inﬁnite number of eigenvalues with
a graph.
Hint 29.6
Hint 29.7
Find the solution for λ = 0, λ < 0 and λ > 0. A problem is self-adjoint if it satisﬁes Green’s identity.
Hint 29.8
Write the equation in self-adjoint form. The Bessel equation of the ﬁrst kind and order zero satisﬁes the problem,
y′′ + 1
xy′ + y = 0,
|y(0)| < ∞,
y(r) = 0,
where r is a positive root of J0(x). Make the change of variables ξ = x/r, u(ξ) = y(x).
1421

Hint 29.9
Hint 29.10
1422

29.6
Solutions
Solution 29.1
Recall that constant coeﬃcient equations are shift invariant. If u(x) is a solution, then so is u(x −c).
We substitute y = eγx into the constant coeﬃcient equation.
y′′ + 2αy′ + λy = 0
γ2 + 2αγ + λ = 0
γ = −α ±
√
α2 −λ
First we consider the case λ = α2. A set of solutions of the diﬀerential equation is
e−αx, x e−αx	
The homogeneous solution that satisﬁes the left boundary condition y(a) = 0 is
y = c(x −a) e−αx .
Since only the trivial solution with c = 0 satisﬁes the right boundary condition, λ = α2 is not an eigenvalue.
Next we consider the case λ ̸= α2. We write
γ = −α ± ı
√
λ −α2.
Note that ℜ(
√
λ −α2) ≥0. A set of solutions of the diﬀerential equation is
n
e(−α±ı
√
λ−α2)xo
By taking the sum and diﬀerence of these solutions we obtain a new set of linearly independent solutions.
n
e−αx cos
√
λ −α2x

, e−αx sin
√
λ −α2x
o
The solution which satisﬁes the left boundary condition is
y = c e−αx sin
√
λ −α2(x −a)

.
1423

For nontrivial solutions, the right boundary condition y(b) = 0 imposes the constraint
e−αb sin
√
λ −α2(b −a)

= 0
√
λ −α2(b −a) = nπ,
n ∈Z
We have the eigenvalues
λn = α2 +
 nπ
b −a
2
,
n ∈Z
with the eigenfunctions
φn = e−αx sin

nπx −a
b −a

.
To write the problem in Sturm-Liouville form, we multiply by the integrating factor
e
R
2α dx = e2αx .
 e2αx y′′ + λ e2αx y = 0,
y(a) = y(b) = 0
Now we verify that the Sturm-Liouville properties are satisﬁed.
• The eigenvalues
λn = α2 +
 nπ
b −a
2
,
n ∈Z
are real.
• There are an inﬁnite number of eigenvalues
λ1 < λ2 < λ3 < · · · ,
α2 +

π
b −a
2
< α2 +
 2π
b −a
2
< α2 +
 3π
b −a
2
< · · · .
1424

There is a least eigenvalue
λ1 = α2 +

π
b −a
2
,
but there is no greatest eigenvalue, (λn →∞as n →∞).
• For each eigenvalue, we found one unique, (to within a multiplicative constant), eigenfunction φn. We were able
to choose the eigenfunctions to be real-valued. The eigenfunction
φn = e−αx sin

nπx −a
b −a

.
has exactly n −1 zeros in the open interval a < x < b.
• The eigenfunctions are orthogonal with respect to the weighting function σ(x) = e2ax.
Z b
a
φn(x)φm(x)σ(x) dx =
Z b
a
e−αx sin

nπx −a
b −a

e−αx sin

mπx −a
b −a

e2ax dx
=
Z b
a
sin

nπx −a
b −a

sin

mπx −a
b −a

dx
= b −a
π
Z π
0
sin(nx) sin(mx) dx
= b −a
2π
Z π
0
(cos((n −m)x) −cos((n + m)x)) dx
= 0
if n ̸= m
• The eigenfunctions are complete. Any piecewise continuous function f(x) deﬁned on a ≤x ≤b can be expanded
in a series of eigenfunctions
f(x) ∼
∞
X
n=1
cnφn(x),
1425

where
cn =
R b
a f(x)φn(x)σ(x) dx
R b
a φ2
n(x)σ(x) dx
.
The sum converges to 1
2(f(x−) + f(x+)). (We do not prove this property.)
• The eigenvalues can be related to the eigenfunctions with the Rayleigh quotient.
λn =

−pφn
dφn
dx
b
a +
R b
a

p
  dφn
dx
2 −qφ2
n

dx
R b
a φ2
nσ dx
=
R b
a

e2αx  e−αx   nπ
b−a cos
 nπ x−a
b−a

−α sin
 nπ x−a
b−a
2
dx
R b
a
 e−αx sin
 nπ x−a
b−a
2 e2αx dx
=
R b
a
  nπ
b−a
2 cos2  nπ x−a
b−a

−2α nπ
b−a cos
 nπ x−a
b−a

sin
 nπ x−a
b−a

+ α2 sin2  nπ x−a
b−a

dx
R b
a sin2  nπ x−a
b−a

dx
=
R π
0
  nπ
b−a
2 cos2(x) −2α nπ
b−a cos(x) sin(x) + α2 sin2(x)

dx
R π
0 sin2(x) dx
= α2 +
 nπ
b −a
2
Now we expand a function f(x) in a series of the eigenfunctions.
f(x) ∼
∞
X
n=1
cn e−αx sin

nπx −a
b −a

,
1426

where
cn =
R b
a f(x)φn(x)σ(x) dx
R b
a φ2
n(x)σ(x) dx
=
2n
b −a
Z b
a
f(x) eαx sin

nπx −a
b −a

dx
Solution 29.2
This is an Euler equation. We substitute y = (x + 1)α into the equation.
y′′ +
λ
(x + 1)2y = 0
α(α −1) + λ = 0
α = 1 ±
√
1 −4λ
2
First consider the case λ = 1/4. A set of solutions is
n√
x + 1,
√
x + 1 ln(x + 1)
o
.
Another set of solutions is
√
x + 1,
√
x + 1 ln
x + 1
2

.
The solution which satisﬁes the boundary condition y(1) = 0 is
y = c
√
x + 1 ln
x + 1
2

.
Since only the trivial solution satisﬁes the y(2) = 0, λ = 1/4 is not an eigenvalue.
1427

Now consider the case λ ̸= 1/4. A set of solutions is
n
(x + 1)(1+
√
1−4λ)/2, (x + 1)(1−
√
1−4λ)/2o
.
We can write this in terms of the exponential and the logarithm.
√
x + 1 exp

ı
√
4λ −1
2
ln(x + 1)

,
√
x + 1 exp

−ı
√
4λ −1
2
ln(x + 1)

.
Note that
√
x + 1 exp

ı
√
4λ −1
2
ln
x + 1
2

,
√
x + 1 exp

−ı
√
4λ −1
2
ln
x + 1
2

.
is also a set of solutions. The new factor of 2 in the logarithm just multiplies the solutions by a constant. We write
the solution in terms of the cosine and sine.
√
x + 1 cos
√
4λ −1
2
ln
x + 1
2

,
√
x + 1 sin
√
4λ −1
2
ln
x + 1
2

.
The solution of the diﬀerential equation which satisﬁes the boundary condition y(1) = 0 is
y = c
√
x + 1 sin
√
1 −4λ
2
ln
x + 1
2

.
Now we use the second boundary condition to ﬁnd the eigenvalues.
y(2) = 0
sin
√
4λ −1
2
ln
3
2

= 0
√
4λ −1
2
ln
3
2

= nπ,
n ∈Z
λ = 1
4
 
1 +
 2nπ
ln(3/2)
2!
,
n ∈Z
1428

n = 0 gives us a trivial solution, so we discard it. Discarding duplicate solutions, The eigenvalues and eigenfunctions
are
λn = 1
4 +

nπ
ln(3/2)
2
,
yn =
√
x + 1 sin

nπln((x + 1)/2)
ln(3/2)

,
n ∈Z+.
Now we verify that the eigenvalues and eigenfunctions satisfy the properties of regular Sturm-Liouville problems.
• The eigenvalues are real.
• There are an inﬁnite number of eigenvalues
λ1 < λ2 < λ3 < · · ·
1
4 +

π
ln(3/2)
2
< 1
4 +

2π
ln(3/2)
2
< 1
4 +

3π
ln(3/2)
2
< · · ·
There is a least least eigenvalue
λ1 = 1
4 +

π
ln(3/2)
2
,
but there is no greatest eigenvalue.
• The eigenfunctions are orthogonal with respect to the weighting function σ(x) = 1/(x + 1)2. Let n ̸= m.
Z 2
1
yn(x)ym(x)σ(x) dx
=
Z 2
1
√
x + 1 sin

nπln((x + 1)/2)
ln(3/2)
 √
x + 1 sin

mπln((x + 1)/2)
ln(3/2)

1
(x + 1)2 dx
=
Z π
0
sin(nx) sin(mx)ln(3/2)
π
dx
= ln(3/2)
2π
Z π
0
(cos((n −m)x) −cos((n + m)x)) dx
= 0
1429

• The eigenfunctions are complete. A function f(x) deﬁned on (1 . . . 2) has the series representation
f(x) ∼
∞
X
n=1
cnyn(x) =
∞
X
n=1
cn
√
x + 1 sin

nπln((x + 1)/2)
ln(3/2)

,
where
cn = ⟨yn|1/(x + 1)2|f⟩
⟨yn|1/(x + 1)2|yn⟩=
2
ln(3/2)
Z 2
1
sin

nπln((x + 1)/2)
ln(3/2)

1
(x + 1)3/2f(x) dx
Solution 29.3
Recall that Euler equations are scale invariant. If u(x) is a solution, then so is u(cx) for any nonzero constant c.
We substitute y = xγ into the Euler equation.
y′′ + 2α + 1
x
y′ + λ
x2y = 0
γ(γ −1) + (2α + 1)γ + λ = 0
γ2 + 2αγ + λ = 0
γ = −α ±
√
α2 −λ
First we consider the case λ = α2. A set of solutions of the diﬀerential equation is

x−α, x−α ln x
	
The homogeneous solution that satisﬁes the left boundary condition y(a) = 0 is
y = cx−α(ln x −ln a) = cx−α ln
x
a

.
Since only the trivial solution with c = 0 satisﬁes the right boundary condition, λ = α2 is not an eigenvalue.
Next we consider the case λ ̸= α2. We write
γ = −α ± ı
√
λ −α2.
1430

Note that ℜ(
√
λ −α2) ≥0. A set of solutions of the diﬀerential equation is
n
x−α±ı
√
λ−α2o
n
x−α e±ı
√
λ−α2 ln xo
.
By taking the sum and diﬀerence of these solutions we obtain a new set of linearly independent solutions.
n
x−α cos
√
λ −α2 ln x

, x−α sin
√
λ −α2 ln x

,
o
The solution which satisﬁes the left boundary condition is
y = cx−α sin
√
λ −α2 ln
x
a

.
For nontrivial solutions, the right boundary condition y(b) = 0 imposes the constraint
b−α sin
√
λ −α2 ln
b
a

√
λ −α2 ln
b
a

= nπ,
n ∈Z
We have the eigenvalues
λn = α2 +

nπ
ln(b/a)
2
,
n ∈Z
with the eigenfunctions
φn = x−α sin

nπln(x/a)
ln(b/a)

.
To write the problem in Sturm-Liouville form, we multiply by the integrating factor
e
R
(2α+1)/x dx = e(2α+1) ln x = x2α+1.
1431

 x2α+1y′′ + λx2α−1y = 0,
y(a) = y(b) = 0
Now we verify that the Sturm-Liouville properties are satisﬁed.
• The eigenvalues
λn = α2 +

nπ
ln(b/a)
2
,
n ∈Z
are real.
• There are an inﬁnite number of eigenvalues
λ1 < λ2 < λ3 < · · · ,
α2 +

π
ln(b/a)
2
< α2 +

2π
ln(b/a)
2
< α2 +

3π
ln(b/a)
2
< · · ·
There is a least eigenvalue
λ1 = α2 +

π
ln(b/a)
2
,
but there is no greatest eigenvalue, (λn →∞as n →∞).
• For each eigenvalue, we found one unique, (to within a multiplicative constant), eigenfunction φn. We were able
to choose the eigenfunctions to be real-valued. The eigenfunction
φn = x−α sin

nπln(x/a)
ln(b/a)

.
has exactly n −1 zeros in the open interval a < x < b.
1432

• The eigenfunctions are orthogonal with respect to the weighting function σ(x) = x2α−1.
Z b
a
φn(x)φm(x)σ(x) dx =
Z b
a
x−α sin

nπln(x/a)
ln(b/a)

x−α sin

mπln(x/a)
ln(b/a)

x2α−1 dx
=
Z b
a
sin

nπln(x/a)
ln(b/a)

sin

mπln(x/a)
ln(b/a)
 1
x dx
= ln(b/a)
π
Z π
0
sin(nx) sin(mx) dx
= ln(b/a)
2π
Z π
0
(cos((n −m)x) −cos((n + m)x)) dx
= 0
if n ̸= m
• The eigenfunctions are complete. Any piecewise continuous function f(x) deﬁned on a ≤x ≤b can be expanded
in a series of eigenfunctions
f(x) ∼
∞
X
n=1
cnφn(x),
where
cn =
R b
a f(x)φn(x)σ(x) dx
R b
a φ2
n(x)σ(x) dx
.
The sum converges to 1
2(f(x−) + f(x+)). (We do not prove this property.)
1433

• The eigenvalues can be related to the eigenfunctions with the Rayleigh quotient.
λn =

−pφn
dφn
dx
b
a +
R b
a

p
  dφn
dx
2 −qφ2
n

dx
R b
a φ2
nσ dx
=
R b
a

x2α+1 
x−α−1 
nπ
ln(b/a) cos

nπ ln(x/a)
ln(b/a)

−α sin

nπ ln(x/a)
ln(b/a)
2
dx
R b
a

x−α sin

nπ ln(x/a)
ln(b/a)
2
x2α−1 dx
=
R b
a

nπ
ln(b/a)
2
cos2 (·) −2α
nπ
ln(b/a) cos (·) sin (·) + α2 sin2 (·)

x−1 dx
R b
a sin2 
nπ ln(x/a)
ln(b/a)

x−1 dx
=
R π
0

nπ
ln(b/a)
2
cos2(x) −2α
nπ
ln(b/a) cos(x) sin(x) + α2 sin2(x)

dx
R π
0 sin2(x) dx
= α2 +

nπ
ln(b/a)
2
Now we expand a function f(x) in a series of the eigenfunctions.
f(x) ∼
∞
X
n=1
cnx−α sin

nπln(x/a)
ln(b/a)

,
where
cn =
R b
a f(x)φn(x)σ(x) dx
R b
a φ2
n(x)σ(x) dx
=
2n
ln(b/a)
Z b
a
f(x)xα−1 sin

nπln(x/a)
ln(b/a)

dx
1434

Solution 29.4
y′′ −y′ + λy = 0,
y(0) = y(1) = 0.
The factor that will put this equation in Sturm-Liouville form is
F(x) = exp
Z x
−1 dx

= e−x .
The diﬀerential equation becomes
d
dx
 e−x y′
+ λ e−x y = 0.
Thus we see that the eigenfunctions will be orthogonal with respect to the weighting function σ = e−x.
Substituting y = eαx into the diﬀerential equation yields
α2 −α + λ = 0
α = 1 ±
√
1 −4λ
2
α = 1
2 ±
p
1/4 −λ.
If λ < 1/4 then the solutions to the diﬀerential equation are exponential and only the trivial solution satisﬁes the
boundary conditions.
If λ = 1/4 then the solution is y = c1 ex/2 +c2x ex/2 and again only the trivial solution satisﬁes the boundary
conditions.
Now consider the case that λ > 1/4.
α = 1
2 ± ı
p
λ −1/4
The solutions are
ex/2 cos(
p
λ −1/4 x),
ex/2 sin(
p
λ −1/4 x).
The left boundary condition gives us
y = c ex/2 sin(
p
λ −1/4 x).
1435

The right boundary condition demands that
p
λ −1/4 = nπ,
n = 1, 2, . . .
Thus we see that the eigenvalues and eigenfunctions are
λn = 1
4 + (nπ)2,
yn = ex/2 sin(nπx).
If f(x) is a piecewise continuous function then we can expand it in a series of the eigenfunctions.
f(x) =
∞
X
n=1
an ex/2 sin(nπx)
The coeﬃcients are
an =
R 1
0 f(x) e−x ex/2 sin(nπx) dx
R 1
0 e−x(ex/2 sin(nπx))2 dx
=
R 1
0 f(x) e−x/2 sin(nπx) dx
R 1
0 sin2(nπx) dx
= 2
Z 1
0
f(x) e−x/2 sin(nπx) dx.
Solution 29.5
Consider the eigenvalue problem
y′′ + λy = 0
y(0) = 0
y(1) + y′(1) = 0.
Since this is a Sturm-Liouville problem, there are only real eigenvalues. By the Rayleigh quotient, the eigenvalues are
λ =
−φdφ
dx

1
0 +
R 1
0
  dφ
dx
2
dx
R 1
0 φ2 dx
,
1436

λ =
φ2(1) +
R 1
0
  dφ
dx
2
dx
R 1
0 φ2 dx
.
This demonstrates that there are only positive eigenvalues. The general solution of the diﬀerential equation for positive,
real λ is
y = c1 cos
√
λx

+ c2 sin
√
λx

.
The solution that satisﬁes the left boundary condition is
y = c sin
√
λx

.
For nontrivial solutions we must have
sin
√
λ

+
√
λ cos
√
λ

= 0
√
λ = −tan
√
λ

.
The positive solutions of this equation are eigenvalues with corresponding eigenfunctions sin
√
λx

. In Figure 29.1
we plot the functions x and −tan(x) and draw vertical lines at x = (n −1/2)π, n ∈N.
From this we see that there are an inﬁnite number of eigenvalues, λ1 < λ2 < λ3 < · · · . In the limit as n →∞,
λn →(n −1/2)π. The limit is approached from above.
Now consider the eigenvalue problem
y′′ + y = µy
y(0) = 0
y(1) + y′(1) = 0.
From above we see that the eigenvalues satisfy
p
1 −µ = −tan
p
1 −µ

and that there are an inﬁnite number of eigenvalues. For large n, µn ≈1 −(n −1/2)π. The eigenfunctions are
φn = sin
p
1 −µnx

.
1437

Figure 29.1: x and −tan(x).
To solve the inhomogeneous problem, we expand the solution and the inhomogeneity in a series of the eigenfunctions.
f =
∞
X
n=1
fnφn,
fn =
R 1
0 f(x)φn(x) dx
R 1
0 φ2
n(x) dx
y =
∞
X
n=1
ynφn
We substitite the expansions into the diﬀerential equation to determine the coeﬃcients.
y′′ + y = f
∞
X
n=1
µnynφn =
∞
X
n=1
fnφn
y =
∞
X
n=1
fn
µn
sin
p
1 −µnx

1438

Solution 29.6
Consider the eigenvalue problem
y′′ + y = µy
y(0) = 0
y(1) + y′(1) = 0.
From Exercise 29.5 we see that the eigenvalues satisfy
p
1 −µ = −tan
p
1 −µ

and that there are an inﬁnite number of eigenvalues. For large n, µn ≈1 −(n −1/2)π. The eigenfunctions are
φn = sin
p
1 −µnx

.
To solve the inhomogeneous problem, we expand the solution and the inhomogeneity in a series of the eigenfunctions.
f =
∞
X
n=1
fnφn,
fn =
R 1
0 f(x)φn(x) dx
R 1
0 φ2
n(x) dx
y =
∞
X
n=1
ynφn
We substitite the expansions into the diﬀerential equation to determine the coeﬃcients.
y′′ + y = f
∞
X
n=1
µnynφn =
∞
X
n=1
fnφn
y =
∞
X
n=1
fn
µn
sin
p
1 −µnx

Solution 29.7
First consider λ = 0. The general solution is
y = c1 + c2x.
1439

y = cx satisﬁes the boundary conditions. Thus λ = 0 is an eigenvalue.
Now consider negative real λ. The general solution is
y = c1 cosh
√
−λx

+ c2 sinh
√
−λx

.
The solution that satisﬁes the left boundary condition is
y = c sinh
√
−λx

.
For nontrivial solutions of the boundary value problem, there must be negative real solutions of
√
−λ −sinh
√
−λ

= 0.
Since x = sinh x has no nonzero real solutions, this equation has no solutions for negative real λ. There are no negative
real eigenvalues.
Finally consider positive real λ. The general solution is
y = c1 cos
√
λx

+ c2 sin
√
λx

.
The solution that satisﬁes the left boundary condition is
y = c sin
√
λx

.
For nontrivial solutions of the boundary value problem, there must be positive real solutions of
√
λ −sin
√
λ

= 0.
Since x = sin x has no nonzero real solutions, this equation has no solutions for positive real λ. There are no positive
real eigenvalues.
There is only one real eigenvalue, λ = 0, with corresponding eigenfunction φ = x.
1440

The diﬃculty with the boundary conditions, y(0) = 0, y′(0) −y(1) = 0 is that the problem is not self-adjoint. We
demonstrate this by showing that the problem does not satisfy Green’s identity. Let u and v be two functions that
satisfy the boundary conditions, but not necessarily the diﬀerential equation.
⟨u, L[v]⟩−⟨L[u], v⟩= ⟨u, v′′⟩−⟨u′′, v⟩
= [uv′]1
0 −⟨u′, v′⟩−⟨u′, v′⟩−[u′v]1
0 + ⟨u′, v′⟩−⟨u′, v′⟩
= u(1)v′(1) −u′(1)v(1)
Green’s identity is not satisﬁed,
⟨u, L[v]⟩−⟨L[u], v⟩̸= 0;
The problem is not self-adjoint.
Solution 29.8
First we write the equation in formally self-adjoint form,
L[y] ≡(xy′)′ = −λxy,
|y(0)| < ∞,
y(1) = 0.
Let λ be an eigenvalue with corresponding eigenfunction φ. We derive the Rayleigh quotient for λ.
⟨φ, L[φ]⟩= ⟨φ, −λxφ⟩
⟨φ, (xφ′)′⟩= −λ⟨φ, xφ⟩
[φxφ′]1
0 −⟨φ′, xφ′⟩= −λ⟨φ, xφ⟩
We apply the boundary conditions and solve for λ.
λ = ⟨φ′, xφ′⟩
⟨φ, xφ⟩
The Bessel equation of the ﬁrst kind and order zero satisﬁes the problem,
y′′ + 1
xy′ + y = 0,
|y(0)| < ∞,
y(r) = 0,
1441

where r is a positive root of J0(x). We make the change of variables ξ = x/r, u(ξ) = y(x) to obtain the problem
1
r2u′′ + 1
rξ
1
ru′ + u = 0,
|u(0)| < ∞,
u(1) = 0,
u′′ + 1
ξ u′ + r2u = 0,
|u(0)| < ∞,
u(1) = 0.
Now r2 is the eigenvalue of the problem for u(ξ). From the Rayleigh quotient, the minimum eigenvalue obeys the
inequality
r2 ≤⟨φ′, xφ′⟩
⟨φ, xφ⟩,
where φ is any test function that satisﬁes the boundary conditions. Taking φ = 1 −x we obtain,
r2 ≤
R 1
0 (−1)x(−1) dx
R 1
0 (1 −x)x(1 −x) dx
= 6,
r ≤
√
6
Thus the smallest zero of J0(x) is less than or equal to
√
6 ≈2.4494. (The smallest zero of J0(x) is approximately
2.40483.)
Solution 29.9
We assume that 0 < l < π.
Recall that the solution of a second order diﬀerential equation with piecewise continuous coeﬃcient functions is
piecewise C2. This means that the solution is C2 except for a ﬁnite number of points where it is C1.
First consider the case λ = 0. A set of linearly independent solutions of the diﬀerential equation is {1, z}. The
solution which satisﬁes y(0) = 0 is y1 = c1z. The solution which satisﬁes y(π) = 0 is y2 = c2(π −z). There is a
solution for the problem if there are there are values of c1 and c2 such that y1 and y2 have the same position and slope
at z = l.
y1(l) = y2(l),
y′
1(l) = y′
2(l)
c1l = c2(π −l),
c1 = −c2
1442

Since there is only the trivial solution, c1 = c2 = 0, λ = 0 is not an eigenvalue.
Now consider λ ̸= 0. For 0 ≤z ≤l a set of linearly independent solutions is
n
cos(
√
aλz), sin(
√
aλz)
o
.
The solution which satisﬁes y(0) = 0 is
y1 = c1 sin(
√
aλz).
For l < z ≤π a set of linearly independent solutions is
n
cos(
√
bλz), sin(
√
bλz)
o
.
The solution which satisﬁes y(π) = 0 is
y2 = c2 sin(
√
bλ(π −z)).
λ ̸= 0 is an eigenvalue if there are nontrivial solutions of
y1(l) = y2(l),
y′
1(l) = y′
2(l)
c1 sin(
√
aλl) = c2 sin(
√
bλ(π −l)),
c1
√
aλ cos(
√
aλl) = −c2
√
bλ cos(
√
bλ(π −l))
We divide the second equation by
p
(λ) since λ ̸= 0 and write this as a linear algebra problem.

sin(
√
aλl)
−sin(
√
bλ(π −l))
√a cos(
√
aλl)
√
b sin(
√
bλ(π −l))
 c1
c2

=
0
0

This system of equations has nontrivial solutions if and only if the determinant of the matrix is zero.
√
b sin(
√
aλl) sin(
√
bλ(π −l)) + √a cos(
√
aλl) sin(
√
bλ(π −l)) = 0
We can use trigonometric identities to write this equation as
(
√
b −√a) sin
√
λ(l√a −(π −l)
√
b)

+ (
√
b + √a) sin
√
λ(l√a + (π −l)
√
b)

= 0
1443

Clearly this equation has an inﬁnite number of solutions for real, positive λ. However, it is not clear that this equation
does not have non-real solutions. In order to prove that, we will show that the problem is self-adjoint. Before going on
to that we note that the eigenfunctions have the form
φn(z) =
(
sin
 √aλnz

0 ≤z ≤l
sin
 √bλn(π −z)

l < z ≤π.
Now we prove that the problem is self-adjoint. We consider the class of functions which are C2 in (0 . . . π) except
at the interior point x = l where they are C1 and which satisfy the boundary conditions y(0) = y(π) = 0. Note that
the diﬀerential operator is not deﬁned at the point x = l. Thus Green’s identity,
⟨u|q|Lv⟩= ⟨Lu|q|v⟩
is not well-deﬁned. To remedy this we must deﬁne a new inner product. We choose
⟨u|v⟩≡
Z l
0
uv dx +
Z π
l
uv dx.
This new inner product does not require diﬀerentiability at the point x = l.
The problem is self-adjoint if Green’s indentity is satisﬁed. Let u and v be elements of our class of functions. In
1444

addition to the boundary conditions, we will use the fact that u and v satisfy y(l−) = y(l+) and y′(l−) = y′(l+).
⟨v|Lu⟩=
Z l
0
vu′′ dx +
Z π
l
vu′′ dx
= [vu′]l
0 −
Z l
0
v′u′ dx + [vu′]π
l −
Z π
l
v′u′ dx
= v(l)u′(l) −
Z l
0
v′u′ dx −v(l)u′(l) −
Z π
l
v′u′ dx
= −
Z l
0
v′u′ dx −
Z π
l
v′u′ dx
= −[v′u]l
0 +
Z l
0
v′′u dx −[v′u]π
l +
Z π
l
v′′u dx
= −v′(l)u(l) +
Z l
0
v′′u dx + v′(l)u(l) +
Z π
l
v′′u dx
=
Z l
0
v′′u dx +
Z π
l
v′′u dx
= ⟨Lv|Lu⟩
The problem is self-adjoint. Hence the eigenvalues are real. There are an inﬁnite number of positive, real eigenvalues
λn.
Solution 29.10
1. Let v be an eigenfunction with the eigenvalue λ. We start with the diﬀerential equation and then take the inner
product with v.
(pv′′)′′ −(qv′)′ + rv = λsv
⟨v, (pv′′)′′ −(qv′)′ + rv⟩= ⟨v, λsv⟩
1445

We use integration by parts and utilize the homogeneous boundary conditions.
[v(pv′′)′]b
a −⟨v′, (pv′′)′⟩−[vqv′]b
a + ⟨v′, qv′⟩+ ⟨v, rv⟩= λ⟨v, sv⟩
−[v′pv′′]b
a + ⟨v′′, pv′′⟩+ ⟨v′, qv′⟩+ ⟨v, rv⟩= λ⟨v, sv⟩
λ = ⟨v′′, pv′′⟩+ ⟨v′, qv′⟩+ ⟨v, rv⟩
⟨v, sv⟩
We see that if p, q, r, s ≥0 then the eigenvalues will be positive. (Of course we assume that p and s are not
identically zero.)
2. First we prove that this problem is self-adjoint. Let u and v be functions that satisfy the boundary conditions,
but do not necessarily satsify the diﬀerential equation.
⟨v, L[u]⟩−⟨L[v], u⟩= ⟨v, (pu′′)′′ −(qu′)′ + ru⟩−⟨(pv′′)′′ −(qv′)′ + rv, u⟩
Following our work in part (a) we use integration by parts to move the derivatives.
= (⟨v′′, pu′′⟩+ ⟨v′, qu′⟩+ ⟨v, ru⟩) −(⟨pv′′, u′′⟩+ ⟨qv′, u′⟩+ ⟨rv, u⟩)
= 0
This problem satisﬁes Green’s identity,
⟨v, L[u]⟩−⟨L[v], u⟩= 0,
and is thus self-adjoint.
Let vk and vm be eigenfunctions corresponding to the distinct eigenvalues λk and λm. We start with Green’s
identity.
⟨vk, L[vm]⟩−⟨L[vk], vm⟩= 0
⟨vk, λmsvm⟩−⟨λksvk, vm⟩= 0
(λm −λk)⟨vk, svm⟩= 0
⟨vk, svm⟩= 0
The eigenfunctions are orthogonal with respect to the weighting function s.
1446

3. From part (a) we know that there are only positive eigenvalues. The general solution of the diﬀerential equation
is
φ = c1 cos(λ1/4x) + c2 cosh(λ1/4x) + c3 sin(λ1/4x) + c4 sinh(λ1/4x).
Applying the condition φ(0) = 0 we obtain
φ = c1(cos(λ1/4x) −cosh(λ1/4x)) + c2 sin(λ1/4x) + c3 sinh(λ1/4x).
The condition φ′′(0) = 0 reduces this to
φ = c1 sin(λ1/4x) + c2 sinh(λ1/4x).
We substitute the solution into the two right boundary conditions.
c1 sin(λ1/4) + c2 sinh(λ1/4) = 0
−c1λ1/2 sin(λ1/4) + c2λ1/2 sinh(λ1/4) = 0
We see that sin(λ1/4) = 0. The eigenvalues and eigenfunctions are
λn = (nπ)4,
φn = sin(nπx),
n ∈N.
1447

Chapter 30
Integrals and Convergence
Never try to teach a pig to sing. It wastes your time and annoys the pig.
-?
30.1
Uniform Convergence of Integrals
Consider the improper integral
Z ∞
c
f(x, t) dt.
The integral is convergent to S(x) if, given any ϵ > 0, there exists T(x, ϵ) such that

Z τ
c
f(x, t) dt −S(x)
 < ϵ
for all τ > T(x, ϵ).
The sum is uniformly convergent if T is independent of x.
Similar to the Weierstrass M-test for inﬁnite sums we have a uniform convergence test for integrals. If there exists
a continuous function M(t) such that |f(x, t)| ≤M(t) and
R ∞
c
M(t) dt is convergent, then
R ∞
c
f(x, t) dt is uniformly
convergent.
1448

If
R ∞
c
f(x, t) dt is uniformly convergent, we have the following properties:
• If f(x, t) is continuous for x ∈[a, b] and t ∈[c, ∞) then for a < x0 < b,
lim
x→x0
Z ∞
c
f(x, t) dt =
Z ∞
c

lim
x→x0 f(x, t)

dt.
• If a ≤x1 < x2 ≤b then we can interchange the order of integration.
Z x2
x1
Z ∞
c
f(x, t) dt

dx =
Z ∞
c
Z x2
x1
f(x, t) dx

dt
• If ∂f
∂x is continuous, then
d
dx
Z ∞
c
f(x, t) dt =
Z ∞
c
∂
∂xf(x, t) dt.
30.2
The Riemann-Lebesgue Lemma
Result 30.2.1 If
R b
a |f(x)| dx exists, then
Z b
a
f(x) sin(λx) dx →0 as λ →∞.
Before we try to justify the Riemann-Lebesgue lemma, we will need a preliminary result. Let λ be a positive constant.

Z b
a
sin(λx) dx
 =


−1
λ cos(λx)
b
a

≤2
λ.
1449

We will prove the Riemann-Lebesgue lemma for the case when f(x) has limited total ﬂuctuation on the interval
(a, b). We can express f(x) as the diﬀerence of two functions
f(x) = ψ+(x) −ψ−(x),
where ψ+ and ψ−are positive, increasing, bounded functions.
From the mean value theorem for positive, increasing functions, there exists an x0, a ≤x0 ≤b, such that

Z b
a
ψ+(x) sin(λx) dx
 =
ψ+(b)
Z b
x0
sin(λx) dx

≤|ψ+(b)|2
λ.
Similarly,

Z b
a
ψ−(x) sin(λx) dx
 ≤|ψ−(b)|2
λ.
Thus

Z b
a
f(x) sin(λx) dx
 ≤2
λ(|ψ+(b)| + |ψ−(b)|)
→0
as λ →∞.
30.3
Cauchy Principal Value
30.3.1
Integrals on an Inﬁnite Domain
The improper integral
R ∞
−∞f(x) dx is deﬁned
Z ∞
−∞
f(x) dx = lim
a→−∞
Z 0
a
f(x) dx + lim
b→∞
Z b
0
f(x) dx,
1450

when these limits exist. The Cauchy principal value of the integral is deﬁned
PV
Z ∞
−∞
f(x) dx = lim
a→∞
Z a
−a
f(x) dx.
The principal value may exist when the integral diverges.
Example 30.3.1
R ∞
−∞x dx diverges, but
PV
Z ∞
−∞
x dx = lim
a→∞
Z a
−a
x dx = lim
a→∞(0) = 0.
If the improper integral converges, then the Cauchy principal value exists and is equal to the value of the integral.
The principal value of the integral of an odd function is zero. If the principal value of the integral of an even function
exists, then the integral converges.
30.3.2
Singular Functions
Let f(x) have a singularity at x = 0. Let a and b satisfy a < 0 < b. The integral of f(x) is deﬁned
Z b
a
f(x) dx = lim
ϵ1→0−
Z ϵ1
a
f(x) dx + lim
ϵ2→0+
Z b
ϵ2
f(x) dx,
when the limits exist. The Cauchy principal value of the integral is deﬁned
PV
Z b
a
f(x) dx = lim
ϵ→0+
Z −ϵ
a
f(x) dx +
Z b
ϵ
f(x) dx

,
when the limit exists.
Example 30.3.2 The integral
Z 2
−1
1
x dx
1451

diverges, but the principal value exists.
PV
Z 2
−1
1
x dx = lim
ϵ→0+
Z −ϵ
−1
1
x dx +
Z 2
ϵ
1
x dx

= lim
ϵ→0+

−
Z 1
ϵ
1
x dx +
Z 2
ϵ
1
x dx

=
Z 2
1
1
x dx
= log 2
1452

Chapter 31
The Laplace Transform
31.1
The Laplace Transform
The Laplace transform of the function f(t) is deﬁned
L[f(t)] =
Z ∞
0
e−st f(t) dt,
for all values of s for which the integral exists. The Laplace transform of f(t) is a function of s which we will denote
ˆf(s). 1
A function f(t) is of exponential order α if there exist constants t0 and M such that
|f(t)| < M eαt,
for all t > t0.
If
R t0
0 f(t) dt exists and f(t) is of exponential order α then the Laplace transform ˆf(s) exists for ℜ(s) > α.
Here are a few examples of these concepts.
• sin t is of exponential order 0.
1Denoting the Laplace transform of f(t) as F(s) is also common.
1453

• t e2t is of exponential order α for any α > 2.
• et2 is not of exponential order α for any α.
• tn is of exponential order α for any α > 0.
• t−2 does not have a Laplace transform as the integral diverges.
Example 31.1.1 Consider the Laplace transform of f(t) = 1. Since f(t) = 1 is of exponential order α for any α > 0,
the Laplace transform integral converges for ℜ(s) > 0.
ˆf(s) =
Z ∞
0
e−st dt
=

−1
s e−st
∞
0
= 1
s
Example 31.1.2 The function f(t) = t et is of exponential order α for any α > 1. We compute the Laplace transform
of this function.
ˆf(s) =
Z ∞
0
e−st t et dt
=
Z ∞
0
t e(1−s)t dt
=

1
1 −st e(1−s)t
∞
0
−
Z ∞
0
1
1 −s e(1−s)t dt
= −

1
(1 −s)2 e(1−s)t
∞
0
=
1
(1 −s)2
for ℜ(s) > 1.
1454

Example 31.1.3 Consider the Laplace transform of the Heaviside function,
H(t −c) =
(
0
for t < c
1
for t > c,
where c > 0.
L[H(t −c)] =
Z ∞
0
e−st H(t −c) dt
=
Z ∞
c
e−st dt
=
e−st
−s
∞
c
= e−cs
s
for ℜ(s) > 0
Example 31.1.4 Next consider H(t −c)f(t −c).
L[H(t −c)f(t −c)] =
Z ∞
0
e−st H(t −c)f(t −c) dt
=
Z ∞
c
e−st f(t −c) dt
=
Z ∞
0
e−s(t+c) f(t) dt
= e−cs ˆf(s)
31.2
The Inverse Laplace Transform
The inverse Laplace transform in denoted
f(t) = L−1[ ˆf(s)].
1455

We compute the inverse Laplace transform with the Mellin inversion formula.
f(t) =
1
ı2π
Z α+ı∞
α−ı∞
est ˆf(s) ds
Here α is a real constant that is to the right of the singularities of ˆf(s).
To see why the Mellin inversion formula is correct, we take the Laplace transform of it. Assume that f(t) is of
exponential order α. Then α will be to the right of the singularities of ˆf(s).
L[L−1[ ˆf(s)]] = L
 1
ı2π
Z α+ı∞
α−ı∞
ezt ˆf(z) dz

=
Z ∞
0
e−st 1
ı2π
Z α+ı∞
α−ı∞
ezt ˆf(z) dz dt
We interchange the order of integration.
=
1
ı2π
Z α+ı∞
α−ı∞
ˆf(z)
Z ∞
0
e(z−s)t dt dz
Since ℜ(z) = α, the integral in t exists for ℜ(s) > α.
=
1
ı2π
Z α+ı∞
α−ı∞
ˆf(z)
s −z dz
We would like to evaluate this integral by closing the path of integration with a semi-circle of radius R in the right half
plane and applying the residue theorem. However, in order for the integral along the semi-circle to vanish as R →∞,
ˆf(z) must vanish as |z| →∞. If ˆf(z) vanishes we can use the maximum modulus bound to show that the integral
along the semi-circle vanishes. This we assume that ˆf(z) vanishes at inﬁnity.
Consider the integral,
1
ı2π
I
C
ˆf(z)
s −z dz,
1456

Im(z)
α
Re(z)
+iR
α-iR
s
Figure 31.1: The Laplace Transform Pair Contour.
where C is the contour that starts at α −ıR, goes straight up to α + ıR, and then follows a semi-circle back down to
α −ıR. This contour is shown in Figure 31.1.
If s is inside the contour then
1
ı2π
I
C
ˆf(z)
s −z dz = ˆf(s).
Note that the contour is traversed in the negative direction. Since ˆf(z) decays as |z| →∞, the semicircular contribution
to the integral will vanish as R →∞. Thus
1
ı2π
Z α+ı∞
α−ı∞
ˆf(z)
s −z dz = ˆf(s).
Therefore, we have shown than
L[L−1[ ˆf(s)]] = ˆf(s).
f(t) and ˆf(s) are known as Laplace transform pairs.
1457

31.2.1
ˆf(s) with Poles
Example 31.2.1 Consider the inverse Laplace transform of 1/s2. s = 1 is to the right of the singularity of 1/s2.
L−1
 1
s2

=
1
ı2π
Z 1+ı∞
1−ı∞
est 1
s2 ds
Let BR be the contour starting at 1 −ıR and following a straight line to 1 + ıR; let CR be the contour starting at
1 + ıR and following a semicircular path down to 1 −ıR. Let C be the combination of BR and CR. This contour is
shown in Figure 31.2.
Im(s)
α
Re(s)
+iR
α-iR
BR
CR
Figure 31.2: The Path of Integration for the Inverse Laplace Transform.
1458

Consider the line integral on C for R > 1.
1
ı2π
I
C
est 1
s2 ds = Res

est 1
s2, 0

= d
ds est 
s=0
= t
If t ≥0, the integral along CR vanishes as R →∞. We parameterize s.
s = 1 + R eıθ,
π
2 ≤θ ≤3π
2
 est  =
et(1+R eıθ) = et etR cos θ ≤et

Z
CR
est 1
s2 ds
 ≤
Z
CR
est 1
s2
 ds
≤πR et
1
(R −1)2
→0 as R →∞
Thus the inverse Laplace transform of 1/s2 is
L−1
 1
s2

= t,
for t ≥0.
Let ˆf(s) be analytic except for isolated poles at s1, s2, . . . , sN and let α be to the right of these poles. Also, let
ˆf(s) →0 as |s| →∞. Deﬁne BR to be the straight line from α −ıR to α + ıR and CR to be the semicircular path
1459

from α + ıR to α −ıR. If R is large enough to enclose all the poles, then
1
ı2π
I
BR+CR
est ˆf(s) ds =
N
X
n=1
Res(est ˆf(s), sn)
1
ı2π
Z
BR
est ˆf(s) ds =
N
X
n=1
Res(est ˆf(s), sn) −1
ı2π
Z
CR
est ˆf(s) ds.
Now let’s examine the integral along CR. Let the maximum of | ˆf(s)| on CR be MR. We can parameterize the
contour with s = α + R eıθ, π/2 < θ < 3π/2.

Z
CR
est ˆf(s) ds
 =

Z 3π/2
π/2
et(α+R eiθ) ˆf(α + R eıθ)Rı eıθ dθ

≤
Z 3π/2
π/2
eαt etR cos θ RMR dθ
= RMR eαt
Z π
0
e−tR sin θ dθ
If t ≥0 we can use Jordan’s Lemma to obtain,
< RMR eαt π
tR.
= MR eαt π
t
We use that MR →0 as R →∞.
→0
as R →∞
1460

Thus we have an expression for the inverse Laplace transform of ˆf(s).
1
ı2π
Z α+ı∞
α−ı∞
est ˆf(s) ds =
N
X
n=1
Res(est ˆf(s), sn)
L−1[ ˆf(s)] =
N
X
n=1
Res(est ˆf(s), sn)
Result 31.2.1 If ˆf(s) is analytic except for poles at s1, s2, . . . , sN and ˆf(s) →0 as |s| →∞
then the inverse Laplace transform of ˆf(s) is
f(t) = L−1[ ˆf(s)] =
N
X
n=1
Res(est ˆf(s), sn),
for t > 0.
Example 31.2.2 Consider the inverse Laplace transform of
1
s3−s2.
First we factor the denominator.
1
s3 −s2 = 1
s2
1
s −1.
Taking the inverse Laplace transform,
L−1

1
s3 −s3

= Res

est 1
s2
1
s −1, 0

+ Res

est 1
s2
1
s −1, 1

= d
ds
est
s −1

s=0
+ et
=
−1
(−1)2 + t
−1 + et
1461

Thus we have that
L−1

1
s3 −s2

= et −t −1,
for t > 0.
Example 31.2.3 Consider the inverse Laplace transform of
s2 + s −1
s3 −2s2 + s −2.
We factor the denominator.
s2 + s −1
(s −2)(s −ı)(s + ı).
Then we take the inverse Laplace transform.
L−1

s2 + s −1
s3 −2s2 + s −2

= Res

est
s2 + s −1
(s −2)(s −ı)(s + ı), 2

+ Res

est
s2 + s −1
(s −2)(s −ı)(s + ı), ı

+ Res

est
s2 + s −1
(s −2)(s −ı)(s + ı), −ı

= e2t + eıt 1
ı2 + e−ıt −1
ı2
Thus we have
L−1

s2 + s −1
s3 −2s2 + s −2

= sin t + e2t,
for t > 0.
1462

31.2.2
ˆf(s) with Branch Points
Example 31.2.4 Consider the inverse Laplace transform of
1
√s. √s denotes the principal branch of s1/2. There is a
branch cut from s = 0 to s = −∞and
1
√s = e−ıθ/2
√r ,
for −π < θ < π.
Let α be any positive number. The inverse Laplace transform of
1
√s is
f(t) =
1
ı2π
Z α+ı∞
α−ı∞
est 1
√s ds.
We will evaluate the integral by deforming it to wrap around the branch cut. Consider the integral on the contour
shown in Figure 31.3. C+
R and C−
R are circular arcs of radius R. B is the vertical line at ℜ(s) = α joining the two arcs.
Cϵ is a semi-circle in the right half plane joining ıϵ and −ıϵ. L+ and L−are lines joining the circular arcs at ℑ(s) = ±ϵ.
Since there are no residues inside the contour, we have
1
ı2π
 Z
B
+
Z
C+
R
+
Z
L+ +
Z
Cϵ
+
Z
L−+
Z
C−
R
!
est 1
√s ds = 0.
We will evaluate the inverse Laplace transform for t > 0.
First we will show that the integral along C+
R vanishes as R →∞.
Z
C+
R
· · · ds =
Z π/2
π/2−δ
· · · dθ +
Z π
π/2
· · · dθ.
The ﬁrst integral vanishes by the maximum modulus bound. Note that the length of the path of integration is less than
1463

π/2−δ
Cε
L+
L-
-
CR
R
C+
B
−π/2+δ
Figure 31.3: Path of Integration for 1/√s
2α.

Z π/2
π/2−δ
· · · dθ
 ≤
 
max
s∈C+
R
est 1
√s

!
(2α)
= eαt
1
√
R
(2α)
→0 as R →∞
1464

The second integral vanishes by Jordan’s Lemma. A parameterization of C+
R is s = R eıθ.

Z π
π/2
eR eıθ t
1
√
R eıθ dθ
 ≤
Z π
π/2
eR eıθ t
1
√
R eıθ
 dθ
≤
1
√
R
Z π
π/2
eR cos(θ)t dθ
≤
1
√
R
Z π/2
0
e−Rt sin(φ) dφ
<
1
√
R
π
2Rt
→0 as R →∞
We could show that the integral along C−
R vanishes by the same method. Now we have
1
ı2π
Z
B
+
Z
L+ +
Z
Cϵ
+
Z
L−

est 1
√s ds = 0.
We can show that the integral along Cϵ vanishes as ϵ →0 with the maximum modulus bound.

Z
Cϵ
est 1
√s ds
 ≤

max
s∈Cϵ
est 1
√s


(πϵ)
< eϵt 1
√ϵπϵ
→0
as ϵ →0
Now we can express the inverse Laplace transform in terms of the integrals along L+ and L−.
f(t) ≡
1
ı2π
Z α+ı∞
α−ı∞
est 1
√s ds = −1
ı2π
Z
L+ est 1
√s ds −1
ı2π
Z
L−est 1
√s ds
1465

On L+, s = r eıπ, ds = eıπ dr = −dr; on L−, s = r e−ıπ, ds = e−ıπ dr = −dr. We can combine the integrals along
the top and bottom of the branch cut.
f(t) = −1
ı2π
Z 0
∞
e−rt −ı
√r(−1) dr −1
ı2π
Z ∞
0
e−rt
ı
√r(−1) dr
=
1
ı2π
Z ∞
0
e−rt ı2
√r dr
We make the change of variables x = rt.
=
1
π
√
t
Z ∞
0
e−x 1
√x dx
We recognize this integral as Γ(1/2).
=
1
π
√
tΓ(1/2)
=
1
√
πt
Thus the inverse Laplace transform of
1
√s is
f(t) =
1
√
πt,
for t > 0.
31.2.3
Asymptotic Behavior of ˆf(s)
Consider the behavior of
ˆf(s) =
Z ∞
0
e−st f(t) dt
1466

as s →+∞. Assume that f(t) is analytic in a neighborhood of t = 0. Only the behavior of the integrand near t = 0
will make a signiﬁcant contribution to the value of the integral. As you move away from t = 0, the e−st term dominates.
Thus we could approximate the value of ˆf(s) by replacing f(t) with the ﬁrst few terms in its Taylor series expansion
about the origin.
ˆf(s) ∼
Z ∞
0
e−st

f(0) + tf ′(0) + t2
2 f ′′(0) + · · ·

dt
as s →+∞
Using
L [tn] =
n!
sn+1
we obtain
ˆf(s) ∼f(0)
s
+ f ′(0)
s2
+ f ′′(0)
s3
+ · · ·
as s →+∞.
Example 31.2.5 The Taylor series expansion of sin t about the origin is
sin t = t −t3
6 + O(t5).
Thus the Laplace transform of sin t has the behavior
L[sin t] ∼1
s2 −1
s4 + O(s−6)
as s →+∞.
We corroborate this by expanding L[sin t].
L[sin t] =
1
s2 + 1
=
s−2
1 + s−2
= s−2
∞
X
n=0
(−1)ns−2n
= 1
s2 −1
s4 + O(s−6)
1467

31.3
Properties of the Laplace Transform
In this section we will list several useful properties of the Laplace transform. If a result is not derived, it is shown in
the Problems section. Unless otherwise stated, assume that f(t) and g(t) are piecewise continuous and of exponential
order α.
• L[af(t) + bg(t)] = aL[f(t)] + bL[g(t)]
• L[ect f(t)] = ˆf(s −c) for s > c + α
• L[tnf(t)] = (−1)n dn
dsn[ ˆf(s)]
for n = 1, 2, . . .
• If
R β
0
f(t)
t dt exists for positive β then
L
f(t)
t

=
Z ∞
s
ˆf(σ) dσ.
• L
hR t
0 f(τ) dτ
i
=
ˆf(s)
s
• L
 d
dtf(t)

= s ˆf(s) −f(0)
L
h
d2
dt2f(t)
i
= s2 ˆf(s) −sf(0) −f ′(0)
To derive these formulas,
L
 d
dtf(t)

=
Z ∞
0
e−st f ′(t) dt
=
 e−st f(t)
∞
0 −
Z ∞
0
−s e−st f(t) dt
= −f(0) + s ˆf(s)
L
 d2
dt2f(t)

= sL[f ′(t)] −f ′(0)
= s2 ˆf(s) −sf(0) −f ′(0)
1468

• Let f(t) and g(t) be continuous. The convolution of f(t) and g(t) is deﬁned
h(t) = (f ∗g) =
Z t
0
f(τ)g(t −τ) dτ =
Z t
0
f(t −τ)g(τ) dτ
The convolution theorem states
ˆh(s) = ˆf(s)ˆg(s).
To show this,
ˆh(s) =
Z ∞
0
e−st
Z t
0
f(τ)g(t −τ) dτ dt
=
Z ∞
0
Z ∞
τ
e−st f(τ)g(t −τ) dt dτ
=
Z ∞
0
e−sτ f(τ)
Z ∞
τ
e−s(t−τ) g(t −τ) dt dτ
=
Z ∞
0
e−sτ f(τ) dτ
Z ∞
0
e−sη g(η) dη
= ˆf(s)ˆg(s)
• If f(t) is periodic with period T then
L[f(t)] =
R T
0 e−st f(t) dt
1 −e−sT
.
Example 31.3.1 Consider the inverse Laplace transform of
1
s3−s2. First we factor the denominator.
1
s3 −s2 = 1
s2
1
s −1
1469

We know the inverse Laplace transforms of each term.
L−1
 1
s2

= t,
L−1

1
s −1

= et
We apply the convolution theorem.
L−1
 1
s2
1
s −1

=
Z t
0
τ et−τ dτ
= et 
−τ e−τt
0 −et
Z t
0
−e−τ dτ
= −t −1 + et
L−1
 1
s2
1
s −1

= et −t −1.
Example 31.3.2 We can ﬁnd the inverse Laplace transform of
s2 + s −1
s3 −2s2 + s −2
with the aid of a table of Laplace transform pairs. We factor the denominator.
s2 + s −1
(s −2)(s −ı)(s + ı)
1470

We expand the function in partial fractions and then invert each term.
s2 + s −1
(s −2)(s −ı)(s + ı) =
1
s −2 −ı/2
s −ı + ı/2
s + ı
s2 + s −1
(s −2)(s −ı)(s + ı) =
1
s −2 +
1
s2 + 1
L−1

1
s −2 +
1
s2 + 1

= e2t + sin t
31.4
Constant Coeﬃcient Diﬀerential Equations
Example 31.4.1 Consider the diﬀerential equation
y′ + y = cos t,
for t > 0,
y(0) = 1.
We take the Laplace transform of this equation.
sˆy(s) −y(0) + ˆy(s) =
s
s2 + 1
ˆy(s) =
s
(s + 1)(s2 + 1) +
1
s + 1
ˆy(s) = 1/2
s + 1 + 1
2
s + 1
s2 + 1
Now we invert ˆy(s).
y(t) = 1
2 e−t +1
2 cos t + 1
2 sin t,
for t > 0
Notice that the initial condition was included when we took the Laplace transform.
1471

One can see from this example that taking the Laplace transform of a constant coeﬃcient diﬀerential equation
reduces the diﬀerential equation for y(t) to an algebraic equation for ˆy(s).
Example 31.4.2 Consider the diﬀerential equation
y′′ + y = cos(2t),
for t > 0,
y(0) = 1, y′(0) = 0.
We take the Laplace transform of this equation.
s2ˆy(s) −sy(0) −y′(0) + ˆy(s) =
s
s2 + 4
ˆy(s) =
s
(s2 + 1)(s2 + 4) +
s
s2 + 1
From the table of Laplace transform pairs we know
L−1

s
s2 + 1

= cos t,
L−1

1
s2 + 4

= 1
2 sin(2t).
We use the convolution theorem to ﬁnd the inverse Laplace transform of ˆy(s).
y(t) =
Z t
0
1
2 sin(2τ) cos(t −τ) dτ + cos t
= 1
4
Z t
0
sin(t + τ) + sin(3τ −t) dτ + cos t
= 1
4

−cos(t + τ) −1
3 cos(3τ −t)
t
0
+ cos t
= 1
4

−cos(2t) + cos t −1
3 cos(2t) + 1
3 cos(t)

+ cos t
= −1
3 cos(2t) + 4
3 cos(t)
1472

Alternatively, we can ﬁnd the inverse Laplace transform of ˆy(s) by ﬁrst ﬁnding its partial fraction expansion.
ˆy(s) =
s/3
s2 + 1 −
s/3
s2 + 4 +
s
s2 + 1
= −s/3
s2 + 4 + 4s/3
s2 + 1
y(t) = −1
3 cos(2t) + 4
3 cos(t)
Example 31.4.3 Consider the initial value problem
y′′ + 5y′ + 2y = 0,
y(0) = 1,
y′(0) = 2.
Without taking a Laplace transform, we know that since
y(t) = 1 + 2t + O(t2)
the Laplace transform has the behavior
ˆy(s) ∼1
s + 2
s2 + O(s−3),
as s →+∞.
31.5
Systems of Constant Coeﬃcient Diﬀerential Equations
The Laplace transform can be used to transform a system of constant coeﬃcient diﬀerential equations into a system
of algebraic equations. This should not be surprising, as a system of diﬀerential equations can be written as a single
diﬀerential equation, and vice versa.
Example 31.5.1 Consider the set of diﬀerential equations
y′
1 = y2
y′
2 = y3
y′
3 = −y3 −y2 −y1 + t3
1473

with the initial conditions
y1(0) = y2(0) = y3(0) = 0.
We take the Laplace transform of this system.
sˆy1 −y1(0) = ˆy2
sˆy2 −y2(0) = ˆy3
sˆy3 −y3(0) = −ˆy3 −ˆy2 −ˆy1 + 6
s4
The ﬁrst two equations can be written as
ˆy1 = ˆy3
s2
ˆy2 = ˆy3
s .
We substitute this into the third equation.
sˆy3 = −ˆy3 −ˆy3
s −ˆy3
s2 + 6
s4
(s3 + s2 + s + 1)ˆy3 = 6
s2
ˆy3 =
6
s2(s3 + s2 + s + 1).
We solve for ˆy1.
ˆy1 =
6
s4(s3 + s2 + s + 1)
ˆy1 = 1
s4 −1
s3 +
1
2(s + 1) +
1 −s
2(s2 + 1)
1474

We then take the inverse Laplace transform of ˆy1.
y1 = t3
6 −t2
2 + 1
2 e−t +1
2 sin t −1
2 cos t.
We can ﬁnd y2 and y3 by diﬀerentiating the expression for y1.
y2 = t2
2 −t −1
2 e−t +1
2 cos t + 1
2 sin t
y3 = t −1 + 1
2 e−t −1
2 sin t + 1
2 cos t
1475

31.6
Exercises
Exercise 31.1
Find the Laplace transform of the following functions:
1. f(t) = eat
2. f(t) = sin(at)
3. f(t) = cos(at)
4. f(t) = sinh(at)
5. f(t) = cosh(at)
6. f(t) = sin(at)
t
7. f(t) =
Z t
0
sin(au)
u
du
8. f(t) =
(
1,
0 ≤t < π
0,
π ≤t < 2π
and f(t + 2π) = f(t) for t > 0. That is, f(t) is periodic for t > 0.
Hint, Solution
Exercise 31.2
Show that L[af(t) + bg(t)] = aL[f(t)] + bL[g(t)].
Hint, Solution
Exercise 31.3
Show that if f(t) is of exponential order α,
L[ect f(t)] = ˆf(s −c) for s > c + α.
1476

Hint, Solution
Exercise 31.4
Show that
L[tnf(t)] = (−1)n dn
dsn[ ˆf(s)]
for n = 1, 2, . . .
Hint, Solution
Exercise 31.5
Show that if
R β
0
f(t)
t dt exists for positive β then
L
f(t)
t

=
Z ∞
s
ˆf(σ) dσ.
Hint, Solution
Exercise 31.6
Show that
L
Z t
0
f(τ) dτ

=
ˆf(s)
s .
Hint, Solution
Exercise 31.7
Show that if f(t) is periodic with period T then
L[f(t)] =
R T
0 e−st f(t) dt
1 −e−sT
.
Hint, Solution
1477

Exercise 31.8
The function f(t) t ≥0, is periodic with period 2T; i.e. f(t + 2T) ≡f(t), and is also odd with period T; i.e.
f(t + T) = −f(t). Further,
Z T
0
f(t) e−st dt = ˆg(s).
Show that the Laplace transform of f(t) is ˆf(s) = ˆg(s)/(1 + e−sT). Find f(t) such that ˆf(s) = s−1 tanh(sT/2).
Hint, Solution
Exercise 31.9
Find the Laplace transform of tν, ν > −1 by two methods.
1. Assume that s is complex-valued. Make the change of variables z = st and use integration in the complex plane.
2. Show that the Laplace transform of tν is an analytic function for ℜ(s) > 0. Assume that s is real-valued. Make
the change of variables x = st and evaluate the integral. Then use analytic continuation to extend the result to
complex-valued s.
Hint, Solution
Exercise 31.10 (mathematica/ode/laplace/laplace.nb)
Show that the Laplace transform of f(t) = ln t is
ˆf(s) = −Log s
s
−γ
s ,
where
γ = −
Z ∞
0
e−t ln t dt.
[ γ = 0.5772 . . . is known as Euler’s constant.]
Hint, Solution
Exercise 31.11
Find the Laplace transform of tν ln t. Write the answer in terms of the digamma function, ψ(ν) = Γ′(ν)/Γ(ν). What
is the answer for ν = 0?
Hint, Solution
1478

Exercise 31.12
Find the inverse Laplace transform of
ˆf(s) =
1
s3 −2s2 + s −2
with the following methods.
1. Expand ˆf(s) using partial fractions and then use the table of Laplace transforms.
2. Factor the denominator into (s −2)(s2 + 1) and then use the convolution theorem.
3. Use Result 31.2.1.
Hint, Solution
Exercise 31.13
Solve the diﬀerential equation
y′′ + ϵy′ + y = sin t,
y(0) = y′(0) = 0,
0 < ϵ ≪1
using the Laplace transform. This equation represents a weakly damped, driven, linear oscillator.
Hint, Solution
Exercise 31.14
Solve the problem,
y′′ −ty′ + y = 0,
y(0) = 0, y′(0) = 1,
with the Laplace transform.
Hint, Solution
Exercise 31.15
Prove the following relation between the inverse Laplace transform and the inverse Fourier transform,
L−1[ ˆf(s)] = 1
2π ect F−1[ ˆf(c + ıω)],
1479

where c is to the right of the singularities of ˆf(s).
Hint, Solution
Exercise 31.16 (mathematica/ode/laplace/laplace.nb)
Show by evaluating the Laplace inversion integral that if
ˆf(s) =
π
s
1/2
e−2(as)1/2,
s1/2 = √s for s > 0,
then f(t) = e−a/t /
√
t. Hint: cut the s-plane along the negative real axis and deform the contour onto the cut.
Remember that
R ∞
0 e−ax2 cos(bx) dx =
p
π/4a e−b2/4a.
Hint, Solution
Exercise 31.17 (mathematica/ode/laplace/laplace.nb)
Use Laplace transforms to solve the initial value problem
d4y
dt4 −y = t,
y(0) = y′(0) = y′′(0) = y′′′(0) = 0.
Hint, Solution
Exercise 31.18 (mathematica/ode/laplace/laplace.nb)
Solve, by Laplace transforms,
dy
dt = sin t +
Z t
0
y(τ) cos(t −τ) dτ,
y(0) = 0.
Hint, Solution
Exercise 31.19 (mathematica/ode/laplace/laplace.nb)
Suppose u(t) satisﬁes the diﬀerence-diﬀerential equation
du
dt + u(t) −u(t −1) = 0,
t ≥0,
1480

and the ‘initial condition’ u(t) = u0(t), −1 ≤t ≤0, where u0(t) is given. Show that the Laplace transform ˆu(s) of
u(t) satisﬁes
ˆu(s) =
u0(0)
1 + s −e−s +
e−s
1 + s −e−s
Z 0
−1
e−st u0(t) dt.
Find u(t), t ≥0, when u0(t) = 1. Check the result.
Hint, Solution
Exercise 31.20
Let the function f(t) be deﬁned by
f(t) =
(
1
0 ≤t < π
0
π ≤t < 2π,
and for all positive values of t so that f(t + 2π) = f(t). That is, f(t) is periodic with period 2π. Find the solution of
the intial value problem
d2y
dt2 −y = f(t);
y(0) = 1,
y′(0) = 0.
Examine the continuity of the solution at t = nπ, where n is a positive integer, and verify that the solution is continuous
and has a continuous derivative at these points.
Hint, Solution
Exercise 31.21
Use Laplace transforms to solve
dy
dt +
Z t
0
y(τ) dτ = e−t,
y(0) = 1.
Hint, Solution
1481

Exercise 31.22
An electric circuit gives rise to the system
Ldi1
dt + Ri1 + q/C = E0
Ldi2
dt + Ri2 −q/C = 0
dq
dt = i1 −i2
with initial conditions
i1(0) = i2(0) = E0
2R,
q(0) = 0.
Solve the system by Laplace transform methods and show that
i1 = E0
2R + E0
2ωL e−αt sin(ωt)
where
α = R
2L
and
ω2 =
2
LC −α2.
Hint, Solution
Exercise 31.23
Solve the initial value problem,
y′′ + 4y′ + 4y = 4 e−t,
y(0) = 2, y′(0) = −3.
Hint, Solution
1482

31.7
Hints
Hint 31.1
Use the diﬀerentiation and integration properties of the Laplace transform where appropriate.
Hint 31.2
Hint 31.3
Hint 31.4
If the integral is uniformly convergent and ∂g
∂s is continuous then
d
ds
Z b
a
g(s, t) dt =
Z b
a
∂
∂sg(s, t) dt
Hint 31.5
Z ∞
s
e−tx dt = 1
x e−sx
Hint 31.6
Use integration by parts.
Hint 31.7
Z ∞
0
e−st f(t) dt =
Z ∞
n=0
(n+1)T
X
nT
e−st f(t) dt
1483

The sum can be put in the form of a geometric series.
∞
X
n=0
αn =
1
1 −α,
for |α| < 1
Hint 31.8
Hint 31.9
Write the answer in terms of the Gamma function.
Hint 31.10
Hint 31.11
Hint 31.12
Hint 31.13
Hint 31.14
Hint 31.15
Hint 31.16
1484

Hint 31.17
Hint 31.18
Hint 31.19
Hint 31.20
Hint 31.21
Hint 31.22
Hint 31.23
1485

31.8
Solutions
Solution 31.1
1.
L
eat
=
Z ∞
0
e−st eat dt
=
Z ∞
0
e−(s−a)t dt
=

−e−(s−a)t
s −a
∞
0
for ℜ(s) > ℜ(a)
L
eat
=
1
s −a
for ℜ(s) > ℜ(a)
2.
L[sin(at)] =
Z ∞
0
e−st sin(at) dt
= 1
ı2
Z ∞
0
 e(−s+ıa)t −e(−s−ıa)t
dt
= 1
ı2
−e(−s+ıa)t
s −ıa
+ e(−s−ıa)t
s + ıa
∞
0
,
for ℜ(s) > 0
= 1
ı2

1
s −ıa −
1
s + ıa

L[sin(at)] =
a
s2 + a2
for ℜ(s) > 0
1486

3.
L[cos(at)] = L
 d
dt
sin(at)
a

= sL
sin(at)
a

−sin(0)
L[cos(at)] =
s
s2 + a2
for ℜ(s) > 0
4.
L[sinh(at)] =
Z ∞
0
e−st sinh(at) dt
= 1
2
Z ∞
0
 e(−s+a)t −e(−s−a)t
dt
= 1
2
−e(−s+a)t
s −a
+ e(−s−a)t
s + a
∞
0
for ℜ(s) > |ℜ(a)|
= 1
2

1
s −a −
1
s + a

L[sinh(at)] =
a
s2 −a2
for ℜ(s) > |ℜ(a)|
5.
L[cosh(at)] = L
 d
dt
sinh(at)
a

= sL
sinh(at)
a

−sinh(0)
L[cosh(at)] =
s
s2 −a2
for ℜ(s) > |ℜ(a)|
1487

6. First note that
L
sin(at)
t

(s) =
Z ∞
s
L[sin(at)](σ) dσ.
Now we use the Laplace transform of sin(at) to compute the Laplace transform of sin(at)/t.
L
sin(at)
t

=
Z ∞
s
a
σ2 + a2 dσ
=
Z ∞
s
1
(σ/a)2 + 1
dσ
a
=
h
arctan
σ
a
i∞
s
= π
2 −arctan
s
a

L
sin(at)
t

= arctan
a
s

7.
L
Z t
0
sin(aτ)
τ
dτ

= 1
sL
sin(at)
t

L
Z t
0
sin(aτ)
τ
dτ

= 1
s arctan
a
s

8.
L[f(t)] =
R 2π
0
e−st f(t) dt
1 −e−2πs
=
R π
0 e−st dt
1 −e−2πs
=
1 −e−πs
s(1 −e−2πs)
1488

L[f(t)] =
1
s(1 + e−πs)
Solution 31.2
L[af(t) + bg(t)] =
Z ∞
0
e−st  af(t) + bg(t)

dt
= a
Z ∞
0
e−st f(t) dt + b
Z ∞
0
e−st g(t) dt
= aL[f(t)] + bL[g(t)]
Solution 31.3
If f(t) is of exponential order α, then ect f(t) is of exponential order c + α.
L[ect f(t)] =
Z ∞
0
e−st ect f(t) dt
=
Z ∞
0
e−(s−c)t f(t) dt
= ˆf(s −c) for s > c + α
Solution 31.4
First consider the Laplace transform of t0f(t).
L[t0f(t)] = ˆf(s)
1489

Now consider the Laplace transform of tnf(t) for n ≥1.
L[tnf(t)] =
Z ∞
0
e−st tnf(t) dt
= −d
ds
Z ∞
0
e−st tn−1f(t) dt
= −d
dsL[tn−1f(t)]
Thus we have a diﬀerence equation for the Laplace transform of tnf(t) with the solution
L[tnf(t)] = (−1)n dn
dsnL[t0f(t)] for n ∈Z0+,
L[tnf(t)] = (−1)n dn
dsn ˆf(s) for n ∈Z0+.
Solution 31.5
If
R β
0
f(t)
t dt exists for positive β and f(t) is of exponential order α then the Laplace transform of f(t)/t is deﬁned for
s > α.
L
f(t)
t

=
Z ∞
0
e−st 1
t f(t) dt
=
Z ∞
0
Z ∞
s
e−σt dσ f(t) dt
=
Z ∞
s
Z ∞
0
e−σtf(t) dt dσ
=
Z ∞
s
ˆf(σ) dσ
1490

Solution 31.6
L
Z t
0
f(τ) dτ

=
Z ∞
0
e−st
Z t
0
f(τ) dτ dx
=

−e−st
s
Z t
0
f(τ) dτ
∞
0
−
Z ∞
0
−e−st
s
d
dt
Z t
0
f(τ) dτ

dt
= 1
s
Z ∞
0
e−st f(t) dt
= 1
s
ˆf(s)
1491

Solution 31.7
f(t) is periodic with period T.
L[f(t)] =
Z ∞
0
e−st f(t) dt
=
Z T
0
e−st f(t) dt +
Z 2T
T
e−st f(t) dt + · · ·
=
∞
X
n=0
Z (n+1)T
nT
e−st f(t) dt
=
∞
X
n=0
Z T
0
e−s(t+nT) f(t + nT) dt
=
∞
X
n=0
e−snT
Z T
0
e−st f(t) dt
=
Z T
0
e−st f(t) dt
∞
X
n=0
e−snT
=
R T
0 e−st f(t) dt
1 −e−sT
1492

Solution 31.8
ˆf(s) =
Z ∞
0
e−st f(t) dt
=
n
X
0
Z (n+1)T
nT
e−st f(t) dt
=
n
X
0
Z T
0
e−s(t+nT) f(t + nT) dt
=
n
X
0
e−snT
Z T
0
e−st(−1)nf(t) dt
=
Z T
0
e−st f(t) dt
n
X
0
(−1)n  e−sTn
ˆf(s) =
ˆg(s)
1 + e−sT ,
for ℜ(s) > 0
Consider ˆf(s) = s−1 tanh(sT/2).
s−1 tanh(sT/2) = s−1esT/2 −e−sT/2
esT/2 + e−sT/2
= s−11 −e−sT
1 + e−sT
We have
ˆg(s) ≡
Z T
0
f(t) e−st dt = 1 −e−st
s
.
1493

By inspection we see that this is satisﬁed for f(t) = 1 for 0 < t < T. We conclude:
f(t) =
(
1
for t ∈[2nT . . . (2n + 1)T),
−1
for t ∈[(2n + 1)T . . . (2n + 2)T),
where n ∈Z.
Solution 31.9
The Laplace transform of tν, ν > −1 is
ˆf(s) =
Z ∞
0
e−st tν dt.
Assume s is complex-valued. The integral converges for ℜ(s) > 0 and ν > −1.
Method 1.
We make the change of variables z = st.
ˆf(s) =
Z
C
e−z z
s
ν 1
s dz
= s−(ν+1)
Z
C
e−z zν dz
C is the path from 0 to ∞along arg(z) = arg(s). (Shown in Figure 31.4).
Since the integrand is analytic in the domain ϵ < r < R, 0 < θ < arg(s), the integral along the boundary of this
domain vanishes.
 Z R
ϵ
+
Z R eı arg(s)
R
+
Z ϵ eı arg(s)
R eı arg(s) +
Z ϵ
ϵ eı arg(s)
!
e−z zν dz = 0
We show that the integral along CR, the circular arc of radius R, vanishes as R →∞with the maximum modulus
1494

Im(z)
Re(z)
arg(s)
Figure 31.4: The Path of Integration.
integral bound.

Z
CR
e−z zν dz
 ≤R| arg(s)| max
z∈CR
e−z zν
= R| arg(s)| e−R cos(arg(s)) Rν
→0
as R →∞.
The integral along Cϵ, the circular arc of radius ϵ, vanishes as ϵ →0. We demonstrate this with the maximum modulus
integral bound.

Z
Cϵ
e−z zν dz
 ≤ϵ| arg(s)| max
z∈Cϵ
e−z zν
= ϵ| arg(s)| e−ϵ cos(arg(s)) ϵν
→0
as ϵ →0.
1495

Taking the limit as ϵ →0 and R →∞, we see that the integral along C is equal to the integral along the real axis.
Z
C
e−z zν dz =
Z ∞
0
e−z zν dz
We can evaluate the Laplace transform of tν in terms of this integral.
L [tν] = s−(ν+1)
Z ∞
0
e−t tν dt
L [tν] = Γ(ν + 1)
sν+1
In the case that ν is a non-negative integer ν = n > −1 we can write this in terms of the factorial.
L [tn] =
n!
sn+1
Method 2.
First note that the integral
ˆf(s) =
Z ∞
0
e−st tν dt
exists for ℜ(s) > 0. It converges uniformly for ℜ(s) ≥c > 0. On this domain of uniform convergence we can
interchange diﬀerentiation and integration.
d ˆf
ds = d
ds
Z ∞
0
e−st tν dt
=
Z ∞
0
∂
∂s
 e−st tν
dt
=
Z ∞
0
−t e−st tν dt
= −
Z ∞
0
e−st tν+1 dt
1496

Since ˆf ′(s) is deﬁned for ℜ(s) > 0, ˆf(s) is analytic for ℜ(s) > 0.
Let σ be real and positive. We make the change of variables x = σt.
ˆf(σ) =
Z ∞
0
e−x x
σ
ν 1
σ dx
= σ−(ν+1)
Z ∞
0
e−x xν dx
= Γ(ν + 1)
σν+1
Note that the function
ˆf(s) = Γ(ν + 1)
sν+1
is the analytic continuation of ˆf(σ). Thus we can deﬁne the Laplace transform for all complex s in the right half plane.
ˆf(s) = Γ(ν + 1)
sν+1
Solution 31.10
Note that ˆf(s) is an analytic function for ℜ(s) > 0. Consider real-valued s > 0. By deﬁnition, ˆf(s) is
ˆf(s) =
Z ∞
0
e−st ln t dt.
We make the change of variables x = st.
ˆf(s) =
Z ∞
0
e−x ln
x
s
 dx
s
= 1
s
Z ∞
0
e−x (ln x −ln s) dx
= −ln |s|
s
Z ∞
0
e−x dx + 1
s
Z ∞
0
e−x ln x dx
= −ln s
s
−γ
s ,
for real s > 0
1497

The analytic continuation of ˆf(s) into the right half-plane is
ˆf(s) = −Log s
s
−γ
s .
Solution 31.11
Deﬁne
ˆf(s) = L[tν ln t] =
Z ∞
0
e−st tν ln t dt.
This integral deﬁnes ˆf(s) for ℜ(s) > 0. Note that the integral converges uniformly for ℜ(s) ≥c > 0. On this domain
we can interchange diﬀerentiation and integration.
ˆf ′(s) =
Z ∞
0
∂
∂s
 e−st tν ln t

dt = −
Z ∞
0
t e−st tν Log t dt
Since ˆf ′(s) also exists for ℜ(s) > 0, ˆf(s) is analytic in that domain.
1498

Let σ be real and positive. We make the change of variables x = σt.
ˆf(σ) = L [tν ln t]
=
Z ∞
0
e−σt tν ln t dt
=
Z ∞
0
e−x x
σ
ν
ln x
σ
1
σ dx
=
1
σν+1
Z ∞
0
e−x xν(ln x −ln σ) dx
=
1
σν+1
Z ∞
0
e−x xν ln x dx −ln σ
Z ∞
0
e−x xν dx

=
1
σν+1
Z ∞
0
∂
∂ν
 e−x xν
dx −ln σΓ(ν + 1)

=
1
σν+1
 d
dν
Z ∞
0
e−x xν dx −ln σΓ(ν + 1)

=
1
σν+1
 d
dν Γ(ν + 1) −ln σΓ(ν + 1)

=
1
σν+1Γ(ν + 1)
Γ′(ν + 1)
Γ(ν + 1) −ln σ

=
1
σν+1Γ(ν + 1) (ψ(ν + 1) −ln σ)
Note that the function
ˆf(s) =
1
sν+1Γ(ν + 1) (ψ(ν + 1) −ln s)
is an analytic continuation of ˆf(σ). Thus we can deﬁne the Laplace transform for all s in the right half plane.
L[tν ln t] =
1
sν+1Γ(ν + 1) (ψ(ν + 1) −ln s)
for ℜ(s) > 0.
1499

For the case ν = 0, we have
L[ln t] = 1
s1Γ(1) (ψ(1) −ln s)
L[ln t] = −γ −ln s
s
,
where γ is Euler’s constant
γ =
Z ∞
0
e−x ln x dx = 0.5772156629 . . .
Solution 31.12
Method 1. We factor the denominator.
ˆf(s) =
1
(s −2)(s2 + 1) =
1
(s −2)(s −ı)(s + ı)
We expand the function in partial fractions and simplify the result.
1
(s −2)(s −ı)(s + ı) = 1/5
s −2 −(1 −ı2)/10
s −ı
−(1 + ı2)/10
s + ı
ˆf(s) = 1
5
1
s −2 −1
5
s + 2
s2 + 1
We use a table of Laplace transforms to do the inversion.
L[e2t] =
1
s −2,
L[cos t] =
s
s2 + 1,
L[sin t] =
1
s2 + 1
f(t) = 1
5
 e2t −cos t −2 sin t

Method 2. We factor the denominator.
ˆf(s) =
1
s −2
1
s2 + 1
1500

From a table of Laplace transforms we note
L[e2t] =
1
s −2,
L[sin t] =
1
s2 + 1.
We apply the convolution theorem.
f(t) =
Z t
0
sin τ e2(t−τ) dτ
f(t) = 1
5
 e2t −cos t −2 sin t

Method 3.
We factor the denominator.
ˆf(s) =
1
(s −2)(s −ı)(s + ı)
ˆf(s) is analytic except for poles and vanishes at inﬁnity.
f(t) =
X
sn=2,ı,−ı
Res

est
(s −2)(s −ı)(s + ı), sn

=
e2t
(2 −ı)(2 + ı) +
eıt
(ı −2)(ı2) +
e−ıt
(−ı −2)(−ı2)
= e2t
5 + (−1 + ı2) eıt
10
+ (−1 −ı2) e−ıt
10
= e2t
5 + −eıt + e−ıt
10
+ ıeıt −e−ıt
5
f(t) = 1
5
 e2t −cos t −2 sin t

1501

Solution 31.13
y′′ + ϵy′ + y = sin t,
y(0) = y′(0) = 0,
0 < ϵ ≪1
We take the Laplace transform of this equation.
(s2ˆy(s) −sy(0) −y′(0)) + ϵ(sˆy(s) −y(0)) + ˆy(s) = L[sin(t)]
(s2 + ϵs + 1)ˆy(s) = L[sin(t)]
ˆy(s) =
1
s2 + ϵs + 1L[sin(t)]
ˆy(s) =
1
(s + ϵ
2)2 + 1 −ϵ2
4
L[sin(t)]
We use a table of Laplace transforms to ﬁnd the inverse Laplace transform of the ﬁrst term.
L−1
"
1
(s + ϵ
2)2 + 1 −ϵ2
4
#
=
1
q
1 −ϵ2
4
e−ϵt/2 sin
 r
1 −ϵ2
4 t
!
We deﬁne
α =
r
1 −ϵ2
4
to get rid of some clutter. Now we apply the convolution theorem to invert 2 ˆys.
y(t) =
Z t
0
1
α e−ϵτ/2 sin (ατ) sin(t −τ) dτ
y(t) = e−ϵt/2
1
ϵ cos (αt) + 1
2α sin (αt)

−1
ϵ cos t
The solution is plotted in Figure 31.5 for ϵ = 0.05.
2Evaluate the convolution integral by inspection.
1502

20
40
60
80
100
-15
-10
-5
5
10
15
Figure 31.5: The Weakly Damped, Driven Oscillator
Solution 31.14
We consider the solutions of
y′′ −ty′ + y = 0,
y(0) = 0, y′(0) = 1
which are of exponential order α for any α > 0. We take the Laplace transform of the diﬀerential equation.
s2ˆy −1 + d
ds (sˆy) + ˆy = 0
ˆy′ +

s + 2
s

ˆy = 1
s
ˆy(s) = 1
s2 + ce−s2/2
s2
1503

We use that
ˆy(s) ∼y(0)
s
+ y′(0)
s2
+ · · ·
to conclude that c = 0.
ˆy(s) = 1
s2
y(t) = t
Solution 31.15
L−1[ ˆf(s)] =
1
ı2π
Z c+ı∞
c−ı∞
est ˆf(s) ds
First we make the change of variable s = c + σ.
L−1[ ˆf(s)] =
1
ı2π ect
Z ı∞
−ı∞
eσt ˆf(c + σ) dσ
Then we make the change of variable σ = ıω.
L−1[ ˆf(s)] = 1
2π ect
Z ∞
−∞
eıωt ˆf(c + ıω) dω
L−1[ ˆf(s)] = 1
2π ect F−1[ ˆf(c + ıω)]
Solution 31.16
We assume that ℜ(a) ≥0. We are considering the principal branch of the square root: s1/2 = √s. There is a branch
cut on the negative real axis. ˆf(s) is singular at s = 0 and along the negative real axis. Let α be any positive number.
The inverse Laplace transform of
  π
s
1/2 e−2(as)1/2 is
f(t) =
1
ı2π
Z α+ı∞
α−ı∞
est π
s
1/2
e−2(as)1/2 ds.
1504

We will evaluate the integral by deforming it to wrap around the branch cut. Consider the integral on the contour
shown in Figure 31.6. C+
R and C−
R are circular arcs of radius R. B is the vertical line at ℜ(s) = α joining the two arcs.
Cϵ is a semi-circle in the right half plane joining ıϵ and −ıϵ. L+ and L−are lines joining the circular arcs at ℑ(s) = ±ϵ.
π/2−δ
Cε
L+
L-
-
CR
R
C+
B
−π/2+δ
Figure 31.6: Path of Integration
Since there are no residues inside the contour, we have
1
ı2π
 Z
B
+
Z
C+
R
+
Z
L+ +
Z
Cϵ
+
Z
L−+
Z
C−
R
!
est π
s
1/2
e−2(as)1/2 ds = 0.
We will evaluate the inverse Laplace transform for t > 0.
First we will show that the integral along C+
R vanishes as R →∞. We parametrize the path of integration with
s = R eıθ and write the integral along C+
R as the sum of two integrals.
Z
C+
R
· · · ds =
Z π/2
π/2−δ
· · · dθ +
Z π
π/2
· · · dθ
1505

The ﬁrst integral vanishes by the maximum modulus bound. Note that the length of the path of integration is less than
2α.

Z π/2
π/2−δ
· · · dθ
 ≤

max
θ∈[π/2−δ...π/2]
est π
s
1/2
e−2(as)1/2


(2α)
= eαt
√π
√
R
(2α)
→0 as R →∞
The second integral vanishes by Jordan’s Lemma.

Z π
π/2
eR eıθ t
√π
√
R eıθ e−2
√
aR eıθ dθ
 ≤
Z π
π/2
eR eıθ t
√π
√
R eıθ e−2√a
√
R eıθ/2
 dθ
≤
√π
√
R
Z π
π/2
eR cos(θ)t dθ
≤
√π
√
R
Z π/2
0
e−Rt sin(φ) dφ
<
√π
√
R
π
2Rt
→0 as R →∞
We could show that the integral along C−
R vanishes by the same method.
Now we have
1
ı2π
Z
B
+
Z
L+ +
Z
Cϵ
+
Z
L−

est π
s
1/2
e−2(as)1/2 ds = 0.
1506

We show that the integral along Cϵ vanishes as ϵ →0 with the maximum modulus bound.

Z
Cϵ
est π
s
1/2
e−2(as)1/2 ds
 ≤

max
s∈Cϵ
est π
s
1/2
e−2(as)1/2


(πϵ)
≤eϵt
√π
√ϵ πϵ
→0
as ϵ →0.
Now we can express the inverse Laplace transform in terms of the integrals along L+ and L−
f(t) ≡
1
ı2π
Z α+ı∞
α−ı∞
est π
s
1/2
e−2(as)1/2 ds
= −1
ı2π
Z
L+ est π
s
1/2
e−2(as)1/2 ds −1
ı2π
Z
L−est π
s
1/2
e−2(as)1/2 ds.
On L+, s = r eıπ, ds = eıπ dr = −dr; on L−, s = r e−ıπ, ds = e−ıπ dr = −dr. We can combine the integrals along
the top and bottom of the branch cut.
f(t) = −1
ı2π
Z 0
∞
e−rt
√π
ı√r e−ı2√a√r(−dr) −1
ı2π
Z ∞
0
e−rt
√π
−ı√r eı2√a√r(−dr)
=
1
2√π
Z ∞
0
e−rt 1
√r

e−ı2√a√r + eı2√a√r
dr
=
1
2√π
Z ∞
0
1
√r e−rt 2 cos
 2√a√r

dr
1507

We make the change of variables x = √r.
=
1
√π
Z ∞
0
1
x e−tx2 cos
 2√ax

2x dx
=
2
√π
Z ∞
0
e−tx2 cos
 2√ax

dx
=
2
√π
r π
4t e−4a/(4t)
= e−a/t
√
t
Thus the inverse Laplace transform is
f(t) = e−a/t
√
t
Solution 31.17
We consider the problem
d4y
dt4 −y = t,
y(0) = y′(0) = y′′(0) = y′′′(0) = 0.
We take the Laplace transform of the diﬀerential equation.
s4ˆy(s) −s3y(0) −s2y′(0) −sy′′(0) −y′′′(0) −ˆy(s) = 1
s2
s4ˆy(s) −ˆy(s) = 1
s2
ˆy(s) =
1
s2(s4 −1)
There are several ways in which we could carry out the inverse Laplace transform to ﬁnd y(t). We could expand the
right side in partial fractions and then use a table of Laplace transforms. Since the function is analytic except for
1508

isolated singularities and vanishes as s →∞we could use the result,
L−1[ ˆf(s)] =
N
X
n=1
Res

est ˆf(s), sn

,
where {sk}n
k=1 are the singularities of ˆf(s). Since we can write the function as a product of simpler terms we could
also apply the convolution theorem.
We will ﬁrst do the inverse Laplace transform by expanding the function in partial fractions to obtain simpler rational
functions.
1
s2(s4 −1) =
1
s2(s −1)(s + 1)(s −ı)(s + ı)
= a
s2 + b
s +
c
s −1 +
d
s + 1 +
e
s −ı +
f
s + ı
a =

1
s4 −1

s=0
= −1
b =
 d
ds
1
s4 −1

s=0
= 0
c =

1
s2(s + 1)(s −ı)(s + ı)

s=1
= 1
4
d =

1
s2(s −1)(s −ı)(s + ı)

s=−1
= −1
4
e =

1
s2(s −1)(s + 1)(s + ı)

s=ı
= −ı1
4
f =

1
s2(s −1)(s + 1)(s −ı)

s=−ı
= ı1
4
1509

Now we have simple functions that we can look up in a table.
ˆy(s) = −1
s2 + 1/4
s −1 −1/4
s + 1 +
1/2
s2 + 1
y(t) =

−t + 1
4 et −1
4 e−t +1
2 sin t

H(t)
y(t) =

−t + 1
2 (sinh t + sin t)

H(t)
We can also do the inversion with the convolution theorem.
1
s2(s4 −1) = 1
s2
1
s2 + 1
1
s2 −1
From a table of Laplace transforms we know,
L−1
 1
s2

= t,
L−1

1
s2 + 1

= sin t,
L−1

1
s2 −1

= sinh t.
Now we use the convolution theorem to ﬁnd the solution for t > 0.
L−1

1
s4 −1

=
Z t
0
sinh(τ) sin(t −τ) dτ
= 1
2 (sinh t −sin t)
L−1

1
s2(s4 −1)

=
Z t
0
1
2 (sinh τ −sin τ) (t −τ) dτ
= −t + 1
2 (sinh t + sin t)
1510

Solution 31.18
dy
dt = sin t +
Z t
0
y(τ) cos(t −τ) dτ
sˆy(s) −y(0) =
1
s2 + 1 + ˆy(s)
s
s2 + 1
(s3 + s)ˆy(s) −sˆy(s) = 1
ˆy(s) = 1
s3
y(t) = t2
2
Solution 31.19
The Laplace transform of u(t −1) is
L[u(t −1)] =
Z ∞
0
e−st u(t −1) dt
=
Z ∞
−1
e−s(t+1) u(t) dt
= e−s
Z 0
−1
e−st u(t) dt + e−s
Z ∞
0
e−st u(t) dt
= e−s
Z 0
−1
e−st u0(t) dt + e−s ˆu(s).
1511

We take the Laplace transform of the diﬀerence-diﬀerential equation.
sˆu(s) −u(0) + ˆu(s) −e−s
Z 0
−1
e−st u0(t) dt + e−s ˆu(s) = 0
(1 + s −e−s)ˆu(s) = u0(0) + e−s
Z 0
−1
e−st u0(t) dt
ˆu(s) =
u0(0)
1 + s −e−s +
e−s
1 + s −e−s
Z 0
−1
e−st u0(t) dt
Consider the case u0(t) = 1.
ˆu(s) =
1
1 + s −e−s +
e−s
1 + s −e−s
Z 0
−1
e−st dt
ˆu(s) =
1
1 + s −e−s +
e−s
1 + s −e−s

−1
s + 1
s es

ˆu(s) = 1/s + 1 −e−s /s
1 + s −e−s
ˆu(s) = 1
s
u(t) = 1
Clearly this solution satisﬁes the diﬀerence-diﬀerential equation.
Solution 31.20
We consider the problem,
d2y
dt2 −y = f(t),
y(0) = 1,
y′(0) = 0,
where f(t) is periodic with period 2π and is deﬁned by,
f(t) =
(
1
0 ≤t < π,
0
π ≤t < 2π.
1512

We take the Laplace transform of the diﬀerential equation.
s2ˆy(s) −sy(0) −y′(0) −ˆy(s) = ˆf(s)
s2ˆy(s) −s −ˆy(s) = ˆf(s)
ˆy(s) =
s
s2 −1 +
ˆf(s)
s2 −1
By inspection, (of a table of Laplace transforms), we see that
L−1

s
s2 −1

= cosh(t)H(t),
L−1

1
s2 −1

= sinh(t)H(t).
Now we use the convolution theorem.
L−1
" ˆf(s)
s2 −1
#
=
Z t
0
f(τ) sinh(t −τ) dτ
The solution for positive t is
y(t) = cosh(t) +
Z t
0
f(τ) sinh(t −τ) dτ.
Clearly the solution is continuous because the integral of a bounded function is continuous. The ﬁrst derivative of the
solution is
y′(t) = sinh t + f(t) sinh(0) +
Z t
0
f(τ) cosh(t −τ) dτ
y′(t) = sinh t +
Z t
0
f(τ) cosh(t −τ) dτ
We see that the ﬁrst derivative is also continuous.
1513

Solution 31.21
We consider the problem
dy
dt +
Z t
0
y(τ) dτ = e−t,
y(0) = 1.
We take the Laplace transform of the equation and solve for ˆy.
sˆy −y(0) + ˆy
s =
1
s + 1
ˆy =
s(s + 2)
(s + 1)(s2 + 1)
We expand the right side in partial fractions.
ˆy = −
1
2(s + 1) +
1 + 3s
2(s2 + 1)
We use a table of Laplace transforms to do the inversion.
y = −1
2 e−t +1
2(sin(t) + 3 cos(t))
Solution 31.22
We consider the problem
Ldi1
dt + Ri1 + q/C = E0
Ldi2
dt + Ri2 −q/C = 0
dq
dt = i1 −i2
i1(0) = i2(0) = E0
2R,
q(0) = 0.
1514

We take the Laplace transform of the system of diﬀerential equations.
L

sˆi1 −E0
2R

+ Rˆi1 + ˆq
C = E0
s
L

sˆi2 −E0
2R

+ Rˆi2 −ˆq
C = 0
sˆq = ˆi1 −ˆi2
We solve for ˆi1, ˆi2 and ˆq.
ˆi1 = E0
2
 1
Rs +
1/L
s2 + Rs/L + 2/(CL)

ˆi2 = E0
2
 1
Rs −
1/L
s2 + Rs/L + 2/(CL)

ˆq = CE0
2
1
s −
s + R/L
s2 + Rs/L + 2/(CL)

We factor the polynomials in the denominators.
ˆi1 = E0
2
 1
Rs +
1/L
(s + α −ıω)(s + α + ıω)

ˆi2 = E0
2
 1
Rs −
1/L
(s + α −ıω)(s + α + ıω)

ˆq = CE0
2
1
s −
s + 2α
(s + α −ıω)(s + α + ıω)

Here we have deﬁned
α = R
2L
and
ω2 =
2
LC −α2.
1515

We expand the functions in partial fractions.
ˆi1 = E0
2
 1
Rs +
ı
2ωL

1
s + α + ıω −
1
s + α −ıω

ˆi2 = E0
2
 1
Rs −
ı
2ωL

1
s + α + ıω −
1
s + α −ıω

ˆq = CE0
2
1
s + ı
2ω

α + ıω
s + α −ıω −
α −ıω
s + α + ıω

Now we can do the inversion with a table of Laplace transforms.
i1 = E0
2
 1
R +
ı
2ωL
 e(−α−ıω)t −e(−α+ıω)t
i2 = E0
2
 1
R −
ı
2ωL
 e(−α−ıω)t −e(−α+ıω)t
q = CE0
2

1 + ı
2ω
 (α + ıω) e(−α+ıω)t −(α −ıω) e(−α−ıω)t
We simplify the expressions to obtain the solutions.
i1 = E0
2
 1
R + 1
ωL e−αt sin(ωt)

i2 = E0
2
 1
R −1
ωL e−αt sin(ωt)

q = CE0
2

1 −e−αt 
cos(ωt) + α
ω sin(ωt)

Solution 31.23
We consider the problem
y′′ + 4y′ + 4y = 4 e−t,
y(0) = 2, y′(0) = −3
1516

We take the Laplace transform of the diﬀerential equation and solve for ˆy(s).
s2ˆy −sy(0) −y′(0) + 4sˆy −4y(0) + 4ˆy =
4
s + 1
s2ˆy −2s + 3 + 4sˆy −8 + 4ˆy =
4
s + 1
ˆy =
4
(s + 1)(s + 2)2 + 2s + 5
(s + 2)2
ˆy =
4
s + 1 −
2
s + 2 −
3
(s + 2)2
We take the inverse Laplace transform to determine the solution.
y = 4 e−t −(2 + 3t) e−2t
1517

Chapter 32
The Fourier Transform
32.1
Derivation from a Fourier Series
Consider the eigenvalue problem
y′′ + λy = 0,
y(−L) = y(L),
y′(−L) = y′(L).
The eigenvalues and eigenfunctions are
λn =
nπ
L
2
for n ∈Z0+
φn = π
L eınπx/L,
for n ∈Z
The eigenfunctions form an orthogonal set. A piecewise continuous function deﬁned on [−L . . . L] can be expanded in
a series of the eigenfunctions.
f(x) ∼
∞
X
n=−∞
cn
π
L eınπx/L
1518

The Fourier coeﬃcients are
cn =
D
π
L eınπx/L f(x)
E
D
π
L eınπx/L
 π
L eınπx/L
E
= 1
2π
Z L
−L
e−ınπx/L f(x) dx.
We substitute the expression for cn into the series for f(x).
f(x) ∼
∞
X
n=−∞
 1
2L
Z L
−L
e−ınπξ/L f(ξ) dξ

eınπx/L .
We let ωn = nπ/L and ∆ω = π/L.
f(x) ∼
∞
X
ωn=−∞
 1
2π
Z L
−L
e−ıωnξ f(ξ) dξ

eıωnx ∆ω.
In the limit as L →∞, (and thus ∆ω →0), the sum becomes an integral.
f(x) ∼
Z ∞
−∞
 1
2π
Z ∞
−∞
e−ıωξ f(ξ) dξ

eıωx dω.
Thus the expansion of f(x) for ﬁnite L
f(x) ∼
∞
X
n=−∞
cn
π
L eınπx/L
cn = 1
2π
Z L
−L
e−ınπx/L f(x) dx
1519

in the limit as L →∞becomes
f(x) ∼
Z ∞
−∞
ˆf(ω) eıωx dω
ˆf(ω) = 1
2π
Z ∞
−∞
f(x) e−ıωx dx.
Of course this derivation is only heuristic. In the next section we will explore these formulas more carefully.
32.2
The Fourier Transform
Let f(x) be piecewise continuous and let
R ∞
−∞|f(x)| dx exist. We deﬁne the function I(x, L).
I(x, L) = 1
2π
Z L
−L
Z ∞
−∞
f(ξ) eıωξ dξ

e−ıωx dω.
Since the integral in parentheses is uniformly convergent, we can interchange the order of integration.
= 1
2π
Z ∞
−∞
Z L
−L
f(ξ) eıω(ξ−x) dω

dξ
= 1
2π
Z ∞
−∞

f(ξ) eıω(ξ−x)
ı(ξ −x)
L
−L
dξ
= 1
2π
Z ∞
−∞
f(ξ)
1
ı(ξ −x)
 eıL(ξ−x) −e−ıL(ξ−x)
dξ
= 1
π
Z ∞
−∞
f(ξ)sin(L(ξ −x))
ξ −x
dξ
= 1
π
Z ∞
−∞
f(ξ + x)sin(Lξ)
ξ
dξ.
1520

In Example 32.3.3 we will show that
Z ∞
0
sin(Lξ)
ξ
dξ = π
2 .
Continuous Functions.
Suppose that f(x) is continuous.
f(x) = 1
π
Z ∞
−∞
f(x)sin(Lξ)
ξ
dξ
I(x, L) −f(x) = 1
π
Z ∞
−∞
f(x + ξ) −f(x)
ξ
sin(Lξ) dξ.
If f(x) has a left and right derivative at x then f(x+ξ)−f(x)
ξ
is bounded and
R ∞
−∞
 f(x+ξ)−f(x)
ξ
 dξ < ∞. We use the
Riemann-Lebesgue lemma to show that the integral vanishes as L →∞.
1
π
Z ∞
−∞
f(x + ξ) −f(x)
ξ
sin(Lξ) dξ →0 as L →∞.
Now we have an identity for f(x).
f(x) = 1
2π
Z ∞
−∞
Z ∞
−∞
f(ξ) eıωξ dξ

e−ıωx dω.
Piecewise Continuous Functions.
Now consider the case that f(x) is only piecewise continuous.
f(x+)
2
= 1
π
Z ∞
0
f(x+)sin(Lξ)
ξ
dξ
f(x−)
2
= 1
π
Z 0
−∞
f(x−)sin(Lξ)
ξ
dξ
1521

I(x, L) −f(x+) + f(x−)
2
=
Z 0
−∞
f(x + ξ) −f(x−)
ξ

sin(Lξ) dξ
−
Z ∞
0
f(x + ξ) −f(x+)
ξ

sin(Lξ) dξ
If f(x) has a left and right derivative at x, then
f(x + ξ) −f(x−)
ξ
is bounded for ξ ≤0, and
f(x + ξ) −f(x+)
ξ
is bounded for ξ ≥0.
Again using the Riemann-Lebesgue lemma we see that
f(x+) + f(x−)
2
= 1
2π
Z ∞
−∞
Z ∞
−∞
f(ξ) eıωξ dξ

e−ıωx dω.
1522

Result 32.2.1 Let f(x) be piecewise continuous with
R ∞
−∞|f(x)| dx < ∞.
The Fourier
transform of f(x) is deﬁned
ˆf(ω) = F[f(x)] = 1
2π
Z ∞
−∞
f(x) e−ıωx dx.
We see that the integral is uniformly convergent. The inverse Fourier transform is deﬁned
f(x+) + f(x−)
2
= F−1[ ˆf(ω)] =
Z ∞
−∞
ˆf(ω) eıωx dω.
If f(x) is continuous then this reduces to
f(x) = F−1[ ˆf(ω)] =
Z ∞
−∞
ˆf(ω) eıωx dω.
32.2.1
A Word of Caution
Other texts may deﬁne the Fourier transform diﬀerently. The important relation is
f(x) =
Z ∞
−∞
 1
2π
Z ∞
−∞
f(ξ) e∓ıωξ dξ

e±ıωx dω.
Multiplying the right side of this equation by 1 = 1
αα yields
f(x) = 1
α
Z ∞
−∞
 α
2π
Z ∞
−∞
f(ξ) e∓ıωξ dξ

e±ıωx dω.
1523

Setting α =
√
2π and choosing sign in the exponentials gives us the Fourier transform pair
ˆf(ω) =
1
√
2π
Z ∞
−∞
f(x) e−ıωx dx
f(x) =
1
√
2π
Z ∞
−∞
ˆf(ω) eıωx dω.
Other equally valid pairs are
ˆf(ω) =
Z ∞
−∞
f(x) e−ıωx dx
f(x) = 1
2π
Z ∞
−∞
ˆf(ω) eıωx dω,
and
ˆf(ω) =
Z ∞
−∞
f(x) eıωx dx
f(x) = 1
2π
Z ∞
−∞
ˆf(ω) e−ıωx dω.
Be aware of the diﬀerent deﬁnitions when reading other texts or consulting tables of Fourier transforms.
32.3
Evaluating Fourier Integrals
32.3.1
Integrals that Converge
If the Fourier integral
F[f(x)] = 1
2π
Z ∞
−∞
f(x) e−ıωx dx,
1524

converges for real ω, then ﬁnding the transform of a function is just a matter of direct integration. We will consider
several examples of such garden variety functions in this subsection. Later on we will consider the more interesting
cases when the integral does not converge for real ω.
Example 32.3.1 Consider the Fourier transform of e−a|x|, where a > 0. Since the integral of e−a|x| is absolutely
convergent, we know that the Fourier transform integral converges for real ω. We write out the integral.
F
e−a|x|
= 1
2π
Z ∞
−∞
e−a|x| e−ıωx dx
= 1
2π
Z 0
−∞
eax−ıωx dx + 1
2π
Z ∞
0
e−ax−ıωx dx
= 1
2π
Z 0
−∞
e(a−ıℜ(ω)+ℑ(ω))x dx + 1
2π
Z ∞
0
e(−a−ıℜ(ω)+ℑ(ω))x dx
The integral converges for |ℑ(ω)| < a. This domain is shown in Figure 32.1.
Re(z)
Im(z)
Figure 32.1: The Domain of Convergence
1525

Now We do the integration.
F
e−a|x|
= 1
2π
Z 0
−∞
e(a−ıω)x dx + 1
2π
Z ∞
0
e−(a+ıω)x dx
= 1
2π
e(a−ıω)x
a −ıω
0
−∞
+ 1
2π

−e−(a+ıω)x
a + ıω
∞
0
= 1
2π

1
a −ıω +
1
a + ıω

= 1
π
a
π(ω2 + a2),
for |ℑ(ω)| < a
We can extend the domain of the Fourier transform with analytic continuation.
F
e−a|x|
=
a
π(ω2 + a2),
for ω ̸= ±ıa
Example 32.3.2 Consider the Fourier transform of f(x) =
1
x−ıα, α > 0.
F

1
x −ıα

= 1
2π
Z ∞
−∞
1
x −ıα e−ıωx dx
The integral converges for ℑ(ω) = 0. We will evaluate the integral for positive and negative real values of ω.
For ω > 0, we will close the path of integration in the lower half-plane. Let CR be the contour from x = R to
x = −R following a semicircular path in the lower half-plane. The integral along CR vanishes as R →∞by Jordan’s
Lemma.
Z
CR
1
x −ıα e−ıωx dx →0
as R →∞.
Since the integrand is analytic in the lower half-plane the integral vanishes.
F

1
x −ıα

= 0
1526

For ω < 0, we will close the path of integration in the upper half-plane. Let CR denote the semicircular contour from
x = R to x = −R in the upper half-plane. The integral along CR vanishes as R goes to inﬁnity by Jordan’s Lemma.
We evaluate the Fourier transform integral with the Residue Theorem.
F

1
x −ıα

= 1
2π2πi Res
 e−ıωx
x −iα, iα

= ı eαω
We combine the results for positive and negative values of ω.
F

1
x −ıα

=
(
0
for ω > 0,
ı eαω
for ω < 0
32.3.2
Cauchy Principal Value and Integrals that are Not Absolutely Convergent.
That the integral of f(x) is absolutely convergent is a suﬃcient but not a necessary condition that the Fourier transform
of f(x) exists. The integral
R ∞
−∞f(x) e−ıωx dx may converge even if
R ∞
−∞|f(x)| dx does not. Furthermore, if the Fourier
transform integral diverges, its principal value may exist. We will say that the Fourier transform of f(x) exists if the
principal value of the integral exists.
F[f(x)] = −
Z ∞
−∞
f(x) e−ıωx dx
Example 32.3.3 Consider the Fourier transform of f(x) = 1/x.
ˆf(ω) = 1
2π −
Z ∞
−∞
1
x e−ıωx dx
If ω > 0, we can close the contour in the lower half-plane. The integral along the semi-circle vanishes due to Jordan’s
Lemma.
lim
R→∞
Z
CR
1
x e−ıωx dx = 0
1527

We can evaluate the Fourier transform with the Residue Theorem.
ˆf(ω) = 1
2π
−1
2

(2πi) Res
1
x e−ıωx, 0

ˆf(ω) = −ı
2,
for ω > 0.
The factor of −1/2 in the above derivation arises because the path of integration is in the negative, (clockwise),
direction and the path of integration crosses through the ﬁrst order pole at x = 0. The path of integration is shown in
Figure 32.2.
Re(z)
Im(z)
Figure 32.2: The Path of Integration
If ω < 0, we can close the contour in the upper half plane to obtain
ˆf(ω) = ı
2,
for ω < 0.
For ω = 0 the integral vanishes because 1
x is an odd function.
ˆf(0) = 1
2π = −
Z ∞
−∞
1
x dx = 0
1528

We collect the results in one formula.
ˆf(ω) = −ı
2 sign(ω)
We write the integrand for ω > 0 as the sum of an odd and and even function.
1
2π −
Z ∞
−∞
1
x e−ıωx dx = −ı
2
−
Z ∞
−∞
1
x cos(ωx) dx + −
Z ∞
−∞
−ı
x sin(ωx) dx = −ıπ
The principal value of the integral of any odd function is zero.
−
Z ∞
−∞
1
x sin(ωx) dx = π
If the principal value of the integral of an even function exists, then the integral converges.
Z ∞
−∞
1
x sin(ωx) dx = π
Z ∞
0
1
x sin(ωx) dx = π
2
Thus we have evaluated an integral that we used in deriving the Fourier transform.
32.3.3
Analytic Continuation
Consider the Fourier transform of f(x) = 1. The Fourier integral is not convergent, and its principal value does not
exist. Thus we will have to be a little creative in order to deﬁne the Fourier transform. Deﬁne the two functions
f+(x) =





1
for x > 0
1/2
for x = 0
0
for x < 0
,
f−(x) =





0
for x > 0
1/2
for x = 0
1
for x < 0
.
1529

Note that 1 = f−(x) + f+(x).
The Fourier transform of f+(x) converges for ℑ(ω) < 0.
F[f+(x)] = 1
2π
Z ∞
0
e−ıωx dx
= 1
2π
Z ∞
0
e(−ıℜ(ω)+ℑ(ω))x dx.
= 1
2π
e−ıωx
−ıω
∞
0
= −
ı
2πω
for ℑ(ω) < 0
Using analytic continuation, we can deﬁne the Fourier transform of f+(x) for all ω except the point ω = 0.
F[f+(x)] = −
ı
2πω
We follow the same procedure for f−(x). The integral converges for ℑ(ω) > 0.
F[f−(x)] = 1
2π
Z 0
−∞
e−ıωx dx
= 1
2π
Z 0
−∞
e(−ıℜ(ω)+ℑ(ω))x dx
= 1
2π
e−ıωx
−ıω
0
−∞
=
ı
2πω.
Using analytic continuation we can deﬁne the transform for all nonzero ω.
F[f−(x)] =
ı
2πω
1530

Now we are prepared to deﬁne the Fourier transform of f(x) = 1.
F[1] = F[f−(x)] + F[f+(x)]
= −
ı
2πω +
ı
2πω
= 0,
for ω ̸= 0
When ω = 0 the integral diverges. When we consider the closure relation for the Fourier transform we will see that
F[1] = δ(ω).
32.4
Properties of the Fourier Transform
In this section we will explore various properties of the Fourier Transform. I would like to avoid stating assumptions on
various functions at the beginning of each subsection. Unless otherwise indicated, assume that the integrals converge.
32.4.1
Closure Relation.
Recall the closure relation for an orthonormal set of functions {φ1, φ2, . . .},
∞
X
n=1
φn(x)φn(ξ) ∼δ(x −ξ).
There is a similar closure relation for Fourier integrals. We compute the Fourier transform of δ(x −ξ).
F[δ(x −ξ)] = 1
2π
Z ∞
−∞
δ(x −ξ) e−ıωx dx
= 1
2π e−ıωξ
1531

Next we take the inverse Fourier transform.
δ(x −ξ) ∼
Z ∞
−∞
1
2π e−ıωξ eıωx dω
δ(x −ξ) ∼1
2π
Z ∞
−∞
eıω(x−ξ) dω.
Note that the integral is divergent, but it would be impossible to represent δ(x −ξ) with a convergent integral.
32.4.2
Fourier Transform of a Derivative.
Consider the Fourier transform of y′(x).
F[y′(x)] = 1
2π
Z ∞
−∞
y′(x) e−ıωx dx
=
 1
2πy(x) e−ıωx
∞
−∞
−1
2π
Z ∞
−∞
(−ıω)y(x) e−ıωx dx
= ıω 1
2π
Z ∞
−∞
y(x) e−ıωx dx
= ıωF[y(x)]
Next consider y′′(x).
F[y′′(x)] = F
 d
dx(y′(x))

= ıωF[y′(x)]
= (ıω)2F[y(x)]
= −ω2F[y(x)]
1532

In general,
F

y(n)(x)

= (ıω)nF[y(x)].
Example 32.4.1 The Dirac delta function can be expressed as the derivative of the Heaviside function.
H(x −c) =
(
0
for x < c,
1
for x > c
Thus we can express the Fourier transform of H(x −c) in terms of the Fourier transform of the delta function.
F[δ(x −c)] = ıωF[H(x −c)]
1
2π
Z ∞
−∞
δ(x −c) e−ıωx dx = ıωF[H(x −c)]
1
2π e−ıcω = ıωF[H(x −c)]
F[H(x −c)] =
1
2πıω e−ıcω
1533

32.4.3
Fourier Convolution Theorem.
Consider the Fourier transform of a product of two functions.
F[f(x)g(x)] = 1
2π
Z ∞
−∞
f(x)g(x) e−ıωx dx
= 1
2π
Z ∞
−∞
Z ∞
−∞
ˆf(η) eıηx dη

g(x) e−ıωx dx
= 1
2π
Z ∞
−∞
Z ∞
−∞
ˆf(η)g(x) eı(η−ω)x dx

dη
=
Z ∞
−∞
ˆf(η)
 1
2π
Z ∞
−∞
g(x) e−ı(ω−η)x dx

dη
=
Z ∞
−∞
ˆf(η)G(ω −η) dη
The convolution of two functions is deﬁned
f ∗g(x) =
Z ∞
−∞
f(ξ)g(x −ξ) dξ.
Thus
F[f(x)g(x)] = ˆf ∗ˆg(ω) =
Z ∞
−∞
ˆf(η)ˆg(ω −η) dη.
1534

Now consider the inverse Fourier Transform of a product of two functions.
F−1[ ˆf(ω)ˆg(ω)] =
Z ∞
−∞
ˆf(ω)ˆg(ω) eıωx dω
=
Z ∞
−∞
 1
2π
Z ∞
−∞
f(ξ) e−ıωξ dξ

ˆg(ω) eıωx dω
= 1
2π
Z ∞
−∞
Z ∞
−∞
f(ξ)ˆg(ω) eıω(x−ξ) dω

dξ
= 1
2π
Z ∞
−∞
f(ξ)
Z ∞
−∞
ˆg(ω) eıω(x−ξ) dω

dξ
= 1
2π
Z ∞
−∞
f(ξ)g(x −ξ) dξ
Thus
F−1[ ˆf(ω)ˆg(ω)] = 1
2πf ∗g(x) = 1
2π
Z ∞
−∞
f(ξ)g(x −ξ) dξ,
F[f ∗g(x)] = 2π ˆf(ω)ˆg(ω).
These relations are known as the Fourier convolution theorem.
Example 32.4.2 Using the convolution theorem and the table of Fourier transform pairs in the appendix, we can ﬁnd
the Fourier transform of
f(x) =
1
x4 + 5x2 + 4.
We factor the fraction.
f(x) =
1
(x2 + 1)(x2 + 4)
From the table, we know that
F

2c
x2 + c2

= e−c|ω|
for c > 0.
1535

We apply the convolution theorem.
F[f(x)] = F
1
8
2
x2 + 1
4
x2 + 4

= 1
8
Z ∞
−∞
e−|η| e−2|ω−η| dη

= 1
8
Z 0
−∞
eη e−2|ω−η| dη +
Z ∞
0
e−η e−2|ω−η| dη

First consider the case ω > 0.
F[f(x)] = 1
8
Z 0
−∞
e−2ω+3η dη +
Z ω
0
e−2ω+η dη +
Z ∞
ω
e2ω−3η dη

= 1
8
1
3 e−2ω + e−ω −e−2ω +1
3 e−ω

= 1
6 e−ω −1
12 e−2ω
Now consider the case ω < 0.
F[f(x)] = 1
8
Z ω
−∞
e−2ω+3η dη +
Z 0
ω
e2ω−η dη +
Z ∞
0
e2ω−3η dη

= 1
8
1
3 eω −e2ω + eω +1
3 e2ω

= 1
6 eω −1
12 e2ω
We collect the result for positive and negative ω.
F[f(x)] = 1
6 e−|ω| −1
12 e−2|ω|
1536

A better way to ﬁnd the Fourier transform of
f(x) =
1
x4 + 5x2 + 4
is to ﬁrst expand the function in partial fractions.
f(x) =
1/3
x2 + 1 −
1/3
x2 + 4
F[f(x)] = 1
6F

2
x2 + 1

−1
12F

4
x2 + 4

= 1
6 e−|ω| −1
12 e−2|ω|
32.4.4
Parseval’s Theorem.
Recall Parseval’s theorem for Fourier series. If f(x) is a complex valued function with the Fourier series P∞
n=−∞cn eınx
then
2π
∞
X
n=−∞
|cn|2 =
Z π
−π
|f(x)|2 dx.
Analogous to this result is Parseval’s theorem for Fourier transforms.
Let f(x) be a complex valued function that is both absolutely integrable and square integrable.
Z ∞
−∞
|f(x)| dx < ∞
and
Z ∞
−∞
|f(x)|2 dx < ∞
1537

The Fourier transform of f(−x) is ˆf(ω).
F
h
f(−x)
i
= 1
2π
Z ∞
−∞
f(−x) e−ıωx dx
= −1
2π
Z −∞
∞
f(x) eıωx dx
= 1
2π
Z ∞
−∞
f(x) e−ıωx dx
= ˆf(ω)
We apply the convolution theorem.
F−1[2π ˆf(ω) ˆf(ω)] =
Z ∞
−∞
f(ξ)f(−(x −ξ)) dξ
Z ∞
−∞
2π ˆf(ω) ˆf(ω) eıωx dω =
Z ∞
−∞
f(ξ)f(ξ −x) dξ
We set x = 0.
2π
Z ∞
−∞
ˆf(ω) ˆf(ω) dω =
Z ∞
−∞
f(ξ)f(ξ) dξ
2π
Z ∞
−∞
| ˆf(ω)|2 dω =
Z ∞
−∞
|f(x)|2 dx
This is known as Parseval’s theorem.
1538

32.4.5
Shift Property.
The Fourier transform of f(x + c) is
F[f(x + c)] = 1
2π
Z ∞
−∞
f(x + c) e−ıωx dx
= 1
2π
Z ∞
−∞
f(x) e−ıω(x−c) dx
F[f(x + c)] = eıωc ˆf(ω)
The inverse Fourier transform of ˆf(ω + c) is
F−1[ ˆf(ω + c)] =
Z ∞
−∞
ˆf(ω + c) eıωx dω
=
Z ∞
−∞
ˆf(ω) eı(ω−c)x dω
F−1[ ˆf(ω + c)] = e−ıcx f(x)
32.4.6
Fourier Transform of x f(x).
The Fourier transform of xf(x) is
F[xf(x)] = 1
2π
Z ∞
−∞
xf(x) e−ıωx dx
= 1
2π
Z ∞
−∞
ıf(x) ∂
∂ω(e−ıωx) dx
= ı ∂
∂ω
 1
2π
Z ∞
−∞
f(x) e−ıωx dx

1539

F[xf(x)] = ı∂ˆf
∂ω.
Similarly, you can show that
F[xnf(x)] = (i)n ∂n ˆf
∂ωn.
32.5
Solving Diﬀerential Equations with the Fourier Transform
The Fourier transform is useful in solving some diﬀerential equations on the domain (−∞. . . ∞) with homogeneous
boundary conditions at inﬁnity. We take the Fourier transform of the diﬀerential equation L[y] = f and solve for ˆy. We
take the inverse transform to determine the solution y. Note that this process is only applicable if the Fourier transform
of y exists. Hence the requirement for homogeneous boundary conditions at inﬁnity.
We will use the table of Fourier transforms in the appendix in solving the examples in this section.
Example 32.5.1 Consider the problem
y′′ −y = e−α|x|,
y(±∞) = 0,
α > 0, α ̸= 1.
We take the Fourier transform of this equation.
−ω2ˆy(ω) −ˆy(ω) =
α/π
ω2 + α2
We take the inverse Fourier transform to determine the solution.
ˆy(ω) =
−α/π
(ω2 + α2)(ω2 + 1)
= −α
π
1
α2 −1

1
ω2 + 1 −
1
ω2 + α2

=
1
α2 −1

α/π
ω2 + α2 −α 1/π
ω2 + 1

1540

y(x) = e−α|x| −α e−|x|
α2 −1
Example 32.5.2 Consider the Green function problem
G′′ −G = δ(x −ξ),
y(±∞) = 0.
We take the Fourier transform of this equation.
−ω2 ˆG −ˆG = F[δ(x −ξ)]
ˆG = −
1
ω2 + 1F[δ(x −ξ)]
We use the Table of Fourier transforms.
ˆG = −πF
e−|x|
F[δ(x −ξ)]
We use the convolution theorem to do the inversion.
G = −π 1
2π
Z ∞
−∞
e−|x−η| δ(η −ξ) dη
G(x|ξ) = −1
2 e|x−ξ|
The inhomogeneous diﬀerential equation
y′′ −y = f(x),
y(±∞) = 0,
has the solution
y = −1
2
Z ∞
−∞
f(ξ) e−|x−ξ| dξ.
When solving the diﬀerential equation L[y] = f with the Fourier transform, it is quite common to use the convolution
theorem. With this approach we have no need to compute the Fourier transform of the right side. We merely denote
it as F[f] until we use f in the convolution integral.
1541

32.6
The Fourier Cosine and Sine Transform
32.6.1
The Fourier Cosine Transform
Suppose f(x) is an even function. In this case the Fourier transform of f(x) coincides with the Fourier cosine transform
of f(x).
F[f(x)] = 1
2π
Z ∞
−∞
f(x) e−ıωx dx
= 1
2π
Z ∞
−∞
f(x)(cos(ωx) −ı sin(ωx)) dx
= 1
2π
Z ∞
−∞
f(x) cos(ωx) dx
= 1
π
Z ∞
0
f(x) cos(ωx) dx
The Fourier cosine transform is deﬁned:
Fc[f(x)] = ˆfc(ω) = 1
π
Z ∞
0
f(x) cos(ωx) dx.
Note that ˆfc(ω) is an even function. The inverse Fourier cosine transform is
F−1
c [ ˆfc(ω)] =
Z ∞
−∞
ˆfc(ω) eıωx dω
=
Z ∞
−∞
ˆfc(ω)(cos(ωx) + ı sin(ωx)) dω
=
Z ∞
−∞
ˆfc(ω) cos(ωx) dω
= 2
Z ∞
0
ˆfc(ω) cos(ωx) dω.
1542

Thus we have the Fourier cosine transform pair
f(x) = F−1
c [ ˆfc(ω)] = 2
Z ∞
0
ˆfc(ω) cos(ωx) dω,
ˆfc(ω) = Fc[f(x)] = 1
π
Z ∞
0
f(x) cos(ωx) dx.
32.6.2
The Fourier Sine Transform
Suppose f(x) is an odd function. In this case the Fourier transform of f(x) coincides with the Fourier sine transform
of f(x).
F[f(x)] = 1
2π
Z ∞
−∞
f(x) e−ıωx dx
= 1
2π
Z ∞
−∞
f(x)(cos(ωx) −ı sin(ωx)) dx
= −ı
π
Z ∞
0
f(x) sin(ωx) dx
Note that ˆf(ω) = F[f(x)] is an odd function of ω. The inverse Fourier transform of ˆf(ω) is
F−1[ ˆf(ω)] =
Z ∞
−∞
ˆf(ω) eıωx dω
= 2ı
Z ∞
0
ˆf(ω) sin(ωx) dω.
Thus we have that
f(x) = 2ı
Z ∞
0

−ı
π
Z ∞
0
f(x) sin(ωx) dx

sin(ωx) dω
= 2
Z ∞
0
 1
π
Z ∞
0
f(x) sin(ωx) dx

sin(ωx) dω.
1543

This gives us the Fourier sine transform pair
f(x) = F−1
s [ ˆfs(ω)] = 2
Z ∞
0
ˆfs(ω) sin(ωx) dω,
ˆfs(ω) = Fs[f(x)] = 1
π
Z ∞
0
f(x) sin(ωx) dx.
Result 32.6.1 The Fourier cosine transform pair is deﬁned:
f(x) = F−1
c [ ˆfc(ω)] = 2
Z ∞
0
ˆfc(ω) cos(ωx) dω
ˆfc(ω) = Fc[f(x)] = 1
π
Z ∞
0
f(x) cos(ωx) dx
The Fourier sine transform pair is deﬁned:
f(x) = F−1
s [ ˆfs(ω)] = 2
Z ∞
0
ˆfs(ω) sin(ωx) dω
ˆfs(ω) = Fs[f(x)] = 1
π
Z ∞
0
f(x) sin(ωx) dx
32.7
Properties of the Fourier Cosine and Sine Transform
32.7.1
Transforms of Derivatives
Cosine Transform.
Using integration by parts we can ﬁnd the Fourier cosine transform of derivatives. Let y be a
function for which the Fourier cosine transform of y and its ﬁrst and second derivatives exists. Further assume that y
1544

and y′ vanish at inﬁnity. We calculate the transforms of the ﬁrst and second derivatives.
Fc[y′] = 1
π
Z ∞
0
y′ cos(ωx) dx
= 1
π

y cos(ωx)
∞
0 + ω
π
Z ∞
0
y sin(ωx) dx
= ωˆyc(ω) −1
πy(0)
Fc[y′′] = 1
π
Z ∞
0
y′′ cos(ωx) dx
= 1
π

y′ cos(ωx)
∞
0 + ω
π
Z ∞
0
y′ sin(ωx) dx
= −1
πy′(0) + ω
π

y sin(ωx)
∞
0 −ω2
π
Z ∞
0
y cos(ωx) dx
= −ω2 ˆfc(ω) −1
πy′(0)
Sine Transform.
You can show, (see Exercise 32.3), that the Fourier sine transform of the ﬁrst and second derivatives
are
Fs[y′] = −ω ˆfc(ω)
Fs[y′′] = −ω2ˆyc(ω) + ω
π y(0).
1545

32.7.2
Convolution Theorems
Cosine Transform of a Product.
Consider the Fourier cosine transform of a product of functions. Let f(x) and
g(x) be two functions deﬁned for x ≥0. Let Fc[f(x)] = ˆfc(ω), and Fc[g(x)] = ˆgc(ω).
Fc[f(x)g(x)] = 1
π
Z ∞
0
f(x)g(x) cos(ωx) dx
= 1
π
Z ∞
0

2
Z ∞
0
ˆfc(η) cos(ηx) dη

g(x) cos(ωx) dx
= 2
π
Z ∞
0
Z ∞
0
ˆfc(η)g(x) cos(ηx) cos(ωx) dx dη
We use the identity cos a cos b = 1
2(cos(a −b) + cos(a + b)).
= 1
π
Z ∞
0
Z ∞
0
ˆfc(η)g(x)
 cos((ω −η)x) + cos((ω + η)x)

dx dη
=
Z ∞
0
ˆfc(η)
 1
π
Z ∞
0
g(x) cos((ω −η)x) dx + 1
π
Z ∞
0
g(x) cos((ω + η)x) dx

dη
=
Z ∞
0
ˆfc(η)
 ˆgc(ω −η) + ˆgc(ω + η)

dη
ˆgc(ω) is an even function. If we have only deﬁned ˆgc(ω) for positive argument, then ˆgc(ω) = ˆgc(|ω|).
=
Z ∞
0
ˆfc(η)
 ˆgc(|ω −η|) + ˆgc(ω + η)

dη
1546

Inverse Cosine Transform of a Product.
Now consider the inverse Fourier cosine transform of a product of
functions. Let Fc[f(x)] = ˆfc(ω), and Fc[g(x)] = ˆgc(ω).
F−1
c [ ˆfc(ω)ˆgc(ω)] = 2
Z ∞
0
ˆfc(ω)ˆgc(ω) cos(ωx) dω
= 2
Z ∞
0
 1
π
Z ∞
0
f(ξ) cos(ωξ) dξ

ˆgc(ω) cos(ωx) dω
= 2
π
Z ∞
0
Z ∞
0
f(ξ)ˆgc(ω) cos(ωξ) cos(ωx) dω dξ
= 1
π
Z ∞
0
Z ∞
0
f(ξ)ˆgc(ω)
 cos(ω(x −ξ)) + cos(ω(x + ξ))

dω dξ
= 1
2π
Z ∞
0
f(ξ)

2
Z ∞
0
ˆgc(ω) cos(ω(x −ξ)) dω + 2
Z ∞
0
ˆgc(ω) cos(ω(x + ξ)) dω

dξ
= 1
2π
Z ∞
0
f(ξ)
 g(|x −ξ|) + g(x + ξ)

dξ
Sine Transform of a Product.
You can show, (see Exercise 32.5), that the Fourier sine transform of a product
of functions is
Fs[f(x)g(x)] =
Z ∞
0
ˆfs(η)
 ˆgc(|ω −η|) −ˆgc(ω + η)

dη.
Inverse Sine Transform of a Product.
You can also show, (see Exercise 32.6), that the inverse Fourier sine
transform of a product of functions is
F−1
s [ ˆfs(ω)ˆgc(ω)] = 1
2π
Z ∞
0
f(ξ)
 g(|x −ξ|) −g(x + ξ)

dξ.
1547

Result 32.7.1 The Fourier cosine and sine transform convolution theorems are
Fc[f(x)g(x)] =
Z ∞
0
ˆfc(η)

ˆgc(|ω −η|) + ˆgc(ω + η)

dη
F−1
c [ ˆfc(ω)ˆgc(ω)] = 1
2π
Z ∞
0
f(ξ)
 g(|x −ξ|) + g(x + ξ)

dξ
Fs[f(x)g(x)] =
Z ∞
0
ˆfs(η)
 ˆgc(|ω −η|) −ˆgc(ω + η)

dη
F−1
s [ ˆfs(ω)ˆgc(ω)] = 1
2π
Z ∞
0
f(ξ)
 g(|x −ξ|) −g(x + ξ)

dξ
32.7.3
Cosine and Sine Transform in Terms of the Fourier Transform
We can express the Fourier cosine and sine transform in terms of the Fourier transform. First consider the Fourier
cosine transform. Let f(x) be an even function.
Fc[f(x)] = 1
π
Z ∞
0
f(x) cos(ωx) dx
We extend the domain integration because the integrand is even.
= 1
2π
Z ∞
−∞
f(x) cos(ωx) dx
Note that
R ∞
−∞f(x) sin(ωx) dx = 0 because the integrand is odd.
= 1
2π
Z ∞
−∞
f(x) e−ıωx dx
= F[f(x)]
1548

Fc[f(x)] = F[f(x)],
for even f(x).
For general f(x), use the even extension, f(|x|) to write the result.
Fc[f(x)] = F[f(|x|)]
There is an analogous result for the inverse Fourier cosine transform.
F−1
c
h
ˆf(ω)
i
= F−1 h
ˆf(|ω|)
i
For the sine series, we have
Fs[f(x)] = ıF [sign(x)f(|x|)]
F−1
s
h
ˆf(ω)
i
= −ıF−1 h
sign(ω) ˆf(|ω|)
i
Result 32.7.2 The results:
Fc[f(x)] = F[f(|x|)]
F−1
c
h
ˆf(ω)
i
= F−1 h
ˆf(|ω|)
i
Fs[f(x)] = ıF[sign(x)f(|x|)]
F−1
s
h
ˆf(ω)
i
= −ıF−1 h
sign(ω) ˆf(|ω|)
i
allow us to evaluate Fourier cosine and sine transforms in terms of the Fourier transform.
This enables us to use contour integration methods to do the integrals.
32.8
Solving Diﬀerential Equations with the Fourier Cosine and
Sine Transforms
Example 32.8.1 Consider the problem
y′′ −y = 0,
y(0) = 1,
y(∞) = 0.
1549

Since the initial condition is y(0) = 1 and the sine transform of y′′ is −ω2ˆyc(ω) + ω
πy(0) we take the Fourier sine
transform of both sides of the diﬀerential equation.
−ω2ˆyc(ω) + ω
π y(0) −ˆyc(ω) = 0
−(ω2 + 1)ˆyc(ω) = −ω
π
ˆyc(ω) =
ω
π(ω2 + 1)
We use the table of Fourier Sine transforms.
y = e−x
Example 32.8.2 Consider the problem
y′′ −y = e−2x,
y′(0) = 0,
y(∞) = 0.
Since the initial condition is y′(0) = 0, we take the Fourier cosine transform of the diﬀerential equation. From the table
of cosine transforms, Fc[e−2x] = 2/(π(ω2 + 4)).
−ω2ˆyc(ω) −1
πy′(0) −ˆyc(ω) =
2
π(ω2 + 4)
ˆyc(ω) = −
2
π(ω2 + 4)(ω2 + 1)
= −2
π
 1/3
ω2 + 1 −
1/3
ω2 + 4

= 1
3
2/π
ω2 + 4 −2
3
1/π
ω2 + 1
y = 1
3 e−2x −2
3 e−x
1550

32.9
Exercises
Exercise 32.1
Show that
H(x + c) −H(x −c) = sin(cω)
πω
.
Hint, Solution
Exercise 32.2
Using contour integration, ﬁnd the Fourier transform of
f(x) =
1
x2 + c2,
where ℜ(c) ̸= 0
Hint, Solution
Exercise 32.3
Find the Fourier sine transforms of y′(x) and y′′(x).
Hint, Solution
Exercise 32.4
Prove the following identities.
1. F[f(x −a)] = e−ıωa ˆf(ω)
2. F[f(ax)] = 1
|a|
ˆf
ω
a

Hint, Solution
Exercise 32.5
Show that
Fs[f(x)g(x)] =
Z ∞
0
ˆfs(η)
 ˆgc(|ω −η|) −ˆgc(ω + η)

dη.
1551

Hint, Solution
Exercise 32.6
Show that
F−1
s [ ˆfs(ω)ˆgc(ω)] = 1
2π
Z ∞
0
f(ξ)
 g(|x −ξ|) −g(x + ξ)

dξ.
Hint, Solution
Exercise 32.7
Let ˆfc(ω) = Fc[f(x)], ˆfc(ω) = Fs[f(x)], and assume the cosine and sine transforms of xf(x) exist. Express Fc[xf(x)]
and Fs[xf(x)] in terms of ˆfc(ω) and ˆfc(ω).
Hint, Solution
Exercise 32.8
Solve the problem
y′′ −y = e−2x,
y(0) = 1,
y(∞) = 0,
using the Fourier sine transform.
Hint, Solution
Exercise 32.9
Prove the following relations between the Fourier sine transform and the Fourier transform.
Fs[f(x)] = ıF[sign(x)f(|x|)]
F−1
s
h
ˆf(ω)
i
= −ıF−1 h
sign(ω) ˆf(|ω|)
i
Hint, Solution
Exercise 32.10
Let ˆfc(ω) = Fc[f(x)] and ˆfc(ω) = Fs[f(x)]. Show that
1. Fc[xf(x)] =
∂
∂ω ˆfc(ω)
1552

2. Fs[xf(x)] = −∂
∂ω ˆfc(ω)
3. Fc[f(cx)] = 1
c ˆfc
  ω
c

for c > 0
4. Fs[f(cx)] = 1
c ˆfc
  ω
c

for c > 0.
Hint, Solution
Exercise 32.11
Solve the integral equation,
Z ∞
−∞
u(ξ) e−a(x−ξ)2 dξ = e−bx2,
where a, b > 0, a ̸= b, with the Fourier transform.
Hint, Solution
Exercise 32.12
Evaluate
1
π
Z ∞
0
1
x e−cx sin(ωx) dx,
where ω is a positive, real number and ℜ(c) > 0.
Hint, Solution
Exercise 32.13
Use the Fourier transform to solve the equation
y′′ −a2y = e−a|x|
on the domain −∞< x < ∞with boundary conditions y(±∞) = 0.
Hint, Solution
1553

Exercise 32.14
1. Use the cosine transform to solve
y′′ −a2y = 0 on x ≥0 with y′(0) = b, y(∞) = 0.
2. Use the cosine transform to show that the Green function for the above with b = 0 is
G(x, ξ) = −1
2a e−a|x−ξ| −1
2a e−a(x−ξ) .
Hint, Solution
Exercise 32.15
1. Use the sine transform to solve
y′′ −a2y = 0 on x ≥0 with y(0) = b, y(∞) = 0.
2. Try using the Laplace transform on this problem. Why isn’t it as convenient as the Fourier transform?
3. Use the sine transform to show that the Green function for the above with b = 0 is
g(x; ξ) = 1
2a
 e−a(x−ξ) −e−a|x+ξ|
Hint, Solution
Exercise 32.16
1. Find the Green function which solves the equation
y′′ + 2µy′ + (β2 + µ2)y = δ(x −ξ),
µ > 0, β > 0,
in the range −∞< x < ∞with boundary conditions y(−∞) = y(∞) = 0.
1554

2. Use this Green’s function to show that the solution of
y′′ + 2µy′ + (β2 + µ2)y = g(x),
µ > 0, β > 0,
y(−∞) = y(∞) = 0,
with g(±∞) = 0 in the limit as µ →0 is
y = 1
β
Z x
−∞
g(ξ) sin[β(x −ξ)]dξ.
You may assume that the interchange of limits is permitted.
Hint, Solution
Exercise 32.17
Using Fourier transforms, ﬁnd the solution u(x) to the integral equation
Z ∞
−∞
u(ξ)
[(x −ξ)2 + a2] dξ =
1
x2 + b2
0 < a < b.
Hint, Solution
Exercise 32.18
The Fourer cosine transform is deﬁned by
ˆfc(ω) = 1
π
Z ∞
0
f(x) cos(ωx) dx.
1. From the Fourier theorem show that the inverse cosine transform is given by
f(x) = 2
Z ∞
0
ˆfc(ω) cos(ωx) dω.
2. Show that the cosine transform of f ′′(x) is
−ω2 ˆfc(ω) −f ′(0)
π
.
1555

3. Use the cosine transform to solve the following boundary value problem.
y′′ −a2y = 0
on x > 0
with y′(0) = b, y(∞) = 0
Hint, Solution
Exercise 32.19
The Fourier sine transform is deﬁned by
ˆfs(ω) = 1
π
Z ∞
0
f(x) sin(ωx) dx.
1. Show that the inverse sine transform is given by
f(x) = 2
Z ∞
0
ˆfs(ω) sin(ωx) dω.
2. Show that the sine transform of f ′′(x) is
ω
π f(0) −ω2 ˆfs(ω).
3. Use this property to solve the equation
y′′ −a2y = 0
on x > 0
with y(0) = b, y(∞) = 0.
4. Try using the Laplace transform on this problem. Why isn’t it as convenient as the Fourier transform?
Hint, Solution
Exercise 32.20
Show that
F[f(x)] = 1
2 (Fc[f(x) + f(−x)] −ıFs[f(x) −f(−x)])
where F, Fc and Fs are respectively the Fourier transform, Fourier cosine transform and Fourier sine transform.
Hint, Solution
1556

Exercise 32.21
Find u(x) as the solution to the integral equation:
Z ∞
−∞
u(ξ)
(x −ξ)2 + a2 dξ =
1
x2 + b2,
0 < a < b.
Use Fourier transforms and the inverse transform. Justify the choice of any contours used in the complex plane.
Hint, Solution
1557

32.10
Hints
Hint 32.1
H(x + c) −H(x −c) =
(
1
for |x| < c,
0
for |x| > c
Hint 32.2
Consider the two cases ℜ(ω) < 0 and ℜ(ω) > 0, closing the path of integration with a semi-circle in the lower or upper
half plane.
Hint 32.3
Hint 32.4
Hint 32.5
Hint 32.6
Hint 32.7
Hint 32.8
Hint 32.9
1558

Hint 32.10
Hint 32.11
The left side is the convolution of u(x) and e−ax2.
Hint 32.12
Hint 32.13
Hint 32.14
Hint 32.15
Hint 32.16
Hint 32.17
Hint 32.18
Hint 32.19
Hint 32.20
1559

Hint 32.21
1560

32.11
Solutions
Solution 32.1
F[H(x + c) −H(x −c)] = 1
2π
Z ∞
−∞
(H(x + c) −H(x −c)) e−ıωx dx
= 1
2π
Z c
−c
e−ıωx dx
= 1
2π
e−ıωx
−ıω
c
−c
= 1
2π
e−ıωc
−ıω −eıωc
−ıω

F[H(x + c) −H(x −c)] = sin(cω)
πω
Solution 32.2
F

1
x2 + c2

= 1
2π
Z ∞
−∞
1
x2 + c2 e−ıωx dx
= 1
2π
Z ∞
−∞
e−ıωx
(x −ıc)(x + ıc) dx
If ℜ(ω) < 0 then we close the path of integration with a semi-circle in the upper half plane.
F

1
x2 + c2

= 1
2π2πi Res

e−ıωx
(x −ıc)(x + ıc), x = ıc

= 1
2c ecω
1561

If ω > 0 then we close the path of integration in the lower half plane.
F

1
x2 + c2

= −1
2π2πi Res

e−ıωx
(x −ıc)(x + ıc), −ıc

= 1
2c e−cω
Thus we have that
F

1
x2 + c2

= 1
2c e−c|ω|,
for ℜ(c) ̸= 0.
Solution 32.3
Fs[y′] = 1
π
Z ∞
0
y′ sin(ωx) dx
= 1
π
h
y sin(ωx)
i∞
0 −ω
π
Z ∞
0
y cos(ωx) dx
= −ωˆyc(ω)
Fs[y′′] = 1
π
Z ∞
0
y′′ sin(ωx) dx
= 1
π
h
y′ sin(ωx)
i∞
0 −ω
π
Z ∞
0
y′ cos(ωx) dx
= −ω
π
h
y cos(ωx)
i∞
0 −ω2
π
Z ∞
0
y sin(ωx) dx
= −ω2ˆys(ω) + ω
π y(0).
1562

Solution 32.4
1.
F[f(x −a)] = 1
2π
Z ∞
−∞
f(x −a) e−ıωx dx
= 1
2π
Z ∞
−∞
f(x) e−ıω(x+a) dx
= e−ıωa 1
2π
Z ∞
−∞
f(x) e−ıωx dx
F[f(x −a)] = e−ıωa ˆf(ω)
2. If a > 0, then
F[f(ax)] = 1
2π
Z ∞
−∞
f(ax) e−ıωx dx
= 1
2π
Z ∞
−∞
f(ξ) e−ıωξ/a 1
a dξ
= 1
a
ˆf
ω
a

.
If a < 0, then
F[f(ax)] = 1
2π
Z ∞
−∞
f(ax) e−ıωx dx
= 1
2π
Z −∞
∞
e−ıωξ/a 1
a dξ
= −1
a
ˆf
ω
a

.
Thus
F[f(ax)] = 1
|a|
ˆf
ω
a

.
1563

Solution 32.5
Fs[f(x)g(x)] = 1
π
Z ∞
0
f(x)g(x) sin(ωx) dx
= 1
π
Z ∞
0

2
Z ∞
0
ˆfs(η) sin(ηx) dη

g(x) sin(ωx) dx
= 2
π
Z ∞
0
Z ∞
0
ˆfs(η)g(x) sin(ηx) sin(ωx) dx dη
Use the identity, sin a sin b = 1
2[cos(a −b) −cos(a + b)].
= 1
π
Z ∞
0
Z ∞
0
ˆfs(η)g(x)
h
cos((ω −η)x) −cos((ω + η)x)
i
dx dη
=
Z ∞
0
ˆfs(η)
 1
π
Z ∞
0
g(x) cos((ω −η)x) dx −1
π
Z ∞
0
g(x) cos((ω + η)x) dx

dη
Fs[f(x)g(x)] =
Z ∞
0
ˆfs(η)

Gc(|ω −η|) −Gc(ω + η)

dη
1564

Solution 32.6
F−1
s [ ˆfs(ω)Gc(ω)] = 2
Z ∞
0
ˆfs(ω)Gc(ω) sin(ωx) dω
= 2
Z ∞
0
 1
π
Z ∞
0
f(ξ) sin(ωξ) dξ

Gc(ω) sin(ωx) dω
= 2
π
Z ∞
0
Z ∞
0
f(ξ)Gc(ω) sin(ωξ) sin(ωx) dω dξ
= 1
π
Z ∞
0
Z ∞
0
f(ξ)Gc(ω)
h
cos(ω(x −ξ)) −cos(ω(x + ξ))
i
dω dξ
= 1
2π
Z ∞
0
f(ξ)

2
Z ∞
0
Gc(ω) cos(ω(x −ξ)) dω −2
Z ∞
0
Gc(ω) cos(ω(x + ξ)) dω)

dξ
= 1
2π
Z ∞
0
f(ξ)[g(x −ξ) −g(x + ξ)] dξ
F−1
s [ ˆfs(ω)Gc(ω)] = 1
2π
Z ∞
0
f(ξ)

g(|x −ξ|) −g(x + ξ)

dξ
Solution 32.7
Fc[xf(x)] = 1
π
Z ∞
0
xf(x) cos(ωx) dx
= 1
π
Z ∞
0
f(x) ∂
∂ω(sin(ωx)) dx
= ∂
∂ω
1
π
Z ∞
0
f(x) sin(ωx) dx
= ∂
∂ω
ˆfs(ω)
1565

Fs[xf(x)] = 1
π
Z ∞
0
xf(x) sin(ωx) dx
= 1
π
Z ∞
0
f(x) ∂
∂ω(−cos(ωx)) dx
= −∂
∂ω
1
π
Z ∞
0
f(x) cos(ωx) dx
= −∂
∂ω
ˆfc(ω)
Solution 32.8
y′′ −y = e−2x,
y(0) = 1,
y(∞) = 0
We take the Fourier sine transform of the diﬀerential equation.
−ω2ˆys(ω) + ω
π y(0) −ˆys(ω) = 2ω/π
ω2 + 4
ˆys(ω) = −
ω/π
(ω2 + 4)(ω2 + 1) +
ω/π
(ω2 + 1)
= ω/(3π)
ω2 + 4 −ω/(3π)
ω2 + 1 +
ω/π
ω2 + 1
= 2
3
ω/π
ω2 + 1 + 1
3
ω/π
ω2 + 4
y = 2
3 e−x +1
3 e−2x
Solution 32.9
Consider the Fourier sine transform. Let f(x) be an odd function.
Fs[f(x)] = 1
π
Z ∞
0
f(x) sin(ωx) dx
1566

Extend the integration because the integrand is even.
= 1
2π
Z ∞
−∞
f(x) sin(ωx) dx
Note that
R ∞
−∞f(x) cos(ωx) dx = 0 as the integrand is odd.
= 1
2π
Z ∞
−∞
f(x)ı e−ıωx dx
= ıF[f(x)]
Fs[f(x)] = ıF[f(x)],
for odd f(x).
For general f(x), use the odd extension, sign(x)f(|x|) to write the result.
Fs[f(x)] = ıF[sign(x)f(|x|)]
Now consider the inverse Fourier sine transform. Let ˆf(ω) be an odd function.
F−1
s
h
ˆf(ω)
i
= 2
Z ∞
0
ˆf(ω) sin(ωx) dω
Extend the integration because the integrand is even.
=
Z ∞
−∞
ˆf(ω) sin(ωx) dω
Note that
R ∞
−∞ˆf(ω) cos(ωx) dω = 0 as the integrand is odd.
=
Z ∞
−∞
ˆf(ω)(−i) eıωx dω
= −ıF−1 h
ˆf(ω)
i
1567

F−1
s
h
ˆf(ω)
i
= −ıF−1 h
ˆf(ω)
i
,
for odd ˆf(ω).
For general ˆf(ω), use the odd extension, sign(ω) ˆf(|ω|) to write the result.
F−1
s
h
ˆf(ω)
i
= −ıF−1 h
sign(ω) ˆf(|ω|)
i
Solution 32.10
Fc[xf(x)] = 1
π
Z ∞
0
xf(x) cos(ωx) dx
= 1
π
Z ∞
0
f(x) ∂
∂ω sin(ωx) dx
= ∂
∂ω
1
π
Z ∞
0
f(x) sin(ωx) dx
= ∂
∂ω
ˆfs(ω)
Fs[xf(x)] = 1
π
Z ∞
0
xf(x) sin(ωx) dx
= 1
π
Z ∞
0
f(x) ∂
∂ω(−cos(ωx)) dx
= −∂
∂ω
1
π
Z ∞
0
f(x) cos(ωx) dx
= −∂
∂ω
ˆfc(ω)
1568

Fc[f(cx)] = 1
π
Z ∞
0
f(cx) cos(ωx) dx
= 1
π
Z ∞
0
f(ξ) cos
ω
c ξ
 dξ
c
= 1
c
ˆfc
ω
c

Fs[f(cx)] = 1
π
Z ∞
0
f(cx) sin(ωx) dx
= 1
π
Z ∞
0
f(ξ) sin
ω
c ξ
 dξ
c
= 1
c
ˆfs
ω
c

Solution 32.11
Z ∞
−∞
u(ξ) e−a(x−ξ)2 dξ = e−bx2
We take the Fourier transform and solve for U(ω).
2πU(ω)F
h
e−ax2i
= F
h
e−bx2i
2πU(ω)
1
√
4πa
e−ω2/(4a) =
1
√
4πb
e−ω2/(4b)
U(ω) = 1
2π
ra
b e−ω2(a−b)/(4ab)
Now we take the inverse Fourier transform.
U(ω) = 1
2π
ra
b
p
4πab/(a −b)
p
4πab/(a −b)
e−ω2(a−b)/(4ab)
1569

u(x) =
a
p
π(a −b)
e−abx2/(a−b)
Solution 32.12
I = 1
π
Z ∞
0
1
x e−cx sin(ωx) dx
= 1
π
Z ∞
0
Z ∞
c
e−zx dz

sin(ωx) dx
= 1
π
Z ∞
c
Z ∞
0
e−zx sin(ωx) dx dz
= 1
π
Z ∞
c
ω
z2 + ω2 dz
= 1
π
h
arctan
 z
ω
i∞
c
= 1
π
π
2 −arctan
 c
ω

= 1
π arctan
ω
c

Solution 32.13
We consider the diﬀerential equation
y′′ −a2y = e−a|x|
on the domain −∞< x < ∞with boundary conditions y(±∞) = 0. We take the Fourier transform of the diﬀerential
equation and solve for ˆy(ω).
−ω2ˆy −a2ˆy =
a
π(ω2 + a2)
ˆy(ω) = −
a
π(ω2 + a2)2
1570

We take the inverse Fourier transform to ﬁnd the solution of the diﬀerential equation.
y(x) =
Z ∞
−∞
−
a
π(ω2 + a2)2 eıxω dω
Note that since ˆy(ω) is a real-valued, even function, y(x) is a real-valued, even function. Thus we only need to evaluate
the integral for positive x. If we replace x by |x| in this expression we will have the solution that is valid for all x.
For x ≥0, we evaluate the integral by closing the path of integration in the upper half plane and using the Residue
Theorem and Jordan’s Lemma.
y(x) = −a
π
Z ∞
−∞
1
(ω −ıa)2(ω + ıa)2 eıxω dω
= −ı2π a
π Res

1
(ω −ıa)2(ω + ıa)2 eıxω, ω = ıa

= −ı2a lim
ω→ıa
d
dω

eıxω
(ω + ıa)2

= −ı2a lim
ω→ıa
 ıx eıxω
(ω + ıa)2 −
2 eıxω
(ω + ıa)3

= −ı2a
ıx e−ax
−4a2 −2 e−ax
−ı8a3

= −(1 + ax) e−ax
2a2
The solution of the diﬀerential equation is
y(x) = −1
2a2(1 + a|x|) e−a|x| .
1571

Solution 32.14
1. We take the Fourier cosine transform of the diﬀerential equation.
−ω2ˆy(ω) −b
π −a2ˆy(ω) = 0
ˆy(ω) = −
b
π(ω2 + a2)
Now we take the inverse Fourier cosine transform. We use the fact that ˆy(ω) is an even function.
y(x) = F−1
c

−
b
π(ω2 + a2)

= F−1

−
b
π(ω2 + a2)

= −b
πı2π Res

1
ω2 + a2 eıωx, ω = ıa

= −ı2b lim
ω→ıa
 eıωx
ω + ıa

,
for x ≥0
y(x) = −b
a e−ax
2. The Green function problem is
G′′ −a2G = δ(x −ξ) on x, ξ > 0,
G′(0; ξ) = 0,
G(∞; ξ) = 0.
We take the Fourier cosine transform and solve for ˆG(ω; ξ).
−ω2 ˆG −a2 ˆG = Fc[δ(x −ξ)]
ˆG(ω; ξ) = −
1
ω2 + a2Fc[δ(x −ξ)]
1572

We express the right side as a product of Fourier cosine transforms.
ˆG(ω; ξ) = −π
aFc[e−ax]Fc[δ(x −ξ)]
Now we can apply the Fourier cosine convolution theorem.
F−1
c
[Fc[f(x)]Fc[g(x)]] = 1
2π
Z ∞
0
f(t)
 g(|x −t|) + g(x + t)

dt
G(x; ξ) = −π
a
1
2π
Z ∞
0
δ(t −ξ)
  e−a|x−t| + e−a(x+t) 
dt
G(x; ξ) = −1
2a
 e−a|x−ξ| + e−a(x+ξ)
Solution 32.15
1. We take the Fourier sine transform of the diﬀerential equation.
−ω2ˆy(ω) + bω
π −a2ˆy(ω) = 0
ˆy(ω) =
bω
π(ω2 + a2)
Now we take the inverse Fourier sine transform. We use the fact that ˆy(ω) is an odd function.
y(x) = F−1
s

bω
π(ω2 + a2)

= −ıF−1

bω
π(ω2 + a2)

= −ı b
πı2π Res

ω
ω2 + a2 eıωx, ω = ıa

= 2b lim
ω→ıa
 ω eıωx
ω + ıa

= b e−ax
for x ≥0
1573

y(x) = b e−ax
2. Now we solve the diﬀerential equation with the Laplace transform.
y′′ −a2y = 0
s2ˆy(s) −sy(0) −y′(0) −a2ˆy(s) = 0
We don’t know the value of y′(0), so we treat it as an unknown constant.
ˆy(s) = bs + y′(0)
s2 −a2
y(x) = b cosh(ax) + y′(0)
a
sinh(ax)
In order to satisfy the boundary condition at inﬁnity we must choose y′(0) = −ab.
y(x) = b e−ax
We see that solving the diﬀerential equation with the Laplace transform is not as convenient, because the boundary
condition at inﬁnity is not automatically satisﬁed. We had to ﬁnd a value of y′(0) so that y(∞) = 0.
3. The Green function problem is
G′′ −a2G = δ(x −ξ) on x, ξ > 0,
G(0; ξ) = 0,
G(∞; ξ) = 0.
We take the Fourier sine transform and solve for ˆG(ω; ξ).
−ω2 ˆG −a2 ˆG = Fs[δ(x −ξ)]
ˆG(ω; ξ) = −
1
ω2 + a2Fs[δ(x −ξ)]
1574

We write the right side as a product of Fourier cosine transforms and sine transforms.
ˆG(ω; ξ) = −π
aFc[e−ax]Fs[δ(x −ξ)]
Now we can apply the Fourier sine convolution theorem.
F−1
s
[Fs[f(x)]Fc[g(x)]] = 1
2π
Z ∞
0
f(t)
 g(|x −t|) −g(x + t)

dt
G(x; ξ) = −π
a
1
2π
Z ∞
0
δ(t −ξ)
  e−a|x−t| −e−a(x+t) 
dt
G(x; ξ) = 1
2a
 e−a(x−ξ) −e−a|x+ξ|
Solution 32.16
1. We take the Fourier transform of the diﬀerential equation, solve for ˆG and then invert.
G′′ + 2µG′ +
 β2 + µ2
G = δ(x −ξ)
−ω2 ˆG + ı2µω ˆG +
 β2 + µ2 ˆG = e−ıωξ
2π
ˆG = −
e−ıωξ
2π (ω2 −ı2µω −β2 −µ2)
G =
Z ∞
−∞
−
e−ıωξ eıωx
2π(ω2 −ı2µω −β2 −µ2) dω
G = −1
2π
Z ∞
−∞
eıω(x−ξ)
(ω + β −ıµ)(ω −β −ıµ) dω
For x > ξ we close the path of integration in the upper half plane and use the Residue theorem. There are two
simple poles in the upper half plane. For x < ξ we close the path of integration in the lower half plane. Since
1575

the integrand is analytic there, the integral is zero. G(x; ξ) = 0 for x < ξ. For x > ξ we have
G(x; ξ) = −1
2πı2π
 
Res

eıω(x−ξ)
(ω + β −ıµ)(ω −β −ıµ), ω = −β + ıµ

+ Res

eıω(x−ξ)
(ω + β −ıµ)(ω −β −ıµ), ω = −β −ıµ
 !
G(x; ξ) = −ı
eı(−β+ıµ)(x−ξ)
−2β
+ eı(β+ıµ)(x−ξ)
2β

G(x; ξ) = 1
β e−µ(x−ξ) sin(β(x −ξ)).
Thus the Green function is
G(x; ξ) = 1
β e−µ(x−ξ) sin(β(x −ξ))H(x −ξ).
2. We use the Green function to ﬁnd the solution of the inhomogeneous equation.
y′′ + 2µy′ +
 β2 + µ2
y = g(x),
y(−∞) = y(∞) = 0
y(x) =
Z ∞
−∞
g(ξ)G(x; ξ) dξ
y(x) =
Z ∞
−∞
g(ξ) 1
β e−µ(x−ξ) sin(β(x −ξ))H(x −ξ) dξ
y(x) = 1
β
Z x
−∞
g(ξ) e−µ(x−ξ) sin(β(x −ξ)) dξ
We take the limit µ →0.
y = 1
β
Z x
−∞
g(ξ) sin(β(x −ξ)) dξ
1576

Solution 32.17
First we consider the Fourier transform of f(x) = 1/(x2 + c2) where ℜ(c) > 0.
ˆf(ω) = F

1
x2 + c2

= 1
2π
Z ∞
−∞
1
x2 + c2 e−ıωx dx
= 1
2π
Z ∞
−∞
e−ıωx
(x −ıc)(x + ıc) dx
If ω < 0 then we close the path of integration with a semi-circle in the upper half plane.
ˆf(ω) = 1
2π2πi Res

e−ıωx
(x −ıc)(x + ıc), x = ıc

= ecω
2c ,
for ω < 0
Note that f(x) = 1/(x2 + c2) is an even function of x so that ˆf(ω) is an even function of ω. If ˆf(ω) = g(ω) for ω < 0
then f(ω) = g(−|ω|) for all ω. Thus
F

1
x2 + c2

= 1
2c e−c|ω| .
Now we consider the integral equation
Z ∞
−∞
u(ξ)
[(x −ξ)2 + a2] dξ =
1
x2 + b2
0 < a < b.
1577

We take the Fourier transform, utilizing the convolution theorem.
2πˆu(ω)e−a|ω|
2a
= e−b|ω|
2b
ˆu(ω) = a e−(b−a)|ω|
2πb
u(x) =
a
2πb2(b −a)
1
x2 + (b −a)2
u(x) =
a(b −a)
πb(x2 + (b −a)2)
Solution 32.18
1. Note that ˆfc(ω) is an even function. We compute the inverse Fourier cosine transform.
f(x) = F−1
c
h
ˆfc(ω)
i
=
Z ∞
−∞
ˆfc(ω) eıωx dω
=
Z ∞
−∞
ˆfc(ω)(cos(ωx) + ı sin(ωx)) dω
=
Z ∞
−∞
ˆfc(ω) cos(ωx) dω
= 2
Z ∞
0
ˆfc(ω) cos(ωx) dω
1578

2.
Fc [y′′] = 1
π
Z ∞
0
y′′ cos(ωx) dx
= 1
π [y′ cos(ωx)]∞
0 + ω
π
Z ∞
0
y′ sin(ωx) dx
= −1
πy′(0) + ω
π [y sin(ωx)]∞
0 −ω2
π
Z ∞
0
y cos(ωx) dx
Fc[y′′] = −ω2ˆyc(ω) −y′(0)
π
3. We take the Fourier cosine transform of the diﬀerential equation.
−ω2ˆy(ω) −b
π −a2ˆy(ω) = 0
ˆy(ω) = −
b
π(ω2 + a2)
Now we take the inverse Fourier cosine transform. We use the fact that ˆy(ω) is an even function.
y(x) = F−1
c

−
b
π(ω2 + a2)

= F−1

−
b
π(ω2 + a2)

= −b
πı2π Res

1
ω2 + a2 eıωx, ω = ıa

= −ı2b lim
ω→ıa
 eıωx
ω + ıa

,
for x ≥0
y(x) = −b
a e−ax
1579

Solution 32.19
1. Suppose f(x) is an odd function. The Fourier transform of f(x) is
F[f(x)] = 1
2π
Z ∞
−∞
f(x) e−ıωx dx
= 1
2π
Z ∞
−∞
f(x)(cos(ωx) −ı sin(ωx)) dx
= −ı
π
Z ∞
0
f(x) sin(ωx) dx.
Note that ˆf(ω) = F[f(x)] is an odd function of ω. The inverse Fourier transform of ˆf(ω) is
F−1[ ˆf(ω)] =
Z ∞
−∞
ˆf(ω) eıωx dω
= 2ı
Z ∞
0
ˆf(ω) sin(ωx) dω.
Thus we have that
f(x) = 2ı
Z ∞
0

−ı
π
Z ∞
0
f(x) sin(ωx) dx

sin(ωx) dω
= 2
Z ∞
0
 1
π
Z ∞
0
f(x) sin(ωx) dx

sin(ωx) dω.
This gives us the Fourier sine transform pair
f(x) = 2
Z ∞
0
ˆfs(ω) sin(ωx) dω,
ˆfs(ω) = 1
π
Z ∞
0
f(x) sin(ωx) dx.
1580

2.
Fs[y′′] = 1
π
Z ∞
0
y′′ sin(ωx) dx
= 1
π
h
y′ sin(ωx)
i∞
0 −ω
π
Z ∞
0
y′ cos(ωx) dx
= −ω
π
h
y cos(ωx)
i∞
0 −ω2
π
Z ∞
0
y sin(ωx) dx
Fs[y′′] = −ω2ˆys(ω) + ω
π y(0)
3. We take the Fourier sine transform of the diﬀerential equation.
−ω2ˆy(ω) + bω
π −a2ˆy(ω) = 0
ˆy(ω) =
bω
π(ω2 + a2)
Now we take the inverse Fourier sine transform. We use the fact that ˆy(ω) is an odd function.
y(x) = F−1
s

bω
π(ω2 + a2)

= −ıF−1

bω
π(ω2 + a2)

= −ı b
πı2π Res

ω
ω2 + a2 eıωx, ω = ıa

= 2b lim
ω→ıa
 ω eıωx
ω + ıa

= b e−ax
for x ≥0
y(x) = b e−ax
1581

4. Now we solve the diﬀerential equation with the Laplace transform.
y′′ −a2y = 0
s2ˆy(s) −sy(0) −y′(0) −a2ˆy(s) = 0
We don’t know the value of y′(0), so we treat it as an unknown constant.
ˆy(s) = bs + y′(0)
s2 −a2
y(x) = b cosh(ax) + y′(0)
a
sinh(ax)
In order to satisfy the boundary condition at inﬁnity we must choose y′(0) = −ab.
y(x) = b e−ax
We see that solving the diﬀerential equation with the Laplace transform is not as convenient, because the boundary
condition at inﬁnity is not automatically satisﬁed. We had to ﬁnd a value of y′(0) so that y(∞) = 0.
Solution 32.20
The Fourier, Fourier cosine and Fourier sine transforms are deﬁned:
F[f(x)] = 1
2π
Z ∞
−∞
f(x) e−ıωx dx,
F[f(x)]c = 1
π
Z ∞
0
f(x) cos(ωx) dx,
F[f(x)]s = 1
π
Z ∞
0
f(x) sin(ωx) dx.
We start with the right side of the identity and apply the usual tricks of integral calculus to reduce the expression to
the left side.
1
2 (Fc[f(x) + f(−x)] −ıFs[f(x) −f(−x)])
1582

1
2π
Z ∞
0
f(x) cos(ωx) dx +
Z ∞
0
f(−x) cos(ωx) dx −ı
Z ∞
0
f(x) sin(ωx) dx + ı
Z ∞
0
f(−x) sin(ωx) dx

1
2π
Z ∞
0
f(x) cos(ωx) dx −
Z −∞
0
f(x) cos(−ωx) dx −ı
Z ∞
0
f(x) sin(ωx) dx −ı
Z −∞
0
f(x) sin(−ωx) dx

1
2π
Z ∞
0
f(x) cos(ωx) dx +
Z 0
−∞
f(x) cos(ωx) dx −ı
Z ∞
0
f(x) sin(ωx) dx −ı
Z 0
−∞
f(x) sin(ωx) dx

1
2π
Z ∞
−∞
f(x) cos(ωx) dx −ı
Z ∞
−∞
f(x) sin(ωx) dx

1
2π
Z ∞
−∞
f(x) e−ıωx dx
F[f(x)]
Solution 32.21
We take the Fourier transform of the integral equation, noting that the left side is the convolution of u(x) and
1
x2+a2.
2πˆu(ω)F

1
x2 + a2

= F

1
x2 + b2

We ﬁnd the Fourier transform of f(x) =
1
x2+c2. Note that since f(x) is an even, real-valued function, ˆf(ω) is an
even, real-valued function.
F

1
x2 + c2

= 1
2π
Z ∞
−∞
1
x2 + c2 e−ıωx dx
1583

For x > 0 we close the path of integration in the upper half plane and apply Jordan’s Lemma to evaluate the integral
in terms of the residues.
= 1
2πı2π Res

e−ıωx
(x −ıc)(x + ıc), x = ıc

= ıe−ıωıc
2ıc
= 1
2c e−cω
Since ˆf(ω) is an even function, we have
F

1
x2 + c2

= 1
2c e−c|ω| .
Our equation for ˆu(ω) becomes,
2πˆu(ω) 1
2a e−a|ω| = 1
2b e−b|ω|
ˆu(ω) =
a
2πb e−(b−a)|ω| .
We take the inverse Fourier transform using the transform pair we derived above.
u(x) =
a
2πb
2(b −a)
x2 + (b −a)2
u(x) =
a(b −a)
πb(x2 + (b −a)2)
1584

Chapter 33
The Gamma Function
33.1
Euler’s Formula
For non-negative, integral n the factorial function is
n! = n(n −1) · · · (1),
with
0! = 1.
We would like to extend the factorial function so it is deﬁned for all complex numbers.
Consider the function Γ(z) deﬁned by Euler’s formula
Γ(z) =
Z ∞
0
e−t tz−1 dt.
(Here we take the principal value of tz−1.) The integral converges for ℜ(z) > 0. If ℜ(z) ≤0 then the integrand will
be at least as singular as 1/t at t = 0 and thus the integral will diverge.
1585

Diﬀerence Equation.
Using integration by parts,
Γ(z + 1) =
Z ∞
0
e−t tz dt
=
h
−e−t tzi∞
0 −
Z ∞
0
−e−t ztz−1 dt.
Since ℜ(z) > 0 the ﬁrst term vanishes.
= z
Z ∞
0
e−t tz−1 dt
= zΓ(z)
Thus Γ(z) satisﬁes the diﬀerence equation
Γ(z + 1) = zΓ(z).
For general z it is not possible to express the integral in terms of elementary functions. However, we can evaluate
the integral for some z. The value z = 1 looks particularly simple to do.
Γ(1) =
Z ∞
0
e−t dt =
h
−e−t i∞
0 = 1.
Using the diﬀerence equation we can ﬁnd the value of Γ(n) for any positive, integral n.
Γ(1) = 1
Γ(2) = 1
Γ(3) = (2)(1) = 2
Γ(4) = (3)(2)(1) = 6
· · · = · · ·
Γ(n + 1) = n!.
1586

Thus the Gamma function, Γ(z), extends the factorial function to all complex z in the right half-plane. For non-
negative, integral n we have
Γ(n + 1) = n!.
Analyticity.
The derivative of Γ(z) is
Γ′(z) =
Z ∞
0
e−t tz−1 log t dt.
Since this integral converges for ℜ(z) > 0, Γ(z) is analytic in that domain.
33.2
Hankel’s Formula
We would like to ﬁnd the analytic continuation of the Gamma function into the left half-plane. We accomplish this
with Hankel’s formula
Γ(z) =
1
ı2 sin(πz)
Z
C
et tz−1 dt.
Here C is the contour starting at −∞below the real axis, enclosing the origin and returning to −∞above the real
axis. A graph of this contour is shown in Figure 33.1. Again we use the principle value of tz−1 so there is a branch cut
on the negative real axis.
The integral in Hankel’s formula converges for all complex z. For non-positive, integral z the integral does not
vanish. Thus because of the sine term the Gamma function has simple poles at z = 0, −1, −2, . . .. For positive,
integral z, the integrand is entire and thus the integral vanishes. Using L’Hospital’s rule you can show that the points,
z = 1, 2, 3, . . . are removable singularities and the Gamma function is analytic at these points. Since the only zeroes of
sin(πz) occur for integral z, Γ(z) is analytic in the entire plane except for the points, z = 0, −1, −2, . . ..
1587

Figure 33.1: The Hankel Contour.
Diﬀerence Equation.
Using integration by parts we can derive the diﬀerence equation from Hankel’s formula.
Γ(z + 1) =
1
ı2 sin(π(z + 1))
Z
C
et tz dt
=
1
−ı2 sin(πz)
h
et tzi−∞+ı0
−∞−ı0 −
Z
C
et ztz−1 dt

=
1
ı2 sin(πz)z
Z
C
et tz−1 dt
= zΓ(z).
Evaluating Γ(1),
Γ(1) = lim
z→1
R
C et tz−1 dt
ı2 sin(πz) .
1588

Both the numerator and denominator vanish. Using L’Hospital’s rule,
= lim
z→1
R
C et tz−1 log t dt
ı2π cos(πz)
=
R
C et log t dt
ı2π
Let Cr be the circle of radius r starting at −π radians and going to π radians.
=
1
ı2π
Z −r
−∞
et[log(−t) −πi] dt +
Z
Cr
et log t dt +
Z −∞
−r
et[log(−t) + πi] dt

=
1
ı2π
Z −∞
−r
et[−log(−t) + πi] dt +
Z −∞
−r
et[log(−t) + πi] dt +
Z
Cr
et log t dt

=
1
ı2π
Z −∞
−r
et ı2π dt +
Z
Cr
et log t dt

The integral on Cr vanishes as r →0.
=
1
ı2πı2π
Z −∞
0
et dt
= 1.
Thus we obtain the same value as with Euler’s formula. It can be shown that Hankel’s formula is the analytic continuation
of the Gamma function into the left half-plane.
33.3
Gauss’ Formula
Gauss deﬁned the Gamma function as an inﬁnite product. This form is useful in deriving some of its properties. We
can obtain the product form from Euler’s formula. First recall that
e−t = lim
n→∞

1 −t
n
n
.
1589

Substituting this into Euler’s formula,
Γ(z) =
Z ∞
0
e−t tz−1 dt
= lim
n→∞
Z n
0

1 −t
n
n
tz−1 dt.
With the substitution τ = t/n,
= lim
n→∞
Z 1
0
(1 −τ)nnz−1τ z−1n dτ
= lim
n→∞nz
Z 1
0
(1 −τ)nτ z−1 dτ.
Let n be an integer. Using integration by parts we can evaluate the integral.
Z 1
0
(1 −τ)nτ z−1 dτ =
(1 −τ)nτ z
z
1
0
−
Z 1
0
−n(1 −τ)n−1τ z
z dτ
= n
z
Z 1
0
(1 −τ)n−1τ z dτ
= n(n −1)
z(z + 1)
Z 1
0
(1 −τ)n−2τ z+1 dτ
=
n(n −1) · · · (1)
z(z + 1) · · · (z + n −1)
Z 1
0
τ z+n−1 dτ
=
n(n −1) · · · (1)
z(z + 1) · · · (z + n −1)
 τ z+n
z + n
1
0
=
n!
z(z + 1) · · · (z + n)
1590

Thus we have that
Γ(z) = lim
n→∞nz
n!
z(z + 1) · · · (z + n)
= 1
z lim
n→∞
(1)(2) · · · (n)
(z + 1)(z + 2) · · · (z + n)nz
= 1
z lim
n→∞
1
(1 + z)(1 + z/2) · · · (1 + z/n)nz
= 1
z lim
n→∞
1
(1 + z)(1 + z/2) · · · (1 + z/n)
2z3z · · · nz
1z2z · · · (n −1)z
Since limn→∞
(n+1)z
nz
= 1 we can multiply by that factor.
= 1
z lim
n→∞
1
(1 + z)(1 + z/2) · · · (1 + z/n)
2z3z · · · (n + 1)z
1z2z · · · nz
= 1
z
∞
Y
n=1

1
1 + z/n
(n + 1)z
nz

Thus we have Gauss’ formula for the Gamma function
Γ(z) = 1
z
∞
Y
n=1

1 + 1
n
z 
1 + z
n
−1
.
We derived this formula from Euler’s formula which is valid only in the left half-plane. However, the product formula
is valid for all z except z = 0, −1, −2, . . ..
33.4
Weierstrass’ Formula
1591

The Euler-Mascheroni Constant.
Before deriving Weierstrass’ product formula for the Gamma function we will
need to deﬁne the Euler-Mascheroni constant
γ = lim
n→∞

1 + 1
2 + 1
3 + · · · + 1
n

−log n

= 0.5772 · · · .
In deriving the Euler product formula, we had the equation
Γ(z) = lim
n→∞

nz
n!
z(z + 1) · · · (z + n)

.
= lim
n→∞

z−1 
1 + z
1
−1 
1 + z
2
−1
· · ·

1 + z
n
−1
nz

1
Γ(z) = lim
n→∞
h
z

1 + z
1
 
1 + z
2

· · ·

1 + z
n

e−z log ni
= lim
n→∞

z

1 + z
1

e−z 
1 + z
2

e−z/2 · · ·

1 + z
n

e−z/n exp

1 + 1
2 + · · · + 1
n −log n

z

Weierstrass’ formula for the Gamma function is then
1
Γ(z) = z eγz
∞
Y
n=1
h
1 + z
n

e−z/ni
.
Since the product is uniformly convergent, 1/Γ(z) is an entire function. Since 1/Γ(z) has no singularities, we see
that Γ(z) has no zeros.
1592

Result 33.4.1 Euler’s formula for the Gamma function is valid for ℜ(z) > 0.
Γ(z) =
Z ∞
0
e−t tz−1 dt
Hankel’s formula deﬁnes the Γ(z) for the entire complex plane except for the points z =
0, −1, −2, . . ..
Γ(z) =
1
ı2 sin(πz)
Z
C
et tz−1 dt
Gauss’ and Weierstrass’ product formulas are, respectively
Γ(z) = 1
z
∞
Y
n=1

1 + 1
n
z 
1 + z
n
−1
and
1
Γ(z) = z eγz
∞
Y
n=1
h
1 + z
n

e−z/ni
.
33.5
Stirling’s Approximation
In this section we will try to get an approximation to the Gamma function for large positive argument. Euler’s formula
is
Γ(x) =
Z ∞
0
e−t tx−1 dt.
We could ﬁrst try to approximate the integral by only looking at the domain where the integrand is large. In Figure 33.2
the integrand in the formula for Γ(10), e−t t9, is plotted.
1593

5
10
15
20
25
30
10000
20000
30000
40000
Figure 33.2: Plot of the integrand for Γ(10)
We see that the ”important” part of the integrand is the hump centered around x = 9. If we ﬁnd where the
integrand of Γ(x) has its maximum
d
dx
 e−t tx−1
= 0
−e−t tx−1 + (x −1) e−t tx−2 = 0
(x −1) −t = 0
t = x −1,
we see that the maximum varies with x. This could complicate our analysis. To take care of this problem we introduce
1594

the change of variables t = xs.
Γ(x) =
Z ∞
0
e−xs(xs)x−1x ds
= xx
Z ∞
0
e−xs sxs−1 ds
= xx
Z ∞
0
e−x(s−log s) s−1 ds
The integrands, (e−x(s−log s) s−1), for Γ(5) and Γ(20) are plotted in Figure 33.3.
1
2
3
4
0.001
0.002
0.003
0.004
0.005
0.006
0.007
1
2
3
4
5·10-10
1·10-9
1.5·10-9
2·10-9
Figure 33.3: Plot of the integrand for Γ(5) and Γ(20).
We see that the important part of the integrand is the hump that seems to be centered about s = 1. Also note
that the the hump becomes narrower with increasing x. This makes sense as the e−x(s−log s) term is the most rapidly
varying term. Instead of integrating from zero to inﬁnity, we could get a good approximation to the integral by just
integrating over some small neighborhood centered at s = 1. Since s −log s has a minimum at s = 1, e−x(s−log s)
has a maximum there. Because the important part of the integrand is the small area around s = 1, it makes sense to
1595

approximate s −log s with its Taylor series about that point.
s −log s = 1 + 1
2(s −1)2 + O

(s −1)3
Since the hump becomes increasingly narrow with increasing x, we will approximate the 1/s term in the integrand with
its value at s = 1. Substituting these approximations into the integral, we obtain
Γ(x) ∼xx
Z 1+ϵ
1−ϵ
e−x(1+(s−1)2/2) ds
= xx e−x
Z 1+ϵ
1−ϵ
e−x(s−1)2/2 ds
As x →∞both of the integrals
Z 1−ϵ
−∞
e−x(s−1)2/2 ds
and
Z ∞
1+ϵ
e−x(s−1)2/2 ds
are exponentially small. Thus instead of integrating from 1 −ϵ to 1 + ϵ we can integrate from −∞to ∞.
Γ(x) ∼xx e−x
Z ∞
−∞
e−x(s−1)2/2 ds
= xx e−x
Z ∞
−∞
e−xs2/2 ds
= xx e−x
r
2π
x
Γ(x) ∼
√
2πxx−1/2 e−x
as x →∞.
This is known as Stirling’s approximation to the Gamma function. In the table below, we see that the approximation
is pretty good even for relatively small argument.
1596

n
Γ(n)
√
2πxx−1/2 e−x
relative error
5
24
23.6038
0.0165
15
8.71783 · 1010
8.66954 · 1010
0.0055
25
6.20448 · 1023
6.18384 · 1023
0.0033
35
2.95233 · 1038
2.94531 · 1038
0.0024
45
2.65827 · 1054
2.65335 · 1054
0.0019
In deriving Stirling’s approximation to the Gamma function we did a lot of hand waving. However, all of the steps can
be justiﬁed and better approximations can be obtained by using Laplace’s method for ﬁnding the asymptotic behavior
of integrals.
1597

33.6
Exercises
Exercise 33.1
Given that
Z ∞
−∞
e−x2 dx = √π,
deduce the value of Γ(1/2). Now ﬁnd the value of Γ(n + 1/2).
Exercise 33.2
Evaluate
R ∞
0 e−x3 dx in terms of the gamma function.
Exercise 33.3
Show that
Z ∞
0
e−x sin(log x) dx = Γ(ı) + Γ(−ı)
2
.
1598

33.7
Hints
Hint 33.1
Use the change of variables, ξ = x2 in the integral. To ﬁnd the value of Γ(n + 1/2) use the diﬀerence relation.
Hint 33.2
Make the change of variable ξ = x3.
Hint 33.3
1599

33.8
Solutions
Solution 33.1
Z ∞
−∞
e−x2 dx = √π
Z ∞
0
e−x2 dx =
√π
2
Make the change of variables ξ = x2.
Z ∞
0
e−ξ 1
2ξ−1/2 dξ =
√π
2
Γ(1/2) = √π
Recall the diﬀerence relation for the Gamma function Γ(z + 1) = zΓ(z).
Γ(n + 1/2) = (n −1/2)Γ(n −1/2)
= 2n −1
2
Γ(n −1/2)
= (2n −3)(2n −1)
22
Γ(n −3/2)
= (1)(3)(5) · · · (2n −1)
2n
Γ(1/2)
Γ(n + 1/2) = (1)(3)(5) · · · (2n −1)
2n
√π
1600

Solution 33.2
We make the change of variable ξ = x3, x = ξ1/3, dx = 1
3ξ−2/3 dξ.
Z ∞
0
e−x3 dx =
Z ∞
0
e−ξ 1
3ξ−2/3 dξ
= 1
3Γ
1
3

Solution 33.3
Z ∞
0
e−x sin(log x) dx =
Z ∞
0
e−x 1
ı2
 eı log x −e−ı log x
dx
= 1
ı2
Z ∞
0
e−x  xı −x−ı
dx
= 1
ı2 (Γ(1 + ı) −Γ(1 −ı))
= 1
ı2 (ıΓ(ı) −(−ı)Γ(−ı))
= Γ(ı) + Γ(−ı)
2
1601

Chapter 34
Bessel Functions
Ideas are angels. Implementations are a bitch.
34.1
Bessel’s Equation
A commonly encountered diﬀerential equation in applied mathematics is Bessel’s equation
y′′ + 1
zy′ +

1 −ν2
z2

y = 0.
For our purposes, we will consider ν ∈R0+. This equation arises when solving certain partial diﬀerential equations
with the method of separation of variables in cylindrical coordinates. For this reason, the solutions of this equation are
sometimes called cylindrical functions.
This equation cannot be solved directly. However, we can ﬁnd series representations of the solutions. There is
a regular singular point at z = 0, so the Frobenius method is applicable there. The point at inﬁnity is an irregular
singularity, so we will look for asymptotic series about that point. Additionally, we will use Laplace’s method to ﬁnd
deﬁnite integral representations of the solutions.
1602

Note that Bessel’s equation depends only on ν2 and not ν alone. Thus if we ﬁnd a solution, (which of course
depends on this parameter), yν(z) we know that y−ν(z) is also a solution. For this reason, we will consider ν ∈R0+.
Whether or not yν(z) and y−ν(z) are linearly independent, (distinct solutions), remains to be seen.
Example 34.1.1 Consider the diﬀerential equation
y′′ + 1
zy′ + ν2
z2 y = 0
One solution is yν(z) = zν. Since the equation depends only on ν2, another solution is y−ν(z) = z−ν. For ν ̸= 0, these
two solutions are linearly independent.
Now consider the diﬀerential equation
y′′ + ν2y = 0
One solution is yν(z) = cos(νz). Therefore, another solution is y−ν(z) = cos(−νz) = cos(νz). However, these two
solutions are not linearly independent.
34.2
Frobeneius Series Solution about z = 0
We note that z = 0 is a regular singular point, (the only singular point of Bessel’s equation in the ﬁnite complex plane.)
We will use the Frobenius method at that point to analyze the solutions. We assume that ν ≥0.
The indicial equation is
α(α −1) + α −ν2 = 0
α = ±ν.
If ±ν do not diﬀer by an integer, (that is if ν is not a half-integer), then there will be two series solutions of the
Frobenius form.
y1(z) = zν
∞
X
k=0
akzk,
y2(z) = z−ν
∞
X
k=0
bkzk
1603

If ν is a half-integer, the second solution may or may not be in the Frobenius form. In any case, then will always be at
least one solution in the Frobenius form. We will determine that series solution. y(z) and it derivatives are
y =
∞
X
k=0
akzk+ν,
y′ =
∞
X
k=0
(k + ν)akzk+ν−1,
y′′ =
∞
X
k=0
(k + ν)(k + ν −1)akzk+ν−2.
We substitute the Frobenius series into the diﬀerential equation.
z2y′′ + zy′ +
 z2 −ν2
y = 0
∞
X
k=0
(k + ν)(k + ν −1)akzk+ν +
∞
X
k=0
(k + ν)akzk+ν +
∞
X
k=0
akzk+ν+2 −
∞
X
k=0
ν2akzk+ν = 0
∞
X
k=0
 k2 + 2kν

akzk +
∞
X
k=2
ak−2zk = 0
We equate powers of z to obtain equations that determine the coeﬃcients. The coeﬃcient of z0 is the equation
0 · a0 = 0. This corroborates that a0 is arbitrary, (but non-zero). The coeﬃcient of z1 is the equation
(1 + 2ν)a1 = 0
a1 = 0
The coeﬃcient of zk for k ≥2 gives us
 k2 + 2kν

ak + ak−2 = 0.
ak = −
ak−2
k2 + 2kν = −
ak−2
k(k + 2ν)
From the recurrence relation we see that all the odd coeﬃcients are zero, a2k+1 = 0. The even coeﬃcients are
a2k = −
a2k−2
4k(k + ν) =
(−1)ka0
22kk!Γ(k + ν + 1)
1604

Thus we have the series solution
y(z) = a0
∞
X
k=0
(−1)k
22kk!Γ(k + ν + 1)z2k.
a0 is arbitrary. We choose a0 = 2−ν. We call this solution the Bessel function of the ﬁrst kind and order ν and denote
it with Jν(z).
Jν(z) =
∞
X
k=0
(−1)k
k!Γ(k + ν + 1)
z
2
2k+ν
Recall that the Gamma function is non-zero and ﬁnite for all real arguments except non-positive integers. Γ(x)
has singularities at x = 0, −1, −2, . . ..
Therefore, J−ν(z) is well-deﬁned when ν is not a positive integer.
Since
J−ν(z) ∼z−ν at z = 0, J−ν(z) is clear linearly independent to Jν(z) for non-integer ν. In particular we note that there
are two solutions of the Frobenius form when ν is a half odd integer.
J−ν(z) =
∞
X
k=0
(−1)k
k!Γ(k −ν + 1)
z
2
2k−ν
,
for ν ̸∈Z+
Of course for ν = 0, Jν(z) and J−ν(z) are identical. Consider the case that ν = n is a positive integer. Since
Γ(x) →+∞as x →0, −1, −2, . . . we see the the coeﬃcients in the series for J−nu(z) vanish for k = 0, . . . , n −1.
J−n(z) =
∞
X
k=n
(−1)k
k!Γ(k −n + 1)
z
2
2k−n
J−n(z) =
∞
X
k=0
(−1)k+n
(k + n)!Γ(k + 1)
z
2
2k+n
J−n(z) = (−1)n
∞
X
k=0
(−1)k
k!(k + n)!
z
2
2k+n
J−n(z) = (−1)nJn(z)
Thus we see that J−n(z) and Jn(z) are not linearly independent for integer n.
1605

34.2.1
Behavior at Inﬁnity
With the change of variables z = 1/ζ, w(z) = u(ζ) Bessel’s equation becomes
ζ4u′′ + 2ζ3u′ + ζ
 −ζ2
u′ +
 1 −ν2ζ2
u = 0
u′′ + 1
ζ u′ +
 1
ζ4 −ν2
ζ2

u = 0.
The point ζ = 0 and hence the point z = ∞is an irregular singular point. We will ﬁnd the leading order asymptotic
behavior of the solutions as z →+∞.
Controlling Factor.
We starti with Bessel’s equation for real argument.
y′′ + 1
xy′ +

1 −ν2
x2

y = 0
We make the substitution y = es(x).
s′′ + (s′)2 + 1
xs′ + 1 −ν2
x2 = 0
We know that ν2
x2 ≪1 as x →∞; we will assume that s′′ ≪(s′)2 as x →∞.
(s′)2 + 1
xs′ + 1 ∼0
as x →∞
To simplify the equation further, we will try the possible two-term balances.
1. (s′)2 + 1
xs′ ∼0
→
s′ ∼−1
x
This balance is not consistent as it violates the assumption that 1 is smaller
than the other terms.
2. (s′)2 + 1 ∼0
→
s′ ∼±ı
This balance is consistent.
3.
1
xs′ + 1 ∼0
→
s′ ∼−x
This balance is inconsistent as (s′)2 isn’t smaller than the other terms.
Thus the only dominant balance is s′ ∼±ı. This balance is consistent with our initial assumption that s′′ ≪(s′)2.
Thus s ∼±ıx and the controlling factor is e±ıx.
1606

Leading Order Behavior.
In order to ﬁnd the leading order behavior, we substitute s = ±ıx+t(x) where t(x) ≪x
as x →∞into the diﬀerential equation for s. We ﬁrst consider the case s = ıx + t(x). We assume that t′ ≪1 and
t′′ ≪1/x.
t′′ + (ı + t′)2 + 1
x(ı + t′) + 1 −ν2
x2 = 0
t′′ + ı2t′ + (t′)2 + ı
x + 1
xt′ −ν2
x2 = 0
We use our assumptions about the behavior of t′ and t′′.
ı2t′ + ı
x ∼0
t′ ∼−1
2x
t ∼−1
2 ln x
as x →∞.
This asymptotic behavior is consistent with our assumptions.
Substituting s = −ıx + t(x) will also yield t ∼−1
2 ln x. Thus the leading order behavior of the solutions is
y ∼c e±ıx−1
2 ln x+u(x) = cx−1/2 e±ıx+u(x)
as x →∞,
where u(x) ≪ln x as x →∞.
By substituting t = −1
2 ln x+u(x) into the diﬀerential equation for t, you could show that u(x) →const as x →∞.
Thus the full leading order behavior of the solutions is
y ∼cx−1/2 e±ıx+u(x)
as x →∞
where u(x) →0 as x →∞. Writing this in terms of sines and cosines yields
y1 ∼x−1/2 cos(x + u1(x)),
y2 ∼x−1/2 sin(x + u2(x)),
as x →∞,
1607

where u1, u2 →0 as x →∞.
Result 34.2.1 Bessel’s equation for real argument is
y′′ + 1
xy′ +

1 −ν2
x2

y = 0.
If ν is not an integer then the solutions behave as linear combinations of
y1 = xν,
and
y2 = x−ν
at x = 0. If ν is an integer, then the solutions behave as linear combinations of
y1 = xν,
and
y2 = x−ν + cxν log x
at x = 0. The solutions are asymptotic to a linear combination of
y1 = x−1/2 sin(x + u1(x)),
and
y2 = x−1/2 cos(x + u2(x))
as x →+∞, where u1, u2 →0 as x →∞.
34.3
Bessel Functions of the First Kind
Consider the function exp(1
2z(t −1/t)). We can expand this function in a Laurent series in powers of t,
e
1
2z(t−1/t) =
∞
X
n=−∞
Jn(z)tn,
1608

where the coeﬃcient functions Jn(z) are
Jn(z) =
1
ı2π
I
τ −n−1 e
1
2z(τ−1/τ) dτ.
Here the path of integration is any positive closed path around the origin. exp(1
2z(t−1/t)) is the generating function
for Bessel function of the ﬁrst kind.
34.3.1
The Bessel Function Satisﬁes Bessel’s Equation
We would like to expand Jn(z) in powers of z. The ﬁrst step in doing this is to make the substitution τ = 2t/z.
Jn(z) =
1
ı2π
I 2t
z
−n−1
exp
1
2z
2t
z −z
2t
 2
z dt
=
1
ı2π
z
2
n I
t−n−1 et−z2/4t dt
We diﬀerentiate the expression for Jn(z).
J′
n(z) =
1
ı2π
nzn−1
2n
I
t−n−1 et−z2/4t dt + 1
ı2π
z
2
n I
t−n−1
−2z
4t

et−z2/4t dt
=
1
ı2π
z
2
n I n
z −z
2t

t−n−1 et−z2/4t dt
J′′
n(z) =
1
ı2π
z
2
n I n
z
n
z −z
2t

+

−n
z2 −1
2t

−z
2t
n
z −z
2t

t−n−1 et−z2/4t dt
=
1
ı2π
z
2
n I n2
z2 −nz
2zt −n
z2 −1
2t −nz
2zt + z2
4t2

t−n−1 et−z2/4t dt
=
1
ı2π
z
2
n I n(n −1)
z2
−2n + 1
2t
+ z2
4t2

t−n−1 et−z2/4t dt
1609

We substitute Jn(z) into Bessel’s equation.
J′′
n + 1
zJ′
n +

1 −n2
z2

Jn
=
1
ı2π
z
2
n I n(n −1)
z2
−2n + 1
2t
+ z2
4t2

+
 n
z2 −1
2t

+

1 −n2
z2

t−n−1 et−z2/4t dt
=
1
ı2π
z
2
n I 
1 −n + 1
t
+ z2
4t2

t−n−1 et−z2/4t dt
=
1
ı2π
z
2
n I
d
dt

t−n−1 et−z2/4t
dt
Since t−n−1 et−z2/4t is analytic in 0 < |t| < ∞when n is an integer, the integral vanishes.
= 0.
Thus for integer n, Jn(z) satisﬁes Bessel’s equation.
Jn(z) is called the Bessel function of the ﬁrst kind. The subscript is the order. Thus J1(z) is a Bessel function
of order 1. J0(x) and J1(x) are plotted in the ﬁrst graph in Figure 34.1. J5(x) is plotted in the second graph in
Figure 34.1. Note that for non-negative, integer n, Jn(z) behaves as zn at z = 0.
34.3.2
Series Expansion of the Bessel Function
We expand exp(−z2/4t) in the integral expression for Jn.
Jn(z) =
1
ı2π
z
2
n I
t−n−1 et−z2/4t dt
=
1
ı2π
z
2
n I
t−n−1 et
 ∞
X
m=0
−z2
4t
m 1
m!
!
dt
1610

2
4
6
8
10
12
14
-0.4
-0.2
0.2
0.4
0.6
0.8
1
5
10
15
20
-0.2
-0.1
0.1
0.2
0.3
Figure 34.1: Plots of J0(x), J1(x) and J5(x).
For the path of integration, we are free to choose any contour that encloses the origin. Consider the circular path on
|t| = 1. Since the integral is uniformly convergent, we can interchange the order of integration and summation.
Jn(z) =
1
ı2π
z
2
n
∞
X
m=0
(−1)mz2m
22mm!
I
t−n−m−1 et dt
1611

Let n be a non-negative integer.
1
ı2π
I
t−n−m−1 et dt = lim
z→0

1
(n + m)!
dn+m
dzn+m(ez)

=
1
(n + m)!
We have the series expansion
Jn(z) =
∞
X
m=0
(−1)m
m!(n + m)!
z
2
n+2m
for n ≥0.
Now consider J−n(z), (n positive).
J−n(z) =
1
ı2π
z
2
−n
∞
X
m=1
(−1)mz2m
22mm!
I
tn−m−1 et dt
For m ≥n, the integrand has a pole of order m −n + 1 at the origin.
1
ı2π
I
tn−m−1 et dt =
(
1
(m−n)!
for m ≥n
0
for m < n
The expression for J−n is then
J−n(z) =
∞
X
m=n
(−1)m
m!(m −n)!
z
2
−n+2m
=
∞
X
m=0
(−1)m+n
(m + n)!m!
z
2
n+2m
= (−1)nJn(z).
Thus we have that
J−n(z) = (−1)nJn(z)
for integer n.
1612

34.3.3
Bessel Functions of Non-Integer Order
The generalization of the factorial function is the Gamma function. For integer values of n, n! = Γ(n+1). The Gamma
function is deﬁned for all complex-valued arguments. Thus one would guess that if the Bessel function of the ﬁrst kind
were deﬁned for non-integer order, it would have the deﬁnition,
Jν(z) =
∞
X
m=0
(−1)m
m!Γ(ν + m + 1)
z
2
ν+2m
.
The Integrand for Non-Integer ν.
Recall the deﬁnition of the Bessel function
Jν(z) =
1
ı2π
z
2
ν I
t−ν−1 et−z2/4t dt.
When ν is an integer, the integrand is single valued. Thus if you start at any point and follow any path around the
origin, the integrand will return to its original value. This property was the key to Jn satisfying Bessel’s equation. If ν
is not an integer, then this property does not hold for arbitrary paths around the origin.
A New Contour.
First, since the integrand is multiple-valued, we need to deﬁne what branch of the function we
are talking about. We will take the principal value of the integrand and introduce a branch cut on the negative real
axis. Let C be a contour that starts at z = −∞below the branch cut, circles the origin, and returns to the point
z = −∞above the branch cut. This contour is shown in Figure 34.2.
Thus we deﬁne
Jν(z) =
1
ı2π
z
2
ν I
C
t−ν−1 et−z2/4t dt.
Bessel’s Equation.
Substituting Jν(z) into Bessel’s equation yields
J′′
ν + 1
zJ′
ν +

1 −ν2
z2

Jν =
1
ı2π
z
2
ν I
C
d
dt

t−ν−1 et−z2/4t
dt.
Since t−ν−1 et−z2/4t is analytic in 0 < |z| < ∞and | arg(z)| < π, and it vanishes at z = −∞, the integral is zero.
Thus the Bessel function of the ﬁrst kind satisﬁes Bessel’s equation for all complex orders.
1613

Figure 34.2: The Contour of Integration.
Series Expansion.
Because of the et factor in the integrand, the integral deﬁning Jν converges uniformly. Expanding
e−z2/4t in a Taylor series yields
Jν(z) =
1
ı2π
z
2
ν
∞
X
m=0
(−1)mz2m
22mm!
I
C
t−ν−m−1 et dt
Since
1
Γ(α) =
1
ı2π
I
C
t−α−1 et dt,
we have the series expansion of the Bessel function
Jν(z) =
∞
X
m=0
(−1)m
m!Γ(ν + m + 1)
z
2
ν+2m
.
1614

Linear Independence.
We use Abel’s formula to compute the Wronskian of Bessel’s equation.
W(z) = exp

−
Z z 1
ζ dζ

= e−log z = 1
z
Thus to within a function of ν, the Wronskian of any two solutions is 1/z. For any given ν, there are two linearly
independent solutions. Note that Bessel’s equation is unchanged under the transformation ν →−ν. Thus both Jν and
J−ν satisfy Bessel’s equation. Now we must determine if they are linearly independent. We have already shown that
for integer values of ν they are not independent. (J−n = (−1)nJn.) Assume that ν is not an integer. We compute the
Wronskian of Jν and J−ν.
W[Jν, J−ν] =

Jν
J−ν
J′
ν
J′
−ν

= JνJ′
−ν −J−νJ′
ν
We substitute in the expansion for Jν
=
 ∞
X
m=0
(−1)m
m!Γ(ν + m + 1)
z
2
ν+2m
!  ∞
X
n=0
(−1)n(−ν + 2n)
n!Γ(−ν + n + 1)2
z
2
−ν+2n−1
!
−
 ∞
X
m=0
(−1)m
m!Γ(−ν + m + 1)
z
2
−ν+2m
!  ∞
X
n=0
(−1)n(ν + 2n)
n!Γ(ν + n + 1)2
z
2
ν+2n−1
!
Since the Wronskian is a function of ν times 1/z the coeﬃcients of all of the powers of z except 1/z must vanish.
=
−ν
zΓ(ν + 1)Γ(−ν + 1) −
ν
zΓ(−ν + 1)Γ(ν + 1)
= −
2
zΓ(ν)Γ(1 −ν)
1615

Using an identity for the Gamma function simpliﬁes this expression.
= −2
πz sin(πν)
Since the Wronskian is nonzero for non-integer ν, Jν and J−ν are independent functions when ν is not an integer. In
this case, the general solution of Bessel’s equation is aJν + bJ−ν.
34.3.4
Recursion Formulas
In showing that Jν satisﬁes Bessel’s equation for arbitrary complex ν, we obtained
I
C
d
dt

t−ν et−z2/4t
dt = 0.
Expanding the integral,
I
C

t−ν + z2
4 t−ν−2 −νt−ν−1

et−z2/4t dt = 0.
1
ı2π
z
2
ν I
C

t−ν + z2
4 t−ν−2 −νt−ν−1

et−z2/4t dt = 0.
Since Jν(z) =
1
ı2π(z/2)ν H
C t−ν−1 et−z2/4t dt,
"2
z
−1
Jν−1 +
2
z
 z2
4 Jν+1 −νJν
#
= 0.
Jν−1 + Jν+1 = 2ν
z Jν
1616

Diﬀerentiating the integral expression for Jν,
J′
ν(z) =
1
ı2π
νzν−1
2ν
I
C
t−ν−1 et−z2/4t dt + 1
ı2π
z
2
ν I
C
t−ν−1 
−z
2t

et−z2/4t dt
J′
ν(z) = ν
z
1
ı2π
z
2
ν I
C
t−ν−1 et−z2/4t dt −1
ı2π
z
2
ν+1 I
C
t−ν−2 et−z2/4t dt
J′
ν = ν
z Jν −Jν+1
From the two relations we have derived you can show that
J′
ν = 1
2(Jν−1 + Jν+1)
and
J′
ν = Jν−1 −ν
z Jν.
1617

Result 34.3.1 The Bessel function of the ﬁrst kind, Jν(z), is deﬁned,
Jν(z) =
1
ı2π
z
2
ν I
C
t−ν−1 et−z2/4t dt.
The Bessel function has the expansion,
Jν(z) =
∞
X
m=0
(−1)m
m!Γ(ν + m + 1)
z
2
ν+2m
.
The asymptotic behavior for large argument is
Jν(z) ∼
r
2
πz

cos

z −νπ
2 −π
4

+ e|ℑ(z)| O
 |z|−1
as |z| →∞, | arg(z)| < π.
The Wronskian of Jν(z) and J−ν(z) is
W(z) = −2
πz sin(πν).
Thus Jν(z) and J−ν(z) are independent when ν is not an integer. The Bessel functions satisfy
the recursion relations,
Jν−1 + Jν+1 = 2ν
z Jν
J′
ν = ν
z Jν −Jν+1
J′
ν = 1
2(Jν−1 −Jν+1)
J′
ν = Jν−1 −ν
z Jν.
1618

34.3.5
Bessel Functions of Half-Integer Order
Consider J1/2(z). Start with the series expansion
J1/2(z) =
∞
X
m=0
(−1)m
m!Γ(1/2 + m + 1)
z
2
1/2+2m
.
Use the identity Γ(n + 1/2) = (1)(3)···(2n−1)
2n
√π.
=
∞
X
m=0
(−1)m2m+1
m!(1)(3) · · · (2m + 1)√π
z
2
1/2+2m
=
∞
X
m=0
(−1)m2m+1
(2)(4) · · · (2m) · (1)(3) · · · (2m + 1)√π
1
2
1/2+m
z1/2+2m
=
 2
πz
1/2
∞
X
m=0
(−1)m
(2m + 1)!z2m+1
We recognize the sum as the Taylor series expansion of sin z.
=
 2
πz
1/2
sin z
Using the recurrence relations,
Jν+1 = ν
z Jν −J′
ν
and
Jν−1 = ν
z Jν + J′
ν,
we can ﬁnd Jn+1/2 for any integer n.
1619

Example 34.3.1 To ﬁnd J3/2(z),
J3/2(z) = 1/2
z J1/2(z) −J′
1/2(z)
= 1/2
z
 2
π
1/2
z−1/2 sin z −

−1
2
  2
π
1/2
z−3/2 sin z −
 2
π
1/2
z−1/2 cos z
= 2−1/2π−1/2z−3/2 sin z + 2−1/2π−1/2z−3/2 sin z −2−1/2π−1/2 cos z
=
 2
π
1/2
z−3/2 sin z −
 2
π
1/2
z−1/2 cos z
=
 2
π
1/2  z−3/2 sin z −z−1/2 cos z

.
You can show that
J−1/2(z) =
 2
πz
1/2
cos z.
Note that at a ﬁrst glance it appears that J3/2 ∼z−1/2 as z →0. However, if you expand the sine and cosine you
will see that the z−1/2 and z1/2 terms vanish and thus J3/2(z) ∼z3/2 as z →0 as we showed previously.
Recall that we showed the asymptotic behavior as x →+∞of Bessel functions to be linear combinations of
x−1/2 sin(x + U1(x))
and
x−1/2 cos(x + U2(x))
where U1, U2 →0 as x →+∞.
34.4
Neumann Expansions
Consider expanding an analytic function in a series of Bessel functions of the form
f(z) =
∞
X
n=0
anJn(z).
1620

If f(z) is analytic in the disk |z| ≤r then we can write
f(z) =
1
ı2π
I
f(ζ)
ζ −z dζ,
where the path of integration is |ζ| = r and |z| < r. If we were able to expand the function
1
ζ−z in a series of Bessel
functions, then we could interchange the order of summation and integration to get a Bessel series expansion of f(z).
The Expansion of 1/(ζ −z).
Assume that
1
ζ−z has the uniformly convergent expansion
1
ζ −z = c0(ζ)J0(z) + 2
∞
X
n=1
cn(ζ)Jn(z),
where each cn(ζ) is analytic. Note that
 ∂
∂ζ + ∂
∂z

1
ζ −z =
−1
(ζ −z)2 +
1
(ζ −z)2 = 0.
Thus we have
 ∂
∂ζ + ∂
∂z
 "
c0(ζ)J0(z) + 2
∞
X
n=1
cn(ζ)Jn(z)
#
= 0
"
c′
0J0 + 2
∞
X
n=1
c′
nJn
#
+
"
c0J′
0 + 2
∞
X
n=1
cnJ′
n
#
= 0.
Using the identity 2J′
n = Jn−1 −Jn+1,
"
c′
0J0 + 2
∞
X
n=1
c′
nJn
#
+
"
c0(−J1) +
∞
X
n=1
cn(Jn−1 −Jn+1)
#
= 0.
1621

Collecting coeﬃcients of Jn,
(c′
0 + c1)J0 +
∞
X
n=1
(2c′
n + cn+1 −cn−1)Jn = 0.
Equating the coeﬃcients of Jn, we see that the cn are given by the relations,
c1 = −c′
0,
and
cn+1 = cn−1 −2c′
n.
We can evaluate c0(ζ). Setting z = 0,
1
ζ = c0(ζ)J0(0) + 2
∞
X
n=1
cn(ζ)Jn(0)
1
ζ = c0(ζ).
Using the recurrence relations we can calculate the cn’s. The ﬁrst few are:
c1 = −−1
ζ2 = 1
ζ2
c2 = 1
ζ −2−2
ζ3 = 1
ζ + 4
ζ3
c3 = 1
ζ2 −2
−1
ζ2 −12
ζ4

= 3
ζ2 + 24
ζ4 .
We see that cn is a polynomial of degree n + 1 in 1/ζ. One can show that
cn(ζ) =



2n−1n!
ζn+1

1 +
ζ2
2(2n−2) +
ζ4
2·4·(2n−2)(2n−4) + · · · +
ζn
2·4···n·(2n−2)···(2n−n)

for even n
2n−1n!
ζn+1

1 +
ζ2
2(2n−2) +
ζ4
2·4·(2n−2)(2n−4) + · · · +
ζn−1
2·4···(n−1)·(2n−2)···(2n−(n−1))

for odd n
1622

Uniform Convergence of the Series.
We assumed before that the series expansion of
1
ζ−z is uniformly convergent.
The behavior of cn and Jn are
cn(ζ) = 2n−1n!
ζn+1 + O(ζ−n),
Jn(z) =
zn
2nn! + O(zn+1).
This gives us
cn(ζ)Jn(z) = 1
2ζ
z
ζ
n
+ O
 
1
ζ
z
ζ
n+1!
.
If
 z
ζ
 = ρ < 1 we can bound the series with the geometric series P ρn. Thus the series is uniformly convergent.
Neumann Expansion of an Analytic Function.
Let f(z) be a function that is analytic in the disk |z| ≤r.
Consider |z| < r and the path of integration along |ζ| = r. Cauchy’s integral formula tells us that
f(z) =
1
ı2π
I
f(ζ)
ζ −z dζ.
Substituting the expansion for
1
ζ−z,
=
1
ı2π
I
f(ζ)
 
co(ζ)J0(z) + 2
∞
X
n=1
cn(ζ)Jn(z)
!
dζ
= J0(z) 1
ı2π
I f(ζ)
ζ
dζ +
∞
X
n=1
Jn(z)
ıπ
I
cn(ζ)f(ζ) dζ
= J0(z)f(0) +
∞
X
n=1
Jn(z)
ıπ
I
cn(ζ)f(ζ) dζ.
1623

Result 34.4.1 let f(z) be analytic in the disk, |z| ≤r. Consider |z| < r and the path of
integration along |ζ| = r. f(z) has the Bessel function series expansion
f(z) = J0(z)f(0) +
∞
X
n=1
Jn(z)
ıπ
I
cn(ζ)f(ζ) dζ,
where the cn satisfy
1
ζ −z = c0(ζ)J0(z) + 2
∞
X
n=1
cn(ζ)Jn(z).
34.5
Bessel Functions of the Second Kind
When ν is an integer, Jν and J−ν are not linearly independent. In order to ﬁnd an second linearly independent solution,
we deﬁne the Bessel function of the second kind, (also called Weber’s function),
Yν =
(Jν(z) cos(νπ)−J−ν(z)
sin(νπ)
when ν is not an integer
limµ→ν
Jµ(z) cos(µπ)−J−µ(z)
sin(µπ)
when ν is an integer.
Jν and Yν are linearly independent for all ν.
In Figure 34.3 Y0 and Y1 are plotted in solid and dashed lines, respectively.
1624

5
10
15
20
-1
-0.75
-0.5
-0.25
0.25
0.5
Figure 34.3: Bessel Functions of the Second Kind
Result 34.5.1 The Bessel function of the second kind, Yν(z), is deﬁned,
Yν =
(Jν(z) cos(νπ)−J−ν(z)
sin(νπ)
when ν is not an integer
limµ→ν
Jµ(z) cos(µπ)−J−µ(z)
sin(µπ)
when ν is an integer.
The Wronskian of Jν(z) and Yν(z) is
W[Jν, Yν] = 2
πz.
Thus Jν(z) and Yν(z) are independent for all ν. The Bessel functions of the second kind
satisfy the recursion relations,
Yν−1 + Yν+1 = 2ν
z Yν
Y ′
ν = ν
z Yν −Yν+1
Y ′
ν = 1
2(Yν−1 −Yν+1)
Y ′
ν = Yν−1 −ν
z Yν.
1625

34.6
Hankel Functions
Another set of solutions to Bessel’s equation is the Hankel functions,
H(1)
ν (z) = Jν(z) + ıYν(z),
H(2)
ν (z) = Jν(z) −ıYν(z)
Result 34.6.1 The Hankel functions are deﬁned
H(1)
ν (z) = Jν(z) + ıYν(z),
H(2)
ν (z) = Jν(z) −ıYν(z)
The Wronskian of H(1)
ν (z) and H(2)
ν (z) is
W[H(1)
ν , H(2)
ν ] = −ı4
πz.
The Hankel functions are independent for all ν.
The Hankel functions satisfy the same
recurrence relations as the other Bessel functions.
34.7
The Modiﬁed Bessel Equation
The modiﬁed Bessel equation is
w′′ + 1
zw′ −

1 + ν2
z2

w = 0.
1626

This equation is identical to the Bessel equation except for a sign change in the last term. If we make the change of
variables ξ = ız, u(ξ) = w(z) we obtain the equation
−u′′ −1
ξ u′ −

1 −ν2
ξ2

u = 0
u′′ + 1
ξ u′ +

1 −ν2
ξ2

u = 0.
This is the Bessel equation. Thus Jν(ız) is a solution to the modiﬁed Bessel equation. This motivates us to deﬁne the
modiﬁed Bessel function of the ﬁrst kind
Iν(z) = ı−νJν(ız).
Since Jν and J−ν are linearly independent solutions when ν is not an integer, Iν and I−ν are linearly independent
solutions to the modiﬁed Bessel equation when ν is not an integer.
The Taylor series expansion of Iν(z) about z = 0 is
Iν(z) = ı−νJν(ız)
= ı−ν
∞
X
m=0
(−1)m
m!Γ(ν + m + 1)
ız
2
ν+2m
= ı−ν
∞
X
m=0
(−1)mıνı2m
m!Γ(ν + m + 1)
z
2
ν+2m
=
∞
X
m=0
1
m!Γ(ν + m + 1)
z
2
ν+2m
Modiﬁed Bessel Functions of the Second Kind.
In order to have a second linearly independent solution when
ν is an integer, we deﬁne the modiﬁed Bessel function of the second kind
Kν(z) =
(
π
2
I−ν−Iν
sin(νπ)
when ν is not an integer,
limµ→ν
π
2
I−µ−Iµ
sin(µπ)
when ν is an integer.
1627

1
2
3
4
2
4
6
8
10
Figure 34.4: Modiﬁed Bessel Functions
Iν and Kν are linearly independent for all ν. In Figure 34.4 I0 and K0 are plotted in solid and dashed lines, respectively.
1628

Result 34.7.1 The modiﬁed Bessel functions of the ﬁrst and second kind, Iν(z) and Kν(z),
are deﬁned,
Iν(z) = ı−νJν(ız).
Kν(z) =
(π
2
I−ν−Iν
sin(νπ)
when ν is not an integer,
limµ→ν π
2
I−µ−Iµ
sin(µπ)
when ν is an integer.
The modiﬁed Bessel function of the ﬁrst kind has the expansion,
Iν(z) =
∞
X
m=0
1
m!Γ(ν + m + 1)
z
2
ν+2m
The Wronskian of Iν(z) and I−ν(z) is
W[Iν, I−ν] = −2
πz sin(πν).
Iν(z) and I−ν(z) are linearly independent when ν is not an integer. The Wronskian of Iν(z)
and Kν(z) is
W[Iν, Kν] = −1
z.
Iν(z) and Kν(z) are independent for all ν. The modiﬁed Bessel functions satisfy the recursion
relations,
Aν−1 −Aν+1 = 2ν
z Aν
A′
ν = Aν+1 + ν
z Aν
A′
ν = 1
2(Aν−1 + Aν+1)
A′
ν = Aν−1 −ν
z Aν.
where A stands for either I or K.
1629

34.8
Exercises
Exercise 34.1
Consider Bessel’s equation
z2y′′(z) + zy′(z) +
 z2 −ν2
y = 0
where ν ≥0. Find the Frobenius series solution that is asymptotic to tν as t →0. By multiplying this solution by a
constant, deﬁne the solution
Jν(z) =
∞
X
k=1
(−1)k
k!Γ(k + ν + 1)
z
2
2k+ν
.
This is called the Bessel function of the ﬁrst kind and order ν. Clearly J−ν(z) is deﬁned and is linearly independent to
Jν(z) if ν is not an integer. What happens when ν is an integer?
Exercise 34.2
Consider Bessel’s equation for integer n,
z2y′′ + zy′ +
 z2 −n2
y = 0.
Using the kernel
K(z, t) = e
1
2z(t−1
t),
ﬁnd two solutions of Bessel’s equation. (For n = 0 you will ﬁnd only one solution.) Are the two solutions linearly
independent? Deﬁne the Bessel function of the ﬁrst kind and order n,
Jn(z) =
1
ı2π
I
C
t−n−1 e
1
2 z(t−1/t) dt,
where C is a simple, closed contour about the origin. Verify that
e
1
2z(t−1/t) =
∞
X
n=−∞
Jn(z)tn.
This is the generating function for the Bessel functions.
1630

Exercise 34.3
Use the generating function
e
1
2z(t−1/t) =
∞
X
n=−∞
Jn(z)tn
to show that Jn satisﬁes Bessel’s equation
z2y′′ + zy′ +
 z2 −n2
y = 0.
Exercise 34.4
Using
Jn−1 + Jn+1 = 2n
z Jn
and
J′
n = n
z Jn −Jn+1,
show that
J′
n = 1
2(Jn−1 −Jn+1)
and
J′
n = Jn−1 −n
z Jn.
Exercise 34.5
Find the general solution of
w′′ + 1
zw′ +

1 −1
4z2

w = z.
Exercise 34.6
Show that Jν(z) and Yν(z) are linearly independent for all ν.
Exercise 34.7
Compute W[Iν, I−ν] and W[Iν, Kν].
Exercise 34.8
Using the generating function,
exp
z
2

t −1
t

=
+∞
X
n=−∞
Jn(z)tn,
1631

verify the following identities:
1.
2n
z Jn(z) = Jn−1(z) + Jn+1(z).
This relation is useful for recursively computing the values of the higher order Bessel functions.
2.
J′
n(z) = 1
2 (Jn−1 −Jn+1) .
This relation is useful for computing the derivatives of the Bessel functions once you have the values of the Bessel
functions of adjacent order.
3.
d
dz
 z−nJn(z)

= −z−nJn+1(z).
Exercise 34.9
Use the Wronskian of Jν(z) and J−ν(z),
W [Jν(z), J−ν(z)] = −2 sin νπ
πz
,
to derive the identity
J−ν+1(z)Jν(z) + J−ν(z)Jν−1(z) = 2
πz sin νπ.
Exercise 34.10
Show that, using the generating function or otherwise,
J0(z) + 2J2(z) + 2J4(z) + 2J6(z) + · · · = 1
J0(z) −2J2(z) + 2J4(z) −2J6(z) + · · · = cos z
2J1(z) −2J3(z) + 2J5(z) −· · · = sin z
J2
0(z) + 2J2
1(z) + 2J2
2(z) + 2J2
3(z) + · · · = 1
1632

Exercise 34.11
It is often possible to “solve” certain ordinary diﬀerential equations by converting them into the Bessel equation by
means of various transformations. For example, show that the solution of
y′′ + xp−2y = 0,
can be written in terms of Bessel functions.
y(x) = c1x1/2J1/p
2
pxp/2

+ c2x1/2Y1/p
2
pxp/2

Here c1 and c2 are arbitrary constants. Thus show that the Airy equation,
y′′ + xy = 0,
can be solved in terms of Bessel functions.
Exercise 34.12
The spherical Bessel functions are deﬁned by
jn(z) =
r π
2zJn+1/2(z),
yn(z) =
r π
2zYn+1/2(z),
kn(z) =
r π
2zKn+1/2(z),
in(z) =
r π
2zIn+1/2(z).
1633

Show that
j1(z) = sin z
z2
−cos z
z
,
i0(z) = sinh z
z
,
k0(z) = π
2z exp(−z).
Exercise 34.13
Show that as x →∞,
Kn(x) ∝e−x
√x

1 + 4n2 −1
8x
+ (4n2 −1)(4n2 −9)
128x2
+ · · ·

.
1634

34.9
Hints
Hint 34.2
Hint 34.3
Hint 34.4
Use the generating function
e
1
2z(t−1/t) =
∞
X
n=−∞
Jn(z)tn
to show that Jn satisﬁes Bessel’s equation
z2y′′ + zy′ +
 z2 −n2
y = 0.
Hint 34.6
Use variation of parameters and the Wronskian that was derived in the text.
Hint 34.7
Compute the Wronskian of Jν(z) and Yν(z). Use the relation
W [Jν, J−ν] = −2
πz sin(πν)
Hint 34.8
Derive W[Iν, I−ν] from the value of W[Jν, J−ν]. Derive W[Iν, Kν] from the value of W[Iν, I−ν].
Hint 34.9
Hint 34.10
1635

Hint 34.11
Hint 34.12
Hint 34.13
Hint 34.14
1636

34.10
Solutions
Solution 34.1
Bessel’s equation is
L[y] ≡z2y′′ + zy′ +
 z2 −n2
y = 0.
We consider a solution of the form
y(z) =
Z
C
e
1
2 z(t−1/t) v(t) dt.
We substitute the form of the solution into Bessel’s equation.
Z
C
L
h
e
1
2z(t−1/t)i
v(t) dt = 0
Z
C
 
z21
4

t + 1
t
2
+ z1
2

t −1
t
2
+
 z2 −n2
!
e
1
2z(t−1/t) v(t) dt = 0
(34.1)
By considering
d
dtt e
1
2z(t−1/t) =
1
2x

t + 1
t

+ 1

e
1
2 z(t−1/t)
d2
dt2t2 e
1
2z(t−1/t) =
 
1
4x2

t + 1
t
2
+ x

2t + 1
t

+ 2
!
e
1
2 z(t−1/t)
we see that
L
h
e
1
2z(t−1/t)i
=
 d2
dt2t2 −3 d
dtt +
 1 −n2
e
1
2z(t−1/t) .
Thus Equation 34.1 becomes
Z
C
 d2
dt2t2 e
1
2z(t−1/t) −3 d
dtt e
1
2 z(t−1/t) +(1 −n2) e
1
2z(t−1/t)

v(t) dt = 0
1637

We apply integration by parts to move derivatives from the kernel to v(t).
h
t2 e
1
2z(t−1/t) v(t)
i
C −
h
t e
1
2z(t−1/t) v′(t)
i
C +
h
−3t e
1
2z(t−1/t) v(t)
i
C +
Z
C
e
1
2z(t−1/t)  t2v′′(t) + 3tv(t) +
 1 −n2
v(t)

dt =
h
e
1
2z(t−1/t)  (t2 −3t)v(t) −tv′(t)
i
C +
Z
C
e
1
2z(t−1/t)  t2v′′(t) + 3tv(t) + (1 −n2)v(t)

dt = 0
In order that the integral vanish, v(t) must be a solution of the diﬀerential equation
t2v′′ + 3tv +
 1 −n2
v = 0.
This is an Euler equation with the solutions {tn−1, t−n−1} for non-zero n and {t−1, t−1 log t} for n = 0.
Consider the case of non-zero n. Since
e
1
2 z(t−1/t)   t2 −3t

v(t) −tv′(t)

is single-valued and analytic for t ̸= 0 for the functions v(t) = tn−1 and v(t) = t−n−1, the boundary term will vanish if
C is any closed contour that that does not pass through the origin. Note that the integrand in our solution,
e
1
2 z(t−1/t) v(t),
is analytic and single-valued except at the origin and inﬁnity where it has essential singularities. Consider a simple
closed contour that does not enclose the origin. The integral along such a path would vanish and give us y(z) = 0.
This is not an interesting solution. Since
e
1
2 z(t−1/t) v(t),
has non-zero residues for v(t) = tn−1 and v(t) = t−n−1, choosing any simple, positive, closed contour about the origin
will give us a non-trivial solution of Bessel’s equation. These solutions are
y1(t) =
Z
C
tn−1 e
1
2z(t−1/t) dt,
y2(t) =
Z
C
t−n−1 e
1
2 z(t−1/t) dt.
Now consider the case n = 0. The two solutions above concide and we have the solution
y(t) =
Z
C
t−1 e
1
2z(t−1/t) dt.
1638

Choosing v(t) = t−1 log t would make both the boundary terms and the integrand multi-valued. We do not pursue the
possibility of a solution of this form.
The solution y1(t) and y2(t) are not linearly independent. To demonstrate this we make the change of variables
t →−1/t in the integral representation of y1(t).
y1(t) =
Z
C
tn−1 e
1
2z(t−1/t) dt
=
Z
C
(−1/t)n−1 e
1
2z(−1/t+t) −1
t2 dt
=
Z
C
(−1)nt−n−1 e
1
2z(t−1/t) dt
= (−1)ny2(t)
Thus we see that a solution of Bessel’s equation for integer n is
y(t) =
Z
C
t−n−1 e
1
2z(t−1/t) dt
where C is any simple, closed contour about the origin.
Therefore, the Bessel function of the ﬁrst kind and order n,
Jn(z) =
1
ı2π
I
C
t−n−1 e
1
2 z(t−1/t) dt
is a solution of Bessel’s equation for integer n. Note that Jn(z) is the coeﬃcient of tn in the Laurent series of e
1
2z(t−1/t).
This establishes the generating function for the Bessel functions.
e
1
2z(t−1/t) =
∞
X
n=−∞
Jn(z)tn
1639

Solution 34.2
The generating function is
e
z
2 (t−1/t) =
∞
X
n=−∞
Jn(z)tn.
In order to show that Jn satisﬁes Bessel’s equation we seek to show that
∞
X
n=−∞
 z2J′′
n(z) + zJn(z) + (z2 −n2)Jn(z)

tn = 0.
To get the appropriate terms in the sum we will diﬀerentiate the generating function with respect to z and t. First we
diﬀerentiate it with respect to z.
1
2

t −1
t

e
z
2 (t−1/t) =
∞
X
n=−∞
J′
n(z)tn
1
4

t −1
t
2
e
z
2 (t−1/t) =
∞
X
n=−∞
J′′
n(z)tn
Now we diﬀerentiate with respect to t and multiply by t get the n2Jn term.
z
2

1 + 1
t2

e
z
2 (t−1/t) =
∞
X
n=−∞
nJn(z)tn−1
z
2

t + 1
t

e
z
2 (t−1/t) =
∞
X
n=−∞
nJn(z)tn
z
2

1 −1
t2

e
z
2 (t−1/t) +z2
4

t + 1
t
2
e
z
2 (t−1/t) =
∞
X
n=−∞
n2Jn(z)tn−1
z
2

t −1
t

e
z
2 (t−1/t) +z2
4

t + 1
t
2
e
z
2 (t−1/t) =
∞
X
n=−∞
n2Jn(z)tn
1640

Now we can evaluate the desired sum.
∞
X
n=−∞
 z2J′′
n(z) + zJn(z) +
 z2 −n2
Jn(z)

tn
=
 
z2
4

t −1
t
2
+ z
2

t −1
t

+ z2 −z
2

t −1
t

−z2
4

t + 1
t
2!
e
z
2 (t−1/t)
∞
X
n=−∞
 z2J′′
n(z) + zJn(z) +
 z2 −n2
Jn(z)

tn = 0
z2J′′
n(z) + zJn(z) +
 z2 −n2
Jn(z) = 0
Thus Jn satisﬁes Bessel’s equation.
Solution 34.3
J′
n = n
z Jn −Jn+1
= 1
2(Jn−1 + Jn+1) −Jn+1
= 1
2(Jn−1 −Jn+1)
J′
n = n
z Jn −Jn+1
= n
z Jn −
2n
z Jn −Jn−1

= Jn−1 −n
z Jn
1641

Solution 34.4
The linearly independent homogeneous solutions are J1/2 and J−1/2. The Wronskian is
W[J1/2, J−1/2] = −2
πz sin(π/2) = −2
πz.
Using variation of parameters, a particular solution is
yp = −J1/2(z)
Z z ζJ−1/2(ζ)
−2/πζ
dζ + J−1/2(z)
Z z ζJ1/2(ζ)
−2/πζ dζ
= π
2 J1/2(z)
Z z
ζ2J−1/2(ζ) dζ −π
2 J−1/2(z)
Z z
ζ2J1/2(ζ) dζ.
Thus the general solution is
y = c1J1/2(z) + c2J−1/2(z) + π
2 J1/2(z)
Z z
ζ2J−1/2(ζ) dζ −π
2 J−1/2(z)
Z z
ζ2J1/2(ζ) dζ.
We could substitute
J1/2(z) =
 2
πz
1/2
sin z
and
J−1/2 =
 2
πz
1/2
cos z
into the solution, but we cannot evaluate the integrals in terms of elementary functions. (You can write the solution in
terms of Fresnel integrals.)
1642

Solution 34.5
W [Jν, Yν] =

Jν
Jν cot(νπ) −J−ν csc(νπ)
J′
ν
J′
ν cot(νπ) −J′
−ν csc(νπ)

= cot(νπ)

Jν
Jν
J′
ν
J′
ν
 −csc(νπ)

Jν
J−ν
J′
ν
J′
−ν

= −csc(νπ)−2
πz sin(πν)
= 2
πz
Since the Wronskian does not vanish identically, the functions are independent for all values of ν.
Solution 34.6
Iν(z) = ı−νJν(ız)
W [Iν, I−ν] =

Iν
I−ν
I′
ν
I′
−ν

=

ı−νJν(ız)
ıνJ−ν(ız)
ı−νıJ′
ν(ız)
ıνıJ′
−ν(ız)

= ı

Jν(ız)
J−ν(ız)
J′
ν(ız)
J′
−ν(ız)

= ı −2
ıπz sin(πν)
= −2
πz sin(πν)
1643

W [Iν, Kν] =

Iν
π
2 csc(πν)(I−ν −Iν)
I′
ν
π
2 csc(πν)(I′
−ν −I′
ν)

= π
2 csc(πν)

Iν
I−ν
I′
ν
I′
−ν
 −

Iν
Iν
I′
ν
I′
ν


= π
2 csc(πν)−2
πz sin(πν)
= −1
z
Solution 34.7
1. We diferentiate the generating function with respect to t.
e
z
2 (t−1/t) =
∞
X
n=−∞
Jn(z)tn
z
2

1 + 1
t2

e
z
2 (t−1/t) =
∞
X
n=−∞
Jn(z)ntn−1

1 + 1
t2

∞
X
n=−∞
Jn(z)tn = 2
z
∞
X
n=−∞
Jn(z)ntn−1
∞
X
n=−∞
Jn(z)tn +
∞
X
n=−∞
Jn(z)tn−2 = 2
z
∞
X
n=−∞
Jn(z)ntn−1
∞
X
n=−∞
Jn−1(z)tn−1 +
∞
X
n=−∞
Jn+1(z)tn−1 = 2
z
∞
X
n=−∞
Jn(z)ntn−1
Jn−1(z) + Jn+1(z) = 2
zJn(z)n
2n
z Jn(z) = Jn−1(z) + Jn+1(z)
1644

2. We diferentiate the generating function with respect to z.
e
z
2 (t−1/t) =
∞
X
n=−∞
Jn(z)tn
1
2

t −1
t

e
z
2 (t−1/t) =
∞
X
n=−∞
J′
n(z)tn
1
2

t −1
t

∞
X
n=−∞
Jn(z)tn =
∞
X
n=−∞
J′
n(z)tn
1
2
 
∞
X
n=−∞
Jn(z)tn+1 −
∞
X
n=−∞
Jn(z)tn−1
!
=
∞
X
n=−∞
J′
n(z)tn
1
2
 
∞
X
n=−∞
Jn−1(z)tn −
∞
X
n=−∞
Jn+1(z)tn
!
=
∞
X
n=−∞
J′
n(z)tn
1
2 (Jn−1(z) −Jn+1(z)) = J′
n(z)
J′
n(z) = 1
2 (Jn−1 −Jn+1)
3.
d
dz
 z−nJn(z)

= −nz−n−1Jn(z) + z−nJ′
n(z)
= −1
2z−n2n
z Jn(z) + z−n1
2 (Jn−1(z) −Jn+1(z))
= −1
2z−n (Jn+1(z) + Jn−1(z)) + 1
2z−n (Jn−1(z) −Jn+1(z))
d
dz
 z−nJn(z)

= −z−nJn+1(z)
1645

Solution 34.8
For this part we will use the identities
J′
ν(z) = ν
z Jν(z) −Jν+1(z),
J′
ν(z) = Jν−1(z) −ν
z Jν(z).

Jν(z)
J−ν(z)
J′
ν(z)
J′
−ν(z)
 = −2 sin(νπ)
πz

Jν(z)
J−ν(z)
Jν−1(z) −ν
zJν
−ν
zJ−ν(z) −J−ν+1(z)
 = −2 sin(νπ)
πz

Jν(z)
J−ν(z)
Jν−1(z)
−J−ν+1(z)
 −ν
z

Jν(z)
J−ν(z)
Jν(z)
J−ν(z)
 = −2 sin(νπ)
πz
−Jν+1(z)Jν(z) −Jν(z)Jν−1(z) = −2 sin(νπ)
πz
J−ν+1(z)Jν(z) + J−ν(z)Jν−1(z) = 2
πz sin νπ
Solution 34.9
The generating function for the Bessel functions is
e
1
2z(t−1/t) =
∞
X
n=−∞
Jn(z)tn.
(34.2)
1. We substitute t = 1 into the generating function.
∞
X
n=−∞
Jn(z) = 1
J0(z) +
∞
X
n=1
Jn(z) +
∞
X
n=1
J−n(z) = 1
1646

We use the identity J−n = (−1)nJn.
J0(z) +
∞
X
n=1
(1 + (−1)n) Jn(z) = 1
J0(z) + 2
∞
X
n=2
even n
Jn(z) = 1
J0(z) + 2
∞
X
n=1
J2n(z) = 1
2. We substitute t = ı into the generating function.
∞
X
n=−∞
Jn(z)ın = eız
J0(z) +
∞
X
n=1
Jn(z)ın +
∞
X
n=1
J−n(z)ı−n = eız
J0(z) +
∞
X
n=1
Jn(z)ın +
∞
X
n=1
(−1)nJn(z)(−ı)n = eız
J0(z) + 2
∞
X
n=1
Jn(z)ın = eız
(34.3)
Next we substitute t = −ı into the generating function.
J0(z) + 2
∞
X
n=1
(−1)nJn(z)ın = e−ız
(34.4)
1647

Dividing the sum of Equation 34.3 and Equation 34.4 by 2 gives us the desired identity.
J0(z) +
∞
X
n=1
(1 + (−1)n) Jn(z)ın = cos z
J0(z) + 2
∞
X
n=2
even n
Jn(z)ın = cos z
J0(z) + 2
∞
X
n=2
even n
(−1)n/2Jn(z) = cos z
J0(z) + 2
∞
X
n=1
(−1)nJ2n(z) = cos z
3. Dividing the diﬀerence of Equation 34.3 and Equation 34.4 by ı2 gives us the other identity.
−ı
∞
X
n=1
(1 −(−1)n) Jn(z)ın = sin z
2
∞
X
n=1
odd n
Jn(z)ın−1 = sin z
2
∞
X
n=1
odd n
(−1)(n−1)/2Jn(z) = sin z
2
∞
X
n=0
(−1)nJ2n+1(z) = sin z
1648

4. We substitute −t for t in the generating function.
e−1
2z(t−1/t) =
∞
X
n=−∞
Jn(z)(−t)n.
(34.5)
We take the product of Equation 34.2 and Equation 34.5 to obtain the ﬁnal identity.
 
∞
X
n=−∞
Jn(z)tn
!  
∞
X
m=−∞
Jm(z)(−t)m
!
= e
1
2 z(t−1/t) e−1
2z(t−1/t) = 1
Note that the coeﬃcients of all powers of t except t0 in the product of sums must vanish.
∞
X
n=−∞
Jn(z)tnJ−n(z)(−t)−n = 1
∞
X
n=−∞
J2
n(z) = 1
J2
0(z) + 2
∞
X
n=1
J2
n(z) = 1
Solution 34.10
First we make the change of variables y(x) = x1/2v(x). We compute the derivatives of y(x).
y′ = x1/2v′ + 1
2x−1/2v,
y′′ = x1/2v′′ + x−1/2v′ −1
4x−3/2v.
1649

We substitute these into the diﬀerential equation for y.
y′′ + xp−2y = 0
x1/2v′′ + x−1/2v′ −1
4x−3/2v + xp−3/2v = 0
x2v′′ + xv′ +

xp −1
4

v = 0
Then we make the change of variables v(x) = u(ξ), ξ = 2
pxp/2. We write the derivatives in terms of ξ.
x d
dx = xdξ
dx
d
dξ = xxp/2−1 d
dξ = p
2ξ d
dξ
x2 d2
dx2 + x d
dx = x d
dxx d
dx = p
2ξ d
dξ
p
2ξ d
dξ = p2
4 ξ2 d2
dξ2 + p2
4 ξ d
dξ
We write the diﬀerential equation for u(ξ).
p2
4 ξ2u′′ + p2
4 ξu′ +
p2
4 ξ2 −1
4

u = 0
u′′ + 1
ξ u′ +

1 −
1
p2ξ2

u = 0
This is the Bessel equation of order 1/p. We can write the general solution for u in terms of Bessel functions of the
ﬁrst kind if p ̸= ±1. Otherwise, we use a Bessel function of the second kind.
u(ξ) = c1J1/p(ξ) + c2J−1/p(ξ) for p ̸= 0, ±1
u(ξ) = c1J1/p(ξ) + c2Y1/p(ξ) for p ̸= 0
1650

We write the solution in terms of y(x).
y(x) = c1
√xJ1/p
2
pxp/2

+ c2
√xJ−1/p
2
pxp/2

for p ̸= 0, ±1
y(x) = c1
√xJ1/p
2
pxp/2

+ c2
√xY1/p
2
pxp/2

for p ̸= 0
The Airy equation y′′ + xy = 0 is the case p = 3. The general solution of the Airy equation is
y(x) = c1
√xJ1/3
2
3x3/2

+ c2
√xJ−1/3
2
3x3/2

.
Solution 34.11
Consider J1/2(z). We start with the series expansion.
J1/2(z) =
∞
X
m=0
(−1)m
m!Γ(1/2 + m + 1)
z
2
1/2+2m
.
Use the identity Γ(n + 1/2) = (1)(3)···(2n−1)
2n
√π.
=
∞
X
m=0
(−1)m2m+1
m!(1)(3) · · · (2m + 1)√π
z
2
1/2+2m
=
∞
X
m=0
(−1)m2m+1
(2)(4) · · · (2m) · (1)(3) · · · (2m + 1)√π
1
2
1/2+m
z1/2+2m
=
 2
πz
1/2
∞
X
m=0
(−1)m
(2m + 1)!z2m+1
1651

We recognize the sum as the Taylor series expansion of sin z.
=
 2
πz
1/2
sin z
Using the recurrence relations,
Jν+1 = ν
z Jν −J′
ν
and
Jν−1 = ν
z Jν + J′
ν,
we can ﬁnd Jn+1/2 for any integer n.
We need J3/2(z) to determine j1(z). To ﬁnd J3/2(z),
J3/2(z) = 1/2
z J1/2(z) −J′
1/2(z)
= 1/2
z
 2
π
1/2
z−1/2 sin z −

−1
2
  2
π
1/2
z−3/2 sin z −
 2
π
1/2
z−1/2 cos z
= 2−1/2π−1/2z−3/2 sin z + 2−1/2π−1/2z−3/2 sin z −2−1/2π−1/2 cos z
=
 2
π
1/2
z−3/2 sin z −
 2
π
1/2
z−1/2 cos z
=
 2
π
1/2  z−3/2 sin z −z−1/2 cos z

.
The spherical Bessel function j1(z) is
j1(z) = sin z
z2
−cos z
z
.
The modiﬁed Bessel function of the ﬁrst kind is
Iν(z) = ı−νJν(ız).
1652

We can determine I1/2(z) from J1/2(z).
I1/2(z) = ı−1/2
r
2
ıπz sin(ız)
= −ı
r
2
πzı sinh(z)
=
r
2
πz sinh(z)
The spherical Bessel function i0(z) is
i0(z) = sinh z
z
.
The modiﬁed Bessel function of the second kind is
Kν(z) = lim
µ→ν
π
2
I−µ −Iµ
sin(µπ)
Thus K1/2(z) can be determined in terms of I−1/2(z) and I1/2(z).
K1/2(z) = π
2
 I−1/2 −I1/2

We determine I−1/2 with the recursion relation
Iν−1(z) = I′
ν(z) + ν
z Iν(z).
I−1/2(z) = I′
1/2(z) + 1
2zI1/2(z)
=
r
2
πz−1/2 cosh(z) −1
2
r
2
πz−3/2 sinh(z) + 1
2z
r
2
πz−1/2 sinh(z)
=
r
2
πz cosh(z)
1653

Now we can determine K1/2(z).
K1/2(z) = π
2
 r
2
πz cosh(z) −
r
2
πz sinh(z)
!
=
r π
2z e−z
The spherical Bessel function k0(z) is
k0(z) = π
2z e−z .
Solution 34.12
The Point at Inﬁnity. With the change of variables z = 1/ζ, w(z) = u(ζ) the modiﬁed Bessel equation becomes
w′′ + 1
zw′ −

1 + n2
z2

w = 0
ζ4u′′ + 2ζ3u′ + ζ
 −ζ2
u′ −
 1 + n2ζ2
u = 0
u′′ + 1
ζ u′ −
 1
ζ4 −n2
ζ2

u = 0.
The point ζ = 0 and hence the point z = ∞is an irregular singular point. We will ﬁnd the leading order asymptotic
behavior of the solutions as z →+∞.
Controlling Factor. Starting with the modiﬁed Bessel equation for real argument
y′′ + 1
xy′ −

1 + n2
x2

y = 0,
we make the substitution y = es(x) to obtain
s′′ + (s′)2 + 1
xs′ −1 −n2
x2 = 0.
1654

We know that n2
x2 ≪1 as x →∞; we will assume that s′′ ≪(s′)2 as x →∞. This gives us
(s′)2 + 1
xs′ −1 ∼0
as x →∞.
To simplify the equation further, we will try the possible two-term balances.
1. (s′)2 + 1
xs′ ∼0
→
s′ ∼−1
x
This balance is not consistent as it violates the assumption that 1 is smaller
than the other terms.
2. (s′)2 −1 ∼0
→
s′ ∼±1
This balance is consistent.
3.
1
xs′ −1 ∼0
→
s′ ∼x
This balance is inconsistent as (s′)2 isn’t smaller than the other terms.
Thus the only dominant balance is s′ ∼±1. This balance is consistent with our initial assumption that s′′ ≪(s′)2.
Thus s ∼±x and the controlling factor is e±x. We are interested in the decaying solution, so we will work with the
controlling factor e−x.
Leading Order Behavior. In order to ﬁnd the leading order behavior, we substitute s = −x+t(x) where t(x) ≪x
as x →∞into the diﬀerential equation for s. We assume that t′ ≪1 and t′′ ≪1/x.
t′′ + (−1 + t′)2 + 1
x(−1 + t′) −1 −n2
x2 = 0
t′′ −2t′ + (t′)2 −1
x + 1
xt′ −n2
x2 = 0
Using our assumptions about the behavior of t′ and t′′,
−2t′ −1
x ∼0
t′ ∼−1
2x
t ∼−1
2 ln x
as x →∞.
1655

This asymptotic behavior is consistent with our assumptions.
Thus the leading order behavior of the decaying solution is
y ∼c e−x−1
2 ln x+u(x) = cx−1/2 e−x+u(x)
as x →∞,
where u(x) ≪ln x as x →∞.
By substituting t = −1
2 ln x+u(x) into the diﬀerential equation for t, you could show that u(x) →const as x →∞.
Thus the full leading order behavior of the decaying solution is
y ∼cx−1/2 e−x
as x →∞
where u(x) →0 as x →∞. It turns out that the asymptotic behavior of the modiﬁed Bessel function of the second
kind is
Kn(x) ∼
r π
2x e−x
as x →∞
Asymptotic Series. Now we ﬁnd the full asymptotic series for Kn(x) as x →∞. We substitute
Kn(x) ∼
r π
2x e−x w(x)Kn(x) ∝e−x
√x
into the modiﬁed Bessel equation, where w(x) is a Taylor series about x = ∞, i.e.,
Kn(x) ∼
r π
2x e−x
∞
X
k=0
akx−k,
a0 = 1.
First we diﬀerentiate the expression for Kn(x).
K′
n(x) ∼
r π
2x e−x

w′ −

1 + 1
2x

w

K′′
n(x) ∼
r π
2x e−x

w′′ −

2 + 1
x

w′ +

1 + 1
x + 3
4x2

w

1656

We substitute these expressions into the modiﬁed Bessel equation.
x2y′′ + xy′ −
 x2 + n2
y = 0
x2w′′ −
 2x2 + x

w′ +

x2 + x + 3
4

w + xw′ −

x + 1
2

w −
 x2 + n2
w = 0
x2w′′ −2x2w′ +
1
4 −n2

w = 0
We compute the derivatives of the Taylor series.
w′ =
∞
X
k=1
(−k)akx−k−1
=
∞
X
k=0
(−k −1)ak+1x−k−2
w′′ =
∞
X
k=1
(−k)(−k −1)akx−k−2
=
∞
X
k=0
(−k)(−k −1)akx−k−2
We substitute these expression into the diﬀerential equation.
x2
∞
X
k=0
k(k + 1)akx−k−2 + 2x2
∞
X
k=0
(k + 1)ak+1x−k−2 +
1
4 −n2

∞
X
k=0
akx−k = 0
∞
X
k=0
k(k + 1)akx−k + 2
∞
X
k=0
(k + 1)ak+1x−k +
1
4 −n2

∞
X
k=0
akx−k = 0
1657

We equate coeﬃcients of x to obtain a recurrence relation for the coeﬃcients.
k(k + 1)ak + 2(k + 1)ak+1 +
1
4 −n2

ak = 0
ak+1 = n2 −1/4 −k(k + 1)
2(k + 1)
ak
ak+1 = n2 −(k + 1/2)2
2(k + 1)
ak
ak+1 = 4n2 −(2k + 1)2
8(k + 1)
ak
We set a0 = 1. We use the recurrence relation to determine the rest of the coeﬃcients.
ak =
Qk
j=1 (4n2 −(2j −1)2)
8kk!
Now we have the asymptotic expansion of the modiﬁed Bessel function of the second kind.
Kn(x) ∼
r π
2x e−x
∞
X
k=0
Qk
j=1 (4n2 −(2j −1)2)
8kk!
x−k,
as x →∞
Convergence. We determine the domain of convergence of the series with the ratio test. The Taylor series about
inﬁnity will converge outside of some circle.
lim
k→∞

ak+1(x)
ak(x)
 < 1
lim
k→∞

ak+1x−k−1
akx−k
 < 1
lim
k→∞

4n2 −(2k + 1)2
8(k + 1)
 |x|−1 < 1
∞< |x|
1658

The series does not converge for any x in the ﬁnite complex plane. However, if we take only a ﬁnite number of terms
in the series, it gives a good approximation of Kn(x) for large, positive x. At x = 10, the one, two and three term
approximations give relative errors of 0.01, 0.0006 and 0.00006, respectively.
1659

Part V
Partial Diﬀerential Equations
1660

Chapter 35
Transforming Equations
I’m about two beers away from ﬁne.
Let {xi} denote rectangular coordinates. Let {ai} be unit basis vectors in the orthogonal coordinate system {ξi}.
The distance metric coeﬃcients hi can be deﬁned
hi =
s∂x1
∂ξi
2
+
∂x2
∂ξi
2
+
∂x3
∂ξi
2
.
The gradient, divergence, etc., follow.
∇u = a1
h1
∂u
∂ξ1
+ a2
h2
∂u
∂ξ2
+ a3
h3
∂u
∂ξ3
∇· v =
1
h1h2h3
 ∂
∂ξ1
(h2h3v1) + ∂
∂ξ2
(h3h1v2) + ∂
∂ξ3
(h1h2v3)

∇2u =
1
h1h2h3
 ∂
∂ξ1
h2h3
h1
∂u
∂ξ1

+ ∂
∂ξ2
h3h1
h2
∂u
∂ξ2

+ ∂
∂ξ3
h1h2
h3
∂u
∂ξ3

1661

35.1
Exercises
Exercise 35.1
Find the Laplacian in cylindrical coordinates (r, θ, z).
x = r cos θ,
y = r sin θ,
z
Hint, Solution
Exercise 35.2
Find the Laplacian in spherical coordinates (r, φ, θ).
x = r sin φ cos θ,
y = r sin φ sin θ,
z = r cos φ
Hint, Solution
1662

35.2
Hints
Hint 35.1
Hint 35.2
1663

35.3
Solutions
Solution 35.1
h1 =
p
(cos θ)2 + (sin θ)2 + 0 = 1
h2 =
p
(−r sin θ)2 + (r cos θ)2 + 0 = r
h3 =
√
0 + 0 + 12 = 1
∇2u = 1
r
 ∂
∂r

r∂u
∂r

+ ∂
∂θ
1
r
∂u
∂θ

+ ∂
∂z

r∂u
∂z

∇2u = 1
r
∂
∂r

r∂u
∂r

+ 1
r2
∂2u
∂θ2 + ∂2u
∂z2
Solution 35.2
h1 =
p
(sin φ cos θ)2 + (sin φ sin θ)2 + (cos φ)2 = 1
h2 =
p
(r cos φ cos θ)2 + (r cos φ sin θ)2 + (−r sin φ)2 = r
h3 =
p
(−r sin φ sin θ)2 + (r sin φ cos θ)2 + 0 = r sin φ
∇2u =
1
r2 sin φ
 ∂
∂r

r2 sin φ∂u
∂r

+ ∂
∂φ

sin φ∂u
∂φ

+ ∂
∂θ

1
sin φ
∂u
∂θ

∇2u = 1
r2
∂
∂r

r2∂u
∂r

+
1
r2 sin φ
∂
∂φ

sin φ∂u
∂φ

+
1
r2 sin φ
∂2u
∂θ2
1664

Chapter 36
Classiﬁcation of Partial Diﬀerential Equations
36.1
Classiﬁcation of Second Order Quasi-Linear Equations
Consider the general second order quasi-linear partial diﬀerential equation in two variables.
a(x, y)uxx + 2b(x, y)uxy + c(x, y)uyy = F(x, y, u, ux, uy)
(36.1)
We classify the equation by the sign of the discriminant. At a given point x0, y0, the equation is classiﬁed as one of
the following types:
b2 −ac > 0 :
hyperbolic
b2 −ac = 0 :
parabolic
b2 −ac < 0 :
elliptic
If an equation has a particular type for all points x, y in a domain then the equation is said to be of that type in the
domain. Each of these types has a canonical form that can be obtained through a change of independent variables.
The type of an equation indicates much about the nature of its solution.
We seek a change of independent variables, (a diﬀerent coordinate system), such that Equation 36.1 has a simpler
form. We will ﬁnd that a second order quasi-linear partial diﬀerential equation in two variables can be transformed to
1665

one of the canonical forms:
uξψ = G(ξ, ψ, u, uξ, uψ),
hyperbolic
uξξ = G(ξ, ψ, u, uξ, uψ),
parabolic
uξξ + uψψ = G(ξ, ψ, u, uξ, uψ),
elliptic
Consider the change of independent variables
ξ = ξ(x, y),
ψ = ψ(x, y).
We calculate the partial derivatives of u.
ux = ξxuξ + ψxuψ
uy = ξyuξ + ψyuψ
uxx = ξ2
xuξξ + 2ξxψxuξψ + ψ2
xuψψ + ξxxuξ + ψxxuψ
uxy = ξxξyuξξ + (ξxψy + ξyψx)uξψ + ψxψyuψψ + ξxyuξ + ψxyuψ
uyy = ξ2
yuξξ + 2ξyψyuξψ + ψ2
yuψψ + ξyyuξ + ψyyuψ
Substituting these into Equation 36.1 yields an equation in ξ and ψ.
 aξ2
x + 2bξxξy + cξ2
y

uξξ + 2 (aξxψx + b(ξxψy + ξyψx) + cξyψy) uξψ
+
 aψ2
x + 2bψxψy + cψ2
y

uψψ = H(ξ, ψ, u, uξ, uψ)
α(ξ, ψ)uξξ + β(ξ, ψ)uξψ + γ(ξ, ψ)uψψ = H(ξ, ψ, u, uξ, uψ)
(36.2)
36.1.1
Hyperbolic Equations
We start with a hyperbolic equation, (b2−ac > 0). We seek a change of independent variables that will put Equation 36.1
in the form
uξψ = G(ξ, ψ, u, uξ, uψ)
(36.3)
1666

We require that the uξξ and uψψ terms vanish. That is α = γ = 0 in Equation 36.2. This gives us two constraints on
ξ and ψ.
aξ2
x + 2bξxξy + cξ2
y = 0,
aψ2
x + 2bψxψy + cψ2
y = 0
(36.4)
ξx
ξy
= −b +
√
b2 −ac
a
,
ψx
ψy
= −b −
√
b2 −ac
a
ξx + b −
√
b2 −ac
a
ξy = 0,
ψx + b +
√
b2 −ac
a
ψy = 0
Here we chose the signs in the quadratic formulas to get diﬀerent solutions for ξ and ψ.
Now we have ﬁrst order quasi-linear partial diﬀerential equations for the coordinates ξ and ψ. We solve these
equations with the method of characteristics. The characteristic equations for ξ are
dy
dx = b −
√
b2 −ac
a
,
d
dxξ(x, y(x)) = 0
Solving the diﬀerential equation for y(x) determines ξ(x, y). We just write the solution for y(x) in the form F(x, y(x)) =
const. Since the solution of the diﬀerential equation for ξ is ξ(x, y(x)) = const, we then have ξ = F(x, y). Upon
solving for ξ and ψ we divide Equation 36.2 by β(ξ, ψ) to obtain the canonical form.
Note that we could have solved for ξy/ξx in Equation 36.4.
dx
dy = −ξy
ξx
= b −
√
b2 −ac
c
This form is useful if a vanishes.
Another canonical form for hyperbolic equations is
uσσ −uττ = K(σ, τ, u, uσ, uτ).
(36.5)
1667

We can transform Equation 36.3 to this form with the change of variables
σ = ξ + ψ,
τ = ξ −ψ.
Equation 36.3 becomes
uσσ −uττ = G
σ + τ
2
, σ −τ
2
, u, uσ + uτ, uσ −uτ

.
Example 36.1.1 Consider the wave equation with a source.
utt −c2uxx = s(x, t)
Since 0 −(1)(−c2) > 0, the equation is hyperbolic. We ﬁnd the new variables.
dx
dt = −c,
x = −ct + const,
ξ = x + ct
dx
dt = c,
x = ct + const,
ψ = x −ct
Then we determine t and x in terms of ξ and ψ.
t = ξ −ψ
2c
,
x = ξ + ψ
2
We calculate the derivatives of ξ and ψ.
ξt = c
ξx = 1
ψt = −c
ψx = 1
Then we calculate the derivatives of u.
utt = c2uξξ −2c2uξψ + c2uψψ
uxx = uξξ + uψψ
1668

Finally we transform the equation to canonical form.
−2c2uξψ = s
ξ + ψ
2
, ξ −ψ
2c

uξψ = −1
2c2s
ξ + ψ
2
, ξ −ψ
2c

If s(x, t) = 0, then the equation is uξψ = 0 we can integrate with respect to ξ and ψ to obtain the solution,
u = f(ξ) + g(ψ). Here f and g are arbitrary C2 functions. In terms of t and x, we have
u(x, t) = f(x + ct) + g(x −ct).
To put the wave equation in the form of Equation 36.5 we make a change of variables
σ = ξ + ψ = 2x,
τ = ξ −ψ = 2ct
utt −c2uxx = s(x, t)
4c2uττ −4c2uσσ = s
σ
2 , τ
2c

uσσ −uττ = −1
4c2s
σ
2 , τ
2c

Example 36.1.2 Consider
y2uxx −x2uyy = 0.
For x ̸= 0 and y ̸= 0 this equation is hyperbolic. We ﬁnd the new variables.
dy
dx = −
p
y2x2
y2
= −x
y ,
y dy = −x dx,
y2
2 = −x2
2 + const,
ξ = y2 + x2
dy
dx =
p
y2x2
y2
= x
y ,
y dy = x dx,
y2
2 = x2
2 + const,
ψ = y2 −x2
1669

We calculate the derivatives of ξ and ψ.
ξx = 2x
ξy = 2y
ψx = −2x
ψy = 2y
Then we calculate the derivatives of u.
ux = 2x(uξ −uψ)
uy = 2y(uξ + uψ)
uxx = 4x2(uξξ −2uξψ + uψψ) + 2(uξ −uψ)
uyy = 4y2(uξξ + 2uξψ + uψψ) + 2(uξ + uψ)
Finally we transform the equation to canonical form.
y2uxx −x2uyy = 0
−8x2y2uξψ −8x2y2uξψ + 2y2(uξ −uψ) + 2x2(uξ + uψ) = 0
161
2(ξ −ψ)1
2(ξ + ψ)uξψ = 2ξuξ −2ψuψ
uξψ = ξuξ −ψuψ
2(ξ2 −ψ2)
Example 36.1.3 Consider Laplace’s equation.
uxx + uyy = 0
Since 0 −(1)(1) < 0, the equation is elliptic. We will transform this equation to the canical form of Equation 36.3.
We ﬁnd the new variables.
dy
dx = −ı,
y = −ıx + const,
ξ = x + ıy
dy
dx = ı,
y = ıx + const,
ψ = x −ıy
1670

We calculate the derivatives of ξ and ψ.
ξx = 1
ξy = ı
ψx = 1
ψy = −ı
Then we calculate the derivatives of u.
uxx = uξξ + 2uξψ + uψψ
uyy = −uξξ + 2uξψ −uψψ
Finally we transform the equation to canonical form.
4uξψ = 0
uξψ = 0
We integrate with respect to ξ and ψ to obtain the solution, u = f(ξ) + g(ψ). Here f and g are arbitrary C2
functions. In terms of x and y, we have
u(x, y) = f(x + ıy) + g(x −ıy).
This solution makes a lot of sense, because the real and imaginary parts of an analytic function are harmonic.
36.1.2
Parabolic equations
Now we consider a parabolic equation, (b2 −ac = 0).
We seek a change of independent variables that will put
Equation 36.1 in the form
uξξ = G(ξ, ψ, u, uξ, uψ).
(36.6)
We require that the uξψ and uψψ terms vanish. That is β = γ = 0 in Equation 36.2. This gives us two constraints on
ξ and ψ.
aξxψx + b(ξxψy + ξyψx) + cξyψy = 0,
aψ2
x + 2bψxψy + cψ2
y = 0
1671

We consider the case a ̸= 0. The latter constraint allows us to solve for ψx/ψy.
ψx
ψy
= −b −
√
b2 −ac
a
= −b
a
With this information, the former constraint is trivial.
aξxψx + b(ξxψy + ξyψx) + cξyψy = 0
aξx(−b/a) + b(ξx + ξy(−b/a)) + cξy = 0
(ac −b2)ξy = 0
0 = 0
Thus we have a ﬁrst order partial diﬀerential equation for the ψ coordinate which we can solve with the method of
characteristics.
ψx + b
aψy = 0
The ξ coordinate is chosen to be anything linearly independent of ψ. The characteristic equations for ψ are
dy
dx = b
a,
d
dxψ(x, y(x)) = 0
Solving the diﬀerential equation for y(x) determines ψ(x, y). We just write the solution for y(x) in the form F(x, y(x)) =
const. Since the solution of the diﬀerential equation for ψ is ψ(x, y(x)) = const, we then have ψ = F(x, y). Upon
solving for ψ and choosing a linearly independent ξ, we divide Equation 36.2 by α(ξ, ψ) to obtain the canonical form.
In the case that a = 0, we would instead have the constraint,
ψx + b
cψy = 0.
36.1.3
Elliptic Equations
We start with an elliptic equation, (b2−ac < 0). We seek a change of independent variables that will put Equation 36.1
in the form
uσσ + uττ = G(σ, τ, u, uσ, uτ)
(36.7)
1672

If we make the change of variables determined by
ξx
ξy
= −b + ı
√
ac −b2
a
,
ψx
ψy
= −b −ı
√
ac −b2
a
,
the equation will have the form
uξψ = G(ξ, ψ, u, uξ, uψ).
ξ and ψ are complex-valued. If we then make the change of variables
σ = ξ + ψ
2
,
τ = ξ −ψ
2ı
we will obtain the canonical form of Equation 36.7. Note that since ξ and ψ are complex conjugates, σ and τ are
real-valued.
Example 36.1.4 Consider
y2uxx + x2uyy = 0.
(36.8)
For x ̸= 0 and y ̸= 0 this equation is elliptic. We ﬁnd new variables that will put this equation in the form uξψ = G(·).
From Example 36.1.2 we see that they are
dy
dx = −ı
p
y2x2
y2
= −ıx
y ,
y dy = −ıx dx,
y2
2 = −ıx2
2 + const,
ξ = y2 + ıx2
dy
dx = ı
p
y2x2
y2
= ıx
y ,
y dy = ıx dx,
y2
2 = ıx2
2 + const,
ψ = y2 −ıx2
The variables that will put Equation 36.8 in canonical form are
σ = ξ + ψ
2
= y2,
τ = ξ −ψ
2ı
= x2
1673

We calculate the derivatives of σ and τ.
σx = 0
σy = 2y
τx = 2x
τy = 0
Then we calculate the derivatives of u.
ux = 2xuτ
uy = 2yuσ
uxx = 4x2uττ + 2uτ
uyy = 4y2uσσ + 2uσ
Finally we transform the equation to canonical form.
y2uxx + x2uyy = 0
σ(4τuττ + 2uτ) + τ(4σuσσ + 2uσ) = 0
uσσ + uττ = −1
2σuσ −1
2τ uτ
36.2
Equilibrium Solutions
Example 36.2.1 Consider the equilibrium solution for the following problem.
ut = uxx,
u(x, 0) = x,
ux(0, t) = ux(1, t) = 0
Setting ut = 0 we have an ordinary diﬀerential equation.
d2u
dx2 = 0
1674

This equation has the solution,
u = ax + b.
Applying the boundary conditions we see that
u = b.
To determine the constant, we note that the heat energy in the rod is constant in time.
Z 1
0
u(x, t) dx =
Z 1
0
u(x, 0) dx
Z 1
0
b dx =
Z 1
0
x dx
Thus the equilibrium solution is
u(x) = 1
2.
1675

36.3
Exercises
Exercise 36.1
Classify and transform the following equation into canonical form.
uxx + (1 + y)2uyy = 0
Hint, Solution
Exercise 36.2
Classify as hyperbolic, parabolic, or elliptic in a region R each of the equations:
1. ut = (pux)x
2. utt = c2uxx −γu
3. (qux)x + (qut)t = 0
where p(x), c(x, t), q(x, t), and γ(x) are given functions that take on only positive values in a region R of the (x, t)
plane.
Hint, Solution
Exercise 36.3
Transform each of the following equations for φ(x, y) into canonical form in appropriate regions
1. φxx −y2φyy + φx −φ + x2 = 0
2. φxx + xφyy = 0
The equation in part (b) is known as Tricomi’s equation and is a model for transonic ﬂuid ﬂow in which the ﬂow speed
changes from supersonic to subsonic.
Hint, Solution
1676

36.4
Hints
Hint 36.1
Hint 36.2
Hint 36.3
1677

36.5
Solutions
Solution 36.1
For y = −1, the equation is parabolic. For this case it is already in the canonical form, uxx = 0.
For y ̸= −1, the equation is elliptic.
We ﬁnd new variables that will put the equation in the form uξψ =
G(ξ, ψ, u, uξ, uψ).
dy
dx = ı
p
(1 + y)2 = ı(1 + y)
dy
1 + y = ıdx
log(1 + y) = ıx + c
1 + y = c eıx
(1 + y) e−ıx = c
ξ = (1 + y) e−ıx
ψ = ξ = (1 + y) eıx
The variables that will put the equation in canonical form are
σ = ξ + ψ
2
= (1 + y) cos x,
τ = ξ −ψ
ı2
= (1 + y) sin x.
We calculate the derivatives of σ and τ.
σx = −(1 + y) sin x
σy = cos x
τx = (1 + y) cos x
τy = sin x
Then we calculate the derivatives of u.
ux = −(1 + y) sin(x)uσ + (1 + y) cos(x)uτ
uy = cos(x)uσ + sin(x)uτ
uxx = (1 + y)2 sin2(x)uσσ + (1 + y)2 cos2(x)uττ −(1 + y) cos(x)uσ −(1 + y) sin(x)uτ
uyy = cos2(x)uσσ + sin2(x)uττ
1678

We substitute these results into the diﬀerential equation to obtain the canonical form.
uxx + (1 + y)2uyy = 0
(1 + y)2 (uσσ + uττ) −(1 + y) cos(x)uσ −(1 + y) sin(x)uτ = 0
 σ2 + τ 2
(uσσ + uττ) −σuσ −τuτ = 0
uσσ + uττ = σuσ + τuτ
σ2 + τ 2
Solution 36.2
1.
ut = (pux)x
puxx + 0uxt + 0utt + pxux −ut = 0
Since 02 −p0 = 0, the equation is parabolic.
2.
utt = c2uxx −γu
utt + 0utx −c2uxx + γu = 0
Since 02 −(1)(−c2) > 0, the equation is hyperbolic.
3.
(qux)x + (qut)t = 0
quxx + 0uxt + qutt + qxux + qtut = 0
Since 02 −qq < 0, the equation is elliptic.
1679

Solution 36.3
1. For y ̸= 0, the equation is hyperbolic. We ﬁnd the new independent variables.
dy
dx =
p
y2
1
= y,
y = c ex,
e−x y = c,
ξ = e−x y
dy
dx = −
p
y2
1
= −y,
y = c e−x,
ex y = c,
ψ = ex y
Next we determine x and y in terms of ξ and ψ.
ξψ = y2,
y =
p
ξψ
ψ = ex p
ξψ,
ex =
p
ψ/ξ,
x = 1
2 log
ψ
ξ

We calculate the derivatives of ξ and ψ.
ξx = −e−x y = −ξ
ξy = e−x =
p
ξ/ψ
ψx = ex y = ψ
ψy = ex =
p
ψ/ξ
Then we calculate the derivatives of φ.
∂
∂x = −ξ ∂
∂ξ + ψ ∂
∂ψ,
∂
∂y =
s
ξ
ψ
∂
∂ξ +
s
ψ
ξ
∂
∂ψ
φx = −ξφξ + ψφψ,
φy =
s
ξ
ψφξ +
s
ψ
ξ φψ
φxx = ξ2φξξ −2ξψφξψ + ψ2φψψ + ξφξ + ψφψ,
φyy = ξ
ψφξξ + 2φξψ + ψ
ξ φψψ
1680

Finally we transform the equation to canonical form.
φxx −y2φyy + φx −φ + x2 = 0
−4ξψφξψ + ξφξ + ψφψ −ξφξ + ψφψ −φ + log
ψ
ξ

= 0
φξψ = 1
2ξφψ + φ −log
ψ
ξ

For y = 0 we have the ordinary diﬀerential equation
φxx + φx −φ + x2 = 0.
2. For x < 0, the equation is hyperbolic. We ﬁnd the new independent variables.
dy
dx =
√
−x,
y = 2
3x
√
−x + c,
ξ = 2
3x
√
−x −y
dy
dx = −
√
−x,
y = −2
3x
√
−x + c,
ψ = 2
3x
√
−x + y
Next we determine x and y in terms of ξ and ψ.
x = −
3
4(ξ + ψ)
1/3
,
y = ψ −ξ
2
We calculate the derivatives of ξ and ψ.
ξx =
√
−x =
3
4(ξ + ψ)
1/6
,
ξy = −1
ψx =
3
4(ξ + ψ)
1/6
,
ψy = 1
1681

Then we calculate the derivatives of φ.
φx =
3
4(ξ + ψ)
1/6
(φξ + φψ)
φy = −φξ + φψ
φxx =
3
4(ξ + ψ)
1/3
(φξξ + φψψ) + (6(ξ + ψ))1/3φξψ + (6(ξ + ψ))−2/3 (φξ + φψ)
φyy = φξξ −2φξψ + φψψ
Finally we transform the equation to canonical form.
φxx + xφyy = 0
(6(ξ + ψ))1/3φξψ + (6(ξ + ψ))1/3φξψ + (6(ξ + ψ))−2/3 (φξ + φψ) = 0
φξψ = −φξ + φψ
12(ξ + ψ)
For x > 0, the equation is elliptic. The variables we deﬁned before are complex-valued.
ξ = ı2
3x3/2 −y,
ψ = ı2
3x3/2 + y
We choose the new real-valued variables.
α = ξ −ψ,
β = −ı(ξ + ψ)
We write the derivatives in terms of α and β.
φξ = φα −ıφβ
φψ = −φα −ıφβ
φξψ = −φαα −φββ
1682

We transform the equation to canonical form.
φξψ = −φξ + φψ
12(ξ + ψ)
−φαα −φββ = −−2ıφβ
12ıβ
φαα + φββ = −φβ
6β
1683

Chapter 37
Separation of Variables
37.1
Eigensolutions of Homogeneous Equations
37.2
Homogeneous Equations with Homogeneous Boundary Condi-
tions
The method of separation of variables is a useful technique for ﬁnding special solutions of partial diﬀerential equations.
We can combine these special solutions to solve certain problems. Consider the temperature of a one-dimensional rod
of length h 1. The left end is held at zero temperature, the right end is insulated and the initial temperature distribution
is known at time t = 0. To ﬁnd the temperature we solve the problem:
∂u
∂t = κ∂2u
∂x2,
0 < x < h,
t > 0
u(0, t) = ux(h, t) = 0
u(x, 0) = f(x)
1Why h? Because l looks like 1 and we use L to denote linear operators
1684

We look for special solutions of the form, u(x, t) = X(x)T(t). Substituting this into the partial diﬀerential equation
yields
X(x)T ′(t) = κX′′(x)T(t)
T ′(t)
κT(t) = X′′(x)
X(x)
Since the left side is only dependent on t, the right side in only dependent on x, and the relation is valid for all t and
x, both sides of the equation must be constant.
T ′
κT = X′′
X = −λ
Here −λ is an arbitrary constant. (You’ll see later that this form is convenient.) u(x, t) = X(x)T(t) will satisfy the
partial diﬀerential equation if X(x) and T(t) satisfy the ordinary diﬀerential equations,
T ′ = −κλT
and
X′′ = −λX.
Now we see how lucky we are that this problem happens to have homogeneous boundary conditions 2. If the left
boundary condition had been u(0, t) = 1, this would imply X(0)T(t) = 1 which tells us nothing very useful about
either X or T. However the boundary condition u(0, t) = X(0)T(t) = 0, tells us that either X(0) = 0 or T(t) = 0.
Since the latter case would give us the trivial solution, we must have X(0) = 0. Likewise by looking at the right
boundary condition we obtain X′(h) = 0.
We have a regular Sturm-Liouville problem for X(x).
X′′ + λX = 0,
X(0) = X′(h) = 0
The eigenvalues and orthonormal eigenfunctions are
λn =
(2n −1)π
2h
2
,
Xn =
r
2
h sin
(2n −1)π
2h
x

,
n ∈Z+.
2Actually luck has nothing to do with it. I planned it that way.
1685

Now we solve the equation for T(t).
T ′ = −κλnT
T = c e−κλnt
The eigen-solutions of the partial diﬀerential equation that satisfy the homogeneous boundary conditions are
un(x, t) =
r
2
h sin
p
λnx

e−κλnt .
We seek a solution of the problem that is a linear combination of these eigen-solutions.
u(x, t) =
∞
X
n=1
an
r
2
h sin
p
λnx

e−κλnt
We apply the initial condition to ﬁnd the coeﬃcients in the expansion.
u(x, 0) =
∞
X
n=1
an
r
2
h sin
p
λnx

= f(x)
an =
r
2
h
Z h
0
sin
p
λnx

f(x) dx
37.3
Time-Independent Sources and Boundary Conditions
Consider the temperature in a one-dimensional rod of length h.
The ends are held at temperatures α and β,
respectively, and the initial temperature is known at time t = 0. Additionally, there is a heat source, s(x), that is
independent of time. We ﬁnd the temperature by solving the problem,
ut = κuxx + s(x),
u(0, t) = α,
u(h, t) = β,
u(x, 0) = f(x).
(37.1)
Because of the source term, the equation is not separable, so we cannot directly apply separation of variables. Fur-
thermore, we have the added complication of inhomogeneous boundary conditions. Instead of attacking this problem
directly, we seek a transformation that will yield a homogeneous equation and homogeneous boundary conditions.
1686

Consider the equilibrium temperature, µ(x). It satisﬁes the problem,
µ′′(x) = −s(x)
κ
= 0,
µ(0) = α,
µ(h) = β.
The Green function for this problem is,
G(x; ξ) = x<(x> −h)
h
.
The equilibrium temperature distribution is
µ(x) = αx −h
h
+ β x
h −1
κh
Z h
0
x<(x> −h)s(ξ) dξ,
µ(x) = α + (β −α)x
h −1
κh

(x −h)
Z x
0
ξs(ξ) dξ + x
Z h
x
(ξ −h)s(ξ) dξ

.
Now we substitute u(x, t) = v(x, t) + µ(x) into Equation 37.1.
∂
∂t(v + µ(x)) = κ ∂2
∂x2(v + µ(x)) + s(x)
vt = κvxx + κµ′′(x) + s(x)
vt = κvxx
(37.2)
Since the equilibrium solution satisﬁes the inhomogeneous boundary conditions, v(x, t) satisﬁes homogeneous boundary
conditions.
v(0, t) = v(h, t) = 0.
The initial value of v is
v(x, 0) = f(x) −µ(x).
1687

We seek a solution for v(x, t) that is a linear combination of eigen-solutions of the heat equation. We substitute the
separation of variables, v(x, t) = X(x)T(t) into Equation 37.2
T ′
κT = X′′
X = −λ
This gives us two ordinary diﬀerential equations.
X′′ + λX = 0,
X(0) = X(h) = 0
T ′ = −κλT.
The Sturm-Liouville problem for X(x) has the eigenvalues and orthonormal eigenfunctions,
λn =
nπ
h
2
,
Xn =
r
2
h sin
nπx
h

,
n ∈Z+.
We solve for T(t).
Tn = c e−κ(nπ/h)2t .
The eigen-solutions of the partial diﬀerential equation are
vn(x, t) =
r
2
h sin
nπx
h

e−κ(nπ/h)2t .
The solution for v(x, t) is a linear combination of these.
v(x, t) =
∞
X
n=1
an
r
2
h sin
nπx
h

e−κ(nπ/h)2t
We determine the coeﬃcients in the series with the initial condition.
v(x, 0) =
∞
X
n=1
an
r
2
h sin
nπx
h

= f(x) −µ(x)
an =
r
2
h
Z h
0
sin
nπx
h

(f(x) −µ(x)) dx
1688

The temperature of the rod is
u(x, t) = µ(x) +
∞
X
n=1
an
r
2
h sin
nπx
h

e−κ(nπ/h)2t
37.4
Inhomogeneous Equations with Homogeneous Boundary Con-
ditions
Now consider the heat equation with a time dependent source, s(x, t).
ut = κuxx + s(x, t),
u(0, t) = u(h, t) = 0,
u(x, 0) = f(x).
(37.3)
In general we cannot transform the problem to one with a homogeneous diﬀerential equation. Thus we cannot represent
the solution in a series of the eigen-solutions of the partial diﬀerential equation. Instead, we will do the next best thing
and expand the solution in a series of eigenfunctions in Xn(x) where the coeﬃcients depend on time.
u(x, t) =
∞
X
n=1
un(t)Xn(x)
We will ﬁnd these eigenfunctions with the separation of variables, u(x, t) = X(x)T(t) applied to the homogeneous
equation, ut = κuxx, which yields,
Xn(x) =
r
2
h sin
nπx
h

,
n ∈Z+.
We expand the heat source in the eigenfunctions.
s(x, t) =
∞
X
n=1
sn(t)
r
2
h sin
nπx
h

sn(t) =
r
2
h
Z h
0
sin
nπx
h

s(x, t) dx,
1689

We substitute the series solution into Equation 37.3.
∞
X
n=1
u′
n(t)
r
2
h sin
nπx
h

= −κ
∞
X
n=1
un(t)
nπ
h
2 r
2
h sin
nπx
h

+
∞
X
n=1
sn(t)
r
2
h sin
nπx
h

u′
n(t) + κ
nπ
h
2
un(t) = sn(t)
Now we have a ﬁrst order, ordinary diﬀerential equation for each of the un(t). We obtain initial conditions from the
initial condition for u(x, t).
u(x, 0) =
∞
X
n=1
un(0)
r
2
h sin
nπx
h

= f(x)
un(0) =
r
2
h
Z h
0
sin
nπx
h

f(x) dx ≡fn
The temperature is given by
u(x, t) =
∞
X
n=1
un(t)
r
2
h sin
nπx
h

,
un(t) = fn e−κ(nπ/h)2t +
Z t
0
e−κ(nπ/h)2(t−τ) sn(τ) dτ.
37.5
Inhomogeneous Boundary Conditions
Consider the temperature of a one-dimensional rod of length h. The left end is held at the temperature α(t), the
heat ﬂow at right end is speciﬁed, there is a time-dependent source and the initial temperature distribution is known
at time t = 0. To ﬁnd the temperature we solve the problem:
ut = κuxx + s(x, t),
0 < x < h,
t > 0
(37.4)
u(0, t) = α(t),
ux(h, t) = β(t)
u(x, 0) = f(x)
1690

Transformation to a homogeneous equation.
Because of the inhomogeneous boundary conditions, we cannot
directly apply the method of separation of variables. However we can transform the problem to an inhomogeneous
equation with homogeneous boundary conditions.
To do this, we ﬁrst ﬁnd a function, µ(x, t) which satisﬁes the
boundary conditions. We note that
µ(x, t) = α(t) + xβ(t)
does the trick. We make the change of variables
u(x, t) = v(x, t) + µ(x, t)
in Equation 37.4.
vt + µt = κ (vxx + µxx) + s(x, t)
vt = κvxx + s(x, t) −µt
The boundary and initial conditions become
v(0, t) = 0,
vx(h, t) = 0,
v(x, 0) = f(x) −µ(x, 0).
Thus we have a heat equation with the source s(x, t) −µt(x, t). We could apply separation of variables to ﬁnd a
solution of the form
u(x, t) = µ(x, t) +
∞
X
n=1
un(t)
r
2
h sin
(2n −1)πx
2h

.
Direct eigenfunction expansion.
Alternatively we could seek a direct eigenfunction expansion of u(x, t).
u(x, t) =
∞
X
n=1
un(t)
r
2
h sin
(2n −1)πx
2h

.
Note that the eigenfunctions satisfy the homogeneous boundary conditions while u(x, t) does not. If we choose any
ﬁxed time t = t0 and form the periodic extension of the function u(x, t0) to deﬁne it for x outside the range (0, h), then
1691

this function will have jump discontinuities. This means that our eigenfunction expansion will not converge uniformly.
We are not allowed to diﬀerentiate the series with respect to x. We can’t just plug the series into the partial diﬀerential
equation to determine the coeﬃcients. Instead, we will multiply Equation 37.4, by an eigenfunction and integrate from
x = 0 to x = h. To avoid diﬀerentiating the series with respect to x, we will use integration by parts to move derivatives
from u(x, t) to the eigenfunction. (We will denote λn =

(2n−1)π
2h
2
.)
r
2
h
Z h
0
sin(
p
λnx)(ut −κuxx) dx =
r
2
h
Z h
0
sin(
p
λnx)s(x, t) dx
u′
n(t) −
r
2
hκ
h
ux sin(
p
λnx)
ih
0 +
r
2
hκ
p
λn
Z h
0
ux cos(
p
λnx) dx = sn(t)
u′
n(t) −
r
2
hκ(−1)nux(h, t) +
r
2
hκ
p
λn
h
u cos(
p
λnx)
ih
0 +
r
2
hκλn
Z h
0
u sin(
p
λnx) dx = sn(t)
u′
n(t) −
r
2
hκ(−1)nβ(t) −
r
2
hκ
p
λnu(0, t) + κλnun(t) = sn(t)
u′
n(t) + κλnun(t) =
r
2
hκ
p
λnα(t) + (−1)nβ(t)

+ sn(t)
Now we have an ordinary diﬀerential equation for each of the un(t). We obtain initial conditions for them using the
initial condition for u(x, t).
u(x, 0) =
∞
X
n=1
un(0)
r
2
h sin(
p
λnx) = f(x)
un(0) =
r
2
h
Z h
0
sin(
p
λnx)f(x) dx ≡fn
1692

Thus the temperature is given by
u(x, t) =
r
2
h
∞
X
n=1
un(t) sin(
p
λnx),
un(t) = fn e−κλnt +
r
2
hκ
Z t
0
e−κλn(t−τ) p
λnα(τ) + (−1)nβ(τ)

dτ.
37.6
The Wave Equation
Consider an elastic string with a free end at x = 0 and attached to a massless spring at x = 1. The partial diﬀerential
equation that models this problem is
utt = uxx
ux(0, t) = 0,
ux(1, t) = −u(1, t),
u(x, 0) = f(x),
ut(x, 0) = g(x).
We make the substitution u(x, t) = ψ(x)φ(t) to obtain
φ′′
φ = ψ′′
ψ = −λ.
First we consider the problem for ψ.
ψ′′ + λψ = 0,
ψ′(0) = ψ(1) + ψ′(1) = 0.
To ﬁnd the eigenvalues we consider the following three cases:
λ < 0. The general solution is
ψ = a cosh(
√
−λx) + b sinh(
√
−λx).
1693

ψ′(0) = 0
⇒
b = 0.
ψ(1) + ψ′(1) = 0
⇒
a cosh(
√
−λ) + a
√
−λ sinh(
√
−λ) = 0
⇒
a = 0.
Since there is only the trivial solution, there are no negative eigenvalues.
λ = 0. The general solution is
ψ = ax + b.
ψ′(0) = 0
⇒
a = 0.
ψ(1) + ψ′(1) = 0
⇒
b + 0 = 0.
Thus λ = 0 is not an eigenvalue.
λ > 0. The general solution is
ψ = a cos(
√
λx) + b sin(
√
λx).
ψ′(0)
⇒
b = 0.
ψ(1) + ψ′(1) = 0
⇒
a cos(
√
λ) −a
√
λ sin(
√
λ) = 0
⇒
cos(
√
λ) =
√
λ sin(
√
λ)
⇒
√
λ = cot(
√
λ)
By looking at Figure 37.1, (the plot shows the functions f(x) = x, f(x) = cot x and has lines at x = nπ), we
see that there are an inﬁnite number of positive eigenvalues and that
λn →(nπ)2 as n →∞.
The eigenfunctions are
ψn = cos(
p
λnx).
1694

2
4
6
8
10
-2
2
4
6
8
10
Figure 37.1: Plot of x and cot x.
The solution for φ is
φn = an cos(
p
λnt) + bn sin(
p
λnt).
Thus the solution to the diﬀerential equation is
u(x, t) =
∞
X
n=1
cos(
p
λnx)[an cos(
p
λnt) + bn sin(
p
λnt)].
Let
f(x) =
∞
X
n=1
fn cos(
p
λnx)
g(x) =
∞
X
n=1
gn cos(
p
λnx).
1695

From the initial value we have
∞
X
n=1
cos(
p
λnx)an =
∞
X
n=1
fn cos(
p
λnx)
an = fn.
The initial velocity condition gives us
∞
X
n=1
cos(
p
λnx)
p
λnbn =
∞
X
n=1
gn cos(
p
λnx)
bn =
gn
√λn
.
Thus the solution is
u(x, t) =
∞
X
n=1
cos(
p
λnx)

fn cos(
p
λnt) + gn
√λn
sin(
p
λnt)

.
37.7
General Method
Here is an outline detailing the method of separation of variables for a linear partial diﬀerential equation for u(x, y, z, . . .).
1. Substitute u(x, y, z, . . .) = X(x)Y (y)Z(z) · · · into the partial diﬀerential equation. Separate the equation into
ordinary diﬀerential equations.
2. Translate the boundary conditions for u into boundary conditions for X, Y , Z, . . .. The continuity of u may give
additional boundary conditions and boundedness conditions.
3. Solve the diﬀerential equation(s) that determine the eigenvalues. Make sure to consider all cases. The eigen-
functions will be determined up to a multiplicative constant.
1696

4. Solve the rest of the diﬀerential equations subject to the homogeneous boundary conditions. The eigenvalues will
be a parameter in the solution. The solutions will be determined up to a multiplicative constant.
5. The eigen-solutions are the product of the solutions of the ordinary diﬀerential equations. φn = XnYnZn · · · .
The solution of the partial diﬀerential equation is a linear combination of the eigen-solutions.
u(x, y, z, . . .) =
X
anφn
6. Solve for the coeﬃcients, an using the inhomogeneous boundary conditions.
1697

37.8
Exercises
Exercise 37.1
Solve the following problem with separation of variables.
ut −κ(uxx + uyy) = q(x, y, t),
0 < x < a, 0 < y < b
u(x, y, 0) = f(x, y),
u(0, y, t) = u(a, y, t) = u(x, 0, t) = u(x, b, t) = 0
Hint, Solution
Exercise 37.2
Consider a thin half pipe of unit radius laying on the ground. It is heated by radiation from above. We take the initial
temperature of the pipe and the temperature of the ground to be zero. We model this problem with a heat equation
with a source term.
ut = κuxx + A sin(x)
u(0, t) = u(π, t) = 0,
u(x, 0) = 0
Hint, Solution
Exercise 37.3
Consider Laplace’s Equation ∇2u = 0 inside the quarter circle of radius 1 (0 ≤θ ≤π
2, 0 ≤r ≤1). Write the problem
in polar coordinates u = u(r, θ) and use separation of variables to ﬁnd the solution subject to the following boundary
conditions.
1.
∂u
∂θ (r, 0) = 0,
u

r, π
2

= 0,
u(1, θ) = f(θ)
2.
∂u
∂θ (r, 0) = 0,
∂u
∂θ

r, π
2

= 0,
∂u
∂r (1, θ) = g(θ)
Under what conditions does this solution exist?
1698

Hint, Solution
Exercise 37.4
Consider the 2-D heat equation
ut = ν(uxx + uyy),
on a square plate 0 < x < 1, 0 < y < 1 with two sides insulated
ux(0, y, t) = 0
ux(1, y, t) = 0,
two sides with ﬁxed temperature
u(x, 0, t) = 0
u(x, 1, t) = 0,
and initial temperature
u(x, y, 0) = f(x, y).
1. Reduce this to a set of 3 ordinary diﬀerential equations using separation of variables.
2. Find the corresponding set of eigenfunctions and give the solution satisfying the given initial condition.
Hint, Solution
Exercise 37.5
Solve the 1-D heat equation
ut = νuxx,
on the domain 0 < x < π subject to conditions that the ends are insulated (i.e. zero ﬂux)
ux(0, t) = 0
ux(π, t) = 0,
and the initial temperature distribution is u(x, 0) = x.
Hint, Solution
1699

Exercise 37.6
Obtain Poisson’s formula to solve the Dirichlet problem for the circular region 0 ≤r < R, 0 ≤θ < 2π. That is,
determine a solution φ(r, θ) to Laplace’s equation
∇2φ = 0
in polar coordinates given φ(R, θ). Show that
φ(r, θ) = 1
2π
Z 2π
0
φ(R, α)
R2 −r2
R2 + r2 −2Rr cos(θ −α) dα
Hint, Solution
Exercise 37.7
Consider the temperature of a ring of unit radius. Solve the problem
ut = κuθθ,
u(θ, 0) = f(θ)
with separation of variables.
Hint, Solution
Exercise 37.8
Solve the Laplace’s equation by separation of variables.
∆u ≡uxx + uyy = 0,
0 < x < 1,
0 < y < 1,
u(x, 0) = f(x),
u(x, 1) = 0,
u(0, y) = 0,
u(1, y) = 0
Here f(x) is an arbitrary function which is known.
Hint, Solution
Exercise 37.9
Solve Laplace’s equation in the unit disk with separation of variables.
∆u = 0,
0 < r < 1
u(1, θ) = f(θ)
1700

The Laplacian in cirular coordinates is
∆u ≡∂2u
∂r2 + 1
r
∂u
∂r + 1
r2
∂2u
∂θ2 .
Hint, Solution
Exercise 37.10
Find the normal modes of oscillation of a drum head of unit radius. The drum head obeys the wave equation with zero
displacement on the boundary.
∆v ≡1
r
∂
∂r

r∂v
∂r

+ 1
r2
∂2v
∂θ2 = 1
c2
∂2v
∂t2 ,
v(1, θ, t) = 0
Hint, Solution
Exercise 37.11
Solve the equation
φt = a2φxx,
0 < x < l,
t > 0
with boundary conditions φ(0, t) = φ(l, t) = 0, and initial conditions
φ(x, 0) =
(
x,
0 ≤x ≤l/2,
l −x,
l/2 < x ≤l.
Comment on the diﬀerentiability ( that is the number of ﬁnite derivatives with respect to x ) at time t = 0 and at time
t = ϵ, where ϵ > 0 and ϵ ≪1.
Hint, Solution
Exercise 37.12
Consider a one-dimensional rod of length L with initial temperature distribution f(x). The temperatures at the left
and right ends of the rod are held at T0 and T1, respectively. To ﬁnd the temperature of the rod for t > 0, solve
ut = κuxx,
0 < x < L,
t > 0
u(0, t) = T0,
u(L, t) = T1,
u(x, 0) = f(x),
1701

with separation of variables.
Hint, Solution
Exercise 37.13
For 0 < x < l solve the problem
φt = a2φxx + w(x, t)
(37.5)
φ(0, t) = 0,
φx(l, t) = 0,
φ(x, 0) = f(x)
by means of a series expansion involving the eigenfunctions of
d2β(x)
dx2
+ λβ(x) = 0,
β(0) = β′(l) = 0.
Here w(x, t) and f(x) are prescribed functions.
Hint, Solution
Exercise 37.14
Solve the heat equation of Exercise 37.13 with the same initial conditions but with the boundary conditions
φ(0, t) = 0,
cφ(l, t) + φx(l, t) = 0.
Here c > 0 is a constant. Although it is not possible to solve for the eigenvalues λ in closed form, show that the
eigenvalues assume a simple form for large values of λ.
Hint, Solution
Exercise 37.15
Use a series expansion technique to solve the problem
φt = a2φxx + 1,
t > 0,
0 < x < l
with boundary and initial conditions given by
φ(x, 0) = 0,
φ(0, t) = t,
φx(l, t) = −cφ(l, t)
1702

where c > 0 is a constant.
Hint, Solution
Exercise 37.16
Let φ(x, t) satisfy the equation
φt = a2φxx
for 0 < x < l, t > 0 with initial conditions φ(x, 0) = 0 for 0 < x < l, with boundary conditions φ(0, t) = 0 for t > 0,
and φ(l, t) + φx(l, t) = 1 for t > 0. Obtain two series solutions for this problem, one which is useful for large t and the
other useful for small t.
Hint, Solution
Exercise 37.17
A rod occupies the portion 1 < x < 2 of the x-axis. The thermal conductivity depends on x in such a manner that the
temperature φ(x, t) satisﬁes the equation
φt = A2(x2φx)x
(37.6)
where A is a constant. For φ(1, t) = φ(2, t) = 0 for t > 0, with φ(x, 0) = f(x) for 1 < x < 2, show that the
appropriate series expansion involves the eigenfunctions
βn(x) =
1
√x sin
πn ln x
ln 2

.
Work out the series expansion for the given boundary and initial conditions.
Hint, Solution
Exercise 37.18
Consider a string of length L with a ﬁxed left end a free right end. Initially the string is at rest with displacement f(x).
Find the motion of the string by solving,
utt = c2uxx,
0 < x < L,
t > 0,
u(0, t) = 0,
ux(L, t) = 0,
u(x, 0) = f(x),
ut(x, 0) = 0,
1703

with separation of variables.
Hint, Solution
Exercise 37.19
Consider the equilibrium temperature distribution in a two-dimensional block of width a and height b. There is a heat
source given by the function f(x, y). The vertical sides of the block are held at zero temperature; the horizontal sides
are insulated. To ﬁnd this equilibrium temperature distribution, solve the potential equation,
uxx + uyy = f(x, y),
0 < x < a,
0 < y < b,
u(0, y) = u(a, y) = 0,
uy(x, 0) = uy(x, b) = 0,
with separation of variables.
Hint, Solution
Exercise 37.20
Consider the vibrations of a stiﬀbeam of length L. More precisely, consider the transverse vibrations of an unloaded
beam, whose weight can be neglected compared to its stiﬀness. The beam is simply supported at x = 0, L. (That is,
it is resting on fulcrums there. u(0, t) = 0 means that the beam is resting on the fulcrum; uxx(0, t) = 0 indicates that
there is no bending force at that point.) The beam has initial displacement f(x) and velocity g(x). To determine the
motion of the beam, solve
utt + a2uxxxx = 0,
0 < x < L,
t > 0,
u(x, 0) = f(x),
ut(x, 0) = g(x),
u(0, t) = uxx(0, t) = 0,
u(L, t) = uxx(L, t) = 0,
with separation of variables.
Hint, Solution
Exercise 37.21
The temperature along a magnet winding of length L carrying a current I satisﬁes, (for some α > 0):
ut = κuxx + I2αu.
1704

The ends of the winding are kept at zero, i.e.,
u(0, t) = u(L, t) = 0;
and the initial temperature distribution is
u(x, 0) = g(x).
Find u(x, t) and determine the critical current ICR which is deﬁned as the least current at which the winding begins to
heat up exponentially. Suppose that α < 0, so that the winding has a negative coeﬃcient of resistance with respect to
temperature. What can you say about the critical current in this case?
Hint, Solution
Exercise 37.22
The ”e-folding” time of a decaying function of time is the time interval, ∆e, in which the magnitude of the function
is reduced by at least 1
e. Thus if u(x, t) = e−αt f(x) + e−βt g(x) with α > β > 0 then ∆e = 1
β. A body with heat
conductivity κ has its exterior surface maintained at temperature zero. Initially the interior of the body is at the uniform
temperature T > 0. Find the e-folding time of the body if it is:
a) An inﬁnite slab of thickness a.
b) An inﬁnite cylinder of radius a.
c) A sphere of radius a.
Note that in (a) the temperature varies only in the z direction and in time; in (b) and (c) the temperature varies only
in the radial direction and in time.
d) What are the e-folding times if the surfaces are perfectly insulated, (i.e., ∂u
∂n = 0, where n is the exterior normal
at the surface)?
Hint, Solution
1705

Exercise 37.23
Solve the heat equation with a time-dependent diﬀusivity in the rectangle 0 < x < a, 0 < y < b. The top and bottom
sides are held at temperature zero; the lateral sides are insulated. We have the initial-boundary value problem:
ut = κ(t) (uxx + uyy) ,
0 < x < a,
0 < y < b,
t > 0,
u(x, 0, t) = u(x, b, t) = 0,
ux(0, y, t) = ux(a, y, t) = 0,
u(x, y, 0) = f(x, y).
The diﬀusivity, κ(t), is a known, positive function.
Hint, Solution
Exercise 37.24
A semi-circular rod of inﬁnite extent is maintained at temperature T = 0 on the ﬂat side and at T = 1 on the curved
surface:
x2 + y2 = 1,
y > 0.
Find the steady state temperature in a cross section of the rod using separation of variables.
Hint, Solution
Exercise 37.25
Use separation of variables to ﬁnd the steady state temperature u(x, y) in a slab: x ≥0, 0 ≤y ≤1, which has zero
temperature on the faces y = 0 and y = 1 and has a given distribution: u(y, 0) = f(y) on the edge x = 0, 0 ≤y ≤1.
Hint, Solution
Exercise 37.26
Find the solution of Laplace’s equation subject to the boundary conditions.
∆u = 0,
0 < θ < α,
a < r < b,
u(r, 0) = u(r, α) = 0,
u(a, θ) = 0,
u(b, θ) = f(θ).
Hint, Solution
1706

Exercise 37.27
a) A piano string of length L is struck, at time t = 0, by a ﬂat hammer of width 2d centered at a point ξ, having
velocity v. Find the ensuing motion, u(x, t), of the string for which the wave speed is c.
b) Suppose the hammer is curved, rather than ﬂat as above, so that the initial velocity distribution is
ut(x, 0) =
(
v cos

π(x−ξ)
2d

,
|x −ξ| < d
0
|x −ξ| > d.
Find the ensuing motion.
c) Compare the kinetic energies of each harmonic in the two solutions. Where should the string be struck in order
to maximize the energy in the nth harmonic in each case?
Hint, Solution
Exercise 37.28
If the striking hammer is not perfectly rigid, then its eﬀect must be included as a time dependent forcing term of the
form:
s(x, t) =
(
v cos

π(x−ξ)
2d

sin
  πt
δ

,
for |x −ξ| < d,
0 < t < δ,
0
otherwise.
Find the motion of the string for t > δ. Discuss the eﬀects of the width of the hammer and duration of the blow with
regard to the energy in overtones.
Hint, Solution
Exercise 37.29
Find the propagating modes in a square waveguide of side L for harmonic signals of frequency ω when the propagation
speed of the medium is c. That is, we seek those solutions of
utt −c2∆u = 0,
1707

where u = u(x, y, z, t) has the form u(x, y, z, t) = v(x, y, z) eıωt, which satisfy the conditions:
u(x, y, z, t) = 0
for
x = 0, L,
y = 0, L,
z > 0,
lim
z→∞|u| ̸= ∞and ̸= 0.
Indicate in terms of inequalities involving k = ω/c and appropriate eigenvalues, λn,m say, for which n and m the
solutions un,m satisfy the conditions.
Hint, Solution
Exercise 37.30
Find the modes of oscillation and their frequencies for a rectangular drum head of width a and height b. The modes of
oscillation are eigensolutions of
utt = c2∆u,
0 < x < a, 0 < y < b,
u(0, y) = u(a, y) = u(x, 0) = u(x, b) = 0.
Hint, Solution
Exercise 37.31
Using separation of variables solve the heat equation
φt = a2 (φxx + φyy)
in the rectangle 0 < x < lx, 0 < y < ly with initial conditions
φ(x, y, 0) = 1,
and boundary conditions
φ(0, y, t) = φ(lx, y, t) = 0,
φy(x, 0, t) = φy(x, ly, t) = 0.
Hint, Solution
1708

Exercise 37.32
Using polar coordinates and separation of variables solve the heat equation
φt = a2∇2φ
in the circle 0 < r < R0 with initial conditions
φ(r, θ, 0) = V
where V is a constant, and boundary conditions
φ(R0, θ, t) = 0.
1. Show that for t > 0,
φ(r, θ, t) = 2V
∞
X
n=1
exp

−a2j2
0,n
R2
0
t
 J0 (j0,nr/R0)
j0,nJ1(j0,n) ,
where j0,n are the roots of J0(x):
J0(j0,n) = 0,
n = 1, 2, . . .
Hint: The following identities may be of some help:
Z R0
0
rJ0 (j0,nr/R0) J0 (j0,mr/R0) dr = 0,
m ̸= n,
Z R0
0
rJ2
0 (j0,nr/R0) dr = R2
0
2 J2
1(j0,n),
Z r
0
rJ0(βr)dr = r
β J1(βr)
for any β.
2. For any ﬁxed r, 0 < r < R0, use the asymptotic approximation for the Jn Bessel functions for large argument
(this can be found in any standard math tables) to determine the rate of decay of the terms of the series solution
for φ at time t = 0.
Hint, Solution
1709

Exercise 37.33
Consider the solution of the diﬀusion equation in spherical coordinates given by
x
=
r sin θ cos φ,
y
=
r sin θ sin φ,
z
=
r cos θ,
where r is the radius, θ is the polar angle, and φ is the azimuthal angle. We wish to solve the equation on the surface
of the sphere given by r = R, 0 < θ < π, and 0 < φ < 2π. The diﬀusion equation for the solution Ψ(θ, φ, t) in these
coordinates on the surface of the sphere becomes
∂Ψ
∂t = a2
R2
 1
sin θ
∂
∂θ

sin θ∂Ψ
∂θ

+
1
sin2 θ
∂2Ψ
∂φ2

.
(37.7)
where a is a positive constant.
1. Using separation of variables show that a solution Ψ can be found in the form
Ψ(θ, φ, t) = T(t)Θ(θ)Φ(φ),
where T,Θ,Φ obey ordinary diﬀerential equations in t,θ, and φ respectively.
Derive the ordinary diﬀerential
equations for T and Θ, and show that the diﬀerential equation obeyed by Φ is given by
d2Φ
dφ2 −cΦ = 0,
where c is a constant.
2. Assuming that Ψ(θ, φ, t) is determined over the full range of the azimuthal angle, 0 < φ < 2π, determine the
allowable values of the separation constant c and the corresponding allowable functions Φ. Using these values
of c and letting x = cos θ rewrite in terms of the variable x the diﬀerential equation satisﬁed by Θ. What are
appropriate boundary conditions for Θ? The resulting equation is known as the generalized or associated Legendre
equation.
1710

3. Assume next that the initial conditions for Ψ are chosen such that
Ψ(θ, φ, t = 0) = f(θ),
where f(θ) is a speciﬁed function which is regular at the north and south poles (that is θ = 0 and θ = π).
Note that the initial condition is independent of the azimuthal angle φ. Show that in this case the method of
separation of variables gives a series solution for Ψ of the form
Ψ(θ, t) =
∞
X
l=0
Al exp(−λ2
l t)Pl(cos θ),
where Pl(x) is the l’th Legendre polynomial, and determine the constants λl as a function of the index l.
4. Solve for Ψ(θ, t), t > 0 given that f(θ) = 2 cos2 θ −1.
Useful facts:
d
dx

(1 −x2)dPl(x)
dx

+ l(l + 1)Pl(x) = 0
P0(x)
=
1
P1(x)
=
x
P2(x)
=
3
2x2 −1
2
Z 1
−1
dxPl(x)Pm(x) =
( 0
if l ̸= m
2
2l + 1
if l = m
Hint, Solution
Exercise 37.34
Let φ(x, y) satisfy Laplace’s equation
φxx + φyy = 0
1711

in the rectangle 0 < x < 1, 0 < y < 2, with φ(x, 2) = x(1 −x), and with φ = 0 on the other three sides. Use a series
solution to determine φ inside the rectangle. How many terms are required to give φ(1
2, 1) with about 1% (also 0.1%)
accuracy; how about φx(1
2, 1)?
Hint, Solution
Exercise 37.35
Let ψ(r, θ, φ) satisfy Laplace’s equation in spherical coordinates in each of the two regions r < a, r > a, with ψ →0
as r →∞. Let
lim
r→a+ ψ(r, θ, φ) −lim
r→a−ψ(r, θ, φ) = 0,
lim
r→a+ ψr(r, θ, φ) −lim
r→a−ψr(r, θ, φ) = P m
n (cos θ) sin(mφ),
where m and n ≥m are integers. Find ψ in r < a and r > a. In electrostatics, this problem corresponds to that of
determining the potential of a spherical harmonic type charge distribution over the surface of the sphere. In this way
one can determine the potential due to an arbitrary surface charge distribution since any charge distribution can be
expressed as a series of spherical harmonics.
Hint, Solution
Exercise 37.36
Obtain a formula analogous to the Poisson formula to solve the Neumann problem for the circular region 0 ≤r < R,
0 ≤θ < 2π. That is, determine a solution φ(r, θ) to Laplace’s equation
∇2φ = 0
in polar coordinates given φr(R, θ). Show that
φ(r, θ) = −R
2π
Z 2π
0
φr(R, α) ln

1 −2r
R cos(θ −α) + r2
R2

dα
within an arbitrary additive constant.
Hint, Solution
1712

Exercise 37.37
Investigate solutions of
φt = a2φxx
obtained by setting the separation constant C = (α + ıβ)2 in the equations obtained by assuming φ = X(x)T(t):
T ′
T = C,
X′′
X = C
a2.
Hint, Solution
1713

37.9
Hints
Hint 37.1
Hint 37.2
Hint 37.3
Hint 37.4
Hint 37.5
Hint 37.6
Hint 37.7
Impose the boundary conditions
u(0, t) = u(2π, t),
uθ(0, t) = uθ(2π, t).
Hint 37.8
Apply the separation of variables u(x, y) = X(x)Y (y). Solve an eigenvalue problem for X(x).
Hint 37.9
Hint 37.10
1714

Hint 37.11
Hint 37.12
There are two ways to solve the problem. For the ﬁrst method, expand the solution in a series of the form
u(x, t) =
∞
X
n=1
an(t) sin
nπx
L

.
Because of the inhomogeneous boundary conditions, the convergence of the series will not be uniform.
You can
diﬀerentiate the series with respect to t, but not with respect to x. Multiply the partial diﬀerential equation by the
eigenfunction sin(nπx/L) and integrate from x = 0 to x = L. Use integration by parts to move derivatives in x from
u to the eigenfunctions. This process will yield a ﬁrst order, ordinary diﬀerential equation for each of the an’s.
For the second method: Make the change of variables v(x, t) = u(x, t) −µ(x), where µ(x) is the equilibrium
temperature distribution to obtain a problem with homogeneous boundary conditions.
Hint 37.13
Hint 37.14
Hint 37.15
Hint 37.16
Hint 37.17
1715

Hint 37.18
Use separation of variables to ﬁnd eigen-solutions of the partial diﬀerential equation that satisfy the homogeneous
boundary conditions. There will be two eigen-solutions for each eigenvalue. Expand u(x, t) in a series of the eigen-
solutions. Use the two initial conditions to determine the constants.
Hint 37.19
Expand the solution in a series of eigenfunctions in x. Determine these eigenfunctions by using separation of variables
on the homogeneous partial diﬀerential equation. You will ﬁnd that the answer has the form,
u(x, y) =
∞
X
n=1
un(y) sin
nπx
a

.
Substitute this series into the partial diﬀerential equation to determine ordinary diﬀerential equations for each of the
un’s. The boundary conditions on u(x, y) will give you boundary conditions for the un’s. Solve these ordinary diﬀerential
equations with Green functions.
Hint 37.20
Solve this problem by expanding the solution in a series of eigen-solutions that satisfy the partial diﬀerential equation
and the homogeneous boundary conditions. Use the initial conditions to determine the coeﬃcients in the expansion.
Hint 37.21
Use separation of variables to ﬁnd eigen-solutions that satisfy the partial diﬀerential equation and the homogeneous
boundary conditions. The solution is a linear combination of the eigen-solutions. The whole solution will be exponentially
decaying if each of the eigen-solutions is exponentially decaying.
Hint 37.22
For parts (a), (b) and (c) use separation of variables. For part (b) the eigen-solutions will involve Bessel functions. For
part (c) the eigen-solutions will involve spherical Bessel functions. Part (d) is trivial.
Hint 37.23
The solution is a linear combination of eigen-solutions of the partial diﬀerential equation that satisfy the homogeneous
boundary conditions. Determine the coeﬃcients in the expansion with the initial condition.
1716

Hint 37.24
The problem is
urr + 1
rur + 1
r2uθθ = 0,
0 < r < 1,
0 < θ < π
u(r, 0) = u(r, π) = 0,
u(0, θ) = 0,
u(1, θ) = 1
The solution is a linear combination of eigen-solutions that satisfy the partial diﬀerential equation and the three
homogeneous boundary conditions.
Hint 37.25
Hint 37.26
Hint 37.27
Hint 37.28
Hint 37.29
Hint 37.30
Hint 37.31
Hint 37.32
1717

Hint 37.33
Hint 37.34
Hint 37.35
Hint 37.36
Hint 37.37
1718

37.10
Solutions
Solution 37.1
We expand the solution in eigenfunctions in x and y which satify the boundary conditions.
u =
∞
X
m,n=1
umn(t) sin
mπx
a

sin
nπy
b

We expand the inhomogeneities in the eigenfunctions.
q(x, y, t) =
∞
X
m,n=1
qmn(t) sin
mπx
a

sin
nπy
b

qmn(t) = 4
ab
Z a
0
Z b
0
q(x, y, t) sin
mπx
a

sin
nπy
b

dy dx
f(x, y) =
∞
X
m,n=1
fmn sin
mπx
a

sin
nπy
b

fmn = 4
ab
Z a
0
Z b
0
f(x, y) sin
mπx
a

sin
nπy
b

dy dx
1719

We substitute the expansion of the solution into the diﬀusion equation and the initial condition to determine initial
value problems for the coeﬃcients in the expansion.
ut −κ(uxx + uyy) = q(x, y, t)
∞
X
m,n=1

u′
mn(t) + κ
mπ
a
2
+
nπ
b
2
umn(t)

sin
mπx
a

sin
nπy
b

=
∞
X
m,n=1
qmn(t) sin
mπx
a

sin
nπy
b

u′
mn(t) + κ
mπ
a
2
+
nπ
b
2
umn(t) = qmn(t)
u(x, y, 0) = f(x, y)
∞
X
m,n=1
umn(0) sin
mπx
a

sin
nπy
b

=
∞
X
m,n=1
fmn sin
mπx
a

sin
nπy
b

umn(0) = fmn
We solve the ordinary diﬀerential equations for the coeﬃcients umn(t) subject to their initial conditions.
umn(t) =
Z t
0
exp

−κ
mπ
a
2
+
nπ
b
2
(t −τ)

qmn(τ) dτ + fmn exp

−κ
mπ
a
2
+
nπ
b
2
t

Solution 37.2
After looking at this problem for a minute or two, it seems like the answer would have the form
u = sin(x)T(t).
This form satisﬁes the boundary conditions. We substitute it into the heat equation and the initial condition to determine
1720

T
sin(x)T ′ = −κ sin(x)T + A sin(x),
T(0) = 0
T ′ + κT = A,
T(0) = 0
T = A
κ + c e−κt
T = A
κ
 1 −e−κt
Now we have the solution of the heat equation.
u = A
κ sin(x)
 1 −e−κt
Solution 37.3
First we write the Laplacian in polar coordinates.
urr + 1
rur + 1
r2uθθ = 0
1. We introduce the separation of variables u(r, θ) = R(r)Θ(θ).
R′′Θ + 1
rR′Θ + 1
r2RΘ′′ = 0
r2R′′
R + rR′
R = −Θ′′
Θ = λ
We have a regular Sturm-Liouville problem for Θ and a diﬀerential equation for R.
Θ′′ + λΘ = 0,
Θ′(0) = Θ(π/2) = 0
(37.8)
r2R′′ + rR′ −λR = 0,
R is bounded
1721

First we solve the problem for Θ to determine the eigenvalues and eigenfunctions. The Rayleigh quotient is
λ =
R π/2
0
(Θ′)2 dθ
R π/2
0
Θ2 dθ
Immediately we see that the eigenvalues are non-negative. If Θ′ = 0, then the right boundary condition implies
that Θ = 0. Thus λ = 0 is not an eigenvalue. We ﬁnd the general solution of Equation 37.8 for positive λ.
Θ = c1 cos
√
λθ

+ c2 sin
√
λθ

The solution that satisﬁes the left boundary condition is
Θ = c cos
√
λθ

.
We apply the right boundary condition to determine the eigenvalues.
cos
√
λπ
2

= 0
λn = (2n −1)2,
Θn = cos ((2n −1)θ) ,
n ∈Z+
Now we solve the diﬀerential equation for R. Since this is an Euler equation, we make the substitition R = rα.
r2R′′
n + rR′
n −(2n −1)2Rn = 0
α(α −1) + α −(2n −1)2 = 0
α = ±(2n −1)
Rn = c1r2n−1 + c2r1−2n
The solution which is bounded in 0 ≤r ≤1 is
Rn = r2n−1.
1722

The solution of Laplace’s equation is a linear combination of the eigensolutions.
u =
∞
X
n=1
unr2n−1 cos ((2n −1)θ)
We use the boundary condition at r = 1 to determine the coeﬃcients.
u(1, θ) = f(θ) =
∞
X
n=1
un cos ((2n −1)θ)
un = 4
π
Z π/2
0
f(θ) cos ((2n −1)θ) dθ
2. We introduce the separation of variables u(r, θ) = R(r)Θ(θ).
R′′Θ + 1
rR′Θ + 1
r2RΘ′′ = 0
r2R′′
R + rR′
R = −Θ′′
Θ = λ
We have a regular Sturm-Liouville problem for Θ and a diﬀerential equation for R.
Θ′′ + λΘ = 0,
Θ′(0) = Θ′(π/2) = 0
(37.9)
r2R′′ + rR′ −λR = 0,
R is bounded
First we solve the problem for Θ to determine the eigenvalues and eigenfunctions. We recognize this problem as
the generator of the Fourier cosine series.
λn = (2n)2,
n ∈Z0+,
Θ0 = 1
2,
Θn = cos (2nθ) ,
n ∈Z+
1723

Now we solve the diﬀerential equation for R. Since this is an Euler equation, we make the substitition R = rα.
r2R′′
n + rR′
n −(2n)2Rn = 0
α(α −1) + α −(2n)2 = 0
α = ±2n
R0 = c1 + c2 ln(r),
Rn = c1r2n + c2r−2n,
n ∈Z+
The solutions which are bounded in 0 ≤r ≤1 are
Rn = r2n.
The solution of Laplace’s equation is a linear combination of the eigensolutions.
u = u0
2 +
∞
X
n=1
unr2n cos (2nθ)
We use the boundary condition at r = 1 to determine the coeﬃcients.
ur(1, θ) =
∞
X
n=1
2nun cos(2nθ) = g(θ)
Note that the constant term is missing in this cosine series. g(θ) has such a series expansion only if
Z π/2
0
g(θ) dθ = 0.
This is the condition for the existence of a solution of the problem. If this is satisﬁed, we can solve for the
coeﬃcients in the expansion. u0 is arbitrary.
un = 4
π
Z π/2
0
g(θ) cos (2nθ) dθ,
n ∈Z+
1724

Solution 37.4
1.
ut = ν(uxx + uyy)
XY T ′ = ν(X′′Y T + XY ′′T)
T ′
νT = X′′
X + Y ′′
Y
= −λ
X′′
X = −Y ′′
Y −λ = −µ
We have boundary value problems for X(x) and Y (y) and a diﬀerential equation for T(t).
X′′ + µX = 0,
X′(0) = X′(1) = 0
Y ′′ + (λ −µ)Y = 0,
Y (0) = Y (1) = 0
T ′ = −λνT
2. The solutions for X(x) form a cosine series.
µm = m2π2,
m ∈Z0+,
X0 = 1
2,
Xm = cos(mπx)
The solutions for Y (y) form a sine series.
λmn = (m2 + n2)π2,
n ∈Z+,
Yn = sin(nπx)
We solve the ordinary diﬀerential equation for T(t).
Tmn = e−ν(m2+n2)π2t
We expand the solution of the heat equation in a series of the eigensolutions.
u(x, y, t) = 1
2
∞
X
n=1
u0n sin(nπy) e−νn2π2t +
∞
X
m=1
∞
X
n=1
umn cos(mπx) sin(nπy) e−ν(m2+n2)π2t
1725

We use the initial condition to determine the coeﬃcients.
u(x, y, 0) = f(x, y) = 1
2
∞
X
n=1
u0n sin(nπy) +
∞
X
m=1
∞
X
n=1
umn cos(mπx) sin(nπy)
umn = 4
Z 1
0
Z 1
0
f(x, y) cos(mπx) sin(nπy) dx dy
Solution 37.5
We use the separation of variables u(x, t) = X(x)T(t) to ﬁnd eigensolutions of the heat equation that satisfy the
boundary conditions at x = 0, π.
ut = νuxx
XT ′ = νX′′T
T ′
νT = X′′
X = −λ
The problem for X(x) is
X′′ + λX = 0,
X′(0) = X′(π) = 0.
The eigenfunctions form the familiar cosine series.
λn = n2,
n ∈Z0+,
X0 = 1
2,
Xn = cos(nx)
Next we solve the diﬀerential equation for T(t).
T ′
n = −νn2Tn
T0 = 1,
Tn = e−νn2t
We expand the solution of the heat equation in a series of the eigensolutions.
u(x, t) = 1
2u0 +
∞
X
n=1
un cos(nx) e−νn2t
1726

We use the initial condition to determine the coeﬃcients in the series.
u(x, 0) = x = 1
2u0 +
∞
X
n=1
un cos(nx)
u0 = 2
π
Z π
0
x dx = π
un = 2
π
Z π
0
x cos(nx) dx =
(
0
even n
−4
πn2
odd n
u(x, t) = π
2 −
∞
X
n=1
odd n
4
πn2 cos(nx) e−νn2t
Solution 37.6
We expand the solution in a Fourier series.
φ = 1
2a0(r) +
∞
X
n=1
an(r) cos(nθ) +
∞
X
n=1
bn(r) sin(nθ)
We substitute the series into the Laplace’s equation to determine ordinary diﬀerential equations for the coeﬃcients.
∂
∂r

r∂φ
∂r

+ 1
r2
∂2φ
∂θ2 = 0
a′′
0 + 1
ra′
0 = 0,
a′′
n + 1
ra′
n −n2an = 0,
b′′
n + 1
rb′
n −n2bn = 0
The solutions that are bounded at r = 0 are, (to within multiplicative constants),
a0(r) = 1,
an(r) = rn,
bn(r) = rn.
Thus φ(r, θ) has the form
φ(r, θ) = 1
2c0 +
∞
X
n=1
cnrn cos(nθ) +
∞
X
n=1
dnrn sin(nθ)
1727

We apply the boundary condition at r = R.
φ(R, θ) = 1
2c0 +
∞
X
n=1
cnRn cos(nθ) +
∞
X
n=1
dnRn sin(nθ)
The coeﬃcients are
c0 = 1
π
Z 2π
0
φ(R, α) dα,
cn =
1
πRn
Z 2π
0
φ(R, α) cos(nα) dα,
dn =
1
πRn
Z 2π
0
φ(R, α) sin(nα) dα.
We substitute the coeﬃcients into our series solution.
φ(r, θ) = 1
2π
Z 2π
0
φ(R, α) dα + 1
π
∞
X
n=1
 r
R
n Z 2π
0
φ(R, α) cos(n(θ −α)) dα
φ(r, θ) = 1
2π
Z 2π
0
φ(R, α) dα + 1
π
Z 2π
0
φ(R, α)ℜ
 ∞
X
n=1
 r
R
n
eın(θ−α)
!
dα
φ(r, θ) = 1
2π
Z 2π
0
φ(R, α) dα + 1
π
Z 2π
0
φ(R, α)ℜ
 
r
R eı(θ−α)
1 −r
R eı(θ−α)
!
dα
φ(r, θ) = 1
2π
Z 2π
0
φ(R, α) dα + 1
π
Z 2π
0
φ(R, α)ℜ
 
r
R eı(θ−α) −
  r
R
2
1 −2 r
R cos(θ −α) +
  r
R
2
!
dα
φ(r, θ) = 1
2π
Z 2π
0
φ(R, α) dα + 1
π
Z 2π
0
φ(R, α)
Rr cos(θ −α) −r2
R2 + r2 −2Rr cos(θ −α) dα
φ(r, θ) = 1
2π
Z 2π
0
φ(R, α)
R2 −r2
R2 + r2 −2Rr cos(θ −α) dα
Solution 37.7
In order that the solution is continuously diﬀerentiable, (which it must be in order to satisfy the diﬀerential equation),
we impose the boundary conditions
u(0, t) = u(2π, t),
uθ(0, t) = uθ(2π, t).
1728

We apply the separation of variables u(θ, t) = Θ(θ)T(t).
ut = κuθθ
ΘT ′ = κΘ′′T
T ′
κT = Θ′′
Θ = −λ
We have the self-adjoint eigenvalue problem
Θ′′ + λΘ = 0,
Θ(0) = Θ(2π),
Θ′(0) = Θ′(2π)
which has the eigenvalues and orthonormal eigenfunctions
λn = n2,
Θn =
1
√
2π
eınθ,
n ∈Z.
Now we solve the problems for Tn(t) to obtain eigen-solutions of the heat equation.
T ′
n = −n2κTn
Tn = e−n2κt
The solution is a linear combination of the eigen-solutions.
u(θ, t) =
∞
X
n=−∞
un
1
√
2π
eınθ e−n2κt
We use the initial conditions to determine the coeﬃcients.
u(θ, 0) =
∞
X
n=−∞
un
1
√
2π
eınθ = f(θ)
un =
1
√
2π
Z 2π
0
e−ınθ f(θ) dθ
1729

Solution 37.8
Substituting u(x, y) = X(x)Y (y) into the partial diﬀerential equation yields
X′′
X = −Y ′′
Y
= −λ.
With the homogeneous boundary conditions, we have the two problems
X′′ + λX = 0,
X(0) = X(1) = 0,
Y ′′ −λY = 0,
Y (1) = 0.
The eigenvalues and orthonormal eigenfunctions for X(x) are
λn = (nπ)2,
Xn =
√
2 sin(nπx).
The general solution for Y is
Yn = a cosh(nπy) + b sinh(nπy).
The solution for that satisﬁes the right homogeneous boundary condition, (up to a multiplicative constant), is
Yn = sinh(nπ(1 −y))
u(x, y) is a linear combination of the eigen-solutions.
u(x, y) =
∞
X
n=1
un
√
2 sin(nπx) sinh(nπ(1 −y))
We use the inhomogeneous boundary condition to determine coeﬃcients.
u(x, 0) =
∞
X
n=1
un
√
2 sin(nπx) sinh(nπ) = f(x)
un =
√
2
Z 1
0
sin(nπξ)f(ξ) dξ
1730

Solution 37.9
We substitute u(r, θ) = R(r)Θ(θ) into the partial diﬀerential equation.
∂2u
∂r2 + 1
r
∂u
∂r + 1
r2
∂2u
∂θ2 = 0
R′′Θ + 1
rR′Θ + 1
r2RΘ′′ = 0
r2R′′
R + rR′
R = −Θ′′
Θ = λ
r2R′′ + rR′ −λR = 0,
Θ′′ + λΘ = 0
We assume that u is a strong solution of the partial diﬀerential equation and is thus twice continuously diﬀerentiable,
(u ∈C2). In particular, this implies that R and Θ are bounded and that Θ is continuous and has a continuous ﬁrst
derivative along θ = 0. This gives us a boundary value problem for Θ and a diﬀerential equation for R.
Θ′′ + λΘ = 0,
Θ(0) = Θ(2π),
Θ′(0) = Θ′(2π)
r2R′′ + rR′ −λR = 0,
R is bounded
The eigensolutions for Θ form the familiar Fourier series.
λn = n2,
n ∈Z0+
Θ(1)
0
= 1
2,
Θ(1)
n = cos(nθ),
n ∈Z+
Θ(2)
n = sin(nθ),
n ∈Z+
Now we ﬁnd the bounded solutions for R. The equation for R is an Euler equation so we use the substitution
R = rα.
r2R′′
n + rR′
n −λnRn = 0
α(α −1) + α −λn = 0
α = ±
p
λn
1731

First we consider the case λ0 = 0. The solution is
R = a + b ln r.
Boundedness demands that b = 0. Thus we have the solution
R = 1.
Now we consider the case λn = n2 > 0. The solution is
Rn = arn + br−n.
Boundedness demands that b = 0. Thus we have the solution
Rn = rn.
The solution for u is a linear combination of the eigensolutions.
u(r, θ) = a0
2 +
∞
X
n=1
(an cos(nθ) + bn sin(nθ)) rn
The boundary condition at r = 1 determines the coeﬃcients in the expansion.
u(1, θ) = a0
2 +
∞
X
n=1
[an cos(nθ) + bn sin(nθ)] = f(θ)
an = 1
π
Z 2π
0
f(θ) cos(nθ) dθ,
bn = 1
π
Z 2π
0
f(θ) sin(nθ) dθ
Solution 37.10
A normal mode of frequency ω is periodic in time.
v(r, θ, t) = u(r, θ) eıωt
1732

We substitute this form into the wave equation to obtain a Helmholtz equation, (also called a reduced wave equation).
1
r
∂
∂r

r∂u
∂r

+ 1
r2
∂2u
∂θ2 = −ω2
c2 u,
u(1, θ) = 0,
∂2u
∂r2 + 1
r
∂u
∂r + 1
r2
∂2u
∂θ2 + k2u = 0,
u(1, θ) = 0
Here we have deﬁned k = ω
c . We apply the separation of variables u = R(r)Θ(θ) to the Helmholtz equation.
r2R′′Θ + rR′Θ + RΘ′′ + k2r2RΘ = 0,
r2R′′
R + rR′
R + k2r2 = −Θ′′
Θ = λ2
Now we have an ordinary diﬀerential equation for R(r) and an eigenvalue problem for Θ(θ).
R′′ + 1
rR′ +

k2 −λ2
r2

R = 0,
R(0) is bounded,
R(1) = 0,
Θ′′ + λ2Θ = 0,
Θ(−π) = Θ(π),
Θ′(−π) = Θ′(π).
We compute the eigenvalues and eigenfunctions for Θ.
λn = n,
n ∈Z0+
Θ0 = 1
2,
Θ(1)
n = cos(nθ),
Θ(2)
n = sin(nθ),
n ∈Z+
The diﬀerential equations for the Rn are Bessel equations.
R′′
n + 1
rR′
n +

k2 −n2
r2

Rn = 0,
Rn(0) is bounded,
Rn(1) = 0
The general solution is a linear combination of order n Bessel functions of the ﬁrst and second kind.
Rn(r) = c1Jn(kr) + c2Yn(kr)
1733

Since the Bessel function of the second kind, Yn(kr), is unbounded at r = 0, the solution has the form
Rn(r) = cJn(kr).
Applying the second boundary condition gives us the admissable frequencies.
Jn(k) = 0
knm = jnm,
Rnm = Jn(jnmr),
n ∈Z0+,
m ∈Z+
Here jnm is the mth positive root of Jn. We combining the above results to obtain the normal modes of oscillation.
v0m = 1
2J0(j0mr) eıcj0mt,
m ∈Z+
vnm = cos(nθ + α)Jnm(jnmr) eıcjnmt,
n, m ∈Z+
Some normal modes are plotted in Figure 37.2. Note that cos(nθ + α) represents a linear combination of cos(nθ) and
sin(nθ). This form is preferrable as it illustrates the circular symmetry of the problem.
Solution 37.11
We will expand the solution in a complete, orthogonal set of functions {Xn(x)}, where the coeﬃcients are functions
of t.
φ =
X
n
Tn(t)Xn(x)
We will use separation of variables to determine a convenient set {Xn}. We substitite φ = T(t)X(x) into the diﬀusion
equation.
φt = a2φxx
XT ′ = a2X′′T
T ′
a2T = X′′
X = −λ
T ′ = −a2λT,
X′′ + λX = 0
1734

Note that in order to satisfy φ(0, t) = φ(l, t) = 0, the Xn must satisfy the same homogeneous boundary conditions,
Xn(0) = Xn(l) = 0. This gives us a Sturm-Liouville problem for X(x).
X′′ + λX = 0,
X(0) = X(l) = 0
λn =
nπ
l
2
,
Xn = sin
nπx
l

,
n ∈Z+
Thus we seek a solution of the form
φ =
∞
X
n=1
Tn(t) sin
nπx
l

.
(37.10)
This solution automatically satisﬁes the boundary conditions. We will assume that we can diﬀerentiate it. We will
substitite this form into the diﬀusion equation and the initial condition to determine the coeﬃcients in the series, Tn(t).
First we substitute Equation 37.10 into the partial diﬀerential equation for φ to determine ordinary diﬀerential equations
for the Tn.
φt = a2φxx
∞
X
n=1
T ′
n(t) sin
nπx
l

= −a2
∞
X
n=1
nπ
l
2
Tn(t) sin
nπx
l

T ′
n = −
anπ
l
2
Tn
1735

Now we substitute Equation 37.10 into the initial condition for φ to determine initial conditions for the Tn.
∞
X
n=1
Tn(0) sin
nπx
l

= φ(x, 0)
Tn(0) =
R l
0 sin
  nπx
l

φ(x, 0) dx
R l
0 sin2   nπx
l

dx
Tn(0) = 2
l
Z l
0
sin
nπx
l

φ(x, 0) dx
Tn(0) = 2
l
Z l/2
0
sin
nπx
l

x dx + 2
l
Z l/2
0
sin
nπx
l

(l −x) dx
Tn(0) =
4l
n2π2 sin
nπ
2

T2n−1(0) = (−1)n
4l
(2n −1)2π2,
T2n(0) = 0,
n ∈Z+
We solve the ordinary diﬀerential equations for Tn subject to the initial conditions.
T2n−1(t) = (−1)n
4l
(2n −1)2π2 exp
 
−
a(2n −1)π
l
2
t
!
,
T2n(t) = 0,
n ∈Z+
This determines the series representation of the solution.
φ = 4
l
∞
X
n=1
(−1)n

l
(2n −1)π
2
exp
 
−
a(2n −1)π
l
2
t
!
sin
(2n −1)πx
l

From the initial condition, we know that the the solution at t = 0 is C0.
That is, it is continuous, but not
diﬀerentiable. The series representation of the solution at t = 0 is
φ = 4
l
∞
X
n=1
(−1)n

l
(2n −1)π
2
sin
(2n −1)πx
l

.
1736

That the coeﬃcients decay as 1/n2 corroborates that φ(x, 0) is C0.
The derivatives of φ with respect to x are
∂2m−1
∂x2m−1φ = 4(−1)m+1
l
∞
X
n=1
(−1)n
(2n −1)π
l
2m−3
exp
 
−
a(2n −1)π
l
2
t
!
cos
(2n −1)πx
l

∂2m
∂x2mφ = 4(−1)m
l
∞
X
n=1
(−1)n
(2n −1)π
l
2m−2
exp
 
−
a(2n −1)π
l
2
t
!
sin
(2n −1)πx
l

For any ﬁxed t > 0, the coeﬃcients in the series for ∂n
∂xφ decay exponentially. These series are uniformly convergent in
x. Thus for any ﬁxed t > 0, φ is C∞in x.
Solution 37.12
ut = κuxx,
0 < x < L,
t > 0
u(0, t) = T0,
u(L, t) = T1,
u(x, 0) = f(x),
Method 1. We solve this problem with an eigenfunction expansion in x. To ﬁnd an appropriate set of eigenfunctions,
we apply the separation of variables, u(x, t) = X(x)T(t) to the partial diﬀerential equation with the homogeneous
boundary conditions, u(0, t) = u(L, t) = 0.
(XT)t = (XT)xx
XT ′ = X′′T
T ′
T = X′′
X = −λ2
We have the eigenvalue problem,
X′′ + λ2X = 0,
X(0) = X(L) = 0,
which has the solutions,
λn = nπx
L ,
Xn = sin
nπx
L

,
n ∈N.
1737

We expand the solution of the partial diﬀerential equation in terms of these eigenfunctions.
u(x, t) =
∞
X
n=1
an(t) sin
nπx
L

Because of the inhomogeneous boundary conditions, the convergence of the series will not be uniform.
We can
diﬀerentiate the series with respect to t, but not with respect to x. We multiply the partial diﬀerential equation by
an eigenfunction and integrate from x = 0 to x = L. We use integration by parts to move derivatives from u to the
eigenfunction.
ut −κuxx = 0
Z L
0
(ut −κuxx) sin
mπx
L

dx = 0
Z L
0
 ∞
X
n=1
a′
n(t) sin
nπx
L
!
sin
mπx
L

dx −κ
h
ux sin
mπx
L
iL
0 + κmπ
L
Z L
0
ux cos
mπx
L

dx = 0
L
2 a′
m(t) + κmπ
L
h
u cos
mπx
L
iL
0 + κ
mπ
L
2 Z L
0
u sin
mπx
L

dx = 0
L
2 a′
m(t) + κmπ
L ((−1)mu(L, t) −u(0, t)) + κ
mπ
L
2 Z L
0
 ∞
X
n=1
an(t) sin
nπx
L
!
sin
mπx
L

dx = 0
L
2 a′
m(t) + κmπ
L ((−1)mT1 −T0) + κL
2
mπ
L
2
am(t) = 0
a′
m(t) + κ
mπ
L
2
am(t) = κ2mπ
L2 (T0 −(−1)mT1)
1738

Now we have a ﬁrst order diﬀerential equation for each of the an’s. We obtain initial conditions for each of the an’s
from the initial condition for u(x, t).
u(x, 0) = f(x)
∞
X
n=1
an(0) sin
nπx
L

= f(x)
an(0) = 2
L
Z L
0
f(x) sin
nπx
L

dx ≡fn
By solving the ﬁrst order diﬀerential equation for an(t), we obtain
an(t) = 2(T0 −(−1)nT1)
nπ
+ e−κ(nπ/L)2t

fn −2(T0 −(−1)nT1)
nπ

.
Note that the series does not converge uniformly due to the 1/n term.
Method 2. For our second method we transform the problem to one with homogeneous boundary conditions so
that we can use the partial diﬀerential equation to determine the time dependence of the eigen-solutions. We make the
change of variables v(x, t) = u(x, t) −µ(x) where µ(x) is some function that satisﬁes the inhomogeneous boundary
conditions. If possible, we want µ(x) to satisfy the partial diﬀerential equation as well. For this problem we can choose
µ(x) to be the equilibrium solution which satisﬁes
µ′′(x) = 0,
µ(0)T0,
µ(L) = T1.
This has the solution
µ(x) = T0 + T1 −T0
L
x.
With the change of variables,
v(x, t) = u(x, t) −

T0 + T1 −T0
L
x

,
1739

we obtain the problem
vt = κvxx,
0 < x < L,
t > 0
v(0, t) = 0,
v(L, t) = 0,
v(x, 0) = f(x) −

T0 + T1 −T0
L
x

.
Now we substitute the separation of variables v(x, t) = X(x)T(t) into the partial diﬀerential equation.
(XT)t = κ(XT)xx
T ′
κT = X′′
X = −λ2
Utilizing the boundary conditions at x = 0, L we obtain the two ordinary diﬀerential equations,
T ′ = −κλ2T,
X′′ = −λ2X,
X(0) = X(L) = 0.
The problem for X is a regular Sturm-Liouville problem and has the solutions
λn = nπ
L ,
Xn = sin
nπx
L

,
n ∈N.
The ordinary diﬀerential equation for T becomes,
T ′
n = −κ
nπ
L
2
Tn,
which, (up to a multiplicative constant), has the solution,
Tn = e−κ(nπ/L)2t .
Thus the eigenvalues and eigen-solutions of the partial diﬀerential equation are,
λn = nπ
L ,
vn = sin
nπx
L

e−κ(nπ/L)2t,
n ∈N.
1740

Let v(x, t) have the series expansion,
v(x, t) =
∞
X
n=1
an sin
nπx
L

e−κ(nπ/L)2t .
We determine the coeﬃcients in the expansion from the initial condition,
v(x, 0) =
∞
X
n=1
an sin
nπx
L

= f(x) −

T0 + T1 −T0
L
x

.
The coeﬃcients in the expansion are the Fourier sine coeﬃcients of f(x) −
 T0 + T1−T0
L
x

.
an = 2
L
Z L
0

f(x) −

T0 + T1 −T0
L
x

sin
nπx
L

dx
an = fn −2(T0 −(−1)nT1)
nπ
With the coeﬃcients deﬁned above, the solution for u(x, t) is
u(x, t) = T0 + T1 −T0
L
x +
∞
X
n=1

fn −2(T0 −(−1)nT1)
nπ

sin
nπx
L

e−κ(nπ/L)2t .
Since the coeﬃcients in the sum decay exponentially for t > 0, we see that the series is uniformly convergent for positive
t. It is clear that the two solutions we have obtained are equivalent.
Solution 37.13
First we solve the eigenvalue problem for β(x), which is the problem we would obtain if we applied separation of
variables to the partial diﬀerential equation, φt = φxx. We have the eigenvalues and orthonormal eigenfunctions
λn =
(2n −1)π
2l
2
,
βn(x) =
r
2
l sin
(2n −1)πx
2l

,
n ∈Z+.
1741

We expand the solution and inhomogeneity in Equation 37.5 in a series of the eigenvalues.
φ(x, t) =
∞
X
n=1
Tn(t)βn(x)
w(x, t) =
∞
X
n=1
wn(t)βn(x),
wn(t) =
Z l
0
βn(x)w(x, t) dx
Since φ satisﬁes the same homgeneous boundary conditions as β, we substitute the series into Equation 37.5 to
determine diﬀerential equations for the Tn(t).
∞
X
n=1
T ′
n(t)βn(x) = a2
∞
X
n=1
Tn(t)(−λn)βn(x) +
∞
X
n=1
wn(t)βn(x)
T ′
n(t) = −a2
(2n −1)π
2l
2
Tn(t) + wn(t)
Now we substitute the series for φ into its initial condition to determine initial conditions for the Tn.
φ(x, 0) =
∞
X
n=1
Tn(0)βn(x) = f(x)
Tn(0) =
Z l
0
βn(x)f(x) dx
We solve for Tn(t) to determine the solution, φ(x, t).
Tn(t) = exp
 
−
(2n −1)aπ
2l
2
t
!  
Tn(0) +
Z t
0
wn(τ) exp
 (2n −1)aπ
2l
2
τ
!
dτ
!
Solution 37.14
Separation of variables leads to the eigenvalue problem
β′′ + λβ = 0,
β(0) = 0,
β(l) + cβ′(l) = 0.
1742

First we consider the case λ = 0. A set of solutions of the diﬀerential equation is {1, x}. The solution that satisﬁes
the left boundary condition is β(x) = x. The right boundary condition imposes the constraint l + c = 0. Since c is
positive, this has no solutions. λ = 0 is not an eigenvalue.
Now we consider λ ̸= 0. A set of solutions of the diﬀerential equation is {cos(
√
λx), sin(
√
λx)}. The solution that
satisﬁes the left boundary condition is β = sin(
√
λx). The right boundary condition imposes the constraint
c sin
√
λl

+
√
λ cos
√
λl

= 0
tan
√
λl

= −
√
λ
c
For large λ, the we can determine approximate solutions.
p
λnl ≈(2n −1)π
2
, n ∈Z+
λn ≈
(2n −1)π
2l
2
, n ∈Z+
The eigenfunctions are
βn(x) =
sin
 √λnx

qR l
0 sin2  √λnx

dx
, n ∈Z+.
We expand φ(x, t) and w(x, t) in series of the eigenfunctions.
φ(x, t) =
∞
X
n=1
Tn(t)βn(x)
w(x, t) =
∞
X
n=1
wn(t)βn(x),
wn(t) =
Z l
0
βn(x)w(x, t) dx
1743

Since φ satisﬁes the same homgeneous boundary conditions as β, we substitute the series into Equation 37.5 to
determine diﬀerential equations for the Tn(t).
∞
X
n=1
T ′
n(t)βn(x) = a2
∞
X
n=1
Tn(t)(−λn)βn(x) +
∞
X
n=1
wn(t)βn(x)
T ′
n(t) = −a2λnTn(t) + wn(t)
Now we substitute the series for φ into its initial condition to determine initial conditions for the Tn.
φ(x, 0) =
∞
X
n=1
Tn(0)βn(x) = f(x)
Tn(0) =
Z l
0
βn(x)f(x) dx
We solve for Tn(t) to determine the solution, φ(x, t).
Tn(t) = exp
 −a2λnt
 
Tn(0) +
Z t
0
wn(τ) exp
 a2λnτ

dτ

Solution 37.15
First we seek a function u(x, t) that satisﬁes the boundary conditions u(0, t) = t, ux(l, t) = −cu(l, t). We try a
function of the form u = (ax + b)t. The left boundary condition imposes the constraint b = 1. We then apply the
right boundary condition no determine u.
at = −c(al + 1)t
a = −
c
1 + cl
u(x, t) =

1 −
cx
1 + cl

t
1744

Now we deﬁne ψ to be the diﬀerence of φ and u.
ψ(x, t) = φ(x, t) −u(x, t)
ψ satisﬁes an inhomogeneous diﬀusion equation with homogeneous boundary conditions.
(ψ + u)t = a2(ψ + u)xx + 1
ψt = a2ψxx + 1 + a2uxx −ut
ψt = a2ψxx +
cx
1 + cl
The initial and boundary conditions for ψ are
ψ(x, 0) = 0,
ψ(0, t) = 0,
ψx(l, t) = −cψ(l, t).
We solved this system in problem 2. Just take
w(x, t) =
cx
1 + cl,
f(x) = 0.
The solution is
ψ(x, t) =
∞
X
n=1
Tn(t)βn(x),
Tn(t) =
Z t
0
wn exp
 −a2λn(t −τ)

dτ,
wn(t) =
Z l
0
βn(x)
cx
1 + cl dx.
This determines the solution for φ.
1745

Solution 37.16
First we solve this problem with a series expansion. We transform the problem to one with homogeneous boundary
conditions. Note that
u(x) =
x
l + 1
satisﬁes the boundary conditions. (It is the equilibrium solution.) We make the change of variables ψ = φ −u. The
problem for ψ is
ψt = a2ψxx,
ψ(0, t) = ψ(l, t) + ψx(l, t) = 0,
ψ(x, 0) =
x
l + 1.
This is a particular case of what we solved in Exercise 37.14. We apply the result of that problem. The solution for
φ(x, t) is
φ(x, t) =
x
l + 1 +
∞
X
n=1
Tn(t)βn(x)
βn(x) =
sin
 √λnx

qR l
0 sin2  √λnx

dx
,
n ∈Z+
tan
√
λl

= −
√
λ
Tn(t) = Tn(0) exp
 −a2λnt

Tn(0) =
Z l
0
βn(x)
x
l + 1 dx
This expansion is useful for large t because the coeﬃcients decay exponentially with increasing t.
1746

Now we solve this problem with the Laplace transform.
φt = a2φxx,
φ(0, t) = 0,
φ(l, t) + φx(l, t) = 1,
φ(x, 0) = 0
sˆφ = a2 ˆφxx,
ˆφ(0, s) = 0,
ˆφ(l, s) + ˆφx(l, s) = 1
s
ˆφxx −s
a2 ˆφ = 0,
ˆφ(0, s) = 0,
ˆφ(l, s) + ˆφx(l, s) = 1
s
The solution that satisﬁes the left boundary condition is
ˆφ = c sinh
√sx
a

.
We apply the right boundary condition to determine the constant.
ˆφ =
sinh
 √sx
a

s

sinh
 √sl
a

+
√s
a cosh
 √sl
a

1747

We expand this in a series of simpler functions of s.
ˆφ =
2 sinh
 √sx
a

s

exp
 √sl
a

−exp

−
√sl
a

+
√s
a

exp
 √sl
a

+ exp

−
√sl
a

ˆφ =
2 sinh
 √sx
a

s exp
 √sl
a

1
1 +
√s
a −

1 −
√s
a

exp

−2√sl
a

ˆφ =
exp
 √sx
a

−exp

−
√sx
a

s

1 +
√s
a

exp
 √sl
a

1
1 −

1−√s/a
1+√s/a

exp

−2√sl
a

ˆφ =
exp
 √s(x−l)
a

−exp
 √s(−x−l)
a

s

1 +
√s
a

∞
X
n=0
1 −√s/a
1 + √s/a
n
exp

−2√sln
a

ˆφ = 1
s
 
∞
X
n=0
(1 −√s/a)n
(1 + √s/a)n+1 exp

−
√s((2n + 1)l −x)
a

−
∞
X
n=0
(1 −√s/a)n
(1 + √s/a)n+1 exp

−
√s((2n + 1)l + x)
a
 !
By expanding
(1 −√s/a)n
(1 + √s/a)n+1
in binomial series all the terms would be of the form
s−m/2−3/2 exp

−
√s((2n ± 1)l ∓x)
a

.
1748

Taking the ﬁrst term in each series yields
ˆφ ∼
a
s3/2

exp

−
√s(l −x)
a

−exp

−
√s(l + x)
a

,
as s →∞.
We take the inverse Laplace transform to obtain an appoximation of the solution for t ≪1.
φ(x, t) ∼2a2√
πt


exp

−(l−x)2
4a2t

l −x
−
exp

−(l+x)2
4a2t

l + x


−π

erfc
l −x
2a
√
t

−erfc
 l + x
2a
√
t

,
for t ≪1
Solution 37.17
We apply the separation of variables φ(x, t) = X(x)T(t).
φt = A2  x2φx

x
XT ′ = TA2  x2X′′
T ′
A2T = (x2X′)′
X
= −λ
This gives us a regular Sturm-Liouville problem.
 x2X′′ + λX = 0,
X(1) = X(2) = 0
This is an Euler equation. We make the substitution X = xα to ﬁnd the solutions.
x2X′′ + 2xX′ + λX = 0
(37.11)
α(α −1) + 2α + λ = 0
α = −1 ±
√
1 −4λ
2
α = −1
2 ± ı
p
λ −1/4
1749

First we consider the case of a double root when λ = 1/4. The solutions of Equation 37.11 are {x−1/2, x−1/2 ln x}.
The solution that satisﬁes the left boundary condition is X = x−1/2 ln x. Since this does not satisfy the right boundary
condition, λ = 1/4 is not an eigenvalue.
Now we consider λ ̸= 1/4. The solutions of Equation 37.11 are
 1
√x cos
p
λ −1/4 ln x

, 1
√x sin
p
λ −1/4 ln x

.
The solution that satisﬁes the left boundary condition is
1
√x sin
p
λ −1/4 ln x

.
The right boundary condition imposes the constraint
p
λ −1/4 ln 2 = nπ,
n ∈Z+.
This gives us the eigenvalues and eigenfunctions.
λn = 1
4 +
 nπ
ln 2
2
,
Xn(x) =
1
√x sin
nπ ln x
ln 2

,
n ∈Z+.
We normalize the eigenfunctions.
Z 2
1
1
x sin2
nπ ln x
ln 2

dx = ln 2
Z 1
0
sin2(nπξ) dξ = ln 2
2
Xn(x) =
r
2
ln 2
1
√x sin
nπ ln x
ln 2

,
n ∈Z+.
From separation of variables, we have diﬀerential equations for the Tn.
T ′
n = −A2
1
4 +
 nπ
ln 2
2
Tn
Tn(t) = exp

−A2
1
4 +
 nπ
ln 2
2
t

1750

We expand φ in a series of the eigensolutions.
φ(x, t) =
∞
X
n=1
φnXn(x)Tn(t)
We substitute the expansion for φ into the initial condition to determine the coeﬃcients.
φ(x, 0) =
∞
X
n=1
φnXn(x) = f(x)
φn =
Z 2
1
Xn(x)f(x) dx
Solution 37.18
utt = c2uxx,
0 < x < L,
t > 0,
u(0, t) = 0,
ux(L, t) = 0,
u(x, 0) = f(x),
ut(x, 0) = 0,
We substitute the separation of variables u(x, t) = X(x)T(t) into the partial diﬀerential equation.
(XT)tt = c2(XT)xx
T ′′
c2T = X′′
X = −λ2
With the boundary conditions at x = 0, L, we have the ordinary diﬀerential equations,
T ′′ = −c2λ2T,
X′′ = −λ2X,
X(0) = X′(L) = 0.
1751

The problem for X is a regular Sturm-Liouville eigenvalue problem. From the Rayleigh quotient,
λ2 = −[φφ′]L
0 +
R L
0 (φ′)2 dx
R L
0 φ2 dx
=
R L
0 (φ′)2 dx
R L
0 φ2 dx
we see that there are only positive eigenvalues. For λ2 > 0 the general solution of the ordinary diﬀerential equation is
X = a1 cos(λx) + a2 sin(λx).
The solution that satisﬁes the left boundary condition is
X = a sin(λx).
For non-trivial solutions, the right boundary condition imposes the constraint,
cos (λL) = 0,
λ = π
L

n −1
2

,
n ∈N.
The eigenvalues and eigenfunctions are
λn = (2n −1)π
2L
,
Xn = sin
(2n −1)πx
2L

,
n ∈N.
The diﬀerential equation for T becomes
T ′′ = −c2
(2n −1)π
2L
2
T,
which has the two linearly independent solutions,
T (1)
n
= cos
(2n −1)cπt
2L

,
T (2)
n
= sin
(2n −1)cπt
2L

.
1752

The eigenvalues and eigen-solutions of the partial diﬀerential equation are,
λn = (2n −1)π
2L
,
n ∈N,
u(1)
n = sin
(2n −1)πx
2L

cos
(2n −1)cπt
2L

,
u(2)
n = sin
(2n −1)πx
2L

sin
(2n −1)cπt
2L

.
We expand u(x, t) in a series of the eigen-solutions.
u(x, t) =
∞
X
n=1
sin
(2n −1)πx
2L
 
an cos
(2n −1)cπt
2L

+ bn sin
(2n −1)cπt
2L

.
We impose the initial condition ut(x, 0) = 0,
ut(x, 0) =
∞
X
n=1
bn
(2n −1)cπ
2L
sin
(2n −1)πx
2L

= 0,
bn = 0.
The initial condition u(x, 0) = f(x) allows us to determine the remaining coeﬃcients,
u(x, 0) =
∞
X
n=1
an sin
(2n −1)πx
2L

= f(x),
an = 2
L
Z L
0
f(x) sin
(2n −1)πx
2L

dx.
The series solution for u(x, t) is,
u(x, t) =
∞
X
n=1
an sin
(2n −1)πx
2L

cos
(2n −1)cπt
2L

.
1753

Solution 37.19
uxx + uyy = f(x, y),
0 < x < a,
0 < y < b,
u(0, y) = u(a, y) = 0,
uy(x, 0) = uy(x, b) = 0
We will solve this problem with an eigenfunction expansion in x. To determine a suitable set of eigenfunctions, we
substitute the separation of variables u(x, y) = X(x)Y (y) into the homogeneous partial diﬀerential equation.
uxx + uyy = 0
(XY )xx + (XY )yy = 0
X′′
X = −Y ′′
Y
= −λ2
With the boundary conditions at x = 0, a, we have the regular Sturm-Liouville problem,
X′′ = −λ2X,
X(0) = X(a) = 0,
which has the solutions,
λn = nπ
a ,
Xn = sin
nπx
a

,
n ∈Z+.
We expand u(x, y) in a series of the eigenfunctions.
u(x, y) =
∞
X
n=1
un(y) sin
nπx
a

We substitute this series into the partial diﬀerential equation and boundary conditions at y = 0, b.
∞
X
n=1

−
nπ
a
2
un(y) sin
nπx
a

+ u′′
n(y) sin
nπx
a

= f(x)
∞
X
n=1
u′
n(0) sin
nπx
a

=
∞
X
n=1
u′
n(b) sin
nπx
a

= 0
1754

We expand f(x, y) in a Fourier sine series.
f(x, y) =
∞
X
n=1
fn(y) sin
nπx
a

fn(y) = 2
a
Z a
0
f(x, y) sin
nπx
a

dx
We obtain the ordinary diﬀerential equations for the coeﬃcients in the expansion.
u′′
n(y) −
nπ
a
2
un(y) = fn(y),
u′
n(0) = u′
n(b) = 0,
n ∈Z+.
We will solve these ordinary diﬀerential equations with Green functions.
Consider the Green function problem,
g′′
n(y; η) −
nπ
a
2
gn(y; η) = δ(y −η),
g′
n(0; η) = g′
n(b; η) = 0.
The homogeneous solutions
cosh
nπy
a

and
cosh
nπ(y −b)
a

satisfy the left and right boundary conditions, respectively. We compute the Wronskian of these two solutions.
W(y) =

cosh(nπy/a)
cosh(nπ(y −b)/a)
nπ
a sinh(nπy/a)
nπ
a sinh(nπ(y −b)/a)

= nπ
a

cosh
nπy
a

sinh
nπ(y −b)
a

−sinh
nπy
a

cosh
nπ(y −b)
a

= −nπ
a sinh
nπb
a

The Green function is
gn(y; η) = −a cosh(nπy</a) cosh(nπ(y> −b)/a)
nπ sinh(nπb/a)
.
1755

The solutions for the coeﬃcients in the expansion are
un(y) =
Z b
0
gn(y; η)fn(η) dη.
Solution 37.20
utt + a2uxxxx = 0,
0 < x < L, t > 0,
u(x, 0) = f(x),
ut(x, 0) = g(x),
u(0, t) = uxx(0, t) = 0,
u(L, t) = uxx(L, t) = 0,
We will solve this problem by expanding the solution in a series of eigen-solutions that satisfy the partial diﬀerential
equation and the homogeneous boundary conditions. We will use the initial conditions to determine the coeﬃcients in
the expansion. We substitute the separation of variables, u(x, t) = X(x)T(t) into the partial diﬀerential equation.
(XT)tt + a2(XT)xxxx = 0
T ′′
a2T = −X′′′′
X
= −λ4
Here we make the assumption that 0 ≤arg(λ) < π/2, i.e., λ lies in the ﬁrst quadrant of the complex plane. Note that
λ4 covers the entire complex plane. We have the ordinary diﬀerential equation,
T ′′ = −a2λ4T,
and with the boundary conditions at x = 0, L, the eigenvalue problem,
X′′′′ = λ4X,
X(0) = X′′(0) = X(L) = X′′(L) = 0.
For λ = 0, the general solution of the diﬀerential equation is
X = c1 + c2x + c3x2 + c4x3.
1756

Only the trivial solution satisﬁes the boundary conditions. λ = 0 is not an eigenvalue. For λ ̸= 0, a set of linearly
independent solutions is
{eλx, eıλx, e−λx, e−ıλx}.
Another linearly independent set, (which will be more useful for this problem), is
{cos(λx), sin(λx), cosh(λx), sinh(λx)}.
Both sin(λx) and sinh(λx) satisfy the left boundary conditions. Consider the linear combination c1 cos(λx)+c2 cosh(λx).
The left boundary conditions impose the two constraints c1 + c2 = 0, c1 −c2 = 0. Only the trivial linear combination
of cos(λx) and cosh(λx) can satisfy the left boundary condition. Thus the solution has the form,
X = c1 sin(λx) + c2 sinh(λx).
The right boundary conditions impose the constraints,
(
c1 sin(λL) + c2 sinh(λL) = 0,
−c1λ2 sin(λL) + c2λ2 sinh(λL) = 0
(
c1 sin(λL) + c2 sinh(λL) = 0,
−c1 sin(λL) + c2 sinh(λL) = 0
This set of equations has a nontrivial solution if and only if the determinant is zero,

sin(λL)
sinh(λL)
−sin(λL)
sinh(λL)
 = 2 sin(λL) sinh(λL) = 0.
Since sinh(z) is nonzero in 0 ≤arg(z) < π/2, z ̸= 0, and sin(z) has the zeros z = nπ, n ∈N in this domain, the
eigenvalues and eigenfunctions are,
λn = nπ
L ,
Xn = sin
nπx
L

,
n ∈N.
1757

The diﬀerential equation for T becomes,
T ′′ = −a2 nπ
L
4
T,
which has the solutions,

cos

a
nπ
L
2
t

, sin

a
nπ
L
2
t

.
The eigen-solutions of the partial diﬀerential equation are,
u(1)
n = sin
nπx
L

cos

a
nπ
L
2
t

,
u(2)
n = sin
nπx
L

sin

a
nπ
L
2
t

,
n ∈N.
We expand the solution of the partial diﬀerential equation in a series of the eigen-solutions.
u(x, t) =
∞
X
n=1
sin
nπx
L
 
cn cos

a
nπ
L
2
t

+ dn sin

a
nπ
L
2
t

The initial condition for u(x, t) and ut(x, t) allow us to determine the coeﬃcients in the expansion.
u(x, 0) =
∞
X
n=1
cn sin
nπx
L

= f(x)
ut(x, 0) =
∞
X
n=1
dna
nπ
L
2
sin
nπx
L

= g(x)
cn and dn are coeﬃcients in Fourier sine series.
cn = 2
L
Z L
0
f(x) sin
nπx
L

dx
dn =
2L
aπ2n2
Z L
0
g(x) sin
nπx
L

dx
1758

Solution 37.21
ut = κuxx + I2αu,
0 < x < L,
t > 0,
u(0, t) = u(L, t) = 0,
u(x, 0) = g(x).
We will solve this problem with an expansion in eigen-solutions of the partial diﬀerential equation. We substitute the
separation of variables u(x, t) = X(x)T(t) into the partial diﬀerential equation.
(XT)t = κ(XT)xx + I2αXT
T ′
κT −I2α
κ
= X′′
X = −λ2
Now we have an ordinary diﬀerential equation for T and a Sturm-Liouville eigenvalue problem for X. (Note that we
have followed the rule of thumb that the problem will be easier if we move all the parameters out of the eigenvalue
problem.)
T ′ = −
 κλ2 −I2α

T
X′′ = −λ2X,
X(0) = X(L) = 0
The eigenvalues and eigenfunctions for X are
λn = nπ
L ,
Xn = sin
nπx
L

,
n ∈N.
The diﬀerential equation for T becomes,
T ′
n = −

κ
nπ
L
2
−I2α

Tn,
which has the solution,
Tn = c exp

−

κ
nπ
L
2
−I2α

t

.
1759

From this solution, we see that the critical current is
ICR =
rκ
α
π
L.
If I is greater that this, then the eigen-solution for n = 1 will be exponentially growing. This would make the whole
solution exponentially growing. For I < ICR, each of the Tn is exponentially decaying. The eigen-solutions of the
partial diﬀerential equation are,
un = exp

−

κ
nπ
L
2
−I2α

t

sin
nπx
L

,
n ∈N.
We expand u(x, t) in its eigen-solutions, un.
u(x, t) =
∞
X
n=1
an exp

−

κ
nπ
L
2
−I2α

t

sin
nπx
L

We determine the coeﬃcients an from the initial condition.
u(x, 0) =
∞
X
n=1
an sin
nπx
L

= g(x)
an = 2
L
Z L
0
g(x) sin
nπx
L

dx.
If α < 0, then the solution is exponentially decaying regardless of current. Thus there is no critical current.
Solution 37.22
1760

a) The problem is
ut(x, y, z, t) = κ∆u(x, y, z, t),
−∞< x < ∞,
−∞< y < ∞,
0 < z < a,
t > 0,
u(x, y, z, 0) = T,
u(x, y, 0, t) = u(x, y, a, t) = 0.
Because of symmetry, the partial diﬀerential equation in four variables is reduced to a problem in two variables,
ut(z, t) = κuzz(z, t),
0 < z < a,
t > 0,
u(z, 0) = T,
u(0, t) = u(a, t) = 0.
We will solve this problem with an expansion in eigen-solutions of the partial diﬀerential equation that satisfy the
homogeneous boundary conditions. We substitute the separation of variables u(z, t) = Z(z)T(t) into the partial
diﬀerential equation.
ZT ′ = κZ′′T
T ′
κT = Z′′
Z = −λ2
With the boundary conditions at z = 0, a we have the Sturm-Liouville eigenvalue problem,
Z′′ = −λ2Z,
Z(0) = Z(a) = 0,
which has the solutions,
λn = nπ
a ,
Zn = sin
nπz
a

,
n ∈N.
The problem for T becomes,
T ′
n = −κ
nπ
a
2
Tn,
with the solution,
Tn = exp

−κ
nπ
a
2
t

.
1761

The eigen-solutions are
un(z, t) = sin
nπz
a

exp

−κ
nπ
a
2
t

.
The solution for u is a linear combination of the eigen-solutions. The slowest decaying eigen-solution is
u1(z, t) = sin
πz
a

exp

−κ
π
a
2
t

.
Thus the e-folding time is
∆e = a2
κπ2.
b) The problem is
ut(r, θ, z, t) = κ∆u(r, θ, z, t),
0 < r < a,
0 < θ < 2π,
−∞< z < ∞,
t > 0,
u(r, θ, z, 0) = T,
u(0, θ, z, t) is bounded,
u(a, θ, z, t) = 0.
The Laplacian in cylindrical coordinates is
∆u = urr + 1
rur + 1
r2uθθ + uzz.
Because of symmetry, the solution does not depend on θ or z.
ut(r, t) = κ

urr(r, t) + 1
rur(r, t)

,
0 < r < a,
t > 0,
u(r, 0) = T,
u(0, t) is bounded,
u(a, t) = 0.
We will solve this problem with an expansion in eigen-solutions of the partial diﬀerential equation that satisfy
the homogeneous boundary conditions at r = 0 and r = a. We substitute the separation of variables u(r, t) =
1762

R(r)T(t) into the partial diﬀerential equation.
RT ′ = κ

R′′T + 1
rR′T

T ′
κT = R′′
R + R′
rR = −λ2
We have the eigenvalue problem,
R′′ + 1
rR′ + λ2R = 0,
R(0) is bounded, R(a) = 0.
Recall that the Bessel equation,
y′′ + 1
xy′ +

λ2 −ν2
x2

y = 0,
has the general solution y = c1Jν(λx) + c2Yν(λx). We discard the Bessel function of the second kind, Yν, as it
is unbounded at the origin. The solution for R(r) is
R(r) = J0(λr).
Applying the boundary condition at r = a, we see that the eigenvalues and eigenfunctions are
λn = βn
a ,
Rn = J0
βnr
a

,
n ∈N,
where {βn} are the positive roots of the Bessel function J0.
The diﬀerential equation for T becomes,
T ′
n = −κ
βn
a
2
Tn,
which has the solutions,
Tn = exp
 
−κ
βn
a
2
t
!
.
1763

The eigen-solutions of the partial diﬀerential equation for u(r, t) are,
un(r, t) = J0
βnr
a

exp
 
−κ
βn
a
2
t
!
.
The solution u(r, t) is a linear combination of the eigen-solutions, un. The slowest decaying eigenfunction is,
u1(r, t) = J0
β1r
a

exp
 
−κ
β1
a
2
t
!
.
Thus the e-folding time is
∆e = a2
κβ2
1
.
c) The problem is
ut(r, θ, φ, t) = κ∆u(r, θ, φ, t),
0 < r < a,
0 < θ < 2π,
0 < φ < π,
t > 0,
u(r, θ, φ, 0) = T,
u(0, θ, φ, t) is bounded,
u(a, θ, φ, t) = 0.
The Laplacian in spherical coordinates is,
∆u = urr + 2
rur + 1
r2uθθ + cos θ
r2 sin θuθ +
1
r2 sin2 θuφφ.
Because of symmetry, the solution does not depend on θ or φ.
ut(r, t) = κ

urr(r, t) + 2
rur(r, t)

,
0 < r < a,
t > 0,
u(r, 0) = T,
u(0, t) is bounded,
u(a, t) = 0
1764

We will solve this problem with an expansion in eigen-solutions of the partial diﬀerential equation that satisfy
the homogeneous boundary conditions at r = 0 and r = a. We substitute the separation of variables u(r, t) =
R(r)T(t) into the partial diﬀerential equation.
RT ′ = κ

R′′T + 2
rR′T

T ′
κT = R′′
R + 2
r
R′
R = −λ2
We have the eigenvalue problem,
R′′ + 2
rR′ + λ2R = 0,
R(0) is bounded,
R(a) = 0.
Recall that the equation,
y′′ + 2
xy′ +

λ2 −ν(ν + 1)
x2

y = 0,
has the general solution y = c1jν(λx) + c2yν(λx), where jν and yν are the spherical Bessel functions of the ﬁrst
and second kind. We discard yν as it is unbounded at the origin. (The spherical Bessel functions are related to
the Bessel functions by
jν(x) =
r π
2xJν+1/2(x).)
The solution for R(r) is
Rn = j0(λr).
Applying the boundary condition at r = a, we see that the eigenvalues and eigenfunctions are
λn = γn
a ,
Rn = j0
γnr
a

,
n ∈N.
The problem for T becomes
T ′
n = −κ
γn
a
2
Tn,
1765

which has the solutions,
Tn = exp

−κ
γn
a
2
t

.
The eigen-solutions of the partial diﬀerential equation are,
un(r, t) = j0
γnr
a

exp

−κ
γn
a
2
t

.
The slowest decaying eigen-solution is,
u1(r, t) = j0
γ1r
a

exp

−κ
γ1
a
2
t

.
Thus the e-folding time is
∆e = a2
κγ2
1
.
d) If the edges are perfectly insulated, then no heat escapes through the boundary. The temperature is constant for
all time. There is no e-folding time.
Solution 37.23
We will solve this problem with an eigenfunction expansion. Since the partial diﬀerential equation is homogeneous, we
will ﬁnd eigenfunctions in both x and y. We substitute the separation of variables u(x, y, t) = X(x)Y (y)T(t) into the
partial diﬀerential equation.
XY T ′ = κ(t) (X′′Y T + XY ′′T)
T ′
κ(t)T = X′′
X + Y ′′
Y
= −λ2
X′′
X = −Y ′′
Y −λ2 = −µ2
1766

First we have a Sturm-Liouville eigenvalue problem for X,
X′′ = µ2X,
X′(0) = X′(a) = 0,
which has the solutions,
µm = mπ
a ,
Xm = cos
mπx
a

,
m = 0, 1, 2, . . . .
Now we have a Sturm-Liouville eigenvalue problem for Y ,
Y ′′ = −

λ2 −
mπ
a
2
Y,
Y (0) = Y (b) = 0,
which has the solutions,
λmn =
rmπ
a
2
+
nπ
b
2
,
Yn = sin
nπy
b

,
m = 0, 1, 2, . . . ,
n = 1, 2, 3, . . . .
A few of the eigenfunctions, cos
  mπx
a

sin
  nπy
b

, are shown in Figure 37.3.
The diﬀerential equation for T becomes,
T ′
mn = −
mπ
a
2
+
nπ
b
2
κ(t)Tmn,
which has the solutions,
Tmn = exp

−
mπ
a
2
+
nπ
b
2 Z t
0
κ(τ) dτ

.
The eigen-solutions of the partial diﬀerential equation are,
umn = cos
mπx
a

sin
nπy
b

exp

−
mπ
a
2
+
nπ
b
2 Z t
0
κ(τ) dτ

.
The solution of the partial diﬀerential equation is,
u(x, y, t) =
∞
X
m=0
∞
X
n=1
cmn cos
mπx
a

sin
nπy
b

exp

−
mπ
a
2
+
nπ
b
2 Z t
0
κ(τ) dτ

.
1767

We determine the coeﬃcients from the initial condition.
u(x, y, 0) =
∞
X
m=0
∞
X
n=1
cmn cos
mπx
a

sin
nπy
b

= f(x, y)
c0n = 2
ab
Z a
0
Z b
0
f(x, y) sin
nπ
b

dy dx
cmn = 4
ab
Z a
0
Z b
0
f(x, y) cos
mπ
a

sin
nπ
b

dy dx
Solution 37.24
The steady state temperature satisﬁes Laplace’s equation, ∆u = 0. The Laplacian in cylindrical coordinates is,
∆u(r, θ, z) = urr + 1
rur + 1
r2uθθ + uzz.
Because of the homogeneity in the z direction, we reduce the partial diﬀerential equation to,
urr + 1
rur + 1
r2uθθ = 0,
0 < r < 1,
0 < θ < π.
The boundary conditions are,
u(r, 0) = u(r, π) = 0,
u(0, θ) = 0,
u(1, θ) = 1.
We will solve this problem with an eigenfunction expansion. We substitute the separation of variables u(r, θ) = R(r)T(θ)
into the partial diﬀerential equation.
R′′T + 1
rR′T + 1
r2RT ′′ = 0
r2R′′
R + rR′
R = −T ′′
T = λ2
1768

We have the regular Sturm-Liouville eigenvalue problem,
T ′′ = −λ2T,
T(0) = T(π) = 0,
which has the solutions,
λn = n,
Tn = sin(nθ),
n ∈N.
The problem for R becomes,
r2R′′ + rR′ −n2R = 0,
R(0) = 0.
This is an Euler equation. We substitute R = rα into the diﬀerential equation to obtain,
α(α −1) + α −n2 = 0,
α = ±n.
The general solution of the diﬀerential equation for R is
Rn = c1rn + c2r−n.
The solution that vanishes at r = 0 is
Rn = crn.
The eigen-solutions of the diﬀerential equation are,
un = rn sin(nθ).
The solution of the partial diﬀerential equation is
u(r, θ) =
∞
X
n=1
anrn sin(nθ).
We determine the coeﬃcients from the boundary condition at r = 1.
u(1, θ) =
∞
X
n=1
an sin(nθ) = 1
an = 2
π
Z π
0
sin(nθ) dθ = 2
πn (1 −(−1)n)
1769

The solution of the partial diﬀerential equation is
u(r, θ) = 4
π
∞
X
n=1
odd n
rn sin(nθ).
Solution 37.25
The problem is
uxx + uyy = 0,
0 < x,
0 < y < 1,
u(x, 0) = u(x, 1) = 0,
u(0, y) = f(y).
We substitute the separation of variables u(x, y) = X(x)Y (y) into the partial diﬀerential equation.
X′′Y + XY ′′ = 0
X′′
X = −Y ′′
Y
= λ2
We have the regular Sturm-Liouville problem,
Y ′′ = −λ2Y,
Y (0) = Y (1) = 0,
which has the solutions,
λn = nπ,
Yn = sin(nπy),
n ∈N.
The problem for X becomes,
X′′
n = (nπ)2X,
which has the general solution,
Xn = c1 enπx +c2 e−nπx .
The solution that is bounded as x →∞is,
Xn = c e−nπx .
1770

The eigen-solutions of the partial diﬀerential equation are,
un = e−nπx sin(nπy),
n ∈N.
The solution of the partial diﬀerential equation is,
u(x, y) =
∞
X
n=1
an e−nπx sin(nπy).
We ﬁnd the coeﬃcients from the boundary condition at x = 0.
u(0, y) =
∞
X
n=1
an sin(nπy) = f(y)
an = 2
Z 1
0
f(y) sin(nπy) dy
Solution 37.26
The Laplacian in polar coordinates is
∆u ≡urr + 1
rur + 1
r2uθθ.
Since we have homogeneous boundary conditions at θ = 0 and θ = α, we will solve this problem with an eigenfunction
expansion. We substitute the separation of variables u(r, θ) = R(r)Θ(θ) into Laplace’s equation.
R′′Θ + 1
rR′Θ + 1
r2RΘ′′ = 0
r2R′′
R + rR′
R = −Θ′′
Θ = λ2.
1771

We have a regular Sturm-Liouville eigenvalue problem for Θ.
Θ′′ = −λ2Θ,
Θ(0) = Θ(α) = 0
λn = nπ
α ,
Θn = sin
nπθ
α

,
n ∈Z+.
We have Euler equations for Rn. We solve them with the substitution R = rβ.
r2R′′
n + rR′
n −
nπ
α
2
Rn = 0,
Rn(a) = 0
β(β −1) + β −
nπ
α
2
= 0
β = ±nπ
α
Rn = c1rnπ/α + c2r−nπ/α.
The solution, (up to a multiplicative constant), that vanishes at r = a is
Rn = rnπ/α −a2nπ/αr−nπ/α.
Thus the series expansion of our solution is,
u(r, θ) =
∞
X
n=1
un
 rnπ/α −a2nπ/αr−nπ/α
sin
nπθ
α

.
We determine the coeﬃcients from the boundary condition at r = b.
u(b, θ) =
∞
X
n=1
un
 bnπ/α −a2nπ/αb−nπ/α
sin
nπθ
α

= f(θ)
un =
2
α (bnπ/α −a2nπ/αb−nπ/α)
Z α
0
f(θ) sin
nπθ
α

dθ
1772

Solution 37.27
a) The mathematical statement of the problem is
utt = c2uxx,
0 < x < L,
t > 0,
u(0, t) = u(L, t) = 0,
u(x, 0) = 0,
ut(x, 0) =
(
v
for |x −ξ| < d
0
for |x −ξ| > d.
Because we are interest in the harmonics of the motion, we will solve this problem with an eigenfunction expansion in
x. We substitute the separation of variables u(x, t) = X(x)T(t) into the wave equation.
XT ′′ = c2X′′T
T ′′
c2T = X′′
X = −λ2
The eigenvalue problem for X is,
X′′ = −λ2X,
X(0) = X(L) = 0,
which has the solutions,
λn = nπ
L ,
Xn = sin
nπx
L

,
n ∈N.
The ordinary diﬀerential equation for the Tn are,
T ′′
n = −
nπc
L
2
Tn,
which have the linearly independent solutions,
cos
nπct
L

,
sin
nπct
L

.
1773

The solution for u(x, t) is a linear combination of the eigen-solutions.
u(x, t) =
∞
X
n=1
sin
nπx
L
 
an cos
nπct
L

+ bn sin
nπct
L

Since the string initially has zero displacement, each of the an are zero.
u(x, t) =
∞
X
n=1
bn sin
nπx
L

sin
nπct
L

Now we use the initial velocity to determine the coeﬃcients in the expansion. Because the position is a continuous
function of x, and there is a jump discontinuity in the velocity as a function of x, the coeﬃcients in the expansion will
decay as 1/n2.
ut(x, 0) =
∞
X
n=1
nπc
L bn sin
nπx
L

=
(
v
for |x −ξ| < d
0
for |x −ξ| > d.
nπc
L bn = 2
L
Z L
0
ut(x, 0) sin
nπx
L

dx
bn =
2
nπc
Z ξ+d
ξ−d
v sin
nπx
L

dx
= 4Lv
n2π2c sin
nπd
L

sin
nπξ
L

The solution for u(x, t) is,
u(x, t) = 4Lv
π2c
∞
X
n=1
1
n2 sin
nπd
L

sin
nπξ
L

sin
nπx
L

sin
nπct
L

.
1774

b) The form of the solution is again,
u(x, t) =
∞
X
n=1
bn sin
nπx
L

sin
nπct
L

We determine the coeﬃcients in the expansion from the initial velocity.
ut(x, 0) =
∞
X
n=1
nπc
L bn sin
nπx
L

=
(
v cos

π(x−ξ)
2d

for |x −ξ| < d
0
for |x −ξ| > d.
nπc
L bn = 2
L
Z L
0
ut(x, 0) sin
nπx
L

dx
bn =
2
nπc
Z ξ+d
ξ−d
v cos
π(x −ξ)
2d

sin
nπx
L

dx
bn =
(
8dL2v
nπ2c(L2−4d2n2) cos
  nπd
L

sin
  nπξ
L

for d ̸= L
2n,
v
n2π2c
 2nπd + L sin
  2nπd
L

sin
  nπξ
L

for d = L
2n
The solution for u(x, t) is,
u(x, t) = 8dL2v
π2c
∞
X
n=1
1
n(L2 −4d2n2) cos
nπd
L

sin
nπξ
L

sin
nπx
L

sin
nπct
L

for d ̸= L
2n,
u(x, t) =
v
π2c
∞
X
n=1
1
n2

2nπd + L sin
2nπd
L

sin
nπξ
L

sin
nπx
L

sin
nπct
L

for d = L
2n.
c) The kinetic energy of the string is
E = 1
2
Z L
0
ρ (ut(x, t))2 dx,
1775

where ρ is the density of the string per unit length.
Flat Hammer. The nth harmonic is
un = 4Lv
n2π2c sin
nπd
L

sin
nπξ
L

sin
nπx
L

sin
nπct
L

.
The kinetic energy of the nth harmonic is
En = ρ
2
Z L
0
∂un
∂t
2
dx = 4Lv2
n2π2 sin2
nπd
L

sin2
nπξ
L

cos2
nπct
L

.
This will be maximized if
sin2
nπξ
L

= 1,
nπξ
L
= π(2m −1)
2
,
m = 1, . . . , n,
ξ = (2m −1)L
2n
,
m = 1, . . . , n
We note that the kinetic energies of the nth harmonic decay as 1/n2.
Curved Hammer. We assume that d ̸= L
2n. The nth harmonic is
un =
8dL2v
nπ2c(L2 −4d2n2) cos
nπd
L

sin
nπξ
L

sin
nπx
L

sin
nπct
L

.
The kinetic energy of the nth harmonic is
En = ρ
2
Z L
0
∂un
∂t
2
dx =
16d2L3v2
π2(L2 −4d2n2)2 cos2
nπd
L

sin2
nπξ
L

cos2
nπct
L

.
1776

This will be maximized if
sin2
nπξ
L

= 1,
ξ = (2m −1)L
2n
,
m = 1, . . . , n
We note that the kinetic energies of the nth harmonic decay as 1/n4.
Solution 37.28
In mathematical notation, the problem is
utt −c2uxx = s(x, t),
0 < x < L,
t > 0,
u(0, t) = u(L, t) = 0,
u(x, 0) = ut(x, 0) = 0.
Since this is an inhomogeneous partial diﬀerential equation, we will expand the solution in a series of eigenfunctions in
x for which the coeﬃcients are functions of t. The solution for u has the form,
u(x, t) =
∞
X
n=1
un(t) sin
nπx
L

.
Substituting this expression into the inhomogeneous partial diﬀerential equation will give us ordinary diﬀerential equa-
tions for each of the un.
∞
X
n=1

u′′
n + c2 nπ
L
2
un

sin
nπx
L

= s(x, t).
We expand the right side in a series of the eigenfunctions.
s(x, t) =
∞
X
n=1
sn(t) sin
nπx
L

.
1777

For 0 < t < δ we have
sn(t) = 2
L
Z L
0
s(x, t) sin
nπx
L

dx
= 2
L
Z L
0
v cos
π(x −ξ)
2d

sin
πt
δ

sin
nπx
L

dx
=
8dLv
π(L2 −4d2n2) cos
nπd
L

sin
nπξ
L

sin
πt
δ

.
For t > δ, sn(t) = 0. Substituting this into the partial diﬀerential equation yields,
u′′
n +
nπc
L
2
un =
(
8dLv
π(L2−4d2n2) cos
  nπd
L

sin
  nπξ
L

sin
  πt
δ

,
for t < δ,
0
for t > δ.
Since the initial position and velocity of the string is zero, we have
un(0) = u′
n(0) = 0.
First we solve the diﬀerential equation on the range 0 < t < δ. The homogeneous solutions are
cos
nπct
L

,
sin
nπct
L

.
Since the right side of the ordinary diﬀerential equation is a constant times sin(πt/δ), which is an eigenfunction of the
diﬀerential operator, we can guess the form of a particular solution, pn(t).
pn(t) = d sin
πt
δ

We substitute this into the ordinary diﬀerential equation to determine the multiplicative constant d.
pn(t) = −
8dδ2L3v
π3(L2 −c2δ2n2)(L2 −4d2n2) cos
nπd
L

sin
nπξ
L

sin
πt
δ

1778

The general solution for un(t) is
un(t) = a cos
nπct
L

+ b sin
nπct
L

−
8dδ2L3v
π3(L2 −c2δ2n2)(L2 −4d2n2) cos
nπd
L

sin
nπξ
L

sin
πt
δ

.
We use the initial conditions to determine the constants a and b. The solution for 0 < t < δ is
un(t) =
8dδ2L3v
π3(L2 −c2δ2n2)(L2 −4d2n2) cos
nπd
L

sin
nπξ
L
  L
δcn sin
nπct
L

−sin
πt
δ

.
The solution for t > δ, the solution is a linear combination of the homogeneous solutions. This linear combination is
determined by the position and velocity at t = δ. We use the above solution to determine these quantities.
un(δ) =
8dδ2L4v
π3δcn(L2 −c2δ2n2)(L2 −4d2n2) cos
nπd
L

sin
nπξ
L

sin
nπcδ
L

u′
n(δ) =
8dδ2L3v
π2δ(L2 −c2δ2n2)(L2 −4d2n2) cos
nπd
L

sin
nπξ
L
 
1 + cos
nπcδ
L

The fundamental set of solutions at t = δ is

cos
nπc(t −δ)
L

, L
nπc sin
nπc(t −δ)
L

From the initial conditions at t = δ, we see that the solution for t > δ is
un(t) =
8dδ2L3v
π3(L2 −c2δ2n2)(L2 −4d2n2) cos
nπd
L

sin
nπξ
L

 L
δcn sin
nπcδ
L

cos
nπc(t −δ)
L

+ π
δ

1 + cos
nπcδ
L

sin
nπc(t −δ)
L

.
1779

Width of the Hammer. The nth harmonic has the width dependent factor,
d
L2 −4d2n2 cos
nπd
L

.
Diﬀerentiating this expression and trying to ﬁnd zeros to determine extrema would give us an equation with both
algebraic and transcendental terms. Thus we don’t attempt to ﬁnd the maxima exactly. We know that d < L. The
cosine factor is large when
nπd
L
≈mπ,
m = 1, 2, . . . , n −1,
d ≈mL
n ,
m = 1, 2, . . . , n −1.
Substituting d = mL/n into the width dependent factor gives us
d
L2(1 −4m2)(−1)m.
Thus we see that the amplitude of the nth harmonic and hence its kinetic energy will be maximized for
d ≈L
n
The cosine term in the width dependent factor vanishes when
d = (2m −1)L
2n
,
m = 1, 2, . . . , n.
The kinetic energy of the nth harmonic is minimized for these widths.
For the lower harmonics, n ≪
L
2d, the kinetic energy is proportional to d2; for the higher harmonics, n ≫
L
2d, the
kinetic energy is proportional to 1/d2.
1780

Duration of the Blow. The nth harmonic has the duration dependent factor,
δ2
L2 −n2c2δ2
 L
ncδ sin
nπcδ
L

cos
nπc(t −δ)
L

+ π
δ

1 + cos
nπcδ
L

sin
nπc(t −δ)
L

.
If we assume that δ is small, then
L
ncδ sin
nπcδ
L

≈π.
and
π
δ

1 + cos
nπcδ
L

≈2π
δ .
Thus the duration dependent factor is about,
δ
L2 −n2c2δ2 sin
nπc(t −δ)
L

.
Thus for the lower harmonics, (those satisfying n ≪
L
cδ), the amplitude is proportional to δ, which means that the
kinetic energy is proportional to δ2. For the higher harmonics, (those with n ≫L
cδ), the amplitude is proportional to
1/δ, which means that the kinetic energy is proportional to 1/δ2.
Solution 37.29
Substituting u(x, y, z, t) = v(x, y, z) eıωt into the wave equation will give us a Helmholtz equation.
−ω2v eıωt −c2(vxx + vyy + vzz) eıωt = 0
vxx + vyy + vzz + k2v = 0.
We ﬁnd the propagating modes with separation of variables. We substitute v = X(x)Y (y)Z(z) into the Helmholtz
equation.
X′′Y Z + XY ′′Z + XY Z′′ + k2XY Z = 0
−X′′
X = Y ′′
Y + Z′′
Z + k2 = ν2
1781

The eigenvalue problem in x is
X′′ = −ν2X,
X(0) = X(L) = 0,
which has the solutions,
νn = nπ
L ,
Xn = sin
nπx
L

.
We continue with the separation of variables.
−Y ′′
Y
= Z′′
Z + k2 −
nπ
L
2
= µ2
The eigenvalue problem in y is
Y ′′ = −µ2Y,
Y (0) = Y (L) = 0,
which has the solutions,
µn = mπ
L ,
Ym = sin
mπy
L

.
Now we have an ordinary diﬀerential equation for Z,
Z′′ +

k2 −
π
L
2  n2 + m2
Z = 0.
We deﬁne the eigenvalues,
λ2
n,m = k2 −
π
L
2  n2 + m2
.
If k2 −
  π
L
2 (n2 + m2) < 0, then the solutions for Z are,
exp
 
±
sπ
L
2
(n2 + m2) −k2

z
!
.
We discard this case, as the solutions are not bounded as z →∞.
1782

If k2 −
  π
L
2 (n2 + m2) = 0, then the solutions for Z are,
{1, z}
The solution Z = 1 satisﬁes the boundedness and nonzero condition at inﬁnity. This corresponds to a standing wave.
If k2 −
  π
L
2 (n2 + m2) > 0, then the solutions for Z are,
e±ıλn,mz .
These satisfy the boundedness and nonzero conditions at inﬁnity. For values of n, m satisfying k2−
  π
L
2 (n2 + m2) ≥0,
there are the propagating modes,
un,m = sin
nπx
L

sin
mπy
L

eı(ωt±λn,mz) .
Solution 37.30
utt = c2∆u,
0 < x < a, 0 < y < b,
(37.12)
u(0, y) = u(a, y) = u(x, 0) = u(x, b) = 0.
We substitute the separation of variables u(x, y, t) = X(x)Y (y)T(t) into Equation 37.12.
T ′′
c2T = X′′
X + Y ′′
Y
= −ν
X′′
X = −Y ′′
Y −ν = −µ
This gives us diﬀerential equations for X(x), Y (y) and T(t).
X′′ = −µX,
X(0) = X(a) = 0
Y ′′ = −(ν −µ)Y,
Y (0) = Y (b) = 0
T ′′ = −c2νT
1783

First we solve the problem for X.
µm =
mπ
a
2
,
Xm = sin
mπx
a

Then we solve the problem for Y .
νm,n =
mπ
a
2
+
nπ
b
2
,
Ym,n = sin
nπy
b

Finally we determine T.
Tm,n = cos
sin
 
cπ
rm
a
2
+
n
b
2
t
!
The modes of oscillation are
um,n = sin
mπx
a

sin
nπy
b
 cos
sin
 
cπ
rm
a
2
+
n
b
2
t
!
.
The frequencies are
ωm,n = cπ
rm
a
2
+
n
b
2
.
Figure 37.4 shows a few of the modes of oscillation in surface and density plots.
Solution 37.31
We substitute the separation of variables φ = X(x)Y (y)T(t) into the diﬀerential equation.
φt = a2 (φxx + φyy)
(37.13)
XY T ′ = a2 (X′′Y T + XY ′′T)
T ′
a2T = X′′
X + Y ′′
Y
= −ν
T ′
a2T = −ν,
X′′
X = −ν −Y ′′
Y
= −µ
1784

First we solve the eigenvalue problem for X.
X′′ + µX = 0,
X(0) = X(lx) = 0
µm =
mπ
lx
2
,
Xm(x) = sin
mπx
lx

,
m ∈Z+
Then we solve the eigenvalue problem for Y .
Y ′′ + (ν −µm)Y = 0,
Y ′(0) = Y ′(ly) = 0
νmn = µm +
nπ
ly
2
,
Ymn(y) = cos
nπy
ly

,
n ∈Z0+
Next we solve the diﬀerential equation for T, (up to a multiplicative constant).
T ′ = −a2νmnT
T(t) = exp
 −a2νmnt

The eigensolutions of Equation 37.13 are
sin(µmx) cos(νmny) exp
 −a2νmnt

,
m ∈Z+, n ∈Z0+.
We choose the eigensolutions φmn to be orthonormal on the xy domain at t = 0.
φm0(x, y, t) =
s
2
lxly
sin(µmx) exp
 −a2νmnt

,
m ∈Z+
φmn(x, y, t) =
2
p
lxly
sin(µmx) cos(νmny) exp
 −a2νmnt

,
m ∈Z+, n ∈Z+
The solution of Equation 37.13 is a linear combination of the eigensolutions.
φ(x, y, t) =
∞
X
m=1
n=0
cmnφmn(x, y, t)
1785

We determine the coeﬃcients from the initial condition.
φ(x, y, 0) = 1
∞
X
m=1
n=0
cmnφmn(x, y, 0) = 1
cmn =
Z lx
0
Z ly
0
φmn(x, y, 0) dy dx
cm0 =
s
2
lxly
Z lx
0
Z ly
0
sin(µmx) dy dx
cm0 =
p
2lxly
1 −(−1)m
mπ
,
m ∈Z+
cmn =
2
p
lxly
Z lx
0
Z ly
0
sin(µmx) cos(νmny) dy dx
cmn = 0,
m ∈Z+, n ∈Z+
φ(x, y, t) =
∞
X
m=1
cm0φm0(x, y, t)
φ(x, y, t) =
∞
X
m=1
odd m
2
p
2lxly
mπ
sin(µmx) exp
 −a2νmnt

Addendum. Note that an equivalent problem to the one speciﬁed is
φt = a2 (φxx + φyy) ,
0 < x < lx, −∞< y < ∞,
φ(x, y, 0) = 1,
φ(0, y, t) = φ(ly, y, t) = 0.
Here we have done an even periodic continuation of the problem in the y variable. Thus the boundary conditions
φy(x, 0, t) = φy(x, ly, t) = 0
1786

are automatically satisﬁed. Note that this problem does not depend on y. Thus we only had to solve
φt = a2φxx,
0 < x < lx
φ(x, 0) = 1,
φ(0, t) = φ(ly, t) = 0.
Solution 37.32
1. Since the initial and boundary conditions do not depend on θ, neither does φ. We apply the separation of variables
φ = u(r)T(t).
φt = a2∆φ
(37.14)
φt = a21
r (rφr)r
(37.15)
T ′
a2T = 1
r(ru′)′ = −λ
(37.16)
We solve the eigenvalue problem for u(r).
(ru′)′ + λu = 0,
u(0) bounded,
u(R) = 0
First we write the general solution.
u(r) = c1J0
√
λr

+ c2Y0
√
λr

The Bessel function of the second kind, Y0, is not bounded at r = 0, so c2 = 0. We use the boundary condition
at r = R to determine the eigenvalues.
λn =
j0,n
R
2
,
un(r) = cJ0
j0,nr
R

1787

We choose the constant c so that the eigenfunctions are orthonormal with respect to the weighting function r.
un(r) =
J0

j0,nr
R

rR R
0 rJ2
0

j0,nr
R

=
√
2
RJ1(j0,n)J0
j0,nr
R

Now we solve the diﬀerential equation for T.
T ′ = −a2λnT
Tn = exp
 
−
aj0,n
R2
2
t
!
The eigensolutions of Equation 37.14 are
φn(r, t) =
√
2
RJ1(j0,n)J0
j0,nr
R

exp
 
−
aj0,n
R2
2
t
!
The solution is a linear combination of the eigensolutions.
φ =
∞
X
n=1
cn
√
2
RJ1(j0,n)J0
j0,nr
R

exp
 
−
aj0,n
R2
2
t
!
1788

We determine the coeﬃcients from the initial condition.
φ(r, θ, 0) = V
∞
X
n=1
cn
√
2
RJ1(j0,n)J0
j0,nr
R

= V
cn =
Z R
0
V r
√
2
RJ1(j0,n)J0
j0,nr
R

dr
cn = V
√
2
RJ1(j0,n)
R
j0,n/RJ1 (j0,n)
cn =
√
2 V R
j0,n
φ(r, θ, t) = 2V
∞
X
n=1
J0

j0,nr
R

j0,nJ1(j0,n) exp
 
−
aj0,n
R2
2
t
!
2.
Jν(r) ∼
r
2
πr cos

r −πν
2 −π
4

,
r →+∞
jν,n ∼

n + ν
2 −1
4

π
For large n, the terms in the series solution at t = 0 are
J0

j0,nr
R

j0,nJ1(j0,n) ∼
q
2R
πj0,nr cos

j0,nr
R
−π
4

j0,n
q
2
πj0,n cos
 j0,n −3π
4

∼
R
r(n −1/4)π
cos

(n−1/4)πr
R
−π
4

cos ((n −1)π)
.
1789

The coeﬃcients decay as 1/n.
Solution 37.33
1. We substitute the separation of variables Ψ = T(t)Θ(θ)Φ(φ) into Equation 37.7
T ′ΘΦ = a2
R2
 1
sin θ
∂
∂θ(sin θ TΘ′Φ) +
1
sin2 θTΘΦ′′

R2T ′
a2T =

1
sin θ Θ(sin θ Θ′)′ +
1
sin2 θ
Φ′′
Φ

= −µ
sin θ
Θ (sin θ Θ′)′ + µ sin2 θ = −Φ′′
Φ = ν
We have diﬀerential equations for each of T, Θ and Φ.
T ′ = −µ a2
R2T,
1
sin θ(sin θ Θ′)′ +

µ −
ν
sin2 θ

Θ = 0,
Φ′′ + νΦ = 0
2. In order that the solution be continuously diﬀerentiable, we need the periodic boundary conditions
Φ(0) = Φ(2π),
Φ′(0) = Φ′(2π).
The eigenvalues and eigenfunctions for Φ are
νn = n2,
Φn =
1
√
2π
eınφ,
n ∈Z.
Now we deal with the equation for Θ.
x = cos θ,
Θ(θ) = P(x),
sin2 θ = 1 −x2,
d
dx =
1
sin θ
d
dθ
1
sin θ(sin2 θ
1
sin θ Θ′)′ +

µ −
ν
sin2 θ

Θ = 0
  1 −x2
P ′′ +

µ −
n2
1 −x2

P = 0
P(x) should be bounded at the endpoints, x = −1 and x = 1.
1790

3. If the solution does not depend on θ, then the only one of the Φn that will appear in the solution is Φ0 = 1/
√
2π.
The equations for T and P become
  1 −x2
P ′′ + µP = 0,
P(±1) bounded,
T ′ = −µ a2
R2T.
The solutions for P are the Legendre polynomials.
µl = l(l + 1),
Pl(cos θ),
l ∈Z0+
We solve the diﬀerential equation for T.
T ′ = −l(l + 1) a2
R2T
Tl = exp

−a2l(l + 1)
R2
t

The eigensolutions of the partial diﬀerential equation are
Ψl = Pl(cos θ) exp

−a2l(l + 1)
R2
t

.
The solution is a linear combination of the eigensolutions.
Ψ =
∞
X
l=0
AlPl(cos θ) exp

−a2l(l + 1)
R2
t

1791

4. We determine the coeﬃcients in the expansion from the initial condition.
Ψ(θ, 0) = 2 cos2 θ −1
∞
X
l=0
AlPl(cos θ) = 2 cos2 θ −1
A0 + A1 cos θ + A2
3
2 cos2 θ −1
2

+ · · · = 2 cos2 θ −1
A0 = −1
3,
A1 = 0,
A2 = 4
3,
A3 = A4 = · · · = 0
Ψ(θ, t) = −1
3P0(cos θ) + 4
3P2(cos θ) exp

−6a2
R2 t

Ψ(θ, t) = −1
3 +

2 cos2 θ −2
3

exp

−6a2
R2 t

Solution 37.34
Since we have homogeneous boundary conditions at x = 0 and x = 1, we will expand the solution in a series of
eigenfunctions in x. We determine a suitable set of eigenfunctions with the separation of variables, φ = X(x)Y (y).
φxx + φyy = 0
(37.17)
X′′
X = −Y ′′
Y
= −λ
We have diﬀerential equations for X and Y .
X′′ + λX = 0,
X(0) = X(1) = 0
Y ′′ −λY = 0,
Y (0) = 0
The eigenvalues and orthonormal eigenfunctions for X are
λn = (nπ)2,
Xn(x) =
√
2 sin(nπx),
n ∈Z+.
1792

The solutions for Y are, (up to a multiplicative constant),
Yn(y) = sinh(nπy).
The solution of Equation 37.17 is a linear combination of the eigensolutions.
φ(x, y) =
∞
X
n=1
an
√
2 sin(nπx) sinh(nπy)
We determine the coeﬃcients from the boundary condition at y = 2.
x(1 −x) =
∞
X
n=1
an
√
2 sin(nπx) sinh(nπ2)
an sinh(2nπ) =
√
2
Z 1
0
x(1 −x) sin(nπx) dx
an = 2
√
2(1 −(−1)n)
n3π3 sinh(2nπ)
φ(x, y) = 8
π3
∞
X
n=1
odd n
1
n3 sin(nπx)sinh(nπy)
sinh(2nπ)
The solution at x = 1/2, y = 1 is
φ(1/2, 1) = −8
π3
∞
X
n=1
odd n
1
n3
sinh(nπ)
sinh(2nπ).
1793

Let Rk be the relative error at that point incurred by taking k terms.
Rk =

−8
π3
P∞
n=k+2
odd n
1
n3
sinh(nπ)
sinh(2nπ)
−8
π3
P∞
n=1
odd n
1
n3
sinh(nπ)
sinh(2nπ)

Rk =
P∞
n=k+2
odd n
1
n3
sinh(nπ)
sinh(2nπ)
P∞
n=1
odd n
1
n3
sinh(nπ)
sinh(2nπ)
Since R1 ≈0.0000693169 we see that one term is suﬃcient for 1% or 0.1% accuracy.
Now consider φx(1/2, 1).
φx(x, y) = 8
π2
∞
X
n=1
odd n
1
n2 cos(nπx)sinh(nπy)
sinh(2nπ)
φx(1/2, 1) = 0
Since all the terms in the series are zero, accuracy is not an issue.
Solution 37.35
The solution has the form
ψ =
(
αr−n−1P m
n (cos θ) sin(mφ),
r > a
βrnP m
n (cos θ) sin(mφ),
r < a.
The boundary condition on ψ at r = a gives us the constraint
αa−n−1 −βan = 0
β = αa−2n−1.
1794

Then we apply the boundary condition on ψr at r = a.
−(n + 1)αa−n−2 −nαa−2n−1an−1 = 1
α = −an+2
2n + 1
ψ =
(
−an+2
2n+1r−n−1P m
n (cos θ) sin(mφ),
r > a
−a−n+1
2n+1 rnP m
n (cos θ) sin(mφ),
r < a
Solution 37.36
We expand the solution in a Fourier series.
φ = 1
2a0(r) +
∞
X
n=1
an(r) cos(nθ) +
∞
X
n=1
bn(r) sin(nθ)
We substitute the series into the Laplace’s equation to determine ordinary diﬀerential equations for the coeﬃcients.
∂
∂r

r∂φ
∂r

+ 1
r2
∂2φ
∂θ2 = 0
a′′
0 + 1
ra′
0 = 0,
a′′
n + 1
ra′
n −n2an = 0,
b′′
n + 1
rb′
n −n2bn = 0
The solutions that are bounded at r = 0 are, (to within multiplicative constants),
a0(r) = 1,
an(r) = rn,
bn(r) = rn.
Thus φ(r, θ) has the form
φ(r, θ) = 1
2c0 +
∞
X
n=1
cnrn cos(nθ) +
∞
X
n=1
dnrn sin(nθ)
We apply the boundary condition at r = R.
φr(R, θ) =
∞
X
n=1
ncnRn−1 cos(nθ) +
∞
X
n=1
ndnRn−1 sin(nθ)
1795

In order that φr(R, θ) have a Fourier series of this form, it is necessary that
Z 2π
0
φr(R, θ) dθ = 0.
In that case c0 is arbitrary in our solution. The coeﬃcients are
cn =
1
πnRn−1
Z 2π
0
φr(R, α) cos(nα) dα,
dn =
1
πnRn−1
Z 2π
0
φr(R, α) sin(nα) dα.
We substitute the coeﬃcients into our series solution to determine it up to the additive constant.
φ(r, θ) = R
π
∞
X
n=1
1
n
 r
R
n Z 2π
0
φr(R, α) cos(n(θ −α)) dα
φ(r, θ) = R
π
Z 2π
0
φr(R, α)
∞
X
n=1
1
n
 r
R
n
cos(n(θ −α)) dα
φ(r, θ) = R
π
Z 2π
0
φr(R, α)
∞
X
n=1
Z r
0
ρn−1
Rn dρℜ
 eın(θ−α)
dα
φ(r, θ) = R
π
Z 2π
0
φr(R, α)ℜ
 Z r
0
1
ρ
∞
X
n=1
ρn
Rn eın(θ−α) dρ
!
dα
φ(r, θ) = R
π
Z 2π
0
φr(R, α)ℜ
 Z r
0
1
ρ
ρ
R eı(θ−α)
1 −ρ
R eı(θ−α) dρ
!
dα
φ(r, θ) = −R
π
Z 2π
0
φr(R, α)ℜ

ln

1 −r
R eı(θ−α)
dα
φ(r, θ) = −R
π
Z 2π
0
φr(R, α) ln
1 −r
R eı(θ−α) dα
φ(r, θ) = −R
2π
Z 2π
0
φr(R, α) ln

1 −2 r
R cos(θ −α) + r2
R2

dα
1796

Solution 37.37
We will assume that both α and β are nonzero. The cases of real and pure imaginary have already been covered.
We solve the ordinary diﬀerential equations, (up to a multiplicative constant), to ﬁnd special solutions of the diﬀusion
equation.
T ′
T = (α + ıβ)2,
X′′
X = (α + ıβ)2
a2
T = exp
 (α + ıβ)2t

,
X = exp

±α + ıβ
a
x

T = exp
  α2 −β2
t + ı2αβt

,
X = exp

±α
a x ± ıβ
ax

φ = exp
 α2 −β2
t ± α
a x + ı

2αβt ± β
ax

We take the sum and diﬀerence of these solutions to obtain
φ = exp
 α2 −β2
t ± α
a x
 cos
sin

2αβt ± β
ax

1797

Figure 37.2: The Normal Modes u01 through u34
1798

m=2, n=1
m=2, n=2
m=2, n=3
m=1, n=1
m=1, n=2
m=1, n=3
m=0, n=1
m=0, n=2
m=0, n=3
Figure 37.3: The eigenfunctions cos
  mπx
a

sin
  nπy
b

1799

m=3,n=1
m=3,n=2
m=3,n=3
m=2,n=1
m=2,n=2
m=2,n=3
m=1,n=1
m=1,n=2
m=1,n=3
m=3,n=1 m=3,n=2 m=3,n=3
m=2,n=1 m=2,n=2 m=2,n=3
m=1,n=1 m=1,n=2 m=1,n=3
Figure 37.4: The modes of oscillation of a rectangular drum head.
1800

Chapter 38
Finite Transforms
Example 38.0.1 Consider the problem
∆u −1
c2
∂2u
∂t2 = δ(x −ξ)δ(y −η) e−ıωt
on −∞< x < ∞, 0 < y < b,
with
uy(x, 0, t) = uy(x, b, t) = 0.
Substituting u(x, y, t) = v(x, y) e−ıωt into the partial diﬀerential equation yields the problem
∆v + k2v = δ(x −ξ)δ(y −η)
on −∞< x < ∞, 0 < y < b,
with
vy(x, 0) = vy(x, b) = 0.
We assume that the solution has the form
v(x, y) = 1
2c0(x) +
∞
X
n=1
cn(x) cos
nπy
b

,
(38.1)
1801

and apply a ﬁnite cosine transform in the y direction. Integrating from 0 to b yields
Z b
0
vxx + vyy + k2v dy =
Z b
0
δ(x −ξ)δ(y −η) dy,

vy
b
0 +
Z b
0
vxx + k2v dy = δ(x −ξ),
Z b
0
vxx + k2v dy = δ(x −ξ).
Substituting in Equation 38.1 and using the orthogonality of the cosines gives us
c′′
0(x) + k2c0(x) = 2
bδ(x −ξ).
Multiplying by cos(nπy/b) and integrating form 0 to b yields
Z b
0
 vxx + vyy + k2v

cos
nπy
b

dy =
Z b
0
δ(x −ξ)δ(y −η) cos
nπy
b

dy.
The vyy term becomes
Z b
0
vyy cos
nπy
b

dy =
h
vy cos
nπy
b
ib
0 −
Z b
0
−nπ
b vy sin
nπy
b

dy
=
hnπ
b v sin
nπy
b
ib
0 −
Z b
0
nπ
b
2
v cos
nπy
b

dy.
The right-hand-side becomes
Z b
0
δ(x −ξ)δ(y −η) cos
nπy
b

dy = δ(x −ξ) cos
nπη
b

.
1802

Thus the partial diﬀerential equation becomes
Z b
0

vxx −
nπ
b
2
v + k2v

cos
nπy
b

dy = δ(x −ξ) cos
nπη
b

.
Substituting in Equation 38.1 and using the orthogonality of the cosines gives us
c′′
n(x) +

k2 −
nπ
b
2
cn(x) = 2
bδ(x −ξ) cos
nπη
b

.
Now we need to solve for the coeﬃcients in the expansion of v(x, y). The homogeneous solutions for c0(x) are
e±ıkx. The solution for u(x, y, t) must satisfy the radiation condition. The waves at x = −∞travel to the left and the
waves at x = +∞travel to the right. The two solutions of that will satisfy these conditions are, respectively,
y1 = e−ıkx,
y2 = eıkx .
The Wronskian of these two solutions is ı2k. Thus the solution for c0(x) is
c0(x) = e−ıkx< eıkx>
ıbk
We need to consider three cases for the equation for cn.
k > nπ/b Let α =
p
k2 −(nπ/b)2. The homogeneous solutions that satisfy the radiation condition are
y1 = e−ıαx,
y2 = eıαx .
The Wronskian of the two solutions is ı2α. Thus the solution is
cn(x) = e−ıαx< eıαx>
ıbα
cos
nπη
b

.
In the case that cos
  nπη
b

= 0 this reduces to the trivial solution.
1803

k = nπ/b The homogeneous solutions that are bounded at inﬁnity are
y1 = 1,
y2 = 1.
If the right-hand-side is nonzero there is no way to combine these solutions to satisfy both the continuity and
the derivative jump conditions. Thus if cos
  nπη
b

̸= 0 there is no bounded solution. If cos
  nπη
b

= 0 then the
solution is not unique.
cn(x) = const.
k < nπ/b Let β =
p
(nπ/b)2 −k2. The homogeneous solutions that are bounded at inﬁnity are
y1 = eβx,
y2 = e−βx .
The Wronskian of these solutions is −2β. Thus the solution is
cn(x) = −eβx< e−βx>
bβ
cos
nπη
b

In the case that cos
  nπη
b

= 0 this reduces to the trivial solution.
1804

38.1
Exercises
Exercise 38.1
A slab is perfectly insulated at the surface x = 0 and has a speciﬁed time varying temperature f(t) at the surface
x = L. Initially the temperature is zero. Find the temperature u(x, t) if the heat conductivity in the slab is κ = 1.
Exercise 38.2
Solve
uxx + uyy = 0,
0 < x < L,
y > 0,
u(x, 0) = f(x),
u(0, y) = g(y),
u(L, y) = h(y),
with an eigenfunction expansion.
1805

38.2
Hints
Hint 38.1
Hint 38.2
1806

38.3
Solutions
Solution 38.1
The problem is
ut = uxx,
0 < x < L, t > 0,
ux(0, t) = 0,
u(L, t) = f(t),
u(x, 0) = 0.
We will solve this problem with an eigenfunction expansion. We ﬁnd these eigenfunction by replacing the inhomogeneous
boundary condition with the homogeneous one, u(L, t) = 0.
We substitute the separation of variables v(x, t) =
X(x)T(t) into the homogeneous partial diﬀerential equation.
XT ′ = X′′T
T ′
T = X′′
X = −λ2.
This gives us the regular Sturm-Liouville eigenvalue problem,
X′′ = −λ2X,
X′(0) = X(L) = 0,
which has the solutions,
λn = π(2n −1)
2L
,
Xn = cos
π(2n −1)x
2L

,
n ∈N.
Our solution for u(x, t) will be an eigenfunction expansion in these eigenfunctions. Since the inhomogeneous boundary
condition is a function of t, the coeﬃcients will be functions of t.
u(x, t) =
∞
X
n=1
an(t) cos(λnx)
Since u(x, t) does not satisfy the homogeneous boundary conditions of the eigenfunctions, the series is not uniformly
convergent and we are not allowed to diﬀerentiate it with respect to x. We substitute the expansion into the partial
1807

diﬀerential equation, multiply by the eigenfunction and integrate from x = 0 to x = L. We use integration by parts to
move derivatives from u to the eigenfunctions.
ut = uxx
Z L
0
ut cos(λmx) dx =
Z L
0
uxx cos(λmx) dx
Z L
0
 ∞
X
n=1
a′
n(t) cos(λnx)
!
cos(λmx) dx = [ux cos(λmx)]L
0 +
Z L
0
uxλm sin(λmx) dx
L
2 a′
m(t) = [uλm sin(λmx)]L
0 −
Z L
0
uλ2
m cos(λmx) dx
L
2 a′
m(t) = λmu(L, t) sin(λmL) −λ2
m
Z L
0
 ∞
X
n=1
an(t) cos(λnx)
!
cos(λmx) dx
L
2 a′
m(t) = λm(−1)nf(t) −λ2
m
L
2 am(t)
a′
m(t) + λ2
mam(t) = (−1)nλmf(t)
From the initial condition u(x, 0) = 0 we see that am(0) = 0. Thus we have a ﬁrst order diﬀerential equation and an
initial condition for each of the am(t).
a′
m(t) + λ2
mam(t) = (−1)nλmf(t),
am(0) = 0
This equation has the solution,
am(t) = (−1)nλm
Z t
0
e−λ2
m(t−τ) f(τ) dτ.
Solution 38.2
uxx + uyy = 0,
0 < x < L,
y > 0,
u(x, 0) = f(x),
u(0, y) = g(y),
u(L, y) = h(y),
1808

We seek a solution of the form,
u(x, y) =
∞
X
n=1
un(y) sin
nπx
L

.
Since we have inhomogeneous boundary conditions at x = 0, L, we cannot diﬀerentiate the series representation with
respect to x. We multiply Laplace’s equation by the eigenfunction and integrate from x = 0 to x = L.
Z L
0
(uxx + uyy) sin
mπx
L

dx = 0
We use integration by parts to move derivatives from u to the eigenfunctions.
h
ux sin
mπx
L
iL
0 −mπ
L
Z L
0
ux cos
mπx
L

dx + L
2 u′′
m(y) = 0
h
−mπ
L u cos
mπx
L
iL
0 −
mπ
L
2 Z L
0
u sin
mπx
L

dx + L
2 u′′
m(y) = 0
−mπ
L h(y)(−1)m + mπ
L g(y) −L
2
mπ
L
2
um(y) + L
2 u′′
m(y) = 0
u′′
m(y) −
mπ
L
2
um(y) = 2mπ ((−1)mh(y) −g(y))
Now we have an ordinary diﬀerential equation for the un(y). In order that the solution is bounded, we require that each
un(y) is bounded as y →∞. We use the boundary condition u(x, 0) = f(x) to determine boundary conditions for the
um(y) at y = 0.
u(x, 0) =
∞
X
n=1
un(0) sin
nπx
L

= f(x)
un(0) = fn ≡2
L
Z L
0
f(x) sin
nπx
L

dx
1809

Thus we have the problems,
u′′
n(y) −
nπ
L
2
un(y) = 2nπ ((−1)nh(y) −g(y)) ,
un(0) = fn,
un(+∞) bounded,
for the coeﬃcients in the expansion. We will solve these with Green functions. Consider the associated Green function
problem
G′′
n(y; η) −
nπ
L
2
Gn(y; η) = δ(y −η),
Gn(0; η) = 0,
Gn(+∞; η) bounded.
The homogeneous solutions that satisfy the boundary conditions are
sinh
nπy
L

and
e−nπy/L,
respectively. The Wronskian of these solutions is

sinh
  nπy
L

e−nπy/L
nπ
L sinh
  nπy
L

−nπ
L e−nπy/L
 = −nπ
L e−2nπy/L .
Thus the Green function is
Gn(y; η) = −L sinh
  nπy<
L
 e−nπy>/L
nπ e−2nπη/L
.
Using the Green function we determine the un(y) and thus the solution of Laplace’s equation.
un(y) = fn e−nπy/L +2nπ
Z ∞
0
Gn(y; η) ((−1)nh(η) −g(η)) dη
u(x, y) =
∞
X
n=1
un(y) sin
nπx
L

.
1810

Chapter 39
The Diﬀusion Equation
1811

39.1
Exercises
Exercise 39.1
Is the solution of the Cauchy problem for the heat equation unique?
ut −κuxx = q(x, t),
−∞< x < ∞,
t > 0
u(x, 0) = f(x)
Exercise 39.2
Consider the heat equation with a time-independent source term and inhomogeneous boundary conditions.
ut = κuxx + q(x)
u(0, t) = a,
u(h, t) = b,
u(x, 0) = f(x)
Exercise 39.3
Is the Cauchy problem for the backward heat equation
ut + κuxx = 0,
u(x, 0) = f(x)
(39.1)
well posed?
Exercise 39.4
Derive the heat equation for a general 3 dimensional body, with non-uniform density ρ(x), speciﬁc heat c(x), and
conductivity k(x). Show that
∂u(x, t)
∂t
= 1
cρ∇· (k∇u(x, t))
where u is the temperature, and you may assume there are no internal sources or sinks.
Exercise 39.5
Verify Duhamel’s Principal: If u(x, t, τ) is the solution of the initial value problem:
ut = κuxx,
u(x, 0, τ) = f(x, τ),
1812

then the solution of
wt = κwxx + f(x, t),
w(x, 0) = 0
is
w(x, t) =
Z t
0
u(x, t −τ, τ) dτ.
Exercise 39.6
Modify the derivation of the diﬀusion equation
φt = a2φxx,
a2 = k
cρ,
(39.2)
so that it is valid for diﬀusion in a non-homogeneous medium for which c and k are functions of x and φ and so that
it is valid for a geometry in which A is a function of x. Show that Equation (39.2) above is in this case replaced by
cρAφt = (kAφx)x .
Recall that c is the speciﬁc heat, k is the thermal conductivity, ρ is the density, φ is the temperature and A is the
cross-sectional area.
1813

39.2
Hints
Hint 39.1
Hint 39.2
Hint 39.3
Hint 39.4
Hint 39.5
Check that the expression for w(x, t) satisﬁes the partial diﬀerential equation and initial condition. Recall that
∂
∂x
Z x
a
h(x, ξ) dξ =
Z x
a
hx(x, ξ) dξ + h(x, x).
Hint 39.6
1814

39.3
Solutions
Solution 39.1
Let u and v both be solutions of the Cauchy problem for the heat equation. Let w be the diﬀerence of these solutions.
w satisﬁes the problem
wt −κwxx = 0,
−∞< x < ∞,
t > 0,
w(x, 0) = 0.
We can solve this problem with the Fourier transform.
ˆwt + κω2 ˆw = 0,
ˆw(ω, 0) = 0
ˆw = 0
w = 0
Since u −v = 0, we conclude that the solution of the Cauchy problem for the heat equation is unique.
Solution 39.2
Let µ(x) be the equilibrium temperature. It satisﬁes an ordinary diﬀerential equation boundary value problem.
µ′′ = −q(x)
κ ,
µ(0) = a,
µ(h) = b
To solve this boundary value problem we ﬁnd a particular solution µp that satisﬁes homogeneous boundary conditions
and then add on a homogeneous solution µh that satisﬁes the inhomogeneous boundary conditions.
µ′′
p = −q(x)
κ ,
µp(0) = µp(h) = 0
µ′′
h = 0,
µh(0) = a,
µh(h) = b
We ﬁnd the particular solution µp with the method of Green functions.
G′′ = δ(x −ξ),
G(0|ξ) = G(h|ξ) = 0.
1815

We ﬁnd homogeneous solutions which respectively satisfy the left and right homogeneous boundary conditions.
y1 = x,
y2 = h −x
Then we compute the Wronskian of these solutions and write down the Green function.
W =

x
h −x
1
−1
 = −h
G(x|ξ) = −1
hx< (h −x>)
The homogeneous solution that satisﬁes the inhomogeneous boundary conditions is
µh = a + b −a
h
x
Now we have the equilibrium temperature.
µ = a + b −a
h
x +
Z h
0
−1
hx< (h −x>)

−q(ξ)
κ

dξ
µ = a + b −a
h
x + h −x
hκ
Z x
0
ξq(ξ) dξ + x
hκ
Z h
x
(h −ξ)q(ξ) dξ
Let v denote the deviation from the equilibrium temperature.
u = µ + v
v satisﬁes a heat equation with homogeneous boundary conditions and no source term.
vt = κvxx,
v(0, t) = v(h, t) = 0,
v(x, 0) = f(x) −µ(x)
We solve the problem for v with separation of variables.
v = X(x)T(t)
XT ′ = κX′′T
T ′
κT = X′′
X = −λ
1816

We have a regular Sturm-Liouville problem for X and a diﬀerential equation for T.
X′′ + λX = 0,
X(0) = X(λ) = 0
λn =
nπ
h
2
,
Xn = sin
nπx
h

,
n ∈Z+
T ′ = −λκT
Tn = exp

−κ
nπ
h
2
t

v is a linear combination of the eigensolutions.
v =
∞
X
n=1
vn sin
nπx
h

exp

−κ
nπ
h
2
t

The coeﬃcients are determined from the initial condition, v(x, 0) = f(x) −µ(x).
vn = 2
h
Z h
0
(f(x) −µ(x)) sin
nπx
h

dx
We have determined the solution of the original problem in terms of the equilibrium temperature and the deviation
from the equilibrium. u = µ + v.
Solution 39.3
A problem is well posed if there exists a unique solution that depends continiously on the nonhomogeneous data.
First we ﬁnd some solutions of the diﬀerential equation with the separation of variables u = X(x)T(t).
ut + κuxx = 0,
κ > 0
XT ′ + κX′′T = 0
T ′
κT = −X′′
X = λ
X′′ + λX = 0,
T ′ = λκT
u = cos
√
λx

eλκt,
u = sin
√
λx

eλκt
1817

Note that
u = ϵ cos
√
λx

eλκt
satisﬁes the Cauchy problem
ut + κuxx = 0,
u(x, 0) = ϵ cos
√
λx

Consider ϵ ≪1. The initial condition is small, it satisﬁes |u(x, 0)| < ϵ. However the solution for any positive time
can be made arbitrarily large by choosing a suﬃciently large, positive value of λ. We can make the solution exceed the
value M at time t by choosing a value of λ such that
ϵ eλκt > M
λ > 1
κt ln
M
ϵ

.
Thus we see that Equation 39.1 is ill posed because the solution does not depend continuously on the initial data. A
small change in the initial condition can produce an arbitrarily large change in the solution for any ﬁxed time.
Solution 39.4
Consider a Region of material, R. Let u be the temperature and φ be the heat ﬂux. The amount of heat energy in
the region is
Z
R
cρu dx.
We equate the rate of change of heat energy in the region with the heat ﬂux across the boundary of the region.
d
dt
Z
R
cρu dx = −
Z
∂R
φ · n ds
We apply the divergence theorem to change the surface integral to a volume integral.
d
dt
Z
R
cρu dx = −
Z
R
∇· φ dx
Z
R

cρ∂u
∂t + ∇· φ

dx = 0
1818

Since the region is arbitrary, the integral must vanish identically.
cρ∂u
∂t = −∇· φ
We apply Fourier’s law of heat conduction, φ = −k∇u, to obtain the heat equation.
∂u
∂t = 1
cρ∇· (k∇u)
Solution 39.5
We verify Duhamel’s principal by showing that the integral expression for w(x, t) satisﬁes the partial diﬀerential equation
and the initial condition. Clearly the initial condition is satisﬁed.
w(x, 0) =
Z 0
0
u(x, 0 −τ, τ) dτ = 0
Now we substitute the expression for w(x, t) into the partial diﬀerential equation.
∂
∂t
Z t
0
u(x, t −τ, τ) dτ = κ ∂2
∂x2
Z t
0
u(x, t −τ, τ) dτ + f(x, t)
u(x, t −t, t) +
Z t
0
ut(x, t −τ, τ) dτ = κ
Z t
0
uxx(x, t −τ, τ) dτ + f(x, t)
f(x, t) +
Z t
0
ut(x, t −τ, τ) dτ = κ
Z t
0
uxx(x, t −τ, τ) dτ + f(x, t)
Z t
0
(ut(x, t −τ, τ) dτ −κuxx(x, t −τ, τ)) dτ
Since ut(x, t −τ, τ) dτ −κuxx(x, t −τ, τ) = 0, this equation is an identity.
1819

Solution 39.6
We equate the rate of change of thermal energy in the segment (α . . . β) with the heat entering the segment through
the endpoints.
Z β
α
φtcρA dx = k(β, φ(β))A(β)φx(β, t) −k(α, φ(α))A(α)φx(α, t)
Z β
α
φtcρA dx = [kAφx]β
α
Z β
α
φtcρA dx =
Z β
α
(kAφx)x dx
Z β
α
cρAφt −(kAφx)x dx = 0
Since the domain is arbitrary, we conclude that
cρAφt = (kAφx)x .
1820

Chapter 40
Laplace’s Equation
40.1
Introduction
Laplace’s equation in n dimensions is
∆u = 0
where
∆= ∂2
∂x2
1
+ · · · + ∂2
∂x2
n
.
The inhomogeneous analog is called Poisson’s Equation.
−∆u = f(x)
CONTINUE
40.2
Fundamental Solution
The fundamental solution of Poisson’s equation in Rn satisﬁes
−∆G = δ(x −ξ).
1821

40.2.1
Two Dimensional Space
If n = 2 then the fundamental solution satisﬁes
−
 ∂2
∂x2 + ∂2
∂y2

G = δ(x −ξ)δ(y −ψ).
Since the product of delta functions, δ(x −ξ)δ(y −ψ) is circularly symmetric about the point (ξ, ψ), we look for a
solution in the form u(x, y) = v(r) where r =
p
((x −ξ)2 + (y −ψ)2).
CONTINUE
1822

40.3
Exercises
Exercise 40.1
Is the solution of the following Dirichlet problem unique?
uxx + uyy = q(x, y),
−∞< x < ∞,
y > 0
u(x, 0) = f(x)
Exercise 40.2
Is the solution of the following Dirichlet problem unique?
uxx + uyy = q(x, y),
−∞< x < ∞,
y > 0
u(x, 0) = f(x),
u bounded as x2 + y2 →∞
Exercise 40.3
Not all combinations of boundary conditions/initial conditions lead to so called well-posed problems. Essentially, a well
posed problem is one where the solutions depend continuously on the boundary data. Otherwise it is considered “ill
posed”.
Consider Laplace’s equation on the unit-square
uxx + uyy = 0,
with u(0, y) = u(1, y) = 0 and u(x, 0) = 0, uy(x, 0) = ϵ sin(nπx).
1. Show that even as ϵ →0, you can ﬁnd n so that the solution can attain any ﬁnite value for any y > 0. Use this
to then show that this problem is ill posed.
2. Contrast this with the case where u(0, y) = u(1, y) = 0 and u(x, 0) = 0, u(x, 1) = ϵ sin(nπx). Is this well
posed?
Exercise 40.4
Use the fundamental solutions for the Laplace equation
∇2G = δ(x −ξ)
1823

in three dimensions
G(x|ξ) = −
1
4π|x −ξ|
to derive the mean value theorem for harmonic functions
u(p) =
1
4πR2
Z
∂SR
u(ξ) dAξ,
that relates the value of any harmonic function u(x) at the point x = p to the average of its value on the boundary
of the sphere of radius R with center at p, (∂SR).
Exercise 40.5
Use the fundamental solutions for the modiﬁed Helmholz equation
∇2u −λu = δ(x −ξ)
in three dimensions
u±(x|ξ) =
−1
4π|x −ξ| e±
√
λ|x−ξ|,
to derive a “generalized” mean value theorem:
sinh
√
λR

√
λR
u(p) =
1
4πR2
Z
∂S
u(x) dA
that relates the value of any solution u(x) at a point P to the average of its value on the sphere of radius R (∂S) with
center at P.
Exercise 40.6
Consider the uniqueness of solutions of ∇2u(x) = 0 in a two dimensional region R with boundary curve C and a
boundary condition n · ∇u(x) = −a(x)u(x) on C. State a non-trivial condition on the function a(x) on C for which
solutions are unique, and justify your answer.
1824

Exercise 40.7
Solve Laplace’s equation on the surface of a semi-inﬁnite cylinder of unit radius, 0 < θ < 2π, z > 0, where the solution,
u(θ, z) is prescribed at z = 0: u(θ, 0) = f(θ).
Exercise 40.8
Solve Laplace’s equation in a rectangle.
wxx + wyy = 0,
0 < x < a,
0 < y < b,
w(0, y) = f1(y),
w(a, y) = f2(y),
wy(x, 0) = g1(x),
w(x, b) = g2(x)
Proceed by considering w = u + v where u and v are harmonic and satisfy
u(0, y) = u(a, y) = 0,
uy(x, 0) = g1(x),
u(x, b) = g2(x),
v(0, y) = f1(y),
v(a, y) = f2(y),
vy(x, 0) = v(x, b) = 0.
1825

40.4
Hints
Hint 40.1
Hint 40.2
Hint 40.3
Hint 40.4
Hint 40.5
Hint 40.6
Hint 40.7
Hint 40.8
1826

40.5
Solutions
Solution 40.1
Let u and v both be solutions of the Dirichlet problem. Let w be the diﬀerence of these solutions. w satisﬁes the
problem
wxx + wyy = 0,
−∞< x < ∞,
y > 0
w(x, 0) = 0.
Since w = cy is a solution. We conclude that the solution of the Dirichlet problem is not unique.
Solution 40.2
Let u and v both be solutions of the Dirichlet problem. Let w be the diﬀerence of these solutions. w satisﬁes the
problem
wxx + wyy = 0,
−∞< x < ∞,
y > 0
w(x, 0) = 0,
w bounded as x2 + y2 →∞.
We solve this problem with a Fourier transform in x.
−ω2 ˆw + ˆwyy = 0,
ˆw(ω, 0) = 0,
ˆw bounded as y →∞
ˆw =
(
c1 cosh ωy + c2 sinh(ωy),
ω ̸= 0
c1 + c2y,
ω = 0
ˆw = 0
w = 0
Since u −v = 0, we conclude that the solution of the Dirichlet problem is unique.
1827

Solution 40.3
1. We seek a solution of the form u(x, y) = sin(nπx)Y (y). This form satisﬁes the boundary conditions at x = 0, 1.
uxx + uyy = 0
−(nπ)2Y + Y ′′ = 0,
Y (0) = 0
Y = c sinh(nπy)
Now we apply the inhomogeneous boundary condition.
uy(x, 0) = ϵ sin(nπx) = cnπ sin(nπx)
u(x, y) = ϵ
nπ sin(nπx) sinh(nπy)
For ϵ = 0 the solution is u = 0. Now consider any ϵ > 0. For any y > 0 and any ﬁnite value M, we can choose
a value of n such that the solution along y = 0 takes on all values in the range [−M . . . M]. We merely choose
a value of n such that
sinh(nπy)
nπ
≥M
ϵ .
Since the solution does not depend continuously on boundary data, this problem is ill posed.
2. We seek a solution of the form u(x, y) = c sin(nπx) sinh(nπy). This form satisﬁes the diﬀerential equation and
the boundary conditions at x = 0, 1 and at y = 0. We apply the inhomogeneous boundary condition at y = 1.
u(x, 1) = ϵ sin(nπx) = c sin(nπx) sinh(nπ)
u(x, y) = ϵ sin(nπx)sinh(nπy)
sinh(nπ)
For ϵ = 0 the solution is u = 0. Now consider any ϵ > 0. Note that |u| ≤ϵ for (x, y) ∈[0 . . . 1] × [0 . . . 1]. The
solution depends continuously on the given boundary data. This problem is well posed.
Solution 40.4
The Green function problem for a sphere of radius R centered at the point ξ is
∆G = δ(x −ξ),
G

|x−ξ|=R = 0.
(40.1)
1828

We will solve Laplace’s equation, ∆u = 0, where the value of u is known on the boundary of the sphere of radius R in
terms of this Green function.
First we solve for u(x) in terms of the Green function.
Z
S
(u∆G −G∆u) dξ =
Z
S
uδ(x −ξ) dξ = u(x)
Z
S
(u∆G −G∆u) dξ =
Z
∂S

u∂G
∂n −G∂u
∂n

dAξ
=
Z
∂S
u∂G
∂n dAξ
u(x) =
Z
∂S
u∂G
∂n dAξ
We are interested in the value of u at the center of the sphere. Let ρ = |p −ξ|
u(p) =
Z
∂S
u(ξ)∂G
∂ρ (p|ξ) dAξ
We do not need to compute the general solution of Equation 40.1. We only need the Green function at the point
x = p. We know that the general solution of the equation ∆G = δ(x −ξ) is
G(x|ξ) = −
1
4π|x −ξ| + v(x),
where v(x) is an arbitrary harmonic function. The Green function at the point x = p is
G(p|ξ) = −
1
4π|p −ξ| + const.
1829

We add the constraint that the Green function vanishes at ρ = R. This determines the constant.
G(p|ξ) = −
1
4π|p −ξ| +
1
4πR
G(p|ξ) = −1
4πρ +
1
4πR
Gρ(p|ξ) =
1
4πρ2
Now we are prepared to write u(p) in terms of the Green function.
u(p) =
Z
∂S
u(ξ)
1
4πρ2 dAξ
u(p) =
1
4πR2
Z
∂S
u(ξ) dAξ
This is the Mean Value Theorem for harmonic functions.
Solution 40.5
The Green function problem for a sphere of radius R centered at the point ξ is
∆G −λG = δ(x −ξ),
G

|x−ξ|=R = 0.
(40.2)
We will solve the modiﬁed Helmholtz equation,
∆u −λu = 0,
where the value of u is known on the boundary of the sphere of radius R in terms of this Green function.
in terms of this Green function.
Let L[u] = ∆u −λu.
Z
S
(uL[G] −GL[u]) dξ =
Z
S
uδ(x −ξ) dξ = u(x)
1830

Z
S
(uL[G] −GL[u]) dξ =
Z
S
(u∆G −G∆u) dξ
=
Z
∂S

u∂G
∂n −G∂u
∂n

dAξ
=
Z
∂S
u∂G
∂n dAξ
u(x) =
Z
∂S
u∂G
∂n dAξ
We are interested in the value of u at the center of the sphere. Let ρ = |p −ξ|
u(p) =
Z
∂S
u(ξ)∂G
∂ρ (p|ξ) dAξ
We do not need to compute the general solution of Equation 40.2. We only need the Green function at the point
x = p. We know that the Green function there is a linear combination of the fundamental solutions,
G(p|ξ) = c1
−1
4π|p −ξ| e
√
λ|p−ξ| +c2
−1
4π|p −ξ| e−
√
λ|p−ξ|,
such that c1 +c2 = 1. The Green function is symmetric with respect to x and ξ. We add the constraint that the Green
1831

function vanishes at ρ = R. This gives us two equations for c1 and c2.
c1 + c2 = 1,
−c1
4πR e
√
λR −c2
4πR e−
√
λR = 0
c1 = −
1
e2
√
λR −1
,
c2 =
e2
√
λR
e2
√
λR −1
G(p|ξ) =
sinh
√
λ(ρ −R)

4πρ sinh
√
λR

Gρ(p|ξ) =
√
λ cosh
√
λ(ρ −R)

4πρ sinh
√
λR

−
sinh
√
λ(ρ −R)

4πρ2 sinh
√
λR

Gρ(p|ξ)

|ξ|=R =
√
λ
4πR sinh
√
λR

Now we are prepared to write u(p) in terms of the Green function.
u(p) =
Z
∂S
u(ξ)
√
λ
4πρ sinh
√
λR
 dAξ
u(p) =
Z
∂S
u(x)
√
λ
4πR sinh
√
λR
 dA
Rearranging this formula gives us the generalized mean value theorem.
sinh
√
λR

√
λR
u(p) =
1
4πR2
Z
∂S
u(x) dA
1832

Solution 40.6
First we think of this problem in terms of the the equilibrium solution of the heat equation. The boundary condition
expresses Newton’s law of cooling. Where a = 0, the boundary is insulated. Where a > 0, the rate of heat loss is
proportional to the temperature. The case a < 0 is non-physical and we do not consider this scenario further. We
know that if the boundary is entirely insulated, a = 0, then the equilibrium temperature is a constant that depends on
the initial temperature distribution. Thus for a = 0 the solution of Laplace’s equation is not unique. If there is any
point on the boundary where a is positive then eventually, all of the heat will ﬂow out of the domain. The equilibrium
temperature is zero, and the solution of Laplace’s equation is unique, u = 0. Therefore the solution of Laplace’s
equation is unique if a is continuous, non-negative and not identically zero.
Now we prove our assertion. First note that if we substitute f = v∇u in the divergence theorem,
Z
R
∇· f dx =
Z
∂R
f · n ds,
we obtain the identity,
Z
R
(v∆u + ∇v∇u) dx =
Z
∂R
v∂u
∂n ds.
(40.3)
Let u be a solution of Laplace’s equation subject to the Robin boundary condition with our restrictions on a. We take
v = u in Equation 40.3.
Z
R
(∇u)2 dx =
Z
C
u∂u
∂n ds = −
Z
C
au2 ds
Since the ﬁrst integral is non-negative and the last is non-positive, the integrals vanish. This implies that ∇u = 0. u is
a constant. In order to satisfy the boundary condition where a is non-zero, u must be zero. Thus the unique solution
in this scenario is u = 0.
Solution 40.7
The mathematical statement of the problem is
∆u ≡uθθ + uzz = 0,
0 < θ < 2π,
z > 0,
u(θ, 0) = f(θ).
1833

We have the implicit boundary conditions,
u(0, z) = u(2π, z),
uθ(0, z) = uθ(0, z)
and the boundedness condition,
u(θ, +∞) bounded.
We expand the solution in a Fourier series. (This ensures that the boundary conditions at θ = 0, 2π are satisﬁed.)
u(θ, z) =
∞
X
n=−∞
un(z) eınθ
We substitute the series into the partial diﬀerential equation to obtain ordinary diﬀerential equations for the un.
−n2un(z) + u′′
n(z) = 0
The general solutions of this equation are
un(z) =
(
c1 + c2z,
for n = 0,
c1 enz +c2 e−nz
for n ̸= 0.
The bounded solutions are
un(z) =





c e−nz,
for n > 0,
c,
for n = 0,
c enz,
for n < 0,
= c e−|n|z .
We substitute the series into the initial condition at z = 0 to determine the multiplicative constants.
u(θ, 0) =
∞
X
n=−∞
un(0) eınθ = f(θ)
un(0) = 1
2π
Z 2π
0
f(θ) e−ınθ dθ ≡fn
1834

Thus the solution is
u(θ, z) =
∞
X
n=−∞
fn eınθ e−|n|z .
Note that
u(θ, z) →f0 = 1
2π
Z 2π
0
f(θ) dθ
as z →+∞.
Solution 40.8
The decomposition of the problem is shown in Figure 40.1.
w =g (x)
w=0
∆
w=g (x)
w=f (y)
w=f (y)
2
1
2
y
1
u =g (x)
u=0
∆
u=g (x)
u=0
v=0
∆
v=f (y)
v=f (y)
=
+
u=0
v=0
v =0
2
y
1
1
2
y
Figure 40.1: Decomposition of the problem.
First we solve the problem for u.
uxx + uyy = 0,
0 < x < a,
0 < y < b,
u(0, y) = u(a, y) = 0,
uy(x, 0) = g1(x),
u(x, b) = g2(x)
We substitute the separation of variables u(x, y) = X(x)Y (y) into Laplace’s equation.
X′′
X = −Y ′′
Y
= −λ2
1835

We have the eigenvalue problem,
X′′ = −λ2X,
X(0) = X(a) = 0,
which has the solutions,
λn = nπ
a ,
Xn = sin
nπx
a

,
n ∈N.
The equation for Y (y) becomes,
Y ′′
n =
nπ
a
2
Yn,
which has the solutions,
enπy/a, e−nπy/a	
or
n
cosh
nπy
a

, sinh
nπy
a
o
.
It will be convenient to choose solutions that satisfy the conditions, Y (b) = 0 and Y ′(0) = 0, respectively.

sinh
nπ(b −y)
a

, cosh
nπy
a

The solution for u(x, y) has the form,
u(x, y) =
∞
X
n=1
sin
nπx
a
 
αn sinh
nπ(b −y)
a

+ βn cosh
nπy
a

.
We determine the coeﬃcients from the inhomogeneous boundary conditions. (Here we see how our choice of solutions
1836

for Y (y) is convenient.)
uy(x, 0) =
∞
X
n=1
−nπ
a αn sin
nπx
a

cosh
nπb
a

= g1(x)
αn = −a
nπ sech
nπb
a
 2
a
Z a
0
g1(x) sin
nπx
a

dx
u(x, y) =
∞
X
n=1
βn sin
nπx
a

cosh
nπy
a

βn = sech
nπb
a
 2
a
Z a
0
g2(x) sin
nπx
a

dx
Now we solve the problem for v.
vxx + vyy = 0,
0 < x < a,
0 < y < b,
v(0, y) = f1(y),
v(a, y) = f2(y),
vy(x, 0) = 0,
v(x, b) = 0
We substitute the separation of variables u(x, y) = X(x)Y (y) into Laplace’s equation.
X′′
X = −Y ′′
Y
= λ2
We have the eigenvalue problem,
Y ′′ = −λ2Y,
Y ′(0) = Y (b) = 0,
which has the solutions,
λn = (2n −1)π
2b
,
Yn = cos
(2n −1)πy
2b

,
n ∈N.
The equation for X(y) becomes,
X′′
n =
(2n −1)π
2b
2
Xn.
1837

We choose solutions that satisfy the conditions, X(a) = 0 and X(0) = 0, respectively.

sinh
(2n −1)π(a −x)
2b

, sinh
(2n −1)πx
2b

The solution for v(x, y) has the form,
v(x, y) =
∞
X
n=1
cos
(2n −1)πy
2b
 
γn sinh
(2n −1)π(a −x)
2b

+ δn sinh
(2n −1)πx
2b

.
We determine the coeﬃcients from the inhomogeneous boundary conditions.
v(0, y) =
∞
X
n=1
γn cos
(2n −1)πy
2b

sinh
(2n −1)πa
2b

= f1(y)
γn = csch
(2n −1)πa
2b
 2
b
Z b
0
f1(y) cos
(2n −1)πy
2b

dy
v(a, y) =
∞
X
n=1
δn cos
(2n −1)πy
2b

sinh
(2n −1)πa
2b

= f2(y)
δn = csch
(2n −1)πa
2b
 2
b
Z b
0
f2(y) cos
(2n −1)πy
2b

dy
With u and v determined, the solution of the original problem is w = u + v.
1838

Chapter 41
Waves
1839

41.1
Exercises
Exercise 41.1
Consider the 1-D wave equation
utt −uxx = 0
on the domain 0 < x < 4 with initial displacement
u(x, 0) =
(
1,
1 < x < 2
0,
otherwise,
initial velocity ut(x, 0) = 0, and subject to the following boundary conditions
1.
u(0, t) = u(4, t) = 0
2.
ux(0, t) = ux(4, t) = 0
In each case plot u(x, t) for t = 1
2, 1, 3
2, 2 and combine onto a general plot in the x, t plane (up to a suﬃciently large
time) so the behavior of u is clear for arbitrary x, t.
Exercise 41.2
Sketch the solution to the wave equation:
u(x, t) = 1
2 (u(x + ct, 0) + u(x −ct, 0)) + 1
2c
Z x+ct
x−ct
ut(τ, 0) dτ,
for various values of t corresponding to the initial conditions:
1. u(x, 0) = 0,
ut(x, 0) = sin ωx
where ω is a constant,
1840

2. u(x, 0) = 0,
ut(x, 0) =





1
for 0 < x < 1
−1
for −1 < x < 0
0
for |x| > 1.
Exercise 41.3
1. Consider the solution of the wave equation for u(x, t):
utt = c2uxx
on the inﬁnite interval −∞< x < ∞with initial displacement of the form
u(x, 0) =
(
h(x)
for x > 0,
−h(−x)
for x < 0,
and with initial velocity
ut(x, 0) = 0.
Show that the solution of the wave equation satisfying these initial conditions also solves the following semi-inﬁnite
problem: Find u(x, t) satisfying the wave equation utt = c2uxx in 0 < x < ∞, t > 0, with initial conditions
u(x, 0) = h(x), ut(x, 0) = 0, and with the ﬁxed end condition u(0, t) = 0. Here h(x) is any given function with
h(0) = 0.
2. Use a similar idea to explain how you could use the general solution of the wave equation to solve the ﬁnite
interval problem (0 < x < l) in which u(0, t) = u(l, t) = 0 for all t, with u(x, 0) = h(x) and ut(x, 0) = 0. Take
h(0) = h(l) = 0.
Exercise 41.4
The deﬂection u(x, T) = φ(x) and velocity ut(x, T) = ψ(x) for an inﬁnite string (governed by utt = c2uxx) are
measured at time T, and we are asked to determine what the initial displacement and velocity proﬁles u(x, 0) and
ut(x, 0) must have been. An alert student suggests that this problem is equivalent to that of determining the solution
of the wave equation at time T when initial conditions u(x, 0) = φ(x), ut(x, 0) = −ψ(x) are prescribed. Is she correct?
If not, can you rescue her idea?
1841

Exercise 41.5
In obtaining the general solution of the wave equation the interval was chosen to be inﬁnite in order to simplify the
evaluation of the functions α(ξ) and β(ξ) in the general solution
u(x, t) = α(x + ct) + β(x −ct).
But this general solution is in fact valid for any interval be it inﬁnite or ﬁnite. We need only choose appropriate functions
α(ξ), β(ξ) to satisfy the appropriate initial and boundary conditions. This is not always convenient but there are other
situations besides the solution for u(x, t) in an inﬁnite domain in which the general solution is of use. Consider the
“whip-cracking” problem,
utt = c2uxx,
(with c a constant) in the domain x > 0, t > 0 with initial conditions
u(x, 0) = ut(x, 0) = 0
x > 0,
and boundary conditions
u(0, t) = γ(t)
prescribed for all t > 0. Here γ(0) = 0. Find α and β so as to determine u for x > 0, t > 0.
Hint:
(From physical considerations conclude that you can take α(ξ) = 0. Your solution will corroborate this.)
Use the initial conditions to determine α(ξ) and β(ξ) for ξ > 0. Then use the initial condition to determine β(ξ) for
ξ < 0.
Exercise 41.6
Let u(x, t) satisfy the equation
utt = c2uxx;
(with c a constant) in some region of the (x, t) plane.
1. Show that the quantity (ut −cux) is constant along each straight line deﬁned by x −ct = constant, and that
(ut + cux) is constant along each straight line of the form x + ct = constant. These straight lines are called
characteristics; we will refer to typical members of the two families as C+ and C−characteristics, respectively.
Thus the line x −ct = constant is a C+ characteristic.
1842

2. Let u(x, 0) and ut(x, 0) be prescribed for all values of x in −∞< x < ∞, and let (x0, t0) be some point in the
(x, t) plane, with t0 > 0. Draw the C+ and C−characteristics through (x0, t0) and let them intersect the x-axis
at the points A,B. Use the properties of these curves derived in part (a) to determine ut(x0, t0) in terms of initial
data at points A and B. Using a similar technique to obtain ut(x0, τ) with 0 < τ < t, determine u(x0, t0) by
integration with respect to τ, and compare this with the solution derived in class:
u(x, t) = 1
2 (u(x + ct, 0) + u(x −ct, 0)) + 1
2c
Z x+ct
x−ct
ut(τ, 0)dτ.
Observe that this “method of characteristics” again shows that u(x0, t0) depends only on that part of the initial
data between points A and B.
Exercise 41.7
The temperature u(x, t) at a depth x below the Earth’s surface at time t satisﬁes
ut = κuxx.
The surface x = 0 is heated by the sun according to the periodic rule:
u(0, t) = T cos(ωt).
Seek a solution of the form
u(x, t) = ℜ
 A eıωt−αx
.
a) Find u(x, t) satisfying u →0 as x →+∞, (i.e. deep into the Earth).
b) Find the temperature variation at a ﬁxed depth, h, below the surface.
c) Find the phase lag δ(x) such that when the maximum temperature occurs at t0 on the surface, the maximum at
depth x occurs at t0 + δ(x).
d) Show that the seasonal, (i.e. yearly), temperature changes and daily temperature changes penetrate to depths in
the ratio:
xyear
xday
=
√
365,
where xyear and xday are the depths of same temperature variation caused by the diﬀerent periods of the source.
1843

Exercise 41.8
An inﬁnite cylinder of radius a produces an external acoustic pressure ﬁeld u satisfying:
utt = c2δu,
by a pure harmonic oscillation of its surface at r = a. That is, it moves so that
u(a, θ, t) = f(θ) eıωt
where f(θ) is a known function. Note that the waves must be outgoing at inﬁnity, (radiation condition at inﬁnity).
Find the solution, u(r, θ, t). We seek a periodic solution of the form,
u(r, θ, t) = v(r, θ) eıωt .
Exercise 41.9
Plane waves are incident on a “soft” cylinder of radius a whose axis is parallel to the plane of the waves. Find the
ﬁeld scattered by the cylinder. In particular, examine the leading term of the solution when a is much smaller than the
wavelength of the incident waves. If v(x, y, t) is the scattered ﬁeld it must satisfy:
Wave Equation:
vtt = c2∆v,
x2 + y2 > a2;
Soft Cylinder:
v(x, y, t) = −eı(ka cos θ−ωt), on r = a,
0 ≤θ < 2π;
Scattered:
v is outgoing as r →∞.
Here k = ω/c. Use polar coordinates in the (x, y) plane.
Exercise 41.10
Consider the ﬂow of electricity in a transmission line. The current, I(x, t), and the voltage, V (x, t), obey the telegra-
pher’s system of equations:
−Ix = CVt + GV,
−Vx = LIt + RI,
1844

where C is the capacitance, G is the conductance, L is the inductance and R is the resistance.
a) Show that both I and V satisfy a damped wave equation.
b) Find the relationship between the physical constants, C, G, L and R such that there exist damped traveling
wave solutions of the form:
V (x, t) = e−γt(f(x −at) + g(x + at)).
What is the wave speed?
1845

41.2
Hints
Hint 41.1
Hint 41.2
Hint 41.3
Hint 41.4
Hint 41.5
From physical considerations conclude that you can take α(ξ) = 0. Your solution will corroborate this. Use the initial
conditions to determine α(ξ) and β(ξ) for ξ > 0. Then use the initial condition to determine β(ξ) for ξ < 0.
Hint 41.6
Hint 41.7
a) Substitute u(x, t) = ℜ(A eıωt−αx) into the partial diﬀerential equation and solve for α. Assume that α has
positive real part so that the solution vanishes as x →+∞.
Hint 41.8
Seek a periodic solution of the form,
u(r, θ, t) = v(r, θ) eıωt .
1846

Solve the Helmholtz equation for v with a Fourier series expansion,
v(r, θ) =
∞
X
n=−∞
vn(r) eınθ .
You will ﬁnd that the vn satisfy Bessel’s equation. Choose the vn so that u satisﬁes the boundary condition at r = a
and the radiation condition at inﬁnity.
The Bessel functions have the asymptotic behavior,
Jn(ρ) ∼
r 2
πρ cos(ρ −nπ/2 −π/4),
as ρ →∞,
Yn(ρ) ∼
r 2
πρ sin(ρ −nπ/2 −π/4),
as ρ →∞,
H(1)
n (ρ) ∼
r 2
πρ ei(ρ−nπ/2−π/4),
as ρ →∞,
H(2)
n (ρ) ∼
r 2
πρ e−i(ρ−nπ/2−π/4),
as ρ →∞.
Hint 41.9
Hint 41.10
1847

41.3
Solutions
Solution 41.1
1. The initial position is
u(x, 0) = H
1
2 −
x −3
2


.
We extend the domain of the problem to (−∞. . . ∞) and add image sources in the initial condition so that
u(x, 0) is odd about x = 0 and x = 4. This enforces the boundary conditions at these two points.
utt −uxx = 0,
x ∈(−∞. . . ∞),
t ∈(0 . . . ∞)
u(x, 0) =
∞
X
n=−∞

H
1
2 −
x −3
2 −8n


−H
1
2 −
x −13
2 −8n


,
ut(x, 0) = 0
We use D’Alembert’s solution to solve this problem.
u(x, t) = 1
2
∞
X
n=−∞

H
1
2 −
x −3
2 −8n −t


+ H
1
2 −
x −3
2 −8n + t


−H
1
2 −
x −13
2 −8n −t


−H
1
2 −
x −13
2 −8n + t


The solution for several times is plotted in Figure 41.1. Note that the solution is periodic in time with period 8.
Figure 41.3 shows the solution in the phase plane for 0 < t < 8. Note the odd reﬂections at the boundaries.
2. The initial position is
u(x, 0) = H
1
2 −
x −3
2


.
We extend the domain of the problem to (−∞. . . ∞) and add image sources in the initial condition so that
1848

1
2
3
4
-0.4
-0.2
0.2
0.4
1
2
3
4
-0.4
-0.2
0.2
0.4
1
2
3
4
-0.4
-0.2
0.2
0.4
1
2
3
4
-0.4
-0.2
0.2
0.4
Figure 41.1: The solution at t = 1/2, 1, 3/2, 2 for the boundary conditions u(0, t) = u(4, t) = 0.
u(x, 0) is even about x = 0 and x = 4. This enforces the boundary conditions at these two points.
utt −uxx = 0,
x ∈(−∞. . . ∞),
t ∈(0 . . . ∞)
u(x, 0) =
∞
X
n=−∞

H
1
2 −
x −3
2 −8n


+ H
1
2 −
x −13
2 −8n


,
ut(x, 0) = 0
We use D’Alemberts solution to solve this problem.
u(x, t) = 1
2
∞
X
n=−∞

H
1
2 −
x −3
2 −8n −t


+ H
1
2 −
x −3
2 −8n + t


+H
1
2 −
x −13
2 −8n −t


+ H
1
2 −
x −13
2 −8n + t


1849

The solution for several times is plotted in Figure 41.2. Note that the solution is periodic in time with period 8.
Figure 41.3 shows the solution in the phase plane for 0 < t < 8. Note the even reﬂections at the boundaries.
1
2
3
4
0.2
0.4
0.6
0.8
1
1
2
3
4
0.2
0.4
0.6
0.8
1
1
2
3
4
0.2
0.4
0.6
0.8
1
1
2
3
4
0.2
0.4
0.6
0.8
1
Figure 41.2: The solution at t = 1/2, 1, 3/2, 2 for the boundary conditions ux(0, t) = ux(4, t) = 0.
Solution 41.2
1.
u(x, t) = 1
2 (u(x + ct, 0) + u(x −ct, 0)) + 1
2c
Z x+ct
x−ct
ut(τ, 0) dτ
u(x, t) = 1
2c
Z x+ct
x−ct
sin(ωτ) dτ
u(x, t) = sin(ωx) sin(ωct)
ωc
Figure 41.4 shows the solution for c = ω = 1.
1850

-1/2
-1
-1/2
0
0
-1/2
-1/2
0
0
1
1/2
1/2
0
0
0
0
0
0
0
0
1/2
1/2
1
1
1/2
1/2
0
0
0
0
0
0
0
0
1
1
1/2
1/2
1
1/2
1/2
1
1
1/2
1/2
1
x
t
x
t
Figure 41.3: The solution in the phase plane for the boundary conditions u(0, t) = u(4, t) = 0 and ux(0, t) =
ux(4, t) = 0.
2. We can write the initial velocity in terms of the Heaviside function.
ut(x, 0) =





1
for 0 < x < 1
−1
for −1 < x < 0
0
for |x| > 1.
ut(x, 0) = −H(x + 1) + 2H(x) −H(x −1)
1851

-5
0
5
x
0
2
4
6
t
-1
-0.5
0
0.5
1
u
-5
0
5
x
Figure 41.4: Solution of the wave equation.
We integrate the Heaviside function.
Z b
a
H(x −c) dx =





0
for b < c
b −a
for a > c
b −c
otherwise
If a < b, we can express this as
Z b
a
H(x −c) dx = min(b −a, max(b −c, 0)).
1852

Now we ﬁnd an expression for the solution.
u(x, t) = 1
2 (u(x + ct, 0) + u(x −ct, 0)) + 1
2c
Z x+ct
x−ct
ut(τ, 0) dτ
u(x, t) = 1
2c
Z x+ct
x−ct
(−H(τ + 1) + 2H(τ) −H(τ −1)) dτ
u(x, t) = −min(2ct, max(x + ct + 1, 0)) + 2 min(2ct, max(x + ct, 0)) −min(2ct, max(x + ct −1, 0))
Figure 41.5 shows the solution for c = 1.
-4
-2
0
2
4
x
0
1
2
3
t
-1
-0.5
0
0.5
1
u
-4
-2
0
2
x
Figure 41.5: Solution of the wave equation.
1853

Solution 41.3
1. The solution on the interval (−∞. . . ∞) is
u(x, t) = 1
2(h(x + ct) + h(x −ct)).
Now we solve the problem on (0 . . . ∞). We deﬁne the odd extension of h(x).
ˆh(x) =
(
h(x)
for x > 0,
−h(−x)
for x < 0, = sign(x)h(|x|)
Note that
ˆh′(0−) = d
dx(−h(−x))

x→0+ = h′(0+) = ˆh′(0+).
Thus ˆh(x) is piecewise C2. Clearly
u(x, t) = 1
2(ˆh(x + ct) + ˆh(x −ct))
satisﬁes the diﬀerential equation on (0 . . . ∞).
We verify that it satisﬁes the initial condition and boundary
condition.
u(x, 0) = 1
2(ˆh(x) + ˆh(x)) = h(x)
u(0, t) = 1
2(ˆh(ct) + ˆh(−ct)) = 1
2(h(ct) −h(ct)) = 0
2. First we deﬁne the odd extension of h(x) on the interval (−l . . . l).
ˆh(x) = sign(x)h(|x|),
x ∈(−l . . . l)
Then we form the odd periodic extension of h(x) deﬁned on (−∞. . . ∞).
ˆh(x) = sign

x −2l
x + l
2l

h
x −2l
x + l
2l


,
x ∈(−∞. . . ∞)
1854

We note that ˆh(x) is piecewise C2. Also note that ˆh(x) is odd about the points x = nl, n ∈Z. That is,
ˆh(nl −x) = −ˆh(nl + x). Clearly
u(x, t) = 1
2(ˆh(x + ct) + ˆh(x −ct))
satisﬁes the diﬀerential equation on (0 . . . l). We verify that it satisﬁes the initial condition and boundary condi-
tions.
u(x, 0) = 1
2(ˆh(x) + ˆh(x))
u(x, 0) = ˆh(x)
u(x, 0) = sign

x −2l
x + l
2l

h
x −2l
x + l
2l


u(x, 0) = h(x)
u(0, t) = 1
2(ˆh(ct) + ˆh(−ct)) = 1
2(ˆh(ct) −ˆh(ct)) = 0
u(l, t) = 1
2(ˆh(l + ct) + ˆh(l −ct)) = 1
2(ˆh(l + ct) −ˆh(l + ct)) = 0
Solution 41.4
Change of Variables.
Let u(x, t) be the solution of the problem with deﬂection u(x, T) = φ(x) and velocity
ut(x, T) = ψ(x). Deﬁne
v(x, τ) = u(x, T −τ).
We note that u(x, 0) = v(x, T). v(τ) satisﬁes the wave equation.
vττ = c2vxx
The initial conditions for v are
v(x, 0) = u(x, T) = φ(x),
vτ(x, 0) = −ut(x, T) = −ψ(x).
Thus we see that the student was correct.
1855

Direct Solution. D’Alembert’s solution is valid for all x and t. We formally substitute t −T for t in this solution
to solve the problem with deﬂection u(x, T) = φ(x) and velocity ut(x, T) = ψ(x).
u(x, t) = 1
2 (φ(x + c(t −T)) + φ(x −c(t −T))) + 1
2c
Z x+c(t−T)
x−c(t−T)
ψ(τ) dτ
This satisﬁes the wave equation, because the equation is shift-invariant. It also satisﬁes the initial conditions.
u(x, T) = 1
2 (φ(x) + φ(x)) + 1
2c
Z x
x
ψ(τ) dτ = φ(x)
ut(x, t) = 1
2 (cφ′(x + c(t −T)) −cφ′(x −c(t −T))) + 1
2 (ψ(x + c(t −T)) + ψ(x −c(t −T)))
ut(x, T) = 1
2 (cφ′(x) −cφ′(x)) + 1
2 (ψ(x) + ψ(x)) = ψ(x)
Solution 41.5
Since the solution is a wave moving to the right, we conclude that we could take α(ξ) = 0. Our solution will corroborate
this.
The form of the solution is
u(x, t) = α(x + ct) + β(x −ct).
We substitute the solution into the initial conditions.
u(x, 0) = α(ξ) + β(ξ) = 0,
ξ > 0
ut(x, 0) = cα′(ξ) −cβ′(ξ) = 0,
ξ > 0
We integrate the second equation to obtain the system
α(ξ) + β(ξ) = 0,
ξ > 0,
α(ξ) −β(ξ) = 2k,
ξ > 0,
which has the solution
α(ξ) = k,
β(ξ) = −k,
ξ > 0.
1856

Now we substitute the solution into the initial condition.
u(0, t) = α(ct) + β(−ct) = γ(t),
t > 0
α(ξ) + β(−ξ) = γ(ξ/c),
ξ > 0
β(ξ) = γ(−ξ/c) −k,
ξ < 0
This determines u(x, t) for x > 0 as it depends on α(ξ) only for ξ > 0. The constant k is arbitrary. Changing k does
not change u(x, t). For simplicity, we take k = 0.
u(x, t) = β(x −ct)
u(x, t) =
(
0
for x −ct < 0
γ(t −x/c)
for x −ct > 0
u(x, t) = γ(t −x/c)H(ct −x)
Solution 41.6
1. We write the value of u along the line x −ct = k as a function of t: u(k + ct, t). We diﬀerentiate ut −cux with
respect to t to see how the quantity varies.
d
dt (ut(k + ct, t) −cux(k + ct, t)) = cuxt + utt −c2uxx −cuxt
= utt −c2uxx
= 0
Thus ut −cux is constant along the line x −ct = k. Now we examine ut + cux along the line x + ct = k.
d
dt (ut(k −ct, t) + cux(k −ct, t)) = −cuxt + utt −c2uxx + cuxt
= utt −c2uxx
= 0
ut + cux is constant along the line x + ct = k.
1857

2. From part (a) we know
ut(x0, t0) −cux(x0, t0) = ut(x0 −ct0, 0) −cux(x0 −ct0, 0)
ut(x0, t0) + cux(x0, t0) = ut(x0 + ct0, 0) + cux(x0 + ct0, 0).
We add these equations to ﬁnd ut(x0, t0).
ut(x0, t0) = 1
2 (ut(x0 −ct0, 0) −cux(x0 −ct0, 0)ut(x0 + ct0, 0) + cux(x0 + ct0, 0))
Since t0 was arbitrary, we have
ut(x0, τ) = 1
2 (ut(x0 −cτ, 0) −cux(x0 −cτ, 0)ut(x0 + cτ, 0) + cux(x0 + cτ, 0))
for 0 < τ < t0. We integrate with respect to τ to determine u(x0, t0).
u(x0, t0) = u(x0, 0) +
Z t0
0
1
2 (ut(x0 −cτ, 0) −cux(x0 −cτ, 0)ut(x0 + cτ, 0) + cux(x0 + cτ, 0)) dτ
= u(x0, 0) + 1
2
Z t0
0
(−cux(x0 −cτ, 0) + cux(x0 + cτ, 0)) dτ
+ 1
2
Z t0
0
(ut(x0 −cτ, 0) + ut(x0 + cτ, 0)) dτ
= u(x0, 0) + 1
2 (u(x0 −ct0, 0) −u(x0, 0) + u(x0 + ct0, 0) −u(x0, 0))
+ 1
2c
Z x0−ct0
x0
−ut(τ, 0) dτ + 1
2c
Z x0+ct0
x0
ut(τ, 0) dτ
= 1
2 (u(x0 −ct0, 0) + u(x0 + ct0, 0)) + 1
2c
Z x0+ct0
x0−ct0
ut(τ, 0) dτ
We have D’Alembert’s solution.
u(x, t) = 1
2 (u(x −ct, 0) + u(x + ct, 0)) + 1
2c
Z x+ct
x−ct
ut(τ, 0) dτ
1858

Solution 41.7
a) We substitute u(x, t) = A eıωt−αx into the partial diﬀerential equation and take the real part as the solution. We
assume that α has positive real part so the solution vanishes as x →+∞.
ıωA eıωt−αx = κα2A eıωt−αx
ıω = κα2
α = (1 + ı)
r ω
2κ
A solution of the partial diﬀerential equation is,
u(x, t) = ℜ

A exp

ıωt −(1 + ı)
r ω
2κx

,
u(x, t) = A exp

−
r ω
2κx

cos

ωt −
r ω
2κx

.
Applying the initial condition, u(0, t) = T cos(ωt), we obtain,
u(x, t) = T exp

−
r ω
2κx

cos

ωt −
r ω
2κx

.
b) At a ﬁxed depth x = h, the temperature is
u(h, t) = T exp

−
r ω
2κh

cos

ωt −
r ω
2κh

.
Thus the temperature variation is
−T exp

−
r ω
2κh

≤u(h, t) ≤T exp

−
r ω
2κh

.
1859

c) The solution is an exponentially decaying, traveling wave that propagates into the Earth with speed ω/
p
ω/(2κ) =
√
2κω. More generally, the wave
e−bt cos(ωt −ax)
travels in the positive direction with speed ω/a. Figure 41.6 shows such a wave for a sequence of times.
Figure 41.6: An Exponentially Decaying, Traveling Wave
The phase lag, δ(x) is the time that it takes for the wave to reach a depth of x. It satisﬁes,
ωδ(x) −
r ω
2κx = 0,
δ(x) =
x
√
2κω.
d) Let ωyear be the frequency for annual temperature variation, then ωday = 365ωyear. If xyear is the depth that a
particular yearly temperature variation reaches and xday is the depth that this same variation in daily temperature
reaches, then
exp

−
rωyear
2κ xyear

= exp

−
rωday
2κ xday

,
1860

rωyear
2κ xyear =
rωday
2κ xday,
xyear
xday
=
√
365.
Solution 41.8
We seek a periodic solution of the form,
u(r, θ, t) = v(r, θ) eıωt .
Substituting this into the wave equation will give us a Helmholtz equation for v.
−ω2v = c2∆v
vrr + 1
rvr + 1
r2vθθ + ω2
c2 v = 0
We have the boundary condition v(a, θ) = f(θ) and the radiation condition at inﬁnity. We expand v in a Fourier series
in θ in which the coeﬃcients are functions of r. You can check that eınθ are the eigenfunctions obtained with separation
of variables.
v(r, θ) =
∞
X
n=−∞
vn(r) eınθ
We substitute this expression into the Helmholtz equation to obtain ordinary diﬀerential equations for the coeﬃcients
vn.
∞
X
n=−∞

v′′
n + 1
rv′
n +
ω2
c2 −n2
r2

vn

eınθ = 0
The diﬀerential equations for the vn are
v′′
n + 1
rv′
n +
ω2
c2 −n2
r2

vn = 0.
1861

which has as linearly independent solutions the Bessel and Neumann functions,
Jn
ωr
c

,
Yn
ωr
c

,
or the Hankel functions,
H(1)
n
ωr
c

,
H(2)
n
ωr
c

.
The functions have the asymptotic behavior,
Jn(ρ) ∼
r 2
πρ cos(ρ −nπ/2 −π/4),
as ρ →∞,
Yn(ρ) ∼
r 2
πρ sin(ρ −nπ/2 −π/4),
as ρ →∞,
H(1)
n (ρ) ∼
r 2
πρ ei(ρ−nπ/2−π/4),
as ρ →∞,
H(2)
n (ρ) ∼
r 2
πρ e−i(ρ−nπ/2−π/4),
as ρ →∞.
u(r, θ, t) will be an outgoing wave at inﬁnity if it is the sum of terms of the form ei(ωt−constr). Thus the vn must have
the form
vn(r) = bnH(2)
n
ωr
c

for some constants, bn. The solution for v(r, θ) is
v(r, θ) =
∞
X
n=−∞
bnH(2)
n
ωr
c

eınθ .
1862

We determine the constants bn from the boundary condition at r = a.
v(a, θ) =
∞
X
n=−∞
bnH(2)
n
ωa
c

eınθ = f(θ)
bn =
1
2πH(2)
n (ωa/c)
Z 2π
0
f(θ) e−ınθ dθ
u(r, θ, t) = eıωt
∞
X
n=−∞
bnH(2)
n
ωr
c

eınθ
Solution 41.9
We substitute the form v(x, y, t) = u(r, θ) e−ıωt into the wave equation to obtain a Helmholtz equation.
c2∆u + ω2u = 0
urr + 1
rur + 1
r2uθθ + k2u = 0
We solve the Helmholtz equation with separation of variables. We expand u in a Fourier series.
u(r, θ) =
∞
X
n=−∞
un(r) eınθ
We substitute the sum into the Helmholtz equation to determine ordinary diﬀerential equations for the coeﬃcients.
u′′
n + 1
ru′
n +

k2 −n2
r2

un = 0
This is Bessel’s equation, which has as solutions the Bessel and Neumann functions, {Jn(kr), Yn(kr)} or the Hankel
functions, {H(1)
n (kr), H(2)
n (kr)}.
1863

Recall that the solutions of the Bessel equation have the asymptotic behavior,
Jn(ρ) ∼
r 2
πρ cos(ρ −nπ/2 −π/4),
as ρ →∞,
Yn(ρ) ∼
r 2
πρ sin(ρ −nπ/2 −π/4),
as ρ →∞,
H(1)
n (ρ) ∼
r 2
πρ ei(ρ−nπ/2−π/4),
as ρ →∞,
H(2)
n (ρ) ∼
r 2
πρ e−i(ρ−nπ/2−π/4),
as ρ →∞.
From this we see that only the Hankel function of the ﬁrst kink will give us outgoing waves as ρ →∞. Our solution
for u becomes,
u(r, θ) =
∞
X
n=−∞
bnH(1)
n (kr) eınθ .
We determine the coeﬃcients in the expansion from the boundary condition at r = a.
u(a, θ) =
∞
X
n=−∞
bnH(1)
n (ka) eınθ = −eıka cos θ
bn = −
1
2πH(1)
n (ka)
Z 2π
0
eıka cos θ e−ınθ dθ
We evaluate the integral with the identities,
Jn(x) =
1
2πin
Z 2π
0
eıx cos θ eınθ dθ,
J−n(x) = (−1)nJn(x).
1864

Thus we obtain,
u(r, θ) = −
∞
X
n=−∞
(−ı)nJn(ka)
H(1)
n (ka)
H(1)
n (kr) eınθ .
When a ≪1/k, i.e. ka ≪1, the Bessel function has the behavior,
Jn(ka) ∼(ka/2)n
n!
.
In this case, the n ̸= 0 terms in the sum are much smaller than the n = 0 term. The approximate solution is,
u(r, θ) ∼−H(1)
0 (kr)
H(1)
0 (ka)
,
v(r, θ, t) ∼−H(1)
0 (kr)
H(1)
0 (ka)
e−ıωt .
Solution 41.10
a)
(
−Ix = CVt + GV,
−Vx = LIt + RI
First we derive a single partial diﬀerential equation for I. We diﬀerentiate the two partial diﬀerential equations with
respect to x and t, respectively and then eliminate the Vxt terms.
(
−Ixx = CVtx + GVx,
−Vxt = LItt + RIt
−Ixx + LCItt + RCIt = GVx
1865

We use the initial set of equations to write Vx in terms of I.
−Ixx + LCItt + RCIt + G(LIt + RI) = 0
Itt + RC + GL
LC
It + GR
LC I −1
LC Ixx = 0
Now we derive a single partial diﬀerential equation for V . We diﬀerentiate the two partial diﬀerential equations
with respect to t and x, respectively and then eliminate the Ixt terms.
(
−Ixt = CVtt + GVt,
−Vxx = LItx + RIx
−Vxx = RIx −LCVtt −LGVt
We use the initial set of equations to write Ix in terms of V .
LCVtt + LGVt −Vxx + R(CVt + GV ) = 0
Vtt + RC + LG
LC
Vt + RG
LC V −1
LC Vxx = 0.
Thus we see that I and V both satisfy the same damped wave equation.
b) We substitute V (x, t) = e−γt(f(x −at) + g(x + at)) into the damped wave equation for V .

γ2 −RC + LG
LC
γ + RG
LC

e−γt(f + g) +

−2γ + RC + LG
LC

a e−γt(−f ′ + g′)
+ a2 e−γt(f ′′ + g′′) −1
LC e−γt(f ′′ + g′′) = 0
Since f and g are arbitrary functions, the coeﬃcients of e−γt(f + g), e−γt(−f ′ + g′) and e−γt(f ′′ + g′′) must vanish.
This gives us three constraints.
a2 −1
LC = 0,
−2γ + RC + LG
LC
= 0,
γ2 −RC + LG
LC
γ + RG
LC = 0
1866

The ﬁrst equation determines the wave speed to be a = 1/
√
LC. We substitute the value of γ from the second
equation into the third equation.
γ = RC + LG
2LC
,
−γ2 + RG
LC = 0
In order for damped waves to propagate, the physical constants must satisfy,
RG
LC −
RC + LG
2LC
2
= 0,
4RGLC −(RC + LG)2 = 0,
(RC −LG)2 = 0,
RC = LG.
1867

Chapter 42
Similarity Methods
Introduction.
Consider the partial diﬀerential equation (not necessarily linear)
F
∂u
∂t , ∂u
∂x, u, t, x

= 0.
Say the solution is
u(x, t) = x
t sin
 t1/2
x1/2

.
Making the change of variables ξ = x/t, f(ξ) = u(x, t), we could rewrite this equation as
f(ξ) = ξ sin
 ξ−1/2
.
We see now that if we had guessed that the solution of this partial diﬀerential equation was only dependent on powers
of x/t we could have changed variables to ξ and f and instead solved the ordinary diﬀerential equation
G
df
dξ , f, ξ

= 0.
By using similarity methods one can reduce the number of independent variables in some PDE’s.
1868

Example 42.0.1 Consider the partial diﬀerential equation
x∂u
∂t + t∂u
∂x −u = 0.
One way to ﬁnd a similarity variable is to introduce a transformation to the temporary variables u′, t′, x′, and the
parameter λ.
u = u′λ
t = t′λm
x = x′λn
where n and m are unknown. Rewriting the partial diﬀerential equation in terms of the temporary variables,
x′λn∂u′
∂t′ λ1−m + t′λm∂u′
∂x′λ1−n −u′λ = 0
x′∂u′
∂t′ λ−m+n + t′∂u′
∂x′λm−n −u′ = 0
There is a similarity variable if λ can be eliminated from the equation. Equating the coeﬃcients of the powers of λ in
each term,
−m + n = m −n = 0.
This has the solution m = n. The similarity variable, ξ, will be unchanged under the transformation to the temporary
variables. One choice is
ξ = t
x = t′λn
x′λm = t′
x′.
Writing the two partial derivative in terms of ξ,
∂
∂t = ∂ξ
∂t
d
dξ = 1
x
d
dξ
∂
∂x = ∂ξ
∂x
d
dξ = −t
x2
d
dξ
1869

The partial diﬀerential equation becomes
du
dξ −ξ2du
dξ −u = 0
du
dξ =
u
1 −ξ2
Thus we have reduced the partial diﬀerential equation to an ordinary diﬀerential equation that is much easier to solve.
u(ξ) = exp
Z ξ
dξ
1 −ξ2

u(ξ) = exp
Z ξ 1/2
1 −ξ + 1/2
1 + ξ dξ

u(ξ) = exp

−1
2 log(1 −ξ) + 1
2 log(1 + ξ)

u(ξ) = (1 −ξ)−1/2(1 + ξ)1/2
u(x, t) =
1 + t/x
1 −t/x
1/2
Thus we have found a similarity solution to the partial diﬀerential equation. Note that the existence of a similarity
solution does not mean that all solutions of the diﬀerential equation are similarity solutions.
Another Method.
Another method is to substitute ξ = xαt and determine if there is an α that makes ξ a similarity
variable. The partial derivatives become
∂
∂t = ∂ξ
∂t
d
dξ = xα d
dξ
∂
∂x = ∂ξ
∂x
d
dξ = αxα−1t d
dξ
1870

The partial diﬀerential equation becomes
xα+1du
dξ + αxα−1t2du
dξ −u = 0.
If there is a value of α such that we can write this equation in terms of ξ, then ξ = xαt is a similarity variable. If
α = −1 then the coeﬃcient of the ﬁrst term is trivially in terms of ξ. The coeﬃcient of the second term then becomes
−x−2t2. Thus we see ξ = x−1t is a similarity variable.
Example 42.0.2 To see another application of similarity variables, any partial diﬀerential equation of the form
F

tx, u, ut
x , ux
t

= 0
is equivalent to the ODE
F

ξ, u, du
dξ , du
dξ

= 0
where ξ = tx. Performing the change of variables,
1
x
∂u
∂t = 1
x
∂ξ
∂t
du
dξ = 1
xxdu
dξ = du
dξ
1
t
∂u
∂x = 1
t
∂ξ
∂x
du
dξ = 1
t tdu
dξ = du
dξ .
For example the partial diﬀerential equation
u∂u
∂t + x
t
∂u
∂x + tx2u = 0
which can be rewritten
u1
x
∂u
∂t + 1
t
∂u
∂x + txu = 0,
1871

is equivalent to
udu
dξ + du
dξ + ξu = 0
where ξ = tx.
1872

42.1
Exercises
Exercise 42.1
Consider the 1-D heat equation
ut = νuxx
Assume that there exists a function η(x, t) such that it is possible to write u(x, t) = F(η(x, t)). Re-write the PDE
in terms of F(η), its derivatives and (partial) derivatives of η. By guessing that this transformation takes the form
η = xtα, ﬁnd a value of α so that this reduces to an ODE for F(η) (i.e. x and t are explicitly removed). Find the
general solution and use this to ﬁnd the corresponding solution u(x, t). Is this the general solution of the PDE?
Exercise 42.2
With ξ = xαt, ﬁnd α such that for some function f, φ = f(ξ) is a solution of
φt = a2φxx.
Find f(ξ) as well.
1873

42.2
Hints
Hint 42.1
Hint 42.2
1874

42.3
Solutions
Solution 42.1
We write the derivatives of u(x, t) in terms of derivatives of F(η).
ut = αxtα−1F ′ = αη
t F ′
ux = tαF ′
uxx = t2αF ′′ = η2
x2F ′′
We substitite these expressions into the heat equation.
αη
t F ′ = ν η2
x2F ′′
F ′′ = α
ν
x2
t
1
ηF ′
We can write this equation in terms of F and η only if α = −1/2. We make this substitution and solve the ordinary
diﬀerential equation for F(η).
F ′′
F ′ = −η
2ν
log(F ′) = −η2
4ν + c
F ′ = c exp

−η2
4ν

F = c1
Z
exp

−η2
4ν

dη + c2
We can write F in terms of the error function.
F = c1 erf
 η
2√ν

+ c2
1875

We write this solution in terms of x and t.
u(x, t) = c1 erf

x
2
√
νt

+ c2
This is not the general solution of the heat equation. There are many other solutions. Note that since x and t do not
explicitly appear in the heat equation,
u(x, t) = c1 erf
 
x −x0
2
p
ν(t −t0)
!
+ c2
is a solution.
Solution 42.2
We write the derivatives of φ in terms of f.
φt = ∂ξ
∂t
∂
∂ξf = xαf ′ = t−1ξf ′
φx = ∂ξ
∂x
∂
∂ξf = αxα−1tf ′
φxx = f ′ ∂
∂x
 αxα−1t

+ αxα−1tαxα−1t ∂
∂ξf ′
φxx = α2x2α−2t2f ′′ + α(α −1)xα−2tf ′
φxx = x−2  α2ξ2f ′′ + α(α −1)ξf ′
We substitute these expressions into the diﬀusion equation.
ξf ′ = x−2t
 α2ξ2f ′′ + α(α −1)ξf ′
In order for this equation to depend only on the variable ξ, we must have α = −2. For this choice we obtain an ordinary
1876

diﬀerential equation for f(ξ).
f ′ = 4ξ2f ′′ + 6ξf ′
f ′′
f ′ =
1
4ξ2 −3
2ξ
log(f ′) = −1
4ξ −3
2 log ξ + c
f ′ = c1ξ−3/2 e−1/(4ξ)
f(ξ) = c1
Z ξ
t−3/2 e−1/(4t) dt + c2
f(ξ) = c1
Z 1/(2√ξ)
e−t2 dt + c2
f(ξ) = c1 erf
 1
2√ξ

+ c2
1877

Chapter 43
Method of Characteristics
43.1
First Order Linear Equations
Consider the following ﬁrst order wave equation.
ut + cux = 0
(43.1)
Let x(t) be some path in the phase plane. Perhaps x(t) describes the position of an observer who is noting the value
of the solution u(x(t), t) at their current location. We diﬀerentiate with respect to t to see how the solution varies for
the observer.
d
dtu(x(t), t) = ut + x′(t)ux
(43.2)
We note that if the observer is moving with velocity c, x′(t) = c, then the solution at their current location does not
change because ut + cux = 0. We will examine this more carefully.
By comparing Equations 43.1 and 43.2 we obtain ordinary diﬀerential equations representing the position of an
observer and the value of the solution at that position.
dx
dt = c,
du
dt = 0
1878

Let the observer start at the position x0. Then we have an initial value problem for x(t).
dx
dt = c,
x(0) = x0
x(t) = x0 + ct
These lines x(t) are called characteristics of Equation 43.1.
Let the initial condition be u(x, 0) = f(x). We have an initial value problem for u(x(t), t).
du
dt = 0,
u(0) = f(x0)
u(x(t), t) = f(x0)
Again we see that the solution is constant along the characteristics. We substitute the equation for the characteristics
into this expression.
u(x0 + ct, t) = f(x0)
u(x, t) = f(x −ct)
Now we see that the solution of Equation 43.1 is a wave moving with velocity c. The solution at time t is the initial
condition translated a distance of ct.
43.2
First Order Quasi-Linear Equations
Consider the following quasi-linear equation.
ut + a(x, t, u)ux = 0
(43.3)
We will solve this equation with the method of characteristics. We diﬀerentiate the solution along a path x(t).
d
dtu(x(t), t) = ut + x′(t)ux
(43.4)
1879

By comparing Equations 43.3 and 43.4 we obtain ordinary diﬀerential equations for the characteristics x(t) and the
solution along the characteristics u(x(t), t).
dx
dt = a(x, t, u),
du
dt = 0
Suppose an initial condition is speciﬁed, u(x, 0) = f(x). Then we have ordinary diﬀerential equation, initial value
problems.
dx
dt = a(x, t, u),
x(0) = x0
du
dt = 0,
u(0) = f(x0)
We see that the solution is constant along the characteristics. The solution of Equation 43.3 is a wave moving with
velocity a(x, t, u).
Example 43.2.1 Consider the inviscid Burger equation,
ut + uux = 0,
u(x, 0) = f(x).
We write down the diﬀerential equations for the solution along a characteristic.
dx
dt = u,
x(0) = x0
du
dt = 0,
u(0) = f(x0)
First we solve the equation for u. u = f(x0). Then we solve for x. x = x0 + f(x0)t. This gives us an implicit solution
of the Burger equation.
u(x0 + f(x0)t, t) = f(x0)
1880

43.3
The Method of Characteristics and the Wave Equation
Consider the one dimensional wave equation,
utt = c2uxx.
We make the change of variables, a = ux, b = ut, to obtain a coupled system of ﬁrst order equations.
at −bx = 0
bt −c2ax = 0
We write this as a matrix equation.
a
b

t
+
 0
−1
−c2
0
 a
b

x
= 0
The eigenvalues and eigenvectors of the matrix are
λ1 = −c,
λ2 = c,
ξ1 =
1
c

,
ξ2 =
 1
−c

.
The matrix is diagonalized by a similarity transformation.
−c
0
0
c

=
1
1
c
−c
−1  0
−1
−c2
0
 1
1
c
−c

We make a change of variables to diagonalize the system.
a
b

=
1
1
c
−c
 α
β

1
1
c
−c
 α
β

t
+
 0
−1
−c2
0
 1
1
c
−c
 α
β

x
= 0
1881

Now we left multiply by the inverse of the matrix of eigenvectors to obtain an uncoupled system that we can solve
directly.
α
β

t
+
−c
0
0
c
 α
β

x
= 0.
α(x, t) = p(x + ct),
β(x, t) = q(x −ct),
Here p, q ∈C2 are arbitrary functions. We change variables back to a and b.
a(x, t) = p(x + ct) + q(x −ct),
b(x, t) = cp(x + ct) −cq(x −ct)
We could integrate either a = ux or b = ut to obtain the solution of the wave equation.
u = F(x −ct) + G(x + ct)
Here F, G ∈C2 are arbitrary functions. We see that u(x, t) is the sum of a waves moving to the right and left with
speed c. This is the general solution of the one-dimensional wave equation. Note that for any given problem, F and
G are only determined to whithin an additive constant. For any constant k, adding k to F and subtracting it from G
does not change the solution.
u = (F(x −ct) + k) + (G(x −ct) −k)
43.4
The Wave Equation for an Inﬁnite Domain
Consider the Cauchy problem for the wave equation on −∞< x < ∞.
utt = c2uxx,
−∞< x < ∞, t > 0
u(x, 0) = f(x),
ut(x, 0) = g(x)
We know that the solution is the sum of right-moving and left-moving waves.
u(x, t) = F(x −ct) + G(x + ct)
(43.5)
1882

The initial conditions give us two constraints on F and G.
F(x) + G(x) = f(x),
−cF ′(x) + cG′(x) = g(x).
We integrate the second equation.
−F(x) + G(x) = 1
c
Z
g(x) dx + const
Here Q(x) =
R
q(x) dx. We solve the system of equations for F and G.
F(x) = 1
2f(x) −1
2c
Z
g(x) dx −k,
G(x) = 1
2f(x) + 1
2c
Z
g(x) dx + k
Note that the value of the constant k does not aﬀect the solution, u(x, t). For simplicity we take k = 0. We substitute
F and G into Equation 43.5 to determine the solution.
u(x, t) = 1
2 (f(x −ct) + f(x + ct)) + 1
2c
Z x+ct
g(x) dx −
Z x−ct
g(x) dx

u(x, t) = 1
2 (f(x −ct) + f(x + ct)) + 1
2c
Z x+ct
x−ct
g(ξ) dξ
u(x, t) = 1
2 (u(x −ct, 0) + u(x + ct, 0)) + 1
2c
Z x+ct
x−ct
ut(ξ, 0) dξ
43.5
The Wave Equation for a Semi-Inﬁnite Domain
Consider the wave equation for a semi-inﬁnite domain.
utt = c2uxx,
0 < x < ∞, t > 0
u(x, 0) = f(x),
ut(x, 0) = g(x),
u(0, t) = h(t)
1883

Again the solution is the sum of a right-moving and a left-moving wave.
u(x, t) = F(x −ct) + G(x + ct)
For x > ct, the boundary condition at x = 0 does not aﬀect the solution. Thus we know the solution in this domain
from our work on the wave equation in the inﬁnite domain.
u(x, t) = 1
2 (f(x −ct) + f(x + ct)) + 1
2c
Z x+ct
x−ct
g(ξ) dξ,
x > ct
From this, F(ξ) and G(ξ) are determined for ξ > 0.
F(ξ) = 1
2f(ξ) −1
2c
Z
g(ξ) dξ,
ξ > 0
G(ξ) = 1
2f(ξ) + 1
2c
Z
g(ξ) dξ,
ξ > 0
In order to determine the solution u(x, t) for x, t > 0 we also need to determine F(ξ) for ξ < 0. To do this, we
substitute the form of the solution into the boundary condition at x = 0.
u(0, t) = h(t),
t > 0
F(−ct) + G(ct) = h(t),
t > 0
F(ξ) = −G(−ξ) + h(−ξ/c),
ξ < 0
F(ξ) = −1
2f(−ξ) −1
2c
Z −ξ
g(ψ) dψ + h(−ξ/c),
ξ < 0
We determine the solution of the wave equation for x < ct.
u(x, t) = F(x −ct) + G(x + ct)
u(x, t) = −1
2f(−x + ct) −1
2c
Z −x+ct
g(ξ) dξ + h(t −x/c) + 1
2f(x + ct) + 1
2c
Z x+ct
g(ξ) dξ,
x < ct
u(x, t) = 1
2 (−f(−x + ct) + f(x + ct)) + 1
2c
Z x+ct
−x+ct
g(ξ) dξ + h(t −x/c),
x < ct
1884

Finally, we collect the solutions in the two domains.
u(x, t) =
(
1
2 (f(x −ct) + f(x + ct)) + 1
2c
R x+ct
x−ct g(ξ) dξ,
x > ct
1
2 (−f(−x + ct) + f(x + ct)) + 1
2c
R x+ct
−x+ct g(ξ) dξ + h(t −x/c),
x < ct
43.6
The Wave Equation for a Finite Domain
Consider the wave equation for the inﬁnite domain.
utt = c2uxx,
−∞< x < ∞, t > 0
u(x, 0) = f(x),
ut(x, 0) = g(x)
If f(x) and g(x) are odd about x = 0, (f(x) = −f(−x), g(x) = −g(−x)), then u(x, t) is also odd about x = 0. We
can demonstrate this with D’Alembert’s solution.
u(x, t) = 1
2 (f(x −ct) + f(x + ct)) + 1
2c
Z x+ct
x−ct
g(ξ) dξ
−u(−x, t) = −1
2 (f(−x −ct) + f(−x + ct)) −1
2c
Z −x+ct
−x−ct
g(ξ) dξ
= 1
2 (f(x + ct) + f(x −ct)) −1
2c
Z x−ct
x+ct
g(−ξ) (−dξ)
= 1
2 (f(x −ct) + f(x + ct)) + 1
2c
Z x+ct
x−ct
g(ξ) dξ
= u(x, t)
Thus if the initial conditions f(x) and g(x) are odd about a point then the solution of the wave equation u(x, t) is
also odd about that point. The analogous result holds if the initial conditions are even about a point. These results
are useful in solving the wave equation on a ﬁnite domain.
1885

Consider a string of length L with ﬁxed ends.
utt = c2uxx,
0 < x < L, t > 0
u(x, 0) = f(x),
ut(x, 0) = g(x),
u(0, t) = u(L, t) = 0
We extend the domain of the problem to x ∈(−∞. . . ∞). We form the odd periodic extensions ˜f and ˜g which are
odd about the points x = 0, L.
If a function h(x) is deﬁned for positive x, then sign(x)h(|x|) is the odd extension of the function. If h(x) is deﬁned
for x ∈(−L . . . L) then its periodic extension is
h

x −2L
x + L
2L

.
We combine these two formulas to form odd periodic extensions.
˜f(x) = sign

x −2L
x + L
2L

f
x −2L
x + L
2L


˜g(x) = sign

x −2L
x + L
2L

g
x −2L
x + L
2L


Now we can write the solution for the vibrations of a string with ﬁxed ends.
u(x, t) = 1
2

˜f(x −ct) + ˜f(x + ct)

+ 1
2c
Z x+ct
x−ct
˜g(ξ) dξ
43.7
Envelopes of Curves
Consider the tangent lines to the parabola y = x2. The slope of the tangent at the point (x, x2) is 2x. The set of
tangents form a one parameter family of lines,
f(x, t) = t2 + (x −t)2t = 2tx −t2.
1886

-1
1
-1
1
Figure 43.1: A parabola and its tangents.
The parabola and some of its tangents are plotted in Figure 43.1.
The parabola is the envelope of the family of tangent lines. Each point on the parabola is tangent to one of the
lines. Given a curve, we can generate a family of lines that envelope the curve. We can also do the opposite, given
a family of lines, we can determine the curve that they envelope. More generally, given a family of curves, we can
determine the curve that they envelope. Let the one parameter family of curves be given by the equation F(x, y, t) = 0.
For the example of the tangents to the parabola this equation would be y −2tx + t2 = 0.
Let y(x) be the envelope of F(x, y, t) = 0. Then the points on y(x) must lie on the family of curves. Thus y(x)
must satisfy the equation F(x, y, t) = 0. The points that lie on the envelope have the property,
∂
∂tF(x, y, t) = 0.
We can solve this equation for t in terms of x and y, t = t(x, y). The equation for the envelope is then
F(x, y, t(x, y)) = 0.
1887

Consider the example of the tangents to the parabola. The equation of the one-parameter family of curves is
F(x, y, t) ≡y −2tx + t2 = 0.
The condition Ft(x, y, t) = 0 gives us the constraint,
−2x + 2t = 0.
Solving this for t gives us t(x, y) = x. The equation for the envelope is then,
y −2xx + x2 = 0,
y = x2.
Example 43.7.1 Consider the one parameter family of curves,
(x −t)2 + (y −t)2 −1 = 0.
These are circles of unit radius and center (t, t). To determine the envelope of the family, we ﬁrst use the constraint
Ft(x, y, t) to solve for t(x, y).
Ft(x, y, t) = −2(x −t) −2(y −t) = 0
t(x, y) = x + y
2
Now we substitute this into the equation F(x, y, t) = 0 to determine the envelope.
F

x, y, x + y
2

=

x −x + y
2
2
+

y −x + y
2
2
−1 = 0
x −y
2
2
+
y −x
2
2
−1 = 0
(x −y)2 = 2
y = x ±
√
2
The one parameter family of curves and its envelope is shown in Figure 43.2.
1888

-3
-2
-1
1
2
3
-3
-2
-1
1
2
3
Figure 43.2: The envelope of (x −t)2 + (y −t)2 −1 = 0.
43.8
Exercises
Exercise 43.1
Consider the small transverse vibrations of a composite string of inﬁnite extent, made up of two homogeneous strings
of diﬀerent densities joined at x = 0. In each region 1) x < 0, 2) x > 0 we have
utt −c2
juxx = 0
j = 1, 2
c1 ̸= c2,
and we require continuity of u and ux at x = 0. Suppose for t < 0 a wave approaches the junction x = 0 from the
left, i.e. as t approaches 0 from negative values:
u(x, t) =
(
F(x −c1t)
x < 0, t ≤0
0
x > 0, t ≤0
As t increases further, the wave reaches x = 0 and gives rise to reﬂected and transmitted waves.
1889

1. Formulate the appropriate initial values for u at t = 0.
2. Solve the initial-value problem for −∞< x < ∞, t > 0.
3. Identify the incident, reﬂected and transmitted waves in your solution and determine the reﬂection and transmission
coeﬃcients for the junction in terms of c1 and c2. Comment also on their values in the limit c1 →c2.
Exercise 43.2
Consider a semi-inﬁnite string, x > 0. For all time the end of the string is displaced according to u(0, t) = f(t). Find
the motion of the string, u(x, t) with the method of characteristics and then with a Fourier transform in time. The
wave speed is c.
Exercise 43.3
Solve using characteristics:
uux + uy = 1,
u

x=y = x
2.
Exercise 43.4
Solve using characteristics:
(y + u)ux + yuy = x −y,
u

y=1 = 1 + x.
1890

43.9
Hints
Hint 43.1
Hint 43.2
1. Because the left end of the string is being displaced, there will only be right-moving waves. Assume a solution of
the form
u(x, t) = F(x −ct).
2. Take a Fourier transform in time. Use that there are only outgoing waves.
Hint 43.3
Hint 43.4
1891

43.10
Solutions
Solution 43.1
1.
u(x, 0) =
(
F(x),
x < 0
0,
x > 0
ut(x, 0) =
(
−c1F ′(x),
x < 0
0,
x > 0
2. Regardless of the initial condition, the solution has the following form.
u(x, t) =
(
f1(x −c1t) + g1(x + c1t),
x < 0
f2(x −c2t) + g1(x + c2t),
x > 0
For x < 0, the right-moving wave is F(x −c1t) and the left-moving wave is zero for x < −c1t. For x > 0, there
is no left-moving wave and the right-moving wave is zero for x > c2t. We apply these restrictions to the solution.
u(x, t) =
(
F(x −c1t) + g(x + c1t),
x < 0
f(x −c2t),
x > 0
We use the continuity of u and ux at x = 0 to solve for f and g.
F(−c1t) + g(c1t) = f(−c2t)
F ′(−c1t) + g′(c1t) = f ′(−c2t)
We integrate the second equation.
F(−t) + g(t) = f(−c2t/c1)
−F(−t) + g(t) = −c1
c2
f(−c2t/c1) + a
1892

We solve for f for x < c2t and for g for x > −c1t.
f(−c2t/c1) =
2c2
c1 + c2
F(−t) + b,
g(t) = c2 −c1
c1 + c2
F(−t) + b
By considering the case that the solution is continuous, F(0) = 0, we conclude that b = 0 since f(0) = g(0) = 0.
f(t) =
2c2
c1 + c2
F(c1t/c2),
g(t) = c2 −c1
c1 + c2
F(−t)
Now we can write the solution for u(x, t) for t > 0.
u(x, t) =
(
F(x −c1t) + c2−c1
c1+c2F(−x −c1t)H(x + c1t),
x < 0
2c2
c1+c2F

c1
c2(x −c2t)

H(c2t −x),
x > 0
3. The incident, reﬂected and transmitted waves are, respectively,
F(x −c1t),
c2 −c1
c1 + c2
F(−x −c1t)H(x + c1t),
2c2
c1 + c2
F
c1
c2
(x −c2t)

H(c2t −x).
The reﬂection and transmission coeﬃcients are, respectively,
c1 −c2
c1 + c2
,
2c2
c1 + c2
.
In the limit as c1 →c2, the reﬂection coeﬃcient vanishes and the transmission coeﬃcient tends to unity.
Solution 43.2
1. Method of characteristics. The problem is
utt −c2uxx = 0,
x > 0,
−∞< t < ∞,
u(0, t) = f(t).
1893

Because the left end of the string is being displaced, there will only be right-moving waves. The solution has the
form
u(x, t) = F(x −ct).
We substitute this into the boundary condition.
F(−ct) = f(t)
F(ξ) = f

−ξ
c

u(x, t) = f(t −x/c)
2. Fourier transform. We take the Fourier transform in time of the wave equation and the boundary condition.
utt = c2uxx,
u(0, t) = f(t)
−ω2ˆu = c2ˆuxx,
ˆu(0, ω) = ˆf(ω)
ˆuxx + ω2
c2 ˆu = 0,
ˆu(0, ω) = ˆf(ω)
The general solution of this ordinary diﬀerential equation is
ˆu(x, ω) = a(ω) eıωx/c +b(ω) e−ıωx/c .
The radiation condition, (u(x, t) must be a wave traveling in the positive direction), and the boundary condition
at x = 0 will determine the constants a and b. Consider the solution u(x, t) we will obtain by taking the inverse
Fourier transform of ˆu.
u(x, t) =
Z ∞
−∞
 a(ω) eıωx/c +b(ω) e−ıωx/c eıωt dω
u(x, t) =
Z ∞
−∞
 a(ω) eıω(t+x/c) +b(ω) eıω(t−x/c)
dω
1894

The ﬁrst and second terms in the integrand are left and right traveling waves, respectively. In order that u is a
right traveling wave, it must be a superposition of right traveling waves. We conclude that a(ω) = 0. We apply
the boundary condition at x = 0, we solve for ˆu.
ˆu(x, ω) = ˆf(ω) e−ıωx/c
Finally we take the inverse Fourier transform.
u(x, t) =
Z ∞
−∞
ˆf(ω) eıω(t−x/c) dω
u(x, t) = f(t −x/c)
Solution 43.3
uux + uy = 1,
u

x=y = x
2
(43.6)
We form du
dy.
du
dy = ux
dx
dy + uy
We compare this with Equation 43.6 to obtain diﬀerential equations for x and u.
dx
dy = u,
du
dy = 1.
(43.7)
The initial data is
x(y = α) = α,
u(y = α) = α
2 .
(43.8)
We solve the diﬀerenial equation for u (43.7) subject to the initial condition (43.8).
u(x(y), y) = y −α
2
1895

The diﬀerential equation for x becomes
dx
dy = y −α
2 .
We solve this subject to the initial condition (43.8).
x(y) = 1
2(y2 + α(2 −y))
This deﬁnes the characteristic starting at the point (α, α). We solve for α.
α = y2 −2x
y −2
We substitute this value for α into the solution for u.
u(x, y) = y(y −4) + 2x
2(y −2)
This solution is deﬁned for y ̸= 2. This is because at (x, y) = (2, 2), the characteristic is parallel to the line x = y.
Figure 43.3 has a plot of the solution that shows the singularity at y = 2.
Solution 43.4
(y + u)ux + yuy = x −y,
u

y=1 = 1 + x
(43.9)
We diﬀerentiate u with respect to s.
du
ds = ux
dx
ds + uy
dy
ds
We compare this with Equation 43.9 to obtain diﬀerential equations for x, y and u.
dx
ds = y + u,
dy
ds = y,
du
ds = x −y
1896

-2
0
2
x
-2
0
2
y
-10
0
10
u
Figure 43.3: The solution u(x, y).
We parametrize the initial data in terms of s.
x(s = 0) = α,
y(s = 0) = 1,
u(s = 0) = 1 + α
We solve the equation for y subject to the inital condition.
y(s) = es
This gives us a coupled set of diﬀerential equations for x and u.
dx
ds = es +u,
du
ds = x −es
The solutions subject to the initial conditions are
x(s) = (α + 1) es −e−s,
u(s) = α es + e−s .
1897

We substitute y(s) = es into these solutions.
x(s) = (α + 1)y −1
y,
u(s) = αy + 1
y
We solve the ﬁrst equation for α and substitute it into the second equation to obtain the solution.
u(x, y) = 2 + xy −y2
y
This solution is valid for y > 0. The characteristic passing through (α, 1) is
x(s) = (α + 1) es −e−s,
y(s) = es .
Hence we see that the characteristics satisfy y(s) ≥0 for all real s. Figure 43.4 shows some characteristics in the (x, y)
plane with starting points from (−5, 1) to (5, 1) and a plot of the solution.
-10-7.5 -5 -2.5
2.5 5 7.5 10
0.25
0.5
0.75
1
1.25
1.5
1.75
2
-2
-1
0
1
2
x
0.5
1
1.5
2
y
0
5
10
15
u
0
5
Figure 43.4: Some characteristics and the solution u(x, y).
1898

Chapter 44
Transform Methods
44.1
Fourier Transform for Partial Diﬀerential Equations
Solve Laplace’s equation in the upper half plane
∇2u = 0
−∞< x < ∞, y > 0
u(x, 0) = f(x)
−∞< x < ∞
Taking the Fourier transform in the x variable of the equation and the boundary condition,
F
∂2u
∂x2 + ∂2u
∂y2

= 0,
F [u(x, 0)] = F [f(x)]
−ω2U(ω, y) + ∂2
∂y2U(ω, y) = 0,
U(ω, 0) = F(ω).
The general solution to the equation is
U(ω, y) = a eωy +b e−ωy .
1899

Remember that in solving the diﬀerential equation here we consider ω to be a parameter. Requiring that the solution
be bounded for y ∈[0, ∞) yields
U(ω, y) = a e−|ω|y .
Applying the boundary condition,
U(ω, y) = F(ω) e−|ω|y .
The inverse Fourier transform of e−|ω|y is
F−1 e−|ω|y
=
2y
x2 + y2.
Thus
U(ω, y) = F(ω) F

2y
x2 + y2

F [u(x, y)] = F [f(x)] F

2y
x2 + y2

.
Recall that the convolution theorem is
F
 1
2π
Z ∞
−∞
f(x −ξ)g(ξ) dξ

= F(ω)G(ω).
Applying the convolution theorem to the equation for U,
u(x, y) = 1
2π
Z ∞
−∞
f(x −ξ)2y
ξ2 + y2
dξ
u(x, y) = y
π
Z ∞
−∞
f(x −ξ)
ξ2 + y2 dξ.
1900

44.2
The Fourier Sine Transform
Consider the problem
ut = κuxx,
x > 0,
t > 0
u(0, t) = 0,
u(x, 0) = f(x)
Since we are given the position at x = 0 we apply the Fourier sine transform.
ˆut = κ

−ω2ˆu + 2
πωu(0, t)

ˆut = −κω2ˆu
ˆu(ω, t) = c(ω) e−κω2t
The initial condition is
ˆu(ω, 0) = ˆf(ω).
We solve the ﬁrst order diﬀerential equation to determine ˆu.
ˆu(ω, t) = ˆf(ω) e−κω2t
ˆu(ω, t) = ˆf(ω)Fc

1
√
4πκt
e−x2/(4κt)

We take the inverse sine transform with the convolution theorem.
u(x, t) =
1
4π3/2√
κt
Z ∞
0
f(ξ)

e−|x−ξ|2/(4κt) −e−(x+ξ)2/(4κt)
dξ
44.3
Fourier Transform
Consider the problem
∂u
∂t −∂u
∂x + u = 0,
−∞< x < ∞,
t > 0,
1901

u(x, 0) = f(x).
Taking the Fourier Transform of the partial diﬀerential equation and the initial condition yields
∂U
∂t −ıωU + U = 0,
U(ω, 0) = F(ω) = 1
2π
Z ∞
−∞
f(x) e−ıωx dx.
Now we have a ﬁrst order diﬀerential equation for U(ω, t) with the solution
U(ω, t) = F(ω) e(−1+ıω)t .
Now we apply the inverse Fourier transform.
u(x, t) =
Z ∞
−∞
F(ω) e(−1+ıω)t eıωx dω
u(x, t) = e−t
Z ∞
−∞
F(ω) eıω(x+t) dω
u(x, t) = e−t f(x + t)
1902

44.4
Exercises
Exercise 44.1
Find an integral representation of the solution u(x, y), of
uxx + uyy = 0 in −∞< x < ∞, 0 < y < ∞,
subject to the boundary conditions:
u(x, 0) = f(x), −∞< x < ∞;
u(x, y) →0 as x2 + y2 →∞.
Exercise 44.2
Solve the Cauchy problem for the one-dimensional heat equation in the domain −∞< x < ∞, t > 0,
ut = κuxx,
u(x, 0) = f(x),
with the Fourier transform.
Exercise 44.3
Solve the Cauchy problem for the one-dimensional heat equation in the domain −∞< x < ∞, t > 0,
ut = κuxx,
u(x, 0) = f(x),
with the Laplace transform.
Exercise 44.4
1. In Exercise ?? above, let f(−x) = −f(x) for all x and verify that φ(x, t) so obtained is the solution, for x > 0,
of the following problem: ﬁnd φ(x, t) satisfying
φt = a2φxx
in 0 < x < ∞, t > 0, with boundary condition φ(0, t) = 0 and initial condition φ(x, 0) = f(x). This technique,
in which the solution for a semi-inﬁnite interval is obtained from that for an inﬁnite interval, is an example of
what is called the method of images.
1903

2. How would you modify the result of part (a) if the boundary condition φ(0, t) = 0 was replaced by φx(0, t) = 0?
Exercise 44.5
Solve the Cauchy problem for the one-dimensional wave equation in the domain −∞< x < ∞, t > 0,
utt = c2uxx,
u(x, 0) = f(x),
ut(x, 0) = g(x),
with the Fourier transform.
Exercise 44.6
Solve the Cauchy problem for the one-dimensional wave equation in the domain −∞< x < ∞, t > 0,
utt = c2uxx,
u(x, 0) = f(x),
ut(x, 0) = g(x),
with the Laplace transform.
Exercise 44.7
Consider the problem of determining φ(x, t) in the region 0 < x < ∞, 0 < t < ∞, such that
φt = a2φxx,
(44.1)
with initial and boundary conditions
φ(x, 0) = 0
for all x > 0,
φ(0, t) = f(t)
for all t > 0,
where f(t) is a given function.
1. Obtain the formula for the Laplace transform of φ(x, t), Φ(x, s) and use the convolution theorem for Laplace
transforms to show that
φ(x, t) =
x
2a√π
Z t
0
f(t −τ) 1
τ 3/2 exp

−x2
4a2τ

dτ.
1904

2. Discuss the special case obtained by setting f(t) = 1 and also that in which f(t) = 1 for 0 < t < T, with
f(t) = 0 for t > T. Here T is some positive constant.
Exercise 44.8
Solve the radiating half space problem:
ut = κuxx,
x > 0,
t > 0,
ux(0, t) −αu(0, t) = 0,
u(x, 0) = f(x).
To do this, deﬁne
v(x, t) = ux(x, t) −αu(x, t)
and ﬁnd the half space problem that v satisﬁes. Solve this problem and then show that
u(x, t) = −
Z ∞
x
e−α(ξ−x) v(ξ, t) dξ.
Exercise 44.9
Show that
Z ∞
0
ω e−cω2 sin(ωx) dω = x√π
4c3/2 e−x2/(4c) .
Use the sine transform to solve:
ut = uxx,
x > 0,
t > 0,
u(0, t) = g(t),
u(x, 0) = 0.
Exercise 44.10
Use the Fourier sine transform to ﬁnd the steady state temperature u(x, y) in a slab: x ≥0, 0 ≤y ≤1, which has zero
temperature on the faces y = 0 and y = 1 and has a given distribution: u(y, 0) = f(y) on the edge x = 0, 0 ≤y ≤1.
Exercise 44.11
Find a harmonic function u(x, y) in the upper half plane which takes on the value g(x) on the x-axis. Assume that u
and ux vanish as |x| →∞. Use the Fourier transform with respect to x. Express the solution as a single integral by
using the convolution formula.
1905

Exercise 44.12
Find the bounded solution of
ut = κuxx −a2u,
0 < x < ∞, t > 0,
−ux(0, t) = f(t),
u(x, 0) = 0.
Exercise 44.13
The left end of a taut string of length L is displaced according to u(0, t) = f(t). The right end is ﬁxed, u(L, t) = 0.
Initially the string is at rest with no displacement. If c is the wave speed for the string, ﬁnd it’s motion for all t > 0.
Exercise 44.14
Let ∇2φ = 0 in the (x, y)-plane region deﬁned by 0 < y < l, −∞< x < ∞, with φ(x, 0) = δ(x −ξ), φ(x, l) = 0, and
φ →0 as |x| →∞. Solve for φ using Fourier transforms. You may leave your answer in the form of an integral but in
fact it is possible to use techniques of contour integration to show that
φ(x, y|ξ) = 1
2l

sin(πy/l)
cosh[π(x −ξ)/l] −cos(πy/l)

.
Note that as l →∞we recover the result derived in class:
φ →1
π
y
(x −ξ)2 + y2,
which clearly approaches δ(x −ξ) as y →0.
1906

44.5
Hints
Hint 44.1
The desired solution form is: u(x, y) =
R ∞
−∞K(x −ξ, y)f(ξ) dξ. You must ﬁnd the correct K. Take the Fourier
transform with respect to x and solve for ˆu(ω, y) recalling that ˆuxx = −ω2ˆu. By ˆuxx we denote the Fourier transform
with respect to x of uxx(x, y).
Hint 44.2
Use the Fourier convolution theorem and the table of Fourier transforms in the appendix.
Hint 44.3
Hint 44.4
Hint 44.5
Use the Fourier convolution theorem. The transform pairs,
F[π(δ(x + τ) + δ(x −τ))] = cos(ωτ),
F[π(H(x + τ) −H(x −τ))] = sin(ωτ)
ω
,
will be useful.
Hint 44.6
Hint 44.7
Hint 44.8
v(x, t) satisﬁes the same partial diﬀerential equation. You can solve the problem for v(x, t) with the Fourier sine
1907

transform. Use the convolution theorem to invert the transform.
To show that
u(x, t) = −
Z ∞
x
e−α(ξ−x) v(ξ, t) dξ,
ﬁnd the solution of
ux −αu = v
that is bounded as x →∞.
Hint 44.9
Note that
Z ∞
0
ω e−cω2 sin(ωx) dω = −∂
∂x
Z ∞
0
e−cω2 cos(ωx) dω.
Write the integral as a Fourier transform.
Take the Fourier sine transform of the heat equation to obtain a ﬁrst order, ordinary diﬀerential equation for ˆu(ω, t).
Solve the diﬀerential equation and do the inversion with the convolution theorem.
Hint 44.10
Hint 44.11
Hint 44.12
Hint 44.13
Hint 44.14
1908

44.6
Solutions
Solution 44.1
1. We take the Fourier transform of the integral equation, noting that the left side is the convolution of u(x) and
1
x2+a2.
2πˆu(ω)F

1
x2 + a2

= F

1
x2 + b2

We ﬁnd the Fourier transform of f(x) =
1
x2+c2. Note that since f(x) is an even, real-valued function, ˆf(ω) is an
even, real-valued function.
F

1
x2 + c2

= 1
2π
Z ∞
−∞
1
x2 + c2 e−ıωx dx
For x > 0 we close the path of integration in the upper half plane and apply Jordan’s Lemma to evaluate the
integral in terms of the residues.
= 1
2πı2π Res

e−ıωx
(x −ıc)(x + ıc), x = ıc

= ıe−ıωıc
ı2c
= 1
2c e−cω
Since ˆf(ω) is an even function, we have
F

1
x2 + c2

= 1
2c e−c|ω| .
Our equation for ˆu(ω) becomes,
2πˆu(ω) 1
2a e−a|ω| = 1
2b e−b|ω|
ˆu(ω) =
a
2πb e−(b−a)|omega| .
1909

We take the inverse Fourier transform using the transform pair we derived above.
u(x) =
a
2πb
2(b −a)
x2 + (b −a)2
u(x) =
a(b −a)
πb(x2 + (b −a)2)
2. We take the Fourier transform of the partial diﬀerential equation and the boundary condtion.
uxx + uyy = 0,
u(x, 0) = f(x)
−ω2ˆu(ω, y) + ˆuyy(ω, y) = 0,
ˆu(ω, 0) = ˆf(ω)
This is an ordinary diﬀerential equation for ˆu in which ω is a parameter. The general solution is
ˆu = c1 eωy +c2 e−ωy .
We apply the boundary conditions that ˆu(ω, 0) = ˆf(ω) and ˆu →0 and y →∞.
ˆu(ω, y) = ˆf(ω) e−ωy
We take the inverse transform using the convolution theorem.
u(x, y) = 1
2π
Z ∞
−∞
e−(x−ξ)y f(ξ) dξ
Solution 44.2
ut = κuxx,
u(x, 0) = f(x),
We take the Fourier transform of the heat equation and the initial condition.
ˆut = −κω2ˆu,
ˆu(ω, 0) = ˆf(ω)
1910

This is a ﬁrst order ordinary diﬀerential equation which has the solution,
ˆu(ω, t) = ˆf(ω) e−κω2t .
Using a table of Fourier transforms we can write this in a form that is conducive to applying the convolution theorem.
ˆu(ω, t) = ˆf(ω)F
r π
κt e−x2/(4κt)

u(x, t) =
1
2
√
πκt
Z ∞
−∞
e−(x−ξ)2/(4κt) f(ξ) dξ
Solution 44.3
We take the Laplace transform of the heat equation.
ut = κuxx
sˆu −u(x, 0) = κˆuxx
ˆuxx −s
κ ˆu = −f(x)
κ
(44.2)
The Green function problem for Equation 44.2 is
G′′ −s
κG = δ(x −ξ),
G(±∞; ξ) is bounded.
The homogeneous solutions that satisfy the left and right boundary conditions are, respectively,
exp
√sa
x

,
exp

−
√sa
x

.
We compute the Wronskian of these solutions.
W =

exp
 √s
a x

exp

−
√s
a x

√s
a exp
 √sa
x

−
√s
a exp

−
√sa
x


= −2
r s
κ
1911

The Green function is
G(x; ξ) = exp
 p s
κx<

exp
 −p s
κx>

−2p s
κ
G(x; ξ) = −
√κ
2√s exp

−
r s
κ|x −ξ|

.
Now we solve Equation 44.2 using the Green function.
ˆu(x, s) =
Z ∞
−∞
−f(ξ)
κ G(x; ξ) dξ
ˆu(x, s) =
1
2√κs
Z ∞
−∞
f(ξ) exp

−
r s
κ|x −ξ|

dξ
Finally we take the inverse Laplace transform to obtain the solution of the heat equation.
u(x, t) =
1
2
√
πκt
Z ∞
−∞
f(ξ) exp

−(x −ξ)2
4κt

dξ
Solution 44.4
1. Clearly the solution satisﬁes the diﬀerential equation. We must verify that it satisﬁes the boundary condition,
1912

φ(0, t) = 0.
φ(x, t) =
1
2a
√
πt
Z ∞
−∞
f(ξ) exp

−(x −ξ)2
4a2t

dξ
φ(x, t) =
1
2a
√
πt
Z 0
−∞
f(ξ) exp

−(x −ξ)2
4a2t

dξ +
1
2a
√
πt
Z ∞
0
f(ξ) exp

−(x −ξ)2
4a2t

dξ
φ(x, t) =
1
2a
√
πt
Z ∞
0
f(−ξ) exp

−(x + ξ)2
4a2t

dξ +
1
2a
√
πt
Z ∞
0
f(ξ) exp

−(x −ξ)2
4a2t

dξ
φ(x, t) = −
1
2a
√
πt
Z ∞
0
f(ξ) exp

−(x + ξ)2
4a2t

dξ +
1
2a
√
πt
Z ∞
0
f(ξ) exp

−(x −ξ)2
4a2t

dξ
φ(x, t) =
1
2a
√
πt
Z ∞
0
f(ξ)

exp

−(x −ξ)2
4a2t

exp

−(x + ξ)2
4a2t

dξ
φ(x, t) =
1
2a
√
πt
Z ∞
0
f(ξ) exp

−x2 + ξ2
4a2t
 
exp
 xξ
2a2t

−exp

−xξ
2a2t

dξ
φ(x, t) =
1
a
√
πt
Z ∞
0
f(ξ) exp

−x2 + ξ2
4a2t

sinh
 xξ
2a2t

dξ
Since the integrand is zero for x = 0, the solution satisﬁes the boundary condition there.
2. For the boundary condition φx(0, t) = 0 we would choose f(x) to be even. f(−x) = f(x). The solution is
φ(x, t) =
1
a
√
πt
Z ∞
0
f(ξ) exp

−x2 + ξ2
4a2t

cosh
 xξ
2a2t

dξ
The derivative with respect to x is
φx(x, t) =
1
2a3√πt3/2
Z ∞
0
f(ξ) exp

−x2 + ξ2
4a2t
 
ξ sinh
 xξ
2a2t

−x cosh
 xξ
2a2t

dξ.
Since the integrand is zero for x = 0, the solution satisﬁes the boundary condition there.
1913

Solution 44.5
utt = c2uxx,
u(x, 0) = f(x),
ut(x, 0) = g(x),
With the change of variables
τ = ct,
∂
∂τ = ∂t
∂τ
∂
∂t = 1
c
∂
∂t,
v(x, τ) = u(x, t),
the problem becomes
vττ = vxx,
v(x, 0) = f(x),
vτ(x, 0) = 1
cg(x).
(This change of variables isn’t necessary, it just gives us fewer constants to carry around.) We take the Fourier transform
in x of the equation and the initial conditions, (we consider τ to be a parameter).
ˆvττ(ω, τ) = −ω2ˆv(ω, τ),
ˆv(ω, τ) = ˆf(ω),
ˆvτ(ω, τ) = 1
c ˆg(ω)
Now we have an ordinary diﬀerential equation for ˆv(ω, τ), (now we consider ω to be a parameter). The general solution
of this constant coeﬃcient diﬀerential equation is,
ˆv(ω, τ) = a(ω) cos(ωτ) + b(ω) sin(ωτ),
where a and b are constants that depend on the parameter ω. We applying the initial conditions to obtain ˆv(ω, τ).
ˆv(ω, τ) = ˆf(ω) cos(ωτ) + 1
cω ˆg(ω) sin(ωτ)
With the Fourier transform pairs
F[π(δ(x + τ) + δ(x −τ))] = cos(ωτ),
F[π(H(x + τ) −H(x −τ))] = sin(ωτ)
ω
,
we can write ˆv(ω, τ) in a form that is conducive to applying the Fourier convolution theorem.
ˆv(ω, τ) = F[f(x)]F[π(δ(x + τ) + δ(x −τ))] + 1
cF[g(x)]F[π(H(x + τ) −H(x −τ))]
1914

v(x, τ) = 1
2π
Z ∞
−∞
f(ξ)π(δ(x −ξ + τ) + δ(x −ξ −τ)) dξ
+ 1
c
1
2π
Z ∞
−∞
g(ξ)π(H(x −ξ + τ) −H(x −ξ −τ)) dξ
v(x, τ) = 1
2(f(x + τ) + f(x −τ)) + 1
2c
Z x+τ
x−τ
g(ξ) dξ
Finally we make the change of variables t = τ/c, u(x, t) = v(x, τ) to obtain D’Alembert’s solution of the wave equation,
u(x, t) = 1
2(f(x −ct) + f(x + ct)) + 1
2c
Z x+ct
x−ct
g(ξ) dξ.
Solution 44.6
With the change of variables
τ = ct,
∂
∂τ = ∂t
∂τ
∂
∂t = 1
c
∂
∂t,
v(x, τ) = u(x, t),
the problem becomes
vττ = vxx,
v(x, 0) = f(x),
vτ(x, 0) = 1
cg(x).
We take the Laplace transform in τ of the equation, (we consider x to be a parameter),
s2V (x, s) −sv(x, 0) −vτ(x, 0) = Vxx(x, s),
Vxx(x, s) −s2V (x, s) = −sf(x) −1
cg(x),
Now we have an ordinary diﬀerential equation for V (x, s), (now we consider s to be a parameter). We impose the
boundary conditions that the solution is bounded at x = ±∞. Consider the Green’s function problem
gxx(x; ξ) −s2g(x; ξ) = δ(x −ξ),
g(±∞; ξ) bounded.
1915

esx is a homogeneous solution that is bounded at x = −∞. e−sx is a homogeneous solution that is bounded at
x = +∞. The Wronskian of these solutions is
W(x) =

esx
e−sx
s esx
−s e−sx
 = −2s.
Thus the Green’s function is
g(x; ξ) =
(
−1
2s esx e−sξ
for x < ξ,
−1
2s esξ e−sx
for x > ξ, = −1
2s e−s|x−ξ| .
The solution for V (x, s) is
V (x, s) = −1
2s
Z ∞
−∞
e−s|x−ξ|(−sf(ξ) −1
cg(ξ)) dξ,
V (x, s) = 1
2
Z ∞
−∞
e−s|x−ξ| f(ξ) dξ + 1
2cs
Z ∞
−∞
e−s|x−ξ| g(ξ)) dξ,
V (x, s) = 1
2
Z ∞
−∞
e−s|ξ| f(x −ξ) dξ + 1
2c
Z ∞
−∞
e−s|ξ|
s
g(x −ξ)) dξ.
Now we take the inverse Laplace transform and interchange the order of integration.
v(x, τ) = 1
2L−1
Z ∞
−∞
e−s|ξ| f(x −ξ) dξ

+ 1
2cL−1
Z ∞
−∞
e−s|ξ|
s
g(x −ξ)) dξ

v(x, τ) = 1
2
Z ∞
−∞
L−1 e−s|ξ|
f(x −ξ) dξ + 1
2c
Z ∞
−∞
L−1
e−s|ξ|
s

g(x −ξ)) dξ
v(x, τ) = 1
2
Z ∞
−∞
δ(τ −|ξ|)f(x −ξ) dξ + 1
2c
Z ∞
−∞
H(τ −|ξ|)g(x −ξ)) dξ
v(x, τ) = 1
2(f(x −τ) + f(x + τ)) + 1
2c
Z τ
−τ
g(x −ξ) dξ
1916

v(x, τ) = 1
2(f(x −τ) + f(x + τ)) + 1
2c
Z −x+τ
−x−τ
g(−ξ) dξ
v(x, τ) = 1
2(f(x −τ) + f(x + τ)) + 1
2c
Z x+τ
x−τ
g(ξ) dξ
Now we write make the change of variables t = τ/c, u(x, t) = v(x, τ) to obtain D’Alembert’s solution of the wave
equation,
u(x, t) = 1
2(f(x −ct) + f(x + ct)) + 1
2c
Z x+ct
x−ct
g(ξ) dξ.
Solution 44.7
1. We take the Laplace transform of Equation 44.1.
sˆφ −φ(x, 0) = a2 ˆφxx
ˆφxx −s
a2 ˆφ = 0
(44.3)
We take the Laplace transform of the initial condition, φ(0, t) = f(t), and use that ˆφ(x, s) vanishes as x →∞
to obtain boundary conditions for ˆφ(x, s).
ˆφ(0, s) = ˆf(s),
ˆφ(∞, s) = 0
The solutions of Equation 44.3 are
exp

±
√s
a x

.
The solution that satisﬁes the boundary conditions is
ˆφ(x, s) = ˆf(s) exp

−
√s
a x

.
1917

We write this as the product of two Laplace transforms.
ˆφ(x, s) = ˆf(s)L

x
2a√πt3/2 exp

−x2
4a2t

We invert using the convolution theorem.
φ(x, t) =
x
2a√π
Z t
0
f(t −τ) 1
τ 3/2 exp

−x2
4a2τ

dτ.
2. Consider the case f(t) = 1.
φ(x, t) =
x
2a√π
Z t
0
1
τ 3/2 exp

−x2
4a2τ

dτ
ξ =
x
2a√τ ,
dξ = −
x
4aτ 3/2
φ(x, t) = −2
√π
Z x/(2a
√
t)
∞
e−ξ2 dξ
φ(x, t) = erfc

x
2a
√
t

Now consider the case in which f(t) = 1 for 0 < t < T, with f(t) = 0 for t > T. For t < T, φ is the same as
before.
φ(x, t) = erfc

x
2a
√
t

,
for 0 < t < T
1918

Consider t > T.
φ(x, t) =
x
2a√π
Z t
t−T
1
τ 3/2 exp

−x2
4a2τ

dτ
φ(x, t) = −2
√π
Z x/(2a
√
t)
x/(2a√t−T)
e−ξ2 dξ
φ(x, t) = erf

x
2a
√
t −T

−erf

x
2a
√
t

Solution 44.8
ut = κuxx,
x > 0,
t > 0,
ux(0, t) −αu(0, t) = 0,
u(x, 0) = f(x).
First we ﬁnd the partial diﬀerential equation that v satisﬁes. We start with the partial diﬀerential equation for u,
ut = κuxx.
Diﬀerentiating this equation with respect to x yields,
utx = κuxxx.
Subtracting α times the former equation from the latter yields,
utx −αut = κuxxx −ακuxx,
∂
∂t (ux −αu) = κ ∂2
∂x2 (ux −αu) ,
vt = κvxx.
1919

Thus v satisﬁes the same partial diﬀerential equation as u. This is because the equation for u is linear and homogeneous
and v is a linear combination of u and its derivatives. The problem for v is,
vt = κvxx,
x > 0,
t > 0,
v(0, t) = 0,
v(x, 0) = f ′(x) −αf(x).
With this new boundary condition, we can solve the problem with the Fourier sine transform. We take the sine transform
of the partial diﬀerential equation and the initial condition.
ˆvt(ω, t) = κ

−ω2ˆv(ω, t) + 1
πωv(0, t)

,
ˆv(ω, 0) = Fs [f ′(x) −αf(x)]
ˆvt(ω, t) = −κω2ˆv(ω, t)
ˆv(ω, 0) = Fs [f ′(x) −αf(x)]
Now we have a ﬁrst order, ordinary diﬀerential equation for ˆv. The general solution is,
ˆv(ω, t) = c e−κω2t .
The solution subject to the initial condition is,
ˆv(ω, t) = Fs [f ′(x) −αf(x)] e−κω2t .
Now we take the inverse sine transform to ﬁnd v. We utilize the Fourier cosine transform pair,
F−1
c
h
e−κω2ti
=
r π
κt e−x2/(4κt),
to write ˆv in a form that is suitable for the convolution theorem.
ˆv(ω, t) = Fs [f ′(x) −αf(x)] Fc
r π
κt e−x2/(4κt)

1920

Recall that the Fourier sine convolution theorem is,
Fs
 1
2π
Z ∞
0
f(ξ) (g(|x −ξ|) −g(x + ξ)) dξ

= Fs[f(x)]Fc[g(x)].
Thus v(x, t) is
v(x, t) =
1
2
√
πκt
Z ∞
0
(f ′(ξ) −αf(ξ))

e−|x−ξ|2/(4κt) −e−(x+ξ)2/(4κt)
dξ.
With v determined, we have a ﬁrst order, ordinary diﬀerential equation for u,
ux −αu = v.
We solve this equation by multiplying by the integrating factor and integrating.
∂
∂x
 e−αx u

= e−αx v
e−αx u =
Z x
e−αξ v(x, t) dξ + c(t)
u =
Z x
e−α(ξ−x) v(x, t) dξ + eαx c(t)
The solution that vanishes as x →∞is
u(x, t) = −
Z ∞
x
e−α(ξ−x) v(ξ, t) dξ.
1921

Solution 44.9
Z ∞
0
ω e−cω2 sin(ωx) dω = −∂
∂x
Z ∞
0
e−cω2 cos(ωx) dω
= −1
2
∂
∂x
Z ∞
−∞
e−cω2 cos(ωx) dω
= −1
2
∂
∂x
Z ∞
−∞
e−cω2+ıωx dω
= −1
2
∂
∂x
Z ∞
−∞
e−c(ω+ıx/(2c))2 e−x2/(4c) dω
= −1
2
∂
∂x e−x2/(4c)
Z ∞
−∞
e−cω2 dω
= −1
2
rπ
c
∂
∂x e−x2/(4c)
= x√π
4c3/2 e−x2/(4c)
ut = uxx,
x > 0,
t > 0,
u(0, t) = g(t),
u(x, 0) = 0.
We take the Fourier sine transform of the partial diﬀerential equation and the initial condition.
ˆut(ω, t) = −ω2ˆu(ω, t) + ω
π g(t),
ˆu(ω, 0) = 0
Now we have a ﬁrst order, ordinary diﬀerential equation for ˆu(ω, t).
∂
∂t

eω2t ˆut(ω, t)

= ω
π g(t) eω2t
ˆu(ω, t) = ω
π e−ω2t
Z t
0
g(τ) eω2τ dτ + c(ω) e−ω2t
1922

The initial condition is satisﬁed for c(ω) = 0.
ˆu(ω, t) = ω
π
Z t
0
g(τ) e−ω2(t−τ) dτ
We take the inverse sine transform to ﬁnd u.
u(x, t) = F−1
s
ω
π
Z t
0
g(τ) e−ω2(t−τ) dτ

u(x, t) =
Z t
0
g(τ)F−1
s
hω
π e−ω2(t−τ)i
dτ
u(x, t) =
Z t
0
g(τ)
x
2√π(t −τ)3/2 e−x2/(4(t−τ)) dτ
u(x, t) =
x
2√π
Z t
0
g(τ)e−x2/(4(t−τ))
(t −τ)3/2 dτ
Solution 44.10
The problem is
uxx + uyy = 0,
0 < x, 0 < y < 1,
u(x, 0) = u(x, 1) = 0,
u(0, y) = f(y).
We take the Fourier sine transform of the partial diﬀerential equation and the boundary conditions.
−ω2ˆu(ω, y) + k
πu(0, y) + ˆuyy(ω, y) = 0
ˆuyy(ω, y) −ω2ˆu(ω, y) = −k
πf(y),
ˆu(ω, 0) = ˆu(ω, 1) = 0
1923

This is an inhomogeneous, ordinary diﬀerential equation that we can solve with Green functions. The homogeneous
solutions are
{cosh(ωy), sinh(ωy)}.
The homogeneous solutions that satisfy the left and right boundary conditions are
y1 = sinh(ωy),
y2 = sinh(ω(y −1)).
The Wronskian of these two solutions is,
W(x) =

sinh(ωy)
sinh(ω(y −1))
ω cosh(ωy)
ω cosh(ω(y −1))

= ω (sinh(ωy) cosh(ω(y −1)) −cosh(ωy) sinh(ω(y −1)))
= ω sinh(ω).
The Green function is
G(y|η) = sinh(ωy<) sinh(ω(y> −1))
ω sinh(ω)
.
The solution of the ordinary diﬀerential equation for ˆu(ω, y) is
ˆu(ω, y) = −ω
π
Z 1
0
f(η)G(y|η) dη
= −1
π
Z y
0
f(η)sinh(ωη) sinh(ω(y −1))
sinh(ω)
dη −1
π
Z 1
y
f(η)sinh(ωy) sinh(ω(η −1))
sinh(ω)
dη.
With some uninteresting grunge, you can show that,
2
Z ∞
0
sinh(ωη) sinh(ω(y −1))
sinh(ω)
sin(ωx) dω = −2
sin(πη) sin(πy)
(cosh(πx) −cos(π(y −η)))(cosh(πx) −cos(π(y + η))).
1924

Taking the inverse Fourier sine transform of ˆu(ω, y) and interchanging the order of integration yields,
u(x, y) = 2
π
Z y
0
f(η)
sin(πη) sin(πy)
(cosh(πx) −cos(π(y −η)))(cosh(πx) −cos(π(y + η))) dη
+ 2
π
Z 1
y
f(η)
sin(πy) sin(πη)
(cosh(πx) −cos(π(η −y)))(cosh(πx) −cos(π(η + y))) dη.
u(x, y) = 2
π
Z 1
0
f(η)
sin(πη) sin(πy)
(cosh(πx) −cos(π(y −η)))(cosh(πx) −cos(π(y + η))) dη
Solution 44.11
The problem for u(x, y) is,
uxx + uyy = 0,
−∞< x < ∞, y > 0,
u(x, 0) = g(x).
We take the Fourier transform of the partial diﬀerential equation and the boundary condition.
−ω2ˆu(ω, y) + ˆuyy(ω, y) = 0,
ˆu(ω, 0) = ˆg(ω).
This is an ordinary diﬀerential equation for ˆu(ω, y). So far we only have one boundary condition. In order that u
is bounded we impose the second boundary condition ˆu(ω, y) is bounded as y →∞. The general solution of the
diﬀerential equation is
ˆu(ω, y) =
(
c1(ω) eωy +c2(ω) e−ωy,
for ω ̸= 0,
c1(ω) + c2(ω)y,
for ω = 0.
Note that eωy is the bounded solution for ω < 0, 1 is the bounded solution for ω = 0 and e−ωy is the bounded solution
for ω > 0. Thus the bounded solution is
ˆu(ω, y) = c(ω) e−|ω|y .
1925

The boundary condition at y = 0 determines the constant of integration.
ˆu(ω, y) = ˆg(ω) e−|ω|y
Now we take the inverse Fourier transform to obtain the solution for u(x, y). To do this we use the Fourier transform
pair,
F

2c
x2 + c2

= e−c|ω|,
and the convolution theorem,
F
 1
2π
Z ∞
−∞
f(ξ)g(x −ξ) dξ

= ˆf(ω)ˆg(ω).
u(x, y) = 1
2π
Z ∞
−∞
g(ξ)
2y
(x −ξ)2 + y2 dξ.
Solution 44.12
Since the derivative of u is speciﬁed at x = 0, we take the cosine transform of the partial diﬀerential equation and the
initial condition.
ˆut(ω, t) = κ

−ω2ˆu(ω, t) −1
πux(0, t)

−a2ˆu(ω, t),
ˆu(ω, 0) = 0
ˆut +
 κω2 + a2
ˆu = κ
πf(t),
ˆu(ω, 0) = 0
This ﬁrst order, ordinary diﬀerential equation for ˆu(ω, t) has the solution,
ˆu(ω, t) = κ
π
Z t
0
e−(κω2+a2)(t−τ) f(τ) dτ.
1926

We take the inverse Fourier cosine transform to ﬁnd the solution u(x, t).
u(x, t) = κ
πF−1
c
Z t
0
e−(κω2+a2)(t−τ) f(τ) dτ

u(x, t) = κ
π
Z t
0
F−1
c
h
e−κω2(t−τ)i
e−a2(t−τ) f(τ) dτ
u(x, t) = κ
π
Z t
0
r
π
κ(t −τ) e−x2/(4κ(t−τ)) e−a2(t−τ) f(τ) dτ
u(x, t) =
rκ
π
Z t
0
e−x2/(4κ(t−τ))−a2(t−τ)
√t −τ
f(τ) dτ
Solution 44.13
Mathematically stated we have
utt = c2uxx,
0 < x < L,
t > 0,
u(x, 0) = ut(x, 0) = 0,
u(0, t) = f(t),
u(L, t) = 0.
We take the Laplace transform of the partial diﬀerential equation and the boundary conditions.
s2ˆu(x, s) −su(x, 0) −ut(x, 0) = c2ˆuxx(x, s)
ˆuxx = s2
c2 ˆu,
ˆu(0, s) = ˆf(s),
ˆu(L, s) = 0
Now we have an ordinary diﬀerential equation. A set of solutions is
n
cosh
sx
c

, sinh
sx
c
o
.
The solution that satisﬁes the right boundary condition is
ˆu = a sinh
s(L −x)
c

.
1927

The left boundary condition determines the multiplicative constant.
ˆu(x, s) = ˆf(s)sinh(s(L −x)/c)
sinh(sL/c)
If we can ﬁnd the inverse Laplace transform of
ˆu(x, s) = sinh(s(L −x)/c)
sinh(sL/c)
then we can use the convolution theorem to write u in terms of a single integral. We proceed by expanding this function
in a sum.
sinh(s(L −x)/c)
sinh(sL/c)
= es(L−x)/c −e−s(L−x)/c
esL/c −e−sL/c
= e−sx/c −e−s(2L−x)/c
1 −e−2sL/c
=
 e−sx/c −e−s(2L−x)/c
∞
X
n=0
e−2nsL/c
=
∞
X
n=0
e−s(2nL+x)/c −
∞
X
n=0
e−s(2(n+1)L−x)/c
=
∞
X
n=0
e−s(2nL+x)/c −
∞
X
n=1
e−s(2nL−x)/c
Now we use the Laplace transform pair:
L[δ(x −a)] = e−sa .
L−1
sinh(s(L −x)/c)
sinh(sL/c)

=
∞
X
n=0
δ(t −(2nL + x)/c) −
∞
X
n=1
δ(t −(2nL −x)/c)
1928

We write ˆu in the form,
ˆu(x, s) = L[f(t)]L
" ∞
X
n=0
δ(t −(2nL + x)/c) −
∞
X
n=1
δ(t −(2nL −x)/c)
#
.
By the convolution theorem we have
u(x, t) =
Z t
0
f(τ)
 ∞
X
n=0
δ(t −τ −(2nL + x)/c) −
∞
X
n=1
δ(t −τ −(2nL −x)/c)
!
dτ.
We can simplify this a bit. First we determine which Dirac delta functions have their singularities in the range τ ∈(0..t).
For the ﬁrst sum, this condition is
0 < t −(2nL + x)/c < t.
The right inequality is always satisﬁed. The left inequality becomes
(2nL + x)/c < t,
n < ct −x
2L .
For the second sum, the condition is
0 < t −(2nL −x)/c < t.
Again the right inequality is always satisﬁed. The left inequality becomes
n < ct + x
2L .
We change the index range to reﬂect the nonzero contributions and do the integration.
u(x, t) =
Z t
0
f(τ)


⌊ct−x
2L ⌋
X
n=0
δ(t −τ −(2nL + x)/c)
⌊ct+x
2L ⌋
X
n=1
δ(t −τ −(2nL −x)/c)

dτ.
1929

u(x, t) =
⌊ct−x
2L ⌋
X
n=0
f(t −(2nL + x)/c)
⌊ct+x
2L ⌋
X
n=1
f(t −(2nL −x)/c)
Solution 44.14
We take the Fourier transform of the partial diﬀerential equation and the boundary conditions.
−ω2 ˆφ + ˆφyy = 0,
ˆφ(ω, 0) = 1
2π e−ıωξ,
ˆφ(ω, l) = 0
We solve this boundary value problem.
ˆφ(ω, y) = c1 cosh(ω(l −y)) + c2 sinh(ω(l −y))
ˆφ(ω, y) = 1
2π e−ıωξ sinh(ω(l −y))
sinh(ωl)
We take the inverse Fourier transform to obtain an expression for the solution.
φ(x, y) = 1
2π
Z ∞
−∞
eıω(x−ξ) sinh(ω(l −y))
sinh(ωl)
dω
1930

Chapter 45
Green Functions
45.1
Inhomogeneous Equations and Homogeneous Boundary Con-
ditions
Consider a linear diﬀerential equation on the domain Ωsubject to homogeneous boundary conditions.
L[u(x)] = f(x)
for x ∈Ω,
B[u(x)] = 0
for x ∈∂Ω
(45.1)
For example, L[u] might be
L[u] = ut −κ∆u,
or
L[u] = utt −c2∆u.
and B[u] might be u = 0, or ∇u · ˆn = 0.
If we ﬁnd a Green function G(x; ξ) that satisﬁes
L[G(x; ξ)] = δ(x −ξ),
B[G(x; ξ)] = 0
then the solution to Equation 45.1 is
u(x) =
Z
Ω
G(x; ξ)f(ξ) dξ.
1931

We verify that this solution satisﬁes the equation and boundary condition.
L[u(x)] =
Z
Ω
L[G(x; ξ)]f(ξ) dξ
=
Z
Ω
δ(x −ξ)f(ξ) dξ
= f(x)
B[u(x)] =
Z
Ω
B[G(x; ξ)]f(ξ) dξ
=
Z
Ω
0 f(ξ) dξ
= 0
45.2
Homogeneous Equations and Inhomogeneous Boundary Con-
ditions
Consider a homogeneous linear diﬀerential equation on the domain Ωsubject to inhomogeneous boundary conditions,
L[u(x)] = 0
for x ∈Ω,
B[u(x)] = h(x)
for x ∈∂Ω.
(45.2)
If we ﬁnd a Green function g(x; ξ) that satisﬁes
L[g(x; ξ)] = 0,
B[g(x; ξ)] = δ(x −ξ)
then the solution to Equation 45.2 is
u(x) =
Z
∂Ω
g(x; ξ)h(ξ) dξ.
1932

We verify that this solution satisﬁes the equation and boundary condition.
L[u(x)] =
Z
∂Ω
L[g(x; ξ)]h(ξ) dξ
=
Z
∂Ω
0 h(ξ) dξ
= 0
B[u(x)] =
Z
∂Ω
B[g(x; ξ)]h(ξ) dξ
=
Z
∂Ω
δ(x −ξ)h(ξ) dξ
= h(x)
Example 45.2.1 Consider the Cauchy problem for the homogeneous heat equation.
ut = κuxx,
−∞< x < ∞,
t > 0
u(x, 0) = h(x),
u(±∞, t) = 0
We ﬁnd a Green function that satisﬁes
gt = κgxx,
−∞< x < ∞,
t > 0
g(x, 0; ξ) = δ(x −ξ),
g(±∞, t; ξ) = 0.
Then we write the solution
u(x, t) =
Z ∞
−∞
g(x, t; ξ)h(ξ) dξ.
To ﬁnd the Green function for this problem, we apply a Fourier transform to the equation and boundary condition
1933

for g.
ˆgt = −κω2ˆg,
ˆg(ω, 0; ξ) = F[δ(x −ξ)]
ˆg(ω, t; ξ) = F[δ(x −ξ)] e−κω2t
ˆg(ω, t; ξ) = F[δ(x −ξ)]F
r π
κt exp

−x2
4κt

We invert using the convolution theorem.
g(x, t; ξ) = 1
2π
Z ∞
−∞
δ(ψ −ξ)
r π
κt exp

−(x −ψ)2
4κt

dψ
=
1
√
4πκt exp

−(x −ξ)2
4κt

The solution of the heat equation is
u(x, t) =
1
√
4πκt
Z ∞
−∞
exp

−(x −ξ)2
4κt

h(ξ) dξ.
45.3
Eigenfunction Expansions for Elliptic Equations
Consider a Green function problem for an elliptic equation on a ﬁnite domain.
L[G] = δ(x −ξ),
x ∈Ω
(45.3)
B[G] = 0,
x ∈∂Ω
Let the set of functions {φn} be orthonormal and complete on Ω. (Here n is the multi-index n = n1, . . . , nd.)
Z
Ω
φn(x)φm(x) dx = δnm
1934

In addition, let the φn be eigenfunctions of L subject to the homogeneous boundary conditions.
L [φn] = λnφn,
B [φn] = 0
We expand the Green function in the eigenfunctions.
G =
X
n
gnφn(x)
Then we expand the Dirac Delta function.
δ(x −ξ) =
X
n
dnφn(x)
dn =
Z
Ω
φn(x)δ(x −ξ) dx
dn = φn(ξ)
We substitute the series expansions for the Green function and the Dirac Delta function into Equation 45.3.
X
n
gnλnφn(x) =
X
n
φn(ξ)φn(x)
We equate coeﬃcients to solve for the gn and hence determine the Green function.
gn = φn(ξ)
λn
G(x; ξ) =
X
n
φn(ξ)φn(x)
λn
Example 45.3.1 Consider the Green function for the reduced wave equation, ∆u −k2u in the rectangle, 0 ≤x ≤a,
0 ≤y ≤b, and vanishing on the sides.
1935

First we ﬁnd the eigenfunctions of the operator L = ∆−k2 = 0. Note that φ = X(x)Y (y) is an eigenfunction of
L if X is an eigenfunction of
∂2
∂x2 and Y is an eigenfunction of
∂2
∂y2. Thus we consider the two regular Sturm-Liouville
eigenvalue problems:
X′′ = λX,
X(0) = X(a) = 0
Y ′′ = λY,
Y (0) = Y (b) = 0
This leads us to the eigenfunctions
φmn = sin
mπx
a

sin
nπy
b

.
We use the orthogonality relation
Z 2π
0
sin
mπx
a

sin
nπx
a

dx = a
2δmn
to make the eigenfunctions orthonormal.
φmn =
2
√
ab
sin
mπx
a

sin
nπy
b

,
m, n ∈Z+
The φmn are eigenfunctions of L.
L [φmn] = −
mπ
a
2
+
nπ
b
2
+ k2

φmn
By expanding the Green function and the Dirac Delta function in the φmn and substituting into the diﬀerential equation
we obtain the solution.
G =
∞
X
m,n=1
2
√
ab sin
  mπξ
a

sin
  nπψ
b

2
√
ab sin
  mπx
a

sin
  nπy
b

−
  mπ
a
2 +
  nπ
b
2 + k2

G(x, y; ξ, ψ) = −4ab
∞
X
m,n=1
sin
  mπx
a

sin
  mπξ
a

sin
  nπy
b

sin
  nπψ
b

(mπb)2 + (nπa)2 + (kab)2
1936

Example 45.3.2 Consider the Green function for Laplace’s equation, ∆u = 0 in the disk, |r| < a, and vanishing at
r = a.
First we ﬁnd the eigenfunctions of the operator
∆= ∂2
∂r2 + 1
r
∂
∂r + 1
r2
∂2
∂θ2.
We will look for eigenfunctions of the form φ = Θ(θ)R(r). We choose the Θ to be eigenfunctions of
d2
dθ2 subject to
the periodic boundary conditions in θ.
Θ′′ = λΘ,
Θ(0) = Θ(2π),
Θ′(0) = Θ′(2π)
Θn = einθ,
n ∈Z
We determine R(r) by requiring that φ be an eigenfunction of ∆.
∆φ = λφ
(ΘnR)rr + 1
r(ΘnR)r + 1
r2(ΘnR)θθ = λΘnR
ΘnR′′ + 1
rΘnR′ + 1
r2(−n2)ΘnR = λΘR
For notational convenience, we denote λ = −µ2.
R′′ + 1
rR′ +

µ2 −n2
r2

R = 0,
R(0) bounded,
R(a) = 0
The general solution for R is
R = c1Jn(µr) + c2Yn(µr).
The left boundary condition demands that c2 = 0. The right boundary condition determines the eigenvalues.
Rnm = Jn
jn,mr
a

,
µnm = jn,m
a
1937

Here jn,m is the mth positive root of Jn. This leads us to the eigenfunctions
φnm = einθ Jn
jn,mr
a

We use the orthogonality relations
Z 2π
0
e−imθ einθ dθ = 2πδmn,
Z 1
0
rJν(jν,mr)Jν(jν,nr) dr = 1
2 (J′
ν(jν,n))2 δmn
to make the eigenfunctions orthonormal.
φnm =
1
√πa|J′n(jn,m)| einθ Jn
jn,mr
a

,
n ∈Z,
m ∈Z+
The φnm are eigenfunctions of L.
∆φnm = −
jn,m
a
2
φnm
By expanding the Green function and the Dirac Delta function in the φnm and substituting into the diﬀerential equation
we obtain the solution.
G =
∞
X
n=−∞
∞
X
m=1
1
√πa|J′n(jn,m)| e−inϑ Jn

jn,mρ
a

1
√πa|J′n(jn,m)| einθ Jn

jn,mr
a

−

jn,m
a
2
G(r, θ; ρ, ϑ) = −
∞
X
n=−∞
∞
X
m=1
1
π(jn,mJ′n(jn,m))2 ein(θ−ϑ) Jn
jn,mρ
a

Jn
jn,mr
a

1938

45.4
The Method of Images
Consider Poisson’s equation in the upper half plane.
∇2u = f(x, y),
−∞< x < ∞,
y > 0
u(x, 0) = 0,
u(x, y) →0 as x2 + y2 →∞
The associated Green function problem is
∇2G = δ(x −ξ)δ(y −ψ),
−∞< x < ∞,
y > 0
G(x, 0|ξ, ψ) = 0,
G(x, y|ξ, ψ) →0 as x2 + y2 →∞.
We will solve the Green function problem with the method of images. We expand the domain to include the lower
half plane. We place a negative image of the source in the lower half plane. This will make the Green function odd
about y = 0, i.e. G(x, 0|ξ, ψ) = 0.
∇2G = δ(x −ξ)δ(y −ψ) −δ(x −ξ)δ(y + ψ),
−∞< x < ∞,
y > 0
G(x, y|ξ, ψ) →0 as x2 + y2 →∞
Recall that the inﬁnite space Green function which satisﬁes ∆F = δ(x −ξ)δ(y −ψ) is
F(x, y|ξ, ψ) = 1
4π ln
 (x −ξ)2 + (y −ψ)2
.
We solve for G by using the inﬁnite space Green function.
G = F(x, y|ξ, ψ) −F(x, y|ξ, −ψ)
= 1
4π ln
 (x −ξ)2 + (y −ψ)2
−1
4π ln
 (x −ξ)2 + (y + ψ)2
= 1
4π ln
(x −ξ)2 + (y −ψ)2
(x −ξ)2 + (y + ψ)2

1939

We write the solution of Poisson’s equation using the Green function.
u(x, y) =
Z ∞
0
Z ∞
−∞
G(x, y|ξ, ψ)f(ξ, ψ) dξ dψ
u(x, y) =
Z ∞
0
Z ∞
−∞
1
4π ln
(x −ξ)2 + (y −ψ)2
(x −ξ)2 + (y + ψ)2

f(ξ, ψ) dξ dψ
1940

45.5
Exercises
Exercise 45.1
Consider the Cauchy problem for the diﬀusion equation with a source.
ut −κuxx = s(x, t),
u(x, 0) = f(x),
u →0 as x →±∞
Find the Green function for this problem and use it to determine the solution.
Exercise 45.2
Consider the 2-dimensional wave equation
utt −c2(uxx + uyy) = 0.
1. Determine the fundamental solution for this equation. (i.e. response to source at t = τ, x = ξ). You may ﬁnd
the following information useful about the Bessel function:
J0(x) = 1
π
Z π
0
eıx cos θ dθ,
Z ∞
0
J0(ax) sin(bx) dx =
(
0,
0 < b < a
1
√
b2−a2,
0 < a < b
2. Use the “method of descents” to recover the 1-D fundamental solution.
Exercise 45.3
Consider the linear wave equation
utt = c2uxx,
with constant c, on the inﬁnite domain −∞< x < ∞.
1. By using the Fourier transform ﬁnd the solution of Gtt = c2Gxx subject to initial conditions G(x, 0) = 0,
Gt(x, 0) = δ(x −ξ).
1941

2. Now use this to ﬁnd u in the case where c = 1, u(x, 0) = 0, and
ut(x, 0) =
(
0
|x| > 1
1 −|x|
|x| < 1
Sketch the solution in x for ﬁxed times t < 1 and t > 1 and also indicate on the x, t (t > 0) plane the regions of
qualitatively diﬀerent behavior of u.
Exercise 45.4
Consider a generalized Laplace equation with non-constant coeﬃcients of the form:
∇2u + A(x) · ∇u + h(x)u = q(x),
on a region V with u = 0 on the boundary S. Suppose we ﬁnd a Green function which satisﬁes
∇2G + A(x) · ∇G + h(x)G = δ(x −ξ).
Use the divergence theorem to derive an appropriate generalized Green’s identity and show that
u(ξ) ̸=
Z
V
G(x|ξ)q(x) dx.
What equation should the Green function satisfy? Note: this equation is called the adjoint of the original partial
diﬀerential equation.
Exercise 45.5
Consider Laplace’s equation in the inﬁnite three dimensional domain with two sources of equal strength C, opposite
sign and separated by a distance ϵ.
∇2u = Cδ(x −ξ+) −Cδ(x −ξ−),
where ξ± = (± ϵ
2, 0, 0).
1. Find the solution in terms of the fundamental solutions.
1942

2. Now consider the limit in which the distance between sources goes to zero (ϵ →0) and the strength increases in
such a way that Cϵ = D remains ﬁxed. Show that the solution can be written
u = −Dx
4πr3,
where r = |x|. This is called the response to a dipole located at the origin, with strength D, and oriented in the
positive x direction.
3. Show that in general the response to a unit (D = 1) dipole at an arbitrary point ξ0 and oriented in the direction
of the unit vector a is
u(x) = −1
4π∇ξ

1
|x −ξ|

ξ=ξ0
· ⃗a
Exercise 45.6
Consider Laplace’s equation
∇2u = 0,
inside the unit circle with boundary condition u = f(θ). By using the Green function for the Dirichlet problem on the
circle:
G(x|ξ) = 1
2π ln

|x −ξ|
|ξ||x −ξ∗|

,
where ξ and ξ∗have the same polar angle and |ξ∗| =
1
|ξ|, show that the solution may be expressed in polar coordinates
as
u(r, θ) = 1 −r2
2π
Z 2π
0
f(ϑ)
1 + r2 −2r cos(θ −ϑ) dϑ.
Exercise 45.7
Consider an alternate derivation of the fundamental solution of Laplace’s equation
∇2u = δ(x),
with u →0 as |x| →∞in three dimensions.
1943

1. Convert this equation to spherical coordinates. You may deﬁne a new delta function
δ3(r) = δ(x)δ(y)δ(z)
such that
Z
B
δ3(r) dV =
(
1
if B contains the origin
0
otherwise
2. Show, by symmetry, that this can be reduced to an ordinary diﬀerential equation. Solve to ﬁnd the general
solution of the homogeneous equation. Now determine the constants by using the constraint that u →0 as
|x| →∞, and by integrating the partial diﬀerential equation over a small ball around the origin (and using Gauss’
theorem).
3. Now use similar ideas to re-derive the fundamental solution in two dimensions.
Can we still say u →0 as
|x| →∞? Use instead the constraint that u = 0 when |x| = 1.
4. Finally derive the 2-D solution from the 3-D one using the “method of descent”. Consider Laplace’s equation in
three dimensions with a line source at x = 0, y = 0, −∞< z < ∞,
uxx + uyy + uzz = δ(x)δ(y).
Use the fundamental solution to ﬁnd u(r) where r =
p
x2 + y2, and without loss of generality we have taken
the plane at z = 0. Then evaluate this integral to ﬁnd u. (Hint: ﬁrst try to compute ur)
Exercise 45.8
Consider the heat equation on the bounded domain 0 < x < L with ﬁxed temperature at each end. Use Laplace
transforms to determine the Green Function which satisﬁes
Gt −νGxx = δ(x −ξ)δ(t),
G(0, t) = 0
G(L, t) = 0,
G(x, 0−) = 0.
1. First show that
L[G(x, t)] = cosh
 p s
ν(L −x> + x<)

−cosh
 p s
ν(L −x> −x<)

2√νs sinh
 p s
νL

1944

2. Show that this can be re-written as
L[G(x, t)] =
∞
X
k=−∞
1
2√νs e−√s
ν |x−ξ−2kL| −
1
2√νs e−√s
ν |x+ξ−2kL| .
3. Use this to ﬁnd G in terms of fundamental solutions
f(x, t) =
1
2
√
νπt
e−x2
4νt,
and comment on how this Green’s function corresponds to “real” and “image” sources. Additionally compare
this to the alternative expression,
G(x, t) = 2
L
∞
X
n=1
e−νn2π2
L2
t sin nπx
L sin nπξ
L ,
and comment on the convergence of the respective formulations for small and large time.
Exercise 45.9
Consider the Green function for the 1-D heat equation
Gt −νGxx = δ(x −ξ)δ(t −τ),
on the semi-inﬁnite domain with insulated end
Gx(0, t) = 0,
G →0 as x →∞,
and subject to the initial condition
G(x, τ −) = 0.
1. Solve for G with the Fourier cosine transform.
1945

2. (15 points) Relate this to the fundamental solution on the inﬁnite domain, and discuss in terms of responses to
“real” and “image” sources. Give the solution for x > 0 of
ut −νuxx = q(x, t),
ux(0, t) = 0,
u →0 as x →∞,
u(x, 0) = f(x).
Exercise 45.10
Consider the heat equation
ut = νuxx + δ(x −ξ)δ(t),
on the inﬁnite domain −∞< x < ∞, where we assume u →0 as x →±∞and initially u(x, 0−) = 0.
1. First convert this to a problem where there is no forcing, so that
ut = νuxx
with an appropriately modiﬁed initial condition.
2. Now use Laplace tranforms to convert this to an ordinary diﬀerential equation in ˆu(x, s), where ˆu(x, s) =
L[u(x, t)]. Solve this ordinary diﬀerential equation and show that
ˆu(x, s) =
1
2√νse−√s
ν |x−ξ|.
Recall ˆf(s) = L[f(t)] =
R ∞
0 e−st f(t) dt.
3. Finally use the Laplace inversion formula and Cauchy’s Theorem on an appropriate contour to compute u(x, t).
Recall
f(t) = L−1[F(s)] =
1
ı2π
Z
Γ
F(s) est ds,
where Γ is the Bromwich contour (s = a + ıt where t ∈(−∞. . . ∞) and a is a non-negative constant such that
the contour lies to the right of all poles of ˆf).
1946

Exercise 45.11
Derive the causal Green function for the one dimensional wave equation on (−∞..∞). That is, solve
Gtt −c2Gxx = δ(x −ξ)δ(t −τ),
G(x, t; ξ, τ) = 0
for t < τ.
Use the Green function to ﬁnd the solution of the following wave equation with a source term.
utt −c2uxx = q(x, t),
u(x, 0) = ut(x, 0) = 0
Exercise 45.12
By reducing the problem to a series of one dimensional Green function problems, determine G(x, ξ) if
∇2G = δ(x −ξ)
(a) on the rectangle 0 < x < L, 0 < y < H and
G(0, y; ξ, ψ) = Gx(L, y; ξ, ψ) = Gy(x, 0; ξ, ψ) = Gy(x, H; ξ, ψ) = 0
(b) on the box 0 < x < L, 0 < y < H, 0 < z < W with G = 0 on the boundary.
(c) on the semi-circle 0 < r < a, 0 < θ < π with G = 0 on the boundary.
(d) on the quarter-circle 0 < r < a, 0 < θ < π/2 with G = 0 on the straight sides and Gr = 0 at r = a.
Exercise 45.13
Using the method of multi-dimensional eigenfunction expansions, determine G(x, x0) if
∇2G = δ(x −x0)
and
1947

(a) on the rectangle (0 < x < L, 0 < y < H)
at x = 0,
G = 0
at y = 0,
∂G
∂y = 0
at x = L,
∂G
∂x = 0
at y = H,
∂G
∂y = 0
(b) on the rectangular shaped box (0 < x < L, 0 < y < H, 0 < z < W) with G = 0 on the six sides.
(c) on the semi-circle (0 < r < a, 0 < θ < π) with G = 0 on the entire boundary.
(d) on the quarter-circle (0 < r < a, 0 < θ < π/2) with G = 0 on the straight sides and ∂G/∂r = 0 at r = a.
Exercise 45.14
Using the method of images solve
∇2G = δ(x −x0)
in the ﬁrst quadrant (x ≥0 and y ≥0) with G = 0 at x = 0 and ∂G/∂y = 0 at y = 0. Use the Green function to
solve in the ﬁrst quadrant
∇2u = 0
u(0, y) = g(y)
∂u
∂y (x, 0) = h(x).
1948

Exercise 45.15
Consider the wave equation deﬁned on the half-line x > 0:
∂2u
∂t2 = c2∂2u
∂x2 + Q(x, t),
u(x, 0) = f(x)
∂u
∂t (x, 0) = g(x)
u(0, t) = h(t)
(a) Determine the appropriate Green’s function using the method of images.
(b) Solve for u(x, t) if Q(x, t) = 0, f(x) = 0, and g(x) = 0.
(c) For what values of t does h(t) inﬂuence u(x1, t1). Interpret this result physically.
Exercise 45.16
Derive the Green functions for the one dimensional wave equation on (−∞..∞) for non-homogeneous initial conditions.
Solve the two problems
gtt −c2gxx = 0,
g(x, 0; ξ, τ) = δ(x −ξ),
gt(x, 0; ξ, τ) = 0,
γtt −c2γxx = 0,
γ(x, 0; ξ, τ) = 0,
γt(x, 0; ξ, τ) = δ(x −ξ),
using the Fourier transform.
Exercise 45.17
Use the Green functions from Problem 45.11 and Problem 45.16 to solve
utt −c2uxx = f(x, t),
x > 0,
−∞< t < ∞
u(x, 0) = p(x),
ut(x, 0) = q(x).
Use the solution to determine the domain of dependence of the solution.
1949

Exercise 45.18
Show that the Green function for the reduced wave equation, ∆u −k2u = 0 in the rectangle, 0 ≤x ≤a, 0 ≤y ≤b,
and vanishing on the sides is:
G(x, y; ξ, ψ) = 2
a
∞
X
n=1
sinh(σny<) sinh(σn(y> −b))
σn sinh(σnb)
sin
nπx
a

sin
nπξ
a

,
where
σn =
r
k2 + n2π2
a2 .
Exercise 45.19
Find the Green function for the reduced wave equation ∆u −k2u = 0, in the quarter plane: 0 < x < ∞, 0 < y < ∞
subject to the mixed boundary conditions:
u(x, 0) = 0,
ux(0, y) = 0.
Find two distinct integral representations for G(x, y; ξ, ψ).
Exercise 45.20
Show that in polar coordinates the Green function for ∆u = 0 in the inﬁnite sector, 0 < θ < α, 0 < r < ∞, and
vanishing on the sides is given by,
G(r, θ, ρ, ϑ) = 1
4π ln


cosh

π
α ln r
ρ

−cos
  π
α(θ −ϑ)

cosh

π
α ln r
ρ

−cos
  π
α(θ + ϑ)


.
Use this to ﬁnd the harmonic function u(r, θ) in the given sector which takes on the boundary values:
u(r, θ) = u(r, α) =
(
0
for r < c
1
for r > c.
1950

Exercise 45.21
The Green function for the initial value problem,
ut −κuxx = 0,
u(x, 0) = f(x),
on −∞< x < ∞is
G(x, t; ξ) =
1
√
4πκt
e−(x−ξ)2/(4κt) .
Use the method of images to ﬁnd the corresponding Green function for the mixed initial-boundary problems:
1. ut = κuxx,
u(x, 0) = f(x) for x > 0,
u(0, t) = 0,
2. ut = κuxx,
u(x, 0) = f(x) for x > 0,
ux(0, t) = 0.
Exercise 45.22
Find the Green function (expansion) for the one dimensional wave equation utt −c2uxx = 0 on the interval 0 < x < L,
subject to the boundary conditions:
a)
u(0, t) = ux(L, t) = 0,
b)
ux(0, t) = ux(L, t) = 0.
Write the ﬁnal forms in terms showing the propagation properties of the wave equation, i.e., with arguments ((x±ξ)±
(t −τ)).
Exercise 45.23
Solve, using the above determined Green function,
utt −c2uxx = 0,
0 < x < 1,
t > 0,
ux(0, t) = ux(1, t) = 0,
u(x, 0) = x2(1 −x)2,
ut(x, 0) = 1.
For c = 1, ﬁnd u(x, t) at x = 3/4, t = 7/2.
1951

45.6
Hints
Hint 45.1
Hint 45.2
Hint 45.3
Hint 45.4
Hint 45.5
Hint 45.6
Hint 45.7
Hint 45.8
Hint 45.9
Hint 45.10
1952

Hint 45.11
Hint 45.12
Take a Fourier transform in x. This will give you an ordinary diﬀerential equation Green function problem for ˆG. Find
the continuity and jump conditions at t = τ. After solving for ˆG, do the inverse transform with the aid of a table.
Hint 45.13
Hint 45.14
Hint 45.15
Hint 45.16
Hint 45.17
Hint 45.18
Use Fourier sine and cosine transforms.
Hint 45.19
The the conformal mapping z = wπ/α to map the sector to the upper half plane. The new problem will be
Gxx + Gyy = δ(x −ξ)δ(y −ψ),
−∞< x < ∞,
0 < y < ∞,
G(x, 0, ξ, ψ) = 0,
G(x, y, ξ, ψ) →0 as x, y →∞.
Solve this problem with the image method.
1953

Hint 45.20
Hint 45.21
Hint 45.22
1954

45.7
Solutions
Solution 45.1
The Green function problem is
Gt −κGxx = δ(x −ξ)δ(t −τ),
G(x, t|ξ, τ) = 0 for t < τ,
G →0 as x →±∞
We take the Fourier transform of the diﬀerential equation.
ˆGt + κω2 ˆG = F[δ(x −ξ)]δ(t −τ),
ˆG(ω, t|ξ, τ) = 0 for t < τ
Now we have an ordinary diﬀerential equation Green function problem for ˆG. The homogeneous solution of the ordinary
diﬀerential equation is
e−κω2t
The jump condition is
ˆG(ω, 0; ξ, τ +) = F[δ(x −ξ)].
We write the solution for ˆG and invert using the convolution theorem.
ˆG = F[δ(x −ξ)] e−κω2(t−τ) H(t −τ)
ˆG = F[δ(x −ξ)]F
r
π
κ(t −τ) e−x2/(4κ(t−τ))

H(t −τ)
G = 1
2π
Z ∞
−∞
δ(x −y −ξ)
r
π
κ(t −τ) e−y2/(4κ(t−τ)) dyH(t −τ)
G =
1
p
4πκ(t −τ)
e−(x−ξ)2/(4κ(t−τ)) H(t −τ)
We write the solution of the diﬀusion equation using the Green function.
u =
Z ∞
0
Z ∞
−∞
G(x, t|ξ, τ)s(ξ, τ) dξ dτ +
Z ∞
−∞
G(x, t|ξ, 0)f(ξ) dξ
u =
Z t
0
1
p
4πκ(t −τ)
Z ∞
−∞
e−(x−ξ)2/(4κ(t−τ)) s(ξ, τ) dξ dτ +
1
√
4πκt
Z ∞
−∞
e−(x−ξ)2/(4κt) f(ξ) dξ
1955

Solution 45.2
1. We apply Fourier transforms in x and y to the Green function problem.
Gtt −c2(Gxx + Gyy) = δ(t −τ)δ(x −ξ)δ(y −η)
ˆˆGtt + c2  α2 + β2 ˆˆG = δ(t −τ) 1
2π e−ıαξ 1
2π e−ıβη
This gives us an ordinary diﬀerential equation Green function problem for ˆˆG(α, β, t). We ﬁnd the causal solution.
That is, the solution that satisﬁes ˆˆG(α, β, t) = 0 for t < τ.
ˆˆG =
sin
p
α2 + β2c(t −τ)

c
p
α2 + β2
1
4π2 e−ı(αξ+βη) H(t −τ)
Now we take inverse Fourier transforms in α and β.
G =
Z ∞
−∞
Z ∞
−∞
eı(α(x−ξ)+β(y−η))
4π2c
p
α2 + β2 sin
p
α2 + β2c(t −τ)

dα dβH(t −τ)
We make the change of variables α = ρ cos φ, β = ρ sin φ and do the integration in polar coordinates.
G =
1
4π2c
Z 2π
0
Z ∞
0
eıρ((x−ξ) cos φ+(y−η) sin φ)
ρ
sin (ρc(t −τ)) ρ dρ dφH(t −τ)
1956

Next we introduce polar coordinates for x and y.
x −ξ = r cos θ,
y −η = r sin θ
G =
1
4π2c
Z ∞
0
Z 2π
0
eırρ(cos θ cos φ+sin θ sin φ) dφ sin (ρc(t −τ)) dρH(t −τ)
G =
1
4π2c
Z ∞
0
Z 2π
0
eırρ cos(φ−θ) dφ sin (ρc(t −τ)) dρH(t −τ)
G =
1
2πc
Z ∞
0
J0(rρ) sin (ρc(t −τ)) dρH(t −τ)
G =
1
2πc
1
p
(c(t −τ))2 −r2H(c(t −τ) −r)H(t −τ)
G(x, t|ξ, τ) =
H(c(t −τ) −|x −ξ|)
2πc
p
(c(t −τ))2 −|x −ξ|2
2. To ﬁnd the 1D Green function, we consider a line source, δ(x)δ(t). Without loss of generality, we have taken the
1957

source to be at x = 0, t = 0. We use the 2D Green function and integrate over space and time.
gtt −c2∆g = δ(x)δ(t)
g =
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
H

c(t −τ) −
p
(x −ξ)2 + (y −η)2

2πc
p
(c(t −τ))2 −(x −ξ)2 −(y −η)2δ(ξ)δ(τ) dξ dη dτ
g =
1
2πc
Z ∞
−∞
H

ct −
p
x2 + η2

p
(ct)2 −x2 −η2
dη
g =
1
2πc
Z √
(ct)2−x2
−√
(ct)2−x2
1
p
(ct)2 −x2 −η2 dηH (ct −|x|)
g(x, t|0, 0) = 1
2cH (ct −|x|)
g(x, t|ξ, τ) = 1
2cH (c(t −τ) −|x −ξ|)
Solution 45.3
1.
Gtt = c2Gxx,
G(x, 0) = 0,
Gt(x, 0) = δ(x −ξ)
ˆGtt = −c2ω2 ˆG,
ˆG(ω, 0) = 0,
ˆGt(ω, 0) = F[δ(x −ξ)]
ˆG = F[δ(x −ξ)] 1
cω sin(cωt)
ˆG = π
c F[δ(x −ξ)]F[H(ct −|x|)]
G(x, t) = π
c
1
2π
Z ∞
−∞
δ(x −ξ −η)H(ct −|η|) dη
G(x, t) = 1
2cH(ct −|x −ξ|)
1958

2. We can write the solution of
utt = c2uxx,
u(x, 0) = 0,
ut(x, 0) = f(x)
in terms of the Green function we found in the previous part.
u =
Z ∞
−∞
G(x, t|ξ)f(ξ) dξ
We consider c = 1 with the initial condition f(x) = (1 −|x|)H(1 −|x|).
u(x, t) = 1
2
Z x+t
x−t
(1 −|ξ|)H(1 −|ξ|) dξ
First we consider the case t < 1/2. We will use fact that the solution is symmetric in x.
u(x, t) =















0,
x + t < −1
1
2
R x+t
−1 (1 −|ξ|) dξ,
x −t < −1 < x + t
1
2
R x+t
x−t (1 −|ξ|) dξ,
−1 < x −t, x + t < 1
1
2
R 1
x−t(1 −|ξ|) dξ,
x −t < 1 < x + t
0,
1 < x −t
u(x, t) =

























0,
x + t < −1
1
4(1 + t + x)2
x −t < −1 < x + t
(1 + x)t
−1 < x −t, x + t < 0
1
2(2t −t2 −x2)
x −t < 0 < x + t
(1 −x)t
0 < x −t, x + t < 1
1
4(1 + t −x)2
x −t < 1 < x + t
0,
1 < x −t
1959

Next we consider the case 1/2 < t < 1.
u(x, t) =















0,
x + t < −1
1
2
R x+t
−1 (1 −|ξ|) dξ,
x −t < −1 < x + t
1
2
R x+t
x−t (1 −|ξ|) dξ,
−1 < x −t, x + t < 1
1
2
R 1
x−t(1 −|ξ|) dξ,
x −t < 1 < x + t
0,
1 < x −t
u(x, t) =

























0,
x + t < −1
1
4(1 + t + x)2
−1 < x + t < 0
1
4(1 −t2 + 2t(1 −x) + x(2 −x))
x −t < −1, 0 < x + t
1
2(2t −t2 −x2)
−1 < x −t, x + t < 1
1
4(1 −t2 + 2t(1 + x) −x(2 + x))
x −t < 0, 1 < x + t
1
4(1 + t −x)2
0 < x −t < 1
0,
1 < x −t
1960

Finally we consider the case 1 < t.
u(x, t) =















0,
x + t < −1
1
2
R x+t
−1 (1 −|ξ|) dξ,
−1 < x + t < 1
1
2
R 1
−1(1 −|ξ|) dξ,
x −t < −1, 1 < x + t
1
2
R 1
x−t(1 −|ξ|) dξ,
−1 < x −t < 1
0,
1 < x −t
u(x, t) =

























0,
x + t < −1
1
4(1 + t + x)2
−1 < x + t < 0
1
4(1 −(t + x −2)(t + x))
0 < x + t < 1
1
2
x −t < −1, 1 < x + t
1
4(1 −(t −x −2)(t −x))
−1 < x −t < 0
1
4(1 + t −x)2
0 < x −t < 1
0,
1 < x −t
Figure 45.1 shows the solution at t = 1/2 and t = 2.
-2
-1
1
2
0.1
0.2
0.3
0.4
0.5
-4
-2
2
4
0.1
0.2
0.3
0.4
0.5
Figure 45.1: The solution at t = 1/2 and t = 2.
Figure 45.2 shows the behavior of the solution in the phase plane. There are lines emanating form x = −1, 0, 1
showing the range of inﬂuence of these points.
1961

x
u=0
u=0
u=1
Figure 45.2: The behavior of the solution in the phase plane.
Solution 45.4
We deﬁne
L[u] ≡∇2u + a(x) · ∇u + h(x)u.
We use the Divergence Theorem to derive a generalized Green’s Theorem.
Z
V
uL[v] dx =
Z
V
u(∇2v + a · ∇v + hv) dx
Z
V
uL[v] dx =
Z
V
(u∇2v + ∇· (uva) −v∇· (au) + huv) dx
Z
V
uL[v] dx =
Z
V
v(∇2u −∇· (au) + hu) dx +
Z
∂V
(u∇v −v∇u + uva) · n dA
Z
V
(uL[v] −vL∗[u]) dx =
Z
∂V
(u∇v −v∇u + uva) · n dA
We deﬁne the adjoint operator L∗.
L∗[u] = ∇2u −∇· (au) + hu
1962

We substitute the solution u and the adjoint Green function G∗into the generalized Green’s Theorem.
Z
V
(G∗L[u] −uL∗[G∗]) dx =
Z
∂V
(G∗∇u −u∇G∗+ vG∗a) · n dA
Z
V
(G∗q −uL∗[G∗]) dx = 0
If the adjoint Green function satisﬁes L∗[G∗] = δ(x−ξ) then we can write u as an integral of the adjoint Green function
and the inhomegeneity.
u(ξ) =
Z
V
G∗(x|ξ)q(x) dx
Thus we see that the adjoint Green function problem is the appropriate one to consider. For L[G] = δ(x −ξ),
u(ξ) ̸=
Z
V
G(x|ξ)q(x) dx
Solution 45.5
1.
∇2u = Cδ(x −ξ+) −Cδ(x −ξ−)
u = −
C
4π|x −ξ+| +
C
4π|x −ξ−|
1963

2. We take c = D/ϵ and consider the limit ϵ →0.
u = lim
ϵ→0 −D
4πϵ
 
1
p
(x −ϵ/2)2 + y2 + z2 −
1
p
(x + ϵ/2)2 + y2 + z2
!
u = lim
ϵ→0 −D
4πϵ
p
(x + ϵ/2)2 + y2 + z2 −
p
(x −ϵ/2)2 + y2 + z2
p
((x −ϵ/2)2 + y2 + z2)((x + ϵ/2)2 + y2 + z2)
u = lim
ϵ→0 −D
4πϵ
 r + ϵx
2r + O(ϵ2)

−
 r −ϵx
2r + O(ϵ2)

r2 + O(ϵ)
u = lim
ϵ→0 −D
4π
x
r + O(ϵ)
r2 + O(ϵ)
u = −Dx
4πr3
3. Let ξ± = ξ0 ± ϵa/2.
∇2u = lim
ϵ→0
1
ϵ
 δ(x −ξ+) −δ(x −ξ−)

u = −1
4π lim
ϵ→0
1
ϵ

1
|x −(ξ0 + ϵa/2)| −
1
|x −(ξ0 −ϵa/2)|

We note that this is the deﬁnition of a directional derivative.
u(x) = −1
4π∇ξ

1
|x −ξ|

ξ=ξ0
· ⃗a
Solution 45.6
The Green function is
G(x|ξ) = 1
2π ln

|x −ξ|
|ξ||x −ξ∗|

.
1964

We write this in polar coordinates. Denote x = r eıθ and ξ = ρ eıϑ. Let φ = θ −ϑ be the diﬀerence in angle between
x and ξ.
G(x|ξ) = 1
2π ln
 
p
r2 + ρ2 −2rρ cos φ
ρ
p
r2 + 1/ρ2 −2(r/ρ) cos φ
!
G(x|ξ) = 1
4π ln
 r2 + ρ2 −2rρ cos φ
r2ρ2 + 1 −2rρ cos φ

We solve Laplace’s equation with the Green function.
u(x) =
I
f(ξ)∇ξG(x|ξ) · n ds
u(r, θ) =
Z 2π
0
f(ϑ)Gρ(r, θ|1, ϑ) dϑ
Gρ = 1
2π
ρ −r4ρ + r(r2 −1)(ρ2 + 1) cos φ
(r2 + ρ2 −2rρ cos φ)(r2ρ2 + 1 −2rρ cos φ)
Gρ(r, θ|1, ϑ) = 1
2π
1 −r2
1 + r2 −2r cos φ
u(r, θ) = 1 −r2
2π
Z 2π
0
f(ϑ)
1 + r2 −2r cos(θ −ϑ) dϑ
Solution 45.7
1.
∆G = δ(x −ξ)δ(y −η)δ(z −ζ)
1
r2
∂
∂r

r2∂G
∂r

+
1
r2 sin φ
∂
∂φ

sin(φ)∂G
∂φ

+
1
r2 sin φ
∂2G
∂θ2 = δ3(r)
2. Since the Green function has spherical symmetry, Gφ = Gθ = 0. This reduces the problem to an ordinary
diﬀerential equation.
1
r2
∂
∂r

r2∂G
∂r

= δ3(r)
1965

We ﬁnd the homogeneous solutions.
urr + 2
rur = 0
ur = c e−2 ln r = cr−2
u = c1
r + c2
We consider the solution that vanishes at inﬁnity.
u = c
r
Thus we see that G = c/r. We determine the constant by integrating ∆G over a sphere about the origin, R.
ZZZ
R
∆G dx = 1
ZZ
∂R
∇G · n ds = 1
ZZ
∂R
Gr ds = 1
Z π
0
Z 2π
0
−c
r2r2 sin(φ) dθdφ = 1
−4πc = 1
c = −1
4π
G = −1
4πr
1966

3. We write the Laplacian in circular coordinates.
∆G = δ(x −ξ)δ(y −η)
1
r
∂
∂r

r∂G
∂r

+ 1
r2
∂2G
∂θ2 = δ2(r)
Since the Green function has circular symmetry, Gθ = 0. This reduces the problem to an ordinary diﬀerential
equation.
1
r
∂
∂r

r∂G
∂r

= δ2(r)
We ﬁnd the homogeneous solutions.
urr + 1
rur = 0
ur = c e−ln r = cr−1
u = c1 ln r + c2
There are no solutions that vanishes at inﬁnity. Instead we take the solution that vanishes at r = 1.
u = c ln r
1967

Thus we see that G = c ln r. We determine the constant by integrating ∆G over a ball about the origin, R.
ZZ
R
∆G dx = 1
Z
∂R
∇G · n ds = 1
Z
∂R
Gr ds = 1
Z 2π
0
c
rr dθ = 1
2πc = 1
G = 1
2π ln r
1968

4.
u =
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
−
1
4π(r −ρ)δ(ξ)δ(η) dξ dη dζ
u = −1
4π
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
δ(ξ)δ(η)
p
(x −ξ)2 + (y −η)2 + (z −ζ)2 dξ dη dζ
u = −1
4π
Z ∞
−∞
1
p
x2 + y2 + (z −ζ)2 dζ
u = −1
4π
Z ∞
−∞
1
p
r2 + ζ2 dζ
ur = 1
4π
Z ∞
−∞
r
(r2 + ζ2)3/2 dζ
ur = 1
4π
2
r
u = 1
2π ln r
Solution 45.8
1. We take the Laplace transform of the diﬀerential equation and the boundary conditions in x.
Gt −νGxx = δ(x −ξ)δ(t −τ)
s ˆG −ν ˆGxx = δ(x −ξ)
ˆGxx −s
ν
ˆG = −1
ν δ(x −ξ),
ˆG(0, t) = ˆG(L, t) = 0
Now we have an ordinary diﬀerential equation Green function problem. We ﬁnd homogeneous solutions which
respectively satisfy the left and right boundary conditions and compute their Wronskian.
y1 = sinh
rs
ν x

,
y2 = sinh
rs
ν (L −x)

1969

W =

sinh
 p s
νx

sinh
 p s
ν(L −x)

p s
ν cosh
 p s
νx

−p s
ν cosh
 p s
ν(L −x)


= −2
rs
ν

sinh
rs
ν x

cosh
rs
ν (L −x)

+ cosh
rs
ν x

sinh
rs
ν (L −x)

= −2
rs
ν sinh
rs
ν L

We write the Green function in terms of the homogeneous solutions of the Wronskian.
ˆG = −1
ν
1
−2p s
ν sinh
 p s
νL
 sinh
rs
ν x<

sinh
rs
ν (L −x>)

ˆG = sinh
 p s
νx<

sinh
 p s
ν(L −x>)

2√νs sinh
 p s
νL

ˆG = cosh
 p s
ν(L −x> + x<)

−cosh
 p s
ν(L −x> −x<)

2√νs sinh
 p s
νL

2. We expand 1/ sinh(x) in a series.
1
sinh(x) =
2
ex −e−x
=
2 e−x
1 −e−2x
= 2 e−x
∞
X
n=0
e−2nx
= 2
∞
X
n=0
e−(2n+1)x
1970

We use the expansion of the hyperbolic cosecant in our expression for the Green function.
ˆG = e
√
s/ν(L−x>+x<) + e−√
s/ν(L−x>+x<) −e
√
s/ν(L−x>−x<) −e−√
s/ν(L−x>−x<)
4√νs sinh
 p s
νL

ˆG =
1
2√νs

e
√
ν/s(L−x>+x<) + e−√
ν/s(L−x>+x<)
−e
√
ν/s(L−x>−x<) −e−√
ν/s(L−x>−x<)
∞
X
n=0
e−(2n+1)√
s/νL
ˆG =
1
2√νs
 ∞
X
n=0
e
√
s/ν(−x>+x<−2nL) +
∞
X
n=0
e
√
s/ν(x>−x<−2(n+1)L)
−
∞
X
n=0
e
√
s/ν(−x>−x<−2nL) −
∞
X
n=0
e
√
s/ν(x>+x<−2(n+1)L)
!
ˆG =
1
2√νs
 ∞
X
n=0
e
√
s/ν(−x>+x<−2nL) +
−1
X
n=−∞
e
√
s/ν(x>−x<+2nL)
−
∞
X
n=0
e
√
s/ν(−x>−x<−2nL) −
−1
X
n=−∞
e
√
s/ν(x>+x<+2nL)
!
ˆG =
1
2√νs
 
∞
X
n=−∞
e−√
s/ν|x<−x>−2nL| −
∞
X
n=−∞
e−√
s/ν|x<+x>−2nL|
!
ˆG =
1
2√νs
 
∞
X
n=−∞
e−√
s/ν|x−ξ−2nL| −
∞
X
n=−∞
e−√
s/ν|x+ξ−2nL|
!
1971

3. We take the inverse Laplace transform to ﬁnd the Green function for the diﬀusion equation.
G =
1
2
√
πνt
 
∞
X
n=−∞
e(x−ξ−2nL)2/(4νt) −
∞
X
n=−∞
e(x+ξ−2nL)2/(4νt)
!
G =
∞
X
n=−∞
f(x −ξ −2nL, t) −
∞
X
n=−∞
f(x + ξ −2nL, t)
On the interval (−L . . . L), there is a real source at x = ξ and a negative image source at x = −ξ. This pattern
is repeated periodically.
The above formula is useful when approximating the solution for small time, t ≪1. For such small t, the terms
decay very quickly away from n = 0. A small number of terms could be used for an accurate approximation.
The alternate formula is useful when approximating the solution for large time, t ≫1. For such large t, the terms
in the sine series decay exponentially Again, a small number of terms could be used for an accurate approximation.
Solution 45.9
1. We take the Fourier cosine transform of the diﬀerential equation.
Gt −νGxx = δ(x −ξ)δ(t −τ)
ˆGt −ν

−ω2 ˆG −1
πGx(0, t)

= Fc[δ(x −ξ)]δ(t −τ)
ˆGt + νω2 ˆG = Fc[δ(x −ξ)]δ(t −τ)
ˆG = Fc[δ(x −ξ)] e−νω2(t−τ) H(t −τ)
ˆG = Fc[δ(x −ξ)]Fc
r
π
ν(t −τ) e−x2/(4ν(t−τ))

H(t −τ)
1972

We do the inversion with the convolution theorem.
G = 1
2π
Z ∞
0
δ(η −ξ)
r
π
ν(t −τ)

e−|x−η|2/(4ν(t−τ)) + e−(x+η)2/(4ν(t−τ))
dηH(t −τ)
G(x, t; ξ, τ) =
1
p
4πν(t −τ)

e−(x−ξ)2/(4ν(t−τ)) + e−(x+ξ)2/(4ν(t−τ))
H(t −τ)
2. The fundamental solution on the inﬁnite domain is
F(x, t; ξ, τ) =
1
p
4πν(t −τ)
e−(x−ξ)2/(4ν(t−τ)) H(t −τ).
We see that the Green function on the semi-inﬁnite domain that we found above is a sum of fundamental solutions.
G(x, t; ξ, τ) = F(x, t; ξ, τ) + F(x, t; −ξ, τ)
Now we solve the inhomogeneous problem.
u(x, t) =
Z t
0
Z ∞
0
G(x, t; ξ, τ)q(ξ, τ) dξ dτ +
Z ∞
0
G(x, t; ξ, 0)f(ξ) dξ
u(x, t) =
1
√
4πν
Z t
0
Z ∞
0
1
√t −τ

e−(x−ξ)2/(4ν(t−τ)) + e−(x+ξ)2/(4ν(t−τ))
q(ξ, τ) dξ dτ
+
1
√
4πνt
Z ∞
0

e−(x−ξ)2/(4νt) + e−(x+ξ)2/(4νt)
f(ξ) dξ
Solution 45.10
1. We integrate the heat equation from t = 0−to t = 0+ to determine an initial condition.
ut = νuxx + δ(x −ξ)δ(t)
u(x, 0+) −u(x, 0−) = δ(x −ξ)
1973

Now we have an initial value problem with no forcing.
ut = νuxx,
for t > 0,
u(x, 0) = δ(x −ξ)
2. We take the Laplace transform of the initial value problem.
sˆu −u(x, 0) = νˆuxx
ˆuxx −s
ν ˆu = −1
ν δ(x −ξ),
ˆu(±∞, s) = 0
The solutions that satisfy the left and right boundary conditions are, respectively,
u1 = e
√
s/νx,
u2 = e−√
s/νx
We compute the Wronskian of these solutions and then write the solution for ˆu.
W =

e
√
s/νx
e−√
s/νx
p
s/ν e
√
s/νx
−
p
s/ν e−√
s/νx
 = −2
rs
ν
ˆu = −1
ν
e
√
s/νx< e−√
s/νx>
−2p s
ν
ˆu =
1
2√νs e−√
s/ν|x−ξ|
3. In Exercise 31.16, we showed that
L−1
rπ
s e−2√as

= e−a/t
√
t .
We use this result to do the inverse Laplace transform.
u(x, t) =
1
2
√
πνt
e−(x−ξ)2/(4νt)
1974

Solution 45.11
Gtt −c2Gxx = δ(x −ξ)δ(t −τ),
G(x, t; ξ, τ) = 0
for t < τ.
We take the Fourier transform in x.
ˆGtt + c2ω2G = F[δ(x −ξ)]δ(t −τ),
ˆG(ω, 0; ξ, τ −) = ˆGt(ω, 0; ξ, τ −) = 0
Now we have an ordinary diﬀerential equation Green function problem for ˆG. We have written the causality condition,
the Green function is zero for t < τ, in terms of initial conditions. The homogeneous solutions of the ordinary diﬀerential
equation are
{cos(cωt), sin(cωt)}.
It will be handy to use the fundamental set of solutions at t = τ:

cos(cω(t −τ)), 1
cω sin(cω(t −τ))

.
The continuity and jump conditions are
ˆG(ω, 0; ξ, τ +) = 0,
ˆGt(ω, 0; ξ, τ +) = F[δ(x −ξ)]
We write the solution for ˆG and invert using the convolution theorem.
ˆG = F[δ(x −ξ)]H(t −τ) 1
cω sin(cω(t −τ))
ˆG = H(t −τ)F[δ(x −ξ)]F
hπ
c H(c(t −τ) −|x|)
i
G = H(t −τ)π
c
1
2π
Z ∞
−∞
δ(y −ξ)H(c(t −τ) −|x −y|) dy
G = 1
2cH(t −τ)H(c(t −τ) −|x −ξ|)
G = 1
2cH(c(t −τ) −|x −ξ|)
1975

The Green function for ξ = τ = 0 and c = 1 is plotted in Figure 45.3 on the domain x ∈(−1..1), t ∈(0..1). The
Green function is a displacement of height
1
2c that propagates out from the point x = ξ in both directions with speed c.
The Green function shows the range of inﬂuence of a disturbance at the point x = ξ and time t = τ. The disturbance
inﬂuences the solution for all ξ −ct < x < ξ + ct and t > τ.
-1
-0.5
0
0.5
1
x
0
0.2
0.4
0.6
0.8
1
t
0
0.2
0.4
-1
-0.5
0
0 5
.2
0.4
0.6
0.8
1
t
Figure 45.3: Green function for the wave equation.
Now we solve the wave equation with a source.
utt −c2uxx = q(x, t),
u(x, 0) = ut(x, 0) = 0
u =
Z ∞
0
Z ∞
−∞
G(x, t|ξ, t)q(ξ, τ) dξ dτ
u =
Z ∞
0
Z ∞
−∞
1
2cH(c(t −τ) −|x −ξ|)q(ξ, τ) dξ dτ
u = 1
2c
Z t
0
Z x+c(t−τ)
x−c(t−τ)
q(ξ, τ) dξ dτ
1976

Solution 45.12
1. We expand the Green function in eigenfunctions in x.
G(x; ξ) =
∞
X
n=1
an(y) sin
(2n −1)πx
2L

We substitute the expansion into the diﬀerential equation.
∇2
∞
X
n=1
an(y)
r
2
L sin
(2n −1)πx
2L

= δ(x −ξ)δ(y −ψ)
∞
X
n=1
 
a′′
n(y) −
(2n −1)π
2L
2
an(y)
! r
2
L sin
(2n −1)πx
2L

= δ(y −ψ)
∞
X
n=1
r
2
L sin
(2n −1)πξ
2L
 r
2
L sin
(2n −1)πx
2L

a′′
n(y) −
(2n −1)π
2L
2
an(y) =
r
2
L sin
(2n −1)πξ
2L

δ(y −ψ)
From the boundary conditions at y = 0 and y = H, we obtain boundary conditions for the an(y).
a′
n(0) = a′
n(H) = 0.
The solutions that satisfy the left and right boundary conditions are
an1 = cosh
(2n −1)πy
2L

,
an2 = cosh
(2n −1)π(H −y)
2L

.
The Wronskian of these solutions is
W = −(2n −1)π
2L
sinh
(2n −1)π
2

.
1977

Thus the solution for an(y) is
an(y) =
r
2
L sin
(2n −1)πξ
2L
 cosh

(2n−1)πy<
2L

cosh

(2n−1)π(H−y>)
2L

−(2n−1)π
2L
sinh

(2n−1)π
2

an(y) = −
2
√
2L
(2n −1)π csch
(2n −1)π
2

cosh
(2n −1)πy<
2L

cosh
(2n −1)π(H −y>)
2L

sin
(2n −1)πξ
2L

.
This determines the Green function.
G(x; ξ) = −2
√
2L
π
∞
X
n=1
1
2n −1 csch
(2n −1)π
2

cosh
(2n −1)πy<
2L

cosh
(2n −1)π(H −y>)
2L

sin
(2n −1)πξ
2L

sin
(2n −1)πx
2L

2. We seek a solution of the form
G(x; ξ) =
∞
X
m=1
n=1
amn(z)
2
√
LH
sin
mπx
L

sin
nπy
H

.
We substitute this into the diﬀerential equation.
∇2
∞
X
m=1
n=1
amn(z)
2
√
LH
sin
mπx
L

sin
nπy
H

= δ(x −ξ)δ(y −ψ)δ(z −ζ)
1978

∞
X
m=1
n=1

a′′
mn(z) −
mπ
L
2
+
nπ
H
2
amn(z)

2
√
LH
sin
mπx
L

sin
nπy
H

= δ(z −ζ)
∞
X
m=1
n=1
2
√
LH
sin
mπξ
L

sin
nπψ
H

2
√
LH
sin
mπx
L

sin
nπy
H

a′′
mn(z) −π
m
L
2
+
 n
H
2
amn(z) =
2
√
LH
sin
mπξ
L

sin
nπψ
H

δ(z −ζ)
From the boundary conditions on G, we obtain boundary conditions for the amn.
amn(0) = amn(W) = 0
The solutions that satisfy the left and right boundary conditions are
amn1 = sinh
 rm
L
2
+
 n
H
2
πz
!
,
amn2 = sinh
 rm
L
2
+
 n
H
2
π(W −z)
!
.
The Wronskian of these solutions is
W = −
rm
L
2
+
 n
H
2
π sinh
 rm
L
2
+
 n
H
2
πW
!
.
Thus the solution for amn(z) is
amn(z) =
2
√
LH
sin
mπξ
L

sin
nπψ
H

sinh
q  m
L
2 +
  n
H
2 πz<

sinh
q  m
L
2 +
  n
H
2 π(W −z>)

−
q  m
L
2 +
  n
H
2 π sinh
q  m
L
2 +
  n
H
2 πW

1979

amn(z) = −
2
πλmn
√
LH
csch (λmnπW) sin
mπξ
L

sin
nπψ
H

sinh (λmnπz<) sinh (λmnπ(W −z>)) ,
where
λmn =
rm
L
2
+
 n
H
2
.
This determines the Green function.
G(x; ξ) = −
4
πLH
∞
X
m=1
n=1
1
λmn
csch (λmnπW) sin
mπξ
L

sin
mπx
L

sin
nπψ
H

sin
nπy
H

sinh (λmnπz<) sinh (λmnπ(W −z>))
3. First we write the problem in circular coordinates.
∇2G = δ(x −ξ)
Grr + 1
rGr + 1
r2Gθθ = 1
rδ(r −ρ)δ(θ −ϑ),
G(r, 0; ρ, ϑ) = G(r, π; ρ, ϑ) = G(0, θ; ρ, ϑ) = G(a, θ; ρ, ϑ) = 0
Because the Green function vanishes at θ = 0 and θ = π we expand it in a series of the form
G =
∞
X
n=1
gn(r) sin(nθ).
We substitute the series into the diﬀerential equation.
∞
X
n=1

g′′
n(r) + 1
rg′
n(r) −n2
r2 gn(r)

sin(nθ) = 1
rδ(r −ρ)
∞
X
n=1
2
π sin(nϑ) sin(nθ)
g′′
n(r) + 1
rg′
n(r) −n2
r2 gn(r) = 2
πr sin(nϑ)δ(r −ρ)
1980

From the boundary conditions on G, we obtain boundary conditions for the gn.
gn(0) = gn(a) = 0
The solutions that satisfy the left and right boundary conditions are
gn1 = rn,
gn2 =
r
a
n
−
a
r
n
.
The Wronskian of these solutions is
W = 2nan
r
.
Thus the solution for gn(r) is
gn(r) = 2
πρ sin(nϑ)
rn
<
  r>
a
n −

a
r>
n
2nan
ρ
gn(r) = 1
nπ sin(nϑ)
r<
a
n r>
a
n
−
 a
r>
n
.
This determines the solution.
G =
∞
X
n=1
1
nπ
r<
a
n r>
a
n
−
 a
r>
n
sin(nϑ) sin(nθ)
4. First we write the problem in circular coordinates.
Grr + 1
rGr + 1
r2Gθθ = 1
rδ(r −ρ)δ(θ −ϑ),
G(r, 0; ρ, ϑ) = G(r, π/2; ρ, ϑ) = G(0, θ; ρ, ϑ) = Gr(a, θ; ρ, ϑ) = 0
Because the Green function vanishes at θ = 0 and θ = π/2 we expand it in a series of the form
G =
∞
X
n=1
gn(r) sin(2nθ).
1981

We substitute the series into the diﬀerential equation.
∞
X
n=1

g′′
n(r) + 1
rg′
n(r) −4n2
r2 gn(r)

sin(2nθ) = 1
rδ(r −ρ)
∞
X
n=1
4
π sin(2nϑ) sin(2nθ)
g′′
n(r) + 1
rg′
n(r) −4n2
r2 gn(r) = 4
πr sin(2nϑ)δ(r −ρ)
From the boundary conditions on G, we obtain boundary conditions for the gn.
gn(0) = g′
n(a) = 0
The solutions that satisfy the left and right boundary conditions are
gn1 = r2n,
gn2 =
r
a
2n
+
a
r
2n
.
The Wronskian of these solutions is
W = −4na2n
r
.
Thus the solution for gn(r) is
gn(r) = 4
πρ sin(2nϑ)
r2n
<
  r>
a
2n +

a
r>
2n
−4na2n
ρ
gn(r) = −1
πn sin(2nϑ)
r<
a
2n
 r>
a
2n
+
 a
r>
2n!
This determines the solution.
G = −
∞
X
n=1
1
πn
r<
a
2n
 r>
a
2n
+
 a
r>
2n!
sin(2nϑ) sin(2nθ)
1982

Solution 45.13
1. The set
{Xn} =

sin
(2m −1)πx
2L
∞
m=1
are eigenfunctions of ∇2 and satisfy the boundary conditions Xn(0) = X′
n(L) = 0. The set
{Yn} =
n
cos
nπy
H
o∞
n=0
are eigenfunctions of ∇2 and satisfy the boundary conditions Y ′
n(0) = Y ′
n(H) = 0. The set

sin
(2m −1)πx
2L

cos
nπy
H
∞
m=1,n=0
are eigenfunctions of ∇2 and satisfy the boundary conditions of this problem. We expand the Green function in
a series of these eigenfunctions.
G =
∞
X
m=1
gm0
r
2
LH sin
(2m −1)πx
2L

+
∞
X
m=1
n=1
gmn
2
√
LH
sin
(2m −1)πx
2L

cos
nπy
H

We substitute the series into the Green function diﬀerential equation.
∆G = δ(x −ξ)δ(y −ψ)
1983

−
∞
X
m=1
gm0
(2m −1)π
2L
2 r
2
LH sin
(2m −1)πx
2L

−
∞
X
m=1
n=1
gmn
 (2m −1)π
2L
2
+
nπy
H
2
!
2
√
LH
sin
(2m −1)πx
2L

cos
nπy
H

=
∞
X
m=1
r
2
LH sin
(2m −1)πξ
2L
 r
2
LH sin
(2m −1)πx
2L

+
∞
X
m=1
n=1
2
√
LH
sin
(2m −1)πξ
2L

cos
nπψ
H

2
√
LH
sin
(2m −1)πx
2L

cos
nπy
H

We equate terms and solve for the coeﬃcients gmn.
gm0 = −
r
2
LH

2L
(2m −1)π
2
sin
(2m −1)πξ
2L

gmn = −
2
√
LH
1
π2
  2m−1
2L
2 +
  n
H
2 sin
(2m −1)πξ
2L

cos
nπψ
H

This determines the Green function.
2. Note that
(r
8
LHW sin
kπx
L

, sin
mπy
H

, sin
nπz
W

: k, m, n ∈Z+
)
is orthonormal and complete on (0 . . . L) × (0 . . . H) × (0 . . . W). The functions are eigenfunctions of ∇2. We
expand the Green function in a series of these eigenfunctions.
G =
∞
X
k,m,n=1
gkmn
r
8
LHW sin
kπx
L

sin
mπy
H

sin
nπz
W

1984

We substitute the series into the Green function diﬀerential equation.
∆G = δ(x −ξ)δ(y −ψ)δ(z −ζ)
−
∞
X
k,m,n=1
gkmn
 kπ
L
2
+
mπ
H
2
+
nπ
W
2
! r
8
LHW sin
kπx
L

sin
mπy
H

sin
nπz
W

=
∞
X
k,m,n=1
r
8
LHW sin
kπξ
L

sin
mπψ
H

sin
nπζ
W

r
8
LHW sin
kπx
L

sin
mπy
H

sin
nπz
W

We equate terms and solve for the coeﬃcients gkmn.
gkmn = −
q
8
LHW sin
  kπξ
L

sin
  mπψ
H

sin
  nπζ
W

π2
  k
L
2 +
  m
H
2 +
  n
W
2
This determines the Green function.
3. The Green function problem is
∆G ≡Grr + 1
rGr + 1
r2Gθθ = 1
rδ(r −ρ)δ(θ −ϑ).
We seek a set of functions {Θn(θ)Rnm(r)} which are orthogonal and complete on (0 . . . a) × (0 . . . π) and which
are eigenfunctions of the laplacian. For the Θn we choose eigenfunctions of
∂2
∂θ2.
Θ′′ = −ν2Θ,
Θ(0) = Θ(π) = 0
νn = n,
Θn = sin(nθ),
n ∈Z+
1985

Now we look for eigenfunctions of the laplacian.
(RΘn)rr + 1
r(RΘn)r + 1
r2(RΘn)θθ = −µ2RΘn
R′′Θn + 1
rR′Θn −n2
r2 RΘn = −µ2RΘn
R′′ + 1
rR′ +

µ2 −n2
r2

R = 0,
R(0) = R(a) = 0
The general solution for R is
R = c1Jn(µr) + c2Yn(µr).
the solution that satisﬁes the left boundary condition is R = cJn(µr). We use the right boundary condition to
determine the eigenvalues.
µm = jn,m
a ,
Rnm = Jn
jn,mr
a

,
m, n ∈Z+
here jn,m is the mth root of Jn.
Note that

sin(nθ)Jn
jn,mr
a

: m, n ∈Z+

is orthogonal and complete on (r, θ) ∈(0 . . . a) × (0 . . . π). We use the identities
Z π
0
sin2(nθ) dθ = π
2 ,
Z 1
0
rJ2
n(jn,mr) dr = 1
2J2
n+1(jn,m)
to make the functions orthonormal.

2
√π a|Jn+1(jn,m)| sin(nθ)Jn
jn,mr
a

: m, n ∈Z+

We expand the Green function in a series of these eigenfunctions.
G =
∞
X
n,m=1
gnm
2
√π a|Jn+1(jn,m)|Jn
jn,mr
a

sin(nθ)
1986

We substitute the series into the Green function diﬀerential equation.
Grr + 1
rGr + 1
r2Gθθ = 1
rδ(r −ρ)δ(θ −ϑ)
−
∞
X
n,m=1
jn,m
a
2
gnm
2
√π a|Jn+1(jn,m)|Jn
jn,mr
a

sin(nθ)
=
∞
X
n,m=1
2
√π a|Jn+1(jn,m)|Jn
jn,mρ
a

sin(nϑ)
2
√π a|Jn+1(jn,m)|Jn
jn,mr
a

sin(nθ)
We equate terms and solve for the coeﬃcients gmn.
gnm = −
 a
jn,m
2
2
√π a|Jn+1(jn,m)|Jn
jn,mρ
a

sin(nϑ)
This determines the green function.
4. The Green function problem is
∆G ≡Grr + 1
rGr + 1
r2Gθθ = 1
rδ(r −ρ)δ(θ −ϑ).
We seek a set of functions {Θn(θ)Rnm(r)} which are orthogonal and complete on (0 . . . a) × (0 . . . π/2) and
which are eigenfunctions of the laplacian. For the Θn we choose eigenfunctions of
∂2
∂θ2.
Θ′′ = −ν2Θ,
Θ(0) = Θ(π/2) = 0
νn = 2n,
Θn = sin(2nθ),
n ∈Z+
1987

Now we look for eigenfunctions of the laplacian.
(RΘn)rr + 1
r(RΘn)r + 1
r2(RΘn)θθ = −µ2RΘn
R′′Θn + 1
rR′Θn −(2n)2
r2
RΘn = −µ2RΘn
R′′ + 1
rR′ +

µ2 −(2n)2
r2

R = 0,
R(0) = R(a) = 0
The general solution for R is
R = c1J2n(µr) + c2Y2n(µr).
the solution that satisﬁes the left boundary condition is R = cJ2n(µr). We use the right boundary condition to
determine the eigenvalues.
µm = j′
2n,m
a
,
Rnm = J2n
j′
2n,mr
a

,
m, n ∈Z+
here j′
n,m is the mth root of J′n.
Note that

sin(2nθ)J′
2n
j′
2n,mr
a

: m, n ∈Z+

is orthogonal and complete on (r, θ) ∈(0 . . . a) × (0 . . . π/2). We use the identities
Z π
0
sin(mθ) sin(nθ) dθ = π
2 δmn,
Z 1
0
rJν(j′
ν,mr)Jν(j′
ν,nr) dr = j′2
ν,n −ν2
2j′2
ν,n
 Jν(j′
ν,n)
2 δmn
to make the functions orthonormal.



2j′
2n,m
√π a
q
j′2
2n,m −4n2|J2n(j′2n,m)|
sin(2nθ)J2n
j′
2n,mr
a

: m, n ∈Z+



1988

We expand the Green function in a series of these eigenfunctions.
G =
∞
X
n,m=1
gnm
2j′
2n,m
√π a
q
j′2
2n,m −4n2|J2n(j′2n,m)|
J2n
j′
2n,mr
a

sin(2nθ)
We substitute the series into the Green function diﬀerential equation.
Grr + 1
rGr + 1
r2Gθθ = 1
rδ(r −ρ)δ(θ −ϑ)
−
∞
X
n,m=1
j′
2n,m
a
2
gnm
2j′
2n,m
√π a
q
j′2
2n,m −4n2|J2n(j′2n,m)|
J2n
j′
2n,mr
a

sin(2nθ)
=
∞
X
n,m=1
2j′
2n,m
√π a
q
j′2
2n,m −4n2|J2n(j′2n,m)|
J2n
j′
2n,mρ
a

sin(2nϑ)
2j′
2n,m
√π a
q
j′2
2n,m −4n2|J2n(j′2n,m)|
J2n
j′
2n,mr
a

sin(2nθ)
We equate terms and solve for the coeﬃcients gmn.
gnm = −

a
j′2n,m
2
2j′
2n,m
√π a
q
j′2
2n,m −4n2|J2n(j′2n,m)|
J2n
j′
2n,mρ
a

sin(2nϑ)
This determines the green function.
Solution 45.14
We start with the equation
∇2G = δ(x −ξ)δ(y −ψ).
1989

We do an odd reﬂection across the y axis so that G(0, y; ξ, ψ) = 0.
∇2G = δ(x −ξ)δ(y −ψ) −δ(x + ξ)δ(y −ψ)
Then we do an even reﬂection across the x axis so that Gy(x, 0; ξ, ψ) = 0.
∇2G = δ(x −ξ)δ(y −ψ) −δ(x + ξ)δ(y −ψ) + δ(x −ξ)δ(y + ψ) −δ(x + ξ)δ(y + ψ)
We solve this problem using the inﬁnite space Green function.
G = 1
4π ln
 (x −ξ)2 + (y −ψ)2
−1
4π ln
 (x + ξ)2 + (y −ψ)2
+ 1
4π ln
 (x −ξ)2 + (y + ψ)2
−1
4π ln
 (x + ξ)2 + (y + ψ)2
G = 1
4π ln
((x −ξ)2 + (y −ψ)2) ((x −ξ)2 + (y + ψ)2)
((x + ξ)2 + (y −ψ)2) ((x + ξ)2 + (y + ψ)2)

Now we solve the boundary value problem.
u(ξ, ψ) =
Z
S

u(x, y)∂G
∂n −G∂u(x, y)
∂n

dS +
Z
V
G∆u dV
u(ξ, ψ) =
Z 0
∞
u(0, y)(−Gx(0, y; ξ, ψ)) dy +
Z ∞
0
−G(x, 0; ξ, ψ)(−uy(x, 0)) dx
u(ξ, ψ) =
Z ∞
0
g(y)Gx(0, y; ξ, ψ) dy +
Z ∞
0
G(x, 0; ξ, ψ)h(x) dx
u(ξ, ψ) = −ξ
π
Z ∞
0

1
ξ2 + (y −ψ)2 +
1
ξ2 + (y + ψ)2

g(y) dy + 1
2π
Z ∞
0
ln
(x −ξ)2 + ψ2
(x + ξ)2 + ψ2

h(x) dx
u(x, y) = −x
π
Z ∞
0

1
x2 + (y −ψ)2 +
1
x2 + (y + ψ)2

g(ψ) dψ + 1
2π
Z ∞
0
ln
(x −ξ)2 + y2
(x + ξ)2 + y2

h(ξ) dξ
1990

Solution 45.15
First we ﬁnd the inﬁnite space Green function.
Gtt −c2Gxx = δ(x −ξ)δ(t −τ),
G = Gt = 0 for t < τ
We solve this problem with the Fourier transform.
ˆGtt + c2ω2 ˆG = F[δ(x −ξ)]δ(t −τ)
ˆG = F[δ(x −ξ)]H(t −τ) 1
cω sin(cω(t −τ))
ˆG = H(t −τ)F[δ(x −ξ)]F
hπ
c H(c(t −τ) −|x|)
i
G = H(t −τ)π
c
1
2π
Z ∞
−∞
δ(y −ξ)H(c(t −τ) −|x −y|) dy
G = 1
2cH(t −τ)H(c(t −τ) −|x −ξ|)
G = 1
2cH(c(t −τ) −|x −ξ|)
1. So that the Green function vanishes at x = 0 we do an odd reﬂection about that point.
Gtt −c2Gxx = δ(x −ξ)δ(t −τ) −δ(x + ξ)δ(t −τ)
G = 1
2cH(c(t −τ) −|x −ξ|) −1
2cH(c(t −τ) −|x + ξ|)
2. Note that the Green function satisﬁes the symmetry relation
G(x, t; ξ, τ) = G(ξ, −τ; x, −t).
This implies that
Gxx = Gξξ,
Gtt = Gττ.
1991

We write the Green function problem and the inhomogeneous diﬀerential equation for u in terms of ξ and τ.
Gττ −c2Gξξ = δ(x −ξ)δ(t −τ)
(45.4)
uττ −c2uξξ = Q(ξ, τ)
(45.5)
We take the diﬀerence of u times Equation 45.4 and G times Equation 45.5 and integrate this over the domain
(0, ∞) × (0, t+).
Z t+
0
Z ∞
0
(uδ(x −ξ)δ(t −τ) −GQ) dξ dτ =
Z t+
0
Z ∞
0
 uGττ −uττG −c2 (uGξξ −uξξG)

dξ dτ
u(x, t) =
Z t+
0
Z ∞
0
GQ dξ dτ +
Z t+
0
Z ∞
0
 ∂
∂τ (uGτ −uτG) −c2 ∂
∂ξ (uGξ −uξG)

dξ dτ
u(x, t) =
Z t+
0
Z ∞
0
GQ dξ dτ +
Z ∞
0
[uGτ −uτG]t+
0
dξ −c2
Z t+
0
[uGξ −uξG]∞
0 dτ
u(x, t) =
Z t+
0
Z ∞
0
GQ dξ dτ −
Z ∞
0
[uGτ −uτG]τ=0 dξ + c2
Z t+
0
[uGξ]ξ=0 dτ
We consider the case Q(x, t) = f(x) = g(x) = 0.
u(x, t) = c2
Z t+
0
h(τ)Gξ(x, t; 0, τ) dτ
We calculate Gξ.
G = 1
2c (H(c(t −τ) −|x −ξ|) −H(c(t −τ) −|x + ξ|))
Gξ = 1
2c (δ(c(t −τ) −|x −ξ|)(−1) sign(x −ξ)(−1) −δ(c(t −τ) −|x + ξ|)(−1) sign(x + ξ))
Gξ(x, t; 0, ψ) = 1
cδ(c(t −τ) −|x|) sign(x)
1992

We are interested in x > 0.
Gξ(x, t; 0, ψ) = 1
cδ(c(t −τ) −x)
Now we can calculate the solution u.
u(x, t) = c2
Z t+
0
h(τ)1
cδ(c(t −τ) −x) dτ
u(x, t) =
Z t+
0
h(τ)δ

(t −τ) −x
c

dτ
u(x, t) = h

t −x
c

3. The boundary condition inﬂuences the solution u(x1, t1) only at the point t = t1 −x1/c. The contribution from
the boundary condition u(0, t) = h(t) is a wave moving to the right with speed c.
Solution 45.16
gtt −c2gxx = 0,
g(x, 0; ξ, τ) = δ(x −ξ),
gt(x, 0; ξ, τ) = 0
ˆgtt + c2ω2ˆgxx = 0,
ˆg(x, 0; ξ, τ) = F[δ(x −ξ)],
ˆgt(x, 0; ξ, τ) = 0
ˆg = F[δ(x −ξ)] cos(cωt)
ˆg = F[δ(x −ξ)]F[π(δ(x + ct) + δ(x −ct))]
g = 1
2π
Z ∞
−∞
δ(ψ −ξ)π(δ(x −ψ + ct) + δ(x −ψ −ct)) dψ
g(x, t; ξ) = 1
2(δ(x −ξ + ct) + δ(x −ξ −ct))
1993

γtt −c2γxx = 0,
γ(x, 0; ξ, τ) = 0,
γt(x, 0; ξ, τ) = δ(x −ξ)
ˆγtt + c2ω2ˆγxx = 0,
ˆγ(x, 0; ξ, τ) = 0,
ˆγt(x, 0; ξ, τ) = F[δ(x −ξ)]
ˆγ = F[δ(x −ξ)] 1
cω sin(cωt)
ˆγ = F[δ(x −ξ)]F
hπ
c (H(x + ct) + H(x −ct))
i
γ = 1
2π
Z ∞
−∞
δ(ψ −ξ)π
c (H(x −ψ + ct) + H(x −ψ −ct)) dψ
γ(x, t; ξ) = 1
2c(H(x −ξ + ct) + H(x −ξ −ct))
Solution 45.17
u(x, t) =
Z ∞
0
Z ∞
−∞
G(x, t; ξ, τ)f(ξ, τ) dξ dτ +
Z ∞
−∞
g(x, t; ξ)p(ξ) dξ +
Z ∞
−∞
γ(x, t; ξ)q(ξ) dξ
u(x, t) = 1
2c
Z ∞
0
Z ∞
−∞
H(t −τ)(H(x −ξ + c(t −τ)) −H(x −ξ −c(t −τ)))f(ξ, τ) dξ dτ
+ 1
2
Z ∞
−∞
(δ(x −ξ + ct) + δ(x −ξ −ct))p(ξ) dξ + 1
2c
Z ∞
−∞
(H(x −ξ + ct) + H(x −ξ −ct))q(ξ) dξ
u(x, t) = 1
2c
Z t
0
Z ∞
−∞
(H(x −ξ + c(t −τ)) −H(x −ξ −c(t −τ)))f(ξ, τ) dξ dτ
+ 1
2(p(x + ct) + p(x −ct)) + 1
2c
Z x+ct
x−ct
q(ξ) dξ
u(x, t) = 1
2c
Z t
0
Z x+c(t−τ)
x−c(t−τ)
f(ξ, τ) dξ dτ + 1
2(p(x + ct) + p(x −ct)) + 1
2c
Z x+ct
x−ct
q(ξ) dξ
1994

This solution demonstrates the domain of dependence of the solution. The ﬁrst term is an integral over the triangle
domain {(ξ, τ) : 0 < τ < t, x −cτ < ξ < x + cτ}. The second term involves only the points (x ± ct, 0). The third
term is an integral on the line segment {(ξ, 0) : x −ct < ξ < x + ct}. In totallity, this is just the triangle domain. This
is shown graphically in Figure 45.4.
x-ct
x+ct
Domain of
Dependence
x,t
Figure 45.4: Domain of dependence for the wave equation.
Solution 45.18
Single Sum Representation. First we ﬁnd the eigenfunctions of the homogeneous problem ∆u −k2u = 0. We
substitute the separation of variables, u(x, y) = X(x)Y (y) into the partial diﬀerential equation.
X′′Y + XY ′′ −k2XY = 0
X′′
X = k2 −Y ′′
Y
= −λ2
We have the regular Sturm-Liouville eigenvalue problem,
X′′ = −λ2X,
X(0) = X(a) = 0,
1995

which has the solutions,
λn = nπ
a ,
Xn = sin
nπx
a

,
n ∈N.
We expand the solution u in a series of these eigenfunctions.
G(x, y; ξ, ψ) =
∞
X
n=1
cn(y) sin
nπx
a

We substitute this series into the partial diﬀerential equation to ﬁnd equations for the cn(y).
∞
X
n=1

−
nπ
a
2
cn(y) + c′′
n(y) −k2cn(y)

sin
nπx
a

= δ(x −ξ)δ(y −ψ)
The series expansion of the right side is,
δ(x −ξ)δ(y −ψ) =
∞
X
n=1
dn(y) sin
nπx
a

dn(y) = 2
a
Z a
0
δ(x −ξ)δ(y −ψ) sin
nπx
a

dx
dn(y) = 2
a sin
nπξ
a

δ(y −ψ).
The the equations for the cn(y) are
c′′
n(y) −

k2 +
nπ
a
2
cn(y) = 2
a sin
nπξ
a

δ(y −ψ),
cn(0) = cn(b) = 0.
The homogeneous solutions are {cosh(σny), sinh(σny)}, where σn =
p
k2(nπ/a)2. The solutions that satisfy the
boundary conditions at y = 0 and y = b are, sinh(σny) and sinh(σn(y −b)), respectively. The Wronskian of these
1996

solutions is,
W(y) =

sinh(σny)
sinh(σn(y −b))
σn cosh(σny)
σn cosh(σn(y −b))

= σn (sinh(σny) cosh(σn(y −b)) −sinh(σn(y −b)) cosh(σny))
= σn sinh(σnb).
The solution for cn(y) is
cn(y) = 2
a sin
nπξ
a
 sinh(σny<) sinh(σn(y> −b))
σn sinh(σnb)
.
The Green function for the partial diﬀerential equation is
G(x, y; ξ, ψ) = 2
a
∞
X
n=1
sinh(σny<) sinh(σn(y> −b))
σn sinh(σnb)
sin
nπx
a

sin
nπξ
a

.
Solution 45.19
We take the Fourier cosine transform in x of the partial diﬀerential equation and the boundary condition along y = 0.
Gxx + Gyy −k2G = δ(x −ξ)δ(y −ψ)
−α2 ˆG(α, y) −1
π
ˆGx(0, y) + ˆGyy(α, y) −k2 ˆG(α, y) = 1
π cos(αξ)δ(y −ψ)
ˆGyy(α, y) −(k2 + α2) ˆG(α, y) == 1
π cos(αξ)δ(y −ψ),
ˆG(α, 0) = 0
Then we take the Fourier sine transform in y.
−β2 ˆˆG(α, β) + β
π
ˆˆG(α, 0) −(k2 + α2) ˆˆG(α, β) = 1
π2 cos(αξ) sin(βψ)
ˆˆG = −cos(αξ) sin(βψ)
π2(k2 + α2 + β2)
1997

We take two inverse transforms to ﬁnd the solution. For one integral representation of the Green function we take the
inverse sine transform followed by the inverse cosine transform.
ˆˆG = −cos(αξ)sin(βψ)
π
1
π(k2 + α2 + β2)
ˆˆG = −cos(αξ)Fs[δ(y −ψ)]Fc

1
√
k2 + α2 e−
√
k2+α2y

ˆG(α, y) = −cos(αξ) 1
2π
Z ∞
0
δ(z −ψ)
1
√
k2 + α2

exp

−
√
k2 + α2|y −z|

−exp

−
√
k2 + α2(y + z)

dz
ˆG(α, y) = −
cos(αξ)
2π
√
k2 + α2

exp

−
√
k2 + α2|y −ψ|

−exp

−
√
k2 + α2(y + ψ)

G(x, y; ξ, ψ) = −1
π
Z ∞
0
cos(αξ)
√
k2 + α2

exp

−
√
k2 + α2|y −ψ|

−exp

−
√
k2 + α2(y + ψ)

dα
For another integral representation of the Green function, we take the inverse cosine transform followed by the inverse
sine transform.
ˆˆG(α, β) = −sin(βψ)cos(αξ)
π
1
π(k2 + α2 + β2)
ˆˆG(α, β) = −sin(βψ)Fc[δ(x −ξ)]Fc
"
1
p
k2 + β2 e−√
k2+β2x
#
ˆG(x, β) = −sin(βψ) 1
2π
Z ∞
0
δ(z −ξ)
1
p
k2 + β2

e−√
k2+β2|x−z| + e−√
k2+β2(x+z)
dz
ˆG(x, β) = −sin(βψ) 1
2π
1
p
k2 + β2

e−√
k2+β2|x−ξ| + e−√
k2+β2(x+ξ)
G(x, y; ξ, ψ) = −1
π
Z ∞
0
sin(βy) sin(βψ)
p
k2 + β2

e−√
k2+β2|x−ξ| + e−√
k2+β2(x+ξ)
dβ
1998

Solution 45.20
The problem is:
Grr + 1
rGr + 1
r2Gθθ = δ(r −ρ)δ(θ −ϑ)
r
,
0 < r < ∞,
0 < θ < α,
G(r, 0, ρ, ϑ) = G(r, α, ρ, ϑ) = 0,
G(0, θ, ρ, ϑ) = 0
G(r, θ, ρ, ϑ) →0 as r →∞.
Let w = r eiθ and z = x + iy. We use the conformal mapping, z = wπ/α to map the sector to the upper half z plane.
The problem is (x, y) space is
Gxx + Gyy = δ(x −ξ)δ(y −ψ),
−∞< x < ∞,
0 < y < ∞,
G(x, 0, ξ, ψ) = 0,
G(x, y, ξ, ψ) →0 as x, y →∞.
We will solve this problem with the method of images. Note that the solution of,
Gxx + Gyy = δ(x −ξ)δ(y −ψ) −δ(x −ξ)δ(y + ψ),
−∞< x < ∞,
−∞< y < ∞,
G(x, y, ξ, ψ) →0 as x, y →∞,
satisﬁes the condition, G(x, 0, ξ, ψ) = 0. Since the inﬁnite space Green function for the Laplacian in two dimensions is
1
4π ln
 (x −ξ)2 + (y −ψ)2
,
the solution of this problem is,
G(x, y, ξ, ψ) = 1
4π ln
 (x −ξ)2 + (y −ψ)2
−1
4π ln
 (x −ξ)2 + (y + ψ)2
= 1
4π ln
(x −ξ)2 + (y −ψ)2
(x −ξ)2 + (y + ψ)2

.
1999

Now we solve for x and y in the conformal mapping.
z = wπ/α = (r eiθ)π/α
x + iy = rπ/α(cos(θπ/α) + i sin(θπ/α))
x = rπ/α cos(θπ/α),
y = rπ/α sin(θπ/α)
We substitute these expressions into G(x, y, ξ, ψ) to obtain G(r, θ, ρ, ϑ).
G(r, θ, ρ, ϑ) = 1
4π ln
(rπ/α cos(θπ/α) −ρπ/α cos(ϑπ/α))2 + (rπ/α sin(θπ/α) −ρπ/α sin(ϑπ/α))2
(rπ/α cos(θπ/α) −ρπ/α cos(ϑπ/α))2 + (rπ/α sin(θπ/α) + ρπ/α sin(ϑπ/α))2

= 1
4π ln
r2π/α + ρ2π/α −2rπ/αρπ/α cos(π(θ −ϑ)/α)
r2π/α + ρ2π/α −2rπ/αρπ/α cos(π(θ + ϑ)/α)

= 1
4π ln
(r/ρ)π/α/2 + (ρ/r)π/α/2 −cos(π(θ −ϑ)/α)
(r/ρ)π/α/2 + (ρ/r)π/α/2 −cos(π(θ + ϑ)/α)

= 1
4π ln
eπ ln(r/ρ)/α /2 + eπ ln(ρ/r)/α /2 −cos(π(θ −ϑ)/α)
eπ ln(r/ρ)/α /2 + eπ ln(ρ/r)/α /2 −cos(π(θ + ϑ)/α)

G(r, θ, ρ, ϑ) = 1
4π ln


cosh

π/α
ln
r
ρ

−cos(π(θ −ϑ)/α)
cosh

π/α
ln
r
ρ

−cos(π(θ + ϑ)/α)


Now recall that the solution of
∆u = f(x),
subject to the boundary condition,
u(x) = g(x),
is
u(x) =
Z Z
f(ξ)G(x; ξ) dAξ +
I
g(ξ)∇ξG(x; ξ) · ˆn dsξ.
2000

The normal directions along the lower and upper edges of the sector are −ˆθ and ˆθ, respectively. The gradient in polar
coordinates is
∇ξ = ˆρ ∂
∂ρ +
ˆϑ
ρ
∂
∂ϑ.
We only need to compute the ˆϑ component of the gradient of G. This is
1
ρ
∂
∂ρG =
sin(π(θ −ϑ)/α)
4αρ

cosh

π
α ln r
ρ

−cos(π(θ −ϑ)/α)
 +
sin(π(θ −ϑ)/α)
4αρ

cosh

π
α ln r
ρ

−cos(π(θ + ϑ)/α)

Along ϑ = 0, this is
1
ρGϑ(r, θ, ρ, 0) =
sin(πθ/α)
2αρ

cosh

π
α ln r
ρ

−cos(πθ/α)
.
Along ϑ = α, this is
1
ρGϑ(r, θ, ρ, α) = −
sin(πθ/α)
2αρ

cosh

π
α ln r
ρ

+ cos(πθ/α)
.
2001

The solution of our problem is
u(r, θ) =
Z c
∞
−
sin(πθ/α)
2αρ

cosh

π
α ln r
ρ

+ cos(πθ/α)
 dρ +
Z ∞
c
−
sin(πθ/α)
2αρ

cosh

π
α ln r
ρ

−cos(πθ/α)
 dρ
u(r, θ) =
Z ∞
c
−sin(πθ/α)
2αρ

cosh

π
α ln r
ρ

−cos(πθ/α)
 +
sin(πθ/α)
2αρ

cosh

π
α ln r
ρ

+ cos(πθ/α)
 dρ
u(r, θ) = −1
α sin
πθ
α

cos
πθ
α
 Z ∞
c
1
ρ

cosh2 
π
α ln r
ρ

−cos2   πθ
α
 dρ
u(r, θ) = −1
α sin
πθ
α

cos
πθ
α
 Z ∞
ln(c/r)
1
cosh2   πx
α

−cos2   πθ
α
 dx
u(r, θ) = −2
α sin
πθ
α

cos
πθ
α
 Z ∞
ln(c/r)
1
cosh
  2πx
α

−cos
  2πθ
α
 dx
Solution 45.21
First consider the Green function for
ut −κuxx = 0,
u(x, 0) = f(x).
The diﬀerential equation and initial condition is
Gt = κGxx,
G(x, 0; ξ) = δ(x −ξ).
The Green function is a solution of the homogeneous heat equation for the initial condition of a unit amount of heat
concentrated at the point x = ξ. You can verify that the Green function is a solution of the heat equation for t > 0
and that it has the property:
Z ∞
−∞
G(x, t; ξ) dx = 1,
for t > 0.
This property demonstrates that the total amount of heat is the constant 1. At time t = 0 the heat is concentrated at
the point x = ξ. As time increases, the heat diﬀuses out from this point.
2002

The solution for u(x, t) is the linear combination of the Green functions that satisﬁes the initial condition u(x, 0) =
f(x). This linear combination is
u(x, t) =
Z ∞
−∞
G(x, t; ξ)f(ξ) dξ.
G(x, t; 1) and G(x, t; −1) are plotted in Figure 45.5 for the domain t ∈[1/100..1/4], x ∈[−2..2] and κ = 1.
0.1
0.2
-2
-1
0
1
2
0
1
2
0.1
0.2
Figure 45.5: G(x, t; 1) and G(x, t; −1)
Now we consider the problem
ut = κuxx,
u(x, 0) = f(x) for x > 0,
u(0, t) = 0.
2003

Note that the solution of
Gt = κGxx,
x > 0,
t > 0,
G(x, 0; ξ) = δ(x −ξ) −δ(x + ξ),
satisﬁes the boundary condition G(0, t; ξ) = 0. We write the solution as the diﬀerence of inﬁnite space Green functions.
G(x, t; ξ) =
1
√
4πκt
e−(x−ξ)2/(4κt) −
1
√
4πκt
e−(x+ξ)2/(4κt)
=
1
√
4πκt

e−(x−ξ)2/(4κt) −e−(x+ξ)2/(4κt)
G(x, t; ξ) =
1
√
4πκt
e−(x2+ξ2)/(4κt) sinh
 xξ
2κt

Next we consider the problem
ut = κuxx,
u(x, 0) = f(x) for x > 0,
ux(0, t) = 0.
Note that the solution of
Gt = κGxx,
x > 0,
t > 0,
G(x, 0; ξ) = δ(x −ξ) + δ(x + ξ),
satisﬁes the boundary condition Gx(0, t; ξ) = 0. We write the solution as the sum of inﬁnite space Green functions.
G(x, t; ξ) =
1
√
4πκt
e−(x−ξ)2/(4κt) +
1
√
4πκt
e−(x+ξ)2/(4κt)
G(x, t; ξ) =
1
√
4πκt
e−(x2+ξ2)/(4κt) cosh
 xξ
2κt

The Green functions for the two boundary conditions are shown in Figure 45.6.
2004

0.050.1
0.150.20.250
0.2
0.4
0.6
0.8
1
0
1
2
.050.1
0.150.2
0.050.1
0.150.20.250
0.2
0.4
0.6
0.8
1
0
1
2
.050.1
0.150.2
Figure 45.6: Green functions for the boundary conditions u(0, t) = 0 and ux(0, t) = 0.
Solution 45.22
a) The Green function problem is
Gtt −c2Gxx = δ(t −τ)δ(x −ξ),
0 < x < L,
t > 0,
G(0, t; ξ, τ) = Gx(L, t; ξ, τ) = 0,
G(x, t; ξ, τ) = 0 for t < τ.
The condition that G is zero for t < τ makes this a causal Green function. We solve this problem by expanding G in
a series of eigenfunctions of the x variable. The coeﬃcients in the expansion will be functions of t. First we ﬁnd the
eigenfunctions of x in the homogeneous problem. We substitute the separation of variables u = X(x)T(t) into the
2005

homogeneous partial diﬀerential equation.
XT ′′ = c2X′′T
T ′′
c2T = X′′
X = −λ2
The eigenvalue problem is
X′′ = −λ2X,
X(0) = X′(L) = 0,
which has the solutions,
λn = (2n −1)π
2L
,
Xn = sin
(2n −1)πx
2L

,
n ∈N.
The series expansion of the Green function has the form,
G(x, t; ξ, τ) =
∞
X
n=1
gn(t) sin
(2n −1)πx
2L

.
We determine the coeﬃcients by substituting the expansion into the Green function diﬀerential equation.
Gtt −c2Gxx = δ(x −ξ)δ(t −τ)
∞
X
n=1
 
g′′
n(t) +
(2n −1)πc
2L
2
gn(t)
!
sin
(2n −1)πx
2L

= δ(x −ξ)δ(t −τ)
We need to expand the right side of the equation in the sine series
δ(x −ξ)δ(t −τ) =
∞
X
n=1
dn(t) sin
(2n −1)πx
2L

dn(t) = 2
L
Z L
0
δ(x −ξ)δ(t −τ) sin
(2n −1)πx
2L

dx
dn(t) = 2
L sin
(2n −1)πξ
2L

δ(t −τ)
2006

By equating coeﬃcients in the sine series, we obtain ordinary diﬀerential equation Green function problems for the gn’s.
g′′
n(t; τ) +
(2n −1)πc
2L
2
gn(t; τ) = 2
L sin
(2n −1)πξ
2L

δ(t −τ)
From the causality condition for G, we have the causality conditions for the gn’s,
gn(t; τ) = g′
n(t; τ) = 0 for t < τ.
The continuity and jump conditions for the gn are
gn(τ +; τ) = 0,
g′
n(τ +; τ) = 2
L sin
(2n −1)πξ
2L

.
A set of homogeneous solutions of the ordinary diﬀerential equation are

cos
(2n −1)πct
2L

, sin
(2n −1)πct
2L

Since the continuity and jump conditions are given at the point t = τ, a handy set of solutions to use for this problem
is the fundamental set of solutions at that point:

cos
(2n −1)πc(t −τ)
2L

,
2L
(2n −1)πc sin
(2n −1)πc(t −τ)
2L

The solution that satisﬁes the causality condition and the continuity and jump conditions is,
gn(t; τ) =
4
(2n −1)πc sin
(2n −1)πξ
2L

sin
(2n −1)πc(t −τ)
2L

H(t −τ).
Substituting this into the sum yields,
G(x, t; ξ, τ) = 4
πcH(t −τ)
∞
X
n=1
1
2n −1 sin
(2n −1)πξ
2L

sin
(2n −1)πc(t −τ)
2L

sin
(2n −1)πx
2L

.
We use trigonometric identities to write this in terms of traveling waves.
2007

G(x, t; ξ, τ) = 1
πcH(t −τ)
∞
X
n=1
1
2n −1
 
sin
(2n −1)π((x −ξ) −c(t −τ))
2L

+ sin
(2n −1)π((x −ξ) + c(t −τ))
2L

−sin
(2n −1)π((x + ξ) −c(t −τ))
2L

−sin
(2n −1)π((x + ξ) + c(t −τ))
2L
 !
b) Now we consider the Green function with the boundary conditions,
ux(0, t) = ux(L, t) = 0.
First we ﬁnd the eigenfunctions in x of the homogeneous problem. The eigenvalue problem is
X′′ = −λ2X,
X′(0) = X′(L) = 0,
which has the solutions,
λ0 = 0,
X0 = 1,
λn = nπ
L ,
Xn = cos
nπx
L

,
n = 1, 2, . . . .
The series expansion of the Green function for t > τ has the form,
G(x, t; ξ, τ) = 1
2g0(t) +
∞
X
n=1
gn(t) cos
nπx
L

.
(Note the factor of 1/2 in front of g0(t). With this, the integral formulas for all the coeﬃcients are the same.) We
2008

determine the coeﬃcients by substituting the expansion into the partial diﬀerential equation.
Gtt −c2Gxx = δ(x −ξ)δ(t −τ)
1
2g′′
0(t) +
∞
X
n=1

g′′
n(t) +
nπc
L
2
gn(t)

cos
nπx
L

= δ(x −ξ)δ(t −τ)
We expand the right side of the equation in the cosine series.
δ(x −ξ)δ(t −τ) = 1
2d0(t) +
∞
X
n=1
dn(t) cos
nπx
L

dn(t) = 2
L
Z L
0
δ(x −ξ)δ(t −τ) cos
nπx
L

dx
dn(t) = 2
L cos
nπξ
L

δ(t −τ)
By equating coeﬃcients in the cosine series, we obtain ordinary diﬀerential equations for the gn.
g′′
n(t; τ) +
nπc
L
2
gn(t; τ) = 2
L cos
nπξ
L

δ(t −τ),
n = 0, 1, 2, . . .
From the causality condition for G, we have the causality condiions for the gn,
gn(t; τ) = g′
n(t; τ) = 0 for t < τ.
The continuity and jump conditions for the gn are
gn(τ +; τ) = 0,
g′
n(τ +; τ) = 2
L cos
nπξ
L

.
The homogeneous solutions of the ordinary diﬀerential equation for n = 0 and n > 0 are respectively,
{1, t},

cos
nπct
L

, sin
nπct
L

.
2009

Since the continuity and jump conditions are given at the point t = τ, a handy set of solutions to use for this problem
is the fundamental set of solutions at that point:
{1, t −τ},

cos
nπc(t −τ)
L

, L
nπc sin
nπc(t −τ)
L

.
The solutions that satisfy the causality condition and the continuity and jump conditions are,
g0(t) = 2
L(t −τ)H(t −τ),
gn(t) =
2
nπc cos
nπξ
L

sin
nπc(t −τ)
L

H(t −τ).
Substituting this into the sum yields,
G(x, t; ξ, τ) = H(t −τ)
 
t −τ
L
+ 2
πc
∞
X
n=1
1
n cos
nπξ
L

sin
nπc(t −τ)
L

cos
nπx
L
!
.
We can write this as the sum of traveling waves.
G(x, t; ξ, τ) = t −τ
L
H(t−τ)+ 1
2πcH(t−τ)
∞
X
n=1
1
n
 
−sin
nπ((x −ξ) −c(t −τ))
2L

+ sin
nπ((x −ξ) + c(t −τ))
2L

−sin
nπ((x + ξ) −c(t −τ))
2L

+ sin
nπ((x + ξ) + c(t −τ))
2L
 !
2010

Solution 45.23
First we derive Green’s identity for this problem. We consider the integral of uL[v] −L[u]v on the domain 0 < x < 1,
0 < t < T.
Z T
0
Z 1
0
(uL[v] −L[u]v) dx dt
Z T
0
Z 1
0
 u(vtt −c2vxx −(utt −c2uxx)v

dx dt
Z T
0
Z 1
0
 ∂
∂x, ∂
∂t

·
 −c2(uvx −uxv), uvt −utv

dx dt
Now we can use the divergence theorem to write this as an integral along the boundary of the domain.
I
∂Ω
 −c2(uvx −uxv), uvt −utv

· n ds
The domain and the outward normal vectors are shown in Figure 45.7.
Writing out the boundary integrals, Green’s identity for this problem is,
Z T
0
Z 1
0
 u(vtt −c2vxx) −(utt −c2uxx)v

dx dt = −
Z 1
0
(uvt −utv)t=0 dx
+
Z 0
1
(uvt −utv)t=T dx −c2
Z T
0
(uvx −uxv)x=1 dt + c2
Z 1
T
(uvx −uxv)x=0 dt
The Green function problem is
Gtt −c2Gxx = δ(x −ξ)δ(t −τ),
0 < x, ξ < 1,
t, τ > 0,
Gx(0, t; ξ, τ) = Gx(1, t; ξ, τ) = 0,
t > 0, G(x, t; ξ, τ) = 0
for
t < τ.
If we consider G as a function of (ξ, τ) with (x, t) as parameters, then it satisﬁes:
Gττ −c2Gξξ = δ(x −ξ)δ(t −τ),
Gξ(x, t; 0, τ) = Gξ(x, t; 1, τ) = 0,
τ > 0, G(x, t; ξ, τ) = 0
for
τ > t.
2011

x=0
x=1
t=0
t=T
n=(0,-1)
n=(1,0)
n=(0,1)
n=(-1,0)
Figure 45.7: Outward normal vectors of the domain.
Now we apply Green’s identity for u = u(ξ, τ), (the solution of the wave equation), and v = G(x, t; ξ, τ), (the Green
function), and integrate in the (ξ, τ) variables. The left side of Green’s identity becomes:
Z T
0
Z 1
0
 u(Gττ −c2Gξξ) −(uττ −c2uξξ)G

dξ dτ
Z T
0
Z 1
0
(u(δ(x −ξ)δ(t −τ)) −(0)G) dξ dτ
u(x, t).
Since the normal derivative of u and G vanish on the sides of the domain, the integrals along ξ = 0 and ξ = 1 in
Green’s identity vanish. If we take T > t, then G is zero for τ = T and the integral along τ = T vanishes. The one
remaining integral is
−
Z 1
0
(u(ξ, 0)Gτ(x, t; ξ, 0) −uτ(ξ, 0)G(x, t; ξ, 0) dξ.
2012

Thus Green’s identity allows us to write the solution of the inhomogeneous problem.
u(x, t) =
Z 1
0
(uτ(ξ, 0)G(x, t; ξ, 0) −u(ξ, 0)Gτ(x, t; ξ, 0)) dξ.
With the speciﬁed initial conditions this becomes
u(x, t) =
Z 1
0
(G(x, t; ξ, 0) −ξ2(1 −ξ)2Gτ(x, t; ξ, 0)) dξ.
Now we substitute in the Green function that we found in the previous exercise. The Green function and its derivative
are,
G(x, t; ξ, 0) = t +
∞
X
n=1
2
nπc cos(nπξ) sin(nπct) cos(nπx),
Gτ(x, t; ξ, 0) = −1 −2
∞
X
n=1
cos(nπξ) cos(nπct) cos(nπx).
The integral of the ﬁrst term is,
Z 1
0
 
t +
∞
X
n=1
2
nπc cos(nπξ) sin(nπct) cos(nπx)
!
dξ = t.
The integral of the second term is
Z 1
0
ξ2(1 −ξ)2
 
1 + 2
∞
X
n=1
cos(nπξ) cos(nπct) cos(nπx)
!
dξ = 1
30 −3
∞
X
n=1
1
n4π4 cos(2nπx) cos(2nπct).
Thus the solution is
u(x, t) = 1
30 + t −3
∞
X
n=1
1
n4π4 cos(2nπx) cos(2nπct).
2013

For c = 1, the solution at x = 3/4, t = 7/2 is,
u(3/4, 7/2) = 1
30 + 7
2 −3
∞
X
n=1
1
n4π4 cos(3nπ/2) cos(7nπ).
Note that the summand is nonzero only for even terms.
u(3/4, 7/2) = 53
15 −
3
16π4
∞
X
n=1
1
n4 cos(3nπ) cos(14nπ)
= 53
15 −
3
16π4
∞
X
n=1
(−1)n
n4
= 53
15 −
3
16π4
−7π4
720
u(3/4, 7/2) = 12727
3840
2014

Chapter 46
Conformal Mapping
2015

46.1
Exercises
Exercise 46.1
Use an appropriate conformal map to ﬁnd a non-trivial solution to Laplace’s equation
uxx + uyy = 0,
on the wedge bounded by the x-axis and the line y = x with boundary conditions:
1. u = 0 on both sides.
2. du
dn = 0 on both sides (where n is the inward normal to the boundary).
Exercise 46.2
Consider
uxx + uyy = δ(x −ξ)δ(y −ψ),
on the quarter plane x, y > 0 with u(x, 0) = u(0, y) = 0 (and ξ, ψ > 0).
1. Use image sources to ﬁnd u(x, y; ξ, ψ).
2. Compare this to the solution which would be obtained using conformal maps and the Green function for the upper
half plane.
3. Finally use this idea and conformal mapping to discover how image sources are arrayed when the domain is now
the wedge bounded by the x-axis and the line y = x (with u = 0 on both sides).
Exercise 46.3
ζ = ξ + ıη is an analytic function of z, ζ = ζ(z). We assume that ζ′(z) is nonzero on the domain of interest. u(x, y)
is an arbitrary smooth function of x and y. When expressed in terms of ξ and η, u(x, y) = υ(ξ, η). In Exercise 8.15
we showed that
∂2υ
∂ξ2 + ∂2υ
∂η2 =

dζ
dz

−2 ∂2u
∂x2 + ∂2u
∂y2

.
2016

1. Show that if u satisﬁes Laplace’s equation in the z-plane,
uxx + uyy = 0,
then υ satisﬁes Laplace’s equation in the ζ-plane,
υξξ + υηη = 0,
2. Show that if u satisﬁes Helmholtz’s equation in the z-plane,
uxx + uyy = λu,
then in the ζ-plane υ satisﬁes
υξξ + υηη = λ

dz
dζ

2
υ.
3. Show that if u satisﬁes Poisson’s equation in the z-plane,
uxx + uyy = f(x, y),
then υ satisﬁes Poisson’s equation in the ζ-plane,
υξξ + υηη =

dz
dζ

2
φ(ξ, η),
where φ(ξ, η) = f(x, y).
4. Show that if in the z-plane, u satisﬁes the Green function problem,
uxx + uyy = δ(x −x0)δ(y −y0),
then in the ζ-plane, υ satisﬁes the Green function problem,
υξξ + υηη = δ(ξ −ξ0)δ(η −η0).
2017

Exercise 46.4
A semi-circular rod of inﬁnite extent is maintained at temperature T = 0 on the ﬂat side and at T = 1 on the curved
surface:
x2 + y2 = 1,
y > 0.
Use the conformal mapping
w = ξ + ıη = 1 + z
1 −z,
z = x + ıy,
to formulate the problem in terms of ξ and η. Solve the problem in terms of these variables. This problem is solved
with an eigenfunction expansion in Exercise ??. Verify that the two solutions agree.
Exercise 46.5
Consider Laplace’s equation on the domain −∞< x < ∞, 0 < y < π, subject to the mixed boundary conditions,
u = 1
on y = 0, x > 0,
u = 0
on y = π, x > 0,
uy = 0
on y = 0, y = π, x < 0.
Because of the mixed boundary conditions, (u and uy are given on separate parts of the same boundary), this problem
cannot be solved with separation of variables. Verify that the conformal map,
ζ = cosh−1(ez),
with z = x + ıy, ζ = ξ + ıη maps the inﬁnite interval into the semi-inﬁnite interval, ξ > 0, 0 < η < π. Solve Laplace’s
equation with the appropriate boundary conditions in the ζ plane by inspection. Write the solution u in terms of x and
y.
2018

46.2
Hints
Hint 46.1
Hint 46.2
Hint 46.3
Hint 46.4
Show that w = (1 + z)/(1 −z) maps the semi-disc, 0 < r < 1, 0 < θ < π to the ﬁrst quadrant of the w plane. Solve
the problem for v(ξ, η) by taking Fourier sine transforms in ξ and η.
To show that the solution for v(ξ, η) is equivalent to the series expression for u(r, θ), ﬁrst ﬁnd an analytic function
g(w) of which v(ξ, η) is the imaginary part. Change variables to z to obtain the analytic function f(z) = g(w). Expand
f(z) in a Taylor series and take the imaginary part to show the equivalence of the solutions.
Hint 46.5
To see how the boundary is mapped, consider the map,
z = log(cosh ζ).
The problem in the ζ plane is,
vξξ + vηη = 0,
ξ > 0,
0 < η < π,
vξ(0, η) = 0,
v(ξ, 0) = 1,
v(ξ, π) = 0.
To solve this, ﬁnd a plane that satisﬁes the boundary conditions.
2019

46.3
Solutions
Solution 46.1
We map the wedge to the upper half plane with the conformal transformation ζ = z4.
1. We map the wedge to the upper half plane with the conformal transformation ζ = z4. The new problem is
uξξ + uηη = 0,
u(ξ, 0) = 0.
This has the solution u = η. We transform this problem back to the wedge.
u(x, y) = ℑ
 z4
u(x, y) = ℑ
 x4 + ı4x3y −6x2y2 −ı4xy3 + y4
u(x, y) = 4x3y −4xy3
u(x, y) = 4xy
 x2 −y2
2. We don’t need to use conformal mapping to solve the problem with Neumman boundary conditions. u = c is a
solution to
uxx + uyy = 0,
du
dn = 0
on any domain.
Solution 46.2
1. We add image sources to satisfy the boundary conditions.
uxx + uyy = δ(x −ξ)δ(y −η) −δ(x + ξ)δ(y −η) −δ(x −ξ)δ(y + η) + δ(x + ξ)δ(y + η)
u = 1
2π

ln
p
(x −ξ)2 + (y −η)2

−ln
p
(x + ξ)2 + (y −η)2

−ln
p
(x −ξ)2 + (y + η)2

+ ln
p
(x + ξ)2 + (y + η)2

2020

u = 1
4π ln
((x −ξ)2 + (y −η)2) ((x + ξ)2 + (y + η)2)
((x + ξ)2 + (y −η)2) ((x −ξ)2 + (y + η)2)

2. The Green function for the upper half plane is
G = 1
4π ln
((x −ξ)2 + (y −η)2)
((x −ξ)2 + (y + η)2)

We use the conformal map,
c = z2,
c = a + ıb.
a = x2 −y2,
b = 2xy
We compute the Jacobian of the mapping.
J =

ax
ay
bx
by
 =

2x
−2y
2y
2x
 = 4
 x2 + y2
We transform the problem to the upper half plane, solve the problem there, and then transform back to the ﬁrst
quadrant.
uxx + uyy = δ(x −ξ)δ(y −η)
(uaa + ubb)

dc
dz

2
= 4
 x2 + y2
δ(a −α)δ(b −β)
(uaa + ubb) |2z|2 = 4
 x2 + y2
δ(a −α)δ(b −β)
uaa + ubb = δ(a −α)δ(b −β)
u = 1
4π ln
((a −α)2 + (b −β)2)
((a −α)2 + (b + β)2)

u = 1
4π ln
((x2 −y2 −ξ2 + η2)2 + (2xy −2ξη)2)
((x2 −y2 −ξ2 + η2)2 + (2xy + 2ξη)2)

u = 1
4π ln
((x −ξ)2 + (y −η)2) ((x + ξ)2 + (y + η)2)
((x + ξ)2 + (y −η)2) ((x −ξ)2 + (y + η)2)

2021

We obtain the some solution as before.
3. First consider
∆u = δ(x −ξ)δ(y −η),
u(x, 0) = u(x, x) = 0.
Enforcing the boundary conditions will require 7 image sources obtained from 4 odd reﬂections. Refer to Fig-
ure 46.1 to see the reﬂections pictorially. First we do a negative reﬂection across the line y = x, which adds a
negative image source at the point (η, ξ) This enforces the boundary condition along y = x.
∆u = δ(x −ξ)δ(y −η) −δ(x −η)δ(y −ξ),
u(x, 0) = u(x, x) = 0
Now we take the negative image of the reﬂection of these two sources across the line y = 0 to enforce the
boundary condition there.
∆u = δ(x −ξ)δ(y −η) −δ(x −η)δ(y −ξ) −δ(x −ξ)δ(y + η) + δ(x −η)δ(y + ξ)
The point sources are no longer odd symmetric about y = x. We add two more image sources to enforce that
boundary condition.
∆u = δ(x −ξ)δ(y −η) −δ(x −η)δ(y −ξ) −δ(x −ξ)δ(y + η) + δ(x −η)δ(y + ξ)
+ δ(x + η)δ(y −ξ) −δ(x + ξ)δ(y −η)
Now sources are no longer odd symmetric about y = 0. Finally we add two more image sources to enforce that
boundary condition. Now the sources are odd symmetric about both y = x and y = 0.
∆u = δ(x −ξ)δ(y −η) −δ(x −η)δ(y −ξ) −δ(x −ξ)δ(y + η) + δ(x −η)δ(y + ξ)
+ δ(x + η)δ(y −ξ) −δ(x + ξ)δ(y −η) + δ(x + ξ)δ(y + η) −δ(x + η)δ(y + ξ)
Solution 46.3
∂2υ
∂ξ2 + ∂2υ
∂η2 =

dζ
dz

−2 ∂2u
∂x2 + ∂2u
∂y2

.
2022

Figure 46.1: Odd reﬂections to enforce the boundary conditions.
1.
uxx + uyy = 0

dζ
dz

2
(υξξ + υηη) = 0
υξξ + υηη = 0
2023

2.
uxx + uyy = λu

dζ
dz

2
(υξξ + υηη) = λυ
υξξ + υηη = λ

dz
dζ

2
υ
3.
uxx + uyy = f(x, y)

dζ
dz

2
(υξξ + υηη) = φ(ξ, η)
υξξ + υηη =

dz
dζ

2
φ(ξ, η)
4. The Jacobian of the mapping is
J =

xξ
yξ
xη
yη
 = xξyη −xηyξ = x2
ξ + y2
ξ.
Thus the Dirac delta function on the right side gets mapped to
1
x2
ξ + y2
ξ
δ(ξ −ξ0)δ(η −η0).
Next we show that |dz/dζ|2 has the same value as the Jacobian.

dz
dζ

2
= (xξ + ıyξ)(xξ −ıyξ) = x2
ξ + y2
ξ
2024

Now we transform the Green function problem.
uxx + uyy = δ(x −x0)δ(y −y0)

dζ
dz

2
(υξξ + υηη) =
1
x2
ξ + y2
ξ
δ(ξ −ξ0)δ(η −η0)
υξξ + υηη = δ(ξ −ξ0)δ(η −η0)
Solution 46.4
The mapping,
w = 1 + z
1 −z,
maps the unit semi-disc to the ﬁrst quadrant of the complex plane.
We write the mapping in terms of r and θ.
ξ + ıη = 1 + r eıθ
1 −r eıθ = 1 −r2 + ı2r sin θ
1 + r2 −2r cos θ
ξ =
1 −r2
1 + r2 −2r cos θ
η =
2r sin θ
1 + r2 −2r cos θ
Consider a semi-circle of radius r. The image of this under the conformal mapping is a semi-circle of radius
2r
1−r2 and
center 1+r2
1−r2 in the ﬁrst quadrant of the w plane. This semi-circle intersects the ξ axis at 1−r
1+r and 1+r
1−r. As r ranges
from zero to one, these semi-circles cover the ﬁrst quadrant of the w plane. (See Figure 46.2.)
We also note how the boundary of the semi-disc is mapped to the boundary of the ﬁrst quadrant of the w plane.
The line segment θ = 0 is mapped to the real axis ξ > 1. The line segment θ = π is mapped to the real axis 0 < ξ < 1.
Finally, the semi-circle r = 1 is mapped to the positive imaginary axis.
2025

-1
1
1
1
2
3
4
5
1
2
3
4
5
Figure 46.2: The conformal map, w = 1+z
1−z.
The problem for v(ξ, η) is,
vξξ + vηη = 0,
ξ > 0,
η > 0,
v(ξ, 0) = 0,
v(0, η) = 1.
We will solve this problem with the Fourier sine transform. We take the Fourier sine transform of the partial diﬀerential
equation, ﬁrst in ξ and then in η.
−α2ˆv(α, η) + α
π v(0, η) + ˆv(α, η) = 0,
ˆv(α, 0) = 0
−α2ˆv(α, η) + α
π + ˆv(α, η) = 0,
ˆv(α, 0) = 0
−α2ˆˆv(α, β) +
α
π2β −β2ˆˆv(α, β) + β
π ˆv(α, 0) = 0
ˆˆv(α, β) =
α
π2β(α2 + β2)
Now we utilize the Fourier sine transform pair,
Fs
e−cx
=
ω/π
ω2 + c2,
2026

to take the inverse sine transform in α.
ˆv(ξ, β) = 1
πβ e−βξ
With the Fourier sine transform pair,
Fs
h
2 arctan
x
c
i
= 1
ω e−cω,
we take the inverse sine transform in β to obtain the solution.
v(ξ, η) = 2
π arctan
η
ξ

Since v is harmonic, it is the imaginary part of an analytic function g(w). By inspection, we see that this function is
g(w) = 2
π log(w).
We change variables to z, f(z) = g(w).
f(z) = 2
π log
1 + z
1 −z

We expand f(z) in a Taylor series about z = 0,
f(z) = 4
π
∞
X
n=1
oddn
zn
n ,
and write the result in terms of r and θ, z = r eıθ.
f(z) = 4
π
∞
X
n=1
oddn
rn eıθ
n
2027

u(r, θ) is the imaginary part of f(z).
u(r, θ) = 4
π
∞
X
n=1
oddn
1
nrn sin(nθ)
This demonstrates that the solutions obtained with conformal mapping and with an eigenfunction expansion in Exer-
cise ?? agree.
Solution 46.5
Instead of working with the conformal map from the z plane to the ζ plane,
ζ = cosh−1(ez),
it will be more convenient to work with the inverse map,
z = log(cosh ζ),
which maps the semi-inﬁnite strip to the inﬁnite one. We determine how the boundary of the domain is mapped so
that we know the appropriate boundary conditions for the semi-inﬁnite strip domain.
A
{ζ : ξ > 0, η = 0}
7→
{log(cosh(ξ)) : ξ > 0} = {z : x > 0, y = 0}
B
{ζ : ξ > 0, η = π}
7→
{log(−cosh(ξ)) : ξ > 0} = {z : x > 0, y = π}
C
{ζ : ξ = 0, 0 < η < π/2}
7→
{log(cos(η)) : 0 < η < π/2} = {z : x < 0, y = 0}
D
{ζ : ξ = 0, π/2 < η < π}
7→
{log(cos(η)) : π/2 < η < π} = {z : x < 0, y = π}
From the mapping of the boundary, we see that the solution v(ξ, η) = u(x, y), is 1 on the bottom of the semi-inﬁnite
strip, 0 on the top. The normal derivative of v vanishes on the vertical boundary. See Figure 46.3.
In the ζ plane, the problem is,
vξξ + vηη = 0,
ξ > 0,
0 < η < π,
vξ(0, η) = 0,
v(ξ, 0) = 1,
v(ξ, π) = 0.
2028

z=log(cosh(  ))
D
A
B
C
D
A
C
B
x
y
ξ
η
ζ
z=log(cosh(  ))
D
A
B
C
D
A
C
B
x
y
ξ
η
ζ
u =0
y
u =0
y
x
y
ξ
η
u=1
u=0
v=1
v=0
v =0
ξ
Figure 46.3: The mapping of the boundary conditions.
By inspection, we see that the solution of this problem is,
v(ξ, η) = 1 −η
π.
The solution in the z plane is
u(x, y) = 1 −1
πℑ
 cosh−1(ez)

,
where z = x + ıy. We will ﬁnd the imaginary part of cosh−1(ez) in order to write this explicitly in terms of x and y.
2029

Recall that we can write the cosh−1 in terms of the logarithm.
cosh−1(w) = log

w +
√
w2 −1

cosh−1(ez) = log

ez +
√
e2z −1

= log

ez 
1 +
√
1 −e−2z

= z + log

1 +
√
1 −e−2z

Now we need to ﬁnd the imaginary part. We’ll work from the inside out. First recall,
√x + ıy =
rp
x2 + y2 exp

ı tan−1 y
x

=
4p
x2 + y2 exp
 ı
2 tan−1 y
x

,
so that we can write the innermost factor as,
√
1 −e−2z =
p
1 −e−2x cos(2y) + ı e−2x sin(2y)
=
4p
(1 −e−2x cos(2y))2 + (e−2x sin(2y))2 exp
 ı
2 tan−1

e−2x sin(2y)
1 −e−2x cos(2y)

=
4p
1 −2 e−2x cos(2y) + e−4x exp
 ı
2 tan−1

sin(2y)
e2x −cos(2y)

We substitute this into the logarithm.
log

1 +
√
1 −e−2z

= log

1 +
4p
1 −2 e−2x cos(2y) + e−4x exp
 ı
2 tan−1

sin(2y)
e2x −cos(2y)

Now we can write η.
η = ℑ

z + log

1 +
√
1 −e−2z

η = y + tan−1


4p
1 −2 e−2x cos(2y) + e−4x sin

1
2 tan−1 
sin(2y)
e2x −cos(2y)

1 +
4p
1 −2 e−2x cos(2y) + e−4x cos

1
2 tan−1 
sin(2y)
e2x −cos(2y)



2030

Finally we have the solution, u(x, y).
u(x, y) = 1 −y
π −1
π tan−1


4p
1 −2 e−2x cos(2y) + e−4x sin

1
2 tan−1 
sin(2y)
e2x −cos(2y)

1 +
4p
1 −2 e−2x cos(2y) + e−4x cos

1
2 tan−1 
sin(2y)
e2x −cos(2y)



2031

Chapter 47
Non-Cartesian Coordinates
47.1
Spherical Coordinates
Writing rectangular coordinates in terms of spherical coordinates,
x = r cos θ sin φ
y = r sin θ sin φ
z = r cos φ.
2032

The Jacobian is

cos θ sin φ
−r sin θ sin φ
r cos θ cos φ
sin θ sin φ
r cos θ sin φ
r sin θ cos φ
cos φ
0
−r sin φ

= r2 sin φ

cos θ sin φ
−sin θ
cos θ cos φ
sin θ sin φ
cos θ
sin θ cos φ
cos φ
0
−sin φ

=
r2 sin φ(−cos2 θ sin2 φ −sin2 θ cos2 φ −cos2 θ cos2 φ −sin2 θ sin2 φ)

= r2 sin φ(sin2 φ + cos2 φ)
= r2 sin φ.
Thus we have that
ZZZ
V
f(x, y, z) dx dy dz =
ZZZ
V
f(r, θ, φ)r2 sin φ dr dθ dφ.
47.2
Laplace’s Equation in a Disk
Consider Laplace’s equation in polar coordinates
1
r
∂
∂r

r∂u
∂r

+ 1
r2
∂2u
∂θ2 = 0,
0 ≤r ≤1
subject to the the boundary conditions
1. u(1, θ) = f(θ)
2. ur(1, θ) = g(θ).
2033

We separate variables with u(r, θ) = R(r)T(θ).
1
r(R′T + rR′′T) + 1
r2RT ′′ = 0
r2R′′
R + rR′
R = −T ′′
T = λ
Thus we have the two ordinary diﬀerential equations
T ′′ + λT = 0,
T(0) = T(2π),
T ′(0) = T ′(2π)
r2R′′ + rR′ −λR = 0,
R(0) < ∞.
The eigenvalues and eigenfunctions for the equation in T are
λ0 = 0,
T0 = 1
2
λn = n2,
T (1)
n
= cos(nθ),
T (2)
n
= sin(nθ)
(I chose T0 = 1/2 so that all the eigenfunctions have the same norm.)
For λ = 0 the general solution for R is
R = c1 + c2 log r.
Requiring that the solution be bounded gives us
R0 = 1.
For λ = n2 > 0 the general solution for R is
R = c1rn + c2r−n.
Requiring that the solution be bounded gives us
Rn = rn.
2034

Thus the general solution for u is
u(r, θ) = a0
2 +
∞
X
n=1
rn [an cos(nθ) + bn sin(nθ)] .
For the boundary condition u(1, θ) = f(θ) we have the equation
f(θ) = a0
2 +
∞
X
n=1
[an cos(nθ) + bn sin(nθ)] .
If f(θ) has a Fourier series then the coeﬃcients are
a0 = 1
π
Z 2π
0
f(θ) dθ
an = 1
π
Z 2π
0
f(θ) cos(nθ) dθ
bn = 1
π
Z 2π
0
f(θ) sin(nθ) dθ.
For the boundary condition ur(1, θ) = g(θ) we have the equation
g(θ) =
∞
X
n=1
n [an cos(nθ) + bn sin(nθ)] .
g(θ) has a series of this form only if
Z 2π
0
g(θ) dθ = 0.
2035

The coeﬃcients are
an = 1
nπ
Z 2π
0
g(θ) cos(nθ) dθ
bn = 1
nπ
Z 2π
0
g(θ) sin(nθ) dθ.
47.3
Laplace’s Equation in an Annulus
Consider the problem
∇2u = 1
r
∂
∂r

r∂u
∂r

+ 1
r2
∂2u
∂θ2 = 0,
0 ≤r < a,
−π < θ ≤π,
with the boundary condition
u(a, θ) = θ2.
So far this problem only has one boundary condition. By requiring that the solution be ﬁnite, we get the boundary
condition
|u(0, θ)| < ∞.
By specifying that the solution be C1, (continuous and continuous ﬁrst derivative) we obtain
u(r, −π) = u(r, π)
and
∂u
∂θ (r, −π) = ∂u
∂θ (r, π).
We will use the method of separation of variables. We seek solutions of the form
u(r, θ) = R(r)Θ(θ).
2036

Substituting into the partial diﬀerential equation,
∂2u
∂r2 + 1
r
∂u
∂r + 1
r2
∂2u
∂θ2 = 0
R′′Θ + 1
rR′Θ = −1
r2RΘ′′
r2R′′
R
+ rR′
R = −Θ′′
Θ = λ
Now we have the boundary value problem for Θ,
Θ′′(θ) + λΘ(θ) = 0,
−π < θ ≤π,
subject to
Θ(−π) = Θ(π)
and
Θ′(−π) = Θ′(π)
We consider the following three cases for the eigenvalue, λ,
λ < 0. No linear combination of the solutions, Θ = exp(
√
−λθ), exp(−
√
−λθ), can satisfy the boundary conditions.
Thus there are no negative eigenvalues.
λ = 0. The general solution solution is Θ = a + bθ. By applying the boundary conditions, we get Θ = a. Thus we
have the eigenvalue and eigenfunction,
λ0 = 0,
A0 = 1.
λ > 0. The general solution is Θ = a cos(
√
λθ)+b sin(
√
λθ). Applying the boundary conditions yields the eigenvalues
λn = n2,
n = 1, 2, 3, . . .
with the associated eigenfunctions
An = cos(nθ)
and
Bn = sin(nθ).
2037

The equation for R is
r2R′′ + rR′ −λnR = 0.
In the case λ0 = 0, this becomes
R′′ = −1
rR′
R′ = a
r
R = a log r + b
Requiring that the solution be bounded at r = 0 yields (to within a constant multiple)
R0 = 1.
For λn = n2, n ≥1, we have
r2R′′ + rR′ −n2R = 0
Recognizing that this is an Euler equation and making the substitution R = rα,
α(α −1) + α −n2 = 0
α = ±n
R = arn + br−n.
requiring that the solution be bounded at r = 0 we obtain (to within a constant multiple)
Rn = rn
The general solution to the partial diﬀerential equation is a linear combination of the eigenfunctions
u(r, θ) = c0 +
∞
X
n=1
[cnrn cos nθ + dnrn sin nθ] .
2038

We determine the coeﬃcients of the expansion with the boundary condition
u(a, θ) = θ2 = c0 +
∞
X
n=1
[cnan cos nθ + dnan sin nθ] .
We note that the eigenfunctions 1, cos nθ, and sin nθ are orthogonal on −π ≤θ ≤π. Integrating the boundary
condition from −π to π yields
Z π
−π
θ2 dθ =
Z π
−π
c0 dθ
c0 = π2
3 .
Multiplying the boundary condition by cos mθ and integrating gives
Z π
−π
θ2 cos mθ dθ = cmam
Z π
−π
cos2 mθ dθ
cm = (−1)m8π
m2am
.
We multiply by sin mθ and integrate to get
Z π
−π
θ2 sin mθ dθ = dmam
Z π
−π
sin2 mθ dθ
dm = 0
Thus the solution is
u(r, θ) = π2
3 +
∞
X
n=1
(−1)n8π
n2an
rn cos nθ.
2039

Part VI
Calculus of Variations
2040

Chapter 48
Calculus of Variations
2041

48.1
Exercises
Exercise 48.1
Discuss the problem of minimizing
R α
0 ((y′)4 −6(y′)2) dx, y(0) = 0, y(α) = β. Consider both C1[0, α] and C1
p[0, α],
and comment (with reasons) on whether your answers are weak or strong minima.
Exercise 48.2
Consider
1.
R x1
x0 (a(y′)2 + byy′ + cy2) dx, y(x0) = y0, y(x1) = y1, a ̸= 0,
2.
R x1
x0 (y′)3 dx, y(x0) = y0, y(x1) = y1.
Can these functionals have broken extremals, and if so, ﬁnd them.
Exercise 48.3
Discuss ﬁnding a weak extremum for the following:
1.
R 1
0 ((y′′)2 −2xy) dx,
y(0) = y′(0) = 0,
y(1) =
1
120
2.
R 1
0
  1
2(y′)2 + yy′ + y′ + y

dx
3.
R b
a (y2 + 2xyy′) dx,
y(a) = A,
y(b) = B
4.
R 1
0 (xy + y2 −2y2y′) dx,
y(0) = 1,
y(1) = 2
Exercise 48.4
Find the natural boundary conditions associated with the following functionals:
1.
RR
D F(x, y, u, ux, uy) dx dy
2.
RR
D
 p(x, y)(u2
x + u2
y) −q(x, y)u2
dx dy +
R
Γ σ(x, y)u2 ds
Here D represents a closed boundary domain with boundary Γ, and ds is the arc-length diﬀerential. p and q are known
in D, and σ is known on Γ.
2042

Exercise 48.5
The equations for water waves with free surface y = h(x, t) and bottom y = 0 are
φxx + φyy = 0
0 < y < h(x, t),
φt + 1
2φ2
x + 1
2φ2
y + gy = 0
on y = h(x, t),
ht + φxhx −φy = 0,
on y = h(x, t),
φy = 0
on y = 0,
where the ﬂuid motion is described by φ(x, y, t) and g is the acceleration of gravity. Show that all these equations may
be obtained by varying the functions φ(x, y, t) and h(x, t) in the variational principle
δ
ZZ
R
 Z h(x,t)
0

φt + 1
2φ2
x + 1
2φ2
y + gy

dy
!
dx dt = 0,
where R is an arbitrary region in the (x, t) plane.
Exercise 48.6
Extremize the functional
R b
a F(x, y, y′) dx, y(a) = A, y(b) = B given that the admissible curves can not penetrate
the interior of a given region R in the (x, y) plane. Apply your results to ﬁnd the curves which extremize
R 10
0 (y′)3 dx,
y(0) = 0, y(10) = 0 given that the admissible curves can not penetrate the interior of the circle (x −5)2 + y2 = 9.
Exercise 48.7
Consider the functional
R √y ds where ds is the arc-length diﬀerential (ds =
p
(dx)2 + (dy)2). Find the curve or
curves from a given vertical line to a given ﬁxed point B = (x1, y1) which minimize this functional. Consider both the
classes C1 and C1
p.
Exercise 48.8
A perfectly ﬂexible uniform rope of length L hangs in equilibrium with one end ﬁxed at (x1, y1) so that it passes over
a frictionless pin at (x2, y2). What is the position of the free end of the rope?
2043

Exercise 48.9
The drag on a supersonic airfoil of chord c and shape y = y(x) is proportional to
D =
Z c
0
dy
dx
2
dx.
Find the shape for minimum drag if the moment of inertia of the contour with respect to the x-axis is speciﬁed; that
is, ﬁnd the shape for minimum drag if
Z c
0
y2 dx = A,
y(0) = y(c) = 0,
(c, A given).
Exercise 48.10
The deﬂection y of a beam executing free (small) vibrations of frequency ω satisﬁes the diﬀerential equation
d2
dx2

EI dy
dx

−ρω2y = 0,
where EI is the ﬂexural rigidity and ρ is the linear mass density. Show that the deﬂection modes are extremals of the
problem
δω2 ≡δ
 R L
0 EI(y′′)2 dx
R L
0 ρy2 dx
!
= 0,
(L = length of beam)
when appropriate homogeneous end conditions are prescribed. Show that stationary values of the ratio are the squares
of the natural frequencies.
Exercise 48.11
A boatman wishes to steer his boat so as to minimize the transit time required to cross a river of width l. The path of
the boat is given parametrically by
x = X(t),
y = Y (t),
for 0 ≤t ≤T. The river has no cross currents, so the current velocity is directed downstream in the y-direction. v0 is
the constant boat speed relative to the surrounding water, and w = w(x, y, t) denotes the downstream river current at
point (x, y) at time t. Then,
˙X(t) = v0 cos α(t),
˙Y (t) = v0 sin α(t) + w,
2044

where α(t) is the steering angle of the boat at time t. Find the steering control function α(t) and the ﬁnal time T that
will transfer the boat from the initial state (X(0), Y (0)) = (0, 0) to the ﬁnal state at X(t) = l in such a way as to
minimize T.
Exercise 48.12
Two particles of equal mass m are connected by an inextensible string which passes through a hole in a smooth
horizontal table. The ﬁrst particle is on the table moving with angular velocity ω =
p
g/α in a circular path, of radius
α, around the hole. The second particle is suspended vertically and is in equilibrium. At time t = 0, the suspended
mass is pulled downward a short distance and released while the ﬁrst mass continues to rotate.
1. If x represents the distance of the second mass below its equilibrium at time t and θ represents the angular
position of the ﬁrst particle at time t, show that the Lagrangian is given by
L = m

˙x2 + 1
2(α −x)2 ˙θ2 + gx

and obtain the equations of motion.
2. In the case where the displacement of the suspended mass from equilibrium is small, show that the suspended
mass performs small vertical oscillations and ﬁnd the period of these oscillations.
Exercise 48.13
A rocket is propelled vertically upward so as to reach a prescribed height h in minimum time while using a given ﬁxed
quantity of fuel. The vertical distance x(t) above the surface satisﬁes,
m¨x = −mg + mU(t),
x(0) = 0,
˙(x)(0) = 0,
where U(t) is the acceleration provided by engine thrust. We impose the terminal constraint x(T) = h, and we wish
to ﬁnd the particular thrust function U(t) which will minimize T assuming that the total thrust of the rocket engine
over the entire thrust time is limited by the condition,
Z T
0
U 2(t) dt = k2.
Here k is a given positive constant which measures the total amount of fuel available.
2045

Exercise 48.14
A space vehicle moves along a straight path in free space. x(t) is the distance to its docking pad, and a, b are its
position and speed at time t = 0. The equation of motion is
¨x = M sin V,
x(0) = a,
˙x(0) = b,
where the control function V (t) is related to the rocket acceleration U(t) by U = M sin V , M = const. We wish
to dock the vehicle in minimum time; that is, we seek a thrust function U(t) which will minimize the ﬁnal time T
while bringing the vehicle to rest at the origin with x(T) = 0, ˙x(T) = 0. Find U(t), and in the (x, ˙x)-plane plot the
corresponding trajectory which transfers the state of the system from (a, b) to (0, 0). Account for all values of a and b.
Exercise 48.15
Find a minimum for the functional I(y) =
R m
0
√y + h
p
1 + (y′)2 dx in which h > 0, y(0) = 0, y(m) = M > −h.
Discuss the nature of the minimum, (i.e., weak, strong, . . . ).
Exercise 48.16
Show that for the functional
R
n(x, y)
p
1 + (y′)2 dx, where n(x, y) ≥0 in some domain D, the Weierstrass E function
E(x, y, q, y′) is non-negative for arbitrary ﬁnite p and y′ at any point of D. What is the implication of this for Fermat’s
Principle?
Exercise 48.17
Consider the integral
R 1+y2
(y′)2 dx between ﬁxed limits. Find the extremals, (hyperbolic sines), and discuss the Jacobi,
Legendre, and Weierstrass conditions and their implications regarding weak and strong extrema. Also consider the value
of the integral on any extremal compared with its value on the illustrated strong variation. Comment!
PiQi are vertical segments, and the lines QiPi+1 are tangent to the extremal at Pi+1.
Exercise 48.18
Consider I =
R x1
x0 y′(1 + x2y′) dx, y(x0) = y0, y(x1) = y1. Can you ﬁnd continuous curves which will minimize I if
(i)
x0 = −1,
y0 = 1,
x1 = 2,
y1 = 4,
(ii)
x0 = 1,
y0 = 3,
x1 = 2,
y1 = 5,
(iii)
x0 = −1,
y0 = 1,
x1 = 2,
y1 = 1.
2046

Exercise 48.19
Starting from
ZZ
D
(Qx −Py) dx dy =
Z
Γ
(P dx + Q dy)
prove that
(a)
ZZ
D
φψxx dx dy =
ZZ
D
ψφxx dx dy +
Z
Γ
(φψx −ψφx) dy,
(b)
ZZ
D
φψyy dx dy =
ZZ
D
ψφyy dx dy −
Z
Γ
(φψy −ψφy) dx,
(c)
ZZ
D
φψxy dx dy =
ZZ
D
ψφxy dx dy −1
2
Z
Γ
(φψx −ψφx) dx + 1
2
Z
Γ
(φψy −ψφy) dy.
Then, consider
I(u) =
Z t1
t0
ZZ
D
 −(uxx + uyy)2 + 2(1 −µ)(uxxuyy −u2
xy)

dx dy dt.
Show that
δI =
Z t1
t0
ZZ
D
(−∇4u)δu dx dy dt +
Z t1
t0
Z
Γ

P(u)δu + M(u)∂(δu)
∂n

ds dt,
where P and M are the expressions we derived in class for the problem of the vibrating plate.
Exercise 48.20
For the following functionals use the Rayleigh-Ritz method to ﬁnd an approximate solution of the problem of minimizing
the functionals and compare your answers with the exact solutions.
•
Z 1
0
 (y′)2 −y2 −2xy

dx,
y(0) = 0 = y(1).
For this problem take an approximate solution of the form
y = x(1 −x) (a0 + a1x + · · · + anxn) ,
2047

and carry out the solutions for n = 0 and n = 1.
•
Z 2
0
 (y′)2 + y2 + 2xy

dx,
y(0) = 0 = y(2).
•
Z 2
1

x(y′)2 −x2 −1
x
y2 −2x2y

dx,
y(1) = 0 = y(2)
Exercise 48.21
Let K(x) belong to L1(−∞, ∞) and deﬁne the operator T on L2(−∞, ∞) by
Tf(x) =
Z ∞
−∞
K(x −y)f(y) dy.
1. Show that the spectrum of T consists of the range of the Fourier transform ˆK of K, (that is, the set of all values
ˆK(y) with −∞< y < ∞), plus 0 if this is not already in the range. (Note: From the assumption on K it follows
that ˆK is continuous and approaches zero at ±∞.)
2. For λ in the spectrum of T, show that λ is an eigenvalue if and only if ˆK takes on the value λ on at least some
interval of positive length and that every other λ in the spectrum belongs to the continuous spectrum.
3. Find an explicit representation for (T −λI)−1f for λ not in the spectrum, and verify directly that this result
agrees with that givenby the Neumann series if λ is large enough.
Exercise 48.22
Let U be the space of twice continuously diﬀerentiable functions f on [−1, 1] satisfying f(−1) = f(1) = 0, and
W = C[−1, 1]. Let L : U 7→W be the operator
d2
dx2. Call λ in the spectrum of L if the following does not occur:
There is a bounded linear transformation T : W 7→U such that (L −λI)Tf = f for all f ∈W and T(L −λI)f = f
for all f ∈U. Determine the spectrum of L.
2048

Exercise 48.23
Solve the integral equations
1. φ(x) = x + λ
Z 1
0
 x2y −y2
φ(y) dy
2. φ(x) = x + λ
Z x
0
K(x, y)φ(y) dy
where
K(x, y) =
(
sin(xy)
for x ≥1 and y ≤1,
0
otherwise
In both cases state for which values of λ the solution obtained is valid.
Exercise 48.24
1. Suppose that K = L1L2, where L1L2 −L2L1 = I. Show that if x is an eigenvector of K corresponding to the
eigenvalue λ, then L1x is an eigenvector of K corresponding to the eigenvalue λ −1, and L2x is an eigenvector
corresponding to the eigenvalue λ + 1.
2. Find the eigenvalues and eigenfunctions of the operator K ≡−d
dt + t2
4 in the space of functions u ∈L2(−∞, ∞).
(Hint: L1 = t
2 + d
dt, L2 = t
2 −d
dt. e−t2/4 is the eigenfunction corresponding to the eigenvalue 1/2.)
Exercise 48.25
Prove that if the value of λ = λ1 is in the residual spectrum of T, then λ1 is in the discrete spectrum of T ∗.
Exercise 48.26
Solve
1.
u′′(t) +
Z 1
0
sin(k(s −t))u(s) ds = f(t),
u(0) = u′(0) = 0.
2049

2.
u(x) = λ
Z π
0
K(x, s)u(s) ds
where
K(x, s) = 1
2 log

sin
  x+s
2

sin
  x−s
2

 =
∞
X
n=1
sin nx sin ns
n
3.
φ(s) = λ
Z 2π
0
1
2π
1 −h2
1 −2h cos(s −t) + h2φ(t) dt,
|h| < 1
4.
φ(x) = λ
Z π
−π
cosn(x −ξ)φ(ξ) dξ
Exercise 48.27
Let K(x, s) = 2π2 −6π|x −s| + 3(x −s)2.
1. Find the eigenvalues and eigenfunctions of
φ(x) = λ
Z 2π
0
K(x, s)φ(s) ds.
(Hint: Try to ﬁnd an expansion of the form
K(x, s) =
∞
X
n=−∞
cn eın(x−s) .)
2. Do the eigenfunctions form a complete set? If not, show that a complete set may be obtained by adding a suitable
set of solutions of
Z 2π
0
K(x, s)φ(s) ds = 0.
2050

3. Find the resolvent kernel Γ(x, s, λ).
Exercise 48.28
Let K(x, s) be a bounded self-adjoint kernel on the ﬁnite interval (a, b), and let T be the integral operator on L2(a, b)
with kernel K(x, s). For a polynomial p(t) = a0+a1t+· · ·+antn we deﬁne the operator p(T) = a0I+a1T +· · ·+anT n.
Prove that the eigenvalues of p(T) are exactly the numbers p(λ) with λ an eigenvalue of T.
Exercise 48.29
Show that if f(x) is continuous, the solution of
φ(x) = f(x) + λ
Z ∞
0
cos(2xs)φ(s) ds
is
φ(x) = f(x) + λ
R ∞
0 f(s) cos(2xs) ds
1 −πλ2/4
.
Exercise 48.30
Consider
Lu = 0 in D,
u = f on C,
where
Lu ≡uxx + uyy + aux + buy + cu.
Here a, b and c are continuous functions of (x, y) on D + C. Show that the adjoint L∗is given by
L∗v = vxx + vyy −avx −bvy + (c −ax −by)v
and that
Z
D
(vLu −uL∗v) =
Z
C
H(u, v),
(48.1)
2051

where
H(u, v) ≡(vux −uvx + auv) dy −(vuy −uvy + buv) dx
=

v∂u
∂n −u∂v
∂n + auv ∂x
∂n + buv ∂y
∂n

ds.
Take v in (48.1) to be the harmonic Green function G given by
G(x, y; ξ, η) = 1
2π log
 
1
p
(x −ξ)2 + (y −η)2
!
+ · · · ,
and show formally, (use Delta functions), that (48.1) becomes
−u(ξ, η) −
Z
D
u(L∗−∆)G dx dy =
Z
C
H(u, G)
(48.2)
where u satisﬁes Lu = 0, (∆G = δ in D, G = 0 on C). Show that (48.2) can be put into the forms
u +
Z
D
 (c −ax −by)G −aGx −bGy

u dx dy = U
(48.3)
and
u +
Z
D
(aux + buy + cu)G dx dy = U,
(48.4)
where U is the known harmonic function in D with assumes the boundary values prescribed for u. Finally, rigorously
show that the integrodiﬀerential equation (48.4) can be solved by successive approximations when the domain D is
small enough.
Exercise 48.31
Find the eigenvalues and eigenfunctions of the following kernels on the interval [0, 1].
1.
K(x, s) = min(x, s)
2052

2.
K(x, s) = emin(x,s)
(Hint: φ′′ + φ′ + λ ex φ = 0 can be solved in terms of Bessel functions.)
Exercise 48.32
Use Hilbert transforms to evaluate
1. −
Z ∞
−∞
sin(kx) sin(lx)
x2 −z2
dx
2. −
Z ∞
−∞
cos(px) −cos(qx)
x2
dx
3. −
Z ∞
−∞
−(x2 −ab) sin x + (a + b)x cos x
x(x2 + a2)(x2 + b2)
dx
Exercise 48.33
Show that
−
Z ∞
−∞
(1 −t2)1/2 log(1 + t)
t −x
dt = π

x log 2 −1 + (1 −x2)1/2 π
2 −arcsin(x)

.
Exercise 48.34
Let C be a simple closed contour. Let g(t) be a given function and consider
1
ıπ −
Z
C
f(t) dt
t −t0
= g(t0)
(48.5)
Note that the left side can be written as F +(t0)+F −(t0). Deﬁne a function W(z) such that W(z) = F(z) for z inside
C and W(z) = −F(z) for z outside C. Proceeding in this way, show that the solution of (48.5) is given by
f(t0) = 1
ıπ −
Z
C
g(t) dt
t −t0
.
2053

Exercise 48.35
If C is an arc with endpoints α and β, evaluate
(i)
1
ıπ −
Z
C
1
(τ −β)1−γ(τ −α)γ(τ −ζ) dτ,
where 0 < γ < 1
(ii)
1
ıπ −
Z
C
τ −β
τ −α
γ
τ n
τ −ζ dτ,
where 0 < γ < 1,
integer n ≥0.
Exercise 48.36
Solve
−
Z 1
−1
φ(y)
y2 −x2 dy = f(x).
Exercise 48.37
Solve
1
ıπ −
Z 1
0
f(t)
t −x dt = λf(x),
where −1 < λ < 1.
Are there any solutions for λ > 1? (The operator on the left is self-adjoint. Its spectrum is −1 ≤λ ≤1.)
Exercise 48.38
Show that the general solution of
tan(x)
π
−
Z 1
0
f(t)
t −x dt = f(x)
is
f(x) =
k sin(x)
(1 −x)1−x/πxx/π .
Exercise 48.39
Show that the general solution of
f ′(x) + λ −
Z
C
f(t)
t −x dt = 1
2054

is given by
f(x) =
1
ıπλ + k e−ıπλx,
(k is a constant). Here C is a simple closed contour, λ a constant and f(x) a diﬀerentiable function on C. Generalize
the result to the case of an arbitrary function g(x) on the right side, where g(x) is analytic inside C.
Exercise 48.40
Show that the solution of
−
Z
C

1
t −x + P(t −x)

f(t) dt = g(x)
is given by
f(t) = −1
π2 −
Z
C
g(τ)
τ −t dτ −1
π2
Z
C
g(τ)P(τ −t) dτ.
Here C is a simple closed curve, and P(t) is a given entire function of t.
Exercise 48.41
Solve
−
Z 1
0
f(t)
t −x dt + −
Z 3
2
f(t)
t −x dt = x
where this equation is to hold for x in either (0, 1) or (2, 3).
Exercise 48.42
Solve
Z x
0
f(t)
√x −t dt + A
Z 1
x
f(t)
√t −x dt = 1
where A is a real positive constant. Outline brieﬂy the appropriate method of A is a function of x.
2055

48.2
Hints
Hint 48.1
Hint 48.2
Hint 48.3
Hint 48.4
Hint 48.5
Hint 48.6
Hint 48.7
Hint 48.8
Hint 48.9
Hint 48.10
2056

Hint 48.11
Hint 48.12
Hint 48.13
Hint 48.14
Hint 48.15
Hint 48.16
Hint 48.17
Hint 48.18
Hint 48.19
Hint 48.20
Hint 48.21
2057

Hint 48.22
Hint 48.23
Hint 48.24
Hint 48.25
Hint 48.26
Hint 48.27
Hint 48.28
Hint 48.29
Hint 48.30
Hint 48.31
Hint 48.32
2058

Hint 48.33
Hint 48.34
Hint 48.35
Hint 48.36
Hint 48.37
Hint 48.38
Hint 48.39
Hint 48.40
Hint 48.41
Hint 48.42
2059

48.3
Solutions
Solution 48.1
C1[0, α] Extremals
Admissible Extremal. First we consider continuously diﬀerentiable extremals. Because the Lagrangian is a
function of y′ alone, we know that the extremals are straight lines. Thus the admissible extremal is
ˆy = β
αx.
Legendre Condition.
ˆFy′y′ = 12(ˆy′)2 −12
= 12
 β
α
2
−1
!





< 0
for |β/α| < 1
= 0
for |β/α| = 1
> 0
for |β/α| > 1
Thus we see that β
αx may be a minimum for |β/α| ≥1 and may be a maximum for |β/α| ≤1.
Jacobi Condition. Jacobi’s accessory equation for this problem is
( ˆF,y′y′h′)′ = 0
 
12
 β
α
2
−1
!
h′
!′
= 0
h′′ = 0
The problem h′′ = 0, h(0) = 0, h(c) = 0 has only the trivial solution for c > 0. Thus we see that there are no
conjugate points and the admissible extremal satisﬁes the strengthened Legendre condition.
2060

A Weak Minimum. For |β/α| > 1 the admissible extremal β
αx is a solution of
the Euler equation, and satisﬁes the strengthened Jacobi and Legendre conditions.
Thus it is a weak minima. (For |β/α| < 1 it is a weak maxima for the same
reasons.)
Weierstrass Excess Function. The Weierstrass excess function is
E(x, ˆy, ˆy′, w) = F(w) −F(ˆy′) −(w −ˆy′)F,y′(ˆy′)
= w4 −6w2 −(ˆy′)4 + 6(ˆy′)2 −(w −ˆy′)(4(ˆy′)3 −12ˆy′)
= w4 −6w2 −
β
α
4
+ 6
β
α
2
−(w −β
α)(4
β
α
3
−12β
α)
= w4 −6w2 −w
 
4β
α
β
α
2
−3
!
+ 3
β
α
4
−6
β
α
2
We can ﬁnd the stationary points of the excess function by examining its derivative. (Let λ = β/α.)
E′(w) = 4w3 −12w + 4λ
 (λ)2 −3

= 0
w1 = λ,
w2 = 1
2

−λ −
√
4 −λ2

w3 = 1
2

−λ +
√
4 −λ2

The excess function evaluated at these points is
E(w1) = 0,
E(w2) = 3
2

3λ4 −6λ2 −6 −
√
3λ(4 −λ2)3/2
,
E(w3) = 3
2

3λ4 −6λ2 −6 +
√
3λ(4 −λ2)3/2
.
E(w2) is negative for −1 < λ <
√
3 and E(w3) is negative for −
√
3 < λ < 1. This implies that the weak
minimum ˆy = βx/α is not a strong local minimum for |λ| <
√
3|.
Since E(w1) = 0, we cannot use the
Weierstrass excess function to determine if ˆy = βx/α is a strong local minima for |β/α| >
√
3.
2061

C1
p[0, α] Extremals
Erdmann’s Corner Conditions. Erdmann’s corner conditions require that
ˆF,y′ = 4(ˆy′)3 −12ˆy′
and
ˆF −ˆy′ ˆF,y′ = (ˆy′)4 −6(ˆy′)2 −ˆy′(4(ˆy′)3 −12ˆy′)
are continuous at corners. Thus the quantities
(ˆy′)3 −3ˆy′
and
(ˆy′)4 −2(ˆy′)2
are continuous. Denoting p = ˆy′
−and q = ˆy′
+, the ﬁrst condition has the solutions
p = q,
p = 1
2

−q ±
√
3
p
4 −q2

.
The second condition has the solutions,
p = ±q,
p = ±
p
2 −q2
Combining these, we have
p = q,
p =
√
3, q = −
√
3,
p = −
√
3, q =
√
3.
Thus we see that there can be a corner only when ˆy′
−= ±
√
3 and ˆy′
+ = ∓
√
3.
Case 1, β = ±
√
3α. Notice the the Lagrangian is minimized point-wise if y′ =
±
√
3. For this case the unique, strong global minimum is
ˆy =
√
3 sign(β)x.
Case 2, |β| <
√
3|α|.
For this case there are an inﬁnite number of strong
minima. Any piecewise linear curve satisfying y′
−(x) = ±
√
3 and y′
+(x) = ±
√
3
and y(0) = 0, y(α) = β is a strong minima.
Case 3, |β| >
√
3|α|. First note that the extremal cannot have corners. Thus the
unique extremal is ˆy = β
αx. We know that this extremal is a weak local minima.
2062

Solution 48.2
1.
Z x1
x0
(a(y′)2 + byy′ + cy2) dx,
y(x0) = y0,
y(x1) = y1,
a ̸= 0
Erdmann’s First Corner Condition. ˆFy′ = 2aˆy′ +bˆy must be continuous at a corner. This implies that ˆy must
be continuous, i.e., there are no corners.
The functional cannot have broken extremals.
2.
Z x1
x0
(y′)3 dx,
y(x0) = y0,
y(x1) = y1
Erdmann’s First Corner Condition. ˆFy′ = 3(y′)2 must be continuous at a corner. This implies that ˆy′
−= ˆy′
+.
Erdmann’s Second Corner Condition. ˆF −ˆy′ ˆFy′ = (ˆy′)3−ˆy′3(ˆy′)2 = −2(ˆy′)3 must be continuous at a corner.
This implies that ˆy is continuous at a corner, i.e. there are no corners.
The functional cannot have broken extremals.
Solution 48.3
1.
Z 1
0
 (y′′)2 −2xy

dx,
y(0) = y′(0) = 0,
y(1) =
1
120
Euler’s Diﬀerential Equation. We will consider C4 extremals which satisfy Euler’s DE,
( ˆF,y′′)′′ −( ˆF,y′)′ + ˆF,y = 0.
For the given Lagrangian, this is,
(2ˆy′′)′′ −2x = 0.
2063

Natural Boundary Condition. The ﬁrst variation of the performance index is
δJ =
Z 1
0
( ˆF,yδy + ˆF,y′δy′ + ˆFy′′δy′′) dx.
From the given boundary conditions we have δy(0) = δy′(0) = δy(1) = 0. Using Euler’s DE, we have,
δJ =
Z 1
0
(( ˆFy′ −( ˆF,y′′)′)′δy + ˆF,y′δy′ + ˆFy′′δy′′) dx.
Now we apply integration by parts.
δJ =
h
( ˆFy′ −( ˆF,y′′)′)δy
i1
0 +
Z 1
0
(−( ˆFy′ −( ˆF,y′′)′)δy′ + ˆF,y′δy′ + ˆFy′′δy′′) dx
=
Z 1
0
(( ˆF,y′′)′δy′ + ˆFy′′δy′′) dx
=
h
ˆF,y′′δy′i1
0
= ˆF,y′′(1)δy′(1)
In order that the ﬁrst variation vanish, we need the natural boundary condition ˆF,y′′(1) = 0. For the given
Lagrangian, this condition is
ˆy′′(1) = 0.
The Extremal BVP. The extremal boundary value problem is
y′′′′ = x,
y(0) = y′(0) = y′′(1) = 0,
y(1) =
1
120.
The general solution of the diﬀerential equation is
y = c0 + c1x + c2x2 + c3x3 +
1
120x5.
2064

Applying the boundary conditions, we see that the unique admissible extremal is
ˆy = x2
120(x3 −5x + 5).
This may be a weak extremum for the problem.
Legendre’s Condition. Since
ˆF,y′′y′′ = 2 > 0,
the strengthened Legendre condition is satisﬁed.
Jacobi’s Condition. The second variation for F(x, y, y′′) is
d2J
dϵ2

ϵ=0
=
Z b
a

ˆF,y′′y′′(h′′)2 + 2 ˆF,yy′′hh′′ + ˆF,yyh2
dx
Jacobi’s accessory equation is,
(2 ˆF,y′′y′′h′′ + 2 ˆF,yy′′h)′′ + 2 ˆF,yy′′h′′ + 2 ˆF,yyh = 0,
(h′′)′′ = 0
Since the boundary value problem,
h′′′′ = 0,
h(0) = h′(0) = h(c) = h′′(c) = 0,
has only the trivial solution for all c > 0 the strengthened Jacobi condition is satisﬁed.
A Weak Minimum. Since the admissible extremal,
ˆy = x2
120(x3 −5x + 5),
satisﬁes the strengthened Legendre and Jacobi conditions, we conclude that it is
a weak minimum.
2065

2.
Z 1
0
1
2(y′)2 + yy′ + y′ + y

dx
Boundary Conditions. Since no boundary conditions are speciﬁed, we have the Euler boundary conditions,
ˆF,y′(0) = 0,
ˆF,y′(1) = 0.
The derivatives of the integrand are,
F,y = y′ + 1,
F,y′ = y′ + y + 1.
The Euler boundary conditions are then
ˆy′(0) + ˆy(0) + 1 = 0,
ˆy′(1) + ˆy(1) + 1 = 0.
Erdmann’s Corner Conditions. Erdmann’s ﬁrst corner condition speciﬁes that
ˆFy′(x) = ˆy′(x) + ˆy(x) + 1
must be continuous at a corner. This implies that ˆy′(x) is continuous at corners, which means that there are no
corners.
Euler’s Diﬀerential Equation. Euler’s DE is
(F,y′)′ = Fy,
y′′ + y′ = y′ + 1,
y′′ = 1.
The general solution is
y = c0 + c1x + 1
2x2.
2066

The boundary conditions give us the constraints,
c0 + c1 + 1 = 0,
c0 + 2c1 + 5
2 = 0.
The extremal that satisﬁes the Euler DE and the Euler BC’s is
ˆy = 1
2
 x2 −3x + 1

.
Legendre’s Condition. Since the strengthened Legendre condition is satisﬁed,
ˆF,y′y′(x) = 1 > 0,
we conclude that the extremal is a weak local minimum of the problem.
Jacobi’s Condition. Jacobi’s accessory equation for this problem is,

ˆF,y′y′h′′
−

ˆF,yy −( ˆF,yy′)′
h = 0,
h(0) = h(c) = 0,
(h′)′ −(−(1)′) h = 0,
h(0) = h(c) = 0,
h′′ = 0,
h(0) = h(c) = 0,
Since this has only trivial solutions for c > 0 we conclude that there are no conjugate points. The extremal
satisﬁes the strengthened Jacobi condition.
The only admissible extremal,
ˆy = 1
2
 x2 −3x + 1

,
satisﬁes the strengthened Legendre and Jacobi conditions and is thus a weak
extremum.
2067

3.
Z b
a
(y2 + 2xyy′) dx,
y(a) = A,
y(b) = B
Euler’s Diﬀerential Equation. Euler’s diﬀerential equation,
(F,y′)′ = Fy,
(2xy)′ = 2y + 2xy′,
2y + 2xy′ = 2y + 2xy′,
is trivial. Every C1 function satisﬁes the Euler DE.
Erdmann’s Corner Conditions. The expressions,
ˆF,y′ = 2xy,
ˆF −ˆy′ ˆF,y′ = ˆy2 + 2xˆyˆy′ −ˆy′(2xˆh) = ˆy2
are continuous at a corner. The conditions are trivial and do not restrict corners in the extremal.
Extremal. Any piecewise smooth function that satisﬁes the boundary conditions ˆy(a) = A, ˆy(b) = B is an
admissible extremal.
An Exact Derivative. At this point we note that
Z b
a
(y2 + 2xyy′) dx =
Z b
a
d
dx(xy2) dx
=

xy2b
a
= bB2 −aA2.
The integral has the same value for all piecewise smooth functions y that satisfy the boundary conditions.
Since the integral has the same value for all piecewise smooth functions that satisfy
the boundary conditions, all such functions are weak extrema.
2068

4.
Z 1
0
(xy + y2 −2y2y′) dx,
y(0) = 1,
y(1) = 2
Erdmann’s Corner Conditions. Erdmann’s ﬁrst corner condition requires ˆF,y′ = −2ˆy2 to be continuous, which
is trivial. Erdmann’s second corner condition requires that
ˆF −ˆy′ ˆF,y′ = xˆy + ˆy2 −2ˆy2ˆy′ −ˆy′(−2ˆy2) = xˆy + ˆy2
is continuous. This condition is also trivial. Thus the extremal may have corners at any point.
Euler’s Diﬀerential Equation. Euler’s DE is
(F,y′)′ = F,y,
(−2y2)′ = x + 2y −4yy′
y = −x
2
Extremal. There is no piecewise smooth function that satisﬁes Euler’s diﬀerential
equation on its smooth segments and satisﬁes the boundary conditions y(0) = 1,
y(1) = 2. We conclude that there is no weak extremum.
Solution 48.4
1. We require that the ﬁrst variation vanishes
ZZ
D
 Fuh + Fuxhx + Fuyhy

dx dy = 0.
We rewrite the integrand as
ZZ
D
 Fuh + (Fuxh)x + (Fuyh)y −(Fux)xh −(Fuy)yh

dx dy = 0,
2069

ZZ
D
 Fu −(Fux)x −(Fuy)y

h dx dy +
ZZ
D
 (Fuxh)x + (Fuyh)y

dx dy = 0.
Using the Divergence theorem, we obtain,
ZZ
D
 Fu −(Fux)x −(Fuy)y

h dx dy +
Z
Γ
(Fux, Fuy) · n h ds = 0.
In order that the line integral vanish we have the natural boundary condition,
(Fux, Fuy) · n = 0
for (x, y) ∈Γ.
We can also write this as
Fux
dy
ds −Fuy
dx
ds = 0
for (x, y) ∈Γ.
The Euler diﬀerential equation for this problem is
Fu −(Fux)x −(Fuy)y = 0.
2. We consider the natural boundary conditions for
ZZ
D
F(x, y, u, ux, uy) dx dy +
Z
Γ
G(x, y, u) ds.
We require that the ﬁrst variation vanishes.
ZZ
D
 Fu −(Fux)x −(Fuy)y

h dx dy +
Z
Γ
(Fux, Fuy) · n h ds +
Z
Γ
Guh ds = 0,
ZZ
D
 Fu −(Fux)x −(Fuy)y

h dx dy +
Z
Γ
 (Fux, Fuy) · n + Gu

h ds = 0,
In order that the line integral vanishes, we have the natural boundary conditions,
(Fux, Fuy) · n + Gu = 0
for (x, y) ∈Γ.
2070

For the given integrand this is,
(2pux, 2puy) · n + 2σu = 0
for (x, y) ∈Γ,
p∇u · n + σu = 0
for (x, y) ∈Γ.
We can also denote this as
p∂u
∂n + σu = 0
for (x, y) ∈Γ.
Solution 48.5
First we vary φ.
ψ(ϵ) =
ZZ
R
 Z h(x,t)
0

φt + ϵηt + 1
2(φx + ϵηx)2 + 1
2(φy + ϵηy)2 + gy

dy
!
dx dt
ψ′(0) =
ZZ
R
 Z h(x,t)
0
(ηt + φxηx + φyηy) dy
!
dx dt = 0
ψ′(0) =
ZZ
R
 ∂
∂t
Z h(x,t)
0
η dy −[ηht]y=h(x,t) + ∂
∂x
Z h(x,t)
0
φxη dy −[φxηhx]y=h(x,t) −
Z h(x,t)
0
φxxη dy
+ [φyη]h(x,t)
0
−
Z h(x,t)
0
φyyη dy

dx dt = 0
Since η vanishes on the boundary of R, we have
ψ′(0) =
ZZ
R

−[(htφxhx −φy)η]y=h(x,t) −[φyη]y=0 −
Z h(x,t)
0
(φxx + φyy)η dy

dx dt = 0.
From the variations η which vanish on y = 0, h(x, t) we have
∇2φ = 0.
2071

This leaves us with
ψ′(0) =
ZZ
R

−[(htφxhx −φy)η]y=h(x,t) −[φyη]y=0

dx dt = 0.
By considering variations η which vanish on y = 0 we obtain,
htφxhx −φy = 0
on y = h(x, t).
Finally we have
φy = 0
on y = 0.
Next we vary h(x, t).
ψ(ϵ) =
ZZ
R
Z h(x,t)+ϵη(x,t)
0

φt + 1
2φ2
x + 1
2φ2
y + gy

dx dt
ψ′(ϵ) =
ZZ
R

φt + 1
2φ2
x + 1
2φ2
y + gy

y=h(x,t)
η dx dt = 0
This gives us the boundary condition,
φt + 1
2φ2
x + 1
2φ2
y + gy = 0
on y = h(x, t).
Solution 48.6
The parts of the extremizing curve which lie outside the boundary of the region R must be extremals, (i.e., solutions of
Euler’s equation) since if we restrict our variations to admissible curves outside of R and its boundary, we immediately
obtain Euler’s equation. Therefore an extremum can be reached only on curves consisting of arcs of extremals and
parts of the boundary of region R.
Thus, our problem is to ﬁnd the points of transition of the extremal to the boundary of R. Let the boundary of R
be given by φ(x). Consider an extremum that starts at the point (a, A), follows an extremal to the point (x0, φ(x0)),
follows the ∂R to (x1, φ(x1)) then follows an extremal to the point (b, B). We seek transversality conditions for the
points x0 and x1. We will extremize the expression,
I(y) =
Z x0
a
F(x, y, y′) dx +
Z x1
x0
F(x, φ, φ′) dx +
Z b
x1
F(x, y, y′) dx.
2072

Let c be any point between x0 and x1. Then extremizing I(y) is equivalent to extremizing the two functionals,
I1(y) =
Z x0
a
F(x, y, y′) dx +
Z c
x0
F(x, φ, φ′) dx,
I2(y) =
Z x1
c
F(x, φ, φ′) dx +
Z b
x1
F(x, y, y′) dx,
δI = 0
→
δI1 = δI2 = 0.
We will extremize I1(y) and then use the derived transversality condition on all points where the extremals meet ∂R.
The general variation of I1 is,
δI1(y) =
Z x0
a

Fy −d
dxFy′

dx + [Fy′δy]x0
a + [(F −y′Fy′)δx]x0
a
+ [Fφ′δφ(x)]c
x0 + [(F −φ′Fφ′)δx]c
x0 = 0
Note that δx = δy = 0 at x = a, c. That is, x = x0 is the only point that varies. Also note that δφ(x) is not
independent of δx. δφ(x) →φ′(x)δx. At the point x0 we have δy →φ′(x)δx.
δI1(y) =
Z x0
a

Fy −d
dxFy′

dx + (Fy′φ′δx)

x0
+ ((F −y′Fy′)δx)

x0
−(Fφ′φ′δx)

x0
−((F −φ′Fφ′)δx)

x0
= 0
δI1(y) =
Z x0
a

Fy −d
dxFy′

dx + ((F(x, y, y′) −F(x, φ, φ′) + (φ′ −y′)Fy′)δx)

x0
= 0
Since δI1 vanishes for those variations satisfying δx0 = 0 we obtain the Euler diﬀerential equation,
Fy −d
dxFy′ = 0.
2073

Then we have
((F(x, y, y′) −F(x, φ, φ′) + (φ′ −y′)Fy′)δx)

x0
= 0
for all variations δx0. This implies that
(F(x, y, y′) −F(x, φ, φ′) + (φ′ −y′)Fy′)

x0
= 0.
Two solutions of this equation are
y′(x0) = φ′(x0)
and
Fy′ = 0.
Transversality condition. If Fy′ is not identically zero, the extremal must be
tangent to ∂R at the points of contact.
Now we apply this result to to ﬁnd the curves which extremize
R 10
0 (y′)3 dx, y(0) = 0, y(10) = 0 given that the
admissible curves can not penetrate the interior of the circle (x −5)2 + y2 = 9. Since the Lagrangian is a function of
y′ alone, the extremals are straight lines.
The Erdmann corner conditions require that
Fy′ = 3(y′)2
and
F −y′Fy′ = (y′)3 −y′3(y′)2 = −2(y′)3
are continuous at corners. This implies that y′ is continuous. There are no corners.
We see that the extrema are
y(x) =





±3
4x,
for 0 ≤x ≤16
5 ,
±
p
9 −(x −5)2,
for 16
5 ≤x ≤34
5 ,
∓3
4x,
for 34
5 ≤x ≤10.
Note that the extremizing curves neither minimize nor maximize the integral.
2074

Solution 48.7
C1 Extremals. Without loss of generality, we take the vertical line to be the y axis. We will consider x1, y1 > 1. With
ds =
p
1 + (y′)2 dx we extremize the integral,
Z x1
0
√y
p
1 + (y′)2 dx.
Since the Lagrangian is independent of x, we know that the Euler diﬀerential equation has a ﬁrst integral.
d
dxFy′ −Fy = 0
y′Fy′y + y′′Fy′y′ −Fy = 0
d
dx(y′Fy′ −F) = 0
y′Fy′ −F = const
For the given Lagrangian, this is
y′√y
y′
p
1 + (y′)2 −√y
p
1 + (y′)2 = const,
(y′)2√y −√y(1 + (y′)2) = const
p
1 + (y′)2,
√y = const
p
1 + (y′)2
y = const is one solution. To ﬁnd the others we solve for y′ and then solve the diﬀerential equation.
y = a(1 + (y′)2)
y′ = ±
r
y −a
a
dx =
r
a
y −a dy
2075

±x + b = 2
p
a(y −a)
y = x2
4a ± bx
2a + b2
4a + a
The natural boundary condition is
Fy′

x=0 =
√yy′
p
1 + (y′)2

x=0
= 0,
y′(0) = 0
The extremal that satisﬁes this boundary condition is
y = x2
4a + a.
Now we apply y(x1) = y1 to obtain
a = 1
2

y1 ±
q
y2
1 −x2
1

for y1 ≥x1. The value of the integral is
Z x1
0
sx2
4a + a
 
1 +
 x
2a
2
dx = x1(x2
1 + 12a2)
12a3/2
.
By denoting y1 = cx1, c ≥1 we have
a = 1
2

cx1 ± x1
√
c2 −1

The values of the integral for these two values of a are
√
2(x1)3/2−1 + 3c2 ± 3c
√
c2 −1
3(c ±
√
c2 −1)3/2
.
2076

The values are equal only when c = 1. These values, (divided by √x1), are plotted in Figure 48.1 as a function of c.
The former and latter are ﬁne and coarse dashed lines, respectively. The extremal with
a = 1
2

y1 +
q
y2
1 −x2
1

has the smaller performance index. The value of the integral is
x1(x2
1 + 3(y1 +
p
y2
1 −x2
1)2
3
√
2(y1 +
p
y2
1 −x2
1)3
.
The function y = y1 is an admissible extremal for all x1. The value of the integral for this extremal is x1√y1 which
is larger than the integral of the quadratic we analyzed before for y1 > x1.
1.2
1.4
1.6
1.8
2
2.5
3
3.5
4
Figure 48.1:
Thus we see that
ˆy = x2
4a + a,
a = 1
2

y1 +
q
y2
1 −x2
1

2077

is the extremal with the smaller integral and is the minimizing curve in C1 for y1 ≥x1. For y1 < x1 the C1 extremum
is,
ˆy = y1.
C1
p Extremals. Consider the parametric form of the Lagrangian.
Z t1
t0
p
y(t)
p
(x′(t))2 + (y′(t))2 dt
The Euler diﬀerential equations are
d
dtfx′ −fx = 0
and
d
dtfy′ −fy = 0.
If one of the equations is satisﬁed, then the other is automatically satisﬁed, (or the extremal is straight). With either
of these equations we could derive the quadratic extremal and the y = const extremal that we found previously. We
will ﬁnd one more extremal by considering the ﬁrst parametric Euler diﬀerential equation.
d
dtfx′ −fx = 0
d
dt
 
p
y(t)x′(t)
p
(x′(t))2 + (y′(t))2
!
= 0
p
y(t)x′(t)
p
(x′(t))2 + (y′(t))2 = const
Note that x(t) = const is a solution. Thus the extremals are of the three forms,
x = const,
y = const,
y = x2
4a + bx
2a + b2
4a + a.
2078

The Erdmann corner conditions require that
Fy′ =
√yy′
p
1 + (y′)2,
F −y′Fy′ = √y
p
1 + (y′)2 −
√y(y′)2
p
1 + (y′)2 =
√y
p
1 + (y′)2
are continuous at corners. There can be corners only if y = 0.
Now we piece the three forms together to obtain C1
p extremals that satisfy the Erdmann corner conditions. The
only possibility that is not C1 is the extremal that is a horizontal line from (0, 0) to (x1, 0) and then a vertical line from
(x1, y1). The value of the integral for this extremal is
Z y1
0
√
t dt = 2
3(y1)3/2.
Equating the performance indices of the quadratic extremum and the piecewise smooth extremum,
x1(x2
1 + 3(y1 +
p
y2
1 −x2
1)2
3
√
2(y1 +
p
y2
1 −x2
1)3
= 2
3(y1)3/2,
y1 = ±x1
p
3 ± 2
√
3
√
3
.
The only real positive solution is
y1 = x1
p
3 + 2
√
3
√
3
≈1.46789 x1.
The piecewise smooth extremal has the smaller performance index for y1 smaller than this value and the quadratic
extremal has the smaller performance index for y1 greater than this value.
The C1
p extremum is the piecewise smooth extremal for y1 ≤x1
p
3 + 2
√
3/
√
3
and is the quadratic extremal for y1 ≥x1
p
3 + 2
√
3/
√
3.
2079

Solution 48.8
The shape of the rope will be a catenary between x1 and x2 and be a vertically hanging segment after that. Let the
length of the vertical segment be z. Without loss of generality we take x1 = y2 = 0. The potential energy, (relative to
y = 0), of a length of rope ds in 0 ≤x ≤x2 is mgy = ρgy ds. The total potential energy of the vertically hanging
rope is m(center of mass)g = ρz(−z/2)g. Thus we seek to minimize,
ρg
Z x2
0
y ds −1
2ρgz2,
y(0) = y1,
y(x2) = 0,
subject to the isoperimetric constraint,
Z x2
0
ds −z = L.
Writing the arc-length diﬀerential as ds =
p
1 + (y′)2 dx we minimize
ρg
Z x2
0
y
p
1 + (y′)2 ds −1
2ρgz2,
y(0) = y1,
y(x2) = 0,
subject to,
Z x2
0
p
1 + (y′)2 dx −z = L.
Consider the more general problem of ﬁnding functions y(x) and numbers z which extremize I ≡
R b
a F(x, y, y′) dx+
f(z) subject to J ≡
R b
a G(x, y, y′) dx + g(z) = L.
Suppose y(x) and z are the desired solutions and form the comparison families, y(x)+ϵ1η1(x)+ϵ2η2(x), z +ϵ1ζ1 +
ϵ2ζ2. Then, there exists a constant such that
∂
∂ϵ1
(I + λJ)

ϵ1,ϵ2=0 = 0
∂
∂ϵ2
(I + λJ)

ϵ1,ϵ2=0 = 0.
2080

These equations are
Z b
a
 d
dxH,y′ −Hy

η1 dx + h′(z)ζ1 = 0,
and
Z b
a
 d
dxH,y′ −Hy

η2 dx + h′(z)ζ2 = 0,
where H = F + λG and h = f + λg. From this we conclude that
d
dxH,y′ −Hy = 0,
h′(z) = 0
with λ determined by
J =
Z b
a
G(x, y, y′) dx + g(z) = L.
Now we apply these results to our problem. Since f(z) = −1
2ρgz2 and g(z) = −z we have
−ρgz −λ = 0,
z = −λ
ρg.
It was shown in class that the solution of the Euler diﬀerential equation is a family of catenaries,
y = −λ
ρg + c1 cosh
x −c2
c1

.
One can ﬁnd c1 and c2 in terms of λ by applying the end conditions y(0) = y1 and y(x2) = 0. Then the expression for
y(x) and z = −λ/ρg are substituted into the isoperimetric constraint to determine λ.
Consider the special case that (x1, y1) = (0, 0) and (x2, y2) = (1, 0).
In this case we can use the fact that
y(0) = y(1) to solve for c2 and write y in the form
y = −λ
ρg + c1 cosh
x −1/2
c1

.
2081

Applying the condition y(0) = 0 would give us the algebraic-transcendental equation,
y(0) = −λ
ρg + c1 cosh
 1
2c1

= 0,
which we can’t solve in closed form. Since we ran into a dead end in applying the boundary condition, we turn to the
isoperimetric constraint.
Z 1
0
p
1 + (y′)2 dx −z = L
Z 1
0
cosh
x −1/2
c1

dx −z = L
2c1 sinh
 1
2c1

−z = L
With the isoperimetric constraint, the algebraic-transcendental equation and z = −λ/ρg we now have
z = −c1 cosh
 1
2c1

,
z = 2c1 sinh
 1
2c1

−L.
For any ﬁxed L, we can numerically solve for c1 and thus obtain z. You can derive that there are no solutions unless
L is greater than about 1.9366. If L is smaller than this, the rope would slip oﬀthe pin. For L = 2, c1 has the values
0.4265 and 0.7524. The larger value of c1 gives the smaller potential energy. The position of the end of the rope is
z = −0.9248.
Solution 48.9
Using the method of Lagrange multipliers, we look for stationary values of
R c
0 ((y′)2 + λy2) dx,
δ
Z c
0
((y′)2 + λy2) dx = 0.
2082

The Euler diﬀerential equation is
d
dxF(, y′) −F,y = 0,
d
dx(2y′) −2λy = 0.
Together with the homogeneous boundary conditions, we have the problem
y′′ −λy = 0,
y(0) = y(c) = 0,
which has the solutions,
λn = −
nπ
c
2
,
yn = an sin
nπx
c

,
n ∈Z+.
Now we determine the constants an with the moment of inertia constraint.
Z c
0
a2
n sin2 nπx
c

dx = ca2
n
2
= A
Thus we have the extremals,
yn =
r
2A
c sin
nπx
c

,
n ∈Z+.
The drag for these extremals is
D = 2A
c
Z c
0
nπ
c
2
cos2 nπx
c

dx = An2π2
c2
.
We see that the drag is minimum for n = 1. The shape for minimum drag is
ˆy =
r
2A
c sin
nπx
c

.
2083

Solution 48.10
Consider the general problem of determining the stationary values of the quantity ω2 given by
ω2 =
R b
a F(x, y, y′, y′′) dx
R b
a G(x, y, y′, y′′) dx
≡I
J .
The variation of ω2 is
δω2 = JδI −IδJ
J2
= 1
J

δI −I
J δJ

= 1
J
 δI −ω2δJ

.
The the values of y and y′ are speciﬁed on the boundary, then the variations of I and J are
δI =
Z b
a
 d2
dx2F,y′′ −d
dxF,y′ + F,y

δy dx,
δJ =
Z b
a
 d2
dx2G,y′′ −d
dxG,y′ + G,y

δy dx
Thus δω2 = 0 becomes
R b
a

d2
dx2H,y′′ −d
dxH,y′ + H,y

δy dx
R b
a G dx
= 0,
where H = F −ω2G. A necessary condition for an extremum is
d2
dx2H,y′′ −d
dxH,y′ + H,y = 0
where H ≡F −ω2G.
For our problem we have F = EI(y′′)2 and G = ρy so that the extremals are solutions of
d2
dx2

EI dy
dx

−ρω2y = 0,
2084

With homogeneous boundary conditions we have an eigenvalue problem with deﬂections modes yn(x) and corresponding
natural frequencies ωn.
Solution 48.11
We assume that v0 > w(x, y, t) so that the problem has a solution for any end point. The crossing time is
T =
Z l
0

˙X(t)
−1
dx = 1
v0
Z l
0
sec α(t) dx.
Note that
dy
dx = w + v0 sin α
v0 cos α
= w
v0
sec α + tan α
= w
v0
sec α +
√
sec2 α −1.
We solve this relation for sec α.

y′ −w
v0
sec α
2
= sec2 α −1
(y′)2 −2 w
v0
y′ sec α + w2
v2
0
sec2 α = sec2 α −1
(v2
0 −w2) sec2 α + 2v0wy′ sec α −v2
0((y′)2 + 1) = 0
sec α = −2v0wy′ ±
p
4v2
0w2(y′)2 + 4(v2
0 −w2)v2
0((y′)2 + 1)
2(v2
0 −w2)
sec α = v0
−wy′ ±
p
v2
0((y′)2 + 1) −w2
(v2
0 −w2)
2085

Since the steering angle satisﬁes −π/2 ≤α ≤π/2 only the positive solution is relevant.
sec α = v0
−wy′ +
p
v2
0((y′)2 + 1) −w2
(v2
0 −w2)
Time Independent Current. If we make the assumption that w = w(x, y) then we can write the crossing time
as an integral of a function of x and y.
T(y) =
Z l
0
−wy′ +
p
v2
0((y′)2 + 1) −w2
(v2
0 −w2)
dx
A necessary condition for a minimum is δT = 0. The Euler diﬀerential equation for this problem is
d
dxF,y′ −F,y = 0
d
dx
 
1
v2
0 −w2
 
−w +
v2
0y′
p
v2
0((y′)2 + 1) −w2
!!
−
wy
(v2
0 −w2)2
 
w(v2(1 + 2(y′)2) −w2)
p
v2
0((y′)2 + 1) −w2
−y′(v2
0 + w2)
!
By solving this second order diﬀerential equation subject to the boundary conditions y(0) = 0, y(l) = y1 we obtain the
path of minimum crossing time.
Current w = w(x). If the current is only a function of x, then the Euler diﬀerential equation can be integrated to
obtain,
1
v2
0 −w2
 
−w +
v2
0y′
p
v2
0((y′)2 + 1) −w2
!
= c0.
Solving for y′,
y′ = ±
w + c0(v2
0 −w2)
v0
p
1 −2c0w −c2
0(v2
0 −w2)
.
Since y(0) = 0, we have
y(x) = ±
Z x
0
w(ξ) + c0(v2
0 −(w(ξ))2)
v0
p
1 −2c0w(ξ) −c2
0(v2
0 −(w(ξ))2)
.
2086

For any given w(x) we can use the condition y(l) = y1 to solve for the constant c0.
Constant Current. If the current is constant then the Lagrangian is a function of y′ alone. The admissible
extremals are straight lines. The solution is then
y(x) = y1x
l .
Solution 48.12
1. The kinetic energy of the ﬁrst particle is 1
2m((α −x)ˆθ)2. Its potential energy, relative to the table top, is zero.
The kinetic energy of the second particle is 1
2mˆx2. Its potential energy, relative to its equilibrium position is
−mgx. The Lagrangian is the diﬀerence of kinetic and potential energy.
L = m

˙x2 + 1
2(α −x)2 ˙θ2 + gx

The Euler diﬀerential equations are the equations of motion.
d
dtL, ˙x −Lx = 0,
d
dtL, ˙θ −Lθ = 0
d
dt(2m ˙x) + m(α −x) ˙θ2 −mg = 0,
d
dt

m(α −x)2 ˙θ2
= 0
2¨x + (α −x) ˙θ2 −g = 0,
(α −x)2 ˙θ2 = const
When x = 0, ˙θ = ω =
p
g/α. This determines the constant in the equation of motion for θ.
˙θ =
α√αg
(α −x)2
Now we substitute the expression for ˙θ into the equation of motion for x.
2¨x + (α −x)
α3g
(α −x)4 −g = 0
2087

2¨x +

α3
(α −x)3 −1

g = 0
2¨x +

1
(1 −x/α)3 −1

g = 0
2. For small oscillations,
 x
α
 ≪1. Recall the binomial expansion,
(1 + z)a =
∞
X
n=0
a
n

zn,
for |z| < 1,
(1 + z)a ≈1 + az,
for |z| ≪1.
We make the approximation,
1
(1 −x/α)3 ≈1 + 3x
α,
to obtain the linearized equation of motion,
2¨x + 3g
α x = 0.
This is the equation of a harmonic oscillator with solution
x = a sin
p
3g2α(t −b)

.
The period of oscillation is,
T = 2π
√
2α3g.
Solution 48.13
We write the equation of motion and boundary conditions,
¨x = U(t) −g,
x(0) = ˙x(0) = 0,
x(T) = h,
2088

as the ﬁrst order system,
˙x = 0,
x(0) = 0,
x(T) = h,
˙y = U(t) −g,
y(0) = 0.
We seek to minimize,
T =
Z T
0
dt,
subject to the constraints,
˙x −y = 0,
˙y −U(t) + g = 0,
Z T
0
U 2(t) dt = k2.
Thus we seek extrema of
Z T
0
H dt ≡
Z T
0
 1 + λ(t)( ˙x −y) + µ(t)( ˙y −U(t) + g) + νU 2(t)

dt.
Since y is not speciﬁed at t = T, we have the natural boundary condition,
H, ˙y

t=T = 0,
µ(T) = 0.
The ﬁrst Euler diﬀerential equation is
d
dtH, ˙x −H,x = 0,
d
dtλ(t) = 0.
2089

We see that λ(t) = λ is constant. The next Euler DE is
d
dtH, ˙y −H,y = 0,
d
dtµ(t) + λ = 0.
µ(t) = −λt + const
With the natural boundary condition, µ(T) = 0, we have
µ(t) = λ(T −t).
The ﬁnal Euler DE is,
d
dtH, ˙U −H,U = 0,
µ(t) −2νU(t) = 0.
Thus we have
U(t) = λ(T −t)
2ν
.
This is the required thrust function. We use the constraints to ﬁnd λ, ν and T.
Substituting U(t) = λ(T −t)/(2ν) into the isoperimetric constraint,
R T
0 U 2(t) dt = k2 yields
λ2T 3
12ν2 = k2,
U(t) =
√
3k
T 3/2 (T −t).
The equation of motion for x is
¨x = U(t) −g =
√
3k
T 3/2 (T −t).
2090

Integrating and applying the initial conditions x(0) = ˙x(0) = 0 yields,
x(t) = kt2(3T −t)
2
√
3T 3/2
−1
2gt2.
Applying the condition x(T) = h gives us,
k
√
3T 3/2 −1
2gT 2 = h,
1
4g2T 4 −k
3T 3 + ghT 2 + h2 = 0.
If k ≥4
p
2/3g3/2√
h then this fourth degree polynomial has positive, real solutions for T. With strict inequality, the
minimum time is the smaller of the two positive, real solutions. If k < 4
p
2/3g3/2√
h then there is not enough fuel to
reach the target height.
Solution 48.14
We have ¨x = U(t) where U(t) is the acceleration furnished by the thrust of the vehicles engine. In practice, the engine
will be designed to operate within certain bounds, say −M ≤U(t) ≤M, where ±M is the maximum forward/backward
acceleration. To account for the inequality constraint we write U = M sin V (t) for some suitable V (t). More generally,
if we had φ(t) ≤U(t) ≤ψ(t), we could write this as U(t) = ψ+φ
2
+ ψ−φ
2
sin V (t).
We write the equation of motion as a ﬁrst order system,
˙x = y,
x(0) = a,
x(T) = 0,
˙y = M sin V,
y(0) = b,
y(T) = 0.
Thus we minimize
T =
Z T
0
dt
subject to the constraints,
˙x −y = 0
˙y −M sin V = 0.
2091

Consider
H = 1 + λ(t)( ˙x −y) + µ(t)( ˙y −M sin V ).
The Euler diﬀerential equations are
d
dtH, ˙x −H,x = 0
⇒
d
dtλ(t) = 0
⇒
λ(t) = const
d
dtH, ˙y −H,y = 0
⇒
d
dtµ(t) + λ = 0
⇒
µ(t) = −λt + const
d
dtH, ˙V −H,V = 0
⇒
µ(t)M cos V (t) = 0
⇒
V (t) = π
2 + nπ.
Thus we see that
U(t) = M sin
π
2 + nπ

= ±M.
Therefore, if the rocket is to be transferred from its initial state to is speciﬁed ﬁnal state in minimum time with
a limited source of thrust, (|U| ≤M), then the engine should operate at full power at all times except possibly for a
ﬁnite number of switching times. (Indeed, if some power were not being used, we would expect the transfer would be
speeded up by using the additional power suitably.)
To see how this ”bang-bang” process works, we’ll look at the phase plane. The problem
˙x = y,
x(0) = c,
˙y = ±M,
y(0) = d,
has the solution
x(t) = c + dt ± M t2
2 ,
y(t) = d ± Mt.
We can eliminate t to get
x = ± y2
2M + c ∓d2
2M .
These curves are plotted in Figure 48.2.
2092

Figure 48.2:
There is only curve in each case which transfers the initial state to the origin. We will denote these curves γ and Γ,
respectively. Only if the initial point (a, b) lies on one of these two curves can we transfer the state of the system to the
origin along an extremal without switching. If a =
b2
2M and b < 0 then this is possible using U(t) = M. If a = −b2
2M
and b > 0 then this is possible using U(t) = −M. Otherwise we follow an extremal that intersects the initial position
until this curve intersects γ or Γ. We then follow γ or Γ to the origin.
Solution 48.15
Since the integrand does not explicitly depend on x, the Euler diﬀerential equation has the ﬁrst integral,
F −y′Fy′ = const.
p
y + h
p
1 + (y′)2 −y′ y′√y + h
p
1 + (y′)2 = const
√y + h
p
1 + (y′)2 = const
y + h = c2
1(1 + (y′)2)
2093

q
y + h −c2
1 = c1y′
c1 dy
p
y + h −c2
1
= dx
2c1
q
y + h −c2
1 = x −c2
4c2
1(y + h −c2
1) = (x −c2)2
Since the extremal passes through the origin, we have
4c2
1(h −c2
1) = c2
2.
4c2
1y = x2 −2c2x
(48.6)
Introduce as a parameter the slope of the extremal at the origin; that is, y′(0) = α. Then diﬀerentiating (48.6) at
x = 0 yields 4c2
1α = −2c2. Together with c2
2 = 4c2
1(h −c2
1) we obtain c2
1 =
h
1+α2 and c2 = −2αh
1+α2. Thus the equation
of the pencil (48.6) will have the form
y = αx + 1 + α2
4h
x2.
(48.7)
To ﬁnd the envelope of this family we diﬀerentiate ( 48.7) with respect to α to obtain 0 = x + α
2hx2 and eliminate α
between this and ( 48.7) to obtain
y = −h + x2
4h.
See Figure 48.3 for a plot of some extremals and the envelope.
All extremals (48.7) lie above the envelope which in ballistics is called the parabola of safety. If (m, M) lies outside
the parabola, M < −h + m2
4h , then it cannot be joined to (0, 0) by an extremal. If (m, M) is above the envelope
then there are two candidates. Clearly we rule out the one that touches the envelope because of the occurrence of
conjugate points. For the other extremal, problem 2 shows that E ≥0 for all y′. Clearly we can embed this extremal
in an extremal pencil, so Jacobi’s test is satisﬁed. Therefore the parabola that does not touch the envelope is a strong
minimum.
2094

h
2h
x
-h
y
Figure 48.3: Some Extremals and the Envelope.
Solution 48.16
E = F(x, y, y′) −F(x, y, p) −(y′ −p)Fy′(x, y, p)
= n
p
1 + (y′)2 −n
p
1 + p2 −(y′ −p)
np
p
1 + p2
=
n
p
1 + p2
p
1 + (y′)2p
1 + p2 −(1 + p2) −(y′ −p)p

=
n
p
1 + p2
p
1 + (y′)2 + p2 + (y′)2p2 −2y′p + 2y′p −(1 + py′)

=
n
p
1 + p2
p
(1 + py′)2 + (y′ −p)2 −(1 + py′)

≥0
2095

The speed of light in an inhomogeneous medium is ds
dt =
1
n(x,y. The time of transit is then
T =
Z (b,B)
(a,A)
dt
ds ds =
Z b
a
n(x, y)
p
1 + (y′)2 dx.
Since E ≥0, light traveling on extremals follow the time optimal path as long as the extremals do not intersect.
Solution 48.17
Extremals. Since the integrand does not depend explicitly on x, the Euler diﬀerential equation has the ﬁrst integral,
F −y′F,y′ = const.
1 + y2
(y′)2 −y′−2(1 + y2)
(y′)3
= const
dy
p
1 + (y′)2 = const dx
arcsinh(y) = c1x + c2
y = sinh(c1x + c2)
Jacobi Test. We can see by inspection that no conjugate points exist. Consider the central ﬁeld through (0, 0),
sinh(cx), (See Figure 48.4).
We can also easily arrive at this conclusion analytically as follows: Solutions u1 and u2 of the Jacobi equation are
given by
u1 = ∂y
∂c2
= cosh(c1x + c2),
u2 = ∂y
∂c1
= x cosh(c1x + c2).
Since u2/u1 = x is monotone for all x there are no conjugate points.
2096

-3
-2
-1
1
2
3
-3
-2
-1
1
2
3
Figure 48.4: sinh(cx)
Weierstrass Test.
E = F(x, y, y′) −F(x, y, p) −(y′ −p)F,y′(x, y, p)
= 1 + y2
(y′)2 −1 + y2
p2
−(y′ −p)−2(1 + y2)
p3
= 1 + y2
(y′)2p2
p3 −p(y′)2 + 2(y′)3 −2p(y′)2
p

= 1 + y2
(y′)2p2
(p −y′)2(p + 2y′)
p

For p = p(x, y) bounded away from zero, E is one-signed for values of y′ close to p. However, since the factor (p+2y′)
can have any sign for arbitrary values of y′, the conditions for a strong minimum are not satisﬁed.
Furthermore, since the extremals are y = sinh(c1x + c2), the slope function p(x, y) will be of one sign only if the
range of integration is such that we are on a monotonic piece of the sinh. If we span both an increasing and decreasing
section, E changes sign even for weak variations.
2097

Legendre Condition.
F,y′y′ = 6(1 + y2)
(y′)4
> 0
Note that F cannot be represented in a Taylor series for arbitrary values of y′ due to the presence of a discontinuity in
F when y′ = 0. However, F,y′y′ > 0 on an extremal implies a weak minimum is provided by the extremal.
Strong Variations. Consider
R 1+y2
(y′)2 dx on both an extremal and on the special piecewise continuous variation in
the ﬁgure. On PQ we have y′ = ∞with implies that 1+y2
(y′)2 = 0 so that there is no contribution to the integral from
PQ.
On QR the value of y′ is greater than its value along the extremal PR while the value of y on QR is less than the
value of y along PR. Thus on QR the quantity 1+y2
(y′)2 is less than it is on the extremal PR.
Z
QR
1 + y2
(y′)2 dx <
Z
PR
1 + y2
(y′)2 dx
Thus the weak minimum along the extremal can be weakened by a strong variation.
Solution 48.18
The Euler diﬀerential equation is
d
dxF,y′ −F,y = 0.
d
dx(1 + 2x2y′) = 0
1 + 2x2y′ = const
y′ = const 1
x2
y = c1
x + c2
(i) No continuous extremal exists in −1 ≤x ≤2 that satisﬁes y(−1) = 1 and y(2) = 4.
2098

(ii) The continuous extremal that satisﬁes the boundary conditions is y = 7 −4
x. Since F,y′y′ = 2x2 ≥0 has a Taylor
series representation for all y′, this extremal provides a strong minimum.
(iii) The continuous extremal that satisﬁes the boundary conditions is y = 1. This is a strong minimum.
Solution 48.19
For identity (a) we take P = 0 and Q = φψx −ψφx. For identity (b) we take P = φψy −ψφy and Q = 0. For identity
(c) we take P = −1
2(φψx −ψφx) and Q = 1
2(φψy −ψφy).
ZZ
D
1
2(φψy −ψφy)x −

−1
2

(φψx −ψφx)y

dx dy =
Z
Γ

−1
2(φψx −ψφx) dx + 1
2(φψy −ψφy) dy

ZZ
D
1
2(φxψy + φψxy −ψxφy −ψφxy) + 1
2(φyψxφψxy −ψyφx −ψφxy)

dx dy
= −1
2
Z
Γ
(φψx −ψφx) dx + 1
2
Z
Γ
(φψy −ψφy) dy
ZZ
D
φψxy dx dy =
ZZ
D
ψφxy dx dy −1
2
Z
Γ
(φψx −ψφx) dx + 1
2
Z
Γ
(φψy −ψφy) dy
The variation of I is
δI =
Z t1
t0
ZZ
D
(−2(uxx + uyy)(δuxx + δuyy) + 2(1 −µ)(uxxδuyy + uyyδuxx −2uxyδuxy)) dx dy dt.
From (a) we have
ZZ
D
−2(uxx + uyy)δuxx dx dy =
ZZ
D
−2(uxx + uyy)xxδu dx dy
+
Z
Γ
−2((uxx + uyy)δux −(uxx + uyy)xδu) dy.
2099

From (b) we have
ZZ
D
−2(uxx + uyy)δuyy dx dy =
ZZ
D
−2(uxx + uyy)yyδu dx dy
−
Z
Γ
−2((uxx + uyy)δuy −(uxx + uyy)yδu) dy.
From (a) and (b) we get
ZZ
D
2(1 −µ)(uxxδuyy + uyyδuxx) dx dy
=
ZZ
D
2(1 −µ)(uxxyy + uyyxx)δu dx dy
+
Z
Γ
2(1 −µ)(−(uxxδuy −uxxyδu) dx + (uyyδux −uyyxδu) dy).
Using c gives us
ZZ
D
2(1 −µ)(−2uxyδuxy) dx dy =
ZZ
D
2(1 −µ)(−2uxyxyδu) dx dy
+
Z
Γ
2(1 −µ)(uxyδux −uxyxδu) dx
−
Z
Γ
2(1 −µ)(uxyδuy −uxyyδu) dy.
Note that
∂u
∂n ds = ux dy −uy dx.
Using the above results, we obtain
δI = 2
Z t1
t0
ZZ
D
(−∇4u)δu dx dy dt + 2
Z t1
t0
Z
Γ
∂(∇2u)
∂n
δu + (∇2u)∂(δu)
∂n

ds dt
+ 2(1 −µ)
Z t1
t0
Z
Γ
(uyyδux −uxyδuy) dy + (uxyδux −uxxδuy) dx

dt.
2100

Solution 48.20
1. Exact Solution. The Euler diﬀerential equation is
d
dxF,y′ = F,y
d
dx[2y′] = −2y −2x
y′′ + y = −x.
The general solution is
y = c1 cos x + c2 sin x −x.
Applying the boundary conditions we obtain,
y = sin x
sin 1 −x.
The value of the integral for this extremal is
J
sin x
sin 1 −x

= cot(1) −2
3 ≈−0.0245741.
n = 0. We consider an approximate solution of the form y(x) = ax(1−x). We substitute this into the functional.
J(a) =
Z 1
0
 (y′)2 −y2 −2xy

dx = 3
10a2 −1
6a
The only stationary point is
J′(a) = 3
5a −1
6 = 0
a = 5
18.
2101

Since
J′′
 5
18

= 3
5 > 0,
we see that this point is a minimum. The approximate solution is
y(x) = 5
18x(1 −x).
This one term approximation and the exact solution are plotted in Figure 48.5. The value of the functional is
J = −5
216 ≈−0.0231481.
0.2
0.4
0.6
0.8
1
0.01
0.02
0.03
0.04
0.05
0.06
0.07
Figure 48.5: One Term Approximation and Exact Solution.
n = 1. We consider an approximate solution of the form y(x) = x(1 −x)(a + bx). We substitute this into the
functional.
J(a, b) =
Z 1
0
 (y′)2 −y2 −2xy

dx =
1
210
 63a2 + 63ab + 26b2 −35a −21b

2102

We ﬁnd the stationary points.
Ja = 1
30(18a + 9b −5) = 0
Jb =
1
210(63a + 52b −21) = 0
a = 71
369,
b = 7
41
Since the Hessian matrix
H =
Jaa
Jab
Jba
Jbb

=
 3
5
3
10
3
10
26
105

,
is positive deﬁnite,
3
5 > 0,
det(H) = 41
700,
we see that this point is a minimum. The approximate solution is
y(x) = x(1 −x)
 71
369 + 7
41x

.
This two term approximation and the exact solution are plotted in Figure 48.6. The value of the functional is
J = −136
5535 ≈−0.0245709.
2. Exact Solution. The Euler diﬀerential equation is
d
dxF,y′ = F,y
d
dx[2y′] = 2y + 2x
y′′ −y = x.
2103

0.2
0.4
0.6
0.8
1
0.01
0.02
0.03
0.04
0.05
0.06
0.07
Figure 48.6: Two Term Approximation and Exact Solution.
The general solution is
y = c1 cosh x + c2 sinh x −x.
Applying the boundary conditions, we obtain,
y = 2 sinh x
sinh 2 −x.
The value of the integral for this extremal is
J = −2(e4 −13)
3(e4 −1) ≈−0.517408.
Polynomial Approximation. Consider an approximate solution of the form
y(x) = x(2 −x)(a0 + a1x + · · · anxn).
2104

The one term approximate solution is
y(x) = −5
14x(2 −x).
This one term approximation and the exact solution are plotted in Figure 48.7. The value of the functional is
J = −10
21 ≈−0.47619.
0.5
1
1.5
2
-0.35
-0.3
-0.25
-0.2
-0.15
-0.1
-0.05
Figure 48.7: One Term Approximation and Exact Solution.
The two term approximate solution is
y(x) = x(2 −x)

−33
161 −7
46x

.
This two term approximation and the exact solution are plotted in Figure 48.8. The value of the functional is
J = −416
805 ≈−0.51677.
2105

0.5
1
1.5
2
-0.35
-0.3
-0.25
-0.2
-0.15
-0.1
-0.05
Figure 48.8: Two Term Approximation and Exact Solution.
Sine Series Approximation. Consider an approximate solution of the form
y(x) = a1 sin
πx
2

+ a2 sin (πx) + · · · + an sin

nπx
2

.
The one term approximate solution is
y(x) = −
16
π(π2 + 4) sin
πx
2

.
This one term approximation and the exact solution are plotted in Figure 48.9. The value of the functional is
J = −
64
π2(π2 + 4) ≈−0.467537.
The two term approximate solution is
y(x) = −
16
π(π2 + 4) sin
πx
2

+
2
π(π2 + 1) sin(πx).
2106

0.5
1
1.5
2
-0.35
-0.3
-0.25
-0.2
-0.15
-0.1
-0.05
Figure 48.9: One Term Sine Series Approximation and Exact Solution.
This two term approximation and the exact solution are plotted in Figure 48.10. The value of the functional is
J = −
4(17π2 + 20)
π2(π4 + 5π2 + 4) ≈−0.504823.
3. Exact Solution. The Euler diﬀerential equation is
d
dxF,y′ = F,y
d
dx[2xy′] = −2x2 −1
x
y −2x2
y′′ + 1
xy′ +

1 −1
x2

y = −x
The general solution is
y = c1J1(x) + c2Y1(x) −x
2107

0.5
1
1.5
2
-0.3
-0.2
-0.1
Figure 48.10: Two Term Sine Series Approximation and Exact Solution.
Applying the boundary conditions we obtain,
y = (Y1(2) −2Y1(1))J1(x) + (2J1(1) −J1(2))Y1(x)
J1(1)Y1(2) −Y1(1)J1(2)
−x
The value of the integral for this extremal is
J ≈−0.310947
Polynomial Approximation. Consider an approximate solution of the form
y(x) = (x −1)(2 −x)(a0 + a1x + · · · anxn).
The one term approximate solution is
y(x) = (x −1)(2 −x)
23
6(40 log 2 −23)
2108

This one term approximation and the exact solution are plotted in Figure 48.11. The one term approximation is
a surprisingly close to the exact solution. The value of the functional is
J = −
529
360(40 log 2 −23) ≈−0.310935.
1.2
1.4
1.6
1.8
2
0.05
0.1
0.15
0.2
Figure 48.11: One Term Polynomial Approximation and Exact Solution.
Solution 48.21
1. The spectrum of T is the set,
{λ : (T −λI) is not invertible.}
2109

(T −λI)f = g
Z ∞
−∞
K(x −y)f(y) dy −λf(x) = g
ˆK(ω) ˆf(ω) −λ ˆf(ω) = ˆg(ω)

ˆK(ω) −λ

ˆf(ω) = ˆg(ω)
We may not be able to solve for ˆf(ω), (and hence invert T −λI), if λ = ˆK(ω). Thus all values of ˆK(ω) are in
the spectrum. If ˆK(ω) is everywhere nonzero we consider the case λ = 0. We have the equation,
Z ∞
−∞
K(x −y)f(y) dy = 0
Since there are an inﬁnite number of L2(−∞, ∞) functions which satisfy this, (those which are nonzero on a set
of measure zero), we cannot invert the equation. Thus λ = 0 is in the spectrum. The spectrum of T is the range
of ˆK(ω) plus zero.
2. Let λ be a nonzero eigenvalue with eigenfunction φ.
(T −λI)φ = 0,
∀x
Z ∞
−∞
K(x −y)φ(y) dy −λφ(x) = 0,
∀x
Since K is continuous, Tφ is continuous. This implies that the eigenfunction φ is continuous. We take the
Fourier transform of the above equation.
ˆK(ω)ˆφ(ω) −λˆφ(ω) = 0,
∀ω

ˆK(ω) −λ

ˆφ(ω) = 0,
∀ω
2110

If φ(x) is absolutely integrable, then ˆφ(ω) is continous. Since φ(x) is not identically zero,
ˆ
φ(ω) is not identically
zero. Continuity implies that
ˆ
φ(ω) is nonzero on some interval of positive length, (a, b). From the above equation
we see that ˆK(ω) = λ for ω ∈(a, b).
Now assume that ˆK(ω) = λ in some interval (a, b). Any function ˆφ(ω) that is nonzero only for ω ∈(a, b) satisﬁes

ˆK(ω) −λ

ˆφ(ω) = 0,
∀ω.
By taking the inverse Fourier transform we obtain an eigenfunction φ(x) of the eigenvalue λ.
3. First we use the Fourier transform to ﬁnd an explicit representation of u = (T −λI)−1f.
u = (T −λI)−1f(T −λI)u = f
Z ∞
−∞
K(x −y)u(y) dy −λu = f
2π ˆKˆu −λˆu = ˆf
ˆu =
ˆf
2π ˆK −λ
ˆu = −1
λ
ˆf
1 −2π ˆK/λ
For |λ| > |2π ˆK| we can expand the denominator in a geometric series.
ˆu = −1
λ
ˆf
∞
X
n=0
 
2π ˆK
λ
!n
u = −1
λ
∞
X
n=0
1
λn
Z ∞
−∞
Kn(x −y)f(y) dy
2111

Here Kn is the nth iterated kernel. Now we form the Neumann series expansion.
u = (T −λI)−1 f
= −1
λ

I −1
λT
−1
f
= −1
λ
∞
X
n=0
1
λnT nf
= −1
λ
∞
X
n=0
1
λnT nf
= −1
λ
∞
X
n=0
1
λn
Z ∞
−∞
Kn(x −y)f(y) dy
The Neumann series is the same as the series we derived with the Fourier transform.
Solution 48.22
We seek a transformation T such that
(L −λI)Tf = f.
We denote u = Tf to obtain a boundary value problem,
u′′ −λu = f,
u(−1) = u(1) = 0.
This problem has a unique solution if and only if the homogeneous adjoint problem has only the trivial solution.
u′′ −λu = 0,
u(−1) = u(1) = 0.
This homogeneous problem has the eigenvalues and eigenfunctions,
λn = −
nπ
2
2
,
un = sin
nπ
2 (x + 1)

,
n ∈N.
2112

The inhomogeneous problem has the unique solution
u(x) =
Z 1
−1
G(x, ξ; λ)f(ξ) dξ
where
G(x, ξ; λ) =









−
sin(
√
−λ(x<+1)) sin(
√
−λ(1−x>))
√
−λ sin(2
√
−λ)
,
λ < 0,
−1
2(x< + 1)(1 −x>),
λ = 0,
−
sinh(
√
λ(x<+1)) sinh(
√
λ(1−x>))
√
λ sinh(2
√
λ)
,
λ > 0,
for λ ̸= −(nπ/2)2, n ∈N. We set
Tf =
Z 1
−1
G(x, ξ; λ)f(ξ) dξ
and note that since the kernel is continuous this is a bounded linear transformation. If f ∈W, then
(L −λI)Tf = (L −λI)
Z 1
−1
G(x, ξ; λ)f(ξ) dξ
=
Z 1
−1
(L −λI)[G(x, ξ; λ)]f(ξ) dξ
=
Z 1
−1
δ(x −ξ)f(ξ) dξ
= f(x).
2113

If f ∈U then
T(L −λI)f =
Z 1
−1
G(x, ξ; λ)
 f ′′(ξ) −λf(ξ)

dξ
= [G(x, ξ; λ)f ′(ξ)]1
−1 −
Z 1
−1
G′(x, ξ; λ)f ′(ξ) dξ −λ
Z 1
−1
G(x, ξ; λ)f(ξ) dξ
= [−G′(x, ξ; λ)f(ξ)]1
−1 +
Z 1
−1
G′′(x, ξ; λ)f(ξ) dξ −λ
Z 1
−1
G(x, ξ; λ)f(ξ) dξ
=
Z 1
−1
 G′′(x, ξ; λ) −λG(x, ξ; λ)

f(ξ) dξ
=
Z 1
−1
δ(x −ξ)f(ξ) dξ
= f(x).
L has the point spectrum λn = −(nπ/2)2, n ∈N.
Solution 48.23
1. We see that the solution is of the form φ(x) = a + x + bx2 for some constants a and b. We substitute this into
the integral equation.
φ(x) = x + λ
Z 1
0
 x2y −y2
φ(y) dy
a + x + bx2 = x + λ
Z 1
0
 x2y −y2
(a + x + bx2) dy
a + bx2 = λ
60
 −(15 + 20a + 12b) + (20 + 30a + 15b)x2
By equating the coeﬃcients of x0 and x2 we solve for a and b.
a = −
λ(λ + 60)
4(λ2 + 5λ + 60),
b = −
5λ(λ −60)
6(λ2 + 5λ + 60)
2114

Thus the solution of the integral equation is
φ(x) = x −
λ
λ2 + 5λ + 60
5(λ −24)
6
x2 + λ + 60
4

.
2. For x < 1 the integral equation reduces to
φ(x) = x.
For x ≥1 the integral equation becomes,
φ(x) = x + λ
Z 1
0
sin(xy)φ(y) dy.
We could solve this problem by writing down the Neumann series. Instead we will use an eigenfunction expansion.
Let {λn} and {φn} be the eigenvalues and orthonormal eigenfunctions of
φ(x) = λ
Z 1
0
sin(xy)φ(y) dy.
We expand φ(x) and x in terms of the eigenfunctions.
φ(x) =
∞
X
n=1
anφn(x)
x =
∞
X
n=1
bnφn(x),
bn = ⟨x, φn(x)⟩
We determine the coeﬃcients an by substituting the series expansions into the Fredholm equation and equating
2115

coeﬃcients of the eigenfunctions.
φ(x) = x + λ
Z 1
0
sin(xy)φ(y) dy
∞
X
n=1
anφn(x) =
∞
X
n=1
bnφn(x) + λ
Z 1
0
sin(xy)
∞
X
n=1
anφn(y) dy
∞
X
n=1
anφn(x) =
∞
X
n=1
bnφn(x) + λ
∞
X
n=1
an
1
λn
φn(x)
an

1 −λ
λn

= bn
If λ is not an eigenvalue then we can solve for the an to obtain the unique solution.
an =
bn
1 −λ/λn
=
λnbn
λn −λ = bn +
λbn
λn −λ
φ(x) = x +
∞
X
n=1
λbn
λn −λφn(x),
for x ≥1.
If λ = λm, and ⟨x, φm⟩= 0 then there is the one parameter family of solutions,
φ(x) = x + cφm(x) +
∞
X
n=1
n̸=m
λbn
λn −λφn(x),
for x ≥1.
If λ = λm, and ⟨x, φm⟩̸= 0 then there is no solution.
Solution 48.24
1.
Kx = L1L2x = λx
2116

L1L2(L1x) = L1(L1l2 −I)x
= L1(λx −x)
= (λ −1)(L1x)
L1L2(L2x) = (L2L1 + I)L2x
= L2λx + L2x
= (λ + 1)(L2x)
2.
L1L2 −L2L1 =
 d
dt + t
2
 
−d
dt + t
2

−

−d
dt + t
2
  d
dt + t
2

= −d
dt + t
2
d
dt + 1
2I −t
2
d
dt + t2
4 I −

−d
dt −t
2
d
dt −1
2I + t
2
d
dt + t2
4 I

= I
L1L2 = −d
dt + 1
2I + t2
4 I = K + 1
2I
We note that e−t2/4 is an eigenfunction corresponding to the eigenvalue λ = 1/2. Since L1 e−t2/4 = 0 the result
of this problem does not produce any negative eigenvalues. However, Ln
2 e−t2/4 is the product of e−t2/4 and a
polynomial of degree n in t. Since this function is square integrable it is and eigenfunction. Thus we have the
eigenvalues and eigenfunctions,
λn = n −1
2,
φn =
 t
2 −d
dt
n−1
e−t2/4,
for n ∈N.
Solution 48.25
Since λ1 is in the residual spectrum of T, there exists a nonzero y such that
⟨(T −λ1I)x, y⟩= 0
2117

for all x. Now we apply the deﬁnition of the adjoint.
⟨x, (T −λ1I)∗y⟩= 0,
∀x
⟨x, (T ∗−λ1I)y⟩= 0,
∀x
(T ∗−λ1I)y = 0
y is an eigenfunction of T ∗corresponding to the eigenvalue λ1.
Solution 48.26
1.
u′′(t) +
Z 1
0
sin(k(s −t))u(s) ds = f(t),
u(0) = u′(0) = 0
u′′(t) + cos(kt)
Z 1
0
sin(ks)u(s) ds −sin(kt)
Z 1
0
cos(ks)u(s) ds = f(t)
u′′(t) + c1 cos(kt) −c2 sin(kt) = f(t)
u′′(t) = f(t) −c1 cos(kt) + c2 sin(kt)
The solution of
u′′(t) = g(t),
u(0) = u′(0) = 0
using Green functions is
u(t) =
Z t
0
(t −τ)g(τ) dτ.
Thus the solution of our problem has the form,
u(t) =
Z t
0
(t −τ)f(τ) dτ −c1
Z t
0
(t −τ) cos(kτ) dτ + c2
Z t
0
(t −τ) sin(kτ) dτ
u(t) =
Z t
0
(t −τ)f(τ) dτ −c1
1 −cos(kt)
k2
+ c2
kt −sin(kt)
k2
2118

We could determine the constants by multiplying in turn by cos(kt) and sin(kt) and integrating from 0 to 1. This
would yields a set of two linear equations for c1 and c2.
2.
u(x) = λ
Z π
0
∞
X
n=1
sin nx sin ns
n
u(s) ds
We expand u(x) in a sine series.
∞
X
n=1
an sin nx = λ
Z π
0
 ∞
X
n=1
sin nx sin ns
n
!  ∞
X
m=1
am sin ms
!
ds
∞
X
n=1
an sin nx = λ
∞
X
n=1
sin nx
n
∞
X
m=1
Z π
0
am sin ns sin ms ds
∞
X
n=1
an sin nx = λ
∞
X
n=1
sin nx
n
∞
X
m=1
π
2 amδmn
∞
X
n=1
an sin nx = π
2 λ
∞
X
n=1
an
sin nx
n
The eigenvalues and eigenfunctions are
λn = 2n
π ,
un = sin nx,
n ∈N.
3.
φ(θ) = λ
Z 2π
0
1
2π
1 −r2
1 −2r cos(θ −t) + r2φ(t) dt,
|r| < 1
We use Poisson’s formula.
φ(θ) = λu(r, θ),
2119

where u(r, θ) is harmonic in the unit disk and satisﬁes, u(1, θ) = φ(θ). For a solution we need λ = 1 and that
u(r, θ) is independent of r. In this case u(θ) satisﬁes
u′′(θ) = 0,
u(θ) = φ(θ).
The solution is φ(θ) = c1 + c2θ. There is only one eigenvalue and corresponding eigenfunction,
λ = 1,
φ = c1 + c2θ.
4.
φ(x) = λ
Z π
−π
cosn(x −ξ)φ(ξ) dξ
We expand the kernel in a Fourier series. We could ﬁnd the expansion by integrating to ﬁnd the Fourier coeﬃcients,
but it is easier to expand cosn(x) directly.
cosn(x) =
1
2(eıx + e−ıx)
n
= 1
2n
n
0

eınx +
n
1

eı(n−2)x + · · · +

n
n −1

e−ı(n−2)x +
n
n

e−ınx

2120

If n is odd,
cosn(x) = 1
2n
"n
0

(eınx + e−ınx) +
n
1

(eı(n−2)x + e−ı(n−2)x) + · · ·
+

n
(n −1)/2

(eıx + e−ıx)
#
= 1
2n
n
0

2 cos(nx) +
n
1

2 cos((n −2)x) + · · · +

n
(n −1)/2

2 cos(x)

=
1
2n−1
(n−1)/2
X
m=0
n
m

cos((n −2m)x)
=
1
2n−1
n
X
k=1
odd k

n
(n −k)/2

cos(kx).
2121

If n is even,
cosn(x) = 1
2n
"n
0

(eınx + e−ınx) +
n
1

(eı(n−2)x + e−ı(n−2)x) + · · ·
+

n
n/2 −1

(ei2x + e−i2x) +
 n
n/2
#
= 1
2n
n
0

2 cos(nx) +
n
1

2 cos((n −2)x) + · · · +

n
n/2 −1

2 cos(2x) +
 n
n/2

= 1
2n
 n
n/2

+
1
2n−1
(n−2)/2
X
m=0
n
m

cos((n −2m)x)
= 1
2n
 n
n/2

+
1
2n−1
n
X
k=2
even k

n
(n −k)/2

cos(kx).
We will denote,
cosn(x −ξ) = a0
2
n
X
k=1
ak cos(k(x −ξ)),
where
ak = 1 + (−1)n−k
2
1
2n−1

n
(n −k)/2

.
We substitute this into the integral equation.
φ(x) = λ
Z π
−π
 
a0
2
n
X
k=1
ak cos(k(x −ξ))
!
φ(ξ) dξ
φ(x) = λa0
2
Z π
−π
φ(ξ) dξ + λ
n
X
k=1
ak

cos(kx)
Z π
−π
cos(kξ)φ(ξ) dξ + sin(kx)
Z π
−π
sin(kξ)φ(ξ) dξ

2122

For even n, substituting φ(x) = 1 yields λ =
1
πa0. For n and m both even or odd, substituting φ(x) = cos(mx)
or φ(x) = sin(mx) yields λ =
1
πam. For even n we have the eigenvalues and eigenvectors,
λ0 =
1
πa0
,
φ0 = 1,
λm =
1
πa2m
,
φ(1)
m = cos(2mx),
φ(2)
m = sin(2mx),
m = 1, 2, . . . , n/2.
For odd n we have the eigenvalues and eigenvectors,
λm =
1
πa2m−1
,
φ(1)
m = cos((2m −1)x),
φ(2)
m = sin((2m −1)x),
m = 1, 2, . . . , (n + 1)/2.
Solution 48.27
1. First we shift the range of integration to rewrite the kernel.
φ(x) = λ
Z 2π
0
 2π2 −6π|x −s| + 3(x −s)2
φ(s) ds
φ(x) = λ
Z −x+2π
−x
 2π2 −6π|y| + 3y2
φ(x + y) dy
We expand the kernel in a Fourier series.
K(y) = 2π2 −6π|y| + 3y2 =
∞
X
n=−∞
cn eıny
cn = 1
2π
Z −x+2π
−x
K(y) e−ıny dy =
(
6
n2,
n ̸= 0,
0,
n = 0
K(y) =
∞
X
n=−∞
n̸=0
6
n2 eıny =
∞
X
n=1
12
n2 cos(ny)
2123

K(x, s) =
∞
X
n=1
12
n2 cos(n(x −s)) =
∞
X
n=1
12
n2
 cos(nx) cos(nx) + sin(nx) sin(ns)

Now we substitute the Fourier series expression for the kernel into the eigenvalue problem.
φ(x) = 12λ
Z 2π
0
 ∞
X
n=1
1
n2
 cos(nx) cos(ns) + sin(nx) sin(ns)

!
φ(s) ds
From this we obtain the eigenvalues and eigenfunctions,
λn = n2
12π,
φ(1)
n =
1
√π cos(nx),
φ(2)
n =
1
√π sin(nx),
n ∈N.
2. The set of eigenfunctions do not form a complete set. Only those functions with a vanishing integral on [0, 2π]
can be represented. We consider the equation
Z 2π
0
K(x, s)φ(s) ds = 0
Z 2π
0
 ∞
X
n=1
12
n2
 cos(nx) cos(ns) + sin(nx) sin(ns)

!
φ(s) ds = 0
This has the solutions φ = const. The set of eigenfunctions
φ0 =
1
√
2π,
φ(1)
n =
1
√π cos(nx),
φ(2)
n =
1
√π sin(nx),
n ∈N,
is a complete set. We can also write the eigenfunctions as
φn =
1
√
2π
eınx,
n ∈Z.
2124

3. We consider the problem
u −λTu = f.
For λ ̸= λ, (λ not an eigenvalue), we can obtain a unique solution for u.
u(x) = f(x) +
Z 2π
0
Γ(x, s, λ)f(s) ds
Since K(x, s) is self-adjoint and L2(0, 2π), we have
Γ(x, s, λ) = λ
∞
X
n=−∞
n̸=0
φn(x)φn(s)
λn −λ
= λ
∞
X
n=−∞
n̸=0
1
2π eınx e−ıns
n2
12π −λ
= 6λ
∞
X
n=−∞
n̸=0
eın(x−s)
n2 −12πλ
Γ(x, s, λ) = 12λ
∞
X
n=1
cos(n(x −s))
n2 −12πλ
Solution 48.28
First assume that λ is an eigenvalue of T, Tφ = λφ.
p(T)φ =
n
X
k=0
anT nφ
=
n
X
k=0
anλnφ
= p(λ)φ
2125

p(λ) is an eigenvalue of p(T).
Now assume that µ is an eigenvalues of p(T), p(T)φ = µφ. We assume that T has a complete, orthonormal set of
eigenfunctions, {φn} corresponding to the set of eigenvalues {λn}. We expand φ in these eigenfunctions.
p(T)φ = µφ
p(T)
X
cnφn = µ
X
cnφn
X
cnp(λn)φn =
X
cnµφn
p(λn) = µ,
∀n such that cn ̸= 0
Thus all eigenvalues of p(T) are of the form p(λ) with λ an eigenvalue of T.
Solution 48.29
The Fourier cosine transform is deﬁned,
ˆf(ω) = 1
π
Z ∞
0
f(x) cos(ωx) dx,
f(x) = 2
Z ∞
0
ˆf(ω) cos(ωx) dω.
We can write the integral equation in terms of the Fourier cosine transform.
φ(x) = f(x) + λ
Z ∞
0
cos(2xs)φ(s) ds
φ(x) = f(x) + λπ ˆφ(2x)
(48.8)
2126

We multiply the integral equation by 1
π cos(2xs) and integrate.
1
π
Z ∞
0
cos(2xs)φ(x) dx = 1
π
Z ∞
0
cos(2xs)f(x) dx + λ
Z ∞
0
cos(2xs)ˆφ(2x) dx
ˆφ(2s) = ˆf(2s) + λ
2
Z ∞
0
cos(xs)ˆφ(x) dx
ˆφ(2s) = ˆf(2s) + λ
4φ(s)
φ(x) = −4
λ
ˆf(2x) + 4
λ
ˆφ(2x)
(48.9)
We eliminate ˆφ between (48.8) and (48.9).

1 −πλ2
4

φ(x) = f(x) + λπ ˆf(2x)
φ(x) = f(x) + λ
R ∞
0 f(s) cos(2xs) ds
1 −πλ2/4
Solution 48.30
Z
D
vLu dx dy =
Z
D
v(uxx + uyy + aux + buy + cu) dx dy
=
Z
D
(v∇2u + avux + bvuy + cuv) dx dy
=
Z
D
(u∇2v + avux + bvuy + cuv) dx dy +
Z
C
(v∇u −u∇v) · n ds
=
Z
D
(u∇2v −auvx −buvy −uvax −uvby + cuv) dx dy +
Z
C

auv ∂x
∂n + buv ∂y
∂n

ds +
Z
C

v∂u
∂n −u∂v
∂n

2127

Thus we see that
Z
D
(vLu −uL∗v) dx dy =
Z
C
H(u, v) ds,
where
L∗v = vxx + vyy −avx −bvy + (c −ax −by)v
and
H(u, v) =

v∂u
∂n −u∂v
∂n + auv ∂x
∂n + buv ∂y
∂n

.
Let G be the harmonic Green function, which satisﬁes,
∆G = δ in D,
G = 0 on C.
Let u satisfy Lu = 0.
Z
D
(GLu −uL∗G) dx dy =
Z
C
H(u, G) ds
−
Z
D
uL∗G dx dy =
Z
C
H(u, G) ds
−
Z
D
u∆G dx dy −
Z
D
u(L∗−∆)G dx dy =
Z
C
H(u, G) ds
−
Z
D
uδ(x −ξ)δ(y −η) dx dy −
Z
D
u(L∗−∆)G dx dy =
Z
C
H(u, G) ds
−u(ξ, η) −
Z
D
u(L∗−∆)G dx dy =
Z
C
H(u, G) ds
2128

We expand the operators to obtain the ﬁrst form.
u +
Z
D
u(−aGx −bGy + (c −ax −by)G) dx dy = −
Z
C

G∂u
∂n −u∂G
∂n + auG∂x
∂n + buG∂y
∂n

ds
u +
Z
D
((c −ax −by)G −aGx −bGy)u dx dy =
Z
C
u∂G
∂n ds
u +
Z
D
((c −ax −by)G −aGx −bGy)u dx dy = U
Here U is the harmonic function that satisﬁes U = f on C.
We use integration by parts to obtain the second form.
u +
Z
D
(cuG −axuG −byuG −auGx −buGy) dx dy = U
u +
Z
D
(cuG −axuG −byuG + (au)xG + (bu)yG) dx dy −
Z
C

auG∂y
∂n + buG∂x
∂n

ds = U
u +
Z
D
(cuG −axuG −byuG + axuG + auxG + byuG + buyG) dx dy = U
u +
Z
D
(aux + buy + cu)G dx dy = U
Solution 48.31
1. First we diﬀerentiate to obtain a diﬀerential equation.
φ(x) = λ
Z 1
0
min(x, s)φ(s) ds = λ
Z x
0
es φ(s) ds +
Z 1
x
ex φ(s) ds

φ′(x) = λ

xφ(x) +
Z 1
x
φ(s) ds −xφ(x)

= λ
Z 1
x
φ(s) ds
φ′′(x) = −λφ(x)
2129

We note that that φ(x) satisﬁes the constraints,
φ(0) = λ
Z 1
0
0 · φ(s) ds = 0,
φ′(1) = λ
Z 1
1
φ(s) ds = 0.
Thus we have the problem,
φ′′ + λφ = 0,
φ(0) = φ′(1) = 0.
The general solution of the diﬀerential equation is
φ(x) =







a + bx
for λ = 0
a cos
√
λx

+ b sin
√
λx

for λ > 0
a cosh
 √
−λx

+ b sinh
 √
−λx

for λ < 0
We see that for λ = 0 and λ < 0 only the trivial solution satisﬁes the homogeneous boundary conditions. For
positive λ the left boundary condition demands that a = 0. The right boundary condition is then
b
√
λ cos
√
λ

= 0
The eigenvalues and eigenfunctions are
λn =
(2n −1)π
2
2
,
φn(x) = sin
(2n −1)π
2
x

,
n ∈N
2130

2. First we diﬀerentiate the integral equation.
φ(x) = λ
Z x
0
es φ(s) ds +
Z 1
x
ex φ(s) ds

φ′(x) = λ

ex φ(x) + ex
Z 1
x
φ(s) ds −ex φ(x)

= λ ex
Z 1
x
φ(s) ds
φ′′(x) = λ

ex
Z 1
x
φ(s) ds −ex φ(x)

φ(x) satisﬁes the diﬀerential equation
φ′′ −φ′ + λ ex φ = 0.
We note the boundary conditions,
φ(0) −φ′(0) = 0,
φ′(1) = 0.
In self-adjoint form, the problem is
 e−x φ′′ + λφ = 0,
φ(0) −φ′(0) = 0,
φ′(1) = 0.
The Rayleigh quotient is
λ = [−e−x φφ′]1
0 +
R 1
0 e−x(φ′)2 dx
R 1
0 φ2 dx
= φ(0)φ′(0) +
R 1
0 e−x(φ′)2 dx
R 1
0 φ2 dx
= (φ(0))2 +
R 1
0 e−x(φ′)2 dx
R 1
0 φ2 dx
2131

Thus we see that there are only positive eigenvalues. The diﬀerential equation has the general solution
φ(x) = ex/2 
aJ1

2
√
λ ex/2
+ bY1

2
√
λ ex/2
We deﬁne the functions,
u(x; λ) = ex/2 J1

2
√
λ ex/2
,
v(x; λ) = ex/2 Y1

2
√
λ ex/2
.
We write the solution to automatically satisfy the right boundary condition, φ′(1) = 0,
φ(x) = v′(1; λ)u(x; λ) −u′(1; λ)v(x; λ).
We determine the eigenvalues from the left boundary condition, φ(0) −φ′(0) = 0. The ﬁrst few are
λ1 ≈0.678298
λ2 ≈7.27931
λ3 ≈24.9302
λ4 ≈54.2593
λ5 ≈95.3057
The eigenfunctions are,
φn(x) = v′(1; λn)u(x; λn) −u′(1; λn)v(x; λn).
Solution 48.32
1. First note that
sin(kx) sin(lx) = sign(kl) sin(ax) sin(bx)
where
a = max(|k|, |l|),
b = min(|k|, |l|).
Consider the analytic function,
eı(a−b)x −eı(a+b)
2
= sin(ax) sin(bx) −ı cos(ax) sin(bx).
2132

−
Z ∞
−∞
sin(kx) sin(lx)
x2 −z2
dx = sign(kl) −
Z ∞
−∞
sin(ax) sin(bx)
x2 −z2
dx
= sign(kl) 1
2z −
Z ∞
−∞
sin(ax) sin(bx)
x −z
−sin(ax) sin(bx)
x + z

dx
= −π sign(kl) 1
2z (−cos(az) sin(bz) + cos(−az) sin(−bz))
−
Z ∞
−∞
sin(kx) sin(lx)
x2 −z2
dx = sign(kl)π
z cos(az) sin(bz)
2. Consider the analytic function,
eı|p|x −eı|q|x
x
= cos(|p|x) −cos(|q|x) + ı(sin(|p|x) −sin(|q|x))
x
.
−
Z ∞
−∞
cos(px) −cos(qx)
x2
dx = −
Z ∞
−∞
cos(|p|x) −cos(|q|x)
x2
dx
= −π lim
x→0
sin(|p|x) −sin(|q|x)
x
−
Z ∞
−∞
cos(px) −cos(qx)
x2
dx = π(|q| −|p|)
3. We use the analytic function,
ı(x −ıa)(x −ıb) eıx
(x2 + a2)(x2 + b2) = −(x2 −ab) sin x + (a + b)x cos x + ı((x2 −ab) cos x + (a + b)x sin x)
(x2 + a2)(x2 + b2)
−
Z ∞
−∞
−(x2 −ab) sin x + (a + b)x cos x
x(x2 + a2)(x2 + b2)
= −π lim
x→0
(x2 −ab) cos x + (a + b)x sin x
(x2 + a2)(x2 + b2)
= −π−ab
a2b2
2133

−
Z ∞
−∞
−(x2 −ab) sin x + (a + b)x cos x
(x2 + a2)(x2 + b2)
= π
ab
Solution 48.33
We consider the function
G(z) =
 (1 −z2)1/2 + ız

log(1 + z).
For (1 −z2)1/2 = (1 −z)1/2(1 + z)1/2 we choose the angles,
−π < arg(1 −z) < π,
0 < arg(1 + z) < 2π,
so that there is a branch cut on the interval (−1, 1). With this choice of branch, G(z) vanishes at inﬁnity. For the
logarithm we choose the principal branch,
−π < arg(1 + z) < π.
For t ∈(−1, 1),
G+(t) =
√
1 −t2 + ıt

log(1 + t),
G−(t) =

−
√
1 −t2 + ıt

log(1 + t),
G+(t) −G−(t) = 2
√
1 −t2 log(1 + t),
1
2
 G+(t) + G−(t)

= ıt log(1 + t).
For t ∈(−∞, −1),
G+(t) = ı
√
1 −t2 + t

(log(−t −1) + ıπ) ,
G−(t) = ı

−
√
1 −t2 + t

(log(−t −1) −ıπ) ,
2134

G+(t) −G−(t) = −2π
√
t2 −1 + t

.
For x ∈(−1, 1) we have
G(x) = 1
2
 G+(x) + G−(x)

= ıx log(1 + x)
=
1
ı2π
Z
−
∞−1−2π(
√
t2 −1 + t)
t −x
dt + 1
ı2π
Z 1
−1
2
√
1 −t2 log(1 + t)
t −x
dt
From this we have
Z 1
−1
√
1 −t2 log(1 + t)
t −x
dt
= −πx log(1 + x) + π
Z ∞
1
t −
√
t2 −1
t + x
dt
= π

x log(1 + x) −1 + π
2
√
1 −x2 −
√
1 −x2 arcsin(x) + x log(2) + x log(1 + x)

Z 1
−1
√
1 −t2 log(1 + t)
t −x
dt = π

x log x −1 +
√
1 −x2
π
2 −arcsin(x)

Solution 48.34
Let F(z) denote the value of the integral.
F(z) = 1
ıπ −
Z
C
f(t) dt
t −z
From the Plemelj formula we have,
F +(t0) + F −(t0) = 1
ıπ −
Z
C
f(t)
t −t0
dt,
f(t0) = F +(t0) −F −(t0).
2135

With W(z) deﬁned as above, we have
W +(t0) + W −(t0) = F +(t0) −F −(t0) = f(t0),
and also
W +(t0) + W −(t0) = 1
ıπ −
Z
C
W +(t) −W −(t)
t −t0
dt
= 1
ıπ −
Z
C
F +(t) + F −(t)
t −t0
dt
= 1
ıπ −
Z
C
g(t)
t −t0
dt.
Thus the solution of the integral equation is
f(t0) = 1
ıπ −
Z
C
g(t)
t −t0
dt.
2136

Solution 48.35
(i)
G(τ) = (τ −β)−1
τ −β
τ −α
γ
G+(ζ) = (ζ −β)−1
ζ −β
ζ −α
γ
G−(ζ) = e−ı2πγ G+(ζ)
G+(ζ) −G−(ζ) = (1 −e−ı2πγ)(ζ −β)−1
ζ −β
ζ −α
γ
G+(ζ) + G−(ζ) = (1 + e−ı2πγ)(ζ −β)−1
ζ −β
ζ −α
γ
G+(ζ) + G−(ζ) = 1
ıπ −
Z
C
(1 −e−ı2πγ) dτ
(τ −β)1−γ(τ −α)γ(τ −ζ)
1
ıπ −
Z
C
dτ
(τ −β)1−γ(τ −α)γ(τ −ζ) = −ı cot(πγ)(ζ −β)γ−1
(ζ −α)γ
(ii) Consider the branch of
z −β
z −α
γ
2137

that tends to unity as z →∞. We ﬁnd a series expansion of this function about inﬁnity.
z −β
z −α
γ
=

1 −β
z
γ 
1 −α
z
−γ
=
 ∞
X
j=0
(−1)j
γ
j
 β
z
j!  ∞
X
k=0
(−1)k
−γ
k
 α
z
k
!
=
∞
X
j=0
 
j
X
k=0
(−1)j

γ
j −k
−γ
k

βj−kαk
!
z−j
Deﬁne the polynomial
Q(z) =
n
X
j=0
 
j
X
k=0
(−1)j

γ
j −k
−γ
k

βj−kαk
!
zn−j.
Then the function
G(z) =
z −β
z −α
γ
zn −Q(z)
2138

vanishes at inﬁnity.
G+(ζ) =
ζ −β
ζ −α
γ
ζn −Q(ζ)
G−(ζ) = e−ı2πγ
ζ −β
ζ −α
γ
ζn −Q(ζ)
G+(ζ) −G−(ζ) =
ζ −β
ζ −α
γ
ζn  1 −e−ı2πγ
G+(ζ) + G−(ζ) =
ζ −β
ζ −α
γ
ζn  1 + e−ı2πγ
−2Q(ζ)
1
iπ −
Z
C
τ −β
τ −α
γ
τ n  1 −e−ı2πγ
1
τ −ζ dτ =
ζ −β
ζ −α
γ
ζn  1 + e−ı2πγ
−2Q(ζ)
1
iπ −
Z
C
τ −β
τ −α
γ
τ n
τ −ζ dτ = −ı cot(πγ)
ζ −β
ζ −α
γ
ζn −(1 −ı cot(πγ))Q(ζ)
1
iπ −
Z
C
τ −β
τ −α
γ
τ n
τ −ζ dτ = −ı cot(πγ)
ζ −β
ζ −α
γ
ζn −Q(ζ)

−Q(ζ)
Solution 48.36
−
Z 1
−1
φ(y)
y2 −x2 dy = 1
2x −
Z 1
−1
φ(y)
y −x dy −1
2x −
Z 1
−1
φ(y)
y + x dy
= 1
2x −
Z 1
−1
φ(y)
y −x dy + 1
2x −
Z 1
−1
φ(−y)
y −x dy
= 1
2x −
Z 1
−1
φ(y) + φ(−y)
y −x
dy
2139

1
2x −
Z 1
−1
φ(y) + φ(−y)
y −x
dy = f(x)
1
ıπ −
Z 1
−1
φ(y) + φ(−y)
y −x
dy = 2x
ıπ f(x)
φ(x) + φ(−x) =
1
ıπ
√
1 −x2 −
Z 1
−1
2y
ıπ f(y)
p
1 −y2
1
y −x dy +
k
√
1 −x2
φ(x) + φ(−x) = −
1
π2√
1 −x2 −
Z 1
−1
2yf(y)
p
1 −y2
y −x
dy +
k
√
1 −x2
φ(x) = −
1
π2√
1 −x2 −
Z 1
−1
yf(y)
p
1 −y2
y −x
dy +
k
√
1 −x2 + g(x)
Here k is an arbitrary constant and g(x) is an arbitrary odd function.
Solution 48.37
We deﬁne
F(z) =
1
ı2π −
Z 1
0
f(t)
t −z dt.
The Plemelj formulas and the integral equation give us,
F +(x) −F −(x) = f(x)
F +(x) + F −(x) = λf(x).
We solve for F + and F −.
F +(x) = (λ + 1)f(x)
F −(x) = (λ −1)f(x)
By writing
F +(x)
F −(x) = λ + 1
λ −1
2140

we seek to determine F to within a multiplicative constant.
log F +(x) −log F −(x) = log
λ + 1
λ −1

log F +(x) −log F −(x) = log
1 + λ
1 −λ

+ ıπ
log F +(x) −log F −(x) = γ + ıπ
We have left oﬀthe additive term of ı2πn in the above equation, which will introduce factors of zk and (z −1)m in
F(z). We will choose these factors so that F(z) has integrable algebraic singularites and vanishes at inﬁnity. Note that
we have deﬁned γ to be the real parameter,
γ = log
1 + λ
1 −λ

.
By the discontinuity theorem,
log F(z) =
1
ı2π
Z 1
0
γ + ıπ
t −z dz
=
1
2 −ı γ
2π

log
1 −z
−z

= log
 z −1
z
1/2−ıγ/(2π)!
2141

F(z) =
z −1
z
1/2−ıγ/(2π)
zk(z −1)m
F(z) =
1
p
z(z −1)
z −1
z
−ıγ/(2π)
F ±(x) = e±ıπ(−ıγ/(2π))
p
x(1 −x)
1 −x
x
−ıγ/(2π)
F ±(x) =
e±γ/2
p
x(1 −x)
1 −x
x
−ıγ/(2π)
Deﬁne
f(x) =
1
p
x(1 −x)
1 −x
x
−ıγ/(2π)
.
We apply the Plemelj formulas.
1
ıπ −
Z 1
0
 eγ/2 −e−γ/2 f(t)
t −x dt =
 eγ/2 + e−γ/2
f(x)
1
ıπ −
Z 1
0
f(t)
t −x dt = tanh
γ
2

f(x)
Thus we see that the eigenfunctions are
φ(x) =
1
p
x(1 −x)
1 −x
x
−ı tanh−1(λ)/π
for −1 < λ < 1.
The method used in this problem cannot be used to construct eigenfunctions for λ > 1. For this case we cannot
ﬁnd an F(z) that has integrable algebraic singularities and vanishes at inﬁnity.
2142

Solution 48.38
1
ıπ −
Z 1
0
f(t)
t −x dt = −
ı
tan(x)f(x)
We deﬁne the function,
F(z) =
1
ı2π −
Z 1
0
f(t)
t −z dt.
The Plemelj formula are,
F +(x) −F −(x) = f(x)
F +(x) + F −(x) = −
ı
tan(x)f(x).
We solve for F + and F −.
F ±(x) = 1
2

±1 −
ı
tan(x)

f(x)
From this we see
F +(x)
F −(x) = 1 −ı/ tan(x)
−1 −ı/ tan(x) = eı2x .
We seek to determine F(z) up to a multiplicative constant. Taking the logarithm of this equation yields
log F +(x) −log F −(x) = ı2x + ı2πn.
The ı2πn term will give us the factors (z −1)k and zm in the solution for F(z). We will choose the integers k and m
so that F(z) has only algebraic singularities and vanishes at inﬁnity. We drop the ı2πn term for now.
log F(z) =
1
ı2π
Z 1
0
ı2t
t −z dt
log F(z) = 1
π + z
π log
1 −z
−z

F(z) = e1/π
z −1
z
z/π
2143

We replace e1/π by a multiplicative constant and multiply by (z −1)1 to give F(z) the desired properties.
F(z) =
c
(z −1)1−z/πzz/π
We evaluate F(z) above and below the branch cut.
F ±(x) =
c
e±(ıπ−ıx)(1 −x)1−x/πxx/π =
c e±ıx
(1 −x)1−x/πxx/π
Finally we use the Plemelj formulas to determine f(x).
f(x) = F +(x) −F −(x) =
k sin(x)
(1 −x)1−x/πxx/π
Solution 48.39
Consider the equation,
f ′(z) + λ
Z
C
f(t)
t −z dt = 1.
Since the integral is an analytic function of z oﬀC we know that f(z) is analytic oﬀC. We use Cauchy’s theorem to
evaluate the integral and obtain a diﬀerential equation for f(x).
f ′(x) + λ −
Z
C
f(t)
t −x dt = 1
f ′(x) + ıλπf(x) = 1
f(x) =
1
ıλπ + c e−ıλπx
Consider the equation,
f ′(z) + λ
Z
C
f(t)
t −z dt = g(z).
2144

Since the integral and g(z) are analytic functions inside C we know that f(z) is analytic inside C. We use Cauchy’s
theorem to evaluate the integral and obtain a diﬀerential equation for f(x).
f ′(x) + λ −
Z
C
f(t)
t −x dt = g(x)
f ′(x) + ıλπf(x) = g(x)
f(x) =
Z x
z0
e−ıλπ(x−ξ) g(ξ) dξ + c e−ıλπx
Here z0 is any point inside C.
Solution 48.40
−
Z
C

1
t −x + P(t −x)

f(t) dt = g(x)
1
ıπ −
Z
C
f(t)
t −x dt = 1
ıπg(x) −1
ıπ
Z
C
P(t −x)f(t) dt
We know that if
1
ıπ −
Z
C
f(τ)
τ −ζ dτ = g(ζ)
then
f(ζ) = 1
ıπ −
Z
C
g(τ)
τ −ζ dτ.
2145

We apply this theorem to the integral equation.
f(x) = −1
π2 −
Z
C
g(t)
t −x dt + 1
π2 −
Z
C
Z
C
P(τ −t)f(τ) dτ

1
t −x dt
= −1
π2 −
Z
C
g(t)
t −x dt + 1
π2
Z
C

−
Z
C
P(τ −t)
t −x
dt

f(τ) dτ
= −1
π2 −
Z
C
g(t)
t −x dt −1
ıπ
Z
C
P(t −x)f(t) dt
Now we substitute the non-analytic part of f(t) into the integral. (The analytic part integrates to zero.)
= −1
π2 −
Z
C
g(t)
t −x dt −1
ıπ
Z
C
P(t −x)

−1
π2 −
Z
C
g(τ)
τ −t dτ

dt
= −1
π2 −
Z
C
g(t)
t −x dt −1
π2
Z
C

−1
ıπ −
Z
C
P(t −x)
τ −t
dt

g(τ) dτ
= −1
π2 −
Z
C
g(t)
t −x dt −1
π2
Z
C
P(τ −x)g(τ) dτ
f(x) = −1
π2 −
Z
C
g(t)
t −x dt −1
π2
Z
C
P(t −x)g(t) dt
Solution 48.41
Solution 48.42
2146

Part VII
Nonlinear Diﬀerential Equations
2147

Chapter 49
Nonlinear Ordinary Diﬀerential Equations
2148

49.1
Exercises
Exercise 49.1
A model set of equations to describe an epidemic, in which x(t) is the number infected, y(t) is the number susceptible,
is
dx
dt = rxy −γx,
dy
dt = −rxy + β,
where r > 0, β ≥0, γ ≥0. Initially x = x0, y = y0 at t = 0. Directly from the equations, without using the phase
plane:
1. Find the solution, x(t), y(t), in the case β = γ = 0.
2. Show for the case β = 0, γ ̸= 0 that x(t) ﬁrst decreases or increases according as ry0 < γ or ry0 > γ. Show
that x(t) →0 as t →∞in both cases. Find x as a function of y.
3. In the phase plane: Find the position of the singular point and its type when β > 0, γ > 0.
Exercise 49.2
Find the singular points and their types for the system
du
dx = ru + v(1 −v)(p −v),
r > 0, 0 < p < 1,
dv
dx = u,
which comes from one of our nonlinear diﬀusion problems. Note that there is a solution with
u = α(1 −v)
for special values of α and r. Find v(x) for this special case.
2149

Exercise 49.3
Check that r = 1 is a limit cycle for
dx
dt = −y + x(1 −r2)
dy
dt = x + y(1 −r2)
(r = x2 + y2), and that all solution curves spiral into it.
Exercise 49.4
Consider
ϵ ˙y = f(y) −x
˙x = y
Introduce new coordinates, R, θ given by
x = R cos θ
y = 1
√ϵR sin θ
and obtain the exact diﬀerential equations for R(t), θ(t). Show that R(t) continually increases with t when R ̸= 0.
Show that θ(t) continually decreases when R > 1.
Exercise 49.5
One choice of the Lorenz equations is
˙x = −10x + 10y
˙y = Rx −y −xz
˙z = −8
3z + xy
Where R is a positive parameter.
2150

1. Invistigate the nature of the sigular point at (0, 0, 0) by ﬁnding the eigenvalues and their behavior for all 0 < R <
∞.
2. Find the other singular points when R > 1.
3. Show that the appropriate eigenvalues for these other singular points satisfy the cubic
3λ3 + 41λ2 + 8(10 + R)λ + 160(R −1) = 0.
4. There is a special value of R, call it Rc, for which the cubic has two pure imaginary roots, ±ıµ say. Find Rc and
µ; then ﬁnd the third root.
Exercise 49.6
In polar coordinates (r, φ), Einstein’s equations lead to the equation
d2v
dφ2 + v = 1 + ϵv2,
v = 1
r,
for planetary orbits. For Mercury, ϵ = 8 × 10−8. When ϵ = 0 (Newtonian theory) the orbit is given by
v = 1 + A cos φ, period 2π.
Introduce θ = ωφ and use perturbation expansions for v(θ) and ω in powers of ϵ to ﬁnd the corrections proportional to
ϵ.
[A is not small; ϵ is the small parameter].
Exercise 49.7
Consider the problem
¨x + ω2
0x + αx2 = 0,
x = a, ˙x = 0 at t = 0
Use expansions
x = a cos θ + a2x2(θ) + a3x3(θ) + · · · , θ = ωt
ω = ω0 + a2ω2 + · · · ,
2151

to ﬁnd a periodic solution and its natural frequency ω.
Note that, with the expansions given, there are no “secular term” troubles in the determination of x2(θ), but x2(θ)
is needed in the subsequent determination of x3(θ) and ω.
Show that a term aω1 in the expansion for ω would have caused trouble, so ω1 would have to be taken equal to
zero.
Exercise 49.8
Consider the linearized traﬃc problem
dpn(t)
dt
= α [pn−1(t) −pn(t)] ,
n ≥1,
pn(0) = 0,
n ≥1,
p0(t) = aeıωt,
t > 0.
(We take the imaginary part of pn(t) in the ﬁnal answers.)
1. Find p1(t) directly from the equation for n = 1 and note the behavior as t →∞.
2. Find the generating function
G(s, t) =
∞
X
n=1
pn(t)sn.
3. Deduce that
pn(t) ∼Aneıωt,
as t →∞,
and ﬁnd the expression for An. Find the imaginary part of this pn(t).
Exercise 49.9
1. For the equation modiﬁed with a reaction time, namely
d
dtpn(t + τ) = α[pn−1(t) −pn(t)]
n ≥1,
ﬁnd a solution of the form in 1(c) by direct substitution in the equation. Again take its imaginary part.
2152

2. Find a condition that the disturbance is stable, i.e. pn(t) remains bounded as n →∞.
3. In the stable case show that the disturbance is wave-like and ﬁnd the wave velocity.
2153

49.2
Hints
Hint 49.1
Hint 49.2
Hint 49.3
Hint 49.4
Hint 49.5
Hint 49.6
Hint 49.7
Hint 49.8
Hint 49.9
2154

49.3
Solutions
Solution 49.1
1. When β = γ = 0 the equations are
dx
dt = rxy,
dy
dt = −rxy.
Adding these two equations we see that
dx
dt = −dy
dt .
Integrating and applying the initial conditions x(0) = x0 and y(0) = y0 we obtain
x = x0 + y0 −y
Substituting this into the diﬀerential equation for y,
dy
dt = −r(x0 + y0 −y)y
dy
dt = −r(x0 + y0)y + ry2.
2155

We recognize this as a Bernoulli equation and make the substitution u = y−1.
−y−2dy
dt = r(x0 + y0)y−1 −r
du
dt = r(x0 + y0)u −r
d
dt
 e−r(x0+y0)tu

= −re−r(x0+y0)t
u = er(x0+y0)t
Z t
−re−r(x0+y0)t dt + cer(x0+y0)t
u =
1
x0 + y0
+ cer(x0+y0)t
y =

1
x0 + y0
+ cer(x0+y0)t
−1
Applying the initial condition for y,

1
x0 + y0
+ c
−1
= y0
c = 1
y0
−
1
x0 + y0
.
The solution for y is then
y =

1
x0 + y0
+
 1
y0
−
1
x0 + y0

er(x0+y0)t
−1
Since x = x0 + y0 −y, the solution to the system of diﬀerential equations is
x = x0 + y0 −
 1
y0
+
1
x0 + y0
 1 −er(x0+y0)t−1
,
y =
 1
y0
+
1
x0 + y0
 1 −er(x0+y0)t−1
.
2156

2. For β = 0, γ ̸= 0, the equation for x is
˙x = rxy −γx.
At t = 0,
˙x(0) = x0(ry0 −γ).
Thus we see that if ry0 < γ, x is initially decreasing. If ry0 > γ, x is initially increasing.
Now to show that x(t) →0 as t →∞. First note that if the initial conditions satisfy x0, y0 > 0 then x(t), y(t) > 0
for all t ≥0 because the axes are a seqaratrix. y(t) is is a strictly decreasing function of time. Thus we see
that at some time the quantity x(ry −γ) will become negative. Since y is decreasing, this quantity will remain
negative. Thus after some time, x will become a strictly decreasing quantity. Finally we see that regardless of
the initial conditions, (as long as they are positive), x(t) →0 as t →∞.
Taking the ratio of the two diﬀerential equations,
dx
dy = −1 + γ
ry.
x = −y + γ
r ln y + c
Applying the intial condition,
x0 = −y0 + γ
r ln y0 + c
c = x0 + y0 −γ
r ln y0.
Thus the solution for x is
x = x0 + (y0 −y) + γ
r ln
 y
y0

.
3. When β > 0 and γ > 0 the system of equations is
˙x = rxy −γx
˙y = −rxy + β.
2157

The equilibrium solutions occur when
x(ry −γ) = 0
β −rxy = 0.
Thus the singular point is
x = β
γ ,
y = γ
r .
Now to classify the point. We make the substitution u = (x −β
γ ), v = (y −γ
r ).
˙u = r

u + β
γ
 
v + γ
r

−γ

u + β
γ

˙v = −r

u + β
γ
 
v + γ
r

+ β
˙u = rβ
γ v + ruv
˙v = −γu −rβ
γ v −ruv
The linearized system is
˙u = rβ
γ v
˙v = −γu −rβ
γ v
Finding the eigenvalues of the linearized system,

λ
−rβ
γ
γ
λ + rβ
γ
 = λ2 + rβ
γ λ + rβ = 0
2158

λ =
−rβ
γ ±
q
(rβ
γ )2 −4rβ
2
Since both eigenvalues have negative real part, we see that the singular point is asymptotically stable. A plot
of the vector ﬁeld for r = γ = β = 1 is attached. We note that there appears to be a stable singular point at
x = y = 1 which corroborates the previous results.
Solution 49.2
The singular points are
u = 0, v = 0,
u = 0, v = 1,
u = 0, v = p.
The point u = 0, v = 0. The linearized system about u = 0, v = 0 is
du
dx = ru
dv
dx = u.
The eigenvalues are

λ −r
0
−1
λ
 = λ2 −rλ = 0.
λ = 0, r.
Since there are positive eigenvalues, this point is a source. The critical point is unstable.
The point u = 0, v = 1. Linearizing the system about u = 0, v = 1, we make the substitution w = v −1.
du
dx = ru + (w + 1)(−w)(p −1 −w)
dw
dx = u
du
dx = ru + (1 −p)w
dw
dx = u
2159


λ −r
(p −1)
−1
λ
 = λ2 −rλ + p −1 = 0
λ = r ±
p
r2 −4(p −1)
2
Thus we see that this point is a saddle point. The critical point is unstable.
The point u = 0, v = p. Linearizing the system about u = 0, v = p, we make the substitution w = v −p.
du
dx = ru + (w + p)(1 −p −w)(−w)
dw
dx = u
du
dx = ru + p(p −1)w
dw
dx = u

λ −r
p(1 −p)
−1
λ
 = λ2 −rλ + p(1 −p) = 0
λ = r ±
p
r2 −4p(1 −p)
2
Thus we see that this point is a source. The critical point is unstable.
The solution of for special values of α and r. Diﬀerentiating u = αv(1 −v),
du
dv = α −2αv.
2160

Taking the ratio of the two diﬀerential equations,
du
dv = r + v(1 −v)(p −v)
u
= r + v(1 −v)(p −v)
αv(1 −v)
= r + (p −v)
α
Equating these two expressions,
α −2αv = r + p
α −v
α.
Equating coeﬃcients of v, we see that α =
1
√
2.
1
√
2 = r +
√
2p
Thus we have the solution u =
1
√
2v(1 −v) when r =
1
√
2 −
√
2p. In this case, the diﬀerential equation for v is
dv
dx = 1
√
2v(1 −v)
−v−2 dv
dx = −1
√
2v−1 + 1
√
2
2161

We make the change of variablles y = v−1.
dy
dx = −1
√
2y + 1
√
2
d
dx

ex/
√
2y

= ex/
√
2
√
2
y = e−x/
√
2
Z x ex/
√
2
√
2 dx + ce−x/
√
2
y = 1 + ce−x/
√
2
The solution for v is
v(x) =
1
1 + ce−x/
√
2.
Solution 49.3
We make the change of variables
x = r cos θ
y = r sin θ.
Diﬀerentiating these expressions with respect to time,
˙x = ˙r cos θ −r ˙θ sin θ
˙y = ˙r sin θ + r ˙θ cos θ.
Substituting the new variables into the pair of diﬀerential equations,
˙r cos θ −r ˙θ sin θ = −r sin θ + r cos θ(1 −r2)
˙r sin θ + r ˙θ cos θ = r cos θ + r sin θ(1 −r2).
2162

Multiplying the equations by cos θ and sin θ and taking their sum and diﬀerence yields
˙r = r(1 −r2)
r ˙θ = r.
We can integrate the second equation.
˙r = r(1 −r2)
θ = t + θ0
At this point we could note that ˙r > 0 in (0, 1) and ˙r < 0 in (1, ∞). Thus if r is not initially zero, then the solution
tends to r = 1.
Alternatively, we can solve the equation for r exactly.
˙r = r −r3
˙r
r3 = 1
r2 −1
We make the change of variables u = 1/r2.
−1
2 ˙u = u −1
˙u + 2u = 2
u = e−2t
Z t
2e2t dt + ce−2t
u = 1 + ce−2t
r =
1
√
1 + ce−2t
Thus we see that if r is initiall nonzero, the solution tends to 1 as t →∞.
2163

Solution 49.4
The set of diﬀerential equations is
ϵ ˙y = f(y) −x
˙x = y.
We make the change of variables
x = R cos θ
y = 1
√ϵR sin θ
Diﬀerentiating x and y,
˙x = ˙R cos θ −R ˙θ sin θ
˙y = 1
√ϵ
˙R sin θ + 1
√ϵR ˙θ cos θ.
The pair of diﬀerential equations become
√ϵ ˙R sin θ + √ϵR ˙θ cos θ = f
 1
√ϵR sin θ

−R cos θ
˙R cos θ −R ˙θ sin θ = 1
√ϵR sin θ.
˙R sin θ + R ˙θ cos θ = −1
√ϵR cos θ 1
√ϵf
 1
√ϵR sin θ

˙R cos θ −R ˙θ sin θ = 1
√ϵR sin θ.
2164

Multiplying by cos θ and sin θ and taking the sum and diﬀerence of these diﬀerential equations yields
˙R = 1
√ϵ sin θf
 1
√ϵR sin θ

R ˙θ = −1
√ϵR + 1
√ϵ cos θf
 1
√ϵR sin θ

.
Dividing by R in the second equation,
˙R = 1
√ϵ sin θf
 1
√ϵR sin θ

˙θ = −1
√ϵ + 1
√ϵ
cos θ
R f
 1
√ϵR sin θ

.
We make the assumptions that 0 < ϵ < 1 and that f(y) is an odd function that is nonnegative for positive y and
satisﬁes |f(y)| ≤1 for all y.
Since sin θ is odd,
sin θf
 1
√ϵR sin θ

is nonnegative. Thus R(t) continually increases with t when R ̸= 0.
If R > 1 then

cos θ
R f
 1
√ϵR sin θ
 ≤
f
 1
√ϵR sin θ

≤1.
Thus the value of ˙θ,
−1
√ϵ + 1
√ϵ
cos θ
R f
 1
√ϵR sin θ

,
is always nonpositive. Thus θ(t) continually decreases with t.
2165

Solution 49.5
1. Linearizing the Lorentz equations about (0, 0, 0) yields


˙x
˙y
˙z

=


−10
10
0
R
−1
0
0
0
−8/3




x
y
z


The eigenvalues of the matrix are
λ1 = −8
3,
λ2 = −11 −
√
81 + 40R
2
λ3 = −11 +
√
81 + 40R
2
.
There are three cases for the eigenvalues of the linearized system.
R < 1. There are three negative, real eigenvalues. In the linearized and also the nonlinear system, the origin is
a stable, sink.
R = 1. There are two negative, real eigenvalues and one zero eigenvalue. In the linearized system the origin
is stable and has a center manifold plane. The linearized system does not tell us if the nonlinear system is
stable or unstable.
R > 1. There are two negative, real eigenvalues, and one positive, real eigenvalue. The origin is a saddle point.
2. The other singular points when R > 1 are
 
±
r
8
3(R −1), ±
r
8
3(R −1), R −1
!
.
2166

3. Linearizing about the point
 r
8
3(R −1),
r
8
3(R −1), R −1
!
yields


˙X
˙Y
˙Z

=




−10
10
0
1
−1
−
q
8
3(R −1)
q
8
3(R −1)
q
8
3(R −1)
−8
3






X
Y
Z


The characteristic polynomial of the matrix is
λ3 + 41
3 λ2 + 8(10 + R)
3
λ + 160
3 (R −1).
Thus the eigenvalues of the matrix satisfy the polynomial,
3λ3 + 41λ2 + 8(10 + R)λ + 160(R −1) = 0.
Linearizing about the point
 
−
r
8
3(R −1), −
r
8
3(R −1), R −1
!
yields


˙X
˙Y
˙Z

=




−10
10
0
1
−1
q
8
3(R −1)
−
q
8
3(R −1)
−
q
8
3(R −1)
−8
3






X
Y
Z


The characteristic polynomial of the matrix is
λ3 + 41
3 λ2 + 8(10 + R)
3
λ + 160
3 (R −1).
2167

Thus the eigenvalues of the matrix satisfy the polynomial,
3λ3 + 41λ2 + 8(10 + R)λ + 160(R −1) = 0.
4. If the characteristic polynomial has two pure imaginary roots ±ıµ and one real root, then it has the form
(λ −r)(λ2 + µ2) = λ3 −rλ2 + µ2λ −rµ2.
Equating the λ2 and the λ term with the characteristic polynomial yields
r = −41
3 ,
µ =
r
8
3(10 + R).
Equating the constant term gives us the equation
41
3
8
3(10 + Rc) = 160
3 (Rc −1)
which has the solution
Rc = 470
19 .
For this critical value of R the characteristic polynomial has the roots
λ1 = −41
3
λ2 = 4
19
√
2090
λ3 = −4
19
√
2090.
Solution 49.6
The form of the perturbation expansion is
v(θ) = 1 + A cos θ + ϵu(θ) + O(ϵ2)
θ = (1 + ϵω1 + O(ϵ2))φ.
2168

Writing the derivatives in terms of θ,
d
dφ = (1 + ϵω1 + · · · ) d
dθ
d2
dφ2 = (1 + 2ϵω1 + · · · ) d2
dθ2.
Substituting these expressions into the diﬀerential equation for v(φ),

1 + 2ϵω1 + O(ϵ2)
 
−A cos θ + ϵu′′ + O(ϵ2)

+ 1 + A cos θ + ϵu(θ) + O(ϵ2)
= 1 + ϵ

1 + 2A cos θ + A2 cos2 θ + O(ϵ)

ϵu′′ + ϵu −2ϵω1A cos θ = ϵ + 2ϵA cos θ + ϵA2 cos2 θ + O(ϵ2).
Equating the coeﬃcient of ϵ,
u′′ + u = 1 + 2ϵ(1 + ω1)A cos θ + 1
2A2(cos 2θ + 1)
u′′ + u = (1 + 1
2A2) + 2ϵ(1 + ω1)A cos θ + 1
2A2 cos 2θ.
To avoid secular terms, we must have ω1 = −1. A particular solution for u is
u = 1 + 1
2A2 −1
6A2 cos 2θ.
The the solution for v is
v(φ) = 1 + A cos((1 −ϵ)φ) + ϵ

1 + 1
2A2 −1
6A2 cos(2(1 −ϵ)φ)

+ O(ϵ2).
2169

Solution 49.7
Substituting the expressions for x and ω into the diﬀerential equations yields
a2

ω2
0
d2x2
dθ2 + x2

+ α cos2 θ

+ a3

ω2
0
d2x3
dθ2 + x3

−2ω0ω2 cos θ + 2αx2 cos θ

+ O(a4) = 0
Equating the coeﬃcient of a2 gives us the diﬀerential equation
d2x2
dθ2 + x2 = −α
2ω2
0
(1 + cos 2θ).
The solution subject to the initial conditions x2(0) = x′
2(0) = 0 is
x2 =
α
6ω2
0
(−3 + 2 cos θ + cos 2θ).
Equating the coeﬃcent of a3 gives us the diﬀerential equation
ω2
0
d2x3
dθ2 + x3

+ α2
3ω2
0
−

2ω0ω2 + 5α2
6ω2
0

cos θ + α2
3ω2
0
cos 2θ + α2
6ω2
0
cos 3θ = 0.
To avoid secular terms we must have
ω2 = −5α2
12ω0
.
Solving the diﬀerential equation for x3 subject to the intial conditions x3(0) = x′
3(0) = 0,
x3 =
α2
144ω4
0
(−48 + 29 cos θ + 16 cos 2θ + 3 cos 3θ).
Thus our solution for x(t) is
x(t) = a cos θ + a2
 α
6ω2
0
(−3 + 2 cos θ + cos 2θ)

+ a3

α2
144ω4
0
(−48 + 29 cos θ + 16 cos 2θ + 3 cos 3θ)

+ O(a4)
2170

where θ =

ω0 −a2 5α2
12ω0

t.
Now to see why we didn’t need an aω1 term. Assume that
x = a cos θ + a2x2(θ) + O(a3);
θ = ωt
ω = ω0 + aω1 + O(a2).
Substituting these expressions into the diﬀerential equation for x yields
a2 
ω2
0(x′′
2 + x2) −2ω0ω1 cos θ + α cos2 θ

= O(a3)
x′′
2 + x2 = 2ω1
ω0
cos θ −α
2ω2
0
(1 + cos 2θ).
In order to eliminate secular terms, we need ω1 = 0.
Solution 49.8
1. The equation for p1(t) is
dp1(t)
dt
= α[p0(t) −p1(t)].
dp1(t)
dt
= α[aeıωt −p1(t)]
d
dt
 eαtp1(t)

= αaeαteıωt
p1(t) =
αa
α + ıωeıωt + ce−αt
Applying the initial condition, p1(0) = 0,
p1(t) =
αa
α + ıω
 eıωt −e−αt
2171

2. We start with the diﬀerential equation for pn(t).
dpn(t)
dt
= α[pn−1(t) −pn(t)]
Multiply by sn and sum from n = 1 to ∞.
∞
X
n=1
p′
n(t)sn =
∞
X
n=1
α[pn−1(t) −pn(t)]sn
∂G(s, t)
∂t
= α
∞
X
n=0
pnsn+1 −αG(s, t)
∂G(s, t)
∂t
= αsp0 + α
∞
X
n=1
pnsn+1 −αG(s, t)
∂G(s, t)
∂t
= αaseıωt + αsG(s, t) −αG(s, t)
∂G(s, t)
∂t
= αaseıωt + α(s −1)G(s, t)
∂
∂t
 eα(1−s)tG(s, t)

= αaseα(1−s)teıωt
G(s, t) =
αas
α(1 −s) + ıωeıωt + C(s)eα(s−1)t
The initial condition is
G(s, 0) =
∞
X
n=1
pn(0)sn = 0.
The generating function is then
G(s, t) =
αas
α(1 −s) + ıω
 αeıωt −eα(s−1)t
.
2172

3. Assume that |s| < 1. In the limit t →∞we have
G(s, t) ∼
αas
α(1 −s) + ıωeıωt
G(s, t) ∼
as
1 + ıω/α −seıωt
G(s, t) ∼
as/(1 + ıω/α)
1 −s/(1 + ıω/α)eıωt
G(s, t) ∼
aseıωt
1 + ıω/α
∞
X
n=0

s
1 + ıω/α
n
G(s, t) ∼aeıωt
∞
X
n=1
sn
(1 + ıω/α)n
Thus we have
pn(t) ∼
a
(1 + ıω/α)neıωt
as t →∞.
ℑ(pn(t)) ∼ℑ

a
(1 + ıω/α)neıωt

= a
 1 −ıω/α
1 + (ω/α)2
n
[cos(ωt) + ı sin(ωt)]
=
a
(1 + (ω/α)2)n [cos(ωt)ℑ[(1 −ıω/α)n] + sin(ωt)ℜ[(1 −ıω/α)n]]
=
a
(1 + (ω/α)2)n

cos(ωt)
n
X
j=1
odd j
(−1)(j+1)/2 ω
α
j
+ sin(ωt)
n
X
j=0
even j
(−1)j/2 ω
α
j


2173

Solution 49.9
1. Substituting pn = Aneıωt into the diﬀerential equation yields
Anıωeıω(t+τ) = α[An−1eıωt −Aneıωt]
An(α + ıωeıωτ) = αAn−1
We make the substitution An = rn.
rn(α + ıωeıωτ) = αrn−1
r =
α
α + ıωeıωτ
Thus we have
pn(t) =

1
1 + ıωeıωτ/α
n
eıωt.
2174

Taking the imaginary part,
ℑ(pn(t)) = ℑ

1
1 + ı ω
αeıωτ
n
eıωt

= ℑ

1 −ı ω
αe−ıωτ
1 + ı ω
α(eıωτ −e−ıωτ) + (ω
α)2
n  cos(ωt) + ı sin(ωt)

= ℑ
1 −ω
α sin(ωτ) −ı ω
α cos(ωτ)
1 −2ω
α sin(ωτ) + (ω
α)2
n  cos(ωt) + ı sin(ωt)

=

1
1 −2ω
α sin(ωτ) + (ω
α)2
n h
cos(ωt)ℑ
h
1 −ω
α sin(ωτ) −ıω
α cos(ωτ)
ni
+ sin(ωt)ℜ
h
1 −ω
α sin(ωτ) −ıω
α cos(ωτ)
ni i
=

1
1 −2ω
α sin(ωτ) + (ω
α)2
n
h
cos(ωt)
n
X
j=1
odd j
(−1)(j+1)/2 hω
α cos(ωτ)
ij h
1 −ω
α sin(ωτ)
in−j
+ sin(ωt)
n
X
j=0
even j
(−1)j/2 hω
α cos(ωτ)
ij h
1 −ω
α sin(ωτ)
in−j i
2175

2. pn(t) will remain bounded in time as n →∞if

1
1 + ı ω
αeıωτ
 ≤1
1 + ıω
αeıωτ
2
≥1
1 −2ω
α sin(ωτ) +
ω
α
2
≥1
ω
α ≥2 sin(ωτ)
3.
2176

Chapter 50
Nonlinear Partial Diﬀerential Equations
2177

50.1
Exercises
Exercise 50.1
Consider the nonlinear PDE
ut + uux = 0.
The solution u is constant along lines (characteristics) such that x −ut = k for any constant k. Thus the slope of
these lines will depend on the initial data u(x, 0) = f(x).
1. In terms of this initial data, write down the equation for the characteristic in the x, t plane which goes through
the point (x, t) = (ξ, 0).
2. State a criteria on f such that two characteristics will intersect at some positive time t. Assuming intersections
do occur, what is the time of the ﬁrst intersection? You may assume that f is everywhere continuous and
diﬀerentiable.
3. Apply this to the case where f(x) = 1 −e−x2 to indicate where and when a shock will form and sketch (roughly)
the solution both before and after this time.
Exercise 50.2
Solve the equation
φt + (1 + x)φx + φ = 0
in
−∞< x < ∞, t > 0,
with initial condition φ(x, 0) = f(x).
Exercise 50.3
Solve the equation
φt + φx +
αφ
1 + x = 0
in the region 0 < x < ∞, t > 0 with initial condition φ(x, 0) = 0, and boundary condition φ(0, t) = g(t). [Here α is a
positive constant.]
2178

Exercise 50.4
Solve the equation
φt + φx + φ2 = 0
in −∞< x < ∞, t > 0 with initial condition φ(x, 0) = f(x). Note that the solution could become inﬁnite in ﬁnite
time.
Exercise 50.5
Consider
ct + ccx + µc = 0,
−∞< x < ∞, t > 0.
1. Use the method of characteristics to solve the problem with
c = F(x) at t = 0.
(µ is a positive constant.)
2. Find equations for the envelope of characteristics in the case F ′(x) < 0.
3. Deduce an inequality relating max |F ′(x)| and µ which decides whether breaking does or does not occur.
Exercise 50.6
For water waves in a channel the so-called shallow water equations are
ht + (hv)x = 0
(50.1)
(hv)t +

hv2 + 1
2gh2

x
= 0, g = constant.
(50.2)
Investigate whether there are solutions with v = V (h), where V (h) is not posed in advance but is obtained from
requiring consistency between the h equation obtained from (1) and the h equation obtained from (2).
There will be two possible choices for V (h) depending on a choice of sign. Consider each case separately. In
each case ﬁx the arbitrary constant that arises in V (h) by stipulating that before the waves arrive, h is equal to the
undisturbed depth h0 and V (h0) = 0.
Find the h equation and the wave speed c(h) in each case.
2179

Exercise 50.7
After a change of variables, the chemical exchange equations can be put in the form
∂ρ
∂t + ∂σ
∂x = 0
(50.3)
∂ρ
∂t = ασ −βρ −γρσ;
α, β, γ = positive constants.
(50.4)
1. Investigate wave solutions in which ρ = ρ(X), σ = σ(X), X = x −Ut, U = constant, and show that ρ(X)
must satisfy an ordinary diﬀerential equation of the form
dρ
dX = quadratic in ρ.
2. Discuss ths “smooth shock” solution as we did for a diﬀerent example in class. In particular ﬁnd the expression
for U in terms of the values of ρ as X →±∞, and ﬁnd the sign of dρ/dX. Check that
U = σ2 −σ1
ρ2 −ρ1
in agreement with the “discontinuous theory.”
Exercise 50.8
Find solitary wave solutions for the following equations:
1. ηt + ηx + 6ηηx −ηxxt = 0. (Regularized long wave or B.B.M. equation)
2. utt −uxx −
  3
2u2
xx −uxxxx = 0. (“Boussinesq”)
3. φtt −φxx + 2φxφxt + φxxφt −φxxxx = 0. (The solitary wave form is for u = φx)
4. ut + 30u2u1 + 20u1u2 + 10uu3 + u5 = 0. (Here the subscripts denote x derivatives.)
2180

50.2
Hints
Hint 50.1
Hint 50.2
Hint 50.3
Hint 50.4
Hint 50.5
Hint 50.6
Hint 50.7
Hint 50.8
2181

50.3
Solutions
Solution 50.1
1.
x = ξ + u(ξ, 0)t
x = ξ + f(ξ)t
2. Consider two points ξ1 and ξ2 where ξ1 < ξ2. Suppose that f(ξ1) > f(ξ2). Then the two characteristics passing
through the points (ξ1, 0) and (ξ2, 0) will intersect.
ξ1 + f(ξ1)t = ξ2 + f(ξ2)t
t =
ξ2 −ξ1
f(ξ1) −f(ξ2)
We see that the two characteristics intersect at the point
(x, t) =

ξ1 + f(ξ1)
ξ2 −ξ1
f(ξ1) −f(ξ2),
ξ2 −ξ1
f(ξ1) −f(ξ2)

.
We see that if f(x) is not a non-decreasing function, then there will be a positive time when characteristics
intersect.
Assume that f(x) is continuously diﬀerentiable and is not a non-decreasing function. That is, there are points
where f ′(x) is negative. We seek the time T of the ﬁrst intersection of characteristics.
T =
min
ξ1<ξ2
f(ξ1)>f(ξ2)
ξ2 −ξ1
f(ξ1) −f(ξ2)
(f(ξ2)−f(ξ1))/(ξ2−ξ1) is the slope of the secant line on f(x) that passes through the points ξ1 and ξ2. Thus we
seek the secant line on f(x) with the minimum slope. This occurs for the tangent line where f ′(x) is minimum.
T = −
1
minξ f ′(ξ)
2182

3. First we ﬁnd the time when the characteristics ﬁrst intersect. We ﬁnd the minima of f ′(x) with the derivative
test.
f(x) = 1 −e−x2
f ′(x) = 2x e−x2
f ′′(x) =
 2 −4x2 e−x2 = 0
x = ± 1
√
2
The minimum slope occurs at x = −1/
√
2.
T = −
1
−2 e−1/2 /
√
2 = e1/2
√
2 ≈1.16582
Figure 50.1 shows the solution at various times up to the ﬁrst collision of characteristics, when a shock forms.
After this time, the shock wave moves to the right.
Solution 50.2
The method of characteristics gives us the diﬀerential equations
x′(t) = (1 + x)
x(0) = ξ
dφ
dt = −φ
φ(ξ, 0) = f(ξ)
Solving the ﬁrst diﬀerential equation,
x(t) = cet −1,
x(0) = ξ
x(t) = (ξ + 1)et −1
The second diﬀerential equation then becomes
φ(x(t), t) = ce−t,
φ(ξ, 0) = f(ξ),
ξ = (x + 1)e−t −1
φ(x, t) = f((x + 1)e−t −1)e−t
2183

-3 -2-1
1 2 3
0.2
0.4
0.6
0.8
1
-3 -2-1
1 2 3
0.2
0.4
0.6
0.8
1
-3 -2-1
1 2 3
0.2
0.4
0.6
0.8
1
-3 -2-1
1 2 3
0.2
0.4
0.6
0.8
1
Figure 50.1: The solution at t = 0, 1/2, 1, 1.16582.
Thus the solution to the partial diﬀerential equation is
φ(x, t) = f((x + 1)e−t −1)e−t.
Solution 50.3
dφ
dt = φt + x′(t)φx = −αφ
1 + x
The characteristic curves x(t) satisfy x′(t) = 1, so x(t) = t + c. The characteristic curve that separates the region
with domain of dependence on the x axis and domain of dependence on the t axis is x(t) = t. Thus we consider the
two cases x > t and x < t.
• x > t. x(t) = t + ξ.
2184

• x < t. x(t) = t −τ.
Now we solve the diﬀerential equation for φ in the two domains.
• x > t.
dφ
dt = −αφ
1 + x,
φ(ξ, 0) = 0,
ξ = x −t
dφ
dt = −
αφ
1 + t + ξ
φ = c exp

−α
Z t
1
t + ξ + 1 dt

φ = cexp (−α log(t + ξ + 1))
φ = c(t + ξ + 1)−α
applying the initial condition, we see that
φ = 0
• x < t.
dφ
dt = −αφ
1 + x,
φ(0, τ) = g(τ),
τ = t −x
dφ
dt = −
αφ
1 + t −τ
φ = c(t + 1 −τ)−α
φ = g(τ)(t + 1 −τ)−α
φ = g(t −x)(x + 1)−α
2185

Thus the solution to the partial diﬀerential equation is
φ(x, t) =
(
0
for x > t
g(t −x)(x + 1)−α
for x < t.
Solution 50.4
The method of characteristics gives us the diﬀerential equations
x′(t) = 1
x(0) = ξ
dφ
dt = −φ2
φ(ξ, 0) = f(ξ)
Solving the ﬁrst diﬀerential equation,
x(t) = t + ξ.
The second diﬀerential equation is then
dφ
dt = −φ2,
φ(ξ, 0) = f(ξ),
ξ = x −t
φ−2dφ = −dt
−φ−1 = −t + c
φ =
1
t −c
φ =
1
t + 1/f(ξ)
φ =
1
t + 1/f(x −t).
Solution 50.5
1. Taking the total derivative of c with respect to t,
dc
dt = ct + dx
dt cx.
2186

Equating terms with the partial diﬀerential equation, we have the system of diﬀerential equations
dx
dt = c
dc
dt = −µc.
subject to the initial conditions
x(0) = ξ,
c(ξ, 0) = F(ξ).
We can solve the second ODE directly.
c(ξ, t) = c1e−µt
c(ξ, t) = F(ξ)e−µt
Substituting this result and solving the ﬁrst ODE,
dx
dt = F(ξ)e−µt
x(t) = −F(ξ)
µ
e−µt + c2
x(t) = F(ξ)
µ
(1 −e−µt) + ξ.
The solution to the problem at the point (x, t) is found by ﬁrst solving
x = F(ξ)
µ
(1 −e−µt) + ξ
for ξ and then using this value to compute
c(x, t) = F(ξ)e−µt.
2187

2. The characteristic lines are given by the equation
x(t) = F(ξ)
µ
(1 −e−µt) + ξ.
The points on the envelope of characteristics also satisfy
∂x(t)
∂ξ
= 0.
Thus the points on the envelope satisfy the system of equations
x = F(ξ)
µ
(1 −e−µt) + ξ
0 = F ′(ξ)
µ
(1 −e−µt) + 1.
By substituting
1 −e−µt = −
µ
F ′(ξ)
into the ﬁrst equation we can eliminate its t dependence.
x = −F(ξ)
F ′(ξ) + ξ
Now we can solve the second equation in the system for t.
e−µt = 1 +
µ
F ′(ξ)
t = −1
µ log

1 +
µ
F ′(ξ)

2188

Thus the equations that describe the envelope are
x = −F(ξ)
F ′(ξ) + ξ
t = −1
µ log

1 +
µ
F ′(ξ)

.
3. The second equation for the envelope has a solution for positive t if there is some x that satisﬁes
−1 <
µ
F ′(x) < 0.
This is equivalent to
−∞< F ′(x) < −µ.
So in the case that F ′(x) < 0, there will be breaking iﬀ
max |F ′(x)| > µ.
Solution 50.6
With the substitution v = V (h), the two equations become
ht + (V + hV ′)hx = 0
(V + hV ′)ht + (V 2 + 2hV V ′ + gh)hx = 0.
We can rewrite the second equation as
ht + V 2 + 2hV V ′ + gh
V + hV ′
hx = 0.
Requiring that the two equations be consistent gives us a diﬀerential equation for V .
V + hV ′ = V 2 + 2hV V ′ + gh
V + hV ′
V 2 + 2hV V ′ + h2(V ′)2 = V 2 + 2hV V ′ + gh
(V ′)2 = g
h.
2189

There are two choices depending on which sign we choose when taking the square root of the above equation.
Positive V′.
V ′ =
rg
h
V = 2
p
gh + const
We apply the initial condition V (h0) = 0.
V = 2√g(
√
h −
p
h0)
The partial diﬀerential equation for h is then
ht + (2√g(
√
h −
p
h0)h)x = 0
ht + √g(3
√
h −2
p
h0)hx = 0
The wave speed is
c(h) = √g(3
√
h −2
p
h0).
Negative V′.
V ′ = −
rg
h
V = −2
p
gh + const
We apply the initial condition V (h0) = 0.
V = 2√g(
p
h0 −
√
h)
2190

The partial diﬀerential equation for h is then
ht + √g(2
p
h0 −3
√
h)hx = 0.
The wave speed is
c(h) = √g(2
p
h0 −3
√
h).
Solution 50.7
1. Making the substitutions, ρ = ρ(X), σ = σ(X), X = x−Ut, the system of partial diﬀerential equations becomes
−Uρ′ + σ′ = 0
−Uρ′ = ασ −βρ −γρσ.
Integrating the ﬁrst equation yields
−Uρ + σ = c
σ = c + Uρ.
Now we substitute the expression for σ into the second partial diﬀerential equation.
−Uρ′ = α(c + Uρ) −βρ −γρ(c + Uρ)
ρ′ = −α

ρ + c
U

+ β
U ρ + γρ

ρ + c
U

Thus ρ(X) satisﬁes the ordinary diﬀerential equation
ρ′ = γρ2 +
γc
U + β
U −α

ρ −αc
U .
2191

2. Assume that
ρ(X) →ρ1 as X →+∞
ρ(X) →ρ2 as X →−∞
ρ′(X) →0 as X →±∞.
Integrating the ordinary diﬀerential equation for ρ,
X =
Z ρ
dρ
γρ2 +
  γc
U + β
U −α

ρ −αc
U
.
We see that the roots of the denominator of the integrand must be ρ1 and ρ2. Thus we can write the ordinary
diﬀerential equation for ρ(X) as
ρ′(X) = γ(ρ −ρ1)(ρ −ρ2) = γρ2 −γ(ρ1 + ρ2)ρ + γρ1ρ2.
Equating coeﬃcients in the polynomial with the diﬀerential equation for part 1, we obtain the two equations
−αc
U = γρ1ρ2,
γc
U + β
U −α = −γ(ρ1 + ρ2).
Solving the ﬁrst equation for c,
c = −Uγρ1ρ2
α
.
Now we substitute the expression for c into the second equation.
−γUγρ1ρ2
αU
+ β
U −α = −γ(ρ1 + ρ2)
β
U = α + γ2ρ1ρ2
α
−γ(ρ1 + ρ2)
Thus we see that U is
U =
αβ
α2 + γ2ρ1ρ2 −−αγ(ρ1 + ρ2).
2192

Since the quadratic polynomial in the ordinary diﬀerential equation for ρ(X) is convex, it is negative valued
between its two roots. Thus we see that
dρ
dX < 0.
Using the expression for σ that we obtained in part 1,
σ2 −σ1
ρ2 −ρ1
= c + Uρ2 −(c + Uρ1)
ρ2 −ρ1
= U ρ2 −ρ1
ρ2 −ρ1
= U.
Now let’s return to the ordinary diﬀerential equation for ρ(X)
ρ′(X) = γ(ρ −ρ1)(ρ −ρ2)
X =
Z ρ
dρ
γ(ρ −ρ1)(ρ −ρ2)
X = −
1
γ(ρ2 −ρ1)
Z ρ 
1
ρ −ρ1
+
1
ρ2 −ρ

dρ
X −X0 = −
1
γ(ρ2 −ρ1) ln
ρ −ρ1
ρ2 −ρ

−γ(ρ2 −ρ1)(X −X0) = ln
ρ −ρ1
ρ2 −ρ

ρ −ρ1
ρ2 −ρ = exp (−γ(ρ2 −ρ1)(X −X0))
ρ −ρ1 = (ρ2 −ρ) exp (−γ(ρ2 −ρ1)(X −X0))
ρ [1 + exp (−γ(ρ2 −ρ1)(X −X0))] = ρ1 + ρ2 exp (−γ(ρ2 −ρ1)(X −X0))
2193

Thus we obtain a closed form solution for ρ
ρ = ρ1 + ρ2 exp (−γ(ρ2 −ρ1)(X −X0))
1 + exp (−γ(ρ2 −ρ1)(X −X0))
Solution 50.8
1.
ηt + ηx + 6ηηx −ηxxt = 0
We make the substitution
η(x, t) = z(X),
X = x −Ut.
(1 −U)z′ + 6zz′ + Uz′′′ = 0
(1 −U)z + 3z2 + Uz′′ = 0
1
2(1 −U)z2 + z3 + 1
2U(z′)2 = 0
(z′)2 = U −1
U
z2 −2
U z3
z(X) = U −1
2
sech2
 
1
2
r
U −1
U
X
!
η(x, t) = U −1
2
sech2
 
1
2
 r
U −1
U
x −
p
(U −1)Ut
!!
The linearized equation is
ηt + ηx −ηxxt = 0.
Substituting η = e−αx+βt into this equation yields
β −α −α2β = 0
β =
α
1 −α2.
2194

We set
α2 = U −1
U
.
β is then
β =
α
1 −α2
=
p
(U −1)/U
1 −(U −1)/U)
=
p
(U −1)U
U −(U −1)
=
p
(U −1)U.
The solution for η becomes
αβ
2 sech2
αx −βt
2

where
β =
α
1 −α2.
2.
utt −uxx −
3
2u2

xx
−uxxxx = 0
We make the substitution
u(x, t) = z(X),
X = x −Ut.
2195

(U 2 −1)z′′ −
3
2z2
′′
−z′′′′ = 0
(U 2 −1)z′ −
3
2z2
′
−z′′′ = 0
(U 2 −1)z −3
2z2 −z′′ = 0
We multiply by z′ and integrate.
1
2(U 2 −1)z2 −1
2z3 −1
2(z′)2 = 0
(z′)2 = (U 2 −1)z2 −z3
z = (U 2 −1) sech2
1
2
√
U 2 −1X

u(x, t) = (U 2 −1) sech2
1
2
√
U 2 −1x −U
√
U 2 −1t

The linearized equation is
utt −uxx −uxxxx = 0.
Substituting u = e−αx+βt into this equation yields
β2 −α2 −α4 = 0
β2 = α2(α2 + 1).
We set
α =
√
U 2 −1.
2196

β is then
β2 = α2(α2 + 1)
= (U 2 −1)U 2
β = U
√
U 2 −1.
The solution for u becomes
u(x, t) = α2 sech2
αx −βt
2

where
β2 = α2(α2 + 1).
3.
φtt −φxx + 2φxφxt + φxxφt −φxxxx
We make the substitution
φ(x, t) = z(X),
X = x −Ut.
(U 2 −1)z′′ −2Uz′z′′ −Uz′′z′ −z′′′′ = 0
(U 2 −1)z′′ −3Uz′z′′ −z′′′′ = 0
(U 2 −1)z′ −3
2(z′)2 −z′′′ = 0
Multiply by z′′ and integrate.
1
2(U 2 −1)(z′)2 −1
2(z′)3 −1
2(z′′)2 = 0
(z′′)2 = (U 2 −1)(z′)2 −(z′)3
z′ = (U 2 −1) sech2
1
2
√
U 2 −1X

φx(x, t) = (U 2 −1) sech2
1
2
√
U 2 −1x −U
√
U 2 −1t

.
2197

The linearized equation is
φtt −φxx −φxxxx
Substituting φ = e−αx+βt into this equation yields
β2 = α2(α2 + 1).
The solution for φx becomes
φx = α2 sech2
αx −βt
2

where
β2 = α2(α2 + 1).
4.
ut + 30u2u1 + 20u1u2 + 10uu3 + u5 = 0
We make the substitution
u(x, t) = z(X),
X = x −Ut.
−Uz′ + 30z2z′ + 20z′z′′ + 10zz′′′ + z(5) = 0
Note that (zz′′)′ = z′z′′ + zz′′′.
−Uz′ + 30z2z′ + 10z′z′′ + 10(zz′′)′ + z(5) = 0
−Uz + 10z3 + 5(z′)2 + 10zz′′ + z(4) = 0
Multiply by z′ and integrate.
−1
2Uz2 + 5
2z4 + 5z(z′)2 −1
2(z′′)2 + z′z′′′ = 0
2198

Assume that
(z′)2 = P(z).
Diﬀerentiating this relation,
2z′z′′ = P ′(z)z′
z′′ = 1
2P ′(z)
z′′′ = 1
2P ′′(z)z′
z′′′z′ = 1
2P ′′(z)P(z).
Substituting this expressions into the diﬀerential equation for z,
−1
2Uz2 + 5
2z4 + 5zP(z) −1
2
1
4(P ′(z))2 + 1
2P ′′(z)P(z) = 0
4Uz2 + 20z4 + 40zP(z) −(P ′(z))2 + 4P ′′(z)P(z) = 0
Substituting P(z) = az3 + bz2 yields
(20 + 40a + 15a2)z4 + (40b + 20ab)z3 + (4b2 + 4U)z2 = 0
This equation is satisﬁed by b2 = U, a = −2. Thus we have
(z′)2 =
√
Uz2 −2z3
z =
√
U
2
sech2
1
2U 1/4X

u(x, t) =
√
U
2
sech2
1
2(U 1/4x −U 5/4t)

2199

The linearized equation is
ut + u5 = 0.
Substituting u = e−αx+βt into this equation yields
β −α5 = 0.
We set
α = U 1/4.
The solution for u(x, t) becomes
α2
2 sech2
αx −βt
2

where
β = α5.
2200

Part VIII
Appendices
2201

Appendix A
Greek Letters
The following table shows the greek letters, (some of them have two typeset variants), and their corresponding
Roman letters.
Name
Roman
Lower
Upper
alpha
a
α
beta
b
β
chi
c
χ
delta
d
δ
∆
epsilon
e
ϵ
epsilon (variant)
e
ε
phi
f
φ
Φ
phi (variant)
f
ϕ
gamma
g
γ
Γ
eta
h
η
iota
i
ι
kappa
k
κ
lambda
l
λ
Λ
mu
m
µ
2202

nu
n
ν
omicron
o
o
pi
p
π
Π
pi (variant)
p
ϖ
theta
q
θ
Θ
theta (variant)
q
ϑ
rho
r
ρ
rho (variant)
r
ϱ
sigma
s
σ
Σ
sigma (variant)
s
ς
tau
t
τ
upsilon
u
υ
Υ
omega
w
ω
Ω
xi
x
ξ
Ξ
psi
y
ψ
Ψ
zeta
z
ζ
2203

Appendix B
Notation
C
class of continuous functions
Cn
class of n-times continuously diﬀerentiable functions
C
set of complex numbers
δ(x)
Dirac delta function
F[·]
Fourier transform
Fc[·]
Fourier cosine transform
Fs[·]
Fourier sine transform
γ
Euler’s constant, γ =
R ∞
0 e−x Log x dx
Γ(ν)
Gamma function
H(x)
Heaviside function
H(1)
ν (x)
Hankel function of the ﬁrst kind and order ν
H(2)
ν (x)
Hankel function of the second kind and order ν
ı
ı ≡√−1
Jν(x)
Bessel function of the ﬁrst kind and order ν
Kν(x)
Modiﬁed Bessel function of the ﬁrst kind and order ν
L[·]
Laplace transform
2204

N
set of natural numbers, (positive integers)
Nν(x)
Modiﬁed Bessel function of the second kind and order ν
R
set of real numbers
R+
set of positive real numbers
R−
set of negative real numbers
o(z)
terms smaller than z
O(z)
terms no bigger than z
−
R
principal value of the integral
ψ(ν)
digamma function, ψ(ν) =
d
dν log Γ(ν)
ψ(n)(ν)
polygamma function, ψ(n)(ν) =
dn
dνnψ(ν)
u(n)(x)
∂nu
∂xn
u(n,m)(x, y)
∂n+mu
∂xn∂ym
Yν(x)
Bessel function of the second kind and order ν, Neumann function
Z
set of integers
Z+
set of positive integers
2205

Appendix C
Formulas from Complex Variables
Analytic Functions.
A function f(z) is analytic in a domain if the derivative f ′(z) exists in that domain.
If f(z) = u(x, y) + ıv(x, y) is deﬁned in some neighborhood of z0 = x0 + ıy0 and the partial derivatives of u and v
are continuous and satisfy the Cauchy-Riemann equations
ux = vy,
uy = −vx,
then f ′(z0) exists.
Residues.
If f(z) has the Laurent expansion
f(z) =
∞
X
n=−∞
anzn,
then the residue of f(z) at z = z0 is
Res(f(z), z0) = a−1.
2206

Residue Theorem.
Let C be a positively oriented, simple, closed contour. If f(z) is analytic in and on C except
for isolated singularities at z1, z2, . . . , zN inside C then
I
C
f(z) dz = ı2π
N
X
n=1
Res(f(z), zn).
If in addition f(z) is analytic outside C in the ﬁnite complex plane then
I
C
f(z) dz = ı2π Res
 1
z2f
1
z

, 0

.
Residues of a pole of order n.
If f(z) has a pole of order n at z = z0 then
Res(f(z), z0) = lim
z→z0

1
(n −1)!
dn−1
dzn−1 [(z −z0)nf(z)]

.
Jordan’s Lemma.
Z π
0
e−R sin θ dθ < π
R.
Let a be a positive constant. If f(z) vanishes as |z| →∞then the integral
Z
C
f(z) eıaz dz
along the semi-circle of radius R in the upper half plane vanishes as R →∞.
Taylor Series.
Let f(z) be a function that is analytic and single valued in the disk |z −z0| < R.
f(z) =
∞
X
n=0
f (n)(z0)
n!
(z −z0)n
The series converges for |z −z0| < R.
2207

Laurent Series.
Let f(z) be a function that is analytic and single valued in the annulus r < |z −z0| < R. In this
annulus f(z) has the convergent series,
f(z) =
∞
X
n=−∞
cn(z −z0)n,
where
cn =
1
ı2π
I
f(z)
(z −z0)n+1 dz
and the path of integration is any simple, closed, positive contour around z0 and lying in the annulus. The path of
integration is shown in Figure C.1.
C
Im(z)
Re(z)
R
r
Figure C.1: The Path of Integration.
2208

Appendix D
Table of Derivatives
Note: c denotes a constant and ′ denotes diﬀerentiation.
d
dx(fg) = df
dxg + f dg
dx
d
dx
f
g = f ′g −fg′
g2
d
dxf c = cf c−1f ′
d
dxf(g) = f ′(g)g′
d2
dx2f(g) = f ′′(g)(g′)2 + f ′g′′
dn
dxn(fg) =
n
0
dnf
dxn g +
n
1
dn−1f
dxn−1
dg
dx +
n
2
dn−2f
dxn−2
d2g
dx2 + · · · +
n
n

f dng
dxn
2209

d
dx ln x = 1
|x|
d
dxcx = cx ln c
d
dxf g = gf g−1df
dx + f g ln f dg
dx
d
dx sin x = cos x
d
dx cos x = −sin x
d
dx tan x = sec2 x
d
dx csc x = −csc x cot x
d
dx sec x = sec x tan x
d
dx cot x = −csc2 x
d
dx arcsin x =
1
√
1 −x2,
−π
2 ≤arcsin x ≤π
2
2210

d
dx arccos x = −
1
√
1 −x2,
0 ≤arccos x ≤π
d
dx arctan x =
1
1 + x2,
−π
2 ≤arctan x ≤π
2
d
dx sinh x = cosh x
d
dx cosh x = sinh x
d
dx tanh x = sech2 x
d
dx csch x = −csch x coth x
d
dx sech x = −sech x tanh x
d
dx coth x = −csch2 x
d
dx arcsinh x =
1
√
x2 + 1
d
dx arccosh x =
1
√
x2 −1,
x > 1, arccosh x > 0
d
dx arctanh x =
1
1 −x2,
x2 < 1
2211

d
dx
Z x
c
f(ξ) dξ = f(x)
d
dx
Z c
x
f(ξ) dξ = −f(x)
d
dx
Z h
g
f(ξ, x) dξ =
Z h
g
∂f(ξ, x)
∂x
dξ + f(h, x)h′ −f(g, x)g′
2212

Appendix E
Table of Integrals
Z
udv
dx dx = uv −
Z
vdu
dx dx
Z f ′(x)
f(x) dx = log f(x)
Z
f ′(x)
2
p
f(x)
dx =
p
f(x)
Z
xα dx = xα+1
α + 1
for α ̸== −1
Z 1
x dx = log x
Z
eax dx = eax
a
2213

Z
abx dx =
abx
b log a
for a > 0
Z
log x dx = x log x −x
Z
1
x2 + a2 dx = 1
a arctan x
a
Z
1
x2 −a2 dx =
(
1
2a log a−x
a+x
for x2 < a2
1
2a log x−a
x+a
for x2 > a2
Z
1
√
a2 −x2 dx = arcsin x
|a| = −arccos x
|a|
for x2 < a2
Z
1
√
x2 ± a2 dx = log(x +
√
x2 ± a2)
Z
1
x
√
x2 −a2 dx = 1
|a| sec−1 x
a
Z
1
x
√
a2 ± x2 dx = −1
a log
a +
√
a2 ± x2
x

Z
sin(ax) dx = −1
a cos(ax)
Z
cos(ax) dx = 1
a sin(ax)
2214

Z
tan(ax) dx = −1
a log cos(ax)
Z
csc(ax) dx = 1
a log tan ax
2
Z
sec(ax) dx = 1
a log tan
π
4 + ax
2

Z
cot(ax) dx = 1
a log sin(ax)
Z
sinh(ax) dx = 1
a cosh(ax)
Z
cosh(ax) dx = 1
a sinh(ax)
Z
tanh(ax) dx = 1
a log cosh(ax)
Z
csch(ax) dx = 1
a log tanh ax
2
Z
sech(ax) dx = i
a log tanh
iπ
4 + ax
2

Z
coth(ax) dx = 1
a log sinh(ax)
2215

Z
x sin ax dx = 1
a2 sin ax −x
a cos ax
Z
x2 sin ax dx = 2x
a2 sin ax −a2x2 −2
a3
cos ax
Z
x cos ax dx = 1
a2 cos ax + x
a sin ax
Z
x2 cos ax dx = 2x cos ax
a2
+ a2x2 −2
a3
sin ax
2216

Appendix F
Deﬁnite Integrals
Integrals from −∞to ∞.
Let f(z) be analytic except for isolated singularities, none of which lie on the real axis.
Let a1, . . . , am be the singularities of f(z) in the upper half plane; and CR be the semi-circle from R to −R in the
upper half plane. If
lim
R→∞

R max
z∈CR |f(z)|

= 0
then
Z ∞
−∞
f(x) dx = ı2π
m
X
j=1
Res (f(z), aj) .
Let b1, . . . , bn be the singularities of f(z) in the lower half plane. Let CR be the semi-circle from R to −R in the lower
half plane. If
lim
R→∞

R max
z∈CR |f(z)|

= 0
then
Z ∞
−∞
f(x) dx = −ı2π
n
X
j=1
Res (f(z), bj) .
2217

Integrals from 0 to ∞.
Let f(z) be analytic except for isolated singularities, none of which lie on the positive real
axis, [0, ∞). Let z1, . . . , zn be the singularities of f(z). If f(z) ≪zα as z →0 for some α > −1 and f(z) ≪zβ as
z →∞for some β < −1 then
Z ∞
0
f(x) dx = −
n
X
k=1
Res (f(z) log z, zk) .
Z ∞
0
f(x) log dx = −1
2
n
X
k=1
Res
 f(z) log2 z, zk

+ ıπ
n
X
k=1
Res (f(z) log z, zk)
Assume that a is not an integer. If zaf(z) ≪zα as z →0 for some α > −1 and zaf(z) ≪zβ as z →∞for some
β < −1 then
Z ∞
0
xaf(x) dx =
ı2π
1 −eı2πa
n
X
k=1
Res (zaf(z), zk) .
Z ∞
0
xaf(x) log x dx =
ı2π
1 −eı2πa
n
X
k=1
Res (zaf(z) log z, zk) , +
π2a
sin2(πa)
n
X
k=1
Res (zaf(z), zk)
Fourier Integrals.
Let f(z) be analytic except for isolated singularities, none of which lie on the real axis. Suppose
that f(z) vanishes as |z| →∞. If ω is a positive real number then
Z ∞
−∞
f(x) eıωx dx = ı2π
n
X
k=1
Res(f(z) eıωz, zk),
where z1, . . . , zn are the singularities of f(z) in the upper half plane. If ω is a negative real number then
Z ∞
−∞
f(x) eıωx dx = −ı2π
n
X
k=1
Res(f(z) eıωz, zk),
where z1, . . . , zn are the singularities of f(z) in the lower half plane.
2218

Appendix G
Table of Sums
∞
X
n=1
rn =
r
1 −r,
for |r| < 1
N
X
n=1
rn = r −rN+1
1 −r
b
X
n=a
n = (a + b)(b + 1 −a)
2
N
X
n=1
n = N(N + 1)
2
b
X
n=a
n2 = b(b + 1)(2b + 1) −a(a −1)(2a −1)
6
2219

N
X
n=1
n2 = N(N + 1)(2N + 1)
6
∞
X
n=1
(−1)n+1
n
= log(2)
∞
X
n=1
1
n2 = π2
6
∞
X
n=1
(−1)n+1
n2
= π2
12
∞
X
n=1
1
n3 = ζ(3)
∞
X
n=1
(−1)n+1
n3
= 3ζ(3)
4
∞
X
n=1
1
n4 = π4
90
∞
X
n=1
(−1)n+1
n4
= 7π4
720
2220

∞
X
n=1
1
n5 = ζ(5)
∞
X
n=1
(−1)n+1
n5
= 15ζ(5)
16
∞
X
n=1
1
n6 = π6
945
∞
X
n=1
(−1)n+1
n6
= 31π6
30240
2221

Appendix H
Table of Taylor Series
(1 −z)−1 =
∞
X
n=0
zn
|z| < 1
(1 −z)−2 =
∞
X
n=0
(n + 1)zn
|z| < 1
(1 + z)α =
∞
X
n=0
α
n

zn
|z| < 1
ez =
∞
X
n=0
zn
n!
|z| < ∞
log(1 −z) = −
∞
X
n=1
zn
n
|z| < 1
2222

log
1 + z
1 −z

= 2
∞
X
n=1
z2n−1
2n −1
|z| < 1
cos z =
∞
X
n=0
(−1)nz2n
(2n)!
|z| < ∞
sin z =
∞
X
n=0
(−1)nz2n+1
(2n + 1)!
|z| < ∞
tan z = z + z3
3 + 2z5
15 + 17z7
315 + · · ·
|z| < π
2
cos−1 z = π
2 −

z + z3
2 · 3 + 1 · 3z5
2 · 4 · 5 + 1 · 3 · 5z7
2 · 4 · 6 · 7 + · · ·

|z| < 1
sin−1 z = z + z3
2 · 3 + 1 · 3z5
2 · 4 · 5 + 1 · 3 · 5z7
2 · 4 · 6 · 7 + · · ·
|z| < 1
tan−1 z =
∞
X
n=1
(−1)n+1z2n−1
2n −1
|z| < 1
cosh z =
∞
X
n=0
z2n
(2n)!
|z| < ∞
sinh z =
∞
X
n=0
z2n+1
(2n + 1)!
|z| < ∞
2223

tanh z = z −z3
3 + 2z5
15 −17z7
315 + · · ·
|z| < π
2
Jν(z) =
∞
X
n=0
(−1)n
n!Γ(ν + n + 1)
z
2
ν+2n
|z| < ∞
Iν(z) =
∞
X
n=0
1
n!Γ(ν + n + 1)
z
2
ν+2n
|z| < ∞
2224

Appendix I
Table of Laplace Transforms
I.1
Properties of Laplace Transforms
Let f(t) be piecewise continuous and of exponential order α. Unless otherwise noted, the transform is deﬁned for s > 0.
To reduce clutter, it is understood that the Heaviside function H(t) multiplies the original function in the following two
tables.
f(t)
Z ∞
0
e−st f(t) dt
1
ı2π
Z c+ı∞
c−ı∞
ets ˆf(s) ds
ˆf(s)
af(t) + bg(t)
a ˆf(s) + bˆg(s)
d
dtf(t)
s ˆf(s) −f(0)
2225

d2
dt2f(t)
s2 ˆf(s) −sf(0) −f ′(0)
dn
dtnf(t)
sn ˆf(s) −sn−1f(0)
−sn−2f ′(0) −· · · −f (n−1)(0)
Z t
0
f(τ) dτ
ˆf(s)
s
Z t
0
Z τ
0
f(s) ds dτ
ˆf(s)
s2
ect f(t)
ˆf(s −c)
s > c + α
1
cf
t
c

,
c > 0
ˆf(cs)
1
c e(b/c)t f
t
c

,
c > 0
ˆf(cs −b)
f(t −c)H(t −c),
c > 0
e−cs ˆf(s)
tf(t)
−d
ds
ˆf(s)
tnf(t)
(−1)n dn
dsn ˆf(s)
f(t)
t ,
Z 1
0
f(t)
t
dt exists
Z ∞
s
ˆf(t) dt
2226

Z t
0
f(τ)g(t −τ) dτ,
f, g ∈C0
ˆf(s)ˆg(s)
f(t),
f(t + T) = f(t)
R T
0 e−st f(t) dt
1 −e−sT
f(t),
f(t + T) = −f(t)
R T
0 e−st f(t) dt
1 + e−sT
I.2
Table of Laplace Transforms
f(t)
Z ∞
0
e−st f(t) dt
1
ı2π
Z c+ı∞
c−ı∞
ets ˆf(s) ds
ˆf(s)
1
1
s
t
1
s2
tn, for n = 0, 1, 2, . . .
n!
sn+1
t1/2
√π
2 s−3/2
2227

t−1/2
√πs−1/2
tn−1/2,
n ∈Z+
(1)(3)(5) · · · (2n −1)√π
2n
s−n−1/2
tν,
ℜ(ν) > −1
Γ(ν + 1)
sν+1
Log t
−γ −Log s
s
tν Log t,
ℜ(ν) > −1
Γ(ν + 1)
sn+1
(ψ(ν + 1) −Log s)
δ(t)
1
s > 0
δ(n)(t),
n ∈Z0+
sn
s > 0
ect
1
s −c
s > c
t ect
1
(s −c)2
s > c
tn−1 ect
(n −1)!, n ∈Z+
1
(s −c)n
s > c
sin(ct)
c
s2 + c2
cos(ct)
s
s2 + c2
2228

sinh(ct)
c
s2 −c2
s > |c|
cosh(ct)
s
s2 −c2
s > |c|
t sin(ct)
2cs
(s2 + c2)2
t cos(ct)
s2 −c2
(s2 + c2)2
tn ect,
n ∈Z+
n!
(s −c)n+1
edt sin(ct)
c
(s −d)2 + c2
s > d
edt cos(ct)
s −d
(s −d)2 + c2
s > d
δ(t −c)
(
0
for c < 0
e−sc
for c > 0
H(t −c) =
(
0
for t < c
1
for t > c
1
s e−cs
Jν(ct)
cn
√
s2 + c2  s +
√
s2 + c2ν
ν > −1
2229

Iν(ct)
cn
√
s2 −c2  s −
√
s2 + c2ν
ℜ(s) > c, ν > −1
2230

Appendix J
Table of Fourier Transforms
f(x)
1
2π
Z ∞
−∞
f(x) e−ıωx dx
Z ∞
−∞
F(ω) eıωx dω
F(ω)
af(x) + bg(x)
aF(ω) + bG(ω)
f (n)(x)
(ıω)nF(ω)
xnf(x)
ınF (n)(ω)
f(x + c)
eıωc F(ω)
e−ıcx f(x)
F(ω + c)
f(cx)
|c|−1F(ω/c)
2231

f(x)g(x)
F ∗G(ω) =
Z ∞
−∞
F(η)G(ω −η) dη
1
2πf ∗g(x) = 1
2π
Z ∞
−∞
f(ξ)g(x −ξ) dξ
F(ω)G(ω)
e−cx2,
c > 0
1
√
4πc
e−ω2/4c
e−c|x|,
c > 0
c/π
ω2 + c2
2c
x2 + c2,
c > 0
e−c|ω|
1
x −ıα,
α > 0
(
0
for ω > 0
ı eαω
for ω < 0
1
x −ıα,
α < 0
(
ı eαω
for ω > 0
0
for ω < 0
1
x
−ı
2 sign(ω)
H(x −c) =
(
0
for x < c
1
for x > c
1
ı2πω e−ıcω
2232

e−cx H(x),
ℜ(c) > 0
1
2π(c + ıω)
ecx H(−x),
ℜ(c) > 0
1
2π(c −ıω)
1
δ(ω)
δ(x −ξ)
1
2π e−ıωξ
π(δ(x + ξ) + δ(x −ξ))
cos(ωξ)
−ıπ(δ(x + ξ) −δ(x −ξ))
sin(ωξ)
H(c −|x|) =
(
1
for |x| < c
0
for |x| > c, c > 0
sin(cω)
πω
2233

Appendix K
Table of Fourier Transforms in n Dimensions
f(x)
1
(2π)n
Z
Rnf(x) e−ıωx dx
Z
RnF(ω) eıωx dω
F(ω)
af(x) + bg(x)
aF(ω) + bG(ω)
π
c
n/2
e−nx2/4c
e−cω2
2234

Appendix L
Table of Fourier Cosine Transforms
f(x)
1
π
Z ∞
0
f(x) cos (ωx) dx
2
Z ∞
0
C(ω) cos (ωx) dω
C(ω)
f ′(x)
ωS(ω) −1
πf(0)
f ′′(x)
−ω2C(ω) −1
πf ′(0)
xf(x)
∂
∂ωFs[f(x)]
f(cx),
c > 0
1
cC
ω
c

2235

2c
x2 + c2
e−cω
e−cx
c/π
ω2 + c2
e−cx2
1
√
4πc
e−ω2/(4c)
rπ
c e−x2/(4c)
e−cω2
2236

Appendix M
Table of Fourier Sine Transforms
f(x)
1
π
Z ∞
0
f(x) sin (ωx) dx
2
Z ∞
0
S(ω) sin (ωx) dω
S(ω)
f ′(x)
−ωC(ω)
f ′′(x)
−ω2S(ω) + 1
πωf(0)
xf(x)
−∂
∂ωFc[f(x)]
f(cx),
c > 0
1
cS
ω
c

2237

2x
x2 + c2
e−cω
e−cx
ω/π
ω2 + c2
2 arctan
x
c

1
ω e−cω
1
x e−cx
1
π arctan
ω
c

1
1
πω
2
x
1
x e−cx2
ω
4c3/2√π e−ω2/(4c)
√πx
2c3/2 e−x2/(4c)
ω e−cω2
2238

Appendix N
Table of Wronskians
W [x −a, x −b]
b −a
W
eax, ebx
(b −a) e(a+b)x
W [cos(ax), sin(ax)]
a
W [cosh(ax), sinh(ax)]
a
W [eax cos(bx), eax sin(bx)]
b e2ax
W [eax cosh(bx), eax sinh(bx)]
b e2ax
W [sin(c(x −a)), sin(c(x −b))]
c sin(c(b −a))
W [cos(c(x −a)), cos(c(x −b))]
c sin(c(b −a))
W [sin(c(x −a)), cos(c(x −b))]
−c cos(c(b −a))
2239

W [sinh(c(x −a)), sinh(c(x −b))]
c sinh(c(b −a))
W [cosh(c(x −a)), cosh(c(x −b))]
c cosh(c(b −a))
W [sinh(c(x −a)), cosh(c(x −b))]
−c cosh(c(b −a))
W
edx sin(c(x −a)), edx sin(c(x −b))

c e2dx sin(c(b −a))
W
edx cos(c(x −a)), edx cos(c(x −b))

c e2dx sin(c(b −a))
W
edx sin(c(x −a)), edx cos(c(x −b))

−c e2dx cos(c(b −a))
W
edx sinh(c(x −a)), edx sinh(c(x −b))

c e2dx sinh(c(b −a))
W
edx cosh(c(x −a)), edx cosh(c(x −b))

−c e2dx sinh(c(b −a))
W
edx sinh(c(x −a)), edx cosh(c(x −b))

−c e2dx cosh(c(b −a))
W [(x −a) ecx, (x −b) ecx]
(b −a) e2cx
2240

Appendix O
Sturm-Liouville Eigenvalue Problems
• y′′ + λ2y = 0, y(a) = y(b) = 0
λn =
nπ
b −a,
yn = sin
nπ(x −a)
b −a

,
n ∈N
⟨yn, yn⟩= b −a
2
• y′′ + λ2y = 0, y(a) = y′(b) = 0
λn = (2n −1)π
2(b −a) ,
yn = sin
(2n −1)π(x −a)
2(b −a)

,
n ∈N
⟨yn, yn⟩= b −a
2
• y′′ + λ2y = 0, y′(a) = y(b) = 0
λn = (2n −1)π
2(b −a) ,
yn = cos
(2n −1)π(x −a)
2(b −a)

,
n ∈N
2241

⟨yn, yn⟩= b −a
2
• y′′ + λ2y = 0, y′(a) = y′(b) = 0
λn =
nπ
b −a,
yn = cos
nπ(x −a)
b −a

,
n = 0, 1, 2, . . .
⟨y0, y0⟩= b −a,
⟨yn, yn⟩= b −a
2
for n ∈N
2242

Appendix P
Green Functions for Ordinary Diﬀerential
Equations
• G′ + p(x)G = δ(x −ξ), G(ξ−: ξ) = 0
G(x|ξ) = exp

−
Z x
ξ
p(t) dt

H(x −ξ)
• y′′ = 0, y(a) = y(b) = 0
G(x|ξ) = (x< −a)(x> −b)
b −a
• y′′ = 0, y(a) = y′(b) = 0
G(x|ξ) = a −x<
• y′′ = 0, y′(a) = y(b) = 0
G(x|ξ) = x> −b
2243

• y′′ −c2y = 0, y(a) = y(b) = 0
G(x|ξ) = sinh(c(x< −a)) sinh(c(x> −b))
c sinh(c(b −a))
• y′′ −c2y = 0, y(a) = y′(b) = 0
G(x|ξ) = −sinh(c(x< −a)) cosh(c(x> −b))
c cosh(c(b −a))
• y′′ −c2y = 0, y′(a) = y(b) = 0
G(x|ξ) = cosh(c(x< −a)) sinh(c(x> −b))
c cosh(c(b −a))
• y′′ + c2y = 0, y(a) = y(b) = 0, c ̸= npi
b−a, n ∈N
G(x|ξ) = sin(c(x< −a)) sin(c(x> −b))
c sin(c(b −a))
• y′′ + c2y = 0, y(a) = y′(b) = 0, c ̸= (2n−1)pi
2(b−a) , n ∈N
G(x|ξ) = −sin(c(x< −a)) cos(c(x> −b))
c cos(c(b −a))
• y′′ + c2y = 0, y′(a) = y(b) = 0, c ̸= (2n−1)pi
2(b−a) , n ∈N
G(x|ξ) = cos(c(x< −a)) sin(c(x> −b))
c cos(c(b −a))
2244

• y′′ + 2cy′ + dy = 0, y(a) = y(b) = 0, c2 > d
G(x|ξ) = e−cx< sinh(
√
c2 −d(x< −a)) e−cx< sinh(
√
c2 −d(x> −b))
√
c2 −d e−2cξ sinh(
√
c2 −d(b −a))
• y′′ + 2cy′ + dy = 0, y(a) = y(b) = 0, c2 < d,
√
d −c2 ̸=
nπ
b−a, n ∈N
G(x|ξ) = e−cx< sin(
√
d −c2(x< −a)) e−cx< sin(
√
d −c2(x> −b))
√
d −c2 e−2cξ sin(
√
d −c2(b −a))
• y′′ + 2cy′ + dy = 0, y(a) = y(b) = 0, c2 = d
G(x|ξ) = (x< −a) e−cx<(x> −b) e−cx<
(b −a) e−2cξ
2245

Appendix Q
Trigonometric Identities
Q.1
Circular Functions
Pythagorean Identities
sin2 x + cos2 x = 1,
1 + tan2 x = sec2 x,
1 + cot2 x = csc2 x
Angle Sum and Diﬀerence Identities
sin(x + y) = sin x cos y + cos x sin y
sin(x −y) = sin x cos y −cos x sin y
cos(x + y) = cos x cos y −sin x sin y
cos(x −y) = cos x cos y + sin x sin y
2246

Function Sum and Diﬀerence Identities
sin x + sin y = 2 sin 1
2(x + y) cos 1
2(x −y)
sin x −sin y = 2 cos 1
2(x + y) sin 1
2(x −y)
cos x + cos y = 2 cos 1
2(x + y) cos 1
2(x −y)
cos x −cos y = −2 sin 1
2(x + y) sin 1
2(x −y)
Double Angle Identities
sin 2x = 2 sin x cos x,
cos 2x = cos2 x −sin2 x
Half Angle Identities
sin2 x
2 = 1 −cos x
2
,
cos2 x
2 = 1 + cos x
2
Function Product Identities
sin x sin y = 1
2 cos(x −y) −1
2 cos(x + y)
cos x cos y = 1
2 cos(x −y) + 1
2 cos(x + y)
sin x cos y = 1
2 sin(x + y) + 1
2 sin(x −y)
cos x sin y = 1
2 sin(x + y) −1
2 sin(x −y)
Exponential Identities
eıx = cos x + ı sin x,
sin x = eıx −e−ıx
ı2
,
cos x = eıx + e−ıx
2
2247

Q.2
Hyperbolic Functions
Exponential Identities
sinh x = ex −e−x
2
,
cosh x = ex + e−x
2
tanh x = sinh x
cosh x = ex −e−x
ex + e−x
Reciprocal Identities
csch x =
1
sinh x,
sech x =
1
cosh x,
coth x =
1
tanh x
Pythagorean Identities
cosh2 x −sinh2 x = 1,
tanh2 x + sech2 x = 1
Relation to Circular Functions
sinh(ıx) = ı sin x
sinh x = −ı sin(ıx)
cosh(ıx) = cos x
cosh x = cos(ıx)
tanh(ıx) = ı tan x
tanh x = −ı tan(ıx)
Angle Sum and Diﬀerence Identities
sinh(x ± y) = sinh x cosh y ± cosh x sinh y
cosh(x ± y) = cosh x cosh y ± sinh x sinh y
tanh(x ± y) = tanh x ± tanh y
1 ± tanh x tanh y = sinh 2x ± sinh 2y
cosh 2x ± cosh 2y
coth(x ± y) = 1 ± coth x coth y
coth x ± coth y = sinh 2x ∓sinh 2y
cosh 2x −cosh 2y
2248

Function Sum and Diﬀerence Identities
sinh x ± sinh y = 2 sinh 1
2(x ± y) cosh 1
2(x ∓y)
cosh x + cosh y = 2 cosh 1
2(x + y) cosh 1
2(x −y)
cosh x −cosh y = 2 sinh 1
2(x + y) sinh 1
2(x −y)
tanh x ± tanh y = sinh(x ± y)
cosh x cosh y
coth x ± coth y = sinh(x ± y)
sinh x sinh y
Double Angle Identities
sinh 2x = 2 sinh x cosh x,
cosh 2x = cosh2 x + sinh2 x
Half Angle Identities
sinh2 x
2 = cosh x −1
2
,
cosh2 x
2 = cosh x + 1
2
Function Product Identities
sinh x sinh y = 1
2 cosh(x + y) −1
2 cosh(x −y)
cosh x cosh y = 1
2 cosh(x + y) + 1
2 cosh(x −y)
sinh x cosh y = 1
2 sinh(x + y) + 1
2 sinh(x −y)
See Figure Q.1 for plots of the hyperbolic circular functions.
2249

-2
-1
1
2
-3
-2
-1
1
2
3
-2
-1
1
2
-1
-0.5
0.5
1
Figure Q.1: cosh x, sinh x and then tanh x
2250

Appendix R
Bessel Functions
R.1
Deﬁnite Integrals
Let ν > −1.
Z 1
0
rJν(jν,mr)Jν(jν,nr) dr = 1
2 (J′
ν(jν,n))2 δmn
Z 1
0
rJν(j′
ν,mr)Jν(j′
ν,nr) dr = j′2
ν,n −ν2
2j′2
ν,n
 Jν(j′
ν,n)
2 δmn
Z 1
0
rJν(αmr)Jν(αnr) dr =
1
2α2
n
a2
b2 + α2
n −ν2

(Jν(αn))2 δmn
Here αn is the nth positive root of aJν(r) + brJ′ν(r), where a, b ∈R.
2251

Appendix S
Formulas from Linear Algebra
Kramer’s Rule.
Consider the matrix equation
A⃗x = ⃗b.
This equation has a unique solution if and only if det(A) ̸= 0. If the determinant vanishes then there are either no
solutions or an inﬁnite number of solutions. If the determinant is nonzero, the solution for each xj can be written
xj = det Aj
det A
where Aj is the matrix formed by replacing the jth column of A with b.
Example S.0.1 The matrix equation
1
2
3
4
 x1
x2

=
5
6

,
has the solution
x1 =

5
2
6
4


1
2
3
4

= 8
−2 = −4,
x2 =

1
5
3
6


1
2
3
4

= −9
−2 = 9
2.
2252

Appendix T
Vector Analysis
Rectangular Coordinates
f = f(x, y, z),
⃗g = gxi + gyj + gzk
∇f = ∂f
∂xi + ∂f
∂y j + ∂f
∂z k
∇· ⃗g = ∂gx
∂x + ∂gy
∂y + ∂gz
∂z
∇× ⃗g =

i
j
k
∂
∂x
∂
∂y
∂
∂z
gx
gy
gz

∆f = ∇2f = ∂2f
∂x2 + ∂2f
∂y2 + ∂2f
∂z2
2253

Spherical Coordinates
x = r cos θ sin φ,
y = r sin θ sin φ,
z = r cos φ
f = f(r, θ, φ),
⃗g = grr + gθθ + gφφ
Divergence Theorem.
ZZ
∇· u dx dy =
I
u · n ds
Stoke’s Theorem.
ZZ
(∇× u) · ds =
I
u · dr
2254

Appendix U
Partial Fractions
A proper rational function
p(x)
q(x) =
p(x)
(x −a)nr(x)
Can be written in the form
p(x)
(x −α)nr(x) =

a0
(x −α)n +
a1
(x −α)n−1 + · · · + an−1
x −α

+ (· · · )
where the ak’s are constants and the last ellipses represents the partial fractions expansion of the roots of r(x). The
coeﬃcients are
ak = 1
k!
dk
dxk
p(x)
r(x)
 
x=α
.
Example U.0.2 Consider the partial fraction expansion of
1 + x + x2
(x −1)3 .
2255

The expansion has the form
a0
(x −1)3 +
a1
(x −1)2 +
a2
x −1.
The coeﬃcients are
a0 = 1
0!(1 + x + x2)|x=1 = 3,
a1 = 1
1!
d
dx(1 + x + x2)|x=1 = (1 + 2x)|x=1 = 3,
a2 = 1
2!
d2
dx2(1 + x + x2)|x=1 = 1
2(2)|x=1 = 1.
Thus we have
1 + x + x2
(x −1)3
=
3
(x −1)3 +
3
(x −1)2 +
1
x −1.
Example U.0.3 Consider the partial fraction expansion of
1 + x + x2
x2(x −1)2 .
The expansion has the form
a0
x2 + a1
x +
b0
(x −1)2 +
b1
x −1.
2256

The coeﬃcients are
a0 = 1
0!
1 + x + x2
(x −1)2
 
x=0
= 1,
a1 = 1
1!
d
dx
1 + x + x2
(x −1)2
 
x=0
=
 1 + 2x
(x −1)2 −2(1 + x + x2)
(x −1)3
 
x=0
= 3,
b0 = 1
0!
1 + x + x2
x2
 
x=1
= 3,
b1 = 1
1!
d
dx
1 + x + x2
x2
 
x=1
=
1 + 2x
x2
−2(1 + x + x2)
x3
 
x=1
= −3,
Thus we have
1 + x + x2
x2(x −1)2 = 1
x2 + 3
x +
3
(x −1)2 −
3
x −1.
If the rational function has real coeﬃcients and the denominator has complex roots, then you can reduce the work
in ﬁnding the partial fraction expansion with the following trick: Let α and α be complex conjugate pairs of roots of
the denominator.
p(x)
(x −α)n(x −α)nr(x) =

a0
(x −α)n +
a1
(x −α)n−1 + · · · + an−1
x −α

+

a0
(x −α)n +
a1
(x −α)n−1 + · · · + an−1
x −α

+ (· · · )
Thus we don’t have to calculate the coeﬃcients for the root at α. We just take the complex conjugate of the coeﬃcients
for α.
Example U.0.4 Consider the partial fraction expansion of
1 + x
x2 + 1.
2257

The expansion has the form
a0
x −i +
a0
x + i
The coeﬃcients are
a0 = 1
0!
1 + x
x + i
 
x=i
= 1
2(1 −i),
a0 = 1
2(1 −i) = 1
2(1 + i)
Thus we have
1 + x
x2 + 1 =
1 −i
2(x −i) +
1 + i
2(x + i).
2258

Appendix V
Finite Math
Newton’s Binomial Formula.
(a + b)n =
n
X
k=0
k
n

an−kbk
= an + nan−1b + n(n −1)
2
an−2b2 + · · · + nabn−1 + bn,
The binomial coeﬃcients are,
k
n

=
n!
k!(n −k)!.
2259

Appendix W
Probability
W.1
Independent Events
Once upon a time I was talking with the father of one of my colleagues at Caltech. He was an educated man. I think
that he had studied Russian literature and language back when he was in college. We were discussing gambling. He
told me that he had a scheme for winning money at the game of 21. I was familiar with counting cards. Being a
mathematician, I was not interested in hearing about conditional probability from a literature major, but I said nothing
and prepared to hear about his particular technique. I was quite surprised with his “method”: He said that when he
was on a winning streak he would bet more and when he was on a losing streak he would bet less. He conceded that
he lost more hands than he won, but since he bet more when he was winning, he made money in the end.
I respectfully and thoroughly explained to him the concept of an independent event. Also, if one is not counting
cards then each hand in 21 is essentially an independent event. The outcome of the previous hand has no bearing on
the current. Throughout the explanation he nodded his head and agreed with my reasoning. When I was ﬁnished he
replied, “Yes, that’s true. But you see, I have a method. When I’m on my winning streak I bet more and when I’m on
my losing streak I bet less.”
I pretended that I understood. I didn’t want to be rude. After all, he had taken the time to explain the concept
of a winning streak to me. And everyone knows that mathematicians often do not easily understand practical matters,
2260

particularly games of chance.
Never explain mathematics to the layperson.
W.2
Playing the Odds
Years ago in a classroom not so far away, your author was being subjected to a presentation of a lengthy proof. About
ﬁve minutes into the lecture, the entire class was hopelessly lost. At the forty-ﬁve minute mark the professor had a
combinatorial expression that covered most of a chalk board. From his previous queries the professor knew that none
of the students had a clue what was going on. This pleased him and he had became more animated as the lecture had
progressed. He gestured to the board with a smirk and asked for the value of the expression. Without a moment’s
hesitation, I nonchalantly replied, “zero”. The professor was taken aback. He was clearly impressed that I was able to
evaluate the expression, especially because I had done it in my head and so quickly. He enquired as to my method.
“Probability”, I replied. “Professors often present diﬃcult problems that have simple, elegant solutions. Zero is the
most elegant of numerical answers and thus most likely to be the correct answer. My second guess would have been
one.” The professor was not amused.
Whenever a professor asks the class a question which has a numeric answer, immediately respond, “zero”. If you are
asked about your method, casually say something vague about symmetry. Speak with conﬁdence and give non-verbal
cues that you consider the problem to be elementary. This tactic will usually suﬃce. It’s quite likely that some kind of
symmetry is involved. And if it isn’t your response will puzzle the professor. They may continue with the next topic,
not wanting to admit that they don’t see the “symmetry” in such an elementary problem. If they press further, start
mumbling to yourself. Pretend that you are lost in thought, perhaps considering some generalization of the result. They
may be a little irked that you are ignoring them, but it’s better than divulging your true method.
2261

Appendix X
Economics
There are two important concepts in economics. The ﬁrst is “Buy low, sell high”, which is self-explanitory. The
second is opportunity cost, the highest valued alternative that must be sacriﬁced to attain something or otherwise
satisfy a want. I discovered this concept as an undergraduate at Caltech. I was never very in to computer games, but I
found myself randomly playing tetris. Out of the blue I was struck by a revelation: “I could be having sex right now.”
I haven’t played a computer game since.
2262

Appendix Y
Glossary
Phrases often have diﬀerent meanings in mathematics than in everyday usage. Here I have collected deﬁnitions of
some mathematical terms which might confuse the novice.
beyond the scope of this text: Beyond the comprehension of the author.
diﬃcult: Essentially impossible. Note that mathematicians never refer to problems they have solved as being diﬃcult.
This would either be boastful, (claiming that you can solve diﬃcult problems), or self-deprecating, (admitting
that you found the problem to be diﬃcult).
interesting: This word is grossly overused in math and science. It is often used to describe any work that the author
has done, regardless of the work’s signiﬁcance or novelty. It may also be used as a synonym for diﬃcult. It has a
completely diﬀerent meaning when used by the non-mathematician. When I tell people that I am a mathematician
they typically respond with, “That must be interesting.”, which means, “I don’t know anything about math or
what mathematicians do.” I typically answer, “No. Not really.”
non-obvious or non-trivial: Real fuckin’ hard.
one can prove that . . . : The “one” that proved it was a genius like Gauss. The phrase literally means “you haven’t
got a chance in hell of proving that . . . ”
2263

simple: Mathematicians communicate their prowess to colleagues and students by referring to all problems as simple
or trivial. If you ever become a math professor, introduce every example as being “really quite trivial.” 1
Here are some less interesting words and phrases that you are probably already familiar with.
corollary: a proposition inferred immediately from a proved proposition with little or no additional proof
lemma: an auxiliary proposition used in the demonstration of another proposition
theorem: a formula, proposition, or statement in mathematics or logic deduced or to be deduced from other formulas
or propositions
1For even more fun say it in your best Elmer Fudd accent. “This next pwobwem is weawy quite twiviaw”.
2264

Index
a + i b form, 174
Abel’s formula, 895
absolute convergence, 509
adjoint
of a diﬀerential operator, 900
of operators, 1292
analytic, 347
Analytic continuation
Fourier integrals, 1529
analytic continuation, 419
analytic functions, 2206
anti-derivative, 455
Argand diagram, 174
argument
of a complex number, 176
argument theorem, 483
asymptotic expansions, 1228
integration by parts, 1241
asymptotic relations, 1228
autonomous D.E., 973
average value theorem, 481
Bernoulli equations, 965
Bessel functions, 1602
generating function, 1609
of the ﬁrst kind, 1608
second kind, 1624
Bessel’s equation, 1602
Bessel’s Inequality, 1274
Bessel’s inequality, 1318
bilinear concomitant, 902
binomial coeﬃcients, 2259
binomial formula, 2259
boundary value problems, 1090
branch
principal, 7
branch point, 256
branches, 7
calculus of variations, 2041
canonical forms
constant coeﬃcient equation, 999
of diﬀerential equations, 999
cardinality
of a set, 3
Cartesian form, 174
2265

Cartesian product
of sets, 3
Cauchy convergence, 509
Cauchy principal value, 622, 1527
Cauchy’s inequality, 479
Cauchy-Riemann equations, 353, 2206
clockwise, 230
closed interval, 3
closure relation
and Fourier transform, 1531
discrete sets of functions, 1275
codomain, 4
comparison test, 512
completeness
of sets of functions, 1275
sets of vectors, 34
complex conjugate, 173, 174
complex derivative, 346, 347
complex inﬁnity, 231
complex number, 173
magnitude, 175
modulus, 175
complex numbers, 171
arithmetic, 183
set of, 3
vectors, 183
complex plane, 174
ﬁrst order diﬀerential equations, 791
computer games, 2262
connected region, 229
constant coeﬃcient diﬀerential equations, 911
continuity, 52
uniform, 54
continuous
piecewise, 53
continuous functions, 52, 519, 522
contour, 229
traversal of, 230
contour integral, 447
convergence
absolute, 509
Cauchy, 509
comparison test, 512
Gauss’ test, 519
in the mean, 1274
integral test, 513
of integrals, 1448
Raabe’s test, 519
ratio test, 515
root test, 517
sequences, 508
series, 509
uniform, 519
convolution theorem
and Fourier transform, 1534
for Laplace transforms, 1469
convolutions, 1469
counter-clockwise, 230
curve, 229
closed, 229
2266

continuous, 229
Jordan, 229
piecewise smooth, 229
simple, 229
smooth, 229
deﬁnite integral, 117
degree
of a diﬀerential equation, 763
del, 150
delta function
Kronecker, 34
derivative
complex, 347
determinant
derivative of, 890
diﬀerence
of sets, 4
diﬀerence equations
constant coeﬃcient equations, 1153
exact equations, 1147
ﬁrst order homogeneous, 1148
ﬁrst order inhomogeneous, 1150
diﬀerential calculus, 47
diﬀerential equations
autonomous, 973
constant coeﬃcient, 911
degree, 763
equidimensional-in-x, 976
equidimensional-in-y, 978
Euler, 921
exact, 766, 926
ﬁrst order, 762, 777
homogeneous, 763
homogeneous coeﬃcient, 773
inhomogeneous, 763
linear, 763
order, 762
ordinary, 762
scale-invariant, 981
separable, 771
without explicit dep. on y, 927
diﬀerential operator
linear, 886
Dirac delta function, 1022, 1276
direction
negative, 230
positive, 230
directional derivative, 150
discontinuous functions, 52, 1315
discrete derivative, 1146
discrete integral, 1146
disjoint sets, 4
domain, 4
economics, 2262
eigenfunctions, 1308
eigenvalue problems, 1308
eigenvalues, 1308
elements
2267

of a set, 2
empty set, 2
entire, 347
equidimensional diﬀerential equations, 921
equidimensional-in-x D.E., 976
equidimensional-in-y D.E., 978
Euler diﬀerential equations, 921
Euler’s formula, 179
Euler’s notation
i, 172
Euler’s theorem, 773
Euler-Mascheroni constant, 1591
exact diﬀerential equations, 926
exact equations, 766
exchanging dep. and indep. var., 971
extended complex plane, 231
extremum modulus theorem, 482
Fibonacci sequence, 1158
ﬂuid ﬂow
ideal, 369
formally self-adjoint operators, 1293
Fourier coeﬃcients, 1269, 1313
behavior of, 1327
Fourier convolution theorem, 1534
Fourier cosine series, 1322
Fourier cosine transform, 1542
of derivatives, 1544
table of, 2235
Fourier series, 1308
and Fourier transform, 1518
uniform convergence, 1331
Fourier Sine series, 1323
Fourier sine series, 1407
Fourier sine transform, 1543
of derivatives, 1544
table of, 2237
Fourier transform
alternate deﬁnitions, 1523
closure relation, 1531
convolution theorem, 1534
of a derivative, 1532
Parseval’s theorem, 1537
shift property, 1539
table of, 2231, 2234
Fredholm alternative theorem, 1090
Fredholm equations, 1008
Frobenius series
ﬁrst order diﬀerential equation, 796
function
bijective, 5
injective, 5
inverse of, 5
multi-valued, 5
single-valued, 4
surjective, 5
function elements, 419
functional equation, 375
fundamental set of solutions
of a diﬀerential equation, 898
2268

fundamental theorem of algebra, 481
fundamental theorem of calculus, 120
gambler’s ruin problem, 1145, 1154
Gamma function, 1585
diﬀerence equation, 1585
Euler’s formula, 1585
Gauss’ formula, 1589
Hankel’s formula, 1587
Weierstrass’ formula, 1591
Gauss’ test, 519
generating function
for Bessel functions, 1608
geometric series, 510
Gibb’s phenomenon, 1336
gradient, 150
Gramm-Schmidt orthogonalization, 1261
greatest integer function, 5
Green’s formula, 902, 1293
harmonic conjugate, 359
harmonic series, 511, 548
Heaviside function, 784, 1022
holomorphic, 347
homogeneous coeﬃcient equations, 773
homogeneous diﬀerential equations, 763
homogeneous functions, 773
homogeneous solution, 780
homogeneous solutions
of diﬀerential equations, 887
i
Euler’s notation, 172
ideal ﬂuid ﬂow, 369
identity map, 4
ill-posed problems, 788
linear diﬀerential equations, 896
image
of a mapping, 4
imaginary number, 173
imaginary part, 173
improper integrals, 125
indeﬁnite integral, 111, 455
indicial equation, 1180
inﬁnity
complex, 231
ﬁrst order diﬀerential equation, 801
point at, 231
inhomogeneous diﬀerential equations, 763
initial conditions, 782
inner product
of functions, 1265
integers
set of, 2
integral bound
maximum modulus, 449
integral calculus, 111
integral equations, 1008
boundary value problems, 1008
initial value problems, 1008
integrals
2269

improper, 125
integrating factor, 779
integration
techniques of, 122
intermediate value theorem, 53
intersection
of sets, 3
interval
closed, 3
open, 3
inverse function, 5
inverse image, 4
irregular singular points, 1196
ﬁrst order diﬀerential equations, 799
j electrical engineering, 172
Jordan curve, 229
Jordan’s lemma, 2207
Kramer’s rule, 2252
Kronecker delta function, 34
L’Hospital’s rule, 73
Lagrange’s identity, 902, 930, 1292
Laplace transform
inverse, 1455
Laplace transform pairs, 1457
Laplace transforms, 1453
convolution theorem, 1469
of derivatives, 1468
Laurent expansions, 615, 2206
Laurent series, 538, 2208
ﬁrst order diﬀerential equation, 795
leading order behavior
for diﬀerential equations, 1232
least integer function, 5
least squares ﬁt
Fourier series, 1315
Legendre polynomials, 1262
limit
left and right, 49
limits of functions, 47
line integral, 445
complex, 447
linear diﬀerential equations, 763
linear diﬀerential operator, 886
linear space, 1255
Liouville’s theorem, 479
magnitude, 175
maximum modulus integral bound, 449
maximum modulus theorem, 482
Mellin inversion formula, 1456
minimum modulus theorem, 482
modulus, 175
multi-valued function, 5
nabla, 150
natural boundary, 419
Newton’s binomial formula, 2259
norm
2270

of functions, 1265
normal form
of diﬀerential equations, 1002
null vector, 23
one-to-one mapping, 5
open interval, 3
opportunity cost, 2262
optimal asymptotic approximations, 1247
order
of a diﬀerential equation, 762
of a set, 3
ordinary points
ﬁrst order diﬀerential equations, 791
of linear diﬀerential equations, 1163
orthogonal series, 1268
orthogonality
weighting functions, 1267
orthonormal, 1266
Parseval’s equality, 1318
Parseval’s theorem
for Fourier transform, 1537
partial derivative, 148
particular solution, 780
of an ODE, 1040
particular solutions
of diﬀerential equations, 887
periodic extension, 1314
piecewise continuous, 53
point at inﬁnity, 231
diﬀerential equations, 1196
polar form, 179
potential ﬂow, 369
power series
deﬁnition of, 523
diﬀerentiation of, 530
integration of, 530
radius of convergence, 524
uniformly convergent, 523
principal argument, 176
principal branch, 7
principal root, 189
principal value, 622, 1527
pure imaginary number, 173
Raabe’s test, 519
range
of a mapping, 4
ratio test, 515
rational numbers
set of, 2
Rayleigh’s quotient, 1404
minimum property, 1404
real numbers
set of, 2
real part, 173
rectangular unit vectors, 24
reduction of order, 928
and the adjoint equation, 929
2271

diﬀerence equations, 1156
region
connected, 229
multiply-connected, 229
simply-connected, 229
regular, 347
regular singular points
ﬁrst order diﬀerential equations, 794
regular Sturm-Liouville problems, 1398
properties of, 1406
residuals
of series, 510
residue theorem, 619, 2207
principal values, 631
residues, 615, 2206
of a pole of order n, 615, 2207
Riccati equations, 967
Riemann zeta function, 511
Riemann-Lebesgue lemma, 1449
root test, 517
Rouche’s theorem, 484
scalar ﬁeld, 148
scale-invariant D.E., 981
separable equations, 771
sequences
convergence of, 508
series, 508
comparison test, 512
convergence of, 508, 509
Gauss’ test, 519
geometric, 510
integral test, 513
Raabe’s test, 519
ratio test, 515
residuals, 510
root test, 517
tail of, 509
set, 2
similarity transformation, 1868
single-valued function, 4
singularity, 363
branch point, 363
stereographic projection, 232
Stirling’s approximation, 1593
subset, 3
proper, 3
Taylor series, 533, 2207
ﬁrst order diﬀerential equations, 792
table of, 2222
transformations
of diﬀerential equations, 999
of independent variable, 1005
to constant coeﬃcient equation, 1006
to integral equations, 1008
trigonometric identities, 2246
uniform continuity, 54
uniform convergence, 519
2272

of Fourier series, 1331
of integrals, 1448
union
of sets, 3
variation of parameters
ﬁrst order equation, 782
vector
components of, 24
rectangular unit, 24
vector calculus, 147
vector ﬁeld, 148
vector-valued functions, 147
Volterra equations, 1008
wave equation
D’Alembert’s solution, 1914
Fourier transform solution, 1914
Laplace transform solution, 1915
Weber’s function, 1624
Weierstrass M-test, 520
well-posed problems, 788
linear diﬀerential equations, 896
Wronskian, 891, 892
zero vector, 23
2273


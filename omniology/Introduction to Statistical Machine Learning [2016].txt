Introduction to
Statistical Machine
Learning


Introduction to
Statistical Machine
Learning
Masashi Sugiyama
AMSTERDAM • BOSTON • HEIDELBERG • LONDON
NEW YORK • OXFORD • PARIS • SAN DIEGO
SAN FRANCISCO • SINGAPORE • SYDNEY • TOKYO
Morgan Kaufmann Publishers is an Imprint of Elsevier

Acquiring Editor: Todd Green
Editorial Project Manager: Amy Invernizzi
Project Manager: Mohanambal Natarajan
Designer: Maria Ines Cruz
Morgan Kaufmann is an imprint of Elsevier
225 Wyman Street, Waltham, MA 02451 USA
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
No part of this publication may be reproduced or transmitted in any form or by any means, electronic or
mechanical, including photocopying, recording, or any information storage and retrieval system, without
permission in writing from the publisher. Details on how to seek permission, further information about
the Publisher’s permissions policies and our arrangements with organizations such as the Copyright
Clearance Center and the Copyright Licensing Agency, can be found at our website:
www.elsevier.com/permissions.
This book and the individual contributions contained in it are protected under copyright by the Publisher
(other than as may be noted herein).
Notices
Knowledge and best practice in this field are constantly changing. As new research and experience
broaden our understanding, changes in research methods or professional practices, may become
necessary. Practitioners and researchers must always rely on their own experience and knowledge in
evaluating and using any information or methods described herein. In using such information or methods
they should be mindful of their own safety and the safety of others, including parties for whom they have
a professional responsibility.
To the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume any
liability for any injury and/or damage to persons or property as a matter of products liability, negligence
or otherwise, or from any use or operation of any methods, products, instructions, or ideas contained in
the material herein.
Library of Congress Cataloging-in-Publication Data
A catalog record for this book is available from the Library of Congress
British Library Cataloging-in-Publication Data
A catalogue record for this book is available from the British Library.
ISBN: 978-0-12-802121-7
For information on all Morgan Kaufmann publications
visit our website at www.mkp.com

Contents
Biography .................................................................................. xxxi
Preface.....................................................................................xxxiii
PART 1
INTRODUCTION
CHAPTER 1
Statistical Machine Learning
3
1.1
Types of Learning ...................................................... 3
1.2
Examples of Machine Learning Tasks ............................. 4
1.2.1
Supervised Learning ........................................ 4
1.2.2
Unsupervised Learning ..................................... 5
1.2.3
Further Topics ................................................ 6
1.3
Structure of This Textbook ........................................... 8
PART 2
STATISTICS AND PROBABILITY
CHAPTER 2
Random Variables and Probability Distributions
11
2.1
Mathematical Preliminaries ....................................... 11
2.2
Probability ............................................................. 13
2.3
Random Variable and Probability Distribution ................ 14
2.4
Properties of Probability Distributions .......................... 16
2.4.1
Expectation, Median, and Mode ....................... 16
2.4.2
Variance and Standard Deviation ...................... 18
2.4.3
Skewness, Kurtosis, and Moments .................... 19
2.5
Transformation of Random Variables ............................ 22
CHAPTER 3
Examples of Discrete Probability Distributions
25
3.1
Discrete Uniform Distribution..................................... 25
3.2
Binomial Distribution ............................................... 26
3.3
Hypergeometric Distribution....................................... 27
3.4
Poisson Distribution ................................................. 31
3.5
Negative Binomial Distribution ................................... 33
3.6
Geometric Distribution.............................................. 35
CHAPTER 4
Examples of Continuous Probability Distributions
37
4.1
Continuous Uniform Distribution................................. 37
4.2
Normal Distribution.................................................. 37
4.3
Gamma Distribution, Exponential Distribution, and Chi-
Squared Distribution ................................................ 41
4.4
Beta Distribution ..................................................... 44
4.5
Cauchy Distribution and Laplace Distribution ................ 47
4.6
t-Distribution and F-Distribution ................................. 49
v

vi
Contents
CHAPTER 5
Multidimensional Probability Distributions
51
5.1
Joint Probability Distribution...................................... 51
5.2
Conditional Probability Distribution ............................. 52
5.3
Contingency Table.................................................... 53
5.4
Bayes’ Theorem....................................................... 53
5.5
Covariance and Correlation ........................................ 55
5.6
Independence......................................................... 56
CHAPTER 6
Examples of Multidimensional Probability Distributions
61
6.1
Multinomial Distribution ........................................... 61
6.2
Multivariate Normal Distribution ................................. 62
6.3
Dirichlet Distribution ................................................ 63
6.4
Wishart Distribution ................................................. 70
CHAPTER 7
Sum of Independent Random Variables
73
7.1
Convolution ............................................................ 73
7.2
Reproductive Property .............................................. 74
7.3
Law of Large Numbers .............................................. 74
7.4
Central Limit Theorem .............................................. 77
CHAPTER 8
Probability Inequalities
81
8.1
Union Bound .......................................................... 81
8.2
Inequalities for Probabilities ...................................... 82
8.2.1
Markov’s Inequality and Chernoff’s Inequality...... 82
8.2.2
Cantelli’s Inequality and Chebyshev’s Inequality .. 83
8.3
Inequalities for Expectation ....................................... 84
8.3.1
Jensen’s Inequality ........................................ 84
8.3.2
Hölder’s Inequality and Schwarz’s Inequality....... 85
8.3.3
Minkowski’s Inequality ................................... 86
8.3.4
Kantorovich’s Inequality ................................. 87
8.4
Inequalities for the Sum of Independent Random Vari-
ables..................................................................... 87
8.4.1
Chebyshev’s Inequality and Chernoff’s
Inequality.................................................... 88
8.4.2
Hoeffding’s Inequality and Bernstein’s
Inequality.................................................... 88
8.4.3
Bennett’s Inequality....................................... 89
CHAPTER 9
Statistical Estimation
91
9.1
Fundamentals of Statistical Estimation ........................ 91
9.2
Point Estimation...................................................... 92
9.2.1
Parametric Density Estimation ......................... 92
9.2.2
Nonparametric Density Estimation .................... 93
9.2.3
Regression and Classification........................... 93

Contents
vii
9.2.4
Model Selection............................................ 94
9.3
Interval Estimation................................................... 95
9.3.1
Interval Estimation for Expectation of Normal
Samples...................................................... 95
9.3.2
Bootstrap Confidence Interval .......................... 96
9.3.3
Bayesian Credible Interval............................... 97
CHAPTER 10
Hypothesis Testing
99
10.1
Fundamentals of Hypothesis Testing ............................ 99
10.2
Test for Expectation of Normal Samples...................... 100
10.3
Neyman-Pearson Lemma......................................... 101
10.4
Test for Contingency Tables...................................... 102
10.5
Test for Difference in Expectations of Normal Samples .. 104
10.5.1 Two Samples without Correspondence ............. 104
10.5.2 Two Samples with Correspondence.................. 105
10.6
Nonparametric Test for Ranks................................... 107
10.6.1 Two Samples without Correspondence ............. 107
10.6.2 Two Samples with Correspondence.................. 108
10.7
Monte Carlo Test ................................................... 108
PART 3
GENERATIVE APPROACH TO STATISTICAL
PATTERN RECOGNITION
CHAPTER 11
Pattern Recognition via Generative Model Estimation
113
11.1
Formulation of Pattern Recognition ........................... 113
11.2
Statistical Pattern Recognition ................................. 115
11.3
Criteria for Classifier Training ................................... 117
11.3.1 MAP Rule.................................................. 117
11.3.2 Minimum Misclassification Rate Rule.............. 118
11.3.3 Bayes Decision Rule .................................... 119
11.3.4 Discussion................................................. 121
11.4
Generative and Discriminative Approaches .................. 121
CHAPTER 12
Maximum Likelihood Estimation
123
12.1
Definition............................................................. 123
12.2
Gaussian Model..................................................... 125
12.3
Computing the Class-Posterior Probability................... 127
12.4
Fisher’s Linear Discriminant Analysis (FDA)................. 130
12.5
Hand-Written Digit Recognition ................................ 133
12.5.1 Preparation................................................ 134
12.5.2 Implementing Linear Discriminant Analysis ...... 135
12.5.3 Multiclass Classification ............................... 136
CHAPTER 13
Properties of Maximum Likelihood Estimation
139

viii
Contents
13.1
Consistency.......................................................... 139
13.2
Asymptotic Unbiasedness........................................ 140
13.3
Asymptotic Efficiency ............................................. 141
13.3.1 One-Dimensional Case ................................. 141
13.3.2 Multidimensional Cases................................ 141
13.4
Asymptotic Normality ............................................. 143
13.5
Summary ............................................................. 145
CHAPTER 14
Model Selection for Maximum Likelihood Estimation
147
14.1
Model Selection .................................................... 147
14.2
KL Divergence ...................................................... 148
14.3
AIC..................................................................... 150
14.4
Cross Validation..................................................... 154
14.5
Discussion ........................................................... 154
CHAPTER 15
Maximum Likelihood Estimation for Gaussian Mixture
Model
157
15.1
Gaussian Mixture Model.......................................... 157
15.2
MLE ................................................................... 158
15.3
Gradient Ascent Algorithm....................................... 161
15.4
EM Algorithm ....................................................... 162
CHAPTER 16
Nonparametric Estimation
169
16.1
Histogram Method ................................................. 169
16.2
Problem Formulation .............................................. 170
16.3
KDE.................................................................... 174
16.3.1 Parzen Window Method ................................ 174
16.3.2 Smoothing with Kernels................................ 175
16.3.3 Bandwidth Selection.................................... 176
16.4
NNDE ................................................................. 178
16.4.1 Nearest Neighbor Distance............................ 178
16.4.2 Nearest Neighbor Classifier ........................... 179
CHAPTER 17
Bayesian Inference
185
17.1
Bayesian Predictive Distribution................................ 185
17.1.1 Definition .................................................. 185
17.1.2 Comparison with MLE .................................. 186
17.1.3 Computational Issues................................... 188
17.2
Conjugate Prior ..................................................... 188
17.3
MAP Estimation .................................................... 189
17.4
Bayesian Model Selection........................................ 193
CHAPTER 18
Analytic Approximation of Marginal Likelihood
197
18.1
Laplace Approximation ........................................... 197
18.1.1 Approximation with Gaussian Density .............. 197

Contents
ix
18.1.2 Illustration................................................. 199
18.1.3 Application to Marginal Likelihood
Approximation ............................................ 200
18.1.4 Bayesian Information Criterion (BIC) ............... 200
18.2
Variational Approximation........................................ 202
18.2.1 Variational Bayesian EM (VBEM) Algorithm....... 202
18.2.2 Relation to Ordinary EM Algorithm.................. 203
CHAPTER 19
Numerical Approximation of Predictive Distribution
205
19.1
Monte Carlo Integration........................................... 205
19.2
Importance Sampling ............................................. 207
19.3
Sampling Algorithms .............................................. 208
19.3.1 Inverse Transform Sampling .......................... 208
19.3.2 Rejection Sampling ..................................... 212
19.3.3 Markov Chain Monte Carlo (MCMC) Method ...... 214
CHAPTER 20
Bayesian Mixture Models
221
20.1
Gaussian Mixture Models......................................... 221
20.1.1 Bayesian Formulation................................... 221
20.1.2 Variational Inference.................................... 223
20.1.3 Gibbs Sampling .......................................... 228
20.2
Latent Dirichlet Allocation (LDA)............................... 229
20.2.1 Topic Models.............................................. 230
20.2.2 Bayesian Formulation................................... 231
20.2.3 Gibbs Sampling .......................................... 232
PART 4
DISCRIMINATIVE APPROACH TO STATISTICAL
MACHINE LEARNING
CHAPTER 21
Learning Models
237
21.1
Linear-in-Parameter Model....................................... 237
21.2
Kernel Model ........................................................ 239
21.3
Hierarchical Model................................................. 242
CHAPTER 22
Least Squares Regression
245
22.1
Method of LS........................................................ 245
22.2
Solution for Linear-in-Parameter Model ...................... 246
22.3
Properties of LS Solution......................................... 250
22.4
Learning Algorithm for Large-Scale Data..................... 251
22.5
Learning Algorithm for Hierarchical Model .................. 252
CHAPTER 23
Constrained LS Regression
257
23.1
Subspace-Constrained LS........................................ 257
23.2
ℓ2-Constrained LS.................................................. 259

x
Contents
23.3
Model Selection .................................................... 262
CHAPTER 24
Sparse Regression
267
24.1
ℓ1-Constrained LS.................................................. 267
24.2
Solving ℓ1-Constrained LS ....................................... 268
24.3
Feature Selection by Sparse Learning ........................ 272
24.4
Various Extensions ................................................. 272
24.4.1 Generalized ℓ1-Constrained LS ....................... 273
24.4.2 ℓp-Constrained LS....................................... 273
24.4.3 ℓ1 + ℓ2-Constrained LS.................................. 274
24.4.4 ℓ1,2-Constrained LS...................................... 276
24.4.5 Trace Norm Constrained LS ........................... 278
CHAPTER 25
Robust Regression
279
25.1
Nonrobustness of ℓ2-Loss Minimization ...................... 279
25.2
ℓ1-Loss Minimization .............................................. 280
25.3
Huber Loss Minimization......................................... 282
25.3.1 Definition .................................................. 282
25.3.2 Stochastic Gradient Algorithm ....................... 283
25.3.3 Iteratively Reweighted LS.............................. 283
25.3.4 ℓ1-Constrained Huber Loss Minimization .......... 286
25.4
Tukey Loss Minimization ......................................... 290
CHAPTER 26
Least Squares Classification
295
26.1
Classification by LS Regression................................. 295
26.2
0/1-Loss and Margin............................................... 297
26.3
Multiclass Classification.......................................... 300
CHAPTER 27
Support Vector Classification
303
27.1
Maximum Margin Classification ................................ 303
27.1.1 Hard Margin Support Vector Classification ........ 303
27.1.2 Soft Margin Support Vector Classification ......... 305
27.2
Dual Optimization of Support Vector Classification........ 306
27.3
Sparseness of Dual Solution..................................... 308
27.4
Nonlinearization by Kernel Trick................................ 311
27.5
Multiclass Extension .............................................. 312
27.6
Loss Minimization View........................................... 314
27.6.1 Hinge Loss Minimization............................... 315
27.6.2 Squared Hinge Loss Minimization................... 316
27.6.3 Ramp Loss Minimization............................... 318
CHAPTER 28
Probabilistic Classification
321
28.1
Logistic Regression ................................................ 321
28.1.1 Logistic Model and MLE ............................... 321
28.1.2 Loss Minimization View ................................ 324

Contents
xi
28.2
LS Probabilistic Classification .................................. 325
CHAPTER 29
Structured Classification
329
29.1
Sequence Classification .......................................... 329
29.2
Probabilistic Classification for Sequences ................... 330
29.2.1 Conditional Random Field............................. 330
29.2.2 MLE......................................................... 333
29.2.3 Recursive Computation................................. 333
29.2.4 Prediction for New Sample............................ 336
29.3
Deterministic Classification for Sequences .................. 337
PART 5
FURTHER TOPICS
CHAPTER 30
Ensemble Learning
343
30.1
Decision Stump Classifier........................................ 343
30.2
Bagging............................................................... 344
30.3
Boosting .............................................................. 346
30.3.1 Adaboost................................................... 348
30.3.2 Loss Minimization View ................................ 348
30.4
General Ensemble Learning ..................................... 354
CHAPTER 31
Online Learning
355
31.1
Stochastic Gradient Descent .................................... 355
31.2
Passive-Aggressive Learning..................................... 356
31.2.1 Classification.............................................. 357
31.2.2 Regression................................................. 358
31.3
Adaptive Regularization of Weight Vectors (AROW)........ 360
31.3.1 Uncertainty of Parameters............................. 360
31.3.2 Classification.............................................. 361
31.3.3 Regression................................................. 362
CHAPTER 32
Confidence of Prediction
365
32.1
Predictive Variance for ℓ2-Regularized LS.................... 365
32.2
Bootstrap Confidence Estimation............................... 367
32.3
Applications ......................................................... 368
32.3.1 Time-series Prediction.................................. 368
32.3.2 Tuning Parameter Optimization ...................... 369
CHAPTER 33
Semisupervised Learning
375
33.1
Manifold Regularization .......................................... 375
33.1.1 Manifold Structure Brought by Input Samples ... 375
33.1.2 Computing the Solution................................ 377
33.2
Covariate Shift Adaptation ....................................... 378
33.2.1 Importance Weighted Learning....................... 378

xii
Contents
33.2.2 Relative Importance Weighted Learning ........... 382
33.2.3 Importance Weighted Cross Validation ............. 382
33.2.4 Importance Estimation ................................. 383
33.3
Class-balance Change Adaptation.............................. 385
33.3.1 Class-balance Weighted Learning.................... 385
33.3.2 Class-balance Estimation .............................. 386
CHAPTER 34
Multitask Learning
391
34.1
Task Similarity Regularization................................... 391
34.1.1 Formulation ............................................... 391
34.1.2 Analytic Solution......................................... 392
34.1.3 Efficient Computation for Many Tasks .............. 393
34.2
Multidimensional Function Learning .......................... 394
34.2.1 Formulation ............................................... 394
34.2.2 Efficient Analytic Solution............................. 397
34.3
Matrix Regularization.............................................. 397
34.3.1 Parameter Matrix Regularization..................... 397
34.3.2 Proximal Gradient for Trace Norm
Regularization ............................................ 400
CHAPTER 35
Linear Dimensionality Reduction
405
35.1
Curse of Dimensionality .......................................... 405
35.2
Unsupervised Dimensionality Reduction ..................... 407
35.2.1 PCA ......................................................... 407
35.2.2 Locality Preserving Projection ........................ 410
35.3
Linear Discriminant Analyses for Classification............. 412
35.3.1 Fisher Discriminant Analysis.......................... 413
35.3.2 Local Fisher Discriminant Analysis.................. 414
35.3.3 Semisupervised Local Fisher Discriminant
Analysis .................................................... 417
35.4
Sufficient Dimensionality Reduction for Regression....... 419
35.4.1 Information Theoretic Formulation .................. 419
35.4.2 Direct Derivative Estimation .......................... 422
35.5
Matrix Imputation .................................................. 425
CHAPTER 36
Nonlinear Dimensionality Reduction
429
36.1
Dimensionality Reduction with Kernel Trick................. 429
36.1.1 Kernel PCA ................................................ 429
36.1.2 Laplacian Eigenmap .................................... 433
36.2
Supervised Dimensionality Reduction with Neural
Networks ............................................................. 435
36.3
Unsupervised Dimensionality Reduction with
Autoencoder ......................................................... 436
36.3.1 Autoencoder............................................... 436

Contents
xiii
36.3.2 Training by Gradient Descent ......................... 437
36.3.3 Sparse Autoencoder..................................... 439
36.4
Unsupervised Dimensionality Reduction with Restricted
Boltzmann Machine ............................................... 440
36.4.1 Model....................................................... 441
36.4.2 Training by Gradient Ascent........................... 442
36.5
Deep Learning ...................................................... 446
CHAPTER 37
Clustering
447
37.1
k-Means Clustering ................................................ 447
37.2
Kernel k-Means Clustering....................................... 448
37.3
Spectral Clustering ................................................ 449
37.4
Tuning Parameter Selection ..................................... 452
CHAPTER 38
Outlier Detection
457
38.1
Density Estimation and Local Outlier Factor ................ 457
38.2
Support Vector Data Description ............................... 458
38.3
Inlier-Based Outlier Detection .................................. 464
CHAPTER 39
Change Detection
469
39.1
Distributional Change Detection................................ 469
39.1.1 KL Divergence ............................................ 470
39.1.2 Pearson Divergence ..................................... 470
39.1.3 L2-Distance ............................................... 471
39.1.4 L1-Distance ............................................... 474
39.1.5 Maximum Mean Discrepancy (MMD) ............... 476
39.1.6 Energy Distance.......................................... 477
39.1.7 Application to Change Detection in Time Series . 477
39.2
Structural Change Detection .................................... 478
39.2.1 Sparse MLE ............................................... 478
39.2.2 Sparse Density Ratio Estimation..................... 482
References ................................................................................ 485
Index ....................................................................................... 491


List of Figures
Fig. 1.1
Regression.
5
Fig. 1.2
Classification.
5
Fig. 1.3
Clustering.
6
Fig. 1.4
Outlier detection.
6
Fig. 1.5
Dimensionality reduction.
7
Fig. 2.1
Combination of events.
12
Fig. 2.2
Examples of probability mass function. Outcome of throwing a fair six-sided
dice (discrete uniform distribution U{1,2,. . . ,6}).
14
Fig. 2.3
Example of probability density function and its cumulative distribution
function.
15
Fig. 2.4
Expectation is the average of x weighted according to f (x), and median is
the 50% point both from the left-hand and right-hand sides. α-quantile for
0 ≤α ≤1 is a generalization of the median that gives the 100α% point from
the left-hand side. Mode is the maximizer of f (x).
16
Fig. 2.5
Income distribution. The expectation is 62.1 thousand dollars, while the
median is 31.3 thousand dollars.
17
Fig. 2.6
Skewness.
20
Fig. 2.7
Kurtosis.
20
Fig. 2.8
Taylor series expansion at the origin.
21
Fig. 2.9
One-dimensional change of variables in integration. For multidimensional
cases, see Fig. 4.2.
23
Fig. 3.1
Probability mass functions of binomial distribution Bi(n,p).
26
Fig. 3.2
Sampling from a bag. The bag contains N balls which consist of M < N
balls labeled as “A” and N −M balls labeled as “B.” n balls are sampled from
the bag, which consists of x balls labeled as “A” and n −x balls labeled as “B.”
27
Fig. 3.3
Sampling with and without replacement. The sampled ball is returned to the
bag before the next ball is sampled in sampling with replacement, while
the next ball is sampled without returning the previously sampled ball in
sampling without replacement.
28
Fig. 3.4
Probability mass functions of hypergeometric distribution HG(N, M,n).
29
Fig. 3.5
Probability mass functions of Bi(n, M/N) and HG(N, M,n) for N = 100,
M = 90, and n = 90.
29
Fig. 3.6
Probability mass functions of Poisson distribution Po(λ).
34
Fig. 3.7
Probability mass functions of negative binomial distribution NB(k,p).
34
Fig. 3.8
Probability mass functions of geometric distribution Ge(p).
35
Fig. 4.1
Gaussian integral.
39
Fig. 4.2
Two-dimensional change of variables in integration.
40
Fig. 4.3
Probability density functions of normal density N(µ,σ2).
40
Fig. 4.4
Standard normal distribution N(0,1). A random variable following N(0,1)
is included in [−1,1] with probability 68.27%, in [−2,2] with probability
95.45%, and in [−3,3] with probability 99.73%.
41
xv

xvi
List of Figures
Fig. 4.5
Gamma function. Γ(α + 1) = α! holds for non-negative integer α, and the
gamma function smoothly interpolates the factorials.
42
Fig. 4.6
Probability density functions of gamma distribution Ga(α,λ).
43
Fig. 4.7
Probability density functions of beta distribution Be(α, β).
46
Fig. 4.8
Probability density functions of Cauchy distribution Ca(a,b), Laplace distri-
bution La(a,b), and normal distribution N(a,b2).
48
Fig. 4.9
Probability density functions of t-distribution t(d), Cauchy distribution
Ca(0,1), and normal distribution N(0,1).
49
Fig. 4.10
Probability density functions of F-distribution F(d,d′).
50
Fig. 5.1
Correlation coefficient ρx,y. Linear relation between x and y can be captured.
57
Fig. 5.2
Correlation coefficient for nonlinear relations. Even when there is a nonlinear
relation between x and y, the correlation coefficient can be close to zero if the
probability distribution is symmetric.
58
Fig. 5.3
Example of x and y which are uncorrelated but dependent.
59
Fig. 6.1
Probability density functions of two-dimensional normal distribution N(µ,Σ)
with µ = (0,0)⊤.
64
Fig. 6.2
Eigenvalue decomposition.
65
Fig. 6.3
Contour lines of the normal density. The principal axes of the ellipse are
parallel to the eigenvectors of variance-covariance matrix Σ, and their length
is proportional to the square root of the eigenvalues.
66
Fig. 6.4
Probability density functions of Dirichlet distribution Dir(α). The center of
gravity of the triangle corresponds to x(1) = x(2) = x(3) = 1/3, and each
vertex represents the point that the corresponding variable takes one and the
others take zeros.
69
Fig. 6.5
Vectorization operator and Kronecker product.
71
Fig. 7.1
Arithmetic mean, geometric mean, and harmonic mean.
76
Fig. 7.2
Law of large numbers.
77
Fig. 7.3
Central limit theorem. The solid lines denote the normal densities.
78
Fig. 8.1
Markov’s inequality.
83
Fig. 8.2
Chebyshev’s inequality.
84
Fig. 8.3
Convex function and tangent line.
85
Fig. 8.4
h(u) = (1 + u) log(1 + u) −u and g(u) =
u2
2+2u/3.
90
Fig. 9.1
Confidence interval for normal samples.
96
Fig. 9.2
Bootstrap resampling by sampling with replacement.
97
Fig. 10.1
Critical region and critical value.
101
Fig. 11.1
Hand-written digit image and its vectorization.
114
Fig. 11.2
Constructing a classifier is equivalent to determine a discrimination function,
decision regions, and decision boundaries.
115

List of Figures
xvii
Fig. 11.3
Dimensionality reduction onto a two-dimensional subspace by principal
component analysis (see Section 35.2.1).
116
Fig. 11.4
Illustration of hand-written digit samples in the pattern space.
117
Fig. 11.5
MAP rule.
118
Fig. 11.6
Minimum misclassification rate rule.
119
Fig. 12.1
Likelihood equation, setting the derivative of the likelihood to zero, is
a necessary condition for the maximum likelihood solution but is not a
sufficient condition in general.
124
Fig. 12.2
Log function is monotone increasing.
125
Fig. 12.3
Formulas for vector and matrix derivatives [80].
126
Fig. 12.4
MATLAB code for MLE with one-dimensional Gaussian model.
128
Fig. 12.5
Example of MLE with one-dimensional Gaussian model.
128
Fig. 12.6
Orthogonal projection.
130
Fig. 12.7
Mahalanobis distance having hyperellipsoidal contours.
130
Fig. 12.8
Linear discriminant analysis.
132
Fig. 12.9
When the classwise sample ratio n1/n2 is changed.
132
Fig. 12.10
When the classwise sample distributions are rotated.
133
Fig. 12.11
Matrix and third tensor.
134
Fig. 12.12
Misclassified test patterns.
136
Fig. 12.13
MATLAB code for multiclass classification by FDA.
137
Fig. 12.14
Confusion matrix for 10-class classification by FDA. The correct classifica-
tion rate is 1798/2000 = 89.9%.
138
Fig. 13.1
Bias-variance decomposition of expected squared error.
140
Fig. 13.2
MATLAB code for illustrating asymptotic normality of MLE.
145
Fig. 13.3
Example of asymptotic normality of MLE.
145
Fig. 14.1
Model selection. Too simple model may not be expressive enough to
represent the true probability distribution, while too complex model may
cause unreliable parameter estimation.
148
Fig. 14.2
For nested models, log-likelihood is monotone nondecreasing as the model
complexity increases.
150
Fig. 14.3
AIC is the sum of the negative log-likelihood and the number of parameters.
151
Fig. 14.4
Big-o and small-o notations.
152
Fig. 14.5
Cross validation.
154
Fig. 14.6
Algorithm of likelihood cross validation.
155
Fig. 15.1
MLE for Gaussian model.
158
Fig. 15.2
Example of Gaussian mixture model: q(x)
=
0.4N(x; −2,1.52) +
0.2N(x; 2,22) + 0.4N(x; 3,12).
159
Fig. 15.3
Schematic of gradient ascent.
161
Fig. 15.4
Algorithm of gradient ascent.
161
Fig. 15.5
Step size ε in gradient ascent. The gradient flow can overshoot the peak if ε
is large, while gradient ascent is slow if ε is too small.
162
Fig. 15.6
EM algorithm.
163

xviii
List of Figures
Fig. 15.7
Maximizing the lower bound b(θ) of the log-likelihood log L(θ).
164
Fig. 15.8
Jensen’s inequality for m = 2. log is a concave function.
164
Fig. 15.9
MATLAB code of EM algorithm for Gaussian mixture model.
166
Fig. 15.10
Example of EM algorithm for Gaussian mixture model. The size of ellipses
is proportional to the mixing weights {wℓ}m
ℓ=1.
167
Fig. 16.1
Examples of Gaussian MLE.
170
Fig. 16.2
Example of histogram method.
170
Fig. 16.3
MATLAB code for inverse transform sampling (see Section 19.3.1) for
probability density function shown in Fig. 16.1(b). The bottom function
should be saved as “myrand.m.”
171
Fig. 16.4
Choice of bin width in histogram method.
171
Fig. 16.5
Notation of nonparametric methods.
172
Fig. 16.6
Probability P approximated by the size of rectangle.
172
Fig. 16.7
Normalized variance of binomial distribution.
173
Fig. 16.8
Parzen window method.
174
Fig. 16.9
Example of Parzen window method.
175
Fig. 16.10
Example of Gaussian KDE. Training samples are the same as those in
Fig. 16.9.
176
Fig. 16.11
Choice of kernel bandwidth h in KDE.
177
Fig. 16.12
MATLAB code for Gaussian KDE with bandwidth selected by likelihood
cross validation. A random number generator “myrand.m” shown in Fig. 16.3
is used.
177
Fig. 16.13
Example of Gaussian KDE with bandwidth selected by likelihood cross
validation.
178
Fig. 16.14
MATLAB code for NNDE with the number of nearest neighbors selected by
likelihood cross validation. A random number generator “myrand.m” shown
in Fig. 16.3 is used.
179
Fig. 16.15
Example of NNDE with the number of nearest neighbors selected by
likelihood cross validation.
180
Fig. 16.16
Example of nearest neighbor classifier.
181
Fig. 16.17
Algorithm of cross validation for misclassification rate.
182
Fig. 16.18
MATLAB code for k-nearest neighbor classifier with k chosen by cross
validation. The bottom function should be saved as “knn.m.”
183
Fig. 16.19
Confusion matrix for 10-class classification by k-nearest neighbor classifier.
k = 1 was chosen by cross validation for misclassification rate. The correct
classification rate is 1932/2000 = 96.6%.
183
Fig. 17.1
Bayes vs. MLE. The maximum likelihood solution pML is always confined
in the parametric model q(x; θ), while the Bayesian predictive distribution
pBayes(x) generally pops out from the model.
187
Fig. 17.2
MAP estimation.
190
Fig. 17.3
Example of MLE for Gaussian model. When the number of training samples,
n, is small, MLE tends to overfit the samples.
190
Fig. 17.4
MATLAB code for penalized MLE with one-dimensional Gaussian model.
192
Fig. 17.5
Example of MAP estimation with one-dimensional Gaussian model.
192

List of Figures
xix
Fig. 17.6
MATLAB code for empirical Bayes.
195
Fig. 17.7
Example of empirical Bayes.
195
Fig. 18.1
Laplace approximation.
199
Fig. 19.1
Numerical computation of π by Monte Carlo integration.
206
Fig. 19.2
MATLAB code for numerically computing π by Monte Carlo integration.
207
Fig. 19.3
MATLAB code for importance sampling.
208
Fig. 19.4
Examples of probability density function p(θ) and its cumulative distribution
function P(θ). Cumulative distribution function is monotone nondecreasing
and satisfies limθ→−∞P(θ) = 0 and limθ→∞P(θ) = 1.
209
Fig. 19.5
Inverse transform sampling.
209
Fig. 19.6
θ ≤θ′ implies P(θ) ≤P(θ′).
210
Fig. 19.7
Laplace distribution.
211
Fig. 19.8
MATLAB code for inverse transform sampling.
211
Fig. 19.9
Example of inverse transform sampling for Laplace distribution.
212
Fig. 19.10
Algorithm of rejection sampling.
212
Fig. 19.11
Illustration of rejection sampling when the proposal distribution is uniform.
213
Fig. 19.12
MATLAB code for rejection sampling.
214
Fig. 19.13
Example of rejection sampling.
214
Fig. 19.14
Computational efficiency of rejection sampling. (a) When the upper bound
of the probability density, κ, is small, proposal points are almost always
accepted and thus rejection sampling is computationally efficient. (b) When
κ is large, most of the proposal points will be rejected and thus rejection
sampling is computationally expensive.
215
Fig. 19.15
Random walk.
216
Fig. 19.16
MATLAB code for Metropolis-Hastings sampling. The bottom function
should be saved as “pdf.m.”
217
Fig. 19.17
Example of Metropolis-Hastings sampling.
218
Fig. 19.18
MATLAB code for Gibbs sampling.
219
Fig. 19.19
Example of Gibbs sampling.
220
Fig. 20.1
Variational Bayesian formulation of Gaussian mixture model.
223
Fig. 20.2
VBEM algorithm for Gaussian mixture model. (α0, β0,W0,ν0) are hyperpa-
rameters.
225
Fig. 20.3
MATLAB code of VBEM algorithm for Gaussian mixture model.
226
Fig. 20.4
Example of VBEM algorithm for Gaussian mixture model. The size of
ellipses is proportional to the mixing weights {wℓ}m
ℓ=1. A mixture model of
five Gaussian components is used here, but three components have mixing
coefficient close to zero and thus they are almost eliminated.
227
Fig. 20.5
MATLAB code of collapsed Gibbs sampling for Gaussian mixture model.
229
Fig. 20.6
Example of collapsed Gibbs sampling for Gaussian mixture model. A mix-
ture model of five Gaussian components is used here, but only two compo-
nents remain and no samples belong to the remaining three components.
230
Fig. 21.1
Linear-in-input model cannot approximate nonlinear functions.
238

xx
List of Figures
Fig. 21.2
Multidimensional basis functions. The multiplicative model is expressive, but
the number of parameters grows exponentially in input dimensionality. On
the other hand, in the additive model, the number of parameters grows only
linearly in input dimensionality, but its expression power is limited.
239
Fig. 21.3
Gaussian kernel with bandwidth h and center c.
240
Fig. 21.4
One-dimensional Gaussian kernel model. Gaussian functions are located at
training input samples {xi}n
i=1 and their height {θi}n
i=1 is learned.
241
Fig. 21.5
Two-dimensional Gaussian kernel model. The curse of dimensionality is
mitigated by only approximating the learning target function in the vicinity
of training input samples.
241
Fig. 21.6
Sigmoidal function.
242
Fig. 21.7
Hierarchical model as a three-layered network.
243
Fig. 22.1
Generalized inverse.
247
Fig. 22.2
Singular value decomposition.
248
Fig. 22.3
MATLAB code for LS regression.
249
Fig. 22.4
Example of LS regression with sinusoidal basis functions φ(x)
=
(1,sin x
2 ,cos x
2 ,sin 2x
2 ,cos 2x
2 ,. . . ,sin 15x
2 ,cos 15x
2 )⊤.
249
Fig. 22.5
Geometric interpretation of LS method for linear-in-parameter model. Train-
ing output vector y is projected onto the range of Φ, denoted by R(Φ), for
denoising purposes.
251
Fig. 22.6
Algorithm of stochastic gradient descent for LS regression with a linear-in-
parameter model.
252
Fig. 22.7
MATLAB code of stochastic gradient descent for LS regression with the
Gaussian kernel model.
253
Fig. 22.8
Example of stochastic gradient descent for LS regression with the Gaussian
kernel model. For n = 50 training samples, the Gaussian bandwidth is set at
h = 0.3.
254
Fig. 22.9
Gradient descent for nonlinear models. The training squared error JLS is
nonconvex and there exist multiple local optimal solutions in general.
254
Fig. 22.10
MATLAB code for error back-propagation algorithm.
255
Fig. 22.11
Example of regression by error back-propagation algorithm.
255
Fig. 23.1
Examples of LS regression for linear-in-parameter model when the
noise
level
in
training
output
is
high.
Sinusoidal
basis
functions
{1,sin x
2 ,cos x
2 ,sin 2x
2 ,cos 2x
2 ,. . . ,sin 15x
2 ,cos 15x
2 } are used in ordinary LS,
while its subset {1,sin x
2 ,cos x
2 ,sin 2x
2 ,cos 2x
2 ,. . . ,sin 5x
2 ,cos 5x
2 } is used in
the subspace-constrained LS method.
258
Fig. 23.2
Constraint in parameter space.
258
Fig. 23.3
MATLAB code for subspace-constrained LS regression.
259
Fig. 23.4
Parameter space in ℓ2-constrained LS.
259
Fig. 23.5
Lagrange dual problem.
260
Fig. 23.6
MATLAB code of ℓ2-constrained LS regression for Gaussian kernel model.
262
Fig. 23.7
Example of ℓ2-constrained LS regression for Gaussian kernel model. The
Gaussian bandwidth is set at h = 0.3, and the regularization parameter is set
at λ = 0.1.
262

List of Figures
xxi
Fig. 23.8
Parameter space in generalized ℓ2-constrained LS.
263
Fig. 23.9
Examples of ℓ2-constrained LS with the Gaussian kernel model for different
Gaussian bandwidth h and different regularization parameter λ.
263
Fig. 23.10
MATLAB code of cross validation for ℓ2-constrained LS regression.
264
Fig. 23.11
Example of cross validation for ℓ2-constrained LS regression. The cross
validation error for all Gaussian bandwidth h and regularization parameter λ
is plotted, which is minimized at (h,λ) = (0.3,0.1). See Fig. 23.9 for learned
functions.
265
Fig. 23.12
Matrix inversion lemma.
265
Fig. 24.1
Parameter space in ℓ1-constrained LS.
268
Fig. 24.2
The solution of ℓ1-constrained LS tends to be on one of the coordinate axes,
which is a sparse solution.
269
Fig. 24.3
Alternating direction method of multipliers.
270
Fig. 24.4
MATLAB code of ℓ1-constrained LS by ADMM for Gaussian kernel model.
271
Fig. 24.5
Example of ℓ1-constrained LS for Gaussian kernel model. 38 out of 50
parameters are zero.
271
Fig. 24.6
Unit ℓp-balls.
274
Fig. 24.7
Properties of ℓp-constraint.
275
Fig. 24.8
Unit (ℓ1 + ℓ2)-norm ball for balance parameter τ = 1/2, which is similar to
the unit ℓ1.4-ball. However, while the ℓ1.4-ball has no corner, the (ℓ1+ℓ2)-ball
has corners.
276
Fig. 24.9
Constraints in three-dimensional parameter space.
277
Fig. 24.10
Trace norm of a matrix.
278
Fig. 25.1
LS solution for straight-line model fθ(x) = θ1 + θ2x, which is strongly
affected by an outlier.
280
Fig. 25.2
ℓ2-loss and ℓ1-loss. The ℓ2-loss magnifies large residuals.
281
Fig. 25.3
Solution of least absolute deviations for straight-line model fθ(x) = θ1 + θ2x
for the same training samples as Fig. 25.1. Least absolute deviations give a
much more robust solution than LS.
281
Fig. 25.4
Huber loss, with threshold η = 1.
282
Fig. 25.5
Quadratic upper bound ηr2
2c + ηc
2 −η2
2 of Huber loss ρHuber(r) for c > 0,
which touches each other at r = ±c.
284
Fig. 25.6
Weight functions for Huber loss minimization and Tukey loss minimization.
285
Fig. 25.7
Updated solution θ is no worse than current solution θ.
285
Fig. 25.8
Iteratively reweighted LS for Huber loss minimization.
286
Fig. 25.9
MATLAB code of iteratively reweighted LS for Huber loss minimization.
Straight-line model fθ(x) = θ1 + θ2x is used, with threshold η = 1.
287
Fig. 25.10
Examples of iteratively reweighted LS for Huber loss minimization. Straight-
line model fθ(x) = θ1 + θ2x is used, with threshold η = 1.
288
Fig. 25.11
Quadratic upper bound θ2
2c + c
2 of absolute value |θ| for c > 0, which touches
each other at θ = ±c.
289
Fig. 25.12
Iteratively reweighted LS for ℓ1-regularized Huber loss minimization.
289
Fig. 25.13
MATLAB code of iteratively reweighted LS for ℓ1-regularized Huber loss
minimization with Gaussian kernel model.
290

xxii
List of Figures
Fig. 25.14
Example of ℓ1-regularized Huber loss minimization with Gaussian kernel
model.
291
Fig. 25.15
Tukey loss, with threshold η = 3.
291
Fig. 25.16
Example of Tukey loss minimization. Tukey loss minimization gives more
robust solutions than Huber loss minimization, but only a local optimal
solution can be obtained.
292
Fig. 26.1
Binary classification as function approximation.
296
Fig. 26.2
MATLAB code of classification by ℓ2-regularized LS for Gaussian kernel
model.
296
Fig. 26.3
Example of classification by ℓ2-regularized LS for Gaussian kernel model.
297
Fig. 26.4
0/1-loss and ℓ2-loss as functions of margin m = fθ(x)y.
298
Fig. 26.5
Example of ℓ2-loss minimization for linear-in-input model. Since the ℓ2-
loss has a positive slope when m > 1, the obtained solution contains some
classification error even though all samples can be correctly classified in
principle.
299
Fig. 26.6
Popular surrogate loss functions.
300
Fig. 26.7
One-versus-rest reduction of multiclass classification problem.
301
Fig. 26.8
One-versus-one reduction of multiclass classification problem.
301
Fig. 27.1
Linear-in-input binary classifier fw,γ(x) = w⊤x + γ. w and γ are the normal
vector and the intercept of the decision boundary, respectively.
304
Fig. 27.2
Decision boundaries that separate all training samples correctly.
304
Fig. 27.3
Decision boundary of hard margin support vector machine. It goes through
the center of positive and negative training samples, w⊤x+ +γ = +1 for some
positive sample x+ and w⊤x−+ γ = −1 for some negative sample x−.
305
Fig. 27.4
Soft margin support vector machine allows small margin errors.
306
Fig. 27.5
Quadratic programming.
307
Fig. 27.6
Example of linear support vector classification. Among 200 dual parameters
{αi}n
i=1, 197 parameters take zero and only 3 parameters specified by the
square in the plot take nonzero values.
309
Fig. 27.7
KKT optimality conditions.
310
Fig. 27.8
When αi
= 0, xi is inside the margin and correctly classified. When
0 < αi < C, xi is on the margin border (the dotted lines) and correctly
classified. When αi = C, xi is outside the margin, and if ξi > 1, mi < 0 and
thus xi is misclassified.
310
Fig. 27.9
Nonlinearization of support vector machine by kernel trick.
311
Fig. 27.10
MATLAB code of support vector classification for Gaussian kernel.
quadprog.m included in Optimization Toolbox is required. Free alternatives
to quadprog.m are available, e.g. from http://www.mathworks.com/
matlabcentral/fileexchange/.
313
Fig. 27.11
Example of support vector classification for Gaussian kernel.
314
Fig. 27.12
Hinge loss and squared hinge loss.
315
Fig. 27.13
Hinge loss as maximizer of 1 −m and 0.
316
Fig. 27.14
Iterative retargeted LS for ℓ2-regularized squared hinge loss minimization.
317

List of Figures
xxiii
Fig. 27.15
MATLAB code of iterative retargeted LS for ℓ2-regularized squared hinge
loss minimization.
318
Fig. 27.16
Example of ℓ2-regularized squared hinge loss minimization.
319
Fig. 27.17
Examples of support vector classification with outliers.
319
Fig. 27.18
Ramp loss and squared ramp loss.
320
Fig. 28.1
Stochastic gradient algorithm for logistic regression.
322
Fig. 28.2
MATLAB code of stochastic gradient ascent for logistic regression.
323
Fig. 28.3
Example of stochastic gradient ascent for logistic regression.
324
Fig. 28.4
Logistic loss.
325
Fig. 28.5
MATLAB code for LS probabilistic classification.
327
Fig. 28.6
Example of LS probabilistic classification for the same data set as Fig. 28.3.
328
Fig. 29.1
Classification of sequence of hand-written digits.
330
Fig. 29.2
Sequence classification.
331
Fig. 29.3
Stochastic gradient algorithm for conditional random field.
333
Fig. 29.4
Dynamic programming, which solves a complex optimization problem by
breaking it down into simpler subproblems recursively. When the number of
steps to the goal is counted, dynamic programming trace back the steps from
the goal. In this case, many subproblems of counting the number of steps
from other positions are actually shared and thus dynamic programming can
efficiently reuse the solutions to reduce the computation costs.
334
Fig. 30.1
Ensemble learning. Bagging trains weak learners in parallel, while boosting
sequentially trains weak learners.
344
Fig. 30.2
Decision stump and decision tree classifiers. A decision stump is a depth-one
version of a decision tree.
344
Fig. 30.3
MATLAB code for decision stump classification.
345
Fig. 30.4
Example of decision stump classification.
345
Fig. 30.5
Algorithm of bagging.
346
Fig. 30.6
MATLAB code of bagging for decision stumps.
346
Fig. 30.7
Example of bagging for decision stumps.
347
Fig. 30.8
Algorithm of adaboost.
349
Fig. 30.9
Confidence of classifier in adaboost. The confidence of classifier φ, denoted
by θ, is determined based on the weighted misclassification rate R.
350
Fig. 30.10
MATLAB code of adaboost for decision stumps.
351
Fig. 30.11
Example of adaboost for decision stumps.
352
Fig. 30.12
Exponential loss.
353
Fig. 30.13
Loss functions for boosting.
353
Fig. 31.1
Choice of step size. Too large step size overshoots the optimal solution, while
too small step size yields slow convergence.
356
Fig. 31.2
Algorithm of passive-aggressive classification.
358
Fig. 31.3
MATLAB code for passive-aggressive classification.
359
Fig. 31.4
Example of passive-aggressive classification.
359
Fig. 31.5
MATLAB code for passive-aggressive regression with the ℓ2-loss.
360

xxiv
List of Figures
Fig. 31.6
Algorithm of AROW classification.
362
Fig. 31.7
MATLAB code for AROW classification.
363
Fig. 31.8
Examples of passive-aggressive and AROW classifications.
363
Fig. 31.9
MATLAB code for AROW regression.
364
Fig. 32.1
MATLAB code for analytic computation of predictive variance.
367
Fig. 32.2
Examples of analytic computation of predictive variance. The shaded area
indicates the confidence interval.
368
Fig. 32.3
MATLAB code for bootstrap-based confidence estimation.
369
Fig. 32.4
Examples of bootstrap-based confidence estimation. The shaded area indi-
cates the confidence interval.
370
Fig. 32.5
Problem of time-series prediction.
370
Fig. 32.6
Time-series prediction from previous samples.
370
Fig. 32.7
MATLAB code for time-series prediction by ℓ2-regularized LS.
371
Fig. 32.8
Examples of time-series prediction by ℓ2-regularized LS. The shaded areas
indicate the confidence intervals.
372
Fig. 32.9
Bayesian optimization. The shaded areas indicate the confidence intervals.
373
Fig. 33.1
Semisupervised classification. Samples in the same cluster are assumed to
belong to the same class.
376
Fig. 33.2
MATLAB code for Laplacian-regularized LS.
379
Fig. 33.3
Examples of Laplacian-regularized LS compared with ordinary LS. Dots
denote unlabeled training samples.
380
Fig. 33.4
Covariate shift in regression. Input distributions change, but the input-output
relation is unchanged.
380
Fig. 33.5
MATLAB code for importance weighted LS.
381
Fig. 33.6
Example of LS learning under covariate shift. The dashed lines denote
learned functions.
381
Fig. 33.7
Relative importance when p′(x) is the Gaussian density with expectation 0
and variance 1 and p(x) is the Gaussian density with expectation 0.5 and
variance 1.
383
Fig. 33.8
Algorithm of importance weighted cross validation.
384
Fig. 33.9
MATLAB code for LS relative density ratio estimation for Gaussian kernel
model.
386
Fig. 33.10
Example of LS relative density ratio estimation. ×’s in the right plot show
estimated relative importance values at {xi}n
i=1.
387
Fig. 33.11
Class-balance change, which affects the decision boundary.
387
Fig. 33.12
Class-prior estimation by distribution matching.
388
Fig. 33.13
MATLAB code for class-balance weighted LS.
389
Fig. 33.14
Example of class-balance weighted LS. The test class priors are estimated
as p′(y = 1) = 0.18 and p′(y = 2) = 0.82, which are used as weights in
class-balance weighted LS.
390
Fig. 34.1
MATLAB code for multitask LS.
394
Fig. 34.2
Examples of multitask LS. The dashed lines denote true decision boundaries
and the contour lines denote learned results.
395

List of Figures
xxv
Fig. 34.3
Alternate learning of task similarity γt,t′ and solution θ.
396
Fig. 34.4
Multidimensional function learning.
396
Fig. 34.5
Continuous Sylvester equation.
398
Fig. 34.6
MATLAB code for multidimensional regression.
399
Fig. 34.7
Examples of multidimensional regression.
399
Fig. 34.8
Proximal gradient method.
401
Fig. 34.9
MATLAB code for multitask learning with trace norm regularization.
402
Fig. 34.10
Examples of multitask LS with trace norm regularization. The data set is the
same as Fig. 34.2. The dashed lines denote true decision boundaries and the
contour lines denote learned results.
403
Fig. 35.1
Curse of dimensionality.
406
Fig. 35.2
Linear dimensionality reduction. Transformation by a fat matrix T corre-
sponds to projection onto a subspace.
407
Fig. 35.3
Data centering.
407
Fig. 35.4
PCA, which tries to keep the position of original samples when the dimen-
sionality is reduced.
408
Fig. 35.5
MATLAB code for PCA.
409
Fig. 35.6
Example of PCA. The solid line denotes the one-dimensional embedding
subspace found by PCA.
409
Fig. 35.7
Locality preserving projection, which tries to keep the cluster structure of
original samples when the dimensionality is reduced.
410
Fig. 35.8
Popular choices of similarity measure.
411
Fig. 35.9
MATLAB code for locality preserving projection.
412
Fig. 35.10
Example of locality preserving projection. The solid line denotes the one-
dimensional embedding subspace found by locality preserving projection.
413
Fig. 35.11
MATLAB code for Fisher discriminant analysis.
414
Fig. 35.12
Examples of Fisher discriminant analysis. The solid lines denote the found
subspaces to which training samples are projected.
415
Fig. 35.13
MATLAB code for local Fisher discriminant analysis.
417
Fig. 35.14
Examples of local Fisher discriminant analysis for the same data sets as
Fig. 35.12. The solid lines denote the found subspaces to which training
samples are projected.
418
Fig. 35.15
MATLAB code for semisupervised local Fisher discriminant analysis.
420
Fig. 35.16
Examples of semisupervised local Fisher discriminant analysis. Lines denote
the found subspaces to which training samples are projected. “LFDA” stands
for local Fisher discriminant analysis, “SELF” stands for semisupervised
LFDA, and “PCA” stands for principal component analysis.
421
Fig. 35.17
MATLAB code for supervised dimensionality reduction based on QMI.
423
Fig. 35.18
Example of supervised dimensionality reduction based on QMI. The solid
line denotes the found subspace to which training samples are projected.
424
Fig. 35.19
MATLAB code for unsupervised dimensionality reduction based on QMI.
425
Fig. 35.20
Example of unsupervised dimensionality reduction based on QMI. The solid
line denotes the found subspace to which training samples are projected.
426
Fig. 35.21
MATLAB code for unsupervised matrix imputation.
426

xxvi
List of Figures
Fig. 35.22
Example of unsupervised matrix imputation. The gray level indicates the
value of each entry in [−5, 5].
427
Fig. 36.1
Nonlinear PCA in a feature space. “×” denotes a sample, the solid line
denotes the one-dimensional embedding subspace found by PCA, and “◦”
denotes a projected sample.
430
Fig. 36.2
Eigenvalue problems for PCA. Appropriately choosing the expression of
eigenvalue problem depending on whether matrix Ψ is fat or skinny allows
us to reduce the computational costs.
431
Fig. 36.3
MATLAB code for kernel PCA with Gaussian kernels.
433
Fig. 36.4
Examples of kernel PCA with Gaussian kernels. Original two-dimensional
samples are transformed to infinite-dimensional feature space by Gaussian
kernels with width h, and then PCA is applied to reduce the dimensionality
to two.
434
Fig. 36.5
MATLAB code of Laplacian eigenmap for 10-nearest neighbor similarity.
435
Fig. 36.6
Example of Laplacian eigenmap for 10-nearest neighbor similarity.
435
Fig. 36.7
Dimensionality reduction by neural network. The number of hidden nodes is
smaller than the number of input (and output) nodes.
436
Fig. 36.8
Autoencoder. Input and output are the same and the number of hidden nodes
is smaller than the number of input nodes.
437
Fig. 36.9
Chain rule for autoencoder.
438
Fig. 36.10
MATLAB code for denoising autoencoder. See Section 12.5 for details of
hand-written digit data set “digit.mat.”
439
Fig. 36.11
Example of denoising autoencoder.
440
Fig. 36.12
Restricted Boltzmann machine.
441
Fig. 36.13
Contrastive divergence algorithm for restricted Boltzmann machine. Note
that q(z|x = xi) and q(x|z = zi) can be factorized as Eq. (36.6), which
allows efficient computation.
443
Fig. 36.14
MATLAB code for denoising restricted Boltzmann machine. See Sec-
tion 12.5 for details of hand-written digit data set “digit.mat.”
443
Fig. 36.15
Example of denoising restricted Boltzmann machine.
444
Fig. 36.16
Construction of deep neural network by stacking.
445
Fig. 37.1
k-means clustering algorithm.
448
Fig. 37.2
MATLAB code for k-means clustering.
449
Fig. 37.3
Example of k-means clustering. A filled square denotes a cluster center.
450
Fig. 37.4
Algorithm of spectral clustering.
451
Fig. 37.5
MATLAB code for spectral clustering.
451
Fig. 37.6
Example of spectral clustering.
452
Fig. 37.7
Clustering can be regarded as compressing d-dimensional vector x into c-
valued scalar y.
453
Fig. 37.8
MATLAB code for LS QMI estimation.
455
Fig. 37.9
Example of LS QMI estimation.
456
Fig. 38.1
MATLAB code for local outlier factor.
459

List of Figures
xxvii
Fig. 38.2
Example of outlier detection by local outlier factor. The diameter of circles
around samples is proportional to the value of local outlier factor.
460
Fig. 38.3
Support vector data description. A hypersphere that contains most of the
training samples is found. Samples outside the hypersphere are regarded as
outliers.
460
Fig. 38.4
MATLAB code of support vector data description for Gaussian kernel.
quadprog.m included in Optimization Toolbox is required. Free alternatives
to quadprog.m are available, e.g. from http://www.mathworks.com/
matlabcentral/fileexchange/.
463
Fig. 38.5
Examples of support vector data description for Gaussian kernel. Circled
samples are regarded as outliers.
464
Fig. 38.6
Inlier-based outlier detection by density ratio estimation. For inlier density
p′(x) and test sample density p(x), the density ratio w(x) = p′(x)/p(x) is
close to one when x is an inlier and it is close to zero when x is an outlier.
465
Fig. 38.7
MATLAB code of KL density ratio estimation for Gaussian kernel model
with Gaussian bandwidth chosen by cross validation. The bottom function
should be saved as “KLIEP.m.”
466
Fig. 38.8
Example of KL density ratio estimation for Gaussian kernel model.
467
Fig. 39.1
MATLAB code for LS density difference estimation.
473
Fig. 39.2
Example of LS density difference estimation. ×’s in the right plot show
estimated density difference values at {xi}n
i=1 and {x′
i′}n′
i′=1.
474
Fig. 39.3
Lower bound of sign (t) by −2 max(0,1 −t) + 1.
475
Fig. 39.4
Change detection in time series.
478
Fig. 39.5
MATLAB code for change detection in time series based on the energy
distance.
479
Fig. 39.6
Examples of change detection in time series based on the energy distance.
480
Fig. 39.7
Structural change in Gaussian Markov networks.
480
Fig. 39.8
MATLAB code of a gradient-projection algorithm of ℓ1-constraint MLE
for Gaussian Markov networks. The bottom function should be saved as
“L1BallProjection.m.”
481
Fig. 39.9
MATLAB code of a gradient-projection algorithm of ℓ1-constraint KL den-
sity ratio estimation for Gaussian Markov networks. “L1BallProjection.m”
is given in Fig. 39.8.
484


List of Tables
Table 5.1
Example of Contingency Table
53
Table 7.1
Convolution
75
Table 10.1
Type-I Error α (False Positive) and Type-II Error β (False Negative)
102
Table 10.2
Contingency Table for x ∈{1,. . . ,ℓ} and y ∈{1,. . . ,m}. cx,y Denotes
the Frequency of (x, y), dx
= m
y=1 cx,y, ey
= ℓ
x=1 cx,y, and n
=
ℓ
x=1
m
y=1 cx,y
103
Table 10.3
Wilcoxon Rank-Sum Test. In this Example, r1 = 3, r2 = 5.5, r3 = 1, and the
Rank-Sum is r = 9.5
107
Table 11.1
Example Of Asymmetric Loss
120
xxix


Biography
MASASHI SUGIYAMA
Masashi Sugiyama received the degrees of
Bachelor of Engineering, Master of Engineer-
ing, and Doctor of Engineering in Computer
Science from Tokyo Institute of Technology,
Japan in 1997, 1999, and 2001, respectively.
In 2001, he was appointed Assistant Professor
in the same institute, and he was promoted to
Associate Professor in 2003. He moved to the
University of Tokyo as Professor in 2014. He
received an Alexander von Humboldt Foun-
dation Research Fellowship and researched at
Fraunhofer Institute, Berlin, Germany, from
2003 to 2004. In 2006, he received a Euro-
pean Commission Program Erasmus Mundus
Scholarship and researched at the University
of Edinburgh, Edinburgh, UK. He received
the Faculty Award from IBM in 2007 for his
contribution to machine learning under non-
stationarity, the Nagao Special Researcher Award from the Information Processing
Society of Japan in 2011 and the Young Scientists’ Prize from the Commendation
for Science and Technology by the Minister of Education, Culture, Sports, Science
and Technology Japan for his contribution to the density-ratio paradigm of machine
learning. His research interests include theories and algorithms of machine learning
and data mining, and a wide range of applications such as signal processing, image
processing, and robot control.
xxxi


Preface
Machine learning is a subject in computer science, aimed at studying theories,
algorithms, and applications of systems that learn like humans. Recent development
of computers and sensors allows us to access a huge amount of data in diverse
domains such as documents, audio, images, movies, e-commerce, electric power,
medicine, and biology. Machine learning plays a central role in analyzing and
benefiting from such big data.
This textbook is devoted to presenting mathematical backgrounds and practical
algorithms of various machine learning techniques, targeting undergraduate and
graduate students in computer science and related fields. Engineers who are applying
machine learning techniques in their business and scientists who are analyzing their
data can also benefit from this book.
A distinctive feature of this book is that each chapter concisely summarizes the
main idea and mathematical derivation of particular machine learning techniques, fol-
lowed by compact MATLAB programs. Thus, readers can study both mathematical
concepts and practical values of various machine learning techniques simultaneously.
All MATLAB programs are available from
“http://www.ms.k.u-tokyo.ac.jp/software/SMLbook.zip”.
This book begins by giving a brief overview of the field of machine learning in
Part 1. Then Part 2 introduces fundamental concepts of probability and statistics,
which form the mathematical basis of statistical machine learning. Part 2 was written
based on
Sugiyama, M.
Probability and Statistics for Machine Learning,
Kodansha, Tokyo, Japan, 2015. (in Japanese).
Part 3 and Part 4 present a variety of practical machine learning algorithms in the
generative and discriminative frameworks, respectively. Then Part 5 covers various
advanced topics for tackling more challenging machine learning tasks. Part 3 was
written based on
Sugiyama, M.
Statistical Pattern Recognition: Pattern Recognition Based on Generative
Models,
Ohmsha, Tokyo, Japan, 2009. (in Japanese),
and Part 4 and Part 5 were written based on
Sugiyama, M.
An Illustrated Guide to Machine Learning,
Kodansha, Tokyo, Japan, 2013. (in Japanese).
The author would like to thank researchers and students in his groups at the
University of Tokyo and Tokyo Institute of Technology for their valuable feedback
on earlier manuscripts.
Masashi Sugiyama
The University of Tokyo
xxxiii


PART
INTRODUCTION 1
1
STATISTICAL MACHINE LEARNING . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3


CHAPTER
STATISTICAL MACHINE
LEARNING
1
CHAPTER CONTENTS
Types of Learning ................................................................... 3
Examples of Machine Learning Tasks................................................ 4
Supervised Learning .......................................................... 4
Unsupervised Learning ........................................................ 5
Further Topics ................................................................. 6
Structure of This Textbook .......................................................... 8
Recent development of computers and the Internet allows us to immediately access a
vast amount of information such as texts, sounds, images, and movies. Furthermore,
a wide range of personal data such as search logs, purchase records, and diagnosis
history are accumulated everyday. Such a huge amount of data is called big data,
and there is a growing tendency to create new values and business opportunities
by extracting useful knowledge from data. This process is often called data mining,
and machine learning is the key technology for extracting useful knowledge. In this
chapter, an overview of the field of machine learning is provided.
1.1 TYPES OF LEARNING
Depending on the type of available data, machine learning can be categorized into
supervised learning, unsupervised learning, and reinforcement learning.
Supervised learning would be the most fundamental type of machine learning,
which considers a student learning from a supervisor through questioning and
answering. In the context of machine learning, a student corresponds to a computer
and a supervisor corresponds to a user of the computer, and the computer learns a
mapping from a question to its answer from paired samples of questions and answers.
The objective of supervised learning is to acquire the generalization ability, which
refers to the capability that an appropriate answer can be guessed for unlearned
questions. Thus, the user does not have to teach everything to the computer, but the
computer can automatically cope with unknown situations by learning only a fraction
of knowledge. Supervised learning has been successfully applied to a wide range
of real-world problems, such as hand-written letter recognition, speech recognition,
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00012-1
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
3

4
CHAPTER 1
STATISTICAL MACHINE LEARNING
image recognition, spam filtering, information retrieval, online advertisement, recom-
mendation, brain signal analysis, gene analysis, stock price prediction, weather fore-
casting, and astronomy data analysis. The supervised learning problem is particularly
called regression if the answer is a real value (such as the temperature), classification
if the answer is a categorical value (such as “yes” or “no”), and ranking if the answer
is an ordinal value (such as “good,” “normal,” or “poor”).
Unsupervised learning considers the situation where no supervisor exists and a
student learns by himself/herself. In the context of machine learning, the computer
autonomously collects data through the Internet and tries to extract useful knowledge
without any guidance from the user. Thus, unsupervised learning is more automatic
than supervised learning, although its objective is not necessarily specified clearly.
Typical tasks of unsupervised learning include data clustering and outlier detection,
and these unsupervised learning techniques have achieved great success in a wide
range of real-world problems, such as system diagnosis, security, event detection, and
social network analysis. Unsupervised learning is also often used as a preprocessing
step of supervised learning.
Reinforcement learning is aimed at acquiring the generalization ability in the
same way as supervised learning, but the supervisor does not directly give answers
to the student’s questions. Instead, the supervisor evaluates the student’s behavior
and gives feedback about it. The objective of reinforcement learning is, based on the
feedback from the supervisor, to let the student improve his/her behavior to maximize
the supervisor’s evaluation. Reinforcement learning is an important model of the
behavior of humans and robots, and it has been applied to various areas such as
autonomous robot control, computer games, and marketing strategy optimization.
Behind reinforcement learning, supervised and unsupervised learning methods such
as regression, classification, and clustering are often utilized.
The focus on this textbook is supervised learning and unsupervised learning. For
reinforcement learning, see references [99, 105]
1.2 EXAMPLES OF MACHINE LEARNING TASKS
In this section, various supervised and unsupervised learning tasks are introduced in
more detail.
1.2.1 SUPERVISED LEARNING
The objective of regression is to approximate a real-valued function from its samples
(Fig. 1.1). Let us denote the input by d-dimensional real vector x, the output by a real
scalar y, and the learning target function by y = f (x). The learning target function
f is assumed to be unknown, but its input-output paired samples {(xi, yi)}n
i=1 are
observed. In practice, the observed output value yi may be corrupted by some
noise ϵi, i.e., yi = f (xi) + ϵi. In this setup, xi corresponds to a question that a
student asks the supervisor, and yi corresponds to the answer that the supervisor
gives to the student. Noise ϵi may correspond to the supervisor’s mistake or

1.2 EXAMPLES OF MACHINE LEARNING TASKS
5
FIGURE 1.1
Regression.
FIGURE 1.2
Classification.
the student’s misunderstanding. The learning target function f corresponds to the
supervisor’s knowledge, which allows him/her to answer any questions. The objective
of regression is to let the student learn this function, by which he/she can also answer
any questions. The level of generalization can be measured by the closeness between
the true function f and its approximation f .
On the other hand, classification is a pattern recognition problem in a supervised
manner (Fig. 1.2). Let us denote the input pattern by d-dimensional vector x and its
class by a scalar y ∈{1,. . . ,c}, where c denotes the number of classes. For training
a classifier, input-output paired samples {(xi, yi)}n
i=1 are provided in the same way
as regression. If the true classification rule is denoted by y = f (x), classification
can also be regarded as a function approximation problem. However, an essential
difference is that there is no notion of closeness in y: y = 2 is closer to y = 1 than
y = 3 in the case of regression, but whether y and y′ are the same is the only concern
in classification.
The problem of ranking in supervised learning is to learn the rank y of a sample
x. Since the rank has the order, such as 1 < 2 < 3, ranking would be more similar to
regression than classification. For this reason, the problem of ranking is also referred
to as ordinal regression. However, different from regression, exact output value y
is not necessary to be predicted, but only its relative value is needed. For example,
suppose that “values” of three instances are 1, 2, and 3. Then, since only the ordinal
relation 1 < 2 < 3 is important in the ranking problem, predicting the values as
2 < 4 < 9 is still a perfect solution.
1.2.2 UNSUPERVISED LEARNING
Clustering is an unsupervised counter part of classification (Fig. 1.3), and its objective
is to categorize input samples {xi}n
i=1 into clusters 1,2,. . . ,c without any supervision
{yi}n
i=1. Usually, similar samples are supposed to belong to the same cluster, and

6
CHAPTER 1
STATISTICAL MACHINE LEARNING
FIGURE 1.3
Clustering.
FIGURE 1.4
Outlier detection.
dissimilar samples are supposed to belong to different clusters. Thus, how to measure
the similarity between samples is the key issue in clustering.
Outlier detection, which is also referred to as anomaly detection, is aimed at
finding irregular samples in a given data set {xi}n
i=1. In the same way as clustering,
the definition of similarity between samples plays a central role in outlier detection,
because samples that are dissimilar from others are usually regarded as outliers
(Fig. 1.4).
The objective of change detection, which is also referred to as novelty detection,
is to judge whether a newly given data set {x′
i′}n′
i′=1 has the same property as the
original data set {xi}n
i=1. Similarity between samples is utilized in outlier detection,
while similarity between data sets is needed in change detection. If n′ = 1, i.e., only
a single point is provided for detecting change, the problem of change detection may
be reduced to an outlier problem.
1.2.3 FURTHER TOPICS
In addition to supervised and unsupervised learnings, various useful techniques are
available in machine learning.
Input-output paired samples {(xi, yi)}n
i=1 are used for training in supervised
learning, while input-only samples {xi}n
i=1 are utilized in unsupervised learning.
In many supervised learning techniques, collecting input-only samples {xi}n
i=1 may
be easy, but obtaining output samples {yi}n
i=1 for {xi}n
i=1 is laborious. In such
a case, output samples may be collected only for m ≪n input samples, and
the remaining n −m samples are input-only. Semisupervised learning is aimed at
learning from both input-output paired samples {(xi, yi)}m
i=1 and input-only samples
{xi}n
i=m+1. Typically, semisupervised learning methods extract distributional infor-
mation such as cluster structure from the input-only samples {xi}n
i=m+1 and utilize
that information for improving supervised learning from input-output paired samples
{(xi, yi)}m
i=1.

1.2 EXAMPLES OF MACHINE LEARNING TASKS
7
FIGURE 1.5
Dimensionality reduction.
Given weak learning algorithms that perform only slightly better than a random
guess, ensemble learning is aimed at constructing a strong learning algorithm by
combining such weak learning algorithms. One of the most popular approaches is
voting by the weak learning algorithms, which may complement the weakness of
each other.
Standard learning algorithms consider vectorial data x. However, if data have a
two-dimensional structure such as an image, directly learning from matrix data would
be more promising than vectorizing the matrix data. Studies of matrix learning or
tensor learning are directed to handle such higher-order data.
When data samples are provided sequentially one by one, updating the learning
results to incorporate new data would be more natural than re-learning all data from
scratch. Online learning is aimed at efficiently handling such sequentially given data.
When solving a learning task, transferring knowledge from other similar learning
tasks would be helpful. Such a paradigm is called transfer learning or domain
adaptation. If multiple related learning tasks need to be solved, solving them
simultaneously would be more promising than solving them individually. This idea
is called multitask learning and can be regarded as a bidirectional variant of transfer
learning.
Learning from high-dimensional data is challenging, which is often referred to as
the curse of dimensionality. The objective of dimensionality reduction is to extract
essential information from high-dimensional data samples {xi}n
i=1 and obtain their
low-dimensional expressions {zi}n
i=1 (Fig. 1.5). In linear dimensionality reduction,
the low-dimensional expressions {zi}n
i=1 are obtained as zi = T xi using a fat matrix
T. Supervised dimensionality reduction tries to find low-dimensional expressions
{zi}n
i=1 as preprocessing, so that the subsequent supervised learning tasks can be
solved easily. On the other hand, unsupervised dimensionality reduction tries to find
low-dimensional expressions {zi}n
i=1 such that certain structure of original data is
maintained, for example, for visualization purposes. Metric learning is similar to
dimensionality reduction, but it has more emphasis on learning the metric in the
original high-dimensional space rather than reducing the dimensionality of data.

8
CHAPTER 1
STATISTICAL MACHINE LEARNING
1.3 STRUCTURE OF THIS TEXTBOOK
The main contents of this textbook consist of the following four parts.
Part 2 introduces fundamental concepts of statistics and probability, which will be
extensively utilized in the subsequent chapters. Those who are familiar with statistics
and probability or those who want to study machine learning immediately may skip
Part 2.
Based on the concept of statistics and probability, Part 3, Part 4, and Part 5
introduce various machine learning techniques. These parts are rather independent,
and therefore readers may start from any part based on their interests.
Part 3 targets the generative approach to statistical pattern recognition. The basic
idea of generative methods is that the probability distribution of data is modeled to
perform pattern recognition. When prior knowledge on data generation mechanisms
is available, the generative approach is highly useful. Various classification and
clustering techniques based on generative model estimation in the frequentist and
Bayesian frameworks will be introduced.
Part 4 focuses on the discriminative approach to statistical machine learning. The
basic idea of discriminative methods is to solve target machine learning tasks such
as regression and classification directly without modeling data-generating probability
distributions. If no prior knowledge is available for data generation mechanisms, the
discriminative approach would be more promising than the generative approach. In
addition to statistics and probability, knowledge of optimization theory also plays an
important role in the discriminative approach, which will also be covered.
Part 5 is devoted to introducing various advanced issues in machine learning, in-
cluding ensemble learning, online learning, confidence of prediction, semisupervised
learning, transfer learning, multitask learning, dimensionality reduction, clustering,
outlier detection, and change detection.
Compact MATLAB codes are provided for the methods introduced in Part 3,
Part 4, and Part 5. So readers can immediately test the algorithms and learn their
numerical behaviors.

PART
STATISTICS AND
PROBABILITY 2
2
RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS . . . . . . . . . . . 11
3
EXAMPLES OF DISCRETE PROBABILITY DISTRIBUTIONS . . . . . . . . . . . . 25
4
EXAMPLES OF CONTINUOUS PROBABILITY DISTRIBUTIONS . . . . . . . . 37
5
MULTIDIMENSIONAL PROBABILITY DISTRIBUTIONS . . . . . . . . . . . . . . . . 51
6
EXAMPLES OF MULTIDIMENSIONAL PROBABILITY DISTRIBUTIONS . . 61
7
SUM OF INDEPENDENT RANDOM VARIABLES . . . . . . . . . . . . . . . . . . . . . . . 73
8
PROBABILITY INEQUALITIES. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
9
STATISTICAL ESTIMATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
10
HYPOTHESIS TESTING. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99

10
PART 2
STATISTICS AND PROBABILITY
Probability and statistics are important mathematical tools used in the state-
of-the-art machine learning methods, and are becoming indispensable subjects of
science in the era of big data. Part 2 of this book is devoted to providing fundamentals
of probability and statistics.
Chapter 2 overviews the basic notions of random variables and probability
distributions. Chapter 3 and Chapter 4 illustrate examples of discrete and contin-
uous probability distributions, respectively. Chapter 5 introduces concepts used in
multidimensional data analysis, and Chapter 6 give examples of multidimensional
probability distributions. Chapter 7 discusses the asymptotic behavior of the sum
of independent random variables, and Chapter 8 shows various inequalities related to
random variables. Finally, Chapter 9 and Chapter 10 cover fundamentals of statistical
estimation and hypothesis testing, respectively.

CHAPTER
RANDOM VARIABLES
AND PROBABILITY
DISTRIBUTIONS
2
CHAPTER CONTENTS
Mathematical Preliminaries ....................................................... 11
Probability ........................................................................ 13
Random Variable and Probability Distribution ..................................... 14
Properties of Probability Distributions ............................................. 16
Expectation, Median, and Mode ............................................ 16
Variance and Standard Deviation............................................ 18
Skewness, Kurtosis, and Moments .......................................... 19
Transformation of Random Variables .............................................. 22
In this chapter, the notions of random variables and probability distributions are
introduced, which form the basis of probability and statistics. Then simple statistics
that summarize probability distributions are discussed.
2.1 MATHEMATICAL PRELIMINARIES
When throwing a six-sided die, the possible outcomes are only 1, 2, 3, 4, 5, 6, and
no others. Such possible outcomes are called sample points and the set of all sample
points is called the sample space.
An event is defined as a subset of the sample space. For example, event A that any
odd number appears is expressed as
A = {1,3,5}.
The event with no sample point is called the empty event and denoted by ∅. An
event consisting only of a single sample point is called an elementary event, while
an event consisting of multiple sample points is called a composite event. An event
that includes all possible sample points is called the whole event. Below, the notion
of combining events is explained using Fig. 2.1.
The event that at least one of the events A and B occurs is called the union of
events and denoted by A ∪B. For example, the union of event A that an odd number
appears and event B that a number less than or equal to three appears is expressed as
A ∪B = {1,3,5} ∪{1,2,3} = {1,2,3,5}.
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00013-3
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
11

12
CHAPTER 2
RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS
(a) Event A
(b) Event B
(c) Complementary event Ac
(d) Union of events
(e) Intersection of events
(f) Disjoint events
(g) Distributive law 1
(h) Distributive law 2
(i) De Morgan’s law 1
(j) De Morgan’s law 2
FIGURE 2.1
Combination of events.
On the other hand, the event that both events A and B occur simultaneously is called
the intersection of events and denoted by A ∩B. The intersection of the above events
A and B is given by
A ∩B = {1,3,5} ∩{1,2,3} = {1,3}.
If events A and B never occur at the same time, i.e.,
A ∩B = ∅,

2.2 PROBABILITY
13
events A and B are called disjoint events. The event that an odd number appears
and the event that an even number appears cannot occur simultaneously and thus are
disjoint. For events A, B, and C, the following distributive laws hold:
(A ∪B) ∩C = (A ∩C) ∪(B ∩C),
(A ∩B) ∪C = (A ∪C) ∩(B ∪C).
The event that event A does not occur is called the complementary event of A and
denoted by Ac. The complementary event of the event that an odd number appears is
that an odd number does not appear, i.e., an even number appears. For the union and
intersection of events A and B, the following De Morgan’s laws hold:
(A ∪B)c = Ac ∩Bc,
(A ∩B)c = Ac ∪Bc.
2.2 PROBABILITY
Probability is a measure of likeliness that an event will occur and the probability that
event A occurs is denoted by Pr(A). A Russian mathematician, Kolmogorov, defined
the probability by the following three axioms as abstraction of the evident properties
that the probability should satisfy.
1. Non-negativity: For any event Ai,
0 ≤Pr(Ai) ≤1.
2. Unitarity: For entire sample space Ω,
Pr(Ω) = 1.
3. Additivity: For any countable sequence of disjoint events A1, A2,. . .,
Pr(A1 ∪A2 ∪· · · ) = Pr(A1) + Pr(A2) + · · · .
From the above axioms, events A and B are shown to satisfy the following
additive law:
Pr(A ∪B) = Pr(A) + Pr(B) −Pr(A ∩B).
This can be extended to more than two events: for events A, B, and C,
Pr(A ∪B ∪C) = Pr(A) + Pr(B) + Pr(C)
−Pr(A ∩B) −Pr(A ∩C) −Pr(B ∩C)
+ Pr(A ∩B ∩C).

14
CHAPTER 2
RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS
FIGURE 2.2
Examples of probability mass function. Outcome
of throwing a fair six-sided dice (discrete uniform
distribution U{1,2,. . . ,6}).
2.3 RANDOM VARIABLE AND PROBABILITY
DISTRIBUTION
A variable is called a random variable if probability is assigned to each realization
of the variable. A probability distribution is the function that describes the mapping
from any realized value of the random variable to probability.
A countable set is a set whose elements can be enumerated as 1,2,3,. . .. A
random variable that takes a value in a countable set is called a discrete random
variable. Note that the size of a countable set does not have to be finite but can be
infinite such as the set of all natural numbers. If probability for each value of discrete
random variable x is given by
Pr(x) = f (x),
f (x) is called the probability mass function. Note that f (x) should satisfy
∀x, f (x) ≥0,
and

x
f (x) = 1.
The outcome of throwing a fair six-sided die, x ∈{1,2,3,4,5,6}, is a discrete random
variable, and its probability mass function is given by f (x) = 1/6 (Fig. 2.2).
A random variable that takes a continuous value is called a continuous random
variable. If probability that continuous random variable x takes a value in [a,b] is
given by
Pr(a ≤x ≤b) =
b
a
f (x)dx,
(2.1)

2.3 RANDOM VARIABLE AND PROBABILITY DISTRIBUTION
15
(a) Probability density function f (x)
(b) Cumulative distribution function F(x)
FIGURE 2.3
Example of probability density function and its cumulative distribution function.
f (x) is called a probability density function (Fig. 2.3(a)). Note that f (x) should satisfy
∀x, f (x) ≥0,
and

f (x)dx = 1.
For example, the outcome of spinning a roulette, x ∈[0,2π), is a continuous random
variable, and its probability density function is given by f (x) = 1/(2π). Note that
Eq. (2.1) also has an important implication, i.e., the probability that continuous
random variable x exactly takes value b is actually zero:
Pr(b ≤x ≤b) =
b
b
f (x)dx = 0.
Thus, the probability that the outcome of spinning a roulette is exactly a particular
angle is zero.
The probability that continuous random variable x takes a value less than or equal
to b,
F(b) = Pr(x ≤b) =
b
−∞
f (x)dx,
is called the cumulative distribution function (Fig. 2.3(b)). The cumulative distribu-
tion function F satisfies the following properties:
•
Monotone nondecreasing: x < x′ implies F(x) ≤F(x′).
•
Left limit: limx→−∞F(x) = 0.
•
Right limit: limx→+∞F(x) = 1.
If the derivative of a cumulative distribution function exists, it agrees with the
probability density function:
F′(x) = f (x).

16
CHAPTER 2
RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS
FIGURE 2.4
Expectation is the average of x weighted according to f (x), and median
is the 50% point both from the left-hand and right-hand sides. α-quantile
for 0 ≤α ≤1 is a generalization of the median that gives the 100α%
point from the left-hand side. Mode is the maximizer of f (x).
Pr(a ≤x) is called the upper-tail probability or the right-tail probability, while
Pr(x ≤b) is called the lower-tail probability or the left-tail probability. The upper-tail
and lower-tail probabilities together are called the two-sided probability, and either
of them is called a one-sided probability.
2.4 PROPERTIES OF PROBABILITY DISTRIBUTIONS
When discussing properties of probability distributions, it is convenient to have
simple statistics that summarize probability mass/density functions. In this section,
such statistics are introduced.
2.4.1 EXPECTATION, MEDIAN, AND MODE
The expectation is the value that a random variable is expected to take (Fig. 2.4). The
expectation of random variable x, denoted by E[x], is defined as the average of x
weighted according to probability mass/density function f (x):
Discrete: E[x] =

x
x f (x),
Continuous: E[x] =

x f (x)dx.
Note that, as explained in Section 4.5, there are probability distributions such as the
Cauchy distribution that the expectation does not exist (diverges to infinity).
The expectation can be defined for any function ξ of x similarly:
Discrete: E[ξ(x)] =

x
ξ(x) f (x),

2.4 PROPERTIES OF PROBABILITY DISTRIBUTIONS
17
FIGURE 2.5
Income distribution. The expectation is 62.1 thousand dollars, while the
median is 31.3 thousand dollars.
Continuous: E[ξ(x)] =

ξ(x) f (x)dx.
For constant c, the expectation operator E satisfies the following properties:
E[c] = c,
E[x + c] = E[x] + c,
E[cx] = cE[x].
Although the expectation represents the “center” of a probability distribution, it
can be quite different from what is intuitively expected in the presence of outliers. For
example, in the income distribution illustrated in Fig. 2.5, because one person earns
1 million dollars, all other people are below the expectation, 62.1 thousand dollars.
In such a situation, the median is more appropriate than the expectation, which is
defined as b such that
Pr(x ≤b) = 1/2.
That is, the median is the “center” of a probability distribution in the sense that it is
the 50% point both from the left-hand and right-hand sides. In the example of Fig. 2.5,
the median is 31.3 thousand dollars and it is indeed in the middle of everybody.
The α-quantile for 0 ≤α ≤1 is a generalization of the median that gives b such
that
Pr(x ≤b) = α.
That is, the α-quantile gives the 100α% point from the left-hand side (Fig. 2.4) and
is reduced to the median when α = 0.5.

18
CHAPTER 2
RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS
Let us consider a probability density function f defined on a finite interval [a,b].
Then the minimizer of the expected squared error, defined by
E

(x −y)2
=
b
a
(x −y)2 f (x)dx,
with respect to y is shown to agree with the expectation of x. Similarly, the minimizer
y of the expected absolute error, defined by
E

|x −y|

=
b
a
|x −y| f (x)dx,
(2.2)
with respect to y is shown to agree with the expectation of x. Furthermore, a weighted
variant of Eq. (2.2),
b
a
|x −y|α f (x)dx,
|x −y|α =

(1 −α)(x −y)
(x > y),
α(y −x)
(x ≤y),
is minimized with respect to y by the α-quantile of x.
Another popular statistic is the mode, which is defined as the maximizer of f (x)
(Fig. 2.4).
2.4.2 VARIANCE AND STANDARD DEVIATION
Although the expectation is a useful statistic to characterize probability distributions,
probability distributions can be different even when they share the same expectation.
Here, another statistic called the variance is introduced to represent the spread of the
probability distribution.
The variance of random variable x, denoted by V[x], is defined as
V[x] = E

(x −E[x])2
.
In practice, expanding the above expression as
V[x] = E

x2 −2xE[x] + (E[x])2
= E[x2] −(E[x])2
often makes the computation easier. For constant c, variance operator V satisfies the
following properties:
V[c] = 0,
V[x + c] = V[x],
V[cx] = c2V[x].
Note that these properties are quite different from those of the expectation.

2.4 PROPERTIES OF PROBABILITY DISTRIBUTIONS
19
The square root of the variance is called the standard deviation and is denoted by
D[x]:
D[x] =

V[x].
Conventionally, the variance and the standard deviation are denoted by σ2 and σ,
respectively.
2.4.3 SKEWNESS, KURTOSIS, AND MOMENTS
In addition to the expectation and variance, higher-order statistics such as the
skewness and kurtosis are also often used. The skewness and kurtosis represent
asymmetry and sharpness of probability distributions, respectively, and defined as
Skewness: E (x −E[x])3
(D[x])3
,
Kurtosis: E (x −E[x])4
(D[x])4
−3.
(D[x])3 and (D[x])4 in the denominators are for normalization purposes and −3
included in the definition of the kurtosis is to zero the kurtosis of the normal
distribution (see Section 4.2). As illustrated in Fig. 2.6, the right tail is longer than
the left tail if the skewness is positive, while the left tail is longer than the right tail
if the skewness is negative. The distribution is perfectly symmetric if the skewness is
zero. As illustrated in Fig. 2.7, the probability distribution is sharper than the normal
distribution if the kurtosis is positive, while the probability distribution is duller than
the normal distribution if the kurtosis is positive.
The above discussions imply that the statistic,
νk = E

(x −E[x])k
,
plays an important role in characterizing probability distributions. νk is called the kth
moment about the expectation, while
µk = E[xk]
is called the kth moment about the origin. The expectation, variance, skewness, and
kurtosis can be expressed by using µk as
Expectation: µ1,
Variance: µ2 −µ2
1,
Skewness:
µ3 −3µ2µ1 + 2µ3
1
(µ2 −µ2
1)
3
2
,
Kurtosis:
µ4 −4µ3µ1 + 6µ2µ2
1 −3µ4
1
(µ2 −µ2
1)2
−3.

20
CHAPTER 2
RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS
(a) Skewness: −0.32
(b) Skewness: 0
(c) Skewness: 0.32
FIGURE 2.6
Skewness.
(a) Kurtosis: −1.2
(b) Kurtosis: 0
(c) Kurtosis: 3
FIGURE 2.7
Kurtosis.
Probability distributions will be more constrained if the expectation, variance,
skewness, and kurtosis are specified. As the limit, if the moments of all orders
are specified, the probability distribution is uniquely determined. The moment-
generating function allows us to handle the moments of all orders in a systematic
way:
Mx(t) = E[et x] =


x
et x f (x)
(Discrete),

et x f (x)dx
(Continuous).
Indeed, substituting zero to the kth derivative of the moment-generating function with
respect to t, M(k)
x (t), gives the kth moment:
M(k)
x (0) = µk.
Below, this fact is proved.

2.4 PROPERTIES OF PROBABILITY DISTRIBUTIONS
21
The value of function g at point t can be expressed as
g(t) = g(0) + t g′(0)
1!
+ t2 g′′(0)
2!
+ · · · .
If higher-order terms in the right-hand side are ignored and the infinite sum
is approximated by a finite sum, an approximation to g(t) can be obtained.
When only the first-order term g(0) is used, g(t) is simply approximated
by g(0), which is too rough. However, when the second-order term tg′(0)
is included, the approximation gets better, as illustrated below. By further
including higher-order terms, the approximation gets more accurate and
converges to g(t) if all terms are included.
FIGURE 2.8
Taylor series expansion at the origin.
Given that the kth derivative of function et x with respect to t is xket x, the Taylor
series expansion (Fig. 2.8) of function et x at the origin with respect to t yields
et x = 1 + (tx) + (tx)2
2!
+ (tx)3
3!
+ · · · .
Taking the expectation of both sides gives
E[et x] = Mx(t) = 1 + tµ1 + t2 µ2
2! + t3 µ3
3! + · · · .
Taking the derivative of both sides yields
M′
x(t) = µ1 + µ2t + µ3
2! t2 + µ4
3! t3 + · · · ,
M′′
x (t) = µ2 + µ3t + µ4
2! t2 + µ5
3! t3 + · · · ,
...
M(k)
x (t) = µk + µk+1t + µk+2
2! t2 + µk+3
3! t3 + · · · .
Substituting zero into this gives M(k)
x (0) = µk.

22
CHAPTER 2
RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS
Depending on probability distributions, the moment-generating function does not
exist (diverges to infinity). On the other hand, its sibling called the characteristic
function always exists:
φx(t) = Mix(t) = Mx(it),
where i denotes the imaginary unit such that i2 = −1. The characteristic function
corresponds to the Fourier transform of a probability density function.
2.5 TRANSFORMATION OF RANDOM VARIABLES
If random variable x is transformed as
r = ax + b,
the expectation and variance of r are given by
E[r] = aE[x] + b and V[r] = a2V[x].
Setting a =
1
D[x] and b = −E[x]
D[x] yields
z =
x
D[x] −E[x]
D[x] = x −E[x]
D[x]
,
which has expectation 0 and variance 1. This transformation from x to z is called
standardization.
Suppose that random variable x that has probability density f (x) defined on X is
obtained by using transformation ξ as
x = ξ(r).
Then the probability density function of z is not simply given by f  ξ(r), because
f  ξ(r) is not integrated to 1 in general. For example, when x is the height of a
person in centimeter and r is its transformation in meter, f  ξ(r) should be divided
by 100 to be integrated to 1.
More generally, as explained in Fig. 2.9, if the Jacobian dx
dr is not zero, the scale
should be adjusted by multiplying the absolute Jacobian as
g(r) = f  ξ(r) 
dx
dr

.
g(r) is integrated to 1 for any transform x = ξ(r) such that dx
dr , 0.

2.5 TRANSFORMATION OF RANDOM VARIABLES
23
Integration of function f (x) over X can be expressed by using function g(r)
on R such that
x = g(r) and X = g(R)
as

X
f (x)dx =

R
f  g(r) 
dx
dr

dr.
This allows us to change variables of integration from x to r.  dx
dr
 in the right-
hand side corresponds to the ratio of lengths when variables of integration are
changed from x to r. For example, for
f (x) = x and X = [2,3],
integration of function f (x) over X is computed as

X
f (x)dx =
3
2
xdx =
1
2 x2
3
2
= 5
2.
On the other hand, g(r) = r2 yields
R = [
√
2,
√
3],
f  g(r) = r2,
and
dx
dr = 2r.
This results in

R
f  g(r) 
dx
dr

dr =

√
3
√
2
r2 · 2rdr =
1
2r4
√
3
√
2
= 5
2.
FIGURE 2.9
One-dimensional change of variables in integration. For multidimensional cases, see Fig. 4.2.
For linear transformation
r = ax + b and a , 0,
x = r −b
a
yields dx
dr = 1
a, and thus
g(r) = 1
|a| f
r −b
a

is obtained.


CHAPTER
EXAMPLES OF
DISCRETE
PROBABILITY
DISTRIBUTIONS
3
CHAPTER CONTENTS
Discrete Uniform Distribution ..................................................... 25
Binomial Distribution ............................................................. 26
Hypergeometric Distribution....................................................... 27
Poisson Distribution............................................................... 31
Negative Binomial Distribution .................................................... 33
Geometric Distribution ............................................................ 35
In this chapter, popular discrete probability distributions and their properties such as
the expectation, the variance, and the moment-generating functions are illustrated.
3.1 DISCRETE UNIFORM DISTRIBUTION
The discrete uniform distribution is the probability distribution for N events {1,. . . ,
N} that occur with equal probability. It is denoted by U{1,. . . , N}, and its probability
mass function is given by
f (x) = 1
N
for x = 1,. . . , N.
From the series formulas,
N

x=1
x = N(N + 1)
2
and
N

x=1
x2 = N(N + 1)(2N + 1)
6
,
the expectation and variance of U{1,. . . , N} can be computed as
E[x] = N + 1
2
and V[x] = N2 −1
12
.
The probability mass function of U{1,. . . ,6} is plotted in Fig. 2.2.
The probability mass function of the discrete uniform distribution U{a,a +
1,. . . ,b} for finite a < b is given by
f (x) =
1
b −a + 1
for x = a,a + 1,. . . ,b.
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00014-5
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
25

26
CHAPTER 3
EXAMPLES OF DISCRETE PROBABILITY DISTRIBUTIONS
FIGURE 3.1
Probability mass functions of binomial distribution Bi(n,p).
Its expectation and variance are given by
E[x] = a + b
2
and V[x] = (b −a + 1)2 −1
12
.
3.2 BINOMIAL DISTRIBUTION
Bernoulli trials are independent repeated trials of an experiment with two possible
outcomes, say success and failure. Repeated independent tosses of the same coin are
typical Bernoulli trials. Let p be the probability of success (getting heads in the coin
toss) and q (= 1 −p) be the probability of failure (getting tails in the coin toss). The
binomial distribution is the probability distribution of the number x of successful
trials in n Bernoulli trials and is denoted by Bi(n,p).
The probability of having x successful trials is given by px, while the probability
of having n −x unsuccessful trials is given by qn−x. The number of combinations of
x successful trials and n −x unsuccessful trials is given by
n!
x!(n−x)!, where
n! = n × (n −1) × · · · × 2 × 1
denotes the factorial.
n!
x!(n−x)! is called the binomial coefficient and is denoted by  n
x
:
n
x

=
n!
x!(n −x)!.
Putting them together based on the axioms of the probability provided in Section 2.2,
the probability mass function of Bi(n,p) is given by
f (x) = pxqn−x
n
x

for x = 0,1,. . . , N.
Probability mass functions of the binomial distribution for n = 10 are illustrated in
Fig. 3.1.

3.3 HYPERGEOMETRIC DISTRIBUTION
27
FIGURE 3.2
Sampling from a bag. The bag contains N balls which consist of
M < N balls labeled as “A” and N −M balls labeled as “B.” n
balls are sampled from the bag, which consists of x balls labeled
as “A” and n −x balls labeled as “B.”
The moment-generating function of Bi(n,p) can be obtained by using the
binomial theorem,
(p + q)n =
n

x=0
n
x

pxqn−x,
as
Mx(t) =
n

x=0
et x
n
x

pxqn−x =
n

x=0
n
x

(pet)xqn−x
= (pet + q)n.
From this, the expectation and variance of Bi(n,p) can be computed as
E[x] = np and V[x] = npq.
The expectation np would be intuitive because a Bernoulli trial with probability of
success p is repeated n times. The variance npq is maximized when p = 0.5, while
it is minimized when p = 0 or p = 1. This is also intuitive because it is difficult to
predict the success or failure of a trial when the probability of success is 0.5.
The binomial distribution with n = 1, Bi(1,p), is specifically called the Bernoulli
distribution.
3.3 HYPERGEOMETRIC DISTRIBUTION
Let us consider the situation illustrated in Fig. 3.2: n balls are sampled from the bag
containing N balls, where M balls are labeled as “A” and N −M balls are labeled

28
CHAPTER 3
EXAMPLES OF DISCRETE PROBABILITY DISTRIBUTIONS
(a) Sampling with replacement
(b) Sampling without replacement
FIGURE 3.3
Sampling with and without replacement. The sampled ball is re-
turned to the bag before the next ball is sampled in sampling with
replacement, while the next ball is sampled without returning the
previously sampled ball in sampling without replacement.
as “B.” In this situation, there are two sampling schemes, as illustrated in Fig. 3.3:
The first scheme is called sampling with replacement, which requires to return the
sampled ball to the bag before the next ball is sampled. The other scheme is called
sampling without replacement, where the next ball is sampled without returning the
previously sampled ball to the bag.
In sampling with replacement, a ball is always sampled from the bag containing
all N balls. This sampling process corresponds to the Bernoulli trials, and thus the
probability distribution of obtaining x balls labeled as “A” when n balls are sampled
with replacement is given by Bi(n, M/N).
On the other hand, in sampling without replacement, the number of balls
contained in the bag is decreasing as the sampling process is progressed. Thus, the
ratio of balls labeled as “A” and “B” in the bag depends on the history of sampling.
The probability distribution of obtaining x balls labeled as “A” when n balls are
sampled without replacement is called the hypergeometric distribution and denoted
by HG(N, M,n).
The number of combinations of sampling x balls labeled as “A” from M balls is
given by  M
x
, the number of combinations of sampling n−x balls labeled as “B” from
N −M balls is given by  N−M
n−x
, and the number of combinations of sampling n balls
from N balls is given by  N
n
. Putting them together, the probability mass function of
HG(N, M,n) is given by
f (x) =
 M
x
 N−M
n−x

 N
n

for x = 0,1,. . . ,n.
Although the domain of x is {0,1,. . . ,n}, the actual range of x is given by
{max(0,n −(N −M)),. . . ,min(n, M)} ,
because the number of balls in the bag is limited.

3.3 HYPERGEOMETRIC DISTRIBUTION
29
FIGURE 3.4
Probability mass functions of hypergeometric distribution HG(N, M,n).
FIGURE 3.5
Probability mass functions of Bi(n, M/N) and HG(N, M,n) for N = 100, M = 90, and n = 90.
For N = 100 and n = 10, the probability mass functions of HG(N, M,n) and
Bi(n, M/N) are plotted in Fig. 3.4 and Fig. 3.1, respectively. These graphs show that
the probability mass functions are quite similar to each other, meaning that sampling
with or without replacement does not affect that much when only n = 10 balls are
sampled from N = 100 balls. Indeed, as N and M tend to infinity under n and the
ratio N/M fixed, HG(N, M,n) is shown to agree with Bi(n, M/N).
Next, let us sample n = 90 from the bag of N = 100 balls where M = 90 balls
are labeled as “A.” In this situation, even though the probability is very low, sampling
with replacement can result in selecting one of the 10 balls labeled as “B” in all
90 trials. On the other hand, sampling without replacement results in selecting balls
labeled as “B” at most 10 times and balls labeled as “A” at least 80 times. Thus, in
this situation, the probability mass functions of the hypergeometric distribution and
the binomial distribution are quite different, as illustrated in Fig. 3.5.

30
CHAPTER 3
EXAMPLES OF DISCRETE PROBABILITY DISTRIBUTIONS
The expectation and variance of HG(N, M,n) are given by
E[x] = nM
N
and V[x] = nM(N −M)(N −n)
N2(N −1)
,
which are proved below.
The expectation E[x] can be expressed as
E[x] =
1
 N
n

n

x=0
x
M
x
N −M
n −x

=
1
 N
n

n

x=1
x
M
x
N −M
n −x

(the term with x = 0 is zero)
= M
 N
n

n

x=1
M −1
x −1
N −M
n −x

M
x

= M
x
M −1
x −1

= M
 N
n

n−1

x=0
M −1
x
N −M
n −x −1

(let x ←x −1)
= nM
N
1
 N−1
n−1

n−1

x=0
M −1
x
N −M
n −x −1

N
n

= N
n
N −1
n −1

.
(3.1)
Since the probability mass function satisfies 
x f (x) = 1,
N
n

=
n

x=0
M
x
N −M
n −x

.
(3.2)
Letting M ←M −1, N ←N −1, and n ←n −1 in Eq. (3.2) yields
N −1
n −1

=
n−1

x=0
M −1
x
N −M
n −x −1

,
and substituting this into Eq. (3.1) gives E[x] = nM
N .
The variance V[x] can be expressed as
V[x] = E[x(x −1)] + E[x] −(E[x])2.
(3.3)
Similar derivation to the expectation and using Eq. (3.2) yield
E[x(x −1)] = n(n −1)M(M −1)
N(N −1)
,
and substituting this into Eq. (3.3) gives V[x] = nM(N−M)(N−n)
N 2(N−1)
.
The moment-generating function of HG(N, M,n) is given by
Mx(t) = E[et x] =
 N−M
n

 N
n
 F(−n,−M, N −M −n + 1,et),

3.4 POISSON DISTRIBUTION
31
where
F(a,b,c,d) =
∞

x=0
(a)x(b)x
(c)x
dx
x! ,
(a)x =

a(a + 1) · · · (a + x −1)
(x > 0)
1
(x = 0)
is the hypergeometric series. The name, the hypergeometric distribution, stems
from the fact that its moment-generating function can be expressed using the
hypergeometric series.
3.4 POISSON DISTRIBUTION
If the probability of success in Bernoulli trials is very small, every trial is almost
always failed. However, even if the probability of success, p, is extremely small, the
Bernoulli trials will succeed a small number of times, as long as the number of trials,
n, is large enough. Indeed, given that the expectation of binomial distribution Bi(n,p)
is np, repeating the Bernoulli trials n = 10000000 times with probability of success
p = 0.0000003 yields three successful trials on average:
np = 10000000 × 0.0000003 = 3.
This implies that the probability that the number of successful trials x is nonzero
may not be that small, if the number of trials n is large enough. More precisely, given
that the probability mass function of Bi(n,p) is
f (x) =
n
x

px(1 −p)n−x,
the probability for x = 5 is
10000000
5

(0.0000003)5(0.9999997)9999995.
However, calculating (0.9999997)9999995 requires 9999995-time multiplications of
0.9999997, which is computationally expensive. At a glance, the use of approxima-
tion 0.9999997 ≈1 works fine:
(0.9999997)9999995 ≈19999995 = 1.
However, the correct value is
(0.9999997)9999995 ≈0.0498 ≪1,
and thus the above approximation is very poor.

32
CHAPTER 3
EXAMPLES OF DISCRETE PROBABILITY DISTRIBUTIONS
This problem can be overcome by applying Poisson’s law of small numbers: for
p = λ/n,
lim
n→∞
n
x

px(1 −p)n−x = e−λλx
x!
.
Let us prove this. First, the left-hand side of the above equation can be expressed as
lim
n→∞
n
x
λ
n
x 
1 −λ
n
n−x
= lim
n→∞
n!
x!(n −x)!
λ
n
x 
1 −λ
n
n−x
= λx
x! lim
n→∞
n!
(n −x)!nx

1 −λ
n
n 
1 −λ
n
−x
.
(3.4)
Here, it holds that
lim
n→∞
n!
(n −x)!nx = lim
n→∞
n
n × n −1
n
× · · · × n −x + 1
n
= lim
n→∞1 ×
1 −1
n
1
× · · · ×
1 −x
n + 1
n
1
= 1.
Also, setting t = −λ
n in the definition of the Euler number e,
e = lim
t→0 (1 + t)
1
t ,
(3.5)
yields
lim
n→∞

1 −λ
n
n
= e−λ.
Furthermore,
lim
n→∞

1 −λ
n
−x
= 1
holds. Putting them together yields
lim
n→∞
n
x
λ
n
x 
1 −λ
n
n−x
= e−λλx
x!
.
The Poisson distribution, denoted by Po(λ), is the probability distribution whose
probability mass function is given by
f (x) = e−λλx
x!
.
(3.6)

3.5 NEGATIVE BINOMIAL DISTRIBUTION
33
Since this corresponds to the binomial distribution with p = λ/n, if 1/n is regarded
as time and an event occurs λ times on average in unit time, f (x) corresponds to the
probability that the event occurs x times in unit time.
Eq. (3.6) is non-negative and the Taylor series expansion of the exponential
function at the origin,
eλ = 1 + λ1
1! + λ2
2! + · · · =
∞

x=0
λx
x! ,
(3.7)
yields
∞

x=0
f (x) =
∞

x=0
e−λλx
x!
= e−λ
∞

x=0
λx
x! = e−λeλ = 1.
Thus, Eq. (3.6) is shown to be the probability mass function.
The moment-generating function of Po(λ) is given by
Mx(t) = E[et x] =
∞

x=0
et xe−λλx
x!
= exp

λ(et −1)

,
where Eq. (3.7) is used for the derivation. From this, the expectation and variance of
Po(λ) are obtained as
E[x] = λ
and V[x] = λ.
Interestingly, the expectation and variance of the Poisson distribution are equal.
It was shown in Section 3.2 that the expectation and variance of the binomial
distribution (i.e., without applying Poisson’s law of small numbers) are given by
E[x] = np and V[x] = np(1 −p).
Setting p = λ/n yields
lim
n→∞np = λ
and
lim
n→∞np(1 −p) = λ,
which imply that application of Poisson’s law of small numbers does not essentially
change the expectation and variance.
Probability mass functions of Po(λ) are illustrated in Fig. 3.6, showing that the
expectation and variance grow as λ is increased.
3.5 NEGATIVE BINOMIAL DISTRIBUTION
Let us consider Bernoulli trials with probability of success p. Then the number of
unsuccessful trials x until the kth success is obtained follows the negative binomial
distribution, which is denoted by NB(k,p).

34
CHAPTER 3
EXAMPLES OF DISCRETE PROBABILITY DISTRIBUTIONS
FIGURE 3.6
Probability mass functions of Poisson distribution Po(λ).
FIGURE 3.7
Probability mass functions of negative binomial distribution NB(k,p).
Since the kth success is obtained at the (k + x)th trial, the (k + x)th trial is always
successful. On the other hand, the number of combinations of x unsuccessful trials in
the first (k + x −1) trials is given by  k+x−1
x
. Putting them together, the probability
mass function of NB(k,p) is given by
f (x) =
k + x −1
x

pk(1 −p)x.
(3.8)
Probability mass functions of NB(k,p) for p = 0.4 are illustrated in Fig. 3.7.
The binomial coefficient  r
x
 can be extended to negative number r = −k < 0:
−k
x

= (−k −x + 1)(−k −x + 2) · · · (−k −1)(−k)
x(x −1) · · · 2 · 1
.
With this negative binomial coefficient, the probability mass function defined in(3.8)
can be expressed as
f (x) = (k + x −1)(k + x −2) · · · (k + 1)k
x(x −1) · · · 2 · 1
pk(1 −p)x
= (−1)x
−k
x

pk(1 −p)x.
(3.9)
The name, the negative binomial distribution, stems from this fact.

3.6 GEOMETRIC DISTRIBUTION
35
FIGURE 3.8
Probability mass functions of geometric distribution Ge(p).
The binomial theorem explained in Section 3.2 can also be generalized to negative
numbers as
∞

x=0
−k
x

tx = (1 + t)−k.
Setting t = p −1 in the above equation yields that Eq. (3.9) satisfies
∞

x=0
f (x) = pk
∞

x=0
−k
x

(p −1)x = 1.
This generalized binomial theorem allows us to obtain the moment-generating
function of NB(k,p) as
Mx(t) = E[et x] =
∞

x=0
et x
−k
x

pk(p −1)x
= pk
∞

x=0
−k
x

{(p −1)et}x =

p
1 −(1 −p)et
k
.
From this, the expectation and variance of NB(k,p) are obtained as
E[x] = k(1 −p)
p
and V[x] = k(1 −p)
p2
.
The negative binomial distribution is also referred to as the Pascal distribution.
3.6 GEOMETRIC DISTRIBUTION
Let us consider Bernoulli trials with probability of success p. Then the number of
unsuccessful trials x until the first success is obtained follows the geometric distri-
bution, which is denoted by Ge(p). The geometric distribution Ge(p) is equivalent to

36
CHAPTER 3
EXAMPLES OF DISCRETE PROBABILITY DISTRIBUTIONS
the negative binomial distribution NB(k,p) with k = 1. Thus, its probability mass
function is given by
f (x) = p(1 −p)x,
where the probability decreases exponentially as x increases. Probability mass
functions of Ge(p) are illustrated in Fig. 3.8.
Given that Ge(p) is equivalent to NB(1,p), its moment-generating function is
given by
Mx(t) =
p
1 −(1 −p)et ,
and the expectation and variance are given by
E[x] = 1 −p
p
and V[x] = 1 −p
p2 .

CHAPTER
EXAMPLES OF
CONTINUOUS
PROBABILITY
DISTRIBUTIONS
4
CHAPTER CONTENTS
Continuous Uniform Distribution .................................................. 37
Normal Distribution ............................................................... 37
Gamma Distribution, Exponential Distribution, and Chi-Squared Distribution ...... 41
Beta Distribution.................................................................. 44
Cauchy Distribution and Laplace Distribution...................................... 47
t-Distribution and F-Distribution .................................................. 49
In this chapter, popular continuous probability distributions and their properties such
as the expectation, the variance, and the moment-generating functions are illustrated.
4.1 CONTINUOUS UNIFORM DISTRIBUTION
The continuous uniform distribution has a constant probability density over a finite
interval [a,b]:
f (x) =

1
b−a
(a ≤x ≤b),
0
(otherwise).
The continuous uniform distribution is denoted by U(a,b), and its expectation and
variance are given by
E[x] = a + b
2
and V[x] = (b −a)2
12
.
4.2 NORMAL DISTRIBUTION
The normal distribution, also known as the Gaussian distribution, would be the
most important continuous distribution. For −∞< µ < ∞and σ > 0, the normal
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00015-7
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
37

38
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
distribution is denoted by N(µ,σ2), and its probability density is given by
f (x) =
1
σ
√
2π
exp

−(x −µ)2
2σ2

.
The fact that the above f (x) is integrated to one can be proven by the Gaussian
integral shown in Fig. 4.1 as
∞
−∞
f (x)dx =
1
σ
√
2π
∞
−∞
exp

−(x −µ)2
2σ2

dx
= σ
√
2
σ
√
2π
∞
−∞
exp

−r2
dr = 1.
Here, variables of integration are changed from x to r =
x−µ
σ
√
2 (i.e., dx
dr = σ
√
2), as
explained in Fig. 2.9.
The constants µ and σ included in normal distribution N(µ,σ2) correspond to the
expectation and standard deviation:
E[x] = µ and V[x] = σ2.
As explained in Section 2.4.3, the expectation and variance can be obtained
through the moment-generating function: Indeed, the moment-generating function
of N(µ,σ2) is given by
Mx(t) =
∞
−∞
et x f (x)dx
=
1
σ
√
2π
∞
−∞
exp

−(x −µ)2
2σ2
+ tx

dx
=
1
σ
√
2π
∞
−∞
exp

−x2 −2(µ + σ2t)x + µ2
2σ2

dx
=
1
σ
√
2π
∞
−∞
exp *
,
−
 x −(µ + σ2t)2
2σ2
+ µt + σ2t2
2
+
-
dx
= exp

µt + σ2t2
2
∞
−∞
1
σ
√
2π
exp *
,
−
 x −(µ + σ2t)2
2σ2
+
-
dx
= exp

µt + σ2t2
2

.
(4.1)
Note that, in the above derivation, completing the square,
x2 + 2ax + b = 0 ⇐⇒
(x + a)2 −a2 + b = 0,
(4.2)
is used to have the probability density function of N(µ+σ2t,σ2), which is integrated
to 1.
Probability density functions of N(µ,σ2) for µ = 0 are illustrated in Fig. 4.3,
showing that the normal density is symmetric and bell-shaped.

4.2 NORMAL DISTRIBUTION
39
The Gaussian integral bridges the two well-known irrational numbers, e =
2.71828 · · · and π = 3.14159 · · · , as
∞
−∞
e−x2dx = √π.
To prove this, let us consider change of variables in integration (see Fig. 4.2)
for f (x, y) = e−(x2+y2) and X = Y = [−∞,∞]. Let g(r,θ) = r cos θ and
h(r,θ) = r sin θ. Then
R = [0,∞], Θ = [0,2π], J = *.
,
cos θ −r sin θ
sin θ r cos θ
+/
-
, and det(J) = r,
which yields
∞
−∞
∞
−∞
e−(x2+y2)dxdy =
2π
0
∞
0
re−r2drdθ =
2π
0
dθ
∞
0
re−r2dr
=
2π
0
dθ
∞
0
re−r2dr = 2π

−1
2e−r2∞
0
= π.
Consequently,
∞
−∞
e−x2dx =
∞
−∞
e−x2dx
2
=
∞
−∞
e−x2dx
∞
−∞
e−y2dy

=
∞
−∞
∞
−∞
e−(x2+y2)dxdy = √π.
FIGURE 4.1
Gaussian integral.
If random variable x follows N(µ,σ2), its affine transformation
r = ax + b
follows N(aµ + b,a2σ2). This can be proved by the fact that the probability density
function of r is given as follows (see Section 2.5):
g(r) = 1
|a| f
r −b
a

.

40
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
Let us extend change of variables in integration from one dimension (see
Fig. 2.9) to two dimensions. Integration of function f (x, y) over X × Y can
be computed with x = g(r,θ), y = h(r,θ), X = g(R,Θ), and Y = h(R,Θ) as

X

Y
f (x, y)dydx =

R

Θ
f  g(r,θ),h(r,θ)|det(J)|dθdr,
where J is called the Jacobian matrix and det(J) denotes the determinant of
J:
J = *.
,
∂x
∂r
∂x
∂θ
∂y
∂r
∂y
∂θ
+/
-
and det(J) = ∂x
∂r
∂y
∂θ −∂x
∂θ
∂y
∂r .
The determinant is the product of all eigenvalues and it corresponds to the
ratio of volumes when (x, y) is changed to (r,θ). det(J) is often called the
Jacobian. The above formula can be extended to more than two dimensions.
FIGURE 4.2
Two-dimensional change of variables in integration.
FIGURE 4.3
Probability density functions of normal density N(µ,σ2).
Setting a =
1
D[x] and b = −E[x]
D[x] yields
z =
x
D[x] −E[x]
D[x] = x −E[x]
D[x]
,
which follows standard normal distribution N(0,1) (Fig. 4.4).

4.3 GAMMA, EXPONENTIAL, AND CHI-SQUARED DISTRIBUTIONS
41
FIGURE 4.4
Standard normal distribution N(0,1). A
random variable following N(0,1) is in-
cluded in [−1,1] with probability 68.27%,
in [−2,2] with probability 95.45%, and in
[−3,3] with probability 99.73%.
4.3 GAMMA DISTRIBUTION, EXPONENTIAL
DISTRIBUTION, AND CHI-SQUARED
DISTRIBUTION
As explained in Section 3.4, the Poisson distribution represents the probability that
the event occurring λ times on average in unit time occurs x times in unit time. On
the other hand, the elapsed time x that the event occurring λ times on average in
unit time occurs α times follows the gamma distribution. The gamma distribution for
positive real constants α and λ is denoted by Ga(α,λ).
The probability density function of Ga(α,λ) is given by
f (x) = λα
Γ(α) xα−1e−λx
for x ≥0,
where
Γ(α) =
∞
0
xα−1e−xdx
(4.3)
is called the gamma function.
∞
0
f (x)dx = 1 can be proved by changing variables of
integration as y = λx (see Fig. 2.9):
∞
−∞
f (x)dx = λα
Γ(α)
∞
0
xα−1e−λxdx = λα
Γ(α)
∞
0
y
λ
α−1
e−y 1
λ dy
=
1
Γ(α)
∞
0
yα−1e−ydy = 1.

42
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
FIGURE 4.5
Gamma function. Γ(α + 1)
=
α! holds
for non-negative integer α, and the gamma
function smoothly interpolates the factorials.
The gamma function fulfills
Γ(1) =
∞
0
e−xdx = −e−x∞
0 = 1.
Furthermore, integration by parts for functions u(x) and v(x) and their derivatives
u′(x) and v′(x) given by
b
a
u(x)v′(x)dx =

u(x)v(x)
b
a −
b
a
u′(x)v(x)dx
(4.4)
yields
Γ(α) =
∞
0
e−x xα−1dx =

e−x xα
α
∞
0
−
∞
0
(−e−x) xα
α dx
= 1
α
∞
0
e−x x(α+1)−1dx = Γ(α + 1)
α
.
Putting them together, the gamma function for non-negative integer α is shown to
fulfill
Γ(α + 1) = αΓ(α) = α(α −1)Γ(α −1) = · · · = α!Γ(1) = α!.
Thus, the gamma function can be regarded as generalization of the factorial to real
numbers (see Fig. 4.5). Furthermore, change of variables x = y2 in integration yields
Γ(α) =
∞
0
y2(α−1)e−y2 dx
dy dy = 2
∞
0
y2α−1e−y2dy,
(4.5)

4.3 GAMMA, EXPONENTIAL, AND CHI-SQUARED DISTRIBUTIONS
43
FIGURE 4.6
Probability density functions of gamma distribution Ga(α,λ).
and applying the Gaussian integral (Fig. 4.1) to this results in
Γ
1
2

= 2
∞
0
e−y2dy =
∞
−∞
e−y2dy = √π.
Probability density functions of Ga(α,λ) are illustrated in Fig. 4.6. The probabil-
ity density is monotone decreasing as x increases when α ≤1, while the probability
density increases and then decreases as x increases when α > 1.
By changing variables of integration as y = (λ −t)x, the moment-generating
function of Ga(α,λ) can be expressed as
Mx(t) = E[et x] = λα
Γ(α)
∞
0
xα−1e−(λ−t)xdx
= λα
Γ(α)
∞
0

y
λ −t
α−1
e−y
1
λ −t dy = λα
Γ(α)
Γ(α)
(λ −t)α =

λ
λ −t
α
.

44
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
From this, the expectation and variance of Ga(α,λ) are given by
E[x] = α
λ
and V[x] = α
λ2 .
Gamma distribution Ga(α,λ) for α = 1 is called the exponential distribution and
is denoted by Exp(λ). The probability density function of Exp(λ) is given by
f (x) = λe−λx.
The elapsed time x that the event occurring λ times on average in unit time occurs
for the first time follows the exponential distribution.
Gamma distribution Ga(α,λ) for α = n/2 and λ = 1/2 where n is an integer
is called the chi-squared distribution with n degrees of freedom. This is denoted by
χ2(n), and its probability density function is given by
f (x) = x
n
2 −1e−x
2
2
n
2 Γ( n
2 )
.
Let z1,. . . , zn independently follow the standard normal distribution N(0,1). Then
their squared sum,
x =
n

i=1
z2
i ,
follows χ2(n). The chi-squared distribution plays an important role in hypothesis
testing explained in Chapter 10.
4.4 BETA DISTRIBUTION
For positive real scalars α and β, the beta distribution, denoted by Be(α, β), is the
probability distribution whose probability density is given by
f (x) = xα−1(1 −x)β−1
B(α, β)
for 0 ≤x ≤1,
where
B(α, β) =
1
0
xα−1(1 −x)β−1dx
is the beta function. When α and β are positive integers, αth smallest value (or
equivalently βth largest value) among the α + β −1 random variables independently
following the continuous uniform distribution U(0,1) follows the beta distribution.
Change of variables in integration as x = (sin θ)2 allows us to express the beta
function using sinusoidal functions as

4.4 BETA DISTRIBUTION
45
B(α, β) =

π
2
0
(sin θ)2(α−1) 
1 −(sin θ)2β−1 dx
dθ dθ
=

π
2
0
(sin θ)2(α−1)(cos θ)2(β−1) · 2 sin θ cos θdθ
= 2

π
2
0
(sin θ)2α−1(cos θ)2β−1dθ.
(4.6)
Furthermore, from Eq. (4.5), the gamma product Γ(α)Γ(β) can be expressed as
Γ(α)Γ(β) =

2
∞
0
u2α−1e−u2du

2
∞
0
v2β−1e−v2dv

= 4
∞
0
∞
0
u2α−1v2β−1e−(u2+v2)dudv.
If variables of integrations are changed as u = r sin θ and v = r cos θ (see Fig. 4.2) in
the above equation, then
Γ(α)Γ(β) = 4

π
2
0
∞
0
r2(α+β)−2e−r2(sin θ)2α−1(cos θ)2β−1rdrdθ
=

2
∞
0
r2(α+β)−1e−r2dr

*
,
2

π
2
0
(sin θ)2α−1(cos θ)2β−1dθ+
-
= Γ(α + β)B(α, β),
where Eq. (4.6) was used. Thus, the beta function can be expressed by using the
gamma function as
B(α, β) = Γ(α)Γ(β)
Γ(α + β) .
(4.7)
This allows us to compute, e.g., the following integrals:

π
2
0
(sin θ)2ndθ =

π
2
0
(sin θ)2(n+ 1
2 )−1(cos θ)2 1
2 −1dθ = B

n + 1
2, 1
2

,

π
2
0
(sin θ)2n+1dθ =

π
2
0
(sin θ)2(n+1)−1(cos θ)2 1
2 −1dθ = B

n + 1, 1
2

.
Probability density functions of Be(α, β) are illustrated in Fig. 4.7. This shows
that the profile of the beta density drastically changes depending on the values of
α and β, and the beta distribution is reduced to the continuous uniform distribution
when α = β = 1.
The expectation and variance of Be(α, β) are given by
E[x] =
α
α + β
and V[x] =
α β
(α + β)2(α + β + 1).

46
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
FIGURE 4.7
Probability density functions of beta distribution Be(α, β).
The expectation can be proved by applying integration by parts as
E[x] =
1
B(α, β)
1
0
xxα−1(1 −x)β−1dx
=
1
B(α, β)
1
0
xα(1 −x)β−1dx
=
1
B(α, β)


xα

−(1 −x)β
β
1
0
−
1
0
αxα−1

−(1 −x)β
β

dx

= α
β
1
B(α, β)
1
0
xα−1(1 −x)β−1(1 −x)dx
= α
β
1
B(α, β)
1
0
xα−1(1 −x)β−1dx −
1
0
xxα−1(1 −x)β−1dx

= α
β
 1 −E[x].

4.5 CAUCHY DISTRIBUTION AND LAPLACE DISTRIBUTION
47
Similar computation applied to E[x2] gives
E[x2] = α + 1
β

E[x] −E[x2]

,
which yields
E[x2] =
α(α + 1)
(α + β)(α + β + 1).
Plugging this into
V[x] = E[x2] − E[x]2
gives V[x] =
αβ
(α+β)2(α+β+1).
In Section 6.3, the beta distribution will be extended to multiple dimensions.
4.5 CAUCHY DISTRIBUTION AND LAPLACE
DISTRIBUTION
Let z and z′ be the random variables independently following the standard normal
distribution N(0,1). Then their ratio,
x = z
z′,
follows the standard Cauchy distribution, whose probability density function is given
by f (x) =
1
π(x2+1). Its generalization using a real scalar a and a positive real b is given
by
f (x) =
b
π((x −a)2 + b2).
This is called the Cauchy distribution and is denoted by Ca(a,b).
Computing the expectation of the standard Cauchy distribution yields
E[x] =
+∞
−∞
x f (x)dx =
+∞
−∞
x
π(x2 + 1)dx
= 1
2π

log(1 + x2)
+∞
−∞= 1
2π
lim
α→+∞,β→−∞log 1 + α2
1 + β2 ,
which means that the value depends on how fast α approaches +∞and β approaches
−∞. For this reason, Ca(a,b) does not have the expectation. The limiting value
when α = −β is called the principal value, which is given by a and represents
the “location” of the Cauchy distribution. Since the expectation does not exist, the
Cauchy distribution does not have the variance and all higher moments, either. The
positive scalar b represents the “scale” of the Cauchy distribution.

48
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
FIGURE 4.8
Probability density functions of Cauchy distribution Ca(a,b), Laplace distribution La(a,b), and
normal distribution N(a,b2).
Let y and y′ be random variables independently following the exponential
distribution Exp(1). Then their difference,
x = y −y′,
follows the standard Laplace distribution, whose probability density function is given
by f (x) = 1
2 exp (−|x|). Its generalization using a real scalar a and a positive real b is
given by
f (x) = 1
2b exp

−|x −a|
b

.
This is called the Laplace distribution and is denoted by La(a,b). Since the Laplace
distribution can be regarded as extending the exponential distribution to the negative
domain, it is also referred to as the double exponential distribution.
When |t| < 1/b, the moment-generating function of La(a,b) is given by
Mx(t) = 1
2b
a
−∞
exp

xt + x
b −a
b

dx + 1
2b
+∞
a
exp

xt −x
b + a
b

dx
= 1
2

1
1 + bt exp

xt + x
b −a
b
a
−∞
−1
2

1
1 −bt exp

xt −x
b + a
b
a
−∞
= exp(at)
1 −b2t2 .
From this, the expectation and variance of La(a,b) are given by
E[x] = a and V[x] = 2b2.
Probability density functions of Cauchy distribution Ca(a,b), Laplace distribution
La(a,b), and normal distribution N(a,b2) are illustrated in Fig. 4.8. Since the Cauchy

4.6 t-DISTRIBUTION AND F-DISTRIBUTION
49
FIGURE 4.9
Probability density functions of t-distribution t(d), Cauchy distribution Ca(0,1), and normal
distribution N(0,1).
and Laplace distributions have heavier tails than the normal distribution, realized
values can be quite far from the origin. For this reason, the Cauchy and Laplace
distributions are often used for modeling data with outliers. Note that the Laplace
density is not differentiable at the origin.
4.6 t-DISTRIBUTION AND F-DISTRIBUTION
Let z be a random variable following the standard normal distribution N(0,1) and y be
a random variable following the chi-squared distribution with d degrees of freedom
χ2(d). Then their ratio,
x =
z

y/d
,
follows the t-distribution denoted by t(d). Following its inventor’s pseudonym, the
t-distribution is also referred to as Student’s t-distribution.
The probability density function of t-distribution t(d) is given as follows
(Fig. 4.9):
f (x) =
1
B( d
2 , 1
2)
√
d

1 + x2
d
−d+1
2
,
where B is the beta function explained in Section 4.4. The t-distribution agrees with
the Cauchy distribution when the degree of freedom is d = 1, and it is reduced to the
normal distribution when the degree of freedom tends to ∞. The expectation exists
when d ≥2, and the variance exists when d ≥3, which are given by
E[x] = 0 and V[x] =
d
d −2.
Let y and y′ be random variables following the chi-squared distributions with d
and d′ degrees of freedom, respectively. Then their ratio,
x = y/d
y′/d′,

50
CHAPTER 4
CONTINUOUS PROBABILITY DISTRIBUTIONS
FIGURE 4.10
Probability density functions of F-distribution F(d,d′).
follows the F-distribution denoted by F(d,d′). Following its inventor’s name, the
F-distribution is also referred to as Snedecor’s F-distribution.
The probability density function of F-distribution F(d,d′) is given as follows
(Fig. 4.10):
f (x) =
1
B(d/2,d′/2)
d
d′
d
2
x
d
2 −1

1 + d
d′ x
−d+d′
2
for x ≥0.
The expectation exists when d′ ≥3, and the variance exists when d′ ≥5, which are
given by
E[x] =
d′
d′ −2
and V[x] = 2d′2(d + d′ −2)
d(d′ −2)2(d′ −4).
If y follows the t-distribution t(d), then y2 follows the F-distribution F(1,d).
The t-distribution and the F-distribution play important roles in hypothesis testing
explained in Chapter 10. The t-distribution is also utilized for deriving the confidence
interval in Section 9.3.1.

CHAPTER
MULTIDIMENSIONAL
PROBABILITY
DISTRIBUTIONS
5
CHAPTER CONTENTS
Joint Probability Distribution ...................................................... 51
Conditional Probability Distribution ............................................... 52
Contingency Table ................................................................ 53
Bayes’ Theorem ................................................................... 53
Covariance and Correlation ........................................................ 55
Independence..................................................................... 56
So far, properties of one-dimensional random variable x are discussed. When
multiple random variables are available, we may be interested in knowing the
dependency of one variable on another, which will give us more information. In this
chapter, the relation between two random variables x and y is discussed.
5.1 JOINT PROBABILITY DISTRIBUTION
The probability that discrete random variables x and y take a value in a countable
set is denoted by Pr(x, y). The function that describes the mapping from any realized
value of the random variables to probability is called the joint probability distribution,
and its probability mass function f (x, y) is called the joint probability mass function:
Pr(x, y) = f (x, y).
Similarly to the one-dimensional case, f (x, y) should satisfy
f (x, y) ≥0 and

x,y
f (x, y) = 1.
The probability mass functions of x or y alone, g(x) and h(y), can be expressed by
using f (x, y) as
g(x) =

y
f (x, y) and h(y) =

x
f (x, y).
These are called the marginal probability mass functions and the corresponding prob-
ability distributions are called the marginal probability distributions. The process of
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00016-9
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
51

52
CHAPTER 5
MULTIDIMENSIONAL PROBABILITY DISTRIBUTIONS
computing a marginal probability distribution from the joint probability distribution
is called marginalization.
When x and y are continuous random variables, the joint probability density
function f (x, y) is defined as
Pr(a ≤x ≤b, c ≤y ≤d) =
d
c
b
a
f (x, y)dxdy.
Similarly to the one-dimensional case, f (x, y) should satisfy
f (x, y) ≥0 and

f (x, y)dxdy = 1.
The probability density functions of x or y alone, g(x) and h(y), can be expressed by
using f (x, y) as
Pr(a ≤x ≤b) =
b
a

f (x, y)dydx =
b
a
g(x)dx,
Pr(c ≤y ≤d) =
d
c

f (x, y)dxdy =
d
c
h(y)dy,
where
g(x) =

f (x, y)dy and h(y) =

f (x, y)dx
are the marginal probability density functions.
5.2 CONDITIONAL PROBABILITY DISTRIBUTION
For discrete random variables x and y, the probability of x given y is denoted
by Pr(x|y) and called the conditional probability distribution. Since Pr(x|y) is the
probability that x occurs after y occurs, it is given by
Pr(x|y) = Pr(x, y)
Pr(y) .
(5.1)
Based on this, the conditional probability mass function is given by
g(x|y) = f (x, y)
h(y) .
(5.2)
Since the conditional probability distribution is a probability distribution, its expec-
tation and variance can also be defined, which are called the conditional expectation
and conditional variance, respectively,
E[x|y] =

x
x g(x|y) and V[x|y] = E

(x −E[x|y])2|y

.

5.3 CONTINGENCY TABLE
53
Table 5.1 Example of Contingency Table
x \ y
Sleepy during the
Lecture
Not Sleepy during
the Lecture
Total
Like statistics and
probability
20
40
60
Dislike statistics
and probability
20
20
40
Total
40
60
100
When x and y are continuous random variables, Pr(y) = 0 and thus the conditional
probability cannot be defined by Eq. (5.1). However, the conditional probability
density function can be defined in the same way as Eq. (5.2), and the conditional
expectation is given by
E[x|y] =

x g(x|y)dx.
5.3 CONTINGENCY TABLE
A contingency table summarizes information of multiple discrete random variables.
An example of the contingency table is given in Table 5.1: x is the random variable
representing students’ likes and dislikes of probability and statistics, while y is the
random variable representing their drowsiness during the lecture.
A contingency table corresponds to a probability mass function. The right-most
column is called the row marginal total, the bottom row is called the column marginal
total, and the right-bottom cell is called the grand total. The row marginal total and
the column marginal total correspond to marginal probability mass functions, while
each row and each column correspond to conditional probability mass functions.
Hypothesis testing using the contingency table will be explained in Section 10.4.
5.4 BAYES’ THEOREM
When probability Pr(y|x) of effect y given cause x is known, Bayes’ theorem allows
us to compute the probability Pr(x|y) of cause x given effect y as
Pr(x|y) = Pr(y|x)Pr(x)
Pr(y)
.
Since Pr(x) is the probability of cause x before effect y is known, it is called the prior
probability of x. On the other hand, Pr(x|y) is the probability of cause x after effect
y is known, and it is called the posterior probability of x. Bayes’ theorem can be
immediately proved by the definition of joint probability distributions:
Pr(x|y)Pr(y) = Pr(x, y) = Pr(y|x)Pr(x).

54
CHAPTER 5
MULTIDIMENSIONAL PROBABILITY DISTRIBUTIONS
Bayes’ theorem also holds for continuous random variables x and y, using probability
density functions as
g(x|y) = h(y|x)g(x)
h(y)
.
Let us illustrate the usefulness of Bayes’ theorem through an example of a
polygraph. Let x be a random variable representing whether a subject’s word is
true or false, and let y be its prediction by a polygraph. The polygraph has excellent
performance, such that
Pr(y = false| x = false) = 0.99,
Pr(y = true| x = true) = 0.95.
Suppose that the prior probability is
Pr(x = false) = 0.001.
If the polygraph says that the subject’s word is false, can we believe its decision? The
reliability of the polygraph can be evaluated by comparing
Pr(x = false| y = false) and Pr(x = true| y = false).
Since marginalization of x yields
Pr(y = false) = Pr(y = false| x = false)Pr(x = false)
+ Pr(y = false| x = true)Pr(x = true)
= Pr(y = false| x = false)Pr(x = false)
+

1 −Pr(y = true| x = true)

1 −Pr(x = false)

= 0.99 × 0.001 + (1 −0.95) × (1 −0.001)
≈0.051,
Bayes’ theorem results in
Pr(x = false| y = false) = Pr(y = false| x = false)Pr(x = false)
Pr(y = false)
≈0.99 × 0.001
0.051
≈0.019.
Thus,
Pr(x = true| y = false) = 1 −Pr(x = false| y = false)
≈0.981
holds and therefore
Pr(x = false| y = false) ≪Pr(x = true| y = false).
Consequently, we conclude that the output of the polygraph, y = false, is not reliable.

5.5 COVARIANCE AND CORRELATION
55
The above analysis shows that, if the prior probability that the subject tells a lie,
Pr(x = false), is small, the output of the polygraph, y = false, is not reliable. If
Pr(x = false) > 0.048,
it follows that
Pr(x = false| y = false) > Pr(x = true| y = false),
showing that the output of the polygraph, y = false, becomes reliable.
5.5 COVARIANCE AND CORRELATION
If random variables x and y are related to each other, change in one variable may
affect the other one.
The variance of random variables x and y, V[x + y], does not generally agree with
the sum of each variance, V[x] + V[y]. Indeed, V[x + y] and V[x] + V[y] are related
as
V[x + y] = V[x] + V[y] + 2Cov[x, y],
where Cov[x, y] is the covariance of x and y defined by
Cov[x, y] = E
 x −E[x] y −E[y]
.
Increasing x tends to increase y if Cov[x, y] > 0, while increasing x tends to decrease
y if Cov[x, y] < 0. If Cov[x, y] ≈0, x and y are unrelated to each other.
The covariance is useful to create a portfolio of stocks. Let x and y be the stock
prices of companies A and B, respectively. If Cov[x, y] > 0, buying the stocks of both
companies increases the variance. Therefore, the property tends to fluctuate and there
is chance to gain a big profit (and at the same time, there is the possibility to lose a lot).
On the other hand, if Cov[x, y] < 0, buying the stocks of both companies decreases
the variance. Therefore, the risk is hedged and the property is more stabilized (and at
the same time, there is less opportunities to gain a big profit).
The matrix that summarizes the variance and covariance of x and y is called the
variance–covariance matrix:
Σ = E


*
,
x
y
+
-
−E *
,
x
y
+
-


*
,
x
y
+
-
−E *
,
x
y
+
-

⊤
= *
,
V[x]
Cov[x, y]
Cov[y, x]
V[y]
+
-
,
where ⊤denotes the transposes. Since Cov[y, x] = Cov[x, y], the variance–covariance
matrix is symmetric.
The correlation coefficient between x and y, denoted by ρx,y, is defined as
Cov[x, y] normalized by the product of standard deviations

V[x] and

V[y],

56
CHAPTER 5
MULTIDIMENSIONAL PROBABILITY DISTRIBUTIONS
ρx,y =
Cov[x, y]

V[x]

V[y]
.
Since the correlation coefficient is a normalized variant of the covariance, it essen-
tially plays the same role as the covariance. However, the correlation coefficient is
bounded as
−1 ≤ρx,y ≤1,
(5.3)
and therefore the absolute strength of the relation between x and y can be known.
Eq. (5.3) can be proved by the generic inequality
|E[x]| ≤E[|x|]
and Schwarz’s inequality explained in Section 8.3.2,
|Cov[x, y]| ≤E|(x −E[x])(y −E[y])| ≤

V[x]

V[y].
Examples of the correlation coefficient are illustrated in Fig. 5.1. If ρx,y > 0, x
and y have the same tendency and x and y are said to be positively correlated. On the
other hand, if ρx,y < 0, x and y have the opposite tendency and x and y are said to
be negatively correlated. x and y are deterministically proportional to each other if
ρx,y = ±1, and x and y are unrelated if ρx,y ≈0. If ρx,y = 0, x and y are said to be
uncorrelated to each other. Thus, the correlation coefficient allows us to capture the
relatedness between random variables. However, if x and y have nonlinear relation,
the correlation coefficient can be close to zero, as illustrated in Fig. 5.2.
5.6 INDEPENDENCE
x and y are said to be statistically independent if
f (x, y) = g(x)h(y) for all x and y.
Conversely, if x and y are not independent, they are said to be dependent. If x and y
are independent, the following properties hold:
•
Conditional probability is independent of the condition,
g(x|y) = g(x) and h(y|x) = h(y).
•
The expectation of the product agrees with the product of the expectations,
E[xy] = E[x]E[y].
•
The moment-generating function of the sum agrees with the product of the
moment-generating functions,
Mx+y(t) = Mx(t)My(t).

5.6 INDEPENDENCE
57
(a) Positive correlation
(b) Negative correlation
(c) No correlation
FIGURE 5.1
Correlation coefficient ρx,y. Linear relation between x and y can be captured.
•
Uncorrelated,
Cov[x, y] = 0.
Although independence and no correlation both mean that x and y are “unrelated,”
independence is stronger than no correlation. Indeed, independence implies no
correlation but not vice versa. For example, random variables x and y whose joint

58
CHAPTER 5
MULTIDIMENSIONAL PROBABILITY DISTRIBUTIONS
(a) Quadratic function
(b) Checker board
(c) Cross
FIGURE 5.2
Correlation coefficient for nonlinear relations. Even when there is a nonlinear relation between x
and y, the correlation coefficient can be close to zero if the probability distribution is symmetric.
probability density function is given by
f (x, y) =

1
(|x| + |y| ≤
1√
2)
0
(otherwise)

5.6 INDEPENDENCE
59
FIGURE 5.3
Example of x and y which are uncorrelated but dependent.
have no correlation but dependent (see Fig. 5.3). More precisely, the uncorrelatedness
of x and y can be confirmed by
Cov[x, y] = E
 x −E[x]  y −E[y]
= E[xy]
=

1√
2
−1√
2
x *.
,

1√
2 −|x|
−1√
2 +|x|
ydy+/
-
dx = −

1√
2
−1√
2
√
2x|x|dx = 0,
while dependence of x and y, f (x, y) , g(x)h(y), can be confirmed by
g(x) = max  0,
√
2 −2|x|,
h(y) = max  0,
√
2 −2|y|.


CHAPTER
EXAMPLES OF
MULTIDIMENSIONAL
PROBABILITY
DISTRIBUTIONS
6
CHAPTER CONTENTS
Multinomial Distribution .......................................................... 61
Multivariate Normal Distribution .................................................. 62
Dirichlet Distribution .............................................................. 63
Wishart Distribution ............................................................... 70
In this chapter, popular multidimensional probability distributions and their proper-
ties such as the expectation, the variance, and the moment-generating functions are
illustrated.
6.1 MULTINOMIAL DISTRIBUTION
The binomial distribution explained in Section 3.2 is the probability distribution
of the number x of successful trials in n Bernoulli trials with the probability of
success p. The multinomial distribution is an extension of the binomial distribution
to multidimensional cases.
Let us consider a d-sided dice with the probability of obtaining each side p =
(p1,. . . ,pd)⊤, where
p1,. . . ,pd ≥0 and
d

j=1
pj = 1.
Let x = (x(1),. . . , x(d))⊤be the number of times each side appears when the dice is
thrown n times, where
x ∈∆d,n =

x  x(1),. . . , x(d) ≥0, x(1) + · · · + x(d) = n

.
The probability distribution that x follows is the multinomial distribution and is
denoted by Mult(n, p).
The probability that the jth side appears x(j) times is (pj)x(j), and the number of
combinations each of the d sides appears x(1),. . . , x(d) times for n trials is given by
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00017-0
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
61

62
CHAPTER 6
MULTIDIMENSIONAL PROBABILITY DISTRIBUTIONS
n!
x(1)!···x(d)!. Putting them together, the probability mass function of Mult(n, p) is given
by
f (x) =
n!
x(1)! · · · x(d)!(p1)x(1) · · · (pd)x(d).
When d = 2, Mult(n, p) is reduced to Bi(n,p1).
The binomial theorem can also be extended to multinomial theorem:
(p1 + · · · + pd)n =

x ∈∆d,n
n!
x(1)! · · · x(d)!(p1)x(1) · · · (pd)x(d),
with which the moment-generating function of Mult(n, p) can be computed as
Mx(t) = E[et⊤x] =

x ∈∆d,n
et1x(1) · · · etd x(d)
n!
x(1)! · · · x(d)!(p1)x(1) · · · (pd)x(d)
=

x∈∆d,n
n!
x(1)! · · · x(d)!(p1et1)x(1) · · · (pdetd)x(d)
= (p1et1 + · · · + pdetd)n.
From this, the expectation of and (co)variance of Mult(n, p) can be computed as
E[x(j)] = npj
and Cov[x(j), x(j′)] =

npj(1 −pj)
(j = j′),
−npj pj′
(j , j′).
6.2 MULTIVARIATE NORMAL DISTRIBUTION
When random variables y(1),. . . , y(d) independently follow the standard normal
distribution N(0,1), the joint probability density function of y = (y(1),. . . , y(d))⊤
is given by
g(y) =
d

j=1
1
√
2π
exp

−(y(j))2
2

=
1
(2π)d/2 exp

−1
2 y⊤y

.
The expectation and variance-covariance matrices of y are given by
E[y] = 0 and V[y] = I,
where 0 denotes the zero vector and I denotes the identity matrix.

6.3 DIRICHLET DISTRIBUTION
63
Let us transform y by d × d invertible matrix T and d-dimensional vector µ as
x = T y + µ.
Then the joint probability density function of x is given by
f (x) = g(y)|det(T)|−1
=
1
(2π)d/2det(Σ)1/2 exp

−1
2(x −µ)⊤Σ−1(x −µ)

,
where det(T) is the Jacobian (Fig. 4.2), and
Σ = TT ⊤.
This is the general form of the multivariate normal distribution and is denoted by
N(µ,Σ). The expectation and variance-covariance matrices of N(µ,Σ) are given by
E[x] = T E[y] + µ = µ,
V[x] = V[T y + µ] = TV[y]T ⊤= Σ.
Probability density functions of two-dimensional normal distribution N(µ,Σ) are
illustrated in Fig. 6.1. The contour lines of two-dimensional normal distributions
are elliptic, and their principal axes agree with the coordinate axes if the variance-
covariance matrix is diagonal. Furthermore, the elliptic contour lines become
spherical if all diagonal elements are equal (i.e., the variance-covariance matrix is
proportional to the identity matrix).
Eigendecomposition of variance-covariance matrix Σ (see Fig. 6.2) shows that the
principal axes of the ellipse are parallel to the eigenvectors of Σ, and their length is
proportional to the square root of the eigenvalues (Fig. 6.3).
6.3 DIRICHLET DISTRIBUTION
Let α = (α1,. . . ,αd)⊤be a d-dimensional vector with positive entries and let
y(1),. . . , y(d) be random variables that independently follow the gamma distribution
Ga(αj,λ). For V = d
j=1 y(j), let
x = (x(1),. . . , x(d))⊤=
y(1)
V ,. . . , y(d)
V
⊤
.
Then the distribution that the above d-dimensional vector x follows is called the
Dirichlet distribution and is denoted by Dir(α). The domain of x is given by
∆d =

x  x(1),. . . , x(d) ≥0, x(1) + · · · + x(d) = 1

.
Drawing a value from a Dirichlet distribution corresponds to generating a (unfair)
d-sided die.

64
CHAPTER 6
MULTIDIMENSIONAL PROBABILITY DISTRIBUTIONS
(a) When Σ is generic, the contour lines are elliptic
(b) When Σ is diagonal, the principal axes of the elliptic contour lines agree with the coordinate
axes
(c) When Σ is proportional to identity, the contour lines are spherical
FIGURE 6.1
Probability density functions of two-dimensional normal distribution N(µ,Σ) with µ = (0,0)⊤.

6.3 DIRICHLET DISTRIBUTION
65
For d × d matrix A, a nonzero vector φ and a scalar λ that satisfy
Aφ = λφ
are called an eigenvector and an eigenvalue of A, respectively. Generally,
there exist d eigenvalues λ1,. . . ,λd and they are all real when A is
symmetric. A matrix whose eigenvalues are all positive is called a positive
definite matrix, and a matrix whose eigenvalues are all non-negative is called
a positive semidefinite matrix. Eigenvectors φ1,. . . ,φd corresponding to
eigenvalues λ1,. . . ,λd are orthogonal and are usually normalized, i.e., they
are orthonormal as
φ⊤
j φj′ =

1
(j = j′),
0
(j , j′).
A matrix A can be expressed by using its eigenvectors and eigenvalues as
A =
d

j=1
λ jφjφ⊤
j = ΦΛΦ⊤,
where Φ = (φ1,. . . ,φd) and Λ is the diagonal matrix with diagonal elements
λ1,. . . ,λd. This is called eigenvalue decomposition of A. In MATLAB,
eigenvalue decomposition can be performed by the eig function. When all
eigenvalues are nonzero, the inverse of A can be expressed as
A−1 =
d

j=1
λ−1
j φjφ⊤
j = ΦΛ−1Φ⊤.
For d × d matrix A and d × d positive symmetric matrix B, a nonzero vector
φ and a scalar λ that satisfy
Aφ = λBφ
are called a generalized eigenvector and a generalized eigenvalue of A,
respectively.
FIGURE 6.2
Eigenvalue decomposition.

66
CHAPTER 6
MULTIDIMENSIONAL PROBABILITY DISTRIBUTIONS
FIGURE 6.3
Contour lines of the normal density. The
principal axes of the ellipse are parallel
to the eigenvectors of variance-covariance
matrix Σ, and their length is proportional to
the square root of the eigenvalues.
The probability density function of Dir(α) is given by
f (x) =
d
j=1(x(j))α j−1
Bd(α)
,
where
Bd(α) =

∆d
d

j=1
(x(j))α j−1dx
(6.1)
is the d-dimensional beta function.
For x = ( √p sin θ)2 with 0 ≤p ≤1, Eq. (4.6) implies
p
0
xα−1(p −x)β−1dx
=

π
2
0
( √p sin θ)2(α−1) 
p −( √p sin θ)2β−1 dx
dθ dθ
=

π
2
0
pα−1(sin θ)2(α−1)pβ−1(cos θ)2(β−1) · 2p sin θ cos θdθ
= pα+β−1 *
,
2

π
2
0
(sin θ)2α−1(cos θ)2β−1dθ+
-
= pα+β−1B(α, β).

6.3 DIRICHLET DISTRIBUTION
67
Letting p = 1 −d−2
j=1 x(j), the integration with respect to x(d−1) in
Bd(α) =
1
0
(x(1))α1−1 ×
1−x(1)
0
(x(2))α2−1×
· · · ×
1−d−2
j=1 x(j)
0
(x(d−1))αd−1−1
× *.
,
1 −
d−1

j=1
x(j)+/
-
αd−1
dx(1)dx(2) · · · dx(d−1)
can be computed as
1−d−2
j=1 x(j)
0
(x(d−1))αd−1−1 *.
,
1 −
d−1

j=1
x(j)+/
-
αd−1
dx(d−1)
= *.
,
1 −
d−2

j=1
x(j)+/
-
d
j=d−1 α j−1
B(αd−1,αd).
Similarly, letting p = 1−d−3
j=1 x(j), the integration with respect to x(d−2) in the above
equation can be computed as
1−d−3
j=1 x(j)
0
(x(d−2))αd−2−1 *.
,
1 −
d−2

j=1
x(j)+/
-
d
j=d−1 α j−1
dx(d−2)
= *.
,
1 −
d−3

j=1
x(j)+/
-
d
j=d−2 α j−1
B *.
,
αd−2,
d

j=d−1
αj+/
-
.
Repeating this computation yields
Bd(α) =
d−1

j=1
B *.
,
αj,
d

j′=j+1
αj′+/
-
.
Applying
B(α, β) = Γ(α)Γ(β)
Γ(α + β)
given in Eq. (4.7) to the above equation, the d-dimensional beta function can be
expressed by using the gamma function as
Bd(α) =
d−1

j=1
Γ(αj)Γ
d
j′=j+1 αj′

Γ
d
j′=j αj′

=
d
j=1 Γ(αj)
Γ(α0)
,

68
CHAPTER 6
MULTIDIMENSIONAL PROBABILITY DISTRIBUTIONS
where
α0 =
d

j=1
αj.
When d = 2, x(2) = 1 −x(1) holds and thus the Dirichlet distribution is reduced to
the beta distribution shown in Section 4.4:
f (x) = xα1−1(1 −x)α2−1
B(α1,α2)
.
Thus, the Dirichlet distribution can be regarded as a multidimensional extension of
the beta distribution.
The expectation of Dir(α) is given by
E[x(j)] =

x(j) d
j′=1(x(j′))α j′−1dx(j)
Bd(α)
= Bd(α1,. . . ,α j−1,αj + 1,αj+1,. . . ,αd)
Bd(α1,. . . ,α j−1,αj,αj+1,. . . ,αd)
= Γ(α1) · · · Γ(αj−1)Γ(αj + 1)Γ(αj+1) · · · Γ(αd) × Γ(α0)
Γ(α0 + 1) × Γ(α1) · · · Γ(αj−1)Γ(αj)Γ(αj+1) · · · Γ(αd)
= Γ(αj + 1)Γ(α0)
Γ(α0 + 1)Γ(αj) = αjΓ(αj)Γ(α0)
α0Γ(α0)Γ(αj)
= αj
α0
.
The variance and covariance of Dir(α) can also be obtained similarly as
Cov[x(j), x(j′)] =

αj(α0 −αj)
α2
0(α0 + 1)
(j = j′),
−
αjαj′
α2
0(α0 + 1)
(j , j′).
Probability density functions of Dir(α) when d = 3 are illustrated in Fig. 6.4.
When α1 = α2 = α3 = α, the probability density takes larger values around the
corners of the triangle if each α < 1, the probability density is uniform if each α = 1,
and the probability density takes larger values around the center if each α > 1. When
α1,α2,α3 are nonuniform, the probability density tends to take large values around
the corners of the triangle with large αj.
When the Dirichlet parameters α are all equal, i.e.,
α1 = · · · = αd = α,

6.3 DIRICHLET DISTRIBUTION
69
(a) α = (0.99, 0.99, 0.99)⊤
(b) α = (1, 1, 1)⊤
(c) α = (3, 3, 3)⊤
(d) α = (1.05, 0.95, 0.95)⊤
(e) α = (4, 4, 1)⊤
(f) α = (16, 4, 4)⊤
FIGURE 6.4
Probability density functions of Dirichlet distribution Dir(α). The center of gravity of the triangle
corresponds to x(1) = x(2) = x(3) = 1/3, and each vertex represents the point that the
corresponding variable takes one and the others take zeros.

70
CHAPTER 6
MULTIDIMENSIONAL PROBABILITY DISTRIBUTIONS
the probability density function is simplified as
f (x) = Γ(αd)
Γ(α)d
d

j=1
(x(j))α−1.
This is called symmetric Dirichlet distribution and is denoted by Dir(α). The common
parameter α is often referred to as the concentration parameter.
6.4 WISHART DISTRIBUTION
Let x1,. . . , xn be d-dimensional random variables independently following the
normal distribution N(0,Σ), and let
S =
n

i=1
xi x⊤
i
be the scatter matrix which is assumed to be invertible. The probability distribution
that S follows is called the Wishart distribution with n degrees of freedom and is
denoted by W(Σ,d,n). The probability density function of W(Σ,d,n) is given by
f (S) = det(S)
n−d−1
2
exp  −1
2tr  Σ−1S
det(2Σ)
n
2 Γd( n
2 )
,
where det(·) denotes the determinant of a matrix, tr(·) denotes the trace of a matrix,
Γd(·) denotes the d-dimensional gamma function defined by
Γd(a) =

S∈S+
d
det(S)a−d+1
2 exp   −tr(S)dS,
(6.2)
and S+
d denotes the set of all d × d positive symmetric matrices.
When Σ =
1
2 I, the definition of d-dimensional gamma function immediately
yields

S∈S+
d
f (S)dS = 1.
(6.3)
Since the Jacobian is given by det(2Σ)
d+1
2 when S is transformed to 1
2Σ−1S (Fig. 4.2),
Eq. (6.3) is also satisfied for generic Σ ∈S+
d.
d-dimensional gamma function Γd(·) can be expressed by using two-dimensional
gamma function Γ(·) as
Γd(a) = π
d(d−1)
4
d

j=1
Γ

a + 1 −j
2

= π
d−1
2 Γd−1(a)Γ

a + 1 −d
2

.

6.4 WISHART DISTRIBUTION
71
An operator that transforms an m × n matrix A = (a1,. . . , an) to the mn-
dimensional vector,
vec(A) = (a⊤
1 ,. . . , a⊤
n)⊤,
is called the vectorization operator. The operator that transforms an m × n
matrix A and a p × q matrix B to an mp × nq matrix as
A ⊗B =
*.....
,
a1,1B · · · a1,nB
...
...
...
am,1B · · · am,nB
+/////
-
is called the Kronecker product. The vectorization operator and the Kro-
necker product satisfy the following properties:
vec(ABC) = (C⊤⊗A)vec(B)
= (I ⊗AB)vec(C)
= (C⊤B⊤⊗I)vec(A),
(A ⊗B)−1 = A−1 ⊗B−1,
(A ⊗B)(C ⊗D) = (AC ⊗BD),
tr(A ⊗B) = tr(A)tr(B),
tr(AB) = vec(A⊤)⊤vec(B),
tr(ABCD) = vec(A⊤)⊤(D⊤⊗B)vec(C).
These formulas allow us to compute the product of big matrices efficiently.
FIGURE 6.5
Vectorization operator and Kronecker product.
Thus, the Wishart distribution can be regarded as a multidimensional extension of
the gamma distribution. When d = 1, S and Σ become scalars, and letting S = x and
Σ = 1 yields
f (x) = x
n
2 −1 exp  −x
2

2
n
2 Γ   n
2

.
This is equivalent to the probability density function of the chi-squared distribution
with n degrees of freedom explained in Section 4.3.

72
CHAPTER 6
MULTIDIMENSIONAL PROBABILITY DISTRIBUTIONS
The moment-generating function of W(Σ,d,n) can be obtained by transforming
S to ( 1
2Σ−1 −T)S as
MS(T) = E[etr(T S)]
=

S∈S+
d
det(S)
n−d−1
2
exp  −tr  ( 1
2Σ−1 −T)S
2
dn
2 det(Σ)
n
2 Γd( n
2 )
dS
= det(I −2TΣ)−n
2 .
The expectation and variance-covariance matrices of S that follows W(Σ,d,n) are
given by
E[S] = nΣ and V[vec(S)] = 2nΣ ⊗Σ,
where vec(·) is the vectorization operator and ⊗denotes the Kronecker product (see
Fig. 6.5).

CHAPTER
SUM OF INDEPENDENT
RANDOM VARIABLES
7
CHAPTER CONTENTS
Convolution ....................................................................... 73
Reproductive Property............................................................. 74
Law of Large Numbers ............................................................ 74
Central Limit Theorem ............................................................ 77
In this chapter, the behavior of the sum of independent random variables is first
investigated. Then the limiting behavior of the mean of independent and identically
distributed (i.i.d.) samples when the number of samples tends to infinity is discussed.
7.1 CONVOLUTION
Let x and y be independent discrete variables, and z be their sum:
z = x + y.
Since x + y = z is satisfied when y = z −x, the probability of z can be computed by
summing the probability of x and z −x over all x. For example, let z be the sum of
the outcomes of two 6-sided dice, x and y. When z = 7, these dice take
(x, y) = (1,6),(2,5),(3,4),(4,3),(5,2),(6,1),
and summing up the probabilities of occurring these combinations gives the proba-
bility of z = 7.
The probability mass function of z, denoted by k(z), can be expressed as
k(z) =

x
g(x)h(z −x),
where g(x) and h(y) are the probability mass functions of x and y, respectively. This
operation is called the convolution of x and y and denoted by x ∗y. When x and y are
continuous, the probability density function of z = x + y, denoted by k(z), is given
similarly as
k(z) =

g(x)h(z −x)dx,
where g(x) and h(y) are the probability density functions of x and y, respectively.
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00018-2
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
73

74
CHAPTER 7
SUM OF INDEPENDENT RANDOM VARIABLES
7.2 REPRODUCTIVE PROPERTY
When the convolution of two probability distributions in the same family again yields
a probability distribution in the same family, that family of probability distributions is
said to be reproductive. For example, the normal distribution is reproductive, i.e., the
convolution of normal distributions N(µx,σ2
x) and N(µy,σ2
y) yields N(µx + µy,σ2
x +
σ2
y).
When x and y are independent, the moment-generating function of their sum,
x + y, agrees with the product of their moment-generating functions:
Mx+y(t) = Mx(t)My(t).
Let x and y follow N(µx,σ2
x) and N(µy,σ2
y), respectively. As shown in Eq.(4.1), the
moment-generating function of normal distribution N(µx,σ2
x) is given by
Mx(t) = exp

µxt + σ2
xt2
2

.
Thus, the moment-generating function of the sum, Mx+y(t), is given by
Mx+y(t) = Mx(t)My(t)
= exp

µxt + σ2
xt2
2

exp *
,
µyt +
σ2
yt2
2
+
-
= exp *
,
(µx + µy)t +
(σ2
x + σ2
y)t2
2
+
-
.
Since this is the moment-generating function of N(µx+µy,σ2
x+σ2
y), the reproductive
property of normal distributions is proved.
Similarly, computation of the moment-generating function of Mx+y(t) for inde-
pendent random variables x and y proves the reproductive properties for the binomial,
Poisson, negative binomial, gamma, and chi-squared distributions (see Table 7.1).
The Cauchy distribution does not have the moment-generating function, but the
computation of the characteristic function φx(t) = Mix(t) (see Section 2.4.3) shows
that the convolution of Ca(ax,bx) and Ca(ay,by) yields Ca(ax + ay,bx + by).
On the other hand, the geometric distribution Ge(p) (which is equivalent to the
binomial distribution NB(1,p)) and the exponential distribution Exp(λ) (which is
equivalent to the gamma distribution Ga(1,λ)) do not have the reproductive properties
for p and λ.
7.3 LAW OF LARGE NUMBERS
Let x1,. . . , xn be random variables and f (x1,. . . , xn) be their joint probability
mass/density function. If f (x1,. . . , xn) can be represented by using a probability
mass/density function g(x) as
f (x1,. . . , xn) = g(x1) × · · · × g(xn),

7.3 LAW OF LARGE NUMBERS
75
Table 7.1 Convolution
Distribution
x
y
x + y
Normal
N(µx, σ2
x)
N(µy, σ2
y)
N(µx + µy, σ2
x + σ2
y)
Binomial
Bi(nx, p)
Bi(ny, p)
Bi(nx + ny, p)
Poisson
Po(λx)
Po(λy)
Po(λx + λy)
Negative binomial
NB(kx, p)
NB(ky, p)
NB(kx + ky, p)
Gamma
Ga(αx, λ)
Ga(αy, λ)
Ga(αx + αy, λ)
Chi-squared
χ2(nx)
χ2(ny)
χ2(nx + ny)
Cauchy
Ca(ax, bx)
Ca(ay, by)
Ca(ax + ay, bx + by)
x1,. . . , xn are mutually independent and follow the same probability distribution.
Such x1,. . . , xn are said to be i.i.d. with probability density/mass function g(x) and
denoted by
x1,. . . , xn
i.i.d.
∼g(x).
When x1,. . . , xn are i.i.d. random variables having expectation µ and variance
σ2, the sample mean (Fig. 7.1),
x = 1
n
n

i=1
xi,
satisfies
E[x] = 1
n
n

i=1
E[xi] = µ,
V[x] = 1
n2
n

i=1
V[xi] = σ2
n .
This means that the average of n samples has the same expectation as the original
single sample, while the variance is reduced by factor 1/n. Thus, if the number
of samples tends to infinity, the variance vanishes and thus the sample average x
converges to the true expectation µ.
The weak law of large numbers asserts this fact more precisely. When the original
distribution has expectation µ, the characteristic function φx(t) of the average of
independent samples can be expressed by using the characteristic function φx(t) of a
single sample x as
φx(t) =

φx
t
n
n
=

1 + iµ t
n + · · ·
n
.

76
CHAPTER 7
SUM OF INDEPENDENT RANDOM VARIABLES
The mean of samples x1,. . . , xn usually refers to the arithmetic mean, but other means
such as the geometric mean and the harmonic mean are also often used:
Arithmetic mean: 1
n
n

i=1
xi,
Geometric mean: *.
,
n

i=1
xi+/
-
1
n
,
Harmonic mean:
1
1
n
n
i=1
1
xi
.
For example, suppose that the weight increased by the factors 2%, 12%, and 4%
in the last three years, respectively. Then the average increase rate is not given
by the arithmetic mean (0.02 + 0.12 + 0.04)/3 = 0.06, but the geometric mean
(1.02 × 1.12 × 1.04)
1
3 ≈1.0591. When climbing up a mountain at 2 kilometer per
hour and going back at 6 kilometer per hour, the mean velocity is not given by the
arithmetic mean (2 + 6)/2 = 4 but by the harmonic mean 2d/( d
2 + d
6 ) = 3 for distance
d, according to the formula “velocity = distance/time.” When x1,. . . , xn > 0, the
arithmetic, geometric, and harmonic means satisfy
1
n
n

i=1
xi ≥*.
,
n

i=1
xi+/
-
1
n
≥
1
1
n
n
i=1
1
xi
,
and the equality holds if and only if x1 = · · · = xn. The generalized mean is defined
for p , 0 as
*.
,
1
n
n

i=1
xp
i
+/
-
1
p
.
The generalized mean is reduced to the arithmetic mean when p = 1, the geometric
mean when p →0, and the harmonic mean when p = −1. The maximum of x1,. . . , xn
is given when p →+∞, and the minimum of x1,. . . , xn is given when p →−∞.
When p = 2, it is called the root mean square.
FIGURE 7.1
Arithmetic mean, geometric mean, and harmonic mean.
Then Eq. (3.5) shows that the limit n →∞of the above equation yields
lim
n→∞φx(t) = eit µ.

7.4 CENTRAL LIMIT THEOREM
77
(a) Standard normal distribution N(0, 1)
(b) Standard Cauchy distribution Ca(0, 1)
FIGURE 7.2
Law of large numbers.
Since eit µ is the characteristic function of a constant µ,
lim
n→∞Pr(|x −µ| < ε) = 1
holds for any ε > 0. This is the weak law of large numbers and x is said to
converge in probability to µ. If the original distribution has the variance, its proof
is straightforward by considering the limit n →∞of Chebyshev’s inequality (8.4)
(see Section 8.2.2).
On the other hand, the strong law of large numbers asserts
Pr

lim
n→∞x = µ

= 1,
and x is said to almost surely converge to µ. The almost sure convergence is a more
direct and stronger concept than the convergence in probability.
Fig. 7.2 exhibits the behavior of the sample average x =
1
n
n
i=1 xi when
x1,. . . , xn are i.i.d. with the standard normal distribution N(0,1) or the standard
Cauchy distribution Ca(0,1). The graphs show that, for the normal distribution which
possesses the expectation, the increase of n yields the convergence of the sample
average x to the true expectation 0. On the other hand, for the Cauchy distribution
which does not have the expectation, the sample average x does not converge even if
n is increased.
7.4 CENTRAL LIMIT THEOREM
As explained in Section 7.2, the average of independent normal samples follows the
normal distribution. If the samples follow other distributions, which distribution does

78
CHAPTER 7
SUM OF INDEPENDENT RANDOM VARIABLES
(a) Continuous uniform distribution U(0, 1)
(b) Exponential distribution Exp(1)
(c) Distribution used in Fig. 19.11
FIGURE 7.3
Central limit theorem. The solid lines denote the normal densities.
the sample average follow? Fig. 7.3 exhibits the histograms of the sample averages
for the continuous uniform distribution U(0,1), the exponential distribution Exp(1),
and the probability distribution used in Fig. 19.11, together with the normal densities
with the same expectation and variance. This shows that the histogram of the sample
average approaches the normal density as the number of samples, n, increases.
The central limit theorem asserts this fact more precisely: for standardized random
variable
z = x −µ
σ/ √n,

7.4 CENTRAL LIMIT THEOREM
79
the following property holds:
lim
n→∞Pr(a ≤z ≤b) =
b
a
1
√
2π
e−x2/2dx.
Since the right-hand side is the probability density function of the standard normal
distribution integrated from a to b, z is shown to follow the standard normal
distribution in the limit n →∞. In this case, z is said to converge in law or
converge in distribution to the standard normal distribution. More informally, z is
said to asymptotically follow the normal distribution or z has asymptotic normality.
Intuitively, the central limit theorem shows that, for any distribution, as long as it has
the expectation µ and variance σ2, the sample average x approximately follows the
normal distribution with expectation µ and variance σ2/n when n is large.
Let us prove the central limit theorem by showing that the moment-generating
function of
z = x −µ
σ/ √n
is given by the moment-generating function of the standard normal distribution, et2/2.
Let
yi = xi −µ
σ
and express z as
z =
1
√n
n

i=1
xi −µ
σ
=
1
√n
n

i=1
yi.
Since yi has expectation 0 and variance 1, the moment-generating function of yi is
given by
Myi(t) = 1 + 1
2t2 + · · · .
This implies that the moment-generating function of z is given by
Mz(t) =

Myi/ √n(t)
n =

Myi
t√n
n
=

1 + t2
2n + · · ·
n
.
If the limit n →∞of the above equation is considered, Eq. (3.5) yields
lim
n→∞Mz(t) = et2/2,
which means that z follows the standard normal distribution.


CHAPTER
PROBABILITY
INEQUALITIES
8
CHAPTER CONTENTS
Union Bound...................................................................... 81
Inequalities for Probabilities ...................................................... 82
Markov’s Inequality and Chernoff’s Inequality .............................. 82
Cantelli’s Inequality and Chebyshev’s Inequality ............................ 83
Inequalities for Expectation ....................................................... 84
Jensen’s Inequality .......................................................... 84
Hölder’s Inequality and Schwarz’s Inequality ............................... 85
Minkowski’s Inequality ...................................................... 86
Kantorovich’s Inequality..................................................... 87
Inequalities for the Sum of Independent Random Variables........................ 87
Chebyshev’s Inequality and Chernoff’s Inequality ........................... 88
Hoeffding’s Inequality and Bernstein’s Inequality........................... 88
Bennett’s Inequality......................................................... 89
If probability mass/density function f (x) is given explicitly, the values of the
probability (density) can be computed. However, in reality, f (x) itself may not be
given explicitly, but only partial information such as the expectation E[x] or the
variance V[x] is given. In this chapter, various inequalities are introduced that can
be used for evaluating the probability only from partial information. See [19] for
more details.
8.1 UNION BOUND
Let us recall the additive law of probabilities shown in Section 2.2:
Pr(A ∪B) = Pr(A) + Pr(B) −Pr(A ∩B).
Since Pr(A ∩B) is non-negative, the following inequality is immediately obtained:
Pr(A ∪B) ≤Pr(A) + Pr(B),
which is called the union bound. Even if Pr(A∪B) is difficult to obtain explicitly, the
union bound gives its upper bound from the probabilities of each event. The union
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00019-4
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
81

82
CHAPTER 8
PROBABILITY INEQUALITIES
bound can be extended to multiple events: for A1,. . . , AN,
Pr(A1 ∪· · · ∪AN) ≤Pr(A1) + · · · + Pr(AN).
8.2 INEQUALITIES FOR PROBABILITIES
In this section, inequalities for probabilities based on the expectation and variance
are introduced.
8.2.1 MARKOV’S INEQUALITY AND CHERNOFF’S
INEQUALITY
For non-negative random variable x having expectation E[x],
Pr x ≥a ≤E[x]
a
(8.1)
holds for any positive scalar a (Fig. 8.1). This is called Markov’s inequality, which
allows us to know the upper bound of the probability only from the expectation. Since
Pr(x < a) = 1 −Pr(x ≥a), a lower bound can also be obtained similarly:
Pr x < a ≥1 −E[x]
a
.
Markov’s inequality can be proved by the fact that the function
g(x) =

a
(x ≥a),
0
(0 ≤x < a),
defined for x ≥0 satisfies x ≥g(x):
E[x] ≥E[g(x)] = a Pr(x ≥a).
For arbitrary non-negative and monotone increasing function ϕ(x), Markov’s
inequality can be generalized as
Pr(x ≥a) = Pr

ϕ(x) ≥ϕ(a)

≤E[ϕ(x)]
ϕ(a)
.
(8.2)
Setting ϕ(x) = et x for t > 0 in Eq. (8.2) yields
Pr(x ≥a) = Pr

et x ≥eta
≤E[et x]
eta
,
(8.3)
which is called Chernoff’s inequality. Minimizing the right-hand side of (8.3) with
respect to t yields a tighter upper bound.

8.2 INEQUALITIES FOR PROBABILITIES
83
FIGURE 8.1
Markov’s inequality.
8.2.2 CANTELLI’S INEQUALITY AND CHEBYSHEV’S
INEQUALITY
Markov’s inequality upper-bounds the probability based on the expectation E[x].
Here, upper bounds of the probability based on the variance V[x] in addition to the
expectation E[x] are introduced.
When a random variable x possesses the expectation E[x] and variance V[x], the
generic inequality (coming from a ≥b =⇒a2 ≥b2)
Pr a ≥b ≤Pr a2 ≥b2
and Markov’s inequality (8.1) yield that for a positive scalar ε,
Pr

x −E[x] ≥ε

= Pr

ε(x −E[x]) + V[x] ≥V[x] + ε2
≤Pr
ε(x −E[x]) + V[x]	2 ≥V[x] + ε2	2
≤Eε(x −E[x]) + V[x]	2
V[x] + ε2	2
=
V[x]
V[x] + ε2 .
This is called Cantelli’s inequality or one-sided Chebyshev’s inequality. Similarly,
the following inequality also holds:
Pr

x −E[x] ≤−ε

≤
V[x]
V[x] + ε2 .
Furthermore, Markov’s inequality (8.1) yields
Pr

|x −E[x]| ≥ε

= Pr

(x −E[x])2 ≥ε2
≤V[x]
ε2 ,
(8.4)
which is called Chebyshev’s inequality (Fig. 8.2). While Markov’s inequality can only
bound one-sided probabilities, Chebyshev’s inequality allows us to bound two-sided
probabilities. A lower bound can also be obtained similarly:
Pr

|x −E[x]| < ε

≥1 −V[x]
ε2 .
(8.5)

84
CHAPTER 8
PROBABILITY INEQUALITIES
FIGURE 8.2
Chebyshev’s inequality.
Chebyshev’s inequality can be extended to an arbitrary interval [a,b] as
Pr

a < x < b

≥1 −
V[x] +

E[x] −a+b
2
2
b−a
2
2
,
which can be proved by applying Markov’s inequality as
Pr

(x ≤a) ∪(b ≤x)

= Pr

x −a + b
2

≥b −a
2

= Pr *
,

x −a + b
2
2
≥
b −a
2
2
+
-
≤E[(x −a+b
2 )2]
b−a
2
2
=
V[x] +

E[x] −a+b
2
2
b−a
2
2
.
Note that the above inequality is reduced to original Chebyshev’s inequality (8.5) by
setting
a = −ε + E[x] and b = ε + E[x].
8.3 INEQUALITIES FOR EXPECTATION
In this section, inequalities for the expectation are introduced.
8.3.1 JENSEN’S INEQUALITY
For all θ ∈[0,1] and all a < b, a real-valued function h(x) that satisfies
h θa + (1 −θ)b ≤θh(a) + (1 −θ)h(b)
(8.6)
is called a convex function (see Fig. 8.3).

8.3 INEQUALITIES FOR EXPECTATION
85
FIGURE 8.3
Convex function and tangent line.
If h(x) is convex, for any c, there exists a tangent line
g(x) = αx + β
such that it touches h(x) at c and lower-bounds h(x) (Fig. 8.3):
g(c) = h(c) and g(x) ≤h(x) for all x.
Setting c = E[x] yields
Eh(x) ≥Eg(x) = αE[x] + β = g(E[x]) = h(E[x]),
which is called Jensen’s inequality. In practice, computing the expectation Eh(x)
is often hard due to nonlinear transformation h(x). On the other hand, computing
h(E[x]) may be straightforward because it is just a nonlinear transformation of the
expectation. Thus, Jensen’s inequality allows us to know a lower bound of Eh(x)
even if it is hard to compute.
Jensen’s inequality can be extended to multidimensional convex function h(x):
Eh(x) ≥h(E[x]).
8.3.2 HÖLDER’S INEQUALITY AND SCHWARZ’S
INEQUALITY
For scalars p and q such that
1
p + 1
q = 1,
if random variables |x|p and |y|q possess the expectations, the following Hölder’s
inequality holds:
E|xy| ≤ E|x|p1/p  E|y|q1/q .
(8.7)

86
CHAPTER 8
PROBABILITY INEQUALITIES
Hölder’s inequality can be proved as follows. For 0 ≤θ ≤1, setting h(x) = ex in
Eq. (8.6) yields
eθa+(1−θ)b ≤θea + (1 −θ)eb.
Setting
θ = 1
p,
1 −θ = 1
q,
a = log
|x|p
E|x|p,
and b = log
|y|q
E|y|q
yields
|xy|
 E|x|p1/p  E|y|q1/q ≤1
p
|x|p
E|x|p + 1
q
|y|q
E|y|q.
Taking the expectation of both sides yields Eq. (8.7) because the expectation of the
right-hand side is 1.
Hölder’s inequality for p = q = 2 is particularly referred to as Schwarz’s
inequality:
E|xy| ≤

E|x|2 
E|y|2.
8.3.3 MINKOWSKI’S INEQUALITY
For p ≥1, Minkowski’s inequality is given by
 E|x + y|p1/p ≤ E|x|p1/p +  E|y|p1/p .
(8.8)
Minkowski’s inequality can be proved as follows. A generic inequality
|x + y| ≤|x| + |y|
yields
E|x + y|p ≤E|x| · |x + y|p−1 + E|y| · |x + y|p−1.
When p = 1, this immediately yields Eq.(8.8). When p > 1, applying Hölder’s
inequality to each term in the right-hand side yields
E|x| · |x + y|p−1 ≤ E|x|p1/p 
E|x + y|(p−1)q1/q ,
E|y| · |x + y|p−1 ≤ E|y|p1/p 
E|x + y|(p−1)q1/q ,
where q =
p
p−1. Then
E|x + y|p ≤
 E|x|p1/p +  E|y|p1/p E|x + y|p1−1/p
holds, and dividing the both side by  E|x + y|p1−1/p yields Eq. (8.8).

8.4 INEQUALITY FOR THE SUM
87
8.3.4 KANTOROVICH’S INEQUALITY
For random variable x such that 0 < a ≤x ≤b, Kantorovich’s inequality is given by
E[x]E
1
x

≤(a + b)2
4ab
.
(8.9)
Kantorovich’s inequality can be proved as follows. A generic inequality
0 ≤(b −x)(x −a) = (a + b −x)x −ab
yields
1
x ≤a + b −x
ab
,
which yields
E[x]E
1
x

≤E[x](a + b −E[x])
ab
.
Completing the square as
E[x](a + b −E[x]) = −

E[x] −a + b
2
2
+ (a + b)2
4
≤(a + b)2
4
yields
E[x]E
1
x

≤−(E[x] −(a + b)/2)2 + (a + b)2/4
ab
≤(a + b)2
4ab
,
which proves Eq. (8.9).
8.4 INEQUALITIES FOR THE SUM OF INDEPENDENT
RANDOM VARIABLES
In this section, inequalities for the sum and average of independent random variables
x1,. . . , xn,
x =
n

i=1
xi
and
x = 1
n
n

i=1
xi,
are introduced.

88
CHAPTER 8
PROBABILITY INEQUALITIES
8.4.1 CHEBYSHEV’S INEQUALITY AND CHERNOFF’S
INEQUALITY
For x −E[x], Chebyshev’s inequality (8.4) yields
Pr

|x −E[x]| ≥ε

= Pr

(x −E[x])2 ≥ε2
≤V[x]
ε2
=
n
i=1 V[xi]
ε2
.
When V[x1] = · · · = V[xn] = σ2, this yields
Pr

|x −E[x]| ≥ε

≤σ2
nε2 .
This upper bound is proportional to 1/n.
Similarly, for arbitrary positive t, Chernoff’s inequality (8.3) yields
Pr

x −E[x] ≥ε

≤exp (−tε) E

exp *
,
t
n

i=1
(xi −E[xi])+
-

= exp (−tε)
n

i=1
E

exp

t(xi −E[xi])

.
(8.10)
This upper bound is the product of the moment-generating functions of xi −E[xi]
for i = 1,. . . ,n, and therefore it is expected to decrease exponentially with respect
to n.
8.4.2 HOEFFDING’S INEQUALITY AND BERNSTEIN’S
INEQUALITY
For random variables xi such that ai ≤xi ≤bi for i = 1,. . . ,n, applying Hoeffding’s
formula,
E

exp

t(xi −E[xi])

≤exp
t2(bi −ai)2
8

,
to Chernoff’s inequality (8.10) yields
Pr

x −E[x] ≥ε

≤exp *
,
t2
8
n

i=1
(bi −ai)2 −tε+
-
.
Setting
t =
4ε
n
i=1(bi −ai)2

8.4 INEQUALITY FOR THE SUM
89
to minimize the above upper bound yields
Pr

x −E[x] ≥ε

≤exp

−
2ε2
n
i=1(bi −ai)2

.
This is called Hoeffding’s inequality. Its variant for sample average x is given as
Pr

x −E[x] ≥ε

≤exp *
,
−
2nε2
1
n
n
i=1(bi −ai)2 +
-
.
For random variables xi such that |xi −E[xi]| ≤a for i = 1,. . . ,n, Bernstein’s
inequality is given as
Pr

x −E[x] ≥ε

≤exp

−
ε2
2 n
i=1 V[xi] + 2aε/3

.
Its derivation will be explained in Section 8.4.3. Its variant for sample average x is
given as
Pr

x −E[x] ≥ε

≤exp *
,
−
nε2
2
n
n
i=1 V[xi] + 2aε/3
+
-
.
When V[x1] = · · · = V[xn] = ε, this yields
Pr

x −E[x] ≥ε

≤exp

−
nε
2 + 2a/3

.
Thus, for small positive ε, Bernstein’s inequality exp(−nε) gives a tighter upper
bound than Hoeffding’s inequality exp(−nε2). This is because Bernstein’s inequality
uses the variance of V[xi], while Hoeffding’s inequality only uses the domain [ai,bi]
of each random variable xi.
8.4.3 BENNETT’S INEQUALITY
For random variables xi such that |xi −E[xi]| ≤a for i = 1,. . . ,n, applying Bennett’s
formula,
E

exp

t(xi −E[xi])

≤exp

V[xi]exp(ta) −1 −ta
a2

,
to Chernoff’s inequality (8.10) yields
Pr

x −E[x] ≥ε

≤exp *
,
n

i=1
V[xi]exp(ta) −1 −ta
a2
−tε+
-
.
Setting
t = 1
a log

aε
n
i=1 V[xi] + 1


90
CHAPTER 8
PROBABILITY INEQUALITIES
FIGURE 8.4
h(u) = (1 +u) log(1 +u) −u and g(u) =
u2
2+2u/3.
to minimize the above upper bound yields
Pr

x −E[x] ≥ε

≤exp

−
n
i=1 V[xi]
a2
h

aε
n
i=1 V[xi]

,
where
h(u) = (1 + u) log(1 + u) −u.
This is called Bennett’s inequality.
For u ≥0, the following inequality holds (Fig. 8.4):
h(u) ≥g(u) =
u2
2 + 2u/3.
Further upper-bounding Bennett’s inequality by this actually gives Bernstein’s
inequality explained in Section 8.4.2. Thus, Bennett’s inequality gives a tighter upper
bound than Bernstein’s inequality, although it is slightly more complicated than
Bernstein’s inequality.

CHAPTER
STATISTICAL
ESTIMATION
9
CHAPTER CONTENTS
Fundamentals of Statistical Estimation............................................ 91
Point Estimation .................................................................. 92
Parametric Density Estimation .............................................. 92
Nonparametric Density Estimation .......................................... 93
Regression and Classification ............................................... 93
Model Selection ............................................................. 94
Interval Estimation ................................................................ 95
Interval Estimation for Expectation of Normal Samples..................... 95
Bootstrap Confidence Interval ............................................... 96
Bayesian Credible Interval .................................................. 97
So far, various properties of random variables and probability distributions have
been discussed. However, in practice, probability distributions are often unknown
and only samples are available. In this chapter, an overview of statistical estimation
for identifying an underlying probability distribution from samples is provided.
9.1 FUNDAMENTALS OF STATISTICAL ESTIMATION
A quantity estimated from samples is called an estimator and is denoted with a “hat.”
For example, when the expectation µ of a probability distribution is estimated by the
sample average, its estimator is denoted as
µ = 1
n
n

i=1
xi.
An estimator is a function of samples {xi}n
i=1 and thus is a random variable. On the
other hand, if particular values are plugged in the estimator, the obtained value is
called an estimate.
A set of probability mass/density functions described with a finite-dimensional
parameter θ is called a parametric model and is denoted by g(x; θ). In the notation
g(x; θ), x before the semicolon is a random variable and θ after the semicolon is
a parameter. For example, a parametric model corresponding to the d-dimensional
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00020-0
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
91

92
CHAPTER 9
STATISTICAL ESTIMATION
normal distribution,
g(x; µ,Σ) =
1
(2π)d/2 
det(Σ)
exp

−1
2(x −µ)⊤Σ−1(x −µ)

,
has expectation vector µ and variance-covariance matrix Σ as parameters.
An approach to statistical estimation by identifying the parameter in a parametric
model is called a parametric method, while a nonparametric method does not use
parametric models or uses a parametric model having infinitely many parameters.
Below, samples D = {xi}n
i=1 are assumed i.i.d. with f (x) (see Section 7.3).
9.2 POINT ESTIMATION
Point estimation gives a best estimate of an unknown parameter from samples. Since
methods of point estimation will be extensively explored in Part 3 and Part 4, only a
brief overview is provided in this section.
9.2.1 PARAMETRIC DENSITY ESTIMATION
Maximum likelihood estimation determines the parameter value so that samples at
hand, D = {xi}n
i=1, are generated most probably. The likelihood is the probability
that the samples D are generated:
L(θ) =
n

i=1
g(xi; θ),
and maximum likelihood estimation maximizes the likelihood:
θML = argmax
θ
L(θ),
where argmaxθ L(θ) is the maximizer of L(θ) with respect to θ. See Chapter 12,
details of maximum likelihood estimation.
The parameter θ is regarded as a deterministic variable in maximum likelihood
estimation, while it is regarded as a random variable in Bayesian inference. Then the
following probabilities can be considered:
Prior probability: p(θ),
Likelihood: p(D|θ),
Posterior probability: p(θ|D).
Typical Bayesian point-estimators are given as the posterior expectation or the
posterior mode:
Posterior expectation:

θp(θ|D)dθ,
posterior mode: argmax
θ
p(θ|D).

9.2 POINT ESTIMATION
93
Estimating the posterior mode is often called maximum a posteriori probability
estimation. The posterior probability can be computed by Bayes’ theorem explained
in Section 5.4 as
p(θ|D) = p(D|θ)p(θ)
p(D)
=
p(D|θ)p(θ)

p(D|θ′)p(θ′)dθ′ .
Thus, given the likelihood p(D|θ) and the prior probability p(θ), the posterior
probability p(θ|D) can be computed. However, the posterior probability p(θ|D)
depends on subjective choice of the prior probability p(θ) and its computation
can be cumbersome if the posterior probability p(θ|D) has a complex profile. See
Chapter 17, Section 17.3, and Section 19.3 for the details of Bayesian inference.
Maximum likelihood estimation is sometimes referred to as frequentist inference
when it is contrasted to Bayesian inference.
9.2.2 NONPARAMETRIC DENSITY ESTIMATION
Kernel density estimation (KDE) is a nonparametric technique to approximate the
probability density function f (x) from samples D = {xi}n
i=1 as
fKDE(x) = 1
n
n

i=1
K(x, xi),
where K(x, x′) is a kernel function. Typically, the Gaussian kernel function,
K(x, x′) =
1
(2πh2)d/2 exp

−∥x −x′∥2
2h2

,
is used, where h > 0 is the bandwidth of the Gaussian function and d denotes the
dimensionality of x, and ∥x∥=
√
x⊤x denotes the Euclidean norm.
Nearest neighbor density estimation (NNDE) is another nonparametric method
given by
fNNDE(x) =
kΓ( d
2 + 1)
nπ
d
2 ∥x −xk ∥d ,
where xk denotes the kth nearest sample to x among x1,. . . , xn and Γ(·) denotes the
gamma function explained in Section 4.3.
See Chapter 16 for the derivation and properties of nonparametric density
estimation.
9.2.3 REGRESSION AND CLASSIFICATION
Regression is a problem to estimate a function from d-dimensional input x to a real
scalar output y based on input-output paired samples {(xi, yi)}n
i=1.

94
CHAPTER 9
STATISTICAL ESTIMATION
The method of least squares (LS) fits a regression model r(x; α) to data by
minimizing the squared sum of residuals:
αLS = argmin
α
n

i=1

yi −r(xi; α)
2.
A nonparametric Gaussian kernel model is a popular choice as a regression model:
r(x; α) =
n

j=1
αj exp *
,
−∥x −xj∥2
2h2
+
-
,
where h > 0 is the bandwidth of the Gaussian kernel. To avoid overfitting to noisy
samples, regularization (see Chapter 23) is effective:
αRLS = argmin
α

n

i=1

yi −r(xi; α)
2 + λ∥α∥2

,
where λ ≥0 is the regularization parameter to control the strength of regularization.
The LS method is equivalent to maximum likelihood estimation if output y is
modeled by the normal distribution with expectation r(x; α):
1
σ
√
2π
exp *
,
−
 y −r(x; α)2
2σ2
+
-
.
Similarly, the regularized LS method is equivalent to Bayesian maximum a posteriori
probability estimation if the normal prior probability,
1
(2πλ2)n/2 exp

−∥α∥2
2λ2

,
is used for parameter α = (α1,. . . ,αn)⊤. See Chapter 22, Chapter 23, Chapter 24,
and Chapter 25 for the details of regression.
When output value y takes c discrete categorical value, the function estimation
problem is called classification. When c = 2, setting y = ±1 allows us to naively use
(regularized) LS regression in classification. See Chapter 26, Chapter 27, Chapter 30,
and Chapter 28 for the details of classification.
9.2.4 MODEL SELECTION
The performance of statistical estimation methods depends on the choice of tuning
parameters such as the regularization parameters and the Gaussian bandwidth.
Choosing such tuning parameter values based on samples is called model selection.
In the frequentist approach, cross validation is the most popular model selection
method: First, samples D = {xi}n
i=1 are split into k disjoint subsets D1,. . . ,Dk.
Then statistical estimation is performed with D\Dj (i.e., all samples without

9.3 INTERVAL ESTIMATION
95
Dj), and its estimation error (such as the log-likelihood in density estimation, the
squared error in regression, and the misclassification rate in classification) for Dj is
computed. This process is repeated for all j = 1,. . . ,k and the model that minimizes
the average estimation error is chosen as the most promising one. See Chapter 14 for
the details of frequentist model selection.
In the Bayesian approach, the model M that maximizes the marginal likelihood,
p(D|M) =

p(D|θ,M)p(θ|M)dθ,
is chosen as the most promising one. This approach is called type-II maximum
likelihood estimation or the empirical Bayes method. See Section 17.4 for the details
of Bayesian model selection.
9.3 INTERVAL ESTIMATION
Since an estimator θ is a function of samples D = {xi}n
i=1, its value depends on
the realizations of the samples. Thus, it would be practically more informative if not
only a point-estimated value but also its reliability is provided. The interval that an
estimator θ is included with probability at least 1−α is called the confidence interval
with confidence level 1 −α. In this section, methods for estimating the confidence
interval are explained.
9.3.1 INTERVAL ESTIMATION FOR EXPECTATION OF
NORMAL SAMPLES
For one-dimensional i.i.d. samples x1,. . . , xn with normal distribution N(µ,σ2), if
the expectation µ is estimated by the sample average,
µ = 1
n
n

i=1
xi,
the standardized estimator
z = µ −µ
σ/ √n
follows the standard normal distribution N(0,1). Thus, the confidence interval of µ
with confidence level 1 −α can be obtained as

µ −σ
√n zα/2, µ + σ
√n zα/2

,
where [−zα/2,+zα/2] corresponds to the middle 1 −α probability mass of the
standard normal density (see Fig. 9.1). However, to compute the confidence interval
in practice, knowledge of the standard deviation σ is necessary.

96
CHAPTER 9
STATISTICAL ESTIMATION
FIGURE 9.1
Confidence interval for normal samples.
When σ is unknown, it is estimated from samples as
σ =


1
n −1
n

i=1
(xi −µ)2.
In this case, an estimator standardized with σ,
t = µ −µ
σ/ √n,
follows the t-distribution with n−1 degrees of freedom (Section 4.6). For this reason,
standardization with σ is sometimes called Studentization.
The middle 1 −α probability mass of the t-density (see Section 4.6) gives the
confidence interval with confidence level 1 −α as

µ −σ
√ntα/2, µ + σ
√ntα/2

,
where [−tα/2,+tα/2] corresponds to the middle 1−α probability mass of the t-density
with n −1 degrees of freedom (as in Fig. 9.1). As shown in Fig. 4.9, the t-density has
heavier tails than the normal density.
9.3.2 BOOTSTRAP CONFIDENCE INTERVAL
The above method for computing the confidence interval is applicable only to the
average of normal samples. For statistics other than the expectation estimated from
samples following a non-normal distribution, the probability distribution of the

9.3 INTERVAL ESTIMATION
97
FIGURE 9.2
Bootstrap resampling by sampling with replacement.
estimator cannot be explicitly obtained in general. In such a situation, the use of
bootstrap allows us to numerically compute the confidence interval.
In the bootstrap method, n pseudo samples D′ = {x′
i}n
i=1 are gathered by
sampling with replacement from the original set of samples D = {xi}n
i=1. Because
of sampling with replacement, some samples in the original set D = {xi}n
i=1 may be
selected multiple times and others may not be selected in D′ = {x′
i}n
i=1 (Fig. 9.2).
From the bootstrapped samples D′ = {x′
i}n
i=1, an estimator θ′ of the target statistic
is computed. These resampling and estimation procedures are repeated many times
and the histogram of the estimator θ′ can be constructed. Extracting the middle
1 −α probability mass of the histogram (as in Fig. 9.1) gives the confidence interval
[−bα/2,+bα/2] with confidence level 1 −α.
As illustrated above, the bootstrap method allows us to construct the confidence
interval for any statistic and any probability distribution. Furthermore, not only the
confidence interval but also any statistics such as the variance and higher-order
moments of any estimator can be numerically evaluated by the bootstrap method.
However, since the resampling and estimation procedures need to be repeated many
times, the computation cost of the bootstrap method can be expensive.
9.3.3 BAYESIAN CREDIBLE INTERVAL
In Bayesian inference, the middle 1 −α probability mass of the posterior probability
p(θ|D) corresponds to the confidence interval with confidence level 1 −α. This
is often referred to as the Bayesian credible interval. Thus, in Bayesian inference,
the confidence interval can be naively obtained without additional computation.
However, if the posterior probability p(θ|D) has a complex profile, computation of
the confidence interval can be cumbersome. Moreover, dependency of the confidence
interval on subjective choice of the prior probability p(θ) can be an issue in practice.


CHAPTER
HYPOTHESIS TESTING10
CHAPTER CONTENTS
Fundamentals of Hypothesis Testing .............................................. 99
Test for Expectation of Normal Samples .......................................... 100
Neyman-Pearson Lemma ......................................................... 101
Test for Contingency Tables ...................................................... 102
Test for Difference in Expectations of Normal Samples ........................... 104
Two Samples without Correspondence ..................................... 104
Two Samples with Correspondence......................................... 105
Nonparametric Test for Ranks .................................................... 107
Two Samples without Correspondence ..................................... 107
Two Samples with Correspondence......................................... 108
Monte Carlo Test ................................................................. 108
When tossing a coin 20 times, heads are obtained 17 times. Can we then conclude
that the coin is biased? The framework of hypothesis testing allows us to answer this
question statistically. In this chapter, the basic idea of hypothesis testing and standard
tests is introduced.
10.1 FUNDAMENTALS OF HYPOTHESIS TESTING
The hypothesis that we want to test is called a null hypothesis, while the opposite is
called the alternative hypothesis. In the above coin-toss example, the null hypothesis
is that the coin is not biased (i.e., the probability of obtaining heads is 1/2), while the
alternative hypothesis is that the coin is biased (i.e., the probability of obtaining heads
is not 1/2). The null hypothesis and the alternative hypothesis are often denoted as
H0 and H1, respectively.
In hypothesis testing, probability that the current samples are obtained under the
null hypothesis is computed. If the probability, called the p-value, is less than the pre-
specified significance level α, then the null hypothesis is rejected; otherwise the null
hypothesis is accepted. Conventionally, significance level α is set at either 5% or 1%.
As shown in Section 3.2, the probability of obtaining heads in coin tossing follows
the binomial distribution. Thus, if the coin is not biased (i.e., the probability of
obtaining heads is 1/2), the probability that heads are obtained more than or equal to
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00021-2
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
99

100
CHAPTER 10
HYPOTHESIS TESTING
17 times for 20 trials is given by
20
17

+
20
18

+
20
19

+
20
20

×
1
2
20
≈0.0013.
If significance level α is set at 0.01, 0.0013 is less than the significance level. Thus,
the null hypothesis is rejected and the alternative hypothesis is accepted, and the coin
is concluded to be biased. Note that the probabilities of observing heads more than
or equal to 16, 15, and 14 times are 0.0059, 0.0207, and 0.0577, respectively. Thus,
if heads is observed no more than 15 times, the null hypothesis is accepted under
significance level α = 0.01 and the coin is concluded not to be biased.
As illustrated above, when rejecting a null hypothesis by hypothesis testing, the
null hypothesis is shown to seldom occur based on samples. On the other hand, when
a null hypothesis is accepted, its validity is not actively proved—there is no strong
enough evidence that the null hypothesis is wrong and thus the null hypothesis is
accepted inevitably. Such a logic is called proof by contradiction.
A two-sided test is aimed at testing whether the observed value is equal to a
target value. For example, if a computationally efficient algorithm of a machine
learning method is developed, a two-sided test is used to confirm whether the same
performance can still be obtained by the new algorithm. On the other hand, a one-
sided test is aimed at testing whether the observed value is larger (or smaller) than a
target value. If a new machine learning method is developed, a one-sided test is used
to see whether superior performance can be obtained by the new method.
In hypothesis testing, a test statistic z that can be computed from samples is
considered, and its probability distribution is computed under the null hypothesis. If
the value z of the test statistic computed from the current samples can occur only with
low probability, then the null hypothesis is rejected; otherwise the null hypothesis is
accepted. The region to which rejected z belongs is called a critical region, and its
threshold is called the critical value. The critical region in a two-sided test is the
left and right tails of the probability mass/density of test statistic z, while that in a
one-sided test is the left (or right) tail (see Fig. 10.1).
10.2 TEST FOR EXPECTATION OF NORMAL SAMPLES
For one-dimensional i.i.d. normal samples x1,. . . , xn with variance σ2, a test for the
null hypothesis that its expectation is µ is introduced.
Since the sample average,
µ = 1
n
n

i=1
xi,
follows normal distribution N(µ,σ2/n) under the null hypothesis that the expectation
is µ, its standardization,
z =
µ −µ

σ2/n
,

10.3 NEYMAN-PEARSON LEMMA
101
(a) Two-sided test (critical value zα/2)
(b) One-sided test (critical value zα)
FIGURE 10.1
Critical region and critical value.
follows the standard normal distribution N(0,1). The hypothesis test that uses the
above z as a test statistic is called a z-test. The critical region and critical values are
set in the same way as in Fig. 10.1.
When the variance σ2 is unknown, it is replaced with an unbiased estimator,
σ2 =
1
n −1
n

i=1
(xi −µ)2.
Then the test statistic,
t =
µ −µ

σ2/n
,
follows the t-distribution with n−1 degrees of freedom under the null hypothesis that
the expectation is µ [7]. This is called a t-test.
10.3 NEYMAN-PEARSON LEMMA
The error that the correct null hypothesis is rejected is called the type-I error or
false positive. In the coin-toss example, the type-I error corresponds to concluding
that an unbiased coin is biased. On the other hand, the error that the incorrect null
hypothesis is accepted is called the type-II error or false negative, which corresponds
to concluding that a biased coin is unbiased (Table 10.1). The type-II error is denoted
by β, and 1−β is called the power of the test. In the framework of hypothesis testing,
the type-I error is set at α, and the type-II error is reduced (equivalently the power is
increased) as much as possible.

102
CHAPTER 10
HYPOTHESIS TESTING
Table 10.1 Type-I Error α (False Positive) and Type-II Error β (False
Negative)
Truth
Test Result
Null Hypothesis is
Correct
Alternative Hypothesis is
Correct
Null hypothesis is accepted
OK
Type-II error (false
negative)
Alternative hypothesis is
accepted
Type-I error (false positive)
OK
Suppose that null hypothesis θ = θ0 and alternative hypothesis θ = θ1 are tested
using samples D = {x1,. . . , xn}. Under the null hypothesis θ = θ0, let ηα be a scalar
such that
Pr
L(θ0)
L(θ1) ≤ηα

= α,
where L(θ) is the likelihood. Then setting the critical value at ηα and the critical
region as
L(θ0)
L(θ1) ≤ηα
minimizes the type-II error (equivalently maximizes the power) subject to the
constraint that the type-I error is fixed at α. This is called the Neyman-Pearson lemma
and a test based on the ratio of likelihoods is called a likelihood-ratio test.
10.4 TEST FOR CONTINGENCY TABLES
In this section, a goodness-of-fit test and an independence test for contingency tables
(see Table 10.2) are introduced. For discrete random variables x ∈{1,. . . ,ℓ} and
y ∈{1,. . . ,m}, the Pearson divergence from px,y to qx,y is considered below:
ℓ

x=1
m

y=1
 px,y −qx,y
2
qx,y
.
(10.1)
In the goodness-of-fit test, the null hypothesis that the sample joint probability
mass function f (x, y) = cx,y/n is equivalent to a target value f (x, y) is tested. More
specifically, Pearson divergence (10.1) for
px,y = f (x, y) and qx,y = f (x, y)

10.4 TEST FOR CONTINGENCY TABLES
103
Table 10.2 Contingency Table for x ∈{1,. . . ,ℓ} and y ∈{1,. . . ,m}.
cx,y Denotes the Frequency of (x, y), dx = m
y=1 cx,y, ey = ℓ
x=1 cx,y, and
n = ℓ
x=1
m
y=1 cx,y
x \ y
1
· · ·
m
Total
1
c1,1
· · ·
c1,m
d1
...
...
. . .
...
...
ℓ
cℓ,1
· · ·
cℓ,m
dℓ
Total
e1
· · ·
em
n
is used as a test statistic, and the critical region is computed based on the fact that
the Pearson divergence follows the chi-squared distribution with ℓm −1 degrees of
freedom [7].
In the independence test, statistical independence between random variables x
and y is tested by considering the null hypothesis that the sample joint probability
mass function f (x, y) = cx,y/n is equivalent to the product of marginals g(x)h(y),
where
g(x) = dx
n = 1
n
m

y=1
cx,y,
h(y) = ey
n = 1
n
ℓ

x=1
cx,y.
More specifically, Pearson divergence (10.1) for
px,y = f (x, y) and qx,y = g(x)h(y)
is used as a test statistic, and the critical region is computed based on the fact that the
Pearson divergence follows the chi-squared distribution with (ℓ−1)(m −1) degrees
of freedom when x and y follow multinomial distributions.
The test that uses the Kullback-Leibler (KL) divergence (see Section 14.2),
ℓ

x=1
m

y=1
px,y log px,y
qx,y
,
instead of the Pearson divergence is called a G-test. This is a likelihood-ratio test
and the KL divergence approximately follows the chi-squared distribution with
(ℓ−1)(m −1) degrees of freedom.
A test whose test statistic (approximately) follows the chi-squared distribution
under the null hypothesis is called a chi-square test [7].

104
CHAPTER 10
HYPOTHESIS TESTING
10.5 TEST FOR DIFFERENCE IN EXPECTATIONS OF
NORMAL SAMPLES
Let D = {x1,. . . , xn} and D′ = {x′
1,. . . , x′
n′} be the i.i.d. samples with normal
distributions N(µ,σ2) and N(µ′,σ2), where the variance is common but the expecta-
tions can be different. In this section, a test for the difference in expectation, µ −µ′,
is introduced.
10.5.1 TWO SAMPLES WITHOUT CORRESPONDENCE
Let µ and µ′ be the sample averages for D = {x1,. . . , xn} and D′ = {x′
1,. . . , x′
n′}:
µ = 1
n
n

i=1
xi
and
µ′ = 1
n′
n′

i=1
x′
i.
Since D = {x1,. . . , xn} and D′ = {x′
1,. . . , x′
n′} are statistically independent of
each other, the variance of the difference in sample average, µ −µ′, is given by
σ2(1/n + 1/n′) under the null hypothesis µ = µ′. Thus, its standardization,
zu =
µ −µ′

σ2(1/n + 1/n′)
,
follows the standard normal distribution N(0,1). The test that uses the above zu as a
test statistic is called an unpaired z-test. The critical region and critical values are set
in the same way as in Fig. 10.1.
When the variance σ2 is unknown, it is replaced with an unbiased estimator:
σ2
u =
n
i=1(xi −µ)2 + n′
i=1(x′
i −µ′)2
n + n′ −2
.
Then the test statistic,
tu =
µ −µ′

σ2u(1/n + 1/n′)
,
follows the t-distribution with n+n′−2 degrees of freedom under the null hypothesis
µ = µ′. This is called an unpaired t-test.
If the variances of D = {x1,. . . , xn} and D′ = {x′
1,. . . , x′
n′} can be different, the
variances, say σ2 and σ′2, are replaced with their unbiased estimators:
σ2 =
n
i=1(xi −µ)2
n −1
,
σ′2 =
n′
i=1(x′
i −µ′)2
n′ −1
.

10.5 TEST FOR DIFFERENCE IN EXPECTATIONS OF NORMAL SAMPLES
105
Then, under the null hypothesis µ = µ′, the test statistic,
tW =
µ −µ′

σ2/n + σ′2/n′,
approximately follows the t-distribution with round(k) degrees of freedom, where
k =
 σ2/n + σ′2/n′2
σ4/(n2(n −1)) + σ′4/(n′2(n′ −1)),
and round(·) rounds off the value to the nearest integer. This is called Welch’s t-test.
Under the null hypothesis σ2 = σ′2, the test statistic,
F = σ2
σ′2 ,
follows the F-distribution explained in Section 4.6. This is called the F-test, which
allows us to test the equality of the variance.
10.5.2 TWO SAMPLES WITH CORRESPONDENCE
Suppose that two sets of samples, D = {x1,. . . , xn} and D′ = {x′
1,. . . , x′
n′}, have
correspondence, i.e., for n = n′, the samples are paired as
{(x1, x′
1),. . . ,(xn, x′
n)}.
Then, whether the expectations of D and D′ are equivalent can be tested by the
unpaired z-test:
zu =
∆µ

2σ2/n
,
where ∆µ is the average of the difference between the paired samples:
∆µ = 1
n
n

i=1
(xi −x′
i) = µ −µ′.
In this situation, the power of the test (see Section 10.3) can be improved if
positive correlation exists between the two samples. More specifically, under the null
hypothesis µ −µ′ = 0, the variance of ∆µ is given by 2σ2(1 −ρ)/n, where ρ is the
correlation coefficient:
ρ =
Cov[x, x′]

V[x]

V[x′]
.
Then its standardization,
zp =
∆µ

2σ2(1 −ρ)/n
,

106
CHAPTER 10
HYPOTHESIS TESTING
follows the standard normal distribution N(0,1). The test that uses the above zp as a
test statistic is called a paired z-test. If ρ > 0,
|zp| > |zu|
holds and thus the power of the test can be improved.
When the variance σ2 is unknown, it is replaced with
σ2
p =
n
i=1(xi −x′
i −∆µ)2
2(n −1)
.
(10.2)
Then the test statistic,
tp =
∆µ

2σ2p(1 −ρ)/n
,
follows the t-distribution with n −1 degrees of freedom under the null hypothesis
µ −µ′ = 0. This is called a paired t-test.
The test statistic of the unpaired t-test for n = n′ can be expressed as
tu =
∆µ

2σ2u/n
,
where
σ2
u =
n
i=1
 (xi −µ)2 + (x′
i −µ′)2
2(n −1)
.
σ2
u and σ2
p defined in Eq. (10.2) can be expressed by using σ2
u as
σ2
p =
n
i=1(xi −x′
i −∆µ)2
2(n −1)
=
n
i=1((xi −µ) −(x′
i −µ′))2
2(n −1)
=
n
i=1(xi −µ)2 + n
i=1(x′
i −µ′)2 −2 n
i=1(xi −µ)(x′
i −µ′)
2(n −1)
= σ2
u −
Cov[x, x′],
where 
Cov[x, x′] is the sample covariance given by

Cov[x, x′] =
1
n −1
n

i=1
(xi −µ)(x′
i −µ′).
If 
Cov[x, x′] > 0,
|tp| > |tu|
holds and thus the power of the test can be improved.

10.6 NONPARAMETRIC TEST FOR RANKS
107
Table 10.3 Wilcoxon Rank-Sum Test. In this Example, r1 = 3, r2 = 5.5,
r3 = 1, and the Rank-Sum is r = 9.5
D
x3
x1
x2
D′
x′
2
x′
4
x′
3
x′
1
Sample
value
−2
0
1
3.5
7
7
7.1
Rank
1
2
3
4
5.5
5.5
7
10.6 NONPARAMETRIC TEST FOR RANKS
In the previous section, samples D = {x1,. . . , xn} and D′ = {x′
1,. . . , x′
n′} were
assumed to follow normal distributions. When samples do not follow the normal
distributions, particularly in the presence of outliers, tests based on the normality
may not be reliable. In this section, nonparametric tests that do not require parametric
assumptions on the probability distributions are introduced.
10.6.1 TWO SAMPLES WITHOUT CORRESPONDENCE
Without loss of generality, assume n ≤n′ below (if n > n′, just D and D′ are
swapped to satisfy n ≤n′).
Let us merge all samples x1,. . . , xn and x′
1,. . . , x′
n′ together and sort them in the
ascending order. Let us denote the ranks of x1,. . . , xn in the set of n + n′ samples
by r1,. . . ,rn. If there are ties, the mean rank is used. For example, if xi the third
smallest sample in x1,. . . , xn and x′
1,. . . , x′
n′, ri is set at 3; if the fifth and sixth
smallest samples share the same value, their ranks are 5.5 (Table 10.3). The Wilcoxon
rank-sum test uses the sum of the ranks of x1,. . . , xn,
r =
n

i=1
ri,
a test statistic.
Under the null hypothesis that D and D′ follow the same probability distribution,
the above test statistic r approximately follows the normal distribution with expecta-
tion and variance given by
µ = n(n + n′ + 1)
2
,
σ2 = nn′(n + n′ + 1)
12
.
Since the standardized statistic (r −µ)/σ follows the standard normal distribution
N(0,1), setting the critical region and critical values as in Fig. 10.1 allows us to
perform hypothesis testing.
The Mann-Whitney U-test is essentially the same as the Wilcoxon rank-sum test.

108
CHAPTER 10
HYPOTHESIS TESTING
10.6.2 TWO SAMPLES WITH CORRESPONDENCE
For n = n′, suppose that two sets of samples D = {x1,. . . , xn} and D′ =
{x′
1,. . . , x′
n′} have correspondence as
{(x1, x′
1),. . . ,(xn, x′
n)}.
Let us remove pairs such that xi = x′
i and reduce the value of n accordingly (i.e., n
denotes the number of sample pairs such that xi , x′
i below). Let us sort the sample
pairs in the ascending order of |xi−x′
i|, and let ri be its rank. If there are ties, the mean
rank is used in the same way as in the Wilcoxon rank-sum test. Then the Wilcoxon
signed-rank test uses the sum of the ranks of samples such that xi −x′
i > 0,
s =

i:xi−x′
i >0
ri,
a test statistic.
Under the null hypothesis that D and D′ follow the same probability distribution,
the above test statistic s approximately follows the normal distribution with expecta-
tion and variance given by
µ = n(n + 1)
4
,
σ2 = n(n + 1)(2n + 1)
24
.
Since the standardized statistic (s −µ)/σ follows the standard normal distribution
N(0,1), setting the critical region and critical values as in Fig. 10.1 allows us to
perform hypothesis testing.
10.7 MONTE CARLO TEST
The test statistics introduced above all (approximately) follow the normal distribu-
tion, t-distribution, and chi-squared distribution under the null hypothesis. However,
if a test statistic is more complicated, its distribution cannot be analytically derived
even approximately. In such a situation, computing the value of a test statistic
using samples generated by a Monte Carlo method and numerically obtaining the
critical region are practically useful. The Monte Carlo method is a generic name
of algorithms that use random numbers, and its name stems from the Monte Carlo
Casino in Monaco. A test based on the Monte Carlo method is called a Monte Carlo
test.
For testing whether the expectation of samples D = {x1,. . . , xn} is µ (which was
discussed in Section 10.2) by a Monte Carlo test, the bootstrap method introduced in
Section 9.3.2 is used. More specifically, bootstrap resampling of D = {x1,. . . , xn}
and computing their average are repeated many times, and a histogram of the average
is constructed. Then hypothesis testing can be approximately performed by verifying

10.7 MONTE CARLO TEST
109
whether the target value µ is included in the critical region (see Fig. 10.1). The
p-value can also be approximated from the histogram. As illustrated above, the
bootstrap-based hypothesis test is highly general and can be applied to any statistic
computed from samples following any probability distribution.
In the contingency table explained in Section 10.4, enumeration of all possible
combinations allows us to obtain the probability distribution of any test statistic. For
2 × 2 contingency tables, computing the p-value by such an exhaustive way is called
Fisher’s exact test. In the Monte Carlo test, combinations are randomly generated and
the test is performed approximately. Since this allows us to numerically approximate
the p-value for contingency tables of arbitrary size and for arbitrary test statistic, it
can be regarded as generalization of Fisher’s exact test.
For testing the null hypothesis that D = {x1,. . . , xn} and D′ = {x′
1,. . . , x′
n′}
follow the same probability distributions, let us merge all samples x1,. . . , xn and
x′
1,. . . , x′
n′ together and partition them into two sets with sizes n and n′. Then
enumeration of all possible partitions allows us to obtain the probability distribution
of any test statistic. This is called a permutation test, and the Monte Carlo test can be
regarded as its approximate implementation with a limited number of repetitions.


PART
GENERATIVE
APPROACH TO
STATISTICAL
PATTERN
RECOGNITION 3
11
PATTERN RECOGNITION VIA GENERATIVE MODEL ESTIMATION . . . 113
12
MAXIMUM LIKELIHOOD ESTIMATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
13
PROPERTIES OF MAXIMUM LIKELIHOOD ESTIMATION. . . . . . . . . . . . . 139
14
MODEL SELECTION FOR MAXIMUM LIKELIHOOD ESTIMATION . . . . . 147
15
MAXIMUM LIKELIHOOD ESTIMATION FOR GAUSSIAN MIXTURE
MODEL. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
16
NONPARAMETRIC ESTIMATION. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
17
BAYESIAN INFERENCE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
18
ANALYTIC APPROXIMATION OF MARGINAL LIKELIHOOD . . . . . . . . . . 197

112
PART 3
STATISTICAL PATTERN RECOGNITION: GENERATIVE APPROACH
19
NUMERICAL APPROXIMATION OF PREDICTIVE DISTRIBUTION . . . . 205
20
BAYESIAN MIXTURE MODELS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
The objective of pattern recognition is to classify a given pattern x to one of the
pre-specified classes, y. For example, in hand-written digit recognition, pattern x is
an image of hand-written digit and class y corresponds to the number the image
represents. The number of classes is 10 (i.e., from “0” to “9”). Among various
approaches, statistical pattern recognition tries to learn a classifier based on statistical
properties of training samples. In Part 3, an approach to statistical pattern recognition
based on estimation of the data-generating probability distribution.
After the problem of pattern recognition based on generative model estimation
is formulated in Chapter 11, various statistical estimators are introduced. These
methods are categorizes as either parametric or non-parametric and either frequentist
or Bayesian.
First, a standard parametric frequentist method called maximum likelihood es-
timation is introduced in Chapter 12, its theoretical properties are investigated in
Chapter 13, the issue of model selection is discussed in Chapter 14, and the algorithm
for Gaussian mixture models called the expectation–maximization algorithm is intro-
duced in Chapter 15. Then, non-parametric frequentist methods called kernel density
estimation and nearest neighbor density estimation are introduced in Chapter 16.
The basic ideas of the parametric Bayesian approach is introduced in Chapter 17,
its analytic approximation methods are discussed in Chapter 18, and its numerical
approximation methods are introduced in Chapter 19. Then practical Bayesian
inference algorithms for Gaussian mixture models and topic models are introduced
in Chapter 20, which also includes a non-parametric Bayesian approach.

CHAPTER
PATTERN
RECOGNITION VIA
GENERATIVE MODEL
ESTIMATION
11
CHAPTER CONTENTS
Formulation of Pattern Recognition .............................................. 113
Statistical Pattern Recognition ................................................... 115
Criteria for Classifier Training..................................................... 117
MAP Rule .................................................................. 117
Minimum Misclassification Rate Rule...................................... 118
Bayes Decision Rule ....................................................... 119
Discussion.................................................................. 121
Generative and Discriminative Approaches ....................................... 121
In this chapter, the framework of pattern recognition based on generative model
estimation is first explained. Then, criteria for quantitatively evaluating the goodness
of a classification algorithm are discussed.
11.1 FORMULATION OF PATTERN RECOGNITION
In this section, the problem of statistical pattern recognition is mathematically
formulated.
Let x be a pattern (which is also called a feature vector, an input variable, an
independent variable, an explanatory variable, an exogenous variable, a predictor
variable, a regressor, and a covariate), which is a member of a subset X of the
d-dimensional Euclidean space Rd:
x ∈X ⊂Rd,
X is called the pattern space. Let y be a class (which is also called a category, an
output variable, a target variable, and a dependent variable) to which a pattern x
belongs. Let c be the number of classes, i.e.,
y ∈Y = {1,. . . ,c}.
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00022-4
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
113

114
CHAPTER 11
PATTERN RECOGNITION
(a) Hand-written digit
image
(b) Vectorization of two-dimensional image
FIGURE 11.1
Hand-written digit image and its vectorization.
In hand-written digit recognition, a scanned digit image is a pattern. If the scanned
image consists of 16 × 16 pixels, pattern x is a 256-dimensional real vector which
vertically stacks the pixel values as illustrated in Fig. 11.1. Rigorously speaking, pixel
values are integers (e.g., 0–255), but they are regarded as real numbers here. When the
pixel values are normalized to be in [0,1], the pattern space is given by X = [0,1]256.
Classes are the numbers “0,” “1,” ..., “9,” and thus the number of classes is c = 10.
A classifier is a mapping from a pattern x to a class y. Such a mapping is called a
discrimination function (see Fig. 11.2(a)) and is denoted by f (x). A region to which
patterns in class y belong is called a decision region (see Fig. 11.2(b)) and is denoted
by Xy. A boundary between decision regions is called a decision boundary. Thus,
pattern recognition is equivalent to dividing the pattern space X into decision regions
{Xy}c
y=1.
In practice, the discrimination function (or decision regions or decision bound-
aries) is unknown. Here, pattern x and class y are treated as random variables and
learn the optimal discrimination function based on their statistical properties. Such
an approach is called statistical pattern recognition.
Let us illustrate how hard directly constructing a discrimination function (or
decision regions and decision boundaries) in hand-written digit recognition. Let the

11.2 STATISTICAL PATTERN RECOGNITION
115
(a) Discrimination function
(b) Decision
region
and
decision
boundary
FIGURE 11.2
Constructing a classifier is equivalent to determine a discrimination function, decision regions,
and decision boundaries.
number of pixels be 100 (=10 × 10) and each pixel takes an integer from 0 to 255.
Then the number of possible images is
256100 = (28)100 = (210)80 ≈(103)80 = 10240,
which is an astronomical number having 240 zeros after the first one. Therefore,
even for a toy hand-written digit recognition example from tiny images with 10 × 10
pixels, just enumerating all possible images is not realistic. In practice, instead of
just memorizing classes of all possible patterns, the class of unlearned patterns may
be predicted from some learned patterns. The capability that unlearned patterns can
be classified correctly is called the generalization ability. The objective of pattern
recognition is to let a classifier being equipped with the generalization ability.
11.2 STATISTICAL PATTERN RECOGNITION
In this section, a statistical approach to pattern recognition is explained.
Suppose that pairs of patterns and their classes, called training samples, are
available:
{(xi, yi) | xi ∈X, yi ∈Y}n
i=1,
where n denotes the number of training samples. Among the n training samples, the
number of samples which belong to class y is denoted by ny. Below, the training
samples are assumed to be generated for i = 1,. . . ,n as the following:
1. Class yi is selected according to the class-prior probability p(y).
2. For chosen class yi, pattern xi is generated according to the class-conditional
probability density p(x|y = yi).

116
CHAPTER 11
PATTERN RECOGNITION
FIGURE 11.3
Dimensionality reduction onto a two-dimensional subspace by principal component analysis (see
Section 35.2.1).
Then training samples {(xi, yi)}n
i=1 independently follow the joint probability density
p(x, y), i.e., {(xi, yi)}n
i=1 are i.i.d. with p(x, y) (see Section 7.3). This i.i.d. assumption
is one of the most fundamental presuppositions in statistical pattern recognition.
Machine learning techniques when this i.i.d. assumption is violated are discussed
in Chapter 33; see also [81, 101].
Given training samples, what is the best way to learn the discrimination function?
Let us illustrate the distribution of patterns for the hand-written digit data shown in
Fig. 11.1. Since the hand-written digit samples are 256-dimensional, their distribution
cannot be directly visualized. Here, a dimensionality reduction method called
principal component analysis (PCA) is used to reduce the dimensionality from 256
to 2. More specifically, the variance-covariance matrix of training samples {xi}n
i=1 is
eigendecomposed (see Fig. 6.2), and eigenvalues λ1 ≥· · · ≥λd and corresponding
eigenvectors φ1,. . . ,φd are obtained. Then each training sample xi is transformed as
(φ⊤
1 xi,φ⊤
2 xi)⊤, where the eigenvectors φ1,. . . ,φd are assumed to be normalized to
have unit norm. Since φ⊤
j xi corresponds to the length of projection of xi along φj,
the above transformation is the projection of xi onto the subspace spanned by φ1 and
φ2 (Fig. 11.3). As detailed in Section 35.2.1, PCA gives the best approximation to
original data in a lower-dimensional subspace, and therefore it is often used for data
visualization.
The PCA projection of the hand-written digit data shown in Fig. 11.1 is plotted
in Fig. 11.4(a), showing that digit “2” is distributed more broadly than digit “1.” This
well agrees with the intuition that the shape of “2” may have more individuality than
the shape of “1.”
What is the best decision boundary for the samples plotted in Fig. 11.4(a)?
Examples of decision boundaries are illustrated in Fig. 11.4(b). The decision
boundary shown by the solid line is a complicated curve, but it can perfectly separate
“1” and “2.” On the other hand, the decision boundaries shown by the dashed line
and dashed-dotted line are much simpler, but some training samples are classified

11.3 CRITERIA FOR CLASSIFIER TRAINING
117
(a) Distribution of hand-written digits
(b) Examples of decision boundary
FIGURE 11.4
Illustration of hand-written digit samples in the pattern space.
incorrectly. For the purpose of classifying training samples, the decision boundary
shown by the solid line is better than those shown by the dashed line and dashed-
dotted line. However, the true objective of pattern recognition is not only to classify
training samples correctly but also to classify unlearned test samples given in the
future, i.e., to acquire the generalization ability, as mentioned in Section 11.1.
11.3 CRITERIA FOR CLASSIFIER TRAINING
In order to equip a classifier with the generalization ability, it is important to define a
criterion that quantitatively evaluate the goodness of a discrimination function (or
decision regions or decision boundaries). In this section, three examples of such
criteria are introduced and their relation is discussed.
11.3.1 MAP RULE
When deciding which class a given pattern belongs to, it would be natural to choose
the one with the highest probability. This corresponds to choosing the class that
maximizes the class-posterior probability p(y|x), i.e., pattern x is classified into class
y, where
y = argmax
y
p(y|x).
Here, “argmax” indicates the argument of the maximum, i.e., the maximizer of an
objective function. Such a decision rule is called the MAP rule. The MAP rule is
equivalent to setting the decision regions as follows (Fig. 11.5):
Xy = {x | p(y|x) ≥p(y′|x) for all y′ , y}.
(11.1)

118
CHAPTER 11
PATTERN RECOGNITION
FIGURE 11.5
MAP rule.
11.3.2 MINIMUM MISCLASSIFICATION RATE RULE
Another natural idea is to choose the class with the lowest misclassification error,
which is called the minimum misclassification rate rule.
Let pe(y →y′) be the probability that a pattern in class y is misclassified into
class y′. Since pe(y →y′) is equivalent to the probability that pattern x in class y
falls into decision region Xy′ (see Fig. 11.6), it is given by
pe(y →y′) =

x∈Xy′
p(x|y)dx.
Then the probability that a pattern in class y is classified into an incorrect class,
denoted by pe(y), is given as
pe(y) =

y′,y
pe(y →y′) =

y′,y

x ∈Xy′
p(x|y)dx
=

y′,y

x∈Xy′
p(x|y)dx +

x∈Xy
p(x|y)dx −

x ∈Xy
p(x|y)dx
= 1 −

x∈Xy
p(x|y)dx.
The second term in the last equation,

x∈Xy
p(x|y)dx,
denotes the probability that a pattern in class y is classified into class y, i.e., the
correct classification rate. Thus, the above equation shows the common fact that the
misclassification rate is given by one minus the correct classification rate.
Finally, the overall misclassification rate, denoted by pe, is given by the expecta-
tion of pe(y) over all classes:
pe =
c

y=1
pe(y)p(y).

11.3 CRITERIA FOR CLASSIFIER TRAINING
119
FIGURE 11.6
Minimum misclassification rate rule.
The minimum misclassification rate rule finds the classifier that minimizes the
above pe.
The overall misclassification rate pe can be expressed as
pe =
c

y=1
*
,
1 −

x ∈Xy
p(x|y)dx+
-
p(y)
=
c

y=1
p(y) −
c

y=1

x ∈Xy
p(x|y)p(y)dx
= 1 −
c

y=1

x∈Xy
p(y, x)dx = 1 −
c

y=1

x ∈Xy
p(y|x)p(x)dx.
Thus, minimizing pe is equivalent to determining the decision regions {Xy}c
y=1 so
that the second term, c
y=1

x ∈Xy p(y|x)p(x)dx, is maximized. This can be achieved
by setting Xy to be the set of all x such that
p(y|x) ≥p(y′|x)
for all y′ , y.
This is actually equivalent to Eq. (11.1), and therefore minimizing the misclassifica-
tion error is actually equivalent to maximizing the class-posterior probability.
11.3.3 BAYES DECISION RULE
According to the MAP rule (equivalently the minimum misclassification rate rule),
when the probability of precipitation is 40%, the anticipated weather is no rain. If
there will be no rain, then there is no need to carry an umbrella. However, perhaps
many people will bring an umbrella with them if the probability of precipitation is
40%. This is because, the loss of no rain when an umbrella is carried (i.e., need to
carry a slightly heavier bag) is much smaller than the loss of having rain when no
umbrella is carried (i.e., getting wet with rain and catching a cold) in reality (see
Table 11.1). In this way, choosing the class that has the smallest loss is called the
Bayes decision rule.

120
CHAPTER 11
PATTERN RECOGNITION
Table 11.1 Example Of Asymmetric Loss
Carry An Umbrella
Leave An Umbrella At
Home
Rain
Avoid get wet
Get wet and catch cold
No rain
Bag is heavy
Bag is light
Let ℓy,y′ be the loss that a pattern in class y is misclassified into class y′. Since the
probability that pattern x belongs to class y is given by the class-posterior probability
p(y|x), the expected loss for classifying pattern x into class y′ is given by
R(y′|x) =
c

y=1
ℓy,y′p(y|x).
This is called the conditional risk for pattern x.
In the Bayes decision rule, pattern x is classified into the class that incurs the
minimum conditional risk. More specifically, pattern x is classified into class y,
where
y = argmin
y
R(y|x).
This is equivalent to determining the decision regions {Xy}c
y=1 as
Xy = {x | R(y|x) ≤R(y′|x) for all y′ , y}.
The expectation of the conditional risk for all x is called the total risk:
R =

X
R(y|x)p(x)dx,
where y is an output of a classifier. The value of the total risk for the Bayes decision
rule is called the Bayes risk, and this is the lowest possible risk for the target
classification problem. Note that the Bayes risk is not zero in general, meaning that
the risk cannot be zero even with the optimally trained classifier.
Suppose the loss for correct classification is set at zero and the loss for incorrect
classification is set at a positive constant ℓ:
ℓy,y′ =

0
(y = y′),
ℓ
(y , y′).
(11.2)
Then the conditional risk is expressed as
R(y|x) = ℓ

y′,y
p(y′|x) = ℓ*.
,
c

y′=1
p(y′|x) −p(y|x)+/
-
= ℓ(1 −p(y|x)) .
(11.3)

11.4 GENERATIVE AND DISCRIMINATIVE APPROACHES
121
Since ℓis just a proportional constant, minimization of Eq. (11.3) is equivalent to
maximization of class-posterior probability p(y|x). Thus, when loss ℓy,y′ is given by
Eq. (11.2), the Bayes decision rule is reduced to the MAP rule (and therefore the
minimum misclassification rate rule, too).
11.3.4 DISCUSSION
Among the MAP rule, the minimum misclassification rate rule, and the Bayes
decision rule, the Bayes decision rule seems to be natural and the most powerful.
However, in practice, it is often difficult to precisely determine the loss ℓy,y′, which
makes the use of the Bayes decision rule not straightforward. For example, in the
rain-umbrella example described in Table 11.1, it would be clear that the loss of no
rain when an umbrella is carried is much smaller than the loss of having rain when
no umbrella is carried. However, it is not immediately clear how small the loss of no
rain when an umbrella is carried should be.
For this reason, in the following sections, we focus on the MAP rule (and therefore
the minimum misclassification rate rule, too).
11.4 GENERATIVE AND DISCRIMINATIVE
APPROACHES
Learning a classifier based on the MAP rule requires to find a maximizer of the
class-posterior probability p(y|x). Pattern recognition through estimation of the class-
posterior probability p(y|x) is called the discriminative approach and will be covered
in Part 4.
Another approach is to use the Bayes’ theorem explained in Section 5.4 to express
the class-posterior probability p(y|x) as
p(y|x) = p(x|y)p(y)
p(x)
.
Since the denominator in the right-hand side, p(x), is independent of class y, it can
be ignored when the class-posterior probability p(y|x) is maximized with respect to
y:
p(y|x) ∝p(x|y)p(y),
where “∝” means “proportional to.” Statistical pattern recognition through estimation
of the class-conditional probability density p(x|y) and the class-prior probability
p(y) is called the generative approach since
p(x|y)p(y) = p(x, y),
which is the data-generating probability distribution.
In the following chapters, the generative approach to statistical pattern recognition
is explored. The class-prior probability p(y) may simply be estimated by the ratio of

122
CHAPTER 11
PATTERN RECOGNITION
training samples in class y, i.e.,
p(y) = ny
n ,
(11.4)
where ny denotes the number of training samples in class y and n denotes the
number of all training samples. On the other hand, the class-conditional probability
p(x|y) is generally a high-dimensional probability density function, and therefore its
estimation is not straightforward. In the following chapters, various approaches to
estimating the class-conditional probability p(x|y) will be discussed.
For the sake of simplicity, the problem of estimating an unconditional probability
density p(x) from its i.i.d. training samples {xi}n
i=1 is considered in the following
chapters, because this allows us to estimate a conditional probability density p(x|y)
by only using ny samples in class y, {xi}i:yi=y, for density estimation.
Methods of probability density functions are categorized into parametric and
nonparametric methods. The parametric methods seek the best approximation to the
true probability density function from a parameterized family of probability density
functions, called a parametric model. For example, the Gaussian model contains
the mean vector and the variance-covariance matrix as parameters and they are
estimated from training samples. Once a parametric model is considered, the problem
of estimating a probability density function is reduced to the problem of learning a
parameter in the model. On the other hand, methods that do not use parametric models
are called nonparametric.
In the following chapters, various parametric and nonparametric methods will be
introduced.

CHAPTER
MAXIMUM LIKELIHOOD
ESTIMATION
12
CHAPTER CONTENTS
Definition ........................................................................ 123
Gaussian Model .................................................................. 125
Computing the Class-Posterior Probability ........................................ 127
Fisher’s Linear Discriminant Analysis (FDA) ...................................... 130
Hand-Written Digit Recognition .................................................. 133
Preparation ................................................................. 134
Implementing Linear Discriminant Analysis................................ 135
Multiclass Classification ................................................... 136
MLE is a generic method for parameter estimation proposed in the early twentieth
century. Thanks to its excellent theoretical and practical properties, it is still one of
the most popular techniques even now and it forms the basis of various advanced
machine learning techniques. In this chapter, the definition of MLE, its application to
Gaussian models, and its usage in pattern recognition are explained.
12.1 DEFINITION
In this section, the definition of MLE is provided.
A set of probability density functions specified by a finite number of parameters
is called a parametric model. Let us denote a parametric model by q(x; θ), a
parameter vector by θ, and the domain of parameters by Θ, respectively. Let b be
the dimensionality of θ:
θ = (θ(1),. . . ,θ(b))⊤.
In the notation q(x; θ), x before the semicolon is a random variable and θ after the
semicolon is a parameter.
A natural idea to specify the value of parameter θ is to maximize the chance
of obtaining the current training samples {x}n
i=1. To this end, let us consider the
probability that the training samples {x}n
i=1 are produced under parameter θ. This
probability viewed as a function of parameter θ is called the likelihood and denoted
by L(θ). Under the i.i.d. assumption (see Section 11.2), the likelihood is expressed as
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00023-6
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
123

124
CHAPTER 12
MAXIMUM LIKELIHOOD ESTIMATION
FIGURE 12.1
Likelihood equation, setting the derivative of
the likelihood to zero, is a necessary condition
for the maximum likelihood solution but is not
a sufficient condition in general.
L(θ) =
n

i=1
q(xi; θ).
MLE finds the maximizer of the likelihood,
θML = argmax
θ∈Θ
L(θ),
and then a density estimator is given by
p(x) = q(x; θML).
If parametric model q(x; θ) is differentiable with respect to θ, θML satisfies
∂
∂θ L(θ)
θ=θML
= 0b,
(12.1)
where 0b denotes the b-dimensional zero vector, and
∂
∂θ denotes the partial
derivative with respect to θ. The partial derivative with respect to vector θ gives the
b-dimensional vector whose ℓth element is given by
∂
∂θ(ℓ) :
∂
∂θ =

∂
∂θ(1) ,. . . ,
∂
∂θ(b)
⊤
.
Eq. (12.1) is called the likelihood equation and is a necessary condition for the
maximum likelihood solution. Note, however, that it is not generally a sufficient
condition, i.e., the maximum likelihood solution always satisfies Eq. (12.1), but
solving Eq. (12.1) does not necessarily give the solution (Fig. 12.1).

12.2 GAUSSIAN MODEL
125
FIGURE 12.2
Log function is monotone increasing.
Since the log function is monotone increasing, the maximizer of the likelihood
can also be obtained by maximizing the log-likelihood (Fig. 12.2):
θML = argmax
θ∈Θ
log L(θ) = argmax
θ∈Θ

n

i=1
log q(xi; θ)

.
While the original likelihood contains the product of probability densities, the log-
likelihood contains the sum of log probability densities, which is often easier to
compute in practice. The likelihood equation for the log-likelihood is given by
∂
∂θ log L(θ)
θ=θML
= 0b.
Below, MLE for the Gaussian model is explained in detail. MLE for the Gaussian
mixture model will be explained in Chapter 15.
12.2 GAUSSIAN MODEL
In Section 4.2 and Section 6.2, the Gaussian distribution was introduced. The
Gaussian model corresponds to a parametric model for the Gaussian distribution and
is given for d-dimensional pattern x as
q(x; µ,Σ) =
1
(2π)
d
2 det(Σ)
1
2
exp

−1
2(x −µ)⊤Σ−1(x −µ)

.
Here, d-dimensional vector µ and d × d matrix Σ are the parameters of the Gaussian
model, and det(·) denotes the determinant. µ and Σ correspond to the expectation

126
CHAPTER 12
MAXIMUM LIKELIHOOD ESTIMATION
The following formulas hold for vector and matrix derivatives:
∂µ⊤Σ−1µ
∂µ
= 2Σ−1µ, ∂x⊤Σ−1µ
∂µ
= Σ−1x,
∂x⊤Σ−1x
∂Σ
= −Σ−1xx⊤Σ−1, ∂log(det(Σ))
∂Σ
= Σ−1, ∂tr(Σ
−1Σ)
∂Σ
= Σ
−1.
FIGURE 12.3
Formulas for vector and matrix derivatives [80].
matrix and the variance-covariance matrix, respectively,
µ = E[x] =

xq(x; µ,Σ)dx,
Σ = V[x] =

(x −µ)(x −µ)⊤q(x; µ,Σ)dx.
For i.i.d. training samples {xi}n
i=1 following the Gaussian model q(x; µ,Σ), the
log-likelihood is given by
log L(µ,Σ) = −nd log 2π
2
−n log(det(Σ))
2
−1
2
n

i=1
(xi −µ)⊤Σ−1(xi −µ).
The likelihood equation for the Gaussian model is given as

∂
∂µ log L(µ,Σ)
µ=µML
= 0d,
∂
∂Σ log L(µ,Σ)
Σ=ΣML
= Od×d,
where Od×d denotes the d × d zero matrix.
For deriving the maximum likelihood solutions, formulas for vector and matrix
derivatives shown in Fig. 12.3 are useful. Indeed, the partial derivatives of the log-
likelihood with respect to vector µ and matrix Σ are given by
∂log L
∂µ
= nΣ−1µ + Σ−1
n

i=1
xi,
∂log L
∂Σ
= −n
2Σ−1 + 1
2Σ−1 *
,
n

i=1
(xi −µ)(xi −µ)⊤+
-
Σ−1.

12.3 COMPUTING THE CLASS-POSTERIOR PROBABILITY
127
Then the maximum likelihood estimators µML and ΣML are given as
µML = 1
n
n

i=1
xi,
ΣML = 1
n
n

i=1
(xi −µML)(xi −µML)⊤,
which correspond to the sample mean and the sample variance-covariance matrix.
Here, we assumed that we have enough training samples so that ΣML is invertible.
The above Gaussian model considered a generic variance-covariance matrix Σ,
but a slightly simplified one having no correlation can also be considered (see
Fig. 6.1). This corresponds to restricting Σ to be a diagonal matrix:
Σ = diag

(σ(1))2,. . . ,(σ(d))2
.
Then the Gaussian model having no correlation can be expressed as
q(x; µ,σ(1),. . . ,σ(d)) =
d

j=1
1

2π(σ(j))2 exp

−(x(j) −µ(j))2
2(σ(j))2

,
where x(j) and µ(j) denotes the jth elements of d-dimensional vectors x and µ,
respectively. The maximum likelihood solution for σ(j) is given by
σ(j)
ML =


1
n
n

i=1
(x(j)
i
−µ(j)
i )2.
The Gaussian model can be further simplified if all variances (σ(j))2 are assumed
to be equal. Denoting the common variance by σ2, the Gaussian model is expressed
as
q(x; µ,σ) =
1
(2πσ2)
d
2
exp

−(x −µ)⊤(x −µ)
2σ2

.
Then the maximum likelihood solution for σ is given by
σML =


1
nd
n

i=1
(xi −µ)⊤(xi −µ) =



1
d
d

j=1
(σ(j)
ML)2.
A MATLAB code for MLE with one-dimensional Gaussian model is given in
Fig. 12.4, and its behavior is illustrated in Fig. 12.5.
12.3 COMPUTING THE CLASS-POSTERIOR
PROBABILITY
Let us come back to the pattern recognition problem and learn the class-conditional
probability density p(x|y) by MLE with Gaussian models:

128
CHAPTER 12
MAXIMUM LIKELIHOOD ESTIMATION
n=5; m=0; s=1; x=s*randn(n,1)+m; mh=mean(x); sh=std(x,1);
X=linspace(-4,4,100); Y=exp(-(X-m).^2./(2*s^2))/(2*pi*s);
Yh=exp(-(X-mh).^2./(2*sh^2))/(2*pi*sh);
figure(1); clf; hold on;
plot(X,Y,’r-’,X,Yh,’b--’,x,zeros(size(x)),’ko’);
legend(’True’,’Estimated’);
FIGURE 12.4
MATLAB code for MLE with one-dimensional Gaussian model.
FIGURE 12.5
Example of MLE with one-dimensional
Gaussian model.
p(x|y) =
1
(2π)
d
2 det(Σy)
1
2
exp

−1
2(x −µy)⊤Σ
−1
y (x −µy)

.
Here, µy and Σy are the maximum likelihood estimators of the expectation and
variance-covariance matrices for class y:
µy = 1
ny

i:yi=y
xi,
(12.2)
Σy = 1
ny

i:yi=y
(xi −µy)(xi −µy)⊤,
(12.3)
where ny denotes the number of training samples in class y.
As shown in Fig. 12.2, the log function is monotone increasing and maximizing
the log class-posterior probability is often more convenient than maximizing the plain

12.3 COMPUTING THE CLASS-POSTERIOR PROBABILITY
129
class-posterior probability. From the Bayes’ theorem explained in Section 5.4,
p(y|x) = p(x|y)p(y)
p(x)
,
the log class-posterior probability is expressed as
log p(y|x) = log p(x|y) + log p(y) −log p(x).
As shown in Eq. (11.4), the class-prior probability is simply estimated by the ratio of
training samples:
p(y) = ny
n .
Then the log class-posterior probability log p(y|x) can be estimated as
log p(y|x) = log p(x|y) + log p(y) −log p(x)
= −d
2 log(2π) −1
2 log(det(Σy)) −1
2(x −µy)⊤Σ
−1
y (x −µy)
+ log ny
n −log p(x)
= −1
2(x −µy)⊤Σ
−1
y (x −µy) −1
2 log(det(Σy)) + log ny
n + C,
where C is a constant independent of y. As shown above, if p(x|y) is estimated by a
Gaussian model, log p(y|x) becomes a quadratic function of x.
The first term in the above equation,
(x −µ)⊤Σ
−1(x −µ),
is called the Mahalanobis distance between x and µ. The Mahalanobis distance
regards the set of points on a hyperellipsoid specified by Σ as the same distance. To
understand this, let us eigendecompose (see Fig. 6.2) the variance-covariance matrix
Σ as
Σφ = λφ,
where {λ j}d
j=1 are the eigenvalues of Σ and {φj}d
j=1 are the corresponding eigenvec-
tors that are normalized to have a unit norm. Then Σ can be expressed as
Σ =
d

j=1
λ jφjφ⊤
j ,
and Σ
−1 is written explicitly as
Σ
−1 =
d

j=1
1
λ j
φjφ⊤
j .

130
CHAPTER 12
MAXIMUM LIKELIHOOD ESTIMATION
FIGURE 12.6
Orthogonal projection.
FIGURE 12.7
Mahalanobis distance having hy-
perellipsoidal contours.
Since {φj}d
j=1 are mutually orthogonal, the Mahalanobis distance can be expressed
as
(x −µ)⊤Σ
−1(x −µ) =
d

j=1
(φ⊤
j (x −µ))2
λ j
,
φ⊤
j (x −µ) corresponds to the length of the projection of x −µ onto φj (see Fig. 12.6).
Thus, the Mahalanobis distance is the squared sum of the projection lengths divided
by λ j, and the set of points having the same Mahalanobis distance (say, 1) satisfies
d

j=1
1
λ j
(φ⊤
j (x −µ))2 = 1.
This represents the hyperellipsoid with the principal axes parallel to {φj}d
j=1 and
length { λ j}d
j=1. When the dimensionality of x is d = 2, the hyperellipsoid is
reduced to the ellipse (Fig. 12.7).
When the number of classes is c = 2, the decision boundary is the set of points
having the same class-posterior probability (i.e., 1/2):
p(y = 1|x) = p(y = 2|x).
(12.4)
Therefore, if p(x|y) is estimated by a Gaussian model, the decision boundary is a
quadratic hypersurface.
12.4 FISHER’S LINEAR DISCRIMINANT ANALYSIS
(FDA)
Suppose further that the variance-covariance matrix of Σy of each class is the same:
Σ1 = · · · = Σc = Σ,

12.4 FISHER’S LINEAR DISCRIMINANT ANALYSIS (FDA)
131
where Σ is the common variance-covariance matrix that is independent of class y.
Then the maximum likelihood solution Σ for the common variance-covariance matrix
Σ is given by
Σ = 1
n
c

y=1

i:yi=y
(xi −µy)⊤(xi −µy) =
c

y=1
ny
n
Σy.
(12.5)
This shows that Σ is the weighted average of each solution Σy (see Eq. (12.3))
according to the ratio of samples ny/n.
With Σ, the log class-posterior probability log p(y|x) can be estimated as
log p(y|x) = −1
2 x⊤Σ
−1x + x⊤Σ
−1µy −1
2 µ⊤
y Σ
−1µy
−1
2 log(det(Σ)) + log ny
n + C
= x⊤Σ
−1µy −1
2 µ⊤
y Σ
−1µy + log ny
n + C′,
(12.6)
where C′ is a constant independent of y. Thus, when the variance-covariance matrix
is common to all classes, the log class-posterior probability is a linear function of x.
Eq.(12.4) implies that the decision boundary when the number of classes is c = 2
is given by
a⊤x + b = 0,
where
a = Σ
−1(µ1 −µ2),
b = −1
2(µ⊤
1 Σ
−1µ1 −µ⊤
2 Σ
−1µ2) + log n1
n2
.
This shows that the decision boundary is a hyperplane. This classification method is
called Fisher’s linear discriminant analysis (FDA) [41].
Let us compute the FDA solution for two-dimensional training samples plotted
in Fig. 12.8, where “◦” and “×” denote training samples in class 1 and class 2 for
n = 300 and n1 = n2 = 150. The Gaussian models with common variance-covariance
matrix are used to approximate p(x|y = 1) and p(x|y = 2), and the following
maximum likelihood estimators are obtained:
µ1 = *.
,
2
0
+/
-
,
µ2 = *.
,
−2
0
+/
-
,
and Σ = *.
,
1 0
0 9
+/
-
.
Then the FDA solution is given by
a = *.
,
4
0
+/
-
and b = 0.

132
CHAPTER 12
MAXIMUM LIKELIHOOD ESTIMATION
FIGURE 12.8
Linear discriminant analysis.
(a) n1/n2 = 1/9
(b) n1/n2 = 9
FIGURE 12.9
When the classwise sample ratio n1/n2 is changed.
The obtained decision boundary is plotted in Fig. 12.8, showing that the FDA solution
separates the two Gaussian distributions in the middle.
If the classwise sample ratio n1/n2 is changed, the decision boundary yields
a = *.
,
4
0
+/
-
and b = log n1
n2
.
As illustrated in Fig. 12.9, the decision boundary shifts depending on the classwise
sample ratio n1/n2.

12.5 HAND-WRITTEN DIGIT RECOGNITION
133
(a) β = π/4
(b) β = −π/4
FIGURE 12.10
When the classwise sample distributions are rotated.
Next, let us set n1/n2 = 1 again and change the common variance-covariance
matrix Σ to
Σ = *.
,
9 −8 cos2 β
8 sin β cos β
8 sin β cos β
9 −8 sin2 β
+/
-
,
which corresponds to rotating the classwise sample distributions by angle β. Then
the FDA solution is given by
a = *.
,
1 + 8 cos2 β
−8 sin β cos β
+/
-
and b = 0,
where
Σ
−1 = *.
,
1 −8
9 sin2 β
−8
9 sin β cos β
−8
9 sin β cos β
1 −8
9 cos2 β
+/
-
is used in the derivation. As illustrated in Fig. 12.10, the decision boundary rotates
according to the rotation of the data set.
12.5 HAND-WRITTEN DIGIT RECOGNITION
In this section, FDA is applied to hand-written digit recognition using MATLAB. The
digit data set available from
http://www.ms.k.u-tokyo.ac.jp/sugi/data/digit.mat
is used.

134
CHAPTER 12
MAXIMUM LIKELIHOOD ESTIMATION
FIGURE 12.11
Matrix and third tensor.
12.5.1 PREPARATION
First, let us load the digit data in the memory space as follows.
> load digit.mat
The whos function shows that variable X contains digit data for training and variable
T contains digit data for testing:
> whos
Name
Size
Bytes
Class
T
256x200x10
4096000
double
X
256x500x10
10240000
double
Here, X and T are the third tensors, meaning that they are arrays taking three
arguments (see Fig. 12.11). Each digit image is represented by a 256-dimensional
vector, which is a vectorized expression of a 16 × 16 pixel image, as explained in
Fig. 11.1(b). Each element take a real value in [−1,1], where −1 corresponds to black
and 1 corresponds to white. X contains 5000 digit samples (500 samples for each digit
from “0” to “9”) and T contains 2000 digit samples (200 samples for each digit from
“0” to “9”).
For example, the 23rd training sample of digit “5” can be extracted into variable
x as follows:
> x=X(:,23,5);
Note that digit “0” corresponds to 10 in the third argument. The extracted sample can
be visualized as follows:
> imagesc(reshape(x,[16 16])’)

12.5 HAND-WRITTEN DIGIT RECOGNITION
135
12.5.2 IMPLEMENTING LINEAR DISCRIMINANT
ANALYSIS
Let us implement FDA for classifying digits “1” and “2.” First, let us clear the
memory space and reload the digit data:
> clear all
> load digit.mat
Suppose that patterns of digits “1” and “2” follow the normal distribution with a
common variance-covariance matrix. The sample means of each class can be obtained
as follows (see Eq. (12.2)):
> mu1=mean(X(:,:,1),2);
> mu2=mean(X(:,:,2),2);
The common variance-covariance matrix can be obtained as follows (see Eq.(12.5)):
> S=(cov(X(:,:,1)’)+cov(X(:,:,2)’))/2;
Then the class-posterior probability for a test pattern can be computed as follows (see
Eq. (12.6)):
> t=T(:,1,2);
> invS=inv(S+0.000001*eye(256));
> p1=mu1’*invS*t-mu1’*invS*mu1/2;
> p2=mu2’*invS*t-mu2’*invS*mu2/2;
Note that irrelevant constants are ignored here, and 0.000001*eye(256) is added
to S to avoid numerical problems when it is inverted, where eye(256) denotes
the 256 × 256 identity matrix. Investigating the difference of the class-posterior
probabilities allows us to classify the test pattern:
> sign(p1-p2)
ans = -1
In this example, the test pattern is classified into class “2,” meaning that the
classification is successful. If sign(p1-p2) is 1, the test pattern is classified into
class “1.”
Classification results for all test patterns in class “2” can be obtained as follows:
> t=T(:,:,2);
> p1=mu1’*invS*t-mu1’*invS*mu1/2;
> p2=mu2’*invS*t-mu2’*invS*mu2/2;
> result=sign(p1-p2);

136
CHAPTER 12
MAXIMUM LIKELIHOOD ESTIMATION
(a) 69th test pattern
(b) 180th test pattern
FIGURE 12.12
Misclassified test patterns.
Note that p1 and p2 are the horizontal vectors. The classification results can be
summarized as follows:
> sum(result==-1)
ans = 198
> sum(result~=-1)
ans = 2
This shows that, among 200 test patterns in class “2,” 198 samples are correctly
classified and 2 samples are misclassified. Thus, the correct classification rate is
2/200 = 99%. Misclassified samples can be found as follows:
> find(result~=-1)
ans =
69
180
These test patterns are actually difficult to classify even for human (in particular, the
69th pattern), as illustrated in Fig. 12.12:
> imagesc(reshape(t(:,69),[16 16])’)
> imagesc(reshape(t(:,180),[16 16])’)
12.5.3 MULTICLASS CLASSIFICATION
Next, digit samples in classes “1,” “2,” and “3” are classified by FDA, under the
assumption that the variance-covariance matrix is common to all classes. A MATLAB
code is provided in Fig. 12.13. Here the classification results are summarized in
the confusion matrix, which contains the number of times patterns in class y is
categorized into class y′ in the (y, y′) element.

12.5 HAND-WRITTEN DIGIT RECOGNITION
137
clear all
load digit.mat X T
[d,m,c]=size(T); c=3;
S=zeros(d,d);
for y=1:c
mu(:,y)=mean(X(:,:,y),2);
S=S+cov(X(:,:,y)’)/c;
end
h=inv(S)*mu;
for y=1:c
p(:,:,y)=h’*T(:,:,y)-repmat(sum(mu.*h)’,[1,m])/2;
end
[pmax P]=max(p);
P=squeeze(P);
for y=1:c
C(:,y)=sum(P==y);
end
C
FIGURE 12.13
MATLAB code for multiclass classification by FDA.
The following confusion matrix is obtained:
> C
C =
199
1
0
0
192
8
0
2
198
This means that
•
199 test patterns in class “1” are correctly classified and the remaining 1 pattern
is misclassified as “2.”
•
192 test patterns in class “2” are correctly classified and the remaining 8 patterns
are misclassified as “3.”
•
198 test patterns in class “3” are correctly classified and the remaining 3 patterns
are misclassified as “2.”
If c=3; is commented out in the program in Fig. 12.13, all patterns in all 10
classes are classified. The obtained confusion matrix is shown in Fig. 12.14, showing

138
CHAPTER 12
MAXIMUM LIKELIHOOD ESTIMATION
FIGURE 12.14
Confusion matrix for 10-class classification by FDA. The
correct classification rate is 1798/2000 = 89.9%.
that 1798 test samples out of 2000 samples are correctly classified. Thus, the correct
classification rate is
1798/2000 = 89.9%.

CHAPTER
PROPERTIES OF
MAXIMUM LIKELIHOOD
ESTIMATION
13
CHAPTER CONTENTS
Consistency ...................................................................... 139
Asymptotic Unbiasedness ........................................................ 140
Asymptotic Efficiency ............................................................ 141
One-Dimensional Case ..................................................... 141
Multidimensional Cases .................................................... 141
Asymptotic Normality ............................................................ 143
Summary ........................................................................ 145
In the previous chapter, the definition of MLE and its usage in pattern recognition
were explained. In this chapter, the asymptotic behavior of MLE is theoretically
investigated. Throughout this chapter, the true probability density function p(x) is
assumed to be included in the parametric model q(x; θ), i.e., there exists θ∗such that
q(x; θ∗) = p(x).
13.1 CONSISTENCY
First, consistency of MLE is explored. Consistency is the property that the optimal
solution θ∗can be obtained asymptotically (i.e., in the limit that the number of
training samples n tends to infinity), which would be a minimum requirement for
reasonable estimators.
More precisely, let θn be an estimator obtained from n i.i.d. training samples.
Then, an estimator θn is said to be consistent if the following property is satisfied for
any θ∗∈Θ and any ε > 0:
lim
n→∞Pr(∥θn −θ∗∥≥ε) = 0.
This means that θn converges in probability to θ∗(Section 7.3) and is denoted as
θn
p
−→θ∗.
MLE was proved to be consistent under mild assumptions. For example, the
consistency of the maximum likelihood estimator of the expectation for the one-
dimensional Gaussian model, µML = 1
n
n
i=1 xi, was shown in Section 7.3.
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00024-8
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
139

140
CHAPTER 13
PROPERTIES OF MAXIMUM LIKELIHOOD ESTIMATION
FIGURE 13.1
Bias-variance decomposition of expected
squared error.
Next, let us investigate the relation between consistency and the squared error
∥θ −θ∗∥2. Markov’s inequality shown in Section 8.2.1 gives the following upper
bound:
Pr(∥θ −θ∗∥≥ε) = Pr(∥θ −θ∗∥2 ≥ε2) ≤1
ε2 E

∥θ −θ∗∥2
,
where E

∥θ −θ∗∥2
is the expected squared error and E denotes the expectation
over all training samples {xi}n
i=1 following i.i.d. with p(x):
E[•] =

· · ·

• p(x1) · · · p(xn)dx1 · · · dxn.
(13.1)
This upper bound shows that if the expected squared error of an estimator vanishes
asymptotically, the estimator possesses consistency.
13.2 ASYMPTOTIC UNBIASEDNESS
The expected squared error can be decomposed as
E

∥θ −θ∗∥2
= E

∥θ −E[θ] + E[θ] −θ∗∥2
= E

∥θ −E[θ]∥2
+ ∥E[θ] −θ∗∥2 + 2E

(θ −E[θ])⊤(E[θ] −θ∗)

= E

∥θ −E[θ]∥2
+ ∥E[θ] −θ∗∥2 + 2[(E[θ] −E[θ])⊤(E[θ] −θ∗)]
= E

∥θ −E[θ]∥2
+ ∥E[θ] −θ∗∥2,
where the first term and second term are called the variance term and the bias term,
respectively (Fig. 13.1).
An estimator θ is said to be unbiased if
E[θ] = θ∗,

13.3 ASYMPTOTIC EFFICIENCY
141
and an estimator θn is said to be asymptotically unbiased if
E[θn]
p
−→θ∗.
MLE was shown to be asymptotically unbiased under mild assumptions.
13.3 ASYMPTOTIC EFFICIENCY
Consistency and asymptotic unbiasedness are properties for infinitely many training
samples. However, in practice, the number of training samples cannot be infinity,
and therefore an estimator with consistency and asymptotic unbiasedness is not
necessarily superior. Indeed, as explained in Section 13.2, not only the bias but also
the variance has to be taken into account to reduce the expected squared error.
13.3.1 ONE-DIMENSIONAL CASE
Efficiency concerns the variance of an estimator. To explain the concept of efficiency
more precisely, let us first consider a one-dimensional case where the parameter to be
estimated, θ, is a scalar. Let V denote the variance operator over all training samples
{(xi, yi)}n
i=1 following i.i.d. with p(x):
V(•) = E[(• −E[•])2],
(13.2)
where E is defined in Eq. (13.1). Then the variance of any unbiased estimator θ is
lower-bounded as
V(θ) = E[(θ −θ∗)2] ≥
1
nF(θ∗),
which is called the Cramér-Rao inequality [31, 82]. Here, F(θ) is called Fisher
information:
F(θ) =
∂
∂θ log q(x; θ)
2
q(x; θ)dx.
The partial derivative of log q(x; θ),
∂
∂θ log q(x; θ),
(13.3)
is often called the Fisher score. An unbiased estimator θ is said to be efficient if the
Cramér-Rao lower bound is attained with strict equality:
V(θ) =
1
nF(θ∗).
13.3.2 MULTIDIMENSIONAL CASES
To extend the above definition to multidimensional cases, let us define the Fisher
information matrix F(θ) as

142
CHAPTER 13
PROPERTIES OF MAXIMUM LIKELIHOOD ESTIMATION
F(θ) =
∂
∂θ log q(x; θ)
∂
∂θ⊤log q(x; θ)

q(x; θ)dx.
(13.4)
Here,
∂
∂θ and
∂
∂θ⊤denote the vertical and horizontal vectors of partial derivatives,
respectively. That is, the (j, j′)th element of F(θ) for θ = (θ(1),. . . ,θ(b))⊤is given by
Fj, j′(θ) =

∂
∂θ(j) log q(x; θ)

∂
∂θ(j′) log q(x; θ)

q(x; θ)dx.
Then a multidimensional version of the Cramér-Rao inequality is given for any
unbiased estimator θ as
V(θ) = E[(θ −θ∗)(θ −θ∗)⊤] ≥1
n F(θ∗)−1.
Here, the inequality A ≥B for square matrices A and B means that A −B is
a positive semidefinite matrix: a matrix C is said to be positive semidefinite if it
satisfies ϕ⊤Cϕ ≥0 for any vector ϕ. An unbiased estimator θ is said to be efficient if
the above multidimensional version of the Cramér-Rao lower bound is attained with
strict equality:
V(θ) = 1
n F(θ∗)−1.
The concept of efficiency is defined for unbiased estimators. Therefore, since
MLE is asymptotically unbiased, but not generally unbiased with finite samples,
MLE is not efficient. An estimator is said to be asymptotic efficient if the Cramér-Rao
lower bound is attained asymptotically:
nV(θ)
p
−→F(θ∗)−1.
MLE was shown to be asymptotically efficient under mild assumptions.
Suppose that q(x; θ) is twice differentiable. Then

∂2
∂θ∂θ⊤log q(x; θ)

q(x; θ)dx
=

∂2
∂θ∂θ⊤q(x; θ)
q(x; θ)
q(x; θ)dx −

∂
∂θ q(x; θ)
∂
∂θ⊤q(x; θ)
q(x; θ)2
q(x; θ)dx
=

∂2
∂θ∂θ⊤q(x; θ)dx −
∂
∂θ log q(x; θ)
∂
∂θ⊤log q(x; θ)

q(x; θ)dx
=
∂2
∂θ∂θ⊤

q(x; θ)dx −F(θ) =
∂2
∂θ∂θ⊤1 −F(θ) = −F(θ).
Therefore, the Fisher information matrix defined in Eq. (13.4) can be expressed as
F(θ) = −

∂2
∂θ∂θ⊤log q(x; θ)

q(x; θ)dx.
(13.5)

13.4 ASYMPTOTIC NORMALITY
143
The matrix
∂2
∂θ∂θ⊤log q(x; θ)
is called the Hessian matrix of log q(x; θ), which plays an important role in
optimization theory. Eq. (13.5) shows that the Fisher information matrix agrees with
the expected negative Hessian matrix of log q(x; θ).
13.4 ASYMPTOTIC NORMALITY
If an estimator approximately follows the normal distribution when the number of
training samples n is large, it is said to possess asymptotic normality. In this section,
asymptotic normality of the maximum likelihood estimator is explained.
As explained in Section 7.4, the central limit theorem asserts that, for n one-
dimensional i.i.d. samples {xi}n
i=1 having expectation 0 and variance 1, their mean
xn = 1
n
n
i=1 xi satisfies
lim
n→∞Pr(a ≤√nxn ≤b) =
1
√
2π
b
a
exp

−x2
2

dx.
This means that √nxn asymptotically follows the standard normal distribution.
The central limit theorem can be extended to multidimensions as follows. Let
{xi}n
i=1 be the i.i.d. samples having expectation 0d and variance-covariance matrix
Σ, where d denotes the dimensionality of xi. Let xn be the sample average:
xn = 1
n
n

i=1
xi.
Then √nxn approximately follows the d-dimensional normal distribution with
expectation 0d and variance-covariance matrix Σ (Section 6.2):
√nxn
d
−→N(0d,Σ),
where “d” denotes the convergence in distribution (see Section 7.4).
Based on the above central limit theorem, asymptotic normality of the maximum
likelihood estimator is explained. Let ξ(θ) be the sample average of the Fisher score
(see Eq. (13.3)):
ξ(θ) = 1
n
n

i=1
∂
∂θ log q(xi; θ).
Since the maximum likelihood estimator θML maximizes the log-likelihood, it
satisfies ξ(θML) = 0b. Then the Taylor series expansion (Fig. 2.8) of the left-hand
side about the true parameter θ∗yields
ξ(θ∗) −F(θ∗)(θML −θ∗) + r = 0b,

144
CHAPTER 13
PROPERTIES OF MAXIMUM LIKELIHOOD ESTIMATION
where r is the residual vector that contains higher-order terms and
F(θ) = −1
n
n

i=1
∂2
∂θ∂θ⊤log q(xi; θ).
Since F(θ) is a sample approximation of Fisher information matrix (13.5), it
converges in probability to the true Fisher information matrix in the limit n →∞:
F(θ)
p
−→F(θ).
Also, consistency of MLE implies that the residual r can be ignored in the limit
n →∞. Thus,
√n(θML −θ∗)
p
−→F(θ∗)−1 √nξ(θ∗).
(13.6)
The true parameter θ∗maximizes the expected log-likelihood E [log q(x; θ)] with
respect to θ, where E is the expectation operator over p(x):
E[•] =

• p(x)dx.
Then the first-order optimality (Fig. 12.1) yields
E
∂
∂θ log q(x; θ)
θ=θ∗

= 0b.
Furthermore, Eq. (13.4) implies
V
∂
∂θ log q(x; θ)
θ=θ∗

= F(θ∗),
where V denotes the variance operator over p(x):
V[•] =

(• −E[•])(• −E[•])⊤p(x)dx.
Therefore, the central limit theorem asserts that √nξ(θ∗) asymptotically follows the
normal distribution with expectation 0b and variance-covariance matrix F(θ∗):
√nξ(θ∗)
d
−→N(0b,F(θ∗)).
Moreover, the variance-covariance matrix of F(θ∗)−1 √nξ(θ∗) is given by
VF(θ∗)−1 √nξ(θ∗) = F(θ∗)−1V  √nξ(θ∗) F(θ∗)−1
= F(θ∗)−1F(θ∗)F(θ∗)−1 = F(θ∗)−1,

13.5 SUMMARY
145
FIGURE 13.2
MATLAB code for illustrating asymptotic normality of
MLE.
FIGURE 13.3
Example of asymptotic normality of
MLE.
where V denotes the variance with respect to all training samples {xi}n
i=1 drawn
i.i.d. from p(x) (see Eq. (13.2)). Therefore, Eq. (13.6) yields
√n(θML −θ∗)
d
−→N(0b,F(θ∗)−1).
This means that the maximum likelihood estimator θML asymptotically follows the
normal distribution with expectation θ∗and variance-covariance matrix 1
n F(θ∗)−1.
The above asymptotic normality of MLE implies that MLE is asymptotically
unbiased. Furthermore, the variance-covariance matrix 1
n F(θ∗)−1 vanishes asymptot-
ically, meaning that the bias and variance terms explained in Section 13.2 also vanish
asymptotically. Therefore, the expected squared error, which is the sum of the bias
and variance terms, also vanishes asymptotically. Moreover, the above results show
that the Cramér-Rao lower bound is attained asymptotically, meaning that MLE is
asymptotically efficient.
A MATLAB code for illustrating the asymptotic normality of
µML = 1
n
n

i=1
xi,
when p(x) is the uniform distribution on [−0.5,0.5], is given in Fig. 13.2, and its
behavior is illustrated in Fig. 13.3.
13.5 SUMMARY
MLE is consistent and asymptotically unbiased, and therefore its validity is theo-
retically guaranteed when the number of training samples is infinite. Furthermore,
since MLE is asymptotically efficient, its high reliability is guaranteed even when the
number of training samples is not infinite, but large.

146
CHAPTER 13
PROPERTIES OF MAXIMUM LIKELIHOOD ESTIMATION
However, when the number of training samples is not large, MLE is not
necessarily a good method. Also, efficiency just guarantees that the variance is the
smallest among unbiased estimators; the expected squared error, which is the sum
of the bias and variance terms, is not guaranteed to be small. Indeed, a slightly
biased estimator can have significantly smaller variance than the efficient estimator.
In such a case, a biased estimator can have much smaller expected squared error than
the efficient estimator. Such a biased estimator will be discussed in Chapter 17 and
Chapter 23.

CHAPTER
MODEL SELECTION
FOR MAXIMUM
LIKELIHOOD
ESTIMATION
14
CHAPTER CONTENTS
Model Selection.................................................................. 147
KL Divergence ................................................................... 148
AIC .............................................................................. 150
Cross Validation .................................................................. 154
Discussion ....................................................................... 154
So far, a parametric model was assumed to be given and fixed. However, in practice, it
is often necessary to choose a parametric model from some candidates. In this chap-
ter, a data-driven method to choose an appropriate parametric model is explained.
14.1 MODEL SELECTION
As shown in Chapter 12, “Gaussian models” have several different variations.
(A) Variance-covariance matrix Σ is generic:
q(x; µ,Σ) =
1
(2π)
d
2 det(Σ)
1
2
exp

−1
2(x −µ)⊤Σ−1(x −µ)

.
(B) Variance-covariance matrix Σ is diagonal:
q(x; µ,(σ(1))2,. . . ,(σ(d))2) =
d

j=1
1

2π(σ(j))2 exp

−(x(j) −µ(j))2
2(σ(j))2

.
(C) Variance-covariance matrix Σ is proportional to the identity matrix:
q(x; µ,σ2) =
1
(2πσ2)
d
2
exp

−(x −µ)⊤(x −µ)
2σ2

.
To obtain a good approximation to the true probability density function by
a parametric method, a parametric model that (approximately)
contains the true
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00025-X
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
147

148
CHAPTER 14
MODEL SELECTION FOR MLE
FIGURE 14.1
Model selection. Too simple model may not be expressive enough to represent the true probability
distribution, while too complex model may cause unreliable parameter estimation.
probability density function may be required. From this viewpoint, a richer model
that contains various probability density functions is preferable. On the other hand,
as explained in Chapter 12, the good performance of MLE is guaranteed only when
the number of training samples is large (relative to the number of parameters). This
implies that a simple model that has a small number of parameters is desirable.
Therefore, in practice, a model that fulfills these two conflicting requirements in a
balanced way should be selected (Fig. 14.1).
However, since the appropriate balance depends on the unknown true probability
distribution, let us consider the following data-driven procedure for model selection:
1. Prepare parametric models {qm(x; θ)}m.
2. For each parametric model qm(x; θ), compute maximum likelihood estimator θm
and obtain a density estimator pm(x) as
pm(x) = qm(x; θm).
3. From the obtained density estimators {pm(x)}m, choose the one that is closest to
the true probability density function p(x).
The problem of finding the most promising model from a set of model candidates
is called model selection. At a glance, finding the estimator that is closest to the true
probability density function p(x) in Step 3 is not possible because p(x) is unknown.
Below, two model selection approaches called the Akaike information criterion (AIC)
and cross validation are introduced for coping with this problem.
14.2 KL DIVERGENCE
To perform model selection following the above procedure, a “closeness” measure
between the true probability density function p(x) and its estimator p(x) is needed.
Mathematically, for a set X, function g : X × X →R that satisfies the following four
conditions for any x, y, z ∈X is called a distance function:
Non-negativity: g(x, y) ≥0,
Symmetry: g(x, y) = g(y, x),
Identity: g(x, y) = 0 ⇐⇒x = y,

14.2 KL DIVERGENCE
149
Triangle inequality: g(x, y) + g(y, z) ≥g(x, z).
In this section, a typical closeness measure called the Kullback-Leibler divergence
(KL divergence) [64] is introduced.
The KL divergence from p to p is defined as
KL(p∥p) = E

log p(x)
p(x)

,
(14.1)
where E is the expectation operator over p(x):
E[•] =

• p(x)dx.
(14.2)
The KL divergence does not actually satisfy symmetric and triangle inequality,
meaning that it is not mathematically a distance. Nevertheless, the KL divergence
still satisfies non-negativity and identity. Therefore, even though it is not a proper
distance, a smaller KL divergence would imply that p is a better approximator to p.
Since the KL divergence KL(p∥p) contains unknown true probability density
function p(x), it cannot be directly computed. Let us expand Eq. (14.1) as
KL(p∥p) = E [log p(x)] −E [log p(x)] ,
where E [log p(x)] is the negative entropy of p that is a constant independent of p.
In the context of model selection, the KL divergence is used for comparing different
models, and therefore the constant term is irrelevant. For this reason, the first term is
ignored and only the second term E [log p(x)], which is the expected log-likelihood,
is considered below.
Even if the first term is ignored, the second term still contains the expectation
over unknown p and thus it cannot be directly computed. The most naive approach
to estimating the expectation would be the sample average, i.e., the expected log-
likelihood is approximated by the average log-likelihood over i.i.d. samples {xi}n
i=1
with p(x):
E[log p(x)] ≈1
n
n

i=1
log p(xi).
Then, a larger log-likelihood approximately implies a smaller KL divergence, and
therefore a model with a large log-likelihood would be close to the true probability
density function p.
However, the naive use of the log-likelihood for model selection does not work
well in practice. To illustrate this, let us consider model selection from the three
Gaussian models introduced in the beginning of Section 14.1:
(A) Variance-covariance matrix Σ is generic.
(B) Variance-covariance matrix Σ is diagonal.
(C) Variance-covariance matrix Σ is proportional to the identity matrix.

150
CHAPTER 14
MODEL SELECTION FOR MLE
FIGURE 14.2
For nested models, log-likelihood is mono-
tone nondecreasing as the model complexity
increases.
These three models are nested, meaning that model (C) is a special case of model
(B) and model (B) is a special case of model (A). This implies that the maximum
likelihood solution in model (C) is also included in model (B), and therefore
model (B) can achieve at least the same log-likelihood as model (C). Similarly, the
maximum likelihood solutions in model (B) and model (C) are also included in model
(A), and therefore model (A) can achieve at least the same log-likelihood as model
(B) and model (C). For this reason, when model selection is performed for nested
models based on the log-likelihood, the largest model (e.g., model (A) in the above
Gaussian model examples) is always chosen (Fig. 14.2).
14.3 AIC
As explained above, the naive use of log-likelihood for model selection results in just
always selecting the most complex model. This is caused by the fact that the average
log-likelihood is not an accurate enough estimator of the expected log-likelihood. For
appropriate model selection, therefore, a more accurate estimator of the expected log-
likelihood is needed. The Akaike information criterionAIC gives a better estimator of
the expected log-likelihood [1].
AIC is defined as
AIC = −
n

i=1
log q(xi; θML) + b,
(14.3)
where the first term is the negative log-likelihood and the second term is the number
of parameters. Thus, AIC can be regarded as correcting the negative log-likelihood
adding by the number of parameters.
Let us intuitively explain why AIC can yield better model selection. As illustrated
in Fig. 14.2, the negative log-likelihood is monotone nondecreasing as the number
of parameters increases. On the other hand, the number of parameters is monotone

14.3 AIC
151
FIGURE 14.3
AIC is the sum of the negative log-likelihood
and the number of parameters.
increasing as the number of parameters increases. Therefore, AIC is balancing
these conflicting functions by summing them up. As a result, an appropriate model
that achieves a reasonably large log-likelihood with a reasonably small number of
parameters tends to have a small AIC value (Fig. 14.3).
In various areas of science and engineering, Occam’s razor, also known as the
principle of parsimony, is often used as a guiding principle. Occam’s razor suggests
choosing a simpler hypothesis among similar hypotheses, and AIC can be regarded as
justifying the use of Occam’s razor in statistical inference. More specifically, among
the models that achieve similar log-likelihood values, AIC suggests choosing the one
that has the smallest number of parameters.
Next, theoretical validity of AIC is investigated. Let J(θ) be the negative expected
log-likelihood of q(x; θ),
J(θ) = −E[log q(x; θ)],
where E is the expectation operator over p(x) (see Eq.(14.2)). Let θ∗be the minimizer
of J(θ):
θ∗= argmin
θ
J(θ).
According to asymptotic theory, the expectation of J(θML) can be expanded using θ∗
as follows [63, 107]:
E[J] = −E

1
n
n

i=1
log q(xi; θML)

+ 1
ntr

Q(θ∗)G(θ∗)−1
+ o(n−1),
(14.4)
where E denotes the expectation over all training samples {xi}n
i=1 following
i.i.d. with p(x):
E[•] =

· · ·

• p(x1) · · · p(xn)dx1 · · · dxn.

152
CHAPTER 14
MODEL SELECTION FOR MLE
f (n) = O(g(n)) means that
lim
n→∞
f (n)
g(n) < ∞,
while f (n) = o(g(n)) means that
lim
n→∞
f (n)
g(n) = 0.
Intuitively, O(n−1) denotes a term that has the same size as n−1, while o(n−1)
denotes a term that is smaller than n−1.
FIGURE 14.4
Big-o and small-o notations.
o(n−1) denotes a term that is smaller than n−1 asymptotically (see Fig. 14.4). Matrices
Q(θ) and G(θ) are defined as
Q(θ) = E
∂
∂θ log q(x; θ) ∂
∂θ⊤log q(x; θ)
θ

,
G(θ) = −E

∂2
∂θ∂θ⊤log q(x; θ)
θ

.
Q(θ) looks similar to the Fisher information matrix F(θ) (see Eq. (13.4)):
F(θ) =
∂
∂θ log q(x; θ)
∂
∂θ⊤log q(x; θ)

q(x; θ)dx.
However, Q(θ) includes the expectation over true p(x), while F(θ) includes the
expectation over model q(x; θ). G(θ) is the negative Hessian matrix of log q(x; θ)
expected over true p(x).
G(θ) can be expanded as
G(θ) = −E

∂2
∂θ∂θ⊤log q(x; θ)

= −E

∂2
∂θ∂θ⊤q(x; θ)
q(x; θ)

+ E

∂
∂θ q(x; θ)
q(x; θ)
∂
∂θ⊤q(x; θ)
q(x; θ)

= −E

∂2
∂θ∂θ⊤q(x; θ)
q(x; θ)

+ E
∂
∂θ log q(x; θ) ∂
∂θ⊤log q(x; θ)

= −

∂2
∂θ∂θ⊤q(x; θ)
q(x; θ)
p(x)dx + Q(θ).

14.3 AIC
153
Now, suppose that the parametric model q(x; θ) contains the true probability density
function p(x), i.e., there exists θ∗such that q(x; θ∗) = p(x). Then G(θ∗) can be
expressed as
G(θ∗) = −

∂2
∂θ∂θ⊤q(x; θ)dx + Q(θ∗) = −
∂2
∂θ∂θ⊤

q(x; θ)dx + Q(θ∗)
= −
∂2
∂θ∂θ⊤1 + Q(θ∗) = Q(θ∗).
Thus, the second term in Eq. (14.4) can be expressed as
tr

Q(θ∗)G(θ∗)−1
= tr (Ib) = b,
which agrees with the second term in AIC, i.e., the number of parameters.
As explained above, AIC assumes that the parametric model q(x; θ) contains the
true probability density function p(x). The Takeuchi information criterion (TIC) is a
generalization of AIC that removes this assumption [63, 107]. TIC is defined as
TIC = −
n

i=1
log q(xi; θML) + tr
Q(θML) G(θML)−1
,
where Q(θML) and G(θML) are the sample approximations to Q(θML) and G(θML),
respectively:
Q(θML) = 1
n
n

i=1
∂
∂θ log q(xi; θ) ∂
∂θ⊤log q(xi; θ)
θ=θML
,
G(θML) = −1
n
n

i=1
∂2
∂θ∂θ⊤log q(xi; θ)
θ=θML
.
The law of large numbers and consistency of θML yield that Q(θML) and G(θML)
converge in probability to Q(θ∗) and G(θ∗), respectively:
Q(θML)
p
−→Q(θ∗) and
G(θML)
p
−→G(θ∗).
It is known that the expected TIC divided by n converges to J and its error has
smaller order than n−1:
1
nE[TIC] = E[J(θML)] + o(n−1).
On the other hand, if the negative average log-likelihood is naively used as an
estimator of J(θML),
E

−1
n
n

i=1
log q(xi; θML)

= E[J(θML)] + O(n−1)
holds, where O(n−1) denotes a term that has the same size as n−1 (see Fig. 14.4). Thus,
TIC is a more accurate estimator of J(θML) than the negative average log-likelihood.
Since TIC does not require the assumption that the parametric model q(x; θ) contains

154
CHAPTER 14
MODEL SELECTION FOR MLE
FIGURE 14.5
Cross validation.
the true probability density function p(x), it is more general than AIC. However, since
AIC is much simpler to implement, it seems to be more frequently used in practice
than TIC.
14.4 CROSS VALIDATION
Although AIC is easy to implement, its validity is only guaranteed when the number
of training samples is large. Here, a more flexible model selection method called
cross validation is introduced.
The basic idea of cross validation is to split training samples D = {xi}n
i=1 into
the estimation samples and the validation samples. The estimation samples are used
for estimating the probability density function, and the validation samples are used
for evaluating the validity of the estimated probability density function. If the log-
likelihood is used as the evaluation measure, the model that gives the largest log-
likelihood for the validation samples is chosen as the most promising one.
However, simply splitting the training samples into the estimation subset and the
validation subset for model selection can cause strong dependency on data splitting.
To mitigate this problem, cross validation splits the training samples into t disjoint
subsets of (approximately) the same size (see Fig. 14.5). Then (t −1) subsets are used
for estimation and the remaining subset is used for validation. The choice of (t −1)
subsets has t possibilities and thus all t possibilities are investigated and the average
log-likelihood is regarded as the final score for model selection.
The algorithm of cross validation is summarized in Fig. 14.6.
14.5 DISCUSSION
Let us interpret the statistical meaning of cross validation using the KL divergence
from true probability density p to its estimator p(ℓ)
j :

14.5 DISCUSSION
155
1. Prepare candidates of models: {Mj}j.
2. Split training samples D
=
{xi}n
i=1 into t disjoint subsets of
(approximately) the same size: {Dℓ}t
ℓ=1.
3. For each model candidate Mj
(a) For each split ℓ= 1,. . . ,t
i.
Obtain density estimator p(ℓ)
j (x) using model Mj from all
training samples without Dℓ.
ii. Compute the average log-likelihood J(ℓ)
j
of p(ℓ)
j (x) for holdout
samples Dℓ:
J(ℓ)
j
=
1
|Dℓ|

x′∈Dℓ
log p(ℓ)
j (x′),
where |Dℓ| denotes the number of elements in set Dℓ.
(b) Compute the average log-likelihood Jj over all t splits:
Jj = 1
t
t
ℓ=1
J(ℓ)
j .
4. Choose the model Mj that maximizes the average log-likelihood:
j = argmax
j
Jj.
5. Obtain the final density estimator using chosen model Mj, from all
training samples {xi}n
i=1.
FIGURE 14.6
Algorithm of likelihood cross validation.
KL(p∥p) = E

log p(x)
p(ℓ)
j (x)

= E [log p(x)] −E

log p(ℓ)
j (x)

.
Since the first term is constant, let us ignore it and define the second term as J:
J = −E[log p(ℓ)
j (x)].
The cross validation score J(ℓ)
j
(see Fig. 14.6) is known to be an almost unbiased es-
timator of J [118]. Thus, model selection by likelihood cross validation corresponds
to finding the model that minimizes the KL divergence, which is the same objective
as the AIC explained in Section 14.3.

156
CHAPTER 14
MODEL SELECTION FOR MLE
Indeed, the likelihood cross validation and the AIC are known to perform
similarly when the number of training samples is large [97]. However, for a small
number of training samples, likelihood cross validation seems to be more reliable
in practice. Furthermore, while the AIC is applicable only to model selection
of maximum likelihood estimation under the KL divergence, cross validation is
applicable to any estimation method and any error metric. For example, as shown in
Fig. 16.17, cross validation can be applied to choosing tuning parameters in pattern
recognition under the misclassification rate criterion. Thus, cross validation is a
useful alternative to the AIC. A practical drawback of cross validation is that it is
computationally expensive due to repeated estimation and validation. However, note
that the cross validation procedure can be easily parallelized for multiple servers or
cores.

CHAPTER
MAXIMUM LIKELIHOOD
ESTIMATION FOR
GAUSSIAN MIXTURE
MODEL
15
CHAPTER CONTENTS
Gaussian Mixture Model.......................................................... 157
MLE ............................................................................. 158
Gradient Ascent Algorithm ....................................................... 161
EM Algorithm .................................................................... 162
Fisher’s linear discriminant analysis introduced in Chapter 12 is a simple and prac-
tical classification method. However, approximating class-conditional probability
densities by the Gaussian models can be too restrictive in practice. In this chapter,
a more expressive model called the Gaussian mixture model is introduced and its
MLE is discussed.
15.1 GAUSSIAN MIXTURE MODEL
If patterns in a class are distributed in several clusters, approximating the class-
conditional distribution by a single Gaussian model may not be appropriate. For
example, Fig. 15.1(a) illustrates the situation where a unimodal distribution is
approximated by a single Gaussian model, which results in accurate estimation. On
the other hand, Fig. 15.1(b) illustrates the situation where a multimodal distribution is
approximated by a single Gaussian model, which performs poorly even with a large
number of training samples.
A Gaussian mixture model, defined by
q(x; θ) =
m

ℓ=1
wℓN(x; µℓ,Σℓ),
is suitable to approximate such multimodal distributions. Here, N(x; µ,Σ) denotes a
Gaussian model with expectation µ and variance-covariance matrix Σ:
N(x; µ,Σ) =
1
(2π)d/2det(Σ)1/2 exp

−1
2(x −µ)⊤Σ−1(x −µ)

.
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00026-1
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
157

158
CHAPTER 15
MLE FOR GAUSSIAN MIXTURE MODEL
(a) Unimodal distribution
(b) Multimodal distribution
FIGURE 15.1
MLE for Gaussian model.
Thus, a Gaussian mixture model is a linear combination of m Gaussian models
weighted according to {wℓ}m
ℓ=1. The parameter θ of the Gaussian mixture model
is given by
θ = (w1,. . . ,wm, µ1,. . . , µm,Σ1,. . . ,Σm).
The Gaussian mixture model q(x; θ) should satisfy the following condition to be
a probability density function:
∀x ∈X, q(x; θ) ≥0 and

X
q(x; θ)dx = 1.
To this end, {wℓ}m
ℓ=1 are imposed to be
w1,. . . ,wm ≥0 and
m

ℓ=1
wℓ= 1.
(15.1)
Fig. 15.2 illustrates an example of the Gaussian mixture model, which represents a
multimodal distribution by linearly combining multiple Gaussian models.
15.2 MLE
The parameter θ in the Gaussian mixture model is learned by MLE explained in
Chapter 12. The likelihood is given by
L(θ) =
n

i=1
q(xi; θ),
(15.2)

15.2 MLE
159
(a) Each Gaussian component
(b) Gaussian mixture model
FIGURE 15.2
Example of Gaussian mixture model: q(x) = 0.4N(x; −2,1.52) + 0.2N(x; 2,22) + 0.4N(x; 3,12).
and MLE finds its maximizer with respect to θ. When the above likelihood is
maximized for the Gaussian mixture model, the constraints given by Eq. (15.1) need
to be satisfied:
θ = argmax
θ
L(θ)
subject to w1,. . . ,wm ≥0 and
m

ℓ=1
wℓ= 1.
Due to the constraints, the maximizer θ cannot be simply obtained by setting the
derivative of the likelihood to zero. Here, w1,. . . ,wm are re-parameterized as
wℓ=
exp(γℓ)
m
ℓ′=1 exp(γℓ′),
(15.3)
and {γℓ}m
ℓ=1 are learned, which automatically fulfills Eq. (15.1).
The maximum likelihood solution θ satisfies the following likelihood equation
for log L(θ):

∂
∂γℓ
log L(θ)
θ=θ
= 0,
∂
∂µℓ
log L(θ)
θ=θ
= 0d,
∂
∂Σℓ
log L(θ)
θ=θ
= Od×d,
(15.4)

160
CHAPTER 15
MLE FOR GAUSSIAN MIXTURE MODEL
where 0d denotes the d-dimensional zero vector and Od×d denotes the d × d zero
matrix. Substituting Eq. (15.3) into Eq. (15.2), the log-likelihood is expressed as
log L(θ) =
n

i=1
log
m

ℓ=1
exp(γℓ)N(xi; µℓ,Σℓ) −n log
m

ℓ=1
exp(γℓ).
Taking the partial derivative of the above log-likelihood with respect to γℓgives
∂
∂γℓ
log L(θ) =
n

i=1
exp(γℓ)N(xi; µℓ,Σℓ)
m
ℓ′=1 exp(γℓ′)N(xi; µℓ′,Σℓ′) −
nγℓ
m
ℓ′=1 exp(γℓ′)
=
n

i=1
ηi,ℓ−nwℓ,
where ηi,ℓis defined as
ηi,ℓ=
wℓN(xi; µℓ,Σℓ)
m
ℓ′=1 wℓ′N(xi; µℓ′,Σℓ′).
Similarly, taking the partial derivatives of the above log-likelihood with respect to µℓ
and Σℓ(see Fig. 12.3 for the derivative formulas) gives
∂
∂µℓ
log L(θ) =
n

i=1
ηi,ℓΣ−1
ℓ(xi −µℓ),
∂
∂Σℓ
log L(θ) = 1
2
n

i=1
ηi,ℓ

Σ−1
ℓ(xi −µℓ)(xi −µℓ)⊤Σ−1
ℓ−Σ−1
ℓ

.
Setting the above derivatives to zero shows that the maximum likelihood solution wℓ,
µℓ, and Σℓshould satisfy

wℓ= 1
n
n

i=1
ηi,ℓ,
µℓ=
n
i=1 ηi,ℓxi
n
i′=1 ηi′,ℓ
,
Σℓ=
n
i=1 ηi,ℓ(xi −µℓ)(xi −µℓ)⊤
n
i′=1 ηi′,ℓ
,
(15.5)
where ηi,ℓis called the responsibility of the ℓth component for sample xi:
ηi,ℓ=
wℓN(xi; µℓ,Σℓ)
m
ℓ′=1 wℓ′N(xi; µℓ′,Σℓ′)
.
(15.6)
In the above likelihood equation, variables are entangled in a complicated way and
there is no known method to solve it analytically. Below, two iterative algorithms are
introduced to numerically find a solution: the gradient method ad the expectation-
maximization (EM) algorithm.

15.3 GRADIENT ASCENT ALGORITHM
161
FIGURE 15.3
Schematic of gradient ascent.
1. Initialize the solution θ.
2. Compute the gradient of log-likelihood log L(θ) at the current
solution θ:
∂
∂θ log L(θ)
θ=θ
.
3. Update the parameter to go up the gradient:
θ ←−θ + ε ∂
∂θ log L(θ)
θ=θ
,
where ε is a small positive scalar.
4. Iterate 2–3 until convergence.
FIGURE 15.4
Algorithm of gradient ascent.
15.3 GRADIENT ASCENT ALGORITHM
A gradient method is a generic and simple optimization approach that iteratively
updates the parameter to go up (down in the case of minimization) the gradient of
an objective function (Fig. 15.3). The algorithm of gradient ascent is summarized in
Fig. 15.4. Under a mild assumption, a gradient ascent solution is guaranteed to be
local optimal, which corresponds to a peak of a local mountain and the objective
value cannot be increased by any local parameter update.
A stochastic variant of the gradient method is to randomly choose a sample and
update the parameter to go up the gradient for the selected sample. Such a stochastic
method, called the stochastic gradient algorithm, was also shown to produce to a
local optimal solution [4].

162
CHAPTER 15
MLE FOR GAUSSIAN MIXTURE MODEL
(a) Too large ε
(b) Too small ε
FIGURE 15.5
Step size ε in gradient ascent. The gradient flow can overshoot
the peak if ε is large, while gradient ascent is slow if ε is too
small.
Note that the (stochastic) gradient method does not only necessarily give the
global optimal solution but also a local optimal solution, as illustrated in Fig. 15.3.
Furthermore, its performance relies on the choice of the step size ε, which is not easy
to determine in practice. If ε is large, gradient ascent is fast in the beginning, but the
gradient flow can overshoot the peak (Fig. 15.5(a)). On the other hand, if ε is small,
the peak may be found, but gradient ascent is slow in the beginning (Fig. 15.5(b)).
To overcome this problem, starting from a large ε and then reducing ε gradually,
called simulated annealing, would be useful. However, the choice of initial ε and the
decreasing factor of ε is not straightforward in practice.
To mitigate the problem that only a local optimal solution can be found, it is
practically useful to run the gradient algorithm multiple times from different initial
solutions and choose the one that gives the best solution.
15.4 EM ALGORITHM
The difficulty of tuning the step size ε in the gradient method can be overcome by
the EM algorithm [36]. The EM algorithm was originally developed for obtaining
a maximum likelihood solution when input x is only partially observable. MLE for
Gaussian mixture models can actually be regarded as learning from incomplete data,
and the EM algorithm gives an efficient means to obtain a local optimal solution.

15.4 EM ALGORITHM
163
1. Initialize parameters {wℓ, µℓ,Σℓ}m
ℓ=1.
2. E-step: Compute responsibilities {ηi,ℓ}n
i=1,
m
ℓ=1 from current pa-
rameters {wℓ, µℓ,Σℓ}m
ℓ=1:
ηi,ℓ←−
wℓN(xi; µℓ,Σℓ)
m
ℓ′=1 wℓ′N(xi; µℓ′,Σℓ′)
.
3. M-step: Update parameters {wℓ, µℓ,Σℓ}m
ℓ=1 from current respon-
sibilities {ηi,ℓ}n
i=1,
m
ℓ=1:
wℓ←−1
n
n

i=1
ηi,ℓ,
µℓ←−
n
i=1 ηi,ℓxi
n
i′=1 ηi′,ℓ
,
Σℓ←−
n
i=1 ηi,ℓ(xi −µℓ)(xi −µℓ)⊤
n
i′=1 ηi′,ℓ
,
4. Iterate 2–3 until convergence.
FIGURE 15.6
EM algorithm.
As summarized in Fig. 15.6, the EM algorithm consists of the E-step and the M-
step, which correspond to updating the solution based on necessary condition (15.5)
and computing its auxiliary variable (15.6) alternately.
The E-step and the M-step can be interpreted as follows:
E-step: A lower bound b(θ) of log-likelihood log L(θ) that touches at current
solution θ is obtained:
∀θ, log L(θ) ≥b(θ),
and
log L(θ) = b(θ).
Note that this lower-bounding step corresponds to computing the expectation
over unobserved variables, which is why this step is called the E-step.
M-step: The maximizer θ′ of the lower bound b(θ) is obtained.
θ′ = argmax
θ
b(θ).
As illustrated in Fig. 15.7, iterating the E-step and the M-step increases the log-
likelihood (precisely, the log-likelihood is monotone nondecreasing).

164
CHAPTER 15
MLE FOR GAUSSIAN MIXTURE MODEL
FIGURE 15.7
Maximizing the lower bound b(θ) of the log-likelihood log L(θ).
FIGURE 15.8
Jensen’s inequality for m = 2. log is a concave function.
The lower bound in the E-step is derived based on Jensen’s inequality explained
in Section 8.3.1: for η1,. . . ,ηm ≥0 and m
ℓ=1 ηℓ= 1,
log *
,
m

ℓ=1
ηℓuℓ+
-
≥
m

ℓ=1
ηℓlog uℓ.
(15.7)
For m = 2, Jensen’s inequality is simplified as
log (η1u1 + η2u2) ≥η1 log u1 + η2 log u2,
(15.8)
which can be intuitively understood by the concavity of the log function (see
Fig. 15.8).
The log-likelihood log L(θ) can be expressed by using the responsibility ηi,ℓ(see
Eq. (15.6)) as
log L(θ) =
n

i=1
log *
,
m

ℓ=1
wℓN(xi; µℓ,Σℓ)+
-
=
n

i=1
log *
,
m

ℓ=1
ηi,ℓ
wℓN(xi; µℓ,Σℓ)
ηi,ℓ
+
-
.
(15.9)

15.4 EM ALGORITHM
165
By associating wℓN(xi; µℓ,Σℓ)/ηi,ℓin Eq. (15.9) with uℓin Jensen’s inequality
(15.7), lower bound b(θ) of the log-likelihood log L(θ) can be obtained as
log L(θ) ≥
n

i=1
m

ℓ=1
ηi,ℓlog
wℓN(xi; µℓ,Σℓ)
ηi,ℓ

= b(θ).
This lower bound b(θ) touches log L(θ) when θ = θ, because Eq. (15.6) implies
b(θ) =
n

i=1
*
,
m

ℓ=1
ηi,ℓ+
-
log *
,
wℓN(xi; µℓ,Σℓ)
ηi,ℓ
+
-
=
n

i=1
log *
,
m

ℓ′=1
wℓ′N(xi; µℓ′,Σℓ′)+
-
= log L(θ).
The maximizer θ′ of the lower bound b(θ) in the M-step should satisfy

∂
∂γℓ
b(θ)
θ=θ′ = 0,
∂
∂µℓ
b(θ)
θ=θ′ = 0d,
∂
∂Σℓ
b(θ)
θ=θ′ = Od×d,
from which the maximizer θ′ can be obtained as

w′
ℓ= 1
n
n

i=1
ηi,ℓ,
µ′
ℓ=
n
i=1 ηi,ℓxi
n
i′=1 ηi′,ℓ
,
Σ
′
ℓ=
n
i=1 ηi,ℓ(xi −µℓ)(xi −µℓ)⊤
n
i′=1 ηi′,ℓ
.
The above explanation showed that the log-likelihood is monotone nondecreasing
by iterating the E-step and the M-step. Furthermore, the EM algorithm was proved to
produce a local optimal solution [120].
A MATLAB code for the EM algorithm is given in Fig. 15.9, and its behavior
is illustrated in Fig. 15.10. Here, the mixture model of five Gaussian components is
fitted to the mixture of two Gaussian distributions. As shown in Fig. 15.10, two of
the five Gaussian components fit the true two Gaussian distributions well, and the
remaining three Gaussian components are almost eliminated. Indeed, the learned

166
CHAPTER 15
MLE FOR GAUSSIAN MIXTURE MODEL
x=[2*randn(1,100)-5 randn(1,50); randn(1,100) randn(1,50)+3];
[d,n]=size(x);
m=5;
e=rand(n,m);
S=zeros(d,d,m);
for o=1:10000
e=e./repmat(sum(e,2),[1 m]);
g=sum(e);
w=g/n;
mu=(x*e)./repmat(g,[d 1]);
for k=1:m
t=x-repmat(mu(:,k),[1 n]);
S(:,:,k)=(t.*repmat(e(:,k)’,[d 1]))*t’/g(k);
e(:,k)=w(k)*det(S(:,:,k))^(-1/2) ...
*exp(-sum(t.*(S(:,:,k)\t))/2);
end
if o>1 && norm(w-w0)+norm(mu-mu0)+norm(S(:)-S0(:))<0.001
break
end
w0=w;
mu0=mu;
S0=S;
end
figure(1); clf; hold on
plot(x(1,:),x(2,:),’ro’);
v=linspace(0,2*pi,100);
for k=1:m
[V,D]=eig(S(:,:,k));
X=3*w(k)*V’*[cos(v)*D(1,1); sin(v)*D(2,2)];
plot(mu(1,k)+X(1,:),mu(2,k)+X(2,:),’b-’)
end
FIGURE 15.9
MATLAB code of EM algorithm for Gaussian mixture model.
mixing coefficients are given as
(w1, w2, w3, w4, w5) = (0.09,0.32,0.05,0.06,0.49).
If the most responsible mixing component,
yi = argmax
ℓ
ηi,ℓ,

15.4 EM ALGORITHM
167
(a) After 1 iteration
(b) After 5 iteration
(c) After 10 iteration
(d) After 20 iteration
(e) After 50 iteration
(f) After 126 iteration
FIGURE 15.10
Example of EM algorithm for Gaussian mixture model. The size of ellipses is proportional to the
mixing weights {wℓ}m
ℓ=1.
is selected for each sample xi, density estimation with a mixture model can be
regarded as clustering. Indeed, the EM algorithm for a Gaussian mixture model is
reduced to the k-means clustering algorithm for Σℓ= σ2
ℓI. See Chapter 37 for details.


CHAPTER
NONPARAMETRIC
ESTIMATION
16
CHAPTER CONTENTS
Histogram Method ............................................................... 169
Problem Formulation ............................................................. 170
KDE ............................................................................. 174
Parzen Window Method .................................................... 174
Smoothing with Kernels .................................................... 175
Bandwidth Selection ....................................................... 176
NNDE............................................................................ 178
Nearest Neighbor Distance................................................. 178
Nearest Neighbor Classifier ................................................ 179
So far, estimation with parametric methods was discussed, which is useful when
an appropriate parametric model is available (Fig. 16.1(a)). However, if the true
probability density is highly complicated, it may be difficult to prepare an appropriate
parametric model and then parametric methods do not work well (Fig. 16.1(b)).
In this chapter, nonparametric estimation methods are introduced that do not use
parametric models.
16.1 HISTOGRAM METHOD
The simplest nonparametric method would be the histogram method.
In the
histogram method, the pattern space X is partitioned into several bins. Then, in each
bin, the probability density is approximated by a constant proportional to the number
of training samples that fall into the bin. An example of the histogram method is
illustrated in Fig. 16.2, and its MATLAB code is provided in Fig. 16.3. The graph
shows that the profile of a complicated probability density function can be captured
well by the histogram method.
However, the histogram method has several drawbacks:
•
Probability densities become discontinuous across different bins.
•
Appropriately determining the shape and size of bins is not straightforward (see
Fig. 16.4).
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00027-3
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
169

170
CHAPTER 16
NONPARAMETRIC ESTIMATION
(a) When the true probability density is simple
(b) When the true probability density is compli-
cated
FIGURE 16.1
Examples of Gaussian MLE.
FIGURE 16.2
Example of histogram method.
•
Naive splitting of the pattern space such as the equidistant grid exponentially
grows the number of bins with respect to the input dimensionality d.
In this chapter, nonparametric methods that can address these issues are introduced.
16.2 PROBLEM FORMULATION
Let us consider the problem of estimating p(x′), the value of probability density at
point x′. Let R be a region in the pattern space X that contains the point of interest

16.2 PROBLEM FORMULATION
171
n=10000; x=myrand(n); s=0.1; b=[0:s:5];
figure(1); clf; hold on
a=histc(x,b); bar(b,a/s/n,’histc’)
function x=myrand(n)
x=zeros(1,n); u=rand(1,n);
t=(0<=u & u<1/8); x(t)=sqrt(8*u(t));
t=(1/8<=u & u<1/4); x(t)=2-sqrt(2-8*u(t));
t=(1/4<=u & u<1/2); x(t)=1+4*u(t);
t=(1/2<=u & u<3/4); x(t)=3+sqrt(4*u(t)-2);
t=(3/4<=u & u<=1); x(t)=5-sqrt(4-4*u(t));
FIGURE 16.3
MATLAB code for inverse transform sampling (see Section 19.3.1) for probability density
function shown in Fig. 16.1(b). The bottom function should be saved as “myrand.m.”
(a) Too small bin width
(b) Appropriate bin width
(c) Too large bin width
FIGURE 16.4
Choice of bin width in histogram method.
x′, and let V be its volume:
V =

R
dx.
The probability P that a pattern x falls into region R is given by
P =

R
p(x)dx.
These notations are summarized in Fig. 16.5.

172
CHAPTER 16
NONPARAMETRIC ESTIMATION
FIGURE 16.5
Notation of nonparametric methods.
FIGURE 16.6
Probability P approximated by the
size of rectangle.
The probability P may be approximated by two ways. One is to use the point of
interest x′ as follows (Fig. 16.6):
P ≈V p(x′).
(16.1)
The other is to use k, the number of training samples that fall into the region R, as
P ≈k
n .
(16.2)
Then combining Eq. (16.1) and Eq. (16.2) allows us to eliminate P and obtain
p(x′) ≈k
nV .
(16.3)
This is the fundamental form of nonparametric density estimation that approximates
p(x′), the value of probability density at point x′, without using any parametric
model.
The accuracy of approximation (16.3) depends on the accuracy of Eq. (16.1) and
Eq. (16.2), which can be controlled by the choice of region R. Let us evaluate the
accuracy of Eq. (16.1) and Eq. (16.2) in terms of the choice of R. Eq. (16.1) is exact
if the probability density is constant in the region R. Thus, if the region R is smaller,
Eq. (16.1) may be more accurate.
Next, let us evaluate the accuracy of Eq. (16.2). The probability that k points out
of n training samples fall into the region R follows the binomial distribution (see
Section 3.2). Its probability mass function is given by
n
k

Pk(1 −P)n−k,
(16.4)
where  n
k
 is the binomial coefficient:
n
k

=
n!
(n −k)!k!.

16.2 PROBLEM FORMULATION
173
FIGURE 16.7
Normalized variance of binomial distribution.
The expectation and variance of binomial random variable k are given by
E[k] = nP and V[k] = nP(1 −P).
Then it can be easily confirmed that the expectation of k/n agrees with true P:
E
k
n

= P.
However, the fact that the expectation of k/n agrees with true P does not
necessarily mean k/n is a good estimator of P. Indeed, if its variance is large, k/n
may be a poor estimator of P. Here, let us normalize k by nP as
z = k
nP,
so that the expectation of z is always 1 for any P:
E[z] = E[k]
nP = 1.
The variance of z is given by
V[z] = V[k]
(nP)2 = 1 −P
nP .
If V[z] is small, k/n will be a good approximator to P. Fig. 16.7 plots V[z] as a
function of P, showing that larger P gives smaller V[z]. P can be increased if region
R is widen, and thus larger R makes Eq. (16.2) more accurate.
As explained above, Eq. (16.1) is more accurate if R is taken to be small, while
Eq.(16.2) is more accurate if R is taken to be large. Thus, region R should be chosen

174
CHAPTER 16
NONPARAMETRIC ESTIMATION
(a) Region R
(b) Parzen window function
FIGURE 16.8
Parzen window method.
appropriately to improve the accuracy of approximation (16.3). In the following
sections, two methods to determine region R based on training samples {xi}n
i=1 are
introduced. In Section 16.3, the volume V of region R is fixed, and the number of
training samples k that fall into R is determined from data. On the other hand, in
Section 16.4, k is fixed, and the volume V of region R is determined from data.
16.3 KDE
In this section, the volume V of region R is fixed, and the number of training samples
k that fall into R is determined from data.
16.3.1 PARZEN WINDOW METHOD
As region R, let us consider the hypercube with edge length h centered at x in region
R (Fig. 16.8(a)). Its volume V is given by
V = hd,
(16.5)
where d is the dimensionality of the pattern space. The number of training samples
falling into region R is expressed as
k =
n

i=1
W
x −xi
h

,
(16.6)
where W(x) is called the Parzen window function defined for
x = (x(1),. . . , x(d))⊤
as follows (Fig. 16.8(b)):
W(x) =

1
max
i=1,...,d |x(i)| ≤1
2,
0
otherwise.
h is called the bandwidth of the Parzen window function.

16.3 KDE
175
(a) Each Parzen window function
(b) Parzen window estimator
FIGURE 16.9
Example of Parzen window method.
Substituting Eq. (16.5) and Eq. (16.6) into Eq. (16.3) gives the following density
estimator:
pParzen(x) =
1
nhd
n

i=1
W
x −xi
h

.
This estimator called the Parzen window method and its numerical behavior are
illustrated in Fig. 16.9. The result resembles that of the histogram method, but
the bin widths are determined adaptively based on the training samples. However,
discontinuity of estimated densities across different bins still remains in the Parzen
window method.
16.3.2 SMOOTHING WITH KERNELS
The problem of discontinuity can be effectively overcome by KDE, which uses a
smooth kernel function K(x) instead of the Parzen window function:
pKDE(x) =
1
nhd
n

i=1
K
x −xi
h

.
Note that the kernel function should satisfy
∀x ∈X, K(x) ≥0,
and

X
K(x)dx = 1.
The Gaussian kernel is a popular choice as a kernel function:
K(x) =
1
(2π)
d
2
exp

−x⊤x
2

,

176
CHAPTER 16
NONPARAMETRIC ESTIMATION
(a) Each Gaussian kernel function
(b) Kernel density estimator
FIGURE 16.10
Example of Gaussian KDE. Training samples are the same as those in Fig. 16.9.
where the bandwidth h corresponds to the standard deviation of the Gaussian density
function. An example of Gaussian KDE is illustrated in Fig. 16.10, showing that a
nice smooth density estimator is obtained.
A generalized KDE,
pKDE(x) =
1
n det(H)
n

i=1
K

H−1(x −xi)

,
(16.7)
may also be considered, where H is the d × d positive definite matrix called
the bandwidth matrix. If K(x) is the Gaussian function, HH⊤corresponds to the
variance-covariance matrix of the Gaussian density function.
16.3.3 BANDWIDTH SELECTION
The estimator pKDE(x) obtained by KDE depends on the bandwidth h (Fig. 16.11).
Here, data-driven methods to choose h are introduced.
For generalized KDE (16.7), let us consider a diagonal bandwidth matrix H:
h = diag

h(1),. . . ,h(d)
,
where d denotes the dimensionality of input x. When the true probability distribution
is Gaussian, the optimal bandwidth is given asymptotically as follows [90, 93]:
h(j) =

4
(d + 2)n

1
d+4
σ(j),

16.3 KDE
177
(a) h = 0.07
(b) h = 0.3
(c) h = 0.7
FIGURE 16.11
Choice of kernel bandwidth h in KDE.
n=500; x=myrand(n); x2=x.^2; hs=[0.01 0.1 0.5]; t=5;
d2=repmat(x2,[n 1])+repmat(x2’,[1 n])-2*x’*x;
v=mod(randperm(n),t)+1;
for i=1:length(hs)
hh=2*hs(i)^2; P=exp(-d2/hh)/sqrt(pi*hh);
for j=1:t
s(j,i)=mean(log(mean(P(v~=j,v==j))));
end
end
[dum,a]=max(mean(s)); h=hs(a); hh=2*h^2;
ph=mean(exp(-d2/hh)/(sqrt(pi*hh)));
figure(1); clf; plot(x,ph,’r*’); h
FIGURE 16.12
MATLAB code for Gaussian KDE with bandwidth selected by likelihood cross validation. A
random number generator “myrand.m” shown in Fig. 16.3 is used.
where σ(j) denotes the standard deviation of the jth element of x. Since σ(j) may be
unknown in practice, it is estimated from samples as
σ(j) =



1
n −1
n

i=1
*
,
x(j)
i
−1
n
n

i=1
x(j)
i +
-
2
.
This is called Silverman’s bandwidth selector.
Although Silverman’s bandwidth selector is easy to implement, its validity is
only guaranteed when the true probability distribution is Gaussian. For more flexible
model selection, cross validation explained in Section 14.4 is highly useful. A

178
CHAPTER 16
NONPARAMETRIC ESTIMATION
(a) CV score
(b) Estimated density
FIGURE 16.13
Example of Gaussian KDE with bandwidth selected by likelihood cross validation.
MATLAB code of likelihood cross validation for Gaussian KDE is provided in
Fig. 16.12, and its behavior is illustrated in Fig. 16.13.
16.4 NNDE
In KDE explained above, the volume V of region R is fixed, and the number of
training samples k that fall into region R is determined from data. In this section, an
alternative method is introduced, where k is fixed and the volume V of region R is
determined from data.
16.4.1 NEAREST NEIGHBOR DISTANCE
As region R, let us consider the hypersphere with radius r centered at x. Then the
volume V of region R is given by
V =
π
d
2 rd
Γ( d
2 + 1)
,
where Γ(·) is the gamma function (see Section 4.3).
Setting the radius r at the minimum number such that k training samples are
included in the hypersphere, Eq. (16.3) immediately gives the following density
estimator:
pKNN(x) = kΓ( d
2 + 1)
nπ
d
2 rd
.
(16.8)
This is called NNDE.
k controls the smoothness of the density estimator, and it would be natural
to increase k as the number of training samples n grows. Indeed, to guarantee

16.4 NNDE
179
n=500; x=myrand(n); x2=x.^2;
ks=[10 50 100]; t=5; g=gamma(3/2);
d2=repmat(x2,[n 1])+repmat(x2’,[1 n])-2*x’*x;
v=mod(randperm(n),t)+1;
for j=1:t
S=sort(d2(v~=j,v==j));
for i=1:length(ks)
k=ks(i); r=sqrt(S(k+1,:));
s(j,i)=mean(log(k*g./(sum(v~=j)*sqrt(pi)*r)));
end
end
[dum,a]=max(mean(s)); k=ks(a);
m=1000; X=linspace(0,5,m);
D2=repmat(X.^2,[n 1])+repmat(x2’,[1 m])-2*x’*X;
S=sort(D2); r=sqrt(S(k+1,:))’; Ph=k*g./(n*sqrt(pi)*r);
figure(1); clf; plot(X,Ph,’r*’); k
FIGURE 16.14
MATLAB code for NNDE with the number of nearest neighbors selected by likelihood cross
validation. A random number generator “myrand.m” shown in Fig. 16.3 is used.
consistency of pKNN(x) (i.e., pKNN(x) converges to p(x) as n tends to infinity), k
should satisfy the following conditions [70]:
lim
n→∞k = ∞and
lim
n→∞
k
n = 0.
For example, k = √n satisfy the above condition.
However, likelihood cross validation explained in Section 14.4 is more useful in
practice. A MATLAB code of likelihood cross validation for NNDE is provided in
Fig. 16.14, and its behavior is illustrated in Fig. 16.15.
16.4.2 NEAREST NEIGHBOR CLASSIFIER
Finally, NNDE is applied to estimating the class-conditional probability density
p(x|y), and pattern recognition is performed based on the MAP rule (Section 11.3.1).
Pattern Recognition with Nearest Neighbor Distance
From Bayes’ theorem (see Section 5.4), the class-posterior probability p(y|x) is
expressed as
p(y|x) =
p(x|y)p(y)
c
y′=1 p(x|y′)p(y′) ∝p(x|y)p(y).

180
CHAPTER 16
NONPARAMETRIC ESTIMATION
(a) CV score
(b) Estimated density
FIGURE 16.15
Example of NNDE with the number of nearest neighbors selected by likelihood cross validation.
If NNDE with k = 1 is used, the class-conditional probability density p(x|y) is
estimated as
p(x|y) ≈Γ( d
2 + 1)
nyπ
d
2 rdy
,
where ry denotes the distance between input pattern x and the nearest training sample
among training samples in class y, and ny denotes the number of training samples in
class y. If the class-prior probability p(y) is approximated as
p(y) ≈ny
n ,
the class-posterior probability p(y|x) is approximated as
p(y|x) ≈Γ( d
2 + 1)
nyπ
d
2 rdy
ny
n ∝1
rdy
.
This means that, to perform pattern recognition based on NNDE, only rd
y is necessary.
Since the class y that maximizes 1/rd
y is the class that minimizes ry, the input
pattern x is simply classified into the same class as the nearest training sample
in NNDE-based pattern recognition. This pattern recognition method is called the
nearest neighbor classifier.
Fig. 16.16 illustrates an example of the nearest neighbor classifier, showing that
the decision boundaries can be obtained from the Voronoi diagram.
k-Nearest Neighbor Classifier
The nearest neighbor classifier is intuitive and easy to implement, but it is not robust
against outliers. For example, one of the training samples has an incorrect class label;
the decision regions contain an isolated region (see Fig. 16.16).

16.4 NNDE
181
FIGURE 16.16
Example of nearest neighbor classifier.
To mitigate this problem, NNDE for k > 1 is useful:
p(x|y) =
k
nyVy
,
where ny denotes the number of training samples in class y and Vy denotes the
volume of the minimum hypersphere centered at x that contains k training samples.
However, the above method requires computation of hyperspheres for each class.
A simpler implementation would be to find a hypersphere for training samples in all
classes and to estimate the marginal probability p(x) as
p(x) = k
nV .
Since p(x) can be decomposed as
p(x) =
c

y=1
p(x|y)p(y),
estimating the class-prior probability p(y) by
p(y) = ny
n
yields
k
nV ≈
c

y=1
p(x|y)ny
n .
Finally, the class-posterior probability (x|y) is estimated as
p(x|y) = ky
nyV ,
(16.9)

182
CHAPTER 16
NONPARAMETRIC ESTIMATION
1. Split labeled training samples Z = {(xi, yi)}n
i=1 into t disjoint subsets of
(approximately) the same size: {Zℓ}t
ℓ=1.
2. For each model candidate Mj
(a) For each split ℓ= 1,. . . ,t
i.
Obtain a classifier y(ℓ)
j (x) using model Mj from all training
samples without Zℓ.
ii. Compute the misclassification rate J(ℓ)
j
for y(ℓ)
j (x) for holdout
samples Zℓ:
J(ℓ)
j
=
1
|Zℓ|

(x′,y′)∈Zℓ
I(y(ℓ)
j (x′) , y′),
where |Zℓ| denotes the number of elements in the set Zℓand
I(e) = 1 if condition e is true and I(e) = 0 otherwise.
(b) Compute the average misclassification rate Jj over all t splits:
Jj = 1
t
t
ℓ=1
J(ℓ)
j .
3. Choose the model Mj that minimizes the average misclassification rate:
j = argmin
j
Jj.
4. Obtain a classifier using chosen model Mj, from all training samples
{(xi, yi)}n
i=1.
FIGURE 16.17
Algorithm of cross validation for misclassification rate.
where ky denotes the number of training samples in the hypersphere that belong to
class y. This allows us to construct a classifier in a simple way, which is called the
k-nearest neighbor classifier.
Cross Validation for Misclassification Rate
The tuning parameter k in the k-nearest neighbor classifier can be chosen by likeli-
hood cross validation for NNDE. However, in the context of pattern recognition, it
would be more direct to choose k by cross validation in terms of the misclassification
rate. The algorithm of cross validation for the misclassification rate is summarized in
Fig. 16.17.

16.4 NNDE
183
load digit.mat X T; [d,m,c]=size(X); X=reshape(X,[d m*c]);
Y=reshape(repmat([1:c],[m 1]),[1 m*c]);
ks=[1:10]; t=5; v=mod(randperm(m*c),t)+1;
for i=1:t
Yh=knn(X(:,v~=i),Y(v~=i),X(:,v==i),ks);
s(i,:)=mean(Yh~=repmat(Y(v==i),[length(ks) 1]),2);
end
[dum,a]=min(mean(s)); k=ks(a); [d,r,c]=size(T);
T=reshape(T,[d r*c]); U=reshape(knn(X,Y,T,k),[r c]);
for i=1:c, C(:,i)=sum(U==i); end, C, sum(diag(C))/sum(sum(C))
function U=knn(X,Y,T,ks)
m=size(T,2); D2=repmat(sum(T.^2,1),[size(X,2) 1]);
D2=D2+repmat(sum(X.^2,1)’,[1 m])-2*X’*T; [dum,z]=sort(D2,1);
for i=1:length(ks)
k=ks(i);
for j=1:m
Z=sort(Y(z(1:k,j))); g=find([1 Z(1:end-1)~=Z(2:end)]);
[dum,a]= max([g(2:end) k+1]-g); U(i,j)=Z(g(a));
end, end
FIGURE 16.18
MATLAB code for k-nearest neighbor classifier with k chosen by cross validation. The bottom
function should be saved as “knn.m.”
FIGURE 16.19
Confusion matrix for 10-class classification by k-nearest neighbor classifier. k = 1 was chosen by
cross validation for misclassification rate. The correct classification rate is 1932/2000 = 96.6%.

184
CHAPTER 16
NONPARAMETRIC ESTIMATION
A MATLAB code for 10-class hand-written digit recognition (see Section 12.5)
by the k-nearest neighbor classifier with k chosen by cross validation in terms of
the misclassification rate is provided in Fig. 16.18. The obtained confusion matrix is
shown in Fig. 16.19, where k = 1 was chosen by cross validation. The confusion
matrix shows that 1932 test samples out of 2000 samples are correctly classified,
meaning that the correct classification rate is
1932/2000 = 96.6%.
On the other hand, as shown in Section 12.5, the correct classification rate for the
same data set by FDA is
1798/2000 = 89.9%.
Therefore, for this hand-written digit recognition experiment, the k-nearest neighbor
classifier works much better than FDA.

CHAPTER
BAYESIAN INFERENCE17
CHAPTER CONTENTS
Bayesian Predictive Distribution.................................................. 185
Definition................................................................... 185
Comparison with MLE ...................................................... 186
Computational Issues ...................................................... 188
Conjugate Prior .................................................................. 188
MAP Estimation.................................................................. 189
Bayesian Model Selection ........................................................ 193
In the framework of MLE, parameter θ in parametric model q(x; θ) was treated as
a deterministic variable. In this chapter, Bayesian inference [15] is introduced which
handles parameter θ as a random variable.
17.1 BAYESIAN PREDICTIVE DISTRIBUTION
In this section, the basic idea of Bayesian inference is explained.
17.1.1 DEFINITION
If θ is regarded as a random variable, the following probabilities can be determined:
p(θ), p(θ|D), p(D|θ),
and p(D,θ),
where
D = {xi}n
i=1.
p(θ|D) is called the posterior probability of parameter θ given training samples D,
while p(θ) is called the prior probability of θ before observing training samples D.
p(D|θ) denotes the likelihood, which is the same quantity as the one used in MLE
(see Section 12.1), but it is regarded as a conditional probability in the Bayesian
framework:
p(D|θ) =
n

i=1
q(xi|θ).
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00028-5
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
185

186
CHAPTER 17
BAYESIAN INFERENCE
Note that the parametric model q(x|θ) is also represented as a conditional probability
in the Bayesian framework. p(D,θ) denotes the joint probability of training samples
D and parameter θ.
The joint probability p(D,θ) can be expressed as
p(D,θ) = p(D|θ)p(θ),
and its marginalization over θ gives

p(D,θ)dθ = p(D).
Thus, the marginal probability p(D) can be expressed as
p(D) =

*
,
n

i=1
q(xi|θ)+
-
p(θ)dθ.
The solution of Bayesian inference pBayes(x), called the Bayesian predictive
distribution, is given as the expectation of model q(x|θ) over the posterior probability
p(θ|D):
pBayes(x) =

q(x|θ)p(θ|D)dθ.
(17.1)
Since the posterior probability p(θ|D) can be expressed by using Bayes’ theorem
(see Section 5.4) as
p(θ|D) = p(D|θ)p(θ)
p(D)
=
n
i=1 q(xi|θ)p(θ)
n
i=1 q(xi|θ′)p(θ′)dθ′,
(17.2)
the Bayesian predictive distribution pBayes(x) can be expressed as
pBayes(x) =

q(x|θ)
n
i=1 q(xi|θ)p(θ)
n
i=1 q(xi|θ′)p(θ′)dθ′dθ.
(17.3)
This shows that the Bayesian predictive distribution pBayes(x) can actually be
computed without any learning, if parametric model q(x|θ) and prior probability
p(θ) are specified.
17.1.2 COMPARISON WITH MLE
In MLE, unknown true probability density p(x) is approximated by a parametric
model q(x; θ) with a single parameter estimate θML. On the other hand, in Bayesian
inference, infinitely many parameters are simultaneously
considered and their

17.1 BAYESIAN PREDICTIVE DISTRIBUTION
187
FIGURE 17.1
Bayes vs. MLE. The maximum likelihood so-
lution pML is always confined in the parametric
model q(x; θ), while the Bayesian predictive
distribution pBayes(x) generally pops out from
the model.
average over model q(x|θ) weighted according to the posterior probability p(θ|D) is
used as a density estimator. Let us intuitively explain the difference between Bayesian
inference and MLE using Fig. 17.1.
A parametric model q(x|θ) is a set of probability density functions and it is
denoted by a dotted line in Fig. 17.1. In practice, parametric model q(x|θ) is more
or less misspecified, meaning that the true probability density p(x) is not exactly
included in the parametric model q(x|θ). MLE finds the probability density function
in the parametric model that maximizes the likelihood, pML(x), which is equivalent to
finding the projection of p(x) onto the model under the empirical KL divergence (see
Section 14.2). On the other hand, in Bayesian inference, by taking the expectation of
parametric model q(x|θ) over the posterior probability p(θ|D), the solution pBayes(x)
is not generally confined in the model. In the illustration in Fig. 17.1, the solution
pBayes(x) pops out from the model to the right-hand side, and consequently it is closer
to the true probability density p(x) than pML(x).
The fundamental difference between Bayesian inference and MLE lies in whether
parameter θ is handled as a deterministic or random variable. However, in real-
ity, more significant philosophical difference is involved. More specifically, prior
probability p(θ) can contain subjective knowledge in Bayesian inference, which can
arbitrarily change the solution. On the other hand, MLE is objective and its solution is
purely computed from data. When non-Bayesian inference is contrasted to Bayesian
inference, it is sometimes referred to as frequentist inference.

188
CHAPTER 17
BAYESIAN INFERENCE
17.1.3 COMPUTATIONAL ISSUES
As explained in Section 17.1, the Bayesian predictive distribution pBayes(x) can be
computed without any learning in principle, if parametric model q(x|θ) and prior
probability p(θ) are specified:
pBayes(x) =

q(x|θ)p(θ|D)dθ,
(17.4)
where
p(θ|D) =
n
i=1 q(xi|θ)p(θ)
n
i=1 q(xi|θ′)p(θ′)dθ′ .
(17.5)
However, computation of the two integrations above is not straightforward if the
dimension of θ is high. Thus, a main technical challenge in Bayesian inference is
how to efficiently compute high-dimensional integrations.
To easily handle the integration in Eq.(17.4), it is preferable to obtain the posterior
probability p(θ|D) analytically. One possibility is to choose the prior probability
p(θ) so that the parametric form of the posterior probability p(θ|D) can be explicitly
obtained. Such a prior choice will be explained in Section 17.2. For nonconjugate
choice of prior probabilities, analytic approximation techniques of the integration in
Eq. (17.5) will be discussed in Chapter 18.
For handling the integration in Eq. (17.4), i.e., the expectation of parametric
model q(x|θ) over posterior probability p(θ|D), the simplest approximation scheme
would be to use a single point θ taken from the posterior probability. Such a single-
point approximation will be introduced in Section 17.3. Techniques for numerically
approximating the posterior expectation will be discussed in Chapter 19.
17.2 CONJUGATE PRIOR
As discussed above, it is convenient to analytically handle the integration in
Eq. (17.5). If the prior probability p(θ) is chosen so that the posterior probability
p(θ|D) takes the same parametric form as the prior probability p(θ), the posterior
probability p(θ|D) can be analytically obtained just by specifying its parameters.
Such a prior choice is called a conjugate prior for the likelihood p(D|θ).
Let us illustrate an example of the conjugate prior for the Gaussian model with
expectation 0 and variance σ2, where the inverse of the variance τ = σ−2, called
the precision, is regarded as a parameter (i.e., the parameter is θ = τ). Then the
parametric model is expressed as
q(x|τ) =
τ
2π exp

−τx2
2

.
(17.6)
For this model, let us employ the gamma distribution (see Section 4.3) as the prior
probability for precision τ:
p(τ; α, β) ∝τα−1e−βτ.
(17.7)

17.3 MAP ESTIMATION
189
For parametric model (17.6) combined with prior probability (17.7), the posterior
probability is again the gamma distribution:
p(τ|D) ∝
n

i=1
q(xi|τ)p(τ; α, β)
∝τn/2 exp *
,
−τ
2
n

i=1
x2
i +
-
τα−1e−βτ
= τ α−1e−βτ,
where the posterior parameters α and β are given as
α = α + n
2
and
β = β +
n
i=1 x2
i
2
.
Thus, the posterior probability can be obtained analytically just by computing the
posterior parameters α and β.
As shown above, conjugate priors are extremely useful from the viewpoint of
computation. However, conjugate priors depend on the parametric form of likelihood
p(D|θ), and they may not be available depending on p(D|θ). Furthermore, the
meaning of choosing conjugate priors is not clear from the viewpoint of statistical
inference. For general nonconjugate priors, analytic approximation techniques of the
posterior probability will be introduced in Chapter 18.
17.3 MAP ESTIMATION
Given the posterior probability p(θ|D) analytically, the next step is to compute the
posterior expectation:
pBayes(x) =

q(x|θ)p(θ|D)dθ.
In this section, MAP estimation is introduced, which approximates the above
integration by a single point θMAP:
pMAP(x) = q(x|θMAP),
where θMAP is the maximizer of the posterior probability p(θ|D) (see Fig. 17.2):
θMAP = argmax
θ
p(θ|D).
Since MAP estimation approximates the target density by a single point θMAP, its
property is actually close to MLE. Indeed, the MAP solution θMAP can be expressed
as
θMAP = argmax
θ
*
,
n

i=1
log q(xi|θ) + log p(θ)+
-
,
(17.8)

190
CHAPTER 17
BAYESIAN INFERENCE
FIGURE 17.2
MAP estimation.
(a) When n = 10
(b) When n = 2
FIGURE 17.3
Example of MLE for Gaussian model. When the number of training samples, n, is small, MLE
tends to overfit the samples.
and thus it actually minimizes the sum of the log-likelihood and an additional term
log p(θ).
As explained in Chapter 13, MLE tends to overfit the training samples if the
sample size is small (Fig. 17.3). The additional term log p(θ) in Eq. (17.8) can work
as a penalty to mitigate overfitting. For this reason, MAP estimation is also referred
to as penalized MLE. MAP estimation tries to increase not only the likelihood but
also the prior probability, and therefore the solution tends to be biased toward the
parameter having a larger prior probability. Penalizing the objective function in this
way is also called regularization (see Chapter 23 for details).
Let us specifically compute the MAP solution for the Gaussian model with
expectation µ and variance-covariance matrix Id (i.e., the parameter is θ = µ):
q(x|µ) =
1
(2π)
d
2
exp

−(x −µ)⊤(x −µ)
2

.

17.3 MAP ESTIMATION
191
Let us consider the following Gaussian prior:
p(µ; β) =
1
(2π β2)
d
2
exp

−µ⊤µ
2β2

,
(17.9)
which prefers µ closer to the origin. For this setup, the penalized log-likelihood is
given by
PL(µ) =
n

i=1
log q(xi|µ) + log p(µ)
= −nd
2 log(2π) −1
2
n

i=1
∥xi −µ∥2 −d
2 log(2π β2) −
1
2β2 ∥µ∥2.
Taking the derivative of PL(µ) and setting it to zero yield
∂
∂µ PL(µ) =
n

i=1
(xi −µ) −1
β2 µ = 0d,
from which the MAP solution µMAP is obtained as
µMAP =
1
n + β−2
n

i=1
xi.
On the other hand, the maximum likelihood solution µMLE for this model is given
by
µMLE = 1
n
n

i=1
xi.
β > 0 implies
µ(j)
MAP
 =
1
n + β−2

n

i=1
x(j)
i

< 1
n

n

i=1
x(j)
i

= µ(j)
MLE
 ,
where x(j)
i , µ(j)
MAP, and µ(j)
MLE denote the jth elements of vectors xi, µMAP, and µMLE,
respectively. Thus, the MAP solution µMAP is always closer to the origin than the
maximum likelihood solution µMLE, which the Gaussian prior (17.9) favors.
A MATLAB code for penalized MLE with one-dimensional Gaussian model is
given in Fig. 17.4, and its behavior is illustrated in Fig. 17.5. This demonstrates that,
if the prior probability is chosen properly (i.e., β ≈1), MAP estimation can give a
better solution than MLE.
In MAP estimation, the maximizer of the posterior probability (i.e., the mode)
was used. An alternative idea is to use the mean of the posterior probability:
p(x) = q(x|θ),

192
CHAPTER 17
BAYESIAN INFERENCE
n=12; mu=0.5; x=randn(n,1)+mu;
bs=[0.01:0.01:3]; bl=length(bs);
MLE=mean(x);
for i=1:bl
MAP(i)=sum(x)/(n+bs(i).^(-2));
end
figure(1); clf; hold on;
plot(bs,mu*ones(1,bl),’k:’);
plot(bs,MAP,’r-’);
plot(bs,MLE*ones(1,bl),’b-.’);
xlabel(’\beta’); ylabel(’\mu’);
legend(’True’,’MAP’,’MLE’,4);
FIGURE 17.4
MATLAB code for penalized MLE with one-dimensional Gaussian model.
FIGURE 17.5
Example
of
MAP
estimation
with
one-
dimensional Gaussian model.
where
θ =

θp(θ|D)dθ.
If the posterior probability p(θ|D) is a popular probability distribution, its expec-
tation may be known analytically and then the posterior expectation θ can also be
obtained analytically.
However, single-point approximation of the Bayesian predictive distribution
pBayes(x) loses the distinctive feature of Bayesian inference that the solution pBayes(x)

17.4 BAYESIAN MODEL SELECTION
193
can pop out from the parametric model q(x|θ) (see Fig. 17.1). To enjoy Bayesianity,
it is essential to compute the integration at least approximately. In Chapter 19,
techniques for numerically approximating the posterior expectation will be discussed.
17.4 BAYESIAN MODEL SELECTION
In Bayesian inference, the prior probability of parameters is utilized. If such prior
knowledge is not available, the prior probability has to be determined by a user. Since
the solution of Bayesian inference depends on the choice of the prior probability (see
Fig. 17.5), it must be determined in an objective and appropriate way. In this section,
choice of prior probabilities and models in the Bayesian framework is addressed.
Suppose that the prior probability is parameterized by β:
p(θ; β),
where β is called the hyperparameter and this should be distinguished from ordinary
parameter θ in the parametric model q(x|θ). As explained in Section 14.3, model
selection of MLE is possible by cross validation, and it can also be applied to
Bayesian inference. Below, an alternative approach that is specific to Bayesian
inference is introduced.
MLE is aimed at setting parameter θ so that training samples {x}n
i=1 at hand
are most typically generated. The fundamental idea of Bayesian model selection is
to apply MLE to hyperparameter β. More specifically, the probability that training
samples D = {x}n
i=1 at hand are generated is expressed as
p(D; β) =

n

i=1
q(xi|θ)p(θ; β)dθ = ML(β).
(17.10)
Eq. (17.10) viewed as a function of β is called the marginal likelihood, which is also
referred to as the evidence and its negative log is called the free energy.
The method of determining hyperparameter β so that the marginal likelihood is
maximized is called empirical Bayes, type-II MLE, or evidence maximization:
βEB = argmax
β
ML(β).
In addition to the hyperparameter, parametric model q(x|θ) may also be selected by
empirical Bayes. That is, among a set of model candidates, the one that maximizes
the marginal likelihood may be selected as the most promising one.
Let us specifically compute the marginal likelihood for one-dimensional Gaussian
parametric model with expectation µ and variance 1,
q(x|µ) =
1
√
2π
exp

−(x −µ)2
2

,
(17.11)

194
CHAPTER 17
BAYESIAN INFERENCE
and a Gaussian prior probability with expectation 0 and variance β2:
p(µ; β) =
1

2π β2 exp

−µ2
2β2

.
(17.12)
The marginal likelihood can be expressed as
ML(β) =

n

i=1
q(xi|µ)p(µ; β)dµ
= (2π)−n
2 (2π β2)−1
2

exp *
,
−1
2
n

i=1
(xi −µ)2 −µ2
2β2 +
-
dµ.
For
µMAP =
1
n + β−2
n

i=1
xi,
completing the square (see Eq. (4.2)) yields
ML(β) = (2π)−n
2 (2π β2)−1
2 exp *
,
−1
2
n

i=1
(xi −µMAP)2 −
µ2
MAP
2β2 +
-
×

exp *
,
−(µ −µMAP)2
2(n + β−2)−1 +
-
dµ
= (2π)−n
2 (nβ2 + 1)−1
2 exp *
,
−1
2
n

i=1
(xi −µMAP)2 −
1
2β2 µ2
MAP+
-
,
where Gaussian integral shown in Fig. 4.1,

exp *
,
−(µ −µMAP)2
2(n + β−2)−1 +
-
dµ =

2π
n + β−2
1
2
,
is used. Consequently, the log marginal likelihood is expressed as
log ML(β) = −n
2 log 2π −1
2 log(nβ2 + 1)
−1
2
n

i=1
(xi −µMAP)2 −
µ2
MAP
2β2 .
A MATLAB code of empirical Bayes for parametric model (17.11) and prior
probability(17.12) is given in Fig. 17.6, and its behavior is illustrated in Fig. 17.7. In
this example, βEB = 2.19 is chosen as the empirical Bayes solution, and true µ = 0.5
was estimated by the MAP method as µMAP = 0.537, which is slightly better than
MLE µMLE = 0.546.

17.4 BAYESIAN MODEL SELECTION
195
n=12; mu=0.5; x=randn(n,1)+mu;
bs=[0.01:0.01:3]; bl=length(bs);
MLE=mean(x);
for i=1:bl
bb=bs(i)^(-2); MAP(i)=sum(x)/(n+bb);
logML(i)=-n/2*log(2*pi)-sum((x-MAP(i)).^2)/2 ...
-MAP(i)^2/(2*bb)-log(n*bb+1);
end
[dummy,c]=max(logML);
figure(1); clf; hold on;
plot(bs,logML,’r-’);
plot(bs(c),logML(c),’ro’);
xlabel(’\beta’); legend(’log ML’,4);
figure(2); clf; hold on;
plot(bs,mu*ones(1,bl),’k:’,’LineWidth’,5);
plot(bs,MAP,’r-’,’LineWidth’,2);
plot(bs,MLE*ones(1,bl),’b-.’,’LineWidth’,2);
plot(bs(c),MAP(c),’ro’,’LineWidth’,4,’MarkerSize’,10);
xlabel(’\beta’); ylabel(’\mu’);
legend(’True’,’MAP’,’MLE’,4);
FIGURE 17.6
MATLAB code for empirical Bayes.
(a) Log marginal likelihood
(b) MLE vs. MAP
FIGURE 17.7
Example of empirical Bayes.


CHAPTER
ANALYTIC
APPROXIMATION OF
MARGINAL LIKELIHOOD18
CHAPTER CONTENTS
Laplace Approximation ........................................................... 197
Approximation with Gaussian Density ...................................... 197
Illustration ................................................................. 199
Application to Marginal Likelihood Approximation ......................... 200
Bayesian Information Criterion (BIC)....................................... 200
Variational Approximation ........................................................ 202
Variational Bayesian EM (VBEM) Algorithm ................................ 202
Relation to Ordinary EM Algorithm......................................... 203
As discussed in Section 17.2, the use of conjugate priors allows us to avoid the
explicit computation of the integration in the marginal likelihood:
ML(β) =

n

i=1
q(xi|θ)p(θ; β)dθ.
In this chapter, general choices of prior probabilities are considered and analytic
approximation methods of the marginal likelihood are introduced.
18.1 LAPLACE APPROXIMATION
In this section, Laplace approximation is introduced, which allows us to analytically
approximate the integration of any twice-differentiable non-negative function f (x):

f (θ)dθ.
18.1.1 APPROXIMATION WITH GAUSSIAN DENSITY
Let θ be the maximizer of f (θ) with respect to θ:
θ = argmax
θ
f (θ).
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00029-7
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
197

198
CHAPTER 18
ANALYTIC APPROXIMATION OF MARGINAL LIKELIHOOD
Let us apply the Taylor series expansion (Fig. 2.8) to log f (θ) about the maximizer
θ:
log f (θ) = log f (θ) + (θ −θ)⊤∂
∂θ log f (θ)
θ=θ
+ 1
2(θ −θ)⊤H(θ −θ) + · · · ,
(18.1)
where H is the Hessian matrix:
H =
∂2
∂θ∂θ⊤log f (θ)
θ=θ
.
Since θ is the maximizer of log f (θ),
∂
∂θ log f (θ)
θ=θ
= 0b
holds and thus the first-order term in Eq. (18.1) is zero.
Let log f (θ) be Eq. (18.1) up to the second-order terms (Fig. 18.1):
log f (θ) = log f (θ) + 1
2(θ −θ)⊤H(θ −θ).
Exponentiating both sides yields
f (θ) = f (θ) exp
1
2(θ −θ)⊤H(θ −θ)

.
Recalling that the integration of the normal density is 1,
1
(2π)
b
2 det(−H)
1
2

exp

−1
2(θ −θ)⊤(−H)(θ −θ)

dθ = 1,
integration of f (θ) yields

f (θ)dθ = f (θ)

(2π)b
det(−H) ≈

f (θ)dθ,
where b denotes the dimensionality of θ. This is Laplace approximation to

f (θ)dθ.
Since approximating log f (θ) by a quadratic function corresponds to approxi-
mating f (θ) by an unnormalized Gaussian function, Laplace approximation is quite
accurate if f (θ) is close to Gaussian. For this reason, Laplace approximation is also
referred to as Gaussian approximation.

18.1 LAPLACE APPROXIMATION
199
FIGURE 18.1
Laplace approximation.
18.1.2 ILLUSTRATION
Let us illustrate how to compute Laplace approximation to

f (θ)dθ, where
f (θ) = N(θ; 0,12) + N(θ; 0,22),
N(θ; µ,σ2) =
1
√
2πσ2 exp

−(θ −µ)2
2σ2

.
Note that N(θ; µ,σ2) is a normal density and thus the true value is 2.
The maximizer of f (θ) is given by
θ = argmax
θ
f (θ) = 0.
Then
f (θ) =
1
√
2π
exp *
,
−
θ2
2
+
-
+
1
√
8π
exp *
,
−
θ2
8
+
-
=
3
2
√
2π
,
f ′(θ) = ∂
∂θ log f (θ)
θ=θ
= −θN(θ; 0,12) −
θ
4 N(θ; 0,22) = 0,
f ′′(θ) = ∂2
∂θ2 log f (θ)
θ=θ
= (θ2 −1)N(θ; 0,12) + *
,
θ2
16 −1
4
+
-
N(θ; 0,22)
= −
9
8
√
2π
,
which yield
H = ∂2
∂θ2 log f (θ)
θ=θ
= f ′′(0) f (0) −f ′(0)2
f (0)2
= −3
4.

200
CHAPTER 18
ANALYTIC APPROXIMATION OF MARGINAL LIKELIHOOD
Thus, the Laplace approximation is given by
f (0)

2π
−H =
√
3 ≈1.732.
18.1.3 APPLICATION TO MARGINAL LIKELIHOOD
APPROXIMATION
For the marginal likelihood,
ML(β) =

n

i=1
q(xi|θ)p(θ; β)dθ.
Laplace approximation with respect to θ yields
ML(β) ≈
n

i=1
q(xi|θMAP)p(θMAP; β)

(2π)b
det(−H),
where b denotes the dimensionality of θ (i.e., the number of parameters), and
θMAP = argmax
θ

n

i=1
log q(xi|θ) + log p(θ; β)

,
H =
∂2
∂θ∂θ⊤*
,
n

i=1
log q(xi|θ) + log p(θ; β)+
-
θ=θMAP
.
(18.2)
If the number of training samples, n, is large, the central limit theorem (see
Section 7.4) asserts that the posterior probability p(θ|D) converges in distribution to
the Gaussian distribution. Therefore, the Laplace-approximated marginal likelihood
would be accurate when a large number of training samples are available.
18.1.4 BAYESIAN INFORMATION CRITERION (BIC)
The logarithm of the Laplace-approximated marginal likelihood is given by
log ML(β) ≈
n

i=1
log q(xi|θMAP) + log p(θMAP; β)
+ b
2 log(2π) −1
2 log (det(−H)) .
(18.3)
Let us further approximate this under the assumption that the number of training
samples, n, is large.
The first term n
i=1 log q(xi|θMAP) in Eq. (18.3) has asymptotic order n (see
Fig. 14.4):
n

i=1
log q(xi|θMAP) = O(n).

18.1 LAPLACE APPROXIMATION
201
On the other hand, the second term log p(θMAP; β) and the third term b
2 log(2π) are
independent of n:
log p(θMAP; β) = O(1) and
b
2 log(2π) = O(1).
According to the law of large numbers (see Section 7.3), the Hessian matrix H
defined by Eq. (18.2) divided by n converges in probability to 
H:
1
n H =
∂2
∂θ∂θ⊤*
,
1
n
n

i=1
log q(xi|θ) + 1
n log p(θ; β)+
-
θ=θMAP
p
−→
H,
where

H =
∂2
∂θ∂θ⊤

E [log q(x|θ)]
θ=θMAP
.
Since 
H is a b × b matrix,
det(n 
H) = nbdet( 
H)
holds and therefore
1
2 log (det(−H))
p
−→b
2 log n + 1
2 log

det(−
H)

is obtained. The first term b
2 log n is proportional to log n, while the second term
1
2 log

det(−
H)

is independent of n:
b
2 log n = O(log n) and
1
2 log

det(−
H)

= O(1).
Here, suppose that n is large enough so that terms with O(1) can be ignored. Then
only n
i=1 log q(xi|θMAP) and b
2 log n remain. Furthermore, the difference between
the MAP solution θMAP and the maximum likelihood solution θMLE is known to be
O(n−1):
n

i=1
log q(xi|θMAP) =
n

i=1
log q(xi|θMLE) + O(n−1).
Consequently, Laplace approximation of the log marginal likelihood given by
Eq. (18.3) can be further approximated as
log ML(β) ≈
n

i=1
log q(xi|θMLE) −b
2 log n.

202
CHAPTER 18
ANALYTIC APPROXIMATION OF MARGINAL LIKELIHOOD
The negative of the right-hand side is called the BIC:
BIC = −
n

i=1
log q(xi|θMLE) + b
2 log n.
BIC is quite simple and thus is popularly used in model selection. However, BIC is
no longer dependent on prior probabilities and thus cannot be used for setting the
prior probability. It is also known that BIC is equivalent to the minimum description
length (MDL) criterion [84], which was derived in a completely different framework.
BIC is similar to AIC explained in Section 14.3, but the second term is different:
AIC = −
n

i=1
log q(xi; θMLE) + b.
When n > e2 ≈7.39, BIC has a stronger penalty than AIC and thus a simpler
model would be chosen. However, since AIC and BIC are derived in completely
different frameworks (KL divergence approximation and the marginal likelihood
approximation), it cannot be simply concluded which one is more superior than the
other.
18.2 VARIATIONAL APPROXIMATION
When the integrand n
i=1 q(xi|θ)p(θ; β) is not close to Gaussian, Laplace-
approximated marginal likelihood may not be accurate. In this section, variational
approximation is introduced, which finds the best approximation to the marginal
likelihood in a limited function class that is easier to compute [71].
18.2.1 VARIATIONAL BAYESIAN EM (VBEM) ALGORITHM
The marginal likelihood can be expressed as
ML(β) = p(D; β) =

p(D,η,θ; β)dηdθ,
where η is called a latent variable. Let us consider probability density functions
for η and θ, denoted by q(η) and r(θ), called the trial distributions. Then Jensen’s
inequality (see Section 8.3.1) gives the following lower bound:
log ML(β) = log

p(D,η,θ; β)dηdθ
= log

q(η)r(θ) p(D,η,θ; β)
q(η)r(θ)
dηdθ ≥−F(q,r),
where
F(q,r) =

q(η)r(θ) log
q(η)r(θ)
p(D,η,θ; β)dηdθ

18.2 VARIATIONAL APPROXIMATION
203
is a functional of probability density functions q and r called the variational free
energy (while −log ML(β) is called the free energy). If q and r are chosen to
minimize the variational free energy, a good approximator to the log marginal
likelihood may be obtained.
Setting the partial derivatives of the variational free energy at zero,
∂
∂q F(q,r) = 0 and
∂
∂r F(q,r) = 0,
yields that the solutions should satisfy
q(η) ∝exp

r(θ) log p(D,η|θ; β)dθ

,
(18.4)
r(θ) ∝p(θ) exp

q(η) log p(D,η|θ; β)dη

.
(18.5)
Since no method is known to analytically solve Eq. (18.4) and Eq. (18.5), these
equations are used as updating formulas like the EM algorithm (see Section 15.4).
Such an EM-like algorithm is called the VBEM algorithm, and Eq. (18.4) and
Eq. (18.5) are, respectively, called the VB-E step and VB-M step.
The variational free energy can be expressed by using the KL divergence (see
Section 14.2) as
F(q,r) = KL q(η)r(θ)p(η,θ|D; β) −log ML(β).
This shows that reducing the variational free energy F(q,r) corresponds to reducing
KL q(η)r(θ)p(η,θ|D; β), and therefore the VBEM algorithm can be regarded as
approximating p(η,θ|D; β) by q(η)r(θ).
For this reason, r(θ) obtained by the VBEM algorithm may be a good approxi-
mation to the posterior probability p(θ|D; β).
18.2.2 RELATION TO ORDINARY EM ALGORITHM
As explained in Section 15.4, the ordinary EM algorithm maximized a lower bound
of the likelihood. Here, it is shown that the ordinary EM algorithm can also be
interpreted as a variational approximation. Jensen’s inequality (Section 8.3.1) yields
the following lower bound of the log-likelihood:
log p(D|θ) = log

p(D,η|θ)dη
= log

q(η|D,θ′) p(D,η|θ)
q(η|D,θ′)dη ≥b(q,θ),
where
b(q,θ) =

q(η|D,θ′) log p(D,η|θ)
q(η|D,θ′)dη.

204
CHAPTER 18
ANALYTIC APPROXIMATION OF MARGINAL LIKELIHOOD
∂
∂q b(q,θ) = 0 yields q(η|D,θ′) = p(η|D,θ), which is the E-step of the ordinary
EM algorithm. Then finding θ′ that satisfies
∂
∂θ b(q,θ)|θ=θ′ = 0 is the M-step of the
ordinary EM algorithm.
The relation between the ordinary EM algorithm and the VBEM algorithm can
be further elucidated by the use of Dirac’s delta function δ(·), which satisfies for any
function g : R →R and any real number κ,
∞
−∞
g(τ)δ(κ −τ)dτ = g(κ).
Thus, convolution with Dirac’s delta function allows us to extract the value of an
arbitrary function g(·) at an arbitrary point κ. Dirac’s delta function δ(·) can be
expressed as the limit of the normal density:
δ(τ) = lim
σ→0
1
√
2πσ2 exp

−τ2
2σ2

.
For multidimensional τ = (τ1,. . . ,τd)⊤, Dirac’s delta function is defined in an
elementwise manner as
δ(τ) = δ(τ1) × · · · × δ(τd).
Dirac’s delta function, setting
r(θ′) = δ(θ′ −θ)
in the VB-E step yields
q(η) ∝p(D,η|θ) ∝p(η|D,θ),
which agrees with the ordinary E-step.

CHAPTER
NUMERICAL
APPROXIMATION OF
PREDICTIVE
DISTRIBUTION
19
CHAPTER CONTENTS
Monte Carlo Integration .......................................................... 205
Importance Sampling ............................................................ 207
Sampling Algorithms ............................................................. 208
Inverse Transform Sampling................................................ 208
Rejection Sampling ........................................................ 212
Markov Chain Monte Carlo (MCMC) Method ............................... 214
MAP estimation explained in Section 17.3 is a useful alternative to MLE due to its
simplicity and ability to mitigate overfitting. However, since only a single parameter
value,
θMAP = argmax
θ
p(θ|D),
is used, the distinctive feature of Bayesian inference that infinitely many parameters
are considered is lost. Therefore, the obtained density estimator is always included
in the parametric model (Fig. 17.1). In this chapter, algorithms for numerically
approximating the Bayesian predictive distribution,
pBayes(x) =

q(x|θ)p(θ|D)dθ,
(19.1)
are introduced.
19.1 MONTE CARLO INTEGRATION
The Monte Carlo method is a generic name of algorithms that use random numbers,
and its name stems from the Monte Carlo Casino in Monaco. In this section, the
method of Monte Carlo integration is introduced to numerically approximate the
integration in Eq. (19.1).
More specifically, Monte Carlo integration approximates the expectation of a
function g(θ),

g(θ)p(θ)dθ,
(19.2)
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00030-3
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
205

206
CHAPTER 19
NUMERICAL APPROXIMATION OF DISTRIBUTION
FIGURE 19.1
Numerical computation of π by Monte Carlo
integration.
by the average over i.i.d. samples {θi}n
i=1 following p(θ) as
1
n
n

i=1
g(θi).
(19.3)
By the law of large numbers (see Section 7.3), consistency of Monte Carlo integration
is guaranteed. Namely, in the limit n →∞, Eq. (19.3) converges in probability to
Eq. (19.2).
As illustration, let us approximate π ≈3.14 by Monte Carlo integration. Let us
consider a 2 × 2 square and its inscribed circle of radius 1 (Fig. 19.1). Let g(x, y) be
the function defined by
g(x, y) =

1
x2 + y2 ≤1,
0
otherwise,
and let p(x, y) be the uniform distribution on [−1,1]2:
p(x, y) =

1/4
−1 ≤x, y ≤1,
0
otherwise.
Then the area of the inscribed circle, which is equal to π, is expressed as
π = 4

g(x, y)p(x, y)dxdy.

19.2 IMPORTANCE SAMPLING
207
n=10000000; x=rand(n,2)*2-1; pih=4*mean(sum(x.^2,2)<=1)
FIGURE 19.2
MATLAB code for numerically computing π by Monte Carlo integration.
Let us draw n i.i.d. samples {(xi, yi)}n
i=1 following the uniform distribution p(x, y)
and compute the above expectation by Monte Carlo integration:
π ≈4
n
n

i=1
g(xi, yi) = 4n′
n ,
where n′ denotes the number of samples included in the circle. In the limit n →∞,
4n′/n converges in probability to π.
A MATLAB code for numerically computing π by Monte Carlo integration is
given in Fig. 19.2, which tends to give 3.14.
19.2 IMPORTANCE SAMPLING
To perform Monte Carlo integration for approximating the Bayesian
predictive
distribution given by Eq. (19.1), random samples need to be generated following
the posterior probability p(θ|D). Techniques to generate random samples from an
arbitrary probability distribution will be discussed in Section 19.3. In this section,
another approach called importance sampling is introduced, which approximates the
expectation over any (complicated) target probability density p(θ) based on another
(simple) probability density p′(θ) such as the normal distribution. p′(θ) is called a
proxy distribution.
In importance sampling, samples {θ′
i′}n′
i′=1 drawn i.i.d. from p′(θ) are first
generated. Then the expectation of a function g(θ) over any p(θ) is approximated
by the average over {θ′
i′}n′
i′=1 weighted according to the importance p(θ)/p′(θ):

g(θ)p(θ)dθ =

g(θ) p(θ)
p′(θ)

p′(θ)dθ ≈1
n′
n′

i′=1
g(θi′) p(θi′)
p′(θi′).
As the name stands for, p(θ)/p′(θ) indicates how important a sample drawn from
p′(θ) is in p(θ). By the law of large numbers (see Section 7.3), consistency of
importance sampling is guaranteed. Namely, in the limit n →∞, the importance-
weighted average
1
n′
n′
i′=1 g(θi′) p(θi′)
p′(θi′) converges in probability to the true expecta-
tion

g(θ)p(θ)dθ.
Let us compute the expectation of function g(θ) = θ2 over the standard Laplace
distribution (see Section 4.5) by importance sampling using the standard normal
distribution as a proxy:

208
CHAPTER 19
NUMERICAL APPROXIMATION OF DISTRIBUTION
n=10000000; s=3; x=randn(n,1)*s; x2=x.^2; ss=2*s^2;
t=mean(x2.*(exp(-abs(x))/2)./(exp(-x2./ss)/sqrt(ss*pi)))
FIGURE 19.3
MATLAB code for importance sampling.

θ2p(θ)dθ ≈1
n′
n′

i′=1
θ2
i′ p(θi′)
p′(θi′),
(19.4)
where p(θ) and p′(θ) are the standard Laplace and Gaussian densities:
p(θ) = 1
2 exp(−|θ|) and p′(θ) =
1
√
2πσ2 exp

−θ2
2σ2

.
Since the left-hand side of Eq. (19.4) is actually the variance of the standard Laplace
distribution, La(0,1), its true value is 2. A MATLAB code for computing the right-
hand side of Eq. (19.4) is given in Fig. 19.3, which tends to give 2.00.
Although importance sampling is guaranteed to be consistent, its variance can
be large depending on the choice of a proxy distribution. This implies that a large
number of samples may be needed to obtain a reliable value by importance sampling.
19.3 SAMPLING ALGORITHMS
In this section, methods for generating random samples from an arbitrary probability
distribution are introduced, based on a random sample generator for the uniform
distribution or the normal distribution (e.g. the rand or the randn functions in
MATLAB). By directly generating i.i.d. samples {θi}n
i=1 from p(θ), the expectation
of a function g(θ) can be approximated as

g(θ)p(θ)dθ ≈1
n
n

i=1
g(θi).
19.3.1 INVERSE TRANSFORM SAMPLING
Inverse transform sampling generates a one-dimensional random sample θ that fol-
lows a probability distribution with density p(θ) based on a uniform random variable
u on [0,1] [62] and the cumulative distribution function of p(θ). The cumulative
distribution function of p(θ), denoted by P(θ), is defined as follows (Fig. 19.4):
P(θ) =
θ
−∞
p(u)du.
Let θ = P−1(u) be the inverse function of u = P(θ). Then, for the uniform random
variable u on [0,1], θ = P−1(u) has probability density p(θ) (Fig. 19.5). Thus, for n

19.3 SAMPLING ALGORITHMS
209
(a) Probability density function p(θ)
(b) Cumulative distribution function P(θ)
FIGURE 19.4
Examples of probability density function p(θ) and its cumulative distribution function P(θ).
Cumulative distribution function is monotone nondecreasing and satisfies limθ→−∞P(θ) = 0 and
limθ→∞P(θ) = 1.
FIGURE 19.5
Inverse transform sampling.
uniform random variables {ui}n
i=1 on [0,1],
{θi | θi = P−1(ui)}n
i=1
are i.i.d. with p(θ).
The validity of the above algorithm can be proved as follows. Since θ = P−1(u),
for any τ,
Pr(θ ≤τ) = Pr(P−1(u) ≤τ).
As illustrated in Fig. 19.6, θ ≤θ′ implies P(θ) ≤P(θ′), and therefore
Pr(P−1(u) ≤τ) = Pr(u ≤P(τ)).

210
CHAPTER 19
NUMERICAL APPROXIMATION OF DISTRIBUTION
FIGURE 19.6
θ ≤θ′ implies P(θ) ≤P(θ′).
Furthermore, since u follows the uniform distribution on [0,1],
Pr(u ≤P(τ)) =
P(τ)
0
du = P(τ)
holds and therefore
Pr(θ ≤τ) = P(τ).
This means that the cumulative distribution function of θ generated by inverse
transform sampling agrees with the target P(τ).
Let us generate random samples {θi}n
i=1 that are i.i.d. with the standard Laplace
distribution La(0,1). The probability density function p(θ), cumulative distribution
function P(θ), and its inverse function P−1(u) are given as follows (see Fig. 19.7):
p(θ) = 1
2e−|θ|,
P(θ) = 1
2

1 + sign(θ)

1 −e−|θ|
,
P−1(u) = −sign

u −1
2

log

1 −2

u −1
2


,
where sign(θ) denotes the sign function:
sign(θ) =

1
(θ > 0),
0
(θ = 0),
−1
(θ < 0).
A MATLAB code for inverse transform sampling is given in Fig. 19.8, and its
behavior is illustrated in Fig. 19.9.

19.3 SAMPLING ALGORITHMS
211
(a) Probability density function p(θ)
(b) Cumulative distribution function P(θ)
(c) Inverse cumulative distribution function
P−1(u)
FIGURE 19.7
Laplace distribution.
n=10000; u=rand(1,n); y=-sign(u-1/2).*log(1-2*abs(u-1/2));
figure(1); clf; hist(u,50);
figure(2); clf; hist(y,linspace(-8,8,30));
FIGURE 19.8
MATLAB code for inverse transform sampling.
As shown above, inverse transform sampling is a simple algorithm to generate
samples following an arbitrary distribution. However, it can be applied only to one-
dimensional distributions. Furthermore, the inverse cumulative distribution function
θ = P−1(u) needs to be explicitly computed, which can be difficult depending on the
probability distributions.

212
CHAPTER 19
NUMERICAL APPROXIMATION OF DISTRIBUTION
(a) Generated uniform samples u
(b) Obtained Laplace samples
FIGURE 19.9
Example of inverse transform sampling for Laplace distribution.
For i = 1,. . . ,n, iterate the following steps:
1. Generate a proposal point θ′ following a proposal distribution
p′(θ).
2. Generate a uniform random sample v on [0, κ].
3. If v > p(θ′)/p′(θ′), reject the proposal point θ′ and go to 1.
4. Accept the proposal point θ′ and set θi ←−θ′.
5. Increase the index: i ←−i + 1.
FIGURE 19.10
Algorithm of rejection sampling.
19.3.2 REJECTION SAMPLING
Rejection sampling is a computer-intensive approach to generating random samples
following p(θ) based on samples drawn i.i.d. from another density p′(θ) [117].
p′(θ) is called a proposal distribution, and a simple distribution such as the uniform
distribution or the normal distribution is usually chosen.
The assumption required in rejection sampling is that an upper bound of the
probability density ratio p(θ)/p′(θ) exists and is known:
max
θ
p(θ)
p′(θ)

≤κ < ∞.
In rejection sampling, a sample θ′ is generated following a proposal distribution
p′(θ), which is called a proposal point. Then from the uniform distribution on [0, κ],

19.3 SAMPLING ALGORITHMS
213
FIGURE 19.11
Illustration of rejection sampling when the pro-
posal distribution is uniform.
a sample v is generated, which is used for evaluating the proposal point θ′. If
v ≤p(θ′)
p′(θ′),
the proposal point θ′ is accepted, i.e. set θi = θ′ and i ←i + 1. Otherwise, the
proposal point θ′ is rejected. This procedure is repeated until n points are accepted.
Then the accepted samples {θi}n
i=1 have probability density p(θ). The algorithm of
rejection sampling is summarized in Fig. 19.10.
The idea of rejection sampling can be more easily understood if p(θ) is defined
on a finite set X (say [0,1]) and the uniform distribution on X is used as the proposal
distribution p′(θ). As illustrated in Fig. 19.11, samples generated below/above the
curve of p(θ) are accepted/rejected in this setup, which results in samples having
probability density p(θ).
A MATLAB code for rejection sampling for the probability density function,
p(θ) =

1
4θ
(0 ≤θ < 1),
1
2 −1
4θ
(1 ≤θ < 2),
1
4
(2 ≤θ < 3),
−3
2 + 1
2θ
(3 ≤θ < 4),
5
2 −1
2θ
(4 ≤θ ≤5),
is given in Fig. 19.12, and its behavior is illustrated in Fig. 19.13.
Rejection sampling only requires the upper bound κ, while inverse transform
sampling needs analytic computation of the inverse cumulative distribution function.

214
CHAPTER 19
NUMERICAL APPROXIMATION OF DISTRIBUTION
n=10000; u=5*rand(n,1); v=0.6*rand(n,1); y=zeros(n,1);
t=(0<=u & u<1); y(t)=0.25*u(t);
t=(1<=u & u<2); y(t)=-0.25*u(t)+0.5;
t=(2<=u & u<3); y(t)=0.25*ones(size(u(t)));
t=(3<=u & u<4); y(t)=0.5*u(t)-1.5;
t=(4<=u & u<=5); y(t)=-0.5*u(t)+2.5;
x=u(v<=y);
figure(1); clf; hold on; hist(x,50);
FIGURE 19.12
MATLAB code for rejection sampling.
(a) Generated uniform samples (θ, v)
(b) Obtained samples following p(θ)
FIGURE 19.13
Example of rejection sampling.
Thus, it is much easier to implement in practice. Furthermore, rejection sampling is
immediately applicable to multidimensional probability distributions.
However, depending on the profile of p(θ), the probability of accepting proposal
points can be small and then rejection sampling is computationally expensive. For
example, when p(θ) has a sharp profile, κ tends to be large and then the acceptance
rate is small (see Fig. 19.14). Furthermore, depending on the choice of proposal
distribution p′(θ), κ can be infinity and then rejection sampling is not applicable.
19.3.3 MARKOV CHAIN MONTE CARLO (MCMC)
METHOD
Rejection sampling cannot be used when the upper bound κ is unknown, while
MCMC methods do not have such a restriction. A stochastic process is a random

19.3 SAMPLING ALGORITHMS
215
(a) Efficient
(b) Less efficient
FIGURE 19.14
Computational efficiency of rejection sampling. (a) When the upper bound of the probability
density, κ, is small, proposal points are almost always accepted and thus rejection sampling is
computationally efficient. (b) When κ is large, most of the proposal points will be rejected and
thus rejection sampling is computationally expensive.
variable that changes over time, and a Markov chain θ1,. . . ,θn is a stochastic process
where sample θi at time i depends only on the previous sample θi−1.
Metropolis-Hastings sampling
Metropolis-Hastings sampling is a MCMC method [52, 72] that generates samples
following a Markov chain. Thus, the proposal distribution to draw the next sample
θi+1 depends on the current sample θi:
p′(θ|θi).
A proposal point θ′ generated from the above proposal distribution p′(θ|θi) is
evaluated by a uniform random variable v on [0,1]. More specifically, if
v ≤p(θ′)p′(θi|θ′)
p(θi)p′(θ′|θi),
then the proposal point θ′ is accepted and set
θi+1 = θ′.
Otherwise, the proposal point θ′ is rejected and set
θi+1 = θi.
Note the difference from rejection sampling that the previous value is used when the
proposal point is rejected. The initial value θ0 needs to be chosen by a user.

216
CHAPTER 19
NUMERICAL APPROXIMATION OF DISTRIBUTION
FIGURE 19.15
Random walk.
As a proposal distribution p′(θ|θi), the Gaussian distribution with expectation θi
and variance-covariance matrix σ2Ib may be used:
p′(θ|θi) =
1
(2πσ2)b/2 exp

−(θ −θi)⊤(θ −θi)
2σ2

,
where b denotes the dimensionality of θ. Since the Gaussian distribution is symmet-
ric, i.e.
p′(θ|θi) = p′(θi|θ),
the threshold for rejection is simplified as
p(θ′)
p(θi).
When a local probability distribution such as the Gaussian distribution is used
as the proposal distribution, the proposal point θ′ is generated around the previous
parameter θi, and therefore points θ1,θ2,. . . move gradually. Such a random sequence
is called a random walk (Fig. 19.15).
A MATLAB code for Metropolis-Hastings sampling is given in Fig. 19.16, and
its behavior is illustrated in Fig. 19.17.
Unlike rejection sampling, Metropolis-Hastings sampling can be used without
knowing the upper bound κ. Furthermore, even when the target probability density
p(θ) is not explicitly available, Metropolis-Hastings sampling can still be employed,
as long as p(θ) is known up to the normalization term. More specifically, suppose
that p(θ) is unknown, but its unnormalized counterpart p(θ) is available:
p(θ) = p(θ)
Z ,

19.3 SAMPLING ALGORITHMS
217
n=100000; t=zeros(2,n);
for i=2:n
u=randn(2,1)+t(:,i-1);
if rand<=pdf(u)/pdf(t(:,i-1))
t(:,i)=u;
else
t(:,i)=t(:,i-1);
end
end
figure(1); clf; hold on
plot(t(1,1:500),t(2,1:500),’b-’);
figure(2); clf; hold on
c=4; m=30; Q=zeros(m,m);
a=linspace(-c,c,m); b=-c+2*c/m*[1:m];
for k=1:n
x=find(ceil(t(1,k)-b)==0,1);
y=find(ceil(t(2,k)-b)==0,1);
Q(x,y)=Q(x,y)+1;
end
surf(a,a,Q’/(2*c/m)^2/n);
figure(3); clf; hold on
P=zeros(m,m);
for X=1:m, for Y=1:m
P(X,Y)=pdf([a(X);a(Y)]);
end, end
surf(a,a,P’);
function p=pdf(x)
S1=[0.5 0; 0 4]; m1=x-[0; -2]; w1=0.7;
S2=[4 0; 0 1]; m2=x-[2; 0]; w2=0.3;
p=w1*sqrt(det(S1))/(2*pi)*exp(-m1’*S1*m1/2) ...
+w2*sqrt(det(S2))/(2*pi)*exp(-m2’*S2*m2/2);
FIGURE 19.16
MATLAB code for Metropolis-Hastings sampling. The bottom function should be saved as
“pdf.m.”

218
CHAPTER 19
NUMERICAL APPROXIMATION OF DISTRIBUTION
(a) Trajectory of generated
samples (only first 500 sam-
ples are plotted)
(b) Histogram of generated samples
(c) True probability density
FIGURE 19.17
Example of Metropolis-Hastings sampling.
where
Z =

p(θ)dθ
is the normalization constant which is unknown. Even in this situation, the rejection
threshold can be computed as
p(θ′)
p(θi) = p(θ′)/Z
p(θi)/Z = p(θ′)
p(θi),
and therefore Metropolis-Hastings sampling can still be executed without any
modification.
However, in the same way as rejection sampling, Metropolis-Hastings sampling
is less efficient when the acceptance rate is low.
Gibbs sampling
Gibbs sampling is another MCMC method that generates samples in an elementwise
manner in multidimensional probability distributions [47].
More specifically, at time step i, for j = 1,. . . ,d, the proposal point of the
jth dimension, θ(j)
i , is generated from the proposal distribution that depends on the
previous sample θi−1 = (θ(1)
i−1,. . . ,θ(d)
i−1)⊤:
p(θ(j)|θ(1)
i ,. . . ,θ(j−1)
i
,θ(j+1)
i−1 ,. . . ,θ(d)
i−1).
Note that the above conditional density is different from
p(θ(j)|θ(1)
i−1,. . . ,θ(j−1)
i−1 ,θ(j+1)
i−1 ,. . . ,θ(d)
i−1),

19.3 SAMPLING ALGORITHMS
219
n=100000; r=0.5; s=sqrt(1-r^2); t=zeros(2,n);
for i=2:n
t(1,i)=s*randn+r*t(2,i-1); t(2,i)=s*randn+r*t(1,i);
end
figure(1); clf; hold on
u1=repmat(t(1,:),[2,1]); u1=u1(:);
u2=repmat(t(2,:),[2,1]); u2=u2(:);
plot(u1([2:200 200]),u2(1:200),’b-’);
figure(2); clf; hold on
c=4; m=30; Q=zeros(m,m);
a=linspace(-c,c,m); b=-c+2*c/m*[1:m];
for k=1:n
x=find(ceil(t(1,k)-b)==0,1);
y=find(ceil(t(2,k)-b)==0,1);
Q(x,y)=Q(x,y)+1;
end
surf(a,a,Q’/(2*c/m)^2/n);
figure(3); clf; hold on
P=zeros(m,m);
for X=1:m, for Y=1:m
P(X,Y)=exp(-(a(X)^2-2*r*a(X)*a(Y)+a(Y)^2)/2/s)/(2*pi*s);
end, end
surf(a,a,P’);
FIGURE 19.18
MATLAB code for Gibbs sampling.
i.e. not all values of the previous sample θi−1 are used, but the new values
θ(1)
i ,. . . ,θ(j−1)
i
are used up to the (j −1)th dimension.
Let us illustrate a more specific algorithm of Gibbs sampling for the
two-dimensional Gaussian distribution:
p(θ) = N(θ; 02,Σρ),
where, for −1 < ρ < 1,
Σρ = *.
,
1 ρ
ρ 1
+/
-
.
For θ = (θ(1),θ(2))⊤, the conditional density is given by
p(θ(1)|θ(2)) = N(θ(1); ρθ(2),1 −ρ2).

220
CHAPTER 19
NUMERICAL APPROXIMATION OF DISTRIBUTION
(a) Trajectory of generated
samples (only first 100 sam-
ples are plotted)
(b) Histogram of generated samples
(c) True probability density
FIGURE 19.19
Example of Gibbs sampling.
A MATLAB code of Gibbs sampling for the above Gaussian distribution is given in
Fig. 19.18, and its behavior is illustrated in Fig. 19.19.
A variant of Gibbs sampling, which generates samples not only in a dimen-
sionwise manner but also for some dimensions together, is called blocked Gibbs
sampling [95]. Another variant that marginalizes some of the conditioning variables
θ(1)
i ,. . . ,θ(j−1)
i
,θ(j+1)
i−1 ,. . . ,θ(d)
i−1 is called collapsed Gibbs sampling [68].
Since Gibbs sampling does not involve proposal distributions and rejection, it
is computationally efficient. However, it requires sampling from the conditional
distribution, which may not always be straightforward in practice.
Discussions
Samples {θi}n
i=1 generated by rejection sampling are guaranteed to be mutually
independent and have density p(θ) for finite n. On the other hand, samples {θi}n
i=1
generated by MCMC methods are generally dependent on each other due to the
incremental nature of Markov chains. Moreover, samples generated in an early stage
may depend on the choice of initial value θ0. For these reasons, samples generated
by MCMC methods have density p(θ) only in the limit n →∞.
To mitigate these problems, it is common to discard samples generated in an early
stage to decrease the dependency on the initial value (which is often referred to as
burn-in) and adopt samples of only every m time steps to weaken mutual dependency
between samples.

CHAPTER
BAYESIAN MIXTURE
MODELS
20
CHAPTER CONTENTS
Gaussian Mixture Models......................................................... 221
Bayesian Formulation ...................................................... 221
Variational Inference ....................................................... 223
Gibbs Sampling ............................................................ 228
Latent Dirichlet Allocation (LDA) ................................................. 229
Topic Models ............................................................... 230
Bayesian Formulation ...................................................... 231
Gibbs Sampling ............................................................ 232
In this chapter, within the Bayesian inference framework explained in Chapter 17,
practical Bayesian inference algorithms for mixture models are presented. First,
algorithms based on variational approximation and Gibbs sampling for Gaussian
mixture models are introduced. Then a variational Bayesian algorithm for topic
models is explained.
20.1 GAUSSIAN MIXTURE MODELS
In this section, Bayesian inference algorithms for Gaussian mixture models (see
Section 15.1) are introduced.
20.1.1 BAYESIAN FORMULATION
Let us consider the mixture of m Gaussian models:
q(x|W,M,S) =
m

ℓ=1
wℓN(x|µℓ,S−1
ℓ),
where W = (w1,. . . ,wm), M = (µ1,. . . , µm), S = (S1,. . . ,Sm), and N(x|µ,S−1)
denotes the Gaussian density with expectation µ and variance-covariance matrix S−1
(or precision matrix S):
N(x|µ,S−1) =

det(S)
(2π)d/2 exp

−1
2(x −µ)⊤S(x −µ)

.
(20.1)
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00031-5
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
221

222
CHAPTER 20
BAYESIAN MIXTURE MODELS
For training samples D = {x1,. . . , xn} drawn independently from the true density
p(x), the likelihood p(D|W,M,S) is given by
p(D|W,M,S) =
n

i=1
q(xi|W,M,S).
For mixing weights W, the symmetric Dirichlet distribution (see Section 6.3) is
considered as the prior probability:
p(W; α0) = Dir(W; α0) ∝
m

ℓ=1
wα0−1
ℓ
,
where Dir(W; α) denotes the symmetric Dirichlet density with concentration param-
eter α. Note that the Dirichlet distribution is conjugate (see Section 17.2) for the
discrete distribution given by Eq. (20.3).
For Gaussian expectations M and Gaussian precision matrices S, the product
of the normal distribution and the Wishart distribution (see Section 6.4), called the
normal-Wishart distribution, is considered as the prior probability:
p(M,S; β0,W0,ν0)
=
m

ℓ=1
N(µℓ|0,(β0Sℓ)−1)W(Sℓ; W0,ν0)
∝
m

ℓ=1
det(Sℓ)
ν0−d
2
−1 exp

−β0
2 µ⊤
ℓSℓµℓ−1
2tr

W −1
0 Sℓ

,
where W(S; W,ν) denotes the Wishart density with ν degrees of freedom:
W(S; W,ν) = det(S)
ν−d−1
2
exp  −1
2tr  W −1S
det(2W)
ν
2 Γd(ν
2)
.
Here, d denotes the dimensionality of input x and Γd(·) denotes the d-dimensional
gamma function defined by Eq. (6.2):
Γd
ν
2

=

S+
d
det(S)
ν−d−1
2
exp  −tr(S)dS,
where S+
d denotes the set of all d×d positive symmetric matrices. Note that the above
normal-Wishart distribution is conjugate for the multivariate normal distribution with
unknown expectation and unknown precision matrix.
The above formulation is summarized in Fig. 20.1. By the Bayes’ theorem, the
posterior probability p(W,M,S|D) is given as
p(W,M,S|D; α0, β0,W0,ν0)
= p(D|W,M,S)p(W; α0)p(M,S; β0,W0,ν0)
p(D; α0, β0,W0,ν0)
,
(20.2)
which is not computationally tractable. Below, practical approximate inference
methods are introduced.

20.1 GAUSSIAN MIXTURE MODELS
223
FIGURE 20.1
Variational Bayesian formulation of Gaussian mixture model.
To derive approximate inference methods, let us consider latent variables,
Z = {z1,. . . , zn},
for training samples D = {x1,. . . , xn}, where each zi = (zi,1,. . . , zi,m)⊤is an
m-dimensional vector that indicates mixture component selection. Specifically, if the
kth component is selected, the kth element of zi is 1 and all other elements are 0.
The probability of observing Z given mixing weights W = (w1,. . . ,wm) can be
expressed as
p(Z|W) =
n

i=1
m

ℓ=1
wzi,ℓ
ℓ
.
(20.3)
20.1.2 VARIATIONAL INFERENCE
Here, the variational technique introduced in Section 18.2 is employed to approxi-
mately compute the posterior probability p(W,M,S|D) given by Eq. (20.2).
The marginal likelihood can be expressed as
ML(α0, β0,W0,ν0)
= p(D; α0, β0,W0,ν0)
=

p(D,Z,W,M,S; α0, β0,W0,ν0)dZdWdMdS
=

p(D|Z,M,S)p(Z|W)p(W; α0)
× p(M,S; β0,W0,ν0)dZdWdMdS.

224
CHAPTER 20
BAYESIAN MIXTURE MODELS
For the above marginal likelihood, let us consider the following trial distributions:
q(Z)r(W,M,S).
Then, from Eq. (18.4), the VB-E step is given by
q(Z) ∝exp

r(W,M,S) log p(D,Z|W,M,S)dWdMdS

= exp

r(W,M,S) log p(D|Z,M,S)dWdMdS
+

r(W,M,S) log p(Z|W)dWdMdS

.
(20.4)
Similarly, from Eq. (18.5), the VB-M step is given by
r(W,M,S) ∝p(W,M,S; α0, β0,W0,ν0)
× exp

q(Z) log p(D,Z|W,M,S)dZ

= p(W; α0) exp

q(Z) log p(Z|W)dZ

× p(M,S; β0,W0,ν0) exp

q(Z) log p(D|Z,M,S)dZ

. (20.5)
Combining Eq. (20.4) and Eq. (20.5) yields the VBEM algorithm described in
Fig. 20.2 (see Section 10.2.1 of reference [15] for details).
As mentioned in Section 18.2.1, r(W,M,S) obtained by the VBEM algorithm
may be a good approximation to the posterior probability p(W,M,S|D):
r(W,M,S) = Dir(W|α)
m

ℓ=1
N(µℓ|hℓ,(βℓSℓ)−1)W(Sℓ|
Wℓ,νℓ).
The expectations of the marginals of r(W,M,S) can be obtained as
wℓ=
αℓ
m
ℓ′=1 αℓ′ ,
µℓ= hℓ,
and
Sℓ= νℓ
Wℓ,
which may be used as the most plausible parameter values. Then the following
density estimator is obtained:
p(x) =
m

ℓ=1
wℓN(x|µℓ, S−1
ℓ).
A MATLAB code for computing this VBEM-based density estimator is given
in Fig. 20.3, and its behavior is illustrated in Fig. 20.4. Here, the mixture model
of five Gaussian components is fitted to the mixture of two Gaussian distributions.
As shown in Fig. 20.4, two out of five Gaussian components fit the true two
Gaussian distributions well, and the remaining three Gaussian components are almost
eliminated—the learned mixing coefficients are given as

20.1 GAUSSIAN MIXTURE MODELS
225
1. Initialize parameters {αℓ,hℓ, βℓ, 
Wℓ,νℓ}m
ℓ=1.
2. VB-E step: Compute the distribution of Z = {z1,. . . , zn} from
current solution {αℓ,hℓ, βℓ, 
Wℓ,νℓ}m
ℓ=1:
q(Z) =
n

i=1
m

ℓ=1
ηzi,ℓ
i,ℓ,
where ηi,ℓ=
ρi,ℓ
m
ℓ′=1 ρi,ℓ′ .
{ηi,ℓ}n
i=1,
m
ℓ=1 are the responsibilities computed as
ρi,ℓ= exp

ψ(αℓ) −ψ *
,
m

ℓ′=1
αℓ′+
-
+ 1
2
d

j=1
ψ
νℓ+ 1 −j
2

+ 1
2 log det(
Wℓ) −
d
2βℓ
−νℓ
2 (xi −hℓ)⊤
Wℓ(xi −hℓ)

,
where ψ(α) denotes the digamma function defined as the log-
derivative of the gamma function Γ(α):
ψ(α) = d
dα log Γ(α) = Γ′(α)
Γ(α) .
3. VB-M step: Compute the joint distribution of W = (w1,. . . ,wm),
M = (µ1,. . . , µm), and S = (S1,. . . ,Sm) from current responsibili-
ties {ηi,ℓ}n
i=1,
m
ℓ=1:
r(W,M,S) = Dir(W|α)
m

ℓ=1
N(µℓ|hℓ,(βℓSℓ)−1)W(Sℓ|
Wℓ,νℓ),
where
γℓ=
n

i=1
ηi,ℓ,
cℓ= 1
γℓ
n

i=1
ηi,ℓxi,
hℓ= γℓ
βℓ
cℓ,
αℓ= α0 + γℓ,
βℓ= β0 + γℓ,
νℓ= ν0 + γℓ,
and

Wℓ=

W −1
0
+
n

i=1
ηi,ℓ(xi −cℓ)(xi −cℓ)⊤+
β0γℓ
β0 + γℓ
cℓc⊤
ℓ
−1
.
4. Iterate 2–3 until convergence.
FIGURE 20.2
VBEM algorithm for Gaussian mixture model. (α0, β0,W0,ν0) are hyperparameters.

226
CHAPTER 20
BAYESIAN MIXTURE MODELS
x=[2*randn(1,100)-5 randn(1,50); randn(1,100) randn(1,50)+3];
[d,n]=size(x); m=5; e=rand(n,m); W=zeros(d,d,m); b0=1;
for o=1:10000
e=e./repmat(sum(e,2),[1 m]);
g=sum(e); a=1+g; b=b0+g; nu=3+g; w=a/sum(a);
xe=x*e; c=xe./repmat(g,[d 1]); h=xe./repmat(b,[d 1]);
for k=1:m
t1=x-repmat(c(:,k),[1 n]); t2=x-repmat(h(:,k),[1 n]);
W(:,:,k)=inv(eye(d)+(t1.*repmat(e(:,k)’,[d 1]))*t1’ ...
+c(:,k)*c(:,k)’*b0*g(k)/(b0+g(k)));
t3=sum(psi((nu(k)+1-[1:d])/2))+log(det(W(:,:,k)));
e(:,k)=exp(t3/2+psi(a(k))-psi(sum(a))-d/2/b(k) ...
-sum(t2.*(W(:,:,k)*t2))*nu(k)/2);
end
if o>1 && norm(w-w0)+norm(h-h0)+norm(W(:)-W0(:))<0.001
break
end
w0=w; h0=h; W0=W;
end
figure(1); clf; hold on
plot(x(1,:),x(2,:),’ro’); v=linspace(0,2*pi,100);
for k=1:m
[V,D]=eig(nu(k)*W(:,:,k));
X=3*w(k)*V’*[cos(v)/D(1,1); sin(v)/D(2,2)];
plot(h(1,k)+X(1,:),h(2,k)+X(2,:),’b-’)
end
FIGURE 20.3
MATLAB code of VBEM algorithm for Gaussian mixture model.
(w1, w2, w3, w4, w5) = (0.01,0.33,0.01,0.01,0.65).
As explained in Section 18.2.1, the negative of the variational free energy,
F(q,r) =

q(Z)r(W,M,S)
× log
q(Z)r(W,M,S)
p(D,Z,W,M,S; α0, β0,W0,ν0)dZdWdMdS,
gives a lower bound of the log marginal likelihood, ML(α0, β0,W0,ν0). Based on the
dependency in Fig. 20.1, the variational free energy can be expressed as

20.1 GAUSSIAN MIXTURE MODELS
227
(a) After 1 iteration
(b) After 5 iterations
(c) After 10 iterations
(d) After 15 iterations
(e) After 20 iterations
(f) After 58 iterations
FIGURE 20.4
Example of VBEM algorithm for Gaussian mixture model. The size of ellipses is proportional
to the mixing weights {wℓ}m
ℓ=1. A mixture model of five Gaussian components is used here, but
three components have mixing coefficient close to zero and thus they are almost eliminated.
F(q,r) =

q(Z)r(W,M,S)

log q(Z) + log r(W,M,S)
−log p(D|Z,M,S) −log p(Z|W) −log p(W; α0)
−log p(M,S; β0,W0,ν0)

dZdWdMdS,

228
CHAPTER 20
BAYESIAN MIXTURE MODELS
which allows us to choose the hyperparameters and the number of mixing com-
ponents by the empirical Bayes method (see Section 17.4). See Section 10.2.2 of
reference [15] for more details of computing the variational free energy for Gaussian
mixture models.
20.1.3 GIBBS SAMPLING
Next, the Gibbs sampling technique introduced in Section 19.3.3 is employed to
numerically approximate the posterior probability
p(W,M,S|D) given by
Eq. (20.2).
Collapsed Gibbs sampling for assignment zi can be performed as follows [74]:
p(zi,ℓ=1| 
Z,D; α0, β0,W0,ν0)
∝(αℓ−1) × t *
,
xicℓ,
βℓ+ 1
βℓ(νℓ−d + 1)

W −1
ℓ,νℓ−d + 1+
-
,
(20.6)
where 
Z = {zi′}i′,i, αℓ= α0 + nℓ, βℓ= β0 + nℓ, νℓ= ν0 + nℓ,

W −1
ℓ
= *.
,
W0 +

i:zi,ℓ=1
xi x⊤
i −
n2
ℓ
βℓ
cℓc⊤
ℓ+/
-
−1
,
nℓ= n
i=1 zi,ℓ, cℓ=
1
nℓ
n
i:zi,ℓ=1 xi, and t(x|µ,Σ,ν) denotes the probability density
function of the multivariate Student’s t-distribution:
t(x|µ,Σ,ν) =
Γ
ν+d
2

Γ(ν
2)(νπ)d/2det(Σ)1/2

1 + 1
ν (x −µ)⊤Σ−1(x −µ)
−ν+d
2
.
After estimating assignment Z, parameters W,M,S for each Gaussian component
may be estimated separately as
wℓ=
αℓ
m
ℓ′=1 αℓ′ ,
µℓ= nℓ
βℓ
cℓ,
and
Sℓ= νℓ
Wℓ.
A MATLAB code of collapsed Gibbs sampling for Gaussian mixture models is
given in Fig. 20.5, and its behavior is illustrated in Fig. 20.6. A mixture model of five
Gaussian components is used here, but only two components remain and no samples
belong to the remaining three components after Gibbs sampling.
The Dirichlet process, denoted by DP(α0,p0), is a probability distribution of
probability distributions, and a Dirichlet distribution is generated from a Dirichlet
process. p0 is called the base distribution which corresponds to the mean of
the Dirichlet distribution, while α0 is called the concentration parameter which
corresponds to the precision (i.e., the inverse variance) of the Dirichlet distribution.
The Dirichlet process mixture is a mixture model using the Dirichlet process as a
prior probability and is equivalent to considering an infinite-dimensional Dirichlet
distribution. Indeed, taking the limit m →∞for α0 = α′
0/m in Eq. (20.6) gives a
collapsed Gibbs sampling procedure for the Dirichlet process mixture model.

20.2 LATENT DIRICHLET ALLOCATION (LDA)
229
x=[2*randn(1,100)-5 randn(1,50); randn(1,100) randn(1,50)+3];
[d,n]=size(x); m=5; z=mod(randperm(n),m)+1;
a0=1; b0=1; n0=1; W0=eye(d);
for o=1:100
for i=1:n
g=(1:n~=i); X=x(:,g); Z=z(g);
for k=1:m
p(k)=0; e=(Z==k); t=sum(e);
if t~=0
u=n0+t-d+1; b=b0+t; c=sum(X(:,e),2); xi=x(:,i)-c/t;
W=inv((b+1)/b/u*(W0+X(:,e)*X(:,e)’-c*c’/b));
p(k)=(a0+t-1)*gamma((u+d)/2)/gamma(u/2)*u^(-d/2) ...
*sqrt(det(W))*(1+xi’*W*xi/u)^(-(u+d)/2);
end, end
z(i)=find(cumsum(p/sum(p))>rand,1);
end, end
figure(1); clf; hold on
plot(x(1,:),x(2,:),’ro’); v=linspace(0,2*pi,100);
for k=1:m
e=(z==k); t=sum(e); nu(k)=n0+t; u=nu(k)-d+1; b=b0+t;
c=sum(x(:,e),2); w(k)=a0+t; h(:,k)=c/b; W(:,:,k)=zeros(d);
if t~=0
W(:,:,k)=inv((W0+x(:,e)*x(:,e)’-c*c’/b));
end
end
w=w./sum(w);
for k=1:m
[V,D]=eig(nu(k)*W(:,:,k));
X=3*w(k)*V’*[cos(v)/D(1,1); sin(v)/D(2,2)];
plot(h(1,k)+X(1,:),h(2,k)+X(2,:),’b-’)
end
FIGURE 20.5
MATLAB code of collapsed Gibbs sampling for Gaussian mixture model.
20.2 LATENT DIRICHLET ALLOCATION (LDA)
One of the most successful applications of the Bayesian generative approach is topic
modeling called LDA [16]. In this section, an implementation of LDA based on Gibbs
sampling is explained [51].

230
CHAPTER 20
BAYESIAN MIXTURE MODELS
(a) After 1 iteration
(b) After 5 iterations
(c) After 10 iterations
(d) After 20 iterations
(e) After 30 iterations
(f) After 100 iterations
FIGURE 20.6
Example of collapsed Gibbs sampling for Gaussian mixture model. A mixture model of five
Gaussian components is used here, but only two components remain and no samples belong to
the remaining three components.
20.2.1 TOPIC MODELS
A topic model is a generative model for a set of D documents,
W = {w1,. . . ,wD},

20.2 LATENT DIRICHLET ALLOCATION (LDA)
231
where the dth document wd = (wd,1,. . . ,wd,nd) is a sequence of nd words in a
corpus with W unique words. More specifically, each word wn,d ∈{1,. . . ,W} is
supposed to be given by the mixture of T topics as
p(wn,d|θ(d),φ(1),. . . ,φ(T)) =
T

tn,d=1
p(tn,d|θ(d))p(wn,d|tn,d,φ(tn,d)),
where tn,d ∈{1,. . . ,T} is a latent variable that indicates the topic of wn,d. For topic
t = 1,. . . ,T, the probability,
p(t|θ(d)) = θ(d)
t
≥0,
denotes the discrete probability of topic t with parameter θ(d) = (θ(d)
1 ,. . . ,θ(d)
T )⊤such
that T
t=1 θ(d)
t
= 1. For w = 1,. . . ,W, the probability,
p(w|t,φ(t)) = ϕ(t)
w ≥0,
denotes the discrete probability of observing word w under topic t with parameter
φ(t) = (ϕ(t)
1 ,. . . ,ϕ(t)
W )⊤such that W
w=1 ϕ(t)
w = 1. This generative model means that,
for each word w in document d, topic t is chosen following the discrete probability
with parameter θ(d) and then word w is chosen following the discrete probability with
parameter φ(t).
The goal of topic modeling is to specify the multinomial parameters θ(d) and φ(t)
from data W.
20.2.2 BAYESIAN FORMULATION
Here, instead of directly learning the parameters θ(d) and φ(t), let us consider the
posterior probability of the set T of all topics for all documents W:
T = {t1,. . . , tD},
where td = (td,1,. . . ,td,nd).
For the multinomial parameters θ(d) and φ(t), the symmetric Dirichlet distribu-
tions (see Section 6.3) are considered as the prior probabilities:
p(θ(d); α) = Dir(θ(d); α) and p(φ(t); β) = Dir(φ(t); β),
where Dir(· ; α) denotes the symmetric Dirichlet density with concentration parame-
ter α. Note that the Dirichlet distributions are conjugate (see Section 17.2) for discrete
distributions.
The posterior probability of topics T is given by
p(T |W; α, β) =
p(W,T ; α, β)

T p(W,T ; α, β),
(20.7)

232
CHAPTER 20
BAYESIAN MIXTURE MODELS
where the marginal likelihood p(W,T ; α, β) can be expressed analytically as
p(W,T ; α, β)
=
D

d=1
nd

n=1

θ(d)
tn,dDir(θ(d); α)dθ(d)

ϕ(tn,d)
wn,d Dir(φ(tn,d); β)dφ(tn,d)
=
Γ(Tα)
Γ(α)T
D
D

d=1
T
t=1 Γ(n(d)
t
+ α)
Γ(T
t=1 n(d)
t
+ Tα)
Γ(W β)
Γ(β)W
T
T

t=1
W
w=1 Γ(n(w)
t
+ β)
Γ(W
w=1 n(w)
t
+ W β)
,
where n(d)
t
denotes the number of times a word from document d is assigned to topic
t, n(w)
t
denotes the number of times word w is assigned to topic t, and Γ(·) denotes
the gamma function (see Section 4.3).
20.2.3 GIBBS SAMPLING
Let us use Gibbs sampling (see Section 19.3.3) to approximate the posterior
probability p(T |W; α, β).
Let 
W and 
T be defined in the same way as W and T , but wn,d and tn,d are
excluded. Then the conditional probability needed for Gibbs sampling is given by
p(tn,d| 
T ,W) = p(tn,d| 
T ,wn,d, 
W)
∝p(tn,d| 
T )p(wn,d|tn,d, 
T , 
W).
(20.8)
The first term in Eq. (20.8) can be expressed as
p(tn,d| 
T ) =

p(tn,d|θ(d))p(θ(d)| 
T )dθ(d),
(20.9)
where Bayes’ theorem yields
p(θ(d)| 
T ) ∝p( 
T |θ(d))p(θ(d); α).
Here, the prior probability p(θ(d); α) is chosen to be symmetric Dirichlet distribution
with concentration parameter α, which is conjugate for the discrete distribution
p( 
T |θ(d)). Then the posterior probability p(θ(d)| 
T ) is also the symmetric Dirichlet
distribution with concentration parameter n(d)
tn,d + α, where n(d)
tn,d is defined in the
same way as n(d)
tn,d, but wn,d and tn,d are excluded. Then Eq. (20.9) can be computed
analytically as
p(tn,d| 
T ) =

θ(d)
tn,dDir(θ(d);n(d)
tn,d + α)dθ(d) =
n(d)
tn,d + α
T
t=1 n(d)
t
+ Tα
.
Similarly, the second term in Eq. (20.8) can be expressed as
p(wn,d|tn,d, 
T , 
W) =

p(wn,d|tn,d,φ(tn,d))p(φ(tn,d)| 
T , 
W)dφ(tn,d),
(20.10)

20.2 LATENT DIRICHLET ALLOCATION (LDA)
233
where Bayes’ theorem yields
p(φ(tn,d)| 
T , 
W) ∝p( 
W|φ(tn,d), 
T )p(φ(tn,d); β).
Here, the prior probability p(φ(tn,d); β) is chosen to be the symmetric Dirichlet
distribution with concentration parameter β, which is conjugate for the discrete
distribution p( 
W|φ(tn,d), 
T ). Then the posterior probability p(φ(tn,d)| 
T , 
W) is also
the symmetric Dirichlet distribution with concentration parameter n(wn,d)
tn,d
+ β, where
n(wn,d)
tn,d
is defined in the same way as n(wn,d)
tn,d
, but wn,d and tn,d are excluded. Then
Eq. (20.10) can be computed analytically as
p(wn,d|tn,d, 
T , 
W) =

ϕ(tn,d)
wn,d Dir(φ(tn,d);n(wn,d)
tn,d
+ β)dφ(tn,d)
=
n(wn,d)
tn,d
+ β
W
w=1 n(w)
tn,d + W β
.
Summarizing the above equations, Eq. (20.8) can be expressed as
p(tn,d| 
T ,W) ∝
n(d)
tn,d + α
T
t=1 n(d)
t
+ Tα
×
n(wn,d)
tn,d
+ β
W
w=1 n(w)
tn,d + W β
,
which shows that Gibbs sampling can be efficiently performed just by counting the
number of words.
Finally, with estimated topics 
T , the solutions θ(d)
t
and ϕ(w)
t
for any w ∈
{1,. . . ,W}, t ∈{1,. . . ,T}, and d ∈{1,. . . , D} can be computed as
θ(d)
t
=
n(d)
tn,d + α
T
t=1 n(d)
t
+ Tα
and
ϕ(w)
t
=
n(wn,d)
tn,d
+ β
W
w=1 n(w)
tn,d + W β
.


PART
DISCRIMINATIVE
APPROACH TO
STATISTICAL
MACHINE
LEARNING
4
21
LEARNING MODELS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
22
LEAST SQUARES REGRESSION. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
23
CONSTRAINED LS REGRESSION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
24
SPARSE REGRESSION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
25
ROBUST REGRESSION. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
26
LEAST SQUARES CLASSIFICATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
27
SUPPORT VECTOR CLASSIFICATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
28
PROBABILISTIC CLASSIFICATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
29
STRUCTURED CLASSIFICATION. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329

236
PART 4
STATISTICAL MACHINE LEARNING: DISCRIMINATIVE APPROACH
As discussed in Chapter 11, the problem of statistical pattern recognition is
formulated as the problem of estimating the class-posterior probability p(y|x). In the
generative approach explored in Part 3, the problem of estimating the class-posterior
probability p(y|x) is replaced with the problem of estimating the joint probability
p(x, y) based on the following equality:
argmax
y
p(y|x) = argmax
y
p(x, y).
Generative model estimation is the most general approach in statistical machine
learning, because knowing the data generating model is equivalent to knowing
everything about the data. Therefore, any kind of data analysis is possible through
generative model estimation.
When a good parametric model is available, maximum likelihood estimation or
Bayesian methods will be highly useful for generative model estimation (see Part 3).
However, without strong prior knowledge on parametric models, estimating the
generative model is statistically a hard problem. Non-parametric methods introduced
in Chapter 16 could be used if prior knowledge on generative models is not available,
but non-parametric methods tend to perform poorly when the dimensionality of data
is not small.
In Part 4, an alternative approach to generative model estimation called the
discriminative approach is explored. In the discriminative approach, the class-
posterior probability p(y|x) is directly modeled. More specifically, p(y|x) is regarded
as the sum of a function f (x) and some noise, and the problem of function
approximation from input–output paired samples {(xi, yi)}n
i=1 is considered. Such
a problem formulation is called supervised learning, where output y is regarded
as supervision from the (noisy) oracle. The supervised learning problem is called
regression if the output y is continuous, and is called classification if the output y is
categorical.
After introducing standard function models used for regression and classification
in Chapter 21, various regression and classification techniques will be introduced.
The most fundamental regression technique called the least squares method is intro-
duced in Chapter 22, its constraint (or regularized) variants for avoiding overfitting
are introduced in Chapter 23. In Chapter 24 and Chapter 25, more advanced regres-
sion techniques considering sparsity and robustness will be discussed, respectively.
In Chapter 26, it is shown that the least squares regression method can also be
used for classification, and various issues specific to classification will be discussed.
In Chapter 27, a powerful classification algorithm based on the maximum margin
principle called the support vector machine and its robust variant is introduced.
In Chapter 28, a probabilistic pattern recognition method that directly learns the
class-posterior probability p(y|x) called logistic regression is introduced. Finally, in
Chapter 29, classification of sequence data is discussed.

CHAPTER
LEARNING MODELS 21
CHAPTER CONTENTS
Linear-in-Parameter Model ....................................................... 237
Kernel Model .................................................................... 239
Hierarchical Model ............................................................... 242
Through Part 4, let us consider the supervised learning problem of approximating
a function y = f (x) from its input-output paired training samples {(xi, yi)}n
i=1 (see
Section 1.2.1). The input x is assumed to be a real-valued d-dimensional vector, and
the output y is a real scalar in the case of regression and a categorical value {1,. . . ,c}
in the case of classification, where c denotes the number of classes. In this chapter,
various models for supervised learning are introduced.
21.1 LINEAR-IN-PARAMETER MODEL
For simplicity, let us begin with a one-dimensional learning target function f . The
simplest model for approximating f would be the linear-in-input model θ × x. Here,
θ denotes a scalar parameter and the target function is approximated by learning the
parameter θ. Although the linear-in-input model is mathematically easy to handle, it
can only approximate a linear function (i.e., a straight line) and thus its expression
power is limited (Fig. 21.1).
The linear-in-parameter model is an extension of the linear-in-input model that
allows approximation of nonlinear functions:
fθ(x) =
b

j=1
θ jϕj(x),
where ϕj(x) and θ j are a basis function and its parameter, respectively, and b denotes
the number of basis functions. The linear-in-parameter model may be compactly
expressed as
fθ(x) = θ⊤φ(x),
where
φ(x) = (ϕ1(x),. . . ,ϕb(x))⊤,
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00032-7
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
237

238
CHAPTER 21
LEARNING MODELS
FIGURE 21.1
Linear-in-input model cannot approximate
nonlinear functions.
θ = (θ1,. . . ,θb)⊤,
and ⊤denotes the transpose. The linear-in-parameter is still linear in terms of
parameters θ, but it can express nonlinear functions, e.g., by using polynomial
functions
φ(x) = (1, x, x2,. . . , xb−1)⊤,
or sinusoidal functions for b = 2m + 1
φ(x) = (1,sin x,cos x,sin 2x,cos 2x,. . . ,sin mx,cos mx)⊤,
as basis functions.
The linear-in-parameter can be naturally extended to d-dimensional input vector
x = (x(1),. . . , x(d))⊤as
fθ(x) =
b

j=1
θ jϕj(x) = θ⊤φ(x).
(21.1)
Below, methods for constructing multidimensional basis functions from one-
dimensional basis functions are discussed.
The multiplicative model expresses multidimensional basis functions by the
product of one-dimensional basis functions as
fθ(x) =
b′

j1=1
· · ·
b′

jd=1
θ j1,..., jd ϕj1(x(1)) · · · ϕjd(x(d)),
where b′ denotes the number of parameters in each dimension. Since all possible
combinations of one-dimensional basis functions are considered in the multiplicative
model, it can express complicated functions, as illustrated in Fig. 21.2(a). However,

21.2 KERNEL MODEL
239
(a) Multiplicative model
(b) Additive model
FIGURE 21.2
Multidimensional basis functions. The multiplicative model is expressive, but the number of
parameters grows exponentially in input dimensionality. On the other hand, in the additive model,
the number of parameters grows only linearly in input dimensionality, but its expression power is
limited.
the total number of parameters is (b′)d, which grows exponentially as input dimen-
sionality d increases. For example, when b′ = 10 and d = 100, the total number of
parameters is given by
10100 = 1 000 · · · 000
              
100
,
which is an astronomical number having 100 zeros after the first one and cannot
be handled in computers. Such exponential growth in input dimensionality is often
referred to as the curse of dimensionality.
The additive model expresses multidimensional basis functions by the sum of
one-dimensional basis functions as
fθ(x) =
d

k=1
b′

j=1
θk, jϕj(x(k)).
In the additive model, the total number of parameters is b′d, which grows only
linearly as input dimensionality d increases. For example, when b′ = 10 and d = 100,
the total number of parameters is 10 ×100 = 1000 and thus it can be easily handled by
standard computers. However, since only the sum of one-dimensional basis functions
is considered in the additive model, it cannot express complicated functions, as
illustrated in Fig. 21.2(b).
21.2 KERNEL MODEL
In the linear-in-parameter model introduced above, basis functions are fixed to, e.g.,
polynomial functions or sinusoidal functions without regard to training samples

240
CHAPTER 21
LEARNING MODELS
FIGURE 21.3
Gaussian kernel with bandwidth h and center c.
{(xi, yi)}n
i=1. In this section, the kernel model is introduced, which uses training input
samples {xi}n
i=1 for basis function design.
Let us consider a bivariate function called the kernel function K(·,·). The kernel
model is defined as the linear combination of {K(x, xj)}n
j=1:
fθ(x) =
n

j=1
θ jK(x, xj).
(21.2)
As a kernel function, the Gaussian kernel would be the most popular choice:
K(x, c) = exp

−∥x −c∥2
2h2

,
where ∥· ∥denotes the ℓ2-norm:
∥x∥=
√
x⊤x.
h and c are, respectively, called the Gaussian bandwidth and the Gaussian center
(see Fig. 21.3).
In the Gaussian kernel model, Gaussian functions are located at training input
samples {xi}n
i=1 and their height {θi}n
i=1 is learned (Fig. 21.4). Since the Gaussian
function is local around its center, the Gaussian kernel model can approximate the
learning target function only in the vicinity of training input samples (Fig. 21.5).
This is quite different from the multiplicative model which tries to approximate the
learning target function over the entire input space.
In the kernel model, the number of parameters is given by the number of training
samples, n, which is independent of the dimensionality d of the input variable x.
Thus, even when d is large, the kernel model can be easily handled in standard
computers, as long as n is not too large. Even if n is very large, only a subset {cj}b
j=1

21.2 KERNEL MODEL
241
FIGURE 21.4
One-dimensional Gaussian kernel model. Gaussian functions are located at training input samples
{xi}n
i=1 and their height {θi}n
i=1 is learned.
FIGURE 21.5
Two-dimensional Gaussian kernel model. The curse of dimensionality is mitigated by only
approximating the learning target function in the vicinity of training input samples.
of training input samples {xi}n
i=1 may be used as kernel centers for reducing the
computation costs:
fθ(x) =
b

j=1
θ jK(x, cj).

242
CHAPTER 21
LEARNING MODELS
FIGURE 21.6
Sigmoidal function.
Since kernel model (21.2) is linear in terms of the parameter θ = (θ1,. . . ,
θn)⊤, it can also be regarded as a linear-in-parameter model. However, basis functions
{K(x, xj)}n
j=1 depend on training input samples {xi}n
i=1 and the number of basis
function grows as the number of training samples increases. For this reason, in
statistics, the kernel model is categorized as a nonparametric model and is clearly
differentiated from linear-in-parameter models which are parametric. However,
within the scope of Part 4, the kernel model may be treated almost in the same way
as the linear-in-parameter models.
Another important advantage of the kernel model is that it can be easily extended
to nonvectorial x such as a sequence (with different length) and a tree (with different
depth) [46]. More specifically, since input x only appears in the kernel function
K(x, x′) in kernel model (21.2), the expression of x itself does not matter as long
as the kernel function K(x, x′) for two input objects x and x′ is defined.
Machine learning with kernel functions is called the kernel method and has been
studied extensively [89].
21.3 HIERARCHICAL MODEL
A model that is nonlinear in terms of parameters is referred to as a nonlinear model.
Among nonlinear models, a three-layer hierarchical model is a popular choice:
fθ(x) =
b

j=1
αjϕ(x; βj),
where ϕ(x; β) is a basis function parameterized by β. A hierarchical model is linear
in terms of parameter α = (α1,. . . ,αb)⊤, as in Eq. (21.1). However, in terms of the
basis parameters {βj}b
j=1, the hierarchical model is nonlinear.

21.3 HIERARCHICAL MODEL
243
FIGURE 21.7
Hierarchical model as a three-layered network.
As a basis function, the sigmoidal function (see Fig. 21.6),
ϕ(x; β) =
1
1 + exp (−x⊤w −γ),
β = (w⊤,γ)⊤,
and the Gaussian function (see Fig. 21.3),
ϕ(x; β) = exp

−∥x −c∥2
2h2

,
β = (c⊤,h)⊤,
are standard choices. The sigmoidal function is motivated by the activity of neurons
in human’s brain. For this reason, a hierarchical model is also called a neural network
and a basis function is called an activation function. To reduce the computation cost,
the rectified linear function,
ϕ(x; β) = max(x⊤w + γ,0),
β = (w⊤,γ)⊤,
is also popularly used, although it is not differentiable at zero.
On the other hand, the Gaussian function is essentially the same as the Gaussian
kernel introduced in Section 21.2, but the Gaussian bandwidth and center are also
learned in the hierarchical model. For this reason, a neural network model with the
Gaussian activation function is expected to be more expressive than the Gaussian
kernel model.
A hierarchical model can be expressed by a three-layered network, as illustrated
in Fig. 21.7. A notable characteristic of the neural network model is that a mapping
between a parameter and a function is not necessarily one-to-one. In particular,
different parameter values can yield the same function. For example, a neural network
model with b = 2,
fθ(x) = α1ϕ(x; w1,γ1) + α2ϕ(x; w2,γ2)

244
CHAPTER 21
LEARNING MODELS
for w1 = w2 = w and γ1 = γ2 = γ, represents the same function if α1 + α2 is a
constant:
fθ(x) = (α1 + α2)ϕ(x; w,γ).
This causes the Fisher information matrix (see Section 13.3) to be singular, which
makes it impossible to apply standard statistical machine learning theory. To cope
with this problem, Bayesian inference was shown to be promising [119].
Due to the nonlinearity of the neural network model with respect to parameters,
parameter learning is usually carried out by a gradient descent method called the error
back-propagation algorithm [85]. However, only a local optimal solution may be
found by the gradient method (see Section 15.3). To cope with this problem, running
the gradient-based learning procedure multiple times from different initial solutions is
effective. Furthermore, pretraining of each layer by an unsupervised learning method
is demonstrated to be useful in practice [55].

CHAPTER
LEAST SQUARES
REGRESSION
22
CHAPTER CONTENTS
Method of LS .................................................................... 245
Solution for Linear-in-Parameter Model........................................... 246
Properties of LS Solution......................................................... 250
Learning Algorithm for Large-Scale Data ......................................... 251
Learning Algorithm for Hierarchical Model ....................................... 252
Let us consider a regression problem of learning a real-valued function y = f (x)
defined on d-dimensional input space from input-output paired training samples
{(xi, yi)}n
i=1. In practice, the training output values {yi}n
i=1 may be noisy observa-
tions of the true values { f (xi)}n
i=1. In this chapter, the most fundamental regression
technique called LS regression is introduced.
22.1 METHOD OF LS
Let θ be the parameter of model fθ(x). The method of LS, learns the parameter θ so
that the squared difference between training output {yi}n
i=1 and model output fθ(xi)
is minimized:
θLS = argmin
θ
JLS(θ),
where
JLS(θ) = 1
2
n

i=1

yi −fθ(xi)
2.
(22.1)
Since the squared error (yi −fθ(xi))2 is the ℓ2-norm of the residual
yi −fθ(xi),
the LS method is also referred to as ℓ2-loss minimization.
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00033-9
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
245

246
CHAPTER 22
LEAST SQUARES REGRESSION
22.2 SOLUTION FOR LINEAR-IN-PARAMETER MODEL
For a linear-in-parameter model (see Section 21.1),
fθ(x) =
b

j=1
θiϕi(x) = θ⊤φ(x),
the training squared error JLS is expressed as
JLS(θ) = 1
2 ∥y −Φθ∥2,
where
y = (y1,. . . , yn)⊤
is the n-dimensional vector consisting of training output values and Φ is the n × b
matrix called design matrix:
Φ =
*.....
,
ϕ1(x1) · · · ϕb(x1)
...
...
...
ϕ1(xn) · · · ϕb(xn)
+/////
-
.
The partial derivative of the training squared error JLS with respect to parameter
θ is given by
∇θ JLS =
∂JLS
∂θ1
,. . . , ∂JLS
∂θb
⊤
= Φ⊤y −Φ⊤Φθ.
Setting this to zero shows that the solution should satisfy
Φ⊤Φθ = Φ⊤y.
Then the LS solution θLS is given by
θLS = Φ†y,
where Φ† is the generalized inverse (see Fig. 22.1) of the design matrix Φ. When
Φ⊤Φ is invertible, the generalized inverse Φ† is expressed as
Φ† = (Φ⊤Φ)−1Φ⊤.
A MATLAB code of LS regression for the linear-in-parameter model with
sinusoidal basis functions,
φ(x) =

1,sin x
2,cos x
2,sin 2x
2 ,cos 2x
2 ,. . . ,sin 15x
2 ,cos 15x
2
⊤
,
is provided in Fig. 22.3. In the program, instead of explicitly obtaining the generalized
inverse Φ† by the pinv function, the equation Φθ = y is directly solved by

22.2 SOLUTION FOR LINEAR-IN-PARAMETER MODEL
247
A matrix X is called the generalized inverse of real matrix A, if it satisfies
the following four conditions:
AX A = A,
X AX = X,
(X A)⊤= X A,
(AX)⊤= AX.
The generalized inverse of A is often denoted by A† and is also referred to
as the Moore-Penrose pseudoinverse. The ordinary inverse is defined only
for full-rank square matrices, while the generalized inverse is defined for
singular or even nonsquare matrices. In MATLAB, the generalized inverse
can be obtained by the pinv function. A generalized inverse of a scalar a is
given by
a† =

1/a
(a , 0),
0
(a = 0),
and a generalized inverse of a d × m matrix A is given by
A† =
min(d,m)

k=1
κ†
kψkφ⊤
k ,
where ψk, φk, and κk are a left singular vector, a right singular vector, and
a singular value of A, respectively (see Fig. 22.2). When A is square and full
rank, its generalized inverse A† is reduced to the ordinary inverse A−1.
FIGURE 22.1
Generalized inverse.
t=p\y, which is computationally more efficient. The behavior of LS regression is
illustrated in Fig. 22.4, showing that a complicated nonlinear function can be nicely
approximated.
A LS method where the loss function for the ith training sample is weighted
according to wi ≥0 is referred to as weighted LS:
min
θ
1
2
n

i=1
wi

yi −fθ(xi)
2.

248
CHAPTER 22
LEAST SQUARES REGRESSION
Singular value decomposition is an extension eigenvalue decomposition (see
Fig. 6.2) to nonsquare matrices. For d×m matrix A, a d-dimensional nonzero
vector ψ, an m-dimensional nonzero vector ϕ, and a non-negative scalar κ
such that
Aϕ = κψ
are called a left singular vector, a right singular vector, and a singular value
of A, respectively. Generally, there exist c singular values κ1,. . . , κc, where
c = min(d,m).
Singular vectors ϕ1,. . . ,ϕc and ψ1,. . . ,ψc corresponding to singular values
κ1,. . . , κc are mutually orthogonal and are usually normalized, i.e., they are
orthonormal as
ϕ⊤
k ϕk′ =

1
(k = k′)
0
(k , k′)
and ψ⊤
k ψk′ =

1
(k = k′),
0
(k , k′).
A matrix A can be expressed by using its singular vectors and singular values
as
A =
c

k=1
κkψkϕ⊤
k .
In MATLAB, singular value decomposition can be performed by the svd
function.
FIGURE 22.2
Singular value decomposition.
The solution of weighted LS is given as
(Φ⊤WΦ)†Φ⊤W y,
where W is the diagonal matrix consisting of weights:
W = diag (w1,. . . ,wn) .
The kernel model introduced in Section 21.2 is also linear in terms of parameters:
fθ(x) =
n

j=1
θ jK(x, xj).
(22.2)

22.2 SOLUTION FOR LINEAR-IN-PARAMETER MODEL
249
n=50; N=1000; x=linspace(-3,3,n)’; X=linspace(-3,3,N)’;
pix=pi*x; y=sin(pix)./(pix)+0.1*x+0.05*randn(n,1);
p(:,1)=ones(n,1); P(:,1)=ones(N,1);
for j=1:15
p(:,2*j)=sin(j/2*x); p(:,2*j+1)=cos(j/2*x);
P(:,2*j)=sin(j/2*X); P(:,2*j+1)=cos(j/2*X);
end
t=p\y; F=P*t;
figure(1); clf; hold on; axis([-2.8 2.8 -0.5 1.2]);
plot(X,F,’g-’); plot(x,y,’bo’);
FIGURE 22.3
MATLAB code for LS regression.
FIGURE 22.4
Example of LS regression with sinusoidal basis
functions φ(x) = (1,sin x
2 ,cos x
2 ,sin 2x
2 ,cos 2x
2 ,
. . . ,sin 15x
2 ,cos 15x
2 )⊤.
Thus, the LS solution for the kernel model can also be obtained in the same way, by
replacing the design matrix Φ with the kernel matrix:
K =
*.....
,
K(x1, x1) · · · K(x1, xn)
...
...
...
K(xn, x1) · · · K(xn, xn)
+/////
-
.

250
CHAPTER 22
LEAST SQUARES REGRESSION
22.3 PROPERTIES OF LS SOLUTION
Let us consider singular value decomposition (see Fig. 22.2) of design matrix Φ:
Φ =
min(n,b)

k=1
κkψkϕ⊤
k ,
where ψk, ϕk, and κk are a left singular vector, a right singular vector, and a singular
value of Φ, respectively.
As explained in Fig. 22.1, the generalized inverse Φ† of the matrix Φ can be
expressed as
Φ† =
min(n,b)

k=1
κ†
kϕkψ⊤
k ,
where κ† is the generalized inverse of scalar κ:
κ† =

1/κ
(κ , 0),
0
(κ = 0).
Then the LS solution θLS can be expressed as
θLS =
min(n,b)

k=1
κ†
k(ψ⊤
k y)ϕk.
The n-dimensional vector consisting of the output values of the LS solution fθLS
at training input samples {xi}n
i=1 is given by
  fθLS(x1),. . . , fθLS(xn)⊤= ΦθLS = ΦΦ†y.
Since (ΦΦ†)2 = ΦΦ† and (ΦΦ†)⊤= ΦΦ†, ΦΦ† is the projection matrix onto the
range of Φ and thus the LS solution actually projects the training output vector y
onto the range of Φ.
When there exists a parameter θ∗such that the true learning target function f
is given by fθ∗, the vector consisting of the output values of the true function f at
training input samples {xi}n
i=1 is given by
  f (x1),. . . , f (xn)⊤= Φθ∗.
This means that the true output vector belongs to the range of Φ, and thus the LS
method tries to remove noise included in y by projecting it onto the range of Φ
(Fig. 22.5).

22.4 LEARNING ALGORITHM FOR LARGE-SCALE DATA
251
FIGURE 22.5
Geometric interpretation of LS method for
linear-in-parameter model. Training output
vector y is projected onto the range of Φ,
denoted by R(Φ), for denoising purposes.
When the expectation of noise is zero, the LS solution θLS is an unbiased
estimator of the true parameter θ∗:
E[θLS] = θ∗,
where E denotes the expectation over the noise included in training output {yi}n
i=1.
Even if the model is misspecified (see Section 17.1.2), i.e., f , fθ for any θ, E[θLS]
converges to the optimal parameter in the model, as the number of training samples, n,
tends to infinity. This property is called asymptotic unbiasedness (see Section 13.2).
22.4 LEARNING ALGORITHM FOR LARGE-SCALE DATA
The size of design matrix Φ is n×b and thus it cannot be stored in the memory space if
the number of training samples, n, and the number of parameters, b, are large. In such
a situation, the stochastic gradient algorithm introduced in Section 15.3 is useful,
which updates parameter θ to go down the gradient of the training squared error JLS.
For linear-in-parameter models, the training squared error JLS is a convex function
(see Fig. 8.3). Thus, the solution obtained by the stochastic gradient algorithm gives
the global optimal solution. The algorithm of the stochastic gradient algorithm for
linear-in-parameter models is summarized in Fig. 22.6.
In Fig. 22.7, a MATLAB code of stochastic gradient descent for LS regression
with the Gaussian kernel model is provided:
fθ(x) =
n

j=1
θ jK(x, xj),
where
K(x, c) = exp

−∥x −c∥2
2h2

.

252
CHAPTER 22
LEAST SQUARES REGRESSION
1. Initialize the parameter θ.
2. Choose the ith training sample (xi, yi) randomly.
3. Update the parameter θ to go down the gradient as
θ ←−θ −ε∇J(i)
LS(θ),
where ε is a small positive scalar called the step size, and ∇J(i)
LS is the
gradient of the training squared error for the ith training sample:
∇J(i)
LS(θ) = −φ(xi)

yi −fθ(xi)

.
4. Iterate 2–3 until convergence.
FIGURE 22.6
Algorithm of stochastic gradient descent for LS regression with a linear-in-parameter model.
For n = 50 training samples, the Gaussian bandwidth is set at h = 0.3. The behavior
of the algorithm is illustrated in Fig. 22.8, where starting from a random initial value,
a function close to the final solution is obtained after around 200 iterations. However,
11,556 iterations are required until convergence.
The convergence speed of the stochastic gradient algorithm depends on the step
size (e=0.1 in Fig. 22.7) and the convergence condition (norm(t-t0)<0.000001
in Fig. 22.7). The algorithm may converge faster if these parameters are tuned. In
particular, as for the step size, starting from a large value and gradually decreasing
it would be appropriate, as discussed in Section 15.3. However, appropriately
implementing this idea is not straightforward in practice.
22.5 LEARNING ALGORITHM FOR HIERARCHICAL
MODEL
Stochastic gradient descent is a popular choice also for training hierarchical models
such as neural networks introduced in Section 21.3. However, since the training
squared error JLS is not a convex function in hierarchical models, there exist multiple
local optimal solutions in general and the gradient method may only find one of the
local optimal solutions (Fig. 22.9). In practice, starting from different initial values,
gradient descent is performed multiple times and the solution that gives the smallest
training squared error is chosen as the most promising one.
For the sigmoidal neural network model introduced in Section 21.3,
fθ(x) =
b

j=1
αjϕ(x; βj,γj),

22.5 LEARNING ALGORITHM FOR HIERARCHICAL MODEL
253
n=50; x=linspace(-3,3,n)’; pix=pi*x;
y=sin(pix)./(pix)+0.1*x+0.05*randn(n,1);
hh=2*0.3^2; t=randn(n,1); e=0.1;
for o=1:n*1000
i=ceil(rand*n);
ki=exp(-(x-x(i)).^2/hh); t0=t+e*ki*(y(i)-ki’*t);
if norm(t-t0)<0.000001, break, end
t=t0;
end
N=1000; X=linspace(-3,3,N)’;
K=exp(-(repmat(X.^2,1,n)+repmat(x.^2’,N,1)-2*X*x’)/hh);
F=K*t;
figure(1); clf; hold on; axis([-2.8 2.8 -0.5 1.2]);
plot(X,F,’g-’); plot(x,y,’bo’);
FIGURE 22.7
MATLAB code of stochastic gradient descent for LS regression with the Gaussian kernel model.
where
ϕ(x; β,γ) =
1
1 + exp (−x⊤β −γ),
the gradient ∇θ J(i)
LS can be computed efficiently as
∂J(i)
LS
∂αj
= −zi, jri,
∂J(i)
LS
∂β(k)
j
= −αj zi, j
 1 −zi, j
 x(k)
i ri,
∂J(i)
LS
∂γj
= −αj zi, j
 1 −zi, j
 ri,
where β(k)
j
denotes the kth element of vector βj, x(k)
i
denotes the kth element of
vector xi, ri denotes the residual for the ith training sample,
ri = yi −fθ(xi),
and zi, j denotes the output of the jth basis function for the ith training sample:
zi, j = ϕ(xi; βj,γj).

254
CHAPTER 22
LEAST SQUARES REGRESSION
(a) Initial value
(b) After 50 iterations
(c) After 100 iterations
(d) After 150 iterations
(e) After 200 iterations
(f) Final solution (after 11,556 it-
erations)
FIGURE 22.8
Example of stochastic gradient descent for LS regression with the Gaussian kernel model. For
n = 50 training samples, the Gaussian bandwidth is set at h = 0.3.
FIGURE 22.9
Gradient descent for nonlinear models. The
training squared error JLS is nonconvex and
there exist multiple local optimal solutions in
general.
Since the residual ri is propagated backward when computing the gradient, the gra-
dient method for neural networks is often referred to as error back-propagation [85].
If input xi is augmented as
xi = (x⊤
i ,1)⊤∈Rd+1,

22.5 LEARNING ALGORITHM FOR HIERARCHICAL MODEL
255
n=50; N=1000; x=linspace(-3,3,n)’; pix=pi*x;
y=sin(pix)./(pix)+0.1*x+0.05*randn(n,1);
x(:,2)=1; d=1; m=20; e=0.1; a=ones(m,1); b=randn(m,d+1);
for o=1:n*100000
i=ceil(rand*n); z=1./(1+exp(-b*x(i,:)’)); r=y(i)-a’*z;
a0=a+e*z*r; b0=b+e*(a.*z.*(1-z)*x(i,:))*r;
if norm(a-a0)+norm(b-b0)<0.00000001, break, end
a=a0; b=b0;
end
X=linspace(-3,3,N)’; X(:,2)=1; Y=a’*(1./(1+exp(-b*X’)));
figure(1); clf; hold on; axis([-2.8 2.8 -0.5 1.2]);
plot(X(:,1),Y,’g-’); plot(x(:,1),y,’bo’);
FIGURE 22.10
MATLAB code for error back-propagation algorithm.
(a) Initial value
(b) After 100 iterations
(c) After 500 iterations
(d) After 1000 iterations
(e) After 10,000 iterations
(f) Final solution (after 1229,742
iterations)
FIGURE 22.11
Example of regression by error back-propagation algorithm.

256
CHAPTER 22
LEAST SQUARES REGRESSION
the basis parameter,
βj = (β⊤
j ,γj)⊤∈Rd+1,
can be updated together as
∂J(i)
LS
∂β(k)
j
= −αj zi, j
 1 −zi, j
 x(k)
i ri
for k = 1,. . . ,d + 1.
A MATLAB code of the error back-propagation algorithm for a three-layered
neural network is provided in Fig. 22.10, and its behavior is illustrated in Fig. 22.11.
This shows that the solution fits the training samples well after many iterations.
Although the error back-propagation is applicable to any neural networks models,
learning a neural network having many layers is often difficult in practice. Indeed,
the error is propagated only to parameters close to the output layer, and parameters
near the input layer are rarely updated. To efficiently learn a deep neural network,
iteratively initializing each layer one by one based on unsupervised learning is shown
to be useful [55]. Such a method will be explained in Chapter 36.

CHAPTER
CONSTRAINED LS
REGRESSION
23
CHAPTER CONTENTS
Subspace-Constrained LS ........................................................ 257
ℓ2-Constrained LS................................................................ 259
Model Selection.................................................................. 262
The least squared method introduced in Chapter 22 forms the basis of various
machine learning techniques. However, the naive LS method often yields overfitting
to noisy training samples (Fig. 23.1(a)). This is caused by the fact that the model
is too complicated compared with the number of available training samples. In this
chapter, constrained LS methods are introduced for controlling the model complexity.
23.1 SUBSPACE-CONSTRAINED LS
In the LS method for linear-in-parameter model,
fθ(x) =
b

j=1
θ jϕj(x) = θ⊤φ(x),
parameters {θ j}b
j=1 can be determined without any constraint, implying that the entire
parameter space is used for learning (see Fig. 23.2(a)). In this section, the method
of subspace-constrained LSsubspace-constrained least squares is introduced, which
imposes a subspace constraint in the parameter space:
min
θ
JLS(θ)
subject to Pθ = θ,
where P is a b × b projection matrix such that
P2 = P and P⊤= P.
As illustrated in Fig. 23.2(b), with the constraint Pθ = θ, parameter θ is confined in
the range of P.
The solution θ of subspace-constrained LS can be obtained simply by replacing
the design matrix Φ with ΦP:
θ = (ΦP)†y.
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00034-0
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
257

258
CHAPTER 23
CONSTRAINED LS REGRESSION
(a) Ordinary LS
(b) Subspace-constrained least squared
FIGURE 23.1
Examples of LS regression for linear-in-parameter model when the noise level in training
output is high. Sinusoidal basis functions {1,sin x
2 ,cos x
2 ,sin 2x
2 ,cos 2x
2 ,. . . ,sin 15x
2 ,cos 15x
2 }
are
used
in
ordinary
LS,
while
its
subset
{1,sin x
2 ,cos x
2 ,sin 2x
2 ,cos 2x
2 ,. . . ,sin 5x
2 ,
cos 5x
2 } is used in the subspace-constrained LS method.
(a) Ordinary LS
(b) Subspace-constrained LS
FIGURE 23.2
Constraint in parameter space.
A MATLAB code for subspace-constrained LS is provided in Fig. 23.3, where a
linear-in-parameter model with sinusoidal basis functions,

1,sin x
2,cos x
2,sin 2x
2 ,cos 2x
2 ,. . . ,sin 15x
2 ,cos 15x
2

,
is constrained to be confined in the subspace spanned by

1,sin x
2,cos x
2,sin 2x
2 ,cos 2x
2 ,. . . ,sin 5x
2 ,cos 5x
2

.
The behavior of subspace-constrained LS is illustrated in Fig. 23.1(b), showing
that the constraint effectively contributes to mitigating overfitting. Although the
projection matrix P is manually determined in the above example, it may be
determined in a data-dependent way, e.g., by PCA introduced in Section 35.2.1 or
multitask learning with the trace norm introduced in Section 34.3.

23.2 ℓ2-CONSTRAINED LS
259
n=50; N=1000; x=linspace(-3,3,n)’; X=linspace(-3,3,N)’;
pix=pi*x; y=sin(pix)./(pix)+0.1*x+0.2*randn(n,1);
p(:,1)=ones(n,1); P(:,1)=ones(N,1);
for j=1:15
p(:,2*j)=sin(j/2*x); p(:,2*j+1)=cos(j/2*x);
P(:,2*j)=sin(j/2*X); P(:,2*j+1)=cos(j/2*X);
end
t1=p\y; F1=P*t1;
t2=(p*diag([ones(1,11) zeros(1,20)]))\y; F2=P*t2;
figure(1); clf; hold on; axis([-2.8 2.8 -0.8 1.2]);
plot(X,F1,’g-’); plot(X,F2,’r--’); plot(x,y,’bo’);
legend(’LS’,’Subspace-Constrained LS’);
FIGURE 23.3
MATLAB code for subspace-constrained LS regression.
FIGURE 23.4
Parameter space in ℓ2-constrained LS.
23.2 ℓ2-CONSTRAINED LS
In the subspace-constrained LS method, the constraint is given by a projection matrix
P. However, since P has many degrees of freedom, it is not easy to handle in practice.
In this section, an alternative approach called ℓ2-constrained LSℓ2-constrained least
squares is introduced:
min
θ
JLS(θ)
subject to ∥θ∥2 ≤R2,
(23.1)
where R ≥0. As illustrated in Fig. 23.4, parameters are searched within an origin-
centered hypersphere in the ℓ2-constrained LS method, where R denotes the radius
of the hypersphere.

260
CHAPTER 23
CONSTRAINED LS REGRESSION
Let us consider the constrained optimization problem,
min
t
f (t) subject to g(t) ≤0,
where f : Rd →R and g : Rd →Rp are the differentiable convex functions.
Its Lagrange dual problem is given as
max
λ
inf
t L(t,λ) subject to λ ≥0,
where
λ = (λ1,. . . ,λp)⊤
is called the Lagrange multipliers and
L(t,λ) = f (t) + λ⊤g(t)
is called the Lagrange function (or simply the Lagrangian). The solution
of the Lagrange dual problem for t agrees with the original constrained
optimization problem.
FIGURE 23.5
Lagrange dual problem.
Lagrange duality (Fig. 23.5) yields that the solution of optimization problem
(23.1) can be obtained by solving the following Lagrange dual problem:
max
λ
min
θ

JLS(θ) + λ
2

∥θ∥2 −R2
subject to λ ≥0.
Although the Lagrange multiplier λ is determined based on the radius R in principle,
λ may be directly specified in practice. Then the solution of ℓ2-constrained LS θ is
given by
θ = argmin
θ

JLS(θ) + λ
2 ∥θ∥2

.
(23.2)
The first term in Eq. (23.2) measures the goodness of fit to training samples, while
the second term λ
2 ∥θ∥2 measures the amount of overfitting.
Differentiating the objective function in Eq. (23.2) with respect to parameter θ
and setting it to zero give the solution of ℓ2-constrained LS analytically as
θ = (Φ⊤Φ + λI)−1Φ⊤y,
(23.3)

23.2 ℓ2-CONSTRAINED LS
261
where I denotes the identity matrix. From Eq. (23.3), ℓ2-constrained LS enhances
the regularity of matrix Φ⊤Φ by adding λI, which contributes to stabilizing the
computation of its inverse. For this reason, ℓ2-constrained LS is also called ℓ2-
regularization learning, the second term ∥θ∥2 in Eq.(23.2) is called a regularizer, and
λ is called the regularization parameter. In statistics, ℓ2-constrained LS is referred to
as ridge regression [56].
Let us consider singular value decomposition (see Fig. 22.2) of design matrix Φ:
Φ =
min(n,b)

k=1
κkψkϕ⊤
k ,
where ψk, φk, and κk are a left singular vector, a right singular vector, and a singular
value of Φ, respectively. Then the solution of ℓ2-constrained LS can be expressed as
θ =
min(n,b)

k=1
κk
κ2
k + λ ψ⊤
k yϕk.
When λ = 0, ℓ2-constrained LS is reduced to ordinary LS. When the design matrix
Φ is ill-conditioned in the sense that a very small singular value exists, the inverse
of that singular value, κk/κ2
k (= 1/κk), can be very large. Then noise included in
training output vector y is magnified by the factor 1/κk in ordinary LS. On the other
hand, in ℓ2-constrained LS, κ2
k in the denominator is increased by adding a positive
scalar λ, which prevents κk/(κ2
k + λ) from being too large and thus overfitting can be
mitigated.
A MATLAB code for ℓ2-constrained LS is provided in Fig. 23.6,
where the
Gaussian kernel model is used:
fθ(x) =
n

j=1
θ jK(x, xj),
where
K(x, c) = exp

−∥x −c∥2
2h2

.
The Gaussian bandwidth is set at h = 0.3, and the regularization parameter is set at
λ = 0.1. The behavior of ℓ2-constrained LS is illustrated in Fig. 23.7, showing that
overfitting can be successfully avoided.
ℓ2-constrained LS can be slightly generalized by using a b×b positive semidefinite
matrix G as
min
θ
JLS(θ)
subject to θ⊤Gθ ≤R2,
which is called generalized ℓ2-constrained LS. When matrix G, called a regular-
ization matrix, is positive and symmetric, θ⊤Gθ ≤R2 represents an ellipsoidal

262
CHAPTER 23
CONSTRAINED LS REGRESSION
n=50; N=1000; x=linspace(-3,3,n)’; X=linspace(-3,3,N)’;
pix=pi*x; y=sin(pix)./(pix)+0.1*x+0.2*randn(n,1);
x2=x.^2; X2=X.^2; hh=2*0.3^2; l=0.1;
k=exp(-(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’)/hh);
K=exp(-(repmat(X2,1,n)+repmat(x2’,N,1)-2*X*x’)/hh);
t1=k\y; F1=K*t1; t2=(k^2+l*eye(n))\(k*y); F2=K*t2;
figure(1); clf; hold on; axis([-2.8 2.8 -1 1.5]);
plot(X,F1,’g-’); plot(X,F2,’r--’); plot(x,y,’bo’);
legend(’LS’,’L2-Constrained LS’);
FIGURE 23.6
MATLAB code of ℓ2-constrained LS regression for Gaussian kernel model.
(a) Ordinary LS
(b) ℓ2-constrained LS
FIGURE 23.7
Example of ℓ2-constrained LS regression for Gaussian kernel model. The Gaussian bandwidth is
set at h = 0.3, and the regularization parameter is set at λ = 0.1.
constraint (Fig. 23.8). The solution of generalized ℓ2-constrained LS can be obtained
in the same way as ordinary ℓ2-constrained LS by
θ = (Φ⊤Φ + λG)−1Φ⊤y.
23.3 MODEL SELECTION
In the previous sections, constrained LS was demonstrated to contribute to mitigating
overfitting. However, the behavior of constrained LS depends on the choice of con-

23.3 MODEL SELECTION
263
FIGURE 23.8
Parameter space in generalized ℓ2-constrained
LS.
(a) (h, λ) = (0.03, 0.0001)
(b) (h, λ) = (0.03, 0.1)
(c) (h, λ) = (0.03, 100)
(d) (h, λ) = (0.3, 0.0001)
(e) (h, λ) = (0.3, 0.1)
(f) (h, λ) = (0.3, 100)
(g) (h, λ) = (3, 0.0001)
(h) (h, λ) = (3, 0.1)
(i) (h, λ) = (3, 100)
FIGURE 23.9
Examples of ℓ2-constrained LS with the Gaussian kernel model for different Gaussian bandwidth
h and different regularization parameter λ.

264
CHAPTER 23
CONSTRAINED LS REGRESSION
n=50; x=linspace(-3,3,n)’; pix=pi*x;
y=sin(pix)./(pix)+0.1*x+0.2*randn(n,1);
x2=x.^2; xx=repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’;
hhs=2*[0.03 0.3 3].^2; ls=[0.0001 0.1 100];
m=5; u=mod(randperm(n),m)+1;
for hk=1:length(hhs)
hh=hhs(hk); k=exp(-xx/hh);
for i=1:m
ki=k(u~=i,:); kc=k(u==i,:); yi=y(u~=i); yc=y(u==i);
for lk=1:length(ls)
t=(ki’*ki+ls(lk)*eye(n))\(ki’*yi);
g(hk,lk,i)=mean((yc-kc*t).^2);
end, end, end
[gl,ggl]=min(mean(g,3),[],2); [ghl,gghl]=min(gl);
L=ls(ggl(gghl)); HH=hhs(gghl);
N=1000; X=linspace(-3,3,N)’;
K=exp(-(repmat(X.^2,1,n)+repmat(x2’,N,1)-2*X*x’)/HH);
k=exp(-xx/HH); t=(k^2+L*eye(n))\(k*y); F=K*t;
figure(1); clf; hold on; axis([-2.8 2.8 -0.7 1.7]);
plot(X,F,’g-’); plot(x,y,’bo’);
FIGURE 23.10
MATLAB code of cross validation for ℓ2-constrained LS regression.
straining parameters such as the projection matrix P and the regularization parameter
λ. Furthermore, choice of basis/kernel functions also affects the performance.
Fig. 23.9 illustrates the solutions of ℓ2-constrained LS with the Gaussian kernel
model for different Gaussian bandwidth h and different regularization parameter λ.
The obtained functions are highly fluctuated if the Gaussian bandwidth h is too
small, while they are overly smoothed if the Gaussian bandwidth h is too large.
Similarly, overfitting is prominent if the regularization parameter λ is too small, and
the obtained functions become too flat if the regularization parameter λ is too large.
In this particular example, h = 0.3 and λ = 0.1 would be a reasonable choice.
However, the best values of h and λ depend on various unknown factors such as the
true learning target function and the noise level.
The problem of data-dependent choice of these tuning parameters, called model
selection, can be addressed by cross validation (see Section 14.4 and Section 16.4.2).
Note that naive model selection based on the training squared error simply yields

23.3 MODEL SELECTION
265
FIGURE 23.11
Example of cross validation for ℓ2-constrained LS regression.
The cross validation error for all Gaussian bandwidth h and
regularization parameter λ is plotted, which is minimized at
(h,λ) = (0.3,0.1). See Fig. 23.9 for learned functions.
For invertible and symmetric matrix A, it holds that
(A + bb⊤)−1 = A−1 −A−1bb⊤A−1
1 + b⊤A−1b .
If c −b⊤A−1b , 0, it holds that
*.
,
A b
b⊤c
+/
-
−1
= *.
,
A−1 + αA−1bb⊤A−1 −αA−1b
−αb⊤A−1
α
+/
-
,
where
α =
1
c −b⊤A−1b .
FIGURE 23.12
Matrix inversion lemma.
overfitting. For example, in Fig. 23.9, choosing the smallest h and λ minimizes the
training squared error, which results in the heaviest overfitting. The algorithm of cross
validation is exactly the same as the one described in Fig. 16.17, but the squared loss
function is used to compute the validation error:

266
CHAPTER 23
CONSTRAINED LS REGRESSION
J(ℓ)
j
=
1
|Zℓ|

(x′,y′)∈Zℓ
(y′ −f (ℓ)
j (x′))2,
where |Zℓ| denotes the number of elements in the set Zℓand f (ℓ)
j
denotes the function
learned using model Mj from all training samples without Zℓ.
A MATLAB code of cross validation for the data in Fig. 23.9 is provided in
Fig. 23.10, and its behavior is illustrated in Fig. 23.11. In this example, the Gaussian
bandwidth h is chosen from {0.03,0.3,3}, the regularization parameter λ is chosen
from {0.0001,0.1,100}, and the number of folds in cross validation is set at 5. The
cross validation error is minimized at (h,λ) = (0.3,0.1), which would be a reasonable
choice as illustrated in Fig. 23.9.
Cross validation when the number of folds is set at the number of training samples
n, i.e., n −1 samples are used for training and only the remaining single sample is
used for validation, is called leave-one-out cross validation. Naive implementation
of leave-one-out cross validation requires n repetitions of training and validation,
which is computationally demanding when n is large. However, for ℓ2-constrained
LS, the score of leave-one-out cross validation can be computed analytically without
repetition as follows [77]:
1
n ∥
H−1H y∥2,
(23.4)
where H is the n × n matrix defined as
H = I −Φ(Φ⊤Φ + λI)−1Φ⊤,
and 
H is the diagonal matrix with diagonal elements the same as H. In the derivation
of Eq. (23.4), the matrix inversion lemma was utilized (see Fig. 23.12).

CHAPTER
SPARSE REGRESSION 24
CHAPTER CONTENTS
ℓ1-Constrained LS................................................................ 267
Solving ℓ1-Constrained LS........................................................ 268
Feature Selection by Sparse Learning ............................................ 272
Various Extensions ............................................................... 272
Generalized ℓ1-Constrained LS ............................................. 273
ℓp-Constrained LS ......................................................... 273
ℓ1 + ℓ2-Constrained LS ..................................................... 274
ℓ1,2-Constrained LS ........................................................ 276
Trace Norm Constrained LS ................................................ 278
ℓ2-constrained LS for linear-in-parameter model,
fθ(x) =
b

j=1
θ jϕj(x),
is highly useful in practice. However, when the number of parameters, b, is very
large, computing the output value fθ(x) for test sample x is time-consuming. In this
chapter, a learning method that tends to produce a sparse solution is introduced. A
sparse solution means that many of the parameters {θ j}b
j=1 take exactly zero, and
thus fθ(x) can be computed efficiently.
24.1 ℓ1-CONSTRAINED LS
ℓ2-constrained LS uses the ℓ2-norm as a constraint, while sparse learning uses the
ℓ1-norm as a constraint:
min
θ
1
2 ∥y −Φθ∥2 subject to ∥θ∥1 ≤R2,
(24.1)
where the ℓ1-norm of vector θ = (θ1,. . . ,θb)⊤is defined as the absolute sum of all
elements:
∥θ∥1 =
b

j=1
|θ j|.
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00035-2
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
267

268
CHAPTER 24
SPARSE REGRESSION
FIGURE 24.1
Parameter space in ℓ1-constrained LS.
The region that ∥θ∥1 ≤R2 specifies is illustrated in Fig. 24.1, which has corners on
the coordinate axes. This shape is called the ℓ1-ball, and ℓ1-constrained LS is also
called ℓ1-regularization learning.
Actually, these corners are the key to produce a sparse solution. Let us intuitively
explain this idea using Fig. 24.2. For linear-in-parameter model,
fθ(x) =
b

j=1
θ jϕj(x) = θ⊤φ(x),
the training squared error is a convex quadratic function with respect to θ:
JLS(θ) = 1
2 ∥y −Φθ∥2.
Thus, in the parameter space, JLS has ellipsoidal contours and its minimum is the
LS solution θLS. As illustrated in Fig. 24.2(a), the solution of ℓ2-constrained LS is
given as the point where the ellipsoidal contour and the ℓ2-ball touch. Similarly, the
solution of ℓ1-constrained LS is given as the point where the ellipsoidal contour and
the ℓ1-ball touch. Since the ℓ1-ball has corners on the coordinate axes, the solution
tends to be at one of the corners, which is a sparse solution (see Fig. 24.2(b)).
In statistics, ℓ1-constrained LS is referred to as lasso regression [110].
24.2 SOLVING ℓ1-CONSTRAINED LS
Since the ℓ1-norm is not differentiable at the origin, solving ℓ1-constrained LS
problem (24.1) is not as straightforward as ℓ2-constrained LS. In this section, a
general optimization technique called the alternating direction method of multipliers
(ADMM) [20] is applied to ℓ1-constrained LS and gives a simple yet practical
algorithm. The algorithm of ADMM is summarized in Fig. 24.3.

24.2 SOLVING ℓ1-CONSTRAINED LS
269
(a) ℓ2-constrained LS
(b) ℓ1-constrained LS
FIGURE 24.2
The solution of ℓ1-constrained LS tends to be on one of the coordinate axes, which is a sparse
solution.
Let us express the optimization problem of ℓ1-constrained LS, Eq. (24.1), as
min
θ,z
1
2 ∥y −Φθ∥2 + λ∥z∥1

subject to θ = z,
where λ ≥0. The augmented Lagrange function for this optimization problem is
given by
L(θ, z, u) = 1
2 ∥y −Φθ∥2 + λ∥z∥1 + u⊤(θ −z) + 1
2 ∥θ −z∥2,
where u is the Lagrange multipliers. Since
∂L
∂θ = −Φ⊤(y −Φθ) + u + θ −z,
setting this to zero yields the following update equation for θ:
θ(k+1) = argmin
θ
L(θ, z(k), u(k))
= (Φ⊤Φ + I)−1(Φ⊤y + z(k) −u(k)).
From
min
z

λ|z| + u(θ −z) + 1
2(θ −z)2

= max(0,θ + u −λ) + min(0,θ + u + λ),

270
CHAPTER 24
SPARSE REGRESSION
The ADMM algorithm for solving the constrained optimization problem,
min
θ,z [ f (θ) + g(z)] subject to Aθ + Bz = c,
is given as follows:
θ(k+1) = argmin
θ
L(θ, z(k), u(k)),
z(k+1) = argmin
z
L(θ(k+1), z, u(k)),
u(k+1) = u(k) + Aθ(k+1) + Bz(k+1) −c,
where u is the Lagrange multipliers and L is the augmented Lagrange
function defined as
L(θ, z, u) = f (θ) + g(z) + u⊤(Aθ + Bz −c)
+ 1
2 ∥Aθ + Bz −c∥2.
An advantage of ADMM is that no tuning parameter such as the step size in
the gradient algorithm (see Fig. 22.6) is involved.
FIGURE 24.3
Alternating direction method of multipliers.
the update equation for z is given as
z(k+1) = argmin
z
L(θ(k+1), z, u(k))
= max(0,θ(k+1) + u(k) −λ1) + min(0,θ(k+1)u(k) + λ1),
where 1 is the vector with all ones. Finally, the update equation for u is given as
follows (see Fig. 24.3):
u(k+1) = u(k) + θ(k+1) −z(k+1).
A MATLAB code of ℓ1-constrained LS by ADMM for the Gaussian kernel model,
fθ(x) =
n

j=1
θ j exp *
,
−∥x −xj∥2
2h2
+
-
,
is provided in Fig. 24.4, and its behavior is illustrated in Fig. 24.5. The obtained
solution is actually very similar to the one obtained by ℓ2-constrained LS (see

24.2 SOLVING ℓ1-CONSTRAINED LS
271
n=50; x=linspace(-3,3,n)’; pix=pi*x;
y=sin(pix)./(pix)+0.1*x+0.2*randn(n,1);
hh=2*0.3^2; l=0.1; x2=x.^2;
k=exp(-(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’)/hh);
ky=k*y; A=inv(k^2+eye(n)); t0=zeros(n,1); z=t0; u=t0;
for o=1:1000
t=A*(ky+z-u); z=max(0,t+u-l)+min(0,t+u+l); u=u+t-z;
if norm(t-t0)<0.0001, break, end
t0=t;
end
N=1000; X=linspace(-3,3,N)’;
K=exp(-(repmat(X.^2,1,n)+repmat(x2’,N,1)-2*X*x’)/hh); F=K*t;
figure(1); clf; hold on; axis([-2.8 2.8 -1 1.5]);
plot(X,F,’g-’); plot(x,y,’bo’); sum(abs(t)<0.001)
FIGURE 24.4
MATLAB code of ℓ1-constrained LS by ADMM for Gaussian kernel model.
FIGURE 24.5
Example of ℓ1-constrained LS for Gaussian kernel model. 38 out
of 50 parameters are zero.
Fig. 23.7). However, 38 out of 50 parameters are zero in ℓ1-constrained LS, while
all 50 parameters are nonzero in ℓ2-constrained LS.
When the stochastic gradient algorithm (see Fig. 22.6) is used to obtain the LS
solution for linear-in-parameter model,

272
CHAPTER 24
SPARSE REGRESSION
fθ(x) =
b

j=1
θ jϕj(x) = θ⊤φ(x),
parameter θ is updated as
θ ←−θ + εφ(x)

y −fθ(x)

,
where ε > 0 is the step size and (x, y) is a randomly chosen training sample. To obtain
a sparse solution by the stochastic gradient algorithm, parameter θ = (θ1,. . . ,θb)⊤
may be thresholded once in every several gradient updates as follows [66]:
∀j = 1,. . . ,b,
θ j ←−

max(0,θ j −λε)
(θ j > 0),
min(0,θ j + λε)
(θ j ≤0).
24.3 FEATURE SELECTION BY SPARSE LEARNING
Let us apply sparse learning to the linear-in-input model for x = (x(1),. . . , x(d))⊤:
fθ(x) =
d

j=1
θ j x(j) = θ⊤x.
If θ j = 0, then the jth input variable x(j) does not appear in the final prediction model.
Thus, feature selection can be performed by sparse learning.
For example, suppose that the income of a person is modeled by
θ1 × Education + θ2 × Age + θ3 × Ability + θ4 × Parents’ income,
and sparse learning gives θ4 = 0. This means that parents’ income is not related to
the income of the child.
If such feature selection is naively performed, all 2d combinations of d features
x(1),. . . , x(d) need to be investigated, which is not tractable when d is large. In
practice, greedy strategies such as forward selection of adding a feature one by one
and backward elimination of deleting a feature one by one are often used. However,
since such greedy approaches do not consider the dependency between features,
they do not necessarily give good feature combinations. If sparse learning based
on the ℓ1-norm is used, dependency between features can be taken into account to
some extent. However, feature selection by sparse learning is possible only for the
linear-in-input model. If linear-in-parameter models are used, just a subset of basis
functions is selected, which is different from feature selection because every basis
function depends on all features in general.
24.4 VARIOUS EXTENSIONS
In this section, various extensions to ℓ1-constrained LS are introduced.

24.4 VARIOUS EXTENSIONS
273
24.4.1 GENERALIZED ℓ1-CONSTRAINED LS
For some matrix F, generalized ℓ1-constrained LS is defined as
min
θ
1
2 ∥y −Φθ∥2 subject to ∥Fθ∥1 ≤R2,
and its ADMM form is given by
min
θ,z
1
2 ∥y −Φθ∥2 + λ∥z∥1

subject to Fθ = z.
Then the ADMM update formulas are given by
θ(k+1) = (Φ⊤Φ + F⊤F)−1(Φ⊤y + F⊤z(k) −F⊤u(k)),
z(k+1) = max(0,Fθ(k+1) + u(k) −λ1) + min(0,Fθ(k+1)u(k) + λ1),
u(k+1) = u(k) + Fθ(k+1) −z(k+1).
A notable example of generalized ℓ1-constrained LS in statistics is fused lasso
[111]:
min
θ
1
2 ∥y −Φθ∥2 subject to
b−1

j=1
|θ j+1 −θ j| ≤R2,
which corresponds to
Fj, j′ =

1
(j′ = j + 1),
−1
(j′ = j),
0
(otherwise).
When Φ is the identity matrix, this problem is called the total variation denoising in
the signal processing community.
24.4.2 ℓp-CONSTRAINED LS
Let us generalize ℓ2-constrained LS and ℓ1-constrained LS to ℓp-constrained LS for
p ≥0:
∥θ∥p ≤R2.
Here, the ℓp-norm of vector θ = (θ1,. . . ,θb)⊤is defined as
∥θ∥p =

b
j=1 δ(θ j , 0)
(p = 0),
b
j=1 |θ j|p1
p
(0 < p < ∞),
max |θ1|,. . . ,|θb|	
(p = ∞),

274
CHAPTER 24
SPARSE REGRESSION
FIGURE 24.6
Unit ℓp-balls.
where
δ(θ j , 0) =

1
(θ j , 0),
0
(θ j = 0).
Thus, the ℓ0-norm gives the number of nonzero elements, and the ℓ∞-norm gives the
maximum element.
Fig. 24.6 shows the unit ℓp-balls (i.e. ∥θ∥p = 1) for p = 0.5, 1, 2, and 5. The
ℓp-ball has corners on the coordinate axes when p ≤1, and the ℓp-ball has a convex
shape when p ≥1. As illustrated in Fig. 24.2, having corners on the coordinate axes
is the key to produce a sparse solution. On the other hand, having a convex shape is
essential to obtain the global optimal solution. Thus, p = 1 is the best choice that
only allows sparsity induction under the convex formulation (see Fig. 24.7).
24.4.3 ℓ1 + ℓ2-CONSTRAINED LS
ℓ1-constrained LS is a useful method in practice, but it has several drawbacks.
When the number of parameters, b, is larger than the number of training samples,
n, the number of nonzero values in the solution of ℓ1-constrained LS is at most n.

24.4 VARIOUS EXTENSIONS
275
FIGURE 24.7
Properties of ℓp-constraint.
This is not a problem when a kernel model is used, because b = n:
fθ(x) =
n

j=1
θ jK(x, xj).
However, this could be a critical limitation when feature selection is performed using
the linear-in-input model (see Section 24.3),
fθ(x) =
d

j=1
θ j x(j) = θ⊤x,
because only n features can be selected at most.
When several basis functions are similar to each other and having group structure,
ℓ1-constrained LS tends to choose only one of them and ignore the rest. This means
that, when feature selection is performed using the linear-in-input model, only one
feature is chosen from a group of correlated features. When kernel models are used,
the kernel basis functions tend to have group structure, if training input samples
{xi}n
i=1 have cluster structure.
ℓ1 + ℓ2-constrained LS can overcome the above drawbacks of ℓ1-constrained LS,
which uses the sum of the ℓ1-norm and the ℓ2-norm as a constraint:
(1 −τ)∥θ∥1 + τ∥θ∥2 ≤R2,
where 0 ≤τ ≤1 controls the balance between the ℓ1-norm and ℓ2-norm constraints.
The (ℓ1 + ℓ2)-constraint is reduced to the ℓ1-constraint if τ = 0, and the (ℓ1 + ℓ2)-
constraint is reduced to the ℓ2-constraint if τ = 1. When 0 ≤τ < 1, the region
(1 −τ)∥θ∥1 + τ∥θ∥2 ≤R2 has corners on the coordinate axes.
Fig. 24.8 illustrates the unit (ℓ1 +ℓ2)-ball for balance parameter τ = 1/2, which is
similar to the unit ℓ1.4-ball. However, while the ℓ1.4-ball has no corner as the ℓ2-ball,
the (ℓ1 + ℓ2)-ball has corners. Thus, the (ℓ1 + ℓ2)-constraint tends to produce a sparse
solution.
Let us express the optimization problem of (ℓ1 + ℓ2)-constrained LS as
min
θ,z
1
2 ∥y −Φθ∥2 + η∥θ∥2 + λ∥z∥1

subject to θ = z,

276
CHAPTER 24
SPARSE REGRESSION
FIGURE 24.8
Unit (ℓ1 + ℓ2)-norm ball for balance parameter τ = 1/2, which is similar to the unit ℓ1.4-ball.
However, while the ℓ1.4-ball has no corner, the (ℓ1 + ℓ2)-ball has corners.
where λ,η ≥0. Then the solution of (ℓ1 + ℓ2)-constrained LS can be obtained almost
in the same way as ℓ1-constrained LS by ADMM (see Section 24.2), but only the
update equation for θ is changed as
θ(k+1) = (Φ⊤Φ + (η + 1)I)−1(Φ⊤y + z(k) −u(k)).
Even when the number of parameters is larger than the number of training
samples, i.e. b > n, (ℓ1 + ℓ2)-constrained LS can produce a solution with more
than n nonzero elements. Furthermore, (ℓ1 + ℓ2)-constrained LS tends to choose
features in a groupwise manner, i.e. all features in the same group tend to be discarded
simultaneously. However, in addition to the regularization parameter λ, the balance
parameter τ needs to be tuned, which is cumbersome in practice.
In statistics, (ℓ1 +ℓ2)-constrained LS is referred to as elastic-net regression [124].
24.4.4 ℓ1,2-CONSTRAINED LS
Suppose that the parameter vector θ = (θ1,. . . ,θb)⊤has group structure as
θ = (θ(1)⊤,. . . ,θ(t)⊤)⊤,
where θ(j) ∈Rb j and t
j=1 bj = b. Then, LS with ℓ1,2-constraint,
t
j=1
∥θ(j)∥≤R,

24.4 VARIOUS EXTENSIONS
277
(a) ℓ2-constraint
(b) ℓ1-constraint
(c) ℓ1,2-constraint
FIGURE 24.9
Constraints in three-dimensional parameter space.
tends to give a groupwise sparse solution, i.e. some of parameter subvector θ(j)
becomes zero. Such a learning method is called ℓ1,2-constrained LS.
The ℓ2-constraint, ℓ1-constraint, and ℓ1,2-constraint in three-dimensional param-
eter space are illustrated in Fig. 24.9. The ℓ1,2-region consists of the ℓ2-part and
the ℓ1-part, and if the solution is on the top/bottom peak of the ℓ1,2-region, it will
be sparse as (θ1,θ2,θ3) = (0,0,±R). On the other hand, if the solution is on the
circle in the (θ1,θ2)-plane, the solution will be sparse as (θ1,θ2,θ3) = (a,b,0) for
a2 + b2 = R2.
In statistics, ℓ1,2-constrained LS is referred to as group-lasso regression [122].
The ℓ1,2-constraint plays an important role in advanced machine learning topics
such as multitask feature selection (Section 34.3), structural change detection
(Section 39.2.2), and multiple kernel learning [11].

278
CHAPTER 24
SPARSE REGRESSION
The trace norm of matrix Θ ∈Rd1×d2, denoted by ∥Θ∥tr, is defined as
∥Θ∥tr =
min(d1,d2)

k=1
σk,
where σk is a singular value of Θ (see Fig. 22.2). The trace norm is also
referred to as the nuclear norm. Given that singular values are non-negative,
the trace norm ∥Θ∥tr can be regarded as the ℓ1-norm on singular values. Thus,
using the trace norm ∥Θ∥tr as a regularizer tends to produce a sparse solution
on singular values (see Chapter 24), implying that Θ becomes a low-rank
matrix. Note that if Θ is squared and diagonal, ∥Θ∥tr is reduced to the ℓ1-
norm of the diagonal entries. Thus, the trace norm can be regarded as an
extension of the ℓ1-norm from vectors to matrices.
FIGURE 24.10
Trace norm of a matrix.
24.4.5 TRACE NORM CONSTRAINED LS
So far, data samples {xi}n
i=1 are assumed to be d-dimensional vectors. However, in
some applications such as image analysis, a sample xi may be a matrix (correspond-
ing to a two-dimensional image). Such matrix samples can be naively handled if
they are vectorized (see Fig. 6.5), but then the two-dimensional structure of images
is completely lost. Here, a regularization technique for matrix samples is introduced
[112].
Suppose that matrix-input scalar-output paired samples {(Xi, yi)}n
i=1 are given as
training data, where Xi ∈Rd1×d2 and yi ∈R. Let us employ the linear-in-input
model for parameter matrix Θ ∈Rd1×d2,
fΘ(X) = tr  Θ⊤X ,
which is equivalent to the vectorized linear-in-input model vec(Θ)⊤vec(X). To utilize
the two-dimensional structure, the trace norm ∥Θ∥tr is employed as a constraint (see
Fig. 24.10):
min
Θ∈Rd1×d2
1
2
n

i=1

yi −fΘ(Xi)
2 subject to ∥Θ∥tr ≤R.
Note that this optimization problem is convex, and thus the global optimal solution
can be easily obtained, e.g, by the proximal gradient method (see Fig. 34.8). Thanks
to the trace norm constraint, the solution of Θ tends to be a low-rank matrix.

CHAPTER
ROBUST REGRESSION25
CHAPTER CONTENTS
Nonrobustness of ℓ2-Loss Minimization .......................................... 279
ℓ1-Loss Minimization............................................................. 280
Huber Loss Minimization......................................................... 282
Definition................................................................... 282
Stochastic Gradient Algorithm ............................................. 283
Iteratively Reweighted LS .................................................. 283
ℓ1-Constrained Huber Loss Minimization .................................. 286
Tukey Loss Minimization ......................................................... 290
Although LS is useful in various practical applications, it is sensitive to outliers,
samples containing large noise. In this chapter, alternative learning methods that
possess high robustness against outliers are introduced.
25.1 NONROBUSTNESS OF ℓ2-LOSS MINIMIZATION
A LS solution for straight-line model fθ(x) = θ1 + θ2x obtained from 10 training
samples is illustrated in Fig. 25.1. If there is no outlier as in Fig. 25.1(a), a good
solution can be obtained by LS. However, if there exists an outlier as in Fig. 25.1(b),
the solution is strongly affected by the outlier.
When a large number of training samples are handled, it would be natural to
consider that more or less outliers are included. In such a situation, LS is less reliable,
as illustrated in Fig. 25.1. One way to cope with outliers is to remove them in advance,
which will be discussed in Chapter 38. Another approach is to perform learning so
that the solution is less sensitive to outliers. The ability to be less sensitive to outliers
is referred to as robustness. In this chapter, robust learning methods are explored.
In the LS method, the goodness of fit to training samples is measured by the
ℓ2-loss function:
JLS(θ) = 1
2
n

i=1
r2
i ,
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00036-4
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
279

280
CHAPTER 25
ROBUST REGRESSION
(a) Without outliers
(b) With an outlier
FIGURE 25.1
LS solution for straight-line model fθ(x) = θ1 + θ2x, which is strongly affected by an outlier.
where ri denotes the residual for the ith training sample (xi, yi):
ri = yi −fθ(xi).
Since the ℓ2-loss squares the error, large error tends to be magnified significantly.
For example, in Fig. 25.1 the output of the rightmost training sample is moved from
y ≈3 to y ≈−4. Then the residual 7 has the “power” of 72 = 49 to pull the regression
function down, which causes significant change in the solution.
25.2 ℓ1-LOSS MINIMIZATION
To cope with the nonrobustness of the LS method, let us employ the ℓ1-loss or the
absolute loss (Fig. 25.2):
θLA = argmin
θ
JLA(θ),
where JLA(θ) =
n

i=1
|ri|.
This learning method is called ℓ1-loss minimization or least absolute deviations
regression.
An example of ℓ1-loss minimization is shown in Fig. 25.3, which is obtained in the
same setup as in Fig. 25.1 (how to compute the solution of least absolute deviations
will be explained later in Section 25.3). This shows that ℓ1-loss minimization is much
more robust against outliers than ℓ2-loss minimization, and ℓ1-loss minimization
gives almost the same solution as ℓ2-loss minimization if there are no outliers.
For a constant model fθ(x) = θ, the LS method gives the mean of training output
samples {yi}n
i=1:
θLS = argmin
θ
n

i=1
(θ −yi)2 = mean

{yi}n
i=1

.

25.2 ℓ1-LOSS MINIMIZATION
281
FIGURE 25.2
ℓ2-loss and ℓ1-loss. The ℓ2-loss magnifies large
residuals.
(a) Without outliers
(b) With an outlier
FIGURE 25.3
Solution of least absolute deviations for straight-line model fθ(x) = θ1+θ2x for the same training
samples as Fig. 25.1. Least absolute deviations give a much more robust solution than LS.
On the other hand, least absolute deviations gives the median (see Section 2.4.1) of
the training output samples {yi}n
i=1:
θLA = argmin
θ
n

i=1
|θ −yi| = median

{yi}n
i=1

.
The mean of {yi}n
i=1 is affected if one of the sample values is changed, while the
median is not affected by changing sample values as long as the order of samples is

282
CHAPTER 25
ROBUST REGRESSION
FIGURE 25.4
Huber loss, with threshold η = 1.
unchanged. The robustness of least absolute deviations is brought by such a property
of the ℓ1-loss.
25.3 HUBER LOSS MINIMIZATION
The ℓ1-loss gives a robust solution, but high robustness implies that learning is
performed without fully utilizing the information brought by training samples. In
the extreme case, the estimator that does not learn anything from training samples
but just always gives zero is the most robust approach, which is clearly meaningless.
This extreme example implies that enhancing the robustness too much can degrade
the performance of learning from inlier samples. The notion of efficiency discussed
in Section 13.3 concerns the variance of an estimator, which can be interpreted as
how much information can be learned from given training samples. Thus, achieving
robustness and efficiency in a balanced way is crucial in practice. In this section, the
Huber loss [58] is introduced, which can control the balance between robustness and
efficiency.
25.3.1 DEFINITION
The Huber loss is defined with the ℓ2-loss and the ℓ1-loss as follows (Fig. 25.4):
ρHuber(r) =

r2/2
(|r| ≤η),
η|r| −η2/2
(|r| > η).
This means that the Huber loss is reduced to the ℓ2-loss if the absolute residual |r|
is less than or equal to a threshold η (i.e., that sample may be an inlier), while it is
reduced to the ℓ1-loss if the absolute residual |r| is larger than a threshold η (i.e., that

25.3 HUBER LOSS MINIMIZATION
283
sample may be an outlier). Note that the ℓ1-loss is modified as η|r| −η2/2 so that it
is smoothly connected to the ℓ2-loss r2/2. The method of Huber loss minimization is
given by
min
θ
J(θ),
where J(θ) =
n

i=1
ρHuber(ri).
(25.1)
25.3.2 STOCHASTIC GRADIENT ALGORITHM
Let us consider the linear-in-parameter model:
fθ(x) =
b

j=1
θ jϕj(x) = θ⊤φ(x).
Since the Huber loss is once differentiable as
ρ′
Huber(r) =

r
(|r| ≤η),
−η
(r < −η),
η
(r > η),
the solution of Huber loss minimization may be obtained by the stochastic gradient
algorithm (see Section 15.3) as follows:
θ ←−θ + ε ∂ρHuber
∂θ
θ=θ
,
where (xi, yi) is a randomly chosen training sample and ε > 0 is the step size.
25.3.3 ITERATIVELY REWEIGHTED LS
Another approach is iteratively reweighted LS, which considers the following
quadratic upper bound of the absolute-value part of the Huber loss:
η|r| −η2
2 ≤η
2cr2 + ηc
2 −η2
2
for c > 0.
As illustrated in Fig. 25.5, this quadratic upper bound touches the Huber loss at
r = ±c.
Let us consider an iterative optimization procedure and construct an upper bound
for ci = |ri|:
η|ri| −η2
2 ≤
η
2|ri|r2
i + η|ri|
2
−η2
2 ,

284
CHAPTER 25
ROBUST REGRESSION
FIGURE 25.5
Quadratic upper bound ηr2
2c + ηc
2 −η2
2 of Huber loss ρHuber(r) for
c > 0, which touches each other at r = ±c.
where ri denotes the residual of the current solution for the ith training sample
(xi, yi). If the absolute-value part of the Huber loss is upper-bounded by this, Huber
loss minimization problem Eq. (25.1) yields the following weighted LS problem:
θ = argmin
θ
J(θ),
where J(θ) = 1
2
n

i=1
wir2
i + C,
where C = 
i:|ri |>η(η|ri|/2−η2/2) is a constant that is independent of θ, and weight
wi is defined as follows (Fig. 25.6):
wi =

1
(|ri| ≤η),
η/|ri|
(|ri| > η).
As explained in Section 22.1, the solution of weighted LS is given analytically as
θ = (Φ⊤
WΦ)−1Φ⊤
W y,
(25.2)
where 
W denotes the diagonal matrix with diagonal elements w1,. . . , wn.
Since the upper bound J touches the original objective function J at current
solution θ = θ,
J(θ) = J(θ)
holds. Since θ is the minimizer of J,
J(θ) ≥J(θ).

25.3 HUBER LOSS MINIMIZATION
285
FIGURE 25.6
Weight functions for Huber loss minimization
and Tukey loss minimization.
FIGURE 25.7
Updated solution θ is no worse than current solution θ.
Furthermore, J upper-bounds J and thus
J(θ) ≥J(θ)
holds. Summarizing the above relations,
J(θ) = J(θ) ≥J(θ) ≥J(θ)
holds, meaning that the updated solution θ is no worse than the current solution θ in
terms of the objective value J (Fig. 25.7). By iterating this upper bound minimization,
the solution of Huber loss minimization can be obtained. The pseudocode of this
algorithm, called iteratively reweighted LS, is provided in Fig. 25.8. If singular value

286
CHAPTER 25
ROBUST REGRESSION
1. Initialize parameter θ, e.g., by ordinary LS as
θ ←−(Φ⊤Φ)−1Φ⊤y.
2. Compute the weight matrix W from the current solution θ as
W = diag (w1,. . . ,wn) ,
where wi =

1
(|ri| ≤η),
η/|ri|
(|ri| > η),
and ri = yi −fθ(xi) is the residual.
3. Compute the solution θ based on the weight matrix W as
θ ←−(Φ⊤WΦ)−1Φ⊤W y.
4. Iterate 2–3 until convergence.
FIGURE 25.8
Iteratively reweighted LS for Huber loss minimization.
decomposition (see Fig. 22.2) of design matrix,
Φ =
b

j=1
κ jψjϕ⊤
j ,
is computed in advance, update of θ can be performed more efficiently as
θ ←−
b

j=1
ψ⊤
j W y
κ jψ⊤
j Wψj
ϕj.
A MATLAB code of iteratively reweighted LS for Huber loss minimization is
provided in Fig. 25.9, and its behavior is illustrated in Fig. 25.10. This shows that
only two iterations give almost the final solution. The iteration converges after four
iterations and a robust solution can be obtained.
When the threshold η is taken to be sufficiently small, the Huber loss may be
regarded as a smooth approximation to the ℓ1-loss. Thus, the solution of ℓ1-loss
minimization can be approximately obtained by the above iteratively weighted LS
algorithm.
25.3.4 ℓ1-CONSTRAINED HUBER LOSS MINIMIZATION
As explained in Chapter 24, the ℓ1-constraint tends to give a sparse solution. Here,
ℓ1-constrained Huber loss minimization is considered:

25.3 HUBER LOSS MINIMIZATION
287
n=10; N=1000; x=linspace(-3,3,n)’; X=linspace(-4,4,N)’;
y=x+0.2*randn(n,1); y(n)=-4;
p(:,1)=ones(n,1); p(:,2)=x; t=p\y; e=1;
for o=1:1000
r=abs(p*t-y); w=ones(n,1); w(r>e)=e./r(r>e);
t0=(p’*(repmat(w,1,2).*p))\(p’*(w.*y));
if norm(t-t0)<0.001, break, end
t=t0;
end
P(:,1)=ones(N,1); P(:,2)=X; F=P*t;
figure(1); clf; hold on; axis([-4 4 -4.5 3.5]);
plot(X,F,’g-’); plot(x,y,’bo’);
FIGURE 25.9
MATLAB code of iteratively reweighted LS for Huber loss minimization. Straight-line model
fθ(x) = θ1 + θ2x is used, with threshold η = 1.
min
θ
n

i=1
ρHuber(ri)
subject to ∥θ∥1 ≤R2,
which produces a robust and sparse solution.
As shown above, the optimization problem of Huber loss minimization can
be solved by iteratively reweighted LS, which was obtained by a quadratic upper
bounding technique. Similarly, ℓ1-regularized LS, whose solution was obtained by
ADMM in Section 24.2, can also be solved by iteratively reweighted LS. More
specifically, the absolute value of parameter θ can be upper-bounded by a quadratic
function as
|θ| ≤θ2
2c + c
2
for c > 0,
which touches the absolute value at θ = ±c (see Fig. 25.11).
Let us consider an iterative optimization procedure and construct an upper bound
for cj = θ j:
|θ j| ≤|θ j|†
2 θ2
j + |θ j|
2 ,
where θ j is the current solution and † denotes the generalized inverse (see Fig. 22.1).
If the ℓ1-norm of parameter vector θ = (θ1,. . . ,θb)⊤is upper-bounded by this, the

288
CHAPTER 25
ROBUST REGRESSION
(a) Initial value
(b) After 1 iteration
(c) After 2 iterations
(d) After 4 iterations
FIGURE 25.10
Examples of iteratively reweighted LS for Huber loss minimization. Straight-line model fθ(x) =
θ1 + θ2x is used, with threshold η = 1.
ℓ1-regularized LS problem,
min
θ
1
2 ∥y −Φθ∥2 + λ∥θ∥1

,
yields the following generalized ℓ2-regularized LS:
θ = argmin
θ
J(θ),
where J(θ) = JLS(θ) + λ
2 θ⊤Θ
†θ + C,
where C = b
j=1 |θ j|/2 is a constant that is independent of θ. As explained in
Section 23.2, the solution of generalized ℓ2-regularized LS is given analytically as
θ = (Φ⊤Φ + λΘ
†)−1Φ⊤y.

25.3 HUBER LOSS MINIMIZATION
289
FIGURE 25.11
Quadratic upper bound θ2
2c + c
2 of absolute value |θ| for c > 0,
which touches each other at θ = ±c.
1. Initialize parameter θ, e.g., randomly or by ordinary LS as
θ ←−Φ†y.
2. Compute the weight matrix W and regularization matrix Θ from the
current solution θ as
W = diag (w1,. . . ,wn)
and Θ = diag (|θ1|,. . . ,|θb|) ,
where weight wi is defined using residual ri = yi −fθ(xi) as
wi =

1
(|ri| ≤η),
η/|ri|
(|ri| > η).
3. Compute the solution θ based on the weight matrix W regularization
matrix Θ as
θ ←−(Φ⊤WΦ + λΘ†)−1Φ⊤W y.
4. Iterate 2–3 until convergence.
FIGURE 25.12
Iteratively reweighted LS for ℓ1-regularized Huber loss minimization.

290
CHAPTER 25
ROBUST REGRESSION
n=50; x=linspace(-3,3,n)’; pix=pi*x;
y=sin(pix)./(pix)+0.1*x+0.2*randn(n,1); y(n/2)=-0.5;
hh=2*0.3^2; l=0.1; e=0.1; t=randn(n,1); x2=x.^2;
k=exp(-(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’)/hh);
for o=1:1000
r=abs(k*t-y); w=ones(n,1); w(r>e)=e./r(r>e);
Z=k*(repmat(w,1,n).*k)+l*pinv(diag(abs(t)));
t0=(Z+0.000001*eye(n))\(k*(w.*y));
if norm(t-t0)<0.001, break, end
t=t0;
end
N=1000; X=linspace(-3,3,N)’;
K=exp(-(repmat(X.^2,1,n)+repmat(x2’,N,1)-2*X*x’)/hh); F=K*t;
figure(1); clf; hold on; axis([-2.8 2.8 -1 1.5]);
plot(X,F,’g-’); plot(x,y,’bo’);
FIGURE 25.13
MATLAB code of iteratively reweighted LS for ℓ1-regularized Huber loss minimization with
Gaussian kernel model.
The iteratively reweighted LS algorithms for ℓ1-regularized LS and Huber loss
minimization can be combined to obtain a sparse and robust solution, as summarized
in Fig. 25.12.
A MATLAB code of iteratively reweighted LS for ℓ1-regularized Huber loss
minimization is provided in Fig. 25.13, where the Gaussian kernel model,
fθ(x) =
n

j=1
θ j exp *
,
−∥x −xj∥2
2h2
+
-
,
is used and the Gaussian bandwidth is set at h = 0.3. Since plain LS is numerically
unstable, it is stabilized by adding 10−6 to the diagonal elements of Φ⊤WΦ (more
precisely, KWK since the Gaussian kernel model is used). The behavior of ℓ1-
regularized Huber loss minimization is illustrated in Fig. 25.14. Regardless of the
presence or absence of the ℓ1-regularizer, ℓ2-loss minimization is strongly affected by
the outlier at around x = 0, while Huber loss minimization can successfully suppress
its influence. With the ℓ1-regularizer, ℓ2-loss minimization gives 38 zero parameters
out of 50 parameters, and Huber loss minimization gives 36 zero parameters.
25.4 TUKEY LOSS MINIMIZATION
The Huber loss nicely controls the balance between efficiency and robustness by
combining the ℓ1-loss and the ℓ2-loss. However, as long as the ℓ1-loss is used, strong

25.4 TUKEY LOSS MINIMIZATION
291
(λ,η) = (0,∞)
(a) ℓ2-loss minimization
(λ,η) = (0.1,∞)
(b) ℓ1-regularized ℓ2-loss minimization
(λ,η) = (0,0.1)
(c) Huber loss minimization
(λ,η) = (0.1,0.1)
(d) ℓ1-regularized Huber loss minimization
FIGURE 25.14
Example of ℓ1-regularized Huber loss minimization with Gaussian kernel model.
FIGURE 25.15
Tukey loss, with threshold η = 3.

292
CHAPTER 25
ROBUST REGRESSION
(a) Huber loss minimization
(b) Tukey loss minimization 1
(c) Tukey loss minimization 2
FIGURE 25.16
Example of Tukey loss minimization. Tukey loss minimization gives more robust solutions than
Huber loss minimization, but only a local optimal solution can be obtained.
outliers can still affect the solution significantly. Indeed, as illustrated in Fig. 25.6, the
weight wi used in Huber loss minimization is not zero for large absolute residuals.
In such a hard circumstance, the Tukey loss [73] is useful (Fig. 25.15):
ρTukey(r) =


1 −1 −r2/η23
η2/6
(|r| ≤η),
η2/6
(|r| > η).
The Tukey loss is constant η2/6 if the absolute residual |r| is larger than a threshold
η. Thus, Tukey loss minimization is expected to be more robust than Huber loss
minimization.
However, the Tukey loss is not a convex function (see Fig. 8.3). Thus, there may
exist multiple local optimal solutions and finding the global one is not straightfor-
ward. In practice, iteratively reweighted LS with the following weight is used to find
a local optimal solution (Fig. 25.6):
w =

 1 −r2/η22
(|r| ≤η),
0
(|r| > η).
Since the Tukey weight is zero for |r| > η, Tukey loss minimization is not at all
influenced by strong outliers.
A MATLAB code of iteratively reweighted LS for Tukey loss minimization is
essentially the same as that for Huber loss minimization shown in Fig. 25.9. Only the
weight computation for the Huber loss,
w=ones(n,1); w(r>e)=e./r(r>e);
is replaced with that for the Tukey loss:

25.4 TUKEY LOSS MINIMIZATION
293
w=zeros(n,1); w(r<=e)=(1-r(r<=e).^2/e^2).^2;
The behavior of Tukey loss minimization is illustrated in Fig. 25.16, where
the threshold is set at η = 1. In this example, Tukey loss minimization gives an
even more robust solution than Huber loss minimization. However, since Tukey
loss minimization is a nonconvex optimization problem, different solutions may be
obtained if the initial solution is changed or training samples are slightly perturbed.
Indeed, as illustrated in Fig. 25.16(c), slightly changing the noise included in training
output samples gives another local optimal solution, which is significantly different
from the original one in Fig. 25.16(b).
For general differentiable symmetric loss ρ(r), iteratively reweighted LS with
weight wi = ρ′(ri)/ri gives a (local) optimal solution, where ρ′(r) denotes the
derivative of ρ(r) [57].


CHAPTER
LEAST SQUARES
CLASSIFICATION 26
CHAPTER CONTENTS
Classification by LS Regression .................................................. 295
0/1-Loss and Margin ............................................................. 297
Multiclass Classification.......................................................... 300
In Chapter 22–Chapter 25, various regression techniques for estimating real-valued
output were introduced. In this chapter, the classification problem of estimating
category-valued output is tackled by the LS regression method introduced in
Chapter 22. In the context of classification, input x is referred to as a pattern and
output y is referred to as a label.
26.1 CLASSIFICATION BY LS REGRESSION
Let us consider a binary classification problem where the label y takes either +1 or
−1. Then the classification problem can be regarded as approximating a binary-valued
function f (x) ∈{+1,−1} (Fig. 26.1). Such a function may be naively learned by the
LS method introduced in Chapter 22:
θ = argmin
θ
1
2
n

i=1

fθ(xi) −yi
2.
Then, an estimate y of the label y for a test pattern x is obtained by the sign of the
learned function:
y =

+1
( fθ(x) ≥0),
−1
( fθ(x) < 0).
(26.1)
Various extensions of the LS method introduced in Chapter 23, Chapter 24, and
Chapter 25 may also be utilized for classification.
A MATLAB code of classification by ℓ2-regularized LS for the Gaussian kernel
model,
fθ(x) =
n

j=1
θ j exp *
,
−∥x −xj∥2
2h2
+
-
,
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00037-6
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
295

296
CHAPTER 26
LEAST SQUARES CLASSIFICATION
FIGURE 26.1
Binary classification as function approximation.
n=200; a=linspace(0,4*pi,n/2);
u=[a.*cos(a) (a+pi).*cos(a)]’+rand(n,1);
v=[a.*sin(a) (a+pi).*sin(a)]’+rand(n,1);
x=[u v]; y=[ones(1,n/2) -ones(1,n/2)]’;
x2=sum(x.^2,2); hh=2*1^2; l=0.01;
k=exp(-(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’)/hh);
t=(k^2+l*eye(n))\(k*y);
m=100; X=linspace(-15,15,m)’; X2=X.^2;
U=exp(-(repmat(u.^2,1,m)+repmat(X2’,n,1)-2*u*X’)/hh);
V=exp(-(repmat(v.^2,1,m)+repmat(X2’,n,1)-2*v*X’)/hh);
figure(1); clf; hold on;
contourf(X,X,sign(V’*(U.*repmat(t,1,m))));
plot(x(y==1,1),x(y==1,2),’bo’);
plot(x(y==-1,1),x(y==-1,2),’rx’);
colormap([1 0.7 1; 0.7 1 1]); axis([-15 15 -15 15]);
FIGURE 26.2
MATLAB code of classification by ℓ2-regularized LS for Gaussian kernel model.
is provided in Fig. 26.2, and its behavior is illustrated in Fig. 26.3. This shows that
entangled data can be successfully classified by ℓ2-regularized LS.
Let us consider the linear-in-input model,
fθ(x) = θ⊤x,
and assign {+1/n+,−1/n−} to labels instead of {+1,−1}, where n+ and n−denote
the number of positive and negative training samples, respectively. The LS solution
in this setup is given by
θLS = Σ
−1(µ+ −µ−),

26.2 0/1-LOSS AND MARGIN
297
FIGURE 26.3
Example of classification by ℓ2-regularized LS
for Gaussian kernel model.
where
Σ = 1
n
n

i=1
xi x⊤
i , µ+ = 1
n+
n

i:yi=+1/n+
xi,
and
µ−= 1
n−
n

i:yi=−1/n−
xi.
This solution actually agrees with FDA explained in Section 12.4, which was proved
to achieve the optimal classification performance when samples in the positive and
negative classes follow the Gaussian distributions with a common covariance matrix.
Thus, LS classification for the linear-in-input model bridges the generative and
discriminative approaches.
26.2 0/1-LOSS AND MARGIN
As shown in Eq. (26.1), classification is performed based on the sign of a learned
function, not the value of the function itself. Thus, in classification, the 0/1-loss
would be more natural than the ℓ2-loss:
1
2

1 −sign ( fθ(x)y)

.
The 0/1-loss can be equivalently expressed as
δ

sign ( fθ(x)) , y

=

1
(sign ( fθ(x)) , y),
0
(sign ( fθ(x)) = y),

298
CHAPTER 26
LEAST SQUARES CLASSIFICATION
FIGURE 26.4
0/1-loss and ℓ2-loss as functions of margin m = fθ(x)y.
which means that the 0/1-loss takes 1 if x is misclassified and 0 if x is correctly
classified. Thus, the 0/1-loss counts the number of misclassified samples.
The 0/1-loss is plotted as a function of m = fθ(x)y in Fig. 26.4, which takes
0 if m > 0 and 1 if m ≤0. m > 0 means that fθ(x) and y have the same sign,
i.e., the sample x is classified correctly. On the other hand, m ≤0 corresponds
to misclassification of x. Thus, the value of the 0/1-loss does not depend on the
magnitude of m, but only its sign. However, if m is decreased from a positive value,
the 0/1-loss suddenly takes 1 if m goes below zero. Thus, having a larger m would
be safer. For this reason, m = fθ(x)y is called the margin for sample (x, y).
As illustrated in Fig. 26.4, the 0/1-loss is a binary-valued function and the
derivative is zero everywhere, except at zero where the 0/1-loss is not differentiable.
For this reason, 0/1-loss minimization for a rich enough model fθ(x),
min
θ
1
2
n

i=1

1 −sign ( fθ(xi)yi)

,
is essentially a discrete optimization problem of assigning either +1 or −1 to each
training sample. This is a combinatorial problem with 2n possibilities, and the
number of combinations grows exponentially with respect to n. Therefore, it cannot
be solved when n is not small. Even if a solution that vanishes the 0/1-loss can be
found, the solution may not be unique due to the flatness of the 0/1-loss.
In regression, the ℓ2-loss is commonly used as training and generalization error
metrics. On the other hand, in classification, although it would be natural to use the
0/1-loss as a generalization error metric since it corresponds to the misclassification
rate, the 0/1-loss cannot be directly used as a training error metric due to computa-
tional intractability. For this reason, a surrogate loss is usually employed for training
a classifier. In the LS classification method, the ℓ2-loss is used as a surrogate for the
0/1-loss.

26.2 0/1-LOSS AND MARGIN
299
FIGURE 26.5
Example of ℓ2-loss minimization for linear-in-input model.
Since the ℓ2-loss has a positive slope when m
>
1, the
obtained solution contains some classification error even though
all samples can be correctly classified in principle.
When y ∈{+1,−1}, y2 = 1 and 1/y = y hold. Then the ℓ2-loss, which is defined
as the square of the residual r = y −fθ(x), can be expressed in terms of the margin
m = fθ(x)y as
r2 =

y −fθ(x)
2 = y2
1 −fθ(x)/y
2 =

1 −fθ(x)y
2
=

1 −m
2.
The ℓ2-loss is plotted as a function of margin m in Fig. 26.4. The ℓ2-loss is positive
when m < 1, and it has a negative slope as a function of m. Therefore, differently from
the 0/1-loss, the ℓ2-loss allows gradient-based learning to reduce the error. However,
the ℓ2-loss is positive also for m > 1 and it has a positive slope as a function of m.
Thus, when m > 1, ℓ2-loss minimization reduces the margin toward m = 1.
Since m = 1 still correctly classifies the sample, reducing the margin toward
m = 1 when m > 1 does not cause any problem at a glance. However, if ℓ2-loss
minimization is applied to the data illustrated in Fig. 26.5, the obtained solution
contains some classification error, even though the samples can be linearly separated
in principle. This is actually caused by the fact that the ℓ2-loss has a positive slope
when m > 1. As a surrogate for the 0/1-loss, a monotone decreasing function of
margin m would be reasonable.
In Fig. 26.6, various popular surrogate loss functions are plotted, which will be
explained in the following chapters.

300
CHAPTER 26
LEAST SQUARES CLASSIFICATION
FIGURE 26.6
Popular surrogate loss functions.
26.3 MULTICLASS CLASSIFICATION
So far, binary classification was considered. However, in many practical applications
of pattern recognition, the number of classes, c, may be more than two. For example,
the number of classes is c = 26 in hand-written alphabet recognition. In this
section, two methods to reduce a multiclass classification problem into a set of binary
classification problems are explained. A method to directly solve the multiclass
classification problems will be discussed in Chapter 27 and Chapter 28.
One way to reduce a multiclass classification problem into binary classification
problems is the one-versus-rest method (Fig. 26.7), which considers c binary
classification problems of one class versus the other classes. More specifically, for
y = 1,. . . ,c, the yth binary classification problem assigns label +1 to samples in class
y and −1 to samples in all other classes. Let fy(x) be a learned decision function for
the yth binary classification problem. Then, test sample x is classified into class y
that gives the highest score:
y = argmax
y=1,...,c
fy(x).
Another way to reduce a multiclass classification problem into binary classifica-
tion problems is the one-versus-one method (Fig. 26.8), which considers c(c −1)/2
binary classification problems of one class versus another class. More specifically, for
y, y′ = 1,. . . ,c, the (y, y′)th binary classification problem assigns label +1 to samples
in class y and −1 to samples in class y′. Let fy,y′(x) be a learned decision function

26.3 MULTICLASS CLASSIFICATION
301
FIGURE 26.7
One-versus-rest reduction of multiclass classification
problem.
FIGURE 26.8
One-versus-one reduction of multiclass classification
problem.
for the (y, y′)th binary classification problem. Then, test sample x is classified into
class y that gathers the highest votes:
fy,y′(x) ≥0 ⇒Vote for class y,
fy,y′(x) < 0 ⇒Vote for class y′.
One-versus-rest consists only of c binary classification problems, while one-
versus-one consists of c(c −1)/2 binary classification problems. Thus, one-versus-
rest is more compact than one-versus-one. However, a binary classification problem
in one-versus-one involves samples only in two classes, while that in one-versus-
rest involves samples in all classes. Thus, each binary classification problem in one-
versus-one may be solved more efficiently than that in one-versus-rest. Suppose
that each class contains n/c training samples, and the computational complexity of
classifier training is linear with respect to the number of training samples. Then the
total computational complexity for one-versus-rest and one-versus-one is both O(cn).
However, if classifier training takes super-linear time, which is often the case in many

302
CHAPTER 26
LEAST SQUARES CLASSIFICATION
classification algorithms, one-versus-one is computationally more efficient than one-
versus-rest.
One-versus-one would also be advantageous in that each binary classification
problem is balanced. More specifically, when each class contains n/c training
samples, each binary classification problem in one-versus-one contains n/c positive
and n/c negative training samples, while each binary classification problem in one-
versus-rest contains n/c positive and n(c −1)/c negative training samples. The latter
situation is often referred to as class imbalance, and obtaining high classification
accuracy in the imbalanced case is usually more challenging than the balanced case.
Furthermore, each binary classification problem in one-versus-one would be simpler
than one-versus-rest, because the “rest” class in one-versus-rest usually possesses
multimodality, which makes classifier training harder.
On the other hand, one-versus-one has a potential limitation that voting can be
tied: for example, when c = 3,
f1,2(x) ≥0 ⇒Vote for class 1,
f2,3(x) ≥0 ⇒Vote for class 2,
f1,3(x) < 0 ⇒Vote for class 3.
In such a situation, weighted voting according to the value of fy,y′(x) could be a
practical option. However, it is not clear what the best way to weight is.
As discussed above, both one-versus-rest and one-versus-one approaches have
pros and cons. A method to directly solve multiclass classification problems will
be introduced in Chapter 27 and Chapter 28. However, the direct method does
not necessarily perform better than the reduction approaches, because multiclass
classification problems are usually more complicated than binary classification
problems. In practice, the best approach should be selected depending on the target
problem and other constraints.

CHAPTER
SUPPORT VECTOR
CLASSIFICATION 27
CHAPTER CONTENTS
Maximum Margin Classification .................................................. 303
Hard Margin Support Vector Classification ................................. 303
Soft Margin Support Vector Classification.................................. 305
Dual Optimization of Support Vector Classification ............................... 306
Sparseness of Dual Solution...................................................... 308
Nonlinearization by Kernel Trick.................................................. 311
Multiclass Extension ............................................................. 312
Loss Minimization View .......................................................... 314
Hinge Loss Minimization ................................................... 315
Squared Hinge Loss Minimization ......................................... 316
Ramp Loss Minimization ................................................... 318
In the previous chapter, LS regression was shown to be used also for classification.
However, due to the nonmonotonicity of the ℓ2-loss function, classification by ℓ2-
loss minimization is rather unnatural as a surrogate for the 0/1-loss. In this chapter, a
classification algorithm called the support vector machine is introduced, which uses
a more natural loss function called the hinge loss.
27.1 MAXIMUM MARGIN CLASSIFICATION
In this section, a classification algorithm called the support vector machine is
introduced, which is based on the margin maximization principle [115].
27.1.1 HARD MARGIN SUPPORT VECTOR
CLASSIFICATION
Let us consider a linear-in-input binary classifier:
fw,γ(x) = w⊤x + γ,
(27.1)
where w and γ are the parameters of the model which correspond to the normal
vector and the intercept of the decision boundary, respectively (Fig. 27.1). If w and
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00038-8
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
303

304
CHAPTER 27
SUPPORT VECTOR CLASSIFICATION
FIGURE 27.1
Linear-in-input binary classifier fw,γ(x)
=
w⊤x +γ. w and γ are the normal vector and the
intercept of the decision boundary, respectively.
(a) Small margin
(b) Large margin
(c) Small margin
FIGURE 27.2
Decision boundaries that separate all training samples correctly.
γ are learned so that margins for all training samples are positive, i.e.
fw,γ(xi)yi = (w⊤xi + γ)yi > 0,
∀i = 1,. . . ,n,
all training samples {(xi, yi)}n
i=1 can be correctly classified. Since the open set
(w⊤xi + γ)yi > 0 is not easy to handle mathematically, let us convert this constraint
to a closed set based on the fact that the scale of w and γ is arbitrary:
(w⊤xi + γ)yi ≥1,
∀i = 1,. . . ,n.
If there exists (w,γ) such that the above condition is fulfilled, the set of training
samples {(xi, yi)}n
i=1 is said to be linearly separable. For a linearly separable set
of training samples, there usually exist infinitely many decision boundaries that
correctly separate all training samples. Here, let us choose the one that separates
all training samples with the maximum margin, where the margin for a set of
training samples {(xi, yi)}n
i=1 is defined as the minimum of the normalized margin

27.1 MAXIMUM MARGIN CLASSIFICATION
305
FIGURE 27.3
Decision boundary of hard margin support vector machine. It
goes through the center of positive and negative training samples,
w⊤x+ +γ = +1 for some positive sample x+ and w⊤x−+γ = −1
for some negative sample x−.
mi = (w⊤xi + γ)yi/∥w∥for i = 1,. . . ,n (Fig. 27.2):
min
i=1,...,n
(w⊤xi + γ)yi
∥w∥

=
1
∥w∥.
Geometrically, the margin for a set of training samples corresponds to the half
distance between two extreme decision boundaries w⊤x + γ = +1 and w⊤x + γ =
−1 (Fig. 27.3). The classifier that maximizes the above margin (or equivalently
minimizes the squared inverse margin) is called the hard margin support vector
machine [18]:
min
w,γ
1
2 ∥w∥2 subject to (w⊤xi + γ)yi ≥1,
∀i = 1,. . . ,n.
27.1.2 SOFT MARGIN SUPPORT VECTOR
CLASSIFICATION
The hard margin support vector machine requires linear separability, which may not
always be satisfied in practice. The soft margin support vector machine [30] relaxes
this requirement by allowing error ξ = (ξ1,. . . ,ξn)⊤for margins (Fig. 27.4):
min
w,γ,ξ

1
2 ∥w∥2 + C
n

i=1
ξi

subject to (w⊤xi + γ)yi ≥1 −ξi, ξi ≥0,
∀i = 1,. . . ,n,
(27.2)
where C > 0 is a tuning parameter that controls the margin errors. The margin error
ξ = (ξ1,. . . ,ξn)⊤is also referred to as slack variables in optimization. Larger C

306
CHAPTER 27
SUPPORT VECTOR CLASSIFICATION
FIGURE 27.4
Soft margin support vector machine allows
small margin errors.
makes the margin error n
i=1 ξi small and then soft margin support vector machine
approaches hard margin support vector machine.
Below, the soft margin support vector machine may be merely called the support
vector machine.
27.2 DUAL OPTIMIZATION OF SUPPORT VECTOR
CLASSIFICATION
The optimization problem of support vector classification (27.2) takes the form of
quadratic programming (Fig. 27.5), where the objective is a quadratic function and
constraints are linear. Since quadratic programming has been extensively studied in
the optimization community and various practical algorithms are available, which
can be readily used for obtaining the solution of support vector classification.
However, although original linear-in-input model (27.1) contains only d + 1
parameters (i.e. w ∈Rd and γ ∈R), quadratic optimization problem (27.2) contains
additional parameter ξ ∈Rn. Thus, the total number of parameters to be optimized
is n + d + 1, which does not scale well to large data sets.
The Lagrange function (Fig. 23.5) of optimization problem (27.2) is given by
L(w,γ,ξ,α, β)
= 1
2 ∥w∥2 + C
n

i=1
ξi −
n

i=1
αi

(w⊤xi + γ)yi −1 + ξi

−
n

i=1
βiξi.
Then Lagrange dual optimization problem is given by
max
α,β inf
w,γ,ξ L(w,γ,ξ,α, β) subject to α ≥0, β ≥0.

27.2 DUAL OPTIMIZATION OF SUPPORT VECTOR CLASSIFICATION
307
Quadratic programming is the optimization problem defined for matrices F
and G and vectors f and g as
min
θ
1
2θ⊤Fθ + f ⊤θ

subject to Gθ ≤g,
where vector inequality Gθ ≤g denotes the elementwise inequalities:
*.
,
a
b
+/
-
≤*.
,
c
d
+/
-
⇐⇒

a ≤c,
b ≤d.
Matrix F is supposed to be positive, i.e. all eigenvalues (see Fig. 6.2) are
positive. When F is ill-conditioned in the sense that a very small eigenvalue
exists, a small positive constant may be added to the diagonal elements of F
to improve numerical stability.
FIGURE 27.5
Quadratic programming.
The first-order optimality condition of infw,γ,ξ L(w,γ,ξ,α, β) yields
∂L
∂w = 0 =⇒
w =
n

i=1
αi yi xi,
∂L
∂γ = 0 =⇒
n

i=1
αi yi = 0,
∂L
∂ξi
= 0 =⇒αi + βi = C,
∀i = 1,. . . ,n,
showing that w and β can be expressed by α. Furthermore, the condition αi + βi = C
allows us to eliminate the margin error ξ from the Lagrange function.
Summarizing the above computations, the Lagrange dual optimization problem is
simplified as
α = argmax
α

n

i=1
αi −1
2
n

i, j=1
αiαj yi yj x⊤
i xj

subject to
n

i=1
αi yi = 0, 0 ≤αi ≤C
for i = 1,. . . ,n.
(27.3)

308
CHAPTER 27
SUPPORT VECTOR CLASSIFICATION
This is a quadratic programming problem which only contains n variables to be
optimized, and therefore it may be solved more efficiently than original optimization
problem (27.2). The solution α of Lagrange dual optimization problem (27.3) gives
the solution of support vector classification as
w =
n

i=1
αi yi xi.
The solution of intercept γ can be obtained by using xi such that 0 < αi < C as
γ = yi −

j:αi >0
αj yj x⊤
i xj.
(27.4)
The derivation of γ above will be explained in Section 27.3. When the linear-in-input
model without the intercept,
fw,γ(x) = w⊤x,
is used, the constraint,
n

i=1
αi yi = 0,
can be removed from Lagrange dual optimization problem (27.3).
While LS classification could not perfectly classify the training samples plotted
in Fig. 26.5, support vector classification can achieve perfect separation (Fig. 27.6).
Among 200 dual parameters {αi}n
i=1, 197 parameters take zero and only 3 parameters
specified by the square in the plot take nonzero values. The reason for this sparsity
induction will be explained in the next section.
The quadratic programming approach to obtaining the solution of support vector
classification explained above is handy and useful to understand the properties
of support vector classification. However, its scalability to large-scale data is
limited and more advanced optimization techniques have been extensively studied,
e.g. [25].
27.3 SPARSENESS OF DUAL SOLUTION
The ℓ1-constraint was shown to induce a sparse solution in Chapter 24. On the other
hand, as shown in Fig. 27.6, the dual solution α of support vector classification tends
to be sparse even though the ℓ1-constraint is not used.
To explain the sparsity induction mechanism of support vector classification,
let us investigate the optimality conditions called the KKT conditions (Fig. 27.7).
Dual variables and constraints satisfy the following conditions called complementary
slackness:
αi(mi −1 + ξi) = 0 and
βiξi = 0, ∀i = 1,. . . ,n,

27.3 SPARSENESS OF DUAL SOLUTION
309
FIGURE 27.6
Example of linear support vector classifica-
tion. Among 200 dual parameters {αi}n
i=1, 197
parameters take zero and only 3 parameters
specified by the square in the plot take nonzero
values.
where mi = (w⊤xi + γ)yi denotes the margin for the ith training sample (xi, yi).
Furthermore, ∂L
∂ξi = 0 yields
αi + βi = C.
Summarizing the above conditions, the dual variable αi and margin mi satisfy the
following relations (Fig. 27.8):
•
αi = 0 implies mi ≥1: the ith training sample xi is on the margin border or
inside the margin and is correctly classified.
•
0 < αi < C implies mi = 1: xi is on the margin border and correctly classified.
•
αi = C implies mi ≤1: xi is on the margin border or outside the margin. If
ξi > 1, mi < 0 and then xi is misclassified.
•
mi > 1 implies αi = 0: if xi is inside the margin, αi = 0.
•
mi < 1 implies αi = C: if xi is outside the margin, αi = C.
A training input sample xi such that αi > 0 is called a support vector, because
those points “support” the decision boundary. In other words, nonsupport vectors do
not affect the classification boundary. Support vector xi such that 0 < αi < C is on
the margin border (see Fig. 27.8 again) and satisfy
mi = (w⊤xi + γ)yi = 1.

310
CHAPTER 27
SUPPORT VECTOR CLASSIFICATION
The solution of constrained optimization problem,
min
t
f (t) subject to g(t) ≤0,
where f : Rd →R and g : Rd →Rp are the differentiable convex functions,
satisfies the following Karush-Kuhn-Tucker (KKT) conditions:
∂L
∂t = 0,
g(t) ≤0,
λ ≥0,
and λigi(t) = 0, ∀i = 1,. . . ,n,
where L(t,λ) =
f (t) + λ⊤g(t) is the the Lagrange function (Fig. 23.5)
and λ = (λ1,. . . ,λp)⊤are the Lagrange multipliers. The last condition
λigi(t) = 0 is called complementary slackness because at least either of λi
or gi(t) is zero.
FIGURE 27.7
KKT optimality conditions.
FIGURE 27.8
When αi = 0, xi is inside the margin and correctly classified.
When 0 < αi < C, xi is on the margin border (the dotted lines)
and correctly classified. When αi = C, xi is outside the margin,
and if ξi > 1, mi < 0 and thus xi is misclassified.
This implies that the intercept γ satisfies
γ = 1/yi −w⊤xi = yi −w⊤xi, ∀i : 0 < αi < C,
from which the solution γ in Eq. (27.4) is obtained.

27.4 NONLINEARIZATION BY KERNEL TRICK
311
FIGURE 27.9
Nonlinearization of support vector machine by kernel trick.
27.4 NONLINEARIZATION BY KERNEL TRICK
In this section, support vector classification introduced for linear-in-input model
(27.1) is extended to nonlinear models. More specifically, training input samples
{xi}n
i=1 are transformed to a feature space using a nonlinear function ψ (Fig. 27.9).
Then, for the transformed samples {ψ(xi)}n
i=1, the linear support vector machine
is trained. A linear decision boundary obtained in the feature space is a nonlinear
decision boundary obtained in the original input space.
If the dimensionality of the feature space is higher than the original input space,
it is more likely that training samples are linearly separable. However, too high
dimensionality causes large computation costs.
To cope with this problem, a technique called the kernel trick [30, 89] is useful.
In the optimization problem of the linear support vector machine, Eq.(27.3), training
input samples {xi}n
i=1 appear only in terms of their inner product:
x⊤
i xj = ⟨xi, xj⟩.
Similarly, in the above nonlinear support vector machine, training input samples
{xi}n
i=1 appear only in
⟨ψ(xi),ψ(xj)⟩.
This means that the nonlinear support vector machine is trainable even if ψ(xi) and
ψ(xj) are unknown, as long as their inner product ⟨ψ(xi),ψ(xj)⟩is known.
The kernel trick is to directly specify the inner product by a kernel function K(·,·)
as
⟨ψ(x),ψ(x′)⟩= K(x, x′).
This shows that, as long as the kernel function can be computed with a computation
cost independent of the dimensionality of the feature space, the computation cost

312
CHAPTER 27
SUPPORT VECTOR CLASSIFICATION
of training the support vector machine is independent of the dimensionality of the
feature space. As a kernel function, the polynomial kernel,
K(x, x′) = (x⊤x′ + a)p,
and the Gaussian kernel,
K(x, x′) = exp

−∥x −x′∥2
2h2

,
are popular choices, where a is a positive real scalar, p is a positive integer, and h
is a positive real scalar. It is known that the feature vector ψ that corresponds to the
Gaussian kernel is actually infinite dimensional. Therefore, without the kernel trick,
the solution cannot be computed explicitly.
A MATLAB code of support vector classification for the Gaussian kernel is
provided in Fig. 27.10, and its behavior is illustrated in Fig. 27.11. This shows
that nonlinearly entangled data can be successfully classified by support vector
classification with the Gaussian kernel.
Since the kernel support vector machine only uses the kernel value K(x, x′), as
long as the kernel function is defined, nonvectorial pattern x such as sequences,
trees, and graphs can be handled [46]. Note that the kernel trick can be applied
not only to support vector machines but also to any algorithms that handle training
samples only in terms of their inner products, for example, clustering (Chapter 37)
and dimensionality reduction (Section 36.1).
27.5 MULTICLASS EXTENSION
As explained in Section 26.3, a multiclass classification problem can be reduced to a
set of binary classification problems. In this section, a direct formulation of multiclass
support vector classification [34] is provided.
Let us classify a test sample x into class y as
y = argmax
y=1,...,c
w⊤
y x,
where wy denotes the parameter vector for class y. In this situation, it is desirable
that w⊤
y x is larger than w⊤
y x for y , y with a large margin. Based on this idea, hard
margin multiclass support vector classification for separable data is formulated as
min
w1,...,wc
1
2
c

y=1
∥wy∥2
subject to w⊤
yi xi −w⊤
y xi ≥1,
∀i = 1,. . . ,n, ∀y , yi.
Similarly to the binary case, the above formulation can be extended to soft margin
multiclass support vector classification using the margin error ξ = (ξ1,. . . ,ξn)⊤as

27.5 MULTICLASS EXTENSION
313
n=200; a=linspace(0,4*pi,n/2);
u=[a.*cos(a) (a+pi).*cos(a)]’+rand(n,1);
v=[a.*sin(a) (a+pi).*sin(a)]’+rand(n,1);
x=[u v]; y=[ones(1,n/2) -ones(1,n/2)]’;
x2=sum(x.^2,2); hh=2*1^2; l=0.01;
k=exp(-(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’)/hh);
Q=(y*y’).*k+0.01*eye(n); q=-ones(n,1);
H=[-eye(n); eye(n)]; h=[zeros(n,1); ones(n,1)/(2*l)];
a=quadprog(Q,q,H,h); t=y.*a;
m=100; X=linspace(-15,15,m)’; X2=X.^2;
U=exp(-(repmat(u.^2,1,m)+repmat(X2’,n,1)-2*u*X’)/hh);
V=exp(-(repmat(v.^2,1,m)+repmat(X2’,n,1)-2*v*X’)/hh);
figure(1); clf; hold on; axis([-15 15 -15 15]);
contourf(X,X,sign(V’*(U.*repmat(t,1,m))));
plot(x(y==1,1),x(y==1,2),’bo’);
plot(x(y==-1,1),x(y==-1,2),’rx’);
colormap([1 0.7 1; 0.7 1 1]);
FIGURE 27.10
MATLAB code of support vector classification for Gaussian kernel. quadprog.m included in
Optimization Toolbox is required. Free alternatives to quadprog.m are available, e.g. from
http://www.mathworks.com/matlabcentral/fileexchange/.
min
w1,...,wc,ξ

1
2
c

y=1
∥wy∥2 + C
n

i=1
ξi

subject to w⊤
yi xi −w⊤
y xi ≥ti,y −ξi,
∀i = 1,. . . ,n, ∀y = 1,. . . ,c,
where
ti,y =

0
(y = yi),
1
(y , yi).
Note that the constraint w⊤
yi xi −w⊤
y xi ≥ti,y −ξi is reduced to ξi ≥0 when y = yi.
The Lagrange dual (Fig. 23.5) of the above optimization problem is given by
max
α1,1,...,αn,c

n

i=1
αi,yi −1
2
n

i, j=1
c

y=1
αi,yαj,y x⊤
i xj


314
CHAPTER 27
SUPPORT VECTOR CLASSIFICATION
FIGURE 27.11
Example of support vector classification for
Gaussian kernel.
subject to
c

y=1
αi,y = 0,
∀i = 1,. . . ,n,
αi,y ≤C(1 −ti,y),
∀i = 1,. . . ,n, ∀y = 1,. . . ,c.
This is a quadratic programming problem and thus the solution αi,y can be obtained
by standard optimization software.
From the dual solution αi,y, the primal solution wy can be obtained as
wy =
n

i=1
αi,y xi.
Then a test sample x is classified into class y as
y = argmax
y=1,...,c
n

i=1
αi,y x⊤
i x.
Through this dual formulation, the kernel trick introduced in Section 27.4 can be
applied to nonlinearize multiclass support vector classification by replacing x⊤x′
with K(x, x′).
27.6 LOSS MINIMIZATION VIEW
So far, support vector classification was derived based on margin maximization and
the kernel trick. Although such a derivation is quite different from LS classification

27.6 LOSS MINIMIZATION VIEW
315
(a) Hinge loss
(b) Squared hinge loss
FIGURE 27.12
Hinge loss and squared hinge loss.
introduced in Chapter 26, support vector classification can actually be regarded as
an extension of LS classification. In this section, support vector classification is
interpreted as loss minimization. Another view of support vector classification in
L1-distance approximation is given in Section 39.1.4.
27.6.1 HINGE LOSS MINIMIZATION
Let us consider the hinge loss as a surrogate for the 0/1-loss (Fig. 27.12(a)):
max

0,1 −m

=

1 −m
(m ≤1),
0
(m > 1),
where m denotes the margin m = fθ(x)y. The hinge loss is zero for m ≥1, which is
the same as the 0/1-loss. On the other hand, for m < 1, the hinge loss takes a positive
value 1 −m and its slope is negative.
For the kernel model with intercept γ,
fθ,γ(x) =
n

j=1
θ jK(x, xj) + γ,
let us consider hinge loss minimization with generalized ℓ2-regularization:
min
θ,γ

n

i=1
max

0,1 −fθ,γ(xi)yi

+ λ
2 θ⊤Kθ

,
where Ki, j = K(xi, xj). As illustrated in Fig. 27.13, the hinge loss corresponds to the
maximum of 1 −m and 0:
max{0,1 −m} = min
ξ
ξ
subject to ξ ≥1 −m, ξ ≥0.

316
CHAPTER 27
SUPPORT VECTOR CLASSIFICATION
FIGURE 27.13
Hinge loss as maximizer of 1 −m and 0.
Then the optimization problem of regularized hinge loss minimization is expressed
as
min
θ,γ,ξ

n

i=1
ξi + λ
2 θ⊤Kθ

subject to ξi ≥1 −fθ,γ(xi)yi, ξi ≥0,
∀i = 1,. . . ,n.
(27.5)
Let us recall the primal optimization problem of kernel support vector classifica-
tion (see (27.2)):
min
w,γ,ξ

1
2 ∥w∥2 + C
n

i=1
ξi

subject to ξi ≥1 −(w⊤ψ(xi) + γ)yi, ξi ≥0,
∀i = 1,. . . ,n.
Letting C = 1/λ and w = n
j=1 θ jψ(xj) and using the kernel trick ψ(xi)⊤ψ(xj) =
K(xi, xj) in the above optimization problem show that support vector classification
is actually equivalent to ℓ2-regularized hinge loss minimization given by Eq. (27.5).
This loss minimization interpretation allows us to consider various extensions of
support vector classification, which are described below.
27.6.2 SQUARED HINGE LOSS MINIMIZATION
For margin m = fθ(x)y, let us consider the squared hinge loss:
1
2

max

0,1 −m
2 =

1
2(1 −m)2
(m ≤1),
0
(m > 1),
which is differentiable everywhere (Fig. 27.12(b)). Then ℓ2-regularized squared hinge
loss minimization,
min
θ

1
2
n

i=1

max

0,1 −fθ(xi)yi
2 + λ
2 ∥θ∥2

,

27.6 LOSS MINIMIZATION VIEW
317
1. Initialize parameter θ, e.g. randomly or by ℓ2-regularized LS as
θ ←−(Φ⊤Φ + λI)−1Φ⊤y.
2. Compute the retarget matrix V from the current solution θ as
V = diag (v1,. . . ,vn) ,
where
vi = θ⊤φ(xi)yi + max(0,1 −θ⊤φ(xi)yi).
3. Compute the solution θ based on the retarget matrix V:
θ ←−(Φ⊤Φ + λI)−1Φ⊤V y.
4. Iterate 2–3 until convergence.
FIGURE 27.14
Iterative retargeted LS for ℓ2-regularized squared hinge loss minimization.
may be solved by a simple stochastic gradient algorithm introduced in Section 15.3.
Another approach to obtaining the solution of ℓ2-regularized squared hinge loss
minimization is to put
v = m + max 0,1 −m	
and express the squared hinge loss as

max 0,1 −m	2 = (v −m)2
=

v −fθ(x)y
2 =

vy −fθ(x)
2,
where y = ±1 is used. Since v is unknown, let us consider an iterative procedure and
replace it with an estimate v computed from the current solution. Then the solution
of ℓ2-regularized squared hinge loss minimization can be obtained by iterative
retargeted LS (see Fig. 27.14).
A MATLAB code of iterative retargeted LS for ℓ2-regularized squared hinge loss
minimization is provided in Fig. 27.15, and its behavior is illustrated in Fig. 27.16.
This shows that nonlinearly entangled data can be successfully classified almost in
the same way as ordinary support vector classification with the Gaussian kernel (see
Fig. 27.11).

318
CHAPTER 27
SUPPORT VECTOR CLASSIFICATION
n=200; a=linspace(0,4*pi,n/2);
u=[a.*cos(a) (a+pi).*cos(a)]’+rand(n,1);
v=[a.*sin(a) (a+pi).*sin(a)]’+rand(n,1);
x=[u v]; y=[ones(1,n/2) -ones(1,n/2)]’;
x2=sum(x.^2,2); hh=2*1^2; l=1;
k=exp(-(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’)/hh);
A=inv(k’*k+l*eye(n))*k’; t=rand(n,1);
for o=1:1000
z=(k*t).*y; w=z+max(0,1-z); t0=A*(w.*y);
if norm(t-t0)<0.001, break, end
t=t0;
end
m=100; X=linspace(-15,15,m)’; X2=X.^2;
U=exp(-(repmat(u.^2,1,m)+repmat(X2’,n,1)-2*u*X’)/hh);
V=exp(-(repmat(v.^2,1,m)+repmat(X2’,n,1)-2*v*X’)/hh);
figure(1); clf; hold on; axis([-15 15 -15 15]);
contourf(X,X,sign(V’*(U.*repmat(t,1,m))));
plot(x(y==1,1),x(y==1,2),’bo’);
plot(x(y==-1,1),x(y==-1,2),’rx’);
colormap([1 0.7 1; 0.7 1 1]);
FIGURE 27.15
MATLAB code of iterative retargeted LS for ℓ2-regularized squared hinge loss minimization.
27.6.3 RAMP LOSS MINIMIZATION
As illustrated in Fig. 27.12, the (squared) hinge loss is not upper-bounded. For this
reason, learning with such an unbounded loss tends to be strongly affected by an
outlier (Fig. 27.17(a)).
As discussed in Chapter 25, using a bounded loss can enhance the robustness
against outliers. For margin m = fθ(x)y, let us consider the ramp loss (Fig. 27.18(a)):
min

1,max  0,1 −m
=

1
(m < 0),
1 −m
(0 ≤m ≤1),
0
(m > 1).

27.6 LOSS MINIMIZATION VIEW
319
FIGURE 27.16
Example of ℓ2-regularized squared hinge loss
minimization.
(a) Ordinary support vector classification
(b) Robust support vector classification
FIGURE 27.17
Examples of support vector classification with outliers.
Due to the boundedness of the ramp loss, ramp loss minimization,
min
θ
n

i=1
min

1,max  0,1 −fθ(xi)yi

,

320
CHAPTER 27
SUPPORT VECTOR CLASSIFICATION
(a) Ramp loss
(b) Squared ramp loss
FIGURE 27.18
Ramp loss and squared ramp loss.
is more robust against outliers than hinge loss minimization, as illustrated
Fig. 27.17(b). However, since the ramp loss is a nonconvex function, obtaining the
global optimal solution is not straightforward. A ramp loss variant of the support
vector machine is also referred to as the robust support vector machine.
A variation of the ramp loss is the squared ramp loss (Fig. 27.18(b)):
1
2

min

1,max  0,1 −m2 =

1
2
(m < 0),
1
2(1 −m)2
(0 ≤m ≤1),
0
(m > 1).
A local optimal solution of squared ramp loss minimization,
min
θ
n

i=1

min

1,max  0,1 −fθ(xi)yi
2 ,
can be obtained by iteratively retargeted LS in the same as squared hinge loss
minimization (see Section 27.6.2). The only difference is that the definition of v in
iteratively retargeted LS is replaced with
v = m + min{1,max(0,1 −m)}.

CHAPTER
PROBABILISTIC
CLASSIFICATION 28
CHAPTER CONTENTS
Logistic Regression .............................................................. 321
Logistic Model and MLE ................................................... 321
Loss Minimization View .................................................... 324
LS Probabilistic Classification .................................................... 325
In Chapter 26 and Chapter 27, classification methods that deterministically assign
a test pattern x into class y were discussed. In this chapter, methods of proba-
bilistic classification are introduced, which choose the class probabilistically. More
specifically, for a test pattern x, the class-posterior probability p(y|x) is learned
in probabilistic classification. Then the class that maximizes the estimated class-
posterior probability p(y|x) is chosen:
y = argmax
y=1,...,c
p(y|x).
The value of p(y|x) may be interpreted as the reliability of assigning label y to pattern
x, which allows the possibility of rejecting the pattern x if the reliability is low.
28.1 LOGISTIC REGRESSION
In this section, a standard probabilistic classifier called logistic regression is intro-
duced.
28.1.1 LOGISTIC MODEL AND MLE
The logistic model parameterizes the class-posterior probability p(y|x) in a log-linear
form as
q(y|x; θ) =
exp
b
j=1 θ(y)
j ϕj(x)

c
y′=1 exp
b
j=1 θ(y′)
j
ϕj(x)
=
exp

θ(y)⊤φ(x)

c
y′=1 exp

θ(y′)⊤φ(x)
,
(28.1)
where the exponential function in the numerator is for restricting the output to be
positive and the summation in the denominator is for restricting the output summed
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00039-X
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
321

322
CHAPTER 28
PROBABILISTIC CLASSIFICATION
1. Initialize θ.
2. Choose the ith training sample randomly (xi, yi).
3. Update parameter θ = (θ(1)⊤,. . . ,θ(c)⊤)⊤to go up the gradient:
θ(y) ←−θ(y) + ε∇y Ji(θ)
for y = 1,. . . ,c,
where ε > 0 is the step size, and ∇y Ji is the gradient of Ji(θ) =
log q(yi|xi; θ) with respect to θ(y):
∇y Ji(θ) = −
exp

θ(y)⊤φ(xi)

φ(xi)
c
y′=1 exp

θ(y′)⊤φ(xi)
+

φ(xi)
(y = yi),
0
(y , yi).
4. Iterate 2 and 3 until convergence.
FIGURE 28.1
Stochastic gradient algorithm for logistic regression.
to one. Since the logistic model contains parameters {θ(y)
j }b
j=1 for each class y =
1,. . . ,c, the entire parameter vector θ is bc-dimensional:
θ = (θ(1)
1 ,. . . ,θ(1)
b
                  
Class 1
,. . . ,θ(c)
1 ,. . . ,θ(c)
b
                  
Class c
)⊤.
Note that the number of parameters can be reduced from bc to b(c −1) by using
c
y=1 q(y|x; θ) = 1, but for simplicity the above bc-dimensional formulation is
considered below.
The parameter θ is learned by the MLE (Chapter 12). More specifically, θ is
determined so as to maximize the log-likelihood, i.e. the log-probability that the
current training samples {(xi, yi)}n
i=1 are generated:
max
θ
n

i=1
log q(yi|xi; θ).
Since the above objective function is differentiable with respect to θ, the solution
may be obtained by a stochastic gradient algorithm (Section 15.3), as described in
Fig. 28.1.
A MATLAB code of stochastic gradient ascent for logistic regression is provided
in Fig. 28.2, where the log-Gaussian kernel model,
q(y|x; θ) ∝exp *.
,
n

j=1
θ j exp *
,
−∥x −xj∥2
2h2
+
-
+/
-
,

28.1 LOGISTIC REGRESSION
323
n=90; c=3; y=ones(n/c,1)*[1:c]; y=y(:);
x=randn(n/c,c)+repmat(linspace(-3,3,c),n/c,1); x=x(:);
hh=2*1^2; t=randn(n,c);
for o=1:n*1000
i=ceil(rand*n); yi=y(i); ki=exp(-(x-x(i)).^2/hh);
ci=exp(ki’*t); t0=t-0.1*(ki*ci)/(1+sum(ci));
t0(:,yi)=t0(:,yi)+0.1*ki;
if norm(t-t0)<0.000001, break, end
t=t0;
end
N=100; X=linspace(-5,5,N)’;
K=exp(-(repmat(X.^2,1,n)+repmat(x.^2’,N,1)-2*X*x’)/hh);
figure(1); clf; hold on; axis([-5 5 -0.3 1.8]);
C=exp(K*t); C=C./repmat(sum(C,2),1,c);
plot(X,C(:,1),’b-’); plot(X,C(:,2),’r--’);
plot(X,C(:,3),’g:’);
plot(x(y==1),-0.1*ones(n/c,1),’bo’);
plot(x(y==2),-0.2*ones(n/c,1),’rx’);
plot(x(y==3),-0.1*ones(n/c,1),’gv’);
legend(’q(y=1|x)’,’q(y=2|x)’,’q(y=3|x)’)
FIGURE 28.2
MATLAB code of stochastic gradient ascent for logistic regression.
is used for approximating the class-posterior probability. The behavior of Gaussian
kernel logistic regression is illustrated in Fig. 28.3, showing that the class-posterior
probability is well-approximated.
The log-likelihood of logistic regression for the ith training sample (xi, yi) is
given by
log q(yi|xi; θ) = fyi −log *.
,
c

y=1
exp( fy)+/
-
,
where fy = θ(y)⊤φ(xi). In numerical computation, the log-sum-exp term,
log *.
,
c

y=1
exp( fy)+/
-
,

324
CHAPTER 28
PROBABILISTIC CLASSIFICATION
FIGURE 28.3
Example of stochastic gradient ascent for logistic regression.
is often cumbersome. When fy ≫1 for some y, exp( fy) may cause overflow (e.g.
exp(700) ≈10300), resulting in log(∞) = ∞. When fy ≪−1 for all y = 1,. . . ,c,
exp( fy) may cause underflow (e.g. exp(−700) ≈10−300), resulting in log(0) = −∞.
To cope with these numerical problems, the following expression of the log-sum-exp
term is useful in practice:
log *.
,
c

y=1
exp( fy)+/
-
= log *.
,
exp( fy′) *.
,
c

y=1
exp   fy −fy′+/
-
+/
-
= fy′ + log *.
,
c

y=1
exp   fy −fy′+/
-
,
where y′ = argmaxy=1,...,c{θ(y)⊤φ(xi)}. With this expression, overflow can be
avoided because fy −fy′ ≤0 for all y = 1,. . . ,c, and underflow does not cause
log(0) because fy −fy′ = 0 holds for y = y′.
28.1.2 LOSS MINIMIZATION VIEW
Let us focus on the binary classification problem for y ∈{+1,−1}. Then the sum-to-
one constraint,
q(y = +1|x; θ) + q(y = −1|x; θ) = 1,
implies that the number of parameters in the logistic model can be reduced from 2b
to b:
q(y|x; θ) =
1
1 + exp   −y fθ(x),

28.2 LS PROBABILISTIC CLASSIFICATION
325
FIGURE 28.4
Logistic loss.
where fθ(x) = b
j=1 θ jϕj(x). Then the maximum log-likelihood criterion for this
simplified model can be expressed as
min
θ
n

i=1
log  1 + exp(−mi),
(28.2)
where mi = fθ(xi)yi is the margin for the ith training sample (xi, yi). Eq. (28.2) can
be interpreted as surrogate loss minimization (see Section 26.2) with the logistic loss
(Fig. 28.4):
log(1 + exp(−m)).
Thus, although logistic regression was derived in a very different framework from LS
classification (Chapter 26) and support vector classification (Chapter 27), they can all
be interpreted as surrogate loss minimization.
28.2 LS PROBABILISTIC CLASSIFICATION
In this section, an alternative to logistic regression based on LS fitting of the class-
posterior probability is introduced.
Let us model the class-posterior probability p(y|x) by the linear-in-parameter
model:
q(y|x; θ(y)) =
b

j=1
θ(y)
j ϕj(x) = θ(y)⊤φ(x).
(28.3)
Differently from logistic model (28.1), the above model for class y depends only on
the parameter for class y, θ(y) = (θ(y)
1 ,. . . ,θ(y)
b )⊤, because the normalization term is
not included.

326
CHAPTER 28
PROBABILISTIC CLASSIFICATION
In LS probabilistic classification, the class-posterior model q(y|x; θ(y)) is learned
for each class separately. More specifically, the parameter θ(y) is learned to minimize
the expected squared error to the true class-posterior probability p(y|x):
Jy(θ(y)) = 1
2

q(y|x; θ(y)) −p(y|x)
2p(x)dx
= 1
2

q(y|x; θ(y))2p(x)dx −

q(y|x; θ(y))p(y|x)p(x)dx
+ 1
2

p(y|x)2p(x)dx,
(28.4)
where p(x) denotes the marginal probability density of training input samples
{xi}n
i=1.
The second term in Eq. (28.4), p(y|x)p(x), can be expressed as
p(y|x)p(x) = p(x, y) = p(x|y)p(y),
where p(x|y) denotes the class-conditional probability density for samples in class y,
{xi}i:yi=y, and p(y) denotes the class-prior probability for training output samples
{yi}n
i=1. The first two terms in Jy,

q(y|x; θ(y))2p(x)dx
and

q(y|x; θ(y))p(y)p(x|y)dx,
are the expectations over p(x) and p(x|y), respectively, although they are not
accessible. Here, the expectations are approximated by the sample averages:
1
n
n

i=1
q(y|xi; θ(y))2
and
1
ny

i:yi=y
q(y|xi; θ(y))p(y),
where ny denotes the number of training samples in class y. Further approximating
p(y) by the sample ratio ny/n, ignoring the third term in Eq. (28.4) because it is
constant, and including the ℓ2-regularizer yield the following training criterion:
Jy(θ(y)) = 1
2n
n

i=1
q(y|xi; θ(y))2 −1
n

i:yi=y
q(y|xi; θ(y)) + λ
2n ∥θ(y)∥2
= 1
2nθ(y)⊤Φ⊤Φθ(y) −1
nθ(y)⊤Φ⊤π(y) + λ
2n ∥θ(y)∥2,
where π(y) = (π(y)
1 ,. . . ,π(y)
n )⊤is defined as π(y)
i
=

1
(yi = y),
0
(yi , y).
Since Jy(θ(y)) is a
quadratic function, its minimizer can be obtained analytically by setting its derivative
to zero:
θ(y) =  Φ⊤Φ + λI−1 Φ⊤π(y).

28.2 LS PROBABILISTIC CLASSIFICATION
327
n=90; c=3; y=ones(n/c,1)*[1:c]; y=y(:);
x=randn(n/c,c)+repmat(linspace(-3,3,c),n/c,1); x=x(:);
hh=2*1^2; x2=x.^2; l=0.1; N=100; X=linspace(-5,5,N)’;
k=exp(-(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’)/hh);
K=exp(-(repmat(X.^2,1,n)+repmat(x2’,N,1)-2*X*x’)/hh);
for yy=1:c
yk=(y==yy); ky=k(:,yk);
ty=(ky’*ky+l*eye(sum(yk)))\(ky’*yk);
Kt(:,yy)=max(0,K(:,yk)*ty);
end
ph=Kt./repmat(sum(Kt,2),1,c);
figure(1); clf; hold on; axis([-5 5 -0.3 1.8]);
plot(X,ph(:,1),’b-’); plot(X,ph(:,2),’r--’);
plot(X,ph(:,3),’g:’);
plot(x(y==1),-0.1*ones(n/c,1),’bo’);
plot(x(y==2),-0.2*ones(n/c,1),’rx’);
plot(x(y==3),-0.1*ones(n/c,1),’gv’);
legend(’p(y=1|x)’,’p(y=2|x)’,’p(y=3|x)’)
FIGURE 28.5
MATLAB code for LS probabilistic classification.
However, differently from logistic model (28.1), current linear-in-parameter model
(28.3) can take a negative value and may not be summed to one. To cope with this
problem, the solution is postprocessed as
p(y|x) =
max(0,θ(y)⊤φ(x))
c
y′=1 max(0,θ(y′)⊤φ(x))
.
(28.5)
A MATLAB code of LS probabilistic classification for the Gaussian kernel model,
q(y|x; θ(y)) =

j:y j=y
θ(y)
j
exp *
,
−∥x −xj∥2
2h2
+
-
,
is provided in Fig. 28.5, and its behavior is illustrated in Fig. 28.6. This shows that
almost the same solution as logistic regression (see Fig. 28.3) can be obtained by LS
probabilistic classification.
As illustrated above, LS probabilistic classification behaves similarly to logistic
regression. In logistic regression, logistic model (28.1) containing bc parameters is

328
CHAPTER 28
PROBABILISTIC CLASSIFICATION
FIGURE 28.6
Example of LS probabilistic classification for
the same data set as Fig. 28.3.
trained once for all classes. On the other hand, in LS probabilistic classification,
linear-in-parameter model (28.3) containing only b parameters is trained for each
class y = 1,. . . ,c. When c is large, training small linear-in-parameter models contain-
ing only b parameters c times would be computationally more efficient than training
a big logistic model containing bc parameters once. Furthermore, the solution of LS
probabilistic classification can be obtained analytically. However, it involves ad hoc
postprocessing, which may corrupt the solution when the number of training samples
is small. Thus, using logistic regression would be more reliable when the number
of training samples is small, while using LS probabilistic classification would be
computationally more efficient when the number of training samples is large.

CHAPTER
STRUCTURED
CLASSIFICATION 29
CHAPTER CONTENTS
Sequence Classification .......................................................... 329
Probabilistic Classification for Sequences ........................................ 330
Conditional Random Field.................................................. 330
MLE ........................................................................ 333
Recursive Computation..................................................... 333
Prediction for New Sample................................................. 336
Deterministic Classification for Sequences ....................................... 337
When classifying a set of input patterns having certain data structure such as a
sequence of letters and a parsing tree of a sentence, the classification performance
is expected to be improved if such structural information is utilized. In this chapter,
taking a sequence of letters as an example of such structure, methods of structured
classification are introduced.
29.1 SEQUENCE CLASSIFICATION
Let us consider the problem of classifying a sequence of m patterns where each
pattern belongs to one of the c classes. Let x and y be sequences of m patterns and m
classes, and let x(k) and y(k) be their kth elements:
x = (x(1),. . . , x(m)),
y = (y(1),. . . , y(m)).
For example, when recognizing a sequence of five hand-written digits (see
Fig. 29.1), m = 5 and c = 10. If such a sequence is decomposed into five digits,
ordinary classification algorithms introduced in the previous chapters can be used for
classifying each digit. However, this decomposition approach loses useful contextual
information that can drastically improve the classification accuracy, for example, the
same number may rarely appear consecutively and some number appears frequently
after a certain number. On the other hand, if the entire sequence of digits is directly
recognized, the number of classes grows exponentially with respect to the length of
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00040-6
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
329

330
CHAPTER 29
STRUCTURED CLASSIFICATION
FIGURE 29.1
Classification of sequence of hand-written digits.
the sequence. In the example of Fig. 29.1, (m,c) = (5,10) and thus the number of
classes is 105, which is hard to handle.
In this chapter, intermediate approaches are introduced, which take into account
some contextual information with the computational complexity kept moderately.
Note that, although only sequence data are considered, methods introduced in this
chapter can be applied to more general structured data.
29.2 PROBABILISTIC CLASSIFICATION FOR
SEQUENCES
In this section, the logistic model introduced in Section 28.1 is extended to the
conditional random field [65] for sequence classification.
29.2.1 CONDITIONAL RANDOM FIELD
To introduce the conditional random field, let us first consider the logistic model
applied to sequence classification, i.e. each pattern x(k) is independently classified
into one of the c classes (Fig. 29.2(a)):
q(y|x; θ) =
exp

θ⊤
y φ(x)

c

y′=1
exp

θ⊤
y′φ(x)
,
where φ(x) ∈Rb is the vector of basis functions for a single pattern x, θy ∈Rb is
the parameter vector for class y, and
θ = (θ⊤
1 ,. . . ,θ⊤
c )⊤∈Rbc
is the vector of all parameters. However, this decomposition approach cannot utilize
the contextual information contained in sequence data.

29.2 PROBABILISTIC CLASSIFICATION FOR SEQUENCES
331
(a) Separate recognition of each pattern
(b) Naive recognition of all m patterns
(c) Efficient recognition of all m patterns
FIGURE 29.2
Sequence classification.
On the other hand, if the entire sequence is classified at once, a classification
problem with c = cm classes needs to be solved (Fig. 29.2(b)):
q(y|x; θ) =
exp

θ
⊤
y φ(x)

c

y(1),...,y(m)=1
exp

θ
⊤
y′φ(x)
,
(29.1)
where
φ(x) = (φ(x(1))⊤,. . . ,φ(x(m))⊤)⊤∈Rbm
is the vector of basis functions for pattern sequence x = (x(1),. . . , x(m)),
θy = (θy(1)⊤,. . . ,θy(m)⊤)⊤∈Rbm
is the parameter vector for class y = (y(1),. . . , y(m)), and
θ = (θ1⊤,. . . ,θc⊤)⊤∈Rbmc
is the vector of all parameters. However, since the number of classes c = cm grows
exponentially with respect to the sequence length m, handling the above logistic
model may be intractable.

332
CHAPTER 29
STRUCTURED CLASSIFICATION
To avoid the exponential growth in the number of classes, let us assume that y(k)
is determined only by x(k) and y(k−1) when classifying the entire sequence x. Note
that this approach is different from merely classifying two consecutive patterns into
one of the c2 classes, because of the overlap (see Fig. 29.2(c)).
To implement the above idea, let us consider the following model, called the
conditional random field:
q(y|x; ζ) =
exp (ζ ⊤ϕ(x, y))
c

y(1),...,y(mi )=1
exp  ζ ⊤ϕ(x, y′)
,
(29.2)
where the basis function ϕ(x, y) depend not only on input x but also on output y.
Conditional random field(29.2) is reduced to the logistic model for sequence x given
by Eq. (29.1), if
ζ = θ
and ϕ(x, y) = e(c)
y
⊗φ(x),
where e(c)
y
is the c-dimensional indicator vector of class y (i.e. the element corre-
sponding class y is 1 and all other elements are 0), and ⊗denotes the Kronecker
product (see Fig. 6.5):
For f = ( f1,. . . , fu)⊤∈Ru, g = (g1,. . . ,gv)⊤∈Rv,
f ⊗g = ( f1g1, f1g2,. . . , f1gv, f2g1, f2g2,. . . , f2gv,. . . ,
fug1, fug2,. . . , fugv)⊤∈Ruv.
Thus, naively using conditional random field (29.2) is just an overly redundant
expression of Eq. (29.1).
Let us simplify conditional random field(29.2) by taking into account the structure
described in Fig. 29.2(c). More specifically, basis function ϕ(x, y) is defined as the
sum of basis functions depending only on two consecutive patterns:
ϕ(x, y) =
m

k=1
ϕ(x(k), y(k), y(k−1)),
(29.3)
where y(0) = y(1). As two-pattern basis function ϕ(x(k), y(k), y(k−1)), for example,
ϕ(x(k), y(k), y(k−1)) = *..
,
e(c)
y(k) ⊗φ(x(k))
e(c)
y(k) ⊗e(c)
y(k−1)
+//
-
∈Rcb+c2,
(29.4)
may be used. Then the dimensionality of parameter vector ζ is cb + c2, which is
independent of the sequence length m.

29.2 PROBABILISTIC CLASSIFICATION FOR SEQUENCES
333
1. Initialize ζ.
2. Choose the ith training sample randomly (xi, yi).
3. Update parameter ζ to go up the gradient:
ζ ←−ζ + ε
*........
,
ϕi(xi, yi) −
c

y(1),...,y(mi )=1
exp  ζ ⊤ϕi(xi, y) ϕi(xi, y)
c

y(1),...,y(mi )=1
exp  ζ ⊤ϕi(xi, y)
+////////
-
,
where ε > 0 is the step size and y = (y(1),. . . , y(mi)).
4. Iterate 2 and 3 until convergence.
FIGURE 29.3
Stochastic gradient algorithm for conditional random field.
29.2.2 MLE
Suppose that labeled sequences,

(xi, yi)  xi = (x(1)
i ,. . . , x(mi)
i
), yi = (y(1)
i ,. . . , y(mi)
i
)
n
i=1,
are provided as training samples, where the length mi of sequence xi can be different
for each sample. With these training samples, the parameter ζ in conditional random
field (29.2) is learned by MLE (see Chapter 12):
max
ζ
n

i=1
log
exp (ζ ⊤ϕi(xi, yi))
c

y(1),...,y(mi )=1
exp  ζ ⊤ϕi(xi, y)
,
where
ϕi(xi, yi) =
mi

k=1
ϕ(x(k)
i , y(k)
i , y(k−1)
i
).
The maximum likelihood solution may be obtained by a stochastic gradient algo-
rithm (Section 15.3), as described in Fig. 29.3.
29.2.3 RECURSIVE COMPUTATION
However, the computational complexity of the second term of the gradient (see
Fig. 29.3),

334
CHAPTER 29
STRUCTURED CLASSIFICATION
FIGURE 29.4
Dynamic programming, which solves a complex optimization problem by breaking it down
into simpler subproblems recursively. When the number of steps to the goal is counted,
dynamic programming trace back the steps from the goal. In this case, many subproblems
of counting the number of steps from other positions are actually shared and thus dynamic
programming can efficiently reuse the solutions to reduce the computation costs.
c

y(1),...,y(mi )=1
exp  ζ ⊤ϕi(xi, y) ϕi(xi, y)
c

y(1),...,y(mi )=1
exp  ζ ⊤ϕi(xi, y)
,
(29.5)
grows exponentially with respect to the sequence length mi. To cope with this
problem, let us utilize the decomposition given by (29.3) based on dynamic
programming (Fig. 29.4).
First, let us split the summation in the denominator of Eq. (29.5) into (y(1),. . . ,
y(mi−1)) and y(mi):
c

y(1),...,y(mi )=1
exp *
,
mi

k=1
ζ ⊤ϕ(x(k)
i , y(k), y(k−1))+
-
=
c

y(mi )=1
Ami(y(mi)),
(29.6)
where
Aτ(y) =
c

y(1),...,y(τ−1)=1
exp
τ−1

k=1
ζ ⊤ϕ(x(k)
i , y(k), y(k−1)) + ζ ⊤ϕ(x(τ)
i , y, y(τ−1))

.

29.2 PROBABILISTIC CLASSIFICATION FOR SEQUENCES
335
Aτ(y(τ)) can be expressed recursively using Aτ−1(y(τ−1)) as
Aτ(y(τ)) =
c

y(1),...,y(τ−1)=1
exp *
,
τ

k=1
ζ ⊤ϕ(x(k)
i , y(k), y(k−1))+
-
=
c

y(τ−1)=1
Aτ−1(y(τ−1)) exp

ζ ⊤ϕ(x(τ)
i , y(τ), y(τ−1))

.
The computational complexity of naively computing Ami(y(mi)) is O(cmi), while that
of computing Ami(y(mi)) using the above recursive expression from A1(y(1)) is only
O(c2mi). Note that the complexity for computing ζ ⊤ϕ(x, y, y′) is not included here.
Next, let us split the summation in the numerator of Eq. (29.5) into (y(1),. . . ,
y(k′−2)), (y(k′−1), y(k′)), and (y(k′+1),. . . , y(mi)):
c

y(1),...,y(mi )=1
exp *
,
mi

k=1
ζ ⊤ϕ(x(k)
i , y(k), y(k−1))+
-
*
,
mi

k′=1
ϕ(x(k′)
i
, y(k′), y(k′−1))+
-
=
mi

k′=1
c

y(1),...,y(k′−2)=1
c

y(k′−1),y(k′)=1
c

y(k′+1),...,y(mi )=1
exp *
,
mi

k=1
ζ ⊤ϕ(x(k)
i , y(k), y(k−1))+
-
ϕ(x(k′)
i
, y(k′), y(k′−1))
=
mi

k′=1
c

y(k′−1),y(k′)=1
ϕ(x(k′)
i
, y(k′), y(k′−1))
× exp

ζ ⊤ϕ(x(k′)
i
, y(k′), y(k′−1))

Ak′−1(y(k′−1))Bk′(y(k′)),
(29.7)
where
Bτ(y) =
c

y(τ+1),...,y(mi )=1
exp

mi

k=τ+2
ζ ⊤ϕ(x(k)
i , y(k), y(k−1))
+ ζ ⊤ϕ(x(τ+1)
i
, y(τ+1), y)

.
Bτ(y(τ)) can be expressed recursively using Bτ+1(y(τ+1)) as
Bτ(y(τ)) =
c

y(τ+1),...,y(mi )=1
exp

mi

k=τ+1
ζ ⊤ϕ(x(k)
i , y(k), y(k−1))

=
c

y(τ+1)=1
Bτ+1(y(τ+1)) exp

ζ ⊤ϕ(x(τ+1)
i
, y(τ+1), y(τ))

.

336
CHAPTER 29
STRUCTURED CLASSIFICATION
The computational complexity of naively computing B1(y(1)) is O(cmi), while that
of computing B1(y(1)) using the above recursive expression from Bmi(y(mi)) is only
O(c2mi). Note that the complexity for computing ζ ⊤ϕ(x, y, y′) is not included here.
Summarizing the above computations, stochastic gradient ascent for the condi-
tional random field can be performed efficiently by computing {Ak(y(k))}mi
k=1 and
{Bk(y(k))}mi
k=1 in advance using the recursive expressions.
29.2.4 PREDICTION FOR NEW SAMPLE
Trained conditional random field q(y|x; ζ) allows us to predict the most probable
label sequence y = (y(1),. . . , y(m)) for a test pattern sequence x = (x(1),. . . , x(m)) as
argmax
y(1),...,y(m)∈{1,...,c}
q(y|x; ζ).
However, the computational complexity for naive maximization over y(1),. . . , y(m)
grows exponentially with respect to the sequence length m. Here, dynamic program-
ming is used again to speed up the maximization.
Let us simplify the maximization problem as
argmax
y(1),...,y(m)∈{1,...,c}
exp
ζ ⊤ϕ(x, y)

c

y=1
exp
ζ ⊤ϕ(x, y)

=
argmax
y(1),...,y(m)∈{1,...,c}
ζ ⊤ϕ(x, y)
and decompose the maximization in the right-hand side into (y(1),. . . , y(m−1)) and
y(m):
max
y(1),...,y(m)∈{1,...,c}
ζ ⊤ϕ(x, y) =
max
y(m)∈{1,...,c} Pm(y(m)),
where
Pτ(y) =
max
y(1),...,y(τ−1)∈{1,...,c}
τ−1

k=1
ζ ⊤ϕ(x(k), y(k), y(k−1)) + ζ ⊤ϕ(x(τ), y, y(τ−1))

.
Pτ(y(τ)) can be expressed by recursively using Pτ−1(y(τ−1)) as
Pτ(y(τ)) =
max
y(1),...,y(τ−1)∈{1,...,c}

τ

k=1
ζ ⊤ϕ(x(k), y(k), y(k−1))

=
max
y(τ−1)∈{1,...,c}

Pτ−1(y(τ−1)) + ζ ⊤ϕ(x(τ), y(τ), y(τ−1))

.
The computational complexity of naively computing Pm(y(m)) is O(cm), while that
of computing Pm(y(m)) using the above recursive expression from P1(y(1)) is only
O(c2m). Note that the complexity for computing ζ ⊤ϕ(x, y, y′) is not included here.

29.3 DETERMINISTIC CLASSIFICATION FOR SEQUENCES
337
29.3 DETERMINISTIC CLASSIFICATION FOR
SEQUENCES
The conditional random field models the class-posterior probability, which corre-
sponds to a probabilistic classification method. On the other hand, in deterministic
classification, only the class label is learned. In this section, based on the formulation
of multiclass support vector classification given in Section 27.5, a deterministic
sequence classification method is introduced. More specifically,
fwy(x) = w⊤
y x
is used as a score function for class y, and a test sample x is classified into class y as
y = argmax
y=1,...,c
fwy(x).
The parameter wy is learned as
min
w1,...,wc,ξ

1
2
c

y=1
∥wy∥2 + C
n

i=1
ξi

subject to w⊤
yi xi −w⊤
y xi ≥ti,y −ξi,
∀i = 1,. . . ,n, ∀y = 1,. . . ,c,
where
ti,y =

0
(y = yi),
1
(y , yi).
Note that, when y = yi, the constraint w⊤
yi xi −w⊤
y xi ≥ti,y −ξi is reduced to ξi ≥0.
Applying the feature expression of the conditional random field, ζ ⊤ϕ(x, y), to the
above optimization yields
min
ζ,ξ

1
2 ∥ζ∥2 + C
n

i=1
ξi

subject to ζ ⊤∆ϕi(y) ≥ti,y −ξi,
∀i = 1,. . . ,n, ∀y = (1,. . . ,1)
          
m
,. . . ,(c,. . . ,c)
        
m
,
where
∆ϕi(y) = ϕi(xi, yi) −ϕi(xi, y).
The Lagrange dual (Fig. 23.5) of the above optimization problem is given by
max
{αi,y }i,y
J({αi,y}i,y)
subject to
c

y(1),...,y(m)=1
αi,y = C,
∀i = 1,. . . ,n,
αi,y ≥0,
∀i = 1,. . . ,n, ∀y(1),. . . , y(m) = 1,. . . ,c,
(29.8)

338
CHAPTER 29
STRUCTURED CLASSIFICATION
where
J({αi,y}i,y) =
n

i=1
c

y(1),...,y(m)=1
αi,yti,y
−1
2
n

i,i′=1
c

y(1),...,y(m)=1
c

y′(1),...,y′(m)=1
αi,yαi′,y′∆ϕi(y)⊤∆ϕi′(y′).
This is a quadratic programming problem and thus the solution {αi,y}i,y can be
obtained by standard optimization software in principle. However, the number of
optimization variables is exponential with respect to the sequence length, m, and thus
it is not computationally tractable.
Here, let us utilize the same simplification trick as the conditional random field:
ϕ(x, y) =
m

k=1
ϕ(x(k), y(k), y(k−1)),
which yields
∆ϕi(y) =
mi

k=1
∆ϕi(x(k)
i , y(k), y(k−1)),
where
∆ϕi(x(k)
i , y(k), y(k−1)) = ϕi(x(k)
i , y(k)
i , y(k−1)
i
) −ϕi(x(k)
i , y(k), y(k−1)).
Then the objective function in optimization problem (29.8) can be expressed as
J({αi,y}i,y) =
n

i=1
m

k=1
c

y(k)=1
µi(y(k))ti,y
−1
2
n

i,i′=1
m

k,k′=1
c

y(k−1),y(k)=1
c

y(k′−1),y(k′)=1
∆ϕi(x(k)
i , y(k), y(k−1))⊤
× ∆ϕi′(x(k′)
i′ , y(k′), y(k′−1))µi(y(k−1), y(k))µi(y(k′−1), y(k′)),
where
µi(y(k)) =
c

y(1),...,y(k−1),y(k+1),...,y(mi )=1
αi,y,
µi(y(k−1), y(k)) =
c

y(1),...,y(k−2),y(k+1),...,y(mi )=1
αi,y.
Thus, exponentially many variables {αi,y}i,y do not have to be optimized, but only
optimizing {µi(y(k))}mi
k=1 and {µi(y(k−1), y(k))}mi
k=1 for i = 1,. . . ,n is sufficient.

29.3 DETERMINISTIC CLASSIFICATION FOR SEQUENCES
339
However, since {µi(y(k))}mi
k=1 and {µi(y(k−1), y(k))}mi
k=1 are related to each other, the
additional constraint,
c

y(k−1)=1
µi(y(k−1), y(k)) = µi(y(k)),
is necessary, together with the original constraints:
c

y(k−1)=1
µi(y(k)) = C and
µi(y(k−1), y(k)) ≥0.
From the dual solution {µi(y(k))}mi
k=1 and {µi(y(k−1), y(k))}mi
k=1, the primal solu-
tion ζ can be obtained as
ζ =
n

i=1
mi

k=1
c

y(k−1),y(k)=1
µi(y(k−1), y(k))∆ϕi(x(k)
i , y(k), y(k−1)).
Once the solution ζ is obtained, classification of a test sample x = (x(1),. . . , x(m)),
argmax
y(1),...,y(m)∈{1,...,c}
ζ ⊤ϕ(x, y),
can be performed exactly in the same way as that described in Section 29.2.4.
This method is referred to as the structured support vector machine [114].


PART
FURTHER
TOPICS
5
30
ENSEMBLE LEARNING. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
31
ONLINE LEARNING . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
32
CONFIDENCE OF PREDICTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
33
SEMISUPERVISED LEARNING . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
34
MULTITASK LEARNING . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
35
LINEAR DIMENSIONALITY REDUCTION. . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
36
NONLINEAR DIMENSIONALITY REDUCTION . . . . . . . . . . . . . . . . . . . . . . . 429
37
CLUSTERING . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
38
OUTLIER DETECTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
39
CHANGE DETECTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469

342
PART 5
FURTHER TOPICS
Part 5 is devoted to introducing various advanced topics in machine learning.
In Chapter 30, methods of ensemble learning are introduced, which are aimed
at combining multiple weak learning algorithms to produce a strong learning
algorithm. In Chapter 31, methods of online learning are introduced, which provide
computationally efficient means to learn from training data given sequentially. In
Chapter 32, methods to estimate the confidence of prediction are introduced.
Then various techniques for improving the performance of supervised learning
based on side information are discussed. In Chapter 33, the framework of semisuper-
vised learning is discussed, which tries to make use of additional input-only samples.
This chapter also includes methods of transfer learning, which are aimed at utilizing
training data of other related learning tasks. In Chapter 34, methods of multitask
learning are introduced, which solve multiple related learning tasks simultaneously
by sharing common information.
In Chapter 35, methods of dimensionality reduction are introduced for extracting
useful low-dimensional feature representations, covering linear supervised and unsu-
pervised methods. Then Chapter 36 focuses on non-linear dimensionality reduction
methods.
Finally, various unsupervised learning methods are covered. In Chapter 37,
methods of clustering are introduced, which are aimed at grouping data samples
based on their similarity. In Chapter 38, methods of outlier detection are introduced,
which try to identify anomalous samples in a given data set. In Chapter 39, methods
of change detection between data sets are introduced.

CHAPTER
ENSEMBLE LEARNING30
CHAPTER CONTENTS
Decision Stump Classifier ........................................................ 343
Bagging .......................................................................... 344
Boosting ......................................................................... 346
Adaboost ................................................................... 348
Loss Minimization View .................................................... 348
General Ensemble Learning ...................................................... 354
Ensemble learning is a framework of combining multiple weak learning algorithms
to produce a strong learning algorithm. In this chapter, two types of ensemble
learning approaches, bagging and boosting, are introduced (Fig. 30.1). Although the
idea of ensemble learning can be applied to both regression and classification, only
classification is considered in this chapter.
30.1 DECISION STUMP CLASSIFIER
As an example of a weak learning algorithm, let us consider a decision stump
classifier, which is a depth-one version of decision trees. More specifically, a decision
stump classifier randomly chooses one of the elements in the d-dimensional input
vector x = (x(1),. . . , x(d))⊤and classification is performed by thresholding the
chosen element. This means that the decision boundary is parallel to one of the
coordinate axes (Fig. 30.2).
The decision stump may be a poor classifier in terms of the classification accuracy
because of its low degree of freedom. Nevertheless, at least, it has advantage in
computation costs because there only exist n + 1 solutions for n training samples.
Indeed, the global optimal solution can be easily obtained by just sorting the n
training samples along the chosen axis and find the best interval that minimizes the
classification error.
A MATLAB code for decision stump classification is provided in Fig. 30.3, and
its behavior is illustrated in Fig. 30.4. This shows that, as expected, decision stump
classification performs poorly.
In the rest of this chapter, ensemble learning methods for improving the perfor-
mance of this poor decision stump classifier are introduced.
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00041-8
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
343

344
CHAPTER 30
ENSEMBLE LEARNING
(a) Bagging
(b) Boosting
FIGURE 30.1
Ensemble learning. Bagging trains weak learners in parallel, while boosting sequentially trains
weak learners.
(a) Decision stump
(b) Decision tree
FIGURE 30.2
Decision stump and decision tree classifiers. A decision stump is a depth-one version of a decision
tree.
30.2 BAGGING
The name, bagging, stems from bootstrap aggregation [21]. The bootstrap method
is a resampling technique to generate slightly different data sets from the original
training data set (see Section 9.3.2), and bagging aggregates many classifiers trained
with slightly different data sets (Fig. 30.1(a)). Since slightly different data sets
give slightly different classifiers, averaging them is expected to give a more stable
classifier. The algorithm of bagging is described in Fig. 30.5.

30.2 BAGGING
345
x=randn(50,2); y=2*(x(:,1)>x(:,2))-1;
X0=linspace(-3,3,50); [X(:,:,1) X(:,:,2)]=meshgrid(X0);
d=ceil(2*rand); [xs,xi]=sort(x(:,d));
el=cumsum(y(xi)); eu=cumsum(y(xi(end:-1:1)));
e=eu(end-1:-1:1)-el(1:end-1); [em,ei]=max(abs(e));
c=mean(xs(ei:ei+1)); s=sign(e(ei)); Y=sign(s*(X(:,:,d)-c));
figure(1); clf; hold on; axis([-3 3 -3 3]);
colormap([1 0.7 1; 0.7 1 1]); contourf(X0,X0,Y);
plot(x(y==1,1),x(y==1,2),’bo’);
plot(x(y==-1,1),x(y==-1,2),’rx’);
FIGURE 30.3
MATLAB code for decision stump classification.
FIGURE 30.4
Example of decision stump classification.
A MATLAB code of bagging for decision stumps is provided in Fig. 30.6, and
its behavior is illustrated in Fig. 30.7. This shows that ensemble learning by bagging
improves the performance of a decision stump significantly. Note that each bootstrap
training procedure is independent of each other, and thus bagging can be computed
efficiently in parallel computing environments.
Bagging applied to decision trees is called the random forest [22], which is known
to be a highly practical algorithm.

346
CHAPTER 30
ENSEMBLE LEARNING
1. For j = 1,. . . ,b
(a) Randomly choose n samples from {(xi, yi)}n
i=1 with replacement (see
Fig. 3.3).
(b) Train a classifier φj with the randomly resampled data set.
2. Output the average of {φj}b
j=1 as the final solution f :
f (x) ←−1
b
b

j=1
φj(x).
FIGURE 30.5
Algorithm of bagging.
n=50; x=randn(n,2); y=2*(x(:,1)>x(:,2))-1;
b=5000; a=50; Y=zeros(a,a);
X0=linspace(-3,3,a); [X(:,:,1) X(:,:,2)]=meshgrid(X0);
for j=1:b
db=ceil(2*rand); r=ceil(n*rand(n,1));
xb=x(r,:); yb=y(r); [xs,xi]=sort(xb(:,db));
el=cumsum(yb(xi)); eu=cumsum(yb(xi(end:-1:1)));
e=eu(end-1:-1:1)-el(1:end-1);
[em,ei]=max(abs(e)); c=mean(xs(ei:ei+1));
s=sign(e(ei)); Y=Y+sign(s*(X(:,:,db)-c))/b;
end
figure(1); clf; hold on; axis([-3 3 -3 3]);
colormap([1 0.7 1; 0.7 1 1]); contourf(X0,X0,sign(Y));
plot(x(y==1,1),x(y==1,2),’bo’);
plot(x(y==-1,1),x(y==-1,2),’rx’);
FIGURE 30.6
MATLAB code of bagging for decision stumps.
30.3 BOOSTING
While bagging trained multiple weak learning machines in parallel, boosting [87]
trains them in a sequential manner (Fig. 30.1(b)). In this section, a fundamental
algorithm of boosting is introduced.

30.3 BOOSTING
347
(a) j = 1
(b) j = 2
(c) j = 3
(d) j = 4
(e) j = 5
(f) j = 6
(g) Average of b = 5000 decision stumps
FIGURE 30.7
Example of bagging for decision stumps.

348
CHAPTER 30
ENSEMBLE LEARNING
30.3.1 ADABOOST
In boosting, a weak classifier is first trained with training samples as usual:
{(xi, yi) | xi ∈Rd, yi ∈{+1,−1}}n
i=1.
Because the weak classifier is less powerful, perhaps all training samples cannot
be correctly classified. The basic idea of boosting is to assign large weight to the
(difficult) training samples that were not correctly classified by the first classifier and
train the second classifier with the weights. Thanks to this weighting scheme, the
second classifier trained in this way would correctly classify some of the difficult
training samples.
Since difficult training samples tend to have large weights, repeating this weighted
learning procedure would lead to a classifier that can correctly classify the most
difficult training samples. On the other hand, during this iterative procedure, easy
training samples have relatively small weights and thus the final classifier may
incorrectly classify such easy training samples. For this reason, boosting does not
only use the final classifier but also considers weighted voting of all classifiers
obtained through the iterative learning procedure. There are various different ways
to perform weighted voting, and adaboost is one of the popular approaches, which is
summarized in Fig. 30.8.
The update formula for sample weights {wi}n
i=1 in Fig. 30.8 can be expressed as
wi ←−wi exp

−θ jφj(xi)yi

,
followed by normalization to satisfy n
i=1 wi
= 1. This implies that adaboost
decreases the sample weight if margin mi = φj(xi)yi is positive (i.e. the ith training
sample is correctly classified) and increases the sample weight if margin mi is
negative. The derivation of this update formula will be explained in Section 30.3.2.
The confidence θ j of weak classifier φj in Fig. 30.8,
θ j = 1
2 log 1 −R(φj)
R(φj)
,
takes a large/small value if the weighted misclassification rate R(φ) is small/large
(Fig. 30.9). The derivation of this formula will also be explained in Section 30.3.2.
A MATLAB code of adaboost for decision stumps is provided in Fig. 30.10, and
its behavior is illustrated in Fig. 30.11. This shows that ensemble learning by boosting
gives a strong classifier.
30.3.2 LOSS MINIMIZATION VIEW
The adaboost algorithm introduced above was derived as an ensemble learning
method, which is quite different from the LS formulation explained in Chapter 26.
However, adaboost can actually be interpreted as an extension of the LS method,
and this interpretation allows us to derive, e.g. robust and probabilistic variations of
adaboost.

30.3 BOOSTING
349
1. Initialize sample weights {wi}n
i=1 for {(xi, yi)}n
i=1 to be uniform and the
final strong classifier f to be zero:
w1 = · · · = wn = 1/n and
f ←−0.
2. For j = 1,. . . ,b
(a) Train a weak classifier φj with the current sample weights {wi}n
i=1 to
minimize the weighted misclassification rate R(φ):
φj = argmin
ϕ
R(φ),
where
R(φ) =
n

i=1
wi
2

1 −φ(xi)yi

.
(b) Set the confidence θ j of weak classifier φj at
θ j = 1
2 log 1 −R(φj)
R(φj)
.
(c) Update the strong classifier f as
f ←−f + θ jφj.
(d) Update the sample weights {wi}n
i=1 as
wi ←−
exp

−f (xi)yi

n
i′=1 exp

−f (xi′)yi′
,
∀i = 1,. . . ,n.
FIGURE 30.8
Algorithm of adaboost.
As a surrogate loss to the 0/1-loss (see Section 26.2), let us consider the
exponential loss (Fig. 30.12):
exp(−m),
where m = fθ(x)y is the margin. For the linear-in-parameter model,
fθ(x) =
b

j=1
θ jφj(x),
where {φj(x)}b
j=1 are binary-valued basis functions that only takes either −1 or +1,
let us consider exponential loss minimization:

350
CHAPTER 30
ENSEMBLE LEARNING
FIGURE 30.9
Confidence of classifier in adaboost. The confidence of classifier
φ, denoted by θ, is determined based on the weighted misclassi-
fication rate R.
min
θ
n

i=1
exp

−fθ(xi)yi

.
(30.1)
Suppose that not only the parameters {θ j}b
j=1 but also the basis functions {φj}b
j=1
can be learned in a sequential manner one by one. Let f be the learned function
obtained so far and let θ and φ be the next parameter and basis function to be learned,
respectively. Then Eq. (30.1) yields weighted exponential loss minimization:
min
θ,ϕ
n

i=1
exp

−
f (xi) + θφ(xi)

yi

= min
θ,ϕ
n

i=1
wi exp

−θφ(xi)yi

,
where the weight wi is given by
wi = exp

−f (xi)yi

.
For the sake of simplicity, θ ≥0 is assumed below (when θ < 0, the sign of
φ is flipped to satisfy θ ≥0). Then the above sequential weighted exponential loss
minimization can be rewritten as
n

i=1
wi exp

−θφ(xi)yi

= exp(−θ)

i:yi=ϕ(xi)
wi + exp(θ)

i:yi,ϕ(xi)
wi
=

exp(θ) −exp(−θ)

n

i=1
wi
2

1 −φ(xi)yi

+ exp(−θ)
n

i=1
wi.
(30.2)

30.3 BOOSTING
351
n=50; x=randn(n,2); y=2*(x(:,1)>x(:,2))-1; b=5000;
a=50; Y=zeros(a,a); yy=zeros(size(y)); w=ones(n,1)/n;
X0=linspace(-3,3,a); [X(:,:,1) X(:,:,2)]=meshgrid(X0);
for j=1:b
wy=w.*y; d=ceil(2*rand); [xs,xi]=sort(x(:,d));
el=cumsum(wy(xi)); eu=cumsum(wy(xi(end:-1:1)));
e=eu(end-1:-1:1)-el(1:end-1);
[em,ei]=max(abs(e)); c=mean(xs(ei:ei+1)); s=sign(e(ei));
yh=sign(s*(x(:,d)-c)); R=w’*(1-yh.*y)/2;
t=log((1-R)/R)/2; yy=yy+yh*t; w=exp(-yy.*y); w=w/sum(w);
Y=Y+sign(s*(X(:,:,d)-c))*t;
end
figure(1); clf; hold on; axis([-3 3 -3 3]);
colormap([1 0.7 1; 0.7 1 1]); contourf(X0,X0,sign(Y));
plot(x(y==1,1),x(y==1,2),’bo’);
plot(x(y==-1,1),x(y==-1,2),’rx’);
FIGURE 30.10
MATLAB code of adaboost for decision stumps.
Thus, the minimizer φ of Eq. (30.2) with respect to φ is given by weighted 0/1-loss
minimization:
φ = argmin
ϕ
n

i=1
wi
2

1 −φ(xi)yi

.
The minimizer θ of Eq. (30.2) with respect to θ can be obtained by substituting φ
to φ in Eq. (30.2), differentiating it with respect to θ, and setting it at 0:

exp(θ) + exp(−θ)

n

i=1
wi
2

1 −φ(xi)yi

−exp(−θ)
n

i=1
wi = 0.
Solving this for θ yields
θ = 1
2 log 1 −R
R
,
where R denotes the weighted 0/1-loss for φ:
R =

n

i=1
wi
2

1 −φ(xi)yi


n

i′=1
wi′

.

352
CHAPTER 30
ENSEMBLE LEARNING
(a) j = 1
(b) j = 2
(c) j = 3
(d) j = 4
(e) j = 5
(f) j = 6
(g) Combination of b = 5000 decision stumps
FIGURE 30.11
Example of adaboost for decision stumps.

30.3 BOOSTING
353
FIGURE 30.12
Exponential loss.
FIGURE 30.13
Loss functions for boosting.
Since φ and θ obtained above are equivalent to the one used in adaboost (see
Fig. 30.8), adaboost was shown to be equivalent to sequential weighted exponential
loss minimization.
The above equivalence allows us to consider variations of adaboost for other loss
functions. For example, a modification of adaboost, called madaboost [37], uses

−m + 1/2
(m ≤0)
exp(−2m)/2
(m > 0)

354
CHAPTER 30
ENSEMBLE LEARNING
as a loss function (Fig. 30.13). This loss function leads to robust classification since
it increases only linearly. Another popular variation is logitboost [43], which uses
log(1 + exp(−2m))
as a loss function (Fig. 30.13). Logitboost corresponds to a boosting version of logis-
tic regression introduced in Section 28.1, which allows probabilistic interpretation.
30.4 GENERAL ENSEMBLE LEARNING
Beyond systematic ensemble learning methods such as bagging and boosting, voting
by various different learning algorithms is highly promising in practical applications.
For example, the winning algorithms in real-world data analysis competitions such
as Netflix Prize1 and KDD Cup 20132 adopt such voting approaches. If additional
validation data samples are available, voting weights may be optimized so that the
prediction error for the validation samples is minimized.
Thus, beyond improving each learning algorithm, ensemble learning can be
regarded as a final means to boost the prediction performance.
1http://www.netflixprize.com/.
2http://www.kdd.org/kddcup2013/content/kdd-cup-2013-workshop-0.

CHAPTER
ONLINE LEARNING 31
CHAPTER CONTENTS
Stochastic Gradient Descent ..................................................... 355
Passive-Aggressive Learning...................................................... 356
Classification ............................................................... 357
Regression ................................................................. 358
Adaptive Regularization of Weight Vectors (AROW) ............................... 360
Uncertainty of Parameters ................................................. 360
Classification ............................................................... 361
Regression ................................................................. 362
The supervised learning methods explained so far handled all training samples
{(xi, yi)}n
i=1 at the same time, which is called batch learning. On the other hand,
when training samples are provided one by one in a sequential manner, it would
be effective to perform online learning, i.e. a new training sample is incrementally
learned upon the current learned result. Online learning is also useful when the
number of training samples is so large that all training samples cannot be stored
in memory. In this chapter, various online learning algorithms are introduced.
For simplicity, only the linear-in-input model is considered in this chapter:
fθ(x) = θ⊤x.
However, all methods introduced in this section can be easily extended to fθ(x)
= θ⊤φ(x), linear-in-parameter models (see Section 21.1) with basis function φ(x).
31.1 STOCHASTIC GRADIENT DESCENT
The stochastic gradient algorithm introduced in Section 22.4 is one of the typical
online learning algorithms, which updates the parameter to reduce the loss for a new
training sample.
More specifically, for the new training sample (x, y) and a loss function J, the
parameter θ is updated along the gradient of the loss ∇J as
θ ←−θ −ε∇J(θ),
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00042-X
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
355

356
CHAPTER 31
ONLINE LEARNING
(a) Too large step size
(b) Too small step size
FIGURE 31.1
Choice of step size. Too large step size overshoots the optimal solution, while too small step size
yields slow convergence.
where ε > 0 denotes the step size. See Fig. 22.6 for the detailed algorithm of the
stochastic gradient method.
Although stochastic gradient is simple and easy to use, choice of step size ε
is often cumbersome in practice, as illustrated in Fig. 31.1: too large step size
overshoots the optimal solution, while too small step size yields slow convergence.
Starting from a large ε and then reducing ε gradually, called simulated annealing,
would be useful to control the step size. However, choice of initial ε and the
decreasing factor of ε is not straightforward in practice.
For simple loss functions such as the squared loss,
J(θ) = 1
2

y −θ⊤x
2,
it is actually possible to find the minimizer with respect to θ analytically. Then step
size selection no longer matters, which is highly appealing in practice. However, this
often changes the parameter θ drastically, implying that the knowledge acquired so
far can be completely spoiled by addition of a single, possibly noisy training sample.
Thus, in practice, it is important to choose the step size to be not too large.
31.2 PASSIVE-AGGRESSIVE LEARNING
In this section, a simple online learning method called passive-aggressive learning
[32] is introduced, which better controls the step size.
More specifically, in passive-aggressive learning, the amount of change from the
current solution θ is included as a penalty term:
θ = argmin
θ

J(θ) + λ
2 ∥θ −θ∥2

,
(31.1)
where λ > 0 is the passiveness parameter. Minimizing the first term in Eq. (31.1)
corresponds to aggressively updating the parameter, while minimizing the second

31.2 PASSIVE-AGGRESSIVE LEARNING
357
term in Eq. (31.1) corresponds to passively keeping the previous solution. Below,
passive-aggressive algorithms for classification and regression are introduced.
31.2.1 CLASSIFICATION
For classification, let us employ the squared hinge loss (see Fig. 27.12):
J(θ) = 1
2

max

0,1 −m
2 ,
where m = θ⊤xy denotes the margin. Then the solution of passive-aggressive
classification can be obtained analytically as follows.
First, the squared hinge loss can be expressed as
J(θ) = min
ξ
1
2ξ2
subject to ξ ≥1 −m and ξ ≥0.
Note that ξ ≥0 is actually automatically fulfilled because the objective function only
contains ξ2 and ξ = 0 is the solution when 1 −m ≤0. Then the objective function in
Eq. (31.1) can be written as
θ = argmin
θ,ξ
1
2ξ2 + λ
2 ∥θ −θ∥2

subject to ξ ≥1 −m.
(31.2)
When 1 −m ≤0, ξ = 0 is the solution and thus θ = θ is obtained. This means
that the new solution θ is the same as the current solution θ.
When 1 −m > 0, let us introduce the Lagrange multiplier α to constrained
optimization problem (31.2) and define the Lagrange function (see Fig. 23.5) as
follows:
L(θ,ξ,α) = 1
2ξ2 + λ
2 ∥θ −θ∥2 + α(1 −m −ξ).
(31.3)
Then the KKT conditions (Fig. 27.7) yield
∂L
∂θ = 0 =⇒θ = θ + αy
λ x,
(31.4)
∂L
∂ξ = 0 =⇒
ξ = α.
Substituting these conditions into Lagrange function (31.3) and eliminating θ and ξ
yield
L(α) = −α2
2
∥x∥2
λ
+ 1

+ α

1 −θ⊤xy

.
Taking the derivative of L(α) and setting it at zero give the maximizer α analytically
as
α = 1 −θ⊤xy
∥x∥2/λ + 1.

358
CHAPTER 31
ONLINE LEARNING
1. Initialize θ ←−0.
2. Update the parameter θ using a new training sample (x, y) as
θ ←−θ + y max(0,1 −θ⊤xy)
∥x∥2 + λ
x.
3. Go to Step 2.
FIGURE 31.2
Algorithm of passive-aggressive classification.
Substituting this back into Eq. (31.4) gives
θ = θ + (1 −θ⊤xy)y
∥x∥2 + λ
x.
Altogether, the final solution θ is expressed as
θ = θ + y max(0,1 −θ⊤xy)
∥x∥2 + λ
x.
The algorithm of passive-aggressive classification is summarized in Fig. 31.2.
A MATLAB code for passive-aggressive classification is provided in Fig. 31.3,
and its behavior is illustrated in Fig. 31.4. This shows that, after three iterations,
almost the same solution as the final one can be obtained that well separates positive
and negative samples. In this implementation, the intercept of the linear model is
expressed by augmenting input vector x = (x(1),. . . , x(d)) as (x⊤,1)⊤:
fθ(x) = θ⊤(x⊤,1)⊤=
d

j=1
θ j x(j) + θd+1.
Note that, for the ordinary hinge loss (Fig. 27.12(a)),
J(θ) = max

0,1 −m

,
the passive-aggressive algorithm still gives an analytic update formula:
θ ←−θ + y min
1
λ , max(0,1 −m)
∥x∥2

x.
31.2.2 REGRESSION
The idea of passive-aggressive learning can also be applied to regression. For residual
r = y −θ⊤x, let us employ the ℓ2-loss and the ℓ1-loss (Fig. 25.2):
J(θ) = 1
2r2 and
J(θ) = |r|.

31.2 PASSIVE-AGGRESSIVE LEARNING
359
n=200; x=[randn(1,n/2)-5 randn(1,n/2)+5; 5*randn(1,n)]’;
y=[ones(n/2,1);-ones(n/2,1)];
x(:,3)=1; p=randperm(n); x=x(p,:); y=y(p);
t=zeros(3,1); l=1;
for i=1:n
xi=x(i,:)’; yi=y(i);
t=t+yi*max(0,1-t’*xi*yi)/(xi’*xi+l)*xi;
end
figure(1); clf; hold on; axis([-10 10 -10 10]);
plot(x(y==1,1),x(y==1,2),’bo’);
plot(x(y==-1,1),x(y==-1,2),’rx’);
plot([-10 10],-(t(3)+[-10 10]*t(1))/t(2),’k-’);
FIGURE 31.3
MATLAB code for passive-aggressive classification.
(a) After 3 iterations
(b) After 5 iterations
(c) After 200 iterations
FIGURE 31.4
Example of passive-aggressive classification.
Then, with a similar derivation to the classification case, the update formulas for
passive-aggressive regression can be obtained analytically as
θ ←−θ +
r
∥x∥2 + λ x and θ ←−θ + sign (r) min
1
λ , |r|
∥x∥2

x,
where sign (r) denotes the sign of r. As shown in Section 22.4, the stochastic gradient
update rule for the ℓ2-loss is given by
θ ←−θ + εrx,

360
CHAPTER 31
ONLINE LEARNING
n=50; N=1000; x=linspace(-3,3,n)’; x=x(randperm(n));
pix=pi*x; y=sin(pix)./(pix)+0.1*x+0.05*randn(n,1);
hh=2*0.3^2; t=randn(n,1); l=1;
for i=1:n
ki=exp(-(x-x(i)).^2/hh);
t=t+(y(i)-t’*ki)/(ki’*ki+l)*ki;
end
X=linspace(-3,3,N)’;
K=exp(-(repmat(X.^2,1,n)+repmat(x.^2’,N,1)-2*X*x’)/hh);
F=K*t;
figure(1); clf; hold on; axis([-2.8 2.8 -0.5 1.2]);
plot(X,F,’g-’); plot(x,y,’bo’);
FIGURE 31.5
MATLAB code for passive-aggressive regression with the ℓ2-loss.
where ε
>
0 is the step size. This means that passive-aggressive regression
corresponds to stochastic gradient with step size ε = 1/(∥x∥2 + λ), and thus the
step size is adaptively chosen based on the sample x.
A MATLAB code for passive-aggressive regression with the ℓ2-loss is provided
in Fig. 31.5.
31.3 ADAPTIVE REGULARIZATION OF WEIGHT
VECTORS (AROW)
Since the passive-aggressive algorithm uses unbounded loss functions, it suffers a lot
from outliers. This problem can be mitigated if a bounded loss function, such as the
ramp loss (see Section 27.6.3) and the Tukey loss (see Section 25.4), is used. However,
bounded loss functions are nonconvex and thus optimization becomes cumbersome
in practice. In this section, a robust online learning algorithm called AROW [33] is
introduced, which utilizes the sequential nature of online learning.
31.3.1 UNCERTAINTY OF PARAMETERS
In AROW, parameter θ is not point-estimated, but its distribution is learned to take
into account its uncertainty.
As a distribution of parameters, let us consider the Gaussian distribution whose
probability density function is given as follows (Fig. 6.1):
(2π)−d/2det(Σ)−1/2 exp

−1
2(θ −µ)⊤Σ−1(θ −µ)

,

31.3 ADAPTIVE REGULARIZATION OF WEIGHT VECTORS (AROW)
361
where det(·) denotes the determinant, µ denotes the expectation vector, and Σ denotes
the variance-covariance matrix. Below, the Gaussian distribution with expectation
vector µ and variance-covariance matrix Σ is denoted by N(µ,Σ).
AROW learns the expectation vector µ and variance-covariance matrix Σ to
minimize the following criterion:
J(µ) + 1
2 x⊤Σx + λKL

N(µ,Σ)N(µ,Σ)

.
(31.5)
The first term J(µ) denotes the loss for a new training sample (x, y) when parameter
θ = µ is used for prediction. The second term 1
2 x⊤Σx is the regularization term for
variance-covariance matrix Σ, which is adaptive to the training input vector x. The
third term λKL(N(µ,Σ)∥N(µ,Σ)) controls the amount of change from the current
solution, where λ > 0 denotes the passiveness parameter, µ and Σ are the current
solutions for µ and Σ, and KL(p∥q) denotes the KL divergence (see Section 14.2)
from density p to density q:
KL(p∥q) =

p(x) log p(x)
q(x)dx.
Passive-aggressive learning introduced in Section 31.2 used the Euclidean distance
∥θ −θ∥2 for controlling the amount of change from the current solution θ. On the
other hand, AROW uses the KL divergence for taking into account the uncertainty
of parameters, which is expected to contribute to better controlling the amount of
change from the current solution.
The KL divergence for Gaussian distributions can be expressed analytically as
KL

N(µ,Σ)N(µ,Σ)

= 1
2
*
,
log det(Σ)
det(Σ) + tr(Σ
−1Σ) + (µ −µ)⊤Σ
−1(µ −µ) −d+
-
,
where d denotes the dimensionality of input vector x.
Below, specific AROW algorithms for classification and regression are intro-
duced.
31.3.2 CLASSIFICATION
Let us use the squared hinge loss (see Fig. 27.12) for expectation vector µ:
J(µ) = 1
2

max

0,1 −µ⊤xy
2 .
Then setting the derivative of Eq. (31.5) with respect to µ at zero yields that the
solution µ satisfies
µ = µ + y max(0,1 −µ⊤xy)Σx/β,
where β = x⊤Σx + λ.

362
CHAPTER 31
ONLINE LEARNING
1. Initialize µ ←−0 and Σ ←−I.
2. Update parameters µ and Σ as follows, if the margin m = µ⊤xy for a new
training sample (x, y) satisfies m < 1:
µ ←−µ + y max(0,1 −m)Σx/β and Σ ←−Σ −Σxx⊤Σ/β,
where β = x⊤Σx + λ.
3. Go to Step 2.
FIGURE 31.6
Algorithm of AROW classification.
The matrix derivative formulas (see Fig. 12.3),
∂
∂Σ log(det(Σ)) = Σ−1
and
∂
∂Σtr

Σ
−1Σ

= Σ
−1,
allow us to compute the partial derivative of Eq. (31.5) with respect to Σ, and setting
it at zero gives the solution Σ analytically as
Σ
−1 = Σ
−1 −xx⊤/λ.
Further applying the matrix inversion lemma (see Fig. 23.12) yields
Σ = Σ −Σxx⊤Σ/β.
This expression allows us to directly obtain the solution Σ without computing its
inverse explicitly.
The algorithm of AROW classification is summarized in Fig. 31.6. When the
dimensionality d of the input vector x is large, updating the d × d variance-
covariance matrix Σ is computationally expensive. A practical approach to mitigating
this problem is to only maintain the diagonal elements of Σ and regard all off-diagonal
elements as zero, which corresponds to only considering the Gaussian distribution
whose principal axes of the elliptic contour lines agree with the coordinate axes (see
Fig. 6.1).
A MATLAB code for AROW classification is provided in Fig. 31.7, and its
behavior is illustrated in Fig. 31.8. This shows that AROW suppresses the influence
of outliers better than passive-aggressive learning.
31.3.3 REGRESSION
Let us use the squared loss for the expectation vector µ:
J(µ) = 1
2
 y −µ⊤x2 .

31.3 ADAPTIVE REGULARIZATION OF WEIGHT VECTORS (AROW)
363
n=50; x=[randn(1,n/2)-15 randn(1,n/2)-5; randn(1,n)]’;
y=[ones(n/2,1); -ones(n/2,1)]; x(1:2,1)=x(1:2,1)+10;
x(:,3)=1; p=randperm(n); x=x(p,:); y=y(p);
mu=zeros(3,1); S=eye(3); l=1;
for i=1:n
xi=x(i,:)’; yi=y(i); z=S*xi; b=xi’*z+l; m=yi*mu’*xi;
if m<1, mu=mu+yi*(1-m)*z/b; S=S-z*z’/b; end
end
figure(1); clf; hold on; axis([-20 0 -2 2]);
plot(x(y==1,1),x(y==1,2),’bo’);
plot(x(y==-1,1),x(y==-1,2),’rx’);
plot([-20 0],-(mu(3)+[-20 0]*mu(1))/mu(2),’k-’);
FIGURE 31.7
MATLAB code for AROW classification.
(a) Passive-aggressive
(b) AROW
FIGURE 31.8
Examples of passive-aggressive and AROW classifications.
Then, with a similar derivation to the classification case, the update formula for
AROW regression can be obtained analytically as
µ ←−µ + (y −µ⊤x)Σx/β and Σ ←−Σ −Σxx⊤Σ/β,
where β = x⊤Σx + λ. This method is also referred to as recursive LS. When Σ
is fixed to the identity matrix and setting θ = µ, AROW regression is reduced to

364
CHAPTER 31
ONLINE LEARNING
n=50; N=1000; x=linspace(-3,3,n)’; X=linspace(-3,3,N)’;
pix=pi*x; y=sin(pix)./(pix)+0.1*x+0.05*randn(n,1);
hh=2*0.3^2; m=randn(n,1); S=eye(n); l=1;
for j=1:100
for i=1:n
ki=exp(-(x-x(i)).^2/hh); Sk=S*ki;
b=ki’*Sk+l; m=m+Sk*(y(i)-ki’*m)/b; S=S-Sk*Sk’/b;
end, end
K=exp(-(repmat(X.^2,1,n)+repmat(x.^2’,N,1)-2*X*x’)/hh);
F=K*m;
figure(1); clf; hold on; axis([-2.8 2.8 -0.5 1.2]);
plot(X,F,’g-’); plot(x,y,’bo’);
FIGURE 31.9
MATLAB code for AROW regression.
passive-aggressive regression. Thus, AROW can be regarded as a natural extension
of passive-aggressive learning.
A MATLAB code for AROW regression is provided in Fig. 31.9.

CHAPTER
CONFIDENCE OF
PREDICTION
32
CHAPTER CONTENTS
Predictive Variance for ℓ2-Regularized LS ........................................ 365
Bootstrap Confidence Estimation ................................................. 367
Applications ..................................................................... 368
Time-series Prediction...................................................... 368
Tuning Parameter Optimization ............................................ 369
By the supervised learning methods introduced in Part 4, output values for arbitrary
input points can be predicted. In addition to predicting output values, however, it is
useful to assess the confidence of prediction in some applications. The probabilistic
classification methods introduced in Chapter 28 allow us to learn the confidence of
classification, but its range of applications was limited to classification. In this chap-
ter, more general methods for assessing the confidence of prediction are discussed.
32.1 PREDICTIVE VARIANCE FOR ℓ2-REGULARIZED LS
Let us consider regression by ℓ2-regularized LS for linear-in-parameter models (see
Chapter 23). More specifically, from input-output paired samples {(xi, yi)}n
i=1, a
predictor of output for input x is obtained as
f (x) =
b

j=1
θ jϕj(x) = θ⊤φ(x),
where θ = (θ1,. . . ,θb)⊤is the vector of learned parameters given by
θ = (Φ⊤Φ + λI)−1Φ⊤y,
and φ(x) = (ϕ1(x),. . . ,ϕb(x))⊤is the vector of basis functions. Φ is the design
matrix given by
Φ =
*.....
,
ϕ1(x1) · · · ϕb(x1)
...
...
...
ϕ1(xn) · · · ϕb(xn)
+/////
-
,
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00043-1
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
365

366
CHAPTER 32
CONFIDENCE OF PREDICTION
λ ≥0 is the regularization parameter, I is the identity matrix, and y = (y1,. . . , yn)⊤
is the training output vector.
Suppose that training output vector y is given by
y = Φθ∗+ ϵ.
Here, θ∗is the true parameter and ϵ is a noise vector such that
E[ϵ] = 0 and E[ϵϵ⊤] = σ2I,
where E denotes the expectation over ϵ and σ2 denotes the noise variance. Then the
variance of prediction at a test input point x,
V(x) = E
f (x) −E[ f (x)]
2
,
can be computed analytically as
V(x) = E

φ(x)⊤θ −φ(x)⊤E[θ]
2
= E

φ(x)⊤ (Φ⊤Φ + λI)−1Φ⊤(Φθ∗+ ϵ −Φθ∗−E[ϵ])2
= E

φ(x)⊤(Φ⊤Φ + λI)−1Φ⊤ϵ
2
= φ(x)⊤(Φ⊤Φ + λI)−1Φ⊤E[ϵϵ⊤]Φ(Φ⊤Φ + λI)−1φ(x)
= σ2Φ(Φ⊤Φ + λI)−1φ(x)
2.
The noise variance σ2 may be estimated as
σ2 = ∥U y∥2
tr (U) ,
where
U = I −Φ(Φ⊤Φ + λI)−1Φ⊤.
tr (U) is often called the effective number of parameters. When λ = 0, the above σ2
is reduced to the standard unbiased estimator of the noise variance, given by the sum
of squared residuals divided by the number of parameters.
A MATLAB code for analytic computation of predictive variance is provided in
Fig. 32.1, and its behavior is illustrated in Fig. 32.2. This shows that, depending on
the density of training samples, confidence intervals are adaptively estimated.
Note that the above analytic computation technique can be extended to any
estimator θ that is linear with respect to y. It can also be applied to correlated noise,
as long as the variance-covariance matrix of the noise can be estimated.

32.2 BOOTSTRAP CONFIDENCE ESTIMATION
367
n=50; x=linspace(-3,3,n)’; %x=randn(n,1);
N=1000; X=linspace(-3,3,N)’;
pix=pi*x; y=sin(pix)./(pix)+0.1*x+0.2*randn(n,1);
x2=x.^2; xx=repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’;
hhs=2*[0.03 0.3 3].^2; ls=[0.0001 0.1 100];
m=5; u=mod(randperm(n),m)+1;
for hk=1:length(hhs)
hh=hhs(hk); k=exp(-xx/hh);
for i=1:m
ki=k(u~=i,:); kc=k(u==i,:); yi=y(u~=i); yc=y(u==i);
for lk=1:length(ls)
t=(ki’*ki+ls(lk)*eye(n))\(ki’*yi);
g(hk,lk,i)=mean((yc-kc*t).^2);
end, end, end
[gl,ggl]=min(mean(g,3),[],2); [ghl,gghl]=min(gl);
L=ls(ggl(gghl)); HH=hhs(gghl);
K=exp(-(repmat(X.^2,1,n)+repmat(x2’,N,1)-2*X*x’)/HH);
k=exp(-xx/HH); Q=inv(k^2+L*eye(n)); Qk=Q*k’; t=Qk*y; F=K*t;
V=sum((K*Q*K’).^2,2)*sum((y-k’*t).^2)/(N-sum(sum(k.*Qk)));
figure(1); clf; hold on; axis([-2.8 2.8 -0.7 1.7]);
errorbar(X,F,sqrt(V),’y-’); plot(X,mean(F,2),’g-’);
plot(x,y,’bo’);
FIGURE 32.1
MATLAB code for analytic computation of predictive variance.
32.2 BOOTSTRAP CONFIDENCE ESTIMATION
The above analytic computation approach is useful to assess the confidence of
prediction. However, its range of applications is rather limited due to the linearity
assumption. Here, a more general approach based on the bootstrap technique (see
Section 9.3.2) is introduced, which can be applied to any learning methods for
assessing any statistics beyond the variance.
More specifically, from the original set of samples D
=
{(xi, yi)}n
i=1, n
pseudosamples D′ = {(x′
i, y′
i)}n
i=1 are generated by sampling with replacement (see
Fig. 3.3). Then, from the bootstrap samples D′ = {(x′
i, y′
i)}n
i=1, a predictor f (x) is
computed. This resampling and prediction procedure is repeated many times and its
variance (or any statistics such as quantiles) is computed.
A MATLAB code for bootstrap-based confidence estimation is provided in
Fig. 32.3, and its behavior is illustrated in Fig. 32.4. This shows that similar results
to Fig. 32.2 can be numerically obtained by bootstrapping.

368
CHAPTER 32
CONFIDENCE OF PREDICTION
(a) Uniform inputs
(b) Nonuniform inputs
FIGURE 32.2
Examples of analytic computation of predictive variance. The shaded area indicates the confi-
dence interval.
32.3 APPLICATIONS
In this section, useful applications of predictive confidence are described.
32.3.1 TIME-SERIES PREDICTION
Let us consider the problem of time-series prediction, i.e., given {yi}t
i=1, the objective
is to predict yt+1 (Fig. 32.5).
Given that input variables {xi}t
i=1 are not available in the current time-series
prediction setup, a simple modeling approach is to use m previous output values
{yt−i+1}m
i=1 to predict the next value yt+1 (Fig. 32.6):
yt+1 =
b

j=1
θ jϕj(yt−m+1,. . . , yt).
This approach also allows multiple-step ahead prediction as
yt+2 =
b

j=1
θ jϕj(yt−m+2,. . . , yt,yt+1),
yt+3 =
b

j=1
θ jϕj(yt−m+3,. . . , yt,yt+1,yt+2).
It is expected that prediction of distant future is more difficult, which may be assessed
by the prediction variance.
A MATLAB code of time-series prediction by ℓ2-regularized LS for the linear-
in-input model (which is also referred to as the autoregressive model in the context

32.3 APPLICATIONS
369
n=50; x0=linspace(-3,3,n)’; %x0=randn(n,1);
N=1000; X=linspace(-3,3,N)’;
pix=pi*x0; y0=sin(pix)./(pix)+0.1*x0+0.2*randn(n,1);
for s=1:100
r=ceil(n*rand(n,1));x=x0(r,:); y=y0(r);
x2=x.^2; xx=repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’;
hhs=2*[0.03 0.3 3].^2; ls=[0.0001 0.1 100];
m=5; u=mod(randperm(n),m)+1;
for hk=1:length(hhs)
hh=hhs(hk); k=exp(-xx/hh);
for i=1:m
ki=k(u~=i,:); kc=k(u==i,:); yi=y(u~=i); yc=y(u==i);
for lk=1:length(ls)
t=(ki’*ki+ls(lk)*eye(n))\(ki’*yi);
g(hk,lk,i)=mean((yc-kc*t).^2);
end, end, end
[gl,ggl]=min(mean(g,3),[],2); [ghl,gghl]=min(gl);
L=ls(ggl(gghl)); HH=hhs(gghl);
K=exp(-(repmat(X.^2,1,n)+repmat(x2’,N,1)-2*X*x’)/HH);
k=exp(-xx/HH); t=(k^2+L*eye(n))\(k*y); F(:,s)=K*t;
end
figure(1); clf; hold on; axis([-2.8 2.8 -0.7 1.7]);
errorbar(X,mean(F,2),std(F,0,2),’y-’);
plot(X,mean(F,2),’g-’); plot(x0,y0,’bo’);
FIGURE 32.3
MATLAB code for bootstrap-based confidence estimation.
of time-series prediction),
yt+1 =
b

j=1
θ j yt−j+1,
is provided in Fig. 32.7. Its behavior illustrated in Fig. 32.8 shows that the confidence
interval tends to grow as distant future is predicted.
32.3.2 TUNING PARAMETER OPTIMIZATION
The supervised learning methods introduced in Part 4 contain various tuning param-
eters such as the regularization parameter and the Gaussian bandwidth. Those tuning
parameters may be optimized by cross validation with respect to the prediction error,

370
CHAPTER 32
CONFIDENCE OF PREDICTION
(a) Uniform inputs
(b) Nonuniform inputs
FIGURE 32.4
Examples of bootstrap-based confidence estimation. The shaded area indicates the confidence
interval.
FIGURE 32.5
Problem of time-series prediction.
FIGURE 32.6
Time-series prediction from previous samples.

32.3 APPLICATIONS
371
u=17; z(1:u+1,1)=1.2; n=200; d=20; N=n+130;
for t=u+1:u+N+d
z(t+1)=0.9*z(t)+0.2*z(t-u)/(1+z(t-u)^10);
end
z=z(u+1:u+N+d); Y=z(d+1:end); x0=zeros(n+d-1,d);
%z=z+0.1*randn(N+d,1);
for i=1:d
x0(i:n+d-1,i)=z(1:n+d-i);
end
x0=x0(d:end,:); y0=z(d+1:n+d);
B=100; v=zeros(N-n+d,B);
for s=1:B
r=ceil(n*rand(n,1)); x=x0(r,:); y=y0(r);
x2=sum(x.^2,2); xx=repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’;
hhs=median(xx(:))*2*[0.5,0.2:1.5].^2; ls=[0.01 0.1 1 10];
m=5; u=mod(randperm(n),m)+1;
for hk=1:length(hhs)
hh=hhs(hk); k=exp(-xx/hh);
for i=1:m
ki=k(u~=i,:); kc=k(u==i,:); yi=y(u~=i); yc=y(u==i);
for lk=1:length(ls)
t=(ki’*ki+ls(lk)*eye(n))\(ki’*yi);
g(hk,lk,i)=mean((yc-kc*t).^2);
end, end, end
[gl,ggl]=min(mean(g,3),[],2); [ghl,gghl]=min(gl);
L=ls(ggl(gghl)); HH=hhs(gghl); k=exp(-xx/HH);
t=(k^2+L*eye(n))\(k*y); v(:,s)=[y0(n-d+1:n); zeros(N-n,1)];
for i=1:N-n
X=fliplr(v(i:d+i-1,s)’);
K=exp(-(repmat(sum(X.^2),1,n)+x2’-2*X*x’)/HH);
v(d+i,s)=K*t;
end, end
figure(1); clf; hold on; a=mean(v(d+1:end,:),2);
errorbar([n+1:N],a,std(v(d+1:end,:),0,2),’y-’);
plot([1:N],Y,’r--’); plot([n+1:N],a,’g-’);
plot([1:n],y0,’ko’); legend(’’,’True’,’Estimated’,’Sample’,4)
FIGURE 32.7
MATLAB code for time-series prediction by ℓ2-regularized LS.

372
CHAPTER 32
CONFIDENCE OF PREDICTION
(a) Noiseless case
(b) Noisy case
FIGURE 32.8
Examples of time-series prediction by ℓ2-regularized LS. The shaded areas indicate the confi-
dence intervals.

32.3 APPLICATIONS
373
(a) Choosing the third input point z3
(b) After observing the third output value y3
FIGURE 32.9
Bayesian optimization. The shaded areas indicate the confidence intervals.
as discussed in Section 23.3. However, naive grid search over tuning parameters is
computationally expensive when multiple tuning parameters are optimized.
To cope with this problem, Bayesian optimization [12] has attracted a great
deal of attention recently. Its basic idea is to find the maximizer of an unknown
function g(z):
z∗= argmax
z
g(z).
In the current setup, the function g corresponds the prediction performance (i.e., the
negative of the prediction error), and z corresponds to the tuning parameters.
If cross validation is performed for some values zi, corresponding output values
yi = g(zi) can be observed. Then the unknown function g(z) can be learned from
those obtained values {(zi, yi)}n
i=1 by any regression method. The basic idea of

374
CHAPTER 32
CONFIDENCE OF PREDICTION
Bayesian optimization is to iteratively learn the function g(z) from {(zi, yi)}n
i=1 and
choose the next input point zn+1 to observe its output value yn+1 (Fig. 32.9(a)).
For function gn(z) learned from {(zi, yi)}n
i=1, the most naive choice of zn+1 would
be the maximizer:
zn+1 = argmax
z
gn(z).
A more sensible approach is to take into account the uncertainty of the learned
function gn(z), e.g., by the upper confidence bound [10]:
zn+1 = argmax
z

gn(z) + kσn(z)

,
where k is a constant and σn(z) is the standard deviation of gn(z).
Other popular choices are the probability of improvement (PI) and the expected
improvement (EI):
PI:
zn+1 = argmax
z
Pr

gn(z) ≥(1 + k) max
i=1,...,n yi

,
EI:
zn+1 = argmax
z
∞
0
Pr

gn(z) >
max
i=1,...,n yi + m

mdm,
where k > 0 is a constant. When gn(z) is assumed to follow a Gaussian distribution,
the PI and the EI (i.e., the right-hand sides of the above equations) can be computed
analytically.

CHAPTER
SEMISUPERVISED
LEARNING
33
CHAPTER CONTENTS
Manifold Regularization .......................................................... 375
Manifold Structure Brought by Input Samples ............................. 375
Computing the Solution .................................................... 377
Covariate Shift Adaptation ....................................................... 378
Importance Weighted Learning............................................. 378
Relative Importance Weighted Learning.................................... 382
Importance Weighted Cross Validation ..................................... 382
Importance Estimation ..................................................... 383
Class-balance Change Adaptation ................................................ 385
Class-balance Weighted Learning .......................................... 385
Class-balance Estimation .................................................. 386
Supervised learning is performed based on input-output paired training samples {(xi,
yi)}n
i=1. However, gathering many input-output paired samples is often expensive in
practical applications. On the other hand, input-only samples {xi}n+n′
i=n+1 can be easily
collected abundantly. For example, in web page classification, class label yi (such as
“sport”, “computer”, and “politics”) should be manually given after carefully inves-
tigating web page xi, which requires a huge amount of human labor. On the other
hand, input-only web pages {xi}n+n′
i=n+1 can be automatically collected by crawlers.
In this chapter, methods of semisupervised learning [28, 101] are introduced,
which utilize input-only samples {xi}n+n′
i=n+1 in addition to input-output paired samples
{(xi, yi)}n
i=1.
33.1 MANIFOLD REGULARIZATION
In this section, a method of semisupervised learning based on manifold regularization
is introduced.
33.1.1 MANIFOLD STRUCTURE BROUGHT BY INPUT
SAMPLES
Supervised learning from input-output paired samples {(xi, yi)}n
i=1 can be regarded
as estimating the conditional density p(y|x). On the other hand, unsupervised
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00044-3
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
375

376
CHAPTER 33
SEMISUPERVISED LEARNING
FIGURE 33.1
Semisupervised classification. Samples in the
same cluster are assumed to belong to the same
class.
learning from input-only samples {xi}n+n′
i=n+1 can be regarded as estimating the
marginal density p(x). Thus, without any assumption, input-only samples {xi}n+n′
i=n+1
do not help improve the estimation of the conditional density p(y|x). For this reason,
semisupervised learning imposes a certain assumption between p(x) and p(y|x) and
utilizes the estimation of p(x) to improve the accuracy of estimating p(y|x).
Below, a semisupervised learning method based on a manifold assumption is
introduced. Mathematically, a manifold is a topological space that can be locally
approximated by Euclidean space. On the other hand, a manifold is just regarded
as a local region in the context of semisupervised learning. More specifically, the
manifold assumption means that input samples appear only on manifolds and output
values change smoothly on the manifolds. In the case of classification, this means that
samples in the same cluster belong to the same class (Fig. 33.1).
The Gaussian kernel model introduced in Section 21.2 actually utilizes this
manifold assumption (see Fig. 21.5):
fθ(x) =
n

j=1
θ jK(x, xj),
K(x, c) = exp

−∥x −c∥2
2h2

.
That is, by locating smooth Gaussian functions on input samples {xi}n
i=1, a smooth
function over the input manifold can be learned. In semisupervised learning, the
above model may be augmented to locate Gaussian kernels also on input-only
samples {xi}n+n′
i=n+1:
fθ(x) =
n+n′

j=1
θ jK(x, xj).
(33.1)
The parameters in model (33.1) are learned so that output at input samples,
{ fθ(xi)}n+n′
i=1 , is similar to each other. For example, in the case of ℓ2-regularized

33.1 MANIFOLD REGULARIZATION
377
LS, the optimization problem is given as
min
θ
1
2
n

i=1

fθ(xi) −yi
2 + λ
2 ∥θ∥2
+ ν
4
n+n′

i,i′=1
Wi,i′

fθ(xi) −fθ(xi′)
2
,
(33.2)
where the first and second terms correspond to ℓ2-regularized LS and the third term is
called the Laplacian regularizer, following the terminology in spectral graph theory
[29]. ν ≥0 is the regularization parameter for semisupervised learning that controls
the smoothness on the manifolds. Wi,i′ ≥0 denotes the similarity between xi and
xi′, which takes a large value if xi and xi′ are similar, and a small value if xi and xi′
are dissimilar. Popular choices of the similarity measure are described in Fig. 35.8.
Below the similarity matrix W is assumed to be symmetric (i.e. Wi,i′ = Wi′,i).
33.1.2 COMPUTING THE SOLUTION
Here, how to compute the solution of Laplacian-regularized LS is explained. Let D
be the diagonal matrix whose diagonal elements are given by the row-sums of matrix
W:
D = diag *.
,
n+n′

i=1
W1,i,. . . ,
n+n′

i=1
Wn+n′,i+/
-
,
and let L = D −W. Then the third term in Eq. (33.2) can be rewritten as
n+n′

i,i′=1
Wi,i′

fθ(xi) −fθ(xi′)
2
=
n+n′

i=1
Di,i fθ(xi)2 −2
n+n′

i,i′=1
Wi,i′ fθ(xi) fθ(xi′) +
n+n′

i′=1
Di′,i′ fθ(xi)2
= 2
n+n′

i,i′=1
Li,i′ fθ(xi) fθ(xi′).
Thus, Eq. (33.2) for kernel model (33.1) can be reduced to the generalized
ℓ2-regularized LS as follows:
min
θ
1
2
n

i=1
n+n′

j=1
θ jK(xi, xj) −yi
2
+ λ
2
n+n′

j=1
θ2
j
+ ν
2
n+n′

j, j′=1
θ jθ j′
n+n′

i,i′=1
Li,i′K(xi, xj)K(xi′, xj′)

.

378
CHAPTER 33
SEMISUPERVISED LEARNING
This can be compactly rewritten as
min
θ
1
2 ∥Kθ −y∥2 + λ
2 ∥θ∥2 + ν
2θ⊤KLKθ

,
where
K =
*.....
,
K(x1, x1)
· · ·
K(x1, xn+n′)
...
...
...
K(xn+n′, x1) · · · K(xn+n′, xn+n′)
+/////
-
,
θ = (θ1,. . . ,θn,θn+1,. . . ,θn+n′)⊤,
y = (y1,. . . , yn,0,. . . ,0
    
n′
)⊤.
Note that the vector y defined above contains n′ zeros since {yi}n+n′
i=n+1 are not
available. The solution θ can be obtained analytically as
θ = (K2 + λI + νKLK)−1K y.
A MATLAB code for Laplacian-regularized LS is provided in Fig. 33.2, and
its behavior is illustrated in Fig. 33.3. This is an extremely difficult classification
problem where only two labeled training samples are available. Laplacian-regularized
LS gives a decision boundary that separates two point clouds, which is appropriate
when the cluster assumption holds. On the other hand, ordinary LS gives a decision
boundary that goes through the middle of the two labeled training samples, which is
not reasonable under the cluster assumption.
In the above explanation, Laplacian regularization was applied to LS. However,
the idea of Laplacian regularization is generic and it can be combined with various
regression and classification techniques introduced in Part 4.
33.2 COVARIATE SHIFT ADAPTATION
The Laplace regularization method introduced above utilized the manifold structure
of input samples. In this section, a semisupervised learning method called covari-
ate shift adaptation is introduced [101], which explicitly takes into account the
probability distributions of {(xi, yi)}n
i=1 and {x′
i′}n′
i′=1. A covariate is another name
for an input variable, and covariate shift refers to the situation where {xi}n
i=1 and
{x′
i′}n′
i′=1 follow different probability distributions, but the conditional density p(y|x)
is unchanged.
33.2.1 IMPORTANCE WEIGHTED LEARNING
An example of covariate shift in regression is illustrated in Fig. 33.4, where
the learning target function f (x) is unchanged. However, {xi}n
i=1 are distributed

33.2 COVARIATE SHIFT ADAPTATION
379
n=200; a=linspace(0,pi,n/2);
u=-10*[cos(a)+0.5 cos(a)-0.5]’+randn(n,1);
v=10*[sin(a) -sin(a)]’+randn(n,1);
x=[u v]; y=zeros(n,1); y(1)=1; y(n)=-1;
x2=sum(x.^2,2); hh=2*1^2;
k=exp(-(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’)/hh); w=k;
t=(k^2+1*eye(n)+10*k*(diag(sum(w))-w)*k)\(k*y);
m=100; X=linspace(-20,20,m)’; X2=X.^2;
U=exp(-(repmat(u.^2,1,m)+repmat(X2’,n,1)-2*u*X’)/hh);
V=exp(-(repmat(v.^2,1,m)+repmat(X2’,n,1)-2*v*X’)/hh);
figure(1); clf; hold on; axis([-20 20 -20 20]);
colormap([1 0.7 1; 0.7 1 1]);
contourf(X,X,sign(V’*(U.*repmat(t,1,m))));
plot(x(y==1,1),x(y==1,2),’bo’);
plot(x(y==-1,1),x(y==-1,2),’rx’);
plot(x(y==0,1),x(y==0,2),’k.’);
FIGURE 33.2
MATLAB code for Laplacian-regularized LS.
around x = 1, while {x′
i′}n′
i′=1 are distributed around x = 2. Thus, this is a (weak)
extrapolation problem.
Fig. 33.6(a) illustrates the function obtained by fitting the straight line model,
fθ(x) = θ1 + θ2x,
(33.3)
to samples {(xi, yi)}n
i=1 by ordinary LS:
min
θ
1
2
n

i=1

fθ(xi) −yi
2.
This shows that, although the samples {(xi, yi)}n
i=1 are appropriately fitted by LS,
prediction of output values at {x′
i′}n′
i′=1 is poor.
In such a covariate shift situation, it intuitively seems that only using samples
{(xi, yi)}n
i=1 near {x′
i′}n′
i′=1 can provide good prediction of output values at {x′
i′}n′
i′=1.
This intuitive idea can be more formally realized by importance weighting. More
specifically, importance weighted LS is given by
min
θ
1
2
n

i=1
w(xi)

fθ(xi) −yi
2,

380
CHAPTER 33
SEMISUPERVISED LEARNING
(a) Laplacian-regularized LS
(b) Ordinary LS
FIGURE 33.3
Examples of Laplacian-regularized LS compared with ordinary LS. Dots denote unlabeled
training samples.
(a) Input densities and importance
(b) Learning target function and training samples
FIGURE 33.4
Covariate shift in regression. Input distributions change, but the input-output relation is un-
changed.
where w(x) is called the importance function which is defined as the ratio of
probability density p′(x) for {x′
i′}n′
i′=1 and probability density p(x) for {xi}n
i=1:
w(x) = p′(x)
p(x) .

33.2 COVARIATE SHIFT ADAPTATION
381
n=100; u=randn(n,1)/4+2; x=randn(n,1)/2+1;
w=2*exp(-8*(x-2).^2+2*(x-1).^2); %w=ones(n,1);
y=sin(pi*x)./(pi*x)+0.1*randn(n,1);
x(:,2)=1; t=(x’*(repmat(w,1,2).*x))\(x’*(w.*y));
X=linspace(-1,3,100); Y=sin(pi*X)./(pi*X);
u(:,2)=1; v=u*t;
figure(1); clf; hold on;
plot(x(:,1),y,’bo’); plot(X,Y,’r-’); plot(u(:,1),v,’kx’);
FIGURE 33.5
MATLAB code for importance weighted LS.
(a) Ordinary LS
(b) Importance weighted LS
FIGURE 33.6
Example of LS learning under covariate shift. The dashed lines denote learned functions.
This importance weighted learning is based on the idea of importance sampling
(see Section 19.2), which approximates the expectation with respect to p′(x) by
importance weighted average with respect to p(x):

g(x)p′(x)dx =

g(x) p′(x)
p(x) p(x)dx ≈1
n
n

i=1
g(xi)w(xi).
A MATLAB code of importance weighted LS for straight line model (33.3) is
provided in Fig. 33.5, and its behavior is illustrated in Fig. 33.6(b). This shows that
importance weighting contributes to improving the accuracy of predicting output
values at {x′
i′}n′
i′=1.

382
CHAPTER 33
SEMISUPERVISED LEARNING
In the above example, importance weighting was applied to LS. However, the idea
of importance weighting is generic and it can be combined with various regression
and classification techniques introduced in Part 4.
33.2.2 RELATIVE IMPORTANCE WEIGHTED LEARNING
As illustrated in Fig. 33.6(b), prediction performance under covariate shift can be
improved by importance weighted learning.
Fig. 33.4(a) illustrates the importance function w(x), which is monotone increas-
ing as x grows. This means that, among all training samples {(xi, yi)}n
i=1, only a few
of them with large x have large importance values and the importance values of others
are negligibly small. Thus, importance weighted learning is actually performed only
from a few training samples, which can be unstable in practice.
This instability is caused by the fact that the importance function w(x) takes large
values for some x. Thus, the instability problem is expected to be mitigated if a
smooth variant of the importance function, such as the relative importance [121]
defined as
wβ(x) =
p′(x)
βp′(x) + (1 −β)p(x),
is employed, where β ∈[0,1] controls the smoothness. The relative importance
function wβ(x) is reduced to the ordinary importance function p′(x)/p(x) if β = 0. It
gets smoother if β is increased, and it yields the uniform weight wβ(x) = 1 if β = 1
(Fig. 33.7). Since the importance is always non-negative, the relative importance is
no larger than 1/β:
wβ(x) =
1
β + (1 −β) p(x)
p′(x)
≤1
β .
33.2.3 IMPORTANCE WEIGHTED CROSS VALIDATION
The performance of relative importance weighted learning depends on the choice
of smoothness β. Also, choice of model fθ(x) and regularization parameters
significantly affects the final performance.
In Section 23.3, a model selection method based on cross validation was intro-
duced, which estimates the prediction error for test samples. Actually, the validity
of cross validation is ensured only when {xi}n
i=1 and {x′
i′}n′
i′=1 follow the same
probability distributions. Thus, performing cross validation under covariate shift
can produce undesired solutions. Under covariate shift, a variant called importance
weighted cross validation [102] is useful. The algorithm of importance weighted
cross validation is summarized in Fig. 33.8, which is essentially the same as ordinary
cross validation, but the validation error is computed with importance weights.

33.2 COVARIATE SHIFT ADAPTATION
383
(a) Probability densities
(b) Relative importance
FIGURE 33.7
Relative importance when p′(x) is the Gaussian density with expectation 0 and variance 1 and
p(x) is the Gaussian density with expectation 0.5 and variance 1.
33.2.4 IMPORTANCE ESTIMATION
In (relative) importance weighted learning and cross validation, importance weights
are needed. However, the importance function is usually unknown and only samples
{xi}n
i=1 and {x′
i′}n′
i′=1 are available. A naive way to estimate the importance function
w(x) = p′(x)/p(x) is to separately estimate density p′(x) from {x′
i′}n′
i′=1 and p(x)
from {xi}n
i=1 and then compute the ratio of estimated densities. However, such a
two-step approach is unreliable because division by an estimated density can magnify
the estimation error significantly. Here, a direct importance estimator without going
through density estimation is introduced.
Let us model the relative importance function wβ(x) by a linear-in-parameter
model:
wα(x) =
b

j=1
αjψj(x) = α⊤ψ(x),
where α = (α1,. . . ,αb)⊤is the parameter vector and ψ(x) = (ψ1(x),. . . ,ψb(x))⊤is
the vector of basis functions. The parameter α is learned so that the following J(α)
is minimized:
J(α) = 1
2

wα(x) −wβ(x)
2
βp′(x) + (1 −β)p(x)

dx
= 1
2

α⊤ψ(x)ψ(x)⊤α

βp′(x) + (1 −β)p(x)

dx
−

α⊤ψ(x)p′(x)dx + C,

384
CHAPTER 33
SEMISUPERVISED LEARNING
1. Prepare candidates of models: {Mj}j.
2. Split training samples D = {(xi, yi)}n
i=1 into t disjoint subsets of
(approximately) the same size: {Dℓ}t
ℓ=1.
3. For each model candidate Mj
(a) For each split ℓ= 1,. . . ,t
i.
Obtain learned function f (ℓ)
j (x) using model Mj from all
training samples without Dℓ.
ii. Compute the average prediction error G(ℓ)
j
of f (ℓ)
j (x) for
holdout samples Dℓ.
•
Regression (squared loss):
G(ℓ)
j
=
1
|Dℓ|

(x,y)∈Dℓ
w(x)

y −f (ℓ)
j (x)
2,
where |Dℓ| denotes the number of elements in the set Dℓ.
•
Classification (0/1-loss):
G(ℓ)
j
=
1
|Dℓ|

(x,y)∈Dℓ
w(x)
2

1 −sign
f (ℓ)
j (x)y

.
(b) Compute the average prediction error Gj over all t splits:
Gj = 1
t
t
ℓ=1
G(ℓ)
j .
4. Choose the model Mj that minimizes the average prediction error:
j = argmin
j
Gj.
5. Obtain the final function approximator using chosen model Mj from
all training samples {(xi, yi)}n
i=1.
FIGURE 33.8
Algorithm of importance weighted cross validation.
where C
=
1
2

wβ(x)p′(x)dx is independent of α and thus can be ignored.
Approximating the expectations by sample averages and adding the ℓ2-regularizer
yield the following optimization problem:

33.3 CLASS-BALANCE CHANGE ADAPTATION
385
min
α
1
2 α⊤Gβα −α⊤h + λ
2 ∥α∥2

,
where
Gβ = β
n′
n′

i′=1
ψ(x′
i′)ψ(x′
i′)⊤+ 1 −β
n
n

i=1
ψ(xi)ψ(xi)⊤,
h = 1
n′
n′

i′=1
ψ(x′
i′).
The minimizer α can be obtained analytically as
α =
G + λI
−1 h.
This method is called LS relative density ratio estimation [121]. The regularization
parameter λ and parameters included in basis functions ψ can be optimized by cross
validation with respect to the squared error J.
A MATLAB code of LS relative density ratio estimation for the Gaussian kernel
model,
wα(x) =
n

j=1
αj exp *
,
−∥x −xj∥2
2h2
+
-
,
is provided in Fig. 33.9, and its behavior is illustrated in Fig. 33.10. This shows that
the true relative importance function is nicely estimated.
33.3 CLASS-BALANCE CHANGE ADAPTATION
In the previous section, semisupervised learning methods for covariate shift adap-
tation were introduced, which can be naturally applied in regression. On the other
hand, in classification, class-balance change is a natural situation, where the class-
prior probabilities differ in {(xi, yi)}n
i=1 and {x′
i′}n′
i′=1 (see Fig. 33.11), but the
class-conditional probability p(x|y) remains unchanged. In this section, adaptation
methods for class-balance change are introduced.
33.3.1 CLASS-BALANCE WEIGHTED LEARNING
The bias caused by class-balance change can be canceled by class-balance weighted
learning.
More specifically, the class-prior ratio p′(y)/p(y) is used as a weighting factor,
where p(y) and p′(y) are the class-prior probabilities for {xi}n
i=1 and {x′
i′}n′
i′=1,
respectively. For example, in the case of LS, the learning criterion is given by
min
θ
1
2
n

i=1
p′(yi)
p(yi)

fθ(xi) −yi
2,

386
CHAPTER 33
SEMISUPERVISED LEARNING
n=300; x=randn(n,1); y=randn(n,1)+0.5;
hhs=2*[1 5 10].^2; ls=10.^[-3 -2 -1]; m=5; b=0.5;
x2=x.^2; xx=repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’;
y2=y.^2; yx=repmat(y2,1,n)+repmat(x2’,n,1)-2*y*x’;
u=mod(randperm(n),m)+1; v=mod(randperm(n),m)+1;
for hk=1:length(hhs)
hh=hhs(hk); k=exp(-xx/hh); r=exp(-yx/hh);
for i=1:m
ki=k(u~=i,:); ri=r(v~=i,:); h=mean(ki)’;
kc=k(u==i,:); rj=r(v==i,:);
G=b*ki’*ki/sum(u~=i)+(1-b)*ri’*ri/sum(v~=i);
for lk=1:length(ls)
l=ls(lk); a=(G+l*eye(n))\h; kca=kc*a;
g(hk,lk,i)=b*mean(kca.^2)+(1-b)*mean((rj*a).^2);
g(hk,lk,i)=g(hk,lk,i)/2-mean(kca);
end, end, end
[gl,ggl]=min(mean(g,3),[],2); [ghl,gghl]=min(gl);
L=ls(ggl(gghl)); HH=hhs(gghl);
k=exp(-xx/HH); r=exp(-yx/HH);
s=r*((b*k’*k/n+(1-b)*r’*r/n+L*eye(n))\(mean(k)’));
figure(1); clf; hold on; plot(y,s,’rx’);
FIGURE 33.9
MATLAB code for LS relative density ratio estimation for Gaussian kernel model.
which is called class-balance weighted LS. Beyond LS, this class-balance change
adaptation technique can be applied to various classification methods introduced in
Part 4.
Model selection of class-balance weighted learning may be carried out by class-
balance weighted cross validation, which is essentially the same as importance
weighted cross validation introduced in Section 33.2.3, but p′(yi)/p(yi) is used as
a weight.
33.3.2 CLASS-BALANCE ESTIMATION
To use class-balance weighted learning, the class-prior probabilities p(y) and p′(y)
are needed, which are often unknown in practice. For labeled samples {(xi, yi)}n
i=1,
estimating the class prior p(y) is straightforward by ny/n, where ny denotes the
number of samples in class y. However, p′(y) cannot be estimated naively because of

33.3 CLASS-BALANCE CHANGE ADAPTATION
387
(a) Input samples
(b) Relative importance (β = 0.5)
FIGURE 33.10
Example of LS relative density ratio estimation. ×’s in the right plot show estimated relative
importance values at {xi}n
i=1.
(a) Labeled samples {(xi, yi)}n
i=1
(b) Unlabeled samples {x′
i′}n′
i′=1
FIGURE 33.11
Class-balance change, which affects the decision boundary.
lack of output values {y′
i′}n′
i′=1 for {x′
i′}n′
i′=1. Below, a practical estimator of p′(y) is
introduced [38].
The basic idea of estimating p′(y) is to fit a mixture qπ(x) of classwise input
densities p(x|y) to p′(x) (Fig. 33.12):
qπ(x) =
c

y=1
πyp(x|y),
where c denotes the number of classes and the coefficient πy corresponds to p′(y).

388
CHAPTER 33
SEMISUPERVISED LEARNING
FIGURE 33.12
Class-prior estimation by distribution matching.
Matching of qπ to p′ can be performed, for example, under the KL divergence
(see Section 14.2),
KL(p′∥qπ) =

p′(x) log p′(x)
qπ(x)dx,
or the L2-distance:
L2(p′,qπ) =

p′(x) −qπ(x)
2dx.
The KL divergence can be accurately estimated by KL density ratio estimation
explained in Section 38.3, while the L2-distance can be accurately estimated by LS
density difference estimation introduced in Section 39.1.3.
Another useful distance measure is the energy distance [106], which is the
weighted squared distance between characteristic functions:
DE(p′,qπ) =

Rd ∥ϕp′(t) −ϕqπ(t)∥2 *.
,
π
d+1
2
Γ
d+1
2
∥t∥d+1+/
-
−1
dt,
where ∥· ∥denotes the Euclidean norm, ϕp denotes the characteristic function (see
Section 2.4.3) of p, Γ(·) is the gamma function (see Section 4.3), and d denotes the
dimensionality of x. Thanks to the careful design of the weight function, the energy
distance can be equivalently expressed as
DE(p′,qπ) = 2Ex′∼p′,x∼qπ ∥x′ −x∥−Ex′,x′∼p′∥x′ −x′∥−Ex,x∼qπ ∥x −x∥
= 2π⊤b −π⊤Aπ + C,
where Ex′∼p′ denotes the expectation with respect to x′ following density p′ and
C is a constant independent of π. A is the c × c symmetric matrix and b is the
c-dimensional vector defined as
Ay,y = Ex∼p(x|y),x∼p(x|y)∥x −x∥,

33.3 CLASS-BALANCE CHANGE ADAPTATION
389
x=[[randn(90,1)-2; randn(10,1)+2] 2*randn(100,1)];
x(:,3)=1; y=[ones(90,1); 2*ones(10,1)]; n=length(y);
X=[[randn(10,1)-2; randn(90,1)+2] 2*randn(100,1)];
X(:,3)=1; Y=[ones(10,1); 2*ones(90,1)]; N=length(Y);
x2=sum(x.^2,2); X2=sum(X.^2,2);
xx=sqrt(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’);
xX=sqrt(repmat(x2,1,N)+repmat(X2’,n,1)-2*x*X’);
for i=1:2
s(i)=sum(y==i)/n; b(i)=mean(mean(xX(y==i,:)));
for j=1:2
A(i,j)=mean(mean(xx(y==i,y==j)));
end, end
v=(A(1,2)-A(2,2)-b(1)+b(2))/(2*A(1,2)-A(1,1)-A(2,2));
v=min(1,max(0,v)); v(2)=1-v; w=v(y)./s(y); z=2*y-3;
u=x\z; t=(x’*(repmat(w’,1,size(x,2)).*x))\(x’*(w’.*z));
figure(1); clf; hold on
plot([-5 5],-(u(3)+[-5 5]*u(1))/u(2),’k--’);
plot([-5 5],-(t(3)+[-5 5]*t(1))/t(2),’g-’);
plot(X(Y==1,1),X(Y==1,2),’bo’);
plot(X(Y==2,1),X(Y==2,2),’rx’);
legend(’Unweighted’,’Weighted’); axis([-5 5 -10 10])
FIGURE 33.13
MATLAB code for class-balance weighted LS.
by = Ex′∼p′,x∼p(x|y)∥x′ −x∥.
Although DE(p′,qπ) is a concave function with respect to π = (π1,. . . ,πc)⊤, it
is a convex function with respect to π1,. . . ,πc−1 for πc = 1 −c−1
y=1 πy and thus its
minimizer can be easily obtained [61]. For example, when c = 2 with π1 = π and
π2 = 1 −π, DE(p′,qπ) can be expressed as a function of π up to a constant as
J(π) = aπ2 −2bπ,
where
a = 2A1,2 −A1,1 −A2,2 = DE(p(x|y = 1),p(x|y = 2)) ≥0,
b = A1,2 −A2,2 −b1 + b2.
Since J(π) is convex with respect to π, its minimizer is given analytically as
min(1,max(0,b/a)). Note that Ay,y and by can be empirically approximated as
Ay,y =
1
nyny

i:yi=y

i:yi=y
∥xi −xi∥,

390
CHAPTER 33
SEMISUPERVISED LEARNING
(a) {xi}n
i=1 with (n1, n2) = (90, 10)
(b) {x′
i′}n′
i′=1 with (n′
1, n′
2) = (10, 90)
FIGURE 33.14
Example of class-balance weighted LS. The test class priors are estimated as p′(y = 1)
= 0.18 and p′(y = 2) = 0.82, which are used as weights in class-balance weighted LS.
by =
1
n′ny
n′

i′=1

i:yi=y
∥x′
i′ −xi∥.
A MATLAB code for class-balance weighted LS is provided in Fig. 33.13, and its
behavior is illustrated in Fig. 33.14. This shows that the class prior can be estimated
reasonably well and class-balance weighted learning contributes to improving the
classification accuracy for test input points {x′
i′}n′
i′=1.

CHAPTER
MULTITASK LEARNING34
CHAPTER CONTENTS
Task Similarity Regularization .................................................... 391
Formulation ................................................................ 391
Analytic Solution ........................................................... 392
Efficient Computation for Many Tasks...................................... 393
Multidimensional Function Learning ............................................. 394
Formulation ................................................................ 394
Efficient Analytic Solution ................................................. 397
Matrix Regularization ............................................................ 397
Parameter Matrix Regularization ........................................... 397
Proximal Gradient for Trace Norm Regularization .......................... 400
When solving multiple related learning tasks, solving them together simultaneously
by sharing information is expected to give a better solution than solving them
independently. This is the basic idea of multitask learning [24]. In this chapter,
practically useful multitask learning methods are introduced.
Let us consider T tasks indexed by t = 1,. . . ,T and assume that each input-output
paired training sample (xi, yi) is accompanied with task index ti:
{(xi, yi,ti) | ti ∈{1,. . . ,T}}n
i=1.
34.1 TASK SIMILARITY REGULARIZATION
In this section, a multitask learning method with task similarity regularization [40] is
introduced.
34.1.1 FORMULATION
For the tth learning task, let us employ a linear-in-parameter model,
b

j=1
θt, jϕj(x) = θ⊤
t φ(x),
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00045-5
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
391

392
CHAPTER 34
MULTITASK LEARNING
where θt = (θt,1,. . . ,θt,b)⊤is the parameter vector for the tth task and the basis
functions φ(x) are common to all tasks.
The idea of task similarity regularization is to impose the parameters θ1,. . . ,θT
to take similar values and learn all parameters,
θ = (θ⊤
1 ,. . . ,θ⊤
T )⊤∈RbT,
simultaneously. Let us employ the ℓ2-regularized LS (see Section 23.2) and learn θ
so that the following J(θ) is minimized:
J(θ) = 1
2
n

i=1

yi −φ(xi)⊤θti
2 + 1
2
T

t=1
λt ∥θt ∥2 + 1
4
T

t,t′=1
γt,t′∥θt −θt′∥2,
where λt ≥0 is the ℓ2-regularization parameter for the tth task, and γt,t′ ≥0 is the
similarity between the tth task and the t′th task. If γt,t′ = 0 for all t,t′ = 1,. . . ,T, the
third term in J(θ) disappears. Then there is no interaction between tasks and thus this
corresponds to merely learning T tasks separately. On the other hand, if γt,t′ > 0, θt
and θt′ are imposed to be close to each other and thus information can be implicitly
shared between the tth task and the t′th task. If all γt,t′ are large enough, all solutions
are imposed to be the same and thus a single solution that is common to all tasks is
obtained from all training samples {(xi, yi)}n
i=1.
34.1.2 ANALYTIC SOLUTION
J(θ) can be compactly expressed as
J(θ) = 1
2 ∥y −Ψθ∥2 + 1
2θ⊤(C ⊗Ib)θ,
(34.1)
where
y = (y1,. . . , yn)⊤∈Rn,
Ψ = (ψt1(x1),. . . ,ψtn(xn))⊤∈Rn×bT,
ψt(x) =

0⊤
b(t−1),φ(x)⊤,0⊤
b(T−t)
⊤∈RbT .
C is the T × T matrix defined as
Ct,t′ =

λt + T
t′′=1 γt,t′′ −γt,t
(t = t′),
−γt,t′
(t , t′),
and ⊗denotes the Kronecker product, i.e. for E ∈Rm×n and F ∈Rp×q,
E ⊗F =
*.....
,
E1,1F · · · E1,nF
...
...
...
Em,1F · · · Em,nF
+/////
-
∈Rmp×nq.

34.1 TASK SIMILARITY REGULARIZATION
393
Then the minimizer θ of J(θ) satisfies
(Ψ⊤Ψ + C ⊗Ib)θ = Ψ⊤y,
(34.2)
and θ can be obtained analytically as
θ =  Ψ⊤Ψ + C ⊗Ib
−1 Ψ⊤y.
(34.3)
34.1.3 EFFICIENT COMPUTATION FOR MANY TASKS
The size of matrix (Ψ⊤Ψ + C ⊗Ib) is bT × bT, and thus directly computing the
solution by the above analytic form is not tractable if the number of tasks, T, is large.
However, since the rank of matrix Ψ⊤Ψ is at most n, the solution can be computed
efficiently if n < bT.
More specifically, the solution θ⊤
t φ(x) for the tth task can be expressed as
θ⊤
t φ(x) = θ⊤ψt(x) = y⊤A−1bt.
(34.4)
Here, A and bt are the n × n matrix and n-dimensional vector defined as
Ai,i′ = [Ψ(C−1 ⊗Ib)Ψ⊤+ In]i,i′
= [C−1]ti,ti′φ(xi)⊤φ(xi′) +

1
(i = i′),
0
(i , i′),
bt,i = [Ψ(C−1 ⊗Ib)ψt(x)]i
= [C−1]t,tiφ(xi)⊤φ(x),
where [C−1]t,t′ denotes the (t,t′)th element of matrix C−1. Since the size of matrix
A and the dimensionality of vector bt are independent of the number of tasks, T,
Eq. (34.4) would be computationally more efficient than Eq. (34.3) if n < bT.
Moreover, since A−1 is independent of task index t, it needs to be computed only once
across all tasks. Note that the following formulas regarding the Kronecker product
were utilized in the derivation of Eq. (34.4):
Ψ  Ψ⊤Ψ + C ⊗Ib
−1 = (Ψ(C ⊗Ib)−1Ψ⊤+ In)−1Ψ(C ⊗Ib)−1,
(C ⊗Ib)−1 = C−1 ⊗Ib.
A MATLAB code for multitask LS is provided in Fig. 34.1, and its behavior is
illustrated in Fig. 34.2. This shows that multitask classification outperforms single-
task classification.
In the above multitask method, the task similarity γt,t′ was assumed to be known.
When γt,t′ is unknown, it may be alternately learned as described in Fig. 34.3.

394
CHAPTER 34
MULTITASK LEARNING
n=2; T=6; y=[ones(n/2,T); -ones(n/2,T)];
x=[randn(2,n,T); ones(1,n,T)]; r(1,1,:)=pi*[1:T]/T/10;
c=repmat(cos(r),[1 n/2]); x(1,:,:)=x(1,:,:)+[c -c];
s=repmat(sin(r),[1 n/2]); x(2,:,:)=x(2,:,:)+[s -s];
Ci=inv(-ones(T,T)+diag(T*ones(T,1)+0.01));
a=repmat([1:T],[n 1]); a=a(:); m=20; X=linspace(-4,4,m);
b=repmat(X,[m 1]); bt=b’; XX=[b(:)’; bt(:)’; ones(1,m^2)];
yAi=y(:)’*inv(Ci(a,a).*(x(:,:)’*x(:,:))+eye(n*T));
figure(1); clf; colormap([1 0.7 1; 0.7 1 1]);
for k=1:T
%
Y=yAi*(repmat(Ci(a,k),[1 3]).*x(:,:)’)*XX;
q=x(:,:,k); Y=((q*q’+0.01*eye(3))\(q*y(:,k)))’*XX;
subplot(2,3,k); hold on; contourf(X,X,reshape(Y,m,m));
plot(x(1,y(:,k)==1,k),x(2,y(:,k)==1,k),’bo’);
plot(x(1,y(:,k)==-1,k),x(2,y(:,k)==-1,k),’rx’);
plot(99*sin(r(k))*[1 -1],99*cos(r(k))*[-1 1],’k--’);
axis([-4 4 -4 4]);
end
FIGURE 34.1
MATLAB code for multitask LS.
34.2 MULTIDIMENSIONAL FUNCTION LEARNING
In this section, the problem of multidimensional function learning is discussed, which
can be regarded as a special case of multitask learning.
34.2.1 FORMULATION
Let us consider the problem of learning a T-dimensional function,
f (x) = ( f1(x),. . . , fT(x))⊤,
from input-output paired training samples:
{(xi, yi) | xi ∈Rd, yi = (y(1)
i ,. . . , y(T)
i
)⊤∈RT }n
i=1.
If each dimension of output y is regarded as a task, the problem of multidimen-
sional function learning can be regarded as multitask learning. The difference is that
multidimensional function learning shares input points across all tasks, while input
points are generally different in multitask learning (Fig. 34.4). Thus, the number of
training samples in multidimensional function learning is actually nT in the context
of multitask learning.

34.2 MULTIDIMENSIONAL FUNCTION LEARNING
395
(a) Single-task classification
(b) Multitask classification
FIGURE 34.2
Examples of multitask LS. The dashed lines denote true decision boundaries and the contour lines
denote learned results.
Multidimensional function learning for classification is specifically called mul-
tilabel classification, which can be regarded as a generalization of multiclass

396
CHAPTER 34
MULTITASK LEARNING
1. Initialize task similarity, e.g. γt,t′ = γ > 0, ∀t,t′.
2. Learn parameter θ based on the current task similarity γt,t′.
3. Update task similarity γt,t′ based on the similarity between θt and θt′,
e.g.
γt,t′ = η exp(−κ∥θt −θt′∥2),
where η ≥0 and κ ≥0 are tuning parameters and may be determined by
cross validation.
4. Iterate 2–3 until convergence.
FIGURE 34.3
Alternate learning of task similarity γt,t′ and solution θ.
(a) Multitask regression
(b) Multidimensional regression
FIGURE 34.4
Multidimensional function learning.
classification (see Section 26.3). A pattern x belongs to one of the classes exclusively
in multiclass classification, while x can belong to multiple classes simultaneously in
multilabel classification. For example, an image can belong to the classes of “human,”
“dog,” and “building” at the same time in image recognition, and a sound clip can
belong to the classes of “conversation,” “noise,” and “background music” at the

34.3 MATRIX REGULARIZATION
397
same time in audio recognition. In multilabel classification, co-occurrence of multiple
classes can be utilized by multitask learning. For example, an image containing a dog
often contains a human at the same time.
However, in multidimensional function learning, the number of parameters is bT
and the actual number of samples is nT, both of which are proportional to the number
of tasks, T. Thus, both the solutions explained in Section 34.1.2 and Section 34.1.3
are not computationally efficient in multidimensional function learning.
34.2.2 EFFICIENT ANALYTIC SOLUTION
Let us arrange the parameter vectors θ1,. . . ,θT, the output vectors y1,. . . , yn, and
the basis vectors φ(x1),. . . ,φ(xn) into matrices as
Θ = (θ1,. . . ,θT) ∈Rb×T,
Y = (y1,. . . , yn)⊤∈Rn×T,
Φ = (φ(x1),. . . ,φ(xn))⊤∈Rn×b.
Then, Eq. (34.2), which multitask solutions should satisfy, can be rewritten as
Φ⊤ΦΘ + ΘC = Φ⊤Y.
This form is known as the continuous Sylvester equation, which often arises in control
theory [94], and can be solved efficiently, as explained in Fig. 34.5.
A MATLAB code for multidimensional regression is provided in Fig. 34.6, and its
behavior is illustrated in Fig. 34.7. This shows that multitask regression outperforms
single-task regression.
34.3 MATRIX REGULARIZATION
The multitask learning method introduced in Section 34.1 explicitly used task
similarity γt,t′ to control the amount of information sharing. In this section, another
multitask learning approach is introduced, which does not involve task similarity.
34.3.1 PARAMETER MATRIX REGULARIZATION
The basic idea is to share information across multiple tasks by regularizing the
parameter matrix:
Θ = (θ1,. . . ,θT) ∈Rb×T .
More specifically, for a squared loss function, the multitask learning criterion to be
minimized with respect to Θ is given by
1
2
n

i=1

yi −φ(xi)⊤θti
2 + λR(Θ),
where R(Θ) denotes some regularization functional for Θ.

398
CHAPTER 34
MULTITASK LEARNING
The continuous Sylvester equation for some matrices A ∈Rb×b, B ∈RT×T,
and Z ∈Rb×T with respect to Θ ∈Rb×T is given by
AΘ + ΘB = Z.
The use of the Kronecker product and the vectorization operator (see
Fig. 6.5) allows us to rewrite the above equation as
(IT ⊗A + B ⊗Ib)vec(Θ) = vec(Z),
where IT denotes the T × T identity matrix. This shows that the continuous
Sylvester equation with respect to a b × T matrix can be seen as a linear
equation with respect to a bT-dimensional vector, which is computationally
expensive to solve naively. However, the continuous Sylvester equation can
be solved more efficiently. For example, let u1,. . . ,ub and u1,. . . , ub be
eigenvalues and eigenvectors of A (see Fig. 6.2), and let v1,. . . ,vT and
v1,. . . ,vT be eigenvalues and eigenvectors of B, respectively. Then, when
uj + vt , 0 for all j = 1,. . . ,b and t = 1,. . . ,T, the solution Θ of the above
continuous Sylvester equation with respect to Θ can be obtained analytically
as
Θ = (u1,. . . , ub)Q(v1,. . . ,vT)⊤,
where Q is the b × T matrix defined as
Qj,t =
u⊤
j Zvt
uj + vt
.
FIGURE 34.5
Continuous Sylvester equation.
If the squared Frobenius norm ∥Θ∥2
Frob is used for regularization, no information
is shared across different tasks since this is equivalent to the sum of ℓ2-norms of
parameter vectors θt:
∥Θ∥2
Frob =
T

t=1
b

j=1
Θ2
t, j =
T

t=1
∥θt ∥2.
Thus, a more intricate norm should be used for matrix regularization.
For example, the trace norm (see Fig. 24.10) tends to produce a low-rank
solution:

34.3 MATRIX REGULARIZATION
399
n=30; x=linspace(-3,3,n)’; pix=pi*x; T=3;
y=repmat(sin(pix)./(pix)+0.1*x,1,T)+0.1*repmat([1:T],n,1);
y=y+0.5*randn(n,T); N=1000; X=linspace(-3,3,N)’; piX=pi*X;
Y=repmat(sin(piX)./(piX)+0.1*X,1,T)+0.1*repmat([1:T],N,1);
G=10*ones(T,T); %G=zeros(T,T);
l=0.1; C=l*eye(T)+diag(sum(G))-G; hh=1; x2=x.^2;
k=exp(-(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’)/hh);
K=exp(-(repmat(X.^2,1,n)+repmat(x2’,N,1)-2*X*x’)/hh);
[U,u]=eig(k’*k); [V,v]=eig(C);
Q=U’*k’*y*V./(repmat(diag(u),1,T)+repmat(diag(v)’,n,1));
S=U*Q*V’; F=K*S;
figure(1); clf; hold on; axis([-inf inf -inf inf])
plot(X,Y,’r-’); plot(X,F,’g--’); plot(x,y,’bo’);
FIGURE 34.6
MATLAB code for multidimensional regression.
(a) Single-task regression
(b) Multitask regression
FIGURE 34.7
Examples of multidimensional regression.
∥Θ∥tr =
min(b,T)

k=1
σk,
where σk is a singular value of Θ (see Fig. 22.2). Since a low-rank solution
in multitask learning confines parameters of each task in a common subspace,
information can be shared across different tasks [6]. In Section 23.1, the method

400
CHAPTER 34
MULTITASK LEARNING
of subspace-constrained LS was introduced, which confines the LS solution in a
given subspace. Multitask learning based on the trace norm can be regarded as
automatically learning the subspace with the help of other learning tasks.
Another possibility of matrix regularization for multitask learning is to use the
ℓ2,1-norm ∥Θ∥2,1:
∥Θ∥2,1 =
T

t=1


b

j=1
Θ2
t, j,
which tends to give a row-wise sparse solution (see Section 24.4.4). This means that
θ1, j,. . . ,θT, j tend to simultaneously vanish for several j ∈{1,. . . ,b}, and thus
feature selection can be performed across different tasks, if linear basis function
φ(x) = x is used [8] (see also Section 24.3).
Below, a practical optimization method for multitask LS with trace norm regular-
ization is introduced.
34.3.2 PROXIMAL GRADIENT FOR TRACE NORM
REGULARIZATION
Since the trace norm ∥Θ∥tr is a convex function, the global optimal solution to
multitask LS with trace norm regularization can be obtained, for example, by the
proximal gradient method described in Fig. 34.8.
More specifically, the proximal gradient method updates the parameter matrix Θ
from some initial value as
Θ ←−prox

Θ −ε∇L(Θ)

,
(34.5)
where ε > 0 is the step size. In Eq. (34.5), L(Θ) is the loss function for all training
samples:
L(Θ) = 1
2
n

i=1

yi −φ(xi)⊤Θeti
2,
where et denotes the T-dimensional vector with all zeros but the tth element being
one, i.e. Θet = θt. ∇L(Θ) is the gradient of L given by
∇L(Θ) =
n

i=1

φ(xi)⊤Θeti −yi

φ(xi)e⊤
ti,
where the following matrix derivative formula is used:
∂φ(xi)⊤Θeti
∂Θ
= φ(xi)e⊤
ti .
The proximal operator prox(Θ) in Eq. (34.5) is given by
prox(Θ) = argmin
U

∥U∥tr +
1
2ελ ∥U −Θ∥2
Frob

,

34.3 MATRIX REGULARIZATION
401
Let us consider the following optimization problem:
min
θ
L(θ) + R(θ),
where L(θ) is a convex differentiable function and R(θ) is a closed convex
function. The proximal gradient method finds the minimizer of L(θ) + R(θ)
by updating θ from some initial value as
θ ←−argmin
u

R(u) + L(θ) + ∇L(θ)⊤(u −θ) + 1
2ε ∥u −θ∥2

,
where ∇L(θ) is the gradient of L with respect to θ and ε > 0 corresponds to
the step size. If the above minimization with respect to u can be solved ef-
ficiently (e.g. analytically), the proximal gradient method is computationally
efficient. The above update rule can be equivalently expressed as
θ ←−proxεR

θ −ε∇L(θ)

,
where proxR(θ) is called the proximal operator defined as
proxR(θ) = argmin
u

R(u) + 1
2 ∥u −θ∥2

.
This implies that the proximal gradient method can be regarded as a
generalization of the projected gradient method. Indeed, when R(θ) is the
indicator function for some set S,
R(θ) =

0
(θ ∈S),
∞
(θ < S),
the proximal operator is reduced to an ordinary projection operator and thus
the proximal gradient method is a projected gradient method.
FIGURE 34.8
Proximal gradient method.
where ∥Θ∥Frob denotes the Frobenius norm:
∥Θ∥Frob =


b1,b2

k1,k2=1
Θ2
k1,k2 =

tr  ΘΘ⊤.

402
CHAPTER 34
MULTITASK LEARNING
n=2; T=6; y=[ones(n/2,T); -ones(n/2,T)];
x=[randn(2,n,T); ones(1,n,T)]; r(1,1,:)=pi*[1:T]/T/10;
c=repmat(cos(r),[1 n/2]); x(1,:,:)=x(1,:,:)+[c -c];
s=repmat(sin(r),[1 n/2]); x(2,:,:)=x(2,:,:)+[s -s];
t0=randn(3,T); e=0.1; l=4;
for o=1:1000
for k=1:T
gt(:,k)=x(:,:,k)*(x(:,:,k)’*t0(:,k)-y(:,k));
end
[U,S,V]=svd(t0-e*gt,’econ’);
S=diag(max(0,diag(S)-e*l)); t=U*S*V’;
if norm(t-t0)<0.001, break, end
t0=t;
end
figure(1); clf; colormap([1 0.7 1; 0.7 1 1]);
m=20; X=linspace(-4,4,m); b=repmat(X,[m 1]); bt=b’;
for k=1:T
Y=t(:,k)’*[b(:)’; bt(:)’; ones(1,m^2)];
subplot(2,3,k); hold on; contourf(X,X,reshape(Y,m,m));
plot(x(1,y(:,k)==1,k),x(2,y(:,k)==1,k),’bo’);
plot(x(1,y(:,k)==-1,k),x(2,y(:,k)==-1,k),’rx’);
plot(99*sin(r(k))*[1 -1],99*cos(r(k))*[-1 1],’k--’);
axis([-4 4 -4 4]);
end
FIGURE 34.9
MATLAB code for multitask learning with trace norm regularization.
A notable fact is that the above proximal operator can be analytically expressed as
follows [78]:
prox(Θ) =
min(b1,b2)

k=1
max(0,σk −ελ)ψkφ⊤
k ,
where ψk, φk, and σk are a left singular vector, a right singular vector, and a singular
value of Θ, respectively:
Θ =
min(b1,b2)

k=1
σkψkφ⊤
k .

34.3 MATRIX REGULARIZATION
403
(a) Single-task classification
(b) Multitask classification
FIGURE 34.10
Examples of multitask LS with trace norm regularization. The data set is the same as Fig. 34.2.
The dashed lines denote true decision boundaries and the contour lines denote learned results.

404
CHAPTER 34
MULTITASK LEARNING
This means that singular values less than ελ are rounded off to zero and other singular
values are reduced by ελ. This operation is called soft thresholding, which can be
computed very efficiently.
A MATLAB code for multitask LS with trace norm regularization is provided in
Fig. 34.9, and its behavior using the same data set as in Fig. 34.2 is illustrated in
Fig. 34.10. This shows that multitask classification with trace norm regularization
works reasonably well.

CHAPTER
LINEAR
DIMENSIONALITY
REDUCTION
35
CHAPTER CONTENTS
Curse of Dimensionality .......................................................... 405
Unsupervised Dimensionality Reduction.......................................... 407
PCA ........................................................................ 407
Locality Preserving Projection .............................................. 410
Linear Discriminant Analyses for Classification ................................... 412
Fisher Discriminant Analysis ............................................... 413
Local Fisher Discriminant Analysis......................................... 414
Semisupervised Local Fisher Discriminant Analysis ....................... 417
Sufficient Dimensionality Reduction for Regression .............................. 419
Information Theoretic Formulation ......................................... 419
Direct Derivative Estimation................................................ 422
Matrix Imputation ................................................................ 425
Handling high-dimensional data is often cumbersome in practical data analysis. In
this chapter, supervised and unsupervised methods of linear dimensionality reduction
are introduced for reducing the dimensionality of data while preserving intrinsic
information contained in the data.
35.1 CURSE OF DIMENSIONALITY
As the dimensionality of input x grows, any learning problem significantly gets
harder and harder. For example, let us sample 5 points from [0,1] at regular intervals.
Collecting samples in the same way in d-dimensional space requires 5d points
(Fig. 35.1(a)), which grows exponentially with respect to d. Since collecting 5d
points when d is large is not possible in practice (e.g. 5100 ≈1070), samples are
always scarce in high-dimensional problems.
Another trouble in high-dimensional problems is that our geometric intuition can
be misleading. For example, let us consider the inscribed hypersphere of the unit
hypercube in d-dimensional space (Fig. 35.1(b)). When d = 1, the volume of the
inscribed hypersphere is 1, which is the same as the hypercube. When d is increased
to 2 and 3, the volume of the hypercube is still 1, but the volume of the inscribed
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00046-7
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
405

406
CHAPTER 35
LINEAR DIMENSIONALITY REDUCTION
(a) Grid sampling in high-dimensional space. The required number of samples grows
exponentially with respect to the dimensionality. Thus, samples are always scarce in
high-dimensional space
(b) Volume of inscribed hypersphere in high-dimensional space. While the volume
of the unit hypercube is always 1, the volume of the inscribed hypersphere tends to
0 as the dimensionality grows. Thus, the inscribed hypersphere can be ignored in
high-dimensional space
FIGURE 35.1
Curse of dimensionality.
hypersphere is decreased to approximately 0.79 and 0.52, respectively. Our geometric
intuition is that, even though the inscribed hypersphere is smaller than the hypercube,
the inscribed hypersphere is not extremely small. However, as d is further increased,
the volume of the hypercube is still 1, but the volume of the inscribed hypersphere
tends to 0. This means that, when d is large, the inscribed hypersphere is almost
negligible.
As illustrated above, handling high-dimensional data is cumbersome in practice,
which is often referred to as the curse of dimensionality. In the following sections,
various methods of dimensionality reduction are introduced.
Linear dimensionality reduction transforms the original samples {xi}n
i=1 to low-
dimensional expressions {zi}n
i=1 by a linear transformation T ∈Rm×d (Fig. 35.2):
zi = T xi,
T is called an embedding matrix. This chapter is devoted to introducing various linear
dimensionality reduction methods. Nonlinear methods will be covered in Chapter 36.
In the rest of this chapter, training input samples {xi}n
i=1 are assumed to be
centralized as Fig. 35.3:

35.2 UNSUPERVISED DIMENSIONALITY REDUCTION
407
(a) Dimensionality reduction with matrix T
(b) Projection onto a linear subspace
FIGURE 35.2
Linear dimensionality reduction. Transformation by a fat matrix T corresponds to projection onto
a subspace.
FIGURE 35.3
Data centering.
xi ←−xi −1
n
n

i′=1
xi′.
35.2 UNSUPERVISED DIMENSIONALITY REDUCTION
This section covers unsupervised dimensionality reduction, which is aimed at
decreasing the dimensionality of input samples {xi}n
i=1 without losing their intrinsic
information.
35.2.1 PCA
First, a classical unsupervised linear dimensionality reduction method called PCA
[59] is introduced.
PCA tries to keep the position of original samples when the dimensionality is
reduced (Fig. 35.4). More specifically, under the constraint that zi is an orthogonal
projection of xi, embedding matrix T that keeps zi as close to xi as possible is found.
The constraint that zi is an orthogonal projection of xi is equivalent to the fact that
embedding matrix T satisfies TT ⊤= Im, where Im is the m × m identity matrix.

408
CHAPTER 35
LINEAR DIMENSIONALITY REDUCTION
FIGURE 35.4
PCA, which tries to keep the position of original
samples when the dimensionality is reduced.
However, as illustrated in Fig. 35.2(b), the dimensionality of xi is different from
that of zi, and thus the distance between xi and zi cannot be directly measured. Here,
m-dimensional expression zi is transformed back to d-dimensional space by T ⊤and
then the Euclidean distance to xi is computed:
n

i=1
∥T ⊤T xi −xi∥2 = −tr  TCT ⊤ + tr (C) ,
where C is the total scatter matrix:
C =
n

i=1
xi x⊤
i .
Summarizing the above discussion, the optimization problem of PCA is given by
max
T ∈Rm×d tr  TCT ⊤ subject to TT ⊤= Im.
Although this is a nonconvex optimization problem, a global optimal solution is given
analytically by
T = (ξ1,. . . ,ξm)⊤,
where ξ1,. . . ,ξd are eigenvectors of matrix C associated with eigenvalues λ1 ≥· · · ≥
λd ≥0 (Fig. 6.2):
Cξ = λξ.
Note that other global optimal solutions are given by
T = G(ξ1,. . . ,ξm)⊤,
where G is any orthogonal matrix such that G−1 = G⊤.
The embedding matrix of PCA is an orthogonal projection onto the subspace
spanned by eigenvectors associated with large eigenvalues. In other words, by

35.2 UNSUPERVISED DIMENSIONALITY REDUCTION
409
n=100; x=[2*randn(n,1) randn(n,1)];
%x=[2*randn(n,1) 2*round(rand(n,1))-1+randn(n,1)/3];
x=x-repmat(mean(x),[n,1]);
[t,v]=eigs(x’*x,1);
figure(1); clf; hold on; axis([-6 6 -6 6]);
plot(x(:,1),x(:,2),’rx’)
plot(9*[-t(1) t(1)],9*[-t(2) t(2)]);
FIGURE 35.5
MATLAB code for PCA.
FIGURE 35.6
Example of PCA. The solid line denotes the one-dimensional embedding subspace found by
PCA.
removing eigenvectors associated with small eigenvalues, the gap from the original
samples is kept minimum. Note that PCA makes data samples uncorrelated, i.e. the
sample variance-covariance matrix of {zi}n
i=1 is diagonal:
n

i=1
zi z⊤
i = diag (λ1,. . . ,λm) .
A MATLAB code for PCA is provided in Fig. 35.5, and its behavior is illustrated
in Fig. 35.6. This shows that samples are embedded into a one-dimensional subspace
so that they do not change a lot. However, PCA does not necessarily maintain useful
structure of data such as clusters.

410
CHAPTER 35
LINEAR DIMENSIONALITY REDUCTION
FIGURE 35.7
Locality preserving projection, which tries to keep the cluster
structure of original samples when the dimensionality is reduced.
In Section 23.1, subspace-constrained LS was introduced, which requires a
subspace in the input space. If PCA is used in subspace-constrained LS, the
corresponding regression method is called principal component regression.
35.2.2 LOCALITY PRESERVING PROJECTION
To preserving cluster structure of data when dimensionality is reduced, locality
preserving projection [53] is useful (Fig. 35.7).
In locality preserving projection, similarity between samples xi and xi′, denoted
by 0 ≤Wi,i′ ≤1, is utilized. Wi,i′ takes a large value (i.e. close to one) if xi and
xi′ are “similar,” while Wi,i′ takes a small value (i.e. close to zero) if xi and xi′ are
“dissimilar.” The similarity is assumed to be symmetric, i.e. Wi,i′ = Wi′,i. Popular
choices of the similarity measure are summarized in Fig. 35.8.
Locality preserving projection determines embedding matrix T so that sample
pairs with high similarity are close to each other in the embedding space, i.e. the
following criterion is minimized:
1
2
n

i,i′=1
Wi,i′∥T xi −T xi′∥2.
(35.1)
However, this minimization problem has a trivial solution T
=
O, which is
meaningless. To avoid such a degenerated solution, an appropriate constraint such
as
T XDX⊤T ⊤= Im
is imposed, where
X = (x1,. . . , xn) ∈Rd×n,

35.2 UNSUPERVISED DIMENSIONALITY REDUCTION
411
•
Gaussian similarity:
Wi,i′ = exp

−∥xi −xi′∥2
2t2

,
where t > 0 is a tuning parameter that controls the decay of Gaussian
tails.
•
k-nearest-neighbor similarity:
Wi,i′ =

1
(xi ∈Nk(xi′) or xi′ ∈Nk(xi)),
0
(otherwise),
where Nk(x) denotes the set of k-nearest-neighbor samples of x in
{xi}n
i=1, and k ∈{1,. . . ,n} is a tuning parameter that controls the local-
ity. Note that k-nearest-neighbor similarity produces a sparse similarity
matrix W, which is often advantageous in practice.
•
Local scaling similarity [123]:
Wi,i′ = exp

−∥xi −xi′∥2
2titi′

,
where ti is the local scaling defined by
ti = ∥xi −x(k)
i ∥,
and x(k)
i
denotes the kth nearest neighbor of xi in {xi}n
i=1. It is also
practical to combine k-nearest-neighbor similarity and local scaling
similarity.
FIGURE 35.8
Popular choices of similarity measure.
Di,i′ =

n

i′′=1
Wi,i′′
(i = i′),
0
(i , i′).
For
L = D −W,
(35.2)

412
CHAPTER 35
LINEAR DIMENSIONALITY REDUCTION
n=100; x=[2*randn(n,1) randn(n,1)];
%x=[2*randn(n,1) 2*round(rand(n,1))-1+randn(n,1)/3];
x=x-repmat(mean(x),[n,1]); x2=sum(x.^2,2);
W=exp(-(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’));
D=diag(sum(W,2)); L=D-W; z=x’*D*x; z=(z+z’)/2;
[t,v]=eigs(x’*L*x,z,1,’sm’);
figure(1); clf; hold on; axis([-6 6 -6 6]);
plot(x(:,1),x(:,2),’rx’)
plot(9*[-t(1) t(1)],9*[-t(2) t(2)]);
FIGURE 35.9
MATLAB code for locality preserving projection.
Eq. (35.1) can be compactly expressed as tr (T XLX⊤T ⊤). The matrix L is called the
graph Laplacian matrix, which plays an important role in spectral graph theory (see
Section 33.1).
Summarizing the above discussion, the optimization problem of locality preserv-
ing projection is given as
min
T ∈Rm×d tr  T XLX⊤T ⊤ subject to T XDX⊤T ⊤= Im.
Although this is a nonconvex optimization problem, a global optimal solution is given
analytically by
T = (ξd,ξd−1 . . . ,ξd−m+1)⊤,
where ξ1,. . . ,ξd are generalized eigenvectors of (XLX⊤, XDX⊤) associated with
generalized eigenvalues λ1 ≥. . . ≥λd ≥0 (Fig. 6.2):
XLX⊤ξ = λXDX⊤ξ.
Thus, the embedding matrix of locality preserving projection is given by the minor
generalized eigenvectors of (XLX⊤, XDX⊤).
A MATLAB code for locality preserving projection is provided in Fig. 35.9, and
its behavior is illustrated in Fig. 35.10. This shows that cluster structure is nicely
preserved.
35.3 LINEAR DISCRIMINANT ANALYSES FOR
CLASSIFICATION
In this section, methods of supervised linear dimensionality reduction based on input-
output paired training samples {(xi, yi)}n
i=1 for classification (i.e. y ∈{1,. . . ,c}) are
introduced.

35.3 LINEAR DISCRIMINANT ANALYSES FOR CLASSIFICATION
413
FIGURE 35.10
Example of locality preserving projection. The solid line denotes the one-dimensional embedding
subspace found by locality preserving projection.
35.3.1 FISHER DISCRIMINANT ANALYSIS
First, let us introduce a classical supervised linear dimensionality reduction method
called Fisher discriminant analysis [42].
The basic idea of Fisher discriminant analysis is to find a transformation matrix
T so that sample pairs in the same class get closer to each other and sample pairs in
different classes are far apart. More specifically, let S(w) be the within-class scatter
matrix and S(b) be the between-class scatter matrix defined as
S(w) =
c

y=1

i:yi=y
(xi −µy)(xi −µy)⊤∈Rd×d,
(35.3)
S(b) =
c

y=1
ny µy µ⊤
y ∈Rd×d,
(35.4)
where 
i:yi=y denotes the summation over i such that yi = y. µy denotes the mean
of training samples in class y:
µy = 1
ny

i:yi=y
xi,
where ny denotes the number of training samples in class y. Then the optimization
problem of Fisher discriminant analysis is given as
max
T ∈Rm×d tr

(TS(w)T ⊤)−1TS(b)T ⊤
,
where TS(w)T ⊤denotes the within-class scatter matrix after dimensionality reduction
and TS(b)T ⊤denotes the between-class scatter matrix after dimensionality reduction.

414
CHAPTER 35
LINEAR DIMENSIONALITY REDUCTION
n=100; x=randn(n,2);
x(1:n/2,1)=x(1:n/2,1)-4; x(n/2+1:end,1)=x(n/2+1:end,1)+4;
%x(1:n/4,1)=x(1:n/4,1)-4; x(n/4+1:n/2,1)=x(n/4+1:n/2,1)+4;
x=x-repmat(mean(x),[n,1]);
y=[ones(n/2,1); 2*ones(n/2,1)];
m1=mean(x(y==1,:));
x1=x(y==1,:)-repmat(m1,[n/2,1]);
m2=mean(x(y==2,:));
x2=x(y==2,:)-repmat(m2,[n/2,1]);
[t,v]=eigs(n/2*m1’*m1+n/2*m2’*m2,x1’*x1+x2’*x2,1);
figure(1); clf; hold on; axis([-8 8 -6 6]);
plot(x(y==1,1),x(y==1,2),’bo’)
plot(x(y==2,1),x(y==2,2),’rx’)
plot(99*[-t(1) t(1)],99*[-t(2) t(2)], ’k-’)
FIGURE 35.11
MATLAB code for Fisher discriminant analysis.
Thus, Fisher discriminant analysis finds a transformation matrix T that decreases the
within-class scatter and increases the between-class scatter.
Let λ1 ≥· · · ≥λd ≥0 and ξ1,. . . ,ξd be the generalized eigenvalues and
generalized eigenvectors of (S(b),S(w)):
S(b)ξ = λS(w)ξ.
Then the solution T of Fisher discriminant analysis is given analytically as
T = (ξ1,. . . ,ξm)⊤.
A MATLAB code for Fisher discriminant analysis is provided in Fig. 35.11, and
its behavior is illustrated in Fig. 35.12. In these examples, two-dimensional samples
are projected onto one-dimensional subspaces, and a subspace that nicely separates
samples in two classes can be found in Fig. 35.12(a). However, in Fig. 35.12(b),
samples in two classes are mixed up due to the within-class multimodality in class
“◦.”
35.3.2 LOCAL FISHER DISCRIMINANT ANALYSIS
As illustrated in Fig. 35.12, Fisher discriminant analysis can give an undesired
solution if within-class multimodality exists. Another limitation of Fisher discrim-
inant analysis is that the between-class scatter matrix S(b) has rank at most c −1.

35.3 LINEAR DISCRIMINANT ANALYSES FOR CLASSIFICATION
415
(a)
(b)
FIGURE 35.12
Examples of Fisher discriminant analysis. The solid lines denote the found subspaces to which
training samples are projected.
This means that generalized eigenvectors ξ1,. . . ,ξd used for computing the solution
make sense only up to the (c −1)th generalized eigenvector. Thus, in practice, the
reduced dimensionality m should be less than the number of classes, c. This is highly
restrictive when c is small. Indeed, in binary classification where c = 2, m should be
1. To overcome these limitations, local Fisher discriminant analysis [98] introduced
here.
The within-class scatter matrix S(w) and the between-class scatter matrix S(b) used
in Fisher discriminant analysis defined by Eqs (35.3) and (35.4) can be expressed as
S(w) = 1
2
n

i,i′=1
Q(w)
i,i′(xi −xi′)(xi −xi′)⊤,
S(b) = 1
2
n

i,i′=1
Q(b)
i,i′(xi −xi′)(xi −xi′)⊤,
where
Q(w)
i,i′ =

1/ny > 0
(yi = yi′ = y),
0
(yi , yi′),
Q(b)
i,i′ =

1/n −1/ny < 0
(yi = yi′ = y),
1/n > 0
(yi , yi′).

416
CHAPTER 35
LINEAR DIMENSIONALITY REDUCTION
These pairwise expressions allow intuitive understanding of the behavior of Fisher
discriminant analysis, i.e. sample pairs in the same class get close to each other and
sample pairs in different classes are far apart. At the same time, failure of Fisher
discriminant analysis in the presence of within-class multimodality can be explained
by the fact that all samples in the same class are gotten close to each other even if
they form multiple clusters.
To cope with this problem, local Fisher discriminant analysis uses the local
within-class scatter matrix S(lw) and the local between-class scatter matrix S(lb)
defined as
S(lw) = 1
2
n

i,i′=1
Q(lw)
i,i′ (xi −xi′)(xi −xi′)⊤,
S(lb) = 1
2
n

i,i′=1
Q(lb)
i,i′(xi −xi′)(xi −xi′)⊤,
where
Q(lw)
i,i′ =

Wi,i′/ny
(yi = yi′ = y),
0
(yi , yi′),
Q(lb)
i,i′ =

Wi,i′(1/n −1/ny)
(yi = yi′ = y),
1/n
(yi , yi′).
0 ≤Wi,i′ ≤1 denotes a similarity between sample xi and sample xi′ (see Fig. 35.8).
In the above local scatter matrices, similarity Wi,i′ is applied to sample pairs in the
same class, which mitigates faraway sample pairs in the same class to be gotten
close to each other strongly and thus within-class cluster structure tends to be
preserved. Local Fisher discriminant analysis can be interpreted as applying locality
preserving projection introduced in Section 35.2.2 in a classwise manner on top of
Fisher discriminant analysis. Note that similarity Wi,i′ is not applied to sample pairs
in different classes, since they should be faraway even if they belong to different
clusters.
The optimization problem of local Fisher discriminant analysis is given by
max
T ∈Rm×d tr

(TS(lw)T ⊤)−1TS(lb)T ⊤
,
which has exactly the same form as the original Fisher discriminant analysis. Thus,
the solution T of local Fisher discriminant analysis can be obtained analytically in
the same way by
T = (ξ1,. . . ,ξm)⊤,

35.3 LINEAR DISCRIMINANT ANALYSES FOR CLASSIFICATION
417
n=100; x=randn(n,2);
x(1:n/2,1)=x(1:n/2,1)-4; x(n/2+1:end,1)=x(n/2+1:end,1)+4;
%x(1:n/4,1)=x(1:n/4,1)-4; x(n/4+1:n/2,1)=x(n/4+1:n/2,1)+4;
x=x-repmat(mean(x),[n,1]); y=[ones(n/2,1); 2*ones(n/2,1)];
Sw=zeros(2,2); Sb=zeros(2,2);
for j=1:2
p=x(y==j,:); p1=sum(p); p2=sum(p.^2,2); nj=sum(y==j);
W=exp(-(repmat(p2,1,nj)+repmat(p2’,nj,1)-2*p*p’));
G=p’*(repmat(sum(W,2),[1 2]).*p)-p’*W*p;
Sb=Sb+G/n+p’*p*(1-nj/n)+p1’*p1/n; Sw=Sw+G/nj;
end
[t,v]=eigs((Sb+Sb’)/2,(Sw+Sw’)/2,1);
figure(1); clf; hold on; axis([-8 8 -6 6]);
plot(x(y==1,1),x(y==1,2),’bo’)
plot(x(y==2,1),x(y==2,2),’rx’)
plot(99*[-t(1) t(1)],99*[-t(2) t(2)], ’k-’)
FIGURE 35.13
MATLAB code for local Fisher discriminant analysis.
where ξ1,. . . ,ξd are generalized eigenvectors associated with generalized eigenval-
ues λ1 ≥· · · ≥λd ≥0 of (S(lb),S(lw)):
S(lb)ξ = λS(lw)ξ.
Another important advantage of local Fisher discriminant analysis is that the low-
rank problem of S(b) can also be avoided. More specifically, since S(lb) contains
the similarity Wi,i′, S(lb) usually has full rank. Then all generalized eigenvectors
ξ1,. . . ,ξd make sense and thus the reduced dimensionality m can be arbitrarily large.
A MATLAB code for local Fisher discriminant analysis is provided in Fig. 35.13,
and its behavior is illustrated in Fig. 35.14. This shows that local Fisher discriminant
analysis performs well even in the presence of within-class multimodality.
35.3.3 SEMISUPERVISED LOCAL FISHER
DISCRIMINANT ANALYSIS
Supervised dimensionality reduction tends to overfit if the number of training samples
is small. Here, semisupervised local Fisher discriminant analysis [100] is introduced,
which utilizes unlabeled samples {xi}n+n′
i=n+1 in addition to ordinary labeled training
samples {(xi, yi)}n
i=1 to mitigate overfitting (Section 33.1).

418
CHAPTER 35
LINEAR DIMENSIONALITY REDUCTION
FIGURE 35.14
Examples of local Fisher discriminant analysis for the same data sets as Fig. 35.12. The solid
lines denote the found subspaces to which training samples are projected.
The basic idea of semisupervised local Fisher discriminant analysis is to com-
bine (supervised) local Fisher discriminant analysis with (unsupervised) PCA. As
explained in Section 35.2.1, the solution of PCA is given by the leading eigenvectors
of the total scatter matrix:
S(t) =
n+n′

i=1
(xi −µ(t))(xi −µ(t))⊤,
where µ(t) denotes the mean of all (i.e. labeled and unlabeled) input samples
{xi}n+n′
i=1 :
µ(t) =
1
n + n′
n+n′

i=1
xi.
Note that µ(t) is not necessarily zero even though input of labeled samples, {xi}n
i=1,
is assumed to be centralized through this chapter (Fig. 35.3). Similarly, the solution
of local Fisher discriminant analysis is given by solving a generalized eigenvalue
problem, as shown in Section 35.3.2. Based on these facts, the basic idea of
semisupervised local Fisher discriminant analysis is to combine these eigenvalue
problems.
More specifically, instead of the local scatter matrices used in local Fisher
discriminant analysis, semisupervised local Fisher discriminant analysis uses the
following semisupervised local scatter matrices:
S(slw) = (1 −β)S(lw) + βS(t),

35.4 SUFFICIENT DIMENSIONALITY REDUCTION FOR REGRESSION
419
S(slb) = (1 −β)S(lb) + βI,
where I denotes the identity matrix and β ∈[0,1] controls the balance between
labeled and unlabeled samples. Based on these scatter matrices, the optimization
problem of semisupervised local Fisher discriminant analysis is given as
max
T ∈Rm×d tr

(TS(slw)T ⊤)−1TS(slb)T ⊤
,
which has exactly the same form as the original Fisher discriminant analysis. Thus,
the solution T of semisupervised local Fisher discriminant analysis can be obtained
analytically in the same way by
T = (ξ1,. . . ,ξm)⊤,
where ξ1,. . . ,ξd are generalized eigenvectors associated with generalized eigenval-
ues λ1 ≥· · · ≥λd ≥0 of (S(slb),S(slw)):
S(slb)ξ = λS(slw)ξ.
This solution is reduced to (supervised) local Fisher discriminant analysis if β = 0
and is reduced to (unsupervised) PCA if β = 1. For 0 < β < 1, semisupervised
local Fisher discriminant analysis is expected to give a solution that bridges the two
extreme cases.
A MATLAB code for semisupervised local Fisher discriminant analysis is
provided in Fig. 35.15, and its behavior is illustrated in Fig. 35.16. This shows that
semisupervised local Fisher discriminant analysis can successfully avoid overfitting.
35.4 SUFFICIENT DIMENSIONALITY REDUCTION FOR
REGRESSION
The discriminant analysis methods introduced in the previous section explicitly used
the class labels and thus cannot be applied to regression. Here, another supervised
dimensionality reduction method called sufficient dimensionality reduction [67] is
introduced, which can also be applied to regression.
35.4.1 INFORMATION THEORETIC FORMULATION
The basic idea of sufficient dimensionality reduction is to find a transformation matrix
T that makes x conditionally independent of output y given projection z = T x:
p(x, y|z) = p(x|z)p(y|z).
This means that z contains all information about output y.
Such a transformation matrix T is characterized as the maximizer of mutual
information (MI) [92]:
MI =

p(z, y) log p(z, y)
p(z)p(y)dzdy.

420
CHAPTER 35
LINEAR DIMENSIONALITY REDUCTION
n=2; m=200; x=0.1*randn(n,2); b=0.9; %b=0.001; b=1;
x(:,1)=x(:,1)+[repmat(3,[n/2,1]); repmat(-3,[n/2,1])];
%x(1:n/2,2)=x(1:n/2,2)+repmat(5,[n/2,1]);
xx=randn(m,2).*repmat([1 2],[m 1]);
xx(:,1)=xx(:,1)+[repmat(-3,[m/2,1]); repmat(3,[m/2,1])];
%x(:,2)=x(:,2)*1.7; xx(:,2)=xx(:,2)*1.7;
mu=mean([x;xx]); x=x-repmat(mu,[n,1]);
xx=xx-repmat(mu,[m,1]); y=[ones(n/2,1); 2*ones(n/2,1)];
x2=sum(x.^2,2); Qlb=zeros(n,n); Qlw=zeros(n,n);
W=exp(-(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’));
for j=1:2
Wy=W.*((y==j)/2*(y==j)’); Qlw=Qlw+Wy/sum(y==j);
Qlb=Qlb+Wy*(1/n-1/sum(y==j))+(y==j)/n/2*(y~=j)’;
end
Srlb=(1-b)*x’*(diag(sum(Qlb))-Qlb)*x+b*cov([x; xx],1);
Srlw=(1-b)*x’*(diag(sum(Qlw))-Qlw)*x+b*eye(2);
[t,v]=eigs((Srlb+Srlb’)/2,(Srlw+Srlw’)/2,1);
figure(1); clf; hold on; axis([-6 6 -6 6]);
plot(xx(:,1),xx(:,2),’k.’);
plot(x(y==1,1),x(y==1,2),’bo’);
plot(x(y==2,1),x(y==2,2),’rx’);
plot(99*[-t(1) t(1)],99*[-t(2) t(2)], ’k-’);
FIGURE 35.15
MATLAB code for semisupervised local Fisher discriminant analysis.
MI is actually the KL divergence (see Section 14.2) from the joint probability density
p(z, y) to the product of marginals p(z)p(y). Therefore, it is always non-negative and
is zero if and only if p(z, y) = p(z)p(y), i.e. z and y are statistically independent
(see Section 5.6). Maximizing MI with respect to T implies maximizing statistical
dependency between z and y, which is intuitively understandable as supervised
dimensionality reduction.
The value of MI can be approximated by the KL density ratio estimator described
in Section 38.3. However, because the log function and the density ratio
p(z,y)
p(z)p(y) are
included in MI, it tends to be sensitive to outliers. Here, let us use a variant of MI
based on the L2-distance called the quadratic mutual information (QMI) [113]:
QMI = 1
2

f (z, y)2dzdy,

35.4 SUFFICIENT DIMENSIONALITY REDUCTION FOR REGRESSION
421
FIGURE 35.16
Examples of semisupervised local Fisher discriminant analysis. Lines denote the found subspaces
to which training samples are projected. “LFDA” stands for local Fisher discriminant analysis,
“SELF” stands for semisupervised LFDA, and “PCA” stands for principal component analysis.
where f (z, y) is the density difference function defined as
f (z, y) = p(z, y) −p(z)p(y).
Then a maximizer of QMI with respect to transformation matrix T may be obtained
by gradient ascent:
T ←−T + ε ∂QMI
∂T
,
where ε > 0 is the step size.

422
CHAPTER 35
LINEAR DIMENSIONALITY REDUCTION
35.4.2 DIRECT DERIVATIVE ESTIMATION
The partial derivative of QMI with respect to Tℓ,k can be expressed as
∂QMI
∂Tℓ,k
= 1
2
∂f (z, y)2
∂Tℓ,k
dzdy =

f (z, y)∂f (z, y)
∂Tℓ,k
dzdy
=

p(z, y)
m

ℓ′=1
∂f (z, y)
∂z(ℓ′)
∂z(ℓ′)
∂Tℓ,k
dzdy
−

p(z)p(y)
m

ℓ′=1
∂f (z, y)
∂z(ℓ′)
∂z(ℓ′)
∂Tℓ,k
dzdy.
For z = (z(1),. . . , z(m))⊤= T x, the partial derivative ∂z(ℓ′)
∂Tℓ,k is given by
∂z(ℓ′)
∂Tℓ,k
=

x(k)
(ℓ= ℓ′),
0
(ℓ, ℓ′).
Further approximating the expectations by the sample averages yields
∂QMI
∂Tℓ,k
≈1
n
n

i=1
∂f (zi, yi)
∂z(ℓ)
x(k)
i
−1
n2
n

i,i′=1
∂f (zi, yi′)
∂z(ℓ)
x(k)
i .
(35.5)
A naive way to approximate ∂f (z,y)
∂z(ℓ)
in Eq. (35.5) is to separately estimate the
densities p(z, y), p(z), and p(y) from {(zi, yi)}n
i=1, {zi}n
i=1, and {yi}n
i=1, to plug the
estimated densities in f (z, y), and to compute the partial derivative with respect to
z(ℓ). However, density estimation is a hard statistical estimation problem and thus
such a plug-in derivative estimator may not be reliable. Below, a direct estimator of
∂f (z,y)
∂z(ℓ)
without density estimation is introduced [108].
Let us employ the following Gaussian kernel model for approximating ∂f (z,y)
∂z(ℓ) :
gα(z, y) =
n

j=1
αj exp *
,
−∥z −z j∥2 + (y −yj)2
2h2
+
-
.
The parameter vector α = (α1,. . . ,αn)⊤is learned so that the following squared
error is minimized:
J(α) =

gα(z, y) −∂f (z, y)
∂z(ℓ)
2
dzdy
=

gα(z, y)2dzdy −2

gα(z, y)∂f (z, y)
∂z(ℓ) dzdy + C,

35.4 SUFFICIENT DIMENSIONALITY REDUCTION FOR REGRESSION
423
n=500; x=(rand(n,2)*2-1)*10; y=sin(x(:,1)/10*pi);
y2=y.^2; yy=repmat(y2,1,n)+repmat(y2’,n,1)-2*y*y’;
e=10; h=1; r=exp(-yy/(2*h)); rr=sum(r)’/(n^2);
t0=randn(2,1); t0=t0/norm(t0); c=pi*h;
for o=1:10000
z=x*t0; z2=z.^2; zz=repmat(z2,1,n)+repmat(z2’,n,1)-2*z*z’;
k=exp(-zz/(2*h)); kz=k.*(repmat(z’,[n 1])-repmat(z,[1 n]));
U=c*exp(-(zz+yy)/(4*h)); v=mean(kz.*r/h,2)-sum(kz,2).*rr/h;
a=(U+0.1*eye(n))\v; g=(k.*r)*x/n-(k*x).*repmat(rr,[1 2]);
t=t0+e*g’*a; t=t/norm(t);
if norm(t-t0)<0.00001, break, end
t0=t;
end
figure(1); clf; hold on; axis([-10 10 -10 10]); colormap gray
scatter3(x(:,1),x(:,2),y,100,y,’filled’); colorbar;
plot(99*[-t(1) t(1)],99*[-t(2) t(2)],’k-’);
FIGURE 35.17
MATLAB code for supervised dimensionality reduction based on QMI.
where C =
∂f (z,y)
∂z(ℓ)
2 dzdy is a constant independent of parameter α and thus is
ignored. Suppose that gα(z, y) f (z, y) →0 as z and y tend to ±∞. Then integration
by parts (4.4) yields

gα(z, y)∂f (z, y)
∂z(ℓ) dzdy = −
∂gα(z, y)
∂z(ℓ)
f (z, y)dzdy.
Plugging this into J(α), approximating the expectations by the sample averages, and
adding the ℓ2-regularizer result in the following optimization problem:
min
α

α⊤Uα −2α⊤vℓ+ λ∥α∥2
,
where λ ≥0 is the regularization parameter, and U and vℓare the n × n matrix and
the n-dimensional vector defined as
Uj, j′ = ( √πh)m+1 exp *
,
−∥z j −z j′∥2 + (yj −yj′)2
4h2
+
-
,
vℓ, j =
1
nh2
n

i=1
exp *
,
−∥zi −z j∥2 + (yi −yj)2
2h2
+
-
(z(ℓ)
i
−z(ℓ)
j )
−
1
n2h2
n

i,i′=1
exp *
,
−∥zi −z j∥2 + (yi′ −yj)2
2h2
+
-
(z(ℓ)
i
−z(ℓ)
j ).

424
CHAPTER 35
LINEAR DIMENSIONALITY REDUCTION
FIGURE 35.18
Example of supervised dimensionality reduction based on QMI.
The solid line denotes the found subspace to which training
samples are projected.
The above minimizer can be obtained analytically as
αℓ= (U + λI)−1 vℓ.
Then, Eq. (35.5) yields
∂QMI
∂Tℓ,k
≈1
n
n

i=1
gαℓ(zi, yi)x(k)
i
−1
n2
n

i,i′=1
gαℓ(zi, yi′)x(k)
i .
Tuning parameters such as the regularization parameter λ and the Gaussian band-
width h can be optimized by cross validation with respect to the squared error J (or
the misclassification error if a classifier is applied after dimensionality reduction).
A MATLAB code for QMI-based supervised dimensionality reduction is provided
in Fig. 35.17, and its behavior is illustrated in Fig. 35.18. This shows that a subspace
in the input space that strongly depends on output is obtained.
If QMI between x and its projection z = T x is considered, the QMI-based dimen-
sionality reduction method can actually be applied in unsupervised dimensionality
reduction. A MATLAB code for unsupervised dimensionality reduction based on
QMI is provided in Fig. 35.19, and its behavior is illustrated in Fig. 35.20. This shows
that similar results to locality preserving projection are obtained without explicitly
using sample similarity.
Note that the sufficient dimensionality reduction method introduced above can
also be applied to classification.

35.5 MATRIX IMPUTATION
425
n=100; x=[2*randn(n,1) randn(n,1)];
%x=[2*randn(n,1) 2*round(rand(n,1))-1+randn(n,1)/3];
x=x-repmat(mean(x),[n,1]); x2=sum(x.^2,2);
yy=repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’;
e=10; h=1; l=exp(-yy/(2*h)); ll=sum(l)’/(n^2);
t0=randn(2,1); t0=t0/norm(t0); c=pi*h;
for o=1:10000
z=x*t0; z2=z.^2; zz=repmat(z2,1,n)+repmat(z2’,n,1)-2*z*z’;
k=exp(-zz/(2*h)); kz=k.*(repmat(z’,[n 1])-repmat(z,[1 n]));
U=c*exp(-(zz+yy)/(4*h)); v=mean(kz.*l/h,2)-sum(kz,2).*ll/h;
a=(U+0.1*eye(n))\v; g=(k.*l)*x/n-(k*x).*repmat(ll,[1 2]);
t=t0+e*g’*a; t=t/norm(t);
if norm(t-t0)<0.00001, break, end
t0=t;
end
figure(1); clf; hold on; axis([-6 6 -6 6]);
plot(x(:,1),x(:,2),’rx’,’LineWidth’,2,’MarkerSize’,12);
plot(9*[-t(1) t(1)],9*[-t(2) t(2)],’k-’);
FIGURE 35.19
MATLAB code for unsupervised dimensionality reduction based on QMI.
35.5 MATRIX IMPUTATION
Let us consider a matrix X ∈Rd1×d2 with missing entries, i.e. Xk1,k2 is observed
only for (k1,k2) ∈K and Xk1,k2 is missing for all (k1,k2) < K . The objective
of matrix imputation is to fill the missing entries from observed entries. Such an
imputation problem arises, for example, in recommender systems [83], where entry
Xk1,k2 corresponds to the rating of item k1 by user k2. Given a subset K of ratings of
items by users, a recommender system finds the items that are expected to be most
favored by a target user.
Naive approaches to matrix imputation would be to pad the missing entries with
zeros or the average of observed entries. However, such naive methods are not
necessarily useful in practice. The idea for imputing the missing element introduced
here is to approximate the matrix X with a low-rank matrix.
A naive implementation of this idea is to assume that X can be decomposed into
the product of U ∈Rd1×r and V ∈Rr×d2:
X = UV ∈Rd1×d2,

426
CHAPTER 35
LINEAR DIMENSIONALITY REDUCTION
FIGURE 35.20
Example of unsupervised dimensionality reduction based on QMI. The solid line denotes the
found subspace to which training samples are projected.
d1=20; d2=10; n=100; K=(reshape(randperm(d1*d2),[d1,d2])<=n);
r=3; X=randn(d1,r)*randn(r,d2); T0=randn(d1,d2); e=0.1; l=1;
for o=1:1000
[U,S,V]=svd(T0-e*((T0.*K)-(X.*K)),’econ’);
S=diag(max(0,diag(S)-e*l)); T=U*S*V’;
if norm(T-T0)<0.001, break, end
T0=T;
end
figure(1); imagesc(X.*K); figure(2); imagesc(T);
FIGURE 35.21
MATLAB code for unsupervised matrix imputation.
where r is the rank of X. Then the factorized matrices U and V are estimated from the
observed elements, i.e. for Θ = UV, the following optimization problem is solved:
min
U ∈Rd1×r,V ∈Rr×d2
1
2

(k1,k2)∈K

Xk1,k2 −Θk1,k2
2.
Although this minimization problem may be naively solved by gradient descent,
finding the global minimizer is not straightforward in practice due to its nonconvexity.

35.5 MATRIX IMPUTATION
427
(a) Original matrix of size 20 × 10
with rank 3
(b) Observed matrix with 100 missing
entries indicated by crosses
(c) Recovered matrix with rank 3
(d) Zero-filled singular value decom-
position with rank 3
FIGURE 35.22
Example of unsupervised matrix imputation. The gray level indicates the value of each entry in
[−5, 5].
To overcome the nonconvexity, let us employ regularization with the trace norm
∥Θ∥tr (see Fig. 24.10), which tends to produce a low-rank solution:
∥Θ∥tr =
min(d1,d2)

k=1
σk,
where σk is a singular value of Θ (see Fig. 22.2). Then fitting Θ to X with trace
norm regularization allows us to recover the missing entries:
min
Θ∈Rd1×d2
1
2

(k1,k2)∈K

Xk1,k2 −Θk1,k2
2 + λ∥Θ∥tr,

428
CHAPTER 35
LINEAR DIMENSIONALITY REDUCTION
where λ > 0 is the regularization parameter. Since this is a convex optimization
problem, the global optimal solution can be easily obtained, for example, by the
proximal gradient method explained in Fig. 34.8.
A MATLAB code for unsupervised matrix imputation is provided in Fig. 35.21,
and its behavior is illustrated in Fig. 35.22. This shows that missing entries can be
recovered by trace norm regularization better than singular value decomposition after
filling the missing entries with zeros.

CHAPTER
NONLINEAR
DIMENSIONALITY
REDUCTION
36
CHAPTER CONTENTS
Dimensionality Reduction with Kernel Trick ...................................... 429
Kernel PCA ................................................................. 429
Laplacian Eigenmap ....................................................... 433
Supervised Dimensionality Reduction with Neural Networks ...................... 435
Unsupervised Dimensionality Reduction with Autoencoder ....................... 436
Autoencoder................................................................ 436
Training by Gradient Descent .............................................. 437
Sparse Autoencoder ........................................................ 439
Unsupervised Dimensionality Reduction with Restricted Boltzmann Machine ..... 440
Model ...................................................................... 441
Training by Gradient Ascent ................................................ 442
Deep Learning ................................................................... 446
In this chapter, supervised and unsupervised methods of nonlinear dimensionality
reduction are introduced, including approaches based on kernels and neural networks.
36.1 DIMENSIONALITY REDUCTION WITH KERNEL
TRICK
The linear dimensionality reduction methods explained in the previous chapter handle
training samples only in terms of their inner products. This means that the kernel
trick (see Section 27.4) is applicable to obtain nonlinear methods. More specifically,
training input samples {xi}n
i=1 are first transformed by a nonlinear mapping ψ
and then ordinary linear dimensionality reduction algorithms are applied in the
transformed space, which corresponds to nonlinear dimensionality reduction in the
original input space. In this section, kernel-based nonlinear dimensionality reduction
methods are introduced.
36.1.1 KERNEL PCA
Let us illustrate the behavior of PCA in a feature space. The original two-dimensional
samples xi = (x(1)
i , x(2)
i )⊤are plotted in Fig. 36.1(a). Due to the curved spiral shape,
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00047-9
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
429

430
CHAPTER 36
NONLINEAR DIMENSIONALITY REDUCTION
(a) Original two-dimensional samples {xi}n
i=1
(b) Transformed samples {ψ(xi)}n
i=1 in the polar
coordinate system
(c) Result of PCA transformed back to the original
input space
FIGURE 36.1
Nonlinear PCA in a feature space. “×” denotes a sample, the solid line denotes the one-
dimensional embedding subspace found by PCA, and “◦” denotes a projected sample.
linear PCA does not work properly for this data set. If the samples are transformed
into the polar system (i.e. radius r and angle θ), the spiral shape can be nicely
unfolded, as plotted in Fig. 36.1(b). Then linear PCA in the transformed space can
well capture the global structure of the unfolded data. Finally, transforming back the
projected samples to the original input space gives nice nonlinear projection of the
original spiral-shape samples, as plotted in Fig. 36.1(c).

36.1 DIMENSIONALITY REDUCTION WITH KERNEL TRICK
431
FIGURE 36.2
Eigenvalue problems for PCA. Appropriately choosing the expression of eigenvalue problem
depending on whether matrix Ψ is fat or skinny allows us to reduce the computational costs.
If the dimensionality of the feature space is not high, directly performing PCA as
explained above is fine. That is, the eigenvalue problem for the total scatter matrix C
in the feature space is solved:
Cξ = λξ,
where
C =
n

i=1
ψ(xi)ψ(xi)⊤.
However, if the dimensionality of the feature space is high, the computational
complexity grows significantly. More extremely, if the Gaussian kernel,
K(x, x′) = exp

−∥x −x′∥2
2h2

,
(36.1)
is used as a nonlinear mapping, the dimensionality of the feature space is actually
infinite, and thus explicitly performing PCA in the feature space is not possible.
To cope with this problem, let us consider the eigenvalue problem for the kernel
matrix K [88]:
Kα = λα,
where the (i,i′)th element of K is defined as
Ki,i′ = K(xi, xi′) = ⟨ψ(xi),ψ(xi′)⟩.
The matrices C and K can be expressed by using the design matrix
Ψ = (ψ(x1),. . . ,ψ(xn))
as
C = ΨΨ⊤and K = Ψ⊤Ψ.

432
CHAPTER 36
NONLINEAR DIMENSIONALITY REDUCTION
This implies that the eigenvalues of C and K are actually common. Furthermore,
eigenvector α of K and eigenvector ξ of C are related to each other as follows (see
Fig. 36.2):
ξ = Ψα and α = Ψ⊤ξ.
While the size of covariance matrix C depends on the dimensionality of the feature
space, the size of kernel matrix K depends only on the number of samples and is
independent of the dimensionality of the feature space. Thus, if the dimensionality
of the feature space is larger than the number of samples, solving the eigenvalue
problem with K is computationally more efficient.
As explained in Section 35.2.1, PCA requires centering of samples. However,
when the eigenvalue problem with K is solved, feature vectors {ψ(xi)}n
i=1 are not
explicitly handled and thus feature vectors cannot be centralized directly. In kernel
PCA, the kernel matrix K is implicitly centralized as
K ←−HKH,
where
H = In −1
n1n×n
is the centering matrix, In denotes the n-dimensional identity matrix, and 1n×n is the
n × n matrix with all ones.
Another issue to be considered is that kernel PCA requires eigenvectors to be
normalized as ∥ξj∥= 1, but solving the eigenvalue problem with K usually gives
eigenvectors such that ∥αj∥= 1. Therefore, normalization of eigenvectors should be
carried out explicitly as
αj ←−
1
λ j
αj
for j = 1,. . . ,m,
which comes from
∥ξj∥=

∥ξj∥2 =

∥Ψαj∥2 =

⟨Ψ⊤Ψαj,αj⟩=

⟨Kαj,αj⟩=

λ j.
Summarizing the above discussions, final embedding solutions of samples
{xi}n
i=1 by kernel PCA are given by
(z1,. . . , zn) =

1
√λ1
α1,. . . ,
1
√λm
αm
⊤
HKH,
where α1,. . . ,αm are eigenvectors of HKH corresponding to the m largest eigen-
values. Similarly, an embedding solution of a new sample x by kernel PCA is given
by
z =

1
√λ1
α1,. . . ,
1
√λm
αm
⊤
H

k −1
n K1n

,

36.1 DIMENSIONALITY REDUCTION WITH KERNEL TRICK
433
n=200; a=linspace(0,pi,n/2);
u=[a.*cos(a) (a+pi).*cos(a)]’;
v=[a.*sin(a) (a+pi).*sin(a)]’;
x=[u v]; y=[ones(1,n/2) 2*ones(1,n/2)]’;
x2=sum(x.^2,2); hh=2*2^2; H=eye(n)-ones(n,n)/n;
K=exp(-(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’)/hh);
G=H*K*H; [A,L]=eigs(G,2); z=(diag(diag(L).^(-1/2))*A’*G)’;
figure(1); clf; hold on;
plot(z(y==1,1),z(y==1,2),’bo’);
plot(z(y==2,1),z(y==2,2),’rx’);
FIGURE 36.3
MATLAB code for kernel PCA with Gaussian kernels.
where 1n denotes the n-dimensional vectors with all ones and
k = (K(x, x1),. . . ,K(x, xn))⊤.
A MATLAB code of PCA for Gaussian kernel (36.1) is provided in Fig. 36.3,
and its behavior is illustrated in Fig. 36.4. This shows that, while the original two-
dimensional samples are not linearly separable, PCA for Gaussian kernel with width
h = 2 gives linearly separable embedding in the feature space.
36.1.2 LAPLACIAN EIGENMAP
A kernelized version of locality preserving projection is called the Laplacian eigen-
map [13]. In the generalized eigenvalue problem of locality preserving projection,
XLX⊤ξ = λXDX⊤ξ,
multiplying X⊤from the left-hand side and letting ξ = Xβ yield
X⊤XLX⊤Xβ = λX⊤XDX⊤Xβ.
If X⊤X is replaced with kernel matrix K, a kernelized version of locality preserving
projection is obtained as
KLK β = λKDK β.
If K is invertible, multiplying K−1 from the left-hand side and letting α = K β
yield
Lα = λDα,
(36.2)

434
CHAPTER 36
NONLINEAR DIMENSIONALITY REDUCTION
(a)
Original
two-dimensional
samples
(b) h = 0.5
(c) h = 0.7
(d) h = 1
(e) h = 2
(f) h = 5
FIGURE 36.4
Examples of kernel PCA with Gaussian kernels. Original two-dimensional samples are trans-
formed to infinite-dimensional feature space by Gaussian kernels with width h, and then PCA is
applied to reduce the dimensionality to two.
which is the eigenvalue problem that the Laplacian eigenmap solves. Let λ1 ≥· · · ≥
λn ≥0 and α1,. . . ,αn be the generalized eigenvalues and generalized eigenvectors
of Eq. (36.2). Then the definition of graph Laplacian matrix L, Eq. (35.2), yields
L1n = 0n. This means that λn = 0 and αn = 1n hold, which is trivial. In the
Laplacian eigenmap, this trivial eigenvector is removed and the final embedding
solution is obtained as
(z1,. . . , zn) = (αn−1,αn−2,. . . ,αn−m)⊤.
If the similarity matrix W is sparse (see Fig. 35.8), L = D −W is also sparse and
thus eigenvalue problem (36.2) can be solved efficiently even if n is large.
A MATLAB code of the Laplacian eigenmap for 10-nearest neighbor similarity
(see Fig. 35.8) is provided in Fig. 36.5, and its behavior is illustrated in Fig. 36.6.
This shows that the “swiss roll” structure can be nicely unfolded.

36.2 SUPERVISED DIMENSIONALITY REDUCTION
435
n=1000; k=10; a=3*pi*rand(n,1);
x=[a.*cos(a) 30*rand(n,1) a.*sin(a)];
x=x-repmat(mean(x),[n,1]); x2=sum(x.^2,2);
d=repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’; [p,i]=sort(d);
W=sparse(d<=ones(n,1)*p(k+1,:)); W=(W+W’~=0);
D=diag(sum(W,2)); L=D-W; [z,v]=eigs(L,D,3,’sm’);
figure(1); clf; hold on; view([15 10]);
scatter3(x(:,1),x(:,2),x(:,3),40,a,’o’);
figure(2); clf; hold on; scatter(z(:,2),z(:,1),40,a,’o’);
FIGURE 36.5
MATLAB code of Laplacian eigenmap for 10-nearest neighbor similarity.
(a) Original three-dimensional samples {xi}n
i=1
(b) Embedded two-dimensional samples {zi}n
i=1
FIGURE 36.6
Example of Laplacian eigenmap for 10-nearest neighbor similarity.
36.2 SUPERVISED DIMENSIONALITY REDUCTION
WITH NEURAL NETWORKS
A neural network is a nonlinear model having hierarchical structure. If the number
of layers is three (i.e. input, hidden, and output layers), its function is expressed as
follows (see Section 21.3):
fθ(x) =
b

j=1
αjϕ(x; βj),

436
CHAPTER 36
NONLINEAR DIMENSIONALITY REDUCTION
FIGURE 36.7
Dimensionality reduction by neural network. The number of
hidden nodes is smaller than the number of input (and output)
nodes.
where ϕ(x; β) is a basis function parameterized by β. As explained in Section 22.5,
LS training of neural networks based on input-output paired training samples
{(xi, yi)}n
i=1 can be carried out by the gradient method called the error back-
propagation algorithm.
If the number of hidden units, b, is set to be less than the dimensionality of input
x, such a neural network can be used for supervised dimensionality reduction. More
specifically, the output of the hidden layer,
z = (ϕ(x; β1),. . . ,ϕ(x; βb))⊤,
can be regarded as a dimensionality reduction solution (Fig. 36.7).
36.3 UNSUPERVISED DIMENSIONALITY REDUCTION
WITH AUTOENCODER
In this section, the back-propagation approach is applied to unsupervised dimension-
ality reduction.
36.3.1 AUTOENCODER
The autoencoder [14, 116] is a three-layer neural network given by
x = ϕ  W ⊤ϕ(W x + c) + b ,
where W, b, and c are parameters. As illustrated in Fig. 36.8, the autoencoder has
a bottleneck layer, and the connection weights between the first and second layers
and the connection weights between the second and third layers are shared, i.e. W
and W ⊤are used. The connection weights are learned so that output is as close to
input as possible, by which a compressed expression of input can be obtained in the
bottleneck layer. Let us use the sigmoidal activation function (see Fig. 21.6) for the

36.3 UNSUPERVISED DIMENSIONALITY REDUCTION
437
FIGURE 36.8
Autoencoder. Input and output are the same and the number of
hidden nodes is smaller than the number of input nodes.
second and third layers:
φ(x) =
1
1 + exp (−x⊤w −a).
36.3.2 TRAINING BY GRADIENT DESCENT
For d-dimensional input x = (x(1),. . . , x(d))⊤and d-dimensional output
y = (y(1),. . . , y(d))⊤= ϕ  W ⊤ϕ(W x + c) + b ,
the parameters W, b, and c are learned by the stochastic gradient algorithm
introduced in Section 15.3:
W ←−W −ε∇W J(W, b, c),
b ←−b −ε∇b J(W, b, c),
c ←−c −ε∇c J(W, b, c),
where the squared loss,
J(W, b, c) = 1
2
d

k=1

x(k) −y(k)2,
(36.3)
may be used for regression (this corresponds to the log-likelihood of Gaussian
distributions), and the cross entropy loss,
J(W, b, c) = −
d

k=1

x(k) log y(k) + (1 −x(k)) log(1 −y(k))

,
(36.4)
may be used for binary classification (this corresponds to the log-likelihood of
binomial distributions).
For
y = ϕ (h) , h = W ⊤z + b, z = ϕ(g),
and g = W x + c,
(36.5)

438
CHAPTER 36
NONLINEAR DIMENSIONALITY REDUCTION
FIGURE 36.9
Chain rule for autoencoder.
the chain rule of the derivative (Fig. 36.9) yields
∂J
∂Wj,k
=
∂J
∂y(k)
∂y(k)
∂hk
∂hk
∂Wj,k
+
∂J
∂y(k)
∂y(k)
∂hk
∂hk
∂z(j)
∂z(j)
∂gj
∂gj
∂Wj,k
,
∂J
∂bk
=
∂J
∂y(k)
∂y(k)
∂hk
∂hk
∂bk
,
∂J
∂cj
=
d

k=1
∂J
∂y(k)
∂y(k)
∂hk
∂hk
∂z(j)
∂z(j)
∂gj
∂gj
∂cj
.
Then the gradients are given by
∇W J = z(∇b J)⊤+ (∇c J)x⊤,
∇b J =

(y −x) ∗y ∗(1 −y)
(squared loss),
y −x
(cross entropy loss),
∇c J = W∇b J ∗z ∗(1 −z),
where “∗” denotes the elementwise product.
A MATLAB code for dimensionality reduction by the autoencoder is provided
in Fig. 36.10, and its behavior is illustrated in Fig. 36.11. Here, the autoencoder
is trained with 100 noiseless images of hand-written digit “2” (see Section 12.5
for the details of the hand-written digit data set). Then a test image illustrated in
Fig. 36.11(b), which is not included in the training data set, is input to the trained
autoencoder. Since the autoencoder is trained only with noiseless images, noise
components may be eliminated in the second layer. Then a denoised image illustrated
in Fig. 36.11(c) is obtained as output, which looks much better than the output of
linear PCA illustrated in Fig. 36.11(d).

36.3 UNSUPERVISED DIMENSIONALITY REDUCTION
439
load digit.mat; x=X(:,1:100,2); [d,n]=size(x); g=min(x,[],2);
x=(x-repmat(g,1,n))./repmat(max(x,[],2)-g,1,n); x=(x>0.5);
m=10; e=0.01/n; W0=randn(m,d); b0=randn(d,1); c0=rand(m,1);
for o=1:100000
Z=1./(1+exp(-W0*x-repmat(c0,[1 n])));
Y=1./(1+exp(-W0’*Z-repmat(b0,[1 n])));
nb=Y-x; % Cross entropy loss
%
nb=(Y-x).*Y.*(1-Y); % Squared loss
nc=(W0*nb).*Z.*(1-Z); W=W0-e*(Z*nb’+nc*x’);
b=b0-e*sum(nb,2); c=c0-e*sum(nc,2);
if norm(W-W0)+norm(b-b0)+norm(c-c0)<0.003, break, end
W0=W; b0=b; c0=c;
end
t=T(:,1,2)>0.5; u=t; u(rand(d,1)>0.9)=1; u(rand(d,1)>0.9)=0;
z=1./(1+exp(-W*t-c)); y=1./(1+exp(-W’*z-b));
figure(1); clf; hold on; colormap gray
subplot(1,3,1); imagesc(reshape(t,[16 16])’)
subplot(1,3,2); imagesc(reshape(u,[16 16])’)
subplot(1,3,3); imagesc(reshape(y>0.5,[16 16])’)
FIGURE 36.10
MATLAB code for denoising autoencoder. See Section 12.5 for details of hand-written digit data
set “digit.mat.”
36.3.3 SPARSE AUTOENCODER
Let us consider a nonbottleneck neural network, i.e. the number of hidden nodes,
m, is larger than the number of input (and output) nodes, d. If this architecture is
used as an autoencoder, just the identity mapping may be learned as a trivial solution.
However, if the activations in the hidden layer, z = (z(1),. . . , z(m)) (see Eq. (36.5)),
are enforced to be sparse, meaningful features may be extracted.
More specifically, in addition to the loss function J(W, b, c) (see Eqs (36.3) and
(36.4)), a sparsity-inducing regularization term is minimized at the same time:
min
W,b,c J(W, b, c) + λ
m

j=1
R(z(j)),
where λ > 0 is the regularization parameter and R(z) is the regularization functional,
e.g. the ℓ1-norm R(z) = |z| (see Chapter 24), or the KL divergence (see Section 14.2)

440
CHAPTER 36
NONLINEAR DIMENSIONALITY REDUCTION
(a) Original noiseless image
(b) Noisy image
(c) Denoised image by autoencoder (10 hid-
den nodes)
(d) Denoised image by linear PCA (reduced
to 10-dimensional subspace)
FIGURE 36.11
Example of denoising autoencoder.
from a constant ρ ∈(0,1):
R(z) = ρ log
ρ
z/m + (1 −ρ) log (1 −ρ)
1 −z/m .
36.4 UNSUPERVISED DIMENSIONALITY REDUCTION
WITH RESTRICTED BOLTZMANN MACHINE
Although the autoencoder is an unsupervised neural network, it is trained in a
supervised way by regarding output as input. In this section, another unsupervised

36.4 UNSUPERVISED DIMENSIONALITY REDUCTION
441
FIGURE 36.12
Restricted Boltzmann machine.
neural network called the restricted Boltzmann machine [55, 96] is introduced, which
is trained in an unsupervised way.
36.4.1 MODEL
The restricted Boltzmann machine is a two-layered neural network illustrated in
Fig. 36.12, where the left layer for d-dimensional input x = (x(1),. . . , x(d))⊤is called
the visible layer and the right layer for m-dimensional hidden variable
z = (z(1),. . . , z(m))⊤∈{0,1}m
is called the hidden layer. Fig. 36.12 shows that any unit in the input layer and
any unit in the hidden layer are completely connected. On the other hand, units in
the input layer are not connected to each other and units in the hidden layer are
not connected to each other. For the moment, let us assume that input is binary:
x ∈{0,1}d.
The restricted Boltzmann machine is a model of joint probability q(x, z) given by
q(x, z) =
e−E(x,z)

x′∈{0,1}d

z′∈{0,1}m
e−E(x′,z′) .
Here, E(x, z) is called the energy function defined as
E(x, z) = −b⊤x −c⊤z −z⊤W x,
where W, b, and c are parameters. Because there is no connection between the input
and hidden units, the conditional probabilities can be factorized as
q(z|x) =
m

j=1
q(z(j)|x) and q(x|z) =
d

k=1
q(x(k)|z),
(36.6)

442
CHAPTER 36
NONLINEAR DIMENSIONALITY REDUCTION
where each conditional probability is given by the sigmoidal function:
q(z(j) = 1|x) =
1
1 + exp(−d
k=1 Wj,k x(k) −cj)
,
q(x(k) = 1|z) =
1
1 + exp(−m
j=1 Wj,k z(j) −bk).
36.4.2 TRAINING BY GRADIENT ASCENT
For training the restricted Boltzmann machine, let us employ MLE (see Chapter 12).
However, since hidden variable z is not observable, MLE cannot be performed
directly. Here, let us consider the marginal model,
q(x) =

z ∈{0,1}m
q(x, z),
and perform MLE for this model:
max
W,b,c L(W, b, c),
where
L(W, b, c) = 1
n
n

i=1
log *.
,

z ∈{0,1}m
q(xi, z)+/
-
.
The gradient of the log-likelihood is given by
∂L
∂θ = −1
n
n

i=1

z ∈{0,1}m
∂E
∂θ q(z|x = xi) +

x∈{0,1}d

z ∈{0,1}m
∂E
∂θ q(x, z),
(36.7)
where θ represents either W, b, or c, and ∂E
∂θ is given by
∂E
∂W = −z⊤x,
∂E
∂b = −x,
and
∂E
∂c = −z.
Thanks to factorization(36.6), the first term in Eq.(36.7) can be computed naively as
−1
n
n

i=1

z ∈{0,1}m
∂E
∂θ q(z|x = xi) = −1
n
n

i=1
∂E
∂θ
m

j=1

z(j)∈{0,1}
q(z(j)|x = xi).
On the other hand, the second term in Eq.(36.7) can be approximated by the empirical
distribution of samples {xi}n
i=1 as

x ∈{0,1}d

z ∈{0,1}m
∂E
∂θ q(x, z) =

x∈{0,1}d

z ∈{0,1}m
∂E
∂θ q(z|x)p(x)
≈1
n
n

i=1

z ∈{0,1}m
∂E
∂θ q(z|xi).

36.4 UNSUPERVISED DIMENSIONALITY REDUCTION
443
1. Let xi = xi for i = 1,. . . ,n.
2. Generate {zi}n
i=1 from {xi}n
i=1 following q(z|x = xi).
3. Generate {xi}n
i=1 from {zi}n
i=1 following q(x|z = zi).
4. Iterate 2–3 until convergence.
FIGURE 36.13
Contrastive divergence algorithm for restricted Boltzmann machine. Note that q(z|x = xi) and
q(x|z = zi) can be factorized as Eq. (36.6), which allows efficient computation.
load digit.mat; x=X(:,1:100,2); [d,n]=size(x); g=min(x,[],2);
x=(x-repmat(g,1,n))./repmat(max(x,[],2)-g,1,n); x=(x>0.5);
m=10; e=0.01/n; W0=randn(m,d); b0=randn(d,1); c0=rand(m,1);
for o=1:100000
pZ=1./(1+exp(-W0*x-repmat(c0,1,n))); Z=1*(rand(m,n)<pZ);
pY=1./(1+exp(-W0’*Z-repmat(b0,1,n))); Y=1*(rand(d,n)<pY);
pX=1./(1+exp(-W0*Y-repmat(c0,1,n))); W=W0+e*(pZ*x’-pX*Y’);
b=b0+e*(x*prod(pZ)’-Y*prod(pX)’);
c=c0+e*(sum(pZ,2)-sum(pX,2));
if norm(W-W0)+norm(b-b0)+norm(c-c0)<0.007, break, end
W0=W; b0=b; c0=c;
end
t=T(:,1,2)>0.5; u=t; u(rand(d,1)>0.9)=1; u(rand(d,1)>0.9)=0;
z=1./(1+exp(-W*t-c)); y=1./(1+exp(-W’*z-b));
figure(1); clf; hold on; colormap gray
subplot(1,3,1); imagesc(reshape(t,[16 16])’)
subplot(1,3,2); imagesc(reshape(u,[16 16])’)
subplot(1,3,3); imagesc(reshape(y>0.5,[16 16])’)
FIGURE 36.14
MATLAB code for denoising restricted Boltzmann machine. See Section 12.5 for details of hand-
written digit data set “digit.mat.”
However, since this is exactly the same as the negative of the first term, the gradient
approximately computed in this way always vanishes and thus is useless in practice.
To cope with this problem, let us consider another set of samples {xi}n
i=1 generated
by the contrastive divergence algorithm [54], described in Fig. 36.13, which is based
on Gibbs sampling (see Section 19.3.3).

444
CHAPTER 36
NONLINEAR DIMENSIONALITY REDUCTION
(a) Original noiseless image
(b) Noisy image
(c) Denoised image by restricted Boltzmann
machine (10 hidden nodes)
(d) Denoised image by linear PCA (reduced to
10-dimensional subspace)
FIGURE 36.15
Example of denoising restricted Boltzmann machine.
Finally, the gradient of the log-likelihood is approximated as
∂L
∂θ ≈1
n
n

i=1
∂E
∂θ
m

j=1

z(j)∈{0,1}

−q(z(j)|x = xi) + q(z(j)|x = xi)

.
Based on this approximated gradient, the parameters W, b, and c can be learned by
gradient ascent (see Section 15.3).

36.4 UNSUPERVISED DIMENSIONALITY REDUCTION
445
FIGURE 36.16
Construction of deep neural network by stacking.
When input x is continuous, the energy function may be replaced with
E(x, z) = 1
2 x⊤x −b⊤x −c⊤z −z⊤W x,
which results in the Gaussian conditional probability:
q(x(k) = 1|z) =
1
√
2π
exp *..
,
−1
2
*.
,
x(k) −bk −
m

j=1
z(j)Wj,k+/
-
2
+//
-
.
A MATLAB code for dimensionality reduction by the restricted Boltzmann
machine is provided in Fig. 36.14, and its behavior is illustrated in Fig. 36.15. This is

446
CHAPTER 36
NONLINEAR DIMENSIONALITY REDUCTION
the same denoising experiment as Fig. 36.11, and the restricted Boltzmann machine
also works much better than linear PCA.
36.5 DEEP LEARNING
In Section 22.5, the error back-propagation algorithm for supervised training of
neural networks was introduced. As pointed out there, due to the hierarchical
structure of neural networks, gradient-based training is prone to be trapped by a local
optimal solution.
It was experimentally demonstrated that training autoencoders or restricted
Boltzmann machines layer by layer by unsupervised learning and stacking them (see
Fig. 36.16) can produce good initialization of connection weights [55, 86]. In other
words, such deep autoencoders and deep Boltzmann machines can be regarded as
good feature extractors for succeeding supervised learning tasks.
Extensive research on deep learning is on going and latest information can be
found, e.g. from “http://deeplearning.net/.”

CHAPTER
CLUSTERING
37
CHAPTER CONTENTS
k-Means Clustering .............................................................. 447
Kernel k-Means Clustering ....................................................... 448
Spectral Clustering............................................................... 449
Tuning Parameter Selection ...................................................... 452
In this chapter, the problem of unsupervised classification of input-only samples
{xi}n
i=1 called clustering is discussed, which is aimed at grouping data samples based
on their similarity.
37.1 k-MEANS CLUSTERING
The method of k-means clustering is one of the most fundamental algorithms for
clustering, which aims at finding cluster labels,
{yi | yi ∈{1,. . . ,c}}n
i=1,
so that the sum of within-cluster scatters is minimized:
min
y1,...,yn ∈{1,...,c}
c

y=1

i:yi=y
∥xi −µy∥2,
where 
i:yi=y denotes the sum over i such that yi = y,
µy = 1
ny

i:yi=y
xi
denotes the center of center y, and ny denotes the number of samples in cluster y.
However, solving this optimization problem will take exponential computation
time with respect to the number of samples n [3], and thus is computationally
infeasible if n is large. In practice, a local optimal solution is found by iteratively
assigning a single sample to the cluster with the nearest center in a greedy manner:
yi ←−argmin
y ∈{1,...,c}
∥xi −µy∥2,
i = 1,. . . ,n.
(37.1)
The algorithm of k-means clustering is summarized in Fig. 37.1.
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00048-0
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
447

448
CHAPTER 37
CLUSTERING
1. Initialize cluster centers µ1,. . . , µc.
2. Update cluster labels y1,. . . , yn for samples x1,. . . , xn:
yi ←−argmin
y ∈{1,...,c}
∥xi −µy∥2, i = 1,. . . ,n.
3. Update cluster centers µ1,. . . , µc:
µy ←−1
ny

i:yi=y
xi, y = 1,. . . ,c,
where ny denotes the number of samples in cluster y.
4. Iterate 2 and 3 until convergence.
FIGURE 37.1
k-means clustering algorithm.
A MATLAB code for k-means clustering is provided in Fig. 37.2, and its behavior
is illustrated in Fig. 37.3. This shows that the k-means clustering algorithm gives a
reasonable solution for this data set.
37.2 KERNEL k-MEANS CLUSTERING
Since k-means clustering uses the Euclidean distance ∥x−µy∥for determining cluster
assignment, it can only produce linearly separable clusters, as illustrated in Fig. 37.3.
Here, let us apply the kernel trick introduced in Section 27.4 to obtain a nonlinear
version of k-means clustering, called kernel k-means clustering [48].
More specifically, let us express the squared Euclidean distance ∥x −µy∥2 in
Eq. (37.1) by using the inner product ⟨x, x′⟩as
∥x −µy∥2 =

x −1
ny

i:yi=y
xi

2
= ⟨x, x⟩−2
ny

i:yi=y
⟨x, xi⟩+ 1
n2y

i,i′:yi=yi′=y
⟨xi, xi′⟩.
Then, replacing the inner product with the kernel function K(x, x′) immediately gives
the kernel k-means clustering algorithm:
y ←−argmin
y ∈{1,...,c}

−2
ny

i:yi=y
K(x, xi) + 1
n2y

i,i′:yi=yi′=y
K(xi, xi′)

,
where irrelevant constant ⟨x, x⟩= K(x, x) is ignored.

37.3 SPECTRAL CLUSTERING
449
n=300; c=3; t=randperm(n);
x=[randn(1,n/3)-2 randn(1,n/3) randn(1,n/3)+2;
randn(1,n/3) randn(1,n/3)+4 randn(1,n/3)]’;
m=x(t(1:c),:); x2=sum(x.^2,2); s0(1:c,1)=inf;
for o=1:1000
m2=sum(m.^2,2);
[d,y]=min(repmat(m2,1,n)+repmat(x2’,c,1)-2*m*x’);
for t=1:c
m(t,:)=mean(x(y==t,:)); s(t,1)=mean(d(y==t));
end
if norm(s-s0)<0.001, break, end
s0=s;
end
figure(1); clf; hold on;
plot(x(y==1,1),x(y==1,2),’bo’);
plot(x(y==2,1),x(y==2,2),’rx’);
plot(x(y==3,1),x(y==3,2),’gv’);
FIGURE 37.2
MATLAB code for k-means clustering.
Since ordinary k-means clustering in the kernel feature space corresponds to
nonlinear clustering in the original input space, it can give clustering solutions
with nonlinear boundaries. However, due to high nonlinearity brought by the kernel
function, optimization of kernel k-means clustering is much harder then that of
ordinary k-means clustering and the solution depends even strongly on initialization.
For this reason, the use of kernel k-means clustering is not straightforward in
practice.
37.3 SPECTRAL CLUSTERING
Strong dependency of kernel k-means clustering on the initial solution is more
prominent if the dimensionality of the feature space is higher. Spectral clustering
addresses this issue by combining clustering with dimensionality reduction.
More specifically, after transforming samples to a feature space by a kernel
function, spectral clustering applies locality preserving projection introduced in
Section 35.2.2 to reduce the dimensionality, which has the property that cluster
structure of data tends to be preserved. Note that locality preserving projection in the
feature space is equivalent to the Laplacian eigenmap introduced in Section 36.1.2.

450
CHAPTER 37
CLUSTERING
(a) Initial value
(b) After 2 iterations
(c) After 4 iterations
(d) Final solution (after 6 iterations)
FIGURE 37.3
Example of k-means clustering. A filled square denotes a cluster center.
Then, after dimensionality reduction, spectral clustering employs ordinary (not
kernel) k-means clustering to obtain cluster labels. The algorithm of spectral
clustering is summarized in Fig. 37.4.
A MATLAB code for spectral clustering is provided in Fig. 37.5, and its
behavior is illustrated in Fig. 37.6. The original two-dimensional samples {xi}n
i=1
in Fig. 37.6(a) are projected onto a one-dimensional subspace in the feature space,
which gives embedded samples {zi}n
i=1 degenerated into two points as illustrated in
Fig. 37.6(b). Then applying ordinary k-means clustering to the degenerated samples

37.3 SPECTRAL CLUSTERING
451
1. Apply Laplacian eigenmap to samples {xi}n
i=1 and obtain (c −1)-
dimensional expressions {zi}n
i=1, where c is the number of clusters.
2. Apply(non-kernelized) k-means clustering to the embedded samples
{zi}n
i=1 and obtain cluster labels {yi}n
i=1.
FIGURE 37.4
Algorithm of spectral clustering.
n=500; c=2; k=10; t=randperm(n); a=linspace(0,2*pi,n/2)’;
x=[a.*cos(a) a.*sin(a); (a+pi).*cos(a) (a+pi).*sin(a)];
x=x+rand(n,2); x=x-repmat(mean(x),[n,1]); x2=sum(x.^2,2);
d=repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’; [p,i]=sort(d);
W=sparse(d<=ones(n,1)*p(k+1,:)); W=(W+W’~=0);
D=diag(sum(W,2)); L=D-W; [z,v]=eigs(L,D,c-1,’sm’);
m=z(t(1:c),:); s0(1:c,1)=inf; z2=sum(z.^2,2);
for o=1:1000
m2=sum(m.^2,2);
[u,y]=min(repmat(m2,1,n)+repmat(z2’,c,1)-2*m*z’);
for t=1:c
m(t,:)=mean(z(y==t,:)); s(t,1)=mean(d(y==t));
end
if norm(s-s0)<0.001, break, end
s0=s;
end
figure(1); clf; hold on; axis([-10 10 -10 10]);
plot(x(y==1,1),x(y==1,2),’bo’);
plot(x(y==2,1),x(y==2,2),’rx’);
FIGURE 37.5
MATLAB code for spectral clustering.
{zi}n
i=1 gives cluster labels {yi}n
i=1 illustrated in Fig. 37.6(c). This clustering solution
actually corresponds to clustering two spirals in the original input space, as illustrated
in Fig. 37.6(d).

452
CHAPTER 37
CLUSTERING
(a) Original two-dimensional samples {xi}n
i=1
(d) Clustering results {yi}n
i=1 in the original input
space
(b) One-dimensional projection {zi}n
i=1 by
Laplacian eigenmap
(c) Clustering results {yi}n
i=1 obtained by ordi-
nary k-means clustering in the feature
FIGURE 37.6
Example of spectral clustering.
37.4 TUNING PARAMETER SELECTION
Clustering solutions obtained by kernel k-means and spectral clustering depend on
the choice of kernels such as the Gaussian bandwidth. In this section, the problem of
tuning parameter selection is addressed.
Clustering can be regarded as compressing d-dimensional samples {xi}n
i=1 into c-
valued labels {yi}n
i=1 (Fig. 37.7). Based on this data compression view, let us choose
tuning parameters so that the amount of information cluster labels {yi}n
i=1 contain on
input samples {xi}n
i=1 is maximized.
MI [92] allows us to measure the amount of information cluster labels {yi}n
i=1
contain on input samples {xi}n
i=1:
MI =

c

y=1
p(x, y) log p(x, y)
p(x)p(y)dx.
MI is actually the KL divergence (see Section 14.2) from the joint probability p(x, y)
to the product of marginals p(x)p(y). Therefore, it is always non-negative and is

37.4 TUNING PARAMETER SELECTION
453
FIGURE 37.7
Clustering can be regarded as compressing d-dimensional vector
x into c-valued scalar y.
zero if and only if p(x, y) = p(x)p(y), i.e. x and y are statistically independent (see
Section 5.6). MI allows us to measure the dependency of x and y, i.e. the amount of
information y contains on x.
The value of MI can be approximated by the KL density ratio estimator described
in Section 38.3. However, because the log function and the ratio of probability
densities are included in MI, it tends to be sensitive to outliers. Here, let us use a
variant of MI based on the L2-distance called the QMI [113]:
QMI =

c

y=1

p(x, y) −p(x)p(y)
2dx,
which is also always non-negative and takes zero if and only if x and y are statistically
independent, i.e. p(x, y) = p(x)p(y).
Below, an estimator of QMI from {(xi, yi)}n
i=1 called LS QMI estimation is
introduced, which is an application of LS density difference estimation introduced
in Section 39.1.3 to QMI. More specifically, LS QMI estimation does not involve
estimation of p(x, y), p(x), and p(y), but it directly estimates the density difference
function:
f (x, y) = p(x, y) −p(x)p(y).
Let us employ the Gaussian kernel model for approximating the density difference
f (x, y):
fα(y)(x, y) =
ny

j=1
α(y)
j
exp *.
,
−
∥x −x(y)
j ∥2
2h2
+/
-
,
where α(y) = (α1,. . . ,αny)⊤is a parameter vector for cluster y, ny denotes the
number of samples in cluster y, and {x(y)
i }ny
i=1 denote samples in cluster y among
{xi}n
i=1. The parameter vector {α(y)}c
y=1 is learned so that the following squared
error is minimized:

454
CHAPTER 37
CLUSTERING
J(α(1),. . . ,α(c)) =

c

y=1

fα(y)(x, y) −f (x, y)
2dx
=

c

y=1
fα(y)(x, y)2dx −2

c

y=1
fα(y)(x, y) f (x, y)dx + C,
where C =
c
y=1 f (x, y)2dx is a constant independent of parameter α and thus
is ignored. The expectation in the second term may be approximated by the sample
average and p(y) ≈ny/n as

fα(y)(x, y) f (x, y)dx
=

fα(y)(x, y)p(x|y)p(y)dx −

fα(y)(x, y)p(x)p(y)dx
≈1
ny
ny

i=1
fα(y)(x(y)
i , y)ny
n −1
n
n

i=1
fα(y)(xi, y)ny
n
= 1
n
ny

i=1
fα(y)(x(y)
i , y) −ny
n2
n

i=1
fα(y)(xi, y).
Further adding the ℓ2-regularizer yields the following criterion:
min
α(1),...,α(c)
c

y=1

α(y)⊤U(y)α(y) −2α(y)⊤v(y) + λ∥α(y)∥2
,
where λ ≥0 is the regularization parameter and U(y) and v(y) are the ny × ny matrix
and the ny-dimensional vector defined as
U(y)
j, j′ = ( √πh)d exp *.
,
−
∥x(y)
j
−x(y)
j′ ∥2
4h2
+/
-
,
v(y)
j
= 1
n
ny

i=1
exp *.
,
−
∥x(y)
i
−x(y)
j ∥2
2h2
+/
-
−ny
n2
n

i=1
exp *.
,
−
∥xi −x(y)
j ∥2
2h2
+/
-
.
This optimization problem can be solved analytically and separately for each
y = 1,. . . ,c as
α(y) =

U(y) + λI
−1 v(y).
Then QMI can be approximated as
E
QMI =
c

y=1
*
,
1
n
ny

i=1
f α(y)(x(y)
i , y) −ny
n2
n

i=1
f α(y)(xi, y)+
-
=
c

y=1
α(y)⊤v(y),

37.4 TUNING PARAMETER SELECTION
455
n=500; a=linspace(0,2*pi,n/2)’; y=[ones(1,n/2) zeros(1,n/2)];
x=[a.*cos(a) a.*sin(a); (a+pi).*cos(a) (a+pi).*sin(a)];
x=x+rand(n,2); x=x-repmat(mean(x),[n,1]); x2=sum(x.^2,2);
d=repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’;
hs=[0.5 1 2].^2; ls=10.^[-5 -4 -3]; m=2; r=size(x,2)/2;
u=mod(randperm(n),m)+1; hhs=2*[0.5 1 2].^2;
g=zeros(length(hhs),length(ls),m);
for hk=1:length(hs)
h=hs(hk); k=exp(-d/(2*h));
for j=unique(y)
t=(y==j); U=(pi*h)^r*exp(-d(t,t)/(4*h));
for i=1:m
ai=(u~=i); ni=sum(a); aj=(u==i); nj=sum(aj);
vi=sum(k(t,ai&t),2)/ni-sum(k(t,ai),2)*sum(ai&t)/(ni^2);
vj=sum(k(t,aj&t),2)/nj-sum(k(t,aj),2)*sum(aj&t)/(nj^2);
for lk=1:length(ls)
l=ls(lk); a=(U+l*eye(sum(t)))\vi;
g(hk,lk,i)=g(hk,lk,i)+a’*U*a-2*vj’*a;
end, end, end, end
g=mean(g,3); [gl,ggl]=min(g,[],2); [ghl,gghl]=min(gl);
L=ls(ggl(gghl)); H=hs(gghl); s=0;
for j=unique(y)
t=(y==j); ny=sum(t); U=(pi*H)^r*exp(-d(t,t)/(4*H));
k=exp(-d(t,:)/(2*H)); v=sum(k(:,t),2)/n-sum(k,2)*ny/(n^2);
a=(U+L*eye(ny))\v; s=s+v’*a;
end
disp(sprintf(’Information=%g’,s));
FIGURE 37.8
MATLAB code for LS QMI estimation.
which comes from the following expression of QMI:
QMI =

c

y=1
f (x, y)

p(x, y) −p(x)p(y)

dx.
All tuning parameters such as the regularization parameter λ and the Gaussian
bandwidth h can be optimized by cross validation with respect to the squared error
J. This is a strong advantage in unsupervised clustering.

456
CHAPTER 37
CLUSTERING
(a) k = 1
(b) k = 10
(c) k = 100
(d) Estimated QMI
FIGURE 37.9
Example of LS QMI estimation.
A MATLAB code for LS QMI estimation is provided in Fig. 37.8, and its
behavior is illustrated in Fig. 37.9. This shows that setting the parameter k included
in the nearest neighbor similarity at a too small value (Fig. 37.9(a)) or too large
value (Fig. 37.9(c)) does not produce a desirable clustering solution. On the other
hand, if the parameter k is chosen so that the LS QMI estimator is maximized, an
intermediate value is selected (Fig. 37.9(d)) and this choice works well for this data
set (Fig. 37.9(b)).

CHAPTER
OUTLIER DETECTION 38
CHAPTER CONTENTS
Density Estimation and Local Outlier Factor ...................................... 457
Support Vector Data Description.................................................. 458
Inlier-Based Outlier Detection .................................................... 464
The objective of outlier detection is to find outlying samples in input-only training
samples {xi}n
i=1. If the labels of inliers and outliers are available for {xi}n
i=1, outlier
detection can be formulated as supervised classification. However, since outliers may
be highly diverse and their tendency may change over time, learning a stable decision
boundary between inliers and outliers is often difficult in practice. In this chapter,
various unsupervised outlier detection methods are introduced.
In Part 4, supervised learning methods that are robust against outliers were
introduced. When the number of outliers is expected not to be too large, robust
learning from a data set contaminated with outliers may be useful. On the other hand,
if outliers are expected to be more abundant in the data set, removing them in advance
by an outlier detection method would be more appropriate.
38.1 DENSITY ESTIMATION AND LOCAL OUTLIER
FACTOR
A naive outlier detection method is based on density estimation. More specifically, the
probability density p(x) of samples {xi}n
i=1 is estimated using, e.g. one of the density
estimators described in Part 3, and then samples having low probability densities are
regarded as outliers. However, since estimating the probability density in low-density
regions is difficult due to the shortage of samples, such a density estimation approach
may be unreliable for outlier detection purposes. The local outlier factor [23] is a
stabilized variant of the density estimation approach that finds samples isolated from
other samples.
Let us define the reachability distance (RD) from x to x′ as
RDk(x, x′) = max

∥x −x(k)∥,∥x −x′∥

,
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00049-2
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
457

458
CHAPTER 38
OUTLIER DETECTION
where x(k) is the kth nearest neighbor of x in {xi}n
i=1. The RD can be regarded as a
stabilized variant of the Euclidean distance ∥x −x′∥so that the distance is not less
than ∥x −x(k)∥. Based on the RD, the local RD of x is defined as
LRDk(x) = *
,
1
k
k

i=1
RDk(x(i), x)+
-
−1
,
which is the inverse of the average RDs from x(i) to x. When x is isolated from
surrounding samples, the local RD takes a small value.
The local outlier factor of x is defined as
LOFk(x) =
1
k
k

i=1
LRDk(x(i))
LRDk(x)
,
and x is regarded as an outlier if LOFk(x) takes a large value. LOFk(x) is the ratio
of the average local RD of x(i) and the local RD of x, and x is regarded as an outlier
if x(i) is in a high-density region and x is in a low-density region. Conversely, if x(i)
is in a low-density region and x is in a high-density region, LOFk(x) takes a small
value and thus x is regarded as an inlier.
A MATLAB code for computing the local outlier factor is provided in Fig. 38.2,
and its behavior is illustrated in Fig. 38.1. This shows that isolated samples tend to
have large outlier scores. However, the behavior of the local outlier factor depends
on the choice of nearest neighbors, k, and it is not straightforward to optimize k in
practice.
38.2 SUPPORT VECTOR DATA DESCRIPTION
Support vector data description [109] is an unsupervised outlier detection algorithm
that does not involve explicit density estimation.
Let us consider a hypersphere with center c and radius
√
b on Rd, and learn c and
b to include all training samples {xi}n
i=1 (i.e., the minimum enclosing ball is found):
min
c,b b subject to ∥xi −c∥2 ≤b
for i = 1,. . . ,n.
(38.1)
Note that, not the radius
√
b, but the squared radius b is optimized for the convexity
of the optimization problem [26].
Support vector data description is a relaxed variant of the minimum enclosing ball
problem which finds a hypersphere that contains most of the training samples {xi}n
i=1
(Fig. 38.3):

38.2 SUPPORT VECTOR DATA DESCRIPTION
459
n=100; x=[(rand(n/2,2)-0.5)*20; randn(n/2,2)]; x(n,1)=14;
k=3; x2=sum(x.^2,2);
[s,t]=sort(sqrt(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’),2);
for i=1:k+1
for j=1:k
RD(:,j)=max(s(t(t(:,i),j+1),k),s(t(:,i),j+1));
end
LRD(:,i)=1./mean(RD,2);
end
LOF=mean(LRD(:,2:k+1),2)./LRD(:,1);
figure(1); clf; hold on
plot(x(:,1),x(:,2),’rx’);
for i=1:n
plot(x(i,1),x(i,2),’bo’,’MarkerSize’,LOF(i)*10);
end
FIGURE 38.1
MATLAB code for local outlier factor.
min
c,b,ξ

b + C
n

i=1
ξi

subject to ∥xi −c∥2 ≤b + ξi
for i = 1,. . . ,n,
ξi ≥0
for i = 1,. . . ,n,
b ≥0,
(38.2)
where C > 0 controls the number of training samples included in the hypersphere
and ξi is the margin error for xi. Samples outside the hypersphere are regarded as
outliers, i.e., a test sample x is regarded as an outlier if
∥x −c∥2 > b,
(38.3)
where c and b are the solutions of optimization problem (38.2).
Optimization problem (38.2) (and also minimum enclosing ball problem (38.1))
has quadratic constraints, and directly handling them in optimization can be compu-
tationally expensive. Below, let us consider its Lagrange dual problem (Fig. 23.5).
Given the fact that the constraint b ≥0 can be dropped without changing the
solution when C > 1/n [27], the Lagrange dual of optimization problem (38.2) is
given by
max
α,β inf
c,b,ξ L(c,b,ξ,α, β) subject to α ≥0, β ≥0,

460
CHAPTER 38
OUTLIER DETECTION
FIGURE 38.2
Example of outlier detection by local outlier factor. The diameter
of circles around samples is proportional to the value of local
outlier factor.
FIGURE 38.3
Support vector data description. A hypersphere that contains
most of the training samples is found. Samples outside the
hypersphere are regarded as outliers.
where α and β are the Lagrange multipliers and L(c,b,ξ,α, β) is the Lagrange
function defined by
L(c,b,ξ,α, β) = b + C
n

i=1
ξi −
n

i=1
αi

b + ξi −∥xi −c∥2
−
n

i=1
βiξi.

38.2 SUPPORT VECTOR DATA DESCRIPTION
461
The first-order optimality conditions of infc,b,ξ L(c,b,ξ,α, β) yield
∂L
∂b = 0 =⇒
n

i=1
αi = 1,
∂L
∂c = 0 =⇒
c =
n

i=1
αi xi
n

i=1
αi
=
n

i=1
αi xi,
(38.4)
∂L
∂ξi
= 0 =⇒αi + βi = C,
∀i = 1,. . . ,n,
and thus the Lagrange dual problem can be expressed as
max
α

n

i=1
αiQi,i −
n

i, j=1
αiαjQi, j

subject to 0 ≤αi ≤C
for i = 1,. . . ,n,
n

i=1
αi = 1,
(38.5)
where
Qi, j = x⊤
i xj.
This is a quadratic programming problem (Fig. 27.5), which can be efficiently
solved by standard optimization software. However, note that the above quadratic
programming problem is convex only when the n × n matrix Q is nonsingular. When
it is singular, Q may be ℓ2-regularized by adding a small positive constant to the
diagonal elements (see Section 23.2) in practice.
Note that, if C > 1, optimization problem(38.2) is reduced to minimum enclosing
ball problem (38.1), meaning that the solution does not depend on C [27]. This
resembles the relation between hard margin support vector classification and soft
margin support vector classification introduced in Chapter 27. On the other hand,
when 0 < C ≤1/n, b = 0 is actually the solution [27], which is not useful for
outlier detection because all samples are regarded as outliers. Thus, support vector
data description is useful only when
1
n < C ≤1.
From the KKT conditions (Fig. 27.7) of dual optimization problem (38.5),
the following properties hold in the same way as support vector classification
(Chapter 27):

462
CHAPTER 38
OUTLIER DETECTION
•
αi = 0 implies ∥xi −c∥2 ≤b.
•
0 < αi < C implies ∥xi −c∥2 = b.
•
αi = C implies ∥xi −c∥2 ≥b.
•
∥xi −c∥2 < b implies αi = 0.
•
∥xi −c∥2 > b implies αi = C.
Thus, xi is on the surface or in the interior of the hypersphere when αi = 0, xi lies
on the surface when 0 < αi < C, and xi is on the surface or is in the exterior of the
hypersphere when αi = C. On the other hand, αi = 0 when xi is in the strict interior
of the hypersphere and αi = C when xi is in the strict exterior of the hypersphere.
Similarly to the case of support vector classification, sample xi such that αi > 0
is called a support vector. From Eq. (38.4), the solution c is given by
c =

i:αi >0
αi xi.
Since ∥xi −c∥2 = b holds for xi such that 0 < αi < C, the solution b is given by
b = xi −c2 .
As explained in Eq. (38.3), with the solutions c and b, a test sample x is regarded as
an outlier if, for i such that 0 < αi < C,
∥x −c∥2 −b = ∥x −c∥2 −xi −c2
= x⊤x −2
n

j=1
αj x⊤xj −ai > 0,
(38.6)
where
ai = x⊤
i xi −2
n

j=1
αj x⊤
i xj.
Note that ai can be computed in advance independent of the test sample x.
In the same way as support vector classification, support vector data description
can be nonlinearized by the kernel trick (Section 27.4). More specifically, for kernel
function K(x, x′), Lagrange dual problem (38.5) becomes
max
α

n

i=1
αiK(xi, xi) −
n

i, j=1
αiαjK(xi, xj)

subject to 0 ≤αi ≤C
for i = 1,. . . ,n,
n

i=1
αi = 1,

38.2 SUPPORT VECTOR DATA DESCRIPTION
463
n=50; x=randn(n,2); x(:,2)=x(:,2)*4; x(1:20,1)=x(1:20,1)*3;
C=0.04; h=[C*ones(n,1); zeros(n,1); 1; -1];
H=[eye(n); -eye(n); ones(1,n); -ones(1,n)]; x2=sum(x.^2,2);
K=exp(-(repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’));
a=quadprog(K,zeros(n,1),H,h); s=ones(n,1)-2*K*a;
s=s-mean(s(find((0<a)&(a<C)))); u=(s>0.001);
figure(1); clf; hold on; axis equal;
plot(x(:,1),x(:,2),’rx’); plot(x(u,1),x(u,2),’bo’);
FIGURE 38.4
MATLAB code of support vector data description for Gaussian kernel. quadprog.m included
in Optimization Toolbox is required. Free alternatives to quadprog.m are available, e.g. from
http://www.mathworks.com/matlabcentral/fileexchange/.
and outlier criterion (38.6) becomes
K(x, x) −2
n

j=1
αjK(x, xj) −ai > 0,
where, for i such that 0 < αi < C,
ai = K(xi, xi) −2
n

j=1
αjK(xi, xj).
When K(xi, xi) is constant for all i = 1,. . . ,n, which is satisfied, e.g. by the
Gaussian kernel,
K(x, x′) = exp

−∥x −x′∥2
2h2

,
the above optimization problem is simplified as
min
α
n

i, j=1
αiαjK(xi, xj)
subject to 0 ≤αi ≤C
for i = 1,. . . ,n,
n

i=1
αi = 1.
A MATLAB code of support vector data description for the Gaussian kernel is
provided in Fig. 38.4, and its behavior is illustrated in Fig. 38.5. This shows that

464
CHAPTER 38
OUTLIER DETECTION
(a) C = 0.035
(b) C = 0.04
(c) C = 0.042
(d) C = 0.045
FIGURE 38.5
Examples of support vector data description for Gaussian kernel. Circled samples are regarded as
outliers.
results of outlier detection depend on the choice of the trade-off parameter C (and the
Gaussian bandwidth), and appropriately determining these tuning parameters is not
straightforward in practice because of the unsupervised nature of outlier detection.
38.3 INLIER-BASED OUTLIER DETECTION
Since outliers tend to be highly diverse and their tendency may change over time,
it is not easy to directly define outliers. On the other hand, inliers are often stable
and thus indirectly defining outliers as samples that are different from inliers would
be promising. In this section, such inlier-based outlier detection is discussed, under
the assumption that, in addition to the test data set {xi}n
i=1 from which outliers are
detected, another data set {x′
i′}n′
i′=1 that only contains inliers is available.

38.3 INLIER-BASED OUTLIER DETECTION
465
FIGURE 38.6
Inlier-based outlier detection by density ratio estimation. For inlier density p′(x) and test sample
density p(x), the density ratio w(x) = p′(x)/p(x) is close to one when x is an inlier and it is
close to zero when x is an outlier.
A naive approach to inlier-based outlier detection is to estimate the probability
density function p′(x) of inliers {x′
i′}n′
i′=1, and a test sample xi having a low
estimated probability density p′(xi) is regarded as an outlier. However, this naive
approach suffers the same drawback as the density estimation approach introduced in
Section 38.1, i.e., estimating the probability density in low-density regions is difficult.
Here, let us consider the ratio of inlier density p′(x) and test sample density p(x),
w(x) = p′(x)
p(x) ,
which is close to one when x is an inlier and it is close to zero when x is an outlier
(Fig. 38.6). Thus, the density ratio would be a suitable inlier score.
The density ratio w(x) = p′(x)/p(x) can be estimated by estimating p(x) and
p′(x) separately from {xi}n
i=1 and {x′
i′}n′
i′=1 and taking their ratio. However, division
by an estimated density magnifies the estimation error and thus is not a reliable
approach. Here, a method to directly estimate the density ratio w(x) = p′(x)/p(x)
without density estimation of p(x) and p′(x), called KL density ratio estimation
[76, 104], is introduced.

466
CHAPTER 38
OUTLIER DETECTION
n=50; x=randn(n,1); y=randn(n,1); y(n)=5;
x2=x.^2; xx=repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’;
y2=y.^2; yx=repmat(y2,1,n)+repmat(x2’,n,1)-2*y*x’;
m=5; u=mod(randperm(n),m)+1; v=mod(randperm(n),m)+1;
hhs=2*[1 5 10].^2;
for hk=1:length(hhs)
hh=hhs(hk); k=exp(-xx/hh); r=exp(-yx/hh);
for i=1:m
a=KLIEP(k(u~=i,:),r(v~=i,:));
g(hk,i)=mean(r(u==i,:)*a-mean(log(k(u==i,:)*a)));
end, end
[gh,ggh]=min(mean(g,2)); HH=hhs(ggh);
k=exp(-xx/HH); r=exp(-yx/HH); s=r*KLIEP(k,r);
figure(1); clf; hold on; plot(y,s,’rx’);
function a=KLIEP(k,r)
a0=rand(size(k,2),1); b=mean(r)’; n=size(k,1);
for o=1:10000
a=a0-0.001*(b-k’*(1./(k*a0))/n); %a=max(0,a);
if norm(a-a0)<0.001, break, end
a0=a;
end
FIGURE 38.7
MATLAB code of KL density ratio estimation for Gaussian kernel model with Gaussian
bandwidth chosen by cross validation. The bottom function should be saved as “KLIEP.m.”
More specifically, let us use a linear-in-parameter model for approximating the
density ratio w(x) = p′(x)/p(x):
wα(x) =
b

j=1
αjψj(x) = α⊤ψ(x),
where the basis functions {ψj(x)}b
j=1 are assumed to be non-negative. Since
wα(x)p(x) can be regarded as a model of p′(x), the parameter α is learned so that
wα(x)p(x) is as close to p′(x) as possible.
For this model matching, let us employ the generalized KL divergence for non-
negative functions f and g that are not necessarily integrated to 1:
gKL( f ∥g) =

f (x) log f (x)
g(x) dx −

f (x)dx +

g(x)dx.

38.3 INLIER-BASED OUTLIER DETECTION
467
(a) Inliers and test samples
(b) Estimated density ratio
FIGURE 38.8
Example of KL density ratio estimation for Gaussian kernel model.
When f and g are normalized, the above generalized KL divergence is reduced
to the ordinary KL divergence since the second and third terms vanish. Under the
generalized KL divergence, the parameter α is learned to minimize
gKL(p′∥wαp) =

p′(x) log
p′(x)
wα(x)p(x)dx −1 +

wα(x)p(x)dx.
Approximating the expectations by the sample averages and ignoring irrelevant
constants yield the following optimization problem:
min
α

1
n
n

i=1
wα(xi) −1
n′
n′

i′=1
log wα(x′
i′)

.
This is a convex optimization problem and thus the global optimal solution can be
easily obtained, e.g. by a gradient method.
A critical drawback of the local outlier factor and support vector data description
explained in the previous sections is that there is no objective model selection
method. Thus, tuning parameters should be selected subjectively based on some prior
knowledge. On the other hand, KL density ratio estimation allows objective model
selection by cross validation (see Section 14.4 and Section 16.4.2) in terms of the KL
divergence. This is practically a significant advantage in outlier detection.
A MATLAB code of KL density ratio estimation for the Gaussian kernel model,
wα(x) =
n′

j=1
αj exp *
,
−
∥x −x′
j∥2
2h2
+
-
,
(38.7)
is provided in Fig. 38.7, where the Gaussian bandwidth h is chosen by cross
validation. Its behavior is illustrated in Fig. 38.8, showing that a sample at x = 5,

468
CHAPTER 38
OUTLIER DETECTION
which is isolated from other samples, takes the lowest density ratio value. Thus, it is
regarded as the most plausible point to be an outlier.
A possible variation of KL density ratio estimation is to impose non-negativity
α ≥0 when the generalized KL divergence is minimized. This additional non-
negativity constraint guarantees that a learned density ratio function wα(x) is
non-negative. Another useful property brought by this non-negativity constraint is
that the solution of α tends to be sparse.
Implementing this non-negativity idea in MATLAB is straightforward just by
rounding up negative parameter values to zero in each gradient step, as described
in Fig. 38.7.

CHAPTER
CHANGE DETECTION 39
CHAPTER CONTENTS
Distributional Change Detection .................................................. 469
KL Divergence.............................................................. 470
Pearson Divergence ........................................................ 470
L2-Distance ................................................................ 471
L1-Distance ................................................................ 474
Maximum Mean Discrepancy (MMD)....................................... 476
Energy Distance ............................................................ 477
Application to Change Detection in Time Series ........................... 477
Structural Change Detection ..................................................... 478
Sparse MLE ................................................................ 478
Sparse Density Ratio Estimation ........................................... 482
The objective of change detection is to investigate whether change exists between
two data sets {xi}n
i=1 and {x′
i′}n′
i′=1. In this chapter, two statistical approaches to
change detection, distributional change detection and structural change detection,
are explored. Below, {xi}n
i=1 and {x′
i′}n′
i′=1 are assumed to be drawn independently
from the probability distributions with densities p(x) and p′(x), respectively.
39.1 DISTRIBUTIONAL CHANGE DETECTION
Distributional change detection is aimed at identifying change in probability dis-
tributions behind {xi}n
i=1 and {x′
i′}n′
i′=1. This can be achieved by estimating a
distance or a divergence between p(x) and p′(x). As explained in Section 14.2, a
distance satisfies four conditions: non-negativity, symmetry, identity, and the triangle
inequality. On the other hand, a divergence is a pseudodistance that still acts
like a distance, but it may violate some of the above conditions. In this section,
divergence and distance measures between probability distributions that are useful
for change detection and their approximators from samples {xi}n
i=1 and {x′
i′}n′
i′=1 are
introduced.
An Introduction to Statistical Machine Learning. DOI: 10.1016/B978-0-12-802121-7.00050-9
Copyright © 2016 by Elsevier Inc. All rights of reproduction in any form reserved.
469

470
CHAPTER 39
CHANGE DETECTION
39.1.1 KL DIVERGENCE
The most popular divergence measure in statistics and machine learning would be
the KL divergence (see Section 14.2), because of its compatible with MLE (see
Chapter 12):
KL(p∥p′) =

p(x) log p(x)
p′(x)dx.
Due to the log function which is sharp near zero, the KL divergence may allow
sensitive detection of small change in probability distributions. Another advantage
of the KL divergence is that it is invariant under input metric change, i.e., the value
of the KL divergence does not change even if x is transformed to x by any mapping
[5].
The KL divergence can be approximated from samples {xi}n
i=1 and {x′
i′}n′
i′=1 by
the direct density ratio estimator introduced in Section 38.3. More specifically, an
estimator w(x) of the density ratio function,
w(x) = p(x)
p′(x),
can be obtained by KL density ratio estimation. Then, the KL divergence can be
immediately approximated as

KL = 1
n
n

i=1
log w(xi).
However, due to the log function, estimation of the KL divergence is prone to be
sensitive to outliers.
39.1.2 PEARSON DIVERGENCE
Ali-Silvey-Csiszár divergences [2, 35], which is also known as f -divergences, are
generalization of the KL divergence defined as
F(p∥p′) =

p′(x) f
p(x)
p′(x)

dx,
where f (t) is a convex function (see Fig. 8.3) such that f (1) = 0. It can be easily
confirmed that, with f (t) = t log t, the Ali-Silvey-Csiszár divergence is reduced to
the KL divergence. Note that all Ali-Silvey-Csiszár divergences are invariant under
input metric change.
The Pearson divergence [79] is a squared-loss variant of the KL divergence
defined as the Ali-Silvey-Csiszár divergence with the squared function f (t) = (t−1)2:
PE(p∥p′) =

p′(x)
p(x)
p′(x) −1
2
dx.

39.1 DISTRIBUTIONAL CHANGE DETECTION
471
Since the Pearson divergence does not include the log function, it would be more
robust against outliers. However, it still includes the density ratio function p(x)/p′(x),
which tends to be a sharp function and is possibly unbounded. Therefore, its accurate
approximation is not straightforward in practice. As discussed in Section 33.2.2, this
problem can be mitigated by considering the relative density ratio for β ∈[0,1]:
wβ(x) =
p(x)
βp(x) + (1 −β)p′(x).
The Pearson divergence extended using the relative density ratio is called the relative
Pearson divergence [121]:
rPE(p∥p′) = PE(p∥βp + (1 −β)p′)
=

βp(x) + (1 −β)p′(x)

wβ(x) −1
2dx.
The relative Pearson divergence can be approximated from samples {xi}n
i=1 and
{x′
i′}n′
i′=1 by the direct relative density ratio estimator introduced in Section 33.2.4.
More specifically, an estimator wβ(x) of the relative density ratio function wβ(x)
can be obtained by LS relative density ratio estimation. Then, the relative Pearson
divergence can be approximated as

rPE = 1
n
n

i=1
wβ(xi) −1,
which comes from the following expression of the relative Pearson divergence:
rPE(p∥p′) =

p(x)wβ(x)dx −1.
The tuning parameter β ∈[0,1] controls the trade-off between sensitivity and
robustness of the divergence measure, which should be appropriately chosen in
practice.
39.1.3 L2-DISTANCE
The L2-distance is another standard distance measure between probability distribu-
tions:
L2(p,p′) =

f (x)2dx,
where
f (x) = p(x) −p′(x).
The L2-distance is a proper distance measure, and thus it is symmetric and satisfies
the triangle inequality unlike the KL divergence and the (relative) Pearson diver-
gence. Furthermore, the density difference f (x) is always bounded as long as each

472
CHAPTER 39
CHANGE DETECTION
density is bounded. Therefore, the L2-distance is stable, without the need of tuning
any control parameter such as β in the relative Pearson divergence.
The L2-distance can be approximated from samples {xi}n
i=1 and {x′
i′}n′
i′=1 by
LS density difference estimation [103] that directly estimates the density difference
function f (x) without estimating p(x) and p′(x) (see Section 37.4 for direct
estimation of p(x, y) −p(x)p(y)). More specifically, let us consider the following
Gaussian density difference model:
fα(x) =
n+n′

j=1
αj exp *
,
−∥x −cj∥2
2h2
+
-
,
where h > 0 denotes the Gaussian bandwidth and
(c1,. . . , cn, cn+1,. . . , cn+n′) = (x1,. . . , xn, x′
1,. . . , x′
n′)
are the Gaussian centers. The parameters α = (α1,. . . ,αn+n′)⊤are estimated by LS
fitting to the true density difference function f (x):
min
α

fα(x) −f (x)
2dx.
Its empirical criterion where an irrelevant constant is ignored and the expectation is
approximated by the sample average is given by
min
α

α⊤Uα −2α⊤v + λ∥α∥2
,
where the ℓ2-regularizer λ∥α∥2 is included. U is the (n + n′) × (n + n′) matrix with
the (j, j′)th element defined by
Uj, j′ =

exp *
,
−∥x −cj∥2
2h2
+
-
exp *
,
−∥x −cj′∥2
2h2
+
-
dx
= (πh2)d/2 exp *
,
−∥cj −cj′∥2
4h2
+
-
,
where d denotes the dimensionality of x. v is the (n + n′)-dimensional vector with
the jth element defined by
vj = 1
n
n

i=1
exp *
,
−∥xi −cj∥2
2h2
+
-
−1
n′
n′

i′=1
exp *
,
−∥x′
i′ −cj∥2
2h2
+
-
.
This is a convex optimization problem, and the global optimal solution α can be
obtained analytically as
α = (U + λI)−1v.

39.1 DISTRIBUTIONAL CHANGE DETECTION
473
n=200; x=randn(n,1); y=randn(n,1)+1;
hhs=2*[0.5 1 3].^2; ls=10.^[-2 -1 0]; m=5;
x2=x.^2; xx=repmat(x2,1,n)+repmat(x2’,n,1)-2*x*x’;
y2=y.^2; yx=repmat(y2,1,n)+repmat(x2’,n,1)-2*y*x’;
u=mod(randperm(n),m)+1; v=mod(randperm(n),m)+1;
for hk=1:length(hhs)
hh=hhs(hk); k=exp(-xx/hh); r=exp(-yx/hh);
U=(pi*hh/2)^(1/2)*exp(-xx/(2*hh));
for i=1:m
vh=mean(k(u~=i,:))’-mean(r(v~=i,:))’;
z=mean(k(u==i,:))-mean(r(v==i,:));
for lk=1:length(ls)
l=ls(lk); a=(U+l*eye(n))\vh; g(hk,lk,i)=a’*U*a-2*z*a;
end, end, end
[gl,ggl]=min(mean(g,3),[],2); [ghl,gghl]=min(gl);
L=ls(ggl(gghl)); HH=hhs(gghl);
k=exp(-xx/HH); r=exp(-yx/HH); vh=mean(k)’-mean(r)’;
U=(pi*HH/2)^(1/2)*exp(-xx/(2*HH));
a=(U+L*eye(n))\vh; s=[k;r]*a; L2=a’*vh;
figure(1); clf; hold on; plot([x;y],s,’rx’);
FIGURE 39.1
MATLAB code for LS density difference estimation.
The Gaussian width h and the regularization parameter λ may be optimized by cross
validation with respect to the squared error criterion.
Finally, the L2-distance can be approximated by
L2 = 1
n
n

i=1
f α(xi) −1
n′
n′

i′=1
f α(x′
i′) = v⊤α,
(39.1)
which comes from the following expression of the L2-distance:
L2(p,p′) =

p(x) −p′(x)

f (x)dx.
A MATLAB code for LS density difference estimation is provided in Fig. 39.1,
and its behavior is illustrated in Fig. 39.2. This shows that the density difference
function can be accurately estimated.

474
CHAPTER 39
CHANGE DETECTION
(a) Samples
(b) Densities and density differences
FIGURE 39.2
Example of LS density difference estimation. ×’s in the right plot show estimated density
difference values at {xi}n
i=1 and {x′
i′}n′
i′=1.
39.1.4 L1-DISTANCE
The L2-distance can be generalized to the class of Ls-distances for s > 0:
Lt(p,p′) =

p(x) −p′(x)
sdx.
Among the class of Ls-distances, the L1-distance is also a member of the class of
the Ali-Silvey-Csiszár divergences with the absolute function f (t) = |t −1| (see
Section 39.1.2):
L1(p,p′) =

p(x) −p′(x)dx =

p′(x)

p(x)
p′(x) −1

dx.
This implies that the L1-distance is also invariant under input metric change.
Approximation of the L1-distance from samples {xi}n
i=1 and {x′
i′}n′
i′=1 can
actually be performed by the (weighted) support vector machine introduced in
Chapter 27 [75]. More specifically, since |t| = sign (t) t, the L1-distance can be
expressed as
L1(p,p′) =

sign

p(x) −p′(x)

p(x) −p′(x)

dx,
where sign(t) denotes the sign function:
sign(t) =

1
(t > 0),
0
(t = 0),
−1
(t < 0).

39.1 DISTRIBUTIONAL CHANGE DETECTION
475
FIGURE 39.3
Lower bound of sign (t) by −2 max(0,1−t)+1.
For a density difference model fα(x) with parameter α, the L1-distance L1(p,p′) can
be lower-bounded as
L1(p,p′) ≥max
α

sign

fα(x)

p(x) −p′(x)

dx
= max
α

sign

fα(x)

p(x)dx +

sign

−fα(x)

p′(x)dx

,
where the last term is due to sign(t) = −sign(−t). As plotted in Fig. 39.3, the sign
function can be lower-bounded by
−2 max(0,1 −t) + 1 =

1
(t > 1),
2t −1
(t ≤1).
Based on this, the L1-distance L1(p,p′) can be further lower-bounded as
L1(p,p′) ≥2 −2 min
α

max

0,1 −fα(x)

p(x)dx
+

max

0,1 + fα(x)

p′(x)dx

.
Let us employ a linear-in-parameter density difference model:
fα(x) =
b

j=1
αjψj(x) = α⊤ψ(x).
Then the empirical version of the above maximization problem (without irrelevant
multiplicative and additive constants) is given by
min
α

1
n
n

i=1
max

0,1 −α⊤ψ(xi)

+ 1
n′
n′

i′=1
max

0,1 + α⊤ψ(x′
i′)

.

476
CHAPTER 39
CHANGE DETECTION
Let us assign class labels yi = +1 to xi for i = 1,. . . ,n and y′
i′ = −1 to x′
i′
for i′ = 1,. . . ,n′. If n = n′, the above optimization problem agrees with hinge loss
minimization (see Section 27.6):
min
α

n

i=1
max

0,1 −yiα⊤ψ(xi)

+
n′

i′=1
max

0,1 −y′
i′α⊤ψ(x′
i′)

.
If n , n′, L1-distance approximation corresponds to weighted hinge loss minimiza-
tion with weight 1/n for {xi}n
i=1 and 1/n′ for {x′
i′}n′
i′=1 .
The above formulation shows that the support vector machine is actually approx-
imating the sign of the density difference. More specifically, let p+(x) and p−(x) be
the probability density functions of samples in the positive class and negative class,
respectively. Then, the support vector machine approximates
sign

p+(x) −p−(x)

,
which is the optimal decision function. Thus, support vector classification can
be interpreted as directly approximating the optimal decision function without
estimating the densities p+(x) and p−(x).
39.1.5 MAXIMUM MEAN DISCREPANCY (MMD)
MMD [17] measures the distance between embeddings of probability distributions in
a reproducing kernel Hilbert space [9].
More specifically, the MMD between p and p′ is defined as
MMD(p,p′) = Ex,x∼p[K(x,x)] + Ex′,x′∼p′[K(x′,x′)]
−2Ex∼p,x′∼p′[K(x, x′)],
where K(x, x′) is a reproducing kernel, Ex∼p denotes the expectation with respect to
x following density p. MMD(p,p′) is always non-negative, and MMD(p,p′) = 0 if
and only if p = p′ when K(x, x′) is a characteristic kernel [45] such as the Gaussian
kernel.
An advantage of MMD is that it can be directly approximated using samples as
1
n2
n

i,i=1
K(xi, xi) + 1
n′2
n′

i′,i′=1
K(x′
i′, x′
i′) −2
nn′
n

i=1
n′

i′=1
K(xi, x′
i′).
Thus, no estimation is involved when approximating MMD from samples. However,
it is not clear how to choose kernel functions in practice. Using the Gaussian kernel
with bandwidth set at the median distance between samples is a popular heuristic
[49], but this does not always work well in practice [50].

39.1 DISTRIBUTIONAL CHANGE DETECTION
477
39.1.6 ENERGY DISTANCE
Another useful distance measure is the energy distance [106] introduced in
Section 33.3.2:
DE(p,p′) =

Rd ∥ϕp(t) −ϕp′(t)∥2 *.
,
π
d+1
2
Γ
d+1
2
∥t∥d+1+/
-
−1
dt,
where ∥· ∥denotes the Euclidean norm, ϕp denotes the characteristic function (see
Section 2.4.3) of p, Γ(·) is the gamma function (see Section 4.3), and d denotes the
dimensionality of x.
An important property of the energy distance is that it can be expressed as
DE(p,p′) = 2Ex∼p,x′∼p′∥x −x′∥−Ex,x∼p∥x −x∥−Ex′,x′∼p′∥x′ −x′∥,
where Ex∼p denotes the expectation with respect to x following density p. This can
be directly approximated using samples as
2
nn′
n

i=1
n′

i′=1
∥xi −x′
i′∥−1
n2
n

i,i=1
∥xi −xi∥−1
n′2
n′

i′,i′=1
∥x′
i′ −x′
i′∥.
Thus, no estimation and no tuning parameters are involved when approximating the
energy distance from samples, which is a useful properties in practice.
Actually, it was shown [91] that the energy distance is a special case of MMD.
Indeed, MMD with kernel function defined as
K(x, x′) = −∥x −x′∥+ ∥x∥+ ∥x′∥
agrees with the energy distance.
39.1.7 APPLICATION TO CHANGE DETECTION IN TIME
SERIES
Let us consider the problem of change detection in time series (Fig. 39.4(a)). More
specifically, given time series samples {yi}N
i=1, the objective is to identify whether
change in probability distributions exists between yt and yt+1 for some t. This
problem can be tackled by estimating a distance (or a divergence) between the
probability distributions of {yi}t
i=t−n+1 and {yi}t+n
i=t+1.
A challenge in change detection in time series is that samples {yi}N
i=1 are
often dependent over time, which violates the presumption in this chapter. A
practical approach to mitigate this problem is to vectorize data [60], as illustrated in
Fig. 39.4(b). That is, instead of handling time series sample yi as it is, its vectorization
with k consecutive samples xi = (yi,. . . , yi+k−1)⊤is considered, and a distance (or a
divergence) is estimated from D = {xi}t
i=t−n+1 and D′ = {xi}t+n
i=t+1.
A MATLAB code for change detection in time series based on the energy distance
is provided in Fig. 39.5, and its behavior is illustrated in Fig. 39.6. This shows that
the energy distance well captures the distributional change in time series.

478
CHAPTER 39
CHANGE DETECTION
(a) Time series samples
(b) Vectorization to reduce time dependency
FIGURE 39.4
Change detection in time series.
39.2 STRUCTURAL CHANGE DETECTION
Distributional change detection introduced in the previous section focused on
investigating whether change exists in probability distributions. The aim of structural
change detection introduced in this section is to analyze change in the dependency
structure between elements of d-dimensional variable x = (x(1),. . . , x(d))⊤.
39.2.1 SPARSE MLE
Let us consider a Gaussian Markov network, which is a d-dimensional Gaussian
model with expectation zero (Section 6.2):
q(x; Θ) = det(Θ)1/2
(2π)d/2 exp

−1
2 x⊤Θx

,
where not the variance-covariance matrix, but its inverse called the precision matrix
is parameterized by Θ. If Θ is regarded as an adjacency matrix, the Gaussian Markov
network can be visualized as a graph (see Fig. 39.7). An advantage of this precision-
based parameterization is that the connectivity governs conditional independence.

39.2 STRUCTURAL CHANGE DETECTION
479
N=300; k=5; n=10; m=N-k+1; E=nan(1,N);
y=zeros(1,N); y(101:200)=3; y=y+randn(1,N);
%y=sin([1:N]/2); y(101:200)=sin([101:200]);
x=toeplitz(y); x=x(1:k,1:m); x2=sum(x.^2);
D=sqrt(repmat(x2’,1,m)+repmat(x2,m,1)-2*x’*x);
for t=n:N-n-k+1
a=[t-n+1:t]; b=[t+1:t+n];
E(t)=2*mean(mean(D(a,b)))-mean(mean(D(a,a))) ...
-mean(mean(D(b,b)));
end
figure(1); clf; hold on; plot(y,’b-’); plot(E,’r--’);
legend(’Time series’,’Energy distance’)
FIGURE 39.5
MATLAB code for change detection in time series based on the energy distance.
For example, in the Gaussian Markov network illustrated in the left-hand side of
Fig. 39.7, x(1) and x(2) are connected via x(3). This means that x(1) and x(2) are
conditionally independent given x(3).
Suppose that {xi}n
i=1 and {x′
i′}n′
i′=1 are drawn independently from the Gaussian
Markov networks with precision matrices Θ and Θ′, respectively. Then analyzing
Θ −Θ′ allows us to identify the change in Markov network structure (see Fig. 39.7
again).
A sparse estimate of Θ may be obtained by MLE with the ℓ1-constraint (see
Chapter 24):
max
Θ
n

i=1
log q(xi; Θ) subject to ∥Θ∥1 ≤R2,
where R ≥0 is the radius of the ℓ1-ball. This method is also referred to as the
graphical lasso [44].
The derivative of log q(x; Θ) with respect to Θ is given by
∂log q(x; Θ)
∂Θ
= 1
2Θ−1 −1
2 xx⊤,
where the following formulas are used for its derivation:
∂log det(Θ)
∂Θ
= Θ−1 and
∂x⊤Θx
∂Θ
= xx⊤.
A MATLAB code of a gradient-projection algorithm of ℓ1-constraint MLE for
Gaussian Markov networks is given in Fig. 39.8, where projection onto the ℓ1-ball
is computed by the method developed in [39].

480
CHAPTER 39
CHANGE DETECTION
(a) Bias change
(b) Frequency change
FIGURE 39.6
Examples of change detection in time series based on the energy distance.
FIGURE 39.7
Structural change in Gaussian Markov networks.
For the true precision matrices
Θ =
*.....
,
2 0 1
0 2 0
1 0 2
+/////
-
and Θ′ =
*.....
,
2 0 0
0 2 0
0 0 2
+/////
-
,

39.2 STRUCTURAL CHANGE DETECTION
481
TT=[2 0 1; 0 2 0; 1 0 2];
%TT=[2 0 0; 0 2 0; 0 0 2];
%TT=[2 1 0; 1 2 1; 0 1 2];
%TT=[2 0 1; 0 2 1; 1 1 2];
d=3; n=50; x=TT^(-1/2)*randn(d,n); S=x*x’/n;
T0=eye(d); C=5; e=0.1;
for o=1:100000
T=T0+e*(inv(T0)-S);
T(:)=L1BallProjection(T(:),C);
if norm(T-T0)<0.00000001, break, end
T0=T;
end
T, TT
function w=L1BallProjection(x,C)
u=sort(abs(x),’descend’); s=cumsum(u);
r=find(u>(s-C)./(1:length(u))’,1,’last’);
w=sign(x).*max(0,abs(x)-max(0,(s(r)-C)/r));
FIGURE 39.8
MATLAB code of a gradient-projection algorithm of ℓ1-constraint MLE for Gaussian Markov
networks. The bottom function should be saved as “L1BallProjection.m.”
sparse MLE gives
Θ =
*.....
,
1.382
0
0.201
0
1.788
0
0.201
0
1.428
+/////
-
and
Θ
′ =
*.....
,
1.617
0
0
0
1.711
0
0
0
1.672
+/////
-
.
Thus, the true sparsity patterns of Θ and Θ′ (in off-diagonal elements) can be
successfully recovered. Since
Θ −Θ′ =
*.....
,
0 0 1
0 0 0
1 0 0
+/////
-
and
Θ −Θ
′ =
*.....
,
−0.235
0
0.201
0
0.077
0
0.201
0
−0.244
+/////
-
,
change in sparsity patterns (in off-diagonal elements) can be correctly identified.

482
CHAPTER 39
CHANGE DETECTION
On the other hand, when the true precision matrices are
Θ =
*.....
,
2 1 0
1 2 1
0 1 2
+/////
-
and Θ′ =
*.....
,
2 0 1
0 2 1
1 1 2
+/////
-
,
sparse MLE gives
Θ =
*.....
,
1.303 0.348
0
0.348 1.157 0.240
0
0.240 1.365
+/////
-
and
Θ
′ =
*.....
,
1.343
0
0.297
0
1.435 0.236
0.297 0.236 1.156
+/////
-
.
Thus, the true sparsity patterns of Θ and Θ′ can still be successfully recovered.
However, since
Θ −Θ′ =
*.....
,
0 1 −1
1 0 0
−1 0 0
+/////
-
and
Θ −Θ
′ =
*.....
,
−0.040
0.348 −0.297
0.348 −0.278
0.004
−0.297
0.004
0.209
+/////
-
,
change in sparsity patterns was not correctly identified (although 0.004 is reasonably
close to zero). This shows that, when a nonzero unchanged edge exists, say Θk,k′ =
Θ′
k,k′ > 0 for some k and k′, it is difficult to identify this unchanged edge because
Θk,k′ ≈Θ′
k,k′ does not necessarily hold by separate sparse MLE from {xi}n
i=1 and
{x′
i′}n′
i′=1.
39.2.2 SPARSE DENSITY RATIO ESTIMATION
As illustrated above, sparse MLE can perform poorly in structural change detection.
Another limitation of sparse MLE is the Gaussian assumption. A Gaussian Markov
network can be extended to a non-Gaussian model as
q(x; θ) =
q(x; θ)

q(x; θ)dx ,
where, for a feature vector f (x, x′),
q(x; θ) = exp *
,

k ≥k′
θ⊤
k,k′ f (x(k), x(k′))+
-
.
This model is reduced to the Gaussian Markov network if
f (x, x′) = −1
2 xx′,

39.2 STRUCTURAL CHANGE DETECTION
483
while higher-order correlations can be captured by considering higher-order terms in
the feature vector. However, applying sparse MLE to non-Gaussian Markov networks
is not straightforward in practice because the normalization term

q(x; θ)dx is often
computationally intractable.
To cope with these limitations, let us handle the change in parameters, θk,k′−θ′
k,k′,
directly via the following density ratio function:
q(x; θ)
q(x; θ′) ∝exp *
,

k ≥k′
(θk,k′ −θ′
k,k′)⊤f (x(k), x(k′))+
-
.
Based on this expression, let us consider the following density ratio model:
r(x; α) =
exp *
,

k ≥k′
α⊤
k,k′ f (x(k), x(k′))+
-

p′(x) exp *
,

k ≥k′
α⊤
k,k′ f (x(k), x(k′))+
-
dx
,
(39.2)
where αk,k′ is the difference of parameters:
αk,k′ = θk,k′ −θ′
k,k′.
p′(x) in the denominator of Eq.(39.2) comes from the fact that r(x; α) approximates
p(x)/p′(x) and thus the normalization constraint,

r(x; α)p′(x)dx = 1,
is imposed.
Let us learn the parameters {αk,k′}k ≥k′ by a group-sparse variant (see
Section 24.4.4) of KL density ratio estimation explained in Section 38.3 [69]:
min
{αk,k′}k≥k′ log 1
n′
n′

i′=1
exp *
,

k ≥k′
α⊤
k,k′ f (x′(k)
i′ , x′(k′)
i′
)+
-
−1
n
n

i=1

k ≥k′
α⊤
k,k′ f (x(k)
i , x(k′)
i
)
subject to

k ≥k′
∥αk,k′∥≤R2,
where R ≥0 controls the sparseness of the solution.
A MATLAB code of a gradient-projection algorithm of sparse KL density ratio
estimation for Gaussian Markov networks is given in Fig. 39.9. For the true precision
matrices
Θ −Θ′ =
*.....
,
2 0 1
0 2 0
1 0 2
+/////
-
−
*.....
,
2 0 0
0 2 0
0 0 2
+/////
-
=
*.....
,
0 0 1
0 0 0
1 0 0
+/////
-
,

484
CHAPTER 39
CHANGE DETECTION
Tp=[2 0 1; 0 2 0; 1 0 2]; Tq=[2 0 0; 0 2 0; 0 0 2];
Tp=[2 1 0; 1 2 1; 0 1 2]; Tq=[2 0 1; 0 2 1; 1 1 2];
d=3; n=50; xp=Tp^(-1/2)*randn(d,n); Sp=xp*xp’/n;
xq=Tq^(-1/2)*randn(d,n); A0=eye(d); C=1; e=0.1;
for o=1:1000000
U=exp(sum((A0*xq).*xq));
A=A0-e*((repmat(U,[d 1]).*xq)*xq’/sum(U)-Sp);
A(:)=L1BallProjection(A(:),C);
if norm(A-A0)<0.00000001, break, end
A0=A;
end
-2*A, Tp-Tq
FIGURE 39.9
MATLAB code of a gradient-projection algorithm of ℓ1-constraint KL density ratio estimation
for Gaussian Markov networks. “L1BallProjection.m” is given in Fig. 39.8.
sparse KL density ratio estimation gives
*.....
,
0
0 1.000
0
0
0
1.000 0
0
+/////
-
.
This implies that the change in sparsity patterns can be correctly identified.
Even when nonzero unchanged edges exist as
Θ −Θ′ =
*.....
,
2 1 0
1 2 1
0 1 2
+/////
-
−
*.....
,
2 0 1
0 2 1
1 1 2
+/////
-
=
*.....
,
0 1 −1
1 0 0
−1 0 0
+/////
-
,
sparse KL density ratio estimation gives
*.....
,
0
0.707 −0.293
0.707
0
0
−0.293
0
0
+/////
-
.
Thus, the change in Markov network structure can still be correctly identified.

Bibliography
REFERENCES
1. Akaike H. A new look at the statistical model identification. IEEE Trans Automat Control 1974;AC-
19(6):716–23.
2. Ali SM, Silvey SD. A general class of coefficients of divergence of one distribution from another. J
Roy Statist Soc Ser B 1966;28(1):131–42.
3. Aloise D, Deshpande A, Hansen P, Popat P. NP-hardness of Euclidean sum-of-squares clustering.
Mach Learn 2009;75(2):245–9.
4. Amari S. Theory of adaptive pattern classifiers. IEEE Trans Electron Comput 1967;EC-16(3):299–
307.
5. Amari S, Nagaoka H. Methods of information geometry. Providence (RI, USA): Oxford University
Press; 2000.
6. Amit Y, Fink M, Srebro N, Ullman S. Uncovering shared structures in multiclass classification. In:
Ghahramani Z, editor. Proceedings of the 24th annual international conference on machine learning.
Omnipress; 2007. p. 17–24.
7. Anderson TW. An introduction to multivariate statistical analysis. 2nd ed. New York (NY, USA):
Wiley; 1984.
8. Argyriou A, Evgeniou T, Pontil M. Convex multi-task feature learning. Mach Learn 2008;73(3):243–
72.
9. Aronszajn N. Theory of reproducing kernels. Trans Amer Math Soc 1950;68:337–404.
10. Auer P. Using confidence bounds for exploitation-exploration trade-offs. J Mach Learn Res
2002;3:397–422.
11. Bach FR, Lanckriet GRG, Jordan MI. Multiple kernel learning, conic duality, and the SMO algorithm.
In: Proceedings of the twenty-first international conference on machine learning. New York (NY,
USA): ACM Press; 2004. p. 6–13.
12. Bartlett P, Pereira FCN, Burges CJC, Bottou L, Weinberger KQ, editors. Practical Bayesian
optimization of machine learning algorithms; 2012.
13. Belkin M, Niyogi P. Laplacian eigenmaps for dimensionality reduction and data representation.
Neural Comput 2003;15(6):1373–96.
14. Bengio Y. Learning deep architectures for AI. Found Trends Mach Learn 2009;1(2):1–127.
15. Bishop CM. Pattern recognition and machine learning. New York (NY, USA): Springer; 2006.
16. Blei DM, Ng AY, Jordan MI. Latent Dirichlet allocation. J Mach Learn Res 2003;3:993–1022.
17. Borgwardt KM, Gretton A, Rasch MJ, Kriegel H-P, Schölkopf B, Smola AJ. Integrating structured
biological data by kernel maximum mean discrepancy. Bioinformatics 2006;22(14):e49–57.
18. Boser BE, Guyon IM, Vapnik VN. A training algorithm for optimal margin classifiers. In: Haussler
D, editor. Proceedings of the fifth annual ACM workshop on computational learning theory. ACM
Press; 1992. p. 144–52.
19. Boucheron S, Lugosi G, Bousquet O. Concentration inequalities. In: Bousquet O, von Luxburg
U, Rätsch G, editors. Advanced lectures on machine learning. Lecture notes in computer science,
vol. 3176. Berlin (Heidelberg): Springer; 2004. p. 208–40.
20. Boyd S, Parikh N, Chu E, Peleato B, Eckstein J. Distributed optimization and statistical learning via
the alternating direction method of multipliers. Found Trends Mach Learn 2011;3(1):1–122.
21. Breiman L. Bagging predictors. Mach Learn 1996;26(2):123–40.
22. Breiman L. Random forests. Mach Learn 2001;45(1):5–32.
485

486
Bibliography
23. Breunig MM, Kriegel H-P, Ng RT, Sander J. LOF: identifying density-based local outliers. In: Chen
W, Naughton JF, Bernstein PA, editors. Proceedings of the ACM SIGMOD international conference
on management of data; 2000. p. 93–104.
24. Caruana R. Multitask learning. Mach Learn 1997;28:41–75.
25. Chang CC, Lin CJ. LIBSVM: a library for support vector machines. Technical report, Department
of
Computer
Science,
National
Taiwan
University,
http://www.csie.ntu.edu.tw/˜cjlin/
libsvm/; 2001.
26. Chang C-C, Tsai H-C, Lee Y-J. A minimum enclosing balls labeling method for support vector
clustering. Technical report, National Taiwan University of Science and Technology, 2007.
27. Chang W-C, Lee C-P, Lin C-J. A revisit to support vector data description. Technical report, National
Taiwan University, 2013.
28. Chapelle O, Schölkopf B, Zien A, editors. Semi-supervised learning. Cambridge (MA, USA): MIT
Press; 2006.
29. Chung FRK. Spectral graph theory. Providence (RI, USA): American Mathematical Society; 1997.
30. Cortes C, Vapnik V. Support-vector networks. Mach Learn 1995;20:273–97.
31. Cramér H. Mathematical methods of statistics. Princeton (NJ, USA): Princeton University Press;
1946.
32. Crammer K, Dekel O, Keshet J, Shalev-Shwartz S, Singer Y. Online passive-aggressive algorithms.
J Mach Learn Res 2006;7(March):551–85.
33. Crammer K, Kulesza A, Dredze M. Adaptive regularization of weight vectors. In: Bengio Y,
Schuurmans D, Lafferty J, Williams CKI, Culotta A, editors. Advances in neural information
processing systems, vol. 22. 2009. p. 414–22.
34. Crammer K, Singer Y. On the algorithmic implementation of multiclass kernel-based vector
machines. J Mach Learn Res 2001;2:265–92.
35. Csiszár I. Information-type measures of difference of probability distributions and indirect observa-
tion. Studia Sci Math Hungar 1967;2:229–318.
36. Dempster AP, Laird NM, Rubin DB. Maximum likelihood from incomplete data via the EM
algorithm. J Roy Statist Soc Ser B 1977;39(1):1–38.
37. Domingo C, Watanabe O. MadaBoost: a modification of AdaBoost. In: Proceedings of the thirteenth
annual conference on computational learning theory; 2000. p. 180–9.
38. du Plessis MC, Sugiyama M. Semi-supervised learning of class balance under class-prior change by
distribution matching. Neural Netw 2014;50:110–9.
39. Duchi J, Shalev-Shwartz S, Singer Y, Chandra T. Efficient projections onto the ℓ1-ball for learning
in high dimensions. In: McCallum A, Roweis S, editors. Proceedings of the 25th annual international
conference on machine learning. Omnipress; 2008. p. 272–9.
40. Evgeniou T, Pontil M. Regularized multi-task learning. In: Proceedings of the tenth ACM SIGKDD
international conference on knowledge discovery and data mining. ACM; 2004. p. 109–17.
41. Fisher RA. The use of multiple measurements in taxonomic problems. Ann Eugenics 1936;7(2):179–
88.
42. Fisher RA. The use of multiple measurements in taxonomic problems. Ann Eugenics 1936;7:179–88.
43. Friedman J, Hastie T, Tibshirani R. Additive logistic regression: a statistical view of boosting. Ann
Statist 2000;28(2):337–407.
44. Friedman J, Hastie T, Tibshirani R. Sparse inverse covariance estimation with the graphical lasso.
Biostatistics 2008;9(3):432–41.
45. Fukumizu K, Sriperumbudur BK, Gretton A, Schölkopf B. Characteristic kernels on groups and
semigroups. In: Koller D, Schuurmans D, Bengio Y, Bottou L, editors. Advances in neural
information processing systems, vol. 21. 2009. p. 473–80.
46. Gärtner T. Kernels for structured data. Singapore: World Scientific; 2008.

Bibliography
487
47. Geman S, Geman D. Stochastic relaxation, Gibbs distributions and the Bayesian restoration of
images. IEEE Trans Pattern Anal Mach Intell 1984;6:721–41.
48. Girolami M. Mercer kernel-based clustering in feature space. IEEE Trans Neural Netw
2002;13(3):780–4.
49. Gretton A, Borgwardt KM, Rasch M, Schölkopf B, Smola AJ. A kernel method for the two-sample-
problem. In: Schölkopf B, Platt J, Hoffman T, editors. Advances in neural information processing
systems, vol. 19. Cambridge (MA, USA): MIT Press; 2007. p. 513–20.
50. Gretton A, Sriperumbudur B, Sejdinovic D, Strathmann H, Balakrishnan S, Pontil M, et al. Optimal
kernel choice for large-scale two-sample tests. In: Bartlett P, Pereira FCN, Burges CJC, Bottou L,
Weinberger KQ, editors. Advances in neural information processing systems, vol. 25. 2012. p. 1214–
22.
51. Griffiths TL, Steyvers M. Finding scientific topics. Proc Natl Acad Sci USA 2004;101:5228–35.
52. Hastings WK. Monte Carlo sampling methods using Markov chains and their applications.
Biometrika 1970;57(1):97–109.
53. He X, Niyogi P. Locality preserving projections. In: Thrun S, Saul L, Schölkopf B, editors. Advances
in neural information processing systems, vol. 16. Cambridge (MA, USA): MIT Press; 2004. p. 153–
60.
54. Hinton GE. Training products of experts by minimizing contrastive divergence. Neural Comput
2002;14(8):1771–800.
55. Hinton GE, Salakhutdinov RR. Reducing the dimensionality of data with neural networks. Science
2006;313(5786):504–7.
56. Hoerl AE, Kennard RW. Ridge regression: biased estimation for nonorthogonal problems. Techno-
metrics 1970;12(3):55–67.
57. Holland PW, Welsch RE. Robust regression using iteratively reweighted least-squares. Comm Statist
Theory Methods 1978;6(9):813–27.
58. Huber PJ. Robust statistics. New York (NY, USA): Wiley; 1981.
59. Jolliffe IT. Principal component analysis. New York (NY, USA): Springer-Verlag; 1986.
60. Kawahara Y, Sugiyama M. Sequential change-point detection based on direct density-ratio estima-
tion. Stat Anal Data Min 2012;5(2):114–27.
61. Kawakubo H, du Plessis M, Sugiyama MC. Coping with class balance change in classification: class-
prior estimation with energy distance. Technical report IBISML2014-71, IEICE, 2014.
62. Knuth DE. Seminumerical algorithms. The art of computer programming. vol. 2 Reading (MA,
USA): Addison-Wesley; 1998.
63. Konishi S, Kitagawa G. Generalized information criteria in model selection. Biometrika
1996;83(4):875–90.
64. Kullback S, Leibler RA. On information and sufficiency. Ann Math Stat 1951;22:79–86.
65. Lafferty J, Mccallum A, Pereira F. Conditional random fields: probabilistic models for segmenting
and labeling sequence data. In: Proceedings of the 18th international conference on machine learning;
2001. p. 282–9.
66. Langford J, Li L, Zhang T. Sparse online learning via truncated gradient. J Mach Learn Res
2009;10:777–801.
67. Li K. Sliced inverse regression for dimension reduction. J Amer Statist Assoc 1991;86(414):316–42.
68. Liu JS. The collapsed Gibbs sampler in Bayesian computations with applications to a gene regulation
problem. J Amer Statist Assoc 1994;89(427):958–66.
69. Liu S, Quinn J, Gutmann MU, Sugiyama M. Direct learning of sparse changes in Markov networks
by density ratio estimation. Neural Comput 2014;26(6):1169–97.
70. Loftsgaarden DO, QuesenBerry CP. A nonparametric estimate of a multivariate density function.
Ann Math Stat 1965;36(3):1049–51.

488
Bibliography
71. Mackay DJC. Information theory, inference, and learning algorithms. Cambridge (UK): Cambridge
University Press; 2003.
72. Metropolis N, Rosenbluth AW, Rosenbluth MN, Teller AH, Teller E. Equations of state calculations
by fast computing machines. J Chem Phys 1953;21(6):1087–92.
73. Mosteller F, Tukey JW, editors. Data analysis and regression. Reading (MA, USA): Addison-Wesley;
1977.
74. Murphy KP. Machine learning: a probabilistic perspective. Cambridge (Massachusetts, USA): MIT
Press; 2012.
75. Nguyen X, Wainwright MJ, Jordan MI. On surrogate loss functions and f -divergences. Ann Statist
2009;37(2):876–904.
76. Nguyen X, Wainwright MJ, Jordan MI. Estimating divergence functionals and the likelihood ratio by
convex risk minimization. IEEE Trans Inform Theory 2010;56(11):5847–61.
77. Orr MJL. Introduction to radial basis function networks. Technical report, Center for Cognitive
Science, University of Edinburgh, 1996.
78. Parikh N, Boyd S. Proximal algorithms. Found Trends Optim 2013;1(3):123–231.
79. Pearson K. On the criterion that a given system of deviations from the probable in the case of a
correlated system of variables is such that it can be reasonably supposed to have arisen from random
sampling. Phil Mag 5 1900;50(302):157–75.
80. Petersen KB, Pedersen MS. The matrix cookbook. Technical report, Technical University of
Denmark, 2012.
81. Quiñonero-Candela J, Sugiyama M, Schwaighofer A, Lawrence N, editors. Dataset shift in machine
learning. Cambridge (Massachusetts, USA): MIT Press; 2009.
82. Rao C. Information and the accuracy attainable in the estimation of statistical parameters. Bull
Calcutta Math Soc 1945;37:81–9.
83. Ricci F, Rokach L, Shapira B, Kantor PB, editors. Recommender systems handbook. New York (NY,
USA): Springer; 2010.
84. Rissanen J. Modeling by shortest data description. Automatica 1978;14(5):465–71.
85. Rumelhart DE, Hinton GE, Williams RJ. Learning representations by back-propagating errors.
Nature 1986;323:533–6.
86. Salakhutdinov RR, Hinton GE. Deep Boltzmann machines. In: van Dyk D, Welling M, editors.
Proceedings of twelfth international conference on artificial intelligence and statistics. JMLR
workshop and conference proceedings, vol. 5. Beach (FL, USA): Clearwater; 2009. p. 448–55.
87. Schapire RE, Freund Y. Boosting: foundations and algorithms. Cambridge (Massachusetts, USA):
MIT Press; 2012.
88. Schölkopf B, Smola A, Müller K-R. Nonlinear component analysis as a kernel eigenvalue problem.
Neural Comput 1998;10(5):1299–319.
89. Schölkopf B, Smola AJ. Learning with kernels. Cambridge (MA, USA): MIT Press; 2002.
90. Scott DW. Multivariate density estimation: theory, practice and visualization. New York (NY, USA):
Wiley; 1992.
91. Sejdinovic D, Sriperumbudur B, Gretton A, Fukumizu K. Equivalence of distance-based and RKHS-
based statistics in hypothesis testing. Ann Statist 2013;41(5):2263–91.
92. Shannon C. A mathematical theory of communication. Bell Syst Tech J 1948;27:379–423.
93. Silverman BW. Density estimation for statistics and data analysis. London (UK): Chapman and Hall;
1986.
94. Sima V. Algorithms for linear-quadratic optimization. New York (NY, USA): Marcel Dekker; 1996.
95. Smith AFM, Roberts GO. Bayesian computation via the Gibbs sampler and related Markov chain
Monte Carlo methods. J Roy Statist Soc Ser B 1993;55:3–24.

Bibliography
489
96. Smolensky P. Information processing in dynamical systems: foundations of harmony theory.
In: Rumelhart DE, McClelland JL, editors. Parallel distributed processing: explorations in the
microstructure of cognition, vol. 1. Cambridge (MA, USA): MIT Press; 1986. p. 194–281.
97. Stone M. An asymptotic equivalence of choice of model by cross-validation and Akaike’s criterion.
J Roy Statist Soc Ser B 1977;39:44–7.
98. Sugiyama M. Dimensionality reduction of multimodal labeled data by local Fisher discriminant
analysis. J Mach Learn Res 2007;8(May):1027–61.
99. Sugiyama M. Statistical reinforcement learning: modern machine learning approaches. Boca Raton
(Florida, USA): Chapman and Hall, CRC; 2015.
100. Sugiyama M, Idé T, Nakajima S, Sese J. Semi-supervised local Fisher discriminant analysis for
dimensionality reduction. Mach Learn 2010;78(1–2):35–61.
101. Sugiyama M, Kawanabe M. Machine learning in non-stationary environments: introduction to
covariate shift adaptation. Cambridge (Massachusetts, USA): MIT Press; 2012.
102. Sugiyama M, Krauledat M, Müller K-R. Covariate shift adaptation by importance weighted cross
validation. J Mach Learn Res 2007;8(May):985–1005.
103. Sugiyama M, Suzuki T, Kanamori T, du Plessis MC, Liu S, Takeuchi I. Density-difference estimation.
Neural Comput 2013;25(10):2734–75.
104. Sugiyama M, Suzuki T, Nakajima S, Kashima H, von Bünau P, Kawanabe M. Direct importance
estimation for covariate shift adaptation. Ann Inst Statist Math 2008;60(4):699–746.
105. Sutton RS, Barto GA. Reinforcement learning: an introduction. Cambridge (MA, USA): MIT Press;
1998.
106. Székely GJ, Rizzo ML. Energy statistics: a class of statistics based on distances. J Statist Plann
Inference 2013;143(8):1249–72.
107. Takeuchi K. Distribution of information statistics and validity criteria of models. Math Sci
1976;153:12–8 [in Japanese].
108. Tangkaratt V, Sasaki H, Sugiyama M. Direct estimation of the derivative of quadratic mutual
information with application in supervised dimension reduction. Technical report 1508.01019, arXiv;
2015.
109. Tax DMJ, Duin RPW. Support vector data description. Mach Learn 2004;54(1):45–66.
110. Tibshirani R. Regression shrinkage and subset selection with the lasso. J Roy Statist Soc Ser B
1996;58(1):267–88.
111. Tibshirani R, Saunders M, Rosset S, Zhu J, Knight K. Sparsity and smoothness via the fused lasso. J
Roy Statist Soc Ser B 2005;67:91–108.
112. Tomioka R, Aihara K. Classifying matrices with a spectral regularization. In: Ghahramani Z, editor.
Proceedings of the 24th annual international conference on machine learning. Omnipress; 2007. p.
895–902.
113. Torkkola K. Feature extraction by non-parametric mutual information maximization. J Mach Learn
Res 2003;3:1415–38.
114. Tsochantaridis I, Joachims T, Hofmann T, Altun Y. Large margin methods for structured and
interdependent output variables. J Mach Learn Res 2005;6:1453–84.
115. Vapnik VN. Statistical learning theory. New York (NY, USA): Wiley; 1998.
116. Vincent P, Larochelle H, Bengio Y, Manzagol PA. Extracting and composing robust features
with denoising autoencoders. In: Proceedings of 25th annual international conference on machine
learning; 2008. p. 1096–103.
117. von Neumann J. Various techniques used in connection with random digits. In: Householder AS,
Forsythe GE, Germond HH, editors. Monte Carlo methods; National bureau of standards applied
mathematics series, vol. 12. 1951. p. 36–8.

490
Bibliography
118. Wahba G. Spline models for observational data. Philadelphia (PA, USA): Society for Industrial and
Applied Mathematics; 1990.
119. Watanabe S. Algebraic geometry and statistical learning theory. Cambridge (UK): Cambridge
University Press; 2009.
120. Wu CFJ. On the convergence properties of the EM algorithm. Ann Statist 1983;11:95–103.
121. Yamada M, Suzuki T, Kanamori T, Hachiya H, Sugiyama M. Relative density-ratio estimation for
robust distribution comparison. Neural Comput 2013;25(5):1324–70.
122. Yuan M, Lin Y. Model selection and estimation in regression with grouped variables. J Roy Statist
Soc Ser B 2006;68(1):49–67.
123. Zelnik-Manor L, Perona P. Self-tuning spectral clustering. In: Saul LK, Weiss Y, Bottou L, editors.
Advances in neural information processing systems, vol. 17. Cambridge (MA, USA): MIT Press;
2005. p. 1601–8.
124. Zou H, Hastie T. Regularization and variable selection via the elastic net. J Roy Statist Soc Ser B
2005;67(2):301–20.


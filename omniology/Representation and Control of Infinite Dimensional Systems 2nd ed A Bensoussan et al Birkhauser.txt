

Systems & Control: Foundations & Applications
Series Editor
Tamer Bas¸ar, University of Illinois at Urbana-Champaign
Editorial Board
Karl Johan ˚Astr¨om, Lund University of Technology, Lund, Sweden
Han-Fu Chen, Academia Sinica, Beijing
William Helton, University of California, San Diego
Alberto Isidori, University of Rome (Italy) and
Washington University, St. Louis
Petar V. Kokotovi´c, University of California, Santa Barbara
Alexander Kurzhanski, Russian Academy of Sciences, Moscow
and University of California, Berkeley
H. Vincent Poor, Princeton University
Mete Soner, Koc¸ University, Istanbul

Alain Bensoussan
Giuseppe Da Prato
Michel C. Delfour
Sanjoy K. Mitter
Representation and Control of
Inﬁnite Dimensional Systems
Second Edition
Birkh¨auser
Boston • Basel • Berlin

Alain Bensoussan
School of Management
University of Texas at Dallas
Ofﬁce 3.211
Richardson, TX 75083-0688
USA
Giuseppe Da Prato
Scuola Normale Superiore
Palazzo della Carovana
Piano 3, studio 56
Pisa 56126
Italy
Michel C. Delfour
Centre de recherches math´ematiques
and
D´epartement de math´ematiques et de statistique
Universit´e de Montr´eal
C.P. 6128, succ. Centre-ville
Montr´eal (QC), Canada H3C 3J7
Sanjoy K. Mitter
Department of Electrical Engineering
and Computer Science
Laboratory for Information and
Decision Systems
Massachusetts Institute of Technology
77 Massachusetts Avenue
Cambridge, MA 02139
USA
Mathematics Subject Classiﬁcation: 34H05, 47D03, 47N70, 49-02, 49J27, 49K27, 93C25, 93C95
Library of Congress Control Number: 2006936851
ISBN-10: 0-8176-4461-X
e-ISBN-10: 0-8176-4581-0
ISBN-13: 978-0-8176-4461-1
e-ISBN-13: 978-0-8176-4581-6
Printed on acid-free paper.
c⃝2007 Birkh¨auser Boston
All rights reserved. This work may not be translated or copied in whole or in part without the writ-
ten permission of the publisher (Birkh¨auser Boston, c/o Springer Science+Business Media LLC, 233
Spring Street, New York, NY 10013, USA), except for brief excerpts in connection with reviews or
scholarly analysis. Use in connection with any form of information storage and retrieval, electronic
adaptation, computer software, or by similar or dissimilar methodology now known or hereafter de-
veloped is forbidden.
The use in this publication of trade names, trademarks, service marks and similar terms, even if they
are not identiﬁed as such, is not to be taken as an expression of opinion as to whether or not they are
subject to proprietary rights.
9 8 7 6 5 4 3 2 1
www.birkhauser.com
(SB)

In memory of Andr´ee

Preface to the Second Edition
A new edition in a single volume
Over the past decade, more and more sophisticaced mathematical tools and
approaches have been incorporated in the ﬁeld of Control of inﬁnite dimen-
sional systems. This was motivated by a whole range of challenging applica-
tions arising from new phenomenological studies, technological developments,
and more stringent design requirements. At the same time, researchers and
advanced engineers have been steadily using an impressive amount of very
sophisticated mathematics in their analysis, synthesis, and design of systems.
What was regarded as too abstract, specialized, or theoretical in 1990 has now
become a standard part of the toolkit.
The decision to produce a second edition of the original 1992–1993 two-
volume edition is further motivated by several other factors. Over the years
the book has been recognized as a key reference in the ﬁeld, and a revised and
corrected edition was desirable. Even if some good books on the control of
inﬁnite dimensional linear systems have appeared since then, we felt that the
original material has not aged too much and that the breadth of its presenta-
tion is still attractive and very competitive. The result is a completely revised
and corrected second edition in a single convenient volume with integrated
bibliography and index.
The book has been restructured into ﬁve parts and each part into several
chapters. The most signiﬁcant changes occurred in Part I, which now provides
a very broad account of ﬁnite dimensional linear systems. It serves as a back-
ground and a motivation for the other parts. The scope of this part has been
expanded by adding topics such as dissipative systems in Chapter 1 and intro-
ducing a new Chapter 2 on linear quadratic two-person zero-sum diﬀerential
games that provides an example of a solution to the matrix Riccati diﬀerential
equation that is not necessarily positive semi-deﬁnite and some connections
with the H∞-theory and dissipative systems of Chapter 1.

viii
Preface to the Second Edition
Description of the ﬁve parts
Part I on the control of ﬁnite dimensional linear dynamical systems and linear
quadratic two-person zero-sum diﬀerential games presents a broad review of
the ﬁeld. It is an introduction and a motivation for the book, so that the reader
understands the background from which the inﬁnite dimensional theory is
developed and also obtains an appreciation of the substantial conceptual and
technical diﬃculties that had to be overcome to give a satisfactory treatment
of the subject in the inﬁnite dimensional context. Chapter 1 is concerned with
the theory of controllability and observability of linear systems, and the role
these concepts play in the study of the quadratic cost problem over an inﬁnite
time horizon. It has been expanded to include, in addition to the updated
section on the H∞-theory, a new section on the theory of dissipative systems
of J. C. Willems [2].
A new Chapter 2 on linear quadratic two-person zero-sum diﬀerential
games has been included to broaden the perspective. The pioneering work
in that area has been done in the paper of P. Bernhard [2] in 1979 and
the seminal book of T. Bas¸ar and P. Bernhard [1] in 1991 and 1995. This
chapter is self-contained with complete mathematical proofs. It focuses on the
open loop case. New results using the invariant embedding approach of R. Bell-
man in the style of J. L. Lions [3] are included to provide further insight into
the associated inf sup (open loop upper value) and sup inf (open loop lower
value) problems. This is combined with developments by P. Zhang [1] in 2005
who established that in the linear quadratic case the duality gap is either zero
or inﬁnite. This means that only three cases can occur: (a) sup inf = −∞and
inf sup ﬁnite; (b) sup inf ﬁnite and inf sup = +∞; and (c) sup inf and inf
sup both ﬁnite; in which case, there is equality and the existence of a saddle
point. In particular it illustrates the occurrence of symmetrical solutions to
the matrix Riccati diﬀerential equation that are not necessarily positive semi-
deﬁnite. It also connects with the glimpse of H∞-theory and the new section
on the theory of dissipative systems at the end of Chapter 1.
Part II deals with the representation of inﬁnite dimensional systems. It
develops semigroup theory and variational methods for the representation of
inﬁnite dimensional systems such as dynamical partial diﬀerential equations
and delay diﬀerential systems. Chapter 1 gives a unique presentation of the
theory of semigroups of linear operators integrated with interpolation theory.
It brings together advanced concepts and techniques that are usually treated
independently. Chapter 2 provides a basic introduction to the variational the-
ory of parabolic systems. It nicely describes the explicit and illuminating con-
nection between the early work of T. Kato and J.-L. Lions and the later results
of P. Auscher, S. Hofmann, J. L. Lewis, and Ph. Tchamitchian [1] in
2001 on the Kato’s conjecture on the domain of the square root operators.
Chapter 3 contains the basic constructions to eﬀectively deal with unbounded
control and observation operators via semigroup methods.

Preface to the Second Edition
ix
Chapter 4 on the modeling of diﬀerential systems with delays in the state,
control, and observation is self-contained. To our knowledge, it is possibly
the only book where the state space theory is completely developed using
the fundamental structural operators introduced by Delfour and Manitius in
1976–1977 and further extended to the neutral case by M. C. Delfour and
J. Karrakchou [1,2] in 1987.
Part III is devoted to the generic qualitative properties of controlled sys-
tems. It studies the controllability for an inﬁnite dimensional abstract linear
dynamical system, which can be specialized to obtain results for controllability
of parabolic and hyperbolic partial diﬀerential equations both when control is
exercised in the interior of the domain and when control is exercised through
the boundary. The important problem of exact controllability of hyperbolic
equations in appropriate spaces, which leads to stabilizability properties for
these systems (and hence veriﬁcation of the ﬁnite cost condition for inﬁnite
time problems) is discussed in detail in this chapter. The systematic use of
eigenvalues and eigenfunctions of appropriate diﬀerential operators to obtain
the results is a somewhat novel aspect of this chapter.
Parts IV and V present a dynamical programming approach to the optimal
linear quadratic control problem over a ﬁnite and an inﬁnite time horizon of
certain classes of inﬁnite dimensional systems. Dynamic programming is a
deep conceptual idea. Its applications are manifold. For the philosophically
minded reader, we have provided a quotation from Soren Kierkegaard on
page xvi of the Preface to Volume II of the First Edition as an indication that
philosophy might often anticipate developments in science.
Part IV is devoted to the quadratic cost optimal control problem over a
ﬁnite time horizon. We develop the theory for an abstract dynamical model
satisfying certain semigroup (group) assumptions, and then we treat concrete
situations by verifying these assumptions using diﬀerential equations (partial,
functional-differential) methods. This chapter presents a reasonably complete
treatment of the subject, which includes both boundary control and bound-
ary observation (not necessarily simultaneously) for parabolic and hyperbolic
systems. Technically, these are the more diﬃcult parts of the book because
they involve unbounded control and observation operators. Many of the re-
sults presented here appeared in book form for the ﬁrst time in 1993. The
approach we adopt is dynamic programming, which leads to a synthesis of the
optimal control in feedback form via a study of an operator Riccati diﬀerential
equation. The systematic use of dynamic programming gives a uniﬁed view of
this topic.
Part V, the ﬁnal part of the book, is concerned with the quadratic cost
optimal control problem over an inﬁnite time horizon. Here the concepts of
stabilizability and detectability play an essential role. The properties of stabi-
lizability and detectability have to be veriﬁed, in some sense directly, for the
parabolic case (which has a ﬁnite dimensional unstable part) and via the the-
ory of exact controllability (for example) in the hyperbolic case. Thus, there is
a close relation between Part V and Parts I and III of the book. The approach

x
Preface to the Second Edition
here is again via dynamic programming and a study of an algebraic matrix
Riccati equation. We also prove the stability of the closed loop system. In
many (but not all) situations, the theory of the inﬁnite time quadratic cost
problem in the inﬁnite dimensional case is as complete as for the ﬁnite dimen-
sional situation. Again, many results here appeared for the ﬁrst time in book
form in 1993.
Acknowledgments
It is a pleasant task to acknowledge the support of the Centre de recherches
math´ematiques of the Universit´e de Montr´eal and the Natural Sciences and
Engineering Council through Discovery Grant–8730 in the “re-engineering” of
the ﬁrst edition of the book into this new second edition.
We are very grateful to Thomas Grasso of Birkh¨auser Boston Inc. for his
cooperation and eﬀorts since the 2003 IEEE CDC Meeting to promote the
book and for his kind invitation to consider a second edition. It is also a
pleasure to thank Tamer Ba¸sar, Editor-in-Chief, for accepting it in the book
series Systems & Control: Foundations & Applications.
Most sincere thanks to Louise Letendre of the Centre de Recherches
Math´ematiques for her colossal, patient, and truly professional work of com-
pletely reorganizing and upgrading the old LaTeX ﬁles of the original two-
volume book to the latest AMS LaTex standards in a single volume. Very
special thanks also go to Andr´e Montpetit of the Centre de Recherches
Math´ematiques who provided his technical support, experience, and talent
to adapt, upgrade, and ajust the TeX ﬁles and the special format of the bib-
liography to the latest Birkh¨auser/Springer macros.
Additional earlier acknowledgments can be found in the Prefaces to Vol-
ume I (pages xii to xiii) and II (pages xvi to xvii) of the ﬁrst edition.
October, 2006
Alain Bensoussan, Richardson, USA
Giuseppe Da Prato, Pisa, Italia
Michel C. Delfour, Montreal, Canada
Sanjoy K. Mitter, Cambridge, USA.

Preface to Volume I of the First Edition
At the end of the 1960s, the state space theory of linear systems both for time-
invariant and time-varying systems had essentially been worked out. The basic
concepts of controllability, observability, and their relationship to the theory
of minimal realizations received almost a complete treatment in the work
of Kalman and others. The feedback solution of the quadratic cost optimal
control problem, both in a ﬁnite and an inﬁnite time interval were well under-
stood. The concepts of controllability and observability and the weaker no-
tions of stabilizability and detectability play an essential role through stability
ideas in the solution of the inﬁnite time quadratic cost optimal control prob-
lem. The ideas of optimal control from the Hamilton–Jacobi–Caratheodory,
dynamic programming, calculus of variations (and in its modern form the
McShane–Pontryagin maximum principle) points of view had received deﬁn-
itive treatments. As far as deterministic control problems for ﬁnite dimen-
sional linear systems are concerned, attention shifted away from optimality
to synthesis problems, such as decoupling, regulator, and tracking with inter-
nal stability, and to a uniﬁcation of the time and frequency domain points of
view. A qualitative theory of nonlinear control of ﬁnite dimensional dynamical
systems began its development at the same time.
It is a fact that most lumped parameter systems are approximations of
distributed parameter systems, and hence, the study of inﬁnite dimensional
systems such as control of systems governed by partial diﬀerential equations
and functional diﬀerential equations are both of intrinsic interest and poten-
tially important for application areas such as chemical process control, control
of elastic structures, and even for challenging issues such as the stabilization of
plasma instabilities. It was, in some sense, natural to concentrate on optimal
control problems for inﬁnite dimensional systems so that the well-developed
theory of calculus of variations and Hamilton–Jacobi theory could be suit-
ably generalized and used. Clearly the ﬁrst step in such a development is a
reasonably complete understanding of the linear quadratic problem both over
a ﬁnite and an inﬁnite time interval. This book attempts to do this with a
reasonable degree of completeness.

xii
Preface to Volume I of the First Edition
This book has been many years in the making, and the original technical
reports date back to 1975. It must also be one of the most cited books with
the appended words “to appear.” The reasons for this delay in publication
are complex and could be the subject of psycho-analytical research or a study
into the origins of human frailties. It started out as a collaborative eﬀort of
Alain Bensoussan, Michel C. Delfour, and Sanjoy Mitter. From the outset
it was our intention to write a book that would go beyond what was then
available in the literature of optimal control of inﬁnite dimensional systems
and cover such topics as the feedback boundary control of hyperbolic systems
when control is exercised through Dirichlet boundary conditions, feedback
control of hyperbolic systems over an inﬁnite time interval, and the control of
neutral functional diﬀerential equations. In retrospect, we seriously underes-
timated the diﬃculty of carrying out such a program. Indeed the solution of
many of these control problems required results that were then not available
in the partial diﬀerential equations literature. For example, the solution of
certain problems of optimal control of hyperbolic equations with a quadratic
cost but over an inﬁnite time interval required a result that would enable one
to conclude that the set of admissible controls is nonempty. Such a result is
a byproduct of the theory of exact controllability of hyperbolic systems, a
theory that was only developed in the 1980s. Our research interests had also
changed in the mid-1970s, and although we continued to think about the sub-
ject matter of this book, we did not return to it seriously until the mid-1980s
or so. Even then the book would never have been completed but for the eﬀorts
of Giuseppe Da Prato who joined the team approximately two years ago (one
of the coauthors takes full credit for having had the brilliant idea of enlisting
Da Prato in the project). The result is a book by four authors, from four
diﬀerent countries, who met in Pisa, Paris, Montr´eal, and Cambridge, Mass.
to bring this long endeavor to completion—surely a model of international
cooperation. Readers of the book will have to judge whether this eﬀort has
been successful.
Earlier versions of some chapters of this book have been used for graduate
courses at the Scuola Normale Superiore (Chapter 1) and at the Universit´e
de Montr´eal (Chapters 1 and 4). Other parts have been used for graduate
seminars.
It is a pleasant task to thank the many institutions that have made this
work possible—Universit´e de Paris–Dauphine and INRIA, France; Scuola
Normale Superiore, Pisa, Italy; Centre de recherches math´ematiques and
D´epartement de math´ematiques et de statistique, Universit´e de Montr´eal,
Canada; Department of Electrical Engineering and Computer Science and the
Laboratory for Information and Decision Systems, Massachusetts Institute of
Technology, Cambridge, MA, USA. The research of Giuseppe Da Prato has
been partially supported by the Italian National Project MURST “Equazioni
di Evoluzione e Applicazioni Fisico-Matematiche.” The research of Michel
C. Delfour has been supported by the Natural Sciences and Engineering Coun-
cil operating grant OGP–8730 and infrastructure grant INF–7939, the “Min-

Preface to Volume I of the First Edition
xiii
ist`ere de l’´Education du Qu´ebec” through a FCAR Team Grant, the France–
Qu´ebec exchange program, and a Killam Fellowship from Canada Council.
The research of Sanjoy Mitter has been supported by the U.S. National Sci-
ence Foundation in the 1970s, the U.S. Air Force Oﬃce of Scientiﬁc Research
over a long period, and the U.S. Army Research Oﬃce through the Center for
Intelligent Control Systems for the last six years. This support is gratefully
acknowledged.
It is a pleasure to thank Edwin Beschler of Birkh¨auser Boston Inc. for his
cooperation, Ann Kostant also of Birkh¨auser for expert help in word process-
ing, and Margaret Flaherty of the Laboratory for Information and Decision
(M.I.T.) and Diane de Filippis of the Centre de recherches math´ematiques (U.
de M.) for typing large parts of the manuscript with precision and care.
The Earth,
Alain Bensoussan, Paris, France
April 15, 1992
Giuseppe Da Prato, Pisa, Italia
Michel C. Delfour, Montr´eal, Canada
Sanjoy Mitter, Cambridge, USA.

Preface to Volume II of the First Edition
Volume I of this two-volume book has dealt with the question of modeling and
representation of inﬁnite dimensional systems. Volume II is concerned with
the optimal control of certain classes of inﬁnite dimensional systems with a
quadratic cost criterion, both over a ﬁnite and an inﬁnite time horizon. The
knowledge of Volume I is a prerequisite for reading Volume II.
Volume II consists of three parts. Chapter 1 of Part I is concerned with
the theory of controllability and observability of ﬁnite dimensional linear sys-
tems, and the role these concepts play in the study of the quadratic cost
problem over an inﬁnite time horizon. This chapter is included so that the
reader understands the background from which the inﬁnite dimensional the-
ory developed and obtains an appreciation of the substantial conceptual and
technical diﬃculties that had to be overcome to give a satisfactory treatment
of the subject in the inﬁnite dimensional context. Chapter 2 is devoted to the
study of controllability for an inﬁnite dimensional abstract linear dynamical
system that can be specialized to obtain results for controllability of parabolic
and hyperbolic partial diﬀerential equations both when control is exercised in
the interior of the domain and when control is exercised through the bound-
ary. The important problem of exact controllability of hyperbolic equations in
appropriate spaces, which leads to stabilizability properties for these systems
(and hence veriﬁcation of the “ﬁnite cost” condition for inﬁnite time prob-
lems), is discussed in detail in this chapter. The systematic use of eigenvalues
and eigenfunctions of appropriate diﬀerential operators to obtain the results
is a somewhat novel aspect of this chapter.
Part II of the book is devoted to the quadratic cost optimal control problem
over a ﬁnite time horizon. We develop the theory for an abstract dynamical
model satisfying certain semigroup (group) assumptions, and then we treat
concrete situations by verifying these assumptions using diﬀerential equations
(partial, functional-differential) methods. This chapter presents a reasonably
complete treatment of the subject, which includes both boundary control and
boundary observation (not necessarily simultaneously) for parabolic and hy-
perbolic systems. Technically, these are the more diﬃcult parts of the book

xvi
Preface to Volume II of the First Edition
because they involve unbounded control and observation operators. Many of
the results presented here appear in book form for the ﬁrst time.
The approach we adopt is dynamic programming, which leads to a synthesis
of the optimal control in feedback form via a study of an operator Riccati
diﬀerential equation. The systematic use of dynamic programming gives a
uniﬁed view of this topic.
Part III, the ﬁnal part of the book, is concerned with the quadratic cost
optimal control problem over a inﬁnite time horizon. Here the concepts of
stabilizability and detectability play an essential role. The properties of stabi-
lizability and detectability have to be veriﬁed, in some sense directly, for the
parabolic case (which has a ﬁnite dimensional unstable part) and via the the-
ory of exact controllability (for example) in the hyperbolic case. Thus, there
is a close relation between Parts I and III of the book. The approach here is
again via dynamic programming and a study of an algebraic Riccati equation.
We also prove the stability of the closed loop system. In many (but not all)
situations the theory of the inﬁnite time quadratic cost problem in the inﬁnite
dimensional case is as complete as for the ﬁnite dimensional situation. Again,
many results here appear for the ﬁrst time in book form.
Dynamic programming is a deep conceptual idea. Its applications are man-
ifold. For the philosophically minded reader, we have provided the following
quotation from Soren Kierkegaard as an indication that philosophy might
often anticipate developments in science:
It is perfectly true, as philosophers say, that life must be understood
backwards. But they forget the other proposition, that it must be lived
forwards . . . And if one thinks over the proposition it becomes more
and more evident that life can never really be understood in time sim-
ply because at no particular moment can I ﬁnd the necessary resting-
place from which to understand it–backwards.
From entry in Soren Kierkegaard’s Journal for the Year 1843.
Quoted in Richard Wollheim, Thread of life.
Harvard University Press, Cambridge, MA, 1984.
Earlier versions of some chapters of this book have been used for graduate
courses at the Scuola Normale Superiore and at the Universit´e de Montr´eal.
Other parts have been used for graduate seminars.
It is a pleasant task to thank the many institutions that have made this
work possible—Universit´e de Paris–Dauphine and INRIA, France; Scuola
Normale Superiore, Pisa, Italy; Centre de recherches math´ematiques and
D´epartement de math´ematiques et de statistique, Universit´e de Montr´eal,
Canada; Department of Electrical Engineering and Computer Science and the
Laboratory for Information and Decision Systems, Massachusetts Institute of
Technology, Cambridge, MA, USA. The research of Giuseppe Da Prato has
been partially supported by the Italian National Project MURST “Equazioni
di Evoluzione e Applicazioni Fisico-Matematiche.” The research of Michel

Preface to Volume II of the First Edition
xvii
C. Delfour has been supported by the Natural Sciences and Engineering Coun-
cil operating grant OGP–8730 and infrastructure grant INF–7939, the “Min-
ist`ere de l’´Education du Qu´ebec” through a FCAR Team Grant, the France–
Qu´ebec exchange program, and a Killam Fellowship from Canada Council.
The research of Sanjoy Mitter has been supported by the U.S. National Sci-
ence Foundation in the 1970s, the U.S. Air Force Oﬃce of Scientiﬁc Research
over a long period, and the U.S. Army Research Oﬃce through the Center for
Intelligent Control Systems for the last six years. This support is gratefully
acknowledged.
It is a pleasure to thank Georg Schmidt for a critical reading of Part I,
Chapter 2 of the book. His comments have led to many improvements in this
chapter. We thank Edwin Beschler of Birkh¨auser Boston Inc. for his coopera-
tion, Ann Kostant also of Birkh¨auser for expert help in word processing, and
Margaret Flaherty of the Laboratory for Information and Decision Systems
(M.I.T.) for typing large parts of the manuscript with precision and care.
The Earth,
Alain Bensoussan, Paris, France
March 26, 1993
Giuseppe Da Prato, Pisa, Italia
Michel C. Delfour, Montr´eal, Canada
Sanjoy Mitter, Cambridge, USA.

Contents
Preface to the Second Edition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
vii
A new edition in a single volume . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
vii
Description of the ﬁve parts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
viii
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
x
Preface to Volume I of the First Edition . . . . . . . . . . . . . . . . . . . . .
xi
Preface to Volume II of the First Edition . . . . . . . . . . . . . . . . . . . .
xv
List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxvii
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1
Scope of the book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
2
From ﬁnite to inﬁnite dimensional sytems . . . . . . . . . . . . . . . .
2
Notes: some related books on the control of linear systems
that have appeared since 1992. . . . . . . . . . . . . . . . . . . . . . . . . . .
10
Part I
Finite Dimensional Linear Control Dynamical Systems
1
Control of Linear Diﬀerential Systems . . . . . . . . . . . . . . . . . . .
13
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2
Controllability, observability, stabilizability, and detectability
13
2.1
Controllability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2.2
Observability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.3
Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.4
Canonical structure for linear systems . . . . . . . . . . . . . .
20
2.5
The pole-assignment theorem . . . . . . . . . . . . . . . . . . . . .
20
2.6
Stabilizability and detectability . . . . . . . . . . . . . . . . . . .
23
2.7
Applications of controllability and observability . . . . .
25
3
Optimal control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30

xx
Contents
3.1
Finite time horizon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
3.2
Inﬁnite time horizon . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
4
A glimpse into H∞-theory: state feedback case . . . . . . . . . . . .
35
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
4.2
Main results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
5
Dissipative systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
5.1
Deﬁnitions and preliminary results . . . . . . . . . . . . . . . .
39
5.2
Associated variational problems . . . . . . . . . . . . . . . . . . .
40
5.3
Quadratic storage functions . . . . . . . . . . . . . . . . . . . . . . .
43
6
Final remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
2
Linear Quadratic Two-Person Zero-Sum Diﬀerential
Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2
Deﬁnitions, notation, and preliminary results . . . . . . . . . . . . .
50
2.1
System, utility function, and values of the game . . . . .
50
2.2
Properties, semi-derivatives, and convexity/concavity
of Cx0(u, v) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3
Saddle point and coupled state–adjoint state system . . . . . . .
54
4
Finite open loop lower value . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
4.1
Main theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
4.2
Abstract operators and a preliminary lemma . . . . . . . .
57
4.3
Existence and characterization of the minimizers . . . .
59
4.4
Intermediary results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
4.5
Existence and characterization of maximizers of the
minimum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
4.6
Finite open loop lower value for all initial states and
uniqueness of solution of the coupled system . . . . . . . .
66
5
Finite open loop value and open loop saddle point . . . . . . . . .
68
6
Riccati diﬀerential equation in the open loop saddle point
case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
6.1
Invariant embedding with respect to the initial time .
69
6.2
From convexity/concavity in [0, T ] to [s, T] . . . . . . . . .
70
6.3
Open loop saddle point optimality principle. . . . . . . . .
71
6.4
Decoupling of the coupled system . . . . . . . . . . . . . . . . .
75
6.5
Riccati diﬀerential equation . . . . . . . . . . . . . . . . . . . . . . .
77
6.6
Open loop saddle point and Riccati diﬀerential
equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
6.7
The general case of Remark 2.1 . . . . . . . . . . . . . . . . . . .
80
7
Riccati diﬀerential equation and open/closed loop
upper/lower value of the game . . . . . . . . . . . . . . . . . . . . . . . . . .
81

Contents
xxi
Part II
Representation of Inﬁnite Dimensional Linear Control
Dynamical Systems
1
Semigroups of Operators and Interpolation . . . . . . . . . . . . . .
87
1
Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
2
Linear evolution equations and strongly continuous
semigroups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
2.1
Deﬁnitions and preliminary results . . . . . . . . . . . . . . . .
88
2.2
Asymptotic behavior of S(t) . . . . . . . . . . . . . . . . . . . . . .
91
2.3
Spectral properties of the inﬁnitesimal generator . . . .
100
2.4
Hille–Yosida–Miyadera–Feller–Phillips theorem . . . . . .
101
2.5
Adjoint semigroups and their generators. . . . . . . . . . . .
103
2.6
Semigroups of contractions and dissipative operators .
104
2.7
Analytic semigroups . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
2.8
Diﬀerentiable semigroups . . . . . . . . . . . . . . . . . . . . . . . . .
115
2.9
Spectral determining growth condition . . . . . . . . . . . . .
119
2.10
Examples of semigroups . . . . . . . . . . . . . . . . . . . . . . . . . .
122
3
Nonhomogeneous linear evolution equations . . . . . . . . . . . . . . .
128
3.1
Setting of the problem and deﬁnitions . . . . . . . . . . . . . .
128
3.2
Existence and uniqueness of a strong solution . . . . . . .
130
3.3
Existence of a strict solution . . . . . . . . . . . . . . . . . . . . . .
133
3.4
Perturbations of inﬁnitesimal generators . . . . . . . . . . . .
134
3.5
Evolution operators. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
3.6
Maximal regularity results in Hilbert spaces and main
isomorphism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
3.7
Regularity results in C([0, T ]; X) . . . . . . . . . . . . . . . . . .
146
3.8
Examples of nonhomogeneous problems . . . . . . . . . . . .
148
3.9
Point spectrum operators . . . . . . . . . . . . . . . . . . . . . . . . .
151
4
Interpolation spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
4.1
Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
4.2
Spaces of traces T (p, α, X0, X1) . . . . . . . . . . . . . . . . . . .
154
4.3
Spaces of averages (X0, X1)θ,p . . . . . . . . . . . . . . . . . . . .
159
4.4
Interpolation spaces between the domain of a linear
operator A and the space X . . . . . . . . . . . . . . . . . . . . . .
162
4.5
The case of a strongly continuous semigroup . . . . . . . .
163
4.6
The case of an analytic semigroup . . . . . . . . . . . . . . . . .
164
4.7
The interpolation space [X, Y ]θ . . . . . . . . . . . . . . . . . . .
166
5
Fractional powers of dissipative operators . . . . . . . . . . . . . . . . .
167
6
Interpolation spaces and domains of fractional powers of an
operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169

xxii
Contents
2
Variational Theory of Parabolic Systems . . . . . . . . . . . . . . . . .
173
1
Variational diﬀerential equations . . . . . . . . . . . . . . . . . . . . . . . .
173
1.1
Distributed control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
1.2
Boundary control condition . . . . . . . . . . . . . . . . . . . . . . .
176
1.3
Main theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
1.4
A perturbation theorem . . . . . . . . . . . . . . . . . . . . . . . . . .
179
1.5
A regularity theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
180
2
Method of Transposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
2.1
Control through a Dirichlet boundary condition . . . . .
188
2.2
Point control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
2.3
Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
2.4
Application of transposition to the examples of §2.1
and §2.2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
192
2.5
A change of variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
196
2.6
Other isomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
198
3
Second order problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
198
3
Semigroup Methods for Systems With Unbounded
Control and Observation Operators . . . . . . . . . . . . . . . . . . . . . .
201
1
Complements on semigroups . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
1.1
Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
2
Complements on analytic semigroups . . . . . . . . . . . . . . . . . . . .
206
2.1
Regularity results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
2.2
Other representations and the method of change of
variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
3
Unbounded control and observation operators . . . . . . . . . . . . .
210
3.1
Analytic systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
211
3.2
Unbounded control operators . . . . . . . . . . . . . . . . . . . . .
212
3.3
Unbounded observation operators . . . . . . . . . . . . . . . . .
216
3.4
Unbounded control and observation operators . . . . . . .
218
4
Time-invariant variational parabolic systems . . . . . . . . . . . . . .
222
4
State Space Theory of Diﬀerential Systems With Delays .
229
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
2
Examples and orientation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231
2.1
Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231
2.2
Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235
2.3
Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
238
3
Existence theorems for Lipschitzian systems . . . . . . . . . . . . . .
240
3.1
Continuous functions framework. . . . . . . . . . . . . . . . . . .
240
3.2
Lp or product space framework . . . . . . . . . . . . . . . . . . .
245
3.3
Linear time-invariant systems . . . . . . . . . . . . . . . . . . . . .
249
4
State space theory of linear time-invariant systems . . . . . . . . .
252
4.1
Preliminary results and smoothness of the solution . .
252
4.2
First state equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
254

Contents
xxiii
4.3
Transposed and adjoint systems . . . . . . . . . . . . . . . . . . .
260
4.4
Structural operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
4.5
Adjoint semigroup {S⊤∗(t)} and intertwining theorems 265
4.6
Inﬁnitesimal generators A⊤∗and A∗. . . . . . . . . . . . . . .
269
4.7
The companion structural operator G of F . . . . . . . . .
275
5
State space theory of linear control systems . . . . . . . . . . . . . . .
279
5.1
The structural state . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
280
5.2
The extended state . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
286
6
State space theory of linear control systems with observation
297
6.1
The extended state . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
299
6.2
The extended structural state . . . . . . . . . . . . . . . . . . . . .
300
6.3
Intertwining property of the two extended
states . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
308
Part III
Qualitative Properties of Inﬁnite Dimensional Linear
Control Dynamical Systems
1
Controllability and Observability for a Class of Inﬁnite
Dimensional Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
2
Main deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
317
2.1
Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
317
2.2
Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
318
3
Criteria for approximate and exact controllability. . . . . . . . . .
322
3.1
Criterion for approximate controllability. . . . . . . . . . . .
322
3.2
Criteria for exact controllability and continuous
observability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
323
3.3
Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
324
4
Finite dimensional control space . . . . . . . . . . . . . . . . . . . . . . . . .
325
4.1
Finite dimensional case . . . . . . . . . . . . . . . . . . . . . . . . . .
325
4.2
General state space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
328
5
Controllability for the heat equation . . . . . . . . . . . . . . . . . . . . .
330
5.1
Distributed control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
330
5.2
Boundary control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
331
5.3
Neumann boundary control . . . . . . . . . . . . . . . . . . . . . . .
334
5.4
Pointwise control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
338
6
Controllability for skew-symmetric operators . . . . . . . . . . . . . .
339
6.1
Notation and general comments . . . . . . . . . . . . . . . . . . .
339
6.2
Dynamical system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
342
6.3
Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
352
6.4
Exact controllability for T arbitrarily small . . . . . . . . .
357
7
General framework: skew-symmetric operators . . . . . . . . . . . .
362
7.1
Operator A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
362
7.2
Operator B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
363

xxiv
Contents
7.3
Dynamical system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
364
7.4
Exact controllability . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
365
8
Exact controllability of hyperbolic equations . . . . . . . . . . . . . .
367
8.1
Wave equation with Dirichlet boundary control . . . . .
368
8.2
Wave equation with Neumann boundary control . . . . .
369
8.3
Maxwell equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
371
8.4
Plate equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
377
Part IV
Quadratic Optimal Control: Finite Time Horizon
1
Bounded Control Operators: Control Inside the Domain .
385
1
Introduction and setting of the problem . . . . . . . . . . . . . . . . . .
385
2
Solution of the Riccati equation . . . . . . . . . . . . . . . . . . . . . . . . .
386
2.1
Notation and preliminaries . . . . . . . . . . . . . . . . . . . . . . .
386
2.2
Riccati equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
390
2.3
Representation formulas for the solution of the Riccati
equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
395
3
Strict and classical solutions of the Riccati equation . . . . . . .
397
3.1
The general case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
398
3.2
The analytic case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
400
3.3
The variational case . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
403
4
The case of the unbounded observation . . . . . . . . . . . . . . . . . . .
405
4.1
The analytic case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
4.2
The variational case . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
5
The case when A generates a group . . . . . . . . . . . . . . . . . . . . . .
407
6
The linear quadratic control problem with ﬁnite horizon . . . .
408
6.1
The main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
408
6.2
The case of unbounded observation . . . . . . . . . . . . . . . .
410
6.3
Regularity properties of the optimal control. . . . . . . . .
411
6.4
Hamiltonian systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
412
7
Some generalizations and complements . . . . . . . . . . . . . . . . . . .
412
7.1
Nonhomogeneous state equation . . . . . . . . . . . . . . . . . . .
412
7.2
Time-dependent state equation and cost function . . . .
414
7.3
Dual Riccati equation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
416
8
Examples of controlled systems . . . . . . . . . . . . . . . . . . . . . . . . . .
418
8.1
Parabolic equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
418
8.2
Wave equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
420
8.3
Delay equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
423
8.4
Evolution equations in noncylindrical domains . . . . . .
428

Contents
xxv
2
Unbounded Control Operators: Parabolic Equations With
Control on the Boundary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
431
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
431
2
Riccati equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
438
2.1
Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
438
2.2
Riccati equation for α > 1/2 . . . . . . . . . . . . . . . . . . . . . .
439
2.3
Solution of the Riccati equation for α ≤1/2 . . . . . . . .
446
3
Dynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
454
3
Unbounded Control Operators: Hyperbolic Equations
With Control on the Boundary . . . . . . . . . . . . . . . . . . . . . . . . . .
459
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
459
2
Riccati equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
462
3
Dynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
463
4
Examples of controlled hyperbolic systems . . . . . . . . . . . . . . . .
466
5
Some result for general semigroups . . . . . . . . . . . . . . . . . . . . . .
471
Part V
Quadratic Optimal Control: Inﬁnite Time Horizon
1
Bounded Control Operators: Control Inside the Domain .
479
1
Introduction and setting of the problem . . . . . . . . . . . . . . . . . .
479
2
The algebraic Riccati equation . . . . . . . . . . . . . . . . . . . . . . . . . .
480
3
Solution of the control problem. . . . . . . . . . . . . . . . . . . . . . . . . .
486
3.1
Feedback operator and detectability . . . . . . . . . . . . . . .
487
3.2
Stabilizability and stability of the closed loop operator
F in the point spectrum case . . . . . . . . . . . . . . . . . . . . .
489
3.3
Stabilizability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
490
3.4
Exponential stability of F . . . . . . . . . . . . . . . . . . . . . . . .
492
4
Qualitative properties of the solutions of the Riccati equation
493
4.1
Local stability results . . . . . . . . . . . . . . . . . . . . . . . . . . . .
494
4.2
Attractivity properties of a stationary solution . . . . . .
495
4.3
Maximal solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
497
4.4
Continuous dependence of stationary solutions with
respect to the data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
501
4.5
Periodic solutions of the Riccati equation . . . . . . . . . . .
502
5
Some generalizations and complements . . . . . . . . . . . . . . . . . . .
505
5.1
Nonhomogeneous state equation . . . . . . . . . . . . . . . . . . .
505
5.2
Time-dependent state equation and cost function . . . .
507
5.3
Periodic control problems . . . . . . . . . . . . . . . . . . . . . . . .
509
6
Examples of controlled systems . . . . . . . . . . . . . . . . . . . . . . . . . .
513
6.1
Parabolic equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
513
6.2
Wave equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
514
6.3
Strongly damped wave equation . . . . . . . . . . . . . . . . . . .
515

xxvi
Contents
2
Unbounded Control Operators: Parabolic Equations With
Control on the Boundary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
517
1
Introduction and setting of the problem . . . . . . . . . . . . . . . . . .
517
2
The algebraic Riccati equation . . . . . . . . . . . . . . . . . . . . . . . . . .
518
3
Dynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
521
3.1
Existence and uniqueness of the optimal control . . . . .
521
3.2
Feedback operator and detectability . . . . . . . . . . . . . . .
523
3.3
Stabilizability and stability of F in the point spectrum
case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
525
3
Unbounded Control Operators: Hyperbolic Equations
With Control on the Boundary . . . . . . . . . . . . . . . . . . . . . . . . . .
529
1
Introduction and setting of the problem . . . . . . . . . . . . . . . . . .
529
2
Main results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
530
3
Some result for general semigroups . . . . . . . . . . . . . . . . . . . . . .
534
A
An Isomorphism Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
537
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
541
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
569

List of Figures
1.1
Sector Sω,θ0 and path of integration γε,θ. . . . . . . . . . . . . . . . . . .
109
1.2
Paths of integration γε,θ and γ2ε,θ′. . . . . . . . . . . . . . . . . . . . . . . .
111
1.3
Set Σ contained in ρ(A) with K = ∥S′(t0)∥. . . . . . . . . . . . . . . .
117
1.4
Path of integration γ1 ∪γ2 ∪γ3. . . . . . . . . . . . . . . . . . . . . . . . . . .
118
4.1
The functions u, es
+u, and es
−u. . . . . . . . . . . . . . . . . . . . . . . . . . .
253

Introduction
1 Scope of the book
The primary concern of this book1 is the control of linear inﬁnite dimensional
systems, that is, systems whose state space is inﬁnite dimensional and its
evolution is typically described by a linear partial diﬀerential equation, linear
functional diﬀerential equation or linear integral equation.
We focus on two aspects of the control problem:
(i) qualitative properties such as stability, controllability, and observability;
(ii) optimal feedback control of such systems when the performance is mea-
sured by a quadratic cost criterion, both over a ﬁnite and an inﬁnite time
interval.
However, before such a study can be carried out, a detailed investigation
of the problem of representation of inﬁnite dimensional systems is needed.
This is done in Part II of the book, and Parts III to V are devoted to a study
of the control problem. This endeavor is initiated by a broad review of ﬁnite
dimensional systems in Part I so that the reader understands the background
from which the inﬁnite dimensional theory is developed and also obtains an
1 The numbering of equations, theorems, propositions, lemmas, corollaries, deﬁ-
nitions, examples, notations, and remarks is by chapter. When a reference to
another chapter within a part is necessary, it is always followed by the words in
Chapter and the number of the chapter: for instance “in equation (2.1) of Chap-
ter 3”. When a reference to another chapter in a diﬀerent part is necessary, it is
always followed by the words of Chapter, the number of the chapter, of Part, and
the number of the part: for instance “equation (2.1) in Chapter 1 of Part IV”.
The text of theorems, propositions, lemmas, and corollaries is slanted; the text of
deﬁnitions, examples, notations, and remarks is normal shape ended by a square
⊓⊔. The bibliography is by author in alphabetical order. For each author or group
of coauthors, there is a numbering in square brackets starting with one. A refer-
ence to an item by a single author is of the form J. L. Lions [4], and a reference
to an item with several coauthors R. Bellman and K. L. Cooke [1].

2
Introduction
appreciation of the substantial conceptual and technical diﬃculties that had
to be overcome to give a satisfactory treatment of the subject in the inﬁnite
dimensional context.
2 From ﬁnite to inﬁnite dimensional sytems
It is the purpose of this introductory chapter to give an idea of the problematic
and explain the reason for the necessary mathematical development in func-
tional analysis and diﬀerential equations that is carried out in this book. In
the process we provide both a qualitative summary of the diﬀerences between
ﬁnite dimensional systems and inﬁnite dimensional systems as well as provide
a detailed outline of the book. It should be mentioned that technological prob-
lems of importance such as the control of large-space structures, analysis and
control of plasma fusion, and chemical process control where signiﬁcant time
delays are present need an understanding of the theory presented in this book
for their rigorous analysis. We do not present the frequency domain point of
view in this book. If ﬁnite dimensional theory is any guide, then the engineer-
ing solution of technological problems will undoubtedly require a synthesis of
the time and frequency domain viewpoints.
To get an appreciation of the technical problems associated with the con-
trol of inﬁnite dimensional systems, consider the distributed control of wave
motion. Let y(x, t) denote the transverse displacement at time t ≥0 of a
vibrating medium in an n-dimensional bounded open region Ωwith smooth
boundary ∂Ω⊂Rn. Let us assume that
y(x, t) = 0
for x ∈∂Ω
and
t > 0
and let the initial data at time t = 0 be
y(x, 0) = y0
and
∂
∂ty(x, 0) = y1,
for some suﬃciently smooth functions y0 and y1 deﬁned on Ω. Consider the
partial diﬀerential equation
∂2y
∂t2 (t) + Ay(t) = Bu(t)
(2.1)
or the equivalent ﬁrst-order system
⎧
⎪
⎨
⎪
⎩
∂y
∂t (t) −z(t) = 0,
∂z
∂t (t) + Ay(t) = Bu(t),
(2.2)
where the operator

2 From ﬁnite to inﬁnite dimensional sytems
3
A
def
= −
N

i,j=1
∂
∂xi

aij(x) ∂
∂xj

+ a0(x),
a0(x) ≥α > 0
is a uniformly elliptic operator with C∞-coeﬃcients in Ω, that is, aij ∈
C∞(Ω).2 Let the controllers u(·) ∈L2(0, ∞; Rm) and let
B(x)
def
= [B1(x) · · · Bm(x)]
be an n × m matrix with each column in C∞(Ω). We consider the state space
2 Let Ωbe an open subset of Rn. Denote by C(Ω) or C0(Ω) the space of continuous
functions from Ωto R, and for an integer k ≥1,
Ck(Ω)
def
=
n
f ∈Ck−1(Ω) : ∂αf ∈C(Ω), ∀α, |α| = k
o
,
where α = (α1, . . . , αN) ∈NN is a multi-index, |α| = α1 + · · · + αN is the order
of the derivative, and
∂αf
def
=
∂|α|f
∂xα1
1 . . . ∂xαN
N
.
(2.3)
By convention ∂0f will be the function f in order to make sense of the case α = 0.
When |α| = 1, we also use the standard notation ∂if or ∂f/∂xi. Dk(Ω) or Ck
c (Ω)
(resp., D(Ω) or C∞
c (Ω)) will denote the space of all k-times (resp., inﬁnitely)
continuously diﬀerentiable functions with compact support contained in the open
set Ω.
A function f : Ω→R is uniformly continuous if, for each ε > 0, there exists
δ > 0 such that for all x and y in Ωsuch that |x−y| < δ we have |f(x)−f(y)| < ε.
If a function f is bounded and uniformly continuous on Ω, it possesses a unique,
continuous extension to the closure Ωof Ω. Denote by Ck(Ω) the space of functions
f in Ck(Ω) for which ∂αf is bounded and uniformly continuous on Ωfor all α,
0 ≤|α| ≤k. A function f in Ck(Ω) is said to vanish at the boundary of Ωif for
every α, 0 ≤|α| ≤k, and ε > 0, there exists a compact subset K of Ωsuch that,
for all x ∈Ω∩∁K, |∂αf(x)| ≤ε. Denote by Ck
0 (Ω) the space of all such functions.
Clearly Ck
0 (Ω) ⊂Ck(Ω) ⊂Ck(Ω). Endowed with the norm
∥f∥Ck(Ω)
def
=
max
0≤|α|≤k sup
x∈Ω
|∂αf(x)|,
(2.4)
Ck
0 (Ω) and Ck(Ω) are Banach spaces. Finally
C∞(Ω)
def
=
\
k≥0
Ck(Ω), C∞(Ω)
def
=
\
k≥0
Ck(Ω), and C∞
0 (Ω)
def
=
\
k≥0
Ck
0 (Ω).
When f is a vector function from Ωto Rm, the corresponding spaces will be
denoted Ck
0 (Ω)m or Ck
0 (Ω, Rm), Ck(Ω)m or Ck(Ω, Rm), Ck(Ω)m or Ck(Ω, Rm),
etc.
In dimension one, n = 1, and for a bounded open interval ]a, b[, the notation
and the deﬁnition of C(]a, b[) coincide with the ones of C([a, b]). For simplicity
we shall also often write C(a, b) with the implicit convention that C(a, b) stands
for C([a, b]).

4
Introduction
w
def
=
y
z
	
∈W
def
= H1
0(Ω) × L2(Ω),
where H1
0(Ω) denotes the usual Sobolev space of L2-functions, with deriva-
tives, in the distribution sense, belonging to L2(Ω) and vanishing at the bound-
ary. Then (2.2) can be formally written as
dw
dt (t) + ˜Aw(t) = ˜Bu(t),
(2.5)
where
˜A
def
=

0 −I
A
0
	
and
˜B
def
=

0
B
	
.
Here ˜B ∈L(Rm; W) and the operator ˜A is an unbounded operator with a
dense domain
D( ˜A) =

H1
0(Ω) ∩H2(Ω)

× H1
0(Ω) ⊂W.
Because of the presence of the unbounded operator ˜A, it is clear that the
concept of a solution for (2.5) is not immediate. Intuitively, if we want (2.5)
to have a classical solution, then we would need
w0 =
y0
z0
	
∈D( ˜A)
and u(·) ∈C1(Ω). On the other hand, −˜A generates a strongly continuous
semigroup S(t) on W, i.e., S(t) is a bounded linear operator on W satisfying
(i) S(0) = I.
(ii) S(t1 + t2) = S(t1)S(t2),
0 < t1, t2 < ∞.
(iii) t 	→S(t)w: [0, ∞) →W is continuous for each w ∈W.
Then we may say that w(t) is a solution of (2.5) if it satisﬁes
w(t) = S(t)w0 +
 t
0
S(t −s) ˜Bu(s) ds,
(2.6)
where the integral is interpreted in the Bochner sense.
Now, given a strongly continuous semigroup S(t) on X (say a Banach
space), it has a unique inﬁnitesimal generator A, a closed unbounded operator
that is deﬁned on some dense domain D(A). The converse question, namely,
when does A generate a strongly continuous semigroup, is a far more diﬃcult
issue and is the content of the Hille–Yosida theorem. Furthermore, there are
diﬀerent kinds of semigroups, and each plays an important role in the study of
partial diﬀerential and functional diﬀerential equations. We give an informal
listing of some of these below.

2 From ﬁnite to inﬁnite dimensional sytems
5
(i) Diﬀerentiable: S(t)X ⊂D(A) for t > 0. Then S(t)x ∈C∞(X) for all
t > 0 and each ﬁxed x ∈X. Partial diﬀerential equations of diﬀusion type
on a bounded domain with homogeneous Dirichlet boundary conditions
give rise to such semigroups. (§2.8, Chapter 1 of Part II).
(ii) Analytic: S(t) is diﬀerentiable and admits an analytic extension into the
complex t-plane satisfying certain bounds. Abstract parabolic equations in
variational form give rise to such semigroups. (§2.7, Chapter 1 of Part II).
(iii) S(t) is a group (orthogonal or unitary). The operator −˜A considered pre-
viously generates a group. This is the content of Stone’s theorem. (Theo-
rem 2.9, Chapter 1 of Part II).
(iv) S(t) is a compact operator for t large. Hereditary diﬀerential systems
give rise to such semigroups. An extensive study of hereditary diﬀerential
systems is carried out in Chapter 4 of Part II.
(v) Contraction ∥S(t)∥≤1 on 0 ≤t < ∞. Dissipative operators give rise
to such semigroups. This is the content of the Lumer–Phillips theorem
(Theorem 2.6, Chapter 1 of Part II).
An important problem in the control of linear systems is the study of its
stability. Consider the stability problem for the linear system
dy
dt (t) = Ay(t),
A: Rn →Rn.
(2.7)
It is well known that the following are equivalent:
(a) (2.7) is asymptotically stable.
(b) (2.7) is exponentially stable.
(c) ∀y0 ∈Rn,
 ∞
0
|eAty0|2
Rn dt < ∞.
The stability of (2.7) can be tested by looking for a quadratic Lyapunov
function:
V (x) = (x, Px)Rn,
P = P ∗> 0.
This leads to a study of the Lyapunov equation
A∗P + PA = −Q,
Q > 0,
(2.8)
and if A is a stability matrix Re λ < 0, then the solution of (2.8) exists and
can be written as
P =
 ∞
0
eA∗tQeAt dt,
(2.9)
and conversely if (2.8) has a positive deﬁnite solution, then A is a stability
matrix.
The corresponding question of inﬁnite dimensional systems
dy
dt (t) = Ay(t),
y(t) ∈X a Banach space
(2.10)
and A the inﬁnitesimal generator of a strongly continuous semigroup S(t)
leads to a study of the asymptotic behavior of S(t) as t goes to inﬁnity. The

6
Introduction
theorems characterizing the equivalence of conditions (a) and (c) above and
related issues are discussed in §2.2 of Chapter 1 of Part II, where Theorem 2.2
is perhaps the most important. The Lyapunov method leading to the study of
the Lyapunov equation is the content of Theorem 2.4. The relation between
the spectrum of A and the stability of the semigroup S(t) is subtle (see §2.9,
Chapter 1 of Part II).
In general, the questions related to the generation of semigroups receive a
thorough description in §2, Chapter 1 of Part II.
As we have just discussed, not only do we have to deal with homoge-
neous equations of the form (2.10), where A is the inﬁnitesimal generator of
a semigroup, but we also have to deal with nonhomogeneous equations
dy
dt (t) = Ay(t) + f(t),
where f ∈Lp(0, T ; X).
(2.11)
When the nonhomogeneous equation arises from a feedback control prob-
lem, we have to consider equations of the form
⎧
⎨
⎩
dy
dt (t) = Ay(t) + F(t)y(t) + f(t)
on [0, T ],
y(0) = y,
(2.12)
where A is the inﬁnitesimal generator of a strongly continuous semigroup and
F : [0, T ] →L(X) is strongly continuous (L(X) is the space of continuous
linear maps from X to X). This equation leads to the consideration of a
perturbation of the inﬁnitesimal generator A.
There are now many concepts of a solution (see Deﬁnition 3.1, Chapter 1
of Part II). Existence and uniqueness of solutions as well as equivalence of
solutions is discussed in §3.1 to §3.5 of Chapter 1 of Part II. The key ingre-
dient in proving the various existence and uniqueness results is the Yosida
approximation.
The question of regularity of solutions is discussed in §3.6 and §3.7. In
§3.6, the assumption is made that X is a Hilbert space and A is the generator
of an analytic semigroup and we concern ourselves with maximal regularity,
namely, that
dy
dt
and
Ay
have the same regularity as f. Consideration of the nonzero initial condition
case leads to the study of interpolation spaces (studied in detail in §4, Chap-
ter 1 of Part II) and the main Isomorphism Theorem (Theorem 3.1, Chapter 1
of Part II). Regularity can also be studied in C([0, T ]; X)3 with X a Banach
space, and this is carried out in §3.7.
We often have to study nonhomogeneous equations
3 C([0, T ]; X) is the linear space of continuous functions f : [0, T ] →X endowed
with the sup norm. It will also be denoted as C(0, T; X).

2 From ﬁnite to inﬁnite dimensional sytems
7
dy
dt (t) = Ay(t) + f(t)
(2.13)
on the inﬁnite time interval [0, ∞[. These situations arise from consideration
of feedback control problems and related stability issues. When A has a pure
point spectrum and the inﬁnite part of the spectrum is stable and the unstable
part is ﬁnite dimensional, these equations can be eﬀectively studied in [0, ∞[
(§3.9, Chapter 1 of Part II).
In Parts IV and V of the book where we study optimal control problems
with a quadratic cost function where control is exercised through the bound-
ary (or pointwise), we shall need the concept of fractional powers of closed
operators and the semigroups they might generate. This is the subject of §5
and §6 of Chapter 1 of Part II.
Control of partial diﬀerential equations where control is exercised through
the boundary leads to considerable technical diﬃculties. To obtain an under-
standing of these diﬃculties, consider again a vibrating ﬂexible string
∂2y
∂t2 = ∂2y
∂x2
(2.14)
for the transversal displacement
y(t, x)
on 0 ≤x ≤1, t ≥0.
Assume that
y(0, t) = 0
on t ≥0
and control is exercised through the end point x = 1
y(1, t) = u(t),
t ≥0.
(2.15)
We would like to control the pair

y(·, t), dy
dt (·, t)

so that the initial state
y(x, 0) = y0(x),
dy
dt (x, 0) = v0(x)
on 0 ≤x ≤1 is brought to
y(x, T) = 0,
dy
dt (x, T) = 0
on 0 ≤x ≤1
(2.16)
in some ﬁnite time T > 0.
We try to transform the system to a distributed control framework
dw
dt (t) = ˜Aw(t) + ˜Bu(t)
(2.17)

8
Introduction
by deﬁning the state w(·) and the operators ˜A and ˜B appropriately. For this
purpose, deﬁne
Y (x, t) = y(x, t) −
 1
1−
δ1(x)u(t)(x) dx
(2.18)
where the Dirac measure δ1 has a unit weight at x = 0. Then formal calcula-
tions will show that
y(1, t) −y(1−, t) = δ1u(t),
or equivalently,
y(1, t) = u(t)
with y(1−, t) = 0.
Hence we obtain the distributed control system
∂2Y
∂t2 = ∂2Y
∂x2 −δ′
1u(t)
(2.19)
(δ′
1 is the distributional derivative of δ1) with the boundary conditions
Y (0, t) = 0
and
Y (1−, t) = 0
on t ≥0,
and this can be formally put in the form (2.17).
In general, boundary control problems for partial diﬀerential equations
lead to boundary operators B that are unbounded and may involve distribu-
tions as in this example. The dual of boundary control problems are bound-
ary observation problems, and in the most general situation, both control
and observation are exercised through the boundary leading to control and
observation operators that are both unbounded.
In some sense, when control of partial diﬀerential equations is exercised
through the boundary, the parabolic case (the corresponding semigroup is an-
alytic) and the hyperbolic case (the corresponding semigroup is a group) have
to be treated using diﬀerent methods. Chapters 2 and 3 deal with the control
of parabolic systems when control and observation is exercised through the
boundary, leading to control and observation operators that are unbounded.
Chapter 2 of Part II is a summary exposition of variational theory of para-
bolic systems. This theory can also be applied to certain wave equations where
damping is present. A systematic exposition of this method can be found in
J. L. Lions and E. Magenes [1], and this theory was extensively used by
Lions in his book Control of Partial Diﬀerential Equations (cf. J. L. Li-
ons [3]). The main reason for including this chapter in the book is to explain
the Method of Transposition, which is later used in various parts of the book
(for example, in the study of delay systems in Chapter 4 of Part II and in the
study of controllability in Chapter 1 of Part III). The idea of the Method of
Transposition is to deﬁne an integral or weak version of the original equation
involving the adjoint of the operator A, using Green’s formula and integration
by parts (§2.3 and in particular (2.5) of this chapter) and then to obtain a
smooth adjoint isomorphism of the form

2 From ﬁnite to inﬁnite dimensional sytems
9
y 	→

A∗y −dy
dt , y(T )

between suitable spaces. It should be noted that the Method of Transposition
when combined with the change of variable idea of the previous chapter allows
one to treat parabolic systems with control exercised via Dirichlet boundary
conditions. As the boundary control operator becomes “rougher” (e.g., Dirich-
let case), the solution also becomes “rougher” and it becomes more diﬃcult
to perform a “smooth” observation. The balance between the relative “rough-
ness” or “unboundedness” of the control and observation operators is a key
technical issue in the solution of the quadratic control problem as we shall see
in Parts IV and V.
Chapter 3 of Part II is concerned with semigroup methods, and one of the
main theorems in this chapter is an Isomorphism Theorem (Theorem 2.3),
which uses regularity results for analytic semigroups and interpolation spaces
(§4 of Chapter 1). The other key idea used in this chapter is the use of a
change of variable to deﬁne a new state and make sense of the state space
system as an input–output map (see Theorem 2.3 and §3.4, Chapter 3).
The ﬁnal chapter of this part of the book is concerned with the represen-
tation problem for linear functional diﬀerential equations with general control
and observation mechanisms. A fairly complete modern treatment of this sub-
ject that covers both retarded and neutral Functional Diﬀerential Equations
(FDE), diﬀerence equations, integro-diﬀerential equations, and other equa-
tions with a hereditary structure is now available (cf. M. C. Delfour and
J. Karrakchou [1, 2]) . They can now all be treated in the same frame-
work. However, we have chosen to concentrate on FDEs of the retarded type
to provide a better understanding of the basic ideas and constructions, but
everything is readily extendable to more general hereditary structures. The
prototype problem here can be written as
⎧
⎪
⎪
⎨
⎪
⎪
⎩
dx
dt (t) =
N

i=0
[x(t −i) + u(t −i)] ,
y(t) = a0x(t) + a1x(t −N) + b0u(t) + b1u(t −N),
(2.20)
where u(·) denotes the control variable and y(·) denotes the observation. Two
questions need to be resolved to obtain a “current” theory of existence, unique-
ness, and representation of solutions to such systems. The ﬁrst is the choice
of an appropriate function space for initial conditions, and the second is the
choice of a minimal state space for these systems. The choice of initial con-
ditions, namely space of continuous functions C(−h, 0; Rn) and the product
space M p(−h, 0; Rn) = Rn × Lp(−h, 0; Rn), and the corresponding questions
of existence and uniqueness of solutions in appropriate function spaces are
discussed in §3 of Chapter 4 of Part II. §4 is concerned with the state space
representation of FDEs. The key concept of structural operators (§4.4), which
intuitively describes the way the systems combine and transform initial condi-
tions over the initial interval is introduced here. A complete modern treatment

10
Introduction
of the various adjoint systems and semigroups arising out of the original sys-
tem is described in this section. Finally, §5 and §6 give a state space treatment
of linear FDEs of the retarded type when control and observation in various
forms are present. All this readily extends to other types of systems with a
delay structure. It should be mentioned that the state space point of view is
essential for the study of optimal control problems considered in Parts III to
V.
Notes: some related books on the control of linear
systems that have appeared since 1992
Several books related to this one have appeared in the ensuing period between
the publication of the ﬁrst edition of the book and its current revision.
In optimal control, the two-volume book of I. Lasiecka and R. Trig-
giani [15,16] that appeared in 2000 covers in greater detail and provides new
results on optimal control of parabolic and hyperbolic systems with quadratic
cost functions. The book of X. Li and J. Yong [1] published in 1995 is con-
cerned with optimal control of partial diﬀerential equations, and in the last
chapter, the author discusses linear quadratic optimal control problems, both
over a ﬁnite and an inﬁnite time horizon.
The books of R. F. Curtain and H. Zwart [1] published in 1995 and
O. J. Staffans [2] in 2005 are concerned with state space realization the-
ory of linear input–output inﬁnite dimensional systems as well as qualitative
properties such as stability, controllability, and observability. The book by
Staﬀans has considerable overlap with our treatment of semigroup theory as
well as representations of partial diﬀerential equations in terms of semigroups.
Staﬀans’ work also presents a detailed treatment of the theory of B. Sz.-Nagy
and C. Foias [7,8] on contractive semigroups and their unitary dilations from
1965 to 1967. These ideas are intimately connected with scattering theory as
developed by P. Lax and R. S. Phillips [1]. Finally, the systems point of
view as exempliﬁed in the Russian work of V. M. Adamjan, D. Z. Arov,
and M. G. Kre˘ın [1 to 5] receives a detailed treatment here.
The book of H. O. Fattorini [5] published in 1999 also deals with opti-
mal control of nonlinear partial diﬀerential equations via the Maximum Prin-
ciple. Finally, the reader is referred to the book of A. V. Fursikov and
O. Yu. Imanuvilov [1] in 1996 that studies null controllability via Carle-
man estimates.
We have not cited other references because we conﬁned ourselves to linear
control problems.

Part I
Finite Dimensional Linear Control Dynamical
Systems

1
Control of Linear Diﬀerential Systems
1 Introduction
This Part I serves the purpose of an introduction to Parts III to V of the book,
which are mainly concerned with the quadratic cost optimal control problem
for distributed parameter systems and systems with time delay, both over a
ﬁnite and an inﬁnite time interval. For problems over a ﬁnite time interval, the
main tool used is Dynamic Programming, which leads to a Hamilton–Jacobi
equation for the value function. For the class of control problems considered,
the Hamilton–Jacobi equation can be explicitly solved via the study of an
operator Riccati equation. The study of the operator Riccati equation when
control is exercised through the boundary in the case of distributed parameter
systems or when delays are present in the control in the case of systems with
time delay poses additional technical diﬃculties. The results of Part II are
needed to overcome these diﬃculties. For problems over an inﬁnite time inter-
val, the concepts of controllability and observability (and the weaker concepts
of stabilizability and detectability) play an essential role in the development
of the theory.
Problems of optimal control with a quadratic cost function, not necessarily
deﬁnite, are of interest in H∞-theory and diﬀerential games. These problems
are related to the theory of dissipative systems. The last two sections of this
chapter present an introduction to these problems.
2 Controllability, observability, stabilizability, and
detectability
In this section we present the theory of controllability and observability for
ﬁnite dimensional linear systems in a manner that has implications in the
study of controllability and observability for inﬁnite dimensional systems. In
particular, we show that if the system is controllable, then the transfer from

14
I-1 Control of Linear Diﬀerential Systems
the zero-state to any other state can be carried out by means of minimum
energy controls. It is the same idea, which is later used in the theory of exact
controllability of hyperbolic (second order) systems. The mathematical tech-
niques needed to accomplish it, however, are far more diﬃcult. Throughout
this book N denotes the set of integers and R and C the respective ﬁelds of
real and complex numbers. In this chapter the norm in Rd will be denoted | · |
irrespective of the dimension d of the space. We shall use ∥· ∥to denote the
norm in various Hilbert spaces, (·, ·) to denote scalar product and ∗denotes
the adjoint of an operator (and also the dual space).
Consider the linear ﬁnite dimensional control system in the interval [0, T ]:
⎧
⎨
⎩
dx
dt (t) = Ax(t) + Bu(t) in [0, T ],
y(t) = Cx(t)
(2.1)
where the state x(t) ∈Rn, the control functions

t 	→u(t)

∈L2(0, T ; Rm),
the output y(t) ∈Rp, and A ∈L(Rn; Rn), B ∈L(Rm; Rn), and C ∈
L(Rn; Rp).
We assume that the initial state x(0) = 0. The solution of the linear
diﬀerential equation (2.1) can be written as
Ltu
def
= x(t; u) =
 t
0
eA(t−s)Bu(s) ds,
(2.2)
where Lt is a linear bounded transformation from L2(0, t; Rm) into Rn.
2.1 Controllability
Deﬁnition 2.1. We say that the system is controllable (from 0 ∈Rn) in [0, T ]
if given any ¯x ∈Rn, there exists a control function u(·) ∈L2(0, T ; Rm) such
that
x(T ; u) = ¯x.
⊓⊔
Controllability in [0, T ] is therefore equivalent to the surjectivity of the map
u 	→LT u. We now give necessary and suﬃcient conditions for controllability
using elementary facts from the theory of linear transformations on Hilbert
spaces.
Let H and K be Hilbert spaces, and let A ∈L(H; K). Let A∗∈L(K∗; H∗)
be the adjoint operator. In the sequel we identify H and H∗and K and K∗.
Let R(A) denote the range of the operator A and N(A) denote the null space
of A. ⊥denotes the orthogonal complement of a closed subspace.
Proposition 2.1. The following relations are true:
(i) R(A) ⊆N(A∗)⊥and R(A) = N(A∗)⊥⇐⇒R(A) is closed.
(ii) R(A∗) ⊆N(A)⊥and R(A∗) = N(A)⊥⇐⇒R(A) is closed.
(R(A) closed ⇐⇒R(A∗) is closed.)

2 Controllability, observability, stabilizability, and detectability
15
(iii) N(A) = R(A∗)⊥.
(iv) N(A∗) = R(A)⊥.
(v) Furthermore
R(A) is closed ⇐⇒∃c > 0 such that ∥h∥≤c∥Ah∥,
∀h ∈H.
Using the above, we can prove the following proposition.
Proposition 2.2. The following are true:
(i) R(A) is dense in K ⇐⇒N(A∗) = {0}.
(ii) N(A) = {0} ⇐⇒R(A∗) is dense in H.
(iii) R(A) is dense in K ⇐⇒AA∗: K →K satisﬁes AA∗> 0.
(iv) N(A) = {0} ⇐⇒A∗A: H →H satisﬁes A∗A > 0.
(v) A ∈L(H; K) is invertible ⇐⇒R(A) = K, N(A) = {0}
⇐⇒∃c > 0, such that ∥h∥≤c∥Ah∥,
∀h ∈H,
⇐⇒∃c > 0, such that ∥k∥≤c∥AA∗k∥,
∀k ∈K.
Remark 2.1. Much of the above extends to operators A that are densely de-
ﬁned and closed to spaces H and K, which are Banach spaces. For proofs of
these facts, see, for example, M. Schechter [1]. For a use of these ideas in
a systems context, see M. C. Delfour and S. K. Mitter [7].
⊓⊔
We can immediately use the above to get a necessary and suﬃcient con-
dition for controllability. Using (2.2) and noting that
L∗
T : Rn →L2(0, T ; Rm)
: y 	→B∗e(T −·)y,
we get
W(0, T )
def
= LT L∗
T =
 T
0
eA(T −s)BB∗eA∗(T −s) ds.
(2.3)
Hence we have proved.
Theorem 2.1. The system (2.1) is controllable in [0, T ] if and only if W(0, T ) >
0 (positive deﬁnite).
We can transform this criteria of controllability to an algebraic criterion by
noting that
N

W(0, T )

= N(C),
(2.4)
where
C =

B...AB... · · · ...An−1B
	 
B...AB... · · · ...An−1B
	∗
.
(2.5)
This can be proved as follows.
Let x ∈N

W(0, T )

. Then

16
I-1 Control of Linear Diﬀerential Systems
0 =

x, W(0, T )x

=
 T
0
|B∗eA∗(T −t)x|2 dt,
and hence B∗eA∗(T −t)x = 0, ∀t ∈[0, T ].
Diﬀerentiating (n −1)-times with respect to t and setting t = 0, we get
B∗x = 0,
B∗A∗x = 0, . . . , B∗A∗n−1x = 0,
and hence
x ∈N(C).
Conversely, suppose x ∈N(C). By the Cayley–Hamilton theorem,
eA(T −t) =
n−1

i=0
αi(T −t)Ai.
Hence
x∗W(0, T ) =
 T
0
n−1

i=0
αi(T −t)x∗AiB

B∗eA∗(T −t) dt.
Now x∗AiB = 0, i = 0, . . . , n −1, because x ∈N(C). Therefore,
x∗W(0, T ) = 0 and because W(0, T ) is symmetric,
x ∈N

W(0, T )

.
Hence using the fact that W(0, T ) is a symmetric matrix
R

W(0, T )

= R(C).
(2.6)
But
R(C) = R([B...AB... · · · ...An−1B]),
(2.7)
and hence we have proved the following theorem.
Theorem 2.2. The following conditions are equivalent:
(i) System (2.1) is controllable in [0, T ].
(ii) W(0, T ) > 0 (positive-deﬁnite).
(iii) rank

B...AB... · · · ...An−1B
	
= n.
When the system is controllable, we shall refer to (A, B) as a controllable pair.
It is of some interest to establish that if the system is controllable, then
we can transfer the state x0 at time t0 to the state x1 at time t1 by using a
control u∗, which has the least energy
 t1
t0
|u∗(s)|2 ds

2 Controllability, observability, stabilizability, and detectability
17
among all controls that transfer the phase (x0, t0) to the phase (x1, t1).
To establish this, let us use the notation
Φ(t, s) = eA(t−s).
Introduce the adjoint system
⎧
⎨
⎩
dp
dt (t) = −A∗p(t) in [t0, t1[,
p(t1) = η,
(2.8)
where η will be speciﬁed later. The solution of (2.8) is given by
p(t) = Φ∗(t1, t)η.
(2.9)
Now choose the control
u∗(t) = −B∗p(t) = −B∗Φ∗(t1, t)η.
(2.10)
If the control is to transfer (x0, t0) to (x1, t1), we must have
x1 = x(t1) = Φ(t1, t0)x0 −
 t1
t0
Φ(t1, t)BB∗Φ∗(t1, t) dt η,
and hence if the system is controllable and we choose
η = W −1(t0, t1)[Φ(t1, t0)x0 −x1],
the desired transfer will be eﬀected. Let us now prove that u∗(t) is the mini-
mum energy control.
Let ¯u(t) be some other control that eﬀects the desired transfer. Then
x1 = Φ(t1, t0)x0 +
 t1
t0
Φ(t1, t)B¯u(t) dt
= Φ(t1, t0)x0 +
 t1
t0
Φ(t1, t)Bu∗(t) dt.
Therefore
 t1
t0
Φ(t1, t)B[¯u(t) −u∗(t)] dt = 0.
Hence
 t1
t0
([¯u(t) −u∗(t)], −B∗Φ∗(t1, t)η) dt = 0
and
 t1
t0

¯u(t) −u∗(t), u∗(t)

dt = 0.

18
I-1 Control of Linear Diﬀerential Systems
Therefore
 t1
t0
|¯u(t)|2 dt =
 t1
t0
|¯u(t) −u∗(t) + u∗(t)|2 dt
=
 t1
t0
|u∗(t)|2 dt +
 t1
t0
|¯u(t) −u∗(t)|2 dt
>
 t1
t0
|u∗(t)|2 dt
if
¯u ̸= u∗.
Suppose now that U and X are inﬁnite dimensional Hilbert spaces,
A: X
→X is an unbounded linear operator with domain D(A), and
B : U →X is a bounded linear operator. Then there is a diﬃculty with car-
rying through the arguments given earlier (for example, R(LT ) need not be
a closed subspace of X). Nevertheless, R(LT ) can be given the structure of a
Hilbert space with continuous injection in X (Proposition 2.1 in Chapter 1 of
Part II). Furthermore, there is a need to make a distinction between approx-
imate controllability (R(LT ) is dense in X) and exact controllability (whose
deﬁnition is somewhat subtle, see Deﬁnition 2.2 in Chapter 1 of Part III);
indeed R(LT ) = X will usually not hold. However, an abstract criterion for
exact controllability is related to an estimate on LT L∗
T (see Proposition 2.2;
this chapter). It is also of interest to note that the minimum energy control
viewpoint of verifying exact controllability as outlined in this section extends
to inﬁnite dimensional situations, but of course at the cost of using much
more elaborate technical machinery. Chapter 1 of Part III gives a treatment
of exact controllability for parabolic and hyperbolic partial diﬀerential equa-
tions when control is exercised in a distributed manner and when control is
exercised through the boundary.
2.2 Observability
We now discuss the dual concept of observability. For this purpose we may
assume that the input is known, and because we are dealing with linear time-
invariant ﬁnite dimensional systems, we may take u(t) ≡0, t ∈[t0, ∞[. Con-
sider therefore the system
⎧
⎨
⎩
dx
dt (t) = Ax(t),
y(t) = Cx(t),
(2.11)
and let the initial state x(t0) = x0, for some arbitrary x0.
The solution of (2.11) can be written as
y(t) = CeA(t−t0)x0.
(2.12)
Deﬁnition 2.2. The system (2.1) is said to be observable in [t0, t1] if the map

2 Controllability, observability, stabilizability, and detectability
19
O: Rn →L2(t0, t1; Rp): x0 →y(·) = CeA(· −t0)x0
is injective.
⊓⊔
The deﬁnition expresses the fact that we can recover uniquely the initial
state from a knowledge of the output y(·) in the time interval [t0, t1]. The
interval [t0, t1] can in fact be taken to be arbitrarily small. Now,
O is injective ⇐⇒N(O) = {0} ⇐⇒N(O∗O) = {0}.
An easy computation shows that
Σ(t0, t1)
def
= O∗O =
 t1
t0
e(t1−t)A∗C∗Ce(t1−t)A dt,
and clearly
N(O∗O) = {0} ⇐⇒Σ(t0, t1) > 0 (positive-deﬁnite).
It is also easy to show that
N(O∗O) = N([C∗...A∗C∗... · · · ...A∗n−1C∗]∗[C∗...A∗C∗... · · · ...A∗n−1C∗]),
= N([C∗...A∗C∗· · · A∗n−1C∗])
and hence, we have proved the following theorem.
Theorem 2.3. The system (2.1) is observable if and only if
rank[C∗...A∗C∗... · · · ...A∗n−1C∗] = n.
When the system is observable, we shall refer to (A, C) as an observable
pair.
2.3 Duality
There is a formal duality between the concepts of controllability and observ-
ability. For this purpose, we introduce the dual system
⎧
⎨
⎩
dξ
dt (t) = −A∗ξ(t) −C∗γ(t),
η(t) = −B∗ξ(t),
(2.13)
which evolves backward in time.
Then mathematically,
• The system (2.1) is observable ⇐⇒The system (2.13) is controllable.
• The system (2.13) is observable ⇐⇒The system (2.1) is controllable.

20
I-1 Control of Linear Diﬀerential Systems
In this book we do not study observability of partial diﬀerential equations
but exploiting the above duality ideas, much of the theory of controllability
developed in Chapter 1 of Part III can be used to develop a theory of observ-
ability for partial diﬀerential equations (even with unbounded observation
operators).
Conceptually, controllability of a system permits the choice of feedback
controls resulting in certain desirable (e.g., stability) properties of closed loop
systems, whereas observability of a system permits the design of state estima-
tors with desirable properties. We shall discuss this in a later section.
2.4 Canonical structure for linear systems
In this section we wish to obtain a decomposition (structure) theorem for the
system (2.1) that is not necessarily controllable and observable. Let us denote
by
V := R(C) = R([B...AB... · · · ...An−1B]).
V is the smallest A-invariant subspace containing the image of B. Let us also
denote by
W := N(O) = N([C∗...A∗C∗· · · ...A∗n−1C∗]),
and clearly W is the smallest A∗-invariant subspace containing the image of
C∗.
Now we may write the state space X := Rn as
X = (V ∩W ⊥) ⊕(V ⊥∩W ⊥) ⊕(V ∩W) ⊕(V ⊥∩W).
Then there exists a basis in which we may write the system as
d
dt
⎡
⎢⎢⎣
¯x1(t)
¯x2(t)
¯x3(t)
¯x4(t)
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
A11 A12 A13
0
0
A22 A23 A24
0
0
A33
0
0
0
A43 A44
⎤
⎥⎥⎦
⎡
⎢⎢⎣
¯x1(t)
¯x2(t)
¯x3(t)
¯x4(t)
⎤
⎥⎥⎦+
⎡
⎢⎢⎣
B1u1(t)
0
B3u3(t)
0
⎤
⎥⎥⎦,
(2.14)
y(t) = [C1
C2
0
0]
⎡
⎢⎢⎣
¯x1(t)
¯x2(t)
¯x3(t)
¯x4(t)
⎤
⎥⎥⎦.
(2.15)
In the above, (A11,B11), (A33,B3) are controllable pairs and (A11,C1), (A22,C2)
are observable pairs.
2.5 The pole-assignment theorem
An important consequence of a linear system possessing the controllability
property is that by means of linear state feedback control, the poles of the
corresponding closed loop system can be placed in arbitrary locations.

2 Controllability, observability, stabilizability, and detectability
21
By state feedback control, we mean that the control law is of the form
u(t) = Kx(t) + v(t),
where K : Rn →Rm and v(·) is a reference input.
Theorem 2.4. Let Σ = {λi ∈C, i = 1, 2, . . . , n} with the proviso that if λi
is complex, then its conjugate ¯λi ∈Σ. Then the system is controllable if and
only if there exists a feedback control u(t) = Kx(t) for some feedback matrix
K such that the spectrum of A + BK is equal to Σ.
Proof. We start by noting two facts:
(i) Controllability is invariant under non-singular linear transformations
of state space. In case m = 1, that is, we are dealing with the system:
dx
dt (t) = Ax(t) + bu(t),
u(t) ∈R
and
b ∈Rn,
then assuming (A, b) is a controllable pair, there exists a non-singular linear
transformation T : Rn →Rn such that the above system transforms to
˜A =
⎡
⎢⎢⎢⎢⎢⎢⎣
0
1
0
. . .
0
0
0
1
...
...
...
... ...
...
0
0
0 . . .
0
1
α1 α2 . . . αn−1 αn
⎤
⎥⎥⎥⎥⎥⎥⎦
,
˜b =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
...
...
...
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
It is then easy to see that if the characteristic polynomial corresponding to
the desired spectrum Σ is
χ = sn −βnsn−1 · · · −β2s −β1,
then the feedback law
˜u(t) = K˜x(t) + v(t),
K = diag(β1 −α1, . . . , βn −αn)
transforms
d˜x
dt = ˜A˜x(t) + ˜b˜u(t)
to
d˜x
dt = ( ˜A + ˜bk)˜x(t) + ˜bv(t)
such that the spectrum of ( ˜A + ˜bK) in Σ.
(ii) Controllability is not destroyed by state feedback. That is
R

B...(A + BK)B... · · · ...(A + BK)n−1B
	
= R

B...AB... · · · ...An−nB
	
.
Now the general case is reduced to the case m = 1 by virtue of the following
lemma.

22
I-1 Control of Linear Diﬀerential Systems
Lemma 2.1. Let 0 ̸= b ∈R(B). If (A, B) is a controllable pair, there exists
a matrix K ∈L(Rm; Rn) such that (A + BK, b) is a controllable pair.
Let b1 = b and let n1 = dimension of the cyclic subspace X1 generated by
(A, b1). Let x1 = b1 and deﬁne xj = Axj−1 + b1(j = 2, . . . , n1). Then the xj,
j = 1, . . . , n1, forms a basis for the cyclic subspace generated by (A, b1). If
n1 < n, choose b2 ∈R(B) such that b2 ̸∈X1. As (A, B) is controllable, such
a b2 exists. Let n2 be the largest integer such that
x1, x2, . . . , xn1,
b2, Ab2, . . . , An2−1b2
are independent.
Deﬁne
xn1+i = Axn1+i−1 + b2,
i = 1, 2, . . ., n2.
Then x1, x2, . . . , xn1+n2 is a basis for the cyclic subspace generated by (A, b1+
b2). Continuing in this way, we eventually get an independent set of vectors
x1, . . . , xn, and
xi+1 = Axi + ˜bi,
i = 1, . . . , n −1,
and ˜bi ∈R(B).
As ˜bi = Bui for some ui ∈Rm (the control space) and the x1, . . . , xn are
independent, there exists a K1 such that
BK1xi = ˜bi,
i = 1, . . . , n,
where ˜bi ∈R(B) is arbitrary. Therefore
(A + BK1)xi = xi+1,
i = 1, 2, . . . , n −1,
and hence
xi = (A + BK1)i−1b,
i = 1, . . . , n.
Therefore (A + BK1, b) is a controllable pair.
Hence the problem has been reduced to the case of n = 1 and the control
law K = K1 + bK∗achieves the desired property.
To prove the theorem in the other direction, let λi, i = 1, . . . , n, be real
and distinct with λi /∈spectrum of A. Choose K ∈L(Rm; Rn) such that
spectrum (A+BK) = {λ1, . . . , λn}. Let xi, i = 1, . . . , n, be the corresponding
eigenvectors. Hence
xi = (λiI −A)−1BKxi,
i = 1, 2, . . . , n.
Now
(λI −A)−1 =
n

j=1
ρj(λ)Aj−1,
where ρj(λ) are rational functions deﬁned on the complement C\σ(A) of the
spectrum σ(A) of A. Hence

2 Controllability, observability, stabilizability, and detectability
23
xi =
n

j=1
ρj(λi)Aj−1BFxi ∈R([B...AB... · · · ...An−1B]).
Now the xi’s span Rn and hence R([B...AB... · · · ...An−1B]) = n.
⊓⊔
No satisfactory result similar to the pole-assignment theorem is known in
inﬁnite dimensions. It would be interesting to prove a result of this kind for
delay systems
⎧
⎪
⎪
⎨
⎪
⎪
⎩
dx
dt (t) = A1x(t) + A2x(t −h) + Bu(t),
x(0) = φ0 ∈Rn,
x(0) = φ1(θ) a.e. θ ∈[−h, 0),
φ1 ∈L2(−h, 0; Rn).
Corollary 2.1. If the system is controllable, then there exists a feedback con-
trol u(t) = Kx(t) such that the closed loop system
dx
dt (t) = (A + BK)x(t)
is asymptotically stable.
2.6 Stabilizability and detectability
The structure theorem of linear systems and the pole-assignment theorem
motivate the introduction of the concepts of stabilizability and detectability.
Consider the input–state part of the control system (2.1):
dx
dt (t)(t) = Ax(t) + Bu(t).
Then from the structure theorem, we know that there is a coordinate system
in which the above system has a representation
d
dt

¯x1
¯x2
	
(t) =
 ¯A11 ¯A12
0
¯A22
	 
¯x1(t)
¯x2(t)
	
+
 ¯B11
0
	
u(t)
(2.16)
and ( ¯A11, ¯B11) is a controllable pair. Hence, from the corollary of the pole-
assignment system, we can stabilize the system by means of the control u(t) =
K¯x1(t), provided
d¯x2
dt (t) = ¯A22¯x2(t)
is asymptotically stable.
Deﬁnition 2.3. (i) We say that the system (2.1) is open loop stabilizable if
∀x0 ∈Rn,
∃u ∈L2(0, ∞; Rm)
such that the solution x(t; x0, u) of (2.1) satisﬁes
 ∞
0
|x(t; x0, u)|2 dt < ∞.
(2.17)

24
I-1 Control of Linear Diﬀerential Systems
(ii) We say that the system (2.1) is stabilizable by feedback if there exists a
feedback matrix K such that
u(t) = Kx(t)
and
dx
dt = (A + BK)x(t)
is asymptotically stable.
⊓⊔
Remark 2.2. For ﬁnite dimensional linear systems, if the system is open loop
stabilizable, then it is stabilizable by feedback and conversely. Note that by
the previous discussion, stabilizability is a weaker concept than controllability,
since stabilizability only requires that the uncontrollable part of the system
be asymptotically stable.
⊓⊔
The dual of the concept of stabilizability is that of detectability. This is
best discussed by considering the dual system
dξ
dt (t) = −A∗ξ(t) −C∗y(t),
(2.18)
which involves only the state–output part of the system.
Deﬁnition 2.4. The system (2.1) is said to be detectable if the dual system
(2.18) is stabilizable by feedback.
⊓⊔
Remark 2.3. Detectability requires that the unobservable part of the system
be asymptotically stable and hence is a weaker concept than observability.
⊓⊔
A matrix test for stabilizability and detectability can be given.
Theorem 2.5. The following properties are equivalent:
(i) (A, B) is a controllable pair.
(ii) rank[λI −A
...B] = n, ∀λ ∈C.
(iii) rank[λI −A
...B] = n, for each eigenvalue λ of A.
Proof. Note that (ii) and (iii) are equivalent because the ﬁrst n × n block of
the n×(n+m) matrix [λI−A
...B] has full rank whenever λ is not an eigenvalue
of A.
We now prove that (i) =⇒(ii). For this purpose, assume that rank[λI −
A...B] < n for some λ ∈C. Hence the space spanned by the rows of [λI −A...B]
has a dimension less than n and therefore there exists some vector z∗∈Rn
and some λ such that z∗[λI −A...B] = 0. Hence
z∗A = λz∗
and
z∗B = 0,

2 Controllability, observability, stabilizability, and detectability
25
and therefore z∗AkB = λkz∗B = 0, ∀K, and hence
z∗[B...AB... · · · ...An−1B] = 0,
which contradicts controllability.
To prove (i) =⇒(ii) assume (A, B) is not a controllable pair. Then from the
structure theorem, there exists a non singular linear transformation T : Rn →
Rn such that
(A, B) 	→

¯A =

A11 A12
0
A22
	
, ¯B =

B1
0
	
,
where A11, B1 is a controllable pair and A11 is r × r with r < n. Let λ be an
eigenvalue of A∗
22 with the corresponding eigenvector v so that
v∗(λI −A22) = 0. Then the n-vector (̸= 0),
w =
0
v
	
is such that
w∗¯A = λw∗and w∗¯B = 0. Hence z = (T ∗)−1w ̸= 0 satisﬁes z∗(λI −
A)T −1...B = 0, and therefore, z∗(λI −A...B) = 0 contradicting (ii).
⊓⊔
Corollary 2.2. (i) (A, B) is stabilizable if and only if
rank [λI −A
...B] = n,
∀λ ∈C, Re λ ≥0.
(ii) Dually (A, C) is detectable if and only if
rank

λI −A∗
C∗
	
= n,
∀λ ∈C such that Re λ ≥0.
Corollary 2.2 is generalized to an inﬁnite dimensional setting when A is the
generator of a strongly continuous semigroup1 on a Hilbert space H with
etA compact for t > 0 (and indeed for more general situations) and B ∈
L(U, H) and C ∈L(H; Y ) and U and Y are Hilbert spaces (Proposition 3.3
of Chapter 1 in Part V).
2.7 Applications of controllability and observability
The concepts of controllability, observability, and the slightly weaker notions
of stabilizability and detectability have important applications in diverse areas
of systems theory and optimal control. In this section we present two such
applications. The ﬁrst application is for the theory of stability and tests for
stability via the Lyapunov equations. Our second application is for regulator
theory, where we show that if a system is controllable and observable, then
we can build a compensator to regulate the system in a satisfactory way.
1 cf. Deﬁnition 2.1 on page 89.

26
I-1 Control of Linear Diﬀerential Systems
2.7.1 Stability
Our ﬁrst application is to the study of stability of linear diﬀerential equations
via the Lyapunov method.
Consider the stability problem for the equation
dx
dt (t) = Ax(t),
A: Rn →Rn.
(2.19)
It is well known (cf. Chapter 1 in Part II) that the following are equivalent:
(i) (2.19) is asymptotically stable.
(ii) (2.19) is exponentially stable.
(iii) ∀x0 ∈Rn,
 ∞
0
|eAtx0|2 dt < ∞.
To test for stability of (2.19), we look for a Lyapunov function of the form
V (x) = (x, Px),
P ∗= P > 0 positive deﬁnite.
Computing the derivative of V along trajectories of (2.19), we get
dV
dt = (x, A∗P + PAx),
and if it is to be negative, we require that
A∗P + PA = −Q,
Q > 0 positive deﬁnite.
(2.20)
It is well known that if A is a stability matrix (all its eigenvalues have a
strictly negative real part), then the solution of (2.20) can be written as
P =
 ∞
0
eA∗tQeAt dt,
(2.21)
and conversely if (2.20) has a positive deﬁnite solution, then A is a stability
matrix. A generalization of these results in inﬁnite dimensional spaces has
been extensively discussed in Chapter 1 of Part II.
Now suppose that Q ≥0 and factorize Q as Q = C∗C, C ∈L(Rp; Rn).
Introduce the state–output system:
⎧
⎨
⎩
dx
dt (t) = Ax(t),
y(t) = Cx(t).
(2.22)
Deﬁnition 2.5. The state–output system is said to be asymptotically output
stable if
∀x ∈Rn,
 ∞
0
|CeAtx|2 dt < ∞.
⊓⊔

2 Controllability, observability, stabilizability, and detectability
27
Now it is easy to see that if (A, C) is an observable pair and A is asymptotically
stable, then the equation
A∗P + PA = −C∗C
has a solution and
P =
 ∞
0
eA∗tC∗CeAt dt > 0 (positive deﬁnite),
the positive-deﬁniteness being a consequence of observability. Conversely if
the state–output system is asymptotically output stable and if (A, C) is an
observable pair, then A is asymptotically stable. We can now weaken this
further.
Theorem 2.6. If the state–output system is asymptotically output stable (by
some feedback matrix K) and if (A, C) is a detectable pair, then the system
dx
dt (t) = (A + BK)x(t)
is asymptotically stable.
Proof. By hypothesis there exists a K∗∈L(Rn; Rp) such that (A∗+ C∗K∗)
is a stability matrix and hence (A + KC) is a stability matrix. Now write
dx
dt (t) = Ax(t)
as
dx
dt (t) = (A + KC)x(t) −KCx(t),
and we can write its solution as
x(t) = e(A+KC)tx +
 t
0
e(A+KC)(t−s)KCx(s) ds.
(2.23)
Hence
 ∞
0
|x(t)|2 dt
	1/2
≤
 ∞
0
|e(A+KC)tx|2 dt
	1/2
+
 ∞
0

 t
0
e(A+KC)(t−s)KCx(s) ds

2
dt
1/2
.
(2.24)
Furthermore, as (A+KC) is a stability matrix, there exists α > 0 and M ≥1
such that
∀x ∈Rn,
|e(A+KC)tx| ≤Me−αt|x|,
and the second term of the right-hand side of inequality (2.24) can be ma-
jorized by

28
I-1 Control of Linear Diﬀerential Systems
 ∞
0
 t
0
Me−α(t−s)∥K∥|Cx(s)| ds
	2
dt.
Now introduce the function
f(s) =

M∥K∥e−αs,
s ≥0,
0,
otherwise
and
g(s) =

|Cx(s)|,
s ≥0,
0,
otherwise.
Now f ∈L1(−∞, ∞; R) and g ∈L2(−∞, ∞; R). Hence by Young’s inequality
∥f ∗g∥L2 ≤∥f∥L1.∥g∥L2,
where
(f ∗g)(t) =
 ∞
−∞
f(t −s)g(s) ds =
 t
0
M∥K∥e−α(t−s)|Hx(s)| ds.
This proves the theorem.
⊓⊔
Remark 2.4. This theorem and its proof generalize to certain inﬁnite dimen-
sional Hilbert space situations and have implications in the study of the alge-
braic Riccati equation. See §3.4, Chapter 1 of Part V.
⊓⊔
2.7.2 Compensators for linear systems
Consider the linear control system
⎧
⎨
⎩
dx
dt (t) = Ax(t) + Bu(t),
y(t) = Cx(t).
(2.25)
We have seen that if (A, B) is a controllable pair, then the spectrum of the
closed loop system can be made arbitrary by state feedback. We now wish to
discuss the situation where full-state feedback is not available and the control
has to be of the form
u(t) =
 t
0
Λ(t −s)y(s) ds + v(t),
(2.26)
where the kernel Λ corresponds to some ﬁnite dimensional system and v(t)
is a reference input. We would like to investigate the properties of the corre-
sponding closed loop system. We shall construct such a control by
(i) constructing a state estimator ˆx(·) from the observations y(·),

2 Controllability, observability, stabilizability, and detectability
29
(ii) and by constructing an appropriate control as a linear function of the
estimate ˆx(t).
We ﬁrst construct such an estimator in the form
⎧
⎪
⎪
⎨
⎪
⎪
⎩
dˆx
dt (t) = Aˆx(t) + Mν(t) + Bu(t),
ν(t) = y(t) −Cˆx(t),
and M is to be chosen later.
(2.27)
The vector ν(·) has the interpretation as an “innovation” function; that
is, ν(t) is the new information in the output y(t) at time t not contained in
Cˆx(t). Let e(t) = x(t) −ˆx(t) be the error between the state and its estimate.
We see that
de
dt (t) = (A −MC)e(t).
(2.28)
Now if we assume that (A, C) is an observable pair, then the spectrum of
(A −MC) can be made arbitrary by a suitable choice of M. In particular, by
a suitable choice of M, (A −MC) can be made a stability matrix. Note that
for the requirement of stability of the (A −MC), it is enough to assume that
(A, C) is a detectable pair.
Now consider a feedback control of the form
u(t) = −Kˆx(t),
(2.29)
giving us a closed loop system,
dx
dt (t) = Ax(t) −BKˆx(t) = (A −BK)x(t) + BKe(t).
(2.30)
By the pole-assignment theorem, the spectrum of (A −BK) can be made
arbitrary by a suitable choice of K, provided (A, B) is a controllable pair
(stabilizability is enough).
Consider the pair of diﬀerential equations:
d
dt

x(t)
e(t)
	
=

A −BK
BK
0
A −MC
	 
x(t)
e(t)
	
.
(2.31)
It is clear that the spectrum of the above block matrix is determined by
that of A −BK and A −MC and hence can be made arbitrary by proper
choices of K and M and in particular can be made a stability matrix. This is
one of the fundamental results of linear feedback control.
We now turn to a discussion of quadratic-cost optimal control and show
how this theory gives rise to optimal feedback controllers that render the
closed loop system asymptotically stable. It turns out that the theory of con-
trollability and observability plays an important role in the study of optimal
control over an inﬁnite time horizon.

30
I-1 Control of Linear Diﬀerential Systems
3 Optimal control
Consider the linear control system
⎧
⎨
⎩
dx
dt (t) = Ax(t) + Bu(t),
x(s) = x,
y(t) = Cx(t)
(3.1)
on the time interval [s, T], 0 ≤s < T, where x is an arbitrary initial state
in Rn. The notation and terminology are the same as the ones deﬁned in the
section on controllability and observability.
We shall be primarily concerned with the following optimal control prob-
lem: to choose a control ˆu(·) ∈L2(s, T; Rm) that minimizes the cost functional
J(u; s, x) = 1
2
 T
s


u(t), Ru(t)

+

x(t), Qx(t)

dt + 1
2

x(T ), Sx(T )

, (3.2)
where R = R∗> 0, Q = Q∗= C∗C ≥0, and S = S∗≥0. A control
ˆu(·) minimizing J(u; x) will be called an optimal control. We shall not be
concerned with the question of existence and uniqueness of solutions but with
the characterization of the optimal control ˆu(·) and the corresponding optimal
trajectory ˆx(·).
Even though we are interested in solving the optimal control problem over
a ﬁxed interval [0, T ] and for the ﬁxed initial condition x0, it will turn out
to be conceptually important to solve the problem for all initial points (s, x),
0 ≤s < T .
We shall also be interested in the inﬁnite time problem: Find a control
ˆu(·) ∈L2(s, ∞; Rm) that minimizes:
J(u; s, x) = 1
2
 ∞
s


u(t), Ru(t)

+

x(t), Qx(t)

dt.
(3.3)
3.1 Finite time horizon
We ﬁrst discuss the ﬁnite time situation. We are interested in obtaining the op-
timal control ˆu(·) in feedback form ˆu(t) = K(t)x(t), where K(t) ∈L(Rn; Rm).
There are three basic approaches to the solution of this problem. The ﬁrst is
via the Calculus of Variations or equivalently the Maximum Principle and
makes use of the adjoint equation. It can be shown that the optimal control
is characterized by the solution of the following system:
⎧
⎨
⎩
dˆx
dt (t) = Aˆx(t) + Bˆu(t),
ˆx(s) = x,
(3.4)
⎧
⎨
⎩
dp
dt (t) = −A∗p(t) −Qˆx(t),
p(T ) = Sˆx(T ),
(3.5)

3 Optimal control
31
Rˆu(t) + B∗p(t) = 0
s ≤t ≤T.
(3.6)
We recognize this as a two-point boundary value problem. The system (3.4)–
(3.6) can be decoupled as follows. It is here the idea of invariant embedding,
namely, the idea of considering the optimal control problem for all initial (s, x)
where 0 < s < T and x ∈Rn arbitrary becomes important. One ﬁrst shows
that the system of equations
⎧
⎨
⎩
dξ
dt (t) = Aξ(t) −BR−1B∗η(t),
ξ(s) = x,
(3.7)
⎧
⎨
⎩
dη
dt (t) = −A∗η(t) −Qξ(t),
η(T ) = Sξ(T )
(3.8)
admits a unique solution. We can then prove that the mapping
x 	→

ξ(·), η(·)

: Rn →W(s, T; Rn) × W(s, T; Rn),
(3.9)
where
W(s, T; Rn) =

z : z ∈L2(s, T; Rn), dz
dt ∈L2(s, T; Rn)

is an aﬃne continuous map and ﬁnally that the mapping
x 	→η(s): Rn →Rn is an aﬃne mapping.
(3.10)
From this it follows that we can write
η(s) = P(s)x + r(s),
(3.11)
where
P(s) ∈L(Rn; Rn)
and
r(s) ∈Rn.
(3.12)
Let

x(·), p(·)

be a solution of (3.4)–(3.6) in the interval [0, T]. Then
p(t) = P(t)x(t) + r(t),
∀t ∈[0, T ],
and a calculation shows that P(t) and r(t) satisfy:
⎧
⎨
⎩
dP
dt (t) + A∗P(t) + P(t)A −P(t)BR−1B∗P(t) + Q = 0,
P(T ) = S,
(3.13)
⎧
⎨
⎩
dr
dt (t) + A∗r(t) −P(t)BR−1B∗r(t) = 0,
r(T ) = 0.
(3.14)
Clearly r(t) is identically zero and standard results of ordinary diﬀerential
equations prove that (3.12) has a unique global solution.

32
I-1 Control of Linear Diﬀerential Systems
Therefore we have obtained the optimal control in feedback from:
ˆu(t) = −R−1B∗P(t)ˆx(t).
The same result can be obtained using the method of Dynamic Program-
ming. Conceptually, the Calculus of Variations gives necessary conditions of
optimality, whereas the method of Dynamic Programming gives suﬃcient con-
ditions of optimality. But for the strictly convex quadratic cost problem, both
methods give necessary and suﬃcient conditions of optimality.
Let
V (s, x) = inf
u J(u; s, x).
Then an application of the Principle of Optimality leads to the following
partial diﬀerential equation for V :
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
∂V
∂s (s, x) + min
u
1
2

u(s), Ru(s)

+ 1
2

x(s), Qx(s)

+

∇xV (x, s), Ax(s) + Bu(s)
	
= 0,
V (T, x) = (Sx, x),
(3.15)
where ∇xV (x, s) is the gradient of V (x, s) with respect to the vector x. Car-
rying out the minimization, we obtain
u(s) = −R−1B∗∇xV (x, s),
∀s ∈[0, T ].
(3.16)
Hence we obtain the so-called Bellman–Hamilton–Jacobi equation
∂V
∂s (s, x) + 1
2

∇xV (x, s), BR−1B∗∇xV (x, s)

+ 1
2(x, Qx) + (A∗∇xV (s, x), x) = 0.
(3.17)
Now, (3.17) can be solved by looking for a solution of the form:
V (s, x) = 1
2

x, P(s)x

+

x, r(s)

.
(3.18)
We can check that P(s) and r(s) satisﬁes (3.12) and (3.14) previously ob-
tained, and we recover the same results as obtained by Calculus of Variation
arguments.
There is a third approach, in some sense related to the Dynamic Program-
ming approach, to the solution of quadratic cost optimal control problems.
This is the method of completing the squares. The main idea here is to observe
that if
dx
dt = Ax(t) + Bu(t),
and if P(t) = P ∗(t) be such that dP/dt exists in the interval [s, T], then
deﬁning

3 Optimal control
33
z(t) =

u(t)
x(t)
	
and
L(t) =

0
R−1B∗P(t)
P(t)B
dP
dt + A∗P(t) + P(t)A

,
we get
 T
s

z(t), L(t)z(t)

dt −

x(T ), P(T )x(T )

+

x(s), P(s)x(s)

= 0.
(3.19)
Now let P(t) be the global solution on the interval [s, T] of the equation
⎧
⎨
⎩
dP
dt (t) + A∗P(t) + P(t)A −P(t)BR−1B∗P(t), +Q = 0,
P(T ) = S.
(3.20)
Then adding the identity (3.20) to J(u; s, x) and doing some algebraic manip-
ulations, we obtain
J(u; s, x) =
 T
s
|u(t) + R−1B∗P(t)x(t)|2 dt +

x(s), P(s)x(s)

.
(3.21)
Hence the optimal control is given by
ˆu(t) = −R−1B∗P(t)x(t),
(3.22)
and the optimal cost V (s, x) by
V (s, x) =

x, P(s)x

.
(3.23)
In this book, we use the latter two methods in inﬁnite dimensional situ-
ations to obtain results on optimal control for the quadratic cost problem in
a ﬁnite time interval. Apparently, one obtains more general results using this
methodology.
In Part IV, Chapters 1 to 3 develop the optimal control of parabolic,
hyperbolic, and delay equations over a ﬁnite time interval. The observation
operator is usually considered to be also bounded, but the case of unbounded
observation operators is also treated. Chapter 2 deals with parabolic equations
with unbounded control operators, whereas Chapter 3 deals with hyperbolic
equations with unbounded control operators. The latter two chapters are the
most technical, and the full power of the theory developed in Part II needs to
be used.
3.2 Inﬁnite time horizon
The study of the quadratic cost problem over a ﬁnite and an inﬁnite time
interval is essentially a study of the Riccati diﬀerential equation over the
ﬁnite time interval [s, T]

34
I-1 Control of Linear Diﬀerential Systems
dP
dt (t) + A∗P(t) + P(t)A −P(t)BR−1B∗P(t) + C∗C = 0
(3.24)
subject to the terminal condition
P(T ) = PT ≥0,
(3.25)
its asymptotic behavior and the corresponding matrix quadratic equation
A∗P + PA −PBR−1B∗P + C∗C = 0.
(3.26)
The following theorem is proved (in certain inﬁnite dimensional situations) in
§2 of Chapter 1 in Part IV, and in §2 to §4 of Chapter 1 in Part V.
Theorem 3.1. There exists a matrix P(t) satisfying the following:
(i) P(·) is deﬁned and belongs to C1
[s, T]; L(Rn)

and satisﬁes (3.24) and
(3.25).
(ii) P(t) ≥0, s ≤t ≤T and is the unique solution of (3.24)–(3.25).
(iii) Let ˜K(t) be a continuous function on [s, T], and let ˜P(t) be the solution
of the linear diﬀerential equation
d ˜P
dt (t) +

A −B ˜K(t)
∗˜P(t) + ˜P(t)

A −B ˜K(t)

+ C∗C + ˜K(t)∗R ˜K(t) = 0,
(3.27)
˜P(T ) = PT ≥0.
(3.28)
If P(t) is a solution to (3.24) and (3.25), then
P(t) ≤˜P(t),
s ≤t ≤T.
(iv) Consider (3.24) with s →−∞and T = 0. If (A, B) is stabilizable, then
P(t) is bounded on ]−∞, 0]. If (A, C) is detectable, then
P∞=
lim
t→−∞P(t)
exists and is positive semi-deﬁnite.
In that case, P∞is the unique positive semi-deﬁnite solution of (3.26)
and
A −BR−1B∗P∞
is a stability matrix.
The optimal control over the inﬁnite time interval is then given by
ˆu(t) = −R−1B∗P∞ˆx(t).
(3.29)
The study of these control problems when control is exercised through the
boundary is most complicated for hyperbolic equations and is taken up in
Chapter 3 of Part V. The study of exact controllability comes into the picture
here to ensure that the space of admissible controls is nonempty.

4 A glimpse into H∞-theory: state feedback case
35
4 A glimpse into H∞-theory: state feedback case
4.1 Introduction
In many control problems the quadratic criterion is not the most appropriate.
We consider a disturbance attenuation problem when the full state vector can
be measured and introduce the so-called H∞-optimal control problem. There
is now a vast literature on this topic. For two textbook presentations, the
reader is referred to T. Bas¸ar and P. Bernhard [1] and B. A. Francis [1].
Consider the ﬁnite dimensional linear time-invariant system
⎧
⎨
⎩
dx
dt (t) = Ax(t) + Lw(t) + Bu(t),
z(t) = Cx(t) + Du(t),
(4.1)
where u is interpreted as the controls and z as controlled outputs that are to
be made small in the presence of exogenous disturbances w. The matrices and
vectors have appropriate dimensions.
Let S(A, B) denote the set of all constant gain stabilizing state feedback
matrices. That is,
S(A, B) = {K : Re[λi(A + BK)] < 0}.
Then for u ∈S(A, B), the closed loop dynamics take the form
⎧
⎨
⎩
dx
dt (t) = (A + BK)x(t) + Lw(t),
z(t) = (C + DK)x(t),
(4.2)
or in transfer function form
z(s) = TK(s)w(s),
where
TK(s) = (C + DK)

sI −(A + BK)
−1L.
The problem of H∞-optimal state feedback is now stated as
inf
K∈S(A,B) ∥TK∥H∞,
where
∥TK∥H∞= sup
ω σmax∥T (jω)∥
and σmax denotes the maximum singular value of the matrix T (jω).
This H∞-optimal state feedback problem represents a special case of the
more general H∞- optimal output feedback disturbance rejection problem.

36
I-1 Control of Linear Diﬀerential Systems
4.2 Main results
The following assumptions are made on (4.1):
(A1) The pair (A, B) is stabilizable.
(A2) The pair (A, C) is observable.
(A3) D∗[C D] = [0 I].
(A1) is necessary for the stabilization of (4.1). Assumption (A2) is a tech-
nical assumption that guarantees the invertibility of certain algebraic Riccati
equation solutions. The orthogonality assumption (A3) is analogous to no
cross weighting between the state and control in the standard LQ problem.
In case (A3) is violated, the change of control variables from u to v given by
u(t) = (D∗D)−1/2v(t) −(D∗D)−1Cx(t)
yields a new system that satisﬁes (A3). Furthermore, it can be shown that a
controlled output of the form
z(t) = Cx(t) + Du(t) + Eu(t)
may be transformed to the form of (4.1). Thus, (A3) is made with minimal
loss of generality.
The main result is now stated.
Theorem 4.1. Consider the linear system (4.1) under assumptions (A1)
to (A3). Under these conditions, there exists a K ∈S(A, B) such that
∥TK∥H∞< γ if and only if there exists an X = X∗> 0 that satisﬁes
XA + A∗X + C∗C + X
 1
γ2 LL∗−BB∗

X = 0
(4.3)
with A + (1/γ2LL∗−BB∗)X stable (i.e., all eigenvalues in the open left-half
complex plane).
The remainder of this section is devoted to the proof of Theorem 4.1 for γ = 1.
By linearity, this simpliﬁcation is made without loss of generality.
First, some preliminary results from linear quadratic optimization theory
are stated for the linear system
⎧
⎨
⎩
dx
dt (t) = Ax(t) + Bu(t),
x(0) = x0,
y(t) = Cx(t).
Lemma 4.1. Let (A, B) be stabilizable and (A, C) detectable (resp. observ-
able). Then there exists a unique P = P ∗≥0 (resp. > 0) that satisﬁes
PA + A∗P + C∗C −PBB∗P = 0,

4 A glimpse into H∞-theory: state feedback case
37
with A −BB∗P stable. Furthermore, the state feedback control law u(t) =
−B∗Px(t) is a minimizing solution for the problem
inf
u
1
2
 ∞
0

|y(t)|2 + |u(t)|2
dt.
This is Theorem 3.1 restated.
Lemma 4.2. Let G(s) = C(sI −A)−1B with A stable. Then ∥G∥∞< 1 if
and only if there exists an X = X∗≥0 that satisﬁes
XA + A∗X + C∗C + XBB∗X = 0
with A + BB∗X stable. Furthermore, the state feedback u(t) = BB∗Xx(t)
solves
sup
u
1
2
 ∞
0
(|y(t)|2 −|u(t)|2) dt.
For a proof of the above lemma see J. C. Willems [1].
The proof of Theorem 4.1 is now presented. To prove the “if” direction,
assume that X = X∗> 0 satisﬁes (4.3) with A + (LL∗−BB∗)X stable. A
manipulation of (4.3) yields
X(A −BB∗X) + (A −BB∗X)∗X
+ (C −DB∗X)∗(C −DB∗X) + XLL∗X = 0.
(4.4)
Now assumptions (A2) to (A3) imply that the pair (A −BB∗X, C −DB∗X)
is observable. This observability, (4.4), and X > 0 together imply that the
matrix A−BB∗X is stable via standard Lyapunov stability theory. Thus, K =
−B∗X ∈S(A, B). Finally, using (4.4) and the stability of A+(LL∗−BB∗)X
in Lemma 4.2 implies that ∥TK∥H∞< 1, which is the desired result.
To prove the “only if” direction requires the following preliminary result:
Lemma 4.3. There exists an X = X∗> 0 that satisﬁes
XA + A∗X + C∗C + X(LL∗−BB∗)X = 0
(4.5)
with A + (LL∗−BB∗)X stable if and only if there exists a P = P ∗> 0 that
satisﬁes
AP + PA∗+ PC∗CP + LL∗−BB∗= 0
(4.6)
with −A∗−C∗CP stable.
Proof. Let P = X−1. This establishes an equivalence of the existence of pos-
itive deﬁnite solutions to either (4.5) or (4.6). To show equivalence of the
stability conditions, a manipulation of either (4.5) or (4.6) yields the similar-
ity condition
A + (LL∗−BB∗)X = P(−A∗−C∗CP)P −1.
⊓⊔

38
I-1 Control of Linear Diﬀerential Systems
Thus to prove the “only if” portion of Theorem 4.1, it suﬃces to prove
the equivalent condition associated with (4.6).
Toward this end, let K ∈S(A, B) be such that ∥TK∥H∞< 1. Then from
Lemma 4.2, there exists an ˜X = ˜X∗≥0 that satisﬁes
˜X(A + BK) + (A + BK)∗˜X + (C + DK)∗(C + DK) + ˜XLL∗˜X = 0, (4.7)
with A + BK + LL∗˜X stable. A straightforward manipulation of (4.7) yields
˜XA + A∗˜X + C∗C + ˜X(LL∗−BB∗) ˜X = −(K + B∗˜X)∗(K + B∗˜X). (4.8)
Let ˜P = ˜X−1. Note that assumptions (A2)–(A3) imply observability of the
pair (A + BK, C + DK), which in turn implies ˜X > 0, thus allowing the
inversion of ˜X. In terms of ˜P, (4.8) now takes the form
A ˜P + ˜PA∗+ ˜PC∗C ˜P + LL∗−BB∗= −˜V ∗˜V ,
(4.9)
where
˜V = K ˜P + B∗.
Now (4.9) is not quite in the desired form of (4.6). Thus, as in J. C. Willems [1],
let ∆P = P −˜P. Then subtracting (4.9) from (4.6) yields (after some straight-
forward manipulations)
(−˜A)∗∆P + ∆P(−˜A) + ˜V ∗˜V −∆PC∗C∆P = 0,
(4.10)
where ˜A = A∗+ C∗C ˜P. Clearly, if one can ﬁnd a symmetric positive semi-
deﬁnite solution, ∆P, to (4.10), then P =
˜P + ∆P satisﬁes the desired
(4.6). However, (4.6) is of the form of a standard LQ Riccati equation as
in Lemma 4.1. The requisite stabilizability and detectability in Lemma 4.1 is
now shown as follows:
1. The matrix pair (−˜A, C∗) is controllable. This follows directly from as-
sumption (A2) and that −˜A = −A∗−C∗C ˜P.
2. The matrix pair (−˜A, ˜V ) is detectable. To see this, a straightforward ma-
nipulation of (4.9) yields the similarity condition
−˜A −K∗˜V = ˜P(A + BK + LL∗˜X) ˜P −1.
However, recall that ˜X is such that A+BK+LL∗˜X is stable, thus proving
the desired detectability.
These observations along with Lemma 4.1 guarantee the existence and
uniqueness of a positive deﬁnite ∆P, which satisﬁes (4.10). Thus, P
=
˜P + ∆P > 0 and satisﬁes (4.6).
It then remains to be shown that −A∗−C∗CP is stable. However, from
Lemma 4.1, one has that −˜A −C∗C∆P is stable. But
−˜A −C∗C∆P = −A∗−C∗CP,

5 Dissipative systems
39
which, via Lemma 4.3, completes the proof.
It is worthwhile remarking that the current proof exploits the fact that
the condition ∥TK∥≤1 is a characterization of dissipativity in terms of scat-
tering variables. It is well known that the dissipative property in turn can
be characterized in terms of a quadratic Lyapunov function obtained from
an indeﬁnite quadratic cost problem (cf. Lemma 4.2). The exposition here
essentially exploits these ideas to obtain the desired result. We have included
a section on dissipative systems for this reason.
It should be remarked that the state space theory of H∞should only be
considered as a computational device and not as a substitute for the operator
theory solution of H∞-problems. The important questions of approximation
and robustness should be treated from the input–output viewpoint, and it is
unclear whether it is meaningful in the state space description.
The general case, when only measurement feedback is available, was
treated in the important paper of J. C. Doyle, K. Glover, P. P. Khar-
gonekar, and B. A. Francis [1] in 1989. These results were generalized to
certain Inﬁnite Dimensional Systems (the so-called Pritchard–Salamon sys-
tems) by B. van Keulen [1] in 1993. The new element here is that the
feedback solution is determined in terms of two coupled Ricatti equations.
5 Dissipative systems
5.1 Deﬁnitions and preliminary results
Consider the linear, stationary dynamical system
⎧
⎨
⎩
dx
dt = Ax + Bu,
x(0) = x0,
y = Cx + Du,
(5.1)
where x ∈Rn, u ∈Rm, y ∈Rp, and A, B, C, D are constant matrices of
dimensions (n×n), (n×m), (p×n), and (p×m), respectively. We shall assume
that the input functions t 	→u(t) : R →Rm are locally square integrable.
Given x(t0) = x0 ∈Rn, there exists a unique absolutely continuous func-
tion t 	→x(t) : [t0, ∞) →Rn that satisﬁes (5.1) almost everywhere. Corre-
spondingly, the output function t 	→y(t) : [t0, ∞) →Rn is locally square
integrable and the state trajectory t 	→x(t) : [t0, ∞) →Rn is also locally
square integrable. In terms of the above, we denote the input space by U and
the output space by Y.
We make the assumption that (5.1) is reachable (and controllable) and
observable. Hence, the realization of the input–output map
y(t) = CeA(t−t0)x0 +
 t
t0
CeA(t−τ)Bu(τ) dτ + Du(t)
(5.2)

40
I-1 Control of Linear Diﬀerential Systems
given by (5.1) is minimal.
In the sequel, we wish to examine the dissipativeness of system (5.1) with
respect to the supply rate w = (u, y) = u∗y, where we are assuming that
m = p and (·, ·) represents the scalar product on Rn.
Deﬁnition 5.1. A function S : Rn →R+ is called a storage function. The
system (5.1) is said to be dissipative with respect to the supply rate w(t) =
(u(t), y(t)) if there exists a storage function S : Rn →R+ such that for all
x0 ∈Rn and u ∈U, the dissipation inequality
S(x0) +
 t1
t0
(u(t), y(t)) dt ≥S(x(t1))
(5.3)
holds for all t1 ≥t0 in R, where
x(t1) = eA(t1−t0)x0 +
 t1
t0
eA(t1−τ)Bu(τ) dτ
(5.4)
and
y(t) = CeA(t−t0)x0 +
 t1
t0
eA(t−τ)Bu(τ) dτ + Du(t).
(5.5)
Note that if S is smooth, then the dissipation inequality may be written as
S(x) ≥0,
(∇S(x), Ax + Bu) ≤(u, Cx) + (u, Du) ,
(5.6)
∀x ∈Rn, and u ∈Rm.
⊓⊔
The form of (5.6) (note the similarity to the Hamilton–Jacobi equation) sug-
gests that the identiﬁcation of storage functions arises by deﬁning appropriate
variational problems associated with (5.1). We start with the following lemma
(J. C. Willems [1, Lemma 1]).
Lemma 5.1. Let the dynamical system deﬁned by (5.1) be minimal and dis-
sipative with respect to the supply rate w(t) = (u(t), y(t)). Let S be a storage
function. Then there exists an ε > 0 such that
S(x) > S(0) + ε∥x∥2 , ∀x ∈Rn .
5.2 Associated variational problems
Now, deﬁne the following variational problems:
Sa(x0) = −lim
t1→∞inf
u∈U
 t1
0
(u(t), y(t)) dt,
(5.7)
Sr(x0) = −
lim
t−1→−∞inf
u∈U
 0
t−1
(u(t), y(t)) dt,
(5.8)

5 Dissipative systems
41
subject to the constraint (5.1) and the boundary conditions x(t−1) = 0, x(0) =
x0. We adopt the normalization minx∈Rn S(x) = S(0) = 0. Sa is referred to
as the available storage and Sr the required supply. Note that (5.7) and (5.8)
are quadratic variational problems, but they may be singular when D + D∗,
which is nonnegative from dissipativity, is singular, and an optimal control
may not exist even though the desired inﬁmum exists.
The variational problems characterizing the available storage and required
supply may then be solved by considering appropriate solutions of the alge-
braic Riccati equation:
KA + A∗K + (KB −C∗)(D + D∗)−1(B∗K −C) = 0.
(5.9)
Theorem 5.1 (J. C. Willems [2]). The algebraic Riccati equation (5.9)
has a negative deﬁnite solution if and only if (5.1) is dissipative with re-
spect to the supply rate w = (u, y). In that case, there exists only one
real symmetric solution K−with the property Re λ(A−) ≤0, where A−=
A+B(D+D∗)−1(B∗K−C), and only one real symmetric solution K+ with the
property λ(A+) ≥0, where A+ = A−+ (KB −C∗)(D + D∗)−1B∗. Moreover,
0 < K−≤K+ and every real symmetric solution satisﬁes K−1 ≤K ≤K+.
Hence, all real symmetric solution are positive deﬁnite, λ(A−) < 0, and
λ(A+) > 0.
Theorem 5.2. Under the assumptions of Theorem 5.1,
Sa(x) = 1
2(x, K−x) and Sr(x) = 1
2(x, K+x).
Proof. Let us ﬁrst consider the case when D + D∗is nonsingular. Let K be a
real symmetric solution of the algebraic Riccati equation (5.9). Then
d
dt(x, Kx)
along the trajectories of ˙x = Ax + Bu is given by
(u, Cx + Du)
−1
2(u −(D + D∗)−1(B∗K −C)x, (D + D∗)(u −(D + D∗)∗(B∗K −C)x).
Now assume that u ∈U, transfers x0 at t0 to x1 at t1. Then
(x1, Kx1) −(x0, Kx0)
=2
 t1
t0
(u(t), y(t)) dt
−
 t1
t0

u −(D + D∗)−1(B∗K −C)x,
(D + D∗)(u −(D + D∗)−1(B∗K −C)x

dt.

42
I-1 Control of Linear Diﬀerential Systems
Let us put t0 = 0 and K = K−in the above, and noting that K−≥0, we
obtain
−
 t1
0
(u(t), y(t)) dt ≤1
2(x0, K−x0),
∀u ∈U.
Hence, Sa(x0) ≤1
2(x0, K−x0). Now consider the control
u = (D + D∗)−1(B∗K−−C)x.
The closed loop system is then ˙x = A−x, and because Re λ(A−) < 0,
lim
t→∞x(t) = 0.
Hence, along this trajectory,
1
2(x0, K−x0) +
 ∞
0
(u(t), y(t)) dt = 0,
and hence, Sa(x0) = 1
2(x0, K−x0). A similar argument shows that
Sr(x0) = 1
2(x0, K+x0).
We now look at the case when D+D∗is singular. The idea is to replace D
by (D+ ε
2I), where I is the identity matrix. It is clear that the new dynamical
system with D replaced by (D + ε
2I) is also dissipative with respect to w =
(u, y). We now proceed to prove the result through a limiting argument by
considering
lim
ε↓0

D + ε
2I

.
Therefore, consider the algebraic Riccati equation (5.9) with D+ε/2 I in place
of D
KεA + A∗Kε + (KεB −C∗)(D + D∗+ εI)−1(B∗Kε −C∗) = 0
and consider the real symmetric solutions K−
ε and K+
ε with the properties
Re λ(A−
ε ) ≤0 and Re λ(A+
ε ) ≥0, with A−
ε and A+
ε deﬁned analogously as in
Theorem 5.2. We then have 0 < K−
ε ≤K+
ε and all real symmetric solutions
Kε satisfy K−
ε ≤Kε ≤K+
ε . Indeed, we have K−
ε < K+
ε and Re λ(A−
ε ) < 0
and Re λ(A+
ε ) > 0. Now deﬁne
K−= lim
ε↓0 K−
ε
and
K+ = lim
ε↓0 K+
ε .
Then, Theorem 5.2 holds for K−and K+ so deﬁned. To see this, note that K−
ε
and K+
ε are monotone nondecreasing, and hence, the limit K−= limε↓0 K−
ε
and K+ = limε↓0 K+
ε exist.
Now we have

5 Dissipative systems
43
Sε
a(x) = (x, K−
ε x)
and
Sε
r(x) = (x, K+
ε x).
We need to prove that Sε
a and Sε
r are continuous functions of ε, ε ≥0.
Consider Sε
a(x0) and assume that
lim
ε↓ε0 Sε
a(x0) ̸= Sε0
a (x0).
Now Sε
a(x0) is a monotone nonincreasing function of ε. Hence,
lim
ε↓ε0 Sε
a(x0) < Sε0
a (x0).
By the deﬁnition of Sε0
a (x0), it follows that there exists a u ∈U and t1 ≥0,
such that
lim
ε↓ε0 Sε
a(x0) < −
 t1
0
(u(t), y(t)) dt,
with ˙x = Ax + Bu, y = Cx + (D + ε0/2 I)u and x(0) = x0. Now, for ﬁxed u
and t1 and for all ε, ε0 ≤ε ≤ε + δ,
−
 t1
0
(u(t), y(t))dt,
with constraints ˙x = Ax + Bu, y = Cx + (D + ε0/2 I)u and x(0) = x0, is a
continuous function of ε, which contradicts the inequality
lim
ε↓ε0 Sε
a(x0) < Sε0
a (x0).
Hence, Sε
a(x0) is a right continuous function of ε, and hence, 1
2(x, K−x) =
Sa(x). In a similar manner, we can prove Sr(x) = 1
2(x, K+x). Hence, we have
proved that Theorem 5.1 remains true when D + D∗is singular.
⊓⊔
Remark 5.1. Note that when D + D∗is singular, the optimal control may not
exist.
⊓⊔
5.3 Quadratic storage functions
Given a linear, stationary setting, it is natural to consider storage functions
that are quadratic, 1
2(x, Qx), Q = Q∗. The following theorem follows easily
from Theorem 5.2.
Theorem 5.3. The stationary minimal linear dynamical system given by
(5.1) is dissipative with respect to the supply rate w = (u, y) if and only if
the matrix inequalities

A∗Q + QA
QB −C∗
B∗Q −C
−D −D∗
	
≤0,
(5.10)
Q = Q∗≥0,
(5.11)
have a solution. The function
1
2(x, Qx) is a storage function if and only if
Q satisﬁes (5.10) and (5.11). Consequently K−and K+, as deﬁned earlier,
satisfy these inequalities and 0 < K−≤Q ≤K+.

44
I-1 Control of Linear Diﬀerential Systems
Recent research on the synthesis of controllers using linear matrix inequal-
ities and semi-deﬁnite programming (see, for example, the recent book of
S. Boyd and L. Vandenberghe [1]) is related to the ideas presented here.
6 Final remarks
If one were to write another volume it should be concerned with a generaliza-
tion of the following ﬁnite dimensional problem to inﬁnite dimensions:
inf
u
 T
s
w

x(t), u(t)

dt,
(6.1)
where w(x, u) = 1
2[(x, Qx)+(u, Ru)+2(u, Cx)], R = R∗≥0, and Q = Q∗but
without any deﬁniteness condition on Q, subject to the dynamical constraint
⎧
⎨
⎩
dx
dt = Ax(t) + Bu(t),
x(s) = x
(6.2)
and the related inﬁnite time problem
inf
u
 ∞
s
w

x(t), u(t)

dt
(6.3)
with the dynamical constraint (6.2) and ﬁnal value conditions x(∞) = 0 or
x(∞) = free.
A variety of problems such as linear quadratic diﬀerential games, the
Kalman–Yacubovich–Popov lemma of stability theory, and H∞-theory (in
state space form) is related to this topic.
Notes
The results of this chapter are for the most part classical. The concepts of con-
trollability and observability were introduced by R. E. Kalman [1, 3], where
the criteria of controllability and observability and the structure theorem were
developed. In this connection, see also the early paper of E. Gilbert [1]. The
minimum energy control for transfer from the origin to a ﬁnal state was ﬁrst
discussed in R. E. Kalman, Y. C. Ho, and K. S. Narendra [1]. For a
textbook study of these questions as well as a treatment of stability theory,
see R. W. Brockett [1]. The concepts of stabilizability and detectability
were introduced by Wonham in his study of the algebraic Riccati equation
(cf. W. M. Wonham [2]). The criteria for stabilizability and detectability
are due to M. L. J. Hautus [1]. The pole-assignment theorem in the real
case was proved by W. M. Wonham [1] and independently by J. D. Simon

6 Final remarks
45
and S. K. Mitter [1]. The proof presented here follows W. M. Wonham [3].
Theorem 2.6 is due to J. Zabczyk [3]. The idea of a state estimator was ﬁrst
introduced by D. G. Luenberger [1]. For the treatment of compensators for
linear systems as estimator-controller and the use of the pole-assignment theo-
rem in this context, see J. D. Simon and S. K. Mitter [1]. For a discussion of
the quadratic cost optimal control problem, see the book by R. W. Brock-
ett [1] where the completing the square argument is developed. The idea
of invariant embedding used in §3.1 is due to R. Bellman [1]. Its use in
decoupling the Hamiltonian equations in the form described here is due to
J. L. Lions [3]. It was formally used in the same way by S. K. Mitter [1].
Theorem 3.1 is due to W. M. Wonham [1] (under the slightly stronger hy-
pothesis of observability). The results of §4 follow an unpublished manuscript
of S. K. Mitter and J. S. Shamma [1] but are essentially an application
of the results given in the paper of J. C. Willems [1]. There is now a vast
literature on the so-called H∞-approach to the control of linear systems. For
textbook presentations, see T. Bas¸ar and P. Bernhard [1], B. A. Fran-
cis [1], and B. van Keulen [1].

2
Linear Quadratic Two-Person Zero-Sum
Diﬀerential Games
1 Introduction
In this chapter we broaden the general perspective of the book and consider
two-player zero-sum games with linear dynamics and a quadratic utility func-
tion over a ﬁnite time horizon. They can be viewed as a natural extension of
the single player linear quadratic optimal control problem. In particular they
illustrate the occurrence of symmetrical solutions to the matrix Riccati diﬀer-
ential equation that are not necessarily positive semi-deﬁnite. It also connects
with the glimpse of H∞-theory given in the previous chapter.
Since Game Theory arises from a diﬀerent context than the one in Control
Theory, we quickly review some background and terminology. For instance,
the notion of utility function has a long history. Gabriel Cramer [1] (1728)
and then Daniel Bernouilli [2] (1738) ﬁrst proposed as a decision crite-
rion the maximization of expected utility rather than of expected wealth.
Daniel Bernouilli introduced as utility function, the log, to resolve the St. Pe-
tersberg Paradox (Risk Aversion) in 1760. Much later, in an appendix to their
classic work, J. von Neumann and O. Morgenstern [1] (1943) set out for
the ﬁrst time an axiomatic justiﬁcation for this criterion.
We consider two players (most deﬁnitions will carry over to N players). In
a static game, there is only one shot; that is, the game is played only once.
Player 1 decides a value v1, which can be a vector, a real number, or a discrete
variable. Similarly, Player 2 decides v2. Each of them has a utility function
Ci(v1, v2),
i = 1, 2,
that he/she wants to minimize. A Cournot–Nash equilibrium (cf. A. Cour-
not [1] in 1838 and J. Nash [1] in 19501) is a pair (ˆv1, ˆv2) such that
1 The concept of the Nash equilibrium is not exactly original to John F. Nash.
Antoine Augustin Cournot showed how to ﬁnd what we now call the Nash equi-
librium of the Cournot duopoly game. Consequently, some authors refer to it as

48
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
C1(ˆv1, ˆv2) ≤C1(v1, ˆv2),
∀v1 ∈U 1
ad,
C2(ˆv1, ˆv2) ≤C2(ˆv1, v2),
∀v2 ∈U 2
ad,
where U 1
ad and U 2
ad represent the sets of constraints for both players.
In a two-person zero-sum game, one has
C1(v1, v2) = C(v1, v2),
C2(v1, v2) = −C(v1, v2).
So there is in fact only one functional, C(v1, v2). Player 1 wants to minimize
it, and player 2 wants to maximize it. It is a noncooperative game. A Nash
equilibrium satisﬁes the condition
C(ˆv1, v2) ≤C(ˆv1, ˆv2) ≤C(v1, ˆv2),
∀v1 ∈U 1
ad and ∀v2 ∈U 2
ad.
In this context the pair (ˆv1, ˆv2) is called a saddle point and C(ˆv1, ˆv2) the value
of the game. The existence of a saddle point is not so common for games. In
general, the so-called lower and upper values are diﬀerent
sup
v2
inf
v1 C(v1, v2) < inf
v1 sup
v2
C(v1, v2),
and they can respectively be −∞and +∞.
In this chapter we study the dynamical version on a ﬁnite time horizon
of the static zero-sum two-player game by introducing a time-dependent state
vector whose dynamics are governed by a system of linear diﬀerential equa-
tions. At that level there are open loop and closed loop strategies for the
players and sometimes an imperfect knowledge of the state. We shall restrict
ourselves to the open loop case and a perfect knowledge of the state. In that
context, the Min Sup problem was studied in 1969 by M. C. Delfour and
S. K. Mitter [1]. The fundamental theory of closed loop two-player zero-sum
LQ games was given in 1979 by P. Bernhard [2] followed by the seminal
book of T. Bas¸ar and P. Bernhard [1] in 1991 and 1995 that covered
the H∞-theory. The very nice work by P. Zhang [1] in 2005 established the
equivalence between the ﬁniteness of the open loop value of a two-player zero-
sum LQ game and the ﬁniteness of its open loop lower and upper values. It
means that the duality gap, that is the diﬀerence between the upper and the
lower values of the game, is either 0 or +∞. The reader is referred to the
above references for a detailed bibliography of the vast and rich literature on
dynamical games.
a Nash–Cournot equilibrium. However, Nash showed for the ﬁrst time in his dis-
sertation, Non-cooperative games (1950), that Nash equilibria must exist for all
ﬁnite games with any number of players. Until Nash, this had only been proved
for two-player zero-sum games by John von Neumann and Oskar Morgenstern [1]
(2nd edition, 1947).

1 Introduction
49
This new chapter is self-contained with mathematical proofs. Section 2
gives the deﬁnitions, notation, properties, semi-derivatives, and convexity/con-
cavity characterizations of the utility function.
Section 3 gives necessary and suﬃcient conditions for the existence of a
saddle point of the game and introduces the coupled system that will also
arise in the characterization of the open loop lower and upper values of the
game. This will later be completed in Theorem 5.1 in §5 with the equivalent
condition that the value v(x0) of the game is ﬁnite.
Section 4 is devoted to the case where the open loop lower value is ﬁnite.
By duality this also covers the case where the open loop upper value is ﬁnite.
It emphasizes the role of the feasibility condition.
Section 5 combines the results of §4 to characterize the ﬁniteness of the
open loop value of the game, and Theorem 5.1 shows its equivalence with the
existence of an open loop saddle point, completing the results of Theorem 3.1
in §3. It also includes the equivalence between the ﬁniteness of the upper and
lower values and the ﬁniteness of the value of P. Zhang [1]. Therefore, in the
linear-quadratic case, the duality gap is always zero or inﬁnite.
Section 6 constructs the associated Riccati diﬀerential equation in the
open loop saddle point case via invariant embedding with respect to the ini-
tial time. It is also shown that under the assumption of an open loop saddle
point in the time horizon [0, T ] for all initial states, there is an open loop sad-
dle point in the time horizon [s, T] for all s, 0 ≤s < T , and all initial states
(Theorem 6.3 (iii)). From this we get an optimality principle and adapt the
invariant embedding approach of R. Bellman in the style of J. L. Lions [3]
to construct the decoupling symmetrical matrix function P(s) and show that
it is an H1(0, T ) solution of the matrix Riccati diﬀerential equation. Thence
an open loop saddle point in [0, T ] yields closed loop optimal strategies for
both players that achieves a closed loop-closed loop saddle point in the sense of
P. Bernhard [2]. Furthermore, a necessary and suﬃcient set of conditions for
the existence of an open loop saddle point in [0, T ] is the convexity–concavity
of the utility function and the existence of an H1(0, T ) solution to the matrix
Riccati diﬀerential equation. This leaves the cases where either the open loop
lower value is −∞or the open loop upper value is = +∞. Two informative ex-
amples of solutions to the matrix Riccati diﬀerential equation where the open
loop saddle point does not exist are worked out, even when the solution of the
Riccati diﬀerential equation is strictly positive and inﬁnitely diﬀerentiable.
A third example is given where the solution of the Riccati diﬀerential equa-
tion is inﬁnitely diﬀerentiable and strictly positive and the open loop lower
and upper values are +∞. The chapter also brieﬂy discusses the open/close
loop upper/lower value of the game and generalized solutions of the Riccati
diﬀerential equation.
This chapter is an expanded version of M. C. Delfour [16].

50
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
2 Deﬁnitions, notation, and preliminary results
2.1 System, utility function, and values of the game
Given a ﬁnite dimensional Euclidean space Rd of dimension d ≥1, the norm
and inner product will be denoted by |x| and x·y, respectively and irrespective
of the dimension d of the space. Given T > 0, the norm and inner product in
L2(0, T ; Rn) will be denoted ∥f∥and (f, g). The norm in the Sobolev space
H1(0, T ; Rn) will be written ∥f∥H1.
Consider the following two-player zero-sum game over the ﬁnite time in-
terval [0, T ] characterized by the quadratic utility function
Cx0(u, v)
def
= Fx(T ) · x(T ) +
 T
0
Q(t)x(t) · x(t) + |u(t)|2 −|v(t)|2 dt,
(2.1)
where x is the solution of the linear diﬀerential system, the so-called state
equation
dx
dt (t) = A(t)x(t)+B1(t)u(t)+B2(t)v(t)
a.e. in [0, T ],
x(0) = x0, (2.2)
x0 is the initial state at time t = 0, u ∈L2(0, T ; Rm), m ≥1, is the strategy
of the ﬁrst player, and v ∈L2(0, T ; Rk), k ≥1, is the strategy of the second
player. We assume that F is an n × n-matrix and that A, B1, B2, and Q
are matrix-functions of appropriate order that are measurable and bounded
almost everywhere in [0, T ]. Moreover Q(t) and F are symmetrical (but not
necessarily positive semi-deﬁnite). The above assumptions on F, A, B1, B2,
and Q will be used throughout this chapter.
It will be convenient to use the following compact notation and drop the
a.e. in [0, T ] wherever no confusion arises
Cx0(u, v)
def
= Fx(T ) · x(T ) +
 T
0
Qx · x + |u|2 −|v|2 dt
and
x′ = Ax + B1u + B2v
a.e. in [0, T ],
x(0) = x0.
(2.3)
Remark 2.1. The following more general quadratic structure involving cross
terms:
Cx0(u, v)
def
= Fx(T ) · x(T ) +
 T
0
(x, u, v) ·
⎡
⎣
Q S
T
S∗N1
0
T ∗0 −N2
⎤
⎦
⎡
⎣
x
u
v
⎤
⎦dt,
(2.4)
can be brought back to the above form by the folllowing change of variables
⎡
⎣
x
u
v
⎤
⎦=
⎡
⎢⎣
I
0
0
−N −1
1 S∗N −1/2
1
0
N −1
2 T ∗
0
N −1/2
2
⎤
⎥⎦
⎡
⎣
x
¯u
¯v
⎤
⎦,
(2.5)

2 Deﬁnitions, notation, and preliminary results
51
where the new matrix functions N1, N2, S, and T are all assumed to be mea-
surable and bounded and N1(t) and N2(t) are symmetrical positive deﬁnite
matrices such that
∃ν1 > 0 such that ∀u ∈Rm and almost all t,
N1(t)u · u ≥ν1 |u|2,
∃ν2 > 0 such that ∀v ∈Rk and almost all t,
N2(t)v · v ≥ν2 |v|2.
(2.6)
This yields
Fx(T ) · x(T ) +
 T
0

Q −SN −1
1 S∗+ TN −1
2 T ∗
x · x + |¯u|2 −|¯v|2 dt
and the system
x′ = Ax + B1(−N −1
1 S∗x + N −1/2
1
¯u) + B2(N −1
2 T ∗x + N −1/2
2
¯v)
= (A −B1N −1
1 S∗+ B2N −1
2 T ∗)x + B1N −1/2
1
¯u + B2N −1/2
2
¯v.
By introducing the new matrices
Q = Q −SN −1
1 S∗+ TN −1
2 T ∗,
A = A −B1N −1
1 S∗+ B2N −1
2 T ∗,
B1 = B1N −1/2
1
,
B2 = B2N −1/2
2
,
we are back to the simpler initial structure
Cx0(¯u, ¯v)
def
= Fx(T ) · x(T ) +
 T
0
Qx · x + |¯u|2 −|¯v|2 dt,

x′ = Ax + B1¯u + B2¯v,
x(0) = x0.
It is readily seen that
inf
u∈L2(0,T ;Rm)
sup
v∈L2(0,T ;Rk)
Cx0(u, v) =
inf
¯u∈L2(0,T ;Rm)
sup
¯v∈L2(0,T ;Rk)
Cx0(¯u, ¯v),
sup
v∈L2(0,T ;Rk)
inf
u∈L2(0,T ;Rm) Cx0(u, v) =
sup
¯v∈L2(0,T ;Rk)
inf
¯u∈L2(0,T ;Rm) Cx0(¯u, ¯v).
For the H∞-theory, N1(t) = I and N2(t) = γ2I, where γ ̸= 0 and I is the
identity matrix. As for the classical optimal minimizing control problem, it
can be recovred by choosing N2(t) = I and B2(t) = 0.
⊓⊔
We shall also study the existence of a symmetrical solution to the associ-
ated matrix Riccati diﬀerential equation
P ′ + PA + A∗P −PRP + Q = 0
a.e. in [0, T ],
P(T ) = F,
(2.7)
where R = B1B∗
1 −B2B∗
2, in relation with the following objectives of the
game.

52
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
Deﬁnition 2.1. Let x0 be an initial state in Rn at time t = 0.
(i) The game is said to achieve its open loop lower value (resp. upper value)
if
v−(x0)
def
=
sup
v∈L2(0.T ;Rk)
inf
u∈L2(0.T ;Rm) Cx0(u, v)
(2.8)
(resp. v+(x0)
def
=
inf
u∈L2(0.T ;Rm)
sup
v∈L2(0.T ;Rk)
Cx0(u, v))
(2.9)
is ﬁnite. By deﬁnition v−(x0) ≤v+(x0).
(ii) The game is said to achieve its open loop value if its open loop lower
value v−(x0) and upper value v+(x0) are ﬁnite and
v−(x0) = v+(x0).
(2.10)
The open loop value of the game will be denoted by v(x0).
(iii) A pair (¯u, ¯v) in L2(0, T ; Rm) × L2(0, T ; Rk) is an open loop saddle point
of Cx0(u, v) in L2(0, T ; Rm)×L2(0, T ; Rk) if for all u in L2(0, T ; Rm) and
all v in L2(0, T ; Rk)
Cx0(¯u, v) ≤Cx0(¯u, ¯v) ≤Cx0(u, ¯v).
(2.11)
⊓⊔
In general, (ii) does not necessarily imply (iii), but we shall see that it does
for linear-quadratic games.
Deﬁnition 2.2. Associate with x0 ∈Rn the sets and the functions
V (x0)
def
=

v ∈L2(0, T ; Rk) :
inf
u∈L2(0,T ;Rm) Cx0(u, v) > −∞

,
(2.12)
U(x0)
def
=

u ∈L2(0, T ; Rm) :
sup
v∈L2(0,T ;Rk)
Cx0(u, v) < +∞

,
(2.13)
J−
x0(v)
def
=
inf
u∈L2(0,T ;Rm) Cx0(u, v),
J+
x0(u)
def
=
sup
v∈L2(0,T ;Rk)
Cx0(u, v).
(2.14)
⊓⊔
By deﬁnition, V (x0) ̸= ∅if and only if v−(x0) > −∞and U(x0) ̸= ∅if and
only if v+(x0) < +∞. In this chapter we only consider the open loop case.
Corresponding deﬁnitions can be given in the closed loop case, and the reader
is referred to P. Bernhard [2] and T. Bas¸ar and P. Bernhard [1].
2.2 Properties, semi-derivatives, and convexity/concavity of
Cx0(u, v)
The functional Cx0(u, v) is inﬁnitely diﬀerentiable, and since it is quadratic,
its Hessian of second order derivatives is independent of the point (u, v). More

2 Deﬁnitions, notation, and preliminary results
53
precisely2
1
2dCx0(u, v; ¯u, ¯v) = Fx(T ) · ¯y(T ) + (Qx, ¯y) + (u, ¯u) −(v, ¯v),
(2.15)
where x is the solution of (2.3) and ¯y is the solution of
¯y′ = A¯y + B1¯u + B2¯v,
¯y(0) = 0.
(2.16)
It is customary to introduce the adjoint state equation
p′ + A∗p + Qx = 0,
p(T ) = Fx(T )
(2.17)
and rewrite (2.15) for the gradient in the following form:
1
2dCx0(u, v; ¯u, ¯v) = (B∗
1p + u, ¯u) + (B∗
2p −v, ¯v).
(2.18)
As predicted, the Hessian is independent of (u, v):
1
2d2Cx0(u, v; ¯u, ¯v; ˜u, ˜v) = F ˜y(T ) · ¯y(T ) + (Q˜y, ¯y) + (˜u, ¯u) −(˜v, ¯v),
(2.19)
where ¯y is the solution of (2.16) and ˜y is the solution of
˜y′ = A˜y + B1˜u + B2˜v,
˜y(0) = 0.
(2.20)
In particular, for all x0, u, v, ¯u, and ¯v
d2Cx0(u, v; ¯u, ¯v; ¯u, ¯v) = 2 C0(¯u, ¯v),
(2.21)
and this yields the following characterizations of the u-convexity, v-concavity,
and (u, v)-convexity–concavity.
Lemma 2.1. The following statements are equivalent:
(i) the map u 	→C0(u, 0) : L2(0, T ; Rm) →R is convex;
(ii) for all u ∈L2(0, T ; Rm) , C0(u, 0) ≥0;
2 Given a real function f deﬁned on a Banach space B, the ﬁrst directional semi-
derivative at x in the direction v (when it exists) is deﬁned as
df(x; v)
def
= lim
t↘0
f(x + tv) −f(x)
t
.
When the map v →df(x; v) : B →R is linear and continuous, it deﬁnes the
gradient ∇f(x) as an element of the dual B∗of B. The second order bidirectional
derivative at x in the directions (v, w) (when it exists) is deﬁned as
d2f(x; v, w)
def
= lim
t↘0
df(x + tw; v) −df(x; v)
t
.
When the map (v, w) →d2f(x; v, w) : B × B →R is bilinear and continuous, it
deﬁnes the Hessian operator Hf(x) as a continuous linear operator from B to
B∗.

54
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
(iii) infu∈L2(0,T ;Rm) C0(u, 0) = C0(0, 0);
(iv) for all v and x0 the map u 	→Cx0(u, v) : L2(0, T ; Rm) →R is convex.
Proof. (i) ⇒(ii) If u 	→C0(u, 0) : L2(0, T ; Rm) →R is convex, then its
Hessian is positive or zero and the result follows from identity (2.21). (ii) ⇒
(iii) From (ii) and the fact that C0(0, 0) = 0. (iii) ⇒(iv) Since Cx0(u, v) is
quadratic in u, for all u and u′
Cx0(u′, v) −Cx0(u, v) −dCx0(u, v; u′ −u, 0)
= 1
2d2Cx0(u, v; u′ −u, 0; u′ −u, 0) = C0(u′ −u, 0) ≥C0(0, 0) = 0
from identity (2.21). (iv) ⇒(i) Pick x0 = 0 and v = 0.
⊓⊔
Corollary 2.1. The following statements are equivalent:
(i) the map v 	→C0(0, v) : L2(0, T ; Rk) →R is concave;
(ii) for all v ∈L2(0, T ; Rk) , C0(0, v) ≤0;
(iii) supv∈L2(0,T ;Rk) C0(0, v) = C0(0, 0);
(iv) for all u and x0, the map v 	→Cx0(u, v) : L2(0, T ; Rk) →R is concave.
Corollary 2.2. The following statements are equivalent:
(i) the map (u, v) 	→C0(u, v) : L2(0, T ; Rm) × L2(0, T ; Rk) →R is (u, v)-
convex–concave; that is,
∀v ∈L2(0, T ; Rk),
u 	→C0(u, v) is convex and
∀u ∈L2(0, T ; Rm),
v 	→C0(u, v) is concave;
(2.22)
(ii) the pair (0, 0) is a saddle point of C0(u, v):
sup
v∈L2(0,T ;Rk)
C0(0, v) = C0(0, 0) =
inf
u∈L2(0,T ;Rm) C0(u, 0);
(2.23)
(iii) supv∈L2(0,T ;Rk) C0(0, v) = C0(0, 0) = infu∈L2(0,T ;Rm) C0(u, 0);
(iv) for all x0 the map (u, v) 	→Cx0(u, v) : L2(0, T ; Rm) × L2(0, T ; Rk) →R
is (u, v)-convex–concave; that is,
∀v ∈L2(0, T ; Rk),
u 	→Cx0(u, v) is convex and
∀u ∈L2(0, T ; Rm),
v 	→Cx0(u, v) is concave.
(2.24)
3 Saddle point and coupled state–adjoint state system
We ﬁrst obtain necessary and suﬃcient conditions for the existence of a saddle
point of the game and introduces the coupled (state–adjoint state) system (cf.
Notation 3.1 on page 56) that will also arise in the characterization of the
open loop lower and upper values of the game in §4. Theorem 5.1 in §5 will
later complete this theorem with the equivalent condition that the value v(x0)
of the game is ﬁnite.

3 Saddle point and coupled state–adjoint state system
55
Theorem 3.1. The following conditions are equivalent:
(i) There exists an open loop saddle point of Cx0(u, v).
(ii) There exists a solution (ˆu, ˆv) in L2(0, T ; Rm) × L2(0, T ; Rk) of the system
∀u ∈L2(0, T ; Rm), ∀v ∈L2(0, T ; Rk),
dCx0(ˆu, ˆv; u, v) = 0,
(3.1)
and Cx0 is convex–concave in the sense of (2.24).
(iii) There exists a solution (x, p) ∈H1(0, T ; Rn)2 of the coupled system

x′ = Ax −B1B∗
1p + B2B∗
2p,
x(0) = x0
p′ + A∗p + Qx = 0,
p(T ) = Fx(T ),
(3.2)
ˆu = −B∗
1p,
ˆv = B∗
2p,
(3.3)
and
sup
v∈L2(0,T ;Rk)
C0(0, v) = C0(0, 0) =
inf
u∈L2(0,T ;Rm) C0(u, 0).
(3.4)
Under any one of the above conditions, the value of the game is given by
v(x0) = Cx0(ˆu, ˆv) = p(0) · x0.
(3.5)
Proof. (i) ⇒(ii). Let (¯u, ¯v) in L2(0, T ; Rm) × L2(0, T ; Rk) be an open loop
saddle point of Cx0(u, v) in L2(0, T ; Rm)×L2(0, T ; Rk). Then by Deﬁnition 2.1
sup
L2(0,T ;Rk)
Cx0(¯u, v) = Cx0(¯u, ¯v) =
inf
L2(0,T ;Rm) Cx0(u, ¯v).
(3.6)
Since Cx0(u, v) is inﬁnitely diﬀerentiable, the minimizing point ¯u of Cx0(u, ¯v)
with respect to u is characterized by the ﬁrst order condition dCx0(¯u, ¯v; u, 0) =
0 for all u and the second order condition d2Cx0(¯u, ¯v; u, 0; u, 0) ≥0 for all u.
Since d2Cx0(¯u, ¯v; u, 0; u, 0) is independent of (¯u, ¯v) and x0, Cx0(u, v) is con-
vex in u for all x0 and all v. A similar argument for the maximum yields
dCx0(¯u, ¯v; , w) = 0 and d2Cx0(¯u, ¯v; 0, w; 0, w) ≤0 for all w and the concavity
of Cx0(u, v) in v.
(ii) ⇒(i). By assumption Cx0(ˆu, ˆv) is convex–concave and inﬁnitely diﬀer-
entiable and there is a solution to the two ﬁrst order conditions. By I. Eke-
land and R. Temam [1], there exists a saddle point.
(ii) ⇔(iii) From the previous computations of the gradient and Corol-
lary 2.2.
Finally, for the computation of the value
Cx0(ˆu, ˆv) = Fx(T ) · x(T ) + (Qx, x) + ∥B∗
1p∥2 −∥B∗
2p∥2
= p(T ) · x(T ) −(p′ + A∗p, x) + ([B1B∗
1 −B2B∗
2]p, p)
= p(0) · x(0) + (p, x′ −Ax + ([B1B∗
1 −B2B∗
2]p) = p(0) · x0.
⊓⊔

56
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
Notation 3.1. System (3.2) will be referred to as the coupled state–adjoint state
system or simply the coupled system. It will also be useful to denote by Nx,p
the set of all solutions (y, q) of the associated homogeneous coupled system

y′ = Ay −B1B∗
1q + B2B∗
2q,
y(0) = 0,
q′ + A∗q + Qy = 0,
q(T ) = Fy(T ).
(3.7)
Thus system (3.2) has a solution up to an additive pair of Nx,p.
⊓⊔
4 Finite open loop lower value
In this section we give a set of necessary and suﬃcient conditions for the
ﬁniteness of the open loop lower value of the game. By dual assumptions, we
also get a set of necessary and suﬃcient conditions for the ﬁniteness of the
open loop upper value of the game. We ﬁrst state the main theorems. Then, we
proceed in three steps: existence and characterization of a minimizer for each
v ∈V (x0), formulation of the resulting maximization problem with respect
to v ∈V (x0), and existence and characterization of the pair that achieves the
ﬁnite open loop lower value of the game. Finally, we prove the uniqueness of
the solution of the coupled system under the assumption that the open loop
lower value is ﬁnite for all initial states in §4.6.
4.1 Main theorems
The quadratic character of the problem yields surprising equivalences that
reduce the complexity of its solution. We start with the open loop lower value
of the game.
Theorem 4.1. The following conditions are equivalent.
(i) There exist ˆu in L2(0, T ; Rm) and ˆv in L2(0, T ; Rk) such that
Cx0(ˆu, ˆv) =
inf
u∈L2(0,T ;Rm) Cx0(u, ˆv) =
sup
v∈L2(0,T ;Rk)
inf
u∈L2(0,T ;Rm) Cx0(u, v). (4.1)
(ii) The open loop lower value v−(x0) of the game is ﬁnite.
(iii) There exists a solution (x, p) ∈H1(0, T ; Rn)×H1(0, T ; Rn) of the coupled
system (3.2) such that B∗
2p ∈V (x0), the solution pairs (ˆu, ˆv) is given by
the expressions
ˆu = −B∗
1p,
ˆv = B∗
2p,
(4.2)
and
sup
v∈V (0)
inf
u∈L2(0,T ;Rm) C0(u, v) =
inf
u∈L2(0,T ;Rm) C0(u, 0) = C0(0, 0).
(4.3)

4 Finite open loop lower value
57
Under any one of the above conditions, the open loop lower value is given by
v−(x0) = Cx0(ˆu, ˆv) = p(0) · x0.
(4.4)
The proof of this main theorem will be given in §4 and §4.5.
Remark 4.1. The above necessary and suﬃcient conditions for the ﬁniteness
of the open loop value of the game complete the results of P. Zhang [1] by
introducing the feasibility condition (4.3) that is equivalent to saying that the
open loop lower value of the game is zero and that (0, 0) is a solution for the
zero initial state. It also recasts the results in the more intuitive state/adjoint
state framework. Condition (4.3) is equivalent to the convexity of Cx0(u, v)
with respect to u and the concavity of J−
x0(v) = infu∈L2(0,T ;Rm) Cx0(u, v) with
respect to v ∈V (x0).
⊓⊔
This theorem has a counterpart for the upper value v+(x0) of the game.
Theorem 4.2. The following conditions are equivalent:
(i) There exist ˆu in L2(0, T ; Rm) and ˆv in L2(0, T ; Rk) such that
Cx0(ˆu, ˆv) =
sup
v∈L2(0,T ;Rk)
Cx0(ˆu, v) =
inf
u∈L2(0,T ;Rm)
sup
v∈L2(0,T ;Rk)
Cx0(u, v). (4.5)
(ii) The open loop upper value v+(x0) of the game is ﬁnite.
(iii) There exists a solution (x, p) ∈H1(0, T ; Rn)×H1(0, T ; Rn) of the coupled
system (3.2) such that −B∗
1p ∈U(x0), the solution pairs (ˆu, ˆv) is given by
the expressions
ˆu = −B∗
1p,
ˆv = B∗
2p
(4.6)
and
inf
u∈U(0)
sup
v∈L2(0,T ;Rk)
C0(u, v) =
sup
v∈L2(0,T ;Rk)
C0(0, v) = C0(0, 0).
(4.7)
Under any one of the above conditions, the open loop upper value is given by
v+(x0) = Cx0(ˆu, ˆv) = p(0) · x0.
(4.8)
Condition (4.7) says that Cx0(u, v) is concave with respect to v and that
J+
x0(u) = supv∈L2(0,T ;Rk) Cx0(u, v) is convex with respect to u ∈U(x0).
4.2 Abstract operators and a preliminary lemma
The diﬀerential equation (2.2) has a unique solution x = x(·; x0, u, v) in
H1(0, T ; Rn) and the solution map
x0, u, v 	→x(·; x0, u, v): Rn × L2(0, T ; Rm) × L2(0, T ; Rk) →H1(0, T ; Rn)

58
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
is linear and continuous. Thus we can write
x(·; x0, u, v) = K0x0 + K1u + K2v
(4.9)
by introducing the following continuous linear operators:
K0 : Rn →H1(0, T ; Rn),
K1 : L2(0, T ; Rm) →H1(0, T ; Rn),
K2 : L2(0, T ; Rk) →H1(0, T ; Rn).
(4.10)
Finally introduce the continuous self-adjoint linear map
Λ(x, xT )
def
= (Qx, FxT ): L2(0, T ; Rn) × Rn →L2(0, T ; Rn) × Rn,
(4.11)
the continuous linear compact injection
i(x)
def
=

x, x(T )

: H1(0, T ; Rn) →L2(0, T ; Rn) × Rn,
(4.12)
and the continuous bilinear form
qx0

(u, v), (¯u, ¯v)
 def
=

Λi(K0x0 + K1u + K2v), i(K0x0 + K1¯u + K2¯v)

L2×Rn
+ (u, ¯u)L2 −(v, ¯v)L2.
(4.13)
Then
Cx0(u, v) = qx0

(u, v), (u, v)

.
Lemma 4.1. Let U be a Hilbert space, M : U →U a continuous linear self-
adjoint compact operator, f ∈U, c a constant, and j(u) = c + 2(f, u) + ([I +
M]u, u).
(i) Then the following conditions are equivalent:
a)
∃ˆu ∈U,
j(ˆu) = inf
u∈U j(u),
(4.14)
b)
inf
u∈U j(u) > −∞,
(4.15)
c)
∃ˆu ∈U such that [I + M]ˆu + f = 0
(4.16)
and ∀u ∈U,
([I + M]u, u) ≥0.
(4.17)
(ii) Condition (4.16) is equivalent to
∀w ∈ker[I + M],
(f, w) = 0.
(4.18)
(iii) Condition (4.17) is equivalent to the convexity of j.

4 Finite open loop lower value
59
Proof. (i) a) ⇒b). Obvious. b) ⇒c). We proceed by contradiction. Since M
is compact, I + M has closed range Im[I + M].
If (4.16) is not true, then f /∈Im[I + M] = (ker[I + M])⊥since Im[I + M]
is closed. Let P be the orthogonal projection onto Im[I + M]. For λ > 0, let
uλ = −λ(f −Pf). Then
j(uλ) = c + 2(f, uλ) + ([I + M]uλ, uλ) = −λ∥f −Pf∥2.
Since f /∈Im[I + M], then f −Pf ̸= 0, and as λ goes to inﬁnity, j(uλ) goes
to −∞that contradicts condition (4.15).
If condition (4.17) is not veriﬁed, then there exists w ̸= 0 such that
([I + M]w, w) < 0.
For λ > 0 let uλ = ˆu + λw. Then
j(uλ) = c + 2(f, uλ) + ([I + M]uλ, uλ)
= j(ˆu) + 2λ([I + M]ˆu + f, w) + λ2([I + M]w, w)
= j(ˆu) + λ2([I + M]w, w),
and j(uλ) goes to −∞as λ goes to inﬁnity since the coeﬃcient of λ2 is strictly
negative. This contradiction yields (4.15).
c) ⇒a). From conditions (4.16) and (4.17), there exists a ˆu ∈U such that
j(ˆu) = inf
u∈H j(u).
(ii) Since I + M has a closed range, condition (4.16) is equivalent to
f ∈Im[I + M] = (ker[I + M])⊥=⇒∀w ∈ker[I + M],
(f, w) = 0.
(iii) is obvious.
⊓⊔
4.3 Existence and characterization of the minimizers
Theorem 4.3. Given x0 ∈Rn and v ∈L2(0, T ; Rk), the following statements
are equivalent:
(i) There exists ˆu ∈L2(0, T ; Rm) such that
Cx0(ˆu, v) = J−
x0(v) =
inf
u∈L2(0,T ;Rm) Cx0(u, v).
(4.19)
(ii) infu∈L2(0,T ;Rm) Cx0(u, v) > −∞(that is, v ∈V (x0)).
(iii) There exists a pair (x, p) ∈H1(0, T ; Rn) × H1(0, T ; Rn) solution of the
system

x′ = Ax −B1B∗
1p + B2v,
x(0) = x0,
p′ + A∗p + Qx = 0,
p(T ) = Fx(T ),
(4.20)
ˆu(t) = −B∗
1(t)p(t),
(4.21)

60
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
and
inf
u∈L2(0,T ;Rm) C0(u, 0) ≥0.
(4.22)
(iv) The convexity condition (4.22) is veriﬁed and
∀q ∈Np,
x0 · q(0) +
 T
0
v · B∗
2q dt = 0,
(4.23)
where
Np
def
=

q ∈H1(0, T ; Rn) : ∀(y, q) ∈Nx,p
 
(4.24)
and Nx,p denotes the set of all solutions (y, q) of the homogeneous system

y′ = Ay −B1 B∗
1q,
y(0) = 0,
q′ + A∗q + Qy = 0,
q(T ) = Fy(T ).
(4.25)
Under any one of the avove conditions, for all (x, p) solution of (4.20)
J−
x0(v) = p(0) · x0 +
 T
0
B∗
2p · v −|v|2 dt.
(4.26)
Proof. The proof of the theorem will require Lemma 4.1. By deﬁnition
qx0

(u, v), (¯u, ¯v)

=

Λi(K0x0 + K2v), i(K0x0 + K2¯v)

L2×Rn −(v, ¯v)L2
+

Λi(K1u), i(K0x0 + K2¯v)

L2×Rn
+

Λi(K0x0 + K2v), i(K1¯u)

L2×Rn
+

Λi(K1u), i(K1¯u)

L2×Rn + (u, ¯u)L2.
(4.27)
Let M = (iK1)∗Λ(iK1) and f = (iK1)∗Λi(K0x0 + K2v) and c =

Λi(K0x0 +
K2v), i(K0x0 + K2v)

L2×Rn −(v, v)L2. Then
Cx0(u, v) = j(u)
def
= c + 2(f, u) + ([I + M]u, u),
where M is linear, continuous, and compact. So we can use Lemma 4.1.
(i) ⇔(ii) From the equivalence of a) and b) in Lemma 4.1.
(ii) ⇔(iii) From the equivalence of b) and c) in Lemma 4.1 using the
computation of ﬁrst and second order derivatives (2.18) and (2.21) in §2.2.
Condition (4.16) of the set of necessary and suﬃcient conditions of
Lemma 4.1 becomes
∃ˆu, ∀u ∈L2(0, T ; Rm), ([I+(iK1)∗Λ(iK1)]ˆu+(iK1)∗Λi(K0x0+K2v), u) = 0,
∃ˆu, ∀u ∈L2(0, T ; Rm), (ˆu, u) + (Λi(K0x0 + K1ˆu + K2v), iK1u) = 0,
∃ˆu, ∀u ∈L2(0, T ; Rm), (ˆu, u) + (Λix, iyu) = 0,
(4.28)

4 Finite open loop lower value
61
where
x′ = Ax + B1ˆu + B2v,
x(0) = x0.
(4.29)
By introducing the adjoint system
p′ + A∗p + Qx = 0,
p(T ) = Fx(T ),
(4.30)
condition (4.28) becomes
∃ˆu, ∀u ∈L2(0, T ; Rm),
Fx(T ) · yu(T ) +
 T
0
Qx · yu + ˆu · u dt = 0
⇐⇒∃ˆu, ∀u ∈L2(0, T ; Rm),
 T
0
[B∗
1p + ˆu] · u dt = 0 ⇐⇒ˆu = −B∗
1p.
This means that system (4.20) has at least one solution (x, p).
Condition (4.22) follows from condition (4.17) and identity (2.21)
∀¯u,
2 C0(¯u, 0) = d2Cx0(u, v; ¯u, 0; ¯u, 0) = 2([I + M]¯u, ¯u) ≥0.
(4.31)
(iii) ⇔(iv) We use the equivalence between conditions (4.18) and (4.16)
in Lemma 4.1 (ii). So it is suﬃcient to explicit condition (4.18). Note that the
spaces Nx,p and Np are both linear.
Computation of Nu = ker[I + (iK1)∗Λ(iK1)]. Any w ∈Nu is solution of
∀u,

(iK1)∗Λ(iK1)w, u

+ (w, u) = 0,
∀u,
(Λ(iK1w), iK1u) + (w, u) = 0,
∀u,
Fyw(T ) · yu(T ) +
 T
0
Qyw · yu + w · u dt = 0,
where yw is the solution of the equation
y′
w = Ayw + B1w = 0,
yw(0) = 0.
(4.32)
Introduce the solution qw of the adjoint system
q′
w + A∗qw + Qyw = 0,
qw(T ) = Fyw(T ).
By substitution of Qyw and integration by parts
∀u,
Fyw(T ) · yu(T ) +
 T
0
Qyw · yu + w · u dt = 0
⇐⇒∀u,
 T
0
[B∗
1qw + w] · u dt = 0 ⇐⇒w = −B∗
1qw.
By substitution of w in (4.32), the pair (yw, qw) is a solution of system (4.25),
qw ∈Np, and Nu ⊂−B∗
1Np, where the linear subspace Np is given by (4.24).

62
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
Conversely, if w ∈B∗
1Np, there exists a solution (y, q) of system (4.25) such
that w = −B∗
2q and hence yw = y. So by reverting the above sequence of
equivalences, we conclude that w ∈Nu, and since Np is a linear subspace,
Nu = B∗
2Np.
Condition (4.18) now becomes
∀w ∈Nu,

(iK1)∗Λi(K0x0 + K2v), w

= 0,
∀w ∈Nu,
(Λi(K0x0 + K2v), iK1w) = 0,
∀w ∈Nu,
(Λix, iyw) = 0,
where (x, p) is the solution of the system

x′ = Ax + B2v = 0,
x(0) = x0,
p′ + A∗p + Qx = 0,
p(T ) = Fx(T ).
(4.33)
This yields
∀w ∈Nu,
Fx(T ) · yw(T ) +
 T
0
Qx · yw dt = 0,
∀w ∈Nu,
x0 · qw(0)+(v, B∗
2qw)=0
⇒∀q ∈Np,
x0 · q(0)+(v, B∗
2q)=0,
and hence condition (4.23).
⊓⊔
Notation 4.1. Given x0 ∈Rn such that V (x0) ̸= ∅and v ∈V (x0), denote by
P(v, x0) the set of all solutions (x, p) of system (4.20). It is readily checked
that for all p ∈P(v, x0), P(v, x0) = p + Np.
⊓⊔
4.4 Intermediary results
Theorem 4.4.
(i) The sets Nx,p, Np, and B∗
2Np are ﬁnite dimensional linear subspaces of
H1(0, T ; Rn)2, H1(0, T ; Rn), and L2(0, T ; Rk), respectively. P(v, x0) is a
ﬁnite dimensional aﬃne subspace of H1(0, T ; Rn).
(ii) If V (x0) ̸= ∅for some x0 ∈Rn, V (x0) is a closed aﬃne subspace of
L2(0, T ; Rk), V (0) is a non-empty closed linear subspace of L2(0, T ; Rk),
V (0) = (B∗
2Np)⊥,
(4.34)
∀v ∈V (x0),
V (x0) = v + V (0).
(4.35)
(iii) Given v ∈V (x0) and p ∈P(v, x0), deﬁne
v∗def
= v + PV (0)(B∗
2p −v),
(4.36)
where PV (0) is the orthogonal projection onto V (0) in L2(0, T ; Rk). Then
v∗is independent of the choice of p, v∗is unique in V (x0) ∩B∗
2P(v, x0),
and there exists p∗∈P(v, x0) such that v∗= B∗
2p∗. If, in addition, B∗
2p−
v ∈V (0)⊥, then v = v∗= B∗
2p∗.

4 Finite open loop lower value
63
Analogs of Theorems 4.3 and 4.4 hold for the open loop upper value.
Remark 4.2. This theorem is a key element in the proof of the existence of
a maximizer of the inf problem. The basic ideas and the arguments are due
to P. Zhang [1]. For completeness, we have added part (i) to show that the
subspace B∗
2Np is ﬁnite dimensional and hence closed. This is critical in the
proof of part (ii).
⊓⊔
Proof of Theorem 4.4. (i) From system (4.25), Nx,p is a closed linear subspace
of H1(0, T ; Rn)2 as the kernel of the continuous linear map
(x, p) 	→A(x, p)
def
= (−x′ + Ax −B1 B∗
1p, −x(0), p′ + A∗p + Qx, Fx(T ) −p(T ))
: H1(0, T ; Rn)2 →

L2(0, T ; Rn) × Rn2 .
We now use the fact that a topological vector space is ﬁnite dimensional if and
only if every closed bounded set is compact. Indeed let K be a closed bounded
subset of points (y, q) in Nx,p for the L2(0, T ; Rn)2-topology. Since all the ma-
trices in system (4.25) are bounded, the right-hand sides are bounded and
the derivatives (y′, q′) are also bounded in L2(0, T ; Rn)2 and, a fortiori, in
H1(0, T ; Rn)2. Since the injection of H1(0, T ; Rn)2 into L2(0, T ; Rn)2 is com-
pact, then the closure of K in L2(0, T ; Rn)2 is compact. But, by assumption,
we already know that K is closed. Thence K is compact in L2(0, T ; Rn)2 and
Nx,p is ﬁnite dimensional.
(ii) Since V (x0) ̸= ∅, then, by deﬁnition, for all v1, v2 in V (x0), condition
(ii) of Theorem 4.3 is veriﬁed and condition (iii) is also veriﬁed for some pairs
(x1, p1) and (x2, p2) verifying system (4.20). Therefore, for any α ∈R, the pair
(xα, pα) = (αx1 +(1−α)x2, αp1 +(1−α)p2) is also a solution of system (4.20)
for x0 and vα = αv1 + (1−α)v2 ∈V (x0). Identity (4.35) follows from the fact
that V (x0) is an aﬃne subspace. Moreover, from (4.35), V (x0) ̸= ∅necessarily
implies that V (0) ̸= ∅. Finally, from condition (4.23) with x0 = 0
v ∈V (0)
⇔
∀q ∈Np,
 T
0
v · B∗
2q dt = 0
⇔v ∈(B∗
2Np)⊥
and V (0) = (B∗
2Np)⊥, a non-empty closed linear subspace.
(iii) Given p1, p2 in P(v, x0), p2 −p1 ∈Np and
v + PV (0)(B∗
2p2 −v) −(v + PV (0)(B∗
2p1 −v)) = PV (0)(B∗
2(p2 −p1)) = 0,
since B∗
2Np = V (0)⊥. So v∗is independent of the choice of p ∈P(v, x0). Since
V (x0) is aﬃne, then for all v ∈V (x0),
v∗= v + PV (0)(B∗
2p −v) ∈v + V (0) = V (x0),
(4.37)
v∗−B∗
2p = v −B∗
2p −PV (0)(v −B∗
2p) ∈V (0)⊥= B∗
2Np
⇒∃q ∈Np such that v∗−B∗
2p = B∗
2q
⇒v∗= B∗
2(p + q) ∈B∗
2P(v, x0),

64
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
and v∗∈V (x0) ∩B∗
2P(v, x0). This element is unique since for v∗
1 and v∗
2 in
V (x0) ∩B∗
2P(v, x0), v∗
2 −v∗
1 ∈V (0) ∩B∗
2Np = V (0) ∩V (0)⊥= {0}. Finally,
if B∗
2p −v ∈V (0)⊥, then from (4.37) we get v = v∗.
⊓⊔
4.5 Existence and characterization of maximizers of the minimum
Assume that v−(x0) is ﬁnite. By deﬁnition of V (x0)
v−(x0) =
sup
v∈L2(0,T ;Rk)
inf
u∈L2(0,T ;Rm) Cx0(u, v) =
sup
v∈V (x0)
J−
x0(v),
(4.38)
where V (x0) is a closed aﬃne subspace of L2(0, T ; Rk) and by (4.26) and
condition (4.23)
J−
x0(v) =
inf
u∈L2(0,T ;Rm) Cx0(u, v) = p(0) · x0 +
 T
0
B∗
2p · v −|v|2 dt,
(4.39)
or, equivalently,
J−
x0(v) = Fx(T ) · x(T ) +
 T
0
Q(t)x(t) · x(t) + |B∗
1(t)p(t)|2 −|v(t)|2 dt, (4.40)
for all solutions (x, p) of system (4.20). Deﬁne the equivalence class [(x, p)] =
(x, p) + Nx,p. Then for each pair v ∈V (x0), [(x, p)] is the unique solution in
H1(0, T ; Rn) × H1(0, T ; Rn)/Nx,p of system (4.20). So the map
v 	→[(x, p)] : V (x0) →H1(0, T ; Rn) × H1(0, T ; Rn)
Nx,p
(4.41)
is aﬃne and continuous, and the map
(x, p) 	→(x(T ), x, p)
: H1(0, T ; Rn) × H1(0, T ; Rn) →Rn ×L2(0, T ; Rn) × L2(0, T ; Rn)
(4.42)
is continuous and compact.
So we are back to a continuous linear quadratic function J−
x0(v) that is
to be maximized over the closed aﬃne subspace V (x0). The state is now the
pair (x, p) solution of (4.20), but the structure is the same. Lemma 4.1 readily
extends to the case of a sup over a closed aﬃne subspace and the following
conditions are equivalent:
a)
∃ˆv ∈V (x0),
J−
x0(ˆv) =
sup
v∈V (x0)
J−
x0(v),
(4.43)
b)
sup
v∈V (x0)
J−
x0(v) < +∞,
(4.44)
c)
∃ˆv ∈V (x0) such that [I + M]ˆv + f ∈V (0)⊥
(4.45)
and ∀w ∈V (0),
([I + M]w, w) ≤0,
(4.46)

4 Finite open loop lower value
65
for the new compact operator M corresponding to the new state (x, p).
It remains to compute the directional derivative of J−
x0(v) at v ∈V (x0) in
the direction w ∈V (0). By direct computation from formula (2.18)
1
2dCx0(−B∗
1p, v; 0, w) =
 T
0
(B∗
2p −v) · w dt
(4.47)
that is independent of p ∈P(v, x0) for all w ∈V (0) from formula (4.23) of
Theorem 4.3 (iv). Hence
dJ−
x0(v; w) = dCx0(−B∗
1p, v; 0, w)
= 2
 T
0
(B∗
2p −v) · w dt,
∀p ∈P(v, x0).
(4.48)
As for the second order derivative:
1
2d2Cx0(−B∗
1p, v; 0, w; 0, w′)
= Fyw′(T ) · yw(T ) +
 T
0
Qyw′ · yw + B∗
1qw′ · B∗
1qw −w′ · w dt
(4.49)
⇒1
2d2J−
x0(v; w; w) = 1
2d2Cx0(−B∗
1p, v; 0, w; 0, w)
= 1
2J−
0 (w) = 1
2
inf
u∈L2(0.T ;Rm) C0(u, w),
(4.50)
where the last term must be negative or zero for all w ∈V (0). But, from
Theorem 4.3 (iii), C0(u, 0) is convex in u. By using the equivalent condition
of Lemma 2.1 (ii) for the u-convexity of C0(u, 0), we ﬁnally get the following
two-part condition:
sup
v∈V (0)
inf
u∈L2(0,T ;Rm) C0(u, v) ≤0 ≤
inf
u∈L2(0,T ;Rm) C0(u, 0).
(4.51)
This condition is equivalent to condition (4.3) since C0(0, 0) = 0. Indeed
condition (4.3) implies condition (4.51). Conversely from condition (4.51)
C0(0, 0) = 0 ≤
inf
u∈L2(0,T ;Rm) C0(u, 0) ≤
sup
v∈V (0)
inf
u∈L2(0,T ;Rm) C0(u, v)
sup
v∈V (0)
inf
u∈L2(0,T ;Rm) C0(u, v) ≤
inf
u∈L2(0,T ;Rm) C0(u, 0) ≤C0(0, 0) = 0,
and this yields condition (4.3).
Proof of Theorem 4.1.
(i) ⇒(ii) is obvious. (ii) ⇒(iii) From the previous
discussion, the ﬁniteness of v−(x0) is equivalent to
∃ˆv ∈V (x0) such that dJ−
x0(ˆv; w) = 2
 T
0
(B∗
2 ˆp −ˆv) · w dt = 0,
∀w ∈V (0),
d2J−
x0(ˆv; w; w) = 2
inf
u∈L2(0.T ;Rm) C0(u, w) ≤0,
∀w ∈V (0).

66
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
The second order condition combined with the fact that V (x0) ̸= ∅(the
equivalence of part (ii) and (iii) in Theorem 4.3 and hence the convexity
inequality (4.22)) yields condition (4.3). The ﬁrst order condition says that
B∗
2 ˆp −ˆv ∈V (0)⊥. By Theorem 4.4 (iii) there exists ˆp∗∈P(ˆv, x0) such that
ˆv = B∗
2 ˆp∗, where (ˆx∗, ˆp∗) is a solution of (4.20). Since ˆv = B∗
2 ˆp∗, the coupled
system (3.2) has a solution unique up to an element of Nx,p. After substitution
of ˆv = B∗
2 ˆp∗in (4.20), (ˆx∗, ˆp∗) becomes a solution of the coupled system (3.2).
This also yields the identities (4.2). (iii) ⇒(i) By assumption ˆv = B∗
2p ∈
V (x0). The existence of a solution (x, p) to system (3.2) yields the existence
of a solution to system (4.20) of Theorem 4.3 (iii) with ˆu = −B∗
1p as a
minimizer. For all v ∈V (x0)
J−
x0(v) = J−
x0(B∗
2p) + dJ−
x0(B∗
2p; v −B∗
2p) + 1
2d2J−
x0(B∗
2p; v −B∗
2p; v −B∗
2p).
The second order term is negative by condition (4.3) since, by assumption,
B∗
2p ∈V (x0) and hence v −B∗
2p ∈V (0) for all v ∈V (x0). As for the ﬁrst
order term, recall that, in view of (4.2), for all w ∈V (0)
dJ−
x0(B∗
2p; w) =
 T
0
(B∗
2p −v) · w dt = 0.
Thus dJ−
x0(B∗
2p; v−B∗
2p) = 0 since v−B∗
2p ∈V (0). Hence B∗
2p is a maximizer
of J−
x0.
⊓⊔
4.6 Finite open loop lower value for all initial states and
uniqueness of solution of the coupled system
In this section we sharpen the results of the previous section when the open
loop lower value, value, or upper value of the game is ﬁnite for all initial state
x0 ∈Rn. In each case this global assumption yields the uniqueness of solution.
Theorem 4.5. The following conditions are equivalent:
(i) For each x0 ∈Rn, there exist ˆu in L2(0, T ; Rm) and ˆv in L2(0, T ; Rk) such
that
Cx0(ˆu, ˆv) =
inf
u∈L2(0,T ;Rm) Cx0(u, ˆv) =
sup
v∈L2(0,T ;Rk)
inf
u∈L2(0,T ;Rm) Cx0(u, v). (4.52)
(ii) For each x0 ∈Rn, the open loop lower value v−(x0) of the game is ﬁnite.
(iii) For each x0 ∈Rn, there exists a unique pair (x, p) ∈H1(0, T ; Rn)2 so-
lution of the coupled system (3.2) such that B∗
2p ∈V (x0), there exists a
unique pair (ˆu, ˆv) that veriﬁes (3.3), and
sup
v∈V (0)
inf
u∈L2(0,T ;Rm) C0(u, 0) =
inf
u∈L2(0,T ;Rm) C0(u, 0) = C0(0, 0).
(4.53)

4 Finite open loop lower value
67
Note that under any one of the above conditions, (0, 0) is the unique solution
of (4.53).
Remark 4.3. The uniqueness under condition (i) was originally given by
P. Zhang, G. Zheng, Y. Xu, and J. Xi [1] by a diﬀerent argument. Our
short and transparent proof seems to be new. The same proof can readily be
used in the context of Optimal Control (cf. J. L. Lions [3]) .
⊓⊔
Proof. (i) ⇒(ii) is obvious. (ii) ⇒(iii). From Theorem 4.1 where condi-
tion (4.53) is condition (4.3). We only need to show the uniqueness of solution
to the coupled system (3.2). By linearity, this amounts to prove that the so-
lution (y, q) of the homogeneous system (3.7) such that B∗
2q ∈V (0) is (0, 0).
Given an arbitrary x0 consider the expression
q(0) · x0 = q(T ) · x(T ) −
 T
0
q′ · x + q · x′ dt
= Fx(T ) · y(T ) +
 T
0
Qx · y + B∗
1p · B∗
1q −B∗
2p · B∗
2q dt
= 1
2dCx0(ˆu, ˆv; −B∗
1q, B∗
2q) = 0
from (2.15), (3.2), (4.2), and the fact that B∗
2q ∈V (0). Since this identity
is true for all x0 ∈Rn, then q(0) = 0. But now we can look at the coupled
system (3.7) as a linear diﬀerential system of 2n equations in (x, p) with zero
initial condition (y(0), q(0)) = (0, 0) whose unique solution is (y, q) = (0, 0).
This proves the uniqueness. (iii) ⇒(i). Again from Theorem 4.1 since the
conditions are veriﬁed for each x0 ∈Rn.
⊓⊔
We readily have the dual result.
Theorem 4.6. The following conditions are equivalent:
(i) For each x0 ∈Rn, there exist ˆu in L2(0, T ; Rm) and ˆv in L2(0, T ; Rk) such
that
Cx0(ˆu, ˆv) =
sup
v∈L2(0,T ;Rk)
Cx0(ˆu, v) =
inf
u∈L2(0,T ;Rm)
sup
v∈L2(0,T ;Rk)
Cx0(u, v). (4.54)
(ii) For each x0 ∈Rn, the open loop upper value v+(x0) of the game is ﬁnite.
(iii) For each x0 ∈Rn, there exists a unique pair (x, p) ∈H1(0, T ; Rn)2 solu-
tion of the coupled system (3.2) such that −B∗
1p ∈U(x0), there exists a
unique pair (ˆu, ˆv) that veriﬁes (3.3), and
inf
u∈U(0)
sup
v∈L2(0,T ;Rk)
C0(u, v) =
sup
v∈L2(0,T ;Rk)
C0(0, v) = C0(0, 0).
(4.55)
Again, under any one of the above conditions, (0, 0) is the unique solution
of (4.55).

68
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
5 Finite open loop value and open loop saddle point
In this section we show that the necessary and suﬃcient condition for the
ﬁniteness of the value v(x0) of the game is the existence of a saddle point
of the utility function or a Nash equilibrium (cf. footnote 1 on page 48). In
the process, we complete the characterization of an open loop saddle point in
Theorem 3.1 by showing that the, a priori, convexity–concavity of the utility
function is in fact necessary.
Theorem 5.1. The following conditions are equivalent:
(i) There exists an open loop saddle point of Cx0(u, v).
(ii) The open loop value v(x0) of the game is ﬁnite.
(iii) There exists a solution (x, p) ∈H1(0, T ; Rn)×H1(0, T ; Rn) of the coupled
system (3.2), the solution pair (ˆu, ˆv) is given by the expressions (3.3), and
the convexity–concavity (3.4) is veriﬁed.
Under any one of the above conditions, the open loop value is given by (3.5).
Proof. (i) ⇒(ii). Since the utility function has a saddle point, the value of the
game is ﬁnite. (ii) ⇒(iii). From Theorems 4.1 and 4.2, there exists a solution to
the coupled system (3.2) and the convexity–concavity condition (3.4) readily
follows from (4.7) and (4.3). (iii) ⇒(i). From condition (3.4) Cx0(u, v) is
convex–concave. Moreover the coupled system (3.2) has a solution. Therefore
by Theorem 3.1 (iii) Cx0(u, v) has an open loop saddle point.
⊓⊔
Remark 5.1. The common part of the necessary condition for the ﬁniteness of
the lower value v−(x0), value v(x0), and upper value v+(x0) of the game is
the existence of a solution of the coupled system (3.2). The diﬀerence is in the
respective feasibility conditions (4.3), (3.4), and (4.7): v−(0) = 0, v(0) = 0,
and v+(0) = 0.
⊓⊔
We conclude with the enlightening result proved by P. Zhang [1] (Theo-
rem 4.1) that has shed new light on the characterization of a game with ﬁnite
value. One of the consequences is that only three cases can occur: (i) v+(x0)
ﬁnite and v−(x0) = −∞, (ii) v+(x0) = +∞and v−(x0) ﬁnite, and (iii) v(x0)
ﬁnite. So the duality gap is either +∞or 0.
Theorem 5.2. Given x0 ∈Rn, the following statements are equivalent:
(i) There exists an open loop saddle point of Cx0(u, v).
(ii) The open loop value of the game of Cx0(u, v) is ﬁnite.
(iii) Both the open loop lower and upper values of Cx0(u, v) are ﬁnite.
Proof. (i) ⇒(ii) ⇒(iii) are obvious. It remains to prove that (iii) ⇒(i).
From condition (4.3) of Theorem 4.1 and condition (4.7) of Theorem 4.2, we
get condition (3.4) of Theorem 5.1. Finally, Theorems 4.1 and 4.2 together
give the existence of a pair (x, p) ∈H1(0, T ; Rn) × H1(0, T ; Rn) solution of
the coupled system (3.2). Therefore by Theorem 5.1 the utility function has
a saddle point.
⊓⊔

6 Riccati diﬀerential equation in the open loop saddle point case
69
We now get uniqueness of solution to the coupled system when the value
of the game is ﬁnite for all initial state x0 ∈Rn by combining Theorems 4.6
and 4.5 of §4.6.
Theorem 5.3. The following conditions are equivalent:
(i) For each x0 ∈Rn, there exists an open loop saddle point of Cx0(u, v).
(ii) For each x0 ∈Rn, the open loop value v(x0) of the game is ﬁnite.
(iii) For each x0 ∈Rn, there exists a unique pair (x, p) ∈H1(0, T ; Rn)2 so-
lution of the coupled system (3.2), there exists a unique pair (ˆu, ˆv) that
veriﬁes (3.3), and the convexity–concavity condition (3.4) is veriﬁed.
Under any one of the above conditions, (0, 0) is the unique solution of (3.4).
6 Riccati diﬀerential equation in the open loop saddle
point case
6.1 Invariant embedding with respect to the initial time
Consider the linear quadratic game on the time interval [s, T], 0 ≤s < T,
with initial state h ∈Rn at time s
Cs
h(u, v)
def
= Fx(T ) · x(T ) +
 T
s
Qx · x + |u|2 −|v|2 dt,
(6.1)
x′ = Ax + B1u + B2v
a.e. in [s, T],
x(s) = h.
(6.2)
Deﬁnition 6.1. Let h ∈Rn be an initial state at time s, 0 ≤s < T .
(i) The game is said to achieve its open loop lower value (resp. upper value)
if
v−
s (h)
def
=
sup
v∈L2(s.T ;Rk)
inf
u∈L2(s.T ;Rm) Cs
h(u, v),
(6.3)
(resp. v+
s (h)
def
=
inf
u∈L2(s.T ;Rm)
sup
v∈L2(s,T ;Rk)
Cs
h(u, v))
(6.4)
is ﬁnite.
(ii) The game is said to achieve its open loop value if its open loop lower value
v−
s (h) and upper value v+
s (h) are achieved and v−
s (h) = v+
s (h). The open
loop value of the game will be denoted by vs(h).
(iii) A pair (¯u, ¯v) in L2(s, T; Rm) × L2(s, T; Rk) is a open loop saddle point of
Cs
h(u, v) if for all u in L2(s, T; Rm) and all v in L2(s, T; Rk)
Cs
h(¯u, v) ≤Cs
h(¯u, ¯v) ≤Cs
h(u, ¯v).
(6.5)
⊓⊔

70
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
6.2 From convexity/concavity in [0, T ] to [s, T ]
The ﬁrst result is that, if the utility function Cx0(u, v) is convex, concave, or
convex–concave for some x0 on [0, T ], so is Cs
h(u, v) for all h ∈Rn on [s, T]
and all s, 0 ≤s < T .
Theorem 6.1. (i) If, for all (x0, v) ∈Rn ×L2(0, T ; Rk), the map u 	→
Cx0(u, v) is convex, then for all s, 0 ≤s < T, and all (h, v) ∈
Rn ×L2(s, T; Rk) the map u 	→Cs
h(u, v) is convex.
(ii) If, for all (x0, u) ∈Rn ×L2(0, T ; Rm), the map v 	→Cx0(u, v) is concave,
then for all s, 0 ≤s < T, and all (h, u) ∈Rn ×L2(s, T; Rm) the map
v 	→Cs
h(u, v) is concave.
Proof. We only prove (i). From identities (2.19) and (2.21) for all (u, v) ∈
L2(0, T ; Rm) × L2(0, T ; Rk)
∀¯u ∈L2(0, T ; Rm),
d2Cx0(u, v; ¯u, 0; ¯u, 0)
= F ¯y(T ) · ¯y(T ) + (Q¯y, ¯y) + (¯u, ¯u) ≥0,
(6.6)
where ¯y is the solution of
¯y′ = A¯y + B1¯u,
¯y(0) = 0.
(6.7)
To prove the same result on [s, T], associate with each ¯u ∈L2(s, T; Rm) its
extension by zero ˜¯u from [s, T] to [0, T ]. Therefore
∀¯u ∈L2(s, T; Rm),
F ¯y(T ) · ¯y(T ) +
 T
0
Q¯y · ¯y) + ˜¯u · ˜¯u dt ≥0,
(6.8)
where ¯y is the solution of
¯y′ = A¯y + B1˜¯u,
¯y(0) = 0.
(6.9)
Notice that, since ˜¯u is zero in [0, s], ¯y = 0 in [0, s] and ¯y is also solution of
¯y′ = A¯y + B1¯u,
¯y(s) = 0,
(6.10)
⇒∀¯u ∈L2(s, T; Rm),
F ¯y(T ) · ¯y(T ) +
 T
s
Q¯y · ¯y) + ¯u · ¯u dt ≥0.
(6.11)
Hence for all h ∈Rn, all (u, v) ∈L2(s, T; Rm) × L2(s, T; Rk), and all ¯u ∈
L2(s, T; Rm)
d2Cs
h(u, v; ¯u, 0; ¯u, 0) = F ¯y(T ) · ¯y(T ) +
 T
s
Q¯y · ¯y) + ¯u · ¯u dt
= d2Cx0(0, 0; ˜¯u, 0; ˜¯u, 0) ≥0.
Thus for all s and all (h, v), the map u 	→Cs
h(u, v) is convex.
⊓⊔

6 Riccati diﬀerential equation in the open loop saddle point case
71
6.3 Open loop saddle point optimality principle
At this juncture, it is important to notice that the necessary conditions (4.3)
and (4.7) associated with the respective ﬁniteness of the lower and upper
values of the game on [0, T ] do not generally survive on [s, T]. However the
convexity–concavity condition (3.4) does.
Theorem 6.2. Assume that v(x0) is ﬁnite for some x0 ∈Rn, denote by
(x(·; x0), p(·; x0)) a solution of the coupled system (3.2) in [0, T ], and let s,
0 ≤s < T .
(i) The value vs(x(s; x0)) of the game is ﬁnite.
(ii) The restriction of (x, p) to [s, T] is a solution of the coupled system

x′
s = Axs −B1B∗
1ps + B2B∗
2ps a.e. in [s, T],
xs(s) = x(s; x0),
p′
s + A∗ps + Qxs = 0,
ps(T ) = Fxs(T ), (6.12)
the restrictions (us, vs) = (u|[s,T ], v[s,T ]) of the controls (u, v) on [0, T ] to
[s, T] verify
us = −B∗
1ps and vs = B∗
2ps,
vs(x(s; x0)) = ps(s) · x(s; x0),
(6.13)
v(x0) = vs(x(s; x0)) +
 s
0
Qx · x + |u|2 −|v|2 dt,
(6.14)
and
sup
v∈L2(s,T ;Rk)
Cs
0(0, v) = Cs
0(0, 0) =
inf
u∈L2(s,T ;Rm) Cs
0(u, 0).
(6.15)
Proof. From Theorem 5.1 on [s, T], part (i) is equivalent to part (ii) and it is
suﬃcient to prove part (ii). From Theorem 6.1, the convexity–concavity condi-
tions on [0, T ] survive on [s, T] and we get (6.15). Moreover if (x(·; x0), p(·; x0))
is a solution of the coupled system (3.2) in [0, T ] with initial state x0 at
time 0 and the controls (u, v) verify identities (3.3), then the restrictions
(xs, ps) = (x|[s,T ], p|[s,T ]) is a solution to the coupled system (6.12) and the
restrictions (us, vs) = (u|[s,T ], v[s,T ]) of the the controls on [0, T ] verify (6.13).
So, by the analog of Theorems 5.1, we get the ﬁniteness of the value of the
game on [s, T].
⊓⊔
Theorem 6.3. Assume that v(x0) is ﬁnite for all x0 ∈Rn.
(i) The solution (xs, ps) of the coupled system (6.12) and the controls (us, vs)
on [s, T] in (6.13) are unique.
(ii) The map
x0 	→X(s)x0
def
= x(s; x0) : Rn →Rn
(6.16)
is a linear bijection, where (x(·; x0), p(·; x0)) is the unique solution of the
coupled system (3.2) in [0, T ].

72
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
(iii) For all h ∈Rn, the utility function Cs
h(u, v) has a unique open loop sad-
dle point (ˆus, ˆvs) ∈L2(s, T; Rm) × L2(s, T; Rk) and there exists a unique
solution (ˆxs, ˆps) of the coupled system

ˆx′
s = Aˆxs −B1B∗
1 ˆps + B2B∗
2 ˆps a.e. in [s, T],
ˆxs(s) = h,
ˆp′
s + A∗ˆps + Qˆxs = 0 a.e. in [s, T],
ˆps(T ) = F ˆxs(T ), (6.17)
such that ˆus = −B∗
1 ˆps and ˆvs = B∗
2 ˆps.
(6.18)
Proof. Recall that under the assumption that v(x0) is ﬁnite, Cs
h(u, v) is
convex–concave for all s. This means that a saddle point of Cs
h is completely
characterized by the existence of a solution of the coupled system on [s, T].
(i) Assume that the pair (ˆus, ˆvs) is a saddle point of Cs
ˆx(s) on the time
interval [s, T]. Denote by (ˆxs, ˆps) the corresponding solution to the coupled
system (6.12). Consider the following new pair on the interval [0, T ]:
˜u
def
=

ˆu,
in [0, s]
ˆus,
in [s, T]
˜v
def
=

ˆv,
in [0, s],
ˆvs,
in [s, T],
(6.19)
and the corresponding solution (˜x, ˜p) to the state–adjoint state system (2.3)-
(2.17).
If it can be shown that the pair (˜u, ˜v) is a saddle point of Cx0(u, v) on [0, T ],
then by uniqueness of the saddle point on [0, T ], we can conclude that (˜u, ˜v) =
(ˆu, ˆv) and hence (ˆus, ˆvs) = (ˆu|[s,T ], ˆv|[s,T ]). From this we get the uniqueness
of the saddle point of Cs
ˆx(s) on [s, T] and the uniqueness of solution to the
coupled system (6.12). The ﬁrst remark is that ˜x(s) = ˆx(s) and from (6.14)
Cx0(ˆu, ˆv) = v(x0) =
 s
0
Qˆx · ˆx + |ˆu|2 −|ˆv|2 dt + vs(ˆx(s))
=
 s
0
Qˆx · ˆx + |ˆu|2 −|ˆv|2 dt + F ˆxs(T ) · ˆxs(T ) +
 T
s
Qˆxs · ˆxs + |ˆus|2 −|ˆvs|2 dt
⇒Cx0(ˆu, ˆv) = Cx0(˜u, ˜v).
Yet, this is not suﬃcient to conclude that (˜u, ˜v) is a saddle point of Cx0(u, v).
We must show that
sup
v∈L2(0,T ;Rk)
Cx0(˜u, v) = Cx0(˜u, ˜v) =
inf
u∈L2(0,T ;Rm) Cx0(u, ˜v).
(6.20)
The second remark is that, since (˜u −ˆu, ˜v −ˆv) is equal to (0, 0) on [0, s],
(ˆus −ˆu, ˆvs −ˆv) is a saddle point of Cs
0(us, vs). Combining this with the fact
that, by (6.15), (0, 0) is also a saddle point of Cs
0(us, vs), the pairs (ˆus −ˆu, 0)
and (0, ˆvs−ˆv) are also saddle points of Cs
0(us, vs) and Cs
0(ˆus−ˆu, 0) = Cs
0(0, ˆvs−
ˆv) = 0. The third remark is that
Cx0(ˆu, ˜v) = Cx0(ˆu, ˆv) + dCx0(ˆu, ˆv; 0, ˜v −ˆv) + C0(0, ˜v −ˆv)
= Cx0(ˆu, ˆv) + C0(0, ˜v −ˆv).

6 Riccati diﬀerential equation in the open loop saddle point case
73
But, since ˜v −ˆv is equal to 0 on [0, s],
C0(0, ˜v −ˆv) = Cs
0(0, ˆvs −ˆv) = 0
⇒Cx0(ˆu, ˜v) = Cx0(ˆu, ˆv) = Cx0(˜u, ˜v).
We now prove the second part of identity (6.20):
Cx0(u, ˜v) = Cx0(ˆu, ˜v) + dCx0(ˆu, ˜v; u −ˆu, 0) + C0(u −ˆu, 0).
(6.21)
Since (0, 0) is a saddle point of C0(u, v)
inf
u∈L2(0,T ;Rm) C0(u −ˆu, 0) =
inf
u∈L2(0,T ;Rm) C0(u, 0) = 0.
It remains to prove that for all u ∈L2(0, T ; Rm), dCx0(ˆu, ˜v; u −ˆu, 0) = 0.
First observe that
dCx0(ˆu, ˜v; u −ˆu, 0) = dCx0(ˆu, ˆv; u −ˆu, 0) + dC0(0, ˜v −ˆv; u −ˆu, 0)
= dC0(0, ˜v −ˆv; u −ˆu, 0).
Since (0, ˆvs −ˆv) is a saddle point of Cs
0 on [s, T], there exists a pair (ξ, π)
solution of the coupled system

ξ′ = Aξ −B1B∗
1π + B2B∗
2π a.e. in [s, T],
ξ(s) = 0,
π′ + A∗π + Qξ = 0,
π(T ) = Fξ(T ),
(6.22)
0 = −B∗
1π,
ˆvs −ˆv = B∗
2π.
(6.23)
The ﬁrst equation can also be written as
ξ′ = Aξ + B2(ˆvs −ˆv) a.e. in [s, T],
ξ(s) = 0.
Denote by ˜ξ the solution of the state equation (2.3) on [0, T ] corresponding
to the initial state 0 and the control pair (0, ˜v −ˆv)
˜ξ′ = A˜ξ + B2(˜v −ˆv) a.e. in [0, T ],
˜ξ(0) = 0,
and observe that, since the restriction of ˜v −ˆv to [0, s] is 0, ˜ξ = 0 on [0, s] and
˜ξ = ξ on [s, T]. Denoting by y the solution of
y′ = Ay + B1(u −ˆu) a.e. in [0, T ],
y(0) = 0,
we get the following expression (cf. (2.15) and (2.18) for the directional de-
rivative):
dC0(0, ˜v −ˆv; u −ˆu, 0)
= F ˜ξ(T ) · y(T ) +
 T
0
Q˜ξ · y + 0 · (u −ˆu) + (˜v −ˆv) · 0 dt
= F ˜ξ(T ) · y(T ) +
 T
0
Q˜ξ · y dt = F ˜ξ(T ) · y(T ) +
 T
s
Q˜ξ · y dt
= Fξ(T ) · y(T ) +
 T
s
Qξ · y dt =
 T
s
B∗
1π · (u −ˆu) dt = 0,

74
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
since B∗
1π = 0 on [s, T] from (6.23). This establishes the second part of ex-
pression (6.20).
The proof of the ﬁrst part is dual to the proof of the second part. This
yields the uniqueness and completes the proof of part (i).
(ii) The map (6.16) is clearly linear (and continuous). Assume that it is
not bijective, then there exists some x0 ∈Rn, x0 ̸= 0, such that ˆx(s) = 0. The
restriction of (ˆx, ˆp) to the interval [s, T] is a solution of the system

ξ′ = Aξ −B1B∗
1π + B2B∗
2π a.e. in [s, T],
ξ(s) = 0 = ˆx(s),
π′ + A∗π + Qξ = 0 a.e. in [s, T],
π(T ) = Fξ(T ).
(6.24)
But from part (i) the unique solution of system (6.24) is (0, 0). Hence
(ˆx, ˆp) = (0, 0) in [s, T]
⇒(ˆx(s), ˆp(s)) = (0, 0),
⇒

ˆx′ = Aˆx −B1B∗
1 ˆp + B2B∗
2 ˆp a.e. in [0, s],
ˆx(s) = 0,
ˆp′ + A∗ˆp + Qˆx = 0 a.e. in [0, s],
ˆp(s) = 0,
⇒(ˆx, ˆp) = (0, 0) in [0, s]
⇒x0 = ˆx(0) = 0.
This contradicts our initial conjecture that x0 ̸= 0, and we conclude that the
linear map (6.16) is injective and, a fortiori, bijective.
(iii) From part (i) for each h ∈Rn and each s, 0 ≤s < T , there exists a unique
h0 ∈Rn such that h = X(s)h0. But Ch0(u, v) has a unique open loop saddle
point in [0, T ]. From part (i), Cs
X(s)h0(u, v) has a unique open loop saddle
point in [s, T]. The result now follows from the fact that h = X(s)h0. The
equations and the identities follow from theorems from Theorem 6.2 (ii).
⊓⊔
Remark 6.1. The proof of part (i) is not trivial. It is one of the key elements
to get the result of part (iii) that says that Cs
h(u, v) has a saddle point for all
initial state h and all initial times s.
⊓⊔
The next example illustrates that even when the coupled system has a unique
solution in the time interval [0, T ], the uniqueness is not necessarily preserved
on a smaller interval [s, T].
Example 6.1. Consider the one-dimensional example of P. Bernhard [2, Ex-
ample 5.1, page 67] in [0, 2] with x0 ∈R

x′ = (2 −t)u + tv in [0, 2],
x(0) = x0,
(6.25)
Cx0(u, v) = 1
2|x(2)|2 +
 2
0
|u|2 −|v|2 dt.
(6.26)
The solution of the coupled loop system

6 Riccati diﬀerential equation in the open loop saddle point case
75

ˆx′ =

−(2 −t)2 + t2
p1 in [0, 2],
ˆx(0) = x0,
⎧
⎨
⎩
ˆp′ = 0 in [0, 2],
ˆp(2) = 1
2 ˆx(2),
(6.27)
is given by the expressions
ˆx(t) = (t −1)2x0,
ˆp(t) = 1
2x0.
(6.28)
So it exists and is unique for all x0 ∈R. At time t = 1, x(1) = 0 for all x0 ∈R.
Now consider the closed loop system in the time interval [1, 2] for the initial
condition 0 at time t = 1

ˆx′
1 =

−(2 −t)2 + t2
p1 in [1, 2],
ˆx1(1) = 0,
⎧
⎨
⎩
ˆp′
1 = 0 in [1, 2],
ˆp1(2) = 1
2 ˆx1(2).
(6.29)
Its solution is given by the expression
ˆx1(t) = 2(t −1)2c,
ˆp1(t) = c,
(6.30)
up to an arbitrary constant c ∈R. So the solution of this homogeneous system
is not unique. The concatenation ˜u of ˆu on [0, 1] and ˆu1 on [1, 2] associated
with c ̸= x0/2 as deﬁned in (6.19) yields

˜x′ = (2 −t) ˜u + t ˜v in [0, 2],
˜x(0) = x0,
⎧
⎨
⎩
˜p′ = 0 in [0, 2],
˜p(2) = 1
2 ˜x(2).
(6.31)
It is readily seen that ˜x(1) = ˆx(1) at time t = 1, but that the adjoint systems
do not coincide: ˆp(1) = x0/2 ̸= c = ˜p(1) (and in fact for all t ∈[0, 2]).
⊓⊔
6.4 Decoupling of the coupled system
We need the following lemma.
Lemma 6.1. Assume that the open loop saddle point value v(x0) is ﬁnite for
all x0 ∈Rn. Let s, 0 ≤s < T, and (ˆxs, ˆps) be the unique solution of the
coupled system (6.17) with initial state h at time s. Then the map P(s)
h 	→P(s)h
def
= ˆps(s) : Rn →Rn
(6.32)
is linear, continuous, and symmetrical.
Proof. By deﬁnition, P(s) is linear and continuous. For the symmetry, let
(x, p) and (¯x, ¯p) be the solutions of the coupled system (6.17) for the respective
initial states h and ¯h at time s. By symmetry of F, Q(t) and B1(t)B∗
1(t) −
B2(t)B∗
2(t)

76
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
P(s)h · ¯h = p(s) · ¯x(s) = p(T ) · ¯x(T ) −
 T
s
p′ · ¯x + p · ¯x′ dt
= Fx(T ) · ¯x(T )
−
 T
s
−(A∗p + Qx) · ¯x + p · (A¯x −B1B∗
1 ¯p + B2B∗
2 ¯p) dt
= Fx(T ) · ¯x(T ) +
 T
s
Qx · ¯x + p · (B1B∗
1 −B2B∗
2)¯p dt = P(s)¯h · h
and P(s)∗= P(s).
⊓⊔
In view of Lemma 6.1, we use invariant embedding to get more a priori
information on the decoupling matrix function P(s).
Theorem 6.4. Assume that v(x0) is ﬁnite for all x0 ∈Rn.
(i) Given the solution of the coupled system (3.2) in [0, T ] for x0 ∈Rn,
ˆp(s) = P(s)ˆx(s),
0 ≤s ≤T.
(6.33)
(ii) The elements of the matrix function P are H1(0, T )-functions, the ele-
ments of the matrix functions
AP
def
= A −RP,
R
def
= B1B∗
1 −B2B∗
2,
(6.34)
belong to L∞(0, T ), and the closed loop system
ˆx′ = [A −(B1B∗
1 −B2B∗
2) P] ˆx a.e. in [0, T ],
ˆx(0) = x0,
(6.35)
has a unique solution in H1(0, T ; Rn). For all (t, s), 0 ≤s ≤t ≤T ,
the fundamental matrix solution ΦP (t, s) associated with the closed loop
system (6.35) and its inverse ΦP (t, s)−1 are continuous in {(t, s) : 0 ≤
s ≤t ≤T }. For all pairs 0 ≤s ≤t ≤T
∂
∂sΦP (t, s) + ΦP (t, s)AP (s) = 0 a.e. in [0, t],
ΦP (t, t) = I.
(6.36)
Proof. (i) From Theorem 6.2 and Theorem 6.3 (i)
ˆxs = ˆx|[s,T ],
ˆps = ˆp|[s,T ]
⇒ˆp(s) = ˆps(s) = P(s)ˆxs(s) = P(s)ˆx(s),
and we get (6.33). The closed loop system is obtained by direct substitution
of the identity (6.33) for ˆp into the ﬁrst equation of the coupled system (3.2)
in [0, T ].
(ii) Associate with the solution of the coupled system (3.2) in [0, T ], the
matrix function
Λ(s)x0
def
= ˆp(s; x0),
∀x0 ∈Rn, 0 ≤s ≤T.
(6.37)
From (6.33) in part (i) and the invertibility of X(s)

6 Riccati diﬀerential equation in the open loop saddle point case
77
Λ(s)x0
def
= P(s)X(s)x0,
∀x0 ∈Rn, 0 ≤s ≤T
⇒Λ(s) = P(s)X(s), ⇒P(s) = Λ(s)X(s)−1,
0 ≤s ≤T.
Since X(s) is invertible and the elements of the matrices X and Λ are
H1(0, T )-functions
P ′(s) = Λ(s)′X(s)−1 −Λ(s)X(s)−1X(s)′X(s)−1.
(6.38)
In particular the elements of the matrix function P are H1(0, T )-functions.
Then the matrix function AP (t) in (6.34) belongs to L∞(0, T ) and the closed
loop system (6.35) has a unique solution in H1(0, T ; Rn). From this ΦP has
the usual properties of a fundamental matrix solution ΦP in {(t, s) : 0 ≤s ≤
t ≤T }, ΦP (t, 0) = ΦP (t, s)ΦP (s, 0), and
∂ΦP
∂s (t, s) + ΦP (t, s)AP (s) a.e. in [0, T ],
ΦP (t, t) = I.
(6.39)
⊓⊔
6.5 Riccati diﬀerential equation
Under the assumption of the ﬁniteness of the open loop value of the game
in [0, T ] for each initial state, we have used invariant embedding to introduce
the decoupling symmetrical matrix function P(s). To show that it is a solu-
tion of the matrix Riccati diﬀerential equation (2.7), we follow the technique
of P. Bernhard [2, Lemma 3.1] since, from Theorem 6.3 (ii), the matrix
function X(s) is invertible for all s, P(s) = Λ(s)X(s)−1, and P ′(s) is given
by identity (6.38).
Theorem 6.5. Assume that the open loop value v(x0) is ﬁnite for all x0 ∈Rn.
(i) There exists a unique symmetrical solution with elements in H1(0, T ) of
the matrix Riccati diﬀerential equation
P ′ + PA + A∗P −PRP + Q = 0,
P(T ) = F,
(6.40)
where R = B1B∗
1 −B2B∗
2. Moreover,
ˆp(t) = P(t) ˆx(t), 0 ≤t ≤T, and Cx0(ˆu, ˆv) = P(0)x0 · x0,
(6.41)
where (ˆx, ˆp) ∈H1(0, T ; Rn)2 is the unique solution of the coupled sys-
tem (3.2).
(ii) The optimal strategies of the two players are closed loop
ˆu = −B∗
1P ˆx and ˆv = B∗
2P ˆx,
(6.42)
and they achieve a closed loop–closed loop saddle point in the sense of
P. Bernhard [2].

78
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
(iii) For all x0 ∈Rn the function Cx0(u, v) is convex–concave.
Proof. (i) From identity (6.38) in the proof of part (ii) of Theorem 6.4 a
straightforward computation yields that the matrix function P is a solution of
the matrix Riccati diﬀerential equation (6.40). This solution is unique. Indeed
if ¯P is another solution of the Riccati equation, the closed loop system with
¯P has a unique solution ¯x and it is easy to check that ¯p = ¯P ¯x is a solution of
the associated adjoint equation. But there is a unique solution to the coupled
system. By deﬁnition of P via invariant embedding we get that ¯P = P. (ii)
and (iii) From identities (4.2) and (4.3) in Theorem 4.1.
⊓⊔
Remark 6.2. We shall see in Example 7.1 that the fact that the elements of P
are H1(0, T ) is necessary but not suﬃcient to get an open loop saddle point.
⊓⊔
6.6 Open loop saddle point and Riccati diﬀerential equation
The existence of a symmetrical solution in H1(0, T ) to the matrix Riccati
diﬀerential equation (6.40) implies that, for all x0 ∈Rn, there exists a solution
(ˆx, ˆp) ∈H1(0, T ; Rn) × H1(0, T ; Rn) of the coupled system (3.2). However, as
we shall see in Example 6.2, this is not suﬃcient to get an open loop saddle
point of the utility function Cx0(u, v).
Theorem 6.6. A set of necessary and suﬃcient conditions for the existence
of an open loop saddle point of the utility function Cx0(u, v) for all x0 ∈Rn
is
a) the utility function Cx0(u, v) is convex in u and concave in v for some x0,
b) and there exists a (unique) symmetrical solution in H1(0, T ) to the matrix
Riccati diﬀerential equation (6.40).
Proof. From Theorem 6.5 we get a) and b). Conversely, from a) if P is a
solution of the Riccati diﬀerential equation, the closed loop system has a
unique solution xP and pP = PxP is the solution of the adjoint system. It is
then easy to check that the pair (xP , pP ) is indeed a solution of the coupled
system (3.2) in [0, T ]. Finally from the convexity–concavity property b) we
get the existence of the open loop saddle point.
⊓⊔
Remark 6.3. The method of completion of the squares (cf. for instance T. Ba-
s¸ar and P. Bernhard [1, Chapter 9, Theorem 9.4] can also be used here to
obtain
sup
v∈L2(0,T ;Rk)
inf
u∈L2(0,T ;Rm) Cx0(u, v) ≤P(0)x0 · x0
≤
inf
u∈L2(0,T ;Rm)
sup
v∈L2(0,T ;Rk)
Cx0(u, v).
⊓⊔

6 Riccati diﬀerential equation in the open loop saddle point case
79
So it would be tempting to conclude that there is a saddle point without
condition a). But this is not true as will be illustrated in Example 6.2 below.
It is a game without open loop saddle point, where the solution of the Riccati
diﬀerential equation (6.40) is unique strictly positive and inﬁnitely diﬀeren-
tiable. In that example it is shown that U(x0) = ∅for all x0. In order to get a
saddle point, v−(x0) and v+(x0) must both be ﬁnite. Therefore the open loop
lower value of the game will be ﬁnite if b) is veriﬁed and V (x0) ̸= ∅; the open
loop upper value of the game will be ﬁnite if b) is veriﬁed and U(x0) ̸= ∅.
Example 6.2. Consider the utility function and linear dynamics
Cx0(u, v) =
 1
0
2x2 + u2 −v2 dt,
x′ = x + u + v,
x(0) = x0
(6.43)
given by P. Zhang [1]. Here A = B1 = B2 = 1, F = 0, and Q = 2. Now
R = B1B∗
1 −B2B∗
2 = 0 and the associated Riccati diﬀerential equation (6.40)
reduces to
P ′ + 2P + 2 = 0 in [0, 1],
P(1) = 0.
It has a unique inﬁnitely diﬀerentiable solution P(t) = e2(1−t) −1 that is
strictly positive in [0, 1).
We now extend the result of P. Zhang [1] on the nonexistence of an open
loop saddle point from the initial state x0 = 0 to any initial state. For all
x0 ∈R the open loop lower value v−(x0) of the game is ﬁnite, but the open
loop upper value v+(x0) is +∞. Indeed for each v ∈L2(0, T ; R)
inf
u∈L2(0,T ;R) Cx0(u, v) ≤Cx0(−v, v) =
 1
0
2

x0 et2 dt =

e2 −1

(x0)2
⇒v−(x0) =
sup
v∈L2(0,T ;R)
inf
u∈L2(0,T ;R) Cx0(u, v) ≤

e2 −1

(x0)2 .
By deﬁnition of the sup,
v−(x0) =
sup
v∈L2(0,T ;R)
inf
u∈L2(0,T ;R) Cx0(u, v)
≥
inf
u∈L2(0,T ;R) Cx0(u, 0) =
inf
u∈L2(0,T ;R)
 1
0
2x2 + u2 dt ≥0
⇒∀x0 ∈R,
0 ≤v−(x0) ≤

e2 −1

(x0)2 .
For the open loop upper value, associate with each u ∈L2(0, T ; R) the se-
quence of functions vn(t) = −u(t) + n, n ≥1. The corresponding sequence of
states is
xn(t) = etx0 + n
 t
0
et−s ds = etx0 + n(et −1)

80
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
and utility functions
Cx0(u, vn) = n2
 1
0
2(et −1)2 −1 dt + 2n
 1
0
u(t) dt
+
 1
0
(etx0)2 dt + 2n x0
 1
0
et (et −1) dt
= n2
 1
0
1 + 2e2t −4et dt + 2n
 1
0
u(t) dt
+
 1
0
(etx0)2 dt + 2n x0 (e −1)2
≥(e −2)2n2 + 2n

x0 (e −1)2 +
 1
0
u(t) dt
	
+
 1
0
(etx0)2 dt
⇒
sup
v∈L2(0,T ;R)
Cx0(u, v) ≥Cx0(u, vn) →+∞
as n goes to inﬁnity. Therefore for all x0 ∈Rn and all u ∈L2(0, T ; R),
sup
v∈L2(0,T ;R)
Cx0(u, v) = +∞
⇒v+(x0) = +∞
and
U(x0) = ∅,
and there is no open loop saddle point.
⊓⊔
6.7 The general case of Remark 2.1
It is interesting to look at the feedback strategies in the case of the general
utility function with mixed terms (2.4) of Remark 2.1. With the change of
variable (2.5)
⎡
⎣
x
u
v
⎤
⎦=
⎡
⎢⎣
I
0
0
−N −1
1 S∗N −1/2
1
0
N −1
2 T ∗
0
N −1/2
2
⎤
⎥⎦
⎡
⎣
x
¯u
¯v
⎤
⎦
and the matrices A, B1, B2, and Q, the matrix function P is the unique
solution of the Riccati diﬀerential equation
P ′ + PA + A∗P −PRP + Q = 0 a.e. in [0, T ],
P(T ) = F,
(6.44)
where R = B1B
∗
1 −B2B
∗
2 and the feedback strategies are given by
¯u = −B
∗
1Px and ¯v = B
∗
2Px
(6.45)
that yields in terms of the original variables
u = −N −1
1

S∗+ B∗
1P

x and v = N −1
2

T ∗+ B∗
2P

x
(6.46)
and the closed loop system
⎧
⎪
⎨
⎪
⎩
x′ =

A −B1N −1
1

S∗+ B∗
1P

+ B2N −1
2

T ∗+ B∗
2P

x
=

A −B1N −1
1 B∗
1P + B2N −1
2 B∗
2P

x,
x(0) = x0.
(6.47)

7 Riccati diﬀerential equation and open/closed loop upper/lower value of the game
81
7 Riccati diﬀerential equation and open/closed loop
upper/lower value of the game
In the literature, an important issue is the connection between the existence
of a symmetrical solution to the matrix Riccati diﬀerential equation
P ′ + PA + A∗P −PRP + Q = 0 a.e. in [0, T ],
P(T ) = F,
(7.1)
where R = B1B∗
1 −B2B∗
2, in relation with the existence of either an open or a
closed loop lower value, upper value, or saddle point of the game. For instance,
in the closed loop case, quoting P. Bernhard [2] in his introduction
“It has long been known that, for the two-person, zero-sum diﬀerential
game with linear dynamics, quadratic payoﬀ, ﬁxed end-time, and free
end-state (standard LQ game), the existence of a solution to a Riccati
equation is a suﬃcient condition for the existence of a saddle point
within the class of instantaneous state feedback strategies (Refs. 1-2),
and therefore within any wider class (Ref. 3).”3
In the open loop case, the above statements are incomplete (cf. Exam-
ple 6.2), even under the assumptions
F ≥0 and Q(t) ≥0 a.e. in [0, T ]
used in P. Bernhard [2] that necessarily imply the convexity and the coer-
civity of Cx0(u, v) with respect to u and V (x0) = L2(0, T ; Rk) for all x0 ∈Rn.
Even when the solution of the Riccati diﬀerential equation (7.1) is H1(0, T ) or
bounded (Remark 6.3 and Example 6.2), it is also necessary that the utility
function be convex in u and concave in v (Theorem 6.6). In fact, the existence
of a strictly positive and inﬁnitely diﬀerentiable solution to the Riccati diﬀer-
ential equation (7.1) does not even imply that either the open loop lower value
or the open loop upper value of the game be ﬁnite as shown in Example 7.2
below.
This leaves the cases where either the open loop lower or the upper value
of the game explodes. In such cases, the solution of the Riccati diﬀerential
equation might have a blow-up time as illustrated in Example 7.1 below (cf.
P. Bernhard [2], Example 5.1, page 67)
“The following game has a saddle point that survives a conjugate point.”
where he means a closed loop-closed loop saddle point. The conjugate point
corresponds to a blow-up time of the solution of the Riccati equation (7.1)
where the solution is not of the H1(0, T )-type. Finally an open loop saddle
point yields closed loop optimal strategies that achieve a closed loop-closed
loop saddle point in the sense of P. Bernhard [2] (Theorem 6.5), but the
converse is not necessarily true. It is informative to detail the example of P.
Bernhard.
3 Ref. 1 is Y. Ho, A.E. Bryson, and S. Baron [1], Ref. 2 is P. Faurre [1], and
Ref. 3 is P. Bernhard [1].

82
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
Example 7.1. Consider the dynamics and utility function in the time interval
[0, 2]
x′(t) = (2 −t) u(t) + t v(t), a.e. in [0, 2],
x(0) = x0,
(7.2)
Cx0(u, v) = 1
2|x(2)|2 +
 2
0
|u(t)|2 −|v(t)|2 dt.
(7.3)
Here A = 0, B1(t) = 2 −t, B2(t) = t, F = 1/2, Q = 0, and R = B1B∗
1 −
B2B∗
2 = 4(1 −t). It is shown in P. Bernhard [2] that the Riccati equation
reduces to
P ′ −4(1 −t)P 2 = 0,
P(2) = 1/2
⇒P(t) =
1
2(t −1)2 .
Its solution is positive and blows up at t = 1. It is not an element of H1(0, 2).
We now show that there is no open loop saddle point in the time interval [0, 2].
For the open loop lower value of the game, the minimization with respect to
u has a unique solution for all (x0, v) since the utility function u 	→Cx0(u, v)
is convex and bounded below by −∥v∥2
L2. The minimizer is completely char-
acterized by the coupled system
⎧
⎨
⎩
x′(t) = (2 −t) ˆu(t) + t v(t) a.e. in [0, 2],
x(0) = x0,
p′(t) = 0 a.e. in [0, 2],
p(2) = 1
2x(2),
ˆu(t) = −(2 −t) p(t).
From this
x(2) = 3
7

x0 +
 2
0
s v(s) ds
	
and p(t) = 1
2x(2)
and
J−
x0(v)
def
=
inf
u∈L2(0,2;R) Cx0(u, v)
= Cx0(ˆu, v) = 1
2x(2)2 + 1
4 x(2)2
 2
0
(2 −t)2 dt −
 2
0
|v(t)|2 dt
= 7
6x(2)2 −
 2
0
|v(t)|2 dt = 3
14

x0 +
 2
0
s v(s) ds
	2
−
 2
0
|v(t)|2 dt.
It is readily seen that J−
x0 is concave in v and that the supremum with respect
to v of J−
x0(v) exists. Indeed, from the ﬁrst order condition
∀v, 1
2dJ−
x0(ˆv; v) = 3
14

x0 +
 2
0
s ˆv(s) ds
	  2
0
s v(s) ds −
 2
0
ˆv(t) v(t) dt = 0,
there is a unique stationary point ˆv(t) = t x0/2, the Hessian is negative

7 Riccati diﬀerential equation and open/closed loop upper/lower value of the game
83
1
2d2J−
x0(ˆv; v; v) = 3
14
 2
0
s v(s) ds
	2
−
 2
0
|v(t)|2 dt
≤3
14
 2
0
s2 ds
	  2
0
|v(s)|2 ds
	
−
 2
0
|v(t)|2 dt
≤
 3
14
23
3 −1
	  2
0
|v(t)|2 dt = −3
7
 2
0
|v(t)|2 dt ≤0,
and the open loop lower value of the game is v−(x0) = J−
x0(ˆv) = (x0)2/2.
However the open loop upper value of the game is v+(x0) = +∞for all
x0 ∈R. Indeed pick the sequence of controls {vn}, n ≥1, vn(t) = 0 in [0, 1]
and vn(t) = n in [1, 2]. The corresponding sequence of states at time t = 2 is
xn(2) = x0 +
 2
0
(2 −t) u(t) dt + n
 2
1
t dt =

x0 +
 2
0
(2 −t) u(t) dt
	
+ 3
2n.
Denote by X the square bracket that does not depend on n. Then
Cx0(u, vn) = 1
2
X + 3
2n

2
+
 2
0
|u(t)|2 dt −
 2
1
n2 dt
= 1
8n2 + 3
2n X + X2
2 +
 2
0
|u(t)|2 dt →+∞as n →+∞.
Thus for all x0 ∈R and u ∈L2(0, T ; R)
sup
v∈L2(0,T ;R)
Cx0(u, v) = +∞
⇒v+(x0) = +∞
and
U(x0) = ∅.
Therefore, whatever is the initial state x0, Cx0(u, v) has no open loop saddle
point.
⊓⊔
By changing the weight F = 1/2 to 1/3 in the ﬁnal term Fx(2)2 in the
utility function of the above Example 7.1, we get an example of a convex
coercive utility function with respect to u for which there is an inﬁnitely
diﬀerentiable positive solution to the Riccati diﬀerential equation (7.1), but
the open loop upper and lower values are both equal to +∞.
Example 7.2. Consider the state equation (7.2) with the new utility function
Cx0(u, v) = 1
3|x(2)|2 +
 2
0
|u(t)|2 −|v(t)|2 dt.
(7.4)
The Riccati diﬀerential equation (7.1) reduces to
P ′ −4(1 −t)P 2 = 0 in [0, 2],
P(2) = 1/3
⇒P(t) =
1
2(t −1)2 + 1
and its solution is unique, strictly positive, and inﬁnitely diﬀerentiable. Feed-
backs of the form ˆu = −B∗
1Px and ˆv = B∗
2Px would yield Cx0(ˆu, ˆv) =
P(0)x2
0 = x2
0/3.

84
I-2 Linear Quadratic Two-Person Zero-Sum Diﬀerential Games
As in Example 7.1, the utility function (7.4) is both convex and coercive
in the ﬁrst variable u. Therefore, given v, there exists a unique solution ˆu to
the minimization of Cx0(u, v) with respect to u and
J−
x0(v)
def
=
inf
u∈L2(0,2;R) Cx0(u, v)
= Cx0(ˆu, v) = 3

x0 +
 2
0
s v(s) ds
	2
−
 2
0
|v(t)|2 dt.
It is readily seen that J−
x0 is not concave in v and that the supremum with
respect to v of J−
x0(v) is +∞. Indeed, pick the sequence vn(t) = n and let n
go to +∞
Jx0(vn) = 10 n2 + 12 n x0 + 3x2
0 →+∞.
Therefore, the open loop lower value v−(x0) and, a fortiori, the open loop
upper value v+(x0) are both equal to +∞.
Finally, it is interesting to note that the corresponding coupled system on
[0, 2]

ˆx′ =

−(2 −t)2 + t2
ˆp in [0, 2],
ˆx(0) = x0,
⎧
⎨
⎩
ˆp′ = 0 in [0, 2],
ˆp(2) = 1
3 ˆx(2),
has a unique solution given by
ˆx(t) =

2(t −1)2 + 1
 x0
3 ,
ˆp(t) = 1
3x0.
So what is missing to get a ﬁnite open loop lower value is the concavity of the
function v 	→J−
x0(v).
⊓⊔

Part II
Representation of Inﬁnite Dimensional Linear
Control Dynamical Systems

1
Semigroups of Operators and Interpolation
1 Notation
We shall denote by X a complex Banach space of norm | · |, and by L(X)
the Banach algebra of all linear continuous mappings T : X →X. The linear
space L(X) is endowed with the usual norm: For any T ∈L(X)
∥T ∥= sup {|Tx|: x ∈X, |x| ≤1} .
(1.1)
Given a set S ⊂R, C(S; X) will denote the set of all continuous mappings
from S into X. For a closed bounded interval S = [a, b], the space C([a, b]; X)
endowed with the norm
∥f∥C([a,b];X) = sup {|f(t)|: t ∈[a, b]}
(1.2)
is a Banach space. The spaces of k-times continuously diﬀerentiable mappings
will be denoted by Ck(S; X) and Ck([a, b]; X), k ∈N.
Lp(a, b; X) will be the Banach space of equivalent classes of strongly mea-
surable (in the B¨ochner sense) mappings [a, b] →X that are p-integrable,
1 ≤p < ∞, (resp. essentially bounded, p = ∞), with norm
∥f∥Lp(a,b;X) =
 b
a
|f(s)|p ds
1/p
,
(resp. ∥f∥L∞(a,b;X) = ess.sup {|f(t)|, t ∈[a, b]} ).
(1.3)
By W 1,p(a, b; X) we shall denote the set of all mappings f in Lp(a, b; X)
with a vector distributional derivative in Lp(a, b; X). This derivative will be
denoted df/dt or f ′. For each f ∈W 1,p(a, b; X), there exists a unique abso-
lutely continuous function ˜f : [a, b] →X such that ˜f = f and ˜f ′ = f ′ a.e. in
[a, b]. Therefore we shall always identify f and ˜f, which is the integral of its
derivative.

88
II-1 Semigroups of Operators and Interpolation
A linear operator is a linear map A: D(A) ⊂X →X deﬁned on a domain
D(A) that is assumed to be a linear subspace of X. The image or range of A
is denoted R(A). A linear operator is said to be closed if its graph
G(A) = {(x, Ax): x ∈D(A)}
is closed in the product space X ×X. For a closed linear operator A, the graph
norm operator topology of D(A) is deﬁned by the norm
|x|D(A) = |x| + |Ax|,
x ∈D(A).
(1.4)
When D(A) is endowed with the graph norm topology, it is a Banach space
and A becomes a continuous linear operator from D(A) to X.
Associate with the closed linear operator A the operators
Aλ = λI −A: D(A) →X
(1.5)
for arbitrary complex numbers λ. The resolvent set ρ(A) is the set of all
complex numbers λ such that the operator Aλ has a bounded inverse
R(λ, A)
def
= [λI −A]−1
(1.6)
in X. It is known that for closed linear operators A, λ ∈ρ(A) if and only if
R(λ, A) exists and Im(λI −A) = X (cf. E. Hille and R. S. Phillips [1,
Theorem 2.16.3, p. 55]). The complement of ρ(A) is called the spectrum of the
operator A and is denoted by σ(A). The spectrum of A is made up of three
disjoint parts: the continuous spectrum σC(A), which consists of all values of
λ such that Aλ has an unbounded inverse whose domain is dense in X; the
residual spectrum σR(A), which consists of all values of λ such that Aλ has
an inverse whose domain is not dense in X; and the point spectrum σP (A),
which consists of all values of λ for which no inverse exists. In other words:
(i) λ ∈σC(A) if Aλ is one-to-one with a dense image in X not equal to X,
(ii) λ ∈σR(A) if Aλ is one-to-one, but its image is not dense in X,
(iii) λ ∈σP (A) if Aλ is not one-to-one.
2 Linear evolution equations and strongly continuous
semigroups
2.1 Deﬁnitions and preliminary results
Let A: D(A) ⊂X →X be a closed linear operator. Consider the initial value
problem

y′(t) = Ay(t),
t ≥0,
y(0) = x,
x ∈X.
(2.1)

2 Linear evolution equations and strongly continuous semigroups
89
If A is bounded (that is A ∈L(X)) the solution of problem (2.1) is given by
the expression
y(t) = etAx
def
=
∞

k=0
tkAkx
k!
;
(2.2)
moreover, setting SA(t) = etA, the following properties hold:
∀t, s ∈R,
SA(t + s) = SA(t)SA(s),
and
SA(0) = I.
(2.3)
There is a one-to-one correspondence between the abelian group (R, +) and
the subset of transformations SA = {SA(t): t ∈R} in L(X) under composition
“◦”. So we shall say that (SA, ◦) is a group and denote it {SA(t)} or simply
SA. Moreover the mapping t 	→SA(t): R →L(X) is continuous. We say that
SA is uniformly continuous.
When A is unbounded it is still possible to construct a solution of (2.1)
of the form y(t) = SA(t)x in speciﬁc applications. However, in general, SA(t)
is only deﬁned for t ≥0 and (2.3) is only veriﬁed for t ≥0 and s ≥0 (as for
parabolic problems). Thus we say that SA is a semigroup. In addition, SA is in
general not uniformly continuous, but for each x ∈X, the function t 	→SA(t)x
is continuous. We say that SA is strongly continuous. This naturally leads to
the following deﬁnition, which will be followed by a characterization of the
properties of the operator A, which generates the family of operators SA.
Deﬁnition 2.1. A mapping S : [0, +∞[ →L(X) (resp. R →L(X)) is said
to be a strongly continuous semigroup (resp. group) on X if the following
properties hold:
(i) S(0) = I, S(t + s) = S(t)S(s), ∀t, s ≥0 (resp. ∀t, s ∈R),
(ii) for all x ∈X, S(·)x is continuous on [0, ∞[ (resp. R).
⊓⊔
Remark 2.1. A strongly continuous semigroup (resp. group) on X is also re-
ferred to as a semigroup (resp. group) of class C0 on X in the terminology of
E. Hille and R. S. Phillips [1, Chapter X, §10.6, p. 321].
⊓⊔
Remark 2.2. Assume that S is a strongly continuous semigroup on X and
that for all t > 0, S(t)−1 exists in L(X). If ∀x ∈X, S(·)−1x is continuous on
[0, +∞[, then setting
¯S(t) =

S(t)−1
if t ≤0,
S(t)
if t ≥0,
¯S is a strongly continuous group.
⊓⊔
The inﬁnitesimal generator A of S is the linear operator in X deﬁned by
D(A) =

x ∈X : such that the
lim
h↘0+
1
h[S(h)x −x] exists

,
Ax = lim
h↘0+
1
h[S(h)x −x],
∀x ∈D(A).
(2.4)

90
II-1 Semigroups of Operators and Interpolation
Proposition 2.1. Let S be a strongly continuous semigroup on X with inﬁn-
itesimal generator A. Then:
(i) D(A) is dense in X,
(ii) ∀x ∈D(A), S(·)x ∈C1([0, ∞[; X) ∩C

[0, ∞[; D(A)

and
d
dtS(t)x = AS(t)x = S(t)Ax,
t ≥0.
(2.5)
(iii) A is closed.
Proof. (i) Set
Qh = 1
h(S(h) −I)
and
Ma,hx = 1
h
 a+h
a
S(s)x ds,
a ≥0, h > 0.
Then
lim
h→0+ Ma,hx = S(a)x
and
QhM0,t = 1
t (Mt,h −M0,h).
It follows that, for any x ∈X, we have M0,tx ∈D(A) and that
AM0,tx = 1
t (S(t) −I)x,
t > 0.
Since limt→0+ M0,tx = x we see that D(A) is dense in X so that (i) is proved.
(ii) If x ∈D(A) we have QhS(t)x = S(t)Qhx →S(t)Ax as h →0. Thus
S(t)x ∈D(A) and AS(t)x = S(t)Ax. Let us now prove that S(·)x is right
diﬀerentiable for x ∈D(A). If t0 ≥0, and h > 0, we have
1
h[S(t0 + h) −S(t0)x] = QhS(t0)x →AS(t0)x
as h →0.
To prove the left diﬀerentiability, ﬁx t0 > 0 and let h ∈]0, t0[. Then we have
S(t0 −h)x −S(t0)x
(t0 −h) −t0
= S(t0 −h)Qhx →S(t0)Ax = AS(t0)x
as h →0,
where we have used the result that S(·)x is bounded on bounded subsets of
[0, ∞[ by the uniform boundedness theorem. The C1-continuity with values
in X and the continuity with values in D(A) follow from (2.5). Thus (ii) is
proved.
(iii) Consider a sequence {xn} ⊂D(A) such that xn →x ∈X and Axn →
y ∈X. By (2.5) it follows that S(t)xn −xn =
 t
0 S(s)Axn ds, so that, as n
goes to inﬁnity, S(t)x −x =
 t
0 S(s)y ds. Thus by deﬁnition of A,
Qtx = 1
t
 t
0
S(s)y ds →y
as t →0
and
Ax = y.
⊓⊔

2 Linear evolution equations and strongly continuous semigroups
91
Theorem 2.1. Let S be a strongly continuous semigroup on X with inﬁnites-
imal generator A. Then for each x in D(A), the system
dx
dt (t) = Ax(t), t ≥0,
x(0) = x ∈D(A)
(2.6)
has a unique solution x in C1([0, ∞[; X) ∩C

[0, ∞[; D(A)

and
x(t) = S(t)x,
∀t ≥0.
(2.7)
Proof. The existence follows from Proposition 2.1. To prove uniqueness let v
be another solution in C1([0, ∞[; X) ∩C

[0, ∞[; D(A)

. Fix t > 0 and set
z(s) = S(t −s)v(s),
s ∈[0, t].
From Proposition 2.1 and the properties of
v
and
z ∈C1([0, ∞[; X) ∩C

[0, ∞[; D(A)

,
we have for all s in [0, t]
dz
ds(s) = −AS(t −s)v(s) + S(t −s)dv
ds (s) = S(t −s)
dv
ds (s) −Av(s)
	
= 0.
As a result z ∈C1([0, t]; X) and z(s) = z(0) for all s ∈[0, t]. This implies
v(t) = z(t) = S(t)v(0) = S(t)x for all t ≥0.
⊓⊔
2.2 Asymptotic behavior of S(t)
Let S be a strongly continuous semigroup on X. Deﬁne
ω0(S) = inf
t>0
1
t log ∥S(t)∥.
(2.8)
ω0(S) is said to be the type of S.
Proposition 2.2. The type ω0 = ω0(S) of the semigroup S is ﬁnite or equal
to −∞, and moreover,
ω0 = lim
t→∞
1
t log ∥S(t)∥.
(2.9)
Proof. By deﬁnition ω0 is ﬁnite or −∞and
ω0 ≤lim inf
t→∞
1
t log ∥S(t)∥.
So it is suﬃcient to show that
lim sup
t→∞
1
t log ∥S(t)∥≤ω0.

92
II-1 Semigroups of Operators and Interpolation
For each ω > ω0, there exists tω > 0 such that
1
tω
log ∥S(tω)∥< ω.
Any t ≥0 can be written in the form
t = n(t)tω + r(t),
n(t) ∈N, 0 ≤r(t) < tω.
Then
S(t) = S

n(t)tω + r(t)

= S(tω)n(t)S

r(t)

and
log ∥S(t)∥≤n(t) log ∥S(tω)∥+ log
!!S

r(t)
!!.
But ∥S(t)∥is bounded on the compact interval [0, tω] by some constant M > 0
and
log ∥S(t)∥
t
≤n(t) log ∥S(tω)∥+ log M
n(t)tω + r(t)
≤
log ∥S(tω)∥
tω + r(t)/n(t) + log M
t
≤log ∥S(tω)∥
tω
+ log M
t
≤ω + log M
t
.
As t goes to ∞,
lim sup
t→∞
1
t log ∥S(t)∥≤ω.
Since ω > ω0 is arbitrary and ﬁnite, the above inequality holds with ω = ω0.
⊓⊔
Corollary 2.1. If S is a strongly continuous semigroup of type ω0, then, for
any ω > ω0, there exists Mω ≥1 such that
∥S(t)∥≤Mωeωt,
t ≥0.
(2.10)
Proof. Choose tω as in the proof of Proposition 2.2 and set t = n(t)tω + r(t).
If ω0 ≥0, then ω > ω0 ≥0 and we have
∥S(t)∥≤∥S(tω)∥n(t)!!S

r(t)
!! ≤Mω exp(tωn(t)ω) ≤Mωeωt,
where Mω = sup{∥S(s)∥: s ∈[0, tω]}. If ω0 < 0, then we consider the semi-
group S0(t) = e−ω0tS(t) for which the type is 0. So for each ω > ω0, ω−ω0 > 0
and there exists Mω > 1 such that
e−ω0t∥S(t)∥= ∥e−ω0tS(t)∥≤Mωe(ω−ω0)t,
t ≥0.
This completes the proof.
⊓⊔

2 Linear evolution equations and strongly continuous semigroups
93
Semigroups S of negative type can be completely characterized through the
asymptotic behavior of their trajectories. This characterization was ﬁrst intro-
duced by R. Datko [2] in 1970 who extended the Lyapunov’s theorem from
ﬁnite dimensional spaces to Hilbert spaces H and showed that exponential de-
cay is equivalent to the fact that all trajectories belong to L2(0, ∞; H). This
last result was generalized by A. Pazy [1] in 1972 from Hilbert spaces H to
Banach spaces X and from L2-spaces to Lp-spaces, 1 ≤p < ∞. The simpliﬁed
proof of (ii) =⇒(iii) below is due to A. J. Pritchard and J. Zabczyk [1]
in 1981. To our knowledge condition (v) was ﬁrst given by A. Bensoussan,
M. C. Delfour, and S. K. Mitter [1] in 1976 (see also M. C. Delfour [7]
in 1978).
In the following we ﬁrst give the general results in Lp for an arbitrary
Banach space X. Then we specialize to Hilbert spaces and introduce the
Lyapunov’s operator equation.
Theorem 2.2. Let S be a strongly continuous semigroup in a Banach space
X and p ∈[1, ∞[, a real number. The following properties are equivalent:
(i) the property
∀x ∈X,
 ∞
0
|S(t)x|p dt < ∞;
(2.11)
(ii) there exists a constant c > 0 such that
∀x ∈X,
 ∞
0
|S(t)x|p dt ≤cp|x|p;
(2.12)
(iii) the type ω0(S) of S veriﬁes the condition
ω0(S) < 0;
(2.13)
(iv) there exists α > 0 and M ≥1 such that
∀x ∈X,
∀t ≥0,
|S(t)x| ≤Me−αt|x|;
(2.14)
(v) S is asymptotically stable in L(X), that is
lim
t→∞∥S(t)∥= 0.
(2.15)
Corollary 2.2. Let {S(t)} be a strongly continuous semigroup of type ω0(S):
(i) For all ω > ω0(S), A −ωI is the inﬁnitesimal generator of the exponen-
tially stable semigroup {Sω(t)},
Sω(t) = e−ωtS(t),
t ≥0.
(ii) For all x in X, there exists a unique element
y = −
 ∞
0
e−ωsS(s)x ds ∈D(A)
such that
[A −ωI]y = x.

94
II-1 Semigroups of Operators and Interpolation
In other words, for all ω > ω0(S), the operator A −ωI : D(A) →X has a
bounded inverse and for all x in X
 ∞
0
e−ωsS(s)x ds = −[A −ωI]−1x = R(ω, A)x
and R(ω, A)x is the Laplace transform of S(·)x.
Remark 2.3. Statements (iii), (iv), and (v) are independent of p, 1 ≤p < ∞.
As a result (i) and (ii) are true for all p, 1 ≤p < ∞, and conversely it is
suﬃcient to establish (i) or (ii) for some p, 1 ≤p < ∞, to obtain (iii), (iv),
and (v).
⊓⊔
Remark 2.4. In inﬁnite dimension asymptotic stability,
∀x ∈X,
S(t)x →0
as t →∞
does not imply exponential stability. R. Datko [2] has given the following
simple example for a group {S(t)} in the Hilbert space ℓ2 with a bounded
inﬁnitesimal generator A. Let H = ℓ2,
ℓ2 =

x = (x1, . . . , xn, . . . ):
∞

i=1
x2
i < ∞

.
The group is
(S(t)x)n = e−t/nxn,
n = 1, 2, . . ..
It can be shown that
∀x ∈ℓ2,
S(t)x →0
as t →∞,
∀t,
∥S(t)x∥= 1
and
ω0(S) = 0,
(Ax)n = −xn
n ,
n = 1, 2, . . ., D(A) = ℓ2,
σ(A) =

−1
n : n = 1, 2, . . .

∪{0}.
⊓⊔
Remark 2.5. Relaxations of conditions (2.11) from the p-th power of the norm
in X to other functions can be found in several places in the literature. For
instance J. Zabczyk [3, §5, Theorem 5.1 and Remark 5.2] used a continuous
strictly increasing convex function N : [0, ∞[ →[0, ∞[ such that N(0) = 0,
and the condition
∀x ∈X,
∃α > 0,
 ∞
0
N(α(|S(t)x|) dt < ∞.
This condition implies condition (iv) in Theorem 2.2. In 1986 S. Rolewicz [1]
generalized the above result to evolution operators and showed that the con-
vexity assumption can be dropped. It seems that this result was independently
re-discovered in 1987 in L. Markus [1] and W. Littman [1].
⊓⊔

2 Linear evolution equations and strongly continuous semigroups
95
Remark 2.6. For p, 1 ≤p < ∞, and a strongly continuous semigroup S of
negative type
∀x ∈D(A),
S(·)x ∈W 1,p(0, ∞; X) ∩Lp
0, ∞; D(A)

.
(2.16)
⊓⊔
Proof of Theorem 2.2.
(i) =⇒(ii). Deﬁne for each integer k ≥1, the set
Uk =

x ∈X :
 ∞
0
|S(t)x|p dt < k

.
By hypothesis X = "∞
k=1 Uk and because X is a complete metric space
∃k0 > 0,
∃x0 ∈Uk0,
∃r0 > 0 such that B(x0, r0) ⊂Uk0,
where B(x0, r0) is the ball of center x0 and radius r0 (cf., for instance,
J. Horv´ath [1, Corollary to Baire’s theorem, p. 62]. Hence
 ∞
0
|S(t)(x0 + y)|p dt ≤kp
0,
∀y ∈B(0, r0),
and for all y in B(0, r0)
∥S(·)y∥Lp(0,∞;X) ≤∥S(·)(x0 + y)∥Lp(0,∞;X) + ∥S(·)(x0)∥Lp(0,∞;X) ≤2k0.
So for all x in X, x ̸= 0, we can apply the above inequality to y = r0x/|x| and
∥S(·)x∥Lp(0,∞;X) ≤2k0
r0
|x|.
(ii) =⇒(iii). We know that for all ω > ω0 there exists M ≥1 such that
∀t ≥0,
∀x ∈X,
|S(t)x| ≤Meωt|x|.
Choose ω > max{1, ω0}
1
pω(1 −e−pωt)|S(t)x|p =
 t
0
e−pωr|S(t)x|p dr
=
 t
0
e−pωr∥S(r)∥p|S(t −r)x|p dr ≤M p
 ∞
0
|S(t)x|p dt ≤M pcp|x|p.
As a result
∀t > 0,
|S(t)x|p ≤
pω
1 −e−pωt M pcp|x|p.
Fix T > 0. As ∥S(t)∥is bounded on any compact interval [0, T ], T > 0, and
ω > 0, then
∃K > 0 such that ∀t ≥0,
|S(t)x| ≤K|x|,

96
II-1 Semigroups of Operators and Interpolation
where
K = max
#
max {∥S(t)∥: 0 ≤t ≤T } , Mc

pω/(1 −e−pωT )
1/p$
.
As K is independent of x, we have ∥S(t)∥≤K for all t ≥0.
We now use a second estimate for t > 0
t|S(t)x|p =
 t
0
|S(t)x|p dr ≤
 t
0
∥S(r)∥p|S(t −r)x|p dr ≤Kpcp|x|p,
which yields for all t ≥0
∥S(t)∥≤Kct−1/p.
But by deﬁnition of ω0 as an inﬁmum over t > 0
∀t > 0,
1
t log ∥S(t)∥≥ω0
and necessarily
∀t > 0,
eω0t ≤∥S(t)∥≤Kct−1/p.
But this only holds for ω0 < 0.
(iii) =⇒(iv). It is suﬃcient to choose α = 1 when ω0 = −∞and α = −1
2ω0
when ω0 is ﬁnite.
(iv) =⇒(i) and (iv) =⇒(v) are obvious. To complete the proof we prove
that (v) =⇒(iv). By deﬁnition of ω0
∀t ≥0,
∥S(t)∥≥eω0t.
But by hypothesis S(t) →0 as t goes to ∞and necessarily
0 = lim
t→∞∥S(t)∥≥lim
t→∞eω0t,
which in turn implies that ω0 < 0.
⊓⊔
We now prove the corollary.
Proof of Corollary 2.2.
Notice that the type ω0(Sω) = ω0 −ω < 0. So from
the equivalence between (i) and (iii) in Theorem 2.2
∀x ∈X,
Sω(·)x ∈L1(0, ∞; X)
and
y(t) = −
 t
0
Sω(s)x ds →y = −
 ∞
0
Sω(s)x ds
in X
as t goes to +∞. Consider the expression
Sω(t)y −y = −Sω(t)
 ∞
0
Sω(s)x ds +
 ∞
0
Sω(s)x ds
= −
 ∞
0
Sω(t + s)x ds +
 ∞
0
Sω(s)x ds
= −
 ∞
t
Sω(s)x ds +
 ∞
0
Sω(s)x ds =
 t
0
Sω(s)x ds

2 Linear evolution equations and strongly continuous semigroups
97
and
lim
t↘0
Sω(t)y −y
t
= x.
Therefore
y ∈D(A)
and
[A −ωI]y = x.
So A −ωI : D(A) →X is onto. To show that it is one-to-one, consider y ∈
D(A) such that [A −ωI]y = 0. Then
d
dtSω(t)y = Sω(t)[A −ωI]y = 0,
∀t ≥0
and this means that there exists a constant c ∈X such that
∀t ≥0,
Sω(t)y = c.
But {Sω(t)} is exponentially stable and
c = y = Sω(0)y = lim
t→∞Sω(t)y = 0 =⇒y = 0.
This completes the proof.
⊓⊔
A. Pazy [1] pointed out that for semigroups of negative type the quantity
|x|p = ∥S(·)x∥Lp(0,∞;X)
(2.17)
deﬁnes a norm on X and that the property
∃c > 0,
∀x ∈X,
|x|p ≤c|x|
(2.18)
deﬁnes a continuous embedding of X into Lp(0, ∞; X). So it is natural to ask
when are the two norms equivalent.
Theorem 2.3 (A. Pazy [1]). Let X be a Banach space, p, 1 ≤p < ∞, a real
number and S(·) a semigroup of negative type. Then the following conditions
are equivalent:
(i) the norms | · | and | · |p are equivalent on X, that is,
∃m > 0 such that |x|p ≥m|x|;
(ii) the condition
∃t0 > 0,
∃¯c > 0 such that ∀x ∈X,
|S(t0)x| ≥¯c|x|.
Proof. (i) =⇒(ii). For all x in X and t > 0
mp|x|p ≤
 ∞
0
|S(r)x|p dr ≤
 t
0
|S(r)x|p dr +
 ∞
0
|S(t + r)x|p dr
≤
 t
0
∥S(r)∥p|x|p dr + cp|S(t)x|p
≤tM p|x|p + cp|S(t)x|p.

98
II-1 Semigroups of Operators and Interpolation
Choose t0 = mp/2M p and ¯c = m/2c.
(ii) =⇒(i). Use the identity
t0 |S(t0)x|p =
 t0
0
|S(t0 −t)S(t)x|p dt
≤max
[0,t0] ∥S(t)∥p
 ∞
0
|S(t)x|p dt ≤M pcp|x|p
p
and
¯c|x| ≤|S(t0)x| ≤Mc
t1/p
0
|x|.
⊓⊔
Remark 2.7. When condition (ii) is veriﬁed for some t0 > 0, it is also veriﬁed
for all t in [0, t0]. Indeed
c|x| ≤|S(t0)x| = |S(t0 −t)S(t)x| ≤M|S(t)x|.
Any t ≥0 can be decomposed as
t = nt0 + τ,
n an integer and 0 ≤τ < t0.
Then
|S(t)x| = |S(t0)nS(τ)x| ≥cn|S(τ)x| ≥cn+1|x|.
⊓⊔
Corollary 2.3. Assume that the hypotheses of the previous theorem are veri-
ﬁed, that S is of negative type and that for all t ≥0 the image Im S(t) of S(t)
is dense in X. Then | · |p deﬁnes a norm equivalent to the norm | · | on X
if and only if the semigroup S on X has an extension to a group of bounded
operators on X.
Theorem 2.4. Let X = H be a Hilbert space with inner product (·, ·), and let
S be a strongly continuous semigroup on H with inﬁnitesimal generator A.
Then each statement in the Theorem 2.2 is equivalent to:
(vi) there exists a positive symmetric operator P ∈L(H) such that
∀x, y ∈D(A),
(PAx, y) + (Px, Ay) + (x, y) = 0.
(2.19)
Proof. It is suﬃcient to show that (i) with p = 2 implies (vi) and that (vi)
implies (ii) with p = 2.
(i) =⇒(vi). For all t ≥0 and x and y in H deﬁne
(P(t)x, y) =
 t
0
(S(r)x, S(r)y) dr,
(Px, y) =
 ∞
0
(S(r)x, S(r)y) dr.
By hypothesis P and P(t) are well-deﬁned elements of L(H). In addition they
are symmetric and positive. Moreover since (i) =⇒(ii)

2 Linear evolution equations and strongly continuous semigroups
99

P −P(t)

x, y
 =

 ∞
t
(S(r)x, S(r)y) dr

≤∥S(·)x∥L2(t,∞;H)∥S(·)y∥L2(t,∞;H) ≤c|y| ∥S(·)x∥L2(t, ∞; H)
and, as t goes to ∞, P(t)x →Px in H for each x in H.
For all x and y in D(A)
(P(t)Ax, y) + (P(t)x, Ay) =
 t
0
{(S(r)Ax, S(r)y)+(S(r)x, S(r)Ay)} dr
=
 t
0
d
dr (S(r)x, S(r)y) dr=(S(t)x, S(t)y)−(x, y).
Therefore for y = x
lim
t→∞|S(t)x|2 = |x|2 + ⟨PAx, x⟩+ ⟨Px, Ax⟩.
But
 ∞
0
|S(t)x|2 dt < ∞=⇒lim inf
t→∞|S(t)x|2 = 0
and because the limit exists it coincides with its lim inf and
∀x ∈D(A),
lim
t→∞|S(t)x| = 0.
We now go back to our previous computation for x and y
(P(t)Ax, y) + (P(t)x, Ay) = (S(t)x, S(t)y) + (x, y)
and let t go to +∞to obtain (vi).
(vi) =⇒(ii). For each x ∈D(A), S(t)x ∈D(A), ∀t ≥0, and
(PAS(t)x, S(t)x) + (PS(t)x, AS(t)x) + (S(t)x, S(t)x) = 0
or equivalently
d
dt(PS(t)x, S(t)x) + |S(t)x|2 = 0.
By integrating from 0 to T
(PS(T )x, S(T )x) −(Px, x) = −
 T
0
|S(t)x|2 dt.
Since P is positive for all x ∈D(A)
 T
0
|S(t)x|2 dt ≤(Px, x) =⇒
 ∞
0
|S(t)x|2 dt ≤(Px, x)
and we obtain (ii) for p = 2 with c = ∥P∥. By density of D(A) in H, the
result also holds for all x in H.
⊓⊔

100
II-1 Semigroups of Operators and Interpolation
Remark 2.8. Notice that in the Hilbertian case, it is not necessary to use
Baire’s theorem to show (i) =⇒(ii). Instead we show that (i) =⇒(vi) =⇒(ii).
⊓⊔
Corollary 2.4. If ¯P is a positive and symmetric solution of the Lyapunov
equation in L(H), then
∀x ∈H,
( ¯Px, x) ≥(Px, x),
(2.20)
where P in L(H) is deﬁned as
∀x, y ∈H,
(Px, y) =
 ∞
0
(S(t)x, S(t)y) dt.
(2.21)
Proof. For all x in D(A) we proceed as in the proof of the previous theorem
and
d
dt( ¯PS(t)x, S(t)x) + |S(t)x|2 = 0.
Hence
 T
0
|S(t)x|2 dt ≤( ¯Px, x) =⇒(Px, x) =
 ∞
0
|S(t)x|2 dt ≤∥¯P∥|x|2
where the transformation P, as deﬁned in the proof of Theorem 2.4, is well
deﬁned and for all x in D(A), (Px, x) ≤( ¯Px, x). So by density of D(A) in H
the corollary holds for all x in H.
⊓⊔
2.3 Spectral properties of the inﬁnitesimal generator
Proposition 2.3. Let S be a strongly continuous semigroup in X. Assume
that there exist constants M > 0 and ω ∈R be such that
∥S(t)∥≤Meωt,
∀t ≥0.
(2.22)
Then the inﬁnitesimal generator A of S has the following properties:
(i) Cω = {λ ∈C: Re λ > ω} ⊂ρ(A),
(ii) for any λ ∈Cω the resolvent of A is given by
R(λ, A)y =
 ∞
0
e−λtS(t)y dt,
y ∈X.
(2.23)
Proof. We have to show that, for any λ ∈Cω and any y ∈X, the equation
λx −Ax = y
(2.24)
has a unique solution given by x equal to the right-hand-side of (2.23).

2 Linear evolution equations and strongly continuous semigroups
101
Existence.
For λ ∈Cω and y ∈X, x is well-deﬁned as an elements of X by the right-hand
side of (2.23). Then we have
Qhx = 1
h(S(h)x −x) = 1
h(eλh −1)x −1
heλh
 h
0
e−λtS(t)y dt.
As h →0, we ﬁnd that Qhx →Ax and that the right-hand side goes to λx−y.
Therefore λx −y = Ax so that x ∈D(A) and x is a solution of (2.24).
Uniqueness.
Let ¯x ∈D(A) be another solution of (2.24). Then we have
x =
 ∞
0
e−λtS(t)(λ¯x −A¯x) dt = −
 ∞
0
d
dt[e−λtS(t)¯x] dt = ¯x.
Thus x = ¯x.
⊓⊔
2.4 Hille–Yosida–Miyadera–Feller–Phillips theorem
We now give necessary and suﬃcient conditions on a linear operator A to
be the inﬁnitesimal generator of a strongly continuous semigroup. For early
versions of this theorem, the reader is referred to E. Hille [2, p. 238] and
K. Yosida [1] and for modern versions to W. Feller [1], I. Miyadera [1],
and R. S. Phillips [1].
Theorem 2.5. Let A: D(A) ⊂X →X be a linear operator. Then the follow-
ing statements are equivalent:
(i) D(A) is dense in X, there exist real numbers M > 0 and ω ∈R such
that ρ(A) ⊃{λ ∈C: Re λ > ω} and the following inequalities hold:
∥Rk(λ, A)∥≤M(Re λ −ω)−k,
∀k ∈N,
∀λ, Re λ > ω.
(2.25)
(ii) A is the inﬁnitesimal generator of a strongly continuous semigroup S and
there exist real numbers ω ∈R and M > 0 such that
∥S(t)∥≤Meωt,
∀t ≥0.
(2.26)
Proof. (ii) =⇒(i). From Proposition 2.3 {λ ∈C: Re λ > ω} ⊂ρ(A) and from
formula (2.23) and inequality (2.26), we get condition (2.25) for k = 1. From
Proposition 2.1, D(A) is dense in X and A is closed. By diﬀerentiating (2.23)
k times with respect to λ, we get
R(k)(λ, A)y = (−1)k
 ∞
0
tke−λtS(t)y dt,
y ∈X.

102
II-1 Semigroups of Operators and Interpolation
It follows that
∥R(k)(λ, A)∥≤Mk!(Re λ −ω)−k−1,
k ∈N.
But from Proposition 2.3(ii) for all y ∈X
Rk(λ, A)y =
 ∞
0
tk
k!e−λtS(k)y dt = (−1)k
k!
R(k)(λ, A)y
and the conclusion follows.
(i) =⇒(ii). We proceed in four steps.
Step 1. Construction of an approximate semigroup Sn.
For any integer n ∈N such that n > ω, we set
Jn = nR(n, A),
An = AJn
(2.27)
and recall that
An = n2R(n, A) −nI.
The bounded operators An are called the Yosida approximations of A. We
claim that the following identities hold:
lim
n→∞Jnx = x,
x ∈X,
(2.28)
lim
n→∞Anx = Ax,
x ∈D(A).
(2.29)
We ﬁrst prove (2.28). For x ∈D(A), Jnx −x = R(n, A)Ax →0 as n →∞
from (2.25). But ∥Jn∥≤Mn(n −ω)−1 is less than a constant and by density
(2.28) is true for all x in X. Now (2.29) is an immediate consequence of (2.28)
since Anx = JnAx for any x ∈D(A). We now set
Sn(t) = exp(tAn) = e−nt
∞

k=0
n2ktk
k!
Rk(n, A).
(2.30)
By (2.25) it follows that
∥Sn(t)∥≤M exp

ntω/(n −ω)

≤Me2ωt,
if n > 2ω, t > 0.
(2.31)
Step 2. Uniform convergence of Sn(t)x to S(t)x for all x in X on compact
intervals in [0, ∞[.
Fix x ∈X and set un(t) = Sn(t)x. Then, if n, m > ω, we have
⎧
⎨
⎩
d
dt(un −um) = An(un −um) + (An −Am)um,
(um −um)(0) = 0,
un(t) −um(t) =
 t
0
exp

(t −s)An

exp(sAm)(An −Am)x ds,

2 Linear evolution equations and strongly continuous semigroups
103
and, using (2.31), the estimate
|un(t) −um(t)| ≤M 2e2tωt|Anx −Amx|,
m, n > 2ω
(2.32)
follows. In view of (2.29), for all x ∈D(A) {un(t)} is a Cauchy sequence. But
the same is true for any x ∈X by virtue of (2.31). Thus the following limit
exists:
lim
n→∞un(t) = lim
n→∞Sn(t)x = u(t),
x ∈X,
(2.33)
uniformly in t on the bounded intervals of [0, ∞[. Set now S(t)x = u(t). It is
easy to check that S is a strongly continuous semigroup in X and that (2.26)
holds.
Step 3. If x ∈D(A), then S(·)x is diﬀerentiable and
d
dtS(t)x = S(t)Ax = AS(t)x.
(2.34)
In fact by (2.29) and (2.33) it follows that
d
dtun(t) = exp(tAn)Anx →S(t)Ax
as n →∞.
Step 4. A is the inﬁnitesimal generator of S.
Let B be the inﬁnitesimal generator of S. By (2.34) it follows that B is an
extension of A. Then it is suﬃcient to prove that if x ∈D(B) then x ∈D(A).
Let, in fact, x ∈D(B), Re λ > ω, and z = λx −Bx. Then R(λ, A)z ∈D(A)
and
(λ −B)R(λ, A)z = (λ −A)R(λ, A)z = z
so that
x = R(λ, B)z = R(λ, A)z ∈D(A).
⊓⊔
2.5 Adjoint semigroups and their generators
Given a semigroup of continuous linear transformations S(t), t ≥0, on X, it
is always possible to deﬁne the adjoint transformations S∗(t), t ≥0, on X′
and it is easy to check that
∀t ≥0,
∀s ≥0,
S∗(t + s) = S∗(t)S∗(s),
S∗(0) = I,
∀x∗∈X′,
t →S∗(t)x∗: [0, ∞[ →X′ (weak) is continuous .
We shall say that S∗is the adjoint semigroup associated with S. However S∗
is not necessarily a strongly continuous semigroup on X. Fortunately we have
the following result in reﬂexive Banach spaces.
Proposition 2.4. If S is a strongly continuous semigroup on a reﬂexive Ba-
nach space X, the adjoint semigroup S∗is also a strongly continuous semi-
group on X.

104
II-1 Semigroups of Operators and Interpolation
Proof. Let A be the inﬁnitesimal generator of S and let A∗be the adjoint
operator of A. As D(A) is dense in X, R(λ, A)∗= R(¯λ, A∗) and ρ(A) = ρ(A∗)
(cf. E. Hille and R. S. Phillips [1, Theorem 2.6.5, p. 56]). For all λ in ρ(A),
R(λ, A), and R(¯λ, A∗) are bounded and
∥R(¯λ, A∗)∥L(X′) = ∥R(λ, A∗∥L(X′) = ∥R(λ, A)∥L(X).
So if we can show that D(A∗) is dense in X′, all the estimates for A in Theo-
rem 2.5 will be true for A∗and we shall conclude that A∗is the inﬁnitesimal
generator of a strongly continuous semigroup on X′. For x∗in X′ we construct
the approximations x∗
n = nR(n, A∗)x∗, n ≥1. For all x in X
lim
n→∞⟨x∗
n, x⟩= lim
n→∞⟨x∗, nR(n, A)x⟩= ⟨x∗, x⟩.
As X is reﬂexive we conclude that x∗
n →x∗in X′ (weak). But in a reﬂexive
Banach space the strong closure
¯
D(A∗) of D(A∗) is weakly closed because
¯
D(A∗) is a closed linear subspace of X′. So we have proved that D(A∗) is
(strongly) dense in X′ because the strong and weak closures of D(A∗) coincide
(cf. K. Yosida [2, Theorem 11, p. 125]).
⊓⊔
2.6 Semigroups of contractions and dissipative operators
A semigroup of contractions is a strongly continuous semigroup S such that
∥S(t)∥≤1,
∀t ≥0.
(2.35)
If S is a semigroup of contractions, then condition (2.25) reduces to
∥R(λ, A)∥≤
1
Re λ,
∀λ, Re λ > 0,
(2.36)
because we can choose M = 1 and ω = 0.
In this section we shall give another characterization of linear operators
that generate semigroups of contractions. For this we need the deﬁnition of
dissipative operators.
We ﬁrst recall that for any x ∈X the sub-diﬀerential ∂|x| is deﬁned by
∂|x| = {x′ ∈X′: |x′| = 1, ⟨x, x′⟩= |x|},
(2.37)
where X′ is the topological dual of X. By the Hahn–Banach theorem, it follows
that ∂|x| is never empty.
Deﬁnition 2.2.
(i) We say that the linear operator A: D(A) ⊂X →X is
dissipative if
∀x ∈D(A),
∃x′ ∈∂|x| such that Re⟨Ax, x′⟩≤0.
(ii) The linear operator A: D(A) ⊂X →X is said to be maximal dissipative
if it is dissipative and has no proper dissipative extension.
⊓⊔

2 Linear evolution equations and strongly continuous semigroups
105
Proposition 2.5. Let A be the inﬁnitesimal generator of a strongly continu-
ous semigroup of contractions. Then A and its adjoint operator A∗are maxi-
mal dissipative. Moreover
∀λ > 0,
R(λI −A) = X
and
R(λI −A∗) = X′.
Proof. Let x ∈D(A), x′ ∈∂|x|, and h > 0. Then in view of (2.37) and (2.35)
Re⟨S(h)x −x, x′⟩= Re⟨S(h)x, x′⟩−|x| ≤0
since ∥S(h)∥≤1. It follows that
Re⟨Ax, x′⟩= Re lim
h→0
1
h(⟨S(h)x −x, x′⟩) ≤0.
By hypothesis, A is a closed densely deﬁned linear operator on X. Hence its
adjoint A∗is a well-deﬁned closed linear operator on X′ with domain D(A∗)
(cf. E. Hille and R. S. Phillips [1, Theorem 2.11.8, p. 43]). But from (2.36)
for all λ > 0, λ ∈ρ(A), λI −A has a bounded inverse R(λ, A), R(λI −A) = X,
and
∥R(λ, A)∥≤1
λ,
∀λ > 0.
As
¯
D(A) = X and λI −A has a linear bounded inverse, then
R(λI −A∗) = X′
(cf. E. Hille and R. S. Phillips [1, Theorem 2.11.15, p. 45]) and
R(λ, A∗) = (λI −A∗)−1 = (λI −A)−1∗= R(λ, A)∗
(cf. E. Hille and R. S. Phillips [1, Theorem 2.11.14, p. 44]). So for λ > 0,
λI −A∗has a bounded inverse and
∥R(λ, A∗)∥= ∥R(λ, A)∗∥= ∥R(λ, A)∥≤1
λ,
∀λ > 0.
Hence for all y∗∈X′ and λ > 0
|R(λ, A∗)y∗|X′ ≤1
λ|y∗|X′.
For all x∗∈D(A∗)
R(λ, A∗)(λI −A∗)x∗= x∗
and for all x∗∈D(A∗), λ > 0
|x∗|X′ ≤1
λ|(λI −A∗)x∗|X′.
The dissipativity of A∗now follows from the following lemma.

106
II-1 Semigroups of Operators and Interpolation
Lemma 2.1 (A. Pazy [1, Theorem 4.2, p. 14]). A linear operator T is dis-
sipative if and only if
|(λI −T )x| ≥λ|x|,
∀x ∈D(T ),
∀λ > 0.
Finally both A and A∗are maximal dissipative because in both cases
∀λ > 0,
λI −A: D(A) →X
and
λI −A∗: D(A∗) →X′
are bijective maps (one-to-one and onto).
⊓⊔
If A is maximal dissipative, it is generally not the generator of a strongly
continuous semigroup since its domain is not necessarily dense (cf. A. Pazy [2,
Example 4.7, p. 16–17]). Also if A is a dissipative closed densely deﬁned op-
erator that is not maximal, then it will not necessarily be a generator. A
necessary and suﬃcient condition is given by the following theorem.
Theorem 2.6 (G. Lumer and R. S. Phillips [1]). Let A: D(A) ⊂X →
X be a linear operator deﬁned on a Banach space X. A is the inﬁnitesimal
generator of a semigroup of contractions on X if and only if :
(i) A is a closed linear operator with dense domain in X and
(ii) A and its adjoint operator A∗are dissipative.
Proof. (⇐=) By Theorem 2.5 it is suﬃcient to prove that for any y ∈X and
for any λ with Re λ > 0 the equation
λx −Ax = y
(2.38)
has a unique solution and that the following estimate holds:
|x| ≤
1
Re λ|y|.
(2.39)
Now, since A is dissipative we easily obtain the following a priori estimate:
there exists x′ ∈∂|x| such that
Re λ|x| = Re⟨λx, x′⟩= Re⟨Ax, x′⟩+ Re⟨y, x′⟩
≤Re⟨y, x′⟩≤|y|.
This proves the uniqueness of the solution of (2.38) as well as estimate (2.39).
It remains to prove that (λ −A)

D(A)

= X if Re λ > 0.
Step 1. (λ −A)

D(A)

is dense in X if Re λ > 0.
Let x′ ∈X′ and Re λ > 0 such that ⟨λx −Ax, x′⟩= 0 for any x ∈D(A).
We have ⟨Ax, x′⟩= λ⟨x, x′⟩, so that x′ ∈D(A∗) and ⟨x, ¯λx′ −A∗x′⟩for any
x ∈D(A). Since D(A) is dense in X we have ¯λx′ −A∗x′ = 0, which implies
(by (2.39) with A replaced by A∗) that x′ = 0 so that (λ−A)

D(A)

is dense
in X.

2 Linear evolution equations and strongly continuous semigroups
107
Step 2. (λ −A)

D(A)

= X if Re λ > 0.
Let Re λ > 0 and y ∈X. Since (λ −A)

D(A)

is dense in X there exists a
sequence {xn} ⊂D(A) such that yn = λxn −Axn →y as n →∞. By (2.39)
it follows that
|xn −xm| ≤
1
Re λ|yn −ym|
so that {xn} is a Cauchy sequence. Hence there exists x ∈X such that xn →x
in X, Axn = λxn −yn →λx −y. Since A is closed we have x ∈D(A) and
λx −Ax = y.
( =⇒). The converse is true by Proposition 2.5. This completes the proof.
⊓⊔
In the last theorem the dissipativity of both A and A∗are necessary to
obtain a semigroup of contractions. This arises from the fact that in a Banach
space a closed linear maximal dissipative operator A is not necessarily densely
deﬁned even when R(λI −A) = X for all λ > 0. This diﬃculty disappears
when X is a reﬂexive Banach space (cf. A. Pazy [2, Theorem 4.6, p. 16]) and
we can restate Theorem 2.6 as follows.
Theorem 2.7. Let A: D(A) ⊂X →X be a linear operator deﬁned on a
reﬂexive Banach space X. A is the inﬁnitesimal generator of a semigroup of
contractions on X if and only if :
(i) A is dissipative, and
(ii) ∃λ0 > 0 such that R(λ0I −A) = X.
Proof. ( =⇒). From Proposition 2.5
(⇐=) By invoking Lemma 2.1, it is easy to show that for a linear dissipative
operator, A is closed if and only if its range, R(λ0I −A), is closed for some
λ0 > 0. Then from (ii) and (iii) by A. Pazy [2, Theorem 4.6, p. 16],
¯
D(A) = X.
Finally the result follows from A. Pazy [2, Theorem 4.3a].
⊓⊔
To complete the picture we also quote the Hilbert space version of the two
previous theorems.
Theorem 2.8 (R. S. Phillips [2, Theorem 1.1.3, p. 203]). Let A: D(A) ⊂
H →H be a linear operator deﬁned on a Hilbert space H. Then the following
conditions are equivalent:
(i) A is the inﬁnitesimal generator of a semigroup of contractions on H;
(ii) A is maximal dissipative;
(iii) A∗is maximal dissipative.
In the above theorem we have made use of the fact that in a Hilbert space
a maximal dissipative operator is closed (cf. H. Tanabe [1, Theorem 2.1.1,
p. 20]). The original theorem proved by R. S. Phillips [2, Theorem 1.1.3,
p. 203] used the conditions
(ii) A is maximal dissipative and densely deﬁned,

108
II-1 Semigroups of Operators and Interpolation
(iii) A∗is maximal dissipative and densely deﬁned,
which are equivalent to
(ii) A is closed and maximal dissipative,
(iii) A∗is closed and maximal dissipative.
We complete this section with the important subclass of semigroups of
contractions that preserve the norm
∀x,
∀t ≥0,
|S(t)x| = |x|.
Theorem 2.9 (M. H. Stone [1]). Assume that X = H where H is a Hilbert
space:
(i) If A is a self-adjoint operator on H, then B = iA is the inﬁnitesimal
generator of a strongly continuous group of unitary transformations.
(ii) Conversely if S(t) is a strongly continuous group of unitary transforma-
tions with inﬁnitesimal generator B, then iB is self-adjoint.
Proof. (i) By deﬁnition of B
Re⟨Bx, x⟩= Re⟨iAx, x⟩= 0,
∀x ∈D(A).
It follows that the linear operators B and B∗= −B are dissipative. The
conclusion follows from Theorem 2.6.
(ii) Since S(t) is a unitary transformation
|S(t)x|2 = |x|2,
(2.40)
and if x ∈D(B), B the inﬁnitesimal generator of S, then by diﬀerentiating
(2.40) at t = 0 we have Re⟨Bx, x⟩= 0 so that
Im⟨iBx, x⟩= Re⟨Bx, x⟩= 0
and iB is self-adjoint.
⊓⊔
2.7 Analytic semigroups
Assumption A Let A be a closed operator with dense domain D(A) in X.
Assume that there exist ω ∈R and θ0, π/2 < θ0 < π, such that
A-1.
ρ(A) contains a sector Sω,θ0,
Sω,θ0 = {λ ∈C: λ ̸= ω and | arg(λ −ω)| < θ0},

2 Linear evolution equations and strongly continuous semigroups
109
A-2.
There exists M > 0 such that
∥R(λ, A)∥≤
M
|λ −ω|,
λ ∈Sω,θ0.
(2.41)
Now deﬁne the operator
S(t) =
1
2πi

γε,θ
eλtR(λ, A) dλ,
t > 0, S(0) = I,
(2.42)
where γε,θ is the path (oriented in the increasing direction of Im λ) deﬁned in
the following way (see Figure 1.1):
γε,θ = γ+
ε,θ ∪γ−
ε,θ ∪γ0
ε,θ,
θ ∈] π
2 , θ0],
γ±
ε,θ = {z ∈C: z = ω + re±iθ, r ≥ε},
γ0
ε,θ = {z ∈C: z = ω + εe±iη, |η| ≤θ}.
We notice that the integral in (2.42) is convergent because θ > π/2; more-
over it does not depend on the choice of ε and θ as can easily be checked by
using the Cauchy theorem for holomorphic functions.
Theorem 2.10 (E. Hille [1]). Let A be a closed linear operator in X with
dense domain D(A) in X such that assumptions A1–A2 are veriﬁed and let
S be deﬁned by (2.42). Then the following statements hold:
(i) S is a strongly continuous semigroup with inﬁnitesimal generator A.
γε,θ
θ0
θ
Sω,θ0
ω
ε
Im λ
Re λ
Fig. 1.1. Sector Sω,θ0 and path of integration γε,θ.

110
II-1 Semigroups of Operators and Interpolation
(ii) There exist M > 0, N > 0 such that
∥S(t)∥≤Meωt,
∀t > 0,
(2.43)
∥(A −ωI)S(t)∥≤N
t eωt,
∀t > 0,
(2.44)
S ∈C1
]0, ∞[; L(X)

and
S′(t) = AS(t),
∀t > 0.
(2.45)
(iii) S has an analytic extension in a sector S0,θ0−π/2 and
S(z) =
1
2πi

γε,θ+π/2
eλzR(λ, A) dλ,
z = ρeiθ,
θ ∈[0, θ0 −π
2 [ and ρ ≥0.
(2.46)
Proof. We can assume that ω = 0 (otherwise we change A to A−ωI and S(t)
to e−ωtS(t)). The proof is divided into ﬁve steps.
Step 1. S ∈C1
]0, ∞[; L(X)

and S′(t) = AS(t), t > 0.
It is clear that, from (2.42), S is of class C∞in Sω,θ0 and that
S′(t) =
1
2πi

γε,θ
λeλtR(λ, A) dλ
=
1
2πi

γε,θ
eλtdλ +
1
2πi

γε,θ
AeλtR(λ, A) dλ = AS(t)
because

γε,θ eλtdλ = 0.
Step 2. There exist M > 0 and N > 0 such that ∥S(t)∥≤M, ∥AS(t)∥≤N/t.
Setting λt = ξ, (2.42) becomes
S(t) =
1
2πi

tγε,θ
eξR(ξ/t, A) dξ/t =
1
2πi

γε,θ
eξR(ξ/t, A) dξ/t
=
1
2πi
 +∞
ε
exp(reiθ)R(reiθ/t, A)eiθ dr/t
−
 +∞
ε
exp(re−iθ)R(re−iθ/t, A)e−iθ dr/t
+
 θ
−θ
exp(εeiη)R(εeiη/t, A)iεeiη dη/t

from which
∥S(t)∥≤1
2π

2
 ∞
ε
Mer cos θ dr/r +
 θ
−θ
Meε cos η dη

,
∥S′(t)∥≤
1
2πt

2
 ∞
ε
Mer cos θ dr + ε
 θ
−θ
Meε cos η dη


2 Linear evolution equations and strongly continuous semigroups
111
as ε →0, we obtain
∥S′(t)∥≤
M
πt| cos θ|.
Step 3. S is strongly continuous.
As S(t) is bounded (by the second step), it is suﬃcient to prove that
limt→0 S(t)x = x for any x ∈D(A). Let x ∈D(A) and set y = x −Ax. Then
x = R(1, A)y and we have
S(t)x = S(t)R(1, A)y =
1
2πi

γε,θ
eλtR(λ, A)R(1, A)y dλ
=
1
2πi

γε,θ
eλtR(λ, A)y dλ
1−λ −1
2πi

γε,θ
eλtR(1, A)y dλ
1−λ
=
1
2πi

γε,θ
eλtR(λ, A)y dλ
1 −λ.
As t →0 we have
lim
t↘0 S(t)x =
1
2πi

γε,θ
R(λ, A)y dλ
1 −λ = R(1, A)y = x.
Step 4. S(t + s) = S(t)S(s), t, s, > 0.
We have
S(t)S(s) = −1
4π2

γε,θ
eλtR(λ, A) dλ

γ2ε,θ′
eµsR(µ, A) dµ,
where θ′ ∈]π/2, θ[. It follows, using the resolvent identity, that
γε,θ
γ2ε,θ′
λ
θ
θ′
µ
Im λ
Re λ
Fig. 1.2. Paths of integration γε,θ and γ2ε,θ′.

112
II-1 Semigroups of Operators and Interpolation
S(t)S(s) = −1
4π2

γε,θ×γ2ε,θ′
eλt+µs[R(λ, A) −R(µ, A)]dλ dµ
µ −λ
= −1
4π2

γε,θ
eλtR(λ, A) dλ

γ2ε,θ′
eµs
dµ
µ −λ
+
1
4π2

γ2ε,θ′
eµsR(µ, A) dµ

γε,θ
eλt
dλ
µ −λ = S(t + s)
because
1
2πi

γ2ε,θ′
eµs
dµ
µ −λ = eλt
and
1
2πi

γε,θ
eλt
dλ
µ −λ = 0.
Step 5. A is the inﬁnitesimal generator of S.
Let B be the inﬁnitesimal generator of S. By the ﬁrst step it follows that
B is an extension of A. To prove that B = A we follow the same argument as
for the fourth step of the proof of Theorem 2.5.
Finally the proof of statement (iii) of the theorem is straightforward and
it is left to the reader.
⊓⊔
Theorem 2.11. Let {S(t): t ≥0} be a strongly continuous semigroup on X,
A be its inﬁnitesimal generator, and ω ∈R and M > 0 be such that
∥S(t)∥≤Meωt,
∀t ≥0.
(2.47)
Then the following statements are equivalents:
(i) A veriﬁes the following conditions:
∃θ0 > π
2 ,
ρ(A) ⊃Sω,θ0 = {λ ∈C: | arg(λ −ω)| < θ0}
(2.48)
and
∃M > 0, ∀θ ∈]0, θ0[ ,
∥R(λ, A)∥≤
M
|λ −ω|, ∀λ ∈Sω,θ0.
(2.49)
(ii) The map
t 	→S(t): [0, ∞[ →L(X)
(2.50)
belongs to C1
]0, ∞[ ; L(X)

and
∃N > 0,
∀t > 0,
∥(A −ωI)S(t)e−ωt∥≤N
t .
(2.51)
(iii) S has an analytic extension in a sector S0,θ′, 0 < θ′ < π/2 and e−ωtS(t)
is bounded in every closed subsector of S0,θ′.

2 Linear evolution equations and strongly continuous semigroups
113
Proof. We have already shown in Theorem 2.8 that (i) =⇒(ii).
(ii) =⇒(iii). Deﬁne G(t) = e−ωtS(t) and B = A −ωI. By hypothesis G is
diﬀerentiable and for all n ≥1,
G(n)(t) = BnG(t) = [BG(t/n)]n = [G′(t/n)]n,
and G necessarily belongs to C∞
]0, ∞[ ; L(X)

. Now from (2.51)
∥G(n)(t)∥≤nnN nt−n
and the series
F(z) =
∞

n=0
(z −t)n
n!
G(n)(t)
converges in the disk
Ct =

z ∈C: |z −t| <
t
Ne

(2.52)
because the coeﬃcients an = (nN/t)n/n! are such that
an+1
an
= N
t
(n + 1)
n
	n
→Ne
t .
By the Principle of Identity for analytic functions, one sees that F is analytic
in the sector S0,θ(sin θ = 1/eN), which is the envelope of the disks Ct, t > 0.
As S is analytic in S0,θ so is S(t) = eωtG(t). For any ε, 0 < ε < θ, it is now
easy to show that F(z) is bounded in the closure of S0,θ−ε and hence on any
closed subsector of S0,θ.
(iii) =⇒(i). Again it is suﬃcient to prove that for ω = −ε, ε > 0, there exists
ω ∈R and θ0, π/2 < θ0 < π, for which assumptions A-1 and A-2 are veriﬁed
in the sector S0,θ0. The resolvent operator is given by the identity
R(λ, A) =
 ∞
0
e−λtG(t) dt,
∀λ, Re λ > 0.
By hypothesis S(z) is analytic in a sector S0,θ for some θ, 0 < θ < π/2.
Consider the sector
S0,π+θ/2 =

λ ∈C: λ ̸= 0, 0 < arg λ < θ + π
2

(2.53)
and observe that for λ = |λ|eiα in S0,(π+θ)/2 (λ ̸= 0 and 0 < α < θ + π/2) one
can get R(λ, A) by integrating along the line {te−3iθ/4: t ≥0}
R(λ, A) =
 ∞
0
e−λte−3iθ/4S(te−3iθ/4)e−3iθ/4 dt
(2.54)

114
II-1 Semigroups of Operators and Interpolation
because that line is contained in the sector S0,θ. We know that S(z) is bounded
in any subsector. Hence there exists M > 0 such that
∥R(λ, A)∥≤
 ∞
0
e−t Re[λe−3iθ/4]M dt.
(2.55)
But
β = Re[λe−3iθ/4] = |λ| cos

α −3θ
4
	
and
−3θ
4 <α−3θ
4 < π
2 −θ
4 =⇒cos

α−3θ
4
	
≥min

cos

3θ
4

, sin
θ
4

= β0 >0.
Therefore
β ≥|λ|β0 > 0
and
∥R(λ, A)∥≤
M
β0|λ|,
∀λ ∈S0,π+θ/2,
Im λ ≥0.
By repeating the same construction for Im λ ≤0 with the line {te−3iθ/4: t ≥
0}, we obtain the same inequality. Assumptions A-1 and A-2 are then veriﬁed.
⊓⊔
Deﬁnition 2.3. A strongly continuous semigroup {S(t): t ≥0} on X is said
to be analytic if it veriﬁes any one of the conditions of Theorem 2.9.
⊓⊔
The variational case (which will be studied in detail in Chapter 2) is the
following situation. There exists a Hilbert space V such that
V ⊂H algebrically and topologically, and V is dense in H.
(2.56)
The space H is identiﬁed with its dual. So we cannot identify V ′, the dual of
V , to V . On the other hand we have the sequence of embeddings
V ⊂H ≡H′ ⊂V ′,
(2.57)
each space being dense in the next one, with continuous injection. We denote
by ⟨·, ·⟩the duality between V and V ′, and by ∥· ∥the norm in V (recall that
| · | represents the norm and (·, ·) the scalar product in H).
Let a be a continuous bilinear form on V , which is V –H coercive
∃α > 0,
∃β ∈R,
∀v ∈V,
a(v, v) + β|v|2 ≥α∥v∥2.
(2.58)
Associate with a the operator A
D(A) = {v ∈V : w 	→a(v, w) is H-continuous},
(Av, w) = −a(v, w),
∀v ∈D(A),
∀w ∈V.
(2.59)

2 Linear evolution equations and strongly continuous semigroups
115
In the variational literature it is customary to use −A instead of A. The choice
that we have made in (2.59) is more in line with semigroup theory.
We shall check that A generates an analytic semigroup and, thus, that
there exists a unique classical solution of (2.1). Notice that the change of
variable
yβ(t) = e−βty(t)
in the equation
dy
dt = Ay,
y(0) = y0,
yields
dyβ
dt = (A −βI)yβ,
yβ(0) = y0
and that
⟨(−A + βI)v, v⟩V = a(v, v) + β|v|2 ≥α∥v∥2.
So it is suﬃcient to prove the result for β = 0.
Theorem 2.12. Assume (2.58) holds; then A generates an analytic semi-
group {S(t)} in H such that
∥S(t)∥≤eβt.
(2.60)
Proof. Cf. H. Tanabe [1, §5.4 and §3.6, Theorem 3.6.1].
⊓⊔
2.8 Diﬀerentiable semigroups
Deﬁnition 2.4. Let S be a strongly continuous semigroup in X. We say that
S is diﬀerentiable at t0 > 0 if the limit
S′(t0) = lim
h↘0
1
h

S(t0 + h) −S(t0)

(2.61)
exists in L(X).
⊓⊔
Proposition 2.6. Let S be a strongly continuous semigroup with inﬁnitesi-
mal generator A. Assume that S is diﬀerentiable at t0. Then the following
statements hold:
(i) S(t0)x ∈D(A) for any x ∈X and S′(t0) = AS(t0),
(ii) S is diﬀerentiable at any t ≥t0,
(iii) for any n ∈N, S is n times diﬀerentiable at t ≥nt0 and
S(n)(t) = AnS(t),
t ≥nt0.

116
II-1 Semigroups of Operators and Interpolation
Proof. (i) Due to Proposition 2.1 we have
S′(t0)x = AS(t0)x
for all x ∈D(A).
Let now x ∈X and {xn} ⊂D(A) be such that xn →x in X. As n →∞we
have
AS(t0)xn = S′(t0)xn →S′(t0)x
and
S(t0)xn →S(t0)x.
As A is closed S(t0)x ∈D(A) and AS(t0)x = S′(t0)x.
(ii) For any t > t0 we have
lim
h↘0
1
h

S(t+h)−S(t)

= lim
h↘0
1
h[S(t0+h)−S(t0)]S(t−t0) = S′(t0)S(t−t0).
(iii) We only consider the case n = 2; the case n > 2 is treated analogously
by recurrence. We have
lim
h↘0
1
h

S′(2t0 + h) −S′(2t0)

= lim
h↘0 AS(t0) 1
h

S(t0 + h) −S(2t0)

= AS(t0)S′(t0) = A2S(2t0)
and S(2)(2t0) = A2S(2t0). For t > 2t0 we proceed as in part (ii).
⊓⊔
We shall now study some spectral properties of diﬀerentiable semigroups.
Proposition 2.7 (A. Pazy [1]). Let S be a strongly continuous semigroup
and assume that there exists a constant M > 0 such that for all t ≥0,
∥S(t)∥≤M. If S is diﬀerentiable at t0 > 0, then the resolvent set ρ(A) of its
inﬁnitesimal generator A contains the set
Σ = {λ ∈C: Re λ ≤0, | Im λ|et0 Re λ ≥2∥S′(t0)∥}.
(2.62)
Moreover there exists N > 0 such that
∥R(λ, A)∥≤N(1 + |λ|),
λ ∈Σ.
(2.63)
Proof. For any t ≥t0 we set
Bλ(t)x =
 t
0
eλ(t−s)S(s)x ds,
λ ∈C, x ∈X.
(2.64)
We have
B′
λ(t)x = S(t)x + λ
 t
0
eλ(t−s)S(s)x ds,
(2.65)
Bλ(t)(λ −A)x = eλtx −S(t)x.
(2.66)
By diﬀerentiating (2.66) with respect to t, we obtain
B′
λ(t)(λ −A)x = λeλtx −S′(t)x = λeλt

I −1
λe−λtS′(t)
	
x.
(2.67)

2 Linear evolution equations and strongly continuous semigroups
117
−2K
2K
y = 2Ke−x t0
y = −2Ke−x t0
σ(A)
y = Im λ
x = Re λ
Fig. 1.3. Set Σ contained in ρ(A) with K = ∥S′(t0)∥.
But if we choose t and λ such that
!!!!
1
λe−λtS′(t)
!!!! ≤1
2,
(2.68)
then [I −1/λe−λtS′(t)]−1 exists and is bounded. From (2.67) and (2.65)
R(λ, A)x = 1
λe−λt

I −1
λe−λtS′(t)
	−1
B′
λ(t)x
=

I −1
λe−λtS′(t)
	−1 1
λe−λtS(t)x +
 t
0
e−λsS(s)x ds

.
(2.69)
The right-hand side of the above identity is bounded and we conclude that for
t and λ verifying (2.68) λ ∈ρ(A). But for t = t0 and λ ∈Σ, (2.68) is veriﬁed.
Hence Σ ⊂ρ(A).
It remains to verify inequality (2.63). Set K = ∥S′(t0)∥. For λ ∈Σ
1
|λ| exp(−t0 Re λ) ≤K
2 .
(2.70)
From (2.69) with t = t0
∥R(λ, A)∥≤2M
|λ| exp(−t0 Re λ) + 2M
 t0
0
exp(−Re λs) ds.
But for all λ ∈Σ, Re λ ≤0 and
∥R(λ, A)∥≤2M exp(−t0 Re λ)
 1
|λ| + t0
	
.

118
II-1 Semigroups of Operators and Interpolation
−2K
2K
γ1
γ3
γ2
Im λ
Re λ
Fig. 1.4. Path of integration γ1 ∪γ2 ∪γ3.
Therefore from (2.70) we ﬁnally ﬁnd
∥R(λ, A)∥≤MK[1 + t0|λ|].
⊓⊔
We now prove the converse.
Theorem 2.13 (A. Pazy [1]). Let A be the inﬁnitesimal generator of a
strongly continuous semigroup S for which there exists a constant M > 0
such that, for all t > 0, ∥S(t)∥≤M. Moreover assume that there exist three
positive constants a, K, N, such that
ρ(A) ⊃Σ = {λ ∈C: | Im λ|ea Re λ ≥2K},
(2.71)
∥R(λ, A)∥≤N(1 + |λ|),
λ ∈Σ.
(2.72)
Then S is diﬀerentiable for t > 3a and we have for t > 2a
S(t) =
3

i=1
Gi(t) =
3

i=1
1
2πi

γi
eλtR(λ, A) dλ,
(2.73)
where γi is deﬁned by
γ1 = {λ ∈C: Im λ = 2Ke−a Re λ},
γ2 =

λ ∈C: λ = 2Ke−iθ, θ ∈

−π
2 , π
2
	
,
γ3 = {λ ∈C: Im λ = −2Ke−a Re λ}.
(2.74)
Proof. Set

2 Linear evolution equations and strongly continuous semigroups
119
˜S(t) =
3

i=1
˜Si(t),
˜Si(t) =
1
2πi

γi
eλtR(λ, A) dλ,
i = 1, 2, 3.
(2.75)
The integrals in (2.75) are meaningful for t > 2a. To check this for the ﬁrst
one, set λ = x + iy. We have
˜S1(t) =
1
2πi
 −∞
0
etx+ityR(x + iy, A)(1 −2iKae−ax) dx,
(2.76)
and for y = 2Ke−ax, we obtain
∥˜S1(t)∥= N
2π
 −∞
0
etx(1 + |x| + 2Kae−ax) dx,
(2.77)
which is meaningful if t > 2a.
Let us now show that ˜S1(t) is diﬀerentiable for t > 3a. In fact we have
d
dt
˜S1(t) =
1
2πi
 −∞
0
(x + 2Kie−ax)etx+ity
R(x + 2Kie−ax, A)(1 −2iKae−ax) dx
(2.78)
from which
!!!!
d
dt
˜S1(t)
!!!! = N
2π
 −∞
0
etx[1 + |x| + 2Ke−ax]
[|x| + 2Ke−ax][1 + 2Kae−ax] dx,
(2.79)
which is meaningful for t > 3a.
It remains to show that S = ˜S. By a direct computation (using several
times the argument employed to prove (2.62)) it is not diﬃcult to check that
˜S′(t)x = A ˜S(t)x, for any x ∈D(A3).
As D(A3) is dense in X, by the uniqueness of the solution of the Cauchy
problem (cf. Proposition 2.6), it follows that S = ˜S.
⊓⊔
2.9 Spectral determining growth condition
Let A be the inﬁnitesimal generator of a strongly continuous semigroup S of
type ω0(S). We set
s(A) =

sup{Re λ: λ ∈σ(A)},
if σ(A) ̸= Ø,
−∞,
if σ(A) = Ø.
(2.80)
If s(A) = ω0(S) we say that A veriﬁes the spectral determining growth condi-
tion or spectrum determined growth assumption. In this case, by the Corollary
to Proposition 2.2, we can determine the asymptotic behavior of ∥S(t)∥by
the knowledge of the spectrum σ(A) of A.
We start with a general result.

120
II-1 Semigroups of Operators and Interpolation
Proposition 2.8. Let A be the inﬁnitesimal generator of a strongly continu-
ous semigroup {S(t)} on a Banach space X and let ω0(S) denote the type of
{S(t)}. Then
∀t > 0,
{eλt : λ ∈σ(A)} ⊂σ

S(t)

,
(2.81)
and
s(A) ≤ω0(S).
(2.82)
Proof. Given an arbitrary λ, consider the new semigroup
Sλ(t)x = e−λtS(t)
with inﬁnitesimal generator A −λI. For all x ∈D(A),
[S(t) −eλtI]x = eλt[Sλ(t) −I]x,
[S(t) −eλtI]x = eλt
 t
0
S(r)e−λr[A −λI]x dr,
(2.83)
[S(t) −eλtI]x = eλt[A −λI]
 t
0
S(r)e−λrx dr.
(2.84)
If λ ∈σP (A), then [A −λI] is not one-to-one and necessarily, from (2.83),
[S(t) −eλtI] is not one-to-one. Hence eλt ∈σP

S(t)

, ∀t ≥0. If λ ∈σR(A) ∪
σC(A), then [A −λI] is one-to-one but R(A −λI) ̸= X and necessarily, from
(2.84), R([S(t)−eλtI]) ̸= X. So eλt ∈σP

S(t)

∪σR

S(t)

∪σC

S(t)

, ∀t ≥0.
This proves (2.81).
To verify (2.82), we use the spectral radius theorem
lim
n→∞∥S(1)n∥1/n = sup

|λ|: λ ∈σ

S(1)
 
.
In particular from (2.81)
sup{|eλ|: λ ∈σ(A)} ≤sup{|λ|: λ ∈S(1)} = lim
n→∞∥S(n)∥1/n.
Note that |eλ| = eRe λ and that the direction of the above inequality is not
changed by taking the ln of both sides:
s(A) =
sup
λ∈σ(A)
Re λ ≤lim
n→∞
ln ∥S(n)∥
n
= ω0(S).
This proves (2.82) and concludes the proof of the proposition.
⊓⊔
In general inequality (2.82) is not true in the other direction except for
special classes of semigroups.
Proposition 2.9 (R. Triggiani [2]). Let S be an analytic semigroup. Then
ω0(S)=s(A).

2 Linear evolution equations and strongly continuous semigroups
121
Proof. Assume that (2.41) holds and that ω0(S) = 0. This is not a restriction
because if ω0(S) ̸= 0 we can change S(t) to e−ω0(S)tS(t). Suppose now, by
contradiction, that there exists ε > 0 such that s(A) = −2ε. By assumptions
A-1 and A-2 in Assumption A there exists θ ∈]π/2, π[ such that ρ(A) contains
the sector Sε,π/2 and by Cauchy’s theorem
S(t) =
1
2πi

γ+
eλtR(λ, A) dλ +
1
2πi

γ−
eλtR(λ, A) dλ,
where
γ± = {λ ∈C : λ = −ε + ρe±iη, ρ ≥0}
and η is chosen such that π/2 < η < θ and ρ(A) ⊃S−ε,η. Clearly ∥R(λ, A)∥is
bounded in the sector S−ε,η, and moreover, its behavior at inﬁnity is as 1/|λ|.
Now it is easy to check that ∥S(t)∥≤constant e−εt, which is a contradiction
since ω0(S) = 0.
⊓⊔
The most general available results at the moment seem to be the following
ones.
Proposition 2.10.
(i) If X is a Banach space and S is a strongly continu-
ous semigroup that is eventually uniformly continuous, that is
∃t0 > 0,
t 	→S(t): [t0, ∞[ →L(X) is continuous,
then s(A) = ω0(S).
(ii) If X is a Banach lattice and S(t0) is compact for some t0 > 0, then
s(A) = ω0(S).
The proof of (i) can be found in G. Greiner and R. Nagel [1, p. 87] and
the proof of (ii) in F. Neurander [1, p. 205].
Corollary 2.5. If X is a Banach space, the condition s(A) = ω0(S) is veriﬁed
in any of the following situations:
(i) A is bounded,
(ii) ∃t0 > 0, S(t0) is compact,
(iii) S is a diﬀerentiable semigroup,
(iv) S is nilpotent,
(v) S is analytic.
Remark 2.9. The property s(A) = ω0(S) was known for X a Hilbert space
and A bounded (see I. Daleckii and M. Krein [1]). It was proved in 1975
for analytic semigroups by R. Triggiani [2, comments following Proposi-
tion 2.2] and for eventually compact semigroups (that is, there exists some
t0 > 0 for which S(t) is compact for all t ≥t0) by J. Zabczyk [2]. This last
result was a generalization of what was already known for delay systems (cf.
J. K. Hale [2]). The generalization to semigroups that are eventually uni-
formly continuous seems to be due to R. Nagel [1] and L. A. Monauni [1]

122
II-1 Semigroups of Operators and Interpolation
in 1980–1981. For other conditions using the spectral mapping theorem see
E. Hille and R. S. Phillips [1, Section 16.7]. Several counterexamples are
available (for instance E. Hille and R. S. Phillips [1, Section 23.16] where
s(A) = −∞and ω0(S) is an arbitrary real number). This example is con-
structed in an indirect way using fractional integration. Another example has
been given by J. Zabczyk [2] where he shows that for any two real numbers
a < b, one can construct a complex group S(t) with generator A such that
s(A) = a < b = ω0(S).
One question that remains open is to determine whether a similar example
can be constructed for a real group or semigroup.
⊓⊔
2.10 Examples of semigroups
2.10.1 Parabolic equations
Let H be a Hilbert space and A a self-adjoint closed operator in H with dense
domain D(A) in H. We assume that there exists ω ∈R such that
⟨Ax, x⟩≤ω|x|2,
∀x ∈D(A).
(2.85)
By Lumer–Phillips’ theorem, A is the inﬁnitesimal generator of a strongly
continuous semigroup S in H such that
∥S(t)∥≤eωt,
t ≥0.
(2.86)
Proposition 2.11. S is an analytic semigroup.
Proof. First notice that, as A −ωI is self-adjoint negative, then σ(A) ⊂
]−∞, ω]. Then for any λ ∈C\]−∞, ω] and any y ∈H there exists x ∈D(A)
such that
λx −Ax = y.
(2.87)
Set λ = ω + ρeiθ. Then multiply the identity ρeiθx −(A −ω)x = y by e−iθ/2x
and take the real part. We obtain
cos θ
2|x|2 −cos θ
2⟨(A −ωI)x, x⟩= Re[e−iθ/2⟨x, y⟩].
From (2.85) it follows that |x| ≤(ρ cos θ/2)−1|y| so that
∥R(λ, A)∥≤
1
|λ −ω| cos θ/2.
So pick a sector Sω,θ0, for some θ0, π/2 < θ0 < π, and θ, 0 ≤θ < θ0. Then
cos(θ/2) > cos(θ0/2) and
∥R(λ, A)∥≤
1
|λ −ω| cos(θ0/2)
for all λ
in Sω,θ0.
Thus S is an analytic semigroup.
⊓⊔

2 Linear evolution equations and strongly continuous semigroups
123
Consider now an example.
Let Ωbe an open set of Rn with regular boundary ∂Ω. Consider the initial
value problem for t ≥0
∂
∂tu(t, x) =
n

j,k=1
∂
∂xk

ajk(x) ∂
∂xj
u(t, x)

,
x ∈Ω,
u(t, x) = 0,
x ∈∂Ω,
u(0, x) = u0(x),
x ∈Ω,
(2.88)
where ajk are real continuous functions in ¯Ω. Assume that there exists ν > 0,
such that (strong ellipticity)
n

j,k=1
ajkξjξk ≥ν|ξ|2,
ξ ∈Rn.
(2.89)
Set H = L2(Ω) and let A be the linear operator in H deﬁned as
D(A) = H2(Ω) ∩H1
0(Ω),
(Au)(x) =
n

j,k=1
∂
∂xk

ajk(x) ∂
∂xj
u(t, x)

.
(2.90)
Then A is self-adjoint and hypothesis (2.85) is fulﬁlled with ω = λ0, where λ0
is the ﬁrst eigenvalue of A (see for instance S. Agmon [2]).
By setting u(t) = u(t, ·), problem (2.88) can be written as
u′ = Au,
u(0) = u0
and we can solve it by Proposition 2.1. We shall give details in §3 for a more
general situation.
Remark 2.10. If A is an elliptic operator of order 2m, with general boundary
conditions, then, under suitable hypotheses, A generates an analytic semi-
group (see S. Agmon [1]).
⊓⊔
2.10.2 Schr¨odinger equation
Let Ωbe as in the previous examples. Consider for t ≥0 the problem
∂
∂tu(t, x) = i
n

j,k=1
∂
∂xk

ajk(x) ∂
∂xj
u(t, x)

,
x ∈Ω,
u(t, x) = 0,
x ∈∂Ω,
u(0, x) = u0(x),
x ∈Ω,
(2.91)

124
II-1 Semigroups of Operators and Interpolation
then, setting B = iA (where A is deﬁned by (2.90)), we can write problem
(2.91) in the form
u′ = Bu,
u(0) = u0.
(2.92)
By Stone’s Theorem 2.9, B generates a contraction semigroup and problem
(2.91) has a solution.
2.10.3 Wave equation
Let Λ be a strictly positive self-adjoint operator in the Hilbert space K. Con-
sider the Hilbert space H = D(
√
Λ) ⊕K and denote by
Y =
 y
y1
	
the generic element of H. The inner product in H is deﬁned by
⟨Y, Z⟩H = ⟨
√
Λy,
√
Λx⟩K + ⟨y1, z1⟩K.
(2.93)
Let A be the linear operator in H deﬁned as follows
D(A) = D(Λ) ⊕D(
√
Λ),
AY =

y1
−Λy
	
=

0 1
−Λ 0
	  y
y1
	
.
(2.94)
Proposition 2.12. A is the inﬁnitesimal generator of a strongly continuous
semigroup of contractions S on H, and S is given by
S(t) =

cos(
√
Λt )
Λ1/2 sin(
√
Λt)
−Λ1/2 sin(
√
Λt)
cos(
√
Λt)
	
.
(2.95)
Moreover A∗= −A.
Finally assume that σ(Λ) consists of a sequence {λk} of eigenvalues and
that {ek} is a corresponding complete orthonormal set of eigenvectors. Then
we have
σ(A) = {±i
%
λk},
AE±
k = ±i
%
λkE±
k ,
k ∈N,
(2.96)
where E±
k = (ek, ±√λk ek).
Proof. Let Re λ > 0, Y ∈H, then the equation λX −AX = Y is equivalent
to the system

λx −x1 = y,
λx1 + Λx = y1,
whose solution is given by

x = λ(λ2 + Λ)−1y + (λ2 + Λ)−1y1,
x1 = −Λ(λ2 + Λ)−1y + λ(λ2 + Λ)−1y1.

2 Linear evolution equations and strongly continuous semigroups
125
Thus λ ∈ρ(A) and
R(λ, A) =

λ(λ2 + Λ)−1 (λ2 + Λ)−1
−Λ(λ2 + Λ)−1 λ(λ2 + Λ)−1
	
.
(2.97)
Formula (2.95) and the identity −A = A∗are easily veriﬁed as well as the last
statement.
⊓⊔
We now give an example. Let Ωbe a bounded set of Rn with regular
boundary ∂Ω, and let {ajk} be real continuously diﬀerentiable functions in ¯Ω
such that (2.89) holds. Consider the following problem for t ≥0:
∂2
∂t2 u(t, x) =
n

j,k=1
∂
∂xk

ajk(x) ∂
∂xj
u(t, x)

,
x ∈Ω,
u(t, x) = 0,
x ∈∂Ω,
u(0, x) = u0(x),
∂
∂tu(0, x) = u1(x),
x ∈Ω.
(2.98)
Let Λ be the linear operator in K = L2(Ω) deﬁned by
D(Λ) = H2(Ω) ∩H1
0(Ω),
(Λu)(x) = −
n

j,k=1
∂
∂xk

ajk(x) ∂
∂xj
u(t, x)
	
.
(2.99)
Then Λ is self-adjoint and strictly positive. Setting
U(t) =
 u(t, ·)
∂
∂tu(t, ·)

,
problem (2.98) is equivalent to
U ′ = AU,
U(0) = U0 =

u0
u1
	
(2.100)
and can be solved by Proposition 2.12.
2.10.4 Delay equations
Let C and D be given in L(Cn) and consider the problem
z′(t) = Cz(t) + Dz(t −r),
t ≥0,
z(0) = h0 ∈Cn,
z(θ) = h1(θ),
a.e. θ ∈[−r, 0],
(2.101)
where h1 ∈L2(−r, 0; Cn) and r > 0 is the delay.

126
II-1 Semigroups of Operators and Interpolation
Problem (2.101) can be easily solved in successive steps on each time in-
terval of length r. We have
z(t) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
h1(t)
if t ∈[−r, 0[,
etCh0 +
 t
0 e(t−s)CDh1(s −r) ds
if t ∈[0, r],
e(t−r)Cz(r) +
 t
r
e(t−s)CDz(s −r) ds
if t ∈[r, 2r],
(2.102)
and so on.
We consider the Hilbert space H = Cn × L2(−r, 0; Cn) and denote by
h = (h0, h1) its generic element. We deﬁne a semigroup S on H by setting
S(t)h = (z(t), zt),
h = (h0, h1) ∈H,
(2.103)
where z is the solution of (2.101) and zt(θ) = z(t + θ) for θ ∈[−r, 0]. The
semigroup properties S(t + s) = S(t)S(s), S(0) = I are easily checked.
Proposition 2.13. S is a strongly continuous semigroup in H that is diﬀer-
entiable for any t ≥r. The inﬁnitesimal generator A of S is given by
D(A)={h = (h0, h1)∈H : h1 ∈W 1,2(−r, 0; Cn), h0 =h1(0)}
(2.104)
Ah = (Ch1(0) + Dh1(−r), h′
1)
and we have
σ(A) = {λ ∈C : det(λ −C −e−λrD) = 0}.
(2.105)
Proof. In several steps.
Step 1. S is strongly continuous.
For any h = (h0, h1) ∈H, we have S(t)h −h = (z(t) −h0, zt −h1). But if
−r < θ < −t
(zt −h1)(θ) = h1(t + θ) −h1(θ),
and if θ ≥−t
(zt −h1)(θ) = e(t+θ)Ch0 +
 t+θ
0
e(t+θ−s)CDh1(s −r)ds −h1(θ).
Thus we can easily check that S(t)h →h as t ≥0.
Step 2. S(t) is diﬀerentiable for t ≥r.
Let t ∈[r, 2r], h = (h0, h1); then S(t)h = (z(t), zt) where
z(t) = etCh0 +
 t
0
e(t−s)CDz(s −r) ds,
zt(θ) = e(t+θ)Ch0 +
 t+θ
0
e(t+θ−s)CDz(s −r) ds.

2 Linear evolution equations and strongly continuous semigroups
127
It follows that S′(t)h = (z′(t), (zt)′), where
z′(t) = Cz(t) + Dz(t −r),
(zt)′(θ) = Czt(θ) + Dz(t + θ −r).
Thus S is diﬀerentiable for t ≥r.
Step 3. Let B be the inﬁnitesimal generator of S(t). We ﬁrst show that A ⊂B.
Indeed A is given by formula (2.104).
Let h = (h0, h1) belong to the right-hand side of (2.104). Since h1 ∈
W 1,2(−r, 0; Cn) and h1(0) = h0 = z(0), the new function
w(t) =

z(t),
t ∈[0, T ],
h1(t),
t ∈[−r, 0]
belongs to W 1,2(−r, T ; Cn). For each t ∈[0, T ], let
wt(θ) = w(t + θ),
θ ∈[−r, 0].
Then
S(t)h −h
t
= (w(t), wt) −(h0, h1)
t
=
w(t) −w(0)
t
, wt −w0
t

→(w′(0), w′
0) = (Ch1(0) + Dh1(−r), h′
1)
in Cn × L2(−r, 0; Cn). Hence we have proved that if h ∈D(A) then (S(t)h −
h)/t goes to Ah as t goes to 0, Therefore B ⊃A. Finally we can check that
for λ suﬃciently large, we have (λ −A)

D(A)

= H so that A = B.
Step 4. Equation (2.105) holds.
It is suﬃcient to prove that λ ∈ρ(A) if and only if the matrix
∆(λ) = λI −C −eλrD
is invertible. The complex number λ belongs to ρ(A) if and only if
∀k = (k0, k1) ∈H,
∃h ∈D(A), such that λh −Ah = k.
(2.106)
Given k = (k0, k1) ∈H, the equation λh−Ah = k is equivalent to the system
λh0 −Ch0 −Dh1(−r) = k0,
(2.107)
λh1 −h′
1 = k1,
h1(0) = h0.
(2.108)
This is also equivalent to
h1(θ) = eλθh0 +
 0
θ
eλ(θ−s)k1(s) ds,
θ ∈[−r, 0],
(2.109)
λh0 −Ch0 −Dh1(−r) = k0.

128
II-1 Semigroups of Operators and Interpolation
By substituting the value of h1(−r) given by (2.109) in the last equation, we
obtain
(λ −C −e−λrD)h0 = k0 + D
 0
−r
e−λ(r+s)k1(s) ds.
(2.110)
Thus, if (2.106) is true, then for each k = (k0, k1) in H, (2.110) has a unique
solution h0. Therefore ∆(λ) is invertible. Conversely if ∆(λ) is invertible, then
for each k there exists a unique h0 given by (2.109) and a unique h1 given by
(2.109) such that (2.106) be veriﬁed.
⊓⊔
The following corollary is an immediate consequence of Proposition 2.9.
Corollary 2.6. The type of S is given by
ω0(S) = s(A) = sup{Re λ: det(λ −C −e−λrD) = 0}.
(2.111)
Remark 2.11. For more general examples and results for delay systems, see
Chapter 4.
⊓⊔
3 Nonhomogeneous linear evolution equations
3.1 Setting of the problem and deﬁnitions
In this section A represents the inﬁnitesimal generator of a strongly continuous
semigroup S in X. We shall consider the following initial value problem:

u′(t) = Au(t) + f(t),
t ∈[0, T ],
u(0) = x,
(3.1)
for x ∈X and f ∈L1(0, T ; X) and approximations un to its solution u given
by the following approximating initial value problem:

u′
n(t) = Anun(t) + f(t),
un(0) = x,
(3.2)
where An = n2R(n, A)−nI is the Yosida approximation of A. As An ∈L(X),
problem (3.2) has a unique solution given by
un(t) = etAnx +
 t
0
e(t−s)Anf(s) ds.
(3.3)
We now give several deﬁnitions of a solution of problem (3.1) for x ∈X
and f ∈L1(0, T ; X). The concept of solution for rougher data (x, f) will be
discussed in Chapters 2 and 3.

3 Nonhomogeneous linear evolution equations
129
Deﬁnition 3.1. (i) u is a strict solution of problem (3.1) in Lp(0, T ; X) if u
belongs to W 1,p(0, T ; X) ∩Lp
0, T ; D(A)

and

u′(t) = Au(t) + f(t)
a.e. in [0, T ],
u(0) = x.
(ii) u is a strong solution of problem (3.1) in Lp(0, T ; X) if there exists a
sequence {uk} in W 1,p(0, T ; X) ∩Lp
0, T ; D(A)

such that
uk →u,
and
u′
k −Auk →f
in Lp(0, T ; X),
uk(0) →x
in X, as k →∞.
(3.4)
(iii) u is a classical solution of problem (3.1) in Lp(0, T ; X) if for all ε > 0
u ∈W 1,p(ε, T ; X) ∩Lp
ε, T ; D(A)

∩C([0, T ]; X) and

u′(t) = Au(t) + f(t)
a.e. in [0, T ],
u(0) = x.
(iv) The function
u(t) = S(t)x +
 t
0
S(t −s)f(s) ds
is called the mild solution of problem (3.1) if u belongs to C([0, T ]; X).
(v) u is a weak solution of problem (3.1) if u belongs to Lp(0, T ; X), for
all k in D(A∗), ⟨k, u(·)⟩X belongs to W 1,p(0, T ), and for almost all t in [0, T ]
and k in D(A∗)
⎧
⎨
⎩
d
dt⟨k, u(t)⟩= ⟨A∗k, u(t)⟩+ ⟨k, f(t)⟩,
u(0) = x,
where A∗: D(A∗) →X′ is the dual operator of A.
⊓⊔
Remark 3.1. The notion of weak solution for arbitrary Banach spaces was
introduced by J. Ball [1] with the diﬀerence that he requires that u belongs
to C([0, T ]; X) rather than Lp(0, T ; X).
⊓⊔
When X is a reﬂexive Banach space, strong solutions can be characterized
in a slightly diﬀerent way. Recall that for a reﬂexive Banach space, the family
of adjoint transformations {S∗(t) ∈L(X′): t ≥0} is a strongly continuous
semigroup on X′ and that the domain D(A∗) of its inﬁnitesimal generator
A∗is dense in X′. The injection D(A∗) ⊂X′ is continuous and dense when
D(A∗) is endowed with the graph norm topology deﬁned by the norm
|x|D(A∗) = |x|X∗+ |A∗x|X′.
As a result the injection X ≡X′′ ⊂D(A∗)′ is also continuous and dense since
the elements of the bidual X′′ of X can be identiﬁed with those of X.

130
II-1 Semigroups of Operators and Interpolation
Remark 3.2. When X is a reﬂexive Banach space A∗is the generator of the
adjoint semigroup S∗(t) and the notion of weak solution is equivalent to the
existence of a function u in W 1,p(0, T ; D(A∗)′) ∩Lp(0, T ; X) such that

u′ = (A∗)∗u + f
in Lp(0, T ; D(A∗)′),
u(0) = x
in X.
This is the usual setup for parabolic equations in the variational framework,
where the operator −A arises from a variational problem.
⊓⊔
3.2 Existence and uniqueness of a strong solution
Proposition 3.1. Assume that x ∈X and f ∈Lp(0, T ; X). Then problem
(3.1) has a unique strong solution u in Lp(0, T ; X). Moreover u belongs to
C([0, T ]; X) and is given by the formula
u(t) = S(t)x +
 t
0
S(t −s)f(s) ds.
(3.5)
Finally if un is the solution of problem (3.2), then
un →u in Lp(0, T ; X) as n →∞.
Proof. Let u be the function deﬁned by (3.5). Clearly u is continuous. To
check that u is a strong solution of (3.1), set
uk(t) = kR(k, A)u(t),
fk(t) = kR(k, A)f(t),
xk = kR(k, A)x.
By (3.5) it follows that
uk(t) = S(t)xk +
 t
0
S(t −s)fk(s) ds;
by diﬀerentiating with respect to t we see that uk is a strict solution of the
problem

u′
k(t) = Auk(t) + fk(t),
uk(0) = xk.
By (2.28) in Theorem 2.5 it follows that uk →u, fk →f in Lp(0, T ; X) and
xk →x in X. Thus u is a strong solution.
We now prove uniqueness. Let u be a strong solution of (3.1) and let {uk}
be a sequence such that (3.4) holds. We set fk = u′
k −Auk and xk = uk(0).
By integrating the equation
d
ds

S(t −s)uk(s)

= S(t −s)fk(s),
s ∈[0, t]
between 0 and t and letting k go to inﬁnity, we ﬁnd that u is given by (3.5).

3 Nonhomogeneous linear evolution equations
131
As for the last statements we have
|u(t) −un(t)| ≤|S(t)x −etAnx| +
 t
0
|S(t −s)f(s) −e(t−s)Anf(s)| ds
and the conclusion follows from Theorem 2.5.
⊓⊔
Remark 3.3. In the literature a strong solution is generally called a mild so-
lution. We shall also use this terminology in Parts IV and V.
⊓⊔
Proposition 3.2. Given x ∈X and f ∈Lp(0, T ; X), 1 ≤p < ∞, problem
(3.1) has a unique weak solution u that coincides with the strong solution given
by (3.5).
Existence.
Let u be given by (3.5). By deﬁnition u ∈C([0, T ]; X) ⊂Lp(0, T ; X). Given
k in D(A∗) and φ in D(]0, T [), the vectorial distributional derivative of u is
given by
−
 T
0
⟨k, u(t)⟩D(A∗)φ′(t) dt = −
 T
0
&
k, S(t)k +
 t
0
S(t −s)f(s) ds
'	
φ′(t) dt
= −
 T
0
⟨k, S(t)k⟩φ′(t) dt
−
 T
0
ds
 T
s
dt⟨k, S(t −s)f(s)⟩φ′(t).
Now for all x in D(A) and k in D(A∗)
d
dt⟨k, S(t)x⟩= ⟨k, AS(t)x⟩X = ⟨A∗k, S(t)x⟩X
and by density of D(A) in X, this extends to all x in X. In view of this
technical result, we can now integrate by parts in the previous equations
−
 T
0
⟨k, S(t)k⟩φ′(t) dt = −
 T
0
⟨A∗k, S(t)x⟩φ(t) dt
and
−
 T
s
⟨k, S(t −s)f(s)⟩φ′(t) dt =
 T
s
⟨A∗k, S(t −s)f(s)⟩φ(t) dt + ⟨k, f(s)⟩φ(s).
So ﬁnally
−
 T
0
⟨k, u(t)⟩D(A∗)φ′(t) dt =
 T
0
⟨A∗k, u(t)⟩+ ⟨k, f(t)⟩]φ(t) dt.
As u ∈C([0, T ]; X) and f ∈Lp(0, T ; X), ⟨k, u(·)⟩belongs to W 1,p(0, T ) and
for each t in [0, T ]
d
dt⟨k, u(t)⟩= ⟨A∗k, u(t)⟩+ ⟨k, f(t)⟩.
So u is a weak solution of problem (3.1).

132
II-1 Semigroups of Operators and Interpolation
Uniqueness.
It is suﬃcient to prove that u = 0 when x = 0 and f = 0. By deﬁnition of a
weak solution for all k in D(A∗) and t in [0, T ]
d
dt⟨k, u(t)⟩= ⟨A∗k, u(t)⟩
and
lim
t↘0⟨k, u(t)⟩= 0.
Therefore
⟨k, u(t)⟩=
&
A∗k,
 t
0
u(s) ds
'
= ⟨A∗k, z(t)⟩,
where
z(t) =
 t
0
u(s) ds.
We need the following lemma.
Lemma 3.1 (Cf. S. Goldberg [1, p. 127]). Assume that A is a closed linear
densely deﬁned operator on X. Let u and z in X satisfy
⟨k, u⟩= ⟨A∗k, z⟩,
∀k ∈D(A∗).
Then z ∈D(A) and u = Az.
In view of the lemma for each t, z(t) ∈D(A) and u(t) = Az(t). Moreover
since u ∈Lp(0, T ; X), z ∈Lp
0, T ; D(A)

and
dz
dt (t) = u(t) = Az(t) ∈Lp(0, T ; X).
Therefore z ∈Lp
0, T ; D(A)

∩W 1,p(0, T ; X) and
⎧
⎨
⎩
dz
dt (t) = Az(t),
t ∈[0, T ],
z(0) = 0.
So z is a strict and hence a strong solution. By Proposition 3.1 this solution
is unique and equal to zero. The uniqueness now follows from the fact that
u(t) = Az(t) = 0 for all t in [0, T ].
Remark 3.4. The key points of the above proof have been borrowed from
J. Ball [1]. We see that for initial conditions x in X and right-hand-sides f in
Lp(0, T ; X) the two deﬁnitions of weak solutions coincide. In fact J. Ball [1]
proved the following theorem for a closed linear densely deﬁned operator A
on X: For each x ∈X, there exists a unique weak solution u(t) of (3.1)

u ∈C(0, T ]; X)

satisfying u(0) = x if and only if A is the generator of a
strongly continuous semigroup {S(t)} on X.
⊓⊔

3 Nonhomogeneous linear evolution equations
133
Remark 3.5. The last two propositions show the equivalence of a solution in
Lp(0, T ; X) and a strong solution that is the limit in C([0, T ]; X) of strict
solutions. For reﬂexive Banach spaces the application
u →

u′ −(A∗)∗u, u(0)

: W 1,p
0, T ; D(A∗)′
∩C([0, T ]; X) →Lp(0, T ; D(A∗)′) × X
is injective and invertible on the subspace
Lp(0, T ; X) × X
of Lp(0, T ; D(A∗)′) × X.
So it is not an isomorphism, but its image is at least dense.
⊓⊔
3.3 Existence of a strict solution
In order to obtain a strict solution of problem (3.1), we need more regularity
in x and f.
Proposition 3.3.
(i) If x ∈D(A) and f ∈Lp
0, T ; D(A)

then prob-
lem (3.1) has a unique strict solution that belongs to W 1,p(0, T ; X) ∩
C

[0, T ]; D(A)

.
(ii) If x ∈D(A) and f ∈W 1,p(0, T ; X), then problem (3.1) has a unique
strict solution that belongs to C1([0, T ]; X) ∩C

[0, T ]; D(A)

.
Proof. (i) Let u be the strong solution of problem (3.1) and let the solution un
of (3.2) be its associated approximating sequence. By hypotheses the problem

v′(t) = Av(t) + Af(t),
v(0) = Ax
(3.6)
has a strong solution v ∈C([0, T ]; X). Let vn be the solution of the problem

v′
n(t) = Anvn(t) + Anf(t),
vn(0) = Anx.
(3.7)
Clearly, vn = Anun. By Proposition 3.1, we have
un →u,
Anun = vn →v
in C([0, T ]; X),
that is,
nR(n, A)un →u
and
Anun = AnR(n, A)un →v
in C([0, T ]; X).
Since A is closed this implies that v = Au. Then u ∈C

[0, T ]; D(A)

. More-
over
un ∈W 1,p(0, T ; X)
and
u′
n = Anun + f →v
in C([0, T ]; X).

134
II-1 Semigroups of Operators and Interpolation
So un →u in W l,p(0, T ; X) and u′ = Au + f.
(ii) Due to the hypotheses the problem

z′(t) = Az(t) + f ′(t),
z(0) = Ax + f(0)
has a strong solution z ∈C([0, T ]; X). Moreover let zn be the solution of the
problem

z′
n(t) = Anzn(t) + f ′(t),
zn(0) = Anx + f(0).
Clearly zn = u′
n so that, by Proposition 3.1,
un →u,
u′
n = zn →z
in C([0, T ]; X).
It follows that u ∈C1([0, T ]; X) and u′ = z. Moreover Au = u′ −f belongs
to C([0, T ]; X) so that u ∈C

[0, T ]; D(A)

.
⊓⊔
3.4 Perturbations of inﬁnitesimal generators
Consider the problem

u′(t) = Au(t) + F(t)u(t) + f(t),
t ∈[0, T ],
u(0) = x
(3.8)
under the following hypotheses:
(i)
A is the inﬁnitesimal generator of a strongly continuous
semigroup S, and
(ii)
F is a mapping from [0, T ] into L(X)
which is strongly continuous, that is F(·)x belongs to
C([0, T ]; X) for any x ∈X.
(3.9)
Set Sn(t) = exp(tAn). By Theorem 2.5 there exist M ≥1 and ω ∈R such
that
∥S(t)∥≤Meωt,
∥Sn(t)∥≤Meωt,
t ≥0.
(3.10)
Moreover, by the uniform boundedness theorem, there exists K > 0 such that
∥F(t)∥≤K,
t ∈[0, T ].
(3.11)
If f belongs to Lp(0, T ; X), p ≥1, then the function Ff, (Ff)(t) = F(t)f(t),
is B¨ochner measurable, as easily checked, and belongs to Lp(0, T ; X), whereas
if f is continuous then Ff is continuous.
The deﬁnitions of strict, strong, classical, and weak solutions in Deﬁnition
3.1 can be extended in an obvious way to problem (3.8). We say, in addition,

3 Nonhomogeneous linear evolution equations
135
that u is a mild solution of problem (3.8) if u is continuous in [0, T ] and veriﬁes
the integral equation
u(t) = S(t)x +
 t
0
S(t −s){F(s)u(s) + f(s)} ds.
(3.12)
Lemma 3.2. The following statements are equivalent:
(i) u is a strong solution of problem (3.8) in Lp(0, T ; X).
(ii) u is a mild solution of problem (3.8).
Proof. (i) =⇒(ii). Let u be a strong solution, then there exists a sequence
{uk} ⊂W 1,p(0, T ; X) ∩Lp
0, T ; D(A)

such that
uk →u,
u′
k −Auk −Fuk = fk →f
in Lp(0, T ; X),
uk(0) →x
in X.
Then uk is a strict solution of problem (3.1) with x = xk and f = fk + Fuk;
by Proposition 3.1 it follows that
uk(t) = S(t)xk +
 t
0
S(t −s){F(s)uk(s) + fk(s)} ds
and, as k →∞, we ﬁnd that u veriﬁes (3.12).
(ii) =⇒(i). Let u ∈C([0, T ]; X) be a mild solution. Then, again by Propo-
sition 3.1, u is the strong solution of problem (3.1) with f replaced by f +Fu.
Thus, there exists a sequence
{uk} ⊂W 1,p(0, T ; X) ∩Lp
0, T ; D(A)

such that
uk →u,
u′
k −Auk −Fuk →f + Fu,
in Lp(0, T ; X),
uk(0) →x
in X.
It follows that
u′
k −Auk −Fuk →f
so that u is a strong solution of (3.8).
⊓⊔
We now solve problem (3.12). For this it is useful to introduce the approx-
imate equation
un(t) = Sn(t)x +
 t
0
Sn(t −s){F(s)un(s) + f(s)} ds,
(3.13)
which is clearly equivalent to the system

u′
n(t) = Anun(t) + F(t)un(t) + f(t),
t ∈[0, T ],
un(0) = x.
(3.14)

136
II-1 Semigroups of Operators and Interpolation
Proposition 3.4. Assume that F veriﬁes (3.9), x ∈X and f ∈Lp(0, T ; X).
Then the following results are true:
(i) Problem (3.8) has a unique solution u in Lp(0, T ; X) that belongs to
C([0, T ]; X). Moreover for each n, problem (3.14) has a unique solution
un ∈C([0, T ]; X) and un →u in C([0, T ]; X).
(ii) If X is a reﬂexive Banach space, problem (3.8) has a unique weak solu-
tion in W 1,p(0, T ; D(A∗)′) ∩C([0, T ]; X) which coincides with the strong
solution in Lp(0, T ; X).
Proof. (i) Problems (3.8) and (3.14) are respectively equivalent to the equa-
tions
u = w(u),
un = wn(un),
(3.15)
where the mappings w and wn are deﬁned as
w(u)(t) = S(t)x +
 t
0
S(t −s){F(s)u(s) + f(s)} ds,
wn(u)(t) = Sn(t)x +
 t
0
Sn(t −s){F(s)u(s) + f(s)} ds.
Now w and wn map C[0, T ]; X) into itself and, if k ∈N,
|wk(u) −wk(¯u)|C([0,T ];X) ≤1
n!(MKeω+T )n|u −¯u|C([0,T ];X),
|wk
n(u) −wk
n(¯u)|C([0,T ];X) ≤1
n!(MKeω+T )n|u −¯u|C([0,T ];X),
where ω+ = sup{ω, 0}. Now the conclusion follows by the Contraction Map-
ping Principle.
(ii) Existence. We know from part (i) that (3.12) has a unique solution u
in C([0, T ]; X). We show that u belongs to W 1,p(0, T ; D(A∗)′) and that
u′ = [(A∗)∗+ F(t)]u + f
in Lp(0, T ; D(A∗)′).
We compute the vectorial distributional derivative of u: for k in D(A∗) and
φ ∈D(]0, T [)
−
 T
0
⟨k, u(t)⟩φ′(t) dt =−
 T
0
&
k, S(t)x+
 t
0
S(t−s)[F(s)u(s)+f(s)] ds
'
φ′(t) dt
=−
 T
0
⟨S∗(t)k, x⟩φ′(t) dt
−
 T
0
dt
 t
0
ds⟨S∗(t−s)k, F(s)u(s)+f(s)⟩φ′(t)
=
 T
0
& d
dtS∗(t)k, x
'
φ(t) dt

3 Nonhomogeneous linear evolution equations
137
+
 T
0
dt
 T
s
ds
& d
dtS∗(t −s)k, F(s)u(s)+f(s)
'
φ(t)
=
 T
0
⟨S∗(t)A∗k, x⟩φ(t) dt
+
 T
0
dt
 T
s
ds⟨S∗(t−s)A∗k,F(s)u(s)+f(s)⟩φ(t)
+
 T
0
⟨k, F(s)u(s)+f(s)⟩φ(s) ds
=
 T
0
&
k, (A∗)∗

S(t)x +
 t
0
S(t−s)[F(s)u(s)+f(s)] ds
+ F(t)u(t) + f(t)
'
φ(t) dt
	
.
So ﬁnally
u′ = (A∗)∗u(t) + F(t)u(t) + f(t) ∈Lp(0, T ; D(A∗)′)
(3.16)
and u ∈W 1,p([0, T ]; D(A∗)′) ∩C([0, T ]; X).
Uniqueness.
The proof is similar to the one of Proposition 3.2. It is suﬃcient to prove that
for x = 0 and f = 0, (3.16) has only the solution u = 0. So ﬁrst note that
u = 0 is a solution of (∗) with u(0) = 0 in W 1,p(0, T ; D(A∗)′) ∩C([0, T ]; X).
We ﬁx T > 0 and k in D(A∗) and introduce the function v(T, t) = S∗(T −t)k.
Then
d
dt⟨v(T, t), u(t)⟩X = ⟨v(T, t), F(t)u(t)⟩X
and
⟨v(T, T ), u(T )⟩X = ⟨v(T, 0), u(0)⟩+
 T
0
⟨v(T, t), F(t)u(t)⟩dt.
So for all k in D(A∗)
⟨k, u(T )⟩X =
 T
0
⟨k, S(T −t)F(t)u(t)⟩dt
and necessarily
u(T ) =
 T
0
S(T −t)F(t)u(t) dt.
But this means that u is a mild solution for x = 0 and f = 0. So u and 0 are
mild solutions. However since the mild solution is unique u(T ) = 0, ∀T ≥0.
This completes the proof.
⊓⊔
We now study strict solutions. The proof of the following proposition is
the same as the one of Proposition 3.3 and will be left to the reader.

138
II-1 Semigroups of Operators and Interpolation
Proposition 3.5. The following statements hold:
(i) Assume that F veriﬁes (3.9) and that F(·)x ∈C

[0, T ]; D(A)

for any
x ∈D(A). If, in addition, x ∈D(A) and f ∈Lp
0, T ; D(A)

, then
problem (3.8) has a unique strict solution
u ∈W 1,p(0, T ; X) ∩C

[0, T ]; D(A)

.
(ii) Assume that F veriﬁes (3.9) and that F(·)x ∈C1([0, T ]; X) for any
x ∈X. If, in addition, x ∈D(A) and f ∈W 1,p(0, T ; X), then problem
(3.8) has a unique strict solution
u ∈C1([0, T ]; X) ∩C

[0, T ]; D(A)

.
3.5 Evolution operators
Consider the systems

u′(t) = Au(t) + F(t)u(t) + f(t),
t ∈[s, T],
u(s) = x,
s ∈[0, T ],
(3.17)

u′
n(t) = Anun(t) + F(t)un(t) + f(t),
t ∈[s, T],
un(s) = x,
s ∈[0, T ].
(3.18)
The generalization of Propositions 3.4 and 3.5 to these systems is straight-
forward. Then, by Proposition 3.4 we know that system (3.17) (resp. (3.18))
has a unique strong solution. We deﬁne the transformations U and Un of X
as follows:
U(t, s)x = u(t),
Un(t, s)x = un(t),
t ≥0, x ∈X.
As easily seen U(t, s) and Un(t, s) are linear bounded operators in X. The
mapping U
(t, s) 	→U(t, s): ∆T →L(X),
where
∆T = {(t, s): s ∈[0, T ], t ∈[s, T]},
is called the evolution operator associated with A + F.
Proposition 3.6. Assume that A is the inﬁnitesimal generator of a strongly
continuous semigroup on X and that F veriﬁes (3.9). Let U be the evolution
operator relative to A + F and Un the evolution operator relative to An + F.
Then the following statements are true:
(i) U(·)x is continuous in ∆T for any x ∈X.
(ii) For any x ∈X, limn→∞Un(t, s)x = U(t, s)x uniformly in (t, s) ∈∆T .
(iii) If T ≥t ≥s ≥r ≥0, then U(t, s)U(s, r) = U(t, r), U(s, s) = I.

3 Nonhomogeneous linear evolution equations
139
(iv) For any (t, x) ∈∆T , we have
∥U(t, s)| ≤Me(ω+K)(t−s),
∥Un(t, s)∥≤Me(ω+K)(t−s),
(3.19)
where K = sup{∥F(t)∥: 0 ≤t ≤T }.
(v) If f ∈Lp(0, T ; X) the strong solution of (3.8) is given by
u(t) = U(t, 0)x +
 t
0
U(t, s)f(s) ds.
(3.20)
Proof. Let ws and wsn be the mappings in C([s, T]; X) deﬁned by
ws(u)(t) = S(t −s)x +
 t
s
S(t −r)F(r)u(r) dr,
wsn(u)(t) = Sn(t −s)x +
 t
s
Sn(t −r)F(r)u(r) dr.
If k ∈N we have
|wk
s (u) −wk
s (¯u)|C([s,T ];X) ≤1
n!(MKeω+T )n|u −¯u|C([s,T ];X),
|wk
sn(u) −wk
sn(¯u)|C([s,T ];X) ≤1
n!(MKeω+T )n|u −¯u|C([s,T ];X),
where ω+ = sup{ω, 0}. Thus the statements (i) and (ii) follow by the Con-
traction Mapping Principle. Moreover (iii) and (v) follow from (ii) (since the
analogous statements for Un are well known). Finally (iv) is a consequence of
the Gronwall lemma.
⊓⊔
3.6 Maximal regularity results in Hilbert spaces and main
isomorphism
We go back to the problem

u′(t) = Au(t) + f(t),
t ∈[0, T ],
u(0) = x.
(3.21)
By Maximal Regularity we mean that u′ and Au have the same regularity as
f. We assume that
(i) H = X is a Hilbert space and that
(ii) A is the inﬁnitesimal generator of an analytic semigroup
of negative type.
(3.22)
The hypothesis that the type of S be negative is not restrictive since by
the transformation v = eθtu we can change A to A −θI (cf. Theorem 2.11).
Therefore since S is an analytic semigroup of negative type, there exists M > 0
such that
∥R(λ, A)∥≤M
|λ|,
if Re λ > 0.
(3.23)

140
II-1 Semigroups of Operators and Interpolation
Remark 3.6. We shall see in §3.7 that maximal regularity can also be studied
in a general Banach space.
⊓⊔
In the ﬁrst part of this section we study problem (3.21) with u(0) = 0,
that is

u′(t) = Au(t) + f(t),
t ∈[0, T ],
u(0) = 0.
(3.24)
In the second part we will consider the problem (3.21) with u(0) = x and we
shall give conditions on x and f such that maximal regularity holds. This will
naturally lead us to interpolation spaces, which will be studied in detail in §4
and the construction of the main isomorphism.
Assume that f ∈L2(0, T ; H), then by Proposition 3.1, problem (3.24) has
a unique strong solution given by
u(t) =
 t
0
S(t −s)f(s) ds.
(3.25)
In order to study the regularity of u it is convenient to use the Fourier trans-
form in H.
For any u ∈D(R, H) we set
ˆu(k) =
 +∞
−∞
e−iktu(t) dt.
(3.26)
Let {eα}α∈Γ be a complete orthonormal system in H. By Parseval’s equality
applied to the scalar function ⟨u(t), eα⟩, we have
1
2π
 +∞
−∞
|⟨ˆu(k), eα⟩|2 dk =
 +∞
−∞
|⟨u(t), eα⟩|2 dt,
(3.27)
which implies that
1
2π
 +∞
−∞
|ˆu(k)|2 dk =
 +∞
−∞
|u(t)|2 dt.
(3.28)
Then the mapping
u 	→γ(u) = ˆu: D(R, H) →L2(R; H)
is continuous with respect to the L2(R, H)–topology and can be uniquely
extended to all L2(R, H). We shall denote by γ(u) or ˆu such an extension.
3.6.1 A priori estimates for Au
We ﬁrst assume that f ∈D(]0, ∞[ ; H) and consider the problem

3 Nonhomogeneous linear evolution equations
141

¯u′(t) = A¯u(t) + ¯f(t),
t ∈R,
¯u(0) = 0,
(3.29)
where
¯f(t) =

f(t)
if t ∈]0, T [,
0
if t ̸∈]0, T [.
Due to Proposition 3.1, problem (3.29) has a unique strong solution given by
¯u(t) =
⎧
⎪
⎨
⎪
⎩
u(t),
if t ∈[0, T ],
0,
if t ≤0,
e(T −t)Au(T ),
if t ≥T,
(3.30)
where u is the strong solution of problem (3.24). Since S is of negative type,
we have
¯u,
A¯u,
¯u′ ∈L2(R; H).
(3.31)
Lemma 3.3. We have for all f in W 1,2(0, ∞; H) and T > 0
 T
0
|Au(t)|2 dt ≤(M + 1)2
 T
0
|f(t)|2 dt.
(3.32)
Proof. By (3.29) it follows that
 +∞
0
e−ikt¯u′ dt =
 +∞
0
e−iktA¯u(t) dt +
 +∞
0
eikt ¯f dt.
Denote by ˆu and ˆf the Fourier transforms of u and f, respectively. Then
ikˆu(k) = Aˆu(k) + ˆf(k),
so that
ˆu(k) = R(ik, A) ˆf(k),
which implies that
|Aˆu(k)| ≤(M + 1)| ˆf(k)|,
and by Parseval’s inequality, we obtain (3.32).
⊓⊔
3.6.2 Main result for the case u(0) = 0
As S is a semigroup of negative type, |Ax| is a norm on D(A) equivalent to
the graph norm and A is an isomorphism for D(A) onto H. As a result
∥u∥2
L2(0,∞;H) = ∥Au∥2
L2(0,∞;H) + ∥u′∥2
L2(0,∞;H)
is a norm on the space
W(2, 0; D(A), H) = W 1,2(0, ∞; H) ∩L2
0, ∞; D(A)

.
(3.33)
Such spaces will be studied in more detail in §4.
In view of the previous a priori estimates, we have the following result.

142
II-1 Semigroups of Operators and Interpolation
Proposition 3.7. Assume that (3.22) holds and that f belongs to L2(0, ∞; H):
(i) Problem (3.24) has a unique strict solution u in W(2, 0; D(A), H) and
the following inequality is veriﬁed:
∥u∥W ≤
√
2(M + 2)∥f∥L2(0,∞;H).
(3.34)
(ii) The linear map
u 	→u′ −Au
: W0 = {v ∈W

2, 0; D(A), H): v(0) = 0} →L2(0, ∞; H)
(3.35)
is an isomorphism.
Proof. (i) Given f in L2(0, ∞; H), we know that (3.24) has a unique strong
solution in all intervals [0, T ], T > 0, for which inequality (3.32) holds. In
particular we can let T go to ∞and
∥Au∥2
L2(0,∞;H) ≤(M + 1)∥f∥2
L2(0,∞;H).
Moreover because u′ = Au + f
∥u′∥2
L2(0,∞;H) ≤(M + 2)∥f∥2
L2(0,∞;H)
and we get inequality (3.34). So u is a strict solution because
u ∈W 1,2(0, ∞; H) ∩L2
0, ∞; D(A)

.
(ii) As the map u 	→u(0): W 1,2(0, ∞; H) →H is linear and continuous,
W0 is a closed subspace of W. The map (3.35) is clearly linear and continuous.
It is bijective from part (i) because for all f in L2(0, ∞; H), (3.24) has a unique
solution in W0. Hence (3.35) is an isomorphism.
⊓⊔
3.6.3 The case u(0) = x and the main isomorphism
We now consider the problem

u′(t) = Au(t) + f(t),
t ∈[0, T ],
u(0) = x.
(3.36)
In order to complete the isomorphism (3.35) and stay in the space of solu-
tions W(2, 0; D(A), H), it is necessary to introduce appropriate interpolation
spaces, which will be studied in details in §4.
Consider the following subspace:
DA
 1
2, 2
 def
= {u(0): u ∈W(2, 0; D(A), H)}
(3.37)
of H endowed with the quotient norm

3 Nonhomogeneous linear evolution equations
143
∥x∥D = inf{∥u∥W : u ∈W(2, 0; D(A), H) and u(0) = x}.
(3.38)
DA(1/2, 2) is the space of traces T (2, 0; D(A), H), that will be studied in §4.2.
It will be shown in Proposition 4.3 that this space of traces is isomorphic to
the space of averages (D(A), H )1/2,2 as deﬁned in (4.14). In the variational
literature this space is also written [D(A), H]1/2. It is readily seen that the
map
u 	→u(0): W(2, 0; D(A), H) →DA
 1
2, 2

is linear continuous and onto.
Theorem 3.1. The following statements hold:
(i) Assume that (3.22) holds; then the map
u 	→

u′ −Au, u(0)

: W(2, 0; D(A), H) →L2(0, ∞; H) × DA
 1
2, 2

(3.39)
is an isomorphism. In particular for all f in L2(0, ∞; H) and x in DA(1/2, 2),
problem (3.36) has a unique strict solution in L2(0, ∞; H).
(ii) Assume that S is an analytic semigroup in the sense of Deﬁnition 2.3
in §2.7. Then for each T > 0, the map
u 	→

u′ −Au, u(0)

: W 1,2(0, T ; H) ∩L2
0, T ; D(A)

→L2(0, T ; H) × DA
 1
2, 2

(3.40)
is an isomorphism. In particular for all f in L2(0, T ; H) and x in DA(1/2, 2),
problem (3.36) has a unique strict solution in L2(0, T ; H).
Proof. (i) The map (3.39) is clearly linear and continuous by construction.
We show that it is surjective and hence an isomorphism by the Open Map-
ping Theorem (cf. K. Yosida [2, §5, pp.77]). This is equivalent to prove
that for each (f, x) in L2(0, ∞; H) × DA(1/2, 2), there exists a unique so-
lution in W(2, 0; D(A), H) to problem (3.36). So for each x in DA(1/2, 2),
there exists u1 in W(2, 0; D(A), H) such that ul(0) = x and u′
1 −Au1 ∈
L2(0, ∞; H). For each f in L2(0, ∞; H), there exists a unique strict solution
u2 in W(2, 0; D(A), H) to
u′
2 −Au2 = f −(u′
1 −Au1),
u2(0) = 0
by Proposition 3.7. The sum u = u1+u2 belongs to W(2, 0; D(A), H), u(0) = x
and
u′ = u′
1 + u′
2 = u′
1 + Au2 + f −(u′
1 −Au1) = A(u1 + u2) + f = Au + f.
We have constructed a solution u in W(2, 0; D(A), H) to problem (3.36).
Therefore the map (3.39) is surjective. It is injective if for f = 0 and x = 0
the only solution u in W(2, 0; D(A), H) is u = 0. But this is true by Proposi-
tion 3.7 (ii).

144
II-1 Semigroups of Operators and Interpolation
(ii) The term u 	→u′ −Au is well-deﬁned linear and continuous. We only
have to check that for u in W 1,2(0, ∞; H)∩L2
0, ∞; D(A)

, u(0) ∈DA(1/2, 2)
or equivalently that there exists ˜u in W 1,2(0, ∞; H) ∩L2
0, ∞; D(A)

, such
that u(0) = ˜u. It turns out that
˜u(t) =
√
t
1 + tu
 tT
1 + t

is such a function since
∥A˜u∥2
L2(0,∞;H) = ∥Au∥2
L2(0,T ;H)
and moreover
˜u′(t) =
T
√
T
(1 + t)3 u′
 tT
1 + t

−
√
T
(1 + t)2 u
 tT
1 + t

,
|˜u′(t)| ≤T

√
T
1 + tu′
 tT
1 + t
 +

√
T
1 + 1u
 tT
1 + t
.
So ˜u′ ∈L2(0, ∞; H) since each of two terms on the right-hand side does.
Now ﬁrst assume that S is a semigroup of negative type. The map (3.40)
is bijective since for all x ∈DA(1/2, 2) and f ∈L2(0, ∞; H) there exists a
unique solution uT in W 1,2(0, ∞; H) ∩L2
0, ∞; D(A)

to the equation
u′
T −AuT = fT ,
uT (0) = x,
where fT in L2(0, ∞; H) is given by
fT (t) =

f(t)
if 0 ≤t ≤T,
0
if t > T.
Its restriction u to [0, T ] belongs to W 1,2(0, T ; H) ∩L2
0, T ; D(A)

and
u′ = Au + f,
u(0) = x.
The existence implies the surjectivity and the uniqueness the injectivity.
When S(t) is not of negative type, ω0 = ω0(S) ≥0, ﬁx ω > ω0 and deﬁne
Sω(t) = S(t)e−(ω+1)t. Then ω0(Sω) < −1 and Aω = A −(ω + 1)I. Consider
for f in L2(0, T ; H) and x in DA(1/2, 2) the problem
u′
ω −Auω = fω,
uω(0) = x,
where
fω(t) = e−(ω+1)tf(t),
0 ≤t ≤T.
Since Sω is of negative type we are back to the previous situation and we
know that there exists a unique solution uω in W 1,2(0, T ; H)∩L2
0, T ; D(A)

since D(Aω) = D(A). Deﬁne

3 Nonhomogeneous linear evolution equations
145
u(t) = e(ω+1)tuω(t),
0 ≤t ≤T.
Then u belongs to W 1,2(0, T ; H) ∩L2
0, T ; D(A)

, and it is readily seen that
u′ = Au + f,
u(0) = x.
So again we have a bijection and an isomorphism.
⊓⊔
Corollary 3.1. When S is an analytic semigroup in the sense of Deﬁni-
tion 2.3 in §2.7, then for all T > 0

u(0): u ∈W 1,2(0, T ; H) ∩L2
0, T ; D(A)
 
= DA
 1
2, 2

.
Concerning classical solutions, we have the following results.
Proposition 3.8. Assume that f ∈L2(0, T ; H) and x ∈H; then problem
(3.36) has a unique classical solution in L2(0, T ; H).
Proof. Let u be the strong solution of (3.36). Set
u(t) = S(t)x + v(t).
Then, due to Theorem 3.1, v is the strict solution of the problem

v′(t) = Av(t) + f(t),
t ≥0,
v(0) = 0.
As S(t)x is analytic for t > 0, it follows that u is a classical solution of problem
(3.36).
To show uniqueness, it is suﬃcient to show that 0 is the unique classical
solution of the problem

z′(t) = Az(t),
z(0) = 0,
t ≥0.
(3.41)
Let z be a classical solution of (3.41); then z veriﬁes (3.41) for any t > 0. We
have
d
ds

S(t −s)z(s)

= 0
for all 0 < s < t.
By integrating this between ε and t −ε, we obtain
S(ε)z(t −s) = S(t −ε)z(ε).
As z is continuous, letting ε tend to 0 we ﬁnd z = 0.
⊓⊔
More information on problem (3.36) will be given in §4.

146
II-1 Semigroups of Operators and Interpolation
3.7 Regularity results in C([0, T ]; X)
Let X be a Banach space and let A: D(A) ⊂X →X be the generator of an
analytic semigroup S. Let M > 0, N > 0, and ω ∈R be such that
⎧
⎪
⎨
⎪
⎩
∥S(t)∥≤Meωt,
∥(A −ωI)S(t)∥≤N
t eωt,
t > 0,
∥Sn(t)∥≤Meωt,
∥(An −ωI)Sn(t)∥≤N
t eωt,
t > 0.
(3.42)
We consider here the problem

u′(t) = Au(t) + f(t),
u(0) = x ∈X,
(3.43)
where f belongs to C([0, T ]; X). We know that the mild solution of (3.43) is
given by
u(t) = S(t)x +
 t
0
S(t −s)f(s) ds.
(3.44)
The following results are well known (see, for instance, A. Pazy [1]).
Proposition 3.9. Assume that f ∈Cα([0, T ]; X), α ∈]0, 1[, and x ∈D(A).
Then problem (3.43) has a unique strict solution
u ∈C1([0, T ]; X) ∩C

[0, T ]; D(A)

.
Moreover
u′(t) = S(t)

Ax + f(t)

+
 t
0
AS(t −s)

f(s) −f(t)

ds.
(3.45)
Proof. By making the change of variable v(t) = u(t)e−ωt, t ≥0, it is suﬃcient
to prove the theorem for a semigroup S(t) such that ω = 0. We ﬁrst note that
formula (3.44) is meaningful because, by (3.42), we have
AS(t −s)

f(s) −f(t)
 ≤N∥f∥Cα([0,T ];X)|t −s|α−1.
Now set
un(t) = Sn(t)x +
 t
0
Sn(t −s) ds.
Then it is easy to check that
u′
n(t) = Sn(t)

Anx + f(t)

+
 t
0
AnSn(t −s)

f(s) −f(t)

ds.
As x ∈D(A) and
AnSn(t −s)

f(s) −f(t)
 ≤N∥f∥Cα([0,T ];X)|t −s|α−1, we
have, by the Dominated Convergence theorem,

3 Nonhomogeneous linear evolution equations
147
lim
n→∞u′
n(t) = S(t)

Ax + f(t)

+
 t
0
AS(t −s)(f(s) −f(t)r) ds
uniformly in t ∈[0, T ]. It follows that u ∈C1([0, T ]; X) and (3.45) holds.
It remains to prove that u ∈C

[0, T ]; D(A)

. We have in fact Anun(t) =
u′
n(t) −f(t) so that A

nR(n, A)un(t)

→u′(t) −f(t) as n →∞uniformly
in t ∈[0, T ]. As A is closed and nR(n, A)un(t) →u in C([0, T ]; X), it follows
that u ∈C

[0, T ]; D(A)

and u′ −Au = f.
⊓⊔
Proposition 3.10. If f ∈C([0, T ]; X) and x = 0, we have u ∈Cα([0, T ]; X)
for any α ∈]0, 1[.
Proof. Let t > s > 0; then by (3.44), we have
u(t) −u(s) =
 t
s
S(t −r)f(r) dr +
 s
0
dr
 t−r
s−r
S′(σ)f(r) dσ.
By (3.42) it follows that
|u(t) −u(s)| ≤Me|ω|T∥f∥C([0,T ];X)|t −s| + NLe|ω|T∥f∥C([0,T ];X),
where
L =
 s
0
dr
 t−r
s−r
dσ
σ ≤
 s
0
(s −r)−α
 t−r
s−r
σ1−α dσ
≤1
α
 s
0
(s −r)−α dr|t −s|α ≤T 1−α
1 −α
(t −s)α
α
.
Thus, the conclusion follows.
⊓⊔
Consider now the special case f = 0. We have the following results.
Proposition 3.11. Set u(t) = S(t)x, and let α ∈]0, 1[ , T > 0. Then the
following statements are equivalent:
(i) u ∈Cα([0, T ]; X),
(ii) supt∈]0,T ] t−α|S(t)x −x| < ∞.
Proof. (i) =⇒(ii) follows from
t−α|S(t)x −x| ≤∥u∥Cα([0,T ];X).
(ii) =⇒(i). If t > s > 0, we have
S(t)x −S(s)x = S(s)

S(t −s)x −x

so that there exists a constant C > 0 such that
|S(t)x −S(s)x| ≤C|t −s|α.
⊓⊔

148
II-1 Semigroups of Operators and Interpolation
We set
|x|α = sup
t∈]0,T ]
{t−α|S(t)x −x|: α ∈]0, 1[},
(3.46)
DA(α, ∞) = {x ∈X : |x|α < ∞}.
(3.47)
DA(α, ∞) is an interpolation space (see §4).
The following maximal regularity result was proved by E. Sinestrari [1]
(see also G. Da Prato and P. Grisvard [1]).
Theorem 3.2. Assume that f ∈Cα([0, T ]; X), α ∈]0, 1[, x ∈D(A). Let u be
the solution of (3.43). Then the following statements are equivalent:
(i) Ax + f(0) ∈DA(α, ∞),
(ii) u′, Au belongs to Cα([0, T ]; X). Moreover, if (i) or (ii) holds, we have
u′(t) ∈DA(α, ∞) and
sup
t∈]0,T ]
|u′(t)|α < ∞.
We complete this section with a simple analyticity result.
Proposition 3.12. Let f ∈C([0, ∞[; X) and x ∈X. Assume that f has an
analytic extension in a sector Sθ = {λ ∈C: | arg λ| ≤θ} for some θ > 0. Let
θ0 ∈]0, π/2] such that the semigroup S is analytic in Sθ0. Then if u is the
solution of (3.43), u has an analytic extension in Sθ∧θ0.
Proof. Follows immediately from (3.44).
⊓⊔
3.8 Examples of nonhomogeneous problems
3.8.1 Parabolic equations
Let Ωbe an open set of R with smooth boundary ∂Ω. Consider the following
initial value problem with Dirichlet boundary conditions: For all t ≥0
∂u
∂t (t, x) =
n

j,k=l
∂u
∂xk

ajk(x) ∂
∂xj
(t, x)

+ f(t, x),
x ∈Ω,
u(t, x) = 0,
x ∈∂Ω,
u(0, x) = u0(x),
x ∈Ω,
(3.48)
where ajk are real continuously diﬀerentiable functions in ¯Ωand verify the
ellipticity condition (2.89).
Let H = L2(Ω) and let A be the linear operator deﬁned by (2.94). As
proved in §2.9, A is the inﬁnitesimal generator of an analytic semigroup in
H = L2(Ω). The interpolation space DA(1/2, 2) coincides with H1
0(Ω) (cf.
J. L. Lions and E. Magenes [1]).
By using Propositions 3.7 and 3.8, we ﬁnd the following result.

3 Nonhomogeneous linear evolution equations
149
Proposition 3.13. The following statements hold:
(i) If f ∈L2([0, T ] × Ω) and u0 ∈L2(Ω); then problem (3.48) has a unique
classical solution u that belongs to
C

[0, T ]; L2(Ω)

∩W 1,2
ε, T ; L2(Ω)

∩L2
ε, T ; H2(Ω) ∩H1
0(Ω)

,
for any ε ∈]0, T [.
(ii) If f ∈L2([0, T ] × Ω) and u0 ∈H1
0(Ω), then the solution u is strict and
u ∈W 1,2
0, T ; L2(Ω)

∩L2
0, T ; H2(Ω) ∩H1
0(Ω)

.
For characterization of several interpolation spaces between the domain of
an elliptic operator and Lp(Ω) or Cα(Ω), see respectively P. Grisvard [2]
and P. Acquistapace and B. Terreni [2]. Similar results can be obtained
for Neumann or more general boundary conditions.
3.8.2 Schr¨odinger equation
We shall use here the notation of §2.10. Consider the problem: For all t ≥0
∂
∂tu(t, x) = i
n

j,k=l
∂
∂xk

ajk(x) ∂
∂xj
u(t, x)

+ f(t, x),
x ∈Ω,
u(t, x) = 0,
x ∈∂Ω,
u(0, x) = u0(x),
x ∈Ω.
(3.49)
By using Propositions 3.1 and 3.3, we ﬁnd the following results.
Proposition 3.14. The following statements hold:
(i) If f ∈L2([0, T ] × Ω) and u0 ∈L2(Ω), the problem (3.49) has a unique
strong solution
u ∈C

[0, T ]; L2(Ω)

.
(ii) If f ∈L2
0, T ; H2(Ω) ∩H1
0(Ω)

and u0 ∈H2(Ω) ∩H1
0(Ω), then the
solution u is strict and
u ∈W 1,2
0, T ; L2(Ω)

∩C

[0, T ]; H2(Ω) ∩H1
0(Ω)

.
(iii) If f ∈W 1,2
0, T ; L2(Ω)

and u0 ∈H2(Ω) ∩H1
0(Ω), then the solution u
is strict and
u ∈C1
[0, T ]; L2(Ω)

∩C

[0, T ]; H2(Ω) ∩H1
0(Ω)

.

150
II-1 Semigroups of Operators and Interpolation
3.8.3 Wave equation
We use here the notation of §2.10 and consider the problem: For all t ≥0
∂2
∂t2 u(t, x) =
n

j,k=l
∂
∂xk

ajk(x) ∂
∂xj
u(t, x)

,
x ∈Ω,
u(t, x) = 0, x ∈∂Ω,
u(0, x) = u0(x),
∂
∂tu(0, x) = u1(x),
x ∈Ω.
(3.50)
We now prove the following result.
Proposition 3.15. The following statements hold:
(i) Assume that f ∈L2([0, T ] × Ω), u0 ∈H1
0(Ω), and u1 ∈L2(Ω). Then
problem (3.50) has a unique strong solution u that belongs to
C

[0, T ]; H1
0(Ω)

∩C1
[0, T ]; L2(Ω)

.
(ii) Assume that f ∈L2
0, T ; H2(Ω) ∩H1
0(Ω)

, u0 ∈H2(Ω) ∩H1
0(Ω), and
u1 ∈H1
0(Ω). Then u is a strict solution that belongs to
C

[0, T ]; H2(Ω) ∩H1
0(Ω)

∩C1
[0, T ]; H1
0(Ω)

∩W 2,2
0, T ; L2(Ω)

.
(iii) Assume that f ∈W 1,2
0, T ; L2(Ω)

, u0 ∈H2(Ω) ∩H1
0(Ω), and u1 ∈
H1
0(Ω). Then u is a strict solution that belongs to
C

[0, T ]; H2(Ω) ∩H1
0(Ω)

∩C1
[0, T ]; H1
0(Ω)

∩C2
[0, T ]; L2(Ω)

.
Proof. Setting
U(t) =

0
f(t, ·)
	
,
F(t) =
⎡
⎣
u(t, ·)
∂
∂tu(t, ·)
⎤
⎦,
U0 =

u0
u1
	
,
problem (3.50) is equivalent to
U ′ = AU + F(t),
U(0) = U0,
where the operator A is deﬁned by (2.94). Now all conclusions follow from
Propositions 3.1 and 3.3.
⊓⊔

3 Nonhomogeneous linear evolution equations
151
3.9 Point spectrum operators
If L : D(L) ⊂X →X is a linear operator, we set
σ−(L) = {λ ∈σ(L) : Re λ < 0},
(3.51)
σ+(L) = {λ ∈σ(L): Re λ > 0},
(3.52)
σ0(L) = {λ ∈σ(L): Re λ = 0}.
(3.53)
The elements of σ−(L) are called the stable points and those of σ+(L) the
unstable points of the spectrum of L.
In this section we are interested in solutions of the equation
u′(t) = Au(t) + f(t)
(3.54)
either on all the real line or in [0, +∞[. Given
f ∈L2(R; X)

resp. L2([0, +∞[ ; X)

,
we say that u ∈L2(R; X)

resp. L2(0, +∞; X)

is a mild solution of (3.54) if
u(t) = e(t−a)Au(a) +
 t
a
e(t−s)Af(s) ds
(3.55)
for all a and all t in R (resp. in [0, +∞[) such that a ≤t. We will study this
problem under hypothesis (P) below.
We say that a linear operator L: D(L) ⊂X →X in X veriﬁes Assumption
(P) if the following conditions are veriﬁed:
Assumption (P).
(i) L is the inﬁnitesimal generator of a strongly continuous semigroup etL
on X,
(ii) σ0(L) is empty and σ+(L) consists of a ﬁnite set of eigenvalues of ﬁnite
algebraic multiplicity,
(iii) there exists ε > 0, NL > 0 such that
sup{Re λ: λ ∈σ−(L)} < −ε
and
∥etL(I −ΠL)∥≤NLe−εt
∀t ≥0,
(3.56)
where ΠL is the projector on the subspace of all eigenvectors correspond-
ing to the eigenvalues in σ+(L) deﬁned by
ΠL =
1
2πi

γ
R(λ, L) dλ
(3.57)
and γ is a simple Jordan curve around σ+(L) in the open half plane of
all λ such that Re λ > 0.

152
II-1 Semigroups of Operators and Interpolation
We set
X+
L = ΠLX,
X−
L = (I −ΠL)X.
(3.58)
Remark 3.7. Assume that L fulﬁlls (P). Then by hypothesis (ii), X+
L is ﬁnite
dimensional and there exists η > 0 such that
Re λ > η,
for all λ ∈σ+(L).
(3.59)
Moreover the subspace X+
L is stable for the semigroup etL, which can be
extended for t < 0 by the formula
etLΠL =
1
2πi

γ
etλR(λ, L) dλ,
t ∈R.
(3.60)
Finally, by (3.59)–(3.60), it follows that there exists N ′
L > 0 such that
∥etLΠL∥≤N ′
Leηt
∀t ≤0.
(3.61)
⊓⊔
Remark 3.8. Assumption (P) holds in the following cases:
(a) H has a ﬁnite dimension and σ(A) ∩iR = Ø.
(b) etL is compact for any t > 0 and σ(A) ∩iR = Ø,
where (a) is a special case of (b).
⊓⊔
We ﬁrst study solutions of (3.54) on all the real line.
Proposition 3.16. Assume that A veriﬁes hypothesis (P). Then if f
∈
L2(R, X), equation (3.54) has a unique mild solution u ∈L2(R; X) that is
given by the formula
u(t) =
 t
−∞
e(t−s)A(I −ΠA)f(s) ds −
 +∞
t
e(t−s)AΠAf(s) ds.
(3.62)
Proof. First of all, we notice that formula (3.62) is meaningful due to (3.56)
and (3.61).
Step 1 (Existence). Assume ﬁrst that f ∈L2
R; D(A)

and let u be the func-
tion deﬁned by (3.62). Then we have
u′(t) = (I −ΠA)f(t) + A
 t
−∞
e(t−s)A(I −ΠA)f(s) ds
+ ΠAf(t) −A
 −∞
t
e(t−s)AΠAf(s) ds = Au(t) + f(t).
Thus we have proved that if f ∈L2
R; D(A)

, then u is a solution of (3.54).
To prove the existence in general, it is suﬃcient to approximate f(t) by
kR(k, A)f(t) and let k tend to inﬁnity.

3 Nonhomogeneous linear evolution equations
153
Step 2 (Uniqueness). It is suﬃcient to prove that if
u ∈W 1,2(R; X) ∩L2
R; D(A)

is such that u′(t) = Au(t) for all t ∈R, then u(t) = 0 for all t. In fact for
such a u, we have u(t) = etAu(0), so we must have ΠAu(0) = 0 and then
ΠAu(t) = 0 for all t. It follows that for all t > s
u(t) = e(t−s)A(I −ΠA)u(s),
and, as t →+∞, we ﬁnd u(t) = 0 by (3.56).
⊓⊔
Now we consider solutions of (3.54) in [0, +∞[.
Proposition 3.17. Assume that A veriﬁes hypothesis (P).
Then if f
∈
L2(0, +∞; X), the mild solutions of (3.54) in L2(0, +∞; X) are given by the
formula
u(t) = etA(I −ΠA)x +
 t
0
e(t−s)A(I −ΠA)f(s) ds
−
 +∞
t
e(t−s)AΠAf(s) ds,
x ∈X.
(3.63)
Proof.
The fact the function in (3.63) deﬁnes a solution of (3.53) in
L2(0, +∞; X) can be easily checked. Conversely, let u ∈L2(0, +∞; X) be
a mild solution of (3.54) in L2(0, +∞; X). Then we have
u(t) = etAu(0) +
 t
0
e(t−s)Af(s)ds
= etA(I −ΠA)u(0)
+
 t
0
e(t−s)A(I −ΠA)f(s) ds −
 +∞
t
e(t−s)AΠAf(s) ds + v(t), (3.64)
where
v(t) = etA

ΠAu(0) +
 +∞
0
e−sAΠAf(s) ds

.
(3.65)
Now, v is bounded in L2(−∞, 0; X) and by (3.64), v in L2(0, +∞; X). Thus v
is a bounded solution of (3.54) with f(t) = 0, and, by the uniqueness proved
in Proposition 3.16, it follows that v(t) = 0 for all t. In conclusion u is given
by (3.63) and the proof is complete.
⊓⊔
Corollary 3.2. Assume that A veriﬁes Assumption (P). Let x ∈X, f ∈
L2(0, +∞; X) and let u be the mild solution of problem (3.1) in L2(0, +∞; X).
Then the following statements are equivalent:
(i) f ∈L2(0, +∞; X);

154
II-1 Semigroups of Operators and Interpolation
(ii) the following compatibility condition holds
ΠAx +
 +∞
0
e−sAΠAf(s) ds = 0.
(3.66)
Proof. The conclusion follows from (3.64)–(3.65) and Proposition 3.17.
⊓⊔
4 Interpolation spaces
4.1 Notation
We shall consider two Banach spaces X0 and X1 with X0 continuously and
densely embedded in X1. Denote by | · |0 and | · |1 the respective norms of X0
and X1. Let k > 0 be a constant such that
|x|1 ≤k|x|0.
(4.1)
For any p ≥1 and i = 0 or 1 we shall denote by Lp
∗(Xi) the Banach space of
all Bochner measurable mappings u: [0, +∞[ →X1 such that
 ∞
0
|u(t)|p
Xi
dt
t < +∞.
(4.2)
We shall use Hardy’s inequality, which we recall below.
Lemma 4.1. Let u: [0, +∞[ →R be measurable and such that tαu ∈Lp(0, ∞)
for some α ∈]0, 1[. Setting
M(u)(t) = 1
t
 t
0
u(σ) dσ,
(4.3)
we have tαM(u) ∈Lp(0, ∞) and there exists a constant c(α, p) > 0 such that
∥tαM(u)∥Lp(0,∞) ≤c(α, p)∥tαu∥Lp(0,∞).
(4.4)
If Z and W are two Banach spaces, we write Z ∼= W to indicate that they
are isomorphic and Z ⊂W if Z is continuously embedded in W.
We shall now give two equivalent deﬁnitions of interpolation spaces.
4.2 Spaces of traces T (p, α, X0, X1)
For any α ∈R and p ∈[1, ∞[ we shall denote by W(p, α, X0, X1) the set of
all mappings u: [0, ∞[ →X1 such that
tαu ∈Lp(0, ∞; X0),
tαu′ ∈Lp(0, ∞; X1),
(4.5)
where

4 Interpolation spaces
155
u′ = du
dt .
When W(p, α, X0, X1) is endowed with the norm
∥u∥p
W(p,α,X0,X1) = ∥tαu∥p
Lp(0,∞,X0) + ∥tαu′∥p
Lp(0,∞,X1),
(4.6)
it is a Banach space.
Clearly if u ∈W(p, α, X0, X1), then u is a.e. equal to a continuous function
u in ]0, ∞[ with values in X1. We prove now the stronger result.
Proposition 4.1. Let p ≥1, 0 < α+1/p < 1, u ∈W(p, α, X0, X1). Then the
limit of u(t) exists, as t →0, in X1. Moreover, for any T > 0, there exists a
constant CT > 0 such that for all t, 0 ≤t ≤T ,
|u(t)|X1 ≤CT ∥u∥W(p,α,X0,X1).
(4.7)
Proof. We ﬁrst prove (4.7). Let 0 < α + p−1 < 1, q−1 = 1 −p−1, T > 0 ﬁxed,
s, t ∈[0, T ]. By integrating with respect to s, between 0 and T the identity
sαu(t) = sαu(s) + sα
 t
s
u′(σ) dσ,
we obtain
u(t) = T −α−1

(α + 1)
 T
0
sαu(s) ds +
 T
0
σα+1u′(σ) dσ
−
 T
t
[T α+1 −σα+1]u′(σ) dσ

from which
|u(t)|1 ≤T −α−1(α + 1)
 T
0
|sαu(s)| ds + 2
 T
0
|u′(σ)| dσ.
As s−α ∈Lq(0, T ), by using the H¨older inequality, we ﬁnd
|u(t)|1 ≤kT −α−1/p(α+1)∥sαu∥Lp(0,∞;X0)+∥s−α∥Lq(0,T )∥sαu′∥Lp(0,∞;X1)
and (4.7) is proved. Set now un(t) = u(t + 1/n); then, as easily checked, un
belongs to W(p, α, X0, X1) and un →u in W(p, α, X0, X1). Now by (4.7) it
follows that {un} is a Cauchy sequence in C([0, T ]; X1) and the limit of u(t),
as t →0, does exist.
⊓⊔
We now assume that 0 < α + p−1 < 1 and consider the mapping
u 	→γu = u(0): W(p, α, X0, X1) →X1,
(4.8)
which is well-deﬁned by Proposition 4.1. We deﬁne the spaces of traces
T (p, α, X0, X1) by setting

156
II-1 Semigroups of Operators and Interpolation
T (p, α, X0, X1) = {x ∈X1 : ∃u ∈W(p, α, X0, X1), u(0) = x}.
T (p, α, X0, X1), endowed with the norm
|x|T (p,α,X0,X1) = inf{∥u∥W(p,α,X0,X1) : u(0) = x},
(4.9)
is a Banach space. We clearly have
X0 ⊂T (p, α, X0, X1) ⊂X1.
(4.10)
Remark 4.1. If u ∈W(p, α, X0, X1), then clearly, for any t ≥0, u(t) ∈
T (p, α, X0, X1) and
|u(t)|T (p,α,X0,X1) ≤∥u∥W(p,α,X0,X1).
(4.11)
It follows by a density argument that
u ∈C

[0, ∞[; T (p, α, X0, X1)

for any u ∈W(p, α, X0, X1).
(4.12)
⊓⊔
Proposition 4.2. Let p ≥1, 0 < α+1/p < 1. The following statements hold:
(i) The embedding of T (p, α, X0, X1) in X1 is continuous.
(ii) The embedding of X0 in T (p, α, X0, X1) is continuous and dense.
Proof. (i) Due to the Closed Graph theorem, it is suﬃcient to prove that the
embedding
x 	→x: T (p, α, X0, X1) →X1
is closable. Let xn →0 in T (p, α, X0, X1) and xn →y in X1. We have to show
that y = 0. Let in fact un ∈W(p, α, X0, X1) be such that
un(0) = xn,
∥un∥W(p,α,X0,X1) ≤

1 + 1
n

|xn|T (p,α,X0,X1).
This can always be done by deﬁnition of the norm in T (p, α, X0, X1) as an inf
(cf. (4.9)). Then un →0 in W(p, α, X0, X1), so that by (4.7) xn = un(0) →
0 = y in X1.
(ii) Let x ∈X0, φ ∈C∞([0, ∞[: R) such that φ(t) = 1 if t ∈[0, 1
2] and
φ(t) = 0 if t ≥1. Setting u = φx we have
∥u∥p
W p(p,α,X0,X1) =
 1
0
|tαφ(t)x|p
0 dt +
 1
0
|tαφ′(t)x|p
1 dt ≤const. |x|p
X0.
It follows that |x|T (p,α,X0,X) ≤const.|x|0. This proves the continuity of the
embedding X0 →T (p, α, X0, X1).
We now prove the density. Let x ∈T (p, α, X0, X1). By deﬁnition there
exists u ∈W(p, α, X0, X1) such that u(0) = x and we can construct a sequence
εn > 0, εn →0, such that u(εn) ∈X0. For any such ε > 0 deﬁne the function

4 Interpolation spaces
157
zε(t) =

uε(t) −u(t),
0 ≤t ≤ε,
0,
t > 0.
By construction
z′
ε(t) = −u′(t)χ[0,ε](t),
where χ[0,ε] is the characteristic function of the interval [0, ε]. Moreover
∥tαz′
ε∥Lp = ∥tαu′∥Lp(0,ε)
and tαz′
ε →0 in Lp(0, ∞; X1) as ε →0. Similarly zε is a measurable function
from [0, ∞[ to X0 and we wish to show that the Lp-norm of tαzε goes to zero
as ε →0. By deﬁnition
∥tαzε∥p
Lp =
 ∞
0
tα
 ε
t
u′(s) ds χ[0,ε](t)

p
dt =
 ε
0
tαp

 ε
t
u′(s) ds

p
dt
and for 0 < t ≤ε, 1 < p < ∞, and p−1 + q−1 = 1

 ε
t
u′(s) ds
 ≤
 ε
t
s−αq ds
1/q
∥sau′∥Lp(t,ε).
But 1 −αq > 0 and
 ε
t
s−αq ds ≤ε1−αq
1 −αq
because by hypothesis
1 −αq =
p
p −1[1 −p−1 −α] > 0.
Therefore
∥tαzε∥p
Lp ≤
 ε1−αq
1 −αq
	p/q  ε
0
tαp dt ∥sαu′∥p
Lp.
Again by hypothesis αp + 1 > 0 and
 ε
0
tαp dt = εαp+1
αp + 1
because
αp + 1 = p(α + p−1) > 0.
Finally
∥tαzε∥Lp ≤ε(1−αq)1/q
(1 −αq)1/q
ε(αp+1)1/q
(αp + 1)1/q ∥sαu′∥Lp
and
1 −αq
q
+ αp + 1
p
= p−1 + q−1 = 1.

158
II-1 Semigroups of Operators and Interpolation
This shows that tαzε ∈Lp(0, ∞; X0, X1) and
∥tαzε∥Lp ≤ε(1 −αq)−1/q(αp + 1)−1/q∥sαu′∥p.
Therefore zε ∈W(p, α; X0, X1) and zε →0. Hence by Remark 4.1
xn −x = u(εn) −u(0) = zεn(0) →0
in T (p, α; X0, X1) as εn →0.
When p = 1, we have by hypothesis −1 < α < 0
∥tαzε∥L1 =
 ε
0
tα

 ε
t
u′(s) ds
 dt
≤
 ε
0
tα max
[t,ε] s−α dt ∥sαu′∥L1
≤
 ε
0
tα ε−α dt ∥sαu′∥L1
because
t ≤s ≤ε =⇒t−α ≤s−α ≤ε−α.
Therefore because α + 1 > 0
∥tαzε∥L1 ≤εα+1
α + 1ε−α∥sαu′∥L1 =
ε
α + 1∥sαu′∥L1,
and we reach the same conclusion as for p > 1. This proves the density of X0
in T (p, α; X0, X1)
⊓⊔
Example 4.1. Let X0 = H1(R), X1 = L2(R), p = 2, α = 0. Then
W(2, 0, H1
R), L2(R)

=

u ∈L2
0, ∞; H1(R): u′ ∈L2(0, ∞; L2(R)
 
.
For any u ∈W

2, 0, H1(R), L2(R)

we set ˜u(t, x) = u(t)(x) and R2
+ =
{(t, x) ∈R2 : t ≥0}. Then, by Fubini’s theorem, we have
˜u,
∂˜u
∂t , ∂˜u
∂x ∈L2(R2
+)
so that ˜u ∈H1(R2
+) and W

2, 0, H1(R), L2(R)
 ∼= H1(R2
+), whereas
T

2, 0, H1(R), L2(R)
 ∼= H1/2(R2
+).
⊓⊔
We now give the Interpolation Theorem.
Theorem 4.1. Let X0 →X1, Y0 →Y1 be Banach spaces. Let
π ∈L(X0, Y0) ∩L(X1, Y1).
Then if p ≥1 and 0 < α + p−1 < 1, we have
π ∈L

T (p, α, X0, X1), T (p, α, Y0, Y1)

and
∥π∥L

T (p,α,X0,X1),T (p,α,Y0,Y1)
 ≤max{∥π∥L(X0,Y0), ∥π∥L(X1,Y1)}.
(4.13)

4 Interpolation spaces
159
4.3 Spaces of averages (X0, X1)θ,p
For any θ ∈]0, 1[ and any p ∈[1, ∞[ set
(X0, X1)θ,p =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
x ∈X1 :
∃ui : [0, ∞[ →Xi,
i = 0, 1,
t−θu0 ∈Lp
∗(X0),
t1−θu1 ∈Lp
∗(X1),
x = u0(t) = u1(t)
a.e.
⎫
⎪
⎪
⎬
⎪
⎪
⎭
(4.14)
and moreover
|x|(X0,X1)θ,p = inf{∥t−θu0∥Lp
∗(X0) + ∥t1−θu1∥Lp
∗(X1)
: t−θu0 ∈Lp
∗(X0), t1−θu1 ∈Lp
∗(X1)}.
(4.15)
In the variational literature, it is customary to denote by [X0, X1]θ the space
(X0, X1)θ,2. This notation will be used in Chapter 2 and occasionally in the
other parts of the book.
Proposition 4.3. Let p ≥1, 0 < α + p−1 < 1, θ = α + p−1. Then we have
(X0, X1)θ,p ∼= T (p, α, X0, X1).
(4.16)
Proof. We proceed in two steps.
Step 1. We have
T (p, α, X0, X1) ⊂(X0, X1)θ,p.
(4.17)
Let x ∈T (p, α, X0, X1) and u ∈W(p, a, X0, X1) such that x = u(0). Set
= u0(t) = u
1
t

,
u1(t) = −
 1/t
0
u′(s) ds = x −u
1
t

.
(4.18)
We ﬁrst check that
t−θu0 ∈Lp
∗(X0).
(4.19)
In fact we have
 ∞
0
|t−θu0(t)|p
0
dt
t =
 ∞
0
t−θu
1
t

p
0
dt
t
=
 ∞
0
|sθu(s)|p
0
ds
s
=
 ∞
0
|sαu(s)|p
0 ds ≤∥u∥p
W(p,α,X0,X1)
and (4.19) is proved. Then we check that
t1−θu1 ∈Lp
∗(X1).
(4.20)

160
II-1 Semigroups of Operators and Interpolation
We have
 ∞
0
|t1−θu1(t)|p
1
dt
t =
 ∞
0
t1−θ
 1/t
0
u′(σ)

p
1
dt
t
=
 ∞
0
sθ−1
 s
0
u′(σ) dσ

p
1
ds
s
=
 ∞
0
sα 1
s
 s
0
u′(σ) σ

p
1
ds
≤const.
 ∞
0
|sαu′(s)|p
1 ds
≤const. ∥u∥p
W(p,α,X0,X1)
due to Hardy’s inequality. Thus the inclusion (4.17) is proved.
Step 2. We have
(X0, X1)θ,p ⊂T (p, α, X0, X1).
(4.21)
Assume that x = u0(t) + u1(t) with t−θu0 ∈Lp
∗(X0), t1−θu1 ∈Lp
∗(X1). We
ﬁrst regularize u0 and u1 by setting
˜u0(t) = 1
t
 t
0
u0(s) ds,
˜u1(t) = 1
t
 t
0
u1(s) ds.
(4.22)
We still have x = ˜u0(t) + ˜u1(t). We now set
u(t) = ˜u0
1
t

= x −˜u1
1
t

= t
 1/t
0
u0(s) ds
(4.23)
and prove successively that for θ = α + p−1
tαu ∈Lp(0, ∞; X0),
tαu′ ∈Lp(0, ∞; X1),
u(0) = x.
This will prove inclusion (4.21).
We show (4.24). We have
 ∞
0
|tαu(t)|p
0 dt =
 ∞
0
|tθ−1/pu(t)|p
0 dt
=
 ∞
0
|tθ−1/p+1
 1/t
0
u0(σ) dσ|p
0 dt
=
 ∞
0
τ 1/p−θ−1
 τ
0
u0(σ) dσ

p
0
τ −2 dτ
=
 ∞
0
τ −θ
 τ
0
u0(σ) dσ

p
0
τ −1 dτ ≤const.
 ∞
0
|t−θu0(t)|p
0
dt
t

4 Interpolation spaces
161
and (4.24) is proved.
To show (4.24) consider the following identity:
u′(t) = −
 1/t
0
u1(s) ds + 1
t u1
1
t

(4.24)
and set
J =
 ∞
0
|tαu′(t)|p
1 dt.
We have J ≤A1/p + B1/p, where
A =
 ∞
0
tα
 1/t
0
u1(σ) dσ

p
1
dt,
B =
 ∞
0
tα−1u1
1
t

p
1
dt.
Now, setting τ = t−1 and using once again Hardy’s inequality, we have
A =
 ∞
0
τ 1−θ 1
τ
 τ
0
u1(σ) dσ

p
1
τ −1 dτ ≤const.
 ∞
0
|τ 1−θ˜u1(τ)|p
1τ −1 dτ
= const. ∥t1−θ˜u1∥Lp
∗(X1).
B =
 ∞
0
|τ 1−θu1(τ)|p
1τ −1 dτ = ∥t1−θu1∥Lp
∗(X1).
Thus (4.24) follows.
It remains to show (4.24). By Proposition 4.1 the limit α = limt→0 u(t)
exists in X1. By (4.23) it follows that limt→0 u1(t) = x −α = β. So we have
to prove that β = 0. By Hardy’s inequality we have t1−θ˜u1 ∈Lp
∗(X1) so that
 ∞
0
tp(1−θ)−1|˜u1(t)|p
1 dt < ∞, which implies β = 0.
⊓⊔
We now prove some inclusions.
Proposition 4.4. Let p, q ∈[1, ∞[ , θ, ω ∈]0, 1[; then:
(i) if p < q, we have (X0, X1)θ,p ⊂(X0, X1)θ,q′,
(ii) if θ < ω, we have (X0, X1)θ,p ⊂(X0, X1)ω,q′.
Proof. Let x ∈(X0, X1)θ,p, x = u0(t) + u1(t) with t−θu0 ∈Lp
∗(X0), t1−θu1 ∈
Lp
∗(X1). Choose φ ∈D(]0, ∞[) such that φ(t) ≥0,
 ∞
0
φ(s)s−1 ds = 1 and set
vj(t) =
 ∞
0
φ(s)uj
 t
s
ds
s ,
j = 0, 1.
We have x = v0(t) + v1(t). Moreover, due to Young’s inequality,
t−θv0 ∈Lp
∗(X0) ∩L∞(X0) ⊂Lq
∗(X0),
t1−θv1 ∈Lp
∗(X1) ∩L∞(X1) ⊂Lq
∗(X1),
and (i) is proved. Let us now prove (ii). Due to (i) it is suﬃcient to prove that

162
II-1 Semigroups of Operators and Interpolation
(X0, X1)θ,p ⊂(X0, X1)ω,1.
(4.25)
Let x ∈(X0, X1)θ,p, x −u(0), tαu ∈Lp(0, ∞; X0), tαu′ ∈Lp(0, ∞; X1), for
α + p−1 = θ. We can assume u(t) = 0 for any t ≥1. Using H¨older’s inequality
we have
tω−1u ∈L1(0, ∞; X0),
tω−1u′ ∈L1(0, ∞; X1).
(4.26)
In fact we have, for instance
 1
0
|tω−1u(t)|0 dt =
 1
0
tω−1−α|tαu(t)|0 dt
 1
0
|tαu(t)|p
0 dt
 1
0
 sω−1−α)q ds
	1/q
=
p(ω −θ
p −1
1/q  1
0
|tαu(t)|p
0 dt < ∞.
Now (4.26) implies that x ∈T (p, ω −1, X0, X1) ∼= (X0, X1)ω,1.
⊓⊔
4.4 Interpolation spaces between the domain of a linear operator
A and the space X
Let X be a Banach space and A be a closed linear operator in X with dense
domain D(A) in X. We assume that ρ(A) ⊃]0, +∞[ and that there exists a
constant C > 0 such that
∥R(λ, A)∥≤C
λ ,
λ > 0.
(4.27)
Proposition 4.5. Let p ≥1, θ ∈]0, 1[; then we have
(D(A), X)θ,p = {x ∈X : λ1−θAR(λ, A)x ∈Lp
∗(X)}.
(4.28)
Moreover the norm |x|(D(A),X)θ,p is equivalent to
|x|X + ∥λ1−θAR(λ, A)x∥Lp(X).
(4.29)
Proof. The proof proceeds in several steps.
Step 1.
We have the following inclusion:
(D(A), X)θ,p ⊂{x ∈X : λ1−θAR(λ, A)x ∈Lp
∗(X)}.
(4.30)
Let x = u0(t) + u1(t) with t−θu0 ∈Lp
∗

D(A)

, t1−θu1 ∈Lp
∗(X). We have
t1−θAR(t, A)x = t−θ
tR(t, A)

Au0(t) + t1−θ
AR(t, A)

u1(t).
It follows that
∥t−θAR(t, A)x∥p
Lp
∗(X) ≤Cp∥t−θu0∥p
Lp
∗

D(A)
 + (1 + C)∥t1−θu1∥p
Lp
∗(X)
and (4.29) is proved.

4 Interpolation spaces
163
Step 2.
We have
{x ∈X : λ1−θAR(λ, A)x ∈Lp
∗(X)} ⊂(D(A), X)θ,p.
(4.31)
Assume that t1−θAR(t, A)x ∈Lp
∗(X) and set
u0(t) =

0
if t < 1,
tR(t, A)x
if t ≥1,
u1(t) =

x
if t < 1,
−AR(t, A)x
if t ≥1,
so that u0(t) + u1(t) = x. We have t−θu0 ∈Lp
∗

D(A)

. In fact
 ∞
0
|t−θu0(t)|p
D(A)
dt
t =
 ∞
1
|t1−θAR(t, A)x|p dt
t
= ∥t1−θAR(t, A)x∥p
Lp
∗(X).
We ﬁnally prove that t1−θu1 ∈Lp
∗(X). We have in fact
 ∞
0
|t1−θu1(t)|p
X
dt
t =
 1
0
tp(1−θ)−1|x|X dt +
 ∞
1
|t1−θAR(t, A)x|p dt
t
≤∥t1−θAR(t, A)x∥p
Lp
∗(X) +
1
p(1 −θ)|x|X.
⊓⊔
4.5 The case of a strongly continuous semigroup
Proposition 4.6. Assume that A is the inﬁnitesimal generator of a strongly
continuous semigroup S in X of negative type. Then, if p ≥1 and θ ∈]0, 1[,
we have
(D(A), X)θ,p = {x ∈X : tθ−1[S(t)x −x] ∈Lp
∗(X)}.
(4.32)
Moreover the norm |x|(D(A),X)θ,p is equivalent to
|x|X + ∥tθ−1(S(t)x −x)∥Lp
∗(X).
(4.33)
Proof. Again the proof proceeds in several steps.
Step 1.
We have
x ∈(D(A), X)θ,p =⇒tθ−1(S(t)x −x) ∈Lp
∗(X).
(4.34)
We set
1
t (S(t)x −x) = 1
t
 t
0
g(s) ds,
(4.35)

164
II-1 Semigroups of Operators and Interpolation
where g(s) =

I −S(t −s)

u′(s) + S(t −s)Au(s) and u is a function in
W(α, p, D(A), X), α + p−1 =θ, such that u(0) = x. We have
tαu ∈Lp
0, ∞; D(A)

,
which is equivalent to tθu ∈Lp
∗

D(A)

. Therefore tθg ∈Lp
∗(X) so that
tθ−1(S(t)x −x) ∈Lp
∗(X) by virtue of Hardy’s inequality and (4.34) is proved.
Step 2.
We have
tθ−1(S(t)x −x) ∈Lp
∗(X) =⇒x ∈(D(A), X)θ,p.
(4.36)
Let v(t) = tθ−1(S(t)x −x) ∈Lp
∗(X); we have to prove (Proposition 4.5 that
λ1−θAR(λ, A) ∈Lp
∗(X), which is equivalent to show that
AR(λ, A)x = λ
 ∞
0
e−λs(S(s)x −x) ds.
We have
tθ−1AR
1
t , A

x = tθ−2
 ∞
0
e−s/ts1−θv(s) ds =
 ∞
0
v(s)ψ
 t
s
ds
s ,
where ψ(ξ) = ξθ−2e−ξ ∈L1
∗(X). By Young’s inequality it follows that
tθ−1AR(1/t, A)x ∈Lp
∗(X) and the proof is complete.
⊓⊔
4.6 The case of an analytic semigroup
Proposition 4.7. Assume that A is the inﬁnitesimal generator of an analytic
semigroup S of negative type. Then if p ≥1 and θ ∈]0, 1[, we have
(D(A), X)θ,p = {x ∈X : tθAS(t)x ∈Lp
∗(X)}.
(4.37)
Moreover the norm |x|(D(A),X)θ,p is equivalent to
|x|X + ∥tθAS(t)x∥Lp
∗(X).
(4.38)
Proof. Again we proceed in several steps.
Step 1.
We have
tθAS(t)x ∈Lp
∗(X) =⇒t−1(S(t)x −x) ∈Lp
∗(X).
(4.39)
As
t−1(S(t)x −x) = 1
t
 t
0
AS(s)x ds,
if tθAS(t)x ∈Lp
∗(X), we have, by Hardy’s inequality, identity (4.37).

4 Interpolation spaces
165
Step 2.
If v(λ) = λ1−θAR(λ, A) ∈Lp
∗(X), we have t−θAS(1/t)x ∈Lp
∗(X) and then
tθAS(t)x ∈Lp
∗(X). The conclusion follows from Young’s inequality and the
following identity:
t−θAS
1
t

x =
1
2πi

γ
e−λ/t(λ/t)θv(λ)dλ
λ ,
where the path γ is chosen as in §2.7 (Theorem 2.10).
⊓⊔
Remark 4.2. By (4.12) in §4.1, if H is a Hilbert space and A is the inﬁnitesimal
generator of an analytic semigroup, then
W 1,2(0, T ; H) ∩L2
0, T ; D(A)

⊂C

[0, T ]; DA( 1
2, 2)

,
where DA( 1
2, 2) = T (2, 0; D(A), H) ∼= (D(A), H)1/2,2.
⊓⊔
Example 4.2. Let X be a Banach space and set E = Lp(R; X), p ≥1. Then
the linear operator on E deﬁned as
Au = u′,
D(A) = W 1,p(R; X)
(4.40)
is the inﬁnitesimal generator of the semigroup
(S(t)u)(s) = u(t + s).
(4.41)
By Proposition 4.6, it follows that u ∈

D(A), X)

θ,p if and only if
 ∞
0
 +∞
−∞
tθp−p−1|u(t + s) −u(s)|p dt ds < +∞.
(4.42)
We set

W 1,p(R; X), Lp(R; X)

θ,p = W 1−θ,p(R; X),
p ≥1, θ ∈]0, 1[.
(4.43)
⊓⊔
Example 4.3. Let H be a Hilbert space and A be a self-adjoint negative oper-
ator on H. Then A is the inﬁnitesimal generator of an analytic semigroup S
(see §2.2.1). Given the spectral family Eλ associated with A, we have
AS(t)x =
 0
−∞
λeλtdEλ(x),
t > 0, x ∈H
(4.44)
|AS(t)x|2 =
 0
−∞
λ2e2λtd|Eλ(x)|2,
t > 0, x ∈H.
(4.45)
Let x ∈(D(A), H)θ,2. Due to Proposition 4.7, this is equivalent to

166
II-1 Semigroups of Operators and Interpolation
 +∞
0
t2θ−1|AS(t)x|2 dt < ∞.
(4.46)
On the other hand, we have
 +∞
0
t2θ−1|AS(t)x|2 dt =
 0
−∞
d|Eλ(x)|2
 +∞
0
λ2e2λtt2θ−1 dt
= Γ(2θ)2−2θ
 0
−∞
(−λ)2−2θd|Eλ(x)|2
= Γ(2θ)2−2θ|(−A)1−θx|2,
(4.47)
that is
(D(A), H)θ,2 ∼= D

(−A)1−θ
,
(4.48)
where (−A)1−θ is the (1 −θ)-th fractional power of the positive self-adjoint
operator (−A) (cf. F. Riesz and B. Sz.-Nagy [1]).
⊓⊔
4.7 The interpolation space [X, Y ]θ
In §4 we have described the so-called “real interpolation methods” to construct
the interpolation space (X, Y )θ,p. “Complex interpolation methods” can also
be used to obtain interpolation spaces that are denoted by [X, Y ]θ. The nice
feature of [X, Y ]θ is that it only depends on one parameter. In general real
and complex methods yield diﬀerent interpolation spaces (cf. H. Triebel [1,
p. 15 and pp. 55–59]). However, for p = 2 and two Hilbert spaces X and Y
such that X ⊂Y , the spaces coincide
[X, Y ]θ = (X, Y )θ,2
(cf. H. Triebel [1, Remarks 3 and 4, p. 143]).
This discussion is important because in the variational literature, the nota-
tion [X, Y ]θ is widely used. For instance, in J. L. Lions and E. Magenes [1,
Volume I, Chapter 1, §15, p. 108 and comments in §17, pp. 113–114], the
authors use as a deﬁnition of [X, Y ]θ for two Banach spaces X and Y (which
are contained in a locally convex topological vector space Φ, X ⊂Φ, Y ⊂Φ,
with continuous injection of X into Y )
{a: a ∈X + Y, t−(θ+1/2)K(t, a; X, Y ) ∈L2(0, ∞)},
where
K(t, a; X, Y ) =
inf
a0+a1=a[∥a0∥2
X + t2∥a1∥2
Y ]1/2
for a0 ∈X and a1 ∈Y . But this coincides with the deﬁnition of (X, Y )θ,2
given by H. Triebel [1, §1.4.2, p. 29].
As a result in Remark 4.2
DA( 1
2, 2) ∼= (D(A), H)1/2,2 = [D(A), H]1/2
and in Example 4.3 (cf. Equation (4.48))
[D(A), H]θ = (D(A), H)θ,2 ∼= D

(−A)1−θ
.

5 Fractional powers of dissipative operators
167
5 Fractional powers of dissipative operators
We shall not attempt to duplicate the general theory, which is available else-
where. We simply quote a certain number of deﬁnitions and results from
H. Tanabe [1, Chapter 2, §2.3] and A. Pazy [2, §2.6] for operators deﬁned
on a Hilbert space H. Our objective is to use this material in §6 to establish
the connection between the interpolation space [D(A), H]θ = (D(A), H)θ,2
and the domain D

(−A)1−θ
of the fractional power of (−A) for the inﬁnites-
imal generator A of a strongly continuous semigroup. It turns out that when
A is the inﬁnitesimal generator of a strongly continuous semigroup, we shall
always speak of the fractional powers of the operator −A. To avoid carrying
a minus sign everywhere, it is customary to present the theory for a general
operator A instead of −A. With this warning we proceed in the traditional
manner.
Deﬁnition 5.1 (H. Tanabe [1, Deﬁnition 2.3.1, p. 32]). Let A be a closed
linear operator densely deﬁned in a Hilbert space H. The operator A is said
to be of type (ω, M) if:
(i) ∃ω, 0 < ω < π, ∃M ≥1, such that
ρ(A) ⊃{λ: | arg λ| > ω},
∀λ < 0,
∥λ(A −λ)−1∥≤M,
(ii) and ∀ε > 0, ∃Mε > 0 such that
∥λ(A −λ)−1∥≤Mε,
∀λ, | arg λ| > ω + ε.
⊓⊔
Remark 5.1. The type (ω, M) of an operator A is not to be confused with the
type ω0(S) of a strongly continuous semigroup {S(t)} as deﬁned in (2.8) of
§2.2.
⊓⊔
For analytic semigroups {S(t)} such that
∃c > 0,
∃α > 0,
|S(t)x| ≤ce−αt|x|,
∀x ∈H,
(5.1)
with inﬁnitesimal generator A, (−A) is of type (ω, M) for some ω < π/2 and
M > 0 (cf. Assumption A in §2.7). It is also known that an operator A is
maximal dissipative if and only if (−A) is of type (π/2, 1). Thus the class of
operators of type (ω, M) is certainly not empty.
Assume that A is a linear operator of type (ω, M) on H and that A−1 is
bounded. In this case, there exists a neighbourhood U of 0 such that
ρ(A) ⊃S
def
= {λ: | arg λ| > ω} ∪U.
(5.2)
With this assumption we can construct a contour Γ similar to the one in §2.7
and deﬁne for all α > 0 the bounded linear operator
A−α =
1
2πi

Γ
λ−α(A −λ)−1 dλ.
(5.3)

168
II-1 Semigroups of Operators and Interpolation
It is easy to see that Ker[A−α] = {0} and that the following deﬁnition is
meaningful for α > 0:
D(Aα) = R(A−α),
Aα = (A−α)−1.
For α = 0, we set A0 = I.
Proposition 5.1 (H. Tanabe [1, pp. 35–39]). Given a linear operator A of
type (ω, M) such that A−1 is bounded, then:
(i) Aα is a closed operator with a dense domain,
(ii) if 0 < α < β, then D(Aα) ⊃D(Aβ),
(iii) for all α > 0, β > 0
Aα+β = AαAβ = AβAα,
(5.4)
(iv) if 0 < α < 1, then Aα is of type (αω, M),
(v) for 0 ≤α < β ≤1, there exists cα,β > 0 (dependent only on M, α, β)
such that
∀u ∈D(Aβ),
∥Aαu∥≤cα,β∥Aβu∥α/β∥u∥1−α/β.
(5.5)
We now specialize the above results.
Deﬁnition 5.2. The operator A is said to be accretive (resp. maximal accre-
tive) if -A is dissipative (resp. maximal dissipative).
⊓⊔
The reader is referred to §2.6 for a study of the properties of dissipative
operators.
Lemma 5.1 (T. Kato [3, Lemma A.6, p. 272]). If A is a closed and maximal
accretive operator in the Hilbert space H and
∃δ > 0 such that ∀u ∈D(A),
Re(Au, u) ≥δ(u, u),
(5.6)
then for α, 0 < α < 1,
∀u ∈D(Aα),
Re(Aαu, u) ≥δα(u, u).
(5.7)
Theorem 5.1 (T. Kato [3, Theorem 3.1, p. 258]). If A is a closed maximal
accretive operator in the Hilbert space H, then for α, 0 ≤α < 1
2, Aα is a
closed maximal accretive operator and
D(Aα) = D(A∗α)
(5.8)
and there exist constants m1 > 0 and m2 > 0 such that
∀u ∈D(A),
m1∥Aαu∥≤∥A∗αu∥≤m2∥Aαu∥.
(5.9)
For analytic semigroup such that (5.1) is veriﬁed, we have the following
results.

6 Interpolation spaces and domains of fractional powers of an operator
169
Theorem 5.2 (A. Pazy [2, Theorem 6.13, p. 74]). Let −A be the inﬁnitesi-
mal generator of an analytic semigroup T (t). If 0 ∈ρ(A); then:
(i) T (t): X →D(Aα), ∀t > 0, ∀α ≥0,
(ii) ∀x ∈D(Aα), T (t)Aαx = AαT (t)x,
(iii) ∀t > 0, AαT (t) is bounded and there exist Mα > 0 such that
∥AαT (t)∥≤Mαt−αe−δt,
(5.10)
(iv) for all α, 0 < α ≤1,
∃cα > 0,
∀x ∈D(Aα),
∥T (t)x −x∥≤cαtα∥Aαx∥.
(5.11)
6 Interpolation spaces and domains of fractional powers
of an operator
In this section we use the notation [X, Y ]θ as introduced in §4.7 with X and
Y two Hilbert spaces with continuous injection of X into Y .
We have seen that for a negative self-adjoint operator on a Hilbert space,
it is possible to deﬁne the α-th fractional power (−A)α, 0 < α < 1, of −A.
Example 4.3 in §4.6 shows that the domain D((−A)α) of the operator (−A)α
is isomorphic to the interpolation space
[D(A), H]1−α = (D(A), H)1−α,2.
(6.1)
The same type of construction can be repeated for maximal dissipative oper-
ators, and J. L. Lions [2] showed that
D

(−A)α ∼= [D(A), H]1−α,
0 < α < 1.
(6.2)
Moreover it was also known (cf. T. Kato [2, Theorem 3.1, p. 258]) that for
that class of operators
D

(−A)α
= D

(−A∗)α
,
0 < α < 1
2.
(6.3)
These results were extended by A. Yagi [1] to include diﬀerential operators
that are not generally maximal dissipative.
In this section we essentially give an English translation of A. Yagi [1]’s
note to the Comptes Rendus. It is useful to recall the following deﬁnitions,
notation, and relations, which have been used in §3 through §5 (cf. (3.37)–
(3.38), (4.14)–(4.15)), for a Hilbert space H
DA(θ, 2)
def
= T (2, θ; D(A), H),
0 ≤θ ≤1,
[D(A), H]θ = (D(A), H)θ.2,
0 ≤θ ≤1.
Moreover by Proposition 4.3 we know that for all θ, 0 ≤θ <≤1
DA(θ, 2) = T (2, θ; D(A), H) ∼= (D(A), H)θ.2 = [D(A), H]θ.
(6.4)

170
II-1 Semigroups of Operators and Interpolation
Theorem 6.1 (A. Yagi [1]). Let A be a linear operator of type (ω, M) on a
Hilbert space H such that A−1 is bounded in H. Denote by A∗the adjoint of
A. Then the following conditions are equivalent:
(i) for all θ, 0 ≤θ ≤1,
D(Aθ) = [D(A), H]1−θ
and
D(A∗θ) = [D(A∗), H]1−θ;
(6.5)
(ii) there exists α, β, 0 < α, β < 1, such that
D(Aα) ⊂[D(A), H]1−α
and
D(A∗β) ⊂[D(A∗), H]1−β;
(6.6)
(iii) there exists α, β, 0 < α, β < 1, and there exists constants Mα ≥0 and
M ∗
β ≥0 such that
 ∞
0
λ2α−1∥A1−α(λ + A)−1f∥2
H ds
1/2
≤Mα∥f∥H,
∀f ∈H; (6.7)
 ∞
0
λ2β−1∥A∗(1−β)(λ + A)−1g∥2
H dx
1/2
≤M ∗
β∥g∥H,
∀g ∈H. (6.8)
(iv) the holomorphic function A−z : {z : Re z > 0} →L(H, H) (and hence the
holomorphic function A∗(−z)) can be extended to a strongly continuous
function from {z : Re z ≥0} to L(H, H).
Theorem 6.2 (A. Yagi [1]). Assume that A is a linear operator that veriﬁes
the conditions of Theorem 6.1:
(i) If there exists γ, 0 < γ < 1, such that
∀θ,
0 < θ < γ,
[D(A), H]1−θ = [D(A∗), H]1−θ,
(6.9)
then condition (ii) of Theorem 6.1 is veriﬁed.
(ii) If there exists γ, 0 < γ < 1, such that
∀θ,
0 < θ < γ,
D(Aθ) = D(A∗θ),
(6.10)
then condition (iii) of Theorem 6.1 is veriﬁed.
Corollary 6.1. Under the hypotheses of Theorem 6.1 and for some γ, 0 <
γ < 1, the following statements are equivalent:
(i) ∀θ, 0 < θ < γ, [D(A), H]1−θ = [D(A∗), H]1−θ,
(ii) ∀θ, 0 < θ < γ, D(Aθ) = D(A∗θ).
With the above results and Theorem 5.1 in §5 we can now complete the
picture for closed maximal accretive operators.

6 Interpolation spaces and domains of fractional powers of an operator
171
Proposition 6.1. Assume that A is a closed maximal accretive operator in
the Hilbert space H for which A−1 is bounded in H. Then
D(Aθ) = [D(A), H]1−θ, D(A∗θ) = [D(A∗), H]1−θ,
0 ≤θ ≤1,
(6.11)
and
[D(A), H]1−θ = D(Aθ) = D(A∗θ) = [D(A∗), H]1−θ,
∀θ, 0 ≤θ < 1
2. (6.12)
Proof. From Theorem 5.1 D(Aθ) = D(A∗θ), 0 ≤θ < 1/2. Thus from Theo-
rem 6.2(ii) and the equivalence of (i) and (iii) in Theorem 6.1 we obtain (6.11)
and a portion of (6.12).
⊓⊔
Therefore Proposition 6.1 generalizes the result of Example 4.3 in §4
for negative self-adjoint operators that generate an analytic semigroup on
a Hilbert space.
Using the results of P. Grisvard [1], we can now complete the theory
on elliptic operators corresponding to regular boundary problems. We quote
from A. Yagi [1]. Let Ωbe a bounded open domain in Rn with an inﬁnitely
diﬀerentiable boundary Γ. Let A(x; D) be a diﬀerentiable operator of order
2m with inﬁnitely diﬀerentiable coeﬃcients in Ω, and let {Bj(x; D)}1≤j≤m
be m boundary diﬀerential operators of order mj ≤2m −1 with inﬁnitely
diﬀerentiable coeﬃcients on Γ. Assume that
(a) A(x; D) is properly elliptic in Ω(cf. J. L. Lions and E. Magenes [1,
Volume 1, Chapter 2, §1.2, Deﬁnition 1.2]),
(b) {Bj(x; D)}1≤j≤m is a normal system on Γ (cf. J. L. Lions and E. Ma-
genes [1, Volume 1, Chapter 2, §1.4, Deﬁnition 1.4]),
(c) A(x; D) and {βj(x; D)}1≤j≤m verify the conditions in H. Tanabe [1,
Chapter 3, Theorem 3.8.1] for θ = π.
Under the above assumptions deﬁne an operator A in L2(Ω) as follows:
D(A) = {u ∈H2m(Ω): Bj(x; D)u = 0
on Γ, 1 ≤j ≤m},
Au = A(x; D)u.
(6.13)
A is called a realization of A(x; D) in L2(Ω) under the boundary conditions
{Bj(x; D)}1≤j≤m. Its adjoint is given by
D(A∗) = {v ∈H2m(Ω): Cj(x; D)v = 0
on Γ, 1 ≤j ≤m},
A∗v = A(x; D)∗v,
(6.14)
where {Cj(x; D)}1≤j≤m is the adjoint system of {Bj(x; D)}1≤j≤m. The space
D(A) (resp. D(A∗)) will also be denoted H2m
B (Ω) (resp. H2m
C (Ω)) when they
are identiﬁed with a subspace of H2m(Ω). In fact from the a priori estimates
for elliptic operators the two spaces coincide because the graph norm of D(A)
(resp. D(A∗)) is equivalent to the one of H2m(Ω).
To check condition (i) in Theorem 6.2, it is suﬃcient to ﬁnd a constant γ,
0 < γ < 1, such that

172
II-1 Semigroups of Operators and Interpolation

H2m
B (Ω), H0(Ω)

1−θ =

H2m
C (Ω), H0(Ω)

1−θ ,
0 < θ < γ.
(6.15)
But according to P. Grisvard [1] or J. L. Lions [3, Volume 2, Chap-
ter 4,Theorem 14.4], the interpolation spaces are speciﬁcally characterized
by

H2m
B (Ω), H0(Ω)

1−θ
=

u ∈H2mθ(Ω):
Bj(x; D)u = 0
on Γ,
mj < 2mθ −1
2
Bj(x; D)u ∈L2
−1/2(Ω),
mj = 2mθ−1
2

,
where L2
−1/2(Ω) denotes the space of functions ϕ such that d(x, Γ)−1/2ϕ ∈
L2(Ω). A similar characterization holds for [H2m
C (Ω), H0(Ω)]1−θ. So in partic-
ular for all θ, 0 < θ < 1/4m,

H2m
B (Ω), H0(Ω)

1−θ = H2mθ(Ω) =

H2m
C (Ω), H0(Ω)

1−θ .
(6.16)
Hence from Theorem 6.2 (i)
∀θ,
0 < θ <
1
4m,
D(Aθ) = D(A∗θ)
(6.17)
and from the equivalence of (i) and (ii) in Theorem 6.1 for all θ, 0 ≤θ ≤1,
D(Aθ) =

H2m
B (Ω), H0(Ω)

1−θ ,
D(A∗θ) =

H2m
C (Ω), H0(Ω)

1−θ .
Acknowledgments
The authors would like to thank A. Lunardi for useful discussions on interpo-
lation spaces.

2
Variational Theory of Parabolic Systems
1 Variational diﬀerential equations
A complete treatment of variational diﬀerential equations is beyond the scope
of this book. We shall mainly quote some results from the books of J. L. Li-
ons and E. Magenes [1]. In order to motivate the chosen constructions and
models, we give a series of classical examples. We assume that the reader is
familiar with Sobolev spaces and their properties.
In this chapter we use a notation that is slightly diﬀerent from Chapter 1.
This notation is standard in books using the “variational theory.”
Notation 1.1. Let X and Y be two Hilbert spaces such that X ⊂Y . In that
case the interpolation spaces (X, Y )θ,p with p = 2 and [X, Y ]θ coincide for
0 < θ < 1 (cf. Chapter 1 §4.7). We shall use the notation [X, Y ]θ, 0 < θ < 1.
⊓⊔
In addition recall that the operator A that will be used throughout this
chapter will correspond to −A in Chapter 1 whenever it describes the same
system (cf. Chapter 1, §5 and §6).
1.1 Distributed control
Let T > 0 be a real number and let Ωbe a bounded open subset of Rn (n ≥1,
an integer) with “suﬃciently smooth” boundary Γ. We use Γ rather than ∂Ω
as was done in Chapter 1 because the notation is more in line with the liter-
ature on variational equations. Following J. L. Lions and E. Magenes [1],
introduce the cylinder
Q = Ω× ]0, T [,
Σ = Γ × ]0, T [,
(1.1)
the coeﬃcients
aij ∈L∞(Q),
1 ≤i, j ≤n,
a0 ∈L∞(Q)
(1.2)

174
II-2 Variational Theory of Parabolic Systems
and the bilinear form
a(t; ϕ, ψ) =
n

i,j=1

Ω
aij(x, t) ∂ϕ
∂xj
∂ψ
∂xi
dx +

Ω
a0(x, t)ϕψ dx.
(1.3)
1.1.1 Homogeneous Dirichlet boundary conditions
Choose L2(Q) or equivalently L2
0, T ; L2(Ω)

as space of control functions u.
Consider the boundary value problem
⎧
⎪
⎪
⎨
⎪
⎪
⎩
A(t)y + ∂y
∂t = u
in Q,
y = 0
on Σ,
y(x, 0) = y0(x)
in Ω,
(1.4)
where
A(t)φ = −
n

i,j=1
∂
∂xi

aij(x, t) ∂φ
∂xj

+ a0(x, t)φ.
(1.5)
Introduce the spaces
V = H1
0(Ω),
H = L2(Ω)
(1.6)
and the continuous dense injection
i: V →H.
(1.7)
Identify the elements of the dual H′ of H with those of H. This results in the
injections
V
i→H ≡H′ i∗
→V ′,
(1.8)
where i∗denotes the (topological) transposed of the continuous linear injection
i.
The system of equations (1.4) can now be written as a “variational diﬀer-
ential equation” in V ′, the topological dual of V . A more precise deﬁnition
of this terminology will be given in §2. To see this multiply both sides of the
ﬁrst equation (1.4) by an arbitrary element v in V and use Green’s formula
for the term A(t)y

Ω
A(t)y(t)v dx = a(t; y(t), v).
(1.9)
For all v in V
a(t; y(t), v) +

Ω
∂y
∂t v dx = (u(t), iv),
y(0) = y0,
(1.10)
where (·, ·) denotes the inner product in H. This readily suggests to interpret
the time derivative of y as an element dy/dt of V ′ because the linear functional

1 Variational diﬀerential equations
175
v 	→

Ω
∂y
∂t v dx = ⟨i∗u(t), v⟩V −a(t; y(t), v): V →R
(1.11)
is continuous (⟨·, ·⟩V denotes the duality pairing between V ′ and V ). Thus
(1.10) is equivalent to the following equation in V ′:
A(t)y + dy
dt = i∗u,
y(0) = y0,
(1.12)
where A(t) is now interpreted as the continuous linear map from V to V ′
deﬁned by
⟨A(t)φ, ψ⟩V = a(t, φ, ψ).
(1.13)
The idea is now to look for a solution y of (1.12) in the space
W(0, T ) =

φ ∈L2(0, T ; V ): dφ
dt ∈L2(0, T ; V ′)

,
(1.14)
where the time derivative must be carefully interpreted as a distributional
derivative with values in V ′. It can be shown that system (1.12) has a unique
solution y in W(0, T ) for every y0 in H under the following “coercivity” hy-
pothesis:
∃α > 0
such that ∀ξ ∈Rn,
n

i,j=1
aij(x, t)ξiξj ≥α
n

i=1
ξ2
i ,
a.e. in Q.
(1.15)
From the control point of view, the space of controls is U = L2(Ω) = H, the
space of control functions is U = L2(0, T ; U), and the control operator
B : U →H
(1.16)
is the identity operator. With this notation system (1.12) can be rewritten as
A(t)y + dy
dt = i∗Bu,
y(0) = y0.
(1.17)
In general when the control operator is continuous on H = L2(Ω) we say that
it is a “distributed control operator.”
1.1.2 Homogeneous Neumann boundary condition
As in the ﬁrst example we choose control functions u in L2
0, T ; L2(Ω)

and
consider the following boundary value problem:
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
A(t)y + ∂y
∂t = u
in Q,
∂y
∂νA
= 0
on Σ,
y(x, 0) = y0(x)
in Ω,
(1.18)

176
II-2 Variational Theory of Parabolic Systems
where A(t) is given by (1.5),
ν = (ν1, . . . , νn)
is the unit external normal to Γ, and
∂y
∂νA
=
n

i,j=1
aij(x, t) ∂y
∂xj
νj
(1.19)
is the conormal derivative of y with respect to the operator A. Here we choose
V = H1(Ω),
H = L2(Ω)
(1.20)
and still denote by i the continuous injection of V into H. We proceed as in
the ﬁrst example and obtain the variational diﬀerential equation (1.17). To
obtain the existence of a solution here, we need to add to hypothesis (1.15)
that
a0(x, t) ≥α,
a.e. in Q.
(1.21)
1.2 Boundary control condition
1.2.1 Control through a Neumann condition
In this example the space of control functions u is L2(Σ) or equivalently
L2(0, T ; U) with U = L2(Γ). The associated boundary value problem is
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
A(t)y + ∂y
∂t = 0
in Q,
∂y
∂νA
= u
on Σ,
y(x, 0) = y0(x)
in Ω,
(1.22)
where A(t) is given by (1.5) and ∂y/∂νA is the conormal derivative (1.19)
with respect to the operator A. We choose the same space V and H as in the
example of §1.1.2.
To transform system (1.22) into a variational diﬀerential equation in V ′
we again use Green’s formula: For all v in V = H1(Ω)

Ω
A(t)y(t)v dx = a(t; y(t), v) −

Γ
∂y
∂νA
v dΓ
= a(t; y(t), v) −

Γ
u(t)v dΓ.
So for all v in V
a(t; y(t), v) +

Ω
∂y
∂t v dx =

Γ
u(t)v dΓ,
y(0) = y0.
(1.23)

1 Variational diﬀerential equations
177
Here we deﬁne the control operator B : U →V ′ as follows:
⟨Bu, φ⟩V =

Γ
uφ|Γ dΓ,
u ∈L2(Γ), φ ∈V.
(1.24)
It is linear and continuous because the trace operator
φ →φ|Γ : V = H1(Ω) →H1/2(Γ)
(1.25)
and the injection of H1/2(Γ) into L2(Γ) are continuous. The ﬁnal result is
the variational diﬀerential equation
A(t)y + dy
dt = Bu,
y(0) = y0.
(1.26)
By its very nature, B is a “boundary control operator.” It is never a continuous
linear map on L2(Ω). System (1.26) has a unique solution in W(0, T ) under
hypotheses (1.15) and (1.21).
1.2.2 Control through a Dirichlet condition
Consider the boundary value problem
⎧
⎪
⎪
⎨
⎪
⎪
⎩
A(t)y + ∂y
∂t = 0
in Q,
y = u
on Σ,
y(x, 0) = y0(x)
in Ω,
where A(t) is given by (1.5). Here the control enters as a Dirichlet condition
on Γ. This problem cannot be transformed into a variational problem by
integration by parts. We shall see in §2.1 and §2.4 that this more delicate
problem can be handled by the Method of Transposition. However, after a
change of variable, it can also be put in variational form.
1.2.3 Point controls
Another boundary value problem that is not amenable to a variational equa-
tion by integration by parts is when the control is achieved through a ﬁnite
number of points (or locations) in the domain Ω. We shall see in §2.2 and §2.4
that this problem can be handled by the Method of Transposition for domains
in Rn, n ≤3.
1.3 Main theorem
The main theorem that we shall quote is one of several isomorphism theorems,
which can be found in J. L. Lions and E. Magenes [1], Volume 1, Chapter 3,
Theorem 1.1 and Examples in §4.7].

178
II-2 Variational Theory of Parabolic Systems
Let V and H be two Hilbert spaces with the following notation:
| · | (resp. ∥· ∥) is the norm in H (resp. V ),
H′ (resp. V ′) is the topological dual of H (resp. V ),
(·, ·) is the inner product in H,
⟨·, ·⟩V is the duality pairing on V ′ × V.
We assume that there is a continuous injection i: V →H and we identify the
elements of H′ with those of H:
V
i→H ≡H′ →V ′.
(1.27)
Given a ﬁxed real number T > 0 and a family of continuous linear operators
A(t) ∈L(V, V ′),
0 ≤t ≤T,
(1.28)
we make the following assumptions:
∀v, w ∈V,
t 	→⟨A(t)v, w⟩V is measurable on ]0, T [
(1.29)
and for all t ∈[0, T ], ∃c > 0, such that
|⟨A(t)v, w⟩V | ≤c∥v∥: ∥w∥
(1.30)
and ∃α > 0 and ∃λ ∈R such that
∀v ∈V,
⟨A(t)v, v⟩V + λ|v|2 ≥α∥v∥2.
(1.31)
Assumption (1.31) is known as the V –H coercivity of A. Deﬁne the space
W(0, T ) =

y ∈L2(0, T ; V ): dy
dt ∈L2(0, T ; V ′)

,
(1.32)
where the derivative is to be interpreted as a vectorial distributional derivative
with values in V ′.
Given
f ∈L2(0, T ; V ′),
y0 ∈H,
(1.33)
we consider the variational diﬀerential equation
A(t)y + dy
dt = f,
y(0) = y0.
(1.34)
Notation 1.2. In this chapter we use the notation A(t) of the variational lit-
erature. When A(t) = A (independent of t), the inﬁnitesimal generator of the
associated semigroup will be −A (cf. Chapter 1, §2.7, Thm 2.10).
⊓⊔
Theorem 1.1. Under assumptions (1.27) through (1.33), equation (1.34) has
a unique solution y in W(0, T ) and the map
y 	→

A(t)y + dy
dt , y(0)

: W(0, T ) →L2(0, T ; V ′) × H
(1.35)
is an (algebraic and topological isomorphism).

1 Variational diﬀerential equations
179
Proof. See J. L. Lions [4, p. 116, Theorem 1.2] for the case where V is
separable. For a more general and modern proof, the reader is referred to
J. L. Lions and E. Magenes [1, Volume 1, Chapter 3, §4, Theorem 4.1].
⊓⊔
Remark 1.1. It is important to notice that the operators A(t) are neither
assumed to be self-adjoint nor invertible. In particular when H = Rn (n ≥1,
an integer) and the elements of the matrix A are L∞-functions, the V –H
coercivity assumption (1.31) is veriﬁed along with all other hypotheses.
⊓⊔
Each element of W(0, T ) can be identiﬁed with a unique function in
C([0, T ]; H) and the injection
W(0, T ) →C([0, T ]; H)
(1.36)
is continuous (cf. J. L. Lions and E. Magenes [1, Volume 1, Proposition 2.1,
p. 18 and Theorem 3.1, p. 19]). This gives a pointwise meaning to y(t), 0 ≤
t ≤T , as an element of the space H so that y(0) in (1.34) is well deﬁned.
If we deﬁne the adjoint operator A∗(t)
⟨A∗(t)v, w⟩V = ⟨A(t)w, v⟩V ,
(1.37)
then the map
v 	→

A∗(t)v −dv
dt , v(T )

: W(0, T ) →L2(0, T ; V ′) × H
(1.38)
is also an isomorphism. In §2, we shall see how a smooth version of the “adjoint
isomorphism’” can be “transposed” to deal with examples that cannot be
directly modeled by the variational diﬀerential equation (1.34).
1.4 A perturbation theorem
Theorem 1.2. Assume that the hypotheses of Theorem 1.1 are veriﬁed. Let
θ ∈[0, 1] and
K(t): [V, H]θ →[V, H]′
1−θ,
0 ≤t ≤T,
(1.39)
be a family of continuous linear operators such that
∀v ∈[V, H]θ,
t 	→K(t)v belongs to L∞(0, T ; [V, H]′
1−θ).
(1.40)
Then the operator A + K is V –H coercive and the variational diﬀerential
equation
[A(t) + K(t)]y + dy
dt = f ∈L2(0, T ; V ′),
y(0) = y0 ∈H
(1.41)
has a unique solution y in W(0, T ).

180
II-2 Variational Theory of Parabolic Systems
Proof. It is suﬃcient to show that A + K is V –H coercive: there exist β > 0
and µ ∈R such that
∀v ∈V,
⟨[A(t) + K(t)]v, v⟩V + µ|v|2 ≥β∥v∥2.
By hypothesis on A
∃α > 0,
∃λ ∈R
such that ∀v ∈V,
⟨A(t)v, v⟩V + λ|v|2 ≥α∥v∥2.
So for any µ ∈R
aµ = ⟨[A(t) + K(t)]v, v⟩V + µ|v|2
≥α∥v∥2 + (µ −λ)|v|2 −∥K(t)∥L([V,H]θ,[V,H]′
1−θ)|v|θ|v|1−θ,
where |v|θ is the norm in [V, H]θ. By hypothesis (1.40), there exists a constant
k > 0 such that
ess sup
[0,T ]
|K(t)|L([V,H]θ,[V,H]′
1−θ) ≤k.
Moreover from J. L. Lions and E. Magenes [1, Volume 1, Proposition 2.3]
or Proposition 5.1(v) in Chapter 1 (cf. also §4.3)
∀v ∈V,
|v|θ = |v|[V,H]θ ≤c∥v∥θ|v|1−θ.
Therefore there exists c > 0 such that
k|v|θ|v|1−θ ≤c∥v∥|v|
and
c∥v∥|v| = 2c1
2
+α
c ∥v∥
+ c
α|v| ≤c
α
4 ∥v∥2 + c
α|v|2
	
= α
4 ∥v∥2 + c2
α |v|2.
Finally
aµ ≥α∥v∥2 + (µ −λ)|v|2 −α
4 ∥v∥2 −c2
α |v|2 = 3α
4 ∥v∥2 +

µ −λ −c2
α

|v|2
≥3
4α∥v∥2
for µ ≥λ + c2/α. So we can choose β = 3α/4, and this completes the proof
of the V –H coercivity of A + K.
⊓⊔
1.5 A regularity theorem
We now present a regularity result for variational diﬀerential equations due to
C. Bardos [1]. This type of result is useful at various stages in the analysis of
optimal control problems. It can also be used in the construction of “smooth

1 Variational diﬀerential equations
181
adjoint isomorphisms” that are the primary ingredients in the “Method of
Transposition.”
We go back to the deﬁnitions, notations, and hypotheses (1.27) to (1.32)
of §1.3. We denote by
Aλ(t) = A(t) + λI
(1.42)
the operator associated with the λ for which the V –H coercivity assumption
is veriﬁed. So for each t
Aλ(t) ∈L(V, V ′)
(1.43)
is an isomorphism that also induces an isomorphism
Aλ ∈L

L2(0, T ; V ), L2(0, T ; V ′)

,
(Aλv)(t) = Aλ(t)v(t).
(1.44)
For each t ∈[0, T ], we deﬁne the domains of A(t), A, Aλ(t), and Aλ
D

A(t)

= {v ∈V : A(t)v ∈H},
D(A) = {w ∈L2(0, T ; V ): Aw ∈L2(0, T ; H)},
D

Aλ(t)

= {v ∈V : Aλ(t)v ∈H},
D(Aλ) = {w ∈L2(0, T ; V ): Aλw ∈L2(0, T ; H)}.
It is readily seen that
D

Aλ(t)

= D

A(t)

,
D(Aλ) = D(A).
(1.45)
The unbounded operator
Aλ(t): D

A(t)

→H

resp. Aλ : D(A) →L2(0, T ; H)

(1.46)
is an isomorphism when D

A(t)

(resp. D(A)) is embedded with the graph
norm topology of A(t) (resp. A) (cf. T. Kato [2], T. Kato [3], J. L. Li-
ons [2]).
Deﬁne the adjoints A∗(t) and A∗of A(t) and A
⟨A∗(t)v, w⟩V = ⟨A(t)w, v⟩V ,
∀v, w ∈V,
⟨A∗f, g⟩L2(0,T ;V ) = ⟨Ag, f⟩L2(0,T ;V ),
∀f, g ∈L2(0, T ; V ).
(1.47)
Similar deﬁnitions for Aλ(t) and Aλ yield
A∗
λ(t) = A∗(t) −λI,
A∗
λ = A∗
λ −λI.
(1.48)
Moreover, for t ∈[0, T ], Aλ(t) and Aλ are isomorphisms
A∗
λ(t) ∈L(V, V ′),
A∗
λ ∈L

L2(0, T ; V ), L2(0, T ; V ′)

.
(1.49)
As we did for A deﬁne
D

A∗(t)

= {v ∈V : A∗(t)v ∈H},
D(A∗) = {f ∈L2(0, T ; V ): A∗f ∈L2(0, T ; H)}.
(1.50)

182
II-2 Variational Theory of Parabolic Systems
Analogous deﬁnitions for Aλ(t) and Aλ yield
D

A∗
λ(t)

= D

A∗(t)

,
D(A∗
λ) = D(A∗),
(1.51)
and the unbounded linear operator
A∗
λ(t): D

A∗(t)

→H,
A∗
λ : D(A∗) →L2(0, T ; H)
(1.52)
are isomorphisms when D

A∗(t)

and D(A∗) are endowed with their respec-
tive graphs norm topology.
The adjoint of Aλ(t)∗(resp. A∗
λ) can be considered as an extension of Aλ(t)
(resp. Aλ) because it coincides with it on D

Aλ(t)

(resp. D(Aλ)). With that
convention the linear maps
Aλ(t) ∈L

D

A(t)

, H

∩L(V, V ′) ∩L

H, D

A∗(t)
′
,
Aλ ∈L

D(A), L2(0, T ; H)

∩L

L2(0, T ; V ), L2(0, T ; V ′)

∩L(L2(0, T ; H), D(A∗)′)
(1.53)
are isomorphisms. Similarly,
A∗
λ(t) ∈L(D(A∗(t), H) ∩L(V, V ′) ∩L

H, D

A(t)
′
,
A∗
λ ∈L

D(A∗), L2(0, T ; H)

∩L

L2(0, T ; V ), L2(0, T ; V ′)

∩L(L2(0, T ; H), D(A)′)
(1.54)
are isomorphisms.
Remark 1.2. It is possible to interpolate between the diﬀerent isomorphisms
(1.53). So for each α ∈[0, 1] the maps
Aλ(t) ∈L

D

A(t)

, H

α,

D

A∗(t)

, H
′
1−α

,
Aλ ∈L([D(A), L2(0, T ; H)]α, [D(A∗), L2(0, T ; H)]′
1−α)
(1.55)
are isomorphisms. However it is generally not true that

D

Aλ(t)

, H

1/2 =

D

A∗
λ(t)

, H

1/2 = V
or
[D(Aλ), L2(0, T ; H)]1/2 = [D(A∗
λ), L2(0, T ; H)]1/2 = L2(0, T ; V ).
⊓⊔
As Aλ(t) is maximal positive (that is, −Aλ(t) is the generator of a strongly
continuous contraction semigroup), the fractional powers Aα
λ(t) and A∗α
λ (t),
0 < α < 1, are well deﬁned (cf. Part II, Chapter 1, §5 and T. Kato [1,
T. Kato [2]). They are also isomorphisms
Aα
λ(t): D

Aα
λ(t)

→H,
A∗α
λ (t): D

A∗α
λ (t)

→H,
Aα
λ : D(Aα
λ) →L2(0, T ; H),
A∗α
λ : D(A∗α
λ ) →L2(0, T ; H)
(1.56)

1 Variational diﬀerential equations
183
whose domains coincide with the appropriate interpolation spaces (cf. Part II,
Chapter 1, §6, Proposition 6.1)
D

Aα
λ(t)

=

D

A(t)

, H

1−α,
D(Aα
λ)=[D(A), L2(0, T ; H)]1−α,
D

A∗α
λ (t)

=

D

A∗(t)

, H

1−α, D(A∗α
λ )=[D(A∗), L2(0, T ; H)]1−α. (1.57)
Remark 1.3. In view of Remark 1.3 and (1.56)–(1.57), for all α ∈[0, 1]
Aλ(t) ∈L

D

A1−α
λ
(t)

, D

A∗α
λ (t)
′
,
Aλ ∈L

D(A1−α
λ
), D(A∗α
λ )′
. (1.58)
⊓⊔
J. L. Lions [2] has shown that
D

Aα
λ(t)

= D

A∗α
λ (t)

,
0 ≤α < 1
2
(1.59)
and given suﬃcient conditions under which
D

A1/2
λ (t)

= D

A∗1/2
λ
(t)

= V.
(1.60)
One of them is
D

A∗
λ(t)

= D

Aλ(t)

;
(1.61)
another one is more technical but covers the so-called regular elliptic boundary
value problems.
Theorem 1.3. Assume that there exists a Hilbert space X, X ⊂H, such that
V is a closed subspace of [X, H]1/2, D

Aλ(t)

⊂X, D

A∗
λ(t)

⊂X. (1.62)
Then

D

Aλ(t)

, H

1/2 = D

A1/2
λ (t)

= V,

D

A∗
λ(t)

, H

1/2 = D

A∗1/2
λ
(t)

= V.
(1.63)
Proof. Cf. J. L. Lions [2, Theorem 6.1, p. 238] and Proposition 6.1 in Part II,
Chapter 1, §6.
⊓⊔
Example 1.1. Let Ωbe a bounded open subset of Rn with smooth boundary
Γ. Let V be a closed subspace of Hm(Ω) such that
Hm
0 (Ω) ⊂V ⊂Hm(Ω).
(1.64)
Assume that A(t) = A is generated by the bilinear form
a(u, v) =

|p|,|q|≤m

Ω
apq(x)DquDpv dx,
⟨Au, v⟩V = a(u, v),
∃α > 0,
∀v ∈V, a(v, v) ≥α∥v∥2

184
II-2 Variational Theory of Parabolic Systems
for suﬃciently smooth functions apq on Ωand an appropriate choice of V (cf.
J. L. Lions [2, Section 6.2, p. 239] for more details).
The triplet {V, H, a(u, v)} is said to be “regular” if
D(A) ⊂H2m(Ω),
D(A∗) ⊂H2m(Ω).
(1.65)
⊓⊔
Remark 1.4. (1962, J. L. Lions [2, Remark 6.1, p. 240]) It is important to
recall that for mixed boundary value problems the relationship (1.63) does
not hold. For instance it is not known whether
D(A1/2) = D(A∗1/2)
is true for a second-order elliptic operator that is not self-adjoint with a Dirich-
let condition on part of the boundary and a Neumann condition on the other
part. This also applies to the Dirichlet boundary value problem with an irreg-
ular boundary.
⊓⊔
Remark 1.5. In 2002, P. Auscher, S. Hofmann, J. L. Lewis, and Ph. Tcha-
mitchian [1] proved Kato’s conjecture for elliptic operators on Rn. More pre-
cisely, let M = M(x) be an n × n matrix of complex, L∞coeﬃcients, deﬁned
on Rn, and satisfying the ellipticity (or “accretivity”) condition
λ |ξ|2 ≤Re Mξ · ξ∗and |Mξ · ζ∗| ≤Λ |ξ| |ζ|,
(1.66)
for ξ, ζ in Cn and for some λ, Λ such that 0 < λ ≤Λ < ∞; that is, Mξ · ζ∗=
,
j,k aj,k(x)ξkζj. Deﬁne
Af
def
= −div(M∇f),
(1.67)
interpreted in the usual weak sense via a sesquilinear form. Under the accretiv-
ity condition (1.66), one can deﬁne a square root A1/2 =
√
A. They establish
that the domain of the square root of A is the Sobolev space H1(Rn) in any
dimension with the estimate ∥
√
Af∥2 ∼∥∇f∥2. A version of that result also
holds for an operator A with lower order terms (Theorem 6.1).
⊓⊔
In view of identities (1.59), we can rewrite the isomorphisms (1.58) as
follows: For 0 ≤α < 1
2
Aλ(t)∈L

D

A1−α
λ
(t)

, D

Aα
λ(t)
′
,
Aλ ∈L(D(A1−α
λ
), D(Aα
λ)′).
(1.68)
A∗
λ(t)∈L

D(A∗(1−α)
λ
(t), D

A∗α
λ (t)
′
, A∗
λ ∈L

D(A∗(1−α)
λ
), D(A∗α
λ )′
. (1.69)
The next proposition completes the picture.
Proposition 1.1. Assume that
D

A1/2
λ (t)

= D

A∗1/2
λ
(t)

.
(1.70)

1 Variational diﬀerential equations
185
Then

D

A(t)

, H

1/2 = D

A1/2
λ (t)

= V,

D

A∗(t)

, H

1/2 = D

A∗1/2
λ
(t)

= V,
(1.71)
and
A1/2
λ (t) ∈L(V, H),
A∗1/2
λ
(t) ∈L(V, H)
(1.72)
are isomorphisms.
Proof. Cf. J. L. Lions [2, Corollary 5.2].
⊓⊔
Corollary 1.1. The following maps are isomorphisms:
A1/2
λ (t) ∈L

D

A(t)

, V

∩L(V, H) ∩L(H, V ′) ∩L

V ′, D

A∗(t)
′
,
A∗1/2
λ
(t) ∈L

D

A∗(t)

, V

∩L(V, H) ∩L(H, V ′) ∩L

V ′, D

A(t)
′
,
(1.73)
A1/2
λ
∈L(D(A), V) ∩L(V, H) ∩L(H, V′) ∩L(V′, D(A∗)′),
A∗1/2
λ
∈L(D(A∗), V) ∩L(V, H) ∩L(H, V′) ∩L(V′, D(A)′),
(1.74)
where
V = L2(0, T ; V ),
H = L2(0, T ; H) ≡H′,
V′ = L2(0, T ; V ′).
(1.75)
Given two Banach spaces X and Y and a continuous dense injection i from
X to Y , deﬁne
W(0, T ; X, Y ) =

y ∈L2(0, T ; X): dy
dt ∈L2(0, T ; Y )

,
(1.76)
where dy/dt is to be interpreted as the vectorial distributional derivative of
the function iy, (iy)(t) = iy(t). Similarly let
W(0, T ; D(A), H) =

y ∈D(A): dy
dt ∈L2(0, T ; H)

.
(1.77)
Our next objective is to show that under appropriate conditions on A(t), the
linear operator
A1/2
λ
: W(0, T ; D(A), H) →W(0, T ; V, V ′)
(1.78)
is an isomorphism. This will ﬁrst necessitate giving a meaning to the time
derivative Aλ(t)′ of the operator valued function Aλ(t).
Proposition 1.2. Assume that hypotheses (1.27) to (1.31) and hypothesis
(1.71) are veriﬁed. Assume that the operator-valued function
A: [0, T ] →L(V, V ′)
(1.79)
has a derivative A(t)′ in L(V, V ′) almost everywhere in [0, T ] such that

186
II-2 Variational Theory of Parabolic Systems
∃c > 0,
∥A(t)′∥L(V,V ′) ≤c,
a.e. in [0, T ]
(1.80)
and for all v and w in V
t 	→⟨A(t)v, w⟩V belongs to W 1,∞(0, T ; R),
(1.81)
d
dt⟨A(t)v, w⟩V = ⟨A(t)′v, w⟩V .
(1.82)
Then A1/2
λ
: [0, T ] →L(V, V ′) has a derivative A1/2
λ (t)′ ∈L(V, V ′) almost
everywhere in [0, T ] such that
∃c > 0,
∥A1/2
λ (t)′∥L(V,V ′) ≤c
(1.83)
and for all v and w in V
t 	→⟨A1/2
λ (t)v, w⟩V belongs to W 1,∞(0, T ; R),
(1.84)
d
dt⟨A1/2
λ (t)v, w⟩V = ⟨A1/2
λ (t)′v, w⟩V .
(1.85)
Proof. Minor adaptation of C. Bardos [1, Proposition 1.1].
⊓⊔
This proposition gives a meaning to the derivative A1/2
λ (t)′ of the operator
valued function A1/2
λ (t).
Example 1.2. When A(t) is given by a bilinear form
a(t; u, v) =
n

i,j=1
aij(x, t) ∂u
∂xj
∂v
∂xi
+ a0(x, t)uv,
where
a0, ∂a0
∂t ∈L∞(Q),
aij ∈W 1,∞(Q),
then
⟨A′(t)u, v⟩V =
n

i,j=1
∂aij
∂t (x, t) ∂u
∂xj
∂v
∂xi
+ ∂a0
∂t (x, t)uv
and conditions (1.73) and (1.74) are veriﬁed.
⊓⊔
Let y belong to W(0, T ; D(Aλ), H). Then from Proposition 1.1, A1/2
λ y ∈
L2(0, T ; V ). In view of Propositions 1.2 and 1.1
d
dtA1/2
λ y = A1/2
λ
dy
dt + (A1/2
λ )′y ∈L2(0, T ; V ′)
(1.86)
because
dy
dt ∈L2(0, T ; H) =⇒A1/2
λ
dy
dt ∈L2(0, T ; V ′),
y ∈L2
0, T ; D(Aλ)

⊂L2(0, T ; V ) =⇒(A1/2
λ )′y ∈L2(0, T ; H)⊂L2(0, T ; V ′).
By construction, the mapping is continuous. The surjectivity is obvious.

1 Variational diﬀerential equations
187
Proposition 1.3. Under hypotheses (1.27) to (1.31) and (1.71) on A and
hypotheses (1.80) to (1.82) on A′, the linear map
A1/2
λ
: W(0, T ; D(Aλ), H) →W(0, T ; V, V ′)
(1.87)
is an isomorphism.
Corollary 1.2. Under the hypotheses of Proposition 1.3, A1/2
λ
is also an iso-
morphism from W(0, T ; V, V ′) to W(0, T ; H, D(A∗
λ)′), where
W(0, T ; H, D(A∗
λ)′) =

y ∈L2(0, T ; H): dy
dt ∈D(A∗
λ)′

.
(1.88)
We ﬁnally quote the regularity result from C. Bardos [1].
Theorem 1.4. Assume that hypotheses (1.27) to (1.31) and (1.71) on A and
hypotheses (1.80) to (1.82) on A′ are veriﬁed. Consider the variational diﬀer-
ential equation
A(t)y + dy
dt = f,
y(0) = y0
(1.89)
for f in L2(0, T ; H) and y0 in V :
(i) Then (1.89) has a unique solution y in W(0, T ; D(Aλ), H) and the map-
ping
y 	→

A(t)y + dy
dt , y(0)

: W(0, T ; D(Aλ), H) →L2(0, T ; H)×V (1.90)
is an isomorphism.
(ii) Moreover if D

Aλ(t)

is independent of t, the solution y of (1.89) belongs
to C([0, T ]; V ).
Proof. Slight adaptation of the proof of C. Bardos [1] or directly by using
the isomorphism Aλ of Proposition 1.3, Theorem 1.2 and Theorem 1.1.
⊓⊔
Corollary 1.3. Assume that the hypothesis of Theorem 1.3 are veriﬁed. Then
the mapping
v 	→

A∗(t)v −dv
dt , v(T )

: W(0, T ; D(A∗
λ), H) →L2(0, T ; H) × V
(1.91)
is an isomorphism.
Remark 1.6. The isomorphism in part (i) for time-varying systems is to be
compared with the ones in Theorem 3.1 in Chapter 1 and Theorem 1.1 in
this chapter for time-invariant analytic semigroups in a Hilbert space and the
so-called maximal regularity results.
⊓⊔

188
II-2 Variational Theory of Parabolic Systems
Example 1.3. Let Ωbe a bounded open subset of Rn with a C∞boundary.
Let a(t; u, v) be the bilinear form (1.3),
V = H0(Ω),
⟨A(t)u, v⟩V = a(t; u, v).
Assume that
a0,
∂a0
∂t ∈L∞(Q),
aij ∈W 1,∞(Q),
1 ≤i, j ≤n,
and
∃α > 0,
∀ξ ∈Rn,
n

i,j=1
aij(x, t)ξjξi ≥α
n

ı=1
ξ2
i .
The operator A is V –H coercive. Moreover for all u, v in V
-
A(t)
′u, v
.
V =
n

i,j=1
∂aij
∂t (x, t) ∂u
∂xj
∂v
∂xi
+ ∂a0
∂t (x, t)uv
belongs to L∞(0, T ; R) and

A(t)
′ veriﬁes (1.79) and (1.80). Moreover
H1
0(Ω) = V ⊂H1(Ω)
and
D

A∗
λ(t)

= D

Aλ(t)

= H2(Ω) ∩H1
0(Ω) ⊂H2(Ω).
So the last corollary applies to this problem. Here
D(A∗
λ) = L2(0, T ; D),
D = H2(Ω) ∩H1
0(Ω)
and the isomorphism (1.90) goes between the spaces
W(0, T ; D, H) →L2(0, T ; H) × V.
⊓⊔
The reader interested in additional regularity theorems is referred to The-
orems 1.2 and 1.3 in C. Bardos [1], which essentially say that under appro-
priate hypotheses on A(t)′, additional regularity in the space or time variables
yields additional regularity in the solution. When A(t) = A is time-invariant,
we recover the usual results.
2 Method of Transposition
2.1 Control through a Dirichlet boundary condition
As in the example of §1.2.2, the space of control functions u is L2(Σ) or
equivalently L2(0, T ; U) with U = L2(Γ). The boundary value problem is

2 Method of Transposition
189
⎧
⎪
⎪
⎨
⎪
⎪
⎩
A(t)y + ∂y
∂t = 0
in Q,
y = u
on Σ,
y(x, 0) = y0(x)
in Ω,
(2.1)
where A(t) is given by (1.5) with the following hypotheses on the coeﬃcients:
a0 ∈L∞(Ω),
aij ∈W 1,∞(Q),
1 ≤i, j ≤n,
(2.2)
and the coercivity (1.15). We also set V = H0(Ω), H = L2(Ω), and deﬁne the
injection i as in (1.7) and (1.8).
In this example it is not possible to directly obtain a variational diﬀeren-
tial equation in V ′. The method that naturally arises in this context is the
“Method of Transposition.” To see this it is useful to perform the following
formal computation. Pick an arbitrarily suﬃciently smooth function v(x, t)
deﬁned on Q such that v(x, t) = 0 on Σ. Multiply the ﬁrst equation (2.1) by v
and integrate over Q. Use Green’s formula twice on the term A(t)y and inte-
gration by parts on the term ∂y/∂t. The result is as follows for all appropriate
functions v:

Q
y

A∗(t)v −∂v
∂t
	
dx dt +

Ω
y(T )v(T ) dx
= −

Σ
u ∂v
∂νA∗dΣ +

Ω
y0v(0) dx,
(2.3)
where
A∗(t)ψ = −
n

i,j=1
∂
∂xj

aij(x, t) ∂ψ
∂xi

+ a0(x, t)ψ
in Ω
(2.4)
and the co-normal derivative with respect to the operator A∗is given by
∂v
∂νA∗(x, t) =
n

i,j=1
aij(x, t) ∂v
∂xi
(x, t)νi(x)
on Σ,
(2.5)
with ν = (ν1, . . . , νn) the unit external normal to Γ. In the form (2.3), the
data y0 and u explicitly appear in the equation and the unknowns are
y,
y(T ).
(2.6)
In the process we have exhibited the two terms that usually appear in the
adjoint system
A∗(t)v −∂v
∂t ,
v(T ).
(2.7)
If we introduce the space
H2,1(Q) = L2
0, T ; H2(Ω)

∩H1
0, T ; L2(Ω)

(2.8)

190
II-2 Variational Theory of Parabolic Systems
and its subspace
Φ = {v ∈H2,1(Q): v|Σ = 0},
(2.9)
then under appropriate assumptions we shall see that the map
v 	→

A∗(t)v −∂v
∂t , v(T )

: Φ →L2(Q) × H1
0(Ω)
(2.10)
is an isomorphism. Moreover for
u ∈L2(Σ)
and
y0 ∈

H1
0(Ω)
′ = H−1(Ω),
(2.11)
the linear map
v 	→

Σ
u ∂v
∂νA∗dΣ +

Ω
y0v(0) dΩ: Φ →R
(2.12)
is continuous.
If we denote by Da the isomorphism (2.10) and by ℓthe continuous linear
functional (2.12) on Φ, equation (2.3) is now equivalent to ﬁnd
˜y = (y, yT ) ∈L2(Q) × H−1(Ω)
(2.13)
such that
⟨˜y, Dav⟩L2(Q)×H1
0 (Ω) = ℓ(v),
∀v ∈Φ.
(2.14)
But this is equivalent to ﬁnd ˜y in

L2(Q) × H0(Ω)
′ such that
(Da)∗˜y = ℓ
in Φ′.
(2.15)
This problem has a unique solution because the transposed of the isomorphism
Da is also an isomorphism.
So the idea behind the Method of Transposition is to construct a “smooth”
adjoint isomorphism and transpose it in order to make sense of the original
boundary value problem for “rougher” data. When data are smoother we
naturally recover the usual results.
The other ingredient associated with the Method of Transposition is the
theory of interpolation, which enables us to “interpolate” between a “smooth”
and a “rough” version of the same isomorphism.
2.2 Point control
Let {x1, x2, . . . , xN} be a set of points in the domain Ω. Consider the boundary
value problem
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
A(t)y + ∂y
∂t =
N

i=1
δ(xi)ui(t),
y|Σ = 0,
y(x, 0) = y0(x),
(2.16)

2 Method of Transposition
191
where A(t) is given by (1.15) with hypotheses (2.2) and (1.15), δ(xi) is the
Dirac delta function at xi, 1 ≤i ≤N, and ui : [0, T ] →R, 1 ≤i ≤N, are the
components of the control function u: [0, T ] →U with U = RN.
By repeating the construction in the previous section, it is easy to show
that (2.16) is equivalent to

Q
y

A∗(t)v −∂v
∂t
	
dx dt +

Ω
y(T )v(T ) dx
=
 T
0
N

i=1
ui(t)v(xi, t) dt +

Ω
y0v(0) dx
(2.17)
for all v in Φ. The right-hand-side of (2.17) is a continuous linear functional
on Φ when the map
v 	→
 T
0
N

i=1
ui(t)v(xi, t) dt +

Ω
y0v(0) dΩ: Φ →R
(2.18)
is continuous. This happens when y0 belongs to H−1(Ω) and when the eval-
uation map
w 	→w(xi): H2(Ω) →R
(2.19)
is continuous, that is when the dimension n of the space Rn that contains Ωis
less or equal to 3 (cf. J. L. Lions and E. Magenes [1, Volume 1, Theorem 9.8,
p. 45] . As a result for dimensions n = 1, 2, and 3, we can use the Method of
Transposition.
2.3 Main result
In its crudest form the technique used in the previous examples rests on the
following classical lemma.
Lemma 2.1. Given an (algebraic and topological) isomorphism L: X →Y
between two real Banach spaces X and Y , its transpose L∗: Y ′ →X′ is also
an isomorphism between the topological dual spaces Y ′ and X′ for Y and X
endowed with their respective norm topologies. Hence for each x′ in X′, the
variational equation
⟨L∗y′, x⟩X = ⟨x′, x⟩X
∀x ∈x
(2.20)
has a unique solution y′ in Y ′ that is equal to (L∗)−1x′. Moreover the solution
is continuous with respect to the datum x′.
Proof. Cf. N. Dunford and R. S. Schwartz [2, Vol. 1, p. 479, Lem. 7].
⊓⊔
There are many ways to exploit the transposition of a smooth adjoint
isomorphism. For instance, the reader is referred to J. L. Lions and E. Ma-
genes [1, Volume 1] for applications to elliptic (Chapter 2, §6, p. 166) and
parabolic problems (Chapter 3, p. 225). Recently this technique has also been
used by M. C. Delfour [1] for systems with delays in state and control
variables.

192
II-2 Variational Theory of Parabolic Systems
2.4 Application of transposition to the examples of §2.1 and §2.2
We now make precise the constructions of §2.1 and §2.2. Deﬁne
H2,1(Q) = L2
0, T ; H2(Ω)

∩H1
0, T ; L2(Ω)

,
Φ = {v ∈H2,1(Q): v|Σ = 0}
(2.21)
and consider the continuous linear map
v 	→Lav =

A∗(t)v −∂v
∂t , v(T )

: Φ →L2(Q) × H1
0(Ω),
(2.22)
where for all ψ in H2(Ω)
A∗(t)ψ = −
N

i,j=1
∂
∂xj

aij(x, t) ∂ψ
∂xi

+ a0(x, t)ψ.
(2.23)
Assume that
a0, ∂a0
∂t ∈L∞(Q),
aij ∈W 1,∞(Q),
1 ≤i, j ≤n,
(2.24)
and
∃α > 0,
∀ξ ∈Rn,
n

i,j=1
aij(x, t)ξiξj ≥α
n

i=1
ξ2
i
a.e. in Q.
(2.25)
Then A∗is V –H coercive.
Lemma 2.2. Assume that Ωis bounded C∞domain and that the a’s verify
properties (2.24) to (2.25). Then the map La is an isomorphism.
Proof. Cf. C. Bardos [1], L. Tartar [1], and A. Bensoussan and J. L. Li-
ons [1].
⊓⊔
The map La is the isomorphism we shall transpose. Introduce the addi-
tional notation
V = H1
0(Ω),
D = H2(Ω) ∩H1
0(Ω),
H = L2(Ω).
(2.26)
Given any two Hilbert spaces X and Y with a continuous linear injection from
X into Y , we deﬁne the space
W(0, T ; X, Y ) =

v ∈L2(0, T ; X): dv
dt ∈L2(0, T ; Y )

.
(2.27)
Notice that Φ coincides with the space W(0, T ; D, H).
Remark 2.1. In fact Lemma 2.2 can be obtained directly from the Corollary
to Theorem 1.4 and, in addition, because

2 Method of Transposition
193
D

Aλ(t)

= D (independent of t)
the solution of
A∗(t)v −∂v
∂t = f ∈L2(0, T ; H),
v(T ) = vT ∈V
belongs to C(0, T ; V ).
⊓⊔
We also know from interpolation theory (cf. J. L. Lions [2]) that
[D, H]1/2 = [H2(Ω) ∩H1
0(Ω), L2(Ω)]1/2 = H1
0(Ω) = V.
(2.28)
Consider the following functional ω on Φ:
ω(v) =
 T
0
⟨f(t), v(t)⟩D dt + ⟨y0, v(0)⟩V
(2.29)
for f in L2(0, T ; D′) and y0 in V ′, where ⟨·, ·⟩X denotes the duality pairing
on X′ × X. The functional ω is linear and continuous. So we conclude from
Lemma 2.1 that
∃a unique (y, yT ) ∈

L2(Q) × H1
0(Ω)
′ ≡L2(Q) × V ′
(2.30)
such that for all v in Φ
 T
0

y, A∗(t)v −∂v
∂t

dt + ⟨yT , v(T )⟩V =
 T
0
⟨f, v⟩D dt + ⟨y0, v(0)⟩V . (2.31)
So we are now ready to reﬁne this ﬁrst result.
Proposition 2.1. Under the hypotheses of Lemma 2.2, the map
y 	→Dy =

A∗(t)
∗y + dy
dt , y(0)

: W(0, T ; H, D′)
→L2(0, T ; D′) × V ′
(2.32)
is an isomorphism. Moreover, we have the following identity:
for all y ∈W(0, T ; H, D′)
and
v ∈W(0, T ; D, H)
 T
0

y, A∗(t)v −∂v
∂t

dt + ⟨y(T ), v(T )⟩V
=
 T
0
&
A∗(t)
∗y + dy
dt , v
'
D
dt + ⟨y(0), v(0)⟩V ,
(2.33)
where y(0) and y(T ) are to be interpreted as values of the function y in
C(0, T ; V ′)1, which is almost everywhere equal to y in W(0, T ; H, D′) (here

A∗(t)
∗denotes the topological transposed of the linear map A∗(t): D →H).
1 Recall the notation C(0, T; V ′) for C([0, T ]; V ′).

194
II-2 Variational Theory of Parabolic Systems
Proof. We have seen that the pair (y, yT ) is the unique solution of the equation
 T
0

y, A∗(t)v −∂v
∂t

H
dt + ⟨yT , v(T )⟩V
=
 T
0
⟨f, v⟩D dt + ⟨y0, v(0)⟩V ,
∀v ∈Φ.
(2.34)
Let v(t) = ϕ(t)w for some w in D and ϕ in D(]0, T [). The function v belongs
to Φ and its substitution in (2.34) yields
−
 T
0
(y(t), w)H
dϕ
dt (t) dt =
 T
0
-
f(t) −

A∗(t)
∗y, w
.
ϕ(t) dt.
But the right-hand-side is precisely the deﬁnition of the distributional deriv-
ative Dty for the vector distribution y:
⟨y(ϕ), w⟩D =
 T
0
(y(t), w)Hϕ(t) dt, ⟨Dty(ϕ), w⟩D =−
 T
0
(y(t), w)H
dϕ
dt (t) dt.
From (2.34) we conclude that
d
dt(y(t), w)H =
-
f(t) −

A∗(t)
∗y, w
.
D
=⇒Dty = f(t) −

A∗(t)
∗y ∈L2(0, T ; D′).
(2.35)
Therefore y belongs to W(0, T ; H, D′) as predicted. Moreover the injection
W(0, T ; H, D′) →C(0, T ; V ′)
(2.36)
is continuous (cf. J. L. Lions and E. Magenes [1 , Volume 2, p. 34, Theo-
rem 6.2]) because
[H, D′]1/2 = [D, H]′
1,2 = [H2(Ω) ∩H1
0(Ω), L2(Ω)]1/2 = H1
0(Ω)′
(cf. J. L. Lions and E. Magenes [1, Volume 1, p. 204 and p. 178]). As a
result y in W(0, T ; H, D′) is almost everywhere equal to a continuous function
y in C(0, T ; V ′) and y(0) and y(T ) make sense as elements of V ′.
Going back to (2.34) and substituting (2.35) into it we obtain
 T
0

y, ∂v
∂t

H
+ ⟨Dty, v⟩D

dt = ⟨yT , v(T )⟩V −⟨y0, v(T )⟩V .
Choose v(t) = wψ(t) for some w in D and ψ in H1(0, T ; R) and substitute it
into the previous equation:
 T
0

(y(t), w)H
dψ
dt (t) + ⟨Dty, w⟩Dψ(t)

dt
= ⟨yT , w⟩V ψ(T ) −⟨y0, v(0)⟩V ψ(0).
(2.37)

2 Method of Transposition
195
Recall from (2.35) that
⟨Dty, w⟩D = d
dt(y(t), w)H ∈L2(0, T ; R)
so that (2.37) reduces to
 T
0
d
dt{(y(t), w)Hψ(t)} dt = ⟨yT , w⟩V ψ(T ) −⟨y0, v(0)⟩V ψ(0).
As for each w the map
t 	→(y(t), w)H
belongs to H1(0, T ; R), we conclude that
⟨yT , w⟩V = (y(t), w)H|t=T ,
⟨y0, w⟩V = (y(t), w)H|t=0.
(2.38)
It remains to interpret the right-hand sides in (2.38) correctly because in
principle y ∈L2(0, T ; H) has no pointwise character. However we know that
the injection (2.36) is continuous and that the function
t 	→y(t) = i∗
V y(t)
is continuous on [0, T ] with values in V ′. So (2.38) yields
yT = y(T ) ∈V ′ and y(0) = y0 ∈V ′.
(2.39)
This shows that D as deﬁned in (2.32) is indeed an isomorphism. This com-
pletes the proof of the Proposition.
⊓⊔
If we go back to the Dirichlet boundary control to §2.1 and to (2.3), we
have to show that the linear map
v 	→−

Σ
u ∂v
∂νA∗dΣ : L2(0, T ; D) →R
is continuous. Indeed for almost all t, the map
w 	→
∂w
∂νA∗(t)
: D = H2(Ω) ∩H1(Ω) →H1/2(Γ)
and the injection of H1/2(Γ) into L2(Γ) are continuous. This deﬁnes the
control operator B(t): U = L2(Γ) →D′ as follows:
⟨B(t)u, w⟩D =

u,
∂w
∂νA∗(t)

L2(Γ)
(2.40)
and the adjoint operator B∗(t): D →L2(Γ)
B∗(t)w =
∂w
∂νA∗(t)
.
(2.41)

196
II-2 Variational Theory of Parabolic Systems
They are both linear continuous and strongly measurable and bounded as a
function of t because the aij’s belong to W 1,∞(Q) and Ωis bounded and C∞.
With the above deﬁnitions we see that the solution y in W(0, T ; H, D′) of
the operational diﬀerential equation

A∗(t)
∗y + dy
dt = B(t)u,
u ∈L2(0, T ; U), U = L2(Γ),
y(0) = y0 ∈V ′
(2.42)
coincides with the solution of problem (2.1).
Similarly for the example of §2.2, the control functions are
u = (u1, . . . , uN) ∈L2(0, T ; U),
u = RN
and the control operator is
⟨Bu, w⟩D =
N

i=1
uiw(xi),
w ∈D,
u ∈RN.
(2.43)
In other words
Bu(t) =
N

i=1
ui(t)δxi,
where δxi is the Dirac delta function in xi. For domains Ωin Rn, 1 ≤n ≤3, the
operator B : U →D′ is continuous and the operational diﬀerential equation
(2.42) is a good model for point control.
Notation 2.1. The operator

A∗(t)
∗∈L(H, D′) is an extension of the opera-
tor A(t) ∈L(D, H) as deﬁned in (1.5). In the sequel we shall write A(t) instead
of

A∗(t)
∗and keep in mind that A(t) ∈L(D, H) ∩L(V, V ′) ∩L(H, D′).
⊓⊔
2.5 A change of variable
It is a natural question to ask whether the operational diﬀerential equation
A(t)y + dy
dt = f ∈L2(0, T ; D′),
y(0) = y0 ∈V ′,
y ∈W(0, T ; H, D′)
(2.44)
is fundamentally diﬀerent from a variational diﬀerential equation of the form
˜A(t)˜y + d˜y
dt = ˜f ∈L2(0, T ; V ′),
˜y(0) = ˜y0 ∈H,
˜y ∈W(0, T ; V, V ′)
(2.45)
of the type we studied in §1.3.
It turns out that we can go from (2.44) to (2.45) by making the following
change of variable:

2 Method of Transposition
197
˜y(t) = [A1/2
λ (t)]−1y(t) = A−1/2
λ
(t)y(t),
(2.46)
where
Aλ(t) = A(t) + λI
and λ ∈R is the number for which the V –H coercivity is veriﬁed. By Propo-
sition 1.3,
˜y ∈W(0, T ; V, V ′).
Moreover
d˜y
dt = A−1/2
λ
(t)dy
dt +

A−1/2
λ
(t)
′y.
But

A−1/2
λ
(t)
′ = −A−1/2
λ
(t)

A1/2
λ (t)
′A1/2
λ (t)
and the derivative of A1/2
λ (t) makes sense as an element of L(H, V ′) and
y ∈L2(0, T ; H) =⇒

A−1/2
λ
(t)
′y ∈L2(0, T ; V ′).
As for the ﬁrst term
A−1/2
λ
(t) ∈L(D′, V ′),
dy
dt ∈L2(0, T ; D′) =⇒A−1/2
λ
(t)dy
dt ∈L2(0, T ; V ′).
Multiply the ﬁrst equation (2.44) by A−1/2
λ
(t)
A−1/2
λ
(t)A(t)y + A−1/2
λ
(t)dy
dt = A−1/2
λ
(t)f ∈L2(0, T ; V ′),
A−1/2
λ
(0)y(0) = A−1/2
λ
(0)y0 ∈H.
Then
A−1/2
λ
(t)A(t) = A(t)A−1/2
λ
(t)
and
A(t)˜y(t) + d˜y
dt −

A−1/2
λ
(t)
′A1/2
λ (t)˜y = ˜f,
˜y(0) = ˜y0,
where
˜f = A−1/2
λ
(t)f ∈L2(0, T ; V ),
˜y0 = A−1/2
λ
(0)y0 ∈H.
(2.47)
The operator
K(t) = −

A−1/2
λ
(t)
′A1/2
λ (t) = A−1/2
λ
(t)

A1/2
λ (t)
′ ∈L(V, H)
is a perturbation of the operator A(t), and we know from Theorem 1.2 that
for such a perturbation, A + K is V –H coercive.

198
II-2 Variational Theory of Parabolic Systems
From this quick analysis we conclude that ˜y given by (2.46) is the unique
solution in W(0, T ; V, V ′) of (2.44) with
˜A(t) = A(t) + A−1/2
λ
(t)

A1/2
λ (t)
′.
(2.48)
To complete the picture, the control operators in the two examples of §2.1
and §2.2
B(t) ∈L(U, D′)
(2.49)
are transformed into
˜B(t) = A−1/2
λ
(t)B(t) ∈L(U, V ′)
(2.50)
and we are back to variational diﬀerential equation (1.26).
Thus the examples of §2, although quite diﬀerent from those of §1, do
not require a fundamentally new theory and appropriate changes in the state
variable bring a signiﬁcant simpliﬁcation in the general analysis.
2.6 Other isomorphisms
Other isomorphisms can be used as starting points under the hypothesis
a0,
∂a0
∂t ∈L∞(Q),
aij ∈W 1,∞(Q),
1 ≤i, j ≤n,
and the coercivity hypothesis
∃α > 0,
n

i,j=1
aij(x, t)ξjξi ≥α
n

i=1
ξ2
i ,
∀ξ ∈Rn.
For instance the isomorphism
v	→

A∗(t)v−∂v
∂t , v|Σ, v(T )

: H2,1(Q)→L2(Q)×H3/2,3/4(Σ)×H1(Ω), (C.R.)
where C.R. stands for the “compatibility relations” among the three spaces,
or
v 	→

A∗(t)v −∂v
∂t ,
∂v
∂νA∗, v(T )

: H2,1(Q) →L2(Q) × H1/2,1/4(Σ) × H1(Ω)
(cf. A. Bensoussan and J. L. Lions [1, Chapter 2, §6]).
3 Second order problems
Second order problems are usually not variational and do not fall within the
framework of this chapter. For example the wave equation

3 Second order problems
199

y′′ + A(t)y = f,
y(0) = y0,
y′(0) = y1
(3.1)
or Schr¨odinger’s equation are not variational. However in some cases when
damping is added, some second order problems become variational.
The simplest example

y′′ + 2γAy′ + Ay = f,
y(0) = y0,
y′(0) = y1
(3.2)
for γ > 0 falls into this category. In fact (3.2) is known as the parabolic
regularization of (3.1). J. L. Lions [4, Chapter 5, §1, p. 380] shows that
as γ goes to zero the solutions in (3.2) converge to the solutions of (3.1). In
structure theory, (3.2) is often referred to as a structure with viscous damping.
To see that (3.2) is variational we ﬁrst rewrite (3.2) as a ﬁrst order equation
by introducing the new variables
y0 = y,
y1 = y′
(3.3)
and
d
dt
y0
y1
	
+
0
−I
A 2γA
	 y0
y1
	
=
0
f
	
.
(3.4)
Let V and H be two Hilbert spaces with continuous injection of V into H. So
without loss of generality assume that
|v| ≤∥v∥,
∀v ∈V.
This deﬁnes a new operator
A =
0
−I
A 2γA
	
,
(3.5)
where A ∈L(V, V ′) is generated by the bilinear continuous form on V
⟨Av, w⟩= a(v, w),
(3.6)
which is symmetrical, positive, and V –H coercive
a(v, w) = a(w, v),
a(v, v) ≥0,
(3.7)
∃α > 0, ∃λ ∈R,
a(v, v) + λ|v|2
H ≥α∥v∥2
V .
(3.8)
As we have two components we need two new spaces V and H that will play
the role of V and H. The space H will be the product space V × H endowed
with the inner product: For all
→v = (v0, v1) and
→w = (w0, w1)
[
→v ,
→w]
def
= a(v0, w0) + λ(v0, w0)H + (v1, w1)H.
(3.9)

200
II-2 Variational Theory of Parabolic Systems
The space H will be our new pivot space, and it remains to specify the space
V. To do that we look at the continuity of the bilinear form
→a(
→v ,
→w) = [A
→v ,
→w].
(3.10)
By construction
→a(
→v ,
→w) = a(−v1, w0) + λ(−v1, w0) + (Av0 + 2γAv1, w1)
(3.11)
= −a(v1, w0) −λ(v1, w0) + a(v0, w1) + 2γa(v1, w1).
(3.12)
It is clearly continuous on V × V for V = V × V . Moreover,
→a(
→v ,
→v ) = −λ(v1, v0) + 2γa(v1, v1)
(3.13)
and
→a(
→v ,
→v ) + µ[
→v ,
→v ] = −λ(v1, v0) + 2γa(v1, v1) + µ[a(v0, v0)
+ λ|v0|2 + |v1|2]
= 2γ[a(v1, v1) + λ|v1|2] + µ[a(v0, v0) + λ|v0|2]
+ [µ −2γλ]|v1|2 −λ(v1, v0)
≥α[2γ∥v1∥2
V +µ∥v0∥2
V ]+(µ−2γλ)|v1|2−λ(v1, v0)
≥γα[∥v1∥2
V + ∥v0∥2
V ]
(3.14)
for µ ≥max{2γλ, γ +λ2 (4γα2)}. Thus
→a is V −H coercive and all the results
of the previous sections apply.
Another example that is of special interest is the so-called “structural
damping,” which is of the form

y′′ + 2γA1/2y′ + Ay = f,
y(0) = y0,
y′(0) = y1,
(3.15)
and more generally for 0 ≤α ≤1

y′′ + 2γAαy′ + Ay = f,
y(0) = y0,
y′(0) = y1.
(3.16)
It is known (cf. S. Chen and R. Triggiani [1, 2, 3]), that for 1/2 ≤α ≤1,
the corresponding semigroup is analytic, but it is not yet known whether the
problem is variational for 1/2 ≤α < 1.
Acknowledgments
The authors would like to thank M. Sorine for enlightening discussions on the
work of C. Bardos [1].

3
Semigroup Methods for Systems With
Unbounded Control and Observation
Operators
1 Complements on semigroups
Let {S(t)} be a strongly continuous semigroup on the Hilbert space H. Let |·|
and (·, ·) be the norm and inner product in H. Denote by A the inﬁnitesimal
generator of {S(t)} and by D(A) its domain. When D(A) is endowed with
the graph norm of A
∥h∥2
A = |h|2 + |Ah|2,
h ∈D(A),
(1.1)
it becomes a Hilbert space and
A: D(A →H
(1.2)
is a continuous linear operator.
1.1 Notation
Let X and Y be two Hilbert spaces such that X ⊂Y . In that case the
interpolation spaces (X, Y )θ,p with p = 2 and [X, Y ]θ coincide for 0 < θ < 1
(cf. Chapter 1, §4.7). We shall use the notation [X, Y ]θ, θ, 0 < θ < 1.
For all data of the form
y0 ∈D(A),
f ∈L2
0, T ; D(A)

,
(1.3)
the function
y(t) = S(t)y0 +
 t
0
S(t −s)f(s) ds
(1.4)
is the unique solution in the space
V(0, T ; D(A), H) =

y ∈C

[0, T ]; D(A)

: dy
dt ∈L2(0, T ; H)

(1.5)

202
II-3 Semigroup Methods for Unbounded Control and Observation
of the diﬀerential equation
⎧
⎨
⎩
dy
dt = Ay + f,
y(0) = y0.
(1.6)
The notation in (1.5) will be used on several occasions. Given two Banach
spaces with a continuous linear injection of X into Y,
V(0, T ; X, Y ) =

y ∈C([0, T ]; X): dy
dt ∈L2(0, T ; Y )

,
(1.7)
where dy/dt is to be interpreted as the vectorial distributional derivative of y
in C([0, T ]; X) considered as an element of L2(0, T ; Y ).
It is well known that, by density of D(A) in H, the variation of constants
formula (1.4) makes sense for data
y0 ∈H,
f ∈L2(0, T ; H),
(1.8)
and such a solution is usually referred to as a “mild solution” of system (1.6)
(cf. Chapter 1, Deﬁnition 3.1 (iv)). It is useful to be more explicit and precise
about this. We ﬁrst identify the elements of the dual H′ of H with those of
H,
H ≡H′.
Let {S∗(t)} be the adjoint semigroup on H′ = H and let A∗be its inﬁnitesi-
mal generator with domain D(A∗). When D(A∗) is endowed with the graph
norm of A∗, the restriction of {S∗(t)} to D(A∗) is also a strongly continuous
semigroup on D(A∗)
S∗(t) ∈L

D(A∗), D(A∗)

.
(1.9)
For simplicity we still denote by {S∗(t)} the semigroup on D(A∗). Its topo-
logical transposed
S∗(t)∗∈L(D(A∗)′, D(A∗)′)
(1.10)
is also a strongly continuous semigroup on D(A∗)′. It can be viewed as an
extension to D(A∗)′ of the semigroup {S(t)} on H. For this reason we shall
use the same notation {S(t)} and keep in mind that it can be viewed as a
strongly continuous semigroup on either one of the three spaces
D(A) →H ≡H′ →D(A∗)′.
(1.11)
For data as speciﬁed in (1.8), let y in C([0, T ]; H) be the function given
by the variation of constants formula (1.4). For an arbitrary k in D(A∗)

k, y(t)

= (S∗(t)k, y0) +
 t
0

S∗(t −s)k, f(s)

ds.
This function is diﬀerentiable because for all k in D(A∗) and s ∈[0, T [

1 Complements on semigroups
203
t 	→∂S∗
∂t (t −s)k = A∗S∗(t −s)k = S∗(t −s)A∗k
belongs to C([s, T]; H). To see that we compute the distributional derivative:
For all ϕ ∈D(]0, T [) consider the expression
τ =
 T
0

k, y(t)
dϕ
dt (t) dt.
We can assume that y0 = 0. For the second term
τ = −
 T
0
dt
 t
0
ds

S∗(t −s)k, f(s)
dϕ
dt (t)
= −
 T
0
ds
 T
s
dt

S∗(t −s)k, f(s)
dϕ
dt (t)
=
 T
0
ds
 T
s
dt
 d
dtS∗(t −s)k, f(s)

ϕ(t) +
 T
0

k, f(t)

ϕ(t) dt
=
 T
0
dt

A∗k,
 t
0
S(t −s)f(s) ds

+

k, f(t)
	
ϕ(t).
As a result
d
dt

k, y(t)

= (S∗(t)A∗k, y0) +
 t
0

S∗(t −s)A∗k, f(s)

ds +

k, f(t)

=

A∗k, y(t)

+

k, f(t)

and
⎧
⎨
⎩
dy
dt = (A∗)∗y + f,
y(0) = y0,
(1.12)
where
(A∗)∗: H →D(A∗)′
(1.13)
is the transposed of the continuous linear operator
A∗: D(A∗) →H,
(1.14)
when D(A∗) is endowed with the graph norm of A∗. As for the semigroup
{S(t)}, (A∗)∗is to be viewed as an extension of the operator A because they
coincide on D(A). So we adopt the notation
A ∈L(D(A), H) ∩L(H, D(A∗)′)
(1.15)
in both cases and conclude that for data given by (1.8) the function y given by
(1.4) is the unique solution in V(0, T ; H, D(A∗)′) to the diﬀerential equation
(1.6) (cf. Chapter 1, Remarks 3.1 and 3.2).

204
II-3 Semigroup Methods for Unbounded Control and Observation
It is always possible to interpolate pairwise between the various spaces for
A and S(t)
A ∈L([D(A), H]α, [D(A∗), H]′
1−α),
0 < α < 1,
(1.16)
S(t) ∈L([D(A), D(A∗)′]β, [D(A), D(A∗)′]β),
0 < β < 1.
(1.17)
For some types of boundary control problems, it is necessary to also make
sense of the solution of the diﬀerential equation (1.6) for data
y0 ∈D(A∗)′,
f ∈L2(0, T ; D(A∗)′).
(1.18)
The starting point is again the variation of constants formula
y(t) = S(t)y0 +
 t
0
S(t −s)f(s) ds,
(1.19)
where {S(t)} is now considered as a semigroup on D(A∗)′ rather than H.
By deﬁnition y belongs to C([0, T ]; D(A∗)′). We can now repeat the previ-
ous construction with k in D(A∗2), the domain of A∗2, which coincides with
the domain of the inﬁnitesimal generator A∗of {S∗(t)} considered as a semi-
group on D(A∗)
D

A∗; D(A∗)

= D(A∗2; H) = D(A∗2),
(1.20)
where D(L; X) denotes the domain of the unbounded linear operator L con-
sidered as an operator on the space X.
The end result is that for data of the form (1.18), the function y given
by expression (1.19) belongs to V(0, T ; D(A∗)′, D(A∗2)′) and is the unique
solution of the diﬀerential equation (1.6). The following theorem is the analog
of Proposition 3.3 in Chapter 1.
Theorem 1.1. Let {S(t)} be a strongly continuous semigroup and let λ ∈R
be such that
Aλ = A −λI ∈L(D(A), H)
(1.21)
be an isomorphism. The solution y of (1.6) for y0 ∈H and f ∈H1(0, T ; D(A∗)′)
belongs to C1([0, T ]; D(A∗)′) ∩C([0, T ]; H) and for all t, 0 ≤t ≤T ,
y(t) = −A−1
λ f(t) + S(t)[y0 + A−1
λ f(0)]
+
 t
0
S(t −s)A−1
λ
df
ds(s)−λf(s)
	
ds.
(1.22)
Proof. The solution y of (1.6) belongs to C([0, T ]; D(A∗)′) and is given by the
usual formula
y(t) = S(t)y0 +
 t
0
S(t −s)f(s) ds.

1 Complements on semigroups
205
We already know that for y0 ∈H, the ﬁrst term belongs to C([0, T ]; H)
∩C1([0, T ]; D(A∗)′). So we only need to prove the theorem for y0 = 0. Recall
that Aλ ∈L(D(A), H) ∩L(H, D(A∗)′). Hence
y(t) =
 t
0
AλS(t −s)A−1
λ f(s) ds.
(1.23)
But
−d
dsS(t −s) = AS(t −s)
and
y(t) = −
 t
0
S(t −s)A−1
λ λf(s) ds −
 t
0
d
ds[S(t −s)]A−1
λ f(s) ds,
y(t) = −A−1
λ f(t) + S(t)A−1
λ f(0) +
 t
0
S(t −s)A−1
λ [f ′(s) −λf(s)] ds.
By assumption f ∈H1(0, T ; D(A∗)′) implies that
A−1
λ f ∈H1(0, T ; H) ⊂C([0, T ]; H)
and necessarily each term and, a fortiori y, belongs to C([0, T ]; H). Moreover
dy
dt = Ay + f ∈C([0, T ]; D(A∗)′)
and
y ∈C([0, T ]; H) ∩C1([0, T ]; D(A∗)′).
This completes the proof.
⊓⊔
Remark 1.1. Notice that we also recover from identity (1.22) the usual results
y0 ∈D(A),
f ∈H1(0, T ; H) =⇒y ∈V(0, T ; D(A), H).
⊓⊔
Remark 1.2. For rough data identity (1.23) will often be used with the Aλ
outside the integral
y(t) = S(t)y0 + Aλ
 t
0
S(t −s)A−1
l
f(s) ds
(1.24)
in order to take full advantage of the eventual smoothing properties of the
integral.
⊓⊔
This technique readily extends to higher derivatives of the function f.

206
II-3 Semigroup Methods for Unbounded Control and Observation
Corollary 1.1. Assume that, in addition to the assumptions of Theorem 1.1,
y0 ∈D(A) and f ∈C([0, T ]; H) ∩H2(0, T ; D(A∗)′). The solution y of (1.6)
belongs to C1([0, T ]; H) ∩C

[0, T ]; D(A)

and for t, 0 ≤t ≤T ,
y(t)=−A−1
λ

f(t)+A−1
λ

f ′(t)−λf(t)

+S(t)

y0+A−1
λ f(0)+A−2
λ

f ′(0)−λf(0)

+
 t
0
S(t −s)A−2
λ [f ′′(s) −2λf ′(s) + λ2f(s)] ds.
(1.25)
Proof. It is convenient to introduce the following change of variable:
yλ(t) = e−λty(t),
fλ(t) = e−λtf(t),
Sλ(t) = e−λtS(t).
Then
yλ(t) = Sλ(t)y0 +
 t
0
Sλ(t −s)fλ(s) ds
and as in the proof of the theorem
yλ(t) = Sλ(t)y0 +
 t
0
AλSλ(t −s)A−1
λ fλ(s) ds
= Sλ(t)[y0 + A−1
λ fλ(0)] −A−1
λ fλ(t) +
 t
0
Sλ(t −s)A−1
λ f ′
λ(s) ds.
Now the same construction is used for the integral term
 t
0
AλSλ(t −s)A−2
λ f ′
λ(s) ds =
 t
0
Sλ(t −s)A−2
λ f ′′
λ(s) ds
+Sλ(t)A−2
λ f ′
λ(0) −A−2
λ f ′
λ(t).
Finally
yλ(t) = −A−1
λ [f(t) + A−1
λ f ′
λ(t)] + Sλ(t)[y0 + A−1
λ fλ(0) + A−2
λ f ′
λ(0)]
+
 t
0
Sλ(t −s)A−2
λ f ′′
λ(s) ds.
From the last identity it is easy to conclude that y has the expected regularity
and that expression (1.25) can be easily recovered by a change of variable.
⊓⊔
The above statements are quite general. Sharper results are available for
speciﬁc classes of systems such as analytic semigroups.
2 Complements on analytic semigroups
In this section we recall the main regularity result for analytic semigroups.
With the help of interpolation theory we also give intermediary regularity
results. Finally we show how the basic regularity result for analytic semigroups
is at the origin of various methods of change of variable.

2 Complements on analytic semigroups
207
2.1 Regularity results
Given a semigroup {S(t)} of type ω0(S), for each ω > ω0(S), there exists
M ≥1 such that
|S(t)h| ≤Meωt|h|,
∀h ∈H, ∀t ≥0.
(2.1)
The semigroup
Sω(t) = e−ωtS(t)
(2.2)
is a stable semigroup with inﬁnitesimal generator
Aω = A −ωI : D(A) →H
(2.3)
(cf. Chapter 1, Corollary to Theorem 2.2). Similarly for the adjoint semigroup
{S∗
ω(t)}, the associated inﬁnitesimal generator
A∗
ω = A∗−ωI : D(A∗) →H
(2.4)
is also an isomorphism. Therefore the transposed of A∗
ω
(A∗
ω)∗: H →D(A∗)′
(2.5)
is again an isomorphism that can be viewed as an extension of Aω from D(A)
to H because they coincide on D(A) and D(A) is dense in H. Hence
Aω ∈L(D(A), H) ∪L(H, D(A∗)′)
(2.6)
and from Interpolation Theory for all α, 0 ≤α ≤1, the map
Aω : [D(A), H]α →[D(A∗), H]′
1−α
(2.7)
is also an isomorphism.
Recall the following regularity theorem (cf. Chapter 1, §3, Theorem 3.1).
Theorem 2.1. Let {S(t)} be an analytic strongly continuous semigroup. Then
for all T > 0 the linear map
y 	→Dy =
dy
dt −Ay, y(0)

: W(0, T ; D(A), H)
→L2(0, T ; H) × [D(A), H]1/2
(2.8)
is an isomorphism.
Corollary 2.1. Let {S(t)} be an analytic strongly continuous semigroup.
Then for all T > 0 the linear map
y 	→Dy =
dy
dt −Ay, y(0)

: W(0, T ; H, D(A∗)′) →L2(0, T ; D(A∗)′) × [D(A∗), H]′
1/2
(2.9)
is an isomorphism.

208
II-3 Semigroup Methods for Unbounded Control and Observation
Proof. The map (2.9) is clearly linear and continuous. To show that it is
bijective, it is suﬃcient to show that for all
f ∈L2(0, T ; D(A∗)′)
and
y0 ∈[D(A∗), H]′
1/2,
there exists a unique solution y ∈W(0, T ; H, D(A∗)′) such that
y′ −Ay = f,
y(0) = y0.
(2.10)
Deﬁne for ω verifying (2.1)
F = A−1
ω f ∈L2(0, T ; H)
and
Y 0 = A−1
ω y0 ∈[D(A), H]1/2.
By the previous theorem, the equation
Y ′ −AY = F,
Y (0) = Y 0
(2.11)
has a unique solution Y ∈W(0, T ; D(A), H). Now deﬁne
y(t) = AωY (t),
t ∈[0, T ].
It is easy to check that
y = AωY ∈L2(0, T ; H),
y′ = AωY ′ ∈L2(0, T ; D(A∗)′),
y(0) = AωY (0) ∈[D(A∗), H]′
1/2.
(2.12)
Therefore y ∈W(0, T ; H, D(A∗)′) and
y′ −Ay = Aω[Y ′ −AY ] = AωF = f,
y(0) = AωY (0) = AωY 0 = y0. (2.13)
This completes the proof.
⊓⊔
We can now interpolate between isomorphisms (2.8) and (2.9).
Theorem 2.2. Let {S(t)} be an analytic strongly continuous semigroup. Then
for all T > 0, all α, 0 ≤α ≤1, the linear map
⎧
⎪
⎪
⎨
⎪
⎪
⎩
y 	→Dy =
dy
dt −Ay, y(0)

W(0, T ; [D(A), H]α, [D(A∗), H]′
1−α)
→L2(0, T ; [D(A∗), H]′
1−α)×[[D(A), H]α, [D(A∗), H]′
1−α]1/2
(2.14)
is an isomorphism.
If we make the additional hypothesis
[D(A), H]1/2 = [D(A∗), H]1/2
(2.15)
and denote this space by V , then, for α = 1/2, D is an isomorphism
W(0, T ; V, V ′) →L2(0, T ; V ′) × H
(2.16)

2 Complements on analytic semigroups
209
because [V, V ′]1/2 = H. This is to be compared with Theorem 1.1 in Chapter 2.
There is virtually no limit to the up and down interpolation process be-
cause for any integer n ≥1
An
ω : D(An) →D(An−1),
A∗n
ω : D(A∗n) →D(A∗n−1)
(2.17)
are also isomorphisms. So the map
D: W(0, T ; D(A∗n)′, D(A∗(n+1))′)
→L2(0, T ; D(A∗(n+1))′) × [D(A∗n)′, D(A∗(n+1))′]1/2
is also an isomorphism and this can be combined with the results of Theo-
rem 2.2 to obtain a continuum of isomorphisms. Deﬁne for r = n + α, n ≥0
an integer and α, 0 ≤α ≤1 a real number,
Hr = [D(An+1), D(An)]1−α,
H−r = [D(A∗(n+1)), D(A∗n)]′
1−α.
(2.18)
Then for all r in R, the map
D: W(0, T ; Hr, Hr−1) →L2(0, T ; Hr−1) × [Hr, Hr−1]1/2
(2.19)
is an isomorphism.
2.2 Other representations and the method of change of variable
It is well known that some non-homogeneous boundary value problems can be
transformed into simpler homogeneous boundary value problems by an appro-
priate change of the unknown or state variable y (cf. F. Treves [1, pp. 426–
428]). This technique also ﬁnds its analog in Control Theory in the semigroup
setting (cf. H. O. Fattorini [3], A. V. Balakrishnan [3], D. C. Wash-
burn [1], J. Zabczyk [4], and A. Chojnowska-Michalik [1]). Speciﬁc
examples will be given in appropriate chapters.
For analytic semigroups, the regularity property of the map
S : L2(0, T ; D(A∗)′) →W(0, T ; H, D(A∗)′),
(Sf)(t) =
 t
0
S(t −s)f(s) ds
can be used to exhibit the underlying mechanism behind these methods.
Theorem 2.3. Assume that {S(t)} is an analytic strongly continuous semi-
group and that λ ∈R is such that
Aλ = A −λI ∈L(D(A), H)
(2.20)
is an isomorphism. For T > 0, f in L2(0, T ; D(A∗)′) and h in [D(A∗), H]′
1/2,
let
y(t) = S(t)h + (Sf)(t)
(2.21)
be the solution of (2.10) in W(0, T ; H, D(A∗)′)

210
II-3 Semigroup Methods for Unbounded Control and Observation
(i) The solution y is equivalently given by the expression
y(t) = S(t)h + A
 t
0
S(t −s)fλ(s) ds −λ
 t
0
S(t −s)fλ(s) ds,
(2.22)
where
fλ(t) = A−1
λ f(t),
0 ≤t ≤T.
(2.23)
(ii) If, in addition, f ∈W(0, T ; H, D(A∗)′), then
y(t) = Y (t) −fλ(t),
(2.24)
where Y in W(0, T ; H, D(A∗)′) is given by the expression
⎧
⎨
⎩
Y (t) = S(t)Y (0) +
 t
0
S(t −s)dfλ
ds (s) ds,
Y (0) = h + fλ(0).
(2.25)
The proof is obvious and only uses integration by parts and the previous
theorems.
This theory directly applies to parabolic systems with control through a
boundary condition (cf. §4).
3 Unbounded control and observation operators
The space of controls will be a real Hilbert space U and the space of control
functions will be L2(0, T ; U) for some T > 0. The control operator is an
element
B ∈L(U, D(A∗)′),
(3.1)
where D(A∗) is endowed with the graph norm of A∗. We say that B is bounded
if
B ∈L(U, H).
(3.2)
This terminology can be justiﬁed as follows. Consider the operator
B∗∈L(D(A∗), U ′), ∀u ∈U, ∀v ∈D(A∗), ⟨B∗v, u⟩U = ⟨Bu, v⟩D(A∗).
(3.3)
It is readily seen that B is “bounded” in the sense of our deﬁnition if B∗is
bounded as an operator from H′ into U ′
B∗∈L(H′, U ′).
(3.4)
We have seen in the chapters on parabolic systems and delay systems in Part I
how this control structure naturally arises.
The space of observations will be a real Hilbert space Z, and the space
of observation functions will be L2(0, T ; Z) for some T > 0. The observation
operator is an element

3 Unbounded control and observation operators
211
C ∈L(D(A), Z),
(3.5)
where D(A) is endowed with the graph norm of A. We say that C is bounded
if
C ∈L(H, Z).
(3.6)
The operator C∗is the transposed of C
C∗∈L(Z′, D(A)′).
(3.7)
3.1 Analytic systems
The system
⎧
⎨
⎩
dy
dt = Ay + f,
t > 0,
y(0) = y0
(3.8)
will be referred to as an analytic system if the operator A is the inﬁnitesimal
generator of an analytic strongly continuous semigroup {S(t)}. We have seen
in §2 that we have a strong regularity theorem for this class of systems (cf.
Theorem 2.2 and the discussion thereafter). For all α, 0 ≤α ≤1, the linear
map
S : L2(0, T ; [D(A∗), H]′
1−α) →W(0, T ; [D(A), H]α, [D(A∗), H]′
1−α)
(3.9)
is continuous. If we choose for some α, 0 ≤α ≤1,
B∗∈L([D(A∗), H]1−α, U ′),
C ∈L([D(A), H]α, Z),
(3.10)
then the linear map
CSB : L2(0, T ; U) →L2(0, T ; Z)
(3.11)
is continuous. As a result the system makes sense as an input–output system.
To get a complete picture we can add the initial condition or state at time
0
h, u 	→CS(·)h + CSBu:

[D(A), H]α, [D(A∗), H]′
1−α

1/2
× L2(0, T ; U) →L2(0, T ; Z).
(3.12)
This model deals with an observation in the L2(0, T ; Z) sense. Other types of
observations can be contemplated, but the philosophy is always the same: to
make sense of the input–output map in appropriate function spaces.
Notice that when identity (2.15) in §2 is veriﬁed the above discussion
amounts to choose
B ∈L(U, V ′),
C ∈L(V, Z)
(3.13)
and the continuous linear map

h, u 	→CS(·)h + CSBu
: H × L2(0, T ; U) →L2(0, T ; Z).
(3.14)

212
II-3 Semigroup Methods for Unbounded Control and Observation
3.2 Unbounded control operators
In §1 we made sense of the system
⎧
⎨
⎩
dy
dt = Ay + Bu,
y(0) = y0
(3.15)
for the control and control functions
B ∈L(U, D(A∗)′),
u ∈L2(0, T ; U), y0 ∈D(A∗)′.
(3.16)
From that analysis the linear map
SB : L2(0, T ; U) →V(0, T ; D(A∗)′, D(A∗2)′) ⊂C([0, T ]; D(A∗)′)
(3.17)
is continuous. We have seen in §3.1 that for analytic systems, the above map
is continuous with values in W(0, T ; H, D(A∗)′) and
SB : L2(0, T ; U) →L2(0, T ; H).
(3.18)
For other families of semigroups such as systems with delays, we typically
have
B∗: [D(A∗), H]1/2−ε →U ′ continuous ∀ε > 0
(3.19)
and the continuity of the linear map
SB : L2(0, T ; U) →C([0, T ]; H).
(3.20)
There are diﬀerent ways to formulate hypothesis (3.20). We choose to
formulate it in a “dual way” and then show it is equivalent to (3.20).
Hypothesis 3.1. Given T > 0, the linear map
h 	→B∗S∗(·)h: D(A∗) →C([0, T ]; U ′)
(3.21)
can be extended to a continuous linear map
h 	→B∗S∗(·): H →L2(0, T ; U ′),
(3.22)
where B∗is given by (3.3).
⊓⊔
Proposition 3.1. The following statements are equivalent:
(i) Hypothesis (3.1).
(ii) The linear map
u 	→(SBu)(T ): L2(0, T ; U) →H
(3.23)
is continuous.

3 Unbounded control and observation operators
213
(iii) The linear map
u 	→(SBu): L2(0, T ; U) →C([0, T ]; H)
(3.24)
is continuous.
Proof. (iii) =⇒(i) is trivial.
(i) =⇒(ii). For each u in L2(0, T ; U), denote by uT the function
uT (t) = u(T −t),
0 ≤t ≤T.
For all h in D(A∗)
⟨B∗S∗(·)h, uT ⟩L2(0,T ;U) =
& T
0
S(T −s)Bu(s) ds, h
'
D(A∗)
= ⟨(SBu)(T ), h⟩D(A∗).
By Hypothesis 3.1
|⟨(SBu)(T ), h⟩| ≤∥B∗S∗(·)h∥L2(0,T ;U′)∥uT∥L2(0,T ;U)
≤c|h| ∥u|∥L2(0,T ;U)
and necessarily
(SBu)(T ) ∈H,
|(SBu)(T )| ≤c∥u∥L2(0,T ;U).
Therefore the map (3.23) is well deﬁned and continuous.
(ii) =⇒(iii) Note that Hypothesis (3.1) for T implies that the same hy-
pothesis is true for all t, 0 < t ≤T . As a result we can repeat the previous
argument and obtain
|⟨(SBu)(t), h⟩| ≤∥B∗S∗(·)h∥L2(0,t;U′)∥u∥L2(0,t;U)
≤∥B∗S∗(·)h∥L2(0,T ;U′)∥u∥L2(0,T ;U)
≤c|h| ∥u∥L2(0,T ;U).
Hence SBu is a well-deﬁned function from [0, T ] into H and
sup
[0,T [
|(SBu)(t)| ≤c∥u∥L2(0,T ;U).
To prove the continuity of SBu for each u, deﬁne u ∈L2(0, ∞; U)
u(r) =

u(r),
r ∈[0, T ],
0,
r > T.
Then for h in D(A∗)

214
II-3 Semigroup Methods for Unbounded Control and Observation
⟨(SBu)(t), h⟩D(A∗) =
 T
0
⟨B∗S∗(T −s)h, [τ(T −t)u](s)⟩ds,
where τ(t) is the translation semigroup on L2(0, ∞; U)
[τ(t)u](s) = u(t + s),
t ≥0, s ≥0.
So for any t′ and t in [0, T ]
|(SBu)(t′) −(SBu)(t), h >D(A∗) |
≤∥B∗S∗(T −·)h∥L2(0,T ;U)∥τ(T −t′)u −τ(T −t)u∥L2(0,T ;U)
≤∥B∗S∗(·)h∥L2(0,T ;U)∥τ(T −t′)u −τ(T −t)u∥L2(0,T ;U)
≤c|h| ∥τ(T −t′)u −τ(T −t)u∥L2(0,∞;U)
=⇒|(SBu)(t′) −(SBu)(t)|
≤c∥τ(T −t′)u −τ(T −t)u∥L2(0,∞;U).
By strong continuity of the translation semigroup, we conclude that SBu is
continuous and that (iii) is true. This completes the proof of Proposition 3.1.
⊓⊔
Theorem 3.1. Fix T ≥0 and assume that Hypothesis (3.1) is veriﬁed. Then
for data
y0 ∈H,
u ∈L2(0, T ; U),
(3.25)
the function
y(t) = S(t)y0 + (SBu)(t)
(3.26)
is the unique solution in V(0, T ; H, D(A∗)′) to the diﬀerential equation
⎧
⎨
⎩
dy
dt = Ay + Bu,
y(0) = y0,
(3.27)
and there exists a constant c > 0 such that
∥y∥C([0,T ];H) +
!!!!
dy
dt
!!!!
L2(0,T ;D(A∗)′)
≤c[|y0|H + ∥u∥L2(0,T ;U)].
(3.28)
The above results are general and essentially require an hypothesis on the
free adjoint system
⎧
⎨
⎩
−dv
dt = A∗v,
v(T ) = h ∈H,
(3.29)
with observation operator B∗
z(t) = B∗v(t)
(3.30)

3 Unbounded control and observation operators
215
because
z(t) = B∗S∗(T −t)h.
(3.31)
When system (3.15) is time varying (that is, with A∗(t) instead of A∗), Hy-
pothesis 3.1 would require that the linear map
h 	→z : H →L2(0, T ; U ′)
(3.32)
be continuous.
We can also use the method of change of variable in Theorem 2.3 to trans-
form expression (3.26) into
y(t) = S(t)h + Aλ(SA−1
λ Bu)(t).
(3.33)
Deﬁne
Bλ = A−1
λ B ∈L(U, H)
(3.34)
and introduce the change of variable
Y (t) = y(t) + Bλu(t),
0 ≤t ≤T.
(3.35)
Then we have the following equivalent representations:
y(t) = S(t)h + A
 t
0
S(t −s)Bλu(s) ds +
 t
0
S(t −s)[−λBλ]u(s) ds, (3.36)
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
y(t) = Y (t) −Bλu(t),
Y (t) = S(t)Y (0) +
 t
0
S(t −s)Bλ
dy
ds (s) ds
+
 t
0
S(t −s)[−λBλ]u(s) ds,
Y (0) = h + Bλu(0),
u ∈H1(0, T ; U).
(3.37)
This is to be compared with Theorem 2.3.
The next theorem relates expressions (3.26) and (3.36).
Theorem 3.2. Assume that λ is such that Aλ is an isomorphism.
(i) The map
u 	→SBu: L2(0, T ; U) →C([0, T ]; H)
(3.38)
is continuous if and only if the map
u 	→SBλu: L2(0, T ; U) →C

[0, T ]; D(A)

(3.39)
is continuous.
(ii) Hypothesis 3.1 is equivalent to the existence of the continuous linear ex-
tension
h 	→B∗
λS∗(·)h: D(A)′ →L2(0, T ; U ′)
(3.40)
to the map
h 	→B∗
λS∗(·)h: H →C([0, T ]; U ′).
(3.41)

216
II-3 Semigroup Methods for Unbounded Control and Observation
Proof. We use the fact that SBu = Aλ(SBλu).
⊓⊔
It is always possible to make a change of variable to make sense of a control
operator B ∈L(U, D(A∗)′). For instance, by using the resolvent
R(λ, A) = [λI −A]−1,
we have
R(λ, A)SB = SR(λ, A)B.
Deﬁne the new control operator
˜B = R(λ, A)B ∈L(U, H),
˜y(t) = R(λ, A)y(t) =⇒˜y(t) = (S ˜Bu)(t).
It is easy to check that Hypothesis 3.1
h 	→˜B∗S∗(·)h: H →L2(0, T ; U ′)
is equivalent to
h 	→B∗S∗(·)h: D(A∗) →L2(0, T ; U ′),
which is always veriﬁed! The price to pay for that change of variable is that
the new state variable belongs to C([0, T ]; D(A∗)′) rather than C([0, T ]; H).
3.3 Unbounded observation operators
This section is “dual” to §3.2. We start with the free system
y(t) = S(t)h,
h ∈H, t ≥0,
(3.42)
⎧
⎨
⎩
dy
dt = Ay,
y(0) = h.
(3.43)
The observation equation is
z(t) = Cy(t),
0 ≤t ≤T.
(3.44)
From now on, everything we said for B∗and {S∗(t)} in §3.2 is true for C and
{S(t)}.
Hypothesis 3.2. Given T > 0, the linear map
h 	→CS(·)h: D(A) →C([0, T ]; Z)
(3.45)
can be extended to a continuous linear map
h 	→CS(·)h: H →L2(0, T ; Z).
(3.46)
⊓⊔

3 Unbounded control and observation operators
217
Deﬁne for S∗the analog of S in (3.9) for S:
S∗: L2(0, T ; H) →L2(0, T ; H),
(3.47)
(S∗g)(t) =
 t
0
S∗(t −s)g(s) ds.
(3.48)
This operator is not to be confused with the topological dual (S)∗of
S : L2(0, T ; H) →L2(0, T ; H).
(3.49)
They are however related in the following way:
[(S)∗gT ](t) = (S∗g)(T −t),
0 ≤t ≤T,
(3.50)
where
gT (t) = g(T −t),
0 ≤t ≤T.
(3.51)
Proposition 3.2. The following statements are equivalent:
(i) Hypothesis 3.2.
(ii) The linear map
z 	→(S∗C∗z)(T ): L2(0, T ; Z′) →H
(3.52)
is continuous.
(iii) The linear map
z 	→S∗C∗z : L2(0, T ; Z′) →C([0, T ]; H)
(3.53)
is continuous.
Theorem 3.3. Fix T > 0 and assume that Hypothesis 3.2 is veriﬁed. Then
for data
w0 ∈H,
z ∈L2(0, T ; Z′),
(3.54)
the function
w(t) = S∗(t)w0 + (S∗C∗z)(t)
(3.55)
is the unique solution in V(0, T ; H, D(A)′) to the diﬀerential equation
⎧
⎨
⎩
dw
dt = A∗w + C∗z,
w(t) = w0,
(3.56)
and there exists a constant c > 0 such that
∥w∥C([0,T ];H) +
!!!!
dw
dt
!!!!
L2(0,T ;D(A)′)
≤c[|w0|H + ∥z∥L2(0,T ;Z′)].
(3.57)
Theorem 3.4. Assume that λ ∈R is such that Aλ is an isomorphism.

218
II-3 Semigroup Methods for Unbounded Control and Observation
(i) The map
z 	→S∗C∗z : L2(0, T ; Z′) →C([0, T ]; H)
(3.58)
is continuous if and only if the map
z 	→S∗C∗
λz : L2(0, T ; Z′) →C

[0, T ]; D(A∗)

(3.59)
is continuous, where
Cλ = CA−1
λ
∈L(H, Z).
(3.60)
(ii) Hypothesis 3.2 is equivalent to the existence of the continuous linear ex-
tension
h 	→CλS(·)h: D(A∗)′ →L2(0, T ; Z)
(3.61)
to the map
h 	→CλS(·)h: H →C([0, T ]; Z).
(3.62)
3.4 Unbounded control and observation operators
When we consider the controlled system
⎧
⎨
⎩
dy
dt = Ay + Bu,
y(0) = y0,
(3.63)
with the observation
z(t) = Cy(t),
(3.64)
the natural hypothesis that comes is the following one.
Hypothesis 3.3. The linear map
CSB : L2(0, T ; U) →L2(0, T ; Z)
(3.65)
is continuous.
⊓⊔
By duality, this is equivalent to the continuity of the map
⎧
⎨
⎩
B∗S∗C∗: L2(0, T ; Z′) →L2(0, T ; U ′),
(S∗g)(t) =
 T
t
S(s −t)∗g(s) ds.
(3.66)
Unfortunately this last statement does not really provide additional infor-
mation on the speciﬁc connections among C, A, and B to produce the map
(3.65).
In order to gain some insight into this problem we turn to the method
of change of variable introduced in §2.3 (Theorem 2.3) and use it twice to

3 Unbounded control and observation operators
219
simultaneously handle the unboundedness of B and C (cf. also Theorem 1.1
and its corollary).
We proceed formally. The state y is given by the formula
y(t) = S(t)h +
 t
0
S(t −s)Bu(s) ds.
(3.67)
Let λ ∈R be the number for which
Aλ = −[λI −A] ∈L(D(A), H) ∩L(H, D(A∗)′)
is an isomorphism. Deﬁne for all t ≥0
⎧
⎪
⎨
⎪
⎩
yλ(t) = e−λty(t),
uλ(t) = e−λtu(t),
Sλ(t) = S(t)e−λt.
(3.68)
It is easy to check that {Sλ(t)} is also a semigroup and that
d
dtSλ(t)h = AλSλ(t)h,
∀h ∈D(A).
(3.69)
Equation (3.67) is equivalent to
yλ(t) = Sλ(t)h +
 t
0
Sλ(t −s)Buλ(s) ds
(3.70)
and the observation
zλ(t) = e−λtz(t) = Cyλ(t).
(3.71)
In order to remove the unboundedness of B ∈L(U, D(A∗)′) we proceed as
in Theorem 1.1. Assume that
uλ ∈H1(0, T ; U)
(3.72)
and introduce the new operator Bλ and the new variable Y1(t)

Bλ = A−1
λ B ∈L(U, H),
Y1(t) = yλ(t) + Bλuλ(t) ∈L2(0, T ; H).
(3.73)
By performing integration by parts we get
⎧
⎨
⎩
Y1(t) = Sλ(t)Y1(0) +
 t
0
Sλ(t −s)Bλ
duλ
ds (s) ds,
Y1(0) = h + Bλuλ(0) ∈H,
(3.74)
and we conclude that (cf. Theorem 1.1)

220
II-3 Semigroup Methods for Unbounded Control and Observation
Y1 ∈C([0, T ]; H) ∩C1([0, T ]; D(A∗)′).
(3.75)
To remove the unboundedness of C ∈L(D(A), Z), we assume that
uλ ∈H2(0, T ; U),
h ∈D(A)
and
CA−1
λ B ∈L(U, Z)
(3.76)
and introduce the new operator Cλ and the new variable Y2(t)
⎧
⎨
⎩
Cλ = CA−1
λ
∈L(H, Z),
Y2(t) = AλY1(t) + Bλ
duλ
dt (t).
(3.77)
This situation is slightly diﬀerent from the corollary to Theorem 1.1 because
it cannot be a priori assumed that Bu(·) ∈C([0, T ]; H). So we assume that
uλ(0) = 0 in (3.74). By integration by parts we get
⎧
⎪
⎨
⎪
⎩
Y2(t) = Sλ(t)Y2(0) +
 t
0
Sλ(t −s)Bλ
d2uλ
ds2 (s) ds,
Y2(0) = Aλh + Bλ
duλ
dt (0).
(3.78)
The observation equation is
zλ(t) = Cyλ(t) = C[Y1(t) −Bλuλ(t)] = CA−1
λ [AλY1(t) −AλBλuλ(t)]
and
zλ(t) = Cλ

Y2(t) −Bλ
duλ
dt (t)
	
−CλAλBλuλ(t).
Finally
zλ(t) = CλY2(t) −CλBλu′
λ(t) −CλAλBλuλ(t).
(3.79)
The second term belongs to H1(0, T ; Z) and the last term to H2(0, T ; Z)
because we have assumed that
CλAλBλ = CA−1
λ B ∈L(U, Z).
(3.80)
As for the ﬁrst term
CλY2(t) = CSλ(t)h + CλSλ(t)Bu′
λ(0) +
 t
0
CλSλ(t −s)Bλu′′
λ(s) ds,
it is well deﬁned for h ∈D(A) and all uλ ∈H2(0, T ; U) such that uλ(0) = 0.
Finally zλ ∈C([0, T ]; Z) for all h ∈D(A) and u ∈H2(0, T ; U) such that
u(0) = 0. We summarize our conclusions in the next theorem.
Theorem 3.5. Assume that λ ∈R is such that Aλ be an isomorphism. If the
compatibility relation
CA−1
λ B ∈L(U, Z)
(3.81)
is veriﬁed, then the linear map

3 Unbounded control and observation operators
221
u 	→z : {u ∈H2(0, T ; U): u(0) = 0} →L2(0, T ; Z)
(3.82)
is continuous, where
z(t) = eλtzλ(t),
uλ(t) = e−λtu(t),
(3.83)
zλ(t) = −CλAλBλuλ(t) + Cλ
 t
0
Sλ(t −s)Bλ
d2uλ
ds2 (s) ds
−CλBλu′
λ(t) + CλSλ(t)Bλu′
λ(0).
(3.84)
Hypothesis 3.3 is veriﬁed if and only if the map (3.82) can be continuously
extended to all L2(0, T ; U).
Remark 3.1. Hypothesis (3.82)–(3.84) is veriﬁed under the compatibility rela-
tion (3.81). However, it is not clear whether Hypothesis (3.82)–(3.84) implies
(3.81).
⊓⊔
When the control and observation operators are “more unbounded”
B ∈L(U, D(A∗n)′)C ∈L(D(Am), Z),
CA−ℓ
λ B ∈L(U, Z),
1 ≤ℓ≤m + n,
(3.85)
the previous constructions can be repeated. For the control operator we in-
troduce the new variable
Yn(t) = yλ(t) +
n+1

i=0
An−(i+1)
λ
Bλ
diuλ
dti (t)
(3.86)
is continuous and the operator
Bλ = A−n
λ B ∈L(U, H).
(3.87)
The new variable is given by the expression
Yn(t) = Sλ(t)Yn(0) +
 t
0
Sλ(t −s)Bλ
dnuλ
dsn (s) ds.
(3.88)
Similarly on the observation side we deﬁne the new state variable
Yn+m(t) = Am
λ Yn(t) +
m−1

ℓ=0
Am−ℓ−1
λ
Bλ
dn+ℓuλ
dtn+ℓ(t)
(3.89)
and the operator
Cλ = CA−m
λ
∈L(H, Z).
(3.90)
The new state is given by
Yn+m(t) = Sλ(t)Yn+m(0) +
 t
0
Sλ(t −s)dn+muλ
dsn+m (s) ds
(3.91)

222
II-3 Semigroup Methods for Unbounded Control and Observation
and the observation by
zλ(t) = Cyλ(t) = CA−m
λ
Am
λ yλ(t) = CλAm
λ yλ(t)
(3.92)
or
zλ(t) = CλYn+m(t) −
m+n−1

ℓ=0
CλAm+n−1−ℓ
λ
Bλ
dℓuλ
dtℓ(t).
(3.93)
Notice that in view of hypothesis (3.85) the terms in uλ all make sense for
h ∈D(Am), u ∈Hm+n(0, T ; U), and u(ℓ)(0) = 0, 0 ≤ℓ≤m + n −1, and that
a statement similar to the one of Theorem 3.5 can be made.
4 Time-invariant variational parabolic systems
In this section we specialize the results of §1 and §2 in Chapter 2 of Part II
to the time-invariant case, compare them with the ones that can be obtained
by semigroup methods, and relate them to the semigroup model for control
of §3. The starting points are Theorem 1.1 in §1.3 and Theorem 1.4 in §1.5
(Chapter 2 of Part II). More precisely we start with a continuous linear op-
erator
A ∈L(V, V ′)
(4.1)
for which the V –H coerciveness hypothesis
∃α > 0, ∃λ ∈R,
∀v ∈V, ⟨Av, v⟩V + λ|v|2 ≥α∥v∥2
(4.2)
is veriﬁed. Theorem 1.1 says that, for all T > 0, the variational diﬀerential
equation
Ay + dy
dt = f ∈L2(0, T ; V ′),
y(0) = y0 ∈H
(4.3)
has a unique solution y in W(0, T ; V, V ′) ⊂C([0, T ]; H). For f = 0 the solu-
tions of the above equation generate a strongly continuous semigroup {S(t)}
on the Hilbert space H
S(t)y0 = y(t),
t ≥0,
∀y0 ∈H.
(4.4)
Its inﬁnitesimal generator coincides with −A, the domain of which will be
written D(A). We emphasize the fact that in other chapters on semigroup the
inﬁnitesimal generator is usually written A. The notation −A is exceptionally
used to be consistent throughout this section.
In §1 we have seen that the function
y(t) = S(t)y0 +
 t
0
S(t −s)f(s) ds
in D(−A∗)′
(4.5)
is also the solution of system (4.3) in V(0, T ; D(−A∗)′, D(−A∗2)′), which is a
larger space. Theorem 1.1 contains the following regularity property: For all
T > 0, the linear map

4 Time-invariant variational parabolic systems
223
S : L2(0, T ; V ′) →W(0, T ; V, V ′),
(Sf)(t) =
 t
0
S(t −s)f(s) ds
(4.6)
is continuous.
From semigroup theory in Chapter 1 of Part II (cf. §2, Theorem 2.12),
the semigroup {S(t)} generated by −A on the Hilbert space H is analytic.
Moreover for the λ of the coercivity assumption (4.2) the operator
−Aλ = −A −λI
(4.7)
is the inﬁnitesimal generator of the strongly continuous analytic contraction
semigroup
Sλ(t) = e−λtS(t),
t ≥0
(4.8)
on H. As a consequence the linear map
Sλ : L2(0, T ; H) →W(0, T ; D(−A), H) ⊂C([0, T ]; [D(−A), H]1/2),
(Sλf)(t) =
 t
0
Sλ(t −s)f(s) ds
(4.9)
is continuous and there exists a constant M > 0 independent of T > 0 such
that
∥−AλSλf∥L2(0,T ;H) ≤M∥f∥L2(0,T ;H).
(4.10)
But under assumption (4.2), −Aλ is invertible and
∥Sλf∥W(0,T ;D(−A),H) ≤M ′∥f∥L2(0,T ;H)
(4.11)
for some constant M ′ > 0 independent of T and f. From the above property
and the identity
(Sf)(t) = eλt[Sλ(e−λ•f)](t),
(4.12)
we can obtain for Sf an inequality similar to (4.10) but with a constant
M(T, λ) > 0, that now depends on T and λ (monotonically increasing with
λ ≥0):
∥Sf∥W(0,T ;D(−A),H) ≤M(T, λ)∥f∥L2(0,T ;H).
(4.13)
This last identity shows that we have the regularity result
S : L2(0, T ; H) →W(0, T ; D(−A), H)
(4.14)
for the map (4.6). Now −Aλ = −A −λI is an isomorphism from D(−A) onto
H and from H onto D(−A∗)′. It follows from the identity
A−1
λ Sf = SA−1
λ f
(4.15)
that
S : L2(0, T ; D(−A∗)′) →W(0, T ; H, D(−A∗)′)
(4.16)

224
II-3 Semigroup Methods for Unbounded Control and Observation
is also an isomorphism which can be regarded as an extension of (4.14). In
particular for α, 0 ≤α ≤1,
S : L2(0, T ; [H, D(−A∗)′]α)
→W(0, T ; [D(−A), H]α, [H, D(−A∗)′]α)
(4.17)
is an isomorphism that can also be expressed in terms of the domains of the
fractional powers Aα
λ of Aλ (cf. Chapter 1, §6, Proposition 6.1)
D(Aα
λ) = [D(−A), H]1−α = [D(A), H]1−α.
(4.18)
Finally
S : L2(0, T ; D(A∗α
λ )′) →W(0, T ; D(A1−α
λ
), D(A∗α
λ )′)
(4.19)
and in particular for α = 1
2
S : L2(0, T ; D(A∗1/2
λ
)′) →W(0, T ; D(A1/2
λ ), D(A∗1/2
λ
)′).
(4.20)
Comparing the above map to (4.6), we cannot conclude that they coincide.
However if we assume that
D(A1/2
λ ) = [D(A), H]1/2 = [D(A∗), H]1/2 = D(A∗1/2
λ
),
(4.21)
then
[D(A), H]1/2 = V = [D(A∗), H]1/2
(4.22)
and we recover (4.6) (cf. Chapter 2, Remark 1.2 and Theorem 1.3). It says
that in the variational case (4.6) is generally true without assumption (4.22).
For the control through a Neumann boundary condition, we have seen in
§1.2 (Chapter 2, Part II) that the control operator B is of the form
⟨Bu, v⟩V =

Γ
uv

Γ dΓ,
u ∈L2(Γ), v ∈V,
(4.23)
where
H = L2(Ω),
V = H1(Ω),
D(A∗) =

v ∈H2(Ω): ∂v
∂νA

Γ
= 0

,
D(A) =

v ∈H2(Ω): ∂v
∂ν∗
A

Γ
= 0

.
(4.24)
Here the hypotheses of Theorem 1.3 (Chapter 2 in Part II) are veriﬁed with
X = H2(Ω) and
D(A1/2) = D(A∗1/2) = V.
(4.25)
Notice also that not only B ∈L(U, V ′) but also
B ∈L(U, H1/2+2ε(Ω)′),
ε > 0
(4.26)

4 Time-invariant variational parabolic systems
225
because the trace operator
v 	→v|Γ : H1/2+2ε(Ω) →U = L2(Γ)
(4.27)
is continuous. Moreover
H1/2+2ε(Ω) = [D(A∗), H]3/4−ε = D(A∗(1/4+ε))
(4.28)
and
B ∈L(U, D(A∗(1/4+ε))′).
(4.29)
For the control through a Dirichlet boundary condition, we have shown in
§2.1 (Chapter 2 in Part II) that the control operator B is
⟨Bu, v⟩D(A∗) =

Γ
u ∂v
∂νA∗

Γ
dΓ,
u ∈L2(Γ), v ∈D(A∗),
(4.30)
where
H = L2(Ω),
V = H1
0(Ω),
D(−A) = D(−A∗) = H2(Ω) ∩H1
0(Ω),
(4.31)
which necessarily implies (4.23). Here the trace operator
v 	→
∂v
∂vA∗

Γ
: H3/2+2ε(Ω) ∩H1
0(Ω) →L2(Γ),
ε > 0
(4.32)
is continuous,
H3/2+2ε(Ω) ∩H1
0(Ω) = [D(A∗), H]1/4−ε = D(A∗(3/4+ε)),
(4.33)
and
B ∈L(U, D(A∗(3/4+ε))′),
U = L2(Γ).
(4.34)
In both cases hypothesis (4.22) is veriﬁed and
SB : L2(0, T ; U) →W(0, T ; H, D(A∗)′) ⊂L2(0, T ; H).
(4.35)
Sharper results can also be obtained in view of (4.27) and (4.32): For Neumann
SB : L2(0, T ; U) →W(0, T ; D(A3/4−ε), D(A∗(1/4+ε))′),
(4.36)
and for Dirichlet
SB : L2(0, T ; U) →W(0, T ; D(A1/4−ε), D(A∗(3/4+ε))′).
(4.37)
In §2 we have shown that under Hypothesis 3.1 the map
SB : L2(0, T ; U) →C([0, T ]; H)
(4.38)
is continuous. Hypothesis 3.1 amounts to prove that the map

226
II-3 Semigroup Methods for Unbounded Control and Observation
h 	→B∗S∗(·)h: H →L2(0, T ; U ′)
(4.39)
or, equivalently, that the map
h 	→B∗v: H →L2
0, T ; L2(Γ)

≡L2(Σ)
(4.40)
is continuous for v the solution of the adjoint system
A∗v + ∂v
∂t = 0
on ]0, T [,
v(T ) = h.
(4.41)
For the Neumann case
A∗v + ∂v
∂t = 0
in Q,
∂v
∂νA∗= 0
on Σ,
v(T ) = h
in Ω,
(4.42)
and
B∗v = v|Σ.
(4.43)
Here for h in H, v ∈W(0, T ; V, V ′) ⊂L2(0, T ; V ) and
v 	→v|Σ : L2
0, T ; H1(Ω)

→L2
0, T ; L2(Γ)

= L2(Σ)
(4.44)
is linear and continuous. Therefore, Hypothesis 3.1 is veriﬁed.
For the Dirichlet case
A∗v + ∂v
∂t = 0
in Q,
v = 0
on
Σ,
v(T ) = h
in
Ω,
(4.45)
and
B∗v =
∂v
∂νA∗

Σ
.
(4.46)
For h in V , the map
v 	→
∂v
∂νA∗

Σ
: W(0, T ; D(A∗), H) →H1/2,1/4(Σ)
(4.47)
is continuous, but for all h in H,
v ∈H1/2,1/4(Σ)
and
∂v
∂νA∗

Σ
̸∈L2(Σ).
(4.48)
From this Hypothesis 3.1 fails and we would conclude that the elegant
framework of this chapter is not adequate for parabolic systems with control

4 Time-invariant variational parabolic systems
227
through a Dirichlet boundary condition. However if we ﬁrst perform the change
of variable of §2.5 (Chapter 2 in Part II)
˜y(t) = A−1/2y(t),
˜v(t) = A1/2v(t),
(4.49)
then
d˜y
dt = −A˜y + ˜Bu,
˜B = A−1/2B ∈L(U, V ′),
˜y(0) = A−1/2y(0) ∈H,
(4.50)
and
d˜v
dt = −A˜v,
˜v(T ) = A1/2v(T ) ∈H =⇒v(T ) = h ∈V.
(4.51)
Thus in view of (4.45), Hypothesis (3.1) is veriﬁed and the framework of
this chapter is now appropriate for a parabolic system with control through a
Dirichlet boundary condition. In addition,
˜y ∈L2(0, T ; V ) =⇒A−1/2y ∈L2(0, T ; V ) =⇒y ∈L2(0, T ; H),
(4.52)
but we only have
y ∈C([0, T ]; V ′).
(4.53)
In fact we do not need the full force of A−1/2 and A−1/4+ε is suﬃcient because
B ∈L(U, D(A∗(1/4+ε))′) and
˜B = A−1/4+εB ∈L(U, D(A∗1/2)′) = L(U, V ′)
because
A∗(1/4−ε) ∈L

D(A∗1/2), D(A∗(1/4+ε))

.

4
State Space Theory of Diﬀerential Systems
With Delays
1 Introduction
In this chapter our objective is to present a modern approach that provides
a unifying framework for a large family of diﬀerential and integro-diﬀerential
systems with delays. This point of view is of paramount importance for the
Control Theory, Filtering Theory, and Realization Theory of such systems.
The material presented in this chapter is an outgrowth of the lecture notes
in French presented at the INRIA School on Representation and Control of
Delay Systems in June 1984 by M. C. Delfour [14]. The original material
has been restructured, the proofs have been streamlined, and new results have
been introduced in §6.
According to A.D. Myshkis [1], the origin of diﬀerential equations with
delays or “diﬀerential equations with a retarded argument” goes back to Con-
dorcet in a “M´emoire de l’Acad´emie des Sciences” dated 1771 about a prob-
lem studied by Euler in 1740. So it is a topic with a relatively long history.
To appreciate the speciﬁcity of a delay diﬀerential equation, recall that
ordinary diﬀerential equations are essentially local (in time) relations of the
form
F

t, x(t), x(1)(t), x(2)(t), . . . , x(n)(t)

= 0
(1.1)
between the function x and its derivatives x(i), 1 ≤i ≤n. Equation (1.1)
relates x and its derivatives at time t. However other types of relations F can
be constructed that relate x and its derivatives at diﬀerent times. For instance
x(1)(t) −x(t −1) = 0
or x(1)(t) −x( t
2) = 0.
(1.2)
Such relations belong to the general class of “Functional Diﬀerential Equa-
tions or Diﬀerential Equations with a Deviating Argument (in papers from
U.R.S.S.).” They essentially deﬁne a functional relation G between the func-
tion x and its derivative,
G(x, x(1), . . . , x(n)) = 0,
(1.3)

230
II-4 State Space Theory of Diﬀerential Systems With Delays
as opposed to the local relation F in (1.1) at time t. At that level of generality
everything is possible and the variable x need not be interpreted as a function
of time. However it is customary to call the variable t the time and we shall
follow this convention.
Functional Diﬀerential Equations (FDEs) have often been classiﬁed into
three categories: retarded type as in (1.2) advanced type
x(1)(t) = x(t + 2)
(1.4)
or neutral type
x(1)(t) = x(1)(t −1) + x(t −1) + x(t).
(1.5)
In this chapter we shall take the down-to-earth point of view that a real
system is nonanticipatory and limits our analysis to systems where the solution
x(t′), t′ > t, only depends on the past values x(s), s ≤t, of the variable x.
This deﬁnition does not only cover diﬀerential equations with delays but also
important classes of integral or diﬀerence equations as we shall see in §1.2.
This broad family of systems will be referred to as Delay Systems or sometimes
as Hereditary Systems.
Such systems enter into the modelization of many problems: technolog-
ical systems (e.g., chemical processes, rolling mills), the classical two-body
problem in electrodynamics, the dynamics of nuclear reactors, the transmis-
sion line model, population models, biomedical systems (e.g., control of the
human respiratory system), the propagation of diseases in a population, the
control of epidemics, economic systems, and even the famous macro-economic
models “`a la “ Forrester” (cf. M. C. Delfour and A. Manitius [1] for
details and speciﬁc references).
Detailed bibliographies on FDEs have been initiated by A.D. Myshkis [2]
in 1949 and periodically updated by N. H. Choksy [1]. The ﬁrst books writ-
ten in English were probably those of E. Pinney [1] in 1958, R. Bellman
and K. L. Cooke [2] in 1959 and 1963, J. A. Nohel [1] in 1964, A. Ha-
lanay [1] in 1966, and the translations in 1966 of the 1964 book of Elsgolc
and the one of A.D. Myshkis [2] in 1951. This topic has been widely stud-
ied by numerous mathematicians and engineers everywhere in the world. We
recommend the book of J. K. Hale [3] and its bibliography for a detailed
treatment of delay systems up to 1977. Several references to the engineering
literature have been included in our bibliography because problems in Con-
trol Theory have often been the motivation behind the various state space
formulations we shall encounter in this chapter.
This chapter will emphasize the product space approach, which seems to
have been independently introduced in the late sixties–early seventies by
M. Artola [1, 2, 3] for parabolic partial diﬀerential equations with de-
lays, M. C. Delfour and S. K. Mitter [2, 3, 4, 5, 9] for nonlinear
time-varying delay diﬀerential equations, and by Ju. G. Borisovic and
A. S. Turbabin [1] for nonhomogeneous linear time-invariant delay diﬀer-
ential equations. At that time our objective was to bring the theory of sys-
tems with delays more in line with the modern theory of partial diﬀerential

2 Examples and orientation
231
equations in Hilbert spaces and build up a uniﬁed Control Theory of Inﬁnite
Dimensional Systems. In that regard delays systems were very appealing be-
cause they were halfway between ordinary diﬀerential equations and partial
diﬀerential equations.
2 Examples and orientation
2.1 Examples
In this section we provide a series of simple examples to introduce and moti-
vate the model and the constructions, which will be used in this chapter.
Example 2.1. Delay diﬀerential equations.
˙y(t) =
N

i=0
y(t −i),
t > 0, y(θ) = φ(θ),
−N ≤θ ≤0,
(2.1)
where N ≥0 is an integer.
⊓⊔
Example 2.2. Volterra integro-diﬀerential equations.
˙y(t) =
 t
0
A(r −t)y(r) dr,
t > 0, y(0) = φ0,
(2.2)
where A is an L1-function.
⊓⊔
Example 2.3. Integro-diﬀerential equations.
⎧
⎪
⎨
⎪
⎩
˙y(t)
=
 0
−h
A1(θ)y(t + θ) dθ +
 0
−h
A2(θ) ˙y(t + θ) dθ,
t > 0,
y(θ)
= φ(θ),
−h ≤θ ≤0,
(2.3)
where A1 and A2 are square integrable functions.
⊓⊔
Example 2.4. Functional diﬀerential equations of neutral type.
˙x(t) = ˙x(t −1),
t > 0, x(θ) = φ(θ),
−1 ≤θ ≤0.
(2.4)
⊓⊔
Example 2.5. Volterra integral equations.
x(t) =
 t
0
A(s −t)x(s) ds + f(t),
t ≥0,
(2.5)
where A and f are locally integrable.
⊓⊔

232
II-4 State Space Theory of Diﬀerential Systems With Delays
Example 2.6. Diﬀerence equations.
⎧
⎪
⎨
⎪
⎩
x(t) =
N

i=1
Aix(t −i),
t > 0,
x(θ) = φ(θ),
−N ≤θ ≤0,
(2.6)
where N ≥1 is an integer.
⊓⊔
Example 2.7. Delay-diﬀerential equations with delays in the trajectory, con-
trol and observation variables.
⎧
⎪
⎨
⎪
⎩
˙y(t) =
N

i=0
[y(t −i) + u(t −i)],
x(t) = C0y(t) + C1y(t −N) + K0u(t) + K1u(t −N),
(2.7)
where N ≥0 is an integer.
⊓⊔
The ﬁrst observation is that in Examples 2.1, 2.9, and 2.10 (resp. 2.11, 2.12
and 2.13), y(t) (resp. x(t)) is not a good candidate for the state at time t ≥0.
The intuitively natural one is
yt
(resp. xt): I(−h, 0) →R,
yt(θ) = y(t + θ)
(resp. xt

θ) = x(t + θ)

,
(2.8)
where h, 0 < h ≤+∞is the length of the memory of the system and
I(−h, 0) = R ∩[−h, 0].
(2.9)
So in Examples 2.8 and 2.9, the diﬀerential equation is of the form
˙y(t) = Hyt,
t ≥0,
(2.10)
for an appropriate real-valued map H deﬁned on the space of real-valued
continuous functions on I(−h, 0) or, as we shall see later, on some subspace
in the case of inﬁnite memory (h = +∞).
Example 2.8.
h = N,
Hψ =
N

i=1
ψ(−i),

˙y(t) = Hyt,
t > 0,
y0 = φ.
(2.11)
⊓⊔
Example 2.9.
h = +∞,
Hψ =
 0
−∞
A(θ)ψ(θ) dθ,

˙y(t) = Hyt,
t > 0,
y(0) = φ0,
y0 = 0.
⊓⊔

2 Examples and orientation
233
Proceeding in this way with Example 2.10, it is readily seen that the map
H must be deﬁned on the Sobolev space W 1,p(−h, 0; R).
Example 2.10.
0 < h < +∞,
Hψ =
 0
−h
[A1(θ)ψ(θ) + A2(θ) ˙ψ(θ)] dθ.
⊓⊔
A complete theory is available in the product space
M p = R × Lp(−h, 0; R)
(2.12)
for systems of the form
˙y(t) = Hyt + f(t),
t > 0,
y(0) = φ0, y0 = φ1,
(2.13)
where f ∈Lp
loc(0, ∞; R), (φ0, φ1) ∈M p, and H is a continuous linear map
H : W 1,p(−h, 0; R) →R,
1 ≤p < ∞,
(2.14)
for ﬁnite or inﬁnite memory, 0 < h ≤+∞(cf. M. C. Delfour [8]). Moreover
this class of maps cannot be enlarged in this framework.
The construction used in Examples 2.8 to 2.10 does not apply to Exam-
ple 2.11 because the map
Hψ = ˙ψ(−1)
would have to be deﬁned on the smaller space C1(−1, 0; R)1 of continuously
diﬀerentiable functions. To get around this diﬃculty, we group terms involving
a derivative of x:
d
dt[x(t) −x(t −1)] = 0
and note that the above expression is of the form
d
dtMxt = 0,
Mψ = ψ(0) −ψ(−1).
This last construction suggests the more general model
d
dt[Mxt] = Lxt + f(t),
t > 0, x0 = φ.
(2.15)
Equivalently by introducing a new variable y(t)
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
Mxt −y(t) = g(t),
t > 0,
x0 = φ2,
˙y(t) −Hyt −Lxt = f(t),
t > 0,
y(0) = φ0,
y0 = φ1,
(2.16)
we obtain a model that can handle Examples 2.11 to 2.13.
1 Recall the notation C(−1, 0; R) for C([−1, 0]; R).

234
II-4 State Space Theory of Diﬀerential Systems With Delays
Example 2.11. h = 1, Mψ = ψ(0) −ψ(−1), L = 0, and H = 0,
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
Mxt −y(t) = 0,
t > 0,
x0 = φ,
˙y(t) = 0,
t > 0,
y(0) = Mφ.
(2.17)
⊓⊔
Example 2.12. h = +∞, L = 0, and H = 0,
Mψ = ψ(0) −
 0
−∞
A(θ)ψ(θ) dθ,
(2.18)
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
Mxt −y(t) = f(t),
t > 0,
x0 = 0,
˙y(t) = 0,
t > 0,
y(0) = 0.
(2.19)
⊓⊔
Example 2.13. h = N, L = 0, and H = 0,
Mψ = ψ(0) −
N

i=1
Aiψ(−i),
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
Mxt −y(t) = 0,
t > 0,
x0 = φ,
˙y(t) = 0,
t > 0,
y(0) = 0.
(2.20)
⊓⊔
The last example requires an additional operator N in model (2.16):
⎧
⎪
⎨
⎪
⎩
Mxt −Nyt = B1ut + g(t),
t > 0,
˙y(t) −Hyt −Lxt = B0ut + f(t),
t > 0,
x0 = φ2,
y0 = φ1,
u0 = w.
(2.21)
Example 2.14. 2.7 h = N, L = 0,
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
Mψ = ψ(0),
Nψ = C0ψ(0) + C1ψ(−N),
Hψ =
N

i=0
ψ(−i),
B0ψ =
N

i=0
ψ(−i),
B1ψ = K0ψ(0) + K1ψ(−N).
(2.22)

2 Examples and orientation
235
The interesting feature of the application of model (2.21) to Example 2.7 is
that the observation x becomes one of the unknown variables similar to y:
⎧
⎪
⎨
⎪
⎩
˙y(t) −Hyt = B0ut,
t > 0,
y(0) = φ0,
y0 = φ1,
u0 = w,
x(t) −Nyt = B1ut,
t > 0.
(2.23)
⊓⊔
So the ﬁnal mathematical model we shall adopt for our analysis will be
⎧
⎪
⎨
⎪
⎩
Mxt −Nyt = B1ut + f(t),
t > 0,
˙y(t) −Hyt −Lxt = B0ut + g(t),
t > 0,
(y(0), y0, x0) = (φ0, φ1, φ2),
u0 = w.
(2.24)
This model has been studied in detail by M. C. Delfour and J. Karrak-
chou [1, 2], with appropriate assumptions on the various maps.
2.2 Orientation
Model (2.24) is generic of a very large class of delay systems with ﬁnite or
inﬁnite memories. Our objective is to discuss the choice of state and state space
for linear control systems with observation. We ﬁrst consider Lipschitzian
delay-diﬀerential equations of the form

˙x(t) = f(t, xt),
t > 0,
x0 = φ
(2.25)
and then specialize to time-invariant linear control systems of the form

˙x(t) = Lxt + B0ut + f(t),
t > 0,
x0 = φ
(2.26)
with observation equation
y(t) = Cxt + B0ut.
(2.27)
The complete theory for the general model (2.24) can be found in M. C. Del-
four and J. Karrakchou [1, 2]. It was felt that the specialization to system
(2.26)–(2.27) was suﬃcient as a ﬁrst introduction and will better illustrate
the ideas and constructions behind our approach using product spaces and
structural operators.
In §3 we discuss the existence and uniqueness of the solution for Lip-
schitzian systems of the form (2.25). We compare the classical assumptions
for initial conditions in the space of continuous functions with those required
for initial conditions, which are only Lp-functions. In this last case we also

236
II-4 State Space Theory of Diﬀerential Systems With Delays
need an initial point to deﬁne the starting point of the trajectory. This is
the so-called product space approach because the initial datum is a pair
(φ0, φ1) ∈Rn × Lp(−h, 0; Rn). A key result is that all time-invariant lin-
ear systems can be studied in the product space framework under the same
assumptions as in the continuous functions framework. This last result is of
paramount importance for control problems because the state space can be
chosen as a Hilbert or a reﬂexive Banach space bringing delay systems in line
with partial diﬀerential equations in Hilbertian or reﬂexive Sobolev spaces.
In §4 we construct a ﬁrst state and a state equation, study transposed and
adjoint systems, introduce structural operators, and intertwining operators,
and characterize adjoint semigroups and their inﬁnitesimal generators. This
is the extension of the traditional state in the continuous function framework.
In particular the reader will ﬁnd the implicit characterization of the adjoint
of the inﬁnitesimal generator associated with the state for systems with both
ﬁnite or inﬁnite memory. This question was initially raised by R. B. Vin-
ter [2], but its importance has not always been fully appreciated. We shall
see how fundamental it is in the following sections. This section also contains
many other key technical results, which will be extensively used in subsequent
sections
Section 5 introduces the control variable and the two fundamental choices
of deﬁnition of the state. Section 6 incorporates a delayed observation equation
and extends the two deﬁnitions of state to ﬁnally obtain an evolution equation
and an observation equation in a product space Rn × Lp × Lp without delays
but with unbounded control and observation operators.
This chapter will emphasize the importance of the construction of a state,
which is fundamental in Control Theory. For delay systems this key devel-
opment seems to have come from N. N. Krasovskii [1, 2] in 1959. Yet the
use of the nonreﬂexive Banach space of continuous functions at the level of
the classical theory of semigroups was not completely satisfactory for a purely
technical reason. Around 1969 it was then understood that for a very large
class of delay systems the state space could be enlarged from continuous func-
tions to an initial point and an initial Lp-function. This led to the product
space approach, which was independently introduced in the late sixties-early
seventies by M. Artola [1, 2, 3] for parabolic partial diﬀerential equations
with delays, M. C. Delfour [1, 3, 5, 8] and M. C. Delfour and S. K. Mit-
ter [2, 3, 4, 5, 9] for nonlinear time-varying delay diﬀerential equations, and
Ju. G. Borisovic and A. S. Turbabin [1] for nonhomogeneous linear time-
invariant delay diﬀerential equations.
At that time our objective was to bring the theory of systems with delays in
line with the modern theory of partial diﬀerential equations in Hilbert spaces
and build up a uniﬁed Control Theory of Inﬁnite Dimensional Systems. In
that regard delays systems were very appealing as they were halfway between
ordinary diﬀerential equations and partial diﬀerential equations. This techni-
cal contribution made it possible to give a complete mathematical solution
to the linear quadratic optimal control problem over ﬁnite and inﬁnite time

2 Examples and orientation
237
horizons (cf. M. C. Delfour and S. K. Mitter [3, 6, 8]; M. C. Delfour,
C. McCalla, and S. K. Mitter [1]; M. C. Delfour [3, 6, 9, 10, 12, 13,
15]; M. C. Delfour, E. B. Lee, and A. Manitius [1]). In the context of
numerical solutions the product space also turned out to be the appropriate
framework because it was disconnecting the initial point and the initial func-
tion (cf. M. C. Delfour [6]). For subsequent work by many other authors
(Banks, Burns, Gibson, Herdman, Ito, Kappel, Kunisch, Rosen, Schappacher,
Tran, and several others), the reader is referred for instance to the papers of
D. Salamon 4], and I. Lasiecka and A. Manitius [1] and their bibliogra-
phies.
Another important contribution was the construction of the structural op-
erators, which ﬁrst clariﬁed the relationship between the true adjoint and the
transposed system and provided the natural concepts of observability and con-
trollability. They were developed in Montr´eal in the 1975–78 period and ﬁrst
announced in December 1976 at the CDC by A. Manitius [5] and at the IN-
RIA by M. C. Delfour and A. Manitius [1]. For control systems without
delays in the control variable, the transformation of the state by the structural
operator was introducing a new state, the structural state, which was more
natural and better adjusted to the speciﬁc delay structure of the system. A
complete treatment of structural operators along with their implications in
spectral theory was given by M. C. Delfour and A. Manitius [2, 3] for
arbitrary delay functionals L on the space of continuous functions (see also
C. Bernier and A. Manitius [1], and A. Manitius [5, 6, 7, 8]) immediately
saw and exploited their potential in the study of the notions controllability
and observability. An extention of structural operators to time-varying sys-
tems was later done by F. Colonius, A. Manitius, and D. Salamon [1].
But it turned out that structural operators and the associated construction
of the structural state also play a fundamental role in the transformation of
systems with delays in the control variable into an evolution equation without
delays. The appropriate extention of the deﬁnition of structural state2 in Rn×
Lp was introduced by R. B. Vinter and R. H. Kwong [1] and generalized
by M. C. Delfour [15] to general delayed control operators. We still call it
structural state because it roughly corresponds to the transformation of the
usual state and the segment of the control function by the structural operators
associated with the delay structure of the system and the control. Another
state in Rn ×Lp ×Lp, the extended state, was introduced by A. Ichikawa [1,
2, 3]. This state follows the evolution of the pieces of trajectory xt and control
ut. He added ut to the original choice (x(t), xt) for the state in the product
2 In the recent literature the terminology “forcing function state” has been used (cf.
for instance the book by G. Grippenberg, S. O. Londen, and O. Staffans [1]).
This emphasizes one of the many properties of the underlying semigroup. It is ob-
viously a matter of taste. For instance we shall see in the proof of Theorem 5.1 that
this state can also be seen as the resulting product of transposition techniques.
In fact the underlying semigroup is nothing but the adjoint of the semigroup
associated with the state of the transposed system.

238
II-4 State Space Theory of Diﬀerential Systems With Delays
space. This theory will be completed and extended in §5.2. To our knowledge
those two deﬁnitions of the state seem to be the only interesting and natural
ones for linear control systems with delays. In retrospective—and it is now
more striking because we have a uniﬁed framework for the time-invariant
system (see §2.1)—it is extremely interesting to see that these two notions of
state had been discovered by D. H. Miller [1] around 1974 in the context
of Volterra equations.
All this was ﬁne but not quite complete. From the Control Theory point
of view it was necessary to be able to deal with a delayed observation equa-
tion. This is relatively simple to do with the extended state because we have
direct access to xt and ut. However this was far from being obvious with the
structural state. The appropriate construction came from D. Salamon [2]
who added a third component to the structural state. This extended struc-
tural state does not require a full knowledge of xt and ut but only of some
linear combinations through appropriate structural operators associated with
the control and observation delay structure. This construction was extended
and generalized to system (2.24) by J. Karrakchou [1] and M. C. Delfour
and J. Karrakchou [1, 2]. Similar developments using a slightly diﬀerent
approach were also done by D. Salamon [1, 3] for neutral systems.
We would like to emphasize that our objective in this chapter is to give
a systematic introductory but suﬃciently complete treatment of the above-
mentioned material, which is not available in standard textbooks. We feel that
it is now a fundamental part of the theory of delay systems, which brings them
in line with the general theory of control systems in inﬁnite dimension.
The optimal control problem will be covered in the second part of this
book. However it will appear as a special case of the general theory. Never-
theless many references have been included at the end of this chapter. For
early papers using various forms of the Maximum Principle, Dynamic Pro-
gramming, and abstract variational theories, the reader is referred to the ﬁrst
books of G. L. Kharatishvili [1] and M. N. Oguzt¨oreli [1] in 1966 and
the papers by H. T. Banks and A. Manitius [1] in 1974, F. Colonius [1]
in 1982, and the lecture notes of A. Manitius [4] in 1976.
Many papers deal with the theory of partial diﬀerential equations with
delays. It was unfortunately not realistic to include this material here. The
reader is referred to M. Artola [1, 2, 3, 4], and especially the bibliography
in M. Artola [4] on the work of the Italian School; A. Ardito and P. Ric-
ciardi [1]; G. Di Blasio, K. Kunisch, and E. Sinestrari [1]; J. Dyson
and R. Villella-Bressan [2]; K. Kunisch and W. Schappacher [1]; and
C. C. Travis and G. F. Webb [1, 2].
2.3 Notation
R denotes the ﬁeld of real numbers, and for an arbitrary integer n ≥1, Rn
will be the n- dimensional Euclidean space. The norm of x in Rn and the
inner product of x and y will be written |x| and x · y, respectively.

2 Examples and orientation
239
Given −∞≤a ≤b ≤+∞,
I(a, b) = R ∩[a, b].
For a real Banach space X, Lp(a, b; X) will denote the space of all equiva-
lence classes of p-integrable (resp. essentially bounded) Lebesgue measurable
functions on I(a, b) into X for 1 ≤p < ∞(resp. p = ∞). The derivative of a
function x on I(a, b) into X will be denoted ˙x, dx/dt, Dx, or Dtx (in the dis-
tributional sense). The Sobolev space of all y in Lp(a, b; X) with distributional
derivatives Dj
ty, j = 1, . . . , m, in Lp(a, b; X) will be written W m,p(a, b; X).
C(a, b; X) will be the Banach space of all bounded continuous functions
x from I(a, b) into X. For m ≥1, Cm(a, b; X) will be the space of all m-
times bounded continuous diﬀerentiable functions on I(a, b). C0(a, b; X) =
{x ∈C(a, b; X): ∀ε > 0, ∃a compact subset K of I(a, b) such that |x(t)| < ε,
∀t ∈Kc}, where Kc is the complement of K with respect to I(a, b),
Kc = {t ∈I(a, b): t /∈K};
when a and b are ﬁnite I(a, b) = [a, b] and C0 and C coincide. Cc(a, b; X) will
be the subspace of functions of C(a, b; X) with compact support in ]a, b[. It is
not to be confused with the space of bounded continuous functions with sup-
port in I(a, b). In general the two spaces do not coincide except on I(−∞, ∞).
In addition to the above function spaces, we shall also use the notation.
Floc(a, ∞, X) = {y : I(a, ∞) →X : ∀T > a, y|I(a,T ) ∈F(a, T; X)}
for any function space F (for instance F can be C, L2, W 1,p, etc. . . . ).
D(]a, b[; Rn) will denote the vector space of all inﬁnitely continuously dif-
ferentiable functions from ]a, b[ into Rn. W m,p
0
(a, b; Rn) will be the closure of
D(]a, b[; Rn) in W m,p(a, b; Rn).
Given the integers n ≥1 and k ≥1, and real numbers p, 1 ≤p < ∞, and
h, 0 < h ≤+∞(possibly h = +∞), we shall use the following notation for
the two product spaces, which will frequently occur in this chapter:
M p = Rn × Lp(−h, 0; Rn),
Zp = Rn × Lp(−h, 0; Rn) × Lp(−h, 0; Rk).
Whenever confusion is possible, subscripts n and/or k will be added.
Given a real number p, 1 < p < ∞, and an integer ℓ≥1, the elements of
the topological dual Lp(a, b; Rℓ)′ of Lp(a, b; Rℓ) will be identiﬁed with those
of Lq(a, b; Rℓ), where q is the conjugate of p, p−1 + q−1 = 1. Similarly (M p)′
and (Zp)′ will be identiﬁed with M q and (Zq)′, respectively.
Given a real measure µ on a σ-algebra of subsets of a set S, |µ| will denote
the total variation of µ (cf. W. Rudin [1, pp. 117–118]. The total variation of
an n × m matrix β of real measures {βij : 1 ≤i ≤n, 1 ≤j ≤m} is deﬁned as

240
II-4 State Space Theory of Diﬀerential Systems With Delays
|β| =
 n

i=1
m

j=1
|βij|2
1/2
,
where |βij| is the total variation of βij.
3 Existence theorems for Lipschitzian systems
All examples in §2 implicitly suggest the introduction of a product space as the
space of initial conditions. In this chapter we present some general existence
theorems for Lipschitzian systems in both the continuous function and the
product space framework. In this way the reader will be in a better position
to appreciate the relative advantages and limitations and, more importantly,
the complementarity of the two approaches.
3.1 Continuous functions framework
We have seen that a delay system is characterized by the length of its memory
h, 0 < h ≤+∞. When h < +∞, we say that the system has a ﬁnite memory
and when h = +∞an inﬁnite memory. The notation
I(h, 0) = [−h, 0] ∩R
(3.1)
will be very convenient to simultaneously deal with [−h, 0] when h is ﬁnite
and ]−∞, 0] when h is inﬁnite. Denote by C(−h, 0; Rn) the space of bounded
continuous functions from I(−h, 0) →RN. When h = +∞it is not a Banach
space and its elements are not necessarily uniformly continuous. So we could
use one of the subspaces
Cℓ(−∞, 0; Rn) =

φ ∈C(−∞, 0; Rn):
lim
θ→−∞φ(θ) exists

,
C0(−∞, 0; Rn) =

φ ∈C(−∞, 0; Rn):
lim
θ→−∞φ(θ) = 0

.
(3.2)
In this chapter we choose the second subspace and introduce the following
uniform notation for the space of initial conditions:
K(−h, 0; Rn) =

C(−h, 0; Rn),
if h < ∞,
C0(−∞, 0; Rn),
if h = ∞.
(3.3)
We shall see in Lemma 3.5 that this is a natural choice because for p, 1 ≤p <
∞, W 1,p(−∞, 0; Rn) ⊂C0(−∞, 0; Rn). Given a function
f : [0, ∞[ × K(−h, 0; Rn) →Rn,
(3.4)
we consider the diﬀerential equation

3 Existence theorems for Lipschitzian systems
241
⎧
⎨
⎩
dx
dt (t) = f(t, xt),
t ≥0,
x0 = φ ∈K(−h, 0; Rn),
(3.5)
where xt : I(−h, 0) →Rn is deﬁned from the solution x: [0, ∞[ →Rn and the
initial condition φ as follows:
xt(θ) =

x(t + θ),
−t ≤θ ≤0,
φ(t + θ),
θ < −t
(3.6)
for θ in I(−h, 0).
Theorem 3.1. Assume that the function f in (3.4) veriﬁes the following as-
sumptions:
(H1) for each φ in K(−h, 0; Rn), the function
t 	→f(t, φ): [0, ∞[ →Rn
(3.7)
is Lebesque measurable,
(H2) there exists a non-negative locally integrable real function n such that for
all φ1 and φ2 in K(−h, 0; Rn)
|f(t, φ2) −f(t, φ1)| ≤n(t)∥φ2 −φ1∥C,
(3.8)
(H3) and the function
t 	→f(t, 0): [0, ∞[ →Rn
(3.9)
is locally integrable.
Then, given any initial condition φ in K(−h, 0; Rn), there exists a unique
absolutely continuous solution x = x(·; φ) to system (3.5) on [0, ∞[. Moreover
for each T > 0, there exists a constant c(T ) > 0 such that for all φ1 and φ2
in K(−h, 0; Rn)
∥x(·; φ2) −x(·; φ1)∥W 1,1(0,T ;Rn) ≤c(T )∥φ2 −φ1∥C,
(3.10)
where W 1,1(0, T ; Rn) denotes the space of absolutely continuous functions from
[0, T ] to Rn with a derivative in L1(0, T ; Rn).
This theorem is not the only existence theorem that is available. It has
been presented with a set of assumptions that contains as a special case linear
systems; that is, when the map f(t, φ) is aﬃne in φ
f(t, φ) = L(t)φ + f(t).
(3.11)
However it is easy to modify the set of assumptions (H1) to (H3) to obtain local
versions. Similarly for non-Lipschitzian functions f, it is possible to obtain the
analog of the classical Carath´eodory conditions and local existence theorems
(cf. J. K. Hale [1, 3]).
The proof of Theorem 3.1 necessitates the following two classical lemmas.

242
II-4 State Space Theory of Diﬀerential Systems With Delays
Lemma 3.1. Let T > 0 be ﬁxed and assume that assumptions (H1) to (H3)
are veriﬁed:
(i) For each z in K(−h, T ; Rn) the function
t 	→f(t, zt): [0, T ] →Rn
(3.12)
belongs to L1(0, T ; Rn), where
zt(θ) = z(t + θ),
θ ∈I(−h, 0),
t ≥0.
(3.13)
(ii) For all pairs
(φ1, x1),
(φ2, x2) ∈K(−h, 0; Rn) × C(0, T ; Rn)
(3.14)
such that φ1(0) = x1(0) and φ2(0) = x2(0) and all t in [0, T ]
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
 t
0
|f(s, (x2)s) −f(s, (x1)s)| ds,
≤
 t
0
n(s) max{∥φ2 −φ1∥C + ∥x2 −x1∥C(0,s)} ds.
(3.15)
Proof. (i) By deﬁnition of the space K(−h, 0; Rn) each of its element is a
uniformly continuous function and the function t 	→¯z(t) = zt belongs to
C

0, T ; K(−h, 0; Rn)

. In particular
¯z ∈L1
0, T ; K(−h, 0; Rn)

and there exists a sequence of step functions sn : [0, T ] →K(−h, 0; Rn) that
converges pointwise to ¯z for almost all t in [0, T ] and converges globally in
the L1 norm. To show that the function (3.12) is measurable, it is suﬃcient
to establish the following two properties:
1. fn(t) = f

t, sn(t)

→f(t) = f

t, ¯z(t)

, a.e. in [0, T ],
2. for all n, fn is Lebesque measurable on [0, T ].
Then the function f will be Lebesque measurable as a pointwise limit of a
sequence of Lebesque measurable functions.
By assumptions (H2) for each t the map φ 	→f(t, φ) is continuous and
necessarily
fn(t) = f

t, sn(t)

→f(t) = f

t, ¯z(t)

(3.16)
as n goes to inﬁnity. This proves property 1). For property 2) it is suﬃcient
to establish that for each step function s, the function fs(t) = f

t, s(t)

is
measurable. By deﬁnition a step function is of the form
s(t) =
k

i=1
aiχAi(t),
ai ∈K(−h, 0; Rn),
(3.17)

3 Existence theorems for Lipschitzian systems
243
where r ≥1 is a positive integer and the Ai’s are disjoint measurable subsets
of [0, T ] such that the measure of their union
A =
r/
i=1
Ai
is ﬁnite (χAi is the characteristic function of Ai). So fs can be rewritten as
f

t, s(t)

= f(t, 0)[1 −χA(t)] +
r

i=1
f(t, ai)χAi(t),
which is the sum of r + 1 measurable functions by assumptions (H1).
To show that f(t) is integrable, we evaluate its L1- norm
 T
0
|f(t, zt)| dt ≤
 T
0
|f(t, 0)| dt +
 t
0
|f(t, zt) −f(t, 0)| dt
and use assumption (H3) for the ﬁrst term and assumption (H2) for the second
term, which is bounded by
 T
0
n(t)∥zt −0∥C dt ≤∥n∥L1(0,T ;R)∥z∥C(−h,T ;Rn).
(ii) By choice of φi and xi, s 	→(xi)s belongs to C

0, T ; K(−h, 0; Rn)

and
the conclusions follow from part (i) and the Lipschitzian assumption (H2).
⊓⊔
Lemma 3.2. Let α, 0 < α < 1, be a real number and n be a non-negative
function in L1
loc(0, ∞; R):
(i) The function
gα(t) = exp

1
α
 t
0
n(s) ds

,
t ≥0,
(3.18)
is monotonically increasing and greater than or equal to 1.
(ii) For all t ≥0
 t
0
n(s)gα(s) ds ≤αgα(t).
(3.19)
(iii) For all T > 0 and x in C(0, T ; Rn) the quantity
∥x∥Cα(0,T ;Rn) = sup
t∈[0,T ]
{∥x(t)/gα(t)}
(3.20)
is an equivalent norm on the Banach space C(0, T ; Rn) and
1
gα(T )∥x∥C ≤∥x∥Cα ≤∥x∥C.
(3.21)

244
II-4 State Space Theory of Diﬀerential Systems With Delays
Proof. (i) is obvious, (ii) is obtained by diﬀerentiating gα(t), and the equiva-
lence of the norms in (3.21) follows by deﬁnition of the norm (3.20).
⊓⊔
Remark 3.1. The introduction of the function gα for global existence problems
is due to A. Bielecki [1]. It plays the same role as Gronwall’s inequality.
⊓⊔
Proof of Theorem 3.1.
The initial condition φ is ﬁxed. For each x in
C(0, T ; RN) such that x = φ(0), deﬁne the function Fx
(Fx)(t) = φ(0) +
 t
0
f(s, xs) ds,
0 ≤t ≤T.
In view of Lemma 3.1, F maps the closed subset
S = {x ∈C(0, T ; Rn): x(0) = φ(0)}
of C(0, T ; Rn) onto itself. We now prove that for all α, 0 < α < 1, F is a
contracting map on S. So by the Banach ﬁxed point theorem we get existence
and uniqueness of the solution to Fx = x or
x(t) = φ(0) +
 t
0
f(s, xs) ds,
0 ≤t ≤T.
(3.22)
Given x and y in S
|(Fy)(t) −(Fx)(t)| ≤
 t
0
|f(s, ys) −f(s, xs)| ds.
By assumption (H2) and inequality (3.15) in Lemma 3.1, the right-hand side
is bounded by
max
r∈[0,t]
|y(r) −x(r)|
gα(r)
 t
0
n(s)gα(s) ds
and in view of (3.19) in Lemma 3.2,
|(Fy)(t) −(Fx)(t)| ≤αgα(t)∥y −x∥Cα(0,t;Rn).
Finally
∥Fy −Fx∥Cα(0,T ;Rn) ≤α∥y −x∥Cα(0,T ;Rn)
and (3.22) has a unique solution in S. Moreover s 	→f(s, xs) belongs to
L1(0, T ; Rn) and necessarily x belongs to W 1,1(0, T ; Rn).
To obtain the estimate (3.10) let x1 and x2 be the respective solutions of
(3.22) for φ1 and φ2. Then
|x2(t) −x1(t)| ≤|φ2(0) −φ1(0)| +
 t
0
|f(s, x2
s) −f(s, x1
s)| ds,
and by using inequality (3.15) the integral term is bounded by

3 Existence theorems for Lipschitzian systems
245
∥n∥L1(0,T ;R)∥φ2 −φ1∥C +
 t
0
n(s) max
0≤r≤s{∥x2(r) −x1(r)∥} ds.
Now use (3.21) on [0, s] for the last integral, which is bounded by
 t
0
n(s)gα(s)∥x2 −x1∥Cα(0,s;Rn) ds ≤αgα(t)∥x2 −x1∥Cα(0,t;Rn).
Finally
|x2(t) −x1(t)| ≤[1 + ∥n∥L1(0,T )]∥φ2 −φ1∥C + αgα(t)∥x2 −x1∥Cα(0,T ;Rn),
(1 −α)∥x2 −x1∥Cα(0,T ;Rn) ≤(1 + ∥n∥L1(0,T ))∥φ2 −φ1∥C,
and in view of (3.21)
∥x2 −x1∥C(0,T ;Rn) ≤gα(t)
1 −α[1 + ∥n∥L1(0,T )]∥φ2 −φ1∥C.
(3.23)
For the derivative we again use (3.15)
∥˙x2 −˙x1∥L1 ≤
 T
0
|f(t, x2
t ) −f(t, x1
t )| dt
≤∥n∥L1 max{∥x2 −x1∥C, ∥φ2 −φ1∥C},
(3.24)
and by combining (3.23) and (3.24), we obtain (3.10).
⊓⊔
Remark 3.2. The proof of Lemma 3.1 is essentially the original proof given by
C. Carath´eodory [1] in 1927 generalized to Banach spaces.
⊓⊔
3.2 Lp or product space framework
We shall see now that in many cases, it is possible to separate the initial
point x(0) of the solution x to the diﬀerential equation (3.5) from the piece of
function x0 on I(−h, 0), which is necessary to make sense of the right-hand
side of (3.5). This will lead us to consider an initial condition as a point and
a function
φ = (φ0, φ1) ∈M p = Rn × Lp(−h, 0; Rn)
(3.25)
in the product space that will be denoted M p, where p, 1 ≤p < ∞, is a ﬁxed
real number. For the reader interested in the origin of the letter M, it was
just the letter next to L.
As in §3.1 we start with a function f(t, φ) that is deﬁned for φ in
K(−h, 0; Rn). When φ1 is an Lp function the segment
xt(θ) =

x(t + θ),
−t ≤θ ≤0,
φ1(t + θ),
θ < −t,
(3.26)
θ ∈I(−h, 0), is generally not a continuous function in K(−h, 0; Rn), but only
an Lp(−h, 0; Rn) function. So we cannot expect to give a pointwise meaning
to the function t 	→f(t, xt): [0, ∞[ →Rn.

246
II-4 State Space Theory of Diﬀerential Systems With Delays
Example 3.1. Consider the function
f(t, φ) = a(t)φ(−2),
φ ∈C(−2, 0; R),
(3.27)
where a is measurable and bounded on [0, ∞[. Then the function
t 	→f(t, xt) = a(t)x(t −2): [0, 2] →R
(3.28)
belongs to Lp(0, 2; R) for all x in Lp(−2, 2; R).
⊓⊔
Unfortunately not all functions f(t, φ) are extendable as can be seen from
the next example.
Example 3.2. Consider for φ ∈C(−2, 0; R) the function
f(t, φ) =

φ(t −1),
0 ≤t ≤1,
0,
t > 1.
(3.29)
For all continuous functions x in C(−2, 2; R), the function
t 	→f(t, xt) =

x(−1),
0 ≤t ≤1,
0,
1 < t ≤2 : [0, 2] →R
(3.30)
is well deﬁned and discontinuous at t = 1. However it has no extension to
functions x in Lp(−2, 2; R).
⊓⊔
Nevertheless we shall see later that the family of functions f that can be
extended to Lp initial conditions includes all linear time-invariant systems
that are of the form
f(t, φ) = Lφ + f(t),
(3.31)
where L is linear and continuous from K(−h, 0; Rn) into Rn and f belongs to
L1
loc(0, ∞; Rn).
The following theorem is the counterpart of Theorem 3.1.
Theorem 3.2. Fix the real number p, 1 ≤p < ∞, and h, 0 < h ≤+∞. Let
the map f given in (1.4) verify assumptions (H1) to (H3) in Theorem 3.1 and
the following additional assumption:
(H4) there exists a real non-negative monotonically increasing function such
that for all t ≥0 and all z1 and z2 in Cc(−h, t; Rn)
 t
0
|f(s, (z2)s) −f(s, (z1)s)| ds
≤m(t)
  t
−h
|z2(s) −z1(s)|p ds
1/p
.
(3.32)

3 Existence theorems for Lipschitzian systems
247
Then, given any φ = (φ0, φ1) in M p = Rn × Lp(−h, 0; Rn), there exists a
unique absolutely continuous solution x = x(·; φ) to the system
⎧
⎨
⎩
dx
dt (t) = f(t, xt),
t > 0,
(x(0), x0) = (φ0, φ1) = φ ∈M p.
(3.33)
Moreover for each T > 0, there exists a constant c(T ) > 0 such that for all
φ1 and φ2 in M p
∥x(·; φ2) −x(·; φ1)∥W 1,1(0,T ;Rn) ≤c(T )∥φ2 −φ1∥Mp,
(3.34)
where
∥φ∥Mp = ∥(φ0, φ1)∥Mp = [|φ0|p + ∥φ1∥p
Lp]1/p.
(3.35)
To complete this section we now give the proof of the last two theorems.
Theorem 3.2 necessitates two lemmas similar to the ones used in the proof of
Theorem 3.1.
Lemma 3.3. Fix the real numbers p, 1 ≤p < +∞, T > 0 and h, 0 < h ≤∞.
Assume that assumptions (H1) to (H4) are veriﬁed. Associate with each z in
Cc(−h, T ; Rn) the function fz in L1(0, T ; Rn) deﬁned as
fz(t) = f(t, zt),
0 ≤t ≤T.
(3.36)
Then the map
z 	→fz : Cc(−h, T ; Rn) →L1(0, T ; Rn)
(3.37)
extends to a unique uniformly continuous map from Lp(−h, T ; Rn) into
L1(0, T ; Rn). Moreover for all t in [0, T ] and z1 and z2 in Lp(−h, T ; Rn),
 t
0
|f(s, (z2)s) −f(s, (z1)s)| ds ≤m(t)∥z2 −z1∥Lp(−h,t;Rn).
(3.38)
Proof. By Lemma 3.1 to each z in Cc(−h, T ; Rn), we can associate a function
fz in L1(0, T ; Rn) deﬁned by (3.36). By assumption (H4) for all z1 and z2 in
Cc(−h, T ; Rn)
∥fz2 −fz1∥L1(0,T ;Rn) ≤m(T )∥z2 −z1∥Lp(−h,T ;Rn).
Thus the map (3.37) is uniformly continuous for the Lp(−h, T ; Rn) topol-
ogy. By density of Cc(−h, T ; Rn) in Lp(−h, T ; Rn), it has a unique uniformly
continuous extension to all Lp(−h, T ; Rn). As for inequality (3.38) we use as-
sumption (H4) on [0, t] and bound m(t) by m(T ) for 0 ≤t ≤T .
⊓⊔
Lemma 3.4. Let α, 0 < α < 1, p, 1 ≤p < ∞, and c > 0 be ﬁxed real
numbers:

248
II-4 State Space Theory of Diﬀerential Systems With Delays
(i) The function
gα(t) = exp
 c
α
p t
p
	
,
t ≥0,
(3.39)
is monotonically increasing and greater than or equal to 1.
(ii) For all t ≥0
c∥gα∥Lp(0,t;R) ≤αgα(t).
(3.40)
By introducing the norm (3.20) with gα given by (3.39), we also have the
equivalence of the norms on C(0, T ; Rn) and inequalities (3.21).
Proof. Diﬀerentiate gα(s)p with respect to s and integrate with respect to s
from 0 to t
d
dsgα(s)p =
 c
α
p
gα(s)p,
 t
0
cpgα(s)p ds = αp[gα(t)p −1] ≤αpgα(t)p.
⊓⊔
Proof of Theorem 3.2.
The proof follows the same pattern as the one of
Theorem 3.1. The initial condition φ = (φ0, φ1) is ﬁxed. For each x in the
closed subset
S = {x ∈C(0, T ; Rn): x(0) = φ0}
of C(0, T ; Rn), deﬁne the function
(Fx)(t) = φ0 +
 t
0
f(s, xs) ds,
0 ≤t ≤T.
The map x 	→Fx is well deﬁned from S to S. We prove that for any α, 0 <
α < 1, F is contracting on S and apply Banach ﬁxed point theorem to obtain
the existence of a unique x in S such that Fx = x or equivalently
x(t) = φ0 +
 t
0
f(s, xs) ds,
0 ≤t ≤T.
Given x and y in S
|(Fy)(t) −(Fx)(t)| ≤
 t
0
|f(s, ys) −f(s, xs)| ds.
By (3.38) in Lemma 3.3, the right-hand side is bounded by
m(t)∥y −x∥Lp(0,t;Rn).
Construct the function gα(t) in (3.39) with c = m(T ). Then
|(Fy)(t) −(Fx)(t)| ≤m(T )∥y −x∥Cα(0,t;Rn)∥gα∥Lp(0,t;R)
≤αgα(t)∥y −x∥Cα(0,t;Rn)
and F is contracting for the norm Cα. From this point on the proof is essen-
tially the same as the one of Theorem 3.1.
⊓⊔

3 Existence theorems for Lipschitzian systems
249
3.3 Linear time-invariant systems
The price to pay for initial conditions in Lp is the extra assumption (H4).
However (H4) is always veriﬁed for linear time-invariant systems of the form
(3.31).
Theorem 3.3. Assume that the map f(t, φ) is of the form (3.31) for f in
L1
loc(0, ∞; Rn) and a continuous linear map
L: K(−h, 0; Rn) →Rn :
(3.41)
(i) There exists a n × n matrix of regular Borel measures such that
Lφ =
 0
−h
dθ ηφ(θ),
∀φ ∈K(−h, 0; Rn).
(3.42)
(ii) The four assumptions (H1) to (H4) are veriﬁed for all real numbers p,
1 ≤p < ∞. Moreover there exists a constant c > 0 such that for all (t, s),
0 ≤s ≤t, and all z in Cc(−h, t; Rn)
 t
s
|Lzr|p dr
	1/p
≤c∥η∥
sup
θ∈I(−h,0)
 t+θ
s+θ
|z(r)|p dr
1/p
,
(3.43)
where ∥η∥is the total variation of the matrix η.
(iii) Given T > 0, introduce the continuous function Lz,
(Lz)(r) = Lzr,
r ≥0,
(3.44)
for each z in Cc(−h, T ; Rn) and the map
z 	→Lz : Cc(−h, T ; Rn) →Lp(0, T ; Rn).
(3.45)
Then for any T
> 0 the map L has a continuous linear extension to
Lp(−h, T ; Rn)
L: Lp(−h, T ; Rn) →Lp(0, T ; Rn)
(3.46)
and for all pairs (s, T), 0 ≤s ≤T ,
∥Lz∥Lp(s,T ) ≤∥η∥
sup
θ∈I(−h,0)
{∥x∥Lp(s+θ,T +θ).
(3.47)
(iv) For all x in W 1,p(−h, T ; Rn) and t ≥0
 t
0
(L ˙x)(s) ds = Lxt −Lx0,
(3.48)
where ˙x is the ﬁrst derivative of x.

250
II-4 State Space Theory of Diﬀerential Systems With Delays
Proof. (i) By the Riesz’s representation theorem (cf. W. Rudin [1, p. 131,
Theorem 6.19]).
(ii) It is easy to check that assumptions (H1) to (H3) are veriﬁed. As for
assumption (H4) we use Fubini’s theorem applied to complex measures (cf.
W. Rudin [1, Chapter 6 and p. 140, Theorem 7.8]). For all z1 and z2 in
Cc(−h, t; Rn)
 t
0
|f(s, z2
s) −f(s, z1
s)| ds =
 t
0
|L(z2
s −z1
s)| ds
≤
 t
0
ds
 0
−h
dθ |η| |z2(s + θ) −z1(s + θ)|.
As the integrand is continuous with compact support in I(h, 0) × [0, t], it is
m × η measurable (m, the Lebesgue measure) and after changing the order of
integration
 0
−h
dθ |η|
 t
0
ds|z2(s + θ) −z1(s + θ)| ≤m(t)∥z2 −z1∥Lp(−h,t;Rn),
where for t ≥0
m(t) = ∥η∥

t1/q,
1 < p < ∞,
1,
p = 1,
and ∥η∥is the total variation of η on I(−h, 0).
We have established (3.43) for p = 1 because it is a special case of (H4).
For 1 < p < ∞consider for any f in C(0, t; Rn) the following expression for
0 ≤s ≤t:

 t
s
f(r) · Lzr dr
 ≤
 t
s
dr
 0
−h
dθ |η| |f(r)| |z(r + θ)|.
Again the integrand is a continuous function on I(−h, 0)×[0, t] with compact
support. Hence it is measurable and Fubini’s theorem can again be used to
change the order of integration in the last integral
 0
−h
dθ |η|
 t
s
dr|f(r)| |z(r + θ)| ≤
 0
−h
dθ |η| ∥f∥Lq(s,t)∥z∥Lp(s+θ,t+θ)
and

 t
s
f(r) · Lzr dr
 ≤∥f∥Lq(s,t)
 0
−h
dθ |η| ∥z∥Lp(s+θ,t+θ).
By density of C(0, t; Rn) in Lq(0, t; Rn), we obtain (3.43) for all z
in Cc(−h, t; Rn).
(iii) The continuous linear extension of (3.45) again follows by density of
Cc(−h, t; Rn) in Lp(−h, t; Rn) and inequality (3.43).
(iv) The proof of this part requires the following lemma.

3 Existence theorems for Lipschitzian systems
251
Lemma 3.5. Let ℓ≥1 be an integer and p, 1 ≤p < ∞, T > 0 and h,
0 < h ≤+∞(possibly +∞), be real numbers. Then
W 1,p(−h, T ; Rn) ⊂K(−h, T ; Rn)
(3.49)
with continuous injection.
For x in W 1,p(−h, T ; Rn), the diﬀerence
Lxt −Lx0 = L(xt −x0)
is well deﬁned and continuous. For x in W 2,p(−h, T ; Rn),
(xt −x0)(θ) = x(t + θ) −x(θ) =
 t+θ
θ
˙x(s) ds
and
 0
−h
dθ η
 t+θ
θ
˙x(s) ds =
 0
−h
dθ η
 t
0
˙x(s + θ) ds =
 t
0
ds
 0
−h
dθ η ˙xs(θ).
Hence for all t in [0, T ]
Lxt −Lx0 =
 t
0
(L ˙x)(s) ds,
∀x ∈W 2,p(−h, T ; Rn).
Nowbydensityandcontinuitythislastresultistrueforall x in W 1,p(−h, T ; Rn).
⊓⊔
Proof of Lemma 3.5. Any element x of W 1,p(−h, T ; Rn) is almost everywhere
equal
to
a
bounded
continuous
function
¯x
on
I(−h, T )
(cf.
R. A. Adams [1, p. 97, Theorem 5.4]) and the injection in the space of
bounded continuous functions is continuous. This proves the lemma for h
ﬁnite, and we only have to deal with the case h = +∞.
To complete the proof it is now suﬃcient to show that ¯x(t) goes to zero
as t goes to −∞. For all pairs (s, s′), −∞< s ≤s′ ≤t
¯x(t) = ¯x(s′) +
 t
s′ ˙x(r) dr.
Take the Lp norm with respect to s′ from s to t
(t −s)1/p|¯x(t)| ≤∥¯x∥Lp(s,t;Rℓ) + (t −s)∥˙x∥Lp(s,t;Rℓ).
But ¯x = x in Lp(s, t; Rℓ) and for all t ≤T and s = t −1
|¯x(t)| ≤21−1/p∥x∥W 1,p(−∞,t;Rℓ).
For each ε > 0, there exists T ′ < T such that
∀t ≤T ′,
∥x∥W 1,p(−∞,t;Rℓ) ≤ε
and by combining the last two inequalities
∀ε > 0,
∃T ′ ≤T
such that ∀t ≤T, |¯x(t)| ≤ε.
Therefore ¯x belongs to C0(−∞, T; Rℓ).
⊓⊔

252
II-4 State Space Theory of Diﬀerential Systems With Delays
4 State space theory of linear time-invariant systems
In this section we ﬁx p, 1 ≤p < ∞, and specialize to linear systems

˙x(t) = Lxt + f(t),
t ≥0,
(x(0), x0) = (φ0, φ1) ∈M p ≡Rn × Lp(−h, 0; Rn),
(4.1)
where f belongs to Lp
loc(0, ∞; Rn) and L is a continuous map
L: K(−h, 0; Rn) →Rn.
(4.2)
4.1 Preliminary results and smoothness of the solution
We have seen in §3.2 that the right-hand side of the ﬁrst equation (4.1) makes
sense for x in Lp
loc(−h, ∞; Rn) even if L is only deﬁned on the space of contin-
uous functions. This situation is not really surprising because we are dealing
with initial conditions in an inﬁnite dimensional space and the real state of
the system at time t is an appropriately deﬁned piece of function on the time
interval I(t −h, t). In fact two states are considered that are fundamental in
the theory of delay systems. A traditional diﬃculty in the various theorems
and proofs we shall encounter is the notation. A good notation will simplify
the computations and make all the arguments precise in both the ﬁnite and
the inﬁnite memory case. For instance we have introduced the notation Lx in
Theorem 3.3, which is more precise and compact than the traditional nota-
tion Lxt. In addition we shall need the following notation to restrict or extend
functions by 0.
Deﬁnition 4.1. Let ℓ≥1 be an integer and a and b, a < b, be two real
numbers. Let F(a, b; Rℓ) be a set of functions from [a, b] to Rℓ. For each u in
F(a, b; Rℓ) and all s, a ≤s ≤b, deﬁne the functions es
−u and es
+u as follows:
es
−: I(a, ∞) →Rℓ, (es
−u)(t) =

u(t),
a ≤t ≤s,
0,
s < t < ∞,
(4.3)
es
+ : I(−∞, b) →Rℓ, (es
+u)(t) =

0,
−∞< t < s,
u(t),
s ≤t ≤b.
(4.4)
⊓⊔
In other words the subscript “+” indicates that we keep the function u on the
right (positive direction) of s up to b and set it equal to zero on the left of s
down to −∞. Similarly the subscript “ −” indicates that we keep the function
u on the left (negative direction) of s from a and set it equal to zero on the
right of s up to +∞. The construction is illustrated in Figure 4.1.

4 State space theory of linear time-invariant systems
253
b
s
a
(a) u on [a, b]
b
s
a
−∞
(b) es
+u on I(−∞, b)
b
s
a
+∞
(c) es
−u on I(a, ∞)
Fig. 4.1. The functions u, es
+u, and es
−u.
Remark 4.1. In the sequel we shall often make use of the composition Le0
+ of
the maps
e0
+ : Lp(0, T ; Rn) →Lp(−∞, T; Rn)
and
L: Lp(−∞, T; Rn) →Lp(0, T ; Rn).
⊓⊔
Theorem 4.1. Let p, 1 ≤p < ∞, and h, 0 < h ≤+∞, be real numbers and
f ∈Lp
loc(0, ∞; Rn)
and
φ = (φ0, φ1) ∈M p
(4.5)
be given:
(i) The system
˙x = Lx + f,
(x(0), x0) = φ
(4.6)
has a unique solution x in W 1,p
loc (0, ∞; Rn). Moreover, for all T > 0, there
exists a constant c(T ) > 0 such that
∥x∥W 1,p(0,T ;Rn) ≤c(T )[∥φ∥Mp + ∥f∥Lp(0,T ;Rn)].
(4.7)
(ii) When
φ1 ∈W 1,p(−h, 0; Rn)
and
φ0 = φ1(0),
(4.8)
the solution x of (4.6) belongs to W 1,p(−h, T ; Rn) for all T > 0 and
there exists c(T ) > 0 such that
∥x∥W 1,p(−h,T ;Rn) ≤c(T )[∥φ1∥W 1,p + ∥f∥Lp(0,T ;Rn)].
(4.9)

254
II-4 State Space Theory of Diﬀerential Systems With Delays
Remark 4.2. In fact part (i) can be slightly improved. It is possible to show
that for any continuous linear map
L: W 1,p(−h, 0; Rn) →Rn,
(4.10)
system (4.6) has a unique solution x in Cloc(0, ∞; Rn) when (4.6) is interpreted
in an appropriate weak sense (cf. M. C. Delfour [8]).
⊓⊔
4.2 First state equation
Starting with the homogeneous system
˙x = Lx,
(x(0), x0) = φ ∈M p,
(4.11)
we can construct the following semigroup of continuous linear transformations
on
φ 	→S(t)φ = (x(t), xt): M p →M p,
t ≥0.
(4.12)
Theorem 4.2. Let p, 1 ≤p < ∞, and h, 0 < h ≤+∞, be real numbers and
L: K(−h, 0; Rn) →Rn be linear and continuous:
(i) The family {S(t): t ≥0} of transformations of M p deﬁned by (4.12)
forms a strongly continuous semigroup on M p.
(ii) Its inﬁnitesimal generator is characterized by
A(φ0, φ1) = (Lφ1, Dφ1)
(4.13)
for all φ = (φ0, φ1) in the domain D(A) of A
D(A) =

(φ0, φ1) ∈M p :
φ1 ∈W 1,p(−h, 0; Rn)
and
φ0 = φ1(0)

,
(4.14)
where Dφ1 denotes the ﬁrst derivative of φ1.
Notation 4.1. In view of the structure of the elements of D(A) we shall use
the notation ψ for both the function ψ in W 1,p(−h, 0; Rn) and the element
(ψ(0), ψ) of D(A).
⊓⊔
Notice that from the deﬁnition of D(A) and A, the largest family of maps
L that generates a strongly continuous semigroup {S(t)} on M p is precisely
the one of Remark 4.2. This result clearly indicated that the state space
theory of other types of delay systems such as the neutral type could not be
obtained from systems of the form (4.11) (cf. M. C. Delfour [8] for more
details). To complete the picture we immediately give the second theorem for
the nonhomogeneous case, which will require an important technical lemma.
All proofs will be given at the end of the section.
Theorem 4.3. Assume that the assumptions of Theorem 4.2 are veriﬁed and
that x is the solution in W 1,p
loc (0, ∞; Rn) to (4.6) for f ∈Lp
loc(0, ∞; Rn):

4 State space theory of linear time-invariant systems
255
(i) For all t ≥0 the state
˜x(t)
def
= (x(t), xt)
is well deﬁned and
˜x(t) = S(t)φ +
 t
0
S(t −s)(f(s), 0) ds.
(4.15)
If φ ∈D(A), then for all t ≥0, ˜x(t) ∈D(A).
(ii) For 1 < p < ∞and q its conjugate, q−1 + p−1 = 1, ˜x is a solution of the
system
⎧
⎨
⎩
d
dt < ψ, ˜x(t) > +⟨A∗ψ, ˜x(t)⟩+ ψ0 · f(t),
t > 0,
˜x(0) = φ,
∀ψ ∈D(A∗),
(4.16)
where A∗is the inﬁnitesimal generator of the semigroup {S∗(t)} on M q.
(iii) Given p and q as in part (ii) for all T > 0, ˜x is the unique solution in
V(0, T ; M p, D(A∗)′)
=

z ∈C(0, T ; M p): d
dti∗z ∈Lp(0, T ; D(A∗)′)

(4.17)
to the following equation in D(A∗)′:
⎧
⎨
⎩
d
dti∗˜x(t) = (A∗)∗˜x(t) + i∗(f(t), 0),
t > 0,
˜x(0) = φ,
(4.18)
where i∗and (A∗)∗are the topological dual maps of the continuous linear
maps
i: D(A∗) →M q,
A∗: D(A∗) →M q,
where i is the canonical dense injection of D(A∗) into M q and D(A∗) is
endowed with the graph norm topology deﬁned by
∥ψ∥D(A∗) = [∥ψ∥q
Mq + ∥A∗ψ∥q
Mq]1/q.
(4.19)
Remark 4.3. This theorem will be generalized in §5.2 to systems with delays
in the control variable.
⊓⊔
Equation (4.17) is the ﬁrst example of a state equation for delay systems.
It is to be interpreted in the weak sense (4.16). This result is an abstract one
because we have not yet characterized D(A∗), which is quite diﬀerent from
D(A). In most situations (4.15), (4.16), or (4.17) will be suﬃcient. However
we shall give other deﬁnitions of states and state equations that are diﬀerent
from ˜x(t) in (4.15).

256
II-4 State Space Theory of Diﬀerential Systems With Delays
Terminology 1. The pair (x(t), xt) associated with the solution x of system
(4.6) will be called the state of system (4.6) and denoted ˜x(t). The correspond-
ing semigroup {S(t)} deﬁned by (4.12) will be referred to as the semigroup
associated with the state.
This is the natural extension of the traditional terminology for the state
xt in the space of continuous functions. Fundamentally {S(t)} is a translation
semigroup acting on the concatenation of the initial function and the solution
x of system (4.6). The terminology initial function state and initial function
semigroup is also used in the recent literature as in the book of G. Grippen-
berg, S. O. Londen, and O. Staffans [1].
We now proceed to the proof of the two theorems, which will necessitate
the following important technical lemma.
Lemma 4.1. Let p, 1 ≤p < ∞, q, q−1 + p−1 = 1, and T > 0, be real and
ℓ≥1 be an integer:
(i) The function u belongs to Lp(−h, T ; Rℓ) if and only if the function
t 	→(u•)(t) = ut : [0, T ] →Lp(−h, 0; Rℓ)
(4.20)
is continuous. Moreover for all u in Lp(−h, T ; Rℓ), all t, 0 ≤t ≤T , and
all ψ in W 1,q
0
(−h, 0; Rℓ),
Dt⟨ψ, ut⟩Lq×Lp = −⟨Dθ ψ, ut⟩Lq×Lp.
(4.21)
(ii) The following three properties are equivalent:
u ∈W 1,p(−h, T ; Rℓ),
(4.22)
u• ∈C1
0, T ; Lp(−h, 0; Rℓ)

,
(4.23)
u• ∈C

0, T ; W 1,p(−h, 0; Rℓ)

.
(4.24)
Moreover in each case
Dtut = Dθ ut,
0 ≤t ≤T,
(4.25)
and the equality holds in C(0, T ; Lp(−h, 0); Rℓ).
Proof. (i) For any u in Cc(−h, T ; Rℓ) the function (4.20) is well deﬁned and
continuous and the map
u 	→u• : Cc(−h, T ; Rℓ) →C

0, T ; Lp(−h, 0; Rℓ)

(4.26)
is linear and continuous for the Lp-topology:
 0
−h
|ut(θ)|p dθ =
 0
−h
|u(t + θ)|p dθ ≤∥u∥p
Lp(−h,T ;Rℓ)
=⇒∥u•∥C

0,T ;Lp(−h,0;Rℓ)
 ≤∥u∥Lp(−h,T ;Rℓ).

4 State space theory of linear time-invariant systems
257
As a result the map (4.26) has a unique continuous linear extension to all
Lp(−h, T ; Rℓ). Conversely if u• ∈C

0, T ; Lp(−h, 0; Rℓ)

,
∀t ∈[0, T ],
ut ∈Lp(−h, 0; Rℓ) =⇒u ∈Lp(−h, T ; Rℓ).
To compute the vectorial distribution derivative of u• consider the following
expression for ϕ in D(]0, T [) and ψ in W 1,q
0
(−h, 0; Rℓ):
E = −
 T
0
⟨ψ, ut⟩Lq×Lp dϕ
dt (t) dt
= −
 T
0
dt
 0
−h
dθψ(θ) · u(t + θ)dϕ
dt (t).
Notice that e−h
+ ψ is the extension by 0 of the function ψ on I(−h, 0) to ]−∞, 0].
As ψ(−h) = 0, e−h
+ ψ ∈W 1,q(−∞, 0; Rℓ). Change the variable θ to s = t + θ
and extend the bounds t −h and t to −h and T :
E = −
 T
0
dt
 t
t−h
ds(e−h
+ ψ)(s −t) · u(s)dϕ
dt (t)
= −
 T
0
dt
 T
−h
ds(e−h
+ ψ)(s −t) · u(s)dϕ
dt (t).
Now change the order of integration and integrate by parts with respect to t:
E =
 T
−h
ds
 T
0
dt d
dt(e−h
+ ψ)(s −t) · u(s)ϕ(t).
Then change the order of integration once again and change the variable s
back to θ = s −t
E =
 T
0
dt
 T −t
−h−t
dθ

−d
dθ (e−h
+ ψ)(θ)
	
· u(t + θ)ϕ(t)
= −
 T
0
dt
 0
−h
dθdψ
dθ (θ) · ut(θ)ϕ(t).
This establishes (4.21).
(ii) If u ∈W 1,p(−h, T ; Rℓ), then the derivative ˙u ∈Lp(−h, T ; Rℓ) and from
(i) the function t 	→( ˙u)t belongs to C

0, T ; Lp(−h, 0; Rℓ)

. So it is suﬃcient
to establish that Dut =( ˙u)t to conclude that u• ∈C1
0, T ; Lp(−h, 0; Rℓ)

. We
use identity (4.21)
−⟨Dθ ψ, ut⟩= −
 0
−h
dψ
dθ (θ) · u(t + θ) dθ =
 0
−h
ψ(θ) · d
dθu(t + θ) dθ
=
 0
−h
ψ(θ) · ˙u(t + θ) dθ = ⟨ψ, ( ˙u)t⟩

258
II-4 State Space Theory of Diﬀerential Systems With Delays
and necessarily Dut = ( ˙u)t. So (4.22) implies (4.23). To show that (4.23) im-
plies (4.24) again use identity (4.21): For all t in [0, T ] and ψ in W 1,q
0
(−h, 0; Rℓ)
⟨ψ, Dut⟩= Dt⟨ψ, ut⟩= −⟨Dθ ψ, ut⟩.
Therefore
Dut = Dθ ut,
0 ≤t ≤T
(4.27)
and t 	→Dθ ut belongs to C

0, T ; Lp(−h, 0; Rℓ)

. But we already know from
part (i) that ut ∈C

0, T ; Lp(−h, 0; Rℓ)

and necessarily
u• ∈C

0, T ; W 1,p(−h, 0; Rℓ)

.
Finally we show that (4.24) implies (4.22). For h = +∞,
uT ∈W 1,p(−∞, 0; Rℓ) =⇒u ∈W 1,p(−∞, T; Rℓ).
For h ﬁnite
∀t ∈[0, T ],
ut ∈W 1,p(−h, 0; Rℓ) =⇒u|[t−h,t] ∈W 1,p(t −h, t; Rℓ),
and because the interval [−h, T ] is ﬁnite, u ∈W 1,p(−h, T ; Rℓ). In the process
we have established (4.26). This completes the proof of the lemma.
⊓⊔
Proof of Theorem 4.2.
(i) This is a direct consequence of Theorem 4.1.
(ii) By deﬁnition of D(A), for each φ in D(A) and T > 0, the function
t 	→S(t)φ = (x(t), xt)
belongs to C1(0, T ; M p). In particular x ∈C(0, T ; Rn) and
x• ∈C1
0, T ; Lp(−h, 0; Rn)

.
Now by Lemma 4.1 (ii) x ∈W 1,p(−h, T ; Rn) and
φ0 = x(0) = φ1(0)
and
φ1 ∈W 1,p(−h, 0; Rn).
Conversely if φ0 = φ1(0) and φ1 ∈W 1,p(−h, 0; Rn), then by Theorem 4.1
(ii) for all T > 0, the solution x to (4.11) belongs to W 1,p(−h, T ; Rn). By
Lemma 4.1 (ii),
x• ∈C1
0, T ; Lp(−h, 0; Rn)

and
Dtxt = Dθ xt
and by continuity of Dtxt
lim
t↘0 Dtxt = lim
t↘0 Dθ xt = Dθ φ1
in Lp(−h, 0; Rn).
As for the Rn-component we use identity (3.48) in Theorem 3.3 (iv): For all
x in W 1,p(−h, T ; Rn), L ˙x belongs to Lp(0, T ; Rn) and

4 State space theory of linear time-invariant systems
259
Lxt = Lx0 +
 t
0
(L ˙x)(s) ds.
Therefore
lim
t↘0 ˙x(t) = lim
t↘0 Lxt = Lx0 = Lφ1.
We have shown that
lim
t↘0 Dt˜x(t) = lim
t↘0 Dt(x(t), xt) = (Lφ1, Dθ φ1)
in M p.
So φ ∈D(A), which is characterized by (4.14). Moreover we have also estab-
lished identity (4.13) and this completes the proof.
⊓⊔
Proof of Theorem 4.3.
(i) By linearity it is suﬃcient to establish (4.15) for
φ = 0. In that case φ = 0 veriﬁes assumption (4.8) in Theorem 4.1 (ii) and
the solution x of (4.6) is such that for all T > 0
x ∈W 1,p(0, T ; Rn)
and
x(0) = 0.
Hence e0
+x ∈W 1,p(−h, T ; Rn) and by Theorem 4.2 (ii)
˜x(t) = (x(t), (e0
+x)t) ∈D(A),
∀t ≥0.
In particular
dx
dt (t) = L(e0
+x)t + f(t),
and from Lemma 4.1 (ii), e0
+x ∈C

0, T ; W 1,p(−h, 0; Rn)

and
d
dt(e0
+x)t = Dθ(e0
+x)t,
t > 0.
Hence
d
dt ˜x(t) = A˜x(t) + (f(t), 0),
a.e. in [0, ∞[,
˜x(0) = 0.
Now from semigroup theory (cf. Chapter 1, Proposition 3.1), we conclude that
(4.15) is veriﬁed.
(ii) From part (i) we know that ˜x ∈Cloc(0, ∞; M p) and that M p is re-
ﬂexive. Therefore {S(t)} has an adjoint semigroup {S∗(t)} on (M p)′ with a
densely deﬁned inﬁnitesimal generator A∗with domain D(A∗). So for all ψ in
D(A∗), we rewrite (4.15) in the following weak form:
⟨ψ, ˜x(t)⟩= ⟨S∗(t)ψ, φ⟩+
 t
0
⟨S∗(t −s)ψ, (f(s), 0)⟩ds
and
d
dt⟨ψ, ˜x(t)⟩=⟨S(t)∗A∗ψ, φ⟩+
 t
0
⟨S∗(t −s)A∗ψ, (f(s), 0)⟩ds+⟨ψ, (f(t), 0)⟩
= ⟨A∗ψ, ˜x(t)⟩+ ⟨ψ, (f(t), 0)⟩.

260
II-4 State Space Theory of Diﬀerential Systems With Delays
This gives the weak expression (4.16).
(iii) Rewrite (4.16) with the canonical injection and take the duals of i
and A∗to get (4.18), and this gives the characterization of ˜x as an element of
V(0, T ; M p; D(A∗)′) in (4.17). The uniqueness is obvious.
⊓⊔
4.3 Transposed and adjoint systems
In the previous sections we considered the system
˙x(t) = Lxt,
t > 0,
(x(0), x0) = φ ∈M p
(4.28)
associated with the continuous linear map L: K(−h, 0; Rn) →Rn and con-
structed the semigroup {S(t)}. In Theorem 3.3 in §3.3, we have associated
with L a representation in terms of an n×n matrix of regular Borel measures
Lφ =
 0
−h
dθ ηφ(θ).
(4.29)
If we denote by η⊤the transposed of the matrix η, then we can introduce a
new continuous linear map
L⊤: K(−h, 0; Rn) →Rn,
L⊤ψ =
 0
−h
dθ η⊤ψ(θ),
(4.30)
and for each T > 0 and q, 1 ≤q < ∞, the continuous linear map

L⊤: Lq(−h, T ; Rn) →Lq(0, T ; Rn),
(L⊤z)(t) = L⊤zt,
∀t ≥0, ∀z ∈Cc(−h, T ; Rn).
(4.31)
For L⊤and L⊤we have the analog of Theorem 3.3 in §3.3.
Deﬁnition 4.2. Let q, 1 ≤q < ∞, be a real number:
(i) For ψ in M q and g in Lq
loc(0, ∞; Rn) the transposed system is deﬁned as
˙z = L⊤z + g,
(z(0), z0) = (ψ0, ψ1) ∈M q.
(4.32)
(ii) The transposed semigroup {S⊤(t)} is deﬁned as
S⊤(t)ψ = (z(t), zt),
t ≥0, ψ ∈M q,
(4.33)
where z is the solution of (4.32) with g = 0.
⊓⊔
The system (4.32) and the semigroup (4.33) associated with L⊤have the
same properties as system (4.6) and the semigroup (4.12) associated with L.
So the results in the previous section apply with L⊤, S⊤, and q in place of
L, S, and p. In particular the inﬁnitesimal generator A⊤of the semigroup
{S⊤(t)} is given by

4 State space theory of linear time-invariant systems
261
D(A⊤) =

(ψ0, ψ1) ∈M q :
ψ1 ∈W 1,q(−h, 0; Rn)
ψ0 = ψ1(0)

,
A⊤(ψ0, ψ1) = (L⊤ψ1, Dψ1).
(4.34)
Notation 4.2. It will be convenient to use the notation ψ for both the element
ψ of W 1,q(−h, 0; Rn) and the element ψ = (ψ(0), ψ) of D(A⊤). The canonical
injection of D(A⊤) into M q will be denoted by j
ψ = (ψ(0), ψ) 	→jψ = (ψ(0), ψ): D(A⊤) →M q.
⊓⊔
The transposed semigroup {S⊤(t)} is however not equal to the topological
adjoint semigroup {S∗(t)} of {S(t)}. This is a very fundamental aspect of
the theory of delay systems with deep implications for Control Theory. Work-
ing with (4.6) for x will yield an “adjoint system” that is equivalent to the
transposed system (4.32) with a change of variable from t to T −t. However
working with the state equation (4.18) will yield an “adjoint system” in M q
characterized by the adjoint semigroup {S∗(t)}. Of course there is a connec-
tion between the two approaches that is a consequence of the “intertwining
theorem” between S⊤(t) and S∗(t) by the structural operator F, which will
be introduced in the subsequent sections.
Terminology 2. The pair (z(t), zt) associated with the solution z of system
(4.32) will be called the transposed state of system (4.32) and denoted ˜z(t).
The corresponding semigroup {S(t)⊤} deﬁned by (4.33) will be referred to as
the transposed semigroup.
The ﬁrst technical result is an “integration by parts” formula that relates
systems (4.6) and (4.32).
Lemma 4.2. Let T > 0, p, 1 < p < ∞, and q, q−1+p−1 = 1, be real numbers.
Then for all x in W 1,p(0, T ; Rn) and z in W 1,q(0, T ; Rn)
⎧
⎪
⎪
⎨
⎪
⎪
⎩
 T
0
z(T −t) · [ ˙x −Le0
+x](t) dt + z(T ) · x(0),
=
 T
0
[ ˙z −L⊤e0
+z](T −t) · x(t) dt + z(0) · x(T ).
(4.35)
Proof. It is suﬃcient to look at the term
 T
0
z(T −t) · (Le0
+x)(t) dt.
For x and z in Cc(0, T ; Rn), x(0) = 0 and z(0) = 0, e0
+x and e0
+z belong to
Cc(−h, T ; Rn), and

262
II-4 State Space Theory of Diﬀerential Systems With Delays
 T
0
z(T −t) · (Le0
+x)(t) dt =
 T
0
z(T −t) · L(e0
+x)t dt
=
 T
0
z(T −t) ·
 0
−h
dθ η(e0
+x)(t + θ) dt
=
 T
0
 0
−h
dθ η⊤z(T −t) · (e0
+x)(t + θ) dt.
After changing the order of integration
=
 0
−h
 T
0
dθ η⊤z(T −t) · (e0
+x)(t + θ) dt
=
 0
−h
 T +θ
θ
dθ η⊤z(T −s + θ) · (e0
+x)(s) ds
=
 0
−h
 T
0
dθ η⊤(e0
+z)(T −s + θ) · x(s) ds.
Again change the order of integration
=
 T
0
L⊤(e0
+z)T −s · x(s) ds
=
 T
0
(L⊤e0
+z)(T −s) · x(s) ds.
By density of Cc(0, T ; Rn) in Lp(0, T ; Rn) and Lq(0, T ; Rn) for all z in
Lq(0, T ; Rn) and x in Lp(0, T ; Rn)
 T
0
z(T −t) · (Le0
+x)(t) dt =
 T
0
(L⊤e0
+z)(T −s) · x(s) ds.
In particular it is true for z in W 1,q(0, T ; Rn) and x in W 1,p(0, T ; Rn).
⊓⊔
To complete this section we relate the solution z of the transposed system
to the classical backward adjoint system. For simplicity we assume that
ψ1 ∈W 1,q(−h, 0; Rn)
and
ψ0 = ψ1(0).
Under that condition system (4.32) can be rewritten as
˙z(t) = L⊤zt + g(t)
on [0, T ].
(4.36)
Now we introduce the variable
p(t) = z(T −t),
0 ≤t ≤T + h.
(4.37)
Then
d
dtp(t) = −˙z(T −t)

4 State space theory of linear time-invariant systems
263
and
L⊤zT −t =
 0
−h
dθ η⊤z(T −t + θ) =
 0
−h
dθ η⊤p(t −θ) = L⊤pt,
where
pt(θ) = p(t −θ).
(4.38)
Finally we obtain the backward adjoint system
⎧
⎨
⎩
−dp
dt (t) = L⊤pt + g(T −t),
0 < t < T,
(p(T ), pT ) = (ψ0, ψ1).
(4.39)
In general for time-varying systems (4.39) will be the system to work with. The
transposed system can only be obtained in the time-invariant case. Keeping
this in mind and the fact that the remainder of this chapter is devoted to
time-invariant linear systems, we have decided to avoid the backward adjoint
system and work with the transposed system.
4.4 Structural operators
Structural operators are as fundamental to delay systems as Sobolev spaces
to elliptic problems. They capture the fundamental structure of a delay sys-
tem and play a key role in the characterization of the properties of stability,
stabilizability, controllability, and observability. They were ﬁrst announced in
December 1976 for a ﬁnite number of delays by A. Manitius [5] at the CDC
and by M. C. Delfour and A. Manitius [1] at the INRIA. A complete
treatment was later given by M. C. Delfour and A. Manitius [2, 3] for
arbitrary linear delay functionals L continuous on the space of continuous
functions.
There are many ways to introduce structural operators. Intuitively they
describe the way the system combines and transforms initial conditions over
the initial time interval I(0, h). Go back to the linear homogeneous system
˙x = Lx,
(x(0), x0) = (ψ0, ψ1) ∈M p.
(4.40)
Then separate the solution x in W 1,p(0, T ; Rn) from its initial function φ1 in
Lp(−h, 0; Rn)
˙x = Le0
+x + Le0
−φ1,
x(0) = φ0.
(4.41)
System (4.40) does not directly use φ1 but only its image Le0
−φ1. When h is
ﬁnite
(Le0
−φ1)(t) = 0,
t > h,
(4.42)
and it is suﬃcient to consider Le0
−φ1 on the interval [0, h].
This suggests to associate with the function φ1 on I(−h, 0) another func-
tion ¯Lφ1 on I(−h, 0) deﬁned in the following way:

264
II-4 State Space Theory of Diﬀerential Systems With Delays
(¯Lφ1)(α) = (Le0
−φ1)(−α),
α ∈I(−h, 0).
(4.43)
In view of Theorem 3.3 (iii) in §3.3
¯L: Lp(−h, 0; Rn) →Lp(−h, 0; Rn)
(4.44)
is a continuous linear transformation and we can rewrite (4.41) to emphasize
the role of the pair (φ0, ¯Lφ1) ∈M p

˙x(t) = (Le0
+x)(t) + (e−h
+ ¯Lφ1)(−t),
t > 0,
x(0) = φ0.
(4.45)
It is clear that the “real initial data” is the pair
F(φ0, φ1) = (φ0, ¯Lφ1) ∈M p,
(4.46)
which deﬁnes a continuous linear transformation of M p.
Deﬁnition 4.3. Let p, 1 ≤p < ∞, and h, 0 < h ≤+∞, be real numbers:
(i) The operator F deﬁned in (4.46) will be referred to as the structural
operator associated with L.
(ii) The structural operator associated with L⊤is deﬁned as
F ⊤(ψ0, ψ1) = (ψ0, ¯L⊤ψ1),
(4.47)
where
(¯L⊤ψ1)(α) = (L⊤e0
−ψ1)(−α),
α ∈I(−h, 0).
(4.48)
⊓⊔
The following lemma gives further insight into the structure of F and its
relationship to F ⊤.
Lemma 4.3.
(i) For all φ in K(−h, 0; Rn) such that φ(0) = 0,
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
(¯Lφ)(α) =
 α
−h
dθ ηφ(θ −α),
∀α ∈I(−h, 0),
¯Lφ ∈K(−h, 0; Rn),
(¯Lφ)(−h) = 0,
(¯Lφ)(0) = Lφ,
(¯LT φ)(α) =
 α
−h
dθ η⊤φ(θ −α),
∀α ∈I(−h, 0),
¯L⊤φ ∈K(−h, 0; Rn),
(¯L⊤φ)(−h) = 0,
(¯L⊤φ)(0) = L⊤φ.
(4.49)
(ii) For any p, 1 < p < ∞, and its conjugate q, q−1 + p−1 = 1,
¯L∗= ¯L⊤
and
F ∗= F ⊤,
(4.50)
where ¯L∗and F ∗are the dual operators of ¯L and F.

4 State space theory of linear time-invariant systems
265
Proof. (i) It is suﬃcient to establish (4.49) for L. The proof for L⊤is exactly
the same. Given any φ in K(−h, 0; Rn), e0
−φ ∈K(−∞, 0; Rn) is uniformly
continuous and the function
α 	→(e0
−φ)−α : I(−h, 0) →K(−h, 0; Rn)
is also continuous. As L is continuous on K(−h, 0; Rn), the function
α 	→(¯Lφ)(α) = (Le0
−φ)(−α) = L(e0
−φ)−α : I(−h, 0) →Rn
is also continuous and (¯Lφ)(0) = L(e0
−φ)0 = Lφ. For h ﬁnite, (¯Lφ)(α) = 0
for α < −h and by continuity (¯Lφ)(−h) = 0. For h inﬁnite, (¯Lφ)(α) →0
as α →−h because e0
−φ ∈K(−∞, 0; Rn) and (e0
−φ)(α) →0 as α →−∞.
Finally the last two identities (4.49) follow by deﬁnition of ¯L
(¯Lφ)(α) = L(e0
−φ)−α =
 0
−h
dθ η(e0
−φ)(θ −α) =
 α
−h
dθ ηφ(θ −α)
because
(e0
−φ)(θ −α) = 0
for θ −α ≥0.
(ii) We compute the dual of ¯L. For φ and ψ in Cc(−h, 0; Rn)
⟨ψ, ¯Lφ⟩=
 0
−h
ψ(α) · (¯Lφ)(α) dα
=
 0
−h
ψ(α) ·
 0
−h
dθ η(e0
−φ)(θ −α) dα.
Change the order of integration and the variable α to ζ = θ −α
⟨ψ, ¯Lφ⟩=
 0
−h
 θ+h
θ
dθ η⊤ψ(θ −ζ) · (e0
−φ)(ζ) dζ
=
 0
−h
 0
−h
dθ η⊤(e0
−ψ)(θ −ζ) · φ(ζ) dζ
=
 0
−h
dζ
 0
−h
dθ η⊤(e0
−ψ)(θ −ζ) · φ(ζ)
= ⟨¯L⊤ψ, φ⟩.
If the dual (Lp)∗of Lp is identiﬁed with Lq, then we obtain the ﬁrst identity
(4.50). The second one follows immediately from the ﬁrst.
⊓⊔
4.5 Adjoint semigroup {S⊤∗(t)} and intertwining theorems
The next theorem gives a complete characterization of the dual semigroup
{S⊤∗(t)} of S⊤(t)} in terms of the structural operator F. This is the ﬁrst
step toward the so-called intertwining theorem.

266
II-4 State Space Theory of Diﬀerential Systems With Delays
Theorem 4.4. Fix the real number p, 1 < p < ∞, and its conjugate q, q−1 +
p−1 = 1, and identify the elements of (M p)∗with those of M q:
(i) For all ξ = (ξ0, ξ1) in M p and all t ≥0,
S⊤∗(t)ξ = F(x(t), (e0
+x)t) + (0, τ(t)ξ1),
(4.51)
where x ∈W 1,p
loc (0, ∞; Rn) is the solution of the system
˙x(t) = (Le0
+x)(t) + (e−h
+ ξ1)(−t),
t > 0, x(0) = ξ0,
(4.52)
and τ(t) is the right translation operator
[τ(t)u](θ) = (e−h
+ u)(θ −t), θ ∈[−h, 0]
(4.53)
for any arbitrary function u deﬁned on I(−h, 0).
(ii) When ξ = Fφ for some φ ∈M p, then
S⊤∗(t)Fφ = F(x(t), xt),
t ≥0.
(4.54)
Remark 4.4. We shall see in §5 that it is possible to construct a state ˆx(t) and
a state equation similar to (4.16) and (4.18) in Theorem 4.3 for ˜x(t) in the
nonhomogeneous case. We choose to do it later in the general case with the
control term instead of doing the simple case here and repeating the argument
for the control case.
⊓⊔
Corollary 4.1. Fix the real number p, 1 < p < ∞, and its conjugate q, q−1 +
p−1 = 1, and identify the elements of (M p)∗with those of M q:
(i) For all ζ = (ζ0, ζ1) in M q and all t ≥0,
S∗(t)ζ = F ∗(z(t), (e0
+z)t) + (0, τ(t)ζ1),
(4.55)
where z ∈W 1,q
loc (0, ∞; Rn) is the solution of the system
˙z(t) = (L⊤e0
+z)(t) + (e−h
+ ζ1)(−t),
t > 0, z(0) = ζ0,
(4.56)
and τ(t) is the right translation operator deﬁned in (4.53).
(ii) When ζ = F ∗ψ for some ψ ∈M q, then
S∗(t)F ∗ψ = F ∗(z(t), zt),
t ≥0.
(4.57)
To see the complete picture we immediately give the intertwining theorem
of S⊤∗and S with respect to F and the dual result as a corollary. The proofs
will be given at the end of the section. The general version of the theorem
is due to M. C. Delfour and A. Manitius [2, 3], for arbitrary linear de-
lay functionals L continuous on the space of continuous functions (see also
C. Bernier and A. Manitius [1] for a ﬁnite number of delays).

4 State space theory of linear time-invariant systems
267
Theorem 4.5 (Intertwining theorem). Fix the real number p, 1 < p < ∞,
and its conjugate q, q−1 +p−1 = 1. Denote by A⊤∗the inﬁnitesimal generator
of the semigroup {S⊤∗(t)} on M p. Then the following properties hold and are
equivalent:
(i) S⊤∗(t)Fφ = FS(t)φ, ∀φ ∈M p, ∀t ≥0.
(ii) FD(A) ⊂D(A⊤∗) and A⊤∗Fφ = FAφ, ∀φ ∈D(A).
(iii) For all ζ in the resolvent set of A
R(ζ, A⊤∗)Fφ = FR(ζ, A)φ,
∀φ ∈M p,
(4.58)
where R(ζ, A) = [ζI −A]−1 and R(ζ, A⊤∗) = [ζI −A⊤∗]−1.
Corollary 4.2. Fix p and q as in Theorem 4.5. If {S∗(t)} is the dual semi-
group of {S(t)} with inﬁnitesimal generator A∗, the following properties hold
and are equivalent:
(i) S∗(t)F ∗φ = F ∗S⊤(t)φ, ∀φ ∈M q.
(ii) F ∗D(A⊤) ⊂D(A∗) and A∗F ∗φ = F ∗A⊤φ, ∀φ ∈D(A⊤).
(iii) For all ζ in the resolvent set of A⊤,
R(ζ, A∗)F ∗φ = F ∗R(ζ, A⊤)φ,
∀φ ∈M q.
(4.59)
Proof of Theorem 4.4.
(i) Let z be the solution of the transposed system
˙z = L⊤z,
(z(0), z0) = (ψ0, ψ1) ∈M q.
Let T > 0 be an arbitrary time. As x and z belong to W 1,p(0, T ; Rn) and
W 1,q(0, T ; Rn), respectively, we can use identity (4.35) in Lemma 4.2 and the
fact that
L⊤z = L⊤e0
+z + L⊤e0
−ψ1.
Then
(1)
def
=
 T
0
z(T −t) · (e−h
+ ξ1)(−t) dt + z(T ) · ξ0
=
 T
0
(L⊤e0
−ψ1)(T −t) · x(t) dt + ψ0 · x(T )
def
= (2).
Notice the two terms in ξ on the left-hand side (1) and the two terms in ψ on
the right-hand side of (2). Change the variable t to θ = −t in (1):
(1) =
 0
−T
z(T + θ) · (e−h
+ ξ1)(θ) dθ + z(T ) · ξ0
=
 0
−h
(e0
+z)T (θ) · ξ1(θ) dθ + z(T ) · ξ0
= ⟨zT , ξ1⟩−⟨(e0
−ψ1)T , ξ1⟩+ z(T ) · ξ0
= ⟨˜z(T ), ξ⟩Mq×Mp −⟨(e0
−ψ1)T , ξ1⟩Lq×Lp.

268
II-4 State Space Theory of Diﬀerential Systems With Delays
By a simple change of variable
 0
−h
(e0
−ψ1)T (α) · ξ1(α) dα =
 0
−h
ψ1(θ) · (e−h
+ ξ1)(θ −T ) dθ
and ﬁnally by deﬁnition of S⊤and τ
(1) = ⟨S⊤(T )ψ, ξ⟩−
-
ψ, (0, τ(T )ξ1)⟩.
For the right-hand side we change t to α = t −T in (2)
(2) =
 0
−T
(L⊤e0
−ψ1)(−α) · x(T + α) dα + ψ0 · x(T )
=
 0
−h
(¯L⊤ψ1)(α) · (e0
+x)T (α) dα + ψ0 · x(T )
= ⟨F ∗ψ, (x(T ), (e0
+x)T )⟩M q × M p.
By combining the above results for (1) and (2) we obtain (4.51).
(ii) We need the following lemma, which will be proved at the end.
⊓⊔
Lemma 4.4. Let p, 1 ≤p < ∞, be a real number and ψ a function in
Lp(−h, 0; Rn); then
∀t ≥0,
τ(t)(¯Lψ) = ¯L(e0
−ψ)t.
(4.60)
When ξ = Fφ, ξ1 = ¯Lφ1, from (4.60), τ(T )(¯Lφ1) = ¯L(e0
−φ1)T , and hence

0, τ(T )(¯Lφ1)

= F(0, (e0
−φ1)T )
and
S⊤∗(T )ξ = F(x(T ), (e0
+x)T + (e0
−φ1)T ) = F(x(T ), xT ).
Proof. Again we prove (4.60) for functions φ in Cc(−h, 0; Rn) and extend it to
Lp(−h, 0; Rn) by density and continuity. By assumptions e0
−φ ∈Cc(−∞, 0; Rn)
and for α in I(−h, 0)
[τ(t)(¯Lφ)](α) = [e−h
+ (¯Lφ)](α −t)
=

L(e0
−φ)t−α,
t −α ≤h,
0,
t −α > h.
For t −α ≤h
L(e0
−φ)t−α =
 0
−h
dθ η(e0
−φ)(t −α + θ) =
 α
−h
dθ η(e0
−φ)t(θ −α)
because

4 State space theory of linear time-invariant systems
269
α ≤θ ≤0 =⇒t < t −α + θ < t −α =⇒(e0
−φ)(t −α + θ) = 0.
Hence for t −α ≤h
[τ(t)(¯Lφ)](α) = [¯L(e0
−φ)t](α).
When t −α > h
−h ≤θ ≤α =⇒0 < t −α −h ≤t −α + θ =⇒(e0
−φ)(t −α + θ) = 0
and
[¯L(e0
−φ)t](α) = 0.
By regrouping the two cases we obtain (4.60).
⊓⊔
Proof of Theorem 4.5.
(i) is a direct consequence of identity (4.54) in Theo-
rem 4.4 (ii). The equivalence of (iii) and (i) is obtained by using the integral
formulas between the resolvent and the semigroup. Finally (ii) is equivalent to
(i) by the following lemma, which can be found in C. Bernier and A. Man-
itius [1].
⊓⊔
Lemma 4.5. Let Y be a Banach space and K a continuous linear transfor-
mation of Y . Given two strongly continuous semigroups {T1(t)} and {T2(t)}
of bounded linear transformations of Y and their respective inﬁnitesimal gen-
erators B1 and B2, the following statements are equivalent:
(i) T2(t)K = KT1(t), t ≥0.
(ii) KD(B1) ⊂D(B2) and B2K = KB1 on D(B1).
4.6 Inﬁnitesimal generators A⊤∗and A∗
Theorem 4.4 has provided a characterization of the adjoint semigroups {S⊤∗(t)}
and {S∗(t)}, which has a more complex structure than {S⊤(t)} and {S(t)}.
So the next step is to characterize their inﬁnitesimal generators A⊤∗and A∗.
Of course they will inherit of the complexity of the adjoint semigroups. This
question was initially studied by R. B. Vinter [2], but its importance was
not fully appreciated at that time. This characterization was not easy to ob-
tain because its ﬁnal form was not known. It is one area where the structural
operator has provided simpliﬁcation both in the characterization and in the
proof of the result. One source of diﬃculties was the search of an explicit deﬁ-
nition of the inﬁnitesimal generator of the adjoint semigroup. In the following
theorem, we give an implicit characterization or deﬁnition, which turns out
to be a more general and ﬂexible result that applies to systems with ﬁnite or
inﬁnite memory.
Theorem 4.6. Assume that the hypotheses of Theorem 4.4 are veriﬁed:

270
II-4 State Space Theory of Diﬀerential Systems With Delays
(i) The inﬁnitesimal generator A⊤∗of {S⊤∗(t)} is characterized as follows:
D(A⊤∗) =

ξ :
ξ = Fφ + (0, ζ), φ ∈D(A),
ζ ∈W 1,p(−h, 0; Rn), ζ(−h) = 0

(4.61)
and the map
ξ = Fφ + (0, ζ) 	→A⊤∗ξ = FAφ + (ζ(0), −Dζ): D(A⊤∗) →M p (4.62)
is independent of the choice of the representation of ξ in terms of (φ, ζ).
(ii) The inﬁnitesimal generator A∗of {S∗(t)} is characterized as follows:
D(A∗) =

ξ : ξ = F ∗φ + (0, ζ), φ ∈D(A⊤),
ζ ∈W 1,p(−h, 0; Rn), ζ(−h) = 0

(4.63)
and the map
ξ = F ∗φ + (0, ζ) 	→A∗ζ = F ∗A⊤φ + (ζ(0), −Dζ): D(A∗) →M q (4.64)
is independent of the choice of the representation of ξ in terms of (φ, ζ).
Remark 4.5. For h ﬁnite we can choose a representation for D(A⊤∗) of the
form
ξ = (ξ0, ¯L¯ξ0 + ζ), A⊤∗ξ = (L¯ξ0 + ζ(0), −Dζ),
where ¯ξ0 denotes the constant function equal to ξ0 on [−h, 0]. This choice can
be found in the early papers on the topic but does not extend to h = +∞.
⊓⊔
Proof. It is suﬃcient to prove (i). The proof of (ii) is identical up to a change
in the superscript ⊤. We have already established in Theorem 4.5 (ii) that
FD(A) ⊂D(A⊤∗),
A⊤∗Fφ = FAφ,
∀φ ∈D(A).
Hence for all ξ of the form Fφ, φ ∈D(A),
A⊤∗ξ = FAφ = F(Lφ, Dφ) = (Lφ, ¯LDφ).
Now we can always associate with an arbitrary ξ in D(A⊤∗) an element φ1 ∈
W 1,p(−h, 0; Rn) such that φ1(0) = ξ0 and decompose ξ in two terms
ξ = (ξ0, ξ1) = (ξ0, ¯Lφ1) + (0, ξ1 −¯Lφ1) = Fφ + (0, ζ),
where by construction
(ξ0, ¯Lφ1) = (φ1(0), ¯Lφ1) = F(φ1(0), φ1)
and (φ1(0), φ1) ∈D(A). In view of the previous remarks it is now suﬃcient
to characterize the elements of D(A⊤∗) of the form (0, ζ).

4 State space theory of linear time-invariant systems
271
Given (0, ζ) ∈D(A⊤∗), let x and y in W 1,p
loc (0, ∞; Rn) be the solutions of
the systems

˙x(t) = (Le0
+x)(t) + (e−h
+ ζ)(−t),
x(0) = 0,
(4.65)

˙y(t) = (Le0
+y)(t) + (e−h
+ [A⊤∗(0, ζ)]1)(−t),
y(0) = [A⊤∗(0, ζ)]0.
(4.66)
Notice that
∀T > 0,
e0
+x ∈W 1,p(−h, T ; Rn).
Deﬁne
ˆx(t) = S⊤∗(t)(0, ζ),
ˆy(t) = S⊤∗(t)A⊤∗(0, ζ).
By Theorem 4.4 (i)

ˆx(t) = F(x(t), (e0
+x)t) + (0, τ(t)ζ),
ˆy(t) = F(y(t), (e0
+y)t) + (0, τ(t)[A⊤∗(0, ζ)]1)
(4.67)
and for (0, ζ) ∈D(A⊤∗)
dˆx
dt (t) = ˆy(t),
∀t ≥0.
In particular
dx
dt (t) = y(t) =⇒x ∈W 2,p
loc (0, ∞; Rn).
Deﬁne for t ≥0
X(t) =
 t
0
y(s) ds,
t ≥0
and
X(t) = 0,
t ∈I(−h, 0)
and notice that
X = e0
+x,
˙X = e0
+ ˙x = e0
+y.
(4.68)
Integrate (4.66) from 0 to t
˙X(t) = [A⊤∗(0, ζ)]0 +
 t
0
(Le0
+y)(s) ds +
 t
0
(e−h
+ [A⊤∗(0, ζ)]1)(−s) ds.
In view of Theorem 3.3 (iv) and (4.68)
 t
0
(Le0
+y)(s) ds =
 t
0
(L ˙X)(s) ds = LXt −LX0 = LXt
and

272
II-4 State Space Theory of Diﬀerential Systems With Delays
˙X(t) = LXt + [A⊤∗(0, ζ)]0 +
 t
0
(e−h
+ [A⊤∗(0, ζ)]1)(−s) ds.
(4.69)
As ˙x(t) = ˙X(t), t ≥0, the right-hand sides of (4.65) and (4.69) are equal and
in view of the fact that X = e0
+x
(e−h
+ ζ)(−t) = [A⊤∗(0, ζ)]0 +
 t
0
(e−h
+ [A⊤∗(0, ζ)]1)(−s) ds.
Introducing the new variable α = −t for −t ∈I(−h, 0), we obtain
ζ(α) = [A⊤∗(0, ζ)]0 +
 0
α
[A⊤∗(0, ζ)]1(θ) dθ,
α ∈I(−h, 0).
(4.70)
Hence
ζ ∈W 1,p(−h, 0; Rn),
ζ(0) = [A⊤∗(0, ζ)]0,
Dαζ = −[A⊤∗(0, ζ)]1.
(4.71)
When h = ∞, we know by Lemma 3.5 that
ζ ∈W 1,p(−∞, 0; Rn) =⇒ζ ∈C0(−∞, 0; Rn) =⇒ζ(−∞) = 0.
For h ﬁnite we go back to (4.65) and
˙x = y ∈W 1,p
loc (0, ∞; Rn) =⇒˙x ∈Cloc (0, ∞; Rn)
∀T > 0,
e0
+x ∈W 1,p(−h, T ; Rn) =⇒Le0
+x ∈C(0, T ; Rn).
So
(e−h
+ ζ)(−t) = ˙x(t) −(Le0
+x)(t)
is continuous for t ≥0 and in particular continuous at t = h. But
t > h =⇒(e−h
+ ζ)(−t) = 0 =⇒ζ(−h) = 0.
So we have established that
ζ ∈W 1,p(−h, 0; Rn),
ζ(−h) = 0
(4.72)
and that
A⊤∗(0, ζ) = (ζ(0), −Dαζ).
(4.73)
Conversely given ζ verifying (4.72) and x the solution of (4.65), we want
to prove that (0, ζ) ∈D(A⊤∗) or equivalently that for all t ≥0
dˆx
dt (t)
exists in M p,
where ˆx is given by (4.67). Speciﬁcally we shall show that

4 State space theory of linear time-invariant systems
273
dˆx
dt (t) = F

˙x(t), d
dt(e0
+x)t

+

0, d
dtτ(t)ζ

(4.74)
and that
t 	→

˙x(t), d
dt(e0
+x)t, d
dtτ(t)ζ

is continuous. By Theorem 4.1 (ii) e0
+x ∈W 1,p(−h, T ; Rn) for all T > 0 and
by Lemma 4.1 (ii)
(e0
+x)· ∈C1
0, T ; Lp(−h, 0; Rn)

and the function
t 	→d
dt(e0
+x)t : [0, ∞[ →Lp(−h, 0; Rn)
is continuous. By assumption (4.72) on ζ,
e−h
+ ζ ∈W 1,p(−∞, 0; Rn)
and because the function τ(t)ζ is a shift of the function e−h
+ ζ the function
t 	→τ(t)ζ : [0∞[ →Lp(−h, 0; Rn)
is C1 and
t 	→d
dtτ(t)ζ = −τ(t) Dζ
is continuous. To show that ˙x is continuous we prove that the right-hand side
of (4.65) is continuous. The continuity of the term (e−h
+ ζ)(−t) for t ≥0 follows
from the fact that e−h
+ ζ belongs to W 1,p(−∞, 0; Rn). Similarly by Lemma 4.1
(ii),
e0
+x ∈W 1,p(−h, T ; Rn)
implies that
(e0
+x)· ∈C

0, T ; W 1,p(−h, 0; Rn)

and necessarily
t 	→(Le0
+x)(t) = L(e0
+x)t
is continuous. So by deﬁnition of ˆx and continuity of F we obtain (4.74), the
existence of dˆx
dt (t) as a continuous function of t and therefore (0, ζ) ∈D(A⊤∗).
We have established that any element ξ of D(A⊤∗) is of the form
ξ = Fφ + (0, ζ),
φ ∈D(A),
ζ ∈W 1,p(−h, 0; Rn),
ζ(−h) = 0,
(4.75)
and that

274
II-4 State Space Theory of Diﬀerential Systems With Delays
A⊤∗ξ = FAφ + (ζ(0), −Dζ) = (Lφ + ζ(0), ¯LDφ −Dζ).
To complete the proof we have to show that A⊤∗ξ is independent of the
representation of ξ in terms of φ and ζ. By linearity it is suﬃcient to establish
that for all φ in D(A) and ζ verifying (4.72)
ξ = Fφ + (0, ζ) = 0 =⇒A⊤∗ξ = FAφ + (ζ(0), −Dζ) = 0
or for ζ, φ ∈W 1,p(−h, 0; Rn) such that ζ(−h) = 0

φ(0) = 0
¯Lφ + ζ = 0
=⇒

Lφ + ζ(0) = 0,
¯LDφ −Dζ = 0.
(4.76)
This will require the following lemma.
Lemma 4.6. Let p, 1 ≤p < ∞, be a real number:
(i) For all φ in W 1,p(−h, 0; Rn), the map
Fφ = (φ(0), ¯Lφ) 	→Lφ
(4.77)
is well deﬁned, linear, and there exists a constant c > 0 such that for all
φ ∈W 1,p(−h, 0; Rn),
|Lφ| ≤c[|φ(0)| + ∥¯Lφ∥p + ∥¯LDφ∥p].
(4.78)
(ii) For all φ in W 1,p(−h, 0; Rn) such that φ(0) = 0
¯Lφ ∈W 1,p(−h, 0; Rn),
(¯Lφ)(−h) = 0,
(¯Lφ)(0) = Lφ,
Dθ ¯Lφ = −¯LDθ φ.
(4.79)
Going back to the proof of the theorem, φ satisﬁes the conditions of
Lemma 4.6 (ii) and from (4.76)
ζ = −¯Lφ =⇒

ζ(0) = −(¯Lφ)(0) = −Lφ,
Dθ ζ = ¯LDθ φ.
This completes the proof of the theorem.
⊓⊔
Proof of Lemma 4.6.
(i) By linearity the map (4.77) is well deﬁned if
(φ(0), ¯Lφ) = 0 =⇒Lφ = 0.
But by Lemma 3.5, W 1,p(−h, 0; Rn) ⊂K(−h, 0; Rn) and by Lemma 4.3
φ ∈K(−h, 0; Rn),
φ(0) = 0 =⇒(¯Lφ)(0) = Lφ
and necessarily ¯Lφ = 0 =⇒Lφ = (¯Lφ)(0) = 0. For the continuity we associate
with φ ∈W 1,p(−h, 0; Rn), the function

4 State space theory of linear time-invariant systems
275
˜φ(t) =

φ(t),
t ∈I(−h, 0),
φ(0),
t ≥0.
For all T > 0, ˜φ ∈W 1,p(−h, T ; Rn) and
˙˜φ = Dt ˜φ = e0
−Dθ φ.
Then by Theorem 3.3 (iv), for all α ∈I(−h, 0)
 0
α
(¯LDθ φ)(θ) dθ =
 0
α
(Le0
−Dθ φ)(−θ) dθ = L˜φ−α −Lφ.
Let τ = h if h is ﬁnite and τ = 1 if h = ∞. Then taking the Lp norm on [0, τ]
τ 1/p|Lφ| ≤c∥¯LDθ φ∥Lp(−h,0) + ∥L˜φ∥Lp(0,τ).
But
L˜φ = Le0
+ ˜φ + Le0
−˜φ = Le0
+φ(0) + Le0
−φ
and
∥L˜φ∥Lp(0,τ) ≤c∥e0
+φ(0)∥Lp(−h,τ)+∥¯Lφ∥Lp(−h,0)≤cτ1/p|φ(0)|+∥¯Lφ∥Lp(−h,0).
Combining the last inequalities we get (4.78).
(ii) When φ(0) = 0, ˜φ = e0
−φ and for all α in I(−h, 0)
(¯Lφ)(α) = (Le0
−φ)(−α) = L˜φ−α = Lφ +
 0
α
(¯LDθ φ)(θ) dθ.
Hence ¯Lφ ∈W 1,p(−h, 0; Rn) and D¯Lφ = −¯L Dφ. Moreover by Lemma 3.5,
W 1,p(−h, 0; Rn) ⊂K(−h, 0; Rn) and from Lemma 4.3 φ ∈K(−h, 0; Rn) and
φ(0) = 0 imply that (¯Lφ)(0) = Lφ and (¯Lφ)(−h) = 0.
⊓⊔
4.7 The companion structural operator G of F
We have seen in Theorem 4.3 how the adjoint semigroups {S⊤∗(t)} and
{S∗(t)} are related to the solution of systems
˙x(t) = (Le0
+x)(t) + (e−h
+ ξ1)(−t),
t > 0, x(0) = ξ0,
(4.80)
˙z(t) = (L⊤e0
+z)(t) + (e−h
+ ζ1)(−t),
t > 0, z(0) = ζ0.
(4.81)
More precisely
S⊤∗(t)ξ = F(x(t), (e0
+x)t) + (0, τ(t)ξ1),
(4.82)
S∗(t)ζ = F ∗(z(t), (e0
+z)t) + (0, τ(t)ζ1).
(4.83)

276
II-4 State Space Theory of Diﬀerential Systems With Delays
When h is ﬁnite and t ≥h, the second term in (4.82) and (4.83) is zero and
by the intertwining theorem
S⊤∗(t)ξ = S⊤∗(t −h)S⊤∗(h)ξ = S⊤∗(t −h)F(x(h), xh)
= FS(t −h)(x(h), xh),
(4.84)
S∗(t)ζ = S∗(t −h)S∗(h)ζ = S∗(t −h)F ∗(z(h), zh)
= F ∗S⊤(t −h)(z(h), zh).
(4.85)
For t ≥h the structure of S⊤∗(t) and S∗(t) considerably simpliﬁes provided
we have a knowledge of the pairs
Gξ = (x(h), xh),
G⊤ζ = (z(h), zh),
(4.86)
which deﬁne continuous linear operators on M p and M q, respectively. They
are related to F and F ∗because for ξ = Fφ or ζ = F ∗ψ, we obtain

S(h)φ = (x(h), xh) = GFφ,
S⊤(h)ψ = (z(h), zh) = G⊤F ∗ψ,
(4.87)
and
S(h) = GF,
S⊤(h) = G⊤F ∗.
(4.88)
We shall see that G∗= G⊤and that G and G⊤are intertwining operators
similar to F and F ∗. They will play a complementary role to F and F ∗in
the characterization of the F-controllability and the F-observability for delay
systems.
Deﬁnition 4.4. The operators G and G⊤will be referred to as the companion
structural operators of F and F ∗, respectively.
⊓⊔
The operator G was introduced by A. Manitius [6]. It will not be used
in this chapter, but it has played an important role in conjunction with the
operator F especially for the characterization of the notions of controllability
and observability.
Theorem 4.7. Let h, 0 < h < ∞, p, 1 ≤p < ∞, and q, 1 ≤q < ∞, be real
numbers:
(i) The operators
ξ 	→Gξ = (x(h), xh): M p →M p,
(4.89)
ζ 	→G⊤ζ = (z(h), zh): M q →M q
(4.90)
are linear and continuous and
GF = S(h),
G⊤F ∗= S⊤(h).
(4.91)

4 State space theory of linear time-invariant systems
277
(ii) The operators G and G⊤deﬁne isomorphisms
G: M p →D(A),
G⊤: M q →D(A⊤)
(4.92)
when D(A) and D(AT ) are endowed with their respective graph norm
topologies.
(iii) For p, 1 < p < ∞, and its conjugate q, q−1 + p−1 = 1,
G∗= G⊤
(4.93)
when the elements of (M p)∗are identiﬁed with those of M q.
Proof. (i) By construction and the remarks preceding the theorem.
(ii) We only prove this result for G. The map G is injective because Gξ = 0
implies that
(x(h), xh) = 0 =⇒x(t) = 0,
0 ≤t ≤h
and by substituting in (4.80)
ξ1(−t) = 0,
0 ≤t ≤h, ξ0 = 0 =⇒ξ = 0.
It is surjective because for any ψ in D(A)
ψ1 ∈W 1,p(−h, 0; Rn)
and
ψ0 = ψ1(0)
and we can deﬁne the function x in W 1,p(0, h; Rn)
x(t) = ψ(t −h),
0 ≤t ≤h
and the element φ = (φ0, φ1) of M p
φ1(−t) = ˙x(t) −(Le0
+x)(t),
0 ≤t ≤h, φ0 = x(0).
By deﬁnition of G
Gφ = (x(h), xh) = (ψ1(0), ψ1) = ψ.
The continuity of G from M p to D(A) follows from inequality (4.7) in Theo-
rem 4.1 (i).
(iii) To establish (4.93) we use identity (4.35) in Lemma 4.2 with T = h
 h
0
z(h−t)·(˙x−Le0
+x)(t) dt+z(h)·x(0)=
 h
0
( ˙x−Le0
+z)(h−t)·x(t) dt+z(0)·x(h).
By substitution of (4.80) and (4.81)
 h
0
z(h−t)·(e−h
+ ξ1)(−t) dt+z(h)·ξ0 =
 h
0
(e−h
+ ζ1)(−t)·x(h−t) dt+ζ0 ·x(h)
and by changing the variable t to α = −t
 0
−h
zh(α) · ξ1(α) dα + z(h) · ξ0 =
 0
−h
ζ1(α) · xh(α) dα + ζ1 · x(h).
This completes the proof of the theorem.
⊓⊔

278
II-4 State Space Theory of Diﬀerential Systems With Delays
The last theorem is the intertwining theorem for G, S⊤∗and S.
Theorem 4.8 (Intertwining). Let h, 0 < h < ∞, p, 1 < p < ∞, q,
p−1 + q−1 = 1, be real numbers. Then the following properties hold and are
equivalent:
(i) S(t)G = GS⊤∗(t), ∀t ≥0.
(ii) GD(A⊤∗) ⊂D(A) and GA⊤∗ξ = AGξ, ∀ξ ∈D(A⊤∗).
(iii) For all λ in the resolvent set of A⊤∗
R(λ, A)Gξ = GR(λ, A⊤∗)ξ,
∀ξ ∈M p.
(4.94)
Proof. It is suﬃcient to establish (i). The equivalence of (i), (ii), and (iii) is a
consequence of Lemma 4.4. Notice that for t ≥0
(x(t + h), xt+h) = S(t)(x(h), xh) = S(t)Gξ
and
(z(t + h), zt+h) = S⊤(t)(z(h), zh) = S⊤(t)G⊤ζ.
Now substitute in identity (4.35) in Lemma 4.2 the solutions x and z of (4.80)
and (4.81) for T = t + h:
 t+h
0
z(t + h −s) · (e−h
+ ξ1)(−s)ds + z(t + h) · x(0)
=
 t+h
0
(e−h
+ ζ1)(−s) · x(t + h −s) ds + z(0) · x(t + h).
Notice that the integrands are zero for s > h and change the variable s to
α = −s:
 0
−h
z(t + h + α) · ξ1(α) dα + z(t + h) · ξ0
=
 0
−h
ζ1(α) · x(t + h + α) dα + ζ0 · x(t + h).
Therefore
⟨S⊤(t)G⊤ζ, ξ⟩= ⟨ζ, S(t)Gξ⟩
and (i) is veriﬁed.
⊓⊔
Corollary 4.3. Assume that the hypotheses of Theorem 4.8 are veriﬁed. Then
the following properties are veriﬁed and are equivalent:
(i) S⊤(t)G⊤= G⊤S∗(t),
t ≥0.
(ii) G⊤D(A∗) ⊂D(A⊤) and G⊤A∗ξ = A⊤G⊤ξ, ∀ξ ∈D(A∗).
(iii) For all λ in the resolvent set of A∗
R(λ, A⊤)G⊤ξ = G⊤R(λ, A∗)ξ,
∀ξ ∈M q.
(4.95)

5 State space theory of linear control systems
279
Remark 4.6. Both G and G⊤have continuous linear inverse G−1 : D(A) →
M p and (G⊤)−1 : D(A⊤) →M q. So in view of Theorem 4.8 and its corollary
S⊤⋆(t) = G−1S(t)G,
S⋆(t) = (G⊤)−1S⊤(t)G⊤
and the adjoint semigroups are completely characterized in terms of {S(t)}
and {S⊤(t)}.
⊓⊔
5 State space theory of linear control systems
The basic concepts and techniques to deal with linear delay systems have been
introduced in §3 and §4. In this section a control vector u(t) ∈Rm (m ≥1,
an integer) is introduced in the system dynamics through a delay operator B,
which is a continuous linear map
B : K(−h, 0; Rm) →Rn
(5.1)
similar to the continuous linear map
L: K(−h, 0; Rn) →Rn
(5.2)
of §3 and §4.
Consider the linear control system

˙x(t) = Lxt + But + f(t),
t > 0,
(x(0), x0, u0) = (φ0, φ1, w) ∈M p × Lp(−h, 0; Rm)
(5.3)
for the real number p, 1 ≤p < ∞, and f ∈Lp
loc(0, ∞; Rn). The objective
is to construct a state and give a state space formulation of equations (5.3)
where delays in both variables x and u disappear. The motivation behind this
objective is to put (5.3) in the form of an evolution equation in an inﬁnite
dimensional Banach space without delays to bring such systems in line with
the standard Control Theory of inﬁnite dimensional evolution systems.
The two constructions that will be considered here were originally intro-
duced by R. B. Vinter and R. H. Kwong [1] and A. Ichikawa [3]. In the
light of §3.4 they correspond to the states
ˆx(t) = F(x(t), xt)
(5.4)
and
˜x(t) = (x(t), xt)
(5.5)
as deﬁned in (4.51) (Theorem 4.4 and (4.12)). They will require the introduc-
tion of new semigroups that are extensions or generalizations of the semigroups
{S⊤∗(t)} and {S(t)} of §3.4, and the analogs for B of the structural operators
¯L and ¯L⊤for L.

280
II-4 State Space Theory of Diﬀerential Systems With Delays
All results established for L (Theorem 3.3, Lemmas 4.2, 4.3, 4.4, 4.5, and
4.6) extend to B. So they will not be repeated here. We simply introduce the
appropriate notation associated with B. There exists an n × m matrix β of
regular Borel measures such that
Bw =
 0
−h
dθ βw(θ),
∀w ∈K (−h, 0; Rm).
(5.6)
For each T > 0, the continuous linear map

B: Lp(−h, T ; Rm) →Lp(0, T ; Rn),
(Bv)(t) = Bvt,
∀t ≥0, ∀v ∈Cc(−h, T ; Rm)
(5.7)
has the same properties as L. Denote by β⊤the transposed of the matrix β
and introduce the continuous linear map
φ 	→B⊤φ =
 0
−h
dθ β⊤φ(θ): K(−h, 0; Rn) →Rm.
(5.8)
For q, 1 ≤q < ∞, and all T > 0, we also have the continuous linear map

B⊤: Lq(−h, T ; Rn) →Lq(0, T ; Rm),
(B⊤z)(t) = B⊤zt,
∀t ≥0, ∀z ∈Cc(−h, T ; Rn).
(5.9)
Finally we associate with B and B⊤the structural operators
 ¯B : Lp(−h, 0; Rm) →Lp(−h, 0; Rn),
( ¯Bw)(α) = (Be0
−w)(−α),
α ∈I(−h, 0),
(5.10)
 ¯B⊤: Lq(−h, 0; Rn) →Lq(−h, 0; Rm),
( ¯B⊤ψ)(α) = (B⊤e0
−ψ)(−α),
α ∈I(−h, 0).
(5.11)
When 1 < p < ∞and q is the conjugate of p, p−1 + q−1 = 1
¯B∗= ¯B⊤,
(5.12)
where the elements of (Lp)′ are identiﬁed with those of Lq.
5.1 The structural state
The ﬁrst step is to rewrite system (5.3) in the more accurate form
˙x = Lx + Bu + f,
(x(0), x0, u0) = (φ0, φ1, w) ∈M p × Lp(−h, 0; Rm).
(5.13)

5 State space theory of linear control systems
281
Then separate the solution x(t), t ≥0 and the control u(t), t ≥0, from the
pieces of initial functions φ1 and w:
˙x = Le0
+x + Be0
+u + Le0
−φ1 + Be0
−w + f,
x(0) = φ0.
(5.14)
As in §4.4 notice that system (5.14) does not directly use the initial functions
φ1 and w but only the sum of their images Le0
−φ1 + Be0
−w. When h is ﬁnite
(Le0
−φ1)(t) + (Be0
−w)(t) = 0,
t > h
(5.15)
and it is suﬃcient to consider the eﬀect of the initial functions on the interval
[0, h]. We recognize the structural operators ¯L and ¯B extended by 0 for t > h:
(Le0
−φ1)(t) + (Be0
−w)(t) =

e−h
+ (¯Lφ1 + ¯Bw)

(−t),
t ≥0.
(5.16)
So the true initial condition to system (5.13) is the pair
(φ0, ¯Lφ1 + ¯Bw) = Fφ + (0, ¯Bw)
(5.17)
and this suggests the introduction of the following state:
ˆx(t) = (x(t), ¯Lxt + ¯But) = F(x(t), xt) + (0, ¯But)
(5.18)
at time t ≥0. Compare (5.18) to (4.51) in Theorem 4.3 or to the intertwining
identity in Theorem 4.4 between {S⊤∗(t)} and {S(t)}. It is clear that the
state (5.18) is related to a nonhomogeneous version of the evolution equation
associated with the semigroup {S⊤∗(t)}. So we are led to the embedding of
the initial system (5.14) into the following larger family of systems:
⎧
⎪
⎨
⎪
⎩
˙x(t) = (Le0
+x)(t) + (Be0
+u)(t) + (e−h
+ ξ1)(−t) + f(t),
t > 0,
x(0) = ξ0,
ξ = (ξ0, ξ1) ∈M p,
f ∈Lp
loc(0, ∞; Rn),
u ∈Lp
loc(0, ∞; Rm).
(5.19)
Deﬁnition 5.1. The structural state ˆx(t) at time t ≥0 is deﬁned by
ˆx(t) = (x(t), ¯L(e0
+x)t + ¯B(e0
+u)t + τ(t)ξ1),
(5.20)
where τ(t) is the right translation operator deﬁned in (4.53) of Theorem 4.3
[τ(t)u](θ) = (e−h
+ u)(θ −t),
θ ∈I(−h, 0)
(5.21)
for any arbitrary function u deﬁned on I(−h, 0). If ξ1 = ¯Lφ1 + ¯Bw, then the
deﬁnition (5.20) reduces to (5.18).
⊓⊔
Other
terminologies
have
been
used:
the
Vinter-Kwong
state
in
M. C. Delfour [14] or the forcing function state in G. Grippenberg,
S. O. Londen, and O. Staffans [1].
It turns out that the state ˆx(t) is the solution of a nonhomogeneous dif-
ferential equation that need to be interpreted in an appropriate weak sense.
In the process all delays will disappear and the equation will take the same
form as its ﬁnite dimensional analog.

282
II-4 State Space Theory of Diﬀerential Systems With Delays
Notation 5.1. The restriction of the continuous linear operator
B⊤: K(−h, 0; Rn) →Rm
to D(A⊤) is well deﬁned because W 1,q(−h, 0; Rn) is continuously embedded
into K(−h, 0; Rn) (cf. Lemma 3.5. Its restriction to D(A⊤)
ψ = (ψ(0), ψ) 	→B⊤ψ: D(A⊤) →Rm
(5.22)
will also be denoted B⊤.
⊓⊔
Theorem 5.1. Let the real number p, 1 < p < ∞, and its conjugate q, q−1 +
p−1 = 1, be given. Assume that x in W 1,p
loc (0, ∞; Rn) is the solution of system
(5.19) for ξ ∈M p, f ∈Lp
loc(0, ∞; Rn) and u ∈Lp
loc(0, ∞; Rm), and let ˆx(t) be
the structural state constructed from x in (5.20)–(5.21):
(i) For all ψ in D(A⊤) and t ≥0
⟨ψ, ˆx(t)⟩= ⟨S⊤(t)ψ, ξ⟩+
 t
0
[B⊤S⊤(t −r)ψ · u(r)
+ ⟨S⊤(t −s)ψ, (f(s), 0)⟩] dr.
(5.23)
(ii) The structural state ˆx(t) is a solution of the weak equation: For all t > 0
and all ψ ∈D(A⊤)
⎧
⎨
⎩
d
dt⟨ψ, ˆx(t)⟩= ⟨A⊤ψ, ˆx(t)⟩+ B⊤ψ · u(t) + ψ(0) · f(t),
ˆx(0) = ξ.
(5.24)
(iii) For each T > 0, the state ˆx is the unique solution in
V(0, T ; M p, D(A⊤)′)
=

z ∈C(0, T ; M p): d
dtj∗z ∈Lp(0, T ; D(A⊤)′)

(5.25)
to the following equation:
⎧
⎨
⎩
d
dtj∗ˆx(t) = (A⊤)∗ˆx(t) + (B⊤)∗u(t) + j∗(f(t), 0),
t > 0,
ˆx(0) = ξ,
(5.26)
where j∗, (A⊤)∗, and (B⊤)∗are the dual maps of the continuous linear
operators
j : D(A⊤) →M q, A⊤: D(A⊤) →M q, and B⊤: D(A⊤) →Rm (5.27)
deﬁned on D(A⊤) (cf. Notation 4.2, Deﬁnition 4.2, Notation 5.1).

5 State space theory of linear control systems
283
Remark 5.1. In their original work, R. B. Vinter and R. H. Kwong [1]
considered a delay operator B of the form
Bw = B0w(0) +
 0
−h
B1(θ)w(θ) dθ
(5.28)
for n × m matrices B0 and B1(θ), where the elements of B1 belong to
Lp(−h, 0; R) and a ﬁnite memory h. In this case
B⊤ψ = B⊤
0 ψ(0) +
 0
−h
B1(θ)⊤ψ(θ) dθ
(5.29)
and both B and B⊤can be considered as linear maps on Rm × Lp(−h, 0; Rm)
and M q = Rn × Lq(−h, 0; Rn)
⎧
⎪
⎪
⎨
⎪
⎪
⎩
B(w0, w1) = B0w0 +
 0
−h
B1(θ)w1(θ) dθ,
B⊤(ψ0, ψ1) = B⊤
0 ψ0 +
 0
−h
B⊤
1 (θ)ψ1(θ) dθ.
(5.30)
This of course is the limit case where everything stays in the state space M q
and equation (5.23) reduces to
ˆx(t) = S⊤∗(t)∗ξ +
 t
0
S⊤∗(t −r)[(B⊤)∗u(r) + (f(r), 0)] dr
(5.31)
because B⊤∗: Rm →M p is now linear and continuous. The weak formulations
(5.23), (5.24), and (5.26) were introduced by M. C. Delfour [15] to study
the corresponding Riccati equation. They were absolutely necessary to handle
control operators with pure delays.
⊓⊔
Remark 5.2. Notice that the underlying semigroup is the adjoint of the trans-
posed semigroup associated with L. A careful interpretation of the proof of
Theorem 5.1 indicates that the structural state is really the resulting product
of transposition techniques applied to the diﬀerential equation (5.19). The
elements entering into the structural state are obtained by isolating on one
side the transposed semigroup associated with the transposed system.
⊓⊔
Proof of Theorem 5.1.
In view of the linearity of system (5.19) and of the
deﬁnition of the semigroup {S⊤∗(t)} in Theorem 4.3, it is suﬃcient to establish
identity (5.23) for ξ = 0. Again the key tool is the integration by part formula
(4.35) in Lemma 4.2 for T = t
 t
0
z(t −s) · [ ˙x −Le0
+x](s) ds + z(t) · x(0)
=
 t
0
[ ˙z −L⊤e0
+z](s) · x(t −s) ds + z(0) · x(t),
(5.32)

284
II-4 State Space Theory of Diﬀerential Systems With Delays
where x is the solution of
˙x = Le0
+x + Be0
+u + f,
x(0) = 0
(5.33)
and z is the solution of
˙z = L⊤e0
+z + L⊤e0
−ψ,
z(0) = ψ(0),
ψ ∈D(A⊤).
(5.34)
By techniques analogous to the ones used in the proof of Lemma 4.2 we can
also show that for u in Lp
loc(0, ∞; Rm)
 t
0
z(t −s) · (Be0
+u)(s) ds =
 t
0
(B⊤e0
+z)(t −s) · u(s) ds.
(5.35)
Substract (5.35) from (5.32) and use (5.33) and (5.34):
 t
0
z(t −s) · f(s) ds
=
 t
0
[(L⊤e0
−ψ)(s) · x(t −s) −(B⊤e0
+z)(t −s) · u(s)] ds + ψ(0) · x(t)
and the right-hand side becomes
 t
0
[(L⊤e0
−ψ)(s) · x(t −s) + (B⊤e0
−ψ)(s) · u(t −s)] ds + ψ(0) · x(t)
=
 t
0
[z(t −s) · f(s) + (B⊤e0
+z)(t −s) · u(s) + (B⊤e0
−ψ)(s) · u(t −s)] ds.
But this can be rewritten as
 0
−h
[(¯L⊤ψ)(α) · (e0
+x)t(α) + ( ¯B⊤ψ)(α) · (e0
+u)t(α)] dα
+ ψ(0) · x(t) =
 t
0
z(t −s) · f(s) + (B⊤z)(t −s) · u(s) ds,
and as
S⊤(r)ψ = (z(r), zr) ∈D(A⊤),
we have
(B⊤z)(t −s) = B⊤zt−s = B⊤S⊤(t −s)ψ,
when B⊤is viewed as a continuous linear operator from D(A⊤) into Rm. So
ﬁnally as ¯L⊤= ¯L∗, (¯L⊤)∗= ¯L and ¯B⊤= ¯B∗and ( ¯B⊤)∗= ¯B
⟨(ψ(0), ψ, (x(t), ¯L(e0
+x)t + ¯B(e0
+u)t)⟩
=
 t
0

-
S⊤(t −s)ψ,

0, f(s)
.
+ ¯B⊤S⊤(t −s)ψ · u(s)

ds.

5 State space theory of linear control systems
285
The left-hand side of the above expression is precisely ⟨ψ, ˆx(t)⟩for ξ = 0 and
this completes the proof of part (i).
(ii) The diﬀerential equation (5.24) is obtained by diﬀerentiating (5.23)
using the fact that for all T > 0
z(·) = S⊤(·)ψ ∈C0
0, T ; D(AT)

∩C1(0, T ; M q).
For instance if ϕ ∈D(]0, T [, T > 0, the distributional derivative of the term
F(t) =
 t
0
z(t −s) · f(s) ds
is given by the expression
−
 T
0
∂ϕ
∂t (t)
 t
0
z(t −s) · f(s) ds dt
= −
 T
0
dsf(s) ·
 T
s
dt∂ϕ
∂t (t)z(t −s)
=
 T
0
dsf(s) ·
 T
s
dtϕ(t)dz
dt (t −s) + ϕ(s)z(0)

=
 T
0
ϕ(t)
 t
0
dsdz
dt (t −s) · f(s) + z(0) · f(t)
	
dt
and
dF
dt (t) =
 t
0
dz
dt (t −s) · f(s) ds + z(0) · f(t)
=
 t
0
⟨S⊤(t −s)A⊤ψ, (f(s), 0)⟩ds + ψ(0) · f(t).
We repeat the above computation for the term in u but with ψ ∈D

(A⊤)2
,
and the sum of the derivatives of the right-hand side of (5.23) for ψ in
D

(A⊤)2
is equal to
⟨S⊤(t)A⊤ψ, ξ⟩+
 t
0
⟨S⊤(t −s)A⊤ψ, (f(s), 0)⟩ds
+ ψ(0) · f(t) +
 t
0
B⊤S⊤(t −s)A⊤ψ · u(s) ds + B⊤ψ · u(t)
= ⟨A⊤ψ, ˆx(t)⟩+ B⊤ψ · u(t) + ψ(0) · f(t).
So we obtain (5.24) for ψ ∈D

(A⊤)2
and this identity extends by linearity,
continuity, and density of D

(A⊤)2
in D(A⊤) to all elements ψ of D(A⊤).
(iii) From (5.23) ˆx ∈C(0, T ; M p) and from (5.24) we obtain expression
(5.26) and
d
dtj∗z ∈Lp(0, T ; D(A⊤)′). The last element to complete the proof

286
II-4 State Space Theory of Diﬀerential Systems With Delays
is the uniqueness. By linearity it is suﬃcient to prove that for ξ = 0, f = 0,
and u = 0 the solution ˆx in V(0, T ; M p, D(A⊤)′) to
d
dtj∗ˆx(t) = (A⊤)∗ˆx(t),
ˆx(0) = 0,
is zero. Fix t′, 0 < t′ ≤T , and construct the function
g(t) = ⟨S⊤(t′ −t)ψ, ˆx(t)⟩,
0 ≤t ≤t′,
for ψ ∈D(A⊤). Then S⊤(·)ψ ∈C1(0, t′; M q) ∩C

0, t′; D(A⊤)

and hence
g ∈W 1,p(0, t′; R). Moreover g(0) = 0 and
dg
dt (t) = −⟨A⊤S⊤(t′ −t)ψ, ˆx(t)⟩Mq×Mp
+ ⟨S⊤(t′ −t)ψ, (A⊤)∗ˆx(t)⟩D(A⊤)×D(A⊤)′ = 0.
Then
0 = g(0) = g(t′) = ⟨ψ, ˆx(t′)⟩
and because this is true for all ψ in D(A⊤), ˆx(t′) = 0, ∀t′ > 0. This completes
the proof of the theorem.
⊓⊔
5.2 The extended state
In this section we come back to system (5.13) and extend the deﬁnition of the
ﬁrst state ˜x(t) = (x(t), xt) introduced in §4.2 to
˜x(t) = (x(t), xt, ut) ∈Zp,
Zp def
= Rn × Lp(−h, 0; Rn) × Lp(−h, 0; Rm).
(5.36)
So we increase the size of the state space by adding a third component. The
following presentation is new and extends to general delay operators the orig-
inal work of A. Ichikawa [1, 3].
Terminology 1. The extended state associated with L and B will be deﬁned
by (5.36) and denoted by ˜x(t).
5.2.1 The extended semigroup { ˜S(t)}
Because of this added feature we have to somehow extend the deﬁnition of the
semigroup {S(t)}. For this purpose we introduce the homogeneous system

˙x(t) = (Le0
+x)(t) + (Le0
−φ1)(t) + (Be0
−w)(t),
x(0) = φ0,
(φ0, φ1, w) ∈Zp
(5.37)
and the following linear transformations { ˜S(t): t ≥0} of Zp:
˜S(t)(φ0, φ1, w) = (x(t), xt, (e0
−w)t),
t ≥0.
(5.38)

5 State space theory of linear control systems
287
Terminology 2. The semigroup ˜S(t) will be called the extended semigroup as-
sociated with L and B.
This terminology is a little bit more descriptive than the Ichikawa semi-
group used in M. C. Delfour [14]. We shall use in § 6 the extended semigroup
associated with L⊤and C⊤, where C is an appropriate delay observation func-
tional. This is a natural extension of the transposed semigroup of §4.3.
Theorem 5.2. Let p, 1 ≤p < ∞, be a real number, let x be the solu-
tion of (5.37) in W 1,p
loc (0, ∞; Rn) corresponding to (φ0, φ1, w) ∈Zp, and let
˜S(t)(φ0, φ1, w) be given by (5.38):
(i) The family { ˜S(t): t ≥0} forms a strongly continuous semigroup of con-
tinuous linear transformations of Zp.
(ii) The inﬁnitesimal generator ˜A of { ˜S(t)} is given by
˜A(φ0, φ1, w) = (Lφ1 + Bw, Dφ1, Dw),
(5.39)
where
D( ˜A) =
⎧
⎪
⎨
⎪
⎩
(φ0, φ1, w) ∈Zp :
(φ0, φ1) ∈D(A)
w ∈W 1,p(−h, 0; Rm)
w(0) = 0
⎫
⎪
⎬
⎪
⎭
.
(5.40)
(iii) For p, 1 < p < ∞, and its conjugate q, q−1 + p−1 = 1, the adjoint
semigroup { ˜S∗(t)} is characterized by
˜S∗(t)(ξ0, ξ1, ξ2) = (z(t), ¯L⊤(e0
+z)t + τ(t)ξ1, ¯B⊤(e0
+z)t + τ(t)ξ2), (5.41)
where z is the solution of system

˙z(t) = (L⊤e0
+z)(t) + (e−h
+ ξ1)(−t),
z(0) = ξ0.
(5.42)
Its inﬁnitesimal generator is given by the map
ξ = (ψ(0), ¯L⊤ψ + ζ, ¯B⊤ψ + λ)
	→˜A∗ξ = (L⊤ψ + ζ(0), ¯L⊤Dψ −Dζ, ¯B⊤Dψ −Dλ),
(5.43)
which is independent of the choice of the representation of the element ξ
in the domain of A∗,
D( ˜A∗) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
ξ :
ξ = (ψ(0), ¯L⊤ψ + ζ, ¯B⊤ψ + λ)
(ψ(0), ψ) ∈D(A⊤)
ζ ∈W 1,q(−h, 0; Rn), ζ(−h) = 0
λ ∈W 1,q(−h, 0; Rm), λ(−h) = 0
⎫
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎭
(5.44)
in terms of (ψ, ζ, λ).

288
II-4 State Space Theory of Diﬀerential Systems With Delays
Proof. (i) The state ˜x(t) = ˜S(t)(φ0, φ1, w) is well deﬁned because
(Le0
−φ1)(t) = (e−h
+ ¯Lφ1)(−t), (Be0
−w)(t) = (e−h
+ ¯Bw)(−t),
and by Theorem 4.1 (i) there exists a constant c(T ) > 0 such that for all
T > 0
∥x∥W 1,p(0,T ;Rn) ≤C(T )[|φ0| + ∥¯Lφ2∥Lp + ∥¯Bw∥Lp]
(5.45)
and t 	→(e0
−w)t is the shift operator for the function e0
−w. It is linear and
continuous by inequality (5.45). The semigroup property is obvious and the
continuity with respect to t follows by continuity of x for t ≥0 and the
continuity of the shift operator t 	→xt, t 	→(e0
−w)t for Lp-functions.
(ii) Assume that (φ0, φ1, w) ∈D( ˜A). Then for all T > 0
˜x ∈C1(0, T ; M p) ∩C

0, T ; D( ˜A)

.
In particular
x ∈C1(0, T ; Rn),
x• ∈C1
0, T ; Lp(−h, 0; Rn)

,
(e0
−w)• ∈Lp(−h, 0; Rm).
By Lemma 4.1 (ii)
x ∈W 1,p(−h, T ; Rn),
(e0
−w) ∈W 1,p(−h, T ; Rm)
(5.46)
and
Dtut = Dθ ut,
Dt(e0
−w)t = Dθ (e0
−w)t.
(5.47)
As a result
φ1 = x|I(−h,0) ∈W 1,p(−h, 0; Rn),
φ1(0) = x(0) = φ0 =⇒(φ0, φ1) ∈D(A⊤).
Therefore we have established that any (φ0, φ1, w) ∈D( ˜A) veriﬁes
(φ0, φ1) ∈D(A⊤),
w ∈W 1,p(−h, 0; Rm),
w(0) = 0.
(5.48)
Finally because (5.46) is true, by Lemma 4.2 (ii)
x• ∈C

0, T ; W 1,p(−h, 0; Rn)

,
(e0
−w)• ∈C

0, T ; W 1,p(−h, 0; Rm)

and
dx
dt (t) = Lxt + B(e0
−w)t
has a continuous right-hand side. The same is true of the right-hand sides in
(5.47) and

5 State space theory of linear control systems
289
˜A(φ0, φ1, w) = lim
t↘0
dx
dt (t), d
dtxt, d
dt(e0
−w)t

= (Lφ1 + Bw, Dφ1, Dw).
Conversely
if
(φ0, φ1, w)
verify
(5.48),
then,
for
all
T
>
0,
e0
−w
∈W 1,p(−h, T ; Rm) and by Theorem 4.1 (ii), x ∈W 1,p(−h, T ; Rn). By
Lemma 4.1 (ii)
x• ∈C

0, T ; W 1,p(−h, 0; Rn)

and
(e0
−w)• ∈C

0, T ; W 1,p(−h, 0; Rm)

.
Thus
˙x(t) = Lxt + B(e0
−w)t
has a continuous right-hand side and ˙x ∈C(0, T ; Rn). Always by Lemma 4.1 (ii)
x• ∈C1
0, T ; Lp(−h, 0; Rn)

,
(e0
−w)• ∈C1
0, T ; Lp(−h, 0; Rm)

and necessarily
d
dt ˆx(t) =
dx
dt (t), d
dtxt, d
dt(e0
−w)t

exists and is continuous for t ≥0. So the limit as t goes to zero exists and by
deﬁnition (φ0, φ1, w) ∈D( ˜A).
(iii) Again we use the integration by parts formula (4.35) in Lemma 4.2
for the solution x of (5.37) and the solution z of (5.42) with T = t:
 t
0
z(t −s) · [ ˙x −Le0
+x](s) ds + z(t) · x(0)
=
 t
0
( ˙z −LT e0
+z)(t) · x(t −s) ds + z(0) · x(t).
By substitution
 t
0
z(t −s) · [(Le0
−φ1)(−s) + (Be0
−w)(−s)] ds + z(t) · φ0
=
 t
0
(e−h
+ ξ1)(−s) · x(t −s) ds + ξ0 · x(t)
and
 0
−h
(e0
+z)t(α)·[¯Lφ1+ ¯Bw](α) dα+z(t)·φ0 =
 0
−h
ξ1(α)·(e0
+x)t(α) dα+ξ0·x(t)
or in a more compact form
⟨(ξ0, ξ1), (x(t), (e0
+x)t)⟩= z(t) · φ0 + ⟨¯L⊤(e0
+z)t, φ1⟩+ ⟨¯B⊤(e0
+z)t, w⟩. (5.49)

290
II-4 State Space Theory of Diﬀerential Systems With Delays
To complete the above identity, we add the terms
⟨ξ2, (e0
−w)t⟩
and
⟨ξ1, (e0
−φ1)t⟩
and use the identity
⟨ξ2, (e0
−w)t⟩=
 0
−h
ξ2(α) • (e0
−w)(t + α) dα =
 t
t−h
ξ2(θ −t) · (e0
−w)(θ) dθ,
⟨ξ2, (e0
−w)t⟩=
 0
−h
(e−h
+ ξ2)(θ −t) · w(θ) dθ = ⟨τ(t)ξ2, w⟩,
(5.50)
and similarly
⟨ξ1, (e0
−φ1)t⟩= ⟨τ(t)ξ1, φ1⟩.
(5.51)
Combining (5.49), (5.50), and (5.51) we obtain
⟨(ξ0, ξ1, ξ2), (x(t), xt, (e0
−w)t)⟩
= ⟨(z(t), ¯L(e0
+z)t + τ(t)ξ1, ¯B⊤(e0
+z)t + τ(t)ξ2), (φ0, φ1, w)⟩
and henceforth the adjoint semigroup expression (5.41).
We now turn to the characterization of the inﬁnitesimal generator ˜A∗. It
is readily seen that ˜S∗has a special structure
˜S∗(t)ξ = (S∗(t)(ξ0, ξ1), ¯B(e0
+z)t + τ(t)ξ2),
(5.52)
˜S∗(t)ξ = (S∗(t)(ξ0, ξ1), 0) +

(0, 0), ¯B(e0
+z)t + τ(t)ξ2
,
(5.53)
where the ﬁrst two components that are related to S∗can be separated from
the third component. So if ξ ∈D( ˜A∗), then (ξ0, ξ1) ∈D(A∗) and by Theo-
rem 4.5
∃ψ ∈D(A⊤),
(that is ψ = (ψ(0), ψ), ψ ∈W 1,q(−h, 0; Rn))
and
∃ζ ∈W 1,q(−h, 0; Rn)
such that
(ξ0, ξ1) = F ∗ψ + (0, ζ) = (ψ(0), ¯L⊤ψ + ζ)
(5.54)
and
lim
t↘0
d
dtS∗(t)(ξ0, ξ1) = A∗[F ∗ψ + (0, ζ)] = F ∗A⊤ψ + (ζ(0), −Dζ)
= (L⊤ψ + ζ(0), ¯L⊤Dψ −Dζ).
(5.55)
But in view of the decomposition (5.54), equation (5.42) reduces to
⎧
⎪
⎨
⎪
⎩
˙z(t)
= (L⊤e0
+z)(t) + (e−h
+ [¯L⊤ψ + ζ])(−t),
= (L⊤z)(t) + (e−h
+ ζ)(−t),
z(0)
= ψ(0),
z0 = ψ.
(5.56)

5 State space theory of linear control systems
291
By Theorem 4.1 (ii), for all T > 0, z ∈W 1,q(−h, T ; Rn) and by Lemma 4.1 (ii)
z• ∈C

0, T ; W 1,q(−h, 0; Rn)

∩C1
0, T ; Lq(−h, 0; Rn)

(5.57)
and
Dt ¯B⊤zt = ¯B⊤Dθ zt.
(5.58)
Now go back to the third component of ˜S∗(t)ξ in (5.53)
F(t) = ¯B⊤(e0
+z)t + τ(t)ξ2 = ¯B⊤zt + τ(t)ξ2 −¯B⊤(e0
−ψ)t
and recall that by Lemma 4.4 applied to B⊤
¯B⊤(e0
−ψ)t = τ(t)[ ¯B⊤ψ].
Then
F(t) = ¯B⊤zt + τ(t)λ,
λ = ξ2 −¯B⊤ψ.
From (5.57) t 	→¯B⊤zt belong to C1
0, T ; Lq(−h, 0; Rn)

and from (5.57)–
(5.58),
¯B⊤z• ∈C

0, T ; W 1,q(−h, 0; Rn)

∩C1
0, T ; Lq(−h, 0; Rn)

(5.59)
and
d
dt
¯B⊤zt = ¯B⊤Dθ zt.
(5.60)
So the assumption ξ ∈D( ˜A∗) implies that
τ(·)λ ∈C1
0, T ; Lq(−h, 0; Rm)

.
(5.61)
But this is the shift operator for the function e−h
0 λ in Lq(−∞, 0; Rm) and by
the analog of Lemma 4.1 (ii)
e−h
0 λ ∈W 1,q(−∞, 0; Rm) =⇒λ ∈W 1,q(−h, 0; Rm),
λ(−h) = 0,
and
d
dtτ(t)λ = −τ(t)Dθ λ
(5.62)
as in the proof of Theorem 4.5. Finally
dF
dt (t) = ¯BT Dθ zt −τ(t)Dθ λ.
(5.63)
To summarize we have shown that if ξ = (ξ0, ξ1, ξ2) ∈D( ˜A∗)
⎧
⎪
⎨
⎪
⎩
ξ0 = ψ(0),
ξ1 = ¯L⊤ψ + ζ,
ξ2 = ¯B⊤ψ + λ,
ψ, ζ ∈W 1,q(−h, 0; Rn),
ζ(−h) = 0,
λ ∈W 1,q(−h, 0; Rm),
λ(−h) = 0.
(5.64)

292
II-4 State Space Theory of Diﬀerential Systems With Delays
Moreover from (5.55), (5.60), and (5.62) as t goes to zero
˜A∗ξ = (L⊤ψ + ζ(0), ¯L⊤Dψ −Dζ, ¯B⊤Dψ −Dλ).
(5.65)
Conversely if (5.64) is veriﬁed, then by Theorem 4.5 (i), (ξ0, ξ1) ∈D(A∗)
and S∗(·)(ξ0, ξ1) belongs to C1(0, T ; M q). Moreover (5.42) reduces to (5.56)
and we have (5.57) and (5.59). Finally condition (5.64) on λ implies that
e−h
+ λ ∈W 1,q(−∞, 0; Rm) and we have (5.61). Therefore ˜S(·)ξ belong to
C1(0, T ; Zq) and necessarily ξ ∈D( ˜A∗).
To complete the proof we must show that expression (5.65) for ˜A∗ξ is
independent of the representation of ξ in terms of (ψ, ζ, λ). By linearity this
is equivalent to show that
ξ = (ψ(0), ¯L⊤ψ + ζ, ¯B⊤ψ + λ) = 0
=⇒˜A∗ξ = (L⊤ψ + ζ(0), ¯L⊤Dψ −Dζ, ¯B⊤Dψ −Dλ) = 0.
So ξ = 0 is equivalent to
ψ(0) = 0,
ψ ∈W 1,q(−h, 0; Rn),
¯L⊤ψ + ζ = 0,
ζ ∈W 1,q(−h, 0; Rn),
ζ(−h) = 0,
¯B⊤ψ + λ = 0,
λ ∈W 1,q(−h, 0; Rm),
λ(−h) = 0.
By Lemma 4.6 extended to L⊤and B⊤
0 = D[¯L⊤ψ + ζ] = −¯L⊤Dψ + Dζ,
0 = D[ ¯B⊤ψ + λ] = −¯B⊤Dψ + Dλ,
0 = [¯Lψ + ζ](0) = L⊤ψ + ζ(0),
and ˜A∗ξ = 0.
⊓⊔
5.2.2 The nonhomogeneous case with control
Go back to system (5.13), which can be rewritten as

˙x = Le0
+x + Be0
+u + Le0
−φ1 + Be0
−w + f,
x(0) = φ0.
(5.66)
Associate with the solution x of (5.66) the extended state
˜x(t) = (x(t), xt, ut),
t ≥0.
(5.67)
The objective is to obtain the analog of Theorem 5.1 for ˜x(t). Here { ˜S∗(t)}
will play the same role as {S⊤(t)} and it will be necessary to construct a
control operator ˆB⊤: D( ˜A∗) →Rm, which will play for ˜x(t) the same role as
B⊤: D(A⊤) →Rm for ˆx(t). The results are presented in the next theorem
followed by a short discussion and the proof.

5 State space theory of linear control systems
293
Theorem 5.3. Let the real number p, 1 < p < ∞, and its conjugate q, q−1 +
p−1 = 1, be given. Denote by x in W 1,p
loc (0, ∞; Rn) the solution of system (5.66)
for (φ0, φ1, w) ∈Zp, f ∈Lp
loc(0, ∞; Rn), and u ∈Lp
loc(0, ∞; Rm), and let ˜x(t)
be the extended state constructed from x in (5.67):
(i) For all ρ in D( ˜A∗) and t ≥0
⟨ρ, ˜x(t)⟩= ⟨˜S∗(t)ρ, (φ0, φ1, w)⟩+
 t
0
[ ˆB⊤˜S∗(t −s)ρ · u(s)
+ ˜S∗(t −s)ρ · (f(s), 0, 0)] ds,
(5.68)
where
ρ = (ψ(0), ¯L⊤ψ + ζ, ¯B⊤ψ + λ) 	→ˆB⊤ρ = B⊤ψ + λ(0)
: D( ˜A∗) →Rm
(5.69)
is a well-deﬁned continuous linear operator on D( ˜A∗) endowed with its
graph norm topology (cf. (5.43) in Theorem 5.2 for the representation of
elements of D( ˜A∗)).
(ii) The extended state ˜x(t) is the solution of the weak equation: For all t > 0
and all ρ ∈D( ˜A∗)
⎧
⎨
⎩
d
dt⟨ρ, ˜x(t)⟩= ⟨˜A∗ρ, ˜x(t)⟩+ ˆB⊤ρ · u(t) + ρ0 · f(t),
˜x(0) = (φ0, φ1, w).
(5.70)
(iii) For each T > 0, the state ˜x is the unique solution in
V(0, T ; Zp, D( ˜A∗)′)
=

y ∈C(0, T ; Zp): d
dt˜ı∗y ∈Lp(0, T ; D( ˜A∗)′)

(5.71)
to the following equation in D( ˜A∗)′:
⎧
⎪
⎪
⎨
⎪
⎪
⎩
d
dt˜ı∗˜x(t)
= ( ˜A∗)∗˜x(t) + ( ˆB⊤)∗u(t) + ˜ı∗(f(t), 0, 0),
t > 0,
˜x(0) = (φ0, φ1, w),
(5.72)
where ˜ı: D( ˜A∗) →M q is the canonical injection of D( ˜A∗) into M q and
˜ı∗, ( ˜A∗)∗and ( ˆB⊤)∗are the dual maps of the continuous linear operators
˜ı: D( ˜A∗) →Zq,
˜A∗: D( ˜A∗) →Zq,
ˆB⊤: D( ˜A∗) →Rm
(5.73)
deﬁned on D( ˜A∗) endowed with its graph norm topology.

294
II-4 State Space Theory of Diﬀerential Systems With Delays
Remark 5.3. The key element of this theorem is the introduction of the op-
erator ˆB⊤, which makes it possible to complete and extend the work of
A. Ichikawa [1, 3] who considered operators B of the form
Bw =
N

i=0
Biw(θi) +
 0
−h
B01(θ)w(θ) dθ,
(5.74)
where N ≥0 is an integer,
−∞< −h = θN < · · · > θi+1 < θi < · · · > θ0 = 0
(5.75)
are real, B0, B1, . . . , BN and B01(θ) are n × m matrices, and the element of
B01(θ) are Lq-functions on I(−h, 0). In that case
ˆB⊤ρ =
N

i=0
B⊤
i ψ(θi) +
 0
−h
B01(θ)⊤ψ(θ) dθ + λ(0)
with the notation of (5.69). But the following limit exists:
lim
α↗0[ ¯B⊤ψ](α) = B⊤ψ −B⊤
0 ψ(0)
and
ˆB⊤ρ = B⊤
0 ψ(0) + [ ¯B⊤ψ + λ](0),
(5.76)
where we recognize the third component ¯B⊤ψ + λ of
ρ = (ψ(0), ¯L⊤ψ + ζ, ¯B⊤ψ + λ)
evaluated at 0. This construction and identity (5.76) can be extended to op-
erators B of the form
Bw = B0w(0) +
 0
−h
dθ β1w(θ),
w ∈K(−h, 0; Rm),
(5.77)
where B0 is an n × m matrix and β1 is an n × m matrix of regular Borel
measures such that
lim
α↗0
 0
α
dθ |β1| = 0.
(5.78)
However this is a special case that does not generalize to arbitrary continuous
linear maps B : K(−h, 0; Rm) →Rn because there is a priori no reason that
the third component ¯B⊤ψ+λ of ρ ∈D( ˜A∗) be deﬁned at the point 0. The dif-
ﬁculties encountered in the construction (5.69) of operator ˆB⊤are analogous
to the ones encountered in the characterization of the inﬁnitesimal generators
A∗of the adjoint semigroups {S∗(t)} in Theorem 4.5.
⊓⊔

5 State space theory of linear control systems
295
Proof of Theorem 5.3.
(i) Let ρ ∈D( ˜A∗) be of the form
⎧
⎪
⎨
⎪
⎩
ρ = (ψ(0), ¯L⊤ψ + ζ, ¯B⊤ψ + λ),
ζ, ψ ∈W 1,q(−h, 0; Rn),
ζ(−h) = 0,
λ ∈W 1,q(−h, 0; Rm),
λ(−h) = 0,
(5.79)
and let z be the solution of the system

˙z(t) = (L⊤e0
+z)(t) + (L⊤e0
−ψ)(t) + (e−h
+ ζ)(−t),
z(0) = ψ(0).
(5.80)
This is the ﬁnal form of (5.42) when ρ ∈D( ˜A∗) is given by (5.79). By con-
struction and the characterization of D( ˜A∗) given in Theorem 5.2
˜S∗(t)ρ = (z(t), ¯L⊤zt + τ(t)ζ, ¯B⊤zt + τ(t)λ).
(5.81)
In view of the linearity of system (5.66) and of the deﬁnition of the semi-
group { ˜S(t)} in Theorem 5.2, it is suﬃcient to establish identity (5.68) for
(φ0, φ1, w) = 0. The ﬁrst step is the use of the integration by parts formula
(4.35) in Lemma 4.2 for T = t
 t
0
z(t −s) · [ ˙x −Le0
+x](s) ds + z(t) · x(0)
=
 t
0
[ ˙z −L⊤e0
+z](s) · x(t −s) ds + z(0) · x(t),
(5.82)
where x is the solution of
˙x = Le0
+x + Be0
+u + f,
x(0) = 0.
(5.83)
Moreover as in (4.35)
 t
0
z(t −s) · (Be0
+u)(s) ds =
 t
0
(B⊤e0
+z)(t −s) · u(s) ds.
(5.84)
Substract (5.84) from (5.82) and use (5.80) and (5.83)
 t
0

z(t −s) · f(s) ds
=
 t
0
[(L⊤e0
−ψ)(s) + (e−h
+ ζ)(−s)] · x(t −s) + (B⊤e0
−ψ)(s) · u(t −s)
	
ds
−
 t
0
(B⊤z)(s) · u(t −s) ds + ψ(0) · x(t).
But this can be reorganized and written as follows:

296
II-4 State Space Theory of Diﬀerential Systems With Delays
ψ(0) · x(t) + ⟨¯L⊤ψ + ζ, (e0
+x)t⟩+ ⟨¯B⊤ψ, (e0
+u)t⟩
=
 t
0
[z(t −s) · f(s) + B⊤zt−s · u(s)] ds.
(5.85)
To complete the identity we add the term
⟨λ, (e0
+u)t⟩=
 0
−h
λ(α) · (e0
+u)(t + α) dα
=
 t
t−h
(e−h
+ λ)(s −t) · (e0
+u)(s) ds
=
 t
0
(e−h
+ λ)(s −t) · u(s) ds.
(5.86)
Therefore from (5.79), (5.85), and (5.86)
⟨ρ, ˜x(t)⟩=
 t
0
{z(t −s) · f(s) + [B⊤zt−s + (e−h
+ λ)(s −t)] · u(s)} ds.
But ρ ∈D( ˜A∗) and hence ˜S∗(t −s)ρ ∈D( ˜A∗) has the representation given in
(5.81) with t −s in place of t.
To complete the proof of the theorem, we need the following lemma, which
will be proved at the end.
⊓⊔
Lemma 5.1. The linear operator
ρ = (ψ(0), ¯L⊤ψ + ζ, ¯B⊤ψ + λ) 	→ˆB⊤ρ = B⊤ψ + λ(0): D( ˜A∗) →Rm (5.87)
is well deﬁned, linear, and continuous on D( ˜A∗) endowed with its graph norm
topology (cf. (5.43) in Theorem 5.2 for the representation of the elements of
D( ˜A∗).
So by deﬁnition of ˆB⊤
ˆB⊤˜S∗(t −s)ρ = ˆB⊤(z(t −s), ¯L⊤zt−s + τ(t −s)ζ, ¯B⊤zt−s + τ(t −s)λ)
= B⊤zt−s + [τ(t −s)λ](0) = B⊤zt−s + (e−h
+ λ)(s −t)
and
⟨ρ, ˜x(t)⟩=
 t
0
[⟨˜S∗(t −s)ρ, (f(s), 0, 0)⟩+ ˆB⊤˜S∗(t −s)ρ · u(s)] ds.
(ii) and (iii) now follow by the same techniques as in the proof of Theorem 5.1
(ii) and (iii).
Remark 5.4. Notice that ˆB⊤ρ does not depend on the second component of
ρ. So it readily extends to

(ψ(0), ρ2, ¯B⊤ψ + λ):
λ ∈W 1.p(−h, 0; Rm),
λ(−h) = 0
ψ ∈W 1.p(−h, 0; Rn),
ρ2 ∈Lq(−h, 0; Rn)

.
⊓⊔

6 State space theory of linear control systems with observation
297
Proof of Lemma 5.1.
The proof is very similar to the proof of Lemma 4.6
with B⊤in place of L. By linearity the map (5.87) is well deﬁned if
(ψ(0), ¯L⊤ψ + ζ, ¯B⊤ψ + λ) = 0 =⇒B⊤ψ + λ(0) = 0.
But from Lemma 4.6 (ii)
ψ ∈W 1,q(−h, 0; Rn)
and
ψ(0) = 0 =⇒( ¯B⊤ψ)(0) = B⊤ψ.
As a result ¯B⊤ψ + λ ∈W 1,q(−h, 0; Rn) and
¯B⊤ψ + λ = 0 =⇒0 = [ ¯B⊤ψ + λ](0) = B⊤ψ + λ(0).
For the continuity we use the same techniques as in Lemma 4.6 (i):
B⊤ψ + λ(0) = B⊤˜ψ−α + λ(α) −
 0
α
[ ¯B⊤Dθ ψ −Dθ λ](θ) dθ
and use the fact that
B⊤˜ψ−α + λ(α) = (B⊤e0
−ψ)(−α) + λ(α) +

B⊤e0
+ψ(0)

(−α)
= [ ¯B⊤ψ + λ](α) +

B⊤e0
+ψ(0)

(−α).
Then by making the same estimates as in Lemma 4.6 (i), there exists c > 0
such that
|B⊤ψ + λ(0)| ≤c[|ψ(0)| + ∥¯B⊤ψ + λ∥q + ∥¯B⊤Dψ −Dλ∥q].
But for ρ = (ψ(0), ¯L⊤ψ + ζ, ¯B⊤ψ + λ) in D( ˜A∗) (cf. (5.43) in Theo-
rem 5.2 (iii),
∥ρ∥D( ˜
A∗) = ∥ρ∥q + ∥˜A∗ρ∥q = |ψ(0)| + ∥¯L⊤ψ + ζ∥q + ∥¯B⊤ψ + λ∥q
+ |L⊤ψ + ζ(0)| + ∥¯L⊤Dψ −Dζ∥q + ∥¯B⊤Dψ −Dλ∥q
and hence
| ˆB⊤ρ| = |B⊤ψ + λ(0)| ≤c ∥ρ∥D( ˜
A∗).
⊓⊔
6 State space theory of linear control systems with
observation
In this last section we add to the linear control system

˙x = Lxt + B0ut + f(t),
t > 0,
(x(0), x0, u0) = (φ0, φ1, w)
(6.1)
the observation equation

298
II-4 State Space Theory of Diﬀerential Systems With Delays
y(t) = Cxt + B1ut,
t > 0
(6.2)
for the continuous linear maps

L: K(−h, 0; Rn) →Rn,
B0 : K(−h, 0; Rm) →Rn,
C : K(−h, 0; Rn) →Rk,
B1 : K(−h, 0; Rm) →Rk
(6.3)
and integers n ≥1, m ≥1, and k ≥1.
The objective is to construct a state, a state equation, and an unbounded
observation operator for the state. As in §3.5 two parallel approaches will be
considered. The ﬁrst one is based on the use of the extended state introduced
in §5.2
˜x(t) = (x(t), xt, ut),
t ≥0,
(6.4)
and the observation operator
(φ(0), φ, w) 	→Cφ + B1w: D( ˜A) →Rk.
(6.5)
The second one is due to D. Salamon [2] who added a third component to
the structural state
ˆx(t) = (x(t), ¯Lxt + ¯B0ut, ¯Cxt + ¯B1ut),
(6.6)
and an appropriate observation operator can be deﬁned on the domain of
the natural semigroup associated with the state ˆx(t) and u(t). This will be
referred to as the extended structural state in §6.2.
In this section we give a presentation that naturally ﬁts within the general
framework adopted in this chapter. Moreover we show that the two states are
not unrelated. They turn out to be also intertwined with respect to a general
structural operator F, which contains all operators that characterize the delay
structure of the system and the control and observation operators. This result
is new, but the essential ideas and constructions were already contained in
the general model developed by M. C. Delfour and J. Karrakchou [1,
2]. All this can obviously be extended to more general delay structures.
Formally in the ﬁrst case the natural semigroup will be the extended semi-
group { ˜S(t)} associated with L and B0. The extended state ˜x will be the
solution of the state equation
⎧
⎨
⎩
d
dt˜ı∗˜x(t) = ( ˜A∗)∗˜x(t) + ( ˆB⊤)∗u(t),
˜x(0) = (φ0, φ1, w),
(6.7)
and the observation will be given by
y(t) = ˜C˜x(t),
(6.8)
where

6 State space theory of linear control systems with observation
299
ˆB⊤: D( ˜A∗) →Rm,
ˆB⊤(ψ(0), ¯L⊤ψ + ζ, ¯B⊤
0 ψ + λ) = B⊤
0 ψ + λ(0),
(6.9)
˜C : D( ˜A) →Rk,
˜C(φ(0), φ, w) = Cφ + B1w.
(6.10)
In the second case the semigroup will be the adjoint { ˜S⊤∗(t)} of the ex-
tended semigroup { ˜S(t)⊤} associated with L⊤and C⊤. It is the analog of the
extended semigroup associated with L and B0 in Terminology 2. The extended
structural state ˆx will be the solution of
⎧
⎨
⎩
d
dt˜j∗ˆx(t) = ( ˜A⊤)∗ˆx(t) + ( ˜B⊤)∗u(t),
ˆx(0) = (φ(0), ¯Lφ1 + ¯B0w, ¯Cφ1 + ¯B1w).
(6.11)
When B1 = 0
y(t) = ˆCˆx(t),
(6.12)
where
˜B⊤: D( ˜A⊤) →Rm,
˜B⊤(ψ(0), ψ, v) = B⊤
0 ψ + B⊤
1 v,
(6.13)
ˆC : D( ˜A⊤∗) →Rk,
ˆC(φ(0), ¯Lφ + ζ, ¯Cφ + λ) = Cφ + λ(0).
(6.14)
When B1 ̸= 0, the situation is slightly more complex and y(t) depends on ˆx(t)
and u(t).
Of course everything will be made more precise as we proceed, but we
would like to already draw some general conclusions to guide the reader in
the subsequent developments of this theory. In §3.5 the use of the structural
state ˆx(t) with two components had a clear advantage over the extended
state ˜x(t) with three components. Here both states have three components.
The operators ˆB⊤and ˜C have the same structure as the operators ˆC and ˜B⊤.
So there is a ﬂavor of duality or intertwining between the two approaches, but
no clear technical advantage in choosing one over the other. In both cases
we come up with a classical evolution system and unbounded control and
observation operators . . . and most important no more delays.
However from the System Theoretic viewpoint, the ﬁrst approach leads to
the observation y(t) as a linear function of the extended state ˜x(t), whereas
in the second approach it is a linear function of

ˆx(t), u(t)

. In other words in
the ﬁrst approach the state contains the control segment ut and the control
does not explicitly appear in the observation equation, whereas in the second
approach the observation equation explicitly contains the control u(t) at time
t.
6.1 The extended state
This case is easy because we have already studied ˜x in Theorem 5.3 for the
system

300
II-4 State Space Theory of Diﬀerential Systems With Delays

˙x = Le0
+x + B0e0
+u + Le0
−φ1 + B0e0
−w + f,
x(0) = φ0.
(6.15)
The observation (6.2) makes sense because
y = Cx + B1u,
(6.16)
where, as in the case of L and L in Theorem 3.3, for all T > 0, we can associate
with C the linear map
C : Lp(−h, T ; Rn) →Lp(0, T ; Rk)
(Cx)(t) = Cxt,
∀x ∈Cc(]−h, T [; Rn), t ∈[0, T ],
(6.17)
and linear maps B0 and B1 are similarly associated with B0 and B1, respec-
tively. The operator
(φ0, φ1, w) 	→˜C(φ0, φ1, w) = Cφ1 + B1w
: Rn × K(−h, 0; Rn) × K(−h, 0; Rm) →Rk
(6.18)
is linear and continuous. For initial conditions of the form
(φ(0), φ, w),
φ ∈K(−h, 0; Rn),
w ∈K(−h, 0; Rm)
(6.19)
and
f ∈Lp
loc(0, ∞; Rn),
u ∈Cloc(0, ∞; Rm), u(0) = w(0),
(6.20)
we have a continuous observation y
y(t) = ˜C˜x(t) = Cxt + B1ut,
t ≥0.
(6.21)
The operator ˜C introduced in (6.18) can also be viewed as a continuous linear
operator
ξ = (φ(0), φ, w) 	→˜Cξ = Cφ + B1w: D( ˜A) →Rk,
(6.22)
when D( ˜A) is endowed with the graph norm topology. When u(t) = 0, t ≥0,
and

φ(0), φ, w)

∈D( ˜A),
˜C˜x(t) = Cxt + B1(e0
−w)t.
(6.23)
6.2 The extended structural state
This section is not necessarily more diﬃcult than the other ones, but certainly
heavier in notation. Yet the constructions and the proofs are very much the
same as in the previous sections. Always as in Theorem 3.3 where we have
associated with L, a matrix of regular Borel measures η, a new operator L⊤,
maps L and L⊤, and structural operators ¯L and ¯L⊤, we associate with C, B0,
B1, the same items

6 State space theory of linear control systems with observation
301
⎧
⎪
⎪
⎨
⎪
⎪
⎩
L
L
η
L⊤
L⊤
¯L
¯L⊤
C
C
γ
C⊤
C⊤
¯C
¯C⊤
B0 B0 β0 B⊤
0
B⊤
0
¯B0
¯B⊤
0
B1 B1 β1 B⊤
1
B⊤
1
¯B1
¯B⊤
1 .
(6.24)
Of course the previous results for L will apply to C, B0, and B1. Recall that
the structural state was deﬁned for the linear control system

˙x(t) = (Le0
+x)(t) + (B0e0
+u)(t) + (e−h
+ ξ1)(−t) + f(t),
t > 0,
x(0) = ξ0
(6.25)
and that (6.15) is a special case of (6.25). The new state will be an extension
of (5.20) by adding a third component.
Deﬁnition 6.1. The extended structural state is denoted by ˆx(t) and deﬁned
as
ˆx(t) = (x(t), ¯L(e0
+x)t + ¯B0(e0
+u)t + τ(t)ξ1, ¯C(e0
+x)t
+ ¯B1(e0
+u)t + τ(t)ξ2).
(6.26)
⊓⊔
The observation equation will be for t ≥0
y(t) = (Ce0
+x)(t) + (B1e0
+u)(t) + (e−h
+ ξ2)(−t).
(6.27)
When
ξ = (ξ0, ξ1, ξ2) = (φ0, ¯Lφ1 + ¯B0w, ¯Cφ1 + ¯B1w),
(6.28)
we exactly recover system (6.15)–(6.16).
6.2.1 The extended semigroup { ˜S⊤(t)} associated
Given the real number q, 1 ≤q < ∞, consider the system

˙z(t) = (L⊤z)(t) + (C⊤e0
−v)(t),
t ≥0,
(z(0), z0, v0) = (ψ0, ψ1, v) ∈Zq
(6.29)
and deﬁne the family of linear transformations
˜S⊤(t)(ψ0, ψ1, v) = (z(t), zt, (e0
−v)t),
t ≥0.
(6.30)
It is clear from §5.2.1 that { ˜S⊤(t)} associated with L⊤and C⊤has the same
structure as { ˜S(t)} deﬁned in (5.38) and that Theorem 5.2 applies with obvi-
ous substitutions. The family { ˜S⊤(t)} is a strongly continuous semigroup of
continuous linear transformations of Zq = Rn×Lq(−h, 0; Rn)×Lq(−h, 0; Rk).
Its inﬁnitesimal generator ˜A⊤is given by

302
II-4 State Space Theory of Diﬀerential Systems With Delays
˜A⊤(ψ(0), ψ, v) = (L⊤ψ + C⊤v, Dψ, Dv),
(6.31)
where
D( ˜A⊤) =
⎧
⎪
⎨
⎪
⎩
(ψ(0), ψ, v) ∈Zq :
(ψ(0), ψ) ∈D(A⊤)
v ∈W 1,q(−h, 0; Rk)
v(−h) = 0
⎫
⎪
⎬
⎪
⎭
.
(6.32)
The canonical injection of D( ˜A⊤) into Zq will be denoted by ˜j. For p, 1 <
p < ∞, and its conjugate q, q−1 + p−1 = 1, the adjoint semigroup { ˜S⊤∗(t)} is
characterized by
˜S⊤∗(t)(ξ0, ξ1, ξ2) = (x(t), ¯L(e0
+x)t + τ(t)ξ1, ¯C(e0
+x)t + τ(t)ξ2),
(6.33)
where x is the solution of system

˙x(t) = (Le0
+x)(t) + (e−h
+ ξ1)(−t),
t > 0,
x(0) = ξ0.
(6.34)
6.2.2 The case B1 = 0
When B1 = 0, the observation equation (6.27) reduces to
y(t) = (Ce0
+x)(t) + (e−h
+ ξ2)(−t),
t ≥0.
(6.35)
For ξ ∈D(A⊤∗),
⎧
⎪
⎨
⎪
⎩
ξ = (φ(0), ¯Lφ + ζ, ¯Cφ + λ),
φ, ζ ∈W 1,p(−h, 0; Rn),
ζ(−h) = 0,
λ ∈W 1,p(−h, 0; Rk),
λ(−h) = 0,
(6.36)
and (6.25) reduces to

˙x(t) = Lxt + (B0e0
+u)(t) + (e−h
+ ζ) (−t) + f(t),
(x(0), x0) = (φ(0), φ),
(6.37)
y(t) = Cxt + (e−h
+ λ)(−t),
t > 0.
(6.38)
Always for ξ ∈D( ˜A⊤∗) of the form (6.36)
ˆx(t) = (x(t), ¯Lxt + ¯B0(e0
+u)t + τ(t)ζ, ¯Cxt + τ(t)λ)
(6.39)
and because e−h
+ λ ∈W 1,p(−∞, 0; Rk), [τ(t)λ](0) = (e−h
+ λ)(−t) and
y(t) = Cxt + (e−h
+ λ)(−t)
= Cxt + [τ(t)λ](0) = ˆCˆx(t).
(6.40)

6 State space theory of linear control systems with observation
303
We recognize the operator
ξ = (φ(0), ¯Lφ + ζ, ¯Cφ + λ) 	→ˆCξ = Cφ + λ(0): D( ˜A⊤∗) →Rk,
(6.41)
which is the analog of the operator ˆB⊤in Lemma 5.1 with L and C in place of
L⊤and B⊤. So it is linear and continuous when D( ˜A⊤∗) is endowed with the
graph norm topology. However recall from Remark 5.4 that it is independent of
the second component of ξ, which can be arbitrarily chosen in L2(−h, 0; RN).
6.2.3 The case B1 ̸= 0
Comparing (6.26) and (6.33) we recognize that they coincide for u(t) = 0,
t ≥0. The next theorem is the analog of Theorem 5.1.
Theorem 6.1. Let the real number p, 1 < p < ∞, and its conjugate q, q−1 +
p−1 = 1, be given. Assume that x in W 1,p
loc (0, ∞; Rn) is the solution of system
(6.25) for ξ ∈M p, f ∈Lp
loc(0, ∞; Rn) and u ∈Lp
loc(0, ∞; Rm), and let ˆx(t) be
the extended structural state constructed from x in (6.26):
(i) For all ρ in D( ˜A⊤) and all t ≥0
⟨ρ, ˆx(t)⟩= ⟨˜S⊤(t)ρ, ξ⟩+
 t
0
[ ˜B⊤˜S⊤(t −r)ρ · u(r)
+ ⟨˜S⊤(t −r)ρ, (f(r), 0, 0)⟩] dr,
(6.42)
where
ρ = (ψ(0), ψv) 	→˜B⊤ρ = B⊤
0 ψ + B⊤
1 v: D( ˜A⊤) →Rm
(6.43)
is linear and continuous on D( ˜A⊤) endowed with its graph norm topology.
(ii) The state ˆx(t) is the solution of the weak equation
⎧
⎨
⎩
d
dt⟨ρ, ˆx(t)⟩= ⟨˜A⊤ρ, ˆx(t)⟩+ ˜B⊤ρ · u(t) + ρ0 · f(t),
t > 0,
ˆx(0) = ξ,
∀ρ ∈D( ˜A⊤).
(6.44)
(iii) For each T > 0, the state ˆx is the unique solution in
V(0, T ; M p; D( ˜A⊤)′)
=

z ∈C(0, T ; M p): d
dt˜j∗z ∈Lp(0, T ; D( ˜A⊤)′)

(6.45)
to the following equation in D( ˜A⊤)′:
⎧
⎨
⎩
d
dt˜j∗ˆx(t) = ( ˜A⊤)∗ˆx(t) + ( ˜B⊤)∗u(t) + ˜j∗(f(t), 0, 0),
t > 0,
ˆx(0) = ξ,
(6.46)

304
II-4 State Space Theory of Diﬀerential Systems With Delays
where ˜j is the canonical injection of D( ˜A⊤) into Zq and ˜j∗, and ( ˜A⊤)∗
and ( ˜B⊤)∗are the dual maps of the continuous linear operators
˜j: D( ˜A⊤) →Zq,
˜A⊤: D( ˜A⊤) →Zq,
˜B⊤: D( ˜A⊤) →Rm
(6.47)
deﬁned on D( ˜A⊤) endowed with the graph norm topology.
Remark 6.1. The operator ˜B⊤is similar to the operator ˜C introduced in
(6.22). It is the restriction to D( ˜A⊤) of the continuous linear operator
(ψ0, ψ1, v) 	→˜B⊤(ψ0, ψ1, v) = B⊤
0 ψ1 + B⊤
1 v
: Rn × K(−h, 0; Rn) × K(−h, 0; Rk) →Rm.
(6.48)
⊓⊔
The proof of this theorem will be given at the end of this section. Before
stating the next theorem, which describes the structure of the observation
equation for smooth data, it is useful to consider the following simple example.
Example 6.1. Consider the linear system without delays
˙x(t) = Ax(t) + B00u(t),
x(0) = x0, t ≥0,
(6.49)
y(t) = C0x(t) + B10u(t),
t ≥0.
(6.50)
Then
Lφ = Aφ(0),
B0w = B00w(0),
Cφ = C0φ(0),
B1w = B10w(0),
and it is readily seen that
¯L = 0,
¯B0 = 0,
¯C = 0,
and
¯B1 = 0.
For initial conditions of the form
ξ = (φ(0), ¯Lφ + ¯B0w, ¯Cφ + ¯B1w) = (φ(0), 0, 0)
the state reduces to
ˆx(t) = (x(t), 0, 0)
and the observation y(t) given by (6.50) necessitates a knowledge of both ˆx(t)
and u(t).
⊓⊔
This example illustrates how the structural operators construct a state ˆx(t)
that is “minimal” in the sense that any artiﬁcial delay structure is removed
or ﬁltered out. It also clearly indicates that the observation y(t) at time t ≥0
necessitates a knowledge of both ˆx(t) and u(t). This fact is well known for
ﬁnite dimensional systems of the type (6.49)–(6.50) without delays.
Consider initial conditions of the form

6 State space theory of linear control systems with observation
305
ξ + (0, ¯B0w, ¯B1w),
(6.51)
where
ξ ∈D( ˜A⊤∗)
and
w ∈W 1,p(−h, 0; Rm);
(6.52)
that is, there exist
φ, ζ ∈W 1,p(−h, 0; Rn),
ζ(−h) = 0,
λ ∈W 1,p(−h, 0; Rk),
λ(−h) = 0
(6.53)
such that
ξ = (φ(0), ¯Lφ + ζ, ¯Cφ + λ).
(6.54)
For initial data of the form (6.51) and
f ∈Lp
loc(0, ∞; Rn),
u ∈W 1,p
loc (0, ∞; Rm),
u(0) = w(0),
(6.55)
equation (6.25) reduces to

˙x(t) = Lxt + B0ut + (e−h
+ ζ)(−t) + f(t),
t ≥0,
(x(0), x0, u0) = (φ(0), φ, w),
(6.56)
y(t) = Cxt + B1ut + (e−h
+ λ)(−t),
t ≥0,
(6.57)
and the state ˆx(t) is given by
(x(t), ¯Lxt + ¯B0ut + τ(t)ζ, ¯Cxt + ¯B1ut + τ(t)λ).
(6.58)
By assumptions (6.51) to (6.55) the term
(x(t), ¯Lxt + τ(t)ζ, ¯Cxt + τ(t)λ) ∈D( ˜A⊤∗),
(6.59)
and by deﬁnition (6.41) of the observation operator ˆC for all t ≥0
ˆC(x(t), ¯Lxt + τ(t)ζ, ¯Cxt + τ(t)λ) = Cxt + (e−h
+ λ)(−t).
(6.60)
So to relate y(t) and ˆx(t) we concentrate on the term
(0, ¯B0ut, ¯B1ut).
(6.61)
As shown in Example 6.1 this term is not rich enough to completely recon-
struct the observation y(t) and deal with nondelayed terms in the variable u.
To fully recover the observation y(t) we need the variable u(t). The situation
is analogous to the one in (6.60) for the variable x, which makes sense because
of the presence of the term x(t). The technical result behind this construction
is Lemma 4.6 applied to B1: For all w ∈W 1,p (−h, 0; Rm), the map
(w(0), ¯B1w) 	→B1w
(6.62)

306
II-4 State Space Theory of Diﬀerential Systems With Delays
is well deﬁned, linear, and
∃c > 0,
such that ∀w ∈W 1,p(−h, 0; Rm),
|B1w| ≤c[|w(0)| + ∥¯B1w∥p + ∥¯B1Dw∥p].
(6.63)
So Lemma 4.6 has the following natural extension here.
Lemma 6.1. Let p, 1 ≤p < ∞, be a real number. For all ξ in D( ˜A⊤∗) of the
form (6.54)–(6.53) and w in W 1,p(−h, 0; Rm) the map

ξ + (0, ¯B0w, ¯B1w), w(0)

=

(φ(0), ¯Lφ + ¯B0w + ζ, ¯Cφ + ¯B1w + λ), w(0)

	→ˆCext

ξ + (0, ¯B0w, ¯B1w), w(0)

= Cφ + B1w + λ(0)
(6.64)
is well deﬁned, linear, and there exists c > 0 such that for all ξ in D( ˜A⊤∗)
and w in W 1,p(−h, 0; Rm)
| ˆCext(ξ+(0, ¯B0w, ¯B1w), w(0)| ≤c[|φ(0)|+∥¯Lφ+ ¯B0w+ζ∥p+∥¯Cφ+ ¯B1w+λ∥p
+ |Lφ + B0w + ζ(0)| + ∥¯L Dφ + ¯B0Dw −Dζ∥p
+ ∥¯CDφ + ¯B1Dw −Dλ∥p + |w(0)|].
(6.65)
Proof. To verify that the map (6.64) is well deﬁned, it is suﬃcient to show
that

(φ(0), ¯Lφ + ¯B0w + ζ, ¯Cφ + ¯B1w + λ, w(0)

= 0
implies that
Cφ + B1w + λ(0) = 0.
In particular
φ(0) =0 and φ ∈W 1,p(−h, 0; Rn) =⇒(¯Lφ)(0) = Lφ and ( ¯Cφ)(0) = Cφ,
w(0) =0 and w∈W 1,p(−h, 0; Rm) =⇒( ¯B0w)(0)=B0w and ( ¯B1w)(0)=B1w.
As a result
¯Cφ + ¯B1w + λ = 0 =⇒0 = [ ¯Cφ + ¯B1w + λ](0) = Cφ + B1w + λ(0).
Inequality (6.65) follows by the same techniques as the ones used in the proof
of Lemma 4.6.
⊓⊔
The direct consequence of Lemma 6.1 is that for initial conditions
ξ ∈D( ˜A⊤∗)
and
w ∈W 1,p(−h, 0; Rm)
and functions f and u verifying (6.55),
ˆCext

ˆx(t), u(t)

= Cxt + B1ut + (e−h
+ λ)(−t) = y(t)
(6.66)
as can be easily veriﬁed. This is the extension of (6.40) to the case where
B1 ̸= 0 and the equivalent of (6.21) for the state ˜x(t).

6 State space theory of linear control systems with observation
307
Proof of Theorem 6.1.
(i) By linearity of system (6.25) and the deﬁnition
(6.33) of the adjoint semigroup { ˜S⊤∗(t)}, it is suﬃcient to establish identity
(6.42) for ξ = 0. We use the integration by parts formula (4.35) in Lemma 4.2
for T = t
⎧
⎪
⎪
⎨
⎪
⎪
⎩
 t
0
z(t −s) · [ ˙x −Le0
+x](s) ds + z(t) · x(0),
=
 t
0
[ ˙z −L⊤e0
+z](s) · x(t −s) ds + z(0) · x(t),
(6.67)
where x is the solution of

˙x(t) = [Le0
+x + B0e0
+u](t),
t ≥0,
x(0) = 0,
(6.68)
y(t) = [Ce0
+x + B1e0
+u](t),
t ≥0,
(6.69)
and z is the solution of

˙z(t) = [L⊤z + C⊤(e0
−v)](t),
t ≥0,
(z(0), z0, (e0
−v)0) = (ψ(0), ψ, v) ∈D( ˜A⊤).
(6.70)
The substitution of (6.68) and (6.70) in (6.67) yields
 t
0
z(t−s) · [B0e0
+u](s) ds =
 t
0
[L⊤e0
−ψ+C⊤e0
−v](s) · x(t −s) ds+ψ(0) · x(t)
=
 0
−h
[¯L⊤ψ + ¯C⊤v](α) · (e0
+x)t(α) dα + ψ(0) · x(t),
and because ¯L⊤= ¯L∗and ¯C⊤= ¯C∗
⟨(ψ(0), ψ, v), (x(t), ¯L(e0
+x)t, ¯C(e0
+x)t)⟩=
 t
0
(B⊤
0 e0
+z)(t −s) · u(s) ds. (6.71)
But
B⊤
0 e0
+z = B⊤
0 z −B⊤
0 e0
−z = B⊤
0 z −B⊤
0 e0
−ψ
and
 t
0
(B⊤
0 e0
−ψ)(t −s) · u(s) ds =
 t
0
(B⊤
0 e0
−ψ)(s) · u(t −s) ds
=
 0
−h
( ¯B⊤
0 ψ)(α) · (e0
+u)(t + α) dα.
Substituting in (6.71) we obtain
⟨(ψ(0), ψ, v), (x(t), ¯L(e0
+x)t + ¯B0(e0
+u)t, ¯C(e0
+x)t)⟩
=
 t
0
(B⊤
0 z)(t −s) · u(s) ds =
 t
0
B⊤
0 zt−s · u(s) ds.
(6.72)

308
II-4 State Space Theory of Diﬀerential Systems With Delays
The missing term to obtain the state ˆx(t) in (6.72) is
⟨v, ¯B1(e0
+u)t⟩= ⟨¯B⊤
1 v, (e0
+u)t⟩=
 0
−h
(B⊤
1 e0
−v)(α) · (e0
+u)(t + α) dα
=
 t
t−h
(B⊤
1 e0
−v)(t −s) · (e0
+u)(s) ds
=
 t
t−h
B⊤
1 (e0
−v)t−s · (e0
+u)(s) ds
=
 t
0
B⊤
1 (e0
−v)t−s · u(s) ds.
Finally
⟨(ψ(0), ψ, v), (x(t), ¯L(e0
+x)t + ¯B0(e0
+u)t, ¯C(e0
+x)t + ¯B1(e0
+u)t)⟩
=
 t
0
[B⊤
0 zt−s + B⊤
1 (e0
−v)t−s] · u(s) ds.
(6.73)
But this is precisely identity (6.42) for ξ = 0 and ˜B⊤given by (6.43).
(ii) and (iii) now follow by the same techniques as in the proof of Theo-
rem 5.1 (ii) and (iii).
⊓⊔
6.3 Intertwining property of the two extended states
In fact the two states are not completely unrelated: They are intertwined with
respect to the structural operator
F : Rn ×L2(−h, 0; Rn)×L2(−h, 0; Rm) →Rn ×L2(−h, 0; Rn)×L2(−h, 0; Rk)
deﬁned by the following matrix of operators:
F =
⎡
⎣
I
0
0
0 ¯L
¯B0
0
¯C
¯B1
⎤
⎦.
(6.74)
Theorem 6.2.
(i) Given initial conditions ξ of the form
ξ = (φ0, ¯Lφ1 + ¯B0w, ¯Cφ1 + ¯B1w) = F(φ0, φ1, w)
(6.75)
for
(φ0, φ1, w) ∈Rn × L2(−h, 0; Rn) × L2(−h, 0; Rm),
(6.76)
then for all u in L2
loc(0, ∞; Rm)
ˆx(t; F(φ0, φ1, w), u) = F ˜x(t; (φ0, φ1, w), u),
(6.77)
where ˆx is given by (6.26) and ˜x by (6.4).

6 State space theory of linear control systems with observation
309
(ii) The following intertwining identities hold:
˜S⊤∗(t)F = F ˜S(t),
∀t ≥0,
(6.78)
ˆCF = ˜C
on D( ˜A),
(6.79)
ˆB⊤F ⊤= ˜B⊤
on D( ˜A⊤),
(6.80)
where
F ∗= F ⊤=
⎡
⎣
I
0
0
0 ¯L⊤¯C⊤
0 ¯B⊤
0
¯B⊤
1
⎤
⎦.
(6.81)
Proof. (i) By deﬁnition (6.26)
ˆx(t) = (x(t), ¯L(e0
+x)t + ¯B0(e0
+u)t + τ(t)ξ1, ¯C(e0
+x)t + ¯B1(e0
+u)t + τ(t)ξ2).
By Lemma 4.4
τ(t)[¯Lφ1 + ¯B0w] = ¯L(e0
−φ1)t + ¯B0(e0
−w)t
and
τ(t)[ ¯Cφ1 + ¯B1w] = ¯C(e0
−φ1)t + ¯B1(e0
−w)t.
But
xt = (e0
+x)t + (e0
−φ1)t,
ut = (e0
+u)t + (e0
−w)t
and ﬁnally
ˆx(t) = (x(t), ¯Lxt + ¯B0ut, ¯Cxt + ¯B1ut)
= F(x(t), xt, ut) = F ˜x(t).
(ii) This is a special case of (i) with u(t) = 0, t ≥0. From (6.33)
˜S⊤∗(t)ξ = (x(t), ¯L(e0
+x)t + τ(t)ξ1, ¯C(e0
+x)t + τ(t)ξ2)
= (x(t), ¯Lxt + τ(t) ¯B0w, ¯Cxt + τ(t) ¯B1w)
= (x(t), ¯Lxt + ¯B0(e0
−w)t, ¯Cxt + ¯B1(e0
−w)t)
= F(x(t), xt, (e0
−w)t) = F ˜S(t)(φ0, φ1, w)
by deﬁnition (5.38) of the extended semigroup of Ichikawa.
To prove identity (6.79) we make use of the intertwining property (6.78)
and Lemma 4.5, which says that the corresponding inﬁnitesimal generators
are also intertwined: For all (φ(0), φ, w) ∈D( ˜A),
FD( ˜A) ⊂D( ˜A⊤∗),
˜A⊤∗F(φ(0), φ, w) = F ˜A(φ(0), φ, w).
Hence
ˆCF(φ(0), φ, w) = ˆC(φ(0), ¯Lφ + ¯B0w, ¯Cφ + ¯B1w)
= Cφ + ( ¯B1w)(0).

310
II-4 State Space Theory of Diﬀerential Systems With Delays
Recall that an element of D( ˜A) is of the form (φ(0), φ, w), where
w ∈W 1,2(−h, 0; Rm),
w(0) = 0.
By Lemma 4.6 applied to B1 instead of L
( ¯B1w)(0) = B1w.
Finally for all (φ(0), φ, w) ∈D( ˜A)
ˆCF(φ(0), φ, w) = Cφ + B1w = ˜C(φ(0), φ, w)
for the operator ˜C deﬁned by (6.10):
ˆCF = ˜C
on D( ˜A).
For the control operator we use the dual intertwining identity
F ⊤˜S⊤(t) = ˜S∗(t)F ⊤
and for all (ψ(0), ψ, v) ∈D( ˜A⊤)
F ⊤D( ˜A⊤) ⊂D( ˜A∗),
˜A∗F ⊤= F ⊤˜A⊤.
By repeating the previous arguments
ˆB⊤F ⊤(ψ(0), ψ, v) = ˆB⊤(ψ(0), ¯L⊤ψ + ¯C⊤v, ¯B⊤
0 ψ + ¯B⊤
1 v)
= B⊤
0 ψ + ( ¯B⊤
1 v)(0)
and as
(ψ(0), ψ, v) ∈D( ˜A⊤) =⇒v ∈W 1,2(−h, 0; Rk)
and
v(0) = 0,
we conclude from identity (4.79) in Lemma 4.6 applied to B⊤
1 that
( ¯B⊤
1 v)(0) = B⊤
1 v
and
ˆB⊤F ⊤= B⊤
0 ψ + B⊤
1 v = ˜B⊤
on D( ˜A⊤).
⊓⊔
Remark 6.2. The intertwining properties of Theorem 6.2 are new in the gen-
eral case. Special cases were already considered in the case without delays in
the observation (cf. M. C. Delfour [15]) and in the case without delays in
the control and observation variables (cf. M. C. Delfour, E. B. Lee, and
A. Manitius [1]). In the latter case, it was shown that the solutions of the
Riccati equations for the states ˜x and ˆx are also intertwined.
⊓⊔
Acknowledgments
The authors are grateful to A. Manitius for very pertinent and useful com-
ments and references on several aspects of the material presented in this chap-
ter.

Part III
Qualitative Properties of Inﬁnite Dimensional
Linear Control Dynamical Systems

1
Controllability and Observability for a Class of
Inﬁnite Dimensional Systems
1 Introduction
In §2.1 and §2.2 of Chapter 1 of Part I, we have discussed criteria for control-
lability, and observability for ﬁnite dimensional systems and have also shown
that when the system is controllable we can transfer the state z0 ∈H at time
t0 to the state z1 ∈H at time t1 using minimum energy controls. These results
were obtained by considering the controllability operator
LT : L2(0, T ; U) →H
: u 	−→
 T
0
e(T −s)ABu(s) ds,
and its adjoint
L∗
T : H →L2(0, T ; U)
: y 	−→B∗e(T −·)A∗y,
and studying the relation between the ranges and null spaces of these two
operators and by showing that controllability is equivalent to invertibility of
LT L∗
T . As we have remarked (see Remark 2.1, Chapter 1 of Part I) in some
sense the same ideas can be used to obtain characterizations of controllability
when the spaces U and X are inﬁnite dimensional Hilbert spaces, but at
the expense of using much elaborate technical machinery. In this chapter we
discuss questions of controllability for parabolic and second-order hyperbolic
equations, the plate equation, and Maxwell’s equations. We ﬁrst deal with
controllability of the abstract linear dynamical system

z′(t) = Az(t) + Bu(t),
z(0) = 0,
(1.1)
evolving in a Hilbert space H and where A is the inﬁnitesimal generator of
a strongly continuous semi-group etA on H and B ∈L(U, H), where U is a

314
III-1 Controllability and Observability
Hilbert space, the control space. In an inﬁnite dimensional setting there are
at least two concepts of controllability: approximate controllability and exact
controllability. These concepts are introduced in Deﬁnitions 2.1 and 2.2, and
criteria of approximate controllability and exact controllability for the above
abstract linear dynamical system are presented in part (b) of Proposition 3.1
in §3.1 and in (3.4) of §3.2. Approximate controllability and null controlla-
bility for parabolic equations (distributed, boundary, and pointwise control),
are studied in §5 by converting it to the abstract model (1.1) in appropriate
spaces. The exact controllability problem for hyperbolic equations (Neumann
and Dirichlet boundary control), the plate equation, and Maxwell’s equation
is studied by ﬁrst studying exact controllability of the model (1.1) by spe-
cializing A to the case of skew-symmetric operators (§6 and §7) and then
converting the various partial diﬀerential equations to this abstract model in
appropriate spaces where the assumptions of the abstract results are veri-
ﬁed. The method of proof for the abstract model makes essential use of the
eigenvalue–eigenfunction structure of an appropriate partial diﬀerential op-
erator related to the generator A and hence assumptions need to be made
on the control operator B in relation to these eigenfunctions (assumptions
that clearly have to be veriﬁed in concrete cases), so that lower estimates on
boundary operators can be obtained.
For partial diﬀerential equations, the study of controllability has some in-
trinsic interest. Indeed, it appears that the most natural approach to the exact
controllability problem for hyperbolic equations is obtained by using the the-
ory of pseudo-diﬀerential operators and micro-local analysis as developed by
Melrose and Sj¨ostrand (see C. Bardos, G. Lebeau, and J. Rauch [1]). For
our purposes the theory of controllability (in particular exact controllability in
suitably deﬁned spaces) provides a criterion for stabilizability, a requirement
for inﬁnite time quadratic control problems to be well posed (see Part V). The
interplay between the choice of state and the control spaces in the formulation
of inﬁnite time quadratic control problems, regularity properties for solutions
of partial diﬀerential equations and verifying exact controllability properties
in these spaces (leading to stabilizability and uniform stabilizability) is subtle
and has only been worked out in the eighties (see, in particular the recent
work of I. Lasiecka and R. Triggiani [11], J. L. Lions [5], and Chapter 2
and 3 of Part V). A general methodological approach to these issues is via the
Hilbert Uniqueness Method (HUM) of J. L. Lions.
The scheme of development adopted in this chapter, although not necessar-
ily leading to the sharpest results, has considerable appeal (especially from an
engineering viewpoint) and provides a bridge between the ﬁnite dimensional
and inﬁnite dimensional theory. It uses the controllability operator and mini-
mum energy transfer between states in an essential way. It is therefore of some
interest in placing it both in historical context and also in the context of most
recent developments in the subject. For this purpose we cite three sources on
which the remainder of this section is based (see I. Lasiecka and R. Trig-
giani [11], J. L. Lions [5], and D. L. Russell [1]). For ﬁnite dimensional sys-

1 Introduction
315
tems, the concept of controllability ﬁrst arose as a technical condition related
to normality and abnormality of Calculus of Variations problems and in ob-
taining structural results on the reachable set (see J. P. LaSalle [1]). In the
same way, the concept of approximate controllability and its characterization
ﬁrst arose in the work of Yu. V. Egorov [1] again as a technical require-
ment in the study of time-optimal control problems for parabolic systems.
An important early contribution to controllability is H. O. Fattorini [1].
There is a close relation between approximate controllability and observabil-
ity (see Deﬁnition 3.1 and Proposition 3.1). In concrete situations involving
parabolic partial diﬀerential equations, approximate controllability is veriﬁed
via observability using uniqueness and unique continuation theorems for the
adjoint system (for an early use of this idea see J. L. Lions [3] who used a
uniqueness theorem of S. Mizohata [1] to achieve the desired objective). Our
results in §5, especially Theorem 5.2 and Lemma 5.1, implicitly use uniqueness
and unique continuation ideas. For other work related to §5, see H. O. Fat-
torini [1, 2, 4]; R. C. MacCamy, V. J. Mizel, and T. I. Seidman [1];
E. J. P. G. Schmidt and N. Weck [1]; and §3.3, Chapter 2, Part V.
A deeper issue in the context of parabolic systems is the concept of null
controllability (exact controllability to the origin), that is, the ability of trans-
ferring an arbitrary state z0 ∈D(A) to the zero state in time T > 0 us-
ing admissible controls. This question and its observability counterpart have
been investigated by several authors (see, for example, H. O. Fattorini
and D. L. Russell [1], V. J. Mizel and T. I. Seidman [1, 2], T. I. Sei-
dman [1], and W. Littman [1]). Most of the results known here are for
parabolic equations in one-dimension (the work of Seidman being an excep-
tion) and are based on deep results in Harmonic Analysis (excepting the work
of Littman, which uses diﬀerent ideas) related to the independence of ex-
ponentials e−λkt, where λk are eigenvalues of appropriate partial diﬀerential
operators entering the dynamical system. The work in the early seventies
started in one-dimension, but both V. J. Mizel and T. I. Seidman [1, 2]
and H. O. Fattorini and D. L. Russell [1] proved the result for balls, and
it was well known that this gave the result for general domains by extension
to a ball containing the domain and then restriction to the domain and its
boundary. Null controllability is not discussed here, but we conjecture that
the methods that we adopt would lead to results for parabolic equations.1
The focus of this chapter is on exact controllability (which is often a mean
of verifying stabilizability) for second-order hyperbolic equations, plate equa-
tions, and Maxwell’s equations. The reason for this is that there is an essential
diﬀerence between the problem of stabilizability (uniform stabilizability) for
parabolic systems and for hyperbolic systems. In the parabolic case, we are
generally dealing with a semigroup that is analytic with a compact resolvent
1 Since 1993, this topic has received more attention. The reader is referred to the
book of A. V. Fursikov and O. Yu. Imanuvilov [1] in 1996 that studies null
controllability via Carleman estimates.

316
III-1 Controllability and Observability
and hence A (the generator) has only ﬁnitely many unstable eigenvalues with
ﬁnite multiplicity. In this case we have to check whether the projection of A
on the unstable subspace is controllable, and this can be done via the Pole-
Assignment Theorem (see Theorem 2.4, Chapter 1, Part I). These issues are
dealt with in §3.3 of Chapter 1 of Part V and §3.3 of Chapter 2 of Part V.
Concrete situations, such as the heat equation with control exercised through
Dirichlet or Neumann boundary conditions, can be handled using these ideas
(see Remark 3.1 of Chapter 2 of Part V for bibliographical references).
For second-order hyperbolic equations (and for plate and Maxwell’s equa-
tion) the stabilizability question is far more diﬃcult because these systems
have an inﬁnite dimensional unstable (marginally stable) part. It is here that
the study of exact controllability is most important and this has been carried
out in J. L. Lions [4, 5] and I. Lasiecka and R. Triggiani [9, 10, 13] and
others (see the bibliographical references cited in the above-mentioned works).
We remark here that the key to these results are estimates on boundary op-
erators.
Let us examine this in the context of the wave equation with Dirichlet
control (see Example 2.1 in Chapter 3 of Part V).
In this case we denote by Λ the operator
Λh = −∆h,
D(Λ) = H2(Ω) ∩H1
0(Ω),
and by D the Dirichlet map
Dv = y ⇐⇒{∆y = 0
in Ω
and
y∂Ω= v}.
and the state space H = L2(Ω) × H−1(Ω) and the control space U = L2(∂Ω).
Then the abstract model is
A =

0 I
−Λ 0
	
,
Bu =

0
ΛDu
	
.
We may then compute
B∗
y1
y2
	
= D∗y2 = −∂
∂ν Λ−1y2
since D∗Λ = −∂
∂ν .
Hence
B∗eA∗t

y1
y2
	
= ∂ϕ
∂ν (t),
(y1, y2) ∈L2(Ω) × H−1(Ω),
and ϕ is the solution of the homogeneous equation
⎧
⎪
⎨
⎪
⎩
ϕtt = ∆ϕ
in Ω× ]0, T [ϕ = 0
in ∂Ω× ]0, T [
ϕ(·, 0) = ϕ0
ϕt(·, 0) = ϕ1
in Ω.
Now

2 Main deﬁnitions
317
ϕ0 = Λ−1y2 ∈D(Λ1/2) = H1
0(Ω),
ϕ1 = y1 ∈L2(Ω).
The two relevant estimates are (see Example 4.3 in Chapter 3 of Part IV and
Example 2.1 in Chapter 3 of Part V):
(i)

Σ

∂ϕ
∂ν

2
dΣ ≤c(T )∥{ϕ0, ϕ1}∥2
H1
0(Ω)×L2(Ω),
(ii)

Σ

∂ϕ
∂ν

2
dΣ ≥c(T )∥{ϕ0, ϕ1}∥2
H1
0(Ω)×L2(Ω)
at least for large T , where Σ = ∂Ω× ]0, T [. By virtue of the fact that
B∗eA∗t

y1
y2
	
= ∂ϕ
∂ν (t),
the inequality (ii) above is equivalent to the invertibility of the controllability
operator
 T
0
etABB∗etA∗dt,
and this is the criterion for exact controllability.
In the case of control through Neumann Boundary conditions it is diﬃcult
to identify the space on which the above estimates on boundary operators
hold.
For a discussion of the above examples in the abstract setting developed
in this chapter, see Theorem 6.1 and §8.1 and §8.2.
2 Main deﬁnitions
2.1 Notation
Let H be a complex Hilbert space, identiﬁed with its dual. More speciﬁcally
H is the complexiﬁed version of a real Hilbert space, also denoted by H to
save the notation. This means that an element of H complexiﬁed is of the
form
h = h1 + ih2,
with h1, h2 ∈H.
The norm in H real is denoted by | · | and the scalar product by (·, ·). In H
(complexiﬁed) the scalar product of h and h′ is denoted by (h, ¯h′) where ¯h′ is
the conjugate of h. The norm of h is |h| = (h, ¯h)1/2.
We consider a linear operator A: D(A) →H and B ∈L(U; H), where U
is also identiﬁed with its dual. We call H the state space and U the control
space. The norm and scalar product in U is denoted in the same way as for
H, and possibly with a subscript U when there is a risk of confusion. A and
B are deﬁned when H and U use scalars that are real valued, and are real
valued. They are extended to the complexiﬁed versions of H and U by setting

318
III-1 Controllability and Observability
Ah = Ah1 + iAh2
and similar formula for B. In the sequel, the complexiﬁed versions are needed
because of eigenvalues (in fact only starting with §6). Note that the operator
A∗h = A∗h1 + iA∗h2
satisﬁes the condition
∀h, h′,
(A∗h, ¯h′) = (h, ¯
Ah′)
and is the adjoint of A considered as a linear operator in H complexiﬁed.
We consider the linear dynamical system

z′(t) = Az(t) + Bv(t),
t ∈(0, T ),
z(0) = 0.
(2.1)
We assume that A is the inﬁnitesimal generator of a strongly continuous
semigroup eAt in H. The control function v(·) belongs to L2(0, T ; U) (U is
the ordinary real version). The solution of (2.1) is denoted by z(t; v) and
naturally
z(t; v) =
 t
0
e(t−s)ABv(s) ds.
(2.2)
2.2 Deﬁnitions
We set
FT = {z(T ; v): ∀v ∈L2(0, T ; U)}.
We can provide FT with the structure of a Hilbert space thanks to the follow-
ing.
Proposition 2.1. The space FT can be structured as a Hilbert space, with
continuous injection in H.
Proof. Consider the closed subspace of L2(0, T ; U) made of v(·) such that
z(T ; v) = 0. Let NT be this subspace, and L2(0, T ; U)/NT be the Hilbert
quotient space of L2(0, T ; U) by NT . If u(·)◦is an element of L2(0, T ; U)/NT,
and u(·) is a representative of u(·)◦, then the quotient norm is given by
|u◦|2 = min{J(v): v ∈L2(0, T ; U), v −u ∈NT },
(2.3)
where we have set
J(v) =
 T
0
|v(t)|2 dt.
If ˆu realizes the minimum in (2.3), then the scalar product in
L2(0, T ; U)/NT

2 Main deﬁnitions
319
is deﬁned by
(u◦, v◦) =
 T
0

ˆu(t), ˆv(t)

dt.
We deﬁne the map ΨT from L2(0, T ; U)/NT to FT by setting
ΨT (u◦) = z(T ; u)
and ΨT is clearly a bijection. We deﬁne on FT a structure of Hilbert space by
setting

(ξ, η)

= (Ψ −1
T ξ, Ψ −1
T η).
(2.4)
By construction ΨT is an isometry. It remains to show that the injection of
FT into H is continuous. We show that
∥ξ∥
|ξ| ≥c,
∀ξ ∈FT .
(2.5)
Indeed if (2.5) does not hold, then there exists a sequence ξn in FT , such that
|ξn| = 1, and ξn →0 in FT . Let u◦
n = Ψ −1
T ξn, and ˆun be the representative of
u◦
n of minimum norm; then ˆun →0 in L2(0, T ; U) and ξn = z(T ; ˆun); therefore
ξn →0 in H, which contradicts the fact that |ξn| = 1. The desired result has
been proved.
⊓⊔
Let Ψ ∗
T be the adjoint of ΨT . It is an isometry from F ′
T to L2(0, T ; U)/NT.
Therefore the map ΨT Ψ ∗
T is an isometry from F ′
T to FT . If ξ ∈FT , deﬁning
ζ∗∈F ′
T by the equation
ΨT Ψ ∗
T ζ∗= ξ
(2.6)
and considering the class Ψ ∗
T ζ∗∈L2(0, T ; U)/NT, then any representative
of this class will realize ξ. The canonical isomorphism from FT into F ′
T is
(ΨT Ψ ∗
T )−1 = (Ψ ∗
T )−1(ΨT )−1. The norm product in F ′
T is ∥ζ∗∥= |Ψ ∗
T ζ∗|.
Consider now the operator
ΓT =
 T
0
etABB∗etA∗dt,
which belongs to L(H; H). We call ΓT the controllability operator. We can
decompose ΓT as a product of an operator with its transpose. For that, intro-
duce π the injection of FT in H and π∗its transpose. Note that π∗H is dense
in F ′
T . We can then give an interpretation of ψ∗
T π∗. We have
ψ∗
T π∗h(t) = equivalence class of B∗e(T −t)A∗h.
(2.7)
Let u0 ∈L2(0, T ; U)/NT. We have
(ψ∗
T π∗h, u0) =

h, z(T ; ˆu)

,
where ˆu is the representative of minimum norm of u0. Hence,

320
III-1 Controllability and Observability
(ψ∗
T π∗h, u0) =
 T
0

B∗e(T −t)A∗h, ˆu(t)

dt.
Therefore what remains to be done is to show that B∗e(T −t)A∗h minimizes
the norm of elements that belong to its equivalence class. According to (2.3)
we look for the element ˆv that minimizes
J(v) =
 T
0
|v(t)|2 dt
among all v such that
 T
0
e(T −t)AB(v(t) −B∗e(T −t)A∗h) dt = 0.
(2.8)
The optimal ˆv satisﬁes (2.8) and
 T
0

ˆv(t), v(t)

dt = 0
for all v such that
 T
0
e(T −t)ABv(t) dt = 0.
It is clear that ˆv(t) = B∗e(T −t)A∗h satisﬁes this necessary condition, and thus
the property (2.7) has been demonstrated.
From (2.7) we deduce at once the decomposition
ΓT = πψT ψ∗
T π∗.
(2.9)
We now introduce the following.
Deﬁnition 2.1. The pair (A, B) is approximately controllable at time T when-
ever FT is densely embedded in H.
⊓⊔
If the pair (A, B) is approximately controllable, then the map π∗is injec-
tive. In that case we can identify an element h of H, with its image π∗h, and
π∗H with H. In that case we have
FT ⊂H ⊂F ′
T
(2.10)
with continuous and dense embedding of each space in the following one.
Moreover from (2.9) we have
ΓT = ψT ψ∗
T
(2.11)
and ΓT extends as an isometry from F ′
T to FT . If h ∈H, then
∥h∥F ′
T = (ΓT h, h)1/2
(2.12)
and F ′
T appears as the completion of H with respect to the norm (2.12).
Suppose that the pair (A, B) is approximately controllable; then D(A) and
FT are two dense subspaces of H. An important question is the comparison
between these two spaces. This justiﬁes the following deﬁnition.

2 Main deﬁnitions
321
Deﬁnition 2.2. The pair (A, B) is exactly controllable at time T whenever
D(A) ⊂FT
with continuous and dense injection.
⊓⊔
One may wonder why we do not use as a deﬁnition of exact controllability
at time T , the property FT = H. The reason is that as soon as H is inﬁnite
dimensional, this property is not generally veriﬁed, except for special families
such as the hyperbolic systems. To partially support this assertion, we can
state the following
Proposition 2.2. R. Triggiani[ 1] If H is inﬁnite dimensional and B is
compact, the property FT = H cannot hold for ﬁnite time T .
Proof. The operator B is compact, there exists a non unique sequence of
orthonormal vectors wk of U and positive numbers αk, which tends monoton-
ically to 0 as k tends to inﬁnity, such that
w′
k = 1
αk
Bwk is an orthonormal sequence of H,
(2.13)
Bv =
∞

k=0
αk(v, wk)w′
k.
(2.14)
Let {vm} be a sequence in L2(0, T ; U), which tends to 0 weakly and zm(T ) =
z(T ; vm). We shall show that
zm(T ) →0 strongly in H.
(2.15)
If (2.15) is proved, then deﬁning F n
T = subset of FT obtained with v restricted
to the ball of center 0 and radius n, it is clear that F n
T has a compact closure
in H. As H is inﬁnite dimensional, ¯
F n
T has an empty interior. It is nowhere
dense.
Now
FT ⊂
∞
/
n=1
¯
F n
T .
Therefore, according to Baire Category theorem, FT cannot coincide with H.
Similarly
¯
/
T >0
FT =
¯
/
m>0
Fm ⊂
∞
/
n,m=1
¯
F n
m
cannot coincide either with H.
It remains to prove (2.15). Using (2.13) one has
zm(T ) =
∞

k=0
αk
 T
0
e(T −t)Aw′
k(vm(t), wk) dt.
(2.16)

322
III-1 Controllability and Observability
Let
zm,N(T ) =
N

k=0
αk
 T
0
e(T −t)Aw′
k(vm(t), wk) dt.
As vm tends to 0 weakly in L2(0, T ; U), its norm remains bounded. Then it
is easy to check that
|zm(T ) −zm,N(T )| ≤CαN
independently of m. As αN tends to 0 as N tends to inﬁnity, to prove (2.15) it
is suﬃcient to prove that zm,N(T ) tends to 0 as m tends to inﬁnity, for ﬁxed
N. For this it is enough to prove that
 T
0
e(T −t)Ahfm(t) dt →0
in H
whenever fm tends to 0 weakly in L2(0, T ) and h is a ﬁxed element of H. This
is an easy consequence of the fact that t 	→e(T −t)Ah is a continuous function
on [0, T ] with values in H. The proof has been completed.
⊓⊔
In the sequel, we shall be interested in obtaining criteria for approximate
and exact controllability.
3 Criteria for approximate and exact controllability
3.1 Criterion for approximate controllability
We introduce the following deﬁnition.
Deﬁnition 3.1. Given T > 0, we say that the pair (A∗, B∗) is observable on
(0, T ) when
B∗etA∗h = 0
on (0, T ) =⇒h = 0.
(3.1)
⊓⊔
We can state the following result.
Proposition 3.1. The following statements are equivalent:
(a) The pair A∗, B∗is observable on (0, T ),
(b) (ΓT h, h)1/2 is a norm on H,
(c) the pair (A, B) is approximately controllable at Γ.
Proof. If the observability property holds, then (ΓT h, h)≥0 and (ΓT h, h)=0
implies B∗etA∗h=0 on (0, T ); therefore, h=0.
If (ΓT h, h)1/2 is a norm on H, then if h satisﬁes B∗etA∗h = 0 on (0, T ),
(ΓT h, h) = 0 hence h = 0. Hence (a) and (b) are equivalent.
Now approximate controllability at T is equivalent to

3 Criteria for approximate and exact controllability
323
(z(T ; v), h) = 0,
∀v =⇒h = 0.
(3.2)
But
(z(T ; v), h) =
0 T
0
e(T −t)ABv(t) dt, h
1
=
 T
0
(v(t), B∗e(T −t)A∗h) dt.
Therefore
∀v,
(z(T ; v), h) = 0 ⇐⇒B∗e(T −t)A∗h = 0
and (3.2) is thus the same thing as observability. Hence (a) and (c).
⊓⊔
3.2 Criteria for exact controllability and continuous observability
From our deﬁnition of exact controllability we deduce the sequence
D(A) ⊂FT ⊂H ⊂F ′
T ⊂

D(A)
′,
(3.3)
each space being densely and continuously embedded in the following one.
From (2.12) we deduce
(ΓT h, h) ≥cT ∥h∥2
(D(A))′
(3.4)
for any h ∈H.
Conversely, if (3.4) holds, then (3.3) holds. Indeed necessarily
F ′
T ⊂

D(A)
′.
(3.5)
To prove (3.5) consider the vector space F ′
T ∩

D(A)
′ provided with the norm
∥ξ∥F ′
T ∩(D(A))′ = max{∥ξ∥F ′
T , ∥ξ∥(D(A))′}.
Let ξ ∈F ′
T and consider a sequence {ξn} in H, ξn
=⇒ξ in F ′
T . Note
that {ξn} is a Cauchy sequence in F ′
T and from (3.4) a Cauchy sequence
in

D(A)
′. Therefore ξn is a Cauchy sequence in F ′
T ∩

D(A)
′. It follows
that ξ ∈F ′
T ∩

D(A)
′, in particular to

D(A)
′. Hence (3.5). From (3.4) the
injection is clearly continuous. As H is dense in

D(A)
′, F ′
T is also densely
embedded in

D(A)
′. By duality we deduce (3.3). Therefore, the estimate
(3.4) is a criterion of exact controllability.
Remark 3.1. There is an analog of Proposition 3.1 for exact controllability. For
this purpose, deﬁne the dual system as follows: First introduce the observation
equation for the system (2.1)
y(t) = Cz(t),
where C ∈L(H; Y ), and the observation space Y is a Hilbert space identiﬁed
with its dual. Then introduce the dual system (backward equation)

324
III-1 Controllability and Observability

p′(t) = −A∗p(t) + C∗y(t),
p(T ) = 0,
and the dual observation equation
u(t) = B∗p(t).
We then say that the pair (A, C) is continuously observable if and only if the
pair (A∗, C∗) is exactly controllable at time 0 (starting at time T ). Therefore
we obtain a criterion analogous to (3.4) for continuous observability.
⊓⊔
3.3 Approximation
Let h be an element of H. There exists a sequence {ξn} of elements of FT
which converges to h in H. Solving the equation
ΨT Ψ ∗
T ζ∗
n = ξn
deﬁnes a unique ζ∗
n in F ′
T . Any control of the equivalence class Ψ ∗
T ζ∗
n realizes
ξn. In this way we can construct a sequence of controls whose corresponding
state at time T is as close as possible to h. But this procedure is not very
constructive. A constructive approach is obtained in the following way: Deﬁne
Jε(v) = J(v) + 1
ε|z(T ; v) −h|2
(3.6)
with
z(T ; v) =
 T
0
e(T −t)ABv(t) dt
(3.7)
and minimize Jε(v) over all v ∈L2(0, T ; U). This problem has a unique solu-
tion uε. We have
|z(T ; uε) −h|2 ≤|z(T ; v) −h|2 + εJ(v),
∀v,
and thus
lim sup
ε→0
|z(T ; uε) −h|2 ≤inf |z(T ; v) −h|2 = 0.
Therefore the sequence z(T ; uε) belongs to FT and converges to h in H as ε
tends to 0. Now we may write the necessary conditions of optimality for uε.
It is easy to check that if pε is the adjoint state deﬁned by
−p′
ε = A∗pε,
pε(T ) = 1
ε(xε(T ) −h),
(3.8)
then
uε(t) = −B∗pε(t) = −B∗e(T −t)A∗pε(T ) = −Ψ ∗
T pε(T ).
(3.9)
Therefore

4 Finite dimensional control space
325
z(T ; uε) = −ΓT pε(T ).
(3.10)
Since pε(T ) ∈H and not just to F ′
T , we have an even better result than that
explained at the beginning of the paragraph.
If h belongs to FT , then we can say something more. Indeed let ˆu be such
that
ΨT ˆu = h
and ˆu has minimum L2 norm. As
J(uε) ≤J(ˆu),
(3.11)
it follows that uε remains bounded in L2. From the formula (3.9), it follows
that pε(T ) remains in a bounded subset of F ′
T . We pick a subsequence, still
denoted by pε(T ), which converges weakly to ζ∗in F ′
T . Going to the limit in
(3.10) yields
h = −ΓT ζ∗.
(3.12)
From (3.11) and the minimality of ˆu, it is easy to check that uε tends to ˆu in
L2(0, T ; U). From (3.9) we deduce also
ˆu = −Ψ ∗
T ζ∗,
(3.13)
and thus we have an approximation procedure for the element ζ∗.
We refer to A. Bel Fekih [1] for related topics.
4 Finite dimensional control space
We shall consider in this section the case when U is ﬁnite dimensional, U =
Rm, and make precise the conditions of controllability (clearly approximate
and exact controllability coincide in that case).
4.1 Finite dimensional case
Assume here that H = Rn and U = Rm, A ∈L(Rn; Rn), B ∈L(Rm; Rn). We
have the following classical result due to R. E. Kalman [1], for which see
Theorems 2.1 and 2.2, Part I, Chapter 1.
Proposition 4.1. The pair (A, B) is controllable at any time T > 0 if and
only if the matrix [B AB . . . An−1B] ∈L(Rnm; Rn) has rank n.
Consider now the case of diagonalizable operators
A =
J

j=1
λjPj,
(4.1)
where λj is complex valued, Pj is a projector, satisfying

326
III-1 Controllability and Observability
Pj = P ∗
j ,
PjPk = δjk,
J

j=1
Pj = I.
Note that we have complexiﬁed the set up, so P ∗
j means
(P ∗
j h, k) = (h, ¯
Pjk) = (Pjh, k);
hence
A∗=
J

j=1
¯λjPj.
We then state the following proposition.
Proposition 4.2. Assume (4.1); then the pair (A, B) is controllable at any
time T > 0, if and only if for any j, the operator PjBB∗Pj has full rank, in
the subspace PjH.
Proof.
• The condition is necessary.
Suppose otherwise that there exists some h, with Pjh ̸= 0, and B∗Pjh = 0;
then
⎡
⎢⎢⎢⎢⎣
B∗
B∗A∗
. . .
. . .
B∗A∗n−1
⎤
⎥⎥⎥⎥⎦
Pjh =
⎡
⎢⎢⎢⎢⎣
B∗Pjh
¯λjB∗Pjh
. . .
. . .
¯λj
n−1B∗Pjh
⎤
⎥⎥⎥⎥⎦
= 0,
which contradicts the fact that the pair (A, B) is controllable.
• The condition is suﬃcient.
Consider an element h such that (ΓT h, h) = 0; hence B∗eA∗th = 0; i.e.,
J

j=1
exp( ¯λjt)B∗PjH = 0,
t ∈(0, T ).
(4.2)
From the analyticity of the function to the left of (4.2), it follows that (4.2)
holds for any t ∈(−∞, +∞). It ﬁrst follows that B∗Pjh = 0, for any j such
that Re λj ̸= 0. Indeed, suppose λ1 is such that | Re λ1| > | Re λj|, ∀j ̸= 1.
We can write
B∗P1h +

j̸=1
exp

( ¯λj −¯λ1)t

B∗Pjh = 0.
Taking the limit as t tends to + or −inﬁnity, according to the fact that Re λ1
is positive or negative, we deduce B∗P1h = 0. More generally, there may be a
set J0 of indices j such that Re λj = Re λ1, for j ∈J0, and | Re λ1| > | Re λj|,
∀j ̸∈J0. Dividing (4.2) by exp(Re λ1t) we deduce

4 Finite dimensional control space
327

j∈J0
e−iγjtB∗Pjh +

j̸∈J0
exp( ¯λj −Re λ1)tB∗Pjh = 0,
(4.3)
where γj = Im λj. Suppose to ﬁx the ideas that Re λ1 > 0. We derive from
(4.3)
0 =
 T
0


j∈J0
e−iγjtB∗Pjh

2
dt
+
 T
0
⎛
⎝
j∈J0
e+iγjtB∗¯Pjh,

k̸∈J0
exp( ¯λk −Re λ1)tB∗Pkh
⎞
⎠dt;
hence
0 = T

j∈J0
(PjBB∗Pjh, h) +

j∈J0

k∈J0
k̸=j
exp(i(γk −γj)T ) −1
i(γk −γj)
(PkBB∗Pjh, h)
+

j∈J0

k̸=J0
exp(¯λk −Re λ1 + iγj)T −1
¯λk −Re λ1 + iγj
(PjBB∗Pkh, h).
Dividing by T , and letting T tend to ∞, we deduce

j∈J0
(PjBB∗Pjh, h) = 0,
or B∗Pjh = 0, ∀j ∈J0. If Re λ1 < 0, one should consider a similar integral
between −T and 0, to deduce a similar result.
Successively, we can treat in the same way all j such that Re λj ̸= 0. So
we have
B∗Pjh = 0,
∀j
with Re λj ̸= 0.
There remains the case when (4.2) reduces to

j∈J0
e−iγjtB∗Pjh = 0,
∀t ∈(−∞, +∞),
where λj = iγj, γj real, and J0 is a subset of {1, . . ., J}. We make a calculation
similar to that done after (4.3) to prove that

j∈J0
(PjBB∗Pjh, h) = 0,
or B∗Pjh = 0, ∀j ∈J0.
Hence, we have proved that B∗Pjh = 0, ∀j, and this implies h = 0, from
the assumption.
⊓⊔

328
III-1 Controllability and Observability
4.2 General state space
We now turn to the case of a general state space, with ﬁnite dimensional
control space U = Rm. We have
Bu =
m

i=1
biui,
with b1, b2, . . . , bm ∈H,
u ∈Rm.
(4.4)
We state (see L. Markus [1]) the following theorem.
Theorem 4.1. Assume that
b1, b2, . . . , bm ∈D∞(A) =
6
k>0
D(Ak).
If the linear set generated by the vectors
Akbj,
j = 1, . . . , m,
k = 0, 1, . . .
is dense in H, then the pair (A, B) is approximately controllable at any time
T > 0. Conversely, assume that bj = etjA ¯bj, with ¯bj ∈D∞(A), tj > 0, and
the trajectories (etA ¯bj, z) for any z ∈H are real analytic; then if the pair
(A, B) is approximately controllable at some T > 0, necessarily the linear set
generated by the vectors {Akbj, j = 1, . . . , m, k = 0, 1, . . . } is dense in H.
Proof. If A, B is not approximately controllable at T > 0, then A∗, B∗is not
observable on (0, T ) (see Proposition 3.1). Hence
∃h ̸= 0,
such that ∀t ∈(0, T ); B∗etA∗h = 0;
hence also
∀u ∈Rm,
(etABu, h) = 0
in (0, T ).
This implies
∀j = 1, . . . , m,
(etAbj, h) = 0
in (0, T ).
(4.5)
From the assumption on the bj’s, we can diﬀerentiate in t as many times as
we wish and set t = 0. We deduce
∀k > 0,
j = 1, . . . , m,
(Akbj, h) = 0.
(4.6)
As h ̸= 0, this contradicts the fact that the set generated by the vectors
{Akbj, j = 1, . . . , m, k = 0, 1, . . . }. is dense in H.
Let us now prove the second part of the statement. Assume that the set
generated by the vectors {Akbj, j = 1, . . . m, k = 0, 1, . . . } is not dense in H.
There exists h ̸= 0 such that (4.6) holds. This means also
(AketjA ¯bj, h) = 0.
(4.7)

4 Finite dimensional control space
329
Set
fj(t) = (etA ¯bj, h),
which is real analytic. Then (4.7) means that fj(tj) = 0, f (k)
j
(tj) = 0, ∀k > 0.
Therefore, fj(t) = 0, ∀t > 0. Writing fj(t + tj) = 0, we have also (4.5).
Therefore the pair (A∗, B∗) is not observable on (0, T ), which implies that the
pair cannot be approximately controllable at T .
⊓⊔
Let us give a variant of Theorem 4.1 in the case when −A has a discrete
spectrum λ1 < λ2 < · · · λk · · · with λk ↑∞and λk has multiplicity rk. It bears
similarity with Proposition 4.2. Using the projector Pk on the ﬁnite dimen-
sional eigenspace corresponding to λk, the semigroup etA is then represented
by the expansion
etAh =

k
e−λktPkh.
(4.8)
We consider Pk as an element of L(H; H). It is self-adjoint and P 2
k = Pk. We
have
z(T ; u) =

k
 T
0
e−(T −t)λkPkBu(t) dt,
(4.9)
(ΓT h, h) =
 T
0


k
e−λktB∗Pkh

2
dt.
(4.10)
We shall prove the following result.
Theorem 4.2. Assume that (4.8) is veriﬁed. Then the pair (A, B) is approx-
imately controllable at T if and only if PkBB∗Pk is full rank, for any k.
Proof. From Proposition 3.1 we know that the pair (A, B), is approximately
controllable at T if and only if (ΓT h, h) is a norm on H. From the expression
(4.10), we deduce that (ΓT h, h) is a norm on H if and only if

k
e−λktB∗Pkh = 0
on (0, T ) =⇒h = 0.
(4.11)
The function
t 	→

k
e−λktB∗Pkh
being analytic, the left part of the statement (4.11) implies also

k
e−λktB∗Pkh = 0,
∀t ≥0.
Multiplying by eλ1t and letting t tend to inﬁnity, we deduce B∗P1h = 0, and
successively B∗Pkh = 0. Therefore (4.11) means also
B∗Pkh = 0,
∀k ≥0 =⇒h = 0.
(4.12)
It is easy to check that (4.12) is equivalent to the property that the matrix
PkBB∗Pk be full rank for any k.
⊓⊔

330
III-1 Controllability and Observability
Remark 4.1. The restriction of the system to Ek (eigenspace corresponding to
λk) is given by the dynamics
z′
k = −λkzk + Bku,
zk(0) = 0,
(4.13)
where Bk = PkB. The state zk lies in the ﬁnite dimensional space Ek. The
pair (−λkI, Bk) is controllable.
⊓⊔
Remark 4.2. Note that the condition (4.12) holds also when U is not ﬁnite
dimensional. Note also that from the formula (4.10) it follows that there exists
a positive constant c such that
(ΓT h, h) ≤c

k
(1 −e−2λkT )|Pkh|2
2λk
.
(4.14)
⊓⊔
5 Controllability for the heat equation
5.1 Distributed control
We consider a bounded domain of Rn, denoted Ω, assumed to be smooth to
simplify a little. Let Γ be the boundary of Ωand Σ = Γ × (0, T ). Let O
be a subdomain of Ω. We denote by χO the characteristic function of the
subdomain O. We consider the dynamic system
⎧
⎪
⎨
⎪
⎩
z′ −∆z = v(x, t)χO,
z

Σ = 0,
z(x, 0) = 0,
(5.1)
with v the control belongs to L2
O×(0, T )

. In fact, (5.1) can be written under
the general framework (2.1). Indeed, let H = L2(Ω) and D(A) = H2(Ω) ∩
H1
0(Ω). We take A = ∆, and D(A) is equipped with the norm |Az|, z ∈D(A).
Note that A is an isometry from D(A) to H. Let next U = L2(O), and if u
belongs to U,
Bu ≡

u(x),
if x ∈O,
0,
if x ̸∈O.
It is well known that there exists a sequence 0 < λ1 < λ2 < · · · λk ↑∞of
eigenvalues and wkj, j = 1, . . . , rk, corresponding eigenvectors where rk is the
multiplicity of λk, with

−∆wkj = λkwkj,
wkj ∈H1
0(Ω),
|wkj

L2 = 1.
(5.2)
Moreover (4.5) hold true. We can then state the

5 Controllability for the heat equation
331
Theorem 5.1. The pair (A, B) deﬁned above is approximately controllable at
any time T > 0.
Proof. From Remark 4.2 it is suﬃcient to check that the condition (4.12)
holds. Note that B∗h = h|O, and thus since B∗
k = B∗Pk, condition (4.12)
means
rk

j=1
(wkj, h) wkj|O = 0
∀k =⇒h = 0.
(5.3)
The function
rk

j=1
(wkj, h) wkj
is analytic in Ω. From (5.3) it follows that it is identically 0. Hence the co-
eﬃcients (wkj, h) = 0. As this holds for any k and j = 1, . . . , rk, necessarily
h = 0. The desired result is thus proved.
⊓⊔
Remark 5.1. From (4.14) we have
(ΓT h, h) ≤C

k
1 −e−2λkT
2λk
rk

j=1
(wkj, h)2
≤C

k
1
λk
rk

j=1
(wkj, h)2 = ∥h∥2
H−1.
This means that
H−1(Ω) ⊂F ′
T
with dense and continuous injection. By duality, it follows that
FT ⊂H1
0(Ω)
with dense and continuous injection. Clearly FT increases with the Lebesgue
measure of O, and FT = H1
0(Ω), when O = Ω. In that case, exact controlla-
bility holds true.
⊓⊔
5.2 Boundary control
Consider the following situation, using a formal write-up
ζ′ −∆ζ = 0,
ζ

Γ = v,
ζ(z; 0) = 0,
(5.4)
where v(t) ∈U = L2(Γ). We need to clarify in what space ζ(T ) lies and to
what extent we can reduce (5.4) to the general framework (2.1).
In fact, we shall show that ζ(T ; v) lies in (H2∩H1
0)′, and ζ ∈C(0; T ; (H2∩
H1
0)′). One deﬁnes ζ(T ) by the transposition method of J. L. Lions and
E. Magenes [1]. Let h be an element of L2(Ω) = H; then (−∆)−1h ∈H2∩H1
0.
Consider the solution ψ of

332
III-1 Controllability and Observability
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
−ψ′ −∆ψ = 0,
ψ

Γ = 0,
ψ(x; T ) = −(−∆)−1h = A−1h
(recalling the notation A = ∆),
(5.5)
and we note that ψ = −(−∆)−1φ, where
⎧
⎪
⎨
⎪
⎩
−φ′ −∆φ = 0,
φ

Γ = 0,
φ(x; T ) = h.
(5.6)
Clearly φ ∈C(0; T ; H) and ψ ∈C(0; T ; H2 ∩H1
0). If we perform a formal
integration by parts between (5.4) and (5.5), we obtain
⟨ζ(T ), ψ(T )⟩= −
 T
0

Γ
v ∂ψ
∂ν dΓ dt,
(5.7)
and this constitutes the deﬁnition of ζ(T ) as an element of (H2 ∩H1
0)′ =

D(A)
′.
Consider now the operator n ∈L

H2; L2(Γ)

deﬁned by
nφ = ∂φ
∂ν .
(5.8)
We set
B = (−∆)−1n∗= −A−1n∗,
(5.9)
which belongs to L(L2(Γ); H). We consider

z′ = Az + Bv,
z(0) = 0.
(5.10)
Noting that (5.6) is equivalent to

−φ′ = Aφ,
φ(T ) = h,
(5.11)
then from (5.10) and (5.11) we deduce
(z(T ), h) =
 T
0
(φ, Bv) dt =
 T
0
(B∗φ, v) dt
= −
 T
0
(nA−1φ, v) dt = −
 T
0
∂ψ
∂ν , v

dt,
(5.12)
which is exactly the right-hand side of (5.7). Hence

5 Controllability for the heat equation
333
(z(T ), h) = (ζ(T ), −(−∆)−1h).
(5.13)
Therefore
z(T ) = −(−∆)−1ζ(T ).
(5.14)
Similarly for any t, z(t) = −(−∆)−1ζ(t), and as z ∈C(0, T ; H),2 we deduce
that ζ ∈C(0; T ; (H2 ∩H1
0)′). Now (5.10) is exactly equivalent to (2.1).
We shall prove the following theorem.
Theorem 5.2. The pair (A, B) is approximately controllable at any time T >
0. The range of ζ(T ; v) is dense in (H2 ∩H1
0)′.
Proof. It is clear that it is suﬃcient to prove that the pair (A, B) is approxi-
mately controllable at any time T > 0. For that we must check the condition
(4.12); i.e.,
rk

j=1
(h, wkj)B∗wkj = 0,
∀k > 0 =⇒h = 0.
(5.15)
This means also
rk

j=1
(h, wkj)∂wkj
∂ν
= 0 =⇒(h, wkj) = 0,
∀j = 1, . . . , rk.
(5.16)
But from the following lemma we have
rk

j=1
(h, wkj)wkj = 0.
Hence the desired result.
⊓⊔
Lemma 5.1. Let w be an eigenfunction of the Laplace operator corresponding
to the eigenvalue λ; i.e.,
−∆w = λw,
w

Γ = 0;
(5.17)
then one has

Γ
∂w
∂ν
2
m · ν dΓ = 2λ

Ω
w2 dx,
(5.18)
where m(x) = x −x0.
Proof. We multiply (5.17) by
mα
∂w
∂xα
(using summation convention) and perform integration by parts. The desired
result follows.
⊓⊔
2 Given a Banach space X, the notation C(0, T ; X) stands for C([0, T ]; X). See also
footnote 2 on page 3 in the Introduction to the book.

334
III-1 Controllability and Observability
Remark 5.2. Let Γ0 be a part of Γ. The control is exerted on Γ0 only, which
means that it is 0 on Γ1 = Γ −Γ0. To treat this case, let us deﬁne
n0φ = ∂φ
∂ν

Γ0
.
Take B = −A−1n∗
0. The result of Theorem 5.2 extends to the pair (A, B)
provided that for an eigenfunction w as in (5.17)
∂w
∂ν

Γ0
= 0
implies w = 0. From the formula (5.18), it is suﬃcient that m · ν ≥0 on
Γ0.
⊓⊔
Remark 5.3. From (4.14) again we deduce
(ΓT h, h) ≤C

k
1 −e−2λkT
2λk
rk

j=1
(wkj, h)2 ≤C(∥h∥H−1)2.
(5.19)
Therefore H−1 ⊂F ′
T , with continuous and dense injection. Hence also
FT ⊂H1
0(Ω), with continuous and dense injection. It follows that the range
of ζ(T ; v), when v lies in L2
0, T ; L2(Γ)

is dense in H−1.
⊓⊔
5.3 Neumann boundary control
We introduce some notation. We take
H = L2(Ω),
V = H1(Ω),
A = ∆−I,
associated with the Neumann boundary condition. Hence
D(A) =

z ∈H2(Ω): ∂z
∂ν = 0

.
Again D(A) is equipped with the norm |Az|, z ∈D(A). The eigenvectors are
deﬁned by
−∆w + w = λw,
∂w
∂ν = 0.
(5.20)
The ﬁrst eigenvalue is 1. It is isolated; hence, 1 < λ2 ≤λ3 · · · and w1 =
1/
%
|Ω|. We set m(x) = x −x0 and deﬁne
Γ0 = {x ∈Γ : m · v > 0},
Γ1 = Γ −Γ0.
(5.21)
Let γ be the trace operator on Γ. Note the γ ∈L

V ; L2(Γ)

∩L

H2(Ω);
H1(Γ)

. Let also σj be the tangential operators on Γ ∈L

H1(Γ); L2(Γ)

,
such that

5 Controllability for the heat equation
335
γ ∂φ
∂xj

Γ
= νj
∂φ
∂ν + σjγφ = νjnφ + σjγφ,
∀φ ∈H2(Ω).
(5.22)
Note that nφ = ν · γDφ, and ν · σ = 0.
Similarly, we deﬁne by γ0, σj,0 (γ1, σj,1) the respective restrictions of γ, σj
to Γ0 and Γ1. We also use the notation
Dσ = (σ1, . . . , σn) ∈L

H1(Γ);

L2(Γ)
n
.
The operators γ, σ1γ, σ2γ, . . . σnγ ∈L

D(A); L2(Γ)

and not identically
0, whereas n = 0 on D(A). This should be compared with the situation in the
Dirichlet case, where γ, σ1γ, σ2γ, . . . σnγ are 0 on D(A) and n is not 0. This
explains why the analogy with the Dirichlet case implies the control to be an
element of U =

L2(Γ)
n+1, instead of just L2(Γ). The operator B is deﬁned
by
B = A−1(γ∗γ∗σ∗
1 γ∗σ∗
2 . . . γ∗σ∗
n)
= A−1(γ∗γ∗D∗
σ) ∈L(U; H).
(5.23)
We can also view U as

L2(Γ0)
n+1 ×

L2(Γ1)
n+1. Some components might
be taken equal to 0; in which case, they are omitted and we use γ∗
0 or γ∗
1
instead of γ∗(or σ∗
1,0, σ∗
1,1 . . . ; D∗
σ,0, D∗
σ,1). We are now interested in the pair
A, B. Let us ﬁrst interpret the dynamic system (5.10), corresponding to this
pair,
z′ = Az + Bv,
z(0) = 0.
(5.24)
Consider also (5.11), i.e.
−φ′ = Aφ,
φ(T ) = h,
(5.25)
then we have
(z(T ), h) =
 T
0
(B∗φ, v) dt =
 T
0

(γψ, v0) +
n

i=1
(σiγψ, vi)

dt,
(5.26)
where ψ = A−1φ and v = (v0, v1, . . . , vn). As nψ = 0, we can also write the
above relation as
(z(T ), h) =
 T
0

Γ

ψv0 +
n

i=1
∂ψ
∂xi
vi

dΓ,
(5.27)
where we omit to indicate explicitly the operator γ.
Note that z(·) ∈C(0, T ; H). Let ζ(t) = Az(t) ∈C

0, T ;

D(A)
′
. Then
from (5.27) we deduce
⟨ζ(T ), ψ(T )⟩=
 T
0

Γ

ψv0 +
n

i=1
∂ψ
∂xi
vi

dΓ.
(5.28)

336
III-1 Controllability and Observability
Note that if v ∈

H1(Γ)
′ and ψ ∈D(A) (hence γψ ∈H1(Γ)), we can write
⟨v, γψ⟩=

Γ

ψv0 +
n

i=1
∂ψ
∂xi
vi

dΓ,
where (v0, v1, . . . , vn) is a representation of v.
Therefore (5.28) reads also
⟨ζ(T ), ψ(T )⟩=
 T
0
⟨v(t), γψ(t)⟩dt.
(5.29)
This is exactly the formula that follows from the deﬁnition of the solution of
⎧
⎪
⎪
⎨
⎪
⎪
⎩
ζ′ −∆ζ = 0,
∂ζ
∂ν = v,
ζ(x, 0) = 0
(5.30)
obtained by the Method of Transposition of J. L. Lions and E. Magenes [1]
(see also Chapter 2 of Part II), using the dual equation
⎧
⎪
⎪
⎨
⎪
⎪
⎩
−ψ′ −∆ψ = 0,
∂ψ
∂ν = 0,
ψ(T ) ∈D(A).
(5.31)
We are interested in the density of the range of ζ(T ; v), solution of (5.30)
at some time T , in the space

D(A)
′. This is equivalent to the density of
z(T ; v) in H, where z is the solution of (5.24). We are in the framework of
the model (2.1), with a semigroup satisfying the condition (4.5), and thus
the problem is that of the approximate controllability of the pair (A, B) at
some time T . The general theory implies that the pair (A, B) is approximately
controllable at any positive time T , if and only if the condition (4.12) holds.
We write the condition (4.12)
rk

j=1
(h, wkj)B∗wkj = 0,
∀k > 0 =⇒h = 0.
(5.32)
This implies that
w =
rk

j=1
(h, wkj)wkj
satisﬁes γw = 0, Dσw = 0. Clearly w = 0; hence the coeﬃcients (h, wkj) = 0,
for j = 1, . . . , rk. Hence h = 0.
It is clear from the above that if we control the whole boundary, we can
take v(t) in L2(Γ) instead of

H1(Γ)
′; i.e., v(t) = v0(t) and v1(t) = · · · =
vn(t) = 0.

5 Controllability for the heat equation
337
On the other hand, we need

H1(Γ)
′ if we control only a part of the
boundary. Let us make this precise. We ﬁrst state a lemma.
Lemma 5.2. The eigenvector w solution of (5.20) satisﬁes the relation

Γ
m · ν|Dσw|2 dΓ + 2(λ −1)

Ω
w2 dx = (λ −1)

Γ
m · νw2 dΓ.
(5.33)
Proof. As usual, we multiply (5.20) by
mα
∂w
∂xα
and perform integration by parts. Details are left to the reader.
⊓⊔
We then state the following theorem.
Theorem 5.3. Let A = ∆−I associated with the Neumann boundary condi-
tion, and the two following cases for U, B. Either U = L2(Γ) and B = A−1γ∗
or U = L2(Γ0) ×

L2(Γ1)
n and B = A−1(γ∗
0 γ∗
1D∗
σ,1). Then the pair (A, B)
is approximately controllable at any T > 0. The range of ζ(T ; v) solution of
(5.40) is dense in

D(A)
′.
Proof. The ﬁrst case has already been discussed. Consider the second case. We
have to prove (5.32). Note ﬁrst that for k = 1, rk = 1, and w1 is a constant.
We necessarily have (h, w1) = 0. Starting with k = 2, we have λk > 1. From
the hypothesis of the statement (5.32) we have, writing
w =
rk

j=1
(h, wkj)wkj,
γ0w = 0,
Dσ,1w = 0.
(5.34)
Splitting the surface integrals in (5.33) in two parts Γ0 and Γ1 and using
the sign properties of m.ν on Γ0, Γ1 and the fact that λ > 1 we deduce that

Ω
w2 dx = 0.
Therefore the coeﬃcients (h, wkj) = 0, for j = 1, . . . , rk. The desired result
has been proved.
⊓⊔
Remark 5.4. Apparently, it is not suﬃcient to take U = L2(Γ0); unlike the
Dirichlet case, see Remark 5.2.
⊓⊔
Remark 5.5. Consider again (ΓT h, h). As usual from (4.14), we can prove that
(ΓT h, h) ≤C(∥h∥V ′)2, recalling that V = H1. Hence V ′ ⊂F ′
T and FT ⊂V ,
with continuous and dense injection. In the case U = L2(Γ) and B = A−1γ∗,
one can obtain an additional result, namely

D(A)
′ ⊂F ′
T ; therefore FT ⊂
D(A). It follows that the range of ζ(T ; v), solution of (5.30), is dense in H as
v(·) varies in L2
0, T ; L2(Γ)

.
⊓⊔

338
III-1 Controllability and Observability
5.4 Pointwise control
We consider the system
⎧
⎪
⎨
⎪
⎩
ζ′ −∆ζ = ,m
i=1 vi(t)δ(x −αi),
ζ

Γ = 0,
ζ(x; 0) = 0,
(5.35)
where the points αi belong to Ωand represent actuators. We assume that the
dimension of the space is n ≤3. We shall reduce the formal model (5.35) to
our general set up.
We take A = ∆with Dirichlet conditions. We deﬁne bi ∈H by the equation
(bi, h) = (A−1h)(αi).
(5.36)
This makes sense because for h ∈H = L2, A−1h belongs to H2 ⊂C(¯Ω)
with continuous injection. Hence the right-hand side of (5.36) is deﬁned and
represents a linear continuous functional on H. Consider the dynamic system
z′ = Az +
m

i=1
vi(t)bi,
z(0) = 0.
(5.37)
Let us check that
ζ(T ) = Az(T )
(5.38)
coincides with the deﬁnition of (5.35) through the Method of Transposition.
Indeed consider
−φ′ = Aφ,
φ(T ) = h.
(5.39)
Then from (5.37) and (5.39), it follows taking into account the deﬁnition of bi
(z(T ), h) =
m

i=1
 T
0
vi(t)ψ(αi, t) dt,
(5.40)
where ψ = A−1φ. But from the Method of Transposition we deﬁne ζ(T ) by

ζ(T ), ψ(T )

=
m

i=1
 T
0
vi(t)ψ(αi, t) dt.
(5.41)
From (5.40) and (5.41), using the fact that ψ(T ) = A−1h, we deduce that
(5.38) holds. Now the system (5.37) has already been studied in §4.2. Consider
the matrices
Fk,
with Fk,ji = wkj(αi),
j = 1, . . . , rk, i = 1, . . . , m;
(5.42)
then we can state the following theorem.

6 Controllability for skew-symmetric operators
339
Theorem 5.4. The pair (A, B), B = (b1, b2, . . . , bm) is approximately con-
trollable at any T > 0, if and only if rank Fk = rk for any k.
Remark 5.6. We refer to A. El Jai and A. J. Pritchard [2] for general
results of controllability in the case of pointwise control.
⊓⊔
Remark 5.7. We assume that the assumption of Theorem 5.4 is satisﬁed. Using
condition (4.14), we obtain again that FT ⊂H1
0 with dense and continuous
injection. Hence the range of ζ(T ; v) is dense in H−1.
⊓⊔
6 Controllability for skew-symmetric operators
6.1 Notation and general comments
We shall need to work here with a complex spectrum. We assume that A has
a purely imaginary spectrum of the form
#
i
%
λj, −i
%
λj : j = 1, . . .
$
,
where
0 < λ1 ≤λ2 ≤· · · ≤λj ≤· · · ,
λj ↑∞,
with only a ﬁnite number of successive eigenvalues possibly equal. If φj is the
eigenvector corresponding to i
%
λj, then ¯φj is the eigenvector corresponding
to −i
%
λj. We assume that
φj, ¯φj
(6.1)
form an orthonormal basis of H (complexiﬁed).
This implies with the notation of the scalar product in H complexiﬁed
(φj, ¯φk) = δjk,
(φj, φk) = 0.
(6.2)
By deﬁnition
Aφj = i
%
λjφj,
A ¯φj = −i
%
λj ¯φj.
(6.3)
If φ is an element of H (real valued), then it can be represented with the
expansion
φ =

j
(cj ¯φj + ¯cjφj),
(6.4)
with
cj = (φ, φj),
¯cj = (φ, ¯φj),
|φ|2 = 2

j
|cj|2.
(6.5)
It is easy to check that the operator A is skew adjoint,
A∗= −A.
(6.6)
In fact we can even characterize the type of operators we are considering,
thanks to the following proposition.

340
III-1 Controllability and Observability
Proposition 6.1. Assume that A is a linear operator from a dense subspace
D(A) of H into H, satisﬁes (6.6) and
the null space of A = {0},
(6.7)

the mapping A + I from D(A) to H is onto
(and thus invertible since already one to one),
(6.8)
and the map T = (A + I)−1 is compact.
(6.9)
Then the spectrum of A is of the form ±i
%
λj, with
0 < λ1 ≤λ2 ≤· · · ≤λj ≤· · · ,
λj ↑∞,
and the corresponding eigenvectors φj and ¯φj form an orthonormal basis for
H. Conversely if A has these spectral properties, then it satisﬁes (6.6) to (6.9).
Proof. Let us assume (6.6); then if z ∈D(A), we can write
(Az, ¯z) = −(A∗z, ¯z) = −(z, Az) = −(Az, ¯z),
and thus (Az, ¯z) is purely imaginary. Hence the eigenvalues are necessarily
purely imaginary. As −1 cannot be an eigenvalue, the map A+I is necessarily
one to one. Moreover if λ is an eigenvalue corresponding to the eigenvector φ,
then ¯φ is an eigenvector corresponding to the eigenvalue ¯λ.
Assume now (6.7), (6.8), and (6.9). Let us check the T is a normal operator
on H; i.e.,
T ∗T = TT ∗.
(6.10)
Note that T ∗= (A∗+ I)−1. Let f ∈H and deﬁne
φ = 1
2{(A + I)−1f + (−A + I)−1f},
ψ = 1
2{−(A + I)−1f + (−A + I)−1f};
then clearly ψ = Aφ; thus
Aφ + φ = (A∗+ I)−1f,
A∗φ + φ = (A + I)−1f,
which proves (6.10).
From the spectral theory of normal compact operators, see T. Kato [4,
p. 260], we can state that T has a discrete spectrum {µ1, µ2, . . . }, where
|µ1| ≥|µ2| ≥· · · ≥|µn| ≥· · · , |µn| tends to 0 as n tends to inﬁnity. Each of
these eigenvalues has a ﬁnite multiplicity. Moreover, as the null space of T is
0, the eigenvectors form an orthonormal basis of H. But eigenvectors of T are
eigenvectors of A, with eigenvalues obtained by the transformation
λ = −1 + 1/µ.
The spectral properties of A are easily deduced.

6 Controllability for skew-symmetric operators
341
Let us check the reverse. The set D(A) is made of elements
φ =

j
(cj ¯φj + ¯cjφj)
such that

j
|cj|2λj < ∞.
It is clear that D(A) is dense in H. If φ ∈D(A), then
Aφ =

j
(−icj
%
λj ¯φj + i ¯cj
%
λjφj).
Therefore we can solve the equation
Aφ + φ = f
(6.11)
as follows:
cj =
fj
(1 −i
%
λj),
where fj denotes the components of f. Therefore, (6.11) has one and only
one solution. To prove (6.9) consider a sequence {f n} that tends to 0 in H
weakly, and let φn be the corresponding solution of (6.11); we must prove
that φn tends to 0 in H strongly. Let f n
j be the components of f n and
cn
j =
f n
j
(1 −i
%
λj)
be the corresponding components of φn.
We have

j
|cn
j |2 =

j
|f n
j |2
1 + λj
≤

j=1,...,N
|f n
j |2 + |f n|2
λN
.
As f n tends to 0 weakly, |f n|2 remains bounded and f n
j tends to 0 for any
ﬁxed j. Moreover λn tends to 0 as N tends to inﬁnity. It follows from the
above inequality that

j
|cn
j |2
tends to 0 as n tends to inﬁnity. This means that φn tends to 0 in H.
⊓⊔
The solution of
z′ = Az,
z(0) = φ
(6.12)
is given by
z(t) = etAφ =

j
(cje−i√
λjt ¯φj + ¯cje+i√
λjtφj),
(6.13)

342
III-1 Controllability and Observability
where cj are the components of φ.
In the sequel we shall use the notation
φ = Re φ + i Im φ
to express the decomposition of an element of H complexiﬁed.
Note also that if φ ∈D(A), then
∥φ∥2
D(A) = |Aφ|2 = 2

j
λj|cj|2
and
φj
%
λj
,
¯φj
%
λj
form an orthonormal basis of D(A). Similarly φj
%
λj, ¯φj
%
λj form an or-
thonormal basis of

D(A)
′.
6.2 Dynamical system
Consider B ∈L(U; H). We shall make several assumptions on B in relation
with the eigenvectors φj.
There exist N integer ≥0 and operators M ∈L(H; H), Λ ∈L(H; D(A)′)
such that
|B∗z|2 ≥(Mz, z),
∀z ∈H,
(6.14)
for all j, k ≥N + 1,
(M Re φj, Im φk) = 0,
(6.15)
for all j, k ≥N + 1, j ̸= k
(M Re φj, Re φk) = (Λ Re φj, Re φk)
λk
+ (Λ Re φk, Re φj)
λj
(6.16)
for all j, k ≥N + 1, j ̸= k
(Λ Re φj, Re φk) + (Λ Re φk, Re φj) +
%
λjλk(M Im φj, Im φk) = 0.
(6.17)
There exists c0 > 0 such that for all j ≥N + 1,
(M Re φj, Re φj) + (M Im φj, Im φj) ≥c0
λj
,
(6.18)
for all j ≥N + 1
(M Re φj, Re φj) −(M Im φj, Im φj) −4(Λ Re φj, Re φj)
λj
 ≤
K
%
λj
(6.19)
and

6 Controllability for skew-symmetric operators
343
λN+1 > λN.
For j ≤N, we consider only the λj having diﬀerent values and call Pj( ¯Pj)
the projector on the ﬁnite dimensional eigensubspace of H corresponding to
the conjugate pair of eigenvalues {i
%
λj, −i
%
λj}; then
|B∗Pjz|2 ≥c1
|Pjz|2
λj
,
∀z ∈H, j ≤N.
(6.20)
With our convention we have
0 < λ1 < λ2 < · · · < λN < λN+1 ≤λN+2 ≤· · · .
An element φ of H is represented by the expansion
φ =
N

j=1
(Pjφ + ¯Pjφ) +

j≥N+1
(cj ¯φj + ¯cjφj).
(6.21)
Naturally if N = 0, the condition (6.20) is void.
We now consider the dynamic system corresponding to the pair (A, B)
z′ = Az + Bv,
z(0) = 0.
(6.22)
Our main result is as follows.
Theorem 6.1. We assume that A satisﬁes (6.6) to (6.9) and B satisﬁes
(6.14) to (6.20); then the pair (A, B) is exactly controllable at T , for T suﬃ-
ciently large.
Proof. We shall consider the controllability operator
ΓT =
 T
0
etABB∗etA∗dt
and prove that
∀h ∈H,
(ΓT h, h) ≥cT ∥h∥2
(D(A))′
(6.23)
for T suﬃciently large.
According to §3.2, this implies the desired result. Consider
h =
N

j=1
(Pjh + ¯Pjh) +

j≥N+1
(cj ¯φj + ¯cjφj)
with cj = (h, φj); then
(ΓT h, h) =
 T
0
|B∗etA∗h|2 dt
= 4
 T
0

B∗
⎛
⎝
N

j=1
Re(eti√
λjPjh) +

j≥N+1
Re(e−ti√
λj ¯cjφj)
⎞
⎠

2
dt
= X1 + X2 + X3,
(6.24)

344
III-1 Controllability and Observability
where
X1 = 4
 T
0

B∗
⎡
⎣
N

j=1
Re(e−ti√
λjPjh)
⎤
⎦

2
dt,
X2 = 8
 T
0
B∗
⎡
⎣
N

j=1
Re(e−ti√
λjPjH)
⎤
⎦,
⎛
⎝B∗
⎡
⎣
j≥N+1
Re(e−ti√
λj ¯cjφj)
⎤
⎦
⎞
⎠dt,
X3 = 4
 T
0

B∗
⎡
⎣
j≥N+1
Re(e−ti√
λj ¯cjφj)
⎤
⎦

2
dt.
By an easy computation
X1 = 2T
N

j=1
|B∗Pjh|2 + Y1
with
Y1 = 2 Re
N

j,k=1
(B∗Pjh, B∗Pkh)1 −e−iT (√
λj+√λk)
i(
%
λj + √λk)
+ 2 Re
N

j=1
N

k=1
k̸=j
(B∗Pjh, B∗¯
Pkh)1 −e−iT (√
λj−√λk)
i(
%
λj −√λk)
,
X2 = 2 Re
N

j=1

k≥N+1
(B∗Pjh, B∗φk¯ck)1 −e−iT (√
λj+√λk)
i(
%
λj + √λk)
+ 2 Re
N

j=1

k≥N+1
(B∗Pjh, B∗¯
φkck)1 −e−iT (√
λj−√λk)
i(
%
λj −√λk)
.
Now according to (6.14)
X3 ≥4
 T
0
⎛
⎝M
⎛
⎝
j≥N+1
Re(e−ti√
λj
¯
cjφj)
⎞
⎠,

j≥N+1
Re(e−ti√
λj ¯cjφj)
⎞
⎠dt.
At this stage it is convenient to introduce the following notation, which will
help reduce the length of the following equations and estimates:
mij = (M Im φi, Im φj),
lij = (Λ Re φi, Re φj).
From (6.15)

6 Controllability for skew-symmetric operators
345
X3 ≥4

j,k≥N+1
 T
0
(M Re φj, Re φk) Re(e−ti√
λj ¯cj) Re(e−ti√λk ¯ck) dt
+4

j,k≥N+1
 T
0
mjk(e−ti√
λj ¯cj) Im(e−ti√λk ¯ck) dt,
and from (6.16),
X3 ≥Y3 + Z3,
where
Y3 = 4

j≥N+1
 T
0
(M Re φj, Re φj)| Re(e−ti√
λj ¯cj)|2 dt
+ 4

j≥N+1
 T
0
(M Im φj, Im φj)| Im(e−ti√
λj ¯cj)|2 dt,
Z3 = 4

k≥N+1
j̸=k
 T
0
ljk
λk
+ lkj
λj
	
Re(e−ti√
λj ¯cj) Re(e−ti√λk ¯ck) dt
+ 4

k≥N+1
j̸=k
 T
0
mjk Im(e−ti√
λj ¯cj) Im(e−ti√λk ¯ck) dt.
Set
aj =
cj
%
λj
.
Then we can write
Z3/4 =

k≥N+1
j̸=k
 T
0

ljk
%
λj
√λk
+ lkj
√λk
%
λj

Re(e−ti√
λj ¯aj) Re(e−ti√λk ¯ak) dt
+

k≥N+1
j̸=k
 T
0
%
λjλkmjk Im(e−ti√
λj ¯aj) Im(e−ti√λk ¯ak) dt.
Then
Z3 = −4

k≥N+1
j̸=k
 T
0
ljk
√λk
d
dt Im(e−ti√
λj ¯aj) Re(e−ti√λk ¯ak) dt
−4

k≥N+1
j̸=k
 T
0
lkj
%
λj
Re(e−ti√
λj ¯aj) d
dt Im(e−ti√λk ¯ak) dt
+ 4

k≥N+1
j̸=k
 T
0
%
λjλk mjk Im(e−ti√
λj ¯aj) Im(e−ti√λk ¯ak) dt.

346
III-1 Controllability and Observability
Integrating by parts and using (6.17) obtains
Z3 =−4

k≥N+1
j̸=k
(Λ Re φj, Re φk)
√λk

Im(eiT√
λj ¯aj) · Re(eiT √λk ¯ak)
−Im( ¯aj) Re( ¯ak)

−4

k≥N+1
j̸=k
(Λ Re φk, Re φj)
%
λj

Im(eiT √λk ¯ak) · Re(eiT√
λj ¯aj)
−Im( ¯ak) Re( ¯aj)

.
Hence also
Z3 = −8
⎛
⎝Λ
⎛
⎝
j≥N+1
Re φj Im(e−iT√
λj ¯aj)
⎞
⎠,

k≥N+1
Re φk
Re(e−iT √λk ¯ak)
√λk
⎞
⎠
+ 8
⎛
⎝Λ
⎛
⎝
j≥N+1
Re φj Im( ¯aj)
⎞
⎠,

k≥N+1
Re φk
Re( ¯ak)
√λk
⎞
⎠
+ 8

j≥N+1
(Λ Re φj, Re φj) Im(e−iT√
λj ¯aj)Re(e−iT√
λj ¯aj)
%
λj
−8

j≥N+1
(Λ Re φj, Re φj) Im( ¯aj)Re( ¯aj)
%
λj
.
Next as easily seen
Y3 = 2T

j≥N+1
|cj|2
(M Re φj, Re φj) + (M Im φj, Im φj)

+ Y ′
3,
with
Y ′
3 =

j≥N+1
{(M Re φj, Re φj) −mjj}Im(c2
j(e2iT√
λj −1))
%
λj
.
Collecting results we deduce that
(ΓT h, h) ≥2T
N

j=1
|B∗Pjh|2 + 2T

j≥N+1
|cj|2[(M Re φj, Re φj) + mjj]
+ Y1 + X2 + Z3 + Y ′
3.
(6.25)
Using the assumptions (6.18) and (6.19) we get, setting c = min{c0, c1}:
(ΓT h, h) ≥cT
⎧
⎨
⎩
N

j=1
|Pjh|2
λj
+

j≥N+1
|aj|2
⎫
⎬
⎭+ Y1 + X2 + Z3 + Y ′
3
≥cT ∥h∥2
(D(A))′ + Y1 + X2 + Z3 + Y ′
3.
(6.26)

6 Controllability for skew-symmetric operators
347
We now estimate the remainder +Y1 + X2 + Z3 + Y ′
3.
We ﬁrst check
Y1 + X2
= 2 Re
⎛
⎜
⎝
N

j=1
BB∗Pjh,
N

k=1
k̸=j

Pkh1−eiT (√
λj+√λk)
i(
%
λj +√λk) + ¯
Pkh1−e−iT (√
λj−√λk)
i(
%
λj −√λk)

+

k≥N+1

φk ¯ck
1 −e−iT (√
λj−√λk)
i(
%
λj +√λk)
+ ¯φkck
1−e−iT (√
λj−√λk)
i(
%
λj −√λk)
⎞
⎠. (6.27)
Hence
|Y1 + X2| ≤βN∥h∥2
(D(A))′,
(6.28)
where
βN =
√
2 sup
h
N

j=1
%
λjΛj
|BB∗Pjh|
|Pjh|
and
Λj = max
λ1/2
j+1(λj + λj+1)1/2
λj+1 −λj
, λ1/2
j−1(λj + λj−1)1/2
λj −λj−1

.
We interpret λj−1 = 0, if j = 1. Now
Z3 + Y ′
3 = T3 + T ′
3,
where
T3 = −8
⎛
⎝Λ
⎛
⎝
j≥N+1
Re φj Im(e−iT√
λj ¯aj)
⎞
⎠,

k≥N+1
Re φk
Re(e−iT √λk ¯ak)
√λk
⎞
⎠
+ 8
⎛
⎝Λ
⎛
⎝
j≥N+1
Re φj Im( ¯aj)
⎞
⎠,

k≥N+1
Re φk
Re( ¯ak)
√λk
⎞
⎠
and
T ′
3 =

j≥N+1

(M Re φj, Re φj) −(M Im φj, Im φj) −4(Λ Re φj Re φj)
λj

Im(c2
j(e2iT√
λj −1))
%
λj
.
As Λ ∈L

H;

D(A)
′
and using the fact that
φj
%
λj
,
¯φj
%
λj

348
III-1 Controllability and Observability
are orthonormal in D(A), we can write the estimate
|T3| ≤4∥Λ∥∥h∥2
(D(A))′.
(6.29)
Similarly thanks to (6.19)
|T ′
3| ≤K∥h∥2
(D(A))′.
(6.30)
From (6.26), (6.28), (6.29), and (6.30), we deduce the desired result.
⊓⊔
Remark 6.1. We have proved that
(ΓT h, h) ≥c(T −T0)∥h∥2
(D(A))′
with
T0 = βN + K + 4∥Λ∥
c
,
where c = min(c0, c1) (= c0 if N = 0). Therefore there is exact controllability
if T > T0. Moreover this estimate implies the sequence
D(A) ⊂FT ⊂H ⊂F ′
T ⊂

D(A)
′
(6.31)
with continuous and dense injection, as seen in §3.2.
⊓⊔
Remark 6.2. In ﬁnite dimension, Theorem 6.1 reduces to Proposition 4.2.
⊓⊔
There is some ﬂexibility in the type of assumption we can make to achieve
exact controllability. We give next a variant of Theorem 6.1.
Theorem 6.2. We make the assumptions (6.6) to (6.9), (6.14), (6.15), (6.20),
and
BB∗∈L

D(A)
′; D(A)

(6.32)
and ∀j, k ≥N + 1, j ̸= k
λjλk(M Re φj, Re φk) = (Λ Re φj, Re φk) + (Λ Re φk, Re φj).
(6.33)
There exists Q ∈L(H; H) ≥0, self-adjoint such that
∀j, k ≥N + 1, j ̸= k
(Λ Re φj, Re φk)
λk
+ (Λ Re φk, Re φj)
λj
+
%
λjλk(M Im φj, Im φk) = (Q Re φj, Re φk).
(6.34)
There exists c0 > such that ∀j ≥N + 1
(M Re φj, Re φj) + (M Im φj, Im φj) −(Q Re φj, Re φj)
λj
≥c0
λ2
j
,
(6.35)
and ∀j, k ≥N + 1,

6 Controllability for skew-symmetric operators
349
(M Re φj, Re φj) −(M Im φj, Im φj)
+ (Q Re φj, Re φj)
λj
−4Λ Reφj, Re φj)
λ2
j
 ≤
K
λ3/2
j
.
(6.36)
Then the pair (A, B) is exactly controllable for T suﬃciently large, and
D(A2) ⊂FT , with dense and continuous injection.
Proof. One proves that
(ΓT h, h) ≥cT ∥h∥2
(D(A2))′,
∀h ∈H.
(6.37)
Of course this estimate is not as good as (6.23), but it is suﬃcient to establish
that (ΓT h, h) is a norm on H and the rest of the statement. We again write
(ΓT h, h) = X1 + X2 + X3,
where
X1 =4
 T
0

B∗
⎛
⎝
N

j=1
Re(e−ti√
λjPjh)
⎞
⎠

2
dt,
X2 =8
 T
0
⎛
⎝B∗
⎛
⎝
N

j=1
Re(e−ti√
λjPjh)
⎞
⎠, B∗
⎛
⎝
j≥N+1
Re(e−ti√
λj ¯cjφj)
⎞
⎠
⎞
⎠dt,
X3 =4
 T
O

B∗
⎛
⎝
j≥N+1
Re(e−ti√
λj ¯cjφj)
⎞
⎠

2
dt.
As in Theorem 6.1
X1 = 2T
N

j=1
|B∗Pjh|2 + Y1.
We can combine
X2 + Y1 = right-hand side of (6.27).
(6.38)
We can state the estimate
|X2 + Y1| ≤β′
N∥h∥2
(D(A2))′,
(6.39)
where
β′
N =
√
2 sup
h
N

j=1
%
λjΛj
∥BB∗Pjh∥D(A)
∥Pjh∥(D(A))′
(6.40)
and

350
III-1 Controllability and Observability
Λj = max
λ1/2
j+1(λj + λj+1)1/2
λj+1 −λj
,
λ1/2
j−1(λj + λj−1)1/2
λj −λj−1

.
Moreover again as in the proof of Theorem 6.1.
X3 ≥4

j,k≥N+1
 T
0
(M Re φj, Re φk) Re(e−ti√
λj ¯cj) · Re(e−ti√λk ¯ck) dt
+ 4

j,k≥N=1
 T
0
(M Im φj, Im φk) Im(e−ti√
λj ¯cj) · Im(e−ti√λk ¯ck) dt.
Let us write bj = cj/λj; then from (6.33)
X3 ≥Y3 + Z3,
where
Y3 = 4

j≥N+1
 T
0
λ2
j(M Re φj, Re φj)| Re(e−ti√
λj ¯bj)|2 dt
+ 4

j≥N+1
 T
0
λ2
j(M Im φj, Im φj)| Im(e−ti√
λj ¯bj)|2 dt,
(6.41)
Z3= 4

k≥N+1
j̸=k
 T
0

(Λ Re φj, Re φk) + (Λ Re φk, Re φj)

· Re(e−ti√
λj ¯bj) Re(e−ti√λk ¯bk) dt
+4

k≥N+1
j̸=k
 T
0
λjλk(M Im φj, Im φk)
· Im(e−ti√
λj ¯bj) Im(e−ti√λk ¯bk) dt.
(6.42)
We can write
 T
0

(Λ Re φj, Re φk) + (Λ Re φk, Re φj)

· Re(e−ti√
λj ¯bj) Re(e−ti√λk ¯bk) dt
= −

Λ Re φj, Re φk
√λk

Re(e−T i√
λj ¯bj) · Im(e−T i√λk ¯bk)−Re ¯bj Im ¯bk

−

Λ Re φk, Re φj
%
λj

Re(e−T i√λk ¯bk) · Im(e−T i√
λj ¯bj)−Re ¯bk Im ¯bj

+
 T
0
(Λ Re φj, Re φk)
%
λj
√λk
+ (Λ Re φk, Re φj)√λk
%
λj

· Im(e−ti√
λj ¯bj) Im(e−ti√λk ¯bk) dt
(6.43)
and thus from the assumption (6.34)
Z3 =−8
⎛
⎝Λ
⎡
⎣
j≥N+1
Re φj Re(e−T i√
λj ¯bj)
⎤
⎦,

k≥N+1
Re φk
√λk
Im(e−T i√λk ¯bk)
⎞
⎠

6 Controllability for skew-symmetric operators
351
+ 8
⎛
⎝Λ
⎡
⎣
j≥N+1
Re φj Re( ¯bj)
⎤
⎦,

k≥N+1
Re φk
√λk
Im( ¯bk)
⎞
⎠
+ 8

j≥N+1

Λ Re φj, Re φj
%
λj

Re(e−T i√
λj ¯bj) Im(e−T i√
λj ¯bj

−Re
 ¯bj) Im( ¯bj)

+ 4
 T
0
⎛
⎝Q

j≥N+1
%
λj Re φj Im(e−T i√
λj ¯bj),

k≥N+1
%
λk Re φk Im(e−T i√λk ¯bk)
⎞
⎠
−4
 T
0

j≥N+1
λj(Q Re φj, Re φj)| Im(e−ti√
λj ¯bj)|2 dt.
(6.44)
Since Q ≥0, we can assert that
X1 + X2 + X3
≥2T
⎧
⎨
⎩
N

j=1
|B∗Pjh|2 +

j≥N+1
|bj|2[λ2
j

(M Re φj, Re φj)+(M Im φj, Im φj)

−λj(Q Re φj, Re φj)]
⎫
⎬
⎭+ Y1 + X2 + ∆1 + ∆2,
(6.45)
with
∆1 =

j≥N+1

λ3/2
j
[(M Re φj, Re φj) −(M Im φj, Im φj)]
+λ1/2
j
(Q Re φj, Re φj) −4(Λ Re φj, Re φj)
λ1/2
j

Im

b2
j(e2T i√
λj −1)

,
∆2 = −8
⎛
⎝Λ

j≥N+1
Re φj Re(e−T i√
λj ¯bj)
⎞
⎠,

k≥N+1
Re φk
√λk
Im(e−T i√λk ¯bk)

+ 8
⎛
⎝Λ
⎡
⎣
j≥N+1
Re φj Re( ¯bj)
⎤
⎦
⎞
⎠,

k≥N+1
Re φk
√λk
Im( ¯bk).
Then, according to the assumption (6.36)
|∆1| ≤K∥h∥2
(D(A2))′
and

352
III-1 Controllability and Observability
|∆2| ≤4∥Λ∥∥h∥2
(D(A2))′.
Then we deduce from the assumptions (6.20) and (6.35)
(ΓT h, h) ≥c(T −T0)∥h∥2
(D(A2))′,
(6.46)
with c = min{c0, c1} (c0 if N = 0) and
T0 = β′
N + K + 4∥Λ∥
c
,
and the desired result has been proved.
⊓⊔
6.3 Approximation
We shall study the properties of ΓT , assuming only (6.6) to (6.9) and thus
nothing on B except naturally B ∈L(U; H).
We now keep only the eigenvalues with diﬀerent values denoted 0 < λ1 <
λ2 · · · < λj < · · · and call Pj the projector, which has been deﬁned in (6.20)
(but now j is any integer). An element φ of H is represented as
φ =

j
(Pjφ + ¯Pjφ)
(6.47)
and
eAtφ =

j
(ei√
λjtPjφ + e−i√
λjt ¯Pjφ).
(6.48)
Call
ΠNφ =

j=1,...,N
(Pjφ + ¯Pjφ).
(6.49)
We also deﬁne the operator Γ by
Γ = 2 Re

j
PjBB∗Pj.
(6.50)
We can state the following proposition.
Proposition 6.2. Assume (6.6) to (6.9); then
ΓT h
T
=⇒Γh
in H,
∀h ∈H,
as T =⇒∞.
(6.51)
Proof. We can write, picking η in H
(ΓT h, η) = (ΓT (h −ΠNh), η) + 2T Re
N

j=1
(PjBB∗Pjh, η) + XN,T + YN,T
(6.52)

6 Controllability for skew-symmetric operators
353
with
XN,T = 2 Re
⎛
⎝
N

j=1
BB∗Pjh,

k̸=j

Pkη 1 −e−iT (√
λj+√λk)
i(
%
λj + √λk)
+ ¯
Pkη 1 −e−iT (√
λj−√λk)
i(
%
λj −√λk)
1
and
YN,T = Re
N

j=1
(BB∗Pjh, Pjη)(1 −e2iT√
λj)
i
%
λj
.
We can check that
|XN,T | ≤βN∥h∥(D(A))′∥η∥(D(A))′,
(6.53)
where βN has been deﬁned in the proof of Theorem 6.1.
Similarly
|YN,T| ≤
√
2
N

j=1
|BB∗Pjh|: ∥η∥(D(A))′.
(6.54)
Now
2T Re
N

j=1
(PjBB∗Pjh, η) = 2T (Γh, η) −2T (Γ(h −ΠNh), η).
Therefore, collecting results, we deduce noticing that ∥ΓT ∥≤T ∥BB∗∥and
∥Γ∥≤∥BB∗∥

ΓT
T −Γ

h
 ≤2∥BB∗∥: |h −ΠNh| +
βN∥h∥(D(A))′ +
√
2 ,N
j=1 |BB∗Pjh|
Tλ1
,
and the desired result follows.
⊓⊔
We next prove the following estimate.
Lemma 6.1.
|(ΓT h, η)| ≤∥BB∗∥(2|h| + T ∥h∥D(A))∥η∥(D(A))′.
(6.55)
Proof. From the deﬁnition of ΓT we have
(ΓT h, η)=4
 T
0
⎛
⎝BB∗
j
Re(eit√
λjPjh), d
dt

k
Im

eit√λk Pkη
√λk
⎞
⎠dt,
and by integrating by parts

354
III-1 Controllability and Observability
=4
⎛
⎝BB∗
j
Re(eiT√
λjPjh),

k
Im

eiT √λk Pkη
√λk
⎞
⎠
−4
⎛
⎝BB∗
j
Re(Pjh),

k
Im
 Pkη
√λk
⎞
⎠
+4
 T
0
⎛
⎝BB∗
j
Im(
%
λjeit√
λjPjh),

k
Im

eit√λk Pkη
√λk
⎞
⎠dt.
Thus the result (6.55) is easily deduced.
⊓⊔
One deduces from (6.55) that
ΓT ∈L

D(A); D(A)

.
(6.56)
Remark 6.3. The formula used in the proof of Lemma 6.1 shows also that that
if BB∗∈L

D(A); D(A)

, then
ΓT ∈L

D(A2); D(A2)

.
⊓⊔
Similarly we check the estimate
|(Γh, η)| ≤∥BB∗∥: ∥h∥D(A)∥η∥(D(A))′,
(6.57)
and then we can assert the following proposition.
Proposition 6.3. One has
ΓT
T h =⇒Γh
in D(A),
∀h ∈D(A),
as T =⇒∞.
(6.58)
Proof. Using the formulas of Proposition 6.2 and Lemma 6.1, we obtain easily
the estimate
!!!!
ΓT
T −Γ

h
!!!!
D(A)
≤2∥BB∗∥

∥h −ΠNh∥D(A) + |h −ΠNh|
T

+ βN∥h∥(D(A))′
T
b +
√
2
N

j=1
|BB∗Pjh| ,
(6.59)
and thus (6.59) follows.
⊓⊔
We now make the assumptions of Theorem 6.1. We ﬁrst notice that (Γh, h)
is also a norm on H, thanks to the following proposition.
Proposition 6.4. We make the assumptions of Theorem 6.1; then
(Γh, h) ≥c∥h∥2
(D(A))′
∀h ∈H.
(6.60)

6 Controllability for skew-symmetric operators
355
Proof. We have
(Γh, h) = 2

j
(|B∗Re Pjh|2 + |B∗Im Pjh|2),
and from (6.14) and (6.20)
≥2

j≥N+1

(M Re Pjh, Re Pjh)+(M Im Pjh, Im Pjh)

+2c1
N

j=1
|Pjh|2
λj
.
Now (6.15) to (6.17) imply that if
Pjh =
rj

k=1
¯
cjkφjk,
then

j≥N+1

(M Re Pjh, Re Pjh) + (M Im Pjh, Im Pjh)

=

j≥N+1
rj

k=1
| ¯
cjk|2
(M Re φjk, Re φjk) + (M Im φjk, Im φjk)

,
and from (6.18)
≥c0

j≥N+1
rj

k=1
| ¯
cjk|2
λj
= c0

j≥N+1
|Pjh|2
λj
.
Therefore collecting results, the desired result follows.
⊓⊔
Let us deﬁne the space F ′ completing H with the norm (6.60). Clearly one
has as for (6.31)
D(A) ⊂F ⊂H ⊂F ′ ⊂

D(A)
′.
(6.61)
As ΓT is invertible from F ′
T to FT and Γ from F ′ to F, we can consider (ΓT )−1
from FT to F ′
T and Γ −1 from F to F ′, hence also from D(A) to

D(A)
′ for
both.
In fact, it will be useful to notice that something more can be said for
Γ −1, namely the following lemma.
Lemma 6.2.
Γ −1 ∈L

D(A2); H

.
Proof. Setting φ = Γ −1h; then from the deﬁnition of Γ
2 Re

j
Pj(BB∗Pjφ) = 2 Re

j
Pjh;

356
III-1 Controllability and Observability
hence
Pj(BB∗Pjφ) = Pjh,
∀j.
Therefore
|B∗Pjφ|2 = (Pjh, ¯Pjφ),
∀j.
But from the proof of Proposition 6.4, it follows that
|B∗Pjφ|2 ≥c|Pjφ|2
λj
.
Therefore
|Pjφ| ≤λj
c |Pjh|.
Hence
|φ|2 = 2

j
|Pjφ|2 ≤2
c

j
λj|Pjh|2 ≤1
c ∥h∥2
D(A2),
which is the desired result.
⊓⊔
We can then state the the following theorem.
Theorem 6.3. We make the assumptions of Theorem 6.1; then one has
T (ΓT )−1h =⇒Γ −1h
in

D(A)
′,
∀h ∈D(A).
(6.62)
Proof. Let us set ρT = T (ΓT)−1h and ρ = Γ −1h.
We begin by proving the weak convergence in

D(A)
′. But
(h, ρT ) =
ΓT
T ρT , ρT

≥cT −T0
T
∥ρT ∥2
(D(A))′
and thus
∥ρT ∥(D(A))′ ≤2
c ∥h∥D(A)
as soon as T > 2T0.
Therefore we can extract a subsequence converging weakly to some σ in

D(A)
′. But we can write for any η in D(A)
ΓT
T η, ρT

= (h, η),
and using Proposition 6.3 we get
(Γη, σ) = (h, η).
Therefore
(Γ(σ −ρ), η) = 0

6 Controllability for skew-symmetric operators
357
for any η in D(A). Hence Γ(σ −ρ) = 0. But Γ is invertible from

D(A)
′ to
D(A); hence ρ = σ. Note that by the uniqueness of the limit, we can assert
that the full subsequence converges. To prove the strong convergence, we ﬁrst
assume that h ∈D(A2); in which case, we know that ρ belongs to H. But
ΓT
T (ρT −ρ), ρT −ρ

= (h, ρT ) −2(h, ρ) +
ΓT
T ρ, ρ

,
and therefore the right-hand side tends to 0. It easily follows that ρT tends
to ρ in

D(A)
′. In the general case, we pick a sequence hµ that belongs to
D(A2) and converges to h in D(A). Let ρµ and ρT µ correspond to hµ. We
ﬁrst notice that
∥ρT µ −ρT ∥≤2
c ∥h −hµ∥D(A).
(6.63)
Now
∥ρT −ρ∥≤∥ρT µ −ρT ∥+ ∥ρT µ −ρµ∥+ ∥ρµ −ρ∥.
From the uniform estimate (6.63) and from the fact that ∥ρT µ −ρµ∥tends
to 0 as T tends to inﬁnity and µ is ﬁxed, we deduce that ρT tends to ρ in

D(A)
′ as T tends to inﬁnity.
⊓⊔
6.4 Exact controllability for T arbitrarily small
In this section, we consider additional assumptions, which will guarantee that
the pair (A, B) is not only exactly controllable for large T , but in fact for
arbitrarily small T . We work in the framework of Theorem 6.1. We replace
(6.19) by
(M Re φj, Re φj) −(M Im φj, Im φj) −4(Λ Re φj, Re φj)
λj

≤K
λj
,
∀j ≥N + 1.
(6.64)
In fact, we shall need (6.64) for j suﬃciently large, but modifying the constant,
we can always assume that it is satisﬁed for j ≥N + 1, without loss of
generality. We also assume that
there exists a Hilbert space W such that D(A) ⊂W ⊂H,
the injection of D(A) in W is compact, and Γ ∈L(H; W ′).
(6.65)
Our objective is to prove the following.
Theorem 6.4. We make the assumptions of Theorem 6.1, except (6.19),
which is replaced by (6.64), and we assume (6.65). Then the pair A, B is
exactly controllable for any time T > 0.

358
III-1 Controllability and Observability
Proof. We begin by proving that for any T > 0, then
(ΓT h, h) = 0 =⇒h = 0.
(6.66)
This will imply that (ΓT h, h)1/2 is a norm on H, for T > 0, arbitrary.
Of course, we already know that it is true for T > T0. Naturally (6.66) is
equivalent to
ΓT h = 0 =⇒h = 0.
(6.67)
We denote by ΠJ the projector on the subspace of H, generated by the
eigenvalues ±i
%
λj, with j ≤J. We shall need to consider only J large, so in
particular J > N; hence for any element h of H, one has
ΠJh =
N

j=1
(Pjh + ¯Pjh) +
J

j=N+1
( ¯cjφj + cj ¯φj).
(6.68)
We shall prove that for any T > 0, there exists J depending on T , suﬃciently
large and a constant βJ, such that
(ΓT h, h) + βJ|ΠJh|2 ≥cT ∥h∥2
(D(A))′,
∀h ∈H.
(6.69)
Assume for a while that this is proved.
We shall then prove that
ΓT h = 0
implies h ∈D(A).
(6.70)
Indeed, from the assumption we have
B∗eA∗th = 0,
t ∈(0, T ).
(6.71)
Let τ such that T −2τ > 0. Consider a sequence of C∞functions on R,
with compact support on (0, τ), denoted by ρj, which converges to δ(0) in the
distribution sense. Deﬁne successively h1,j and h2,j by the formulas
h1,j =
 τ
0
eA∗shρj(s) ds,
(6.72)
h2j =
 τ
0
eA∗sh1,jρj(s) ds;
(6.73)
then h1,j ∈D(A), h2,j ∈D(A2).
Indeed
Ah1,j =
 τ
0
eA∗shρ′
j(s) ds,
Ah2,j =
 τ
0
eA∗sh1,jρ′
j(s) ds.
From (6.71), we deduce easily

6 Controllability for skew-symmetric operators
359
B∗eA∗th1,j = 0,
t ∈(0, T −τ),
B∗eA∗th2,j = 0,
t ∈(0, T −2τ).
(6.74)
As h2,j ∈D(A2), we deduce by diﬀerentiating twice the second relation that
B∗eA∗tA2h2,j = 0,
t ∈(0, T −2τ).
Moreover A2h2,j ∈H. Therefore, we may apply (6.65) with h = A2h2,j, and
T changed into T −2τ. As ΓT −2τA2h2j = 0, we deduce that A2h2,j remains
in a bounded subset of

D(A)
′; hence h2,j remains in a bounded subset of
D(A). On the other hand, h2j converges to h in

D(A)
′ weakly. This follows
from the fact that if φ ∈D(A), then
⟨φ, h2,j⟩

[A + I]
 τ
0
eAsφρj(s) ds, [A + I]−1
 τ
0
eA∗shρj(s) ds

and
[A + I]−1
 τ
0
eA∗shρj(s) ds
tends to h in H strongly, and
[A + I]
 τ
0
eAsφρj(s) ds
tends to φ in H weakly. Therefore h ∈D(A).
Let Y be the subspace of H of elements such that h ∈H, ΓT h = 0. Our
objective is to prove that Y = {0}. We ﬁrst prove that Y is ﬁnite dimensional.
But if h ∈Y , then one can write
ΓT h + βJΠJh = βJΠJh;
hence
h = βJ(ΓT + βJΠJ)−1ΠJh
and
ΠJh = βJΠJ(ΓT + βJΠJ)−1ΠJh.
(6.75)
These formulas prove that Y is a vector subspace of the span of (ΓT +
βJΠJ)−1φj, (ΓT + βJΠJ)−1 ¯φj, j = 1, . . . , J, where we have written for sim-
plicity ΠJ in (6.75) as
ΠJh =
J

j=1
( ¯cjφj + cj ¯φj),
which is always possible by a convenient renumbering.
Now, thanks to (6.70), Ah ∈Y . Furthermore, if h ∈Y , A2h ∈Y and thus

360
III-1 Controllability and Observability
ΓT A2h = βJΠJA2h = βJΠJA2h
and also
(ΓT A2h, A2h) + βJ(ΠJA2h, A2h) = βJ(ΠJA2h, A2h).
Therefore, by (6.69), we deduce
cT ∥A2h∥2
(D(A))′ ≤βJ(λJ)2|h|2
or
cT |Ah|2
H ≤βJ(λJ)2|h|2,
which proves that A is linear continuous on Y . But then Y can be spanned
by the eigenvectors of A. If φj is an eigenvector of A belonging to Y , then
B∗φjeit√
λj = 0,
∀t ∈(0, T ).
By the analyticity property, it follows that
B∗φjeit√
λj = 0,
∀t ∈(−∞, +∞).
In particular (ΓT φj, φj) = 0, ∀T > 0. This is impossible, because we know
that for T suﬃciently large (ΓT h, h)1/2 is a norm on H. Therefore Y = {0},
and (6.67) is proved. It is easy to check then, as a consequence of (6.67) and
(6.69), one has also
(ΓT h, h) ≥c′
T ∥h∥2
(D(A))′,
∀h ∈H, ∀T > 0.
(6.76)
Suppose that (6.76) is not true; then there exists a sequence {hn}, ∥hn∥(D(A))′ =
1, such that (ΓT hn, hn) tends to 0. As (ΓT h, h)1/2 is a norm on H, we have
hn →0 in FT (completed of H with the norm (ΓT h, h)1/2). As hn is bounded
in

D(A)
′, it converges also weakly to 0 in

D(A)
′; hence, Πjhn →0. But
then from (6.69), ∥hn∥(D(A))′ →0, which is impossible.
It remains to prove (6.69). It is suﬃcient to prove that
∀T > 0, ∃J = JT
and
cT such that
(ΓT (h −ΠJh), h −ΠJh) ≥cT ∥h −ΠJh∥2
(D(A))′.
(6.77)
Now
(ΓT (h −ΠJh), h −ΠJh) = 4
 T
0

B∗
⎛
⎝
j≥J+1
Re(e−ti√
λj ¯cjφj)
⎞
⎠

2
dt
and with calculations already done
≥2T

j≥J+1
∥cj∥2
(M Re φj, Re φj) + (M Im φj, Im φj)

+ Z3 + Y ′
3,

6 Controllability for skew-symmetric operators
361
with (recalling that aj = cj/
%
λj)
Z3 = −8
⎛
⎝Λ
⎡
⎣
j≥J+1
Re φj Im(e−iT√
λj ¯aj)
⎤
⎦,

k≥J+1
Re φk
Re(eiT √λk ¯ak)
√λk
⎞
⎠
+ 8
⎛
⎝Λ
⎡
⎣
j≥J+1
Re φj Im( ¯aj)
⎤
⎦,

k≥J+1
Re φk
Re( ¯ak)
√λk
⎞
⎠
+ 8

j≥J+1
(Λ Re φj, Re φj) Im(e−iT√
λj ¯aj)Re(e−iT√
λj ¯aj)
%
λj
−8

j≥J+1
(Λ Re φj, Re φj) Im( ¯aj)Re( ¯aj)
%
λj
and
Y ′
3 =

j≥J+1
[(M Re φj, Re φj) −(M Im φj, Im φj)]Im(c2
j(e2iT√
λj −1))
%
λj
.
Again we can write
Z3 + Y ′
3 = T3 + T ′
3,
where
T3 = −8
⎛
⎝Λ
⎡
⎣
j≥N+1
Re φj Im(e−iT√
λj ¯aj)
⎤
⎦,

k≥N+1
Re φk
Re(e−iT √λk ¯ak)
√λk
⎞
⎠
+ 8
⎛
⎝Λ
⎡
⎣
j≥N+1
Re φj Im( ¯aj)
⎤
⎦,

k≥N+1
Re φk
Re( ¯ak)
√λk
⎞
⎠
and
T ′
3 =

j≥N+1

(M Re φj, Re φj) −(M Im φj, Im φj)
−4(Λ Re φj, Re φj)
λj
	Im(c2
j(e2iT√
λj −1))
%
λj
.
We use (6.64) to assert that
|T ′
3| ≤2K

j≥J+1
|cj|2
λ3/2
j
= 2K

j≥J+1
|aj|2
λ1/2
j
.
Next, from (6.65) we can assert that, for any ε, there exists C(ε) such that
∥h∥W ≤ε∥h∥D(A) + C(ε)|h|

362
III-1 Controllability and Observability
(see J. L. Lions [1]). Therefore, as easily seen
|T3|≤8∥Λ∥L(H;W ′)·
⎡
⎢⎣ε

j≥J+1
|¯aj|2+C(ε)
⎛
⎝
j≥J+1
|¯aj|2
⎞
⎠
1/2 ⎛
⎝
j≥J+1
|¯aj|2
λj
⎞
⎠
1/2⎤
⎥⎦.
Therefore, collecting results we have
(ΓT (h −ΠJh), h −ΠJh)
≥∥h −ΠJh∥2
(D(A))′

c0T −
K
√λJ
−4∥Λ∥ε −C(ε)
%
λj

.
(6.78)
Therefore for any T > 0, choose
ε = c0T
8∥Λ∥,
and J suﬃciently large so that
c0T
2
> K + C(ε)
√λJ
.
This concludes (6.77).
⊓⊔
Remark 6.4. The idea of the regularizing functions ρj has been given to us by
J. L. Lions [6].
⊓⊔
7 General framework: skew-symmetric operators
7.1 Operator A
Let L be a self-adjoint operator in a Hilbert space HL (which is identiﬁed
with its dual). Let D(L) be the domain of L. We assume that there exists a
Hilbert space VL such that
VL ⊂HL ⊂V ′
L,
(7.1)
continuously and densily embedded.
We assume that
⟨Lz1, z2⟩= ((z1, z2)),
∀z1, z2 ∈VL,
(7.2)
where (( , )) denotes the scalar product in VL.
We furthermore assume that
the injection of VL into HL is compact.
(7.3)

7 General framework: skew-symmetric operators
363
In that case L−1 ∈L(HL; HL) is compact and thus there exists an or-
thonormal base wj of HL, of eigenvectors of L; i.e.,
Lwj = λjwj,
|wj|HL = 1,
with 0 < λ1 ≤λ2 · · · ≤λj · · · ↑∞.
Besides D(L) we shall need
∆(L) = {z ∈VL : Lz ∈VL}
and we have the sequence of spaces, each of them being densily and continu-
ously embedded in the next one
∆L ⊂D(L) ⊂VL ⊂HL ⊂V ′
L ⊂

D(L)
′ ⊂∆′
L.
We can associate with L a skew-symmetric operator A, satisfying the assump-
tions (6.6) to (6.9). Take indeed
H = HL × V ′
L,
A =

0 I
−L 0
	
,
D(A) = VL × HL.
(7.4)
We have to pay attention to the fact that because H is identiﬁed with its
dual, V ′
L is in the second component the pivot space, identiﬁed with its dual.
In that framework the dual of HL is

D(L)
′, with the duality
⟨z, ζ⟩= (z, L−1ζ)HL,
∀z ∈HL, ζ ∈

D(L)
′.
Note that the scalar product in H is expressed as follows:
(h, h′)H = (h1, h′
1) + ⟨L−1h2, h′
2⟩.
Moreover

D(A)
′ = V ′
L ×

D(L)
′.
The eigenvectors of A are given by
φj =
1
√
2

wj
i
%
λjwj
	
.
7.2 Operator B
We next deﬁne the operator B. We shall consider a control space U = U1 ×U2
where U1, U2 are Hilbert spaces, identiﬁed with their duals. Let q1, q2 be
operators such that
q1 ∈L(D(L); U1),
q2 ∈L(VL; U2).
(7.5)
We set
Bv = −

L−1q∗
1v1
q∗
2v2
	
(7.6)
and B ∈L(U; H). Note that
B∗h = −

q1L−1h1
q2L−1h2
	
.
(7.7)
The presence of L−1 in the second component of B∗arises from the fact that
the pivot space is V ′
L.

364
III-1 Controllability and Observability
7.3 Dynamical system
Consider the dynamic system
z′ = Az + Bv,
z(0) = 0,
z ∈C([0, T ]; H),
z′ ∈L2
0, T ;

D(A)
′
;
(7.8)
we deduce
z′
1 = z2 −L−1q∗
1v1,
z1(0) = 0,
z′
2 = −Lz1 −q∗
2v2,
z2(0) = 0.
(7.9)
Hence z1 ∈C([0, T ]; HL), z2 ∈C([0, T ]; V ′
L); z′
1 ∈L2(0, T ; V ′
L), z′
2 ∈
L2
0, T ;

D(L)
′
.
We can associate with (7.9) a second order (in time) equation as follows.
Set η = z2; then we also write
η′′ + Lη = q∗
1v1 −q∗
2v′
2,
η(0) = 0,
η′(0) + q∗
2v2(0) = 0,
η ∈C([0, T ]; V ′
L),
η′ + q∗
2v2 ∈C

[0, T ];

D(L)
′
,
η′ ∈L2
0, T ;

D(L)
′
,
η′′ ∈C([0, T ]; ∆′
L) ⊕L2
0, T ;

D(L)
′
⊕H−1(0, T ; V ′
L).
(7.10)
Note that all terms in (7.10) make perfect sense. However, it is important
to notice that the value of η′ is not deﬁned at each point, and is not 0 in
any sense in general. It is useful to reinterpret (7.10) in terms of the Method
of Transposition of J. L. Lions and E. Magenes [1]. Note that for any
ψ0 ∈D(L), ψ1 ∈VL, there exists one and only one solution ψ such that
ψ′′ + Lψ = f,
ψ(T ) = ψ0,
ψ′(T ) = ψ1,
ψ ∈C

[0, T ]; D(L)

,
ψ′ ∈C([0, T ]; VL).
(7.11)
By making appropriate integration by parts between (7.10) and (7.11), we
deduce the formula
⟨ψ1, η(T )⟩−⟨ψ0, η′(T ) + q∗
2v2(T )⟩=
 T
0
[(v1, q1ψ) + (v2, q2ψ′)] dt,
(7.12)
which can be considered as the deﬁnition of η(T ) and η′(T ) + q∗
2v2(T ).
There is a formal but mnemonic way of writing (7.10). Introduce the op-
erator
J0 ∈L

H1(0, T ; U2); L2(0, T ; U2)

deﬁned by
J0u = u′.

7 General framework: skew-symmetric operators
365
Its transpose J∗
0 ∈L

L2(0, T ; U2);

H1(0, T ; U2)
′
. One should not mix up
J∗
0 u, which belongs to

H1(0, T ; U2)
′, with −u′, which belongs to H−1(0, T ; U2).
They coincide only, when applied to H1
0(0, T ; U2). Next, consider q2 as a linear
bounded operator from H1(0, T ; VL) to H1(0, T ; U2), and thus q∗
2 as an ele-
ment of L

H1(0, T ; U2)
′;

H1(0, T ; VL)
′
. Therefore q∗
2J∗
0 ∈L

L2(0, T ; U2);

H1(0, T ; VL)
′
. We write (7.10) as
η′′ + Lη = q∗
1v1 + q∗
2J∗
0 v2,
η(0) = 0,
η′(0) = 0.
(7.13)
We know that
η ∈C([0, T ]; V ′
L) ∩L2(0, T ; HL),
η′ ∈L2
0, T ;

D(L)
′
.
From (7.13) we read
η′′ ∈C([0, T ]; ∆′
L) ⊕L2
0, T ;

D(L)
′
⊕

H1(0, T ; VL)
′.
The writing is formal as far as η′(0) (which is not deﬁned) and η′′ are con-
cerned. What it means is that for any
ψ ∈H1
0, T ; D(L)

∩L2(0, T ; ∆L)
with ψ(T ) = 0, then one has
−
 T
0
(η′, ψ′) dt +
 T
0
(η, Lψ) dt =
 T
0
[(v1, q1ψ) + (v2, q2ψ′)] dt,
(7.14)
where all terms make sense. This equation deﬁnes a unique η because it implies
in particular (7.12). The value of η′ at any given time t (in particular 0) is
not the value of the derivative, but a short for η′ + q∗
2v2.
7.4 Exact controllability
Let us ﬁrst express the operator ΓT ∈L(H; H). By the deﬁnition
(ΓT h, k) =
 T
0
(B∗eA∗th, B∗eA∗tk) dt.
(7.15)
If we write
h =
h1
h2
	
,
k =
k1
k2
	
and deﬁne φ, ψ by
φ′′ + Lφ = 0,
ψ′′ + Lψ = 0,
φ(0) = L−1h1,
ψ(0) = L−1k1,
φ′(0) = −L−1h2,
ψ′(0) = −L−1k2,
(7.16)

366
III-1 Controllability and Observability
we deduce easily from (7.7) that
B∗eA∗th =

−q1φ(t)
q2φ′(t)
	
,
(7.17)
(ΓT h, k) =
 T
0
[(q1φ, q1ψ) + (q2φ′, q2ψ′)] dt.
(7.18)
Therefore the estimate
(ΓT h, k) ≥c(T −T0)∥h∥2
(D(A))′
is clearly equivalent to
 T
0
(|q1φ|2 + |q2φ′|2) dt ≥c(T −T0)(∥φ(0)∥2
VL + |φ′(0)|2
HL).
Moreover
ΓT h =

−L−1
η′(T ) + q∗
2q2φ′(T )

η(T )
	
,
(7.19)
where η is the solution of (7.10) with
v1(t) = −q1φ(t),
v2(t) = q2φ′(t).
If we use the formulation (7.13) for the η equation, then we may write
ΓT h =
−L−1η′(T )
η(T )
	
,
(7.20)
but η′(T ) is not the value of η′ at T , which is not meaningful because η′ is
only an L2 function, but a mnemonic to denote an element of

D(L)
′ deﬁned
by the relation

η′(T ), ψ(T )

−
 T
0
(η′, ψ′) dt +
 T
0
(η, Lψ) dt
=
 T
0
[(v1, q1ψ) + (v2, q2ψ′)] dt
(7.21)
for any ψ ∈H1
0, T ; D(L)

∩L2(0, T ; ∆L).
Let us now try to check how we can satisfy the suﬃcient assumptions of
exact controllability as stated in Theorem 6.1 and ﬁnd the operators M, Λ
satisfying (6.14) and (6.19). We shall suppose that there exist operators π1,
π2 and a form b(ξ, ξ′) such that
π1 ∈L

D(L); D(L)′
,
π2 ∈L(VL; V ′
L),
b is bilinear continuous on HL × VL,
|q1ζ|2 ≥(π1ζ, ζ),
∀ζ ∈D(L),
|q2ζ|2 ≥(π2ζ, ζ),
∀ζ ∈VL,
(7.22)

8 Exact controllability of hyperbolic equations
367
(π1wj, wk) = λjb(wj, wk) + λkb(wk, wj),
∀j ̸= k ≥N + 1,
(7.23)
b(wj, wk) + b(wk, wj) + (π2wj, wk) = 0,
∀j ̸= k ≥N + 1,
(7.24)
1
λj
(π1wj, wj) + (π2wj, wj) ≥2c0,
∀j ≥N + 1,
(7.25)

(π1wj, wj)
λ
3
2
j
−(π2wj, wj)
λ1/2
j
−4b(wj, wj)
λ1/2
j

≤2K,
∀j ≥N + 1.
(7.26)
We then deﬁne
(Mξ, η) = (π1L−1ξ1, L−1η1) + (π2L−1ξ2, L−1η2),
∀ξ, η ∈H,
(7.27)
(Λξ, η) = b(ξ1, η1),
∀ξ ∈H, η ∈D(A).
(7.28)
It is easy to check that all assumptions (6.14)–(6.19) are satisﬁed.
As a consequence of Theorem 6.1 we can state the following theorem.
Theorem 7.1. Consider the pair (A, B) deﬁned in §7.1 and §7.2. Assume
(7.22) to (7.26) and (6.20). Then the pair (A, B) is exactly controllable for T
suﬃciently large.
Let us check what must be added to obtain exact controllability at any
T > 0. We replace (7.26) by

(π1wj, wj)
λj
−(π2wj, wj) −4b(wj, wj)
 ≤2K,
∀j ≥N + 1.
(7.29)
We also assume that there exists a Hilbert space WL such that
VL ⊂WL ⊂HL, the injection of VL in WL is compact
b is bilinear continuous on HL × WL.
(7.30)
Then (7.29) implies (6.64) and (7.30) implies (6.65).
We can then state the following theorem.
Theorem 7.2. We make the assumptions of Theorem 7.1, except (7.26),
which is replaced by (7.29), and we assume (7.30); then the pair (A, B) is
exactly controllable for any T > 0.
8 Exact controllability of hyperbolic equations
We shall apply in this section the results of §6 and §7 to the exact con-
trollability of the wave equation, Maxwell equations, and the plate equation
with boundary control. The domain Ωconsidered in the sequel is smooth and
bounded.

368
III-1 Controllability and Observability
8.1 Wave equation with Dirichlet boundary control
We take L = −∆, with
HL = L2(Ω),
VL = H1
0(Ω),
D(L) = H2(Ω) ∩H1
0(Ω),
∆L = {z ∈D(L): ∆z ∈H1
0(Ω)}.
We take U1 = U2 = L2(Γ) and q1 = ∂/∂ν, q2 = 0. Let
m(x) = x −x0,
R(x0) = sup
x∈Γ
|m(x)|.
We deﬁne
(π1ζ, ζ′) =
1
R(x0)

Γ
m · ν ∂ζ
∂ν
∂ζ′
∂ν dΓ,
π2 = 0,
(8.1)
b(z, ζ) = −
1
R(x0)

Ω

α
mαz ∂ζ
∂xα
dx.
(8.2)
Consider the eigenvectors wj deﬁned by
−∆wj = λjwj,
wj

Γ = 0;
(8.3)
then we have the relation

Γ
m · ν ∂wj
∂ν
∂wk
∂ν dΓ
= (2 −n)
%
λjλk δjk −

Ω

α
mα

λjwj
∂wk
∂xα
+ λkwk
∂wj
∂xα

dx
(8.4)
for any j, k. Then taking N = 0, all assumptions of Theorem 7.1 are satisﬁed.
The system to be controlled is, according to (7.12)
⟨ψ1, η(T )⟩−⟨ψ0, η′(T )⟩=
 T
0

v1, ∂ψ
∂ν

dt,
(8.5)
where ψ satisﬁes
⎧
⎪
⎨
⎪
⎩
ψ′′ −∆ψ = 0,
ψ

Γ = 0,
ψ(T ) = ψ0,
ψ′(T ) = ψ1,
ψ0 ∈H2 ∩H1
0,
ψ1 ∈H1
0.
(8.6)
The relation (8.6) is the “Method of Transposition” deﬁnition of
⎧
⎪
⎪
⎨
⎪
⎪
⎩
η′′ −∆η = 0,
ηΓ = −v1,
η(0) = η′(0) = 0.
(8.7)

8 Exact controllability of hyperbolic equations
369
The exact controllability property is expressed as follows: Given y0 ∈L2,
y1 ∈H−1, for T suﬃciently large, the system
⎧
⎪
⎪
⎨
⎪
⎪
⎩
η′′ −∆η = 0,
φ′′ −∆φ = 0,
η

Γ = ∂φ
∂ν ,
φΓ = 0,
η(0) = η′(0) = 0,
η(T ) = y0,
η′(T ) = y1
(8.8)
has a solution with φ(0) ∈H1
0, and φ′(0) ∈L2.
8.2 Wave equation with Neumann boundary control
We take now
L = −∆+ I,
HL = L2(Ω),
VL = H1(Ω),
D(L) =

z ∈H2(Ω): ∂z
∂ν = 0

,
∆L =

z ∈H3(Ω): ∂z
∂ν = 0

.
The eigenvectors are deﬁned by
⎧
⎨
⎩
−∆wj + wj = λjwj,
∂wj
∂ν = 0.
(8.9)
Note that λ1 = 1 and w1 = 1/|Ω|.
The eigenspace corresponding to λ1 is one dimensional. We next take U1 =

L2(Γ1)
n+1, U2 = L2(Γ0) (see (5.21) for the deﬁnition of Γ0 and Γ1), and
q1z =
 γ1z
γ1Dz
	
for z ∈D(L),
q2z = γ0z,
for z ∈H1.
(8.10)
Note that for
∀z ∈D(L),
γ1Dz = Dσ,1γ1z.
See §5.3 for the notation. Deﬁne then
(π1ζ, ζ′) = −
1
R(x0)

Γ
m · ν(ζζ′ + Dζ · Dζ′) dΓ,
∀ζ, ζ′ ∈D(L),
(8.11)
(π2ζ, ζ′) =
1
R(x0)

Γ
m · νζζ′ dΓ,
∀ζ, ζ′ ∈H1.
(8.12)
Similarly deﬁne
b(z, ζ) = −
1
R(x0)

Ω

α
mαz ∂ζ
∂xα
dx.
(8.13)

370
III-1 Controllability and Observability
We use the following relation among eigenvalues:

Γ
m · ν(wjwk + DwjDwk) dΓ = (n −2)
%
λjλkδjk + 2δjk
+

Ω

α
mα

λjwj
∂wk
∂xα
+ λkwk
∂wj
∂xα

dx.
(8.14)
Pick N = 1. Consider j ̸= k ≥2. It is easily checked that the properties (7.22)
to (7.26) are satisﬁed. Let us check (6.20). It follows from the fact that
|B∗φ1|2 = |Γ|
2|Ω|.
Therefore all assumptions of Theorem 7.1 are satisﬁed.
The system to be controlled is, according to (7.12),
⟨ψ1, η(T )⟩−⟨ψ0, η′(T ) + q∗
2v2(T )⟩
=
 T
0

Γ1
(v10ψ +
n

i=1
v1i
∂ψ
∂xi
) dΓ +
 T
0

Γ0
v2ψ′ dΓ,
(8.15)
where
v1 =
⎡
⎢⎢⎢⎢⎣
v10
v12
. . .
. . .
v1n
⎤
⎥⎥⎥⎥⎦
and ψ is the solution of
⎧
⎪
⎪
⎨
⎪
⎪
⎩
ψ′′ −∆ψ + ψ = f,
∂ψ
∂ν = 0,
ψ(T ) = ψ′(T ) = 0.
(8.16)
We can consider v1 as an element of

H1(Γ1)
′, by the following duality for-
mula (see also (5.29)):
⟨v1, θ⟩=

Γ1
0
v10γ1θ +
n

i=1
v1iγ1
∂θ
∂xi
1
dΓ.
(8.17)
Consider the special case
v10 = γ1χ,
v1i = γ1
∂χ
∂xi
with ∂χ
∂ν = 0.
(8.18)
Then

8 Exact controllability of hyperbolic equations
371
⟨v1, θ⟩=

Γ1

γ1χγ1θ +
n

i=1
γ1
∂χ
∂xi
γ1
∂θ
∂xi

dΓ
=

Γ1

γ1χγ1θ +
n

i=1
σi,1γ1χσi,1γ1θ

dΓ
=

Γ1
{γ1χ −∆σ1γ1χ} γ1θ dΓ,
(8.19)
where
−∆σ1 =

i
σ∗
i,1σi,1
represents the tangential Laplacian on Γ1, which is an element of L

H1(Γ1);

H1(Γ1)
′
.
So we can write (8.15) as
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
η′′ −∆η + η = 0,
∂η
∂ν

Γ0
= −J∗
0 v2,
∂η
∂ν

Γ1
= v1,
η(0) = η′(0) = 0,
(8.20)
with the interpretation of the operator J0 already discussed in §7.3.
Let us express the property of exact controllability. Let y0 ∈L2, y1 ∈

H1(Ω)
′; then for T suﬃciently large, the system
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
η′′ −∆η + η = 0,
φ′′ −∆φ + φ = 0,
∂η
∂ν

Γ0
= −J∗
0φ′,
∂φ
∂ν = 0,
∂η
∂ν

Γ1
= ∆σ1φ −φ,
η(T ) = y0,
η′(T ) = y1,
η(0) = 0,
η′(0) = 0
(8.21)
has a solution with φ(0) ∈H1, φ′(0) ∈L2. We recall that J∗
0 φ′ belongs to

H1
0, T ; L2(Γ0)
′
and −∆σ1φ + φ to L2
0, T ;

H1(Γ1)
′
.
8.3 Maxwell equations
Let us introduce some notations. Let L2(Ω) =

L2(Ω)
3, L2(Γ) =

L2(Γ)
3
and similarly, Hk(Ω) =

Hk(Ω)
3. If v ∈H1(Ω), we set curl v = D ×v, where
× symbolizes the external product. We recall the relations
curl grad v = 0,
div curl v = 0.
Furthermore, the set Ωbeing assumed smooth and singly connected, we have
curl u = 0 =⇒u = Dp,
div u = 0 =⇒u = curl v.

372
III-1 Controllability and Observability
Deﬁne
J = closure in L2(Ω) of functions belonging to

C∞(¯Ω)
3 with divergence 0,
K = closure in L2(Ω) of functions belonging to

C∞
0 (Ω)
3 with divergence 0,
J1 ={v ∈H1(Ω): div v = 0, v × ν = 0},
K1 ={v ∈H1(Ω): div v = 0, v · ν = 0},
where ν represents the external unit normal. We provide J1 and K1 with the
scalar product

(u, v)

= (curl u, curl v)
(8.22)
for which they become Hilbert spaces. Indeed if v ∈J1, curl v = 0, then
v = curl χ,
curl curl χ = 0.
Using the integration by parts formula
(curl φ, ψ) = (φ, curl ψ) +

Γ
φ × ν · ψ dΓ,
(8.23)
we deduce easily that curl χ = 0; hence v = 0. Similarly if v ∈K1, curl v = 0,
then v = Dp and ∆p = 0, with ∂p/∂ν = 0; hence p is a constant, which
implies again v = 0. It can be proved (see (8)) that
J1 ⊂J,
K1 ⊂K
(8.24)
with continuous and dense embedding. Moreover curl is an isometry from J1
to K, and K1 to J. The norm thus deﬁned on J1 and K1 is equivalent to that
induced by H1(Ω). We shall further need the spaces
J2 =

v ∈H2(Ω): div v = 0, v × ν = 0, ν · curl v = 0
 
,
J3 =

v ∈H3(Ω): div v = 0,
v × ν = 0,
ν · curl v = 0,
ν × curl curl v = 0

(8.25)
and
K2 =

v ∈H2(Ω): div v = 0, v · ν = 0, ν × curl v = 0
 
K3 =

v ∈H3(Ω): div v = 0,
ν · v = 0,
ν × curl v = 0,
ν · curl curl v = 0

.
(8.26)
The spaces J2 and K2 are provided with the norms
∥v∥= |curl curl v|

8 Exact controllability of hyperbolic equations
373
and J3, K3 with the norms
∥v∥= |curl curl curl v| .
Then curl is an isometry from J2 to K1, J3 to K2, K2 to J1, and K3 to J2.
Note that from (8.23) we have
(curl φ, ψ) = (curl ψ, φ)
(8.27)
with φ ∈J1, ψ ∈K1, or φ ∈K1, ψ ∈J1. It follows that curl can be extended
as an isometry from J to (K1)′ and K to (J1)′. Similarly it extends as an
isometry from (K1)′ to (J2)′, (J1)′ to (K2)′, (J2)′ to (K3)′, and (K2)′ to
(J3)′.
Summarizing we have the sequences
J3 ⊂J2 ⊂J1 ⊂J ⊂(J1)′ ⊂(J2)′ ⊂(J3)′,
K3 ⊂K2 ⊂K1 ⊂K ⊂(K1)′ ⊂(K2)′ ⊂(K3)′,
(8.28)
with each space being continuously and densely embedded into the next one.
Moreover curl curl is an isometry from J3 to J1, J2 to J, J1 to (J1)′, J to
(J2)′, and (J1)′ to (J3)′, and a similar result with J changed into K.
Deﬁne
HL = K,
VL = K1,
D(L) = K2,
∆(L) = K3,
L = curl curl .
(8.29)
The eigenvectors wj are given by the relations
⎧
⎪
⎨
⎪
⎩
curl curl wj = λjwj,
div wj = 0,
ν · wj = 0,
ν × curl wj = 0
on Γ,
|wj| = 1.
(8.30)
We shall use the following relations among eigenvectors:

Γ
m · ν curl wj · curl wk dΓ = (n −2)
%
λjλk δjk
+

Ω

α
mα

λjwj · ∂wk
∂xα
+ λkwk · ∂wj
∂xα

dx
(8.31)
with again m(x) = x −x0.
Deﬁne next
U2 = L2(Γ0),
U1 = {v ∈L2(Γ1): ν × v = 0},
q1 = −γ1 curl,
q2 = −γ0.
We can take

374
III-1 Controllability and Observability
(π1ζ, ζ′) = −
1
R(x0)

Γ
m · ν curl ζ · curl ζ′ dΓ,
∀ζ, ζ′ ∈K2,
(8.32)
(π2ζ, ζ′) =
1
R(x0)

Γ
m · νζ · ζ′ dΓ,
∀ζ, ζ′ ∈K1,
(8.33)
b(z, ζ) = −
1
R(x0)

Ω

α
mαz · ∂ζ
∂xα
dx,
∀z ∈K, ζ ∈K1.
(8.34)
Then all assumptions of Theorem 7.1 are satisﬁed, with N = 0.
We next interpret the system to be controlled. Consider (7.11) and (7.12);
we get for ψ0 ∈K2, ψ1 ∈Kj
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
ψ′′ + curl curl ψ = 0,
div ψ = 0,
ν · ψ = 0,
ν × curl ψ = 0
on Γ,
ψ(T ) = ψ0,
ψ′(T ) = ψ1,
(8.35)
⟨ψ1, η(T )⟩−⟨ψ0, η′(T ) + q∗
2v2(T )⟩
= −
 T
0

Γ1
v1 · γ1 curl ψ dΓ +

Γ0
v2 · γ0ψ′ dΓ
	
dt.
(8.36)
Now, we use the formula (see (5.22))
γ curl ψ = ν × ∂ψ
∂ν + Dσ × γψ
on Γ
(8.37)
and similar relations on Γ0 and Γ1. Using the fact that v1 belongs to U1, we
have

Γ1
v1 · γ1 curl ψ dΓ = −⟨D∗
σ1 × v1, γ1ψ⟩,

Γ0
v2 · γ0ψ′ dΓ = ⟨J∗
0 v2, γ0ψ⟩.
We then can write (8.36) as follows (using the Method of Transposition):
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
η′′ + curl curl η = 0,
div η = 0,
η(0) = η′(0) = 0,
ν × curl η

Γ1 = D∗
σ1 × v1,
ν × curl η

Γ0 = −J∗
0 v2.
(8.38)
We see that ν × η

Γ1 belongs to L2(0, T ; H1(Γ1)′) and ν × η

Γ0 belongs to

H1(0, T ; L2(Γ0)
′. Let us then express the condition of exact controllability.
Given y0 in K and y1 in (K1)′, we consider the system

8 Exact controllability of hyperbolic equations
375
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
η′′ + curl curl η = 0,
div η = 0,
η(0) = η′(0) = 0,
ν × curl η

Γ1 = D∗
σ1 × γ1 curl φ,
ν × curl η

Γ0 = J∗
0 φ′,
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
φ′′ + curl curl φ = 0,
div φ = 0,
η(T ) = y0,
η′(T ) = y1,
φ · ν = 0,
ν × curl φ = 0,
(8.39)
which has a solution with φ(0) ∈K1 and φ′(0) ∈K, at least for T suﬃciently
large. Furthermore we know that η ∈C([0, T ]; K′
1) and η′ ∈L2([0, T ]; K′
2).
Reducing (8.39) to Maxwell equations by setting
H = curl η,
E = −η′,
ψ = −curl−1 φ′,
we deduce the relations
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
E′ = curl H,
H′ + curl E = 0,
div E = div H = 0,
ν × H

Γ1 = D∗
σ1 × γ1ψ′,
ν × H

Γ0 = J∗
0 φ′,
E(0) = H(0) = 0,
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
φ′ + curl ψ = 0,
ψ′ = curl φ,
div φ = div ψ = 0,
E(T ) = ET ,
H(T ) = HT ,
φ · ν = 0,
ν × ψ = 0,
(8.40)
with
H ∈C([0, T ]; J′
2),
E ∈L2([0, T ]; K′
2),
H′ ∈L2([0, T ]; J′
3),
E′ ∈L2([0, T ]; K′
3),
φ ∈C([0, T ]; K1),
ψ ∈C([0, T ]; J1),
φ′ ∈C([0, T ]; K),
ψ′ ∈C([0, T ]; J).
The values ET and HT are any elements of K′
1 and J′
1, respectively. The
system (8.40) has a solution for T suﬃciently large.
We now turn to another situation where we shall be able to use The-
orem 6.2 instead of Theorem 6.1 (which permitted to state Theorem 7.1).
However we assume the geometric condition
m · ν ≥0.
(8.41)
Consider again (8.29). Take U1 = U2 = L2(Γ) and q1 = −γ, q2 = 0. We
deﬁne A, B as in §7.1 and §7.2. We note that B ∈D(A) and thus (6.31) is
satisﬁed. We then take
(Mξ, η) =
1
R(x0)

Γ
m · νL−1ξ1 · L−1η1 dΓ,
(8.42)
(Λξ, η) =
1
R(x0)

Ω

α
mαξ1 · ∂η1
∂xα
dx,
(8.43)
(Qξ, η) =
1
R(x0)

Γ
m · ν curl L−1ξ1 · curl L−1η1 dΓ.
(8.44)

376
III-1 Controllability and Observability
Let us check the assumptions of Theorem 6.2, with N = 0. Thanks to
(8.41) the assumption (6.14) is satisﬁed and (6.15) is trivial. The property
(6.33) is clear from the deﬁnition of φj and that of M, Λ. From the deﬁnition
of Q and the relation (8.31) the property (6.34) is easily checked. It remains
to check (6.35) and (6.36). But
(M Re φj, Re φj) =
1
2R(x0)λ2
j

Γ
m · ν|wj|2 dΓ,
(Q Re φj, Re φj) =
1
2R(x0)λ2
j

Γ
m · ν| curl wj|2 dΓ
and from (8.31)
(M Re φj, Re φj) −1
λj
(Q Re φj, Re φj) =
1
R(x0)λ2
j
;
therefore (6.35) is satisﬁed with
c0 =
1
R(x0).
Now
(M Re φj, Re φj) + 1
λj
(Q Re φj, Re φj) =
1
R(x0)λ2
j

Γ
m · ν|wj|2 dΓ −1

and

Γ
m · ν|wj|2 dΓ = 2

Ω

α
mαwj · ∂wj
∂xα
dx + n
= 4R(x0)(Λ Re φj, Re φj) + n,
and in particular the property (6.36) follows. The dynamic system is given by
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
η′′ + curl curl η = 0,
div η = 0,
η(0) = η′(0) = 0,
ν × curl η

Γ = −v.
(8.45)
We express the property of exact controllability as follows.
The system
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
η′′ + curl curl η = 0,
div η = 0,
η(0) = η′(0) = 0,
ν × curl η

Γ = γφ,
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
φ′′ + curl curl φ = 0,
div φ = 0,
η(T ) = y0,
η′(T ) = y1,
φ · ν = 0,
ν × curl φ = 0
(8.46)

8 Exact controllability of hyperbolic equations
377
has a solution for T suﬃciently large, with y0 ∈K1, y1 ∈K. Moreover
φ(0) ∈K, φ′(0) ∈(K1)′. We have η ∈C([0, T ]; K), η′ ∈C([0, T ]; (K1)′).
Reducing to Maxwell equations as above we obtain the system
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
E′ = curl H,
H′ + curl E = 0,
div E = div H = 0,
ν × H

Γ = γφ,
E(0) = H(0) = 0,
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
φ′ + curl ψ = 0,
ψ′ = curl φ,
div φ = div ψ = 0,
E(T ) = ET ,
H(T ) = HT ,
φ · ν = 0,
ν × ψ = 0.
(8.47)
For any given ET and HT in K and J, respectively, and for suﬃciently large
T , the system (8.47) has a solution such that
H ∈C([0, T ]; J′
1),
E ∈C([0, T ]; K′
1),
H′ ∈C([0, T ]; J′
2),
E′ ∈C([0, T ]; K′
2),
φ ∈C([0, T ]; K),
ψ ∈C([0, T ]; J),
φ′ ∈C

[0, T ]; (K1)′
,
ψ′ ∈C

[0, T ]; (J1)′
,
and γφ belongs to L2(0, T ; L2(Γ)

.
8.4 Plate equation
We begin with Neumann control. We take HL = L2(Ω), VL = H2
0(Ω), D(L) =
H4(Ω) ∩H2
0(Ω), and
L = ∆2,
∆(L) = {z ∈H2
0(Ω): ∆2z ∈H2
0(Ω)}.
The eigenvalues related to L are deﬁned by
⎧
⎨
⎩
∆2wj = λjwj,
wj|Γ = ∂wj
∂ν

Γ
= 0,
|wj| = 1.
(8.48)
Considering again the multiplier m(x) = x −x0, we can check the relation

Γ
m · ν∆wj∆wk dΓ
= (4 −n)
%
λjλkδjk −

Ω

α
mα

λjwj
∂wk
∂xα
+ λkwk
∂wj
∂xα

dx.
(8.49)
We take U1 = U2 = L2(Γ0), where we recall that Γ0 is the part of the boundary
on which m · ν ≥0. We pick q1 = −γ0∆and q2 = 0.
It is useful to introduce the space H3(Ω) ∩H2
0(Ω), intermediary between
VL and D(L), equipped with the norm ∥z∥= |D(∆z)|. Note that L is an

378
III-1 Controllability and Observability
isomorphism between H3 ∩H2
0 and H−1. We shall check the property of
approximate controllability for any T > 0, by checking the assumptions of
Theorem 7.2. We take N = 0, and we deﬁne
(π1ζ, ζ′) =
1
R(x0)

Γ
m · ν∆ζ∆ζ′ dΓ,
π2 = 0,
(8.50)
b(z, ζ) = −
1
R(x0)

Ω

α
mαz ∂ζ
∂xα
dx.
(8.51)
The properties (7.22), (7.23), and (7.24) are easily veriﬁed. As
(π1wj, wj) =
4λj
R(x0) and b(wj, wj) =
n
2R(x0),
(8.52)
the properties (7.25) and (7.29) are clear. It remains to check (7.30). We deﬁne
WL = H1
0 and the assumption (7.30) is satisﬁed. Therefore, all the assump-
tions of Theorem 7.2 are now satisﬁed. Thus the pair (A, B) is approximately
controllable for arbitrary positive T . Let us interpret this result. The dynam-
ical system is described by
⟨ψ1, η(T )⟩−⟨ψ0, η′(T ) + q∗
2v2(T )⟩= −
 T
0

Γ0
v∆ψ dx dt
(8.53)
for ψ0 ∈H4 ∩H2
0 and ψ1 ∈H2
0 and ψ being the solution of
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
ψ′′ + ∆2ψ = f,
ψ(T ) = ψ′(T ) = 0,
ψ

Γ = ∂ψ
∂ν

Γ
= 0,
ψ ∈C([0, T ]; H4 ∩H2
0),
ψ′ ∈C([0, T ]; H2
0).
(8.54)
We interpret (8.53) as follows:
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
η′′ + ∆2η = 0,
η(0) = η′(0) = 0,
η

Γ = 0,
∂η
∂ν

Γ1
= 0,
∂η
∂ν

Γ0
= v,
η ∈C([0, T ]; V ′
L),
η′ ∈C

[0, T ];

D(L)
′
.
(8.55)
Let us express the controllability property. Given y0 ∈L2(Ω), y1 ∈H−2(Ω),
the system

8 Exact controllability of hyperbolic equations
379
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
η′′ + ∆2η = 0,
φ′′ + ∆2φ = 0,
η(0) = η′(0) = 0,
φ

Γ = ∂φ
∂ν

Γ
= 0,
η

Γ = 0,
∂η
∂ν

Γ1
= 0
η(T ) = y0,
η′(T ) = y1,
∂η
∂ν = ∆φ

Γ0,
φ ∈C([0, T ]; H2
0),
φ′ ∈C([0, T ]; L2),
∆φ

Γ0 ∈L2
0, T ; L2(Γ0)

has a solution for any T > 0.
We next consider a more elaborate case, where we control y and ∆y on
the boundary. We need some notation. Let Jn be such that
Jn =

z ∈Hn : z

Γ = 0, ∆z

Γ = 0, . . . , ∆n−1/2z

Γ = 0
 
.
Hence J1 = H1
0, J2 = H2 ∩H1
0 . . . . We provide Jn with the norm
∥z∥2
Jn =

|∆n/2z|2,
if n is even,
|D∆n/2z|2,
if n is odd.
We set J0 = L2(Ω). The operator −∆is an isometry from Jn to Jn−2, n ≥2.
Denote Jn the dual of Jn, when J0 is the pivot space. Clearly −∆extends
as an isometry between Jn and J−n−2, and we have the sequence
Jn ⊂Jn−1 ⊂Jn−2 ⊂· · · ⊂J1 ⊂J0 ⊂J−1 · · · ⊂J−n · · · .
In the above sequence, suppose that we pick J1 to be the pivot space, identiﬁed
with its dual. Then the dual of J2 is J0, the dual of J3 is J−1, and the dual
of Jn is J−n+2.
The duality pairing is in the ﬁrst case (J0 pivot)

(−∆)n/2u, (−∆)−n/2v

,
if n is even,
(D(−∆)n−1/2u, D(−∆)−n+1/2v),
if n is odd
(8.56)
with u ∈Jn, v ∈J−n.
Now in the case when J1 is chosen as the pivot space, then the duality
pairing is given by

(−∆)n/2u, (−∆)−n/2−1v

,
if n is even,
(D(−∆)n−1/2u, D(−∆)−n−1/2v),
if n is odd
(8.57)
with u ∈Jn, v ∈J−n+2.
We choose in the sequel J1 to be the pivot space. Let HL = J1, VL = J3,
D(L) = J5, and ∆(L) = J7. Then V ′
L = J−1,

D(L)
′ = J−3.

380
III-1 Controllability and Observability
We pick L = ∆2. The eigenvectors are deﬁned by

∆2wj = λjwj,
wj

Γ = ∆wj

Γ = 0,
∥wj∥= 1,
(8.58)
and the wj form an orthonormal system in J1 the pivot space. We shall use
the relations

Γ
m · ν ∂
∂ν ∆wj
∂
∂ν ∆wk dΓ
= (2 + n)
%
λjλkδjk −

Ω

α
mα

λj∆wk
∂wj
∂xα
+ λk∆wj
∂wk
∂xα

dx,
(8.59)

Γ
m · ν
 ∂
∂ν ∆wj
2
dΓ + λj

Γ
m · ν
 ∂
∂ν wj
2
dΓ = 4λj.
(8.60)
We take U1 = U2 = L2(Γ0). We recall that
n0 = ∂
∂ν
on Γ0. We deﬁne q1 = n0∆, q2 = n0. We then take
(π1ζ, ζ′) =
1
R(x0)

Γ
m · ν ∂
∂ν ∆ζ ∂
∂ν ∆ζ′ dΓ,
(8.61)
(π2ζ, ζ′) =
1
R(x0)

γ
m · ν ∂
∂ν ζ ∂
∂ν ζ′ dΓ,
(8.62)
b(z, ζ) = −
1
R(x0)

Ω

α
mα∆ζ ∂z
∂xα
dx.
(8.63)
Let us check that the assumptions of Theorem 7.2 are satisﬁed, with N = 0.
The properties (7.22) and (7.23) are clearly satisﬁed. The relation (7.24) can
also be checked, taking into account the fact that the wj are orthogonal in
H1
0. Let us check (7.25). We have
1
λj
(π1wj, wj) + (π2wj, wj)
=
1
R(x0)
 1
λj

Γ
m · ν

∂
∂ν ∆wj

2
dΓ +

Γ
m · ν

∂
∂ν wj

2
dΓ
	
=
4
R(x0)
from (8.60). Hence (7.25) is veriﬁed. Let us check (7.29). We have
b(wj, wj) = −(π2wj, wj)
2
+ 2 −n
2R(x0);
hence

8 Exact controllability of hyperbolic equations
381
1
λj
(π1wj, wj) −(π2wj, wj) −4b(wj, wj) =
2n
R(x0)
and (7.29) is satisﬁed. Let us ﬁnally check (7.30). We take WL = J2 and note
that
|b(z, ζ)| ≤µ(x0)
R(x0)∥z∥J1∥ζ∥J2.
As the injection of J3 into J2 is compact the assumption (7.30) is also satisﬁed.
Therefore the assumptions of Theorem 7.2 are now satisﬁed. Thus the pair
(A, B) is exactly controllable for arbitrary positive T .
The dynamic system is given by the relation
⟨ψ1, η(T )⟩−⟨ψ0, η′(T ) + q∗
2v2(T )⟩=
 T
0

Γ0

v1
∂
∂ν ∆ψ + v2
∂
∂ν ψ′

dΓ dt,
(8.64)
where ψ0 ∈J5, ψ1 ∈J3, and ψ is the solution of
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
ψ′′ + ∆2ψ = 0,
ψ(T ) = ψ0,
ψ′(T ) = ψ1,
ψ

Γ = ∆ψΓ = ∆2ψΓ = 0,
ψ ∈C([0, T ]; J5),
ψ′ ∈C([0, T ]; J3).
(8.65)
Writing ζ = −∆η, we interpret (8.64) as follows:
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
ζ′′ + ∆2ζ = 0,
ζ(0) = ζ′(0) = 0,
ζ

Γ1 = 0,
∆ζ

Γ1 = 0,
ζ

Γ0 = v1,
∆ζ

Γ0 = J∗
0 v2,
ζ ∈L2(0, T ; J−1) ∩C([0, T ]; J−3),
ζ′ ∈L2([0, T ]; J−5).
(8.66)
Let us express the controllability property. Given y0 in J1 and y1 in J−3, the
system
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
ζ′′ + ∆2ζ = 0,
φ′′ + ∆2φ = 0,
ζ(0) = ζ′(0) = 0,
φ

Γ = ∆φ

Γ = 0,
ζ

Γ1 = 0,
∆ζ

Γ1 = 0,
ζ(T ) = y0,
ζ

Γ0 = −∂
∂ν ∆φ,
∆ζ

Γ0 = J∗
0
∂
∂ν φ′,
ζ′(T ) = y1,
ζ ∈L2(0, T ; J−1) ∩C([0, T ]; J−3),
φ ∈C([0, T ]; J3),
ζ′ ∈L2([0, T ]; J−5),
φ′ ∈C([0, T ]; J1)
(8.67)
has a solution for T > 0 arbitrary.
Remark 8.1. For other examples and results of exact controllability see J. L. Li-
ons [1, 5] where the HUM method is presented in full generality, I. Lasiecka

382
III-1 Controllability and Observability
and R. Triggiani [9, 10, 11, 13], R. Triggiani [4], J. E. Lagnese [1],
and J. E. Lagnese and J. L. Lions [1]. The Maxwell equation is treated in
J. E. Lagnese [2]. The result of exact controllability for the plate equation
is given in E. Zuazua [1] and I. Lasiecka and R. Triggiani [13]. For the
use of microlocal analysis in the context of exact controllability, we refer to
C. Bardos, G. Lebeau, and J. Rauch [1]
⊓⊔

Part IV
Quadratic Optimal Control: Finite Time
Horizon

1
Bounded Control Operators: Control Inside
the Domain
1 Introduction and setting of the problem
In this chapter we consider the dynamical system governed by the equation

x′(t) = Ax(t) + Bu(t),
t ≥0,
x(0) = x0 ∈H,
(1.1)
where A: D(A) ⊂H →H, B : U →H are linear operators deﬁned on the
Hilbert spaces H (state space) and U (control space), respectively. x is the
state and u the control of the system. We shall also consider another Hilbert
space Y , the space of observations. The inner product and norm in H, U, and
Y will be denoted by (·, ·) and | · |. Whenever confusion is possible a subscript
H, U or Y will be added.
Given T > 0, we want to minimize the cost function
J(u) =
 T
0

|Cx(s)|2 + |u(s)|2 
ds + (P0x(T ), x(T ))
(1.2)
over all controls u ∈L2(0, T ; U) subject to the diﬀerential equation constraint
(1.1). Concerning the operators A, B, C, and P0 we shall assume that
(H)
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
(i)
A generates a strongly continuous semigroup etA on H,
(ii)
B ∈L(U; H),
(iii)
P0 ∈L(H) is hermitian and non-negative,
(iv)
C ∈L(H; Y ).
We shall also say that (A, B, C, P0) veriﬁes assumptions (H). Assumption
(H)–(iv) means that the observation operator is bounded. However in §4 be-
low we shall also consider the more general situation when C is unbounded.
Under assumptions (H)–(i)–(ii), problem (1.1) has a unique mild solution

386
IV-1 Bounded Control Operators: Control Inside the Domain
x ∈C([0, T ]; H) for any x0 ∈H and any u ∈L2(0, T ; U) (see Part II, Chap-
ter 1, §3.1). Moreover x is given by the expression
x(t) = etAx0 +
 t
0
e(t−s)ABu(s) ds.
A function u⋆∈L2(0, T ; U) is called an optimal control if
J(u⋆) ≤J(u),
∀u ∈L2(0, T ; U).
In this case the corresponding solution of (1.1) is called an optimal state
and the pair (u⋆, x⋆) an optimal pair. Under hypotheses (H) it is easy to see
that there exists a unique optimal control (because the quadratic form J(u)
is coercive on L2(0, T ; U)). However we are interested in showing that the
optimal control can be obtained as a feedback control (synthesis problem). For
this purpose we shall describe the Dynamic Programming approach, which
consists, essentially, in the following two steps:
Step 1.
We solve the Riccati equation

P ′ = A⋆P + PA −PBB⋆P + C⋆C,
P(0) = P0.
Step 2.
We prove that the optimal control u⋆is related to the optimal state x⋆by the
feedback formula
u⋆(t) = −B⋆P(T −t)x⋆(t),
t ∈[0, T ],
and moreover that x⋆is the solution of the closed loop equation

x′(t) = [A −BB⋆P(T −t)]x(t),
t ∈[0, T ],
x(0) = x0.
2 Solution of the Riccati equation
2.1 Notation and preliminaries
We ﬁrst introduce some notation. Let H be a complex Hilbert space; deﬁne
Σ(H) = {T ∈L(H): T is hermitian},
Σ+(H) = {T ∈Σ(H): (Tx, x) ≥0, ∀x ∈H}.

2 Solution of the Riccati equation
387
The space Σ(H), endowed with the norm of L(H), is a real Banach space
and Σ+(H) is a cone in Σ(H). For any interval I in R, we shall denote by
C

I; Σ(H)

the set of all continuous mappings from I to Σ(H). We next
consider the set of all strongly continuous mappings F : I →Σ(H) (that
is, such that F(·)x is continuous for any x ∈H). We shall mainly use the
following concept of convergence, called strong convergence. A sequence {Fn}
is strongly convergent to F if
∀x ∈H,
lim
n→∞Fn(·)x = F(·)x
in C(I; H),
where C(I; H) is endowed with the topology of uniform convergence on com-
pact subsets of I. This topological space will be denoted by Cs

I; Σ(H)

. If I
is compact we can also consider a stronger topology. Indeed, if F is strongly
continuous, the number
∥F∥= sup
t∈I
∥F(t)∥
(2.1)
is ﬁnite by virtue of the Uniform Boundedness Theorem. The space of all
strongly continuous mappings F : I →Σ(H), endowed with the norm (2.1),
is a Banach space and will be referred to as Cu

I; Σ(H)

, where the subscript
u stands for uniform convergence. Note that the spaces Cu

I; Σ(H)

and
Cs

I; Σ(H)

are equal as sets, but their topologies are diﬀerent. In particular
C

I; Σ(H)

is a proper closed subspace of Cu

I; Σ(H)

, but it is a dense
subspace of Cs

I; Σ(H)

if H is separable. (The fact that H is separable will
not be used in the sequel.) If F ∈Cs

I; Σ(H)

, then it is easy to check that
∥F(·)∥is Lebesgue measurable (see N. Dunford and R. S. Schwartz [1],
Lemma 3, p. 616).
Remark 2.1. The above terminology is both natural and convenient, but it
may deserve some extra general comments. Given a compact interval I in R,
there are two underlying spaces C

I; L(H)

and L

H; C(I; H)

and the space
L

H; C(I; H)

has two diﬀerent topologies. The notation Cu

I; L(H)

applies
to the space L

H; C(I; H)

endowed with its natural Banach space topology;
Cs

I; L(H)

denotes the same space endowed with the weaker non-Banach
topology Ts of pointwise convergence:
an →a ⇔∀x ∈H,
an(x) →a(x)
in C(I; H).
The space L

H; C(I; H)

coincides with the space of all strongly continuous
mappings A: I →L(H). The mapping A induces a map a: H →C(I; H)
deﬁned by a(x)(t) = A(t)x and because
∀x ∈H,
sup
t∈I
|A(t)x| < ∞,
we conclude from the Uniform Boundedness Theorem that
sup
t∈I
∥A(t)∥L(H) < ∞.

388
IV-1 Bounded Control Operators: Control Inside the Domain
By interchanging the order of the sup’s
sup
|x|=1
sup
t∈I
|A(t)x| = sup
t∈I
sup
|x|=1
|A(t)x| = sup
t∈I
∥A(t)∥L(H),
we conclude that a belongs to L

H; C(I; H)

. Using this construction the
space C

I; L(H)

of all uniformly continuous mappings can be identiﬁed
with a closed but smaller subspace of L

H; C(I; H)

because the norm on
L

H; C(I; H)

coincides with the norm on C

I; L(H)

by interchanging the
sup’s. In particular L

H; C(I; H)

contains all strongly continuous semigroups
of class C0, whereas C

I; L(H)

only contains uniformly continuous semi-
groups. The reader can easily check that the space L

H; C(I; H)

, endowed
with the weaker topology Ts, remains closed, but the space C

I; L(H)

is now
dense when H is separable.
⊓⊔
We deﬁne C1
s

I; Σ(H)

as the set of all mappings F ∈Cs

I; Σ(H)

, which
are strongly diﬀerentiable (that is, such that F(·)x is diﬀerentiable for any
x ∈H) and such that their derivative belongs to Cs

I; Σ(H)

. We set
C1
s

I; Σ+(H)

=

F ∈C1
s

I; Σ(H)

: F(t) ∈Σ+(H), ∀t ∈I
 
.
Finally we denote by Cα
I; Σ(H)

the set of all H¨older continuous mappings
from I to Σ(H) with exponent α ∈]0, 1[.
We are given a linear operator A that is the inﬁnitesimal generator of a
strongly continuous semigroup etA in H. By the Hille–Yosida theorem, there
exist M ≥1 and ω ∈R such that
∥etA∥≤Meωt
and
∥etAn∥≤Meωt,
t > 0,
(2.2)
where the An’s are the Yosida approximations of A. For any T > 0 we set
MT = sup{∥etAn∥: t ∈[0, T ], n ∈N}.
Finally we denote by A⋆the adjoint operator of A.
In the sequel, we shall need a generalization of the Contraction Map-
ping Principle. Let T > 0, and let {γn} be a sequence of mappings from
Cu

[0, T ]; Σ(H)

into itself such that
∥γn(P) −γn(Q)∥≤α∥P −Q∥,
for all P, Q ∈Cu

[0, T ]; Σ(H)

and all n ∈N, where α ∈[0, 1[.
Moreover assume that there exists a mapping γ from the space Cu

[0, T ];
Σ(H)

into itself such that
lim
n→∞γm
n (P) = γm(P)
in Cs

[0, T ]; Σ(H)

,
(2.3)
for all P ∈Cu

[0, T ]; Σ(H)

and all m ∈N, where γm and γm
n are deﬁned by
recurrence as

2 Solution of the Riccati equation
389
γ1 = γ,
γm+1(P) = γ

γm(P)

,
γ1
n = γn,
γm+1
n
(P) = γn

γm
n (P)

,
for m = 2, 3, . . ., and P ∈Cs

[0, T ]; Σ(H)

. It is easy to check that
∥γ(P) −γ(Q)∥≤α∥P −Q∥,
∀P, Q ∈Cu

[0, T ]; Σ(H)

.
Then, by the Contraction Mapping Principle, there exist unique Pn and P in
Cu

[0, T ]; Σ(H)

such that
γn(Pn) = Pn
and
γ(P) = P.
However, as we do not assume that
γn(P) →γ(P)
in Cu

[0, T ]; Σ(H)

,
we cannot conclude that Pn →P in Cu

[0, T ]; Σ(H)

, but a weaker result
holds.
Lemma 2.1. Under the previous hypotheses on the sequence of mappings
{γn},
Pn →P
in Cs

[0, T ]; Σ(H)

.
Proof. Set
P 0 = 0,
P 0
n = 0,
and deﬁne
P m = γm(P 0),
P m
n = γm
n (P 0),
m = 1, 2, . . ..
By the Contraction Mapping Principle, we have
lim
m→∞P m = P,
lim
m→∞P m
n = Pn
in Cu

[0, T ]; Σ(H)

,
n = 1, 2, . . ..
Moreover
∥P −P m∥≤
∞

k=m
αk∥γ(P 0)∥,
∥Pn −P m
n ∥≤
∞

k=m
αk∥γn(P 0)∥.
Now ﬁx x in H; then for all t in [0, T ]
|P(t)x −Pn(t)x| ≤|P(t)x −P m(t)x| + |P m(t)x −P m
n (t)x|
+ |P m
n (t)x −Pn(t)x|.
(2.4)
Given ε > 0, there exists mε ∈N such that
∞

k=m
αk{∥γ(P 0)∥+ ∥γn(P 0)∥} ≤ε
2,
(2.5)
for all m ≥mε and all n ∈N. By (2.4) and (2.5) it follows that
|P(t)x −Pn(t)x| ≤ε
2 + |P mε(t)x −P mε
n (t)x|,
∀t ∈[0, T ].
(2.6)
Now (2.3) yields the conclusion.
⊓⊔

390
IV-1 Bounded Control Operators: Control Inside the Domain
2.2 Riccati equation
Let A, B, C, and P0 be given linear operators such that assumptions (H) are
veriﬁed. Consider the following Riccati equation:

P ′ = A⋆P + PA −PBB⋆P + C⋆C,
P(0) = P0.
(2.7)
Problem (2.7) was studied by several authors when H, U, and Y are ﬁ-
nite dimensional. We recall the pioneering work in the early papers by
R. E. Kalman [2] and W. M. Wonham [2]. The ﬁrst systematic approach
in the inﬁnite dimensional case is due to J. L. Lions [3] who solved (2.7) by
variational methods. Here we follow the direct approach by G. Da Prato [1].
For other results see D. L. Lukes and D. L. Russell [1], R. Temam [1],
L. Tartar [1], and R. F. Curtain and A. J. Pritchard [1].
As the operator A is unbounded it is not clear a priori what a solution
of (2.7) means. We shall deﬁne now two kinds of solutions: mild solutions
and weak solutions. It is also possible to deﬁne a notion of strict solution and
classical solution. These deﬁnitions are more technical and will be given in §3.
We ﬁrst notice that if A ∈L(H), then, as is easily checked, problem (2.7) is
equivalent to the following integral equation:
P(t)x = etA⋆P0etAx +
 t
0
esA⋆C⋆CesAx ds
−
 t
0
e(t−s)A⋆P(s)BB⋆P(s)e(t−s)Ax ds,
x ∈H.
(2.8)
Now (2.8) is meaningful for any A, which satisﬁes assumptions (H). In particu-
lar the right-hand side of (2.8) belongs to the function space Cs

[0, T ]; Σ(H)

and it is natural to seek solutions in that space.
Deﬁnition 2.1.
(i) A mild solution of problem (2.7) in the interval [0, T ] is a function P ∈
Cs

[0, T ]; Σ(H)

that veriﬁes the integral equation (2.8).
(ii) A weak solution of problem (2.7) in the interval [0, T ] is a function
P ∈Cs

[0, T ]; Σ(H)

such that, P(0) = P0 and, for any x, y ∈D(A),
(P(·)x, y) is diﬀerentiable in [0, T ] and veriﬁes the equation
d
dt(P(t)x, y) = (P(t)x, Ay) + (P(t)Ax, y)
−

B⋆P(t)x, B⋆P(t)y

+ (Cx, Cy).
(2.9)
⊓⊔
Note that the choice of the space Cs

[0, T ]; Σ(H)

is natural because the
mapping t 	→etA is in general strongly continuous but not uniformly contin-
uous.

2 Solution of the Riccati equation
391
Proposition 2.1. Let P ∈Cs

[0, T ]; Σ(H)

. Then P is a mild solution of
problem (2.7) if and only if P is a weak solution of (2.7).
Proof. If P is a mild solution of (2.7), then for any x, y ∈D(A) we have

P(t)x, y

= (P0etAx, etAy)+
 t
0
([C⋆C−P(s)BB⋆P(s)]e(t−s)Ax, e(t−s)Ay) ds.
It follows that

P(t)x, y

is diﬀerentiable with respect to t and, by a simple
computation, that (2.9) holds. Conversely if P is a weak solution, then it is
easy to check that for all x, y ∈D(A)
d
ds

P(s)e(t−s)Ax, e(t−s)Ay

= (Ce(t−s)Ax, Ce(t−s)Ay) −

B⋆P(t)e(t−s)Ax, B⋆P(t)e(t−s)Ay

.
Integrating from 0 to t we obtain

P(t)x, y

=(etA⋆P0etAx, y)+
 t
0

e(t−s)A⋆[C⋆C−P(s)BB⋆P(s)]e(t−s)Ax, y

ds
for all x, y ∈D(A). As D(A) is dense in H, (2.8) follows.
⊓⊔
It is useful to introduce the approximating problem:

P ′
n = A⋆
nPn + PnAn −PnBB⋆Pn + C⋆C,
Pn(0) = P0,
(2.10)
where An = n2R(n, A) −nI is the Yosida approximation of A and R(n, A)
is the resolvent of A. Problem (2.10) is obviously equivalent to the following
integral equation:
Pn(t) = etA⋆
nP0etAn +
 t
0
esA⋆
nC⋆CesAn ds
−
 t
0
e(t−s)A⋆
nPn(s)BB⋆Pn(s)e(t−s)An ds.
We now solve problem (2.7). We ﬁrst prove the local existence of a solution.
Lemma 2.2. Assume that (H) is veriﬁed, ﬁx T > 0, set
r = 2M 2
T∥P0∥,
(2.11)
and let τ be such that
τ ∈]0, T ],
τ(∥C∥2 + r2∥B∥2) ≤∥P0∥,
2rM 2
T τ ≤1
2,
(2.12)
where MT has been previously deﬁned just below (2.2). Then problems (2.7)
and (2.10) have unique mild solutions P and Pn in the ball

392
IV-1 Bounded Control Operators: Control Inside the Domain
Br,τ =

F ∈Cu

[0, τ]; Σ(H)

: ∥F∥≤r
 
.
Moreover
lim
n→∞Pn = P
in Cs

[0, τ]; Σ(H)

.
(2.13)
Proof. Equation (2.8) (resp. the integral version of (2.10)) can be written in
the form
P = γ(P)

resp. Pn = γn

Pn)

,
where for x ∈H
γ(P)(t)x = etA⋆P0etAx +
 t
0
e(t−s)A⋆[C⋆C −P(s)BB⋆P(s)]e(t−s)Ax ds
and
γn(P)(t)x = etA⋆
nP0etAnx +
 t
0
e(t−s)A⋆
n[C⋆C −P(s)BB⋆P(s)]e(t−s)Anx ds.
Choose now r and τ such that (2.11) and (2.12) hold. We show that γ and γn
are 1
2-contractions on the ball Br,τ of Cu

[0, τ]; Σ(H)

. Let in fact P ∈Br,τ.
Then, recalling (2.2), we have
|γ(P)(t)x| ≤M 2
T {∥P0∥+ τ[∥C∥2 + r2∥B∥2]}|x|
≤2M 2
T∥P0∥|x|,
and analogously
|γn(P)(t)x| ≤2M 2
T ∥P0∥|x|.
It follows that
∥γ(P)(t)∥≤r,
∥γn(P)(t)∥≤r,
∀t ∈[0, τ], n ∈N, P ∈Br,τ
so that γ and γn map Br,τ into Br,τ.
For P, Q ∈Br,τ we have
γ(P)(t)x−γ(Q)(t)x=
 t
0
e(t−s)A⋆[PBB⋆(Q−P)+(Q−P)BB⋆Q](s)e(t−s)Ax ds,
and a similar formula holds for γn(P)(t)x −γn(Q)(t)x. It follows that
∥γ(P)(t) −γ(Q)(t)∥≤2rM 2
T τ∥B∥2∥P −Q∥≤1
2∥P −Q∥,
∥γn(P)(t) −γn(Q)(t)∥≤2rM 2
T τ∥B∥2∥P −Q∥≤1
2∥P −Q∥.
Thus γ and γn are 1
2-contractions in Br,τ and there exist unique mild solutions
P and Pn in Br,τ. Finally (2.13) follows from Lemma 2.1.
⊓⊔
We now prove global uniqueness.

2 Solution of the Riccati equation
393
Lemma 2.3. Assume that (H) is veriﬁed, let T > 0, and let P, Q be two mild
solutions of problem (2.7) in [0, T ]. Then P = Q.
Proof. Set
α = sup
t∈[0,T ]
max{∥P(t)∥, ∥Q(t)∥};
α is ﬁnite by the Uniform Boundedness Theorem. Choose r > 0 and τ ∈[0, T ]
such that
r = 2M 2
Tα,
τ(∥C∥2 + r2∥B∥2) ≤α,
2rM 2
T τ ≤1
2.
By Lemma 2.2 it follows that P(t) = Q(t) for any t ∈[0, τ]. It is now suﬃcient
to repeat this argument in the interval [τ, 2τ] and so on.
⊓⊔
The main result of this section is the following theorem.
Theorem 2.1. Assume that (H) is veriﬁed. Then problem (2.7) has a unique
mild solution P ∈Cs

[0, ∞[; Σ+(H)

. Moreover, for each n ∈N, problem
(2.10) has a unique solution Pn ∈C

[0, ∞[; Σ+(H)

and
lim
n→∞Pn = P
in Cs

[0, T ]; Σ(H)

,
for any T > 0.
Proof. Fix T > 0, set β = M 2
T (∥P0∥+T ∥C∥2), and choose r > 0 and τ ∈]0, T ]
such that
r = 2βM 2
T ,
τ(∥C∥2 + r2∥B∥2) ≤β,
2rτM 2
T ≤1
2.
By Lemma 2.3 there exists a unique solution P (resp. Pn) of (2.7) (resp.
(2.10)) in [0, τ], and Pn →P in Cs

[0, τ]; Σ(H)

. We now prove that
Pn(t) ≥0,
∀t ∈[0, τ].
(2.14)
This will imply that
P(t) ≥0,
∀t ∈[0, τ].
(2.15)
To this end we notice that Pn is the solution of the following linear problem
in [0, τ]:
P ′
n = L⋆
nPn + PnLn + C⋆C,
Pn(0) = P0,
where Ln = An −1
2BB⋆Pn. Denote by Un(t, s), 0 ≤s ≤t ≤τ, the evolution
operator associated with L⋆
n (see Part II, Chapter 1 §3.5). Then we can write
the solution Pn(t) as
Pn(t) = Un(t, 0)P0U ⋆
n(t, 0) +
 t
0
Un(t, s)C⋆C U ⋆
n(t, s) ds.
Thus (2.14) and (2.15) follow immediately.
We now prove that, for n large enough, we have

394
IV-1 Bounded Control Operators: Control Inside the Domain
P(t) ≤βI,
Pn(t) ≤βI,
∀t ∈[0, τ].
(2.16)
These inequalities will allow us to repeat the previous argument in the interval
[τ, 2τ] and so on. In this way the theorem will be proved. We have in fact

Pn(t)x, x

= (P0etAnx, etAnx) +
 t
0
|CesAnx|2 ds
−
 t
0
|B⋆Pn(s)e(t−s)Anx|2 ds ≤β|x|2.
As Pn(t) ≥0 this implies (2.16). The proof is complete.
⊓⊔
We now prove continuous dependence with respect to the data. Consider a
sequence of Riccati equations

(P k)′ = A⋆
kP k + P kAk −P kBkB⋆
kP k + C⋆
kCk,
P k(0) = P k
0 ,
(2.17)
under the following hypotheses:
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
(i)
for any k ∈N, (Ak, Bk, Ck, P k
0 )
fulﬁll (H),
(ii)
for all T > 0 and all x ∈H,
lim
k→∞etAkx = etAx uniformly in [0, T ],
(iii)
for all T > 0 and all x ∈H,
lim
k→∞etA⋆
kx = etA⋆x uniformly in [0, T ],
(iv)
the sequences {Bk}, {B⋆
k}, {Ck}, {C⋆
k}, {P k
0 } are
strongly convergent to B, B⋆, C, C⋆, P0,
respectively.
(2.18)
Theorem 2.2. Assume that (H) and (2.18) hold. Let P and Pk be the respec-
tive mild solutions to (2.7) and (2.17). Then, for any T > 0, we have
lim
k→∞Pk = P
in Cs([0, T ]; H).
(2.19)
Proof. Fix T > 0. By the Uniform Boundedness Theorem there exist positive
numbers p, b, and c such that
∥P k
0 ∥≤p,
∥C⋆
kCk∥≤c,
∥BkB⋆
k∥≤b,
∀k ∈N.
Set β = M 2
T (p + cT ) and choose r and τ ∈]0, T ] such that
r = 2M 2
Tβ,
τ(c + r2b) ≤β,
2M 2
Tτ ≤1
2.
Then, arguing as we did in the proof of Lemma 2.2 we can show that P k(·)x →
P(·)x in C([0, τ]; H) for any x in H. Finally, proceeding as in the proof of
Theorem 2.1, we prove that this argument can be iterated in the interval
[τ, 2τ] and so on.
⊓⊔

2 Solution of the Riccati equation
395
We conclude this section by proving an important monotonicity property
of the solutions of the Riccati equation (2.7).
Proposition 2.2. Consider the Riccati equations:

P ′
i = A⋆Pi + PiA −PiBiB⋆
i Pi + C⋆
i Ci,
Pi(0) = Pi,0,
i = 1, 2.
(2.20)
Assume that (Ai, Bi, Ci, Pi,0), verify (H) for i = 1, 2 and in addition that
P1,0 ≤P2,0,
C⋆
1C1 ≤C⋆
2C2,
B2B⋆
2 ≤B1B⋆
1.
Then we have
P1(t) ≤P2(t),
t ≥0.
(2.21)
Proof. Due to Theorem 2.1 it is suﬃcient to prove (2.21) when A is bounded
(because we can approximate A with the sequence of bounded operators An).
Set Z = P2 −P1; then, as easily checked, Z is the solution to the linear
problem:

Z′ = X⋆Z + ZX −P2[B2B⋆
2 −B1B⋆
1]P2 + C⋆
2C2 −C⋆
1C1,
Z(0) = P2,0 −P1,0,
where
X = A −1
2B1B⋆
1(P1 + P2).
Let V (t, s) be the evolution operator associated with X⋆. Then we have
Z(t) = V (t, 0)(P2,0 −P1,0)V ⋆(t, 0) +
 t
0
V (t, s){C⋆
2C2 −C⋆
1C1}V ⋆(t, s) ds
+
 t
0
V (t, s)P1(s)[B1B⋆
1 −B2B⋆
2]P1(s)V ⋆(t, s) ds
so that Z(t) ≥0 and the conclusion follows.
⊓⊔
2.3 Representation formulas for the solution of the Riccati
equation
We want here to give an explicit formula for the solution of the Riccati equa-
tion (2.7). In fact several variants are possible; see A. V. Balakrishnan [4]
and I. Lasiecka and R. Triggiani [1, 3].
We assume that (H) is veriﬁed and introduce the operator
Kt ∈L

L2(0, t; H); L2(0, t; H)

deﬁned by

396
IV-1 Bounded Control Operators: Control Inside the Domain
(Ktx)(s) =
 t
s
e(ρ−s)Ax(ρ) dρ,
x ∈L2(0, t; H),
s ∈[0, t].
Its adjoint K⋆
t ∈L

L2(0, t; H); L2(0, t; H)

is given by
(K⋆
t x)(s) =
 s
0
e(s−σ)A⋆x(σ) dσ,
s ∈[0, t].
In fact Kt, K⋆
t ∈L

L2(0, t; H); C([0, t]; H)

(with K⋆
t not representing the
dual forthesefunctionalspaces). ThereforewecandeﬁneK0
t ∈L

L2(0, t; H); H

as the operator
K0
t x = (Ktx)(0) =
 t
0
eρAx(ρ) dρ,
x ∈L2(0, t; H),
and its dual (K0
t )⋆∈L

H; L2(0, t; H)

is given by
[(K0
t )⋆h](s) = esA⋆h,
h ∈H.
We can assert the following proposition.
Proposition 2.3. Assume that (H) is veriﬁed, and let P be the mild solution
of (2.7). Then one has the formula
P(t) =

I +

K⋆
t C⋆CKt + (K0
t )⋆P0Kt

BB⋆]−1
× [K⋆
t C⋆Ce(t−·)A + (K0
t )⋆P0etA](t).
(2.22)
Proof. We ﬁrst clarify the meaning of the right-hand side of (2.22). For ﬁxed t,
e(t−·)A is the operator in L

H; L2(0, t; H)

deﬁned by h 	→e(t−s)Ah, s ∈]0, t[.
Next C ∈L(H; Y ) is identiﬁed with an element of L

L2(0, t; H); L2(0, t; Y )

deﬁned by
Cx(·)(s) = Cx(s).
Similarly B ∈L

L2(0, t; U); L2(0, t; H)

and analogous considerations hold
for C⋆, B⋆. Then the operator
Λt = I +

K⋆
t C⋆CKt + (K0
t )⋆P0Kt

BB⋆
is an element of L

L2(0, t; H); L2(0, t; H)

, which even belongs to L

L2(0, t; H);
C([0, t]; H)

. According to Proposition 1.1 in Appendix A, it is invertible and
Λ−1
t
∈L

L2(0, t; H); C([0, t]; H)

and ﬁnally
Λ−1
t [K⋆
t C⋆Ce(t−·)A + (K0
t )⋆P0etA] ∈L

H; C([0, t]; H)

,
and it makes sense to take its value at time t, deﬁning in this way an element
of L(H), and therefore (2.22) has a meaning.

3 Strict and classical solutions of the Riccati equation
397
We shall proceed with the formal derivation of (2.22). The rigorous deriva-
tion is ﬁrst done for the approximation (2.10), where the formal computations
are valid, because An is bounded, and then the ﬁnal result is obtained by going
to the limit. Deﬁne X(t, s) to be the solution of the problem
⎧
⎨
⎩
−∂X(t, s)
∂s
=

A −BB⋆P(s)

X(t, s),
0 < t < s,
X(t, t) = I.
(2.23)
Deﬁne next
Z(t, s) = P(s)X(t, s).
An easy computation (from the Riccati equation and (2.23)) shows that the
pair X(t, s), Z(t, s) is the solution of the Hamiltonian system
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
−∂X(t, s)
∂s
= AX(t, s) −BB⋆Z(t, s),
∂Z(t, s)
∂s
= A⋆Z(t, s) + C⋆CX(t, s),
Z(t, 0) = P0X(t, 0),
X(t, t) = I.
(2.24)
Therefore we have
⎧
⎪
⎨
⎪
⎩
Z(t, s) = esA⋆P0X(t, 0) +
 s
0
e(s−σ)A⋆C⋆CX(t, σ) dσ,
X(t, s) = e(t−s)A −
 t
s
e(ρ−s)ABB⋆Z(t, ρ) dρ,
and combining the two relations we obtain
Z(t, s) = esA⋆P0

etA −
 t
0
eρABB⋆Z(t, ρ) dρ

+
 s
0
e(s−σ)A⋆C⋆C

e(t−σ)A −
 t
σ
e(ρ−σ)ABB⋆Z(t, ρ) dρ

dσ.
Setting Zt(s) = Z(t, s) ∈L

L2(0, t; H); L2(0, t; H)

, this relation reads as
Zt + (K0
t )⋆P0K0
t BB⋆Zt + K⋆
t C⋆CKtBB⋆Zt = K⋆
t C⋆Ce(t−·)A + (K0
t )⋆P0etA.
As P(t) = Zt(t) = Z(t, t), we obtain formula (2.22).
⊓⊔
3 Strict and classical solutions of the Riccati equation
In this section we are interested in strict and classical solutions of the Riccati
equation. The section is divided into three subsections, namely the general
case, the case when etA is an analytic semigroup, and ﬁnally the case when A
is a variational operator. In §3.1 and §3.2 we follow G. Da Prato [1].

398
IV-1 Bounded Control Operators: Control Inside the Domain
3.1 The general case
In order to deﬁne strict solutions to (2.7), we need to give a precise meaning
to the linear mapping A: T 	→A(T ) = A⋆T + TA. For any T ∈Σ(H) we set
ϕT (x, y) = (Tx, Ay) + (Ax, Ty),
x, y ∈D(A)
(3.1)
and deﬁne
D(A) = {T ∈Σ(H): ϕT is continuous in H × H}.
(3.2)
If T ∈D(A) then ϕT has a unique extension (which we still denote by ϕT )
as a continuous sesquilinear form in H × H. In this case there exists a linear
operator that we denote by A(T ) ∈Σ(H) such that

A(T )x, y

= ϕT (x, y),
x, y ∈D(A), T ∈D(A).
(3.3)
Thus we have deﬁned a linear operator from D(A) ⊂Σ(H) →Σ(H). It is
easy to check that A is closed in Σ(H). However D(A) is not dense in Σ(H)
in general. The following proposition shows the relationship between A(T )
and A⋆T + TA .
Proposition 3.1. Let T ∈D(A). Then for any x ∈D(A) we have Tx ∈
D(A⋆) and
A(T )x = A⋆Tx + TAx.
(3.4)
Proof. For any x, y ∈D(A) we have
(Tx, Ay) = ϕT (x, y) −(Ax, Ty).
It follows that, for x ∈D(A), the linear mapping
y 	→(Tx, Ay): D(A) →C,
is continuous for H. This implies that Tx ∈D(A⋆) and for all y ∈H

A(T )x, y

= ϕT (x, y) = (A⋆Tx, y) + (TAx, y),
which proves (3.4).
⊓⊔
Remark 3.1. We deﬁne a semigroup on Σ(H)
etA(T ) = etA⋆TetA,
t ≥0, T ∈Σ(H).
(3.5)
Notice that etA is not strongly continuous in general (this should imply that
etA(T ) →T as t →0 in L(H)). However it is easy to see that
etA(T )x →Tx
as t →0,
∀x ∈H,
∀T ∈Σ(H).
⊓⊔

3 Strict and classical solutions of the Riccati equation
399
We now return to the Riccati equation (2.7), which we write in the form
P ′ = A(P) −PBB⋆P + C⋆C,
P(0) = P0.
(3.6)
We say that P is a strict solution to the Riccati equation (2.7) (or (3.6)) if
⎧
⎪
⎨
⎪
⎩
(i)
P ∈C1
s

[0, ∞[; Σ(H)

,
(ii)
P(t) ∈D(A), ∀t ≥0,
(iii)
A(P) ∈Cs

[0, ∞[ ; Σ(H)

and
(3.6) holds.
(3.7)
Remark 3.2. Assume that P is a strict solution of problem (2.7). By Propo-
sition 3.1 it follows that for any x ∈D(A), P(t)x ∈D(A⋆) and the following
equation holds:
P ′(t)x = A⋆P(t)x + P(t)Ax −P(t)BB⋆P(t)x + C⋆Cx.
(3.8)
By (3.8) one can easily check that P is a weak solution of problem (2.7). Thus,
by Proposition 2.2, a strict solution is a mild solution of (2.7).
⊓⊔
Proposition 3.2. Assume that (H) is veriﬁed and that P0 ∈D(A). Then the
Riccati equation (2.7) has a unique strict solution.
Proof. Uniqueness follows from Theorem 2.1. Let us prove existence. Fix T >
0 and let P (resp. Pn) be the mild solution of (2.7) (resp. (2.10)). Set
An(S) = A⋆
nS + SAn,
n > ω, S ∈Σ(H),
where the An’s are the Yosida approximations of A and ω is given in (2.2)
(note that the An’s are not the Yosida approximations of A). We have

P ′
n = An(Pn) −PnBB⋆Pn + C⋆C,
Pn(0) = P0.
Moreover Vn = P ′
n is the solution of the problem

V ′
n = An(Vn) −PnBB⋆Vn −VnBB⋆Pn,
Vn(0) = An(P0) −P0BB⋆P0 + C⋆C.
We want to show that Vn(·)x is convergent, as n →∞, to V (·)x in C([0, T ]; H),
where V is the solution to the following linear integral equation: For all x in
H
V (t)x=etA⋆V (0)etAx −
 t
0
e(t−s)A⋆[P(s)BB⋆V (s)+V (s)BB⋆P(s)]e(t−s)Ax ds,
which can be easily solved by successive approximations. In fact, by using
similar arguments as in the proof of Lemma 2.2, it follows that

400
IV-1 Bounded Control Operators: Control Inside the Domain
Pn →P,
Vn →V
in Cs([0, T ]; H).
As Vn = P ′
n, we have that P ∈C1
s ([0, T ]; H). It remains to show that P(t) ∈
D(A) and that (3.6) holds. Fix t ≥0 and let x, y ∈D(A). We have
ϕP (t)(x, y) =

P(t)x, Ay

+

Ax, P(t)y

= lim
n→∞


Pn(t)x, Any

+

Anx, Pn(t)y

= lim
n→∞

An

Pn(t)

x, y

= lim
n→∞


Vn(t)x, y

+

Pn(t)BB⋆Pn(t)x, y

−(C⋆Cx, y)]
=

V (t)x, y

+

P(t)BB⋆P(t)x, y

−(C⋆Cx, y).
It follows that P(t) ∈D(A) and
A

P(t)

= P ′(t) + P(t)BB⋆P(t) −C⋆C.
⊓⊔
3.2 The analytic case
We assume here that the semigroup etA is analytic. Then there exists c > 0
and γ > 0 such that
∥etAn∥≤cetγ,
∥AnetAn∥≤c
tetγ,
0 < t < 1.
(3.9)
As etAn(T ) = etA⋆
nTetAn, ∀T ∈L(H), it follows that
∥etAn∥≤c2e2tγ,
∥AnetAn∥≤2c2
t e2tγ,
0 < t < 1.
(3.10)
Moreover the semigroup etA, deﬁned by (3.5), is clearly analytic for t > 0.
We now introduce the notion of classical solution. We say that P ∈
Cs

[0, ∞[ ; Σ(H)

is a classical solution of the Riccati equation (2.7) if
⎧
⎪
⎨
⎪
⎩
(i)
for any t > 0, P(t) is diﬀerentiable, P(t) ∈D(A)
and
P ′(t) = A

P(t)

−P(t)BB⋆P(t) + C⋆C,
t > 0,
(ii)
P(0) = P0.
Proposition 3.3. Assume that (H) is veriﬁed and that etA is an analytic
semigroup. If P ∈Cs

[0, ∞[ ; Σ(H)

is a classical solution of (2.7), then P
is a weak solution and a mild solution of (2.7).
Proof. Let P be a classical solution, let x, y ∈D(A), and let t > 0. By
Proposition 3.1 we have P(t)x ∈D(A⋆) and
A

P(t)

x = A⋆P(t)x + P(t)Ax.
It follows that

3 Strict and classical solutions of the Riccati equation
401
d
dt

P(t)x, y

=

P(t)x, y

+ (P(t)Ax, y) −

B⋆P(t)x, B⋆P(t)y

+ (Cx, Cy)
for all t > 0. By integrating this identity between ε > 0 and t, and letting
ε tend to 0, we ﬁnd that P is a weak solution and then a mild solution of
(2.7).
⊓⊔
Remark 3.3. By Proposition 3.3 it follows that there exists at most one clas-
sical solution of (2.7).
⊓⊔
Theorem 3.1. Assume (H) and that etA is an analytic semigroup. If P ∈
Cs

[0, ∞[ ; Σ(H)

is the mild solution of (2.7), then P is a classical solution
and belongs to C∞
[ε, ∞[; Σ(H)

, ∀ε > 0.
In order to prove Theorem 3.1 we need the following two lemmas.
Lemma 3.1. Assume that etA is an analytic semigroup. Let E ∈Cs

[0, T ];
Σ(H)

for some T > 0, and let F be deﬁned by
F(t)x =
 t
0
e(t−s)A⋆E(s)e(t−s)Ax ds,
x ∈H.
Then for any α ∈]0, 1[, we have F ∈Cα
[0, T ]; Σ(H)

and there exists a
constant CT > 0 such that
|F(t)x −F(r)x| ≤CT |t −r|α∥E∥C([0,T ];Σ(H))|x|,
for all t, r > 0.
Proof. Let x ∈H and 0 < r < t ≤T . We have
F(t)x −F(r)x =
 t
r
e(t−s)A
E(s)

x ds +
 r
0
 t−s
r−s
(AeσA)

E(s)

x dσ.
Taking into account (3.10) we have
∥F(t) −F(r)∥≤c2e2tγ

|t −r| +
 r
0
ds
 t−s
r−s
dσ
σ

∥E∥.
As
 r
0
ds
 t−s
r−s
dσ
σ ≤
 r
0
(r −s)−α ds
 t−s
r−s
σα−1 dσ ≤1
α|t −r|α
 r
0
(r −s)−α ds,
the conclusion follows.
⊓⊔
Lemma 3.2. Assume that etA is an analytic semigroup. Let α ∈]0, 1[, M ∈
Cα
[0, T ]; Σ(H)

, and let G be deﬁned by
G(t)x =
 t
0
e(t−s)A⋆M(s)e(t−s)Ax ds,
x ∈H.
(3.11)
Then G ∈C1
[0, T ]; Σ(H)

∩C

[0, T ]; D(A)

and
G′(t) = A

G(t)

+ M(t).

402
IV-1 Bounded Control Operators: Control Inside the Domain
Proof. Set
Gn(t)x =
 t
0
e(t−s)An
M(s)

x ds.
Then we have
G′
n(t) =
 t
0
Ane(t−s)An
M(s) −M(t)

ds + etAn
M(t)

.
(3.12)
By using (3.10) we have
∥Ane(t−s)An
M(s) −M(t)

∥≤2c2e2tγ|t −s|α−1∥M∥α,
(3.13)
where
∥M∥α =
sup
0≤t<s≤T
∥M(t) −M(s)∥
|t −s|α
.
By (3.12) and (3.13), it follows that G ∈C1
[0, T ]; Σ(H)

and
G′(t)x =
 t
0
Ae(t−s)A
M(s) −M(t)

x ds + etA
M(t)

x.
Finally, arguing as we did in the proof of Proposition 3.2, we ﬁnd that G ∈
C

[0, T ]; D(A)

and that (3.11) holds.
⊓⊔
Proof. Proof of Theorem 3.1. Let P be the mild solution of (2.7), which we
write in the form
P(t)x = etA⋆P0etAx +
 t
0
e(t−s)A⋆E(s)e(t−s)Ax ds,
(3.14)
where x ∈H and E = C⋆C −PBB⋆P. As etA⋆P0etA is analytic in t for t > 0,
from Lemma 3.1 we have P ∈Cα
[ε, T ]; Σ(H)

for 0 < ε < T, α ∈]0, 1].
Moreover for all x in H and t ≥ε
P(t)x = e(t−ε)A⋆P(ε)e(t−ε)Ax +
 t
ε
e(t−s)A⋆E(s)e(t−s)Ax ds.
From Lemma 3.2, it follows that
P ∈C1
[2ε, T ]; Σ(H)

∩C([2ε, T ]; D

A)

,
0 < 2ε < T,
which implies E ∈C1
[2ε, T ]; Σ(H)

. Moreover, by the identity
P(t)x = e(t−2ε)A⋆P(2ε)e(t−2ε)Ax +
 t
2ε
eσA⋆E(t −σ)eσAx dσ,
x ∈H,
with t ≥2ε, it follows that P ∈C2
[3ε, T ]; Σ(H)

, 0 < 3ε < T . By repeating
this argument several times we ﬁnd P ∈C∞([ε, T ]; Σ(H)), for any ε > 0, as
required.
⊓⊔

3 Strict and classical solutions of the Riccati equation
403
The next corollary follows easily from (3.9).
Corollary 3.1. Assume that (H) is veriﬁed and let etA be an analytic semi-
group. Let P ∈Cs([0, ∞[ ; H) be the classical solution to (2.7). Then P belongs
to C

[ε, ∞[ ; L

H; D

(−A⋆)1−ε
for all ε ∈]0, 1[.
Theorem 3.2. Assume that (H) is veriﬁed and that the semigroup etA is
analytic in the sector Sθ = {λ ∈C: | arg λ| < θ} for some θ ∈]0, π/2[. Let P
be the classical solution of problem (2.7). Then P has an analytical extension,
as a function with values in Σ(H), on the sector Sθ.
Proof. Deﬁne r and τ as in (2.12). Then by Lemma 2.2 we have
P(·)x = lim
m→∞P (m)(·)x
in C([0, τ]; H),
x ∈H,
where P (m) are deﬁned by recurrence as
P (0)(t)x = etA⋆P0etAx
P (m+1)(t)x = etA⋆P0etAx +
 t
0
e(t−s)A⋆C⋆Ce(t−s)Ax ds
−
 t
0
e(t−s)A⋆P (m)(s)BB⋆P (m)(s)e(t−s)Ax ds.
Clearly P (m)(·)x are analytic in Sθ. Thus by a classical result, P(·)x is analytic
in Sθ for any x ∈H. This yields the conclusion (see for instance A. E. Tay-
lor [1], Theorem 4.4.F).
⊓⊔
3.3 The variational case
We consider here the situation described in Chapter 2 of Part II. We assume
that there exists a Hilbert space V such that V ⊂H, algebraically and topo-
logically, and V is dense in H. Moreover, H is identiﬁed with its dual, and
V ⊂H ⊂V ′, where V ′ is the dual of V . We denote by ⟨·, ·⟩the duality pair-
ing between V and V ′ and by ∥· ∥the norm on V (recall that | · | and (·, ·)
represent the norm and the scalar product in H). Let a: V × V →R be a
V –H coercive continuous bilinear form; that is
∃α > 0,
∃λ ∈R,
∀x ∈V,
a(v, v) + λ|v|2 ≥α∥v∥2.
Let −A: D(A) ⊂H →H be the operator generated by a
−(Av, v) = a(v, v),
∀v ∈D(A),
and denote by Aλ the operator A −λI. The operator Aλ, as well as A, is
the generator of an analytic semigroup on H (cf. Part II, Chapter 1, §2.7,
Theorem 2.12). Moreover we have seen that when

404
IV-1 Bounded Control Operators: Control Inside the Domain
DA
 1
2, 2

= DA⋆ 1
2, 2

,
then
DA
 1
2, 2

= D

(−Aλ)1/2
= V = D

(−A⋆
λ)1/2
= DA⋆ 1
2, 2

(cf. Part II, Chapter 2, §1.5, Proposition 1.1).
We shall need the following result due to F. Flandoli [6], which is true
for arbitrary variational operators.
Lemma 3.3. Let A be a variational operator in H. Then for any T > 0, there
exists KT > 0 such that
 T
0
|(−A)1/2etAx|2dt ≤KT |x|2,
∀x ∈H.
(3.15)
Proof. For variational operators we know from the result of A. Yagi [1] (cf.
Part II, Chapter 1, Theorem 6.1) that DA( 1
2, 2) and D

(−A)1/2
are isomor-
phic. It is suﬃcient to prove (3.15) for all x ∈D(A); if x ∈D(A), we have
 T
0
|(−A)1/2etAx|2 dt =
 T
0
|AetA(−A)−1/2x|2 dt ≤|(−A)−1/2x|2
DA(1/2,2).
As DA( 1
2, 2) is isomorphic to D

(−A)1/2
, the conclusion follows.
⊓⊔
In this case we have the following additional regularity result.
Theorem 3.3. Assume that (H) is veriﬁed, and let A be a variational oper-
ator. Let P ∈Cs([0, ∞[ ; H) be the classical solution to (2.7). Then for any
t > 0 and x ∈D

(−A)1/2
we have P(t)x ∈D

(−A⋆)1/2
and the mapping
(−A⋆)1/2P(·)(−A)1/2x is continuous for t > 0.
Proof. Let x ∈D

(−A)1/2
and let P be given by (3.14); then we have

P(t)(−A)1/2x, (−A)1/2x

=

P0(−A)1/2etAx, (−A)1/2etAx

+
 t
0

E(s)(−A)1/2e(t−s)Ax, (−A)1/2e(t−s)Ax

ds
≤∥P0∥∥(−A)1/2∥2|etAx|2+∥E∥
 t
0
|(−A)1/2esAx|2 ds.
It follows, recalling (3.15), that

P(t)(−A)1/2x, (−A)1/2x
 ≤c2
t2 e2γT ∥(−A)1/2∥2|x|2 + ∥E∥KT|x|2,
which yields the conclusion.
⊓⊔
So when A is a variational operator generated by a continuous and coercive
bilinear form a such that
D

(−A)1/2
= D

(−A⋆)1/2
,
Theorem 3.3 says that for all t > 0 the operator P(t) belongs to L(V ′; V ).

4 The case of the unbounded observation
405
4 The case of the unbounded observation
In this section we consider the case when the observation C is unbounded.
More precisely we make the following assumption:
(H)–(v)
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
(i)
C ∈L

D(A); Y

(ii)
∃a real continuous function K such that
 t
0
|CesAx|2 ds ≤K(t)|x|2,
∀x ∈D(A).
Clearly, if (H)–(v) holds, then for any t ≥0, there exists a linear operator
FC(t) ∈Σ+(H) such that

FC(t)x, y

=
 t
0
(CesAx, CesAy) ds,
∀x, y ∈D(A).
(4.1)
Lemma 4.1. Assume that (H)–(v) is veriﬁed, and let FC(·) be deﬁned by
(4.1). Then FC ∈Cs

[0, +∞[ ; Σ+(H)

. Moreover, setting
FC,n(t)x =
 t
0
esA⋆(CJn)⋆CJnesAx ds,
x ∈H,
where Jn = nR(n, A), one has for all T > 0
lim
n→∞FC,n(·) = FC(·),
in Cs

[0, T ]; Σ(H)

.
Proof. Let t0 ∈[0, +∞[, t > t0, and x ∈D(A); then we have

8
F(t) −F(t0)

x

2
=

F(t)x−F(t0)x, x

=
 t
t0
|CesAx|2 ds →0,
as t →t0.
As D(A) is dense in H we have
lim
t→t0
8
F(t) −F(t0)

x = 0
for all x ∈H. Consequently, the ﬁrst part of the lemma follows easily. The
last part follows immediately because
FC,n(t) = J⋆
nFCJn.
⊓⊔
We consider now the Riccati equation in the integral form
P(t)x = etA⋆P0etAx + FC(t)x −
 t
0
e(t−s)A⋆P(s)BB⋆P(s)e(t−s)Ax ds (4.2)

406
IV-1 Bounded Control Operators: Control Inside the Domain
and the approximating equation
Pn(t)x = etA⋆
nP0etAnx + FC,n(t)x
−
 t
0
e(t−s)A⋆
nPn(s)BB⋆Pn(s)e(t−s)Anx ds,
n ∈N.
(4.3)
Equation (4.3) has clearly a unique solution Pn ∈C ([0, ∞[ ; Σ+(H)), which
is also the solution of the problem

P ′
n = A⋆
nPn + PnAn −PnBB⋆Pn + (CJn)⋆CJn,
Pn(0) = P0.
We now prove the following theorem.
Theorem 4.1. Assume that (H)–(i)–(ii)–(iii)–(v) are veriﬁed. Then (4.2) has
a unique solution P ∈Cs

[0, ∞[ ; Σ+(H)

and
lim
n→∞Pn(·) = P(·)
in Cs

[0, T ]; Σ(H)

,
∀T > 0.
(4.4)
Proof. The proof is similar to the one of Theorem 2.1, with minor diﬀerences.
Fix T > 0, set
LT =
sup
t∈[0,T ],n∈N
∥FC,n(t)∥,
r = 2(M 2
T∥P0∥+ LT ),
and choose τ ∈[0, T ] such that
τr2M 2
T ∥B∥2 ≤M 2
T ∥P0∥+ LT ,
2τrM 2
T ≤1
2.
Now set
γ(P)(t)x = etA⋆P0etAx + FC(t)x −
 t
0
e(t−s)A⋆P(s)BB⋆P(s)e(t−s)Ax ds,
and let γn(P) be deﬁned in a similar way. Then γ and γn are 1
2-contractions
on the ball
{P ∈Cs([0, τ]; H): ∥P(t)∥≤r, ∀t ∈[0, τ]}.
Thus there exists P (resp. Pn) such that γ(P) = P (resp. γn(Pn) = Pn) and
P (resp. Pn) is the unique solution of (4.2) (resp. (4.3)) in [0, τ]. Now, by
Lemma 2.1, it follows that
lim
n→∞Pn(·) = P(·)
in Cs

[0, τ]; Σ(H)

.
This yields P(t) ≥0, ∀t ∈[0, τ]. From now on the proof is completely similar
to the one of Theorem 2.1.
⊓⊔

5 The case when A generates a group
407
4.1 The analytic case
We choose once and for all λ0 > ω and set Aλ0 = λ0 −A. We assume besides
(H)–(i)–(ii)–(iii)
(H)–(vi) A is the inﬁnitesimal generator of an analytic semigroup,
(H)-(vii) there exists α ∈[0, 1
2[ such that C ∈L

D(Aα
λ0); Y

.
We remark that (H)–(vi)–(vii) imply that there exists a constant Kα > 0 such
that (see Part II, Chapter 1, §5, Theorem 5.2)
∥CetA∥≤Kαeωtt−α,
t ≥0.
It follows that (H)–(v) holds true and so we have the following straightforward
generalization of Theorem 2.1.
Proposition 4.1. Assume that (H)–(i)–(ii)–(iii)–(vi)–(vii) are veriﬁed. Then
the Riccati equation (2.7) has a unique mild solution P ∈Cs

[0, ∞[ ; Σ+(H)

.
4.2 The variational case
As in §3.3, we are given a variational operator A, which is V –H coercive for
some α > 0 and λ ∈R. We assume that V is isomorphic to D

(−A)1/2
and
to D

(−A⋆)1/2
. We assume in addition that
• (H)–(viii) C ∈L(V ; Y ).
In fact this framework corresponds to the limit case of the situation considered
in §4.1, with α = 1
2. If (H)–(viii) holds, then by Lemma 3.3, it follows that
(H)–(v) is also fulﬁlled. Thus we have the ﬁnal result.
Proposition 4.2. Assume that (H)–(i)–(ii)–(iii)–(viii) are veriﬁed. Then the
Riccati equation (2.7) has a unique mild solution P ∈Cs

[0, ∞[ ; Σ+(H)

.
5 The case when A generates a group
We shall denote by Lr(H) the set of elements in L(H) that have a continuous
inverse. It is well known that Lr(H) is open in L(H).
In this section we assume (H) and in addition that A is the inﬁnitesimal
generator of a strongly continuous group of operators. This is equivalent to
say that A fulﬁlls (H)–(i) and −A generates a C0 semigroup e−tA. Besides
Riccati equation (2.7) we shall consider the following:

Q′ = −AQ −QA⋆−QC⋆CQ + BB⋆,
Q(0) = Q0.
(5.1)
We shall show that, as proved in F. Flandoli [2], when Q0 = P −1
0
, then we
have Q(t) = P −1(t). We ﬁrst consider the case when A is bounded.

408
IV-1 Bounded Control Operators: Control Inside the Domain
Proposition 5.1. Assume that (H) is veriﬁed, that A ∈L(H), and that P0 ∈
Lr(H). Let P(·) be the mild solution of (2.7) and Q(·) the mild solution of
(5.1) with Q0 = P −1
0
. Then P(t) ∈Lr(H) for all t > 0 and we have
P(t)−1 = Q(t),
∀t ≥0.
Proof. Set Λ = PQ −I; as easily checked we have

Λ′ = (A∗−PBB∗)Λ −Λ(A∗−C∗CQ) =: L(Λ),
Λ(0) = 0.
As L is a linear bounded operator in L(H) we have Λ(t) = 0, ∀t ≥0 and so
P(t)Q(t) = I, ∀t ≥0. In a similar way one shows that Q(t)P(t) = I, ∀t ≥0.
⊓⊔
We prove now the main result of this section.
Theorem 5.1. Assume that (H) is veriﬁed, that A generates a strongly con-
tinuous group in H, and that P0 ∈Lr(H). Let P(·) be the mild solution of
(2.7) and Q(·) the mild solution of (5.1) with Q0 = P −1
0
. Then P(t) ∈Lr(H)
for all t > 0 and we have
P(t)−1 = Q(t),
∀t ≥0.
Proof. Denote by Pn and Qn the solutions of problems (2.7) and (5.1) with
A replaced by the Yosida approximants An, n ∈N. By Theorem 2.1 we have
lim
n→∞Pn = P,
lim
n→∞Qn = Q
in Cs

[0, T ]; Σ+(H)

for all T > 0. By the previous proposition it follows that
P −1
n (t) = Qn(t),
∀t ≥0.
Letting n tend to inﬁnity on the equality
Pn(t)Qn(t)x = Qn(t)Pn(t)x = x,
x ∈H,
the conclusion follows.
⊓⊔
6 The linear quadratic control problem with ﬁnite
horizon
6.1 The main result
In this section we consider the control problem (1.1)–(1.2). We assume that
(H) is veriﬁed and denote by P ∈Cs

[0, ∞[ ; Σ+(H)

the mild solution of the
Riccati equation (2.7). We ﬁrst consider the closed loop equation

x′(t) = Ax(t) −BB⋆P(T −t)x(t),
t ∈[0, T ],
x(0) = x0 ∈H.
(6.1)

6 The linear quadratic control problem with ﬁnite horizon
409
Proposition 6.1. Assume that (H) is veriﬁed, and let x0 ∈H. Then (6.1)
has a unique mild solution x ∈C([0, T ]; H).
Proof. It follows from Proposition 3.6 (Part II, Chapter 1, §3).
⊓⊔
We now prove a basic identity.
Proposition 6.2. Assume that (H) is veriﬁed, and let u ∈L2(0, T ; U), x0 ∈
H. Let x be the mild solution of the state equation (1.1), and let P be the mild
solution of Riccati equation (2.7). Then the following identity holds:
J(u) =
 T
0
|u(s) + B⋆P(T −s)x(s)|2ds +

P(T )x0, x0

.
(6.2)
Proof. Let Pn be the solution to (2.10), and let xn be the solution to the
problem

x′
n(t) = Anxn(t) + Bu(t),
t ∈[0, T ],
xn(0) = x0 ∈H,
where the An’s are the Yosida approximations of A. We follow here a classical
argument; see for instance R. W. Brockett [1]. By computing the derivative
d
ds

Pn(T −s)xn(s), xn(s)

and completing the squares, we obtain the identity
d
ds

Pn(T −s)xn(s), xn(s)

= |u(s)+B⋆Pn(T −s)xn(s)|2 −|Cxn(s)|2 −|u(s)|2.
Integrating from 0 to T and letting n tend to inﬁnity, we obtain (6.2).
⊓⊔
We are now ready to prove the following result.
Theorem 6.1. Assume that (H) is veriﬁed, and let x0 ∈H. Then there exists
a unique optimal pair (u⋆, x⋆). Moreover the following statements hold:
(i) x⋆∈C([0, T ]; H) is the mild solution to the closed loop equation (6.1),
(ii) u⋆∈C([0, T ]; U) is given by the feedback formula
u⋆(t) = −B⋆P(T −t)x⋆(t),
t ∈[0, T ],
(6.3)
(iii) The optimal cost J(u⋆) is given by
J(u⋆) = (P(T )x0, x0).
(6.4)
Proof. We ﬁrst remark that by identity (6.2) it follows that
J(u) ≥

P(T )x0, x0

,
(6.5)
for any control u ∈L2(0, T ; U). Let now x⋆be the mild solution to (6.1), and
let u⋆be given by (6.3). Setting in (6.2) u = u⋆and taking into account (6.5),

410
IV-1 Bounded Control Operators: Control Inside the Domain
it follows that (u⋆, x⋆) is an optimal pair and that (6.4) holds. It remains to
prove uniqueness. Let (¯u, ¯x) be another optimal pair. Setting in (6.2), u = ¯u
and x = ¯x, we obtain
 T
0
|¯u(s) + B⋆P(T −s)¯x(s)|2 ds = 0,
so that ¯u(s) = −B⋆P(T −s)¯x(s) for almost every s in [0, T ]. But this implies
that ¯x is a mild solution of (6.1) so that ¯x = x⋆and, consequently, ¯u = u⋆.
⊓⊔
6.2 The case of unbounded observation
We now consider the case of unbounded observation, assuming that (H)–(i)–
(ii)–(iii)–(v) are veriﬁed. It is convenient to introduce the linear operator:
ξ 	→LT (ξ): D(A) →L2(0, T ; H),
where (LT ξ)(t) = CetAξ. As, by (H)–(v)
∥LT ξ∥2
L2(0,T ;H) ≤K(T )|ξ|2,
LT has an extension to the whole space H, which will still be denoted by LT .
Let x0 ∈H, u ∈L2(0, T ; U), and let x be the corresponding solution of
(1.1). We want to deﬁne the cost functional J(u); this is not a priori deﬁned
because x(t) does not necessarily belong to D(C) and the term
 T
0 |Cx(t)|2 dt
is not well deﬁned. We set
Cx(t) = (LT x0)(t) +
 t
0
LT

Bu(s)

(t −s) ds.
This deﬁnition is meaningful in virtue of the following lemma.
Lemma 6.1. Let z ∈L2(0, T ; H), and set
w(t) =
 t
0
LT

z(s)

(t −s) ds.
Then w ∈L2(0, T ; H) and the following estimate holds:
∥w∥2
L2(0,T ;H) ≤K(T )∥z∥2
L2(0,T ;H).
Proof. It is suﬃcient to verify the estimate for z ∈L2
0, T ; D(A)

; in this
case we have
w(t) =
 t
0
Ce(t−s)Az(s) ds.
Let ϕ ∈L2(0, T ; H); then we have

6 The linear quadratic control problem with ﬁnite horizon
411
 T
0

w(t), ϕ(t)

dt =
 T
0
ds
 T
s

Ce(t−s)Az(s), ϕ(t)

dt.
It follows that

 T
0

w(t), ϕ(t)

dt
 ≤
 T
0
ds
 T
s
|Ce(t−s)Az(s)|2 dt
1/2  T
s
|ϕ(t)|2dt
1/2
≤K(T )∥ϕ∥2
L2(0,T ;H)∥z∥2
L2(0,T ;H).
As this is true for any ϕ, we obtain the estimate.
⊓⊔
Now the following result is proved as Theorem 6.1.
Theorem 6.2. Assume that (H)–(i)–(ii)–(iii)–(v) are veriﬁed, and let x0 ∈
H. Then there exists a unique optimal pair (u⋆, x⋆). Moreover the following
statements hold:
(i) x⋆∈C([0, T ]; H) is the mild solution to the closed loop equation (6.1),
(ii) u⋆∈C([0, T ]; U) is given by the feedback formula (6.3),
(iii) the optimal cost J(u⋆) is given by (6.4).
6.3 Regularity properties of the optimal control
We give here some regularity results for the optimal pair (u⋆, x⋆).
Proposition 6.3. Assume that (H) is veriﬁed. Let P0 ∈D(A) and x ∈D(A),
where A is the linear operator deﬁned by (3.3). Then
x⋆∈C1([0, T ]; H) ∩C

[0, T ]; D(A)

,
u⋆∈C1([0, T ]; H).
Proof. By Proposition 3.2 we know that the mild solution P to the Riccati
equation (2.7) is a strict solution, so that
P ∈C1
s

[0, T ]; Σ(H)

∩Cs

[0, T ]; D(A)

.
Now the assertion concerning x⋆follows from Proposition 3.3 (Part II, Chap-
ter 1).
⊓⊔
Proposition 6.4. Assume that (H) is veriﬁed and that etA is an analytic
semigroup. Then the following statements hold:
(i) for any ε ∈]0, T/2[, x⋆(resp. u⋆) belongs to C∞(]ε, T −ε[ ; H) (resp.
C∞(]ε, T −ε[ ; U)).
(ii) u⋆and x⋆are analytic in ]0, T [.
Proof. (i) We ﬁrst remark that by (6.3) u∗∈C([0, T ]; U). Moreover, as (x⋆)′ =
Ax⋆+Bu⋆and x⋆(0) = x0, we have, by Proposition 3.10 (Part II, Chapter 1),
x⋆∈Cα([ε, T ]; H) (for any α ∈]0, 1[) and ε ∈[0, T ]. By (6.3) and Theorem 3.1
we have u⋆∈Cα([ε, T −ε]; U). Then, by Proposition 3.9 (Part II, Chapter 1),
x⋆belongs to C1([ε, T −ε]; H). By iterating this argument the assertion (i)
follows. Finally (ii) is a consequence of Theorem 3.2.
⊓⊔

412
IV-1 Bounded Control Operators: Control Inside the Domain
6.4 Hamiltonian systems
Assume that (H) is veriﬁed, let P be the mild solution of (2.7), and let (u⋆, x⋆)
be an optimal pair. In (6.1) it is useful to introduce the following function:
p⋆(t) = P(T −t)x⋆(t)
called the adjoint variable. If we diﬀerentiate p⋆(t), assuming for a while that
the conditions of Proposition 6.3 hold, we easily deduce from (2.7) and (6.1)
that p⋆(t) satisﬁes the equations

−p′(t) = A⋆p(t) + C⋆Cx(t),
P(T ) = P0x(T )
(6.6)
and thus considering (6.6) together with

x′(t) = Ax(t) −BB⋆p(t),
x(0) = x0,
(6.7)
we obtain a system (the two point boundary value problem) whose solution is
(x⋆, p⋆). The system (6.6)–(6.7) is called Hamiltonian system. Once we have
established (6.6)–(6.7) for regular x0, P0, it is easy to extend it to the general
case.
Remark 6.1. One can notice the analogy between (6.6)–(6.7) and (2.24), which
was the source of the explicit formula (2.22) for P(t).
⊓⊔
7 Some generalizations and complements
7.1 Nonhomogeneous state equation
Consider the following optimal control problem: To minimize
J(u) =
 T
0
{|Cx(t)|2 + |u(t)|2} dt +

P0x(T ), x(T )

(7.1)
over all controls u ∈L2(0, T ; U) subject to the diﬀerential equation constraint

x′(t) = Ax(t) + f(t) + Bu(t),
t ∈[0, T ],
x(0) = x0 ∈H.
(7.2)
We assume that hypothesis (H) holds and that f ∈L2(0, T ; H). Moreover
we denote by P the mild solution of the Riccati equation (2.7). We want to
show that it is possible to generalize the Dynamic Programming approach to
this more general situation. The main diﬀerence is the introduction of a dual

7 Some generalizations and complements
413
variable r, deﬁned by the following backward Cauchy problem: For all t in
[0, T ]

r′(t) +

A⋆−P(T −t)BB⋆
r(t) + P(T −t)f(t) = 0,
r(T ) = 0.
(7.3)
Notice that, by the change of variable t →T −t, problem (7.3) reduces to an
initial value problem, which has a unique mild solution.
The following identity generalizes identity (6.2).
Lemma 7.1. Let x0 ∈H, f ∈L2(0, T ; H), and u ∈L2(0, T ; U). Then we
have
J(u) = (P(T )x0, x0) + 2(r(0), x0) +
 T
0

2

r(s), f(s)

−|B⋆r(s)|2 
ds
+
 T
0
|u(s) + B⋆r(s) + B⋆P(T −s)x(s)|2 ds,
(7.4)
where x and r are, respectively, the mild solutions of (7.2) and (7.3).
Proof. Let Pn be the solution of (2.10), and let xn and rn be the solutions of
the problems

x′
n(t) = Anxn(t) + f(t) + Bu(t),
xn(0) = x0,

r′
n(t) = −

A⋆
n −Pn(T −t)BB⋆
rn(t) −Pn(T −t)f(t),
rn(T ) = 0.
Then, by integrating the identity
d
ds

Pn(T −s)xn(s), xn(s)

+ 2

rn(s), xn(s)
 
= |B⋆Pn(T −s)xn(s) + B⋆rn(s) + u(s)|2 + 2

rn(s), f(s)

−|B⋆rn(s)|2
−|Cxn(s)|2 −|u(s)|2
between 0 and T and by letting n tend to inﬁnity, we obtain (7.4).
⊓⊔
By using identity (7.4) we can easily generalize Theorem 6.1. The proof is
similar and will be omitted.
Theorem 7.1. Assume that (H) is veriﬁed, let x0 ∈H, and let f ∈L2(0, T ; H).
Then there exists a unique optimal pair (u⋆, x⋆) for problem (7.1)–(7.2). More-
over the following statements hold:
(i) x⋆is the mild solution to the closed loop equation
x′(t) =

A−BB⋆P(T −t)

x(t)−BB⋆r(t)+f(t),
t ∈[0, T ], x(0) = x0,

414
IV-1 Bounded Control Operators: Control Inside the Domain
(ii) u⋆is given by the feedback formula
u⋆(t) = −B⋆[P(T −t)x⋆(t) + r(t)],
where r is the mild solution to (7.3),
(iii) the optimal cost is given by
J(u⋆) =

P(T )x0, x0

+ 2

r(0), x0

+
 T
0


2r(s), f(s)

−|B⋆r(s)|2
ds.
Remark 7.1. (Tracking Problem). Consider the following optimal control prob-
lem: To minimize
K(u) =
 T
0
{|Cx(t) −ξ(t)|2 + |u(t)|2} dt
+

P0

x(T ) −ξ(T )

,

x(T ) −ξ(T )

(7.5)
over all controls u ∈L2(0, T ; U) subject to the diﬀerential equation constraint

z′(t) = Az(t) + Bu(t),
t ∈[0, T ],
z(0) = z0 ∈H.
(7.6)
Here ξ ∈W 1,2([0, T ]; H)∩L2
0, T ; D(A)

is a given function. Now, by setting
x = z −ξ,
f = Aξ −ξ′,
x0 = z0 −x(0),
equation (7.6) reduces to equation (7.2) and problem (7.5)–(7.6) to problem
(7.1)–(7.2).
⊓⊔
7.2 Time-dependent state equation and cost function
Consider the system

x′(t) = A(t)x(t) + B(t)u(t),
x(0) = x0,
(7.7)
where A(t): D

A(t)

⊂H →H and B(t) ∈L(U; H) are linear operators.
We make the following assumptions on the families {A(t)}t∈[0,T ] and
{B(t)}t∈[0,T ]:

7 Some generalizations and complements
415
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
(i)
A(t): D

A(t)

⊂H →H generates a C0 semigroup in H
for all t ∈[0.T ],
(ii)
there exists a strongly continuous mapping
UA(·, ·): {(t, s) ∈R2 : t ≥s} →L(H)
such that U ∗
A(·, ·) is also strongly continuous and
∂
∂tUA(t, s)x = A(t)UA(t, s)x, UA(s, s)x = x,
∀x ∈D

A(t)

, 0 ≤s ≤t ≤T,
(iii)
we have
lim
n→∞UAn(t, s)x = UA(t, s)x, ∀x ∈H uniformly on
the bounded sets of {(t, s) ∈R2 : t ≥s}, where
UAn(t, s) is the evolution operator generated by the Yosida
approximations of A(t),
(iv)
B(·)u is continuous for all u ∈U.
(7.8)
Under these assumptions problem (7.7) has a unique mild solution given by
x(t) = U(t, 0)x0 +
 t
0
U(t, s)Bu(s) ds.
Assumptions (7.8) are veriﬁed in many problems both parabolic and hy-
perbolic (see for instance P. Acquistapace and B. Terreni [1], A. Lu-
nardi [3], A. Pazy [2], and H. Tanabe [1]).
We want to minimize the cost function
J(u) =
 T
0

|C(s)x(s)|2 + |u(s)|2 
ds +

P0x(T ), x(T )

over all controls u ∈L2(0, T ; U) subject to the diﬀerential equation constraint
(7.7). Concerning the operators B, C, and P0, we shall assume that
P0 ∈Σ+(H),
C ∈Cs

[0, T ]; L(H; Y )

,
B ∈Cs(

0, T ]; L(U; H)

.
(7.9)
It is convenient to consider the backward Riccati equation

Q′ + A⋆Q + QA −QBB⋆Q + C⋆C = 0,
Q(T ) = P0.
(7.10)
Its corresponding mild form is
Q(t)x = U ⋆(T, t)P0U(T, t)x +
 T
t
U ⋆(s, t)C⋆(s)C(s)U(s, t)x ds
−
 T
t
U ⋆(s, t)Q(s)B(s)B⋆(s)Q(s)U(s, t)x ds,
x ∈H,

416
IV-1 Bounded Control Operators: Control Inside the Domain
where U ⋆(s, t) is the adjoint of U(s, t).
The following theorem is proved as Theorem 2.1.
Theorem 7.2. Assume (7.8) and (7.9). Then problem (7.10) has a unique
mild solution Q ∈Cs

[0, ∞[ ; Σ+(H)

and
lim
n→∞Qn = Q
in Cs

[0, ∞[ ; Σ(H)

,
for any T > 0, where Qn is the solution to problem (7.10) with A(t) replaced
by An(t).
The following monotonicity property of the solutions of Riccati equations
is proved as Proposition 2.2.
Proposition 7.1. Consider the Riccati equations

Q′
i + A⋆
i Qi + QiAi −QiBiB⋆
i Qi + C⋆
i Ci = 0,
Qi(T ) = Qi,0,
i = 1, 2.
(7.11)
Assume that (Ai, Bi, Ci, Pi,0) verify (7.8) and (7.9) for i = 1, 2 and that, in
addition,
P1,0 ≤P2,0,
C⋆
1(t)C1(t) ≤C⋆
2(t)C2(t),
B2(t)B⋆
2(t) ≤B1(t)B⋆
1(t),
t ∈[0, T ].
Then we have
P1(t) ≤P2(t),
t ≥0.
(7.12)
By using Theorem 7.2, we can easily generalize Theorem 6.1.
Theorem 7.3. Assume (7.8) and (7.9), and let x0 ∈H. Then there exists
a unique optimal pair (u⋆, x⋆) and u⋆∈C([0, T ]; U) is related to x⋆by the
feedback formula
u⋆(t) = −B⋆(t)Q(t)x⋆(t),
t ∈[0, T ].
Finally,the optimal cost J(u⋆) is given by
J(u⋆) =

Q(0)x0, x0

.
Remark 7.2. For the time-varying variational case the reader is referred to the
book of J. L. Lions [3].
⊓⊔
7.3 Dual Riccati equation
We assume here that (H) is veriﬁed and consider the Riccati equation

P ′ = A∗P + PA −PBB∗P + C∗C,
t ≥0,
P(0) = P0
(7.13)

7 Some generalizations and complements
417
and the dual equation

Q′ = AQ + QA∗−QC∗CQ + BB∗,
t ≥0,
Q(0) = Q0,
(7.14)
where Q0 ∈Σ+(H). Clearly the set (A∗, C∗, B∗, Q0) also veriﬁes assump-
tion (H); thus the equations (7.13) and (7.14) have unique solutions P, Q ∈
Cs

[0, +∞[ ; Σ+(H)

, respectively.
We want to present here a simple formula that gives P in terms of Q. This
formula will turn out to be useful in Chapter 3 below, and it was proved in
V. Barbu and G. Da Prato [1], by variational methods.
Fix T > 0, and set
˜QT (s) = Q(T −s),
s ∈[0, T ],
(7.15)
and
GT (s) = A∗−C∗CQ(T −s),
s ∈[0, T ];
(7.16)
we shall denote by UGT the evolution operator associated with GT .
We ﬁrst prove a lemma.
Lemma 7.2. Fix T > 0, and set
ZT (t) = [I + P(t) ˜QT (t)]−1P(t) = P(t)[I + ˜QT (t)P(t)]−1.
Then for any x ∈H we have
ZT (t)x = UGT (t, 0)ZT (0)U ∗
GT (t, 0)x+
 t
0
UGT (t, s)C∗CU ∗
GT (t, s)x ds. (7.17)
Proof. We assume that A is bounded; otherwise we replace A by its Yosida
approximations An and then we let n tend to inﬁnity. We ﬁrst remark that
the deﬁnition of ZT is meaningful by Proposition 1.1 in Appendix A. Setting
for simplicity Z = ZT and ˜Q = ˜QT we have
(I + P ˜Q)Z = P
and so
(P ′ ˜Q + P ˜Q′)Z + (I + P ˜Q)Z′ = P ′,
which implies
(I + P ˜Q)Z′ = P ′ −(P ′ ˜Q + P ˜Q′)[I + P ˜Q]−1P
= P ′ −(P ′ ˜Q + P ˜Q′)P[I + ˜QP]−1
= [P ′(I + ˜QP) −(P ′ ˜Q + P ˜Q′)P][I + ˜QP]−1
= [P ′ −P ˜Q′P][I + ˜QP]−1.

418
IV-1 Bounded Control Operators: Control Inside the Domain
It follows that
(I + P ˜Q)Z′[I + ˜QP] = P ′ −P ˜Q′P.
Then, by substituting P ′ and Q′ with the expressions given by (7.13) and
(7.14), we have
[I + P ˜Q]Z′[I + ˜QP] = A∗P + PA −PBB∗P + C∗C
+ P[A ˜Q + ˜QA∗−˜QC∗C ˜Q + BB∗]P
= [I + P ˜Q](A∗−C∗C ˜Q)P + P(A −˜QC∗C)[I + ˜QP]
+ [I + P ˜Q]C∗C[I + ˜QP].
Therefore
Z′ = (A∗−C∗C ˜Q)Z + Z(A −˜QC∗C) + C∗C,
and the conclusion follows.
⊓⊔
Theorem 7.4. Assume that (H) is veriﬁed, and let P and Q be the respective
solutions in Cs

[0, +∞[ ; Σ+(H)

of (7.13) and (7.14) with Q0 = 0. Let T > 0
and set
GT (t) = A∗−C∗C ˜QT (t) = A∗−C∗CQ(T −t),
t ∈[0, T ].
Then we have
P(T ) = UGT (T, 0)[I + P0Q(T )]−1P0U ∗
GT (T, 0)
+
 T
0
UGT (T, s)C∗CU ∗
GT (T, s) ds.
(7.18)
Proof. As
ZT (0) = [I + P0Q(T )]−1P0
and ZT (T ) = P(T ), the conclusion follows from (7.17).
⊓⊔
8 Examples of controlled systems
8.1 Parabolic equations
Let Ωbe an open bounded set of Rn with regular boundary ∂Ω. Consider the
state equation
⎧
⎪
⎪
⎨
⎪
⎪
⎩
∂x
∂t (t, ξ) =

∆ξ + c)x(t, ξ) + (Bu(t, ·)

(ξ),
in ]0, T ] × Ω,
x(t, ξ) = 0
on ]0, T ] × ∂Ω,
x(0, ξ) = x0(ξ)
in Ω,
(8.1)

8 Examples of controlled systems
419
where c ∈R and B ∈L

L2(Ω)

. We choose H = U = Y = L2(Ω) as space
of states, controls, and observations. The control u is said to be a distributed
control. We denote by A the linear self-adjoint operator in H:

Ax = ∆ξx + cx,
∀x ∈D(A),
D(A) = H2(Ω) ∩H1
0(Ω).
Setting x(t) = x(t, ·), u(t) = u(t, ·) we can write problem (8.1) in the abstract
form (1.1). By Proposition 2.11 (Part II, Chapter 1), A is the inﬁnitesimal
generator of an analytic semigroup in H. Let C and P0 be non-negative linear
bounded operators in L2(Ω). Consider the following problem: To minimize
J(u) =
 T
0

Ω

Cx(t, ·)

(ξ)
2 +|u(t, ξ)|2 
dt dξ +

Ω

P0x(T, ·)(ξ)x(T, ξ)

dξ
subject to state equation (8.1). We remark that assumption (H) is fulﬁlled.
Due to Theorem 2.1, the Riccati equation (2.7) has a unique solution P ∈
Cs

[0, T ]; Σ+(H)

. Then, by Theorem 6.1, there exists a unique optimal pair
(u⋆, x⋆), where x⋆∈C

[0, T ]; L2(Ω)

is the classical solution in ]0, T [ to the
closed loop equation
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
∂x
∂t (t, ξ) = (∆ξ + c)x(t, ξ) −(BB⋆
P(T −t)x(t, ·)

(ξ),
in ]0, T ] × Ω,
x(t, ξ) = 0
on ]0, T ] × ∂Ω,
x(0, ξ) = x0(ξ)
in Ω,
that is,
∀ε ∈]0, T [,
∂x
∂t
and
∂2x
∂ξ2 ∈L2([ε, T −ε] × Ω).
Moreover u⋆is given by the formula
u⋆(t, ξ) = −

B⋆
P(T −t)x⋆
t, ·)

(ξ)

.
Consider now a problem with unbounded observation: To minimize
J(u) =
 T
0

Ω
{|(∇ξx(t, ξ)|2 + |u(t, ξ)|2} dt dξ +

Ω
|x(T, ξ)|2 dξ
subject to state equation (8.1). In this case the linear operator C is given by
Cx = −
%
∆ξx,
and Proposition 4.2 can be applied.

420
IV-1 Bounded Control Operators: Control Inside the Domain
8.2 Wave equation
Let Ωbe as in the previous example and consider the problem:
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
∂2x
∂t2 (t, ξ) = ∆ξx(t, ξ) +

Bu(t, ·)

(ξ)
in ]0, T ] × Ω,
x(t, ξ) = 0
on ]0, T ] × ∂Ω,
x(0, ξ) = x0(ξ),
in Ω,
∂x
∂t (0, ξ) = x1(ξ)
in Ω.
(8.2)
Set
H = Y = H1
0(Ω) ⊕L2(Ω),
U = L2(Ω)
(8.3)
and denote by X =
9
x0
x1
:
, the generic element of H. The inner product in H
is deﬁned by

x0
x1
	
,

z0
z1
	
=

Ω
(∇ξx0 · ∇ξz0 + x1z1) dξ.
(8.4)
Let Λ be the self-adjoint positive operator on L2(Ω) deﬁned by
D(Λ) = H2(Ω) ∩H1
0(Ω),
Λx = −∆ξx.
(8.5)
Then we have
(X, Z) = (
√
Λx0,
√
Λz0) + (x1, z1).
Deﬁne the linear operator A on H:
⎧
⎪
⎨
⎪
⎩
AX =

0 1
−Λ 0
 
x0
x1

,
∀X ∈D(A),
D(A) = H2(Ω) ∩H1
0(Ω) ⊕H1
0(Ω).
(8.6)
We have A⋆= −A and by Proposition 2.12 (Part II, Chapter 1) we know
that A is the inﬁnitesimal generator of a contraction group in H. Let ﬁnally
B ∈L(U; H) be deﬁned as
Bu =
0
u
	
,
u ∈U;
then B⋆∈L(H, U) is given by
B⋆
x0
x1
	
= x1,
x0
x1
	
∈H,
and we have
BB⋆
x0
x1
	
=
 0
x1
	
=
0 0
0 1
	 x0
x1
	
.

8 Examples of controlled systems
421
Setting
x0(t) = x(t, ·),
x1(t) = ∂x
∂t (t, ·),
u(t) = u(t, ·),
we can write (8.2) in the abstract form
Y ′ = AY + Bu,
Y (0) = Y0,
(8.7)
where
Y (t) =
x0(t)
x1(t)
	
,
Y0 =
x0
x1
	
.
We are interested in the following optimal control problem: To minimize
J(u) =
 T
0

Ω

|∇ξx(t, ξ)|2 +

∂x
∂t (t, ξ)

2
+ |u(t, ξ)|2

dt dξ
+

Ω

|∇ξx(T, ξ)|2 +

∂x
∂t (T, ξ)

2
dξ,
(8.8)
over all u ∈L2([0, ∞] × Ω) subject to (8.2). The cost function J(u) can be
written as
J(u) =
 T
0
{|Y (t)|2
H + |u(t)|2
U} dt + |Y (T )|2
H.
The Riccati equation is
P ′ =
0 −1
Λ 0
	
P + P

0 1
−Λ 0
	
−P
0 0
0 1
	
P +
1 0
0 1
	
,
P(0) =

1 0
0 1
	
.
We can represent P as
P =

P11 P12
P21 P22
	
,
where
P11 ∈L

H1
0(Ω)

,
P12 ∈L

L2(Ω); H1
0(Ω)

,
P21 ∈L

H1
0(Ω); L2(Ω)

,
P22 ∈L

L2(Ω)

,
and the following identities hold:
P ⋆
11 = ΛP11Λ−1,
P ⋆
12 = P21Λ−1,
P ⋆
21 = ΛP12,
P ⋆
22 = P22.
Setting C = I, we can apply Theorem 2.1 and conclude that the Riccati
equation has a unique mild solution P ∈Cs

[0, ∞[ ; Σ+(H)

. Moreover by
Theorem 6.1, there exists a unique optimal pair (u⋆, x⋆) and
u⋆(t, ξ) = −

P21(T −t)x⋆(t, ·)

(ξ) −

P22(T −t)∂x⋆
∂t (t, ·)

(ξ).

422
IV-1 Bounded Control Operators: Control Inside the Domain
The closed loop equation is
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
∂2x
∂t2 (t, ξ) = ∆ξx(t, ξ) −

P21(T −t)x(t, ·)

(ξ)
−

P22(T −t)∂x
∂t (t, ·)

(ξ)
in ]0, T [ × Ω,
x(t, ξ) = 0
on ]0, T [ × ∂Ω,
x(0, ξ) = x0(ξ),
∂x
∂t (t, ξ) = x1(ξ)
in Ω.
Remark 8.1. Similar results can be obtained if one replaces in problem (8.2)
the Dirichlet by the Neumann boundary condition.
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
∂2x
∂t2 (t, ξ) = ∆ξx(t, ξ) +

Bu(t, ·)

(ξ)
in ]0, T ] × Ω,
∂x
∂ν (t, ξ) = 0
on ]0, T ] × ∂Ω,
x(0, ξ) = x0(ξ)
in Ω,
∂x
∂t (0, ξ) = x1(ξ)
in Ω,
(8.9)
where ν is the outward normal to ∂Ω. In this case we set
H = Y = H1(Ω) ⊕L2(Ω),
U = L2(Ω),
(8.10)
and deﬁne the scalar product in H by
x0
x1
	
,
z0
z1
	
=

Ω
(∇ξx0 · ∇ξz0 + x1z1) dξ +

Ω
x0z0 dξ.
(8.11)
Moreover we deﬁne a self-adjoint positive operator Λ1 on L2(Ω) by
⎧
⎨
⎩
D(Λ1) =

x ∈H2(Ω): ∂x
∂ν = 0
on ∂Ω

,
Λ1x = −∆ξx,
(8.12)
and the linear operator A1 on H:
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
A1X =

0 1
−Λ1 0
 
x0
x1

,
∀X ∈D(A1),
D(A1) =

x ∈H2(Ω): ∂x
∂ν = 0
on ∂Ω

⊕H1
0(Ω).
(8.13)
Now all previous considerations can be easily generalized.
⊓⊔

8 Examples of controlled systems
423
8.3 Delay equations
We start with a simple example given in §2.10 (Part II, Chapter 1). A more
elaborate treatment of diﬀerential delay systems with delays in the control
and observation variables will be given after using the general state space
frameworks developed in Chapter 4 of Part II.
Let D, E ∈L(Cn), r > 0, and consider the problem:
⎧
⎪
⎨
⎪
⎩
z′(t) = Dz(t) + Ez(t −r) + u(t),
t ≥0,
z(0) = h0 ∈Cn,
z(θ) = h1(θ),
a.e. θ ∈[−r, 0],
(8.14)
where h1 ∈L2(−r, 0; Cn) and u ∈L2(0, T ; Cn). We shall use here the notation
of §2.10 (Chapter 1 in Part II). We set
H = L2(−r, 0; Cn),
U = Cn,
˜x(t) =

z(t), zt

.
Then problem (8.14) is equivalent to
˜x′ = A˜x + Bu,
˜x(0) = (h0, h1),
where A is deﬁned by (2.104) in Chapter 1 of Part II and B is the linear
operator
u 	→Bu = (u, 0): Cn →H.
We remark that the adjoint operator B⋆is given by
(x0, x1) 	→B⋆(x0, x1) = x0 : H →Cn.
Consider now the problem: To minimize
J(u) =
 T
0
{|z(t)|2 + |u(t)|2} dt + |z(T )|2
over all u ∈L2(0, T ; Cn) subject to (8.14). J(u) can be written as
J(u) =
 T
0
{|Cx(t)|2 + |u(t)|2} dt +

P0z(T ), z(T )

Cn×L2(−r,0;Cn),
where
C(x0, x1) = (x0, 0)
and
P0(x0, x1) = (x0, 0).
The Riccati equation is
P ′ = A⋆P + PA −P 2 + C⋆C,
P(0) = P0,
because, as easily seen, BB⋆= [ I 0
0 0 ].
We can write P as

424
IV-1 Bounded Control Operators: Control Inside the Domain
P =
P01 P01
P10 P11
	
,
where
P00 ∈L(Cn),
P01 ∈L

L2(0, T ; Cn); Cn
,
P10 ∈L

Cn;

L2(0, T ; Cn)

, P11 ∈L

L2(0, T ; Cn
.
Then the optimal control is given by
u⋆(t) = −P00(T −t)z⋆(t) −P01(T −t)z⋆
t .
We now consider the more general situation. The abstract theory devel-
oped in Chapter 4 of Part II (Theorem 4.1) for unbounded observation oper-
ators applies to a class of delay systems with delayed observations where the
control operator of the state equation is bounded. Following the notation and
deﬁnitions of §5 (Chapter 4, Part II), a delay diﬀerential system with controls
and observations is given by

˙x(t) = Lxt + But,
t > 0,
(x(0), x0, u0) = (ϕ0, ϕ1, w) ∈M 2 × L2(−h, 0; Rm),
(8.15)
y(t) = Cxt,
t > 0.
(8.16)
The spaces of controls and observation are U = Rm and Y = Rk, respectively.
By introducing the extended structural state ˆx(t) in H = M 2 × L2(−h, 0; Y )
(cf. Part II, Chapter 4, §6.2, Theorem 6.1, equations (6.11) to (6.14)), system
(8.15)–(8.16) can be transformed in the form
⎧
⎨
⎩
d
dt ˆj∗ˆx(t) = ( ˜A⊤)∗ˆx(t) + ( ˜B⊤)∗u(t),
t > 0,
ˆx(0) = ξ,
(8.17)
where ˜B⊤: D( ˜A⊤) →U and ˆC : D

( ˜A⊤)∗
→Y are continuous linear maps
when the domains are endowed with their respective graph norm topologies.
The operator ˜B⊤is bounded and continuous on H when it is of the form
˜B⊤(ψ0, ψ1, v) = B⊤
0 ψ0 +
 0
−h
B⊤
1 (θ)ψ1(θ) dθ
(8.18)
for some matrices B0 and B1(θ), θ ∈I(−h, 0), such that the elements of B1(·)
belong to L2(−h, 0). Then
( ˜B⊤)∗u =

B0u, B1(·)u, 0

.
(8.19)
So we specialize to systems (8.15) of the form

8 Examples of controlled systems
425
⎧
⎪
⎨
⎪
⎩
˙x(t) = Lxt + B0u(t) +
 0
−h
B1(θ)ψ1(θ)u(t + θ) dθ,
t > 0,
(x(0), x0, u0) = (ϕ0, ϕ1, w).
(8.20)
The operator ˆC is of the general form (cf. Part II, Chapter 4, §6.2.2)
ξ = (ϕ0, ¯Lϕ1 + ζ, ¯Cϕ1 + λ) 	→ˆCξ = Cϕ1 + λ(0): D

( ˜A⊤)∗
→Y,
(8.21)
where (ϕ, ζ, λ) is a representation of the elements of D

( ˜A⊤)∗
.
So we only need to check part (ii) of condition (H)–(v)
 t
0
| ˆCes( ˜
A⊤)∗ξ|2
Y ds ≤K(t)|ξ|2
H,
∀ξ ∈D

( ˜A⊤)∗
.
In the notation of Part II, Chapter 4, et( ˜
A⊤)∗is ( ˜S⊤)∗(t) and
ˆC( ˜S⊤)∗(t)ξ = Cˆx(t) = Cxt + [τ(t)λ](0)
= (Ce0
+x)(t) + (e−h
+ ξ2)(−t),
which is continuous as an element of L2(0, T ; Y ) with respect to ξ and
(e−h
+ ξ2)(−t) is the shift of an L2–function. This holds for each ﬁnite T > 0.
As a result we have the following proposition.
Proposition 8.1. When the mapping B is of the form (8.18), the Riccati
equation

P ′ = ˜A⊤P + P( ˜A⊤)∗−P( ˜B⊤)∗˜B⊤P + ˆC∗ˆC,
P(0) = P0,
(8.22)
has a unique mild solution P in Cs

[0, ∞[; Σ+(H)

, where H = M 2 ×
L2(−h, 0; Y ).
The Riccati equation (8.22) on [0, T ], T > 0, is associated with the mini-
mization of the cost function
J(u) =
 T
0
[|Cxt|2 + |u(t)|2] dt + Fx(T ) · x(T )
(8.23)
over all u in L2(0, T ; Y ), where
P0(ξ0, ξ1, ξ2) = (Fξ0, 0, 0),
∀ξ = (ξ0, ξ1, ξ2).
The optimal control u∗can be expressed in feedback form
u∗(t) = −˜B⊤P(T −t)ˆx(t),
t > 0,
where for the initial conditions in (8.15)
ˆx(t) =

x(t), ¯Lxt + ¯Bu∗
t, ¯Cxt

.

426
IV-1 Bounded Control Operators: Control Inside the Domain
As P(t) can be decomposed into a 3×3 matrix of operators on the state space
H = Rn × L2(−h, 0, Rn) × L2(−h, 0, Y )
P(t) =
⎡
⎣
P00(t) P01(t) P02(t)
P10(t) P11(t) P12(t)
P20(t) P21(t) P22(t)
⎤
⎦,
and ˜B⊤has the special form (8.18)
˜B⊤P(T −t)ˆx(t)=B⊤
0 {P00(T −t)x(t)+P01(T −t)x(t)[¯Lxt+ ¯Bu∗
t ]
+P02(T −t) ¯Cxt} +
 0
−h
B⊤
1 (θ){P10(T −t)x(t)
+ P11(T −t)x(t)[¯Lxt + ¯Bu∗
t] + P12(T −t) ¯Cxt}(θ) dθ.
It is important to notice that the feedback is through the structural operators
¯L, ¯B, and ¯C on xt and u∗
t . This state captures the minimal information
necessary for the feedback synthesis. Without this notion of state, determining
directly expressions as above is almost hopeless in the general case. We leave
it to the reader to specify ¯L, ¯B and ¯C in special cases!
The same problem can also be formulated by using the extended state ˜x(t)
in H = M 2 × L2(−h, 0; U) (cf. Part II, Chapter 4, §§5.2 and 6.1, eqs. (6.7) to
(6.10)) which veriﬁes the state equation
⎧
⎨
⎩
d
dt˜ı∗˜x(t) = ( ˜A∗)∗˜x(t) + ( ˆB⊤)∗u(t),
˜x(0) = (ϕ0ϕ1, w),
(8.24)
with the observation equation
y(t) = ˜C˜x(t)
(8.25)
(cf. Chapter 4, §6, equations (6.7) and (6.8)), where ˆB⊤: D( ˜A∗) →U and
˜C : D( ˜A) →Y are continuous linear maps when the domains are endowed
with their respective graph norm topologies. When the control operator is
given as in (8.20), the operator ˆB⊤is equal to
ˆB⊤(ξ) = B⊤
0 ψ(0) +
 0
−h
B⊤
1 (θ)ψ(θ) dθ + λ(0),
(8.26)
where ψ, ζ, λ is a representation of an arbitrary element ξ of D( ˜A∗)
ξ =

ψ(0), ¯L⊤ψ + ζ, ¯B⊤ψ + λ

.
The operator ˆB⊤is unfortunately not bounded on H. As for ˜C it takes the
simple form
˜C(ϕ(0), ϕ, w) = Cϕ,
(8.27)

8 Examples of controlled systems
427
and part (ii) of condition (H)–(v)
 t
0
| ˜Ces ˜
Aξ|2
Y ds ≤K(t)|ξ|2
H,
∀ξ ∈D( ˜A),
is veriﬁed. In the notation of Part II, Chapter 4, et ˜
A is ˜S(t) and
˜C ˜S(t)ξ = ˜C(x(t), xt, (e0
−w)t) = (Cx)(t),
which is continuous as an element of L2(0, T ; Y ) with respect to ξ in M 2 ×
L2(0, T ; U). However ˆB⊤is not bounded on H because it contains a delta
function. So we cannot use the general theory of §4.
However we have seen in §6.3 of Chapter 4 in Part II that ˆx and ˜x are
intertwined via the structural operator
F =
⎡
⎣
I
0
0
0 ¯L
¯B0
0
¯C
¯B1
⎤
⎦
(8.28)
(cf. (6.74) and (6.77) to (6.80) in Theorem 6.2). Then for the optimal control
u∗
ˆx

t; F(φ0, φ1, w), u∗
= F ˜x

t; (φ0, φ1, w), u∗
.
So it is not too diﬃcult to guess that for the formulation (8.23)–(8.24)–(8.25),
there will be a Riccati equation and a solution Π(t) related to P(t) through
the identity
Π(t) = F ⊤P(t)F.
To see that, multiply the Riccati equation (8.22) by F ⊤on the left and F on
the right and use the intertwining identities
ˆCF = ˜C,
ˆB⊤F ⊤= ˜B⊤,
F ⊤˜A⊤= ˜A∗F ⊤,
˜A⊤∗F = F ˜A
(cf. Part II, Chapter 4, (6.79), (6.80) and from (6.78)) to obtain

Π′ = ˜A∗Π + Π ˜A −Π( ˆB⊤)∗ˆB⊤Π + ˜C∗˜C,
Π(0) = F ⊤P0F.
(8.29)
It corresponds to the control of an inﬁnite dimensional system with both
unbounded control and observation operators. We know what the Riccati
equation associated with the state ˜x(t) will look like, but it remains to give
a precise meaning to such an equation and establish the properties of its
solution. The necessary techniques that would allow us to do it are diﬀerent
than the ones that will be developed in the next two chapters.
The last equation is associated with the same problem (8.23) with
Π0(ξ0, ξ1, ξ2) = (Fξ0, 0, 0),
∀ξ = (ξ0, ξ1, ξ2).
(8.30)

428
IV-1 Bounded Control Operators: Control Inside the Domain
The optimal control u∗can be expressed in feedback form
u∗(t) = −ˆB⊤Π(T −t)˜x(t),
t > 0,
where for the initial conditions in (8.15)
˜x(t) =

x(t), xt, ut

.
However in view of (8.26) the interpretation of ˆB⊤Π(T −t) is not clear unless
some additional properties of Π(t) are obtained. In that respect the extended
state is simpler to deﬁne but because it does not incorporate the delay struc-
ture of the problem, the feedback law is diﬃcult to explicit. For a simple ex-
ample of this property, the reader is referred to M. C. Delfour, E. B. Lee,
and A. Manitius [1] and M. C. Delfour [15]. For a more detailed bibliog-
raphy on the control of delay systems, the reader is referred to the references
in Chapter 4 of Part II. Although the general case with delays in control and
observation has a unique solution, it was not possible here to detail all special
cases and the history of the problems
8.4 Evolution equations in noncylindrical domains
Consider the state equation
⎧
⎪
⎪
⎨
⎪
⎪
⎩
∂x
∂t (t, ξ) = ∆ξx(t, ξ) + u(t, ξ),
t ∈]0, T ],
ξ ∈Ωt,
x(t, ξ) = 0,
t ∈]0, T ],
ξ ∈∂Ωt,
x(0, ξ) = x0(ξ),
ξ ∈Ω0,
(8.31)
where for any t ∈[0, T ], Ωt is a bounded set in Rn with a regular boundary
∂Ωt.
We want to minimize the cost
J(u) =
 T
0
dt

Ωt
{|x(t, ξ)|2 + |u(t, ξ)|2} dξ +

ΩT
|x(T, ξ)|2 dξ
(8.32)
over all controls u ∈L2([0, T ] × Ωt) subject to state equation (8.31). By
L2([0, T ] × Ωt) we mean the set of all functions u
(t, ξ) 	→u(t, ξ): {(t, ξ): t ∈[0, T ], ξ ∈Ωt} →R,
such that t 	→

Ωt |u(t, ξ)|2 dξ is measurable and
 T
0
dt

Ωt
|u(t, ξ)|2 dξ < +∞.
We ﬁrst reduce problem (8.31) to an evolution equation in a cylindrical do-
main, following G. Da Prato and J. P. Zol´esio [1]. To this end we intro-
duce, as in J. P. Zol´esio [1], a suitable change of variables:

8 Examples of controlled systems
429
x(t, ξ) = z

t, Tt(ξ)

,
u(t, ξ) = v

t, Tt(ξ)

,
where Tt is a regular mapping such that
Tt(Ω0) = Ωt,
t ∈[0, T ].
Then problem (8.31) reduces to
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
zt = J−1
t
div

Jt

(DTt)−1⋆(DTt)−1∇z

+

(DTt)−1⋆∇z · d
dtTt + v,
t ∈[0, T ], ξ ∈Ω0,
z(t, ξ) = 0,
t ∈[0, T ],
ξ ∈∂Ω0,
z(0, ξ) = x0(ξ),
ξ ∈Ω0,
(8.33)
where Jt is the determinant of the Jacobian matrix DTt of Tt.
We set H = U = L2(Ω0), z(t, ·) = z(t), v(t, ·) = v(t) and write (8.33) as
an abstract evolution equation in H
z′(t) = A(t)z(t) + v(t),
z(0) = x0,
where A(t) is deﬁned by
⎧
⎨
⎩
A(t)w = J−1
t
div

Jt

(DTt)−1⋆(DTt)−1∇w] +

(DTt)−1⋆∇w · d
dtTt,
D

A(t)

= H2(Ω0) ∩H1
0(Ω0).
A(t) is the realization of an elliptic operator under Dirichlet boundary condi-
tions, and consequently it is the inﬁnitesimal generator of an analytic semi-
group in H. Moreover we have A ∈C1
[0, T ]; L(D; H)

so that by H. Tan-
abe [1], assumptions (7.8) hold. The cost function (8.32) becomes
J(v) =
 T
0
dt

Ω0
Jt

T −1
t
(x)

|z(t, ξ)|2 + |v(t, ξ)|2
dξ +

Ω0
|z(T, ξ)|2 dξ,
and we can study the problem as in §7.2.
Remark 8.2. It is also possible to study problem (8.31) without changing vari-
ables. By proceeding as in P. Cannarsa, G. Da Prato, and J. P. Zol´esio
[1], we assume that there exists a bounded open set D that contains all sets
Ωt, t ∈[0, T ], and introduce unbounded operators {A(t)}, t ∈[0, T ] with
domain depending on time, but in the ﬁxed space H = L2(D)
⎧
⎪
⎨
⎪
⎩
D

A(t)

= {y ∈L2(D): y

Ωt ∈H2(Ωt), y

D\Ωt ∈H2(D\Ωt),
y = 0
on ∂D,
y = 0
on ∂Ωt}
A(t)u = ∆u, ∀u ∈D

A(t)

.
Then we write problem (8.31) in the abstract form (7.7) and prove that (7.8)
hold.
⊓⊔
Remark 8.3. An optimal control problem for the wave equation in moving
domain has been studied with similar techniques in G. Da Prato and
J. P. Zol´esio [2].
⊓⊔

2
Unbounded Control Operators: Parabolic
Equations With Control on the Boundary
1 Introduction
As in the previous chapter, we shall denote by H, U, and Y the Hilbert spaces
of states, controls, and observations, respectively. We consider a dynamical
system, whose state x(t) is subject to the following equation:
x′(t) = Ax(t) + Bu(t),
x(0) = x0 ∈H,
where u ∈L2(0, T ; U) and A: D(A) ⊂H →H generates an analytic semi-
group in H. However, in the current case, the linear operator B is not supposed
to be bounded from U into H. This situation has been discussed at length in
Chapters 1 and 2 (Part II). However some key constructions will be repeated
here as needed. In that case many possibilities could be considered. However,
in practice, it will be natural to consider situations where B maps U into the
dual space

D(A⋆)
′ of D(A⋆). This will be apparent in the following Exam-
ples 1.1 and 1.2. Equivalently, B is supposed to be of the form B = (λ0−A)D,
where D ∈L(U; H) and λ0 is an element in ρ(A). Under these assumptions
we write the state equation as
x′(t) = Ax(t) + (λ0 −A)Du(t),
x(0) = x0,
or in the mild form as
x(t) = etAx0 + (λ0 −A)
 t
0
e(t−s)ADu(s) ds.
(1.1)
Remark that formula (1.1) is meaningful and x ∈L2(0, T ; H); see Chapters 1
to 3 of Part II. This formula will represent the state of our system.
The above representation of the operator B is also discussed in Chapter 2
of Part II. The diﬀerential equation for the state still makes sense for a control
operator B ∈L(U; D(A∗)′), and a representation formula can be obtained for
x(t) (cf. Theorem 1.1 in Chapter 3 of Part II). These results can be sharpened

432
IV-2 Unbounded Control Operators in Parabolic Equations
when A is the generator of an analytic semigroup (cf. Theorems 2.2 and 2.3
in Chapter 3 of Part II) and the natural operator B ∈L(U, D(A∗)′) is D =
[λ0 −A]−1B through formula (1.1).
We shall assume that
(HP)1
⎧
⎪
⎨
⎪
⎩
(i)
A generates an analytic semigroup etA of type ω0
and
λ0 is a real number in ρ(A) such that ω0 < λ0,
(ii)
∃α ∈]0, 1[ such that D ∈L

U, D(Aα)

,
where D(Aα) = D([λ0 −A]α) is the domain of the fractional power [λ0 −A]α
of the operator λ0 −A, as deﬁned in §5 of Chapter 1 in Part II. For more
details on the above techniques, the reader is referred to Chapter 1 (Part II).
Assumption (HP)1 is equivalent to say that
B = [λ0 −A]D = [λ0 −A]1−α[λ0 −A]αD
(1.2)
belongs to L

U, D

(A∗)1−α′
. In the variational case
D

(A∗)1−α
= [D(A∗), H]α =⇒B ∈L(U, [D(A∗), H]′
α)
and the regularity results of Theorem 2.2 (Part II, Chapter 3) would apply:
∀u ∈L2(0, T ; U) and ∀x(0) ∈

[D(A), H]α, [D(A∗), H]′
1−α

1/2
=⇒x ∈W([0, T ; [D(A), H]α, [D(A∗), H]′
1−α]).
(1.3)
Remark 1.1. Assume that (HP)1 is veriﬁed, and let x be deﬁned by (1.1). By
the Closed Graph Theorem, [λ0 −A]αD is a bounded operator. Moreover,
because
(λ0 −A)e(t−s)ADu(s) = [λ0 −A]1−αe(t−s)A[λ0 −A]αDu(s),
there exists a constant kα > 0 such that
|(λ0 −A)e(t−s)ADu(s)| ≤kα(t −s)α−1|u(s)|,
s ∈[0, t].
Thus the state x(·) can also be written as
x(t) = etAx0 +
 t
0
(λ0 −A)e(t−s)ADu(s) ds,
and the following estimate holds:
|x(t)| ≤|etAx0| + kα
 t
0
(t −s)α−1|u(s)| ds.
If α >
1
2, from the H¨older estimate, it follows that x ∈L∞(0, T ; H); this
implies that x ∈C([0, T ]; H) by a standard density argument. In fact setting

1 Introduction
433
xk(t) = etAx0 +
 t
0
(λ0 −A)e(t−s)AkR(k, A)Du(s) ds,
we clearly have xk ∈C([0, T ]; H) and it is easy to check that xk(t) →x(t)
uniformly in t.
If α ≤1/2, then x /∈C([0, T ]; H) in general; however, arguing as before,
we can easily check that [λ0 −A]−βx ∈C([0, T ]; H) for all β > 1/2 −α. In
the sequel, we shall discuss separately the cases α > 1/2 and α ≤1/2; as we
shall see, the second case is much more diﬃcult to deal with.
⊓⊔
Consider the following optimal control problem: To minimize the cost func-
tion
J(u) =
 T
0
{|Cx(t)|2 + |u(t)|2} dt +

P0x(T ), x(T )

,
(1.4)
over all u ∈L2(0, T ; U), subject to the diﬀerential equation constraint (1.1).
We assume, besides (HP)1,
(HP)2
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
(i)
that C ∈L(H; Y ), P0 ∈Σ+(H),
(ii)
and that if α ≤1/2 there exists β ∈[1/2 −α, (1 −α)/2]
such that the following holds
x ∈D([λ0 −A]β) =⇒P0[λ0 −A]βx ∈D([λ0 −A⋆]β),
and
[λ0 −A⋆]βP0[λ0 −A]β is bounded.
If assumption (HP)2 holds, we shall use the notation
Pβ = closure [λ0 −A⋆]βP0[λ0 −A]β,
so that Pβ ∈Σ+(H) and
P0 = [λ0 −A⋆]−βPβ[λ0 −A]−β.
We remark that if α > 1/2, then we do not require additional conditions
on P0, except that P0 ∈Σ+(H), whereas if α ≤1/2, we need the assumption
(HP)2–(ii) to make sense of the term

P0x(T ), x(T )

=

Pβ[λ0 −A]−βx(T ), [λ0 −A]−βx(T )

,
in the cost functional. In fact, if β > 1/2 −α, then [λ0 −A]−βx ∈C([0, T ]; H)
(see Remark 2.1), and the trace value [λ0 −A]−βx(T ) is meaningful.
By Assumption (HP) we mean the set of assumptions (HP)1 and (HP)2.
We shall also say that (A, D, C, P0) fulﬁll (HP). The deﬁnitions of optimal
control, optimal state, and optimal pair are as given in Chapter 1.
We shall study the optimization problem (1.1)–(1.4), by using Dynamic
Programming and by proceeding into the usual two steps.

434
IV-2 Unbounded Control Operators in Parabolic Equations
First step.
We consider the Riccati equation that formally reads as follows:

P ′ = A⋆P + PA −P(λ0 −A)D

(λ0 −A)D
⋆P + C⋆C,
P(0) = P0.
(1.5)
We set
E = [λ0 −A]αD,
V = [λ0 −A⋆]1−αP.
Then E ∈L(U; H) by virtue of assumption (HP)1–(ii) . Thus the meaningless
term P(λ0 −A)D

(λ0 −A)D
⋆P can be written as V ⋆EE⋆V and the Riccati
equation rewritten as follows:

P ′ = A⋆P + PA −V ⋆EE⋆V + C⋆C,
P(0) = P0,
(1.6)
or in the integral form
P(t)x = etA⋆P0etAx +
 t
0
e(t−s)A⋆C⋆Ce(t−s)Ax ds
−
 t
0
e(t−s)A⋆V ⋆(s)EE⋆V (s)e(t−s)Ax ds,
x ∈H.
(1.7)
We look for a solution P(·) of (1.7) such that V (t) ∈L(H) for all t > 0.
Second step.
We shall show that the optimal control u⋆is related to the optimal state y⋆
by the feedback formula
u⋆(t) = −D⋆(λ0 −A⋆)P(T −t)x⋆(t) = −E⋆V (T −t)x⋆(t).
(1.8)
Moreover x⋆(t) is the solution of the closed loop equation
x(t) = etAx0 +
 t
0
[λ0 −A]1−αe(t−s)AEE⋆V (T −s)x(s) ds,
(1.9)
which can be solved by a ﬁxed point argument.
We now give some bibliographical comments. Early papers treating bound-
ary control problems are due to A. V. Balakrishnan [4, 5] and D. C. Wash-
burn [1], who developed further an old idea in H. O. Fattorini [3]. Sim-
pliﬁcations and reﬁnements through domains of fractional powers were intro-
duced in R. Triggiani [3]. Another approach is due to A. Chojnowska-
Michalik [1] and J. Zabczyk [4] (see also Chapter 3 in Part II for an al-
ternate interpretation of their constructions). The Riccati equation (1.7) was
ﬁrst studied by A. V. Balakrishnan [5] when P0 = 0 and by I. Lasiecka

1 Introduction
435
and R. Triggiani [3] when P0 is a positive multiple of the identity. They
were able to build an explicit solution of the Riccati equation by generalizing
the representation formula (2.22) of Chapter 1; we remark, however, that this
method does not give uniqueness. A direct approach to solve (1.7) was ﬁrst
used by F. Flandoli [1] under the assumption that V0 = [λ0 −A⋆]1−αP0 is
bounded; he showed that, in this case, there exists a unique solution P(t) such
that [λ0 −A⋆]1−αP(t) is bounded for any t. Moreover the closed loop (1.9) can
be solved directly and the feedback formula (1.8) holds true. The same direct
approach was used by G. Da Prato and A. Ichikawa [1], which proved that
(1.7) has a unique solution for any P0 ∈Σ+(H) if α > 1/2, whereas if α ≤1/2
they need in addition that the linear operator [λ0 −A⋆]γP0 is bounded for
some γ > 1−2α. For some other results in this direction, see M. C. Delfour
and M. Sorine [1] and M. Sorine [2] using J. L. Lions’ direct method and
A. J. Pritchard and D. Salamon [1].
In this chapter we assume (HP) following the recent paper of F. Flan-
doli [7], and show the existence and uniqueness of the Riccati equation (see
§2.2 below). Then, we solve the control problem in §2.3.
We shall not study the Riccati equation (1.7) when α ≤1/2 and P0 only
belongs to Σ+(H). In this case the cost functional (1.4) is not deﬁned on
the whole L2(0, T ; U), (see Remark 1.1), and the Dynamic Programming ap-
proach, based on the direct solution of (1.7), does not work. Concerning the
existence (but not the uniqueness) of a solution to (1.7) we mention the fol-
lowing results:
(i) If P0 is a positive multiple of the identity, there exists a solution of (1.7)
and one can also solve the corresponding control problem (1.1)–(1.4) (see
I. Lasiecka and R. Triggiani [3], and F. Flandoli [5]).
(ii) There exists P0 ∈Σ+(H) such that (1.7) does not have a solution (see
F. Flandoli [5]).
(iii) A sharp suﬃcient condition, which is “almost” necessary, for the ex-
istence of a solution of (1.7) is given in I. Lasiecka and R. Trig-
giani [14].
It is also possible to study generalizations as in §2.7, §3, and §4 of Chapter 1
in Part I to
(i) nonhomogeneous state equation,
(ii) tracking problem,
(iii) time-dependent state equation and cost function.
The generalizations (i) and (ii) are straightforward, and they will be left to
the reader. For the point (iii), which is much more technical, see the papers by
P. Acquistapace, F. Flandoli, and B. Terreni [1]; P. Acquistapace
and B. Terreni [3]; and P. Acquistapace [1].
We end this section by giving two examples concerning the heat equation.
For a result concerning the strongly damped wave equation, see F. Bucci [1].

436
IV-2 Unbounded Control Operators in Parabolic Equations
Several other examples can be found in the lecture notes by I. Lasiecka and
R. Triggiani [11].
Another approach to boundary control problems using a quadratic cost
function not necessarily coercive can be found in L. Pandolfi [1].
Example 1.1 (Dirichlet boundary condition). Let Ωbe a bounded open set in
RN with a smooth boundary ∂Ω. Consider the optimal control problem: To
minimize
J(u) =
 T
0

Ω
|x(t, ξ)|2 dξ +
 T
0

∂Ω
|u(t, ξ)|2 dσ +

Ω
|Γx(T, ·)(ξ)|2 dξ,
over all controls u ∈L2([0, T ] × ∂Ω) subject to the state constraints
⎧
⎪
⎪
⎨
⎪
⎪
⎩
∂x
∂t (t, ξ) = ∆ξx(t, ξ),
(t, ξ) ∈]0, T [ × Ω,
x(0, ξ) = x0(ξ),
ξ ∈Ω,
x(t, ξ) = u(t, ξ),
(t, ξ) ∈]0, T [ × ∂Ω,
(1.10)
where x0 ∈L2(Ω),
∆ξ =
N

i=1
∂2x
∂ξ2
i
is the Laplace operator and Γ ∈L

L2(∂Ω)

.
We set H = Y = L2(Ω), U = L2(∂Ω), and introduce the Dirichlet realiza-
tion A of the Laplace operator
D(A) = H2(Ω) ∩H1
0(Ω),
Ax = ∆ξx,
∀x ∈D(A).
As is well known (see for instance S. Agmon [2]), A is a strictly negative self-
adjoint operator in L2(Ω), so that assumption (HP)1-(i) holds with λ0 = 0.
Moreover we have
D

(−A)α
=

H2α(Ω),
if α ∈]0, 1/4[,
{u ∈H2α(Ω): u = 0
on ∂Ω},
if α ∈]1/4, 1[.
Let us introduce now the Dirichlet mapping
v 	→Dv = w: L2(∂Ω) →L2(Ω),
where
∆ξw = 0
in Ω,
w(ξ) = v(ξ)
in ∂Ω.
(1.11)
As proved in J. L. Lions and E. Magenes [1], we have D ∈L

L2(∂Ω); H1/2(Ω)

;
therefore D ∈L

L2(∂Ω); D

(−A)α)

for any α ∈]0, 1/4[. Thus Assump-
tions (HP) are veriﬁed provided that α ∈]0, 1/4[, β ∈]1/2 −α, 1/2[, and
Γ ∈L

L2(∂Ω); D

(−A)β)

, as noted in R. Triggiani [1]. We now show,

1 Introduction
437
following A. V. Balakrishnan [4], that problem (1.10) can be set in the
form (1.1).
First we assume that u ∈W 1,2
0, T ; L2(∂Ω)

and introduce a new variable
by setting
y(t, ξ) = x(t, ξ) −

Du(t, ·)

(ξ).
We obtain
⎧
⎪
⎪
⎨
⎪
⎪
⎩
∂y
∂t (t, ξ) = ∆ξy(t, ξ) −∂Du
∂t (t, ξ),
(t, ξ) ∈]0, T [ × Ω,
y(0, ξ) = x0(ξ) −

Du(0, ·)

(ξ),
ξ ∈Ω,
y(t, ξ) = 0,
(t, ξ) ∈]0, T [ × ∂Ω.
Then, we set y(t) = y(t, ·), u(t) = u(t, ·), and write the above problem as
y(t) = etA
x0 −Du(0)

−
 t
0
e(t−s)ADu′(s) ds.
By performing an integration by parts, it is not diﬃcult to check that x fulﬁlls
(1.1). Finally, the hypothesis u ∈W 1,2
[0, T ]; L2(∂Ω)

can be easily removed
by regularization.
⊓⊔
Example 1.2 (Neumann boundary condition). Let Ωbe a bounded open set in
RN with a smooth boundary ∂Ω. Consider the optimal control problem: To
minimize
J(u) =
 T
0

Ω
|x(t, ξ)|2 dξ +
 T
0

∂Ω
|u(t, ξ)|2 dσ +

Ω
|Γx(T, ·)(ξ)|2 dξ,
over all controls u ∈L2([0, T ] × ∂Ω), subject to the state constraints
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
∂x
∂t (t, ξ) = ∆ξx(t, ξ),
(t, ξ) ∈]0, T [ × Ω,
x(0, ξ) = x0(ξ),
ξ ∈Ω,
∂x
∂ν (t, ξ) = u(t, ξ),
(t, ξ) ∈]0, T [ × ∂Ω,
(1.12)
where x0 ∈L2(Ω), Γ ∈L

L2(Ω)

, and ν represents the outward normal to
∂Ω.
We set H = Y = L2(Ω), U = L2(∂Ω), and introduce the Neumann real-
ization A of the Laplace operator
⎧
⎨
⎩
D(A) =

x ∈H2(Ω): ∂x
∂ν = 0

,
Ax = ∆ξx,
∀x ∈D(A).
As is well known (see for instance S. Agmon [2]), A is a nonpositive self-
adjoint operator in L2(Ω), so that Assumption (HP)1–(i) holds with any
λ0 > 0; we choose λ0 = 1. We have

438
IV-2 Unbounded Control Operators in Parabolic Equations
D([λ0 −A]α) =
⎧
⎨
⎩
H2α(Ω),
if α ∈]0, 3/4[,

x ∈H2α(Ω): ∂x
∂ν = 0

,
if α ∈]3/4, 1[.
Introduce the Neumann mapping
v 	→Nv = w: L2(∂Ω) →L2(Ω),
where
∆ξw −w = 0
in Ω,
∂w
∂ν (ξ) = v(ξ)
in ∂Ω.
As proved in J. L. Lions and E. Magenes [1], we have
N ∈L(L2
∂Ω); H3/2(Ω)

;
thus N ∈L

L2(∂Ω); D([λ0 −A]α)

for any α ∈]0, 3/4[; and Assumptions
(HP) are veriﬁed. Moreover, by proceeding as in Example 1.1, we see that
problem (1.12) is equivalent to problem (1.1) with D = N and λ0 = 1.
⊓⊔
Remark 1.2. One obtains similar results for an elliptic operator with general
boundary conditions (see I. Lasiecka [1]).
⊓⊔
2 Riccati equation
2.1 Notation
We use here the notation introduced in §2.1 of Chapter 1, and we assume
that Assumptions (HP) are veriﬁed (that is both (HP)1 and (HP)2) hold. If
P ∈Cs

[a, b]; Σ+(H)

, we set
VP (t) = [λ0 −A⋆]1−αP(t),
t ∈[a, b].
When no confusion may arise, we will drop index P, writing VP = V .
We are going to study the Riccati equation

P ′ = A⋆P + PA −V ⋆EE⋆V + C⋆C,
P(0) = P0,
(2.1)
where
E = [λ0 −A]αD.
We also consider the integral form
P(t)x = etA⋆P0etAx +
 t
0
e(t−s)A⋆C⋆Ce(t−s)Ax ds
−
 t
0
e(t−s)A⋆V ⋆(s)EE⋆V (s)e(t−s)Ax ds,
(2.2)

2 Riccati equation
439
where x ∈H.
It is useful to introduce the approximating problem for k > λ0:

P ′
k = A⋆Pk + PkA −V ⋆
k EkE⋆
kVk + C⋆C,
Pk(0) = P0,
(2.3)
where
Ek = kR(k, A)E,
Vk = VPk = [λ0 −A⋆]1−αPk
and R(k, A) = (k −A)−1. Problem (2.3) can also be written in mild form as
follows:
Pk(t)x = etA⋆P0etAx +
 t
0
e(t−s)A⋆C⋆Ce(t−s)Ax ds
−
 t
0
e(t−s)A⋆V ⋆
k (s)EkE⋆
kVk(s)e(t−s)Ax ds,
(2.4)
where x ∈H.
Remark 2.1. The operators (A, Bk, C, P0),
Bk = [λ0 −A⋆]1−αkR(k, A)[λ0 −A⋆]αD,
fulﬁll assumptions (H) of Chapter 1. So, by Theorem 2.1 of Chapter 1, equa-
tion (2.4) has a unique mild solution Pk ∈Cs

[0, ∞[; Σ+(H)

.
⊓⊔
Finally, in order to make simpler some estimates, we introduce a constant
L such that
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
(i)
L ≥1,
∥(λ0 −A)−1∥≤L, ∥(λ0 −A)−γ∥≤L,
∀γ ∈]0, 1[, ∥EkE⋆
k∥≤L,
∀k ∈N.
(ii)
∥et(A−λ0)∥≤L, t∥(A −λ0)et(A−λ0)∥
≤L, tγ∥(λ0 −A)−γet(A−λ0)∥≤L,
∀γ ∈]0, 1[,
∀t > 0.
2.2 Riccati equation for α > 1/2
Assume that Assumptions (HP) are veriﬁed with α >
1
2, and set, for the
sake of simplicity, λ0 = 0. We shall look for a solution of the Riccati equa-
tion (2.1) in the following functional space. For any interval [a, b] we denote
by Cs,α

[a, b]; Σ(H)

the set of all P ∈Cs

[a, b]; Σ(H)

such that
⎧
⎪
⎪
⎨
⎪
⎪
⎩
(i)
P(t)x ∈D

(−A⋆)1−α
,
∀x ∈H, ∀t ∈]a, b],
(ii)
(−A⋆)1−αP ∈C

]a, b]; L(H)

,
(iii)
lim
t→a(t −a)1−α(−A⋆)1−αP(t)x = 0,
∀x ∈H.
(2.5)

440
IV-2 Unbounded Control Operators in Parabolic Equations
Deﬁne
∥P∥1 = sup
t∈]a,b]
∥(t −a)1−α(−A⋆)1−αP(t)∥.
Cs,α

[a, b]; Σ(H)

endowed with the norm
∥P∥α = ∥P∥+ ∥P∥1
is a Banach space. We set
Cs,α

[a, b]; Σ+(H)

=

P ∈Cs,α

[a, b]; Σ(H)

: P(t) ≥0, ∀t ∈]a, b]
 
and denote by Cs,α

[0, ∞]; Σ(H)

the set of all P ∈Cs

[0, ∞]; Σ(H)

such
that P ∈Cs,α

[0, T ]; Σ(H)

for all T > 0.
Deﬁnition 2.1. A mild solution of problem (2.1) in the interval [0, T] is an
operator valued function P ∈Cs,α

[0, T ]; Σ(H)

that veriﬁes the integral
equation
P(t)x = etA⋆P0etAx +
 t
0
e(t−s)A⋆C⋆Ce(t−s)Ax ds
−
 t
0
e(t−s)A⋆V ⋆(s)EE⋆V (s)e(t−s)Ax ds,
(2.6)
where x ∈H. A weak solution of problem (2.1) in [0, T ] is an operator valued
function P ∈Cs,α

[0, T ]; Σ(H)

such that, for any x, y ∈D(A),

P(·)x, y

is
diﬀerentiable in [0, T ] and veriﬁes
⎧
⎪
⎪
⎨
⎪
⎪
⎩
d
dt

P(t)x, y

=

P(t)x, Ay

+

P(t)Ax, y

+ (Cx, Cy)
−

E⋆V (t)x, E⋆V (t)y

,
P(0) = P0.
(2.7)
⊓⊔
Proposition 2.1. Let P ∈Cs,α

[0, T ]; Σ(H)

. Then P is a mild solution of
problem (2.1) if and only if P is a weak solution.
Proof. It is completely similar to the proof of Proposition 2.3 of Chapter 1,
and so, it will be omitted.
⊓⊔
We need, as in Chapter 1, a generalization of the Contraction Map-
ping Principle. Let T > 0, and let {γk} be a sequence of mappings from
Cs,α

[0, T ]; Σ(H)

into itself, such that
∥γk(P) −γk(Q)∥α ≤a∥P −Q∥α
for some a ∈[0, 1[ and all P, Q ∈Cs,α

[0, T ]; Σ(H)

. Moreover assume that
there exists a mapping γ from Cs,α

[0, T ]; Σ(H)

into itself such that ∀P ∈
Cs,α

[0, T ]; Σ(H)

, ∀m ∈N, ∀x ∈H

2 Riccati equation
441
⎧
⎪
⎪
⎨
⎪
⎪
⎩
lim
k→∞γm
k (P)x = γm(P)x,
in C([0, T ]; H),
lim
k→∞t1−α(−A⋆)1−αγm
k (P)x = t1−α(−A⋆)1−αγm(P)x,
in C([0, T ]; H),
(2.8)
Then, by the Contraction Mapping Principle, there exist unique Pk and P in
Cs,α

[0, T ]; Σ(H)

such that
γk(Pk) = Pk,
γ(P) = P.
The following result can be proved as Lemma 2.1 of Chapter 1.
Lemma 2.1. Under the previous assumptions on the sequence of mappings
{γk}
⎧
⎪
⎪
⎨
⎪
⎪
⎩
lim
k→∞Pk(·)x = P(·)x
in C([0, T ]; H),
lim
k→∞t1−α(−A⋆)1−αPk(·)x = t1−α(−A⋆)1−αP(·)x,
in C([0, T ]; H),
∀x ∈H, T > 0.
(2.9)
The main result of this section is as follows.
Theorem 2.1. Assume that Assumptions (HP) are veriﬁed with α > 1/2.
Then, problem (2.1) has a unique mild solution P ∈Cs,α

[0, T ]; Σ(H)

. More-
over, the solution Pk of problem (2.4) also belongs to Cs,α

[0, T ]; Σ(H)

for
all k ∈N and (2.9) holds for all x ∈H and T > 0.
Proof. We write problem (2.1) as
P = γ(P) = F + H −ϕ(P),
where
F(t) = etA⋆P0etA,
H(t) =
 t
0
e(t−s)A⋆C⋆Ce(t−s)Ads,

ϕ(P)

(t) =
 t
0
e(t−s)A⋆V ⋆(s)EE⋆V (s)e(t−s)A ds.
Analogously, we write problem (2.4) as
Pk = γk(Pk) = F + H −ϕk(Pk),
where

ϕk(P)

(t)x =
 t
0
e(t−s)A⋆V ⋆
k (s)EkE⋆
kVk(s)e(t−s)Ax ds,
x ∈H
and Vk(s) = VPk(s).
In the sequel of the proof T > 0, α ∈]0, 1[ and L ≥1 are ﬁxed. We proceed
in several steps.

442
IV-2 Unbounded Control Operators in Parabolic Equations
Step 1.
F, H ∈Cs,α

[0, T ]; Σ(H)

and
∥F∥α ≤2L2∥P0∥,
∥H∥α ≤

1 + 1
α

L2∥C∥2T.
(2.10)
In fact, we clearly have ∥F∥≤L2∥P0∥. Moreover
∥t1−α(−A⋆)1−αF(t)∥= ∥t1−α(−A⋆)1−αetA⋆P0etA∥≤L2∥P0∥
so that ∥F∥1 ≤L2∥P0∥and the ﬁrst inequality in (2.10) follows. Concerning
the second, it suﬃces to remark that ∥H(t)∥≤L2∥C∥2T and, for all x ∈H,
|t1−α(−A⋆)1−αH(t)| =
t1−α(−A⋆)1−α
 t
0
e(t−s)A⋆C⋆Ce(t−s)A ds

≤L2∥C∥2t1−α
 t
0
(t −s)α−1 ds |x| ≤1
αL2∥C∥2T.
Step 2.
ϕ and ϕk map Cs,α([0, T ]; Σ(H)) into itself. Moreover there exists a constant
C1,T > 0 such that, for all k ∈N and P ∈Cs,α([0, T ]; Σ(H))
∥ϕ(P)∥α + ∥ϕk(P)∥α ≤C1,T ∥P∥2
α,
(2.11)
where
C1,T = 2

1
2α −1 + β(2α −1, α)
	
T 2α−1L3,
and β represents the Euler function,
β(x, y) =
 1
0
tx−1(1 −t)y−1 dt.
We only estimate ∥ϕ(P)∥α. The estimate for ∥ϕk(P)∥α is similar. Let P ∈
Cs,α

[0, T ]; Σ(H)

and x ∈H; then ϕ(P) ∈Cs

[0, T ]; Σ(H)

and we have

ϕ(P)

(t)
 ≤L3
 t
0
s2α−2 ds ∥P∥2
1 ≤
1
2α −1L3∥P∥2
1T 2α−1,
which implies
∥ϕ(P)∥≤
1
2α −1L3∥P∥2
1T 2α−1.
(2.12)
By arguing as in Lemma 3.1 of Chapter 1, it is not diﬃcult to show that
ϕ(P) ∈Cs,α

[0, T ]; Σ(H)

. Moreover we have

2 Riccati equation
443
|t1−α(−A⋆)1−α
ϕ(P)

(t)| ≤L3t1−α
 t
0
(t −s)α−1s2α−2 ds ∥P∥2
1
≤L3∥P∥2
1T 2α−1
 1
0
(1 −σ)α−1σ2α−2 dσ |x|
= L3∥P∥2
1T 2α−1β(2α −1, α),
which yields
∥ϕ(P)∥1 ≤T 2α−1L3β(2α −1, α)∥P∥2
1.
(2.13)
Now, (2.11) follows from (2.12) and (2.13).
Step 3.
There exists a constant C2,T > 0 such that, for all P, Q ∈Cs,α

[0, T ]; Σ(H)

,
we have
∥ϕ(P) −ϕ(Q)∥α+∥ϕk(P)−ϕk(Q)∥α ≤C2,T (∥P∥α+∥Q∥α)∥P −Q∥α,
(2.14)
where
C2,T = 2

1
2α −1 + β(2α −1, α)
	
T 2α−1L3.
In fact, let P, Q ∈Cs,α

[0, T ]; Σ(H)

, V = VP , Z = VQ, and x ∈H; we have
t1−α
ϕ(P)

(t)x −t1−α
ϕ(Q)

(t)

≤
t1−α
 t
0
e(t−s)A⋆

V (s) −Z(s)
⋆EE⋆V (s)

e(t−s)A ds

+
t1−α
 t
0
e(t−s)A⋆
Z⋆(s)EE⋆
V (s) −Z(s)

e(t−s)A ds
 ,
and the conclusion follows by arguing as in the second step.
We denote by B(r, t) the ball
B(r, t) =

P ∈Cs,α

[0, t]; Σ(H)

: ∥P∥α ≤r
 
.
Step 4.
For all p > 0, there exist τ = τ(p) > 0 and r = r(p) > 0, such that
∥P0∥≤p =⇒γ

B(r, τ)

∪γk

B(r, τ)

⊂B(r, τ)
(2.15)
and
∥γ(P) −γ(Q)∥α ≤1
2∥P −Q∥α,
∥γk(P) −γk(Q)∥α ≤1
2∥P −Q∥α,
(2.16)
for all P, Q ∈γ

B(r, τ)

. This follows easily from the inequalities (2.10),
(2.11), and (2.14).

444
IV-2 Unbounded Control Operators in Parabolic Equations
Step 5.
Existence and positivity. Choose p > ∥P0∥, and let τ = τ(p). By (2.15), (2.16),
and the Contraction Mapping Principle, it follows that there exist unique P
and Pk in Cs,α

[0, T ]; Σ(H)

such that
γ(P) = P,
γk(Pk) = Pk.
So problems (2.1) and (2.3) have unique solutions P and Pk, respectively, in
[0, τ]. Moreover, by Lemma 2.1 it follows that (2.9) holds for all x ∈H and
T > 0. Since, by Theorem 2.1 of Chapter 1, Pk(t) ≥0 for all t ∈[0, τ], we
ﬁnally ﬁnd
P(t) ≥0,
for all t ∈[0, τ].
Step 6.
Global existence and uniqueness. We ﬁrst remark that we have shown exis-
tence in the interval [0, τ], which only depends on the norm of P0. Thus, in
order to prove global existence we only have to estimate the norm of P(t),
t ∈[0, τ[. Let P ∈Cs,α

[0, τ]; Σ(H)

be the solution of (2.1) in [0, τ] . As
P(t) ≥0, we have

P(t)x, x

≤(P0etAx, etAx) +
 t
0
|Ce(t−s)Ax|2 ds
≤L2{∥P0∥+ T ∥C∥2}|x|2,
which implies that
∥P(t)∥≤∥L∥2{∥P0∥+ T ∥C∥2},
for all t ∈[0, τ].
Set now p1 = L2{∥P0∥+T ∥C∥2} and τ1 = τ(p1). Let P be the solution to the
Riccati equation (2.1) in [0, τ1]. Proceeding as above, we can solve the Riccati
equation
Q(t) = e(t−τ1+ε)A⋆P(τ1 −ε)e(t−τ1+ε)A +
 t
τ1−ε
e(t−s)A⋆C⋆Ce(t−s)A ds
−
 t
τ1−ε
e(t−s)A⋆V ⋆
Q(s)E⋆EVQ(s)e(t−s)A ds,
in the interval [τ1 −ε, 2τ1 −ε] (here ε is a small positive number to be chosen
later), and we ﬁnd a solution Q ∈Cs,α

[τ1 −ε, 2τ1 −ε]; Σ(H)

such that
∥Q(t)∥≤L2{∥P0∥+ T ∥C∥2},
∀t ∈[τ1 −ε, 2τ1 −ε].
Now, it is easy to check that setting

2 Riccati equation
445
¯P(t) =

P(t),
if t ∈[0, τ1],
Q(t),
if t ∈[τ1, 2τ1 −ε];
then ¯P is the unique solution of (2.1) in [0, 2τ1 −ε]; moreover ¯P(t) ≥0 in
[0, 2τ1 −ε] and
∥¯P(t)∥≤L2{∥P0∥+ T ∥C∥2},
t ∈[0, 2τ1 −ε].
Thus we can repeat this argument successively in the intervals [2τ1 −2ε, 3τ1 −
2ε], [3τ1 −3ε, 4τ1 −3ε], and so on. So we get the conclusions in N steps,
provided N and ε are chosen such that Nτ1 > T and
ε ≤Nτ1 −T
N −1 .
The proof is complete.
⊓⊔
We now prove the continuous dependence with respect to P0 and C of the
solutions of (2.1). Consider a sequence of Riccati equations:
⎧
⎪
⎨
⎪
⎩
dP h
dt
= A⋆P h + P hA −(V h)⋆EE⋆V h + (Ch)⋆Ch,
P h(0) = P h
0 ,
(2.17)
where V h = V P h, h ∈N. Assume that
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
(i)
For any h ∈N, (A, D, Ch, P h
0 ) fulﬁll (HP) with α > 1
2,
(ii)
lim
h→∞(Ch)⋆Chx = C⋆Cx,
for all x ∈H,
(iii)
lim
h→∞P h
0 x = P0x
for all x ∈H,
(2.18)
and prove the following result.
Proposition 2.2. Assume that Assumptions (HP) and (2.18) are veriﬁed
with α > 1
2. Let P and P h be the mild solutions to (2.1) and (2.17), respec-
tively. Then, for any x ∈H and any T > 0, we have
⎧
⎨
⎩
lim
h→∞P h(·)x = P(·)x,
in C([0, T ]; H),
lim
h→∞t1−αV h(·)x = t1−αV (·)x
in C([0, T ]; H).
(2.19)
Proof. Fix T > 0. By the Uniform Boundedness Theorem, there exist positive
numbers q and c such that
∥P h
0 ∥≤q,
∥(Ch)⋆Ch∥≤c,
∀h ∈N.
Set p = L2(q + Tc), τ = τ(p); then, arguing as in the proof of Theorem 2.2
of Chapter 1, we ﬁrst show that (2.19) holds in [0, τ] and then we prove that
this argument can be iterated in the interval [τ, 2τ] and so on.
⊓⊔

446
IV-2 Unbounded Control Operators in Parabolic Equations
We ﬁnally show a monotonicity property of the solutions of the Riccati
equation
Proposition 2.3. Assume that (A, D, Ci, Pi,0) fulﬁll (HP) for i = 1, 2, with
α > 1/2 and, in addition, that
P1,0 ≤P2,0,
C⋆
1C1 ≤C⋆
2C2.
(2.20)
Let Pi, i = 1, 2 be the mild solution of the Riccati equations
⎧
⎨
⎩
d
dtPi = A⋆Pi + PiA −V ⋆
i EE⋆Vi + (Ci)⋆Ci,
Pi(0) = Pi,0,
(2.21)
where Vi = VPi. Then we have
P1(t) ≤P2(t),
t ≥0.
Proof. For any k ∈N, i = 1, 2, let Pi,k be the mild solution to the Riccati
equation
⎧
⎨
⎩
d
dtPi,k = A⋆Pi,k + Pi,kA −V ⋆
i,kEE⋆Vi,k + (Ci)⋆Ci,
Pi,k(0) = Pi,0,
where Vi,k = (−A⋆
k)1−αPi,k. Then, by Proposition 2.2 of Chapter 1,
P1,k(t) ≤P2,k(t),
t ≥0, k ∈N.
The conclusion now follows from Theorem 2.1.
⊓⊔
2.3 Solution of the Riccati equation for α ≤1/2
We assume here that Assumptions (HP) are veriﬁed with α ≤1/2 and some
β ∈]1/2 −α, (1 −α)/2[. We set again λ0 = 0 and denote by Pβ ∈Σ+(H) the
closure of (−A⋆)βP0(−A)β, so that
P0 = (−A⋆)−βPβ(−A)−β.
One can see very easily that the proof of Theorem 2.1 cannot be repeated,
because 2−2α ≥1 and some integrals would be divergent. Thus we introduce,
following F. Flandoli [7], a more complicated functional space and some
additional notations. If [a, b] is any interval, we set
µ[a,b](t) = (t −a)1−α−β,
t ∈[a, b],
and

2 Riccati equation
447
ν[a,b](t, s) = (s −a)1−α−β(t −s)β
(t −a)β
,
a ≤s ≤t ≤b.
If P ∈Cs

[a, b]; Σ+(H)

, we set
VP (t) = (−A⋆)1−αP(t),
t ∈[a, b],
WP (t, s) = (−A⋆)1−αP(s)(−A)βe(t−s)A,
a ≤s ≤t ≤b.
When no confusion may arise, we will drop indexes [a, b] and P, in the formulas
above.
For any interval [a, b] we denote by Cs,α,β

[a, b]; Σ(H)

the set of all P ∈
Cs

[a, b]; Σ(H)

such that (2.5)–(i)–(ii) hold and moreover
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
(i)
P(t)x ∈D

(−A⋆)1−α
,
∀x ∈H, ∀t ∈]a, b],
(ii)
VP ∈C

]a, b]; L(H)

,
(iii)
lim
t→a µ[a,b](t)VP (t)x = 0,
∀x ∈H,
(iv)
lim
(t,s)→(a,a) ν[a,b](t, s)WP (t, s)x = 0,
∀x ∈H.
(2.22)
Deﬁne
∥P∥2 = sup
t∈]a,b]
∥µ[a,b](t)VP (t)∥
and
∥P∥3 =
sup
a≤s<t≤b
∥ν[a,b](t, s)WP (t, s)∥.
Cs,α,β

[a, b]; Σ(H)

, endowed with the norm
∥P∥α,β = ∥P∥+ ∥P∥2 + ∥P∥3,
is a Banach space. We set
Cs,α,β

[a, b]; Σ+(H)

=

P ∈Cs,α,β

[a, b]; Σ(H)

: P(t) ≥0, ∀t ∈]a, b]
 
,
and let Cs,α,β

[0, ∞[; Σ(H)

be the set of all P ∈Cs

[0, ∞[; Σ(H)

such that
P ∈Cs,α,β

[0, T ]; Σ(H)

for all T > 0.
Deﬁnition 2.2. A mild solution of problem (2.1) in the interval [0, T] is an
operator valued function P ∈Cs,α,β

[0, T ]; Σ(H)

, which veriﬁes the integral
equation (2.6). A weak solution of problem (2.1) in [0, T ] is an operator valued
function P ∈Cs,α,β

[0, T ]; Σ(H)

such that, for any x, y ∈D(A),

P(·)x, y

is diﬀerentiable in [0, T ] and veriﬁes (2.7).
⊓⊔
Proposition 2.4. Let P ∈Cs,α,β

[0, T ]; Σ(H)

. Then P is a mild solution of
problem (2.1) if and only if P is a weak solution.
Proof. Completely similar to the proof of Proposition 2.3 of Chapter 1.
⊓⊔

448
IV-2 Unbounded Control Operators in Parabolic Equations
We shall need, as in §2, a generalization of the Contraction Mapping Princi-
ple. LetT > 0, andlet{γk} beasequenceofmappingsfrom Cs,α,β

[0, T ]; Σ(H)

into itself, such that
∥γk(P) −γk(Q)∥α ≤a∥P −Q∥α,
for some a ∈[0, 1[ and all P, Q ∈Cs,α,β

[0, T ]; Σ(H)

. Assume moreover that
there exists a mapping γ from Cs,α,β

[0, T ]; Σ(H)

into itself such that for all
m ∈N and all x ∈H:
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
lim
k→∞γm
k (P)x = γm(P)x
in C([0, T ]; H),
lim
k→∞µ(·)Vγm
k (P )x = µ(·)Vγm(P )x
in C([0, T ]; H),
lim
k→∞ν(·, ·)Wγm
k (P )x = ν(·, ·)Wγm(P )x
in C(∆T ; H),
where ∆T = {(t, s): 0 ≤s ≤t ≤T }, ∀P ∈Cs,α,β

[0, T ]; Σ(H)

. Then,
by the Contraction Mapping Principle, there exist unique Pk and P in
Cs,α,β

[0, T ]; Σ(H)

such that
γk(Pk) = Pk,
γ(P) = P.
The following result can be proved as Lemma 2.1 of Chapter 1.
Lemma 2.2. Under the previous assumptions on the sequence of mappings
{γk}, for all x ∈H
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
lim
k→∞Pk(·)x = P(·)x
in C([0, T ]; H),
lim
k→∞µ(·)VPk(·)x = µ(·)VP (·)x
in C([0, T ]; H),
lim
k→∞ν(·, ·)WPk(·, ·)x = ν(·, ·)WP (·, ·)x
in C(∆T ; H).
(2.23)
Prior to state the main result of this section, we need another lemma.
Lemma 2.3. If P ∈Cs,α,β

[0, T ]; Σ(H)

, we have
∥(−A⋆)1−αP(s)(−A)1−αe(t−s)A∥≤21−αL
tβ
s1−α−β(t −s)(1−α) ∥P∥3.
Proof. Let t ∈]0, T ], s ∈[0, t[ . We have
(−A⋆)1−αP(s)(−A)1−αe(t−s)A
= (−A⋆)1−αP(s)(−A)βe(t+s/2−s)A(−A)1−α−βe(t+s/2−s)A.
It follows that
∥(−A⋆)1−αP(s)(−A)1−αe(t−s)A∥
≤L(t + s)βsα+β−1(t −s)−β
t −s
2
α+β−1
∥P∥3,
which implies the conclusion.
⊓⊔

2 Riccati equation
449
The main result of this section as follows.
Theorem 2.2. Assume that Assumptions (HP) are veriﬁed for α ≤1/2 and
some β ∈]1/2 −α, 1 −α/2[. Then the Riccati equation (2.1) has a unique
mild solution P ∈Cs,α,β

[0, ∞[; Σ+(H)

. Moreover, the solution Pk of prob-
lem (2.4) also belongs to Cs,α,β

[0, ∞[; Σ(H)

for all k ∈N and (2.23) holds
∀x ∈H.
Proof. We write problems (2.1) and (2.4) as
P = γ(P) = F + H −ϕ(P),
Pk = γk(Pk) = F + H −ϕk(Pk),
k ∈N,
where F, H, ϕ, and ϕk are deﬁned as in the proof of Theorem 2.1. We ﬁx
T > 0 and proceed in several steps.
Step 1.
F ∈Cs,α,β

[0, T ]; Σ(H)

and
∥F∥α,β ≤L2∥P0∥+ 2L3∥Pβ∥.
(2.24)
We have indeed
∥t1−α−β(−A⋆)1−αF(t)∥=∥t1−α−β(−A⋆)1−α−βetA⋆Pβ(−A)−βetA∥≤L3∥Pβ∥,
which implies
∥F∥2 ≤L3∥Pβ∥.
(2.25)
Moreover, if t ∈]0, T ] and s ∈]0, t], we have
∥ν(t, s)(−A⋆)1−αF(s)(−A)βe(t−s)A∥= ∥ν(t, s)(−A⋆)1−α−βesA⋆PβetA∥
≤L2[(t −s)/t]β∥Pβ∥≤L3∥Pβ∥
because (t −s)/t ≤1. It follows that
∥F∥3 ≤L3∥Pβ∥.
(2.26)
Now (2.24) follows from (2.25) and (2.26) and the estimate ∥F∥≤L2∥P0∥.
Step 2.
H ∈Cs,α,β

[0, T ]; Σ(H)

and
∥H∥α,β ≤L2∥C∥2T + 1
αL2∥C∥2T 1−β
+L2∥C∥2T 1−2β+2ε
 +∞
0
ρε−1(1 + ρ)β dρ,
(2.27)
where ε = 1
2 min{α, β}.

450
IV-2 Unbounded Control Operators in Parabolic Equations
Let x ∈H; then
|t1−α−β(−A⋆)1−αH(t)x| =
t1−α−β(−A⋆)1−α
 t
0
e(t−s)A⋆C⋆Ce(t−s)Ax ds

≤L2∥C∥2t1−α−β
 t
0
(t −s)α−1 ds |x| ≤1
αL2∥C∥2T 1−β|x|.
It follows that
∥H∥2 ≤1
αL2∥C∥2T 1−β.
(2.28)
Moreover
|ν(t, s)(−A⋆)1−αH(s)(−A)βe(t−s)Ax|
=
ν(t, s)(−A⋆)1−α
 s
0
e(s−σ)A⋆C⋆C(−A)βe(t−σ)A dσ x

≤L2∥C∥2ν(t, s)
 s
0
(s −σ)α−1(t −σ)−β dσ |x|
= L2∥C∥2ν(t, s)
 s/(t−s)
0
(ρ)α−1(1 + ρ)−β dρ |x|.
As ρα−ε ≤[s/(t −s)]α−ε, we have
|ν(t, s)(−A⋆)1−αH(s)(−A)βe(t−s)Ax|
≤L2∥C∥2 (t −s)εs1−β−ε
tβ
 s/(t−s)
0
dρ
ρ1+ε(1 + ρ)β |x|
≤L2∥C∥2T 1−2β+2ε
 +∞
0
dρ
ρ1+ε(1 + ρ)β |x|,
which implies
∥H∥3 ≤L2∥C∥2T 1−2β
 +∞
0
dρ
ρ1+ε(1 + ρ)β .
(2.29)
As ∥H∥≤L2∥C∥2T , the conclusion follows from (2.28) and (2.29).
Step 3.
ϕ and ϕk map Cs,α,β

[0, T ]; Σ(H)

into itself. Moreover there exists two con-
stants C1 > 0 and C2 > 0 such that for all P ∈Cs,α,β

[0, T ]; Σ(H)

∥ϕ(P)∥α,β + ∥ϕ(Pk)∥α,β
≤T 2α+2β−1C1∥P∥2
α,β + C2(∥P∥2∥P∥3 + ∥P∥2
3).
(2.30)
Let P ∈Cs,α,β

[0, T ]; Σ(H)

; we have

2 Riccati equation
451
∥ϕ(P)∥≤L3∥P∥2
2
 t
0
(t −s)2α+2β−2 ds
=
1
2α + 2β −1L3∥P∥2
2T 2α+2β−1,
(2.31)
because 2α + 2β −1 > 0. Moreover for all x ∈H we have
|t1−α−β(ϕ(P))(t)x| =
t1−α−β
 t
0
e(t−s)A⋆V ⋆
P (s)EE⋆VP (s)e(t−s)Ax ds

=
t1−α−β
 t
0
[(−A⋆)1−αP(s)(−A)1−αe(t−s)A]⋆
EE⋆[(−A⋆)1−αP(s)]e(t−s)Ax ds
.
By using Lemma 2.3, we obtain
|t1−α−β
ϕ(P)

(t)x|≤21−αL3t1−α
 t
0
s2α+2β−2(t −s)α−1 ds∥P∥2∥P∥3|x|
=21−αL3t2α+2β−1
 1
0
σ2α+2β−2(1−σ)α−1dσ∥P∥2∥P∥3|x|.
Hence
∥ϕ(P)∥2 ≤21−αL3T 2α+2β−1
 1
0
σ2α+2β−2(1 −σ)α−1 dσ ∥P∥2∥P∥3.
(2.32)
Finally, using once again Lemma 2.3, we ﬁnd
ν(t, s)(−A⋆)1−α
ϕ(P)

(s)(−A)βe(t−s)Ax

=
ν(t, s)
 s
0
[(−A⋆)1−αP(σ)(−A)1−αe(s−σ)A]⋆
EE⋆(−A⋆)1−αP(σ)(−A)βe(t−σ)A]x dσ

≤21−αL3s1−α(t −s)β
 s
0
σ2α+2β−2(t −σ)−β(s −σ)1−α dσ ∥P∥2
3|x|.
As (t −s)β ≤(t −σ)β, we obtain
ν(t, s)(−A⋆)1−α
γ(P)

(s)(−A)βe(t−s)Ax

≤21−αL3s1−α
 s
0
σ2α+2β−2(s −σ)1−α dσ ∥P∥2
3|x|
≤21−αL3s2α+2β−1
 1
0
ρ2α+2β−2(1 −ρ)α−1 dρ ∥P∥2
3|x|
so that
∥γ(P)∥3 ≤21−αL3s2α+2β−1
 1
0
ρ2α+2β−2(1 −ρ)1−αdρ ∥P∥2
3
(2.33)
and (2.30) follows from (2.31), (2.32), and (2.33).

452
IV-2 Unbounded Control Operators in Parabolic Equations
Step 4.
There exists a constant C3 > 0 such that for all P, Q ∈Cs,α,β

[0, T ]; Σ(H)

we have
∥γ(P) −γ(Q)∥α,β + ∥γ(Pk) −γ(Qk)∥α,β
≤C3T 2α+2β−1(∥P∥α,β + ∥Q∥α,β)∥P −Q∥α,β.
(2.34)
The proof is similar to the previous one in Step 3.
We denote now by B(r, t) the ball
B(r, t) =

P ∈Cs,α,β

[0, t]; Σ(H)

: ∥P∥α,β ≤r
 
.
Step 5.
For all p > 0, there exist τ = τ(p) > 0 and r = r(p) > 0, such that
∥Pβ∥≤p =⇒γ

B(r, τ)

∪γk

B(r, τ)

⊂B(r, τ),
(2.35)
and for all P, Q ∈B(r, τ)
⎧
⎨
⎩
∥γ(P) −γ(Q)∥α,β ≤1
2∥P −Q∥α,β,
∥γk(P) −γk(Q)∥α,β ≤1
2∥P −Q∥α,β.
(2.36)
This follows easily from the inequalities (2.24), (2.27), (2.34), and (2.36).
Step 6. Local existence, convergence, and positivity.
Choose p > ∥Pβ∥and τ = τ(p). Then, by (2.35), (2.36), and the Contractions
Mapping Principle, there exist unique P and Pk in Cs,α,β

[0, τ]; Σ(H)

such
that
γ(P) = P,
γk(Pk) = Pk,
(2.37)
and problems (2.1) and (2.4) have unique solutions P and Pk, respectively, in
[0, τ].
Moreover, by Lemma 2.2, (2.23) follows. Finally, by Theorem 2.1 of Chap-
ter 1, we have Pk(t) ≥0 for all t ∈[0, τ]; this yields
P(t) ≥0,
∀t ∈[0, τ].
Step 7. Conclusion.
As the interval [0, τ] depends only on the norm of Pβ, in order to prove global
existence, we have to ﬁnd an estimate for the norm of (P(t))β, t ∈[0, τ[. Let
P ∈Cs,α,β

[0, τ]; Σ(H)

be the solution of (2.1) in [0, τ]. Let x ∈D

(−A)β
,
as P(t) ≥0, we have

2 Riccati equation
453

P(t)(−A)βx, (−A)βx

≤

P0(−A)βetAx, (−A)βetAx

+
 t
0
|C(−A)βesAx|2 ds
≤L2

∥Pβ∥+ 1
β T β∥C∥2

|x|2,
which implies
∥(−A⋆)βP(t)(−A)β∥≤L2

∥Pβ∥+ 1
β T β∥C∥2

|x|2,
(2.38)
for all t ∈[0, τ]. Set now
p = L2

∥Pβ∥+ 1
β T β∥C∥2

|x|2,
τ1 = τ(p);
and let P be the solution to the Riccati equation (2.1) in t ∈[0, τ1]. Proceeding
as in Step 6 of the proof of Theorem 2.1, we prove that there exists a unique
solution of (2.1) in [0, 2τ1 −ε] and so on. The proof is complete.
⊓⊔
We now prove the continuous dependence with respect to Pβ and C of the
solutions of (2.1). Consider the sequence (2.17) of the Riccati equations. We
assume that
⎧
⎪
⎪
⎨
⎪
⎪
⎩
(i)
for any h ∈N, (A, D, Ch, P h
0 ) fulﬁll (HP) with α ≤1/2,
(ii)
lim
n→∞(Ch)⋆Chx = C⋆Cx,
for all x ∈H,
(iii)
lim
n→∞P h
β x = Pβx
for all x ∈H,
(2.39)
and we prove the result
Proposition 2.5. Assume that Assumptions (HP) and (2.39) are veriﬁed for
α ≤1/2. Let P and P h be the mild solutions to (2.1) and (2.17), respectively.
Then, for any x ∈H and any T > 0, we have the following properties: For
all x ∈H.
⎧
⎪
⎪
⎨
⎪
⎪
⎩
lim
h→∞P h(·)x = P(·)x
in C([0, T ]; H),
lim
h→∞µ(·)VP h(·)x = µ(·)VP (·)x
in C([0, T ]; H),
lim
h→∞ν(·, ·)WP k(·, ·)x = ν(·, ·)WP (·, ·)x
in C(∆T ; H).
(2.40)
Proof. Fix T > 0. By the Uniform Boundedness Theorem, there exist positive
numbers qβ and c such that
∀h ∈N,
∥P h
β ∥≤qβ,
∥(Ch)⋆Ch∥≤c.
(2.41)
Set
p = L2

qβ + 1
β T βc

,
τ = τ(p).
Then, arguing as in the proof of Theorem 2.2, we ﬁrst show that Pk →P in
Cs

[0, τ]; Σ(H)

and then we show that this argument can be iterated in the
interval [τ, 2τ] and so on.
⊓⊔

454
IV-2 Unbounded Control Operators in Parabolic Equations
Finally, the following result can be proved as Proposition 2.3.
Proposition 2.6. Assume that (A, D, Ci, Pi,0) fulﬁll Assumptions (HP) for
i = 1, 2, with α ≤1/2, and, in addition, that
P1,0 ≤P2,0,
C⋆
1C1 ≤C⋆
2C2,
Let Pi, i = 1, 2, be the mild solution of the Riccati equations (2.21). Then we
have
P1(t) ≤P2(t),
t ≥0.
3 Dynamic programming
In this section, we are going to solve minimization problem (1.4) associated
with the state equation (1.1). In the sequel, this will be referred to as prob-
lem (1.1)–(1.4). We assume that Assumptions (HP) are veriﬁed with λ0 = 0
(recall that under Assumption (HP)1 properties (1.2) and (1.3) are veriﬁed)
and denote by P and Pk the solutions of the Riccati equations (2.1) and (2.4),
respectively, given by Theorems 2.1 and 2.2. We set also V = VP , Vk = VPk.
We shall extend the arguments of §6 of Chapter 1 based on the fundamental
identity (3.1) and the solution of the closed loop equation (3.2) below.
Proposition 3.1. Let u ∈L2(0, T ; U) and let x be the solution to state equa-
tion (1.1). Then the following identity holds:

P(t)x0, x0

+
 t
0
|u(s) + E⋆V (t −s)x(s)|2 ds
=
 t
0
{|Cx(s)|2 + |u(s)|2} ds +

P0x(t), x(t)

.
(3.1)
Proof. Let u ∈L2(0, T ; U) and let x be the solution to (1.1). Let {uk} be a
sequence in W 1,2(0, T ; U) such that uk →u in L2(0, T ; U), and let xk be the
solution to (1.1) corresponding to uk. We have, as can be easily checked,
xk(t) = etAx0 + Duk(t) −
 t
0
e(t−s)ADu′
k(s) ds.
By Proposition 3.8 in Part II, Chapter 1, it follows that xk(t) is diﬀerentiable
for any t ∈]0, T ] and
x′
k(t) = AetAx0 −A
 t
0
e(t−s)ADu′
k(s) ds.
Now we compute the derivative
d
ds

Pk(T −s)xk(s), xk(s)

,
and integrating from 0 and T and letting k tend to inﬁnity, we get (3.1).
⊓⊔

3 Dynamic programming
455
Let us consider now the closed loop equation
x(t) = etAx0 −
 t
0
(−A)1−αe(t−s)AEE⋆V (T −s)x(s) ds.
(3.2)
Remark that the right-hand side of (3.2) is meaningful for any t ∈[0, T [
because
∥(−A)1−αe(t−s)AEE⋆V (T −s)∥
≤

L2∥P∥α[(T −s)(t −s)]α−1
if α > 1/2,
L2∥P∥α,β(T −s)α+β−1(t −s)α−1
if α ≤1/2.
(3.3)
Proposition 3.2. Assume that Assumptions (HP) hold. Then the following
statements hold true:
(i) If α > 1/2, there exists a unique solution of (3.2), x ∈C([0, T ]; H).
(ii) If α ≤1/2, there exists a unique solution of (3.2), x ∈C([0, T [; H) and
a constant C > 0 such that
|x(t)| ≤C(T −t)−β,
t ∈[0, T [.
(3.4)
Proof. For any y ∈C([0, T ]; H), we set
λ(y)(t) =
 t
0
(−A)1−αe(t−s)AEE⋆V (T −s)x(s) ds,
t ∈[0, T ].
We ﬁrst prove (i). Assume α > 1/2; then by (3.3) using the H¨older estimate,
we have
|(λ(y))(t)| ≤L2∥P∥α
 t
0
(t −s)α−1(T −s)α−1 ds ∥y∥C([0,t];H)
≤L2∥P∥α
 t
0
(t −s)2α−2ds
 t
0
(T −s)2α−2 ds
1/2
∥y∥C([0,t];H)
≤
1
2α −1L2∥P∥αt2α−1∥y∥C([0,t];H),
so that λ ∈L

C([0, t]; H)

for any t ∈]0, T ] and
∥λ∥L(C([0,t];H)) ≤
1
2α −1L2∥P∥αt2α−1.
Thus, if t is small, λ is a contraction and (3.2) has a unique solution in
C([0, t]; H). Now this argument can be repeated in [t, 2t] and so on, giving
the conclusion. We now prove (ii). Fix T1 ∈]0, T [, and let α ≤1/2 and
β ∈]1/2 −α, 1 −α/2[. By (3.3) we have

456
IV-2 Unbounded Control Operators in Parabolic Equations
|(λ(y))(t)| ≤L2∥P∥α,β
 t
0
(t −s)α−1(T −s)α+β−1 ds ∥y∥C([0,t];H)
≤L2∥P∥α,β(T −t)α+β−1
 t
0
sα−1 ds ∥y∥C([0,t];H)
≤1
αL2∥P∥α,β(T −T1)α+β−1tα∥y∥C([0,t];H),
t ∈[0, T ],
so that λ ∈L

C([0, t]; H)

for any t ∈]0, T1] and
∥λ∥L(C([0,t];H)) ≤1
αL2∥P∥α,β(T −T1)α+β−1tα.
Thus, if t is small, λ is a contraction and (3.2) has a unique solution in
C([0, t]; H), and proceeding by iteration, we prove the ﬁrst part of (ii).
It remains to show (3.4). Following F. Flandoli [7], we set
z(t) = V (T −t)x(t),
t ∈[0, T [.
(3.5)
Then z veriﬁes the equation
z(t) = V (T −t)etAx0 −
 t
0
V (T −t)(−A)1−αe(t−s)AEE⋆z(s) ds.
Taking into account Lemma 2.3, it follows that
(T −t)1−α−β|z(t)| ≤L|x0|∥P∥α,β+21−αL2∥P∥α,β
 t
0
(T −s)β(t−s)α−1|z(s)| ds.
Finally set w(t) = (T −t)1−α−βz(t); then we have
|w(t)| ≤L|x0| ∥P∥α,β
+ 21−αL2∥P∥α,β
 t
0
(T −s)α+2β−1(t −s)α−1|w(s)| ds
≤L|x0| ∥P∥α,β
+ 21−αL2∥P∥α,β
 t
0
(t −s)2α+2β−2|w(s)| ds.
(3.6)
As 2 −2α −2β < 1, by the Gronwall’s lemma, we have |w(t)| ≤K, for some
K > 0 and all t ∈[0, T [. It follows that
|z(t)| ≤K(T −t)α+β−1,
t ∈[0, T [.
(3.7)
Now, from (3.2), we have
|x(t)| ≤L|x0| + L2K
 t
0
(t −s)α−1(T −s)α+β−1 ds
= L|x0| + L2K(T −t)2α+β−1
 t/T−t
0
σα−1(1 + σ)α+β−1 dσ.

3 Dynamic programming
457
As σ2α+2β−1 ≤(t/T −t)2α+2β−1, it follows that
|x(t)| ≤L|x0| + L2KT 2α+2β−1(T −t)−β
 ∞
0
σα+2β(1 + σ)α+β−1dσ,
and the proof is complete.
⊓⊔
We now prove the main result of this section.
Theorem 3.1. Assume that Assumptions (HP) are veriﬁed and let x0 ∈H.
Then there exists a unique optimal pair (u⋆, x⋆). Moreover the following state-
ments hold:
(i) x⋆is the mild solution to the closed loop equation (3.2) and x⋆∈
C([0, T ]; H) if α ∈] 1
2, 1[, whereas x⋆∈C(]0, T ]; H) ∩L2(0, T ; H) if
α ∈]0, 1
2[.
(ii) u⋆∈C([0, T ]; U) is given by the feedback formula:
u⋆(t) = −E⋆V (T −t)x⋆(t).
(3.8)
(iii) the optimal cost J(u⋆) is given by
J(u⋆) =

P(T )x0, x0

.
(3.9)
Proof. Let u ∈L2(0, T ; U) and let y be the solution to state equation (1.1).
Then, by (3.1), we have

P(T )x0, x0

+
 T
0
|u(s) + E⋆EV (T −s)x(s)|2 ds = J(u).
(3.10)
It follows that

P(T )x0, x0

≤J(u)
for all u ∈L2(0, T ; U). Now, let x⋆be the solution of the closed loop equa-
tion (3.2). By Proposition 3.2, x⋆belongs to C([0, T ]; H) if α <
1
2 and to
L2(0, T ; H) if α > 1
2 (due to the estimate (3.4)). Now, let u⋆be deﬁned by
(3.8) and set u = u⋆, x = x⋆in (3.10). We ﬁnd

P(T )x0, x0

= J(u⋆), so that
u⋆is optimal. Finally, the uniqueness of the optimal control can be proved as
in the proof of Theorem 6.1 in Chapter 1.
⊓⊔

3
Unbounded Control Operators: Hyperbolic
Equations With Control on the Boundary
1 Introduction
As before, we shall denote by H, U, and Y the Hilbert spaces of states,
controls, and observations, respectively, and consider a dynamical system,
whose state x(·) is the solution of the following equation:

x′(t) = Ax(t) + Bu(t),
t ∈[0, T ],
x(0) = x0 ∈H,
where u ∈L2(0, T ; U) and A: D(A) ⊂H →H generates a strongly continu-
ous group on H. We identify the elements of H′ with those of H so that the
linear operator (A∗)∗: H →D(A∗)′ is a linear extension of the linear opera-
tor A: D(A) →H. As in the previous chapter, the linear operator B is not
supposed to be bounded from U into H, but it belongs to L

U; D(A⋆)′
, or
equivalently, B is of the form B = (λ0−A)E, where E ∈L(U; H) and λ0 is an
element in ρ(A). More precisely, following I. Lasiecka and R. Triggiani [1,
2, 11], we shall assume that
(HH)1
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
(i)
A generates a strongly continuous group etA in H of type ω0
and
λ0 is a real number in ρ(A) such that ω0 < λ0,
(ii)
E ∈L(U; H),
(iii)
∃K > 0 such that
 T
0
|E⋆A⋆esA⋆x|2 ds≤K2|x|2, ∀x∈D(A⋆).
If assumptions (HH)1 hold, then we can give a precise meaning to the state
equation. We have in fact the following result due to I. Lasiecka and
R. Triggiani [1, 2, 11].
Proposition 1.1. Assume that (HH)1 is veriﬁed, and for each control func-
tion u ∈L2(0, T ; U), deﬁne

460
IV-3 Unbounded Control Operators in Hyperbolic Equations
z(t) =
 t
0
e(t−s)AEu(s) ds,
t ∈[0, T ].
(1.1)
Then z(t) ∈D(A) for all t ∈[0, T ] and Az ∈C([0, T ]; H). Moreover the
following inequality holds:
|Az(t)| ≤K∥u∥L2(0,T ;U),
t ∈[0, T ].
(1.2)
Proof. We divide the proof into two steps.
Step 1.
z(t) ∈D(A) for all t ∈[0, T ].
If x ∈D(A⋆) we have

z(t), A⋆x

=
 t
0

u(s), E⋆A⋆e(t−s)A⋆x

ds.
It follows that
|(z(t), A⋆x)|2 ≤∥u∥2
L2(0,T ;U)
 t
0
|E⋆A⋆e(t−s)A⋆x|2 ds
≤K2∥u∥2
L2(0,T ;U)|x|2.
If we identify elements of H′ with those of H, then this inequality implies
that the linear form x 	→

z(t), A⋆x

on H, is continuous, and hence that
z(t) ∈D(A) and that (1.2) holds.
Step 2.
Az ∈C([0, T ]; H).
Let {un} be a sequence in W 1,2(0, T ; U) such that un converges to u in
L2(0, T ; U). Set
zn =
 t
0
e(t−s)AEun(s) ds.
It follows that zn ∈C

[0, T ]; D(A)

(cf. Part II, Chapter 1, Proposition 3.3).
Now setting wn(t) = Azn(t), wn ∈C([0, T ]; H) and we have
|w(t) −wn(t)|2 = |Az(t) −Azn(t)|2 ≤K∥u −un∥2
L2(0,T ;U) →0
as n →∞uniformly in [0, T ]. The proof is complete.
⊓⊔
In the sequel we shall write the solution of the state equation as follows:
x(t) = etAx0 + G(u)(t)
= etAx0 + (λ0 −A)
 t
0
e(t−s)AEu(s) ds.
(1.3)

1 Introduction
461
By Proposition 1.1, G ∈L

L2(0, T ; U); C([0, T ]; H)

.
We now consider the following optimal control problem: To minimize
J(u) =
 T
0
{|Cx(t)|2 + |u(t)|2} dt +

P0x(T ), x(T )

,
(1.4)
over all u ∈L2(0, T ; U), subject to the state equation constraint (1.3). In
addition to the assumptions (HH)1, we assume that (HH)2 C ∈L(H; Y ),
P0 ∈Σ+(H).
In the sequel (HH) will mean that both assumptions (HH)1 and (HH)2 are
veriﬁed. We shall also say that (A, E, C, P0) fulﬁll (HH). From now on we
assume for simplicity that λ0 = 0.
The deﬁnitions of optimal control, optimal state, and optimal pair are as
in the previous chapters.
We again study the optimization problem (1.3)–(1.4) by using Dynamic
Programming. However, in the current case, new technical diﬃculties arise.
Consider in fact the Riccati equation that we formally write as follows:

P ′ = A⋆P + PA −P(E⋆A⋆)⋆E⋆A⋆P + C⋆C,
P(0) = P0.
(1.5)
If the data C and P0 are regular or more precisely if the linear operators
√P0A and CA admit bounded extensions, √P0A and CA, then problem (1.5)
can be easily solved. In fact by setting R = A⋆PA it reduces to

R′ = A⋆R + RA −REE⋆R + (CA)⋆CA,
R(0) = (√P0A)⋆√P0A,
which has a unique solution by Theorem 2.1 of Chapter 1.
However these conditions on C and P0 are not natural, so we prefer to
proceed in the following diﬀerent way:
(i) We consider the approximating problem

P ′
n = A⋆Pn + PnA −Pn(E⋆A⋆
n)⋆E⋆A⋆
nPn + C⋆C,
P(0) = P0,
(1.6)
where An = nAR(n, A) are the Yosida approximations of A, and prove
the convergence in Cs

[0, T ]; Σ(H)

of the sequence {Pn} to an element
P.
(ii) For any n ∈N we consider the control problem: To minimize
Jn(u) =
 T
0
{|Cxn(t)|2 + |u(t)|2} dt +

P0xn(T ), xn(T )

,
(1.7)
over all u ∈L2(0, T ; U), subject to the constraint

462
IV-3 Unbounded Control Operators in Hyperbolic Equations

x′
n(t) = Axn(t) −AnEu(t),
t ∈[0, T ],
xn(0) = x0.
(1.8)
Denote by (u⋆
n, x⋆
n) the corresponding optimal pair given by Theorem 6.1 of
Chapter 1. Then we prove that (u⋆
n, x⋆
n) converges to the optimal pair of
problem (1.4) with state equation (1.3).
Notice that we avoid the diﬃculty of giving a general deﬁnition of a solu-
tion of the Riccati equation (1.5); for us the solution will namely be the limit
of the sequence {Pn}. The proof of the existence of this limit is given in §2,
whereas in §3 we shall see how to use this result to study the initial optimal
control problem.
For an existence and uniqueness result, under suitable regularity assump-
tions on C and P0 (weaker than the ones discussed before), see G. Da Prato,
I. Lasiecka, and R. Triggiani [1]. Finally we notice that in the previ-
ously quoted papers by I. Lasiecka and R. Triggiani, explicit formulas
for the solutions to the Riccati equation and the optimal pair (even under
more general assumptions) are constructed by generalizing the representation
formula (2.22) of Chapter 1.
In §5 we shall consider, following V. Barbu and G. Da Prato [1], the
case when etA is a general semigroup, and a condition similar to (HH) holds.
In this case we are able to deﬁne a solution of the Riccati equation by using
the dual Riccati equation introduced in §7.3 of Chapter 1.
2 Riccati equation
We assume here that (HH) is veriﬁed and consider the Riccati equation (1.6),
whose mild solution is denoted by Pn. For the sake of simplicity, we assume
that P0 ∈Lr(H), that is, that P0 is invertible with a bounded inverse (cf.
Chapter 1, §5); at the end of this section, we shall explain how to handle the
general case.
Following F. Flandoli [2], we consider the Riccati equation

Q′ = −AQ −QA⋆−QC⋆CQ + (E⋆A⋆)⋆E⋆A⋆,
Q(0) = P −1
0
.
(2.1)
and

Q′
n = −AQn −QnA⋆−QnC⋆CQn + (E⋆A⋆
n)⋆E⋆A⋆
n,
Qn(0) = P −1
0
,
(2.2)
where An = nAR(n, A) are the Yosida approximations of A. As the linear
operator E⋆A⋆fulﬁlls the assumptions of Theorem 4.1 of Chapter 1, problem
(2.1) has a unique mild solution Q that belongs to Cs

[0, T ]; Σ(H)

and we
have
lim
n→∞Qn = Q
in Cs

[0, T ]; Σ(H)

(2.3)

3 Dynamic programming
463
and
Qn(t) = P −1
n (t),
∀t ≥0.
(2.4)
Theorem 2.1. Assume that (HH) is veriﬁed and that P0 ∈Lr(H). Let Pn
be the solution to the Riccati equation (1.6). Then we have
lim
n→∞Pn = P
in Cs

[0, T ]; Σ(H)

,
(2.5)
where P(t) = Q−1(t), t ∈[0, T ] and Q is the mild solution of (2.1).
We shall call P the mild solution of (1.5).
Proof. It is suﬃcient to prove that there exists a constant C1 > 0 such that
∥Pn(t)∥≤C1,
∀t ∈[0, T ], ∀n ∈N.
(2.6)
In fact, if (2.6) holds, then we have
∥Q−1
n (t)∥≤C1,
∀t ∈[0, T ], ∀n ∈N.
Thus Q(t) ∈Lr(H), ∀t ∈[0, T ]. Moreover from the identity
Q−1(t) −Pn(t) = Pn(t)[Qn(t) −Q(t)]Q−1(t),
one can prove the existence of the limit (2.5). Finally to prove (2.6), it is
suﬃcient to observe that
Pn(t) ≤etA⋆P0etA +
 t
0
e(t−s)A⋆C⋆Ce(t−s)A ds.
The proof is complete.
⊓⊔
Remark 2.1. Assume that (HH) is veriﬁed and that P0, ¯P0 ∈Lr(H) with
P0 ≤¯P0. Let P and ¯P be the corresponding mild solutions of (1.5). Then we
have P(t) ≤¯P(t), ∀t ≥0.
⊓⊔
Remark 2.2. Assume that P0 does not have a bounded inverse, and denote by
Pε the mild solution of (1.5) with P0 replaced by P0+εI. Then by the previous
remark Pε is decreasing and so the limit of Pε exists in Cs

[0, T ]; Σ(H)

as ε
goes to zero.
⊓⊔
3 Dynamic programming
Here we assume that (HH) is veriﬁed. We denote by P and Pn the mild
solutions of (1.5) and (1.6), and by (u⋆, x⋆) (resp. (u⋆
n, x⋆
n)) the optimal pair
for the control problem (1.3)–(1.4) (resp. (1.7)–(1.8)).
We ﬁrst consider the case when P0 ∈Lr(H).

464
IV-3 Unbounded Control Operators in Hyperbolic Equations
Theorem 3.1. Assume that (HH) is veriﬁed and that P0 ∈Lr(H) ∩Σ+(H).
Then we have
(i) limn→∞u⋆
n = u⋆in L2(0, T ; U),
(ii) limn→∞x⋆
n = x⋆in C([0, T ]; H).
Proof. Let u ∈L2(0, T ; U) and let x be given by (1.3) and xn by (1.8).
We remark that, as xn(t) = etAx0 + n(nI −A)−1G(u), we have xn →x
in C([0, T ]; H). Now by formula (6.2) in Chapter 1, it follows that
(Pn(T )x0, x0) +
 T
0
|u(s) + E⋆A⋆
nPn(T −s)xn(s)|2 ds
=
 T
0
[|Cxn(s)|2 + |u(s)|2] ds +

P0xn(T ), xn(T )

.
(3.1)
Setting u = u⋆
n in (3.1), we ﬁnd
(Pn(T )x0, x0) =
 T
0
[|Cx⋆
n(s)|2 + |u⋆
n(s)|2] ds +

P0x⋆
n(T ), x⋆
n(T )

.
(3.2)
Thus we have

P(T )x0, x0

≤J(u),
∀u ∈L2(0, T ; U).
(3.3)
Moreover from identities (3.1) and (3.2) the sequence {u∗
n} is bounded in
L2(0, T ; U) and so there exists a subsequence {u∗
nk} that weakly converges
to an element ;u of L2(0, T ; U). As
u∗
nk(·) = S(·)x0 −nkR(nk, A)G(u∗
nk)
and
G(u∗
nk) ⇀G(;u)
in L2(0, T ; H),
we have
x∗
nk ⇀;x
in L2(0, T ; H),
where
;x = S(·)x0 −G(;u),
and the symbol ⇀denotes the weak convergence.
By letting n go to inﬁnity in (3.2), we have

P(T )x0, x0

≥JT (;u),
which, along with (3.3), yields ;u = u∗, ;x = x∗and

P(T )x0, x0

= JT (u∗).
Thus the sequence {u∗
n} weakly converges to u∗and it only remains to
prove that u∗
n →u∗. By (3.2) we have

3 Dynamic programming
465
lim
n→∞

∥u∗
n∥2
L2(0,T ;U) + ∥x∗
n∥2
L2(0,T ;H) + |
%
P0x∗
n(T )|2
≤J(u∗) = ∥u∗∥2
L2(0,T ;U) + ∥x∗∥2
L2(0,T ;H) + |
%
P0x∗(T )|2.
As
u∗
n ⇀u∗
in L2(0, T ; U),
x∗
n ⇀x∗
in L2(0, T ; H),
%
P0x∗
n(T ) ⇀
%
P0x∗(T )
in H,
as n →∞, it follows that the above convergences are strong. The proof is
complete.
⊓⊔
We now consider the case where P0 ∈Σ+(H) is not invertible. Deﬁne for
any ε > 0, the cost
Jε(u) =
 T
0
{|Cx(t)|2 + |u(t)|2} dt +

P0x(T ), x(T )

+ ε|x(T )|2,
and let (u⋆
ε, x⋆
ε) be an optimal pair for the corresponding control problem. Let
moreover Pε be the mild solution of the Riccati equation (1.5) with P0 replaced
by P0+εI. As we know (see Remark 3.1), Pε is convergent in Cs

[0, T ]; Σ(H)

to an element P ∈Cs

[0, T ]; Σ(H)

, which has been deﬁned as the mild
solution of (1.5).
Theorem 3.2. Assume that (HH) is veriﬁed and that P0 ∈Σ+(H). Then
there exists a unique optimal pair (u⋆, x⋆) for the control problem (1.4) subject
to the state equation (1.3). Moreover
(i) limε→0 u⋆
ε = u⋆in L2(0, T ; U),
(ii) limε→0 x⋆
ε = x⋆in C([0, T ]; H).
Proof. We have

Pε(T )x0, x0

≤Jε(u),
∀u ∈L2(0, T ; U),
(3.4)

Pε(T )x0, x0

=
 T
0
{|Cx⋆
ε(t)|2 + |u⋆
ε(t)|2}dt
+

P0x(T ), x(T )

+ ε|x⋆
ε(T )|2.
(3.5)
We notice that the sequence {u⋆
ε} is bounded in L2(0, T ; U); we have in fact
 T
0
|u⋆
ε(s)|2 ds ≤Jε(u⋆
ε) ≤Jε(0),
where
Jε(0) =
 T
0
|CesAx0|2 ds + (P0CeT Ax0, CeT Ax0) + ε|eT Ax0|2.
It follows that there exists a sequence {εk} ↓0, and elements ;u ∈L2(0, T ; U),
;x ∈C([0, T ]; H) such that

466
IV-3 Unbounded Control Operators in Hyperbolic Equations
(i) uεk ⇀;u in L2(0, T ; U),
(ii) xεk ⇀;x in L2(0, T ; H),
(iii) xεk(T ) ⇀;x(T ) in H.
Moreover
;x(t) = etAx0 −A
 t
0
e(t−s)AE;u(s) ds,
t ∈[0, T ].
Now from (3.4) it follows that

Pε(T )x0, x0

≤Jε(;u),
and by (3.5)

Pε(T )x0, x0

≥Jε(;u).
The conclusion follows arguing as in the proof of Theorem 3.1.
⊓⊔
Remark 3.1. We shall denote by Pmin the mild solution of the Riccati equation
corresponding to P0 = 0. By Remark 2.1 it follows that Pmin is the minimal
nonnegative solution of the Riccati equation.
⊓⊔
4 Examples of controlled hyperbolic systems
Let A: D(A) ⊂H →H be a linear operator, inﬁnitesimal generator of a
strongly continuous group etA and let E : U →H be a linear bounded op-
erator. In this section we shall discuss several examples such that the key
condition (HH)–(iii) is fulﬁlled. We start with a very simple example.
Example 4.1 (ﬁrst order problem).
Consider a system
⎧
⎪
⎪
⎨
⎪
⎪
⎩
∂x
∂t (t, ξ) = ∂x
∂ξ (t, ξ),
t ∈[0, T ], ξ ∈[0, 2π],
x(0, ξ) = x0(ξ),
ξ ∈[0, 2π],
x(t, 2π) = x(t, 0) + u(t),
t ∈[0, T ].
We choose here H = L2
#(R), the set of all 2π-periodic functions that belong
to L2(0, 2π; R), and U = R. We consider the linear operator A in H

D(A) = W 1,2
# (R) = {x ∈L2
#(R): x′ ∈L2
#(R), x(0) = x(2π)},
Ax = x′,
∀x ∈D(A).
As it is well known, A is the inﬁnitesimal generator of the strongly continuous
group of translations
etAx(ξ) = x(t + ξ),
t ∈R, ξ ∈R.

4 Examples of controlled hyperbolic systems
467
We proceed here as in the Example 1.1 of Chapter 2. First we assume that u
is diﬀerentiable and introduce a new variable y by setting
y(t, ξ) = x(t, ξ) −
eξ
e2π −1u(t),
so that state equation becomes
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
∂y
∂t (t, ξ) = ∂y
∂ξ (t, ξ) +
eξ
e2π −1

u(t) −u′(t)

,
t ∈[0, T ], ξ ∈[0, 2π],
y(0, ξ) = x0(ξ) −
eξ
e2π −1u(0) = y0,
ξ ∈[0, 2π],
y(t, 2π) = y(t, 0),
t ∈[0, T ].
Now we can write this equation in abstract form, namely
y(t, ·) = etAy0 +
 t
0
e(t−s)Az0

u(s) −u′(s)

ds,
where z0 is the element of L2
#(R) deﬁned by
z0(ξ) =
eξ
e2π −1,
ξ ∈[0, 2π].
By integration by parts we eventually ﬁnd that the state equation can be
written in the required form
x′(t) = Ax(t) + (1 −A)Eu(t),
x(0) = x0,
where the linear operator E ∈L(U; H) is deﬁned as
Eα = αz0,
∀α ∈U = R.
Observe that the adjoint operator E⋆∈L(H; U) is deﬁned as
E⋆x = (x, z0),
∀x ∈H.
We now check condition (HH)–(iii). Let t ∈[0, 2π] and x ∈W 1,2
# (R). If
s ∈[0, 2π] we have
E⋆A⋆esA⋆x = (A⋆x, esAz0)
=
1
e2π −1
 2π−s
0
x′(ξ)es+ξ dξ +
 2π
2π−s
x′(ξ)es+ξ−2π dξ

= x(2π −s) −(x, esAz0).
It follows that
 t
0
|E⋆A⋆esA⋆x|2 ds ≤2
 t
0
|x(2π −s)|2 ds + 2t
 t
0
|(x, esAz0)|2 ds
≤2(1 + t2|z0|2)|x|2.
Thus condition (HH)–(iii) is fulﬁlled.
⊓⊔

468
IV-3 Unbounded Control Operators in Hyperbolic Equations
Example 4.2 (Abstract wave equation).
Let Z, U be Hilbert spaces, Λ: D(Λ) ⊂Z →Z a linear self-adjoint strictly
positive operator, D a linear operator in L(U; Z), and λ0 ≥0. Here we are
dealing with the system

y′′(t) + Λy(t) + (λ2
0 + Λ)Du(t) = 0,
t ≥0,
y(0) = y0,
y′(0) = y1.
(4.1)
In order to write problem (4.1) in the form (1.3), we have to introduce the
space D(Λ−1/2) deﬁned as the completion of Z with respect to the norm
∥x∥D(Λ−1/2) = ∥Λ−1/2x∥. Then we choose as space of states and observations
H = Z ⊕D(Λ−1/2), endowed with the inner product:

x0
x1
	
,

¯x0
¯x1
	
= (x0, ¯x0)Z + (Λ−1/2x1, Λ−1/2¯x1)Z,
and deﬁne the linear operator A on H
⎧
⎪
⎨
⎪
⎩
AX =

0 1
−Λ 0
 
x0
x1

,
∀X ∈D(A),
D(A) = D(Λ1/2) ⊕Z.
A is the inﬁnitesimal generator of a strongly continuous group of contractions,
given by the formula
etA =

cos(Λ1/2t)
Λ−1/2 sin(Λ1/2t)
−Λ1/2 sin(Λ1/2t)
cos(Λ1/2t)
	
,
t ∈R.
Setting now
X(t) =
y(t, ·)
y′(t, ·)
	
,
X0 =
y0
y′
0
	
,
we can write (4.1) as

X′(t) = AX(t) + (λ0 −A)Eu(t),
t ≥0,
X(0) = X0,
(4.2)
where E : U →H is deﬁned by
Eη = −
 Dη
λ0Dη
	
,
η ∈U.
We remark that the adjoint of E, E⋆: H →U is given by
E⋆
x0
x1
	
= −D⋆(x0 −λ0Λ−1)x1.
It follows that

4 Examples of controlled hyperbolic systems
469
E⋆A⋆etA⋆x0
x1
	
= −D⋆Λ1/2 sin(Λ1/2t)x0 + D⋆cos(Λ1/2t)x1
+ λ0D⋆Λ−1 cos(Λ1/2t)x0 + λ0D⋆Λ1/2 sin(Λ1/2t)x1.
Now it follows easily that condition (HH)–(iii) is fulﬁlled if and only if there
exists K1 > 0 such that
 t
0
|D⋆Λ1/2 sin(Λ1/2s)z|2
U ds ≤K1(t)|z|2
Z,
∀z ∈D(Λ1/2),
 t
0
|D⋆Λ1/2 cos(Λ1/2s)z|2
U ds ≤K1(t)|z|2
Z,
∀z ∈D(Λ1/2).
(4.3)
⊓⊔
Example 4.3 (Wave equation).
Let Ωbe a bounded set in Rn with a regular boundary ∂Ω. Consider the
problem
⎧
⎪
⎨
⎪
⎩
ytt(t, ξ) = ∆ξy(t, ξ),
t ≥0, ξ ∈Ω,
y(t, ξ) = u(t, ξ),
t ≥0, ξ ∈∂Ω,
y(0, ξ) = y0(ξ),
yt(0, ξ) = y1(ξ), ξ ∈Ω.
(4.4)
We set Z = L2(Ω) and U = L2(∂Ω) and denote by −Λ the Laplace operator
with Dirichlet boundary conditions in Ω. Then, setting y(t) = y(t, ·) and
u(t) = u(t, ·) and denoting by D the Dirichlet mapping deﬁned in Example 1.1
of Chapter 2, we see that problem (4.4) is equivalent to problem (4.1) with
λ0 = 0.
We now prove (HH)–(iii); for this it is enough to show the ﬁrst inequality
(4.3), because the second one can be proven similarly. Thus we ﬁx z ∈D(Λ)
and set
w(t) = D⋆Λ1/2 sin(Λ1/2t)z,
v(t) = Λ−1/2 sin(Λ1/2t)z;
we remark that
w(t) = ∂v
∂ν (t),
where ν is the the outward normal to ∂Ω(because D⋆Λ coincides with the
linear operator ∂/∂ν). Moreover v(t, ·) = v(t) is the classical solution to the
following problem:
⎧
⎪
⎨
⎪
⎩
vtt(t, ξ) = ∆ξv(t, ξ),
t ≥0, ξ ∈Ω,
v(t, ξ) = 0,
t ≥0, ξ ∈∂Ω,
v(0, ξ) = 0,
vt(0, ξ) = x0(ξ), ξ ∈Ω.
(4.5)
Fix T > 0, to prove the ﬁrst inequality in (4.3), it suﬃces to prove that there
exists a constant C1 > 0 such that

470
IV-3 Unbounded Control Operators in Hyperbolic Equations
 T
0
(T −t)

∂v
∂ν (t, ·)

2
L2(∂Ω)
dt ≤C1|x0|2
L2(Ω).
(4.6)
For this purpose, we follow the proof in I. Lasiecka, J. L. Lions, and
R. Triggiani [1]. We consider a regular vector ﬁeld h in Ωthat extends the
outward normal ν; then we multiply the ﬁrst equation in (4.5) by (T −t)h·∇ξv
and set
I =

Q
(T −t)vtth · ∇ξv dt dξ,
J =

Q
(T −t)(∆ξv)h · ∇ξv dt dξ,
where Q = [0, T ] × Ωand “ · ” denotes the scalar product in Rn.
Step 1. Estimate of I.
We have
I =

Ω
[(T −t)(h · ∇ξv)vt]

t=T
t=0
dξ +

Q
(h · ∇ξv)vt dt dξ
−

Q
(T −t)(h · ∇ξvt)vt dt dξ = I1 + I2 + I3.
Clearly, I1 = 0; moreover, by the usual energy estimates, there exists C2 > 0
such that
I2 ≤C2|x0|2
L2(Ω).
(4.7)
Concerning I3 we have
I3 = −1
2

Q
(T −t) divξ(hv2
t ) dt dξ + 1
2

Q
(T −t)v2
t divξ h dt dξ
= 1
2

[0,T ]×∂Ω
(T −t)v2
t h · ν dt dσ + 1
2

Q
(T −t)v2
t divξ h dt dξ.
The ﬁrst term is equal to 0 because v = 0 on ∂Ω; moreover the second one
can be estimated as I2. Thus, there exists a constant C3 > 0 such that
|I| ≤C2|x0|2
L2(Ω).
(4.8)
Step 2. Estimate of J.
Using Gauss–Green formulas for the term J, we obtain the boundary term in
(4.6) minus the term J1 given by the expression
J1 =
n

i,j=1

Q
(T −t)∂ξiv∂ξi(hj∂ξjv) dt dξ
=
n

i,j=1

Q
(T −t)∂ξiv∂ξihj∂ξjv dt dξ +
n

i,j=1

Q
(T −t)hj∂ξiξjv ∂ξiv dt dξ,

5 Some result for general semigroups
471
where ∂ξi denotes the partial derivative with respect to ξi and ∂ij = ∂i(∂j).
Then
J1 =
n

i,j=1

Q
(T −t)∂ξihj∂ξjv∂ξiv dt dξ + 1
2
n

i,j=1

Q
(T −t)hj∂ξj(∂ξiv)2 dt dξ
J1 =
n

i,j=1

Q
(T −t)∂ξihj∂ξjv∂ξiv dt dξ + 1
2
n

i,j=1

Q
(T −t)∂ξj[hj|∂ξiv|2] dt dξ
−1
2
n

i,j=1

Q
(T −t)∂ξjhj|∂ξiv|2 dt dξ
= K1 + 1
2

[0,T ]×∂Ω
(T −t)|∇ξv|2 dt dσ
= K1 + 1
2

[0,T ]×∂Ω
(T −t)

∂v
∂ν

2
dt dσ,
because v = 0 on ∂Ωand
h · ∇ξ = ∂v
∂ν
on ∂Ω.
Now
|K1| ≤C4|x0|2
L2(Ω)
(4.9)
for some constant C4 > 0. We have
J = −K1 + 1
2

[0,T ]×∂Ω
(T −t)

∂v
∂ν

2
dt dσ.
(4.10)
Finally as I = J
 T
0
(T −t)

∂v
∂ν

2
L2(∂Ω)
dt = I + K1,
(4.11)
so that the required estimate (4.6) follows from (4.9), (4.10), and (4.11).
⊓⊔
Remark 4.1. One can consider diﬀerent situations such as the wave equa-
tions with Neumann Boundary conditions, or the plate equations with several
boundary conditions. As for the veriﬁcation of the condition (HH)–(i), the
reader is referred to the review paper by I. Lasiecka and R. Triggiani [11].
⊓⊔
5 Some result for general semigroups
In this section we do not assume that A generates a strongly continuous group
but that a condition weaker than (HH)1 holds.

472
IV-3 Unbounded Control Operators in Hyperbolic Equations
(HH)′
1
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
(i)
A generates a strongly continuous semigroup etA in H of
type ω0 and λ0 is a real number in ρ(A) such that ω0 < λ0,
(ii)
E ∈L(U; H),
(iii)
∃K > 0 such that
 T
0
|E⋆A⋆esA⋆x|2 ds≤K2|x|2, ∀x ∈D(A⋆).
If we replace (HH)1 with (HH)′
1 Proposition 1.1 still holds true as can easily
be seen and the state equation (1.3) is meaningful. We consider again the cost
function (1.4) and assume that (HH)2 is veriﬁed. We denote by (HH)′ the set
of all hypotheses (HH)′
1 and (HH)2. Besides the Riccati equation (1.5) and
the approximating equation (1.6) we consider, as in V. Barbu and G. Da
Prato [1], the dual Riccati equation

Q′ = AQ + QA⋆−QC⋆CQ + (E⋆A⋆)⋆E⋆A⋆,
Q(0) = 0,
(5.1)
as well as the approximating problem

Q′
n = AQn + QnA⋆−QnC⋆CQn + (E⋆A⋆
n)⋆E⋆A⋆
n,
Qn(0) = 0,
(5.2)
where An = nAR(n, A) are the Yosida approximations of A.
Proposition 5.1. Assume (HH)′ and let Pn ∈Cs

[0, T ]; Σ(H)

be the mild
solution of (1.6). Then there is P ∈Cs

[0, T ]; Σ(H)

such that the following
limit exists:
lim
n→∞Pn = P,
in Cs

[0, T ]; Σ(H)

.
In the sequel we shall call P the mild solution of (1.5).
Proof. We ﬁrst remark that the hypotheses of Theorem 4.1 in Chapter 1 are
veriﬁed, so that problems (5.1) and (5.2) have mild solutions, and moreover
the following limit exists:
lim
n→∞Qn = Q
in Cs

[0, T ]; Σ(H)

.
Denote by CT a positive constant such that
∥Qn(t)∥≤CT ,
∀t ∈[0, T ], n ∈N.
(5.3)
Fix now t ∈[0, T ]. Then by Theorem 7.4 in Chapter 1 we have
Pn(t) = UGn,t(t, 0)[I + P0Qn(t)]−1P0U ∗
Gn,t(t, 0)
+
 t
0
UGn,t(t, s)C∗CU ∗
Gn,t(t, s) ds,
(5.4)
where
Gn,t(s) = A∗−C∗CQn(t −s),
s ∈[0, t].
Therefore UGn,t is the evolution operator with respect to Gn,t.

5 Some result for general semigroups
473
Step 1.
There exists C1 > 0 such that
∥UGn,t(τ, s)∥≤C1,
∀(s, t), 0 ≤s ≤τ ≤t,
and
∀n ∈N.
(5.5)
In fact let x ∈H and set ϕn(τ) = UGn,t(τ, s)x. Then we have
ϕn(τ) = e(τ−s)A∗x −
 τ
s
e(τ−σ)A∗C∗CQn(t −σ)ϕn(σ) dσ.
Let MT > 0 such that
∥esA∥≤MT ,
∀s ∈[0, T ].
Then, recalling (5.3), we obtain
|ϕn(τ)| ≤MT |x| + MTC
 τ
s
|ϕn(σ)| dσ,
and the conclusion follows from Gronwall’s lemma.
Step 2.
We have
lim
n→∞UGn,t(τ, s)x = UGt(τ, s)x,
∀x ∈H, 0 ≤s ≤τ ≤t.
(5.6)
Let x ∈H and set ϕn(τ) = UGn,t(τ, s)x and ϕ(τ) = UGn(τ, s)x. Setting
ζn(·) = ϕ(·) −ϕn(·); then ζn is the mild solution of the following problem:

ζ′
n(τ)
=A∗ζn(τ)−C∗CQn(t −τ)ζn(τ)+C∗C

Qn(t −τ)−Q(t −τ)

ϕ(τ),
ζn(0)
= 0,
that is to the integral equation
ζn(τ) =
 τ
s
UGn,t(τ −σ)C∗C

Qn(t −σ) −Q(t −σ)

ϕ(σ) dσ.
Now (5.6) follows from (5.5).
Step 3.
Conclusion. It is suﬃcient to let n tend to inﬁnity in (5.4).
⊓⊔
We can now proceed as in §3. Let (u⋆, x⋆) (resp. (u⋆
n, x⋆
n)) be the optimal
pair for the control problem (1.3)–(1.4) (resp. (1.7)–(1.8)).
Then the following result is proved as Theorem 3.1.
Theorem 5.1. Assume that (HH′) is veriﬁed and that P0 ∈Σ+(H). Then
we have

474
IV-3 Unbounded Control Operators in Hyperbolic Equations
(i) limn→∞u⋆
n = u⋆in L2(0, T ; U),
(ii) limn→∞x⋆
n = x⋆in C([0, T ]; H).
Example 5.1 (Age-dependent equations).
Consider a dynamical system describing the evolution of a certain popu-
lation (see for instance G. F. Webb [5]), governed by the equations
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
pt(t, a) + pa(t, a) + µ(a)p(t, a) = 0,
a ∈[0, a+], t ≥0,
p(0, a) = p0(a),
a ∈[0, a+],
p(t, 0) =
 a+
0
β(b)p(t, b) db + u(t),
t ≥0.
(5.7)
Here p(t, a) is the density of the population of age a at time t, µ is the mortality
rate, β is the birth rate, a+ is the maximal age, and u is the control. We shall
assume that µ ≥0, β ≥0, µ, β ∈C1([0, a+]) and
 a+
0
µ(b) db = +∞,
 a+
0
β(b) exp

−
 β
0
µ(c)dc

db ̸= 1.
(5.8)
We want to minimize a quadratic cost function of the form
J(u) =
 T
0
dt
 a+
0
p2(t, a) da + u2(t)
	
,
(5.9)
over all controls u ∈L2(0, T ) subject to state equation (5.7).
As shown in G. Da Prato and M. Iannelli [1], problem (5.7) can be
written on the abstract form
p(t) = etAp0 −αA
 t
0
e(t−s)AEu(s) ds,
t ∈[0, T ],
(5.10)
where A is the inﬁnitesimal generator of a C0 semigroup on H = L2(0, a+)
and D is a mapping from R into H. More precisely the operator A is given by
Aϕ
def
= −ϕ′ −µϕ,
D(A) =

ϕ ∈H : ϕ′ + µϕ ∈H and ϕ(0) =
 a+
0
β(b)ϕ(b) db

,
(5.11)
and E is deﬁned by
x 	→E(x) = π(x): R →H,
(5.12)
where π is the element of H given by
π(a) = e−
R a
0 µ(b) db
a ∈[0, a+].
(5.13)
Moreover the semigroup generated by A is given by

5 Some result for general semigroups
475
etAϕ =

Bϕ(t −a)π(a)
if a ∈[0, t[
ϕ(a −t)π(t)
if a ∈[t, a+] ,
ϕ ∈L2(0, a+),
(5.14)
where B is the solution to the integral equation
B(a) = Fϕ(a) +
 a
0
K(a −b)B(b) db,
a ∈[0, a+],
(5.15)
and
Fϕ(a) =
 a+
a
ϕ(b −a)K(b)π(a)
π(b) db,
a ∈[0, a+].
(5.16)
We now check hypotheses (HH)′. It is suﬃcient to show that (HH)1–(iii)
holds. If ϕ ∈D(A∗) we have in fact
 a+
0
|E∗A∗etA∗ϕ|2 dt =
 a+
0
|⟨A∗etA∗ϕ, π⟩|2dt
=
 a+
0

d
dt⟨ϕ, etAπ⟩

2
dt.
(5.17)
Moreover, recalling (5.14), we have
d
dt⟨ϕ, etAπ⟩= d
dt
 t
0
ϕ(a)Bπ(t −a)π(a) da +
 a+
t
ϕ(a −t)π(t) da

= ϕ(t)Bπ(0)π(b) +
 t
0
ϕ(a)B′
π(t −a)π(a) da
−ϕ(a+ −t)π(t) +
 a+−t
0
ϕ(a)π′(t) da.
(5.18)
Now, by plugging (5.18) into (5.17) the conclusion follows.
⊓⊔

Part V
Quadratic Optimal Control: Inﬁnite Time
Horizon

1
Bounded Control Operators: Control Inside
the Domain
1 Introduction and setting of the problem
As in Chapter 1 (Part IV) we consider a dynamical system governed by the
following state equation:

x′(t) = Ax(t) + Bu(t),
t ≥0,
x(0) = x0 ∈H,
(1.1)
and we use the notation introduced in §1 of that chapter. We assume that
(H)∞
⎧
⎪
⎨
⎪
⎩
(i)
A generates a C0 semigroup etA
in H,
(ii)
B ∈L(U; H),
(iii)
C ∈L(H; Y ).
Clearly, if the assumptions (H)∞hold, then the assumptions (H) of §1 in
Chapter 1 (Part IV) are veriﬁed with P0 = 0.
We want to minimize the cost function:
J∞(u) =
 ∞
0
{|Cx(s)|2 + |u(s)|2} ds,
(1.2)
over all controls u ∈L2(0, ∞; U) subject to the diﬀerential equation constraint
(1.1). We say that the control u ∈L2(0, ∞; U) is admissible if J∞(u) < ∞.
An admissible control u⋆∈L2(0, ∞; U) is called an optimal control if
J∞(u⋆) ≤J∞(u),
∀u ∈L2(0, ∞; U).
In this case the corresponding solution of (1.1) is called an optimal state and
the pair (u⋆, x⋆) an optimal pair.
Simple examples in ﬁnite dimension show that admissible controls can
fail to exist. When, for any x0 ∈H, an admissible control exists, we say

480
V-1 Bounded Control Operators: Control Inside the Domain
that (A, B) is stabilizable with respect to the observation operator C, or, for
brevity, that (A, B) is C-stabilizable. In this case it is still possible to solve
problem (1.1)–(1.2) by using Dynamic Programming. In fact when there exists
an admissible control, one can show that the minimal solution Pmin(t) to the
Riccati equation (2.7) in §1 of Chapter 1 in Part IV, that is the solution
corresponding to the initial condition P0 = 0, converges, as t →∞to a
solution P ∞
min of the algebraic Riccati equation:
A⋆X + XA −XBB⋆X + C⋆C = 0.
(1.3)
Moreover (1.3) has a nonnegative solution if and only if (A, B) is C-stabilizable.
The minimal nonnegative solution
of (1.3) P ∞
min is the main tool in solv-
ing problem (1.1)–(1.2). In fact one can show (see §3) that if (A, B) is C-
stabilizable, then the optimal control u⋆is given by
u⋆(t) = −B⋆P ∞
minx⋆(t),
(1.4)
where x⋆is the solution to the closed loop equation:

x′(t) = (A −BB⋆P ∞
min)x(t),
t ∈[0, T ],
x(0) = x0 ∈H.
(1.5)
The operator
F = A −BB⋆P ∞
min
(1.6)
is called the feedback operator. It is important for the applications to know
whether F is exponentially stable or not. A suﬃcient condition for this is that
(A, C) be detectable, i.e. that (A⋆, C⋆) be I-stabilizable. This condition is also
necessary if H is ﬁnite dimensional or in special situations (see §3). In §4 we
study some qualitative properties of the solutions of (1.3). In particular we
ﬁnd that if F is exponentially stable, then P ∞
min is globally attractive (among
all nonnegative solutions of Riccati equations), and so it is the unique solution
of (1.3). We also study the existence of a maximal solution P ∞
Max of (1.3). We
prove that a maximal solution P ∞
Max exists if (A, B) is I-stabilizable. In this
case one can show that the inﬁmum of J∞(u) over all u ∈L2(0, ∞; U) such
that x ∈L2(0, ∞; H), subject to the diﬀerential equation constraint (1.1), is
precisely (P ∞
Maxx0, x0).
We end §4 by studying periodic solutions of Riccati equations. Finally §5
is devoted to examples and §6 to complements.
2 The algebraic Riccati equation
We assume here that (H)∞is veriﬁed and consider the system (1.1).
Deﬁnition 2.1.
(i) (A, B) is said to be stabilizable with respect to the
observation C, or C-stabilizable, if for any x0 ∈H there exist u ∈
L2(0, ∞; U) such that Cx ∈L2(0, ∞; Y ), where x is the corresponding
solution to (1.1).

2 The algebraic Riccati equation
481
(ii) (A, B) is called feedback stabilizable with respect to the observation C,
if there exists K ∈L(H; U), N > 0, ω > 0 such that
∥Cet(A−BK)∥≤Ne−ωt,
∀t ≥0.
⊓⊔
We consider the Riccati equation
P ′ = A⋆P + PA −PBB⋆P + C⋆C
(2.1)
and the corresponding stationary equation
A⋆X + XA −XBB⋆X + C⋆C = 0.
(2.2)
In the sequel we shall consider only nonnegative solutions of (2.1) and (2.2).
Deﬁnition 2.2.
(i) We say that X ∈Σ+(H) is a weak solution of (2.2) if
(Xx, Ay) + (Ax, Xy) −(B⋆Xx, B⋆Xy) + (Cx, Cy) = 0
(2.3)
for all x, y ∈D(A).
(ii) X is called a strict solution of (2.2) if X ∈D(A) (the operator deﬁned
by (3.3) in Chapter 1 (Part IV) and
A(X) −XBB⋆X + C⋆C = 0.
(2.4)
⊓⊔
We remark that if X is a strict solution of (2.2), then by Proposition 3.1
(Chapter 1, Part IV), we have
A⋆Xx + XAx −XBB⋆Xx + C⋆Cx = 0
(2.5)
for any x ∈D(A) (because if x ∈D(A) then Xx ∈D(A⋆)).
Finally we introduce the following deﬁnition.
Deﬁnition 2.3. We say that X ∈Σ+(H) is a stationary solution of (2.1) if it
coincides with the mild solution of (2.1) with initial condition P(0) = X.
⊓⊔
Proposition 2.1. Let X ∈Σ+(H), then the following statements are equiv-
alent:
(i) X is a weak solution of (2.2).
(ii) X is a strict solution of (2.2).
(iii) X is a stationary solution of (2.1).
Proof. (i) =⇒(ii). Assume that (2.3) holds for any x, y ∈D(A). Then we
have
ϕX(x, y) = (B⋆Xx, B⋆Xy) −(Cx, Cy),
∀x, y ∈D(A),
where ϕX is the bilinear form deﬁned by (3.1) in Chapter 1 of Part IV. Clearly
ϕX is continuous in H × H so that X ∈D(A) and (2.4) holds.

482
V-1 Bounded Control Operators: Control Inside the Domain
(ii) =⇒(i). Let X ∈D(A) be a strict solution of (2.1) and let x ∈D(A);
then Xx ∈D(A⋆) and (2.5) holds true. Now, if x, y ∈D(A), (2.3) follows and
X is a weak solution.
(ii) =⇒(iii). If X is a strict solution of (2.2), then P(t) = X is a strict
solution. Therefore it is a stationary solution, of (2.1).
(iii) =⇒(i). Let X be a stationary solution of (2.1), and set P(t) = X.
Then by Proposition 2.1 in Chapter 1 of Part IV, P is a weak solution of (2.1)
so that (2.3) holds and X is a weak solution of (2.2).
⊓⊔
Due to Proposition 2.1, all kinds of solutions are the same; in the sequel
we shall call a solution both a weak and a strict solution of (2.2).
We are going to study existence of a solution of the algebraic Riccati
equation. We follow here A. J. Pritchard and J. Zabczyk [1]. It is useful
to consider the solution of the Riccati equation with initial condition 0,

P ′ = A⋆P + PA −PBB⋆P + C⋆C,
P(0) = 0.
(2.6)
Its solution will be denoted by Pmin(·).
Remark 2.1. Pmin(·) is the minimal nonnegative solution of the Riccati equa-
tion. In fact if P0 ∈Σ+(H) and P is the mild solution of (2.1) such that
P(0) = P0, then by Proposition 2.2 in Chapter 1 of Part IV, we have
Pmin(t) ≤P(t),
∀t ≥0.
It follows that if X is a solution of (2.2), then
Pmin(t) ≤X,
∀t ≥0.
⊓⊔
We now prove the following properties.
Proposition 2.2. The following statements hold:
(i) for any x ∈H, (Pmin(·)x, x) is non decreasing,
(ii) assume that, for some R ∈Σ+(H), we have
Pmin(t) ≤R,
∀t ≥0.
Then for all x ∈H the limit
P ∞
minx = lim
t→∞Pmin(t)x,
(2.7)
exists, and P ∞
min is a solution of equation (2.2).
Proof. Let ε > 0, t ≥0 and let P be the solution of (2.1) such that P(0) =
Pmin(ε). By Proposition 2.2 in Chapter 1 of Part IV, we have
Pmin(t + ε) = P(t) ≥Pmin(t)

2 The algebraic Riccati equation
483
and (i) is proved. Assume now Pmin(t) ≤R; as Pmin(t) is nondecreasing and
bounded we can set
γ(x) = lim
t→∞(Pmin(t)x, x),
∀x ∈H.
For x, y ∈H we have
2 Re(Pmin(t)x, y)=(Pmin(t)(x+y), x+y)−(Pmin(t)x, x)−(Pmin(t)y, y),
2 Im(Pmin(t)x, y)=i(Pmin(t)(x+iy), x+iy)−(Pmin(t)x, x)−(Pmin(t)(iy), iy).
So the limit
Γ(x, y) = lim
t→∞(Pmin(t)x, y),
∀x, y ∈H,
exists and the following operator P ∞
min ∈Σ+(H) can be deﬁned:
lim
t→∞(Pmin(t)x, y) = (P ∞
min(t)x, y),
∀x, y ∈H.
It follows that
lim
t→∞([P ∞
min −Pmin(t)]x, x) = 0,
x ∈H,
which is equivalent to
lim
t→∞

P ∞
min −Pmin(t)
1/2x = 0,
∀x ∈H.
This implies that
lim
t→∞

P ∞
min −Pmin(t)

x = 0,
∀x ∈H,
so that (2.7) holds. It remains to show that P ∞
min is a solution of (2.2). For
this we denote by Ph the solution of (2.1) for which Ph(0) = Pmin(h), i.e.
Ph(t) = Pmin(h + t). As
lim
h→∞Pmin(h)x = P ∞
minx,
∀x ∈H,
by Theorem 2.2 in Chapter 1 of Part IV, we have
lim
h→∞Ph(·)x = P ∞
minx
in C([0, T ]; H),
∀x ∈H, ∀T > 0.
Moreover P ∞
min is a solution of (2.1) (hence stationary).
Remark 2.2. Assume that there exists a solution X ∈Σ+(H) of (2.2). Then,
by Proposition 2.2 and Remark 2.1, the solution P ∞
min deﬁned by (2.7) exists.
By the above proposition it follows that
P ∞
min ≤X,
for all solution X ∈Σ+(H) of (2.2). Thus P ∞
min is the minimal solution of the
algebraic Riccati equation (2.2).
⊓⊔

484
V-1 Bounded Control Operators: Control Inside the Domain
We now prove that a nonnegative solution of the algebraic Riccati equation
exists if and only if (A, B) is C-stabilizable.
Proposition 2.3. Assume that (H)∞is veriﬁed and that (A, B) is C-stabil-
izable. Then there exists a minimal solution P ∞
min of (2.2).
Proof. We ﬁrst recall that by (6.2) in Part IV, Chapter 1, we have
(Pmin(t)x0, x0) +
 t
0
|u(s) + B⋆Pmin(t −s)x(s)|2 ds
=
 t
0
{|Cx(s)|2 + |u(s)|2} ds,
(2.8)
for any x0 ∈H and any u ∈L2(0, ∞; U), where x is the solution of (1.1). Let
u be a control in L2(0, ∞; U) such that the corresponding solution x of (1.1)
is such that Cx belongs to L2(0, ∞; Y ). By (2.8) it follows that
sup
t≥0
(Pmin(t)x0, x0) ≤
 ∞
0
{|Cx(s)|2 + |u(s)|2} ds < ∞
for any x0 ∈H. By the Uniform Boundedness Theorem it follows that Pmin(t)
is bounded, so that, by Proposition 2.2, there exists a solution of (2.2).
⊓⊔
In order to prove the converse result it is useful to introduce, for any t > 0,
the following auxiliary optimal control problems over the ﬁnite time horizon
[0, t]: To minimize
Jt(u) =
 t
0
{|Cx(s)|2 + |u(s)|2} ds,
(2.9)
over all controls u ∈L2(0, t; U) subject to the diﬀerential equation constraint
(1.1). By Theorem 6.1 in Part IV, Chapter 1, we know that there exists a
unique optimal pair (xt, ut) for problem (2.9), where xt is the mild solution
to the closed loop equation:

x′
t(s) = [A −BB⋆Pmin(t −s)]xt(s),
0 ≤s ≤t,
xt(0) = x0,
and ut is given by the feedback formula
ut(s) = −B⋆Pmin(t −s)xt(s),
0 ≤s ≤t.
Moreover the optimal cost is given by
(Pmin(t)x0, x0) =
 t
0
{|Cxt(s)|2 + |ut(s)|2} ds.
(2.10)

2 The algebraic Riccati equation
485
Lemma 2.1. Assume that the minimal solution P ∞
min of (2.2) exists. Denote
by x∞the corresponding mild solution to the equation

x′
∞(s) = [A −BB⋆P ∞
min]x∞(s),
s ≥0,
x∞(0) = x0,
and set
u∞(s) = −B⋆P ∞
minx∞(s),
s ≥0.
(2.11)
Then we have
lim
t→∞xt(s) = x∞(s),
s ≥0,
(2.12)
lim
t→∞ut(s) = u∞(s),
s ≥0.
(2.13)
Proof. Fix T > t and set zt = xt −x∞; then for all 0 ≤s ≤T , zt is the mild
solution of the problem:

z′
t(s)=[A−BB⋆Pmin(t−s)]zt(s)+BB⋆[Pmin(t−s)−P ∞
min]x∞(s),
zt(0)=0.
(2.14)
Denote by U(r, s) the evolution operator corresponding to A−BB⋆Pmin(t−·);
then for x ∈H
⎧
⎨
⎩
U(r, σ)x = e(r−σ)Ax −
 r
σ
e(r−ρ)ABB⋆Pmin(t −ρ)U(ρ, σ)x dρ,
U(σ, σ) = I.
It follows that
∥U(r, σ)∥≤Me(r−σ)ω + M∥B∥2∥P ∞
min∥
 r
σ
e(r−ρ)ω∥U(ρ, σ)∥dρ.
By Gronwall’s Lemma we have
∥U(r, σ)∥≤Me(r−σ)[ω+M∥B∥2∥P ∞
min∥],
0 ≤σ ≤r ≤T.
(2.15)
We now return to problem (2.14), which we write in the form
zt(s) =
 s
0
U(s, σ)BB⋆[Pmin(t −σ) −P ∞
min]x∞(σ) dσ.
By (2.15) and by the Dominated Convergence Theorem we obtain zt(s) →0
as t →∞. So (2.12) and then (2.13) follow.
⊓⊔
We can now prove the following proposition.
Proposition 2.4. Assume that there exists a solution of (2.2). Then (A, B)
is C-stabilizable.

486
V-1 Bounded Control Operators: Control Inside the Domain
Proof. Let xt and ut be deﬁned as in the lemma. By (2.10) we have for t ≥T
(P ∞
minx0, x0) ≥
 T
0
{|Cxt(s)|2 + |ut(s)|2} ds
(2.16)
and, as t →∞,
(P ∞
minx0, x0) ≥
 T
0
{|Cx∞(s)|2 + |u∞(s)|2} ds.
(2.17)
But as T is arbitrary we ﬁnd
(P ∞
minx0, x0) ≥
 ∞
0
{|Cx∞(s)|2 + |u∞(s)|2} ds
(2.18)
and thus u∞∈L2(0, ∞; U) is an admissible control.
⊓⊔
3 Solution of the control problem
We now consider the control problem (1.1)–(1.2) and prove the central result.
Theorem 3.1. Assume that the conditions (H)∞are veriﬁed and that (A, B)
is C-stabilizable. Then there exists a unique optimal pair (u⋆, x⋆) for the op-
timal control problem (1.1)–(1.2). Moreover the following statements hold:
(i) x⋆∈C([0, ∞[; H) is the mild solution to the closed loop equation (1.5).
(ii) u⋆∈C([0, ∞[; U) is given by the feedback formula
u⋆(t) = −B⋆P ∞
minx⋆(t),
t ∈[0, T ],
(3.1)
where P ∞
min represents the minimal solution of (2.2).
(iii) The optimal cost J∞(u⋆) is given by
J∞(u⋆) = (P ∞
minx0, x0).
(3.2)
Proof. Let u ∈L2(0, ∞; U) and let x be the corresponding solution of the
state equation (1.1). By the identity (2.8) we have
(Pmin(t)x0, x0) ≤
 t
0
{|Cx(s)|2 + |u(s)|2} ds ≤J∞(u).
It follows that
J∞(u) ≥(P ∞
min(t)x0, x0),
∀u ∈L2(0, ∞; U).
Let now u∞be deﬁned by(2.11); by (2.18) we have
(P ∞
min(t)x0, x0) ≥J∞(u∞),

3 Solution of the control problem
487
so that u∞is optimal. Formula (3.1) with u⋆= u∞, x⋆= x∞follows from
(2.12)–(2.13). It remains to show uniqueness. Let (ˆu, ˆx) be another optimal
pair; then J∞(ˆu) = (P ∞
minx0, x0). Fix T > 0. By applying (2.8) with t ≥T we
obtain
 T
0
|ˆu(s) + B⋆Pmin(t −s)ˆx(s)|2 ds ≤J∞(ˆu) −(Pmin(t)x0, x0)
≤([P ∞
min −Pmin(T )]x0, x0).
As t →∞we have
 T
0
|ˆu(s) + B⋆Pminˆx(s)|2 ds ≤([P ∞
min −Pmin(T )]x0, x0),
and letting T tend to ∞, necessarily ˆu(s) = −B⋆P ∞
minˆx(s). Consequently ˆx is
also a solution of the closed loop equation that necessarily coincides with x∞.
Then ˆu = u∞and the proof is complete.
⊓⊔
We now give a regularity result for the optimal pair (u⋆, x⋆).
Proposition 3.1. Assume that the conditions of Theorem 3.1 are veriﬁed and
let (u⋆, x⋆) be the optimal pair for the control problem (1.1)–(1.2). Then the
following statements hold:
(i) If x0 ∈D(A), then x⋆is a strict solution of (1.5) and belongs to
C1([0, ∞[; H) ∩C

[0, ∞[; D(A)

.
(ii) If etA is an analytic semigroup, then x⋆is a classical solution of (1.5).
Proof. (i) follows from Proposition 3.3 in Part II, Chapter 1, and (ii) from
Proposition 3.9 in Part II, Chapter 1.
⊓⊔
3.1 Feedback operator and detectability
We assume here that the conditions (H) are veriﬁed and that (A, B) is C-
stabilizable. We denote by P ∞
min the minimal solution of the algebraic Riccati
equation (2.2). Under these hypotheses, by Theorem 3.1, there exists a unique
optimal pair (u⋆, x⋆), and

x⋆(t) = etF x0,
t ≥0,
u⋆(t) = −B⋆P ∞
minx⋆(t) = −B⋆P ∞
minetF x0,
t ≥0,
where
F = A −BB⋆P ∞
min
is called the closed loop operator. Moreover, by (3.2) we have
(P ∞
minx0, x0) =
 ∞
0
{|CetFx0|2 + |B⋆P ∞
minetF x0|2|} ds.
(3.3)

488
V-1 Bounded Control Operators: Control Inside the Domain
Remark 3.1. When C = I (or if C−1 ∈L(H)), by (3.3) and Datko’s Theorem,
it follows that F is exponentially stable. In particular, if (A, B) is stabilizable
with respect to the identity I then it is feedback stabilizable.
⊓⊔
It is important for the applications to give conditions under which F is
stable (even when C is not invertible). A suﬃcient condition is given by the
detectability property. In order to deﬁne detectability we introduce the dual
system

y′(t) = A⋆y(t) + C⋆v(t),
t ≥0,
y(0) = y0 ∈H.
For this system the space of states, controls, and observations are H, Y , and
H, respectively.
Deﬁnition 3.1. We say that the pair (A, C) is detectable if the pair (A⋆, C⋆)
is I-stabilizable.
⊓⊔
Remark 3.2. By Remark 3.1 it follows that (A, C) is detectable if and only if
there exists K ∈L(Y ; H) such that A −KC is exponentially stable.
⊓⊔
The next result is due to W. M. Wonham [2] in the ﬁnite dimensional
case and to J. Zabczyk [3] in the general case.
Proposition 3.2. Assume that (A, C) is detectable. Then F is exponentially
stable.
Proof. Let x0 ∈H, x(t) = etF x0; by Remark 3.2 there exists K ∈L(Y ; H)
such that A −KC is exponentially stable. As
F = (A −KC) + (KC −BB⋆P ∞
min),
we have that x is the mild solution to the problem

x′ = (A −KC)x + z,
x(0) = x0,
where z = (KC −BB⋆P ∞
min)x. It follows that
x(t) = et(A−KC)x0 +
 t
0
e(t−s)(A−KC)[KC −BB⋆P ∞
min]x(s) ds.
By (3.3) we have that Cx and BB⋆P ∞
minx belong to L2(0, ∞; H); it follows
that x ∈L2(0, ∞; H) because A −KC is exponentially stable. By Datko’s
Theorem this implies the conclusion.
⊓⊔
Remark 3.3. The assumption that (A, C) is detectable is not a necessary con-
dition that F = A −BB⋆P ∞
min be exponentially stable, as Example 3.1 below
shows. Notice however that this condition is necessary and suﬃcient in some
special cases (see §3.2).
⊓⊔

3 Solution of the control problem
489
The following example was discussed in the reference of G. Da Prato
and M. C. Delfour [1, 2].
Example 3.1. Let H = U = Y = ℓ2, the Hilbert space of all the sequences
{xn} of complex numbers such that ,∞
n=1 |xn|2 < ∞. We set
A{xn} =

n
n + 1xn

,
B = I,
C{xn} =
√2n + 1
n + 1 xn

.
Then (2.2) reduces to
2AX −X2 + C2 = 0,
and we have
P ∞
min = A +
%
A2 + C2 = A + I,
F = A −P ∞
min = −I
Thus F is exponentially stable. We now show that (A, C) is not detectable. In
fact assume, by contradiction, that (A, C) is detectable. Then the algebraic
Riccati equation
2AX −C2X2 + I = 0
has a positive solution X and we have
X{xn} = {(n + 1)xn}.
However this is not a bounded operator in H.
⊓⊔
3.2 Stabilizability and stability of the closed loop
operator F in the point spectrum case
We assume here (H)∞–(i)–(ii); we are interested in the stabilizability of the
system (1.1) under suitable conditions on the spectrum of A. We denote by
σ−(A), σ+(A), and σ0(A) those elements of the spectrum of A that have a
negative, positive, and zero real part, respectively. The elements of σ+(A) ∪
σ0(A) are called unstable points of σ(A). Obviously, if σ+(A)∪σ0(A) is empty
and if A veriﬁes the spectral determining condition (see Part II, Chapter 1,
§2.9 §2.9), then (A, B) is stabilizable. We now assume that
(P)
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
(i)
the set σ+(A) ∪σ0(A) consists of a ﬁnite set of
eigenvalues of ﬁnite algebraic multiplicity,
(ii)
there exists ε > 0, NA > 0 such that
sup
λ∈σ−(A)
Re λ < −ε,
∥etAΠ−
A ∥≤NAe−εt,
∀t ≥0.
Here Π−
A represents the projector on σ−(A) deﬁned by

490
V-1 Bounded Control Operators: Control Inside the Domain
Π−
A =
1
2πi

γ−R(λ, A) dλ
and γ−is a simple Jordan curve around σ−(A). The projectors Π+
A and Π0
A
are deﬁned analogously. We set
H−
A = Π−
A(H),
H+
A = Π+
A(H),
H0
A = Π0
A(H).
Then H−
A , H+
A, H0
A are invariant subspace for the semigroup etA.
Remark 3.4. Assume that A fulﬁlls (P). Then by hypothesis (ii) H+
A is ﬁnite
dimensional and there exists η > 0 such that
Re λ > η,
∀λ ∈σ+(A).
(3.4)
Moreover etAH+
A can be extended for t < 0 by the formula
etAΠ+
A =
1
2πi

γ+ etλR(λ, A) dλ,
t ∈R.
Finally, by (3.4) there exists N ′
A > 0 such that
∥etAΠ+
A∥≤N ′
A,
∀t ≤0.
(3.5)
⊓⊔
Remark 3.5. Assumptions (P) are veriﬁed in each of the following cases:
(i) H is ﬁnite dimensional.
(ii) etA is compact for any t > 0.
⊓⊔
3.3 Stabilizability
In this section we want to give a necessary and suﬃcient condition in order
that (A, B) be stabilizable with respect to the identity I. We ﬁrst prove a
lemma on the existence of solutions of (1.1) in [0, +∞[.
Lemma 3.1. Assume that conditions (H)∞–(i)–(ii) are veriﬁed and moreover
that A fulﬁlls (P) with σ0(A) = ∅. Then the following statements are equiv-
alent:
(i) for all x∈H, there exists u∈L2([0, +∞[ ; U) such that x∈L2([0, +∞[ ; H),
x being the solution to (1.1) corresponding to u.
(ii) Range γ ⊃Π+
A(H), where γ is deﬁned by
u 	→γu =
 ∞
0
e−sAΠ+
ABu(s) ds: L2([0, +∞[ ; U) →H,

3 Solution of the control problem
491
(iii) the mapping
ξ 	→γ⋆ξ = B⋆(Π+
A)⋆e−·AST ARξ : H →L2([0, +∞[ ; U),
is one-to-one,
(iv) for all λ ∈σ+(A⋆),
Ker (B⋆) ∩Ker(λ −A⋆) = {0}.
Proof. Let u ∈L2([0, +∞[ ; U), and let x be the solution to (1.1). Then we
have
x(t) = etAx0 +
 t
0
e(t−s)ABu(s) ds = x−(t) + x+(t),
where
x−(t) = etAΠ−
A x0 +
 t
0
e(t−s)AΠ−
A Bu(s) ds −
 +∞
t
e(t−s)AΠ+
ABu(s) ds,
x+(t) = etA

Π+
Ax0 +
 +∞
0
e−sAΠ+
ABu(s) ds

.
Remark that x−∈L2(0, ∞; H) by (P)–(ii) and (3.5), whereas x+ ∈L2(0, ∞; H)
if and only if
Π+
Ax0 +
 +∞
0
e−sAΠ+
ABu(s) ds = 0.
It follows that (i) ⇐⇒(ii). Moreover as the space Π+
A(H) is ﬁnite dimen-
sional, (ii) ⇐⇒(iii) by the Alternative Principle. Thus it remains to show
that (iii) ⇐⇒(iv). Assume (iii) and, by contradiction, that (iv) does not
hold. Then there exist λ ∈σ+(A⋆) and ξ ∈

Π+
A(H)
⋆diﬀerent from 0 such
that B⋆ξ = 0, λξ = A⋆ξ = 0. Then e−sA⋆ξ = e−λsξ and we have
B⋆e−sA⋆(Π+
A)⋆ξ = 0,
s ≥0,
(3.6)
which is a contradiction.
(iv) =⇒(iii). Assume (iv) and, by contradiction, that (iii) does not hold.
Then there exists ξ ∈

Π+
A(H)
⋆diﬀerent from 0 such that (3.6) holds. By
diﬀerentiating (3.6) several times with respect to s and by setting s = 0 we
see that
p(A⋆)ξ = 0
for all polynomials p. Choose p as the minimal degree polynomial such that
p(A⋆)ξ = 0. Let λ0 be a root of p; then clearly λ0 ∈σ+(A⋆). Set
q(λ) =
p(λ)
λ −λ0
and
ψ = q(A⋆)ξ;
then by (3.6) it follows that ψ ∈Ker(B⋆) ∩Ker(λI −A⋆), which is a contra-
diction.
⊓⊔
The following result is a straightforward generalization of a result due to
M. L. J. Hautus [2].

492
V-1 Bounded Control Operators: Control Inside the Domain
Proposition 3.3. Assume that conditions (H)∞–(i)–(ii) and (P) are veriﬁed.
Then the following statements are equivalent:
(i) (A, B) is stabilizable with respect to I,
(ii) Ker(λ −A⋆) ∩Ker(B⋆) = {0},
∀λ ∈σ0(A) ∪σ+(A).
Proof. We can clearly choose ε > 0 such that, setting Aε = A + εI, then
σ0(Aε) is empty, so that we can apply Lemma 3.1. Setting v(t) = eεtu(t),
z(t) = eεtx(t), problem (1.1) becomes
z′(t) = Aεz(t) + Bv(t),
z(0) = x.
Now assume that (ii) holds. By Lemma 3.1, for all x0 ∈H, there exists
v ∈L2([0, ∞[; U) such that z ∈L2(0, ∞; H); thus (A, B) is I-stabilizable and
(i) holds.
It remains to prove that (i) =⇒(ii). In fact assume (i) and that, by
contradiction, (ii) does not hold. Then there exists λ ∈σ0(A⋆) ∪σ+(A⋆) and
ξ ∈D(A⋆) diﬀerent from 0 such that
B⋆ξ = 0,
λξ −A⋆ξ = 0.
(3.7)
Choose now x ∈H such that (x0, ξ) = 1; by the hypothesis (i) there exists
u ∈L2(0, ∞; U) such that x ∈L2(0, ∞; H). As
(xt, ξ) = (x0, etA⋆ξ) +
 t
0
(u(s), B⋆etA⋆ξ) ds,
by (3.7) it follows that (xt, ξ) = (x0, etA⋆ξ) = eλt, which is a contradiction,
because Re λ ≥0. The proof is complete.
⊓⊔
3.4 Exponential stability of F
We assume here that (A, B) is C-stabilizable and denote by F = A−BB⋆P ∞
min
the closed loop operator. We prove the following result.
Proposition 3.4. Assume that (A, B) is C-stabilizable and that A and F
fulﬁll (P). Then the following statements are equivalent:
(i) F is exponentially stable,
(ii) for any λ ∈σ+(A) ∪σ0(A), we have
Ker(A −λI) ∩Ker(C) = {0}.
(3.8)
Proof. (i) =⇒(ii). Assume that F is exponentially stable and, by contradic-
tion, that (ii) does not hold. Then there exists x0 ̸= 0 and λ0 ∈σ+(A) such
that
Re λ0 ≥0,
Ax0 = λ0x0,
Cx0 = 0.
(3.9)
It follows that

4 Qualitative properties of the solutions of the Riccati equation
493
d
dt(Pmin(t)x0, x0) = 2 Re λ0(Pmin(t)x0, x0) −|B⋆Pmin(t)x0|2
≤2 Re λ0(Pmin(t)x0, x0),
where Pmin is the minimal nonnegative solution of (2.1). As Pmin(0) = 0 by
(3.9), it follows that Pmin(t)x0 = 0 and then P ∞
minx0 = 0. Thus
Fx0 = Ax0 −BB⋆P ∞
minx0 = Ax0 = λx0,
which contradicts the fact that F be exponentially stable.
(ii) =⇒(i) Assume now that (3.8) holds and, by contradiction, that F is
not exponentially stable. Then there exists λ0 ∈C and x0 ̸= 0 in H such that
Fx0 = λ0x0,
Re λ0 ≥0.
Because, as easily checked, P ∞
min veriﬁes
F ⋆P ∞
min + P ∞
minF + P ∞
minBB⋆P ∞
min + C⋆C = 0,
we have
2 Re λ0(P ∞
minx0, x0) + |B⋆P ∞
minx0|2 + |Cx0|2 = 0,
which implies B⋆P ∞
minx0 = 0, Ax0 = λ0x0, and Cx0 = 0, and this fact contra-
dicts (ii).
⊓⊔
We now give a characterization for the stability of F.
Proposition 3.5. Assume that the conditions of Proposition 3.4 are veriﬁed.
Then the following statements are equivalent:
(i) F is exponentially stable,
(ii) (A, C) is detectable.
Proof. (i) =⇒(ii). If F is exponentially stable, then, by Proposition 3.4, (3.8)
holds. But this implies that (A⋆, C⋆) is stabilizable with respect to I (by
Proposition 3.3). Thus (ii) is proved. The implication (i) =⇒(ii) was proved
in Proposition 3.4.
⊓⊔
4 Qualitative properties of the solutions of the Riccati
equation
In this section we assume that conditions (H)∞are veriﬁed and that (A, B)
is stabilizable with respect to C. We again consider the equations
P ′ = A⋆P + PA −PBB⋆P + C⋆C,
(4.1)
A⋆X + XA −XBB⋆X + C⋆C = 0,
(4.2)
and denote by Pmin and P ∞
min the minimal solutions of (4.1) and (4.2), respec-
tively. We say that a solution X ∈Σ+(H) of (4.2) is maximal if

494
V-1 Bounded Control Operators: Control Inside the Domain
∀solution Y ∈Σ+(H)
of (4.2) =⇒Y ≤X.
If a maximal solution exists, it is clearly unique; however a maximal solution
does not exist in general (see Remark 4.2 below). The existence of a maximal
solution under suitable assumptions will be proved in §4.3.
4.1 Local stability results
We study here the exponential stability of any positive solution X of (4.2).
This property of X is naturally related to the exponential stability of the
linear operator A −BB⋆X.
Proposition 4.1. Let X ∈Σ+(H) be a solution of (4.2). Assume that A −
BB⋆X = K is exponentially stable, that is, that there exist a > 0, N > 0 such
that
∥etK∥≤Ne−at,
t ≥0.
(4.3)
Then there exist r > 0 and a > 0 such that, if P0 ∈Σ+(H) and ∥P0 −X∥<
1/2r N 2, we have
∥P(t) −X∥≤re−at,
t ≥0,
where P is the mild solution to (4.1) such that P(0) = P0.
Proof. Set Y (t) = P(t) −X; then Y is the mild solution to the problem
Y ′ = K⋆Y + Y K −Y BB⋆Y,
Y (0) = P0 −X = Y0,
(4.4)
which is equivalent to the equation γ(Y ) = Y , where γ is deﬁned by
γ(Y )(t)x = etK⋆Y0etKx −
 t
0
e(t−s)K⋆Y (s)e(t−s)Kx ds,
(4.5)
for any x ∈H. For any a ≥0 we introduce the Banach space
Ca

[0, ∞[ ; Σ(H)

=

Y ∈Cs

[0, ∞[ ; Σ(H)

: sup
t≥0
∥etaY (t)∥< ∞
 
,
and, for any r > 0 we set
Br =

Y ∈Ca

[0, ∞[ ; Σ(H)

: ∥Y ∥a ≤r
 
.
We now want to solve (4.4) by proving that γ has a ﬁxed point in Br, for a
suitable r. If Y, Z ∈Br we have, taking into account (4.3),
∥γ(Y )∥a ≤N 2∥Y0∥+ N 2r2
ea ∥B∥2
∥γ(Y ) −γ(Z)∥a ≤N 2∥Y0∥+ 2rN 2
ea ∥B∥2∥Y −Z∥.
Now choose r such that
2rN 2
ea ∥B∥2 ≤1
2,
N 2r2
ea ∥B∥2 ≤1
2
and then Y0 such that ∥Y0∥≤ρ/2N 2. Then γ is a contraction on Br and the
conclusion holds.
⊓⊔

4 Qualitative properties of the solutions of the Riccati equation
495
4.2 Attractivity properties of a stationary solution
We ﬁrst show that the minimal solution P ∞
min is globally attractive from below.
Proposition 4.2. Let P0 ∈Σ+(H) such that P0 ≤P ∞
min and let P be the
mild solution of (4.1) with P(0) = P0. Then we have
lim
t→∞P(t)x = P ∞
minx,
∀x ∈H.
Proof. By Proposition 2.2 we have
Pmin(t) ≤P(t) ≤P ∞
min,
so that
0 ≤(P ∞
minx −P(t)x, x) ≤(P ∞
minx −Pmin(t)x, x),
x ∈H,
and by (2.7)
lim
t→∞(P ∞
minx −P(t)x, x) = 0,
x ∈H,
which implies the conclusion.
⊓⊔
We now consider a general positive solution X of (4.2) and set K = A −
BB⋆X. We ﬁrst show that if K is exponentially stable, then X is globally
attractive from above.
Proposition 4.3. Let X ∈Σ+(H) be a solution of (4.2), and assume that
K = A −BB⋆X is exponentially stable and that (4.3) holds. Then for any
P0 ≥X we have
∥P(t) −X∥≤N 2e−2at∥P0 −X∥,
t ≥0,
(4.6)
where P is the solution to (4.1) such that P(0) = P0.
Proof. Set Y = P(t) −X; as P0 ≥X, we have, by Proposition 2.2 in Part IV,
Chapter 1, Y (t) ≥0. Moreover Y is the mild solution to problem (4.4). For
any x ∈D(A) we have (using the fact that Y is a strict solution of (4.4))
d
ds(Y (t −s)esKx, esKx) = |B⋆Y (t −s)esKx|2,
∀x ∈D(A)
so that (because D(A) is dense in H)
(Y (t)x, x) +
 t
0
|BY (t −s)esKx|2 ds = (Y (0)esKx, esKx),
∀x ∈H.
It follows that
(P(t)x −Xx, x) ≤(Y (0)esKx, esKx),
∀x ∈H,
which yields (4.6).
⊓⊔

496
V-1 Bounded Control Operators: Control Inside the Domain
The following result shows that if X is a positive solution of (4.2) such
that K = A −BB⋆X is exponentially stable, then X is maximal.
Proposition 4.4. Let X, Y ∈Σ+(H) be solutions of (4.2). Assume that K =
A −BB⋆X is exponentially stable. Then X ≥Y .
Proof. Set Z = X −Y ; by (4.4) it follows that
(Zx, Kx) + (ZKx, x) + (ZBB⋆Zx, x) = 0,
∀x ∈D(A),
which implies that
d
dt(ZesKx, esKx) = −|BST ARZesKx|2,
∀x ∈D(A).
As D(A) is dense in H, by integrating this between 0 and t, we obtain
(Zx, x) = (ZetKx, etKx) +
 t
0
|B⋆ZesKx|2 ds
≥(ZetKx, etKx),
∀x ∈H.
As t →∞we have (Zx, x) ≥0, ∀x ∈H so that X ≥Y .
⊓⊔
Corollary 4.1. Equation (4.2) has at most one positive solution X such that
A −BB⋆X is exponentially stable.
Proof. Let X, Y ∈Σ+(H) be two solutions of (4.2) such that A −BB⋆X and
A−BB⋆Y are exponentially stable. By Proposition 4.4 it follows that X ≥Y
and Y ≥X so that X = Y .
⊓⊔
Concerning the minimal solution P ∞
min we have a uniqueness result.
Corollary 4.2. If A −BB⋆P ∞
min is exponentially stable, then P ∞
min is the
unique positive solution of (4.2).
Proof. It is suﬃcient to observe that P ∞
min is both the maximal and the mini-
mal solution of (4.2).
⊓⊔
We show now that if F = A−BB⋆P ∞
min is exponentially stable, then P ∞
minis
globally attractive among all the positive solutions of (4.2). In ﬁnite dimension
this result is well known; see W. M. Wonham [2].
Proposition 4.5. If F = A −BB⋆P ∞
min is exponentially stable, then for any
P0 ∈Σ+(H) we have
lim
t→∞P(t)x = P ∞
minx,
∀x ∈H,
(4.7)
where P is the mild solution of (4.1) such that P(0) = P0.

4 Qualitative properties of the solutions of the Riccati equation
497
Proof. Choose n ∈N such that P0 ≤nI, P ∞
min ≤nI. Then by Proposition 2.2
in Part IV, Chapter 1, we have
Pmin(t) ≤P(t) ≤Q(t),
(4.8)
where Q is the mild solution of (4.1) such that Q(0) = nI. Now, as t →∞,
Pmin(t)x →P ∞
minx and, by (4.6), Q(t)x →P ∞
minx; this, along with (4.8),
implies (4.7).
⊓⊔
Remark 4.1. By Proposition 3.5 we know that if (A, C) is detectable, then the
feedback operator F is exponentially stable. Thus, by Corollary 4.2, P ∞
min is
the unique nonnegative solution to (4.2). However, detectability of (A, C) is
not necessary for the uniqueness, as the following example shows. Let H =
U = Y = R2,
A =
0 1
0 0
	
,
B = I,
C = 0;
as easily seen X = 0 is the unique solution of (4.2), but the feedback operator
F = A is not stable and (A, C) is not detectable.
When H is ﬁnite dimensional, M. Sorine [2] has shown that the following
conditions are suﬃcient for the uniqueness of the solution:
(i) (A, B) is C-stabilizable,
(ii) (A⋆, C⋆) is B⋆-stabilizable,
(iii) no eigenvalue of the closed loop operator F lies on the imaginary axis.
This result can be easily generalized to any Hilbert space H provided that A
and F fulﬁll assumptions (P) of §3.2.
⊓⊔
4.3 Maximal solutions
We assume here, beside (H)∞, that (A, B) is I-stabilizable (which, obvi-
ously, is stronger than assuming that (A, B) is C-stabilizable). We follow here
A. Bensoussan [1].
For any ε > 0, we consider the problem: To minimize
Jε(u) =
 ∞
0
{|Cx(s)|2 + ε|x(s)|2 + |u(s)|2} ds,
(4.9)
over all controls u ∈L2(0, ∞; U) subject to the diﬀerential equation constraint
(1.1). Consider the observation Dε = √C⋆C + εI (the observation space being
H). Clearly (A, B) is Dε-stabilizable, so that, by Theorem 3.1, there exists a
unique optimal pair (uε, xε) and a solution Pε of the algebraic Riccati equation
A⋆X + XA −XBB⋆X + C⋆C + εI = 0,
(4.10)
such that

498
V-1 Bounded Control Operators: Control Inside the Domain
x′
ε = (A −BB⋆Pε)x,
xε(0) = x0,
uε = −B⋆Pεxε.
(4.11)
Moreover (A, Dε) is detectable (because Dε is invertible and D−1
ε
∈L(H)).
Thus by Corollary 4.2, Pε is the unique solution in Σ+(H) of (4.10). Moreover,
by Proposition 4.5, given Q ∈Σ+(H) we have
lim
t→∞PQ(t) = Pεx,
∀x ∈H,
(4.12)
where PQ is the mild solution of the Riccati equation
P ′ = A⋆P + PA −PBB⋆P + C⋆C + εI,
P(0) = Q;
that is, Pε is globally attractive.
Proposition 4.6. Assume that conditions (H)∞are veriﬁed and that (A, B)
is I-stabilizable. Then (4.2) has a maximal solution P ∞
Max. Moreover
lim
ε→0 Pεx = P ∞
Maxx,
∀x ∈H.
(4.13)
Proof. We ﬁrst remark that, as {Pε} is nonincreasing (by Proposition 2.2 in
Part II, Chapter 1) and bounded below by 0, the limit
lim
ε→0 Pεx = P ∞
Maxx,
∀x ∈H,
(4.14)
exists. We have to show that P ∞
Max is the required maximal solution.
Step 1.
P ∞
Max is a solution to(4.2).
In fact for all x, y ∈D(A), we have
(PεAx, y) + (Pεx, Ay) −(B⋆Pεx, B⋆Pεx) + (Cx, Cy) + ε(x, y) = 0;
letting ε tend to 0 and using (4.14) we see that P ∞
Max is a solution of (4.2).
Step 2.
P ∞
Max is maximal.
Let Q be any solution of (4.2); then, by Proposition 2.2 in Part II, Chapter 1,
we have PQ(t) ≥Q (where PQ is the solution to (4.9)). Thus, by (4.12) and
(4.14) it follows that P ∞
Max ≥Q, so that P ∞
Max is maximal.
⊓⊔
We will now give the variational interpretation of the maximal solution.

4 Qualitative properties of the solutions of the Riccati equation
499
Theorem 4.1. Assume (H)∞and that (A, B) is I-stabilizable. Let P ∞
Max be
the maximal solution of (4.2) and let x0 ∈H. Then
(P ∞
Maxx0, x0) = inf{J∞(u): u ∈L2(0, ∞; U), x ∈L2(0, ∞; H)},
(4.15)
where (x, u) are subject to the diﬀerential equation constraint (1.1). Moreover
(P ∞
Maxx0, x0) ≥J∞(ˆu),
(4.16)
where ˆu = limε→0 uε and uε is deﬁned by (4.11).
Proof. Set
Uad = inf{J∞(u): u ∈L2(0, ∞; U), x ∈L2(0, ∞; H)},
where x is the solution of (1.1).
We ﬁrst remark that, recalling (4.13) and arguing as we did in the proof
of Lemma 2.1, we can show that the following limits:
ˆx(t) = lim
ε→0 xε(t),
ˆu(t) = lim
ε→0 uε(t),
t ≥0
(4.17)
exist and are uniform on the bounded subsets of [0, ∞[. Let now u ∈Uad. By
identity (6.2) in Chapter 1 of Part IV, we have
(P ∞
Maxx0, x0) +
 t
0
{|u(s) + B⋆P ∞
Maxx(s)|2} ds
=
 t
0
{|Cx(s)|2 + |u(s)|2} ds +

P ∞
Maxx(t), x(t)

.
As x ∈L2(0, ∞; H), there exists a sequence tn ↗∞such that x(tn) →0.
Therefore
(P ∞
Maxx0, x0) +
 t
0
|u(s) + B⋆P ∞
Maxx(s)|2 ds = J∞(u),
(4.18)
which implies that
(P ∞
Maxx0, x0) ≤J∞(u),
∀u ∈Uad.
Letting ε tend to zero in the equality
(Pεx0, x0) = J∞(uε) + ε∥xε∥2
L2(0,∞;H),
one obtains (4.16); also, as uε ∈Uad, the conclusion follows.
⊓⊔
Proposition 4.7. Under the conditions of Theorem 4.1, the following state-
ments are equivalent.
(i) The operator FM = A −BB⋆P ∞
Max is exponentially stable.

500
V-1 Bounded Control Operators: Control Inside the Domain
(ii) (P ∞
Maxx0, x0) = min{J∞(u): u ∈L2(0, ∞; U), x ∈L2(0, ∞; H)}, where x
is given by (1.1).
Proof. (i) =⇒(ii). Let ˆx(t) and ˆu(t) be deﬁned by (4.17). If FM is exponen-
tially stable, we have ˆx ∈L2(0, ∞; H) so that ˆu ∈Uad and (ii) holds.
(ii) =⇒(i). Let u⋆∈L2(0, ∞; U) such that the corresponding solution x⋆
of (1.1) belongs to L2(0, ∞; H). Setting u = u⋆in (4.18), we obtain
 ∞
0
|u⋆(s) + B⋆P ∞
Maxx⋆(s)|2 ds = 0
which implies that
x⋆(t) = etFM x0,
u⋆(t) = −B⋆P ∞
MaxetFM x0,
and the conclusion follows from Datko’s Theorem.
⊓⊔
Corollary 4.3. Assume that the conditions (H)∞are veriﬁed and that (A, B)
is I-stabilizable. Let P ∞
min and P ∞
Max be the minimal and maximal solutions of
the algebraic Riccati equation. Then
(i) P ∞
min ≤P ∞
Max,
(ii) P ∞
min is globally attractive from below,
(iii) P ∞
Max is globally attractive from above.
Proof. (i) is clear, (ii) follows from Proposition 4.2, and (iii) follows from
Theorem 4.1 and Proposition 4.3.
⊓⊔
Remark 4.2.
(i) The maximal solution does not exist in general. Let in fact
H = U = Y = R2,
A =

0 1
−1 0
	
,
B = 0,
C = 0.
Then (4.2) reduces to A⋆X + XA = 0 and X = λI is a solution for all
λ ≥0.
(ii) In (4.15) the inﬁmum is not a minimum in general. In fact, let A = 0,
U = H, B = I, and C = 0. Then the algebraic Riccati equation reduces
to P 2 = 0 and one has P ∞
min = P ∞
Max = 0. Thus the inﬁmum in (4.15) is
0; however the control u = 0 does not belong to Uad if x0 ̸= 0. Remark
also that FM is not exponentially stable.
⊓⊔
Remark 4.3. For the existence of the maximal solution it is not necessary that
(A, B) be I-stabilizable. It is suﬃcient to assume that there exists D ≥C⋆C
such that
(i) (A, B) is D1/2-stabilizable.
(ii) (A, D1/2) is detectable.
In fact, under these assumptions, one can easily repeat the previous argu-
ments.
⊓⊔

4 Qualitative properties of the solutions of the Riccati equation
501
4.4 Continuous dependence of stationary solutions with respect to
the data
We consider here a sequence of Riccati equations
P ′
k = A⋆
kPk + PkAk −PkBkB⋆
kPk + C⋆
kCk,
A⋆
kXk + XkAk −XkBkB⋆
kXk + C⋆
kCk = 0.
(4.19)
Assume that hypotheses (2.18) in Chapter 1 of Part IV are veriﬁed and set
Jk
∞(u) =
 ∞
0
{|Ckxk(s)|2 + |u(s)|2} ds,
u ∈L2(0, ∞; U),
where xk is the mild solution of the system
x′
k = Akxk + Bku,
xk(0) = xk0.
(4.20)
We say that (Ak, Bk) is stabilizable with respect to Ck uniformly in k, if for
any x0 ∈H, there exists u ∈L2(0, ∞; U) such that
Jk
∞(u) < ∞,
∀k ∈N.
(4.21)
If this assumption is veriﬁed, then, according to Theorem 3.1, the minimal
solution of (2.2) exists. We denote it by P ∞
k,min. Finally we set
Fk = Ak −BkB⋆
kP ∞
k,min.
We now prove the following theorem (see also J. S. Gibson [1]).
Theorem 4.2. Assume that assumptions (2.18) in Chapter 1 of Part IV are
veriﬁed and that (Ak, Bk) is stabilizable with respect to Ck uniformly in k. If,
in addition, there exist N > 0 and a > 0 such that
∥etFk∥≤Ne−at,
t ≥0,
(4.22)
then we have
lim
k→∞P ∞
k,minx = P ∞
minx,
∀x ∈H.
Proof. We have
(P ∞
k,minx0, x0) ≤
 ∞
0
{|xk(s)|2 + |u(s)|2} ds,
where xk is the solution of (4.20). Choose u such that (4.21) holds; then, by
using the Uniform Boundedness Theorem, it is easy to show that there exists
c > 0 such that
P ∞
k,min ≤cI,
∀k ∈N.
(4.23)
Now set Vk = Qk −P ∞
k,min (resp. V = Q −P ∞
k ) where Qk (resp. Q) is the
solution of (4.19) (resp. (4.1)) such that Qk(0) = cI (resp. Q(0) = cI). Then
Vk is the mild solution of the problem

502
V-1 Bounded Control Operators: Control Inside the Domain
V ′
k = F ⋆
k Vk + VkFk −VkBkB⋆
kVk,
Vk(0) = cI −P ∞
k,min.
Remark that Vk(0) ≥0 in virtue of (4.23). By (4.22) it follows that
∥Vk(t)∥≤N 2e−2at,
t ≥0.
Moreover
|P ∞
minx−P ∞
k,minx| ≤|P ∞
minx−Q(t)x|+|Q(t)x−Qk(t)x|+|P ∞
k,minx−Qk(t)x|
≤2cN 2e−2at|x| + |Q(t)x −Qk(t)x|,
and the result follows from Theorem 2.2 of Chapter 1 (Part IV).
⊓⊔
4.5 Periodic solutions of the Riccati equation
In this section we study periodic solutions of Riccati equations. For the ﬁnite
dimensional case, see M. A. Shayman [1]. Here we follow G. Da Prato [2,3].
Let T > 0 be ﬁxed. We say that P ∈Cs

R; Σ(H)

is a T -periodic solution
of the Riccati equation
P ′ = A⋆P + PA −PBB⋆P + C⋆C,
(4.24)
if
(i) P(t + T ) = P(t), ∀t ∈R.
(ii) P is a mild solution of (4.24); that is, for any t, s ∈R with t < s we
have
P(t)x = e(t−s)A⋆P(s)e(t−s)Ax +
 s
t
e(t−τ)A⋆C⋆Ce(t−τ)Ax dτ
−
 s
t
e(t−τ)A⋆P(τ)BB⋆P(τ)e(t−τ)Axdτ.
If, in addition, P is not constant we say that P is a nontrivial periodic solution
of (4.24).
Proposition 4.8. Assume that (H)∞is veriﬁed, then the following state-
ments are equivalent:
(i) (A, B) is C-stabilizable,
(ii) there exists a positive T -periodic solution (possibly trivial) of (4.24).
Proof. (i) =⇒(ii) follows from Proposition 2.3 because any stationary solution
is also periodic.
(ii) =⇒(i). Assume, by contradiction, that there exists a T -periodic solu-
tion of (4.24) whereas (i) does not hold. Then, by Proposition 2.2, there exists
x0 ∈H such that (Pmin(t)x0, x0) →∞as t →∞, where Pmin(·) is the mini-
mal positive solution of (4.24). As P(t) ≥Pmin(t) we have (P(t)x0, x0) →∞
as t →∞, which is a contradiction because P is periodic.
⊓⊔

4 Qualitative properties of the solutions of the Riccati equation
503
Lemma 4.1. Assume that (A, B) is C-stabilizable and let P be a positive T -
periodic solution of (4.24). Then we have P(t) ≥P ∞
min, where P ∞
min is the
minimal solution of (2.2). If, in addition, (A, B) is I-stabilizable we have
P(t) ≤P ∞
Max.
Proof. For the ﬁrst statement, it is suﬃcient to let n tend to inﬁnity in the
inequality
P(t) = P(t + nT ) ≥Pmin(t + nT ),
whereas the second one follows from Corollary 4.3.
⊓⊔
Let now P be a positive T -periodic solution of (4.24) and set Q = P −P ∞
min
and F = A −BB⋆P ∞
min. By Lemma 4.1 we have Q ≥0; moreover, as easily
checked, Q is the mild solution of (4.24) such that Q(0) = P(0) −P ∞
min = Q0;
that is

Q′ = F ⋆Q + QF −QBB⋆Q,
Q(0) = P(0) −P ∞
min = Q0.
(4.25)
Problem (4.25) can be explicitly solved; we could use formula (2.22) in Part IV,
Chapter 1, but we prefer to give a simpler proof in the following lemma.
Lemma 4.2. The solution of problem (4.25) is given by the formula
Q(t) = etF ⋆Q0(I + Ω(t)Q0)−1etF ,
(4.26)
where
Ω(t)x =
 t
0
esF BB⋆esF ⋆x ds,
x ∈H.
Proof. First of all we remark that formula (4.26) is meaningful because Ω(t) ∈
Σ(H) so that the inverse of I + Ω(t)Q0 belongs to L(H) (see Proposition 1.1
in Appendix A). Now let Qn be the solution to the approximating problem
Q′
n = F ⋆
nQn + QnFn −QnBB⋆Qn,
Qn(0) = Q0 + 1
nI,
where Fn are the Yosida approximation of F. Setting Vn = Q−1
n , this problem
reduces to the following linear one:
V ′
n = −FnVn −VnF ⋆
n + BB⋆
Vn(0) =

Q0 + 1
nI
−1
.
Thus we have
Vn(t) = e−tFn

Q0 + 1
nI
−1
e−tF ⋆
n +
 t
0
e(t−s)FnBB⋆e(t−s)F ⋆
n ds
= e−tFn

I +
 t
0
esFnBB⋆esF ⋆
n ds Q0 + 1
nI
 
Q0 + 1
nI
−1
e−tF ⋆
n,

504
V-1 Bounded Control Operators: Control Inside the Domain
which implies that
Qn(t) = etF ⋆
n

Q0 + 1
nI
 
I +
 t
0
esFnBB⋆esF ⋆
n dsQ0 + 1
nI
−1
etFn.
Now the conclusion follows from Theorem 2.1 of Chapter 1 in Part IV.
⊓⊔
From Lemma 4.1 we have the following result.
Proposition 4.9. Assume that the conditions (H)∞are veriﬁed and that
(A, B) is stabilizable with respect to C. Let P0 ≥P ∞
min and let P be the so-
lution of (4.24) such that P(0) = P0. Then P is T -periodic if and only if
X = P0 −P ∞
min veriﬁes the equation
eT F ⋆X(I + Ω(T )X)−1eT F = X,
(4.27)
where
Ω(T )x =
 T
0
esF BB⋆esF ⋆x ds,
x ∈H,
and
F = A −BB⋆P ∞
min.
We will now study a special case of (4.27). We remark that it is also
important to decide whether a given solution of (4.27) is a nontrivial periodic
solution. Assume that
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
(i)
A = D + α
2 I,
C = 0,
α > 0,
(ii)
D + D⋆= 0,
e2πD = I,
∥etD∥≤1,
(iii)
there exists δ > 0 such that
 2π
0
eαsesDBB⋆esD⋆ds ≥δ.
(4.28)
Then (4.27) is equivalent to
e2παX(I + ΩX)−1 = X,
(4.29)
where
Ω= Ω(2π) =
 2π
0
eαsesDBB⋆esD⋆ds
and the Riccati equation (4.24) becomes
P ′ = D⋆P + PD + αP −PBB⋆P.
(4.30)
Proposition 4.10. Assume that the hypotheses of Proposition 4.9 hold and
that, in addition, the conditions (4.28) are veriﬁed. Then all solutions of
(4.29) are given by the formula
X = (e2πα −1)Ω−1/2ΞΩ−1/2,
where Ξ is any hermitian projector operator in Σ+(H).

5 Some generalizations and complements
505
Proof. Equation (4.29) is equivalent to (e2πα −1)X = XΩX and setting
Y = Ω1/2XΩ1/2 to (e2πα −1)Y = Y 2; thus the conclusion follows.
⊓⊔
Let X be a solution of (4.29) and let P be the solution of (4.30) such
that P(0) = X; P is 2π-periodic. We now want to see if P is nontrivial. Let
us consider the special case when B = I. In this case we have X = αΞ and
moreover
P(t) = αeαtetD⋆Ξ[I + (eαt −1)Ξ]−1etD.
As Ξ is a projector we have Ξ(I + bΞ)−1 = (1 + b)−1Ξ for any b > 0, so that
P(t) = αetD⋆ΞetD.
Thus, if Ξ = 0 or I, P(t) is constant, whereas if Ξ is diﬀerent from 0 or I,
P(t) is not trivial.
⊓⊔
Remark 4.4 (Unbounded observation operator). Assume here that (H∞)–(i)–
(ii) and (H∞)–(v) are veriﬁed and consider the optimal control (1.1)–(1.2).
Obviously, in the deﬁnition of the cost functional J∞, Cx must be deﬁned as
in §6 of Chapter 1 of Part IV. The deﬁnitions of optimal pair, stabilizability,
and weak and stationary solutions (but not strict solutions) of the Riccati
equation are now the same as before. However several of the previous results
can be generalized, with the exception of strict solutions of the algebraic
Riccati equation and of the detectability, because it is related to the dual
system, which has an unbounded control operator.
⊓⊔
5 Some generalizations and complements
5.1 Nonhomogeneous state equation
We consider, as in §7 of Chapter 1 in Part IV, a system governed by a non-
homogeneous state equation and the following inﬁnite horizon problem: To
minimize
J∞(u) =
 ∞
0
{|Cx(s)|2 + |u(s)|2} ds,
(5.1)
over all controls u ∈L2(0, ∞; U) subject to the diﬀerential equation constraint
x′(t) = Ax(t) + f(t) + Bu(t),
t ≥0,
x(0) = x0.
(5.2)
We assume that
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
(i)
f ∈L2(0, ∞; H),
(ii)
hypotheses (H) hold,
(iii)
(A, B) is C-stabilizable,
(vi)
F = A −BB⋆P ∞
min is exponentially stable.
(5.3)
Under conditions (5.3) it is easy to check that the following problem:

506
V-1 Bounded Control Operators: Control Inside the Domain
r′(t) + F ⋆r(t) + P ∞
minf(t) = 0,
t ≥0,
r(∞) = 0
has a unique mild solution given by
r(t) =
 ∞
t
e(s−t)F ⋆P ∞
minf(s) ds.
We can now prove the following result.
Lemma 5.1. Assume (5.3) and let x0 ∈H and u ∈L2(0, ∞; U); then we
have
J∞(u) = (P ∞
minx0, x0) + 2(r(0), x0) +
 ∞
0

2

r(s), f(s)

−|B⋆r(s)|2 
ds
+
 ∞
0
|u(s) + B⋆r(s) + B⋆P ∞
minx(s)|2 ds.
Proof. It is suﬃcient to set P = Pmin (the minimal solution of the Riccati
equation) in (7.4) in Chapter 1 of Part IV, and to let T tend to inﬁnity.
⊓⊔
Now the following result is easily proved.
Theorem 5.1. Assume (5.3) and let x0 ∈H. Then there exists a unique op-
timal pair (u⋆, x⋆) for problem (5.1)–(5.2). Moreover the following statements
hold.
(i) x⋆is the mild solution to the closed loop equation

x′(t) = [A −BB⋆P ∞
min]x(t) −BB⋆r(t) + f(t),
t ≥0,
x(0) = x0.
(ii) u⋆is given by the feedback formula
u⋆(t) = −B⋆[P ∞
minx⋆(t) + r(t)].
(iii) The optimal cost is given by
J∞(u) = (P ∞
minx0, x0) + 2(r(0), x0) +
 ∞
0

2

r(s), f(s)

−|B⋆r(s)|2 
ds.
Remark 5.1. If f /∈L2(0, ∞; H) it can happen that no admissible control
exists. Consider in fact the following example. Let H = U = Y = C, A = 0,
B = C = 1, and f = 1. It is readily seen that
J∞(u) =
 ∞
0
t +
 t
0
u(s) ds

2
+ |u(t)|2

dt = ∞.
In this case the following cost function seems more appropriate (see G. Da
Prato and A. Ichikawa [4]):
J∞(u) = lim sup
T →∞
1
T
 T
0
{|Cx(s)|2 + |u(s)|2} ds.
⊓⊔

5 Some generalizations and complements
507
5.2 Time-dependent state equation and cost function
We consider here the system

x′(t) = A(t)x(t) + B(t)u(t),
t ≥0,
x(0) = x0,
(5.4)
where A(t): D

A(t)

⊂H →H, t ≥0 and B(t) ∈L(U; H), t ≥0, are linear
operators. We assume that condition (7.8) in Chapter 1 of Part IV is veriﬁed.
We want to minimize the cost function
J∞(u) =
 ∞
0
{|C(s)x(s)|2 + |u(s)|2} ds
over all controls u ∈L2(0, ∞; U) subject to the diﬀerential equation constraint
(5.4).
The deﬁnitions of admissible controls and optimal pair are the same as
before. Also when, for any x0 ∈H, an admissible control exists, we say that
(A, B) is C-stabilizable (cf. Deﬁnition 2.1). In the autonomous case this prob-
lem was studied with the tool of the minimal positive solution of the algebraic
Riccati equation. Now the same role is played by the minimal positive bounded
solution of the Riccati equation
Q′ + A⋆Q + QA −QBB⋆Q + C⋆C = 0
(5.5)
in [0, +∞[. In the remaining of this subsection we follow G. Da Prato and
A. Ichikawa [4].
Deﬁnition 5.1. We say that Q ∈Cs

[0, +∞[ ; Σ+(H)

is a (mild) solution
to (5.5), if for any a > 0, t ∈[0, a] and x ∈H, we have
Q(t)x = U ∗(a, t)Q(a)U(a, t)x +
 a
t
U ∗(s, t)C∗(s)C(s)U(s, t)x ds
−
 a
t
U ∗(s, t)Q(s)B(s)B∗(s)Q(s)U(s, t)x ds.
⊓⊔
Theorem 5.2. Assume that conditions (7.8) and (7.9) in Chapter 1 of
Part IV are veriﬁed, and that (A, B) is C-stabilizable. Then (5.5) has a non-
negative bounded solution Q. This solution is minimal among all nonnegative
bounded solutions of (5.5).
Proof. For any λ > 0 and n > 0 suﬃciently large, we introduce the Riccati
equations
⎧
⎨
⎩
dQλ
dt
+ A⋆Qλ + QλA −QλBB⋆Qλ + C⋆C = 0,
Qλ(λ) = 0,
t ∈[0, λ],
(5.6)
⎧
⎨
⎩
dQλ
n
dt
+ A⋆
nQλ
n + Qλ
nAn −Qλ
nBB⋆Qλ
n + C⋆C = 0,
Qλ
n(λ) = 0,
t ∈[0, λ],
(5.7)

508
V-1 Bounded Control Operators: Control Inside the Domain
where An are the Yosida approximations of A. By Theorem 7.2 of Chap-
ter 1 of Part IV, problems (5.6) and (5.7) have unique solutions Qλ and Qλ
n,
respectively, and moreover for any x ∈H
lim
n→∞Qλ
n(t)x = Qλ(t)x,
(5.8)
uniformly in t ∈[0, λ]. Also, by Proposition 7.1 of Chapter 1 of Part IV,
Qλ
n(t) ≤Qµ
n(t),
t ∈[0, λ]
if µ > λ.
(5.9)
We now prove that there exists C1 > 0 such that
∥Qλ(t)∥≤C1,
∀λ > 0, ∀t ∈[0, λ].
(5.10)
For this it is suﬃcient to prove that
∥Qλ
n(t)∥≤C1,
∀λ > 0, ∀t ∈[0, λ], ∀n > 0.
(5.11)
Let x ∈H and let u ∈L2(0, ∞; U) such that J∞(u) < +∞. Let moreover xn
be the solution of the initial value problem:
x′
n = Anxn + Bu,
xn(0) = x.
Then we have
d
dt

Qλ
n(t)xn(t), xn(t)

= |u(t) + B⋆(t)Qλ
n(t)xn(t)|2 −|C(t)xn(t)|2 −|u(t)|2.
Integrating this from t and λ, we ﬁnd
⟨Qλ
n(t)x, x⟩≤
 λ
t
{|C(s)xn(s)|2 + |u(s)|2} ds ≤Const |x|2,
and the inequalities (5.10) and (5.11) follow.
From the estimates (5.9) and (5.10) the following limit exists:
Q∞(t)x = lim
λ→∞Qλ
n(t)x,
∀x ∈H.
By letting λ tend to inﬁnity in the equality
Qλ(t)x = U ∗(a, t)Qλ(a)U(a, t)x +
 a
t
U ∗(s, t)C∗(s)C(s)U(s, t)x ds
−
 a
t
U ∗(s, t)Qλ(s)B(s)B∗(s)Qλ(s)U(s, t)x ds,
we see that Q∞is a bounded solution of (5.5). It remains to prove minimality.
Let R be a bounded nonnegative solution of (5.5). Then by Proposition 7.1
of Chapter 1 in Part IV and the fact that Qλ(λ) = 0,
R(t) ≥Qλ(t),
∀t ∈[0, λ],
and so R ≥Q∞.
⊓⊔

5 Some generalizations and complements
509
Corollary 5.1. Assume that the conditions of Theorem 5.2 are veriﬁed and
that, in addition, A, B, and C are τ-periodic functions for some τ > 0. Then
(5.5) has a nonnegative τ-periodic solution Q. This solution is minimal among
all nonnegative τ-periodic solutions of (5.5).
Proof. Let Q∞be the minimal nonnegative bounded solution of the Riccati
equation. Then we have
Qλ(t + τ) = Qλ−τ(t),
∀t ∈[0, λ −τ];
as λ →∞, we ﬁnd that Q∞is τ-periodic.
⊓⊔
Remark 5.2. This result was proved in G. Da Prato and A. Ichikawa [3].
If A, B, C are almost periodic and H is a ﬁnite dimensional space, then Q∞
is almost periodic (see T. Morozan [1]).
⊓⊔
5.3 Periodic control problems
Consider a dynamical system governed by a linear equation
x′(t) = A(t)x(t) + f(t) + B(t)u(t)
(5.12)
with periodic coeﬃcients f(·), A(·), B(·) of period 2π.
Then it is natural to consider 2π-periodic controls u. For any Banach space
E we denote by L2
#(E) the space of all 2π-periodic functions u: R →E that
belong to L2(0, 2π; E).
We assume that
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
(i)
Conditions (7.8) and (7.9) in Chapter 1 of Part IV
hold.
(ii)
A, B, C are 2π periodic.
(iii)
f ∈L2
#(H).
(5.13)
Obviously (5.12) does not necessarily have a 2π-periodic solution for any con-
trol u ∈L2
#(U). This happens when no Floquet exponent of A is equal to
1. In such a case we say that A is nonresonant. As it is well known, this is
equivalent to require that 1 belongs to the resolvent set of UA(2π, 0), where
UA(t, s) is the evolution operator relative to A. In this case (5.12) has a unique
mild solution given by the formula
x(t) = UA(t, 0)[I −UA(2π, 0)]−1
 2π
0
UA(2π, s)[Bu(s) + f(s)] ds
+
 t
0
U(t, s)[Bu(s) + f(s)] ds.

510
V-1 Bounded Control Operators: Control Inside the Domain
We are interested in the general case, that is, when A is possibly resonant.
For any u ∈L2
#(U) we set
Λ(u) = {y ∈L2
#(H): y fulﬁlls (5.12)}.
(5.14)
The control u is said to be admissible if Λ(u) ̸= ∅. The set of all admissible
controls will be denoted by Uad.
We want to minimize the cost function
J(u, y) =
 2π
0
{|C(t)y(t)|2 + |u(t)|2} dt
(5.15)
over all u ∈Uad and y ∈Λ(u).
If there exist u⋆∈Uad and x⋆∈Λ(u⋆) such that
J(u⋆, x⋆) ≤J(u, x),
∀u ∈Uad, ∀x ∈Λ(u)
the function u⋆is called an optimal control and the associated state x⋆is
called the optimal state. The pair (u⋆, x⋆) is called an optimal pair.
We will study this minimization problem by using again the Dynamic
Programming approach and by proceeding in the following steps.
(i) We consider a periodic solution to the Riccati equation
Q′ + A⋆Q + QA −QBB⋆Q + C⋆C = 0.
(5.16)
(ii) We look for a periodic solution to the dual equation
r′(t) + (A −BB⋆Q)⋆r(t) + Qf(t) = 0
(5.17)
and to the closed loop equation
x′(t) = (A −BB⋆Q)x(t) −BB⋆r(t) + f(t).
(5.18)
Then, we show that x is an optimal state and that u, given by
u(t) = −B⋆
Qx(t) + r(t)

,
(5.19)
is an optimal control.
Concerning the point (i), the existence of a nonnegative periodic solution of
(5.16) was proved in Corollary 5.1 when (A, B) is C-stabilizable. The approach
described below was introduced in G. Da Prato and A. Ichikawa [3] where
the existence of an optimal pair was proved with the additional condition
that (A, C) be detectable. In fact in this case one can show that the Floquet
exponents of the closed loop operator F = A −BB⋆Q have all modulus
less than 1 and then F is nonresonant and (5.17) and (5.18) have periodic
solutions.
Here we do not assume detectability, but we give, following G. Da
Prato [3], a characterization of the Floquet exponents of F with modulus

5 Some generalizations and complements
511
greater than or equal to one. In the following theorem UF represents the evo-
lution operator associated with F (UF is well deﬁned because F is a bounded
perturbation of A) and Q∞is the minimal nonnegative periodic solution of
the Riccati equation (5.16).
Theorem 5.3. Assume that condition (5.13) is veriﬁed and that (A, B) is C-
stabilizable. Let Q∞be the minimal nonnegative 2π-periodic solution of Riccati
equation (5.16) and F = A −BB∗Q∞. Let µ ∈C such that |µ| ≥1 and let
x0 ∈H be diﬀerent from 0. Then the following statements are equivalent.
(i) UF (2π, 0)x0 = µx0.
(ii) UA(2π, 0)x0 = µx0 and C(t)UA(t, 0)x0 = 0, for all t ≥0.
Moreover if either (i) or (ii) holds true, we have
UF (t, 0)x0 = UA(t, 0)x0,
∀t ≥0.
(5.20)
Proof. (i) =⇒(ii). Let x0 ∈H and let x be the mild solution of the problem
x′ = Fx,
t ≥0,
x(0) = x0,
then the following identity holds:

Q∞(t)x(t), x(t)

−(Q∞(0)x0, x0)
+
 t
0
{|B∗(s)Q∞(s)x(s)|2 + |C(s)x(s)2| ds} = 0.
(5.21)
If x0 ∈D(A) this follows easily by integrating the identity
d
dt

Q∞(t)x(t), x(t)

= −|B⋆(t)Q∞(t)x(t)|2 −|C(t)x(t)|2
between 0 and t. For general x0, (5.21) follows by density.
Let now x0 ∈H be such that UF (2π, 0)x0 = x(2π) = µx0 with |µ| ≥1.
Then, setting t = 2π in (5.21) we ﬁnd
(|µ|2 −1)(Q∞(0)x0, x0) +
 2π
0
{|B∗Q∞x|2 + |Cx2| ds} = 0,
which yields
B⋆(t)Q∞(t)x(t) = 0, C(t)x(t) = 0,
∀t ≥0.
Consequently x′ = Ax, and
x(t) = UF (t, 0)x0 = UA(t, 0)x0,
t ≥0,
which implies (ii).

512
V-1 Bounded Control Operators: Control Inside the Domain
(ii) =⇒(i). Given x0 ∈H, let y be the mild solution of the system y′ = Ay,
y(0) = x0. Moreover let Qλ be the mild solution of the Riccati equation
Q′ + A∗Q + QA −QBB∗Q + C∗C = 0,
Q(2πλ) = 0.
Then the following identity holds:
(Qλ(0)x0, x0) +
 2πλ
0
|B∗(s)Qλ(s)x(s)|2 ds =
 2πλ
0
|C(s)x(s)2| ds.
(5.22)
In fact if x0 ∈D(A) this result readily follows by integrating the identity
d
dt

Qλ(t)y(t), y(t)

= |B⋆(t)Qλ(t)y(t)|2 −|C(t)y(t)|2
between 0 and 2πλ. For general x0, (5.22) follows by density.
Let now x0 be such that
y(2π) = UA(2π, 0)x0 = µx0,
C(t)y(t) = 0,
t ≥0.
Then, by letting λ tend to inﬁnity in (5.22) we ﬁnd
(Q∞(0)x0, x0) +
 ∞
0
|B∗(t)Q∞(t)x(t)|2 dt = 0,
which implies B∗(t)Q∞(t) = 0, t ≥0, so that y′ = Fy and UA(t, 0)x0 =
UF (t, 0)x0 and (i) holds.
⊓⊔
Remark 5.3. Assume that the spectra of UA(2π, 0) and UF (2π, 0) consist only
of eigenvalues (this is for instance the case for parabolic state equations in
bounded domains; see Example 6.1 below). By the above theorem it follows
that F is nonresonant if one of the following conditions holds
(i) A is nonresonant.
(ii) A is resonant but the following implication holds:
x0 ∈H, x0 ̸= 0,
UA(2π, 0)x0 = x0 =⇒C(t0)UA(t0, 0)x0 ̸= 0,
for at least one t0 ≥0.
For a case where the eigenvalues of UA(2π, 0) and UF (2π, 0) have limit
points, see G. Da Prato [7].
⊓⊔
We can prove now the result.
Theorem 5.4. Assume that condition (5.13) is veriﬁed, that A, B is C-
stabilizable, and that F = A −BB∗Q∞is nonresonant, where Q∞is the
minimal nonnegative 2π-periodic solution of the Riccati equation (5.16). Then
there exists a unique optimal pair (u⋆, x⋆) for problem (5.12)–(5.15) and the
following conditions are satisﬁed:

6 Examples of controlled systems
513
(i) x⋆is the unique periodic solution to the closed loop equation (5.18),
(ii) u⋆is given by the feedback formula
u⋆(t) = −B⋆[Q∞(t)x⋆(t) + r⋆(t)],
(5.23)
where r is the unique periodic solution of (5.17),
(iii) the optimal cost J(u⋆, x⋆) is given by
J(u⋆, x⋆) =
 2π
0

r(t), f(t)

−|B⋆(t)r(t)|2 
dt.
(5.24)
Proof. Let u ∈Uad and y ∈Λ(u). By computing
d
dt

Q∞(t)y(t), y(t)

+ 2

r(t), f(t)
 
and by integrating between 0 and 2π, we ﬁnd the identity
J(u, y) = J⋆+
 2π
0
∥R(t)∥2dt,
(5.25)
where
J⋆=
 2π
0

r(t), f(t)

−|B⋆(t)r(t)|2 
dt
and
R(t) = B∗(t)[Q∞(t)y(t + r(t)] + u(t).
We remark that the computation can be made rigorous by approximating A(t)
by their Yosida approximations. It follows that
J(u⋆, y⋆) ≥J⋆,
∀u ∈Uad, ∀y ∈Λ(u).
Now, let x∗be the solution of (5.18) and let u∗be given by (5.23). Setting in
(5.25), u = u⋆and y = y⋆we obtain J(u⋆, y⋆) = J⋆so that the pair u⋆, y⋆is
optimal. Finally uniqueness of the optimal pair is proved as in Theorem 6.1
of Chapter 1 in Part IV.
⊓⊔
Remark 5.4. It is also possible to study almost periodic control problems (see
G. Da Prato and A. Ichikawa [2]).
⊓⊔
6 Examples of controlled systems
6.1 Parabolic equations
We shall continue here the example of §8.1 in Chapter 1 of Part IV. Moreover
we denote by {ϕk} a complete set of eigenvectors of A and by {λk} the

514
V-1 Bounded Control Operators: Control Inside the Domain
corresponding sequence of eigenvalues. We assume that {λk} is nonincreasing.
We consider the inﬁnite horizon control problem: To minimize
J∞(u) =
 ∞
0

Ω

Cx(t, ·)

(ξ)
2 + |u(t, ξ)|2 
dt dξ,
(6.1)
over all u ∈L2([0, ∞]×Ω) subject to condition (1.48) of Chapter 1 in Part II.
We have to discuss the existence of admissible controls. We consider two
cases.
First case. λ1 < 0.
In this case A is exponentially stable so that (A, B) is C-stabilizable. Thus
by Theorem 3.1 and Corollary 4.2, the algebraic Riccati equation
AP + PA −PBB⋆P + C⋆C = 0,
(6.2)
has a unique solution P ∞
min = P ∞
Max and the feedback operator F = A −
BB⋆P ∞
min is exponentially stable.
Second case. λ1 ≥0, λ2 < 0.
In this case we have σ+(A⋆)∪σ0(A⋆) = {λ1}; thus, by Proposition 3.3, (A, B)
is I-stabilizable if and only if B⋆ϕ1 ̸= 0. Under this assumption (6.2) has a
minimal and a maximal nonnegative solution. Moreover by Proposition 3.4,
F is exponentially stable if and only if Cϕ1 is not identically zero. It is easy
to generalize the previous discussion when λm ≥0 and λm+1 < 0 for some
m ∈N.
6.2 Wave equation
We continue here the example of §8.2 in Chapter 1 of Part IV. Consider the
inﬁnite horizon problem: To minimize
J∞(u) =
 ∞
0

Ω
∇ξx

t, ξ)
2 +

∂
∂t(t, ξ)

2
+ |u(t, ξ)∥2

dt dξ,
(6.3)
over all u ∈L2([0, ∞] × Ω) subject to (8.2) in Chapter 1 of Part IV. In this
case the algebraic Riccati equation reads as follows:

0 −1
Λ
0
	
X + X

0 1
−Λ 0
	
−X

0 0
0 1
	
X +

1 0
0 1
	
= 0.
We prove now that (A, B) is I-stabilizable; for this it is suﬃcient to show that
A −2αBB⋆is exponentially stable if 0 < α2 < λ0 where λ0 is the principal
eigenvalue of the Laplace operator in Ω, with Dirichlet boundary conditions.
By a direct computation we ﬁnd

6 Examples of controlled systems
515
et(A−2αBB⋆) = e−αt
⎡
⎢⎣
cos(Et) + α
E sin(Et)
1
E sin(Et)
α2 + E2
E
sin(Et)
−α
E sin(Et) + cos(Et)
⎤
⎥⎦,
where E =
√
Λ −α2I. As (A, C) = (A, I) is detectable, the Riccati equation
has a unique nonnegative solution
P ∞
min = P ∞
Max =
P11 P12
P21 P22
	
.
Then, by Theorem 3.1, there exists a unique optimal pair (u⋆, x⋆) with
u⋆(t, ξ) = −

P21x⋆(t, ·)

(ξ) −

P22
∂x⋆
∂t

(t, ·)(ξ).
Remark 6.1. Similar consideration apply to the wave equation with Neumann
boundary conditions. See Remark 8.1 in Chapter 1 of Part IV.
⊓⊔
Remark 6.2. For a situation in which there exist periodic nontrivial solutions
(see G. Da Prato [6]).
⊓⊔
6.3 Strongly damped wave equation
Let Ωbe an open bounded set of Rn with regular boundary ∂Ω. Consider the
equation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
∂2x
∂t2 x(t, ξ) = ∆ξ(t, ξ) + ρ∆ξ
∂x
∂t (t, ξ)
+

Bu(t, ·)

(ξ)
in ]0, T ] × Ω,
x(t, ξ) = 0
on ]0, T ] × ∂Ω,
x(0, ξ) = x0(ξ),
∂x
∂t (0, ξ) = x1(ξ)
in Ω,
(6.4)
where ν is the outward normal to ∂Ωand ρ is a given positive number.
As in §8.2 of Chapter 1 of Part IV, we choose the spaces H, Y , U as
in (8.3), the scalar product in H as in (8.4), and consider the positive self-
adjoint operator Λ deﬁned by (8.5). As Ωis bounded there exists a complete
orthonormal system in L2(Ω), {ek}, and a sequence of real numbers
0 < µ1 ≤µ2 ≤µ2 ≤· · · →+∞
such that
Λek = µkek,
k = 1, 2, . . ..
Deﬁne the linear operator A1 on H

516
V-1 Bounded Control Operators: Control Inside the Domain
⎧
⎪
⎪
⎨
⎪
⎪
⎩
A1X =

0
1
−Λ −ρΛ
 
x0
x1

,
∀X ∈D(A1),
D(A1) = H2(Ω) ∩H1
0(Ω) ⊕H1
0(Ω).
(6.5)
It is an easy exercise to prove that the spectrum of A1 is given by
σ(A1) = {−1/ρ} ∪
#
−1
2(σµk ±
8
(ρ2µ2
k −4µk): k = 1, 2, . . .
$
and the resolvent by
R(λ, A1) =

λ + ρΛ 1
−Λ
λ
	
[λ2 + ρΛλ + Λ]−1,
∀λ ∈σ(A1).
From the above formula, it is not diﬃcult to prove that A1 is the inﬁnitesimal
generator of an analytic semigroup in H.
Now set
Bu =
0
u
	
,
u ∈U.
Condition (ii) of Proposition 3.3 (Hautus condition) is fulﬁlled because A1 only
has a spectrum with strictly negative real parts, so that (A1, B) is stabilizable
with respect to the observation I.
We can now consider the cost functional (6.3) and solve the corresponding
minimization problem.
Remark 6.3. The strongly damped wave equation with time-dependent peri-
odic coeﬃcients has been studied in G. Da Prato and A. Lunardi [1].
⊓⊔

2
Unbounded Control Operators: Parabolic
Equations With Control on the Boundary
1 Introduction and setting of the problem
As in Chapter 2 of Part IV we consider a dynamical system governed by the
following equation:

x′(t) = Ax(t) + (λ0 −A)Du(t),
t ≥0,
x(0) = x0,
or equivalently
x(t) = etAx0 + (λ0 −A)
 t
0
Du(s) ds,
(1.1)
where x0 ∈H and u ∈L2(0, ∞; U). We assume that
(HP)∞
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
(i)
A generates an analytic semigroup etA of type ω0
and λ0 is a real number in ρ(A) such that ω0 < λ0,
(ii)
∃α ∈]0, 1[ such that D ∈L

U; D([λ0 −A]α)

,
(iii)
C ∈L(H; Y ).
Clearly, if hypotheses (HP)∞hold, then the hypotheses (HP) of Chapter 2
of Part IV are fulﬁlled with P0 = 0. If α ≤1/2, we will choose once and for
all a number β belonging to ]1 −α/2, 1 −α/2[. We want to minimize the cost
function:
J∞(u) =
 ∞
0
{|Cx(s)|2 + |u(s)|2} ds
(1.2)
over all controls u ∈L2(0, ∞; U) subject to the diﬀerential equation constraint
(1.1). We say that the control u ∈L2(0, ∞; U) is admissible if J∞(u) < ∞.
The deﬁnitions of optimal control, optimal state, and optimal pair are the
same as in Chapter 1. When, for any x0 ∈H, an admissible control exists, we
say that (A, AD) is C-stabilizable.

518
V-2 Unbounded Control Operators in Parabolic Equations
In this chapter, we want to generalize the results of Chapter 1. Several
of these generalizations are straightforward and the corresponding proofs will
only be sketched. We will start by proving the existence of a minimal solution
P ∞
min of the algebraic Riccati equation
A⋆X + XA −Y ⋆EE⋆Y + C⋆C = 0,
Y = [λ0 −A⋆]1−αX,
(1.3)
where E = (λ0 −A)αD, under the hypothesis that (A, AD) is C-stabilizable.
We remark that in order that (1.3) be meaningful, we need a regularity prop-
erty of X, namely
X ∈Σ+(H) ∩L

H; D

[λ0 −A⋆]1−α
.
We shall set V ∞
min = [λ0 −A⋆]1−αP ∞
min. The above requirement will make the
proof of existence much more involved than in Chapter 1 (see §2 below). Once
the existence is proved, one can show, quite easily (see §3), that the optimal
control u⋆exists and is given by the feedback formula
u⋆(t) = −E⋆V ∞
minx⋆(t),
where x⋆is the solution to the closed loop equation
x(t) = etAx0 +
 t
0
[λ0 −A]1−αe(t−s)AEE⋆V ∞
minx(s) ds.
(1.4)
Another new diﬃculty arises in the study of the closed loop operator F
(see §3), formally deﬁned by
F = A −EE⋆V ∞
min.
In §3 we prove a characterization of F that enables us to generalize all results
concerning detectability and Hautus conditions of Chapter 1. Also we leave
the reader to extend all results on the qualitative behavior of the solutions of
Riccati equations proved in Chapter 1 .
2 The algebraic Riccati equation
We assume here that the assumptions (HP)∞are veriﬁed and consider system
(1.1) and the Riccati equation
P ′ = A⋆P + PA −V ⋆EE⋆V + C⋆C,
(2.1)
along with the algebraic equation (1.3). We say that
P ∈

Cs,α

[0, ∞[ ; Σ+(H)

,
if α > 1/2,
Cs,α,β([0, ∞[ ; Σ+(H)),
if α ≤1/2

2 The algebraic Riccati equation
519
is a mild solution of (2.1), if
P(t)x = etA⋆P(0)etAx +
 t
0
e(t−s)A⋆C⋆Ce(t−s)Ax ds
+
 t
0
e(t−s)A⋆V ⋆(s)EE⋆V (s)e(t−s)Ax ds.
(2.2)
Moreover, X is said to be a solution of (1.3) if
(i) X ∈Σ+(H) ∩L

H; D([λ0 −A⋆]1−α)

,
(ii) (Xx, Ay) + (Ax, Xy) −(E⋆Y x, E⋆Y y) + (Cx, Cy) = 0
for all x, y ∈D(A), where Y = [λ0 −A⋆]1−αX.
As in the previous chapter, we shall denote by Pmin the minimal nonnega-
tive solution of Riccati equation (2.1); that is, the solution of (2.1) such that
P(0) = 0 (see Propositions 2.3 and 2.4 of Chapter 2 of Part IV).
We start by proving existence in the easier case α > 1/2.
Proposition 2.1. Assume (HP)∞with α > 1/2 and that (A, AD) is C-
stabilizable. Then there exists
P ∞
min ∈Σ+(H) ∩L

H; D([λ0 −A⋆]1−α)

such that
(i) for each x ∈H the following limit exists:
lim
t→∞Pmin(t)x = P ∞
minx,
(2.3)
(ii) P ∞
min ∈Σ+(H) ∩L

H; D([λ0 −A⋆]1−α)

,
(iii) P ∞
min is a solution of (1.3).
Proof. As (A, AD) is C-stabilizable, we can prove, by repeating the proof of
Proposition 1.2.3, that there exists K > 0 such that
∥Pmin(t)∥≤K,
t ≥0.
(2.4)
Now, arguing as we did in the proof of Proposition 2.2 in Chapter 1, we see
that there exists P ∞
min such that (2.3) holds true. Let Q and Qn, n = 0, 1, . . . ,
be the mild solutions (granted by Theorem 2.1 in Chapter 2 of Part IV of the
Riccati equations

Q′ = A⋆Q + QA −V ⋆
QEE⋆VQ + C⋆C,
Q(0) = P ∞
min,
(2.5)
and

Q′
n = A⋆Qn + QnA −V ⋆
Q,nEE⋆VQ,n + C⋆C,
Qn(0) = Pmin(n),
(2.6)

520
V-2 Unbounded Control Operators in Parabolic Equations
where VQ = [λ0 −A⋆]1−αQ and VQ,n = [λ0 −A⋆]1−αQn. We clearly have
Qn(t) = Pmin(t + n),
t > 0.
Moreover, by Proposition 2.2 in Chapter 2 of Part IV, because
Qn(0)x →Q(0)x,
∀x ∈H,
we have
Q(t)x = lim
n→∞Qn(t)x = lim
n→∞Pmin(t + n)x = P ∞
minx,
∀t > 0, x ∈H.
Thus Q(t) is constant and coincides with P ∞
min. As, by Theorem 2.1 in Chap-
ter 2 of Part IV, Q ∈Cs,α

[0, ∞[ ; Σ+(H)

, we see that (ii) holds true.
Finally, (iii) follows from Proposition 2.1 in Chapter 2 of Part IV, because if
x, y ∈D(A), we have
0= d
dt(Q(t)x, y)=(P ∞
minx, Ay)+(P ∞
minAx, y)+(Cx, Cy)−(E⋆V ∞
minx, E⋆V ∞
miny).
The proof is complete.
⊓⊔
Next, we consider the case α ≤1
2. We ﬁrst recall that, by Theorem 2.2 in
Chapter 2 of Part IV, for any x ∈D([λ0 −A]β) we have Pmin(t)[λ0 −A]βx ∈
D([λ0 −A⋆]β), and the linear operator [λ0 −A⋆]βPmin(t)[λ0 −A]β is closable.
We denote by

Pmin(t)

β its closure.
Proposition 2.2. Assume (HP)∞with α ≤
1
2 and that (A, AD) is C-
stabilizable. Then there exists
P ∞
min ∈Σ+(H) ∩L

H; D([λ0 −A⋆]1−α)

such that for all x ∈H
(i) the limits
lim
t→∞Pmin(t)x = P ∞
minx,
∀x ∈H,
(2.7)
lim
t→∞

Pmin(t)

βx = R∞
minx,
∀x ∈H,
(2.8)
exist, where R∞
min = (P ∞
min)β and
(P ∞
min)β = closure of [λ0 −A⋆]βP ∞
min[λ0 −A⋆]β,
(ii) P ∞
min ∈Σ+(H) ∩L

H; D([λ0 −A⋆]1−α)

,
(iii) P ∞
min is a solution of (1.3).
Proof. By proceeding as in the proof of Proposition 2.1, we can prove (2.7)
and that there exists a constant L1 such that

3 Dynamic programming
521
!!
Pmin(t)

β
!! ≤L1,
t ≥0.
(2.9)
We follows here an argument of F. Flandoli [7]; as the proof of (2.9) is easier
for λ0 < 0, we introduce a shifting L = A −λ0I of A. Clearly L is of negative
type and, as easily checked, P(t) = Pmin(t) is the solution to Riccati equation

P ′ = L⋆P + PL + ∆2 −V ⋆EE⋆V,
P(0) = 0,
(2.10)
where
∆2 = C⋆C + (2λ0 + 2)P.
Now, if x ∈D([λ0 −A]β) = D([I −L]β), we have
(P(t)(λ0 −A)βx, (λ0 −A)βx) = (P(t)(I −L)βx, (I −L)βx)
≤
 t
0
|∆(I −L)βe(t−s)Lx|2ds
≤L2[∥C∥2 + 2K(λ0 + 1]
 ∞
0
e−λ0ss−2β ds |x|2,
which implies (2.9). As the family

Pmin(t)

β
 
is bounded and nondecreasing
in β, (2.8) follows.
We now prove (ii). Let Q and Qn be the mild solutions (established in
Theorem 2.2 in Chapter 2 of Part IV of the Riccati equations (2.5) and (2.6)).
We clearly have
Qn(t) = Pmin(t + n),
t > 0.
Moreover, by Proposition 2.2 in Chapter 2 of Part IV, because Qn(0)x →
Q(0)x, ∀x ∈H, we have
Q(t) = lim
n→∞Qn(t)x
= lim
n→∞Pmin(t + n)x = P ∞
minx,
∀t > 0, x ∈H.
Thus Q(t) is constant and coincides with P ∞
min. As, by Theorem 2.2, in Chap-
ter 2 of Part IV, Q ∈Cs,α,β

[0, ∞[ ; Σ+(H)

, we see that (ii) holds true.
Finally, the proof of (iii) is similar to that of the previous proposition. So it
will be omitted. The proof is complete.
⊓⊔
3 Dynamic programming
3.1 Existence and uniqueness of the optimal control
In this section, we consider the control problem (1.1)–(1.2). We assume (HP)∞
and that (A, AD) is C-stabilizable, and denote by

522
V-2 Unbounded Control Operators in Parabolic Equations
P ∞
min ∈Σ+(H) ∩L

H; D([λ0 −A⋆]1−α)

the minimal solution of the algebraic equation (1.3) and we set V ∞
min = [λ0 −
A⋆]1−αP ∞
min.
We ﬁrst recall that, in virtue of (3.1) in Chapter 1, we have
(Pmin(t)x0, x0) +
 t
0
|u(s) + E⋆Vmin(t −s)x(s)|2 ds
=
 t
0
{|Cx(s)|2 + |u(s)|2} ds,
(3.1)
for any x0 ∈H and any u ∈L2(0, ∞; U), where x is the solution of (1.1). Now
we study the closed loop equation (1.4) which is meaningful because P ∞
min ∈
Σ+(H) ∩L

H; D([λ0 −A⋆]1−α)

and ∥[λ0 −A]1−αet(A−λ0)∥≤L(t −s)α−1.
Proposition 3.1. There exists a unique solution x ∈C([0, ∞[ ; H) of (1.4).
Proof. For any x ∈C([0, ∞[; H) we set

λ(y)

(t) =
 t
0
[λ0 −A]1−αe(t−s)AEE⋆V ∞
minx(s) ds,
for all t ∈[0, T ]. Then we have
∥(λ(y))(t)∥≤L2∥V ∞
min∥
 t
0
(t −s)α−1 ds ∥y∥C([0,t];H)
≤
L2
2α −1∥V ∞
min∥tα∥y∥C([0,t];H),
so that λ ∈L

C([0, t]; H)

for any t ∈]0, T ] and
∥λ∥L(C([0,t];H)) ≤
L2
2α −1∥V ∞
min∥tα.
Thus, if t is small, λ is a contraction and (1.4) has a unique solution in
C([0, t]; H). Now this argument can be repeated in the interval [t, 2t] and so
on, giving the conclusion.
⊓⊔
We prove now the main result of this section.
Theorem 3.1. Assume that assumption (HP)∞is veriﬁed and that (A, B) is
C-stabilizable. Then there exists a unique optimal pair (u⋆, x⋆) for the optimal
control problem (1.1)–(1.2). Moreover the following statements hold:
(i) x⋆∈C([0, ∞[ ; H) is the mild solution to the closed loop equation (1.4).
(ii) u⋆∈C([0, ∞[ ; U) is given by the feedback formula
u⋆(t) = −E⋆V ∞
minx⋆(t),
(3.2)
where P ∞
min represents the minimal solution of the algebraic Riccati equa-
tion (1.3).

3 Dynamic programming
523
(iii) the optimal cost J∞(u⋆) is given by
J∞(u⋆) = (P ∞
minx0, x0).
(3.3)
Proof. In view of Proposition 3.1, the proof is similar to that of Theorem 3.1
in Chapter 1. So it will be omitted.
⊓⊔
3.2 Feedback operator and detectability
Assume that the hypotheses of Theorem 3.1 hold and let (u⋆, x⋆) be the
optimal pair for problem (1.1)–(1.2). We want here to construct, following
G. Da Prato and A. Ichikawa [1], a closed loop operator F for the system
(3.1); that is a linear operator, inﬁnitesimal generator of a strongly continuous
semigroup etF , such that
x⋆(t) = etF x0,
t ≥0.
(3.4)
Proposition 3.2. Assume (HP∞) and that (A, B) is C-stabilizable. For any
x0 ∈H set
S(t)x = x⋆(t),
where x⋆(t) is the optimal state corresponding to x0. Then S(·) is an analytic
semigroup in H and its inﬁnitesimal generator F is given by
Fx = (λ0 −A)(AR(λ0, A)x + DE⋆V ∞
minx),
D(F) = {x ∈H : AR(λ0, A)x + DE⋆V ∞
minx ∈D(A)}.
(3.5)
Finally, if the resolvent of A is compact, so is the resolvent of F.
Proof. We ﬁrst remark that for any x0 ∈H, x(t) = S(t)x0 is precisely
the solution of the closed loop equation (1.4). Thus, x is continuous, by
Proposition 3.1, and S(t) is a strongly continuous semigroup in H. Let
F : D(F) ⊂H →H be its inﬁnitesimal generator; we want to prove that
F is given by (3.5). To this aim, let x0 ∈D(F), x(t) = S(t)x0. We have
R(λ0, A)x(t) = etAR(λ0, A)x0 +
 t
0
e(t−s)ADE⋆V ∞
minx(s) ds.
As x(t) is continuously diﬀerentiable, we have
R(λ0, A)x′(t) = AR(λ0, A)x(t) + DE⋆V ∞
minx(t).
Setting t = 0, it follows that
R(λ0, A)Fx0 = AR(λ0, A)x0 + DE⋆V ∞
minx0,
(3.6)
so AR(λ0, A)x0 + DE⋆V ∞
minx0 ∈D(A) and

524
V-2 Unbounded Control Operators in Parabolic Equations
Fx0 = (λ0 −A)(AR(λ0, A)x0 + DE⋆V ∞
minx0).
(3.7)
Conversely assume that
AR(λ0, A)x0 + DE⋆V ∞
minx0 ∈D(A).
(3.8)
Then, by (3.6) and the density of D(F), the linear operator R(λ0, A)F is
closable and its closure N = R(λ0, A)F is bounded. Now, if (3.8) holds, we
have Nx0 ∈D(A) and it is not diﬃcult to show that this implies x0 ∈D(F)
and (λ0 −A)Nx0 = Fx0.
It remains to show that etF is an analytic semigroup. Let
Sω,θ0 = {λ ∈C: | arg(λ −ω)| ≤θ0}
be a sector contained in the resolvent set ρ(A) and assume that
∥R(λ, A)∥≤M(θ)
|λ −ω|,
λ ∈Sω,θ0,
(3.9)
for some constants ω and M(θ) = M(−θ) (see §2.7 of Chapter 1 of Part II).
Let λ ∈Sω,θ0, η ∈H; consider the equation λξ −Fξ = η, which is equivalent
to
λξ −[λ0 −A]{AR(λ0, A)ξ + DE⋆V ∞
minξ} = η.
(3.10)
Multiplying (3.10) by R(λ, A), gives
ξ −[λ0 −A]1−αR(λ, A)EE⋆V ∞
minξ] = R(λ, A)η.
(3.11)
As [λ0 −A]αD is bounded and ∥[λ0 −A]1−αR(λ, A)∥≤const. |λ|−α, there
exists ρ > 0 such that, for any λ ∈{µ ∈Sω,θ0 : |µ| ≥ρ}, (3.11) has a unique
solution ξ = R(λ, F)η, where
R(λ, F) = {I −[λ0 −A]1−αR(λ, A)([λ0 −A]αD)DE⋆V ∞
min}−1R(λ, A). (3.12)
From (3.9) and (3.12) it follows that the semigroup generated by F is analytic
and that if R(λ, A) is compact so is R(λ, F).
⊓⊔
The operator F is said to be the closed loop operator for the optimal
control problem (1.1)–(1.2). Remark that, by (3.3), we know that the function
Cx⋆(t) = CetF x0 belongs to L2(0, ∞; H). Thus, when C = I (or if C−1 ∈
L(H)), by Datko’s Theorem, it follows that F is exponentially stable. Thus,
in this case, (A, B) is feedback stabilizable.
We give now a generalization of Proposition 3.4 of Chapter 1.
Proposition 3.3. Assume that (A, C) is detectable. Then F is exponentially
stable.
Proof. Let x0 ∈H, and let (u⋆, x⋆) be the optimal pair corresponding to x0.
By Remark 3.2 in Chapter 1, there exists K ∈L(Y ; H) such that A −KC is
exponentially stable. Now it is easy to check that

3 Dynamic programming
525
x∗(t) = et(A−KC)x0 +
 t
0
(λ0 −A)e(t−s)(A−KC)Du(s) ds
+
 t
0
e(t−s)(A−KC)KCx(s) ds.
As Cx⋆, u⋆∈L2(0, ∞; U) and A −KC is exponentially stable it follows that
x⋆∈L2(0, ∞; H). Datko’s Theorem yields the conclusion.
⊓⊔
3.3 Stabilizability and stability of F in the point spectrum case
We consider here the system (1.1) under hypothesis (HP∞) and we assume
that A veriﬁes the point spectrum hypotheses (P) (see §3.2 in Chapter 1). We
want to give a necessary and suﬃcient condition in order that (A, AD) be
stabilizable with respect to the identity I. We need a lemma whose proof is
similar to that of Lemma 3.1 (Chapter 1).
Lemma 3.1. Assume (HP∞) and (P) with σ0(A) = ∅. Then the following
statements are equivalent:
(i) For all x ∈H, there exists u ∈L2(0, ∞; U) such that the solution x of
(1.1) belongs to L2(0, ∞; H).
(ii) The mapping
u 	→γu = (λ0 −A)
 ∞
0
e−sAΠ+
ADu(s) ds: L2(0, ∞; U) →H
is onto.
(iii) The mapping
γ⋆: H →L2(0, ∞; U),
(γ⋆ξ)(s) = D⋆[λ0 −A⋆](Π+
A )⋆e−sA⋆ξ,
ξ ∈H
is one-to-one.
(iv) Ker

D⋆(λ −A⋆)

∩Ker(λ −A⋆) = {0}, for all λ ∈σ+(A⋆).
Proof. It is completely similar to the proof of Lemma 3.1 in Chapter 1.
⊓⊔
We now generalize Propositions 3.3, 3.4, and 3.5 of Chapter 1.
Proposition 3.4. Assume (HP)∞and (P). Then the following statements
are equivalent:
(i) (A, AD) is stabilizable with respect to I.
(ii) For any λ ∈σ0(A⋆) ∪σ+(A⋆),
Ker

D⋆(λ −A⋆) ∩Ker(λ −A⋆)

= {0}.
Proposition 3.5. Assume that (A, AD) is C-stabilizable and that A and F
fulﬁll (P). Then the following statements are equivalent:

526
V-2 Unbounded Control Operators in Parabolic Equations
(i) F is exponentially stable.
(ii) For any λ ∈σ+(A) ∪σ0(A), we have
Ker(A −λ) ∩Ker(C) = {0}.
(3.13)
Proposition 3.6. Assume that the hypotheses of Proposition 3.5 hold. Then
the following statements are equivalent:
(i) F is exponentially stable.
(ii) (A, C) is detectable.
Example 3.1. Let Ωbe an open bounded set of Rn with regular boundary ∂Ω.
Consider the state equation
⎧
⎪
⎪
⎨
⎪
⎪
⎩
∂x
∂t (t, ξ) = (∆ξ + c)x(t, ξ)
in ]0, T [ × Ω,
x(t, ξ) = u(t, ξ)
on ]0, T [ × ∂Ω,
x(0, ξ) = x0(ξ)
in Ω,
(3.14)
where c > 0, x0 ∈L2(Ω), and u ∈L2(∂Ω). We choose H = Y = L2(Ω) as
space of states and observations and U = L2(∂Ω) as space of controls. We
denote by A the linear self-adjoint operator in H:

Ax = ∆ξx + cx,
∀x ∈D(A),
D(A) = H2(Ω) ∩H1
0(Ω).
We consider the following problem: To minimize
J(u) =
 ∞
0

Ω
|x(t, ξ)|2 dt dξ +
 ∞
0

∂Ω
|u(t, ξ)|2 dt dξ.
Let D: L2(∂Ω) →L2(Ω) the Dirichlet mapping introduced in Example 1.1 of
Chapter 2 in Part IV; then problem (3.14) can be written as
x(t) = etAx0 + (cI −A)
 t
0
e(t−s)ADu(s) ds,
and hypotheses (HP∞) hold with λ0 = c. We denote by {ϕk} a complete set
of eigenvectors of A and by {−λk} the corresponding sequence of eigenvalues.
We assume that {λk} is a nondecreasing sequence and that
0 < λ1 < c,
λk > c,
k = 2, 3, . . .,
and we prove that (A, AD) is stabilizable. Recalling Proposition 3.4 it is
enough to observe that
D⋆(A⋆−c)ϕ1 = ∂ϕ1
∂ν
is not identically zero; this in fact is a consequence of a result that can be
found in E. J. P. G. Schmidt and N. Weck [1].
⊓⊔

3 Dynamic programming
527
Remark 3.1. Stabilizability for parabolic boundary control problems was stud-
ied by several authors, see T. Nanbu [1], I. Lasiecka and R. Triggiani [4,
5], and H. Amann [1].
These results have been generalized by A. Lunardi [2, 4] for non-
autonomous state equations with periodic coeﬃcients (see also P. K. Med-
ina [1]).
⊓⊔

3
Unbounded Control Operators: Hyperbolic
Equations With Control on the Boundary
1 Introduction and setting of the problem
We use here the notation of Chapter 3 in Part IV. We assume that
(HH)∞
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
(i)
A generates a strongly continuous group etA in H
of type ω0 and λ0 is a real number in ρ(A) such
that ω0 < λ0,
(ii)
E ∈L(U; H),
(iii)
∀T > 0, ∃KT > 0 such that
 t
0
|E⋆A⋆esA⋆x|2 ds ≤K2
T|x|2,
∀x ∈D(A⋆), t ≥0,
(iv)
C ∈L(H; Y ).
Clearly, if (HH)∞hold, then the hypotheses (HH) of Chapter 3 in Part IV
are fulﬁlled with P0 = 0. We want to minimize the cost function:
J∞(u) =
 ∞
0
{|Cx(s)|2 + |u(s)|2}ds,
(1.1)
over all controls u ∈L2(0, ∞; U) subject to the equation constraint
x(t) = etAx0 + G(u)(s),
G(u)(s) = (λ0 −A)
 t
0
e(t−s)AEu(s) ds.
(1.2)
Moreover, x0 ∈H and u ∈L2(0, ∞; U). We recall that by Proposition 3.1 in
Chapter 1 in Part II, x ∈C([0, T ]; H) for all ∈L2(0, T ; U); more precisely
G ∈L

L2(0, T ; U); C([0, T ]; H)

,
∀T > 0.
We say that the control u ∈L2(0, ∞; U) is admissible if J∞(u) < ∞. The
deﬁnitions of optimal control, optimal state, and optimal pair are the same

530
V-3 Unbounded Control Operators in Hyperbolic Equations
as in Chapters 1 and 2. When, for any x0 ∈H, an admissible control exists,
we say that (A, AE) is C-stabilizable.
In order to solve the control problem (1.1)–(1.2), we consider the opti-
mization problems: to minimize
Jt(u) =
 t
0
{|Cx(s)|2 + |u(s)|2} ds,
(1.3)
over all controls u ∈L2(0, t; U) subject to the equation constraint (1.1), and
denote by (u⋆
t , x⋆
t ) the corresponding optimal pair, then we prove the conver-
gence of (u⋆
t , x⋆
t ) to an optimal pair for the problem (1.1)–(1.2).
For a diﬀerent approach to the study of the algebraic Riccati equation and
more results, see the paper by F. Flandoli, I. Lasiecka, and R. Trig-
giani [1], and the Lecture Notes by I. Lasiecka and R. Triggiani [11].
The situation where assumption (HH)∞–(iii) is not fulﬁlled is considered in
I. Lasiecka and R. Triggiani [12], where speciﬁc examples are provided.
2 Main results
The main result of this chapter is as follows.
Theorem 2.1. Assume (HH)∞and that (A, AE) is C-stabilizable. Let (u⋆
t , x⋆
t )
be the optimal pair corresponding to problem (1.2)–(1.3); then there exist
u⋆∈L2(0, ∞; U) and x⋆∈L2(0, ∞; H) such that
(i) limt→∞u⋆
t = u⋆in L2(0, T ; U), for all T > 0.
(ii) limt→∞x⋆
t = x⋆in L2(0, T ; H), for all T > 0.
(iii) (u⋆, x⋆) is an optimal pair for the problem (1.1)–(1.2).
Proof. Denote by Pmin(·) the mild solution to (1.5) of Part IV, Chapter 3,
with P0 = 0. Let x0 ∈H, u ∈L2(0, ∞; U) and let x be the corresponding
solution to (1.1); we have
(Pmin(t)x0, x0) ≤
 t
0
{|Cx(s)|2 + |u(s)|2} ds
≤J∞(u),
∀u ∈L2(0, ∞; U).
(2.1)
As (A, AE) is C-stabilizable, there exists M(x0) > 0 such that
(Pmin(t)x0, x0) ≤M(x0),
∀t > 0.
By the Uniform Boundedness Theorem, it follows that there exists M > 0
such that
∥Pmin(t)∥≤M,
∀t ≥0.
(2.2)
Then there exists the limit

2 Main results
531
P ∞
minx0 = lim
t→∞Pmin(t)x0,
∀x0 ∈H.
From (2.1) it follows that
(P ∞
minx0, x0) ≤J∞(u),
∀u ∈L2(0, ∞; U).
(2.3)
On the other hand we have
(Pmin(t)x0, x0) =
 t
0
{|Cx∗
t (s)|2 + |u∗
t (s)|2} ds,
∀t > 0,
(2.4)
and taking into account (2.2), we see easily that the set {ˆut}t≥0, where
ˆut(s) =

u⋆
t (s),
if s ∈[0, t],
0,
if s ≥t,
is bounded in L2(0, ∞; U). Thus there exists a sequence tn ↑∞and a function
ˆu ∈L2(0, ∞; U) such that
ˆutn ⇀ˆu,
in L2(0, ∞; U) as n →∞.
(2.5)
Set
ˆx(t) = etAx0 + G(u)(t).
Fix now T > 0. Obviously
u∗
tn ⇀ˆu,
in L2(0, T ; U) as n →∞,
and consequently,
x∗
tn ⇀ˆx,
in L2(0, T ; U) as n →∞.
From (2.4) it follows that
(P ∞
minx0, x0) ≥
 T
0
{|ˆx(s)|2 + |ˆu(s)|2}ds,
∀t > 0.
As T is arbitrary we can conclude that Cˆx ∈L2(0, ∞; H) and
(P ∞
minx0, x0) ≥J∞(ˆu),
(2.6)
which, along with (2.3), implies that the pair (ˆu, ˆx) is optimal. It remains to
prove (i) and (ii). We ﬁrst remark that, by the Lebesgue dominated conver-
gence theorem
Cx∗
tn ⇀ˆx,
in L2(0, T ; ∞) as n →∞.
Letting n tend to inﬁnity in the equality
 Tn
0
{|Cx∗
Tn(s)|2 + |u∗
Tn(s)|2} ds = (Pmin(Tn)x0, x0),

532
V-3 Unbounded Control Operators in Hyperbolic Equations
we ﬁnd
lim
n→∞
 ∞
0
{|Cx∗
Tn(s)|2 + |u∗
T (s)|2} ds = (P ∞
minx0, x0).
This implies that the convergences of {u∗
T (s)} to ˆu and of {Cx∗
T (s)} to Cˆx
are strong. As the optimal pair is unique, by the strict convexity of the cost
J∞, (i) and (ii) follows.
⊓⊔
Example 2.1. We consider here the system described in Example 4.3 of Chap-
ter 3 of Part IV, but with T = +∞. In order to apply Theorem 2.1 it is enough
to check that (A, AE) is exactly controllable. As remarked at the end of §2 in
Chapter 2 of Part II, the relevant estimate to prove is that, for T suﬃciently
large, there exists a constant C(T ) > 0 such that
 T
0

∂v
∂ν

2
L2(∂Ω)
dt ≥C(T )E(0),
(2.7)
where v is the solution to the problem
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
vtt(t, ξ) = ∆ξv(t, ξ),
t ≥0, ξ ∈Ω,
v(t, ξ) = 0,
t ≥0, ξ ∈∂Ω,
v(0, ξ) = v0 ∈H1
0(Ω),
vt(0, ξ) = v1 ∈L2(Ω),
(2.8)
and
E(t) = 1
2

Ω
[|∇v(t, ξ)|2 + |vt(t, ξ)|2] dξ dt.
To prove (2.7) we follows J. L. Lions [4]. We introduce the following notation:
X(t) = (vt, ξ · ∇v),
Y (t) = (vt, v),
where (·, ·) denotes the scalar product and | · | the norm in L2(Ω). We ﬁrst
prove the identity
Y (T ) −Y (0) =

Ω×[0,T ]
[v2
t −|∇v|2] dξ dt.
(2.9)
In fact, multiplying both sides of the ﬁrst equation in (2.8) by v and taking
into account that
(v2)tt = 2v2
t + 2vvtt,
we ﬁnd
vvtt = 1
2(v2)tt −v2
t = v∆v.
Integrating on Ω
1
2

Ω
(v2)tt dξ −

Ω
v2
t dξ =

Ω
v∆v dξ = −

Ω
|∇v|2 dξ,

2 Main results
533
and so
1
2
d
dt

Ω
(v2)t dξ =

Ω
[v2
t −|∇v|2] dξ,
which implies (2.9).
Now we multiply both sides of the ﬁrst equation in (2.8) by ξ · ∇v and
integrate in Ω× [0, T ]. We ﬁnd
I = J,
where
I =

Ω×[0,T ]
(ξ · ∇v)vtt dξ dt
and
J =

Ω×[0,T ]
(ξ · ∇v)∆v dξ dt.
We proceed now in three steps.
Step 1. Estimate of I.
By integrating by parts successively in t and in ξ, we ﬁnd
I = X(T ) −X(0) −

Ω×[0,T ]
(ξ · ∇vt)vt dξ dt
= X(T ) −X(0) −1
2

Ω×[0,T ]
ξ · ∇(v2
t ) dξ dt
= X(T ) −X(0) + n
2

Ω×[0,T ]
v2
t dξ dt.
Step 2. Estimate of J.
By integrating by parts in ξ, we ﬁnd
J =
n

h,k=1

Ω×[0,T ]
ξh
∂v
∂ξh
∂2v
∂ξ2
k
dξ dt
=
n

h,k=1

∂Ω×[0,T ]
ξhνk
∂v
∂xh
∂v
∂ξk
dσ dt −
n

h,k=1

Ω×[0,T ]
∂
∂ξk

ξh
∂v
∂ξh
 ∂v
∂ξk
dξ dt
=

∂Ω×[0,T ]
(ξ · ∇v)

∂v
∂ν

2
dσ dt −1
2
n

h,k=1

Ω×[0,T ]
ξh
∂
∂ξk

∂v
∂ξh

2
dξ dt
−

Ω×[0,T ]
|∇v|2 dξ dt.
We observe now that, as v = 0 on ∂Ω, we have ∇v = ∂v
∂ν ν. It follows that

534
V-3 Unbounded Control Operators in Hyperbolic Equations
J =

∂Ω×[0,T ]
(ξ · ∇v)

∂v
∂ν

2
dσ dt −1
2

Ω×[0,T ]
ξ · ∇(|∇v|2) dx dt
−

Ω×[0,T ]
|∇v|2 dξ dt
=

∂Ω×[0,T ]
(ξ · v)

∂v
∂ν

2
dσ dt +
n
2 −1
 
Ω×[0,T ]
|∇v|2 dξ dt.
Step 3. Conclusion.
As I = J, we ﬁnd

∂Ω×[0,T ]
(ξ · v)

∂v
∂ν

2
dσ
= X(T ) −X(0) + n
2

Ω×[0,T ]
v2
t dξ dt −
n
2 −1
 
Ω×[0,T ]
|∇v|2 dx dt
= X(T ) −X(0) + n
2

Ω×[0,T ]
[v2
t −|∇v|2] dξ dt +

Ω×[0,T ]
|∇v|2 dξ dt.
In conclusion

∂Ω×[0,T ]
(ξ · v)

∂v
∂ν

2
dσ dt =

X + n −1
2
Y
	T
0
+ TE(0).
Now, let R > 0 such that Ωis included in the ball B(0, R) and let α0 > 0
such that |v| ≤α0|∇v|; then
|X| ≤R|vt| |∇v| ≤RE(0)
and
|Y | ≤|v| |vt| ≤α0|vt| |∇v| ≤α0E(0),
and so

∂Ω×[0,T ]
(ξ · v)

∂v
∂ν

2
dσ dt ≥E(0)[T −2R −3α0],
and the required estimate (2.7) holds for T large.
⊓⊔
3 Some result for general semigroups
In this section we do not assume that A generates a strongly continuous group
but

3 Some result for general semigroups
535
(HH)′
∞
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
(i)
A generates a strongly continuous semigroup etA
in H of type < λ0, for some real number λ0,
(ii)
E ∈L(U; H),
(iii)
∀T > 0, ∃KT > 0 such that
 t
0
|E⋆A⋆esA⋆x|2 ds ≤K2
T |x|2,
∀x ∈D(A⋆), t ≥0,
(iv)
C ∈L(H; Y ).
By the results of §5 in Chapter 3 of Part IV, we are able to show the existence
of an optimal pair (u∗
t , x∗
t ) of the control problem (1.2)–(1.3) under hypotheses
(HH)′
∞. Then the following theorem is proved as Theorem 2.1.
Theorem 3.1. Assume (HH)′
∞and that (A, AE) is C-stabilizable. Let (u⋆
t , x⋆
t )
be the optimal pair corresponding to problem (1.2)–(1.3). Then there exist
u⋆∈L2(0, ∞; U) and x⋆∈L2(0, ∞; H) such that
(i) limt→∞u⋆
t = u⋆in L2(0, T ; U), for all T > 0.
(ii) limt→∞x⋆
t = x⋆in L2(0, T ; H), for all T > 0.
(iii) (u⋆, x⋆) is an optimal pair for the problem (1.1)–(1.2).
Example 3.1 (Age-dependent equations). We continue here Example 5.1 of
Chapter 3 of Part IV, but taking T = +∞. In order to apply Theorem 2.2 we
have only to check stabilizability. Set
σ1(A) = {λ ∈σ(A): Re λ < 0},
σ2(A) = {λ ∈σ(A): Re λ ≥0},
and denote by Q1 and Q2 the spectral projectors on σ1(A) and σ2(A), re-
spectively. As is well known Q2 is a ﬁnite dimensional projector; moreover,
because the semigroup S(·) is diﬀerentiable for t > a+ the spectral determin-
ing condition holds true, and there exist M > 0 and r > 0 such that
∥S(t)Q1∥≤Me−rt,
t ≥0.
(3.1)
We set p1(t) = Q1p(t), t ≥0 and p2(t) = Q2p(t), t ≥0 and denote by
A1 and A2 the restrictions of A to Q1(H) and Q2(H), respectively. By (3.1)
it follows easily that p1 ∈L2(0, ∞; H) for any u ∈L2(0, ∞). So, we have
only to check that, for any p0 ∈H there exists u ∈L2(0, ∞) such that
p2 ∈L2
0, ∞; Q2(H)

. Now p2 is the solution of the ﬁnite dimensional Cauchy
problem

p′
2(t) = A2p2(t) −αA2Du(t),
p2(0) = Q2p0.
(3.2)
By a well-known result due to Hautus (see Part I, Chapter 1, §2.6), the system
(3.2) is stabilizable if and only if

536
V-3 Unbounded Control Operators in Hyperbolic Equations
Ker(λ −A∗
2) ∩Ker(D∗A∗
2) = {0},
∀λ ∈σ2(A).
(3.3)
Now we can easily check that the spectrum σ(A) of consists of simple
eigenvalues λ that are exactly all the solutions to the equation
1 =
 a+
0
e−λbK(b) db.
(3.4)
If λ is an eigenvalue of A, then a corresponding eigenvector is given by
ϕλ(a) = e−λaπ(a),
a ∈[0, a+].
(3.5)
Moreover the adjoint A∗of A is given by

D(A∗) = {ψ ∈H1(0, a+): ψ(T ) = 0},
A∗ψ = ψ′ −µψ + βψ(0).
(3.6)
Finally σ(A∗) = σ(A), and if λ ∈σ(A∗), then a corresponding eigenvector is
given by
ψλ(a) =
1
π(a)
 a+
a
eλ(a−b)K(b) db,
a ∈[0, a+].
(3.7)
Let now λ ∈σ2(A) and ψ ∈Ker(D∗A∗
2); then we have
1 =
 a+
0
e−λbK(b) db,
(3.8)
and a corresponding eigenvector to λ is given by
ψλ(a) =
1
π(a)
 a+
a
eλ(a−b)K(b) db,
a ∈[0, a+].
(3.9)
It follows, recalling hypotheses (5.8) of Part II, Chapter 3,
D∗A∗
2ψ = ⟨ψ, π⟩= 1
λ
 a+
0
e−λbK(b) db −1
	
̸= 0.
Thus (A, AE) is I-stabilizable.
⊓⊔

A
An Isomorphism Result
We present two proofs of an isomorphism result that plays a key role in the
solution of the linear quadratic optimal control problem. Proposition 1.1 gives
a direct proof, whereas Proposition 1.2 gives a proof based on the variational
characterization of the minimum of a linear quadratic cost function under a
linear state equation constraint. This second proof also yields a general result
on the invertibility of a 2 × 2 matrix of operators, which corresponds to the
usual optimality system obtained from the coupled system in the state and
adjoint state variables. In this appendix X represents a Hilbert space, with
inner product (·, ·) and norm | · | and T and S linear bounded operators in X.
Proposition 1.1. Assume that T and S are symmetric and nonnegative; then
I + TS is one-to-one and onto. Moreover
∥S(I + TS)−1∥≤∥S∥
and
∥(I + TS)−1∥≤1 + ∥T ∥∥S∥.
Proof.
Step 1. I + TS is one-to-one. Let x0 ∈X such that x0 + TSx0 = 0.
Then we have Sx0 + STSx0 = 0, and so
(Sx0, x0) + (TSx0, Sx0) = 0,
which implies (Sx0, x0) = 0, and consequently Sx0 = 0 and ﬁnally 0 =
x0 + TSx0 = x0.
Step 2.
R := (I + TS)(X) is dense in X. Let y0 ∈X be such that

(I + TS)x, y0

= 0,
∀x ∈X.
Then
(x, (I + ST )y0) = 0,
∀x ∈X,
which implies y0 = 0 by Step 1 (exchanging T and S).

538
V-A An Isomorphism Result
Step 3.
0 ≤(S(I +TS)−1x, x) ≤(Sx, x), ∀x ∈R. Let x ∈R and let z = (I +TS)−1x.
We have, recalling that T ≥0, S ≥0,
(Sx, x) = (S(I + TS)z, (I + TS)z)
= (Sz, z) + (Sz, TSz) + (STSz, z) + (STSz, TSz)
≥(Sz, z) + (Sz, TSz) = (Sz, (I + TS)z)
= (S(I + TS)−1x, x) ≥0.
Step 4.
(S(I + TS)−1x, y) = (x, S(I + TS)−1y), ∀x, y ∈R. Let x, y ∈R and let
z = (I + TS)−1x, w = (I + TS)−1y. We have
(S(I + TS)−1x, y) = (Sz, w + TSw) =

(I + TS)z, Sw

= (x, S(I + TS)−1y).
Step 5.
Conclusion. By steps 2, 3, and 4 the symmetric operator S(I + TS)−1 has a
unique extension to a symmetric bounded operator that we denote again by
S(I + TS)−1. Now for any x ∈R we have
(I + TS)−1x = x −TS(I + TS)−1x;
thus, by Step 3
|(I + TS)−1x| ≤|x| + ∥T ∥|Sx|.
This implies that R is closed; because it is dense by Step 2, it is onto.
⊓⊔
The above result was ﬁrst proved in an early version of this book via an
optimal control argument. We give it below because this type of result is useful
in many contexts.
Consider the real Hilbert spaces U and H, the continuous linear operators
B ∈L(U; H),
Q ∈L(H; H)
and assume that
∀x ∈H,
(Qx, x) ≥0,
∀x, y ∈H,
(Qx, y) = (Qy, x).
Deﬁne the following optimal control problem: To ﬁnd u ∈U such that
inf
u∈U J(u),
where

V-A An Isomorphism Result
539
J(u) = 1
2(Qx + 2q, x) + 1
2|u|2
and
x = Bu + f
for some given pair (f, g) ∈H × H.
This problem has a unique solution u ∈U that is completely characterized
by the following optimality condition:
dJ(u; v) = 0,
∀v ∈U,
where
dJ(u; v) = (Qx + q, y) + (u, v)
and
x = Bu + f,
y = Bv.
Introduce the adjoint variable p ∈H
p = Qx + q
and rewrite the optimality condition
(p, Bv) + (u, v) = 0,
∀v ∈U =⇒u + B∗p = 0.
Using this characterization we can now write the coupled system in the form

I BB∗
−Q
I
	 x
p
	
=
f
q
	
.
We conclude that for any given pair (f, q), there exists a pair (x, p) for which
the above identity is veriﬁed. Moreover when f = g = 0,
x + BB∗p = 0 =⇒(x, p) + |B∗p|2 = 0,
−Qx + p = 0 =⇒−(Qx, x) + (p, x) = 0
and x = p = 0. Therefore the matrix of operators is invertible.
Finally set q = 0 and eliminate p in the ﬁrst equation of the coupled
system. Then
x + BB∗Qx = f =⇒[I + BB∗Q]x = f.
Again for any f in H, there exists an x in H for which the above identity is
veriﬁed. For f = 0
x + BB∗Qx = 0 =⇒(Qx, x) + |B∗Qx|2 = 0
=⇒Q1/2x = 0 =⇒x = 0.
Hence I + BB∗Q is invertible.

540
V-A An Isomorphism Result
Proposition 1.2. Given two real Hilbert spaces U and H and two linear
bounded operators
B ∈L(U, H)
and
Q ∈L(H, H)
such that
∀x ∈H,
(Qx, x) ≥0,
∀x, y ∈H,
(Qx, y) = (Qy, x),
then the operators

I
BB∗
−Q
I
	
and
[I + BB∗Q]
are (algebraic and topological) isomorphisms.
This last proposition was also proved by M. Sorine [1].
Proposition 1.1 is now obtained with X = H = U, S = Q, and B = T 1/2.

References
P. Acquistapace
[1] On a family of generators of analytic semigroups, Diﬀerential Equations with
Applications in Biology, Physics, and Engineering (J.A. Goldstein, F. Kappel,
and W. Schappacher, eds.), Marcel Dekker, New York, 1991, pp. 1–6.
P. Acquistapace, F. Bucci, and I. Lasiecka
[1] Optimal boundary control and Riccati theory for abstract dynamics motivated
by hybrid systems of PDEs, Adv. Diﬀerential Equations 10 (2005), no. 12,
1389–1431.
[2] A trace regularity result for thermoelastic equations with application to optimal
boundary control, J. Math. Anal. Appl. 310 (2005), no. 1, 262–277.
P. Acquistapace, F. Flandoli, and B. Terreni
[1] Initial boundary value problems and optimal control for non-autonomous par-
abolic systems, SIAM J. Control Optim. 29 (1991), 89–118.
P. Acquistapace and B. Terreni
[1] Some existence and regularity results for abstract non-autonomous parabolic
equations, J. Math. Anal. Appl. 99 (1984), 9–64.
[2] H¨older classes with boundary conditions as interpolation spaces, Math. Z. 195
(1987), 451–471.
[3] A uniﬁed approach to abstract linear non-autonomous parabolic equations,
Rend. Sem. Mat. Univ. Padova 78 (1987), 47–107.
[4] Boundary control problems for non-autonomous parabolic systems, Stabiliza-
tion of Flexible Structures (J. P. Zol´esio, ed.), Lecture Notes in Control and
Inform. Sci., vol. 147, Springer-Verlag, Berlin, Heidelberg, New York, 1990,
pp. 156–166.
[5] Inﬁnite-horizon linear-quadratic regulator problems for nonautonomous para-
bolic systems with boundary control, SIAM J. Control Optim. 34 (1996), no.
1, 1–30.
[6] Classical solutions of nonautonomous Riccati equations arising in parabolic
boundary control problems, Appl. Math. Optim. 39 (1999), no. 3, 361–409.
[7] Classical solutions of nonautonomous Riccati equations arising in parabolic
boundary control problems. II, Appl. Math. Optim. 41 (2000), no. 2, 199–226.
V. M. Adamjan, D. Z. Arov, and M. G. Kre˘ın
[1] Inﬁnite Hankel matrices and generalized problems of Carathodory-Fej´er and
F. Riesz, (Russian) Funkcional. Anal. i Priloˇzen. 2 (1968), no. 1, 1–19.

542
References
[2] Inﬁnite Hankel matrices and generalized Carathodory-Fej´er and I. Schur prob-
lems, (Russian) Funkcional. Anal. i Priloˇzen. 2 (1968), no. 4, 1–17.
[3] Bounded operators which commute with a C00 class contraction whose rank of
nonunitarity is one, (Russian) Funkcional. Anal. i Priloˇzen. 3 (1969), no. 3,
86–87.
[4] Analytic properties of the Schmidt pairs of a Hankel operator and the general-
ized Schur-Takagi problem, (Russian) Mat. Sb. (N.S.) 86(128) (1971), 34–75.
[5] Inﬁnite Hankel block matrices and related problems of extension, (Russian)
Izv. Akad. Nauk Armjan. SSR Ser. Mat. 6 (1971), no. 2-3, 87–112.
R. A. Adams
[1] Sobolev spaces, Academic Press, New York, 1975.
J. K. Aggarwal
Cf. D. H. Eller, J. K. Aggarwal, and H. T. Banks.
S. Agmon
[1] On the eigenfunctions and the eigenvalues of general elliptic boundary value
problems, Comm. Pure Appl. Math. 15 (1962), 119–147.
[2] Lectures on elliptic boundary-value problems, Van Nostrand, Princeton, New
York, 1965.
G. P. Akilov
Cf. L. V. Kantorivich and G. P. Akilov.
Y. Alekal, P. Brunosvsky, D. H. Chyung, and E. B. Lee
[1] The quadratic problems for systems with time delays, IEEE Trans. Automat.
Control AC-16 (1971), 673–688.
H. Amann
[1] Feedback stabilization of linear and semilinear parabolic systems, Proceedings
of Trends in Semigroup Theory and Applications (Trieste, 1987), Lecture
Notes in Pure and Appl. Math., Marcel Dekker, New York, 1989, pp. 21–57.
A. Ardito and P. Ricciardi
[1] Existence and regularity for linear delay partial diﬀerential equations, Nonlin-
ear Anal. 4, (1980), 411–414.
M. Artola
[1] ´Equations paraboliques `a retardement, C. R. Acad. Sci. Paris S´er. A 264
(1967), 668–671.
[2] Sur les perturbations des ´equations d’´evolution, application `a des probl`emes de
retard, Ann. Sci. ´Ecole Norm. Sup. (4) 2, (1969), 137–253.
[3] Sur une ´equation du premier ordre `a argument retard´e, C. R. Acad. Sci. Paris,
S´er. A 268 (1969), 1540–1543.
[4] ´Equations et in´equations variationnelles `a retard, Ann. Sci. Math. Qu´ebec 1
(1977), 131–152.
P. Auscher, S. Hofmann, J. L. Lewis, and Ph. Tchamitchian
[1] Extrapolation of Carleson measures and the analyticity of Kato’s square-root
operators, Acta Math. 187 (2001), no. 2, 161–190.
G. Avalos and I. Lasiecka
[1] Exact controllability of ﬁnite energy states for an acoustic wave/plate interac-
tion under the inﬂuence of boundary and localized controls, Adv. Diﬀerential
Equations 10 (2005), no. 8, 901–930.
A. V. Balakrishnan
[1] Fractional powers of closed operators and the semigroup generated by them,
Paciﬁc J. Math. 10 (1960), 419–437.

References
543
[2] Introduction to optimization theory in a Hilbert space, Springer-Verlag, Berlin,
1971.
[3] Identiﬁcation and stochastic control of a class of distributed systems with
boundary noise, Control Theory, Numerical Methods and Computer Systems
Modelling (A. Bensoussan and J L. Lions, eds.), Springer-Verlag, New York,
1975, pp. 163–178.
[4] Applied functional analysis, Springer-Verlag, New York, Heidelberg, 1976.
[5] Boundary control of parabolic equations: L-Q-R theory, Theory of Non Lin-
ear Operators, Abh. Akad. Wiss. DDR, Abt. Math. Naturwiss. Tech., vol. 6,
Akademie-Verlag, Berlin, 1978, pp. 11–13.
J. Ball
[1] Strongly continuous semigroups, weak solutions and the variation of constants
formula, Proc. Amer. Math. Soc. 63 (1977), 370–373.
H. T. Banks
[1] The representation of solutions of linear functional diﬀerential equations, J.
Diﬀerential Equations 5 (1969), 399–410.
Cf. D. H. Eller, J. K. Aggarwal, and H. T. Banks.
H. T. Banks and M. Q. Jacobs
[1] An attainable sets approach to optimal control of functional diﬀerential equa-
tions with function space terminal conditions, J. Diﬀerential Equations 13
(1973), 127–149.
H. T. Banks, M. Q. Jacobs, and C. E. Langenhop
[1] Function space controllability for linear functional diﬀerential equations, Dif-
ferential Games and Control Theory, 1973, Proc. NSF–CBMS Conference on
Diﬀerential Games and Control Theory, (1973), University of Rhode Island,
Kingston, R.I., pp. 81-98.
[2] Characterization of the controlled states in W (1)
2
of linear hereditary systems,
SIAM J. Control 13 (1975), 611–649.
H. T. Banks, M. Q. Jacobs, and M. R. Latina
[1] The synthesis of optimal controls for linear, time-optimal problems with re-
tarded controls, J. Optim. Theory Appl. 8 (1971), 319–366.
H. T. Banks and G. A. Kent
[1] Control of functional diﬀerential equations of retarded and neutral type to
target sets in function space, SIAM J. Control 10 (1972), 567–593.
H. T. Banks and K. Kunish
[1] The linear regulator problem for parabolic systems, SIAM J. Control Optim. 22
(1984), no. 5, 684–699.
H. T. Banks and A. Manitius
[1] Application of abstract variational theory to hereditary systems—a survey,
IEEE Trans. Automat. Control AC-19 (1974), 524–533.
[2] Projection series for retarded functional diﬀerential equations with applications
to optimal control problems, J. Diﬀerential Equations 18 (1975), 296–332.
V. Barbu and G. Da Prato
[1] A representation formula for the solutions to the operator Riccati equation,
Diﬀerential Integral Equations 5 (1992), no. 4, 821–829.
C. Bardos
[1] A regularity theorem for parabolic equations, J. Funct. Anal. 7 (1971), 311–322.
C. Bardos, G. Lebeau, and J. Rauch

544
References
[1] Contrˆole et stabilisation dans les probl`emes hyperboliques, Appendix II
to J. L. Lions, Contrˆolabilit´e Exacte, Perturbations et Stabilisation des
Syst`emes Distribu´es, vol. 1, Masson, Paris, 1988.
D. I. Barnea
Cf. H.J. Kuschner and D.I. Barnea.
S. Baron
Cf. Y. Ho, A.E. Bryson, and S. Baron.
T. Bas¸ar and P. Bernhard
[1] H∞-Optimal Control and Related Minimax Design Problems: A Dynamical
Game Approach, 2nd ed. Birkh¨auser, Boston, MA, 1995.
A. Bel Fekih
[1] Une nouvelle approche pour la contrˆolabilit´e et l’observabilit´e des syst`emes
distribu´es, Th`ese, Universit´e de K´enitra, Morocco, 1990.
R. Bellman
[1] Dynamic programming, Princeton University Press, Princeton, NJ, 1957.
[2] Introduction to matrix analysis, McGraw-Hill, New York, 1960.
R. Bellman and K. L. Cooke
[1] Asymptotic behaviour of solutions of diﬀerential-diﬀerence equations, Mem.
Amer. Math. Soc. 35 (1959).
[2] Diﬀerential-diﬀerence equations, Academic Press, New York, 1963.
A. Bensoussan
[1] Observateurs et stabilit´e, Colloque CNES, Paris, 1987.
A. Bensoussan and P. Bernhard
[1] Remarks on the theory of robust control, in “Optimization, optimal control
and partial diﬀerential equations” (Ia¸si, 1992), Internat. Ser. Numer. Math.,
107, Birkh¨auser, Basel, Switzerland, 1992, pp. 149–166.
A. Bensoussan, M. C. Delfour, and S. K. Mitter
[1] The linear quadratic control problem for inﬁnite dimensional systems over inﬁ-
nite horizon; survey and examples, Proc. of the IEEE Conference on Decision
and Control and the 15th Symposium on Adaptive Processes (Clearwater,
1976), Inst. Electr. Electron. Engrs., New York, 1976, pp. 746–751.
A. Bensoussan and J. L. Lions
[1] Applications des in´equations variationnelles en contrˆole stochastique, Dunod,
Bordas, Paris, 1978.
P. Bernhard
[1] Contribution `a l’´etude des jeux diﬀ´erentiels `a deux personnes, somme nulle et
information parfaite, Th`ese, Universit´e de Paris VI, Paris, France, 1978.
[2] Linear-quadratic, two-person, zero-sum diﬀerential games: necessary and suf-
ﬁcient conditions, J. Optim. Theory Appl. 27 (1979), 51–69.
Cf. T. Bas¸ar and P. Bernhard.
Cf. A. Bensoussan and P. Bernhard.
C. Bernier and A. Manitius
[1] On semigroups in Rn × Lp corresponding to diﬀerential equations with delays,
Canad. J. Math. 30 (1978), 296–332.
D. Bernouilli
[1] Exposition of a new theory on the measurement of risk, Econometrica 22
(1954), 23–36; translated by L. Sommer, 1938.
[2] Specimen theoriae novae de mensura sortis, 1938; translated by L. Sommer,
Exposition of a new theory on the measurement of risk, Econometrica 22
(1954), 23–36.

References
545
A. Bielecki
[1] Une remarque sur la m´ethode de Banach–Cacciopoli–Tikhonov dans la th´eorie
des ´equations diﬀ´erentielles ordinaires, Bull. Acad. Polon. Sci. 4 (1956), 261–
264.
Ju. G. Borisovic and A. S. Turbabin
[1] On the Cauchy problem for linear nonhomogeneous diﬀerential equations with
retarded argument, Soviet Math. Doklady 10 (1969), 401–405.
S. Boyd and L. Vandenberghe
[1] Convex Optimization, Cambridge University Press, Cambridge, England,
2004.
R. W. Brockett
[1] Finite dimensional linear systems, Wiley, New York, 1970.
P. Brunosvsky
Cf. Y. Alekal, P. Brunosvsky, D.H. Chyung, and E. B. Lee.
A. E. Bryson
Cf. Y. Ho, A. E. Bryson, and S. Baron.
F. Bucci
[1] A Dirichlet boundary control problem for the strongly damped wave equation,
SIAM J. Control Optim. 30 (1992), no. 5, 1092–1100.
Cf. P. Acquistapace, F. Bucci, and I. Lasiecka.
F. Bucci, I. Lasiecka, and R Triggiani
[1] Singular estimates and uniform stability of coupled systems of hyper-
bolic/parabolic PDEs, Abstr. Appl. Anal. 7 (2002), no. 4, 169–237.
J. A. Burns, T. L. Herdman, and H. R. Stech
[1] Linear functional diﬀerential equations as semigroups on product spaces, SIAM
J. Math. Anal. 14 (1983), 98–116.
P. Cannarsa, G. Da Prato, and J. P. Zol´esio
[1] The damped wave equation in a moving domain, J. Diﬀerential Equations 85
(1990), 1–16.
C. Carath´eodory
[1] Vorlesungen uber Relle Funktionen, Verlag und Druck, Leipzig, Germany,
1927.
S. Chen and R. Triggiani
[1] Proof of two conjectures of G. Chen and D. L. Russell on structural damping
for elastic systems. Approximation and optimization, Lecture Notes in Math.,
vol. 1354 (Proc. of the Seminar in Approximation and Optimization, Havana,
Cuba, 1987), Springer, Berlin, 1988, pp. 234–256.
[2] Proof of extension of two conjectures on structural damping for elastic systems:
the case 1
2 ≤α ≤1, Paciﬁc J. Math. 136 (1989), 15–55.
[3] Gevrey class semigroups arising from elastic systems with gentle dissipation:
the case 0 < α < 1
2, Proc. Amer. Math. Soc. 110 (1990), 401–415.
A. Chojnowska-Michalik
[1] Representation theorem for general stochastic delay equations, Bull. Acad.
Polon. Sci. Sr. Sci. Math. Astronom. Phys. 26 (1978), no. 7, 634–641.
N. H. Choksy
[1] Time delay systems—a bibliography, IRE Autom. Control AC-5 (1960), 66–
70.
D. H. Chyung
Cf. Y. Alekal, P. Brunosvsky, D. H. Chyung, and E. B. Lee.

546
References
D. H. Chyung and E. B. Lee
[1] Linear optimal control problems with time delays, SIAM J. Control 4 (1966),
548–575.
[2] Optimal systems with time delay, Automatic and Remote Control III, vol. 1,
Paper 7F, Inst. Mech. Engrs., Proc. Third IFAC Congress (London, 1966),
London, 1967.
[3] On certain extremal problems involving linear functional diﬀerential equa-
tion models, Mathematical Theory of Control (A.V. Balakrishnan and
L. W. Neustadt, eds.), Academic Press, New York, 1967, pp. 129–142.
[4] Delayed action control problems, Automatica 6 (1970), 395–400.
C. V. Coffman and J. J. Sch¨affer
[1] Linear diﬀerential equations with delays. Existence, uniqueness, growth and
compactness under natural Carath´eodory conditions, J. Diﬀerential Equations
16 (1974), 26–44.
B. D. Coleman and V. J. Mizel
[1] Norms and semi-groups in the theory of fading memory, Arch. Rational Mech.
Anal. 23 (1966), 87–123.
[2] Stability of functional diﬀerential equations, Arch. Rational Mech. Anal. 30
(1968), 173–196.
F. Colonius
[1] The maximum principle for relaxed hereditary diﬀerential systems with func-
tion space end condition, SIAM J. Control 20 (1982), 695–712.
[2] Optimal periodic control, Springer Verlag, New York, 1988.
F. Colonius, A. Manitius, and D. Salamon
[1] Structure theory and duality for time-varying retarded functional diﬀerential
equations, J. Diﬀerential Equations 78 (1989), 320–353.
K. L. Cooke
Cf. R. Bellman and K. L. Cooke.
C. Corduneanu
[1] Th´eor`emes d’existence globale pour les syst`emes `a argument retard´e, Qualita-
tive Methods in the Theory of Non-Linear Vibrations, vol. 2, Proc. Internat.
Sympos. Non-linear Vibrations, 1961, pp. 195–201.
[2] Sur certaines ´equations fonctionnelles de Volterra, Funkcial. Ekvac. 9 (1966),
119–127.
A. Cournot
[1] Recherches sur les principes math´ematiques de la th´eorie des richesses, Paris,
Hachette 1838 (English translation: Researches into the Mathematical Prin-
ciples of the Theory of Wealth, Macmillan, New York, 1897. (Reprinted: Au-
gustus M. Kelley, New York, 1971)).
G. Cramer
[1] Letter to Nicolas Bernouilli (1728), extracts printed in Bernouilli (1738) and
in Sommer’s (1954) translation.
R. F. Curtain and A. J. Pritchard
[1] The inﬁnite dimensional Riccati equation, J. Math. Anal. Appl. 47 (1974),
43–56.
R. F. Curtain and H. Zwart
[1] An introduction to inﬁnite-dimensional linear systems theory, Texts in Applied
Mathematics, 21. Springer-Verlag, New York, 1995.
R. D´ager and E. Zuazua

References
547
[1] Wave propagation, observation and control in 1-d ﬂexible multi-structures,
Math´ematiques & Applications (Berlin) [Mathematics & Applications] 50.
Springer-Verlag, Berlin, 2006.
I. Daleckii and M. Krein
[1] Stability of solutions of diﬀerential equations in Banach space, Izd. Nauka,
Moscow, 1970 (in Russian).
G. Da Prato
[1] Quelques r´esultat d’existence, unicit´e et r´egularit´e pour un probl`eme de la
th´eorie du contrˆole, J. Math. Pures Appl. 52 (1973), no. 9, 353–375.
[2] Synthesis of optimal control for an inﬁnite dimensional periodic problem,
SIAM J. Control Optim. 25 (1987), 706–714.
[3] Periodic solutions of an inﬁnite dimensional Riccati equation, System Mod-
elling and Optimization (A. Pr´ekopa, J. Szelesz´an and B. Strazicky, eds.),
Lecture Notes in Control and Inform. Sci., vol. 84, Springer-Verlag, Berlin,
Heidelberg, New York, 1986, pp. 714–722.
[4] Some remarks on the periodic linear quadratic regulator problem, Mod-
elling and Inverse Problems of Control for Distributed Parameter Systems
(A. Kurzhanski and I. Lasiecka, eds.), Lecture Notes in Control and Inform.
Sci., vol. 154, Springer-Verlag, Berlin, Heidelberg, New York, 1991, pp. 28–42.
[5] Spectral properties of the closed loop operator and periodic control problems,
J. Math. Systems, Estim. Control 3 (1993), no. 1, 41–50.
[6] Some remarks on the periodic linear quadratic regulator problem, Modelling
and Inverse Problems of Control for Distributed Parameter Systems (A.
Kurzhanski and I. Lasiecka, eds.), Lecture Notes in Control and Inform. Sci.,
vol. 154, Springer-Verlag, Berlin, Heidelberg, New York, 1991, pp. 28–42.
[7] Spectral properties of the closed loop operator and periodic control problems,
J. Math. Systems, Estim. Control 3 (1993), no. 1, 41–50.
Cf. V. Barbu and G. Da Prato.
Cf. P. Cannarsa, G. Da Prato, and J. P. Zol´esio.
G. Da Prato and M. C. Delfour
[1] Stabilization and unbounded solutions of the Riccati equation, Proc. 27th IEEE
Conference on Decision and Control, IEEE Publications, New York, 1988,
pp. 352–357.
[2] Unbounded solutions to the linear quadratic control problem, SIAM J. Control
Optim. 30 (1992), 31–48.
G. Da Prato and P. Grisvard
[1] Somme d’op´erateurs lin´eaires et ´equations diﬀ´erentielles op´erationnelles, J.
Math. Pures Appl. 4 (1979), no. 120, 329–396.
G. Da Prato and M. Iannelli
[1] Boundary control problem for age-dependent equations, Evolution Equations,
Control theory, and biomathematics (Han sur Lesse, 1991) (Ph. Cl´ement and
G. Lumer, eds.), Lecture Notes in Pure and Appl. Math., vol. 155, Dekker,
New York, 1994, pp. 91–100.
G. Da Prato and A. Ichikawa
[1] Riccati equations with unbounded coeﬃcients, Ann. Mat. Pura Appl. (4) 140
(1985), 209–221.
[2] Optimal control of linear systems with almost periodic inputs, SIAM J. Control
Optim. 25 (1987), 1007–1019.
[3] Quadratic control for linear periodic systems, Appl. Math. Optim. 18 (1988),
39–66.

548
References
[4] Quadratic control for linear time varying systems, SIAM J. Control Optim. 28
(1990), no. 2, 359–381.
[5] Optimal control for integrodiﬀerential equations of parabolic type, SIAM J.
Control Optim. 31 (1993), no. 5, 1167–1182.
G. Da Prato, I. Lasiecka, and R. Triggiani
[1] A direct study of the Riccati equation arising in hyperbolic boundary control
problems, J. Diﬀerential Equations 64 (1986), 26–47.
G. Da Prato and A. Lunardi
[1] Floquet exponents and stabilizability in time periodic parabolic systems, Appl.
Math. Optim. 22 (1990), 91–113.
[2] Stabilizability of integrodiﬀerential parabolic equations, J. Integral Equations
Appl. 2 (1990), 281–304.
G. Da Prato and J. P. Zol´esio
[1] An optimal control problem for a parabolic equation in non-cylindrical do-
mains, Systems Control Lett. 11 (1988), 73–77.
[2] Existence and optimal control for wave equation in moving domains, Stabiliza-
tion of Flexible Structures (J. P. Zol´esio, ed.), Lecture Notes in Control and
Inform. Sci., vol. 147, Springer-Verlag, Berlin, Heidelberg, New York, 1990,
pp. 167–190.
R. Datko
[1] An extension of a theorem of A.M. Lyapunov to semigroups of operators, J.
Math. Anal. Appl. 24 (1968), 290–295.
[2] Extending a theorem of A.M. Liapunov to Hilbert space, J. Math. Anal. Appl.
32 (1970), 610–616.
[3] Uniform asymptotic stability of evolutionary processes in a Banach space,
SIAM J. Math. Anal. 3 (1972), 428–445.
[4] Unconstrained control problem with quadratic cost, SIAM J. Control 11 (1973),
32–52.
[5] Neutral autonomous functional equation with quadratic cost, SIAM J. Control
12 (1974), 70–82.
M. C. Delfour
[1] Hereditary systems deﬁned on a compact time interval, Ph.D. Thesis, Case
Western Reserve University, Cleveland, OH, 1971.
[2] Function spaces with a projective limit structure, J. Math. Anal. Appl. 42
(1973), 554–568.
[3] Linear hereditary diﬀerential systems and their control, Optimal Control and
Its Applications (B.J. Kirby, ed.), Part II, Springer-Verlag, New York, 1974,
pp. 92–154.
[4] G´en´eralisation des r´esultats de R. Datko sur les fonctions de Lyapunov quadra-
tiques d´eﬁnies sur un espace de Hilbert, CRM–457, Universit´e of Montr´eal,
Qu´ebec, Canada, January 1974.
[5] State theory of linear hereditary diﬀerential systems, J. Math. Anal. Appl. 60
(1977), 8–35.
[6] The linear quadratic optimal control problem for hereditary diﬀerential sys-
tems: theory and numerical solution, Appl. Math. Optim. 3 (1977), no. 2/3,
101–162.
[7] The linear quadratic optimal control problem over an inﬁnite time horizon for
a class of distributed parameter systems, Control of Distributed Parameter
Systems (S.P. Banks and A. J. Pritchard, eds.), Pergamon Press, Oxford,
England, 1978, pp. 57–66.

References
549
[8] The largest class of hereditary systems deﬁning a C0 semigroup on the product
space, Canad. J. Math. 32 (1980), 969–978.
[9] Status of the state space theory of linear hereditary diﬀerential systems with
delays in state and control variables, Analysis and Optimization of Systems
(A. Bensoussan and J. L. Lions, eds.), Springer-Verlag, New York, 1980,
pp. 83–96.
[10] The linear quadratic optimal control theory for systems with delays in state
and control variables, Control Science and Technology for the Progress of
Society (H. Akaski, ed.), vol. I, Pergamon Press, Oxford, New York, 1981,
pp. 361–366.
[11] The role of the structural operator and the quotient space structure in the the-
ory of hereditary diﬀerential equations, Recent Advances in Diﬀerential Equa-
tions (R. Conti, ed.), Academic Press, New York, London, 1981, pp. 111–133.
[12] The product space approach in the state space theory of linear time-invariant
diﬀerential systems with delays in state and control variables, Mathematical
Control Theory, Banach Center Publications, PWN, Polish Scientiﬁc Publish-
ers, vol. 14, Warsaw, Poland, 1983, pp. 147–169.
[13] Linear quadratic optimal control of systems with state and control variables
delays, Automatica 20 (1984), 69–77.
[14] Repr´esentation des syst`emes `a retard: approche ´equation d’´etat, Lecture Notes
for the INRIA School Repr´esentation et Commande des Syst`emes `a Retard
(1984), INRIA Report, Sophia–Antipolis, France, 1984.
[15] The linear-quadratic optimal control problem with delays in state and control
variables: a state space approach, SIAM J. Control Optim. 24 (1986), 835–883.
[16] Linear-Quadratic Diﬀerential Games: Saddle Point and Riccati Diﬀerential
Equation, SIAM J. Control Optim., to appear (accepted October 2006).
Cf. G. Da Prato and M. C. Delfour.
M. C. Delfour and J. Karrakchou
[1] State space theory of linear time-invariant systems with delays in state, control
and observation variables. I, J. Math. Anal. Appl. 125 (1987), 361–399.
[2] State space theory of linear time invariant systems with delays in state, control
and observation variables. II, J. Math. Anal. Appl. 125 (1987), 400–450.
M. C. Delfour, E. B. Lee, and A. Manitius
[1] F-reduction of the operator Riccati equation for hereditary diﬀerential systems,
Automatica 14 (1978), 385–395.
M. C. Delfour and A. Manitius
[1] Control systems with delays: area of applications and present status of the
linear theory, New Trends in Systems Analysis (A. Bensoussan and J. L. Lions,
eds.), Springer-Verlag, New York, 1977, pp. 420–437.
[2] The structural operator F and its role in the theory of retarded systems I, J.
Math. Anal. Appl. 73 (1980), 466–490.
[3] The structural operator F and its role in the theory of retarded systems II, J.
Math. Anal. Appl. 74 (1980), 359–381.
M. C. Delfour, C. McCalla, and S. K. Mitter
[1] Stability and the inﬁnite-time quadratic cost problem for linear hereditary dif-
ferential systems, SIAM J. Control 13 (1975), 48–88.
M. C. Delfour and S. K. Mitter
[1] Reachability of perturbed linear systems and min sup problems, SIAM J. Con-
trol Optim. 7 (1969), 521–533.

550
References
[2] Syst`emes d’´equations diﬀ´erentielles h´er´editaires `a retards ﬁxes. Th´eor`emes
d’existence et d’unicit´e, C. R. Acad. Sci. Paris, S´er. A 272 (1971), 382–385.
[3] Le contrˆole optimal des syst`emes gouvern´es par des ´equations diﬀ´erentielles
h´er´editaires aﬃnes, C. R. Acad. Sci. Paris, S´er. A 272 (1971), 1715–1718.
[4] Syst`emes d’´equations diﬀ´erentielles h´er´editaires `a retards ﬁxes. Une classe de
syst`emes aﬃnes et le probl`eme du syst`eme adjoint, C. R. Acad. Sci. Paris,
S´er. A 272 (1971), 1102–1112.
[5] Hereditary diﬀerential systems with constant delays. I. General case, J. Dif-
ferential Equations 12 (1972), 213–235.
[6] Controllability, observability and optimal feedback control of hereditary diﬀer-
ential systems, SIAM J. Control 10 (1972), 298–328.
[7] Controllability and observability for inﬁnite dimensional systems, SIAM J.
Control 10 (1972), 329–333.
[8] Control of aﬃne systems with memory, Control of Aﬃne Systems with Mem-
ory (R. Conti and A. Ruberti, eds.), 5th Conference on Optimization Tech-
niques, Springer-Verlag, New York, 1973, pp. 292–303.
[9] Hereditary diﬀerential systems with constant delays. II. A class of aﬃne sys-
tems and the adjoint problem, J. Diﬀerential Equations 18 (1975), 18–28.
M. C. Delfour and M. Sorine
[1]
The linear-quadratic optimal control problem for parabolic systems with
boundary control through a Dirichlet condition, Control of distributed param-
eter systems (J. P. Barbary and L. Le Letty, eds.), 1982 (Toulouse, 1982),
IFAC, Laxenburg, Pergamon Press, Oxford, England 1983, pp. 87–90; see also
CRMA–1051, Universit´e de Montr´eal, Qu´ebec, Canada, 1981.
G. Di Blasio, K. Kunisch, and E. Sinestrari
[1] L2 regularity for parabolic partial integro-diﬀerential equations with delay in
the highest order derivatives, J. Math. Anal. Appl. 102 (1984), 38–57.
O. Dieckmann
[1] Perturbed dual semigroups and delay equations, Report AM-R8604, Centre
for Mathematics and Computer Sciences, Stichting Mathematisch Centrum,
Amsterdam, The Netherlands, 1986.
J. C. Doyle, K. Glover, P. P. Khargonekar, and B. A. Francis
[1] State-space solutions to standard H2 and H∞control problems, IEEE Trans.
on Automat. Control 34 (1989), 831–847.
J. Dugungji
[1] Topology, Allyn and Bacon, Inc., Boston, 1966.
N. Dunford and R. S. Schwartz
[1] Linear Operators. I, Interscience, New York, 1967.
[2] Linear Operators. II, Interscience, New York, 1963.
[3] Linear Operators. III, Interscience, New York, 1963.
J. Dyson and R. Villella-Bressan
[1] Functional diﬀerential equations and nonlinear evolution operators, Proc. Roy.
Soc. Edinburgh Sect. A 75 (1976), 223–234.
[2] Nonlinear functional diﬀerential equation in L1-spaces, Nonlinear Anal. 1
(1977), 383–395.
Yu. V. Egorov
[1] Some problems in the theory of optimal control, Soviet Math. 3 (1962), 1080–
1084.
[2] ˇZ. Vyˇcisl. Mat. i Mat. Fiz. 5 (1963), 887–904.

References
551
I. Ekeland and R. Temam
1. Analyse convexe et probl`emes variationnels, Dunod Gauthier-Villars, Paris,
1974.
A. El Jai and A. J. Pritchard
[1] Capteurs et actionneurs dans l’analyse des syst`emes distribu´es, Recherches en
Math´ematiques Appliqu´ees, vol. 3, Masson, Paris, 1986.
[2] Sensors and controls in the analysis of distributed systems, Wiley, New York,
1988.
D. H. Eller, J. K. Aggarwal, and H. T. Banks
[1] Optimal control of linear time delay systems, IEEE Trans. Automat. Control
AC-14 (1969), 678–687.
L. E. Els’golc
[1] Introduction to the theory of diﬀerential equations with deviating argument,
Izdat. Nauka, Moscow 1964 (in Russian); English transl., Holden-Day Inc.,
San Francisco, CA, London, Amsterdam, 1966.
H. O. Fattorini
[1] Some remarks on complete controllability, SIAM J. Control 4 (1966), 686–694.
[2] On complete controllability of linear systems, J. Diﬀerential Equations 3
(1967), 391–402.
[3] Boundary control systems, SIAM J. Control 6 (1968), 349–385.
[4] The Cauchy problem, Addison-Wesley Publishing Company, Reading, MA,
London, Amsterdam, 1983.
[5] Inﬁnite-dimensional optimization and control theory, Encyclopedia of Math-
ematics and its Applications, 62. Cambridge University Press, Cambridge,
England, 1999.
[6] Inﬁnite dimensional linear control systems. The time optimal and norm opti-
mal problems, North-Holland Mathematics Studies, 201. Elsevier Science B.V.,
Amsterdam, 2005.
H. O. Fattorini and D. L. Russell
[1] Exact controllability theorems for linear parabolic equations in one space di-
mension, Arch. Rational Mech. Anal. 43 (1971), 272–292.
P. Faurre
[1] Jeux diﬀ´erentiels `a strat´egie compl`etement optimale et principe de s´eparation,
IFAC World Congress, Warsaw, Poland, 1969.
A. Feintuch
[1] Robust Control Theory in Hilbert Space, Springer-Verlag, Berlin, New York,
1998.
W. Feller
[1] On the generation of unbounded semi-groups of bounded linear operators, Ann.
of Math. 58 (1953), no. 2, 166–174.
F. Flandoli
[1] Riccati equation arising in a boundary control problem with distributed param-
eters, SIAM J. Control Optim. 22 (1984), 76–86.
[2] Invertibility of Riccati operators and controllability of related systems, Systems
Control Lett. 9 (1987), 65–72.
[3] A new approach to L-Q-R problem for hyperbolic dynamics with boundary con-
trol, Proceedings of the 3rd International Conference, Distributed Parameter
Systems (F. Kappel, K. Kunish, and W. Schappacher, eds.), Lectures Notes in
Control and Inform. Sci., vol. 102 , Springer-Verlag, Berlin, Heidelberg, New
York, 1987, pp. 89–111.

552
References
[4] Algebraic Riccati equation arising in a boundary control problem with dis-
tributed parameters, SIAM J. Control Optim. 25 (1987), 612–636.
[5] A
counterexample
in
the
boundary
control
of
parabolic
systems,
Appl. Math. Lett. 3 (1990), no. 2, 47–50.
[6] On the semigroup approach to stochastic diﬀerential equations in Hilbert
spaces, Stochastic Anal. Appl. 10 (1992), no. 2, 181–204.
[7] On the direct solution of Riccati equations arising in boundary control theory,
Ann. Mat. Pura Appl. (4) 163 (1993), 93–131.
Cf. P. Acquistapace, F. Flandoli, and B. Terreni.
F. Flandoli, I. Lasiecka, and R. Triggiani
[1] Algebraic Riccati equations with non smooth observation arising in Hyperbolic
and Euler–Bernoulli boundary control problems, Ann. Mat. Pura Appl. (4)
153 (1988), 307–382.
I. Fl¨ugge-Lot
[1] Cf. D.W. Ross and I. Fl¨ugge-Lot.
C. Foias
[1] Cf. B. Sz.-Nagy and C. Foias.
B. A. Francis
[1] A course in H∞control theory, Lecture Notes in Control and Inform. Sci.,
vol. 88, Springer-Verlag, New York, 1987.
A. V. Fursikov and O. Yu. Imanuvilov
[1] Controllability of evolution equations, Lecture Notes Series, 34. Seoul National
University, Research Institute of Mathematics, Global Analysis Research Cen-
ter, Seoul, Korea, 1996.
J. S. Gibson
[1] The Riccati integral equations for optimal control problems on Hilbert spaces,
SIAM J. Control Optim. 17 (1979), 537–565.
E. Gilbert
[1] Controllability and observability in multivariable control systems, SIAM J.
Control 1 (1963), 128–151.
S. Goldberg
[1] Unbounded linear operators: theory and applications, McGraw-Hill, New York,
1966.
G. Greiner and R. Nagel
[1] Spectral theory, One-Parameter Semigroups of Positive Operators (R. Nagel,
ed.), Springer-Verlag, Berlin, Heidelberg, New York, Tokyo, 1986, pp. 60–97.
G. Grippenberg, S. O. Londen, and O. Staffans
[1] Volterra integral and functional equations, Cambridge University Press, Cam-
bridge, New York, Port Chester, Melbourne, Sydney, 1990.
P. Grisvard
[1] Caract´erisation de quelques espaces d’interpolation, Arch. Rational Mech.
Anal. 25 (1967), 40–63.
[2] Equations diﬀerentielles abstraites, Ann. Sci. ´Ecole Norm. Sup. (4) 2 (1969),
311–395.
Cf. G. Da Prato and P. Grisvard.
R. Gulliver, I. Lasiecka, W. Littman, and R. Triggiani
[1] The case for diﬀerential geometry in the control of single and coupled PDEs:
the structural acoustic chamber. Geometric methods in inverse problems and
PDE control, pp. 73–181, IMA Vol. Math. Appl., 137, Springer, New York,
2004.

References
553
A. Halanay
[1] Diﬀerential equations; stability, oscillations, time lags, Academic Press, New
York, 1966.
J. K. Hale
[1] Linear
functional-diﬀerential
equations
with
constant
coeﬃcients,
Con-
trib. Diﬀerential Equations 2 (1963), 291–319.
[2] Functional diﬀerential equations, Appl. Math. Series, vol. 3, Springer-Verlag,
Berlin, New York, 1971.
[3] Theory of functional diﬀerential equation, Springer–Verlag, New York, 1977.
J. K. Hale and C. Imaz
[1] Existence, uniqueness, continuity and continuation of solutions for retarded
diﬀerential equations, Bol. Soc. Mat. Mexicana (3), 11 (1966), Ser. 2, 29–37.
M. L. J. Hautus
[1] Controllability and observability conditions for linear autonomous systems,
Indag. Math. (N.S.) 31 (1969), 443–448.
[2] Stabilization, controllability and observability of linear autonomous systems,
Indag. Math. (N.S.) 32 (1970), 448–455.
H. J. A. M. Heijmans
[1] Semigroup theory for control on sun-reﬂexive Banach spaces, IMA J. Math.
Control Inform. 4 (1987), no. 2, 111–129.
D. Henry
[1] The adjoint of a linear functional diﬀerential equation and boundary value
problems, J. Diﬀerential Equations 9 (1971), 55–66.
[2] Linear autonomous functional diﬀerential equations of neutral type in Sobolev
space W (1)
2
, Technical report, Department of Mathematics, University of Ken-
tucky, Lexington, KY, 1971.
[3] Linear autonomous neutral functional diﬀerential equations, J. Diﬀerential
Equations 15 (1974), 106–128.
T. L. Herdman
Cf. J. A. Burns, T. L. Herdman, and H. R. Stech.
E. Hille
[1] Linear transformations in Hilbert space and their applications to analysis,
Amer. Math. Soc. Colloq. Publ., vol. XV, New York, 1932.
[2] Functional analysis and semi-groups, Amer. Math. Soc. Colloq. Publ., vol. 31,
New York, 1948.
E. Hille and R. S. Phillips
[1] Functional analysis and semigroups, Amer. Math. Soc. Colloq. Publ., vol. 31,
Providence, R.I., 1957.
L. F. Ho
[1] Observabilit´e fronti`ere de l’´equation des ondes, C. R. Acad. Sci. Paris S´er. I
Math. 302 (1986), no. 12, 443–446.
Y. Ho
Cf. R. E. Kalman, Y. C. Ho, and K. S. Narendra.
Y. Ho, A.E. Bryson, and S. Baron
[1] Diﬀerential games and optimal pursuit-evasion strategies, IEEE Trans. Au-
tomat. Control AC-10 (1965), 385–389.
J. Horv´ath
[1] Topological vector spaces and distributions, vol. I, Addison-Wesley, Reading,
MA, 1966.

554
References
M. Iannelli
Cf. G. Da Prato and M. Iannelli.
A. Ichikawa
[1] Evolution equations with delays, Report 52, Control Theory Centre, University
of Warwick, Coventry, England, 1977.
[2] Optimal (quadratic) control and ﬁltering for evolution equations with delay
in control and observation, Report 53, Control Theory Centre, University of
Warwick, Coventry, England, 1977.
[3] Quadratic control of evolution equations with delay in control, SIAM J. Control
Optim. 20 (1982), 645–668.
Cf. G. Da Prato and A. Ichikawa.
C. Imaz
Cf. J. K. Hale and C. Imaz.
K. Ito
[1] Linear functional diﬀerential equations and control and estimation problems,
Ph.D. dissertation, Washington University, St-Louis, MO, 1981.
M. Q. Jacobs
Cf. H. T. Banks and M. Q. Jacobs.
Cf. H. T. Banks, M. Q. Jacobs, and C. E. Langenhop.
Cf. H. T. Banks, M. Q. Jacobs, and M. R. Latina.
M. Q. Jacobs and Ti-Jeun Kao
1. An optinum settling problem for time lag systems, J. Math. Anal. Appl. 40
(1972), 687–707.
G. S. Jones
[1] Hereditary structure in diﬀerential equations, Math. Systems Theory 1 (1967),
263–278.
R. E. Kalman
[1] On the general theory of control systems, Proc. First IFAC Congress Auto-
matic Control (Moscow), vol. I, Butterworths, London, 1960, pp. 481–492.
[2] Contributions to the theory of optimal control, Bol. Soc. Mat. Mexicana (3) 5
(1960), 102–119.
[3] Mathematical description of linear dynamical systems, SIAM J. Control 1
(1963), 152–192.
R. E. Kalman, Y. C. Ho, and K. S. Narendra
[1] Controllability of linear dynamical systems, Contrib. Diﬀerential Equations 1
(1963), 189–213.
L. V. Kantorovich and G. P. Akilov
[1] Functional analysis and semigroups, Amer. Math. Soc., Providence, R.I., 1957.
F. Kappel
[1] Laplace-transform methods and linear autonomous functional diﬀerential
equations, Ber. Math.-Statis. Sekt. Forsch. Graz 64 (1976), 1–62.
J. Karrakchou
[1] Analyse
et
commande
des
syst`emes
diﬀ´erentiels
fonctionnels
de
type
h´er´editaire, Th`ese de doctorat, Universit´e de Montr´eal, Qu´ebec, Canada, 1984.
Cf. M. C. Delfour and J. Karrakchou.
T. Kato
[1] Notes on fractional powers of linear operators, Proc. Japan Acad. 36 (1960),
94–96.
[2] Fractional powers of dissipative operators, J. Math. Soc. Japan 13 (1961),
246–274.

References
555
[3] Fractional powers of dissipative operators. II, J. Math. Soc. Japan 14 (1962),
243–248.
[4] Perturbation theory for linear operators, Springer Verlag, Berlin, New York,
1966.
[5] Perturbation theory for linear operators, 2nd ed., Springer-Verlag, Berlin,
1976.
G. A. Kent
Cf. H. T. Banks and G. A. Kent.
G. L. Kharatishvili
[1] Optimal processes with retardations, Sakharth. SSR Mech. Akad. Gamothvl.
Centr. 6 (1965), (in Russian).
[2] Optimal processes with a delay, Tbilisi, Georgia, 1966.
H. Koivo and E. B. Lee
[1] Controller systhesis for linear systems with retarded state and control variables
and quadratic cost, Automatica 8 (1972), 203–208.
N. N. Krasovskii
[1] On the analytic construction of an optimal control in a system with time lags,
Prikl. Mat. Mekh. 26 (1962), 39–51; English transl., J. Appl. Math. Mech. 26
(1962), 50–67.
[2] Optimal processes in systems with time lags, Proc. Second IFAC Congress,
Basel, Switzerland, 1963.
[3] Stability of motion, application of Lyapunov’s second method to diﬀerential
systems and equations with delays, Stanford University Press, Stanford, CA,
1963, (translated from Russian).
N. N. Krasovskii and Yu. S. Osipov
[1] The stabilization of motion of a controllable object with time lag in a control
system, Tekhnicheskaya Kibernetika 6 (1963).
[2] Stabilization of a controlled system with time delay, Izv. Adal. Nauk. USSR.
6 (1963); Engl. transl., Engrg. Cybernetics.
M. Krein
Cf. I. Daleckii and M. Krein.
K. Kunisch
Cf. G. Di Blasio, K. Kunisch, and E. Sinestrari.
Cf. H. T. Banks and K. Kunish.
K. Kunisch and M. Mastinsek
[1] Dual semigroups and structural operators for partial diﬀerential equations with
unbounded operators acting on the delays, Report no. 121–1988, Institut fur
Mathematik, Universitat Graz, Graz, Austria, 1988.
K. Kunisch and W. Schappacher
[1] Necessary conditions for partial diﬀerential equations with delays to generate
C0-semigroups, J. Diﬀerential Equations 50 (1983), 49–79.
H. J. Kuschner and D. I. Barnea
[1] On the control of a linear functional diﬀerential equation with quadratic cost,
SIAM J. Control 8 (1970), 257–272.
R. H. Kwong
[1] The linear quadratic Gaussian problem for systems with delays in the state,
control and observations, Proceedings of the 14th Allerton Conference on Cir-
cuit and System Theory, Illinois, 1976, pp. 545–549.

556
References
[2] A linear-quadratic Gaussian theory for systems with delays in the state, control
and observations, Systems Control Group, University of Toronto, Canada,
Report 7714, 1977.
[3] Characterization of kernel functions associated with operator algebraic Riccati
equations for linear delay systems, Systems Control, University of Toronto,
Canada, Report 7906, 1979.
[4] A stability theory of the linear-quadratic-Gaussian problem for systems with
delays in the state, control, and observations, SIAM J. Control Optim. 18
(1980), 49–75.
Cf. R. B. Vinter and R. H. Kwong.
R. H. Kwong and A. D. Willsky
[1] Optimal ﬁltering and ﬁlter stability of linear stochastic delay systems,IEEE
Trans. Automat. Control AC-22 (1977), 196–201.
[2] Estimation and ﬁlter stability of stochastic delay systems, SIAM J. Control
Optim. 16 (1978), 660–681.
J. E. Lagnese
[1] Boundary stabilization of thin plates, SIAM Stud. Appl. Math., vol. 10, SIAM,
Philadelphia, 1989.
[2] Exact controllability of Maxwell’s equations in a general region, SIAM J. Con-
trol Optim. 27 (1989), 374–388.
[3] A singular perturbation problem in exact controllability of the Maxwell system,
ESAIM Control Optim. Calc. Var. 6 (2001), 275–289.
J. E. Lagnese and J. L. Lions
[1] Modelling, analysis and control of thin plates, Recherches en Math´ematiques
Appliqu´ees, vol. 6, Masson, Paris, Milan, Barcelone, Mexico, 1988.
C. E. Langenhop
Cf. H. T. Banks, M. Q. Jacobs, and C. E. Langenhop.
J. P. LaSalle
[1] The time optimal control problem, Theory of Nonlinear Oscillations, vol. 5,
Princeton University Press, Princeton, N.J., 1959, pp. 1–24.
I. Lasiecka
[1] Uniﬁed theory for abstract parabolic boundary problems: a semigroup approach,
Appl. Math. Optim. 6 (1980), 287–333.
[2] Optimal control problems and Riccati equations for systems with unbounded
controls and partially analytic generators—applications to boundary and point
control problems, Functional analytic methods for evolution equations, pp.
313–369, Lecture Notes in Math., 1855, Springer, Berlin, 2004.
Cf. P. Acquistapace, F. Bucci, and I. Lasiecka.
Cf. F. Bucci, I. Lasiecka, and R Triggiani.
Cf. G. Da Prato, I. Lasiecka, and R. Triggiani.
Cf. F. Flandoli, I. Lasiecka, and R. Triggiani.
Cf. R. Gulliver, I. Lasiecka, and W. Littman.
I. Lasiecka, J. L. Lions, and R. Triggiani
[1] Non homogeneous boundary value problems for second order hyperbolic opera-
tors, J. Math. Pures Appl. 65 (1986), 149–192.
I. Lasiecka and A. Manitius
[1] Diﬀerentiability and convergence rates of approximating semigroups for re-
tarded functional diﬀerential equations, SIAM J. Numer. Anal. 25 (1988),
883–907.

References
557
I. Lasiecka and R. Triggiani
[1] A cosine operator approach to modelling L2
`
0, T; L2(Γ)
´
-boundary input hy-
perbolic equations, Appl. Math. Optim. 7 (1981), 35–93.
[2] Regularity of hyperbolic equations under L2(0, T ; L2(Γ))-Dirichlet boundary
terms, Appl. Math. Optim. 10 (1983), 275–286.
[3] Dirichlet boundary control problems for parabolic equations with quadratic
cost; analyticity and Riccati’s feedback synthesis, SIAM J. Control Optim. 21
(1983), 41–67.
[4] Stabilization and structural assignment of Dirichlet boundary feedback para-
bolic equations, SIAM J. Control Optim. 21 (1983), 766–803.
[5] Stabilization of Neumann boundary feedback parabolic equations: the case of
trace in the feedback loop, Appl. Math. Optim. 10 (1983), 307–350.
[6] Riccati
equations
for
hyperbolic
partial
diﬀerential
equations
with
L2(0, T; L2(Γ))-Dirichlet boundary terms, SIAM J. Control Optim. 24
(1986), no. 5, 41–68.
[7] The regulator problem for parabolic equations with Dirichlet boundary con-
trol. I. Riccati’s feedback synthesis, Appl. Math. Optim. 16 (1987), 147–168.
[8] The regulator problem for parabolic equations with Dirichlet boundary con-
trol. II. Galerkin approximation, Appl. Math. Optim. 16 (1987), 187–216.
[9] Exact controllability for the wave equation with Neumann boundary control,
Appl. Math. Optim. 19 (1989), 263–290.
[10] Exact controllability of the Euler–Bernoulli equation with controls in the
Dirichlet and Neumann boundary conditions: a non conservative code, SIAM
J. Control Optim. 27 (1989), 330–373.
[11] Diﬀerential and algebraic Riccati equations with applications to bound-
ary/point control problems: continuous theory and approximation theory, Lec-
ture Notes in Control and Inform. Sci., vol. 164, Springer-Verlag, Berlin, Hei-
delberg, New York, 1991.
[12] Diﬀerential Riccati equations with unbounded coeﬃcients: applications to
boundary control/boundary observation of hyperbolic problems, Nonlinear
Anal. 17 (1991), no. 7, 655–682.
[13] Exact controllability and uniform stabilization of Kirchoﬀplates with bound-
ary controls only in ∆w |Σ, J. Diﬀerential Equations 93 (1991), 62–101.
[14] Riccati diﬀerential equations with unbounded coeﬃcients and non smoothing
terminal condition—the case of analytic semigroups, SIAM J. Math. Anal. 23
(1992), no. 2, 449–481.
[15] Control theory for partial diﬀerential equations: continuous and approxima-
tion theories. I. Abstract parabolic systems, Encyclopedia of Mathematics and
its Applications, 74. Cambridge University Press, Cambridge, England, 2000.
[16] Control theory for partial diﬀerential equations: continuous and approxima-
tion theories. II. Abstract hyperbolic-like systems over a ﬁnite time horizon,
Encyclopedia of Mathematics and its Applications, 75. Cambridge University
Press, Cambridge, England, 2000.
M. R. Latina
Cf. H. T. Banks, M. Q. Jacobs, and M.R. Latina.
P. Lax and R. S. Phillips
[1] Scattering theory for automorphic functions, Annals of Mathematics Studies,
No. 87. Princeton Univ. Press, Princeton, NJ, 1976.
G. Lebeau

558
References
Cf. C. Bardos, G. Lebeau, and J. Rauch.
E. B. Lee
Cf. Y. Alekal, P. Brunosvsky, D. H. Chyung, and E. B. Lee.
Cf. D. H. Chyung and E. B. Lee.
Cf. M. C. Delfour, E. B. Lee, and A. Manitius.
Cf. H. Koivo and E. B. Lee.
A. F. Leont’ev
[1] Diﬀerential-diﬀerence equations, Mat. Sb. 24 (1949), 347–374; English transl.,
Amer. Math. Soc. Translation no. 78, 1952.
B. W. Levinger
[1] A folk theorem in functional diﬀerential equations, J. Diﬀerential Equations 4
(1968), 612–619.
N. Levinson and C. McCalla
[1] Completeness and independence of the exponential solutions of some func-
tional diﬀerential equations, Stud. Appl. Math. 53 (1974), 1–15.
X. Li and J. Yong
[1] Optimal Control Theory for Inﬁnite Dimensional Systems, Systems & Control:
Foundations & Applications, Birkh¨auser Boston, Inc., Boston, MA, 1995.
J. L. Lions
[1] Equations diﬀ´erentielles op´erationnelles et probl`emes aux limites, Springer
Verlag, Berlin, New York, 1962.
[2] Espaces d’interpolation et domaines de puissances fractionnaires d’op´erateurs,
J. Math. Soc. Japan 14 (1962), no. 2, 233–241.
[3] Optimal control of systems described by partial diﬀerential equations, Springer-
Verlag, New York, Heidelberg, Berlin, 1971.
[4] Exact controllability, stabilization and perturbations, SIAM Rev. 30 (1988),
1–68.
[5] Contrˆolabilit´e exacte, perturbations et stabilisation des syst`emes distribu´es,
vols. 1 and 2, Masson, Paris, 1988.
[6] Personal communication.
Cf. A. Bensoussan and J. L. Lions.
Cf. J. E. Lagnese and J. L. Lions.
Cf. I. Lasiecka, J. L. Lions, and R. Triggiani.
J. L. Lions and E. Magenes
[1] Probl`emes aux limites non homog`enes et epplications, vols. 1 and 2, 1968;
vol. 3, 1969, Dunod, Paris.
J. L. Lions and J. Peetre
[1] Sur une classe d’espaces d’interpolation, Publ. Math de l’I.H.E.S. 19 (1964),
5–68.
W. Littman
[1] A generalization of a theorem of Datko and Pazy, Advances in computing and
control (Baton Rouge, LA, 1988), Lecture Notes in Control and Inform. Sci.,
130, Springer, Berlin, 1989, pp. 318–323.
Cf. R. Gulliver, I. Lasiecka, and W. Littman.
W. Littman and L. Markus
[1] Some recent results on control and stabilization of ﬂexible structures, Stabi-
lization of Flexible Structures (A.V. Balakrishnan and J. P. Zol´esio, eds.),
Optimization Software, Inc., New York, Los Angeles, 1988, pp. 151–161.
S. O. Londen

References
559
Cf. G. Grippenberg, S. O. Londen, and O. Staffans.
D. G. Luenberger
[1] An introduction to observers, IEEE Trans. Automat. Control 16 (1971), 596–
602.
D. L. Lukes and D. L. Russell
[1] The quadratic criterion for distributed systems, SIAM J. Control 7 (1969),
101–121.
G. Lumer and R. S. Phillips
[1] Dissipative operators in a Banach space, Paciﬁc J. Math. 11 (1961), 679–698.
A. Lunardi
[1] Interpolation spaces between domains of elliptic operators and spaces of con-
tinuous functions with applications to nonlinear parabolic equations, Math.
Nachr. 121 (1985), 295–318.
[2] Bounded solutions of linear periodic abstract parabolic equations, Proc. Roy.
Soc. Edinburgh Sect. A 110 (1988), 135–159.
[3] Diﬀerentiability with respect to (t, s) of the parabolic evolution operator, Israel
J. Math. 68 (1989), no. 2, 161–184.
[4] Stabilizability of time parabolic equations, SIAM J. Control Optim. 29 (1991),
no. 4, 810–828.
[5] Analytic semigroups and optimal regularity in parabolic problems, Progress
in Nonlinear Diﬀerential Equations and their Applications, 16. Birkh¨auser
Verlag, Basel, Switzerland, 1995.
Cf. G. Da Prato and A. Lunardi.
R. C. MacCamy, V. J. Mizel, and T. I. Seidman
[1] Approximate boundary controllability of the heat equation, J. Math. Anal.
Appl. 23 (1968), 699–703.
E. Magenes
Cf. J. L. Lions and E. Magenes.
A. Manitius
[1] Optimal control of processes with state variable delay, Ph.D. dissertation (Pol-
ish), Technical University of Warsaw, Poland, 1968.
[2] Optimal control of time-lag systems with quadratic performance indexes,
Proc. Fourth IFAC Congress, Warsaw, Poland, 1969.
[3] Optimal control of processes with time delays—a survey and some new results,
Arch. Automat. i Telemech. 15 (1970), 205–221.
[4] Optimal control of hereditary systems, Control Theory and Topics in Func-
tional Analysis, vol. 3, International Atomic Energy Agency Vienna, Austria,
1976, pp. 43–178.
[5] Controllability, observability and stabilizability of retarded systems, Proc. 1976
IEEE Conference on Decision and Control, IEEE Publications, New York,
1976, pp. 752–758.
[6] Completeness and F-completeness of eigenfunctions associated with retarded
functional diﬀerential equations, J. Diﬀerential Equations 35 (1980), 1–29.
[7] Necessary and suﬃcient conditions of approximate controllability for general
linear retarded systems, SIAM J. Control Optim. 19 (1981), 516–532.
[8] F-controllability and observability of linear retarded systems, J. Appl. Math.
Optim. 9 (1982), 73–95.
Cf. H. T. Banks and A. Manitius.
Cf. C. Bernier and A. Manitius.

560
References
Cf. F. Colonius, A. Manitius, and D. Salamon.
Cf. M. C. Delfour, E. B. Lee, and A. Manitius.
Cf. M. C. Delfour and A. Manitius.
Cf. I. Lasiecka and A. Manitius.
A. Manitius and R. Triggiani
[1] Function space controllability of linear retarded systems: a derivation from
abstract operator conditions, SIAM J. Control Optim. 16 (1978), 599–645.
L. Markus
[1] L. Markus.
Cf. W. Littman and L. Markus.
M. Mastinsek
[1] Structural operators for abstract functional diﬀerential equations, Ph.D. dis-
sertation, Univ. E. Kardelja, Ljubljana, Yugoslavia, 1987.
Cf. K. Kunisch and M. Mastinsek.
C. McCalla
Cf. M. C. Delfour, C. McCalla, and S. K. Mitter.
Cf. N. Levinson and C. McCalla.
P. K. Medina
[1] Feedback stabilizability of time–periodic parabolic equations, University of
Z¨urich, Ph.D. Thesis, 1991.
D. H. Mee
[1] An extension of predictor control for systems with control time-delays, Inter-
nat. J. Control 18 (1973), 1151–1168.
D. H. Miller
[1] Linear Volterra integro-diﬀerential equations as semigroups, Funkcial Ek-
vac. 17 (1974), 39–55.
S. K. Mitter
[1] Successive approximation methods for the solution of optimal control problems,
Automatica 3 (1966), 135–149.
Cf. A. Bensoussan, M. C. Delfour, and S. K. Mitter.
Cf. M. C. Delfour, C. McCalla, and S. K. Mitter.
Cf. M. C. Delfour and S. K. Mitter.
Cf. J. D. Simon and S. K. Mitter.
S. K. Mitter and J. S. Shamma
[1] An elementary proof of an H∞-optimal state feedback result, unpublished man-
uscript, 1989.
I. Miyadera
[1] Generation of a strongly continuous semi-group of operators, Tˆohoku Math.
J. 4 (1952), 109–114.
V. J. Mizel
Cf. B. D. Coleman and V. J. Mizel.
Cf. R. C. MacCamy, V. J. Mizel, and T. I. Seidman.
V. J. Mizel and T. I. Seidman
[1] Observation and prediction for the heat equation, J. Math. Anal. Appl. 28
(1969), 303–312.
[2] Observation and prediction for the heat equation. II, J. Math. Anal. Appl. 38
(1972), 149–166.
S. Mizohata
[1] Unicit´e du prolongement des solutions pour quelques op´erateurs diﬀ´erentiels
paraboliques, Mem. Coll. Sci. Univ. Kyoto. Ser. A. Math. 31 (1958), 219–239.

References
561
L. A. Monauni
[1] Exponential decay of solutions to Cauchy’s abstract problem as determined
by the extended spectrum of the dynamical operator, unpublished manuscript,
1981.
O. Morgenstern
Cf. J. von Neumann and O. Morgenstern.
T. Morozan
[1] Almost-periodic solutions for Riccati equations of stochastic control, Appl.
Math. Optim. 30 (1994), no. 2, 127–133.
C. E. Mueller
[1] Optimal feedback control for hereditary processes, Ph.D. Thesis, University of
Minnesota, Minneapolis, MN, 1971.
A.D. Myshkis
[1] General theory of diﬀerential equations with a retarded argument, Uspehi Mat.
Nauk (N.S.) 4 (1949), 99–141 (in Russian); English transl., J. Diﬀerential
Equations, Amer. Math. Soc. Ser. 1, vol. 4, Amer. Math. Soc. Providence
R.I., 1962, pp. 207–267.
[2] Linear diﬀerential equations with retarded argument, Moscow, 1951; English
transl., Gordon and Breach Science Publishers, Inc., New York, 1966.
[3] Linear diﬀerential equations with retarded argument, 2nd ed., Izdat. Nauka,
Moscow, 1972 (in Russian).
R. Nagel
[1] Zur Charakterisierung stabiler Operator halbgruppen, Semesterbericht Funk-
tionalanalysis, T¨ubingen, 1981–1982.
Cf. G. Greiner and R. Nagel.
T. Nanbu
[1] On the stabilization of diﬀusion equations, J. Diﬀerential Equations 52 (1884),
204–233.
K. S. Narendra
Cf. R. E. Kalman, Y. C. Ho, and K. S. Narendra.
J. Nash
[1] Equilibrium points in n-person games, Proceedings of the National Academy
of the USA 36 (1950), no. 1, 48–49.
M. Negreanu and E. Zuazua
[1] Uniform boundary controllability of a discrete 1-D wave equation. Optimiza-
tion and control of distributed systems, Systems Control Lett. 48 (2003), no.
3-4, 261–279.
F. Neurander
[1] Stability of positive semigroups on C0(X), One-Parameter Semigroups of Posi-
tive Operators (R. Nagel, ed.), Springer-Verlag, Berlin, Heidelberg, New York,
Tokyo, 1986, pp. 204–208.
J. A. Nohel
[1] Problems in qualitative behaviour of solutions of nonlinear Volterra equations,
Nonlinear Integral Equations, University of Wisconsin Press, Madison, WI,
1964, pp. 191–214.
M. N. Oguzt¨oreli
[1] Time-lag control systems, Academic Press, New York, 1966.
A. W. Olbrot
[1] Conditions for the existence of stabilizing control for linear processes with
delays in control, Arch. Automat. i Telemech. 17 (1972), 133–147.

562
References
[2] Stabilizability, detectability, and spectrum assignment for linear autonomous
systems with general time delays, IEEE Trans. Automat. Control AC-23
(1978), 887–890.
Yu. S. Osipov
[1] Stabilization of controlled systems with delays, Diﬀerential’nye Uravneniya 1
(1965), 605–618; English transl., J. Diﬀerential Equations 1 (1965), 463–473.
Cf. N. N. Krasovskii and Yu. S. Osipov.
L. Pandolfi
[1] On feedback stabilization of functional diﬀerential equations, Boll. Un. Mat.
Ital. (4), 11 (1975), 626–635.
[2] Canonical realization of systems with delayed controls, Ricerche Automat. 10
(1979), 27–37.
A. Pazy
[1] On the applicability of Lyapunov’s theorem in Hilbert space, SIAM J. Math.
Anal. 3 (1972), 291–294.
[2] Semigroups of linear operators and applications to partial diﬀerential equa-
tions, Springer-Verlag, New York, Berlin, Heidelberg, Tokyo, 1983.
J. Peetre
Cf. J. L. Lions and J. Peetre.
R. S. Phillips
[1] An inversion formula for Laplace transforms and semigroups of linear opera-
tors, Ann. of Math. 59 (1954), no. 2, 325–356.
[2] Dissipative operators and hyperbolic systems of partial diﬀerential equations,
Trans. Amer. Math. Soc. 90 (1959), 193–254.
Cf. E. Hille and R. S. Phillips.
Cf. P. Lax and R. S. Phillips.
Cf. G. Lumer and R. S. Phillips.
E. Pinney
[1] Ordinary diﬀerence-diﬀerential equations, University of California Press,
Berkeley, CA, 1958.
V. M. Popov
[1] Pointwise degeneracy of linear, time-invariant, delay-diﬀerential equations, J.
Diﬀerential Equations 11 (1972), 541–561.
A. J. Pritchard
Cf. R. F. Curtain and A. J. Pritchard.
Cf. A. El Jai and A. J. Pritchard.
A. J. Pritchard and D. Salamon
[1] The linear quadratic optimal control problem for inﬁnite dimensional sys-
tems with unbounded input and output operators, Inﬁnite-Dimensional Sys-
tems, Springer-Verlag, New York 1984; Lecture Notes in Math., no. 1076,
pp. 187–202.
A. J. Pritchard and J. Zabczyk
[1] Stability and stabilizability of inﬁnite dimensional systems, SIAM Rev. 23
(1981), 25–52.
J. Rauch
Cf. C. Bardos, G. Lebeau, and J. Rauch.
W. H. Ray
[1] The optimal control of processes modeled by transfer functions containing pure
time delays, Chemical Engrg. Sci. 24 (1969), 209–216.

References
563
Cf. M. A. Soliman and W. H. Ray.
W. H. Ray and M. A. Soliman
[1] The optimal control of processes containing pure time delays. I. Necessary
conditions for an optimum, Chemical Engrg. Sci. 25 (1970), 1911–1925.
P. B. Reddy
Cf. P. Sannuti and P. B. Reddy.
P. B. Reddy and P. Sannuti
[1] Optimal control of singularly perturbed time-delay systems with an application
to a coupled core nuclear reactor, Proc. IEEE Conf. on Decision and Control,
Phoenix, AZ, 1974, pp. 793–803.
P. Ricciardi
Cf. A. Ardito and P. Ricciardi.
F. Riesz and B. Sz.-Nagy
[1] Le¸cons d’analyse fonctionnelle, Gauthier-Villars, Paris, 1965.
S. Rolewicz
[1] On uniform N-equistability, J. Math. Anal. Appl. 115 (1986), 434–441.
D. W. Ross
[1] Controller design for time lag systems via a quadratic criterion, IEEE Trans.
Automat. Control AC-16 (1971), 664–672.
D. W. Ross and I. Fl¨ugge–Lot
[1] Optimal control of systems described by diﬀerential-diﬀerence equations, Divi-
sion of Engineering Mechanics, Report no. 177, Stanford University, Stanford,
CA, 1967.
[2] An optimal control problem for systems with diﬀerential-diﬀerence equation
dynamics, SIAM J. Control 7 (1969), 609–623.
W. Rudin
[1] Real and complex analysis, McGraw-Hill, New York, 1970.
D. L. Russell
[1] Controllability and stabilizability theory for linear partial diﬀerential equa-
tions: Recent progress and open questions, SIAM Rev. 20 (1978), no. 4, 639–
739.
Cf. H. O. Fattorini and D. L. Russell.
Cf. D. L. Lukes and D. L. Russell.
D. Salamon
[1] On control and observation of neutral systems, Ph.D. Thesis, Bremen Univer-
sity, 1982.
[2] Poster session, SIAM Winter Meeting, Blacksburg, VA, 1983.
[3] Control and observation of neutral systems, Pitman Advanced Publishing Pro-
gram, Boston, London, Melbourne, 1984.
[4] Structure and stability of ﬁnite dimensional approximations for functional dif-
ferential equations, SIAM J. Control Optim. 23 (1985), 928–951.
Cf. A. J. Pritchard and D. Salamon.
Cf. F. Colonius, A. Manitius, and D. Salamon.
P. Sannuti
Cf. P. B. Reddy and P. Sannuti.
P. Sannuti and P. B. Reddy
[1] Asymptotic series solution of optimal systems with small time delays, IEEE
Trans. Automat. Control AC-18 (1973), 250–259.
J. J. Sch¨affer

564
References
Cf. C. V. Coffman and J. J. Schaffer.
W. Schappacher
[1] Asymptotic behaviour of linear C0-semigroups, Dipartimento di Matematica,
Universit´a degli studi di Bari, 70100-Bari, Italia, 1983.
Cf. K. Kunisch and W. Schappacher.
M. Schechter
[1] Principles of functional analysis, Academic Press, New York, London, 1971.
E. J. P. G. Schmidt and N. Weck
[1] On the boundary behaviour of solutions to elliptic and parabolic equations,
with applications to boundary control for parabolic equations, SIAM J. Control
Optim. 16 (1978), 593–598.
J. T. Schwartz
Cf. N. Dunford and J. T. Schwartz.
T. I. Seidman
[1] A well-posed problem for the heat equation, Bull. Amer. Math. Soc. 80 (1974),
901–902.
[2] Observation and prediction for the heat equation. III, J. Diﬀerential Equations
20 (1976), 18–27.
[3] Observation and prediction for the heat equation. IV. Patch observability and
controllability, SIAM J. Control 15 (1977), 412–427.
[4] Time invariance of the reachable set for linear control problems, J. Math. Anal.
Appl. 72 (1979), 17–20.
Cf. R. C. MacCamy, V. J. Mizel, and T. I. Seidman.
Cf. V. J. Mizel and T. I. Seidman.
M. A. Shayman
[1] Phase portrait of the matrix Riccati equation, SIAM J. Control Optim. 24
(1986), 1–64.
S. N. Shimanov
[1] On the theory of linear diﬀerential equation with after eﬀect, Diﬀerential’nye
Uraveniya 1 (1965), 102–116; English transl., J. Diﬀerential Equations 1
(1965), pp. 76–87.
J. D. Simon and S. K. Mitter
[1] A theory of modal control, Inform. and Control 13 (1968), no. 4, 316–353.
E. Sinestrari
[1] On the abstract Cauchy problem of parabolic type in spaces of continuous func-
tions, J. Math. Anal. Appl. 107, (1985), 16–66.
Cf. G. Di Blasio, K. Kunisch, and E. Sinestrari.
M. Slemrod
[1]
Asymptotic behavior of C0 semi-groups as determined by the spectrum of the
generator, Indiana Univ. Math. J. 25 (1976), no. 8, 783–792.
M. A. Soliman
Cf. W. H. Ray and M. A. Soliman.
M. A. Soliman and W. H. Ray
[1] Optimal control of multivariable systems with pure time delays, Automatica 7
(1971), 681–689.
[2] Optimal feedback control for linear-quadratic systems having time delays, In-
ternat. J. Control 15 (1972), 609–627.
[3] On the optimal control of systems having pure time delays and singular arcs. I.
Some necessary conditions for optimality, Internat. J. Control 16 (1972), 963–
976.

References
565
[4] The optimal control of processes containing pure time delays. II. Transfer func-
tion models, Chemical Engrg. Sci. 27 (1972), 2183–2188.
[5] On the optimal control of systems having pure time delays and singular arcs.
II. Computational considerations, Internat. J. Control 18 (1973), 773–783.
M. Sorine
[1] Un r´esultat d’existence et d’unicit´e pour l’´equation de Riccati stationnaire,
CRMA–984, Universit´e de Montr´eal, Qu´ebec, Canada, 1980.
[2] Sur le semigroupe non lin´eaire associ´e `a l’´equation de Riccati, CRMA–1055,
Universit´e de Montr´eal, Qu´ebec, Canada, 1981.
Cf. M. C. Delfour and M. Sorine.
O. J. Staffans
[1] Extended initial and forcing function semigroups generated by a functional
equation, SIAM J. Math. Anal. 16 (1985), 1034–1048.
[2] Well-posed linear systems, Encyclopedia of Mathematics and its Applications,
Cambridge University Press, Cambridge, England, 2005.
Cf. G. Grippenberg, S. O. Londen, and O. Staffans.
H. R. Stech
Cf. J. A. Burns, T. L. Herdman, and H. R. Stech.
M. H. Stone
[1] On one parameter unitary group in Hilbert space, Ann. Math. 33 (1932), 643–
648.
B. Sz.-Nagy
Cf. F. Riesz and B. Sz.-Nagy.
B. Sz.-Nagy and C. Foias
[1] Sur les contractions de l’espace de Hilbert. XI. Transformations unicellulaires,
(French) Acta Sci. Math. (Szeged) 26 (1965), 301–324.
[2] Correction: “Sur les contractions de l’espace de Hilbert. XI. Transformations
unicellulaires”, (French) Acta Sci. Math. (Szeged) 27 (1966), 265.
[3] Sur les contractions de l’espace de Hilbert. XII. Fonctions int´erieures admet-
tant des facteurs ext´erieurs, (French) Acta Sci. Math. (Szeged) 27 (1966),
27–33.
[4] Correction: “Sur les contractions de l’espace de Hilbert. XI. Transformations
unicellulaires”, (French) Acta Sci. Math. (Szeged) 27 (1966), 265.
[5] Similitude des op´erateurs de classe Cρ `a des contractions, (French) C. R. Acad.
Sci. Paris S´er. A-B 264 (1967), A1063–A1065.
[6] ´Echelles continues de sous-espaces invariants, (French) Acta Sci. Math.
(Szeged) 28 (1967), 213–220.
[7] Analyse harmonique des op´erateurs de l’espace de Hilbert, (French) Masson et
Cie, Paris; Akad´emiai Kiad´o, Budapest 1967.
[8] Forme triangulaire d’une contraction et factorisation de la fonction car-
act´eristique, (French) Acta Sci. Math. (Szeged) 28 (1967), 201–212.
H. Tanabe
[1] Equations of evolution, Pitman, London, 1979; English transl., Iwanami,
Tokyo, 1975.
L. Tartar
[1] Interpolation non-lin´eaire et r´egularit´e, J. Funct. Anal. 9 (1972), 469–489.
[2] Sur l’´etude directe d’´equations non lin´eaires int´ervenant en th´eorie du contrˆole
optimal, J. Funct. Anal. 17 (1974), 1–46.
A. E. Taylor

566
References
[1] Introduction to functional analysis, Robert E. Krieger Publishing Company,
2nd ed., Reprint ed., Malabar, FL, 1986.
R. Temam
[1] Sur l’´equation de Riccati associ´ee `a des op´erateurs non born´es en dimension
inﬁnie, J. Funct. Anal. 7 (1971), 85–116.
Cf. I. Ekeland and R. Temam.
B. Terreni
Cf. P. Acquistapace, F. Flandoli, and B. Terreni.
Cf. P. Acquistapace and B. Terreni.
Ti-Jeun Kao
Cf. M .Q. Jacobs and Ti-Jeun Kao.
C. C. Travis and G. F. Webb
[1] Existence and stability for partial functional diﬀerential equations, Trans.
Amer. Math. Soc. 200 (1974), 395–418.
[2] Partial diﬀerential equations with deviating arguments in the time variable, J.
Math. Anal. Appl. 56 (1976), 397–409.
[3] Existence, stability and compactness in the α-norm for partial functional dif-
ferential equations, Trans. Amer. Math. Soc. 240 (1978), 129–143.
F. Treves
[1] Basic linear partial diﬀerential equations, Academic Press, New York, 1975.
H. Triebel
[1] Interpolation theory, function spaces, Diﬀerential Operators, North-Holland
Publishing Company, Amsterdam, New York, Oxford, 1978.
R. Triggiani
[1] On a lack of exact controllability for mild solutions in Banach spaces, J. Math.
Anal. Appl. 50 (1975), 438–446.
[2] On the stabilizability problem in Banach space, J. Math. Anal. Appl. 52 (1975),
383–403.
[3] On Nambu’s boundary stabilization problem for diﬀusion processes, J. Diﬀer-
ential Equations 33 (1979), 189–200.
[4] Exact boundary controllability on L2(Ω) × H−1(Ω) for the wave equation with
Dirichlet control acting on a portion of the boundary, and related problems,
Appl. Math. Optim. 18 (1988), 241–277.
[5] The dual algebraic Riccati equations: additional results under isomorphism of
the Riccati operator, Appl. Math. Lett. 18 (2005), no. 9, 1001–1008.
Cf. F. Bucci, I. Lasiecka, and R Triggiani.
Cf. S. Chen and R. Triggiani.
Cf. G. Da Prato, I. Lasiecka, and R. Triggiani.
Cf. F. Flandoli, I. Lasiecka, and R. Triggiani.
Cf. I. Lasiecka, J. L. Lions, and R. Triggiani.
Cf. I. Lasiecka and R. Triggiani.
Cf. A. Manitius and R. Triggiani.
A.S. Turbabin
Cf. Ju.G. Borisovic and A.S. Turbabin.
B. van Keulen
[1] H∞-control or distributed parameter systems: a state-space approach, Systems
& Control: Foundations & Applications. Birkh¨auser Boston, Inc., Boston, MA,
1993.
H. F. Vandevenne

References
567
[1] Controllability and stabilizability properties of delay systems, Proc. IEEE Con-
ference on Decision and Control, 1972, pp. 370–377.
[2] Qualitative properties of a class of inﬁnite dimensional systems, M.I.T. DSR
Projects 73935, 72917, Report ESL-K-479, Mass. Inst. Tech., Cambridge, MA.
R. Villella-Bressan
Cf. J. Dyson and R. Villella-Bressan.
R. B. Vinter
[1] On a problem of Zabczyk concerning semigroups generated by operators with
non-local boundary conditions, Department of Computing and Control Publi-
cations 7718, Imperial College of Science and Technology, London, 1977.
[2] On the evolution of the state of linear diﬀerential delay equations in M 2:
properties of the generator, J. Inst. Math. Appl. 21 (1978), 13–25.
R. B. Vinter and R. H. Kwong
[1] The inﬁnite time quadratic control problem for linear systems with state and
control delays: an evolution equation approach, SIAM J. Control Optim. 19
(1981), 139–153.
Vo-Khac Khoan
[1] Distributions, analyse de Fourier, op´erateurs aux d´eriv´ees partielles, Tomes I
et II, Librairie Vuilbert, Paris, 1972.
V. Volterra
[1] Sur la th´eorie math´ematique des ph´enom`enes h´er´editaires, J. Math. Pures
Appl. 7 (1928), 247–298.
J. von Neumann and O. Morgenstern
[1] The theory of games and economic behavior, Princeton University Press,
Princeton, NJ, 1943; 2nd ed., 1947; 3rd ed., 1953.
D. C. Washburn
[1] A bound on the boundary input map for parabolic equations with application
to time optimal control, SIAM J. Control 17 (1979), 652–671.
G. F. Webb
[1] Autonomous nonlinear functional diﬀerential equations and nonlinear semi-
groups, J. Math. Anal. Appl. 46 (1974), 1–12.
[2] Functional diﬀerential equations and nonlinear semigroups in Lp-spaces, J.
Diﬀerential Equations 20 (1976), no. 1, 71–89.
[3] Linear functional diﬀerential equations with L2 initial functions, Funkcial.
Ekvac. 19 (1976), no. 1, 65–77.
[4] Asymptotic stability for abstract nonlinear functional diﬀerential equations,
Proc. Amer. Math. Soc. 54 (1976), 225–230.
[5] Theory of nonlinear age-dependent population dynamic, Monogr. Textbooks
Pure Appl. Math., vol. 89, Marcel Dekker, New York, Basel, 1985.
Cf. C. C. Travis and G. F. Webb.
J. C. Willems
[1] Least squares stationary optimal control and the algebraic Riccati equation,
IEEE Trans. Automat. Control AC-16 (1971), 621–634.
[2] Dissipative Dynamical Systems, Part II, Linear Systems with Quadratic Sup-
ply Rates, Archive for Rational Mechanics and Analysis, 45 (1972), No. 5,
352–393.
A. D. Willsky
Cf. R. H. Kwong and A. D. Willsky.
W. M. Wonham

568
References
[1] On pole assignment in multi-input controllable linear systems, IEEE Trans.
Automat. Control AC-12 (1967), no. 6, 660–665.
[2] On a matrix Riccati equation of stochastic control, SIAM J. Control 6 (1968),
no. 4, 681–697.
[3] Linear multivariable control, Springer-Verlag, New York, Berlin, Heidelberg,
Tokyo, 1979, 1985.
J. Xi
Cf. P. Zhang, G. Zheng, Y. Xu, and J. Xi.
Y. Xu
Cf. P. Zhang, G. Zheng, Y. Xu, and J. Xi.
A. Yagi
[1] Co¨ıncidence entre des espaces d’interpolation et des domaines de puissances
fractionnaires d’op´erateurs, C. R. Acad. Sci. Paris S´er. I Math. 299 (1984),
no. 6, 173–176.
K. Yosida
[1] On the diﬀerentiability and the representation of one-parameter semi-groups
of linear operators, J. Math. Soc. Japan 1 (1948), 15–21.
[2] Introduction to functional analysis, Wiley, New York, 1958.
[3] Fractional powers of inﬁnitesimal generators and the analyticity of the semi-
groups generated by them, Proc. Japan Acad. 36 (1960), 86–89.
J. Zabczyk
[1] Remarks on the control of discrete—time distributed parameter systems, SIAM
J. Control 12 (1974), 721–735.
[2] A note on C0-semigroups, Bull. Acad. Polon. Sci. S´er. Sci. Math. Astronom.
Phys. 23 (1975), no. 8 , 895–898.
[3] Remark on the algebraic Riccati equation in Hilbert space, J. Appl. Math.
Optimiz. 2 (1976), 251–258.
[4] Inﬁnite-dimensional systems in control theory, Proceedings of the 41st Ses-
sion of the International Statistical Institute (New Delhi, 1977), vol. 2. With
discussion. Bull. Inst. Internat. Statist. 47 (1977), no. 2, 286–310, 311–314.
Cf. A. J. Pritchard and J. Zabczyk.
P. Zhang
[1] Some results on two-person zero-sum linear quadratic diﬀerential games, SIAM
J. Control Optim. 43 (2005), no. 6, 2157–2165.
P. Zhang, G. Zheng, Y. Xu, and J. Xi
[1] The Riccati equation of game type, Proceedings of the 24th Chinese Control
Conference, Guangzhou, China, July, 2005, pp. 573–577.
G. Zheng and J. Xi
Cf. P. Zhang, G. Zheng, Y. Xu, and J. Xi.
J. P. Zol´esio
[1] The material derivative, Optimization of Distributed Parameters Structures
(E. J. Haug and J. C´ea, eds.), Sijthoﬀand Noordhoﬀ, Alphen aan den Rijn,
The Netherlands, 1981, pp. 1089–1151.
Cf. P. Cannarsa, G. Da Prato, and J. P. Zol´esio.
Cf. G. Da Prato and J. P. Zol´esio.
E. Zuazua
[1] Controlabilit´e exacte d’un mod`ele de plaques vibrantes en un temps arbitraire-
ment petit, C. R. Acad. Sci. Paris S´er. I Math. 304 (1987), 173–176.
Cf. R. D´ager and E. Zuazua.
Cf. M. Negreanu and E. Zuazua.
H. Zwart
Cf. R. F. Curtain and H. Zwart.

Index
A, 254
A∗, 270
A⊤∗, 270
A⊤, 260
˜A, 287
˜A∗, 287
˜A⊤, 301
adjoint
semigroup, 265
state
equation, 53
system
backward, 262, 263
analytic
semigroup, 114
approximation, 352
assumption (P), 151
asymptotic
behavior, 91
B, 279
B⊤, 280, 296
¯B, 280
ˆB⊤, 293, 298, 309
¯B⊤, 280
˜B⊤, 299, 309
B, 280
β⊤, 280
Bellman–Hamilton–Jacobi, 32
boundary
condition
Dirichlet, 174, 225
Neumann, 224
value
problem, 174, 175, 177
C-stabilizable, 480, 507
ˆC, 299, 306, 309
˜C, 299
˜C, 300, 309
C, complex numbers, 14
C, 300
C1
s
`
I; Σ(H)
´
, 388
Cs
h(u, v), 69
Cx0(u, v)
concavity, 54
convexity, 53
convexity–concavity, 54
duality gap, 68
gradient, 53
Hessian, 53
C(a, b), 3
C([a, b]), 3
Calculus of Variations, 30, 315
canonical structure, 20
C(Ω), 3
C∞
c (Ω), 3
change
variable, 196, 209
C∞
0 (Ω), 3
C∞(Ω), 3
C∞(Ω), 3
C0(Ω), 3
Ck
0 (Ω), 3
Ck
0 (Ω, Rm), 3
Ck(Ω), 3

570
Index
Ck(Ω, Rm), 3
Ck(Ω), 3
Ck(Ω, Rm), 3
Ck
c (Ω), 3
closed
operator, 88
closed loop
equation, 386, 408, 480
operator, 487
coercivity
V –H, 179
compensator, 28
completing the squares, 32
continuous
dependence, 445, 453
eventually
uniformly, 121
contractions
inﬁnitesimal
generator, 107
control
boundary, 331
Dirichlet, 177, 368, 436
Neumann, 175, 176, 184, 334, 369,
437
operator, 177
distributed, 173
feedback, 386
minimum energy, 14, 17
operator
distributed, 175
unbounded, 210, 212, 218
optimal, 30, 386, 479, 510
problem, 433
periodic, 509
point, 177, 190
space, 385
controllability, 13, 14
approximate, 315, 320, 322
exact, 14, 323, 357
ﬁnite dimensional, 325
heat equation, 330
hyperbolic equation, 367
operator, 314, 319
skew-symmetric, 339
controllable
approximately, 320, 322
exactly, 343
pair, 16
cost function, 385
coupled system, 56
state-adjoint state, 56
uniqueness, 72
Cs
`
I; Σ(H)
´
, 387
Cs,α([a, b]; Σ(H)), 439
Cs,α([a, b]; Σ+(H)), 440
Cu
`
I; Σ(H)
´
, 387
damping
structural, 200
viscous, 199
D(Ω), 3
delay
control
observation, 232
diﬀerential
equations, 231
equations, 125
systems, 230
∂αf, 3
detectability, 13, 23, 488
deviating
argument, 229
diﬀerence
equations, 232
diﬀerential
equation
operational, 196
Dirichlet
mapping, 436
dissipative systems, 39
distributed control, 330
parabolic, 419
Dk(Ω), 3
domain
operator, 88
dual system, 488
duality, 19
gap, 68
Dynamic Programming, 13, 32, 386,
433, 435, 454, 461, 463, 480, 510,
521
E, 438
es
+u, 252
es
−u, 252
Ek, 439
equation

Index
571
adjoint state, 53
linear
evolution, 88
state, 50
variational, 114, 173
equilibrium
Cournot–Nash, 47
Nash, 47, 48, 68
Nash–Cournot, 48
evolution
equation
linear, 88
nonhomogeneous, 128
F, 264, 308
F ⊤, 264
feasibility condition, 49, 57, 68
feedback
formula, 386
operator
exponential stability, 492
stabilizable, 481, 488
observation, 481
ﬁnite horizon, 408
ﬁrst order problem, 466
formula
variation
constants, 202
fractional
powers
dissipative, 167
domains, 169
framework
continuous functions, 240
product space, 245
function
continuous
uniformly, 3
functional
diﬀerential
equations, 229, 231
G, 276
G⊤, 276
game
concaviity, 70
convexity, 70
convexity-concaviity, 70
coupled system
uniqueness, 72
lower value
open loop, 52, 69
saddle point, 48
open loop, 52, 69
two-player, 50
upper value
open loop, 52, 69
utility function, 50
value, 48
lower, 48
open loop, 52, 69
upper, 48
zero-sum, 50
general model, 233
generator
inﬁnitesimal, 89
gradient, 53
graph
operator, 88
H, 385
H∞
control, 35
theory, 35
Hamilton–Jacobi, 13, 40
Hamiltonian system, 397
heat equation, 330
hereditary
systems, 230
Hessian, 53
(HH), 461
(HH)1, 459
(HH)′, 472
(HH)′
1, 471
(HH)′
∞, 534
(HH)2, 461
(HH)∞, 529
Hille–Phillips
theorem, 101
(HP), 433
(HP)1, 432
(HP)2, 433
(HP)∞, 517
HUM
Hilbert Uniqueness Method, 381
hyperbolic equation, 367
I(−h, 0), 232

572
Index
I(a, b), 239
image
operator, 88
inﬁnitesimal
generator, 89
contractions, 107
perturbation, 134
spectral, 100
integration by parts formula, 261
integro-diﬀerential
equations, 231
interpolation
space, 140, 154, 162, 173, 183, 201
Theorem, 158
intertwining
theorems, 263
isomorphism, 207
adjoint, 179
main, 139, 140, 142, 149, 150
theorem, 177
J+
x0(v)
convexity, 57
J−
x0(v), 64
concavity, 57
gradient, 65
Hessian, 65
K(−h, 0; Rn), 240
L, 249
¯L, 264
L, 249, 253
L⊤, 260
L⊤, 260, 264
L
`
H; C(I; H)
´
, 387
linear
control
observation, 297
systems, 279
evolution
equation, 88
operator, 88
time-invariant, 249
Lipschitzian systems, 240
Lr(H), 407
Lyapunov equation, 98, 100
M p, 233, 245, 254
Maximum Principle, 30
Maxwell equation, 371, 382
memory
ﬁnite, 233, 240
inﬁnite, 232, 233, 240
length, 240
Method of Transposition, 181, 188–190,
331, 336, 338, 364, 368, 374
microlocal analysis, 382
mild solution, 440
minimum energy
transfer, 314
N, integers, 14
Nash equilibrium, 47
Nash–Cournot equilibrium, 48
Neumann
mapping, 438
nonanticipatory, 230
nonresonant, 509
observability, 13, 18, 322
continuous, 323
observable, 322
observation
equation, 302
operator
unbounded, 210, 216, 218
space, 385
unbounded, 405, 410
ω0(S), 91
operational
diﬀerential
equation, 196
operator
closed, 88
delay, 279
dissipative, 104
maximal, 104
domain, 88
evolution, 138
fractional
powers, 167, 169
graph, 88
group, 89
image, 88
linear, 88
norm
topology, 88

Index
573
point spectrum, 151
range, 88
semigroup, 89
skew-symmetric, 339, 362, 363
spectrum, 88
structural, 263, 264
companion, 275
optimal
pair, 386, 479, 510
P(s), 75
∥P ∥1, 440
∥P ∥α, 440
parabolic
equation, 122
systems, 222
Pβ, 433
perturbation
theorem, 179
plate equation, 377, 382
pointwise
control, 338, 339
convergence, 387
pole-assignment, 20, 29
Principle of Optimality, 32
product space, 233
R, real numbers, 14
range
operator, 88
realization, 171
regularity, 207
maximal, 139
theorem, 180
resolvent
set, 88
retarded
argument, 229
Riccati
algebraic equation, 34, 41, 480, 518
classical solution, 390
continuity
data, 394
diﬀerential equation, 51, 77, 81, 386,
390
backward, 415
dual, 417
forward, 416
maximal solution, 480, 493
mild solution, 390, 463, 472
minimal solution, 480, 493
monotonicity property, 395
periodic solution, 502
nontrivial, 502
representation formula, 396
solution, 482
stationary solution, 481
strict solution, 390, 481
weak solution, 390, 481
S, 254
S∗, 266, 269, 287
S⊤∗, 265, 269
S⊤, 260, 265
˜S, 286, 309
˜S⊤∗, 309
˜S⊤, 301
s(A), 119
Σ+(H), 387
saddle point, 48
Schr¨odinger
equation, 123, 149
second order
problems, 198
semiderivative
ﬁrst order, 53
second order, 53
semigroup
adjoint, 103
analytic, 108, 114, 164, 206
compact, 152
complements, 201
contractions, 104
diﬀerentiable, 115
exponential
stability, 93
operator, 89
strongly
continuous, 89, 163
transposed, 202
type, 91
uniform
continuity, 89
Σ(H), 387
solution
classical, 115, 129
mild, 129, 131, 135, 202, 440
strict, 129

574
Index
existence, 133
strong, 129, 131
existence, 130
uniqueness, 130
weak, 129, 440
space
averages, 159
interpolation, 166, 173, 201
traces, 154
spectral
determining
growth condition, 119
properties
diﬀerentiable, 116
spectrum, 88
continuous, 88
determined
growth assumption, see spectral
determining growth condition
essential, 122
point, 88
residual, 88
stability, 26
asymptotic, 94
asymptotic output, 27
exponential, 93
stabilizability, 13, 23, 314
uniform, 314
w.r.t. observation, 480
stabilizable, 24
(A, B) C-stabilizable, 480
feedback, 481
observation, 480
w.r.t. observation, 480
state
equation, 50, 385
delay equation, 423
noncylindrical domains, 428
nonhomogeneous, 412
parabolic, 418
time-dependent, 414
wave equation, 420
extended, 299
extended structural, 300
Ichikawa, 299
optimal, 386, 479, 510
space, 385
structural, 280
theory, 252, 297
Vinter and Kwong, 280
Vinter, Kwong, and Salamon, 300
storage function, 40
strong convergence, 387
sub-diﬀerential, 104
synthesis problem, 386
system
closed loop, 76
input-output, 211
time-invariant, 252
τ(t), 266
T(p, α, X0, X1), 155
theorem
Datko–Pazy, 93
Hille, 109
Hille–Phillips, 101
intertwining, 265, 267, 278
Lumer–Phillips, 106
Pazy, 97
trace
space, 155
transformation
unitary, 108
type
advanced, 230
neutral, 230
retarded, 230
semigroup, 91
U(x0), 52
u•, 256
uniform convergence, 387
utility function, 47, 50
V(0, T ; M p, D(A∗)′), 255
V(0, T ; M p; D( ˜A⊤)′), 303
V(0, T ; Zp, D( ˜A∗)′), 293
V (x0), 52, 64
v(x0), 52
v+(x0), 52
v+
s (h), 69
v−(x0), 52
v−
s (h), 69
variable
change, 196
variational
case, 403
equation, 114, 173

Index
575
systems, 222
Vk, 439
Volterra
integral
equations, 231
integro-diﬀerential
equation, 231
VP , 438
W (p, α, X0, X1), 154
wave
equation, 124, 150, 368, 369, 469
abstract, 468
strongly damped, 515
weak solution, 440
ˆx(t), 298
˜x(t), 254, 286, 292, 298
[X, Y ]θ, 166
(X0, X1)θ,p, 159
xt, 245
Yosida
approximations, 102
Zp, 239, 286


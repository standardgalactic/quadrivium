Studies in Fuzziness and Soft Computing
Dan E. Tamir
Naphtali D. Rishe
Abraham Kandel    Editors 
Fifty Years 
of Fuzzy 
Logic and its 
Applications
www.allitebooks.com

Studies in Fuzziness and Soft Computing
Volume 326
Series editor
Janusz Kacprzyk, Polish Academy of Sciences, Warsaw, Poland
e-mail: kacprzyk@ibspan.waw.pl
www.allitebooks.com

About this Series
The series “Studies in Fuzziness and Soft Computing” contains publications on
various topics in the area of soft computing, which include fuzzy sets, rough sets,
neural networks, evolutionary computation, probabilistic and evidential reasoning,
multi-valued logic, and related ﬁelds. The publications within “Studies in Fuzziness
and Soft Computing” are primarily monographs and edited volumes. They cover
signiﬁcant recent developments in the ﬁeld, both of a foundational and applicable
character. An important feature of the series is its short publication time and world-
wide distribution. This permits a rapid and broad dissemination of research results.
More information about this series at http://www.springer.com/series/2941
www.allitebooks.com

Dan E. Tamir ⋅Naphtali D. Rishe
Abraham Kandel
Editors
Fifty Years of Fuzzy Logic
and its Applications
123
www.allitebooks.com

Editors
Dan E. Tamir
Department of Computer Science
Texas State University
San Marcos, TX
USA
Naphtali D. Rishe
School of Computing and Information
Sciences
Florida International University
Miami, FL
USA
Abraham Kandel
The University of South Florida
Tampa, FL
USA
and
School of Computing and Information
Sciences
Florida International University
Miami, FL
USA
ISSN 1434-9922
ISSN 1860-0808
(electronic)
Studies in Fuzziness and Soft Computing
ISBN 978-3-319-19682-4
ISBN 978-3-319-19683-1
(eBook)
DOI 10.1007/978-3-319-19683-1
Library of Congress Control Number: 2015940726
Springer Cham Heidelberg New York Dordrecht London
© Springer International Publishing Switzerland 2015
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made.
Printed on acid-free paper
Springer International Publishing AG Switzerland is part of Springer Science+Business Media
(www.springer.com)
www.allitebooks.com

To my parents Rivka and Ezra Tamir
Dan E. Tamir
To the joyous Bar Mitzvah of Joshua Meir
Yosef ben Tajana and Naphtali
Naphtali D. Rishe
To my grandchildren Kfeer, Maya, Riley,
Liam, Leo, and Marley
With Love
Abraham Kandel
Recognizing the 50th anniversary of Florida
International University — a peer of Fuzzy
Logic
www.allitebooks.com

Preface
As early as in the era of Lady Ada Loveable and Babbage, scientists seriously
considered the possibility of assigning certain complex activities performed by
human beings to machines. This direction of research has signiﬁcantly intensiﬁed,
with the development of digital computers, through immense contributions by
Turing and von Neumann and the progress in the discipline of artiﬁcial intelligence
(AI). Following a period of enthusiasm about the possibility on one hand, and
computer phobia on the other hand, many of the active AI researchers have faced,
and attempted to resolve, an apparent obstacle. The formal philosophical and
mathematical paradigms applied in AI and related research areas seemed to fall
short of the capability to emulate human reasoning. In some sense, the issue was
that the formalisms were very rigid and did not match the fuzzy nature of human
perception of sets and inference.
A pioneer of artiﬁcial intelligence, L.A. Zadeh was concerned with the dichot-
omy between human reasoning and classical-logic/mathematical/machine precision.
As early as 1961 (and most likely before) Zadeh attempted to resolve this
dichotomy with a formal, mathematical theory of imprecision, aka Fuzzy Set
Theory and Fuzzy Logic. The ﬁrst documented reference to the need for this theory
appears in his 1962 paper “From Circuit Theory to System Theory.” The ﬁrst
formulation of a solution to the dichotomy is proposed in his seminal paper “Fuzzy
Sets” published in Information and Control in 1965. These concepts, as well as
several derivatives of the ideas, such as linguistic variables, Type-2 Fuzzy Logic,
and Z-numbers, introduced by Zadeh, have opened the door to highly fruitful
directions of research, development, and deployment in several areas.
Zadeh’s 1965 paper and subsequent papers have sparked the interest of
numerous researchers and practitioners and resulted in rapid developments in the
ﬁelds of Fuzzy Set Theory, Fuzzy Logic, Fuzzy Systems, and related disciplines.
The ﬁve decades that followed the 1965 paper and his pioneering work have
produced a multitude of research work and applications related to artiﬁcial intel-
ligence, control theory, inference, and reasoning. In recent years, Fuzzy Logic has
been applied in many areas, including neural networks, clustering, data mining, and
software testing.
vii
www.allitebooks.com

The present volume, entitled “Fifty Years of Fuzzy Logic and its Applications,”
was conceived as a way of academic celebration of the ﬁfty years’ anniversary
of the 1965 paper. It includes papers from pioneers and prominent scholars engaged
in research on the theory and applications of fuzzy logic and uncertainty
management. The papers cover a wide range of the spectrum and gamut of
“Fuzziness.”
The volume editors extend sincere gratitude to the distinguished chapter authors
for their invaluable contributions and their kind patience in complying with the
bureaucratic procedures involved in publishing this volume.
viii
Preface
www.allitebooks.com

Acknowledgments
This work is based in part upon work supported by the National Science Foundation
under grants I/UCRC IIP-1338922, AIR IIP-1237818, SBIR IIP-1330943, III-Large
IIS-1213026, MRI (CNS-1429345, CNS-0821345, CNS-1126619), and CREST
HRD-0833093 and by DHS S&T at TerraFly (http://terraﬂy.com) and the NSF
CAKE Center (http://cake.ﬁu.edu).
ix
www.allitebooks.com

Contents
Toward a Restriction-Centered Theory of Truth
and Meaning (RCT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Lotfi A. Zadeh
Functional Solution of the Knowledge Level Control Problem:
The Principles of Fuzzy Logic Rules and Linguistic Variables . . . . . . .
25
Ali M. Abbasov and Shahnaz N. Shahbazova
Learning Systems with FUZZY . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
Sang Wan Lee and Z. Zenn Bien
Fuzzy Modifiers at the Core of Interpretable Fuzzy Systems . . . . . . . .
51
Bernadette Bouchon-Meunier and Christophe Marsala
Human and Machine Intelligence — Between Fuzzy Logic
and Daoist Thought. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
Liya Ding and Xiaogan Liu
Developing Fuzzy State Models as Markov Chain Models with
Fuzzy Encoding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
Dimitar Filev, Ilya Kolmanovsky and Ronald Yager
Incremental Granular Fuzzy Modeling Using Imprecise Data
Streams. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
Daniel Leite and Fernando Gomide
Fuzzy Measures and Integrals: Recent Developments . . . . . . . . . . . . .
125
Michel Grabisch
xi
www.allitebooks.com

Important New Terms and Classifications in Uncertainty
and Fuzzy Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
Madan M. Gupta and Ashu M.G. Solo
Formalization and Visualization of Kansei Information
Based on Fuzzy Set Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
Fangyan Dong and Kaoru Hirota
Cognitive Informatics: A Proper Framework for the Use
of Fuzzy Dynamic Programming for the Modeling
of Regional Development? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
Janusz Kacprzyk
On Discord Between Expected and Actual Developments
in Applications of Fuzzy Logic During Its First Fifty Years. . . . . . . . .
201
George J. Klir
Meta-Heuristic Optimization of a Fuzzy Character Recognizer . . . . . .
227
Alex Tormási and László T. Kóczy
Additive Fuzzy Systems as Generalized Probability Mixture
Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
245
Bart Kosko
Fuzzy Information Retrieval Systems: A Historical Perspective . . . . . .
267
Donald H. Kraft, Erin Colvin, Gloria Bordogna and Gabriella Pasi
Is the World Itself Fuzzy? Physical Arguments
and Unexpected Computational Consequences of Zadeh’s Vision . . . . .
297
Vladik Kreinovich and Olga Kosheleva
Handling Noise and Outliers in Fuzzy Clustering . . . . . . . . . . . . . . . .
315
Christian Borgelt, Christian Braune, Marie-Jeanne Lesot
and Rudolf Kruse
A Fuzzy-Based Approach to Survival Data Mining . . . . . . . . . . . . . . .
337
Mark Last and Hezi Halpert
Knowledge Extraction from Support Vector Machines:
A Fuzzy Logic Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
361
Shahaf Duenyas and Michael Margaliot
On Type-Reduction Versus Direct Defuzzification
for Type-2 Fuzzy Logic Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
387
Jerry M. Mendel
xii
Contents

On Fuzzy Theory for Econometrics . . . . . . . . . . . . . . . . . . . . . . . . . .
401
Hung T. Nguyen and Songsak Sriboonchitta
On Z-numbers and the Machine-Mind for Natural
Language Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
415
Romi Banerjee and Sankar K. Pal
Evolutionary Reduction of Fuzzy Rule-Based Models . . . . . . . . . . . . .
459
Witold Pedrycz, Kuwen Li and Marek Reformat
Geospatial Uncertainty Representation: Fuzzy and Rough
Set Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
483
Frederick Petry and Paul Elmore
How to Efficiently Diagnose and Repair Fuzzy Database Queries
that Fail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
499
Olivier Pivert and Grégory Smits
The Web, Similarity, and Fuzziness . . . . . . . . . . . . . . . . . . . . . . . . . .
519
Parisa D. Hossein Zadeh and Marek Z. Reformat
The Genesis of Fuzzy Sets and Systems – Aspects in Science
and Philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
537
Rudolf Seising
Fuzzy Logic in Speech Technology - Introductory and
Overviewing Glimpses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
581
Horia-Nicolai Teodorescu
Fuzzy Sets: Towards the Scientific Domestication of Imprecision . . . . .
609
Enric Trillas
Type 1 and Full Type 2 Fuzzy System Models . . . . . . . . . . . . . . . . . .
643
I. Burhan Türkşen
Complex Fuzzy Sets and Complex Fuzzy Logic an Overview
of Theory and Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
661
Dan E. Tamir, Naphtali D. Rishe and Abraham Kandel
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
683
Contents
xiii

Toward a Restriction-Centered Theory
of Truth and Meaning (RCT)
LotﬁA. Zadeh
Abstract What is truth? The question does not admit a simple, precise answer.
A dictionary-style deﬁnition is: The truth value of a proposition, p, is the degree to
which the meaning of p is in agreement with factual information, F. A precise
deﬁnition of truth will be formulated at a later point in this paper. The theory
outlined in the following, call it RCT for short, is a departure from tradi-
tional theories of truth and meaning. In RCT, truth values are allowed to be
described in natural language. Examples. Quite true, more or less true, almost true,
largely true, possibly true, probably true, usually true, etc. Such truth values are
referred to as linguistic truth values. Linguistic truth values are not allowed in
traditional logical systems, but are routinely used by humans in everyday reasoning
and everyday discourse. The centerpiece of RCT is a deceptively simple concept—
the concept of a restriction. Informally, a restriction, R(X), on a variable, X, is an
answer to a question of the form: What is the value of X? Possible answers: X = 10,
X is between 3 and 20, X is much larger than 2, X is large, probably X is large,
usually X is large, etc. In RCT, restrictions are preponderantly described in natural
language. An example of a fairly complex description is: It is very unlikely that
there will be a signiﬁcant increase in the price of oil in the near future. The
canonical form of a restriction, R(X), is X isr R, where X is the restricted variable,
R is the restricting relation, and r is an indexical variable which deﬁnes the way in
which R restricts X. X may be an n-ary variable and R may be an n-ary relation.
The canonical form may be interpreted as a generalized assignment statement in
which what is assigned to X is not a value of X, but a restriction on the values
which X can take. A restriction, R(X), is a carrier of information about X.
A restriction is precisiated if X, R and r are mathematically well deﬁned. A key idea
which underlies RCT is referred to as the meaning postulate, MP. MP postulates
that the meaning of a proposition drawn from a natural language, p—or simply p—
may be represented as a restriction, p →X isr R. This expression is referred to as the
L.A. Zadeh (✉)
Department of EECS, University of California, Berkeley, CA94720-1776
e-mail: zadeh@eecs.berkeley.edu
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_1
1

canonical form of p, CF(p). Generally, the variables X, R and r are implicit in
p. Simply stated, MP postulates that a proposition drawn from a natural language
may be interpreted as an implicit assignment statement. MP plays an essential role
in deﬁning the meaning of, and computing with, propositions drawn from natural
language. What should be underscored is that in RCT understanding of meaning is
taken for granted. What really matters is not understanding of meaning but prec-
isiation of meaning. Precisiation of meaning is a prerequisite to reasoning and
computation with information described in natural language. Precisiation of
meaning is a desideratum in robotics, mechanization of decision-making, legal
reasoning, precisiated linguistic summarization with application to data mining, and
other ﬁelds. It should be noted that most—but not all—propositions drawn from
natural language are precisiable. In RCT, truth values form a hierarchy. First order
(ground level) truth values are numerical, lying in the unit interval. Linguistic truth
values are second order truth values and are restrictions on ﬁrst order truth values.
nth order truth values are restrictions on (n-1) order truth values, etc. Another key
idea is embodied in what is referred to as the truth postulate, TP. The truth pos-
tulate, TP, equates the truth value of p to the degree to which X satisﬁes R. This
deﬁnition of truth value plays an essential role in RCT. A distinguishing feature of
RCT is that in RCT a proposition, p, is associated with two distinct truth values—
internal truth value and external truth value. The internal truth value relates to the
meaning of p. The external truth value relates to the degree of agreement of p with
factual information. To compute the degree to which X satisﬁes R, it is necessary to
precisiate X, R and r. In RCT, what is used for this purpose is the concept of an
explanatory database, ED. Informally, ED is a collection of relations which rep-
resent the information which is needed to precisiate X and R or, equivalently, to
compute the truth value of p. Precisiated X, R and p are denoted as X*, R* and p*,
respectively. X and R are precisiated by expressing them as functions of ED. The
precisiated canonical form, CF*(p), is expressed as X*isr* R*. At this point, the
numerical truth value of p, ntp, may be computed as the degree to which X* satisﬁes
R*. In RCT, the factual information, F, is assumed to be represented as a restriction
on ED. The restriction on ED induces a restriction, t, on ntp which can be computed
through the use of the extension principle. The computed restriction on ntp is
approximated to by a linguistic truth value, ltp. Precisiation of propositions drawn
from natural language opens the door to construction of mathematical solutions of
computational problems which are stated in natural language.
Keywords Precisiation of meaning ⋅Computation with restrictions ⋅Assessment
of truth values ⋅Formalization of everyday reasoning
2
L.A. Zadeh

1
Introduction
The concepts of truth and meaning are of fundamental importance in logic, infor-
mation analysis and related ﬁelds. The theory outlined in this paper, call it RCT for
short, is a departure from traditional theories of truth and meaning, principally
correspondence theory, coherence theory, Tarski semantics, truth-conditional
semantics and possible-world semantics [1–3, 5–9].
In large measure, traditional theories of truth and meaning are based on bivalent
logic. RCT is based on fuzzy logic. Standing on the foundation of fuzzy logic, RCT
acquires a capability to enter the realm of everyday reasoning and everyday dis-
course—a realm which is avoided by traditional theories of truth and meaning
largely because it is a realm that does not lend itself to formalization in the classical
tradition.
In RCT, truth values are allowed to be described in natural language. Examples.
Quite true, very true, almost true, probably true, possibly true, usually true, etc.
Such truth values are referred to as linguistic truth values. Linguistic truth values
are not allowed in traditional logical systems.
The centerpiece of RCT is the deceptively simple concept—the concept of a
restriction. The concept of a restriction has greater generality than the concept of
interval, set, fuzzy set and probability distribution. An early discussion of the con-
cept of a restriction appears in [12]. Informally, a restriction, R(X), on a variable, X,
is an answer to a question of the form: What is the value of X? Example. Robert is
staying at a hotel in Berkeley. He asks the concierge, “How long will it take me to
drive to SF Airport?” Possible answers: 1 h, one hour plus/minus 15 min, about 1 h,
usually about 1 h, etc. Each of these answers is a restriction on the variable, Driving.
time. Another example. Consider the proposition, p: Most Swedes are tall. What is
the truth value of p? Possible answers: true, 0.8, about 0.8, high, likely high, possibly
true, etc. In RCT, restrictions are preponderantly described as propositions drawn
from a natural language. Typically, a proposition drawn from a natural language is a
fuzzy proposition, that is, a proposition which contains fuzzy predicates, e.g., tall,
fast, heavy, etc., and/or fuzzy quantiﬁers, e.g., most, many, many more, etc., and/or
fuzzy probabilities, e.g., likely, unlikely, etc. A zero-order fuzzy proposition does
not contain fuzzy quantiﬁers and/or fuzzy probabilities. A ﬁrst-order fuzzy propo-
sition contains fuzzy predicates and/or fuzzy quantiﬁers and/or fuzzy probabilities. It
is important to note that in the realm of natural languages fuzzy propositions are the
norm rather than exception. Traditional theories of truth and meaning provide no
means for reasoning and computation with fuzzy propositions.
Basically, R(X) may be viewed as a limitation on the values which X can take.
Examples.
X = 5
X is between 3 and 7
X is small
X is normally distributed with mean m and variance σ2
It is likely that X is small
Toward a Restriction-Centered Theory of Truth and Meaning (RCT)
3

Summers are usually cold in San Francisco (X is implicit)
Robert is much taller than most of his friends (X is implicit)
As a preview of what lies ahead, it is helpful to draw attention to two key ideas
which underlie RCT. The ﬁrst idea, referred to as the meaning postulate, MP, is that
of representing a proposition drawn from a natural language, p, as a restriction
expressed as
p →X isr R,
where X is the restricted variable, R is the restricting relation, and r is an indexical
variable which deﬁnes the way in which R restricts X. X may be an n-ary variable,
and R may be an n-ary relation. Generally, X and R are implicit in p. Basically, X is
the variable whose value is restricted by p. X is referred to as the focal variable. In
large measure, the choice of X is subjective, reﬂecting one’s perception of the
variable or variables which are restricted by p. However, usually there is a con-
sensus. It should be noted that a semantic network representation of p may be
viewed as a graphical representation of an n-ary focal variable and an n-ary
restricting relation. The expression on the right-hand side of the arrow is referred
to as the canonical form of p, CF(p). CF(p) may be interpreted as a generalized
assignment statement [17]. The assignment statement is generalized in the sense
that what is assigned to X is not a value of X, but a restriction on the values which
X can take. Representation of p as a restriction is motivated by the need to rep-
resent p in a mathematically well-deﬁned form which lends itself to computation.
The second key idea is embodied in what is referred to as the truth postulate, TP.
The truth postulate equates the truth value of p to the degree to which X satisﬁes R.
The degree may be numerical or linguistic. As will be seen in the sequel, in RCT
the truth value of p is a byproduct of precisiation of the meaning of p.
Note. To simplify notation in what follows, in some instances no differentiation
is made between the name of a variable and its instantiation. Additionally, in some
instances no differentiation is made between a proposition, p, and the meaning of p.
2
The Concept of a Restriction—A Brief Exposition
The concept of a restriction is the centerpiece of RCT. As was stated earlier, a
restriction, R(X), on a variable, X, may be viewed as an answer to a question of the
form: What is the value of X? The concept of a restriction is closely related to the
concept of a generalized constraint [18].
R(X) may be viewed as information about X. More concretely, R(X) may be
expressed in a canonical form, CF(R(X)),
CF R X
ð Þ
ð
Þ: X isr R,
4
L.A. Zadeh

where X is the restricted variable, R is the restricting relation, and r is an
indexical variable which deﬁnes the modality of R, that is, the way in which R
restricts X. X may be an n-ary variable and R may be an n-ary relation. A restriction
is precisiated if X, R and r are mathematically well deﬁned. Precisiation of
restrictions plays a pivotal role in RCT. Precisiation of restrictions is a prerequisite
to computation with restrictions. Here is an example of a simple problem which
involves computation with restrictions.
Usually Robert leaves his ofﬁce at about 5 pm.
Usually it takes Robert about an hour to get home from work.
At what time does Robert get home?
Humans have a remarkable capability to deal with problems of this kind using
approximate, everyday reasoning. One of the important contributions of RCT is that
RCT opens the door to construction of mathematical solutions of computational
problems which are stated in a natural language.
2.1 Types of Restrictions
There are many types of restrictions. A restriction is singular if R is a singleton.
Example. X = 5. A restriction is nonsingular if R is not a singleton. Nonsingularity
implies uncertainty. A restriction is direct if the restricted variable is X. A restriction
is indirect if the restricted variable is of the form f(X). Example.
R p
ð Þ:
Zb
a
μ u
ð Þp u
ð Þdu is likely,
is an indirect restriction on p.
Note. In the sequel, the term restriction is sometimes applied to R.
The principal types of restrictions are: possibilistic restrictions, probabilistic
restrictions and Z-restrictions.
Possibilistic restriction (r = blank)
R X
ð Þ: X is A,
where A is a fuzzy set in a space, U, with the membership function, µA. A plays the
role of the possibility distribution of X,
Poss X = u
ð
Þ = μA u
ð Þ.
Toward a Restriction-Centered Theory of Truth and Meaning (RCT)
5

Example.
X
is 
small
restricted variable 
restricting relation (fuzzy set)
The fuzzy set small plays the role of the possibility distribution of X. (Fig. 1)
Example.
Leslie is taller than Ixel
(Height(Leslie), Height(Ixel)) is taller
restricted variable
restricting relation (fuzzy relation)
The fuzzy relation taller is the possibility distribution of ((Height(Leslie), Height
(Ixel)).
Probabilistic restriction (r = p)
R X
ð Þ : X is p p,
where p is the probability density function of X,
Probðu ≤X ≤u + duÞ = p u
ð Þdu.
Example.
Z-restriction (r = z, s is suppressed)
X is a real-valued random variable.
A Z-restriction is expressed as
R X
ð Þ: X iz Z,
where Z is a combination of possibilistic and probabilistic restrictions deﬁned as
Z: Prob X is A
ð
Þ is B,
in which A and B are fuzzy numbers. Usually, A and B are labels drawn from a
natural language. The ordered pair, (A,B), is referred to as a Z-number [19]. The
ﬁrst component, A, is a possibilistic restriction on X. The second component, B, is a
X isp
exp(-(X-m)2
1
2π
/2
2).
√
restricted variable
restricting relation (probability density function)
6
L.A. Zadeh

possibilistic restriction on the certainty (probability) that X is A. A Z-interval is a
fuzzy number in which the ﬁrst component is a fuzzy interval.
Examples.
Probably Robert is tall →Height Robert
ð
Þ iz tall, probable
ð
Þ
Usually temperature is low →Temperature iz low, usually
ð
Þ
Note.
Usually X is A,
is a Z-restriction when A is a fuzzy number.
A Z-valuation is an ordered triple of the form (X,A,B), and (A,B) is a Z-number.
Equivalently, a Z-valuation, (X,A,B), is a Z-restriction on X,
X, A, B
ð
Þ →X iz A, B
ð
Þ.
Examples.
(Age(Robert), young, very likely)
(Trafﬁc, heavy, usually).
Note. A natural language may be viewed as a system of restrictions. In the realm
of natural languages, restrictions are predominantly possibilistic. For this reason, in
this paper we focus our attention on possibilistic restrictions. For simplicity, pos-
sibilistic restrictions are assumed to be trapezoidal.
Example. Figure 2 shows a possibilistic trapezoidal restriction which is associated
with the fuzzy set middle-age.
Fig. 1 Possibilistic
restriction on X
Toward a Restriction-Centered Theory of Truth and Meaning (RCT)
7

2.2 Computation with Restrictions
Computation with restrictions plays an essential role in RCT. In large measure,
computation with restrictions involves the use of the extension principle [10, 13].
A brief exposition of the extension principle is presented in the following. The
extension principle is not a single principle. The extension principle is a collection
of computational rules in which the objects of computation are various types of
restrictions. More concretely, assume that Y is a function of X, Y = f(X), where X
may be an n-ary variable. Assume that what we have is imperfect information
about X, implying that what we know is a restriction on X, R(X). The restriction on
X, R(X), induces a restriction on Y, R(Y). The extension principle is a computa-
tional rule which relates to computation of R(Y) given R(X). In what follows, we
consider only two basic versions of the extension principle. The simplest version
[10] is one in which the restriction is possibilistic and direct. This version of the
extension principle reduces computation R(Y) to the solution of a variational
problem,
Y = f X
ð Þ
R X
ð Þ: X is A
R Y
ð Þ: μY v
ð Þ = supu μA u
ð Þ
ð
Þ
subject to
v = f u
ð Þ,
where µA and µY are the membership functions of A and Y, respectively. Simply
stated,
If X is A then Y is f A
ð Þ,
where f(A) is the image of A under f. A simple example is shown in Fig. 3.
An inverse version of this version of the extension principle is the following.
Fig. 2 Trapezoidal
possibilistic restriction on
Age
8
L.A. Zadeh
www.allitebooks.com

Y = f X
ð Þ
R Y
ð Þ: Y is B
R X
ð Þ: μA u
ð Þ = μB f u
ð Þ
ð
Þ
ð
Þ
Simply stated, A is the preimage of B under f. (Fig. 4)
A slightly more general version [13] is one in which R(X) is possibilistic and
indirect.
Y = f X
ð Þ
R X
ð Þ: g X
ð Þ is A
R Y
ð Þ: μY v
ð Þ = supu μA g u
ð Þ
ð
Þ
ð
Þ
Y/v
X/u
f (A)
A
f
0
Fig. 3 Possibilistic version of the basic extension principle. f(A) is the image of A under f. What
is shown is a trapezoidal approximation to f(A)
Y/
Y/v
f
B
X/u
X/u
preimage of B under f
Fig. 4 Inverse version of the basic possibilistic extension principle. The induced restriction on X
is the preimage of B, the restriction on Y
Toward a Restriction-Centered Theory of Truth and Meaning (RCT)
9

subject to
v = f g u
ð Þ
ð
Þ.
Example.
Given, p: Most Swedes are tall.
Question, q: What is the average height of Swedes?
The ﬁrst step involves precisiation of p and q. For this purpose, it is expedient to
employ the concept of a height density function, h.
h u
ð Þdu = fraction of Swedes whose height lies in the interval u, u + du
½
.
If hmin and hmax are, respectively, the minimum and maximum heights in the
population, we have
Z h max
h min
h u
ð Þdu = 1.
In terms of the height density function, precisiations of q and p, q* and p*, may
be expressed as
q*: ? have =
Z h max
h min
uh u
ð Þdu,
p*
Z h min
h min
μtall u
ð Þh u
ð Þdu is most,
where µtall is the membership function of tall. Applying the basic, indirect, possi-
bilistic version of the extension principle, computation of have is reduced to the
solution of the variational problem
μhave v
ð Þ = sup
h
μmost
Z h max
h min
μtall u
ð Þh u
ð Þdu


,
subject to
V =
Z h max
h min
uh u
ð Þdu,
10
L.A. Zadeh

and
Z h max
h min
h u
ð Þdu = 1.
In RCT, for purposes of reasoning and computation what are needed—in
addition to possibilistic versions of the extension principle—are versions in which
restrictions are probabilistic restrictions and Z-restrictions. These versions of the
extension principle are described in [21].
3
Truth and Meaning
It is helpful to begin with a recapitulation of some of the basic concepts which were
introduced in the Introduction.
There is a close relationship between the concept of truth and the concept of
meaning. To assess the truth value of a proposition, p, it is necessary to understand
the meaning of p. However, understanding the meaning of p is not sufﬁcient. What
is needed, in addition, is precisiation of the meaning of p. Precisiation of the
meaning of p involves representation of p in a form that is mathematically well
deﬁned and lends itself to computation. In RCT, formalization of the concept of
truth is a byproduct of formalization of the concept of meaning. In the following,
unless stated to the contrary, p is assumed to be a proposition drawn from a natural
language. Typically, propositions drawn from a natural language are fuzzy prop-
ositions, that is, propositions which contain fuzzy predicates and/or fuzzy quanti-
ﬁers and/or fuzzy probabilities.
The point of departure in RCT consists of two key ideas: The meaning postulate,
MP, and the truth postulate, TP. MP relates to precisiation of the meaning of
p. More concretely, a proposition is a carrier of information. Information is a
restriction. Reﬂecting these observations, MP postulates that the precisiated
meaning of p—or simply precisiated p—may be represented as a restriction. In
symbols, p may be expressed as
p →X isr R,
where X, R and r are implicit in p. The expression X isr R is referred to as the
canonical form of p, CF(p). In general, X is an n-ary variable and R is a function of
X. Basically, X is a variable such that p is a carrier of information about X. X is
referred to as a focal variable of p. In large measure, the choice of X is subjective. It
should be noted that when X is an n-ary variable, a semantic network representation
of p may be viewed as a graphical representation of the canonical form of p.
Toward a Restriction-Centered Theory of Truth and Meaning (RCT)
11

Examples.
p: Robert is young
Age(Robert) is young
X
R
p: Most Swedes are tall
Proportion(tall Swedes/Swedes) is most
X
R
p: Robert is much taller than most of his friends
Height(Robert) is much taller than 
most of his friends
p: Usually it takes Robert about an hour to get home from work
Travel 
time 
from
office to home iz (approximately 1 hr., usually).
The truth postulate, TP, relates the truth value of p to its meaning. More con-
cretely, consider the canonical form
CF p
ð Þ: X isr R.
TP postulates that the truth value of p is the degree to which X satisﬁes R.
In RCT, truth values form a hierarchy: First-order (ground level), second order,
etc. First order truth values are numerical. For simplicity, numerical truth values are
assumed to be points in the interval. (Fig. 5)
A generic numerical truth value is denoted as nt. Second order truth values are
linguistic. Examples. Quite true, possibly true. A generic linguistic truth value is
denoted as lt. In RCT, linguistic truth values are viewed as restrictions on numerical
truth values. In symbols, lt = R(nt). A generic truth value is denoted as t. t can be nt
or lt.
3.1 Precisiation of X, R and P
Typically, X and R are described in a natural language. To compute the degree to
which X satisﬁes R it is necessary to precisiate X and R. In RCT, what is used for this
purpose is the concept of an explanatory database, ED [16, 20]. Informally, ED is a
collection of relations which represent the information which is needed to precisiate
X and R or, alternatively, to compute the truth value of p. Example. Consider the
proposition, p: Most Swedes are tall. In this case, the information consists of three
relations,
TALL[Height;µ],
MOST[Proportion;µ]
and
POPULATION[Name;
Height]. In TALL, µ is the grade of membership of Height in tall. In MOST, µ is the
grade of membership of Proportion—a point in the unit interval—in most. In POP-
ULATION, Height is the height of Name, where Name is a variable which ranges
12
L.A. Zadeh

over the names of Swedes in a sample population. Equivalently, and more simply,
ED may be taken to consist of the membership function of tall, µtall, the membership
function of most, µmost, and the height density function, h. h is deﬁned as the fraction,
h(u)du, of Swedes whose height is in the interval [u,u + du].
X and R are precisiated by expressing them as functions of ED. Precisiated X, R
and p are denoted as X*, R* and p*, respectively. Thus,
X* = f ED
ð
Þ, R* = g ED
ð
Þ.
The precisiated canonical form, CF*(p), is expressed as X*isr* R*. At this point,
the numerical truth value of p, ntp, may be computed as the degree to which X*
satisﬁes R*. In symbols,
ntp = tr ED
ð
Þ
in which tr is referred to as the truth function (Fig. 6).
What this equation means is that an instantiation of ED induces a value of ntp.
Varying instantiations of ED induces what is referred to as the truth distribution of
p, denoted as Tr(p|ED). The truth distribution of p may be interpreted as the
possibility distribution of ED given p, expressed as Poss(ED|p). Thus, we arrive at
an important equality
Tr pjED
ð
Þ = Poss ED pj
ð
Þ.
In RCT, the precisiated meaning of p is expressed in three equivalent forms. First,
as the precisiated canonical form, CF*(p). Second, as the truth distribution of p, Tr(p|
ED). Third, as the possibility distribution, Poss(ED|p). These representations of the
precisiated meaning of p play an essential role in RCT. The precisiated meaning of p
may be viewed as the computational meaning of p. Of the three equivalent
Fig. 5 Hierarchy of truth values. A numerical truth value is a ﬁrst-order (ground level) truth value.
A linguistic truth value is a second-order truth value. A linguistic truth value is a restriction on
numerical truth values. Typically, a linguistic truth value is a fuzzy set or, equivalently, a
possibility distribution
Toward a Restriction-Centered Theory of Truth and Meaning (RCT)
13

deﬁnitions stated above, the deﬁnition that is best suited for computational purposes
is that which involves the possibility distribution of ED. Adopting this deﬁnition,
what can be stated is the following.
• Deﬁnition. The precisiated (computational) meaning of p is the possibility
distribution of ED, Poss(ED|p), which is induced by p.
A simple example. Consider the proposition, p: Robert is tall. In this case, ED
consists of Height(Robert) and the relation TALL[Height; µ] or, equivalently, the
membership function µtall. We have,
X = Height Robert
ð
Þ, R = tall.
The canonical form reads
Height Robert
ð
Þ is tall.
The precisiated X and R are expressed as
X* = Height Robert
ð
Þ, R* = tall,
where tall is a fuzzy set with the membership function, µtall.
The precisiated canonical form reads
Height Robert
ð
Þ is tall.
Note that in this case the unprecisiated and precisiated canonical forms are
identical. The truth distribution is deﬁned by
ntp = μtall h
ð Þ,
where h is a generic value of Height(Robert).
Fig. 6 A numerical truth
value, nt, is induced by an
instantiation of ED. tr is the
truth function
14
L.A. Zadeh

The basic equality reads
Tr pjh
ð
Þ = Poss h pj
ð
Þ.
More speciﬁcally, if h = 175 cm and µtall(175cm) = 0.9, then 0.9 is the truth value
of p given h = 175 cm, and the possibility that h = 175 cm given p (Fig. 7).
Example. Robert is handsome. In this case, assume that we have a sample popu-
lation of men, Name1, …, Namen with µi being the grade of membership of Namei
in the fuzzy set handsome. The meaning of p is the possibility distribution asso-
ciated with the fuzzy set handsome—the possibility distribution which is induced
by p. The possibility that Namei is handsome is equal to the grade of membership of
Namei in handsome.
A less simple example. Consider the proposition, p: Most Swedes are tall. In this
case, X = Proportion(tall Swedes/Swedes) and R = most. The canonical form of p is
Proportion tall Swedes=Swedes
ð
Þ is most.
The precisiated X and R may be expressed as
X* =
Z h max
h min
h u
ð Þμtall u
ð Þdu,
R* = most,
where most is a fuzzy set with a speciﬁed membership function, µmost.
The precisiated canonical form reads
CF*:
Z h max
h min
h u
ð Þμtall u
ð Þdu is most.
Fig. 7 0.9 = truth value of the proposition Robert is tall, given that Robert’s height is 175 cm.
0.9 = possibility that Robert’s height is 175 cm, given the proposition Robert is tall
Toward a Restriction-Centered Theory of Truth and Meaning (RCT)
15

The truth distribution, Tr(p|ED), is deﬁned by computing the degree, ntp, to
which X* satisﬁes R*,
ntp = μmost
Z h max
h min
h u
ð Þμtalldu


Note that an instantiation of ED induces a numerical truth value, ntp.
Another example. Consider the proposition, p: Robert is much taller than most of
his friends. In this case, assume that X = Proportion of friends of Robert in relation
to whom Robert is much taller, and R = most. The explanatory database, ED,
consists of the relations FRIENDS[Name;µ], HEIGHT[Name;Height], MUCH.
TALLER[Height1;Height2;µ], and Height(Robert). Equivalently, ED may be
expressed as µF(Namei), hi, and µMT(h,hi), i = 1, …, n. In this ED, h = Height
(Robert), hi = Height(Namei), µF(Namei) = grade of membership of Namei in the
fuzzy set of friends of Robert, and µMT(h,hi) = grade of membership of (h, hi) and
the fuzzy set much taller. Precisiated X and R are expressed as,
X* =
1
n ∑
i
μMT h, hi
ð
Þ ∧μF Namei
ð
Þ


, R* = most,
The precisiated meaning of p is expressed as,
Poss EDjp
ð
Þ = μmost
1
n ∑
i
μMT h, hi
ð
Þ ∧μF ið Þ


,
where ∧denotes conjunction.
Note. The concept of an instantiated ED in RCT is related to the concept of a
possible world in traditional theories. Similarly, the concept of a possibility dis-
tribution of the explanatory database is related to the concept of intension.
Precisiation of meaning is the core of RCT and one of its principal contributions.
A summary may be helpful.
3.2 Summary of Precisiation
The point of departure is a proposition, p, drawn from a natural language. The
objective is precisiation of p.
1. Choose a focal variable, X, by interpreting p as an answer to the question: What
is the value of X? Identify the restricting relation, R. R is a function of X. At
this point, X and R are described in a natural language.
2. Construct the canonical form, CF p
ð Þ = X isr R.
16
L.A. Zadeh

3. Construct an explanatory database, ED. To construct ED, ask the question:
What information is needed to express X and R as functions of ED? Alterna-
tively, ask the question: What information is needed to compute the truth value
of p?
4. Precisiate X and R by expressing X and R as functions of ED. Precisiated X and
R are denoted as X* and R*, respectively.
5. Construct the precisiated canonical form,CF* p
ð Þ: X* isr* R*.
6. Equate precisiated p to CF*(p).
7. CF*(p) deﬁnes the possibility distribution of ED given p, Poss ED pj
ð
Þ.
8. CF*(p) deﬁnes the truth distribution of the truth value or p given ED, Tr pjED
ð
Þ.
9. Poss ED pj
ð
Þ = Tr pjED
ð
Þ.
10. Deﬁne the precisiated (computational) meaning of p as the possibility distri-
bution of ED given p, Poss ED pj
ð
Þ. More informatively, the precisiated (com-
putational) meaning of p is the possibility distribution, Poss ED pj
ð
Þ, together
with the procedure which computes Poss ED pj
ð
Þ.
3.3 Truth Qualiﬁcation. Internal and External Truth Values
A truth-qualiﬁed proposition is a proposition of the form t p, where t is the truth
value of p. t may be a numerical truth value, nt, or a linguistic truth value, lt.
Example. It is quite true that Robert is tall. In this case, t = quite true and
p = Robert is tall. A signiﬁcant fraction of propositions drawn from a natural
language are truth-qualiﬁed. An early discussion of truth-qualiﬁcation is contained
in [14]. Application of truth-qualiﬁcation to a resolution of Liar’s paradox is
contained in [15].
In a departure from tradition, in RCT a proposition, p, is associated with two
truth values—internal truth value and external truth value. When necessary, internal
and external truth values are expressed as Int(truth value) and Ext(truth value), or
Int(p) and Ext(p).
Informally, the internal numerical truth value is deﬁned as the degree of
agreement of p with an instantiation of ED. Informally, an external numerical truth
value of p is deﬁned as the degree of agreement of p with factual information, F.
More concretely, an internal numerical truth value is deﬁned as follows.
Deﬁnition.
Int ntp


= tr ED
ð
Þ.
In this equation, ED is an instantiation of the explanatory database, Int(ntp) is the
internal numerical truth value of p, and tr is the truth function which was deﬁned
earlier.
Toward a Restriction-Centered Theory of Truth and Meaning (RCT)
17

More generally, assume that we have a possibilistic restriction on instantiations
of ED, Poss(ED). This restriction induces a possibilistic restriction on ntp which can
be computed through the use of the extension principle. The restriction on ntp may
be expressed as tr(Poss(ED)). The fuzzy set, tr(Poss(ED)), may be approximated by
the membership function of a linguistic truth value. This leads to the following
deﬁnition of an internal linguistic truth value of p.
Deﬁnition.
Int ltp


≈tr Poss ED
ð
Þ
ð
Þ.
In this equation, ≈should be interpreted as a linguistic approximation. In words,
the internal linguistic truth value, Int(ltp), is the image—modulo linguistic
approximation—of the possibility distribution of ED under the truth function, tr. It
is important to note that the deﬁnition of linguistic truth value which was stated in
the previous subsection is, in fact, the deﬁnition of internal linguistic truth value of
p (Fig. 8).
Note. Poss(ED), tr(Poss(ED)) and ltp are fuzzy sets. For simplicity, denote these
fuzzy sets as A, B and C, respectively. Using the extension principle, computation
of ltp reduces to the solution of the variational problem,
μB v
ð Þ = supuμA u
ð Þ
subject to
v = trðuÞ
μC ≈μB.
The external truth value of p, Ext(p), relates to the degree of agreement of p with
factual information, F. In RCT, factual information may be assumed to induce a
Fig. 8 A linguistic truth value, ltp, is induced by a possibilistic restriction on instantiations of ED,
Poss(ED). ltp is a linguistic approximation to the image of Poss(ED) under tr
18
L.A. Zadeh
www.allitebooks.com

possibilistic restriction on ED, Poss(ED|F). In particular, if F instantiates ED, then
the external truth value is numerical. This is the basis for the following deﬁnition.
Deﬁnition. The external numerical truth value of p is deﬁned as
Ext ntp


= tr ED F
j
ð
Þ,
where ED is an instantiation of the explanatory database induced by F.
Simple example. In Fig. 7, if the factual information is that Robert’s height is
175 cm, then the external numerical truth value of p is 0.9.
More generally, if F induces a possibilistic restriction on instantiations of ED,
Poss(ED|F), then the external linguistic truth value of p may be deﬁned as follows.
Deﬁnition.
Ext ltp


≈tr Poss ED F
j
ð
Þ
ð
Þ.
In this equation, ≈should be interpreted as a linguistic approximation. In words,
the external linguistic truth value of p is—modulo linguistic approximation—the
image of Poss(ED|F) under tr.
Example. Consider the proposition, p: Most Swedes are tall. Assume that the
factual information is that the average height of Swedes is around 170 cm. Around
170 cm is a fuzzy set deﬁned by its membership function, µar.170cm. In terms of the
height density function, h, the average height of Swedes may be expressed as
have =
Z h max
h min
uh u
ð Þdu.
The explanatory database consists of µtall, µmost and h. Assuming that µtall and
µmost are ﬁxed, the possibilistic restriction on ED is induced by the indirect pos-
sibilistic restriction
Z h max
h min
uh u
ð Þdu is around 170cm
which is equivalent to the possibility distribution of h expressed as
Poss h hj ave


= μar. 170 cm
Z h max
h min
uh u
ð Þdu


.
An important observation is in order. An internal truth value modiﬁes the
meaning of p. An external truth value does not modify the meaning of p; it places in
evidence the factual information, with the understanding that factual information is
a possibilistic restriction on the explanatory database.
Toward a Restriction-Centered Theory of Truth and Meaning (RCT)
19

How does an internal truth value, t, modify the meaning of p? Assume that the
internal truth value is numerical. The meaning of p is the possibility distribution,
Poss(ED|p). The meaning of nt p is the preimage of nt under the truth function, tr. In
other words, the meaning of p, expressed as the possibility distribution, Poss(ED|p),
is modiﬁed to the possibility distribution Poss(ED|ntp). If the internal truth value is
linguistic, ltp, the modiﬁed meaning is the preimage of ltp, Poss(ED|ltp), under tr
(Fig. 9). More concretely, using the inverse version of the basic extension principle,
we can write
μPoss EDjltp
ð
Þ(uÞ = μtrðPoss EDjltp
ð
ÞÞ tr(u)
ð
Þ,
where u is an instantiation of ED, µPoss(ED|ltp) and µtr(Poss(ED|ltp)) are the membership
functions of Poss(ED|ltp) and tr(Poss(ED|ltp)), respectively.
Simple example. In Fig. 7, the preimage of 0.9 is 175 cm. The meaning of p is the
possibility distribution of tall. The truth value 0.9 modiﬁes the possibility distribution
of tall to Height(Robert) = 175 cm. More generally, when the truth value is linguistic,
ltp, the modiﬁed meaning of p is the preimage of ltp under tr (Fig. 10).
There is a special case which lends itself to a simple analysis. Assume that lt is of
the form h true, where h is a hedge exempliﬁed by quite, very, almost, etc. Assume
that p is of the form X is A, where A is a fuzzy set. In this case, what can be
postulated is that truth-qualiﬁcation modiﬁes the meaning of p as follows.
h true X is A
ð
Þ = X is h A.
h A may be computed through the use of techniques described in early papers on
hedges [4, 11].
Example.
usually true
ð
Þ Snow is white = snow is usually white.
Fig. 9 Modiﬁcation of meaning of p. Modiﬁed meaning of p is the preimage of ltp under tr
20
L.A. Zadeh

Example. (Fig. 11).
It is very true that Robert is tall = Robert is very tall.
A word of caution is in order. Assume that there is no hedge. In this case, the
equality becomes
true X is A
ð
Þ = X is A.
If truth is bivalent, and true is one of its values, this equality is an agreement with
the school of thought which maintains that propositions p and p is true have the
same meaning. In RCT, p and p is true do not have the same meaning. There is a
subtle difference. More concretely, the meaning of p relates to the agreement of p
with a possibilistic restriction on ED. The meaning of p is true relates to a possi-
bilistic restriction which is induced by factual information.
When ltp is an external truth value, the meaning of p is not modiﬁed by ltp.
In RCT, a simplifying assumption which is made regarding the factual informa-
tion, F, is that F may be described as a possibility distribution of instantiations
of ED, Poss(ED|F). The external truth value, ltp, identiﬁes the factual information as
the preimage of ltp under tr,
Fig. 11 Meaning-modiﬁcation induced by hedged truth-qualiﬁcation
Fig. 10 An internal linguistic
truth value modiﬁes the
meaning of p
Toward a Restriction-Centered Theory of Truth and Meaning (RCT)
21

Ext ltp


= tr Poss ED F
j
ð
Þ
ð
Þ
F = Poss EDjExt ltp




.
In conclusion, truth-qualiﬁcation in RCT is paralleled by probability-qualiﬁcation
in probability theory and by possibility-qualiﬁcation in possibility theory.
Truth-qualiﬁcation,
probability-qualiﬁcation
and
possibility-qualiﬁcation
are
intrinsically important issues in logic, information analysis and related ﬁelds.
4
Concluding Remark
The theory outlined in this paper, RCT, may be viewed as a step toward formal-
ization of everyday reasoning and everyday discourse. Unlike traditional theories—
theories which are based on bivalent logic—RCT is based on fuzzy logic. Fuzzy
logic is the logic of classes with unsharp(fuzzy) boundaries. In the realm of
everyday reasoning and everyday discourse, fuzziness of class boundaries is the
rule rather than exception. The conceptual structure of RCT reﬂects this reality.
The theory which underlies RCT is not easy to understand, largely because it
contains many unfamiliar concepts. However, once it is understood, what is
revealed is that the conceptual structure of RCT is simple and natural.
Acknowledgments To Luis Magdalena and Enric Trillas.
References
1. Carnap, R.: Introduction to Semantics. Harvard University Press, Cambridge (1942)
2. Carnap, R.: Meaning and Necessity: A Study in Semantics and Modal Logic, 2d edn.
University of Chicago Press, Chicago (1947)
3. Kirkham, R.L.: Theories of Truth: A Critical Introduction. MIT Press, Cambridge (1992)
4. Lakoff, G.: Hedges: a study in meaning criteria and the logic of fuzzy concepts. In:
Proceedings of the Chicago Linguistics Society, vol. 8, pp. 183–228 (1972)
5. Moore, R.C.: Possible-World Semantics for Autoepistemic Logic, Center for the Study of
Language and Information, Stanford University, (1985)
6. Ramsey, F.P.: Truth and Probability, written 1926. Published 1931, The foundations of
mathematics and other logical essays, Chap. VII, pp. 156–198. In: Braithwaite, R.B. (ed.)
Further Considerations written 1928. Published 1931 op. cit., Chap. VIII, pp. 199–211.
Probability and Partial Belief written 1929. Published 1931, op cit., Ch. IX, pp. 256–257.
Kegan, Paul, Trench, Trubner & Co. Ltd, London. Harcourt, Brace and Co., New York (1931)
7. Roush, S.: Tracking Truth: Knowledge, Evidence, and Science. Clarendon Press, New York
(2005)
8. Tarski, A.: On undecidable statements in enlarged systems of logic and the concept of truth.
J. Symbol. Logic 4(3), 105–112 (1939)
9. Ullmann, S.: Semantics: an introduction to the science of meaning. Barnes & Noble, New
York (1962)
22
L.A. Zadeh

10. Zadeh, L.A.: Fuzzy sets. Inf. Control 8, 338–353 (1965)
11. Zadeh, L.A.: A fuzzy-set-theoretic interpretation of linguistic hedges. J. Cybern. 2, 4–34
(1972)
12. Zadeh, L.A.: Calculus of fuzzy restrictions. In: Zadeh, L.A., Fu, K.S., Tanaka, K., Shimura, M.
(eds.) Fuzzy Sets and Their Applications to Cognitive and Decision Processes, pp. 1–39.
Academic Press, New York (1975)
13. Zadeh, L.A.: The concept of a linguistic variable and its application to approximate reasoning
Part I. Inf. Sci. 8, 199–249 (1975); Part II. Inf. Sci. 8, 301–357 (1975); Part III. Inf. Sc. 9,
43–80 (1975)
14. Zadeh, L.A.: A theory of approximate reasoning. In: Hayes, J., Michie, D., Mikulich, L.I.
(eds.) Machine Intelligence, vol. 9, pp. 149–194. Halstead Press, New York (1979)
15. Zadeh, L.A.: Liar’s paradox and truth-qualiﬁcation principle, ERL Memorandum M79/34.
University of California, Berkeley (1979)
16. Zadeh, L.A.: A fuzzy-set-theoretic approach to the compositionality of meaning: propositions,
dispositions and canonical forms. J. Semant. 3, 253–272 (1983)
17. Zadeh, L.A.: Outline of a computational approach to meaning and knowledge representation
based on the concept of a generalized assignment statement. In: Thoma, M., Wyner, A. (eds.)
Proceedings of the International Seminar on Artiﬁcial Intelligence and Man-Machine Systems,
pp. 198–211. Springer, Heidelberg (1986)
18. Zadeh, L.A.: Generalized theory of uncertainty (GTU)—principal concepts and ideas.
Comput. Stat. Data Anal. 51, 15–46 (2006)
19. Zadeh, L.A.: A note on Z-numbers. Inf. Sci. 181, 2923–2932 (2011)
20. Zadeh, L.A.: Computing with Words—principal concepts and ideas. In: Studies in Fuzziness
and Soft Computing, vol. 277. Springer, Berlin (2012)
21. Zadeh, L.A.: A restriction-centered theory of reasoning and computation, powerpoint
presentation. In: International Conference on Soft Computing and Software Engineering, San
Francisco (2013). Available on request by email
Author Biography
LotﬁA. Zadeh is Professor Emeritus in the Computer Science
Division, Department of EECS, University of California,
Berkeley. In addition, he is serving as the Director of BISC
(Berkeley Initiative in Soft Computing).
LotﬁZadeh is an alumnus of the University of Tehran, MIT
and Columbia University. From 1950 to 1959, LotﬁZadeh was
a member of the Department of Electrical Engineering,
Columbia University. He joined the Department of Electrical
Engineering at UC Berkeley in 1959 and served as its Chair
from 1963 to 1968. During his tenure as Chair, he played a key
role in changing the name of the Department from EE to EECS.
LotﬁZadeh held visiting appointments at the Institute for
Advanced Study, Princeton, NJ; MIT, Cambridge, MA; IBM
Research Laboratory, San Jose, CA; AI Center, SRI International,
Menlo Park, CA; and the Center for the Study of Language and
Information, Stanford University.
LotﬁZadeh is a Fellow of the IEEE, AAAS, ACM, AAAI, and IFSA. He is a member of the
National Academy of Engineering and a Foreign Member of the Finnish Academy of Sciences, the
Polish Academy of Sciences, Korean Academy of Science & Technology, Bulgarian Academy of
Sciences, the International Academy of Systems Studies and the Azerbaijan National Academy of
Toward a Restriction-Centered Theory of Truth and Meaning (RCT)
23

Sciences. He is a recipient of the IEEE Education Medal, the IEEE Richard W. Hamming Medal, the
IEEE Medal of Honor, the ASME Rufus Oldenburger Medal, the B. Bolzano Medal of the Czech
Academy of Sciences, the Kampe de Feriet Medal, the AACC Richard E. Bellman Control Heritage
Award, the Grigore Moisil Prize, the Honda Prize, theOkawa Prize, the AIM Information Science
Award, the IEEE-SMC J. P. WohlCareer Achievement Award, the SOFT Scientiﬁc Contribution
Memorial Award of the Japan Society for Fuzzy Theory, the IEEE Millennium Medal, the ACM 2001
Allen Newell Award, the Norbert Wiener Award of the IEEE Systems, Man and Cybernetics
Society, Civitate Honoris Causa by Budapest Tech (BT) PolytechnicalInstitution, Budapest,
Hungary, the V. Kaufmann Prize, International Association for Fuzzy-Set Management and
Economy (SIGEF), the Nicolaus Copernicus Medal of the Polish Academy of Sciences, the
J. Keith Brimacombe IPMM Award, the Silicon Valley Engineering Hall of Fame, the Heinz
Nixdorf MuseumsForum Wall of Fame, the Egleston Medal, the Franklin Institute Medal, the Medal
of the Foundation by the Trust of the Foundation for the Advancement of Soft Computing, the High
State Award ‘Friendship Order’, from the President of the Republic of Azerbaijan, the Transdisci-
plinary Award and Medal of the Society for Design and Process Sciences, other awards and
twenty-ﬁve honorary doctorates. In 2011, LotﬁZadeh was inducted into the AI Hall of Fame. In 2012,
received the PAAIA Lifetime Achievement Award. In 2013, LotﬁZadeh received the BBVA
Foundation Frontiers of Knowledge Award for the invention and development of fuzzy logic.
LotﬁZadeh is known as the inventor offuzzy logic. His ﬁrst paper, Fuzzy Sets, 1965, is the highest
cited paper in Computer Science (Web of Science) and the seventh highest cited paper in Science
(Web of Science). He has published extensively (over 240 single-authored papers) on a wide variety
of subjects relating to the conception, design and analysis of information/intelligent systems, and is
serving on the editorial boards of over seventy journals.
Prior to the publication of his ﬁrst paper on fuzzy sets in 1965, LotﬁZadeh’swork was concerned in
the main with systems analysis, decision analysis and information systems. His current research is
focused on fuzzy logic, semantics of natural languages, computational theory of perceptions,
computing with words, extended fuzzy logic and Z-numbers.
http://www.cs.berkeley.edu/∼zadeh/
24
L.A. Zadeh

Functional Solution of the Knowledge
Level Control Problem: The Principles
of Fuzzy Logic Rules and Linguistic
Variables
Ali M. Abbasov and Shahnaz N. Shahbazova
Abstract This paper addresses the problem of imitating a teacher evaluating the
students’ levels of knowledge. It proposes application of fuzzy logic to construct
and manage a knowledge control system used for generating evaluating questions.
The system contains a knowledge base with relevant information and a set of rules.
Students build the rules based on the analysis of answers and relevant reactions to
questions. The algorithms governing the system allow for an automatic selection of
sequences of appropriate and customized questions. The presented system for
knowledge control and generating questions is comparable in quality and efﬁciency
with the real teacher’s questioning process.
Keywords Decision making ⋅Uncertainty ⋅Fuzzy logic ⋅Neuro-fuzzy expert
systems ⋅Complex systems ⋅Expert knowledge
1
Introduction
A system for evaluating learnt knowledge and managing it is one of the main
elements of successful educational or research activities. An efﬁcient and effective
process of appraising gained knowledge – called hereafter a knowledge control
process – inﬂuences all aspects of education and research as they rely on the
outcome of learning activities.
A.M. Abbasov
Minister of Communications and High Technologies of the Republic of Azerbaijan,
33 Z. Aliyeva Str., Baku, AZ 1000, Azerbaijan
e-mail: abbasov@mincom.gov.az
URL: http://www.mincom.gov.az
S.N. Shahbazova (✉)
Department of Information Technology & Programming,
Azerbaijan Technical University, 25 H.Cavid Ave., Baku, AZ 1073, Azerbaijan
e-mail: shahbazova@gmail.com
URL: http://www.aztu.gedu.az
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_2
25

From a practical point of view, procedures of knowledge control should allow for
questioning students with the aim of verifying their knowledge and skills in the ﬁeld
of study. The effective control of knowledge should be able to mimic a teacher of the
relevant subject. Therefore, a system that is able to simulate the teacher’s behavior is
the most reasonable to develop. The analysis of the teacher’s behavior in performing
an evaluation process leads to construction of a system for an automated knowledge
control. This system is able to conduct evaluation of students’ knowledge, and
determine correctness and incorrectness of provided answers.
2
Implementation of Intelligent System in Educational
Process
The quality of a teaching system depends on the precise deﬁnition of the charac-
teristics of several key factors deﬁning the student’s knowledge level and abilities:
results of absorbing material recently presented to her, mastering the material
presented in the past, and current moral and psychological state of the student.
The problem of selecting further actions is solved by the system based on these
key factors. The possible actions may be: continuation of the teaching process,
asking questions related to the previous material, repetition of already asked and
answered questions, or completion of the training process.
The system’s electronic catalog stores all data related to the student’s abilities,
her test schedule, etc. In addition, it contains personal data of the student.
In the process of working with the program, the student is able not only to test
her knowledge, but also to learn. This is achieved by the way questions are asked,
and by the presence of all of the questions, comments, and explanations given by
the teacher. Access to the Internet provides the student with the ability to explore
information anywhere in the world, including the best libraries, archives, etc.
Once the sustainable results are achieved for a certain part of the material, the
student can proceed to the next level of difﬁculty of questions. This transition will
allow the student to continue her learning process further.
This step-wise training process gives the student the necessary time to fully
master and strengthen the knowledge of a given material, and then to move to a
new, more difﬁcult material. Each transition is accompanied by a small test on the
previous material, and an analysis of its mastery.
To this date, the work has been done on learning and testing of a group of
students at the same time. A teacher creates shared folders on a particular subject or
subjects. Using these shared folders, the teacher can give tasks and exercises to a
group of students, as well as check their solutions and results. The shared folders
are structured in a way that simpliﬁes the work with groups of students. The teacher
has access to the working directories of students, and is allowed to deal individually
with each student. The same thing happens when a re-take of a course is recom-
mended, or a more detailed analysis of errors in the shared directories is required.
The sophistication of the Intelligent Information System of Learning and Control of
26
A.M. Abbasov and S.N. Shahbazova

Knowledge (IISLCK) allows the teacher to add and edit her material, and to make
corrections according to the latest achievements of science and culture [1].
One of possible ways to improve the functionality of the systems of technical
control of knowledge is the application of intelligent technologies in particular
methods based on the diverse hybrid Expert Systems (ESs). Hybrid ESs represent
different kinds of knowledge and are equipped with conceptual, expert, and factual
methods of its processing.
The main task of the development of hybrid systems is to combine different
forms of representing knowledge and methods of its processing, and merge them
with decision-making approaches of ES. This means, that the actual problem is to
investigate the possibilities of optimal connection of different mechanisms of
knowledge processing to improve the quality, mobility and efﬁciency of ES in
solving problems of a knowledge control process in conditions of uncertainty.
The mobility of ES is due to the mobility of the knowledge base (KB) and its
ability to replenish material/facts/data from different information components
(database, bases of expert knowledge (BEK), the base of conceptual knowledge
(BCK), dynamic ﬁles, etc.), as well as various procedures of drawing conclusions.
The concretization of knowledge processing in solving problems decomposes them
into accurate and inaccurate, complete and incomplete, static and dynamic, single-
valued and multi-valued, etc. In addition, the expert knowledge is inaccurate due to
their subjective character. The approximation and multiple meanings of knowledge
processing means that the ES has to deal with several alternative areas. Therefore,
the processing of incomplete knowledge can use several sources of knowledge.
The application of a fuzzy logic hybrid ES for knowledge control may have at
least three implementations:
(1) Processing of fuzzy uncertainty of expert expressions, i.e. when the pre-
condition is fuzzy variables, but an inference machine is a data extraction
mechanism from these preconditions.
(2) Using a matrix of fuzzy connections, determining a number of factors and
preconditions. The matrix contains the fuzzy relations between variables,
represented as real numbers [0, 1], and determines the cause of a condition.
The matrix and factors form equations of fuzzy relations. The resulting system
is solved using minimum-maximum fuzzy inference mechanism.
(3) Using fuzzy conclusions. This approach is most often used in the construction
of fuzzy knowledge bases [2].
The application of fuzzy hybrid ES to solve problems and control parameters of
knowledge processing extends the capabilities of this class of intelligent systems, as
well as increases their ﬂexibility and mobility. This allows conducting expert
evaluation of a large number of variants, increasing the credibility and accuracy of
the evaluation of the results.
In this paper, the main principles of construction of a neuro-fuzzy hybrid ES
with diverse knowledge and its analysis in conditions of uncertainty of its
parameters are considered. Additionally, the application of a dynamic knowledge
base combined with neural networks (NN) is being investigated [3].
Functional Solution of the Knowledge Level Control Problem …
27

In the neuro-fuzzy hybrid ES, standard model (SM) is stored in the knowledge
base containing processed knowledge, and is reﬁned in the process of acquiring new
knowledge. The real model is formed in a database environment, and communica-
tion with the EM is achieved via the user’s requests. Solving the problem of
designing an intelligent system for quality knowledge control built based on a hybrid
ES is done with taking into account the characteristics of the environment of ES.
The hybrid ES consists of the following parts: a database that stores standard and
factual evidence about the process; the results of their comparison, conceptual,
physical and info logical models; knowledge base (KB) – its static part (knowledge
is stored in the form of expert knowledge (of products) as well as formulas, facts,
dependencies, tables, concepts speciﬁc subject area), and its dynamic part (the
knowledge is stored in combined models of NN in the form of standard of dynamic
processes taking into account the partial or complete uncertainly parameter of
control); a mechanism of logic inference that is based on an algorithm for gener-
ating cause and effect network of events functional-structural model; adaptation
mechanism to coordinate the work of the database (DB) and KB in the process of
logical inference depending on the situation, explaining the mechanism, which is an
interpretation of the process of logical inference; planner coordinating the process
of solving the problem; solver for ﬁnding effective solutions to positive, negative
and mixed statements of problems.
The content, form and algorithms for representing information inside the hybrid
ES are ﬂexible and depend on the complexity of a situation being modeled, and the
speciﬁc and individual characteristics of the user.
The expert presents her knowledge in the form of sets of examples. A derivation
tree is used as the internal form of presentation of the knowledge. A set of examples
is described by attributes. All examples of the same structure, as deﬁned by its
attributes, are linked by logical transitions. In this case, the relevant trees of
inference are combined in such a way that at the terminal vertex of one tree another
tree is added.
The Computational Model of the ES and the DB in solving problems under
uncertainty is given in the form:
W = < A, D, B, F, H > ,
ð2:1Þ
where A – is a set of attributes of DB and KB; D – denotes domains (attribute
values of DB and KB); B – is a set of functional dependencies deﬁned over the
attributes; F – denotes descriptions of all types used in the functional dependencies
B; and H – is a set of fuzzy relations over a set of attributes A [4].
One of the most difﬁcult aspects to achieve in the hybrid ES is the requirement of
dealing with different forms of knowledge representation, such as frames, semantic
networks, databases, the concepts presented in KB, neural networks, fuzzy logic,
genetic algorithms. All of these components have to share a single information
space in the hybrid ES. For example, in the hybrid ES, diverse knowledge is stored
in static components of ES, while dynamic knowledge about the current state of
information is stored in neural networks. The modern information and database
28
A.M. Abbasov and S.N. Shahbazova
www.allitebooks.com

technology (for example, Object Linking and Embedding paradigm) can easily
share diverse knowledge within a single information space [5].
It should be noted that the approach considered here, i.e., the application of
hybrid ES as the basis for the intelligent system for knowledge control in the
presence of uncertainty allows for:
(1) Actively applying the diverse knowledge (conceptual, structural, procedural,
factual, base rule with membership function, rules and fuzzy rules of DB, KB,
BEK procedures) together with inference mechanisms for ﬁnding effective
solutions to the problem of determining the level of student’s knowledge;
(2) Summarizing and improving the conceptual model of representation of diverse
knowledge among relational DB and the managed DBMS; and interacting
with the core of hybrid ES;
(3) Effectively solving the problem of optimizing and distributing information
streams among individual subsystems of the hybrid ES under the conditions of
uncertainty.
The methodology of constructing diverse knowledge storage for hybrid neuro-
fuzzy ES includes the following stages [6]:
(1) The formalization of the domain (the development of a conceptual model);
(2) The description of knowledge model as individual concepts (knowledge) in
the KB;
(3) The formation of KB with the base rule as a managing components of intel-
ligent core;
(4) The description of diverse information to control the student’s knowledge in
the individual sub-systems of the hybrid ES (DB, KB, EKB, a graphical DB,
the computed ﬁles);
(5) Selecting a neural-network model and learning rules;
(6) Development of fuzzy logic procedures;
(7) Distribution of information streams between the ES and its individual
subsystems;
(8) Testing individual subsystems of the ES;
(9) Testing the neuro-fuzzy hybrid ES.
3
The Methodology for Knowledge Control
An important element of the learning system is its ability to make decisions
regarding the level of difﬁculty of questions which should be posed to students.
This should be preformed based on the results of answering previous questions. The
solution to this problem depends on numerous parameters, most of which are
unknown to the system. A fairly accurate answer can be found with the help of the
mathematical apparatus of fuzzy logic [7].
Functional Solution of the Knowledge Level Control Problem …
29

The analysis of the current situation depends on following:
(1) Questions answered correctly by a student;
(2) Questions answered incorrectly by a student;
(3) Question answered incorrectly to previous questions by a student;
(4) Preliminary analysis of a student’s ability;
(5) The number of correct answers coupled with their difﬁculty and in respect to
erroneous answers.
This list reﬂects the real computational tasks. A decision-making process is
carried out in order to select questions, which according to the program, corre-
sponds to student’s ability. An incorrect answer triggers a re-valuation process of
the data about the student and leads to less difﬁcult questions to be asked in the next
time. In the case of a correct answer, the program asks questions with progressive
difﬁculty. This decision-making method allows an individual to make a progress
during the learning process [8]. Furthermore, it gives the most accurate evaluation
of the student abilities.
At the end of the evaluation process, when both student and teacher want to sum
up the result of the educational session, the program analyses the number of correct
answers and their complexity. It starts with updating the relevant database record of
the student, and then begins the process of analysis that aims at providing updated
and correct information about the student.
This information can include: the current level of mastery of the subject of the
student; comparison with previous results of analysis of the student’s incorrect
responses, the visualization of the correct answers with commentary, as well as
comments provided by the teacher while entering questions into the database [9].
The importance of evaluation of the executed test could be adjusted by the
program and/or by the teacher. This approach allows for performing individual
pretests and tests at different levels of difﬁculty.
As stated, due to the large number of external parameters a decision-making
process is done with the help of the mathematical apparatus of fuzzy logic. The
responsible subsystem also includes conducting tests that satisfy the following
requirements [10, 11]:
(1) Protecting answers from unauthorized access;
(2) Preventing a student from modiﬁcation of the number of correct answers to
questions;
(3) Providing equal conditions for the tests.
During the process of testing, the next question is read from the database based
on the inference result obtained from a knowledge base located in the network. The
question is displayed in a form convenient for the student (Fig. 1).
30
A.M. Abbasov and S.N. Shahbazova

These expressions can be represented in the form of conditional statements of
complicated structure. As a very simple example is the expression of the form:
If the Previous Answer = Right,
THEN Correct Answers = Correct Answers + 1
The next level of complication of the statements is to generate weighted ques-
tions (complexity):
If the Previous Answer = right,
Then Weight Correct Answer = Weight Correct Answer +
Table Weight ðIndex Current AnswerÞ
The level of intelligence of the subsystems can be increased by adding records of
the elapsed time and other parameters, and providing complicated logic expres-
sions. In such a case the level of testing provided by subsystems can be compared
with surveys conducted by the real teacher [12]. Additional parameters in mathe-
matical expressions provide the descriptions of the following characteristics: the
ability to remember, attentiveness, reaction speed, decision-making speed, reading
speed, etc.
In the process of working with the program, the student cannot only test her
knowledge, but also learn. This is accomplished when the question is asked, and
also with the access to all of the comments and explanations given by the teacher.
Once stable results are obtained for a certain group of questions, the student can
move on to the questions on the next level of complexity [13]. This transition
enables the student’s further development without being stack at the achieved
results.
The information base 
of the question and 
answer
Selection of 
difficulty the first 
question
Output of the 
question and wait for 
variant of the answer
Block of analysis and decision 
making
Selection of more 
difficult question
Selection of less 
difficult question
Calculation evaulation 
of knowledge
Fig. 1 Simpliﬁed block-schema of the control system of knowledge
Functional Solution of the Knowledge Level Control Problem …
31

A gradual learning process gives the student the necessary time to complete
mastering and strengthening the material, and then transit to a new, more complex
material. Each transition is accompanied by a small test containing questions related
to the previous material, and an analysis of its mastering.
4
Decision Making and the Knowledge Control
in the Managing System
The algorithm of choosing the ﬁrst and subsequent questions uses the results of
carrying out the following tasks.
• preliminary analysis – used to evaluate the level of student’s knowledge for
making a decision regarding the ﬁrst question (students lagging in knowledge
assimilation are asked questions from a group of simple questions, while pre-
pared students are given more difﬁcult questions) [1] (Fig. 2).
• The formula below represents one of functions of the decision making block. Its
essence comes down to choosing the next question, which corresponds to the
student’s level of knowledge. If an incorrect answer is chosen during the
evaluation, the student will be given a less difﬁcult question (2). If the correct
answer is selected, the program will choose the more difﬁcult question (1). The
decision process can be described in the following way.
The most common 
questions (Min) 
The most difficult 
questions (Max) 
Questions of average 
difficulty
Questions from 
which begins 
lagging student
Questions from 
which begins 
prepared student
Fig. 2 Strategy for selection of the ﬁrst question
32
A.M. Abbasov and S.N. Shahbazova

• A student answers the previous question correctly: in such a case the student is
asked a question of increased difﬁculty (Fig. 3). The formula for selecting the
next question is [6]:
Q =
Max A +
ð
Þ + Max A −
ð
Þ
ð
Þ
2
± 2 %
ð4:1Þ
where, Q is the next question, (A+) is the level of difﬁculty of a correctly answered
question, Max(A+) is the maximum level of difﬁculty of the questions to which
the student gave the correct answers, (A-) is the level of difﬁculty of an incorrectly
answered question, and Max(A-) is the maximum level of difﬁculty of the
question to which the student gave the incorrect answer. In case the student has not
given an incorrect answer yet, the assigned value is the maximum level of difﬁ-
culty of the questions for this course. ± 2 % is the maximum deviation in the level
of the next asked question, and it represents randomness in a selection process.
• A student answers the previous question incorrectly: in this case the student is
asked a less difﬁcult question (Fig. 4). The selection formula of the next
question is [7]:
Q =
Max A −
ð
Þ + Min A +
ð
Þ
ð
Þ
2
± 2 %
ð4:2Þ
where, Min(A+) is the minimum level of difﬁculty of the correctly answered
question, while Max(A-) is the maximum level of difﬁculty of the question to
which the student gave the incorrect answer. If the correct answer was not given
by the student, the assigned value is the minimum level of difﬁculty of the
questions for this course.
The process of testing
a student
Correct
answer  
Next
question 
A+
Q
Decision making after the 
first asked question
Fig. 3 The strategy of selection question after the correct answer
Functional Solution of the Knowledge Level Control Problem …
33

The deviation included in the formulas ensures that for every student group
there are no be two students that are given the same questions, even if the order
of correct and incorrect answers are the same [2].
• processing the results and making a decision related to the ﬁnal evaluation or
continuation of testing – the number of correctly answered questions multiplied
by their difﬁculty in relation to the number of mistakes and sets of correctly and
incorrectly answered questions are the input to the decision-making subpro-
gram; this results in a ﬁnal evaluation or, if there remains a high probability of
uncertainty, in continuation of testing (according to formula 4).
Z + jP = f
∑
N
i = 1
A +
i


N
,
∑
M
j = 1
A −
j


M
, A +
1 , A +
2 , . . . , A +
N


, A −
1 , A −
2 , . . . , A −
M


0
B
B
B
@
1
C
C
C
A
ð4:3Þ
where, Z – evaluation of knowledge, P – uncertainty of evaluation, f – the decision
making subprogram which works based on the following characteristics of conducted
testing, (Ai+) – the set of difﬁculty levels of correctly answered questions, (Aj–) – the
set of difﬁculty levels of incorrectly answered questions, N – number of questions
with the correct answers, M – number of questions with the incorrect answers [5].
The result of the formula 4 is a complex number. This number represents the
response of the decision making subprogram and indicates a degree of uncertainty
in the students’ knowledge [3]. This uncertainty provides a level of conﬁdence in
the evaluation process. Its value relates to the coverage of questions in the learning
process. Higher the coverage less the uncertainty, which depends on the number of
asked questions (Fig. 5). For example, the student can answer only a few questions
The process of 
testing a student
Correct
answer
Incorrect
answer 
A+
Q
Decision making after the 
second asked question
A-
Next
question 
Fig. 4 A strategy for selecting the next question after an incorrect answer
34
A.M. Abbasov and S.N. Shahbazova

and obtain an excellent score, but the uncertainty in this case would be very high,
due to the fact that only several questions covering a signiﬁcant amount of extre-
mely difﬁcult course material have been asked.
Thus, the system containing the ﬂexible algorithm of questioning, allows the
teacher to decide about the volume of material covered in the course and the
number of questions that should be asked to students in order to make an accurate
determination of students’ knowledge [4]. The preformed test may be an inter-
mediate exam related to a small amount of learning material (10–20 questions in
20–30 min), or a full-scale exam based on the entire volume of the studied material
(100–150 questions in 3–4 h).
5
Conclusion
The developed decision-making algorithm can determine the level of knowledge of
a tested person on the basis of questioning with the minimum possible number of
questions. This allows providing an evaluation of the students’ knowledge level,
over a short period of time, with a high degree of reliability when compared to the
traditional method of questioning conducted by a teacher. Hence, an ingenious
system of knowledge control has been developed via the application of fuzzy logic.
It is very close to imitating the teacher’s behavior in the process of student’s
questioning. It includes ability and precision that have not been seen before in any
automated system. The proposed system integrates elements of expert systems,
processes of development and populating a database, as well as construction of
powerful and ﬂexible rules. All system’s aspects described above indicate effec-
tiveness and ﬂexibility of algorithms and functions used to build the knowledge
control system. and conﬁrm usefulness of applied newest technologies.
The result of testing of 
students
-
+
+
+
+
+
+
+ -
+
+
+
-
-
-
+
+-
-
-
-
+
+
+
+
+ -
+ +
+
-
-
+
+-
-
-
Fig. 5 Example distribution
of answers after testing
Functional Solution of the Knowledge Level Control Problem …
35

References
1. Shahbazova, S.N., Freisleben, B.: A network-based intellectual information system for
learning and testing. In: Fourth International Conference on Application of Fuzzy Systems and
Soft Computing, pp. 308–313. Siegen, Germany (2000)
2. Barsky, A.B.: Neural networks: recognition, management, decision-making, pp. 30–63.
Finance and statistics, Moscow (2004)
3. Bouchon-Meunier, B., Yager, R.R.: Fuzzy Logic and Soft Computing (Advances in Fuzzy
Systems: Application and Theory), pp. 84–93, 103–119. World Scientiﬁc (1995)
4. Bellman, R., Zadeh, L.A.: Decision-Making in Ambiguous Circumstances, Issues Analysis
and Decision-Making, pp. 180–199. Springer (1976)
5. Bernshteyn, L.S., Bojenyuk, A.V.: Fuzzy Models of Decision Making: Deduction, Induction,
Analogy, pp. 78–99. Univ Tsure, Taganrog (2001)
6. Nikravesh, M., Aminzadeh, F., Zadeh, L.A.: Soft Computing and Intelligent Data Analysis in
Oil Exploration, pp. 273–287 (2003)
7. Zadeh, L.A.: A New Approach to the Analysis of Difﬁculty Systems and Decision Processes,
pp. 23–37. Knowledge, Mathematics Today (1974)
8. Gorbunova, L.G.: On the realization of the rating system in pedagogical high schools. In:
Proceedings of 2nd International Technical Conference, Part 1.—pp. 105–106. University
Education, Penza (1998)
9. Nikravesh, M., Zadeh, L.A., Kacprzyk, J.: Soft Computing for Information Processing and
Analysis, pp. 93–99 (2005)
10. Hanss, M.: Applied Fuzzy Arithmetic: An Introduction with Engineering Applications, 1st
edn., pp. 100–116, 139–147. Springer (2004)
11. Jang, J.S.R., Sun, C.-T.: Neuro-fuzzy modeling and Control. Proc. IEEE 83(3), 378–406
12. Shahbazova, S.N.: Application of fuzzy sets for control of student knowledge. Appl. Comput.
Math. Int. J. 10(1), 195–208 (2011) (Special Issue on Fuzzy set theory and applications,
ISSN:1683-3511)
13. Galushkina, A.I.: Scientiﬁc. Ser. Neyrokompyute ¬ ry and their Applications, vol. 4,
M. IPRZhR, p. 156 (2001)
14. Abbasov, A.M., Shahbazova, S.N.: Model of the applicability of the expert systems based on
neural networks technology and hybrid systems for decision making. In: Zadeh, L.A.,
Abbasov, A.M., Yager, R.R, Shahbazova, S.N., Reformat, M.Z. (eds.) Recent Developments
and New Directions in Soft Computing. Studies in Fuzziness and Soft Computing, vol. 317,
pp. 3–18. ISBN:978-3-319-06322-5, 313030-1 En June, 2014. Springer (2014)
15. Zadeh, L.A., Kacprzyk, J.: Fuzzy Logic for the Management of Uncertainty, pp. 75–84. First
Printing edition, Wiley-Interscience (1992)
16. Zadeh, L.A., Klir, G.J., Yuan, B.: Fuzzy Sets, Fuzzy Logic, and Fuzzy Systems: Selected
Papers by LotﬁA. Zadeh, pp. 60–69 (1996)
17. Shahbazova, S.N.: Applied research in the ﬁeld of automation of learning and knowledge
control. In: Soft Computing : State of the Art Theory and Novel Application. Studies in
Fuzziness and Soft Computing, pp. 223–240. Springer (2012)
18. Yager, R., Filev, D.: Essentials of Fuzzy Modeling and control. Wiley, New York (1994)
19. Shahbazova, S.N.: Development of the knowledge base learning system for distance
education. Int. J. Intell. Syst. 27(4), 343–354. Wiley Periodicals, Inc., Wiley–Blackwell (2012)
20. Shahbazova, S.N.: Creation the model of educational and methodical support based on fuzzy
logic. In: Zadeh, L.A., Abbasov, A.M., Yager, R.R., Shahbazova, S.N., Reformat, M.Z. (eds.)
Recent Developments and New Directions in Soft Computing. Studies in Fuzziness and Soft
Computing, Vol. 317, pp. 317–330. ISBN:978-3-319-06322-5, 313030-1 En June, 2014,
Springer (2014)
36
A.M. Abbasov and S.N. Shahbazova

Authors Biography
Professor Dr. Ali M. Abbasov - Minister of Communications
and High Technologies of the Republic of Azerbaijan. He has a
long and remarkable career in academia, education and the
public sector. His recent experience includes the following
positions: Director of Institute of Information Technologies of
the National Academy of Sciences of the Republic of Azerbaijan
(1991–2000), Rector of Azerbaijan State Economic University
(2000–2004), Minister of Communications and High Technol-
ogies of the Republic of Azerbaijan (2004– present).
As a minister, Professor Dr. Ali Abbasov pays special
attention to the acceleration of transition to the information
society and formation of the digital economy, application of
e-government solutions and new technologies, development of
broadband services and human recourses. He is an initiator of a
few national and regional projects and conducts their implementation. He also played special role
in establishment and further development of the internet in Azerbaijan.
Professor Dr. Ali M. Abbasov has lectures and researches in the ﬁelds of digital economy,
network design and application, artiﬁcial intelligence, Big data and some others. He did his PhD in
microelectronics at the Academy of Sciences of Ukraine. In 1994, he defended his thesis, entitled
“Information processing and systems management”, and was awarded with degree of doctor of
technical sciences. Dr. A.M. Abbasov is a full-time professor since 1996, and is a full member of
the National Academy of Sciences of Azerbaijan since 2001. Furthermore, he is full member The
World Academy of Sciences, International Informatization Academy, International Telecommu-
nication Academy, International Academy of Engineering and as a fellow of the IEEE.
Professor A.M. Abbasov was a member of the National Parliament from 2000 through 2004,
and was a member of Parliamentary Assembly of the Council of Europe for the period of
2001–2004. He is a member of the Strategy Council of the Global Alliance for ICT and
Development since 2008, and is a Commissioner of the Broadband Commission for Digital
Development since 2010.
Assistants: Isbendiyar M. Aliyev, +99412 5982377, isbendiyar@mincom.gov.az
Shahnaz N. Shahbazova, +99412 4374540, shahbazova@gmail.com
Associate Professor Dr. Shahnaz N. Shahbazova received her
Candidate of Technical Sciences degree in 1995 and she is
Associate Professor since 1996. She serves more than 30 years at
“Information Technology and Programming” Department of
Azerbaijan Technical University. She is a Doctor of Philosophy
in Engineering Science and International Personnel Acad-
emy UNESCO (2000). She is an academician of the Interna-
tional Academy of sciences named after LotﬁA. Zadeh, 2002
and vice president of the same academy, 2014.
Her research interests include Artiﬁcial Intelligence, Soft
Computing, Intelligent system, Machine Learning Techniques to
Decision Making and Fuzzy Neural Network. She was awarded
research grants in the following countries: India (1998),
Germany-DAAD (1999), Germany-DAAD (2003), USA, UC
Berkeley, Fulbright Visiting Scholar (2007/2008), Germany-
DAAD
(2010),
Scientiﬁc
Development
Foundation
Under
the
President
of
Azerbaijan
(2014/2015). She is a member of Berkeley Initiative in Soft Computing group (BISC), New
York Academy of Sciences, member of “Deﬁned Candidate Dissertation” Society and member of
Functional Solution of the Knowledge Level Control Problem …
37

International Women Club, Azerbaijan. Dr. Shahbazova was invited to serve as a Program
Committee member for over 30 international conferences and as reviewer for nearly 40
international journals. She was an elected board director of the North American Fuzzy Information
Processing Society (NAFIPS) and served as Program Chair of Organizing committee for the
NAFIPS Conferences. Dr. Shahbazova is a general chair of World Conference on Soft Computing.
Also, she is an international expert UNESCO of implementation ICT in educational environment
in Azerbaijan. She is co-editor of the books: “Soft Computing: State of the Art Theory and Novel
Applications”, (Springer, Series Title: Studies in Fuzziness and Soft Computing, 2011), “Recent
Developments and New Directions in Soft Computing” (Springer, Series Title: Studies in
Fuzziness and Soft Computing, 2014) and Soft Computing: New Directions in Foundations and
Applications (Springer, Series Title: Studies in Fuzziness and Soft Computing, 2015).
Dr. Shahnaz N. Shahbazova is a member of following projects: “Application ICT in education
environment” (USA, Indiana, 2001), Information-communication technologies and higher
education-priorities of modern society development”(Saint Petersburg, 2009), “Establishment of
Educational Network System of Azerbaijan Technical University” (Korea, KOICA, 2011), “The
problem of intelligent fusion of multi-source information in the social network model based on
fuzzy sets” (Baku, 2014/2015).
38
A.M. Abbasov and S.N. Shahbazova
www.allitebooks.com

Learning Systems with FUZZY
Sang Wan Lee and Z. Zenn Bien
Abstract Fuzzy techniques have been proven to effectively tackle the problems of
uncertainty in relationships among variables in systems that learn to adapt to a
changing environment. This paper outlines our challenges for the last 25 years to
design learning systems with fuzzy techniques and their applications to many real
world problems. We then focus on the development of human-in-the-loop systems,
such as a smart home or an assistive robotic environment, that involve different
types of learning strategies. This warrants a full consideration of learning mecha-
nisms in humans that mediate action-selection. We envisage that the principles of
fuzzy theory, when combined with what we know about computational learning
mechanisms in the human brain, will offer a practical guidance on how we design
learning systems to advance user’s experience in real-world scenarios.
Keywords Learning system ⋅Fuzzy ⋅Human-in-the-loop system ⋅System
design ⋅Human brain ⋅Smart home ⋅Multiple learning systems
1
Introduction
Many real-world problems pose daunting challenges for the design of engineering
systems. The crux of them is arguably uncertainty inasmuch as one variable may
not be exclusively dedicated to a single part of a system. There is a grey area where
one variable has multiple roles in a system while there is a black and white area
where contribution of a variable to the system is distinctive. Fuzzy theory directly
S.W. Lee (✉)
Computation & Neural Systems, California Institute of Technology, Pasadena, USA
e-mail: swlee@caltech.edu
Z. Zenn Bien
Department of Electrical Engineering, KAIST, Daejeon, Republic of Korea
e-mail: zbien@kaist.ac.kr
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_3
39

addresses the problem of uncertain nature of the relationship among variables,
affording the fuzzy systems the leverage to model uncertain environments.
One other problem that raises challenges is the fact that a situation changes over
time. This is why a system lacking an ability to cope with it sometimes fails to
deliver credible performance in real-world situations even though the system has
been previously proven to work well in a controlled environment. The system
therefore needs to learn to adapt to a changing environment.
This paper describes our challenges for the last 25 years to design learning
systems with fuzzy techniques as well as their applications to many real world
problems. First, we introduce a few examples of designing fuzziﬁed controllers that
replace conventional control systems. These techniques are also applied to build
human-in-the-loop systems in two different levels – one focusing on action rec-
ognition and the other focusing on intention reading underlying those actions. We
then show how these ideas lead to an invention of integrated systems, such as a
smart home or an assistive robotic environment. Since we have learned that design
of such integrated systems essentially involves a combination of multiple learning
systems, we ﬁnally suggest a new direction of system design based on what have
been known about learning mechanisms in the human brain.
2
Reinventing Control Systems
Performance of an inference system depends on how accurately it describes rela-
tionships among variables. Considering that the degree of complexity exponentially
increases with the number of internal variables, designing such systems inevitably
entails the risk of overﬁtting. This means that the system is vulnerable to noise or
change in an environment. Fuzzy logic remedies this problem by quantifying the
amount of uncertainty in the relationships among internal variables of an inference
system [1, 2]. The fuzzy technique has also been proven to be effective in designing
an adaptive controller for nonlinear systems [3].
The idea of encoding uncertainty by means of fuzzy rule bases has been suc-
cessfully applied to designing a fuzzy controller for many real-world applications.
The technique allows a system to effectively resolve inconsistency in fuzzy rule
bases [4]. This encourages us to deal with more realistic issues, such as multi-
objective or time-delayed system design [5, 6].
Another line of research is to maintain reliable system performance in a dynamic
environment. To meet this need, we attempted to design a nonlinear multi-input-
multi-output system (MIMO) in such a way that incorporates learning capability
[7]. The ﬁrst challenge was that it is difﬁcult to determine whether we create a new
rule base or modify existing ones when we have a new set of observations. We
solved this problem by borrowing an idea from rough set theory, by which we can
ﬁnd a minimal set of rules given new examples [8]. The next challenge was that the
learning requires supervision for ﬁne-tuning. The reinforcement learning, a semi-
supervised learning technique based on Markovian decision process [9], have been
40
S.W. Lee and Z. Zenn Bien

shown to be a necessary component for dealing with system uncertainty [10, 11].
The combination of fuzzy systems and reinforcement learning techniques evolved
into more sophisticated system designs incorporating a various types of neural
networks, demonstrating that the combined system yields dramatic performance
improvement in many real-world applications, such as gesture recognition, facial
expression recognition, and even general-purpose on-line adaptive system, [12–15].
3
Learning in Human-in-the-Loop Systems
There has been a steadily-growing interest in developing service robotic systems
that are capable of serving for human directly. For effective control and manage-
ment, the robots and human are often equally considered as subsystems of the
system: this type of system is called a “human-in-the-loop system”, where the
occurrence of two-way interactions between human and robots is inevitable (for
example, see Fig. 1a). In our studies, we restricted our attention to service robotic
systems that are intended to assist the elderly or the persons with physical disability,
and advocated that one of the major considerations for designing such robotic
systems is “human-friendliness”. In doing so, the robot agents need to exert all the
possible functional capabilities when interaction takes place, including sensing,
recognition, and decision making, to the extent that the system places a minimum-
possible burden to the on-users.
Technical challenge arises when designing each individual robot agents. In order
for the robot agents to function as a viable observer or controller, it is necessary to
learn to recognize various forms of human physical motion. However, difﬁculty
arises when there is uncertainty either in human motions or in environment. Sub-
stantial progress has been made for the last twenty years toward developing reliable
systems that directly tackle this challenge.
The study focuses on two different levels of recognition. The lower level of
recognition deals with physical motions, such as gesture or footprint, and the higher
level of recognition considers subtle features that underlies such motions, such as
facial expressions, emotional states, or action planning. It is noted that the former
approach enables us to design more reliable and robust system, while the latter
creates an opportunity for more efﬁcient two-way interactions between a human and
robots by making predictions about future actions.
3.1 Learning to Recognize Observable States – Physical
Motions
In many real world applications, an image processing lacking robustness against
variability of color or edge is often doomed to failure of recognition. Our previous
studies have demonstrated that fuzzy technique is an efﬁcient tool for robust image
Learning Systems with FUZZY
41

Fig. 1 Different types of human-in-the-loop systems. (a) KARES robotic agent [35]. The system
is equipped with a robotic arm, an eye-mouse, and EMG-based control module. (b) Multiple types
of recognition systems. The fuzzy technique has been applied to various recognition problems,
demonstrating its effectiveness in resolving uncertainty in patterns of physical motions, such as
hand gestures (upper-left), walking (upper-right), electromyography (EMG) signals (lower-left).
The fuzzy learning technique has been also shown to be useful for reading-out of human intention,
such as facial expressions (lower-right)
42
S.W. Lee and Z. Zenn Bien

recognition [16–18]. This is perhaps not a surprising success considering how
efﬁciently the fuzzy rule deals with uncertainty in these variables.
We then focused on developing a robust gesture recognition system: how does
the system successfully learn to recognize hand commands or a sign language when
faced with subjective and noisy representation of gestures? We tackled this problem
by combining a multi-layered neural network and a fuzzy rule base that translates
dynamic trajectories of gestures into discrete entities [13, 19]. This implementation
has led to more interesting idea that the gesture can be used as a soft remote
controller in service robotic environment [20] (Fig. 1b; upper-left), and later
evolved to design of a beat gesture recognition system that interacts with an
automatic music agent, such as a piano playing robot [21]. This technique has also
been applied to systems with adaptation capability, in which the system can explore
a new type of gestures [22] and automatically learn to recognize a new user’s
gestures [23]. It is noted that our studies on gesture recognition over the 10 years
ensued an integrated sign language recognition system that is capable of recog-
nizing and rendering more than 400 sign word gestures in real-time [13, 19, 24], as
well as a patent on baby sign-language recognition [25] and a spin-off product
which is now manufactured by a company.
The developments for dynamic hand gestures recognition and learning systems
afford insight into how the fuzzy technique and learning mechanisms serve to
resolve uncertainty in human motions in general. It stimulates another type of
studies that focus on person identiﬁcation based on dynamic patters of walking
(Fig. 1b; upper-right). Speciﬁcally, the system recognizes a sequence of footprints
by means of an estimation of foot shapes and a trajectory of center of gravity
[26, 27]. By virtue of the fact that the recognition process requires a minimal effort
but natural walking, it suggests an alternative to conventional identiﬁcation based
on ﬁnger prints or eyes which require extra processes for authentication. It also
opens up a possibility of providing personalized services in a service robotic
environment [28].
Whilst a camera and a pressure sensor have been demonstrated to be an effective
means to learn from gesture and walking patterns, respectively, Electromyography
(EMG) offers us more detailed guidance on what motions they actually plan to
perform (Fig. 1b; lower-left). We have demonstrated that application of fuzzy
techniques surmount a difﬁculty in counteracting adverse effects, such as fatigue or
class inseparability [18, 29]. It is noted that this type of systems is particularly
useful for the disabled or amputees given that the brain sends a distinctive signal
pertaining to an intended motion to peripheral muscles.
3.2 Learning to Recognize Latent States – Intention Reading
The above mentioned systems are expected to function to learn from observations.
However, perhaps more fundamental challenge to the study of human motions is
how these actions take place in the ﬁrst place, in other words, what are the hidden
Learning Systems with FUZZY
43

states underlies these actions. We hypothesized that motivational/emotional states
play a pivotal role in galvanizing us into those actions.
A large amount of literatures assume that facial expression is an embodiment of
emotional states, and this premise indeed helped us work out a practical solution to
many human-robot interaction problems [30, 31]. In particular, wrinkles or stret-
ched shapes of a face are known to be very effective features for recognizing facial
emotions [32]. We have made a series of attempts to take these features into account
(Fig. 1b; lower-right). First off, a system was proposed to establish a solid
knowledge base of human experts, namely “fuzzy observer”, which indirectly
quantiﬁes the amount of uncertainty in linguistic variables of the knowledge base
[33]. This system is built upon a multiplayer neural network to perform parameter
adjustment of the fuzzy observer. The idea has then developed into an adaptive
learning scheme, dubbed as “personalized” facial expression recognition, where an
addition of a new classiﬁer, a modiﬁcation of an existing classiﬁer, and a feature
selection process are streamlined and guided in an integrated fashion [15, 34].
4
Design of Integrated Learning Systems
4.1 Application to Smart Homes for Aiding the Disables
The “human-in-the-loop” system, in which both service robots and a human are
considered as a part of its control loop, essentially addresses a need for seamless
operation in our living environment, such as a smart home. It is particularly useful
for people with movement disabilities because the system is required to engage in
daily activities of the users with minimal interruption.
Our ﬁrst effort has been made to design an intelligent robotic agent as a means to
offer various kinds of proactive assistance [35]. The crux of the design was to
balance usability with complexity of functions of a system; a user would be
overwhelmed by the system if it had a complex user interface, regardless of how
versatile the system is. We argued that the remedy to this problem lies in the
psychological implications of how much the users feel comfortable to access a
various functions of the system, called “human-friendly service” [36, 37]. A variety
of human-robot interfaces have been implemented accordingly, including eye-
mouse, head and shoulder user interfaces, and EMG signal interfaces, meeting the
needs of different levels of disability.
From solicited feedbacks on these system from potential end-users with spinal
cord injury, we learned that, in the presence of multiple robotic agents, an inter-
mediate decision maker is required to facilitate effective communication between a
user and individual modules and also to increase accessibility for novice users [37].
This stimulated designing a new type of a service robot, called “Steward robot”
44
S.W. Lee and Z. Zenn Bien

[38]. The steward robot system has two novel features. First, it learns from user’s
behavioral patterns on a daily basis, providing personalized services. Second, the
user interface is equipped with an emotional interaction module, which is intended
to offer a human-friendly environment, as well as to enable the system to collect
natural behavioral data. Taken together, we have demonstrated that a proper
combination of fuzzy learning techniques is essential for efﬁcient human-machine
interaction in a smart home environment.
4.2 Integrating Multiple Learning Systems
We have undergone a transition from the design of each individual learning agent to
the integration of these learning systems. Our earlier studies have focused on a low-
level communication architecture of the smart home for the disabled, where a single
control unit controls communication among multiple devices and robotic agents
[39]. In a subsequent study, we have proposed higher level functional architecture
to exert control over multiple robotic modules [40]. The proposed integrated system
spanned all levels of control, from user interfaces to action units. The ﬁrst layer,
functioning as input devices, provides a user with human-machine interactions in
manifold forms, such as a soft remote controller operated by hand gestures, a voice
recognition system, and other types of sensory devices (a joystick or a touch
screen). The second layer, functioning as monitoring devices, consists of a pressure
sensor-based bed that detects body movement and postures and a health monitoring
system that collect bio-signals. This layer also deals with environment parameters
(illumination, humidity, and temperature). The next layer, a central control unit,
receives inputs from the ﬁrst two layers to execute a command for a variety of
action units. The action units is the last layer of this architecture, which include a
bed-mounted rehabilitation robot, mobile robots, a wheelchair-mounted robot, an
intelligent bed, a robotic hoist, home appliances.
A considerable challenge arises when designing the last action unit layer is how
to organize low-level commands (e.g., “move the robotic hoist”, “turn off the light”,
or “position the wheelchair in front of the hoist”) that is necessary for achieving an
abstract-level goal set by a user (e.g., “I want to go out” or “I want to go to bed”).
Motivated by the way humans draft a plan, the task knowledge organization system
has been proposed by combining a top-down scenario analysis and a bottom-up
commands development [41, 42]. The top-down process develops speciﬁc task
structure by conﬁguring task knowledge from the user’s point of view, and then in
the subsequent bottom-up process, the system simulates the user’s scenario to
assess validity of the developed tasks before actually executing a complete task
sequence. This idea has been demonstrated in the KAIST’s intelligent sweet home
(ISH; Fig. 2) scenarios [42].
Learning Systems with FUZZY
45

4.3 Multiple Learning Systems: Algorithms Versus
Human Brains
The integrated learning system in the last resort needs to be built upon the
understanding of how humans learn and choose different strategies to reinforce
behavior in a coherent manner. This leads to an emergence of an applicability of
neural theory to the design of learning system. A series of seminal studies in
neuroscience, in which dopamine neurons in behaving non-human primates and
those target areas in humans implement a prediction error from a temporal differ-
ence reinforcement learning algorithm [43, 44], encourage us to utilize those types
of learning model for system design. Subsequent studies combining the learning
algorithms and neural data also found that multiple learning systems were imple-
mented in human brains as opposed to just a single system [45]. This remarkable
resemblance between the learning algorithm and the human brains, combined with
our computational proposal of control of multiple learning algorithms to guide
integrated behaviors [46], merits a test to understand how human brains exert
control over these multiple learning systems. Our study recently demonstrated that
Fig. 2 KAIST’s intelligent sweet home scenario. Multiple agents interact to provide a user with
disabled lower-limbs a variety of proactive services, which consists of multiple low-level
commands carried out by a transferring robot (shown in the upper left), a steward robot (shown in
the upper right), a hand gesture-based soft remote control system (shown in the lower left), and an
intelligent bed with a robotic arm (shown in the lower right)
46
S.W. Lee and Z. Zenn Bien

such control mechanism is indeed implanted in the brain, speciﬁcally the allocation
of control is based on the relative degree of “uncertainty” in the estimates from the
two learning systems [47]. We thus envision that fuzzy theory might be useful again
as we begin to understand that human brains use uncertainty information to
implement a control for multiple learning strategies.
5
Outlook
The last three decades is the crucial period in the evolution of fuzzy theory. The
effectiveness in dealing with uncertainty in system variables indeed enables itself to
make a signiﬁcant contribution to designing learning systems. Our developments
using fuzzy techniques span a wide range of learning systems, demonstrating
effectiveness in handling human users as a component in the control loop. The
techniques also lend themselves well to formulating a design principle of an inte-
grated system for the smart home for the disabled, in which multiple learning agents
effectively interact to provide aids for the user.
On the other hand, we begin to understand how our brain learns from experi-
ences, and more importantly, when and how it exerts control over multiple types of
learning strategies based on uncertainty information. A few recent studies, directly
pitting different types of computational learning models against each other to
understand how the human brain arbitrate multiple learning systems, have provided
us with an insight into how we design an integrated learning systems for real-world
applications.
Taken all together, we envisage that the principles of fuzzy theory, when
combined with what we know about computational learning mechanisms in the
human brain, will not only advance user’s experience in real-world scenarios but
also offer a practical guidance on how we design learning systems.
References
1. Chun, M.G., Bien, Z.: Neurocomputational approach to solve a convexly combined fuzzy
relational equation with generalized connectives. Fuzzy Sets Syst. 57, 321–333 (1993)
2. Bien, Z., Chun, M.-G.: An inference network for bidirectional approximate reasoning based on
an equality measure. IEEE Trans. Fuzzy Syst. 2, 177–180 (1994)
3. Huaguang, Z., Cai, L., Bien, Z.: A fuzzy basis function vector-based multivariable adaptive
controller for nonlinear systems. IEEE Trans. Syst. Man. Cybern. B. Cybern. 30, 210–217
(2000)
4. Yu, W., Bien, Z.: Design of Fuzzy Logic Controller with Inconsistent Rule Base. J. Intell.
Fuzzy Syst. 2, (1994)
5. Bien, Z., Kang, D., Yang, S.-H.: Programming approach for fuzzy model-based multiobjective
control systems. Int. J. Uncertain. Fuzziness Knowl.-Based Syst. 7, 293–300 (2011)
Learning Systems with FUZZY
47

6. Myung, H.-C., Bien, Z.Z.: Stability of TSK-type time-delay FLC. Intell. Autom. Soft Comput.
8, 289–302 (2002)
7. Kim, Y.-T., Bien, Z.: Robust self-learning fuzzy controller design for a class of nonlinear
MIMO systems. Fuzzy Sets Syst. 111, 117–135 (2000)
8. Bang, W.-C., Bien, Z.: Incremental inductive learning algorithm in the framework of rough set
theory and its application. Int. J. Fuzzy Syst. 1, 25–36 (1999)
9. Sutton, R.S., Barto, A.G.: Reinforcement Learning. MIT Press (1998)
10. Kang, D.-O., Bien, Z.: Design of multi-objective satisfactory fuzzy logic controller using
reinforcement learning. Int. J. Fuzzy Syst. 2, 139–152 (2000)
11. Myung, H.-C., Bien, Z.Z.: Design of the fuzzy multiobjective controller based on the
eligibility method. Int. J. Intell. Syst. 18, 509–528 (2003)
12. Kwak, B.-D., Park, K.-H., Bien, Z.: Multi-resolution fuzzy min-max neural network with on-
line learning ability using mixture of experts. In: Proceedings of the ICAIET (2004)
13. Kim, J.S., Jang, W., Bien, Z.: A dynamic gesture recognition system for the Korean sign
language (KSL). IEEE Trans. Syst. Man. Cybern. B. Cybern. 26, 354–359 (1996)
14. Lee, S.W., Kim, D.-J., Kim, Y.S., Bien, Z.: Adaptive gabor wavelet neural network for facial
expression recognition - training of feature extractor by novel feature separability criterion. In:
Proceedings of 11th World Congress of International Fuzzy Systems Association (IFSA
2005), Beijing, China (2005)
15. Kim, D.-J., Bien, Z.: A Novel Feature Selection for Fuzzy Neural Networks for Personalized
Facial
Expression
Recognition.
http://search.ieice.org/bin/summary.php?id=e87-a_6_
1386&category=A&year=2004&lang=E&abst= (2004)
16. Lee, K., Bien, Z.: A gray-level corner detector using fuzzy logic. Pattern Recognit. Lett. 17,
939–950 (1996)
17. Wong, F., Park, K.-H., Bien, Z.: Fuzzy rule-based skin color modeling and classiﬁcation. Int.
J. Assist. Robot. Syst. 10, (2009)
18. Han, J.-S., Lee, S.W., Bien, Z.: Feature subset selection using separability index matrix. Inf.
Sci. (Ny) 223, 102–118 (2013)
19. Kim, J., Bien, Z.: Recognition of continuous Korean sign language using gesture tension
model and soft computing technique. IEICE Trans. Inf. Syst. E87-D (2004)
20. Jang, H., Do, J.-H., Jung, J., Park, K.-H., Bien, Z.Z.: View-invariant hand-posture recognition
method for soft-remocon-system. In: 2004 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS) (IEEE Cat. No. 04CH37566), pp. 295–300. IEEE (2004)
21. Park, K.-H., Jeong, S.-H., Pelczar, C., Bien, Z.Z.: Beat gesture recognition and ﬁnger motion
control of a piano playing robot for affective interaction of the elderly. Intell. Serv. Robot. 1,
185–193 (2008)
22. Jeon, M.-J., Lee, S.W., Bien, Z.: Hand gesture recognition using multivariate fuzzy decision
tree and user adaptation. Int. J. Fuzzy Syst. Appl. 1, 15–31 (2011)
23. Yang, S.-E., Park, K.-H., Bien, Z.: Gesture spotting using fuzzy garbage model and user
adaptation. Int. J. Fuzzy Syst. Appl. 1, 47–65 (2011)
24. Kim, S., Jeon, M., Lee, S.W., Park, K.-H., Bien, Z.Z.: Development of assistive software for
disabled and aged people based on user characteristics. In: Proceedings of the 8th International
Symposium Advances in Intelligent Systems (ISIS 2007), pp. 466–470 (2007)
25. Jeong, S.-H., Park, K.-H., Bien, Z.: Multimedia storytelling system and method using Baby
Sign recognition (2006)
26. Jung, J.-W., Bien, Z., Sato, T.: Person recognition method using sequental walking footprints
via overlapped foot shape and center-of pressure trajectory. IEICE Trans. Fundam. Electron.
Commun. Comput. Sci. E87-A (2004)
27. Jung, J.-W., Sato, T., Bien, Z.: Dynamic footprint-based person recognition method using a
hidden markov model and a neural network. Int. J. Intell. Syst. 19, 1127–1141 (2004)
48
S.W. Lee and Z. Zenn Bien
www.allitebooks.com

28. Jung, J.W., Kim, D.J., Bien, Z.: Realization of personalized services for intelligent residential
space based on user identiﬁcation method using sequential walking footprints. J. Syst. Cybern.
Inf. 3, 90–95 (2006)
29. Song, J.-H., Jung, J.-W., Lee, S.-W., Bien, Z.: Robust EMG pattern recognition to muscular
fatigue effect for powered wheelchair control. J. Intell. Fuzzy Syst. Appl. Eng. Technol. 20,
3–12 (2009)
30. Kim, D.-J., Song, W.-K., Han, J.-S., Bien, Z.Z.: Soft computing based intention reading
techniques as a means of human-robot interaction for human centered system. Soft Comput. -
A Fusion Found. Methodol. Appl. 7, 160–166 (2003)
31. Bien, Z.Z., Park, K.-H., Jung, J.-W., Do, J.-H.: Intention reading is essential in human-friendly
interfaces for the elderly and the handicapped. IEEE Trans. Ind. Electron. 52, 1500–1505
(2005)
32. Ekman, P.F.W.: Unmasking the face: a guide to recognizing emotions from facial expressions.
Wiley, Prentice Hall, New York (1978)
33. Park, G., Bien, Z.: Neural network-based fuzzy observer with application to facial analysis.
Pattern Recognit. Lett. 21, 93–105 (2000)
34. Kim, D.J., Bien, Z.: Design of “Personalized” classiﬁer using soft computing techniques for
“Personalized” facial expression recognition. IEEE Trans. Fuzzy Syst. 16, 874–885 (2008)
35. Song, W.-K., Lee, H., Bien, Z.: KARES: intelligent wheelchair-mounted robotic arm system
using vision and force sensor. Rob. Auton. Syst. 28, 83–94 (1999)
36. Bien, Z.Z., Lee, H.-E., Do, J.-H., Kim, Y.-H., Park, K.-H., Yang, S.-E.: Intelligent interaction
for human-friendly service robot in smart house environment. Int. J. Comput. Intell. Syst. 1,
77–93 (2008)
37. Bien, Z., Chung, M.-J., Chang, P.-H., Kwon, D.-S., Kim, D.-J., Han, J.-S., Kim, J.-H., Kim,
D.-H., Park, H.-S., Kang, S.-H., Lee, K., Lim, S.-C.: Integration of a rehabilitation robotic
system (KARES II) with human-friendly man-machine interaction units. Auton. Robots 16,
165–191 (2004)
38. Park, K.-H., Lee, H.-E., Kim, Y., Bien, Z.Z.: A Steward Robot for human-friendly human-
machine interaction in a smart house environment. IEEE Trans. Autom. Sci. Eng. 5, 21–25
(2008)
39. Stefanov, D.H., Bien, Z., Bang, W.-C.: The smart house for older persons and persons with
physical disabilities: structure, technology arrangements, and perspectives. IEEE Trans.
Neural Syst. Rehabil. Eng. 12, 228–250 (2004)
40. Park, K.-H., Bien, Z., Lee, J.-J., Kim, B.K., Lim, J.-T., Kim, J.-O., Lee, H., Stefanov, D.H.,
Kim, D.-J., Jung, J.-W., Do, J.-H., Seo, K.-H., Kim, C.H., Song, W.-G., Lee, W.-J.: Robotic
smart house to assist people with movement disabilities. Auton. Robots 22, 183–198 (2006)
41. Prenzel, O., Lee, S.W., Bien, Z., Graeser, A.: A Study on the Application of the Software
Framework MASSIVE in KAIST’s Intelligent Sweet Home System. Int. J. Assist. Robot.
Mechatron. 9, 74–84 (2008)
42. Lee, S.W., Prenzel, O., Bien, Z.: Applying human learning principles to user-centered IoT
systems. IEEE Comput. 46, 46–52 (2013)
43. McClure, S.M., Berns, G.S., Montague, P.R.: Temporal prediction errors in a passive learning
task activate human striatum. Neuron 38, 339–346 (2003)
44. Schultz, W., Dayan, P., Montague, P.R.: A neural substrate of prediction and reward. Science
80(275), 1593–1599 (1997)
45. Gläscher, J., Daw, N.D., Dayan, P., O’Doherty, J.P.: States versus rewards: dissociable neural
prediction error signals underlying model-based and model-free reinforcement learning.
Neuron 66, 585–595 (2010)
46. Lee, S.W., Kim, Y.S., Bien, Z.: A nonsupervised learning framework of human behavior
patterns based on sequential actions. IEEE Trans. Knowl. Data Eng. 22, 479–492 (2010)
47. Lee, S.W., Shimojo, S., O’Doherty, J.P.: Neural computations underlying arbitration between
model-based and model-free learning. Neuron 81, 1–13 (2014)
Learning Systems with FUZZY
49

Authors Biography
Sang Wan Lee received his PhD in electrical engineering and
computer science from the Korea Advanced Institute of Science
and Technology (KAIST), Republic of Korea, in 2009. He is
currently a Della Martin postdoctoral scholar of computational
and neural systems and behavioral and social neuroscience at the
California Institute of Technology, United States of America.
His research interests include machine learning and computa-
tional neuroscience. Contact him at swlee@caltech.edu.
Z. Zenn Bien is Professor Emeritus in the Department of
Electrical Engineering at the Korea Advanced Institute of
Science and Technology and also in the School of Electrical
and Computer Engineering at Ulsan National Institute of Science
and Technology. He was 1st president of the Korean fuzzy
society founded in 1991 and also served as president of IFSA for
2003 ∼5 term. His research interests include intelligent control
methods, assistive robotics and smart-home systems. Dr. Bien
received his PhD in electrical engineering from the University of
Iowa. He is an IEEE and IFSA Fellow. Contact him at
zbien@kaist.ac.kr.
50
S.W. Lee and Z. Zenn Bien

Fuzzy Modiﬁers at the Core of Interpretable
Fuzzy Systems
Bernadette Bouchon-Meunier and Christophe Marsala
Abstract Fuzzy modiﬁers associated with linguistic hedges have been introduced
by L.A. Zadeh at the early stage of approximate reasoning and they are fundamen-
tal elements in the management of interpretable systems. They can be regarded as
a solution to the construction of fuzzy sets slightly diﬀerent from original ones. We
ﬁrst present the main deﬁnitions of modiﬁers based on mathematical transforma-
tions of membership functions, mainly focusing on so-called post-modiﬁers and pre-
modiﬁers, as well as deﬁnitions based on fuzzy relations. We show that measures of
similarity are useful to evaluate the proximity between the original fuzzy sets and
their modiﬁed form and we point out links between modiﬁers and similarities. We
then propose an overview of application domains which can take advantage of fuzzy
modiﬁers, for instance analogy-based reasoning, rule-based systems, gradual sys-
tems, databases, machine learning, image processing, and description logic. It can
be observed that fuzzy modiﬁers are either constructed in a prior way by means of
formal deﬁnitions or automatically learnt or tuned, for instance in hybrid systems
involving genetic algorithm-based methods.
Keywords
Fuzzy modiﬁers ⋅Linguistics hedges ⋅Similarity
1 Introduction
Human beings are very eﬃcient in coping with real world complexity and human-
like automated systems have been constructed for decades now, with the purpose
of managing large size data, subjective and imperfect information and ill-known
B. Bouchon-Meunier (✉) ⋅C. Marsala
Sorbonne Universités, UPMC Univ Paris 06, UMR 7606, LIP6, 75005 Paris, France
e-mail: Bernadette.Bouchon-Meunier@lip6.fr
B. Bouchon-Meunier ⋅C. Marsala
CNRS, UMR 7606, LIP6, 75005 Paris, France
C. Marsala
e-mail: Christophe.Marsala@lip6.fr
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_4
51

52
B. Bouchon-Meunier and C. Marsala
environments. Concepts managed by human beings are often imprecise, with a core
of easy to classify instances and a shadow of other instances [1].
Fuzzy modeling is well-suited for their representation and their use in automated
systems. It is the reason why fuzzy systems have been a key solution to the man-
agement of complex systems since the introduction of linguistic variables and fuzzy
if-then rules by LotﬁA Zadeh [2–5]. They are based on approximate descriptions of
fuzzy variables by means of fuzzy modalities and relations between such descrip-
tions. Qualities of fuzzy systems such as their expressiveness and their capacity to
manage gradual knowledge have taken a large part in their success in real-world
applications. Similarity, or its brother concepts resemblance, closeness, proximity,
analogy, has been pointed out as fundamental in a number of domains, such as lin-
guistics, semiology, psychology. It is particularly useful in computational intelli-
gence, and especially in fuzzy modeling in which it takes part in the modeling of
imprecision and classes with unsharp boundaries.
We focus in this paper on a particular representation of similarity between con-
cepts by means of the utilization of fuzzy modiﬁers. In his seminal paper [6], L.A.
Zadeh introduced the concept of modiﬁer to represent linguistic hedges such as very,
more or less, slightly, by means of a mathematical transformation of membership
functions based on power functions. Psychometrical analyses and empirical studies
were then proposed by [7–9]. The concept of modiﬁer was extensively studied from a
psychometrical or an empirical point of view [10–13] and gave rise to various works
on mathematical, algebraic, or logical approaches [14, 15] as well as proposals to
use fuzzy modiﬁers in soft computing.
In the ﬁrst section of this paper, we ﬁrst summarize the main formal deﬁnitions of
modiﬁers, based on mathematical transformations or fuzzy relations. Then we con-
sider modiﬁers from the point of view of measures of similarity. The main purpose
of fuzzy modiﬁers being to take a part in the interpretability of fuzzy systems, we
devote the second part to application domains which have made good use of modi-
ﬁers. We conclude on the importance to preserve the link with fuzzy modiﬁers and
linguistic hedges.
2 Fuzzy Modiﬁers
Let U be an ordered universe of discourse and F(U) the set of fuzzy sets of U. For
instance, U could be the set of real numbers ℝ, or a subset of that set. By convention,
we use the same symbol A for a fuzzy set and its membership function. Given the
various forms of linguistic hedges (very, more or less, strongly, at least, extremely,
etc.), we summarize several approaches to their formal deﬁnition.
For membership degrees A(x) associated with elements x of U, the general idea
is to construct a new membership function, denoted by m ◦A deduced from “close”
elements of x in U or to consider a proximity between A(x) and m ◦A(x) for every
x. In a more complex approach, it is also possible to consider a proximity between
A(x) and m ◦A(y), for elements y “close” to x.

Fuzzy Modiﬁers at the Core of Interpretable Fuzzy Systems
53
2.1 Fuzzy Modiﬁers Deﬁned by Mathematical
Transformations
To represent these two proximities, one on U and the other one on [0, 1], we consider
the following generic deﬁnition of fuzzy modiﬁers [16].
A fuzzy modiﬁer can be regarded as a pair m = (g, h), where g ∶[0, 1] ⟶[0, 1]
and h ∶U ⟶U are functions.
If A is a fuzzy set of U, then m ◦A is also a fuzzy set of U deﬁned for every x in U
by: m ◦A(x) = g◦A ◦h(x). This deﬁnition is very general and corresponds to various
transformations of a given fuzzy set, not necessarily related to proximities.
Complementation is one of them, even though the transformation is extreme,
associated with the identity function h and the function deﬁned by g(x) = 1 −x,
representing the linguistic hedge not.
Normalization is another one, not associated with a linguistic hedge, but to a
technical transformation deﬁned by the function h(x) = kx, k being the largest value
of x in U where A attains its maximum and g(x) =
x
A(k) in [17]. We can also imagine
the simple normalization based on the identity function h and the function deﬁned
by g(x) = x
a, with a = max
x
A(x).
Sharpeners or contrast intensiﬁcation operators [18] are also fuzzy modiﬁers, such
that h is again the identity function and g is a function such that g(x) ≥x if x ≥1
2
and g(x) < x if x < 1
2. The most drastic sharpener corresponds to g(x) = 1 if x ≥1
2
and g(x) = 0 if x < 1
2.
We present in the sequel the most important classes of fuzzy modiﬁers used in
formal or applied research, with a focus on their interpretability, associated with
linguistic hedges.
The following contrast enhancement operator is indicated by [19] for information
fusion in signal and image processing:
g(x) = 2x2 if x < 1
2
and
g(x) = 1 −2(1 −x)2 otherwise.
2.2 Post-modiﬁers
If h is the identity function on U, then m is called a post-modiﬁer [20]. Typical post-
modiﬁers are reinforcing modiﬁers and weakening modiﬁers [21, 22] which extend
the seminal forms of modiﬁers introduced by Zadeh [6], deﬁned by g(x) = x𝛼, with
respectively 𝛼≥1 in the case of reinforcing modiﬁers, and 𝛼≤1 in the case of
weakening modiﬁers.

54
B. Bouchon-Meunier and C. Marsala
More generally, reinforcing modiﬁers are such that m ◦A(x) ≤A(x) for every x
in U and correspond to linguistic modiﬁers such as very, really, or strongly with the
idea of a more restrictive view of the underlying concept. The fuzzy set m ◦A is more
speciﬁc and/or more precise than A. The category described by m◦A is included in
the category described by A.
Weakening modiﬁers are such that m◦A(x) ≥A(x) for every x in U and correspond
to linguistic modiﬁers such as approximately, rather or about [21] which yield m ◦A
less speciﬁc and/or less precise than A.
Examples of functions g are homotheties applied to membership functions, either
preserving the support of A and decreasing its speciﬁcity by extending its kernel
(approximately), or preserving the kernel of A and extending its support to decrease
its precision (rather). A softer form of modiﬁer extends both kernel and support to
decrease the speciﬁcity and the precision of A (about). In all these cases, the category
represented by m ◦A is wider than the category represented by A, with less sharp
boundaries.
2.3 Pre-modiﬁers
It should be remarked that such fuzzy modiﬁers do not cover all forms of linguistic
hedges. In some cases, a reinforcement or a weakening of the description represented
by A corresponds to a decrease or an increase of the values of U in its kernel or its
support. For instance, if U is a universe of length, it is very common to consider that
the fuzzy set representing very small has a kernel or a support “before” the kernel or
the support of small, with respect to the order on U. Symmetrically, very long will
be represented by a fuzzy set with a kernel or a support “after” the one of long. This
example shows the complexity of linguistic modiﬁers and the necessity to introduce
another form of fuzzy modiﬁers, as follows.
If g is the identity function on [0, 1], then m is called a pre-modiﬁer [20]. Typical
forms of pre-modiﬁers are translatory modiﬁers [23] associated with functions h
deﬁning translations on U to the right or to the left, h(x) = x + t for every x in U, for
a positive or negative parameter t, to answer the above remark on the direction of the
necessary modiﬁcation according to the meaning of the description represented by A.
Such modiﬁers are neither reinforcing nor weakening. The fuzzy set m ◦A represents
a category which is shifted to the upper zones of U or to the lower ones with respect
to the original category represented by A. Such an approach can be useful in case of
evolving categories, progressively moving on U over time.
2.4 Fuzzy Relation-Based Modiﬁers
The original concept of modiﬁer was introduced to handle similar categories we
can distinguish by means of subtle diﬀerences, for instance expressed by linguistic

Fuzzy Modiﬁers at the Core of Interpretable Fuzzy Systems
55
hedges. It is therefore natural to use measures of similarity [24] or fuzzy relations
[25] to introduce and study fuzzy modiﬁers.
Let us consider a fuzzy relation R on U. [25] introduces families of relation-based
modiﬁers. Let us focus on the family based on a conjonction operator ⊤, such that:
m ◦A(x) = sup
y∈U
⊤(A(y), R(y, x)), for every x ∈U.
(1)
The particular case where ⊤is the minimum yields a membership degree of every
x to m ◦A deﬁned as the maximum value of membership degrees assigned to ele-
ments of U in relation R with x.
Some of these relation-based modiﬁers are expansive or restrictive. In this case,
we can imagine R as a similarity relation, m ◦A being then the maximum member-
ship degree of all elements of U similar to x.
In the case where ⊤is a t-norm and E a ⊤−equivalence (reﬂexive, transitive
and ⊤-transitive), we can consider a fuzzy ordering R such that R(x, y) ≥E(x, y) and
⊤(R(x, y), R(y, x)) ≤E(x, y) for every x and y in U [26]. Then Equation (1) yields
a fuzzy modiﬁer expressed as at least A. If we take the inverse ordering deﬁned by
R(−1)(x, y) = R(y, x), then we obtain a representation of the linguistic hedge at most.
3 Similarites and Modiﬁers
Another manner to take resemblances into account [24] consists in evaluating the
“closeness” of A and m ◦A in order to measure their similarity. We consider a fuzzy
set measure M ∶F(U) ⟶ℝ+ such that M(∅) = 0 and M is monotonous with
respect to the classic inclusion of fuzzy sets ⊆. We also consider a diﬀerence ⊖
between fuzzy sets, such that A ⊖B is monotonous with respect to A and A ⊆B
implies A ⊖B = ∅.
Such a measure can be used to evaluate the similarity between A and m(A) for
a modiﬁer m. We restrict ourselves to so-called (M, 𝜀, 𝜆)-modiﬁers [27] such that
M(m ◦A ⊖A) = 1 −𝜀and M(A ⊖m ◦A) = 1 −𝜆, for two parameters 𝜀and 𝜆in
[0, 1].
Particular cases of such (M, 𝜀, 𝜆)-modiﬁers are (M, 𝜀, 1)-modiﬁers which are
expansive, (M, 1, 𝜆)-modiﬁers which are restrictive, and translatory modiﬁers being
particular cases of (M, 𝜀, 𝜀)-modiﬁers. (M, 1, 1)-modiﬁers can be regarded as pro-
viding the closest modiﬁed forms of the primitive fuzzy set.
An example is the linguistic hedge approximately represented by
m1 ◦A(x) = min(1, 𝜀⋅A(x)),
for every x in U, for 𝜀∈[0, 1], corresponds to a modiﬁer preserving the support of
A and extending its kernel to decrease its speciﬁcity.

56
B. Bouchon-Meunier and C. Marsala
It is easy to see [27] that it can be considered as a (M1, 1, 1)−modiﬁer if we deﬁne
M1(A) = ∫x∈U
A(x)dx and the diﬀerence between fuzzy sets as A ⊖2 B = A(x) if
B(x) = 0 and A ⊖2 B = 0 if B(x) > 0, It can also be regarded as a (M2, 1
𝜀, 1)
when we deﬁne M2(A) = supx∈U A(x) with the diﬀerence A ⊖1 B = max(0, A(x) −
B(x)). This points out the role of the perception of “similarity”, strongly dependent
on parameters 𝜀and 𝜆, as well as chosen operators M and ⊖. Another simple form
of modiﬁer is the representation of uncertainty as follows:
m2 ◦A(x) = max(A(x), 𝜀),
for every x in U, which can be expressed as A with an uncertainty 𝜀. Such modi-
ﬁers are (M2, 1 −𝜀, 1)-modiﬁers with the diﬀerence ⊖1, which shows that they are
not far from the original description A.
Going further in the use of similarities between fuzzy descriptions and their
modiﬁed forms, we consider a measure of similarity S on U deﬁned as a function
S ∶F(U) × F(U) ⟶[0, 1], such that S(A, B) = F(M(A ∩B), M(A ⊖B), M(B ⊖A))
is non-decreasing with respect to M(A ∩B) and non-increasing with respect to
M(A ⊖B) and to M(B ⊖A).
Particular measures are deﬁned, according to their speciﬁc properties [28]. A
measure of satisﬁability is exclusive, which means that S(A, B) = 0 when A ∩B = ∅,
and independent of M(A ⊖B). A measure of inclusion is also exclusive. In addition,
it is independent of M(B ⊖A). They correspond to the idea that A is a reference to
which B is compared, the measure of inclusion being only interested in the extent
to which B can be considered as a particular case of A. A measure of resemblance
is symmetric in M(A ⊖B) and M(B ⊖A), which means that there is no reference
and both A and B have the same status in the research of similarity. It is easy to see
that, if the modiﬁer m is expansive, then A and m ◦A will be compared through a
measure of satisﬁability. If m is restrictive, A and m ◦A will be compared through a
measure of inclusion. If m is translatory, A and m ◦A will be compared by means of
a measure of resemblance [24].
4 Application Domains Using Modiﬁers
4.1 Analogy-Based Reasoning
Evaluations of the proximity or similarity between A and m(A) mentioned previously
can for instance be used in an analogy-based reasoning, or case-based reasoning, to
construct interpretable conclusions from observations [24].
Starting from a rule such as If X is A, then Y is B, for i = 1, … , n, or cases such
that X is A at the same time as Y is B, an observation A′ will be compared to A to
evaluated their similarity S(A, A′). One way to determine the description B′ of Y is

Fuzzy Modiﬁers at the Core of Interpretable Fuzzy Systems
57
to assume that S(A, A′) = S(B, B′) and to make a choice among all fuzzy sets B′
satisfying this constraint.
Using modiﬁers limits the number of solutions and provides an easily inter-
pretable solution. According to the relations between measures of similarity and
modiﬁers, we use an expansive modiﬁer m to describe B′ = m ◦B if we choose
a measure of satisﬁability S.
As an example, let us consider the following measure of satisﬁability: S(A, A′) =
1 −M2(A′ ⊖A). If the value of S(A, A′) is 𝜎, then we can use a modiﬁer such as
m1 for the parameter 1
𝜎interpreted as approximately, or m2 for the parameter 1 −𝜎,
interpreted as A with an uncertainty 1 −𝜎.
If we use a measure of inclusion S, a restrictive modiﬁer is convenient to express
the diﬀerence between the two fuzzy sets. For instance, let us consider the following
measure of inclusion: S(A, A′) = 1 −M2(A ⊖A′). If 𝜎is the obtained value, we can
think of a modiﬁer m such as m ◦B(x) = min(𝜎, B(x)).
If we use a measure of resemblance, we associate it with a translatory modiﬁer
that we will not describe in detail.
4.2 Rule-Base Systems
Rule-based systems and the particular case of fuzzy control are the ﬁrst domains
where modiﬁers help to obtain interpretable results. The interpretability of rule-
based systems is complex and has given rise to various analyses. We can mainly
point out three important factors of this interpretability: the easily understandable
linguistic description of variables, the number of rules and the number of premises
in each rule.
One of the ﬁrst attempts to manage linguistic labels in a fuzzy knowledge-based
system was the linguistic approximation used in the MILORD system to deal with
both uncertainty and imprecision [29].
With regard to easily understandable descriptions of variables, reasoning with
modiﬁers provides interpretable conclusions when using generalized modus ponens
[16, 22] with rules of the form : If X is Ai then Y is Bi, for i = 1, … , n.
In particular, if we use restrictive modiﬁers, we obviously obtain a conclusion
identical with the conclusion of the rule with all classic fuzzy implications [21].
When we use expansive modiﬁers such as approximately deﬁned by m1 ◦Ai to
describe observations, for instance, such rules provide conclusions of the form
m1 ◦Bi itself, or m2 ◦Bi (representing an uncertainty on Bi), or m3 ◦Bi for some
other expansive modiﬁer m3. These forms of conclusions are easily interpretable,
which is not the case when using general modus ponens with any observation. We
can conclude that, if an observation is similar to a premise Ai through a modiﬁer, the
obtained conclusion is also similar to the conclusion of the rule through a modiﬁer
when using the most classic fuzzy implications.

58
B. Bouchon-Meunier and C. Marsala
Fuzzy modiﬁers are also useful to adjust the shape of membership functions
dynamically in the design of a fuzzy controller. In [30], the number of premises
is limited to three to simplify the design of fuzzy controllers. A so-called linguistic
hedge module is added to the fuzzy controller to play the role of powered modiﬁers
and dynamically modify the shape of membership functions to the feedback signal, to
provide additional information without increasing the number of rules. Genetic algo-
rithms are used to tune membership functions by means of modiﬁers in the design
of the rule base.
In a diﬀerent approach, [31] proves that modiﬁers are useful to ﬁnd a trade-oﬀ
between interpretability and accuracy in the construction of rule-based systems. The
authors propose to extend the initial list of linguistic descriptions and to create new
rules of the form: If X1 is mi1 ◦Ai1, and X2 is mi2 ◦Ai2 and … then Y is mi ◦Bi, for
i = 1, … , n. They consider powered, expansive or restrictive modiﬁers, as well as
translatory ones.
In [32], the authors propose fuzzy modiﬁers to be learnt to obtain the best fuzzy
rule-based classiﬁcation system. After a prior rule base is constructed on the basis
of pre-deﬁned linguistic descriptions represented by fuzzy sets, a genetic algorithm-
based method is used to select the best subset of rules and to learn the set of linguistic
modiﬁers to apply to the linguistic variables for the considered fuzzy logic system.
Gonzalez et al. [33] consider also linguistic hedges included in a genetic algo-
rithm in the framework of the inductive learning algorithm called Structural Learn-
ing Algorithm on Vague Environment. They help the user to learn and tune fuzzy
rules.
In [26], it is proposed to introduce modiﬁers to reduce the size of a fuzzy rule
base and thus to enhance its interpretability and expressiveness. The proposal is,
ﬁrst of all, to group rules leading to the same output decision, and to rank them
according to the input parameters they involve. Then, such “neighboring” rules can
be merged and replaced by a rule which summarizes their premises into a single one
constructed by means of a modiﬁer. For instance, in a PD-style fuzzy controller set
of rules, “Negative Big”, “Negative Small” and “Zero” are grouped and these three
fuzzy values are replaced by “at most Zero”. Moreover, the authors highlight the fact
that modiﬁers could be very useful in interpolative reasoning when the fuzzy rule
base has been constructed incomplete. The proposed work in this paper is based on
the use of a fuzzy ordering (see Sect. 2.4).
4.3 Gradual Systems
Their ability to manage graduality is an important property of fuzzy set-based rep-
resentations. The most basic view of graduality corresponds to the unsharp bound-
aries of a category represented by A, in which we enter progressively with slightly
increasing membership degrees and from which we get out with slightly decreasing
membership degrees when we progress along U. This graduality is handled by the
concept of fuzzy set itself.
www.allitebooks.com

Fuzzy Modiﬁers at the Core of Interpretable Fuzzy Systems
59
A second type of graduality corresponds to a change in the boundaries of a cate-
gory, reducing or extending it according to the context or the observations, and this
graduality is clearly handled by means of weakening or reinforcing post-modiﬁers.
A third type of graduality corresponds to a progression along U, in the case where
we have a family of fuzzy sets A1, ..., An, describing a variable deﬁned on U, for
instance classes of a fuzzy partition of U. This graduality corresponds to values of the
variable smoothly evolving from a category Ai to the next one Ai+1. This graduality
is clearly related to the use of adaptive pre-modiﬁers.
A deductive management of graduality [22] enables to take into account various
forms of fuzzy rules. One of the most natural forms of gradual knowledge for human
experts is the following: The more (less) X is A, the more (less) Y is B, for which
solutions are not obvious in automated systems.
Generalized modus ponens and the use of modiﬁers to represent more or less
leads to an automated deductive system translating such rules directly or with con-
sideration of uncertainties, according to the chosen fuzzy implication, in rules of the
form: It is rather certain that the less X is A, the less X is B, or the more certain X
is A, the more certain Y is B. This graduality can be considered as restricted to one
rule and then local.
A global graduality can be managed through a collection of rules such as: The
more (less) X is Ai, the more (less) Y is Bi, for i between 1 and n, the conclusion
moving progressively from Bi to Bi+1 or Bi−1 when the observation varies from Ai
to Ai+1 or Ai−1.
4.4 Other Uses of Modiﬁers
More generally, modiﬁers have been introduced in several other kinds of applica-
tions.
Databases is a domain where linguistic hedges are often seen useful (for instance,
in [34, 35]). In [34], modiﬁers are integrated in SQLf, a fuzzy extension of SQL, the
well-known query language for databases. Here, the “where” part of a select query
is associated with a fuzzy condition that could be deﬁned by means of a modiﬁer.
A modiﬁer is used to deﬁne a partition of the tuples from the database that can be
searched for, for instance, “select * from ... where salary is more or less equal to
...”. As a consequence, querying becomes more naturally expressed thanks to inter-
pretable conditions.
An example of use of modiﬁers in a machine learning task could be found in [36].
In this work, the linguistic hedges (Zadeh’s form) have been introduced in a mining
fuzzy association rules process. Various hedges are generated in order to increase a
fuzzy taxonomy associated with a transaction. The mining of fuzzy association rules
is done with this augmented fuzzy taxonomy in order to ﬁnd a maximum of rules,
taking into account linguistic descriptions.
In [37], a fuzzy description logic is introduced that handles hedges. Concept mod-
iﬁers are introduced as a chain of hedges. The sign of a hedge is used to position the

60
B. Bouchon-Meunier and C. Marsala
hedge with regards to the associated primitive concept. For instance, the sign of very
large is positive with regards to large, and the sign of more or less large is negative.
Modiﬁer Memberships are those of Zadeh (exponentiation). The authors introduce a
semantic based on hedge algebras and propose a decision procedure and an approach
to determine the satisﬁability of fuzzy constraints in their fuzzy description logic.
Image processing or image-based retrieval, in which a linguistic description of
images could be very useful to summarize their content, or to enhance their inter-
pretability, are often based on the use of linguistic hedges. Among existing works,
we can cite [38] which introduces linguistic hedges to model the feedback of the
user when the retrieved image is not fully satisfactory. The user proposes a modiﬁer
on the current query to create a new query that could be processed to enhance the
search.
In the acoustics domain, [39] has proposed the used of modiﬁers in a vocabulary
used to describe noise annoyance expresses by users with diﬀerent languages. Simi-
larity measures are then used to match foreign terms and ﬁnd correspondence among
them.
5 Conclusion
We have presented the main solutions to construct fuzzy modiﬁers associated with
linguistic hedges, in an attempt to expand the vocabulary available to describe
objects, either in a knowledge-base or in the outcomes of a fuzzy system. The purpose
is to provide a ﬂexible way to represent knowledge without increasing the complexity
of the system. The main qualities of fuzzy systems are kept in mind: ﬁrst their inter-
pretability and their ability to handle easy to understand descriptions of objects, and
second the graduality inherent in their deﬁnitions, providing soft transitions between
descriptions and mimicing a very natural facet of human reasoning.
Fuzzy modiﬁers are deﬁned in a prior way on the basis of expert knowledge or
psychometric analyses. Among the various approaches enabling to provide their def-
initions, we have given priority to those which are involved in the interpretability of
fuzzy systems. We have shown that fuzzy modiﬁers can also be learnt or tuned auto-
matically, for instance with the help of genetic algorithms in hybrid systems. It is
clear that these ways to obtain linguistic modiﬁers are similar to those providing
membership functions of fuzzy sets representing linguistic descriptions of variables.
They can be regarded as a kind of standardization of linguistic terms easy to handle
and to share with experts or end users of fuzzy systems.
Future works can focus on the speciﬁc methods to use fuzzy modiﬁers in new
areas, such as fuzzy case-based reasoning, fuzzy inductive learning or fuzzy sum-
marization, to name but a few.

Fuzzy Modiﬁers at the Core of Interpretable Fuzzy Systems
61
References
1. Rissland, E.: Ai and similarity. IEEE Intell. Syst. 21, 39–49 (2006)
2. Zadeh, L.A.: Outline of a new approach to the analysis of complex systems and decision
processes. IEEE Trans. Syst. Man. Cybern. 3:28–44 (1973)
3. Zadeh, L.A.: The concept of a linguistic variable and its application to approximate
reasoning—part I. Inf. Sci. 8, 199–249 (1975)
4. Zadeh, L.A.: The concept of a linguistic variable and its application to approximate
reasoning—part II. Inf. Sci. 8, 301–357 (1975)
5. Zadeh, L.A.: The concept of a linguistic variable and its application to approximate
reasoning—part III. Inf. Sci. 9, 43–80 (1975)
6. Zadeh, L.A.: A fuzzy-set-theoretic interpretation of linguistic hedges. Cybern. Syst. 2(3), 4–34
(1972)
7. Hersh, H.M., Caramazza, A.: A fuzzy set approach to modiﬁers and vagueness in natural lan-
guage. J. Exp. Psychol. 105, 254–276 (1976)
8. Lakoﬀ, G.: Hedges: a study in meaning criteria and the logic of fuzzy concepts. J. Philos. Logic
2, 458508 (1973)
9. Macvicar-Whelm, P.J.: Fuzzy sets, the concept of height and the edge very. IEEE Trans. Syst.
Man Cybern. 8(6), 507–512 (1978)
10. Baldwin, J.F.: Fuzzy logic and fuzzy reasoning. Int. J. Man-Mach. Stud. 11(4), 465–480 (1979)
11. Eshragh, F., Mamdani, E.H.: A general approach to linguistic approximation. Int. J. Man-Mach.
Stud. 11(4), 501–519 (1979)
12. Wenstop, F.: Deductive verbal models of organizations. Int. J. Man-Mach. Stud. 8(3), 293–311
(1976)
13. Zadeh, L.A.: PRUF-a meaning representation language for natural languages. Int. J. Man-
Mach. Stud. 10(4), 395–460 (1978)
14. Ho, N.C., Nam, H.V.: An algebraic approach to linguistic hedges in zadehs fuzzy logic. Fuzzy
Sets Syst. 129, 229–254 (2002)
15. Ying, M., Bouchon-Meunier, B.: Quantiﬁers, modiﬁers and qualiﬁer in fuzzy logic. J. Appl.
Non-class. Logics 7(3), 335–342 (1997)
16. Ying, M., Bouchon-Meunier, B.: Approximate reasoning with linguistic modiﬁers. Int. J. Intell.
Syst. 13, 403–418 (1998)
17. Ostaszewski, K., Karwowski, W.: Linguistic hedges and fuzzy normalization operator. In: Pro-
ceedings of the Third Congress of the International Fuzzy Systems Association, pp. 528–531,
Seattle, Washington, USA (1989)
18. Burillo, P., Fuentes-González, R., González, L., Marin, A.: On contrast intensiﬁcation opera-
tors and fuzzy equality relations. Mathw. Soft Comput. 7, 15–27 (2000)
19. Bloch, I. (ed.): Information Fusion in Signal and Image Processing: Major Probabilistic and
Non-Probabilistic Numerical Approaches. Wiley-ISTE, New York (2007)
20. De Cock, M., Kerre, E.E.: A context-based approach to linguistic hedges. Int. J. Appl. Math.
Comput. Sci. 12, 371–382 (2002)
21. Bouchon, B.: Stability of linguistic modiﬁers compatible with a fuzzy logic. In: Uncertainty in
Intelligent Systems. Lecture Notes in Computer Science, pp. 63–70. Springer, Berlin (1988)
22. Bouchon-Meunier, B.: Fuzzy logic and knowledge representation using linguistic modiﬁers.
In: Kacprzyk, J., Zadeh, L.A. (eds.) Fuzzy Logic for the Management of Uncertainty. Wiley,
New York (1992)
23. Bouchon, B., Yao, J.: Linguistic modiﬁers and gradual membership to a category. Int.l J. Intell.
Syst. 7(1), 25–36 (1992)
24. Bouchon-Meunier, B., Marsala, C.: Linguistic modiﬁers and measures of similarity or resem-
blance. In: Proceedings of the 9th IFSA World Congress, pp. 2195–2199, Vancouver, Canada
(2001)
25. De Cock, M., Kerre, E.E.: Fuzzy modiﬁers based on fuzzy relations. Inf. Sci. 160(1–4), 173–
199 (2004)

62
B. Bouchon-Meunier and C. Marsala
26. Bodenhofer, U., De Cock, M., Kerre, E.E.: Openings and closures of fuzzy preorderings: the-
oretical basics and applications to fuzzy rule-based systems. Int. J. Gen. Syst. 32(4), 343–360
(2003)
27. Bouchon-Meunier, B.: Interpretable decisions by means of similarities and modiﬁers. In: IEEE
International Conference on Fuzzy Systems, Jeju, South Korea (2009)
28. Bouchon-Meunier, B., Rifqi, M., Bothorel, S.: Towards general measures of comparison of
objects. Fuzzy Sets Syst. 84(2), 143–153 (1996)
29. Godo, L., Lopez de Mantaras, R., Sierra, C., Verdaguer, A.: MILORD: the architecture and the
management of linguistically expressed uncertainty. Int. J. Intell. Syst. 4(4), 471–501 (1989)
30. Liu, B.-D., Chen, C.-Y., Tsao, J.-Y.: Design of adaptive fuzzy logic controller based on
linguistic-hedge concepts and genetic algorithms. IEEE Trans. Syst. Man Cybern. B Cybern.
31(1), 32–53 (2001)
31. Casillas, J., Cordon, O., Herrera, F., Magdalena, L.: Accuracy improvements to ﬁnd the balance
interpretability-accuracy in linguistic fuzzy modeling: an overview. In: Casillas, J., Cordon,
O.. Herrera Triguero, F., Magdalena, L. (eds.) Accuracy Improvements in Linguistic Fuzzy
Modeling, vol. 129 of Studies in Fuzziness and Soft Computing, pp. 3–24. Springer, Berlin
(2003)
32. Cordon, O., Del Jesus, M.J., Herrera, F.: Genetic learning of fuzzy rule-based classiﬁcation
systems cooperating with fuzzy reasoning methods. Int. J. Intell. Syst. 13, 1025–1053 (1997)
33. Gonzàlez, A., Pérez, R.: A study about the inclusion of linguistic hedges in a fuzzy rule learning
algorithm. Int. J. Uncertain. Fuzziness Knowl.-Based Syst. 7(3), 257–266 (1999)
34. Bosc, P., Pivert, O.: Fuzzy queries and relational databases. In: Proceedings of the ACM Sym-
posium on Applied Computing (SAC’94), pp. 170–174, Phoenix, Arizona, USA (1994)
35. Nakajima, H., Sogoh, T., Arao, M.: Fuzzy database language and library-fuzzy extension to
sql. In: Proceedings of the Second IEEE International Conference on Fuzzy Systems, vol. 1,
pp. 477–482, San Francisco, CA, USA (1993)
36. Chen, G., Wei, Q.: Fuzzy association rules and the extended mining algorithms. Inf. Sci. 147(1–
4), 201–228 (2002)
37. Hölldobler, S., Khang, T.D., Störr, H.: A fuzzy description logic with hedges as concept mod-
iﬁers. In: Proceedings InTech/VJFuzzy2002, pp. 25–34 (2002)
38. Medasani, S., Krishnapuram, R.: A fuzzy approach to content-based image retrieval. In: Pro-
ceedings of th IEEE International Conference on Fuzzy Systems, pp. 1251–1260, Seoul, Korea
(1999)
39. Botteldooren, D., Verkeyn, A., Cornelis, C., De Cock, M.: On the meaning of noise annoyance
modiﬁers: a fuzzy set theoretical approach. Acta Acustica A United Acustica 88, 239–251
(2002)

Fuzzy Modiﬁers at the Core of Interpretable Fuzzy Systems
63
Authors Biography
Bernadette Bouchon-Meunier graduated from the Ecole Nor-
male Superieure at Cachan, she received the degrees of B.S. in
Mathematics and Computer Science, Ph.D. in Applied Mathe-
matics and D. Sc. in Computer Science from the University
of Paris. She is a director of research emeritus at the National
Centre for Scientiﬁc Research, the former head of the depart-
ment of Databases and Machine Learning in the Computer Sci-
ence Laboratory of the University Paris 6 (LIP6).
She is the Editor-in-Chief of the International Journal
of Uncertainty, Fuzziness and Knowledge-based Systems, the
(co)-editor of 24 books and the (co)-author of ﬁve. She super-
vised 52 PhD students.
Co-executive director of the IPMU International Confer-
ence held every other year since 1986, she served as the FUZZ-
IEEE 2010 and FUZZ-IEEE 2013 Program Chair, the IEEE
SSCI 2011 General Chair and the FUZZ-IEEE 2012 Conference Chair. She is currently the
IEEE Computational Intelligence Society Vice-President for Conferences (2014–2015) and a
member of the IEEE Women in Engineering Committee (2012–2014). She is an IEEE fellow
and an International Fuzzy Systems Association fellow. She received the IEEE Computational
Intelligence Society Meritorious Service Award in 2012. She is presently a distinguished lec-
turer for the IEEE CIS (2014–2016).
Christophe Marsala
is a Full Professor in Computer Sci-
ence at the French Laboratory of Computer Science (LIP6)
of the University Pierre and Marie Curie (Paris, France). He
is the head of the Learning Fuzzy and Intelligent systems
(LFI) team in the Databases and Machine Learning depart-
ment. Christophe Marsala has been with the LIP6 since 1998.
He obtained his PhD in Computer Science from the University
Pierre and Marie Curie in 1998 and his habilitation in Com-
puter Science from the same university in 2010. His research
is concerned with Artiﬁcial Intelligence, fuzzy machine learn-
ing and fuzzy data mining. In particular, his work is concerned
with the use and the extension of machine learning tools to
deal with imprecise data and that can be applied in dynami-
cal environments. He has been an IEEE Member since 2010,
and an EUSFLAT member since 2001. From 2001 to 2005, he
served as secretary of the EUSFLAT Society. He has authored papers in international jour-
nals, international conferences and books and he has served as reviewer for several conferences
and journals on these topics. He gave plenary lectures in international conferences: SITA 2010
(“Fuzzy decision trees : issues, methods, and applications”) and IPMU 2012 (“Fuzzy Machine
Learning in Dynamical Environments”). He has authored or co-authored 23 journal papers or
book chapters, and over 80 conference papers. He is the co-editor of 4 edited books.

Human and Machine
Intelligence — Between Fuzzy
Logic and Daoist Thought
Liya Ding and Xiaogan Liu
Abstract The theory of fuzziness, offering an important scientiﬁc approach in
building intelligent machines, has been researched and developed in the past ﬁfty
years from various perspectives and applied for real world problem solving in many
areas. Daoist thought, being one of the most inﬂuential schools of Chinese phi-
losophy, has been studied for more than two thousand years and its wisdom
exploited from generation to generation. Would a natural echo exist between the
modern fuzzy thinking and the ancient oriental Daoist thought?
Keywords Fuzzy logic ⋅Precisiation ⋅Computing with words ⋅Daoism ⋅
Nonaction
1
Fuzziness in Modeling Reality
Mathematical models are built for human dealing with the real world. However, the
subtle behavior of the natural world (as we perceive it) cannot be modelled by rigid
axioms. Most human concepts lack a rigorous deﬁnition for its vagueness and
imprecision as well as its changing meaning reﬂecting evolving nature and human
societies, the more rigorous the model, the less similar to reality. The great
achievement of the theory of fuzziness is to have succeeded to build models for
entities that lack a rigorous deﬁnition.
The famous Turing Test remains a dream of artiﬁcial intelligence until today.
Human intelligence has been taken as the gold standard of machine intelligence, but
such gold standard also lacks a rigorous deﬁnition. We apprehend the inner and the
outer world by vague feelings, which become progressively more precise. “… the
L. Ding (✉)
National University of Singapore, Singapore, Singapore
e-mail: ding.liya@gmail.com
X. Liu
Chinese University of Hong Kong, Hong Kong, China
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_5
65

machinery of fuzzy logic is needed for mechanization of human reasoning. In this
perspective, fuzzy logic is of direct relevance to achievement of human level
machine intelligence.” (Zadeh) [1]
To further emphasize why and how we can expect that fuzzy logic plays the role
of a bridge from natural to machine intelligence, Professor Zadeh has put forward
the following words:
“Science deals not with reality but with models of reality. In large measure, scientiﬁc
progress is driven by a quest for better models of reality. In the real world, imprecision,
uncertainty and complexity have a pervasive presence. In this setting, construction of better
models of reality requires a better understanding of how to deal effectively with impreci-
sion, uncertainty and complexity. To a signiﬁcant degree, development of fuzzy logic has
been, and continues to be, motivated by this need.” (Zadeh) [2]
What can be read out from the words above are guiding ideas and thought that
are signiﬁcant both technically and philosophically. From technical perspective, the
development of a good model of reality needs to take into account imprecision,
uncertainty and complexity. From philosophical perspective, humans deal with
reality not directly but through models that are only approximation of the real
world, and the better we handle imprecision, uncertainty and complexity, the better
the model we may be able to build.
It is the philosophized thought of fuzzy logic that inspires us to further discuss
some fundamental questions about human and machine intelligence, and explore a
natural echo between fuzzy logic and Daoist thought, being one of the most
inﬂuential schools of Chinese philosophy.
2
Admission of Imperfection
“A concept which has a position of centrality in fuzzy logic is that of a fuzzy set. Informally,
a fuzzy set is a class with a fuzzy boundary, implying a gradual transition from membership
to nonmembership. A fuzzy set is precisiated through graduation, that is, through asso-
ciation with a scale of grades of membership. Thus, membership in a fuzzy set is a matter of
degree. Importantly, in fuzzy logic everything is or is allowed to be graduated, that is, be a
matter of degree. Furthermore, in fuzzy logic everything is or is allowed to be granulated,
with a granule being a clump of attribute-values drawn together by indistinguishability,
equivalence, similarity, proximity or functionality. Graduation and granulation form the
core of fuzzy logic.” (Zadeh) [2]
The concept of “graded membership” applies to a class that lacks a rigorous def-
inition. Such concept is often confused with the concept of probability, which is
caused by an underlying random process or by lack of information. A situation of
lack of information may be improved when more information and data become
available, especially with the growth of big data and information technologies. The
lack of deﬁnition, however, relates to a more fundamental limitation in human’s
cognitive ability.
66
L. Ding and X. Liu

Although imprecision is a phenomena rooted at the limitation in human’s cog-
nitive ability, the idea of fuzziness is not universally accepted even today. To this
situation, Professor Zadeh says, “there are many misconceptions and misunder-
standings regarding fuzzy set theory and fuzzy logic. To some, I was advocating an
abandonment of the deep-seated tradition of striving for rigor and precision.”1
While most scientists are used to consider that accuracy and certainty are essential
principle for exploring true nature of the world, Daoist2 thinkers believe that the
ultimate source and true nature of the universe is uncertain and obscure.
As many philosophical and religious traditions, Daoism has its reﬂection and
doctrine about the source and ground of the universe, which is not so certain and
acknowledgeable. One signiﬁcant piece on this theme, which is found in the Laozi
[3], is not providing merely theory about the beginning of our world and myriad
things in the universe, but also the foundation of all other Daoist teachings and
characters. The unique speculation and reﬂection about the possible entity and state,
from which the universe comes, is a key for understanding the general attributes
and uniqueness of Daoist philosophy. There are noticeable points of Daoist thought,
and one comes of the most signiﬁcant points is concerning the true nature of the
universe.
There was something undifferentiated and yet complete,
Which existed before heaven and earth. (Chap. 25)
The word “undifferentiated” in the above quotation is a render of the Chinese
word “hun” which may suggest mixture, unclear, or shapeless etc. The state of the
origin and the formation of the beginning of the world, and the ground that sustains
myriad things are merely ambiguous. In the Laozi, all sentences related to the
ultimate reason and truth share the same style. Thus Daoist statements about the
true nature of the universe are always hesitating; at least they seem to reﬂect non-
perfect conﬁdence. Thus, the Laozi further states:
It may be considered the mother of the universe.
I do not know its name; I style it “Dao” (Tao, Way).
If forced to give it a name, I shall call it Great. (ibid.)
The author of Laozi frankly admits that he does not know what is the origin or
the mother of the universe. Dao is just a styled symbol or nickname of the source
1LotﬁA. Zadeh has distributed the same message repeatedly, the quotation here is from his
message distributed to BISC online discussion group on Oct 2013.
2The word Daoism suggests complicated thought system (or Daoist philosophy) and Daoist
religious movements. In this essay, the Daoist theories are mainly based on the ﬁrst Daoist text
Daodejing (Tao-te-ching) or the Laozi (Lao-tzu). This has nothing to do with Daoist religious
teachings. The quotations from the Laozi are based on Liu’s complication and adaptation from
various versions and translations, unless speciﬁc citations provided otherwise.
Human and Machine Intelligence …
67

and ground of the universe. It has no proper name. If being forced to give it a name,
he will call it Great. Obviously, great, similar to good, bad, big, small, etc., cannot
be used as a proper name. What the author repeatedly emphasizes is that no one
knows exactly the foundation and true nature of the world, though the author does
believe there is something functioning as the ultimate source and ground of all
beings in the world.
This makes Daoism different from many religious and philosophical doctrines.
The Bible describes clearly and precisely the processing of God’s creation of the
universe; Plato’s theory sees it speciﬁcally and systematically as the relation
between the transcendent kingdom of perfect ideas and the empirical world of
imperfect myriad things. Even in other Chinese or oriental religious and philo-
sophical schools, we ﬁnd such differences from Daoism. Buddhism conﬁdently
disserts that the truth of world is essentially empty, all existence we can see and feel
are just delusion and untrue; Confucian thinkers strongly believe that the moral
doctrine tian-li (heavenly principle) is the ultimate truth of the world.
Further examining, we ﬁnd some key concepts in which both the theory of
fuzziness and the thought of Daoism surprisingly coincide. Here we list a few.
A. True and False
In fuzzy logic everything is, or is allowed to be graduated, that is, be a matter of
degree. Fuzzy logic allows no clear distinguishing between true and false, and
introduces a numerical grade or word to indicate the degree of truth of a piece of
knowledge or information. In fuzzy inference systems, a perfect truth is not required
for data, information and knowledge, and the executions of inference are done on
an approximate basis. This greatly extends the ability of precise reasoning in
handling real world problems with imprecision, incompleteness, or partial truth. In
such, fuzzy logic establishes fuzzy reasoning containing precise reasoning as its
special case.
From the Daoist viewpoint, there is nothing absolutely right and true, yet true
and false dose not distinct clearly. The Laozi claims in Chap. 41:
The Dao which is bright appears to be dark.
The Dao which goes forward appears to fall backward.
The Dao which is level like a valley (hollow).
Great purity appears like disgrace.
Far-reaching virtue appears as if insufﬁcient.
True substance appears to be changeable.
Solid virtue appears as if unsteady.
True substance appears to be changeable.
([4]: 160)
B. Good and Bad
The idea that “membership in a fuzzy set is a matter of degree” made a fundamental
revolution to classical set theory, that is membership and non-membership are not
in an absolute distinguishing. This is depicted in Fig. 1.
68
L. Ding and X. Liu

In our everyday life, good and bad are often treated as two opposite concepts.
However, when the two are applied to make evaluation with the same criteria, they
both appear to be fuzzy sets with unsharpened boundary and possible overlap in
between. The same spirit applies to many pairs of concepts that are usually con-
sidered opposite, such as long and short, big and small, high and low, fast and slow,
etc. The distinguishing factors between these paired concepts are only relative.
Some well accepted and widely adopted schemes in our life, such as “passing” or
“failure” of school examination, are a binary precisiation of “good-bad” deﬁned as
crisp sets on numerical scores. The theory of fuzzy set provides a mathematical tool
to handle concepts that are not fully distinguishable and differentiable.
The sentences previously quoted from Laozi Chap. 41 reveal Daoist position that
everything appears as containing the opposite elements, which are in contradiction
and mutual transformation. In addition, the uncertainty comes from the transfor-
mation of the oppositional elements in all beings or things. For example, the Laozi
argues in Chap. 58:
Calamity is that upon which happiness depends;
Happiness is that in which calamity is latent. …
Then the correct again becomes the perverse
And the goodness will again become evil.
([4]: 167)
Similar argument stated in Chap. 2 of Laozi:
When the people of the world all know [certain] beauty as beauty,
There arises the recognition of ugliness.
When they all know the good as good,
There arises the recognition of evil.
Therefore:
Being and non-being produce each other;
Difﬁcult and easy complete each other;
Long and short contrast each other;
Fig. 1 The concepts of a set and a fuzzy set are derived from the concept of a class through
precisiation. A fuzzy set has a fuzzy boundary. A fuzzy set is precisiated through graduation
(Fig. 1 from Zadeh in [2])
Human and Machine Intelligence …
69

High and low distinguish each other;
Front and back follow each other.
Therefore the sage manages affairs without action.
And spreads doctrines without words. ([4]: 140)
According to the Laozi, all seemingly quite different oppositions are actually in
transition and mutually effected. All these oppositional transformations suggest the
difﬁculty of our knowledge and recognition in reality. It should be noticed that
when mentioning “manages affairs without action” and “spreads doctrines without
words” the Laozi does not mean to do nothing, but special action different from
regular actions by common rulers and people. We shall relate to this point in Sect. 4
with fuzzy decision.
The important ﬁnding here is that both Fuzzy Logic and Daoist thought are
aware of the fundamental limitation in human’s cognitive ability in handling the
complexity of real world. While Daoism establishes the philosophical foundation
for the discussion of this limitation, fuzzy logic provides a practical mathematical
tool to describe concepts that are not fully distinguishable and differentiable. In
other words, Daoism explores the limitations and the reasons that they exist, and
fuzzy logic discusses how we can act under such limitations.
3
Precisiation and Description
In order to have a machinery to deal with imprecision in reality, we need precis-
iation for our understanding and description of reality. Informally, precisiation is an
operation which transforms an object, p, into another object, p*, which is more
precisely deﬁned, in some speciﬁed sense, than p. [2]. This is depicted in Fig. 2.
The signiﬁcance of the theory of precisiation may be understood from two
aspects: a technical aspect, and a philosophical aspect. From the technical aspect,
with the utilization of fuzzy sets, fuzzy truths, fuzzy numbers, type-2 or higher order
fuzziness, it allows but not ignores or rejects undifferentiation; it provides a spec-
trum between true and false, know and don’t-know, to more naturally reﬂect the
Fig. 2 Basic concepts relating to precisiation and cointension. (Fig. 14 from Zadeh in [2])
70
L. Ding and X. Liu

human knowledge imperfection; it offers tools for describing highly complex sit-
uation with mixed types of imperfection involved.
From the philosophical aspect, it accepts the limitations in human cognitive
ability, therefore sets up a rational foundation for discussion of machine intelligence
having human intelligence as the gold standard. There are many things in the
universe that we don’t know, and more importantly there are also many things we
don’t know how much we know, or don’t know how to describe or evaluate what
we know. Precisiation is a process through which we describe, and evaluate the
world we perceive with our limited capability and resources. The result of precis-
iation is a simpliﬁed model of reality but not the true reality, referring to Fig. 2, we
have p* ≠p.
The philosophical thinking behind the fuzziness and precisiation ﬁnds an echo
from the Daoist admission of the limitation of human’s cognition.
The general feature of Dao combines both being and nonbeing, though not in a
strait forward way. All descriptions of Dao seem to suggest non-being (wu), in the
sense that human beings cannot grasp it because it is not any concrete thing humans
can perfectly capture. The Laozi, in Chap. 14, describes it this way:
We look at it and do not see it,
its name is the invisible.
We listen to it and do not hear it,
its name is the inaudible.
We touch it and do not ﬁnd it,
its name is the subtle (formless)….
Inﬁnite and boundless,
it cannot be given any name [‘unnameability’]
It reverts to nothingness,
this is called shape without shape,
image without entity…
([4]: 146)
“It cannot be given any name” or it cannot be nameable implies the impossibility
for human beings to cognize Dao, because it is transcendent and out of our
approach. Similar ideas are presented in the Laozi Chap. 25, which reveals the
Daoist conception of the source and ground of the universe. Here it reads:
There was something undifferentiated and yet complete,
Which existed before heaven and earth.
Soundless and formless, it depends on nothing, and does not change.
It may be considered the mother of the universe.
I do not know its name; I style it “Dao” (Tao, Way).
If forced to give it a name, I shall call it Great.
Now being great means functioning everywhere.
Functioning everywhere means far-reaching.
Being far-reaching means returning to the original state.
This is a reﬂection of Daoist attitude towards human being’s capacity of cog-
nition and understanding. Daoism has little dogmatic assertion; instead, Daoism
intends to faithfully represent the difﬁculty and limitation of human beings obser-
vation and understanding.
Human and Machine Intelligence …
71

The words “soundless and formless” in the above-quoted passage is also a kind
of description of Dao in coping with the ambiguity. Without a deﬁnition a
“description” leaves a room of tolerance for imprecision and vagueness. The lim-
itation of human beings’ intelligence and cognition is an important concern of
Daoism. We can further examine knowing and unknowing in dealing the reality to
better understand Dao. In Chap. 1, the Laozi argues:
The way can be spoken of,
but it will not be the constant way;
The name can be named,
but is will not be the constant name.
The nameless is the beginning of myriad things,
The named is the mother of myriad things. ([5]: 267)
Here the nameless and the named are two characteristic aspects of Dao, which
refer to the human faculty of recognition of Dao and also its limitation.
Furthermore from the admission of the fundamental limitation in human’s
cognitive ability, another important ﬁnding here is that both Fuzzy Logic and
Daoist thought are aware of the gap existing between any description (model) and
the reality being described. While Daoism has this idea supported by its ancient
wisdom, fuzzy logic names precisiation as one of the unique characteristics in
human thinking towards modern machine intelligence.
4
Decision with Imperfection and Action with Nonaction
Fuzzy decision [6], or decision in fuzzy environment, is the conﬂuence of fuzzy
goals and constraints that reﬂect naturally real world situations. One of the key
features found in fuzzy decision is that the decision maker is not forced to give a
precise formulation, merely for the sake of mathematical reasons. A fuzzy decision
is made with a compromise of the satisfaction of multiple constraints and objectives
that are described with fuzziness. The importance or proportion of contribution of
each constraint or objective may be arranged in appropriate ways.
There are two key ideas here that attract our attention. The ﬁrst is that one is not
forced to provide a precise formulation, if the original problem comes from a fuzzy
environment; the second is that a decision is made as the conﬂuence of fuzzy goals
and constraints for a compromise.
Let’s examine Daoist perspectives in decision and action. Viewing the inevitable
transformation of everything in the world, the Laozi argues for a distinctive way of
actions, of which the complicated meaning cannot be translated, therefore we take a
compromised way using nonaction as a token for the Chinese term wu-wei. Lit-
erally, it sounds like no action at all, but actually, it implies a special way of action,
a negation of regular ways, for transcendent and better results. That is summed as a
famous saying: “To do nothing yet leave nothing undone.” This way is different or
opposite from common ways to deal with governance, as well as general affairs.
72
L. Ding and X. Liu

The spirit of nonaction is to do business for better or distinctive outcomes and
minimum side or bad effects.
Aiming at a conﬂuence of fuzzy goals and constraints allows one to make an
optimal decision with multiple criteria and objectives taken into account. Without
requiring forced precision in modeling fuzzy decision making, one will be able to
keep more information (with imprecision and vagueness) from reality and bring that
to ﬁnal decision; and will also be able to minimize the extra inaccuracy introduced
through forced precision in early stages. In other words, no forced action for
precision leads to more accuracy to reality. At this point, we ﬁnd another consis-
tency between the spirits of fuzzy logic and Daoism in their deep roots.
For the Daoist aspects, a typical manner of nonaction is assisting (fu in the
original Laozi literature, which can be translated as to assist, help, or support, etc.)
From the Laozi Chap. 64, we read:
Therefore the sage desires not to desire,
And does not value goods that are hard to come by;
He studies what is not studied,
And makes good of the mistakes of the multitude.
And so the sage is
able to assist the myriad things’ naturalness,
but is unable to act [in the common manner]
As for the meaning of fu or assisting, it is better understood as a spectrum
between two extremes. One extreme is restraint, manipulation, interruption, inter-
ference, exploitation, control, and oppression; the other is pampering, spoiling,
indulgence, permissiveness, and over-protection. Thus, fu or assistance is the
careful and prudent art of sagely leadership; its purpose and objective are com-
pletely aimed at beneﬁting the myriad things, no aspect of which shows off the
sage’s own importance and intelligence or accrues personal beneﬁts. This fu or
assistance is a typical example of nonaction for better results from unusual actions.
The key point lies in the last sentence: the sage assists the myriad creatures to
realize their natural prosperousness, but dares not to act generally in the manner of
the common people. In this way, the sage seems to do nothing yet reaches the best
result: all things get the right chance to develop themselves in a harmony condition.
In the past ﬁfty years LotﬁA. Zadeh, the founding father of fuzzy logic, has
selﬂessly provided his support, and assistance to thousands of scientists and
researchers by guiding new directions, encouraging discussions, and listening to
comments and even disagreements. Without his guidance and continuous efforts in
establishing a friendly, an encouraging, and a harmony environment in the fuzzy
logic community, the achievement of fuzzy logic research would never be the same.
The success of fuzzy logic witnesses how the spirit of fu helps our development in
science and technology.
The study of fuzzy decision has made an important foundation of utilizing
human intelligence for decision making in a fuzzy environment. “Humans have
many remarkable capabilities. Among them there are two that stand out in
importance. First, the capability to converse, communicate, reason and make
rational decisions in an environment of imprecision, uncertainty, incompleteness of
Human and Machine Intelligence …
73

information, partiality of truth and partiality of possibility. …” (Zadeh) [2]. In spite
of the limited cognitive ability in describing the real world with imprecision,
uncertainty, and partial truth, humans are able to approximate the reality for their
problem solving with amazingly simpliﬁed models, through observation and intu-
ition, as well as rational analyses. One important evidence is that a fuzzy rule-based
system in general uses a far less number of rules compared to a typical rule-based
expert system for the same application, through appropriate fuzzy granulation.
Computing with words [1, 2] opens the door for further exploring potential utili-
zation of human wisdom toward the development of human level machine
intelligence.
5
Between Fuzzy Logic and Daoist Thought
Fuzzy logic as a principal member of soft computing has been considered to be
positioned in a “soft” branch of science among the others. On the other hand,
Chinese philosophy has been considered less strict compared to Western philoso-
phy, and Daoist thought is one of the most inﬂuential schools of Chinese philos-
ophy. Inspired by the philosophized thought of fuzzy logic, we have made an
attempt to explore an echo between fuzzy logic and Daoist thought, and compare
similarities of some key concepts from both theories. Table 1 brieﬂy summarizes
our key ﬁndings.
Table 1 Similarities of concepts of fuzzy logic and daoism
Similarities
Differences
1
Understanding the world: Admission of
ambiguity, complex, and transience of the
world surrounding us.
Daoist philosophy is supported by the
metaphysical concept of Dao, though
Dao’s certain features are the
representatives of our empirical world.
Fuzzy logic makes generalization of
classical mathematical tools to deal with
reality, with tolerance of imprecision.
2
Describing the world: Human’s common
talent or capability is not enough for
recognizing the objective world. In other
words, human being’s cognitive ability is
limited.
Daoism emphasizes the inﬁnite and
transience of myriad things in the world.
Fuzzy Logic aims at practical approach for
human intellectual activities through
precisiation.
3
Acting: Human beings should and can try
to approximate the truth through irregular
way of knowing. This approximation is
more accurate than certain precise
descriptions or claims because many
boundaries in the real world are not clear.
Daoist thought develops its theory through
rational observation and analyses, as well
as intuition to approximate the true nature
of the world.
Fuzzy Logic has graduation and
granulation as keys to approach the inﬁnite
and complex reality.
74
L. Ding and X. Liu

In summary, our motivation and the signiﬁcance of such comparison are sup-
ported by several points.
(1) Finding the mutual support between scientiﬁc approaches and philosophical
wisdom, between the modern and antique, and between the West and East
cultures.
(2) Having found more broad and solid theories to correct dogmatic and romantic
belief about inﬁnite human knowledge and capability.
(3) Further exploring human wisdom in rational action with knowledge imper-
fection toward future human level machine intelligence.
(4) The research in Daoism and fuzzy logic can be inspired from each other to
further develop their theories and argumentations.
6
Conclusion
We have argued that important concepts in fuzzy logic and Daoist thought echo each
other from afar. The values of Daoism to modern society have recently been sig-
niﬁcantly recognized in critical issues, such as environment protection, social har-
mony, and management science [7–9]. Would the application offuzzy thinking make
it more executable in modern society for some of the key ideas of Daoist thought?
Would it be possible to further exploit human wisdom in ancient thoughts through
the channel of fuzzy thinking, to support future development of machine intelli-
gence? We do not yet have concrete answers and our ﬁndings are still very pre-
liminary, but we believe such discussion will be beneﬁcial for building a more bright
future of the world, either from system engineering or human society point of view.
References
1. Zadeh, L.A.: Toward human level machine intelligence ─it is achievable? The need for a
paradigm shift. IEEE Comput. Intell. Mag. 3(3), 11–22 (2008)
2. Zadeh, L.A.: Fuzzy logic. In Meyers, R.A.: Encyclopedia of Complexity and Systems Science.
Springer (2009) (Entry 769)
3. Lynn, R.J. tran.: The Classic of the Way and Virtue: A New Translation of the Tao-te Ching of
Laozi as Interpreted by Wang Bi. Columbia University Press, New York (1999)
4. Chan, Wing-tsit.: A Course Book in Chinese Philosophy. Princeton University Press, Princeton
(1973)
5. Lau, D.C.: Tao Te Ching: A, Bilingual edn. The Chinese University Press, Hong Kong (2001)
6. Zimmermann, H.J.: Fuzzy Sets, Decision Making and Expert Systems. Kluwer Academic
Publishers, Netherlands (1987)
7. Liu, X.: Naturalness (Tzu-jan), the core value in Taoism: its ancient meaning and its
signiﬁcance today. In: Kohn, Livia, LaFarque, Michael (eds.) Lao-tzu and the Tao-te-ching,
pp. 210–228. The State University of New York Press, Albany, New York (1998)
Human and Machine Intelligence …
75

8. Liu, X.: A taoist perspective: appreciating and applying the principle of femininity. In: Raines,
J.C., Maguire, D.C. (eds.) What Men Owe to Women: Men’s Voices from World Religions,
pp. 239–257. The State University of New York Press, Albany, New York (2001)
9. Liu, X.: Non-action and the environment today: a conceptual and applied study of Laozi’s
Philosophy. In: Girardot, N.J., Miller, J., Liu, X. (eds.) Daoism and Ecology, pp. 315–339. The
Center for the Study of World Religions, Harvard Divinity School, Cambridge, MA (2001)
Authors Biography
Liya Ding Ph.D. (Computer Science), Meiji University, Japan,
1991.3.
(Thesis: “Research on Fuzzy Prolog”)
B.E. (Computer Science), Shanghai University of Technol-
ogy, China, 1982.1.
Research
Interests: Fuzzy Logic, Approximate Reasoning, Soft Computing, Knowledge-based Systems,
Intelligent Systems and Technology.
Principal Investigator, “Research and Development of Knowware System” (2007 ∼2009), and
“Development and Application of Knowware System” (2011 ∼2013), both funded by Science and
Technology Development Fund, Macau.
Editor and contributor of book, “A New Paradigm of Knowledge Engineering by Soft
Computing”, World Scientiﬁc, 2001.
2011.9 ∼
Member, Intelligent Systems & Technology, Institute of Systems
Science (ISS), National University of Singapore (NUS), Singapore.
2005.10 ∼2011.6
Dean (2008.7 ∼2010.9), Vice Dean (2005.10 ∼2008.6), Faculty of
Information Technology, Macau University of Science and Technology,
Macau.
2002.9 ∼2005.8
Dean, the School of Intelligent Systems & Technology, Inter-University
Institute of Macau (IIUM) (now the University of Saint Joseph), Macau.
1991.5 ∼2002.8
Programme Director, Senior Lecturer, Senior Consultant, Project
Leader, Institute of Systems Science (ISS), National University of
Singapore (NUS), Singapore. Project Leader (1993 ∼1996), RWCP
(Real World Computing Partnership, Ministry of International Trade
and Industry, Japan) -ISS Lab.
76
L. Ding and X. Liu

Dr. Xiaogan LIU receiving his PhD from Peking University in
1985, is professor, and the founding and honorary director of the
Research Centre for Chinese Philosophy and Culture, the
Chinese University of Hong Kong (CUHK). He once taught
and conducted research at Peking University, Harvard Univer-
sity, Princeton University, the National University of Singapore.
He is the author of Classifying the Zhuangzi Chapters,
Orientational Issues in Textual Interpretation, Laozi Gujin
(The Laozi from the Ancient to Current), Quanshi yu Dingxiang
(Hermeneutics and Orientation), Liangjihua yu Fencungan
(Polarization and the Sense of Propriety), the editor and a key
contributor, Dao Companion to Daoist Philosophy (Springer
2015), and the founding editor of the Journal of Chinese
Philosophy and Culture. He served as visiting professor and
keynote speaker at famous universities and institutes in Asia,
America, and Europe. He has received prizes and awards for excellence in teaching and research
works in China, Singapore, and Hong Kong.
Human and Machine Intelligence …
77

Developing Fuzzy State Models as Markov
Chain Models with Fuzzy Encoding
Dimitar Filev, Ilya Kolmanovsky and Ronald Yager
Abstract This paper examines the relationship and establishes the equivalence
between a class of dynamic fuzzy models, called Fuzzy State Models (FSM), and
recently introduced Markov Chain models with fuzzy encoding. The equivalence
between the two models leads to a methodology for learning FSMs from data and a
systematic way for model based design of rule-based fuzzy controllers. The pro-
posed approach is demonstrated on a case study of vehicle adaptive cruise control
system in which an FSM is identiﬁed from simulation data and a fuzzy feedback
controller is generated by exploiting the Stochastic Dynamic Programming (SDP).
Keywords Automotive applications ⋅Fuzzy systems ⋅Granular computing ⋅
Markov models ⋅Possibility theory ⋅Belief functions ⋅Stochastic dynamic
programming
The research of Ilya Kolmanovsky was supported in part by the National Science Foundation
Award Number ECCS 1404814 to the University of Michigan.
D. Filev (✉)
Research & Innovation Center, Ford Motor Company, Dearborn, MI, USA
e-mail: dﬁlev@ford.com
I. Kolmanovsky
Department of Aerospace Engineering, University of Michigan, Ann Arbor, MI, USA
e-mail: ilya@umich.edu
R. Yager
Machine Intelligence Institute, Iona College, New Rochelle, New York, USA
e-mail: yager@panix.com
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_6
79

1
Introduction
Fuzzy systems are widely used in process modeling and control as tools for han-
dling system complexity and accounting for information uncertainty. Most of the
dynamic system applications of the fuzzy systems exploit the following two
interpretations of system dynamics within the If … Then rule framework.
The ﬁrst approach follows Mamdani’s concept of fuzzy control [1, 2] in which a
family of If … Then rules with fuzzy predicates are used to deﬁne a control algo-
rithm realizing nonlinear PI, PD or PID-like control strategies. These are nonlinear
mappings from the state space to the space of control variables that implement
intuitive control strategies with no requirements for an explicit plant model.
The second approach is based on the Takagi-Sugeno (TS) models [3] which
exploit families of rules with fuzzy predicates and functional consequences. The
antecedents of the rules decompose the state space into a set of regions with
corresponding linear deterministic models. The state of the TS model is a nonlinear
combination of the states of the subsystem models. This approach may be viewed as
a generalization of the gain-scheduling technique in which piecewise linear models
that are associated with multiple fuzzily deﬁned regions of the state space are
combined.
Both types of fuzzy models are focused on the deterministic, i.e. the defuzziﬁed
value of the system output. In recent years much progress has been made on
techniques for improving the performance of fuzzy control algorithms, stability
analysis, and systematic design of fuzzy controllers based on TS state and
input-output models of the plant. With the fuzzy decomposition, the non-linear
system is represented by a polytopic nonlinear system of coupled linear models [4,
5]. This polytopic representation leads to sufﬁcient stability conditions for the TS
systems and a systematic design methodology that is based on solving Linear
Matrix Inequalities (LMIs), e.g. [6]. The TS approach, with its strong theoretical
underpinnings, addresses some of the major criticisms regarding the lack of rig-
orous analytical framework of the fuzzy control and places fuzzy control as one of
the tools of modern control theory. However, despite the progress made towards the
development of formal analytical model-based approaches for designing TS fuzzy
control systems, most of the practical fuzzy system applications remain centered
around heuristic rule-based control utilizing If … Then rules. One of the reasons for
this is that the TS approach, although based on the methodology of approximate
reasoning, is mostly focused on linear models with little if any ability to incorporate
subjective information and heuristics. It seems that this observation only conﬁrms
the original assertion of Mamdani who introduced fuzzy logic control as a powerful
tool to “convert heuristic control rules stated by a human operator into an automatic
control strategy” [2].
The progress in Markov Chain models with fuzzy encoding [7–9] suggests that
the If … Then rules that have been mostly used as static mappings or as fuzzy
controllers have the potential for addressing some of the deﬁciencies of the existing
fuzzy models, especially when they are applied to dynamic systems. This includes
80
D. Filev et al.

the ability to represent the system states as possibility distributions, to analytically
describe the evolution of the states, and to formulate and solve general multistage
decision problems in uncertain environment, including optimization problems that
were originally formulated by Bellman and Zadeh in [10].
Speciﬁcally, we are interested in determining an optimal control strategy for a
general time invariant ﬁnite state dynamic system in which the state variable x takes
values from a ﬁnite set of states A = fA1, A2, . . . , Ang and the control variable
u ranges over a ﬁnite set B = fB1, B2, . . . , Brg.
We assume that the states
Ai, i ∈1, . . . , n
f
g, and the controls Bj, j ∈1, . . . , r
f
g, are fuzzy subsets of the state
and control universes X and U. We also assume that system dynamics are described
by a set of rules expressing the following equation,
x + = f x, u
ð
Þ,
where f : X × U →X is a speciﬁed random function deﬁning the transition from the
current state x to the next state x + under the control u. We refer to this special type
of dynamic system models as the Fuzzy State Models (FSMs).
The considered encoding of the state and control variables into fuzzy subsets is
inspired by the ability of the fuzzy partitioning to address the uncertainty in the
coding of the continuous signals [7, 9]. Our interest in dynamic optimization
problems for FSMs is motivated by the growing interest in the applications of the
stochastic dynamic programming and stochastic model predictive control [11–14]
based on Markov Chain Models for on-board applications, especially for the
automotive and aerospace systems exposed to rapid transients and disturbances.
In this paper we address the basics of the FSMs by expanding the recent results
on Markov Chain models with fuzzy encoding [7, 9]. We propose a calculus for
formalizing the FSMs by using concepts and results from the Dempster-Shafer
theory of evidence [15, 16]. By examining the relationship between the possibility
and probability theory [17, 18] we demonstrate the equivalence between the FSMs
and the Markov Chain models with fuzzy encoding [9]. We further investigate the
mechanism of propagating possibility distributions by FSMs and derive a recursive
analytical expression for the possibility distribution that is inferred by a model of
this type. The developments in the paper pertain to the critical question [19]
challenging the ability of the theory of approximate reasoning to deal with prop-
agating the possibility distributions by a fuzzy system. This important theoretical
problem is discussed throughout the paper by utilizing the established relationships
and similarities between fuzzy systems, belief structures, and Markov Chain
models. From that perspective our approach differs from the works on the abstract
dynamic fuzzy system theory, e.g. [20]. Based on the proven equivalence between
the FSMs and the Markov Chain models with fuzzy encoding we propose a sys-
tematic approach to learning FSM from data. We also reveal how the learned FSMs
can be used in conjunction with SDP for model based design of fuzzy controllers.
Results are demonstrated on a case study exploiting a FSM of car following
dynamics and followed by SDP based synthesis of an adaptive cruise controller
implementing rule base type fuzzy control algorithm.
Developing Fuzzy State Models as Markov Chain Models …
81

2
Fuzzy State Models
The conventional fuzzy rule models of the type,
If u is Ai then y is Bi, i = 1, 2, . . . , m
f
g,
ð1Þ
deﬁne mappings from the fuzzy partitioning of the input space (rule antecedents) to
a corresponding partitioning of the output space (rule consequents), and can be
viewed as approximations of static input-output functions of the form, y = TðuÞ.
Many of the fuzzy control applications exploit the Mamdani controller [1, 2]
approach. This approach uses static model (1) representing controller dynamics by
rules mapping the state space (the vector of the error and its derivative) to the space
of control variables:
If x is Ai then u is Bi, i = f1, 2, . . . , mg,
i.e., models of the form u = GðxÞ. These models generalize the structure of the
widely used industrial look-up table (LUT) controllers [21]. Although highly efﬁ-
cient for designing practical heuristic control strategies, these models cannot be
used for plant modeling, model-based design, and for analysis of the stability and
performance of feedback control systems. Their main drawback is the lack of
efﬁcient mechanism to describe the state dynamics and the interaction between the
system inputs, states, and outputs.
Yet a large number of fuzzy models that are used to approximate system
dynamics belong to the TS type [3]:
If x is Ai then x + = Fix + Giu, i = f1, 2, . . . , mg
ð2Þ
Such models use nonlinear weighting functions to combine multiple linear state
models, i.e.,
x + = ∑
n
i = 1
νi x
ð ÞðFix + GiuÞ.
These models result in polytopic representations of the nonlinear dynamics and
have become one of the common tools for modeling and control of piecewise linear
systems. Their main drawback is in their limited interpretability since the entire
model dynamics are captured by the linear subsystems and the role of the fuzziness
is to determine the regions where the linear subsystems are deﬁned. Thus one of the
prospects of introducing the fuzzy system concept – the opportunity of infusing
heuristic information and human knowledge in the control system design – is not
completely utilized in the TS models (2) and the associated control design methods.
Multiple attempts to develop fuzzy system models of Mamdani type (1) that can
be applied to model based design of fuzzy controllers, see e.g. [18, 22–25] have not
resulted in a practical systematic methodology for identiﬁcation of fuzzy models
82
D. Filev et al.

and subsequent synthesis of fuzzy controllers For example, in [18] we examined a
family of fuzzy state models deﬁned by static rules for representing the state space
dynamics,
If u is Bs and x is Ai Then x + is Aj
If u is Bs and x is Ai Then y is Dt,
where Bs, Ai, Āj and Dt are fuzzy subsets deﬁned in the input, current /next state,
and output domain. Some of the main difﬁculties with the practical use of this type
of models remained the lack of a well-deﬁned mechanism for determining the two
families of fuzzy subsets Ai and Āj that describe the logic of state transitions, the
mappings between the input, state, and output space, and the complexity of dealing
with MIMO fuzzy systems.
In this paper we consider the FSMs of the Mamdani type (1) as a different class
of dynamic fuzzy models which can represent the state dynamics in cases where the
system states are vaguely deﬁned and are formalized as fuzzy subsets. We limit the
discussion to rule-based models that involve families of rules of the form:
If x is Ai Then x + is A1 with probability pi1
A2 with probability pi2
. . .
An with probability pin, i = 1, 2, . . . , n
f
g.
ð3Þ
The main reason for considering this kind of models is that they can be viewed
as representing an uncertain dynamic system of the type,
x + = f ðxÞ,
where x ∈X is a state variable taking values from the universe of all states X,
x + ∈X is a variable representing the next state, and A1, A2, ⋯An are fuzzy subsets
of X. The fuzzy subsets are deﬁned by their membership functions, ai x
ð Þ, on the
universe of x. The probabilities pij are the conditional probabilities satisfying
pij = P x + ∈Ajjx ∈Ai


,
∑
n
i = 1
pij = 1 for all 1 ≤j ≤n
ð
Þ,
ð4Þ
and describing the probability of transitions between the current and the next states.
The deﬁnition of the probabilities of fuzzy states A1, A2, . . . , An follows Zadeh’s
deﬁnition [26] of the probability PðFÞ of a fuzzy event F as the Lebesgue-Stieltjes
integral of the membership function μFðxÞ of F,
P F
ð Þ =
Z
X
μF x
ð ÞdP.
Developing Fuzzy State Models as Markov Chain Models …
83

We can expect that the possibility distribution inferred by the FSM (3) depends
on the transition probabilities and the fuzzy subsets A1, A2, . . . , An. One special
case of the FSM (3) is the model with no randomness:
If x is Ai, then x + is Aj,
i, j = 1, 2, . . . , n
f
g.
This model is obtained from the FSM (3) for the case when only one of the
possible transitions from the state i is 100 % certain, i.e. pij = 1 and pis = 0 for
s ∈f1, . . . , ng, s ≠j. Thus the assumption of no randomness essentially transforms
the FSM (3) into the conventional fuzzy model (2).
Further generalizations of the FSM concept (3) can include the dependence of
the transition probabilities on time and external control and disturbance inputs;
these generalizations are not addressed in the present paper, but will be pursued in
the future publications.
As an example, consider a FSM which represents the average trafﬁc conditions
on a certain road section during the day [27]. The typical trafﬁc states can be
described as fuzzy variables (Freely Flowing, Slightly Congested, Moderately
Congested, Jammed) deﬁned on the Navteq Jam Factor Scale (this is a 0–10 scale,
similar to the Richter Scale, characterizing the overall trafﬁc conditions with 0 and
10 being, respectively, the best and the worst trafﬁc conditions) See Fig. 1.
The average trafﬁc dynamics can be summarized by the set of rules that capture
all possible combinations between the states while taking into account the infor-
mation about the probabilities of transitioning between states:
If x is Ai , 1 ≤i ≤4, then x + is
A1 with probability pi1;
A2 with probability pi2;
A3 with probability pi3;
A4 with probability pi4.
0
1
2
3
4
5
6
7
8
9
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
A1        A2      A3        A4
Fig. 1 Fuzzy variables:
Freely Flowing (A1), Slightly
Congested (A2), Moderately
Congested (A3), Jammed (A4)
Trafﬁc deﬁned on the Navteq
Jam Factor scale
84
D. Filev et al.

The conditional probabilities pij. can be learned from
the normalized
sigma-counts cij [7, 9] of observed transitions between the states Ai and Aj,
pij≈cij
coi
,
over a given time interval, where
c0i = ∑
n
j = 1
cij,
is the total number of transitions that are initiated from the state Ai.
For example, a change in the trafﬁc Jam Factor from x = 7 to x + = 2 affects to
different degree the current (A1ð7Þ = 0, A2ð7Þ = 0. 04, A3ð7Þ = 0. 71, A4ð7Þ = 0. 14)
and next (A1ð2Þ = 0. 41, A2ð2Þ = 0. 25, A3ð2Þ = 0, A4ð2Þ = 0) states and changes the
sigma counts, respectively the probabilities, associated with the transitions between
the states in Fig. 1 as follows:
c11 = c11 + 0;
c12 = c12 + 0;
c13 = c13 + 0;
c14 = c14 + 0;
c21 = c21 + 0.04*0.41;
c22 = c22 + 0.04*0.25;
c23 = c23 + 0;
c24 = c24 + 0;
c31 = c31 + 0.71*0.41;
c32 = c32 + 0.71*0.25;
c33 = c33 + 0;
c34 = c34 + 0;
c41 = c41 + 0.14*0.41;
c42 = c42 + 0.14*0.25;
c43 = c43 + 0;
c44 = c44 + 0;
c01 = c01 + 0;
c02 = c02 + 0;
Models of the type (3) are dynamic models that have not been commonly used in
the literature. In what follows, we apply the Dempster aggregation rule in order to
derive a method for formalizing the FSM dynamic fuzzy model (3) that is consistent
with the theory of approximate reasoning.
In the basic fuzzy model (1), the consequent of each rule consists of a fuzzy
subset Bi. The use of a fuzzy subset implies a special kind of uncertainty associated
with the output of a rule. This kind of uncertainty is called a possibilistic uncer-
tainty, and it is a reﬂection of a lack of precision in describing the output. The use of
this imprecision allows one to represent a complex nonlinear function in terms of a
collection of simpler fuzzy rules. The consequent of the FSM (3) includes possi-
bilistic uncertainty - a collection of fuzzy subsets A1, . . . , An. In addition to the
possibilistic uncertainty, the consequent of the FSM features an additional proba-
bilistic uncertainty that is represented by the probability of selecting between
multiple consequent fuzzy subsets. A natural way to deal with both types of
Developing Fuzzy State Models as Markov Chain Models …
85

uncertainty is to consider the consequents to be fuzzy Dempster–Shafer granules
(see, for instance, [18, 27, 28]). Therefore, the output of each rule can be viewed as
a belief structure Mi with focal elements A1, . . . , An that are fuzzy subsets of the
universe X and have weights MiðAjÞ. The FSM (3) is then replaced by the following
set of rules and belief structures
If x is Ai then x + is Mi,
i = f1, 2, . . . , ng,
ð5Þ
where each of the belief structures Mi includes n focal variables that coincide with
the states Ai and are assigned weights as follows:
M1
A1
M1 A1
ð
Þ = p11
A2
M1 A2
ð
Þ = p12
. . .
An
M1 An
ð
Þ = p1n
M2
A1
M2 A1
ð
Þ = p21
A2
M2 A2
ð
Þ = p22
. . .
An
M2 An
ð
Þ = p2n
. . .
Mn
A1
Mn A1
ð
Þ = pn1
A2
Mn A2
ð
Þ = pn2
. . .
An
Mn An
ð
Þ = pnn.
We note the antecedent portion of the rules in (3) and (5) is unchanged. The
inclusion of a belief structure to model the output of a rule essentially means that
MiðAjÞ is the probability that the output of the ith rule belongs to the set Aj. So
rather than being certain as to in which set the output of the ith rule lies we
introduce some degree of randomness in the determination of the outcome set. As
we mentioned above, the use of a Dempster-Shafer belief structure to model the
consequent of a rule brings with it the option of fusing multiple types of uncertainty.
The ﬁrst type of uncertainty is the randomness associated with determining which
of the focal elements of the belief structure Mi is in effect if the rule ﬁres. This
selection is essentially determined by a random experiment that uses the corre-
sponding weights, the MiðAjÞ, as the associated probabilities.
86
D. Filev et al.

The weights MiðAjÞ correspond to the conditional probabilities deﬁned in (4). In
order to simplify the exposition we consider the case of two states, n = 2, and a
model deﬁned by the following rules and belief structures:
If x is A1 then x + is M1
If x is A2 then x + is M2
ð6Þ
where
M1
A1
M1 A1
ð
Þ = p11
A2
M1 A1
ð
Þ = p12
M2
A1
M1 A1
ð
Þ = p11
A2
M1 A2
ð
Þ = p12
and where
p11 + p12 = 1, p21 + p22 = 1.
Suppose that for a given crisp value x0 or a possibility distribution χ0ðxÞ of the
state x, the ﬁring levels of the two rules are τ1 and τ2 where
τi = aiðx0Þ, resp. τi = ⋁Xðai x
ð Þχ0ðxÞÞ.
ð8Þ
The output of each rule can now be viewed as a new belief structure bMi = τiMi
deﬁned on X. The focal elements of bMi are
Fi1 = τia1,
Fi2 = τia2,
where F is a fuzzy subset of X. The weights associated with these new focal
elements remain the same as the ones in bMi [28], i.e. bMi Fij


= MiðAjÞ. Then
following [27] we obtain the possibility distribution χ + inferred by the rules
(assuming a summation type of aggregation and a normalizing coefﬁcient q that
scales χ + to the unit interval):
χ + = 1
q
bM1 + bM2


= 1
q ðτ1M1 + τ2M2Þ
Developing Fuzzy State Models as Markov Chain Models …
87

that can be expressed for each of the focal elements as follows:
bM1
F11 = τ1a1
M1 A1
ð
Þ = p11
F12 = τ1a2
1 A2
ð
Þ = p12
bM2
F21 = τ2a1
M2 A1
ð
Þ = p21
F22 = τ2a2
M2 A2
ð
Þ = p22
We further obtain summation of these two belief structures. The focal elements
of M are obtained according to the Dempster rule as follows:
E1 = F11 + F21 = τ1a1 + τ2a1
M E1
ð
Þ = p11*p21
E2 = F11 + F22 = τ1a1 + τ2a2
M E2
ð
Þ = p11*p22
E3 = F12 + F21 = τ1a2 + τ2a1
M E3
ð
Þ = p12*p21
E4 = F12 + F22 = τ1a2 + τ2a2
M E4
ð
Þ = p12*p22
By aggregating the focal elements and taking into account (7) we obtain:
χ + = 1
q ð τ1a1 + τ2a1
ð
Þp11p21 + τ1a1 + τ2a2
ð
Þp11p22 + τ1a2 + τ2a1
ð
Þp12p21
+ τ1a2 + τ2a2
ð
Þp12p22Þ
= 1
q ðτ1a1p11 + τ2a1p21 + τ1a2p12 + τ2a2p22Þ
Apparently,
q = ∑
2
i = 1
∑
2
j = 1
τjpji = τ1 + τ2
is one possible normalizing coefﬁcient that scales χ + to the unit interval since the
terms
τjpji
q
=
τjpji
τ1 + τ2
, i, j = f1, 2g
88
D. Filev et al.

sum to one, i.e. this type of normalization is equivalent to a weighted aggre-
gation of the focal elements.
The extension of this analytical expression for the possibility distribution
inferred by a FSM for n > 2 is straightforward:
χ + = 1
q
∑
n
i = 1
ai ∑
n
i = 1
τjpji


, q = ∑
n
i = 1
τj.
Alternatively, the above inferred possibility distribution can be formalized using
an equivalent vector/matrix expression:
χ + =
τΠa
∑n
i = 1 τi
= τΠa
τΠe = τΠa,
ð9Þ
where a = a1 x
ð Þa12 x
ð Þ . . . an x
ð Þ
½
T is a matrix of the uniformly sampled membership
functions of the subsets A1, A2, ⋯, and An; τ is a row vector of the ﬁring levels,
τ = τ1τ2 . . . τn
½
,
τ =
τ
∑n
i = 1 τi
,
is the vector of the normalized ﬁring levels, Π is the matrix formed by the con-
ditional probabilities pij, and e is the column vector of ones of size n.
3
Markov Chain Models with Fuzzy Encoding and Fuzzy
State Models Are Equivalent Concepts
Markov Chain models with fuzzy encoding were introduced in [7], motivated by
the approximation properties of fuzzy granules. Following the theory of approxi-
mate reasoning [18], we considered the partitioning the universes of x and x + into
n fuzzy subsets, Aj. Subsets Aj are deﬁned by their membership functions,
aj x
ð Þ: X →0, 1
½
; ∀x ∈X, ∃j, 1 ≤j ≤n, aj x
ð Þ ≠0.
As we now discuss, the FSMs that use a set of appropriately deﬁned rules and
belief structures are similar to Markov Chain models with states being deﬁned as
fuzzy subsets See Fig. 2.
More speciﬁcally, the Markov Chain models with fuzzy granulation [29] can be
expressed as a collection of n2 rules with fuzzy predicates of the form,
Developing Fuzzy State Models as Markov Chain Models …
89

IF x is Ai Then x + is Aj with probability pij
where probabilities pij, i, j ∈1, ⋯n
f
g, , are the elements of the transition probability
matrix, Π. The antecedent and consequent subsets are deﬁned by their membership
functions a x
ð Þ on the universe of x. Note that each of the cells of the Markov Chain
Model with fuzzy encoding corresponds to a rule expressing the relationship
between the possible antecedents and consequents. In [9] we showed that when the
current state of the Markov Chain with fuzzy encoding is deterministic, the
expressions for the next state (fuzzy or deterministic) are given by:
χ + x
ð Þ = ∑n
i = 1 ai x0
ð
Þ ∑n
i = 1 pij ∑n
i = 1 aj x
ð Þ
∑n
i = 1 ai x0
ð
Þ
,
ð10Þ
x +
0 = ∑n
i = 1 ai x0
ð
Þ ∑n
i = 1 pijx̄j
∑n
i = 1 ai x0
ð
Þ
.
ð11Þ
Taking into account that aiðx0Þ essentially corresponds to the degree of ﬁring τi
of the rule with predicate aiðxÞ by (8) we can rewrite (10) into a vector/matrix form:
χ + =
τΠa
∑n
i = 1 τj
= τΠa
τΠe = τΠa,
that is identical to (9). Furthermore, the deterministic output of the Markov Chain
with fuzzy encoding (11) corresponds to the defuzziﬁed valued of the fuzzy state
model (9).
We summarize this important result in the following proposition.
Proposition: The Fuzzy State Model and the Markov Chain model with fuzzy
encoding are equivalent models.
In [9, 30] we showed that for Markov Chain models with fuzzy encoding there
exists an analytical expression of the mapping between a given possibility distri-
bution χ0ðxÞ and the inferred possibility distribution, χ + :
Fig. 2 Markov Chain Model
with fuzzy granulation; the
states are fuzzy subsets [9]
90
D. Filev et al.

χ + = αTΠa = χaTΠa
χaT
0
,
ð12Þ
where the membership function of χ0ðxÞ is represented by a row vector χ and
a0 = ∑n
i = 1 ai. This result was obtained under the following assumptions:
A1. Uniformly sampled (discrete) membership functions aj and possibility dis-
tributions χ and χ + yielding s –dimensional vectors aj = ½a1ja2j . . . asj, χ and χ + .
A2. Use of a correlation type measure
χaT = χ½aT
1aT
2 . . . aT
n,
of compatibility between the vectors χ and aT
j instead of the more conventional
max-product or max-min type similarity measures.
The assumption (A1) simpliﬁes the exposition and can be easily relaxed, for
instance, by replacing the inner product of possibilistic vectors by an integral of a
product of two possibilistic distribution functions over a domain. The assumption
(A2), on the correlation measure, is more critical, but it is reasonable in treating
many application problems. In addition to replacing the nonlinear maximum
operation by a linear inner product operation, this correlation measure may in many
problems provide a more complete characterization of the overall similarity
between the vectors χ and aT
j
(see, for instance, [18, 25]). Under the above
assumptions, the degrees of ﬁring τ and their normalized counterparts τ̄ in (8) can be
expressed as follows:
τ = τ1 τ2 . . . τn
½
 = χ̄½āT
1 āT
2 . . . āT
n = χ̄ āT
τ̄ =
τ
∑n
i = 1 τi
=
χ̄ āT
∑n
i = 1 χ̄ āT = χ̄ āT
χ̄ āT
o
where āo = ∑n
i = 1 āi. Henceforth, under those assumptions expression (9) can be
rewritten in a vector form:
χ̄ + = τ̄ Π ā = χ̄āTΠ ā
χ̄āT
o
ð13Þ
that is identical to (13).
Therefore, the FSM and the MC with fuzzy encoding are also equivalent under
the assumptions A1 and A2. Consequently, the calculus and conclusions related to
the latter can be applied to the former. The following ﬁgure summarizes the result
of this section (Fig. 3).
Developing Fuzzy State Models as Markov Chain Models …
91

4
Generalized Dynamic Fuzzy Models
The demonstrated equivalency between Markov models with fuzzy encoding and
the FSMs has thus far covered only one special class of ﬁrst order dynamic systems,
x k + 1
ð
Þ = f x k
ð Þ
ð
Þ,
where the scalar variable x denotes the state of the system and f is a nonlinear
mapping. We now discuss an extension to systems with multiple state variables (x is
an n-dimensional vector rather than a scalar).
First, we show that higher order dynamic fuzzy models, e.g.:
If x1 is A1
i and x2 is A2
j Then x +
1 is A1
s and x +
2 is A2
t ,
ð14Þ
where n1 subsets A1
i and n2 subsets A2
j partition the universes of the state variables
x1 and x2, and are deﬁned by the respective membership functions,
a1
i ðx1Þ: X1 →½0, 1; ∀x1 ∈X1, ∃i, 1 ≤i, s ≤n1, a1
i ðx1Þ ≠0,
a2
j ðx2Þ: X2 →½0, 1; ∀x2 ∈X2, ∃j, 1 ≤j, t ≤n2, a j
j ðx2Þ ≠0,
can be transformed to the ﬁrst order model (9). By aggregating the state variables x1
and x2, and corresponding subsets A1
i , A2
i , and substituting in (12) we obtain the
following rule,
If z is Ci Then z + is Cj,
ð15Þ
where the new variable z is deﬁned in the two-dimensional Cartesian space X1 × X2.
Ci and Cj are fuzzy subsets (granules) of X1 × X2. Numerically, they are represented
x
x
x+
p11       p12
p13
p21
p22
p23
p31
p32
p33
Α3            Α2           Α1
Α1            Α2           Α3
If x is Ai then x+ is Aj with probability pij,
i, j = 1, 2, 3. 
Fig. 3 Equivalence between Fuzzy State Models and Markov Chain models with fuzzy encoding
92
D. Filev et al.

by the vector products of the discretized membership functions ā1
i1 and ā2
j1 of the
subsets, A1
i1 and A2
j1,
Ci = ā1T
i1 a2
j1.
ð16Þ
By rearranging the elements of Ci in a vector ci we obtain a vector representation
for the membership function of the subset Ci that is formally identical to the single
dimensional case and satisfying assumption A1 and A2. We will further denote by
the symbol × the combined operation of vector product followed by a transfor-
mation to a single dimensional vector, i.e.:
c̄i = ā1
i1 × a2
j1,
ð17Þ
where c̄i stands for the vector expression of the membership function of the subset
Ci. Therefore, if the granules are considered Markov states and they satisfy the
Markov assumption the system can be modeled as a Markov chain with fuzzy
encoding:
If z is Ci Then z + is Cj with probability pij.
ð18Þ
The probabilities, pij, i, j = 1, ⋯, n, where n = n1n2, are the elements of the
transition probability matrix Π covering the transitions between the subsets, Ci.
pij = P z + ∈Cjjz ∈Ci


, ∑n
j = 1 pij = 1. Consequently, expression (11) applies and
determines the possibility distribution inferred by the dynamic fuzzy model (14).
Similarly, the Markov chain theory can also be extended to the case of fuzzy
modeling of stochastic dynamic multiple-state, multiple-input systems:
xðk + 1Þ = f xðkÞ, uðkÞ
ð
Þ,
ð19Þ
where x and u are of dimensions n and r, respectively. In order to simplify the
notations we consider dynamic fuzzy models of the type:
If u1 is B1
l and u2 is B2
m and x1 is Al
i and x2 is A2
j
Then x +
1 is A1
s and x +
2 is A2
t ,
ð20Þ
where u1 ∈U1 and u2 ∈U2 and B1
l and B2
m are fuzzy subsets of U1 and U2 with
cardinalities r1 and r2. Based on the discussion about higher order state model we
can assume that the state variables are granulated as in (12). Similar aggregation of
the input variables u1 and u2 into a new variable, w, and corresponding subsets B1
l
and B2
m into a new subset Ds yields:
Developing Fuzzy State Models as Markov Chain Models …
93

If w is Ds and z is Ci Then z + is Cj
ð21Þ
Alternatively, this rule can be rewritten in the form:
If w is Ds Then
If z is Ci Then z + is Cj.
ð22Þ
Such a granulation of the inputs and states deﬁnes a partitioning of the com-
pound Cartesian input/state space into rn fuzzy granules, where r = r1r2. Assuming
further that the granules Ci satisfy the Markov assumption, the system can be
modeled as a Markov chain with fuzzy encoding
If w is Ds Then
If z is Ci Then z + is Cj with probability p sð Þ
ij
ð23Þ
where the transition probabilities pðsÞ
ij
and corresponding transition probability
matrices ΠðsÞ the are deﬁned as the following condition probabilities:
pðsÞ
ij = P z + ∈Cjjz ∈Ci


, w ∈Ds, s = 1, 2, . . . , r; i, j = 1, 2, . . . n.
ð24Þ
In order to include the impact of the input w on the inferred possibility distri-
bution by (14) we aggregate the corresponding possibility distributions that are
inferred under different degrees of membership of the input w to the subsets
Ds, s = 1, 2, . . . , r, i.e.,
χ̄ + = ∑
r
s = 1
νs
∑r
t = 1 νt
τ̄ ΠðsÞ ā = ∑
r
s = 1
νs
∑r
t = 1 νt
χ̄āTΠðsÞ ā
χ̄āT
o
ð25Þ
where νs
is the degree of membership of the input w in the subsets
Ds, s = 1, 2, . . . , r.
We can visualize the transition probabilities pðsÞ
ij
as r transition probability
matrices ΠðsÞ of size n × n
ð
Þ that are associated with the corresponding subsets Ds
proportionally to the degrees of membership of the input w in Ds, s = 1, 2, . . . , r.
Figure 2 illustrates the transition probabilities for a system with 2 input and 2
state variables that are deﬁned on continuous universes (Fig. 4).
94
D. Filev et al.

5
Learning Generalized Dynamic Fuzzy Models
from Data
In the fuzzy models that were discussed above we considered the case of a second
order system with 2 inputs. The formal extension to the multiple input systems of a
higher order is straightforward and is omitted.
The method and algorithm for learning the transition probabilities for Markov
Chains with fuzzy encoding were discussed in detail in [9]. Leveraging the
equivalence between both models and by applying the learning algorithm from [9]
we get for the transition probability matrix in (18),
ΠðkÞ = diagðF0ðkÞÞ −1FðkÞ,
ð26Þ
where
FðkÞ = Fðk −1Þ + βðτðkÞγðkÞT −Fðk −1ÞÞ,
ð27Þ
FoðkÞ = Foðk −1Þ + β ðτðkÞγðkÞT1M −Foðk −1ÞÞ,
ð28Þ
β is the learning rate, and τðkÞ and γðkÞ are the vectors of membership of the
aggregated vectors z = z k
ð Þ and z + = z k + 1
ð
Þ in the subsets Ci and Cj. The algo-
rithm is initialized as follows:
Fig. 4 Example of a vehicle model with 2 state variables (x1 and x2) and 2 input variables (u1 and u2)
that are deﬁned on continuous universes, e.g. speed and acceleration can be thought as the state
variables and the accelerator and brake pedal positions can be thought of as the inputs. The inputs are
partitioned into 2 and 3 intervals respectively deﬁning 6 input granules, Ds. The ranges of possible
values of the state variables are partitioned into 6 and 3 fuzzy subsets, respectively, deﬁning 18 fuzzy
subsets, Ci. For different input conditions and states, the transition probabilities are described by 6
transition probability matrices ΠðsÞ with the elements, psij = P z + ∈Cjjz ∈Ci, w ∈Ds


, i, j ∈
1, 2, ⋯, 18
f
g, s ∈1, 2, ⋯, 6
f
g
Developing Fuzzy State Models as Markov Chain Models …
95

Fð0Þ = ϵ E; Foð0Þ = Fð0Þ 1M
ð29Þ
with E being a matrix of compatible size and unit elements, and ϵ being a small
nonnegative constant introduced to avoid singularity.
As follows from (24), for the case of multiple inputs, the number of transition
probability matrices corresponds to the number of fuzzy subsets Ds, s = 1, 2, . . . , r
of the aggregated input variables. Apparently, all transition probability matrices are
created in the same way since they summarize the transitions between the states.
However, since the input vector can belong to each of the subsets Ds with a
different degree of membership νs we use this membership to weigh the contri-
butions of corresponding transitions.
Assuming a set of observations (w(k), z(k)), k = 1, 2, …, K and fuzzy subsets
Ds, s = 1, 2, . . . , r for w, then for each of the r transition probability matrices we get
ΠsðkÞ = diagðFs0ðkÞÞ −1FsðkÞ,
ð30Þ
where
FsðkÞ = Fsðk −1Þ + βðνsðkÞτðkÞγðkÞT −Fsðk −1ÞÞ,
ð31Þ
FsoðkÞ = Fsoðk −1Þ + βðνsðkÞτðkÞγðkÞT1M −Fsoðk −1ÞÞ,
ð32Þ
β is the learning rate,τðkÞ and γðkÞ are the vectors of membership of the aggregated
vectors z = z k
ð Þ and z + = zðk + 1Þ in the subsets Ci and Cj, and νs is the degree of
membership of the input w in the subsets Ds, s = 1, 2, . . . , r. The algorithm is
initialized as follows:
Fsð0Þ = ϵ E; Fsoð0Þ = Fsð0Þ 1M
ð33Þ
6
Case Study
We consider an example of adaptive cruise control of a vehicle following another
vehicle in trafﬁc. The model is given by
d t + 1
ð
Þ = d tð Þ + ΔTv tð Þ,
v t + 1
ð
Þ = v tð Þ + ΔT u tð Þ −w tð Þ
ð
Þ,
ð34Þ
where ΔT = 0.25 s is the sampling period, d tð Þ = ρ tð Þ −ρnom is the deviation of the
relative distance, ρ tð Þ, from the nominal safe following distance ρnom (for sim-
plicity, we refer to d tð Þ as the relative distance), v tð Þ is the relative speed, w tð Þ is the
acceleration of the lead vehicle in trafﬁc, and uðtÞ is the acceleration of the follower
vehicle that hosts the control algorithm.
96
D. Filev et al.

Our objective is to develop a model-based control algorithm for the host vehicle
acceleration, u tð Þ, that maintains the relative distance and vehicle following. As a
ﬁrst step, we deﬁne a fuzzy state model of the two vehicles. We model the two
states (the relative distance to a vehicle in trafﬁc, dðtÞ, and the relative speed, vðtÞ)
and the control input (the host vehicle acceleration, u tð Þ) by partitioning their
ranges into 3 fuzzy subsets. The rules derived from this partitioning are of the form,
If u is Ds and v is A1
i and s is A2
j
Then v + is A1
k and s + is A2
l , , i, j, k, l ∈f1, 2, 3g
where Ds stands for the fuzzy subsets Negative, Zero, and Positive of the Host
Vehicle Acceleration u, A1
i and A1
k stand for the fuzzy subsets Negative, Zero, and
Positive of the Relative velocity v, and A2
j and A2
t stand for the fuzzy subsets
Negative, Zero,
and Positive of the Relative Distance d. The fuzzy subsets are
modeled by the membership functions depicted on Fig. 5. A discrete set of control
actions, representing the follower vehicle accelerations,
u ∈U = f −0.5, 0, 0.5g
was considered.
Following the discussion in the previous section we introduce the Cartesian
product subsets of the state variables,
Ct = A1
i and A2
j , i, j ∈1, 2, 3
f
g, 1 ≤t ≤9,
representing the aggregated states v and s. For example, the fuzzy subset C2 = A1
1
and A2
2 represents the following conjunction,
-4
-2
0
2
4
0
0.5
1
Relative Velocity [m/s]
Membership Grade
-10
-5
0
5
10
0
0.5
1
Relative Distance [m]
Membership Grade
Fig. 5 Membership functions
of Relative Velocity and
Relative Distance
Developing Fuzzy State Models as Markov Chain Models …
97

Relative Speed is Negative and Relative Distance is Zero.
Therefore, the dynamic fuzzy model describing the relative distance and velocity
of the two vehicles can be written as a set of the following 243 rules:
If u is Ds Then
If z is Ci Then z + is Cj with probaility p sð Þ
ij ,
ð35Þ
where the transition probabilities pðsÞ
ij
and corresponding transition probability
matrices ΠðsÞ the are deﬁned as the following conditional probabilities:
pðsÞ
ij = Pðz + ∈Cjjz ∈Ci, u ∈DsÞ, s ∈f1, 2, 3g, i, j ∈f1, 2, ⋯, 9g
To determine the transition probabilities, ﬁrst a speed trajectory of the lead
vehicle was deﬁned varying between 20 and 30 m/s based on a Markov Chain with
−1, 0 and 1 m/s vehicle speed change per time step with respective probabilities of
0.3, 0.4 0.3 except for the lower (upper) boundaries, where staying at the same
600
800
1000
1200
-5
0
5
Time (sec)
Relative Speed [m/sec]
600
800
1000
1200
0
50
100
150
Time (sec)
Relative Distance [m]
Fig. 6 Sample trajectories of
the Relative Speed and
Relative Distance
98
D. Filev et al.

value or transitioning to upper (lower) value was permitted with respective prob-
abilities of 0.5. Then model (34) was simulated for three different values of
u ∈U =
−0.5, 0, 0.5
f
g. To avoid values outside of the range of interest, the relative
distance state was saturated between −20 and 150 m while the relative velocity state
was saturated between −5 and 5 m/s. The transition probability matrices were
learned from the simulated trajectories of lead and host vehicle acceleration, relative
distance, and relative speed (selected sections of state trajectories for u = 0. 5 are
shown in Fig. 6) by applying the learning algorithm (30)–(33). These transition
probability matrices are visualized in Fig. 7.
The resulting rules reﬂect the car following dynamics. As an example, noting
that
C7 = A1
3 and A2
1 =
Relative Speed is Positive and Relative Distance is Negative,
C8 = A2
3 and A2
2 =
Relative Speed is Positive and Relative Distance is Zero
0
2
4
6
8
10
0
5
10
0
0.2
0.4
0.6
0.8
1
Current State z
Next State z+
pij(1)
0
2
4
6
8
10
0
5
10
0
0.2
0.4
0.6
0.8
1
Current State z
Next State z+
pij(2)
0
2
4
6
8
10
0
5
10
0
0.2
0.4
0.6
0.8
1
Current State z
Next State z+
pij(3)
Fig. 7 Transition Probability Matrices, ΠðsÞ, s = 1, 2, 3
Developing Fuzzy State Models as Markov Chain Models …
99

the rule
If u is D1 Then
If z is C7 Then z + is C8 with probability pð1Þ
78 ,
is equivalent to the following rule
If Host Vehicle Acceleration is Negative Then
If Relative Speed is Positive and Relative Distance is Negative Then
Next Relative Speed is Positive and
Relative Distance is Zero with probability 0. 41
or
If Host Vehicle Acceleration is Negative and Relative Speed is Positive
Relative Distance is Negative Then
Next Relative Speed is Positive and Next Relative Distance is Zero
with probability 0. 41.
Low rule probability indicates low weight (impact) of a speciﬁc rule and allows
it to be eliminated from the model. In this example 71 rules with probabilities less
than 0.01 were eliminated without affecting the model performance.
In the next step we use the model (35) to design a fuzzy controller that maintains
the relative distance and speed. We apply Stochastic Dynamic Programming (SDP) to
determine the control actions corresponding to the aggregated fuzzy subsets, Ct,
t ∈f1, 2, ⋯, 9g. The penalty function, L Ct
ð
Þ, corresponding to the fuzzy subsets, Ct,
t ∈f1, 2, ⋯, 9g, was chosen to discourage subsets that are further away from the
origin, i.e. away from the state of v = 0, d = 0. We assign lower penalty to the
“self-correcting states” corresponding to relative distance and velocity with opposite
signs. This leads to the choice of penalty for individual states presented in Table 1.
To generate the control policy, the Value Iteration Algorithm was applied to the
Markov Chain model with fuzzy encoding with transition probability matrices
visualized in Fig. 7 and the penalty function deﬁned in Table 1. The value iterations
take the following form,
Table 1 Penalty selection for
different states of relative
distance and velocity
d/v
Negative
Zero
Positive
Negative
1000
500
0
Zero
200
0
200
Positive
0
500
1000
100
D. Filev et al.

Vn C
ð Þ = minu ∈UfL C
ð Þ + qE½Vn −1 C +
ð
Þg,
with discount factor q = 0.90. Figure 8 shows the value iteration convergence.
The optimal control values for each of the fuzzy subsets Ct, t ∈f1, 2, ⋯, 9g, we
determined from SDP are summarized in Table 2.
Thus the optimal controller comprises the following rule base:
If Relative Speed is Negative and Relative Distance is Negative then u = −0.50
If Relative Speed is Negative and Relative Distance is Zero then u = −0.50
If Relative Speed is Negative and Relative Distance is Positive then u = 0
If Relative Speed is Zero and Relative Distance is Negative then u = −0.50
If Relative Speed is Zero and Relative Distance is Zero then u = 0
If Relative Speed is Zero and Relative Distance is Positive then u = 0.50
If Relative Speed is Positive and Relative Distance is Negative then u = 0
If Relative Speed is Positive and Relative Distance is Zero then u = 0.50
If Relative Speed is Positive and Relative Distance is Positive then u = 0.50
0
20
40
60
80
100
0
2000
4000
6000
8000
10000
Iteration No
max|Vn-Vn-1|
Fig. 8 Maximum difference
between value functions in
two subsequent iterations over
the domain
Table 2 Consequent centroids corresponding to the different states of relative distance and
velocity as calculated from the optimal policy derived by the application of the Value Iteration
Algorithm
d/v
Negative
Zero
Positive
Negative
–0.5
–0.5
0
Zero
–0.5
0
0.5
Positive
0
0.5
0.5
Developing Fuzzy State Models as Markov Chain Models …
101

By using the Simpliﬁed Reasoning Method [18] (weighed average aggregation
of the centers of gravity of rule consequents), the controller rule base produces a
control policy, uSDP v, d
ð
Þ, as a function of the relative speed, v, and relative dis-
tance, d shown in Fig. 9.
-20
0
20
-5
0
5
-0.5
0
0.5
d [m]
v [m/sec]
u
Fig. 9 Control surface
derived from the optimal
controller rule-base under the
Simpliﬁed Reasoning Method
0
1000
2000
3000
4000
-3
-2
-1
0
1
2
3
Time (sec)
Relative Speed [m/sec]
0
1000
2000
3000
4000
-10
-5
0
5
10
Time (sec)
Relative Distance [m]
Fig. 10 Relative Speed
(top) and Relative Distance
(bottom) under the application
of the fuzzy controller with
consequents calculated by the
SDP algorithm when
following a randomly
accelerating vehicle with
acceleration varying between
−1 and 1 m/s2
102
D. Filev et al.

Figure 10 illustrates that the host vehicle is able to maintain relative speed and
relative distance in a small range while following a randomly accelerating lead
vehicle with acceleration magnitude between −1 and 1 m/s2 exceeding the follower
vehicle acceleration authority.
Figure 11 illustrates the closed-loop response when the lead vehicle is moving at
a constant speed, i.e., w = 0. The closed-loop trajectories converge so that v tð Þ →0
and d tð Þ →0 m as t →∞, where the equilibrium values of the speed and distance
are the roots of uSDP v, d
ð
Þ = 0. The response is lightly damped but asymptotically
stable. By linearizing the model (34) with u = uSDP v, d
ð
Þ and w = 0 numerically, the
closed-loop eigenvalues are 0.9928 ± 0.0740j and are inside the unit disk, con-
ﬁrming that the closed-loop system is asymptotically stable.
7
Summary and Conclusions
In this paper we analyzed the equivalency between the Fuzzy State Models and the
Markov Chains with fuzzy encoding and demonstrated that these approaches are
identical under certain assumptions. This allowed us to analytically describe the
0
20
40
60
80
100
120
-3
-2
-1
0
1
Time (sec)
Relative Speed [m/sec]
0
20
40
60
80
100
120
-10
0
10
20
30
40
50
Time (sec)
Relative Distance [m]
Fig. 11 Relative Speed
(top) and Relative Distance
(bottom) under the application
of the fuzzy controller with
consequents calculated by the
SDP algorithm when
following a lead vehicle
moving at constant speed
Developing Fuzzy State Models as Markov Chain Models …
103

propagation of possibility distribution by a dynamic feedback fuzzy system and to
derive an analytical model of the possibility distribution inferred by a Fuzzy State
Model. We also showed that the methodology for developing and learning Markov
Chain models with fuzzy encoding can be extended to FSMs of higher order and
multiple inputs. Results were illustrated on a case study where a FSM for vehicle
following dynamics has been learned from sample trajectory data and Stochastic
Dynamic Programming (SDP) was applied to generate a fuzzy controller stabilizing
the relative speed and distance between two vehicles. We believe that these
developments will lead to a framework for systematically addressing the problems
of model-based design, stability, and optimal control of fuzzy systems.
References
1. Mamdani, E.H.: Applications of fuzzy set theory to control systems: a survey. In: Gupta, M.M.,
Saridis, G.N., Gaines, B.R. (eds.) Fuzzy Automata and Decision Processes, pp. 77–88.
North-Holland: Amsterdam (1977)
2. Mamdani, E., Assilian, S.: An experiment in linguistic synthesis with a fuzzy logic controller.
Int. J. Man Mach. Stud. 7, 1–13 (1975)
3. Takagi, T., Sugeno, M.: Fuzzy identiﬁcation of systems and its application to modeling and
control. IEEE Trans. Syst. Man Cybern. 15, 116–132 (1985)
4. Filev, D.: Fuzzy modeling of complex systems. Int. J. Approx. Reason. 5(3), 281–290 (1991)
5. Gain scheduling based control of a class of TSK systems. In: Farinwata, S., Filev, D., Langari,
R. (eds.) Fuzzy Control: Synthesis and Analysis, pp. 321–334. Wiley, London (2000)
6. Tanaka, K., Wang, H.: Fuzzy Control Systems Design and Analysis: A Linear Matrix
Inequality Approach. Wiley, London (2004)
7. Filev, D., Kolmanovsky, I.V.: Markov chain modelling approaches for on-board applications.
In: Proceedings of 2010 American Control Conference, pp. 4139–4145 (2010)
8. McDonough, K., Kolmanovsky, I.V., Filev, D., Szwabowski, S., Yanakiev, D., Michelini, J.:
Stochastic fuel efﬁcient optimal control of vehicle speed. In: Del Rey, L. et al. (eds.) Lecture
Notes in Control and Information Sciences, vol. 455, pp. 147–162. Springer International
Publishing Switzerland (2014)
9. Filev, D., Kolmanovsky, I.V.: Generalized Markov Models for real time modeling of
continuous systems. IEEE Trans. Fuzzy Syst. 22(4), 983–998 (2014)
10. Bellman, R., Zadeh, L.: Decision making in a fuzzy environment. Manage. Sci. 17,
B-141–B-164 (1970)
11. Kolmanovsky, I.V., Siverguina, I., Lygoe, B.: Optimization of powertrain operation policy for
feasibility assessment and calibration: Stochastic Dynamic Programming approach. In:
Proceedings of American Control Conference, Anchorage, Alaska, pp. 1425–1430 (2002)
12. Di Cairano, S., Bernardini, D., Bemporad, A., Kolmanovsky, I.V.: Stochastic MPC with
learning for driver-predictive vehicle control and its application to HEV energy management.
IEEE Trans. Control Syst. Technol. 22(3), 1018–1031 (2014)
13. Bemporad, A., Di Cairano, S.: Model-Predictive Control of discrete hybrid stochastic
automata. IEEE Trans. Autom. Control 56(6), 1307–1321 (2011)
14. Kolmanovsky, I.V., Filev, D.: Stochastic optimal control of systems with soft constraints and
opportunities for automotive applications. In: Proceedings of IEEE Multi-Conference on
Systems and Control, St. Petersburg, Russia, pp. 1265–1270 (2009)
15. Shafer, G.: A Mathematical Theory of Evidence. Princeton University Press, Princeton (1976)
16. Yager, R.R., Liu, L.: (Dempster, A.P., Shafer, G. Advisory Editors) Classic Works of the
Dempster-Shafer Theory of Belief Functions. Springer: Heidelberg (2008)
104
D. Filev et al.

17. Dynkin, E.B., Yushkevich, A.A.: Markov Processes: Theorems and Problems. Nauka,
Moscow, in Russian. English translation published by Plenum, New York (1969)
18. Yager, R.R., Filev, D.: Essentials of Fuzzy Modeling and Control. Wiley, New York (1994)
19. Athans, M.: Presentation at the 1998 IEEE Conference on Decision and Control debate
between Professors LotﬁZadeh and Michael Athans on fuzzy versus conventional control,
Tampa, FL (1998)
20. Kloeden, P.E.: Fuzzy dynamical systems. Fuzzy Sets Syst. 7, 275–296 (1982)
21. Filev, D., Ying, H.: The look-up table controllers and a particular class of Mamdani fuzzy
controllers are equivalent—implications to real-world applications. In: Proceeding of
Joint IFSA World Congress & NAFIPS Annual Meeting, Alberta, Canada, pp. 902–907 (2013)
22. Pedrycz, W.: Fuzzy Control and Fuzzy Systems. Research Studies Press (1993)
23. Wang, L.: A Course in Fuzzy systems and Control. Prentice-Hall International, Inc
24. Passino, K.M., Yurkovich, S.: Fuzzy Control. Addison Wesley Longman, Inc. (1998)
25. Kosko, B.: Fuzzy Engineering. Prentice-Hall, Inc., Upper Saddle River (1996)
26. Zadeh, L.: Probability Measures of Fuzzy Events. J. Math. Anal. Appl. 23, 421–427 (1968)
27. Yager, R., Filev, D.: Using Dempster-Shafer structures to provide probabilistic outputs in
fuzzy systems modeling. In: Trillas, E., et al. (eds.) Combining Experimentation and Theory,
STUDFUZZ 271, pp. 301–327. Springer, Berlin (2012)
28. Yager, R.R.: On the Dempster-Shafer framework and new combination rules. Inf. Sci. 41,
93–137 (1987)
29. Pedrycz, W., Skowron, A., Kreinovich, V.: (eds.) Handbook of Granular Computing. Wiley
(2008)
30. Filev, D., Kolmanovsky, I.V. Yager, R.: On the relationship between fuzzy state models and
Markov chain models with fuzzy encoding. In: Proceeding of IEEE World Conference on Soft
Computing, San Francisco, May 2011
Authors Biography
Dr. Dimitar P. Filev is a Executive Technical Leader -
Intelligent Control & Information Systems, Ford Research &
Innovation Center in Dearborn, Michigan. He is conducting
research in intelligent control, fuzzy and neural systems, and
their applications to automotive engineering. He has published 4
books and over 200 articles in refereed journals and conference
proceedings, and holds numerous US and foreign patents. He is
the recipient of the 2008 Norbert Wiener Award of the IEEE
Systems, Man, & Cybernetics Society, the 2007 International
Fuzzy Systems Association (IFSA) Outstanding Industrial
Applications Award, and the 2015 IEEE Computational Intel-
ligence Society Pioneer award in Fuzzy Systems. He received his
PhD. degree in Electrical Engineering from the Czech Technical
University in Prague in 1979. Dr. Filev is a Fellow of IEEE and
IFSA.
Developing Fuzzy State Models as Markov Chain Models …
105

Ilya Kolmanovsky has received his M.S. and Ph.D. degrees in
aerospace engineering, and the M.A. degree in mathematics from
the University of Michigan, Ann Arbor, in 1993, 1995, and
1995, respectively. He is presently a professor in the department
of aerospace engineering at the University of Michigan, with
research interests in control theory for systems with state and
control constraints, and in control applications to aerospace and
automotive systems. Dr. Kolmanovsky has previously been with
Ford Research and Advanced Engineering in Dearborn, Mich-
igan, for close to 15 years. He is a Fellow of IEEE, a past
recipient of the Donald P. Eckman Award of American
Automatic Control Council, and of IEEE Transactions on
Control Systems Technology Outstanding Paper Award.
Ronald R. Yager is Director of the Machine Intelligence
Institute and Professor of Information Systems at Iona College.
He is editor and chief of the International Journal of Intelligent
Systems. He has published over 500 papers and edited over 30
books in areas related to fuzzy sets, human behavioral modeling,
decision-making under uncertainty and the fusion of informa-
tion. He is among the world’s top 1% most highly cited
researchers with over 40,000 citations in Google Scholar. He
was the recipient of the IEEE Computational Intelligence Society
Pioneer award in Fuzzy Systems. He received the special
honorary medal of the 50-th Anniversary of the Polish Academy
of Sciences. He received the Lifetime Outstanding Achievement
Award from International the Fuzzy Systems Association. He
recently received honorary doctorate degrees, honoris causa,
from the Azerbaijan Technical University and the State Univer-
sity of Information Technologies, Soﬁa Bulgaria. Dr. Yager is a fellow of the IEEE, the New York
Academy of Sciences and the Fuzzy Systems Association. He has served at the National Science
Foundation as program director in the Information Sciences program. He was a NASA/Stanford
visiting fellow and a research associate at the University of California, Berkeley. He has been a
lecturer at NATO Advanced Study Institutes. He is a visiting distinguished scientist at King Saud
University, Riyadh Saudi Arabia. He received his undergraduate degree from the City College of
New York and his Ph. D. from the Polytechnic Institute New York University.
106
D. Filev et al.

Incremental Granular Fuzzy Modeling
Using Imprecise Data Streams
Daniel Leite and Fernando Gomide
Abstract System modeling in dynamic environments needs processing of streams
of sensor data and incremental learning algorithms. This paper suggests an incre-
mental granular fuzzy rule-based modeling approach using streams of fuzzy inter-
val data. Incremental granular modeling is an adaptive modeling framework that uses
fuzzy granular data that originate from unreliable sensors, imprecise perceptions, or
description of imprecise values of a variable in the form fuzzy intervals. The incre-
mental learning algorithm builds the antecedent of functional fuzzy rules and the
rule base of the fuzzy model. A recursive least squares algorithm revises the para-
meters of a state-space representation of the fuzzy rule consequents. Imprecision in
data is accounted for using speciﬁcity measures. An illustrative example concerning
the Rossler attractor is given.
1 Introduction
Data produced by real world systems result from nonlinear, uncertain, and time-
varying dynamic processes. The description of the underlying dynamical behavior
using data models derived from ﬁrst-principles remains unrealistic. Data-driven ori-
entation is becoming increasingly important as a key to complement ﬁrst-principles
orientation. Modeling from data streams requires adaptive adjustment of models to
the dynamic variation of the data. Stream-based modeling algorithms need to be
developed with emphasis on the evolution of the data. The modeling process should
account for data distribution drifts and shifts triggered by the dynamics and the con-
text of the data. Because the volume of data increases continuously, it is not feasible
D. Leite (✉)
Federal University of Lavras, Department of Engineering,
Control and Automation Research Group, Minas Gerais, Brazil
e-mail: daniel.leite@deg.uﬂa.br
F. Gomide
University of Campinas, School of Electrical and Computer Engineering,
Sao Paulo, Brazil
e-mail: gomide@dca.fee.unicamp.br
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_7
107

108
D. Leite and F. Gomide
to process the data eﬃciently using multiple passes. Typically learning procedures
must be designed to operate with one pass of the data.
Granular data emerge as a consequence of the concepts of indistinguishability,
similarity, proximity and functionality [1]. Data granulation can be viewed as a form
of lossy data compression in an environment of imprecision. In many cases, data
streams contain more information than is needed for a particular purpose [2, 3]. For
example, in practice, measurements do not contain more details than the sensors can
distinguish.
A granular mapping is deﬁned on information granules and a quotient structure.
Mapping of granular data consists in associating a set of granules expressed in some
input space to another set of granules draw in an output space. Granular mappings
are frequently encountered in rule-based systems, where the mapping is given by
If-Then type of statements. Computing with granules emphasizes multiple levels of
understanding, analyzing and representing information. Fuzzy granular computing
[4–6] hypothesizes that accepting some level of imprecision may be beneﬁcial and
therefore suggests a balance between precision and uncertainty.
Linguistic and functional rule-based systems are widely known types of fuzzy
systems, which emerged years ago from studies in linguistic modeling and control
systems. Both systems share the same rule antecedent structure, but diﬀer in the way
the consequents are formed. Linguistic fuzzy rules use fuzzy set-based consequents
whereas functional fuzzy rules use functions of the antecedent variables as conse-
quent [7]. Linguistic and functional rule-based systems have been used in granular
data modeling [8, 9].
This chapter addresses system modeling using streams of fuzzy interval data. The
idea is to start with imprecise description of the values of data attributes and represent
them in terms of formal fuzzy objects and functional fuzzy rules whose consequents
are discrete-time state space models. The purpose is to represent nonlinear dynamic
time-varying processes using conceptual entities, such as data granules and associ-
ation rules, with no prior assumption about statistical properties of data. Granular
fuzzy models rely on the concepts of information granule and granular mapping to
encapsulate the imprecision in data streams, and to turn information granules into
knowledge in the form of fuzzy rules.
The chapter is structured as follows. Section 2 addresses an incremental, evolv-
ing modeling approach able to process imprecise data streams. The approach is a
continuous learning algorithm that process pointwise or fuzzy data; does not store
previous samples; does not depend upon prior structural knowledge; self-adapts the
model structure whenever needed; is independent of statistical properties of data;
and does not require ‘prototype’ initialization. A speciﬁcity-weighted recursive least
squares algorithm is used to handle imprecise data when updating the parameters of
the rule consequents. Section 3 presents an illustrative application on one-step esti-
mation of the Rossler system. Section 4 concludes the chapter summarizing the ideas
and suggesting issues for further development.

Incremental Granular Fuzzy Modeling Using Imprecise Data Streams
109
2 Incremental Granular Modeling
2.1 Fuzzy Modeling
Incremental, evolving modeling concerns the gradual development of the model
structure (the fuzzy rule-base) and its parameters. Because data streams often are
non-stationary, the structure of the underlying data model should also be dynamic.
Model adaptation should continuously learn from the information contained in new
data and integrate new information in the current model.
A functional fuzzy rule-based model built from a stream of data is attractive
whenever the underlying process is unknown or changes over time. Usually, a ﬁnite
number of past states x(k), x(k −1), ..., x(k −m), outputs and other exogenous vari-
ables can be part of the fuzzy rules antecedents. This chapter assumes functional
fuzzy rules of the form
Ri: IF x1(k) is M i
1 AND ... AND x𝛹(k) is M i
𝛹
THEN xi(k + 1) = Aix(k)
where x(k) = [x1(k) ... x𝜓(k) ... x𝛹(k)]T is the state at k; i = 1, ..., c is the number of
rules. In incremental modeling, Ai is a matrix of appropriate dimension with variable
entries; M i
𝜓, 𝜓= 1, ..., 𝛹, are membership functions built using the data available.
The number of rules Ri, i = 1, ..., c, is also variable. Superscript i on the left-hand
side of the consequent equation means a local estimation. We assume that all state
variables x(k) are measurable. State observers are not addressed in this chapter.
Consequent matrices and the state vector can be extended to include aﬃne terms
as follows:
̃Ai =
[ 1 0
ai
0 Ai
]
, ̃x =
[
1
x
]
,
(1)
where ai
0 = [ai
10 ... ai
𝜓0 ... ai
𝛹0]T. Rules Ri can be rewritten as:
Ri: IF x1(k) is M i
1 AND ... AND x𝛹(k) is M i
𝛹
THEN ̃xi(k + 1) = ̃Aĩx(k)
In the rest of the paper we omit the tilde from the notation for short, and consider
aﬃne models. For the same reason, the time index k is omitted from the time-varying
membership functions M i
𝜓and matrices Ai.
The state estimate from the functional fuzzy model is found as the weighted
average:
x(k + 1) =
c
∑
i=1
𝜇irxi(k + 1),
(2)

110
D. Leite and F. Gomide
where 𝜇ir is the rescaled activation degree of the i-th rule,
𝜇ir =
𝜇i
c∑
i=1
𝜇i
, so that 𝜇ir ≥0 and
c
∑
i=1
𝜇ir = 1.
(3)
Activation degrees 𝜇i are computed using any conjunctive aggregation operator,
typically a t-norm [7, 10]. t-norms are commutative, associative and monotone oper-
ators on the unit hypercube [0, 1]n whose boundary conditions are T(𝜔, 𝜔, ..., 0) = 0
and T(𝜔, 1, ..., 1) = 𝜔, 𝜔∈[0, 1]. The neutral element of t-norms is e = 1. In this
work we use the product t-norm. Thus
𝜇i =
𝛹
∏
𝜓=1
𝜇i
𝜓.
(4)
where 𝜇i
𝜓is the degree of membership of x𝜓(k) in M i
𝜓. While it is common to assume
that the activation degree 𝜇i of at least one rule Ri is nonzero, this is not the case
in evolving modeling because no fuzzy set exists a priori. Fuzzy sets and rules are
created and developed to gradually cover the input data domain. The number of rules
c increases by a unit if 𝜇i = 0 ∀i. In this case, 𝜇c+1 = 1, that is, the fuzzy sets of the
new rule match the input data. Incremental development of fuzzy sets and rules is
taken up in the next sections.
2.2 Fuzzy Data
Fuzzy data may originate from measurements of unreliable sensors, expert judgment,
imprecision introduced in pre-processing steps, and summarization of numeric data
over time periods (time granulation). Fuzzy data modeling generalizes pointwise
data modeling by allowing fuzzy interval granulation [4, 5].
This chapter concerns fuzzy functional rule-based models and trapezoidal fuzzy
data. A trapezoidal fuzzy set N = (l,𝜆,𝛬,L) allows the modeling of a wide class of
granular objects [11]. A triangular fuzzy set is a trapezoid where 𝜆=𝛬; an interval is
a trapezoid where l=𝜆and 𝛬=L; a singleton is a trapezoid where l=𝜆=𝛬=L. Addi-
tional features that make the trapezoidal representation attractive include: (i) ease
of acquiring the necessary parameters: only four parameters need to be captured. A
trapezoidal fuzzy set can be formed straightforwardly from a trapezoidal datum; and
(ii) many operations on trapezoids can be performed using the endpoints of inter-
vals, which are level sets of trapezoids. The piecewise linearity of the trapezoidal
representation allows calculation of only two level sets (core and support) to obtain
a complete instance.

Incremental Granular Fuzzy Modeling Using Imprecise Data Streams
111
A fuzzy set N ∶X →[0, 1] is upper semi-continuous if the set {x ∈X|𝜇(x) > 𝛼}
is closed, that is, if the 𝛼-cuts of N are closed intervals. If the universe X is the
set of real numbers and N is normal, 𝜇(x) = 1 ∀x ∈[𝜆, 𝛬], then N is a model of
a fuzzy interval, with monotone increasing function 𝜁N : [l, 𝜆[→[0, 1], monotone
decreasing function 𝜄N : ]𝛬, L] →[0, 1], and zero otherwise [7]. A fuzzy interval
N has the following canonical form:
N ∶x →𝜇(x) =
⎧
⎪
⎨
⎪⎩
𝜁N ,
x ∈[l, 𝜆[
1,
x ∈[𝜆, 𝛬]
𝜄N ,
x ∈]𝛬, L]
0,
otherwise
,
(5)
where x is a real number in X. The fuzzy interval N satisﬁes the conditions of
normality (𝜇(x) = 1 for at least one x ∈X) and convexity (𝜇(𝜅x1 + (1 −𝜅)x2) ≥
min{𝜇(x1), 𝜇(x2)}, x1, x2 ∈X, 𝜅∈[0, 1]). If
𝜁N = x −l
𝜆−l and
(6)
𝜄N = L −x
L −𝛬,
(7)
then the fuzzy interval (5) reduces to the model of a trapezoidal membership func-
tion. Moreover, when 𝜆= 𝛬, 𝜇(x) = 1 for a single element x in X. In this case, the
corresponding fuzzy entity is a fuzzy number.
Let x = (x, x, x, x) be a trapezoidal datum. The membership degree of x in the
fuzzy set N can be obtained from (5) if x is degenerated into a singleton. Otherwise,
if x is a symmetric object, i.e. if x −x = x −x ≠0, its membership degree in N can
be computed using the midpoint of x:
mp(x) =
x + x
2
.
(8)
The center of gravity
CoG(x) =
x + 5x + 5x + x
12
(9)
is useful when x is asymmetric. Even though it is apparent that these approximations
of the true value are useful to facilitate computations, they contradict the purpose
of taking into account the data uncertainty into fuzzy models. Additionally, in some
situations, as that shown in Fig. 1, the midpoint (or center of area) approximation
can give zero (or low) membership degree to signiﬁcantly overlapped fuzzy objects.
A measure of similarity between fuzzy granular data is needed to properly consider
all relevant situations.

112
D. Leite and F. Gomide
2.3 Similarity Between Fuzzy Sets
Similarity is a fundamental notion to construct rule-based systems from streams of
data. In this work, data are trapezoidal fuzzy sets. A possible similarity measure for
trapezoids, say x and M i, is:
S(x, M i) = 1 −D(x, M i),
(10)
where D(x, M i) is a distance measure computed as follows:
D(x, M i) =
|x −li| + 2|x −𝜆i| + 2|x −𝛬i| + |x −Li|
6
.
(11)
The value of S equals 1 for identical trapezoids and indicates the maximum degree of
matching between them. S decreases linearly as x and M i depart from each other. In
particular, (11) is a Hamming-like distance where the parameters of the trapezoids
are directly compared. Core parameters have double weight in relation to support
parameters. Although (10) - (11) are simple to compute, involving only basic arith-
metic operations, there are no strong principled reasons to choose this measure. In
fact, there is no generally accepted consensus on a best similarity measure [12].
Let the expansion region of a set M i be denoted by
Ei = [Li −𝜌, li + 𝜌],
(12)
where 𝜌is the maximum width that the set M i is allowed to expand to ﬁt a datum
x; Li −li ≤𝜌at any k. Deﬁne the membership degree of the datum x in the fuzzy set
M i as 𝜇i = S(x, M i) if x ∈Ei, and 𝜇i = 0 otherwise.
Fig. 1
Case where the
membership degree of the
fuzzy datum x in the fuzzy
set N obtained by (8)is zero
despite their signiﬁcant
similarity
The similarity measure (10) can be generalized for vectors of trapezoids, say x =
[x1 ... x𝜓... x𝛹]T and M i = [M i
1 ... M i
𝜓... M i
𝛹]T, as follows
S(x, M i) = 1 −1
6𝛹
𝛹
∑
𝜓=1
(|x
𝜓−li
𝜓| + 2|x𝜓−𝜆i
𝜓| +
+ 2|x𝜓−𝛬i
𝜓| + |x𝜓−Li
𝜓|),
(13)

Incremental Granular Fuzzy Modeling Using Imprecise Data Streams
113
then 𝜇i = S(x, M i) if x ∈Ei. Refer to [12] for a thorough discussion about similarity
measures.
2.4 Incremental Adaptation
The purpose of simultaneously adapting the structure and parameters of dynamic
fuzzy models is to use current information about the process to keep its represen-
tation updated. This section develops model structure identiﬁcation and antecedent
parameter estimation. An incremental learning method is introduced to avoid time
consuming training common in multiple passes learning methods.
Expansion regions Ei, see (12), help to verify if new input data belong to a granule
in the input space. Diﬀerent values of 𝜌produce diﬀerent representations of the same
data set in diﬀerent levels of granularities. For normalized data, 𝜌assumes values in
[0, 1]. If 𝜌is equal to 0, then granules are not expanded. Learning creates a new rule
for each sample, which causes overﬁtting and excessive complexity. If 𝜌is equal to 1,
then a single granule covers the entire data domain. Evolvability is reached choosing
intermediate values for 𝜌.
A rule is created whenever one or more entries of x are not within the expansion
regions Ei of M i, i = 1, ..., c. A new associated granule M c+1 is constructed from
fuzzy sets M c+1
𝜓
, 𝜓= 1, ..., 𝛹, whose parameters match x, that is,
M c+1
𝜓
= (lc+1
𝜓, 𝜆c+1
𝜓, 𝛬c+1
𝜓, Lc+1
𝜓) = (x
𝜓, x𝜓, x𝜓, x𝜓).
(14)
Adaptation of an existing granule M i consists in expanding the support [li
𝜓, Li
𝜓]
and updating the core [𝜆i
𝜓, 𝛬i
𝜓] of its fuzzy sets. Among all granules M i that can
be expanded to include a sample x, the one with highest similarity according to (13)
is chosen. Adaptation proceeds depending on where the datum x𝜓is placed. The
conditions to expand the support are:
If x
𝜓∈[Li
𝜓−𝜌, li
𝜓] then li
𝜓(new) = x
𝜓, and
If x𝜓∈[Li
𝜓, li
𝜓+ 𝜌] then Li
𝜓(new) = x𝜓.
The parameters of the core are recursively updated using:
𝜆i
𝜓(new) =
(wi −1)𝜆i
𝜓+ x𝜓
wi
and
(15)
𝛬i
𝜓(new) =
(wi −1)𝛬i
𝜓+ x𝜓
wi
,
(16)

114
D. Leite and F. Gomide
where wi is the number of times that the granule M i was chosen to be adapted.
Figure 2 shows seven possible adaptation situations. In the ﬁgure, the datum x =
(x, x, x, x) places either outside, partially inside or inside the fuzzy set M i. The learn-
ing procedure creates a new granule M c+1 or adapts the parameters of M i accord-
ingly.
2.5 Speciﬁcity-Weighted Recursive Least Squares Algorithm
A recursive least squares-like (RLS) algorithm is used to adapt the parameters of the
rule consequents as follows.
Consider the consequent of rule Ri:
xi(k + 1) = Aix(k)
(17)
where x = [1 x1 ... x𝜓... x𝛹]T. The elements of Ai are denoted ai
𝜓1𝜓2, 𝜓1, 𝜓2 =
0, ..., 𝛹. Rule Ri is chosen to be adapted whenever its antecedent part M i is more
similar to x(k) than the antecedent part of the remaining rules. When instance x(k+1)
becomes known, equation (17) can be solved for Ai.
Expanding the 𝜓-th row of (17) we have
x𝜓(k + 1) = ai
𝜓0 + ai
𝜓1x1(k) + ... + ai
𝜓𝛹x𝛹(k).
(18)
The standard RLS algorithm can be used for each row of (17) if we replace the trape-
zoids x𝜓by their midpoint (8) or center of gravity (9), depending on their symmetry.
Imprecision in the data can be accounted for by weighing the adjustment of ai
𝜓1𝜓2
using speciﬁcity measures. Speciﬁcity measures refer to the amount of information
conveyed by a fuzzy datum [13]. A highly imprecise fuzzy datum (lower speciﬁcity)
may not be as important as a more precise (higher speciﬁcity) datum.
Let ai
𝜓= [ai
𝜓0 ai
𝜓1 ... ai
𝜓𝛹]T be the vector of unknown coeﬃcients; 𝔛= [1
CoG(x1)(k) ... CoG(x𝛹)(k)] be the regression vector; and 𝔜= [CoG(x𝜓)(k + 1)].
Then, in matrix form, equation (18) becomes
𝔜= 𝔛ai
𝜓.
(19)

Incremental Granular Fuzzy Modeling Using Imprecise Data Streams
115
Fig. 2
Creation and
recursive adaptation of fuzzy
sets

116
D. Leite and F. Gomide
To estimate the coeﬃcients ai
𝜓we let
𝔜= 𝔛ai
𝜓+ 𝜩,
(20)
where 𝜩∶= [𝜀𝜓(k + 1)] and
𝜀𝜓(k + 1) = CoG(x𝜓)(k + 1) −CoG(̂x𝜓)(k + 1)
(21)
is the approximation error. While in batch estimation the rows in 𝔜, 𝔛and 𝜩increase
with the number of available samples, in recursive mode only two rows are kept and
we reformulate equations (19)-(21) as follows:
𝔜=
[
CoG(x𝜓)(k)
CoG(x𝜓)(k + 1)
]
, 𝜩=
[
𝜀𝜓(k)
𝜀𝜓(k + 1)
]
and
𝔛=
[
1 CoG(x1)(k −1) ... CoG(x𝛹)(k −1)
1
CoG(x1)(k)
...
CoG(x𝛹)(k)
]
.
(22)
The rows of the matrices in (22) refer to values before and just after adaptation. The
RLS algorithm chooses ai
𝜓to minimize the functional
J(ai
𝜓) = 𝜩T𝜩.
(23)
ai
𝜓is given by
ai
𝜓= (𝔛T𝔛)−1𝔛T𝔜.
(24)
Let Q = (𝔛T𝔛)−1. From the matrix inversion lemma [14] we avoid inverting 𝔛T𝔛
using:
Q(new) = Q(old)
[
I −
𝔛T𝔛Q(old)
1 + 𝔛(2)Q(old)𝔛T
(2)
]
,
(25)
where I is the identity matrix, and 𝔛(2) is the second row of 𝔛. In practice it is
usual to choose large initial values for the entries of the main diagonal of Q. We use
Q(0) = 103I as the default value.
Performing simple mathematical transformations, the vector of coeﬃcients can
be rearranged recursively as
ai
𝜓(new) = ai
𝜓(old) + Q(new)𝔛T(𝔜−𝔛ai
𝜓(old))
(26)

Incremental Granular Fuzzy Modeling Using Imprecise Data Streams
117
or, similarly,
ai
𝜓(new) = ai
𝜓(old) + Q(new)𝔛T𝜩.
(27)
Yager [11] deﬁnes the speciﬁcity of a trapezoid x𝜓as
sp(x𝜓) = 1 −wdt(x𝜓(0.5)).
(28)
This form of speciﬁcity measure means one minus the width of the 0.5 level set of
x𝜓. In terms of the parameters of x𝜓we get
sp(x𝜓) = 1 −
(x𝜓+ x𝜓) −(x𝜓+ x
𝜓)
2
.
(29)
Let the speciﬁcity of x be the diagonal matrix:
sp(x) = diag([1 sp(x1) ... sp(x𝛹)]).
(30)
Thus, we may add speciﬁcity into equation (27) to account for data uncertainty as
follows:
ai
𝜓(new) = ai
𝜓(old) + sp(x)Q(new)𝔛T𝜩
(31)
Figure 3 gives the idea of the speciﬁcity-weighted RLS algorithm. In the ﬁgure in
the left, the coeﬃcients ai(old) of the approximation function result from recursive
adaptation based on x(1), x(2) and x(3). Note that the data granules x(1), x(2) and x(3)
are of the same size and thus have the same speciﬁcity. When the new datum x(4)
arrives (with the same speciﬁcity as that of previous data), the algorithm weights its
contribution equivalently to the contribution of previous data to adapt ai(old) and
yield ai(new). Conversely, on the right side, the speciﬁcity of the new datum x(4) is
lower than that of x(1), x(2) and x(3). The higher uncertainty on the value of x(4)
causes a smaller adjustment of the approximation function toward x(4).
Fig. 3
Speciﬁcity-weighted RLS algorithm

118
D. Leite and F. Gomide
The speciﬁcity-weighted RLS algorithm described in this section is repeated for
𝜓= 1, ..., 𝛹at each step. Detailed derivations of the RLS algorithm can be found in
[15]. A convergence analysis is given in [16].
3 The Rossler Attractor
This section addresses an application example to show the potential of the evolving
fuzzy granular modeling approach. The Rossler attractor [17] is a system of three
nonlinear ordinary diﬀerential equations that exhibits chaotic dynamics. The equa-
tions have been commonly used as a model of equilibrium in chemical reactions. An
orbit within the attractor follows an outward spiral around an unstable ﬁxed point,
close to the x1 −x2 plane. Once the orbit spirals out enough, it is inﬂuenced by a sec-
ond ﬁxed point that causes a rise and twist in the x3 dimension. In the time domain,
irregular oscillations bounded in a range of values are perceptible.
Here, we use the Rossler equations only to generate a data stream. The objective
is to obtain a fuzzy model of an “unknown” nonlinear dynamical system based on
the data stream. The discrete-time Rossler equations are:
x1(k + 1) = x1(k) + (−x2(k) −x3(k))dt + 𝜂1
x2(k + 1) = x2(k) + (x1(k) + ax2(k))dt + 𝜂2
x3(k + 1) = x3(k) + (b + x1(k)x3(k) −cx3(k))dt + 𝜂3
(32)
The nonlinearity is x1x3. Similar to many articles, we considered a = b = 0.1, and
c = 14. dt is the sampling period; 𝜂i is a random value in [−0.5, 0.5]. The initial state
x(0) is (1; 0; 0). The error introduced by the discretization of the original equations
is negligible for sampling periods dt suﬃciently small compared with the signiﬁcant
time constant of the system. As shown in Fig. 4, the trajectory of the system states
in the phase space settles into an aperiodic oscillation. Trajectories are conﬁned to a
fractal set.
In a ﬁrst experiment a fuzzy model is evolved to approximate (32). The equa-
tions are perceived through pointwise input ([x1(k) x2(k) x3(k)]) and output ([x1(k +
1) x2(k + 1) x3(k + 1)]) data. Data become available gradually to simulate a data
stream. No data is available before learning starts. In addition, no data is stored dur-
ing the entire learning process. The one-step forecasting given by the evolving fuzzy
model using the maximum width allowed for granules, 𝜌, equal to 2 is shown in
Fig. 5. The sampling period was chosen to be dt = 0.005 in this experiment. The
ﬁgure shows the results for k = 10500, ..., 16000. The root mean square error, calcu-
lated as
RMSE = 1
kc
kc
∑
k=1
√
√
√
√
√
3
∑
j=1
(xj(k + 1) −̂xj(k + 1))2,
(33)

Incremental Granular Fuzzy Modeling Using Imprecise Data Streams
119
Fig. 4
Rossler chaotic system: phase space trajectory
is RMSE = 0.0372 for kc = 60000. Five rules were developed during the simulation
period. Their parameters are:
Rule 1:
M 1
1 = (−0.9981, 0.0010, 0.0010, 1.0000)
M 1
2 = (0, 0.7781, 0.7781, 1.5562)
M 1
3 = (−0.0178, 0.0066, 0.0066, 0.0310)
A1 =
⎡
⎢
⎢
⎢⎣
1
0
0
0
−0.0494 0.0049 −0.9385 −2.7209
−0.0151 1.0415 0.1366
0.1307
0.1351 0.0073 −0.0709 −14.0967
⎤
⎥
⎥
⎥⎦
Rule 2:
M 2
1 = (−1.1365, −0.1378, −0.1378, 0.8608)
M 2
2 = (−1.2667, −0.3420, −0.3420, 0.5826)
M 2
3 = (−0.0086, 0.0077, 0.0077, 0.0241)
A2 =
⎡
⎢
⎢
⎢⎣
1
0
0
0
−0.0243 −0.0065 −0.9531
3.9008
−0.0994 1.0198
0.0227
2.8007
0.2135
0.0194
0.0153 −22.7749
⎤
⎥
⎥
⎥⎦
Rule 3:
M 3
1 = (0.8690, 1.1200, 1.1200, 1.3710)
M 3
2 = (−0.9937, 0.0061, 0.0061, 1.0058)
M 3
3 = (−0.0141, 0.0073, 0.0073, 0.0287)

120
D. Leite and F. Gomide
A3 =
⎡
⎢
⎢
⎢⎣
1
0
0
0
−0.1717 0.1292 −1.0691
1.1173
−0.1491 1.1051
0.1056
4.7427
0.7875 −0.5095 0.1014 −17.0389
⎤
⎥
⎥
⎥⎦
Rule 4:
M 4
1 = (−1.6920, −1.3501, −1.3501, −1.0083)
M 4
2 = (−0.7079, 0.2867, 0.2867, 1.2813)
M 4
3 = (−0.0043, 0.0098, 0.0098, 0.0238)
A4 =
⎡
⎢
⎢
⎢⎣
1
0
0
0
0.1070 0.0906 −1.0105
5.5327
0.6310 1.4638 0.1224
1.2655
1.7437 0.8991 −0.3307 −25.7244
⎤
⎥
⎥
⎥⎦
Rule 5:
M 5
1 = (−1.5243, −0.5816, −0.5816, 0.3612)
M 5
2 = (−1.7995, −1.2594, −1.2594, −0.7193)
M 5
3 = (−0.0082, 0.0088, 0.0088, 0.0257)
A5 =
⎡
⎢
⎢
⎢⎣
1
0
0
0
0.6108
0.0624 −0.6953 −8.7299
−0.7245 0.8414 −0.2828
5.8742
−0.4074 −0.2262 −0.2866 −23.5195
⎤
⎥
⎥
⎥⎦
From Fig. 5, the eﬀectiveness of the evolving approach in predicting nonlinear
systems without prior knowledge about the data and system equations can be veriﬁed.
The error signals have relatively small amplitudes compared to the amplitudes of
the system states. An important point in this experiment is that due to exponential
divergence of the trajectories for small diﬀerences in the measurements, parameters
or initial states, a non-evolving (oﬄine-trained) modeling method is unlike to track
the trajectory of the states. Another point is that the higher the number of granules
and rules, the more accurate the state estimation tends to be. However, the state
estimation depends on the availability of suﬃcient data for setting local parameters.
A second experiment consisted in evaluating the ability of the modeling approach
in handling fuzzy data, and detecting and reacting to concept drifts and shifts. We
considered the data as perceptions of the values of a variable. Imprecision of the
values of xj is represented by a fuzzy object of the form (xj −0.5, xj, xj, xj + 0.5). At
k = 11500, the parameters of the Rossler equations are shifted to a = b = 0.2 and
c = 3 to simulate a concept shift. At each step after k = 14000, an oﬀset of 0.02 is

Incremental Granular Fuzzy Modeling Using Imprecise Data Streams
121
Fig. 5
One-step estimation of the Rossler map
added to c whereas an oﬀset of −0.02 is added to b and c to produce gradual change
of parameters. Figure 6 shows the results for the state variable x1. The results for the
remaining state variables are essentially the same.
Fig. 6
One-step estimation of the variable x1 of the Rossler system subject to abrupt (k = 11500)
and gradual (k = 14000, ...) changes of parameters
Note from Fig. 6 that the oscillations of the error rate are stronger after the concept
shift, but the quality of state estimates improves after few time steps. To maintain
an acceptable level of prediction performance when the large and unknown change

122
D. Leite and F. Gomide
occurred, the learning algorithm created an additional fuzzy rule - a 6th rule. Con-
versely, when gradual change of the values of the parameters occurred, the learning
algorithm basically adapted the parameters of existing rules to track the trajectory of
the states. The evolving granular modeling method has shown to be robust to time-
varying parameters and able to handle fuzzy data streams.
The granular incremental modeling was compared with alternative state-of-the-
art evolving modeling approaches. The following models were considered: evolving
Takagi-Sugeno (eTS) [18], Dynamic evolving Neuro-Fuzzy Inference System (DeN-
FIS) [19], extended Takagi-Sugeno (xTS) [20], and the evolving Granular Fuzzy
Model (abbreviated in Table 1 as eGFM) described in this paper. We prioritized
model compactness and estimation performance. The models were developed from
scratch, with no rules nor pre-training. Table 1 summarizes the results of one-step
state forecasting of the Rossler chaos. The RMSE is calculated over non-normalized
data and averaged over 10 runs. The number of samples, kc, see (33), is equal to
60000 in each of the simulations.
Table 1
Rossler Chaos - Prediction Performance
Model
Avg. Rules
RMSE Best
RMSE Avg.
xTS
23.7
0.0727
0.0744 ± 0.0015
eTS
5.5
0.0511
0.0619 ± 0.0096
DENFIS
34.7
0.0485
0.0528 ± 0.0032
eGFM
5.5
0.0303
0.0407 ± 0.0100
The results of Table 1 show that, strictly speaking, eGFM is the most accurate
model from the best and average RMSE point of view. The eGFM produces an aver-
age of 5.5 rules, a rule base as compact as that of eTS. In other words, the granular
modeling approach does not take advantage from a large amount of local processing
units (granules/rules) to achieve the average performance of 0.0407. eGFM beneﬁts
from a combination of ingredients concerning with structural assumptions, peculiar-
ities of the learning algorithm, and fuzzy granular framework to attain that perfor-
mance. The eﬀectiveness of the granular evolving approach in one-step estimation
without prior knowledge about the data is veriﬁed in this experiment.
4 Conclusion
This chapter has introduced an incremental fuzzy granular approach for evolving
modeling of nonlinear time-varying systems. The approach is capable to process
and learn from numeric and/or fuzzy data incrementally. Imprecise data is handled
using speciﬁcity measures of the input data during learning. Experiments with the
time-varying Rossler attractor show the usefulness of the method developed; mean-
while, comparisons with state-of-the-art evolving approaches show its eﬀectiveness.

Incremental Granular Fuzzy Modeling Using Imprecise Data Streams
123
Further research is needed to manage unmeasurable state variables. A systematic
design method for evolving fuzzy observers using input-output data shall be con-
sidered. We will also look into issues related to diﬀerent kinds of nonstationarities
and uncertainties in data streams. Stability analysis and stabilization of time-varying
nonlinear systems is also an important issue to be investigated.
References
1. Zadeh, L.A.: Toward a theory of fuzzy information granulation and its centrality in human
reasoning and fuzzy logic. Fuzzy Sets Syst. 90, 111–127 (1997)
2. Bouchon-Meunier, B. et al. (ed.): Uncertainty in Intelligent and Information Systems. World
Scientiﬁc, Singapore (2008)
3. Zadeh, L.A.: Generalized theory of uncertainty - principal concepts and ideas. Comp. Stats
Data Anal. 51, 15–46 (2006)
4. Leite, D., Ballini, R., Costa, P., Gomide, F.: Evolving fuzzy granular modeling from nonsta-
tionary fuzzy data streams. Evolving Syst. 3(2), 65–79 (2012)
5. Pedrycz, W., Skowron, A., Kreinovich, V. (eds.): Handbook of Granular Computing. Wiley,
Chichester (2008)
6. Bargiela, A., Pedrycz, W.: Granular Computing: An Introduction. Kluwer Academic Publish-
ers, Boston (2002)
7. Pedrycz, W., Gomide, F.: Fuzzy Systems Engineering: Toward Human-Centric Computing.
Wiley, Hoboken (2007)
8. Leite, D., Costa, P., Gomide, F.: Evolving granular neural networks from fuzzy data streams.
Neural Netw. 38, 1–16 (2012)
9. Leite, D., Palhares, R., Campos, V., Gomide, F.: Evolving granular fuzzy model-based control
of nonlinear dynamic systems. IEEE Trans. Fuzzy Syst. 17 (2014). doi:10.1109/TFUZZ.2014.
2333774
10. Beliakov, G., Pradera, A., Calvo, T.: Aggregation Functions: A Guide for Practitioners.
Springer, Berlin (2007)
11. Yager, R.R.: Learning from imprecise granular data using trapezoidal fuzzy set representa-
tions. In: Prade, H., Subrahmanian, V.S. (eds.) LNCS, vol. 4772, pp. 244–254. Springer, Berlin
(2007)
12. Cross, V.V., Sudkamp, T.A.: Similarity and Compatibility in Fuzzy Set Theory: Assessment
and Applications. Physica-Verlag, Heidelberg (2002)
13. Yager, R.R.: Measures of speciﬁcity over continuous spaces under similarity relations. Fuzzy
Sets Syst. 159, 2193–2210 (2008)
14. Young, P.C.: Recursive Estimation and Time-Series Analysis: An Introduction. Springer,
Berlin (1984)
15. Astrom, K.J., Wittenmark, B.: Adaptive Control. Addison-Wesley Publishing Company, Lund
Institute of Technology (1989)
16. Johnson, C.R.: Lectures on Adaptive Parameter Estimation. Prentice-Hall, Upper Saddle River
(1988)
17. Rossler, O.E.: An equation for continuous chaos. Phys. Lett. 57A(5), 397–398 (1976)
18. Angelov, P., Filev, D.: Simpl-eTS: a simpliﬁed method for learning evolving Takagi-Sugeno
fuzzy models. In: IEEE International Conference on Fuzzy Systems, pp. 1068–1073 (2005)
19. Kasabov, N., Song, Q.: DENFIS: dynamic evolving neural-fuzzy inference system and its appli-
cation for time-series prediction. IEEE Trans. Fuzzy Syst. 10–2, 144–154 (2002)
20. Angelov, P., Zhou, X.: Evolving fuzzy systems from data streams in real-time. In: International
Symposium on Evolving Fuzzy Systems, pp. 29–35 (2006)

124
D. Leite and F. Gomide
Authors Biography
Daniel Leite
received the Ph.D. degree (2012) in electri-
cal engineering from the University of Campinas (Unicamp),
Brazil; and the M.Sc. (2007) and B.Sc. (2005) degrees in elec-
trical and control engineering from the Polytechnic Institute of
the Pontiﬁcal Catholic University of Minas Gerais (PUC/MG),
Brazil. From 2012 to 2014 he was a lecturer and postdoc-
toral fellow at the Department of Electronics Engineering, Fed-
eral University of Minas Gerais (UFMG). Currently, he is an
assistant professor at the Federal University of Lavras (UFLA),
Brazil. Daniel was the recipient of the Best PhD Thesis Award
in Artiﬁcial and Computational Intelligence from the Brazilian
Computer Society. His current research interests include adap-
tive and evolving systems, intelligent control, nonlinear system
modeling, machine learning, and granular computing. He has
contributed as a reviewer of several journals and conferences
in his research ﬁelds.
Fernando Gomide received the B.Sc. degree in electrical
engineering from the Polytechnic Institute of the Pontiﬁcal
Catholic University of Minas Gerais (PUC/MG), Brazil, the
M.Sc. degree in electrical engineering from the University of
Campinas (Unicamp), Brazil, and the Ph.D. degree in systems
engineering from Case Western Reserve University (CWRU),
USA. He is professor of the Department of Computer Engi-
neering and Automation (DCA), School of Electrical and Com-
puter Engineering (FEEC) of the University of Campinas since
1983. His interest areas include fuzzy systems, neural, granular,
and evolutionary computation, intelligent data analysis, mod-
eling, control and optimization, decision-making and applica-
tions. He was past vice-president of IFSA (International Fuzzy
Systems Association), past IFSA Secretary, member of the
board of NAFIPS (North American Fuzzy Information Process-
ing Society) and former associate editor of the EUSFLAT Mathware and Soft Computing, IEEE
Transactions on SMC-A/B, Fuzzy Sets and Systems, and IEEE Transactions on Fuzzy Systems,
as past member of the IEEE Emergent Technology Technical Committee. Currently he serves
the editorial boards of Evolving Systems, Fuzzy Optimization and Decision Making, Interna-
tional Journal of Fuzzy Systems, and Soft Computing. He was also a past associate editor of
Controle & Automacao, the journal of the Brazilian Society of Automatics (SBA), the Brazilian
National Member Organization of IFAC and IFSA. He is on the Advisory Board of the Interna-
tional Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, Journal of Advanced
Computational Intelligence, and Intelligent Automation and Soft Computing. He is senior mem-
ber of the IEEE, member of NAFIPS, EUSFLAT, IFSA Fellow Class 2009, and NAFIPS K.
S. Fu Award 2011. He also serves the IEEE Task Force on Adaptive Fuzzy Systems and the
Fuzzy Technical Council of the IEEE Computational Intelligence Society.

Fuzzy Measures and Integrals: Recent
Developments
Michel Grabisch
Abstract This paper surveys the basic notions and most important results around
fuzzy measures and integrals, as proposed independently by Choquet and Sugeno,
as well as recent developments. The latter includes bases and transforms on set func-
tions, fuzzy measures on set systems, the notion of horizontal additivity, basic Cho-
quet calculus on the nonnegative real line introduced by Sugeno, the extension of the
Choquet integral for nonmeasurable functions, and the notion of universal integral.
1 Introduction
This paper gives a survey of the research done on fuzzy measures and integrals
since Sugeno proposed in 1974 the concept of fuzzy measure, with an emphasis
on recent results. This ﬁeld of research lies at the intersection of several independent
domains, which makes it very active and attractive, namely, measure theory, the-
ory of aggregation functions, cooperative game theory, combinatorial optimization,
pseudo-Boolean functions and more generally theoretical computer sciences. As an
illustration of this fact, the word “fuzzy measure” which was coined by Sugeno,
has many diﬀerent names according to the ﬁeld where it is used: nonadditive mea-
sure, capacity, monotone game, pseudo-Boolean function, rank function of a poly-
matroid, etc. Evidently, this short paper cannot make a complete account of all the
research undertaken in this area, a whole book will hardly suﬃces. Indeed, the author
is preparing a monograph on this topic, with the title: “Set functions, games and
capacities in decision making”, to be published by Springer around the end of 2015.
This paper gives a kind of quick and necessarily simpliﬁed summary of selected top-
ics. We recommend the interested reader to consult the main (available) monographs
dealing with fuzzy measures and integrals: Pap [1], Denneberg [2], Wang and Klir
[3], the Handbook of measure theory edited by Pap [4], as well as the edited book [5],
M. Grabisch (✉)
Paris School of Economics, University of Paris I, 106-112, Bd. de L’Hôpital,
75013 Paris, France
e-mail: michel.grabisch@univ-paris1.fr
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_8
125

126
M. Grabisch
and the survey paper [6]. The latter focusses on application in multicriteria decision
making, an aspect which is not covered by this paper, restricting to theory.
To avoid intricacies, in the whole paper the universal set X is ﬁnite, with |X| = n.
We often use ∨, ∧, which collapse to maximum and minimum on ﬁnite sets.
2 Fuzzy Measures
Fuzzy measures introduced by Sugeno [7] are generalization of classical measures,
i.e., additive and nonnegative set functions, whose domain is an algebra on X. As
we will see in Sect. 2.4, the structure of algebra is not needed here, and various struc-
tures can be thought of. For simplicity, we assume = 2X in the ﬁrst subsections,
the general case will be addressed in the last one.
2.1 Deﬁnition, Main Families and Properties
A fuzzy measure on X is a set function 𝜇∶2X →ℝsuch that 𝜇(∅) = 0 and
𝜇obeys monotonicity: A ⊆B ⊆X implies 𝜇(A) ⩽𝜇(B). Fuzzy measures are
also called capacities (after Choquet [8]), nonadditive measures (Denneberg [2]),
monotone measures (Wang and Klir [3]), etc. If in addition 𝜇(X) = 1, then the fuzzy
measure is said to be normalized.
If monotonicity is dropped from the deﬁnition, we obtain nonmonotonic fuzzy
measures, more commonly called games, denoted usually by v.
One of the most important property of fuzzy measures (or games as well) is con-
vexity, a.k.a. supermodularity. A fuzzy measure 𝜇is convex if for all A, B ∈2X,
𝜇(A ∪B) + 𝜇(A ∩B) ⩾𝜇(A) + 𝜇(B). If the reverse inequality holds, 𝜇is said to
be concave or submodular. Convexity is generalized by the so-called k-monotonicity
property: 𝜇is k-monotone for some ﬁxed 2 ⩽k ⩽n if for any family of k sets
A1, … , Ak ∈2X,
𝜇
(
k⋃
i=1
Ai
)
⩾
∑
I⊆{1,…,k}
I≠∅
(−1)|I|+1𝜇
( ⋂
i∈I
Ai
)
.
(1)
Moreover, 𝜇is totally monotone if it is k-monotone for every k ⩾2 (in fact, 2 ⩽k ⩽
2n−2 suﬃces). The k-alternating property is deﬁned similarly, interchanging ⋂and
⋃and reversing inequality. Lastly, 𝜇is said to be maxitive if 𝜇(A∪B) = 𝜇(A)∨𝜇(B),
and minitive if 𝜇(A ∩B) = 𝜇(A) ∧𝜇(B).

Fuzzy Measures and Integrals: Recent Developments
127
The simplest fuzzy measures which can be thought of are 0-1-fuzzy measures:
their range is simply {0, 1}. In game theory, they are called simple games and are
useful in voting theory. Among them, particularly useful are unanimity games (a.k.a.
simple support functions): for any ∅≠A ⊆X, the unanimity game uA is deﬁned by
uA(B) =
{
1,
if B ⊇A
0,
otherwise.
The next remarkable families are possibility and necessity measures: a possibility
(resp., necessity) measure is a normalized maxitive (resp., minitive) fuzzy measure
(Zadeh [9], Dubois and Prade [10]). Necessity measures are particular cases of belief
functions, as proposed by Shafer [11] (similarly, plausibility functions generalize
possibility measures). Mathematically speaking, a belief (resp., plausibility) function
is a normalized totally monotone (resp., alternating) fuzzy measure.
2.2 Transforms and Bases
The set of games, as well as the set of set functions, form a vector space of dimension
2n −1 (resp., 2n). This is not the case for the set of fuzzy measures, which is only
a cone, while the set of normalized capacities is a polytope, whose vertices are the
0-1 fuzzy measures (Stanley [12], Radojevic [13]). In the rest if this section, we deal
with the vector space of set functions (the results can be however easily adapted to
the set of games).
A transform is a mapping 𝛹∶ℝ(2N) →ℝ(2N), assigning to any set function 𝜉
the set function 𝛹𝜉. If the transform is linear and invertible, then it induces a basis
of the vector space of set functions (and similarly for games). Conversely, any basis
induces a linear invertible transformation. This is explicited in the next lemma.
Lemma 1 (Faigle and Grabisch [14]) For every basis {bS}S∈2X of ℝ2X, there exists
a unique linear invertible transform 𝛹such that for any 𝜉∈ℝ2X,
𝜉=
∑
S∈2X
𝛹𝜉(S)bS,
(2)
whose inverse 𝛹−1 is given by 𝜉↦(𝛹−1)𝜉= ∑
T∈2X 𝜉(T)bT.
Conversely, to any transform 𝛹corresponds a unique basis {bS}S∈2X such that
(2) holds, given by bS = (𝛹−1)𝛿S, where 𝛿S is a 0-1-valued set function deﬁned by
𝛿S(T) = 1 if and only T = S.
It is well known that the set of unanimity games forms a basis of the set of games.
Adding the 0-1-valued set function u∅deﬁned by u∅(S) = 1 if and only if S = ∅,

128
M. Grabisch
we get a basis for the vector space of set functions. By Lemma 1, the corresponding
transform, denoted by m, satisﬁes
𝜉(A) =
∑
B⊆A
m𝜉(B)
(A ∈2X),
which yields
m𝜉(A) =
∑
B⊆A
(−1)|A⧵B|
(A ∈2X).
This transform is known as the Möbius transform, famous in combinatorics. Among
the many existing transforms, at least two of them have a special interest. The inter-
action transform [15], generalizing the Shapley value [16] and the interaction index
of Murofushi and Soneda [17], has the following expression:
I𝜉(A) ∶=
∑
B⊆X⧵A
(n −b −a)!b!
(n −a + 1)! 𝛥A𝜉(B) =
∑
K⊆X
|X ⧵(A ∪K)|!|K ⧵A|!
(n −a + 1)!
(−1)|A⧵K|𝜉(K)
for all A ⊆X, where a, b, k are cardinalities of subsets A, B, K, respectively, and
𝛥A𝜉(B) = ∑
K⊆A(−1)|A⧵K|𝜉(B∪K). This transform enables the interpretation of fuzzy
measures in a multicriteria decision making context [6, 18]. The inverse transform
is given by
(I−1)𝜉(S) =
∑
K⊆X
𝛽|K|
|S∩K|𝜉(K),
with coeﬃcients 𝛽l
k given by
𝛽l
k =
k
∑
j=0
(
k
j
)
Bl−j
(k ⩽l),
where the Bj’s are the Bernoulli numbers. It follows from Lemma 1 that the corre-
sponding basis is
bI
T(S) = 𝛽|T|
|T∩S|
(S, T ∈2X).
The interaction transform of 𝜉can be expressed in a simple way through its Möbius
transform:
I𝜉(A) =
∑
B⊇A
1
b −a + 1m𝜉(B).
(3)

Fuzzy Measures and Integrals: Recent Developments
129
The second transform of interest is the so-called Fourier transform, well known
in computer sciences (see, e.g., de Wolf [19] and O’Donnell [20]). The Fourier trans-
form of a set function 𝜉is deﬁned by
F𝜉(S) = 1
2n
∑
K⊆X
(−1)|S∩K|𝜉(K).
Interestingly enough, it is auto-inverse up to the factor 1∕2n:
(F−1)𝜉(S) =
∑
K⊆X
(−1)|S∩K|𝜉(K).
The corresponding basis is therefore
bF
T(S) =
∑
K⊆X
(−1)|S∩K|𝛿T(K) = (−1)|S∩T|
(S, T ∈2X).
The vectors of this basis (not that these are not games) are called parity functions
in the literature of computer sciences. They are up to a recoding equal to the Walsh
functions wS(T) = (−1)|S⧵T| (indeed, bF
T(S) = wS(X ⧵T)). These are a ﬁnite version
of the original functions proposed by Walsh (see Hurst et al. [21]), who form a orho-
normal basis of the set of square integrable functions on [0, 1]. The major advantage
of the Fourier (or Walsh) basis is that it is orthonormal, in the sense that ⟨bF
T, bF
S ⟩= 1
if S = T, and 0 otherwise, where the inner product is deﬁned by
⟨𝜉, 𝜉′⟩= 1
2n
∑
S∈2X
𝜉(S)𝜉′(S).
Another remarkable property is that the Fourier transform turns the convolution
product into an ordinary product (like with the original deﬁnition of the Fourier
transform):
F𝜉∗𝜉′ = F𝜉F𝜉′
where the convolution product of two set functions is deﬁned by
(𝜉∗𝜉′)(S) = 1
2n
∑
T∈2X
𝜉(S𝛥T)𝜉′(T)
(S𝛥T is the symmetric diﬀerence, i.e., (S ∪T) ⧵(S ∩T)).
We ﬁnish this section by giving the bounds of the Möbius transform for a normal-
ized fuzzy measure. Surprisingly, the interval in which the Möbius transform of a
normalized fuzzy measure can vary is not [−1, 1], but its bounds grow rapidly with n,
approximately in
4
n
2
√𝜋n
2
, as shown in [22] (corrected version of an earlier publication
[23]). The precise result is as follows.

130
M. Grabisch
Theorem 1 For any normalized fuzzy measure 𝜇, its Möbius transform satisﬁes for
any A ⊆N, |A| > 1:
−
(|A| −1
l′
|A|
)
⩽m𝜇(A) ⩽
(
|A| −1
l|A|
)
,
with
l|A| = 2
⌊|A|
4
⌋
,
l′
|A| = 2
⌊|A| −1
4
⌋
+ 1
(4)
and for |A| = 1 < n:
0 ⩽m𝜇(A) ⩽1,
and m𝜇(A) = 1 if |A| = n = 1. These upper and lower bounds are attained by the
normalized fuzzy measures 𝜇∗
A, 𝜇A∗, respectively:
𝜇∗
A(B) =
{
1,
if |A| −l|A| ⩽|B ∩A| ⩽|A|
0,
otherwise
,
𝜇A∗(B) =
{
1,
if |A| −l′
|A| ⩽|B ∩A| ⩽|A|
0,
otherwise
for any B ⊆N.
We give in Table 1 the ﬁrst values of the bounds.
Table 1
Lower and upper bounds for the Möbius transform of a normalized fuzzy measure
|A|
1
2
3
4
5
6
7
8
9
10
11
12
u.b. of m𝜇(A) 1
1
1
3
6
10
15
35
70
126
210
462
l.b. of m𝜇(A) 1(0)
−1
−2
−3
−4
−10
−20
−35
−56
−126 −252 −462
2.3 k-additive and p-symmetric Fuzzy Measures
A fuzzy measure 𝜇is additive if 𝜇(A ∪B) = 𝜇(A) + 𝜇(B) for every disjoint
A, B ∈2X. Normalized additive fuzzy measures therefore coincide with probabil-
ity measures. Observing that the Möbius transform of an additive fuzzy measure 𝜇
satisﬁes m𝜇(A) = 0 for all A ∈2X such that |A| > 1, a natural generalization of
additivity is k-additivity: a fuzzy measure 𝜇is k-additive (1 ≤k ≤n) if m𝜇(A) = 0
for all A ∈2X such that |A| > k, and there exists at least one A ∈2X such that
m𝜇(A) ≠0 (Grabisch [15]). It follows that a k-additive fuzzy measure needs only
(n
1
) + (n
2
) + ⋯+ (n
k
) coeﬃcients to be deﬁned, instead of 2n −1.

Fuzzy Measures and Integrals: Recent Developments
131
Due to (3), an equivalent deﬁnition is: 𝜇is k-additive if its interaction transform
I𝜇vanishes for subsets of more than k elements, and there exists a subset A of k ele-
ments such that I𝜇(A) ≠0. Since the interaction transform has a clear interpretation
in the context of multicriteria decision making, k-additive fuzzy measures are of par-
ticular interest. Especially, 2-additive fuzzy measure have the advantage of being the
simplest fuzzy measures (in terms of number of free coeﬃcients) able to represent
interaction between two elements.
k-additive fuzzy measures are families of fuzzy measures which are of poly-
nomial complexity instead of the exponential complexity of general fuzzy mea-
sures. Another set of such families is provided by the concept of p-symmetric
fuzzy measure (Miranda and Grabisch [24, 25]). A fuzzy measure 𝜇is symmetric if
𝜇(A) = 𝜇(B) whenever |A| = |B|. Furthermore, two distinct elements i, j ∈X are
symmetric w.r.t. a fuzzy measure 𝜇(denoted by i ∼𝜇j) if 𝜇(A ∪i) = 𝜇(A ∪j) for
every A ⊆X ⧵{i, j}. Note that ∼𝜇is an equivalence relation, and let us consider its
equivalence classes, which forms a partition of X. Clearly, a symmetric fuzzy mea-
sure has only one such equivalence class, which is X. A natural generalization is:
a fuzzy measure is p-symmetric if ∼𝜇has p equivalence classes. It follows that any
fuzzy measure is p-symmetric for some 1 ≤p ≤n (by the way, also k-additive for
some 1 ≤k ≤n).
Consider a p-symmetric fuzzy measure 𝜇, with set of equivalence classes {A1,
… , Ap}, and a subset B ⊆X. Clearly, the value 𝜇(B) depends uniquely on the num-
bers b1, … , bp, with bi ∶= |Ai ∩B|. Since 0 ⩽bi ⩽|Ai|, it follows that 𝜇needs
∏p
i=1(|Ai| + 1) coeﬃcients to be deﬁned.
2.4 Fuzzy Measures on Set Systems
A set system on X is a subcollection of 2X containing ∅and covering X, that is,
⋃
A∈A = X. We consider in this section fuzzy measures whose domain is a set
system.
We begin by introducing the main families of set systems of interest. The most
classical example borrowed from measure theory is algebra. An algebra is a set sys-
tem closed under ﬁnite union and complementation. Although complementation is
fundamental in classical measure theory, this is no more the case for fuzzy measures
and games, so that other algebraic structures arise:
(i) Set systems closed under union and intersection: (Faigle and Kern [26]) It
follows that such set systems contain X and are distributive lattices. Under the
additional condition that there is no macro-element (i.e., a subset M ⊂X with
|M| > 1 such that for any A ∈, either M ⊆A or A∩M = ∅), from Birkhoﬀ’s
representation theorem, the set of all such set systems is in bijection with the set
of partial orders on X. In other words, any such is generated by a partial order
on X, which can be interpreted as a kind of hierarchy of the elements in X. This
is particularly meaningful when X is a set of players, agents, etc., or criteria.

132
M. Grabisch
(ii) Weakly union-closed set systems: (Algaba [27], Faigle and Grabisch [28, 29])
is weakly union-closed if A, B ∈, A ∩B ≠∅imply A ∪B ∈. This larger
family is motivated by communication graphs. Suppose that a graph (X, E) is
deﬁned on X, with X being the set of nodes, and E being the set of edges, i.e.,
pairs {i, j} with i, j ∈X and i ≠j. Say that a subset A ⊆X is connected if for any
distinct i, j ∈A, there exists a sequence i = i1, i2, … , iq = j of elements of X
such that {ik, ik+1} ∈E for k = 1, … , q −1. Deﬁning as the set of connected
subsets of X, it follows that is weakly union-closed (this is however not a
characterizing property).
(iii) Regular set systems: [30, 31] a set system is regular if it contains X and
any maximal chain1 from ∅to X has length n. Every distributive lattice is a
regular set system. The motivation for such sets systems is more mathemati-
cal: it happens that many concepts around games and fuzzy measures are based
on maximal chains of length n (Shapley value, marginal vectors, Choquet inte-
grals, etc.).
If is a lattice (in particular, if is closed under union and intersection), the
deﬁnition of k-monotonicity is easily adapted by substituting ∪, ∩in (1) by ∨, ∧of
the lattice. It is well-known that when = 2X, there is an equivalence between
total monotonicity and the nonnegativity of the Möbius transform. It has been for a
long time an unsolved issue whether this equivalence still holds if is a lattice, only
recently solved:
Theorem 2 Let 𝜇be fuzzy measure on a lattice . Then 𝜇is totally monotone if and
only if it has a nonnegative Möbius transform.
The “only if” part was shown by Barthélemy [32], and the “if part” recently by
Zhou [33].
3 The Choquet and Sugeno Integrals
The term “fuzzy integral” has been introduced by Sugeno [7] in 1974, and is now
most commonly called the Sugeno integral. However, Choquet already in 1954 pro-
posed a functional w.r.t. a fuzzy measure (or capacity), referred now as the Choquet
integral. As we will see in Sect. 3.8, other integrals w.r.t. fuzzy measures have been
proposed. We study in detail the Choquet and Sugeno integrals, which can be con-
sidered as the most representative (and still very diﬀerent) fuzzy integrals. Except
for Sect. 3.7, we assume that fuzzy measures are deﬁned on = 2X.
1A chain from ∅to X is a sequence ∅= A0, A1, … , Aq = X of sets in such that A0 ⊂A1 ⊂⋯⊂
Aq. Its length is q, and the chain is maximal if no other chain from ∅to X contains it.

Fuzzy Measures and Integrals: Recent Developments
133
3.1 Deﬁnitions and Basic Properties
We begin by introducing the general deﬁnition, which is valid for arbitrary spaces.
For this, we need decumulative distribution functions. Let 𝜇be a fuzzy measure and
f ∶X →ℝ. The decumulative distribution of f w.r.t. 𝜇is
G𝜇,f (t) = 𝜇({x ∈X ∣f(x) ⩾t}
(t ∈ℝ).
We consider ﬁrst nonnegative functions. Let f ∶X →ℝ+ and 𝜇be a fuzzy
measure. The Choquet integral of f w.r.t. 𝜇is deﬁned by
∫f d𝜇= ∫
∞
0
G𝜇,f (t) dt,
(5)
where the right hand-side integral is the Riemann integral. The Sugeno integral of
f w.r.t. 𝜇is deﬁned by
−
∫f d𝜇=
⋁
t⩾0
(G𝜇,f (t) ∧t) =
⋀
t⩾0
(G𝜇,f (t) ∨t).
In words, the Sugeno integral is the abscissa of the intersection point between the
diagonal and the decumulative function, while the Choquet integral is the area below
the decumulative function. It can be proven that it is equivalent to consider a strict
inequality in the deﬁnition of G𝜇,f . Another equivalent formula for the Sugeno inte-
gral is
−
∫f d𝜇=
⋁
A∈
( ⋀
x∈A
f(x) ∧𝜇(A)
)
.
Note that the Choquet integral can be deﬁned w.r.t. games as well. However, since
the decumulative function is no more monotone with games, the deﬁnition of the
Sugeno integral is restricted to fuzzy measures. An elementary property is that for
every A ⊆X, ∫1A d𝜇= 𝜇(A), where 1A is the characteristic function of A. The latter
property holds also for the Sugeno integral, provided 𝜇is normalized. In view of this
property, the Choquet and Sugeno integrals can be considered as extensions of fuzzy
measures.
When X = {x1, … , xn}, the formulas can be made more explicit. For a function
f ∶X →ℝ+, let fi denotes f(xi) for simplicity, and take a permutation 𝜎on {1, … , n}
such that f𝜎(1) ⩽⋯⩽f𝜎(n). Deﬁne A↑
𝜎(i) = {x𝜎(i), x𝜎(i+1), … , x𝜎(n)}, i = 1, … , n. The
Choquet integral is given by

134
M. Grabisch
∫f d𝜇=
n
∑
i=1
(f𝜎(i) −f𝜎(i−1))𝜇(A↑
𝜎(i))
(6)
=
n
∑
i=1
f𝜎(i)
(𝜇(A↑
𝜎(i)) −𝜇(A↑
𝜎(i + 1))),
(7)
with the conventions f𝜎(0) = 0 and A↑
𝜎(n + 1) = ∅.
For the Sugeno integral, we obtain:
−
∫f d𝜇=
n
⋁
i=1
(f𝜎(i) ∧𝜇(A↑
𝜎(i)))
(8)
=
n
⋀
i=0
(f𝜎(i) ∨𝜇(A↑
𝜎(i + 1)))
(9)
with the same conventions.
We consider now the case of real-valued integrands. For any f ∶X →ℝ, we write
f = f + −f −, with f + = 0 ∨f,
f −= (−f)+.
Then the symmetric Choquet integral (a.k.a. Šipoš integral [34]) is deﬁned by
̌
∫f d𝜇= ∫f + d𝜇−∫f −d𝜇.
(10)
The asymmetric Choquet integral, which is the usual deﬁnition, is deﬁned by
∫f d𝜇= ∫f + d𝜇−∫f −d𝜇,
(11)
where 𝜇is the conjugate fuzzy measure, deﬁned by 𝜇(A) = 𝜇(X) −𝜇(X ⧵A) for
any A ∈2X. The asymmetric Choquet integral is translation invariant (it is the only
extension having this property), while the symmetric integral satisﬁes
̌
∫(−f) d𝜇= −
̌
∫f d𝜇.
The case of the Sugeno integral is more cumbersome, essentially due to the fol-
lowing problem. The Sugeno integral is deﬁned through the ∨, ∧operators, playing
the rôle of addition and product respectively (compare (6) with (8)). Remembering
that on the ring of real numbers, a −b is shorthand for a + (−b), a transposition of
formula (10) for the Sugeno integral would read

Fuzzy Measures and Integrals: Recent Developments
135
̌
−
∫f d𝜇= −
∫f + d𝜇
(
−−
∫f −d𝜇
)
(12)
where  is an extension of ∨for real numbers (i.e., a  b = a∨b whenever a, b ⩾0)
such that a (−a) = 0. Surprisingly, such an operator  would be necessarily nonas-
sociative. Indeed,
((−3)  3)  2) = 0  2 = 0 ∨2 = 2
(−3) (3  2) = (−3) (3 ∨2) = (−3)  3 = 0.
The lack of associativity forbids to infer the so-called rule of sign, i.e., (−a) (−b) =
−(a  b), which is necessary for the symmetry of the integral:
̌
−
∫(−f) d𝜇=−
∫f −d𝜇
(
−−
∫f + d𝜇
)
= −
((
−−
∫f −d𝜇
)
 −
∫f + d𝜇
)
= −
̌
−
∫f d𝜇.
(13)
It can be shown [35] that the best operator (in the sense that it is associative on the
largest domain) satisfying the above requirements (including the rule of sign) is the
symmetric maximum, deﬁned by
a  b =
⎧
⎪
⎨
⎪⎩
−(|a| ∨|b|),
if b ≠−a and either |a| ∨|b| = −a or = −b
0,
if b = −a
|a| ∨|b|,
otherwise.
(14)
The symmetric Sugeno integral [36] is therefore deﬁned by (12) and . Up to now,
there is no adequate deﬁnition of an asymmetric Sugeno integral.
3.2 The Choquet Integral as a Linear Interpolator
Consider the following problem: a function I ∶[0, 1]n →[0, 1] is known only on the
vertices of the hypercube [0, 1]n (in particular I(𝟎) = 0, where 𝟎is the 0 vector), and
has to be determined everywhere in the hypercube. This is an interpolation problem,
and there exists many ways to make the interpolation. Noting that the vertices of the
hypercube correspond bijectively to the subsets of X (with |X| = n), it follows that
I is necessarily an extension of a game v: I(1A) = v(A) for every A ∈2X. Hence the
Choquet and Sugeno integrals could be candidate.
Even if we restrict to a linear interpolation, there are still many ways of doing the
interpolation, depending on which vertices are chosen, but there exist two extreme

136
M. Grabisch
ways. If all vertices are used for each point f ∈[0, 1]n, we get the multilinear model
(owen, citeowe88), given by:
I(f) =
∑
A⊆X,A≠varnothing
mv(A)
∏
i∈A
fi
where mv is the Möbius transform of v, deﬁned by v(A) = I(1A) for every A ∈
2X. The other extreme case would be to take the minimum number of vertices so
that the considered vector x is contained in the convex hull of the selected vertices
(parsimonious interpolation). Then this number is n + 1, the number of vertices of
a n-dimensional simplex, and the problem of choosing the right simplices for each
f amounts to the triangulation problem of the hypercube. There is one triangulation
of particular interest since it leads to an interpolation where all constant terms are 0,
the triangulation in the n! canonical simplices, where each simplex is induced by a
permutation 𝜎on {1, … , n}:
S𝜎= {f ∈[0, 1]n ∣f𝜎(1) ⩽f𝜎(2) ⩽⋯⩽f𝜎(n)}.
Then it can be shown that the parsimonious linear interpolation based on the canon-
ical simplices is the Choquet integral. This fact was remarked by Singer [37], and
also Marichal [38].
3.3 Expression W.r.t Transforms
The Choquet integral being linear w.r.t. the game, it is easy to get its expression when
the game is expressed by some linear invertible transform (equivalently, in some
other basis). Let 𝛹be a linear invertible transform, and {b𝛹
A }A∈2X the corresponding
basis of set functions given by Lemma 1. Since these set functions are not necessarily
games, and the Choquet integral needs games to be well deﬁned, we build a basis of
games {b′𝛹
A }A∈2X⧵{∅} as follows:
b′
S(T) =
{
bS(T),
if T ≠∅
0, otherwise
(S ∈2X ⧵{∅}).
(15)
Then for every f ∈ℝX and every game v,
∫f dv = ∫f d
( ∑
∅≠A⊆X
𝛹v(A)b′𝛹
A
)
=
∑
∅≠A⊆X
𝛹v(A) ∫f db′𝛹
A .
(16)
It is therefore suﬃcient to compute ∫f db′𝛹
A for every A ⊆X, A ≠∅.

Fuzzy Measures and Integrals: Recent Developments
137
Applying this to the Möbius transform immedaitely yields the following well-
known formula:
∫f dv =
∑
A⊆X
mv(A)
⋀
i∈A
fi.
(17)
The same methodology is not applicable to the Sugeno integral since it is not
linear w.r.t. the fuzzy measure. It is possible however to obtain a formula similar to
(17), by means of the ordinal Möbius transform. The ordinal Möbius transform of a
fuzzy measure 𝜇is the interval [m] ∶= [m∗, m∗], with m∗= 𝜇, and
m∗(A) =
{
𝜇(A),
if 𝜇(A) > 𝜇(A ⧵i), ∀i ∈A
0,
otherwise
(A ⊆X).
(18)
The above formula has been ﬁrst proposed in [39, 40], then developed in [35]. Then,
it can be proved that the Sugeno integral takes the form:
−
∫f d𝜇=
⋁
A⊆X
( ⋀
i∈A
fi ∧m(A)
)
(19)
where m is any function in [m∗, m∗].
3.4 Properties
The next propositions summarize the main elementary properties of Choquet and
Sugeno integrals. In the whole section, X is supposed to be ﬁnite, and = 2X.
Theorem 3 Let f ∶X →ℝbe a function and a game v. The following properties
hold for the Choquet integral.
(i) Positive homogeneity:
∫𝛼f dv = 𝛼∫f dv
(𝛼⩾0)
(ii) Homogeneity of the symmetric Choquet integral:
̌
∫𝛼f dv = 𝛼
̌
∫f dv
(𝛼∈ℝ)
(iii) Translation invariance:
∫(f + 𝛼1X) dv = ∫f dv + 𝛼v(X)
(𝛼∈ℝ)

138
M. Grabisch
(iv) Asymmetry:
∫(−f) dv = −∫f dv
where v is the conjugate game;
(v) Scale inversion:
∫(𝛼1X −f) dv = 𝛼v(X) −∫f dv
(𝛼∈ℝ)
(vi) Monotonicity w.r.t. the integrand: for any fuzzy measure 𝜇,
f ⩽f ′ ⇒∫f d𝜇⩽∫f ′ d𝜇
(vii) Monotonicity w.r.t. the game for nonnegative integrands: if f ⩾0,
v ⩽v′ ⇒∫f dv ≤∫f dv′
(viii) Linearity w.r.t. the game:
∫f d(v + 𝛼v′) = ∫f dv + 𝛼∫f dv′,
(𝛼∈ℝ)
(ix) Boundaries: inf f and sup f are attained:
inf f = ∫f d𝜇min,
sup f = ∫f d𝜇max,
with 𝜇min(A) = 0 for all A ⊂X, and 𝜇max(A) = 1 for all nonempty A ⊆X;
(x) Continuity.
Theorem 4 Let f ∶X →ℝ+, and 𝜇a fuzzy measure on X. The following properties
hold for the Sugeno integral.
(i) Positive ∧-homogeneity:
−
∫(𝛼1X ∧f) d𝜇= 𝛼∧−
∫f d𝜇
(𝛼⩾0)
(ii) Positive ∨-homogeneity if sup f ⩽𝜇(X):
−
∫(𝛼1X ∨f) d𝜇= 𝛼∨−
∫f d𝜇
(𝛼∈[0, sup f]).

Fuzzy Measures and Integrals: Recent Developments
139
(iii) Hat function: for every 𝛼⩾0 and for every A ∈,
−
∫𝛼1A d𝜇= 𝛼∧𝜇(A)
(iv) Scale inversion: if sup f ⩽𝜇(X),
−
∫(𝜇(X)1X −f) d𝜇= 𝜇(X) −−
∫f d𝜇,
where 𝜇is the conjugate fuzzy measure;
(v) Scale translation:
−
∫(f + 𝛼1X) d𝜇⩽−
∫f d𝜇+ −
∫𝛼d𝜇= −
∫f d𝜇+ 𝛼∧𝜇(X)
(𝛼⩾0)
(vi) Monotonicity w.r.t. the integrand:
f ⩽f ′ ⇒−
∫f d𝜇⩽−
∫f ′ d𝜇
(f, f ′ ∈B+())
(vii) Monotonicity w.r.t. the fuzzy measure:
𝜇⩽𝜇′ ⇒−
∫f d𝜇≤−
∫f d𝜇′
(viii) Max-min linearity w.r.t. the fuzzy measure:
−
∫f d(𝜇∨(𝛼∧𝜇′)) = −
∫f d𝜇∨
(
𝛼∧∫f d𝜇′)
(𝛼⩾0)
(ix) Boundaries: inf f and sup f are attained:
inf f = −
∫f d𝜇min,
sup f = −
∫f d𝜇max,
with 𝜇min, 𝜇max deﬁned as in Theorem 3;
(x) Lipschitz continuity:
||||
−
∫f d𝜇−−
∫g d𝜇||||
⩽𝜇(X) ∧‖f −g‖
(f, g ∈B+())
with ‖f‖ = supx∈X |f(x)| (Chebyshev norm). Hence, if 𝜇is normalized and
f, g are valued on [0, 1], we obtain that the Sugeno integral is 1-Lipschitzian
for the Chebyshev norm.

140
M. Grabisch
A fundamental feature of both Choquet and Sugeno integrals is their relation with
comonotonic functions. Two functions f, g ∶X →ℝare comonotonic if there is no
x, x′ ∈X such that f(x) < f(x′) and g(x) > g(x′) (equivalently, in the case of a
ﬁnite universe, if there exists a permutation 𝜎on X such that f𝜎(1) ⩽⋯⩽f𝜎(n) and
g𝜎(1) ⩽⋯⩽g𝜎(n)).
Theorem 5 Let f, g be comonotonic functions on X (ﬁnite). Then for any game v, the
Choquet integral is comonotonically additive, and the Sugeno integral is comonoton-
ically maxitive and minitive for any fuzzy measure 𝜇:
∫(f + g) dv = ∫f dv + ∫g dv
−
∫(f ∨g) d𝜇= −
∫f d𝜇∨−
∫g d𝜇
−
∫(f ∧g) d𝜇= −
∫f d𝜇∧−
∫g d𝜇.
A more recently introduced type of additivity is called horizontal additivity (see
Šipoš [34], and Benvenuti et al. [41]). Given a function f ∶X →ℝand a constant
c ∈ℝ, the horizontal min-additive decomposition of f is:
f = (f ∧c1X) + (f −(f ∧c1X)).
This amounts to “cut” horizontally the function at level c. Similarly, the horizontal
max-additive decomposition of f is:
f = (f ∨c1X) + (f −(f ∨c1X)).
A functional I ∶ℝX →ℝis horizontally min-additive if for every f ∶X →ℝand
c ∈ℝ,
I(f) = I(f ∧c1X) + I(f −(f ∧c1X)).
Horizontal max-additivity is deﬁned similarly. It turns out that these notions are
equivalent to comonotonic additivity, as shown by Couceiro and Marichal [42]. A
related notion is horizontal median-additivity, introduced by Couceiro and Marichal
[42]. Lastly, we introduce comonotonic modularity. A functional I ∶ℝX →ℝis
modular if for every f, g ∶X →ℝ,
I(f ∨g) + I(f ∧g) = I(f) + I(g).
It can be easily shown that the Choquet integral is comonotonically modular, i.e., for
any comonotonic functions f, g it holds

Fuzzy Measures and Integrals: Recent Developments
141
∫(f ∨g) dv + ∫(f ∧g) dv = ∫f dv + ∫g dv.
This also holds for the Sugeno integral.
The next theorem clariﬁes the important case of supermodular fuzzy measures
for the Choquet integral.
Theorem 6 For any game v, the following conditions are equivalent:
(i) v is supermodular;
(ii) The Choquet integral is superadditive, that is,
∫(f + g) dv ⩾∫f dv + ∫g dv
for all f, g ∶X →ℝ
(iii) The Choquet integral is supermodular, that is,
∫(f ∨g) dv + ∫(f ∧g) dv ⩾∫f dv + ∫g dv
for all f, g ∶X →ℝ;
(iv) The Choquet integral is concave, that is,
∫(𝜆f + (1 −𝜆)g) dv ⩾∫𝜆f dv + (1 −𝜆) ∫g dv
for all 𝜆∈[0, 1], f, g ∶X →ℝ.
(v) The Choquet integral yields the lower expected value onthe core of v:
∫f dv =
min
𝜙∈𝖼𝗈𝗋𝖾(v) ∫f d𝜙,
(20)
where 𝖼𝗈𝗋𝖾(v) is the set of additive games 𝜙on X such that 𝜙(X) = v(X) and
𝜙(S) ⩾v(S) for all S ∈2X.
Lastly, we give the properties of the Sugeno integral concerning maxitivity and
minitivity.
Theorem 7 The following holds:
(i) −
∫(f ∨g) d𝜇= −
∫f d𝜇∨−
∫g d𝜇for all f, g ∈B+() if and only if 𝜇is maxitive;
(ii) −
∫(f ∧g) d𝜇= −
∫f d𝜇∧−
∫g d𝜇for all f, g ∈B+() if and only if 𝜇is minitive.

142
M. Grabisch
3.5 Characterizations
The most famous characterization of the Choquet integral is due to Schmeider [43],
whose adaptation to the ﬁnite case (|X| = n) and = 2X is as follows.
Theorem 8 Let I ∶ℝX →ℝbe a functional. Deﬁne the set function v(A) = I(1A)
on 2X. The following propositions are equivalent:
(i) I is monotone and comonotonically additive;
(ii) v is a fuzzy measure, and for all f ∈ℝN, I(f) = ∫f dv.
The discrete version (with a redundant axiom) was shown by de Campos and Bolaños
[44]. A similar characterization for the Choquet integral w.r.t. games was obtained
by Murofushi et al. [45].
In the discrete case, a characterization using comonotonic modularity was
obtained by Couceiro and Marichal [46, 47].
Theorem 9 Let |X| = n and = 2X, and let I ∶ℝX →ℝbe a functional. Deﬁne
the set function v(A) = I(1A), A ⊆X. The following propositions are equivalent:
(i) I is comonotonically modular and satisﬁes I(𝛼1S) = |𝛼|I(sign (𝛼)1S) for all
𝛼∈ℝand S ⊆X, and I(1X⧵S) = I(1X) + I(−1S);
(ii) v is a game and I(f) = ∫f dv.
The Sugeno integral was characterized in the discrete case by de Campos and
Bolaños [44]. Here follows a simpliﬁed and more general version.
Theorem 10 Let |X| = n, = 2X, and let I ∶(ℝ+)X →ℝ+ be a functional. Deﬁne
the set function 𝜇(A) = I(1A), A ⊆X. The following propositions are equivalent:
(i) I is comonotonically maxitive, satisﬁes I(𝛼1A) = 𝛼∧I(1A) for every 𝛼⩾0 and
A ⊆X, and I(1X) = 1;
(ii) 𝜇is a normalized fuzzy measure on X and I(f) = −
∫f d𝜇.
The next characterization is due to Marichal [48]. Still others can be found in this
reference.
Theorem 11 Let |X| = n, = 2X, and let I ∶[0, 1]X →[0, 1] be a functional.
Deﬁne the set function 𝜇(A) = I(1A), A ⊆X. The following propositions are equiva-
lent:
(i) I is nondecreasing, ∨-homogeneous and ∧-homogeneous;
(ii) 𝜇is a normalized fuzzy measure on X and I(f) = −
∫f d𝜇.
3.6 The Choquet Integral on the Nonnegative Real Line
As remarked by Sugeno in two recent papers [49, 50], so far there is no “Choquet
integral calculus”, similar to classical integral calculus, even if one restricts to func-
tions and measures on the real line. By means of the Laplace transform, Sugeno

Fuzzy Measures and Integrals: Recent Developments
143
established in these two papers the basis of Choquet integral calculus. For this, the
Choquet integral on a restricted domain is used:
∫A
f d𝜇= ∫
∞
0
𝜇({x ⩾t} ∩A) dt
for some A ⊆X. We give now the fundamental theorem.
Theorem 12 Let f ∶ℝ+ →ℝ+ be nondecreasing and continuously diﬀerentiable,
and let 𝜇be a continuous fuzzy measure on ℝ+, such that 𝜇([𝜏, t]) is diﬀerentiable
w.r.t. 𝜏on [0, t] for every t > 0, and 𝜇({t}) = 0 for every t ⩾0. Then
∫[0,t]
f d𝜇= −∫
t
0
𝜕𝜇
𝜕𝜏([𝜏, t])f(𝜏) d𝜏
(t > 0),
where the righthand side integral is the Riemann integral. In particular, for a dis-
torted Lebesgue measure 𝜇h with h being continuously diﬀerentiable, we obtain
∫[0,t]
f d𝜇h = ∫
t
0
𝜕h
𝜕𝜏(t −𝜏)f(𝜏) d𝜏.
(21)
Equation (21) can be computed very easily through the Laplace transform. Denoting
by −1 the inverse Laplace transform, and by H(s) and F(s) the Laplace transforms
of h and f, we have:
∫[0,t]
f d𝜇h = −1(sH(s)F(s)).
3.7 The Choquet Integral of Nonmeasurable Functions
So far we have considered that = 2X, so that every subset is measurable and con-
sequently any function is measurable too (i.e., its level sets belong to ). In the case
where ⊂2X, what about the integral of a nonmeasurable function? The question
may appear quite odd, but makes sense in practical situations, for example in multi-
criteria decision making. In this ﬁeld, X is the set of criteria and 𝜇(A) for some A ⊆X
is interpreted as the overall evaluation of an alternative being satisfactory on criteria
in A, and unsatisfactory or neutral on the others. It may be the case that such an alter-
native is not conceivable, and so no value can be assigned to 𝜇(A). However, when
computing the overall score of an alternative, knowing the vector f of its scores on
every criterion, the set A may be a level set of f (i.e., A = {x ∈X ∣f(x) ⩾t} for some
t), so that f is not measurable and its Choquet integral cannot be computed. In this
section we indicate how to extend the Choquet integral to nonmeasurable functions.
This work is based on [28].

144
M. Grabisch
Let be a ﬁxed set system. We decompose any game v on as v = v+ + v−,
where v+, v−are two totally monotone fuzzy measures:
v+ =
∑
A∈∣mv(A)>0
mv(A)uA,
v−=
∑
A∈∣mv(A)<0
(−mv(A))uA.
(22)
We ﬁrst deﬁne the Choquet integral w.r.t. a totally monotone fuzzy measure b on 
as follows (f is assumed to be nonnegative):
∫
f db = max
{
∑
A∈
𝛼Ab(A) ∣
∑
A∈
𝛼A1A ⩽f, 𝛼A ⩾0, ∀A ∈
}
(23)
= min
{
∑
i∈X
Pifi ∣
∑
i∈A
Pi ⩾b(A), ∀A ∈, Pi ⩾0, ∀i ∈X
}
.
(24)
It can be proved that this is the smallest functional I satisfying positive homogeneity,
superadditivity and I(1A) ⩾b(A) for all A ∈. Now, the Choquet integral for any
function f ∶X →ℝw.r.t. a game v is deﬁned by
∫
f dv = ∫
f dv+ −∫
f dv−.
(25)
We summarize the main properties of this integral.
Theorem 13 Let f ∶X →ℝ+ be a function and v be a game on (X, ), where is
any set system. The following properties hold.
(i) Positive homogeneity:
∫
𝛼f dv = 𝛼∫
f dv
(𝛼⩾0)
(ii) For any S ∈,
∫
f duS = min
i∈S fi
where uS is the unanimity game w.r.t. S;
(iii) If is weakly union-closed,
∫
f dv =
∑
S∈
mv(S) min
i∈S fi
where mv is the Möbius transform of v;

Fuzzy Measures and Integrals: Recent Developments
145
(iv) If is weakly union-closed,
∫
f dv = ∫f d̂v
where the right-hand side integral is the ordinary Choquet integral, and ̂v is a
game on (X, 2X) deﬁned by
̂v(S) = ∫
1S dv =
∑
F maximum in (S)
v(F)
(S ∈2X),
with (S) = {F ∈∣F ⊆S}.
(v) If is weakly union-closed, ∫⋅dv is superadditive if and only if it is concave
if and only if ̂v is supermodular.
From (iv) we see that this integral is essentially the Choquet integral w.r.t. a modiﬁed
game ̂v, and therefore inherits all of its properties. Moreover, ̂v is an extension of v in
the sense that it coincides with v on . It turns out that this integral yields the Choquet
integral for measurable functions, and is indeed an extension of the Choquet integral.
Note however that if v is monotone, ̂v is not necessarily so.
More results can be obtained if is closed under union. In this case, it can be
shown that a fuzzy measure 𝜇on is supermodular if and only if ̂𝜇is, where super-
modularity for 𝜇is deﬁned as follows: for any S, T ∈,
𝜇(S ∪T) + 𝜇((S ∩T)′) ⩾𝜇(S) + 𝜇(T),
where (S ∩T)′ is the largest subset of S ∩T in . Moreover, the following holds.
Theorem 14 Let be a set system closed under union, and 𝜇be a fuzzy measure
on (X, ). The following are equivalent:
(i) For every function f ∶X →ℝ+,
∫
f d𝜇= max
{ ∑
S∈
𝜆S𝜇(S) ∣
∑
S∈
𝜆S1S ⩽f, 𝜆⩾𝟎
}
= min
{ ∑
i∈X
Pifi ∣P(S) ⩾𝜇(S), ∀S ∈, P ⩾𝟎
}
,
where 𝟎indicates the 0 vector.
(ii) ∫⋅d𝜇is superadditive;
(iii) 𝜇is supermodular.

146
M. Grabisch
3.8 Other Integrals
We describe brieﬂy other kinds of integrals deﬁned with respect to fuzzy measures.
Pseudo-additive integrals and fuzzy t-conorm integrals It is possible to deﬁne other
integrals by simply replacing the operations used in the deﬁnitions of Choquet and
Sugeno integrals (sum, product, max, min) by other ones, generally speaking, by
pseudo-additions and pseudo-multiplications. There has been many studies in this
direction, starting from Weber [51] and Kruse [52], then later Sugeno and Murofushi
[53], Murofushi and Sugeno (fuzzy t-conorm integral) [54], Klement, Mesiar and
Pap ((S,U)-integral) [55], Benvenuti et al. [41], and more recently the impressive
study by Sander and Siedekum [56–58].
Basically, the (S, U)-integral uses as basis operators a continuous t-conorm S and
a uninorm U which is distributive w.r.t. S in the following sense:
U(x, S(y, z)) = S(U(x, y), U(x, z))
for all x, y, z ∈[0, 1] such that S(y, z) < 1.
The fuzzy t-conorm integral proposed by Murofushi and Sugeno uses three con-
tinuous t-conorms S1, S2, S3 which are either the maximum or Archimedean, plus a
pseudo-multiplication ⊙, being nondecreasing in each place, continuous on ]0, 1]2,
and satisfying a ⊙x = 0 implies either a = 0 or x = 0, and two distributivity
properties:
(D1) S1(a, b) < 1 implies (S1(a, b)) ⊙x = S3((a ⊙x), (b ⊙x))
(D2) S2(x, y) < 1 implies a ⊙(S2(x, y)) = S3((a ⊙x), (a ⊙y)).
The deﬁnition of the fuzzy t-conorm integral is then:
(S1, S2, S3, ⊙) ∫f d𝜇∶=
n
S3
i=1
(f𝜎(i)
S1−f𝜎(i−1)) ⊙𝜇(A𝜎(i))
with same notation as above, and
S1−is the residuated diﬀerence w.r.t. S1, deﬁned by
a
S1−b ∶= inf{c ∣S1(b, c) ⩾a}
for any (a, b) in [0, 1]2. The Choquet integral is recovered with S1, S−2, S3 being the
Łukasiewicz t-conorm, and ⊙the usual product. The Sugeno integral is recovered
with S1 = S2 = S3 = max and ⊙= min, and the Shilkret [59] integral is obtained
when ⊙is the ordinary product.
The integral proposed by Benvenuti et al. is similar.
Universal integrals Universal integrals, proposed by Klement et al. [60] (see also
a more recent work [61]), try to answer the following question: What is an inte-
gral w.r.t. a fuzzy measure?. The answer given by Klement et al. is axiomatic: they
propose a list of axioms a functional should satisfy to be considered as a integral.

Fuzzy Measures and Integrals: Recent Developments
147
The name “universal” comes from the fact that the integral should be deﬁned for any
measurable space (X, ) where is a 𝜎-algebra.
They ﬁrst deﬁne a pseudo-multiplication as an operator ⊗∶[0, ∞]2 →[0, ∞]
satisfying the following properties: it is nondecreasing in each place, 0 is an anni-
hilator of ⊗, i.e., a ⊗0 = 0 ⊗a = 0, and ⊗has a neutral element e ≠0, i.e.,
a ⊗e = e ⊗a = a.
Let us denote by the set of all Cartesian products (X, )×(X, ) for every
measurable space (X, ), where (X, ) is the set of fuzzy measures on (X, ),
and (X, ) is the set of -measurable functions. A functional I ∶→[0, ∞] is
called a universal integral of it satisﬁes the three following axioms:
(i) For any measurable space (X, ), its restriction to (X, ) × (X, ) is non-
decreasing in each place
(ii) There exists a pseudo-multiplication ⊗such that for all (𝜇, c ⋅1A) ∈, I(𝜇, c ⋅
1A) = c ⊗𝜇(A)
(iii) I(𝜇, f) = I(𝜇′, f ′) if G𝜇,f = G𝜇′,f ′.
Obviously, the Choquet integral and the Sugeno integrals are universal integrals.
It is not diﬃcult to see that a universal integral is a distortion of the decumulative
function by a function J begin nondecreasing and satisfying J(d ⋅1]0,c] = c ⊗d. The
Sugeno and Shilkret integrals belong to the set of smallest universal integrals (in the
sense of the usual partial order on functions), given by
I⊗(𝜇, f) =
sup
t∈]0,infty]
(t ⊗G𝜇,f (t)).
It can be shown that all integrals of the form (5), with product and addition being
replaced by a pseudo-multiplication ⊗and a pseudo-addition ⊕being continuous,
associative, nondecreasing, having 0 as neutral element and being left-distributive
w.r.t. ⊗, are universal integrals.
The concave integral and decomposition integral Recenty, in a series of papers
Lehrer presented the concave integral [62–64], and a more general concept called
the decomposition integral [65], encompassing both the concave integral and the
Choquet integral, as well as the Shilkret integral.
We ﬁrst introduce the concave integral. Let f ∶X →ℝ+ and 𝜇be a fuzzy measure.
The concave integral of f w.r.t. 𝜇is given by:
∫
cav
f d𝜇= sup
{
∑
S⊆X
𝛼S𝜇(S) ∣
∑
S⊆X
𝛼S1S = f,
𝛼S ⩾0, ∀S ⊆X
}
.
(26)
In words, the concave integral is the value achieved by the best decomposition of
the integrand into hat functions. Note that for totally monotone fuzzy measures, the
concave integral and the integral proposed by Faigle and Grabisch coincide (see
Sect. 3.7).
Its main properties are given below.

148
M. Grabisch
Theorem 15 The following properties hold for the concave integral:
(i) For every fuzzy measure 𝜇, the concave integral ∫cav ⋅d𝜇is a concave and
positively homogeneous functional, and satisﬁes ∫cav 1S d𝜇⩾𝜇(S) for all S ∈
2X;
(ii) For every f ∈ℝX
+ and fuzzy measure 𝜇,
∫
cav
f d𝜇= min {I(f) ∣I ∶ℝX
+ →ℝconcave, positively homogeneous,
and such that I(1S) ⩾𝜇(S), ∀S ⊆X}
(iii) For every f ∈ℝX
+ and fuzzy measure 𝜇,
∫
cav
f d𝜇=
min
P additive ,P⩾𝜇∫f dP
(iv) For every f ∈ℝX
+ and fuzzy measure 𝜇,
∫f d𝜇⩽∫
cav
f d𝜇,
and equality holds for every f ∈ℝX
+ if and only if 𝜇is supermodular.
Property (iv) clearly shows that unless the fuzzy measure is supermodular, the Cho-
quet integral and the concave integral diﬀer.
As for the decomposition integral, the idea is simply to ﬁx a “vocabulary” for the
decompositions. If only chains are allowed for the decomposition of a function, then
the Choquet integral obtains as the best achievable value for such decompositions.
If no restriction applies, then the concave integral is obtained. Also, the Shilkret
integral can also be recovered. We refer the reader to [65] for full details on this
complex notion.
References
1. Pap, E.: Null-Additive Set Functions. Kluwer Academic (1995)
2. Denneberg, D.: Non-Additive Measure and Integral. Kluwer Academic (1994)
3. Wang, Z., Klir, G.J.: Generalized Measure Theory. Springer (2009)
4. Pap, E. (ed.): Handbook of Measure Theory. Elsevier (2002)
5. Grabisch, M., Murofushi, T., Sugeno, M.: Fuzzy Measures and Integrals. Theory and Appli-
cations (edited volume). Studies in Fuzziness. Physica Verlag (2000)
6. Grabisch, M., Labreuche, Ch.: A decade of application of the Choquet and Sugeno integrals
in multi-criteria decision aid. Ann. Oper. Res. 175, 247–286 (2010). doi:10.1007/s10479-009-
0655-8
7. Sugeno, M.: Theory of fuzzy integrals and its applications. Ph.D. thesis, Tokyo Institute of
Technology (1974)

Fuzzy Measures and Integrals: Recent Developments
149
8. Choquet, G.: Theory of capacities. Ann. de l’Institut Fourier 5, 131–295 (1953)
9. Zadeh, L.A.: Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets Syst. 1, 3–28 (1978)
10. Dubois, D., Prade, H.: Possibility Theory. Plenum Press (1988)
11. Shafer, G.: A Mathematical Theory of Evidence. Princeton University Press (1976)
12. Stanley, R.: Two poset polytopes. Discrete Comput. Geom. 1, 9–23 (1986)
13. Radojevic, D.: The logical representation of the discrete Choquet integral. Belg. J. Oper. Res.
Stat. Comput. Sci. 38, 67–89 (1998)
14. Faigle, U., Grabisch, M.: Linear transforms, values and least square approximation for coop-
eration systems. working paper (2014)
15. Grabisch, M.: k-order additive discrete fuzzy measures and their representation. Fuzzy Sets
Syst. 92, 167–189 (1997)
16. Shapley, L.S.: A value for n-person games. In: Kuhn, H.W., Tucker, A.W. (eds.) Contributions
to the Theory of Games, Vol. II, number 28 in Annals of Mathematics Studies, pp. 307–317.
Princeton University Press (1953)
17. Murofushi, T., Soneda, S.: Techniques for reading fuzzy measures (III): interaction index. In:
9th Fuzzy System Symposium, pp. 693–696. Sapporo, Japan (1993) (In Japanese)
18. Grabisch, M.: The application of fuzzy integrals in multicriteria decision making. Eur. J. Oper.
Res. 89, 445–456 (1996)
19. de Wolf, R.: A brief introduction to Fourier analysis on the Boolean cube. Theory Comput.
Libr. Grad. Surv. 1, 1–20 (2008)
20. O’Donnell, R.: Analysis of Boolean functions, draft 2.0, ch. 1–3. http://www.cs.cmu.edu/
odonnell11/boolean-analysis (2007)
21. Hurst, S., Miller, D., Muzio, J.: Spectral Techniques in Digital Logic. Academic Press, London
(1985)
22. Grabisch, M., Miranda, P.: Exact bounds of the Möbius inverse of monotone set functions.
working paper (2013)
23. Miranda, P., Grabisch, M.: Optimization issues for fuzzy measures. Int. J. Uncertainty Fuzzi-
ness Knowl. Based Syst. 7(6), 545–560 (1999)
24. Miranda, P., Grabisch, M., Gil, P.: p-symmetric fuzzy measures. Int. J. Uncertainty Fuzziness
Knowl. Based Syst. 10(Suppl.), 105–123 (2002)
25. Miranda, P., Grabisch, M.: p-symmetric bi-capacities. Kybernetika 40(4), 421–440 (2004)
26. Faigle, U., Kern, W.: The Shapley value for cooperative games under precedence constraints.
Int. J. Game Theory 21, 249–266 (1992)
27. Algaba, E., Bilbao, J.M., Borm, P., López, J.J.: The position value for union stable systems.
Math. Meth. Oper. Res. 52, 221–236 (2000)
28. Faigle, U., Grabisch, M.: A discrete Choquet integral for ordered systems. Fuzzy Sets Syst.
168, 3–17 (2011). doi:10.1016/j.fss.2010.10.003
29. Faigle, U., Grabisch, M., Heyne, M.: Monge extensions of cooperation and communication
structures. Eur. J. Oper. Res. 206, 104–110 (2010). doi:10.1016/j.ejor.2010.01.043
30. Honda, A., Grabisch, M.: An axiomatization of entropy of capacities on set systems. Eur. J.
Oper. Res. 190, 526–538 (2008)
31. Lange, F., Grabisch, M.: Values on regular games under Kirchhoﬀ’s laws. Math. Soc. Sci. 58,
322–340 (2009). doi:10.1016/j.mathsocsci.2009.07.003
32. Barthélemy, J.-P.: Monotone functions on ﬁnite lattices: an ordinal approach to capacities,
belief and necessity functions. In: Fodor, J., De Baets, B., Perny, P. (eds.) Preferences and
Decisions under Incomplete Knowledge, pp. 195–208. Physica Verlag (2000)
33. Zhou, C.: Belief functions on distributive lattices. Artif. Intell. 201, 1–31 (2013)
34. Šipoš, J.: Integral with respect to a pre-measure. Math. Slovaca 29, 141–155 (1979)
35. Grabisch, M.: The Möbius function on symmetric ordered structures and its application to
capacities on ﬁnite sets. Discrete Math. 287(1–3), 17–34 (2004)
36. Grabisch, M.: The symmetric Sugeno integral. Fuzzy Sets Syst. 139, 473–490 (2003)
37. Singer, I.: Extensions of functions of 0–1 variables and applications to combinatorial optimiza-
tion. Numer. Funct. Anal. Optim. 7(1), 23–62 (1984)

150
M. Grabisch
38. Marichal, J.-L.: An axiomatic approach of the discrete Choquet integral as a tool to aggregate
interacting criteria. IEEE Trans. Fuzzy Syst. 8(6), 800–807 (2000)
39. Marichal, J.-L., Mathonet, P., Tousset, E.: Mesures ﬂoues déﬁnies sur une échelle ordinale.
working paper (1996)
40. Mesiar, R.: k-order pan-additive discrete fuzzy measures. In: 7th IFSA World Congress, pp.
488–490. Prague, Czech Republic (1997)
41. Benvenuti, P., Mesiar, R., Vivona, D.: Monotone set functions-based integrals. In: Pap, E. (ed.)
Handbook of Measure Theory, pp. 1329–1379. Elsevier Science (02002)
42. Couceiro, M., Marichal, J.-L.: Axiomatizations of Lovász extensions of pseudo-boolean func-
tions. Fuzzy Sets Syst. 181, 28–38 (2011)
43. Schmeidler, D.: Integral representation without additivity. Proc. Am. Math. Soc. 97(2), 255–
261 (1986)
44. de Campos, L., Bolaños, M.J.: Characterization and comparison of Sugeno and Choquet inte-
grals. Fuzzy Sets Syst. 52, 61–67 (1992)
45. Murofushi, T., Sugeno, M., Machida, M.: Non-monotonic fuzzy measures and the Choquet
integral. Fuzzy Sets Syst. 64, 73–86 (1994)
46. Couceiro, M., Marichal, J.-L.: Axiomatizations of quasi-Lovász extensions of pseudo-boolean
functions. Aequat. Math. 82, 213–231 (2011)
47. Couceiro, M., Marichal, J.-L.: Discrete integrals based on comonotonic modularity. Axioms
3, 390–403 (2013)
48. Marichal, J.-L.: On Sugeno integral as an aggregation function. Fuzzy Sets Syst. 114, 347–365
(2000)
49. Sugeno, M.: A note on derivatives of functions with respect to fuzzy measures. Fuzzy Sets
Syst. 222, 1–17 (2013)
50. Sugeno, M.: A way to choquet calculus. IEEE Trans. Fuzzy Systems, to appear
51. Weber, S.: ⊥-decomposable measures and integrals for archimedean t-conorms ⊥. J. Math.
Anal. Appl. 101, 114–138 (1984)
52. Kruse, R.: Fuzzy integrals and conditional fuzzy measures. Fuzzy Sets Syst. 10, 309–313
(1983)
53. Sugeno, M., Murofushi, T.: Pseudo-additive measures and integrals. J. Math. Anal. Appl. 122,
197–222 (1987)
54. Murofushi, T., Sugeno, M.: Fuzzy t-conorm integrals with respect to fuzzy measures : gener-
alization of Sugeno integral and Choquet integral. Fuzzy Sets Syst. 42, 57–71 (1991)
55. Klement, E.P., Mesiar, R., Pap, E.: Triangular Norms. Kluwer Academic Publishers, Dordrecht
(2000)
56. Sander, W., Siedekum, J.: Multiplication, distributivity and fuzzy integral I. Kybernetika 41,
397–422 (2005)
57. Sander, W., Siedekum, J.: Multiplication, distributivity and fuzzy integral II. Kybernetika 41,
469–496 (2005)
58. Sander, W., Siedekum, J.: Multiplication, distributivity and fuzzy integral III. Kybernetika 41,
497–518 (2005)
59. Shilkret, N.: Maxitive measure and integration. Nederl. Akad. Wetensch. Proc. Ser. A 74, 109–
116 (1971)
60. Klement, E.P., Mesiar, R., Pap, E.: A universal integral as common frame for choquet and
sugeno integral. IEEE Trans. Fuzzy Syst. 18, 178–187 (2010)
61. Klement, E.P., Mesiar, R., Spizzichino, F., Stupňanová, A.: Universal integrals based on cop-
ulas. Fuzzy Optim. Decis. Making 13, 273–286 (2014)
62. Azrieli, Y., Lehrer, E.: Extendable cooperative games. J. Public Econ. Theor. 9, 1069–1078
(2007)
63. Lehrer, E., Teper, R.: The concave integral over large spaces. Fuzzy Sets Syst. 159, 2130–2144
(2008)
64. Lehrer, E.: A new integral for capacities. Econ. Theor. 39, 157–176 (2009)
65. Even, Y., Lehrer, E.: Decomposition-integral: Unifying choquet and the concave integrals.
Econ. Theor. 56, 33–58 (2014)

Fuzzy Measures and Integrals: Recent Developments
151
Author Biography
Dr. Michel Grabisch
Dr. Michel Grabisch received his
graduate engineer diploma in 1979 from Ecole nationale des
ingénieurs Electriciens de Grenoble (ENSIEG) with specializa-
tion in nuclear engineering. He received the Ph.D. degree in
signal processing in 1982 from ENSIEG.
After a two years post-doctoral stay at Tokyo Institute
of Technology, he joined Thomson-Sintra ASM in 1984, in
the Advanced Research Group, and worked on decision theory
applied to sonar signals.
From 1989 to 1991, he was in detachment at Tokyo Insti-
tute of Technology where he conducted research on fuzzy inte-
gral, pattern recognition, fuzzy control with Prof. Sugeno. He
also participated during this time to the LIFE project (Labora-
tory for International Fuzzy Engineering Research).
From 1991 to 2000, he was at the Central Research Lab-
oratory of Thomson-CSF in projects involving fuzzy logic and decision theory.
From 2000 to 2002, he was associated professor at Université Pierre et Marie Curie, Paris.
Since 2002, he is professor in computer sciences at Université Panthéon-Sorbonne, Paris. He
is member of the Paris School of Economics. His current interests are game theory, decision
making, and discrete mathematics.
He is area editor of Annals of Operations Research, Mathematical Social Sciences, 4OR,
RAIRO.RO, TOP, Operations Research and Decisions Quarterly, IEEE Tr. on Fuzzy Systems,
Fuzzy Optimization and Decision. He was the organizer of the SING 7 Congress (Spain-Italy-
Netherlands 7th Meeting on Game Theory) in 2011 in Paris.
He has authored and co-authored about 100 papers in refereed international journals, 4
books, and more than 100 papers in refereed international conferences.

Important New Terms and Classiﬁcations
in Uncertainty and Fuzzy Logic
Madan M. Gupta and Ashu M.G. Solo
Abstract Human cognitive and perception processes have a great tolerance for
imprecision or uncertainty. For this reason, the notions of perception and cognition
have great importance in solving many decision making problems in engineering,
medicine, science, and social science as there are innumerable uncertainties in
real-world phenomena. These uncertainties can be broadly classiﬁed as either type
one uncertainty arising from the random behavior of physical processes or type two
uncertainty arising from human perception and cognition processes. Statistical
theory can be used to model the former, but lacks the sophistication to process the
latter. The theory of fuzzy logic has proven to be very effective in processing type
two uncertainty. New computing methods based on fuzzy logic can lead to greater
adaptability, tractability, robustness, a lower cost solution, and better rapport with
reality in the development of intelligent systems. Fuzzy logic is needed to properly
pose and answer queries about quantitatively deﬁning imprecise linguistic terms
like middle class, poor, low inﬂation, medium inﬂation, and high inﬂation.
Imprecise terms like these in natural languages should be considered to have
qualitative deﬁnitions, quantitative deﬁnitions, crisp quantitative deﬁnitions, fuzzy
quantitative deﬁnitions, type-one fuzzy quantitative deﬁnitions, and interval type-
two fuzzy quantitative deﬁnitions. There can be crisp queries, crisp answers, type-
one fuzzy queries, type-one fuzzy answers, interval type-two fuzzy queries, and
interval type-two fuzzy answers.
M.M. Gupta (✉)
Intelligent Systems Research Laboratory, College of Engineering,
University of Saskatchewan, Saskatoon, SK, Canada
e-mail: Madan.Gupta@usask.ca
A.M.G. Solo
Maverick Technologies America Inc, Wilmington, DE, USA
e-mail: amgsolo@mavericktechnologies.us
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_9
153

1
Introduction
For a long time, engineers and scientists have learned from nature and tried to
mimic some of the capabilities observed in humans and animals in electrical and
mechanical machines. The Wright brothers started their work on the ﬁrst airplane
by studying the ﬂight of birds. Most scientists of the time thought that it was the
ﬂapping of wings that was the principle component of ﬂying. However, the Wright
brothers realized that wings were required to increase the buoyancy in air.
In biomedical engineering, the principles of natural science and engineering are
applied to the beneﬁt of the health sciences. The opposite approach, reverse bio-
logical engineering, is used to apply biological principles to the solution of engi-
neering and scientiﬁc problems. In particular, engineers and scientists use this
reverse engineering approach on humans and animals in developing intelligent
systems.
The principle attributes of a human being can be classiﬁed in three categories
(3 Hs): hands, head, and heart. The hands category refers to the physical attributes
of humans. These physical attributes have been somewhat mimicked and somewhat
improved on to surpass the restrictive physical limitations of humans through such
mighty machines as the tractor, assembly line, and aircraft. The head category refers
to the perception and cognition abilities of the brain. The restrictive reasoning
limitations of humans have been supplemented through the ongoing development
of microprocessors. However, the challenge of creating an intelligent system is still
in its incipient stages. Finally, the heart category refers to emotions. Machines can
display simple emotional behavior, but they can’t really feel.
One of the most exciting engineering endeavors involves the effort to mimic
human intelligence. Intelligence implies the ability to comprehend, reason, mem-
orize, learn, adapt, and create. It is often said that everybody makes mistakes, but an
intelligent person learns from his mistakes and avoids repeating them. This fact of
life emphasizes the importance of comprehension, reasoning, learning, and the
ability to improve one’s performance autonomously in the deﬁnition of intelligence.
There are essentially two computational systems: carbon-based organic brains,
which
have
existed
in
humans
and
animals
since
their
inception,
and
electronics-based computers, which have rapidly evolved over the latter half of the
twentieth century and beyond. Technological advances in recent decades have
made it possible to develop computers that are extremely fast and efﬁcient for
numerical computations. However, these computers lack the abilities of humans
and animals in processing cognitive information acquired by natural sensors. For
example, the human brain routinely performs tasks like recognizing a face in an
unfamiliar crowd in 100–200 ms whereas a computer can take days to perform a
task of lesser complexity. While the information perceived through natural sensors
in humans is not numerical, the brain can process such cognitive information
efﬁciently and cause the human to act on it accordingly. Modern day computers fail
miserably in processing such cognitive information.
154
M.M. Gupta and A.M.G. Solo

This leads engineers to wonder if some of the functions and attributes of the
human sensory system, cognitive processor, and motor neurons can be emulated in
an intelligent system. For such an emulation process, it is necessary to understand
the biological and physiological functions of the brain. Hardware can be developed
to model aspects of neurons, the principle element of the brain. Similarly, new
theories and methodologies can be developed to model the human thinking process.
Many advances have been made in mimicking human cognitive abilities. These
advances were mostly inspired by certain biological aspects of the human brain. One
of the intriguing aspects of human perception and cognition is its tolerance for
imprecision and uncertainty [1–10], which characterize most real-world phenomena.
2
Certainty and Precision
The excess of precision and certainty in engineering and scientiﬁc research and
development is often providing unrealizable solutions. Certainty and precision have
much too often become an absolute standard in design, decision making, and
control problems. One of the fundamental aims in science and engineering has been
to move from perceptions to measurements in observations, analysis, and decision
making.
Through the methodology of precise measurements, engineers and scientists
have had many remarkable accomplishments. These include putting people on the
moon and returning them safely to Earth, sending spacecraft to the far reaches of the
solar system, sending rovers to explore the surface of Mars, exploring the oceans
depths, designing computers that can perform billions of computations per second,
developing the nuclear bomb, mapping the human genome, and constructing a
scanning tunneling microscope that can move individual atoms. However, the path
of precision, as manifested in the theories of determinism and stochasticism, has
often caused engineers to be ineffectual and powerless as well as lose scientiﬁc
creativity.
3
Uncertainty and Imprecision in Perception
and Cognition
The attribute of certainty or precision does not exist in human perception and cog-
nition. Alongside many startling achievements using the methodology of precise
measurements, there have been many abysmal failures that include modeling the
behavior of economic, political, social, physical, and biological systems. Engineers
have been unable to develop technology that can decipher sloppy handwriting,
recognize oral speech as well as a human can, translate between languages as well as
Important New Terms and Classiﬁcations in Uncertainty …
155

a human interpreter can, drive a motorcycle in heavy trafﬁc, walk with the agility of a
human or animal, replace the combat infantry soldier, determine the veracity of a
statement by a human subject with an acceptable degree of accuracy, replace judges
and juries, summarize a complicated document, and explain poetry or song lyrics.
Underlying these failures is the inability to manipulate imprecise perceptions instead
of precise measurements in computing methodologies.
Albert Einstein wrote, “So far as the laws of mathematics refer to reality, they
are not certain. And so far as they are certain, they do not refer to reality [11].”
There are various types of uncertainty. However, they can be classiﬁed under
two broad categories: type one uncertainty and type two uncertainty [8–10].
3.1 Type One Uncertainty
Type one uncertainty deals with information that arises from the random behavior
of physical systems. The pervasiveness of this type of uncertainty can be witnessed
in random vibrations of a machine, random ﬂuctuations of electrons in a magnetic
ﬁeld, diffusion of gases in a thermal ﬁeld, random electrical activities of cardiac
muscles, uncertain ﬂuctuations in the weather pattern, and turbulent blood ﬂow
through a damaged cardiac valve. Type one uncertainty has been studied for cen-
turies. Complex statistical mathematics has evolved for the characterization and
analysis of such random phenomena.
3.2 Type Two Uncertainty
Type two uncertainty deals with information or phenomena that arise from human
perception and cognitive processes or from cognitive information in general. This
subject has received relatively little attention. Perception and cognition through
biological sensors (eyes, ears, nose, etc.), perception of pain, and other similar
biological events throughout our nervous system and neural networks deserve
special attention. The perception and cognition phenomena associated with these
processes are characterized by many great uncertainties and cannot be described by
conventional statistical theory. A person can linguistically express perceptions
experienced through the senses, but these perceptions cannot be described using
conventional statistical theory.
Type two uncertainty and the associated cognitive information involve the
activities of neural networks. It may seem strange that such familiar notions have
recently become the focus of intense research. However, it is the relative unfa-
miliarity of these notions and their technological applications in intelligent systems
that have led engineers and scientists to conduct research in the ﬁeld of type two
uncertainty and its associated cognitive information.
156
M.M. Gupta and A.M.G. Solo

4
Fuzzy Logic
Fuzzy logic [12–37] has proven to be a very promising tool for dealing with type
two uncertainty. Stochastic theory is only effective in dealing with type one
uncertainty. The theory of fuzzy logic is based on the notion of relative graded
membership, as inspired by the processes of human perception and cognition.
LotﬁA. Zadeh published his ﬁrst famous paper on fuzzy sets [12] in 1965.
Fuzzy logic can deal with information arising from computational perception
and cognition that is uncertain, imprecise, vague, partially true, or without sharp
boundaries. Fuzzy logic allows for the inclusion of vague human assessments in
computing problems. Also, it provides an effective means for conﬂict resolution of
multiple criteria and better assessment of options. New computing methods based
on fuzzy logic can be used in the development of intelligent systems for decision
making, identiﬁcation, recognition, optimization, and control.
Measurements are crisp numbers, but perceptions are fuzzy numbers or fuzzy
granules, which are groups of objects in which there can be partial membership and
the transition of a membership function is gradual, not abrupt. A granule is a group of
objects put together by similarity, proximity, functionality, or indistinguishability.
Fuzzy logic is extremely useful for many people involved in research and devel-
opment including engineers (electrical, mechanical, civil, chemical, aerospace,
agricultural, biomedical, computer, environmental, geological, industrial, mecha-
tronics), mathematicians, computer software developers and researchers, natural
scientists (biology, chemistry, earth science, physics), medical researchers, social
scientists (economics, management, political science, psychology), public policy
analysts, business analysts, jurists, etc. Indeed, the applications of fuzzy logic, once
thought to be an obscure mathematical curiosity, can be found in many engineering
and scientiﬁc works. Fuzzy logic has been used in numerous applications such as
facial pattern recognition, washing machines, vacuum cleaners, antiskid braking
systems, transmission systems, control of subway systems and unmanned helicopters,
knowledge-based systems for multiobjective optimization of power systems, weather
forecasting systems, models for new product pricing or project risk assessment,
medical diagnosis and treatment plans, and stock trading. This branch of mathematics
has instilled new life into scientiﬁc disciplines that have been dormant for a long time.
5
Qualitative Deﬁnitions, Crisp Quantitative Deﬁnitions,
Type-One Fuzzy Quantitative Deﬁnitions, and Interval
Type-Two Fuzzy Quantitative Deﬁnitions of Imprecise
Words
Type-one fuzzy logic or interval type-two fuzzy logic [38–40] can be used to
properly quantitatively deﬁne many imprecise linguistic terms including tempera-
ture, speed, unemployment levels, and inﬂation. Fuzzy logic is needed to
Important New Terms and Classiﬁcations in Uncertainty …
157

quantitatively deﬁne imprecise linguistic terms like high unemployment, moderate
unemployment, low unemployment, very high unemployment, high inﬂation,
medium inﬂation, low inﬂation, extremely low inﬂation, fast speed, low speed, etc.
Type-one fuzzy sets and interval type-two fuzzy sets have been used for imprecise
linguistic terms in many intelligent systems applications, but this research chapter
proposes the use of type-one fuzzy sets and interval type-two fuzzy sets for the
application of posing and answering queries about quantitatively deﬁning imprecise
linguistic terms in natural languages.
An imprecise word should be considered to have qualitative deﬁnitions and
quantitative deﬁnitions [41–44].
There are multiple qualitative deﬁnitions because a word can have multiple
meanings and because different ways of deﬁning a word can be employed. That is,
different dictionaries use different descriptions to convey the meaning of the same
word.
An imprecise word should be considered to have two types of quantitative
deﬁnitions: crisp quantitative deﬁnitions and fuzzy quantitative deﬁnitions [41–44].
Crisp quantitative deﬁnitions are those made with crisp sets. There are multiple
crisp quantitative deﬁnitions because different individuals have different percep-
tions of the crisp set for imprecise words. A crisp quantitative deﬁnition of annual
inﬂation levels is in Fig. 1.
Fuzzy quantitative deﬁnitions are those made with fuzzy sets. There are multiple
fuzzy quantitative deﬁnitions because different individuals have different percep-
tions of the fuzzy set for imprecise words. Fuzzy quantitative deﬁnitions of annual
inﬂation levels are in Fig. 2 and Fig. 3.
It should be realized that while quantitative deﬁnitions of imprecise words can be
made with crisp sets or fuzzy sets, only fuzzy sets can model the imprecision of
words, so crisp sets have extremely limited value in modeling imprecise words.
An imprecise word should be considered to have two types of fuzzy quantitative
deﬁnitions: type-one fuzzy quantitative deﬁnitions and interval type-two fuzzy
quantitative deﬁnitions [44].
Type-one fuzzy quantitative deﬁnitions are those made with type-one fuzzy sets.
Figure 2 shows type-one fuzzy quantitative deﬁnitions.
Interval type-two fuzzy quantitative deﬁnitions are those made with interval
type-two fuzzy sets. Figure 3 shows interval type-two fuzzy quantitative deﬁnitions.
It is important to distinguish between qualitative deﬁnitions and quantitative
deﬁnitions, crisp quantitative deﬁnitions and fuzzy quantitative deﬁnitions, and
type-one fuzzy quantitative deﬁnitions and interval type-two fuzzy quantitative
deﬁnitions.
158
M.M. Gupta and A.M.G. Solo

6
Crisp Quantitative Deﬁnitions of Inﬂation Levels
6.1 Inﬂation Levels as Crisp Sets
Crisp sets can be arbitrarily deﬁned for low inﬂation, medium inﬂation, and high
inﬂation. These crisp sets are as illustrated in Fig. 1 and are the crisp quantitative
deﬁnitions for low inﬂation, medium inﬂation, and high inﬂation.
For annual inﬂation rates less than 2.5 %, there is a membership of 1 in the low
inﬂation crisp set and a membership of 0 in the other crisp sets. For annual inﬂation
rates between 2.5 % and 5.5 %, there is a membership of 1 in the medium inﬂation
crisp set and a membership of 0 in the other crisp sets. For annual inﬂation rates
greater than 5.5 %, there is a membership of 1 in the high inﬂation crisp set and a
membership of 0 in the other crisp sets. These crisp sets could be deﬁned with
different parameters.
With these crisp sets, an annual inﬂation rate of 2.4999 % is considered low
inﬂation whereas an annual inﬂation rate of 2.5001 is considered medium inﬂation.
This extremely sudden transition from low inﬂation to medium inﬂation for
extremely small differences in annual inﬂation doesn’t make sense and can be
rectiﬁed using type-one fuzzy sets or interval type-two fuzzy sets, as can be seen in
the next sections.
6.2 Crisp Query About Quantitatively Deﬁning Inﬂation
Levels with Crisp Sets
A single crisp query for quantitatively deﬁning annual inﬂation rates with fuzzy sets
could be articulated as follows: “Using historical data on annual inﬂation rates,
classify different annual inﬂation rates into low inﬂation, medium inﬂation, or high
inﬂation.”
Fig. 1 Crisp sets for annual
inﬂation levels.
Important New Terms and Classiﬁcations in Uncertainty …
159

6.3 Crisp Answer in Quantitatively Deﬁning Inﬂation Levels
with Crisp Sets
A crisp answer could be articulated as follows: “An annual inﬂation rate less than
2.5 % is low inﬂation. An annual inﬂation rate between 2.5 % and 5.5 % is medium
inﬂation. An annual inﬂation rate greater than 5.5 % is high inﬂation.”
7
Type-One Fuzzy Quantitative Deﬁnitions
of Inﬂation Levels
7.1 Inﬂation Levels as Type-One Fuzzy Sets
A type-one fuzzy set uses a membership function to assign a degree of membership
from 0 to 1 to each domain value. Type-one fuzzy sets can be arbitrarily deﬁned for
low inﬂation, medium inﬂation, and high inﬂation. These type-one fuzzy sets are as
illustrated in Fig. 2 and are the type-one fuzzy quantitative deﬁnitions for low
inﬂation, medium inﬂation, and high inﬂation.
For annual inﬂation rates less than 2 %, there is a membership of 1 in the low
inﬂation fuzzy set. As annual inﬂation increases from 2 % to 3 %, its membership in
the low inﬂation fuzzy set steadily decreases from 1 to 0 with constant slope and its
membership in the medium inﬂation fuzzy set steadily increases from 0 to 1 with
constant slope. For annual inﬂation rates between 3 % and 5 %, there is a mem-
bership of 1 in the medium inﬂation fuzzy set. As annual inﬂation increases from
5 % to 6 %, its membership in the medium inﬂation fuzzy set steadily decreases
from 1 to 0 with constant slope and its membership in the high inﬂation fuzzy set
steadily increases from 0 to 1 with constant slope. For annual inﬂation rates greater
than 6 %, there is a membership of 1 in the high inﬂation fuzzy set. These fuzzy sets
could be deﬁned with different parameters.
Fig. 2 Type-one fuzzy sets
for annual inﬂation levels.
160
M.M. Gupta and A.M.G. Solo

7.2 Type-One Fuzzy Query About Quantitatively Deﬁning
Inﬂation Levels with Type-One Fuzzy Sets
A type-one fuzzy query for quantitatively deﬁning annual inﬂation rates with fuzzy
sets could be articulated as follows: “Give me a range of inﬂation rates that are
deﬁnitely low inﬂation. Give me a range of inﬂation rates that are partially low
inﬂation and partially medium inﬂation. Give me a range of inﬂation rates that are
deﬁnitely medium inﬂation. Give me a range of inﬂation rates that are partially
medium inﬂation and partially high inﬂation. Give me a range of inﬂation rates that
are deﬁnitely high inﬂation.”
7.3 Type-One Fuzzy Answer About Quantitatively Deﬁning
Inﬂation Levels with Type-One Fuzzy Sets
A type-one fuzzy answer could be articulated as follows: “An annual inﬂation rate
less than 2 % is low inﬂation. An annual inﬂation rate between 2 % and 3 % is
partially low inﬂation and partially medium inﬂation. As annual inﬂation increases
from 2 % to 3 %, its degree of being low inﬂation steadily decreases and its degree
of being medium inﬂation steadily increases. An annual inﬂation rate between 3 %
and 5 % is medium inﬂation. An annual inﬂation rate between 5 % and 6 % is
partially medium inﬂation and partially high inﬂation. As annual inﬂation increases
from 5 % to 6 %, its degree of being medium inﬂation steadily decreases and its
degree of being high inﬂation steadily increases. An annual inﬂation rate greater
than 6 % is high inﬂation.”
8
Interval Type-Two Fuzzy Quantitative Deﬁnitions
of Inﬂation Levels
8.1 Type-Two Fuzzy Sets and Interval Type-Two Fuzzy Sets
A type-two fuzzy set allows the inclusion of uncertainty into the parameters of a
membership function. The membership function of a type-two fuzzy set is in itself a
fuzzy set. A type-two fuzzy set is three-dimensional where the third dimension
indicates the degree of membership of the two-dimensional membership function at
each point in its two-dimensional domain.
In a type-two fuzzy set, a footprint of uncertainty indicates the upper and lower
bounds in the two-dimensional domain of a type-two fuzzy set. A footprint of
uncertainty in a type-two fuzzy set is a region bounded by an upper membership
function and lower membership function.
Important New Terms and Classiﬁcations in Uncertainty …
161

An interval type-two fuzzy set is a type-two fuzzy set in which the third
dimension is constant in value meaning the degree of membership is constant for
the two-dimensional membership function at each point in its two-dimensional
domain. Therefore, the third dimension is ignored.
It would be extremely difﬁcult to linguistically describe an imprecise linguistic
term with a type-two fuzzy set because there is a third dimension that indicates the
degree of membership of the two-dimensional membership function at each point in
its two-dimensional domain. It is much less difﬁcult to linguistically describe an
imprecise linguistic term with an interval type-two fuzzy set because the third
dimension is constant in value and can be ignored. Because it is impractical to
attempt to linguistically describe a type-two fuzzy set for an imprecise linguistic
term, this research chapter only covers the usage of interval type-two fuzzy sets for
describing imprecise linguistic terms.
8.2 Inﬂation Levels as Interval Type-Two Fuzzy Sets
Interval type-two fuzzy sets can be arbitrarily deﬁned for low inﬂation, medium
inﬂation, and high inﬂation. These interval type-two fuzzy sets are as illustrated in
Fig. 3 and are the interval type-two fuzzy quantitative deﬁnitions for low inﬂation,
medium inﬂation, and high inﬂation.
For annual inﬂation rates less than an inﬂation rate between 1.75 % and 2.25 %,
there is a membership of 1 in the low inﬂation fuzzy set. As annual inﬂation
increases from an inﬂation rate between 1.75 % and 2.25 % to an inﬂation rate
between 2.75 % and 3.25 %, its membership in the low inﬂation fuzzy set steadily
decreases from 1 to 0 with a constant slope and its membership in the medium
inﬂation fuzzy set steadily increases from 0 to 1 with a constant slope. For annual
inﬂation rates from an inﬂation rate between 2.75 % and 3.25 % to an inﬂation rate
between 4.75 % and 5.25 %., there is a membership of 1 in the medium inﬂation
fuzzy set. As annual inﬂation increases from an inﬂation rate between 4.75 % and
5.25 % to an inﬂation rate between 5.75 % and 6.25 %, its membership in the
medium inﬂation fuzzy set steadily decreases from 1 to 0 with a constant slope and
its membership in the high inﬂation fuzzy set steadily increases from 0 to 1 with a
constant slope. For annual inﬂation rates greater than an inﬂation rate between
5.75 % and 6.25 %, there is a membership of 1 in the high inﬂation fuzzy set. These
interval type-two fuzzy sets are as illustrated in Fig. 3 and are the interval type-two
fuzzy quantitative deﬁnitions for low inﬂation, medium inﬂation, and high inﬂation.
These interval type-two fuzzy sets could be deﬁned with different parameters.
162
M.M. Gupta and A.M.G. Solo

8.3 Interval Type-Two Fuzzy Query About Quantitatively
Deﬁning Inﬂation Levels with Interval
Type-Two Fuzzy Sets
An interval type-two fuzzy query for quantitatively deﬁning annual inﬂation rates
with fuzzy sets could be articulated as follows: “Give me a range of annual inﬂation
rates below which there is deﬁnitely low inﬂation. Give me a range of annual
inﬂation rates between which there is partially low inﬂation and partially medium
inﬂation. Give me a starting range and ending range of annual inﬂation rates
between which there is deﬁnitely medium inﬂation. Give me a range of annual
inﬂation rates between which there is partially medium inﬂation and partially high
inﬂation. Give me a range of annual inﬂation rates above which there is deﬁnitely
high inﬂation.”
8.4 Interval Type-Two Fuzzy Answers About Quantitatively
Deﬁning Inﬂation Levels with Interval
Type-Two Fuzzy Sets
An interval type-two fuzzy answer could be articulated as follows: “An annual
inﬂation less than an inﬂation rate between 1.75 % and 2.25 % is low inﬂation. As
annual inﬂation increases from an inﬂation rate between 1.75 % and 2.25 % to an
inﬂation rate between 2.75 % and 3.25 %, there is partially low inﬂation and
partially medium inﬂation. As annual inﬂation increases from an inﬂation rate
between 1.75 % and 2.25 % to an inﬂation rate between 2.75 % and 3.25 %, its
degree of being low inﬂation steadily decreases and its degree of being medium
inﬂation steadily increases. An annual inﬂation rate between an inﬂation rate
between 2.75 % and 3.25 % to an inﬂation rate between 4.75 % and 5.25 % is
Fig. 3 Interval type-two
fuzzy sets for annual inﬂation
levels.
Important New Terms and Classiﬁcations in Uncertainty …
163

medium inﬂation. An annual inﬂation from an inﬂation rate between 4.75 % and
5.25 % to an inﬂation rate between 5.75 % and 6.25 % is partially medium inﬂation
and partially high inﬂation. As annual inﬂation increases from an inﬂation rate
between 4.75 % and 5.25 % to an inﬂation rate between 5.75 % and 6.25 %, its
degree of being medium inﬂation steadily decreases and its degree of being high
inﬂation steadily increases. An annual inﬂation rate greater than an inﬂation rate
between 5.75 % and 6.25 % is high inﬂation.”
9
Conclusion
Uncertainty is an inherent phenomenon in the universe and in peoples’ lives. To
some, it may become a cause of anxiety, but to engineers and scientists it becomes a
frontier full of challenges. Engineers and scientists attempt to comprehend the
language of this uncertainty through mathematical tools, but these mathematical
tools are still incomplete. In the past, studies of cognitive uncertainty and cognitive
information were hindered by the lack of suitable tools for modeling such infor-
mation. However, fuzzy logic, neural networks, and other methods have made it
possible to expand studies in this ﬁeld. Whereas stochastic theory is effective in
dealing with type one uncertainty, fuzzy logic is needed for type two uncertainty.
Humans think in imprecise and vague terms. Consequently, human language is
inherently imprecise and vague. A major problem arises when people try to bring
precision into situations where it doesn’t apply, such as deﬁning human linguistic
terms like high inﬂation as being greater than a single precise annual income. An
understanding of the basic principles of type-one fuzzy logic and interval type-two
fuzzy logic can be extremely useful in posing proper questions and giving proper
answers about quantitatively deﬁning imprecise linguistic terms. Imprecise lin-
guistic terms in natural languages should be considered to have qualitative deﬁ-
nitions, crisp quantitative deﬁnitions, fuzzy quantitative deﬁnitions, type-one fuzzy
quantitative deﬁnitions, and interval type-two fuzzy quantitative deﬁnitions.
Crisp queries, crisp answers, and crisp quantitative deﬁnitions are simpler than
type-one fuzzy queries, type-one fuzzy answers, and type-one fuzzy quantitative
deﬁnitions. It’s easier to deﬁne an imprecise linguistic term with a crisp set than
with a type-one fuzzy set, but a type-one fuzzy set allows for the inclusion of
uncertainty in a membership function. If one wants to include uncertainty in a
membership function, then a type-one fuzzy set should be used.
Type-one fuzzy queries, type-one fuzzy answers, and type-one fuzzy quantita-
tive deﬁnitions are simpler than interval type-two fuzzy queries, interval type-two
fuzzy answers, and interval type-two fuzzy quantitative deﬁnitions. It’s easier to
deﬁne an imprecise linguistic term with a type-one fuzzy set than with an interval
type-two fuzzy set, but an interval type-two fuzzy set allows for the inclusion of
uncertainty about the bounds of the membership function. If one wants to include
uncertainty about the bounds of the membership function in a quantitative deﬁnition
of an imprecise linguistic term, then an interval type-two fuzzy set should be used.
164
M.M. Gupta and A.M.G. Solo

References
1. Gupta, M.M.: Cognition, Perception and Uncertainty. In: Gupta, M.M., Yamakawa, T. (eds.)
Fuzzy Logic in Knowledge-Based Systems, Decision and Control, pp. 3–6. North-Holland,
New York (1988)
2. Gupta, M.M.: On Cognitive Computing: Perspectives. In: Gupta, M.M., Yamakawa, T. (eds.)
Fuzzy Computing: Theory, Hardware, and Applications, pp. 7–10. North-Holland, New York
(1988)
3. Gupta, M.M.: Uncertainty and Information: The Emerging Paradigms. Int. J. Neuro Mass
Parallel Comput. Inf. Syst. 2, 65–70 (1991)
4. Gupta, M.M.: Intelligence, Uncertainty and Information. In: Ayyub, B.M., Gupta, M.M.,
Kanal, L.N. (eds.) Analysis and Management of Uncertainty: Theory and Applications,
pp. 3–12. North-Holland, New York (1992)
5. Ayyub, M.,Gupta, M.M. (eds.): Uncertainty Analysis in Engineering and Sciences: Fuzzy
Logic, Statistics and Neural Networks Approach. Kluwer Academic, Boston (1997)
6. Klir, G.J.: Where Do We Stand on Measures of Uncertainty, Ambiguity, Fuzziness and the
Like. Fuzzy Sets Syst. 24(2), 141–160 (1987). Special Issue on Measure of Uncertainty
7. Klir, G.J.: The Many Faces of Uncertainty. In: Ayyub, B.M., Gupta, M.M. (eds.) Uncertainty
Modelling and Analysis: Theory and Applications, pp. 3–19. North-Holland, New York (1994)
8. Solo, A.M.G. Gupta,M.M.: Perspectives on Computational Perception and Cognition under
Uncertainty. In: Proceedings of IEEE International Conference on Industrial Technology
(ICIT) 2000, Taleigaon, Goa, India, vol. 1, issue 2, pp. 221–224, 19-22 Jan 2000
9. Solo, A.M.G., Gupta, M.M.: Uncertainty in Computational Perception and Cognition. In:
Nikravesh, M., Kacprzyk, J., Zadeh, L.A. (eds.) Forging New Frontiers: Fuzzy Pioneers I:
Studies in Fuzziness and Soft Computing, pp. 251–266. Springer Verlag, New York (2007)
10. Gupta, M.M., Solo, A.M.G.: On the Morphology of Uncertainty in Human Perception and
Cognition. In: Proceedings of the First Interdisciplinary CHESS Interactions Conference
pp. 257–271. World Scientiﬁc, Hackensack, N.J. (2010)
11. Einstein, A.: Geometry and Experience. In: The Principle of Relativity: A Collection of
Original Papers on the Special and General Theory of Relativity. Dover, New York (1952)
12. Zadeh, L.A.: Fuzzy Sets. Inf. Control 8, 338–353 (1965)
13. Zadeh, L.A.: A fuzzy-set-theoretic interpretation of linguistic hedges. J. Cybern. 2, 4–34 (1972)
14. Zadeh, L.A.: Outline of a new approach to the analysis of complex system and decision
processes. IEEE Trans. Syst. Man Cybern. 3, 28–44 (1973)
15. Zadeh, L.A.: Calculus of fuzzy restrictions. In: Zadeh, L.A., Fu, K.S., Shimura, M. (eds.)
Fuzzy Sets and Their Applications to Cognitive and Decision Processes, pp. 1–39. Academic,
New York (1975)
16. Zadeh, L.A.: The concept of a linguistic variable and its application to approximate reasoning.
Part I: Information Science, vol. 8, 199–249, Part II: Information Science, vol. 8, 301–357,
Part III: Information Science, vol. 9, 43–80
17. Zadeh, L.A.: Fuzzy sets and information granularity. In: Gupta, M.M., Ragade, R., Yager, R.
(eds.) Advances in Fuzzy Set Theory and Applications, pp. 3–18. North-Holland, New York
(1979)
18. Zadeh, L.A.: A theory of approximate reasoning. In: Hayes, J., Michie, D., Mikulich, L.I.
(eds.) Machine Intelligence, vol. 9, pp. 149–194. Halstead, New York (1979)
19. Zadeh, L.A.: Outline of a computational approach to meaning and knowledge representation
based on the concept of a generalized assignment statement. In: Proceedings of the
International Seminar on Artiﬁcial Intelligence and Man-Machine Systems, pp. 198–211
(1986)
20. Zadeh, L.A.: Fuzzy logic, neural networks, and soft computing. Commun. ACM 37(3), 77–84
(1994)
21. Zadeh, L.A.: Fuzzy logic and the calculi of fuzzy rules and fuzzy graphs: a précis. Multiple
Valued Logic 1, Gordon and Breach Science, pp. 1–38 (1996)
Important New Terms and Classiﬁcations in Uncertainty …
165

22. Zadeh, L.A.: Toward a theory of fuzzy information granulation and its centrality in human
reasoning and fuzzy logic. Fuzzy Sets Syst. 90, 111–127 (1997)
23. Zadeh, L.A.: Outline of a Computational Theory of Perceptions Based on Computing with
Words. In: Sinha, N.K., Gupta, M.M. (eds.) Soft Computing & Intelligent Systems: Theory
and Applications, pp. 3–22. Academic, New York (2000)
24. Gupta, M.M., Jin, L., Homma, N.: Fuzzy Sets and Systems: An Overview, in Static and
Dynamic Neural Networks: From Fundamentals to Advanced Theory, pp. 636–644. Wiley,
Hoboken, N.J. (2003)
25. Gupta, M.M., Saridis, G.N., Gaines, B.R. (eds.): Fuzzy Automata and Decision Processes.
Elsevier North-Holland, New York (1977)
26. Gupta,
M.M.,
Sanchez,
E.
(eds.):
Approximate
Reasoning
in
Decision
Analysis.
North-Holland, New York (1982)
27. Gupta, M.M., Sanchez, E. (eds.): Fuzzy Information and Decision Processes. North-Holland,
New York (1983)
28. Gupta, M.M., Kandel, A., Bandler, W., Kiszka, J.B. (eds.): Approximate Reasoning in Expert
Systems. North-Holland, New York (1985)
29. Kaufmann, A., Gupta, M.M.: Introduction to Fuzzy Arithmetic: Theory and Applications. Van
Nostrand Reinhold, New York (1985)
30. Kaufmann, A., Gupta, M.M.: Fuzzy Mathematical Models in Engineering and Management
Science. North-Holland, Amsterdam (1988)
31. Mitra, S., Gupta, M.M., Kraske, W. (eds.): Neural and Fuzzy Systems: The Emerging Science
of Intelligent Computing. International Society for Optical Computing (SPIE) (1994)
32. Li, H., Gupta, M.M. (eds.): Fuzzy Logic and Intelligent Systems. Kluwer Academic, Boston
(1995)
33. Sinha, N.K., Gupta, M.M. (eds.): Soft Computing & Intelligent Systems: Theory and
Applications, pp. 3–22. Academic, New York (2000)
34. Solo, A M.G.: Fuzzy Grading: Fuzzy Logic for Uncertainty Management of Linguistic
Evaluations. In: Proceedings of the 2010 International Conference on e-Learning, e-Business,
Enterprise Information Systems, and e-Government (EEE’10) pp. 271–276 (2010)
35. Singh, H., Gupta, M.M., Meitzler, T., Hou, Z.-G., Garg, K.K., Solo, A.M.G. (eds.): Real-Life
Applications of Fuzzy Logic. Advances in Fuzzy Sets and Systems. Hindawi, New York
(2013). http://www.hindawi.com/journals/afs/si/537295/. Accessed 31 Dec 2013
36. Kosko, B.: Fuzzy Thinking: The New Science of Fuzzy Logic. Hyperion, New York (1993)
37. Kosko, B.: Heaven in a Chip: Fuzzy Visions of Society and Science in the Digital Age. Three
Rivers, New York (1999)
38. Mendel, J.M., John, R.I.: Type-2 Fuzzy Sets Made Simple. IEEE Trans. Fuzzy Syst.
10, 117–127 (2002)
39. Mendel, J.M., John, R.I., Liu, F.: Interval Type-2 Fuzzy Logic Systems Made Simple. IEEE
Trans. Fuzzy Syst. 14, 808–821 (2006)
40. Mendel, J.M.: Type-2 Fuzzy Sets and Systems: An Overview. IEEE Comput. Intell. Mag.
2, 20–29 (2007)
41. Solo, A.M.G., Gupta, M.M., Homma, N., Hou, Z.-G.: Obama, McCain, and Warren Needed
Fuzzy Logic to Deﬁne ‘Rich’ by Income. In: Proceedings of the 2009 International
e-Learning, e-Business, Enterprise Information Systems, and e-Government (EEE’09), Las
Vegas, pp. 265–270, 13–16 July 2009
42. Solo, A.M.G.: Warren, McCain, and Obama Needed Fuzzy Sets at Presidential Forum.
Advances in Fuzzy Sets and Systems. Hindawi, New York (2013). http://www.hindawi.com/
journals/afs/2012/319718/. Accessed 31 Dec 2013
43. Solo, A.M.G., Gupta, M.M., Homma, N., Hou, Z.-G.: Type-One Fuzzy Logic for
Quantitatively Deﬁning Imprecise Linguistic Terms in Politics and Public Policy. In: Solo,
A.M.G. (ed.) Political Campaigning in the Information Age. IGI Global, Hershey, Penn (2014)
44. Solo, A.M.G.: Interval Type-Two Fuzzy Logic for Quantitatively Deﬁning Imprecise
Linguistic Terms in Politics and Public Policy. In: Solo, A.M.G. (ed.) Political Campaigning
in the Information Age. IGI Global, Hershey, Penn (2014)
166
M.M. Gupta and A.M.G. Solo

Authors Biography
Dr. Madan M. Gupta is a professor (Emeritus) and is holding
the Distinguished Research Chair in the College of Engineering
at the University of Saskatchewan. Also, he is the director of the
Intelligent Systems Research Laboratory.
Dr. Gupta has authored or co-authored over 900 research
papers. He co-authored the seminal book entitled Static and
Dynamic Neural Networks: From Fundamentals to Advanced
Theory. Dr. Gupta has previously co-authored Introduction to
Fuzzy Arithmetic: Theory and Applications (the ﬁrst book on
fuzzy arithmetic) and Fuzzy Mathematical Models in Engineering
and Management Science. Both of these books have been
translated into Japanese. Also, Dr. Gupta has edited or co-edited
about 25 other books as well as conference proceedings and
journals in the ﬁelds of his research interests such as adaptive
control
systems,
fuzzy
computing,
neuro-computing,
neuro-vision systems, and neuro-control systems.
Dr. Gupta was elected fellow of the Institute of Electrical and Electronics Engineers (IEEE) for
his contributions to the theory offuzzy sets and adaptive control systems and for the advancement on
the diagnosis of cardiovascular disease. He was elected fellow of the International Society for
Optical Engineering (SPIE) for his contributions to the ﬁeld of neuro-control and neuro-fuzzy
systems. Also, he was elected fellow of the International Fuzzy Systems Association (IFSA) for his
contributions to fuzzy-neural computing systems. In 1998, Dr. Gupta was honored by the III-
Kaufmann Prize and Gold Medal for his research in the ﬁeld of fuzzy logic. This Gold Medal was
presented by the Foundation FEGI (Fundacio per a l’Estudi de la Gestio en la Incertesa: Fuzzy
Management Research Foundation) and SIGEF (Sociedad Internacional de Gestion Economia:
Fuzzy, International Association for Fuzzy Set Management and Economy) in Reus, Spain. In 1991,
Dr. Gupta was the co-recipient of the Institute of Electrical Engineering Kelvin Premium.
Ashu M. G. Solo is an interdisciplinary researcher and
developer, electrical engineer, computer engineer, intelligent
systems engineer, political and public policy engineer, mathe-
matician, public policy analyst, political operative, writer,
entrepreneur, former infantry platoon commander understudy,
and progressive activist.
Solo has over 550 research and political commentary publi-
cations. He is the creator of multidimensional matrix mathemat-
ics and its subsets, multidimensional matrix algebra and
multidimensional matrix calculus, all of which are published.
He is the originator of public policy engineering, computational
public policy, political engineering, computational politics, and
network politics, all of which are published. He co-developed
some of the best published methods for maintaining power ﬂow
in and multiobjective optimization of radial power distribution
system operations using intelligent systems.
Important New Terms and Classiﬁcations in Uncertainty …
167

Solo is the principal of Maverick Technologies America Inc. and Trailblazer Intelligent
Systems, Inc. He previously worked in nine different research and development labs in universities
and companies. He has served on 212 international program committees for research conferences.
He is a fellow of the British Computer Society. He won two Outstanding Achievement Awards,
two Distinguished Service Awards, and three Achievement Awards from research conferences.
168
M.M. Gupta and A.M.G. Solo

Formalization and Visualization of Kansei
Information Based on Fuzzy Set Approach
Fangyan Dong and Kaoru Hirota
Abstract Kansei or affective-computing related information is easy to express in
terms of fuzzy sets. Three examples of Kansei information, e.g., emotion, atmo-
sphere, and Kansei texture, are formalized by using fuzzy set concept on [−1,1]3
space. They are also visualized by using shape-brightness-size, shape-color-size,
and contour-shape-gradation models, respectively. Their applications to agent to
agent communication, multiagent communication, and online shopping are also
introduced.
1
Introduction
Kansei engineering has been studied originally in Japan and nowadays in worldwide,
and is sometimes referred to affective computing. Its main purpose is to introduce
information processing capability related to human ambiguity or subjectivity in the
computer science/engineering ﬁeld, accordingly it has a good matching with fuzzy
set approach. The authors’ group has been studying Kansei information processing
in various IT ﬁelds. In this article, three topics are introduced, i.e., emotion under-
standing in man-machine interaction, atmosphere analysis/understanding in humans-
robots interaction, and Kansei texture in online shopping.
F. Dong (✉) ⋅K. Hirota
Tokyo Institute of Technology, 4259 Nagatsuta, Midori-Ku, Yokohama 226-8502, Japan
e-mail: tou@acls.titech.ac.jp
K. Hirota
e-mail: hirota@hrt.dis.titech.ac.jp
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_10
169

2
Emotion Understanding in Man-Machine Interaction
Emotion understanding has been studied in man-machine, human-robot, or gener-
ally agent-agent interaction using different type of devices. The ones that are more
close to human way of understand the emotions are those which are based on voice,
face, and gesture information [1, 2]. But the part missing with these approaches is a
lack of experience [3]; learning from the interaction and creating knowledge is what
gives humans the power to understand deeply the emotions of other person.
Humans emotions are complex, and in many situations the emotion displayed in the
face, voice or body gesture sometimes may not indicate the real or absolute emotion
of the individuals [2], making the necessity to create an algorithm to model this
human ability to improve human-robot interaction [4]. To address and to make a
model of this problem, understanding by using information from face, voice,
gesture, and others is called surface level emotion understanding, whereas a deep
level emotion understanding is also proposed [5], where customized learning
knowledge from communication history and a basic knowledge base about the
observed agent are utilized with the observed visual/acoustic/gesture information
input (Fig. 1).
Fig. 1 Concept of deep level emotion understanding
170
F. Dong and K. Hirota

There are many proposals to represent the emotion in so called emotion space.
The resulting emotions are displayed in the arousal pleasure-afﬁnity space [6] as
shown in Fig. 2 to understand where the emotion and the intension is placed, which
is deﬁned as
E = e affinity, e pleasure, e arousal


e affinity, e pleasure, e arousal∈−1, 1
½
,
ð1Þ
where E is the emotion state vector, e_afﬁnity, e_pleasure, and e_arousal are the values
for “Afﬁnity- No-afﬁnity”, “Pleasure-Displeasure”, and “Arousal-Sleep” axes,
respectively. The E is a 3D vector in [-1,1]3 as shown in the emotion centroid in
Fig. 2. But the human emotion is complex and sometimes varies according to the
situation. So it maybe natural to represent the emotion by a fuzzy set as shown by
the cone (generally a distorted cone) in Fig. 2. The emotion represented as a fuzzy
set has generally a complex shaped membership function, but it may be possible to
approximate the complex shaped membership function by an emotion centroid, i.e.,
an average vector, and emotion standard deviation, i.e., a standard deviation vector
in [0,1]3 whose component indicates the standard deviation of the distorted cone
along each axis.
The most important information is indicated by the average vector in [-1,1]3 and
is illustrated by a visualization method using shape-brightness-size model as shown
in Fig. 3. For the pleasure-displeasure axis [-1, 1], some meaningful shapes are
accepted. Based on the culture in Japan where the authors’ group are studying,
Fig. 2 Afﬁnity Pleasure - Arousal space [6]
Formalization and Visualization of Kansei Information …
171

circle represents a positive or good answer, while the X-shape represents a negative
or bad answer. That makes the shape a suitable way to represent the pleasure (= 1)
or displeasure (= -1). In between displeasure and pleasure, continuous deformed
shape from X to circle is used as shown in the upside of Fig. 3.
Brightness inside of the shape is accepted to represent the arousal–sleep axis [-1,
1]. White is the brightest color that denotes vivid, activeness, and arousal (= 1),
while black is the darkest color that denotes gloom, passiveness, and sleep (= -1).
The degree from sleep to arousal is expressed by gray level degree as shown in the
middle of Fig. 3. For the afﬁnity- no-afﬁnity axis [-1, 1], the size of the shape is
used from the smallest in the case of no-afﬁnity (= -1) to the full size in afﬁnity (=
1) as indicated in the bottom of Fig. 3.
A scenario is created to demonstrate the concept of the proposed method, where
the communication is done between a human employee (observed agent) and a
robot secretary (emotion observer) in a company as shown in Fig. 4. The topic is a
meeting room reservation requested by the employee to the secretary followed by
the reservation change because of the employee’s mistake. The employee’s face/
voice/body-gesture are captured by Kinect attached to the robot secretary. They
provide the surface level emotion of the employee to the secretary robot by using
three neural networks, and the deep level emotion is inferred by the secretary robot
using fuzzy inference with the customized knowledge about the employee. The
result is shown in both a vector in [-1,1]3 (bottom left in Fig. 4) and the visuali-
zation method (bottom right in Fig. 4).
Fig. 3 Visualization of emotion
172
F. Dong and K. Hirota

3
Atmosphere Analysis/Understanding
in Humans-Robots Interaction
To make a smooth communication in human-robot/machine or generally agent to agent
interaction, understanding the emotion of others is important. In the case of many to
many agents communication, however, the atmosphere of the society may provide more
important information than the emotion of each agent. Although many studies have
been done on the emotion from viewpoints of cognitive science or human-machine
interface, the atmosphere generated by the communication society/ﬁeld by many agents
has not been studied enough. The authors’ group at Tokyo Institute of Technology has
been studied on many robots and many humans communication through internet, where
the atmosphere of the communication ﬁeld/society by many (huge number of) indi-
viduals plays an important role for the smooth communication [7].
The concept of Fuzzy Atmosﬁeld (FA), is proposed to express the atmosphere in
such humans-robots communication ﬁeld/society [8]. The “Atmosﬁeld” is a new
word from “atmosphere” and “ﬁeld”, and is created by the authors’ group. It is
characterized by a 3D fuzzy cubic space [-1,1]3 as shown in Fig. 5 with “friendly-
hostile”, “lively-calm”, and “casual-formal” axes by doing a cognitive science
experiments and applying principle component analysis.
Fig. 4 Communication between employee and secretary robot
Formalization and Visualization of Kansei Information …
173

The atmosphere in the communication ﬁeld/society is expressed by a point in the
3D fuzzy cubic space [-1,1]3 and maybe varying/moving in the space time by time.
To understand easily such movement of the atmosphere, a graphical representation
method is also proposed as shown in Fig. 6 by using a shape-color-size model,
where “friendly-hostile” information is represented by “shape”, “lively-calm” by
“color”, and “casual-formal” by “size”.
To illustrate the FA and its visualization method, a demonstration scenario
“enjoying home party by using a Mascot Robot System is introduced/performed.
The Mascot Robot System consists of 5 robots, i.e., 4 ﬁxed robots (placed on a TV,
a darts game machine, an information terminal, and a mini-bar) and 1 mobile robot
(Fig. 7). Each of them includes an eye robot, a speech recognition module, and a
notebook PC that controls the robot and the speech recognition module. These
robots are connected together with a server through the internet by RTM (Robot
Technology Middleware developed by AIST, Japan), thus constituting the Robot
System. The Mascot Robot System’s functioning is demonstrated in an ordinary
living room, where casual communication between 5 robots and 4 human beings
(1 host, 2 guests, and 1 walk-in) is conducted based on speech recognition and
mentality expression of eye robots.
Fig. 5 Fuzzy atmosﬁeld
174
F. Dong and K. Hirota

An example scene of Demonstration Video “Enjoying Home Party” is shown in
Fig. 8, where the atmosphere information is indicated in the top center (3D vector
value) and top right (visualized illustration by a shape-color-size model).
4
Kansei Texture in Online Shopping
The online shopping market size becomes doubled in the last 10 years, because
customers can easily purchase various kinds of products anytime and anywhere. In
the online shopping, however, the customers have to imagine the sensation of the
product from a few photos, price, speciﬁcation, reviews, and so on. Therefore, the
quality of the delivered product is sometimes different from imaged one. On the
other hand, when customers purchase a product in the shop, they actually can
observe and take it in their hand, and they can select suitable one based on their
feeling about the value, the tactile sensation, the textures, and so on. It means that
there exists information gap between real shop and online shop. In order to com-
pensate the information gap in online shopping, Kansei Texture which adds new
information on the present net shopping is proposed [9].
Fig. 6 Visualization of fuzzy atmosﬁeld vector
Formalization and Visualization of Kansei Information …
175

The Kansei Texture (“Shokushitsu-kan” in Japanese language) is deﬁned as the
quality index of the feeling information on the tactile or vision sense when people
see the photo/movie image of a product or a real object.
Firstly, many kinds of expression terms which contains the amount of feelings
like onomatopoeia are gathering from the photos of the products. The expression
terms are changed in the amount of feelings which are characterized by 5 tactile
sensations in [-1, 1] scale, i.e., roughness, hardness, dryness, warmness, and
glossiness, based on the result of the subjectivity evaluation questionnaire. The
Kansei Texture is ﬁnally represented in 3-dimensional [-1,1]3 space condensed
from 5-dimensional [-1,1]5 space, and Kansei Texture of a product is shown by the
combination of each value of new deﬁned 3 axes, i.e., “PuruPuru - GotsuGotsu”,
“KachiKachi - FuwaFuwa”, and “ButsuButsu - PikaPika” as shown in Fig. 9.
Fig. 7 Mascot robot system
176
F. Dong and K. Hirota

Fig. 8 An example of “enjoying home party”
Fig. 9 Kansei texture space
[-1,1]3
Formalization and Visualization of Kansei Information …
177

Again, it maybe not easy for general customers in online shopping to understand
3D vectors in [-1,1]3, instead an easily understandable visualization method is
developed [10]. For this purpose a contour-shape-gradation model is used as shown
in Fig. 10. A program has been developed to generate the visualization illustration
by inputting the 3D vector information in the Kansei texture space [-1,1]3 as shown
in Fig. 11. Several illustration examples generated by the program are shown in
Fig. 12.
Fig. 10 Visualization of Kansei texture by CSG model
Fig. 11 Visualization program of Kansei texture
178
F. Dong and K. Hirota

The Kansei texture information is visualized in the online shopping screen as
shown in Fig. 13 (bottom right). The customers are able to understand the Kanse
texture about the good from the visualized Kansei texture information and to
imagine the tactile quality of the good with the photo and the text data.
Fig. 13 Online shopping screen with Kansei texture information
Fig. 12 Examples of visualization
Formalization and Visualization of Kansei Information …
179

5
Conclusions
Formalization procedures of Kansei information are introduced in terms of fuzzy set
approach. Three examples of Kansei information are shown, i.e., emotion, atmo-
sphere, and Kansei texture for the application purposes of man-machine interaction,
humans-robots interaction, and online shopping, respectively. The visualization
methods of the Kansei information are also presented.
References
1. Bethel C.L.: Survery of psychophysiology measurements applied to human-robot interaction.
In: The 16th IEEE International Symposium on Robot and Human interactive Communication,
pp. 732–737 (2007)
2. Fontaine, J.R., Scherer, K.R., Roesch, E.B., Ellsworth, P.C.: The World of Emotions is not
Two-Dimensional. Psychol. Sci. 18(12), 1050–1057 (2007)
3. Kazemifard M., Ghasem-Aghaee N., Koenig B.L., Ören T.I.: An Emotion Understanding
Framework for Intelligent Agents Based on Episodic and Semantic Memories. Autonomous
Agents and Multi-Agent Systems, Springer US, (2013). doi: 10.1007/s10458-012-9214-9
4. Cañamero, L.: Emotion understanding from the perspective of autonomous robots research.
Neural Netw. Emot. Brain 18(4), 445–455 (2005)
5. Garcia, A., Ohnishi, K., Shibata, A., Dong, F., Hirota, K.: Visualization Method of Emotion
Information for Long Distance Interaction, 7th IEEE International Conference Humanoid,
Nanotechnology, Information Technology Communication and Control, Environment and
Management (HNICEM2014). Palawan, Philippine (2014)
6. Yamazaki, Y., Hatakeyama, Y., Dong, F., Nomoto, K., Hirota, K.: Fuzzy Inference based
Mentality Expression for Eye Robot in Afﬁnity Pleasure-Arousal Space. J. Adv. Comput.
Intell. Intell. Inf. 12(3), 304–313 (2008)
7. Hirota K.: Casual communication between robots and humans based on fuzzy atmosﬁeld.
In: 9th International Conference on Information and Management Sciences (IMS2010)
(Plenary Talk), (Urumuchi, China) (2010/8)
8. Liu Z.T., Wu M., Li D.Y., Chen L.F., Dong F.Y., Yamazaki Y., Hirota K.: Concept of fuzzy
atmosﬁeld for representing communication atmosphere and its application to humans-robots
interaction. J. Adv. Comput. Intell. Intel. Inf. 17(1), 3/17 (2013/1)
9. Sakaniwa H., Seki M., Dong F., Hirota K.: KANSEI TEXTURE for remote object image and
its visualization method (in Japanese language). Trans. Japan Soc. Kansei Eng. 13(1), 281/288
(2014)
10. Sakaniwa H., Dong F., Hirota K.: Fuzzy set representation of Kansei texture for online shopping
(best paper awarded). In: The Joint International Conference of Information Technology and
Control Applications 2014 and International Symposium on Computational Intelligence and
Industrial Applications 2014 (ITCA&ISCIIA2014), (Changsha, China), pp. 1/7. Accessed
15–20 Sept 2014
180
F. Dong and K. Hirota

Authors Biography
Fangyan Dong born in Shenyang PRC, Doctor of Engineering
(Tokyo Institute of Technology), Computational Intelligence,
Associate Professor at Tokyo Institute of Technology, Japan
Society of Fuzzy Theory and Intelligent Informatics, More than
10 Best Papers Awards
Kaoru Hirota born in Niigata Japan, Doctor of Engineering
(Tokyo Institute of Technology), Computational Intelligence,
Professor at Tokyo Institute of Technology, Japan Society of
Fuzzy Theory and Intelligent Informatics, More than 20 Awards
(Best Paper/Honoris Causa/Honorary Professorship)
Formalization and Visualization of Kansei Information …
181

Cognitive Informatics: A Proper Framework
for the Use of Fuzzy Dynamic Programming
for the Modeling of Regional Development?
Janusz Kacprzyk
Abstract We advocate Wang’s cognitive informatics as a potentially powerful gen-
eral approach and paradigm to formulate, analyze and solve human centric sys-
tems modeling,decision and control problems. We show the use of fuzzy dynamic
programming for solving a regional development problem in which many crucial
aspects, in particular life quality indicators, are subject to objective and subjective,
by the humans, judgments and evaluations which are closely related to human per-
ceptions and cognitive abilities. We consider how a best (optimal) investment policy
can be obtained under diﬀerent development scenarios.
1 Introduction
The main purpose of this paper is to indicate a potential of Wang’s [26, 27] (cf.
also Wang et al. [29–33] cognitive informatics for providing a novel perspective
through which some decision making and control applications can be viewed. To
be more speciﬁc, we consider the use of multistage decision making (control) under
fuzzy constraints and goals, notably by employing fuzzy dynamic programming (cf.
Kacprzyk [10]) to regional development planning. That new perspective, should
considerably enhanced other human centric and perception oriented perspectives
proposed for solving the problem in question by Kacprzyk [8, 12, 14], Kacprzyk
Francelin and Gomide [18], etc.
The main “pre-inspiration” of this paper, which is meant to show and empha-
size some impprtant research directions that have occured over the last ﬁve decades
of fuzzy sets/logic, may be what the founder of fuzzy sets theory and fuzzy logic,
Professor LotﬁA. Zadeh, has been sating since the very baginning. Namely, namely
J. Kacprzyk (✉)
Systems Research Institute, Polish Academy of Sciences Ul.
Newelska 6, 01-447 Warsaw, Poland
e-mail: kacprzyk@ibspan.waw.pl
J. Kacprzyk
WIT, Warsaw School of Information Technology, Ul. Newelska 6,
01-447 Warsaw, Poland
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_11
183

184
J. Kacprzyk
that – in his opinion – the very esence of fuzzy sets and fuzzy logic would make them
particularly suited for all kinds of applications in broadly perceived human centric
systems, that is, those in which the human being plays a crucial role, or – in a slightly
broader mening - which are meant to analyze and solve problems that are important to
the individuals or soail groups, organizations, etc. Unfortunately, that Zadeh’s early
belief has not been totally fulﬁlled as, for a strange reason, in the ﬁrst breakthrough
in the real world applications of fuzzy logic starting from the early 1980 s with the
so called fuzzy boom in Japan, most applications have been in technology, notably
in relatively simple control of various home appliances, cranes, etc.
However, even in that initial period some other application, more related to
what might be called human centric, have also appeared and have been imple-
mented to solve important real world problems. This paper is about such an applica-
tion, to sustainable regional planning, which has been initiated by the author and
his collaborators, cf. notably Kacprzyk and Straszak [20–23], in the end of the
1970s and beginning of the 1980 s at the International Institute for Applied Systems
Analysis (IIASA) in Laxenburg, Asutria (www.iiasa.at). The models developed have
been widely used by the author and his collaborators in many regional planning
projects in various countries, exempliﬁed by the Upper Noteć Region in Poland,
Tisza Region in Hungary, Kinki region in Japan, to name a few. Those works have
resulted in many research publications, among which the following ones can be
quoted: Kacprzyk [8, 14], Kacprzyk, Francelin and Gomide [18], Kacprzyk and
Straszak [20–23], etc. The models proposed have been not only widely used in prac-
tice, documented also in project reports of limited circulation, but they have also
been mentioned as one of the most successful examples of fuzzy systems modeling
in a Special Volume on the Fiftieth Anniversary opf the Britis Operational Research
Society published in 1987 by Pergamon Press – cf Thomas [25]. This has been the
second inspiration of this paper.
The third inspiration is a recent growth of interest in various types of more human
centric and human consistent modeling, notably cognitive informatics and its related
cognitive modeling, a new area which has been conceptualized and proposed by
Wang [26, 27], and then considerably advanced over the next years by Wang and
his numerous collaborators and followers. For our purposes, which are related to
bradly perceived decision making and control, the work by Wang and Ruhe [32] is
presumably the most relevant.
Brieﬂy speaking, cognitive informatics is a multidisciplinary ﬁeld within infor-
matics, or computer science, that is based on results of cognitive and informa-
tion sciences, and which deals with human information processing mechanisms and
processes and their decision theoretic, engineering, etc. applications in broadly per-
ceived computing, including multistage decision making processes which are of our
interest. The agenda of ccgnitive informatics is to develop and implement mod-
els, tools and techniques, and technologies to facilitate and extend the information
acquisition, comprehension and processing capacity of humans to overcome some
cognitive diﬃculties related to the presence of the human being as a crucial part
of the system. In our case, the system will be highly related to human judgments,
and search for best (optimal) solutions. A limited comprehension, memorizing,

Cognitive Informatics: A Proper Framework ...
185
learning, choice and decision making abilities, satisfaction with partial truth, allow-
ing for not perfect solutions, etc. will be or relevance. Those issues are considered
and solved using tools and techniques derived from many areas like psychology,
behavioral science, neuroscience, artiﬁcial intelligence, linguistics, etc. In our case,
we will concentrate on some cognitive informatics type elements that mostly have
been inspired by psychology and behavioral sciences, as our problem is inherently
related to human judgments and perceptions. Some relation to neuroeconomics can
also be pursued, cf. Kacprzyk [15].
The ﬁrst idea of such a cognitive informatics related perspective for mmore gen-
eral dynamic modeling issues, notably related to dynamic programming under fuzzy
constraints and goals, have been proposed in Kacprzyk’s plenary talk at WCCI-
2014 in Beijing, China (cf. http://www.ieee-wcci2014.org/ﬁles/Janusz.Kacprzyk.
pdf). The very purpose of that talk was to propose of what might be called cognitive
fuzzy dynamic programming. The purpose of this paper is more general, namely to
propose some new perspective in fuzzy systems modeling which might be called cog-
nitive fuzzy modeling. The new fuzzy dynamic programming models presented, in
which the above human speciﬁc aspects will be shown and analyzed and a cognitive
informatics perspective will be indicated, will be shown on a sustainable regional
development considered in terms of expenditures, subsidies, life qualities, etc. For a
slightly diﬀerent approach within the cognitive modeling context, cf. Hotaling and
Busemeyer [4].
2 Fuzzy Dynamic Programming as a Step Towards
Perception Based and Cognitive Multistage Decision
Making and Control
As a point of departure we take the famous Bellman and Zadeh’s [1] model of deci-
sion making under fuzziness in which if X = {x} is some set of possible options
(alternatives, variants, choices, decisions, ...), then the fuzzy goal is deﬁned as a
fuzzy set G in X, characterized by its membership function 𝜇G ∶X ⟶[0, 1] such
that 𝜇G(x) ∈[0, 1] speciﬁes the grade of membership of a particular option x ∈X in
the fuzzy goal G, and the fuzzy constraint is similarly deﬁned as a fuzzy set C in the
set of options X, characterized by 𝜇C ∶X ⟶[0, 1] such that 𝜇C(x) ∈[0, 1] speciﬁes
the grade of membership of a particular option x ∈X in the fuzzy constraint C.
The general problem formulation is: “Attain G and satisfy C” which leads to the
fuzzy decision
𝜇D(x) = 𝜇G(x) ∧𝜇C(x),
for each x ∈X
(1)
where “∧” stands for the minimum that may be replaced, for instance, a t-norm.
The maximizing decision is deﬁned as an x∗∈X such that
𝜇D(x∗) = max
x∈X 𝜇D(x)
(2)

186
J. Kacprzyk
The human cognition related aspect is that, ﬁrst, the strict optimization in (2) may
be viewed to strict and unnecessary and some sort of a satisfactory, good enough
solution could be accepted. Second, in reality the satisfaction of constraints and
attainment of goals have both an objective and subjective aspect. We will mainly
deal with that second aspect.
The Bellman and Zadeh’s [1] framework can therefore be extended by introduc-
ing: an objective fuzzy goal 𝜇Go(x), a subjective fuzzy goal 𝜇Gs(x), an objective fuzzy
constraint 𝜇Co(x), and a subjective fuzzy constraint 𝜇Cs(x).
We wish therefore to “Attain [Go and Gs] and satisfy [Co and Cs]” which leads to
the fuzzy decision
𝜇D(x) = [𝜇Go(x) ∧𝜇Gs(x)] ∧[𝜇Co(x) ∧𝜇Cs(x)],
for each x ∈X
(3)
and the maximizing, or optimal decision is deﬁned as in (2); clearly, remarks on
a relaxation of that condition of a strict optimality are valid here too, as well as
throughout the paper.
This framework can be extended to handle multiple fuzzy constraints and fuzzy
goals, and also fuzzy constraints and fuzzy goals deﬁned in diﬀerent spaces, cf.
Kacprzyk’s [10] book. Namely, if we have: no > 1 objective fuzzy goals – G1
o, … , Gno
o
deﬁned in Y, ns > 1 subjective fuzzy goals – G1
s, … , Gns
s deﬁned in Y, mo > 1 objec-
tive fuzzy constraints – C1
o, … , Cmo
o deﬁned in X, ms > 1 subjective fuzzy constraints
– C1
s , … , Cms
s
deﬁned in X, and a function f ∶X ⟶Y, y = f(x), then
𝜇D(x) =
= (𝜇G1o[f(x)] ∧⋯∧𝜇Gno
o [f(x)]) ∧(𝜇G1s [f(x)] ∧⋯∧𝜇Gns
s [f(x)]) ∧
∧[𝜇C1o(x) ∧⋯∧𝜇Cmo
o (x)] ∧[𝜇C1s (x) ∧⋯∧𝜇Cms
s (x)] ∧
∧[𝜇C1s (x) ∧⋯∧𝜇Cms
s (x)],
for each x ∈X
(4)
and the maximizing decision is deﬁned as (2), i.e. 𝜇D(x∗) = maxx∈X 𝜇D(x).
In the control process dealt with the decision (control) space is U = {u} =
{c1, … , cm}, the state (output) space is X = {x} = {s1, … , sn}, and both are ﬁnite.
We start from an initial state x0 ∈X, apply a decision (control) u0 ∈U, which is
subjected to a fuzzy constraint 𝜇C0(u0), and attain a state x1 ∈X via a known state
transition equation of the system under control S; a fuzzy goal 𝜇G1(x1) is imposed on
x1. Next, we apply u1, subjected to 𝜇C1(u1), and attain x2, subjected to 𝜇G2(x2), etc.
The (deterministic) system under control is described by a state transition
equation
xt+1 = f(xt, ut),
t = 0, 1, …
(5)
where xt, xt+1 ∈X = {s1, … , sn} are the states at t and t + 1, respectively, and
ut ∈U = {c1, … , cm} is the decision (control) at t.
At t, t = 0, 1, …, ut ∈U is subjected to a fuzzy constraint 𝜇Ct(ut), and on xt+1 ∈X
a fuzzy goal is imposed, 𝜇Gt+1(xt+1). The ﬁxed and speciﬁed in advance initial state

Cognitive Informatics: A Proper Framework ...
187
is x0 ∈X, and the termination time (planning horizon), N ∈{1, 2, …}, is ﬁnite, and
ﬁxed and speciﬁed in advance.
The performance of the particular decision making (control) stage t, t = 0, 1, … ,
N −1, is evaluated by
vt = 𝜇Ct(ut) ∧𝜇Gt+1(xt+1) = 𝜇Ct(ut) ∧𝜇Gt+1[f(xt, ut)]
(6)
while the performance of the whole multistage decision making (control) process is
given by the fuzzy decision
𝜇D(u0, … , uN−1 ∣x0) = v0 ∧v1 ∧… ∧vN−1 =
= [𝜇C0(u0) ∧𝜇G1(x1)] ∧… ∧[𝜇CN−1(uN−1) ∧𝜇GN(xN)]
(7)
The problem is to ﬁnd an optimal sequence of decisions (controls) u∗
0, … , u∗
N−1
such that
𝜇D(u∗
0, … , u∗
N−1 ∣x0) =
max
u0,…,uN−1∈U 𝜇D(u0, … , uN−1 ∣x0)
(8)
Kacprzyk’s [10] book provides and wide coverage of various aspects and exten-
sions to this basic formulation.
In the case of an extention proposed in this paper and outlined in Sect. 2 in
which the objective and subjective fuzzy constraints and fuzzy goals are assumed,
which are inherently related to human judgment and cognition, we have, at each
t = 0, 1, … , N −1: an objective fuzzy constraint 𝜇C1
t (ut) and a subjective fuzzy
constraint 𝜇Cts(ut), and an objective fuzzy goal 𝜇Gt+1
o (ut+1) and a subjective fuzzy
constraint 𝜇Gt+1
s (ut+1).
The (extended) performance of the particular stage t, t = 0, 1, … , N −1, is then
given by
vt = [𝜇Cto(ut) ∧𝜇Cts(ut)] ∧[𝜇Gto(xt) ∧𝜇Gts(xt)]
(9)
which can be schematically shown as in Fig. 1.
Fig. 1
Evaluation of
(extended) performance of
decision making (control)
stage t

188
J. Kacprzyk
The (extended) performance of the whole multistage decision making (control)
process is then given by the fuzzy decision
𝜇D(u0, … , uN−1 ∣tx0) = v0 ∧v1 ∧… ∧vN−1 =
= {[𝜇C0o(u0) ∧𝜇C0s (u0)] ∧[𝜇G1o(x1) ∧𝜇G1s (x1)]} ∧…
= ∧{[𝜇CN−1
o
(uN−1) ∧𝜇CN−1
s
(uN−1)] ∧[𝜇GNo (xN) ∧𝜇GNs (xN)]}
(10)
and we seek again an u∗
0, … , u∗
N−1 such that
𝜇D(u∗
0, … , u∗
N−1 ∣x0) =
max
u0,…,uN−1∈U 𝜇D(u0, … , uN−1 ∣x0)
(11)
There is an extremely relevant aspect related to the subjective fuzzy constraints
and fuzzy goals. We will consider the subjective fuzzy goals in which this is pre-
sumably much more pronounced than in the subjective fuzzy constraints. Namely,
it often happens that the (subjective) human satisfaction resulting from the attain-
ment of some level of xt+1, a value of a life quality index, depends not only on the
“objectively attained” value but on how this value is perceived, how it looks like
in comparision with the past, what are future prospects, etc. For simplicity, let us
concentrate in these perceptions and jidgments on the past only.
The trajectory of the multistage decision making (control) process from t = 0 to
a current stage t = k is
Hk = (x0, u0, C0
o, C0
s , x1, G1
o, G1
s, … , uk−1, Ck−1
o
, Ck−1
s
, xk, Gk
o, Gk
s)
(12)
that is, it involves all aspects of what has happened in terms of decisions applied,
states attained, and objective and subjective opinions of how well the fuzzy con-
straints have been satisﬁed and fuzzy goals attained. However, it is often suﬃcient
to take into account the reduced trajectory
hk = (xk−2, uk−2, Ck−2
o
, Ck−2
s
, xk−1, Gk−1
o
, Gk−1
s
, uk−1, Ck−1
o
, Ck−1
s
, xk, Gk
o, Gk
s)
(13)
which only takes into account the current, t = k, and previous stage, t = k −1. Let
us assume this reduced trajectory. Such an approach has a long tradition, e.g. in all
kinds of the Markov decision processes, and has proved to be eﬀective and eﬃcient.
A further simpliﬁcation is that with a trajectory, or reduced trajectory, an evalua-
tion function is associated, E ∶S(Hk) ⟶[0, 1] or e ∶S(hk) ⟶[0, 1], where S(Hk)
and S(hk) are the sets of trajectories and reduced trajectories, respectively, such that
E(Hk) ∈[0, 1] and e(hk) ∈[0, 1] denote the satisfaction of the past development,
from 1 for full satisfaction to 0 for full dissatisfaction, through all intermediate val-
ues. This is again consistent with the human perception.

Cognitive Informatics: A Proper Framework ...
189
The subjective fuzzy constraints and huzzy goals are now:
∙when the (reduced) trajectory is accounted for
{
𝜇Cko(uk ∣hk)
and 𝜇Cks (uk ∣hk)
𝜇Gk+1
o (xk+1 ∣hk) and 𝜇Gk+1
s
(xk+1 ∣hk)
(14)
∙when the evaluation of the (reduced) trajectory is accounted for
{
𝜇Cko[uk ∣E(hk)]
and 𝜇Cks [uk ∣E(hk)]
𝜇Gk+1
o [xk+1 ∣E(hk)] and 𝜇Gk+1
s
[xk+1 ∣E(hk)]
(15)
Problem (8) can be solved using the following two basic traditional techniques:
dynamic programming (cf. Bellman and Zadeh [1], Kacprzyk [7, 10]), and branch-
and-bound (Kacprzyk [5], and also using the two new ones: a neural network (cf.
Francelin, Gomide and Kacprzyk [2, 3], and a genetic algorithm (cf. Kacprzyk [11,
12]. We will only brieﬂy show the use of dynamic programming, and refer the reader
for an extensive coverage on this and other solution techniques to Kacprzyk’s [10]
book.
First, we rewrite (8) as to ﬁnd u∗
0, … , u∗
N−1 such that
𝜇D(u∗
0, … , uN−1 ∣x0) =
=
max
u0,…,uN−1[𝜇C0(u0) ∧𝜇G1(x1) ∧…
… ∧𝜇CN−1(uN−1) ∧𝜇GN(f(xN−1, uN−1))]
(16)
and then, since
𝜇CN−1(uN−1) ∧𝜇GN(f(xN−1, uN−1))
depends only on uN−1, then the maximization with respect to u0, … , uN−1 in (16)
can be split into:
∙the maximization with respect to u0, … , uN−2, and
∙the maximization with respect to uN−1,
written as
𝜇D(u∗
0, … , u∗
N−1 ∣x0) =
=
max
u0,…,uN−2{𝜇C0(u0) ∧𝜇G1(x1) ∧…
… ∧𝜇CN−2(uN−2) ∧𝜇GN−1(xN−1) ∧
∧max
uN−1[𝜇CN−1(uN−1) ∧𝜇GN(f(xN−1, uN−1))]}
(17)
which may be continued for uN−2, uN−3, etc.

190
J. Kacprzyk
This backward iteration leads to the following set of fuzzy dynamic programming
recurrence equations:
⎧
⎪
⎨
⎪⎩
𝜇G
N−i(xN−i) =
= maxuN−i[𝜇CN−i(uN−i) ∧𝜇GN−i(xN−i) ∧𝜇G
N−i+1(xN−i+1)]
xN−i+1 = f(xN−i, uN−i);
i = 0, 1, … , N
(18)
where 𝜇G
N−i(xN−i) is viewed as a fuzzy goal at control stage t = N −i induced by
the fuzzy goal at t = N −i + 1, i = 0, 1, … , N; 𝜇G
N(xN) = 𝜇GN(xN).
The u0, … , uN−1 sought is given by the successive maximizing values of uN−i,
i = 1, … , N in (18) which are obtained as functions of xN−i, i.e. as an optimal policy,
aN−i ∶X ⟶U, such that uN−i = aN−i(xN−i).
It easy to notice that if we use the subjective fuzzy constraints and fuzzy goals to
extend the above fuzzy dynamic programming model, then the very idea of dynamic
programming, i.e. the use of backward iteration represented by the recurrence equa-
tions (18), prohibits the use of subjective fuzzy constraints and subjective fuzzy goals
deﬁned as functions of the trajectory, or any evaluation of the trajectory, as both of
them are somehow calculated on the basis of outcomes of control stages prior to
those which have been accounted for so far since we proceed via backward iteration.
Therefore, if we intend to employ fuzzy dynamic programming, as in this paper, we
can only use the subjective fuzzy constraints and goals depending on the current
value of decision (control) applied and state attained. The involvement of subjec-
tive fuzzy constraints and goals depending on the trajectory or its evaluation needs
another approach as, e.g., the use of a genetic algorithm (cf. Kacprzyk [6, 9, 12,
17]) or a neural network based approach by Francelin, Gomide and Kacprzyk [2] or
Francelin, Kacprzyk and Gomide [3].
Therefore, by involving the line of reasoning (16)–(18), using the objective and
subjective fuzzy constraints and fuzzy goals: 𝜇CN−i
o
(uN−i) and 𝜇CN−i
s
(uN−i), and
𝜇GN−i+1
o
(xN−i+1) and 𝜇GN−i+1
s
(xN−i+1), for i = 1, 2, … , N, we arrive at the following
set of (extended) dynamic programming recurrent equations:
⎧
⎪
⎪
⎨
⎪
⎪⎩
𝜇G
N−i(xN−i) =
= maxuN−i{[𝜇CN−i
o
(uN−i) ∧𝜇CN−i
s
(uN−i)]∧
[𝜇GN−i
o
(xN−i) ∧𝜇GN−i
s
(xN−i) ∧𝜇G
N−i+1(xN−i+1)]}
xN−i+1 = f(xN−i, uN−i);
i = 0, 1, … , N
(19)
3 Sustainable Socioeconomic Regional Development
Planning Under Fuzziness
Regional development planning is a problem of crucial relevance in virtually all
countries but is diﬃcult to formalize and solve as it involves various aspects (politi-
cal, economic, social, environmental, technological, etc.), diﬀerent parties and agents

Cognitive Informatics: A Proper Framework ...
191
(inhabitants, authorities of diﬀerent levels, formal and informal groups, etc.), and
many entities and aspects that are diﬃcult to precisely single out, deﬁne and quan-
tify. Needless to say that the sustainable regional development planning is even more
complex but for a lack of space we will not discuss its speciﬁcs in more detail. To
overcome these diﬃculties, the use of a fuzzy model was Kacprzyk and Straszak [21,
23], and then extended by Kacprzyk [10], and Kacprzyk, Francelin and Gomide [18].
Basically, they consider a (rural) region plagued by severe diﬃculties mainly
related to a poor life quality perceived. Hence, life quality (or, in fact, a perception
therof) should be improved, by some (mostly external) funds (investments) whose
amount and their temporal distribution should be found. We will show now how
the extended, cognitive type and perception based model developed above can be
employed.
For our purposes the essence of socioeconomic regional development may be
depicted as in Fig. 2.
Fig. 2
Essential elements of socioeconomic regional development
The region is represented by a socioeconomic dynamic system under control the
state of which at the development (planning) stage t−1, Xt−1, is characterized by a set
of relevant socioeconomic life quality indicators. Then, the decision (investment), at
t −1, ut−1, changes Xt−1 to Xt; t = 1, … , N, and N is a ﬁnite, ﬁxed and speciﬁed
planning horizon.
The evaluation of a planning stage t, t = 1, … , N, is performed by accounting
for both the “goodness” of the ut−1 applied (i.e. costs), and the “goodness” of the
Xt attained (i.e. beneﬁts); the former has to do with how well some constraints are
satisﬁed, and the latter with how well some goals are attained. We will involve, for
simplicty, a subjective assessment for the attainment of fuzzy goals only.
First, the socioeconomic system is represented as in Fig. 3. Its state (output) Xt is
equated with a life quality index that consists
of the following seven life quality indicators (i.e. Xt = [x1
t , … , x7
t ]):
∙x1
t – economic quality (e.g., wages, salaries, income, ...),
∙x2
t – environmental quality,
∙x3
t – housing quality,
∙x4
t – health service quality,

192
J. Kacprzyk
Fig. 3
Basic elements of the socioeconomic system under control
∙x5
t – infrastructure quality,
∙x6
t – work opportunity,
∙x7
t – leisure time opportunity,
The decision at t−1, ut−1 is investment, and we impose on ut−1 a fuzzy constraint
𝜇Ct−1(ut−1) in a piecewise linear form as shown in Fig. 4 to be read as follows.
Fig. 4
Fuzzy constraints on
investment ut−1
1
mC
t
t
u
-
-
1
1
(
)
ut
p
- 1
ut
c
- 1
ut- 1
The investment may be fully utilized up to up
t−1, hence 𝜇Ct−1(ut−1) = 1 for 0 <
ut−1 < up
t−1. However, this is usually insuﬃcient and some additional contingency
investment is needed, maximally up to uc
t−1 (the more the worse, of course). The
fuzzy constraints are often as shown in the dotted line in Fig. 4 in that too low a
use of available investments should also be avoided, for “political” reasons, as in all
public funding related cases.
The t−1, ut−1 is partitioned into u1
t−1, … , u7
t−1, devoted to improve the respective
life quality indicators, but we will assume here that this rule is ﬁxed.
The temporal evolution of the particular life quality indicators is governed by the
state transition equation

Cognitive Informatics: A Proper Framework ...
193
xi
t = f i
t−1(xi
t−1, ui
t−1),
i = 1, … , 7; t = 1, … , N
(20)
which may be derived by, e.g., using experts’ opinions, past experience, mathemat-
ical models, etc.
The evaluation of development takes into account how well some predetermined
goals are fulﬁlled, i.e. eﬀectiveness, then be related to the investment spent, i.e. eﬃ-
ciency – cf. Kacprzyk’s [10] book.
The eﬀectiveness of regional development involves two aspects: the eﬀectiveness
of a particular development stage, and the eﬀectiveness of the whole development.
The eﬀectiveness of a particular development stage has both an objective and sub-
jective aspect. The objective evaluation is basically the determination of how well the
fuzzy constraints are fulﬁlled, and fuzzy goals are attained. The objective fuzzy goals
concern desired values of the life quality indicators, i.e. concern objective entities;
however, goal attainment is not clear-cut, and a fuzzy goal should rather be used.
For each life quality indicator at t = 1, … , N, xi
t, we deﬁne an objective fuzzy
subgoal Gt,i
o characterized by 𝜇Gt,i
o (xi
t) as shown in Fig. 5
Fig. 5
Objective fuzzy
subgoal
1
mG
t
i
t i x
0
, (
)
xt
i
xt
i
xt
i
to be read as follows: Gt,i
o is fully satisﬁed for xi
t ≥xu
t , where xi
o is some aspiration
level for the indicator xi
t; therefore, 𝜇Gt,i
o (xi
t) = 1, for xi
t ≥xi
t. Less preferable are
xi
t < xi
t < xi
t for which 0 < 𝜇Gt,i
o (xi
t) < 1, and xi
t ≤xi
t are assumed to be impossible,
hence 𝜇Gt,i
o (xi
t) = 0. Notice that an objective fuzzy (sub)goal may be relatively easily
determined by experts by specifying two values only, xti
t and xi
t.
The objective evaluation of the life quality index at t, Xt = [x1
t , … , x7
t ], is obtained
by the aggregation of partial assessments of the particular life quality indicators, i.e.
𝜇Gto(Xt) = 𝜇Gt,1
o (x1
t ) ∧… ∧𝜇Gt,7
o (x7
t )
(21)
and “∧” may be replaced here and later on by another suitable operation as, e.g., a
t-norm [cf. Kacprzyk (1997a)] but this will not be considered here.
Basically, the use of “∧” (minimum) reﬂects a pessimistic, safety-ﬁrst attitude,
and a lack of substitutability (i.e. that a low value of one life quality indicator cannot
be compensated by a higher value of another), which is often adequate.

194
J. Kacprzyk
Finally, note that the objective evaluation concerns more the authorities than the
inhabitants by somehow “mechanically” checking the values of life quality indica-
tors attained against some desired predetermined levels. The inhabitants’ assessment
of the “goodness” of development conc‘erns in fact the (perception of) social satis-
faction resulting from the life quality index attained. This is clearly subjective. The
attained value of a particular life quality indicator at t, xi
t, implies its corresponding
partial social satisfaction si
t derived as in Fig. 6, and its interpretation is basically as
for the objective evaluation shown in Fig. 5.
Fig. 6
Partial social
satisfaction
1
st
i
z
H
t
i
t
(
)
z
H
t
i
t
(
)
xt
i
In general, both zi
t and zi
t may be functions of the trajectory (history) of develop-
ment [cf. (12)]
Ht = [(X1, s1, 𝜇G1o(X1), 𝜇G1s (s1)), … , (Xt, st, 𝜇Gto(st), 𝜇Gts(st))]
where sk = [s1
k, … , s7
k], k = 1, … , t, is the social satisfaction resulting from Xk.
Basically, if Ht is encouraging, then the inhabitants may become more demanding,
and zi
t(Ht) and zi
t(Ht) may move up. On the other hand, if Ht is discouraging, then
zi
t(Ht) and zi
t(Ht) may move down (cf. Kacprzyk [7, 10]). Very often, however, one
can limit the analysis to the reduced trajectory [cf. 13)]. This important aspect will
not be considered here.
The social satisfaction at t is now
st = s1
t ∧… ∧s7
t
(22)
where “∧” again reﬂects a pessimistic, safety-ﬁrst attitude, and a lack of
substitutability.
The social satisfaction st is subjected to a subjective fuzzy goal 𝜇Gts(st) which is
meant similarly as its objective counterpart shown in Fig. 5.
The eﬀectiveness of t is meant as a relation of what has been attained (the life
quality indices and their respective social satisfactions) to what has been “paid for”
(the respective investments), i.e. is a beneﬁt–cost relationship. Formally, the (fuzzy)
eﬀectiveness of stage t is expressed as

Cognitive Informatics: A Proper Framework ...
195
𝜇Et(ut−1, Xt, st) = 𝜇Ct−1(ut−1) ∧𝜇Gto(Xt) ∧𝜇Gts(st)
(23)
and the aggregation reﬂects the nature of a compromise between the interests of the
authorities (for whom the fuzzy constraints and the objective fuzzy goal matter), and
those of the inhabitants (for whom the subjective fuzzy goal, and to some extent the
objective fuzzy goal, matter); the minimum reﬂects a safety-ﬁrst attitude, hence a
“more just” compromise.
Then, the eﬀectiveness measures of the particular t = 1, … , N, 𝜇Et(ut−1, Xt, st)
given by (23), are aggregated to yield the fuzzy eﬀectiveness measure for the whole
development
𝜇E(HN) = 𝜇E1(u0, X1, s1) ∧… ∧𝜇EN(uN−1, XN, sn)
(24)
The fuzzy decision is
𝜇D(u0, … , uN−1 ∣X0, BN) =
= [𝜇C0(u0) ∧𝜇G1o(X1) ∧𝜇G1s (s1)] ∧…
… ∧[𝜇CN−1(uN−1) ∧𝜇GNo (XN) ∧𝜇GNs (sN)]
(25)
and it expresses some crucial compromises between, e.g.:
∙the fuzzy constraints and (objective and subjective) fuzzy goals,
∙the interests of the authorities and inhabitants, etc.
The problem is now to ﬁnd an optimal sequence of controls (investments) u∗
0, … ,
u∗
N−1 such that (under a given policy BN; the optimization of policy is a separate
problem which will not be cosnidered here):
𝜇D(u∗
0, … , u∗
N−1 ∣X0, BN) =
=
max
u0,…,uuN−1
{[𝜇C0(u0) ∧𝜇G1o(X1) ∧𝜇G1s (s1)] ∧…
… ∧[𝜇CN−1(uN−1) ∧𝜇GNo (XN) ∧𝜇GNs (sN)]}
(26)
For illustration we will show a simple example that in its initial form was shown ﬁrst
in Kacprzyk’s [10] book but will be changed with respect to numbers to account for
diﬀerent economic conditions in the present time. Example: The region, predomi-
nantly agricultural, has a population of ca. 120,000 inhabitants, and its arable land is
ca. 450,000 acres. For simplicity, the region’s development will be considered over
the next 3 development stages (years, for simplicity). The life quality index consists
of the four life quality indicators:
∙xI
t – average subsidies in US$ per acre (per year),
∙xII
t – sanitation expenditures (water and sewage) in US$ per capita (per year),
∙xIII
t
– health care expenditures in US$ per capita (per year), and
∙xIV
t
– expenditures for paved roads (new roads and maintenance of the existing
ones) in US$ (per year).

196
J. Kacprzyk
Suppose now that the investments are partitioned into parts devoted to the
improvement of the above life quality indicators due to the ﬁxed partitioning rule
At−1(ut−1, i): 5 % for subsidies, 25 % for sanitation, 45 % for health care, and 25 %
for infrastructure.
Let the initial, at t = 0, values of the life quality indicators be:
xI
0 = 0.5 xII
0 = 15 xIII
0 = 27 xIV
0 = 1, 700, 000
For clarity, we will only take into account the following two scenarios (policies):
∙Policy 1: u0 = $16, 000, 000 u1 = $16, 000, 000 u2 = $16, 000, 000
∙Policy 2: u0 = $15, 000, 000 u1 = $16, 000, 000 u2 = $17, 000, 000
Under Policy 1 and Policy 2, the values of the life quality indicators attained are:
Policy 1: Year(t)
ut
xI
t
xII
t
xIII
t
xIV
t
0
$16, 000, 000
1
$16, 000, 000 0.88 16.7 30 $4, 000, 000
2
$16, 000, 000 0.88 16.7 30 $4, 000, 000
3
0.88 16.7 30 $4, 000, 000
Policy 2: Year(t)
ut
xI
t
xII
t
xIII
t
xIV
t
0
$15, 000, 000
1
$16, 000, 000 0.83 15.6 28.1 $3, 500, 000
2
$17, 000, 000 0.88 16.7 30 $8, 000, 000
3
0.94 17.7 31.9 $2, 250, 000
For the evaluation of the above two development trajectories, for simplicity and
readability we will only take into account the eﬀectiveness of development, and the
objective evaluation only. The consecutive fuzzy constraints and objective fuzzy
subgoals are assumed piecewise linear, i.e. their deﬁnition requires two values only
(cf. Figs. 4 and 5): the aspiration level (i.e. the fully acceptable value) and the lowest
(or highest) possible (still acceptable) value) which are:
t
0 C0 ∶up
0 = $15, 000, 000
uc
0 = $17, 000, 000
1 C1 ∶up
1 = $16, 500, 000
uc
1 = $18, 000, 000 G1,I
o ∶xI
1 = 0.6
xI
1 = 0.85
G1,II
o
∶xII
1 = 14
xII
1 = 16
G1,III
o
∶xIII
1 = 27
xIII
1 = 29
G1,IV
o
∶xIV
1 = $3, 600, 000 xIV
1 = $3, 800, 000

Cognitive Informatics: A Proper Framework ...
197
2 C2 ∶up
2 = $16, 000, 000
uc
1 = $20, 000, 000 G2,I
o ∶xI
2 = 0.7
xI
1 = 0.9
G2,II
o
∶xII
2 = 15
xII
1 = 17
G2,III
o
∶xIII
2 = 28
xIII
1 = 30
G2,IV
o
∶xIV
2 = $3, 800, 000 xIV
2 = $4, 000, 000
3
G3,I
o ∶xI
3 = 0.75
xI
3 = 1
G3,II
o
∶xII
3 = 16
xII
1 = 18.5
G3,III
o
∶xIII
3 = 29
xIII
1 = 31
G3,IV
o
∶xIV
3 = $3, 800, 000 xIV
3 = $4, 200, 000
Using the “∧” (minimum) to reﬂect a safety-ﬁrst attitude, which is clearly prefer-
able in the situation considered (a rural region plagued by aging of the society, out-
migration to neighboring urban areas, economic decay, etc.), the evaluation of the
two investment policies is:
∙Policy 1
𝜇D($16, 000, 000; $16, 000, 000; $16, 000, 000 ∣.) =
= 𝜇C0($16, 000, 000) ∧(𝜇G1,I
o (0.88) ∧
∧𝜇G1,II
o (16.7) ∧𝜇G1,III
o
(30) ∧𝜇G1,IV
o
($4, 000, 000)) ∧
∧𝜇C1($16, 000, 000) ∧(𝜇G2,I
o (0.88) ∧
∧𝜇G2,II
o (16.7) ∧𝜇G2,III
o
(30) ∧𝜇G2,IV
o
($4, 000, 000)) ∧
∧𝜇C2($16, 000, 000) ∧(𝜇G3,I
o (0.88) ∧
∧𝜇G3,II
o (16.7) ∧𝜇G3,III
o
(30) ∧𝜇G3,IV
o
($4, 000, 000)) =
= 0.5 ∧(1 ∧1 ∧1 ∧1) ∧0.8 ∧
∧(0.9 ∧0.85 ∧1 ∧1) ∧1 ∧(0.52 ∧0.28 ∧0.5 ∧0.33) =
= 0.5 ∧0.8 ∧0.28 = 0.28
∙Policy 2
𝜇D($15, 000, 000; $16, 000, 000; $15, 500, 000 ∣.) =
= 𝜇C0($15, 000, 000) ∧(𝜇G1,I
o (0.83) ∧
∧𝜇G1,II
o (15.6) ∧𝜇G1,III
o
(28.1) ∧𝜇G1,IV
o
($3, 750, 000)) ∧
∧𝜇C1($16, 000, 000) ∧(𝜇G2,I
o (0.88) ∧
∧𝜇G2,II
o (16.7) ∧𝜇G2,III
o
(30) ∧𝜇G2,IV
o
($4, 000, 000)) ∧
∧𝜇C2($17, 000, 000) ∧(𝜇G3,I
o (0.94) ∧
∧𝜇G3,II
o (17.7) ∧𝜇G3,III
o
(31.9) ∧𝜇G3,IV
o
($4, 250, 000)) =

198
J. Kacprzyk
= 1 ∧(0.92 ∧0.8 ∧0.55 ∧0.75) ∧0.8 ∧
∧(0.9 ∧0.85 ∧1 ∧1) ∧0.75 ∧(0.76 ∧0.68 ∧1 ∧1) =
= 0.55 ∧0.8 ∧0.68 = 0.55
The second policy is therefore better.
4 Concluding Remarks
We tried to show that Wang’s cognitive informatics may be a potentially power-
ful general approach and paradigm to formulate, analyze and solve human centric
systems modeling, decision and control problems. To be more speciﬁc, we showed
the use of fuzzy dynamic programming for solving a regional development prob-
lem in which many crucial aspects, in particular life quality indicators, were sub-
ject to objective and subjective, by the humans, judgments and evaluations which
are closely related to human perceptions and cognitive abilities. For illustration, we
showed a simple example of regional development planning in which the problem
was to determine a best (optimal) investment policy under diﬀerent development
scenarios, and subject to objective and subjective evaluations.
References
1. Bellman, R.E., Zadeh, L.A.: Decision making in a fuzzy environment. Manag.Sci. 17, 141–164
(1970)
2. Francelin, R.A., Gomide, F.A.C., Kacprzyk, J.: A biologically inspired neural network for
dynamic programming. Int. J. Neural Syst. 11, 561–572 (2001)
3. Francelin, R.A., Kacprzyk, J., Gomide, F.A.C.: Neural network based algorithm for dynamic
system optimization. Asian J. Control 3(2), 131–142 (2001)
4. Hotaling, J.M., Busemeyer, J.R.: DFT-D: a cognitive-dynamical model of dynamic decision
making. Synthese 189(1), 67–80 (2012)
5. Kacprzyk, J.: A branch-and-bound algorithm for the multistage control of a nonfuzzy system
in a fuzzy environment. Contr. Cybern. 7, 51–64 (1978)
6. Kacprzyk, J.: A branch-and-bound algorithm for the multistage control of a fuzzy system in a
fuzzy environment. Kybernetes 8, 139–147 (1979)
7. Kacprzyk, J.: Multistage Decision Making under Fuzziness. Verlag TÜV Rheinland, Cologne
(1983)
8. Kacprzyk, J.: Design of socio-economic regional development policies via a fuzzy decision
making model. In: Straszak, A. (ed.) Large Scale Systems Theory and Applications, Pro-
ceedings of Third IFAC/IFORS Symposium (Warsaw, Poland, 1983), pp. 228–232. Pergamon
Press, Oxford (1984)
9. Kacprzyk, J.: Multistage control under fuzziness using genetic algorithms. Contr. Cybern. 25,
1181–1215 (1996)
10. Kacprzyk, J.: Multistage Fuzzy Control. Wiley, Chichester (1997)
11. Kacprzyk, J.: A genetic algorithm for the multistage control of a fuzzy system in a fuzzy envi-
ronment. Mathware Soft Comput. IV, 219–232 (1997)

Cognitive Informatics: A Proper Framework ...
199
12. Kacprzyk, J.: Multistage control of a stochastic system in a fuzzy environment using a genetic
algorithm. Int. J. Intell. Syst. 13, 1011–1023 (1998)
13. Kacprzyk, J.: Including socio-economic aspects in a fuzzy multistage decision making model
of regional development planning. In: Reznik, L., Dimitrov, V., Kacprzyk, J. (eds.) Fuzzy Sys-
tems Design, pp. 86–102. Physica-Verlag (Springer-Verlag), Heidelberg and New York (1998)
14. Kacprzyk, J.: Towards perception-based fuzzy modelling: an extended multistage fuzzy control
model and its use in sustainable regional development planning. In: Sinak, P., Vaak, J., Hirota,
K. (eds.) Machine Intelligence - Quo Vadis?. World Scientiﬁc, Singapore, pp. 301–337 (2004)
15. Kacprzyk, J.: Neuroeconomics: yet another ﬁeld where rough sets can be useful? In: Chan,
C.-V., Grzymaa-Busse, J.W., Ziarko, W.P. (eds.) Rough Sets and Currect Trends in Computing.
LNAI 5306, Springer, Berlin, pp. 1–12 (2008)
16. Kacprzyk, J.: Fuzzy dynamic programming: interpolative reasoning for an eﬃcient derivation
of optimal control policies. Contr. Cybern. 42(1), 63–84 (2013)
17. Kacprzyk, J.: Multistage fuzzy control of a stochastic system using a bacterial genetic algo-
rithm. In: Grzegorzewski, P., Gagolewski, M., Hryniewicz, O., Gil, M.Á. (eds.) Strengthening
Links Between Data Analysis and Soft Computing, Springer, Heidelberg, pp. 273–283 (2015)
18. Kacprzyk, J., Francelin, R.A., Gomide, F.A.C.: Involving objective and subjective aspects in
multistage decision making and control under fuzziness: dynamic programming and neural
networks. Int. J. Intell. Syst. 14, 79–104 (1999)
19. Kacprzyk, J., Owsiński, J.W., Straszak, A.: Agricultural policy making for integrated regional
development in a mixed economy. In: Titli, A., Singh, M.G. (eds.) Proceedings of Second IFAC
Large Scale Systems Theory and Applications Symposium (Toulouse, France, 1979), pp. 9–21.
Pergamon Press, Oxford (1980)
20. Kacprzyk, J., Straszak, A.: Application of fuzzy decision making models for determining opti-
mal policies in ‘stable’ integrated regional development. In: Wang, P.P., Chang, S.K. (eds.)
Fuzzy Sets Theory and Applications to Policy Analysis and Information Systems, pp. 321–
328. Plenum, New York (1980)
21. Kacprzyk, J., Straszak, A.: A fuzzy approach to the stability of integrated regional develop-
ment. In: Lasker, G.E. (ed.) Applied Systems and Cybernetics, vol. 6, pp. 2997–3004. Perga-
mon Press, New York (1982)
22. Kacprzyk, J., Straszak, A.: Determination of ’stable‘ regional development trajectories via a
fuzzy decision making model. In: Yager, R.R. (ed.) Recent Developments in Fuzzy Sets and
Possibility Theory, pp. 531–541. Pergamon Press, New York (1982)
23. Kacprzyk, J., Straszak, A.: Determination of stable trajectories for integrated regional develop-
ment using fuzzy decision models. IEEE Trans. Syst. Man Cybern. SMC-14, 310–313 (1984)
24. Kacprzyk, J., Sugianto, L.F.: Multistage fuzzy control involving objective and subjective
aspects. In: Proceedings of the 2nd International Conference on Knowledge-Based Intelligent
Electronic Systems KES98. (Adelaide, Australia), pp. 564–573 (1998)
25. Thomas, L.C. (ed.): Golden Developments in Operational Research. Pergamon Press, New
York (1987)
26. Wang, Y.: On Cognitive Informatics. Brain Mind A Transdiscipl. J. Neurosci. Neurophilosophy
4(2), 151–167 (2003)
27. Wang, Y.: The theoretical framework of cognitive informatics. Int. J. Cognitive Inf. Nat. Intell.
1(1), 1–27 (2007)
28. Wang, Y., Baciu, G., Yao, Y., Kinsner, W., Chan, K., Zhang, B., Hameroﬀ, S., Zhong, N.,
Hunag, C.-R., Goertzel, B., Miao, D., Sugawara, K., Wang, G., You, J., Zhang, D., Zhu, H.:
Perspectives on cognitive informatics and cognitive computing. Int. J. Cognitive Inf. Nat. Intell.
4(1), 1–29 (2010)
29. Wang, Y., Kinsner, W.: Recent advances in cognitive informatics. IEEE Trans. Syst. Man
Cybern. (Part C) 36(2), 121–123 (2006)
30. Wang, Y., Kinsner, W., Zhang, D.: Contemporary cybernetics and its faces of cognitive infor-
matics and computational intelligence. IEEE Trans. Syst. Man Cybern. (Part B) 39(4), 823–833
(2009)

200
J. Kacprzyk
31. Wang, Y., Kinsner, W., Anderson, J.A., Zhang, D., Yao, Y., Sheu, P., Tsai, J., Pedrycz, W.,
Latombe, J.-C., Zadeh, L.A., Patel, D., Chan, C.: A doctrine of cognitive informatics. Funda-
menta Informaticae 90(3), 203–228 (2009)
32. Wang, Y., Ruhe, G.: The Cognitive Process of Decision Making. Int. J. Cognitive Inf. Nat.
Intell. 1(2), 73–85 (2007)
33. Wang, Y., Widrow, B., Zhang, B., Kinsner, W., Sugawara, K., Sun, F., Lu, J., Weise, T., Zhang,
D.: Perspectives on the ﬁeld of cognitive informatics and its future development. Int. J. Cog-
nitive Inf. Nat. Intell. 5(1), 1–17 (2011)
34. Zadeh, L.A., Kacprzyk, J. (eds.): Computing with words in information/intelligent systems.
Part 1: Foundations, Part 2: Applications. Physica-Verlag (Springer-Verlag), Heidelberg and
New York (1999)
Author Biography
Janusz Kacprzyk
graduated from the Department of Elec-
tronic, Warsaw University of Technology in Warsaw, Poland
with M.Sc. in automatic control, his Ph.D. in systems analysis
and D.Sc. (“habilitation”) in computer science from the Polish
Academy of Sciences. He is Professor of Computer Science at
the Systems Research Institute, Polish Academy of Sciences,
Professor of Computerized Management Systems at WIT -
Warsaw School of Information Technology, and Professor of
Automatic Control at PIAP - Industrial Institute of Automa-
tion and Measurements, in Warsaw, Poland, and Department
of Electrical and Computer Engineering, Cracow University
of Technology, in Cracow, Poland. He is Honorary Foreign
Professor at the Department of Mathematics, Yli Normal Uni-
versity, Xinjiang, China, and Visiting Scientist at the RIKEN
Brain Research Institute in Tokyo, Japan. He is Full Member
of the Polish Academy of Sciences, and Foreign Member of the Bulgarian Academy of Sci-
ences and Spanish Royal Academy of Economic and Financial Sciences (RACEF). He obtained
the honoraty doctorate (doctor honosris causa) from the Széchenyi István University in Győr,
Hungary. He is Fellow of IEEE and of IFSA.
He was a frequent visiting professor in the USA, Italy, UK, Mexico and China. His main
research interests include the use of computational intelligence, notably fuzzy logic, in deci-
sions, optimization, control, data analysis and data mining, with applications in databases, ICT,
mobile robotics, etc.
He is the author of 5 books, (co)editor of ca. 70 volumes, (co)author of ca. 500 papers.
He is the editor in chief of 6 book series at Springer, and of 2 journals, and a member of
editorial boards of more than 40 journals. He is a member, a member of Adcom (Administrative
Committee) of IEEE CIS, of Award Committee of IEEE CIS, and a Distinguished Lecturer of
IEEE CIS.
He received many awards, notably: The 2006 IEEE CIS Pioneer Award in Fuzzy Systems,
The 2006 Sixth Kaufmann Prize and Gold Medal for pioneering works on soft computing in
economics and management, and The 2007 Pioneer Award of the Silicon Valley Section of
IEEE CIS for contribution in granular computing and computing in words, and Award of the
2010 Polish Neural Network Society for exceptional contributions to the Polish computational
intelligence community, IFSA 2013 Award for outstanding academic contributions and life time
achievement in the ﬁeld of fuzzy systems, and a continuous support of IFSA, WAC (World
Automation Congress) 2014 Lifetime Achievement Award in Soft Computing.
Currently he is President of the Polish Society for Operational and Systems Research and
Past President of IFSA (International Fuzzy Systems Association).

On Discord Between Expected and Actual
Developments in Applications of Fuzzy
Logic During Its First Fifty Years
George J. Klir
Abstract Developments of applications of fuzzy logic during the ﬁrst ﬁfty years of
its existence are examined in this paper with the aim of comparing the actual
developments with the expected ones in various areas of human affairs. It is shown
that in many of the examined areas the actual developments turned out to be very
different from the expected ones. In each area, an attempt is made to explain reasons
for this surprising discord between reasonable expectations and the actual
developments.
Keywords Principle of bivalence ⋅Fuzzy logic ⋅Applications of fuzzy logic
1
Introduction
In this paper, the term fuzzy logic in used in its general, commonsense meaning,
referring to all principles and methods for representing and manipulating knowledge
that employ, in addition to the classical truth values—true and false—intermediary
truth values that are interpreted as degrees of truth. The principal characteristic of
fuzzy logic viewed in this way is the rejection of the bivalence principle of classical
logic—the assumption, inherent in classical logic—that each declarative sentence
has exactly two possible truth values, true and false.
Recognizing that any challenge of the bivalence principle in logic and mathe-
matics is extremely radical explains why such challenges have been very rare in the
long history of logic and mathematics. Prior to the 20th century, only a very few
challenges of the bivalence principle have been discovered by historians of logic,
G.J. Klir (✉)
Binghamton University (SUNY), Binghamton, NY 13902-6000, USA
e-mail: gklir@binghamton.edu
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_12
201

and these happened to be all inconsequential. In the 20th century, the bivalence
principle was challenged more seriously by the emergence of the various many-
valued logics and, eventually, by fuzzy logic.1
It is undeniable that the most signiﬁcant challenge to the principle of bivalence is
closely associated with the introduction of fuzzy set theory by LotﬁZadeh [2]. The
aim of this book is to examine how this theory has developed during the ﬁrst ﬁfty
years of its existence and what is its impact on mathematics and other areas of
human affairs. I chose to focus in this short note on a rather neglected aspect of the
50-year history of the theory—the discrepancy between the expected and actual
applications of the theory in some basic areas of science, engineering, and other
professions (medicine, business, etc.). I also try to explain the cause of this dis-
crepancy in each of the examined areas.
It has often been emphasized by Zadeh that two distinct meaning of the term
fuzzy logic should be recognized, and he introduced the terms fuzzy logic in the
narrow sense and fuzzy logic in the broad sense for these two meanings. This
distinction, which Zadeh described particularly well in [3], is useful and I consider
it relevant for the discussion I intend to pursue in this article, so let me introduce it
from the outset.
Fuzzy logic in the narrow sense is concerned with formal logical systems in
which the truth of each proposition is a matter of degree. It studies the various
propositional, predicate and other fuzzy logic systems that are sound and complete
in a similar way as in classical, bivalent logic. These systems provide foundations
for fuzzy logic in the broad sense, which has a considerably wider and highly
pragmatic agenda.
Fuzzy logic in the broad sense can be loosely characterized as a research pro-
gram that has been pursued under the leadership of LotﬁZadeh since the publi-
cation of his seminal paper [2]. The primary aim of this program is to employ fuzzy
set theory for emulating common-sense human reasoning in natural language and
for utilizing it for various other purposes. In pursuing this aim, fuzzy logic in the
broad sense often reaches beyond the established concepts and results in fuzzy logic
in the narrow sense, which, in turn, motivates further research in fuzzy logic in the
narrow sense.2
Fuzzy logic in the broad sense is a huge undertaking, which has been shaped
over the years by many contributors. Among them, however, LotﬁZadeh has
played a leading role by continually introducing novel ideas, which gradually
expanded the agenda of this research program. From 1965 until the mid 1990s, the
genesis of these ideas is well documented in two large volumes of his collected
papers, edited by Yager et al. [5] and Klir and Yuan [6]. After the mid 1990s, Zadeh
1For historical details regarding this very brief summary, see the recent book by Belohlavek,
Dauben and Klir [1].
2However, this statement makes sense only since the 1990s, when the ﬁrst systems of fuzzy logic
in the narrow emerged through the work of Peter Hájek [4] and other logicians.
202
G.J. Klir

introduced a few additional prime ideas, including those of computing with words
[7], computing with perceptions [8], and general theory of uncertainty [9, 10].
Clearly, the term fuzzy logic (in both its narrow and broad sense) represents a
generic concept that characterizes a wide variety of special systems. In fuzzy logic
in the narrow sense, these systems are distinguished from one another by various
properties such as the set of truth degrees employed and its algebraic structure, truth
functions employed for logic connectives, recognized inference rules, and the like.
In fuzzy logic in the broad sense they are distinguished by the employed set of
membership degrees and its algebraic structure, the employed aggregation opera-
tions on fuzzy sets, modiﬁers representing linguistic hedges, and the like.
In the next, rather short section, I examine theoretical developments in fuzzy
logic over the last ﬁfty years. This is followed by a considerably longer section
concerned with applications of fuzzy logic, which is the core of this paper.
2
Theoretical Developments
The development of fuzzy logic in the broad sense began with the publication of the
seminal paper on fuzzy sets by Zadeh [2]. The concept of a fuzzy set, as introduced
in this paper, is an intuitive one, not an axiomatic one. Its meaning is described in
the paper as follows (page 339):
The notion of a fuzzy set provides a convenient point of departure for the construction of a
conceptual framework which parallels in many respects the framework used in the case of
ordinary sets, but is more general than the latter and, potentially, may prove to have a much
wider scope of applicability…. Such a framework provides a natural way of dealing with
problems in which the source of imprecision is the absence of sharply deﬁned criteria of
class membership.
The agenda of Zadeh’s research program—fuzzy logic in the broad sense—
derives rather naturally from the observations that fuzzy sets generalize ordinary
(classical) sets and that this generalization expands potentially their applicability. In
developing the agenda, Zadeh set on exploring these two observations, and it is
signiﬁcant that he has pursued these explorations in a systematic fashion in his
many publications. It is typical for his publications that each contains not only some
new ideas, but also an extensive overview of relevant previous ideas in the context
of the new ideas. Through this consistent repetition of relevant previous ideas, his
own or in some cases introduced by other contributors, the agenda of fuzzy logic in
the broad sense has gradually evolved in a coherent way.
In his seminal paper [2], Zadeh introduced only a special class of fuzzy sets, the
range of whose membership functions is always the unit interval [0,1]. These are
usually referred to as standard fuzzy sets. However, he made a remark in a footnote
of the paper that this range can be generalized to “a suitable partially ordered set.”
He also made another remark in the same footnote that if values of the membership
function are interpreted as truth values, a multivalued logic is obtained with a
continuum of truth values in [0,1]. These two remarks were taken seriously by
On Discord Between Expected and Actual Developments …
203

Joseph Goguen, a student of Zadeh, who fully elaborated on them in his two early
papers. In the ﬁrst paper [11], he generalized standard fuzzy sets to the so-called L-
fuzzy sets by extending the unit interval of standard fuzzy sets to a more general and
well-conceived algebraic structure of a complete residuated lattice of membership
grades. In the second paper [12], he developed basic ideas of logic for reasoning
with inexact concepts, in which fuzzy sets (standard or L-fuzzy) play the role of
inexact predicates and quantiﬁers. These two papers are historically very important
since they are closely associated not only with the genesis of fuzzy logic in the
broad sense, but also with genesis of fuzzy logic in the narrow sense.
The developments of fuzzy logic from the two viewpoints—the broad one and
the narrow one—have been pursued more or less independently from one another,
primarily due to their very different agendas and because most researchers attracted
to fuzzy logic were interested in either one or the other agenda. Although there have
been some researchers who were interested in both agendas, such as Joseph
Goguen, they were unfortunately very rare.
Contrary to fuzzy logic in the broad sense, the one in the narrow sense has a long
prehistory, associated with the various many-valued logics that have been studied
since the beginning of the 20th century, as is well documented in the book by
Rescher [13]. The connection of many-valued logics with fuzzy logic in the narrow
sense, which was for the ﬁrst time recognized in the above-mentioned paper
Goguen [11], is examined more completely in a large book by Gottwald [14] as
well as in [1]. These books also describe in detail how the various formal systems of
fuzzy logic were developed within the framework of many-valued logics. I do not
cover these theoretical developments in this paper, which is primarily oriented to
applications of fuzzy logic.
In the next section, which is the kernel of this paper, I examine applications of
fuzzy logic in various areas of human affairs. In each area, I focus on the dis-
crepancies between expectations and reality and I try to ﬁnd plausible explanations
for these discrepancies.
3
Applications of Fuzzy Logic
3.1 Motivations for Introducing Fuzzy Sets
The introduction of the concept of a fuzzy set by Zadeh in his seminal paper [2] was
based on well-conceived and convincing motivations, which are expressed not only
in the seminal paper, but also in several of his other early publications. Zadeh’s
earliest thought about the need for fuzzy sets is expressed in his 1962 paper [15],
where he writes (page 857):
For coping with the analysis of biological systems, and that to deal effectively with such
systems, which are generally orders of magnitudes more complex than man-made systems,
we need a radically different kind of mathematics, the mathematics of fuzzy or cloudy
quantities.
204
G.J. Klir

He returned to this theme a few years later in [16], where he wrote (p. 199):
One cannot help feeling that, on the whole, the degree of success achieved by the use of
mathematical techniques in biosciences has been quite limited. What is more disturbing,
however, is the possibility that classical mathematics—with its insistence on rigor and
precision—may never be able to provide totally satisfying answers to the basic questions
related to the behavior of animate systems.
The importance of fuzzy sets for biology, but also for psychology and other so-
called soft sciences was also explicitly recognized by Goguen in his Introduction to
[12]:
The ‘hard’ sciences, such as physics and chemistry, construct exact mathematical models to
make predictions. Certain aspects of reality always escape such models, and we look
hopefully to future reﬁnements. But sometimes there is an elusive fuzziness, a readjustment
to context, or an effect of observer upon observed. These phenomena are particularly
indigenous to natural language, and are common in the ‘soft’ sciences, such as biology and
psychology.
In fact, this whole paper is devoted to the investigation of imprecise concepts,
primarily from the psychological point of view.
In his seminal paper, Zadeh emphasized that fuzzy sets provides a natural tool
for “dealing with problems in which the source of imprecision is the absence of
sharply deﬁned criteria of class membership rather than the presence of random
variables.” The same year, he illustrated in [17] these problems by those of opti-
mization under ill deﬁned constrains. Two years later, in a joint paper with Bellman
and Kalaba [18], they were illustrated by problems of abstraction and pattern
classiﬁcation and later, in another joint paper [19], by decision-making problems in
which “the goals and/or constraints constitute classes of alternatives whose
boundaries are not sharply deﬁned.” The suggested use of fuzzy set theory in
dealing with these problem areas—decision making, pattern classiﬁcation and/or
recognition, and optimization—attracted quickly attention of a small group of
enthusiastic researchers who made substantial advances in these areas already in the
1970s. This is well documented in the early monographs by Kickert [20] on fuzzy
decision making and in the book by Bezdek [21] on fuzzy pattern recognition. The
former also contains a survey of associated fuzzy optimization methods, such as
fuzzy linear programming or fuzzy dynamic programming. Research on the use of
fuzzy logic in these problem areas has even intensiﬁed since the 1970s and has
produced a remarkable spectrum of important results. These applications are cer-
tainly among the most successful applications of fuzzy logic.
In the following, I examine the development of applications of fuzzy logic in
various areas of science, engineering and other areas of human affairs. In each of
considered areas, I focus on comparing the expected developments with the actual
ones. I show that in many cases, the actual developments have turned out very
differently from the expectations and I try in each such case to explain reasons for
the discrepancy.
On Discord Between Expected and Actual Developments …
205

3.2 Engineering
Around the time fuzzy set theory emerged, engineering was not seen as an area in
which the use of fuzzy sets was needed. Yet, one of the most successful and visible
applications of fuzzy set theory was an engineering application—fuzzy control.
Except for the applications in decision making, pattern recognition, and optimi-
zation, mentioned in Sect. 3.1, this was also one of the earliest applications of fuzzy
set theory. Let me explain circumstances that led to this early and exceedingly
successful, but highly unexpected application of fuzzy set theory.
Seven years after publishing his seminal paper, LotﬁZadeh wrote a short, two-
page note [22], in which he argued that the excessive concern with precision and
mathematical rigor in conventional control theory has become counterproductive
because it tends to focus the research in this area only on problems that allow of
exact solution. Hence, problems that are too complex or ill deﬁned to admit of
precise mathematical analysis are avoided as mathematically intractable. He sug-
gested dealing with such “intractable” control problems by fuzzy algorithms, which
he already introduced in an earlier paper [23]. To make his suggestion more spe-
ciﬁc, he illustrates it by a simple fuzzy algorithm for guiding a blindfolded person
from an initial position in a room with no obstacles to a desirable ﬁnal position.
It seems from the way this short note was written that Zadeh did not expect that
his suggestion would be actually pursued any time soon, but presented it rather as a
long-term perspective. However, contrary to Zadeh’s expectations, actual work on
designing, implementing, and testing an experimental fuzzy controller for con-
trolling a small steam engine began shortly after the publication of his note [22] at
Queens Mary College in London by Ebrahim Mamdani with one of his students
(S. Assilian). It was already described, together with some initial experiments in
[24], three years after Zadeh’s note. In his recollections [25], Mamdani describes
circumstances that led to his pioneering work on fuzzy controllers (p. 340):
It was Zadeh’s paper [22] published at that time which persuaded us to use a fuzzy rule-
based approach. Between reading and understanding Zadeh’s paper and having a working
controller took a mere week and it was “surprising” how easy it was to design a rule-based
controller.
In 1880, the ﬁrst commercial fuzzy controller, inspired by basic ideas of
Mamdani’s design, was permanently installed for controlling a cement kiln owned
by F. L. Smidth & Company in Denmark. The controller successfully replaced
control by human operators with computer-based control and even improved
somewhat the performance and cut fuel consumption. This was a great success
since the control by human operators was too expensive and inconvenient as it took
about eight weeks to train a new operator, and the process to be controlled was in
this case too complex and unwieldy for conventional controller. The fuzzy con-
troller was designed by a Danish engineer at the University of Denmark who left the
university to work at the company to develop a computer-based controller. He tried
to do that by using conventional control theory, but he soon discovered that it was
virtually impossible. Fortunately, he came across Mamdani’s work on fuzzy
206
G.J. Klir

controller described in [24], and the idea immediately appealed to him due to its
focus on modeling knowledge of a well-trained operator rather than on the process
to be controlled. He found that the rules described in the textbook commonly used
for training human operators of cement kilns could be readily represented as if-then
rules in a fuzzy controller of the Mamdani type. This ﬁrst commercial fuzzy con-
troller, which is described in [26], has been subsequently used for controlling many
other kilns, mills, and other complex processes. This was undoubtedly at that time
the most signiﬁcant application of fuzzy logic.
However, much more signiﬁcant applications were at the same time under
development in Japan. One was a sophisticated fuzzy control, involving both
feedback and feedforward features, of fully automatic operation of the subway
system in the city of Sendai. This project was conceived in 1979 by two researchers
at Hitachi Systems development Laboratory, Seiji Yasunobu and Shoji Miyamoto,
who were inspired by the novelty of Mamdani’s fuzzy controller. It is likely that the
successful installation of fuzzy controller for controlling the cement kiln in Den-
mark helped them to convince the upper management to support this large and
rather risky project. The city of Sendai switched from trains operated by human
operators to fully automatic operation based on fuzzy control in 1987, and it was a
huge success in all measures. Details of this sophisticated fuzzy controller are
described in [27].
Success of this project motivated many Japanese industries to invest in various
other applications of fuzzy logic. This resulted in a surprising variety of innovative
and sometimes unexpected applications of fuzzy logic, especially fuzzy controllers,
that turned out to be technically as well as commercially highly successful. One
positive outcome of these developments, which are described in detail in a well-
researched book by McNeill and Freiberger [28], was that the visibility of fuzzy
logic tremendously increased and, as a consequence, industries and governments in
some countries, not only in Japan, became more receptive to support research on
fuzzy logic.
The enormous success of fuzzy controllers is my ﬁrst example of discrepancy
between expected and actual applications of fuzzy logic. Indeed, fuzzy control was
in no way among the factors motivating the need for introducing the concept of a
fuzzy set. Yet, it turned out to be an extraordinarily successful early application of
fuzzy sets. Next, I am going to turn to three natural sciences, biology, chemistry,
and physics.
3.3 Biology
As is explained in Sect. 3.1, the envisioned need for mathematics based on fuzzy
logic in biology was one of the primary motivations for Zadeh to introduce fuzzy
sets. Yet, the biological community has shown virtually no interest in exploring this
emerging new mathematics. Biology is thus one area in which the reasonable
expectations have not realized so far. This is surprising and not easy to explain.
On Discord Between Expected and Actual Developments …
207

However, as I see it, this lack of interest may in this case be at least partially
explained by a huge gap between experimental and theoretical biology and by the
strong dominance of the former one. Indeed, most biologists focus on experimental
work, for which they are trained, and pay little or no attention to mathematics.
Theoretical biologists are a small minority among biologists with little inﬂuence on
biology at large. Most biologists are just not interested in the work of theoretical
biologists. Moreover, none of the few theoretical biologists have shown any interest
in exploring the utility of fuzzy set theory in their theories.
I should add that some interest in the use of fuzzy logic in biology has been
shown for the last ﬁfteen years or so, but only in the context of the rapidly growing
new subarea of biology—bioinformatics—a rather narrow, but highly important
subarea, which is closely connected with Human Genome Project. The objective of
this large international collaborative program, which was implemented during the
period from 1990 to 2003, was to determine structures—that is sequences of
deoxyribonucleic acid (DNA) molecules—of all genes of human beings. The
outcome of the project was a very large database containing structures of all human
genes. This database and other biomolecular databases provide researchers in
molecular genetics with huge amount of information. The challenge is to utilize this
information for advancing biological knowledge by answering many profound
biological questions, such as those regarding functions of the individual genes,
processes leading to the three-dimensional structures of proteins, functions of these
structures, and the like. It is this analytical part of bioinformatics, where the use-
fulness of fuzzy logic was suggested already in 2000 in two early papers [29, 30].
These papers were soon followed by a rapid growth of literature on various
applications of fuzzy logic and soft computing in bioinformatics. Just during the
ﬁrst decade of the 21st century, three large edited books devoted to these appli-
cations were published. In 2008, the time was already ripe for publishing the ﬁrst
monograph on these applications [31].
3.4 Chemistry
Chemistry, similarly as physics, has always been considered as belonging to the so-
called hard sciences. As such, the need for fuzzy logic in chemistry was deﬁnitely
not among the motivations for introducing fuzzy sets (see, e.g., the excerpt from
Goguen’s paper [12] in Sect. 3.1). Hence, researchers in fuzzy logic paid virtually
no attention to chemistry for long time after the emergence of fuzzy set theory in
1965. In the early 1990s, however, a few researchers in theoretical chemistry dis-
covered fuzzy logic and began to recognize that it might be potentially useful in
dealing with some unresolved problems in their area. These problems emanated
from the conventional way of viewing some important concepts in chemistry, such
as symmetry or chirality, as bivalent—either true or false in each of their appli-
cations in chemistry. For example, in paper [32], the authors discuss this issue with
respect to the concept of symmetry as employed in chemistry (p. 7843):
208
G.J. Klir

One of the most deeply-rooted paradigms of scientiﬁc thought is that Nature is governed in
many of its manifestation by strict symmetry laws. The continuing justiﬁcation of that
paradigm lies with the very achievements of human knowledge it has created over the
centuries. Yet we ague that the treatment on natural phenomena in terms of “either/or”,
when it comes to a symmetry characteristic property, may become restrictive to the extent
that some of the ﬁne details of phenomenological interpretation may be lost. Atkins writes
in his widely-used text on physical chemistry:3 “Some objects are more symmetrical than
others”, signaling that a scale, quantifying this most basic property, may be in order. The
view we wish to defend in this report is that symmetry can be and, in many instances,
should be treated as a “gray” property, and not necessarily as a black or white property
which exists or does not exist. Why is such continuous symmetry measure important? In
short, replacing a “yes or no” information processing ﬁlter, which acts as a threshold
decision-making barrier which differentiate between two states, with a ﬁlter allowing a full
range of “maybe’s”, enriches, in principle, the information content available for analysis.
In two follow-up papers published in the J. of the American Chemical Society—
115(24), 1993, and 117(1), 1995—the authors further elaborated on the continuous
symmetry measure and introduced, in addition, a continuous chirality measure.
Similar observations and arguments regarding the need to abandon the principle of
bivalence for dealing with some chemical concepts were at the same time advanced
by a fair number of other researchers in theoretical chemistry. It was also
increasingly recognized that it should be beneﬁcial to utilize fuzzy logic for dealing
with these problems.
It was eventually decided to devote one of the annual Mathematical Chemistry
Conferences fully to the role offuzzy logic in chemistry. The title of the conference—
Are the Concepts of Chemistry All Fuzzy?—captured quite well the primary issues
discussed within the area of theoretical chemistry at that time. The conference was
held in 1996 at conference facilities of a major distillery—very appropriate for a
conference of this kind, in Pitlochry, Scotland, in 1996. I was invited to present a
tutorial on fuzzy logic and to represent the fuzzy-logic community. A major outcome
of this conference was a book entitled “Fuzzy Logic in Chemistry”, carefully edited
by Dennis Rouvray [33]. It consists of nine rather extensive chapters that are loosely
based on presentations at the conference. The book convincingly demonstrate that
fuzzy logic is useful not only for representing realistically some fundamental
chemical concepts, such as symmetry chirality, molecular structure, or molecular
shape and size, but also for dealing properly and effectively with some methodo-
logical problems in chemistry, such as problems of molecular recognition, hierar-
chical classiﬁcation, or computer-aided elucidation of molecular structures.
As far as I know, no additional books on fuzzy logic in chemistry have been
published. However, after the publication of [33], fuzzy logic has been routinely
utilized in chemistry not only for dealing with the above-mentioned conceptual
problems, but with various other problems as well. In other words, fuzzy logic has
been rather naturally recognized in chemistry as useful. This situation is clearly
radically different from the corresponding situation in biology, which is described
in Sect. 3.3. In biology, the utility of fuzzy logic was strongly anticipated, but it has
3Atkins, P. W.: Physical Chemistry, 3rd Edition, p. 406. Oxford Univ. Press, Oxford (1986).
On Discord Between Expected and Actual Developments …
209

not yet been recognized by the biological community, with the exception of the
very recent and still rather modest recognition of its role in bioinformatics. In
chemistry, no use of fuzzy logic was anticipated and yet, its utility was discovered
already in the early 1990s by researchers in theoretical chemistry and has gradually
been accepted by the entire chemical community.
3.5 Physics
Physics, which is undoubtedly the most advanced and successful area of science,
was certainly not among the areas in which the need for fuzzy logic was anticipated.
It is perhaps due to the enormous success of physics why the developed mea-
surement routines in physics have virtually never been questioned within the
physics community. A rare dissenter was the outstanding American physicist Percy
Williams Bridgman (1882–1961).4 His rather unorthodox views about measure-
ment in physics are captured reasonably well in the following excerpts from one of
his papers ([34], 227–228, italics added):
The physics of measurement and of the laboratory does not have the yes-no sharpness of
mathematics, but nevertheless employs conventional mathematics as an indispensable tool.
Every physicist combines in his own person, to greater or less degree, the experimental
physicist who makes measurements in the laboratory, and the theoretical physicist who
represents the results of the measurements by the numbers of mathematics. These numbers
are things he says or writes on paper. The jump by which he passes from the operations of
the laboratory to what he says about the operations is a jump which may not be bridged
logically, and is furthermore a jump which ignores certain essential features of the physical
situation. For the mathematics which the physicist uses does not exactly correspond to what
happens to him. In the laboratory every measurement is fuzzy because of error. As far as
reproducing what happens to him is concerned, the mathematics of the physicist might
equally well be the mathematics of the rational numbers… Now one would certainly be
going of one’s way to attempt to force theoretical physics into a straightjacket of the
mathematics of rational numbers as distinguished from the mathematics of all real numbers,
but by forcing it into the straightjacket of any kind of mathematics at all, with its yes-no
sharpness, one is discarding an essential aspect of physical experience and to that extent
renouncing the possibility of exactly reproducing that experience. In this sense, the com-
mitment of physics to the use of mathematics itself constitutes, paradoxically, a renunci-
ation of the possibility of rigor….Now it appears to me, the linkage of error in every sort of
physical measurement must be regarded as inevitable when it is considered that the
knowledge of the measurement, which is all we can be concerned with, is a result of the
coupling of the external situation with a human brain. Even if we had adequate knowledge
of the details of this coupling we admittedly could not yet use this knowledge in formu-
lating in detail how the unavoidable fuzziness should be incorporated in our description of
the world nor how should we modify our present use of mathematics, but with the addi-
tional caveat to every equation, warning that things are not quite as they seem.
4Bridgman was an excellent experimental physicist who had been most of his academic career with
Harvard University. In 1946, he won the Nobel Prize in Physics for his groundbreaking work on
the physics of high pressures. He also wrote extensively on measurement in physics and on various
other aspects of philosophy of science.
210
G.J. Klir

The use of the terms fuzzy and fuzziness by Bridgman in 1959, ﬁve years before
Zadeh’s seminal paper [2], is certainly interesting, especially because his use of
these terms is quite similar to Zadeh’s use. In fact, these terms appear even in some
of Bridgman’s earlier writings.
Bridgman’s critical comments about measurement in physics did not have any
visible impact on physics during his lifetime. However, since the early 1990s some
physicists specializing on measurement have occasionally referred to Bridgman’s
criticism and to the potential role of fuzzy sets in physical measurement; see, for
example a representative paper by Mari [35].
Finally, I should look at the potential role of fuzzy logic in quantum mechanics.
There is an extensive literature on this topic, too large and complex to be even
brieﬂy surveyed in this paper. In any case, none of the many logics, fuzzy or non-
fuzzy, which have been proposed for quantum mechanics thus far, has not been
generally accepted as yet. The situation is well characterized in the monograph by
Chiara et al. [36], which to my best knowledge is the only one that covers logics
that recognize the principle of bivalence as well as those that do no recognize it.
They are referred to in the book as sharp quantum logics and unsharp quantum
logics, respectively. The authors seems to be well aware of the importance of the
prospective unsharp quantum logics, and make this interesting observation (p. 5):
Strangely enough, from the historical point of view, the abstract researches on fuzzy
structures and on quantum structures have undergone quite independent developments for
many decades during the 20th century.Only after the Eighties, there emerged an interesting
convergence between the investigations about fuzzy and quantum structures, in the
framework of the so-called unsharp approach to quantum theory. In this connection a
signiﬁcant conjecture has been proposed: perhaps some apparent mysteries of the quantum
world should be described as special cases of some more general fuzzy phenomena, whose
behavior has not yet been fully understood.
It is also signiﬁcant that on page 37 of this book, the authors describe a speciﬁc
example in quantum theory, in which the principle of bivalence fails.
Physics is thus an area of science in which the use of fuzzy logic was not
expected at all. Nevertheless, its utility in physical measurements was recognized
by some physicists, such as Bridgman a more recently Mari and others. Even more
importantly, fuzzy logic is likely to play some role, potentially a very important role
in some of its incarnations, in quantum mechanics, as suggested in the above
quotation from [36].
3.6 Geology
Similarly, as it was not expected that fuzzy logic would play any useful role in
chemistry, it was not expected that it would be of any use in geology. Now we
know that both of these expectations were wrong. The similarities extend further. In
both of these areas, the utility of fuzzy logic was not recognized by researchers
outside these areas (e.g. those who worked on fuzzy logic and were searching for
On Discord Between Expected and Actual Developments …
211

applications), but researchers within these very areas, and it was recognized in both
areas within approximately the same period—roughly during the 1990s. It seems
that this timing could be explained by the great success of fuzzy logic in the 1990s,
as is explained in Sect. 3.2, which signiﬁcantly increased its visibility.
With the emergence of computer technology around the middle of the 20th
century, geology has undergone a major transformation regarding the nature of
geological knowledge. Since the 19th century, geology has been preoccupied pri-
marily with attempts to understand how the surface of the Earth had developed. A
substantial amount of knowledge was produced by the work of many geologists via
their extensive, systematic, and painstaking observations, combined with com-
monsense reasoning. Knowledge obtained in this way was of course expressed by
the geologists in natural language, without any use of mathematics. After the
emergence of computer technology, this knowledge was gradually dismissed as
useless, as it could not be represented in a computer-acceptable language. This led
to the development of mathematical geology. When some geologist discovered
fuzzy logic around the mid 1990s and became familiar with its capabilities at that
time, they tried to experiment with it by simulating some of the knowledge
described verbally in the older geological books, often directly in the form of if-then
statements. The ﬁrst such simulation was described in 1996 in a paper by Nordland
[37], where it was successfully illustrated by a particular example from the area of
dynamic stratigraphic modeling.
The paper stimulated a fair number of other geologists to pursue similar studies
not only in the same area but also in various other areas of geology. They were
astonished by the excellent results they obtained and that motivated further research
into the use of fuzzy logic in geology. As a result, the literature on applications of
fuzzy logic in geology grew very rapidly at the end of 20th century, and it was
generally felt that the time was ripe for a comprehensive book overview of this
alternative approach to dealing geological problems, once abandoned and then
rediscovered with the help of fuzzy logic. In fact, LotﬁZadeh explicitly suggested
that such a book be published.
The book on fuzzy logic in geology was eventually published in 2004 [38] and
Zadeh wrote a wonderful Foreword to it. The book contains a tutorial on fuzzy logic
for geologists, and a comprehensive overview with a literature review of all rec-
ognized applications of fuzzy logic in geology. In addition, it contains several
chapters that describe in detail applications of fuzzy logic in the areas of strati-
graphic modeling, hydrology and water resources, paleontology, and seismology,
as well as the use of fuzzy logic for dealing with the problems of reef growth and
ancient sea level estimation.
Since the publication of [38], the literature dealing with applications of fuzzy
logic in geology as well as other areas of Earth sciences has substantially expanded
including several monographs and edited volumes. This indicates that the utility of
fuzzy logic is well recognized in this domain.
212
G.J. Klir

3.7 Psychology
Psychology and biology were two areas of science in which fuzzy logic was
expected to play an important role. In fact, the need for mathematics based on fuzzy
logic in these areas was among the main motivations for introducing the concept of
a fuzzy set. As is explained in Sect. 3.3, the expectation has not realized in biology.
It has not realized in psychology as well, but for very different and more compli-
cated reasons. In the following, I am going to brieﬂy survey the history of con-
nections between psychology and fuzzy logic, which is quite extraordinary.
One year before the publication of Zadeh’s seminal paper on fuzzy sets, Robert
Duncan Luce (1925–2012), one of the preeminent ﬁgures in mathematical psy-
chology, published a paper [39], in which he wrote (p. 376):
The language of sets does not always seem adequate to formulate psychological problems.
Put it so baldly, the statement is almost heretical since, in practice, set theory is the accepted
way to formulate mathematical problems and, hence, applied mathematical problems. Still,
we should not forget that set theory is really quite new—less than a century old. It could be
an interim theory. Certainly, when I think about certain psychological problems, I wish it
weren’t the way it is. The boundaries of my “sets,” and of ones that my subjects ordinarily
deal with, are a good deal fuzzier than those in mathematics…. It is quite difﬁcult to pin
down just what elements are and are not members of that set, and I am not sure that it is
possible in principle. Do we merely lack techniques adequate to answer that question today,
or is it basically impossible to answer it?
Luce repeatedly returned in his many publications to the issues raised in this
short excerpt, especially the question posed in the last sentence. Although he has
frequently used in his writings the term “fuzzy” in its various forms, it is unfor-
tunate that he was apparently not aware throughout his whole lifetime about the
existence of fuzzy set theory and fuzzy logic.
Another connection between psychology, especially the psychology of con-
cepts,5 and fuzzy logic was introduced in the classic paper by Goguen [12], from
which I use a few short excerpts (pp. 325–326):
“Exact concepts” are the sort envisioned in pure mathematics, while “inexact concepts” are
rampant in everyday life…. Ordinary logic is much used in mathematics, but applications to
everyday life have been criticized because our normal language habits seem so different.
Various modiﬁcations of orthodox logic have been suggested as remedies, particularly
omission of the Law of Excluded Middle.Ordinary logic represents exact concepts syn-
tactically: that is a concept is given a name (such as ‘man’) which becomes an object of
manipulation in a formal language…. Another representation is the semantic, as in Can-
torian set theory. Here we consider the collection or set of elements exemplifying the
concept and study such manipulations as might be performed on actual physical collections:
lumping together, removing a subcollection, and so on. The laws of set theory describe
5In general, a concept is viewed in psychology as a mental representation of a class of real or
abstract entities, which is usually called a concept category. In the psychology of concepts, a
concept is usually viewed more speciﬁcally as a body of knowledge regarding the entities in the
associated concept category that is stored in the long-term memory (sometimes called a semantic
memory) and employed by default in most of cognitive processes.
On Discord Between Expected and Actual Developments …
213

general properties of these manipulations.Without a semantic representation for inexact
concepts, it is hard to see that one modiﬁcation of traditional logic really provides a more
satisfactory syntactic theory of inexact concepts than another. However, such a represen-
tation is now available. Zadeh [2] has studied fuzzy sets, and suggested a number of
concrete applications.
Another connection between psychology and fuzzy logic emerged from
groundbreaking psychological experiments that were performed by Eleanor Rosch
in the early 1970s and published in a series of articles, two of which are [40, 41].
These experiments consistently demonstrated that membership in concept catego-
ries is not a yes-or-no matter, but rather a matter of degree. This led to an almost
universal rejection of the classical view of concepts, in which each concept category
is deﬁned by a collection of attributes that are both necessary and sufﬁcient. Rosch’s
experiments also revealed that each concept category is associated with an ordering
relation that reﬂects the typicality of individuals in the category as examples of the
concept. The most typical individual(s) can be viewed as natural prototype(s) of the
category. The above-mentioned typicality ordering can then be deﬁned via a suit-
able similarity measure, as thoroughly investigated in the psychological context by
Tversky [42]. This is brieﬂy the essence of a prototype view of concepts that
emerged form Rosch’s experiments as a natural successor to the classical view of
concepts.
Although the results obtained by Rosch and the emerging prototype view of
concepts were suggestive of possible use of fuzzy sets in the psychology of con-
cepts, Rosch herself did not seem to be interested in exploring it. However, her
results have stimulated other psychologists to examine the potential role of fuzzy
sets in psychology. This led to a lively discussion of this issue in psychological
literature throughout the 1970s. However, this positive attitude toward fuzzy logic
has visibly changed to negative attitude since the early 1980s. It is now well
established that this change was triggered by a paper published in 1981 by two
highly inﬂuential cognitive psychologists, Daniel Osherson and Edward Smith [43],
whose aim was a critique of the prototype theory of concepts. This is how they
describe the organization of their paper ([43], p. 36):
We ﬁrst present one version of prototype theory. We then show how it might be extended to
account for conceptual combinations by means of principles derived from fuzzy-set theory.
This extension is demonstrated to be fraught with difﬁculties. We then move on to the issue
of truth conditions for thought, again using fuzzy-set theory as a means of implementing the
prototype approach, and again demonstrating that this implementation won’t work. In a
ﬁnal section, we establish that our analysis holds for virtually any version of prototype
theory, and consider ways of reconciling previous evidence for this theory with the wisdom
of the older kind of theory of concepts.
This paper had such a strong inﬂuence on attitudes toward fuzzy set theory in
psychology that fuzzy set theory was virtually dismissed by the psychological
community as useless. Only some twenty years after the paper by Osherson and
Smith was published, some awkward mathematical errors were accidentally dis-
covered in it. This led to a detailed analysis of all claims about fuzzy set theory in
the paper, which revealed, surprisingly, that they were virtually all erroneous, as is
214
G.J. Klir

shown in [44]. Further investigation [45] revealed how these erroneous claims were
uncritically accepted within the psychological community and used often as
arguments against fuzzy set theory. This extraordinary episode is fully documented
in a book that I coedited with Radim Belohavek [46]. However, the primary aim of
this book is to renew the dialog and hopefully a cooperation of researchers in
psychology with those working in the area of fuzzy logic.
The up-and-downs regarding the use fuzzy logic in psychology certainly do not
coincide with the original expectations. One reason might be that fuzzy logic is not
yet properly developed for its speciﬁc use in psychology. This in fact was already
extensively argued in the late 1980s by Fuhrmann in several of his papers (see, e.g.,
his paper [47]). If he is right, then the cooperation between the two areas will be
essential.
3.8 Economics
Economics is generally viewed as the most advanced social science, primarily due
to the extensive role that mathematics has played in it since the late 19th century. It
is well known, however, that the mathematically ever more sophisticated economic
theories have almost never produced accurate and practical economic predictions,
while experience economists are often able to formulate fairly accurate and useful
economic predictions in linguistic terms, such as “The rate of inﬂation is likely to
increase substantially in the very near future.” Such predictions are based on
common sense reasoning, employing the economist’s knowledge and relevant
information, both expressed in natural language. Due to these observations, fuzzy
logic was broadly expected, soon after it emerged in the mid 1960s, to play an
important role in economics.
This expectation became in some sense a reality in the 1980s through the work
of some French economists under the leadership of distinguished French economist
Claude Ponsard (1927–1990). Inﬂuenced by the early publication of four-volume
French book on fuzzy sets by Arnold Kaufmann,6 Ponsard began to explore the use
of fuzzy set theory in economics in the late 1970s. In one of his early papers [49], he
shows, for example, how fuzzy sets can be used for reformulating the classical
theory of consumer behavior in mathematical economics by discarding its two
unrealistic assumptions, that the consumer can perfectly discriminate between
different goods and that goods satisfying consumer’s needs are all supplied at a
unique point in space. The result is a considerably more realistic theory of consumer
behavior. During the ﬁrst half of the 1980s, Ponsard published a series of papers, in
which he fuzziﬁed other areas of classical economics, and which culminated in the
publication of a book he co-edited with his colleague Bernard Fustier [50]. The
6Introduction a la Theorie des Sous-Ensembles: vol. 1 (1973); vol. 2 (1975); vol. 3 (1975); vol. 4
(1977). Masson et Cie Editeurs. Only the ﬁrst volume was published later in English [48].
On Discord Between Expected and Actual Developments …
215

books contains, ten important papers on fuzzy economics, all written by members
of the Institute for Mathematical Economics at the University of Dijon, which was
at that time directed by Ponsard. The book contains two papers by Ponsard, one on
a theory of spatial general equilibrium in fuzzy economics and one on viewing
spatial oligopoly7 as a fuzzy game. One year after publishing the edited book,
Ponsard generalized the famous Nash equilibrium concept8 by showing that each n-
person non-cooperative fuzzy game with mixed strategies has at least one equi-
librium point.
In 1988, Ponsard begins his excellent survey paper [51] on established fuzzy
models in economics with a question raised by Zadeh in his Foreword to the
classical book by Zimmermann [52]: “Are there, in fact, any signiﬁcant problem
areas in which the use of the theory of fuzzy sets leads to results which could not be
obtained by classical methods”? And he closes the paper by answering the question:
“In economics, the answer is positive. The use of fuzzy subset theory leads to
results which could not be obtained by classical methods.”
In 1988, Ponsard also began to work on a major book with a tentative title
“Fuzzy Economic Space: An Axiomatic Approach”. When he unexpectedly passed
away in 1990, the book manuscript was not yet fully completed. Fortunately, his
main ideas are preserved and further developed in an important book by Billot [53],
who was Ponsard’s doctoral student at that time.
This section would be rather incomplete without mentioning the work by a
British economist George Shackle on the theory of graded possibilities within the
context of economics, long before the theory was interpreted in terms of fuzzy sets
by Zadeh [54]. Due to the limited space of this paper, I take the liberty to refer to
my paper [55], in which I outline Shacke’s unorthodox approach to economics and
describe in fair detail his work on the theory of graded possibilities.
The utility of fuzzy set theory in economics has not yet been fully recognized by
mainstream economists. Nevertheless, the work by Ponsard and the other French
economists, together with the work by Shackle, is sufﬁciently signiﬁcant and
convincing to conclude that the early expectations by the fuzzy community that
fuzzy set theory would play an important role in economics have already been at
least partially met.
3.9 Other Social Sciences
The usefulness of fuzzy set theory in all social sciences, not only in economics, was
generally expected when the theory emerged in the mid 1960s. Zadeh, for example,
devoted one of his early articles fully to this issue [56].
7Market situation inﬂuenced by a few producers.
8Nash, J. F. Equilibrium points in n-person games. Proc. of the National Academy of Sciences 36,
48-49 (1950).
216
G.J. Klir

The classical book by Smithson [57] attracted some attention to the useful role of
fuzzy sets in social sciences, but for many years almost exclusively within the
fuzzy-set community. Fortunately, it eventually attracted the attention of Charles
Ragin, a social scientist, who became seriously interested in exploring the role of
fuzzy sets for bridging the gap between the qualitative and quantitative methods in
social-science research. This led him to write the whole book on this subject [58].
While some social scientists praised the book, most remained skeptical about his
ideas, as is well captured by the following short excerpts from Ragin’s Introduction
to the book (p. 3):
Social scientists generally stay away from anything labeled “fuzzy” because their work is so
often described this way by others, especially by scholars in the “hard” sciences. My initial
title for this book, Fuzzy Social Science, made so many of my colleagues cringe that I felt
compelled to change it so that the adjective “fuzzy” applied to sets, not to social science.
Eight years later, another book by Ragin was published [59], in which he
challenges the conventional approach to social science research and proposes an
alternative approach based on fuzzy set theory that overcomes the various limita-
tions of conventional quantitative as well as qualitative social-science research. He
argues and demonstrates experimentally that the proposed approach has the capa-
bility to narrow the gap between knowledge obtained by qualitative social scientists
and that obtained by quantitative social scientists.
When taken together, the two books by Ragin form an important statement about
the utility of fuzzy set theory in social sciences. Unfortunately, the approach to
social science research proposed by him has not yet been widely accepted by social
scientists.
Perhaps the most important contribution to the use of fuzzy logic in social
sciences at large is at this time the book by Badredine Arﬁ[60], a Finnish political
scientist. He further develops in the book the idea of computing with words, ﬁrst
suggested by Zadeh [7, 8], and applies it to a wide range of problems in social
sciences. In his methodology, he allows both membership grades and truth values to
be linguistic variables. The book contains Forewords by Ragin and Zadeh who both
highly praise it.
Interesting applications of mathematics based on fuzzy logic in political science
emerged from collaboration of political scientists with mathematicians at Creighton
University in Omaha, Nebraska. These applications, which are described in detail in
[61], show that the concept of fuzzy geometry is superior for dealing with some
problems in comparative politics in comparison with the traditional use of classical
Euclidean geometry.
I consider it reasonable to conclude that the expected utility of fuzzy logic in
social sciences has already been demonstrated, even though fuzzy logic has not yet
been fully endorsed within these areas. It is interesting that the most negative
attitude toward fuzzy logic is shown by the quantitative (or mathematical) social
scientists.
On Discord Between Expected and Actual Developments …
217

3.10 Medicine
The need for fuzzy sets in medicine did not motivate, at least not explicitly, their
introduction. However, the utility of fuzzy sets in medicine was recognized quite
early, in about the mid 1970s, and primarily in the area of medical diagnosis.
The use offuzzy set theory in medical diagnosis was ﬁrst suggested and discussed
in a doctoral dissertation by Albin [62], followed shortly by two early papers by
Sanchez [63, 64]. In these papers, Sanchez formulated medical diagnosis in terms of
fuzzy relational equations, which he introduced and investigated in his earlier paper
[65]. A few additional papers regarding the use of fuzzy set theory in medical diag-
nosis were published in the 1970s. However, concentrated and systematic research on
fuzzy-set-based system for computer-assisted medical diagnosis began only in the
1980 and mostly at the Department of medical Computer Science of the University of
Vienna Medical School in Austria under the leadership of Klaus Peter Adlassnig.
Accomplishments of this research over the last two decades of the 20th century are
concisely described in [66]. Various other types of applications of fuzzy set theory in
medicine were also developed during this period and are surveyed in [67].
The literature on applications of fuzzy set theory in medicine rapidly increased in
the new millennium, including some specialized monographs, such as [68–70], and
numerous edited volumes, exempliﬁed by [71] A particularly signiﬁcant is the
scholarly work by Kazem Sadegh-Zadeh9 in analytic philosophy of medicine. In
many of his papers published since 2000, he has consistently argued on both
medical and philosophical grounds that fuzzy logic is the only adequate logic for
medical practice. This argumentation is completely and coherently covered in
Handbook of Analytic Philosophy of Medicine [72], which is a sort of climax of his
lifelong work. This large monograph, consisting of 1,133 pages, covers compre-
hensively and in considerable detail the principal philosophical issues associated
with medicine.
About 40 % of the Handbook is devoted to logical issues involved in clinical
reasoning. After showing that classical ﬁrst-order predicate logic is hopelessly
inadequate in medicine as it is capable of representing only a very small fraction of
language employed in medicine, Sadegh-Zadeh then examines the various modal
extensions of classical logic and shows that even with all these extensions classical
logic is still not sufﬁciently expressive to represent medical language. Next, he
examines non-classical logics, such as paraconsistent, intuitionistic, and many-
valued logics, and shows that each individually would help to overcome some
common difﬁculties in medicine, such as dealing with contradictory medical data or
with situations in which the law of excluded middle does not hold. Finally, he
examines fuzzy logic in detail from the medical point of view and shows that it has
9Kazem Sadegh-Zadeh was born in Tabriz, Iran in 1942. In the 1960s and 1970s, he studied
medicine and philosophy at the German universities of Münster, Berlin, and Göttingen. He has
been for many years with the University of Münster, where he worked in the area of analytic
philosophy of medicine, and where he is now s professor-emeritus.
218
G.J. Klir

all the ingredients needed in medicine. This leads him to the conclusion that fuzzy
logic (representing actually a class of logics) is the only one (one class) among all
currently recognized logics that is fully adequate for clinical reasoning in medicine.
I should add that one year after the publication of Sadegh-Zadeh’s Handbook, a
companion volume to it was published [73], which contains 27 chapters on various
applications of fuzzy logic in medicine.
Medicine is thus somewhat different from the other areas examined in previous
sections of this paper as far as the difference between anticipated and actual utility
of fuzzy logic. The need for fuzzy logic in medicine was not (at least explicitly)
among the factors that contributed to the emergence of fuzzy logic in the mid 1960s.
However, its potential utility in medicine was recognized quite early, in the mid
1970s, and it was expected that it would play an important role in medicine. It
turned out, eventually, that fuzzy logic is the only adequate logic for medicine,
which even exceeded the expectations.
3.11 Management and Business
The need for fuzzy logic in the areas of management and business was not among
the motivations for introducing fuzzy sets. Although some scattered applications of
fuzzy set theory to various problems related to these areas, such as optimization,
scheduling and resource allocation, began to appear in the literature since the late
1970s, the role of fuzzy set theory in these areas was for the ﬁrst time systematically
discussed in a textbook by George and Maria Bojadziev [74] published 1997. Two
years later, a very impressive survey of applications of fuzzy set theory in man-
agement, consisting of four large chapters, is included in [67]. Three of the chapters
deal, respectively, with strategic planning, research and development planning, and
production planning and scheduling. The forth one is devoted to the use of fuzzy
sets in actuarial science, where the book by Ostaszewski [75] —the ﬁrst and still the
only book on using fuzzy set theory in actuarial science—should be highlighted.
In the new millennium, publications devoted to the use of fuzzy set theory in
management and business virtually exploded. As far as management is concerned,
the most important seems to the monograph by Carlsson et al. [76]. In business, two
signiﬁcant monographs deserve to be mentioned in this very short overview [77,
78]. The number of edited volumes in this area is too large to even mention a few
representative samples.
In summary, the utility of fuzzy set theory was not initially recognized in either
management or business. However, some applications of the theory, developed some
ten years after fuzzy set theory emerged, were already indicative that fuzzy sets might
be useful in these areas. This was more explicitly recognized and discussed in the late
1990s. Since that time, the development of applications of fuzzy set theory in
management and business has been very rapid, which showed manifestly that the
theory is of great utility in these areas. As in medicine, discussed in Sect. 3.10, the
actual success of fuzzy set theory in these areas exceeded all expectations.
On Discord Between Expected and Actual Developments …
219

3.12 Music
Within the large variety of the arts, the use of fuzzy logic have occasionally been
suggested in some branches of the arts, such as painting, sculpture, architecture,
poetry, and others, but no signiﬁcant interest resulted from these rather isolated and
ad hoc suggestions. A rare exception is music, where the use of fuzzy logic turns
out to be very natural and signiﬁcant. In order to explain this rather bold claim, I
begin with a short excerpt from Preface to one of the prime monographs on
mathematical theory of tone systems [79]:
There are four important and mutually interacting attributes that we can manipulate to
create or describe any sound. And we can work with these attributes in two different ways:
We can measure them and we can hear them. If we measure them, they are physical
attributes; if we hear them, they are perceptual attributes. The four physical attributes are:
frequency, amplitude, waveform, and duration. Their perceptual counterparts are: pitch,
loudness, timbre, and (psychological) time. There is similarity between hearing and mea-
suring these attributes; however, it is a complex correlation. The two are not exactly
parallel.
As is well captured by this excerpt, the basic elements of music—musical
tones—can be viewed and studied either as physical entities produced by various
musical instruments or as perceptions of these physical entities by humans.
It is well established that human auditory perceptive capabilities are remarkably
tolerant (or insensitive) to small deviations from the ideal (physical) frequencies
representing individual tones. That is, tones whose actual frequencies are sufﬁ-
ciently close to the ideal frequency deﬁning a particular tone in a given tone system
are perceived as the same pitch. In a similar way, human perception is tolerant to
small deviations from the ideal values of the other three physical attributes.
This fundamental dichotomy between physical and perceptual entities applies
not only to individual tones, but to various systems of musical tones as well. The
two most important characteristics of each tonal system are the pitch of each tone
recognized in the system and the pitch differences of any two of the recognized
tones. The former is a psychological concept that represents approximately the tone
frequency. The latter, called musical intervals, are physically deﬁned as ratios of
their frequencies, which perceptually are approximate ratios.
A particular interval whose frequency ratio is 2 is called an octave. Two tones
whose distance is equal to one or more octaves are viewed in the physical domain
as equivalent, and are perceived as approximately equivalent. Most frequently,
especially in Western classical music, 12 tones within each octave are chosen
according to some rules that govern the intervals between consecutive tones in each
octave. The notes together with their locations within each octave form a particular
tonal system. When tones in these systems are viewed a physical entities, classical
mathematics based on bivalent logic is perfectly adequate to deal with them.
However, when the tones are viewed as perceptual entities, the best classical
mathematics can do is to employ intervals of real numbers for representing the
220
G.J. Klir

perceptual tolerances,10 but this is a rather poor representation. Much better rep-
resentation can be obtained by mathematics based on fuzzy logic. The concept of
“being sufﬁciently closed to a number expressing the ideal frequency” can be
approximated in a natural way by an appropriate fuzzy number (granule) con-
structed on the basis of available knowledge regarding characteristics of human
auditory perception.
This fuzzy approximation plays an especially important role in the so-called
well-tempered tuning within a given tonal system. The aim of this tuning is to make
small deviations (tolerated by human perception) from perfect tuning in each key in
order to achieve a perceptually acceptable tuning in all keys. This allows instru-
ments such as pianos or harps, once tuned in a well-tempered way, to play com-
positions in any key and they are all perceived as well tuned.11 Although in any
well-tempered tuning, the sizes of comparable tone intervals in the physical domain
cannot be the same in all keys, this is generally viewed as a musical advantage, as it
gives a slightly distinctive character to compositions written in different keys.
Except for one early paper by Goguen [80], the literature pointing to the role
fuzzy logic in music is almost exclusively associated with the 21st century. In
addition to the book by Haluška [79], the three papers [81–83] seem to be the most
visible representatives of the growing literature in this area.
3.13 Concluding Remarks
Due to the limited space of this paper, I had to make conscious decisions about
which applications of fuzzy logic to include and which to omit. From the well-
established applications, I chose to omit those in image analysis, spatial information
processing, robotics, risk analysis, database systems, computer vision, and a few
others. Moreover, I did not include any of the many applications that are promising,
but have not yet been adequately developed. These include for example those in
archaeology, paleontology, forensic science, humanities, and numerous other areas.
I also did not include application areas such as the law profession, in which the
utility of fuzzy logic is highly suggestive and potentially very signiﬁcant, but where
its actual use encounters various virtually insurmountable barriers, such as political,
ethical, religious, and others.
10This was actually suggested by the Russian musicologist N. A. Garbuzov in 1948 in his book
entitled “Zonal Nature of the Human Aural Perception (in Russian), published by the Academy of
Sciences of the USSR in Moscow and Leningrad.
11The famous systematic collection of 24 preludes and 24 fugues, each written in all 12 major and
12 minor keys, which are known under the German name “Das Wohltemperierte Klavier” (The
Well-Tempered Clavier), were composed by Johann Sebastian Bach to provide an ultimate
practical test that a piano is properly tuned in a well tempered way. After they are all played on the
piano to be tested and each composition is perceived as well tuned, then the piano may be certiﬁed
as perfectly well-tempered.
On Discord Between Expected and Actual Developments …
221

I hope that this review of how applications of fuzzy logic developed in some
basic areas of human affairs during its ﬁrst ﬁfty years of existence, focusing
especially on the often striking differences between the expected and actual
developments in many of these areas, is an appropriate and potentially useful
reﬂection on accomplishments of fuzzy logic on the 50th anniversary of its genesis.
References
1. Belohlavek, R., Dauben, J.W., Klir, G.J.: Fuzzy Logic and Mathematics: A Historical
Perspective. Oxford University Press, New York (2015)
2. Zadeh, L.A.: Fuzzy sets. Inf. Control 8(3), 338–353 (1965)
3. Zadeh, L.A.: Fuzzy logic and the calculi of fuzzy rules and fuzzy graphs: a précis. Multiple-
Valued Logic 1(1), 1–13 (1996)
4. Hájek, P.: Metamathematics of Fuzzy Logic. Kluwer Academic Publishers, Dordrecht (1998)
5. Yager, R.R., Ovchinnikov, S., Tong, R.M., Nguyen, H.T. (eds.): Fuzzy Sets and Applications:
Selected Papers by Zadeh, L.A. Wiley, New York (1987)
6. Klir, G.J., Yuan, B. (eds.): Fuzzy Sets, Fuzzy Logic, and Fuzzy Systems: Selected Papers by
Zadeh, L.A. World Scientiﬁc, Singapore (1996)
7. Zadeh, L.A.: Fuzzy logic = computing with words. IEEE Trans. Fuzzy Syst. 4(2), 103–111
(1996)
8. Zadeh, L.A.: From computing with numbers to computing with words—from manipulation of
measurements to manipulation of perceptions. IEEE Trans. Circuits Syst.—I. Fundam. Theor.
Appl. 45(1), 105–119 (1999)
9. Zadeh, L.A.: Toward a generalized theory of uncertainty (GTU)—an outline. Inf. Sci. 172(1),
1–40 (2005)
10. Zadeh, L.A.: Generalized theory of uncertainty (GTU)—principal concepts and ideas.
Comput. Stat. Data Anal. 51(1), 15–46 (2006)
11. Goguen, J.A.: L-fuzzy sets. J. Math. Anal. Appl. 18, 145–174 (1967)
12. Goguen, J.A.: The logic of inexact concepts. Synthese 19, 325–373 (1968–69)
13. Rescher, N.: Many-Valued Logic. McGraw-Hill, New York (1969)
14. Gottwald, S.: A Treatise on Many-Valued Logics. Research Studies Press, Baldock (2001)
15. Zadeh, L.A.: From circuits to system theory. IRE Proc. 50, 856–865 (1962)
16. Zadeh, L.A.: Biological applications of the theory of fuzzy sets and systems. In: Proctor, L.D.
(ed.) Biocybernetics of the Central Nervous System, pp. 199–212. Little, Brown and
Company, Boston (1969)
17. Zadeh, L.A.: Fuzzy sets and systems. In: Fox, J. (ed.) System Theory, pp. 29–39. Polytechnic
Press, New York (1965)
18. Bellman, R., Kalaba, R., Zadeh, L.A.: Abstraction and pattern classiﬁcation. J. Math. Anal.
Appl. 13(1), 1–7 (1966)
19. Bellman, R.E., Zadeh, L.A.: Decision making in a fuzzy environment. Manage. Sci. 17(4),
141–164 (1970)
20. Kickert, W.J.M.: Fuzzy Theories on Decision-Making: A Critical Review. Martinus Nijhoff,
Leiden (1978)
21. Bezdek, J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Plenum Press,
New York (1981)
22. Zadeh, L.A.: A rationale for fuzzy control. J. Dyn. Syst. Meas. Contr. (Trans. of ASME, Series
G) 94(1), 3–4 (1972)
23. Zadeh, L.A.: Fuzzy algorithms. Inf. Control 12(2), 94–102 (1968)
24. Mamdani, E.H., Assilian, S.: An experiment in linguistic synthesis with a fuzzy logic
controller. Int. J. Man Mach. Stud. 7(1), 1–13 (1975)
222
G.J. Klir

25. Mamdani, E.H.: Twenty years of fuzzy control: experiences gained and lessons learnt. In:
Proceedings of the Second IEEE International Conference on Fuzzy Systems, pp. 339–344.
IEEE, Piscataway, NJ (1993)
26. Holmblad, L.P., Østergaard, J.-J.: Control of a cement kiln by fuzzy logic. In: Gupta, M.M.,
Sanchez, E. (eds.) Fuzzy Information and Decision Processes, pp. 389–399. North-Holland,
Amsterdam (1982)
27. Yasunobu, S., Miyamoto, S.: Automatic train operation system by predictive fuzzy control. In:
Sugeno, M. (ed.) Industrial Applications of Fuzzy Control, pp. 1–18. North Holland,
Amsterdam (1985)
28. McNeill, D., Freiberger, P.: Fuzzy Logic. Simon & Schuster, New York (1993)
29. Woolf, P.J., Wang, Y.: A fuzzy logic approach to analyzing gene expression data. Physiol.
Genomics 3(1), 9–15 (2000)
30. Sadegh-Zadeh, K.: Fuzzy genoms. Artif. Intell. Med. 18(1), 1–28 (2000)
31. Xu, D., Keller, J.M., Popescu, M., Bondugula, R.: Applications of Fuzzy Logic in
Bioinformatics. Imperial College Press, London (2008)
32. Zabrodsky, H., Paleg, S., Avnir, D.: Continuous symmetry measures. J. Am. Chem. Soc. 114
(20), 7843–7851 (1992)
33. Rouvray, D.H. (ed.): Fuzzy Logic in Chemistry. Academic Press, San Diego (1997)
34. Bridgman, P.W.: How much rigor is possible in physics? In: Henkin, L., Suppes, P., Tarski, A.
(eds.) The Axiomatic Method, pp. 225–237. North Holland, Amsterdam (1959)
35. Mari, L.: Beyond the representational viewpoint: a new formalization of measurement.
Measurement 27(2), 71–84 (2000)
36. Chiara, M.D., Giuntini, R., Greechie, R.: Reasoning in Quantum Theory: Sharp and Unsharp
Quantum Logics. Kluwer, Dordrecht (2004)
37. Nordlund, U.: Formalizing geological knowledge—with an example of modeling stratigraphy
using fuzzy logic. J. Sediment. Res. 66(4), 689–712 (1996)
38. Demicco, R.V., Klir, G.J. (eds.): Fuzzy Logic in Geology. Academic Press, San Diego (2004)
39. Luce, R.D.: The mathematics used in mathematical psychology. Am. Math. Mon. 71(4),
364–378 (1964)
40. Rosch, E.H.: Natural categories. Cogn. Psychol. 4(3), 328–350 (1973)
41. Rosch, E.H.: On the internal structure of perceptual and semantic categories. In: Moore, T.M.
(ed.) Cognitive Development and the Acquisition of Language, pp. 111–144. Academic Press,
New York (1973)
42. Tversky, A.: Features of similarity. Psychol. Rev. 84(4), 327–352 (1977)
43. Osherson, D.N., Smith, E.E.: On the adequacy of prototype theory as a theory of concepts.
Cognition 9, 35–58 (1981)
44. Belohlavek, R., Klir, G.J., Lewis, H.W., Way, E.: On the capability of fuzzy set theory to
represent concepts. Int. J. Gen. Syst. 31(6), 569–585 (2002)
45. Belohlavek, R., Klir, G.J., Lewis, H.W., Way, E.: Concepts and fuzzy sets: misunderstandings,
misconceptions, and oversights. Int. J. Approximate Reasoning 51(1), 23–34 (2009)
46. Belohlavek, R., Klir, G.J. (eds.): Concepts and Fuzzy Logic. MIT Press, Cambridge (2011)
47. Fuhrmann, G.: Fuzziness of concepts and concepts of fuzziness. Synthese 75, 349–372 (1988)
48. Kaufmann, A.: Introduction to the Theory of Fuzzy Subsets, vol. 1. Academic Press, New
York (1975)
49. Ponsard, C.: An application of fuzzy subset theory to the analysis of the consumer’s spatial
preferences. Fuzzy Sets Syst. 5(3), 235–244 (1981)
50. Ponsard, C., Fustier, B. (eds.): Fuzzy Economics and Spatial Analysis. Institut de
Mathématiques Economiques et Librairie de l’Université, Dijon (1986)
51. Ponsard, C.: Fuzzy mathematical models in economics. Fuzzy Sets Syst. 28(3), 273–283
(1988)
52. Zimmermann, H.-J.: Fuzzy Set Theory and Its Applications. Kluwer, Boston (1985)
53. Billot, A.: Economic Theory of Fuzzy Equilibriums: An Axiomatic Analysis. Springer, Berlin
(1992)
54. Zadeh, L.A.: Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets Syst. 1(1), 3–28 (1978)
On Discord Between Expected and Actual Developments …
223

55. Klir, G.J.: Uncertainty in economics: the heritage of G. L. S. Shackle. Fuzzy Economic
Review 7(2), 3–21 (2002)
56. Zadeh, L.A.: Fuzzy systems theory: a framework for the analysis of humanistic systems. In:
Cavallo, R. (ed.) Systems Methodology in Social Science Research; Recent Developments,
pp. 25–41. Kluwer-Nijhoff, Boston (1982)
57. Smithson, M.: Fuzzy Set Analysis for Behavioral and Social Sciences. Springer, NewYork
(1987)
58. Ragin, C.C.: Fuzzy-Set Social Science. Univ. of Chicago Press, Chicago (2000)
59. Ragin, C.C.: Redesigning Social Inquiry. Univ. of Chicago Press, Chicago (2008)
60. Arﬁ, B.: Linguistic Fuzzy Logic Methods in Social Sciences. Springer, Berlin (2010)
61. Clark, T.D., Larson, J.M., Mordeson, J.N., Potter, J.D., Wierman, M.J.: Applying Fuzzy
Mathematics to Formal Models in Comparative Politics. Springer, Berlin (2008)
62. Albin, M.A.: Fuzzy Sets and Their Applications to Medical Diagnosis and Pattern
Recognition. Doctoral Dissertation, Univ. of California-Berkeley (1975)
63. Sanchez, E.: Solution in composite fuzzy relation equations: application to medical diagnosis
in Brouwerian logic. In: Gupta, M.M., Sarids, G.N., Gaines, B.R. (eds.) Fuzzy Automata and
Decision Processes, pp. 221–234. North-Holland, New York (1977)
64. Sanchez, E.: Medical diagnosis and composite fuzzy relations. In: Gupta, M.M., Ragade, R.K.,
Yager, R.R. (eds.) Advances in Fuzzy Set Theory and Applications, pp. 434–444. North-
Holland, Amsterdam (1979)
65. Sanchez, E.: Resolution of composite fuzzy relation equations. Inf. Control 30(1), 38–48 (1976)
66. Adlassnig, K.-P.: The section on medical expert and knowledge-based systems at the
department of medical computer sciences of University of Vienna Medical School. Artif.
Intell. Med. 21(1), 139–146 (2001)
67. Zimmermann, H.-J. (ed.): Practical Applications of Fuzzy Technologies. Kluwer, Dordrecht
(1999)
68. Mordeson, J.N., Malik, D.S., Cheng, S.-C.: Fuzzy Mathematics in Medicine. Spriner,
Heidelberg (2000)
69. Rakus-Andersson, E.: Fuzzy and Rough Techniques in Medical Diagnosis and Medication.
Springer, Berlin (2007)
70. Massa, E., Ortega, N.R.S., De Barros, L.C., Struchiner, C.J.: Fuzzy Logic in Action:
Applications in Epidemiology and Beyond. Springer, Berlin (2008)
71. Barro, S., Marín, R. (eds.): Fuzzy Logic in Medicine. Physica-Verlag, Heidelberg (2002)
72. Sadegh-Zadeh, K.: Handbook of Analytic Philosophy of Medicine. Springer, Dordrecht (2012)
73. Seising, R., Tabacchi, M.E. (eds.): Fuzziness and Medicine: Philosophical Reﬂections and
Application Systems in Health Care. Springer, Berlin (2013)
74. Bojadziev, G., Bojadziev, M.: Fuzzy Logic for Business, Finance, and Management. World
Scientiﬁc, Singapore (1997)
75. Ostaszewski, K.: An Investigation into Possible Applications of Fuzzy Methods in Actuarial
Science. Society of Actuaries, Schumburg (1993)
76. Carlsson, C., Fedrizzi, M., Fullér, R.: Fuzzy Logic in Management. Kluwer, Boston (2004)
77. Aliev, R.A., Fazlollahi, B., Aliev, R.R.: Soft Computing and Its Applications in Business and
Economics. Springer, Berlin (2004)
78. Gil-Lafuente, A.M.: Fuzzy Logic in Financial Analysis. Springer, Berlin (2005)
79. Haluška, J.: The Mathematical Theory of Tone Systems. Marcell Dekker, New York (2004)
80. Goguen, J.A.: Complexity of hierarchically organized systems and the structure of musical
experiences. Int. J. Gen. Syst. 3(4), 233–251 (1977)
81. Haluška, J.: Uncertainty measures of well-tempered systems. Int. J. Gen. Syst. 31(1), 73–96 (2002)
82. Liern, V.: Fuzzy tuning systems: the mathematics for musicians. Fuzzy Sets Syst. 150(1),
35–52 (2005)
83. León, T., Liern, V.: Mathematics and soft computing in music. In: Seising, R., Sanz, V. (eds.)
Soft Computing in Humanities and Social Sciences, pp. 451–465. Springer, Berlin (2012)
224
G.J. Klir

Author Biography
George J. Klir is currently a Distinguished Professor Emeritus
of Systems Science at Binghamton University (SUNY). He has
been with the university since 1969. After his retirement in 2008,
his primary research interests has been in the areas of fuzzy set
theory and fuzzy systems, generalized information theory, and
the theory of generalized measures. Throughout his academic
career, he has written 20 books and well over three hundred
research articles. He has also edited 11 books and has been the
founding editor of the Intern. J. of General Systems since 1974.
In addition, he has served as President of the Society for General
Systems
Research,
International
Federation
for
Systems
Research, North American Fuzzy Information Processing Soci-
ety, and International Fuzzy Systems Association. He is a Life
Fellow of IEEE and IFSA, and has received numerous awards
and honors, including six honorary doctoral degrees, the Gold
Medal of Bernard Bolzano, the Kaufmann’s Gold Medal, SUNY Chancellor’s Award, and IEEE
Fuzzy Systems Pioneer Award. His biography is included in Who’s Who in America, Who’s Who
in the World, American Men and Women of Science, Outstanding Educators of America, and
Contemporary Authors. Prior to his retirement, he had been awarded research support for more
than 20 years by grants from NSF, ONR, Air Force, NASA, NATO, Sandia Laboratories, and
some industries.
On Discord Between Expected and Actual Developments …
225

Meta-Heuristic Optimization of a Fuzzy
Character Recognizer
Alex Tormási and László T. Kóczy
Abstract Meta-heuristic algorithms are well researched and widely used in
optimization problems. There are several meta-heuristic optimization algorithms
with various concepts and each has its own advantages and disadvantages. Still it is
difﬁcult to decide which method would ﬁt the best to a given problem. In this study
the optimization of a fuzzy rule-base from a classiﬁer, more speciﬁcally fuzzy
character recognizer is used as the reference problem and the aim of the research
was to investigate the behavior of selected meta-heuristic optimization techniques
in order to develop a multi meta-heuristic algorithm.
Keywords Fuzzy systems ⋅Fuzzy rule-base optimization ⋅Bacterial evolutionary
algorithm Big bang–big crunch algorithm ⋅Imperialist competitive algorithm ⋅
Particle swarm optimization ⋅Multi meta-heuristics
1
Introduction
Genetic [1], bacterial [2] and other evolutionary and population based meta-heuristic
methods [3, 4] are widely used [5] in various computational intelligence related
optimization problems including the tuning of fuzzy sets [6] and other parameters of
fuzzy rule-based systems [7, 8]. It has both theoretical and technical signiﬁcance to
have a deep knowledge of the behavior of meta-heuristic optimization techniques in
A. Tormási (✉)
Department of Information Technology, Széchenyi István University, Győr, Hungary
e-mail: tormasi@sze.hu
A. Tormási ⋅L.T. Kóczy
Department of Automation, Széchenyi István University, Győr, Hungary
e-mail: koczy@sze.hu; koczy@tmit.bme.hu
L.T. Kóczy
Department of Telecommunication and Media Informatics, Budapest University
of Technology and Economics, Budapest, Hungary
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_13
227

fuzzy rule-base optimization in order to develop new, more efﬁcient and accurate
models. The aim of this study was not to ﬁnd the optimal fuzzy rule-base for a
discrete classiﬁer, more precisely a character recognizer system [9], but to study how
the meta-heuristic optimization algorithms handle a given problem under certain
conditions, to ﬁnd out which are their most sensitive parameters and how they could
be improved – if possible at all.
The investigated meta-heuristics [2–4, 10] were selected according to actual
trends in the ﬁeld and to cover various approaches to optimization algorithms. The
paper also includes results of a multi meta-heuristic [11–13] experiment, where
there is a switch between various optimization algorithms, in order to achieve lower
resource usage with faster convergence to the (quasy)optimum.
Various fuzzy systems are well researched and used in a wide scope of prob-
lems; and in many cases these solutions are more accurate and/or more efﬁcient
compared to other conventional methods. The simple way of knowledge repre-
sentation by fuzzy sets makes these systems a great subject for experimenting with
meta-heuristic optimization. A fuzzy rule-based classiﬁer [14–16], more accurately,
a fuzzy character recognizer [17] was selected as the sample problem used in the
experiment. The reasons of this choice were the presence of fuzzy systems in the
problem, the very wide applicability of the classiﬁers (including theoretical and
technical aspects), and the previous in-detail knowledge of the system and of the
dataset. The meta-heuristic algorithms had to be extended to work with multiple
populations without the ability of migration in order to handle the special features
of the problem.
The paper consists of ﬁve sections; after the introduction in Sect. 2, the studied
meta-heuristic methods are summarized including the modiﬁcations made to them
done in order to ﬁt the sample problem. It is followed by the details of the opti-
mization task (the recognizer engine), the used/investigated parameters and other
aspects of the experiment. The results of the study are presented and interpreted
from various aspects in Sect. 4. Section 5 summarizes the results of the presented
work and discusses the possible directions for a future research.
2
Meta-Heuristic Methods Studied
2.1 Bacterial Evolutionary Algorithm
The Bacterial Evolutionary Algorithm (BEA) [2] is inspired by the evolutional
processes of bacteria. Each bacterium in the population represents a solution in the
problem space.
The algorithm uses two main evolutionary operators the bacterial mutation and
the gene transfer; the ﬁrst step of the algorithm is the bacterial mutation. Each
bacterium is selected individually and cloned a maximal number of times. Each
randomly selected allele of the clones is modiﬁed randomly, the modiﬁed allele of
228
A. Tormási and L.T. Kóczy

the alternative bacterium (or the original one’s) together with the best result is
copied to all other clones; this step is repeated until all the alleles have been
selected.
The second step of the algorithm is the gene transfer (infection), in which the
population is sorted by the goodness of the bacteria and divided into two subsets;
the set of “good” and the set of “bad” bacteria. A randomly selected allele of a
random bacterium from the group of good bacteria is copied to a randomly selected
bad bacterium. This step is repeated until the algorithm reaches the maximum
number of infections.
The above steps of the algorithm are repeated until the maximum number of
generations speciﬁed previously has been reached or until other termination con-
ditions (like 100 % result) have been satisﬁed (as seen on Fig. 1).
2.2 Big Bang–Big Crunch Algorithm
The Big Bang-Big Crunch optimization algorithm [3] uses the concept of a physical
cosmology theory. In this theory the universe expands during the Big Bang event
and then it collapses (Big Crunch) into a black hole (repeatedly). The candidate
solutions are points in the search space; these entities are randomly generated in the
(complete) problem space during the initialization step. In the beginning there could
be a great number of points, which will decrease in the next generations of the
universe.
The algorithm can be divided into two main parts, one is the Big Bang phase and
the other is the Big Crunch phase. These steps are repeated until one of the ter-
mination conditions is satisﬁed.
Fig. 1 Flowchart of the
bacterial evolutionary
algorithm
Meta-Heuristic Optimization of a Fuzzy Character Recognizer
229

The points weighted by their ﬁtness are used to determine the center of gravity
for the next generation of the universe. In the next phase the search space is
narrowed down around this new center. One of the greatest advantages of this
algorithm is the fast decrease of the search space, where the dimensions are handled
individually; in the other hand this property of the method causes its main disad-
vantage: it often converges to a local optimum instead of the global optimum.
2.3 Imperialist Competitive Algorithm
The Imperialist Competitive Algorithm (ICA) [4] is an optimization algorithm
inspired by imperialistic competition, where the points (candidate solutions) in the
problem space are the countries. Initial countries are generated randomly over the
problem space and their strength is calculated by the cost function. The countries
with greater strength are the imperialists, while the weaker solutions are the colo-
nies; empires are formed by imperialists taking control over colonies.
The algorithm uses two main operators in the ﬁrst part: the assimilation and the
revolution; during assimilation the colonies are approaching the imperialist country
(in the problem space), while in the revolution phase the position of some colonies
in the problem space are changed. Colonies may turn over the imperialists by
reaching a better position during their movements in the search space caused by the
previously described operators.
Imperialist competition is the second part of the algorithm. The colonies could
be taken from the weakest empire by the stronger ones; the goal of each empire to
eliminate others by taking over them. The power of an empire is calculated from
the aggregated strength of the imperialist and the colonies. The method to calculate
the power of an empire must ensure that its power will increase even if it takes over
the weakest colony, in other words an empire cannot increase its power by losing its
weakest colony. The above steps are repeated until the stop condition is not satisﬁed
as in Fig. 2.
2.4 Particle Swarm Optimization
The Particle Swarm Optimization (PSO) [10] uses the simpliﬁed model of the
dynamics of movements of various animal swarms (or particles). The solutions are
represented by the particles in the search domain; each particle has a position and a
speed vector. The evolution of the population does not use evolutionary operators
unlike in genetic algorithms [1].
The orientation and the speed of the particles are inﬂuenced by all other parti-
cles. An individual particle moves towards the particle with the best local- or global
solution and is inﬂuenced by its personal best position.
230
A. Tormási and L.T. Kóczy

3
Optimization Task and Experiment Properties
3.1 Optimization Task
Basic Concept, Properties and Limitations.
Four key features were deﬁned at the beginning of the development of the
recognition engine:
1. Accuracy: it has to reach an acceptable recognition rate, i.e. at least as good as,
or better than other accepted methods.
2. Efﬁciency: the methods must ﬁt the user’s requirements in response time and in
resources of hardware (complex geometrical transformations and other mathe-
matical functions should be avoided).
Fig. 2 Flowchart of the
imperialist competitive
algorithm
Meta-Heuristic Optimization of a Fuzzy Character Recognizer
231

3. Flexibility of the alphabet: the alphabet must be easily modiﬁable to support
various alphabets and context-sensitive recognition.
4. Learning: it should be able to learn user-speciﬁc writing style.
The general characteristics and properties of fuzzy systems enable them to satisfy
all the features considered above. This fact led us to use fuzzy inference method for
the recognition method. Fuzzy-Based Character Recognizer (FUBAR) is a family of
algorithms of various single-stroke and multi-stroke hand printed (handwritten,
non-cursive, capital letters) character recognition engines. The designed system is a
personalized online recognizer, which means it processes digital ink and deploys
user-speciﬁc information. The basic concept of the designed method is shown in
Fig. 3.
Input Conditioning and Handling.
The input signal of the algorithm consists of two-dimensional (x, y) coordinates in
chronological order, representing the pen-movement (stroke). In unistroke (or
single-stroke) recognizers, letters are represented by a single stroke; while in multi-
stroke systems each symbol is represented by any numbers of strokes (sub-strokes).
The FUBAR algorithm merges the multi-strokes into one unistroke and handles it
accordingly.
Usually, the received signal is non-continuous as a result of the bottlenecks of
the hardware, which causes information loss during recording the pen-movement;
this information-loss causes difﬁculties in the processing, because the positions of
the missing coordinates thus become non-deterministic. The received signal must
be normalized for further processing and better recognition rate. In the FUBAR
Fig. 3 Concept of the FUBAR engine
232
A. Tormási and L.T. Kóczy

algorithm family the points of the received signal are re-sampled; the points
between a given (Euclidean) distance from the reference point are ﬁltered out. The
re-sampling of the strokes also has an anti-aliasing property.
Feature Extraction.
FUBAR uses two kinds of stroke features for the recognition: (1) the width/
height ratio of the stroke and (2) the average number of points in the rows and
columns of the grid drawn around the stroke. The ﬁrst member of the FUBAR
family used a crisp grid (with sharp borders) for the feature extraction, but the
system reached a low average recognition rate as some of the users started to write
faster and use italic writing style The sampled points of the strokes of oblique and
normal characters could be located in completely different rows and columns of the
grid, which caused huge overlap between the features of various letters. Other
methods are rotating the input characters to avoid the negative effects of the italic
writing style, but those methods use complex mathematical transformations, which
dramatically increase the computational complexity of the method. To resolve the
problems caused by the italic writing style, fuzzy grids [18, 19] were proposed. In
fuzzy grids the rows and columns of the grid are deﬁned by fuzzy sets. It can be
also considered as a transformation of the stroke into a fuzzy space. The points in a
fuzzy grid may belong to two different columns or rows at the same time with
various membership values as seen in Fig. 4.
Inference.
In the designed recognition engine a fuzzy rule-based inference method [7, 8] is
used. Each symbol in the alphabet is represented by a single rule. The input
parameters of the rules are the features described above; the output parameter of the
rules is the degree of matching between the features of the input stroke and the
stored rules as seen in Fig. 5. FUBAR returns the character associated to the best
matching rule after the rule evaluation phase.
Fig. 4 Concept of fuzzy grids
Meta-Heuristic Optimization of a Fuzzy Character Recognizer
233

3.2 Extensions of Meta-Heuristic Methods
Each algorithm is extended to use multiple populations without the option of
migration. This is an important modiﬁcation in order to handle the various char-
acters independently and to avoid the overlap between the fuzzy sets representing
the features of various characters. During the evaluation of candidate solutions the
best rules are used from each population.
The initial populations are randomly generated in the original algorithms; in
order to switch between the meta-heuristics, the algorithms must support to use
predetermined populations.
4
Results
4.1 Rule Optimization from Scratch
Bacterial Evolutionary Algorithm.
In this experiment various population sizes, number of clones and number of
gene transfers (infections) were used. Tests were performed with the combination of
each parameter with values between 10 and 30 (increased by 5 in each different
test). The fourth parameter was affecting the bacterial mutation operator. If the
mutation parameter was set to “tolerant”, it accepted a new allele when the system
had the same or better result as with the original allele value; otherwise (“strict”) it
accepted only the mutation from clones with better results. The maximum number
of generations was set to 100.
The results reﬂected that the algorithm reached the same results (or with
insigniﬁcant difference) for various parameter values, except for the mutation
parameter. If it set to “strict”, the algorithm was stuck at the same point during the
process.
The best result (0.44 error rate) was achieved when the size of the population,
the number of clones and the number of infections both were set to 10 and the
mutation type was set to “tolerant”. The average error rates/generations are shown
in Fig. 6 for the training dataset and in Fig. 7 for the validation set.
Fig. 5 Fuzzy rule describing a character
234
A. Tormási and L.T. Kóczy

Big Bang–Big Crunch Algorithm.
The effect of the number of generations and the size of the population were
investigated in this algorithm. The number of generations was changed between 50
and 100, while the size of the population was selected between 10 and 50 (step size
was 10 in both parameters). The results showed that the change of these parameters
does not signiﬁcantly affect the results; the distribution of the error rate was the
same for each scenario, however there was a slightly greater chance to ﬁnd a better
quasy-optimum after more generations. The best result (0.11 error rate) was
achieved in 70 generations and population size 10. The best, the worst and the
average convergence of the error rates/generations for the training dataset is in
Fig. 8 and for the validation data is in Fig. 9.
Fig. 6 Error rates of BEA from ﬂat sets on the training data
Fig. 7 Error rates of BEA from ﬂat sets on the validation data
Meta-Heuristic Optimization of a Fuzzy Character Recognizer
235

The average error rates are more close to the worst case scenario compared to the
best results as it can be seen in the ﬁgures above.
Imperialist Competitive Algorithm.
At the testing of the ICA algorithm the number of countries was changed
between 10 and 30 and the number of generations was between 50 and 100 (the step
was 10 in both cases). The results indicated that the number of generations over 50
did not have any effect on the results, while the greater number of the countries did
result in lower error rates. The best result (0.04 error rate) was achieved in 50
generations and the number of countries was 30. Worst, average and best error
rates/generations for the validation set are shown in Fig. 10 (with 10 countries), in
Fig. 11 (with 20 countries) and in Fig. 12 (with 30 countries).
Fig. 8 Error rates of BBBC from ﬂat sets on the training data
Fig. 9 Error rates of BBBC from ﬂat sets on the validation data
236
A. Tormási and L.T. Kóczy

Fig. 10 Error rates of ICA from ﬂat sets on the validation data with 10 countries
Fig. 11 Error rates of ICA from ﬂat sets on the validation data with 20 countries
Fig. 12 Error rates of ICA from ﬂat sets on the validation data with 30 countries
Meta-Heuristic Optimization of a Fuzzy Character Recognizer
237

Particle Swarm Algorithm.
The number of generations was between 50 and 100, while the size of the swarm
was between 10 and 30 during the test of the Particle Swarm method. The best
result (0.32 error rate) was achieved with swarm size of 20 and in 60 generations.
The results are indicating that, the larger swarm size slightly increases the proba-
bility of a better result. Worst, best and average error rates/generations for vali-
dation set are shown in the ﬁgures below (Figs. 13, 14 and 15).
Fig. 13 Error rates of PSA from ﬂat sets on the validation data with 10 particles
Fig. 14 Error rates of PSA from ﬂat sets on the validation data with 20 particles
238
A. Tormási and L.T. Kóczy

4.2 Rule Optimization with Multi Meta-Heuristics
In this experiment a previous test population was used as a starting point for each
algorithm; this initial population was selected from a BBBC algorithm experiment.
At generation 40 and at error rate of 0.12 the population was backed up and later
loaded into each algorithm several times.
The lowest error rate in the worst cases was achieved by the ICA (0.09), while
BEA, PSA and BBBC reached the error rate of 0.12, 0.13 and 0.13 respectively
(Fig. 16).
The lowest error rate in the best scenarios was achieved by the PSA (0.048), but
the BBBC and ICA algorithms were performing only slightly worse (0.05 error
rate), while the BEA algorithm could not achieve better error rate than 0.078
(Fig. 17).
Fig. 15 Error rates of PSA from ﬂat sets on the validation data with 30 particles
Fig. 16 Worst error rates of BEA, BBBC, ICA, PSA
Meta-Heuristic Optimization of a Fuzzy Character Recognizer
239

The best performing algorithm in average was the ICA with the error rate of
0.0778, the second one was the PSA with a slightly higher error rate 0.0779, while
the BBBC reached 0.1 and the BEA produced the error rate of 0.114 (Fig. 18)
The Imperialist Competitive Algorithm and the Particle Swarm Algorithm per-
formed best during the experiment in all scenarios, while the Big Bang–Big Crunch
algorithm had the second worse results in average. The Bacterial Evolutionary
Algorithm produced the worst results in all three scenarios, but all its results were
close to each other.
5
Conclusions and Discussions
The aim of this research was to investigate the behavior of meta-heuristic algo-
rithms applied on a fuzzy rule-based classiﬁer (multi-stroke character recognizer)
system.
Fig. 17 Best error rates of BEA, BBBC, ICA, PSA
Fig. 18 Average error rates of BEA, BBBC, ICA, PSA
240
A. Tormási and L.T. Kóczy

The experiment suggests that the algorithms are not enough sensitive for their
main parameters (population size, number of generations) to reach higher accuracy
without a signiﬁcant increase in the computational time. A more detailed test of
other algorithm-speciﬁc parameters should be executed in order to ﬁnd a low-cost
way of enhancing their effectiveness.
The results are showing that the ICA algorithm could reach the lowest error rate
(0.04), when the rule-base had to be created from scratch; the second best results
(0.11) were achieved by the BBBC algorithm. The worst result was produced by the
BEA with the error rate of 0.44, while the PSA could decrease the error rate to 0.32.
It is also important to consider that the average generations evaluated in one
second for the BEA, BBBC, ICA and PSA were 0.0667, 4, 0.667 and 1 respectively
(in the experiment environment). The ICA is 10 times faster than the BEA and it
can reach much lower error rates. The PSA is 33 % faster and the BBBC is about 6
times faster than the ICA, while their optimization performances are very close.
This means that it might be beneﬁcial to combine these methods if we could switch
between the algorithms according to the dynamics of the population, the properties
of the problem and the algorithms.
The BBBC and PSA algorithms could be a good choice to start the optimization
process, because it is able to reach average results with a very low cost, but due to
its disadvantages (convergence to local optimum) it does not worth it to use it
during the optimization. In some scenarios they could perform same or slightly
better as the ICA, but it is more like a matter of random situations. BBBC and PSA
should be modiﬁed to avoid local optimum solutions (i.e. “anomalies” in the BBBC
algorithm which could move the center of the universe from these points or restore
some parts of the universe) without signiﬁcantly increasing their processing time.
The overall performance of ICA algorithm was the best, but the evaluation time
of generations is signiﬁcantly higher compared to BBBC and PSA. It might worth
to use ICA instead of the other algorithms, but in some cases it does not perform
that much better than BBBC and PSA, which would make it reasonable to use it
(considering its processing time).
The general properties of the BEA algorithm made it “stable” and it has its own
advantages despite its low results and high computational time. The algorithm
should be improved by reducing its time consuming computations.
The next aim is to research a simple and automatic procedure to test and
investigate the characteristics and other features of the population, where the pre-
sented algorithms are performing best (in terms of results and resource require-
ments). Using this knowledge a more extended alternative method for [BK] might
be developed, which would be able to switch between more than two meta-heuristic
optimization algorithms. The planned method would change to a new optimization
algorithm if the properties of the “environment” and the population are indicating
that another algorithm could be more successful (have better convergence or
solution) in order to save resource.
Acknowledgments This paper is partially supported by the TÁMOP-4.2.2.A-11/1/KONV-2012-
0012 and Hungarian Scientiﬁc Research Fund (OTKA) grants K105529, K108405.
Meta-Heuristic Optimization of a Fuzzy Character Recognizer
241

References
1. Holland, J.H.: Adaption in Natural and Artiﬁcial Systems. The MIT Press, Cambridge (1992)
2. Nawa, N.E., Furuhashi, T.: Fuzzy system parameters discovery by bacterial evolutionary
algorithm. IEEE Trans. Fuzzy Syst. 7(5), 608–616 (1999)
3. Erol, K.O., Eksin, I.: A new optimization method: big bang-big crunch. Adv. Eng. Softw. 37
(2), 106–111 (2006)
4. Atashpaz-Gargari, E., Lucas, C.: Imperialist competitive algorithm: an algorithm for
optimization inspired by imperialistic competition. In: Proceedings of the 2007 IEEE
Congress on Evolutionary Computation vol. 7, pp. 4661–4666. Singapore (2007)
5. Kowalski, P.A., Lukasik, S.: Tuning neural networks with krill herd algorithm. In:
Proceedings of the 6th Győr Symposium and 3rd Hungarian-Polish and 1st Hungarian-
Romanian Joint Conference on Computational Intelligence, ConfCI 2014, pp. 119–128. Győr
(2014)
6. Zadeh, L.A.: Fuzzy sets. Inf. Control 8, 338–353 (1965)
7. Mamdani, E.H., Assilian, S.: An experiment in linguistic synthesis with a fuzzy logic
controller. Int. J. Man Mach. Stud. 7, 1–13 (1975)
8. Takagi, T., Sugeno, M.: Fuzzy identiﬁcation of systems and its applications to modeling and
control. In: IEEE Transactions on Systems, Man and Cybernetics, vol. SMC-15, pp. 116–132
(1985)
9. Tormási, A., Botzheim, J.: Single-stroke character recognition with fuzzy method. In: Balas V.
E., et al. (eds.) New Concepts and Applications in Soft Computing SCI, vol. 417, pp. 27–46
(2012)
10. Eberhart, R., Kennedy, J.: A new optimizer using particle swarm theory. In: Proceedings of the
IEEE International Conference on Neural Networks IV, IEEE Press, pp. 1942–1948.
Piscataway (1995)
11. Dányádi, Zs., Balázs, K., Kóczy, L.T.: A comparative study of various evolutionary
algorithms and their combinations for optimizing fuzzy rule-based inference systems.
Scientiﬁc Bulletin of Politehnica University of Timisoara, Romania, 55(69), 247–254 (2010)
12. Balázs, K., Horváth, Z., Kóczy, L.T.: Hybrid bacterial iterated greedy heuristics for the
permutation ﬂow shop problem. In: In World Congress on Computational Intelligence, WCCI
2012, pp. 1–8. Brisbane, Australia (2012)
13. Balázs, K., Kóczy, L.T.: A remark on adaptive scheduling of optimization algorithms. In:
International Conference on Information Processing and Management of Uncertainty in
Knowledge-Based Systems, IPMU 2010, pp. 719–728. Dortmund, Germany (2010)
14. Ishibuchi, H., Nakashima, T.: Effect of rule weights in fuzzy rule-based classiﬁcation systems.
IEEE Trans. Fuzzy Syst. 9(4), 506–515 (2001)
15. van den Berg, J., Kaymak, U., van den Bergh, W.M.: Fuzzy classiﬁcation using probability-
based rule weighting. In: Proceedings of the 11th IEEE International Conference on Fuzzy
Systems, Hawaii (2002)
16. Ishibuchi, H., Yamamoto, T.: Rule weight speciﬁcation in fuzzy rule-based classiﬁcation
systems. IEEE Trans. Fuzzy Syst. 13(4), 428–435 (2005)
17. Tormási, A., Kóczy, L.T.: Fuzzy-based multi-stroke character recognizer. In: Preprints of the
Federated Conference on Computer Science and Information Systems, pp. 675–678. Krakow
(2013)
18. Tormási, A., Kóczy, L.T.: Comparing the efﬁciency of a fuzzy single-stroke character
recognizer with various parameter values. In: Greco S., et al. (eds.) Proceedings of the IPMU
2012, Part I. CCIS, vol. 297, pp. 260–269 (2012)
19. Tormasi, A., Kóczy, L.T.: Fuzzy single-stroke character recognizer with various rectangle
fuzzy grids. In: Issues and challenges of intelligent systems and computational intelligence,
Springer, pp. 145–159 (2014)
242
A. Tormási and L.T. Kóczy

Authors Biography
Alex Tormási was born in Győr, Hungary in 1985; He received
the B.S. and M.S. degrees in computer science engineering from
the Széchenyi István University (SZE, Győr) in 2008 and 2010;
later in 2010 started his Ph.D. studies at the Multidisciplinary
Doctoral School of Engineering Sciences at SZE.
He was working on several software development projects as
an entrepreneur between 2004 and 2013. However he has been a
visiting lecturer from 2009 until 2011 at the Department of
Mathematics and Computational Sciences and the Department of
Automation at SZE. He started to work as a technical assistant at
the Department of Information Technology at SZE from 2012,
then as an assistant lecturer since 2013.
Mr. Tormási was a recipient of the Scholarship of the
Hungarian Republic three times in academic years 2007/2008,
2008/2009 and 2009/2010 and was awarded with “Young
Scientist Award” in 2013 by the International Fuzzy Systems Association (IFSA). He was an
organizing member at the Fifth Győr Symposium and First Hungarian-Polish Joint Conference on
Computational Intelligence in 2012 and at the Spring Wind 2014 (Engineering Sciences Section).
He was member of the general chair at Széchenyi Doctoral Students’ Conference in 2014 and
member of the international program committee at the Sixth Győr Symposium and Third
Hungarian-Polish and First Hungarian-Romanian Joint Conference on Computational Intelligence
(ConfCI 2014). From 2013 until 2014 he was the head at the Section of the Engineering Sciences
(formerly Engineering, Architectural and Earth Sciences) in the Association of Hungarian PhD and
DLA Students (DOSz).
László T. Kóczy was born in Budapest, Hungary in 1952; He
received the M.Sc., M.Phil. and Ph.D. degrees from the
Technical University of Budapest (BME) in 1975, 1976 and
1977, respectively; and the (postdoctoral) D.Sc. degree from the
Hungarian Academy of Science, all in Electrical/Control Engi-
neering.
He spent most of his career at BME until 2001 and from 2002
at Szechenyi Istvan University (Gyor, SZE). However, he has
been a visiting professor at various universities abroad, namely
in Australia (ANU and others), in Japan (TIT, where he was one
of the LIFE Endowed Fuzzy Theory Chair Professors and at the
same time an advisor to the Laboratory for International Fuzzy
Engineering Research in Yokohama), in Korea (POSTECH),
Austria, and Italy. His focus of research interest is fuzzy systems
and Computational Intelligence topics (evolutionary algorithms,
neural networks), as well as applications. He has published over 450 refereed papers and several
text books on the subject. He did introduce the concept of rule interpolation in sparse fuzzy
models, and hierarchical interpolative fuzzy systems, fuzzy Hough transform, and also fuzzy
signatures and fuzzy situational maps among others. His research interests include applications of
CI for telecommunication, transportation and logistics, vehicles and mobile robots, control,
information retrieval, etc.
Prof. Koczy was an Associate Editor of IEEE TFS and is now of Fuzzy Sets and Systems,
International Journal of Fuzzy Systems, Journal of Advanced Computational Intelligence,
International Journal of of fuzzy Systems, etc. He was the General Chair of FUZZ-IEEE 2004 in
Budapest, Special Session Chair of WCCI 2012 and Panel Chair of FUZZ-IEEE 2013, will be the
Meta-Heuristic Optimization of a Fuzzy Character Recognizer
243

Poster Chair of WCCI 2016; was a chair, co-chair, PC member, etc. at many other scientiﬁc
events. He served in the International Fuzzy Systems Association as President, and he was
Administrative Committee member of IEEE Computational Intelligence Society for two cycles and
he was another two cycles a member of the AdCom of the IEEE Systems Council. At SZE he is
president of the University Doctoral Council and of the University Research Council, he has been
appointed member of the Hungarian Accreditation Committee for Higher Education by the Prime
Minister, where he chairs the Engineering Sub-Committee, he is a member of the National
Doctoral Council (Hungary), of the National Fellowship Committee and of the National Advisory
Board for Industrial Safety.
244
A. Tormási and L.T. Kóczy

Additive Fuzzy Systems as Generalized
Probability Mixture Models
Bart Kosko
Abstract Additive fuzzy systems generalize the popular mixture-density models of
machine learning. Additive fuzzy systems map inputs to outputs by summing ﬁred
then-parts sets and then taking the centroid of the sum. This additive structure
produces a simple convex structure: Outputs are convex combinations of the cen-
troids of the ﬁred then-part sets. Additive systems are uniform function approxi-
mators and admit simple learning laws that grow and tune rules from sample data.
They also behave as conditional expectations with conditional variances and other
higher moment that describe their uncertainty. But they suffer from exponential rule
explosion in high dimensions. Extending ﬁnite-rule additive systems to fuzzy
systems with continuum-many rules overcomes the problem of rule explosion if a
higher-level mixture structure acts as a system of tunable meta-rules. Monte Carlo
sampling can then compute fuzzy-system outputs.
Keywords Additive fuzzy system ⋅Mixture density models ⋅Compounding ⋅
Function approximation ⋅Fuzzy approximation theorem ⋅Learning laws ⋅
Conditional expectations⋅Convex sums⋅E-M algorithm⋅Monte carlo simulation⋅
Importance sampling ⋅Continuum-many fuzzy rules
1
Centroidal Fuzzy Systems as Statistical Estimators
This chapter reviews and extends the main mathematical properties of additive
fuzzy systems [1–9]. Additive fuzzy systems exploit the convex-sum structure that
results from additively combining ﬁred if-then rules. They generalize mixture-
density models from machine learning and pattern recognition because such mix-
tures are convex sums that do not depend on an input value. Additive fuzzy systems
B. Kosko (✉)
Department of Electrical Engineering, Signal and Image Processing Institute,
University of Southern California, Los Angeles, CA, USA
e-mail: kosko@usc.edu
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_14
245

admit simple gradient-descent learnings laws and are uniform function approxi-
mators on compact sets. But they suffer from exponential rule explosion in high
dimensions. This curse of dimensionality limits modeling with additive fuzzy
systems and limits tuning them with sample data. Extending ﬁnite-rule additive
fuzzy systems to continuum-many fuzzy rules overcomes the problem of rule
explosion. It allows the user to deﬁne higher-level fuzzy meta-rules with a mixture
structure and then use modern statistical techniques to tune such continuum-rule
additive systems. This analysis turns on a probabilistic interpretation of fuzzy
systems. This ﬁrst section shows that such a probabilistic interpretation applies to
all centroidal fuzzy systems even if they do not additively combine ﬁred rules.
A fuzzy system is a mapping F: ℝn →ℝ. It uses a set of fuzzy if-then rules to
convert a vector input x to an output FðxÞ. There is no loss of generality if the fuzzy
system is scalar and thus if it maps to the real line ℝ. All results still hold with
appropriate vector notation for vector-valued fuzzy systems F: ℝn →ℝp.
We ﬁrst show that any centroidal fuzzy system deﬁnes a conditional expectation
and hence is a probabilistic or statistical system. The fuzzy system need not be
additive. A non-additive system could combine rules through a maximum operation
or through any other aggregation operation [10–13]. Early fuzzy systems often
combined outputs with a maximum or supremum operation.
A centroidal output sufﬁces to produce a conditional expectation. So the con-
ditional-expectation result does not require an independent probabilistic assump-
tion. It follows instead from just the nonnegativity and the integrability of the then-
part fuzzy sets that all fuzzy if-then rules use. We ﬁrst state some notation for fuzzy
systems and then state and prove the conditional-expectation result as Theorem 1.
A centroidal fuzzy system F: ℝn →ℝis a fuzzy system that computes the output
FðxÞ by taking the centroid of a ﬁnite number m of combined “ﬁred” then-part sets:
FðxÞ = CentroidðBðxÞÞ. Later we will drop the ﬁnite assumption. The term BðxÞ
stands for the combined ﬁred then-parts. The argument x implies that the vector
input x has ﬁred the m rules. The ﬁred combination BðxÞ formally is any non-
negative function b: ℝ×ℝn →ℝ+ that has a ﬁnite integral. The jth rule RAj →Bj has
the linguistic form “If X = Aj then Y = Bj” for if-part fuzzy set Aj⊂ℝn and scalar
then-part fuzzy set Bj⊂ℝ. The unﬁred then-part set Bj
has set function
bj: ℝ→½0, 1. But its ﬁred version BjðxÞ has a two-place argument and thus cor-
responds to the set function bjðx, yÞ: ℝn×ℝ→½0, 1 for vector input x∈ℝn. But we
still write the set function in single-argument notation bjðxÞ for simplicity. The rule
RAj →Bj is a fuzzy subset of the input-output product space ℝn×ℝbecause all input-
output pairs ðx, yÞ satisfy the rule to some degree. So the rule corresponds to a two-
valued set function rAj →Bj: ℝn×ℝ→½0, 1.
The n-dimensional fuzzy set Aj corresponds to a joint set membership or mul-
tivalued indicator function aj: ℝn →½0, 1. Users often assume in practice that the
joint membership function factors into a product of scalar membership functions:
ajðxÞ = ∏m
k = 1ak
j ðxkÞ where each factor set Ak
j ⊂ℝhas set function ak
j : ℝ→½0, 1 for
row vector x = ðx1, . . . , xnÞ. Earlier fuzzy systems sometimes formed the joint set
function aj by taking pairwise minima ajðxÞ = minða1
j ðx1Þ, . . . , ak
j ðxnÞÞ or some
246
B. Kosko

other pairwise triangular-norm operation. The minimum function ignores the
information in all scalar inputs except the smallest one when the inputs differ. The
standard additive fuzzy systems below always work with the simpler product
factorization. The product function preserves the relative values of the scalar inputs.
The then-part set function can be generalized set function. The then-part fuzzy sets
Bj need only have positive and integral set functions bj: ℝ→ℝ+ because of the
normalization involved in taking the centroid. They do not need to map to the unit
interval.
Now suppose the vector input x = ðx1, . . . , xnÞ activates the scalar fuzzy system
F: ℝn →ℝto produce the combined rule ﬁrings BðxÞ. Then Theorem 1 states that
taking the centroid results in a conditional expectation for any fuzzy system that
combines rules to produce BðxÞ.
Theorem
1 Every
centroidal
fuzzy
system
is
a
conditional
expectation:
FðxÞ = E½Y j X = x.
Proof The theorem follows from the deﬁnition of the centroid and from the non-
negativity and integrability of the then-part sets Bj. We also assume that the input x
leads to nontrivial rule ﬁrings. So it leads to a nonzero combination of ﬁred rules
BðxÞ: BðxÞ>0. Then
□
FðxÞ = CentroidðBðxÞÞ
ð1Þ
=
R ∞
−∞y bðxÞ dy
R ∞
−∞bðxÞ dy
ð2Þ
=
R ∞
−∞y bðx, yÞ dy
R ∞
−∞bðx, yÞ dy
ð3Þ
=
Z ∞
−∞
y
bðx, yÞ
R ∞
−∞bðx, yÞ dy
"
#
dy
ð4Þ
=
Z ∞
−∞
y pðy j xÞ dy
ð5Þ
= E½Y j X = x.
ð6Þ
The result follows because pðy j xÞ =
bðx, yÞ
R ∞
−∞bðx, yÞ dy is nonnegative and because
R ∞
−∞pðy j xÞ dy = 1 holds from the nonnegativity and integrability of the b function
if bðx, yÞ>0. So pðy j xÞ is a proper conditional probability density function. Then
E½Y j X = x is a realization of the condition-expectation random variable E½Y j X.
Q.E.D.
Additive Fuzzy Systems as Generalized Probability Mixture Models
247

Theorem 1 yields a system conditional variance V½YjX = x and higher-order
moments so long as the appropriate integrals exist:
V½YjX = x = E½Y2jX = x −E2½YjX = x.
ð7Þ
The conditional density pðy j xÞ gives the second moment as
E½Y2 j X = x =
Z ∞
−∞
y2 pðy j xÞ dy.
ð8Þ
The next section gives closed forms for these moments and for other higher-
order moments when the fuzzy systems are not just centroidal but additive.
A word is in order here about just how a given input x0 ﬁres a rule RAj →Bj. Fuzzy
models assume that the input x0 belongs to the if-part set Aj to degree ajðx0Þ:
ajðx0Þ = Degreeðx0∈AjÞ. Then this membership or “ﬁt” (fuzzy unit) value ajðx0Þ
changes the corresponding then-part Bj to produce the ﬁred then-part set Bjðx0Þ with
set function value bjðx0, yÞ. Viewing the if-part set Aj as a probability density
function would give the null result ajðx0Þ = 0 for all x0 since aj is continuous. We
instead view the input x0 as a delta pulse δðx −x0Þ centered at x0. Then convolution
gives the proper ﬁt value ajðx0Þ for the ﬁred if-part set [1, 5]:
Z ∞
−∞
δðx −x0Þ ajðxÞ dx = ajðx0Þ.
ð9Þ
This convolution result follows from the “sifting” property of the delta function.
It also extends the point fuzzy system to a set fuzzy system that takes an arbitrary
continuous fuzzy set A as input if we deﬁne the corresponding activation in terms of
a more general inner product:
Z ∞
−∞
aðxÞ ajðxÞ dx = ajðAÞ.
ð10Þ
Then the proof of Theorem 1 still gives the system output as FðAÞ = E½Y j X = A.
All the standard-additive results below admit such a set-input extension.
2
Additive Fuzzy Systems as Convex Combinations
of Centroids
Additive fuzzy systems add ﬁred then-part sets to compute the combined set BðxÞ.
This leads to the central fact of additive systems: Their outputs equal the convex
combination of the centroids of the ﬁred then-part sets.
248
B. Kosko

This convex structure carries over into many properties of centroidal additive
systems. It starts with the basic “mixture” fact in Proposition 1 below that the global
output conditional probability density pðy j xÞ is itself a convex combination of the
m then-part conditional probabilities. This convex structure leads in particular to
simple and practical forms for the conditional expectation and conditional variance.
Theorem 3 below shows that the global conditional mean (6) equals a convex sum
of local conditional means. These conditional means are local or rule-speciﬁc in the
sense that their conditional probability density arises from the shape of their cor-
responding ﬁred then-part set.
Theorem 3 also shows that the conditional variance decomposes into two convex
sums. The ﬁrst sum averages the uncertainty that arises from the shapes of the then-
part sets. So here set shape matters. This result differs from simple function
approximation where only the shape of the if-part sets controls the approximation
for default then-part sets. This then-part shape dependence contrasts with many
fuzzy applications that simply replace the then-part sets with spikes centered at
what would otherwise be a then-part set’s centroid. Such a then-part spike simpliﬁes
some computations but it implicitly assumes total certainly about the then-part of
the rule. The second sum averages the uncertainty that arises from interpolating
between rule centroids to produce the system output. This term measures the
inherent uncertainty in the fuzzy system that results from such interpolation. The
other higher-order conditional moments in Theorem 3 involve similar convex sums
and interpolations.
We ﬁrst prove that all additive centroidal fuzzy systems are convex sums of ﬁred
then-part centroids. An additive fuzzy system combines the m ﬁred then part sets by
adding them:
BðxÞ = ∑m
j = 1wj BjðxÞ
ð11Þ
for positive rule weights wj>0. The rule weights need not sum to unity. And they
can depend on the input x. They drop out of the centroidal output FðxÞ if they are all
equal: w1 = ⋯= wn. Then the combined set BðxÞ has a generalized set function
bðy j xÞ for each input x as y ranges over the range space ℝ:
bðy j xÞ = ∑m
j = 1wj bjðy j xÞ.
ð12Þ
We here use the conditional notation bjðy j xÞ: ℝn×ℝ→½0, 1 for the set func-
tion of the ﬁred then-part set BjðxÞ. So the inputs x parametrize the ﬁred then-part
sets.
Each ﬁred then-part set BjðxÞ has an area or volume VjðxÞ:
VjðxÞ =
Z ∞
−∞
bjðy j xÞ dy.
ð13Þ
We again assume that all such integrals are ﬁnite and positive. This gives in turn
an input-dependent centroid cjðxÞ for ﬁred then-part set BjðxÞ:
Additive Fuzzy Systems as Generalized Probability Mixture Models
249

cjðxÞ =
R ∞
−∞y bjðy j xÞ dy
R ∞
−∞bjðy j xÞ dy =
1
VjðxÞ
Z ∞
−∞
bjðx, yÞ dy.
ð14Þ
Then Theorem 2 states that all additive centroidal fuzzy systems equal a convex
combination of ﬁred then-part centroids.
Theorem 2 Additive centroidal systems are convex combinations of ﬁred then-part
centroids:
FðxÞ = ∑m
j = 1pjðxÞ cjðxÞ.
ð15Þ
The convex coefﬁcients pjðxÞ have the ratio form
pjðxÞ =
wj VjðxÞ
∑m
k = 1wk VkðxÞ .
ð16Þ
Proof.
FðxÞ = CentroidðBðxÞÞ =
R ∞
−∞y bðy j xÞ dy
R ∞
−∞bðy j xÞ dy
ð17Þ
=
R ∞
−∞y ∑m
j = 1wj bjðy j xÞ dy
R ∞
−∞∑m
k = 1bjðy j xÞ dy
ð18Þ
=
∑m
j = 1wj
R ∞
−∞y bjðy j xÞ dy
∑m
k = 1wk
R ∞
−∞bjðy j xÞ dy
ð19Þ
=
∑m
j = 1wj VjðxÞ
R ∞
−∞y bjðy j xÞ dy
VjðxÞ


∑m
k = 1wk VkðxÞ
ð20Þ
=
∑m
j = 1wj VjðxÞ cjðxÞ
∑m
k = 1wk VkðxÞ
ð21Þ
= ∑m
j = 1
wj VjðxÞ
∑m
k = 1wk VkðxÞ


cjðxÞ
ð22Þ
= ∑m
j = 1pjðxÞ cjðxÞ.
ð23Þ
The coefﬁcients pjðxÞ are convex because they are nonnegative and sum to unity
by (16). Q.E.D.
□
The convex-sum structure of Theorem 2 underlies much of the power of additive
fuzzy systems. An important example is universal function approximation.
250
B. Kosko

Centroidal additive fuzzy systems F can uniformly approximate any continuous
function f : K⊂ℝn →ℝon a compact set K [2, 5]: jFðxÞ −f ðxÞj <ε for all x given
any initial choice of the error level ε>0. The heart of the proof in the scalar case is
that convexity traps the output FðxÞ between the smallest and largest centroids:
cminðxÞ≤FðxÞ≤cmaxðxÞ. A similar result holds component-wise in higher dimensions
because the vector output FðxÞ lies in a centroidal hyper-rectangle. So in principle
we can always ﬁnd some set of fuzzy rules that makes an additive fuzzy system as
close as we wish to any continuous function. The uniform approximation does not
require either that the if-part sets be fuzzy or that the then-part sets be fuzzy. Binary
rectangles still produce uniform function approximators even if the resulting fuzzy
systems are not as smooth as fuzzy systems with sets based on Gaussian or Cauchy
or other smooth functions. So a fuzzy system need not be fuzzy at all. The raw
approximation power comes not from working with fuzzy matters of degree. It
comes instead from the additive system’s use of parallel if-then rules and its
convexity.
A second example of this convex structure occurs in rule adaptation or learning.
The ratio structure in (21) allows a direct application of the quotient rule of the
differential calculus. Convexity leads to a simple form for the learning gradient term
∂F
∂wj for the rule weight wj [5, 8]: ∂F
∂wj = pjðxÞ
wj
cjðxÞ −FðxÞ


. This leads to the squared-
error-based learning law wjðt + 1Þ = wjðtÞ + μtεðtÞ pjðxÞ
wjðtÞ cj −FðxÞ


where the super-
vised error term εðtÞ = dðtÞ −FðxðtÞÞ requires knowledge of the system’s desired
outcome dðtÞ at a given time instant. The learning coefﬁcients μt usually decrease
linearly in accord with convergence principles from stochastic approximation. The
same form of learning law holds for the then-part volume VjðxÞ and the if-part set
function aj. But there are two more partial derivatives to unpack in the if-part set-
function case because the set function factors and because each factor depends on
shape parameters such as the location and scale of the scalar factor set. The centroid
has an even simpler learning term: ∂F
∂cj = pjðxÞ because of the convex structure. Then
gradient learning can use sample data to tune these system parameters. The simplest
case minimizes the squared error
1
2 f ðxÞ −FðxÞ
ð
Þ2 for some known or unknown
sample function f. Then the fuzzy system will quickly approximate the sampled
function given enough representative samples and enough training iterations. The
rules quickly move to cover the extrema or turning points of f. This reﬂects the
theorem that optimal additive rules cover extrema [3]. Unsupervised clustering
algorithms can help ﬁnd these optimal rules in practice [1]. Such data-driven
clusters area also a good way to initialize the fuzzy system. But the number of rules
involved tends to grow exponentially with the input dimension because the fuzzy
rules deﬁne a graph cover in the input-output product space. This curse of
dimensionality tightly constrains the above learning laws when the fuzzy system
has as few as three input dimensions [8].
The next theorem shows how the convex structure in Theorem 2 passes into the
structure of the additive system’s conditional expectation and conditional variance
and indeed into all its higher conditional moments. The key insight is that additivity
Additive Fuzzy Systems as Generalized Probability Mixture Models
251

induces convexity in the global conditional probability density function pðy j xÞ.
This density decomposes into a convex sum of the m local rule-speciﬁc conditional
probability density functions pBjðy j xÞ:
pBjðy j xÞ =
bjðy j xÞ
R ∞
−∞bjðy j xÞ dy .
ð24Þ
The normalizing denominator is just the input-dependent area or volume VjðxÞ
from (13). This “mixture” result is important enough to state as a separate
proposition.
Proposition 1 pðy j xÞ = ∑m
j = 1pjðxÞ pBjðy j xÞ for pjðxÞ in (16).
Proof.
pðy j xÞ =
bðy j xÞ
R ∞
−∞bðy j xÞ dy
ð25Þ
=
∑m
j = 1wj bjðy j xÞ
R ∞
−∞∑m
k = 1wk bjðy j xÞ dy
ð26Þ
=
∑m
j = 1wj VjðxÞ
bjðy j xÞ
VjðxÞ
h
i
∑m
k = 1wk VkðxÞ
ð27Þ
= ∑m
j = 1
wj VjðxÞ
∑m
k = 1wk VkðxÞ


pBjðy j xÞ from 24
ð
Þ
ð28Þ
= ∑m
j = 1pjðxÞ pBjðy j xÞ from 16
ð
Þ. Q. E. D.
ð29Þ
We can now state and prove the key theorem on the conditional moments of all
additive centroidal systems.
□
Theorem 3 All higher-order moments of additive centroidal fuzzy systems are
convex sums:
ðaÞ
E½Y j X = x = ∑m
j = 1pjðxÞ cjðxÞ = FðxÞ
ð30Þ
ðbÞ
V½Y j X = x = ∑m
j = 1pjðxÞ σ2
BjðxÞ + ∑m
j = 1pjðxÞ cjðxÞ −FðxÞ

2
ð31Þ
ðcÞ
E½ðY −E½Y j X = xÞkjX = x = ∑m
j = 1pjðxÞ ∑k
l = 1
k
l


EBjðxÞ½ðY −cjðxÞÞlðcjðxÞ −FðxÞÞk −l for positive integer k.
ð32Þ
252
B. Kosko

Proof. Result (a) restates Theorems 1 and 2. Result (b) uses Proposition 1:
V½Y j X = x =
Z ∞
−∞
ðy −E½Y j X = xÞ2pðy j xÞ dy
ð33Þ
=
Z ∞
−∞
ðy −E½Y j X = xÞ2∑m
j = 1pjðxÞ pBjðy j xÞ dy
ð34Þ
= ∑m
j = 1pjðxÞ
Z ∞
−∞
ðy −cjðxÞÞ + ðcjðxÞ −FðxÞÞ

2pBjðy j xÞ dy
ð35Þ
from Theorem 1
= ∑m
j = 1pjðxÞ
Z ∞
−∞
ðy −cjðxÞÞ2pBjðy j xÞ dy
+ ∑m
j = 1pjðxÞ½cjðxÞ −FðxÞ2
Z ∞
−∞
pBjðy j xÞ dy
+ 2∑m
j = 1pjðxÞðcjðxÞ −FðxÞÞ
Z ∞
−∞
ðy −cjðxÞÞpBjðy j xÞ dy
ð36Þ
= ∑m
j = 1pjðxÞ σ2
BjðxÞ + ∑m
j = 1pjðxÞ cjðxÞ −FðxÞ

2
ð37Þ
because
the
cross
term
in
(36)
equals
zero
since
R ∞
−∞y pBjðy j xÞ dy
= cjðxÞ = EB, jðxÞ½Y and because σ2
BjðxÞ =
R ∞
−∞ðy −EBjðxÞ½YÞ2pBjðy j xÞ dy.
The conditional higher-order moments (c) follow similarly from the binomial
theorem ðp + qÞn = ∑m
k
n
k


pkqn −k for real numbers p and q and the combinatorial
coefﬁcients
n
k


=
n!
k! ðn −kÞ!. Then
E½ðY −E½Y j X = xÞkjX = x =
Z ∞
−∞
ðy −E½Y j X = xÞk∑m
j = 1pjðxÞ pBjðy j xÞ dy ð38Þ
= ∑m
j = 1pjðxÞ
Z ∞
−∞
ðy −cjðxÞÞ + ðcjðxÞ −FðxÞÞ

kpBjðy j xÞ dy
ð39Þ
= ∑m
j = 1pjðxÞ
Z ∞
−∞
∑k
l
k
l


ðy −cjðxÞÞkðcjðxÞ −FðxÞÞk −lpBjðy j xÞ dy
ð40Þ
= ∑m
j = 1pjðxÞ∑k
l = 0
k
l

 Z ∞
−∞
ðy −cjðxÞÞlpBjðy j xÞ dy


ðcjðxÞ −FðxÞÞk −l
ð41Þ
Additive Fuzzy Systems as Generalized Probability Mixture Models
253

= ∑m
j = 1pjðxÞ ∑k
l = 1
k
l


EBjðxÞ½ðY −cjðxÞÞlðcjðxÞ −FðxÞÞk −l. Q. E. D.
ð42Þ
The penalty term ðcjðxÞ −FðxÞÞk appears in the conditional variance and in all
the conditional higher-order moments. It measures the uncertainty due to rule
interpolation in the convex-sum output FðxÞ. The jth rule tries in effect to make the
global output FðxÞ look like its own centroidal output cjðxÞ. The jth rule tends to
have the most weight in the convex sum if the rule ﬁres dead-on and thus if
pjðxÞ ≈1 tends to hold. Then cjðxÞ ≈FðxÞ tends to hold and thus the interpolation
penalty is small. This is easier to see with the simpler standard additive models in
the next section. The quadratic penalty tends to be higher for rules whose if-parts
ﬁre only slightly. The penalty is most severe for inputs that occur where the if-part
sets are sparse.
The conditional variance gives a natural measure of conﬁdence in the fuzzy
system output FðxÞ. There is no need to invoke Type-2 fuzzy sets or other ad hoc
schemes to capture this second-order uncertainty of a given fuzzy output. Com-
puting the conditional variance involves no more computation than what existing
fuzzy models already use to compute the ﬁrst-order output FðxÞ. So it has arguably
been a needless oversight not to give users this conﬁdence information. The ﬁrst
published plot of the conditional variance of a fuzzy system appeared as Fig. 4 in
the 2005 paper [9]. The ﬁgure shows that rules involving one portion of the input
space have substantially more conﬁdence than the rest.
We conclude this section with two more extensions of the additive model. The
ﬁrst is the extension of the conditional covariance when the fuzzy system is a
vector-valued fuzzy system F: ℝn →ℝp. Then the conditional variance extends to a
p-by-p conditional covariance matrix KYjX = x:
KYjX = x = ∑m
j = 1pjðxÞ KYjX = x, BjðxÞ + ∑m
j = 1pjðxÞ ðcjðxÞ −FðxÞÞðcjðxÞ −FðxÞÞT. ð43Þ
Both the centroid cjðxÞ and output FðxÞ are here p-dimensional column vectors
with the same deﬁnitions as before. The local conditional covariance matrix
KYjX = x, BjðxÞ = EBjðxÞ½ðY −cjðxÞÞðY −cðxÞÞT uses the conditional density pBjðxÞðy j xÞ.
Users can calculate these local matrices directly for simple shapes of the then-part
sets. The weak law of large numbers also allows sample covariance matrices to
approximate these population conditional covariance matrices if there are enough
random samples and if all appropriate moments exist.
The second extension is to combining multiple fuzzy systems. The fuzzy sys-
tems themselves need not be additive.
Suppose there are q separate real-valued fuzzy systems F1, . . . , Fq. The kth
fuzzy system Fk somehow produces its own combined rule ﬁrings BkðxÞ. We also
assign the nonnegative system weight wk to Fk. Then we can additively combine the
weighted rule ﬁrings wkBkðxÞ to give the multi-system combined ﬁrings
BðxÞ = ∑q
k = 1wkBkðxÞ. This allows the system to combine rules or throughputs
254
B. Kosko

rather than simply combining outputs. Taking the centroid gives the global output
as a convex sum of the system centroids: FðxÞ = ∑q
k = 1pkðxÞ ckðxÞ if ck is the system
centroid of BkðxÞ. The result is FðxÞ = ∑q
k = 1pkðxÞ FkðxÞ if the q systems are each
centroidal. The output is richer still if each of the q systems is not just centroidal but
additive [5]:
FðxÞ = ∑q
k = 1∑mk
j = 1pk
j ðxÞ ck
j ðxÞ
ð44Þ
where the convex coefﬁcients are now
pk
j ðxÞ =
wkwk
j Vk
j ðxÞ
∑q
u = 1∑mu
v = 1wuwu
v Vu
v ðxÞ .
ð45Þ
The next section shows that all the above additive results become simpler and
more practical still if the fuzzy systems are standard additive.
3
The Standard Additive Model (SAM): Rule Firing
as Multiplicative Scaling
How exactly does the vector input x ∈ℝn ﬁre the jth rule RAj →Bj? Standard additive
models give a simple and useful answer: multiplication. We say that an additive
fuzzy system F: ℝn →ℝis a standard additive model (SAM) if the ﬁred if-part set
value ajðxÞ multiplicatively scales the then-part Bj [1, 5]:
BjðxÞ = ajðxÞBj .
ð46Þ
The multiplicative scaling shrinks the then-part set Bj over the same base. This
scaling leaves the relative structure of the then-part set unchanged. The min-clip
minðajðxÞ, BjÞ simply discards all then-part set information above the threshold
ajðxÞ. The same problem occurs for almost all other triangular-norm functions of
ajðxÞ and Bj. So a user would need to produce some compelling reason for using
some ﬁring operator other than SAM multiplicative scaling.
The jth rule itself deﬁnes a Cartesian “patch” Aj × Bj or fuzzy subset of the
product space X × Y : RAj →Bj = Aj × Bj = fðx, yÞ ∈X × Y: x ∈Aj & y ∈Bjg. Now
suppose the vector input is x0. Then convolution gives the ﬁred rule as a product [5]:
Z ∞
−∞
δðx −x0Þ RAj →Bjðx, yÞ dx =
Z ∞
−∞
δðx −x0Þ ajðxÞ bjðyÞ dx
ð47Þ
= bjðyÞ
Z ∞
−∞
δðx −x0Þ ajðxÞ dx
ð48Þ
Additive Fuzzy Systems as Generalized Probability Mixture Models
255

= ajðx0Þ bjðyÞ.
ð49Þ
So a traditional multiplicative Cartesian product leads to the SAM scaling
ajðxÞBj .
The SAM structure greatly simpliﬁes the above results for additive fuzzy sys-
tems. The controlling fact is that ajðxÞ factors out of the key SAM calculations. This
leads in turn to an important cancellation that converts the local conditional
probability pBjðy j xÞ to the unconditional probability pBjðyÞ:
pBjðy j xÞ =
bjðy j xÞ
R ∞
−∞bjðy j xÞ dy
ð50Þ
=
ajðxÞ bjðyÞ
R ∞
−∞ajðxÞ bjðyÞ dy
ð51Þ
=
ajðxÞ bjðyÞ
ajðxÞ
R ∞
−∞bjðyÞ dy
ð52Þ
=
bjðyÞ
R ∞
−∞bjðyÞ dy
ð53Þ
= bjðyÞ
Vj
ð54Þ
= pBjðyÞ
ð55Þ
if ajðxÞ > 0. Zero conditional probability pBjðy j xÞ = 0 holds for zero if-part
activations ajðxÞ = 0 so long as the then-part set functions bj are not trivial.
The SAM volume or area Vj is a constant that the user can pre-compute in
advance of running the SAM fuzzy system. This also holds for the SAM then-part
centroids cj:
cjðxÞ =
R ∞
−∞y bjðy j xÞ dy
R ∞
−∞bjðy j xÞ dy
ð56Þ
= ajðxÞ
R ∞
−∞y bjðyÞ dy
ajðxÞ
R ∞
−∞bjðyÞ dy
ð57Þ
= cj
ð58Þ
since again ajðxÞ>0 by assumption.
256
B. Kosko

The SAM structure likewise simpliﬁes Proposition 1 to a convex sum or mixture
of unconditional probability density functions:
pðy j xÞ = ∑m
j = 1pjðxÞ pBjðyÞ.
ð59Þ
This is the result that directly generalizes the mixture probability model below.
These SAM simpliﬁcations now give the SAM Theorem [1, 5, 6] as a special
case of Theorem 2:
FðxÞ =
∑m
j = 1wj ajðxÞ Vj cj
∑m
k = 1wk ajðxÞ Vk
ð60Þ
= ∑m
j = 1pjðxÞ cj.
ð61Þ
But now the convex coefﬁcients are simpler:
pjðxÞ =
ajðxÞ wj Vj
∑m
k = 1akðxÞ wk Vk
.
ð62Þ
This SAM system also enjoys simple forms for adaptation and statistics as well
as for function approximation. One example is the combination of q-many SAM set
systems using (10):
FðAÞ = ∑q
k = 1∑mk
j = 1pk
j ðAÞ ck
j
ð63Þ
for input fuzzy set A. The SAM convex coefﬁcients have the form
pk
j ðAÞ =
wkwk
j ak
j ðAÞ Vk
j
∑q
u = 1∑mu
v = 1wuwu
v ajðAÞ Vu
v
ð64Þ
if ajðAÞ =
R ∞
−∞aðxÞ ajðxÞ dx. Gradient tuning of set-function SAMs can lead to
extremely complicated learning laws for updating the if-part sets [7]. This is not the
case for tuning ordinary point SAMs.
The most important SAM special case historically has been the so-called “center
of gravity” or COG fuzzy system because such systems were the basis of almost all
early applications of fuzzy systems [5, 11, 14]:
FðxÞ =
∑m
j = 1ajðxÞ cj
∑m
k = 1 ajðxÞ .
ð65Þ
So a COG corresponds to SAM with equal volumes and equal weights since then
all the volume and weight terms cancel out of the SAM ratio. Some fuzzy engineers
have ignored the shape of the then-part sets and just worked with a “spike” centroid.
This corresponds to interpreting the then-part set Bj as a delta function centered at
Additive Fuzzy Systems as Generalized Probability Mixture Models
257

the centroid: Bj = δðy −cjÞ. Then the area or volume of Bj integrates to one and thus
CentroidðBjÞ = cj holds from the sifting property of the delta function.
Gaussian sets further simplify the COG model. Using factored if-part set func-
tions ajðxÞ = ∏m
k = 1ak
j ðxkÞ in the SAM/COG ratio then gives the popular radial basis
functions found in the neural network literature [15–18] when both if-part sets and
then-part sets are Gaussian or truncated-Gaussian sets. The idea for rule generation
here comes out of the theory of probability density estimation: Center a Gaussian
ball at the input vector x and center a Gaussian bell curve at the output value y for a
given data pair ðx, yÞ. Then adding up such normalized terms gives a type of
smoothed histogram of the sampled joint probability density function.
Rule weights can depend on then-part-set volumes. This often occurs in
Gaussian SAMs so that a rule with a wide then-part set Bj will not have more
inﬂuence than the same rule with a thinner then-part set. The wider then-part set has
a larger volume Vj and thus has more inﬂuence on the output FðxÞ because the
SAM ratio (60) is increasing in Vj. The choice wj = 1
Vj cancels the volumes in the
SAM ratio. The more common choice wj = 1
V2
j makes the width or size of then-part
sets vary inversely with their inﬂuence on the output. That roughly captures the
intuition that more uncertain rules should have less overall inﬂuence. It also
changes the SAM learning law for the volumes because then [4, 5]
∂F
∂Vj
= pjðxÞ cjðxÞ −FðxÞ


1
Vj
+ 1
wj
∂wj
∂Vj


ð66Þ
= pjðxÞ cjðxÞ −FðxÞ


1
Vj
−V2
j
2
V3
j
 
!
ð67Þ
= −pjðxÞ cjðxÞ −FðxÞ

 1
Vj
.
ð68Þ
This leads to the volume learning law Vjðt + 1Þ = VjðtÞ −μtεðtÞ pjðxÞ
VjðtÞ cj −FðxÞ


. A
cruder approach simply sets the rule weights equal to the inverse variance: wj = 1
σ2
j .
The conditional variance of a SAM simpliﬁes to
V½Y j X = x = ∑m
j = 1pjðxÞ σ2
Bj + ∑m
j = 1pjðxÞ cjðxÞ −FðxÞ

2
ð69Þ
since the then-part set variances σ2
Bj no longer depend on the input x. The COG
case simpliﬁes further because all the then-part variances are the same and thus each
then-part set Bj has the same variance σ2. Then the convex sum structure gives the
COG conditional variance as
V½Y j X = x = σ2 + ∑m
j = 1pjðxÞ cjðxÞ −FðxÞ

2.
ð70Þ
258
B. Kosko

This shows that the shape of the then-part sets matters for higher-order uncer-
tainty even for the simplest COG models because different shapes give different
inherent variances σ2. The positive value σ2 represents the minimal level of
uncertainty that a COG can achieve.
SAM systems also allow exact representation of arbitrary bounded functions
f : ℝn →ℝ. The Watkins Representation Theorem [19–22] states that a SAM system
F needs only two rules to exactly represent a bounded real function f in the sense
that FðxÞ = f ðxÞ for all x. Such representation trivializes the usual problem of
exponential rule explosion in fuzzy function approximation. The catch is that the
two if-part set functions build the bounded function f directly into their deﬁnition:
a1ðxÞ = sup f −f ðxÞ
sup f −inf f and a2ðxÞ = 1 −a1ðxÞ. The two rules have the linguistic form “If
X = A then Y = B1” and “If X = not A then Y = B2”. The then-part sets B1 and B2 can
have any shape so long as the inﬁmum of f is the center of B1 and the supremum is
the center of B2: c1 = inf f and c2 = sup f . The volumes or areas can be any positive
value so long as they are equal: V1 = V2 > 0. Then unity rule weights give
FðxÞ =
∑2
j = 1ajðxÞ Vj cj
∑2
k = 1ajðxÞ Vk
ð71Þ
= a1ðxÞ inf f −ð1 −a1ðxÞÞ sup f
a1ðxÞ + 1 −a1ðxÞ
ð72Þ
=
sup f −f ðxÞ
sup f −inf f


inf f −sup f
ð
Þ + sup f
ð73Þ
= f ðxÞ for all x.
ð74Þ
Such SAM representation is especially useful in modern Bayesian statistics
because it allows just two rules to represent either a bounded prior or a bounded
likelihood probability density function. A common conjugate prior is the beta
probability density function f ðθÞ = Betaðα, βÞ = Γðα + βÞθα −1ð1 −θÞβ −1
ΓðαÞΓðβÞ
for θ in the unit
interval and for positive shape parameters α and β. Here Γ denotes the gamma
function ΓðαÞ =
R ∞
0 xα −1e −xdx. Then a two-rule SAM can exactly represent the
beta density f ðθÞ = Betað5, 8Þ with the set function [21] a1ðθÞ = 1 −1111
7744 θ7ð1 −θÞ4
if c1 = inf f = 0 and c2 = sup f =
Γð13Þ
Γð8ÞΓð5Þ
7
11
 	7
4
11
 	4. The approximation power of
SAMs also allows users to go beyond closed-form densities and use rule-deﬁned
priors or likelihoods and still be assured that the resulting fuzzy system will uni-
formly approximate the underlying Bayesian posterior density f ðθ j xÞ [21]. This
even holds for hierarchical Bayes systems where the prior density itself depends on
a hyperprior density [22] The above scheme for combining SAMs can combine
rule-represented priors or likelihoods with rule-deﬁned priors or likelihoods into a
single SAM system.
Additive Fuzzy Systems as Generalized Probability Mixture Models
259

4
Generalized Mixture Models and Continuum-Many
Rules
We conclude by showing that additive fuzzy systems generalize mixture models
and that they extend to fuzzy systems with continuum-many fuzzy rules.
Mixture models are ﬁnite convex combinations of probability density functions
(pdfs) [23]. Such a convex combination mixture of m pdfs f1, . . . , fm gives a new
pdf f with m modes if the pdfs are sufﬁciently spread out:
f ðxÞ = ∑m
j = 1πj fjðxÞ
ð75Þ
if the nonnegative mixing weights π1, . . . , πm sum to unity: ∑m
j = 1πj = 1. This
convex sum can model taking random samples from a population made up of m-many
subpopulations such as m words or patterns. Then the estimation task is to ﬁnd the
mixture weights and the parameters of the mixed pdfs. The most popular mixture by
far is the Gaussian mixture where fj is a scalar or vector Gaussian Nðμj, θ2
j Þ.
The mixture sum is not arbitrary. It follows from the elementary theorem on total
probability. Suppose m hypothesis sets H1, . . . , Hm partition the sample space Ω.
Suppose the set E⊂Ω represents some observed evidence. Then the theorem on total
probability states that the unconditional probability of the evidence PðEÞ equals the
convex combination of the prior probabilities PðHjÞ and the likelihoods PðE j HjÞ:
PðEÞ = ∑m
j = 1PðHjÞ PðE j HjÞ. This corresponds to the mixture sum because the
evidence is the input x and because πj is the prior probability of the jth class or
mixture element. So fjðxÞ = f ðx j jÞ if the conditional density f ðx j jÞ is the likelihood
that we would observe such an x if it came from the jth class or mixture density.
The ubiquitous Expectation-Maximization (EM) algorithm quite often estimates
the mixing weights and means and variances by iteratively maximizing the likeli-
hood function [23]. The class memberships of the m decision classes correspond to
the hidden or latent variables in the EM algorithm. Then carefully injected noise can
always speed up convergence of the EM algorithm [24–26] as it climbs the nearest
hill of likelihood.
Mixture moments follow directly from the convexity of the mixed sum. This
gives the unconditional mean and variance of the random variable X as [23]
E½X = ∑m
j = 1πj μj
ð76Þ
and
V½X = ∑m
j = 1πj σ2
j + ∑m
j = 1πj μj −E½X

2
ð77Þ
if the mixture’s underlying jth random variable Xj has pdf fj and thus if it has
mean μj and variance σ2
j . We see at once that the mixture mean and variance are
special cases of a SAM fuzzy system’s conditional mean and variance.
260
B. Kosko

The SAM and other additive systems generalize mixture models by making the
mixture weights πj depend on the input x: πjðxÞ = pjðxÞ. This in turn makes the
mixture’s means and variances (and other moments) depend on x and thus become
conditional moments. So mixture models correspond to ﬁxed-input centroidal
additive systems. Then Proposition 1 gives back the deﬁning mixture-density
combination for unﬁred then-part sets:
pðyÞ = ∑m
j = 1pj pBjðyÞ.
ð78Þ
Mixture models sample in effect from convex combinations of m suitably nor-
malized then-part sets.
We can extend SAM models to systems with inﬁnitely many fuzzy rules. The
cardinality of the rules can be countably or uncountably inﬁnite. We will work with
the latter continuum case. This follows from the direct extension of mixture models
to compounding models that weight one pdf with another and then integrate out the
continuous mixture index [23]. Our approach will instead impose a higher-level
mixture structure on the continuum of rules.
Suppose now that the real parameter θ indexes the continuum-many if-part set
functions aθ and the corresponding then-part sets Bθ in continuum-many rules of
the form “If X = Aθ then Y = Bθ”. Then integration gives the combined rule ﬁrings:
bðyjxÞ =
Z θ = ∞
θ = −∞
wθ bθðy j xÞ dθ
ð79Þ
if we assume the integral exists for appropriate nonnegative rule weights wθ. The
proof of Theorem 3 still goes through if (deﬁnite) integrals replace the ﬁnite sums:
FðxÞ =
Z
pθðxÞ cθ dθ
ð80Þ
and
pθðxÞ =
aθðxÞ wθ Vθ
R aϕðxÞ wϕ Vϕ dϕ .
ð81Þ
Consider a simple Gaussian set of rules for a scalar parameter θ. The rules have
vector-Gaussian if-part set functions aθ and scalar Gaussian then-part set func-
tionsbθ: aθð⋅Þ = Nðθ∙1, KθÞ and bθðyÞ = Nðθ, σ2Þ if θ∙1 denotes the n-vector with
all elements equal to θ. Kθ is an n-by-n covariance matrix. It equals the identity
matrix
in
the
simplest
or
“white”
case.
Then
cθ = θ
and
Vθ = 1
since
bθðyÞ = Nðθ, σ2Þ. This gives the output FðxÞ as a simple unconditional expectation
for each x: FðxÞ =
R
pθðxÞ θ dθ = EpθðxÞ½Θ. This approach extends at once to vector
parameters.
Additive Fuzzy Systems as Generalized Probability Mixture Models
261

Computing the convex integral for FðxÞ is more complicated than in the simpler
case of probabilistic compounding. Compounding allows the modeler to pick the
weighting pdf pθ as a normal or gamma or other well-behaved closed-form pdf. But
the SAM convex-sum pθ involves a highly nonlinear transformation of continuum-
many if-part set functions aθ. This transformation may not be tractable. Integrating
it to produce FðxÞ can only compound the computational intractability.
A practical solution is to rely on the weak law of large numbers (WLLN)
through Monte Carlo simulation. The WLLN states that the sample mean
X̄n = 1
n ∑n
k = 1Xk of independent and identically distributed ﬁnite-variance random
variables X1, X2, . . . converges in probability to the population mean E½X :
lim
n →∞PðjX̄n −E½Xj > εÞ = 0 for all ε>0. Monte Carlo simulation interprets an
ordinary deﬁnite integral
R b
a gðxÞ dx as the expectation of a random variable that has
a uniform distribution over ða, bÞ [23]:
Z b
a
gðxÞ dx = ðb −aÞ
Z b
a
gðxÞ dx
b −a = ðb −aÞE½X
ð82Þ
for X∼Uða, bÞ. The user need not integrate the integrand ðb −aÞgðxÞ. The user
need only compute values ðb −aÞgðxkÞ for random uniform draws xk from ða, bÞ.
The random draws can come from any uniform random number generator. Then the
WLLN ensures that 1
n ∑n
k = 1ðb −aÞgðxkÞ≈ðb −aÞE½X =
R b
a gðxÞdx for enough ran-
dom draws xk. The variance in the WLLN estimate decreases linearly with the
number n of draws.
Monte Carlo simulation can estimate the integrals in the continuum-rule SAM
for a given input x. Assume there are n random draws of θ from some ﬁnite interval
ða, bÞ for a fuzzy system deﬁned on the compact interval ½a, b. Then
FðxÞ =
R
aθðxÞ wθ Vθ cθ dθ
R
aϕðxÞ wϕ Vϕ dϕ
ð83Þ
≈
1
n ∑n
k = 1wk akðxÞ Vk ck
1
n ∑n
k = 1wk akðxÞ Vk
ð84Þ
= ∑n
k = 1wk akðxÞ Vk ck
∑n
k = 1wk akðxÞ Vk
ð85Þ
= ∑n
k = 1pkðxÞ ck.
ð86Þ
The result has the same convex-sum form as the ﬁnite-rule SAM even though the
sums use random choices of rules instead of ﬁring all the rules.
262
B. Kosko

The ﬁnal task is to control and shape the overall distribution of the continuum of
fuzzy rules. This allows the fuzzy engineer to deﬁne meta-rules at a much higher
level of abstraction. An engineer can also give the wave-like groupings of rules a
linguistic interpretation such as “small negative” or “medium positive” and the like.
The engineer should be able to pick an initial set of such meta-rules just as in the
case of setting up a ﬁnite SAM. Then there should be some practical way to tune
these meta-rules with data to give different levels of control or function approxi-
mation. This requires a Bayesian-like approach that puts some probabilistic struc-
ture on the parameter θ: Θ∼hðθÞ. This corresponds to the old fuzzy-engineering
task of picking the shapes of if-part and then-part sets.
Mixture densities offer a natural way to deﬁne fuzzy meta-rules over the con-
tinuum of fuzzy rules. The mixture variable is no longer x. It is now θ. Suppose the
fuzzy engineer wants to impose k-many fuzzy meta-rules. This requires mixing k-
many densities:
hðθÞ = ∑k
i = 1πi fiðθÞ.
ð87Þ
The engineer might center the mixture pdfs closer together in regions of the
input space where he desires greater control. An early example of such proximity
control was the fuzzy truck-backer-upper [27]. The truck-and-trailer rig backed up
to a loading dock from a parking lot. Closer and narrower if-part sets near the
loading dock gave ﬁner control near that equilibrium point. A few wide if-part sets
then covered much of the remaining parking lot. The engineer could distribute these
meta-rule mixture pdfs in the same way.
Standard statistical techniques can then compute fuzzy outputs FðxÞ and tune
the fuzzy meta-rules. Monte Carlo simulation can estimate the output FðxÞ for a
given x. But the sampling now cannot be from a uniform density in general. That
would always give the same output on average. The sampling must come instead
from the meta-rule mixture density hðθÞ itself to reﬂect the distribution of the meta-
rules. This is just the well-known technique of importance sampling from mixtures
[28]. Then the E-M algorithm or its variants can tune the mixture parameters based
on sampled inputs x.
There is algorithmic irony in using mixture densities to control fuzzy meta-rules
after seeing that the underlying fuzzy rule based systems generalize mixture den-
sities. There is also a loss of the exponential rule explosion that plagues ordinary
ﬁnite fuzzy systems. This loss holds because the growth in meta-rules is only linear
in the number k of mixed densities. That shifts much of the computational burden to
the sampling task involved in converting an input x to an output FðxÞ.
References
1. Kosko, B.: Neural Networks and Fuzzy Systems, Prentice-Hall (1991)
2. Kosko, B.: Fuzzy systems as universal approximators. IEEE Trans. Comput. 43(11),
1329–1333 (1994)
Additive Fuzzy Systems as Generalized Probability Mixture Models
263

3. Kosko, B.: Optimal fuzzy rules cover extrema. Int. J. Intell. Syst. 10, 249–255 (1995)
4. Dickerson, J.A., Kosko, B.: “Fuzzy Function Approximation with Ellipsoidal Rules”, with
J.A. Dickerson. IEEE Trans. Syst. Man Cybern. 26(4), 542–560 (1996)
5. Kosko, B., Fuzzy Engineering. Prentice-Hall (1996)
6. Kosko, B.: Global stability of generalized additive fuzzy systems. IEEE Trans. Syst. Man
Cybern. 28(3), 441–452 (1998)
7. Mitaim, S., Kosko, B.: Neural fuzzy agents for proﬁle learning and adaptive object matching.
Presence 7(6), 617–637 (1998)
8. Mitaim, S., Kosko, B.: The shape of fuzzy sets in adaptive function approximation. IEEE
Trans. Fuzzy Syst. 9(4), 637–656 (2001)
9. Lee, I., Anderson, W.F., Kosko, B.: Modeling of gunshot bruises in soft body armor with an
adaptive fuzzy system. IEEE Trans. Syst. Man Cybern. 35(6), 1374–1390 (2005)
10. Kandel, A.: Fuzzy Mathematical Techniques with Applications. Addison-Wesley (1986)
11. Klir, G.J., Folger, T.A.: Fuzzy Sets, Uncertainty, and Information. Prentice-Hall (1988)
12. Terano, T., Asai, A., Sugeno, M.: Fuzzy Systems Theory and its Applications. Academic Press
(1992)
13. Zimmerman, H.J.: Fuzzy Set Theory and its Application. Kluwer (1985)
14. Isaka, S., Kosko, B.: Fuzzy Logic. Sci. Am. 269, 76–81 (1993)
15. Jang, J.-S.R., Sun, C.-T.: Functional equivalence between radial basis function networks and
fuzzy inference systems. IEEE Trans. Neural Netw. 4(1), 156–159 (1993)
16. Moody, J., Darken, C.: Fast learning in networks of locally tuned processing units. Neural
Comput. 1, 281–294 (1989)
17. Specht, D.F.: A general regression neural network. IEEE Trans. Neural Netw. 4, 549–557
(1991)
18. Wang, L.-X., Mendel, J.M.: Fuzzy basis functions, universal approximation, and orthogonal
least-squares learning. IEEE Trans. Neural Netw. 3, 802–814 (1992)
19. Watkins, F.A.: Fuzzy Engineering. Ph.D. Dissertation, Department of Electrical Engineering,
UC Irvine, Irvine, CA (1994)
20. Watkins, F.A.: The representation problem for additive fuzzy systems. In: Proceedings of the
IEEE Int. Conference on Fuzzy Systems (IEEE FUZZ), vol. 1, pp. 117–122, March 1995
21. Osoba, O., Mitaim, S., Kosko, B.: Bayesian inference with adaptive fuzzy priors and
likelihoods. IEEE Trans. Syst. Man Cybern.-B 41(5), 1183–1197 (2011)
22. Osoba, O., Mitaim, S., Kosko, B.: Triply fuzzy function approximation for hierarchical
bayesian inference. Fuzzy Optim. Decis. Making 11(3), 241–268 (2012)
23. Hogg, R.V., McKean, J.W., Craig, A.T.: Introduction to Mathematical Statistics, 7th edn.
Prentice Hall, New York (2013)
24. Osoba, O., Mitaim, S., Kosko, B.: The noisy expectation-maximization algorithm. Fluctuation
Noise Lett. 12(3), 1350012-1–1350012-30 (2013)
25. Audhkhasi, K., Osoba, O., Kosko, B.: Noise beneﬁts in backpropagation and deep
bidirectional pre-training. In: Proceedings of the 2013 International Joint Conference on
Neural Networks, pp. 2254–2261, August (2013)
26. Audhkhasi, K., Osoba, O., Kosko, B.: Noise beneﬁts in convolutional neural networks. In:
Proceedings of the 2014 International Conference on Advances in Big Data Analytics,
pp. 73–80, July (2014)
27. Kong, S.G., Kosko, B.: “Adaptive fuzzy systems for backing up a truck-and- trailer”, with
S.G. Kong. IEEE Trans. Neural Netw. 3(2), 211–223 (1992)
28. Cappe, O., Douc, R., Guillin, A., Marin, J.-M., Robert, C.P.: Adaptive importance sampling in
general mixture classes. Stat Comput., 18(4), 447–459 (2008)
264
B. Kosko

Author Biography
Bart Kosko is a Professor of electrical engineering and law with
the University of Southern California (USC) in Los Angeles. He
received degrees in philosophy, economics, mathematics, elec-
trical engineering, and law. Dr. Kosko a Past Director of USC’s
Signal and Image Processing Institute and the organizer of
several conferences on neural networks and fuzzy systems. He
has published the textbooks Neural Networks and Fuzzy
Systems and Fuzzy Engineering, the trade books Fuzzy Think-
ing and Heaven in a Chip, the novel Nanotime, edited the
volume Neural Networks for Signal Processing, and co-edited
the volume Intelligent Signal Processing. His most recent book
is Noise.
Additive Fuzzy Systems as Generalized Probability Mixture Models
265

Fuzzy Information Retrieval Systems:
A Historical Perspective
Donald H. Kraft, Erin Colvin, Gloria Bordogna and Gabriella Pasi
Abstract The application of fuzzy set theory to information retrieval has been
applied, speciﬁcally to Boolean models. This includes fuzzy indexing procedures
deﬁned to represent the varying signiﬁcance of terms in synthesizing the docu-
ments’ contents, the deﬁnition of query languages to allow the expression of soft
selection conditions, and associative retrieval mechanisms to model fuzzy pseudo-
thesauri, fuzzy ontologies, and fuzzy categorizations of documents.
Keywords Fuzzy ⋅Information retrieval ⋅Query ⋅Imprecision ⋅Vagueness ⋅
Indexing ⋅Ememes ⋅Geographic information retrieval
1
Introduction
The objective of this entry is to provide an overview of the application of fuzzy set
theory to design an information retrieval system (IRS). We consider the represen-
tation of uncertainty, imprecision, vagueness and subjectivity, which are charac-
teristics of the process of information searching and retrieval. Salton notes that IR
deals with the representation, storage, and access to “documents” or representatives
of documents (i.e., document surrogates) [49]. The elements of an IRS include a set
of documents, document processing for content analysis, a query describing an
D.H. Kraft (✉) ⋅E. Colvin
Colorado Technical University, Colorado Springs, USA
e-mail: kraft@bit.csc.lsu.edu
E. Colvin
e-mail: eschlapkohl@yahoo.com
G. Bordogna
Istituto per il Rilevamento Elettromagnetico dell’Ambiente, CNR, Milano (MI), Italy
e-mail: bordogna.g@irea.cnr.it
G. Pasi
Disco Università degli Studi di Milano Bicocca, Milano, Italy
e-mail: pasi@disco.unimib.it
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_15
267

information need, a matching of the query and the documents, an output module
with a ranked list of documents deemed relevant, and a user interface.
We will discuss some current trends and key issues in information retrieval, and
give an overview of the basic notions of fuzzy set theory to model IRSs. We will
also provide a description of the traditional fuzzy document representation and a
fuzzy representation of documents structured into logical sections that can be
adapted to the subjective needs of a user. In addition, we give a description of how
the Boolean query language of IR can be extended so as to make it ﬂexible and
suitable to express soft constraints by capturing the vagueness of the user needs.
Both numeric and linguistic selection conditions are introduced to qualify term’s
importance, and we will show how linguistic quantiﬁers are deﬁned to specify soft
aggregation operators of query terms. We will also discuss how fuzzy sets can serve
to deﬁne associative mechanisms to expand the functionalities of IR systems, i.e.,
the capability to represent concepts and to model their semantic relationships.
Fuzzy sets provide notions that can be applied to this purpose allowing to model
fuzzy pseudothesauri and fuzzy ontologies and to build fuzzy categorizations of
documents via fuzzy clustering techniques. Lastly, we present emerging applica-
tions of information retrieval modelled within the fuzzy framework such as geo-
graphic
information
retrieval,
multi-dimensional
relevance
evaluation,
and
discovery of similar web pages contents in multiple searches and ememe identiﬁ-
cation and tracking and discuss fuzzy performance measures for IR systems.
2
Key Issues in Information Retrieval
Modeling the concept of relevance in IR is certainly a key issue, perhaps the most
difﬁcult one, and no doubt the most important one. What makes a document relevant
to a given user is still not fully understood, speciﬁcally when one goes beyond
topicality (i.e., the matching of the topics of the query with the topics of the docu-
ment). This leads to the realization that relevance is imprecise as well as subjective.
A second key issue is the representation of the documents in a collection, as well
as the representation of users’ information needs, especially for the purpose of
matching documents to the queries semantically. This implies introducing incom-
pleteness, approximation, and managing vagueness and imprecision. Yet another
key issue is how to evaluate an information retrieval system’s performance prop-
erly, where imprecision also exists.
2.1 Imprecision, Vagueness, Uncertainty, and Inconsistency
in Information Retrieval
Very often the terms imprecision, vagueness, uncertainty, and inconsistency are
used as synonymous concepts. Nevertheless when they are used to qualify a
characteristic of the information they have a distinct meaning [42].
268
D.H. Kraft et al.

There are several ways to represent imprecise and vague concepts. One can
approach this indirectly by deﬁning similarity or proximity relationships between
each pair of imprecise and vague concepts. If we regard a document as an imprecise or
vague concept, i.e., as bearing a vague content, a numeric value computed by a
similarity measure can be used to express the closeness of any two pairs of docu-
ments. This is the way of dealing with the imprecise and vague document and query
contents via the weights in IR’s vector space model. In this context the documents and
the query are represented as points in a vector space of terms and the distances
between the query and the documents points are used to quantify their similarity [50].
Another way to represent vague and imprecise concepts is by means of the
notion of fuzzy set. The notion of a fuzzy set is an extension to normal set theory
[66]. The notion of fuzzy set has been used in the IR context to represent the vague
concepts expressed in a ﬂexible query for specifying soft selection conditions of the
documents [59].
Uncertainty is related to the truth of a proposition, intended as the conformity of
the information carried by the proposition with the considered reality. Possibility
theory [27, 65] together with the concept of a linguistic variable deﬁned within
fuzzy set theory [67], provide a unifying formal framework to formalize the
management of imprecise, vague and uncertain information [15].
The same information content can be expressed by choosing a trade-off between
the vagueness and the uncertainty embedded in a proposition. A dual representation
can eliminate imprecision and augment the uncertainty, like in the expression “it is
not completely probable that document d fully satisﬁes the query q.” One way to
model IR is to regard it as an uncertain problem [33].
There are two alternative ways to model IR activity. One possibility is to model the
query evaluation mechanism as an uncertain decision process. The concept of rele-
vance is considered binary (crisp), as the query evaluation mechanism computes the
probability of relevance of a document d to a query q. Such an approach, which does
model the uncertainty of the retrieval process, has been introduced and developed
using probabilistic IR models [26, 28, 60]. Another possibility is to interpret the query
as the speciﬁcation of soft “elastic” constraints that the representation of a document
can satisfy to an extent, and to consider the term relevant as a gradual (vague)
concept. This is the approach adopted in fuzzy IR models [9, 33]. In this latter case,
the decision process performed by the query evaluation mechanism computes the
degree of satisfaction of the query by the representation of each document.
This satisfaction degree, called the retrieval status value (RSV), is considered an
estimate of the degree of relevance (or is at least proportional to the relevance) of a
given document with respect to a given user query. An RSV of 1 implies maximum
relevance; an RSV of 0 implies absolutely no relevance. And, an RSV in the
interval (0, 1) implies an intermediate level or degree of relevance.
Inconsistency comes from the simultaneous presence of contradictory informa-
tion about the same reality. An example can be observed when submitting the same
query to several IRSs that adopt different representations of documents and produce
different results. This is actually very common and often occurs when searching for
information over the Internet using different search engines. To solve this kind of
Fuzzy Information Retrieval Systems: A Historical Perspective
269

inconsistency, some fusion strategies can be applied to the ranked lists each search
engine produces. In fact, this is what metasearch engines do [14, 64].
The document representation based on a selection of index terms is invariably
incomplete. When synthesizing the content of a text manually by asking an expert to
select a set of index terms, one introduces subjectivity in the representation. On the
other hand, automatic full-text indexing introduces imprecision since the terms are
not all fully signiﬁcant in characterizing a document’s content. However, these terms
can have a partial signiﬁcance that might also depend upon the context in which they
appear, i.e., which document component. Modern retrieval systems may include
natural language processing capabilities to try to deal with semantics. Thus, one can
move from the notion of a document as a “bag of terms” to having a set of concepts.
This leads to the idea of a taxonomy, i.e., a vocabulary and structure (e.g., a cat or a
dog is a pet), and an ontology, i.e., a set of relationships and rules and constraints
(i.e., dog chases cat). These ideas have their own sets of imprecision or vagueness.
3
Fuzzy Retrieval Models
Fuzzy retrieval models have been deﬁned in order to reduce the imprecision that
characterizes the Boolean indexing process, to represent the user’s vagueness in
queries, and to deal with discriminated answers estimating the partial relevance of
the documents with respect to queries. Extended Boolean models based on fuzzy set
theory have been deﬁned to deal with one or more of these aspects [5, 10, 11, 16,
18, 21, 32, 47, 61]. Surveys of fuzzy extensions for IRSs and of fuzzy general-
izations of the Boolean retrieval model can be found in [9, 33].
Fuzzy “knowledge based” models [35, 36], and fuzzy associative mechanisms
[38–40, 43] have been deﬁned to cope with the incompleteness that characterizes
either the representation of documents or the users’ queries. [37] illustrates a wide
range of methods to generate fuzzy associative mechanisms.
It has been speculated that Boolean logic is passé’, out of vogue. Yet, researchers
have employed p-norms in the vector space model or Bayesian inference nets in the
probabilistic model to incorporate Boolean logic. In addition, the use of Boolean
logic to separate a collection of records into two disjoint classes has been consid-
ered, e.g., using the one-clause-at-a time (OCAT) methodology [56]. Even now
retrieval systems such as Dialog and web search engines such as Google allow for
Boolean connectives.
4
Fuzzy Techniques for Indexing
During the indexing process one wants to provide more speciﬁc and exhaustive
representations of each document’s information content. This means improving
these representations beyond those generated by existing indexing mechanisms. We
270
D.H. Kraft et al.

introduce the fuzzy interpretation of a weighted document representation and a
fuzzy representation of documents structured in logical sections that can be adapted
to a user that has a subjective criteria for interpreting the content of documents
[11, 41]. In order to increase the effectiveness of IRSs, the indexing process plays a
crucial role.
4.1 Vector Space, Probabilistic, and Generalized Boolean
Indexing
The vector space model and the probabilistic models generally adopt a weighted
document representation, which has improved the Boolean document representation
by allowing the association of a numeric weight with each index term [54, 60]. The
automatic computation of the index term [56, 57]. In this case, the indexing
mechanism computes d for each document and t for each term by means of a
function F. An example of F has the index term weight increasing with the fre-
quency of term t in document d but decreasing with the frequency of the term in all
the documents of the archive is given by
F d, t
ð
Þ = tfdt×g IDFt
ð
Þ where tfdt is a normalized term frequency, which can be
deﬁned as:
tfdt =
OCCdt
MAXOCCd
,
ð1Þ
OCCdt is the number of occurrences of t in d; MAXOCCd is the number of
occurrences of the most frequent term in d, IDFt is an inverse document frequency
which can be deﬁned as:
IDFt = Log
N
NDOCt
,
ð2Þ
N is the total number of documents in the archive; NDOCt is the number of
documents indexed by t; and g is a normalizing function.
To simplify the computation of this value, it is possible to heuristically
approximate it: during the archive generation phase, with an expert indicating the
estimated percentage of the average length of each section with respect to the
average length of documents (PERLs). Given the number of occurrences of
the most frequent term in each document d, MAXOCCd, an approximation of the
number of occurrences of the most frequent term in section s of document d is
MAXOCCsd = PERLs*MAXOCCd.
The adoption of weighted indexes allows for an estimate of the relevance, or of
the probability of relevance, of documents to a query [53, 60]. Based on such an
indexing function, and by incorporating Boolean logic into the query, the fuzzy
interpretation of an extended Boolean model has been to adopt a weighted
Fuzzy Information Retrieval Systems: A Historical Perspective
271

document representation and to interpret it as a fuzzy set of terms [17]. From a
mathematical point of view, this is a quite natural extension: the concept of the
signiﬁcance of index terms in describing the information content of a document can
then be naturally described by adopting the function F, such as the one deﬁned
above, as the membership function of the fuzzy set representing a document’s being
in the subset of concepts represented by the term in question. Formally, a document
is represented as a fuzzy set of terms:
Rd = ∑t ∈TμRd tð Þ=t,
ð3Þ
in which the membership function is deﬁned as μRd: D×T →0, 1
½
. In this case,
μRd tð Þ = F d, t
ð
Þ, the membership value, can be obtained by the indexing function F
which is expressed by a numeric score or RSV.
Fuzzy set theory has been applied to deﬁne new and more powerful indexing
models than the one based on the function above. The deﬁnition of new indexing
functions has been motivated by several considerations. First, the F functions do not
take into account the idea that a term can play different roles within a text according
to the distribution of its occurrences. The text can be considered as a black box,
closed to a user’s interpretation. This outlines the fact that relevance judgments are
driven by a subjective interpretation of the document’s structure, and supports the
idea of dynamic and adaptive indexing [2, 11]. By adaptive indexing, we mean
indexing procedures which take into account the users’ desire to interpret the
document contents and to “build” their synthesis on the basis of this interpretation.
An indexing model has been proposed where the occurrences of a term in the
different documents’ sections are taken into account according to speciﬁc criteria,
and the user’s interpretation of the text is modeled [11]. During the retrieval phase,
the user can specify the distinct importance of the sections and decide that a term
must be present in all the sections or in at least a certain number of them in order to
consider the term fully signiﬁcant. A section is a logical subpart identiﬁed by si,
where i∈1,..,n and n is the total number of the sections in the documents. We
assume here that an archive contains documents sharing a common structure.
4.2 Fuzzy Representation of Structured Documents
We also consider the synthesis of a fuzzy representation of structured documents
that takes into account the user needs [11]. A document can be represented as an
entity composed of sections (e.g., title, authors, introduction, and references). The
information role of each term occurrence depends then on the semantics of the
subpart where it is located. This means that to the aim of deﬁning an indexing
function for structured documents the single occurrences of a term may contribute
differently to the signiﬁcance of the term in the whole document. The document’s
subparts may have a different importance determined by the users’ needs.
272
D.H. Kraft et al.

When generating an archive of a set of documents, it is necessary to deﬁne the
sections one wants to employ to structure each document. The structure of the
documents, i.e., the type and number of sections, depends on the semantics of the
documents and on the accuracy of the indexing module. A formal representation of
a document using a fuzzy binary relation: with each pair <section, term>, a sig-
niﬁcance degree in the interval [0,1] is computed to express the signiﬁcance of that
term in that document section. To obtain the overall signiﬁcance degree of a term in
a document, i.e., the index term weight, these values are dynamically aggregated by
taking into account the indications that a user states in the query formulation. Other
non-fuzzy approaches have also introduced the concept of a boosting factor to
emphasize differently the contribution of the index terms occurrences depending on
the document sections to the overall index term weights. However these approaches
compute static index term weights during the indexing process, without taking into
account the user interpretation.
On the contrary, in the fuzzy approach, the aggregation function, is deﬁned on
two levels. First, the user expresses preferences for the document sections (the
equivalent of the boosting factors), second, the user should decide which aggre-
gation function has to be applied to produce the overall signiﬁcance degree. By
adopting this document representation, the same query can select documents in
different relevance order depending on the user’s preferences.
Formally, a document is represented as a fuzzy binary relation,
Rd = ∑t, s
ð
Þ ∈T×S μd t, s
ð
Þ= t, s
ð
Þ, where the value μd t, s
ð
Þ = Fs d, t
ð
Þ expresses the
signiﬁcance of term t in section s of document d. An indexing function Fs:
DxT →[0,1] is then deﬁned for each section s. The overall signiﬁcance degree F(d,t)
is computed by combining the single signiﬁcance degrees of the sections, the Fs(d,t)
s, through an aggregation function speciﬁed by the user.
5
Deﬁnition of Flexible Query Languages
The objective here is to deﬁne query languages that are more expressive and natural
than classical Boolean logic. This is done to capture the vagueness of user needs as
well as to simplify user system interaction. This has been pursued with two different
approaches. First, there has been work on the deﬁnition of soft selection criteria
(soft constraints), which allow the speciﬁcation of the different importance of the
search terms. Query languages based on numeric query term weights with different
semantics have been ﬁrst proposed as an aid to deﬁne more expressive selection
criteria [5, 18, 21, 22, 61]. An evolution of these approaches has been deﬁned that
introduces linguistic query weights, speciﬁed by fuzzy sets such as important or
very important, in order to express the different vague importance of the query
terms [12]. Second, there is the approach of introducing soft aggregation operators
for the selection criteria, characterized by a parametric behavior which can be set
between the two extremes of intersection (AND) and union (OR) as adopted in
Boolean logic. Boolean query languages have been extended and generalized by
Fuzzy Information Retrieval Systems: A Historical Perspective
273

deﬁning aggregation operators as linguistic quantiﬁers such as at least k or about k
[10]. As a consequence of this extension, the exact matching that is employed by a
classical Boolean IRS is softened using a partial matching mechanism that evaluates
the degree of satisfaction of a user’s query for each document. This degree of
satisfaction is the RSV that is used for ranking.
5.1 Term Signiﬁcance
To obtain the overall degree of signiﬁcance of a term in a document, an aggregation
scheme of the values has been suggested, based on a twofold speciﬁcation of the
user [11]. When starting a retrieval session, the user can specify her/his preferences
on the sections s by numeric score αs∈[0,1] where the most important sections have
an importance weight close to 1.
Within fuzzy set theory linguistic quantiﬁers used to specify aggregations are
deﬁned as Ordered Weighted Averaging (OWA) operators [62]. When processing a
query, the ﬁrst step accomplished by the system for evaluating F(d,t) is the selection
of the OWA operator associated with the linguistic quantiﬁer lq, OWAlq. When the
user does not specify any preferences on the documents’ sections, the overall sig-
niﬁcance degree F(d,t) is obtained by applying directly the OWAlq operator to the
values μ1 d, t
ð
Þ, . . . , μn d, t
ð
Þ : F d, t
ð
Þ = OWAlq μ1 d, t
ð
Þ, . . . , μn d, t
ð
Þ
ð
Þ. When dis-
tinct preference scores α1,…,αn are associated with the sections, it is ﬁrst necessary
to modify the values μ1 d, t
ð
Þ, . . . , μn d, t
ð
Þ in order to increase the “contrast” between
the contributions due to important sections with respect to those of less important
ones. The evaluation of the overall signiﬁcance degree F(d,t) is obtained by applying
the operator OWAlq to the modiﬁed degrees a1, . . , an: F d, t
ð
Þ = OWAlq a1, . . , an
ð
Þ.
5.2 Fuzzy Associative Mechanisms
These associative mechanisms allow automatically generating fuzzy pseudothe-
sauri, fuzzy ontologies, and fuzzy clustering techniques to serve three distinct but
compatible purposes. First, fuzzy pseudothesauri and fuzzy ontologies can be used
to contextualize the search by expanding the set of index terms of documents to
include additional terms by taking into account their varying signiﬁcance in rep-
resenting the topics dealt with in the documents. The degree of signiﬁcance of these
associated terms depends on the strength of the associations with a document’s
original descriptors. Second, an alternative use of fuzzy pseudothesauri and fuzzy
ontologies is to expand the query with related terms by taking into account their
varying importance in representing the concepts of interest. The importance of an
additional term is dependent upon its strength of association with the search terms
in the original query. Third, fuzzy clustering techniques, where each document can
be placed within several clusters with a given strength of belonging to each cluster,
274
D.H. Kraft et al.

can be used to expand the set of the documents retrieved in response to a query.
Documents associated with retrieved documents, i.e., in the same cluster, can be
retrieved. The degree of association of a document with the retrieved documents
does inﬂuence its RSV. Another application of fuzzy clustering is that of providing
an alternative way of presenting the results of a search. When the user does not
specify any criterion to aggregate the single degrees of the sections, a default
aggregation operator is used [10]. Since no importance is speciﬁed to differentiate
the contributions of the sections, all of them are assumed to have the same
importance weight of 1.
Associative retrieval mechanisms are deﬁned to enhance the retrieval of IRSs.
They work by retrieving additional documents that are not directly indexed by the
terms in a given query but are indexed by other, related terms, sometimes called
associated descriptors. The most common type of associative retrieval mechanism is
based upon the use of a thesaurus to associate index or query terms with related
terms. In traditional associative retrieval, these associations are crisp.
Fuzzy associative retrieval mechanisms obviously assume fuzzy associations. A
fuzzy association between two sets X = {x1,…,xm} and Y = {y1,…,yn} is formally
deﬁned as a fuzzy relation.
f : X × Y →[0,1], where the value f(x,y) represents the degree or strength of the
association existing between the values x∈X and y∈Y. In information retrieval,
different kinds of fuzzy associations can be derived depending on the semantics of
the sets X and Y.
5.3 Fuzzy Thesauri
A thesaurus is an associative mechanism that can be used to improve both indexing
and querying. The development of thesauri is very costly, as it requires a large
amount of human effort to construct and to maintain. In highly dynamic situations,
i.e., volatile situations, terms are added and new meanings derived for old terms
quite rapidly, so that the thesaurus needs frequent updates. For this reason, methods
for automatic construction of thesauri have been proposed, named pseudothesauri,
based on statistical criteria such as the terms’ co-occurrences, i.e., the simultaneous
appearance of pairs (or larger subsets) of terms in the same documents.
In a thesaurus, the relations deﬁned between terms are of different types. If the
associated descriptor has a more general meaning than the entry term, the relation is
classiﬁed as broader term (BT), while a narrower term (NT) is the inverse relation.
Synonyms and near-synonyms are parts of another type of relationship associated
by a related term (RT) connection.
The concept of a fuzzy thesaurus has been suggested [37, 38, 44, 48], where the
links between terms are weighted to indicate the relative strengths of these asso-
ciations. Fuzzy pseudothesauri are generated when the weights of the links are
automatically computed by considering document relationships rather than concept
relationships [40, 44].
Fuzzy Information Retrieval Systems: A Historical Perspective
275

The ﬁrst work on fuzzy thesauri introduced the notion of fuzzy relations to
represent associations between terms [48, 49]. Let us look at a formal deﬁnition of a
fuzzy thesaurus [38, 39]. Consider T to be the set of index terms and C to be a set of
concepts. Each term t∈T corresponds to a fuzzy set of concepts h(t):
h tð Þ = f < c, t c
ð Þ > j c ∈Cg,
ð4Þ
in which t(c) is the degree to which term t is related to concept c. A measure M is
deﬁned on all of the possible fuzzy sets of concepts, which satisﬁes: M(∅) = 0, M
(C) < ∞, M(A) ≤M(B), if A ⊆B.
A typical example of M is the cardinality of a fuzzy set. The fuzzy RT relation is
represented in a fuzzy thesaurus by the similarity relation between two index terms,
t1 and t2 ∊T and is deﬁned as:
s t1, t2
ð
Þ = M h t1
ð Þ ∩h t2
ð Þ
½
 ̸ M h t1
ð Þ ∪h t2
ð Þ
½
,
ð5Þ
This deﬁnition satisﬁes the following: if terms t1 and t2 are synonymous, i.e., h
(t1) = h(t2), then s(t1,t2) = 1; if t1 and t2 are not semantically related, i.e., h(t1) ∩h
(t2) = ∅, then s(t1,t2) = 0; s(t2,t1) = s(t1,t2) for all t1,t2 ∈T; and if t1 is more
similar to term t3 than to t2, then s(t1,t3) > s(t1,t2). The fuzzy NT relation, indi-
cated as nt, which represents grades of inclusion of a narrower term t1 in another
(broader) term t2, is deﬁned as:
nt t1, t2
ð
Þ = M½h t1
ð Þ ∩h t2
ð Þ
̸ M

½h t1
ð Þ,
ð6Þ
This deﬁnition satisﬁes the following: if term t1’s concept(s) is completely
included within term t2’s concept(s), i.e. h(t1) ⊆h(t2), then nt(t1,t2) = 1; if t1 and
t2 are not semantically related, i.e., h(t1)∩h(t2) = ∅, then nt(t1,t2) = 0; and if the
inclusion of t1’s concept(s) in t2’s concept(s) is greater than the inclusion of t1’s
concept(s) in t3’s concept(s), then nt(t1,t2) > nt(t1,t3).
By assuming M as the cardinality of a set, s and nt are given as:
s t1, t2
ð
Þ = ∑M
k = 1min t1 ck
ð
Þ, t2 ck
ð
Þ
½
 ̸ ∑M
k = 1max t1 ck
ð
Þ, t2 ck
ð
Þ
½
,
ð7Þ
nt t1, t2
ð
Þ = ∑M
k = 1min t1 ck
ð
Þ, t2 ck
ð
Þ
½
 ̸ ∑M
k = 1t1 ck
ð
Þ,
ð8Þ
A fuzzy pseudothesaurus can be deﬁned by replacing the set C in the deﬁnition
of h(t) above with the set of documents D, with the assumption that h(t) is the fuzzy
set of documents indexed by term t. This yields
h tð Þ =
d, t d
ð Þ
ð
Þ j d ∈D
f
g,
ð9Þ
276
D.H. Kraft et al.

in which t(d) = F(d,t) is the index term weight deﬁned above. F can be either a
binary value deﬁning a crisp representation, or it can be a value in [0,1] to deﬁne a
fuzzy representation of documents. The fuzzy RT and the fuzzy NT relations now
are deﬁned as:
s t1, t2
ð
Þ = ∑M
k = 1min F t1, dk
ð
Þ, F t2, dk
ð
Þ
½
 ̸ ∑M
k = 1max F t1, dk
ð
Þ, F t2, dk
ð
Þ
½
,
ð10Þ
nt t1, t2
ð
Þ = ∑M
k = 1min F t1, dk
ð
Þ, F t2, dk
ð
Þ
½
 ̸ ∑M
k = 1F t1, dk
ð
Þ,
ð11Þ
Note that s(t1,t2) and nt(t1,t2) are dependent on the co-occurrences of terms t1 and
t2 in the set of documents, D. The set of index terms of document d, i.e., {t | F(d,t) ≠0
and t∊T}, can be augmented by those terms tA which have s(t,tA) > α and/or
nt(t,tA) > β for parameters α and β∈[0,1].
A thesaurus can be generated based on the max-star transitive closure for lin-
guistic completion of a thesaurus generated initially by an expert linking terms [4].
A probabilistic notion of term relationships can be employed by assuming that if
one given term is a good discriminator between relevant and non relevant docu-
ments, then any term that is closely associated with that given term (i.e., statistically
co-occurring) is likely to be a good discriminator, too [60]. Note that this implies
that thesauri are collection-dependent.
One can also expand on Salton’s [50] use of the F(d,t) values. Salton [51] infers
term relationships from document section similarities. On the other hand, one can
manipulate the F(d,t) values in order to generate co-occurrence statistics to repre-
sent term linkage weights [31]. Here, a synonym link is considered, deﬁned as:
μsynonym t1, t2
ð
Þ = ∑d ∈D½F d, t1
ð
Þ ↔F d, t2
ð
Þ,
ð12Þ
where F d, t1
ð
Þ ↔F d, t2
ð
Þ = min F d, t1
ð
Þ →F d, t2
ð
Þ, F d, t1
ð
Þ ←F d, t2
ð
Þ
½
 and F(d,t1)
F(d,t2) can be deﬁned in variety of ways. For instance, F(d,t1) F(d,t2), the implication
operator, can be deﬁned as ½F d, t1
ð
Þc ∨F d, t2
ð
Þ, where F d, t1
ð
Þc = 1 −F d, t1
ð
Þ is the
complement of F(d, t1) and ∨is the disjunctive (OR) operator deﬁned as the max; or
it can be deﬁned as min(1, 1 −F d, t1
ð
Þ + F d, t2
ð
Þ
½
). A narrower term link (where
term t1 is narrower than term t2, so term t2 is broader than term t1), is deﬁned as:
μnarrower t1, t2
ð
Þ = ∑d ∈D½F d, t1
ð
Þ →F d, t2
ð
Þ,
ð13Þ
Note that fuzzy narrower relationships deﬁned between fuzzy sets can help the
purpose of identifying generalization and specialization of topics, while the fuzzy
similarity relationship between fuzzy sets can be of aid to identify similar topics.
Fuzzy Information Retrieval Systems: A Historical Perspective
277

5.4 Fuzzy Clustering for Documents
Clustering in information retrieval is a method for partitioning D, a given set of
documents, into groups using a measure of similarity (or distance) which is deﬁned
on every pairs of documents. Grouping like documents together is not a new
phenomenon, especially for librarians. The similarity between documents in the
same group should be large, while the similarity between documents in different
groups should be small.
A common clustering method is based on the simultaneous occurrences of
citations in pairs of documents. Documents are clustered using a measure deﬁned
on the space of the citations. Generated clusters can then be used as an index for
information retrieval, i.e., documents which belong to the same clusters as the
documents directly indexed by the terms in the query are retrieved.
Similarity measures have been suggested empirically or heuristically, sometimes
analogously to the similarity measures for documents matched against queries [52,
54, 56]. When adopting a fuzzy set model, clustering can be formalized as a kind of
fuzzy association. In this case, the fuzzy association is deﬁned on the domain
D × D. By assuming R(d) to be the fuzzy set of terms representing a document d
with membership function values d(t) = F(d,t) being the index term weights of term
t in document d, the symmetric fuzzy relation s, as originally deﬁned above, is taken
to be the similarity measure for clustering documents:
s d1, d2
ð
Þ = ∑M
k = 1min d1 tk
ð Þ, d2 tk
ð Þ
½
 ̸ ∑M
k = 1max d1 tk
ð Þ, d2 tk
ð Þ
½
,
ð14Þ
nt d1, d2
ð
Þ = ∑M
k = 1min F tk, d1
ð
Þ, F tk, d2
ð
Þ
½
 ̸ ∑M
k = 1max F tk, d1
ð
Þ, F tk, d2
ð
Þ
½
,
ð15Þ
in which T is the set of index terms in the vocabulary and M is the number of
index terms in T.
In fuzzy clustering, documents can belong to more than one cluster with varying
degree of membership [3]. Each document is assigned a membership value to each
cluster. Modiﬁed fuzzy clustering, also called soft clustering, uses thresholding
mechanisms to limit the number of documents belonging to each cluster. The main
advantage of using modiﬁed fuzzy clustering is the fact that the degree of fuzziness
is controlled.
6
A Query Evaluation Mechanism
Query processing within retrieval can be interpreted as a decision-making activity.
Its aim is to evaluate a set of alternatives or possible solutions, in this case a set of
documents, based upon some criteria or selection conditions in order to select the
optimal list (perhaps ranked) of documents in response to a user’s query.
278
D.H. Kraft et al.

In a Boolean query, the alternatives are the document representations as
described based on the presence or absence of index terms or keywords. The
selection conditions, as expressed by terms speciﬁed in a query, deﬁne a set of
constraints requiring the presence or absence of these terms within a document’s
representation. These conditions are expressed connected by aggregation operators,
i.e., the Boolean logic operators of AND, OR, and NOT. The decision process is
performed through an exact matching function, which is strictly dependent on the
system query language
Given a fuzzy approach to retrieval, query processing can be regarded as a
decision activity affected by vagueness. The query can be seen as the speciﬁcation
of a set of soft constraints, i.e. vague selection conditions that the documents can
satisfy to a partial extent. The documents described through the signiﬁcance degrees
of the index terms constitute the alternatives. The query evaluation mechanism is
regarded as fuzzy decision process that evaluates the degree of satisfaction of the
query constraints by each document representation by applying a partial matching
function. This degree is the RSV and can be interpreted as the degree of relevance
of the document to the query and is used to rank the documents. Then, as a result of
a query evaluation, a fuzzy set of documents is retrieved in which the RSV is the
membership value. In this case the deﬁnition of the partial matching function is
strictly dependent on the query language, speciﬁcally on the semantics of the soft
constraints.
A wish list of requirements that a matching function of an IRS must satisfy has
been proposed [21, 61]. Included in this list is the separability property that the
evaluation of an atomic selection condition for an individual term in a query should
be independent of the evaluation of the other atomic components or their Boolean
connectors. The matching function should be based solely upon a function evalu-
ating atomic conditions. Following the calculation of these evaluations, one can
then aggregate them based upon the Boolean operators in the query. It has been
shown that this property guarantees a homomorphic mapping from the space of all
single terms to the space of all possible Boolean queries using these terms [1]. This
property has been considered widely within fuzzy retrieval models, especially in the
deﬁnition of ﬂexible query languages.
By designing the partial matching mechanism from the bottom-up the separa-
bility property is ensured. First, each atomic selection condition or soft constraint in
the query is evaluated by a function E for a given document. Then the aggregation
operators are applied to the results starting from the inmost operator in the query to
the outermost operator by a function E*. This E function evaluates the soft con-
straints associated with the query atoms on the fuzzy set Rd representing each
document, where these soft constraints are deﬁned as fuzzy subsets. The mem-
bership value μatom(i) is the degree of satisfaction of the soft constraint associated
with the atomic query atom, i.e., E(<atom>,d) = μatom(F(d,t)). In other words, E
evaluates how well the term t, which has an indexing weight F(d,t) for document d,
satisﬁes the soft constraint speciﬁed by atom. The result of the evaluation is a fuzzy
set, ∑d∈D µatom(F(d, t))/d in which μatom(F(d,t)) is interpreted as the RSV of
document d with respect to the query atom.
Fuzzy Information Retrieval Systems: A Historical Perspective
279

The function E*: D×Q →0, 1
½
, where Q is the set of all the proper queries in the
query language, evaluates the ﬁnal RSV of a document, reﬂecting the satisfaction of
the whole query. The deﬁnition of E* depends strictly upon the structure of the
query language, speciﬁcally upon the aggregation operators used to combine the
atomic components. The AND connective is classically deﬁned as the minimum
(min) operator, the OR connective as the maximum (max) operator, and the NOT
connective as the one-minus (1-) or complement operator. These deﬁnitions pre-
serve the idempotence property. A fuzzy generalization of the Boolean query
structure has been deﬁned in which the Boolean operators are replaced by linguistic
quantiﬁers [10]. In this context, linguistic quantiﬁers are used as aggregation
operators to determine the degree of satisfaction for the soft constraints. They allow
to improve as well as to simplify the expressiveness of the Boolean query language.
7
Query Weights
To render a Boolean query language to be more user friendly and more expressive,
one can extend the atomic selection conditions by introducing query term weights
[5, 7, 20, 47]. An example of weighted query is the following: <t1, w1> AND
(<t2, w2> OR <t3, w3>) in which t1, t2, t3, are search terms with numeric weights
w1, w2, and w3 in the interval [0,1]. These weights are implicitly given as being
equal to 1 in the classical Boolean query language.
The concept of query weights raises the problem of their interpretation. Several
authors have realized that the semantics of query weights should be related to the
concept of the “importance” of the terms. Being well aware that the semantics of the
query term weights inﬂuences the deﬁnition of the partial matching function,
speciﬁcally the E function, different semantics for the soft constraint imposed by a
pair <t,w> have been proposed in the literature trying to satisfy as much as possible
properties of the wish list in particular the separability property.
Early on, query weights were interpreted as a relative importance weight where
the separability property does not hold. Two distinct deﬁnitions of E have been
proposed for conjunctive and disjunctive queries, respectively [5, 63]. Later, other
models [20, 47, 61] used an interpretation of the query weights w as a threshold on
the index term weight or as an ideal index term weight [7, 22].
7.1 Implicit Query Weights
The simplest extension of the Boolean model consists of the adoption of a weighted
document representation with a classical Boolean query language [17]. This
retrieval mechanism ranks the retrieved documents in decreasing order of their
signiﬁcance with respect to the user query. In this case, an atomic query consisting
of a single term t is interpreted as the speciﬁcation of a pair/<t,1> in which w = 1
280
D.H. Kraft et al.

is implicitly speciﬁed. The soft constraint associated with <t,1> is then interpreted
as the requirement that the index term weight be “close to 1” and its evaluation is
deﬁned as μw F d, t
ð
Þ
ð
Þ = F d, t
ð
Þ. This means that the desired documents are those
with maximum index term weight for the speciﬁed term t, i.e., closest to 1. This
interpretation implies that the evaluation mechanism tolerates the satisfaction of the
soft constraint associated with <t,1> with a degree equal to F(d,t).
7.2 Relative Importance Query Weights
Here, query weights are interpreted as measures of the “relative importance” of each
term with respect to the other terms in the query [5, 63]. This interpretation allows
the IRS to rank documents so that documents are ranked higher if they have larger
index term weights for those terms that have larger query weights. However, since
it is not possible to have a single deﬁnition for the soft constraint μw that preserves
the “relative importance” semantics independently of the Boolean connectors in the
query, two distinct deﬁnitions of μw have been proposed, depending on the
aggregation operators in the query. This approach, sadly, gives up the separability
property. Two alternative deﬁnitions have been proposed for conjunctive and dis-
junctive queries [5, 63]. The ﬁrst proposal [5] yields μw F d, t
ð
Þ
ð
Þ = w * F d, t
ð
Þ
½
 for
disjunctive queries and μw F d, t
ð
Þ
ð
Þ = max 1, F d, t
ð
Þ=w
ð
Þ for conjunctive queries;
while the second proposal [56] yields μw F d, t
ð
Þ
ð
Þ = min w, F d, t
ð
Þ
½
 for disjunctive
queries and μw F d, t
ð
Þ
ð
Þ = max 1 −w
ð
Þ, F d, t
ð
Þ
½
 for conjunctive queries. Notice
that any weighted Boolean query can be expressed in disjunctive normal form
(DNF) so that any query can be evaluated by using one of these two deﬁnitions.
7.3 Threshold Query Weights
To preserve the separability property, an approach treating the query weights as
thresholds has been suggested [20, 47]. By specifying query weights as thresholds
the user is asking to see all documents “sufﬁciently about” a topic. In this case, the
soft constraint identiﬁed by the numeric query weight can be linguistically
expressed as “more or less over w”. Of course, the lower the threshold, the greater
the number of documents retrieved. Thus, a threshold allows a user to deﬁne a point
of discrimination between under- and over satisfaction.
The simplest formalization of threshold weights has been suggested as a crisp
threshold [47].
μw F d, t
ð
Þ
ð
Þ =
0
for
Fðd, tÞ < w
F d, t
ð
Þ
for
Fðd, tÞ ≥w
(
,
ð16Þ
Fuzzy Information Retrieval Systems: A Historical Perspective
281

In this case, the threshold deﬁnes the minimally acceptable document. Due to its
inherent discontinuity, this formalization might lead to an abrupt variation in the
number of documents retrieved for small changes in the query weights. To remedy
this, continuous threshold formalization has been suggested [20]:
μw F d, t
ð
Þ
ð
Þ =
PðwÞ* F(d, t)
w
for
Fðd, tÞ < w
PðwÞ + QðwÞ* ðF(d, t) −wÞ
ð1 −wÞ
for
Fðd, tÞ ≥w
8
<
:
. ,
ð17Þ
where P(w) and Q(w) might be deﬁned as PðwÞ = 1 +w
2
and Q(w) =1 −w2
4
. For
F(d,t) < w, the μw function measures the closeness of F(d,t) to w; for F(d,t) ≥w,
μw(F(d,t)) expresses the degree of over satisfaction with respect to w, and under
satisfaction with respect to 1.
7.4 Ideal Query Weights
Another interpretation for the query weights has been deﬁned [7, 22]. Here, the
pair <t,w> identiﬁes a set of ideal or perfect documents so that the soft constraint
μw measures how well F(d,t) comes close to w, yielding
μw F d, t
ð
Þ
ð
Þ = elnðkÞ*ðFðd, tÞ −wÞ2,
ð18Þ
The parameter k in the interval [0,1] determines the steepness of the Gaussian
function’s slopes. As a consequence, k will affect the strength of the soft constraint
“close to w”. So, the larger the value of k is, the weaker the constraint becomes.
This parametric deﬁnition makes it possible to adapt the constraint interpretation to
the user concept of “close to w” [7]. The retrieval operation associated with a
pair <t,w> corresponds in this model to the evaluation of a similarity measure
between the importance value w and the signiﬁcance value of t in Rd: w ≈F(d,t).
7.5 Linguistic Query Weights
The main limitation of numeric query weights is their inadequacy in dealing with
the imprecision which characterizes the concept of importance that they represent.
In fact, the use of numeric query weights forces the user to quantify a qualitative
and rather vague notion and to be aware of the weight semantics. Thus, a fuzzy
retrieval model with linguistic query weights has been proposed [12] with a lin-
guistic extension of the Boolean query language based upon the concept of a
linguistic variable [67]. With this approach, the user can select the primary lin-
guistic term “important” together with linguistic hedges (e.g., “very” or “almost”)
282
D.H. Kraft et al.

to qualify the desired importance of the search terms in the query. When deﬁning
such a query language the term set, i.e., the set of all the possible linguistic values
of the linguistic variable importance, must be deﬁned. Such a deﬁnition depends on
the desired granularity that one wants to achieve. The greater the number of the
linguistic terms, the ﬁner the granularity of the concepts that are dealt with. Next,
the semantics for the primary terms must be deﬁned. A pair <t, important> ,
expresses a soft constraint μimportant on the term signiﬁcance values (the F(d,t)
values). The evaluation of the relevance of a given document d to a query consisting
solely of the pair <t, important> is based upon the evaluation of the degree of
satisfaction of the associated soft constraint μimportant.
The problem of giving a meaning to numeric weights reappears here in asso-
ciating a semantic with the linguistic term important. The μimportant function is
deﬁned based on the ideal semantics of the numeric weight to yield [12].
μimportantðFðd, tÞÞ =
elnðkÞ*ðFðd, tÞ −iÞ2
for
F(d, t) < i
1
for
i ≤F(d, t) ≤j
elnðkÞ*ðFðd, tÞ −jÞ2
for
F(d, t) > j
8
<
:
. ,
ð19Þ
We see that if F(d,t) is less than the lower bound i or greater than the upper
bound j, the constraint is under satisﬁed. The strength of the soft constraint μimportant
depends upon both the width of the range [i, j] and the value of the k parameter. The
values i and j delimit the level of importance for the user. We note that as the value
|i - j| increases, the soft constraint becomes less precise. So, for the case of the ideal
semantics of numeric query term weights, k determines the sharpness of the con-
straint in that ask increases, the constraint increases in fuzziness.
We can deﬁne the μimportant function based upon the threshold semantics to yield
[34]
μimportantðF(d, t)Þ =
1 +i
2 *elnðkÞ*ðFðd, tÞ −iÞ2
for
F(d, t) < i
1 +F(d, t)
2
for
i ≤F(d, t) ≤j
1 +j
2 * 1 + F(d, t) −j
2


for
F(d, t) > j
,
8
>
>
<
>
>
:
.
ð20Þ
We note that this compatibility function is continuous and non-decreasing in F(d,t)
over the interval [0,1]. For F(d,t) < i, μimportant increases as a Gaussian function. For F
(d,t) in the interval [i,j], μimportant increases at a linear rate. For F(d,t) > j, μimportant
still increases, but at a lesser rate. The compatibility functions of non-primary terms,
such as very important or fairly important, are derived by modifying the compatibility
functions of primary terms. This is achieved by deﬁning each linguistic hedge as a
modiﬁer operator. For example, the linguistic hedges are deﬁned as translation
operators in [34] to yield:
μvery important (x) = μimportant (x) with ivery = i + 0.2 and jvery = j +0.2 and ∀x ∈
[0,1].
Fuzzy Information Retrieval Systems: A Historical Perspective
283

μaveragely important (x) = μimportant (x) with iaveragely = i - 0.3 and javeragely = j - 0.3
and ∀x ∈[0,1].
μminimally important (x) = μimportant (x) with iminimally = i - 0.5 and jminimally = j - 0.5
and ∀x ∈[0,1],
in which i and j are values in [0,1] delimiting the range of complete satisfaction of
the constraint μimportant. With these deﬁnitions, any value F(d,t) of the basic domain
of the importance variable fully satisﬁes at least one of the constraints deﬁned by
the linguistic query terms. In [30] a query language with linguistic query weights
having heterogeneous semantics have been proposed so as to beneﬁt the full
potential offered of a fuzzy set to model subjective needs.
8
Linguistic Quantiﬁers to Aggregate the Selection
Conditions
In a classical Boolean query language, the AND and OR connectives allow only for
crisp (non-fuzzy) aggregations which do not capture any of the inherent vagueness
of user information needs. For example, the AND used for aggregating M selection
conditions does not tolerate the no satisfaction of but a single condition which could
cause the no retrieval of relevant documents. To deal with this problem, additional
extensions of Boolean queries have been provided which involves the replacement
of the AND and OR connectives with soft operators for aggregating the selection
criteria [46, 54, 55].
Within the framework of fuzzy set theory, a generalization of the Boolean query
language has been deﬁned based upon the concept of linguistic quantiﬁers that are
employed to specify both crisp and vague aggregation criteria of the selection
conditions [10]. New aggregation operators can be speciﬁed by linguistic expres-
sions with self-expressive meaning, such as at least k and most of. They are deﬁned
to exist between the two extremes corresponding to the AND and OR connectives,
which allow requests for all and at least one of the selection conditions, respec-
tively. The linguistic quantiﬁers used as aggregation operators, are deﬁned by
ordered weighted averaging (OWA) operators.
Adopting linguistic quantiﬁers more easily and intuitively formulate the
requirements of a complex Boolean query. A quantiﬁed aggregation function can be
applied not only to single selection conditions, but also to other quantiﬁed expres-
sions. Then, the E* function evaluating the entire query yields a value in [0,1] for
each document d in the archive D. If S is the set of atomic selection conditions and Q
is the set of legitimate Boolean queries over our vocabulary of terms, then the E*
function can be formalized by recursively applying the following rules:
• if q∊S then E * (d, s) = μw(F(d, t)) in which μw(F(d,t)) is the satisfaction degree
of a pair <t,w> by document d with w being either a numeric weight or a
linguistic weight.
284
D.H. Kraft et al.

• if q = quantiﬁer (q1,…,qn) and q1,…,qn ∈Q then E*(d,q) = OWA quantiﬁer(E*
(d,q1),…, E * (d, qn))E * (d, NOTq) = 1 - E * (d, q) in which OWA quantiﬁer is
the OWA operator associated with quantiﬁer.
The formal deﬁnition of the query language with linguistic quantiﬁers with the
following quantiﬁes has been generated [10]
• all replaces AND;
• at least k acts as the speciﬁcation of a crisp threshold of value k on the number
of selection conditions and is deﬁned by a weighting vector wat least k in which
wk = 1, and wj = 0, for i ≤k – noting that at least 1 selects the maximum of the
satisfaction degrees so that it has the same semantics of OR;
• about k is a soft interpretation of the quantiﬁer at least k in which the k value is
not interpreted as a crisp threshold, but as a fuzzy one so that the user is fully
satisﬁed if k or more conditions are satisﬁed but gets a certain degree of sat-
isfaction even if k-1, k-2,…,1 conditions are satisﬁed - this quantiﬁer is deﬁned
by a weighting vector wabout k in which wi =
i
∑k
j = 1j for i ≤k, and wi = 0 for i > k;
• most is deﬁned as a synonym of at least 2
3 n in which n is the total number of
selection conditions.
With respect to non-fuzzy approaches that tried to simplify the Boolean for-
mulations, the fuzzy approach subsumes the Boolean language, allows reformu-
lating Boolean queries in a more synthetic and comprehensible way, and improves
the Boolean expressiveness by allowing ﬂexible aggregations. Other authors have
followed these ideas by proposing alternative formalization of linguistic query
weights and ﬂexible operators based on ordinal labels and ordinal aggregations
[29], thus reducing the complexity of the evaluation mechanism.
9
Emerging Applications of Fuzzy Set Theory to Model
Information Retrieval Tasks
9.1 Geographic Information Retrieval
An emerging task in information retrieval is retrieving documents relevant with
respect to both a content based condition and a geographic condition.
Such applications involve the management of uncertainty and imprecision and
the modeling of user preferences and context. Indexing the geographic content of
documents implies dealing with the ambiguity, synonymy and homonymy of
geographic names in texts. On the other side, the evaluation of queries specifying
both content based conditions and spatial conditions on documents contents
requires representing the vagueness and context dependency of spatial conditions
and the personal user’s preferences. The spatial condition can be speciﬁed lin-
guistically in the query through vague terms such as “close to the North East of
Milan”, whose semantic depends on the user’s context and perception of distance.
Fuzzy Information Retrieval Systems: A Historical Perspective
285

In [25] a geographic information retrieval model has been deﬁned that represents
both the uncertainty in indexing the geographic documents’ content and the user’s
context and preferences in evaluating ﬂexible spatial queries.
Finally, the system allows evaluating two types of queries ﬂexibly combining
the content based condition, such as “vegetarian Restaurants” with the spatial
condition “close to Milano Central Ralway station”. The spatial condition “close”
is deﬁned as a soft constraint on the user’s perceived distance between the docu-
ments’ footprint and query’s footprint [8].
For each retrieved document, two relevance scores are computed with respect to
the two query conditions that are ﬂexibly combined to generate an overall ranked
list of documents. The user can choose the semantic for the combination, that can
be either an asymmetric “and possibly” aggregation between the mandatory content
condition and the optional spatial condition, or a compensative “average” aggre-
gation, deﬁned as a linear combination of the two conditions; further, a relative
preference between the conditions can be speciﬁed to achieve personalization and
effectiveness.
9.2 Aggregation of Multi Dimensional Relevance Dimensions
Relevance assessment is usually based on the evaluation of multiple criteria, also
called relevance dimensions, which are aimed to capture different aspects or
properties of the considered document or document/user context. All the considered
dimensions concur to estimate the utility of a document with respect to the con-
sidered user’s query. The concept of page popularity in search engines is an
example of a relevance dimension that is usefully exploited in the process of
documents’ relevance estimate. In the multidimensional relevance assessment each
dimension is usually evaluated in an independent way, and a numeric score is
associated with each dimension for each document. To obtain an overall relevance
score the single scores will get aggregated into an overall score representing the
document’s RSV.
Among the aggregation operators, traditional non-compensatory operators, such
as the min and the max operator allow to set up a pessimistic (e.g. min, as an example
of T-norm operator) or optimistic (e.g. max, as an example of T-conorm operators)
aggregation scheme, while traditional averaging aggregation operators are totally
compensatory, i.e., a lack in the satisfaction of a criterion can be compensated by the
surplus satisfaction of another one. This property is not very realistic in many real
applications in general, and in particular in Information Retrieval (IR). In [23–25]
two prioritized aggregation operators for multidimensional relevance assessment
have been proposed (the scoring and the prioritized and operators), and they have
been evaluated by a user-centered approach that has been conducted in a person-
alized IR setting, where four relevance dimensions have been considered (aboutness
(or topicality), coverage, appropriateness and reliability). An interesting aspect in
considering a personalized IR setting to evaluate the prioritized aggregation is that
286
D.H. Kraft et al.

the priority over the considered relevance dimensions may be user dependent; by
setting different priority orders over the four considered relevance dimensions,
different types of users’ can be identiﬁed, with distinct search intents. The main
impact in making the prioritized aggregation scheme user dependent is that for a
same query and a same user different document rankings can be obtained.
9.3 Discovery of Similar Contents in Web Pages Retrieved
by Multiple Queries
Another recent application of fuzzy set in information retrieval is related with the
task of organizing and discovering the contents of the Web pages retrieved by
several query reformulations of the same information need.
It may often happen that by reformulating a query by slightly changing some
words or adding new terms to a previous query new and already retrieved docu-
ments are represented in the list of results.
In order to discover the common contents retrieved by two or more queries and
select only diversiﬁed contents by eliminating near duplicates an approach deﬁned
in [6] consists in applying soft operators to distinct lists of Web pages retrieved by
either the same query submitted to distinct search engines or similar queries sub-
mitted to the same search engine.
The approach proposes [6] the soft ranked intersection to generate from two lists
of web pages retrieved by two distinct searches a cluster of Web pages with similar
contents, and the soft ranked union to generate a cluster of Web pages with diver-
siﬁed contents. The soft operators work on the representation of the Web pages
provided by information granules consisting of the Web pages titles, url string, and
snippets displayed in the result page, thus without the need to access the Web page
content so as to achieve efﬁciency. The soft operators allow users to discover and
reveal the hidden shared topics retrieved by multiple Web searches, possibly iden-
tifying near duplicates and selecting Web pages with diversiﬁed contents.
9.4 Ememe Identiﬁcation and Tracking
A very up to date task in information retrieval is the identiﬁcation and tracking of
the evolution of Internet Meme, hereafter named ememe, intended as a unit of
information (idea) replicated and propagated through the Web by one or more
applications such as social networks and blogs. The Web constitutes a huge
information repository, and several Internet based-services and applications repre-
sent a quite rich soil for memes’ growth and evolution. Like blogs, e-mails, and
social network applications.
The conceptual notion of ememe has been intended in a simple form as a replicated
or paraphrased sentence by the scientiﬁc literature on meme tracking. The ﬁrst attempt
Fuzzy Information Retrieval Systems: A Historical Perspective
287

that considers an ememe not as merely a sequence of words or a quoted sentence but as
a set of interrelated concepts has been proposed in [6] where a method based on the
deﬁnition of an ememe by an OWL schema, and a process for retrieving ememe
instances and fusing them by means of Ordered Weighted Averaging Operators has
been proposed. Speciﬁcally, by taking inspiration from the anthropologic literature on
memes a methodology to formally deﬁne, identify, revise and measure some char-
acteristic properties of ememes on the Blogosphere was deﬁned.
The proposed method for identifying ememes on the Blogosphere needs a
preliminary user-system interaction [13]; the user is in fact asked to provide an “a
priori” core deﬁnition of the ememe that he/she wants to identify; the core deﬁnition
can be speciﬁed by a conceptual schema expressed in OWL, and it is provided in
input to a pull mechanism that will search the candidate instances of the given
ememe into one or several source repositories of the blogosphere; the blogs to be
analysed can be also speciﬁed by the user, and if not provided the system will
search on the whole blogosphere.
Once the OWL core deﬁnition has been completed, some textual queries are
automatically generated and submitted to a search engine that will inquiry the
blogosphere to retrieve blogs’ posts that potentially contain ememe instances.
Finally, a ﬁltering process is deﬁned, which takes in input the retrieved blogs’
posts and selects those containing the ememe instances which will be used in the
phase aimed at revising and enriching the original core deﬁnition of the ememe.
The aim of the ﬁltering process is to identify the reliable ememe instances among
the results produced by the search process. The overall satisfaction value obtained
by the aggregation operator is then used to ﬁlter the ﬁrst M relevant ememe
instances. The evaluation function associated with the linguistic quantiﬁer some is
deﬁned by an Ordered Weighted Averaging Operator.
10
Fuzzy Performance Measures
Major factors in evaluating retrieval performance include the cost, time, and effort
to retrieve relevant items, as well as user satisfaction. The standard measures of
effectiveness include recall, the proportion of relevant documents retrieved (related
to 1-α or 1-Type I error in classical statistics the proportion of retrieved documents
that are relevant) and precision, the proportion of retrieved documents that are
relevant (1-β or 1-Type II error in classical statistics). Combining recall and pre-
cision can yield
E = 1 −1 ̸ ½αP −1 + ð1 −αR −1Þ or F = ð1 + αÞ*P*R ̸ ðαP + RÞ,
ð21Þ
where α is a user speciﬁed parameter. Other measures include G = generality
= proportion of documents that are relevant (one very weak rule based on the
notion that the act of retrieval should convey relevance information is that
precision ≥generality), and Fa = fallout = proportion of non relevant documents
288
D.H. Kraft et al.

retrieved. Another measure is S = 1=2
ð
Þ 1 + S + + S −
ð
Þ=Smax
½
 for ordering, where
S+ = number of pairs in ranked list where order is correct; S−= number of pairs in
ranked list where order is correct; Smax = maximal value of S+.
One problem with current criteria to measure the effectiveness of IR systems is
the fact that recall and precision measures have been deﬁned by assuming that
relevance is a Boolean concept. In order to take into account the fact that IR systems
rank the retrieved documents based on their RSVs that are interpreted either as a
probabilities of relevance, similarity degrees of the documents to the query, or as
degrees of relevance, Recall-Precision graphs are produced in which the values of
precision are computed at standard levels of recall. Then, the average of the precision
values at different recall levels is computed to produce a single estimate.
Nevertheless, these measures do not evaluate the actual values of the RSVs
associated with documents and do not take into account the fact that also users can
consider relevance as a gradual concept. For this reason some authors have pro-
posed some fuzzy measure of effectiveness. [19] proposed the evaluation of fuzzy
recall and fuzzy precision, deﬁned as follows:
Fuzzy Precision = ∑dminðed, udÞ
∑ded
,
ð22Þ
Fuzzy Recall = ∑dminðed, udÞ
∑dud
,
ð23Þ
where ud is the user’s evaluation of the relevance of document d (ud can be
binary or deﬁned in the interval [0,1]) and ed is the RSV of document d computer
by the IR system. These measures take into account the actual values of ed and ud,
rather than the rank ordering based in descending order on ed.
These measures can be particularly useful to evaluate the results of fuzzy clus-
tering algorithms.
11
Experimental Results
A comparison of the results produced by using the traditional fuzzy representation of
documents and the fuzzy representation of structured documents can be found in
[10]. In this experiment, a collection of 2500 textual documents about descriptions of
CNR research projects has been considered. The indexing module of the prototypal
information retrieval system named DOMINO, used for the experiment, has been
extended in order to be able to recognize in the documents any structure simply by
specifying it into a deﬁnition ﬁle. In this way it is not necessary to modify the system
when dealing with a new collection of documents with a different structure. The
deﬁnition of the documents sections has been made before starting the archive
generation phase. During this phase it was also necessary to specify the criteria by
which to compute the signiﬁcance degrees of the terms in each section. Two kinds of
Fuzzy Information Retrieval Systems: A Historical Perspective
289

sections have been identiﬁed: the “structured” sections, i.e., the research code, title,
research leader, and the “narrative” sections, containing unstructured textual
descriptions, i.e., the project description and the project objective. It has been
observed that while the values of precision remain unchanged in the two versions of
the system, the values of recall are higher by using the structured representation than
those obtained by using the traditional fuzzy representation.
We illustrate another approach which produces a weighted representation of
documents written in HTML [41]. An HTML document has a speciﬁc syntactic
structure in which its subparts have a given format speciﬁed by the delimiting tags.
In this context, tags are seen as syntactic elements carrying an indication of the
importance of the associated text. When writing a document in HTML, an author
associates varying importance to each of the different subparts of a given document
by delimiting them by means of appropriate tags. Since a certain tag can be
employed more than once, and in different positions inside the document, the
concept of document subpart is not meant as a unique, adjacent piece of text. Such a
structure is subjective and carries the interpretation of the document author. It can
be applied in archives, which collect heterogeneous documents, i.e. documents with
possibly different “logical” structures.
An indexing function has been proposed which provides different weights for the
occurrences of a given term in the document, depending on the tags by which they
are delimited [41]. The overall signiﬁcance degree F(d,t) of a term t in a document d
is computed by ﬁrst evaluating the term signiﬁcance in the different document tags,
and then by aggregating these contributions. With each
tag, a function
Ftag: D×T →0, 1
½
 is associated together an importance weight μtag ∈0, 1
½
. Note
that the greater the emphasis of the text associated with a tag, the greater its
importance weight. A possible ranking of the considered tags has been suggested
[41] in decreasing order of tag importance. The deﬁnition of such a list is quite
subjective, although based on objective assumptions suggested by commonsense.
These rankings include notion such as a larger font, or text in boldface or italics or
appearing in a list can be assumed as having a higher importance.
To simplify the hierarchy of the tags, we see that certain tags can be employed to
accomplish similar aims, so one can group them into different classes. It is assumed
that the members of a class have the same importance weight. Text not delimited by
any tag is included into the lowest class. A simple procedure to compute numeric
importance weights starting from the proposed ranking can be achieved. The def-
inition of Ftag follows the same mechanism as the previous approach [10].
Once the single signiﬁcance degrees of a term into the tags have been computed,
these have to be aggregated in order to produce an overall signiﬁcance degree of the
term into the document. In the aggregation all the signiﬁcance degrees should be
taken into account, so as to consider the contribution of each tag, modulated by
their importance weights. To this aim a weighted mean can be adopted:
A Ftag 1 d, t
ð
Þ, . . . Ftag n d, t
ð
Þ


= ∑i = 1. . nFtag i d, t
ð
Þ*wi
in
which
∑i = 1. . nwi = 1.
Starting from the list of tags in decreasing relative order of their importance, the
numeric weights wi are computed through a simple procedure. Assuming that tagi is
290
D.H. Kraft et al.

more important than tagj iff i < j (being i and j the positions of tagi and tagj
respectively in the ordered list), the numeric importance weight wi associated with
tagi can be computed as: wi = n −i + 1
ð
Þ=∑i = 1. . ni . In the computation of the
overall signiﬁcance degree F(d,t), the inverse document frequency of term t could
be taken into account (the deﬁnition of g(IDFt) is given in formula (2)):
F d, t
ð
Þ = ð∑i = 1...nFtag i d, t
ð
Þ*g IDFt
ð
Þ,
ð24Þ
12
Conclusions
This entry reviews the main objectives and characteristics of the fuzzy modeling of
the information retrieval activity with respect to alternative approaches such as
probabilistic IR and Vector space IR. The focus of the fuzzy approaches is on
modeling imprecision and vagueness of the information with respect to uncertainty.
The fuzzy generalizations of the Boolean Retrieval model have been discussed by
describing the fuzzy indexing of structured documents, the deﬁnition of ﬂexible
query languages subsuming the Boolean language, and the deﬁnition of fuzzy
associations to expand either the indexes or the queries, or to generate fuzzy clusters
of documents. Fuzzy similarity and fuzzy inclusion relationships between fuzzy sets
have been introduced that can help to deﬁne more evolved fuzzy IR models per-
forming “semantic” matching of documents and queries, which is the current trend
of research in Information retrieval.
References
1. Bartschi, M.: Requirements for query evaluation in weighted information retrieval. Inf.
Process. Manage. 21(4), 291–303 (1985)
2. Berrut, C., Chiaramella, Y.: Indexing medical reports in a multimedia environment: the RIME
experimental approach, pp. 187–197. ACM-SIGIR 89, Boston (1986)
3. Bezdek, J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Plenum Press,
New York (1981)
4. Bezdek, J.C., Biswas, G., Huang, L.Y.: Transitive closures of fuzzy thesauri for information-
retrieval systems. Int. J. Man Mach. Stud. 25(3), 343–356 (1986)
5. Bookstein, A.: Fuzzy requests: an approach to weighted Boolean searches. J. Am. Soc. Inf.
Sci. 31(4), 240–247 (1980)
6. Bordogna, G., Campi, A., Psaila, G., Ronchi, S.: Disambiguated query suggestions and
personalized content-similarity & novelty ranking of clustered results to optimize web
searches. Inf. Process. Manage. 201(48), 419–437 (2012)
7. Bordogna, G., Ghisalberti, G., Psaila, G.: Geographic information retrieval: modeling
uncertainty of user’s context. Fuzzy Sets Syst. 196(1), 105–124 (2012)
Fuzzy Information Retrieval Systems: A Historical Perspective
291

8. Bordogna, G., Pasi, G.: The application of fuzzy set theory to model information retrieval. In:
Crestani, F., Pasi, G. (eds.) Soft Computing in Information Retrieval: Techniques and
Applications. Physica-Verlag (2000)
9. Bordogna, G., Pasi, G.: Linguistic aggregation operators in fuzzy information retrieval. Int.
J. Intell. syst. 10(2), 233–248 (1995)
10. Bordogna, G., Pasi, G.: Controlling
information
retrieval through a user adaptive
representation of documents. Int. J. Approximate Reasoning 12, 317–339 (1995)
11. Bordogna, G., Pasi, G.: A fuzzy linguistic approach generalizing Boolean information
retrieval: a model and its evaluation. J. Am. Soc. Inf. Sci. 44(2), 70–82 (1993)
12. Bordogna, G., Pasi, G.: An approach to identify ememes on the blogosphere. In: Proceedings of
the Workshop NLPOE2012 held in collaboration with the 2012 IEEE/WIC/ACM International
Conference on Web Intelligence and Intelligent Agent Technology, Macao, 5-10.12-(2012)
13. Bordogna, G., Pasi, G., Yager, R.: Soft approaches to information retrieval on the WEB. Int.
J. Approximate Reasoning 34, 105–120 (2003)
14. Bosc, P.: Fuzzy databases, in Fuzzy sets in approximate reasoning and information systems.
In: Bezdek, J., Dubois, D., Prade, H. (eds.) The Handbooks of Fuzzy Sets Series, Kluwer
Academic Publishers (1999)
15. Buell, D.A.: A problem in information retrieval with fuzzy sets. J. Am. Soc. Inf. Sci. 36(6),
398–401 (1985)
16. Buell, D.A.: An analysis of some fuzzy subset applications to information retrieval systems.
Fuzzy Sets Syst. 7(1), 35–42 (1982)
17. Buell, D.A., Kraft, D.H.: Performance measurement in a fuzzy retrieval environment. In:
Proceedings of the Fourth International Conference on Information Storage and Retrieval,
pp. 56–62. ACM/SIGIR Forum, Oakland, CA, 16(1), 31 May–2 June, 1981
18. Buell, D.A., Kraft, D.H.: A model for a weighted retrieval system. J. Am. Soc. Inf. Sci. 32(3),
211–216 (1981)
19. Buell, D.A., Kraft, D.H.: Threshold values and Boolean retrieval systems. Inf. Process.
Manage. 17, 127–136 (1981)
20. Cater, S.C., Kraft, D.H.: A generalization and clariﬁcation of the Waller-Kraft wish-list. Inf.
Process. Manage. 25, 15–25 (19R 89)
21. Cater, S.C., Kraft, D.H.: TIRS: a topological information retrieval system satisfying the
requirements of the Waller-Kraft wish list. In: Proceedings of the Tenth Annual ACM/SIGIR
International
Conference
on
Research
and
Development
in
Information
Retrieval,
pp. 171–180. New Orleans, LA, June 1987
22. da Costa Pereira, C., Dragoni, M., Pasi, G.: Multidimensional relevance: a new aggregation
criterion. In: Boughanem, M., Berrut, C., Mothe, J., Soulé-Dupuy, C. (eds.) ECIR 2009.
Lecture Notes in Computer Science, vol. 5478, pp. 264–275. Springer (2009)
23. da Costa Pereira, C., Dragoni, M., Pasi, G.: A prioritized and aggregation operator for
multidimensional relevance assessment. In: Serra, R., Cucchiara, R. (eds.) AI*IA. Lecture
Notes in Computer Science, vol. 5883, pp. 72–81. Springer (2009)
24. da Costa Pereira, C.,
Dragoni, M., Pasi, G.: Multidimensional relevance: prioritized
aggregation in a personalized information retrieval setting. Inf. Process. Manage. 48(2),
340–357 (2012)
25. Crestani, F., Lalmas, M., van Rijsbergen, C.J., Campbell, I.: Is this document relevant? …
Probably. ACM Comput. Surv. 30(4), 528–552 (1998)
26. Dubois, D., Prade, H.: Possibility Theory: An Approach to Computerized Processing of
Uncertainty. Plenum Press, New York (1988)
292
D.H. Kraft et al.

27. Fuhr, N.: Models for retrieval with probabilistic indexing. Inf. Process. Manage. 25(1), 55–72
(1989)
28. Herrera, A., Herrera-Viedma, E.: Aggregation operators for linguistic weighted information.
IEEE Trans. Syst. Man Cybern. Part A Syst. Hum. 27(5), 646–656 (1997)
29. Herrera-Viedma, E., Lopez-Herrera, A.G.: A model of an information retrieval system with
unbalanced fuzzy linguistic information. Int. J. Intell. Syst. 22(11), 1197–1214 (2007)
30. Kohout, L.J., Keravanou, E., Bandler, W.: Information retrieval system using fuzzy relational
products for thesaurus construction. In: Proceedings IFAC Fuzzy Information, pp. 7–13.
Marseille, France (1983)
31. Kraft, D.H.: Advances in information retrieval: where is that /#*%@^ record? In: Yovits, M.
(ed.) Advances in Computers, vol. 24, pp. 277–318. Academic Press, New York (1985)
32. Kraft, D.H., Bordogna, G., Pasi, G.: Fuzzy set techniques in information retrieval. In: Bezdek, J.
C., Dubois, D., Prade, H. (eds.) The Handbooks of Fuzzy Sets Series. Fuzzy Sets in Approximate
Reasoning and Information Systems, pp. 469–510. Kluwer Academic Publishers (1999)
33. Kraft, D.H., Bordogna, G., Pasi, G.: An extended fuzzy linguistic approach to generalize
Boolean information retrieval. J. Inf. Sci. Appl. 2(3), 119–134 (1995)
34. Lucarella,D.,Morara,R.:FIRST:fuzzyinformationretrievalsystem.J.Inf.Sci.17(2),81–91(1991)
35. Lucarella, D., Zanzi, A.: Information retrieval from hypertext: an approach using plausible
inference. Inf. Process. Manage. 29(1), 299–312 (1993)
36. Miyamoto, S.: Fuzzy Sets in Information Retrieval and Cluster Analysis. Kluwer Academic
Publishers (1990)
37. Miyamoto, S.: Information retrieval based on fuzzy associations. Fuzzy Sets Syst. 38(2),
191–205 (1990)
38. Miyamoto, S.: Two approaches for information retrieval through fuzzy associations. IEEE
Trans. Syst. Man Cybern. 19(1), 123–130 (1989)
39. Miyamoto, S., Nakayama, K.: Fuzzy information retrieval based on a fuzzy pseudothesaurus.
IEEE Trans. Syst. Man Cybern. SMC-16(2), 278–282 (1986)
40. Molinari, A., Pasi, G.: A fuzzy representation of HTML documents for information retrieval
systems. In: Proceedings of the IEEE International Conference on Fuzzy Systems, vol. 1,
pp. 107–112. New Orleans, U.S.A., 8–12 September 1996
41. Motro, A.: Imprecision and uncertainty in database systems. In: Bosc, P., Kacprzyk, J. (eds.)
Fuzziness in Database Management Systems, pp. 3–22. Physica-Verlag, Heidelberg (1995)
42. Murai, T., Miyakoshi, M., Shimbo, M.A.: fuzzy document retrieval method based on two-
valued indexing. Fuzzy Sets Syst. 30(2), 103–120 (1989)
43. Neuwirth, E., Reisinger, L.: Dissimilarity and distance coefﬁcients in automation-supported
thesauri. Inf. Syst. 7(1), 47–52 (1982)
44. Nomoto, K., Wakayama, S., Kirimoto, T., Kondo, M.: A fuzzy retrieval system based on
citation. Syst. Control 31(10), 748–755 (1987)
45. Paice, C.D.: Soft evaluation of Boolean search queries in information retrieval systems. Inf.
Technol. Res. Dev. Appl. 3(1), 33–41 (1984)
46. Radecki, T.: Fuzzy set theoretical approach to document retrieval. Inf. Process. Manage. 15(5),
247–260 (1979)
47. Radecki, T.: Mathematical model of information retrieval system based on the concept of
fuzzy thesaurus. Inf. Process. Manage. 12(5), 313–318 (1976)
48. Reisinger, L.: On fuzzy thesauri. In: Bruckman, G., et al. (eds.) COMPSTAT 1974,
pp. 119–127. Physica Verlag, Vienna (1974)
49. Salton, G.: Automatic Text Processing: The Transformation, Analysis And Retrieval of
Information by Computer. Addison Wesley (1989)
Fuzzy Information Retrieval Systems: A Historical Perspective
293

50. Salton, G., Allan, J., Buckley, C., Singhal, A.: Automatic analysis, theme generation, and
summarization of machine-readable texts. Science 264, 1421–1426 (1994)
51. Salton, G., Bergmark, D.: A citation study of computer science literature. IEEE Trans. Prof.
Commun. 22(3), 146–158 (1979)
52. Salton, G., Buckley, C.: Term weighting approaches in automatic text retrieval. Inf. Process.
Manage. 24(5), 513–523 (1988)
53. Salton, G., McGill, M.J.: Introduction to Modern Information Retrieval. McGraw-Hill, New
York (1983)
54. Sanchez, E.: Importance in Knowledge Systems. Inf. Syst. 14(6), 455–464 (1989)
55. Sanchez, S.N., Triantaphyllou, E., Kraft, D.H.: A feature mining based approach for the
classiﬁcationoftextdocumentsintodisjointclasses.Inf.Process.Manage.38(4),583–604(2002)
56. Sparck
Jones,
K.A.:
Automatic
Keyword
Classiﬁcation
for
Information
Retrieval.
Butterworths, London (1971)
57. Sparck Jones, K.A.: A statistical interpretation of term speciﬁcity and its application in
retrieval. J. Documentation 28(1), 11–20 (1972)
58. Tahani, V.: A fuzzy model of document retrieval systems. Inf. Process. Manage. 12(3),
177–187 (1976)
59. van Rijsbergen, C.J.: Information Retrieval. Butterworths & Co. Ltd, London (1979)
60. Waller, W.G., Kraft, D.H.: A mathematical model of a weighted Boolean retrieval system. Inf.
Process. Manage. 15, 235–245 (1979)
61. Yager, R.R.: On ordered weighted averaging aggregation operators in multi criteria decision
making. IEEE Trans. Syst. Man Cybern. 18(1), 183–190 (1988)
62. Yager, R.R.: A note on weighted queries in information retrieval systems. J. Am. Soc. Inf. Sci.
38(1), 23–24 (1987)
63. Yager, R.R., Rybalov, A.: On the fusion of documents from multiple collections information
retrieval systems. J. Am. Soc. Inf. Sci. (1999)
64. Zadeh, L.A.: Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets Syst. 1, 3–28 (1978)
65. Zadeh, L.A.: Fuzzy Sets. Information and control, vol. 8, pp. 338-353 (1965)
66. Zadeh, L.A.: The concept of a linguistic variable and its application to approximate reasoning,
parts I, II. Inf. Sci. 8, 199–249, 301–357 (1975)
Authors Biography
Don Kraft is an alumnus of Purdue University, where he
majored in Industrial Engineering, specializing in operations
research at the baccalaureate, masters, and doctoral levels. He
has taught at Purdue University, the University of Maryland,
Indiana University, the University of California - Berkeley, the
University of California - Los Angeles, the U.S. Air Force
Academy, and Louisiana State University where he served for a
while as the computer science department chair. He is currently
Professor Emeritus from LSU and an adjunct professor at
Colorado Technical University. He has coauthored four books
and multiple scholarly journal articles, book chapters, and
conference proceedings articles. He is an IEEE, AAAS, and
IFSA Fellow, an ACM Distinguished Scientist, a LSU Distin-
guished Professor, and winner of the ASIST Research Award
294
D.H. Kraft et al.

and Award of Merit. He served as Editor of the Journal of the Association for Information Science
and Technology (JASIST) for twenty-four years and was an Associate Editor for the Journal on
Computing and the IEEE Transactions on Fuzzy Systems. He has chaired two ACM/SIGIR
conferences, an ACM CIKM conference, and co-chaired two NAFIPS conferences, and he serves
on the editorial boards of several journals. He is also a member of INFORMS and Sigma Xi. His
research interests include information retrieval, fuzzy set theory, rough set theory, genetic
algorithms, operations research, and information science.
Erin Colvin received her B.S. degree in Computer Science from
Middle Tennessee State University in Murfreesboro, TN in 1999
and her M. Ed. In Secondary Education from Chaminade
University in Honolulu, HI in 2005, as well as her M.S. in
Computer Science from American Sentinel University in 2011.
She is scheduled to receive her D.C.S. in Computer Science from
Colorado Technical University in late 2014. She is currently an
adjunct online instructor at Southern New Hampshire University.
Her research interests include information retrieval, object-
oriented programming languages, fuzzy logic and software
reuse. She is a member of ACM and IEEE.
Gloria Bordogna received the Laurea degree in Physics from
the University of Milano in 1984. In 1986 she joined the
National Research Council of Italy (CNR) where she currently
holds the position of senior research scientist within IREA
CNR in Milano.
From 2003 to 2010, she was adjunct professor of Information
Retrieval and Geographic Information Systems at the Faculty of
Engineering of Bergamo University.
In 2013 she obtained the National Scientiﬁc Qualiﬁcation of
full professor for the information systems scientiﬁc area. She is
in the editorial board of ACM SIGAPP – Applied Computing
Review and of the Scientiﬁc World Journal. She was a project
reviewer of several national research agencies such as the
Research Foundation of Flanders (FWO), from 2008 - 2013, the
European Research Council Executive Agency (ERCEA peer
reviewer for the “Ideas Speciﬁc Program, PE5 and PE7 - Information and Communication Science,
ERC Council Starting Grants” during
2007 and
2010-2013, the French National Research
Agency (ANR) since 2013 and the Swiss National Science Foundation (NSF) in 2013. She also
served in the program committee of international conferences such as ACM SAC, where she co-
organizes the special track on Information Access and Retrieval since 2008, ACM SIGIR and
ACM ECIR, IEEE ACM WI, KES, WWW, and a reviewer for several international journals such
as IEEE Transactions on Fuzzy Systems, IEEE Transactions on Systems. Man & Cybernetics,
Fuzzy Sets & Systems, Information Processing & Management, Journal of the Association for
Information Science and Technology, and Journal of Geographic Information Systems. She
published over 150 papers in journals and proceedings of international conferences and currently
she is leading projects in the area of geographic information management.
Fuzzy Information Retrieval Systems: A Historical Perspective
295

Gabriella Pasi received a Ph.D. in Computer Science at the
Université de Rennes, France. She has been working at the
National Council of Research in Italy till 2005, and as an
Associate Professor at the University of Milano Bicocca from
2005 to 2014. She is now a full professor at the same University,
where she leads the Information Retrieval Research Laboratory.
Her research mainly focuses on modelling and development of
techniques for ﬂexible and personalised/contextual access to
information, and on the problem of aggregation in search. She
served as the program chair of several international conferences
and workshops related to her research areas, and she was the
chair or co-chair of several international events, among which
are the IEEE / WIC / ACM International Joint Conference on
Web Intelligence and Intelligent Agent Technology, Università
degli Studi di Milano Bicocca, 15-18 September 2009, the Ph.D.
School on Web Information Retrieval (WebBar 2007), the Seventh International Conference on
Flexible Query Answering Systems (FQAS 2006), the European Summer school in Information
Retrieval (ESSIR 2000), and the annual track “Information Access and Retrieval” within the ACM
Symposium on Applied Computing. She has published more than 180 papers in international
journals and books, and on the proceeding of international conferences, and she is member of the
editorial board of several international journals. She is the President of the European Association
for Fuzzy Logic and Technology (EUSFLAT).
296
D.H. Kraft et al.

Is the World Itself Fuzzy? Physical
Arguments and Unexpected Computational
Consequences of Zadeh’s Vision
Vladik Kreinovich and Olga Kosheleva
Abstract Fuzzy methodology has been invented to describe imprecise (“fuzzy”)
human statements about the world, statements that use imprecise words from natural
language like “small” or “large”. Usual applications of fuzzy techniques assume that
the world itself is “crisp”, that there are exact equations describing the world, and
fuzziness of our statements is caused by the incompleteness of our knowledge. But
what if the world itself is fuzzy? What if there is no perfect system of equations
describing the physical world – in the sense that no matter what system of equations
we try, there will always be cases when this system leads to wrong predictions? This
is not just a speculation: this idea is actually supported by many physicists. At ﬁrst
glance, this is a pessimistic idea: no matter how much we try, we will never be able
to ﬁnd the the Ultimate Theory of Everything. But it turns out that this idea also has
its optimistic aspects: namely, in this chapter, we show (somewhat unexpectedly),
that if such a no-perfect-theory principle is true, then the use of physical data can
drastically enhance computations.
1 Fuzzy Techniques: The Original Zadeh’s Vision
Pre-Zadeh attitude: everything can be made precise. Scientists and engineers use
both formal languages and an imprecise natural language. In engineering practice,
formulas, derivations, and computations – which are described in a precise language
– intertwine with explanations – which are usually described in a natural language.
Even in formal mathematics, when presenting a proof, a mathematician describes
part of it in precise terms and part in imprecise terms from a natural language:
“one can easily see that”, “since 𝜀is small, the diﬀerence f(x + 𝜀) −f(x) is also
small”, etc.
V. Kreinovich (✉) ⋅O. Kosheleva
University of Texas at El Paso, El Paso, Texas 79968, USA
e-mail: vladik@utep.edu
O. Kosheleva
e-mail: olgak@utep.edu
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_16
297

298
V. Kreinovich and O. Kosheleva
In formal mathematics, usually, the imprecise parts can be reformulated in precise
terms; professional mathematicians can do it, mathematics students are taught how
to do it – and math students do not get good grades until they are able to perform
such a reformulation. In rare occasions, an attempt for such a formalization reveals
a gap in the proof, but in most such cases, this gap is later ﬁlled.
Similarly, when an engineer makes an imprecise argument, it does not necessar-
ily mean that a more precise explanation is not possible: when needed, an engineer
can usually provide a precise quantitative justiﬁcation of his/her original qualitative
decision.
A similar precisiation is often possible beyond science and engineering. For
example, instructors who grade students’ work use seemingly imprecise words like
“excellent”, “good”, “satisfactory”. However, in most cases, these words have a very
precise meaning. In the US grading system, we usually add up well-deﬁned points
that the students got for diﬀerent problems on the test. If the resulting grade is 90
(or higher) out of 100 possible points, we assign the grade “excellent” (A). If the
resulting grade is at least 80 but smaller than 90, we consider this work “good”
(grade B), etc.
Similarly, in medicine, many terms that are, at ﬁrst glance, imprecise, have a very
precise meaning. “High blood pressure” means upper blood pressure above 140,
“fever” means temperature above 37.5 C, “overweight” means that the body-mass
index (body mass in kg divided by the squared height in meters) is above 25, etc. In
law, a child – a seemingly informal notion, with an imprecise transition – is legally
deﬁned as someone younger than 18 years old.
These example led scientists and engineers to conclude that in principle, all the
statements can be made precise. According to this belief, when a statement sounds
imprecise, it is only because we have not learned the corresponding terms yet. Once
we learn these terms, the statement will become very precise.
Zadeh’s vision. In 1965, LotﬁZadeh published his revolutionary paper, in which he
emphasized that:
– in addition to situations when use imprecise terms but have a precise meaning in
mind (“excellent test results” meaning 90+ points),
– there are also many situations when we use imprecise terms for which no precise
meaning is known.
Moreover, he showed that such situations, in which no precise meaning is known, in
which the meaning is “fuzzy”, are ubiquitous in many application areas.
To deal with such situations, L. Zadeh proposed techniques – which he called
fuzzy – that enable researchers to describe their imprecise statements in precise math-
ematical terms, and thus, enables computer-based systems to process such state-
ments. These techniques has led to many successful applications; see, e.g., [3, 5,
6, 9, 16, 20, 22, 23].

Is the World Itself Fuzzy? Physical Arguments and Unexpected ...
299
2 Is the World Itself Fuzzy? and if Yes, What Are Possible
Physical and Computational Consequences?
Traditional viewpoint. The traditional viewpoint in engineering and science is that
the world itself is crisp, it is described by precise equations which, in principle,
enable us to predict either the events themselves (in classical, pre-quantum physics)
or probabilities of diﬀerent events (in quantum physics). The only reason for “fuzzy”
uncertainty is that we only have partial knowledge about the world.
For example, when a meteorologist makes a “fuzzy” statement that there is a good
chance of rain, the meteorologist usually believes that with more information, he/she
would be able to make a more deﬁnite prediction.
But what if the world itself is fuzzy? But what if there are no ultimate equations?
What if, no matter what equations we formulate, no matter how accurate their pre-
dictions are so far, there will always be cases when these equations will lead to wrong
predictions?
In other words, what if not only our knowledge is fuzzy, what if the world itself
is fuzzy?
Somewhat surprisingly, this is what many physicists actually believe. Many physi-
cists indeed believe that every physical theory is approximate – no matter how
sophisticated a theory, no matter how accurate its current predictions, inevitably
new observations will surface which would require a modiﬁcation of this theory;
see, e.g., [2].
This belief can be justiﬁed by the history of physics: no matter how good a phys-
ical theory, no matter how good its accordance with observations, eventually, new
observations appeared which were not fully consistent with the original theory –
and thus, a theory needed to be modiﬁed. For example, for several centuries, New-
tonian physics seems to explain all observable facts – until later, quantum (and then
relativistic) eﬀects were discovered which required changes in physical theories.
At ﬁrst glance, this belief is pessimistic. This belief sounds pessimistic: no matter
how much we try, we will never ﬁnd the Ultimate Theory of Everything.
But maybe there is room for optimism. But is the situation indeed so pessimistic?
After all, physics is not just about ﬁnding equations. Finding equations is an impor-
tant ﬁrst step, but the ultimate goal of physics is not to ﬁnd equations, but to predict
future events – and equations are an important ﬁrst step towards this prediction.
Many physical equations are very complex, solving them is a complex computa-
tional task. From this viewpoint, any possibility to enhance computations would be
a great optimistic development. For example, quantum physics is clearly more pes-
simistic in terms of possibility of predictions, because in quantum physics, we can
often only predict probabilities of future events, and not the events themselves. On
the other hand, research on quantum computing has shown that the use of quantum
eﬀects can drastically enhance computations; see, e.g., [17].

300
V. Kreinovich and O. Kosheleva
How does the no-perfect-theory belief aﬀect computations? In this chapter, we
analyze how the no-perfect-theory belief aﬀects our computational abilities.
At ﬁrst glance, the fact that no theory is perfect seems to make the question of
computability rather hopeless: no matter how seriously we analyze computability
within a given physical theory, eventually, this theory will turn out to be, strictly
speaking, false – and thus, our analysis of what is computable will have to be redone.
In this chapter, we show, however, that in spite of this seeming hopelessness, some
important answers to the question of what is computable can be deduced simply from
the fact no physical theory is perfect – namely, in this case, we show that computa-
tions can be enhanced in comparison with the usual (Turing machine) computability.
Comment.
Some
preliminary
results
from
this
chapter
ﬁrst
appeared
in
[7, 8, 12, 25].
3 How to Describe, in Precise Terms, that No Physical
Theory Is Perfect
Discussion. The statement that no physical theory is perfect means that no matter
what physical theory we have, eventually there will be observations which violate
this theory. To formalize this statement, we need to formalize what are observations
and what is a theory.
What are observations? Each observation can be represented, in the computer, as
a sequence of 0 s and 1s; actually, in many cases, the sensors already produce the
signal in the computer-readable form, as a sequence of 0 s and 1s.
An exact description of each experiment can also be described in precise terms,
and thus, it will be represented in a computer as a sequence of 0 s and 1s. An exper-
iment should specify how long we wait for the result; in this way, we are guaranteed
that we get the result. The coding should be done in such a way that the waiting time
does not exceed a polynomial of the length of the code i; for example, if we want
to wait for t moments of time, we should just add t copies of an appropriate wait
symbol.
In each experiment, we can specify which bit of the result we are interested in;
for convenience, we can consider producing diﬀerent bits as diﬀerent experiments.
Each such experiment is represented as a sequence of 0 s and 1s; by appending 1
at the beginning of this sequence, we can view this sequence as a binary expansion
of a natural number i. This natural number will serve as the “code” describing the
experiment. For example, a sequence 001 is transformed into i = 10012 = 910. (We
need to append 1, because otherwise two diﬀerent sequences 001 and 01 will be
represented by the same integer).
For natural numbers i which correspond to experiment descriptions, let 𝜔i denote
the bit result of the experiment described by the code i.

Is the World Itself Fuzzy? Physical Arguments and Unexpected ...
301
Let us also deﬁne 𝜔i for natural numbers i which do not correspond to a syntac-
tically correct description of experiments. For example, we can take 𝜔i = 0 for such
numbers i.
In these terms, all past and future observations form a (potentially) inﬁnite
sequence 𝜔= 𝜔1𝜔2 … of 0 s and 1s, 𝜔i ∈{0, 1}.
What is a physical theory from the viewpoint of our problem: a set of sequences. A
physical theory may be very complex, but all we care about is which sequences of
observations 𝜔are consistent with this theory and which are not. In other words, for
our purposes, we can identify a physical theory T with the set of all sequences 𝜔
which are consistent with this theory.
Not every set of sequences corresponds to a physical theory: the set T must be non-
empty and deﬁnable. Not every set of sequences comes from a physical theory. First,
a physical theory must have at least one possible sequence of observations, i.e., the
set T must be non-empty.
Second, a theory – and thus, the corresponding set – must be described by a ﬁnite
sequence of symbols in an appropriate language. Sets which are uniquely by (ﬁnite)
formulas are known as deﬁnable. Thus, the set T must be deﬁnable.
Since at any moment of time, we only have ﬁnitely many observations, the set T
must be closed. Another property of a physical theory comes from the fact that at
any given moment of time, we only have ﬁnitely many observations, i.e., we only
observe ﬁnitely many bits. From this viewpoint, we say that observations 𝜔1 … 𝜔n
are consistent with the theory T if there is a continuing inﬁnite sequence which is
consistent with this theory, i.e., which belongs to the set T.
The only way to check whether an inﬁnite sequence 𝜔= 𝜔1𝜔2 … is consistent
with the theory is to check that for every n, the sequences 𝜔1 … 𝜔n are consistent with
the theory T. In other words, we require that for every inﬁnite sequence 𝜔= 𝜔1𝜔2 …,
– if for every n, the sequence 𝜔1 … 𝜔n is consistent with the theory T, i.e., if for
every n, there exists a sequence 𝜔(n) ∈T which has the same ﬁrst n bits as 𝜔, i.e.,
for which 𝜔(n)
i
= 𝜔i for all i = 1, … , n,
– then the sequence 𝜔itself should be consistent with the theory, i.e., this inﬁnite
sequence should also belong to the set T.
From the mathematical viewpoint, we can say that the sequences 𝜔(n) converge to
𝜔: 𝜔(n) →𝜔(or, equivalently, lim 𝜔(n) = 𝜔), where convergence is understood in
terms of the usual metric on the set of all inﬁnite sequences d(𝜔, 𝜔′)
def= 2−N(𝜔,𝜔′),
where N(𝜔, 𝜔′)
def= max{k ∶𝜔1 … 𝜔k = 𝜔′
1 … 𝜔′
k}.
In general, if 𝜔(m) →𝜔in the sense of this metric, this means that for every n,
there exists an integer 𝓁such that for every m ≥𝓁, we have 𝜔(m)
1
… 𝜔(m)
n
= 𝜔1 … 𝜔n.
Thus, if 𝜔(m) ∈T for all m, this means that for every n, a ﬁnite sequence 𝜔1 … 𝜔n
can be a part of an inﬁnite sequence which is consistent with the theory T. In view
of the above, this means that 𝜔∈T.

302
V. Kreinovich and O. Kosheleva
In other words, if 𝜔(m) →𝜔and 𝜔(m) ∈T for all m, then 𝜔∈T. So, the set T
must contain all the limits of all its sequences. In topological terms, this means that
the set T must be closed.
A physical theory must be diﬀerent from a fact and hence, the set T must be nowhere
dense. The assumption that we are trying to formalize is that no matter how many
observations we have which conﬁrm a theory, there eventually will be a new obser-
vation which is inconsistent with this theory. In other words, for every ﬁnite sequence
𝜔1 … 𝜔n which is consistent with the set T, there exists a continuation of this
sequence which does not belong to T. The opposite would be if all the sequences
which start with 𝜔1 … 𝜔n belong to T; in this case, the set T will be dense in this
set. Thus, in mathematical terms, the statement that every ﬁnite sequence which is
consistent with T has a continuation which is not consistent with T means that the
set T is nowhere dense.
Resulting deﬁnition of a theory. By combining the above properties of a set T which
describes a physical theory, we arrive at the following deﬁnition.
Deﬁnition 1. By a physical theory, we mean a non-empty closed nowhere dense
deﬁnable set T.
Mathematical comment. To properly deﬁne what is deﬁnable, we need to have a con-
sistent formal deﬁnition of deﬁnability. In this chapter, we follow a natural deﬁnition
from [10, 11] – which is reproduced in Appendix A.
Formalization of the principle that no physical theory is perfect. In terms of the above
notations, the no-perfect-theory principle simply means that the inﬁnite sequence 𝜔
(describing the results of actual observations) is not consistent with any physical
theory, i.e., that the sequence 𝜔does not belong to any physical theory T. Thus, we
arrive at the following deﬁnition.
Deﬁnition 2. We say that an inﬁnite binary sequence 𝜔is consistent with the no-
perfect-theory principle if the sequence 𝜔does not belong to any physical theory (in
the sense of Deﬁnition 1).
Comment. Are there such sequences in the ﬁrst place? Our answer is yes. Indeed, by
deﬁnition, we want a sequence which does not belong to a union of all deﬁnable phys-
ical theories. Every physical theory is closed nowhere dense set. Every deﬁnable set
is deﬁned by a ﬁnite sequence of symbols, so there are no more than countably many
deﬁnable theories. Thus, the union of all deﬁnable physical theories is contained in
a union of countably many closed nowhere dense sets. Such sets are knows as mea-
ger (or Baire ﬁrst category); it is known that the set of all inﬁnite binary sequences
is not meager. Thus, there are sequences who do not belong to the above union –
i.e., sequences which are consistent with the no-perfect-theory principle; see, e.g.,
[4, 18].

Is the World Itself Fuzzy? Physical Arguments and Unexpected ...
303
4 How to Describe When Access to Physical Observations
Enhances Computability
How to describe general computations. Each computation is a solution to a well-
deﬁned problem. As a result, each bit in the resulting answer satisﬁes a well-deﬁned
mathematical property. All mathematical properties can be described, e.g., in terms
of Zermelo-Fraenkel theory ZF, the standard formalization of set theory. For each
resulting bit, we can formulate a property P which is true if and only if this bit is
equal to 1. In this sense, each bit in each computation result can be viewed as the
truth value of some statement formulated in ZF. Thus, our general ability to compute
can be described as the ability to (at least partially) compute the sequence of truth
values of all statements from ZF.
All well-deﬁned statements from ZF can be numbered, e.g., in lexicographic
order. Let 𝛼n denote the truth value of the n-th ZF statement, and let 𝛼= 𝛼1 … 𝛼n …
denote the inﬁnite sequence formed by these truth values. In terms of this sequence,
our ability to compute is our ability to compute the sequence 𝛼.
Kolmogorov complexity as a way to describe what is easier to compute. We want to
analyze whether the use of physical observations (i.e., of the sequence 𝜔analyzed
in the previous section) can simplify computations. A natural measure of easiness-
to-compute was invented by A. N. Kolmogorov, the founder of modern probability
theory, when he realized that in the traditional probability theory, there is no formal
way to distinguish between:
– ﬁnite sequences which come from observing from truly random processes, and
– orderly sequences like 0101 … 01.
Kolmogorov noticed that an orderly sequence 0101 … 01 can be computed by a short
program, while the only way to compute a truly random sequence 0101 … is to have a
print statement that prints this sequence. He suggested to describe this diﬀerence by
introducing what is now known as Kolmogorov complexity K(x) of a ﬁnite sequence
x: the shortest length of a program (in some programming language) which computes
the sequence x.
– For an orderly sequence x, the Kolmogorov complexity K(x) is much smaller than
the length len(x) of this sequence: K(x) ≪len(x).
– For a truly random sequence x, we have K(x) ≈len(x); see, e.g., [14].
The smaller the diﬀerence len(x) −K(x), the more we are sure that the sequence x is
truly random.
Relative Kolmogorov complexity as a way to describe when using an auxiliary
sequence simpliﬁes computations. The usual notion of Kolmogorov complexity pro-
vides the complexity of computing x “from scratch”. A similar notion of the relative
Kolmogorov complexity K(x | y) can be used to describe the complexity of comput-
ing x when a (potentially inﬁnite) sequence y is given. This relative complexity is
based on programs which are allowed to use y as a subroutine, i.e., programs which,

304
V. Kreinovich and O. Kosheleva
after generating an integer n, can get the n-th bit yn of the sequence y by simply
calling y. When we compute the length of such programs, we just count the length
of the parameters of this call, not the length of the auxiliary program which com-
putes yn – just like when we count the length of a C++ program, we do not count
how many steps it takes to compute, e.g., sin(x), we just count the number of sym-
bols in this function call. The relative Kolmogorov complexity is then deﬁned as the
shortest length of such a y-using program which computes x.
Clearly, if x and y are unrelated, having access to y does not help in computing
x, so K(x | y) ≈K(x). On the other hand, if x coincides with y, then the relative
complexity K(x | y) is very small: all we need is a simple for-loop, in which we call
for each bit yi, i = 1, … , n, and print this bit right away.
Resulting reformulation of our question. In terms of relative Kolmogorov
complexity, the question of whether observations enhance computations is trans-
lated into checking whether K(𝛼1 … 𝛼n | 𝜔) ≈K(𝛼1 … 𝛼n) (in which case there is
no enhancement) or whether K(𝛼1 … 𝛼n | 𝜔) ≪K(𝛼1 … 𝛼n) (in which case there is
a strong enhancement). The larger the diﬀerence K(𝛼1 … 𝛼n) −K(𝛼1 … 𝛼n | 𝜔), the
larger the enhancement.
5 First Result: No-Perfect-Theory Principle Enhances
Computability
Let us show that under the no-perfect-theory principle, observations do indeed
enhance computability.
Proposition 1. Let 𝛼be a sequence of truth values of ZF statements, and let 𝜔be
an inﬁnite binary sequence which is consistent with the no-perfect-theory principle.
Then, for every integer C > 0, there exists an integer n for which K(𝛼1 … 𝛼n | 𝜔) <
K(𝛼1 … 𝛼n) −C.
In other words, in principle, we can have an arbitrary large enhancement.
Comment. For readers’ convenience, all the proof are placed in a special appendix.
6 Can Access to Physical Observations Speed up
Computations?
Are computations feasible? What we have shown so far is that under Zadeh-inspired
no-perfect-theory belief, it is possible to compute things that are not computable in
the usual physical paradigm. From the practical viewpoint, being able to compute
something in principle is important, but even more important is how fast we can
compute it. In many cases, computations are theoretically possible, but not practi-
cally feasible, since they require computation times which are longer than the lifetime

Is the World Itself Fuzzy? Physical Arguments and Unexpected ...
305
of the Universe :-( It is therefore important to analyze which problems are feasibly
computable and which are not. To perform this analysis, we need to deﬁne what is
“feasible” and what is a “problem”.
In computer science, “feasible” is usually interpreted as computable in polyno-
mial time, i.e., in time t bounded by a polynomial of the length n of the input; see,
e.g., [19]. This deﬁnition works in most cases:
– time 2n is non-feasible already for n ≈300, while
– time n2 or n3 is usually feasible.
This is not a perfect deﬁnition:
– on the one hand, time t = 10400 ⋅n is polynomial in n but clearly not feasible;
– on the other hand, computation time exp(10−10⋅n) is not bounded by a polynomial,
but it clearly corresponds to feasible computations.
However, this is the best deﬁnition we have.
By a problem, computer scientists usually understand a problem in which it is
absolutely clear what is a solution and what is not. For example:
– ﬁnding a proof of a given mathematical statement,
– ﬁnding a formula that ﬁts all experimental observations,
– designing a bridge under certain speciﬁcations of strength, cost, etc.,
these are all such problems – while, e.g., the problem of designing a beautiful bridge
is not clearly deﬁned.
In general, we need to ﬁnd a solution that satisﬁes a given set of constraints – or at
least check that such a solution is possible. Once we have a candidate for the solution,
we can feasibly check whether this candidate indeed satisﬁes all the constraints.
A problem of checking whether a given set of constraints has solution is called
a problem of the class NP if we can check, in polynomial time, whether a given
candidate is a solution; see, e.g., [19].
Examples of such problem includes checking whether a given graph can be col-
ored in 3 colors, checking whether a given propositional formula – i.e., formula of
the type
(v1 ∨¬v2 ∨v3) & (v4 ∨¬v2 ∨¬v5) & … ,
is satisﬁable, i.e., whether this formula is true by some combination of the proposi-
tional variables vi, etc.
Each problem from the class NP can be algorithmically solved by trying all possi-
ble candidates. For example, we can check whether a graph can be colored by trying
all possible assignments of colors to diﬀerent vertices of a graph, and we can check
whether a given propositional formula is satisﬁable by trying all 2n possible combi-
nations of true-or-false values v1, … , vn. Such exhaustive search algorithms require
computation time like 2n, time that grows exponentially with n. For medium-size
inputs, e.g., for n ≈300, the resulting time is larger than the lifetime of the Uni-
verse. So, these exhaustive search algorithms are not practically feasible.

306
V. Kreinovich and O. Kosheleva
It is not known whether problems from the class NP can be solved feasibly (i.e.,
in polynomial time): this is the famous open problem P
?=NP. It is known, however,
there are problems in the class NP which are NP-complete in the sense that every
problem from the class NP can be reduced to this problem. Reduction means, in
particular, that if we can ﬁnd a way to eﬃciently solve one NP-complete problem,
then, by reducing other problems from the class NP to this problem, we can thus
eﬃciently solve all the problems from the class NP.
So, it is very important to be able to eﬃciently solve even one NP-hard problem.
(By the way, both above example of NP problems – checking whether a graph can
be colored in 3 colors and whether coloring a propositional formula is satisﬁable –
are NP-complete.)
Can the use of non-standard physics speed up the solution of NP-complete problems?
NP-completeness of a problem means, crudely speaking, that the problem may take
an unrealistically long time to solve – at least on computers based on the usual phys-
ical techniques. A natural question is: can the use of non-standard physics speed up
the solution of these problems?
This question has been analyzed for several speciﬁc physical theories, e.g., for
quantum ﬁled theory, for cosmological solutions with wormholes and/or casual
anomalies. Several possible techniques for solving NP-complete problems are
described in [1, 11, 13, 15, 21].
How does the no-perfect-belief aﬀect the speed of computations? In this chapter, we
show that an important speed-up can be deduced simply from the fact no physical
theory is perfect.
7 Second Result: The Use of Physical Observations Can
Help in Solving NP-Complete Problems
How to represent instances of an NP-complete problem. For each NP-complete prob-
lem , its instances are sequences of symbols. In the computer, each such sequence
is represented as a sequence of 0 s and 1s. Thus, as in the previous sections, we can
append 1 in front of this sequence and interpret the resulting sequence as a binary
code of a natural number i.
In principle, not all natural numbers i correspond to instances of a problem ; we
will denote the set of all natural numbers which correspond to such instances by S.
For each i ∈S, the correct answer (true or false) to the i-th instance of the
problem will be denoted by s,i.
What we mean by using physical observations in computations. In addition to per-
forming computations, our computational device can produce a scheme i for an
experiment, and then use the result 𝜔i of this experiment in future computations.
In other words, given an integer i, we can produce 𝜔i.

Is the World Itself Fuzzy? Physical Arguments and Unexpected ...
307
In precise theory-of-computation terms, the use of physical observations in com-
putations thus means computations that use the sequence 𝜔as an oracle; see, e.g., [19].
Deﬁnition 3. By a ph-algorithm , we mean an algorithm which uses, as an oracle,
a sequence 𝜔which is consistent with the no-perfect-theory principle.
Notation. The result of applying an algorithm using 𝜔to an input i will be denoted
by (𝜔, i).
Deﬁnition 4. Let be an NP-complete problem. We say that a feasible ph-algorithm
solves almost all instances of if for every 𝜀> 0, and for every natural number
n, there exists an integer N ≥n for which the proportion of the instances i ≤N of
the problem which are correctly solved by is greater than 1 −𝜀:
∀𝜀> 0 ∀n ∃N
(
N ≥n &
#{i ≤N ∶i ∈S& (𝜔, i) = s,i}
#{i ≤N ∶i ∈S}
> 1 −𝜀
)
.
Comment. The restriction to suﬃciently long inputs N ≥n makes perfect sense: for
short inputs, NP-completeness is not an issue: we can perform exhaustive search of
all possible bit sequences of length 10, 20, and even 30. The challenge starts when
the length of the input is high.
Proposition 2. For every NP-complete problem , there exists a feasible
ph-algorithm that solves almost all instances of .
In other words, we show that the use of physical observations makes all NP-complete
problems easier-to-solve (in the above-described sense).
Acknowledgments This work was supported in part by the National Science Foundation grants
HRD-0734825, HRD-124212, and DUE-0926721. The authors are thankful to the anonymous ref-
erees for valuable suggestions.
A A Formal Deﬁnition of Deﬁnable Sets
Deﬁnition A1. Let be a theory, and let P(x) be a formula from the language of the
theory , with one free variable x for which the set {x | P(x)} is deﬁned in the theory
. We will then call the set {x | P(x)} -deﬁnable.
Crudely speaking, a set is -deﬁnable if we can explicitly deﬁne it in . The set of
all real numbers, the set of all solutions of a well-deﬁned equation, every set that we
can describe in mathematical terms: all these sets are -deﬁnable.
This does not mean, however, that every set is -deﬁnable: indeed, every -
deﬁnable set is uniquely determined by formula P(x), i.e., by a text in the language
of set theory. There are only denumerably many words and therefore, there are only

308
V. Kreinovich and O. Kosheleva
denumerably many -deﬁnable sets. Since, e.g., in a standard model of set theory
ZF, there are more than denumerably many sets of integers, some of them are thus
not -deﬁnable.
Our objective is to be able to make mathematical statements about -deﬁnable
sets. Therefore, in addition to the theory , we must have a stronger theory in
which the class of all -deﬁnable sets is a set – and it is a countable set.
Denotation. ForeveryformulaFfromthetheory,wedenoteitsGödelnumberby⌊F⌋.
Comment. A Gödel number of a formula is an integer that uniquely determines this
formula. For example, we can deﬁne a Gödel number by describing what this formula
will look like in a computer. Speciﬁcally, we write this formula in LATEX, interpret
every LATEX symbol as its ASCII code (as computers do), add 1 at the beginning of
the resulting sequence of 0 s and 1s, and interpret the resulting binary sequence as
an integer in binary code.
Deﬁnition A2. We say that a theory is stronger than if it contains all formulas,
all axioms, and all deduction rules from , and also contains a special predicate
def(n, x) such that for every formula P(x) from with one free variable, the formula
∀y (def(⌊P(x)⌋, y) ↔P(y)) is provable in .
The existence of a stronger theory can be easily proven: indeed, for =ZF, there
exists a stronger theory . As an example of such a stronger theory, we can sim-
ply take the theory plus all countably many equivalence formulas as described
in Deﬁnition A2 (corresponding to all possible formulas P(x) with one free vari-
able). This theory clearly contains and all the desired equivalence formulas, so
all we need to prove is that the resulting theory is consistent (provided that 
is consistent, of course). Due to compactness principle, it is suﬃcient to prove that
for an arbitrary ﬁnite set of formulas P1(x), … , Pm(x), the theory is consistent
with the above reﬂection-principle-type formulas corresponding to these properties
P1(x), … , Pm(x).
This auxiliary consistency follows from the fact that for such a ﬁnite set, we
can take
def(n, y) ↔(n = ⌊P1(x)⌋& P1(y)) ∨… ∨(n = ⌊Pm(x)⌋& Pm(y)).
This formula is deﬁnable in and satisﬁes all m equivalence properties. The state-
ment is proven.
Important comments. In the main text, we will assume that a theory that is
stronger than has been ﬁxed; proofs will mean proofs in this selected theory .
An important feature of a stronger theory is that the notion of an -deﬁnable
set can be expressed within the theory : a set S is -deﬁnable if and only if
∃n ∈IN ∀y(def(n, y) ↔y ∈S).
In the chapter, when we talk about deﬁnability, we will mean this property
expressed in the theory . So, all the statements involving deﬁnability become
statements from the theory itself, not statements from metalanguage.

Is the World Itself Fuzzy? Physical Arguments and Unexpected ...
309
B Proofs
Proof of Proposition 1. Let us ﬁx an integer C. To prove the desired property for this
C, let us prove that the set T of all the sequences which do not satisfy this property,
i.e., for which K(𝛼1 … 𝛼n | 𝜔) ≥K(𝛼1 … 𝛼n) −C for all n, is a physical theory in
the sense of Deﬁnition 1. For this, we need to prove that this set T is non-empty,
closed, nowhere dense, and deﬁnable. Then, from Deﬁnition 2, it will follow that the
sequence 𝜔does not belong to this set and thus, that the conclusion of Proposition
1 is true.
The set T is clearly non-empty: it contains, e.g., a sequence 𝜔= 00 … 0 … which
doesnotaﬀectcomputations.ThesetT isalsoclearlydeﬁnable:wehavejustdeﬁnedit.
Let us prove that the set T is closed. For that, let us assume that 𝜔(m) →𝜔and
𝜔(m) ∈T for all m. We then need to prove that 𝜔∈T. Indeed, let us ﬁx n, and let us
prove that K(𝛼1 … 𝛼n | 𝜔) ≥K(𝛼1 … 𝛼n) −C. We will prove this by contradiction.
Let us assume that K(𝛼1 … 𝛼n | 𝜔) < K(𝛼1 … 𝛼n) −C. This means that there exists
a program p of length len(p) < K(𝛼1 … 𝛼n) −C which uses 𝜔to compute 𝛼1 … 𝛼n.
This program uses only ﬁnitely many bits of 𝜔; let B be the largest index of these
bits. Due to 𝜔(m) →𝜔, there exists M for which, for all m ≥M, the ﬁrst B bits of 𝜔(m)
coincide with the ﬁrst B bits of the sequence 𝜔. Thus, the same program p will work
exactly the same way – and generate the same sequence 𝛼1 … 𝛼n – if we use 𝜔(m)
instead of 𝜔. But since len(p) < K(𝛼1 … 𝛼n) −C, this would means that the shortest
length K(𝛼1 … 𝛼n | 𝜔(m)) of all the programs which use 𝜔(m) to compute 𝛼1 … 𝛼n
also satisﬁes the inequality K(𝛼1 … 𝛼n | 𝜔(m)) < K(𝛼1 … 𝛼n) −C. This inequality
contradicts to our assumption that 𝜔(m) ∈T and thus, that K(𝛼1 … 𝛼n | 𝜔(m)) ≥
K(𝛼1 … 𝛼n) −C. The contradiction proves that the set T is indeed closed.
Let us now prove that the set T is nowhere dense, i.e., that for every ﬁnite sequence
𝜔1 … 𝜔m, there exists a continuation 𝜔which does not belong to the set T. Indeed, as
such a continuation, we can simply take a sequence 𝜔= 𝜔1 … 𝜔m𝛼1𝛼2 … obtained
by appending 𝛼at the end. For this new sequence, computing 𝛼1 … 𝛼n is straightfor-
ward: we just copy the values 𝛼i from the corresponding places of the new sequence
𝜔. Here, the relative Kolmogorov complexity K(𝛼1 … 𝛼n | 𝜔) is very small and is,
thus, much smaller than the complexity K(𝛼1 … 𝛼n) which – since ZF is not decid-
able – grows with n.
The proposition is proven.
Proof of Proposition 2.
1◦. As the desired ph-algorithm, we will, given an instance i, simply produce the
result 𝜔i of the i-th experiment. Let us prove, by contradiction, that this algorithm
satisﬁes the desired property.
2◦. We want to prove that for every 𝜀> 0 and for every n, there exists an integer
N ≥n for which
#{i ≤N ∶i ∈S& 𝜔i = s,i} > (1 −𝜀) ⋅#{i ≤N ∶i ∈S}.

310
V. Kreinovich and O. Kosheleva
The assumption that this property is not satisﬁed means that for some 𝜀> 0 and for
some integer n, we have
#{i ≤N ∶i ∈S& 𝜔i = s,i} ≤(1 −𝜀) ⋅#{i ≤N ∶i ∈S} for all N ≥n.
(1)
Let T denote the set of all the sequences x that satisfy the property (1), i.e., let
T
def=
{x ∶#{i ≤N ∶i ∈S& xi = s,i} ≤(1 −𝜀) ⋅#{i ≤N ∶i ∈S} for all N ≥n}.
We will prove that this set T is a physical theory in the sense of Deﬁnition 1.
Then, due to Deﬁnition 2 and the fact that the sequence 𝜔satisﬁes the no-perfect-
theory principle, we will be able to conclude that 𝜔∉T, and thus, that the property
(1) is not satisﬁed for the given sequence 𝜔. This will conclude the proof by contra-
diction.
3◦. By deﬁnition of a physical theory T, it is a set which is non-empty, closed,
nowhere dense, and deﬁnable. Let us prove these four properties one by one.
3.1◦. Non-emptiness comes from the fact that the sequence xi for which xi = ¬s,i
for i ∈Sand xi = 0 otherwise clearly belongs to this set: for this sequence, for
every N, we have #{i ≤N ∶i ∈S& xi = s,i} = 0 and thus, the desired property
is satisﬁed.
3.2◦. Let us prove that the set T is closed, i.e., that if we have a family of sequences
x(m) ∈T for which x(m) →x, then x ∈T.
Indeed, let us take any N ≠n, and let us prove that
#{i ≤N ∶i ∈S& xi = s,i} ≤(1 −𝜀) ⋅#{i ≤N ∶i ∈S}
for this N. Due to x(m) →x, there exists M for which, for all m ≥M, the ﬁrst N bits
of x(m) coincide with the ﬁrst N bits of the sequence x: x(m)
i
= 𝜔i for all i ≤N. Thus,
#{i ≤N ∶i ∈S& xi = s,i} = #{i ≤N ∶i ∈S& x(m)
i
= s,i}.
Since x(m) ∈T, we have
#{i ≤N ∶i ∈S& x(m)
i
= s,i} ≤(1 −𝜀) ⋅#{i ≤N ∶i ∈S},
thus
#{i ≤N ∶i ∈S& xi = s,i} ≤(1 −𝜀) ⋅#{i ≤N ∶i ∈S}.
So, the set T is indeed closed.

Is the World Itself Fuzzy? Physical Arguments and Unexpected ...
311
3.3◦. Let us now prove that the set T is nowhere dense, i.e., that for every ﬁnite
sequence x1 … xm, there exists a continuation x which does not belong to the set T.
Indeed, as such a continuation, we can simply take a sequence
x = x1 … xmxm+1xm+2 …
where for i > m, we take xi = s,i if i ∈Sand xi = 0 otherwise. For this new
sequence, for every N, at most m ﬁrst instances may lead to results diﬀerent from
s,i, so we have
#{i ≤N ∶i ∈S& xi = s,i} ≥#{i ≤N ∶i ∈S} −m.
When N →∞, then #{i ≤N ∶i ∈S} →∞, so for suﬃciently large N, we have
#{i ≤N ∶i ∈S} −m > (1 −𝜀) ⋅#{i ≤N ∶i ∈S},
thus,
#{i ≤N ∶i ∈S& xi = s,i} > (1 −𝜀) ⋅#{i ≤N ∶i ∈S},
and we cannot have
#{i ≤N ∶i ∈S& xi = s,i} ≤(1 −𝜀) ⋅#{i ≤N ∶i ∈S}.
Therefore, this continuation does not belong to the set T.
3.4◦. Finally, since the formula (1) explicitly deﬁnes the set T, this set T is clearly
deﬁnable.
So, T is a physical theory, hence 𝜔∉T, and the proposition is proven.
References
1. Aaronson, S.: NP-complete problems and physical reality. ACM SIGACT News 36, 30–52
(2005)
2. Feynman, R., Leighton, R., Sands, M.: The Feynman Lectures on Physics. Addison Wesley,
Massachusetts (2005)
3. Friedman, M., Kandel, A.: Introduction to Pattern Recognition: Statistical, StructuralNeural
and Fuzzy Logic Approaches. World Scientiﬁc, Singapore (1999)
4. Jalal-Kamali, A., Nebesky, O., Durcholz, M.H., Kreinovich, V., Longpré, L.: Towards a
“generic” notion of genericity: from “typical” and “random” to meager, shy, etc. J. Uncertain
Syst. 6(2), 104–113 (2012)
5. Kandel, A.: Fuzzy Expert Systems. CRC Press, Boca Raton, Florida (1991)
6. Kandel, A., Langholz, G.: Fuzzy Control Systems. CRC Press, Boca Raton, Florida (1993)
7. Kosheleva, O.M., Soloviev, S.V.: On the logic of using observable events in decision making.
In: Proceedings of the IX National USSR Symposium on Cybernetics, pp. 49–51. Moscow
(1981) (in Russian)

312
V. Kreinovich and O. Kosheleva
8. Kosheleva, O., Zakharevich, M., Kreinovich, V.: If many physicists are right and no physical
theory is perfect, then by using physical observations, we can feasibly solve almost all instances
of each NP-complete problem. Mathematical Structures and Modeling, to appear (2014)
9. Klir, G., Yuan, B.: Fuzzy Sets and Fuzzy Logic: Theory and Applications, Upper Saddle River.
Prentice Hall, New Jersey (1995)
10. Kreinovich, V.: Toward formalizing non-monotonic reasoning in physics: the use of Kol-
mogorov complexity. Rev. Iberoamericana de Inteligencia Artif. 41, 4–20 (2009)
11. Kreinovich, V., Finkelstein, A.M.: Towards applying computational complexity to foundations
of physics. J. Math. Sci. 134(5), 2358–2382 (2006)
12. Kreinovich, V., Kosheleva, O.: Logic of scientiﬁc discovery: how physical induction aﬀects
what is computable. In: Proceedings of the The International Interdisciplinary Conference Phi-
losophy, Mathematics, Linguistics: Aspects of Interaction 2014 PhML’2014, pp. 116–127. St.
Petersburg, Russia, 21–25 April 2014
13. Kreinovich, V., Margenstern, M.: In some curved spaces, one can solve NP-hard problems in
polynomial time. Notes of Mathematical Seminars of St. Petersburg Department of Steklov
Institute of Mathematics 358, 224–250 (2008). Reprinted in J. Math. Sci. 158(5), 727–740
(2009)
14. Li, M., Vitányi, P.M.B.: An Introduction to Kolmogorov Complexity. Springer, Berlin (2008)
15. Morgenstein, D., Kreinovich, V.: Which algorithms are feasible and which are not depends on
the geometry of space-time. Geombinatorics 4(3), 80–97 (1995)
16. Nguyen, H.T., Walker, E.A.: First Course on Fuzzy Logic. CRC Press, Boca Raton, Florida
(2006)
17. Nielsen, M.A., Chuang, I.L.: Quantum Computation and Quantum Information. Cambridge
University Press, Cambridge (2011)
18. Oxtoby, J.C.: Measure and Category: A Survey of the Analogies between Topological and
Measure Spaces. Springer, New York (1980)
19. Papadimitriou, C.: Computational Complexity. Addison Welsey, Massachusetts (1994)
20. Ross, T.J.: Fuzzy Logic with Engineering Applications. Wiley, New York (2010)
21. Srikanth, R.: The quantum measurement problem and physical reality: a computation theo-
retic perspective, In: Goswami, D. (ed.), Quantum Computing: Back Action 2006, IIT Kanpur,
India, March 2006, AIP Conference Proceedings, vol. 864, pp. 178–193 (2006)
22. Terano, T., Asai, K., Sugeno, M. (eds.): Fuzzy Systems Theory and Its Applications. Academic
Press, California (1992)
23. Teodorescu, H.-N.L., Kandel, A., Jain, L.C.: Fuzzy and Neuro-Fuzzy Systems in Medicine.
CRC Press, Boca Raton, Florida (1998)
24. Zadeh, L.A.: Fuzzy sets. Inf. Control 8, 338–353 (1965)
25. Zakharevich, M., Kosheleva, O.: If many physicists are right and no physical theory is perfect,
then the use of physical observations can enhance computations. J. Uncertain Syst. 8(3), 227–
232 (2014)

Is the World Itself Fuzzy? Physical Arguments and Unexpected ...
313
Authors Biography
Vladik Kreinovich
received his MS in Mathematics and
Computer Science from St. Petersburg University, Russia, in
1974, and PhD from the Institute of Mathematics, Soviet Acad-
emy of Sciences, Novosibirsk, in 1979. He worked with the
Soviet Academy of Sciences and with the National Institute
for Electrical Measuring Instruments, Russia. In 1989, he was
a visiting scholar at Stanford University. Since 1990, he has
worked in the Department of Computer Science at the Univer-
sity of Texas at El Paso. He has also served as an invited pro-
fessor in Paris (University of Paris VI), France; Hong Kong;
St. Petersburg, Russia; and Brazil.
His main interests are the representation and processing
of uncertainty, especially interval computations and intelligent
control. He has published six books, eleven edited books, and
more than 1,000 papers. Vladik is a member of the editorial
board of the international journal “Reliable Computing” (formerly “Interval Computations”) and
several other journals, he is the co-maintainer of the international Web site on interval compu-
tations http://www.cs.utep.edu/interval-comp.
Vladik served as President of the North American Fuzzy Information Processing Society
2012-14; is a foreign member of the Russian Academy of Metrological Sciences; was the recip-
ient of the 2003 El Paso Energy Foundation Faculty Achievement Award for Research awarded
by the University of Texas at El Paso; and was a co-recipient of the 2005 Star Award from the
University of Texas System.
Olga M. Kosheleva received her Master’s degree in Math from
Novosibirsk University, Russia, Master’s degree in Computer
Science from UTEP, and PhD in Computer Engineering from
UTEP in 2003. She is currently an Associate Professor at the
University of Texas at El Paso (UTEP). She also served as an
invited professor in St. Petersburg, Russia, and in Brazil. Her
areas of interest include data processing under uncertainty and
mathematics education. She has published more than 250 peer-
refereed papers. She received 2010 Best Paper Award at the
International Conference of the North American Fuzzy Infor-
mation Processing Society NAFIPS’2010 (Toronto, Canada),
and an Excellent Paper Award at the 11th International Sym-
posium on Management Engineering ISME’2014 (Kitakyushu,
Japan).

Handling Noise and Outliers
in Fuzzy Clustering
Christian Borgelt, Christian Braune, Marie-Jeanne Lesot
and Rudolf Kruse
Abstract Since it is an unsupervised data analysis approach, clustering relies solely
on the location of the data points in the data space or, alternatively, on their relative
distances or similarities. As a consequence, clustering can suﬀer from the presence
of noisy data points and outliers, which can obscure the structure of the clusters in the
data and thus may drive clustering algorithms to yield suboptimal or even misleading
results. Fuzzy clustering is no exception in this respect, although it features an aspect
of robustness, due to which outliers and generally data points that are atypical for the
clusters in the data have a lesser inﬂuence on the cluster parameters. Starting from
this aspect, we provide in this paper an overview of diﬀerent approaches with which
fuzzy clustering can be made less sensitive to noise and outliers and categorize them
according to the component of standard fuzzy clustering they modify.
C. Borgelt (✉)
European Centre for Soft Computing Ediﬁcio de Investigacíon, Campus Mieres,
c/ Gonzalo Gutiérrez Quirós s/n, 33600 Mieres, Spain
e-mail: christian@borgelt.net
C. Braune ⋅R. Kruse
Dept. Knowledge Processing and Language Engineering,
Otto-von-Guericke-Universität Magdeburg, Universitätsplatz 2,
39106 Magdeburg, Germany
e-mail: christian.braune@ovgu.de
R. Kruse
e-mail: rudolf.kruse@ovgu.de
M.-J. Lesot
Sorbonne Universités, UPMC Univ Paris 06, UMR 7606, LIP6, 75005
Paris, France
e-mail: marie-jeanne.lesot@lip6.fr
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_17
315

316
C. Borgelt et al.
1 Introduction
In general, clustering [1–4] is a data analysis method that tries to group the records,
cases or generally data points of a data set in such a way that points in the same
group (or cluster) are as similar as possible, while points in diﬀerent groups are
as dissimilar as possible. There is no predeﬁned target attribute (like a class label)
that guides the analysis process and hence clustering belongs to the so-called unsu-
pervised methods (in contrast to supervised methods like, for example, classiﬁer
construction): it relies solely on the location of the data points in the data space or,
alternatively, on their relative distance or similarity.
Unfortunately, due to this exclusive dependence on location and/or distance infor-
mation, clustering algorithms can suﬀer from noisy data points and outliers that are
present in the data. Such data points, which we may deﬁne informally as points that
do not conform (well) to the actual cluster structure of the data, can obscure the true
cluster structure and thus may lead clustering algorithms to produce results that are
far from optimal or even misleading.
Fuzzy clustering [4–7] is no exception in this respect, although it features an
aspect of robustness, due to which outliers and generally data points that are atypical
for the clusters in the data have a lesser inﬂuence on the cluster parameters (like, for
instance, the location of the cluster centers as well as shape and size parameters that
may be present). We emphasize this aspect in the next section (Sect. 2), in which we
brieﬂy review standard fuzzy clustering.
Afterward we turn to methods that try to make fuzzy clustering (even more)
robust w.r.t. noise and outliers. Such approaches can be roughly categorized into
two classes: (1) approaches that modify the “(inﬂuence) weight” of the (atypical) data
points, either by changing how membership degrees are computed from the (relative)
data point distances to the clusters or by introducing and adapting an explicit data
point weight, and (2) approaches that rely on other distance measures than the usu-
ally employed squared Euclidean distance or transform the distance measure before
computing membership degrees.
Among the approaches in the ﬁrst class are the popular noise cluster approach
[8–10] (Sect. 3), introducing and adapting an explicit data point weight (outlier clus-
tering) [11] (Sect. 4), possibilistic fuzzy clustering [12, 13] and its variants that com-
bine it with standard fuzzy clustering [14–18] (Sect. 5), as well as using an alterna-
tive transformation of the membership degrees [19] (Sect. 6). In the second class we
ﬁnd approaches based on squared and particularly unsquared Minkowski distances
[20–22] (Sect. 7) or transformed or otherwise modiﬁed distance measures [23–26]
(Sect. 8).
2 Fuzzy Clustering
In the clustering approaches we study in this paper, the similarity of data points is
formalized by a distance measure on the data space and the clusters are described
by prototypes that capture the location and possibly also the shape and size of the

Handling Noise and Outliers in Fuzzy Clustering
317
clusters in the data space. With such an approach the general objective of clustering
can be reformulated as the task to ﬁnd a set of cluster prototypes together with an
assignment of the data points to them, so that the data points are as close as possible
to their assigned prototypes. By formalizing this approach, and using for the pro-
totypes only points in the data space that represent the cluster centers, one obtains
immediately the objective function of classical c-means clustering [27–29]: simply
sum the squared distances of the data points to the center of the cluster to which
they are assigned. The c-means clustering algorithm then strives to minimize this
objective function.
Unfortunately, c-means clustering always partitions the data, that is, each data
point is assigned to one cluster and one cluster only. This is often inappropriate,
as it can lead to somewhat arbitrary cluster boundaries and certainly does not treat
points properly that lie between two (or more) clusters without belonging to any of
them unambiguously. A solution to this problem consists in employing one of the
diﬀerent “fuzziﬁcations” of the classical crisp (or hard) scheme (see, for instance,
[4–7, 26, 30]), which modify the objective function of classical c-means clustering
in order to obtain graded cluster memberships. In principle, there are two ways to do
this, namely (1) by membership transformation, which maps the memberships with a
convex function, and (2) by membership regularization, which adds a regularization
term, usually derived from an entropy measure, to the objective function to prevent
crisp assignments (see, for instance, [31] for a discussion). Here we focus on the
ﬁrst approach (membership transformation), because it exhibits a certain robustness
property we are interested in. However, most of the approaches we study in Sects. 3
to 8 can equally well be applied to fuzzy clustering by membership regularization.
Formally, we are given a data set 𝐗= {⃗x1, … , ⃗xn} with n data points, each
of which is an m-dimensional real-valued vector, that is, ∀j; 1 ≤j ≤n ∶⃗xj =
(xj1, … , xjm) ∈ℝm. These data points are to be grouped into c clusters, each of which
is described by a prototype ⃗ci, i = 1, … , c. The set of all prototypes is denoted by
𝐂= {⃗c1, … , ⃗cc}. We conﬁne ourselves here to cluster prototypes that consist only
of a cluster center, that is, ∀i; 1 ≤i ≤c ∶⃗ci = (ci1, … , cim) ∈ℝm, although (most
of) the approaches we study below may just as well be applied if the cluster pro-
totypes comprise shape and size parameters (like, for instance, in [32, 33]). The
assignment of the data points to the cluster centers is encoded as a c × n matrix
𝐔= (uij)1≤i≤c;1≤j≤n, which is often called the partition matrix. In the crisp case, a
matrix element uij ∈{0, 1} states whether data point ⃗xj belongs to cluster ⃗ci (uij = 1)
or not (uij = 0). In the fuzzy case, uij ∈[0, 1] states the degree to which ⃗xj belongs
to ⃗ci (degree of membership).
Since we do not obtain graded memberships by merely allowing uij ∈[0, 1] (see,
for example, [19, 31]), the membership degrees are transformed with a convex map-
ping h ∶[0, 1] →[0, 1]. This yields an objective function of the form [19]
J(𝐗, 𝐂, 𝐔) =
c
∑
i=1
n
∑
j=1
h(uij) d2
ij.

318
C. Borgelt et al.
The clustering task now consists in ﬁnding for a given data set 𝐗and a user-
speciﬁed number of clusters c, cluster prototypes 𝐂and a partition matrix 𝐔such
that J(𝐗, 𝐂, 𝐔) is minimized under the constraints
∀j; 1 ≤j ≤n ∶
c
∑
i=1
uij = 1
and
∀i; 1 ≤i ≤c ∶
n
∑
j=1
uij > 0.
Unfortunately, cluster prototypes 𝐂and a partition matrix 𝐔that minimize J are diﬃ-
cult to ﬁnd by analytic means. Therefore one takes refuge to an alternating optimiza-
tion scheme: starting from randomly chosen cluster centers (for example, sampled
from the data set 𝐗), one iterates (1) updating the partition matrix for ﬁxed cluster
prototypes and (2) updating the cluster prototypes for a ﬁxed partition matrix until
convergence. Convergence may be checked with a limit for the change of the clus-
ter parameters (e.g. center coordinates) or a limit for the change of the membership
degrees from one iteration to the next.
In order to derive the update rule for the partition matrix (and thus for the mem-
bership degrees uij) we need to know the exact form of the function h. The most
common choice is h(uij) = u2
ij, which leads to the standard objective function of
fuzzy clustering [30]. The more general form h(uij) = uw
ij was introduced in [6]. The
exponent w, w > 1, is called the fuzziﬁer, since it controls the “fuzziness” of the
data point assignments: the higher w, the softer the boundaries between the clusters,
while a crisp partition results in the limit for w →1. This leads to the commonly
used objective function [4, 6, 7, 26]
J(𝐗, 𝐔, 𝐂) =
c
∑
i=1
n
∑
j=1
uw
ij d2
ij.
The update rule for the membership degrees is now derived by incorporating the
constraints ∀j; 1 ≤j ≤n ∶∑c
i=1 uij = 1 with Lagrange multipliers into the objective
function. (The second set of constraints, that is, ∀i; 1 ≤i ≤c ∶∑n
j=1 uij > 0 can
usually be neglected, because it is satisﬁed by the clustering result anyway.) This
yields the Lagrange function
L(𝐗, 𝐔, 𝐂, 𝛬) =
c
∑
i=1
n
∑
j=1
uw
ij d2
ij
⏟⏞⏞⏞⏞⏟⏞⏞⏞⏞⏟
=J(𝐗,𝐔,𝐂)
+
n
∑
j=1
𝜆j
(
1 −
c
∑
i=1
uij
)
,
where 𝛬= (𝜆1, … , 𝜆n) are the Lagrange multipliers, one per constraint.
Since a necessary condition for a minimum of the Lagrange function is that the
partial derivatives w.r.t. the membership degrees vanish, we obtain

Handling Noise and Outliers in Fuzzy Clustering
319
𝜕
𝜕ukl
L(𝐗, 𝐔, 𝐂, 𝛬) = w uw−1
kl
d2
kl −𝜆l
!= 0
and thus
ukl =
( 𝜆l
w d2
kl
) 1
w−1
.
Summing these equations over the clusters (in order to be able to exploit the cor-
responding constraints on the membership degrees, which are recovered from the
fact that it is a necessary condition for a minimum that the partial derivatives of the
Lagrange function w.r.t. the Lagrange multipliers vanish), we get
1 =
c
∑
i=1
uij =
c
∑
i=1
( 𝜆j
w d2
ij
) 1
w−1
and thus
𝜆j =
(
c
∑
i=1
(w d2
ij
) 1
1−w
)1−w
.
Therefore we ﬁnally have for the membership degrees ∀i; 1 ≤i ≤c: ∀j; 1 ≤j ≤n:
uij =
d
2
1−w
ij
∑c
k=1 d
2
1−w
kj
and thus for w = 2:
uij =
d−2
ij
∑c
k=1 d−2
kj
.
This rule is fairly intuitive, as it updates the membership degrees according to the
relative inverse squared distances of the data points to the cluster centers.
In order to derive the update rule for the cluster centers, we need to know the
(squared) distances d2
ij. The most common choice is the (squared) Euclidean distance,
that is, d2
ij = (⃗xj −⃗ci)⊤(⃗xj −⃗ci). With this choice, we can easily derive the update
rule for the cluster centers, namely by exploiting that a necessary condition for a
minimum of the objective function J is that the partial derivatives w.r.t. the cluster
centers vanish. Therefore we have ∀k; 1 ≤k ≤c ∶
∇⃗ckJ(𝐗, 𝐂, 𝐔) = ∇⃗ck
c
∑
i=1
n
∑
j=1
uw
ij (⃗xj −⃗ci)⊤(⃗xj −⃗ci)
= −2
n
∑
j=1
uw
ij (⃗xj −⃗ci)
!= 0.
Fig. 1
“Inﬂuence weight”
of a data point between two
cluster centers for diﬀerent
values of the fuzziﬁer w. The
two cluster centers are at the
left and the right border of
the diagram
1
0
inﬂuence weight
w →1
w = 2
w = 3
w = 4
w = 5

320
C. Borgelt et al.
It follows immediately ∀i; 1 ≤i ≤c ∶
⃗ci =
∑n
j=1 uw
ij ⃗xj
∑n
j=1 uw
ij
.
For the topic of this paper it is important to note that this update rule draws on the
transformed membership degrees uw
ij rather than on uij directly. As a consequence
the eﬀective “inﬂuence weight” of a data point on the cluster parameters is not 1 (as
one may be led to believe by the constraints ∀j; 1 ≤j ≤n ∶∑c
i=1 uij = 1), but rather
𝛼j = ∑c
i=1 uw
ij . It is 𝛼j = 1 only if the data point ⃗xj coincides with a cluster center (or
if w →1); otherwise it is 𝛼j < 1.
As an illustration, Fig. 1 shows, for c = 2 clusters, the inﬂuence weight of a data
point lying on a straight line connecting the two cluster centers: one cluster center is
at the left border of the diagram, the other at the right border. Clearly, for a fuzziﬁer
w > 1 the total inﬂuence weight 𝛼j = ∑c
i=1 uw
ij of a data point with a less ambiguous
assignment (that is, close to the left or right border of the diagram) is higher than
that of a more ambiguously assigned data point (in the middle of the diagram). Also,
this inﬂuence weight is the lower, the larger the fuzziﬁer. The minimum inﬂuence
weight is always obtained for equal distances (and thus equal membership degrees
uij = 1∕c) to all c clusters. In this case the inﬂuence weight of the data point is
𝛼= ∑c
i=1(1∕c)w = c ⋅c−w = c1−w.
Note that a unit data point weight is obtained only at the cluster centers or for the
limiting case of crisp clustering (that is, for w →1). This distinguishes fuzzy cluster-
ing from classical (crisp) clustering, where each data point has a unit inﬂuence (on
exactly one cluster). It also distinguishes the membership transformation approach to
fuzzy clustering from an approach that relies on membership regularization, since in
the latter the update rule for the cluster centers refers to untransformed membership
degrees (see, for instance, [31]), thus endowing each data point with a unit eﬀective
inﬂuence weight.
Due to the reduced inﬂuence weight that ambiguously assigned data points receive
in the membership transformation approach, the locations of the cluster centers
outlier
center with outlier
center without outlier
outlier
center with outlier
center without outlier
Fig. 2
Eﬀect of an outlier on the location of a cluster center that minimizes the sum of the squared
(left) and unsquared Euclidean distances (right)

Handling Noise and Outliers in Fuzzy Clustering
321
depend more strongly on those data points that are “typical” for the clusters. This
eﬀect can be desirable and is very much in the spirit of, for instance, robust regres-
sion techniques, in which data points also receive a lower weight if they are not
ﬁtted well by the regression function. This connection to robust statistical methods
was explored in more detail, for example, in [34, 35].
Despite this inherent robustness of fuzzy clustering, the inﬂuence of noisy data
points and outliers on the clustering result can still be too strong to yield suﬃciently
good clustering results. A core reason for this is that the standard objective function
is deﬁned in terms of sums of squared Euclidean distances. Due to this squaring
of distances, outliers can have an overly strong inﬂuence on the cluster parameters.
This is illustrated in Fig. 2 on the left, which shows six data points forming a cluster
(light gray circles at the left bottom) and one outlier (dark gray circle at the top
right). Computing the mean vector of the six data points forming the cluster—that
is, computing the point that minimizes the sum of the squared Euclidean distances to
the data points—yields a center vector that lies, as one would expect, in the middle
of this group of data points (lower left cross). However, if the outlier is included in
this mean computation, the cluster center is strongly pulled out of the cloud of the six
data points towards the outlier. The reason is, of course, that the large distance to the
outlier becomes even bigger by squaring and thus dominates the smaller (squared)
distances to the other six data points, producing an undesirable result.
Summarizing our discussion, we see that we can try to tackle noise and outliers in
fuzzy clustering in essentially two ways: (1) we can try to reduce the inﬂuence weight
of atypical data points even further than the membership transformation already does
(approaches based on this idea are studied in Sects. 3 to 6) or (2) we can change
or transform the distance measure in the objective function to reduce or eliminate
the deteriorating eﬀect of the squared distances (approaches based on this idea are
studied in Sects. 7 and 8).
3 Noise Clustering
The best known and most popular approach to handle noise and outliers in fuzzy
clustering is so-called noise clustering, which was ﬁrst proposed in [8], but received
attention only after it was independently developed again in [9]. The core idea of
this method is to introduce a pseudo-cluster, called the noise cluster, that has no
speciﬁc location, but rather the same distance 𝛿from all data points in 𝐗. Thus data
points that are far away from the actual clusters (in particular: farther away than the
noise distance 𝛿), receive a high degree of membership to the noise cluster. As a
consequence, the inﬂuence of noisy data points and outliers on the parameters of the
actual clusters is reduced, since the membership degrees to the actual clusters now
sum to a value that is the smaller, the higher the degree of membership to the noise
cluster.

322
C. Borgelt et al.
Formally, this leads to the objective function [8, 9]
J(𝐗, 𝐔, 𝐂) =
c
∑
i=1
n
∑
j=1
uw
ij d2
ij + 𝛿2
n
∑
j=1
uw
0j,
where the index i = 0 refers to the noise cluster. Of course, the ﬁrst set of constraints
now includes the noise cluster in the sum, that is, ∀j; 1 ≤j ≤n ∶∑c
i=0 uij = 1. As a
consequence, even the untransformed membership degrees to the actual clusters do
not sum to 1 anymore, but only to 1 −u0j, where
∀j; 1 ≤j ≤n ∶
u0j =
𝛿
2
1−w
𝛿
2
1−w + ∑c
i=1 d
2
1−w
ij
is the degree of membership of the data point ⃗xj to the noise cluster. Clearly, this
reduces the inﬂuence weight (in the sense of Sect. 2) of data points that are atypical
for the actual clusters and thus renders the result much more robust.
Of course, introducing a noise clusters raises the question of how to choose the
noise distance 𝛿. If 𝛿is (too) small, a large portion of the data set will receive a high
degree of membership to the noise cluster, possibly rendering the majority of the
data points noise and outliers. On the other hand, if 𝛿is chosen (too) large, member-
ship degrees to the noise cluster will remain small, possibly rendering its inﬂuence
negligible [36]. A proper choice depends on many aspects [37]: the amount of noise
present in the data set, the employed distance measure, the size of the feature space
(in terms of the range of possible values for the distance measure), the number c
of clusters to the found etc. In [9] it was suggested to compute the noise distance
(in each iteration) from the (unweighted) average distance of the data points to the
cluster prototypes as
𝛿2 = 𝜅
nc
c
∑
i=1
n
∑
j=1
d2
ij,
where 𝜅is user-speciﬁed factor that becomes the actual parameter.
An alternative to this basic approach consist in choosing the noise distance as the
(average) “cluster radius” that is derived from the requirement that the sum of the
hypervolumes of the clusters (as computed with this cluster radius) should equal the
size of the feature space (derived, for example, from the extreme data points) [37].
A good value of the noise distance may also be determined by trying multiple values
for 𝛿(starting at a large value and halving 𝛿in each step), computing the fraction p of
data points that have their highest degree of membership to the noise cluster (and thus
may be considered as being assigned to the noise cluster), ﬁtting the resulting points
(𝛿, p) with a Pareto-curve p = q𝛿−s and ﬁnding the point of this curve at which its

Handling Noise and Outliers in Fuzzy Clustering
323
slope is −1 [38]. Finally, a term may be added to the objective function that controls
what fraction of the data points can be expected to be noise or outliers, thus rendering
the method more robust against bad choices of the noise distance 𝛿[36].
4 Data Point Weights
As we have seen in the preceding section, noise clustering relaxes the constraints
∀j; 1 ≤j ≤n ∶∑c
i=1 uij = 1 somewhat by including the membership degree to the
noise cluster, due to which the membership degrees to the actual clusters can sum to
values less than 1. Alternatively, one may introduce an explicit data point weight and
adapt this weight in the optimization process [11], an approach which is also referred
to as outlier clustering. It permits that the membership degrees eﬀectively sum to
values less than 1 (namely to the data point weights) for atypical data points, while
for very typical data points they may even sum to values larger than 1, endowing
them with a greater inﬂuence on the clusters.
Outlier clustering is based on the objective function [11]
J(𝐗, 𝐔, 𝐂) =
c
∑
i=1
n
∑
j=1
uw
ij
v𝜃
j
d2
ij,
where vj is the weight of the data point ⃗xj and 𝜃is a constant that acts on the data
point weights in an analogous way as the fuzziﬁer w acts on the membership degrees.
A typical choice is therefore 𝜃= 2 (in analogy to the fuzziﬁer w).
To avoid the trivial solution in which all data point weights go to inﬁnity and
thus the value of the objective function becomes zero, a constraint analogous to the
constraints of the membership degrees is introduced, namely [11]
n
∑
j=1
vj = v.
With the natural choice v = n, the total weight n of the n data points is redistributed
to capture the typicality of the data points for the clusters. As an equally natural
alternative, one may choose v = n(1 −𝜌), where 𝜌is a user estimate of the fraction
of data points that are noise or outliers.
Note that the objective function contains (a function of) the reciprocal values
1∕vj of the data point weights, which produces exactly the desired eﬀect: in order
to minimize the objective function, large membership degrees will have to be com-
bined with large data point weights and small membership degrees with small data
point weights. Note also that with this approach the optimization scheme has three
steps: (1) optimize the data point weights for ﬁxed membership degrees and cluster
prototypes, (2) optimize the membership degrees for ﬁxed data point weights and

324
C. Borgelt et al.
cluster prototypes, and ﬁnally (3) optimize the cluster prototypes for ﬁxed data point
weights and membership degrees. Finally, note that for the last step the member-
ship degrees uij and the data point weights vj can be combined into membership
degrees ̃um
ij = um
ij ∕v𝜃
j , since both values are ﬁxed in this step. As a consequence, the
update rules for the cluster parameters are not aﬀected by using outlier clustering and
hence it can also be used, for example, with shape and size parameters for the clusters
(like, for instance, in [32, 33]) or other modiﬁcations of the cluster prototypes.
In order to derive the update rule for the data point weights vj, the same approach
is employed as it was demonstrated in Sect. 2 for the membership degrees. The con-
straint ∑n
j=1 vj = v is incorporated into the objective function with the help of a
Lagrange multiplier. Then the fact is exploited that at the minimum of the objective
function the partial derivatives w.r.t. the data point weights vj must vanish. In this
way we easily obtain [11] ∀j; i ≤j ≤n ∶
vj = v ⋅
(∑c
i=1 uw
ij d2
ij
)
1
𝜃+1
∑n
k=1
(∑c
i=1 uw
ik d2
ik
)
1
𝜃+1
,
which vanishes only if all clusters collapse to a single point. Using a threshold for
the data point weights vj one may ﬁnally identify data points as outliers.
5 Possibilistic Clustering
While the two approaches studied in Sects. 3 and 4 merely relax the constraints
∀j; 1 ≤j ≤n ∶∑c
i=1 uij = 1, by (implicitly or explicitly) allowing the member-
ship degrees to sum to values less than 1 (because the membership degree to the
noise cluster is deducted or an adaptable data point weight is introduced), possi-
bilistic (fuzzy) clustering [12, 13] is more radical and abandons these constraints
altogether, allowing the membership degrees to sum to arbitrary values. However,
this permits the trivial solution ∀i; 1 ≤i ≤c ∶∀j; 1 ≤j ≤n ∶uij = 0, which
obviously minimizes the objective function J(𝐗, 𝐔, 𝐂) = ∑c
i=1
∑n
j=1 uw
ij d2
ij, but, as is
equally obvious, is entirely useless.
To ﬁx this problem, a term is added to the objective function that drives the mem-
bership degrees away from zero, leading to [12]
J(𝐗, 𝐔, 𝐂) =
c
∑
i=1
n
∑
j=1
uw
ij d2
ij +
c
∑
i=1
𝜂i
n
∑
j=1
(1 −uij)w.
Here the 𝜂i are suitable positive numbers (one per cluster ⃗ci, 1 ≤i ≤c) that determine
the (squared) distance at which the membership degree of a point to a cluster is 0.5.

Handling Noise and Outliers in Fuzzy Clustering
325
They are usually initialized, based on the result of a preceding run of standard fuzzy
clustering, as the average fuzzy intra-cluster distance
𝜂i =
∑n
j=1 uw
ij d2
ij
∑n
j=1 uw
ij
and may or may not be updated in each iteration of the optimization process [12].
The membership degrees are then computed as
uij =
(
1 + (d2
ij∕𝜂i
)
1
w−1
)−1
.
It should be noted that the above objective function is truly optimized only if all clus-
ters are identical [39], because the missing constraints decouple the clusters (as can
be seen from the computation of the membership degrees). Possibilistic clustering
thus actually requires that the optimization process gets stuck in a local optimum
in order to yield useful results, which is a somewhat strange property. Although the
missing constraints certainly help dealing with outliers, this property limits the use-
fulness of a pure possibilistic approach, although the problem may be mitigated by
introducing cluster repulsion [39].
A ﬁrst solution to this problem was suggested in [14], which combined pos-
sibilistic and standard fuzzy clustering, where the latter is sometimes also called
probabilistic fuzzy clustering, because of the formal resemblance of the member-
ship degrees of a data point to probabilities, due to the constraints ∀j; 1 ≤j ≤n ∶
∑c
i=1 uij = 1. This approach works with the objective function
J(𝐗, 𝐔, 𝐂) =
c
∑
i=1
n
∑
j=1
(uw
ij + v𝜅
ij) d2
ij,
with the usual constraint for the membership degrees uij, but the constraints ∀i; 1 ≤
i ≤c ∶∑n
j=1 vij = 1 for the possibilistic typicality values vij. However, it turns out
that the membership degrees dominate this approach and since the typicality values
depend on the number n of data points, they become very small for large data sets.
As an improvement, in [15, 16] the objective function
J(𝐗, 𝐔, 𝐂) =
c
∑
i=1
n
∑
j=1
(auw
ij + bv𝜅
ij) d2
ij +
c
∑
i=1
𝜂i
n
∑
j=1
(1 −vij)𝜅
was proposed, which contains a second term that is characteristic for possibilistic
clustering. This leads to the usual (probabilistic) update rule for the membership
degrees uij, while the possibilistic typicality values are updated with

326
C. Borgelt et al.
vij =
(
1 + (bd2
ij∕𝜂i
)
1
𝜅−1
)−1
,
that is, like the membership degrees in possibilistic fuzzy clustering.
A fundamentally diﬀerent solution is the graded possibilistic approach presented
in [17], which allows for a smooth transition between possibilistic and probabilistic
fuzzy clustering. By drawing on an adequately relaxed form of the constraints ∀j; 1 ≤
j ≤n ∶∑c
i=1 uij = 1, data points can have a lower total inﬂuence weight (in the sense
of Sect. 2), but the cluster prototypes are still coupled and (thus) the trivial solution
(that is, ∀i, j ∶uij = 0) is avoided.
The class of constraints suggested in [17] is ∀j; 1 ≤j ≤n ∶∑c
i=1 u[𝜉]
ij = 1, where
[𝜉] = [𝜉∗, 𝜉∗] is an interval variable, with the natural restrictions 0 ≤𝜉∗≤1 and 1 ≤
𝜉∗. These generalized constraints are satisﬁed if for each j there exists a value 𝜉j ∈[𝜉]
such that ∑c
i=1 u
𝜉j
ij = 1. Note that standard probabilistic fuzzy clustering results as
a special case of this scheme for [𝜉] = [1, 1] and possibilistic fuzzy clustering for
[𝜉] = [0, ∞]. Note also that we may choose 𝜉∗= 𝛼and 𝜉∗=
1
𝛼with a single
parameter 𝛼∈[0, 1] as a natural simpliﬁcation.
With this approach the membership degrees are computed as uij = uij,◦∕𝜅j, where
uij,◦is a “free” or “raw” or unnormalized membership degree, as it results from stan-
dard possibilistic fuzzy clustering (see above) and [17]
𝜅j =
⎧
⎪
⎨
⎪⎩
( ∑c
i=1 u1∕𝛼
ij,◦
)𝛼
if ∑c
i=1 u1∕𝛼
ij,◦> 1,
( ∑c
i=1 u𝛼
ij,◦
)1∕𝛼
if ∑c
i=1 u𝛼
ij,◦< 1,
1
otherwise.
An extensive discussion of several formulations of this soft transition or graded pos-
sibilistic approach to fuzzy clustering can be found in [18].
6 Alternative Transformation
A disadvantage of the standard membership transformation approach to fuzzy clus-
tering, which relies on h(uij) = uw
ij (see Sect. 2), is that it always produces mem-
bership degrees. That is, regardless of how far away a data point is from a cluster
center, its membership degree never vanishes. This is one of the core reasons for the
negative inﬂuence of noise and outliers on fuzzy clustering results.
In order to allow some membership degrees to be zero, an alternative membership
transformation was suggested in [19]: h(uij) = 𝛼u2
ij + (1 −𝛼)uij, 𝛼∈(0, 1], or, with
a more easily interpretable parametrization, h(uij) = 1−𝛽
1+𝛽u2
ij +
2𝛽
1+𝛽uij, 𝛽∈[0, 1). It
relies on the standard transformation h(uij) = u2
ij and mixes it with the identity to
avoid a vanishing derivative at zero. The parameter 𝛽is, for two clusters, the ratio

Handling Noise and Outliers in Fuzzy Clustering
327
of the smaller to the larger squared distance, at and below which we get a crisp
assignment [19]. It therefore takes the place of the fuzziﬁer w: the smaller 𝛽, the
softer the boundaries between the clusters.
The update rule for the membership degrees is derived in essentially the same
way as for h(uij) = uw
ij , although one has to pay attention to the fact that crisp assign-
ments are now possible and thus some membership degrees may vanish. The detailed
derivation, which we omit here, can be found in [19, 26]. It yields
uij =
u′
ij
∑c
k=1 u′
kj
with
u′
ij = max
{
0, d−2
ij −
𝛽
1 + 𝛽(cj −1)
cj
∑
k=1
d−2
𝜍(k)j
}
,
where 𝜍∶{1, … , c} →{1, … c} is a mapping function for the cluster indices such
that ∀i; 1 ≤i < c ∶d𝜍(i)j ≤d𝜍(i+1)j (that is, 𝜍sorts the distances) and
cj = max
{
k ||||
d−2
𝜍(k)j >
𝛽
1 + 𝛽(k −1)
k
∑
i=1
d−2
𝜍(i)j
}
is the number of clusters to which the data point xj has a non-vanishing membership.
This update rule is fairly interpretable, as it still assigns membership degrees essen-
tially according to the relative inverse squared distances to the clusters, but subtracts
an oﬀset from them, which makes crisp assignments possible.
7 Unsquared Distances
Up to now we considered how fuzzy clustering can be made more robust by changing
the way in which data points are assigned to the clusters. Now we turn to the more
fundamental approach of changing how distances between the data points and the
clusters are measured.1 As explained in Sect. 2, one of the core reasons for outliers
having a strong inﬂuence on the cluster parameters is the use of squared Euclidean
distances. If we used unsquared Euclidean distances instead, the clustering algo-
rithm would become much more robust w.r.t. noise and outliers. This can be seen
clearly in the right diagram of Fig. 2: the outlier in the top right of the diagram has a
much weaker inﬂuence on the cluster center if it is computed as the point that mini-
mizes the sum of the unsquared Euclidean distances to the data points. Although the
center moves if the outlier is included in the computations, it stays much closer to
the center computed without the outlier and remains inside the group of data points
forming the cluster.
1Note that this approach is not restricted to fuzzy clustering, but can be applied for any clustering
scheme, including classical c-means clustering.

328
C. Borgelt et al.
However, a disadvantage of unsquared Euclidean distances is that the standard
approach of ﬁnding the cluster update rules as it was reviewed in Sect. 2 becomes
problematic, since the square is essential for obtaining (simple) derivatives. Several
solutions have been suggested to solve or circumvent this problem. In the ﬁrst place,
one may rely on a scheme as it was introduced for hard clustering with the c-medoids
algorithm [40]: instead of computing c cluster centers in the data space that minimize
the sum of the distances, one selects those c data points that have this property. This
is achieved by starting with a random selection of c data points as the initial cluster
centers and assigning, as in c-means clustering, each data point to the center that
is closest to it. Then it is tried to improve each cluster center in turn by replacing
it with a data point that is not currently a cluster center. The best replacement is
chosen and then another replacement is sought for improvement. The process stops
if no replacement of a cluster center reduces the sum of unsquared distances to the
data points.
This c-medoids approach has been transferred to fuzzy clustering, for example, in
[41] under the name “relational fuzzy c-means clustering” (RFCM) and in [3] under
the name FANNY (Fuzzy Analysis). The diﬀerence between the two approaches
consists merely in the fuzziﬁer used, which is ﬁxed to 2 in FANNY, but can take any
value greater than 1 in RFCM. An eﬃcient version for large data sets was proposed
in [42]. A combination of this scheme with the noise clustering approach studied in
Sect. 3 was presented in [43].
The restriction that in the c-medoids approach only data points can become cluster
centers can be removed by using so-called c-medians clustering [2]. Again, however,
the problem consists in ﬁnding the c (geometric) medians that minimize the sum of
the distances to the data points. This is easy only if instead of the Euclidean distance
another member of the Minkowski family of distance functions, namely the L1 dis-
tance, is used: dij = ∑m
k=1 |cik −xjk|. In this case the medians can be determined
separately in each of the m dimensions of the data space, reducing the problem to
trivial statistics in one dimension. 2
For any other member Lp, p ≥1, of the Minkowski family, an iterative majoriza-
tion scheme was suggested in [22]. This extends the core idea of [21], which intro-
duced an iterative majorization scheme for squared Minkowski distances with 1 ≤
p ≤2, after the special cases of the L1 distance and the L∞distance had been stud-
ied, for example, in [20] and [44]. The approach in [22] is even more general than
merely allowing unsquared distances from the Minkowski family. Rather it deﬁnes
the objective function as [22]
J(𝐗, 𝐔, 𝐂) =
c
∑
i=1
n
∑
j=1
uw
ij d 2𝜆
ij,p
with
d2𝜆
ij,p =
( m
∑
k=1
|cik −xjk|p)2𝜆
p ,
2Note that computing the membership degrees remains unchanged, regardless of the distance mea-
sure and whether it is squared or not, because for this computation the cluster prototypes are ﬁxed
and thus the distances are eﬀectively constants.

Handling Noise and Outliers in Fuzzy Clustering
329
where p ≥1 is the parameter that selects the member of the Minkowski family of
distance functions and the parameter 𝜆, 0 ≤𝜆≤1, allows to make the clustering
algorithm robust by choosing a small value for 𝜆. For example, p = 2 and 𝜆= 1
2
specify the most interesting case of unsquared Euclidean distances.
Intuitively, the iterative majorization procedure consists in ﬁnding, for the current
state of the cluster prototypes, a suﬃciently simple auxiliary function majorizing the
actual objective function. That is, this auxiliary function touches the objective func-
tion at the current cluster prototypes and is nowhere smaller than the objective func-
tion. Furthermore, it should be easy to ﬁnd the optimum of this majorizing function,
so that one can jump to this optimum in a single step, obtaining new cluster proto-
types. Then a new majorizing function is constructed for the new prototypes and the
process is iterated until convergence. Presenting mathematical details of this scheme
is beyond of the scope of this paper, though. An interested reader is referred to [22],
which provides an extensive treatment.
8 Transformed Distances
Instead of using one of the approaches discussed in the preceding section, one may
also stick with the (squared or unsquared) Euclidean distance and modify the dis-
tance computation or transform the distance measure before computing the mem-
bership degrees to increase robustness. One of the most straightforward approaches
in this direction is to use an 𝜀-insensitive distance function [24]. It contains the c-
medians approach that was mentioned in the preceding section as a special case (for
𝜀= 0), because it employs the objective function [24]
J(𝐗, 𝐔, 𝐂) =
c
∑
i=1
n
∑
j=1
uw
ij dij,𝜀
with
dij,𝜀=
p
∑
k=1
max{0, |xjk −cik| −𝜀},
where 𝜀is the user-speciﬁed insensitivity parameter. The update rules for the mem-
bership degrees and the cluster centers can be derived in a fairly standard fash-
ion from this objective function (using Lagrange multipliers to incorporate the
constraints and partial derivatives), but as the result is mathematically somewhat
involved, we do not reproduce it here, but refer an interested reader to [24].
Note that the idea of an 𝜀-insensitive distance function is essentially to give a
larger weight to typical data points, since the points in the 𝜀-vicinity of a cluster
center are assigned crisply (i.e. uij = 1) to this cluster center, unless such a data
point has a vanishing 𝜀-insensitive distance from multiple cluster centers, in which
case equal membership degrees to all of these clusters are chosen. Together with the
employed unsquared Manhattan distance, this considerably increases the robustness
of the algorithm w.r.t. noise and outliers. This eﬀect is particularly pronounced if a
larger fuzziﬁer is employed (compare Fig. 1, even though this ﬁgure refers to squared
Euclidean distances).

330
C. Borgelt et al.
A more general alternative consist in exploiting the idea of robust estimators
(especially M-estimators, cf. [45]) as in [25], which uses the objective function
J(𝐗, 𝐔, 𝐂) =
c
∑
i=1
n
∑
j=1
uw
ij 𝜌i(dij),
where the 𝜌i, 1 ≤i ≤c, are robust symmetric positive deﬁnite functions having
their minimum at 0 (with 𝜌i(dij) = dw
ij as a special case). In [25] the same function 𝜌
is used for all clusters, which is derived from Tukey’s bisquare function [45]. This
leads to update rules for the membership degrees, in which merely the distances are
replaced by 𝜌(dij), while the cluster centers are updated with
⃗ci =
∑n
j=1 uw
ij fij⃗xj
∑n
j=1 uw
ij fij
where
fij =
d𝜌(dij)
d dij
.
An even more general, but closely related approach is the alternating cluster esti-
mation (ACE) scheme that was proposed in [23] (see also [4]). The idea of this
approach is to abandon the requirement of an objective function that is to be opti-
mized and from which the update rules can be derived. Rather the alternating opti-
mization scheme is taken as the core algorithmic component, for which plausible
update rules are chosen for the two steps of recomputing the membership degrees
and recomputing the cluster parameters.
In its most common form, such an approach ﬁrst transform the distances dij with
a radial function r ∶ℝ→[0, 1] to obtain “free” or “raw” membership degrees
r(dij) to the clusters. These raw membership degrees may then be normalized using,
for instance, the constraints ∀j; 1 ≤j ≤n ∶∑c
i=1 uij = 1. Typical choices for the
radial functions (the name of which stems from the fact that they are deﬁned on a
ray—latin: radius—from the cluster center), are shown in Fig. 3. Especially those
radial functions that have a ﬁnite support (that is, for which exists x0 ∈ℝ+ with
∀x > x0 ∶r(x0) = 0) are well suited for handling noise and outliers, because data
points with a distance outside the support of the radial function have a vanishing
inﬂuence on the corresponding cluster.
x
1
σ
triangular
x
1
σ
cosine
down to 0
x
1
σ
Cauchy
x
1
σ
Gauss
Fig. 3
Radial functions that may be used in alternating cluster estimation (ACE) [23]

Handling Noise and Outliers in Fuzzy Clustering
331
The update rules for this scheme are simply (assuming merely cluster centers)
uij =
r(dij)
∑c
k=1 r(dkj)
and
⃗ci =
∑n
j=1 uw
ij ⃗xj
∑n
j=1 uw
ij
.
Generally, these update rules cannot be derived from an objective function (as
shown in Sect. 2 for the standard case), but are merely transferred from the standard
approach. It should be noted, though, that for certain radial functions, for example,
the (generalized) Gaussian and the Cauchy function
rGauss(x) = e−1
2 ra
and
rCauchy(x) =
1
xa + b,
where a and b are parameters to be speciﬁed by a user, a formulation with the help of
an objective function is possible, so that the needed update rules can be obtained in
the usual way (using Lagrange multipliers to incorporate the constraints and setting
partial derivatives equal to 0, see [26] for details).
A noteworthy alternative, which also relies on an ACE scheme instead of deriving
the update equations for the membership degrees and cluster parameters from an
objective function, is to compute the membership degrees as uij = ((maxk dik) −
dij)∕maxk dik [46]. In this way the degree of membership of the data point that is
farthest from a cluster center always vanishes, which has the additional advantage
that it renders the membership degrees independent of the scale of the data set. Note
that it is closely related to a possibilistic approach, because it is usually not ∀j; 1 ≤
j ≤n ∶∑c
i=1 uij = 1.
9 Summary
In this paper we reviewed several approaches to make fuzzy clustering (even) more
robust against noise and outliers. The studied approaches fall into two categories:
(1) reduce the “inﬂuence weight” of atypical data points and outliers on the cluster
parameters by changing how membership degrees are computed from the distances,
and (2) change the distance function or transform it before the membership com-
putation in order to reduce the degrees of memberships of atypical data points and
outliers. Approaches in the former category are usually easier to handle, because
in them the update rules are fairly easily obtained from an objective function using
standard tools. Changing the distance measure causes more problems in this respect
and thus often either a majorization approach has to be called upon or the rooting
in an objective function is abandoned as in alternating cluster estimation (ACE).
However, all of these approaches have the desired eﬀect of making fuzzy cluster-
ing (even) more robust. Our personal favorites are noise clustering (see Sect. 3) and
using an alternative membership transformation (see Sect. 6). However, this should
not be interpreted as a recommendation against any of the other approaches.

332
C. Borgelt et al.
References
1. Everitt, B.S.: Cluster Analysis. Heinemann, London (1981)
2. Jain, A.K., Dubes, R.C.: Algorithms for Clustering Data. Prentice Hall, Englewood Cliﬀs
(1988)
3. Kaufman, L., Rousseeuw, P.: Finding Groups in Data: An Introduction to Cluster Analysis.
Wiley, New York (1990)
4. Höppner, F., Klawonn, F., Kruse, R., Runkler, T.: Fuzzy Cluster Analysis. Wiley, Chichester
(1999)
5. Ruspini, E.H.: A new approach to clustering. Inf. Control 15(1), 22–32 (1969). Reprinted in
[47], 63–70 (Academic Press, San Diego)
6. Bezdek, J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Plenum Press,
New York (1981)
7. Bezdek, J.C., Keller, J., Krishnapuram, R., Pal, N.: Fuzzy Models and Algorithms for Pattern
Recognition and Image Processing. Kluwer, Dordrecht (1999)
8. Ohashi, Y.: Fuzzy clustering and robust estimation. In: Proceedings 9th Meeting SAS Users
Group International Hollywood Beach, FL, USA (1984)
9. Davé, R.N.: Characterization and detection of noise in clustering. Pattern Recogn. Lett. 12,
657–664 (1991) (Elsevier Science, Amsterdam)
10. Davé, R.N., Sen, S.: On generalizing the noise clustering algorithms. In: Proceedings 7th
International Fuzzy Systems Association World Congress (IFSA’97), 3, 205–210. Academia,
Prague, Czech Republic (1997)
11. Keller, A.: Fuzzy clustering with outliers. In: Proceedings 19th Conference North American
Fuzzy Information Processing Society (NAFIPS’00, Atlanta, Canada), pp. 143–147. IEEE
Press, Piscataway, NJ, USA (2000)
12. Krishnapuram, R., Keller, J.M.: A possibilistic approach to clustering. IEEE Trans. Fuzzy Syst.
1(2) , 98–110 (1993) (IEEE Press, Piscataway)
13. Krishnapuram, R., Keller, J.M.: The possibilistic c-means algorithm: insights and recommen-
dations. IEEE Trans. Fuzzy Syst. 4(3), 385–393 (1996) (IEEE Press, Piscataway)
14. Pal, N.R., Pal, K., Bezdek, J.C.: A mixed C-means clustering model. In: Proceedings 6th IEEE
International Conference on Fuzzy Systems (FUZZIEEE’97, Barcelona, Spain), pp. 11–21.
IEEE Press, Piscataway, NJ, USA (1997)
15. Pal, N.R., Pal, K., Keller, J.M., Bezdek, J.C.: A new hybrid C-means clustering model. In: Pro-
ceedings 13th IEEE International Conference on Fuzzy Systems (FUZZIEEE’04, Budapest,
Hungary), pp. 179–184. IEEE Press, Piscataway, NJ, USA (2004)
16. Pal, N.R., Pal, K., Keller, J.M., Bezdek, J.C.: A possibilistic fuzzy C-means clustering algo-
rithm. IEEE Trans. Fuzzy Syst. 13(4), 517–530 (2005) (IEEE Press, Piscataway)
17. Masulli, F., Rosetta, S.: Soft transition from probabilistic to possibilistic fuzzy clustering. IEEE
Trans. Fuzzy Syst. 14(4), 516–527 (2006) (IEEE Press, Piscataway)
18. Honda, K., Ichihashi, H., Notsu, A., Masulli, F., Rovetta, S.: Formulations, several, for graded
possibilistic approach to fuzzy clustering. In: Proceedings 5th International Conference Rough
Sets and Current Trends in Computing (RSCTC, : Kobe, Japan), pp. 939–948. Springer-Verlag,
Berlin/Heidelberg, Germany (2006)
19. Klawonn, F., Höppner, F.: What is fuzzy about fuzzy clustering? understanding and improving
the concept of the fuzziﬁer. In: Proceedings 5th International Symposium on Intelligent Data
Analysis (IDA: Berlin, Germany), pp. 254–264. Springer-Verlag, Berlin, Germany (2003)
20. Jajuga, K.: L1-norm based fuzzy clustering. Fuzzy Sets Syst. 39(1), 43–50 (1991) (Elsevier
Science, Amsterdam)
21. Groenen, P.J.F., Jajuga, K.: Fuzzy clustering with squared minkowski distances. Fuzzy Sets
Syst. 120, 227–237 (2001) (Elsevier Science, Amsterdam)
22. Groenen, P.J.F., Kaymak, U., van Rosmalen, J.: Fuzzy clustering with minkowski distance
functions. In: Chapter 3 of Valente de Oliveira, J., Pedrycz, W. (eds.) Advances in Fuzzy Clus-
tering and Its Applications. Wiley, Chichester (2007)

Handling Noise and Outliers in Fuzzy Clustering
333
23. Runkler, T.A., Bezdek, J.C.: Alternating cluster estimation: a new tool for clustering and func-
tion approximation. IEEE Trans. Fuzzy Syst. 7(4), 377–393 (1999) (IEEE Press, Piscataway)
24. Łe¸ski, J.: An 𝜀-insensitive approach to fuzzy clustering. Int. J. Appl. Math. Comput. Sci. 11(4),
993–1007 (2001) (University of Zielona Góra, Poland)
25. Frigui, H., Krishnapuram, R.: A robust algorithm for automatic extraction of an unknown num-
ber of clusters from noisy data. Pattern Recogn. Lett. 17, 1223–1232 (1996) (Elsevier Science,
Amsterdam)
26. Borgelt, C.: Prototype-based Classiﬁcation and Clustering. Otto-von-Guericke-University of
Magdeburg, Germany, Habilitationsschrift (2005)
27. Ball, G.H., Hall, D.J.: A clustering technique for summarizing multivariate data. Behav. Sci.
12(2), 153–155 (1967) (Wiley, Chichester)
28. Hartigan, J.A., Wong, M.A.: A k-means clustering algorithm. Appl. Stat. 28, 100–108 (1979)
(Blackwell, Oxford)
29. Lloyd, S.: Least squares quantization in PCM. IEEE Trans. Inf. Theory 28, 129–137 (1982)
(IEEE Press, Piscataway)
30. Dunn, J.C.: A fuzzy relative of the ISODATA process and its use in detecting compact well-
separated clusters. J. Cybern. 3(3), 32–57 (1973). Reprinted in [47], 82–101 (American Society
for Cybernetics, Washington)
31. Borgelt, C.: Objective functions for fuzzy clustering. In: Moewes, C., Nürnberger, A. (eds.)
Computational Intelligence in Intelligent Data Analysis, 3–16. Springer, Berlin/Heidelberg
(2012)
32. Gustafson, E.E., Kessel, W.C.: Fuzzy clustering with a fuzzy covariance matrix. In: Proceed-
ings of the IEEE Conference on Decision and Control (CDC 1979, San Diego, CA), pp. 761–
766. IEEE Press, Piscataway, NJ, USA (1979). Reprinted in [47], 117–122
33. Gath, I., Gevam, A.B.: Unsupervised optimal fuzzy clustering. IEEE Trans. Pattern Anal.
Mach. Intell. (PAMI) 11, 773–781 (1989). Reprinted in [47], 211–218 (IEEE Press, Piscat-
away)
34. Davé, R.N., Krishnapuram, R.: Robust clustering methods: a uniﬁed view. IEEE Trans. Fuzzy
Syst. 5, 270–293 (1997) (IEEE Press, Piscataway)
35. Davé, R.N., Sumit, S.: Generalized noise clustering as a robust fuzzy C-M-estimators model.
In: Proceedings 17th Annual Conference North American Fuzzy Information Processing Soci-
ety (NAFIPS’98, Pensacola Beach, Florida), pp. 256–260. IEEE Press, Piscataway, NJ, USA
(1998)
36. Klawonn, F.: Noise clustering with a ﬁxed fraction of noise. In: Lotﬁ, A., Garibaldi, M. (eds.)
Applications and Science in Soft Computing, 133–138. Springer, Berlin/Heidelberg (2004)
37. Rehm, F., Klawonn, F., Kruse, R.: A novel approach to noise clustering for outlier detection.
Soft Comput. 11(5), 489–494. Springer, Berlin/Heidelberg (2007)
38. Cimino, M.G.C.A., Frosini, G., Lazzerini, B., Marcelloni, F.: On the noise distance in robust
fuzzy C-means. In: Proceedings International Conference on Computational Intelligence
(ICCI, : Istanbul, Turkey), pp. 361–364. Intelligence Society, International Compliance (2004)
39. Timm, H., Borgelt, C., Döring, C., Kruse, R.: An extension to possibilistic fuzzy cluster analy-
sis. Fuzzy Sets Syst. 147, 3–16 (2004) (Elsevier Science, Amsterdam)
40. Rousseeuw, P.J., Leroy, A.M.: Robust Regression and Outlier Detection. Wiley, Chichester
(1987)
41. Hathaway, R.J., Devenport, J.W., Bezdek, J.C.: Relational dual of the C-means clustering algo-
rithm. Pattern Recogn. 22(2), 205–212 (1989) (Elsevier, Amsterdam)
42. Krishnapuram, R., Joshi, A., Yi, L.: A fuzzy relative of the K-medoids algorithm with appli-
cation to document and snippet clustering. In: Proceedings 8th IEEE International Conference
on Fuzzy Systems (FUZZ-IEEE’99, Seoul, Korea), 3, 1281–1286. IEEE Press, Piscataway, NJ,
USA (1999)
43. Sen, S., Dave, R.N.: Clustering of relational data containing noise and outliers. In: Proceedings
7th IEEE International Conference on Fuzzy Systems (FUZZ-IEEE’98, Anchorage, Alaska),
3, 1411–1416. IEEE Press, Piscataway, NJ, USA (1998)

334
C. Borgelt et al.
44. Bobrowski, L., Bezdek, J.C.: C-means clustering with the L1 and L∞norms. IEEE Trans. Syst.
Man Cybern. 21(3), 545–554 (1991) (IEEE Press, Piscataway)
45. Hampel, F.R., Ronchetti, E.M., Rousseeuw, P.J., Stahel, W.A.: Robust Statistics: The Approach
Based on Inﬂuence Functions. Wiley, New York (1986)
46. Binu, T., Raju, G.: A novel fuzzy clustering method for outlier detection in data mining. Int. J.
Recent Trends Eng. 1(2), 161–165 (2009) (Academy Publisher, British Virgin Islands)
47. Bezdek, J.C., Pal, N.R.: Fuzzy Models for Pattern Recognition. IEEE Press, New York (1992)
Authors Biography
Christian Borgelt obtained his PhD in 2000 from the Univer-
sity of Magdeburg, Germany, and the venia legendi for com-
puter science in 2006. Since 2006 he is a principal researcher at
the European Center for Soft Computing in Mieres (Asturias),
Spain, where he leads the Intelligent Data Analysis and Graph-
ical Models Research Unit. His research interests include fre-
quent pattern mining, association rules, clustering algorithms,
decision and regression trees, graphical models, neural net-
works as well as other soft computing, computational intelli-
gence and intelligent data analysis methods.
Christian Braune obtained his masters degree in 2012 from
the Otto-von-Guericke University, Magdeburg, Germany. Since
then he is working on his PhD in the Computational Intelli-
gence group with Rudolf Kruse. His research interests are the
analysis of parallel point processes, clustering and data analysis
in general.

Handling Noise and Outliers in Fuzzy Clustering
335
Marie-Jeanne Lesot obtained her PhD in 2005 from the Uni-
versity Pierre et Marie Curie in Paris, France. Since 2006 she
is an associate professor in the department of Computer Sci-
ence Lab of Paris 6 (LIP6), France, and member of the Learn-
ing and Fuzzy Intelligent systems (LFI) group. Her research
interests focus on fuzzy machine learning with an objective of
data interpretation and semantics integration and, in particular,
to model and manage subjective information; they include sim-
ilarity measures, fuzzy clustering, linguistic summaries, aﬀec-
tive computing and information scoring.
Rudolf Kruse obtained his PhD in 1980 from the University of
Braunschweig, Germany, and the venia legendi for mathematics
in 1984. Following a short stay at the Fraunhofer Gesellschaft,
he joined the University of Braunschweig as a professor for
computer science in 1986. Since 1996 he is a full professor for
computer science at the University of Magdeburg, Germany,
where he leads the computational intelligence research group.
He has carried out research and projects in statistics, artiﬁcial
intelligence, expert systems, fuzzy control, fuzzy data analy-
sis, computational intelligence, and information mining. He is a
fellow of the International Fuzzy Systems Association (IFSA),
fellow of the European Coordinating Committee for Artiﬁcial
Intelligence (ECCAI) and fellow of the Institute of Electrical
and Electronics Engineers (IEEE).

A Fuzzy-Based Approach to Survival Data
Mining
Mark Last and Hezi Halpert
Abstract Traditional data mining algorithms assume that all data on a given object
becomes available simultaneously (e.g., by accessing the object record in a data-
base). However, certain real-world applications, known as survival analysis, or
event history analysis (EHA), deal with monitoring speciﬁc objects, such as medical
patients, in the course of their lifetime. The data streams produced by such appli-
cations contain various events related to the monitored objects. When we observe
an inﬁnite stream of events, at each point in time (the “cut-off point”), some of the
monitored entities are “right-censored”, since they have not experienced the event
of interest yet and we do not know when the event will occur in the future. In
snapshot monitoring, the data stream is observed as a sequence of periodic snap-
shots. Given each snapshot, we are interested to estimate the probability of a critical
event (e.g., patient death or equipment failure) as a function of time for every
monitored object. In this research, we use fuzzy class label adjustment so that
standard classiﬁcation algorithms can seamlessly handle a snapshot stream of both
censored and non-censored data. The objective is to provide reasonably accurate
predictions after observing relatively few snapshots of the data stream and to
improve the classiﬁcation performance with additional information obtained from
each incoming snapshot. The proposed fuzzy-based methodology is evaluated on
real-world snapshot streams from two different domains of survival analysis.
1
Introduction
Survival analysis, or event history analysis (EHA), deals with monitoring speciﬁc
objects/entities in the course of their lifetime [1]. The data streams produced by the
monitored objects contain various events that occur to those objects. In survival
analysis, we are usually interested in estimating the amount of time before the
M. Last (✉) ⋅H. Halpert
Department of Information Systems Engineering, Ben-Gurion University of the Negev,
84105 Beer-Sheva, Israel
e-mail: mlast@bgu.ac.il
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_18
337

occurrence of an event or in the probability of event occurrence during certain
period. The following are some common applications of survival/event history
analysis:
• Health Care. The objects are patients who underwent a medical procedure (like
surgery) and the event of interest may be disease recurrence or patient death
within a certain follow-up period.
• Warranty Management. The objects are products and the event of interest is a
product failure during its warranty period.
• Customer Retention. The objects are service customers and the event of interest
is customer churn.
Since each monitored object is typically described by a set of features (e.g.,
clinical variables in case of a patient), the problem of estimating the time to event
occurrence may be deﬁned as a regression task. Similarly, the problem of pre-
dicting the object survival (non-occurrence of an event) within a pre-deﬁned
follow-up period (e.g., product warranty period) may be considered a classiﬁcation
task with two possible outcomes – “survived” and “failed”. To induce a regression
or a classiﬁcation model, we need a training set of objects with a known value of
the dependent variable or the class label, respectively. In snapshot monitoring, such
a training set may be obtained every time we take a “snapshot” of a data stream
produced by a set of monitored objects, which may be continuously updated by new
objects entering the observation process. The problem is that in each snapshot,
some objects may not have experienced the event of interest or completed the
follow-up period until the time of the snapshot (the “cut-off point”). These
incompletely observed objects are termed “right-censored”, since their time to event
is longer than the time to the current snapshot but we do not know whether they will
experience the event before the end of their follow-up period. On the other hand, we
do know how long these unlabeled objects have survived from the beginning of
their follow-up period (called the “birth event”) to the time of the current snapshot.
In this chapter, we focus on the classiﬁcation task in snapshot streams, where
each snapshot may contain both censored and non-censored observations. The
proposed CENSMINER (CENsored data Stream MINing) algorithm uses a fuzzy
class label adjustment [2] so that standard classiﬁcation algorithms can be trained
on all objects observed in each snapshot rather than just the objects with a known
label. The CENSMINER objective is to provide reasonably accurate predictions
after observing the ﬁrst few snapshots of a data stream and to improve the classi-
ﬁcation performance with additional labeled objects observed in each incoming
snapshot.
The rest of this chapter is organized as follows. Section 2 discusses the related
work on survival analysis and mining censored data. The CENSMINER method-
ology for mining censored data streams is introduced in Sect. 3. In Sect. 4, we
evaluate the effectiveness of the proposed methodology on two real-world snapshot
streams from the warranty and the medical domains, respectively. Section 5 pre-
sents some conclusions and directions for future research.
338
M. Last and H. Halpert

2
Related Work
2.1 Survival Analysis
Moeschberger and Klein in [3] review the ﬁeld of survival analysis and its rela-
tionship to censored data. Traditionally, examples of survival data come from the
medical domain, where the event of interest is usually a disease recurrence or a
patient’s death. An event of interest may also be a positive event, such as a full
recovery from some disease, conception, smoking cessation, and so forth. Usually,
the event of interest is related to a speciﬁc object (e.g., a patient).
The opposite of the death event of an object in survival analysis is the birth
event. The birth event of an object denotes the time point when the object starts
being monitored. For example, in the medical ﬁeld, the birth event may represent
the starting point of a particular treatment. In warranty data, the birth event is
usually the sale event, which indicates the beginning of the product usage.
The term follow-up period, or the prediction period, is used to denote the period
for which the survival probability is of interest, such as the warranty period in
product warranty management. The term cut-off point represents the time point
when the monitoring of all objects has been stopped. Usually, this point is the end
of the experiment. A continuous data stream may be sampled periodically, resulting
in multiple cut-off points, each representing a new snapshot.
The snapshot monitoring approach is more suitable for cases where the change
rate of the monitored stream is relatively slow. This allows periodically taking
discrete samples (“snapshots”) of the data stream and producing a prediction model
from each incoming snapshot. Whenever there is no high rate of changes in the set
of monitored objects, the periodic, snapshot monitoring is preferable over the
continuous, event-driven monitoring, which requires more computational resources.
Additionally, for each sampled object, the time elapsed from its birth event to the
event of death or to the cut-off point is deﬁned as the object’s lifetime (or its age).
For example, in warranty data, the lifetime of a product is the usage time elapsed
since it has been sold.
Figure 1 describes these concepts and their relations.
Two basic concepts in the survival analysis are the survival function and the
hazard function. Assuming X is a nonnegative random variable from a
Fig. 1 Terms in survival analysis
A Fuzzy-Based Approach to Survival Data Mining
339

homogeneous population that represents the age of an individual, the survival
function is the probability of an individual to survive beyond time x. The hazard
rate (or hazard/risk function) is the conditional probability that an individual of age
x will experience the event in the next instant after x, given that the event has not
occurred until x. When X is a discrete random variable, the following equation
describes the relationship between these two functions:
S x
ð Þ = ∏
xj ≤x
1 −λ xj
 


where S(x) is the survival function, the probability of an ndividual of age x to
survive, and λ xj
 
is the hazard function, the “approximate” probability of an
individual of age xj to experience the event in the next instant.
2.2 Survival Probability Estimation from Censored Data
Censored data is very common in the survival analysis, especially when
inter-snapshot times tend to be signiﬁcantly shorter than the overall follow-up
period. Thus, a problem of censored data frequently arises in clinical trials where a
substantial fraction of surviving participants may be removed from the study to
reduce the monitoring costs. Patients with incomplete outcome information are
considered censored or, more precisely, right-censored, since we only know that the
actual time-to-event for that participant exceeds the duration of their follow-up [4].
2.2.1 The Kaplan–Meier Estimator
The Kaplan-Meier method (also called the product-limit estimator) is aimed at
estimating the survival function, i.e., the probability for an individual in a popu-
lation to survive beyond a speciﬁc time. This method was ﬁrst proposed by Böhmer
in 1912 and rediscovered by Kaplan and Meier in 1958 [5]. The Kaplan-Meier
method provides a non-parametric estimation from incomplete observations. The
term observed lifetime refers to the time interval from the birth event of each object
(e.g., a surgery or a product purchase) to its failure or “death” event. The survival
probability is estimated separately for each observed lifetime value in the dataset. It
is also assumed that the probability of surviving each lifetime ti is statistically
independent of every other lifetime. The Kaplan-Meier estimate of the survival
curve Ŝ T
ð Þ is then:
Ŝ T
ð Þ≈∏
ti ≤T
n ti
ð Þ −di
n ti
ð Þ
340
M. Last and H. Halpert

where n ti
ð Þ is the number of objects “at risk” (i.e., not lost through censoring or
failure) at time ti and di is the number of objects which failed at time ti. Usually
di = 1, since ti is chosen as the observed time to failure.
Costella [6] points out some of the disadvantages of the Kaplan-Meier method:
1. The “danger times” deﬁned by the failure occurrence times and represented by
vertical drops in the Kaplan-Meier estimation seem “unnatural” compared to the
original survival curve. The actual survival curve is probably continuous rather
than discrete and is unlikely to decrease locally at those particular failure
occurrence times.
2. With the advance of the monitoring time, there are fewer and fewer remaining
objects at risk (due to censoring and failures). As a result, the impact of a single
failure seems unjustiﬁably magniﬁed if it occurs at a later time.
3. If the last remaining object at risk fails, the Kaplan-Meier estimate of S(t) drops
to zero and does not generally reﬂect the original survival function.
4. For each censored record, the amount of survival time from the previous failure
to the time of censoring is ignored.
In [6], Costella suggests an alternative method to the Kaplan-Meier method,
which offers a better visual representation of survival curves. Another solution for
the fourth problem above (i.e., ignoring the amount of the survival time from the
previous failure to the time of censoring) is to weigh the censored records according
to their relative time of survival. This way the information is not discarded and can
be taken into account when calculating the survival (or failure) probability. This
approach was used in our previous work with warranty data [7].
2.2.2 The Cox Proportional Hazard Models
The Kaplan-Meier estimation is a simple concept: estimate the survival curve when
there are no other covariates than the usage factor (e.g., patient age or vehicle
mileage). Other models are designed to handle covariates as well. D. R. Cox [8]
introduced the semi-parametric proportional hazard model, a continuous statistical
model for time-to-event data. In his approach, the model’s covariates are multi-
plicatively related to the hazard function such that the effect of a unit increase in a
covariate is multiplicative with respect to the hazard rate.
The Cox model contains two different parts: the underlying hazard function
denoted h0 tð Þ (also called the baseline hazard function), which describes how the
hazard (e.g., failure rate) changes over time when setting the covariate values to zero
and the other covariate parameters, which describe how the hazard varies in response
to explanatory covariates. The general form of the Cox model is as follows:
h tjx
ð
Þ = h0 tð Þ * expðxβÞ, where β is the parameter vector. The Cox model is
semi-parametric, since its baseline hazard function h0 tð Þ can be modeled by any
probability distribution function, such as the Gompertz and Weibull distributions (the
nonparametric part), whereas the covariate coefﬁcients must be estimated using the
partial maximum likelihood (the parametric part).
A Fuzzy-Based Approach to Survival Data Mining
341

2.3 Mining Censored Data
2.3.1 Static Datasets
Segal [9] introduced a new approach to splitting criterion in regression trees
induced from datasets with censored target variables. In each node, the
Kaplan-Meier (i.e., survival curve estimation for censored data, see Sect. 2.2.1
above) and its median are ﬁrst stored. Then, all predictive attributes and all splitting
points within an attribute are considered, subject to a minimal ratio of uncensored to
censored records in each child node. The best split is the attribute and the splitting
point, which maximize the between-node separation. In addition to the regression
tree induction itself, this method also has the advantage of presenting informative
characteristics for each terminal node, such as the estimated Kaplan-Meier survival
curve and the Kaplan-Meier median [9].
Zupan et al. [10] deal with predicting the probability of prostate cancer recur-
rence in patients after prostate removal. It is assumed that if a prostate cancer patient
who has undergone a prostatectomy remains disease free for at least 7 years then the
cancer has been successfully cured. In this case, the censored data comes from
non-recurring patients who have been monitored for less than 7 years and whose
outcome at the end of the 7-year period remains uncertain.
Zupan’s suggested approach is to split the prostate cancer survival data into three
groups: patients who experienced a recurrence of cancer (i.e., the outcome is known
to be “recurrence”), patients who did not experience a recurrence and were mon-
itored for more than 7 years after their operation (i.e., the outcome is known to be
“non-recurrence”), and patients who have not experienced a recurrence but have
been monitored for less than 7 years (the outcome is censored). For the third group,
instead of a single outcome, two outcomes are considered: recurrence and
non-recurrence. Each outcome is weighted using the Kaplan-Meier method, which
estimates the probability of non-recurrence at a particular follow-up time based on
all patient groups. For a patient’s follow-up time Tf from the third group, the weight
of the non-recurrence outcome is calculated by the following formula, where
P non −recurrence tð Þ
ð
Þ is the Kaplan-Meier estimation for non-recurrence proba-
bility at a time t:
Prf Tf


= P non −recurrence 7 years
ð
Þ
ð
Þ
P non −recurrence Tf




The weight of the recurrence outcome is: Pr = 1 −Prf .
Since most machine-learning techniques cannot be trained on probabilistic
outcomes, two copies of patient’s record are created, one weighted with Pr values
and the other one with Prf values. The probability of class C (recurrence or
non-recurrence) is then denoted by: P C
ð Þ = ∑E ∈C weight E
ð Þ
∑E weight E
ð Þ , where E is an individual
342
M. Last and H. Halpert

example from the “new” patient’s record. A recurrence probability of higher than
0.5 is considered as a prediction for a patient to recur.
Zupan et al. have used three methods for constructing predictive models: the
Naive Bayes Classiﬁer, the decision tree induction, and the Cox proportional
hazards model (see Sect. 2.2.2 above). The evaluation was performed using strat-
iﬁed 10-fold cross-validation on the basis of classiﬁcation accuracy, speciﬁcity and
sensitivity, correlation of predicted probability and probability estimated by the
Kaplan-Meier method, and the concordance index (i.e., the area under the Receiver
Operating Characteristic (ROC) curve). The non-recurring patients with incomplete
follow-ups in the test set were weighted by the Kaplan–Meier estimates calculated
from the training sets. The Naive Bayes and the Cox proportional hazards models
seemed to perform better than decision trees, although the differences were not
signiﬁcant. The results did not include any comparison of the proposed weighting
technique to training on non-censored records only. Testing results on the actual
labels of non-censored records were not reported either.
2.3.2 Data Streams
A recent paper by [11] considers event history analysis in the data stream setting.
They assume a ﬁxed set of data streams; each corresponding to an object charac-
terized by a feature vector (a vector of covariates). Each stream produces a sequence
of recurrent events related to a speciﬁc object. The authors of [11] introduce an
incremental, adaptive version of the Cox proportional hazard model (see
Sect. 2.2.2), which re-estimates the covariate coefﬁcients of the hazard function at
every discrete point in time by averaging over all sliding time windows covering
that point. The sliding window has a ﬁxed length and it is shifted by one unit at a
time. In each window, the maximum likelihood estimation of the covariate coef-
ﬁcients in each sequence is based on the events observed in that sequence and thus
it ignores the right censoring of the next event, which has not been observed yet.
The continuous monitoring approach of [11] may be appropriate for high fre-
quency data streams of recurrent events, such as earthquakes or tweets. In [12], we
have proposed a snapshot monitoring approach for an inﬁnite stream of “birth” and
“failure” (non-recurrent) events related to a set of objects. In each snapshot, some of
the monitored objects may be right-censored, since their “failure” event has not
been observed yet. Given a snapshot, we are interested to induce a classiﬁcation
model from both censored and non-censored observations for predicting the sur-
vival of each object to the end of a pre-deﬁned follow-up period. In the present
chapter, we describe in detail the survival analysis methodology initially introduced
by us in [12] and evaluate it on two real-world streams of survival data.
A Fuzzy-Based Approach to Survival Data Mining
343

3
The CENSMINER Methodology
3.1 Overview
The CENSMINER algorithm deals with a sequence of periodical snapshots from an
inﬁnite stream of “birth” and “death/failure” events related to a set of monitored
objects. The snapshot frequency does not have to be ﬁxed and the set of objects
under study may be dynamic as well. Two consecutive snapshots may differ in the
following aspects:
(1) New objects have entered the observation process (i.e., their “birth event” took
place between the snapshots). The new objects remain censored (more pre-
cisely, right-censored) as long as they cannot be labeled (cases 2 and 3 below).
(2) Some objects have survived the follow-up period (i.e., the end of their
follow-up period took place between the snapshots and no “failure event” was
observed before that time point). Such objects are assigned the crisp “survived”
label and become uncensored.
(3) Some objects have experienced a “failure event” between the snapshots. If the
event occurred before the end of their follow-up period, the object is assigned
the crisp “failed” label and becomes uncensored as well.
In each snapshot, the algorithm is aimed at predicting (or estimating the prob-
ability of) a “death event”, such as a disease recurrence in a patient or a technical
failure in a car, within a pre-deﬁned follow-up period for all monitored objects
(entities),
which
are
still
censored
and
thus
cannot
be
labeled.
The
classiﬁcation/probability estimation model induced in each snapshot can only be
evaluated on all objects that become uncensored between the current snapshot and
the next one (cases 2 and 3 above). We assume that the focus on these object
records as a testing set represents many real-world scenarios, such as evaluating the
outcome of a new clinical procedure or estimating the reliability of a new car
model, where we are interested in estimating the failure probability for the unla-
beled (i.e., censored) records after as few data snapshots as possible. The model
performance can be evaluated using various measures, such as the Area under ROC
curve (AUC), classiﬁcation accuracy, classiﬁcation sensitivity, etc.
Our fuzzy class label adjustment pre-processing strategy works as follows. Upon
arrival of a new data snapshot, the algorithm identiﬁes all censored objects and
calculates their age value, i.e., the time since their “birth” event. Each censored
object creates two weighted records representing the two fuzzy labels (“survived”
and “failed”), respectively. The age value of the censored object is used to calculate
the weight of each record based on the Kaplan-Meier estimate [10]. Another pro-
cedure compares the current and the previous data snapshots, ﬁnds the records that
have become uncensored since the previous snapshot, and labels them with their
true (crisp) labels.
In general, any classiﬁcation algorithm can be integrated with the proposed
methodology as long as it can process weighted data instances. The problem of
344
M. Last and H. Halpert

learning from data instances, which are assigned fuzzy class labels, can be handled
by assuming different “label proportions” for each instance. This problem setting is
close to the tasks of unsupervised and semi-supervised learning, where only
unlabeled or partially labelled instances are given. The adaptation of a classiﬁcation
algorithm to training instances with label proportions is not always trivial and it has
not gained much attention in the machine learning community. A similar problem
of learning a classiﬁer from group probabilities is known as Multiple-Instance
Learning with Label Proportions (MIL-LP). Rueping, in [13], proposed an MIL-LP
algorithm based on SVR, a version of SVM for regression. Hernández, in [14],
proposed an adaptation of the Naive Bayes Classiﬁer to the MIL-LP problem. In
some classiﬁcation methods, the way of treating the weighted instances is
straightforward, namely, by replacing any reference to an instance, such as sum-
ming, multiplication, etc., by its weight. For example, in the C4.5 decision-tree
induction method, the entropy calculations regarding the splitting criteria of each
node include the summation of probabilities of a class C in the dataset. Usually, the
probabilities are calculated as the number of instances assigned the C label divided
by the number of all instances. However, in the presence of weighted instances,
these probabilities are calculated as the sum of weights of instances with the C label
divided by the sum of all instance weights.
Although the CENSMINER algorithm focuses on a classiﬁcation task, it may be
easily adapted to the probability estimation task, as demonstrated in the warranty
use case below. Additionally, instead of a simple classiﬁcation model, the algorithm
may use the probability estimation tree (PET) model [15], which can compute the
probability of each outcome. The decision whether to induce a classiﬁcation model
or a probability estimation model is domain-dependent. In imbalanced domains,
such as warranty data of high quality products, a classiﬁcation model would be
useless, as it will always predict a “survival”. A probability estimation model would
be more appropriate for such cases.
3.2 The CENSMINER Algorithm
3.2.1 The Notation
The CENSMINER algorithm uses the following notations:
i ∈½1, ∞Þ – a discrete index of each snapshot.
ti – The timing of a snapshot i.
Di – The set of records in a snapshot received at time ti. Each record x ∈Di
contains the information known at the time of ti about the individual (object)
under study, such as a vehicle, a patient or a customer.
DS
i , DF
i – Two sets of weighted records representing the “survival” and the “fail-
ure” outcomes, respectively. Each weighted record is stored as a pair < x, w(x) >,
A Fuzzy-Based Approach to Survival Data Mining
345

where w(x) is the class membership value of an object record x ∈Di in the
respective outcome.
Ci ⊆Di – The subset of censored records in Di, i.e., the records with unknown
outcome at the time ti.
NCi ⊆Di – The set of new uncensored records in snapshot i, i.e., the records that
were censored at time ti−1 and are no longer censored at time ti (i > 1).
In each snapshot i, every object record is either censored or non-censored.
Consequently, if we refer to the set of non-censored records in the ﬁrst snapshot as
NC1, we get the following formula which mathematically deﬁnes the relationship
between the sets above:
Di =
⋃
j ≤i
NCj
 
!
⋃Ci
This formula emphasizes the fact that every object record is represented in any
snapshot either by a censored record or by a record, which became uncensored in
one of the previous snapshots (including the latest one). The records that became
uncensored in the latest snapshot are used as a testing set for evaluating the per-
formance of the last model. It also means that every monitored object, excluding the
records that became uncensored in the ﬁrst snapshot NC1, is used exactly once
during its lifetime for testing a classiﬁcation model.
xτ – The censoring value (age) of x ∈Di. For example, in warranty data, this is
the time elapsed (or the usage value) since vehicle x was sold. In churn prediction,
xτ is the time elapsed since customer x signed up for the service. In medical survival
analysis, xτ is the time elapsed since patient x experienced a medical procedure,
such as a surgery.
event(x) – The age or the usage value of x when it experienced the death/failure
event. For example, in vehicle warranty data, event(x) may contain the mileage of a
vehicle x at its ﬁrst claim. In churn prediction, event(x) may contain the time until
the customer x churned. In medical survival analysis, event(x) may contain the time
until a patient x was diagnosed with a re-infection. For individuals who have not
experienced the event of interest yet, the event(x) value is null.
Mi – The classiﬁcation/probability estimation model induced from the snapshot i.
Aci – The testing performance of the model Mi. The testing performance value
(e.g., classiﬁcation accuracy, AUC, etc.) is calculated on the next snapshot (i.e.,
at the time ti+1) by testing the Mi model on the NCi+1 records described above.
W – The duration of the follow-up (prediction) period deﬁned in Sect. 2.1 above.
We assume that this duration is ﬁxed for all monitored objects disregarding the
time of their birth event.
A – The classiﬁcation/probability estimation algorithm (e.g., C4.5 Decision
Tree) for inducing a classiﬁcation model from each snapshot.
E – The evaluation measure (e.g., AUC) for testing the induced model on each
incoming snapshot.
346
M. Last and H. Halpert

3.2.2 The Model Induction Process
The general input of the CENSMINER algorithm includes a “snapshot stream” – a
potentially inﬁnite sequence of data snapshots. Each snapshot i consists of the
record set Di deﬁned above where some of the records may be censored. A record
contains a list of attributes, which may be used as predictive features. In addition,
each record includes the xτ (the “age” of x) and the event(x) attributes deﬁned
above. The value of event(x) may be null.
In addition, the algorithm has the following global parameters:
• W - the duration of the follow-up period
• A - the classiﬁcation algorithm
• E - the evaluation measure
The algorithm ﬂow is described below:
For each snapshot i:
Input:
Di – The new snapshot i.
Di−1 – The previous snapshot (for i > 1).
Mi−1 – The classiﬁcation/probability estimation model induced from the pre-
vious snapshot Di−1 (for i > 1).
Output:
Aci−1 – The testing performance of the model Mi−1 on the new snapshot i
Mi – The classiﬁcation model induced from the new snapshot i.
The algorithm:
1. Identify the subset of censored records Ci using censored_identiﬁcation_pro-
cedure(Di,W)
2. Identify the subset of new uncensored records NCi using new_uncen-
sored_identiﬁcation_procedure(Di,Di−1,W)
3. If (i > 1)then:
3:1. Calculate Aci−1 by applying Mi−1 on NCi
4. Induce a new prediction model Mi:
4:1. Calculate Si function, the survival curve estimation given the snapshot i,
using the Kaplan Meier estimate [5]:
Si T
ð Þ = ∏
tj < T
n tj
 
−dj
n tj
 
Where Si(T) is the estimated probability of a monitored object to exceed the
lifetime T, n(tj) is the number of objects “at risk” (not lost through cen-
soring or failure) at time tj and dj is the number of objects which failed at
time tj. Usually dj = 1, since tj is chosen as the observed time to failure.
A Fuzzy-Based Approach to Survival Data Mining
347

4:2. Set SW = Si W
ð
Þ, the probability of an individual to survive for the duration
of the follow-up period W
4:3. Calculate the probabilities st x
ð Þ, st x
ð Þ of the two outcome classes for each
record x ∈Di:
4:3:1. if x ∈Ci (i.e., x is censored):
4:3:1:1. Find Si x
ð Þ, the probability of an object x to survive beyond
its age xτ
4:3:2. Set the total probability of the survival st x
ð Þ of an object x up to the
end of the follow-up period:
st x
ð Þ =
SW
Si x
ð Þ
x ∈Ci
0
x ∉Ci and event x
ð Þ ≠null
1
else
8
<
:
4:3:3. Set the total probability of failure st x
ð Þ of record x up to the end of
the prediction period: st x
ð Þ = 1 −st x
ð Þ
4:4. Initialize the two sets of weighted records: DS
i = ∅, DF
i = ∅. DS
i and DF
i
store the “survival” and the “failure” records with their class membership
weights, respectively.
4:5. For each record x ∈Di do:
4:5:1. Append x to the set of “survival” records: DS
i : = DS
i ∪
< x, st x
ð Þ >
f
g
4:5:2. Append x to the set of “failure” records: DF
j : = DF
j ∪f < x, bstg x
ð Þ > g
4:6. Combine DS
i and DF
i into one weighted training dataset D
0
i : D
0
i = DS
i ∪DF
i
4:7. Induce a new model Mi by applying the learning algorithm A to the set D
0
i
5. Return Mi – the classiﬁcation/probability estimation model induced from the
records in snapshot i
censored_identiﬁcation_procedure ðDi, WÞ
Input:
Di – The set of records in a snapshot received at time ti. Some of the records may
be censored.
W – The duration of the follow-up (prediction) period.
Algorithm:
1. Initialize the set of censored records: Ci = ∅
2. For each record x ∈Di do:
2:1. If event(x) is null and xτ < W;
Append x to the set of censored records: Ci: = Ci ∪x
f g
3. return Ci
348
M. Last and H. Halpert

new_uncensored_identiﬁcation_procedure ðDi, Di −1Þ
Input:
Di – The set of records in a snapshot i.
Di−1 – The set of records in the previous snapshot i−1.
Algorithm:
1. If Di−1 is null (the ﬁrst snapshot):
1:1. Return ∅
2. Otherwise:
2:1. Initialize the set of new uncensored records: NCi = ∅.
2:2. For each record x ∈Di do:
2:2:1. If x ∈Ci −1 and x∉Ci;
Append x to the set of new uncensored records: NCi: = NCi ∪x
f g
2:3. Return NCi
4
Empirical Evaluation
4.1 Experimental Settings
Our empirical evaluation is aimed at comparing the performance of the CENS-
MINER methodology, which involves the Kaplan-Meier estimation for the outcome
label of censored records, to a baseline approach, where the censored records are
completely discarded during the model induction process. Both approaches are
evaluated using a variety of standard classiﬁcation algorithms.
4.1.1 Data Sources
In this chapter, we present the results from two real-world survival datasets rep-
resenting two different domains: warranty management and health care. The ﬁrst
data set, the Vehicle Warranty Dataset, contains the warranty data of more than
200,000 vehicles of a given model sold in North America between the years 2008
and 2011 by a major automotive company. The second data set, the STD Dataset,
contains data collected from STD (Sexually Transmitted Diseases) patients, some of
whom were re-infected after a period of time. The dataset was downloaded from the
University of North Texas website.
Table 1 displays some characteristics of the datasets used in our experiments.
A Fuzzy-Based Approach to Survival Data Mining
349

4.1.2 Snapshot Simulation
Both datasets were originally obtained in the form of a single snapshot observed at
the end of the data collection period. In the pre-processing stage, we created several
snapshots from each dataset to emulate a real-world situation where data arrives as a
sequence of consecutive snapshots. For this purpose, we set ﬁctitious snapshot
dates for each dataset. Then we created multiple samples of the original dataset,
each representing the event information, which was available on the date of a
particular snapshot. That is, an object ‘born’ after the snapshot date was removed
from the snapshot sample and each ‘failure’ event occurred after that date was
ignored. All object records associated with a given snapshot (both censored and
uncensored) were copied from the original dataset along with their attributes. In the
case of a ‘failed’ record, the event date was copied as well.
The snapshot arrival rate was set for each simulated data stream in accordance to
the nature of the relevant domain. For example, in the Vehicle Warranty Dataset,
the snapshots were created every 90 days (about three months), a reasonable period
to sample a dataset of 200,000 vehicles, which resulted in the total of 15 snapshots.
We assumed that three months is a sufﬁciently long period to expect a change in the
existing classiﬁcation model. On the other hand, in the STD dataset, we simulated a
snapshot arrival rate of 305 days, which resulted in the total of 6 snapshots.
4.1.3 The Follow-up Period Duration
The duration of the follow-up (prediction) period was also chosen in order to
generate a real-world scenario of the relevant data stream. For the Vehicle Warranty
Dataset, four prediction periods were simulated: 365, 730, 1095, and 550 days. The
ﬁrst three values represent one, two, and three years out of the three-year warranty
period, respectively, and they are reasonable choices to estimate the survival
probability. The value of 550 was chosen because it represents the median
time-to-failure in the Vehicle Warranty Dataset. For the STD Dataset, we simulated
a prediction period of 300 days, which was close to the average time to re-infection
in this data.
4.1.4 Performance Measures
The choice of the most appropriate performance measures for each data stream
depends on its class imbalance ratio. The Vehicle Warranty Dataset (with ∼4 % of
failures) is extremely imbalanced whereas the STD Dataset (with 45 % of
re-infected patients) is relatively balanced. The standard performance metrics (e.g.,
classiﬁcation accuracy) are ineffective in the case of imbalanced classes because
they favor “majority rule” models like “a car under warranty never fails”. There-
fore, in our experiments with the Vehicle Warranty dataset, we use the AUC (Area
Under ROC curve) value instead of the standard accuracy metric.
350
M. Last and H. Halpert

4.1.5 Experimental Environment
The experiments were run in the following environment: Windows7 Enterprise
SP1, Intel i5-2400 CPU 3.10 GHz with 4 GB RAM, and 32-bit architecture. The
classiﬁcation models were induced and tested using a local extension of the MOA
environment, Version 2012.08.31 [16]. The MOA (Massive Online Analysis)
environment allows for the implementation and evaluation of learning algorithms
on evolving data streams. In particular, it allows for implementing and running
various classiﬁcation algorithms. Some of these classiﬁcation algorithms have been
implemented within the MOA environment, while others are available by inter-
facing with WEKA, the Waikato Environment for Knowledge Analysis [17]. The
interface with WEKA is particularly important for running standard classiﬁcation
algorithms on the static training datasets extracted from each snapshot.
4.2 Vehicle Warranty Dataset Results
4.2.1 Data Description
The original dataset contains 204,708 records of vehicles of a given model sold in
North America (i.e., U.S.A and Canada) between April 2008 and December 2011.
Some of the vehicles had one or more warranty claims (e.g., failures) during this
time. We focused on 8,147 vehicles (∼4 % of the entire dataset) that have expe-
rienced a speciﬁc (battery) failure during their warranty period. The ﬁrst claim was
recorded in September of 2008 and the last one in June 2011. Thus, the vehicle sale
date is considered the “birth event” and the ﬁrst battery failure is considered the
“death event”. Recurrent battery failures in the same under-warranty vehicle were
ignored due to their extremely low incidence.
Each vehicle record is associated with a unique identiﬁcation number and has
values for the following attributes:
1. Manufacturing date – The date when the vehicle was produced (a continuous
variable).
Table 1 Dataset characteristics
Characteristic
Vehicle warranty data
STD data
# of snapshots
15
6
Duration of the follow-up period (days)
365, 550, 730, 1095
300
Evaluation method
AUC
Accuracy
Imbalance ratio (%)
4/96
45/55
Missing values
11.70 %
0 %
Data size (total # of observed objects)
204,708
877
# of features (numeric, date, nominal)
3 (1, 1,1)
21 (3, 0, 18)
A Fuzzy-Based Approach to Survival Data Mining
351

2. Sale gap – A continuous non-negative variable representing the number of days
elapsed from the manufacturing time to the sale date.
3. Area of sale – A nominal variable representing the geographic area where the
vehicle was sold. This variable can take one of the following ﬁve values:
MIDWEST, SOUTH, WEST, NORTHEAST or CANADA. About 23 % of
vehicles are missing this value.
Each warranty claim record contains the following information recorded by the
service dealer:
1. Vehicle ID number.
2. Labor code (i.e., failure ID).
3. Job card date – The date when the vehicle failure was identiﬁed by the dealer.
4. Vehicle odometer mileage – The mileage value of the vehicle’s odometer at the
time of failure.
The pre-processing stage included the creation of snapshots from the initial
database in order to emulate a real-world situation where the data arrives in several
snapshots. The snapshot frequency was set to 90 days (about 3 months) beginning
on April 2008 (the ﬁrst sale date in the database). In addition, the ﬁnal snapshot date
was set to December 2011, the date of the last recorded claim in the dataset. We
have created 15 snapshots this way. Table 2 shows the characteristics of the 15
simulated snapshots. One can see that the total number of observed vehicles
increases over time as more vehicles are “born”, i.e., sold to the customers. In the
ﬁrst few snapshots, the records of nearly all monitored vehicles are censored, since
Table 2 Characteristics of simulated snapshots in the vehicle warranty data stream
Snapshot
ID
Number of days since
the beginning of the
stream
Total number of
observed
vehicles
Number of censored vehicles
(prediction period = 365 days)
0
91
26
26
1
182
38,788
38,783
2
273
85,963
85,905
3
365
114,312
114,134
4
456
165,978
165,560
5
547
188,346
149,019
6
638
199,300
112,749
7
730
203,216
88,144
8
821
204,181
37,702
9
912
204,515
15,906
10
1,003
204,595
5,189
11
1,095
204,632
1,385
12
1,186
204,701
507
13
1,277
204,703
185
14
1,352
204,704
117
352
M. Last and H. Halpert

there are still too new to experience a failure or to reach the end of the follow-up
period. However, after Snapshot 4, the number of censored records starts decreasing
as less vehicles of the speciﬁc model are sold whereas more and more vehicles
leave the monitoring process due to a “survival” or a “failure” outcome.
4.2.2 Summary of Results
Figures 2, 3 and 4 present the testing AUC results in each snapshot for prediction
periods of 365, 550, and 730 days, respectively. The results for the longest pre-
diction period of 1095 days (three years) are not shown here, since for this value, it
took almost the entire monitoring period to obtain a sufﬁcient number of surviving
records in the testing set. All classiﬁcation models were induced by the AdaBoost
with Decision Stump algorithm (a one-level decision tree presented in [18] and
wrapped by the AdaBoost [19]), which provided in this dataset higher AUC values
than several other popular classiﬁers such as Decision Tree, Decision Stump, and
Naïve Bayes Classiﬁer (NBC). Each chart compares the baseline approach (using
uncensored records only) to the CENSMINER methodology, which utilizes the
Kaplan-Meier outcome estimation for censored records. Each chart also shows the
Fig. 2 Vehicle warranty data: prediction period = 365 Days
Fig. 3 Vehicle warranty data: prediction period = 550 Days
A Fuzzy-Based Approach to Survival Data Mining
353

weighted AUC values of both approaches, where the testing AUC of every snap-
shot is weighted by the amount of testing (new uncensored) records in that
snapshot.
For the three different prediction periods, the Total Weighted AUC values of the
proposed CENSMINER methodology are signiﬁcantly higher than the results of
the baseline approach. More speciﬁcally, the CENSMINER has an advantage over
the baseline as long as the percentage of censored records in a snapshot is high.
When this percentage goes down, the difference between the two approaches
diminishes, since the Kaplan-Meier estimate is applied to much fewer records. In
addition, there is no difference between the approaches in the ﬁrst few snapshots,
which still have not accumulated any “survived” records to induce a reliable
classiﬁcation model. The number of such snapshots increases with an increase in
the duration of the follow-up period (compare Figs. 2 to 3 and Figs. 3 to 4), since it
takes more time for surviving vehicles to get a “survived” label. The low AUC
values in the last few snapshots, disregarding the follow-up duration, are explained
by the lack of testing records with the “failed” label in those snapshots. Thus, we
may conclude that the CENSMINER tends to reach better classiﬁcation perfor-
mance than the baseline in snapshots having a signiﬁcant amount of censored
records along with some uncensored records from both classes. In other snapshots,
it usually performs the same as the baseline.
4.3 STD Re-Infection Dataset Results
4.3.1 Data Description
The STD (Sexually Transmitted Diseases) dataset was taken from a collection of
survival datasets discussed in [3]. The collection is called KMsurv and it was
downloaded from the University of North Texas website for the purposes of sur-
vival analysis research. The direct link is http://rss.acs.unt.edu/Rdoc/library/
Fig. 4 Vehicle warranty data: prediction period = 730 Days
354
M. Last and H. Halpert

KMsurv/data/std.txt. Unfortunately, this link has stopped being active at the time of
writing this chapter.
The raw data contains records of 877 patients diagnosed with STD. Each record
is characterized by 21 different features. Three features are continuous and the rest
are nominal, mostly binary. Each record has also a value for the binary target
variable “REINFECTION” which indicates whether the patient was eventually
diagnosed with re-infection of STD or not. In addition, each record has a value for
the numeric variable “TIME_TO_REINFECTION”, which contains the number of
days elapsed since the initial diagnosis. The actual dates of the initial diagnosis and
the re-infection (if occurred) are not included in the dataset.
To simulate the snapshot monitoring process, we randomly set the dates of the
initial diagnosis and the re-infection diagnosis (if occurred). We assumed the date of
01/01/2013 to be the CUT_OFF_DATE, which means that patients were no longer
observed after that date. Consequently, for patients who had not been diagnosed with
re-infection yet, the date of the initial diagnosis was set to the CUT_OFF_DATE
(01/01/2013) minus the amount of days elapsed since the initial diagnosis (given by
the TIME_TO_REINFECTION variable). Then, the earliest date of the initial
diagnosis (25/10/2008) was marked as MIN_START_DATE (the start date of the
monitoring process). For all other patients diagnosed with re-infection, the date of
the initial diagnosis was chosen randomly between the MIN_START_DATE and the
last date possible: CUT_OFF_DATE - TIME_TO_REINFECTION. Accordingly,
the re-infection diagnosis date of these patients was set as the (random) initial
diagnosis date plus TIME_TO_REINFECTION. The randomization process was
repeated three times to verify the consistency of the results. Similar to the case of the
Vehicle Warranty data, all snapshot dates were simulated for the period between the
MIN_START_DATE and the CUT_OFF_DATE. In each snapshot and for every
patient, only the event information available on the snapshot date was taken into
account (e.g., the initial diagnosis date and the re-infection diagnosis date).
Table 3 shows the characteristics of the six simulated snapshots with the ﬁrst
randomization scheme. The total number of observed patients increases over time
as more patients are “born”, i.e., diagnosed with STD for the ﬁrst time. In the ﬁrst
snapshot (Snapshot 0), the percentage of censored patient records is higher than in
Table 3 Characteristics of simulated snapshots in the STD Re-infection dataset
Snapshot
ID
Number of days since the
beginning of the stream
Total number of
observed patients
Number of censored records
(prediction period = 300 days)
0
304
128
92
1
608
257
96
2
912
403
117
3
1,216
571
133
4
1,520
836
233
5
1,529
876
262
A Fuzzy-Based Approach to Survival Data Mining
355

the subsequent snapshots, since it has very few patients that reached the end of the
follow-up period (300 days since the initial diagnosis).
4.3.2 Summary of Results
Figures 5, 6 and 7 present the testing accuracy results in each snapshot for the
prediction period of 300 days and three different randomization schemes. All
classiﬁcation models were induced by the J48 Decision Tree algorithm, which
provided in this dataset higher accuracy values than several other popular classiﬁers
such as Naïve Bayes Classiﬁer (NBC) and Support Vector Machine (SVM). Each
chart compares the baseline approach (using uncensored records only) to the
CENSMINER methodology, which utilizes the Kaplan-Meier outcome estimation
for censored records. Each chart also shows the weighted accuracy values of both
approaches, where the testing accuracy of every snapshot is weighted by the
amount of testing (new uncensored) records in that snapshot.
Fig. 5 STD re-infection data: random scheme 1, prediction period = 300 days
Fig. 6 STD re-infection data: random scheme 2, prediction period = 300 days
356
M. Last and H. Halpert

For the three different randomization schemes, the Total Weighted Accuracy
values of the proposed CENSMINER methodology are signiﬁcantly higher than the
results of the baseline approach. Similar to the Vehicle Warranty results, in this
dataset, the CENSMINER has also a greater advantage over the baseline when the
percentage of censored records in a snapshot is higher. These results conﬁrm our
previous conclusion that the CENSMINER tends to reach better classiﬁcation
performance than the baseline in snapshots having a signiﬁcant amount of censored
records along with some uncensored records from both classes. In other snapshots
(like Snapshot 5 in the STD Re-infection Data), it usually performs almost the same
as the baseline.
5
Conclusions
In this chapter, we presented the CENSMINER fuzzy-based methodology, which
allows the standard probability estimation and classiﬁcation algorithms to handle
censored data streams. We consider the domains where the data stream arrives in
the form of consecutive snapshots. In each new snapshot, the performance of the
last classiﬁcation model is tested on the new uncensored records. Then a new model
is induced from on all available data, including the censored records. The censored
records are used in the model induction process by weighting their outcome with
the Kaplan-Meier survival curve estimation. The proposed methodology shows a
signiﬁcant improvement over the baseline approach, which completely discards the
censored data. The CENSMINER methodology tends to be most useful in the
snapshots containing a relatively high percentage of censored records along with
some amount of uncensored records from both classes.
In this study, the Kaplan-Meier estimation was applied to all snapshot records
without any distinction between different object groups that might exist in the data.
In future research, it would be worthwhile to examine the effect of clustering the
monitored objects into relatively homogeneous groups and then calculating the
Fig. 7 STD re-infection data: random scheme 3, prediction period = 300 days
A Fuzzy-Based Approach to Survival Data Mining
357

Kaplan-Meier estimation for each group separately. One can also evaluate addi-
tional parametric and non-parametric approaches to the survival curve estimation.
Since we are dealing with a data stream, it would be natural to examine the use
of incremental classiﬁcation algorithms that are designed to handle this kind of data.
Such algorithms can save the computational effort required for inducing a new
model from every snapshot by reusing or updating the existing one.
Acknowledgments. This work was supported in part by the General Motors Global Research &
Development - India Science Lab.
References
1. Krempl, G., Žliobaite, I., Brzeziński, D., Hüllermeier, E., Last, M., Lemaire, V., Noack, T.,
Shaker, A., Sievi, S., Spiliopoulou, M., Stefanowski, J.: Open challenges for data stream
mining research. ACM SIGKDD Explor. Newsl. 16(1), 1–10 (2014)
2. Pizzi, N., Pedrycz, W.: Fuzzy set theoretic adjustment to training set class labels using robust
location measures. In: Proceedings of the IEEE-INNS-ENNS International Joint Conference
on, pp.109, 112 vol. 3, 2000 (2000)
3. Moeschberger, M.L., Klein, J.P.: Examples of survival data. In: Survival Analysis: Techniques
for Censored and Truncated Data, 2nd edn, pp. 1–20. Springer, Berlin (2003)
4. Fleming, T., Lin, D.: Survival analysis in clinical trials: past developments and future
directions. Biometrics 56(4), 971–983 (2000)
5. Kaplan, E.L., Meier, P.: Nonparametric estimation from incomplete observations. J. Am. Stat.
Assoc. 53(282), 457–481 (1958)
6. Costella, J.: A simple alternative to Kaplan–Meier for survival curves. Peter MacCallum
Cancer Centre Working Paper No (2010)
7. Last, M., Zhmudyak, A., Halpert, H., Chakrabarty, S.: Multi-dimensional failure probability
estimation in automotive industry based on censored warranty data. In: Synergies of Soft
Computing and Statistics for Intelligent Data Analysis. Berlin/Heidelberg (2013)
8. Cox, D.R.: Regression models and life-tables. J. Roy. Stat. Soc. 34(2), 187–220 (1972)
9. Segal, M.: Regression trees for censored data. Biometrics 44(1), 35–47 (1988)
10. Zupan, B., Demsar, J., Kattan, M.W., Beck, R., Bratko, I.: Machine learning for survival
analysis: a case study on recurrence of prostate cancer. Artif. Intell. Med. 20(1), 59–75 (2000)
11. Shaker, A., Hullermeier, E.: Event history analysis on data streams. Int. J. Appl. Math.
Comput. Sci. (to appear)
12. Last, M., Halpert, H.: Survival analysis meets data stream mining. In: First Workshop on
Real-World Challenges for Data Stream Mining (RealStream 2013) (2013)
13. Rueping, S.: SVM classiﬁer estimation from group probabilities. In: International Conference
on Machine Learning, Haifa, Israel (2010)
14. Hernández, J., Inza, I.: Learning naive Bayes models for multiple-instance learning with label
proportions. In: Advances in Artiﬁcial Intelligence, pp. 134–144 (2011)
15. Provost, F., Domingos, P.: Tree Induction for Probability-Based Ranking. Mach. Learn. 52(3),
199–215 (2003)
16. Bifet, A., Holmes, G., Kirkby, R., Pfahringer, B.: MOA: massive online analysis. J. Mach.
Learn. Res. 1601–1604 (2010)
17. Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., Witten, I.H.: The WEKA Data
Mining Software: An Update. ACM SIGKDD Explor. Newsl. 11(1), 10–18 (2009)
358
M. Last and H. Halpert

18. Wayne, I., Pat, L.: Induction of one-level decision trees. In: ML. Aberdeen, Scotland (1992)
19. Freund, Y., Schapire, R., Abe, N.: A short introduction to boosting. J. Japan. Soc. Artif. Intell.
14, 771–780 (1999)
Authors Biography
Mark Last was born in Russia in 1961. He has been living in
Israel since 1977. Mark Last has obtained his B.Sc. (1984), M.
Sc. (1990) and Ph.D. (2000) degrees in Industrial Engineering
from Tel Aviv University, Israel.
Mark Last has been the Head of the Production Control
Department at AVX Israel (1989-1994), a Senior Consultant in
Industrial Engineering and Computing (1994-1998), and a
Visiting Assistant Professor at the Department of Computer
Science and Engineering, University of South Florida, USA
(1999-2001). He is currently a Full Professor at the Department
of Information Systems Engineering, Ben-Gurion University of
the Negev, Israel, where he has started his employment in 2001.
Prof. Last is a Senior Member of the IEEE Computer Society
and a Professional Member of the Association for Computing
Machinery (ACM). He currently serves as an Associate Editor of
IEEE Transactions on Systems, Man, and Cybernetics, where he has received the Best Associate
Editor Award for 2006, and an Associate Editor of Pattern Analysis and Applications (PAA).
Hezi Halpert was born in Israel in 1985. He has obtained his B.
Sc. in Software Engineering in 2012 and M.Sc. degree in
Information Systems Engineering in 2013. Both degrees are
from the Faculty of Engineering Sciences, Ben-Gurion Univer-
sity of the Negev, Beer-Sheva, Israel. His M.Sc. thesis subject
was Mining Censored Data Streams for Survival Analysis.
His current work position (from January 2014) is a Quanti-
tative Data analyst in the BI team, Waze, Google. In this
position, Hezi is responsible for processing large amounts of data
(Big Data analysis) from different data sources in order to
produce insights and models best describing the product users
and improve the decision-making process.
A Fuzzy-Based Approach to Survival Data Mining
359

Knowledge Extraction from Support Vector
Machines: A Fuzzy Logic Approach
Shahaf Duenyas and Michael Margaliot
Abstract Support vector machines (SVMs) proved to be highly eﬃcient computa-
tional tools in various classiﬁcation tasks. However, SVMs are nonlinear classiﬁers
and the knowledge learned by an SVM is encoded in a long list of parameter values,
making it diﬃcult to comprehend what the SVM is actually computing. We show that
certain types of SVMs are mathematically equivalent to a speciﬁc fuzzy–rule base,
called the fuzzy all–permutations rule base (FARB). The equivalent FARB provides
a symbolic representation of the SVM functioning. This leads to a new approach for
knowledge extraction from SVMs. An important advantage of this approach is that
the number of extracted fuzzy rules depends on the number of support vectors in the
SVM. Several simple examples demonstrate the eﬀectiveness of this approach.
Keywords
Support vector machines ⋅Knowledge extraction ⋅Artiﬁcial neural
network models ⋅Fuzzy rule–base ⋅Neurofuzzy systems
1 Introduction
Support vector machines (SVMs) are a popular tool in machine learning combining a
solid theoretical background with eﬃcient learning–from–examples algorithms (see,
e.g., [1–5]). An important advantage of SVMs is that their classiﬁcation decision
is based on a subset of the training examples, referred to as the support vectors.
The number of support vectors may be much smaller than the number of training
examples.
A drawback of SVMs, that is shared by many other artiﬁcial neural network mod-
els, is their black–box nature. SVMs usually employ nonlinear kernel functions, and
the knowledge learnt by the SVM is represented as a set of numerical parameter
values. This makes it diﬃcult to understand what the SVM is actually computing.
S. Duenyas ⋅M. Margaliot (✉)
School of Electrical Engineering–Systems, Tel Aviv University, 69978 Tel Aviv, Israel
e-mail: michaelm@eng.tau.ac.il
URL: http://www.eng.tau.ac.il/∼michaelm
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_19
361

362
S. Duenyas and M. Margaliot
This black–box character hinders a wider acceptance of SVMs, especially in safety–
critical applications (e.g. life–support systems).
In this chapter, we show that the input–output (IO) mapping of certain classes of
SVMs is identical to the IO mapping of a speciﬁc type of a fuzzy rule–base, referred
to as the fuzzy all–permutations rule base (FARB). The knowledge embedded in the
FARB is represented in a symbolic form, so this equivalence yields a new approach
for understanding the functioning of SVMs. We demonstrate the usefulness of this
new approach using several simple examples.
The remainder of this chapter is organized as follows. Section 2 reviews knowl-
edge extraction from SVMs. Section 3 reviews the FARB, and Sect. 4 deﬁnes the
mathematical equivalence between a certain class of SVMs and a corresponding
FARB. This leads to a new approach for knowledge extraction from SVMs. Section 5
demonstrates our knowledge extraction approach using several simple examples. The
ﬁnal section concludes and describes possible directions for further research. An
abridged version of this chapter has appeared in [6].
2 Knowledge Extraction from SVMs
Extracting the knowledge learned by a black–box classiﬁer and representing it in a
comprehensible form is referred to as knowledge extraction (KE). KE from SVMs
and other artiﬁcial neural network (ANN) models has received considerable attention
in the literature. Indeed, potential beneﬁts of successful KE include [7–10]:
Validation.
Explaining how the network actually works in an intelligible fash-
ion can validate its suitability for a speciﬁc application or, alternatively, help the
human expert to identify errors in the conclusion reached by the system. This is
crucial in safety–critical applications [11]. For example, medical decision aids re-
quire approval by government agencies. Gaining such an approval is much easier
for transparent systems that include adequate explanation capabilities [12].
Feature extraction.
During training, classiﬁers learn to identify the relevant fea-
tures in immense data sets. Understanding what these features are, and how they
should be integrated to yield a correct classiﬁcation, may help in gaining a deeper
understanding of the problem. Eﬃcient feature extraction may also assist in im-
proving the accuracy and generalization capabilities of a classiﬁer.
Knowledge reﬁnement and improvement.
An intelligible model can be exam-
ined by human experts and potentially improved in precision and eﬃciency.
Knowledge acquisition for symbolic AI systems.
The most diﬃcult and time con-
suming task in the construction of symbolic AI systems is knowledge acquisi-
tion [13]. Eﬃcient KE from trained black–box classiﬁers may help to overcome
this problem.
Scientiﬁc discovery.
Classiﬁers that learn from examples, such as SVMs, may be
able to solve problems for which no other solution is known (or demonstrate better
performance than any other solution). Understanding the way the classiﬁer works
may thus lead to new discoveries [14].

Knowledge Extraction from Support Vector Machines ...
363
The term rule extraction (RE) is used when the extracted knowledge is stated
in the form of rules. Methods for RE from ANNs have been classiﬁed into three
categories: decompositional, pedagogical, and eclectic [15]. The decompositional
approach focuses on symbolic representation of the hidden and output associations
in the network. It aims to explain what individual components in the ANN are com-
puting. Pedagogical approaches use the trained classiﬁer as an oracle to produce a set
of input–output (IO) examples. This set is the input to the KE algorithm. Pedagog-
ical methods can thus extract knowledge from any classiﬁer, but cannot explain the
speciﬁc structure and parameter values of the classiﬁer. Eclectic methods combine
both the pedagogical and the decompositional approaches. We now brieﬂy review
several known approaches for KE from SVMs. A detailed overview may be found
in [16].
Pedagogical methods are based on querying the SVM to generate IO examples
and then applying standard algorithms for building decision trees [17, 18]. The Trees
Parroting Networks (TREPAN) algorithm introduced by Craven [9, 19] extracts de-
cision trees from trained neural networks. Martens et al. [17] used it to extract rules
from SVMs. The Genetic Rule Extraction (G–REX) algorithm [20] for KE from
ANNs was also modiﬁed to handle SVMs [17]. This method uses genetic program-
ming [21] to evolve an optimal set of rules. The extracted rules may be Boolean,
fuzzy, or M–of–N type rules.
Eclectic approaches use some of the characteristics of the trained SVM, namely,
the support vectors or the separating hyperplane. Fung et al. [22] proposed a KE
approach for hyperplane classiﬁers (including linear SVMs) that extracts rules in
the form:
If 𝓁1 ≤x1 ≤u1 and … and 𝓁n ≤xn ≤un Then class is C.
Here xi is the ith coordinate of the input, 𝓁i, ui are scalars, and C ∈{−1, 1}. Thus
each rule describes a hypercube in the n–dimensional space with edges parallel to
the axes (see also [23]). Rules are extracted by solving a constrained optimization
problem. Nunez et al. [24] introduced the SVM+Prototypes rule extraction method.
The extracted rules describe ellipsoids or hypercubes in the input space, and are de-
rived by combining information from prototype vectors (obtained via clustering) and
support vectors. Martens et al. [25] suggest enriching the training set by generating
more classiﬁed examples near the support vectors, and then applying an algorithm
for building decision trees.
Castro et al. [26] developed a decompositional approach for KE based on the
equivalence between a special fuzzy rule–base (FRB) and an SVM. This approach
is closely related to our work, so we review it in more detail. An SVM is usually
trained using a set of classiﬁed patterns {xi, yi}, where xi ∈ℝn is the feature vector
of example i, and yi ∈{−1, 1} is the classiﬁcation of example i. The IO mapping of
the trained SVM f ∶ℝn →{−1, 1} is given by
f(x) = sgn(h(x)),
(1)

364
S. Duenyas and M. Margaliot
where
h(x) = b∗+
NSV
∑
i=1
𝛼∗
i yiK(x, si).
(2)
Here {si, yi}NSV
i=1 is a subset of the classiﬁed training examples, with the sis known as
the support vectors (SVs), 𝛼∗
i , b∗∈ℝ, and K ∶ℝn × ℝn →ℝis the kernel function.
The superscript ∗is used as these values are determined via solving a quadratic op-
timization problem characterizing classiﬁcation with the best possible error margin
(see, e.g. [3, 4]).
Castro et al. showed that an equivalent mapping is produced by inferring the set
of rules:
R1: If h(x) is positive Then f = 1,
R2: If h(x) is negative Then f = −1,
(3)
where the linguistic terms positive and negative are modeled using the membership
functions 𝜇pos(y) = 𝜎(𝜆y), 𝜇neg(y) = 𝜎∗(𝜆y), with 𝜎(y) = 1∕(1 + e−y), 𝜎∗(y) =
1 −𝜎(y) = 𝜎(−y), and 𝜆→∞. In other words, inferencing the rule–base (3) yields
an input–output mapping x →f(x) that is identical to (1). Obviously, (3) provides
no verbal interpretation of the function h(⋅). To overcome this, Castro et al. noted
that h(x) = b∗+ ∑NSV
i=1 hi(x), where hi(x) = 𝛼∗
i yiK(x,si), and introduced the opera-
tor ∗∶ℝ× ℝ→ℝdeﬁned by:
a ∗b =
ab
ab + (1 −a)(1 −b).
It is straightforward to verify that: 𝜎(a(x + y)) = 𝜎(ax) ∗𝜎(ay), and 𝜎∗(a(x + y)) =
𝜎∗(ax) ∗𝜎∗(ay). Therefore, (3) may be rewritten as:
R1 ∶If {h1(x) is positive} ∗{h2(x) is positive} ∗…
∗{hNSV(x) is positive} ∗{b∗is positive}
Then f = 1,
R2 ∶If {h1(x) is negative} ∗{h2(x) is negative} ∗…
∗{hNSV(x) is negative} ∗{b∗is negative}
Then f = −1.
This set of two fuzzy rules thus provides a symbolic representation of the SVM
functioning. However, it seems that the extracted FRB may be cumbersome and that
the use of the special operator ∗makes the FRB less comprehensible. For other
approaches for KE from SVMs, see [27, 28] and the references therein.

Knowledge Extraction from Support Vector Machines ...
365
In this chapter, we describe a new decompositional method for KE from SVMs
that also relies on the mathematical equivalence between SVMs and a speciﬁc FRB,
namely, the FARB. The FARB was successfully used for KE and knowledge–based
design of ANNs [8, 29–32]. It is based on the hyperbolic FRB ﬁrst suggested in [33],
and further developed in [34–36]. We show that SVMs with either a Multi Layer Per-
ceptron (MLP) or Radial Basis Function (RBF) kernels are mathematically equiva-
lent to a suitable FARB.
3 The FARB
To motivate the deﬁnition of the FARB, we begin with a simple example adapted
from [35] (see also [34]).
0
a0
a1
1
tanh
O
Σ
q
Fig. 1
Graphical representation of the FRB IO mapping
Example 1 Consider an FRB with input q ∈ℝ, output O ∈ℝ, and rules:
R1: If q is equal to k Then O = a0 + a1,
R2: If q is equal to −k Then O = a0 −a1,
where a0, a1, k ∈ℝ, with k > 0. Assume that the linguistic terms equal to k
and equal to −k are modeled using the Gaussian membership functions:
𝜇=k(q) = exp
(
−(q −k)2
2k
)
,
𝜇=−k(q) = exp
(
−(q + k)2
2k
)
,
(4)
respectively. Note that these functions satisfy
𝜇=k(q) −𝜇=−k(q)
𝜇=k(q) + 𝜇=−k(q) =
exp( −(q−k)2
2k
) −exp( −(q+k)2
2k
)
exp( −(q−k)2
2k
) + exp( −(q+k)2
2k
)
= exp(q) −exp(−q)
exp(q) + exp(−q)
= tanh(q).
(5)

366
S. Duenyas and M. Margaliot
Applying the singleton fuzziﬁer and the center of gravity defuzziﬁer [37] to the rule
base yields:
O(q) = (a0 + a1)𝜇=k(q) + (a0 −a1)𝜇=−k(q)
𝜇=k(q) + 𝜇=−k(q)
= a0 + a1 tanh(q).
Hence, this FRB is mathematically equivalent to a feedforward ANN with a single
neuron employing the activation function tanh(⋅) (see Fig. 1).
□
This example motivates the search for an FRB whose IO mapping is mathematically
equivalent to that of a feedforward ANN.
Deﬁnition 1 (FARB) A fuzzy rule base with input q = (q1, … , qm) ∈ℝm and
output O ∈ℝis called a FARB if the following conditions hold:
1. Every input variable qi is characterized by two linguistic terms: termi
−and
termi
+, modeled using the membership functions 𝜇i
−(⋅) and 𝜇i
+(⋅). These member-
ship functions (MFs) satisfy the following property. There exist zi, ri, ui, vi ∈ℝ
and a sigmoid1 function gi ∶ℝ→ℝsuch that
𝜇i
+(q) −𝜇i
−(q)
𝜇i
+(q) + 𝜇i
−(q)
= zigi(uiq −vi) + ri, for all q ∈ℝ.
(6)
2. The form of every rule is:
If q1 is term1
± and … and qm is termm
±
Then O = a0 ± a1 ± a2 ± · · · ± am,
(7)
where termi
± stands for either termi
+ or termi
−, ± stands for either the plus or the
minus sign, and ai ∈ℝ, i = 1, … , m. The actual signs in the Then–part satisfy
the following property. If the term characterizing qi in the If–part is termi
+, then
in the Then–part, ai appears with a plus sign, otherwise ai appears with a minus
sign.
3. The rule–base contains exactly 2m rules spanning in their If–part all the possible
assignment combinations of q1, ..., qm.
This deﬁnition guarantees that the IO mapping of the FARB admits a simple
closed–form expression.
Theorem 1 [8] Applying the product–inference rule, singleton fuzziﬁer, and the
center of gravity defuzziﬁer to a FARB yields:
1We say that a function g ∶ℝ→ℝis a sigmoid if g is continuous and the limits limx→∞g(x)
and limx→−∞g(x) exist.

Knowledge Extraction from Support Vector Machines ...
367
O(q) = a0 +
m
∑
i=1
riai +
m
∑
i=1
ziaigi(uiqi −vi).
(8)
This IO mapping is depicted in Fig. 2.
Several commonly used MFs satisfy the constraint (6) [8, Chap. 2]. We consider
three speciﬁc examples. First, it follows from (5) that the pair of Gaussian mem-
bership functions {𝜇=k, 𝜇=−k} in (4) satisfy (6) with zi = ui = 1, vi = ri = 0,
and gi(x) = tanh(x). Thus we can use the linguistic terms equal to k, and equal to −k
in the FARB. As a second example, consider the linguistic terms larger than k
and smaller than k modeled using the Logistic MFs:
𝜇>k(q) =
1
1 + exp(−𝜏(q −k)),
(9)
𝜇<k(q) =
1
1 + exp(𝜏(q −k)),
respectively (see Fig. 3). It is straightforward to verify that
𝜇>k(q) −𝜇<k(q)
𝜇>k(q) + 𝜇<k(q) = tanh((q −k)𝜏∕2),
so (6) holds for gi(x) = tanh(x), zi = 1, ui = 𝜏∕2, vi = k𝜏∕2, and ri = 0. Third, if we
model the linguistic terms equal to k and not equal to k using the MFs:
g1
z2a2
z1a1
...
O
q1
q2
qm
−vm
−v2
−v1
u1
u2
um
a0 +
m
i=1 riai
zmam
Σ
g2
gm
Fig. 2
Graphical representation of the FARB IO mapping

368
S. Duenyas and M. Margaliot
𝜇=k(q) = exp
(−(q −k)2
2𝜎2
)
,
𝜇≠k(q) = 1 −𝜇=k(q),
(10)
then
𝜇=k(q) −𝜇≠k(q)
𝜇=k(q) + 𝜇≠k(q) = 2 exp
(−(q −k)2
2𝜎2
)
−1,
so for k ≥0, (6) holds for gi(x) = exp(−x2), zi = 2, ri = −1, ui =
√
1∕(2𝜎2),
and vi =
√
k2∕(2𝜎2).
Example 2 Consider a FARB with m = 2 inputs, linguistic terms: term1
−= smaller
than 5, term1
+ = larger than 5 (modeled using (9)), and term2
−= equal to −7,
term2
+ = equal to 7 (modeled using (4)), and parameters a0 = 1, a1 = 1∕3,
and a2 = 2∕5. Thus, the rules are:
R1 ∶If q1 is smaller than 5 and q2 is equal to −7
Then O = a0 −a1 −a2 = 4∕15,
R2 ∶If q1 is larger than 5 and q2 is equal to −7
Then O = a0 + a1 −a2 = 14∕15,
R3 ∶If q1 is smaller than 5 and q2 is equal to 7
Then O = a0 −a1 + a2 = 16∕15,
R4 ∶If q1 is larger than 5 and q2 is equal to 7
Then O = a0 + a1 + a2 = 26∕15.
y
k
Fig. 3
MFs 𝜇>k(y) (solid) and 𝜇<k(y) (dashed) as a function of y

Knowledge Extraction from Support Vector Machines ...
369
The IO mapping of this FARB is: O(q) = F(q)∕D(q), where q = [q1, q2]′,
F(q) = 4
15𝜇<5(q1)𝜇=−7(q2) + 14
15𝜇>5(q1)𝜇=−7(q2)
+ 16
15𝜇<5(q1)𝜇=7(q2) + 26
15𝜇>5(q1)𝜇=7(q2),
and
D(q) = 𝜇<5(q1)𝜇=−7(q2) + 𝜇>5(q1)𝜇=−7(q2)
+ 𝜇<5(q1)𝜇=7(q2) + 𝜇>5(q1)𝜇=7(q2).
It is straightforward to verify that:
F(q)
D(q) = 1 + 1
3 tanh
(𝜏
2(q1 −5)
)
+ 2
5 tanh(q2).
(11)
and this agrees with (8).
□
The fact that the two atoms qi is termi
−and qi is termi
+ in (7) usually contradict,
and the use of the logical and operator suggest that only very few rules will have a
substantial degree of ﬁring for any given input. Hence, the fuzzy rules in the FARB
actually cover the entire input domain, with each rule corresponding to a diﬀerent
region in ℝm. Thus, it is usually possible to determine which rule yielded a speciﬁc
output. This is useful in explaining how the FARB reached a certain conclusion.
Kolman and Margaliot [8, 29] showed that every standard ANN has a correspond-
ing FARB with an identical IO mapping. More precisely, there exists an explicit
transformation T such that
T(ANN) = FARB and T−1(FARB) = ANN.
(12)
This provides a symbolic representation of the knowledge embedded in the ANN.
In the next section, we extend this idea to develop an equivalence between a class
of SVMs and a FARB. Given an SVM in this class, there exists a transformation P
such that:
P(SVM) = ANN.
(13)
Combining this with (12) yields:
T(P(SVM)) = T(ANN) = FARB.
(14)
Thus, the transformation S = T ◦P transforms an SVM into a FARB with an iden-
tical IO mapping. This provides a representation of the knowledge embedded in the
SVM as a set of If-Then fuzzy rules. Note that this method uses no approximations,
and that the extracted FARB is based on standard fuzzy logic tools.

370
S. Duenyas and M. Margaliot
Σ
α∗
2y2
s2
sNSV
...
b∗
x
α∗
1y1
α∗
NSV yNSV
h
s1
K
K
K
Fig. 4
Schematic description of the IO mapping (2) of an SVM
4 The SVM–FARB Equivalence
The computation of the SVM IO mapping h in (2) is depicted graphically in Fig. 4.
It is clear that we can view this as a kind of a feedforward ANN with a hidden–layer
of neurons employing K(⋅, ⋅) as an activation function. Since any standard ANN is
equivalent to a FARB, we may expect this SVM to have a corresponding FARB
with an identical IO mapping. Indeed, comparing (8) with (2) immediately yields
our main result in this section.
Theorem 2 (SVM–FARB equivalence)
Consider the SVM function h given in (2). Suppose that it is possible to ﬁnd a FARB
with: m = NSV inputs qi, parameters ai, and membership functions 𝜇i
−, 𝜇i
+, so that
the following conditions hold
a0 +
m
∑
i=1
riai = b∗,
ziai = 𝛼∗
i yi,
gi(uiqi −vi) = K(x, si).
(15)
Then the IO mapping of the FARB is identical to that of the function h, i.e. O(q)=h(x).
One measure for the quality of rule extraction methods is their ﬁdelity [15], that is,
the percentage of examples on which the original black–box and the extracted rule–
base agree. The FARB provides perfect ﬁdelity, as its IO mapping is identical to that
of the SVM.

Knowledge Extraction from Support Vector Machines ...
371
Recall that the number of SVs may be much smaller than the total number of
training examples. This is a manifestation of the sparseness property of SVMs [2],
i.e. the fact that only a relatively small part of the training examples actually inﬂu-
ence the decision boundary. Since the FARB has 2m = 2NSV rules, this implies that
the FARB may be relatively small, and thus simple to understand even for complex
learning problems. In other words, the spareness property carries over to the equiv-
alent FARB.
Recall that popular kernel functions include [17]:
K(x, y) = xTy, (linear kernel)
K(x, y) = (1 + xTy∕c)d, c ∈ℝ, d ∈ℕ, (polynomial kernel)
K(x, y) = tanh(𝜌xTy + 𝜂), 𝜌> 0, 𝜂< 0, (MLP kernel)
K(x, y) = exp(−‖x −y‖2∕(2̂𝜎2)), ̂𝜎∈ℝ, (RBF or Gaussian kernel).
(16)
The next two corollaries show that for SVMs with MLP or RBF kernel functions,
the equivalence in Theorem 2 indeed holds.
Corollary 1 Consider the SVM function h given in (2) with an MLP kernel, i.e.
h(x) =
NSV
∑
i=1
𝛼∗
i yi tanh(𝜌xTsi + 𝜂) + b∗.
(17)
Consider a FARB with inputs qi = xTsi, i = 1, … , NSV, membership functions 𝜇i
+ =
𝜇>ki, 𝜇i
−= 𝜇<ki modeled using the Logistic MFs in (9) with 𝜏i = 2𝜌, ki = −𝜂∕𝜌,
and parameters a0 = b∗and ai = 𝛼∗
i yi, i = 1, … , NSV. Then the IO mapping of the
FARB is identical to that of the function h.
Proof. For Logistic MFs, (6) holds with zi = 1, ui = 𝜏i∕2 = 𝜌, vi = ki𝜏i∕2 = −𝜂,
ri = 0, and gi(x) = tanh(x), for i = 1, … , NSV. It is straightforward to verify that this
implies that the three conditions in (15) hold.
□
Note that the qis that appear in the FARB If–part have a clear geometric meaning.
Every qi = xTsi is the projection of the input x on an SV. If all the vectors are
normalized, then
qi = xTsi
= ||x||||si|| cos(𝜑)
= cos(𝜑),
where 𝜑is the angle between the vectors x and si. Thus, we can think of qi as a
measure of the “resemblance” between the current input x and the ith SV.
Specializing Theorem 2 to the case of an SVM with an RBF kernel yields the
following.

372
S. Duenyas and M. Margaliot
Corollary 2 Consider an SVM with an RBF kernel, i.e.
h(x) = b∗+
NSV
∑
i=1
𝛼∗
i yi exp
(
−‖x −si‖2
2̂𝜎2
)
.
(18)
Consider a FARB with inputs qi = ‖x−si‖, i = 1, … , NSV, linguistic terms equal to 0
and not equal to 0 modeled using the MFs 𝜇i
+ = 𝜇=0, 𝜇i
−= 𝜇≠0 in (10), and
parameters a0 = b∗+ ∑NSV
i=1 𝛼∗
i yi∕2, ai = 𝛼∗
i yi∕2, i = 1, … , NSV, 𝜎= ̂𝜎. Then the
IO mapping of the FARB is identical to that of the function h.
Proof. For the MFs in (10) with k = 0, (6) holds with zi = 2, ui = 1∕
√
2𝜎2, vi = 0,
ri = −1, and gi(x) = exp(−x2). It is straightforward to verify that this implies that
the three conditions in (15) hold.
□
Summarizing, Corollaries 1 and 2 show that MLP– or RBF–kernel SVMs are
equivalent to a FARB, and explicitly deﬁne the transformation S = T ◦P in (14).
5 Examples
The SVM–FARB equivalence yields a symbolic representation of the IO mapping
of an SVM. The equivalent FARB rules are determined by the SVM structure and
parameters. We now demonstrate this using several simple examples.
Example 3 Consider the training set {xi, yi}2
i=1 given by:
x1 = [1, 0]T , y1 = 1,
x2 = [2, 0]T , y2 = −1.
(19)
Training an SVM with kernel function K(x, y) = tanh(2xTy−5) (i.e. an MLP kernel
with 𝜌= 2 and 𝜂= −5) yields NSV = 2, 𝛼∗
1 = 𝛼∗
2 = 1.313 = 𝛼∗and b∗= 1.3065
(all the SVMs described in this section were derived using the C–support vector
classiﬁcation algorithm [38]). The SVM IO mapping is thus f(x) = sgn(h(x)), where
h(x) = b∗+ 𝛼∗[tanh(2xTx1 −5) −tanh(2xTx2 −5)]
= 1.3065 + 1.313 [tanh(2x1 −5) −tanh(4x1 −5)] .
(20)
Corollary 1 implies that this mapping is also the IO mapping of a FARB with:
two inputs q1 = xTx1 = x1 and q2 = xTx2 = 2x1, Logistic MFs 𝜇>ki, 𝜇<ki
with ki = 2.5, and 𝜏i = 4, and parameters a0 = 1.3065, a1 = 𝛼∗
1y1 = 1.313,
and a2 = 𝛼∗
2y2 = −1.313. This yields the four rule FARB:
R1 ∶If q1 > 2.5 & q2 > 2.5 Then O = a0 + a1 + a2 ≅1.3,
R2 ∶If q1 > 2.5 & q2 < 2.5 Then O = a0 + a1 −a2 ≅3.9,
R3 ∶If q1 < 2.5 & q2 > 2.5 Then O = a0 −a1 + a2 ≅−1.3,

Knowledge Extraction from Support Vector Machines ...
373
R4 ∶If q1 < 2.5 & q2 < 2.5 Then O = a0 −a1 −a2 ≅1.3.
Here and below & denotes ‘and’, qi < 2.5 stands for qi is smaller than 2.5,
and qi > 2.5 for qi is larger than 2.5. In other words, we use a “crisp” notation
such as >, but this is just a shorthand notation for a set modeled using a fuzzy MF.
This rule–base may be written as:
R1 ∶If x1 > 2.5 & x1 > 1.25 Then O ≅1.3,
R2 ∶If x1 > 2.5 & x1 < 1.25 Then O ≅3.9,
R3 ∶If x1 < 2.5 & x1 > 1.25 Then O ≅−1.3,
R4 ∶If x1 < 2.5 & x1 < 1.25 Then O ≅1.3.
Rule R2 is self–contradicting, so we delete it. The other three rules may be summa-
rized as:
If x1 > 2.5 or x1 < 1.25 Then O ≅1.3,
Else O ≅−1.3.
If we deﬁne the FARB decision to be sgn(O) (as in the SVM), then this becomes
If x1 > 2.5 or x1 < 1.25. Then sgn(O) = 1,
Else sgn(O) = −1.
This rule indeed correctly classiﬁes the two examples in (19). Figure 5 depicts the
decision region of the SVM classiﬁer (20). It is clear that this agrees well with the
verbal description given by the above rule.
□
The training set in the Example 3 is clearly linearly separable. The next example
describes KE from an SVM trained on a non–linearly separable training set.
Example 4 Consider the training set:
x1 = [1, 1]T,
y1 = 1,
x2 = [−1, −1]T,
y2 = 1,
x3 = [−1, 1]T,
y3 = −1,
x4 = [1, −1]T,
y4 = −1.
This corresponds to the classic symmetric XOR problem.
Applying the training algorithm (with the same kernel function as in Example 3)
yields an SVM with four SVs (i.e. all the training examples), and parameters 𝛼∗
i =
4.1977 ≅4.2, i = 1, … , 4, and b∗= 0. Thus, the SVM IO mapping is f(x) =
sgn(h(x)), with
h(x) = 4.2( tanh(2xTx1 −5) + tanh(2xTx2 −5)
−tanh(2xTx3 −5) −tanh(2xTx4 −5)).
(21)
Figure 6 depicts the equal height contours h(x) = c, for c = −1, 0, 1. Note that {x ∈
ℝ2 ∶h(x) = 0} is the decision boundary.
By Corollary 1, (21) is also the IO mapping of a FARB with: four inputs

374
S. Duenyas and M. Margaliot
Fig. 5
IO mapping f(x) = sgn(h(x)) of the SVM (20): + (∙) denotes points for which f(x) = 1
(f(x) = −1)
q1 = xTx1 = x1 + x2,
q2 = xTx2 = −x1 −x2,
q3 = xTx3 = −x1 + x2,
q4 = xTx4 = x1 −x2,
and MFs 𝜇>ki, 𝜇<ki deﬁned in (9) with 𝜏i = 2𝜌= 4, and ki = −𝜂∕𝜌= 2.5. The
parameters in the Then–part of the rules are a0 = b∗= 0, a1 = a2 = 4.2, and a3 =
a4 = −4.2.
Out of the sixteen rules in this FARB, seven are self–contradicting. After deleting
them we are left with a nine rule FRB. The IO mapping of this FRB is shown in Fig. 7.
Comparing this with Fig. 6 suggests that the FRB IO mapping is still quite close to
that of the SVM.
To obtain a more comprehensible representation, we use the fact that ﬁve of the
nine rules include O = 0 in their Then part. Since the ﬁnal classiﬁcation decision
is based on whether the output is positive or negative, we delete these rules. We are
left with the following FRB:
R1 ∶If q1 > 2.5 & q2 < 2.5 & q3 < 2.5 & q4 < 2.5
Then O = 8.4,
R2 ∶If q1 < 2.5 & q2 > 2.5 & q3 < 2.5 & q4 < 2.5
Then O = 8.4,
R3 ∶If q1 < 2.5 & q2 < 2.5 & q3 > 2.5 & q4 < 2.5
Then O = −8.4,
R4 ∶If q1 < 2.5 & q2 < 2.5 & q3 < 2.5 & q4 > 2.5
Then O = −8.4.

Knowledge Extraction from Support Vector Machines ...
375
Fig. 6
Equal height contours h(x) = c, for c = −1, 0, 1. The points x1, x2 (x3, x4) are marked
by + (∙)
Rewriting this in terms of the xis yields
R1 ∶If (x1 + x2 > 2.5) & (−2.5 < x2 −x1 < 2.5)
Then O = 8.4,
R2 ∶If (x1 + x2 < −2.5) & (−2.5 < x2 −x1 < 2.5)
Then O = 8.4,
R3 ∶If (−2.5 < x1 + x2 < 2.5) & (x2 −x1 > 2.5)
Then O = −8.4,
R4 ∶If (−2.5 < x1 + x2 < 2.5) & (x2 −x1 < −2.5)
Then O = −8.4.
To simplify this, consider rule R1. The If–part here is
(x1 + x2 > 2.5) & (−2.5 < x2 −x1 < 2.5).
It is straightforward to verify that this condition implies that
(x1 > 0) & (x2 > 0).

376
S. Duenyas and M. Margaliot
Fig. 7
IO mapping O(x1, x2)
of the nine–rule FRB, and
several equal height
contours O(x1, x2) = ci. The
bold contour corresponds
to O(x1, x2) = 0
Similarly, the If–part in Rules R2, R3, and R4 imply
(x1 < 0) & (x2 < 0), (x1 < 0) & (x2 > 0), (x1 > 0) & (x2 < 0),
respectively. The FRB can thus be written as
R1 ∶If (x1 > 0) & (x2 > 0) Then O = 8.4,
R2 ∶If (x1 < 0) & (x2 < 0) Then O = 8.4,
R3 ∶If (x1 < 0) & (x2 > 0) Then O = −8.4,
R4 ∶If (x1 > 0) & (x2 < 0) Then O = −8.4.
This can be summarized as
If sgn(x1) = sgn(x2) Then sgn(O) = 1,
Else sgn(O) = −1,
and this is indeed a succinct verbal description of the XOR function.
We note that in this example the trained SVM turned out to be symmetric in the
sense that b∗= 0, and 𝛼∗
i = 𝛼∗
j , for all i, j. This led a to symmetric FARB where
many of the rules could be deleted without aﬀecting the ﬁdelity.
The next example describes KE from a hierarchical SVM trained to classify the
Iris data set.
Example 5 The Iris classiﬁcation problem is a common benchmark for demon-
strating KE methods from various machine learning algorithms. The data con-
sists of 150 examples of irises classiﬁed into three classes. Each example is in
the form (z1, z2, z3, z4, c) where the zis are physical parameters of the ﬂower (sepal

Knowledge Extraction from Support Vector Machines ...
377
sgn(h1) = 1
SVM1
sgn(h2) = 1
SVM2
sgn(h2) = −1
Class 3
Class 2
Class 1
sgn(h1) = −1
Fig. 8
Graphical representation of the hierarchical classiﬁcation method in Example 5. Class 1 is
Setosa, class 2 is Versicolor and class 3 is Virginica
length, sepal width, petal length and petal width, respectively), and c ∈{1, 2, 3} is
the classiﬁcation in one of three classes: Setosa, Versicolor, and Virginica, respec-
tively. Each class includes 50 examples.
We trained the hierarchical SVM depicted in Fig. 8. If the output of SVM1 sat-
isﬁes f1 = sgn(h1) = 1, then the classiﬁcation decision is Setosa; otherwise the
example is fed into SVM2 that separates between Virginica and Versicolor.
Preprocessing included scaling all the examples via xi =
zi
mi −1, i = 1, … , 4,
where mi is the maximal value of the ith feature over the 150 examples. This guar-
antees that each scaled feature satisﬁes xi ∈[−1, 1].
Knowledge Extraction from SVM1
We used the kernel function K(x, y) = tanh(𝜌xTy + 𝜂), with 𝜌= 1.5, 𝜂= −0.75.
The trained SVM1 includes two SVs, and parameter values y1𝛼∗
1 = 3.32, y2𝛼∗
2 =
−3.32, b∗= −2.178, so its IO mapping is f1(x) = sgn(h1(x)) where:
h1(x) = 3.32( tanh(1.5xTs1 −0.75) −tanh(1.5xTs2 −0.75)) −2.178.
This mapping is depicted in Fig. 9. Substituting the SVs yields:
h1(x) = 3.32( tanh(1.5q1 −0.75) −tanh(1.5q2 −0.75)) −2.178,
(22)
where q1 = 0.468x1 + 0.818x2 −0.652x3 −0.84x4, and q2 = 0.443x1 + 0.181x2 +
0.014x3 −0.2x4.
By Corollary 1, the mapping (22) is also the IO mapping of a FARB with in-
puts qi, i = 1, 2, MFs 𝜇>k, 𝜇<k, and parameter values: 𝜏= 2𝜌= 3, k = −𝜂∕𝜌= 0.5,
a0 = b∗= −2.178, and a1 = 3.32, a2 = −3.32, i.e. the four–rule FARB:
R1 ∶If q1 > 0.5 & q2 > 0.5 Then O = −2.178,

378
S. Duenyas and M. Margaliot
Fig. 9
Classiﬁcation of the data set by SVM1 projected to the x3 −x4 plane. + (∙) denotes points
for which sgn(h1(x)) = 1 (sgn(h1(x)) = −1). SVs are encircled
R2 ∶If q1 > 0.5 & q2 < 0.5 Then O = 4.462,
R3 ∶If q1 < 0.5 & q2 > 0.5 Then O = −8.818,
R4 ∶If q1 < 0.5 & q2 < 0.5 Then O = −2.178.
This may be summarized as:
If q1 > 0.5 & q2 < 0.5 Then sgn(O) = 1,
Else sgn(O) = −1.
Examination of the training set shows that q2 < 0.5 for 148 of the 150 examples, so
we delete this condition from the rule. This yields
If q1 > 0.5 Then sgn(O) = 1,
Else sgn(O) = −1.
Recall that q1 = 0.468x1 + 0.818x2 −0.652x3 −0.84x4 Thus, the simple rule above
includes a single linear function of the xis and, therefore, a linear function of the
ﬂower features (i.e., the zis). To further simplify this rule, rewrite q1 > 0.5 as
−0.652x3 −0.84x4 > 0.5 −0.468x1 −0.818x2.
(23)
Examination of the data set shows that x1 ≥0.0886 and x2 ≥−0.0909 for all the
examples. Hence, we replace the right–hand side of (23) by
0.5 −0.468 ⋅0.0886 −0.818 ⋅(−0.0909) = 0.5329.

Knowledge Extraction from Support Vector Machines ...
379
Fig. 10
Classiﬁcation of the training set using the fuzzy rule extracted from SVM1: + (∙) denotes
examples classiﬁed as Setosa (Versicolor or Virginica). The line is the classiﬁcation threshold
This yields the condition
−0.652x3 −0.84x4 > 0.5329.
(24)
Restating (24) in terms of the original (unnormalized) features via zi = mi(1 + x1)
yields:
−0.19z3 −0.672z4 + 1.492 > 0.5329.
(25)
Recalling that z3 is petal length (PL), and z4 is petal width (PW) provides the ﬁnal
rule:
If (PW < 1.427 −0.282 ⋅PL) Then class is Setosa,
Else class is either Versicolor or Virginica.
This rule provides a correct classiﬁcation for all the 150 examples in the training set
(see Fig. 10). Furthermore, note that the decision line depicted in this ﬁgure suggests
that this fuzzy rule is not too far from an optimal margin classiﬁer.
Knowledge Extraction from SVM2
SVM2 separates between the classes Versicolor and Virginica. These two classes
are not linearly separable. Training is based on the 100 examples in the iris data set
that belong to either class Versicolor or Virginica, and the kernel function K(x, y) =
tanh(𝜌xTy+𝜂), with 𝜌= 0.22, 𝜂= −0.352. This yields an SVM with 14 SVs. Six of
these have very small 𝛼∗
i values, so we set 𝛼∗
i = 0 for these SVs, leaving eight SVs
with parameter values:
𝜶∗= [104, 2719.1, 3708.8, 104, 4417.2, 2010.7, 104, 104],

380
S. Duenyas and M. Margaliot
Fig. 11
Classiﬁcation of SVM2 projected to the x3 −x4 plane: ∙(∗) denotes points for
which sgn(h2) = 1 (sgn(h2) = −1). Errors are squared. The SVs are encircled
and b∗= 18.936. Since b∗≪𝛼∗
i for all i, we also set b∗= 0. Therefore, the IO
mapping of the (simpliﬁed) SVM is f2(x) = sgn(h2(x)) where:
h2(x) = 104k1(x) + 2719.1k2(x) + 3708.8k3(x) + 104k4(x)
−4417.2k5(x) −2010.7k6(x) −104k7(x) −104k8(x),
(26)
with ki(x) = tanh(0.22xTsi −0.352). This SVM correctly classiﬁes 99 of the 100
examples in the training set (see Fig. 11).
By Corollary 1, the mapping in (26) is also the IO mapping of a FARB with:
inputs qi = xTsi, MFs 𝜇>ki, 𝜇<ki, and parameter values ki = −𝜂∕𝜌= 1.6, 𝜏i = 2𝜌=
0.44, a0 = 0, and
[a1, … , a8] = [104, 2719.1, 3708.8, 104, −4417.2, −2010.7, −104, −104].
This yields a FARB with 256 rules. A calculation shows that for all the examples in
the training set, only q3 and q6 ever exceed the value k = 1.6. Thus, all the rules that
contain the linguistic term qi is larger than k for i ∈{1, 2, 4, 5, 7, 8} are deleted
from the rule–base. We are left with an FRB with just four rules:
R1 ∶If q3 > 1.6 & q6 > 1.6 Then O = 3396.2,
R2 ∶If q3 > 1.6 & q6 < 1.6 Then O = 7417.6,
R3 ∶If q3 < 1.6 & q6 > 1.6 Then O = −4021.4,
R4 ∶If q3 < 1.6 & q6 < 1.6 Then O = 0.

Knowledge Extraction from Support Vector Machines ...
381
Fig. 12
Classiﬁcation of the
FRB extracted from SVM2:
+ (∙) denotes points of class
Versicolor (Virginica).
Errors are encircled. The
threshold is doted
We may summarize this using the single rule:
If q3 < 1.6 & q6 > 1.6 Then sgn(O) = −1,
Else sgn(O) = 1.
Substituting q3 and q6 yields:
If 0.696x1 + 0.363x2 + 0.449x3 + 0.36x4 < 1.6
and x1 + 0.727x2 + 0.855x3 + 0.6x4 > 1.6
Then sgn(O) = −1, Else sgn(O) = 1.
Examining the training set, we ﬁnd that the average value of x1 is ̄x1 = 0.5853, and
that x2 ≤0.7272. Replacing x1 by ̄x1 and x2 by 0.7272 in the FRB yields:
If 0.449x3 + 0.36x4 < 0.9287 and 0.855x3 + 0.6x4 > 0.48
Then sgn(O) = −1, Else sgn(O) = 1.
Examining the two parts of this rule, we ﬁnd that the ﬁrst condition in the If–part
holds for all the training example, so we delete this part. This yields:
If 0.855x3 + 0.6x4 > 0.48 Then sgn(O) = −1,
Else sgn(O) = 1.
Converting back to the original feature values yields:
If (7.93 −2 ⋅PW < PL) Then class is Virginica,
Else class is Versicolor,
where PL is petal length, and PW is petal width (in centimeters). This simple rule
classiﬁes the training set with an 4 % error (see Fig. 12).
Figure 13 summarizes the hierarchical FRB obtained by KE from SVM1 and
SVM2. This FRB classiﬁes the data set with an 2.66 % error. Thus for this example
the KE approach provides a highly transparent classiﬁer with a negligible degrada-
tion in performance.
□

382
S. Duenyas and M. Margaliot
Versicolor Virginica
7.93 −2 · PW ≥PL
FRB2
PW ≥1.427 −0.282 · PL
FRB1
PW < 1.427 −0.282 · PL
7.93 −2 · PW < PL
Setosa
Fig. 13
Graphical representation of the extracted hierarchical FRB
6 Discussion
Certain classes of SVMs produce an input–output mapping that is mathematically
equivalent to that of a corresponding FARB. This FARB then provides a symbolic
representation of the SVM functioning stated in the form of fuzzy rules. This yields a
new approach for KE from SVMs. Unlike previous approaches, the FARB uses only
standard tools from fuzzy logic theory. Furthermore, the size of the extracted FARB
depends on the number of SVs, that may be much smaller than the number of training
examples.
We demonstrated this KE approach using several examples of SVMs trained to
solve classiﬁcation tasks. We deliberately used relatively simple tasks, as these allow
a clear presentation of the rule extraction and simpliﬁcation process. Application of
our approach for larger problems is left for future work.
One drawback of the FARB is that the number of rules increases exponentially
with the number of inputs. In our context of KE from SVMs, the number of rules in-
creases exponentially with the number of SVs. An important direction for further re-
search is thus developing a systematic approach for simplifying the extracted FARB
in order to increase transparency. One possible approach is to ﬁrst simplify the SVM
itself, and then transform the simpliﬁed SVM into a corresponding FARB. It is im-
portant to note that several studies have already developed methods for simplifying
SVMs and, in particular, for reducing the number of SVs (see, e.g., [39–46]). This
is motivated by the need to reduce the run time of the training and classiﬁcation
algorithms, but is of course highly relevant in the context of KE using the FARB.
An interesting question is how to modify the training algorithm beforehand so
that the resulting SVM will correspond to a simple as possible FARB. For example,
setting certain parameters (e.g. 𝜌, 𝜂) properly may lead to a FARB with MFs 𝜇>ki,
with the ki values relatively large. Then it may be possible to delete all the rules that
contain the term: xi is larger than ki, as their degree of ﬁring is close to zero.

Knowledge Extraction from Support Vector Machines ...
383
Acknowledgments This research was partially supported by a research grant from the Israel Sci-
ence Foundation (ISF).
References
1. Vapnik, V.N.: Statistical Learning Theory. John Wiley and Sons, New York, NY (1998)
2. Mammone, A., Turchi, M., Cristianini, N.: Support vector machines. Wiley Interdisc. Rev.
Comput. Stat. 1, 283–289 (2009)
3. Taylor, J., Cristianini, N.: Kernel Methods for Pattern Analysis. Cambridge University Press,
Cambridge, UK (2004)
4. Scholkopf, B., Smola, A.: Learning with Kernels: Support Vector Machines, Regularization,
Optimization, and Beyond. MIT Press, Cambridge, MA, USA (2001)
5. Burges, C.J.C.: A tutorial on support vector machines for pattern recognition. Data Min. Knowl.
Disc. 2, 121–167 (1998)
6. Duenyas, S., Margaliot, M.: Knowledge extraction from a class of support vector machines
using the fuzzy all-permutations rule-base. In: IEEE Symposium Series on Computational
Intelligence (SSCI 2011), pp. 1–7. Paris, France (2011)
7. Cloete, I., Zurada, J.M. (eds.): Knowledge-Based Neurocomputing. MIT Press (2000)
8. Kolman, E., Margaliot, M.: Knowledge-Based Neurocomputing: A Fuzzy Logic Approach.
Springer, Berlin (2009)
9. Craven, M.W.: Extracting comprehensible models from trained neural networks. Ph.D. thesis,
Department of Computer Science, University of Wisconsin-Madison (1996)
10. Darbari, A.: Rule extraction from trained ANN: a survey. Deppartment of Computer Science,
Dresden Technology University. Technical report, Institute of Artiﬁcial intelligence (2000)
11. Bose, B.K.: Expert system, fuzzy logic, and neural network applications in power electronics
and motion control. Proc. IEEE 82, 1303–1323 (1994)
12. Hart, A., Wyatt, J.: Evaluating black-boxes as medical decision aids: issues arising from a study
of neural networks. Inf. Health Soc. Care 15, 229–236 (1990)
13. Sestito, S., Dillon, T.S.: Automated Knowledge Acquisition. Prentice Hall (1994)
14. Hunter, L., Klein, T.: Finding relevant biomolecular features. In: Proceedings 1st International
Conference Intelligent Systems for Molecular Biology, pp. 190–197 (1993)
15. Andrews, R., Diederich, J., Tickle, A.B.: Survey and critique of techniques for extracting rules
from trained artiﬁcial neural networks. Knowl.-Based Syst. 8, 373–389 (1995)
16. Martens, D., Huysmans, J., Setiono, R., Vanthienen, J., Baesens, B.: Rule extraction from sup-
port vector machines: An overview of issues and application in credit scoring. In: Diederich,
J. (ed.) Rule Extraction From Support Vector Machines, pp. 33–63. Springer, Berlin (2008)
17. Martens, D., Baesens, B., Gestel, T.V., Vanthienen, J.: Comprehensible credit scoring models
using rule extraction from support vector machines. Eur. J. Oper. Res. 183, 1466–1476 (2007)
18. Barakat, N., Diederich, J.: Learning-based rule-extraction from support vector machines. In:
Proceedings 14th International Conference Computer Theory and Applications (ICCTA’2004),
Alexandria, Egypt (2004)
19. Craven, M., Shavlik, J.: Extracting tree-structured representations of trained neural networks.
In: Mozer, M.C., Jordan, M.I., Petsche, T. (eds.) Advances in Neural Information Processing
Systems 9, pp. 24–30. MA, MIT Press, Cambridge (1996)
20. Johansson, U., Konig, R., Niklasson, L.: The truth is in there - rule extraction from opaque mod-
els using genetic programing. In: Proceedings 17th International Florida AI Research Sympo-
sium Conference (FLAIRS’04), Miami Beach, FL (2004)
21. Koza, J.R.: Genetic Programming: On the Programming of Computers by Means of Natural
Selection. MIT Press, Cambridge, MA (1992)
22. Fung, G., Sandilya, S., Rao, R.B.: Rule extraction from linear support vector machines via
mathematical programming. In: Diederich, J. (ed.) Rule Extraction From Support Vector Ma-
chines, pp. 83–108. Springer, Berlin (2008)

384
S. Duenyas and M. Margaliot
23. Ren, L., Garcez, A.: Symbolic knowledge extraction from support vector machines: a geomet-
ric approach. In: Koppen, M., Kasabov, N., Coghill, G. (eds.), Advances in Neuro-Information
Processing. Volume 5507 of Lecture Notes in Computer Science., pp. 335–343. Berlin,
Springer (2009)
24. Nunez, H., Angulo, C., Catala, A.: Rule extraction from support vector machines. In: Proceed-
ings 10th European Symposium on Artiﬁcial Neural Networks (ESANN’02), pp. 107–112.
Bruges, Belgium (2002)
25. Martens, D., Baesens, B., Van Gestel, T.: Decompositional rule extraction from support vector
machines by active learning. IEEE Trans. Knowl. Data Eng. 21, 178–191 (2009)
26. Castro, J.L., Flores-Hidalgo, L., Mantas, C.J., Puche, J.M.: Extraction of fuzzy rules from
support vector machines. Fuzzy Sets Syst. 158, 2057–2077 (2007)
27. da Costa, F., Chaves, A., Vellasco, M., Tanscheit, R.: Fuzzy rules extraction from support
vector machines for multi-class classiﬁcation. Neural Comput. Appl. 22, 1571–1580 (2013)
28. Moewes, C., Kruse, R.: On the usefulness of fuzzy SVMs and the extraction of fuzzy rules from
SVMs. In: Proceedings 7th Conference European Society for Fuzzy Logic and Technology
(EUSFLAT-2011), pp. 943–948. Aix-Les-Bains, France (2011)
29. Kolman, E., Margaliot, M.: Are artiﬁcial neural networks white boxes? IEEE Trans. Neural
Networks 16, 844–852 (2005)
30. Roth, I., Margaliot, M.: Analysis of artiﬁcial neural network learning near temporary minima:
a fuzzy logic approach. Fuzzy Sets Syst. 161, 2569–2584 (2010)
31. Kolman, E., Margaliot, M.: A new approach to knowledge-based design of recurrent neural
networks. IEEE Trans. Neural Networks 19, 1389–1401 (2008)
32. Kolman, E., Margaliot, M.: Extracting symbolic knowledge from recurrent neural networks-a
fuzzy logic approach. Fuzzy Sets Syst. 160, 145–161 (2009)
33. Margaliot, M., Langholz, G.: Hyperbolic approach to fuzzy modeling and control. In: Pro-
ceedings IEEE International Conference Fuzzy Systems (FUZZ-IEEE’99), pp. 47–52. Seoul,
Korea (1999)
34. Margaliot, M., Langholz, G.: Hyperbolic optimal control and fuzzy control. IEEE Trans. Syst.
Man Cybern. 29, 1–10 (1999)
35. Margaliot, M., Langholz, G.: New Approaches to Fuzzy Modeling and Control - Design and
Analysis. World Scientiﬁc (2000)
36. Margaliot, M., Langholz, G.: A new approach to fuzzy modeling and control of discrete-time
systems. IEEE Trans. Fuzzy Syst. 11, 486–494 (2003)
37. Sousa, J.W.C., Kaymak, U.: Fuzzy Decision Making in Modeling and Control. World Scientiﬁc
(2002)
38. Xuchen, Y.: Iris data set classiﬁcation using support vector machine (2007)
39. Downs, T., Gates, K.E., Masters, A.: Exact simpliﬁcation of support vector solutions. J. Mach.
Learn. Res. 2, 293–297 (2001)
40. Nguyen, D.D., Ho, T.B.: An eﬃcient method for simplifying support vector machines. In: Pro-
ceedings International Conference Machine Learning (ICML’05), pp. 617–624. Bonn, Ger-
many (2005)
41. Bakir, G.H., Bottou, L., Weston, J.: Breaking SVM complexity with cross-training. In: Saul,
L.K., Weiss, Y., Bottou, L. (eds.) Advances in Neural Information Processing Systems 17, pp.
81–88. MA, MIT Press, Cambridge (2005)
42. Zhan, Y., Shen, D.: Design eﬃcient support vector machine for fast classiﬁcation. Pattern
Recogn. 38, 157–161 (2005)
43. Li, Y., Zhang, W., Lin, C.: Simplify support vector machines by iterative learning. Neural Inf.
Process. Lett. Rev. 10, 11–17 (2006)
44. Amari, S., Wu, S.: Improving support vector machine classiﬁers by modifying kernel functions.
Neural Networks 12, 783–789 (1999)
45. Burges, C.: Simpliﬁed support vector decision rules. In: Proceedings 13th International Con-
ference Machine Learning (ICML’96), pp. 71–77. Bari, Italy (1996)
46. Liang, X.: An eﬀective method of pruning support vector machine classiﬁers. IEEE Trans.
Neural Networks 21, 26–38 (2010)

Knowledge Extraction from Support Vector Machines ...
385
Authors Biography
Michael Margaliot received the B.Sc. (cum laude) and M.Sc.
degrees in Electrical Engineering from the Technion - Israel
Institute of Technology - in 1992 and 1995, respectively, and
the Ph.D. degree (summa cum laude) from Tel Aviv University
in 1999. He was a post-doctoral fellow in the Department of
Theoretical Mathematics at the Weizmann Institute of Science,
Rehovot, Israel. In 2000, he joined the faculty of the Depart-
ment of Electrical Engineering-Systems, Tel Aviv University,
where he is currently an associate professor.
Dr. Margaliot’s research interests include stability theory,
switched systems, optimal control theory, Boolean control net-
works, fuzzy modeling and control, and systems biology. He
is co-author of New Approaches to Fuzzy Modeling and Con-
trol: Design and Analysis (World Scientiﬁc, 2000) and of
Knowledge-Based Neurocomputing: A Fuzzy Logic Approach
(Springer, 2009).
Shahaf Duenyas
received the B.Sc. and M.Sc. degrees in
Electrical Engineering from Tel Aviv University in 2010 and
2011, respectively. Shahaf is currently working in Polycom
Israel, a company developing video conferencing algorithms.

On Type-Reduction Versus Direct
Defuzziﬁcation for Type-2 Fuzzy Logic
Systems
Jerry M. Mendel
Abstract This chapter examines type-reduction and direct defuzziﬁcation for
interval type-2 fuzzy logic systems. It provides critiques of type-reduction as an end
to itself as well as of direct defuzziﬁcation, and concludes that: (1) a good way to
categorize type-reduction/direct defuzziﬁcation algorithm papers is as papers that
either focus on algorithms that lead to a type-reduced set, or directly to a defuzziﬁed
value; (2) research on type-reduction as an end to itself has led to results that are
arguably of very little value; and, (3) the practice of base-lining an IT2 FLS that
uses direct defuzziﬁcation against one that uses type-reduction followed by de-
fuzziﬁcation is unnecessary.
1
Introduction
A type-2 fuzzy set (Fig. 1) (T2 FS) can be thought of as a fuzzy-fuzzy set. Its
membership function (MF) no longer has a single value at each value of the primary
variable, but instead is a blurred version of that function, i.e., at each value of the
primary variable the membership is itself a function, called a secondary MF. When
the secondary MF is a constant equal to 1, the T2 FS is called an interval type-2
fuzzy set (IT2 FS) or an interval-valued fuzzy set; otherwise, it is called a general
type-2 fuzzy set (GT2 FS).
The MF of a T2 FS is three-dimensional, with x-axis called the primary variable,
y-axis called the secondary variable and z-axis called the MF value (or secondary
grade). A vertical slice is a plane that is parallel to the MF-value z-axis. The footprint
of uncertainty (FOU) of a T2 FS lies on the x-y plane and includes the closure of all
points on that plane for which the MF value is non-zero; it is the 2D-domain on which
J.M. Mendel (✉)
Ming Hsieh Department of Electrical Engineering, Signal & Image Processing Institute,
University of Southern California, Los Angeles, CA 90089-2564, USA
e-mail: mendel@sipi.usc.edu
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_20
387

sit the secondary grades. The FOU is lower and upper bounded by a lower mem-
bership function (LMF) and an upper membership function (UMF), both of which are
type-1 fuzzy sets (T1 FSs). The FOU can be completely covered by T1 FSs that are
called embedded T1 FSs.
T2 FSs are used in type-2 fuzzy logic systems (T2 FLSs) (Fig. 2). A general T2
FLS (GT2 FLS) was originally called a T2 FLS; however, because most of the
works about T2 FLSs have focused on IT2 FSs, and only more recently on GT2
FSs, we now view the ﬁeld of T2 FLSs as the union of the sub-ﬁelds of IT2 FLSs
and GT2 FLSs.
Practical applications of T2 FLSs (e.g., fuzzy logic control [26]) require a
number at the output of the FLS and not a FS. Readers of this book know that for a
T1 FLS such a number is obtained by a process called defuzziﬁcation, which can be
Fig. 1 Components of a General type-2 fuzzy set
Type-reduced 
Set (Type-1)
Rules
Crisp
inputs
IT2 FSs
Crisp
outputs
x
X
Inference
Type-reducer
Output Processing
X
IT2 FSs
Y
y
Y
Fig. 2 Type-2 fuzzy logic system [23]. When all T2 FSs are IT2 FSs, the GT2 FLS becomes an
IT2 FLS
388
J.M. Mendel

interpreted as the projection of a T1 FS into a type-0 FS. Many kinds of defuzziﬁers
are possible, including center of gravity (centroid), height and center of sets.
So, how does one go from a T2 FS to a number? Nilesh Karnik and I struggled
with how to do this for months and ﬁnally we came up with a two-stage approach
[14, 23]: type-reduction (TR)—a new concept for FSs—followed by defuzziﬁcation.
TR projects a T2 FS into a T1 FS, after which that T1 FS is projected into a type-
0-FS by defuzziﬁcation to obtain a number. Because a type-reduced set is a T1 FS,
defuzziﬁcation is achieved by computing the centroid of that set. For an IT2 FLS
the type-reduced set is an interval set whose center of gravity is the average value of
its two end-points; so, defuzziﬁcation for an IT2 FLS is trivial.
Just as there can be different kinds of defuzziﬁcation methods for a T1 FLS,
there can be different kinds of TR methods for a T2 FLS. It is generally agreed that
all TR methods must satisfy Karnik and Mendel’s [23] fundamental design
requirement for a T2 FLS, namely: When all sources of [membership function]
uncertainty disappear, a T2 FLS must reduce to a comparable T1 FLS. This design
requirement seems as reasonable today (in 2015) as it did to Karnik and Mendel in
2001, and is analogous to what happens to a probability density function when
random uncertainties disappear. In that case, the variance of the pdf goes to zero,
and a probability analysis reduces to a deterministic analysis. So, just as the
capability for a deterministic analysis is embedded within a probability analysis, the
capability for a T1 FLS is embedded within a T2 FLS.
As is stated in [25]: “TR became burned into the architecture of a T2 FLS
because Karnik and Mendel ﬁrst developed all of their T2 concepts and calculations
for a general T2 FS and a GT2 FLS. Although TR was originally developed for a
GT2 FLS, because it was so simple to perform for an IT2 FLS it was kept in the
architecture of an IT2 FLS. There is nothing wrong with doing this; however, in
retrospect we may have been blind-sided by the need for TR in a GT2 FLS from
asking the question ‘Is TR really needed in an IT2 FLS?’
“In fact, there are many ways to go from an IT2 FLS to a number that bypass TR
and still satisfy the fundamental design requirement.”
“A student in a class that I taught some years ago asked: ‘Instead of performing
TR, why can’t we just use a combination of two T1 FLSs, one that uses only the
lower membership functions and the other that uses only the upper membership
functions?’ My answer at that time was: ‘You can’t do this because each end-point
of the type-reduced set uses a mixture of lower and upper membership function
information.’ While my answer was technically correct, it was predicated on using
type-reduction, rather than on what the student had suggested. My answer today
would be: ‘You can do what you are suggesting, and this can be done in different
ways; however, by bypassing TR you may not be able to provide a measure of the
uncertainties that have ﬂowed through all of the IT2 FLS computations (analogous
to a standard deviation).’ For example, you could begin with the architecture of an
IT2 FLS as a linear combination of two T1 FLSs, as in [1], or as the centroid of the
average of the lower and upper membership functions of the aggregated rule ﬁred
sets, as in [29]. All of these IT2 FLSs go directly to the defuzziﬁed output value and
they all satisfy the fundamental design requirement.”
On Type-Reduction Versus Direct Defuzziﬁcation for Type-2 …
389

Unfortunately, TR cannot be performed using a closed-form mathematical for-
mula. Its calculations led to two simple iterative algorithms that have been and
continue to be called KM-Algorithms (or Karnik-Mendel Algorithms) [14, 23].
Because the computational complexity of using GT2 FSs was so great in the late
1990s, until around 2008 we as well as all others focused on TR for IT2 FLSs. It is
fair to say that all thinking about type-reduction was in the context of IT2 FLSs.
Karnik and Mendel’s two-step approach for going from a T2 FS to a number
spawned a plethora of research and subsequent publications that fall into two
categories: (1) type-reduction as an end to itself (i.e., TR viewed as a mathematical
problem but with no application in mind, focusing on ﬁnding improved ways to
perform TR); and, (2) direct defuzziﬁcation (i.e., methods for bypassing TR that
project a T2 FS directly into a number). Unfortunately, some authors use the phrase
“type-reduction” in the titles of papers that are about direct defuzziﬁcation, which is
(in the opinion of this author) arguably incorrect.
Two journal articles that cover all aspects of this appeared in 2013, [24, 33]. The
article by Wu [33] categorizes type-reduction and direct defuzziﬁcation algorithms
into the following two categories:
• Enhancements to the KM TR algorithms (W1) (these are about type-reduction)
and
• Alternative
TR
algorithms
(W2)
(these
are
almost
all
about
direct
defuzziﬁcation)
The article by Mendel [24] categorizes type-reduction algorithms into the fol-
lowing four categories:
• Improved KM algorithms (M1) (these are about type-reduction)
• Understanding the KM/EKM algorithms leading to further improved algorithms
(M2) (these are about type-reduction)
• Non-KM algorithms that preserve the ability to approximate the centroid or
type-reduced set (M3) (these are about type-reduction), and,
• Non-KM algorithms that do not preserve the ability to approximate the centroid
or type-reduced set (M4) (these are about direct defuzziﬁcation)
While both articles contain a wealth of information about type-reduction and
direct defuzziﬁcation algorithms, it requires a considerable effort by readers to relate
an algorithm to its different categorizations across these two papers. In order to help
with this, I will now classify the TR algorithms differently from both of these
articles, as:
• Algorithms that lead to a type-reduced set (Table 1)
• Algorithms that lead directly to a defuzziﬁed value (Table 2)
The connections between these two classes of algorithms and the classiﬁcations
in [24, 33] are given in these tables.
390
J.M. Mendel

Table 1 Algorithms that lead to a type-reduced set
Authors/references
Year
Wu
Class [33]
Mendel
Class [24]
Emphasizes
FLS
Application?
Baselines
against
Speedup
Accuracy
Wu and Mendel [37]
2002
W2
M3
No
No
Time-series forecasting
Nothing
Niewiadomski, Ocheleska and Szczepaniak [30]
2006
Nonea
M3
No
No
Linguistic summarization
Nothing
Melgarejo [22]
2007
W1
M3
Yes
Nob
None
KM
Duran, Bernal and Melgarejo [5]
2008
W1
M3
Yes
Noc
None
EKM
Li, Yi and Zhao [17]
2008
W2
M3d
Yes
Yes
Fuzzy logic control
COS TR
Wu and Mendel [34]
2009
W1
M1
Yes
No
None
KM
Hu, Zhao and Yang [13]
2010
W1
M3
Yes
Yes
None
EKM
Wu and Nie [35]
2011
W1
M1
Yes
No
None
KM and EKM
Liu andMendel [19]
2011
W1e
M2
No
Yes
None
Nothing
Liu, Qin and Wu [21]
2012
W1e
M2
Yes
No
None
Continuous EKM
Liu, Mendel and Wu [20]
2012
Ff
M2
Yes
Yes
None
Nothing
Hu, Wang and Cai [12]
2012
W1
M3
Yes
No
None
EKM
Ulu, Guzelkaya and Eksin [32]
2013
W2
M3
Yes
Yes
None
EKM
a Wu includes this reference but does not include its algorithm in the main body of his paper
b Same accuracy achieved for their algorithms and KM algorithms
c Same accuracy achieved for their algorithms and EKM algorithms
d Although this paper is not listed in [24], if it had been it would be in M3
e Although this paper is not listed in [33], if it had been it would be in W1
f Mentioned only in a footnote in [33]
On Type-Reduction Versus Direct Defuzziﬁcation for Type-2 …
391

Table 2 Algorithms that lead directly to a defuzziﬁed value
Authors/references
Year
Wu
Class [33]
Mendel
Class [24]
Emphasizes
FLS
Application?
Baselines
against
Speedup
Accuracy
Dziech and Gorzalczany [6]
1987
W2a
M4
No
No
Signal transmissionb
Nothing
Gorzalczany [7]
1988
W2
M4
No
No
Fuzzy logic controlc
Nothing
Liang and Mendel [18]
2000
W2
M4d
No
No
Equalization
Nothing
Wu and Tan [36]
2005
W2
M4
Yes
Noe
PI Control
WM uncertainty bounds +
defuzziﬁcation
Coupland and John [3]
2007
W2
M4d
Yes
No
None
Nothing
Nie andTan [29]
2008
W2
M4
Yes
Yesf
Fuzzy logic control
COS TR + defuzziﬁcation and WM
uncertainty bounds + defuzziﬁcation
Greenﬁeld et al. [9]
2009
W2
M4
Yes
Yes
None
KM
Greenﬁeld, Chiclana and John [10]
2009
W2
M4
No
Yes
None
Exhaustive defuzziﬁcation
Greenﬁeld, Chiclana and John [11]
2009
W2
M4
Yes
Yes
None
Exhaustive defuzziﬁcation
Du and Ying [4]
2010
W2
M4d
No
No
Fuzzy logic control
COS TR + defuzziﬁcation
Biglarbegian, Melek and Mendel [1]
2010
W2
M4
No
No
Fuzzy logic control
Nothing
Biglarbegian, Melek and Mendel [2]
2011
W2
M4
No
No
Function approx.,
identiﬁcation
T1 FLS
Greenﬁeld and Chiclana [8]g
2011
W2
M4
No
Yes
None
Exhaustive defuzziﬁcation
Mendel and Liu [27]a, d
2012
W2
M4
Yes
Yes
None
Nie-Tan
Tau, et al. [31]
2012
W2
M4d
Yes
No
Fuzzy logic control
Other controllers
Mendel and Liu [28]a, d
2013
W2
M4
Yes
Yes
None
Nie-Tan
Khosravi, Nahavandi &
Khosravi [15]
2013
W2a
M4d
Yes
Yes
Time-series
prediction,
identiﬁcation
Five other methods
Khosravi & Nahavandi [16]
2014
W2a
M4d
Yes
Yes
Load forecasting
Five other methods
a Although this paper is not listed in [33], if it had been it would be in W2
b Not much detail is provided for this application
c This application is only suggested, but it is not examined in the paper
d Although this paper is not listed in [24], if it had been in would be in M4
e Commonly used control metrics, IAE, ISE and ITAE are compared
f Uses ITAE to do this
g Although “type-reduction” appears in the title of this paper, no type-reduced set is obtained in it
392
J.M. Mendel

2
Explanations of the Tables
Tables 1 and 2 organize the papers chronologically, so that one can see some sort of
continuity in thought as time progresses. Unfortunately, no standards existed when
these studies were performed (nor do they exist today, in 2015) for how to compare
TR or direct defuzziﬁcation algorithms, so it is very difﬁcult to draw statistically
meaningful conclusions from the papers that are in these two tables. Most of the
time the authors used different IT2 FSs (FOUs) (some of which are sampled ver-
sions of continuous FOUs), or randomly chosen samples for the LMFs and UMFs
of discrete FOUs, or FOUs for which mathematical formulas are provided for their
LMF and UMF.
Fortunately, [33] provides statistically meaningful comparisons for two kinds of
FOUs: (1) randomly chosen samples for the LMFs and UMFs of discrete FOUs,
and (2) evolutionary fuzzy logic controller design. If one is interested in which of
the algorithms are the fastest or most accurate, then [33] is the paper to go to.
3
Critique of Type-Reduction as an End to Itself
Research on type-reduction as an end to itself, as summarized in Table 1, uses the
(oft unstated) assumption that one begins with an IT2 FS to perform TR on. It then
focuses on how to improve (or approximate) the KM Algorithms to perform type-
reduction on that given IT2 FS. One or both of two performance measures are used
to evaluate improvements: computing time and accuracy. All of this research
(except for [30, 37]) is done outside of the original context of a T2 FLS, and is
therefore open to some critical examinations.
For starters, the assumption that one begins with an IT2 FS to perform TR on
needs to be questioned. I have explained in [24] that there are two distinctly
different situations that can occur in an IT2 FLS:
1. Fired-rule IT2 FSs are ﬁrst aggregated by means of the union operation resulting
in an aggregated IT2 FS (so that the assumption is valid), after which this
aggregated IT2 FS is type-reduced (this is called centroid type-reduction); and,
2. Fired-rule IT2 FSs are aggregated as part of type-reduction (this is called center-
of sets type-reduction), in which case one does not begin with an aggregated IT2
FS, and so the assumption is invalid.
Obviously the people who are researching type-reduction as an end to itself are
focusing on Situation 1; however, in real-world applications of FLSs it is Situation
2 that is more often used than Situation 1, because (this is also true for T1 FLSs)
performing the aggregation of the ﬁred-rule IT2 FSs is too time consuming. Con-
sequently, research on type-reduction as an end to itself is about things that
arguably will be of very little direct value or use in a T2 FLS.
On Type-Reduction Versus Direct Defuzziﬁcation for Type-2 …
393

One is then led to question whether or not the two metrics that are used in this kind
of research for Situation 1 are important, namely computing time and accuracy.
Consider ﬁrst the metric of computing time. If the results of this research are not
going to be used in a real-time FLS then does it really matter if the algorithms that
perform type-reduction take 10−3, 10−4, 10−5, etc. sec? KM or EKM [34] algo-
rithms are already quite fast; they converge quadratically [19] and have been
observed to take 10−5 to 10−3 s to converge (see, e.g., [34]). The EIASC algorithms
[35] are even faster and they are very easy to understand. So, focusing on con-
vergence time for Situation 1 is to me arguably a red herring because it makes no
perceptual difference to a human when it is already quite small.
Consider next the metric of accuracy. To me an important question is: How
much accuracy is required by the type-reduced set? Unfortunately, this question is
never asked in the papers that use the metric of accuracy. Instead, one is left with an
impression that higher accuracy is always better. Unless one knows what the type-
reduced set will be used for then, I would like to ask: “What is higher accuracy
better for?” To answer this question there are again two different situations/cases
that need to be examined:
• The LMF and UMF are given by mathematical formulas and so they can be
sampled at any desired rate (C1), and
• The LMF and UMF are given only by a collection of samples (C2).
Consider C1, in which the LMF and UMF are given by mathematical formulas,
and so they can be sampled at any desired rate. Then KM algorithms will give very
accurate results. To counter this some authors perform studies in which they reduce
the sampling rate to see how more accurate their algorithms are than the KM
algorithms. The Catch-22 to this work is that they are comparing the results from
their algorithms and the KM algorithms both of which use the reduced sampled data
with so-called true results, where the latter use the highly sampled data. To me this
is circular reasoning, because if one has access to highly sampled data and is
performing the type-reduction computations off-line then why not use all of the
highly sampled data to perform the type-reduction?
Consider next C2 in which the LMF and UMF are given only by a collection of
samples, as would occur if perchance ﬁred rule output sets were aggregated by using
the union (although, as explained above, this is usually avoided). The reason that the
resulting aggregated IT2 FS is given only by a collection of samples is that it has been
computed using an algorithm for the union that only uses sampled values. So, what
exactly does accuracy mean in this situation? In this case the KM algorithms com-
pute the true values of the type-reduced set and so they are 100 % accurate. To get
around this fact, the authors pretend that they have access to the LMF and UMF
given by mathematical formulas and have created a set of sampled values for the
LMF and the UMF from those formulas. This throws us back to the previous case,
but now that case is reached by violating the assumption that the LMF and UMF are
given only by a collection of samples. This again is circular reasoning.
Regrettably, the conclusion I am led to about research on type-reduction as an
end to itself is that it has led to results that are arguably of very little value.
394
J.M. Mendel

4
Critique of Direct Defuzziﬁcation
All research on direct defuzziﬁcation should be applauded as long as the direct
defuzziﬁcation method obeys the already-mentioned fundamental design require-
ment for a T2 FLS, namely: When all sources of [membership function] uncertainty
disappear, a T2 FLS must reduce to a comparable T1 FLS. It should be applauded
because its researchers have had the courage and conviction to challenge the need
for type-reduction in an IT2 FLS. To me, this is how real progress occurs. These
authors (although they may not have actually said it this way or at all) asked: “Why
is type reduction needed in an IT2 FLS? Why can’t we go directly to a defuzziﬁed
output?”
As is demonstrated by the large number of papers in Table 2, there are many
ways to go directly to a defuzziﬁed output; however, many of the authors of direct
defuzziﬁcation papers compare their numerical results with the ones obtained from
using type-reduction followed by defuzziﬁcation, as though the latter was a baseline
approach. So another natural question to ask is:
• Should an IT2 FLS that uses type-reduction followed by defuzziﬁcation be the
baseline IT2 FLS against which all others must be compared?
My answer to this question is “No,” and my reason is that maybe it should be the
other way around, i.e. maybe an IT2 FLS that uses type-reduction followed by
defuzziﬁcation should be compared against an IT2 FLS that uses direct defuzziﬁ-
cation. This may sound like double talk or the chicken before the egg parable, but I
am quite serious about my answer. Let me explain.
Real-world FLSs are designed with application-dependent performance speci-
ﬁcations in mind. Unfortunately, authors (myself included) do not usually state
what those specs are. Instead they assume that the goal is to optimize the perfor-
mance metrics rather than meet those metrics, something that is not done in the
business world.
Many years ago I attended a one-day seminar on entrepreneurship given by
world-famous Peter Drucker. It just so happened that we were sitting at the same
table for lunch. Everyone around the table introduced themselves. When it came to
my turn and I told everyone that I was an engineer, Drucker shook his head. I asked
him why he was doing that, to which he replied:
You engineers are always trying to optimize something, which is why you make such poor
businessmen. A business person develops a product with a certain level of performance as
quickly as possible, manufactures it, sells it and makes a proﬁt. He or she then improves the
product a bit and sells the next version, making even more proﬁt. In the meantime, you
engineers have not gotten off of the block; you are still trying to optimize your product, and
have nothing to show for it.
So if an application’s performance metrics can be met by an IT2 FLS that uses
direct defuzziﬁcation, that should be the end of the story. If, however, those per-
formance metrics cannot be met by such an IT2 FLS then one could try either a
different IT2 FLS that uses direct defuzziﬁcation, or, perhaps as a last resort, an IT2
On Type-Reduction Versus Direct Defuzziﬁcation for Type-2 …
395

FLS that uses type reduction followed by defuzziﬁcation. There is no a priori
guarantee, however, that even the latter will be able to meet the performance
metrics. If it cannot, then one may need to use a GT2 FLS.
5
Conclusions
Ever since Karnik and Mendel introduced the concept of type-reduction, which
leads to solving two optimization problems, one for the left-end and one for the
right end of the type-reduced set, a cottage industry has emerged that has focused
on better ways to perform TR or to bypass it entirely. This paper has tried to provide
a critical examination of the research that has been performed on these two topics.
It has explained that:
1. A good way to categorize type-reduction/direct defuzziﬁcation algorithm papers
is, as: papers that focus on algorithms that lead to
a. A type-reduced set, or
b. Directly to a defuzziﬁed value.
2. Research on type-reduction as an end to itself has led to results that are arguably
of very little value.
3. The practice of base-lining an IT2 FLS that uses direct defuzziﬁcation against
one that uses type-reduction followed by defuzziﬁcation is unnecessary.
Note that if Karnik and Mendel had never introduced type-reduction followed by
defuzziﬁcation, and instead had achieved their design requirement by using direct
defuzziﬁcation, then we would not be having this conversation, and people would be
comparing results obtained by using one kind of direct defuzziﬁcation method against
those obtained by using at least one other kind of direct defuzziﬁcation method.
As a ﬁnal thought, note that type reduction was invented because Karnik and
Mendel thought that the type-reduced set would itself be of value, in that it would
provide a valuable measure of the ﬂow of MF uncertainties through the FLS.
Although it does do this, regrettably, to the best of knowledge of this author, there
is not one application paper that makes use of the type-reduced set in this way. On
the other hand, type-reduction did lead to the KM Algorithms (as well as others)
which are very useful for solving other non-FLS problems, as is explained in [24].
References
1. Biglarbegian, M., Melek, W.W., Mendel, J.M.: On the stability of interval type-2 TSK fuzzy
logic control systems. IEEE Trans. Syst. Man Cybern. Part B: Cybern. 40, 798–818 (2010)
2. Biglarbegian, M., Melek, W.W., Mendel, J.M.: On the robustness of type-1 and interval type-2
fuzzy logic systems in modeling. Inf. Sci. 181, 1325–1347 (2011)
396
J.M. Mendel

3. Coupland, S., John, R.I.: Geometric type-1 and type-2 fuzzy logic systems. IEEE Trans. Fuzzy
Syst. 15(1), 3–15 (2007)
4. Du, X., Ying, H.: Derivation and analysis of the analytical structures of the interval type-2
fuzzy-PI and PD controllers. IEEE Trans. Fuzzy Syst. 18(4), 802–814 (2010)
5. Duran, K., Bernal, H., Melgarejo, M.: Improved iterative algorithm for computing the
generalized centroid of an interval type-2 fuzzy set. In: Proceedings of NAFIPS Conference,
New York, Paper 50056 (2008)
6. Dziech, A., Gorzalczany, M.B.: Decision making in signal transmission problems with
interval-valued fuzzy sets. Fuzzy Sets Syst. 23, 191–203 (1987)
7. Gorzalczany, M.B.: Interval-valued fuzzy controller based on verbal model of object. Fuzzy
Sets Syst. 28, 45–53 (1988)
8. Greenﬁeld, S., Chiclana, F.: Type-reduction of the discretised interval type-2 fuzzy set: what
happens as discretisation becomes ﬁner? In: Proceedings of IEEE Symposium on Advances in
Type-2 Fuzzy Logic System, Paris, pp. 102–109,(2011)
9. Greenﬁeld, S., Chiclana, F., Coupland, S., John, R.I.: The collapsing method for
defuzziﬁcation of discretized interval type-2 fuzzy sets. Inf. Sci. 179, 2055–2069 (2009)
10. Greenﬁeld, S., Chiclana, F., John, R.I.: The collapsing method: does the direction of collapse
affect accuracy? In: Proceeding of IFSA-EUSFLAT, Lisbon, pp. 980–985 (2009)
11. Greenﬁeld, S., Chiclana, F., John, R.I.: Type-reduction of the discretised interval type-2 fuzzy
set. In: Proceedings of IEEE FUZZ Conference, JeJu Island, pp. 738–743 (2009)
12. Hu, H., Wang, Y., Cai, Y.: Advantages of the enhanced opposite direction searching algorithm
for computing the centroid of an interval type-2 fuzzy set. Asian J. Control 14(6), 1–9 (2012)
13. Hu, H., Zhao, G., Yang, H.N.: Fast algorithm to calculate generalized centroid of interval type-
2 fuzzy set. Control Decis. 25(4), 637–640 (2010)
14. Karnik, N., Mendel, J.M.: Centroid of a type-2 fuzzy set. Inf. Sci. 132, 195–220 (2001)
15. Khosravi, A., Nahavandi, S., Khosravi, R.: A new neural network-based type reduction
algorithm for interval type-2 fuzzy logic systems. In: Proceeding of IEEE FUZZ Conf.,
Hyderabad, Paper 1116, (2013)
16. Khosravi, A., Nahavandi, S.: Load forecasting using interval type-2 fuzzy logic systems:
optimal type-reduction. IEEE Trans.Ind. Inf. 10(2), 1055–1063 (2014)
17. Li, C., Yi, J., Zhao, D.: A novel type-reduction method for interval type-2 fuzzy logic systems.
In: Proceedings 5th International Conference Fuzzy Systems Knowledge Discovery, Jinan,
pp. 157–161 (2008)
18. Liang, Q., Mendel, J.M.: Equalization of nonlinear time-varying channels using type-2 fuzzy
adaptive ﬁlters. IEEE Trans. Fuzzy Syst. 8(5), 551–563 (2000)
19. Liu, X., Mendel, J.M.: Connect Karnik-Mendel algorithms to root-ﬁnding for computing the
centroid of an interval type-2 fuzzy set. IEEE Trans. Fuzzy Syst. 19(4), 652–665 (2011)
20. Liu, X., Mendel, J.M., Wu, D.: Study on enhanced Karnik-Mendel algorithms: initialization
explanations and computation improvements. Inf. Sci. 187, 75–91 (2012)
21. Liu, X., Qin, Y., Wu, L.: Fast and direct Karnik-Mendel algorithm computation for the
centroid of an interval type-2 fuzzy set. In: Proceedings IEEE FUZZ Conference, Brisbane,
pp. 1058–1065 (2012)
22. Melgarejo, M.C.A.: A fast recursive method to compute the generalized centroid of an interval
type-2 fuzzy set. In: Proceedings of NAFIPS Conference, San Diego, pp. 190–194 (2007)
23. Mendel, J.M.: Uncertain Rule-Based Fuzzy Logic Systems: Introduction and New Directions.
Prentice-Hall, Upper Saddle River (2001)
24. Mendel, J.M.: On KM algorithms for solving type-2 fuzzy set problems. IEEE Trans. Fuzzy
Syst. 21(3), 426–446 (2013)
25. Mendel, J.M.: Type-2 fuzzy sets and beyond. In: Seising, R., Trillas, E., Moraga, C., Termini, S.
(eds.) On Fuzziness: A Homage to LotﬁA. Zadeh, vol. 2. Ch. 34. Springer (2013)
26. Mendel, J.M., Hagras, H., Tan, W.-W., Melek, W.M., Ying, H.: Introduction to Type-2 Fuzzy
Logic Control. IEEE Press and Wiley, Hoboken (2014)
On Type-Reduction Versus Direct Defuzziﬁcation for Type-2 …
397

27. Mendel,
J.M.,
Liu,
X.:
New
closed-form
solutions
for
Karnik-Mendel
algorithm
+defuzziﬁcation of an interval type-2 fuzzy set. In: Proceedings IEEE FUZZ Conference,
Brisbane, pp. 1610–1617 (2012)
28. Mendel, J.M., Liu, X.: Simpliﬁed interval type-2 fuzzy logic systems. IEEE Trans. Fuzzy Syst.
21(6), 1056–1069 (2013)
29. Nie, M., Tan, W.W.: Towards an efﬁcient type-reduction method for interval type-2 fuzzy
logic systems. In: Proceeding of IEEE FUZZ Conference, Hong Kong, Paper FS0339 (2008)
30. Niewiadomski, A., Ochelska, J., Szczepaniak, P.S.: Interval-valued linguistic summaries of
databases. Control Cybern. 35(2), 415–444 (2006)
31. Tau, C.W., Taur, J.S., Chang, C.-W., Chang, Y.-H.: Simpliﬁed type-2 fuzzy sliding controller
for wing rock system. Fuzzy Sets Syst. 207, 111–129 (2012)
32. Ulu, C., Guzelkaya, M., Eskin, I.: A closed form type-reduction method for piecewise linear
interval type-2 fuzzy sets. Int. J. Approx. Reason. 54(9), 1421–1433 (2013)
33. Wu, D.: Approaches for reducing the computational cost of interval type-2 fuzzy logic
systems: overview and comparisons. IEEE Trans. Fuzzy Syst. 21(1), 80–99 (2013)
34. Wu, D., Mendel, J.M.: Enhanced Karnik-Mendel algorithms. IEEE Trans. Fuzzy Syst. 17(4),
923–934 (2009)
35. Wu, D. and Nie, M.: Comparison and practical implementations of type-reduction algorithms
for type-2 fuzzy sets and systems. In: Proceedings of IEEE FUZZ Conference, Taipei,
pp. 2131–2138 (2011)
36. Wu, D., Tan, W.W.: Computationally efﬁcient type-reduction strategies for a type-2 fuzzy
logic controller. In: Proceedings of IEEE FUZZ Conference, Reno, pp. 353–358, (2005)
37. Wu, H., Mendel, J.M.: Uncertainty bounds and their use in the design of interval type-2 fuzzy
logic systems. IEEE Trans. Fuzzy Syst. 10(5), 622–639 (2002)
Author Biography
Jerry M. Mendel was born in New York City and received the
Ph.D. degree in electrical engineering from the Polytechnic
Institute of Brooklyn, Brooklyn, NY. Currently he is Professor
of Electrical Engineering at the University of Southern Califor-
nia in Los Angeles, where he has been since 1974.
He has published over 550 technical papers and is author and/or
co-author of 11 books, including Uncertain Rule-based Fuzzy
Logic Systems: Introduction and New Directions (Prentice-Hall,
2001), Perceptual Computing: Aiding People in Making Sub-
jective Judgments (Wiley & IEEE Press, 2010), and Introduction
to Type-2 Fuzzy Logic Control: Theory and Application (Wiley &
IEEE Press, 2014). His present research interests include: type-2
fuzzy logic systems and their applications to a wide range of
problems, including smart oil ﬁeld technology, computing with
words, and fuzzy set qualitative comparative analysis.
398
J.M. Mendel

He is a Life Fellow of the IEEE, a Distinguished Member of the IEEE Control Systems Society, and
a Fellow of the International Fuzzy Systems Association. He was President of the IEEE Control
Systems Society in 1986. He was a member of the Administrative Committee of the IEEE
Computational Intelligence Society for nine years, and was Chairman of its Fuzzy Systems Technical
Committee and the Computing With Words Task Force of that TC. Among his awards are the 1983
Best Transactions Paper Award of the IEEE Geoscience and Remote Sensing Society, the 1992
Signal Processing Society Paper Award, the 2002 and 2014 Transactions on Fuzzy Systems
Outstanding Paper Awards, a 1984 IEEE Centennial Medal, an IEEE Third Millenium Medal, and a
Fuzzy Systems Pioneer Award (2008) from the IEEE Computational Intelligence Society.
On Type-Reduction Versus Direct Defuzziﬁcation for Type-2 …
399

On Fuzzy Theory for Econometrics
Hung T. Nguyen and Songsak Sriboonchitta
Abstract This paper aims mainly at informing statisticians and econometricians of
relevant concepts and methods in fuzzy theory that are useful in addressing economic
problems. We emphasize three recent signiﬁcant contributions of fuzzy theory to
economics, namely fuzzy games for capital risk allocations, fuzzy rule bases and
compositional rule of inference for causal inference, and a statistical setting for fuzzy
data based on continuous lattices.
1 Introduction
LotﬁZadeh advocated fuzzy sets as mathematical modeling of fuzzy concepts in nat-
ural language in 1965 [1]. Ever since, while fuzzy theory found signiﬁcant applica-
tions in engineering and technology ﬁelds from its own concepts and methods, it was
Zadeh himself who emphasized that fuzziness, as a distinct concept of uncertainty,
should be a complement to randomness, i.e., when facing uncertainty in real-world
complex systems, we should handle both fuzziness and randomness together. This
paper is about a concrete and important situation where random fuzzy sets appear
naturally and should be studied simultaneously.
This paper is organized as follows. In Sect. 2, biased by our own experience, we
recall two anecdotes concerning fuzzy theory, as a “contribution” to the celebration
of 50 years of fuzzy sets. The rest of the paper is devoted to elaborating on some sig-
niﬁcant contributions of fuzzy theory to a speciﬁc area of social science, namely eco-
nomics, again biased by our own current research interests. In Sect. 3, within the nice
Dedicated to LotﬁZadeh.
H.T. Nguyen (✉)
Department of Mathematical Sciences, New Mexiso State University, Las Cruces, USA
e-mail: hunguyen@nmsu.edu
S. Sriboonchitta
Faculty of Economics, Chiang Mai University, Chiang Mai, Thailand
e-mail: songsakecon@gmail.com
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_21
401

402
H.T. Nguyen and S. Sriboonchitta
connection of Von Neumann’s view of economics and [2], we bring out the neces-
sity to take fuzzy games into account in order to investigate coherent risk allocations
in ﬁnancial risk management. In Sect. 4, we illustrate the appearance of “observed”
time series of fuzzy data in econometric regression with seemingly unobservable
variables (SUV), as well as discussing the necessity of using fuzzy technology in
both statistical and econometric causal inference. To put fuzzy data on a ﬁrm basis
for statistical treatment, we outline, in Sect. 5, our recent development of random
fuzzy sets using topologies from continuous lattices.
2 Two Anecdotes of Fuzzy Theory
Professor Dennis Lindley gave a thought-provoking lecture in Zadeh’s seminar at the
University of California, Berkeley, in 1981, in which he showed that, in the frame-
work of scoring rules, fuzzy sets, belief functions and even conﬁdence intervals are
all inadmissible measures of uncertainty. His message is that “you cannot avoid prob-
ability”. His lecture was published in 1982 (see [3]). In a followed-up debate orga-
nized by a statistical journal, all invited discussants felt strongly that Lindley was
wrong, but Lindley replied something like “everybody said that I am wrong, but
nobody was able to show me where I am wrong, mathematically”.
I. R. Goodman and H. T. Nguyen spent a year or so, starring at Lindley’s paper.
And, ﬁnally, they saw what was wrong with Lindley’s result. There is nothing wrong
with Lindley’s result, only his implications! His result is this. If an uncertainty mea-
sure is admissible, then it must be a function of a probability measure. But then,
that uncertainty measure need not be additive, such as the square of a probability
measure which is precisely a belief function. So, the matter is clear and settled! We
can go on to use fuzziness without contradicting Lindley’s message. Our ﬁndings
were published in 1991 (see [4]). It is interesting to note that the philosophical side
of the above issue seems to lie between (semantic) information and probability. In
1982, A. N. Kolmogorov said “Information theory must precede probability and not
be based on it” (See [5]). This is a quote in the book Information Theory and The
Central Limit Theorem by [6].
The second anecdote is even more “threatening”! C. Elkan presented a paper enti-
tled “The paradoxical success of fuzzy logic” at a Conference on Artiﬁcial Intelli-
gence in 1993 in which he said that all successes attributed to fuzzy logic are due to
something else, and in fact, fuzzy logic does not exist, using a short mathematical
theorem showing that, from his set of axioms for fuzzy logic, fuzzy sets just collapse
to ordinary (crisp) sets! See [7]. Again, a journal debate was organized with a famil-
iar outcome: discussants “argued” that Elkan was wrong, but nobody was able to
point out where he was wrong mathematically! It was Vladik Kreinovich who found
a way to get around Elkan’s theorem by examining logics of experts (See [8]). It
was a good step but not quite an answer to Elkan’s result. Upon seeing that paper by
Nguyen et al., three logicians at New Mexico State University (Las Cruces, USA),

On Fuzzy Theory for Econometrics
403
Mai Gehrke, Carol and Elbert Walker, quickly saw the ﬂaw in Elkan’s theorem. It is
his own system of axioms which was wrong. Their ﬁndings were published in 1997
(See [9]). So, again, we can go on!
3 Fuzzy Games for Financial Risk Management
This section is devoted to the explanation of a signiﬁcant contribution of fuzzy set
theory to economics, in the sense that its generalization of crisp sets is necessary
to solve important problems, and not just a generalization per se. In a sense, this is
somewhat similar to the invention of complex numbers!
Extensions of concepts are familiar phenomena in mathematics. In the applied sci-
ences, mathematical generalizations of concepts are immediately appreciated when
not only they are motivated by concrete needs in applications, but also since they
help to solve problems that otherwise we cannot come up with solutions. The striking
example is the generalization of (pure) strategies in non-cooperative games to mixed
strategies, leading to the existence of Nash equilibria in behavioral economics. Now,
as we will see, Von Neumann’s cooperative (coalitional) games need to be general-
ized to solve a problem in econometrics. It is interesting to mention that R. J. Aumann
and L. S. Shapley (1974), in their famous book Values of Non-Atomic Games, needed
to generalize ordinary sets to obtain what they called “evenly spread sets” to formu-
late their notion of values for games with large masses of players. As they put down
in a footnote [10], their “ideal sets” are formally Zadeh’s fuzzy sets.
Recent literature on the capital risk allocation (CRA) problem revealed a nice
marriage between two seemingly “disjoint” ingredients in studying economics,
namely von Neumann’s game theory, and Haavelmo’s econometrics. The basic ref-
erence is [11] which triggered current intensive research on fuzzy games for practical
applications.
As its name indicates, the CRA problem consists of ﬁnding (“fair”) ways to allo-
cate capital risk of a ﬁrm to its (business) units. Suppose there are n units in a
ﬁrm, each unit has a risk of Xi (a random variable). The risk Xi is quantiﬁed by
using a risk measure r(.). The “cost” of a group of units A ⊆I = {1, 2, ..., n}, is
c(A) = r(∑
i∈A Xi). To capture diversiﬁcation eﬀect, the risk measure r(.) should be
chosen to be “coherent”, i.e., subadditive, such as the Tail Value-At-Risk. For such a
risk measure, the set function v ∶2I →ℝ, deﬁned by v(A) = ∑
i∈A c({i}) −c(A), is
super additive, so that, formally, (I, v) is a coalitional (cooperative) game, in which
units play the role of “players”, subsets of I are coalitions, and v(.) gives the “pay-
oﬀs” of coalitions. Thus, the CRA problem can be reformulated as a coalitional game
which is more “convenient”, in the sense that game solution concepts will be used to
derive allocation principles for CRA problem. Speciﬁcally, a cost allocation is a vec-
tor (x1, x2, ..., xn) ∈ℝn
+ such that ∑n
i=1 xi = c(I) which is obtained from an allocation
(y1, y2, ..., yn) ∈ℝn
+ of the game (I, v) by taking xi = c({i}) −yi.
Speciﬁcally, the Shapley value S of (I, v) is a “coherent” allocation (i.e., a solution
for the CRA problem) when it belongs to the core C (v) of the game (which is non

404
H.T. Nguyen and S. Sriboonchitta
empty here by the choice of a coherent risk measure). Note that, if a game (I, v) can
be modeled arbitrarily, e.g., as a convex game, then it is well-known that not only the
core is non empty, and equals to the unique stable set, but also contains the Shap-
ley value. The problem here is that, we cannot choose the associated characteristic
function v(.) arbitrarily, since it is deﬁned in terms of a (coherent) risk measure r(.),
via the cost function c(.), namely
v(A) =
∑
i∈A
c({i}) −c(A) =
∑
i∈A
r(Xi) −r(
∑
j∈A
Xj)
If (I, v) is a convex game, then necessarily the risk measure r(.) is linear, which is
not acceptable since such a risk measure eliminates the diversiﬁcation eﬀect. Thus,
we are facing a non convex game (I, v), i.e., lacking all the nice properties of convex
games, in particular, a coherent allocation principle for the original CRA problem.
Hence, the question is: for a non convex game, but with non empty core, when the
Shapley value belongs to the core? As [11] put it “We thus fall short of a convincing
proof of the existence of coherent allocations”. The “way out” is fuzzy games!
So let’s us just spell out what are fuzzy games, how they appear, and how they
provide solutions to CRA problem.
In standard coalitional games (I, v), a coalition is a subset A of the set I =
{1, 2, ..., n} of players. It was Aubin (see e.g., [12]) who made the following real-
istic observation. When joining a coalition, players might not necessarily commit
their full resources (e.g., money, time) to it. They could choose their levels of par-
ticipation to a coalition, and not just whether they decide to participate or not. As
such, degrees of participation of players should be taken into account when consid-
ering coalitions (e.g., here, for a “fair” capital risk allocation). A coalition A when
all of its members committing fully is just an ordinary coalition, i.e., a crisp subset
of I, whereas, a “partial” coalition can only be characterized by degrees of partici-
pation of players, i.e., by a function f ∶I →[0, 1] with f(i) represents the degree of
participation of player i in the coalition. Thus, a partial coalition is not a set unless
f ∶I →{0, 1}, but a kind of a “generalized set”, a fuzzy set. It should be noted that
f(.) < 1 is possible: it corresponds to a coalition where no player wants to commit
fully to it.
Remark. Using indicator functions of sets, we identify the power set of I with
the subset {0, 1}n of ℝn, denoted as 2I. The convex hull of {0, 1}n is [0, 1]n which
is identiﬁed as the set of functions I →[0, 1]. Thus, each “membership function”
f ∶I →[0, 1] is written as f(.) = ∑
A⊆I 𝛽(A)1A(.) with 𝛽(.) ≥0 and ∑
A⊆I 𝛽(A) = 1.
As a result, f(i) = ∑
i∈A 𝛽(A) which can be interpreted as the (one-point) coverage
function of the random set S ∶(𝛺, A , P) →2I, with P(S = A) = 𝛽(A), i.e., f(i) =
P(i ∈S). It is important to note that this interpretation is by no means an indication
that fuzziness is subsumed by probability! First, the above connection exhibits only
a weak form of randomness. Membership functions (deﬁning fuzzy sets) are not
probability distributions. Fuzziness is a matter of degree. Let’s elaborate on these
two points. The function A(.) ∶ℝ+ →[0, 1] given by

On Fuzzy Theory for Econometrics
405
A(u) =
⎧
⎪
⎨
⎪⎩
0 if u < 20
u−20
55
if 20 ≤u ≤75
1 if u > 75
is a membership function for the fuzzy concept “high income” (where, e.g., 20 means
$20, 000, annually). The value 30
55 ∈[0, 1] is the degree to which the income of 50
is compatible with the meaning of “high income”. It is not the probability that an
income of 50 is high! Membership functions on general domains (not necessarily
ﬁnite domains) exhibit a weak form of randomness as follows. Let f ∶U →[0, 1],
then there exists a random set (see Sect. 5) S ∶(𝛺, A , P) →2U, such that, for each
u ∈U, f(u) = P(u ∈S). Indeed, it suﬃces to consider a random variable 𝛼∶
(𝛺, A , P) →[0, 1], uniformly distributed, and take S(𝜔) = {u ∈U ∶f(u) ≥𝛼(𝜔)}.
The adjective “weak” refers to the knowledge of the coverage function u →P(u ∈S)
of a random set (as in the theory of sampling in ﬁnite populations) which is weaker
than the knowledge of the “distribution” of the random set. This is illustrated by a
previous work of Robbins (1944). Historically, while random sets appeared natu-
rally in many places, such as stochastic geometry, its formal theory was not rigor-
ously established until 1975 (by [13]). When estimating the “size” (area, volume) of
a random set, [14] did not really consider a formal concept of a random set. This so
since the size of a random set 𝜇(S) (where 𝜇the Lebesgue measure on ℝd) is in fact
a numerical random variable, although it depends on the random set S. Without a
formal concept of random sets, it is not possible to ﬁnd the distribution of the non-
negative random variable 𝜇(S) which is a function of S. The clever result of Robbins
is this. As far as the expected value of 𝜇(S) is concerned, we need much less than
the distribution of 𝜇(S). Speciﬁcally, the knowledge on the coverage function of the
informal random set S is suﬃcient to determine E𝜇(S), a “weaker” form of informa-
tion. The computation of the expected length of a random set of the form S = [0, X]
where X ≥0 is a random variable is simple: the length of S is X, so that
E𝜇(S) = EX = ∫
∞
0
P(X > x)dx = ∫
∞
0
𝜋(x)dx
where
𝜋(x) = P(x ∈S) = P(x ∈[0, X]) = P(X > x)
is the coverage function of the random set S. The Robbins’ formula says that the
above formula is in fact general: For any random set S on ℝd, we have
E𝜇(S) = ∫ℝd 𝜋(x)d𝜇(x)
where 𝜋(.) ∶ℝd →[0, 1] is the coverage function of S.
Coalitional games in which partial coalitions are considered are referred to as
fuzzy games. Just like fuzzy logic (which really means “a logic of fuzzy concepts”

406
H.T. Nguyen and S. Sriboonchitta
and not a logic which is fuzzy!), maybe we should rename fuzzy games by games
with fuzzy coalitions to be speciﬁc?
A fuzzy game is a pair (I, v∗) where I = {1, 2, ..., n} and the generalized “char-
acteristic function” (of coalitional games) is v∗∶[0, 1]n(fuzzy subsets of I)→ℝ,
with v∗(0) = 0, noting that a fuzzy coalition is characterized by a vector 𝜆=
(𝜆1, 𝜆2, ..., 𝜆n) ∈[0, 1]n with 𝜆i being the participation level of player i in that fuzzy
set. An allocation for a fuzzy game is a vector x ∈ℝn such that ∑n
i=1 xi = v∗(I) and,
for any fuzzy coalition 𝜆, ∑n
i=1 𝜆ixi ≥v∗(𝜆) (Noting that, the attainable outcome of
a fuzzy coalition depends obviously on the degrees of commitment of the players).
This is the generalization of the core of a standard coalitional game. However, while
the game is fuzzy, its “fuzzy core” is not.
Remark. The “cost function” of a fuzzy coalition 𝜆is deﬁned in terms of a risk
measure r(.) as c(𝜆) = r(∑n
i=1 𝜆iXi).
In the context of the CRA problem, the associated fuzzy game is this. Let 𝛬=
(𝛬1, 𝛬2, ..., 𝛬n) ∈ℝn
+ be the vector of “business volumes” of units in a ﬁrm. Suppose
each unit can commit some portion of its volume to a coalition. Let denote by 𝜆=
(𝜆1, 𝜆2, ..., 𝜆n) a “resource commitment” of the units to a coalition, where, of course,
0 ≤𝜆≤𝛬(componentwise). Then the degree of membership (level of participation)
of unit i in that coalition is 𝜆i
𝛬i ∈[0, 1]. The fuzzy coalition is determined by the
membership function f ∶I →[0, 1], given by f(i) = 𝜆i
𝛬i .
When taking into account of partial participation in coalitions, such as this impor-
tant situation in ﬁnancial risk management, we arrive as a natural concept of fuzzy
games which turn out to be a “good” example of Aumann and Shapley’s non-atomic
games [10]. Indeed, since each player (business unit) can choose to joint a coalition
by declaring a portion f(i) =
𝜆i
𝛬i ∈[0, 1] of its total business volume, the “real”
players in I = {1, 2, ..., n} become “players” in the inﬁnite set [0, 1]. In other words,
a fuzzy game is a game with inﬁnite number of players. Note that, in this view, coali-
tions of the game ([0, 1], w) are taken as Borel sets of [0, 1], i.e., w(.) ∶B1 →ℝ,
but noting also that ([0, 1], B1, w) is not a measure space since the set function w(.)
is not additive.
Without entering into more technical details, let’s just mention how the extension
to fuzzy games leads to solutions for the CRA problem. Extending basic concepts of
crisp coalitonal games to fuzzy games, such as core of fuzzy games, (Shapley) fuzzy
values, as well as associated coherent risk measures, the solution to the CRA can be
taken as an allocation (fuzzy value map) with values in the core (non fuzzy) of the
fuzzy game. It turns out that the Aumann-Shapley value (extending the usual Shapley
value) of non-atomic games is a coherent CRA solution, when the underlying risk
measure is chosen to be coherent (in an extended sense), and under mild and possible
conditions. See [11] for details.
The advantage of considering fuzzy games in the CRA problem is this. With
coherent risk measures, it is not clear whether the Shapley value is a coherent allo-
cation, but, the Aumann-Shapley value (when it exists) is a coherent allocation (i.e.,
solution of the CRA problem) in the context of fuzzy games.

On Fuzzy Theory for Econometrics
407
4 Fuzzy Methods for Econometric Causal Inference
We turn now to another signiﬁcant contribution of fuzzy theory to econometrics,
namely using fuzzy rule bases to investigating causal inference (speciﬁcally, in
counterfactual problem: how to use a knowledge of a causal structure to “reason” to
unobservable outcomes?), leading to econometric predictions with seemingly unob-
servable variables.
But ﬁrst, while fuzzy technology is well-known in engineering circle, it is not so
for econometric community. Here is a tutorial on the basis of fuzzy technology that
we will employ in this paper. According to [1], fuzzy concepts in a natural language,
such as “low” (temperature), “eﬃciency”, “happiness”, can be modeled mathemati-
cally for information processing purposes. As we will see, fuzzy concepts are usually
values of qualitative (linguistic) variables of interest in decision-making (e.g., not
disabled, partially disabled, fully disabled are “values”/outcomes of the linguistic
variable “disability status”). More importantly, fuzzy concepts are used as coars-
ening schemes in human intelligent behavior (e.g., in intelligent control). Note that
qualitative/latent models and regression are also in the practical statistical tool box.
Using a familiar procedure in mathematics, namely, as far as generalizations
are concerned, some equivalences of a concept are more suitable for the purpose
than others (e.g., an equivalent framework for deriving the Black-Scholes option
pricing formula from PDE is martingales, allowing extensions to other markets),
Zadeh deﬁned fuzzy sets (i.e., mathematical objects representing fuzzy concepts)
by extending the range of ordinary (crisp) set indicator functions (for a complete
theory of fuzzy sets and logics, see e.g., [15]). Let U be a set. A subset A ⊆U is
characterized by it indicator function A(.) ∶U →{0, 1} (note that we use the same
notation A for the set and its indicator function, the context will tell us clearly which
is which!), where A(u) = 1 or 0 according to u ∈A ∈or u ∉A. This equivalent way
to describe a set brings out the notion of membership of elements of U to subsets of
U: when A(u) = 1, the element u is a member of A, whereas when A(u) = 0, u is
not a member of A. Here, membership degrees are only 1 and 0 (there is no partial
membership). Extending the range {0, 1} to the whole unit interval [0, 1] lead to a
generalization of crisp sets to fuzzy sets. Speciﬁcally, a fuzzy subset of U is a func-
tion A(.) ∶U →[0, 1] where A(u) ∈[0, 1] is the membership of an elements u ∈U
in the underlying fuzzy concept.
Having fuzzy data modeled as fuzzy sets, we can proceed to manipulate/process
them by extending operations on the underlying set U. This is achieved by the well-
known extension principle. If 𝜑∶U × V →W and A, B are fuzzy subsets of U, V,
respectively, then 𝜑is extended to fuzzy subsets as
𝜑(A, B)(w) =
max
{(u,v)∶𝜑(u,v)=w}(A(u) ∧B(v))
where ∧denotes minimum (and ∨denotes maximum).
Note that, in one hand, since any function A(.) ∶U →[0, 1] can be recovered
from its level sets A𝛼= {u ∈U ∶A(u) ≥𝛼}, 𝛼∈[0, 1] by A(u) = ∫1
0 A𝛼(u)d𝛼,

408
H.T. Nguyen and S. Sriboonchitta
and on the other hand, manipulations with level sets are simpler, the following well-
known result, known in the literature as Nguyen’s Theorem (See, [16–18]) is useful.
A necessary and suﬃcient condition for
𝜑(A(1)
𝛼, A(2)
𝛼, ..., A(n)
𝛼) = [𝜑(A(1), A(2), ..., A(n))]𝛼
where 𝜑∶U1 × U2 × ... × Un →V, and A(i) is a fuzzy subset on Ui, i = 1, 2, ..., n is
that for each v ∈V,
max
{(u1,u2,...,un)∈𝜑−1(v)}[∧n
i=1A(i)(ui)]
is attained.
While fuzzy logic connectives, including “implication operator” ⟹(represent-
ing “If... Then...” rules in fuzzy technology), are familiar with engineers, they appear
as tools to suggest (nonlinear) models for statistics.
This can be seen as follows. A statistical model, such as a linear regression model
Yi = 𝜃Xi + 𝜀i, i = 1, 2, ..., n, is in fact a collection of “If...Then...” rules, since what
they mean is that, for each i, the model reads “If X is Xi (and 𝜀is 𝜀i), then Y is Yi”
(where “is” stands for “equal”). This observation allows an extension to fuzzy data:
when (Xi, Yi), i = 1, 2, ..., n are fuzzy data (linguistic labels), the “rules” become Ri ∶
“If X is Xi, then Y is Yi” or “Xi ⟹Yi” where the connective “If...Then...” is the
implication ⟹in fuzzy logic, which is a fuzzy relation on the Cartesian product,
say, U × V, i.e. f⟹(u, v) is the degree to which u “implies” v. In the simplest fuzzy
logic system, f⟹(u, v) in “Xi ⟹Yi” can be taken as Xi(u) ∧Yi(v).
In the statistical linear regression, the goal is to arrive at a “prediction formula”
from, say, given numerical data (Xi, Yi), i = 1, 2, ..., n. This is achieved by combining
the “rules” Yi = 𝜃Xi + 𝜀i by using some method of estimation (e.g., least squares,
when random variables have ﬁnite variances) to estimate the parameter 𝜃to arrive
as Y = ̂𝜃nX. A counterpart of such a procedure when the data (Xi, Yi), i = 1, 2, ..., n
are fuzzy is combining the rules “Xi ⟹Yi” by the compositional rule of inference
∨n
i=1[Xi(u)∧Yi(v)] to obtain the combined membership function, where ∨n
i=1[Xi(u)∧
Yi(v)] is the degree to which u implies v given the rule base. Given a value u, the
implied consequence is a fuzzy subset of V given by v →∨n
i=1[Xi(u) ∧Yi(v)]. The
important point is this. Fuzzy rule bases play the role of statistical models in the
presence of fuzzy data. We will elaborate on this important ingredient in the problem
of “estimating” unobservables using nonparametric causal “structures”.
Remark. With respect to the “conditional” implication “A ⟹B” and its degree
of compatibility, used in the previous context, it is of course tempting to ask whether
they can be given a probabilistic ﬂavor? For example, for crisp events A, B on some
probability space (𝛺, A , P), can we view the degree of “A ⟹B” as P(B|A)?
If A ⟹B = Ac ∪B (material implication), then
P(A ⟹B) = P(Ac ∪B) = P(Ac ∪(A ∩B))
= P(Ac) + P(A ∩B) = P(Ac) + P(B|A)P(A) = P(Ac) + P(B|A)[1 −P(Ac)]

On Fuzzy Theory for Econometrics
409
= P(B|A) + P(Ac)[1 −P(B|A)] = P(B|A) + P(Ac)P(Bc|A) ≥P(B|A)
with equality holding if and only if P(Bc∩A) = 0 or P(A) = 1, a rather trivial case.
It is known that there is no operation ▽on the 𝜎−ﬁeld A such that P(A▽B) =
P(B|A) (see Goodman, Nguyen and Walker [19]). Thus, the answer is no, even for
crisp events let alone for fuzzy events (a fuzzy event is a fuzzy subset of 𝛺whose
membership function is measurable). Therefore, if we insist on P(A|B) as the degree
for A ⟹B to be true, we have to represent mathematically the rule A ⟹B
diﬀerently. Since A ⟹B ∉A , it could be an object lying outside of A for the
equation P(A ⟹B) = P(B|A) to hold. This is similar to complex numbers. For the
solution to this problem, see [19]. See also [20–23].
Fuzzy rules and their fusion are very important for reasoning in intelligent sys-
tems. Each rule reﬂects common sense knowledge, and will be used (see later) to
provide models for knowledge acquisition. Just like the case of default reasoning
in computer science, rules could have exceptions (for reasoning with such rules,
see [24]).
We turn now to the question: how can fuzzy technology help statistics? In a sense,
we will point out some signiﬁcant contributions of fuzzy theory, outside the engi-
neering ﬁelds, to statistics in particular, and to decision theory in general.
Regression analysis is the main tool of statistics for investigating relationships
between economic variables. As statistical theory, based upon probability theory,
seems to leave no stone unturned in its path, it addresses also the important situa-
tions where variables (response variables, regressors and covariates) could be latent
or qualitative. However, in this context, there is one stone unturned. It is the case
where regressors are seemingly unobservable and need to be “estimated” to run the
regression. A typical situation is the study of the eﬀect of underground economy
(u.e.) on national economy, recently investigated by [25], and [26]. Let Y denote
the GDP of a country and X denote the size of the u.e. of that country. A simple
linear regression model is Y = aX + b + 𝜀. While Y is observable (say, yearly),
X is not. It is not realistic to make further assumptions to proceed as in standard
practice! To run such a regression, we need to “create” a time series of X. How? It
is precisely here that fuzzy technology could help! But before elaborating on this
possibility, let’s pause and says few words about this interesting demand. Although
X is unobservable, we might be able to identify some main causes of it. Then we
need an ingredient to infer X, in some fashion, from these observable causes. This
sounds somewhat like we are in the context of causal inference? As emphasized by
[27]: “Causal inference requires two additional ingredients: a scientiﬁc language
for articulating causal knowledge, and a mathematical machinery for processing
that knowledge, combining it with data and drawing new causal conclusions about
the phenomenon”. While our problem here is not about causal inference, since we
assume that some causes of X are identiﬁed, and we proceed to “estimate” X from
them. However, fuzzy theory seems to provide the two additional ingredients men-
tioned by Pearl: the “scientiﬁc language for articulating knowledge” is fuzzy logic in
the form of fuzzy “if...then...” rules, and “a mathematical machinery for processing
knowledge” is the compositional rule of inference.

410
H.T. Nguyen and S. Sriboonchitta
Remark. When considering causality, we face two problems: ﬁnding the causes
of an “eﬀect”, and studying the eﬀects of causes of an eﬀect. See Holland [28] for
an excellent paper on causal inference from a statistical viewpoint. The above fuzzy
procedure oﬀers a more realistic way for assessing the eﬀects of given causes on
an “eﬀect” variable, even for the case where the eﬀect variable is not observable.
It does so by using (linguistic) coarsening schemes, resulting in assessing (rather
than “measuring”) causal eﬀects from a common sense knowledge, without impos-
ing untested statistical assumptions. This is a very important issue to explore for
future research and applications, where fuzzy technology could provide a realistic
alternative to statistical approach, somewhat in line with Pearl’s view. It should be
remembered that when we oﬀer an alternative (to existing approaches) we need to
explain why another alternative (e.g., what are its advantages?). In view of the state-
of-the-art of causal inference, an alternative to carry out causal inference by fuzzy
theory should be welcome and well-justiﬁed. Indeed, as [29] put it “I would advise
against regarding any one approach or blending as a complete solution or algorithm
for problems of causal inference; the area remains one rich with open problems and
opportunities for innovation”.
In the speciﬁc case of u.e., causes of u.e. could be “taxation”, “unemployment
rate” and “corruption index”. A typical fuzzy rule is of the form “If taxation is
high, unemployment rate is low, and corruption index is medium, then the size of
u.e. is medium”. Given a fuzzy rule base (consisting of a ﬁnite set of fuzzy rules),
the compositional rule of inference allows the derivation, from observed causes, the
“estimated” size of u.e., expressed as fuzzy sets. With such “fuzzy estimates” of the
regressor “size of u.e.”, we then face a linear regression model Y = aX + b + 𝜀with
fuzzy data (Xi, Yi), i = 1, 2, ..., n. Note that (statistical) regression with fuzzy data is
diﬀerent than conventional “fuzzy regression” in engineering literature, in which the
model is Y = aX + b with coeﬃcients a, b being fuzzy sets, X non fuzzy, resulting
in fuzzy output Y.
In order to carry out statistical regression with fuzzy data, we need to treat fuzzy
data as realizations of bona ﬁde random elements in probability theory. These will
be called random fuzzy sets, and will be discussed in the next section.
5 Incorporating Fuzzy Data into Statistics
In order to view fuzzy data as bona ﬁde statistical observations, it is necessary to
deﬁne the formal concept of a random element whose values are fuzzy sets. The
standard process consists of specifying a measurable space (U, U ) where U is the
set of “oucomes” of the random element X that we wish to deﬁne, and U is an
appropriate 𝜎−ﬁeld of subsets of U, representing events. Also, as usual, the 𝜎−ﬁeld
U is Borel, i.e., constructed from some topology on U. Thus, the main task is to
topologize U. For concreteness, we take U to be the set F of closed fuzzy subsets
of ℝd, i.e., fuzzy subsets whose membership functions f ∶ℝd →[0, 1], are upper
semi continuous, i.e., their level sets A𝛼(f) = {x ∈ℝd ∶f(x) ≥𝛼}, 𝛼∈[0, 1], are

On Fuzzy Theory for Econometrics
411
closed subsets of ℝd. It is not obvious how to come up with some metric in order to
topologize the space F. Fortunately, we are standing on the shoulders of giants (!):
the theory of continuous lattices [30] oﬀers the topology we need here. Note that,
continuous lattices, or “domains” are known also in computer science, see e.g., [31].
Topologies generated by (partial) order relations in general spaces are well-
known. What is interesting is that depending upon how we choose an order relation,
we can obtain an interesting topology. Now, fuzzy sets not only represent the seman-
tic meaning of fuzzy concepts, they are used also as “coarse data”, exhibiting a form
of localization information (just like crisp sets), in the sense that the fuzzy set A is
“more informative” than a fuzzy set B if A is contained in B, in symbol A ⊆B i.e.,
A(u) ≤B(u) for all u ∈U. Thus, an (partial) order relation on F could be taken
as A ≿B (A is “greater” than B) if A ⊆B. In other words, we take as an order
relation on F the reverse order of set inclusion. It is this order ≿which makes F
a continuous lattice. Indeed, ﬁrst, the poset (F, ≿) is a complete lattice. Moreover,
it is a continuous lattice, i.e., for every F ∈F, we have F = ∨{G ∈F ∶G ⋑F}
where the ﬁner relation ⋑(“much less informative than”) or “way below” is deﬁned
as follows. G ⋑F if for every collection D of elements of F for which F ≿∨D,
there is a A ∈D such that G ≿A.
On a continuous lattice (F, ⋟, ⋑), there is a canonical topology, called the Lawson
topology, which is the topology 𝜏with a subbase consisting of sets {G ∈F ∶G ⋑
F} and {G ∈F ∶G F} for all F ∈F, i.e., open sets are taken as arbitrary
unions of ﬁnite intersections of these sets. Note that, the Lawson topology on the set
of closed (crisp) subsets coincides with the Matheron (1975) hit-or-miss topology.
Let B(𝜏) be the Borel 𝜎−ﬁeld generated by the Lawson topology 𝜏on F. Let
(𝛺, A , P) be a probability space. Then by a random fuzzy (closed) set, we mean a
map X ∶𝛺→F, A −B(𝜏)−measurable, i.e., X−1[B(𝜏)] ⊆A .
Remark. Our approach here is to place fuzzy sets as values of random elements
within probability theory, as opposed to “the fuzzy approach to statistical analysis”,
see, e.g., [32].
In the special case of random (crisp) closed sets [13], the counterpart of Lebesgue-
Stieltjes theorem is the Choquet theorem characterizing probability measures by
capacity functionals. Speciﬁcally, let F, K denote the classes of closed and com-
pact subsets of ℝd, respectively, and deﬁne T ∶K →[0, 1] by
T(K) = P(F ∈F ∶F ∩K ≠∅}
then T satisﬁes the following axioms
(1) T(∅) = 0
(2) T is alternating of inﬁnite order, i.e., for any n ≥2 and K1, K2, ..., Kn in K ,
T(∩n
i=1Ki) ≤
∑
∅≠I⊆{1,2,...,n}
T(∪i∈IKi)
(3) If Kn ↘K in K , then T(Kn) ↘T(K)

412
H.T. Nguyen and S. Sriboonchitta
Any function T ∶K
→[0, 1] satisfying the above three axioms is called a
capacity functional. Capacity functionals play the role of distribution functions of
random variables. The collection of closed sets FK = {F ∈F ∶F ∩K ≠∅} plays
the role of intervals (−∞, y] on the real line, in the determination of the distribution
function of a real-valued random variable Y: FY(y) = P(Y ≤y) = PY((−∞, y]).
Like Lebesgue-Stieltjes theorem, the following result simpliﬁes the search for
probability laws governing random evolution of random sets.
Choquet Theorem. If T ∶K →[0, 1] is a capacity functional, then there exists
a unique probability P on B(F) such that P(FK) = T(K) for all K ∈K .
For more details on the above, see Nguyen and Tran [33]. For the extension of
Choquet theorem to random fuzzy sets, see [34]. Finally, for representing fuzzy infor-
mation on a computer system, see, e.g., [35].
References
1. Zadeh, L.A.: Fuzzy sets. Inf. Control 8, 338–353 (1965)
2. Haavelmo, T.: The probability approach in econometrics. Econometrica 12, 1–115 (1944)
3. Lindley, D.: Scoring rules and the inevitability of probability. Intern. Statist. Review 50, 1–26
(1982)
4. Goodman. I.R., Nguyen, H.T., Rogers, G.S.: On the scoring approach to admissibility of uncer-
tainty measures in expert systems. J. Math. Anal. Appl. 159, 550–594 (1991)
5. Kolmogorov, A.N.: On logical foundations of probability theory. In: Probability Theory and
Mathematical Statistics (Tbilisi, 1982). Volume 1021 of Lecture Notes in Mathematics, pp.
1–5. Springer, Berlin (1983)
6. Johnson, O.: Information Theory and The Central Limit Theorem. Imperial College Press,
London (2004)
7. Elkans, C.: The paradoxical success of fuzzy logic. In: AAAI-93 Proceedings, pp. 698–703
(1993)
8. Nguyen, H.T., Kosheleva, O., Kreinovich, V.: Is the success of fuzzy logic really paradoxical?:
toward the actual logic behind expert systems. Int. J. Intell. Syst. 11(5), 295–326 (1996)
9. Gehrke, M., Walker, C., Walker, E.: A mathematical setting for fuzzy sets. Int. J. Uncertainty
Fuzziness Knowl. Based Syst. 5(3), 223–238 (1997)
10. Aumann, R.J., Shapley, L.S.: Values on Non-Atomic Games. Princeton University Press,
Princeton (1974)
11. Denault, M.: Coherent allocation of risk capital. J. Risk. 4, 1–34 (2001)
12. Aubin, J.P.: Optima and Equilibra. Springer, Berlin (1993)
13. Matheron, G.: Random Sets and Integral Geometry. Wiley, New York (1975)
14. Robbins, H.E.: On the measure of a random set. Ann. Math. Statist. 14, 70–74 (1944)
15. Nguyen, H.T., Walker E.A.: A First Course in Fuzzy Logic. Chapman and Hall/CRC Press,
Boca Raton (2006)
16. Fuller, R.: On generalization of Nguyen’s theorem: a short survey of recent developments. In:
Advances in Soft Computing, Robotics and Control, pp. 183–190. Springer, New York (2014)
17. Vorobiev, D., Seikkala, S.: Towards the theory of fuzzy diﬀerential equations. Fuzzy Sets Syst.
125, 231–237 (2002)
18. Bzowski, A., Urbanski, M.K.: A note on Nguyen-Fuller-Keresztfalvi theorem and Zadeh’s
extension principle. Fuzzy Sets Syst. 213, 91–101 (2013)
19. Goodman. I.R., Nguyen, H.T., Walker, E.A.: Conditional Inference and Logic for Intelligent
Systems. North-Holland, Amsterdam (1991)

On Fuzzy Theory for Econometrics
413
20. Milne, P.: Bruno de Finetti and the logic of conditional events. Br. J. Philos. Sci. 48, 195–232
(1997)
21. Milne, P.: Algebras of inytervals and a logic of conditional assertions. J. Philos. Logic. 33(5),
497–548 (2004)
22. Pelessoni, R., Vicig, P.: The Goodman-Nguyen relation in uncertainty measurement. Adv.
Intell. Syst. Comput. 190, 33–37 (2013)
23. Pelessoni, R., Vicig, P.: The Goodman-Nguyen relation within imprecise probability theory,
Int. J. Approximate Reasoning, in press (2014)
24. Bamber, D., Goodman, I.R., Nguyen, H.T.: Robust reasoning with rules that have exceptions.
Ann. Math. Art. Intell. 45, 83–171 (2005)
25. Draeseke, R., Giles, D.E.A.: A fuzzy logic approach to modelling the New Zeland underground
economy. Math. Comp. Simul. 59, 115–123 (2002)
26. Ene, C.M., Hurduc, N.: A fuzzy model to estimate Romanian underground economy. Intern.
Auditing Risk Manage. 2(18), 1–10 (2010)
27. Pearl, J.: Causal inference in statistics: an overview. Statist. Surveys 3, 96–146 (2009)
28. Holland, P.W.: Statistics and causal inference. J. Amer. Statist. Assoc. 81(396), 945–960 (1986)
29. Greenland, S.: An overview of methods for causal inference from observational studies. In:
Gelman, A., Meng, X.L. (eds.) Applied Bayesian Modeling and Causal Inference from Incom-
plete Data Perspectives, pp. 3–13. Wiley, New York (2004)
30. Gierz, G. et al.: A Compemdium of Continuous Lattices, Springer, Berlin (1980)
31. Waszkiewicz, P.: How to do domains model topologies? Electron. Notes Theoretical Comput.
Sci. 83, 1–18 (2004)
32. Copi, R., Gil, M., Kiers, H.: The fuzzy approach to statistical analysis. Comput. Stat. Data
Anal. 51, 1–14 (2006)
33. Nguyen, H.T., Tran, H.: On a continuous lattice approach to modeling of coarse data in systems
analysis. J. Uncertain Syst. 1(1), 62–73 (2007)
34. Nguyen, H.T., Wang, Y., Wei, G.: On Choquet theorem for upper semicontinuous functions.
Int. J. Approximate Reasoning 46, 3–16 (2007)
35. Nguyen, H.T., Kreinovich, V.: How ro fully represent expert information about imprecise prop-
erties in a computer system: random sets, fuzzy sets, and beyond. Int. J. Gen. Syst. 43, 586–609
(2014)
Authors Biography
Hung T. Nguyen was born in Thanh-Hoa, Vietnam, in 1944.
He received the BS degree in Mathematics from University
of Paris XI (France) in 1967, the MS degree on Mathematics
from University of Paris VI (France) in 1968, and the Ph.D. in
Mathematics from University of Lille (France) in 1975. After
spending several years at the university of California, Berke-
ley and the University of Massachusetts, Amherst, he joined
the faculty of Mathematical Sciences, New Mexico State Uni-
versity (USA), where is currently Professor Emeritus. Nguyen
was on the LIFE Chair of Fuzzy Theory at Tokyo Institute
of Technology (Japan) during 1992–1993, Distinguished Visit-
ing Lukacs Professor of Statistics at Bowling Green State Uni-
versity, Ohio, in 2002, Distinguished Fellow of the American
Society of Engineering Education (ASEE), Fellow of the Inter-
national Fuzzy Systems Association (IFSA). He has published
16 books, 6 edited books and more than 100 papers. Nguyen’s current research interests include
Fuzzy Logics and their Applications, Random Set Theory, Risk Analysis, and Causal Inference
in Econometrics.

414
H.T. Nguyen and S. Sriboonchitta
Songsak Sriboonchitta was born in Chachoengsao, Thailand
in 1949. He received Bachelor and Master Degrees in Eco-
nomics from Thammasat University, Thailand in 1972 and
1975 respectively. He also received Ph.D. degree in Agri-
cultural Economics from University of Illinois at Urbana-
Champaign in 1983. He has been working at the Faculty of
Economics, Chiang Mai University since 1975. He was the
dean of the Faculty of Economics, Chiang Mai University dur-
ing 2004–2008. Presently, he is Professor of Economics, presi-
dent of Thailand Econometric Society and also director of Cen-
tre of Excellence in Econometrics. He has published one book
and two edited books and more than 100 papers. His current
research interest is econometrics.

On Z-numbers and the Machine-Mind
for Natural Language Comprehension
Romi Banerjee and Sankar K. Pal
Abstract This article is centred on two themes. The ﬁrst is the extension of
Zadeh’s basic Z-numbers into a tool for level-2 Computing With Words
(CWW) and consequently subjective natural language understanding. We describe
an algorithm and new operators (leading to complex or spectral Z-numbers), use
them to simulate differential diagnosis, and highlight the inherent strengths and
challenges of the Z-numbers. The second theme deals with the design of a, Min-
sky’s Society of Mind based, natural language comprehending machine-mind
architecture. We enumerate its macro-components (function modules and memory
units) and illustrate its working mechanism through simulation of metaphor
understanding; validating system outputs against human-comprehension responses.
The framework uses the aforementioned new Z-number paradigm to precisiate
knowledge-frames. The completeness of the conceptualized architecture is analyzed
through its coverage of mind-layers (Minsky) and cerebral cortex regions. The
research described here draws from multiple disciplines and seeks to contribute to
the cognitive-systems design initiatives for man-machine symbiosis.
Keywords Artiﬁcial general intelligence (AGI) ⋅Cognitive machines ⋅Emotion
machine ⋅Machine subjectivity ⋅Machine understanding ⋅
Man-machine
symbiosis ⋅Qualia ⋅Society of mind ⋅Thinking machines
R. Banerjee (✉) ⋅S.K. Pal
Center for Soft Computing Research, Indian Statistical Institute, Kolkata, India
e-mail: rm.banerjee@gmail.com
S.K. Pal
e-mail: sankar@isical.ac.in
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_22
415

1
Introduction
“Language is, at its core, a system that is both digital and inﬁnite. To my knowledge, there
is no other biological system with these properties…” - [1]
Languages are built from ﬁnite sets of primitive symbols or alphabets, which are
combined into complex elements like words, phrases, clauses, sentences, etc., to
express thoughts and the world in general. Natural language understanding begins
with decoding the context-sensitive perceptions of words, followed by incremental
computations on these interpretations across levels of language units (phrases,
sentences, etc.). A sentient machine capable of natural language comprehension
should typically possess high Machine Intelligence Quotient (MIQ) [2], and can be
envisioned as an intelligent human aid.
Coined by Zadeh in 1996, Computing With Words (CWW) [3] is inspired by the
cognitive process of comprehension. It aspires to capacitate a machine to ‘learn’,
‘think’ and ‘respond’ to words akin to human beings. CWW heralds a paradigm
shift from deﬁnite, numeric processing to imprecise word-sense computations.
Realization of the paradigm requires the machine to learn words – both existing and
new, apply them to form semantically, and ideally syntactically, correct natural
language statements. Meanings of words, thus, require to be translated into some
symbolic form. CWW is founded on the concepts of fuzzy logic [2–4], fuzzy
linguistics [4], test score semantics [5] and Precisiated Natural Language (PNL) [6],
and is the precursor to Computing With Perceptions (CTP) [7].
The Z-number [8], proposed by Zadeh, is a fairly new addition to the CTP
family. Besides unifying fundamentals of CWW, it incorporates an indicator of the
information-reliability of a statement. Z-numbers, thus, can precisiate the infor-
mation (factual and subjective) in a ‘statement’. This article describes our efforts –
through an algorithm and perception-operators (forming complex or spectral
Z-valuations) – towards extending it to CWW in ‘sentences’. Our deﬁned methods
are used to simulate a real-life differential diagnosis scenario. The section on
Z-numbers ends with a study of its strengths and practical challenges. The issue that
interests us profoundly is the endogenous instantiation of machine-subjectivity
towards the formation of bespoke elements of comprehension (will we ever be able
to emulate a naturally functioning brain?).
“The world isn’t just the way it is. It is how we understand it, no? And in understanding
something, we bring something to it, no?” – Yann Martel in Life of Pi, 2001
Encouraged by the capabilities of the Z-numbers, we direct our efforts to the design
of machine-mind architecture – a framework of function modules and memory
constructs that realize the machine-self, emulate thinking and the autogenous
arousal of affects and qualia, for natural language comprehension. The framework is
based on Minsky’s Society of Mind [9] and Emotion Machine [10] theories of
‘productive’ (intuitive, commonsense-based) and ‘reproductive’ (learned, deliber-
ative, reﬂective, self-reﬂective, self-conscious) thinking. It utilizes the Z-numbers
to represent knowledge-frames, facilitating constituent perception computations.
416
R. Banerjee and S.K. Pal

The role of the cerebral cortex regions in language comprehension [11, 12] serves
as our reference of design ‘completeness’.
Besides Zadeh and Minsky’s ideas, our work draws from key natural language
understander design efforts spanning over the last four decades [13–19]. Turing’s
landmark paper [20] and Vannevar Bush’s phenomenal article [21] are primary
stimuli behind the research.
In addition to the expansion of the basic Z-numbers, the novelty in our work lies
in using Minsky’s theories of the mind to design a framework for cognitive lan-
guage understanding. The system aims to formulate procedures of comprehension
customized to the problem being processed, learn from mistakes and improvise as
well. While ‘existing’ language understanders [19, 22–24], either do not ‘reﬂect’,
are not ‘self-reﬂective’ or ‘self-conscious’, or do not possess intuition and com-
monsense, our framework conceptually includes each of these elements.
The design is currently in its very early stages and is subject to evolution with
recurrent knowledge gain on the brain-processes.
The article is divided into the following sections: Sect. 2 – presents an overview
of the fundamental theories (CWW, Z-numbers and the Society of Mind) under-
lying our work; Sect. 3 – describes our work (extension of the Z-numbers and
enumeration of the computational-mind architecture); and Sect. 4 – summarizes the
key elements of this article.
2
Fundamentals
This section is dedicated to a brief discussion of the theories underlying the syn-
thesis of intelligent or thinking machines. The discussion begins with an outline of
the Computing With Words (CWW) paradigm, followed by highlights on the
Z-numbers and design principles of a machine-mind framework.
2.1 CWW
“In the coming years, computing with words is likely to evolve into a basic methodology in its
own right with wide-ranging ramiﬁcations on both the basic and the applied levels.” - [3]
The Computing With Words (CWW) [3, 25] paradigm deals with processing
rhetoric perceptions encoded in the building blocks (words and phrases) of natural
language expressions. CWW is imperative when:
i. Do not know rationale – The available information is imprecise and cannot be
represented as numbers. E.g., a person being called ‘young’ when the exact age
is unknown but is perceived to be within a certain age-group.
On Z-numbers and the Machine-Mind …
417

ii. Do not need rationale – The tolerance to imprecision can be exploited to devise
“tractable, robust and low cost solutions”. E.g., putting up a picture on the wall
based on the directions of another person.
iii. Cannot solve rationale – The problems cannot be solved via numerical com-
puting processes. E.g., automation of driving in trafﬁc.
iv. Cannot deﬁne rationale – Words express more than numbers. E.g., a patient
describing his illness.
The foundations of CWW lie in fuzzy logic [2–4], fuzzy linguistics and infor-
mation granulation [4], test-score semantics for mapping natural language state-
ments to degrees of constraint satisfaction [5], precisiation of natural language [6],
and the computational theory of perceptions [7]. CWW can be divided into levels
[26] – ‘level-1’ being the quantiﬁcation of perceptions of adjectives and adverbs
(words and phrases), while ‘level-2’ focuses on the precisiation of entire natural
language statements. Refer to [27, 28] for a discussion on the generic architecture
and algorithm for CWW.
Since its coinage, the last decade has been a witness to phenomenal research on
CWW – particularly ‘level-1’ CWW. Some signiﬁcant endeavours are: an effort
towards formalization of fuzzy number arithmetic [29]; application of mass
assignment theory on fuzzy sets to realize semantic interpretations of membership
functions [30]; design-concepts of fuzzy Finite State Machine (FSM) that generate
linguistic descriptions of complex phenomenon [31], and simulate emotions [32];
studies on the use of Interval Type-2 Fuzzy Sets (IT2-FS) to represent levels of
ambiguities in word perceptions [33]; amalgamation of concepts of fuzzy sets and
ontology [34]; formalization of the Generalized Constraint Language (GCL) into a
toolkit for CWW [35, 36].
CWW can be intuitively envisioned as capable of supporting the emulation of
“subjective-comprehension” of natural language; thereby contributing to research
initiatives
in
Man-machine
symbiosis
[37],
Artiﬁcial
General
Intelligence
(AGI) [38] and the Intelligent Systems Revolution [20, 39]. The primary challenges
in the construction of a system capable of CWW are:
i. Representation of the richness and inherent ambiguity of natural languages –
perceptions (meanings and qualia) of words and phrases:
a. Accommodate polysemes, homonyms, capitonyms, synonyms, metaphors
and other ﬁgures of speech.
b. Provide for simple, complex, compound, declarative, interrogative, imper-
ative, exclamatory and conditional sentences.
c. Model changes in perceptions and encode reasons thereof – emulate pro-
cedural (predestined) learning.
d. Simulate endogenous arousal of subjective associations to events.
e. Synthesize an evolving context-sensitive rule-base and system lexicon to
guide comprehension computations.
ii. Encoding real-world knowledge and ‘common-sense’.
iii. Construction of concept granules underlying natural language expressions:
418
R. Banerjee and S.K. Pal

a. Translate
natural
language
inputs
to
some
precisiated
form
for
machine-processing.
b. Identify contexts and test subsequent sentence relevance [40].
c. Design rules of computations for word and sentence perceptions; integration
of inter-sentence perceptions towards construction of concept granules.
d. Comprehend semantics despite syntactical errors or incomplete sentences.
e. Encode system inputs and results of computation into machine mentalese
[41] and human-understandable forms, respectively.
iv. Processing time requirements to be of the order of average human cognition
[∼150–300 ms].
v. Natural language comprehension involves processing multi-modal sensory
affects (facial expressions, voice intonations, eye-gaze and brightness, response
times, etc.). Such systems, thus, require components that can accept, interpret
and process these non-verbal or non-textual elements [this has been concep-
tually handled in our machine-mind architecture discussed in Sect. 3.2],
resulting in the integration of CWW, Natural Language Processing (NLP) and
affective computing processes.
2.2 Z-Numbers
“You cannot see what I see because you see what you see. You cannot know what I know
because you know what you know. What I see and what I know cannot be added to what
you see and what you know because they are not of the same kind. Neither can it replace
what you see and what you know, because that would be to replace you yourself.” –
Douglas Adams in Mostly Harmless, 1992.
Voluntary actions are the result of decision-making processes, and decisions
depend on information. Thus, greater the reliability of the information, stronger is
the decision made. The Z-number [8] philosophy aims at encoding the reliability or
the conﬁdence in the information conveyed by natural language statements.
The Z-number draws on the concepts in [2–7, 25, 42]. The novelty of the
Z-numbers lies in the fact that it not only considers perceptions of individual words,
but also the perception of an entire statement. It is a manifestation of level-2 CWW.
Consequently, if it were possible to simulate the endogenous arousal of beliefs on
information, the Z-numbers would prove to be an effective means of representations
of machine-subjectivity.
Given a natural language statement, Y, the ‘Z-number’ of Y is a 2-tuple Z = <A, B>,
where A is the restriction (constraint) on the values of X (a real-valued uncertain
variable, interpreted as the subject of Y) and B is a measure of the reliability (certainty)
of A. Typically, A and B are expressed as words or clauses, and are both fuzzy
numbers. Some examples of Z-numbers are:
On Z-numbers and the Machine-Mind …
419

i. Y1 = This book is absolutely excellent.
Therefore, X = Quality of the book, and Z = <excellent, absolutely>.
ii. Y2 = It takes me about half an hour to reach point A.
Therefore, X = Time to reach point A, and Z = <about half an hour, usually>.
Understandably, A is context-dependent while B summarizes the certainty or belief
in the applicability of A given X within the purview of the designated context. The
value of B could be explicitly quoted in the statement (as in example (i)) or it could
be implicit (as in example (ii)).
The ordered 3-tuple <X, A, B> is referred to as a ‘Z-valuation’. A Z-valuation is
equivalent to an assignment statement ‘X is <A, B>’. As for example:
i. The Z-valuation of Y1 is <Quality of the book, excellent, absolutely>.
Implication: [Quality of the book] is <excellent, absolutely>.
ii. The Z-valuation of Y2 is <Time to reach point A, about half an hour, usually>.
Implication: [Time to reach point A] is <about half an hour, usually>.
A collection of Z-valuations is referred to as ‘Z-information’ and is the stimulus to a
decision-making process.
Preliminary rules of Z-number computations [8] are:
i. For the purpose of computation, the values of A and B need to be precisiated
through association with membership functions, μA, μB respectively.
ii. X and A together deﬁne a random event in R, and the probability of this event,
p, may be expressed as:
p = ∫
R
μA u
ð ÞpX u
ð Þdu
ð1Þ
where, u is a real-valued generic value of X and pX is the underlying (hidden)
probability density of X.
iii. The Z-valuation <X, A, B> is viewed as a generalized constraint on X, and is
deﬁned by:
Probability X is A
ð
Þ is B
or, p = ∫
R
μA u
ð ÞpX u
ð Þdu is B
ð2Þ
(2) is mathematically equivalent to the expression:
p = μBð∫
R
μA u
ð ÞpX u
ð ÞduÞ
subject to, ∫
R
pX u
ð Þdu = 1
ð3Þ
iv. Computation using the Z-numbers is based on the ‘Principle of Extension’.
For example, considering a problem statement of the form:
“It is probable that Mr. Smith is old. What is the probability that he is not?”
Let, X = Mr. Smith’s age, A = old, B = probable, C = not old, D = degree of
certainty;
420
R. Banerjee and S.K. Pal

μA, μB, μC, μD are the membership functions associated with A, B, C and D
respectively;
pX is the underlying (hidden) probability density of X; u is a real-valued generic
value of X.
We therefore have: Probability (X is A) is B; and
We need to evaluate: Probability (X is C) is ? D.
Thus, using the Principle of Extension and expressions (1), (2) and (3),
< X, A, B >
< X, C, ? D > = Probability X is A
ð
Þ is B
Probability X is C
ð
Þis? D =
μBð∫R μA u
ð ÞpX u
ð ÞduÞ
∫R μC u
ð ÞpX u
ð Þdu


is? D
Implying,
μD w
ð Þ = suppXðμBð∫
R
μA u
ð ÞpX u
ð ÞduÞÞ
subject to, w = ∫
R
μC u
ð ÞpX u
ð Þdu and ∫
R
pX u
ð Þdu = 1
ð4Þ
Refer to [43] for a comprehensive discussion on the work done on Z-numbers.
While most of these initiatives concentrate on the deﬁnition of Z-calculi, our focus
is on its use in the embodiment of semantic-sense comprehension of natural lan-
guage sentences. Major challenges in the implementation of the Z-numbers lie in
the lack of: (a) Representation of meanings of natural language elements in some
form of machine-language, and (b) the emulation of endogenous arousal of cer-
tainty values and subsequent metacognition. These issues call for synergistic
research across multiple disciplines: philosophy, psychology, neuroscience and
computer science towards understanding the neural processes of comprehension in
human beings and deﬁning their computational equivalents.
The capabilities of the methodologies elucidated in the preceding sections,
encourage machine-mind synthesis – the design of a machine-mind framework that
is self-evolving and self-organizing, autogenously attaches subjectivity to com-
prehension of the world and possesses common-sense. The following section is
dedicated to a discussion of the essential properties of such a cognitive architecture.
2.3 Machine-Mind
“You end up with a tremendous respect for a human being if you’re a roboticist” – Joseph
Engelberger, 1985
The human brain is a continually evolving, self-organizing computing system that
acquires, constructs, stores and processes symbols. A cognitive system, analogous
to the human brain, therefore, must think, improve by learning, adapt to the
environment, and ﬁnd structure in ever-growing amounts of real-world data. Such
On Z-numbers and the Machine-Mind …
421

systems require analyzing problems from multiple perspectives and ascertain
viewpoints that are in harmony with the context, identify objectives, weigh multiple
solution strategies and activate scheme(s) that predictably lead the system to some
goal state in reasonable time. These solution schemes include commonsense rea-
soning [44–48] and improvisation. Cognitive systems aim at man-machine sym-
biosis [37], e.g., intelligent sentient aids for the unwell and elderly, library
cataloguers, supports for children with learning disorders.
The Cognitive Machine or ‘Thinking Machines’ were pioneered by Turing. In
[49], he describes the design of random, self-organized, self-evolving structures for
the construction of intelligent machines. The pleasure-pain system outlined here is
perhaps the earliest work on ‘understanders’ built on the philosophy of CWW
[3, 25]. The Turing test described in [20] yet stands as a test of machine
think-ability. The last four decades or so has witnessed sporadic work in this area,
with major milestones [19] being achieved over the last decade. [Refer to [50] for a
broad comparison between some well known computational-mind theories.]
Our efforts in the thinking-systems initiative, is based on Minsky’s phenomenal
compilation on the Society of Mind [9] and Emotion Machine [10] theories. These
theories are ultimate culminations of a computational theory of the human mind,
consequential catalysts for ‘thinking’ on ‘thinking’, and are yet to be entirely
realized. While the ‘Society of Mind’ has been widely used [22–24, 51–54], the
‘Emotion Machine’ has seen sparse implementation initiatives [55–57].
Drawing from [58–63], the design prerogatives of a cognitive machine are:
i. Possess a ﬁnite alphabet set, which are used to form complex components like
words, sentences, etc.
ii. Have a ﬁnite, substantial memory unit that can store a large number of inde-
pendently variable symbols.
a. These symbols assume values from elements in the alphabet set; values
represent data and instructions.
b. These values can be generated, stored, searched for, manipulated upon and
deleted. The system thus should include a large and adaptive repository of
information or knowledge.
c. Symbols interpreted as instructions control system behaviour (internal,
external, self-modiﬁcation, intuition, etc.).
d. Symbols that represent information ﬂowing into the system through sensors
and other input devices represent beliefs about the world.
iii. System-knowledge to be inclusive of commonsense [45] and acquired run-time
concepts. Knowledge-handling mechanisms require strategies that can realize
cross-contextual associations [21].
iv. An
adaptive
system
is
reﬂective
[64]
or
history-sensitive
[58]
and
self-conscious [65, 66]. It incorporates structures that represent the self, and
questions its own actions towards robustness and fault-tolerance improvement.
422
R. Banerjee and S.K. Pal

These necessitate maintaining performance statistics for debugging [67], step-
ping and tracing facilities, interfacing with the external world, predicting future
computations, self-optimisation, self-modiﬁcation and self-activation.
v. Data structures to represent the complexity, variety, unpredictability and
degrees of familiarity of an environment.
vi. Emulate neurogenesis [68] by being part of a social system – acquire new forms
of knowledge (e.g. new concepts and language skills), and adapt to changing
goals,
principles,
ideals,
preferences,
likes,
dislikes.
This
demands
motive-comparators (‘critics and selectors’ [10, 69, 70],) to trade-off between
competing alternatives, analyze long-term or short term objectives, ignore or
suppress some motives or needs in the light of others and form new goals.
vii. The system must be comparable to average human processing [51] – conscious
processing (of the order of 100 ms) and unconscious processing (at the speed of
neural ﬁring which is 40–1000 times per second).
2.3.1 Building Blocks of the Society of Mind Theory
The Society of Mind [9] theory construes the mind as a hierarchical, modular
computing mechanism, assembled of the following:
Agents: Building blocks of a computational mind; a component of a cognitive
process that is simple enough to ‘understand’.
Agency: Societies of agents which in unison perform functions more complex than
a single agent. The mind is a society of agencies.
K-lines: Agents (analogous to data and control structures in system architectures)
that turn on designated sets of agents. Types of K-lines:
Nemes: Agents responsible for the representation of an idea (context) or a state
of the mind.
Nomes: Agents that control the manipulation of representations and effect
agencies in a predetermined manner.
Frames [71]: Data structures representing the depiction of events and its constituent
properties. These structures connect into hierarchical connected graphs of nodes
and relations, where ‘top-level’ frames (or ‘frame-headers’) depict designated
abstractions of a situation, while the ‘lower-level’ frames have terminal slots (or
‘sub-frames’) instantiated to event-speciﬁc data. Data entry into the terminal slots is
directed by assignment conditions like ‘name of a person’, ‘pointer to another
sub-frame’, ‘relation to another sub-frame’, etc. Types of frames:
Surface Syntactic Frames: For verb and noun structures, prepositional and
word-order indicator conventions.
Surface Semantic Frames: For action-centred meanings of words, qualiﬁers and
relations involving participants, instruments, trajectories and strategies, goals,
consequences and side-effects.
On Z-numbers and the Machine-Mind …
423

Thematic Frames: For scenarios concerned with topics, activities, portraits,
setting, outstanding problems and strategies commonly linked to a topic.
Narrative Frames: For scaffoldings of typical stories, explanations, and argu-
ments, conventions about foci, protagonists, plot forms, development, etc.;
assists the construction of new, instantiated thematic frame in the mind.
Difference-Engines: Problem solvers based on the identiﬁcation of the dissimi-
larities between the current state of the mind and some goal state.
Censors: Restrain mental activity that precedes unproductive or dangerous actions.
Suppressors: Suppress unproductive or dangerous actions.
Protospecialists: Highly evolved agencies that yield initial behavioural solutions to
basic problems like locomotion, defence mechanisms etc. These develop with time.
Types of Learning:
Accumulating: Remember every experience as a separate case.
Unframing: Find a general description for multiple examples.
Transframing: Form an analogy or mapping between two representations.
Reformulation: Find new schemes of representing existing knowledge.
Predestined Learning: Learning that develops under sufﬁcient internal and
external constraints such that the goal is assured, like learning a language or
learning to walk.
Learning from Attachment Figures: Learning how and when to adopt a par-
ticular goal and prioritize it, based on reinforcement of knowledge by ‘attach-
ment ﬁgures’- acquaintances who profoundly inﬂuence our minds. E.g., ‘praise’
and ‘censure’ from parents and teachers contribute signiﬁcantly to goal learning.
2.3.2 Layers of the Mind
‘Thinking’ is a complex phenomenon entailing the analysis of a given situation
across multiple causal perspectives, consideration of valid propositions and solution
methods, and to apply or improvise upon them towards appropriate solution(s).
This involves recall, manipulation and organization of a vast repertoire of knowl-
edge, and powerful automated reasoning processes. Thinking operates across a
diverse array of mental realms [70, 72] some of which are: (a) Physical: Where
object behaviour is predicted; (b) Social: Dealing with inter-personal relationships;
and, (c) Mental: Reﬂections upon mistakes, failures and successes.
In [10] Minsky describes the mind as ‘thinking’ in terms of a ‘layered-
critic-selector-reﬂective’ [48, 69, 70, 72] framework. The six-layered structure in
Fig. 1 illustrates this model. Each of the layers incorporates ‘critics’ that consider
the external world and the internal system states, and activate ‘selectors’ to initiate
‘thinking’ on the interpretation strategies. The lower levels of the model handle and
represent ‘instinctive reactions’ to the external world, while the higher levels
control the reactions of the lower levels in accordance with the system’s model of
itself. The basic functions [10, 72] of the layers in the model are:
424
R. Banerjee and S.K. Pal

Instinctive or inborn reactions: An implicit database of ‘if situation and goal,
then do action’ reaction-rules like: ‘if there is a seat and you are tired, then sit’;
often instrumental in predicting outcomes to situations.
Learned reactions: A database of <problem_descriptors, action, result, rea-
son> tuples ranked in the decreasing order of reinforcement; greater the rein-
forcement, higher is the probability of the action being recalled. E.g: I am far from
something I need immediately →Run towards it; I feel scared →Run quickly to a
safe place.
Deliberative thinking: Consideration of several alternative solution approaches,
and choosing the best; logic and commonsense reasoning to select solution paths.
E.g: Action A did not quite achieve my goal →Try harder, or ﬁnd out why;
Action A worked but had adverse effects →Try some variant of that action.
Reﬂective
thinking:
Introspection
over
mental
activities
that
went
into
decision-making, rank inference methods, representation selection, etc. E.g.: The
search has become too extensive →Find methods that yield fewer alternatives;
Overlooked some critical feature →Revise problem description.
Self-reﬂective thinking: While the reﬂective layer considers only recent thoughts
that went into some decision-making, the self-reﬂective layer focuses on the entity
that ‘thought’. E.g: I missed an opportunity by not acting quickly enough →Acti-
vate a mental alarm that alerts me whenever I am procrastinating; I can never get
this right →Spend time practicing required skills.
Self-conscious emotion: Veriﬁcation of agreement of decisions with ideals;
self-appraisal. E.g: I think I am good at this task →Can I do it as well as the best
people I know? How is it that other people can solve this problem? →Spend time
with those who are good at it.
Fig. 1 The layers of the
computational mind, as
deﬁned by Minsky in [10]
On Z-numbers and the Machine-Mind …
425

With this brief overview of the theories underlying our efforts, the article now
progresses to a discussion of the work done. In the following section, we illustrate
the very basic of our initiatives towards the design on a Machine-mind or a
computational-mind for man-machine symbiosis. The section begins with a dis-
cussion on the primary features of the Z-number approach to CWW – a mechanism
for processing machine-subjectivity, followed by a presentation of a generic
machine-mind framework for natural language comprehension.
3
Proposed Work: Theory and Results
3.1 The Z-Number Approach to CWW
As has been emphasized in almost each of the preceding sections, the emulation of
subjectivity is a prime property of a sentient system; the Z-number is our principal
tool towards representing and processing machine subjectivity. In this section we
present an algorithm for Z-number based CWW, primitive operators for processing
Z-valuations or Z-information sets and an analysis of the advantages and associated
issues with Z-number realizations.
3.1.1 Z-Number Based Algorithm for CWW [27, 28]
Natural language comprehension, intuitively, follows an incremental-developmental
strategy (described and illustrated in Sect. 3.2) – building upon existing knowledge
and previous comprehension granules. The following algorithm works through a
primitive developmental methodology of understanding complex and compound
sentences (S) by decomposing them into their simple sentence constituents, evalu-
ating and processing their Z-valuation equivalents, and integrating these results into
a comprehension granule of S. Section 3.1.3 presents a snapshot of a real-life
example of execution of the following algorithm for differential diagnosis.
Input: Natural language sentence (I).
Output: Context-dependent response (O) to I.
Assumptions:
i. The system is capable of identifying irrelevant sentences.
ii. The system grasps the perception of a complex or a compound sentence (Y) by –
a. Extracting the simple sentence components of Y.
b. Individually comprehending each of these simple sentence components.
c. Integrating these component perceptions.
426
R. Banerjee and S.K. Pal

Steps:
1. If I is irrelevant
Then
Else
Goto step 10 
Goto step 2
2. If I is a simple sentence
Goto step 3 
Else 
a. Extract the simple sentence component set (I') of I
b. Repeat steps 3 through 4 for each sentence in I'
c. Goto step 5. 
Then
3. Extract the values of X, A and B in I to evaluate the Z-valuation (ZI)
4. ConvertZIintoequivalentmathematicalexpression(ZE)(basedonEqs.(3)and(4))
5. Assemble all ZE to form the logical expression (E) guided by the conjunctions
or connectives in I
6. Convert E to the mathematical expression (M)
7. Evaluate M to receive a set of Z-valuations (ZO) in response
8. Translate ZO into simple sentences (S)
9. If step 8 results in more than one simple sentence
10. Stop
3.1.2 Primitive Z-Number Based Operators for Perception
Manipulation
The operators described here are basic Z-number based operators for perception
intersection and union, and process Z-information sets towards comprehension
granule formations.
On Z-numbers and the Machine-Mind …
427

The simulation of differential diagnosis described in Sect. 3.1.3, utilizes the
following operators – in conjunction with the algorithm described in Sect. 3.1.1, to
arrive at results that coincide with natural intuitive responses of human beings.
Let S1 and S2 be two natural language statements, and Z1 = <X1, A1, B1> and
Z2 = <X2, A2, B2> be the Z-valuations of S1 and S2 respectively
• Operator for the intersection of perceptions ð ∩PÞ [27, 28]
If S1 deﬁnes a requirement and S2 describes an event from the perspective of S1,
i.e.,
X1 = X2 ≠∅,
A1 = A2 ≠∅,
and
B1 ∈fwords that are synonymous
to 0expect0g,
The Intersection of Perceptions ðZ1 ∩P Z2Þ is deﬁned as,
Z1 ∩P Z2
ð
Þ = < X1, A1, B2 >
ð5Þ
This expression describes the certainty with which Z2 complies with Z1, which in
turn is a measure of the certainty with which S2 is in accordance with S1.
For example:
i. Requirement: S1 = I would like my book to be a Miss Marple mystery;
Z1 = <detective, Marple, expected>
Statement in given mystery book: S2 = Miss Marple was investigating into
whereabouts; Z2 = <investigator, Marple, appears_to_be> => Z2 = <detective,
Marple, appears_to_be>
Thus, Z1 ∩P Z2
ð
Þ = <detective, Marple, appears_to_be> => the book appears
to be a Miss Marple mystery.
ii. Requirement: S1 = I prefer my soup piping hot; Z1 = <soup temperature, piping
hot, expected>
Situation: S2 = The soup served is lukewarm; Z2 = <soup temperature, luke-
warm, certainly> => Z2 = <soup temperature, piping hot, false>
Thus, Z1 ∩P Z2
ð
Þ = <soup temperature, piping hot, false> => the soup served
is not piping hot.
The perception-intersection operator deﬁned above could predictably come of
use in scenarios where it is imperative to verify the conﬁdence with which the
current situation satisﬁes a given requirement.
• Operator for the union of perceptions ð ∪PÞ
If S1 and S2 depict progressions in perceptions of the same event,
Where, S1 represents an earlier perception, and S2 is interpreted from the per-
spective of S1 i.e., A1 = A2 ≠∅,
The Union of Perceptions ðZ1 ∪P Z2Þ is deﬁned as,
Z1 ∪P Z2
ð
Þ = < X, A1, B >
Where, X = the event and B = B1, B2
ð
Þ
ð6Þ
i.e., B = an ordered set of certainty progressions
428
R. Banerjee and S.K. Pal

This expression leads to complex or a spectral Z-number (drawing from the basic
philosophy of ‘spectral fuzzy sets’ [73]) or a spectral Z-valuation that describes the
updated certainty perception of the event. [Details on the spectral Z-valuations are
out of the scope of the current article.]
For example:
i. S1 = Miss Marple made certain inquiries; Z1 = <detective, Marple,
appears_to_be>
S2 = Miss Marple solved the mystery; Z2 = <detective, Marple, certainly>
Thus,
Z1 ∪P Z2
ð
Þ = <detective, Marple, (appears_to_be, certainly) => I
wasn’t sure that Miss Marple was the detective, but I now am.
ii. S1 = It took me about an hour to reach point A yesterday; Z1 = <time to reach
point A yesterday, an hour, probably>
S2 = It took me about an hour to reach point today as well; Z2 = <time to reach
point A today, an hour, probably>
Thus, Z1 ∪P Z2
ð
Þ = <time to reach point A, an hour, (probably, probably)
> => It probably takes me an hour to reach point A.
iii. S1 = It took me about an hour to reach point A yesterday; Z1 = <time to reach
point A yesterday, an hour, probably>
S2 = It took me about half-an-hour to reach point today; Z2 = <time to reach
point A today, half-an-hour, supposed> => Z2 = <time to reach point A today,
hour, false>
Thus, Z1 ∪P Z2
ð
Þ = <time to reach point A, an hour, (probably, false)> => I
am not very sure as to how long it would take me to reach point A, close to an
hour perhaps.
The perception-union operator deﬁned above could be of use in the study of
modulations in the conﬁdence in a concept and reasons thereof. Intuitively, lesser
the modulation in certainty, greater is the stability of the association between X and
A in the current context. It might thus be possible to locate abrupt changes in
certainty patterns and identify consequent reasons – a step towards emulation of
intuitive (or predestined) learning by a machine.
Observations:
i. These primitive operators do not claim to lead to deﬁnitive conceptual sum-
maries, but do initiate the formation of instantaneous summaries at time t –
with respect to the information experienced till t.
ii. These operators depend on the ‘meanings’ of the constituent parameters of the
Z-numbers.
iii. The interpretation of S2 from the perspective of S1, as demonstrated in the
examples above, involves loss in information. This, however, is countered by
the substantiation of perception summarization.
The use of these operators involves measures of information-loss and
summarization-degree, following which the interpretations may be modelled to be
On Z-numbers and the Machine-Mind …
429

inclusive of high-information facts and yet arrive at a reasonable balance between
the measures. E.g., in differential diagnosis, it would be imperative to include
symptom-speciﬁcs in the interpretations (refer to Sect. 3.1.3).
3.1.3 An Example of Z-Number Based Comprehension
The experiment described here, simulates an instance of differential diagnosis. We
consider here, a sample discourse (Table 1) between a doctor (D) and patient (P) as
the basis of the simulation.
Assumptions:
i. The Machine (Mc) is aware of the probability distribution of the symptoms and
symptom details per disease.
ii. Mc is proﬁcient in syntactic analysis, and can resolve anaphoric and cataphoric
dependencies.
iii. Mc can categorize symptom descriptions into information granules like
‘fever-presence-symptom’, ‘fever-fall-symptom’.
iv. Mc comprehends ‘meanings’ of words in natural language expressions.
Execution:
Mc should ideally behave like the doctor (D) in the conversation shown in
Table 1.
Table 1 Sample discourse between a patient (P) and a doctor (D)
Natural language statement
Equivalent Z-valuations
D: Please tell me about your
problems
Z1 = <problem, exists, expectedly>
P: It’s been two days since I’ve
been suffering from high fever!
Z11 = <problem, fever, certainly>
=> Z11 = <problem, exists, certainly>
Z111 = <fever intensity, high, certainly>
Z112 = <fever duration, 2 days, probably>
D: Did you note the temperature?
Did you notice if it comes at
speciﬁc times, is accompanied by
sensations of nausea?
Z2 = <fever temperature, above 100, expectedly>
Z3 = <fever arrival time, speciﬁc, uncertain>
Z4 = <fever accompaniment, nausea, uncertain>
P: The fever’s been ranging at
around 102–104° and it’s been
coming in the mornings and
evenings. The temperature falls
after a bout of intense sweating and
is accompanied by shivering,
nausea, and aches all over
Z21 = <fever temperature, 102–104, certainly>
=> Z21 = <fever temperature, above 100, certainly>
Z31 = <fever arrival time, morning and evening, certainly>
=> Z31 = <fever arrival time, morning and evening, certainly>
Z41 = <fever accompaniment, shivering, certainly>
Z42 = <fever accompaniment, nausea, certainly>
=> Z42 = <fever accompaniment, nausea, certainly>
Z43 = <fever accompaniment, aches, certainly>
Z51 = <fever release, sweating certainly>
Z511 = <sweating pattern, intense, certainly>
(continued)
430
R. Banerjee and S.K. Pal

Thus, assuming the probability distributions of disease-symptoms is well-deﬁned,
after precisiating the memberships of the presence and intensity of symptoms from
the Z-information set acquired from the patient, the system computes the rank of the
membership of the probable diseases with respect to the expression:
E = ððZ111
∧Z112ÞðZ21
∧Z31
∧Z41
∧Z42
∧ðZ51
∧Z511ÞÞ ∧ðZ611
∧Z621Þ ∧ðZ71
∧Z81ÞÞ ð7Þ
Table 1 (continued)
Natural language statement
Equivalent Z-valuations
D: What kind of aches?
Z6 = <aches, speciﬁc, expectedly>
P: A blinding headache and
stinging muscle-aches as well
Z61 = <ache, head, certainly>
=> Z61 = <aches, speciﬁc, certainly>
Z611 = <headache intensity, blinding, certainly>
Z62 = <ache, muscles, certainly>
=> Z62 = <aches, speciﬁc, certainly>
Z621 = <muscle-ache, stinging, certainly>
D: Do you have any problems in
your appetite and sleep patterns?
Z7 = <appetite problem, exists, expectedly>
Z8 = <sleep problem, exists, expectedly>
P: I do not have an appetite and
sleep’s rather disturbed with the
headaches
Z71 = <appetite problem, appetite absent, certainly>
=> Z71 = <appetite problem, exists, certainly>
Z81 = <sleep problem, sleep disturbed, certainly>
=> Z81 = <sleep problem, exists, certainly>
Notes:
i. For an expression symbolized by Zijk, i = the Zi
th enquiry, j = the jth response to Zi, k = the kth detail of
Zij.
ii. ∩P is used from the perspective of the doctor on the expression pairs (Z1, Z11), (Z2, Z21), (Z3, Z31), (Z4,
Z42), (Z6, Z61), (Z6, Z62), (Z7, Z71), (Z8, Z81).
iii. On relevance analysis of the Z-valuations, expressions symbolized Z11, Z43, Z61, Z62 are
inconsequential or redundant.
iv. The doctor, intuitively, uses ∪P to unite patient symptom-persistence intensity certainty after a course
of medication. For example, after a certain course of medication, from observations in Table 2, we have:
Z611 ∪P Z91
ð
Þ = < headache intensity, blinding, certainly, rarely
ð
Þ >
= > Z′
91 = < headache, improvement, certainly > ; and
Z9 ∩P Z′
91


= < headache, improvement, certainly >
v. The Z-valuations in the example depict top-level perceptions. Each of the natural language statements
incorporate micro-aspects of subjective experiences (e.g. the subtleties that lead to a headache being
attributed ‘blinding’), which have not been considered.
Table 2 Sample discourse between patient (P) and doctor (D), after a course of medication
Natural Language Statement
Equivalent Z-valuations
D: How is the headache?
Z9 = <headache, improvement, expectedly>
P: There is just a mild throbbing
every now and then
Z91 = <headache intensity, mild, certainly>
=> Z91 = <headache intensity, blinding, rarely>
On Z-numbers and the Machine-Mind …
431

Or, E = ððμcertainly ∫
R
μhigh u
ð ÞPfeverintensity u
ð Þdu ∧μprobably ∫
R
μ2days u
ð ÞPfeverduration u
ð ÞduÞ ∧
ðμcertainly ∫
R
μabove100 u
ð ÞPfevertemperature u
ð Þdu ∧μcertainly ∫
R
μmorningandevening u
ð ÞPfeverarivaltime u
ð ÞduÞ ∧
μcertainly ∫
R
μshivering u
ð ÞPfeveraccompaniment u
ð Þdu ∧μcertainly ∫
R
μnausea u
ð ÞPfeveraccompaniment u
ð Þdu ∧
ðμcertainly ∫
R
μsweating u
ð ÞPfeverrelease u
ð Þdu ∧μcertainly ∫
R
μintense u
ð ÞPsweatingpattern u
ð Þdu ∧
ðμcertainly ∫
R
μblinding u
ð ÞPheadacheintensity u
ð Þdu ∧μcertainly ∫
R
μstinging u
ð ÞPmuscleache u
ð ÞduÞ ∧
ðμcertainly ∫
R
μexists u
ð ÞPappetiteproblem u
ð Þdu ∧μcertainly ∫
R
μexists u
ð ÞPsleepproblem u
ð ÞduÞÞ
ð8Þ
i.e., the system uses Eq. (8) to evaluate and rank the probability of all diseases in its
repertoire, as per Eq. (4)), ideally leading to the Z-valuation expression: (<Disease,
malaria, most likely> ^ <Blood test, required, deﬁnitely>).
3.1.4 Analysis of the Z-Number Approach to CWW
• Pros
The potential of the Z-numbers is illustrated through the experiment described in
Sect. 3.1.3 [refer to [27, 28] for other simulation examples]. Features of interest
are:
i. Though originally a model for the precisiation of natural language statements,
the Z-number methodology can be envisioned to precisiate any natural lan-
guage sentence.
ii. A Z-number captures the perception of a single natural language sentence,
while the Z-information does the same for a group of sentences.
iii. A Z-number summarizes simple sentences only. Thus, if a complex or a
compound sentence (S) were decomposed into its simple sentence constitu-
ents, assimilating these constituent Z-numbers would lead to the Z-information
equivalent of S.
iv. Z-numbers support the identiﬁcation of the context (e.g., ‘fever-occurrence’,
‘fever-presence’ etc. with respect to Table 1) of a statement in the universe of
discourse.
v. Z-information allows grouping statements into context-sensitive granules of
information. This mimics the natural data-compression and subsequent
data-comprehension by the human brain. The mechanism, intuitively, should
help in knowledge-extraction from a sentence.
432
R. Banerjee and S.K. Pal

vi. Z-numbers represent the inherent uncertainty associated with machine
behaviour. Moreover, implicit certainty levels of sentences involve affective
computing principles. Z-numbers consequentially integrate affective comput-
ing and CWW, thereby adding a new dimension or ‘level’ to CWW.
vii. Parameters of Z-numbers are context-independent.
viii. Translation from Z-numbers to simple sentences is straightforward.
• Challenges in the implementation of Z-number based natural language
processing
There are indeed quite some challenges that need to be overcome before the
concept ﬁnds emulation on real systems. Some of these issues are:
i. Construction of a self-evolving system lexicon – with words and phrases
granulated and self-organized into semantic nets and synonym clusters; these
clusters and nets form the value pool for parameter X, given a concept.
ii. Identiﬁcation of an evolutionary fuzzy set model that represents perceptions of
words in A and B.
iii. Identiﬁcation of the probability distribution of the events deﬁned by the words
in A – is it practically possible to evaluate the probability distribution for all
events in a context, and how can the probability distributions be updated?
Could the probability distribution be replaced by better statistical parameters?
Encoding of reasons for modulations in probability.
iv. Creation of a dynamic rule-base or an explanatory database – akin to an
Answer-Library (described in Sect. 3.2.1).
v. Formulation of well-deﬁned rules of computation – towards a grammar of
Z-numbers or perceptions
vi. Identiﬁcation of the category of the input statement – simple, complex or
compound,
declarative,
exclamatory,
interrogative,
conditional
and
imperative.
vii. Decomposition of complex or compound sentences (S) into simple sentence
constituents, comprehension of individual component perceptions and inte-
gration of perceptions – based on the connectives used in S.
viii. Evaluation of the relevance [40] of the sentence with respect to the text corpus.
ix. Identiﬁcation of parameters X, A and B from a sentence. The major challenge
lies in extraction of an implicit affect and translating it into parameter B.
x. Formulation of the logical expression based on conjunctions linking
Z-valuations of the input sentence.
xi. Conversion
of
the
Z-valuation
logical
expression
into
an
equivalent
context-sensitive mathematical expression.
xii. Translation of the components of resultant Z-valuations into simple sentences
and integration into concept granules.
On Z-numbers and the Machine-Mind …
433

xiii. Deﬁnition of algorithms for knowledge-extraction – new words and reasons of
comprehension successes and failures – resulting in text corpus and rule base
updating.
xiv. Formulation of methods that simulate the endogenous arousal of emotions and
qualia.
3.2 The Machine-Mind Framework for Natural Language
Understanding
In this section we present the macro-components and working principle of a
machine-mind framework – built on the philosophy of the Society of Mind –
followed by an analysis of its analogy with the human brain. The framework uses
the Z-number equivalents of knowledge in the machine mind as the operands for
processing. Elaborations on agent-structures and algorithms, and detailed memory
data structure formats, are out of the scope of this article.
Here it might be prudent to mention that though the machine-mind constructs
have been predominantly studied in their capacities as text-understanders, the
principles are applicable to the general concept of natural language understanding.
3.2.1 Macro-Components of the Machine-Mind Architecture: Agencies
and Memory Constructs [62]
Comprehension or understanding involves the execution of a number of complex
conscious and omniscient unconscious cognitive processes that ideally lead to the
following mind-activities:
Prediction – Envisage a future action – causally relate the present to past experi-
ences and judge expectations on the basis of intuition, commonsense, reinforced
learning and reﬂection.
Visualization – Conjure mind-images (real or intentional [74] ) of language
components depicting people, places, events, etc.
Connection – Build factual or conceptual associations between: (a) all frames
recalled and those created for the current language processing event, and (b)
real-world or domain knowledge and new information.
Question and Clariﬁcation – Reﬂect upon, and test the strength, completeness,
correctness and relevance of knowledge associations; re-organization and rectiﬁ-
cation of associations.
434
R. Banerjee and S.K. Pal

Evaluation – Test coherence between perception granules, measure relevance of
each and prune the insigniﬁcant; attach notions of subjectivity or ‘self conscious-
ness’ (emotions, degrees of interest, summarize, biases, etc.).
Intuitively,
i. Comprehension ‘iterates [75, 76]’ through the above stages – working
incrementally on micro-granules of information to form coherent networks of
information and a macro-granule summary of the language unit (a text sample,
for example) being understood. Figure 3 illustrates this point.
ii. These processes are complex, mostly concurrent, and co-operative.
iii. The ‘meaning’ of a word or a phrase implies the manner in which the sense of
the language unit is encoded in the mind. These encodings could be in the
form of precise symbols in the native language of the system or as metaphors,
synonyms or associations with other words. A single word or phrase may have
multiple sensory (visual, auditory, etc.) implications as well.
iv. Prediction and visualization involves all but the topmost two layers of the
mind; connection – the four lower layers; question and clariﬁcation – the
learned, deliberative and reﬂective thinking layers; and evaluation involves the
top three layers.
These functions straddle multiple layers of thinking and involve bi-directional
information percolation. The information that is transferred to the higher layers
relies on the extracted language-sample while that from the higher layers is
conceptual and relates to the reader’s sensibilities acquired through learning,
experience and commonsense reasoning.
v. The above list is not an exhaustive enumeration of the broad mechanisms
leading to comprehension. We hope to add to it in the process of under-
standing how the brain ‘understands’ the real world.
• Mind-agencies
A computational mind typically processes, concurrently arriving multi-modal
sensory inputs with existing knowledge about the real-world and the problem
domain, through the above steps to produce granules of understanding.
We categorize the agencies of such a computational mind into super-agencies,
each representing a complex cognitive functionality like ‘reasoning’ or ‘process-
ing’, and sub-agencies. A super-agency comprises of a cluster of sub-agencies that
work in health and harmony to achieve the super-agency functionality. The
sub-agencies are again built of agents each representing an atomic sub-process of
the sub-agency purpose. Figure 2 is a pictorial representation of the mind-agency
framework.
The super-agencies and constituent sub-agencies in a computational mind are as
follows:
Sensory_Gateway (SG): At any instant, SG serves as the receiver of sensory
information, based on the nature of which, ‘sensory’ sub-agencies [Vision (V),
Audition (A), Olfaction (O), Tactile (Tc), Taste (Ta), Balance (B) [77],
On Z-numbers and the Machine-Mind …
435

Temperature (Te) [77], Pain (P) [77] and Kinesthetic (K) [77]] activate other
framework components for further processing. SG transports system results to the
external world as well.
Sub-agencies like A, O and Te continually receive stimuli from the environment
and process these unconsciously; Tc and K are activated in the ‘turning pages’,
‘scrolling over text’ activities. However, none of these contribute signiﬁcantly to
the ‘comprehension’ phenomenon and have thus not been elaborated upon, in this
article.
Vision (V): The ‘eyes’ of the system – leads to textual symbol-extraction,
symbol-interpretation
(numbers,
alphabets,
punctuation,
etc.),
and
symbol-granulation into complex language elements (words, sentences, etc.).
Deducer (De): The ‘brain’ of the system; is responsible for the emulation of each of
the comprehension processes named above. It receives outputs (data) of SG to
formulate units (frames) of comprehension – utilizing syntax and semantic analysis
mechanisms, relevance-evaluation, affect-extraction, comprehension-evaluation and
error-handling processes; sends out instructions (activation, re-evaluation, error
signals, inhibition) to the other super-agencies. The sub-agencies of De are:
Syntax (Sy): Syntax-resolution and consequent generation and manipulation of
surface syntactic frames.
Semantic (Se): Semantic-resolution, generation and updating of surface
semantic, narrative and thematic frames.
Self (Sf): Seasons comprehension granules with components of subjectivity
(affects, biases, ideals, etc.) based on the system personality; multiple
mental-realm activations.
Recall (Re): Thin-slices a problem into sub-problems, maps problems to
memories and retrieves them from long-term into the working memory for
processing in the current context.
Creative (Cr): Projects and suggests solutions for ‘new’ problems; the hub of
reﬂection, imagination, creativity and system IQ [2].
Summary (Su): Analyzes the distance between the current state of the system
and the projected goal through relevance, affect and comprehension progression
evaluation; can activate or inhibit agencies (under De and SG) based on sum-
mary results; consolidation of memories.
Manager (M): The global administrator of the system; runs in the background and is
responsible for the activation and execution of ‘involuntary’ functions (system-time
management, memory handling, process synchronization, K-line management, frame
encoding/decoding, job scheduling, etc.) that support the functioning of all the other
agencies; continual self-evaluation of system processes and subsequent updating
towards optimal (cost effective and robust) system performance.
• Memory Constructs
The long-term memory stores of knowledge, that support the functioning of the
agencies, are:
436
R. Banerjee and S.K. Pal

Lexicon (L): System vocabulary; a resource of language units – words, phrases,
idioms – and their meanings encoded in machine ‘understandable’ form; includes
meanings of words ‘learnt on the ﬂy’ and jargon; meanings may be encoded as
precise statements or exist in a number of data types (sounds, images, metaphors) –
indicating the different ways the machine ‘understands’ or ‘remembers’ an element.
Answer-Library (AL): Resource of <solution_strategy, result, reasons> for a
given <context_parameters, problem> query.
Concept-Network (CoN): Hypergraph of inter-contextual frame associations;
elements are incorporated consciously or unconsciously, but are retrieved
consciously.
Commonsense-Network (ComN): Network of networks of commonsense and
intuitive (automatic) behaviours; components are retrieved ‘unconsciously’; ele-
ments of L, CoN and AL are incorporated into ComN after prolonged periods of
reinforcement.
The basic working-memory data-structures are as follows; these are referenced by
all the agencies and support the deliberative and reﬂective actions of the system:
Log: A global record of time-stamped agency-activity entries; indicates the
instantaneous state of the system, analyzing which – a number of agencies may be
autogenously or exogenously activated, mechanisms like intelligent backtracking
[78] initiated, error signals generated, etc.; serves as an indicator of solution strategy
results and reasons thereof for the system to ‘reﬂect’ upon.
Frame-Associations (FA): A blackboard or scratchpad for frame manipulations
during the process of understanding; categorized into global and local (per
sub-agency); all frame recollections are placed in the global FA space, while sections
of the global FA are transferred into local FA for deliberations by sub-agencies;
sub-agencies under De use their local FA workspace to reason through the appro-
priateness of multiple solution-perspectives before globally ‘advocating (a <problem,
solution, reason> tuple)’ frame manipulation processes through Log; sub-agencies
under M, use their local FA to reason through system optimization mechanisms; each
sub-agency can share sections or all of its local FA with other agencies; globally
approved suggestions (by Su) are implemented in the global FA, and all updates to
existing networks of information are reﬂected across the long-term memory net-
works; all local sub-agency trials are annotated in local FA; trial-results are annotated
in Log and global FA for deliberation and reﬂection by other agencies.
The system memory-management constructs, used by M, are:
Working-Set (WS): Set of pointers to frame-networks in FA being referenced
within a narrow time-window (intuitively, of the order of seconds).
Active-Frames (AF): Set of pointers to frame-networks in FA being referenced
withinabroadtime-window(intuitivelyoftheorderofminutes);WSisasubset ofAF.
Passive-Frames (PF): Set of pointers to frame-networks in FA that were members
of AF but were pruned away due to irrelevance or lack of use; instead of consol-
idating them back to the long-term memory, these frames remain available during
the entire span of the processing of the current text for quick ‘on-demand’ place-
ment into FA for re-processing.
On Z-numbers and the Machine-Mind …
437

Fig. 2 Macro-components (agencies and memory constructs) of the machine-mind framework
[62]
438
R. Banerjee and S.K. Pal

• Working Mechanism
Referring to the functionalities of the components deﬁned in the preceding section,
the basic working mechanism of the framework (illustrated in Fig. 4), is as follows:
Comprehension is an iterative process, where complex granules of compre-
hension (inclusive of surface and deep semantics) are incrementally developed from
primitive knowledge elements and simpler information granules. Given a sample of
natural language to understand, e.g., a text to read, V is activated and it makes
corresponding Log and global FA entries – indicating the symbols extracted,
granulated and interpreted. These interpretations could include annotations like
(author_name, text_name, title, chapter_name, starting words, word meanings,
etc.), based on the L and ComN memories (frames) retrieved. Once initiated,
V extracts text in saccadic-granules [76] until reading and subsequent compre-
hension is complete. De regulates the length of the saccadic-granules and the
location thereof (initiating re-reads on insufﬁcient comprehension or endogenous
enquiries, etc.).
Fig. 3 The iterative incremental-developmental execution schematic of comprehension [62]
On Z-numbers and the Machine-Mind …
439

All retrievals by V are visible, through working-memory entries, for all the other
agencies to deliberate upon. The sub-agencies under De assess the status (famil-
iarity, syntax, semantics, context, affects, interests, relevance, etc.) of the problem
(words, clauses, sentences, paragraphs, frame-systems, etc.) and opportunistically
‘suggest’ interpretation mechanisms and consequent results. These involve
decomposing the problem into sub-problems and incremental-developmental iter-
ations – through long-term to working-memory frame transfers, agency-speciﬁc
local frame-manipulation trials and broadcasting of predictable success-rendering
schemes, signals to improvise or construct new solutions from scratch, alignment of
interpretations with self-interests, and information consolidation – towards the
formation of a granule of comprehension of the entire text sample. M works
seamlessly in the background to support agency activities.
Every hypothesis, agency operation, information retrieval, or change in the
working-memory is corroborated by a Log entry. This allows Su to constantly
monitor (predict, visualize, question, clarify and evaluate) the convergence of
solutions suggested by the sub-agencies, and accordingly activate or inhibit oper-
ations (e.g. Sy and Se might be requested to re-process an incoherent granule).
Ideally, an inhibited agency possesses the right to ‘question’ Su’s directions, and
thereby all of Su’s instructions are annotated with encoded-reasons for evaluation
and reﬂection. In the current version of the system, though no agency can override
Su’s commands, none of its possible partial processing results are lost. All partially
processed frames or inhibited processing vestiges can be retrieved from PF, on
requirement, for re-analysis. [Refer to [62] for a detailed description of the func-
tions
of
each
of
the
sub-agencies
for
comprehending
the
world
and
text-understanding in particular.]
An algorithmic or effective procedural view of the working principle necessitates
detailed elucidation of the working-memory formats and deﬁnition of frame
structures of the architecture, time and space complexity analyses, and correctness
and completeness veriﬁcations. This article clearly focuses on the higher-level
elements of the framework. Subtle hints towards parameters and tuples of these
macro-constructs have been provided across this article, but we deliberately refrain
from discussions on their ﬁne-grained components.
440
R. Banerjee and S.K. Pal

• Properties of the Framework
Essential properties of the machine-mind framework include:
i. The design of the framework is bound to evolve as we learn more about how
the human brain functions. The primary advantage that function-designated
agencies provides is the ease with which an agency may be modiﬁed without
Fig. 4 The working mechanism of the machine-mind framework [62]. [Refer to [9] for deﬁnitions
of the nemes and nomes mentioned here]
On Z-numbers and the Machine-Mind …
441

affecting the design of the entire framework; introduction of new agencies or
framework components would however involve amendments to every level of
the design.
ii. The agencies are interconnected such that they form a causal system. This is
roughly demonstrated in the feed-back schematic of the system in Fig. 2.
iii. SG depicts instinctive and learned behaviour, while all the other agencies
transverse all the layers of thinking.
iv. SG sub-agencies reference L and ComN, and De references CoN, ComN and
AL.
v. CoN and ComN draws inspiration from ConceptNet [79], while AL from
Hacker [17].
vi. Information storage and retrieval from each of the long-term knowledge dat-
abases
involves
encoding/decoding
processes
across
frame-types
and
data-types.
vii. M is responsible for arbitrating multiple log-access requests from a number of
agencies.
viii. Re, Cr, and Su constitute the Difference-Engines of the machine-mind
framework. These are built on the functional programming [58] paradigm –
where function-modules are combined into bespoke algorithm ﬁtting the
current interpretation problem.
ix. Su is the control shell of the architecture – coordinating inter-agency activities
via heuristics and approximation schemes, to handle combinatorial explosions
of thoughts and solution strategies; ensures tractability of the comprehension
problem. Acts like the Censor and Suppressor of the framework.
It annotates solutions with <problem, process, result, reason> for storage in
AL, and annotates memories by <environment descriptors, problem, solution,
result, reason, affects, beliefs, etc.> for storage in CoN.
x. Agencies possess local critic-selector agents that gauge the effectiveness of
different algorithms to reason and choose the best option. Su monitors global
appropriateness of solution-strategies.
xi. Global FA and Log resemble the global workspace [51, 80, 81] construct of
blackboard architectures [59].
xii. Log serves as the basis of inter-agency communication, thereby grossly
reducing these costs – any message on Log is equivalent to broadcasting it
across all the agencies for reﬂection or deliberation. This instinctively implies
the use of standard formats for Log-messages for uniform comprehension
across the system.
a. Each agency has at least one critic-selector agent dedicated to the analysis
of Log entries and subsequent agency self-activation.
b. Su through Log messages - <agency, operation completed, frame-systems
handled, terminal values before operation, terminal values after operation,
questions in the mind, probable future operations, reasons> tuples -
broadcasts the current status of the interpretation
442
R. Banerjee and S.K. Pal

• The <probable future operations> symbolize hypotheses by Cr,
sub-problems identiﬁed by Re, or suggestions by Su, Se and Sy.
• The <questions in the mind, probable future operations> parameters
indicate frame-terminals with uncertain, missing slot values or inco-
herent granules of comprehension, and exogenously or endogenously
activate speciﬁc sub-agencies.
These activated agencies, execute innate algorithm trials in their local
FA space, and then through Log, ‘suggest’: (a) strategies towards the
resolution of the <probable future operations>, or (b) new operations
altogether. Su analyses this candidate solution space for the effective
mix of partial solutions for the problem.
c. Status updates and records of partial-solution in Log, allows Su to back-
track, `deliberate and reﬂect upon’ schemes in case of erroneous or
cost-ineffective choices made.
xiii. Operations activated by agencies, depends on the encoding of meanings into
frames. The local critic-selector analyses of agency-operations, as well as
global agency-suggestions are analogous to mentalese [41] in the computa-
tional mind. While Log is a manifestation of the mentalese of the computa-
tional mind, frames represent the components of system-mentalese.
xiv. Frame manipulation schemes operate seamlessly across multiple data-types
representing different sensory memories.
xv. Besides parameters that describe a fact or an event, frames include parameters
to denote the system’s belief of the world and itself. The Z-number philoso-
phy,
described
in
the
earlier
sections,
is
an
effective
scheme
for
subjective-belief symbolization, and thus is the macro-knowledge processing
element of the system.
System inputs are translated into Z-number or Z-information sets that are
processed
as
per
the
algorithm
described
in
Sect.
3.1.1.
Given
a
Z-valuation <X, A, B>, the parameters are synonymous to frame-terminal,
slot-value, and the strength in the terminal-slot_value connectivity, respec-
tively. Z-valuations summarize the information in a frame while Z-information
summarizes that in a frame-network. Frame-manipulations are typically
Z-number calculi.
xvi. The sub-agencies under De can be categorized into the following, based on the
levels of information-granules they deal with:
a. Tier 1 – Acknowledge system ‘self’; subjective decisions – Sf
b. Tier 2 – Conjecture abstract or well-deﬁned procedures for text interpre-
tation – Re, Cr, Su
c. Tier 3 – Hypothesize steps of abstract procedures; procedure-step execu-
tion – Se, Sy
On Z-numbers and the Machine-Mind …
443

3.2.2 Demonstration of the Operation of the Machine-Mind
Architecture
Table 3 presents an explicit run through the framework – depicting the stages of
comprehension and the roles of the mind-agencies. A unit of text is being com-
prehended. Components in the table abide by the following schematics:
i.
<bold> => ‘frame_header’
ii.
<italics> => ‘terminal_slot’
iii.
<bold and italics> => ‘slot_value’
iv. () => ‘frame-terminal_slot’ relation
v. Arrow heads => connectivity destinations; destinations could be ‘termi-
nal_slots’ or ‘slot_values’
Assumptions: Each of the mind-agencies and memory constructs is functional, as
per the descriptions in Sect. 3.2.1.
Input text: Smita’s eyes were ﬁreﬂies.
Expected output: Narrative(s) of comprehension.
Observations:
i. Inference results and data of one stage percolate across parallel threads of
operation (co-operating threads) as well as down to the next stage of
comprehension.
ii. Entries across time units indicate Log as well as global FA values.
iii. Activities of M have not been highlighted, as our focus was exclusively on
the emulation of comprehension – unhindered by consideration of system
optimization issues.
iv. The progression of comprehension depicted in Table 3 was validated by the
thought processes of twenty random individuals. These individuals were
asked to list – over a time period of two days – all that their minds processed
in relation to the given text input, and in the order that their thoughts were
activated. Synopsis of the survey results are illustrated in Table 4 and Fig. 5.
The Venn diagram (Fig. 5) of data in Table 4 (where U = universe of
discourse), clearly depicts the system outputs to be in line with majority
human interpretations.
We are currently in the process of performing more such experiments, so as
to understand better the average thought processes given random text
instances. [Refer to [62] for other examples of system execution.]
v. Formats of Z-valuations, with respect to agency-operations, are indicated in
Table 3.
vi. The framework is conceptually a cognitive model of text comprehension, as
it demonstrates: (a) multiple-realm ‘thinking’, (b) ambiguity resolution,
(c) recollection and reﬂection, and (d) subjective decision-making.
vii. The question of interest here is how can a machine prove to be ‘thinking’ or
behaving ‘intelligently’? Ryle states in [82], that the procedure a system uses
444
R. Banerjee and S.K. Pal

Table 3 The action dynamics of comprehension by the computational mind framework
On Z-numbers and the Machine-Mind …
445

to arrive at solutions is an indication of its intelligence; the procedures being
an amalgamation of its knowledge, intuition, commonsense, and experience.
Thus, a measure of think-ability or intelligence of a machine (MIQ [2])
requires parameters that represent the strength of the system in each of these
areas.
Considering the implication of the self-conscious facet of thinking, [83, 84]
indicate the need for effective indicators of ‘conscious’ thoughts and states of
the system. These need to incorporate both objective and subjective machine
responses. (Would a ‘conscious’, ‘thinking’, ‘understanding’ machine be
immune to consciousness disorders leading to psychiatric or neurologic
disorders or minimal conscious states?)
viii. The ultimate test of understanding is if the machine is able to utilize this
newly learnt expression in appropriate situations.
ix. As stated in [59], the key requirements of knowledge based language un-
derstander systems are:
a. Representation and structuring of the problem in a way that permits
decomposition.
b. Total interpretation is to be broken down into hypotheses and modular-
ized into different types of knowledge that can operate independently and
co-operatively.
The
framework
supports
the
conceptual
acknowledgement
of
these
requirements, in the following ways:
a. We have factored the problem of comprehension into its component
functions and assigned their execution to mind-agencies (Sect. 3.2.1).
What remains to be done is factoring the agency-functions into individual
agents – which represent and operate on natural language samples to
construct modules of comprehension.
b. Decomposition of interpretations into hypotheses and knowledge mod-
ularization is executed by:
• Re disintegrates an interpretation problem into “similar” sub-problems
and recalls known solutions.
• Cr conjectures new answers.
• Su periodically summarizes comprehension statuses that consequently
activate solution suggestions by different agencies.
• Critic-selector agents critically analyze multiple approaches towards
the realization of an agency-function.
• Global FA and Log serve as global workspaces for the agencies to
co-operate towards solutions.
• Local FA supports independent agency-activity trials, moderated by
critic-selector agents.
• The
De
agencies
(Sect.
3.2.1),
operate
across
a
number
of
information-granular levels – beginning with the syntax of a word to
its subjective perception.
446
R. Banerjee and S.K. Pal

• Analyses across agencies form pools of candidate partial solutions,
which Su combines into effective global solutions.
• Knowledge, modularized into facts, concepts, intuition, commonsense
and procedures, is referenced by agencies relative to the demands of
the status of comprehension.
Table 5 maps the processing activities, during comprehension of the given input, to
the layers (Sect. 2.3.2) of the mind (the machine-mind) they pertain to, thereby
demonstrating thinking across all the layers. [Refer to [62] for other depictions of
natural language comprehension by the deﬁned machine-mind framework.]
3.2.3 Validation of Completeness of Machine-Mind Conceptualization
• Correspondence between machine-mind agencies and the layers of the
mind
Table 6 maps the layers of the mind and the agencies that emulate them. The agency
functionalities seamlessly traverse the layers (refer to Table 5), cover all of them
and overlap multiple levels. A (*) in a cell indicates coverage of the
layer-functionality (row) by the mind-agency (column).
Table 4 Summary of human subject responses to metaphor – Smita’s eyes were ﬁreﬂies
System input – text – Smita’s eyes were ﬁreﬂies
Number of human subjects in survey – 21
No. of survey
subjects who
stated…
System
interpretation:
‘a’ only
System
interpretation:
‘b’ only
System
interpretations:
‘a’ & ‘b’
System
interpretation:
‘c’ only
1
14
3
0
Fig. 5 Venn diagram
summarizing human
responses to metaphor -
Smita’s eyes were ﬁreﬂies
On Z-numbers and the Machine-Mind …
447

• Correspondence between machine-mind agencies and parts of the human
brain
The human brain serves as our reference. In Table 7 and Table 8 we summarize the
correspondence between the different parts of the brain – that play signiﬁcant roles
in natural language comprehension and agency functionalities. Table 7 depicts the
equivalence between the human memory categories and the framework memory
constructs. Table 8 illustrates the analogy between parts of the cerebral cortex of the
human brain and the framework agencies.
The conceptualized architecture-agencies cover all known memory categories
and essential language processing centres of the human brain.
Having described our efforts towards the emulation of natural language under-
standing, the questions that loom up as major design issues are as follows. It is in
the search for answers to these issues that our research is currently directed:
i. How does a natural organism encode real-world information? Could it provide
clues
to
encoding
the
‘meaning’
of
natural
language
elements
in
machine-understandable form?
Table 5 Correspondence between comprehension activity, for the given input, and layer of mind
functionality
Layer of the
mind
Processing activity
Instinctive
• Read given text
• Updating of long-term memory constructs in response to new metaphorical
interpretation of “ﬁreﬂies”
Learned
• Syntax analysis by Sy: Smita →proper noun, ‘s →[is | was | has],
eyes →[common noun | verb], were →verb, ﬁreﬂies →common noun
• Surface semantic analysis by Se of the words in the sentence
• Re recalls properties of ‘eyes’ and ‘ﬁreﬂies’
• Re recalls Smita has eyes →Smita living (human | non-human)
Deliberative
• Su simulates visuals of “eyes were ﬁreﬂies” based on surface interpretation
as well as metaphorical sense
• Cr and Re map properties of ﬁreﬂies with that of eyes and the subjective
meaning as well => [bright | glow | shining | beautiful |…] => [happy |
excited | …]
• Se prunes irrelevant non-contextual semantic frames
• Se identiﬁes “ﬁreﬂies” being used as an adjective or metaphor
Reﬂective
Self-reﬂective
• Sf awakens affect – incomprehension (confusion); interest (new knowledge
gained)
• Sf contemplating on what could make Smita [happy | excited]? – projection
of self onto Smita
Self-conscious
Sf contemplating on what would make the system [happy | excited], what the
system considers “beautiful”, whether acquaintances remarked on the
system’s beautiful eyes, …
448
R. Banerjee and S.K. Pal

Table 6 Correspondence between computational mind-agencies and layer of mind functionalities
V
Sy
Se
Sf
Re
Cr
Su
M
Instinctive reactions:
Accept sensory input; procedural memory updating
*
*
Learned reactions:
Assign meaning to language elements (alphabets, digits, special symbols, white-spaces, punctuation,
etc.); syntax and semantic analysis;
*
*
*
*
*
*
Deliberative thinking:
Context identiﬁcation; disambiguation of word and sentence meanings; rhetoric and prosodic
analysis; analyze relevance and coherence of ﬂow of understanding; consolidate individually
understood elements into concepts; visualize scenes; deliberative memory updating
*
*
*
*
*
*
Reﬂective thinking:
Reason and optimize deliberative thinking processes; generate curiosity (questions in the
computational mind) and activate schemes to gratify the same; build cross-contextual associations
*
*
*
*
*
*
Self-reﬂective thinking:
Evaluate interest and comprehension progression; overcome cognitive biases and subsequent concept
reformation – identify new knowledge gained, clarify misconceptions
*
*
*
Self-conscious emotions:
Attachment of emotions or levels of interest and perceptions; awareness of social norms and ideals
*
*
*
Layers of 
Computational 
the mind                                                                            
mind-agencies
On Z-numbers and the Machine-Mind …
449

ii. Could scientiﬁc deﬁnitions of the ‘self’ and ‘consciousness’, provide for the
machine-self
–
resulting
in
the
emulation
of
self-reﬂection
and
self-consciousness?
iii. Encoding meta-cognition, i.e., machine-awareness of comprehension – the
autogenous arousal of the ‘Aha!’ moment of understanding
iv. The emulation of endogenous instantiation of information-certainty and
event-affects
v. Tests for machine-thinking and MIQ
vi. Devise measures of information-loss on Z-valuation compaction of frames and
degrees of comprehension
Table 7 Correspondence between computational mind memory constructs and human memory
categories
Human memory categories
Framework memory constructs
Working (temporary representations of
information on current task)
global FA; local FA of De and
M sub-agencies; AF; PF
[WS ⊆AF and is therefore not explicitly
mentioned]
Declarative (represent explicitly stored and
recalled memories)
CoN
Procedural (represent implicitly stored and
recalled memories of automatic behaviours)
ComN
Long-term (semantically encoded declarative
and procedural memories)
L; CoN; ComN; AL
Short-term (memories recalled, without
repetition, for a duration of the order of
seconds; are acoustically encoded)
First set of entries into global FA by SG
Sensory (memories of sensory stimulus after
it has ceased; is of order of milliseconds)
local FA of SG sub-agencies
Visual (visual experiences)
Olfactory (olfactory experiences)
Haptic (tactile experiences)
Taste (taste experiences)
Auditory (auditory experiences)
Memories annotated by the senses they
represent – indicated by their data-types in
ComN and CoN
Autobiographic (personal episodic
experiences)
Subset of CoN
Retrospective (action of remembering the
past)
Constructed out of ComN and CoN
[PF supports the emulation of these
memories]
Prospective (memories activated in the future
based on time and event cues)
450
R. Banerjee and S.K. Pal

4
Conclusion
This article is an elucidation on our study of the Z-number [8] approach to CWW
[3, 25] and the enumeration of high-level Society of Mind [9, 10] elements of a
machine-mind framework for natural language understanding.
The Z-numbers, as deﬁned by Zadeh, provide a basis for the precisiation of the
meaning (both objective and subjective) of natural language statements. We extend
this capability to that for sentences – deﬁne an algorithm for Z-number based CWW
and perception-operators that form complex Z-numbers. These have been used to
simulate an instance of differential diagnosis – thereby demonstrating the possible
strengths and challenges of the paradigm.
Work on the Z-numbers prompted our focusing on the synthesis of mechanisms
for
the
endogenous
instantiation
of
system-affects
and
emulation
of
system-mentalese [41]. Thus, utilizing the primary philosophy of Minsky’s Society
of Mind theories of the human mind and using the human brain as a reference, we
turned to the design of a machine-framework for natural language understanding.
This framework, currently in its early stages, is a co-operating association of
function-modules and memory elements. It conceptually emulates every aspect of
cognitive comprehension – intuition, commonsense, thinking, adapting and learn-
ing. The modiﬁed Z-numbers precisiate knowledge in the framework and allow
system-perception manipulation. The working of the architecture has been dem-
onstrated through a detailed example of metaphor comprehension. System results
have
been
validated
against
human
responses.
System-conceptualization
Table 8 Correspondence between computational mind-agencies and cerebral cortex region
functionalities
Lobe
Parts /Functions of the lobe in relation to language
understanding
Framework
agencies
Occipital
Processes visual information
V
Frontal
Broca’s area (syntax and morphology analysis)
Sy
Self deﬁnition, attention, social behaviour
Sf
Reasoning, judgment, strategic thinking
Re, Cr, Su
Parietal
Angular gyrus (language and number processing, spatial
cognition, memory retrieval, attention mediation and metaphor
comprehension [85, 86])
Se, Sf, Su,
Re, Cr
Temporal
Wernicke’s area (semantic resolution)
Se
Amygdala (affective processing and memory consolidation)
Sf, Su
Hippocampus (storage and consolidation of semantic and
episodic memories)
Su
Basal Ganglia (reinforcement learning, procedural memory –
priming and automatic behaviours or habits, eye movements
[87] and cognition [88])
Sf, Su, Re
Recognition
Re
On Z-numbers and the Machine-Mind …
451

completeness has been studied through correspondence analysis with the layers of
the mind (Minsky) and cerebral cortex regions.
We do not claim that the proposed design mimics the vast repertoire of mind
functions nor have we deﬁned every psychological process in its computational
equivalents, but we have here a set of very basic agencies and methodologies that
work in unison and harmony to realize language understanding. The concepts here
serve as a blueprint for our endeavours. Our current research initiatives are pri-
marily concentrated in deﬁning parameters that represent the dynamic machine-self,
encoding ‘meanings’ (objective and subjective) in machine-understandable form,
and the emulation of metacognition. The designed computational-mind is envisaged
to deﬁne its own self, be self-organized, dynamic, adaptable, and social – behave
like an ‘intelligent’ object [65, 66].
A cognitive system, we believe, applies to the development of ‘intelligent’ and
‘symbiotic’ man-machine interactive systems – plagiarism-checkers, library cata-
loguing systems, text summarizers, differential diagnosis systems, educational aids
for children with reading disorders, etc.
Acknowledgments This project is being carried out under the guidance of Professor Sankar K.
Pal who is an INAE Chair Professor and J.C. Bose Fellow of the Government of India.
References
1. Chomsky, N.: Linguistics and cognitive science: problems and mysteries. In: The Chomskyan
Turn, pp. 26–53. Blackwell Publishing, Oxford (1991)
2. Zadeh, L.A.: Fuzzy logic, neural networks and soft computing. Commun. ACM 37(3), 77–84
(1994)
3. Zadeh, L.A.: Fuzzy logic = computing with words. IEEE Trans. Fuzzy Syst. 4(2), 103–111
(1996)
4. Zadeh, L.A.: Outline of a new approach to the analysis of complex systems and decision
processes. IEEE Trans. Syst. Man Cybern. 3(1), 28–44 (1973)
5. Zadeh, L.A.: Test-score semantics for natural language and meaning representation via pruf.
In: Reiger, B. (ed.) Empirical Semantics, pp. 281–349. Dr. Broackmeyer University Press,
Germany (1982)
6. Zadeh, L.A.: Precisiated natural language (PNL). AI Mag. 25(3), 74–91 (1994)
7. Zadeh, L.A.: A new direction in AI: toward a computational theory of perceptions. AI Mag. 22
(1), 73–84 (2001)
8. Zadeh, L.A.: A note on Z-numbers. Inf. Sci. 181(14), 2923–2932 (2011)
9. Minsky, M.: The Society of Mind. Simon & Schuster Inc., NY (1986)
10. Minsky, M.L.: The Emotion Machine: Commonsense Thinking, Artiﬁcial Intelligence, and the
Future of the Human Mind. Simon and Schuster Inc, New York (2006)
11. Price, C.J.: The anatomy of language: contributions from functional neuroimaging. J. Anat.
197, 335–359 (2000)
12. Ramachandran, V., Blakeslee, S.: Phantoms in the Brain: Probing the Mysteries of the Human
Mind. William Morrow and Company (Harper Collins), NY (1999)
13. Bobrow, D.G.: Natural language input for a computer problem solving system. Ph.D.
dissertation, Massachusetts Institute of Technology (1964)
14. Winston,
P.H.:
Learning
structural
descriptions
from
examples.
Ph.D.
dissertation,
Massachusetts Institute of Technology (1970)
452
R. Banerjee and S.K. Pal

15. Winograd, T.: Procedures as a representation of data in a computer program for understanding
natural language. Ph.D. dissertation, Massachusetts Institute of Technology (1971)
16. Charniak, E.: Toward a model of children’s story comprehension. MIT Artiﬁcial Intelligence
Laboratory, Tech. Rep. (1972)
17. Sussman, G.J.: A computational model of skill acquisition. Ph.D. dissertation, Massachusetts
Institute of Technology (1973)
18. H. Liu, MontyLingua: An end-to-end natural language processor with common sense, Std.
web.media.mit.edu/-∼hugo/-montylingua (2004)
19. Ferrucci, D., Brown, E., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur, A.A., Lally, A.,
Murdock, J.W., Nyberg, E., Prager, J., Schlaefer, N., Welty, C.: Building Watson: An
overview of the DeepQA project. AI Mag. 31(3), 59–78 (2010)
20. Turing, A.M.: Computing machinery and intelligence. Mind 49, 433–460 (1950)
21. Bush, V.: As we may think. Atlantic Mon. http://www.theatlantic.com/magazine/archive/
1945/07/as-we-may-think/303881/ (1945)
22. Zhang, Z., Franklin, S., Dasgupta, D.: Metacognition in software agents using classiﬁer
systems. In: Mostow, J., Rich, C. (eds.) Proceedings of the Fifteenth National Conference on
Artiﬁcial Intelligence and Tenth Innovative Applications of Artiﬁcial Intelligence Conference,
pp. 83–88. AAAI Press, CA (1998)
23. McCauley, L., Franklin, S., Bogner, M.: An emotion-based “conscious” software agent
architecture. In: Paiva, A. (ed.) Affective Interactions. Lecture Notes on Artiﬁcial Intelligence,
vol. 1814. Springer (2000)
24. Franklin, S.: Ida: a conscious artifact? J. Conscious. Stud. 10, 47–66 (2003)
25. Zadeh, L.A.: From computing with numbers to computing with words—from manipulation of
measurements to manipulation of perceptions. IEEE Trans. Circuits Syst. 45(1), 105–119 (1999)
26. Mendel, J.M., Zadeh, L.A., Trillas, E., Yager, R., Lawry, J., Hagras, H., Guadarrama, S.: What
computing with words means to me? IEEE Comput. Intell. Mag. 5(1), 20–26 (2010)
27. Banerjee, R., Pal, S.K.: The Z-number enigma: a study through an experiment. In: Yager, R.
R., Abbasov, A.M., Reformat, M.R., Shahbazova, S.N. (eds.) Soft Computing: State of the Art
Theory and Novel Applications. Studies in Fuzziness and Soft Computing, vol. 291,
pp. 71–88. Springer, Berlin (2013)
28. Pal, S.K., Banerjee, R., Dutta, S., Sarma, S.S.: An insight into the Z-number approach to
CWW. Fundamenta Informaticae 124(1–2), 197–229 (2013)
29. Delgado, M., Duarte, O., Requena, I.: An arithmetic approach for the computing with words
paradigm. Int. J. Intell. Syst. 21, 121–142 (2006)
30. Lawry, J.: A methodology for computing with words. Int. J. Approximate Reasoning 28,
55–89 (2001)
31. Trivino, G., Vander-Heide, A.: An experiment on the description of sequences of fuzzy
perceptions. In: Proceedings of the 8th International Conference on Hybrid Intelligent
Systems, Barcelona, Spain, pp. 228–233. IEEE Press, NJ (2008)
32. Vander-Heide, A., Trivino, G.: Simulating emotional personality in human computer interface.
In: Proceedings of the IEEE International Conference on Fuzzy Systems, Barcelona, Spain,
pp. 1–7. IEEE Press, NJ (2010)
33. Mendel, J.M., Wu, D.: Perceptual computing—Aiding people in making subjective
judgements. IEEE Press, NJ (2010)
34. Reformat, M., Ly, C.: Ontological approach to development of computing with words based
systems. Int. J. Approximate Reasoning 50, 72–91 (2009)
35. Khorasani, E.S., Patel, P., Rahimi, S., Houle, D.: CWJess: Implementation of an expert system
shell for computing with words. In: Proceedings of Federated Conference on Computer
Science and Information Systems, pp. 33–39. IEEE Press, NJ (2011)
36. Khorasani, E., Patel, P., Rahimi, S., Houle, D.: An inference engine toolkit for computing with
words. J. Ambient Intell. Humaniz. Comput. 4(4), 451–470 (2013)
37. Licklider, J.C.R.: Man-computer symbiosis. In: IRE Transactions on Human Factors in
Electronics, vol. HFE-1, pp. 4–11 (1960)
38. Kurzweil, R.: The Singularity is Near. Viking (Penguin Group), NY (2005)
On Z-numbers and the Machine-Mind …
453

39. Zadeh, L.A.: Intelligent systems revolution: Is it real? http://www-bisc.cs.berkeley.edu/zadeh/
papers (1998)
40. Pal, S.K., Banerjee, R.: Context-granulation and subjective information quantiﬁcation.
Theoret. Comput. Sci. 448, 2–14 (2013)
41. Pinker, S.: How the Mind Works. W. W. Norton & Company, NY (1997)
42. Kacprzyk, J., Zadrozny, S.: Computing with words is an implementable paradigm: Fuzzy
queries, linguistic data summaries and natural language generation. IEEE Trans. Fuzzy Syst.
18, 461–472 (2010)
43. Alieva, R., Alizadeh, A., Huseynovd, O.: The arithmetic of discrete Z-numbers. Inf. Sci. 290,
134–155 (2014)
44. Lieberman, H., Liu, H., Singh, P., Barry, B.: Beating common sense into interactive
applications. AI Mag. 25(4), 63–76 (2004)
45. McCarthy, J.: Programs with commonsense. In: Minsky, M. (ed.) Semantic Information
Processing, pp. 403–418. MIT Press, MA (1968)
46. Minsky, M.: Commonsense based interfaces. Commun. ACM 43(8), 67–73 (2000)
47. Singh, P., Barry, B., Liu, H.: Teaching machines about everyday life. BT Technol. J. 22(4)
(2004)
48. Singh, P., Minsky, M., Eslick, I.: Computing commonsense. BT Technol. J. 22(4), 201–210
(2004)
49. Turing, A.: Intelligent machinery http://www.alanturing.net/turing_archive/archive/l/-l32/L32-
011.html (1949)
50. Singh, P.: Examining the society of mind. Comput. Inform. 22(6), 521–543 (2003)
51. Baars, B.J.: A Cognitive Theory of Consciousness. Cambridge University Press, Cambridge
(1988)
52. Kokinov, B.: About modelling some aspects of human memory. In: Man-Computer Interaction
Research (MACINTER-II), pp. 349–359. Elsevier, Amsterdam (1989)
53. Kokinov, B.N.: The dual cognitive architecture: a hybrid multi-agent approach. In: Conn, A.
(ed.)
Proceedings
of
11th
European
Conference
on
Artiﬁcial
Intelligence
(ECAI),
pp. 203–207. Wiley, NJ (1994)
54. Majumdar, A., Sowa, J., Stewart, J.: Pursuing the goal of language understanding. In:
Proceedings of the 16th International Conference on Conceptual Structures: Knowledge
Visualization and Reasoning, pp. 21–42. Springer, Berlin (2008)
55. Singh, P.: Em-one: an architecture for reﬂective commonsense thinking. Ph.D. dissertation,
Massachusetts Institute of Technology, June 2005
56. Morgan, B.: Funk2: a distributed processing language for reﬂective tracing of a large
critic-selector cognitive architecture. In: Proceedings of the Fourth IEEE International
Conference
on
Self-Adaptive
and
Self-Organizing
Systems
Workshop
(SASOW),
pp. 269–274. IEEE Computer Society, CA (2010)
57. Morgan, B.: A substrate for accountable layered systems. Ph.D. dissertation, Program in
Media Arts and Sciences, School of Architecture and Planning, Massachusetts Institute of
Technology (2013)
58. Backus, J.: Can programming be liberated from the von neumann style? A functional style and
its algebra of programs (ACM Turing Award Lecture). Commun. ACM 21(8), 613–641 (1978)
59. Erman,
L.D.,
Hayes-Roth,
F.,
Lesser,
V.R.,
Reddy,
D.R.:
The
Hearsay-II
speech-understanding system: Integrating knowledge to resolve uncertainty. ACM Comput.
Surv. 12(2), 213–253 (1980)
60. Harrison, H., Minsky, M.: Turing Option (Unpublished chapters—25B, 26B). Warner Books
(1992)
61. Sloman, A.: Towards a computational theory of mind. In: Artiﬁcial Intelligence—Human
Effects, pp. 173–182. Ellis Horwood, UK (1984)
62. Banerjee, R., Pal, S.K.: Text comprehension and the computational mind-agencies. Nat.
Comput. (2015). doi:10.1007/s11047-014-9478-x
63. Roy, A.: Artiﬁcial Neural Networks—A science in trouble. ACM SIGKDD Explor. Newslett.
1(2), 33–38 (2000)
454
R. Banerjee and S.K. Pal

64. Maes, P.: Concepts and experiments in computational reﬂection. In: Proceedings of the
Conference on Object-oriented programming systems, languages and applications (OOPSLA),
pp. 147–155 (1987)
65. McCarthy, J.: Making robots conscious of their mental states. In: Machine Intelligence,
pp. 3–17. Oxford University Press, Oxford (1995)
66. McCarthy, J.: The well-designed child. Artif. Intell. 172(18), 2003–2014 (2008)
67. Ashby, W.: Design for a Brain. Butler and Tanner Ltd., London (1952)
68. Chugani, H.T., Behen, M.E., Muzik, O., Juhász, C., Nagy, F., Chugani, D.C.: Local brain
functional activity following early deprivation: a study of post institutionalized Romanian
orphans. NeuroImage 14(6), 1290–1301 (2001)
69. Singh, P. A preliminary collection of reﬂective critics for layered agent architectures. In:
Proceedings of the Safe Agents Workshop (AAMAS). http://web.media.mit.edu/∼push/
ReﬂectiveCritics.pdf (2003)
70. Singh, P., Minsky, M.: An architecture for combining ways to think. In: Proceedings of the
International Conference on Integration of Knowledge Intensive Multi-Agent Systems,
pp. 669–674 (2003)
71. Minsky, M.: A framework for representing knowledge. In: The Psychology of Computer
Vision, pp. 211–277. McGraw-Hill, NY (1975)
72. Singh, P., Minsky, M.: An architecture for cognitive diversity. In: Visions of Mind:
Architectures for Cognition and Affect, pp. 312–331. Information Science Publishing,
Hershey, PA (2005)
73. Pal, S.K., Dasgupta, A.: Spectral fuzzy sets and soft thresholding. Inf. Sci. 65, 65–97 (1992)
74. Husserl, E.: Logical Investigations (Translated from German). Routledge and Kegan Paul Ltd,
London (1970)
75. Ariely, D.: Predictably Irrational: The Hidden Forces that Shape our Decisions. Harper
Collins, NY (2008)
76. Harley, T.A.: The Psychology of Language: From Data to Theory, 3rd edn. Psychology Press
—Taylor and Francis, NY (2008)
77. Robinson, S.K., Aronica, L.: Finding Your Element: How to Discover Your Talents and
Passions and Transform Your Life. Viking (Penguin Group), NY (2013)
78. Stallman, R.M., Sussman, G.J.: Forward reasoning and dependency-directed backtracking in a
system for computer-aided circuit analysis. Artif. Intell. 9, 135–196 (1977)
79. Havasi, C., Speer, R., Alonso, J.: Conceptnet 3: a ﬂexible, multilingual semantic network for
common sense knowledge. In: Proceedings of the Recent Advances in Natural Language
Processing. http://web.media.mit.edu/∼jalonso/cnet3.pdf (2007)
80. Baars, B.J.: In the Theater of Consciousness: The Workspace of the Mind. Oxford University
Press, Oxford (1997)
81. Baars, B.J.: The conscious access hypothesis: origins and recent evidence. Trends in Cogn.
Sci. 6(1), 47–52 (2002)
82. Ryle, G.: The Concept of Mind. University of Chicago Press, IL (1949)
83. Seth, A.K.: The grand challenge of consciousness (opinion article). Front. Psychol. 1(5), 1–2
(2010)
84. Seth, A.K., Izhikevich, E., Reeke, G.N., Edelman, G.M.: Theories and measures of
consciousness: an extended framework. Proceeding of the National Academy of Sciences
(PNAS) 103(28), 10799–10804 (2006)
85. Ramachandran, V., Hubbard, E.: Neural cross wiring and synesthesia. J. Vis. 1(3). http://www.
imprint.co.uk/pdf/R_H-follow-up.pdf (2001)
86. Ramachandran, V., Hubbard, E.: The phenomenology of synaesthesia. J. Conscious. Stud. 10
(8), 49–57 (2003)
87. Hikosaka, O., Takikawa, Y., Kawagoe, R.: Role of the basal ganglia in the control of
purposive saccadic eye movements. Physiol. Rev. 80(3), 953–978 (2000)
88. Stocco, A., Lebiere, C., Anderson, J.: Conditional routing of information to the cortex: a model
of the basal ganglia’s role in cognitive coordination. Psychol. Rev. 117(2), 541–574 (2010)
On Z-numbers and the Machine-Mind …
455

Authors Biography
Romi Banerjee is a Senior Research Fellow at the Center for
Soft Computing Research (CSCR) – a National Facility at the
Indian Statistical Institute, Kolkata, India. She received a B.Sc
degree, a B.Tech degree and an M.Tech degree in computer
science and engineering from the University in Calcutta, in 2007,
2010 and 2012, respectively.
She has been working under the tutelage of Professor
Sankar K. Pal since 2011 – ﬁrst as an M.Tech project trainee
and then as a full-time research student. Besides cognitive
machines and artiﬁcial general intelligence, she is deeply
interested in developmental cognitive neuroscience and the
philosophy of the mind.
Miss Banerjee is a student member of the Association for the
Advancement of Artiﬁcial Intelligence (AAAI), member of
Women in Cognitive Science (WiCS) and the Cognitive Science Society (CSS).
Sankar K. Pal (www.isical.ac.in/∼sankar) is a Distinguished
Scientist of the Indian Statistical Institute and its former Direc-
tor. He is also a J.C. Bose Fellow of the Govt. of India and INAE
Chair Professor. He founded the Machine Intelligence Unit and
the Center for Soft Computing Research: A National Facility in
the Institute in Calcutta. He received a Ph.D. in Radio Physics
and Electronics from the University of Calcutta in 1979, and
another Ph.D. in Electrical Engineering along with DIC from
Imperial College, University of London in 1982. He joined his
Institute in 1975 as a CSIR Senior Research Fellow where he
became a Full Professor in 1987, Distinguished Scientist in 1998
and the Director for the term 2005–10.
He worked at the University of California, Berkeley and the
University of Maryland, College Park in 1986–87; the NASA
Johnson Space Center, Houston, Texas in 1990–92 & 1994; and
in US Naval Research Laboratory, Washington DC in 2004. Since 1997 he has been serving as a
Distinguished Visitor of IEEE Computer Society (USA) for the Asia-Paciﬁc Region, and held
several visiting positions in Italy, Poland, Hong Kong and Australian universities.
Prof. Pal is a Fellow of the IEEE, the World Academy of Sciences (TWAS) - for the
Advancement of Science in Developing Countries, International Association for Pattern recog-
nition, International Association of Fuzzy Systems, and all the four National Academies for
Science/Engineering in India. He is a co-author of seventeen books and more than four hundred
research publications in the areas of Pattern Recognition and Machine Learning, Image Processing,
Data Mining and Web Intelligence, Soft Computing, Neural Nets, Genetic Algorithms, Fuzzy Sets,
Rough Sets and Bioinformatics. He visited about forty countries as a Keynote/ Invited speaker or
an academic visitor.
He has received the 1990 S.S. Bhatnagar Prize (which is the most coveted award for a scientist
in India), 2013 Padma Shri (one of the highest civilian awards) by the President of India and many
prestigious awards in India and abroad including the 1999 G.D. Birla Award, 1998 Om Bhasin
Award, 1993 Jawaharlal Nehru Fellowship, 2000 Khwarizmi International Award from the
President of Iran, 2000–2001 FICCI Award, 1993 Vikram Sarabhai Research Award, 1993 NASA
Tech Brief Award (USA), 1994 IEEE Trans. Neural Networks Outstanding Paper Award, 1995
NASA Patent Application Award (USA), 1997 ITE-R.L. Wadhwa Gold Medal, 2001 INCA-S.H.
456
R. Banerjee and S.K. Pal

Zaheer Medal, 2005–06 Indian Science Congress-P.C. Mahalanobis Birth Centenary Gold Medal
from the Prime Minister of India for Lifetime Achievement, 2007 J.C. Bose Fellowship of the
Government of India, and 2013 Indian National Academy of Engineering (INAE) Chair Profes-
sorship.
Prof. Pal is/ was an Associate Editor of IEEE Trans. Pattern Analysis and Machine Intelligence
(2002–06), IEEE Trans. Neural Networks [1994–98 & 2003–06], Neurocomputing (1995–2005),
Pattern Recognition Letters (1993–2011), Int. J. Pattern Recognition & Artiﬁcial Intelligence,
Applied Intelligence, Information Sciences, Fuzzy Sets and Systems, Fundamenta Informaticae,
LNCS Trans. Rough Sets, Int. J. Computational Intelligence and Applications, IET Image Pro-
cessing, and J. Intelligent Information Systems; Editor-in-Chief, Int. J. Signal Processing, Image
Processing and Pattern Recognition; a Book Series Editor, Frontiers in Artiﬁcial Intelligence and
Applications, IOS Press, and Statistical Science and Interdisciplinary Research, World Scientiﬁc; a
Member, Executive Advisory Editorial Board, IEEE Trans. Fuzzy Systems, Int. Journal on Image
and Graphics, and Int. Journal of Approximate Reasoning; and a Guest Editor of IEEE Computer,
IEEE SMC and Theoretical Computer Science.
On Z-numbers and the Machine-Mind …
457

Evolutionary Reduction of Fuzzy
Rule-Based Models
Witold Pedrycz, Kuwen Li and Marek Reformat
Abstract In the design of fuzzy rule-based models we strive to develop models
that are both accurate and interpretable (transparent). The approach proposed here is
aimed at the enhancement of transparency of the fuzzy model already constructed
with the accuracy criterion in mind by proposing two modiﬁcations to the rules.
First, we introduce a mechanism of reduction of the input space by eliminating
some less essential input variables. This results in rules with the reduced subspaces
of input variables making the rules more transparent. The second approach is
concerned with an isolation of input variables: fuzzy sets deﬁned in the n-dimen-
sional input space and forming the condition part of the rules are subject to a
decomposition process in which some variables are isolated and interpreted sepa-
rately. The reduced dimensionality of the input subspaces in the ﬁrst approach and
the number of isolated input variables in the second one are the essential parameters
controlling impact of enhanced transparency on the accuracy of the obtained fuzzy
model. The two problems identiﬁed above are of combinatorial character and the
optimization tasks emerging there are handled with the use of Genetic Algorithms
(GAs). A series of numeric experiments is reported where we demonstrate the
effectiveness of the two approaches and quantify the relationships between the
criterion of accuracy and interpretability.
W. Pedrycz (✉) ⋅K. Li ⋅M. Reformat
Department of Electrical and Computer Engineering, University of Alberta,
Edmonton, AB T6R 2V4, Canada
e-mail: wpedrycz@ualberta.ca
K. Li
e-mail: kuwen@ualberta.ca
M. Reformat
e-mail: reformat@ualberta.ca
W. Pedrycz
Faculty of Engineering, Department of Electrical and Computer Engineering,
King Abdulaziz University, Jeddah 21589, Saudi Arabia
W. Pedrycz
Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_23
459

Keywords Rule-based models ⋅Interpretability ⋅Reduction ⋅Isolation of
variables ⋅Fuzzy clustering ⋅Curse of dimensionality
1
Introductory Notes
In the construction of fuzzy rule based systems, we have been witnessing a wealth of
design strategies and detailed algorithms involving the technology of Evolutionary
Computing and neurocomputing. Just recent developments reported in this realm can
be found in a series of studies [1–3, 5, 6, 8]. Predominantly, the development of
fuzzy models is guided by the criterion of accuracy. Another fundamental criterion
being at the heart of fuzzy modeling is interpretability (transparency) of resulting
fuzzy models. This criterion is central to fuzzy models however its multifaceted
nature requires a thorough formulation and a detailed quantiﬁcation of essential
aspects of interpretability. Subsequently, it calls for engaging advanced optimization
techniques supporting the realization of the ensuing design.
The concept of interpretability of fuzzy rule-based models has been around for
several decades and attracted a signiﬁcant deal of attention. The transparency offuzzy
models is one of the outstanding and important features offuzzy models. In contrast to
the criterion of accuracy, whose quantiﬁcation is relatively straightforward and easy
to come up with performance indexes, transparency of fuzzy rules is more difﬁcult to
describe. What makes the fuzzy rule-based easier to interpret and comprehend is still
an open issue. It is quite subjective to assess and in one way or another invokes a
factor of subjective judgment given that a human user is ultimately involved in the
evaluation process. What also becomes apparent, is a multifaceted nature of the
problem and a multitude of various approaches supported by various optimization
technologies including evolutionary optimization. When it comes to the main factors
worth considering when discussing a concept of interpretability, we can enumerate a
list of factors that may be involved in the reduction process:
• number of rules forming a rule base of the model,
• number of sub-conditions (input variables) forming a condition part of a given
rule,
• number or rules and the number of input variables,
• complexity of local regression models forming the conclusion part of the rules
(in case of Takagi-Sugeno model)
• interpretability of a family of fuzzy sets formed in the input space for individual
variables.
As a result, given this diversity of possible ways of reduction of rules, it is
difﬁcult to quantify the effect of reduction. For instance, it is not always clear if it
would be better to have a larger number of simple rules (whose condition parts are
linear functions) or a smaller number of rules of a more complex conclusion parts
(say, those of polynomial form).
460
W. Pedrycz et al.

The reader may refer to a large body of studies devoted to the issue of inter-
pretability, cf. [3, 10]. Quite often, given the combinatorial nature of the reduction
problems, Genetic Algorithms are used in the optimization process, see [9]. There
are also more specialized algorithmic vehicles to support the reduction process such
as e.g., singular value decomposition [14] however one has to be cognizant as to the
interpretability of the reduced rules.
As to the overall strategy of rule reduction and interpretability enhancement,
there are two general design strategies: either the reduction is completed once the
model has been constructed or the reduction mechanism of rule-based modeling is
incorporated into the design process from its very beginning.
The objective of the study is to pursue a fundamental issue of building more
transparent and user-centric fuzzy rule-based models by starting from an already
designed fuzzy model. The intent is to make the rules more readable in two different
ways: (a) by reducing the input space (the number of antecedents) of the individual
rules, and (b) by isolating input variables completed for the input variables treated
en block in the condition parts of the rules.
In both these fundamental scenarios we can establish and quantify a tradeoff
between a gradual reduction of accuracy (which is inevitable when realizing any of
the reduction mechanisms of the rules and enhancing its intensity) and the increased
interpretability of the rules. The presentation of the material is structured in the
following way. We brieﬂy revisit the essentials of Takagi-Sugeno rule- based
systems, stressing the proposed design approach in which we use fuzzy clustering
(Sect. 2). The reduction of input space and a formation of input subspaces for each
rule is introduced in Sect. 3; in the same section we also present an optimization
process realized with the use of genetic algorithm. The second approach to the
enhancement of the interpretability of the rule – an isolation of input variables is
discussed in Sect. 5. Detailed experimental studies are reported in Sects. 3 and 6.
In the study, we experiment with a number of publicly available datasets coming
from eight datasets from UCI Machine learning repository and DELVE repository;
their main characteristics are listed in Table 1.
Table 1 Main characteristics of datasets used in the experiments (number of data and
dimensionality –number of input variables)
Dataset
Number of
input variables
Number
of data
Origin of the data
Abalone
8
4177
UCI Machine learning repository (http://
archive.ics.uci.edu/ml/)
Auto MPG
7
392
UCI Machine learning repository
Boston Housing
13
506
UCI Machine learning repository
Computer Activity
21
8,192
DELVE repository (http://www.dcc.fc.up.pt/
∼ltorgo/Regression/DataSets.html)
Concrete strength
8
1030
UCI Machine learning repository
Forest ﬁres
12
517
UCI Machine learning repository
Red wine quality
11
1599
UCI Machine learning repository
White wine
quality
11
4898
UCI Machine learning repository
Evolutionary Reduction of Fuzzy Rule-Based Models
461

2
Fuzzy Rule-Based Models: An Overview and Main
Design Issues
Our point of departure of the reduction processes of the rules is a “standard”
Takagi-Sugeno fuzzy model comprising c rules coming in the following form
−if x is Ai then y = fi x, ai
ð
Þ
ð1Þ
for i = 1, 2, .., c where x ∈Rn. Ai is a fuzzy set deﬁned in the n-dimensional
space while fi is the corresponding local model (linear or nonlinear) endowed with
its parameters ai forming the conclusion part of the ith rule. The design of such
models is well reported in the literature and as usual consists of the two main steps,
namely (a) a construction of condition parts (through clustering of input data done
in the input space) and (b) estimating parameters of the linear models (which leads
to the problem of linear regression). The number of rules (c) is determined by
monitoring the behavior of the model on the training and testing data. The rules in
the form (1) come with several essential properties. The condition part is a fuzzy set
expressed in Rn making the rules concise, which helps avoid a curse of dimen-
sionality we are commonly faced with in rule based systems with a higher number
of input variables. In the case of treating all input variables at the same time, the
number of rules becomes small (c) and the rule base itself is compact. Unfortu-
nately, the interpretability could be negatively impacted as no individual variables
in the condition part are treated and visualized separately.
When it comes to the quantiﬁcation of the accuracy of the fuzzy model (1) its
performance is commonly expressed by the RMSE index computed for the training
set
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
N ∑N
k = 1ðFM(xkÞ −targetkÞ2
r
ð2Þ
where N denotes the number of data in the training set. In the same way, quantiﬁed
is the performance of the constructed model on the testing data (consisting of
M data points)
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
M ∑M
k = 1ðFM(xkÞ −targetkÞ2
r
ð3Þ
Proceeding with the data summarized in Table 1, the performance of the cor-
responding models visualized versus the number of rules is illustrated in a series of
ﬁgures shown below, Fig. 1. The results are reported both for the training and
testing data. In the design, we use a standard version of the Fuzzy C-Means (FCM)
[4] with the fuzziﬁcation coefﬁcient (m) set to 2.0. The algorithm was run for 10
iterations (more speciﬁcally, we completed 10 runs with different splits of data into
training/testing data)
462
W. Pedrycz et al.

2
4
6
8
10
12
14
16
18
20
1.8
1.9
2
2.1
2.2
2.3
2.4
2.5
#rules
RMSE
Training data
Testing data
2
4
6
8
10
12
14
16
18
20
0
10
20
30
40
50
60
70
80
#rules
RMSE
Training data
Testing data
(a) Abalone                                                          (b) Auto MPG
2
4
6
8
10
12
14
16
18
20
0
5
10
15
20
25
30
35
40
45
#rules
RMSE
Training data
Testing data
2
4
6
8
10
12
14
16
18
20
1.5
2
2.5
3
3.5
4
4.5
5
5.5
#rules
RMSE
Training data
Testing data
(c) Boston Housing                                         (d) Computer Activity
2
4
6
8
10
12
14
16
18
20
0
5
10
15
20
25
30
35
40
45
#rules
RMSE
Training data
Testing data
2
4
6
8
10
12
14
16
18
20
40
60
80
100
120
140
160
180
200
#rules
RMSE
Training data
Testing data
(e) Concrete Strength                         
(f) Forest Fires
2
4
6
8
10
12
14
16
18
20
0
0.5
1
1.5
2
2.5
3
3.5
4
#rules
RMSE
Training data
Testing data
2
4
6
8
10
12
14
16
18
20
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
#rules
RMSE
Training data
Testing data
(g) Red Wine Quality                                  (h) White Wine Quality
Fig. 1 Performance of fuzzy models versus the number of rules reported for the training and
testing data for datasets (Table 1)
Evolutionary Reduction of Fuzzy Rule-Based Models
463

For the training sets, there is a general tendency of having lower values of the
RMSE with the increase of the number of rules. The performance reported on the
testing sets points at the memorization effect where the some models tend to lose
their generalization capabilities. By eyeballing these plots, we choose a suitable
number of rules (clusters) where sound approximation abilities come hand in hand
with the generalization of the models. The values selected in this way are collected
in Table 2.
3
Reduction of Input Subspaces in Rule-Based Models
The essence of the enhancement of interpretability of the rules is accomplished by
reducing the number of input variables standing in the condition parts of the rules.
The reduced rules are concisely described in the form
−if x is Ai
½
Xi then y = fiðx, aiÞ
ð4Þ
where the symbol []Xi stresses the fact that the fuzzy set Ai is now effectively
conﬁned to the reduced input space Xi ⊂Rn where some original input variables
have been removed. In other words, dim (Xi) = ni < n.
The computing of the activation level of Ai positioned in this new reduced space
is realized as follows
Ai
½
Xi x
ð Þ = 1=∑c
j = 1ð
. x −vi


ℵi
. x −vj



ℵi
Þ
2
m −1,
ð5Þ
for i = 1,2,.., c; m > 1 where the computations of the distance are realized in the
reduced input space Xi.
The reduction of the rules can be quantiﬁed in terms of a reduction factor ν,
which relates with the number n*c (expressing an overall number of variables
across all the rules) in the following way
Table 2 Number of rules of
fuzzy models constructed for
the corresponding data
Data name
Selected number of rules
Abalone
7
Auto MPG
6
Boston Housing
6
Computer Activity
6
Concrete strength
4
Forest ﬁres
4
Red wine quality
9
White wine quality
5
464
W. Pedrycz et al.

p = n n*c
ð
Þ.
ð6Þ
where p represents a reduced number of input variables used in c rules.
The reduction of the input variables existing in the new, more interpretable rules
is done via engaging the optimization capabilities of Genetic Algorithms (GAs).
More speciﬁcally, we optimize a matrix of allocation of input variables W = [wij]
with c rows and n input variables. The p largest entries of W are selected giving rise
to a binary 0–1 matrix. The p entries with the largest values are set to 1 while the
remaining ones are suppressed to zero. Each row of the matrix formed in this way
identiﬁes the variables to be used in the corresponding reduced rule. If all entries of
the ith row of W are equal to zero, this entails that the corresponding rule does not
exist in the reduced set of rules.
The process of identifying which input variables should be kept in the ante-
cedents of rules is translated into an optimization problem. Its solution is obtained
through evolutionary optimization, namely Genetic Algorithm (GA) [7]. GA uses
elements of natural selection to determine the best solution to a problem by min-
imizing a certain ﬁtness function capturing the essence of the optimization problem.
The best solution is obtained via selecting and modifying a population of potential
solutions (chromosomes). A main ﬂow of GA computing is outlined in Table 3.
Table 3 A ﬂow of
optimization realized by
genetic algorithm
Parameters of genetic algorithm:
iter – number of generations
Z – size of population
pc – crossover rate
pm – mutation rate
Algorithm
1 Initialization:
2 Random generation of chromosomes ck using uniform
distribution in [0,1]), for k = 1, 2,…, Z
3 Iterate (
4 Calculate the ﬁtness value for each chromosome ck
5 Select the candidates for the next generation based on their
ﬁtness values
6 Retain the overall best chromosome so far, say cbest
7 Apply the crossover and mutation operations based on the
crossover (pc) and mutation (pm) rates
8 Until the number of iterations does not exceed the
predetermined limit, iter*
Evolutionary Reduction of Fuzzy Rule-Based Models
465

The two aspects of GA that require special attention and directly impact the
quality of results of the optimization process are: 1) construction of a suitable ﬁtness
function, and 2) mapping a solution to the problem onto a structure of the
chromosome.
A ﬁtness function is used to evaluate possible solutions. Fitness values associ-
ated with solutions (chromosomes) represent their ability to solve a given problem.
The values are used during selection of chromosomes for further processing
(Table 3, line 5). The ﬁtness function has to reﬂect the objective of optimization
process and ensure that the obtained ﬁtness values are adequate for mechanisms of
selection, i.e., the ﬁtness values should be such that a selection process leads to a
diversiﬁed set of potential solutions [11]. In the case of our problem of selecting
input attributes that should be kept in the rules, the ﬁtness function evaluates quality
of fuzzy rules that model a given dataset. The form of the ﬁtness function used in
the optimization is given by (2).
As noted earlier, each chromosome represents a solution to the considered
problem. It consists of simple elements, called genes, that can assume values 1 or 0
(Binary Coded GA), or any real numbers from a speciﬁed range (Real Coded GA).
A chromosome with such a simple structure, called genotype, has to represent a
solution to the problem, i.e., it should be mapped into a form called phenotype that
is adequate for a given problem domain.
For the optimization problem considered in the paper, we use a Real Coded GA
(RCGA). The size of chromosome, i.e., a number of genes gsize, is equal to the
product of a number of rules c and a number of input attributes n of a given dataset:
gsize = cn. Therefore, each chromosome contains information which input attributes
are present in each rule. A single gene is a real number between 0 and 1. The
translation of such a chromosome (genotype) into a solution to our problem
(phenotype) occurs in the following way. The user has to determine a number p of
input attributes or isolated attributes (Sect. 5) that should be kept in all c rules. This
means that among all genes of a chromosome only p of them should be used to
construct c rules. The selection of these genes is done via sorting all genes based on
their values (between 0 and 1) and identifying the ﬁrst p of them. The process is
presented in Fig. 2.
Once an initial population of chromosomes ck has been generated, RCGA starts
an iteration process. A single iteration consists of calculating ﬁtness values
(Table 3, line 4), selecting best chromosomes (line 6), and performing crossover
and mutation operations on selected chromosomes (line 7). All this constitutes a
single generation. After executing a number of generations, GA provides an optimal
solution in a form of a chromosome that gives the highest value of the ﬁtness
function.
466
W. Pedrycz et al.

The purpose of crossover is to exchange information between chromosomes. In
its simplest form it means exchange of genes between a pair of randomly selected,
with the probability pc given by the user, chromosomes. For our RCGA, a linear
crossover is applied [13]. The linear crossover generates three offspring (new
chromosomes): Ok = ðok
1, . . . , ok
i , . . . ok
nÞ, k = 1, 2, 3, where ok
i represents the ith
gene of the kth chromosome. The genes of offspring are built from the genes of two
parents. Let us assume that the parents are: Ck = ðck
1, . . . , ck
i , . . . ck
nÞ, k = 1, 2. The
values of genes of the offspring are calculated in the following way:
o1
i = 1
2 c1
i + 1
2 c2
i , o2
i = 3
2 c1
i −1
2 c2
i and o3
i = −1
2 c1
i + 3
2 c2
i
ð7Þ
Each of the offspring is evaluated, i.e., a ﬁtness value is determined for each of
them. The two most promising offspring are selected to substitute their two parents
in the population.
The mutation operator modiﬁes genes of a single chromosome in order to
introduce diversity to the population. The frequency of modiﬁcations is controlled
by the mutation probability pm provided by the user. Mutation ensures that a search
process covers the whole space of possible solutions. In the case of RCGA, the
Fig. 2 GA chromosome: (i) original matrix of rules, (ii) a single chromosome built from the
whole matrix, (iii) rules built based on this chromosome (there are p black boxes representing
attributes selected for c rules)
Evolutionary Reduction of Fuzzy Rule-Based Models
467

Muhlenbein’s mutation is adopted [12]. For a given parent chromosome
Ck = ðc1, . . . , ci, . . . cnÞ, the gene values of a new – mutated – chromosome are
calculated as c′
i = ci±rangi∙g, where rangi deﬁnes the mutation range, and it is
normally set to 0. 1*ðbi −aiÞ, where bi and ai are the maximum and the minimum of
ci, respectively. The sign + or −is chosen with a probability of 0.5, while
g = ∑15
k = 0ak2 −k, ak ∈f0, 1g is randomly generated with pðai = 1Þ = 1
16 , i = 0. 15.
The operations on a single chromosome are implemented in such a way that at
least one input attribute is kept in each rule.
For reduction of input spaces, if p is a number of input attributes to be kept, the
top p genes will be used to calculate the model output. The rest gsize-p attributes
will be omitted in the calculation.
For isolation of attributes (Sect. 5), if p is a number of isolated attributes in each
rule, the top p attributes in each rule (row of the matrix W) will be marked as
isolated. The rest n −p attributes will be treated as the remaining group of attri-
butes. Thus, the total number of isolated attributes is pc.
4
Experimental Studies
In the following experiments we present how the reduction of the rules proceeds
and how the reduced, more interpretable rules perform. We use different values of ν
and report the corresponding values of the RMSE for the training and the testing
data. The GA used a population of 100 individuals and was run for 100 generations.
The crossover rate was set to 0.8 while the mutation rate was equal to 0.1. The
choice of these numeric values was a result of some preliminary experimentation.
The results are quantiﬁed by reporting the RMSE values obtained for different
values of the reduction index; refer to Fig. 3.
It becomes apparent (and intuitively anticipated) that lower values of ν result in
higher RMSE values. The detailed behavior varies across data with regard to how
far the rules can be reduced and how the differences shape up for the training and
testing data. For example, the reduction could be made quite substantial not
compromising the performance of the model as this becomes present in case of
abalone, auto, concrete, and white wine. In some case, we witness a phenomenon of
increased generalization abilities of the model (lower differences of the RMSE for
the training and testing data for lower values of ν). Figure 4 illustrates the per-
formance of the GA for some selected data; most of the improvement is visible at
the beginning of the optimization (ﬁrst 20–30 generations).
The detailed results of reduction of the number of variables in the rules are
contained in Fig. 5. The shaded regions identify the input variables being retained
in the corresponding rules. This offers a better view as to which input variables can
be dropped and points at a sequence of the variables, which have been eliminated.
468
W. Pedrycz et al.

(a) Abalone                                    
(b) Auto MPG
(c) Boston Housing                                            (d) Computer Activity
(e) Concrete Strength                                   (f) Forest Fires
(g) Red Wine Quality                               
(h) White Wine Quality
Fig. 3 RMSE values of the reduced rule-based models versus reduction level ν
Evolutionary Reduction of Fuzzy Rule-Based Models
469

5
Isolation of Input Variables
To enhance the transparency of the rules, we express the fuzzy set Ai as a Cartesian
product of a single isolated fuzzy set deﬁned in R and a relational remainder
expressed in Rn−1. In other words, we form the expression describing the condition
part as follows
A ∧
i ðxjÞ×A ∧
i ∼x ∼
ð
Þ
ð8Þ
where Ai
^ is a fuzzy sets deﬁned in R and Ai
∼is expressed in Rn−1. Then the rules of
the form read as follows
−if xj is A ∧
i ðxjÞ and x ∼A ∼
i ðx ∼Þ then y is fiðx, aiÞ
ð9Þ
Here a certain input variable (jth one) has been selected to be isolated. The term
isolation pertains to the fact that a certain variable has been chosen and subse-
quently a fuzzy set isolated from the fuzzy set Ai is treated separately and thus
becomes more visible and interpretable.
Obviously, the above Cartesian product is not identical to the original Ai that is
the following holds
Ai ≠ðA ∧
i ×A ∼
i Þ
ð10Þ
In terms of membership functions this means that the following relationship
holds
Ai x
ð Þ ≠minðAi ∧ðxjÞ, Ai ∼ðx ∼ÞÞ
ð11Þ
(a) Abalone                              (b) Auto MPG        
(c) Concrete Strength
Fig. 4 Values of ﬁtness function reported in successive GA generations for ν ranging from 0.1 to
0.9 with step 0.1 show direction of ν
470
W. Pedrycz et al.

(a) Abalone; ν=0.2, 0.4, 0.5                           (b)Auto MPG; ν=0.3, 0.5, 0.7
(c) Boston Housing; ν=0.4, 0.6, 0.9        (d) Computer Activity; ν=0.3, 0.4, 0.9
Fig. 5 Visualization of reduced rules: shaded regions identify the input variables being retained
Evolutionary Reduction of Fuzzy Rule-Based Models
471

(e) Concrete Strength ν=0.3, 0.5, 0.7              (f) Forest Fires ν=0.3, 0.6, 0.8
(g) Red Wine Quality; ν=0.2, 0.4, 0.8
(h) White Wine Quality; ν=0.2, 0.3, 0.4
Fig. 5 (continued)
472
W. Pedrycz et al.

Note that the corresponding membership functions of Ai
^(xj) and Ai
∼(x∼) are
computed as follows
A ∧
i ðxjÞ =
1
∑c
l = 1
xj −vij
xj −vil

2/ðm −1Þ
ð12Þ
A ∼
i
x ∼
ð
Þ =
1
∑c
l = 1
jjx ∼−v ∼
i jj
jjx ∼−v ∼
l jj

2/ðm −1Þ
ð13Þ
The consequence is that if Ai
^(xj) × Ai
∼(x∼) is used as the condition part of the ith
rule, the output of the model is going to be different than the original rule. It is
likely that the accuracy of the model could be reduced as a result of the increased
interpretability of the rules because of the isolation of the input variables. In the
above formulation, one is interested in choosing an individual variable (jth one) for
which the results provided by the rule-based model are as close as possible to those
formed by the original fuzzy model. The selection of the input variable is quite
straightforward through a direct enumeration.
The plots showing the LHS and the RHS relationship is shown for concrete
strength with one or ﬁve isolated input variables.
From the plots above, several observations of a general character can be drawn.
First, the values of the LHS are higher than the corresponding ones for the RHS.
This is reﬂective of the fact that the separation of the variable(s) leads to the higher
activation levels of the rules with eventual reduction of the speciﬁcity of the results
of reasoning. It is also apparent that with the increase of the variables being isolated
– compare Fig. 6a, b, the differences between the values produced by the LHS and
RHS of the expression (10) are more profound. Again, this is not surprising as by
isolating more variables we depart from the RHS more vigorously.
In a general setting, one can realize an isolation of L input variables, which as a
result leads to the rules in the form
−if xj1 is A ∧
i xj1 and xj2 is A ∧
i ðxj2Þ
and ⋯and xjL is A ∧
i ðxjLÞ and x ∼A ∼
i ðx ∼Þ
then y is fiðx, aiÞ
ð14Þ
note that in this case x∼is deﬁned in Rn−L.
Evolutionary Reduction of Fuzzy Rule-Based Models
473

(a) single input variable isolated
(b) 5 isolated input variables
Fig. 6 Values of the original activation of the rules value versus the one with isolated input
variables - concrete strength data set, 4 rules in the rulebase
474
W. Pedrycz et al.

0
1
2
3
4
5
6
7
1.8
2
2.2
2.4
2.6
2.8
3
L
RMSE
Training data
Testing data
0
1
2
3
4
5
6
2
4
6
8
10
12
14
L
RMSE
Training data
Testing data
a) Abalone                                                     b)Auto MPG
0
2
4
6
8
10
12
0
10
20
30
40
50
60
70
L
RMSE
Training data
Testing data
0
2
4
6
8
10
12
14
16
18
20
2
4
6
8
10
12
14
16
L
RMSE
Training data
Testing data
c) Boston Housing 
d) Computer Activity
0
1
2
3
4
5
6
7
5
10
15
20
25
30
35
40
45
50
55
L
RMSE
Training data
Testing data
1
2
3
4
5
6
7
8
9
10
11
60
65
70
75
80
85
90
95
100
L
RMSE
Training data
Testing data
e) Concrete Strength                                            f) Forest Fires
0
1
2
3
4
5
6
7
8
9
10
0
2
4
6
8
10
12
L
RMSE
Training data
Testing data
0
1
2
3
4
5
6
7
8
9
10
0.68
0.7
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
0.88
L
RMSE
Training data
Testing data
g) Red Wine Quality                                       h) White Wine Quality
Fig. 7 Performance of the fuzzy model versus the number of isolated input variables
Evolutionary Reduction of Fuzzy Rule-Based Models
475

Here an optimal choice of L variables gives rise to a combinatorial optimization
problem. This could be solved by GA optimization. Considering that the value of
L is speciﬁed in advance, GA forms an optimal isolation matrix I consisting of
c rows (number of rules) and n rows (number of input variables) where in each row
there are L 1 s indicating the variables which are isolated in the rule. For instance,
for L = 3 the matrix with the entries
I =
1
0
0
1
1
0
0
1
1
1
0
0
. . .
2
4
3
5
ð15Þ
states that in the ﬁrst rule isolated are variables 1, 4, and 5; in the second rule we
isolate variables 2, 3, and 4, and so on.
6
Experiments
The GA was carried out with 100 populations and maximum 50 generations.
Preliminary experiments with GA indicated lack of improvement before 50th
generation, so the maximum generation is set to 50. The population is not large, so
relative large values of moderate crossover and mutation rates are used. The
crossover rate is set as 0.8 and mutation rate is 0.1.
We present the results in a similar way as before by focusing on the presentation
of the rules with isolated variables and showing how the families of isolated
variables impact the performance of the model (Figs. 7, 8 and 9).
(a) Abalone                              (b) Auto MPG                     (c) Concrete Strength
Fig. 8 Fitness function reported in successive GA generations
476
W. Pedrycz et al.

a) Abalone with L= 1, 3, 5                           b)Auto MPG with L= 1, 3, 4
c) Boston Housing with L= 1, 4, 10                 d) Computer Activity with L= 2, 11, 13
Fig. 9 Isolated variables (shaded) obtained for selected values of L
Evolutionary Reduction of Fuzzy Rule-Based Models
477

e) Concrete Strength with L= 1, 2, 5                   f) Forest Fires with L= 2, 7, 9
(g) Red Wine Quality with L= 4, 5, 7                 (h) White Wine Quality with L= 1, 3, 5
Fig. 9 (continued)
478
W. Pedrycz et al.

7
Conclusions
The two approaches enhancing the interpretability of rule-based models are directly
applied to the already constructed Takagi-Sugeno fuzzy models realized with the
use of fuzzy clustering. Both the formation of the input subspace of conditions as
well as the isolation of the input variables are the methods reﬁning multivariable
fuzzy sets produced through fuzzy clustering. The proposed approaches are
quantiﬁable in terms of the level of the interpretability abilities offered by them
(expressed either in terms of the number of variables eliminated or the variables
isolated). This aspect is helpful in determining how much the interpretability could
be enhanced without any signiﬁcant sacriﬁce of accuracy of the model. Furthermore
in this way one could reveal input variables (or their combinations) that are essential
in rule-based modeling.
The approach offers a certain new view at the enhancement of fuzzy rule-based
models. There could be several avenues worth pursuing in the future including (a)
development of a hybrid arrangement of the formation of subspaces of conditions of
the rules associated with some further isolation of variables from such subspaces,
(b) use of other techniques of Evolutionary Optimization in the entire process, (c)
construction of interpretability measures quantifying various facets of the inter-
pretation mechanisms.
References
1. Alcala, R., Gacto, M.J., Herrera, F.: A fast and scalable multiobjective genetic fuzzy system
for linguistic fuzzy modeling in high-dimensional regression problems. IEEE Trans. Fuzzy
Syst. 19(4), 666–681 (2011)
2. Alonso, J.M., Magdalena, L., Guillaume, S.: Linguistic knowledge base simpliﬁcation
regarding accuracy and interpretability. Mathware Soft Comput. 13(3), 203–216 (2006)
3. Ayouni, S., Yahia, S.B., Laurent, A.: Extracting compact and information lossless sets of fuzzy
association rules. Fuzzy Sets Syst. 183(1, 16), 1–25 (2011)
4. Bezdek, J.C.: Pattern recognition with fuzzy objective function algorithms. Plenum Press, New
York (1981)
5. Bodenhofer, U., Bauer, P.: A formal model of interpretability of linguistic variables. In:
Casillas, J., Cordón, O., Herrera, F., Magdalena, L. (eds.) Interpretability Issues in Fuzzy
Modeling, pp. 524–545. Springer, Berlin (2003)
6. Chen, M.Y., Linkens, D.A.: Rule-base self-generation and simpliﬁcation for data-driven fuzzy
models. Fuzzy Sets Syst. 142(2), 243–265 (2004)
7. Goldberg, D.: Genetic Algorithms in Search, Optimization and Machine Learning. Addison-
Wesley (1989)
8. Gacto, M.J., Alcalá, R., Herrera, F.: Integration of an index to preserve the semantic
interpretability in the multiobjective evolutionary rule selection and tuning of linguistic fuzzy
systems. IEEE Trans. Fuzzy Syst. 18(3), 515–531 (2010)
9. Ishibuchi, H., Yamamoto, T.: Fuzzy rule selection by multi-objective genetic local search
algorithms and rule evaluation measures in data mining. Fuzzy Sets Syst. 141(1), 59–88
(2004)
Evolutionary Reduction of Fuzzy Rule-Based Models
479

10. Krone, A., Krause, H., Slawinski, T.: A new rule reduction method for ﬁnding interpretable
and small rule bases in high dimensional search spaces. In: Proceedings of 9th IEEE
International Conference on Fuzzy System, San Antonio, TX, pp. 693–699 (2000)
11. Mitchell, M.: Introduction to Genetic Algorithms. MIT Press (1998)
12. Muhlenbein, H., Schlierkamp-Voosen, D.: Predictive models for the Breeder genetic algorithm
I. Continuous Parameter Optim. Evol. Comput. 1, 25–49 (1993)
13. Wright, A.: Genetic algorithms for real parameter optimization. In: Rawlin, G.J.E (ed.)
Foundations of Genetic Algorithms 1, pp. 205–218. Morgan Kaufmann, San Mateo (1991)
14. Yam, Y., Baranyi, P., Yang, C.T.: Reduction of fuzzy rule base via singular value
decomposition. IEEE Trans. Fuzzy Syst. 7, 120–132 (1999)
Authors Biography
Witold Pedrycz is a Professor and Canada Research Chair
(CRC) in Computational Intelligence in the Department of
Electrical and Computer Engineering, University of Alberta,
Edmonton, Canada. He is also with the Systems Research
Institute of the Polish Academy of Sciences, Warsaw, Poland.
He also holds an appointment of special professorship in the
School of Computer Science, University of Nottingham, UK. In
2009 Dr. Pedrycz was elected a foreign member of the Polish
Academy of Sciences. In 2012 he was elected a Fellow of the
Royal Society of Canada. Witold Pedrycz has been a member of
numerous program committees of IEEE conferences in the area
of fuzzy sets and neurocomputing. In 2007 he received a
prestigious Norbert Wiener award from the IEEE Systems, Man,
and Cybernetics Council. He is a recipient of the IEEE Canada
Computer Engineering Medal 2008. In 2009 he has received a
Cajastur Prize for Soft Computing from the European Centre for Soft Computing for “pioneering
and multifaceted contributions to Granular Computing”. In 2013 has was awarded a Killam Prize.
In the same year he received a Fuzzy Pioneer Award 2013 from the IEEE Computational
Intelligence Society. His main research directions involve Computational Intelligence, fuzzy
modeling and Granular Computing, knowledge discovery and data mining, fuzzy control, pattern
recognition, knowledge-based neural networks, relational computing, and Software Engineering.
He has published numerous papers in this area. He is also an author of 15 research monographs
covering various aspects of Computational Intelligence, data mining, and Software Engineering.
Dr. Pedrycz is intensively involved in editorial activities. He is an Editor-in-Chief of Information
Sciences and Editor-in-Chief of WIREs Data Mining and Knowledge Discovery (Wiley). He
currently serves as an Associate Editor of IEEE Transactions on Fuzzy Systems and is a member
of a number of editorial boards of other international journals.
480
W. Pedrycz et al.

Kuwen Li received the B.S. and M.S. degrees in computer
science from Harbin Engineering University, P. R. China and M.
S. degree in computer engineering in 2005 and Ph. D. degree in
computer engineering in 2014 from the University of Alberta,
Canada. His research interests include Computational Intelli-
gence, fuzzy modeling, knowledge based neural networks, and
software engineering.
Marek Reformat received his M.Sc. degree (with honors) from
Technical University of Poznan, Poland, and his Ph.D. from
University of Manitoba, Canada. Presently, he is a professor
with the Department of Electrical and Computer Engineering,
University of Alberta. The goal of his research activities is to
develop methods and techniques for intelligent data modeling
and analysis leading to translation of data into knowledge, as
well as to design systems that possess abilities to imitate different
aspects of human behavior. He recognizes the concepts of
Computational Intelligence – with fuzzy computing and possi-
bility theory in particular – are key elements necessary for
capturing relationships between pieces of data and knowledge,
and for mimicking human ways of reasoning about opinions and
facts. Dr. Reformat applies elements of fuzzy sets to social
networks, Linked Open Data, and Semantic Web in order to
handle inherently imprecise information, and provide users with unique facts retrieved from the
data.
Dr. Reformat is a past president of the North American Fuzzy Information Processing Society,
and a vice president of the International Fuzzy Systems Association. He has been a member of
program committees of multiple international conferences related to Computational Intelligence
and Software Engineering.
Evolutionary Reduction of Fuzzy Rule-Based Models
481

Geospatial Uncertainty Representation:
Fuzzy and Rough Set Approaches
Frederick Petry and Paul Elmore
Abstract Uncertainty in geospatial data is often considered in the context of
geographical information systems which enable a variety of operations and
manipulation of spatial data. Here we consider how both fuzzy set and rough set
theory has been used to represent geospatial data with uncertainty. Terrain mod-
eling and triangulated irregular networks techniques utilizing fuzzy sets are pre-
sented. Rough set theory is overviewed and its application to spatial data is
described. Issues of uncertainty in the representation of spatial relationships such as
topological and directional relationships are discussed.
Keywords Geographic information systems ⋅Spatial database ⋅Fuzzy sets ⋅
Rough sets ⋅Triangulated irregular networks ⋅Indiscernibility relation ⋅Spatial
relations ⋅Upper and lower approximations
1
Introduction
Representation of uncertainty in geospatial data can viewed in context of the most
important use of such spatial data in geographic information systems (GIS). GIS are
employed extensively throughout governmental and industrial organization for
planning and decision making [1]. Typically underlying a GIS is a spatial database
in which many of the representation issues arise [2, 3].
F. Petry (✉)
Marine Geosciences Division, Geospatial Science and Technology Branch, Bldg. 1005,
Raleigh, USA
e-mail: fred.petry@nrlssc.navy.mil
P. Elmore
Naval Research Laboratory, Stennis Space Center, MS 39529, Washington, USA
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_24
483

Of particular interest for geographical information systems are the ongoing
efforts in the area known as “big data” [4, 5]. Typically with respect to big data
concerns, a major issue involves how to efﬁciently utilize the variety of sources
providing a vast amount of heterogeneous data [6]. Special emphasis is on envi-
ronmental data that has spatial aspects or with a spatial basis [7]. Sensor and
instrumentation advances have tremendously increased the variety and amount of
remote sensed data available. In particular at the NASA EOSDIS (Earth Observing
System Data and Information System) the imagery data archived is greater than
3 PB (Petabytes). Furthermore this system is creating each day an additional 5 TB
(Terabytes) of data. To effectively utilize such volumes of data, data mining
techniques are very critical [8]. One factor that must be considered in particular is
how to deal with the inherent uncertainty involved with the huge amount of such
spatial data in databases.
Uncertainty has been widely accepted as an implicit factor in geographical data
[9, 10]. Varsi [11, 12] has stated that vagueness is one of the principal issues in
geography, since concepts such a river’s length or a peak’s height in a speciﬁc area
are uncertain as the speciﬁcation of a river or peak are vague concepts. Couclelis
[13] posits that uncertainty is an intrinsic property of complex geospatial knowl-
edge and as such should be managed properly. It is not simply a problem or ﬂaw to
be reduced or eliminated. Many of the problems associated with data are prevalent
in all types of database systems. Spatial databases and GIS contain descriptive as
well as positional data. The various forms of uncertainty occur in both types of data,
so many of the issues to be discussed apply to ordinary databases as well. These
same techniques, including integration of data from multiple sources [14], time-
variant data, uncertain data, imprecision in measurement, inconsistent wording of
descriptive data, and “binning” or grouping of data into ﬁxed categories, also are
employed in spatial contexts [15].
Figure 1 illustrates the complexity that can be observed in a real world envi-
ronment. This ﬁgure is an image of the Louisiana gulf coastal region in the area of
the Atchafalaya Bay and illustrates the difﬁculty of specifying the characteristics of
the spatial features. The boundary between the coastline and the Gulf of Mexico,
the relationship of the various waterways and their characterization are difﬁcult to
specify as they exhibit both spatial and temporal uncertainty.
In this chapter we will examine some of the approaches that have been taken to
represent various aspect of geospatial data with fuzzy set and rough set techniques.
Here we consider how both fuzzy set and rough set theory has been used to
represent geospatial data with uncertainty. Terrain modeling and triangulated
irregular networks techniques utilizing fuzzy sets are presented. Rough set theory is
overviewed and its application to spatial data is described. Issues of uncertainty in
the representation of spatial relationships such as topological and directional rela-
tionships are discussed.
484
F. Petry and P. Elmore

2
Fuzzy Set Representations
2.1 Background
In general, the idea of implementing fuzzy set theory as a way to model uncertainty
in spatial databases has a long history. Geographical research investigations of the
use of fuzzy set approaches [16] involved areas such as geographical decision-
making and behavioral geography [17, 18]. However, the most consistent consid-
erations of the utilization of fuzzy set theory in applications to geographic infor-
mation systems was developed initially by Robinson [19]. Several models
appropriate to this situation were considered —fuzzy database representations using
simple membership values in relations, and a similarity–based approach for geo-
spatial features. In an application for which both the data as well as spatial rela-
tionships are imprecise, he modeled this situation as one that entails imprecision
intrinsic to natural language which is possibilistic in nature.
In the following period of time there were a number of efforts using fuzzy set
approaches for spatial databases developed. These included among others: querying
spatial information [20], representing spatial relationships [21], and object-oriented
modeling [22, 23]. Models have been proposed as well that allow for enhancing
database models to manage uncertain geospatial data [24]. A major motivation for
many approaches are the uncertain boundaries associated with geographic features
and objects. Fuzzy sets provide a natural representation for such forms of uncertainty
Fig. 1 Gulf of Mexico coastal region: Atchafalaya Bay area
Geospatial Uncertainty Representation …
485

2.2 Terrain Modeling
Digital elevation models (DEM) have been used to produce fuzzy representations of
terrain features. In Skidmore [25] a formulation of Euclidean distances from the
nearest streamline and ridgeline was used to represent the relative position, of a
speciﬁed location. However to represent ﬁner local morphological characteristics of
a location, a Euclidean distance may not always be adequate.
K-means approach. Another approach [26] used fuzzy k-mean classiﬁers to
provide a continuous classiﬁcation of terrain features. Since fuzzy k-means is
predominately an unsupervised classiﬁcation, it may have a problem to produce
results that provide a compatible matching to a landscape with the views provided
by domain experts such as soil scientists.
Rule-based approach. MacMillan et al. [27] developed a sophisticated fuzzy
classiﬁcation of terrain features using a comprehensive rule-based approach. This
method is particularly suitable for problems requiring intensive terrain analysis
operations and needs extensive information of local landforms from users..
Similarity approach. Locational similarity is another approach to determining a
speciﬁc area’s degree of membership in a spatial category. This is done by com-
puting a similarity of a location of interest to a typical location for the terrain
morphology [28]. For example special terrain features can have a very speciﬁc
interpretations by soil-landscape analysts where unique conditions of soil types
occur in locations of interest. Both a knowledge-based and deﬁnition-based
approach are presented as ways to specify the needed proto-typical locations.
Straightforward rules based on deﬁnitions can be utilized to determine the proto-
typical locations when there is a clear geomorphology. For example there are
algorithms for determining ridgelines and streamlines that can be used. However, if
a terrain feature has only has a local or regional meaning, ﬁnding the typical
location may require knowledge from local experts. This may be captured through
manual delineation using a GIS visualization tool.
The similarities of any other location to those speciﬁed typical locations can be
evaluated based on a set of selected terrain attributes such as elevation, slope
gradient, curvatures, etc. The process of assigning fuzzy membership value to a
location then consists of three steps:
1. A similarity evaluation of a location of interest and a typical location based on
the individual terrain attribute level is calculated.
2. The similarities are then aggregated based on individual terrain attributes. This
produces an overall similarity between a test location of interest and a typical
location.
3. Finally the similarities of the location of concern with all typical locations are
integrated. This results in the test location’s ﬁnal fuzzy membership relative to
the terrain feature of interest.
486
F. Petry and P. Elmore

2.3 Fuzzy Triangulated Irregular Networks
A common approach to represent ﬁeld data in contrast to object-based spatial data is
the Triangulated Irregular Networks (TINs). A TIN represents a surface in a GIS by
a partitioning of the 2-dimensional space into a network of non-overlapping
triangles. Applications using TINs in a GIS have motivated TIN extensions (ETINs)
[29] by using fuzzy memberships, fuzzy numbers and type-2 fuzzy sets. The ETIN
structure uses a mapping function S that speciﬁes a geographic area’s property.
For example consider the problem of selection and acquisition of a site for a
manufacturing facility which is “Close to” Seattle, Washington. So S = 1.0 for the
function indicates a location “near “ Seattle and S = 0.0 is interpreted as “far” from
Seattle. An intermediate value such as S = 0.5 could be interpreted as being “more
or less” close to the city.
Another ETIN alternative extension uses the common, easily implemented
representation by triangular membership function for fuzzy numbers. So here it is
necessary to extend the fuzzy number associated data types with the associated data
value for a point from a simple (crisp) value to a fuzzy set. By associating a
triangular membership function, this can be accomplished at every point of the
region of interest. There are then three characterizing points of importance: two
points where the membership grade equals 0 which delimit the membership
function, and the intermediate point for which the membership grade equals 1.
Lastly an ETIN extension can be based on type-2 fuzzy sets, which are a
generalization of regular fuzzy sets. This type permits imprecision as well as the
modeling of uncertainty regarding the membership grades. Again in our example
we can consider how to capture the certainty about the extent to which a site can be
characterized as “close to” Seattle. If we wish to describe the location of some
person, there could be doubt as to where they are exactly located. The individual
could be in a location close to Seattle, but also near Tacoma Washington. This sort
of imprecision or doubt can be modeled by using a type-2 fuzzy set: the mem-
bership grade on every location is extended to a “fuzzy” membership grade. As a
result, every point will now have an associated fuzzy set over [0,1].
3
Rough Set Approaches to Spatial Data Representation
Another approach for uncertainty representation uses the rough set concept of
indiscernibility of values. We ﬁrst give a background of rough sets and spatial data.
Then we provide the overview needed of rough set theory and discuss the rough set
model for imprecise spatial data.
Geospatial Uncertainty Representation …
487

3.1 Background
A number of geospatial research investigations have considered the applications of
rough set approaches. The ROSE system [30], proposed a description of spatial data
using rough sets. This approach focused on a formal modeling framework for
realm-based spatial data types in general. Based on the resolution at which data is
represented, Worboys [31] developed a model for imprecision of spatial data and
applied it to issues related to the integration of such data. This approach relies on
the use of indiscernibility – a central concept in rough sets. However it does not
carry over the entire framework and is just described as “reminiscent of the theory
of rough sets” [32]. The deﬁnition of a rough classiﬁcation of spatial data and
representation of inexact spatial locations using rough sets was developed by
Ahlqvist et al. [33]. This work also developed a quality metric of a rough classi-
ﬁcation as compared to a crisp classiﬁcation. This technique was evaluated on
ground truth data from vegetation map layers. In it combinations of rough and fuzzy
set approaches were utilized for reclassiﬁcation as needed to obtain the integration
of geographic data.
Other research investigators in a geographical information systems and mapping
environments [34] have developed an approach for the ﬁeld representation of a
spatial entity using a rough raster space which was evaluated for remote sensing
images in a classiﬁcation case study. Using K-labeled partitions, which can rep-
resent maps, Bittner and Stell [35] developed their relationship to rough sets in
order to approximate map objects with vague boundaries. Also they investigated
stratiﬁed partitions and extended this approach using the concepts of stratiﬁed
rough sets. For example in the case of map scale transformations, this approach can
obtain more reﬁned levels of details or granularity in many applications.
3.2 Rough Set Theory
Rough set theory [36] provides another mathematical formalism for capturing
uncertainty. The partitioning of some universe U into equivalence classes is known
as an approximation region in rough set theory. Such a partitioning can be modiﬁed
by increasing or decreasing its granularity, used to group elements together that, for
a particular application, are considered indiscernible, or to “bin” ordered domains
into range groups. The following is a set of common terminology and notation for
rough sets:
U is the universe, which cannot be empty,
R : indiscernibility relation, or equivalence relation,
A = U, R
ð
Þ, an ordered pair, called an approximation space,
[x]R denotes the equivalence class of R containing x, for any element x of U,
elementary sets in A - the equivalence classes of R.
488
F. Petry and P. Elmore

Any ﬁnite union of these elementary sets in A is called a deﬁnable set. A particular
rough set X ⊆U, however, is deﬁned in terms of the deﬁnable sets by specifying its
lower RX
ð
Þ and upper RX


approximation regions:
RX = x∈U x½ 
j
. R ⊆X
f
g
and
RX = x∈U x½ 
j
. R∩X≠∅
f
g
RX is the R-positive region, U −RX is the R-negative region, and RX −RX is
the R-boundary or R-borderline region of the rough set X. This allows for the
distinction between certain and possible inclusion in a rough set. The set approx-
imation regions provide a mechanism for determining whether something certainly
belongs to the rough set, may belong to the rough set, or certainly does not belong
to the rough set. X is called R-deﬁnable if and only if RX = RX. Otherwise,
RX≠RX and X is rough with respect to R. In Fig. 2 the universe U is partitioned
into equivalence classes denoted by the rectangles. Those elements in the lower
approximation of X, RX, are denoted with the letter “p” and elements in the R-
negative region by the letter “n”. All other classes belong to the boundary region of
the upper approximation.
Rough sets use an indiscernibility relation to partition domains into equivalence
classes, and lower and upper approximation regions for distinguishing between
certain and possible (or partial) inclusion in a rough set. The indiscernibility relation
allows us to group items based on some deﬁnition of ‘equivalence’, which basically
n
n
n
n
n
n
n
n
n
p
p
p
X
U
Fig. 2 Illustration of the concept of a rough set X
Geospatial Uncertainty Representation …
489

depends on the application domain. We may use this partitioning to increase or
decrease the granularity of a domain, to group items together that are considered
indiscernible for a given application, or to “bin” ordered domains into range groups.
For example, for data mining applications one may need to vary the partitioning of
domains in systematic ways in the process of discovering rules and relationships in
the data. The indiscernibility relation is fundamental to rough sets. The approxi-
mation regions can only be deﬁned in terms of the indiscernibility relation and the
equivalence classes it creates.
To obtain possible results, in addition to the obvious, certain results encountered
in querying an ordinary spatial database system, we may employ the use of the
boundary region information in addition to that of the lower approximation region.
The results in the lower approximation region are certain. They correspond to exact
matches. The boundary region of the upper approximation contains those results
that are possible, but not certain.
3.3 Rough Set Spatial Example
Let us assume we have a spatial area with a particular set of spatial objects com-
prising the universe U:
U = mast, brook, bayou, river, woods, thicket, pasture, field
f
g.
The particular equivalence relation R* for this speciﬁc area might be deﬁned as
follows:
R* =
mast
½
, brook, bayou, river
½
, woods, thicket
½
, pasture, field
½

f
g
Then given some subset X of spatial objects
X = mast, brook, bayou, river, woods, pasture
f
g,
we would like to deﬁne X in terms of its lower and upper approximations:
RX = mast, brook, bayou, river
f
g,
RX = mast, brook, bayou, river, woods, thicket, pasture, field
f
g
The equivalence classes that are totally included in the subset X form the lower
approximation. Then the upper approximation contains the lower approximation
and the classes that are only partially contained in X. For this example all the values
in the classes [mast] and [brook, bayou, river] are included in X. Hence these
belong to the lower approximation region. Note that the class [woods, thicket] is not
completely contained in X as X does not contain “thicket.” But, [woods, thicket] is
490
F. Petry and P. Elmore

part of the upper approximation since “woods” ∈X. In the example given, the
rough set is:
mast, brook, bayou, river, woods, pasture
f
g
f
.
mast, brook, bayou, river, woods, field
f
g
mast, brook, bayou, river, thicket, pasture
f
g
. mast, brook, bayou, river, thicket, field
f
gg
Although rough set theory deﬁnes the set in its entirety this way, for most
applications one typically will be dealing with only certain parts of this set at any
given time.
3.4 Spatial Data Uncertainty and Rough Sets
The approximation regions of rough sets are useful when information related to
spatial data regions is queried. Consider a region such as a woodland. One can
reasonably conclude that any grid point labeled as “woods” which on all sides is
surrounded by grid points also classiﬁed as “woods” is, indeed a point characterized
by the feature “woods”. But we may also be interested in grid points labeled as
“woods” that adjoin points identiﬁed as “ﬁeld”. Here it is possible that such points
represent ﬁeld areas as well as forest areas but were identiﬁed as “woods” during
the classiﬁcation. Likewise, points identiﬁed as “ﬁeld” but adjacent to “woods”
points may represent areas that contain part of the forest.
This uncertainty maps naturally to the use of the approximation regions of rough
set theory, where the lower approximation region represents certain data and the
boundary region of the upper approximation represents uncertain data. It applies to
spatial database querying and spatial database mining operations.
If we force a ﬁner granulation of the partitioning, a smaller boundary region
results. This occurs when the resolution is increased. As the partitioning becomes
ﬁner and ﬁner, ﬁnally a point is reached where the boundary region is non-existent.
Then the upper and lower approximation regions are the same and there is no
uncertainty in the spatial data as can be determined by the representation of the
model.
3.5 Rough Sets for Gridded Data
For spatial data with a raster data or other non-vector type formats as used for
continuous variables such as temperatures, soil types and population densities, such
data is often mapped with a particular grid representation. A regular matrix-like
structure organizes the spatial coordinates and the speciﬁc data is associated with
Geospatial Uncertainty Representation …
491

point locations on the grid. Clearly for improved data representation, a ﬁner reso-
lution or scale of a grid is desired. However there is a tradeoff between the reso-
lution and the processing required as higher resolutions provide more information,
but at a cost of execution complexity and memory space needed.
It is clear that from the rough set data viewpoint, the process of rasterizing or
gridding data. represents an inherent indiscernibility. Consider for example grid
locations representing various object /feature classiﬁcations such as forested,
agricultural, residential and industrial areas. We ﬁnd that certain grid points may
directly correspond to one of these classiﬁcations but some are in between one or
more of them. A data item at a particular grid point in essence may represent data
near the point as well. This is due to the fact that often point data must be mapped to
the grid using techniques such as nearest-neighbor, averaging, or statistics. We may
set up our rough set indiscernibility relation so that the entire spatial area is par-
titioned into equivalence classes where each point on the grid belongs to an
equivalence class. If we change the resolution of the grid, we are in fact, changing
the granularity of the partitioning, resulting in fewer, but larger classes.
4
Representation of Spatial Relations
Relationships among spatial objects can generally be classiﬁed in three types:
1. Topological - Touches, Disjoint, Overlap,... The border of Vietnam touches the
Laotian border
2. Directional - West, South-East, ... Chicago is West of New York
3. Metric – Distance San Francisco is about 50 km from San Jose
The nine-intersection model allows many topological relations between two
objects C and D to be speciﬁed. This model uses the intersections between the
interior, exterior and boundary of C and D [37]. This section will describe a variety
of approaches introducing uncertainty into these relationships.
4.1 Spatial Relations
Papadias et al. [38] describe a process for determining conﬁguration similarity for
spatial constraints involving distance, direction and topology. Their approach uses
the centroids of objects for distance and extended objects for direction and topology.
Uncertainty in the areas of fuzzy relations such as objects satisfying multiple
directional constraints are handled in their approach. Additionally fuzziness related
to linguistic relationship terms is supported as well. To allow comparison of alter-
native conceptualizations of direction, the concept of graded sections was developed
by another research effort [39]. Section bundles are introduced in order to describe
graded sections. This technique provides a formal means to:
492
F. Petry and P. Elmore

(1) compare alternative candidates related, for example by a direction relation like
“south-west” or “east,”
(2) characterize the candidates relative to their quality - between “satisfactory”
and “not as satisfactory” candidates, and
(3) select the optimal candidate from the set of possibilities.
Vazirgiannis [40] also considers the problem of representing uncertain topo-
logical, directional, and distance relationships on the assumption of crisply bounded
objects. All relationship deﬁnitions for this approach are centroid-based. A minimal
set of topological relations, overlapping and adjacency, are deﬁned based on
Egenhofer’s boundary/interior model [37]. This model is enhanced by providing
degrees of relationship satisfaction. Direction relations are deﬁned by a sinusoidal
function based on the angle between two objects’ centroids. The linguistic terms
corresponding to “near” and “distant” are used for to provide characterizations of
distances, where the ratio of the distance to a maximum application-dependent
distance determines the degree of membership in one of these categories. The three
relationships are combined for query retrieval. Afterward, a similarity measure is
computed for each relationship and then combined into a single, overall similarity
measure.
Another approach to spatial relations uses the histogram of forces [41] to provide
a fuzzy qualitative representation of the relative position between two-dimensional
objects. This can also be used in scene description where relative positions are
represented by fuzzy linguistic expressions. In Guesgen [42] we see the introduc-
tion of several approaches for reasoning about fuzzy spatial relations, including an
extension of Allen’s algorithm and additionally methods for fuzzy constraint sat-
isfaction. Also relevant is [43] which presents a uniﬁed framework for approximate
spatial and temporal reasoning using topological constraints as the representation
schema and fuzzy logic for representing imprecision and uncertainty. The appli-
cation of the resulting fuzzy representation to each of Allen’s interval relationships
[44] is developed as the possibility of the occurrence of the conditions of the
original deﬁnition.
Another approach using Allen’s relationships and based on minimum bounding
rectangles (MBRs) was developed by Cobb and Petry [45, 46]. A minimum
bounding rectangle is determined by the smallest X-Y parallel rectangle which
completely encloses a spatial object and so can provide an effective approximation
of the geometry of such objects. The wide use of MBRs in geographic information
systems and spatial databases is because they provide a computationally efﬁcient
way of locating and accessing objects in a spatial realm. Extending Allen’s tem-
poral relationships [46] into the spatial domain permits the representation of any
relationship that can occur between two one-dimensional (temporal) intervals
including: before, equal, meets, overlaps, during, starts, and ﬁnishes, along with
their inverses.
The binary relationship between objects in both the vertical and horizontal
direction can be entirely deﬁned by a tuple, [rx, ry] as determined by the minimum
bounding rectangles of two objects. Here rx is the one of the Allen’s temporal
Geospatial Uncertainty Representation …
493

relations described above that deﬁnes the interaction of the object MBRs in the x
direction, and ry represents the same for the y direction. For example, for the case of
the relationship, A [ﬁnishes, starts] B, the deﬁnition is given as:
Bxl<Axl<Bx2, Ax2 = Bx2, Byl<Ay2<By2, Ayl = Byl


where {x1,y1} and {x2, y2} represent the lower left and upper right corners,
respectively, of the minimum bounding rectangles.
In Fig. 3 is an example set of four object MBRs, {A,B,C,D}. A subset of the
existing relationships between them consists of:
A before, overlaps
½
B; B before, overlaps −1
½
C; D during, meets
½
C
f
g.
5
Conclusion
We have described a number of approaches using fuzzy set or rough set theory to
describe various aspects of spatial as might be utilized in geographic information
systems. To provide modeling of ﬁner details of spatial data type 2 fuzzy sets [47]
have been considered [48, 49] and other possible approaches include intuitionistic
fuzzy sets [50].
Acknowledgments We would like to thank the Naval Research Laboratory’s Base Program,
Program Element No. 0602435 N for sponsoring this research.
References
1. Longley, P., Goodchild, M., Maguire, D., Rhind, D.: Geographic Information Systems and
Science, 3rd edn. Wiley, Chichestere (2010)
2. Rigaux, P., Scholl, M., Voisard, A.: Spatial Databases with Application to GIS. Morgan
Kaufmann San Francisco (2002)
A
B
C
D
(x1,y1)
(x1,y1)
(x1,y1)
(x1,y1)
(x2,y2)
(x2,y2)
(x2,y2)
(x2,y2)
Fig. 3 MBR relationships: A [before, overlaps] B; B [before, overlaps −1] C; D [during, meets] C
494
F. Petry and P. Elmore

3. Shekar, S., Chawla, S.: Spatial Databases: A Tour. Prentice Hall, Upper Saddle River (2003)
4. Boyd, D., Crawford, K.: Critical questions for big data. Inf. Commun. Soc. 15(5), 662–679
(2012)
5. Michael, K., Miller, K.: Big Data: New opportunities and new challenges. IEEE Comput. 46
(6), 22–24 (2013)
6. Shekar, S., Gunturi, V., Evans, M., Yang, K.: Spatial big-data challenges—intersecting
mobility and cloud computing. In: Proceedings of 11th ACM International Workshop on Data
Engineering for Wireless and Mobile Access, New York, pp. 1–6 (2012)
7. Overpeck, J., Meehl, G., Bony, S., Easterling, D.: Climate data challenges in the 21st century.
Science 331, 700–702 (2011)
8. Vatsavai, R., Chandola, V., Klasky, S., Ganguly, A., Stefandidis, A., Shekhar, S.:
Spatiotemporal data mining in the era of big data. In: Proceedings of ACM Sigspatial:
Bigspatial ‘12, Redondo Beach, CA, pp. 1–10 (2012)
9. Burrough, P., Frank, A. (eds.): Geographic Objects with Indeterminate Boundaries, GISDATA
Series, vol. 2. Taylor and Francis, London (1996)
10. Zhang, J., Goodchild, M .: Uncertainty in Geographical Information. Taylor and Francis,
London (2002)
11. Varsi, A.: Philosophical issues in geography—an introduction. Topoi 20, 119–130 (2001)
12. Varsi, A.: Vagueness in geography. Philos. Geogr. 4, 49–65 (2001)
13. Couclelis, H.: The certainty of uncertainty: GIS and the limits of geographic knowledge.
Trans. GIS 7(2), 165–175 (2003)
14. Worboys, M., Clementini, E.: Integration of imperfect spatial information. J. Vis. Lang.
Comput. 12, 61–80 (2001)
15. Beaubouef, T., Petry, F., Breckenridge, J.: Rough set based uncertainty management for
spatial databases and geographical information systems. In: Suzuki, Y. (ed.) Soft Computing
in Industrial Applications, pp. 471–479. Chap 6a. Springer, London (2000)
16. Zadeh, L.: Fuzzy sets. Inf. Control 8, 338–353 (1965)
17. Gale, S.: Inexactness, fuzzy sets, and the foundations of behavioral geography. Geogr. Anal. 4,
337–349 (1972)
18. Fisher, P.: Boolean and fuzzy regions. In: Burrough, P.A., Frank, A. (eds.) Geographic Objects
with Indeterminate Boundaries, 87–94. Taylor and Francis, London (1996)
19. Robinson, V., Frank, A.: About different kinds of uncertainty in geographic information
systems. In: Proceedings AUTOCARTO 7 Conference, pp. 440–449. American Society for
Photogrammetry and Remote Sensing, Falls Church (1985)
20. Wang, F.: A fuzzy grammar and possibility theory-based natural language user interface for
spatial queries. Int. J. Fuzzy Sets Syst. 113, 147–159 (2000)
21. Cobb, M., Petry, F.: Modeling Spatial data within a fuzzy framework. J. Am. Soc. Inf. Sci. 49
(3), 253–266 (1998)
22. Cross, V., Firat, A.: Fuzzy objects for geographical information systems. Fuzzy Sets Syst. 113,
19–36 (2000)
23. De Tré, G., De Caluwe, R.: A generalized object-oriented database model with generalized
constraints. In: Proceedings of NAFIPS 1999, New York, pp. 381–386 (1999)
24. Morris, A.: A framework for modeling uncertainty in spatial databases. Trans. GIS 7, 83–101
(2003)
25. Skidmore, A.: Terrain position as mapped from a gridded digital elevation model. Int.
J. Geogr. Inf. Syst. 4, 33–49 (1990)
26. Irvin, B., Ventura, S., Slater, B.: Fuzzy and isodata classiﬁcation of landform elements from
digital terrain data in Pleasant Valley, Wisconsin. Geoderma 77, 137–154 (1996)
27. MacMillan, R., Pettapiece, W., Nolan, S., Goddard, T.: A generic procedure for automatically
segmenting landforms into landform elements using DEMs, heuristic rules and fuzzy logic.
Fuzzy Sets Syst. 113, 81–109 (2000)
28. Shi, X., Xing, A., Zhu, A., Wang, R.: Fuzzy representation of special terrain features using a
similarity-based approach. In: Petry, F., Robinson, V., Cobb, M. (eds.) Fuzzy Modeling with
Spatial Information for Geographic Problems, pp. 233–252. Springer, Heidelberg (2005)
Geospatial Uncertainty Representation …
495

29. Verstraete, J., De Tré, G., De Caluwe, R., Hallez, A.: Field based methods for the modeling of
fuzzy spatial data. In: Petry, F., Robinson, V., Cobb, M. (eds.) Fuzzy Modeling with Spatial
Information for Geographic Problems, pp. 41–70. Springer, Heidelberg (2005)
30. Schneider, M.: Spatial data types for database systems. Doctoral Thesis, Fern Universität
Hagen, GR
31. Worboys, M.: Imprecision in ﬁnite resolution spatial data. Geoinformatica 2(3), 257–280
(1998)
32. Worboys, M.: Computation with imprecise geospatial data. Comput. Environ. Urban Syst. 22
(2), 85–106 (1998)
33. Ahlqvist, O., Keukelaar, J., Oukbir, K.: Rough classiﬁcation and accuracy assessment. Int.
J. Geogr. Inf. Sci. 14(5), 475–496 (2000)
34. Wang, S., Yuan, H., Chen, G., Li, D., Shi, W.: Rough spatial interpretation. In: Tsumoto, S.,
Słowiński, R., Komorowski, J., Grzymała-Busse, J.W. (eds.) RSCTC 2004. LNCS (LNAI),
vol. 3066, pp. 435–444. Springer, Heidelberg (2004)
35. Bittner, T., Stell, J.: Stratiﬁed rough sets and vagueness. In: Kuhn, W., Worboys, M.F., Timpf,
S. (eds.) COSIT 2003. LNCS, vol. 2825, pp. 286–303. Springer, Heidelberg (2003)
36. Pawlak, Z.: Rough sets. Int. J. Man-Mach. Stud. 21, 127–134 (1984)
37. Egenhofer, M., Franzosa, R.: Point set topological spatial relations. Int. J. Geogr. Inf. Syst. 5
(2), 161–174 (1991)
38. Papadias, D., Karacapilidis, N., Arkoumnais, D.: Processing fuzzy spatial queries: a
conﬁguration similarity approach. Int. J. Geogr. Inf. Sci. 13, 93–128 (1999)
39. Kulik, L., Eschenbach, C., Habel, C., Schmidtke, H.: A graded approach to directions between
extended objects. In: Egenhofer, M.J., Mark, D.M. (eds.) GIScience 2002. LNCS, vol. 2478,
pp. 119–131. Springer, Heidelberg (2002)
40. Vazirgiannis, M.: Uncertainty handling in spatial relationships. In: ACM-SAC 2000
Proceedings, Como Italy, pp. 215–221 (2000)
41. Matsakis, P.: Understanding the spatial organization of image regions by force histograms. In:
Matsakis, P., Sztandra, L. (eds.) Applying Soft Computing in Deﬁning Spatial Relations,
1–16. Physica Verlag, Heidelberg (2002)
42. Guesgen, H.: Fuzzifying Spatial Relations. In: Matsakis, P., Sztandra, L. (eds.) Applying Soft
Computing in Deﬁning Spatial Relations, GR 99-122. Physica Verlag, Heidelberg (2002)
43. Dutta, S.: Approximate spatial reasoning: integrating qualitative and quantitative constraints.
Int. J. Approx. Reason. 5(3), 307–331 (1991)
44. Allen, J.: Maintaining knowledge about temporal intervals. Commun. ACM 26(11), 832–843
(1983)
45. Cobb, M., Petry, F.: Geometric approximations of spatial boundaries and assessment of fuzzy
spatial relationships. Fuzzy Sets Syst. 113, 111–120 (2000)
46. Petry, F., Cobb, M., Wen, L., Yang, H.: Design of system for managing fuzzy relationships for
integration of spatial data in querying. Fuzzy Sets Syst. 140, 51–73 (2003)
47. Mendel, J., John, R.: Type-2 fuzzy sets made simple. IEEE Trans. Fuzzy Sets 10, 117–127
(2002)
48. Verstraete, J.: Using level-2 fuzzy sets to combine uncertainty and imprecision in fuzzy
regions. In: Mugellini, E., Szczepaniak, P., Pettenati, M., Sokhn, M. (eds.) Advances in
Intelligent Web Mastering, vol. 3, pp. 163–172 (2011)
49. Verstraete, J.: Surface area of level-2 fuzzy regions unifying possibilistic and versitic
interpretations of regions. In: Rutkowski, L., Korytkowski, M., Scherer, R., Tadeusiewicz, R.,
Zadeh, L., Zurada, J. (eds.) Artiﬁcal Intellgience and Soft Computing, pp. 342–349. Springer
(2012)
50. Atanassov, K.: Intuitionstic fuzzy sets. Fuzzy Sets Syst. 20, 87–96 (1986)
496
F. Petry and P. Elmore

Authors Biography
Frederick E. Petry received BS and MS degrees in physics and
a Ph.D. in computer and information science from The Ohio
State University in 1975. He is currently a computer scientist in
the Naval Research Laboratory at the Stennis Space Center
Mississippi (2007-present). He has been on the computer science
faculty of the University of Alabama in Huntsville, the Ohio
State University and at Tulane University for 27 years.
His recent research interests include representation of impre-
cision via fuzzy sets and rough sets in databases, GIS and other
information systems, semantic web systems and artiﬁcial
intelligence including genetic algorithms directing 22 Ph.D.
students in these areas. Dr. Petry has over 375 scientiﬁc
publications including 150 journal articles/book chapters and 9
books written or edited. His monograph on fuzzy databases has
been widely recognized as the deﬁnitive volume on this topic.
He was or is currently an associate editor of IEEE Transactions on Fuzzy Systems, Neural
Processing Letters, International Journal of Intelligent Systems , etc. and area editor of information
systems for Fuzzy Sets and Systems and has been general chairperson of several international
conferences.
He was selected as an IEEE Fellow (Life) in 1996 for his research on the use of fuzzy sets for
modeling imprecision in databases in 2003 Fellow of the International Fuzzy Systems Association
and an ACM Distinguished Scientist in 2008. In 2002 he was chosen as the outstanding researcher
of the year in the Tulane University School of Engineering and received the Naval Research
Laboratory’s Berman Research Publication awards in 2004, 2008 and 2010.
Paul A. Elmore is a research physicist for the Marine
Geosciences Division of the Naval Research Laboratory (NRL),
Stennis Space Center, MS. Since 2007, he has been a principle
investigator of NRL research projects on bathymetry data fusion,
interpolation, uncertainty estimation and databasing. Recent work
includes research on Dempster-Shafer and fuzzy-set theories for
geospatial information uncertainty. He received a B.S. (magna
cum laude) in physics from Millsaps College in Jackson, MS in
1990 and a Ph.D. in physics from the University of Mississippi in
1996. His graduate work was in nonlinear acoustics, and he was a
graduate fellow at the National Center for Physical Acoustics. He
received a fellowship from the American Society of Engineering
Education to pursue postdoctoral research at NRL from 1996-
1998. He worked for the Naval Oceanographic Ofﬁce from 1998-
2001, returning to NRL in 2001. Paul lives in the New Orleans
area with his wife and children
Geospatial Uncertainty Representation …
497

How to Eﬃciently Diagnose and Repair
Fuzzy Database Queries that Fail
Olivier Pivert and Grégory Smits
Abstract Telling the user that there is no result for his/her query is very poorly
informative and corresponds to the kind of situation cooperative systems try to avoid.
Cooperative systems should rather explain the reason(s) of the failure, materialized
by Minimal Failing Subqueries (MFS), and build alternative succeeding queries,
called maXimal Succeeding Subqueries (XSS), that are as close as possible to the
original query. In the particular context of fuzzy querying, we propose an eﬃcient
uniﬁed approach to the computation of gradual MFSs and XSSs that relies on a
fuzzy-cardinality-based summary of the relevant part of the database.
1 Introduction
The paradigm of cooperative answering is originated from the works in the context of
natural-language question-answering done by Kaplan [1] at the end of the seventies.
Cooperative responses to a query are indirect responses or instructions that are more
helpful to the user than direct, literal responses would be. Interest in cooperative
responses in the database ﬁeld arose in the middle of the eighties [2–6]. A major
objective of cooperative answering systems is to avoid producing “there is no result”
when a query fails. Cooperative systems should rather explain the reason(s) of the
failure and suggest succeeding queries that are as close as possible to the original
one. Whereas most cooperative techniques from the literature deal with the “empty
answer set” problem in a classical (Boolean) query setting, in this paper, we rather
consider fuzzy queries that express user preferences. We address the situation where
a fuzzy query “fails” — we will see below that this term may take diﬀerent meanings
— and we propose an explain-and-repair strategy relying on a summary of a part of
the database. This summary provides information about the distribution of the data
O. Pivert (✉) ⋅G. Smits
University of Rennes 1/Irisa, 22305 Lannion, France
e-mail: olivier.pivert@univ-rennes1.fr
G. Smits
e-mail: gregory.smits@univ-rennes1.fr
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_25
499

500
O. Pivert and G. Smits
over the diﬀerent predicates involved in the failing query. Moreover, the complexity
of the summarization process only linearly depends on the size of the database, and
this fuzzy-cardinality-based summary is then eﬃciently used to detect the original
reasons of the query failure and to identify succeeding subqueries.
With respect to Boolean queries, fuzzy queries reduce the risk of obtaining an
empty set of answers since the use of a ﬁner discrimination scale — [0, 1] instead of
{0, 1} — increases the chance for an element to be considered somewhat satisfac-
tory. Nevertheless, the situation may occur where none of the elements of the target
database satisﬁes the query even to a low degree. In the context of fuzzy queries,
beside the empty answer set (EAS) problem stricto sensu, another situation may be
considered as a failure: that where the answer set is not empty but only contains el-
ements which satisfy to a low degree the preferences speciﬁed in the user query. We
show in this paper that a generic type of approach that leverages fuzzy cardinalities
may be employed to (i) provide explanations for both types of situations (empty or
unsatisfactory answer set), and (ii) suggest relevant non-failing alternative queries.
As we will see, minimal failing subqueries [7] constitute useful explanations about
the conﬂicts in a failing query, that may be used to set up a semi-automatic and
targeted relaxation strategy. This relaxation strategy consists in identifying the max-
imal succeeding subqueries of an initial failing query and to let users decide which
relaxation they are ready to accept.
The remainder of the paper is structured as follows. Section 2 provides a refresher
about fuzzy queries and fuzzy cardinalities. Section 3 presents how such a summary
is used in a uniﬁed approach to the computation of minimal failing subqueries and
maximal succeeding subqueries, that respectively explains and repairs the initial fail-
ing fuzzy query. Before discussing related works in Sect. 5, results of a ﬁrst experi-
mentation are presented in Sect. 4. Finally, Sect. 6 recalls the main contributions and
outlines perspectives for future work.
1.1 Principle
Figure 1 graphically illustrates the principle of the approach and the three step
process used by the proposed cooperative system. Faced with a failing or poorly
satisﬁed fuzzy query, the ﬁrst step consists in summarizing the useful part of the
database, this summary being then used to explain the reasons of the failure and to
detect succeeding subqueries as well.

How to Eﬃciently Diagnose and Repair Fuzzy Database Queries that Fail
501
Fig. 1
Overview of the cooperative system
2 Preliminaries
2.1 Fuzzy Sets and Fuzzy Preference Queries
Fuzzy set theory was introduced by L.A. Zadeh [8] for modeling classes or sets
whose boundaries are not clear-cut. For such objects, the transition between full
membership and full mismatch is gradual rather than crisp. Typical examples of
such fuzzy classes are those described using adjectives of the natural language, such
as young, cheap, fast, etc. Formally, a fuzzy set E on a referential U is characterized
by a membership function 𝜇E ∶U →[0, 1] where 𝜇E(u) denotes the grade of mem-
bership of u in E. In particular, 𝜇E(u) = 1 reﬂects full membership of u in E, while
𝜇E(u) = 0 expresses absolute non-membership. When 0 < 𝜇E(u) < 1, one speaks of
partial membership. Two crisp sets are of particular interest when deﬁning a fuzzy
set E:
∙the core C(E) = {u ∈U | 𝜇E(u) = 1}, which gathers the prototypes of E,
∙the support S(E) = {u ∈U | 𝜇E(u) > 0}.
The notion of an 𝛼-cut encompasses both these concepts. The 𝛼-cut E𝛼of a fuzzy
set E is deﬁned as the set of elements from the referential which have a degree of
membership to E at least equal to 𝛼. Straightforwardly, one has: C(E) = E1 and
S(E) = E0+.
The complement of E, denoted by Ec, is deﬁned by 𝜇Ec(u) = 1 −𝜇E(u). Further-
more, E ∩G (resp. E ∪G) is deﬁned the following way:
∙𝜇E∩G(u) = min(𝜇E(u), 𝜇G(u))
∙𝜇E∪G(u) = max(𝜇E(u), 𝜇G(u)).

502
O. Pivert and G. Smits
As usual, the logical counterparts of the theoretical set operators ∩, ∪and the com-
plementation operator correspond respectively to the conjunction ∧, disjunction ∨
and negation ¬. See [9] for more details.
In a database context, fuzzy sets make it possible to rank-order the items that
somewhat satisfy a user query involving gradual preference criteria. The language
called SQLf described in [10, 11] extends SQL so as to support fuzzy queries. The
general principle consists in introducing gradual predicates wherever it makes sense.
The three clauses select, from and where of the base block of SQL are kept in SQLf
and the from clause remains unchanged. The principal diﬀerences concern mainly
two aspects:
∙the calibration of the result since it is made with discriminated elements, which
can be achieved through a number of desired answers (k), a minimal level of sat-
isfaction (𝛼), or both, and
∙the nature of the authorized conditions as mentioned previously.
Therefore, the base block is expressed as:
select [distinct] [k | 𝛼| k, 𝛼] attributes
from relations where fuzzy-condition
where fuzzy-condition may involve both Boolean and fuzzy predicates.
The operations from relational algebra — on which SQLf is based — are extended
to fuzzy relations by considering fuzzy relations as fuzzy sets on the one hand and
by introducing gradual predicates in the appropriate operations (selections and joins
especially) on the other hand. The deﬁnitions of these extended relational operators
can be found in [11, 12]. As an illustration, we give the deﬁnition of the fuzzy selec-
tion hereafter, where 𝜙denotes a fuzzy predicate and r is a fuzzy (gradual) relation:
𝜇𝜎𝜙(r)(t) = min(𝜇r(t), 𝜇𝜙(t)).
2.2 Fuzzy Cardinalities
In the context of ﬂexible querying, fuzzy cardinalities appear to be a convenient
formalism to represent how many tuples from a relation satisfy a fuzzy predicate
(or a logical combination of fuzzy predicates) to various degrees. We assume in
the following that these various membership degrees are deﬁned by a ﬁnite scale
S ∶𝛼1 = 1 > 𝛼2 > ... > 𝛼f > 0. Fuzzy cardinalities can be incrementally computed
and maintained for each predicate involved in the user query and for all possible
conjunctions of predicates as well. Fuzzy cardinalities are represented by means of
a possibility distribution [13] or, without loss of information, with a more compact
representation:
FP = 1∕c1 + 𝛼2∕c2 + ... + 𝛼f ∕cf ,

How to Eﬃciently Diagnose and Repair Fuzzy Database Queries that Fail
503
where ci, i = 1..f is the number of tuples in the concerned relation that are P to a de-
gree at least equal to 𝛼i. For the computation of cardinalities concerning a conjunc-
tion of q fuzzy predicates, like FP1∧P2∧...∧Pq, one takes into account the minimum
of the satisfaction degrees obtained by each tuple t for the concerned predicates,
min(𝜇P1(t), 𝜇P2(t), ..., 𝜇Pq(t)).
2.3 Gradual Minimal Failing Subqueries
An empty set of answers associated with a fuzzy query Q = P1 ∧P2 ∧… ∧Pq
is necessarily due to an empty support (w.r.t. the current state of the database) for
a least one of the subqueries of Q. The notion of an unsatisfactory set of answers
generalizes this problem by considering an empty 𝛼-cut of Q where 𝛼is a user-
deﬁned qualitative threshold. As explained in Sect. 2, the support and the core of a
fuzzy set are particular cases of 𝛼-cuts where 𝛼is respectively equal to 0+ and 1.
In the rest of the paper we only use the notion of an empty 𝛼-cut to refer to failing
queries as well as poorly satisﬁed ones.
Thus, an extreme case of a failing query corresponds to an empty 0+-cut for Q
only. The opposite extreme is when one or several predicates have an empty 0+-cut.
Between these two situations, it is of interest to detect the subqueries composed of
more than one predicate and less than n predicates, that have an empty 0+-cut. From
an empty to an unsatisfactory set of answers, the problem deﬁned above just has to
be slightly revisited, transposing the condition of an empty 0+-cut to 𝛼-cuts, where 𝛼
is taken from the predeﬁned scale of membership degrees S ∶1 = 𝛼1 > 𝛼2 > ... >
𝛼f = 0+.
Deﬁnition 1 Let us consider a query Q = P1 ∧P2 ∧… ∧Pq, and let S and S′ be
two subsets of predicates such that S′ ⊂S ⊆{P1, P2, … , Pq}. A conjunction of
elements from S (resp. S′) is a subquery (resp. strict subquery) of Q.
If one wants to explain why the result of the initial query is empty (resp. unsatis-
factory), one must naturally require that such subqueries be minimal: a subquery Q′
of a query Q constitutes a minimal explanation if the considered 𝛼-cut is empty and
if no (strict) subquery of Q′ has an empty 𝛼-cut. This corresponds to a generalization
of the concept of a Minimal Failing Subquery (MFS) [14].
Let us denote by 𝛼𝛼
Q the set of answers that corresponds to the 𝛼-cut of a query Q
against a given database D: 𝛼𝛼
Q = {t ∈D | 𝜇Q(t) ≥𝛼}.
Deﬁnition 2 A Minimal Failing Subquery of a query Q = P1 ∧P2 ∧… ∧Pq for a
given 𝛼is any subquery Q′ of Q such that 𝛼𝛼
Q′ = ∅and for all strict subquery Q′′ of
Q′, 𝛼𝛼
Q′′ ≠∅.
Property 1 Let Q′ ∈Q be an MFS of Q for a given 𝛼. As we consider convex fuzzy
sets only, Q′ also fails for higher satisfaction degrees 𝛼′ > 𝛼. However, the property

504
O. Pivert and G. Smits
of minimality of the MFS Q is not guaranteed for higher satisfaction degrees. A
subquery Q′ may indeed be an MFS of Q for a given 𝛼without being minimal for
higher satisfaction degrees 𝛼′ > 𝛼as a strict subquery of Q′, say Q′′, may fail for 𝛼′
and not for 𝛼.
The set of minimal failing subqueries of an initial failing fuzzy query Q for a
degree 𝛼is denoted by MFSQ
𝛼.
Example 1 To illustrate this property, let us consider a conjunctive query Q = P1 ∧
P2 ∧P3 ∧P4 ∧P5 and ﬁve tuples whose satisfaction degrees on these ﬁve predicates
are given in Table 1. One may notice that no tuple satisﬁes the subquery Q′ = P1 ∧
P2 ∧P4 to the degree 0.2, whereas at least one tuple satisﬁes each strict subquery
of Q′ to this degree. Thus, Q′ is an MFS of Q for the degree 0.2 and using Property
1, one also knows that Q′ fails for more demanding satisfaction degrees (𝛼≥0.2).
However, despite the fact that Q′ also fails for 𝛼= 0.4, Q′ is not an MFS for that
degree as there exists a strict subquery of Q′, namely P1∧P2, whose 0.4−cut is empty.
Thus, Q′ fails for 𝛼= 0.4 but is not a minimal failing subquery as Q′′ = P1∧P2 ⊂Q′
also fails. This is also the case for P1 ∧P3 ∧P4 that is an MFS for 𝛼= 0.2 but not
for 𝛼= 0.4, as P1 ∧P3 fails for 𝛼= 0.4 making P1 ∧P3 ∧P4 not minimal.⋄
Table 1
Satisfaction of ﬁve tuples wrt. the predicates of Q
tuple
𝜇P1
𝜇P2
𝜇P3
𝜇P4
𝜇P5
t1
0.2
0.5
0
0
0
t2
0.5
0.1
0
0.5
0
t3
0
0.2
1
1
0
t4
0.2
0.5
0.4
0
0
t5
0
0.8
0.7
1
0
2.4 Gradual Maximal Succeeding Subqueries
For a fuzzy query Q that fails at a degree 𝛼, MFSs detected for this degree thus con-
stitute the explanations of the failure. Fixing Q so as to obtain a non empty set of
answers at this degree 𝛼may be achieved by a relaxation of Q, where one or more
predicates appearing in the MFSs are discarded. This (these) predicate(s) is (are)
chosen in such a way that (i) the obtained subquery does not contain any of the de-
tected MFS, and (ii) the subquery is as close as possible to the initial query, i.e. with
a minimal number of removed predicates. Obviously, if the subquery Q′ of Q does
not contain any MFS of Q for a given threshold 𝛼, then one has the guarantee that
Q′ returns a non-empty set of answers. Repairing an initial query Q that fails at a
degree 𝛼consists in identifying the maximal succeeding subqueries of Q for 𝛼. As
several distinct maximal succeeding subqueries may be envisaged for a given thresh-
old, candidate relaxations may be ordered according to two criteria: their closeness

How to Eﬃciently Diagnose and Repair Fuzzy Database Queries that Fail
505
wrt. the initial query Q and the number of answers that they return, which has to be
as close as possible to the quantitative threshold k that may be deﬁned by the user in
his/her query (Sect. 2.1).
Deﬁnition 3 Consider a query Q that fails at a given qualitative threshold 𝛼. A max-
imal succeeding subquery (XSS for short) at a degree 𝛼′ ≤𝛼is a query Q′ ⊆Q such
that |𝛼𝛼′
Q′| > 0, and such that there exists no other query Q′′, Q′ ⊂Q′′ that succeeds
at the degree 𝛼′.
Property 2 Symmetrically to Property 1, a subquery Q′ of a failing query Q that is
an XSS at a degree 𝛼also succeeds for lower satisfaction degrees 𝛼′ < 𝛼. However,
the property of maximality of such an XSS is not guaranteed for lower satisfaction
degrees. A subquery Q′ may indeed be an XSS of Q for a given 𝛼without being
maximal for lower satisfaction degrees 𝛼′ < 𝛼as a query Q′′, Q′ ⊂Q′′ may succeed
for 𝛼′ and not for 𝛼.
The set of maximal succeeding subqueries of an initial failing fuzzy query Q for
a degree 𝛼is denoted by XSSQ
𝛼. Obviously, an XSS of XSSQ
𝛼cannot contain any
predicate (or conjunction of predicates) from MFSQ
𝛼.
3 A Uniﬁed Approach to Failing Queries
Faced with a failing conjunctive query Q = P1 ∧P2 ∧... ∧Pq, the cooperative
system presented in this paper provides explanations for the vacuity of the answer set
using layered MFSs, and also suggests layered and ordered relaxations materialized
by XSSs. The identiﬁcation of these MFSs and XSSs relies on a fuzzy-cardinality-
based summary of a part of the searchable database, whose construction constitutes
the ﬁrst step of our uniﬁed approach.
3.1 Summarizing Step
Considering an initial user query Q = P1 ∧P2 ∧... ∧Pq that fails, the ﬁrst step
of our approach relies on the construction of a fuzzy-cardinality-based summary
of the useful part of the database (one focuses on the predicates from Q). Thus,
the summary is not constructed on the whole database but on the set of items that
somewhat satisfy at least a subquery of Q. This summary tells us how many tuples
from the database satisfy each conjunctive subquery of Q for the diﬀerent satisfaction
degrees involved in the predeﬁned scale S (Sect. 2.2).
A fuzzy-cardinality-based summary wrt. a set of predicates {P1, P2, ..., Pq} is
composed of up to 2q −2 fuzzy cardinalities, i.e. one for each nonempty strict sub-
query of Q. Obviously, it is not necessary to store a fuzzy cardinality for a conjunction
of predicates that is not satisﬁed at all (𝛼f = 0+) by at least one tuple of the database.

506
O. Pivert and G. Smits
As said previously, the fuzzy-cardinality-based summary is not computed on the
whole database but on the set of tuples that somewhat satisfy at least one predicate of
the initial query. Thus, considering a conjunctive fuzzy query Q = P1 ∧P2 ∧...∧Pq,
one ﬁrst builds a disjunctive query P1
0+ ∨P2
0+ ∨... ∨Pq
0+, where Pi
0+ is a crisp set
corresponding to the 0+-cut (i.e., the support) of the fuzzy predicate Pi. Processing
this Boolean disjunctive query returns a relation denoted by r, on which the fuzzy-
cardinality-based summary is then computed using the following algorithm:
for every tuple t of r do
compute 𝜇P1(t), 𝜇P2(t); ... and 𝜇Pq(t);
V ←⟨𝜇P1(t), ..., 𝜇Pq(t)⟩;
update the fuzzy cardinalities for all strict subqueries of Q using V;
done.
Ordered by the inclusion relation, the set of all subqueries of a conjunctive query,
say Q = P1 ∧P2 ∧P3 ∧P4 forms a lattice as illustrated by Fig. 2, with Q as the
upper bound and ∅as the lower bound. For a given satisfaction vector V, updating
the fuzzy cardinalities consists in traversing this lattice using a bottom-up breadth-
ﬁrst strategy. The exploration of a branch of the lattice stops when the current tuple
doest not satisfy at all the corresponding conjunction.
It is worth recalling that the computation of the satisfaction degree 𝜇Q′(t) of a
nonatomic subquery Q′ consists in taking the minimum of the individual satisfaction
degrees of t on the diﬀerent predicates involved. Internally, at the end of the process,
each node of the lattice is associated with its fuzzy cardinality.
Moreover, thanks to Property 1, one knows that if Q ⊂Q′, 𝜇Q(t) = 0 ⇒𝜇Q′
(t) = 0. This property is used to prune branches of the lattice during the update of
the fuzzy cardinalities for a given tuple t.
Q = P1
P2
P3
P4
P2
P3
P4
P1
P3
P4
P1
P2
P4
P1
P2
P3
P2
P3
P1
P2
P1
P3
P1
P4
P2
P4
P3
P4
P1
P2
P3
P4
Fig. 2
Lattice of subqueries of Q = P1 ∧P2 ∧P3 ∧P4
Remark 1 A possible heuristic for optimizing the computation of the fuzzy cardi-
nalities is to consider the diﬀerent predicates in increasing order of the size of their
support as branches of the exploration tree are cut as soon as no item satisﬁes the

How to Eﬃciently Diagnose and Repair Fuzzy Database Queries that Fail
507
conjunction associated with the current node. Indeed, one may assume, in the ab-
sence of further information about data distribution, that the smaller the support of
a predicate is, the more likely the number of items satisfying it is low.
Example 2 Let us consider a fuzzy query Q = P1 ∧P2 ∧P3 ∧P4 ∧P5 that fails even
for the lowest satisfaction threshold 𝛼= 0+, where {P1, P2, P3, P4, P5} are fuzzy
predicates. Table 1 gives the exact satisfaction degrees of ﬁve tuples wrt. the predi-
cates involved in Q, and Table 2 is an extract of the fuzzy-cardinality-based summary
where exact satisfaction degrees are projected on the scale S = ⟨0+, 0.2, 0.4, 0.6,
0.8, 1⟩.⋄
Table 2
Extract of the fuzzy-cardinality-based summary
Q′ ∈P(Q)
|Q′
1|
|Q′
0.8|
|Q′
0.6|
|Q′
0.4|
|Q′
0.2|
|Q′
0.0+|
P1
0
0
0
1
3
3
...
...
...
...
...
...
...
P4
2
2
2
3
3
3
P1 ∧P2
0
0
0
0
2
3
...
...
...
...
...
...
...
P3 ∧P4
1
1
2
2
2
2
P1 ∧P2 ∧P3
0
0
0
0
1
1
...
...
...
...
...
...
...
P2 ∧P3 ∧P4
0
0
1
1
2
2
3.2 Explaining the Failure
In the presence of an empty or unsatisfactory set of answers for a query Q, the con-
ﬂict detection process described hereafter generates layered MFSs for the diﬀerent
satisfaction degrees in S .
The algorithm used to identify the layered MFSs investigates the diﬀerent sub-
queries of the initial failing query Q for the diﬀerent satisfaction degrees in S . To
determine if a subquery Q′ is a failing subquery of Q for a satisfaction degree 𝛼i, one
just has to check the table storing the fuzzy cardinalities.
As illustrated by Fig. 3, detecting layered MFSs relies on a bottom-up breadth-
ﬁrst traversal of the lattice, where for each subquery, one checks the emptiness of
the 𝛼-cut, using the fuzzy cardinalities, in an increasing order of 𝛼from 𝛼f to 𝛼1.
Property 1 is used to propagate failing subqueries to more demanding satisfaction
degrees. The worst case in terms of complexity is when only Q is an MFS for the
highest satisfaction degree considered in S , i.e. 𝛼1 = 1.
In the manner of Apriori [7], Algorithm 1 traverses the lattice of subqueries of Q
(Fig. 2) starting with atomic predicates and the less demanding satisfaction threshold
𝛼f . If no tuple satisﬁes an atomic subquery, say P1, to the degree 𝛼i then P1, as an

508
O. Pivert and G. Smits
Fig. 3
Bottom-up breadth-ﬁrst traversal of the lattice to detect MFSs
atomic subquery, is by deﬁnition an MFS of Q and is also an MFS for 𝛼j ≥𝛼i.
Obviously, no subquery containing P1 will be considered as a candidate MFS during
the rest of the process. In the case where P1 is satisﬁed for the current satisfaction
threshold, say 𝛼i, one checks for more demanding thresholds 𝛼j > 𝛼i if P1 is still
satisﬁed. These tests are performed for each possible atomic subquery of Q.
Then, one goes to the second round of the loop (line 1.7 of Algorithm 1) where
conjunctions containing two nonfailing predicates are generated for the diﬀerent sat-
isfaction degrees of S . For each subquery (line 1.11), one checks the fuzzy cardi-
nalities so as to determine if it is an MFS. A subquery c is identiﬁed as an MFS for
the degree 𝛼i if the function that checks the summary card(c, 𝛼i) returns 0. If one of
these conjunctions, let say P2 ∧P3, is an MFS for a degree 𝛼i one tries to propagate
it to higher satisfaction degrees (see Algorithm 2 where isMinimal (L, MFS𝛼j(Q))
returns true if ∀M ∈MFS𝛼j(Q), M ⊄L, false otherwise). It is worth recalling that
this last test is necessary as the MFS property is not monotonic with respect to 𝛼-
cuts. One indeed has to check, using Algorithm 2, that for each 𝛼j > 𝛼i, no subquery
of P2 ∧P3 has already been detected as an MFS for degree 𝛼j. If it is not the case,
P2 ∧P3 is stored as an MFS of Q for 𝛼j. Obviously, an atomic query that fails is
by deﬁnition an MFS for all 𝛼-cuts. Then, the algorithm goes back to the loop (line
1.7) and conjunctions containing three predicates are generated for each considered
satisfaction degree (line 1.8) taking care that these conjunctions do not contain an
already identiﬁed MFS. This recursive process goes on until candidate conjunctions
cannot be generated anymore.
Remark 2 Even if a qualitative threshold 𝛼u is speciﬁed by the user in the initial
query Q that fails, MFSs are detected for the diﬀerent satisfaction degrees deﬁned
in the scale S . It is indeed important to detect that it is useless to expect any result

How to Eﬃciently Diagnose and Repair Fuzzy Database Queries that Fail
509
if an atomic subquery of Q fails for the least restrictive satisfaction degree 𝛼f . In the
case of Boolean queries, Algorithm 1 can also be used: one just needs to change the
scale of satisfaction degrees into the singleton {1}.
Input: a failing query Q = P1 ∧… ∧Pq; a scale of degrees
S = 𝛼f < ... < 𝛼2 < (𝛼1 = 1);
Output: MFSQ
𝛼i, i ∈S layered MFS’s of Q;
begin
1.1
foreach 𝛼i ∈S do
1.2
MFS𝛼i(Q) ←∅; E𝛼i ←{P1, … , Pn};
1.3
Cand𝛼i ←E𝛼i;
1.4
end
1.5
nbPred ←1;
1.6
while Cand𝛼1 ≠∅do
1.7
foreach 𝛼i ∈S from 𝛼f to 𝛼1 do
1.8
// generation of the candidates of size nbPred
1.9
Cand𝛼i ←{subqueries Q′ composed of nbPred predicates from E𝛼i
1.10
such that ∀Q′′ ⊂Q′, Q′′ ∉MFS𝛼i(Q)};
foreach c in Cand𝛼i do
1.11
if card(c, 𝛼i) = 0 then
1.12
MFS𝛼i(Q) ←MFS𝛼i(Q) ∪{c};
1.13
//Ec is the set of predicates involved in c
1.14
//thus remove Ec from the list of predicates used to generate
1.15
//candidate subqueries of size nbPred + 1
1.16
E𝛼i ←E𝛼i −Ec;
1.17
//Propagate c to higher satisfaction degrees
1.18
MFS = ∪i MFS𝛼i(Q)
1.19
propagate(𝛼i, A, c, MFS, E𝛼1...E𝛼f );
1.20
end
1.21
end
1.22
end
1.23
nbPred →nbPred + 1;
1.24
end
1.25
end
1.26
Algorithm 1: Gradual MFS computation

510
O. Pivert and G. Smits
Input: 𝛼i is a satisfaction degree; S is the scale of degrees; c is the MFS detected
for 𝛼i to propagate; MFSQ
𝛼i, i ∈S layered MFS’s of Q; E𝛼i, iS arrays of
predicates to use for the generation of candidates, one array per degree;
procedure propagate(𝛼i, A, c, MFS, E𝛼1...E𝛼f ) begin
2.1
foreach 𝛼j ∈S | 𝛼j ≥𝛼i do
2.2
if isAtomic(c) or isMinimal(c,MFS𝛼j(Q)) then
2.3
MFS𝛼j(Q) ←MFS𝛼j(Q) ∪{c};
2.4
E𝛼j ←E𝛼j −Ec;
2.5
else
2.6
break;
2.7
end
2.8
end
2.9
end
2.10
end
2.11
Algorithm 2: Procedure that propagates an MFS to higher satisfaction degrees
Example 3 Applied to the summary of Table 2, Table 3 gives the layered MFSs that
explain the failure of the query Q introduced in Example 1.⋄
Table 3
Layered MFSs of Q
𝛼
MFSQ
𝛼
𝛼
MFSQ
𝛼
𝛼1 = 1
{P1, P2, P5}
𝛼4 = 0.4
{P1 ∧P2, P1 ∧P3, P5}
𝛼2 = 0.8
{P1, P2 ∧P3, P5}
𝛼5 = 0.2
{P5, P1 ∧P2 ∧P4,
P1 ∧P3 ∧P4}
𝛼3 = 0.6
{P1, P5}
𝛼f=6 = 0+
{P5}
3.3 Repairing the Failure
Repairing a failing fuzzy query consists in suggesting XSSs for the diﬀerent satisfac-
tion degrees considered in the scale S . Obviously, the identiﬁcation of the maximal
succeeding subqueries for a given degree 𝛼i depends on the minimal failing sub-
queries found for this degree, this (these) failing subquery(ies) being denoted by
MFSQ
𝛼i. Indeed, a subquery Q′ of an initial failing query Q succeeds at a degree 𝛼i if
it does not contain any MFS from MFSQ
𝛼i.
The algorithm that is used to identify layered XSSs is pretty much the same as
the one used for detecting MFSs (Alg. 1). As illustrated by Fig. 4, it consists in per-
forming a top-down breadth-ﬁrst traversal of the lattice of candidate XSSs, this time
starting with the largest subqueries, and considering also the satisfaction degrees
of S in a decreasing order. For a given node, i.e. a subquery Q′ ∈Q, of the lat-
tice and a given satisfaction degree 𝛼i, one just has to check whether Q′ does not

How to Eﬃciently Diagnose and Repair Fuzzy Database Queries that Fail
511
contain any MFS from MFSQ
𝛼i. If Q′ succeeds, i.e. returns a nonempty answer set
whose cardinality is stored in the fuzzy-cardinality-based summary, then Property 2
is used to propagate Q′ to lower satisfaction degrees 𝛼j, 𝛼j < 𝛼i, taking care that the
maximality of the identiﬁed succeeding subquery is preserved for less demanding
satisfaction thresholds. As soon as a subquery Q′ ⊆Q is identiﬁed as an XSS of
Q for a degree 𝛼i then, by deﬁnition of an XSS, one can prune the part of the lat-
tice composed of strict subqueries of Q′. The worst case in terms of complexity is
when the XSSs correspond to atomic subqueries for the least restrictive considered
satisfaction degree 𝛼f .
Fig. 4
Top-down depth-ﬁrst traversal of the lattice to detect XSSs
Example 4 For the failing fuzzy query Q introduced in Example 1, Table 4 gives the
layered repaired queries returned to the user (who may then choose the one that best
ﬁts his/her need).⋄
Table 4
Layered XSSs of Q
𝛼
MFSQ
𝛼
XSSQ
𝛼
𝛼1 = 1
{P1, P2, E}
{P3 ∧P4}
𝛼2 = 0.8
{P1, P2 ∧P3, P5}
{P2 ∧P4, P3 ∧P4}
𝛼3 = 0.6
{P1, P5}
{P2 ∧P3 ∧P4}
𝛼4 = 0.4
{P1 ∧P2, P1 ∧P3, P5}
{P2 ∧P3 ∧P4}
𝛼5 = 0.2
{P1 ∧P2 ∧P4, P1 ∧P3 ∧P4, P5} {P2 ∧P3 ∧P4}
𝛼f=6 = 0+
{P5}
{P1 ∧P2 ∧P3 ∧P4}

512
O. Pivert and G. Smits
3.4 Human Readable Explanations
Explanations of the failure of an initial failing query are given to the user, as well
as succeeding subqueries, in order to let him/her choose the predicate(s) to relax.
The graduality of the detected MFSs and suggested XSSs signiﬁcantly improves the
informativeness of this cooperative feedback, as users can precisely adjust their ex-
pectations. Moreover, thanks to the fuzzy-cardinality-based summary, it is possible
to inform the users about the number of answers returned by each candidate XSS.
This latter piece of information may strongly inﬂuence the user about the predicates
he/she is ready to relax. Example 4 illustrates the structure of the explanations and
suggestions addressed to the user in case of a failing query.
Example 5 Considering the query introduced in Example 1, its MFSs presented in
Example 2 and the subsequent XSSs given in Example 3, the following human read-
able explanations are generated:
No item satisﬁes simultaneously P1, P2, P3, P4 and P5 to the degree 𝛼u because ...
∙no item satisﬁes P5,
∙no item satisﬁes P1 and P4, with P2 or P3 to a degree ≥0.2,
∙no item satisﬁes P1 with P2 or P3 to a degree ≥0.4,
∙no item satisﬁes P1 to a degree ≥0.6;
∙no item satisﬁes P2 with P3 to a degree ≥0.8,
∙no item fully satisﬁes P2.
... but the database contains:
∙k1 items that fully satisfy P3 and P4,
∙k2 items that satisfy P4, with P2 or P3 to a degree ≥0.8,
∙k3 items that satisfy P2, P3 and P4 to a degree ≥0.6,
∙k4 items that satisfy P1, P2, P3 and P4 to a degree > 0.⋄
4 Experimentation
In this section, we present experimental results obtained using a prototype that imple-
ments the approach described above. The goal of this experimentation is to assess the
eﬃciency of the approach with real data, and more precisely with a relation contain-
ing ads about 92,178 second hand cars. The relation schema is {price, make, mileage,
year, length, height, nbseats, acceleration, consumption, co2emission}. The results
presented in this section have been obtained using an Intel Core 2 Duo 2.53GHz
computer with 4Go 1067 MHz of DDR3 ram.

How to Eﬃciently Diagnose and Repair Fuzzy Database Queries that Fail
513
In a ﬁrst experimentation, we have observed the time needed to compute the
fuzzy-cardinality-based summary on preloaded datasets of various sizes. It is worth
recalling that, faced with a failing query, the fuzzy-cardinality-based summary is not
computed on the whole database but on the result of the crisp and disjunctive version
of the initial query.
Figure 5 shows the time needed to compute the summary on datasets of various
sizes for queries involving a ﬁxed number of predicates (here 6). This result empiri-
cally conﬁrms that the complexity of the summarization step linearly depends on the
size of the dataset. Fuzzy cardinalities are stored in main memory in a hashtable, keys
being conjunctions of predicates whereas values are the cardinalities themselves.
To complete the experimentation on the fuzzy-cardinality-based summarization of
a dataset, we have observed the space taken in memory by this hashtable according
to diﬀerent dataset sizes. Figure 6 shows an interesting though predictable phenom-
enon: the convergence of the size of the summary. This convergence can be indeed
explained by the fact that, whatever the number of tuples, the possible combina-
tions of properties that make sense to describe them is ﬁnite and can be quickly
enumerated. This last result has been obtained considering 10 fuzzy predicates for
the summarization, one for each attribute on which a second hand car is described
in our database. Thus, even for very large databases, fuzzy-cardinality-based sum-
maries easily ﬁt in main memory. This phenomenon is all the more pronounced as
there exist correlations between attributes of the considered relation.
Based on the summary stored in main memory, we have applied the cooperative
approach introduced in this paper to identify the MFSs and XSSs of failing queries
and observed the time needed to perform these tasks. Figure 7 shows that for diﬀerent
sizes of failing queries, i.e. from one to ten predicates, the time needed to compute
the MFSs and XSSs is negligible compared with the computation of the summary
which is obviously the most costly task.
As said in Sect. 3, a single disjunctive query has to be submitted to the database in
order to explain and repair a failing fuzzy query. Even with eﬃcient indexes deﬁned
on the diﬀerent searchable attributes, it is not conceivable to apply a naive strategy
where each subquery of the initial failing subquery is submitted to the database. To
conﬁrm the relevance of a summary-based approach that relies on a single query to
the database, Fig. 8 compares the time needed to identify the MFSs of a failing query
Q using our approach and a naive one that runs each subquery of Q (note that the
y-axis corresponds to a logarithmic scale).
These experimentations show that the proposed approach is not very sensitive to
the size of the dataset and remains tractable even for very large databases as long as
the size of the query is reasonable. This is why one can conclude that this approach
may be eﬃciently integrated in many applicative contexts, for instance e-business
as online shops generally propose query interfaces where at most half a dozen of
predicates can be speciﬁed.

514
O. Pivert and G. Smits
Fig. 5
Computation time
of the summarization step
wrt. the size of the DB
Fig. 6
Size of the summary
wrt. the number of
considered predicates
Fig. 7
Computation time
of the diﬀerent steps:
summarization, explanation,
repair
5 Related Works and a Few Words About Complexity
Providing explanations and repairs for a failing query is a crucial issue if one wants
to devise genuinely cooperative information systems. In [14], Godfrey studied the
computational complexity entailed by the identiﬁcation of all of the MFSs and XSSs
for a failing conjunctive query. Even though it is easy to ﬁnd one MFS, Godfrey

How to Eﬃciently Diagnose and Repair Fuzzy Database Queries that Fail
515
Fig. 8
fuzzy-cardinality-
based approach vs. naive
approach (logarithmic scale)
showed that ﬁnding all the minimal causes of the emptiness of the answer set is
NP-Hard and implies to run an exponential number of subqueries. In the context of
recommendation systems, McSherry introduced in [15] a heuristic called coverage
for removing ﬁrst the predicates that are the most likely to be responsible for the
failure. The coverage of a predicate is the number of times this predicate is involved
in the discovered MFSs. In [16], Bidault et al. also studied the problem of identifying
the minimal explanations of a failing query in the particular context of a mediation
system and suggesting possible query repairs.
As databases are getting larger and larger, it is not realistic to run an exponential
number of queries, even if such a strategy would produce very informative explana-
tions. This is why Jannach proposed in [17] an MFS and XSS detection technique
that relies on a single scan of the database. During this full scan of the database, a
binary matrix is built, that contains as many rows as there are tuples, and as many
columns as there are Boolean predicates in the query. A 1 bit is set to a cell if the con-
cerned tuple satisﬁes the concerned predicate, 0 otherwise. This binary matrix is then
used to identify the MFSs and suggest XSSs. Even though the strategy proposed by
Jannach relies on a single query to the database, the size of its binary matrix linearly
depends on the size of the database, which can be highly problematic in practice.
We show in this paper that a uniﬁed approach based on fuzzy cardinalities may
be used to detect the layered MFSs and to repair the initial query as well. The sum-
marization step is obviously the most costly step of these two approaches. Indeed,
the construction of the fuzzy-cardinality-based summary linearly depends on the
number of tuples considered when building this summary. So, one can say that the
proposed approach, as the one introduced in [17], linearly depends on the size of the
database, which is a very positive point, but exponentially depends on the number
of predicates involved in the initial failing query, where the worst case corresponds
to a single MFS Q for the maximal satisfaction degree of 1. This is not a serious
limitation in practice as the number of predicates generally deﬁned by users in their
queries is rather low (≤8) in most application contexts. Thus, even for a medium-
sized database, a fuzzy-cardinality-based summary is signiﬁcantly smaller than the
binary matrix used in [17].

516
O. Pivert and G. Smits
Note also that the works mentioned above all deal exclusively with Boolean fail-
ing queries, whereas our approach is suitable both for Boolean and fuzzy queries.
6 Conclusion
There is a consensus inside the community working on cooperative approaches that
clear explanations have to be given when users are faced with failing queries. These
explanations have to focus on the minimal causes of the failure and should come
along with succeeding queries corresponding to relaxations of the original one. In
this paper, we have proposed an eﬃcient uniﬁed approach based on a summary of
the database composed of fuzzy cardinalities. The main advantage of this approach
is that a single scan of a part of the database is needed in order to identify the causes
of the failure and suggest succeeding queries that are as close as possible to the initial
failing query.
As perspectives for future works, we are currently investigating the computation
of fuzzy cardinalities in a massively distributed system in a context of big data. We
also intend to use correlation measures between attributes or a workload of succeed-
ing queries to deﬁne heuristics that could be used during the XSSs detection step for
inﬂuencing the choice of the predicates to discard ﬁrst.
References
1. Kaplan, S.J.: Cooperative responses from a portable natural language query system. Artif.
Intell. 19, 165–187 (1982)
2. Cuppens, F., Demolombe, R.: Cooperative ansering: a methodology to provide intelligent
access to databases. In: Proceedings of DEXA’88, pp. 333–353 (1988)
3. Gaasterland, T.: Relaxation as a platform for cooperative answering. J. Intell. Inf. Syst. 1(3–4),
296–321 (1992)
4. Motro, A.: Cooperative database system. In: Proceedings of FQAS’94, pp. 1–16 (1994)
5. Ras, R.W., Dardzinska, A.: Intelligent query answering. In: J. Wang (ed.) Encyclopedia of Data
Warehousing and Mining, 2nd edn, vol. II, pp. 1073–1078. Idea Group Inc., Canada (2008)
6. Corella, F., Lewison, K.: A brief overview of cooperative answering. In: Technical report.
http://www.pomcor.com/whitepapers/cooperative_responses.pdf (2009)
7. McSherry, D.: Retrieval failure and recovery in recommender systems. Artif. Intell. Rev. 24(3–
4), 319–338 (2005)
8. Zadeh, L.A.: Fuzzy sets. Inf. Control 8(3), 338–353 (1965)
9. Dubois, D., Prade, H.: Fundamentals of Fuzzy Sets. The Handbooks of Fuzzy Sets, vol. 7.
Kluwer Academic Publishers, Netherlands (2000)
10. Bosc, P., Pivert, O.: SQLf: a relational database language for fuzzy querying. IEEE Trans.
Fuzzy Syst. 3(1), 1–17 (1995)
11. Pivert, O., Bosc, P.: Fuzzy Preference Queries to Relational Databases. Imperial College Press,
London (2012)
12. Bosc, P., Buckles, B., Petry, F., Pivert, O.: Fuzzy databases. In: Bezdek, J., Dubois, D., Prade,
H. (eds.) Fuzzy Sets in Approximate Reasoning and Information Systems. The Handbook of
Fuzzy Sets Series, pp. 403–468. Kluwer Academic Publishers, Dordrecht (1999)

How to Eﬃciently Diagnose and Repair Fuzzy Database Queries that Fail
517
13. Dubois, D., Prade, H.: Fuzzy cardinalities and the modeling of imprecise quantiﬁcation. Fuzzy
Sets Syst. 16, 199–230 (1985)
14. Godfrey, P.: Minimization in cooperative response to failing database queries. Int. J. Coop. Inf.
Syst. 6(2), 95–149 (1997)
15. McSherry, D.: Incremental relaxation of unsuccessful queries. In: Funk, P., González-Calero,
P.A. (eds.) Advances in case-based reasoning. In: Proceedings 7th European Conference, EC-
CBR 2004, Madrid, Spain, August 30 - September 2, 2004, Lecture Notes in Computer Science,
vol. 3155, pp. 331–345. Springer, Berlin (2004)
16. Bidault, A., Froidevaux, C., Safar, B.: Repairing queries in a mediator approach. In: Horn,
W. (ed.) ECAI 2000. Proceedings of the 14th European Conference on Artiﬁcial Intelligence,
Berlin, Germany, August 20–25, 2000, pp. 406–410. IOS Press, Amsterdam (2000)
17. Jannach, D.: Finding preferred query relaxations in content-based recommenders. In: Intel-
ligent Techniques and Tools for Novel System Architectures, pp. 81–97. Springer, London
(2008)
Authors Biography
Olivier Pivert (http://people.irisa.fr/Olivier.Pivert/) received
the Ph.D. degree in computer science from the University of
Rennes 1, France, in 1991. He is currently a full Professor of
computer science at École Nationale Supérieure des Sciences
Appliquées et de Technologie, Lannion, France, and a Member
of the Institut de Recherche en Informatique et Systèmes Aléa-
toires where he heads the research team Shaman (http://www-
shaman.irisa.fr/). His research work mainly concerns the exten-
sion of database systems for fuzzy querying and for dealing
with imprecise/uncertain information. He has published over
250 papers in books, journals, and conference proceedings.
He is the co-author of Fuzzy preference Queries to Relational
Databases (Imperial College Press, 2012) and the co-editor
of Flexible Approaches in Data, Information and Knowledge
Management (Springer, 2014). He is a Member of the editor-
ial board of several technical journals (Fuzzy Sets and Systems, the Journal of Intelligent Infor-
mation Systems and the International Journal of Uncertainty, Fuzziness and Knowledge-Based
Systems).
Grégory Smits received the Ph.D degree in computer sci-
ence from the University of Caen (France) in 2008. He is cur-
rently an Associate Professor at IUT Lannion (University of
Rennes, France) and he is a member of the research labora-
tory named IRISA (Institut de Recherche en Informatique et
Systèmes Aléatoires). His research works mainly concern the
extension of usual relational database systems for fuzzy query-
ing and cooperative answering.

The Web, Similarity, and Fuzziness
Parisa D. Hossein Zadeh and Marek Z. Reformat
Abstract The Internet is perceived as a source of multiple types of information, a
large shopping mall, a social forum and an entertainment hub. The users constantly
browse and search the web in order to ﬁnd things of interest. The keyword-based
search becomes less and less efﬁcient in the case of more reﬁned searches where
details of items become important. The introduction of Resource Description
Framework (RDF) as a relation-based format for data representation allows us to
propose a different way of performing a relevancy-based search. The approach
proposed is based on representation of items as sets of features. This means that
evaluation of items’ relevance is based a feature-based comparison. A more realistic
relevancy determination can be been achieved via categorization and ranking of
features. The calculated relevancy measures for individual categories of features are
aggregated using a fuzzy-based approach.
1
Introduction
Internet is perceived as a repository of information. Everyday, millions of users
search and browse the web. Besides news and information, they also look for items
of possible interest: books, movies, hotels, travel destinations, and many more. It is
anticipated that these items possess speciﬁc or similar features (Tversky 1977).
Additionally, not all of these features are equally important, some of them are
signiﬁcant with a high selective power, while some are entirely negligible.
The improvement of the users’ searching activities depends on development of
web applications that are able to support the users in ﬁnding relevant entities. So
far, identiﬁcation of data satisfying user’s request is realized by matching the query
keywords to pieces of information. Most of the web search engines utilize this
approach and its variations.
P.D. Hossein Zadeh ⋅M.Z. Reformat (✉)
Electrical and Computer Engineering Department, University of Alberta, T6G 2V4
Edmonton, Canada
e-mail: reformat@ualberta.ca
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_26
519

A novel representation of information on the web, introduced by the concept of
Semantic Web (Berners-Lee et al. 2001), changes the way how individual pieces of
information are stored and accessed on Internet. The fundamental data format is
called Resource Description Framework (RDF) (Lassila et al. 2014) and it relies on
a simple concept of a triple: <subject-property-object>. In other words, a triple can
be perceived as a relation existing between two entities: one of them – subject – is
the main entity that is in relation embodied by a property with another item –
subject. It means that any piece of information can be represented as a set of triples
where multiple items are linked to each other being subjects and/or objects in
different triples.
An evaluation of relevancy between two items is associated with determining
similarity between them. Therefore, similarity assessment between two items is an
important and fundamental step in processes and applications related to information
extraction and retrieval, web search, automatic annotation, etc. Application of RDF
as data representation format allows us to propose a different approach to evaluate
items’ relevancy. The approach is based on the fact that an item is represented as a
set of triples while all of them are “tied” by the fact that a subject of these triples is
the same, i.e., it is an item under consideration. In such representation of a single
item, each triple is treated as its feature. This allows us to apply a feature-based
comparison of items and to take advantage of its ﬂexibility and adaptability in a
process of evaluating relatedness between items.
Our approach is based on the idea that properties/features of an entity can be
categorized and ranked based on their importance in describing the entity. The
calculated similarity measures for these categories of features are aggregated using
fuzzy weights associated with the importance of these categories.
This chapter introduces a novel approach suitable for identiﬁcation of related
items. A number of important aspects of the proposed approach are presented here:
• The evaluation of relatedness of two items is performed using features of the
items. The RDF representation allows us to compare items on a feature-by-
feature (triple-by-triple) basis. The principles of the approach are explained in
Sect. 2.
• The importance of features is recognized as essential characteristics of similarity
evaluation process. Different features contribute to the overall similarity in
different ways. It is important to automatically determine importance of features,
as well as to apply a proper aggregation process to combine similarities of
individual features. The process of determining importance of features is based
on Wikipedia Infoboxes1 as explained in Sect. 3.1. Further, a fuzzy-based
method of aggregating evaluated similarities of single features and taking into
account different importance levels of the features are fully explained in
Sect. 3.2.
1http://en.wikipedia.org/wiki/Help:Infobox
520
P.D. Hossein Zadeh and M.Z. Reformat

• The proposed method is applied to a real-life scenario of ﬁnding relevant books.
The results obtained using the proposed approach are compared with the sug-
gestions provided by Google. The case study is presented in Sect. 4.
2
Feature-based Similarity in Linked Data
Similarity is essential for ﬁnding relevant things. This can be further explored by a
need to dig a bit “deeper” and look not only on items as whole units but also at
individual features of these items. Potentially, this can lead to a more reﬁned
similarity estimation process and better results. The introduction of Resource
Description Framework (RDF) provides an opportunity to “see” items as sets of
features and built simple procedures for evaluating similarity of items.
2.1 Linked Data and RDF
The goal of the Semantic Web as an enhancement to the current web is to provide a
meaning and structure to the web content. The Semantic Web’s road map points to
ontology as a way of accomplishing this. Ontology deﬁnes a structural organization
and relations between concepts,2 properties and instances. It also adds semantic
richness and reasoning capabilities. Any type of information expressed with a
means of ontologies can be semantically analyzed and processed leading to more
comprehensive results.
The Semantic Web concept introduces RDF as a way of representing informa-
tion including ontologies and their instances. The fundamental idea is to represent
each piece of data as a triple: <subject-property-object>, where the subject is an
entity being described, the object is an entity describing the subject, and the
property is a “connection” between the subject and object. For example, Godfather
is book is a triple with Godfather as its subject, is its property, and book its object.
In general, a subject of one triple can be an object of another triple, and vice versa.
The growing presence of RDF as a data representation format on the web brings
opportunity to develop new ways how data is processed, and what type of infor-
mation is generated from data.
A single RDF-triple <subject-property-object> can be perceived as a feature of
an entity identiﬁed by the subject. In other words, each single triple is a feature of
its subject. Multiple triples with the same subject constitute a deﬁnition of a given
entity. A simple illustration of this is shown in Fig. 1(a). It is a deﬁnition of
Godfather. If we visualize it, deﬁnition of the entity resembles a star with the
deﬁned objects as its core. We can refer to it as an RDF-star.
2Throughout this chapter, two terms “concept” and “entity” are used interchangeably.
The Web, Similarity, and Fuzziness
521

Quite often a subject and object of one triple can be involved in multiple other
triples, i.e., they can be objects or subjects of other triples. In such a case, multiple
deﬁnitions – RDF-stars – can share features, or some of the features can be centres
of another RDF-stars. Such interconnected triples constitute a network of inter-
leaving deﬁnition of entities, Fig. 1(b).
In general, Uniform Resource Identiﬁers (URI) are used to uniquely identify
subjects and properties. Objects, on the other hand, are either URIs or literals such
as numbers or strings.
Due to the fact that everything is interconnected, we can state that numerous
entities share common features. In such a case, comparison of entities is equivalent
to comparison of RDF-stars. This idea is a pivotal aspect of the proposed approach
for determining relevance of items.
Based on the Semantic Web vision, several knowledge bases have been created
including DBpedia,3 Geonames,4 YAGO,5 and FOAF6. The collection of interre-
lated datasets on the Web is referred to as Linked Data (LD) (Bizer and Berners-Lee
2009). DBpedia is a large linked dataset, which contains Wikipedia data translated
(a)          
(b)
Fig. 1 (a) RDF-stars: a
deﬁnition of Godfather with
one of its features enhanced,
(b) interconnected RDF-stars
representing: Godfather,
Hyperion, The Sicilian, Ubik
and Do Androids Dream of
Electric Sheep
3http://dbpedia.org/About
4http://www.geonames.org/
5http://www.mpi-inf.mpg.de/yago-naga/yago/
6http://www.foaf-project.org/
522
P.D. Hossein Zadeh and M.Z. Reformat

into RDF triples. Even though DBpedia is a large dataset that has over one billion
triples from Wikipedia, it also provides RDF links to other datasets on the Web
such as Geonames and Freebase7. With a growing number of RDF triples on the
web – more than 62 billions8 triples– processing data in RDF format is gaining a
special attention. There are multiple works focusing on RDF data storage and
querying strategies using a specialized query language SPARQL (Levandoski and
Mokbel 2009)(Schmidt et al. 2008).
2.2 Similarity in RDF Data
The underlying idea of the proposed approach for relevancy evaluation is to
determine number of common and relevant features. In the case of RDF deﬁned
concepts, this nicely converts into checking how many features they share as
presented in Fig. 2. Deﬁned entities are books “The Godfather” and “The Sicilian”
that share number of features. Some of these features are identical – the same
property and the same subject (black circles in Fig. 2), while some have the same
object but different properties.
Basically, number of different comparison scenarios can be identiﬁed. It depends
on interpretation of the term “entities that they share”. The possible scenarios are
(for details see Hossein Zadeh and Reformat 2013a, b):
• identical properties and identical objects
• identical properties and similar objects
• similar properties and identical objects
• similar properties and similar objects
Fig. 2 Similarity of RDF
deﬁned concepts: based on
shared objects connected to
the deﬁned entities with the
same properties
7http://www.freebase.com/
8http://stats.lod2.eu
The Web, Similarity, and Fuzziness
523

To better understand the proposed methodology, Fig. 3 shows similarity
assessment between two entities x and y. As can be seen, similarity is evaluated by
taking into account two layers, Layer 1 and Layer 2. Similarity of Layer 1 is
assessed via two components: common objects of the two entities, i.e., the common
object {z} in Fig. 3; and the pair of unique objects {(w, u)}. Similarity of Layer 2 is
to estimate similarity between unique pairs – {(w, u)}. It is evaluated based on all
permutations of objects that are connected to the elements, i.e., {(s, g), (s, f), (s, r),
(t, g), (t, f), and (t, r)}.
3
Similarity Evaluation
The principle idea presented here relies on the assertion that properties of an entity
should have different importance values in similarity assessment between that entity
and other entities. These importance values reﬂect their inﬂuence in describing that
entity. Thus, similarity between entities cannot be ideally calculated with properties
having equal weights. In fact, human judgment of similarity considers relative
importance values for properties of an item. As an example, in a problem of ﬁnding
books similar to a particular book, properties such as “author”, “genre”, and
“subject” are more dominant compared to such properties as “country”, “number of
pages”, and “cover artist”. It is worth noting that the present study should not be
confused with similarity assessment within a context deﬁned via speciﬁc properties.
For example, a context similarity evaluation can be applied in the question, “How
much these two books are similar in the context of their topic?” For similarity
assessment in a context in Linked Data (LD), see (Hossein Zadeh and Reformat
2013a, b). The solution presented in this chapter determines the semantic similarity
between entities expressed in RDF triples while recognizing and dealing with the
importance of each property associated with the entities under study.
Fig. 3 Similarity evaluation
process for Layer 1
and Layer 2
524
P.D. Hossein Zadeh and M.Z. Reformat

A fundamental step in similarity assessment of two entities is comparing features
associated with the entities. Having RDF triples for representing information in LD,
features are represented with properties and their values with objects (Sect. 2.2). For
example, in LD a book can be represented with multiple features (properties) such
as name, author, country, language, genre, and publisher. An example of a real
entity (from DBpedia) described with multiple features is shown in Fig. 4.
The problem of handling importance of features is composed of two sub-prob-
lems: (1). How to recognize dominant properties? (2). How to deal with them? In
real life, it is intuitive to distinguish very important properties of an entity from less
important and not important ones. A computer program requires a well-deﬁned
approach to do the same task since every piece of information in LD is represented
as RDF triples, and all the triples are equally important. Additionally, importance
values of properties for all entities are constantly modiﬁed on the web. Therefore, a
process of determining the importance values is a very time-consuming and
impractical task.
Section 3.1 presents an answer to the ﬁrst sub-problem of how to detect and
distinguish the properties that vary in their importance. We present a solution to this
problem by obtaining this information from the most popular online encyclopedia,
Wikipedia. The next important question is how to use the obtained properties and
their signiﬁcance values in similarity evaluation. The answer to this question, usage
of fuzzy membership functions, is explained in Section 3.2.
3.1 Properties and their Importance
Once the properties and their importance values are deﬁned, we create a fuzzy set
(Zadeh 1965) L of n subsets that categorize the properties with respect to their
importance. Each subset has an assigned linguistic label that corresponds to its
importance degree, such as:
Fig. 4 Book “The Lord of the Rings” with its features
The Web, Similarity, and Fuzziness
525

L = fl1 = critical, l2 = very important, . . . , ln = not importantg
ð1Þ
In other words, properties with equal importance describing an entity are clas-
siﬁed in the same subset. Each subset li may contain any number of properties:
li = fp1, p2, . . . , pmg
ð2Þ
A total number of subsets, n, is a user-deﬁned constant. This helps us to cate-
gorize a property in a proper subset that indicates its importance.
We developed an approach using Wikipedia Infoboxes to ﬁnd key properties of
an entity and to classify them into the suitable importance subset. Infobox is
summarized information represented in a table on the top right-hand side of a
Wikipedia page. It provides information about a particular entity. An example of
Infobox for the book “The Lord of the Rings” is shown in Fig. 5.
First, we obtain the Infobox template9 corresponding to the category of a con-
sidered entity. Properties included in the Infobox template are selected as the
characteristic properties of the entity that we keep. We discard the rest of properties
that the entity has. This step reduces the amount of data to be processed in a
similarity evaluation method. Next, we classify the properties into proper subsets of
L based on their importance in describing the entity.
The main idea proposed here is to exploit the information in the domain of a
property. Domain of a property is a class of the subject in the <subject-property-
object> RDF triple. Basically, the class refers to an item located in a hierarchical
taxonomy. We argue that this information plays a critical role in identifying the
importance of a property. Therefore, we categorize the properties based on the
location of their domains in taxonomy of domains. This approach is justiﬁed
because classes located in higher levels of taxonomy are more abstract than the ones
in lower levels (Hossein Zadeh and Reformat 2013a, b). In general, abstract classes
carry paramount description of an entity compared to less abstract and speciﬁc
classes. Thus, properties with more abstract domains are more important. For
example, considering an entity “book” properties such as {subject, genre, name}
carry important information, intuitively, and they belong to the most abstract class,
“thing”, in DBpedia ontology10. In Fig. 6, a fragment of DBpedia taxonomy related
to an entity “book” is depicted.
In a situation of comparing entities that belong to different Infobox templates,
e.g., a book and a car, same process is followed. However, the obtained similarity
will be very low as it lacks existence of common properties.
In LD, information is represented as a set of triples:
LD =
< s, p, o > s ∈C, p ∈P, o ∈C ∪D
j
.
f
g
ð3Þ
9http://en.wikipedia.org/wiki/Template:Infobox_book
10http://mappings.dbpedia.org/server/ontology/classes/
526
P.D. Hossein Zadeh and M.Z. Reformat

where C, D and P={p1,p2,…,pm} are sets of entities, data values and properties,
respectively. Each entity, c, is a subject in a number of triples connected to it via
properties, p. Therefore, we represent an entity as a set of triples deﬁning it.
Our proposed algorithm for similarity calculation is shown in details in Pseudo
codes 1 (Table 1) and 2 (Table 2). In line 5 (Table 1), a set of common triples
between two entities x and y, Ocommon, is obtained. Ox and Oy are two sets of objects
unique to each entity x and y, respectively. They are initialized in line 6. For all
permutations of elements in sets Ox and Oy, a sub-function Sim_Second_Layer is
Fig. 5 Wikipedia Infobox for
the book “The Lord of the
Rings”
Fig. 6 Small fragment of
DBpedia taxonomy for an
entity “book”
The Web, Similarity, and Fuzziness
527

Table 1 Pseudo code for similarity calculation
Table 2 Pseudo code of Sim_Second_Layer (a, b)
528
P.D. Hossein Zadeh and M.Z. Reformat

called (line 10). Similarites calculated for all pairs (a, b) are obtained and combined
in line 12. Similarity between two entities x and y is calculated in line 16. It should
be noted that to avoid division by zero, which happens in the case if there are no
common objects, the value of similarity, line 8, is set to zero. This situation may
happen when different entities are compared.
In sub-function Sim_Second_Layer, triples of the pair of entities a and b are
extracted, Table 2. Two sets of Oa and Ob are initialized each containing the objects
attached to entities a and b respectively via the property “rdf:type” (line 1). Note
that, only triples having the property “rdf:type” are obtained and the rest are dis-
carded. This is because, the description of an entity provided by the property “rdf:
type” is of a special importance in Linked Data. “rdf:type” is used to say that things
are of certain types. It is worth noting that a similar procedure can be repeated for
the property “rdf:subject”. Results from these two properties may be combined
depending on how the information is expressed in a data set. For simplicity, we
only consider the property “rdf:type” in the proposed similarity computation pro-
cess. Similarity between c and d as the permutations of elements in sets Oa and Ob
is calculated in lines 5–8. If c and d are different, their similarity is calculated using
(Wu and Palmer 1994). Otherwise, their similarity is calculated based on the depth
of the ontology that c or d belongs to. Finally, the maximum of similarities of all
pairs is returned to the main function Similarity_ﬁnal(x,y) (line 11).
3.2 Similarity and Fuzziness
Similarity between two entities x and y is deﬁned as the aggregated similarity
values computed for every subset li:
Simfinalðx, yÞ = aggrðSiml1ðx, yÞ, Siml2ðx, yÞ, . . . , Simlnðx, yÞÞ
ð4Þ
where aggr(.) is an aggregation operator, described later. Similarity values related
to each subset li is obtained as the average of similarities for all properties in that
subset:
Simliðx, yÞ = avgðSiml1
p1ðx, yÞ, Siml1
p2ðx, yÞ, . . . , Siml1
pmðx, yÞÞ
ð5Þ
The Sim(.) function calculates the similarity value between two entities as
deﬁned below. Due to the nature of LD, entities along with their properties are
distributed over the Web in a form of connected RDF triples. Considering an entity
in LD, values of its properties may be a subject of another triple and so on. Those
triples provide further information that can be used in similarity evaluation of an
entity to another. For this reason, we include in similarity evaluation not only the
triples that are directly connected to the entity (Layer 1) but also the triples con-
nected one layer further away from the entity (triples describing the objects of that
entity), see Fig. 3.
The Web, Similarity, and Fuzziness
529

In (4), the aggr(.) operation can be any process that takes the weights of the
similarity values into consideration. The main idea is that similarity measures
calculated for each subset li contribute differently to the ﬁnal similarity according to
their importance. Here, it is deﬁned as the normalized weighted sum of the simi-
larity measures in which weights are the membership degrees obtained in (7). The
ﬁnal similarity is calculated as:
Simfinalðx, yÞ = w1. Siml1ðx, yÞ + w2. Siml2ðx, yÞ + . . . + wn. Simlnðx, yÞ
ð6Þ
where,
wi =
μðSimliðx, yÞÞ
∑iμðSimliðx, yÞÞ
ð7Þ
μð. Þ gives the membership degree for each Simliðx, yÞ and is obtained as follows:
μðSimliðx, yÞÞ = Simliðx, y

ψi
ð8Þ
Here, we know that Simliðx, yÞ ∈½0, 1, therefore larger values of ψi leads to
smaller values of membership degrees. ψ is a signiﬁcance power and is calculated
as:
ψi = i −f ðli, cÞ
ð9Þ
where, i - index of the subset li representing importance of a property, and f ðli, cÞ
is a ratio of a number of properties of a subset li for a given entity to the total
number of properties over all li ‘s of that particular entity. To justify (9) it should be
noted that more important properties have smaller values of i and their μðSimliðx, yÞÞ
are larger. Also, f ðli, cÞ adjusts μðSimliðx, yÞÞ such that if the subset li constitutes a
substantial part of all properties, f ðli, cÞ is larger, i −f ðli, cÞ becomes smaller, and
μðSimliðx, yÞÞ increases. This shows higher inﬂuence of more representative
properties.
All the steps in our approach of calculating similarity between any two entities
are shown in Fig. 7. As it can be seen, triples in Layer 1 and Layer 2 of entities
under similarity assessment are extracted from datasets in LD. After detecting the
category of the entity by examining the related property, the corresponding Infobox
template from Wikipedia is obtained. The Infobox contains key properties of that
entity. We categorize them and rank the subsets of properties L= {l1, l2,…,ln}.
Similarity values for all subsets are calculated separately, and further they are
aggregated to compute the ﬁnal similarity.
530
P.D. Hossein Zadeh and M.Z. Reformat

4
Experiments
To evaluate the above approach, a set of entities is selected from a real-world
dataset DBpedia and their associated RDF triples were extracted. The entities are
instances of a concept “book” in DBpedia. As discussed previously, the Infobox
template representing the concept “book” is extracted. This helps to detect char-
acteristic properties and to group them. Next, a list of classes, {book, thing, work,
written work}, are obtained from this template representing domains of the Infobox
properties. Accordingly, we deﬁne four subsets of properties that group the prop-
erties within the same domain. Based on the location of each domain in the
DBpedia ontology, we assign importance ranking to the created subsets. Table 3
shows the formed subsets for the entity “book” for this experiment.
The last subset in Table 3, null, contains properties related to an entity “book”
that are labeled as non-important and are ignored in the similarity evaluation pro-
cess. It is worth noting that discarding the non-important properties may cause
losing some information. However, this will reduce the time and increase the speed
of the similarity evaluation process especially when large number of entities are
described with large numbers of properties. In addition, selection of these subsets
and assigning each property to a subset can be customized to users’ preferences or
an application context.
Fig. 7 Schematic of the similarity evaluation approach
The Web, Similarity, and Fuzziness
531

Eight instances of the entity “book” are selected: “The Godfather”, “The Sicilian”,
“Do Androids Dream of Electric Sheep?”, “Hyperion”, “Ubik”, “The Master and
Margarita”, “Fools Die” and “The Family Corleone”. Figure 8 illustrates entities and
their features, as well as relationships between. As it can be seen, entities may be
connected directly via common objects or through subsequent connections.
Figure 9 shows similarity results between the entities based on the approach
presented in Sect. 3.
According to the obtained values of similarity between any pair of books, the
books “The Sicilian”, “The Family Corleone” and “Fools die” are ranked as top
three matches. Table 4, compares this result to the Google Knowledge Graph and
Fig. 8 Relationship of the given entities in LD
Table 3 Different subsets of properties and their importance
Subset
Properties
l1
Name, caption, title, country, language, series, subject, genre, publication date
l2
Author, translator, publisher, preceded by, followed by
l3
oclc11, lcc12
l4
Illustrator, cover artist, media type, number of pages, isbn, dewey13
null
First publication date, last publication date, number of volumes, based on,
completion date, license, description, abstract, rights, editor, format, sales, etc
11Online Computer Library Center number
12Library of Congress Classiﬁcation
13Dewey Decimal System Classiﬁcation
532
P.D. Hossein Zadeh and M.Z. Reformat

the suggestions provided by Amazon. For the Google Knowledge Graph, the list is
compiled based on searches performed by the users, and the position of books
reﬂects the frequency searches performed after “The Godfather” was searched for.
It should be noted, that this list contains three books from our experiment. The
swapped position of “The Family Corleone” and “Fools Die” is due to the fact that
our approach assigns high importance to an author. In the Google Graph the
importance of features does not exist. Also, the Amazon website suggests “The
Sicilian” and “The Family Corleone” to the buyers of “The Godfather”. This
emphasizes the high similarity of these books.
Table 5 compares the obtained similarity values of the proposed approach in the
case when similarity values are averaged and weighted. In the averaged case,
properties are considered to have equal importance, thus ﬁnal similarity is averaged
over the similarities of all properties regardless of their dominance in deﬁning an
entity. The weighted approach is the weighted sum of the similarities (6). The
inﬂuence of recognizing importance of properties and the application of fuzzy
membership functions can be easily observed.
Table 4 Results of searching for the book “The Godfather”
Our approach
“The Sicilian”, “Fools Die”, “The Family Corleone”
Google Knowledge
Graph
“The Sicilian”, “The Godfather returns”, “The Family Corleone”,
“The Last Don”, “Fools Die”, “The Fortunate Pilgrim”, …
Amazon
“The Sicilian”, “The Family Corleone”
Fig. 9 Similarity values between entities
The Web, Similarity, and Fuzziness
533

5
Conclusion
The work presented here addresses the problem of relevance assessment between
concepts within the environment of Linked Data (LD). In LD, data is represented in
a form of RDF triples. RDF triples that share the same subject are perceived as
features describing an entity identiﬁed by this subject. In other words, a given
concept is deﬁned via a set of features, and these features can be used to compare
two entities. Such a simple idea is applied here to perform similarity assessment
between concepts.
It is intuitively obvious, that not all features of concepts are equally important.
Therefore, we propose a novel approach to calculate the semantic similarity by
taking into account the importance levels of properties deﬁning concepts. A method
is presented to identify these properties and their degree of importance using the
information included in Wikipedia Infoboxes. Based on these importance levels,
fuzzy membership functions are developed. They are used to determine weights
associated with levels of properties. These weights are further used to aggregate
similarity measures assessed for each group of properties with the same importance.
The proposed approach is deployed to estimate similarities between several
books. The RDF-based deﬁnitions of these books have been obtained from
dBpedia. The results are very encouraging. They are comparable with the lists of
books identiﬁed by Google Knowledge Graph that is composed based on the users’
searches, and by Amazon suggestions that are determined based on the users’
purchases.
References
Berners-Lee, T., Hendler, J., Lassila, O.: The semantic web. Sci. Am. 284, 28–37 (2001)
Bizer, C., Heath, T., Berners-Lee, T.: Linked data-the story so far. Int. J. Semant. Web Inf. Syst. 4,
1–22 (2009)
Table 5 Comparison of similarity measures for averaged and weighted aggregations
Pairs of Concepts
Simfinal −averaged
Simfinal −weighted
(The Godfather, The Sicilian)
0.88
0.91
(The Godfather, Fools Die)
0.73
0.73
(The Godfather, The Family Corleone)
0.53
0.62
(The Godfather, The Master and Margarita)
0.16
0.01
(Do Androids Dream, Hyperion)
0.65
0.85
(Do Androids Dream, The Godfather)
0.12
0.25
(Do Andoirds Dream, Ubik)
0.79
0.88
(Do Andoirds Dream, The Master and
Margarita)
0.19
0.26
534
P.D. Hossein Zadeh and M.Z. Reformat

Hossein Zadeh, P.D., Reformat, M.Z.: Assessment of semantic similarity of concepts deﬁned in
ontology. Inf. Sci. 250:21–39 (2013a)
Hossein Zadeh, P.D., Reformat, M.Z.: Context-aware similarity assessment within semantic space
formed in linked data. J. Amb. Intel. Human. Comp. 4:515–532 (2013b)
Levandoski, J.J., Mokbel, M.F.: RDF data-centric storage. In: Proceedings of ICWS 2009: IEEE
International Conference on Web Services, Los Angeles, CA, USA, pp. 911–918 (2009)
Lassila, O., Swick, R., Lassila, O., Swick, R.: Resource description framework (RDF) model and
syntax speciﬁcation. W3C Tech Reports and Publications (2014). http://www.w3.org/TR/1999/
REC-rdf-syntax-19990222. Accessed 13 Dec 2014
Schmidt, M., Hornung, T., Küchlin, N., Lausen, G., Pinkel, C., Schmidt, M., Hornung, T.,
Küchlin, N., Lausen, G., Pinkel, C.: An experimental comparison of RDF data management
approaches in a SPARQL benchmark scenario. Lecture Notes in Computer Science, vol. 5318,
LNCS, pp. 82–97 (2008)
Tversky, A.: Features of similarity. Psychol. Rev. 84, 327–352 (1977)
Wu, Z., Palmer, M.: Verbs semantics and lexical selection. In: Proceedings of the 32nd Annual
Meeting on Association for Computational Linguistics. Stroudsburg, PA, USA, pp. 133–138
(1994)
Zadeh, L.A.: Fuzzy sets. Inf. Control 8, 338–353 (1965)
Authors Biography
Parisa D. Hossein Zadeh received her B.Sc. degree in
Computer Engineering from University of Science and Tech-
nology, Tehran, Iran in 2006. She worked as a Computer
Engineer for DPI Iran Company (former IBM Co.) from 2005 to
2007, where she led multiple projects in Network and Software
ﬁelds. She received her M.Sc. degree in Electrical and Computer
Engineering from University of Alberta, Edmonton, Canada. She
is currently a Ph.D. candidate in Computer Engineering at
University of Alberta. Her research interests include Semantic
Web, similarity assessment, big data, search engines, fuzzy sets,
data analysis, and machine learning.
Marek Reformat received his M.Sc. degree (with honors) from
Technical University of Poznan, Poland, and his Ph.D. from
University of Manitoba, Canada. Presently, he is a professor
with the Department of Electrical and Computer Engineering,
University of Alberta.
The goal of his research activities is to develop methods and
techniques for intelligent data modeling and analysis leading to
translation of data into knowledge, as well as to design systems
that possess abilities to imitate different aspects of human
behavior. He recognizes the concepts of Computational Intelli-
gence – with fuzzy computing and possibility theory in particular
– are key elements necessary for capturing relationships between
pieces of data and knowledge, and for mimicking human ways of
The Web, Similarity, and Fuzziness
535

reasoning about opinions and facts. Dr. Reformat applies elements of fuzzy sets to social networks,
Linked Open Data, and Semantic Web in order to handle inherently imprecise information, and
provide users with unique facts retrieved from the data.
Dr. Reformat is a past president of the North American Fuzzy Information Processing Society, and
a vice president of the International Fuzzy Systems Association. He has been a member of program
committees of multiple international conferences related to Computational Intelligence and Software
Engineering.
536
P.D. Hossein Zadeh and M.Z. Reformat

The Genesis of Fuzzy Sets
and Systems – Aspects in Science
and Philosophy
Rudolf Seising
Abstract In 1965 LotﬁA. Zadeh founded the theory of Fuzzy Sets and Systems.
This chapter deals with developments in the history of philosophy, logic, and mathe-
matics during the time before and up to the beginning of fuzzy logic and it also gives
a view of its ﬁrst application in control theory. Regarding the term “fuzzy” we note
that older concepts of “vagueness” and “haziness” had previously been discussed in
philosophy, logic, mathematics. This chapter delineates some speciﬁc paths through
the history of the use of these “loose concepts ”. Haziness and fuzziness were con-
cepts of interest in mathematics and philosophy during the second half of the 20th
century. The logico-philosophical history presented here covers the work of Russell,
Black, Hertz, Wittgenstein and others. The mathematical-technical history deals with
the theories founded by Menger and Zadeh. Menger’s concepts of probabilistic met-
rics, hazy sets (ensembles ﬂous) and micro-geometry as well as Zadeh’s theory of
Fuzzy Sets paved the way for the establishment of Soft Computing methods. In the
ﬁrst decade of Fuzzy Sets and Systems, nobody thought that this theory would be
successful in the ﬁeld of applied sciences and technology. Zadeh expected that his
theory would have a role in the future of computer systems as well as Humanities
and Social Sciences. When Mamdani and Assilian picked up the idea of Fuzzy Algo-
rithms to establish a ﬁrst Fuzzy Control system for a small steam engine, this was the
Kick-oﬀfor the “Fuzzy Boom” and Zadehs primary intention trailed away for years.
Then in the new millennium a new movement for Fuzzy Sets in Social Sciences and
Humanities was launched.
1 Introduction
In the summer of 2015 we will commemorate the 50th anniversary of the theory
of Fuzzy Sets and Systems (FSS). This is a fuzzy jubilee because the exact time
when LotﬁA. Zadeh (born 1921, Fig. 1 (a)) discovered the concept of fuzzy sets is
R. Seising (✉)
Institut of History of Medicine, Natural Sciences and Technology “Ernst-Haeckel-Haus”,
Faculty of Biology and Pharmacy,
Friedrich Schiller University, Jena, Germany
e-mail: rudolf.markus.seising@uni-jena.de
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_27
537

538
R. Seising
unknown. The event is roughly assigned to the summer of 1965. This chapter
reviews the genesis of this new mathematical theory following my original (histor-
ical) research work that encompasses inspections of scientiﬁc articles, newspapers,
letters, and – most importantly – interviews with LotﬁA. Zadeh and other protago-
nists of the early years of the theory of FSS. For details see [1].
In Sects. 2 and 3 we brieﬂy follow Zadeh’s development as an electrical engineer
in the 1950s and 1960s from concrete Network theory and Filter theory to abstract
System theory and then to the theory of Fuzzy sets and Systems as a generalized Sys-
tem theory. In Sect. 4, we present short views on historical paths of modern logic and
philosophy of mathematics in the 20th century, the logical analysis of vagueness, the
concepts of statistical metrics and ensembles ﬂous and we consider Wittgenstein’s
late philosophy in relation to our subject. Section 5 gives a review of Zadehs works
on Fuzzy Sets in language and meaning that appeared before the “Fuzzy Boom”
with real-world application systems that is the subject of Sect. 6. Section 7 gives
an outlook on Zadeh’s later theories: Computing with words and with perceptions.
Finally the bibliogrpahy includes some comments to most of the references to this
book contribution.
2 From Electrical Engineering to System Theory
Fuzzy Sets and Systems can look back upon an eventful story in the scientiﬁc envi-
ronment of electrical engineering, including the initial system theory and computer
sciences known during this time, which were part of Zadeh’s training as a student in
Tehran, Iran. Following his immigration to the USA in 1942, Zadeh continued his
studies at the Massachusetts Institute of Technology (MIT) in Cambridge, Massa-
chusetts. He moved to New York in 1946, where he was awarded a Ph.D by Columbia
University in 1949. Since 1958, he has been a Professor of Electrical Engineering at
the University of California at Berkeley. When he established the theory of fuzzy sets
in the mid-1960s, he was already a well-known protagonist of the system theoretical
approach in electrical engineering, which was a new scientiﬁc trend from the 1950s
onward. Together with Charles A. Desoer (1926–2011, Fig. 1 (b)) he published in
1962 Linear System Theory: The State Space Approach [2], which became a standard
textbook. In 1963, together with Desoer’s former Ph.D. student Elijah Polak (born
1931, Fig. 1 (c)) he edited the volume System Theory [3]. In his own contribution
to this volume, which was entitled “The Concepts of System, Aggregate, and State
in System Theory” [4], Zadeh presented his state space approach. Two years later,
when he introduced fuzzy sets, he construed his new theory as a “general system
theory”.
In 1954 – Zadeh was then an instructor at Columbia University in New York - he
wrote for the Columbia Engineering Quarterly an article, named “System Theory”
[5] that begins as follows: “If you never heard of system theory, you need not feel
like an ignoramus. It is not one of the well-established branches of science. In fact, it
has not yet been oﬃcially recognized as a scientiﬁc discipline. It does not appear on

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
539
programmes of meetings of scientiﬁc societies nor in indices to scientiﬁc publica-
tions. It does not have well-deﬁned boundaries, nor does it have settled objectives.”
[5, p. 34]. Zadeh emphasized that all scientiﬁc disciplines are concerned with sys-
tems, but the new branch, named system theory, considers systems as mathematical
constructs rather than physical objects: “The distinguishing characteristic of system
theory is its abstractness.” [5, p. 16] In this and later papers Zadeh quoted the deﬁni-
tion of a “system” from Webster’s Dictionary: A system is “an aggregation or assem-
blage of objects united by some form of interaction or interdependence.” (Fig. 2)
System theorists deal with abstract systems, “that is, systems whose elements
have no particular physical identity” [5, p. 16]; they deal with “black boxes”. Fig. 3
reproduces the illustration to this article – actually a “black box”!
Communication systems are a special type of systems that have been of interest
since the 1950s, when information and communication theory emerged as success-
ful scientiﬁc and technological disciplines. Zadeh was deeply involved in the devel-
opment of this new communication theory and its techniques when he delivered a
lecture on “Some Basic Problems in Communication of Information” at the meeting
of the Section of Mathematics and Engineering of the New York Academy of Sci-
ences in March 1952 [6]. He represented signals as ordered pairs (x(t), y(t)) of points
in a signal space Σ, which is imbedded in a function space with a delta-function
basis. This analogy between projection in a function space and ﬁltration by an ideal
ﬁlter led Zadeh to postulate a function symbolism of ﬁlters in the early 1950s [7].
Thus, N = N1 +N2 represents a ﬁlter consisting of two ﬁlters connected by addition,
N = N1N2 represents their tandem (sequential) combination and N = N1 ||N2 the
separation process (Fig. 4).
Fig. 1
LotﬁA. Zadeh,
Charles A. Desoer, and
Elijah Polak: colleagues at
the University of California,
Berkeley
Fig. 2
Block diagram of
interconnected objects,
[5, p. 17]
Later (in [8]), Zadeh discussed the concept of optimal ﬁlters as opposed to ideal
ﬁlters following Norbert Wiener’s work. Ideal ﬁlters are deﬁned as ﬁlters that achieve

540
R. Seising
a perfect separation of signal and noise. However, in reality there are no ideal ﬁlters,
e.g., an ideal low-pass ﬁlter should retain all frequency components until a certain
threshold until which all components should be fully suppressed. In practice, we can-
not have such a step-shaped separation. Zadeh knew that we get a smooth transition
from 1 to 0 transmission coeﬃcient as the frequency decreases. These transitions
are similar to the well-known membership functions of fuzzy sets. However, in the
1950s the time was not ripe for this new mathematical theory. Zadeh deﬁned optimal
ﬁlters as those that give the “best approximation” of a signal, and he noted that “best
approximations” depend on reasonable criteria. At this time he formulated these cri-
teria in statistical terms.
Fig. 3
Illustration from
Zadeh’s article [5]
Fig. 4
Functional
symbolism of ideal ﬁlters,
[7, p. 225]
But starting in the next decade he wrote the landmark article “From Circuit
Theory to System” for the anniversary edition of the Proceedings of the IRE that
appeared in May 1962 to mark the 50th year of the Institute of Radio Engineers
(IRE) [9]. Here, he could outline problems and applications of system theory and
its relations to network theory, control theory, and information theory. Furthermore,
he pointed out “that the same abstract ‘systems’ notions are operating in various
guises in many unrelated ﬁelds of science is a relatively recent development. It has
been brought about, largely within the past two decades, by the great progress in our
understanding of the behaviour of both inanimate and animate systems—progress

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
541
which resulted on the one hand from a vast expansion in the scientiﬁc and techno-
logical activities directed toward the development of highly complex systems for
such purposes as automatic control, pattern recognition, data-processing, commu-
nication, and machine computation, and, on the other hand, by attempts at quanti-
tative analyses of the extremely complex animate and man-machine systems which
are encountered in biology, neurophysiology, econometrics, operations research and
other ﬁelds” [9, p. 856f].
In this article Zadeh used for the very ﬁrst time the word “fuzzy” and he wrote it
down to characterize his vision of new mathematics:
In fact, there is a fairly wide gap between what might be regarded as ‘animate’ system theo-
rists and ‘inanimate’ system theorists at the present time, and it is not at all certain that this
gap will be narrowed, much less closed, in the near future. There are some who feel that this
gap reﬂects the fundamental inadequacy of the conventional mathematics – the mathemat-
ics of precisely-deﬁned points, functions, sets, probability measures, etc. – for coping with
the analysis of biological systems, and that to deal eﬀectively with such systems, which are
generally orders of magnitude more complex than man-made systems, we need a radically
diﬀerent kind of mathematics, the mathematics of fuzzy or cloudy quantities which are not
describable in terms of probability distributions. [9, p. 857f].
However, when Zadeh published these notions, he did not know what this math-
ematics of fuzzy quantities would look like.
Another method to deal with imperfect or noisy signals in communication systems
was introduced in the 1950s by Richard E. Bellman (1920–1984, Fig. 5 (a)), a young
mathematician working at the RAND Corporation, United States Air Force Project
in Santa Monica, California. Bellman was the founder of the method of Dynamic
Programming [10], and tried to apply his “principle of optimality” in communication
theory.
In the late 1950s, Bellman met Zadeh in New York, where Zadeh worked at
Columbia University. Their friendship lasted until Bellman’s death in 1984. Even
though they considered diverse mathematical aspects of electrical engineering, sys-
tem theory and, later, computer science, they met each other very often and discussed
several aspects of their scientiﬁc work. Bellman was the ﬁrst and most important
critic of Zadeh’s new theory of fuzzy sets in 1965.
Fig. 5
(a): Richard E.
Bellman and (b): Robert E.
Kalaba, (c): LotﬁA. Zadeh

542
R. Seising
3 New View on System Theory: Fuzzy Sets and Systems
Zadeh and Bellman planned to work together at RAND in Santa Monica, California,
in the summer of 1964. Prior to the summer of 1964, Zadeh gave a talk on pattern
recognition at the Wright-Patterson Air Force Base, Dayton, Ohio. It may have been
on this occasion that he started thinking about the use of grades of membership for
pattern classiﬁcation and that he conceived the ﬁrst example of fuzzy mathematics,
which he wrote in one of his ﬁrst papers on the subject: “For example, suppose that
we are concerned with devising a test for diﬀerentiating between handwritten letters
O and D. One approach to this problem would be to give a set of handwritten letters
and indicate their grades of membership in the fuzzy sets O and D. On performing
abstraction on these samples, one obtains the estimates ̃𝜇O and ̃𝜇D of 𝜇O and 𝜇D,
respectively. Then given a letter x which is not one of the given samples, one can
calculate its grades of membership in O and D; and, if O and D have no overlap,
classify x in O or D.” [11, p. 30]
In a few days he extended this concept to the theory of fuzzy sets and a few weeks
later, he discussed this preliminary version of the theory of fuzzy sets with Bellman.
Then he wrote his manuscript on “Fuzzy Sets” [12] and submitted it to the journal
Information and Control in November 1964. “Fuzzy Sets” appeared in June 1965 and
was the ﬁrst article on fuzzy sets in a scientiﬁc journal. However, LotﬁZadeh also
wrote other papers at the time. According to common practice at the department
of electrical engineering in Berkeley, the article “Fuzzy Sets” was preprinted as a
report of the Electronics Research Laboratory in November 1964 [13]. As a result
of his talk in Dayton, Ohio, he wrote a paper which he sent to Bellman who was the
editor of the Journal of Mathematical Analysis and Applications. Bellman agreed to
publish the paper in said journal but the publication appeared late, in 1966, under
the title “Abstraction and Pattern Classiﬁcation” [14]. The authors of the article were
Bellman, his associate Robert E. Kalaba (1926–2004, Fig. 5 (b)) and Zadeh. The text
and the authors’ names are identical to those of the RAND memorandum RM-4307-
PR, which appeared as early as October 1964. This memo was written by Zadeh
alone. Here he deﬁned fuzzy sets for the ﬁrst time in a scientiﬁc paper, establishing
a general framework for the treatment of pattern recognition problems [15].
In April 1965 the Symposium on System Theory was held at Polytechnic Institute
in Brooklyn. At this meeting Zadeh presented “A New View on System Theory”:
a view that deals with the concepts of fuzzy sets, “which provide a way of treating
fuzziness in a quantitative manner.” In the subsequent publication of the proceedings
of this symposium we ﬁnd a shortened manuscript version of the talk. His contribu-
tion was entitled “Fuzzy Sets and Systems” in this publication: [11, p. 29]. In this
lecture and in the paper, Zadeh ﬁrst deﬁned “fuzzy systems” as follows:
A system S is a fuzzy system if (input) u(t), output y(t), or state s(t) of S or any combination
of them ranges over fuzzy sets, [11, p. 33].
In “Fuzzy Sets and Systems” Zadeh explained that “these concepts relate to sit-
uations in which the source of imprecision is not a random variable or a stochastic

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
543
process but rather a class or classes which do not possess sharply deﬁned bound-
aries.” [11, p. 29] His “simple” examples in this brief summary of his new “way of
dealing with classes in which there may be intermediate grades of membership” were
“the ‘class’ of real numbers which are much larger than, say, 10, and “the ‘class’ of
‘bald man’, and also the ‘class’ of adaptive systems.” [11, p. 29] For further details
on the roots of fuzzy systems in system theory, the reader is referred to the author’s
book [1].
In “Fuzzy Sets” [12], Zadeh introduced the new mathematical entities “fuzzy
sets”: “Such classes are not classes or sets in the usual sense of these terms, since
they do not dichotomize all objects into those that belong to the class and those that
do not.” He introduced “the concept of a fuzzy set, that is a class in which there may
be a continuous inﬁnity of grades of membership, with the grade of membership of
an object x in a fuzzy set A represented by a number fA(x) in the interval [0, 1].”1
Zadeh maintained that these new concepts provide a “convenient way of deﬁning
abstraction — a process which plays a basic role in human thinking and communi-
cation.” [11, p. 29] The question was how to generalize various concepts, union of
sets, intersection of sets, and so forth. Zadeh deﬁned equality, containment, com-
plementation, intersection and union relating to fuzzy sets A, B in any universe of
discourse X as follows (for all x ∈X; see Fig. 6):
∙A = B if and only if fA(x) = fB(x),
∙A ⊆B if and only if fA(x) ≤fB(x),
∙¬A is the complement of A if and only if f¬A(x) = 1 −fA(x),
∙A = ∪B if and only if fA∩B(x) = max(fA(x), fB(x)),
∙A = ∩B if and only if fA∩B(x) = min(fA(x), fB(x)),
For his interpretation of fuzzy unions and intersections he wrote a separate para-
graph, which shows a very important analogy to sieves, because Zadeh wrote:
“Speciﬁcally, let fi(x), i = 1, … , n, denote the value of the membership function
of Ai at x. Associate with fi(x) a sieve Si(x) whose meshes are of size fi(x). Then,
Fig. 6
Illustration of the union as maximum of membership functions fA and fB (1, 2) and the
intersection as minimum of membership functions fA and fB (3, 4), [12]
1Later Zadeh and also the whole “fuzzy community” made use of the greek letter 𝜇to mark the
membership function in Fuzzy Set Theory.

544
R. Seising
fi(x) ∨fj(x) and fi(x) ∧fj(x) correspond, respectively, to parallel and series combina-
tions of Si(x) and Sj(x) ,” as shown in Fig. 7.
More generally, a well-formed expression involving Ai, … , An ∪and ∩corre-
sponds to a network of sieves Si(x), … , Sn(x) which can be found by the conventional
synthesis techniques for switching circuits. As a very simple example,
C = [(A1 ∪A2) ∩A3
] ∪A4
(1)
corresponds to the network shown in Fig. 8.” [12, p. 344]
If the reader takes into account the fact that the term “sieve” denotes a ﬁlter, he
will comprehend the analogy of fuzzy sets and electrical ﬁlters as outlined in the ﬁrst
section.
In the decade that followed the ﬁrst publications on Fuzzy Sets and Systems [11–
15] Zadeh expected that they would have a role in the future of computer systems
as well as humanities and social sciences. At that time nobody thought that this
theory would be successful in the ﬁeld of applied sciences and technology. Quite
the contrary, he remarked that he did not expect the incorporation of FSS into the
ﬁelds of sciences and engineering: “What we still lack, and lack rather acutely, are
methods for dealing with systems which are too complex or too ill-deﬁned to admit
of precise analysis. Such systems pervade life sciences, social sciences, philosophy,
economics, psychology and many other ‘soft’ ﬁelds.” [16]
After “Fuzzy Sets” had appeared in print, Zadeh received many requests for oﬀ-
prints. Philosopher Max Black (1909–1988) who had published a paper entitled
Fig. 7
Parallel and serial
combination of sieves
illustrating the fuzzy union,
∪, (maximum) and
intersection, ∩, (minimum),
[12]
Fig. 8
A network of sieves
simulating
{(f1(x) ∨f2(x)) ∧f3(x)}
∨f4(x), [12]

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
545
“Vagueness – An Exercise in Logical Analysis” [17] back in 1937 anticipated a vague
idea from Zadeh’s theory , for he wrote:
The vagueness of the word chair is typical of all terms whose application involves the use
of the senses. In all such cases, ‘borderline case’ and ‘doubtful objects’ are easily found to
which we are unable to say either that the class name does or does not apply. [17, p. 434]
He now told Zadeh in a letter:
You were good enough to send me, some time ago, some of your recent papers on topics
connected with ‘Fuzzy Sets’. If I have not written before, the reason has not been lack of
interests, but an inescapable press of other duties. Now that I have had a chance, at least, to
study your work, I want to express my admiration and interest. I believe that your ingenious
construction promises to provide intellectual tools of great value. In case you have not come
across it, I might draw your attention to an early article of mine ... [18]
He referred to his article [17], that was also already reprinted in his book, Lan-
guage and Philosophy [19] and also to his “more recent article on similar topics
...”: “Reasoning with Loose Concepts” [20]. To understand the history of the log-
ical analysis of vagueness let’s go back to the history of modern science and its
philosophy!
4 Philosophy of Science, Vagueness and Fuzzy Sets
Beginning as early as the 17th century, a primary quality factor in scientiﬁc work has
been a maximal level of exactness. Galileo Galilei (1564–1642) and René Descartes
(1596–1650) started the process of giving modern science its preciseness through
the use of the tools of logic and mathematics. The language of mathematics has
served as a basis for the deﬁnition of theorems, axioms, and proofs. The works of
Isaac Newton (1643–1727), Gottfried Wilhelm Leibniz (1646–1769, Pierre-Simon
Laplace (1749–1827), and many others led to the ascendancy of modern science, fos-
tering the impression that scientists were able to represent – completely and exactly
– all the facts and processes that people observe in the world. But this optimism
has gradually begun to seem somewhat nave in view of the discrepancies between
the exactness of theories and what scientists observe in the real world. From the
empiricist point of view the source of our knowledge is sense experience. John Locke
(1632–1704) used the analogy of the mind of a newborn baby as a “tabula rasa” that
would be written by the sensual perceptions the child has later. In Locke’s opin-
ion these perceptions provide information about the physical world. Locke’s view is
called “material empiricism” whereas so-called idealistic empiricism was the posi-
tion of George Berkeley (1685–1753) and David Hume (1711–1776): the material
world does not exist; only perceptions are real. Immanuel Kant (1724–1804, Fig. 9
(a)) achieved a synthesis of rationalism and empiricism in his magnum opus Cri-
tique of Pure Reason, published in 1781 [21]). Kant argued that human experience
of a world is only possible if the mind provides a systematic structuring of its repre-
sentations that is logically prior to the mental representations that were analyzed by

546
R. Seising
empiricists and rationalists. With these philosophical views alone, we would not be
able to explain the nature of our experience because these views only considered the
results of the interaction between our mind and the world, but not the contribution
made by the mind. Kant concluded that it must be the minds structuring that makes
experience possible.
4.1 Heinrich Hertz’ Philosophy of Science
In his book The Principles of Mechanics Presented in a New Form the German physi-
cist Heinrich Hertz (1857–1894, Fig. 9 (b)) had established his theory of knowledge:
he viewed physical theories as “pictures” of reality. He began his introduction with
the following words:
The most direct, and in a sense the most important, problem which our conscious knowledge
of nature should enable us to solve is the anticipation of future events, so that we may arrange
our present aﬀairs in accordance with such anticipation. As a basis for the solution of this
problem we always make use of our knowledge of events which have already occurred,
obtained by chance observation or by pre-arranged experiment. In endeavoring thus to draw
inferences as to the future from the past, we always adopt the following process. We form
for ourselves images or symbols of external objects; and the form which we give them is
such that the necessary consequents of the images in thought are always the images of the
necessary consequents in nature of the things pictured. [...]
The images which we here speak of are our conceptions of things. With the things them-
selves they are in conformity in one important respect, namely, in satisfying the above-
mentioned requirement. For our purpose it is not necessary that they should be in conformity
with the things in any other respect whatever. As a matter of fact, we do not know, nor have
we any means of knowing, whether our conceptions of things are in conformity with them
in any other than this one fundamental respect. [22, p. 1]
We know from experience the conformity between nature and our mind that is
necessary for that: (logically) inadmissible images are “all images which implic-
itly contradict the laws of our thought.” Although images are logically admissible,
they can be incorrect “if their essential relations contradict the relations of external
things.”
For one external object there can exist more than one correct image, diﬀering in
respect to appropriateness:
Of two images of the same object that is the more appropriate which pictures more of the
essential relations of the object, the one which we may call the more distinct. Of two images
of equal distinctness the more appropriate is the one which contains, in addition to the essen-
tial characteristics, the smaller number of superﬂuous or empty relations, the simpler of the
two. [22, p. 2]
Hertz’s epistemology and his view of scientiﬁc theories as mind-created “images”,
based on the scientist’s experience, was contrary to the dominant view at his time.
Most scientists during the years around the turn of the 20th century regarded empir-
ical theories as objective, and in particular, most of them believed in the existence of

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
547
one unique theory. On the other hand, Hertz knew from the experience he had gath-
ered in the genesis of electrodynamics that various theories with diﬀerent systems
of concepts are possible, and that one theory may eventually become accepted. In
his “Language of images”, he wrote:
What enters into the images for the sake of correctness is contained in the results of expe-
rience, from which the images are built up. What enters into the images, in order that they
may be permissible, is given by the nature of our mind. To the question whether an image is
permissible or not, we can without ambiguity answer yes or no; and our decision will hold
good for all time. And equally without ambiguity we can decide whether an image is correct
or not; but only according to the state of our present experience, and permitting an appeal
to later and riper experience. But we cannot decide without ambiguity whether an image
is appropriate or not; as to this diﬀerences of opinion may arise. One image may be more
suitable for one purpose, another for another; only by gradually testing many images can we
ﬁnally succeed in obtaining the most appropriate. [22, p. 3]
Hertz spoke about “images” or “symbols” of external objects, because they are
replacements for concepts in physical theories (e.g., mechanics, electricity and mag-
netism, and electrodynamics) that are not accessible to our sensory perceptions.
4.2 Wittgenstein’s Tractatus logico-philosophicus
“A picture is a model of reality.”, “We picture facts to ourselves.”, “A picture is a
fact.” These are three consecutive propositions in Ludwig Wittgenstein’s Tractatus
logico-philosophicus [23, prop. 2.1, 2.12, 2.141]. They demonstrate the inﬂuence
of Heinrich Hertz’s Principles of Mechanics on his thinking – a debt that Wittgen-
stein himself acknowledged when he referred to Hertz in his diary [24, p. 476] and
explicitly in another part of the Tractatus: “In the proposition there must be exactly
as many things distinguishable as there are in the state of aﬀairs which it represents.
They must both possess the same logical (mathematical) multiplicity (cf. Hertz’s
Mechanics, on Dynamic Models).” [23, prop. 4.04]
The ﬁrst two propositions in Wittgenstein’s Tractatus are:
1. The world is everything that is the case.
2. The world is the totality of facts, not of things. [23]
Fig. 9
(a): Immanuel Kant
(b): Heinrich Hertz and
(c): Ludwig Wittgenstein

548
R. Seising
Then, in the Tractatus, Wittgenstein (1889–1951, Fig. 9 (c)) wrote that the world
consists of facts. Facts may or may not contain smaller parts. If a fact has no smaller
parts, he calls it an “atomic fact.” If we know all atomic facts, we can describe the
world completely by corresponding “atomic propositions.” – Propositions 3 and 4 in
the Tractatus are:
3. The logical picture of the facts is the thought.
4. The thought is the signiﬁcant pro position. [23]
“The totality of propositions is language.” [23, prop. 4.001] Wittgenstein argued
that sentences in colloquial language are very complex. He conceded that there is
a “silent adjustment to understand colloquial language” but it is “enormously com-
plicated.” Therefore it is “humanly impossible to gather immediately the logic of
language.” [23, prop. 4.002] This is the task of philosophy: “All philosophy is ‘Cri-
tique of language’.” [23, prop. 4.0031] Wittgenstein knew that common linguistic
usage is vague, but at the time when he wrote the Tractatus, he tried to solve this
problem by constructing a precise language – an exact logical language that gives a
unique picture of the real world. Wittgenstein thought that the Tractatus solved all
philosophical problems.
But the Tractatus spared many problems for the future! One of these philosophical
problems concerns the vagueness in our language and also two founders of modern
logic, Gottlob Frege (1848–1925) and Bertrand Russell (1872–1970), focused atten-
tion on and analyzed this problem. A separate and isolated development took place
at the Lvov-Warsaw School of logicians; one of them was Jan Łukasiewicz (1878–
1956) who introduced in 1920 a three-valued logic and later other multi-valued log-
ics. Simultaneously the American mathematician, Emil L. Post (1897–1954), also
introduced a logic of additional truth degrees with n ≥2, where n are the truth
values.
The important contributions of these Polish mathematicians and logicians to mod-
ern logic were recognized when Alfred Tarski (1902–1983) followed an invitation
of the Viennese mathematician Karl Menger to give a lecture in Vienna. All these
thinkers had been inﬂuenced by Frege’s studies. This was especially true of Tadeusz
J. S. Kortabinśki (1866–1938), who argued that a concept for a property is vague
(Polish: chwiejne) if the property may be the case by grades [25] and Kazimierz
Ajdukiewicz (1890–1963), who stated the deﬁnition that “a term is vague if and
only if its use in a decidable context … will make the context undecidable in virtue of
those [language] rules” [26]. The Polish characterization of “vagueness” was there-
fore the existence of ﬂuid boundaries.
4.3 Vagueness and Logic
The German philosopher and mathematician Gottlob Frege confronted the problem
of vagueness when formalizing the mathematical principle of complete induction:
he saw that some predicates are not inductive, viz. they have been deﬁned for all

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
549
natural numbers, but they result in false conclusions, e.g. the predicate “heap” can-
not be evaluated for all natural numbers [27]. When he revised the basics of his
Begriﬀsschrift for a lecture to the Society of Medicine and Science in Jena, Germany,
of the beginning year 1891, Frege reinterpreted concept functions and subsequently
he introduced these functions of concepts everywhere. He stated: If “x+1” is mean-
ingless for all arguments x, then the function x + 1 = 10 has no value and no truth
value either. Thus, the concept “that which when increased by 1 yields 10” would
have no sharp boundaries. Accordingly, for functions the demand on sharp bound-
aries entails that they must have a value for every argument [28]. This is a mathe-
matical verbalization of what is called the classical sorites paradox that can be traced
back to the old Greek word 𝜎o𝜌̄𝜔𝜍(for “heap”) used by Eubulid of Alexandria (4th
century BC). In his Grundgesetze der Arithmetik (Foundations of Arithmetic) that
appeared in the years 1893–1903, Frege called for concepts with sharp boundaries,
because otherwise we could break logical rules and, moreover, the conclusions we
draw could be false [29]. Frege’s speciﬁcation of vagueness as a particular phenom-
enon inﬂuenced other scholars, notably his British contemporary and counterpart
the philosopher and mathematician Bertrand Russell (1872–1970, Fig. 9 (b)), who
published his article on “Vagueness” in 1923 [30].
Russell quoted the sorites—in fact, he did not use mathematical language in this
article, but, for example, discussed colours and “bald men” (Greek: 𝜑𝛼𝜆𝛼𝜅𝜌o𝜍,
falakros, English: fallacy, false conclusion):
Let us consider the various ways in which common words are vague, and let us begin with
such a word as red. It is perfectly obvious, since colours form a continuum, that there are
shades of color concerning which we shall be in doubt whether to call them red or not, not
because we are ignorant of the meaning of the word red, but because it is a word the extent
of whose application is essentially doubtful. This, of course, is the answer to the old puzzle
about the man who went bald. It is supposed that at ﬁrst he was not bald, that he lost his
hairs one-by-one, and that in the end he was bald; therefore, it is argued, there must have
been one hair the loss of which converted him into a bald man. This, of course, is absurd.
Baldness is a vague conception; some men are certainly bald, some are certainly not bald,
while between them there are men of whom it is not true to say they must either be bald or
not bald. [30, p. 85].
Russell also argued that a proper name – and here we can take as an example the
name “LotﬁZadeh” – cannot be considered to be an unambiguous symbol even if
we believe that there is only one person with this name. LotﬁZadeh “was born, and
being born is a gradual process. It would seem natural to suppose that the name was
not attributable before birth; if so, there was doubt, while birth was taking place,
whether the name was attributable or not. If it be said that the name was attributable
before birth, the ambiguity is even more obvious, since no one can decide how long
before birth the name become attributable.” [30, p. 86]
Russell reasoned “that all words are attributable without doubt over a certain area,
but become questionable within a penumbra, outside which they are again certainly
not attributable.” [30, p. 86f] Then he generalized that words of pure logic also have
no precise meanings, e.g. in classical logic the composed proposition “p or q” is
false only when p and q are false and true elsewhere. He went on to claim that the

550
R. Seising
truth values “ ‘true’ and ‘false’ can only have a precise meaning when the symbols
employed – words, perceptions, images … – are themselves precise”. As we have
seen above, this is not possible in practice, so he concludes “that every proposition
that can be framed in practice has a certain degree of vagueness; that is to say, there
is not one deﬁnite fact necessary and suﬃcient for its truth, but certain region of
possible facts, any one of which would make it true. And this region is itself ill-
deﬁned: we cannot assign to it a deﬁnite boundary.” Russell emphasized that there is
a diﬀerence between what we can imagine in theory and what we can observe with
our senses in reality: “All traditional logic habitually assumes that precise symbols
are being employed. It is therefore not applicable to this terrestrial life, but only to
an imagined celestial existence.” [30, p. 88f]. He proposed the following deﬁnition
of accurate representations:
One system of terms related in various ways is an accurate representation of another sys-
tem of terms related in various other ways if there is a oneone relation of the terms of the
one to the terms of the other, and likewise a oneone relation of the relations of the one
to the relations of the other, such that, when two or more terms in the one system have a
relation belonging to that system, the corresponding terms of the other system have the cor-
responding relation belonging to the other system.” And in contrast to this, he stated that
“a representation is vague when the relation of the representing system to the represented
system is not oneone, but onemany. [30, p. 89]
He concluded that “Vagueness, clearly, is a matter of degree, depending upon the
extent of the possible diﬀerences between diﬀerent systems represented by the same
representation. Accuracy, on the contrary, is an ideal limit.” [30, p. 90].
The Cambridge philosopher and mathematician Max Black (1909–1988, Fig. 9
(b)) responded to Russell’s article in his already mentioned article of 1937 [17]. He
diﬀerentiated vagueness from ambiguity, generality, and indeterminacy. He empha-
sized
that the most highly developed and useful scientiﬁc theories are ostensibly expressed in
terms of objects never encountered in experience. The line traced by a draughtsman, no
matter how accurate, is seen beneath the microscope as a kind of corrugated trench, far
removed from the ideal line of pure geometry. And the ‘point-planet’ of astronomy, the
‘perfect gas’ of thermodynamics, and the ‘pure species’ of genetics are equally remote from
exact realization.” [17, p. 427]
Black proposed a new method to symbolize vagueness: “a quantitative diﬀeren-
tiation, admitting of degrees, and correlated with the indeterminacy in the divisions
Fig. 10
(a): Bertrand
Russell, (b): Max Black
and (c): Karl Menger

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
551
made by a group of observers.” [17, p. 441] He assumed that the vagueness of a
word involves variations in its application by diﬀerent users of a language and that
these variations fulﬁll systematic and statistical rules when one symbol has to be
discriminated from another. He referred to situations in which a user of the language
makes a decision whether to apply L or ¬L to an object x. Black exempliﬁed: “Such
a situation arises, for instance, when an engine driver on a foggy night is trying to
decide whether the light in the signal box is really a red or a green light” [17, p. 442]
He deﬁned this discrimination of a symbol x with respect to a symbol L by DxL.
(We obtain DxL = Dx¬L by deﬁnition.) Most speakers of a language and the same
observer in most situations will determine that either L or ¬L is used. In both cases,
among competent observers there is a certain unanimity, a preponderance of correct
decisions. For all DxL with the same x but not necessarily the same observer, m is
the number of L uses and n the number of ¬L uses. On this basis, Black stated the
following deﬁnition (see Fig. 11):
We deﬁne the consistency of application of L to x as the limit to which the ratio m
n tends
when the number of DxL and the number of observers increase indeﬁnitely. […] Since the
consistency of the application, C, is clearly a function of both L and x, it can be written in
the form C(L, x).” [17, p. 442]
In 1963, Black labeled concepts without precise boundaries as “loose concepts”
rather than “vague” ones, in order to avoid misleading and pejorative implications
[20]. Once again he expressly rejected Russell’s assertion that traditional logic is
“not applicable” as a method of conclusion for vague concepts: “Now, if all empirical
Fig. 11
Consistency of application of a typical vague symbol, [17, p. 443]

552
R. Seising
concepts are loose, as I think they are, the policy becomes one of abstention from
any reasoning from empirical premises. If this is a cure, it is one that kills the patient.
If it is always wrong to reason with loose concepts, it will, of course, be wrong to
derive any conclusion, paradoxical or not, from premises in which such concepts are
used. A policy of prohibiting reasoning with loose concepts would destroy ordinary
language – and, for that matter, any improvement upon ordinary language that we
can imagine.” [20, p. 7]
4.4 Menger’s Ensembles Flous
In Vienna in the 1920s and 1930s, Karl Menger (1902–1985, Fig 10 (c)) evolved
into a specialist in topology and geometry, particularly with regard to the theories
of curves, dimensions, and general metrics. After he immigrated to the USA, he
continued his work on these subjects. In 1942, with the intention of generalizing the
theory of metric spaces more in the direction of probabilistic concepts, he introduced
the term “statistical metric”:
A statistical metric is “a set S such that with each two elements (‘points’) p and
q of S, a probability function 𝛱(x; p, q) (The probability that the distance between p
and q is x) is associated satisfying the following conditions:
1. 𝛱(0; p, p) = 1 (The probability is 1 that the distance between p and q is 0.)
2. If p ≠q, then 𝛱(0; p, p) < 1.
3. 𝛱(x; p, q) = 𝛱(x; q, p).
4. T(𝛱(x; p, q), 𝛱(y; q, r) ≤𝛱(x + y; p, r).
where T(𝛼, 𝛽) is a function deﬁned for 0 ≤𝛼≤1 and 0 ≤𝛽≤1, such that
(a) 0 ≤T(𝛼, 𝛽) ≤1.
(b) T is non-decreasing in either variable.
(c) T(𝛼, 𝛽) = T(𝛽, 𝛼).
(d) T(1, 1) = 1.
(e) If 𝛼> 0 then T(𝛼, 1) > 0. [31, p. 535f]
Condition 4., the “triangular inequality” of the statistical metric S implies the
following inequality for all points q and all numbers x between 0 and z:
𝛱(z; p, r) ≥Max; T(𝛱(x; p, q), 𝛱(z −x; q, r))
(2)
Here, Menger introduced the term triangular norm (t-norm) to indicate the func-
tion T.
In 1951 Menger introduced a new notation 𝛥ab for the non-decreasing cumula-
tive distribution function, associated with every ordered pair (a, b) of elements of a
set S and he wrote: “The value 𝛥ab(x) may be interpreted as the probability that the
distance from a to b be < x.” [32, p. 226] Much more interesting is the following
text passage: “We call a and b certainly-indistinguishable if 𝛥ab = 1 for each x > 0.

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
553
Uniting all elements which are certainly indistinguishable from each other into iden-
tity sets, we decompose the space into disjoint sets A, B, …. We may deﬁne for any
a belonging to A and b belonging to B. (The number is independent of the choice of
a and b.) The identity sets form a perfect analog of an ordinary metric space since
they satisfy the condition2:
If A ≠B, then there exists a positive x with 𝛥ab(x) < 1 .”
In the same year Menger addressed the diﬀerence between the mathematical con-
tinuum and the physical continuum. Regarding A, B, and C as elements of a con-
tinuum, he referred to a claim of the French mathematician and philosopher Henri
Poincaré, “that only in the mathematical continuum do the equalities A = B and
B = C imply the equality A = C. In the observable physical continuum, ‘equal’
means ‘indistinguishable’, and A = B and B = C by no means imply A = C. ‘The
raw result of experience may be expressed by the relation A = B, B = C, A < C
which may be regarded as the formula for the physical continuum.’ According to
Poincaré, physical equality is a non-transitive relation.” [33, p. 178]
Menger suggested a realistic description of the equality of elements in the physical
continuum by associating with each pair (A, B) of these elements the probability that
A and B will be found to be indistinguishable. He argued:
For it is only very likely that A and B are equal, and very likely that B and C are equal – why
should it not be less likely that A and C are equal? In fact, why should the equality of A and
C not be less likely than the inequality of A and C?” [33, p. 178]
To solve “Poincaré’s paradox” Menger used his concept of probabilistic relations
and geometry: For the probability E(a, b) that a and b would be equal he postulated:
∙E(a, a) = 1, for every a;
∙E(a, a) = E(b, a), for every a and b;
∙E(a, b) ⋅E(b, c) ≤E(a, c) for every a, b, c.
If E(a, a) = 1, then he called a and b certainly equal. (In this case we obtain the
ordinary equality relation.) “All the elements which are certainly equal to a may be
united to an ‘equality set’, A. Any two such sets are disjoint unless they are identical.”
[33, p. 179]
In addition to studies of well-deﬁned sets, he called for a theory to be developed
in which the relationship between elements and sets is replaced by the probability
that an element belongs to a set; in contrast to ordinary sets, he called these entities
“ensembles ﬂous” [34, p. 226]. Later, Menger used the English term “hazy set” and
to elucidate the contrast he referred to conventional sets as “rigid sets.” [35].
Menger never envisaged a mathematical theory of loose concepts that diﬀers from
probability theory. At a symposium of the American Association for the Advance-
ment of Science organized in 1966 to commemorate the 50th anniversary of Ernst
2In the original paper Menger wrote “>”. The present author thanks Erich Peter Klement for this
correction.

554
R. Seising
Mach’s death, he spoke about “Positivistic Geometry”. When he compared his
“micro geometry” with the theory of fuzzy sets – he wrote: “In a slightly diﬀerent
terminology, this idea was recently expressed by Bellman, Kalaba and Zadeh under
the name fuzzy set.” [35, p. 232] – He did not see that the “slight diﬀerence” between
“degrees” (fuzziness) and “probabilities” is a diﬀerence not just in terminology but
in the meaning of the concepts.
4.5 Wittgenstein’s Family Resemblances
In his later years (after 1947), Wittgenstein turned away from the epistemological
system of the Tractatus with its ideal mapping between the objects of reality and a
logically precise language. This philosophy of his later years is completely diﬀerent
from that of the Tractatus years. It seems as though the two philosophical systems
were created by diﬀerent men because this new view said: If we are not able to ﬁnd
such an exact logical language, then we have to accept the fact that there is vague
linguistic usage in all languages. Then the images, models, and theories that we build
with the words and propositions of our languages to communicate with them are and
will also be vague.
His second main work, the Philosophical Investigations epitomize Wittgenstein’s
late philosophy: “Instead of producing something common to all that we call lan-
guage, I am saying that these phenomena have no one thing in common which makes
us use the same word for all, but that they are related to one another in many diﬀerent
ways. And it is because of this relationship, or these relationships, that we call them
all ‘language’. I will try to explain this.” [36, §65] We ﬁnd the following explanation
in the next paragraph of this book, in keeping with the concept of a game:
Consider for example the proceedings that we call “games”. I mean board-games, card-
games, ball-games, Olympic games, and so on. What is common to them all? Don’t say:
“There must be something common, or they would not be called ‘games’ ” but look and see
whether there is anything common to all. For if you look at them you will not see something
that is common to all, but similarities, relationships, and a whole series of them at that.
To repeat: dont think, but look! Look for example at board-games, with their multifarious
relationships.
Now pass to card-games; here you ﬁnd many correspondences with the ﬁrst group, but
many common features drop out, and others appear. When we pass next to ball-games, much
that is common is retained, but much is lost. Are they all “amusing”? Compare chess with
noughts and crosses. Or is there always winning and losing, or competition between players?
Think of patience. In ball games there is winning and losing; but when a child throws his
ball at the wall and catches it again, this feature has disappeared. Look at the parts played
by skill and luck; and at the diﬀerence between skill in chess and skill in tennis. Think now
of games like ring-a-ring-a-roses; here is the element of amusement, but how many other
characteristic features have disappeared! sometimes similarities of detail. And we can go
through the many, many other groups of games in the same way; can see how similarities
crop up and disappear. And the result of this examination is: we see a complicated network
of similarities overlapping and crisscrossing: sometimes overall similarities. [36, §66]

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
555
In the next paragraph Wittgenstein creates a new concept to describe this new
epistemological system:
I can think of no better expression to characterize these similarities than “family resem-
blances”; for the various resemblances between members of a family: build, features, colour
of eyes, gait, temperament, etc. etc. overlap and crisscross in the same way. And I shall say:
“games” form a family. [36, §67]
Concepts and their families have no sharp boundaries, as he also wrote in [36, §71]:
One might say that the concept “game” is a concept with blurred edges. “But is a blurred
concept a concept at all?” Is an indistinct photograph a picture of a person at all? Is it even
always an advantage to replace an indistinct picture by a sharp one? Isn’t the indistinct one
often exactly what we need? Frege compares a concept to an area and says that an area with
vague boundaries cannot be called an area at all. This presumably means that we cannot do
anything with it. But is it senseless to say: “Stand roughly there”? [36, §71]
And in a later paragraph Wittgenstein wrote: “The results of philosophy are the
uncovering of one or another piece of plain nonsense and bumps that the under-
standing has got by running its head up against the limits of language.” [36, §119]
In other words, our conceptions, images, and symbols of external things or objects
are entities without sharp borders. They are fuzzy entities!
5 Before the “Fuzzy Boom”: Fuzziness
in Language and Meaning
In the 1960s Zadeh was interested in applying fuzzy sets in linguistics. This idea led
to interdisciplinary scientiﬁc exchange on the campus of the University of California
at Berkeley between him and the mathematicians Joseph Goguen (1941–2006, Fig
14 (a)) and Hans-Joachim Bremermann, the psychologist Eleanor Rosch (Heider)
and the linguist George Lakoﬀ. Goguen generalized the fuzzy sets to so-called “L-
sets” [37, 38]. An L-set is a function that maps the fuzzy set carrier X into a partially
ordered set L, i.e. L ∶X →L. The partially ordered set L Goguen called the “truth
set” of A. The elements of L can thus be interpreted as “truth values”; in this respect,
Goguen then referred to a “Logic of Inexact Concepts” [39]. His work was laid out
in terms of logical algebra and category theory, and his proof of a representation
theorem for L-sets within category theory justiﬁed fuzzy set theory as an expansion
of set theory.
5.1 Fuzzy Languages and Fuzzy Algorithms
In 1970 Zadeh presented his paper “Fuzzy Languages and their Relations to
Human and Machine Intelligence” at the conference Man and Computer in Bor-
deaux, France: He said: “As computers become more powerful and thus more inﬂu-

556
R. Seising
ential in human aﬀairs, the philosophical aspects of this question become increas-
ingly overshadowed by the practical need to develop an operational understanding
of the limitations of the machine judgment and decision making ability.” [40, p. 130]
He called it a paradox that the human brain is always solving problems by manip-
ulating “fuzzy concepts” and “multidimensional fuzzy sensory inputs” whereas “the
computing power of the most powerful, the most sophisticated digital computer in
existence” is not able to do this. Therefore, he stated that “in many instances, the
solution to a problem need not be exact, so that a considerable measure of fuzzi-
ness in its formulation and results may be tolerable. The human brain is designed
to take advantage of this tolerance for imprecision whereas a digital computer, with
its need for precise data and instructions, is not.” [40, p. 132] He intended to push
his theory of fuzzy sets to model the imprecise concepts and directives: “Indeed,
it may be argued that much, perhaps most, of human thinking and interaction with
the outside world involves classes without sharp boundaries in which the transition
from membership to non-membership is gradual rather than abrupt.” [40, p. 131] He
stated:
Although present-day computers are not designed to accept fuzzy data or execute fuzzy
instructions, they can be programmed to do so indirectly by treating a fuzzy set as a data-
type which can be encoded as an array […]. Granted that this is not a fully satisfactory
approach to the endowment of a computer with an ability to manipulate fuzzy concepts, it
is at least a step in the direction of enhancing the ability of machines to emulate human
thought processes. It is quite possible, however, that truly signiﬁcant advances in artiﬁcial
intelligence will have to await the development of machines that can reason in fuzzy and
non-quantitative terms in much the same manner as a human being. [40, p. 132]
In August 1967, the Filipino electrical engineer William Go Wee at Purdue Uni-
versity in Indiana had submitted his dissertation “On Generalizations of Adaptive
Algorithms and Application of the Fuzzy Sets Concept to Pattern Classiﬁcation”
[41] that he had written under King Sun Fu, one of the pioneers in the ﬁeld of pat-
tern recognition. Wee had applied the fuzzy sets to iterative learning procedures for
pattern classiﬁcation and had deﬁned a ﬁnite automaton based on Zadeh’s concept
of the fuzzy relation as a model for nonsupervised learning systems: “The decision
maker operates deterministically. The learning section is a fuzzy automaton. The per-
formance evaluator serves as an unreliable ‘teacher’ who tries to teach the ‘student’
to make right decisions.” [40, p. 101]
The fuzzy automaton representing the learning section implemented a “non-
supervised” learning fuzzy algorithm and converged monotonically. Wee showed
that this fuzzy algorithm could not only be used in the area of pattern classiﬁcation
but could also be translated to control and regulation problems. Working with his
doctoral advisor, Wee presented his ﬁndings in the article “A Formulation of Fuzzy
Automata and its Applications as a Model of Learning Systems” [42].
In 1968 Zadeh presented “fuzzy algorithms” [43]. Usual algorithms depend upon
precision. An algorithm must be completely unambiguous and error-free in order to
result in a solution. The path to a solution amounts to a series of commands which
must be executed in succession. Algorithms formulated mathematically or in a pro-
gramming language are based on set theory. Each constant and variable is precisely

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
557
deﬁned; every function and procedure has a deﬁnition set and a value set. Each com-
mand builds upon them. Successfully running a series of commands requires that
each result (output) of the execution of a command lies in the deﬁnition range of
the following command, that it is, in other words, an element of the input set for
the series. Not even the smallest inaccuracies may occur when deﬁning these coor-
dinated deﬁnition and value ranges. He now saw “that in real life situations people
think certain things. They think like algorithms but not precisely deﬁned algorithms
[40]. Inspired by this idea, he wrote:
Essentially, its purpose is to introduce a basic concept which, though fuzzy rather than pre-
cise in nature, may eventually prove to be of use in a wide variety of problems relating to
information processing, control, pattern recognition, system identiﬁcation, artiﬁcial intelli-
gence and, more generally, decision processes involving incomplete or uncertain data. The
concept in question will be called fuzzy algorithm because it may be viewed as a general-
ization, through the process of fuzziﬁcation, of the conventional (nonfuzzy) conception of
an algorithm. [40, p. 94]
To illustrate, fuzzy algorithms may contain fuzzy instructions such as:
(a) “Set y approximately equal to 10 if x is approximately equal to 5,” or
(b) “If x is large, increase y by several units,” or
(c) “If x is large, increase y by several units; if x is small, decrease y by several units;
otherwise keep y unchanged.”
The sources of fuzziness in these instructions are fuzzy sets which are identiﬁed
by their underlined names. [40, p. 94f]
All people function according to fuzzy algorithms in their daily life, Zadeh wrote
– they use recipes for cooking, consult the instruction manual to ﬁx a TV, follow
prescriptions to treat illnesses or heed the appropriate guidance to park a car. Even
though activities like this are not normally called algorithms: “For our point of view,
however, they may be regarded as very crude forms of fuzzy algorithms.” [40, p. 95]
Already in 1969 Zadeh contributed to a NATO summer school on “Architecture
and Design of Digital Computers” in Grenoble with the title “Toward Fuzziness in
Computer Systems. Fuzzy Algorithms and Languages” [44] The association of fuzzi-
ness and computers must have sounded surprising in the late 1960s and referring to
that Zadeh said in its introduction: “At ﬁrst glance, it may appear highly incongru-
ous to mention computers and fuzziness in the same breath, since fuzziness connotes
imprecision whereas precision is a major desideratum in computer design.” [44, p. 9]
In the following paragraphs Zadeh justiﬁed by with arguing that future com-
puter systems will have to perform many more complex information processing tasks
than the computers that he and his contemporaries in the 1960s knew. He expected
that future computers would have to process more and more imprecise information!
“Fuzziness, then, is a concomitant of complexity. This implies that as the complex-
ity of a task or a system for performing that task exceeds a certain threshold, the
system must necessarily become fuzzy in nature. Thus, with the rapid increase in
the complexity of the information processing tasks which the computers are called
upon to perform, a point is likely to be reached perhaps within the next decade when

558
R. Seising
the computers will have to be designed for processing of information in fuzzy form.
In fact, it is this capability – a capability which present-day computers do not pos-
sess – that distinguishes human intelligence from machine intelligence. Without such
capability we cannot build computers that can summarize written text, translate well
from one natural language to another, or perform many other tasks that humans can
do with ease because of their ability to manipulate fuzzy concepts.” [44, p. 10] For
that purpose, Zadeh asserted that “intriguing possibilities for computer systems” are
oﬀered by fuzzy algorithms and fuzzy languages!
To execute fuzzy algorithms by computers they have to get an expression in fuzzy
programming languages. Consequently the next step for Zadeh was to deﬁne fuzzy
languages. “All languages, whether natural of artiﬁcial, tend to evolve and rise in
level through the addition of new words to their vocabulary. These new words are,
in eﬀect, names for ordered subsets of names in the vocabulary to which they are
added.” [44, p. 16]
Real world phenomena are very complex. To characterize or picture these phe-
nomena in terms of our natural languages we use our vocabulary and because this
set of words is restricted, Zadeh argued that this process leads to fuzziness:
Consequently, when we are presented with a class of very high cardinality, we tend to group
its elements together into subclasses in such a way as to reduce the complexity of the infor-
mation processing task involved. When a point is reached where the cardinality of the class
of subclasses exceeds the information handling capacity of the human brain, the boundaries
of the subclasses are forced to become imprecise and fuzziness becomes a manifestation of
this imprecision. This is the reason why the limited vocabulary we have for the description
of colors makes it necessary that the names of colors such as red, green, bleu [sic.], purple,
etc. be, in eﬀect, names of fuzzy rather than non-fuzzy sets. This is why natural languages,
which are much higher in level than programming languages, are fuzzy whereas program-
ming languages are not. [44, p. 10]
Here, Zadeh argued explicitly for programming languages that are – because of
missing rigidness and preciseness and because of their fuzziness – more like natural
languages. He mentioned the concept of stochastic languages that was published
by the Finnish mathematician Paavo Turakainen in Information and Control in the
foregoing year [45], being such an approximation to our human languages using ran-
domizations in the productions, but Zadeh preferred fuzzy productions to achieve a
formal fuzzy language. Then, he presented a short sketch of his program to extend
non-fuzzy formal languages to fuzzy languages which he published in elaborated
form with the co-author Edward T.-Z. Lee in “Note on Fuzzy Languages” [46]. His
deﬁnition in these early papers was given in the terminology of the American com-
puter scientists John Edward Hopcroft and Jeﬀrey David Ullman that was published
in the same year [47].
L is a fuzzy language if it is a fuzzy set in the set V∗
T, the so-called “Kleene closure
of VT, the set of all ﬁnite strings composed of elements of the ﬁnite set of terminals
VT, e.g. VT = {a, b, c, … , z} .The membership function 𝜇L(x) ∶V∗
T →[0, 1] asso-
ciates with each ﬁnite string x, composed of elements in VT, its grade of membership
in L. Here is one of the simple examples that he gave in this article [44]:

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
559
Assume that VT = {0, 1}, and take L to be the fuzzy set L = {(0, 0.9), (1, 0.2), (00, 0.8),
(01, 0.3), (10, 0.7), (11, 0.3)} with the understanding that all the other strings in V∗
T do
not belong to L (i.e., have grade of membership equal to zero). [44, p. 16].
In general the language L has high cardinality and therefore it is not usual to deﬁne
it by a listing of its elements but by a ﬁnite set of generating rules. Thus, in analogy
to the case of non-fuzzy languages Zadeh deﬁned a fuzzy grammar as
a quadruple G = (VN, VT, P, S), where VN is a set of variables (non-terminals) disjoint from
VT, P is a set of [fuzzy] productions and S is an element of VN. The elements of VN (called
[fuzzy] syntactic categories) and S is an abbreviation for the syntactic category ‘sentence’.
The elements of P deﬁne conditioned fuzzy sets in (VT ∪VN). [44, p. 16]
5.2 Fuzzy Relations and Fuzzy Semantics
In 1971, Zadeh deﬁned similarity relations and fuzzy orderings [48]. In doing so, he
was proceeding from the concept of fuzzy relations as a fuzziﬁcation of the relation
concept known in conventional set theory that he had already deﬁned in his seminal
article “Fuzzy Sets” [12]: If X and Y are conventional sets and if X × Y is their
Cartesian product, let: L(X) be the set of all fuzzy sets in X, L(Y) be the set of all
fuzzy sets in Y, and L(X × Y) be the set of all fuzzy sets in X × Y.
Relations between X and Y are subsets of their Cartesian product X × Y, and the
composition t = q ∗r of the relation q ⊆X × Y with the relation r ⊆Y × Z
into the new relation T ⊆X × Z is given by the following deﬁnition: t = q ∗r =
{(x, z), ∃y ∶(x, y) ∈q ∧(y, z) ∈r}.
Fuzzy relations between sets X and Y are subsets in L(X × Y). For three conven-
tional sets X, Y and Z, the fuzzy relation Q between X and Y and the fuzzy relation
R between Y and Z are deﬁned: Q ∈L(X × Y) and R ∈L(Y × Z). The combina-
tion of these two fuzzy relations into a new fuzzy relation T ∈L(X × Z) between X
and Z can then be deﬁned from the fuzzy relations Q and R into the fuzzy relation
T ∈L(X × Z) when the logical conjunctions are replaced by the corresponding ones
of the membership functions.
∙The above deﬁnition of the composition of conventional relations includes a logi-
cal AND (∧), which, for the “fuzziﬁcation”, is replaced by the minimum operator
that is applied to the corresponding membership functions.
∙The above deﬁnition of the composition of conventional relations includes the
expression “∃y” (“there exists a y”). The existing y ∈Y is the ﬁrst or the second
or the third … (and so on); written logically: supy∈Y (∨). In the “fuzziﬁcations”,
the logical OR conjunction is replaced by the maximum operator that is applied
to the corresponding membership functions.
The fuzzy relation T = Q ∗R is therefore deﬁned via Zadeh’s “rule of max-min
combination” for membership functions: 𝜇T(x, y) = maxy∈Ymin {𝜇Q(x, y); 𝜇T(y, z)},
y ∈Y. In inﬁnite sets the max-min composition rule is replaced with the sup-min
composition rule. However, it is adequate to assume here that all of the sets are ﬁnite.

560
R. Seising
As a generalization of the concept of the equivalence relation Zadeh deﬁned the
concept of “similarity”, since the similarity relation he deﬁned is reﬂective, symmet-
rical and (max, min) transitive, i.e. for x, y ∈X the membership function of S has
the following properties:
∙reﬂexivity: 𝜇S(x, x) = 1,
∙symmetry: 𝜇S(x, y) = 𝜇S(y, x),
∙transitivity: 𝜇S(x, z) ≥maxy∈Ymin {𝜇(x,y), 𝜇S(y, x)}.
Zadeh’s occupation with natural and artiﬁcial languages gave rise to his studies in
semantics. This intensive work let him to the question “Can the fuzziness of mean-
ing be treated quantitatively, at least in principle?” [49, p. 160] . His 1971 article
“Quantitative Fuzzy Semantics” [49] starts with this hint:
Few concepts are as basic to human thinking and yet as elusive of precise deﬁnition as the
concept of ‘meaning’. Innumerable papers and books in the ﬁelds of philosophy, psychology,
and linguistics have dealt at length with the question of what is the meaning of ‘meaning’
without coming up with any deﬁnitive answers.”3 [49, p. 159].
Zadeh started a new ﬁeld of research “to point to the possibility of treating the
fuzziness of meaning in a quantitative way and suggest a basis for what might be
called quantitative fuzzy semantics” combining his results on fuzzy languages and
fuzzy relations. In the section “Meaning” of this paper he set up the basics:
Consider two spaces: (a) a universe of discourse, U, and (b) a set of terms, T, which play
the roles of names of subsets of U. Let the generic elements of T and U be denoted by x and
y, respectively. Then he started to deﬁne the meaning M(x) of a term x as a fuzzy subset of
U characterized by a membership function 𝜇(y |x) which is conditioned on x. [49, p. 164f]
One of his examples was:
Let U be the universe of objects which we can see. Let T be the set of terms white, grey,
green, blue, yellow, red, black. Then each of these terms, e.g., red, may be regarded as a
name for a fuzzy subset of elements of U which are red in color. Thus, the meaning of red,
M(red), is a speciﬁed fuzzy subset of U. [49, p. 164f]
In the following section of this paper, that is named “Language”, Zadeh regarded
a language L as a “fuzzy correspondence”, more explicitly, a fuzzy binary relation,
from the term set T = {x} to the universe of discourse U = {y} that is characterized
by the membership function 𝜇L ∶T × U →[0, 1]. If a term x of T is given, then the
membership function 𝜇L(x, y) deﬁnes a set M(x) in U with the following membership
function:𝜇M(x)(y) = 𝜇L(x, y). Zadeh called the fuzzy set M(x) the meaning of the term
x; x is thus the name of M(x).
With this framework Zadeh continued in his 1970 article [40] to establish the
basic aspects of a theory of fuzzy languages that is “much broader and more general
than that of a formal language in its conventional sense.” [40, p. 134] In the following
we quote his deﬁnitions of fuzzy language, structured fuzzy language and meaning:
3In a footnote he named the works of 12 known philosophers, linguists or cognitive scientists.

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
561
Deﬁnition 1 A fuzzy language L is a quadruple L = (U, T, E, N), in which U is
a non-fuzzy universe of discourse; T (called the term set) is a fuzzy set of terms
which serve as names of fuzzy subsets of U; E (called an embedding set for T) is a
collection of symbols and their combinations from which the terms are drawn, i.e.,
T is a fuzzy subset of E; and N is a fuzzy relation from E (or more speciﬁcally, the
support of T(= supp(T) = {x | 𝜇A(x) > 0}) that is a non-fuzzy subset, to U which
will be referred to as a naming relation.
In the case that U and T are inﬁnite large sets, there is no table of membership
values for 𝜇T(x) and 𝜇N(x, y) and therefore the values of these membership functions
have to be computed. To this end, universe of discourse U and term set T have to
be endowed with a structure and therefore Zadeh deﬁned the concept of a structured
fuzzy language.
Deﬁnition 2 A structured fuzzy language L is a quadruple L = (U, ST, E, SN), in
which U is a universe of discourse; E is an embedding set for term set T, ST is a
set of rules, called syntactic rules of L, which collectively provide an algorithm for
computing the membership function, 𝜇T, of the term set T; and SN is a set of rules,
called the semantic rules of L, which collectively provide an algorithm for computing
the membership function, 𝜇N, of the fuzzy naming relation N. The collection of
syntactic and semantic rules of L constitute, respectively, the syntax and semantics
of L.
To deﬁne the concept of meaning, Zadeh characterized the membership function
𝜇N ∶supp(T) × U →[0, 1] representing the strength of the relation between a term
x in T and an object y in U. He clariﬁed:
A language, whether structured or unstructured, will be said to be fuzzy if [term set] T or
[naming relation] N or both are fuzzy. Consequently, a non-fuzzy language is one in which
both T and N are non-fuzzy. In particular, a non-fuzzy structured language is a language
with both non-fuzzy syntax and non-fuzzy semantics.” [40, p. 138]
Thus, natural languages have fuzzy syntax and fuzzy semantics whereas program-
ming languages, as they were usual in the early 1970s, were non-fuzzy structured
languages. The membership functions 𝜇T and 𝜇N for term set and naming relation,
respectively, were two-valued and the compiler used the rules to compute these val-
ues 0 or 1. This means that the compiler decides deterministically by using the syn-
tactic rules whether a string x is a term in T or not and it also determines by using the
semantic rules whether a term x hits an object y or not. On the other hand we have
natural languages, e.g. English, and it is possible that we use sentences that are not
completely correct but also not completely incorrect. These sentences have a degree
of grammaticality between 0 and 1. Of course, native speakers usually use correct
sentences. “In most cases, however, the degree of grammaticality of a sentence is
either zero or one, so that the set of terms in a natural language has a fairly sharply
deﬁned boundary between grammatical and ungrammatical sentences”, Zadeh wrote
[40, p. 138].
Much more fuzziness we ﬁnd in semantics of natural languages: Zadeh gave the
example “if the universe of discourse is identiﬁed with the set of ages from 1 to 100,

562
R. Seising
then the atomic terms young and old do not correspond to sharply deﬁned subsets
of U. The same applies to composite terms such as not very young, not very young
and not very old, etc. In eﬀect, most of the terms in a natural language correspond
to fuzzy rather than non-fuzzy subsets of the universe of discourse.” [40, p. 139]
Zadeh now identiﬁed these fuzzy subsets of the universe of discourse that corre-
spond to terms in natural languages with its “meaning”:
Deﬁnition 3 The meaning of a term x in T is a fuzzy subset M(x) of U in which the
grade of membership of an element y of U is given by 𝜇M(x)(y) = 𝜇N(x, y).
Thus, M(x) is a fuzzy subset of U which is conditioned on x as a parameter and
which is a section of N in the sense that its membership function, 𝜇M(x) ∶U →[0, 1],
is obtained by assigning a particular value, x, to the ﬁrst argument in the membership
function of N.
Zadeh concluded this paper mentioning that “the theory of fuzzy languages is in
an embryonic stage” but he expressed his hope that based on this framework better
models for natural languages will be developed than the models of the “restricted
framework of the classical theory of formal languages.” [40, p. 163]
Later in the 1970s he published important papers summarizing and developing
the concepts we presented above: in 1973 “Outline of a new approach to the analysis
of complex systems and decision processes” [50] appeared in the IEEE Transaction
on Systems, Man, and Cybernetics, in 1975 the three-part article “The concept of a
Lingustic Variable and its Application to Approximate Reasoning” [51] appeared in
the journal Information Sciences, in the same year Zadeh published “Fuzzy Logic
and Approximate Reasoning” in the philosophical journal Synthese [52] and in 1978
Fig. 12
The components of a fuzzy language: U = universe of discourse; T = term set; E =
embedding set for T ; N = naming relation from E to U; x = term; y = object in U; 𝜇M(x, y) =
strength of the relation between x and y; 𝜇T(x) = grade of membership of x in T. [40, p. 136]
Fig. 13
Membership
functions of fuzzy sets M
(young), M (middle-aged)
and M (old), [40, p. 140]

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
563
Zadeh published “PRUF a meaning representation language for natural languages”
in the International Journal of Man-Machine Studies [53].4
During the 1970’s Berkeley-psychologist Eleanor Rosch developed her prototype
theory on the basis of empirical studies. This theory assumes that people perceive
objects in the real world by comparing them to prototypes and then ordering them
accordingly. In this way, according to Rosch, word meanings are formed from proto-
typical details and scenes and then incorporated into lexical contexts depending on
the context or situation. Rosch hypothesized that diﬀerent societies process percep-
tions diﬀerently depending on how they go about solving problems [54]. When the
linguist George Lakoﬀ(born 1941, Fig. 14 (b)) heard about Rosch’s experiments,
he was working at the Center for Advanced Study in Behavioral Sciences at Stan-
ford. During a discussion about prototype theory, someone there mentioned Zadeh’s
name and his idea of linking English words to membership functions and establish-
ing fuzzy categories in this way. Lakoﬀand Zadeh met in 1971/72 at Stanford to
discuss this idea and also the idea of fuzzy logic, after which Lakoﬀwrote his paper
“Hedges: A Study in Meaning Criteria and the Logic of Fuzzy Concepts” [55]. In this
work, Lakoﬀemployed “hedges” (meaning barriers) to categorize linguistic expres-
sions and he invented the term “fuzzy logic” whereas Goguen had used “logic of
inexact concepts”.
Based on his later research, however, Lakoﬀdecided that fuzzy logic was not an
appropriate logic for linguistics, but: “Inspired and inﬂuenced by many discussions
with Professor G. Lakoﬀconcerning the meaning of hedges and their interpretation
in terms of fuzzy sets,” Zadeh had also written an article in 1972 in which he con-
templated “linguistic operators”, which he called “hedges”: “A Fuzzy Set-Theoretic
Interpretation of Hedges”. Here he wrote:
A basic idea suggested in this paper is that a linguistic hedge such as very, more, more or
less, much, essentially, slightly etc. may be viewed as an operator which acts on the fuzzy
set representing the meaning of its operand” [56].
In the 1970s Zadeh had expected that his theory of Fuzzy Sets “provides an
approximate and yet eﬀective means of describing the behavior of systems which
are too complex or too ill-deﬁned to admit of precise mathematical analysis.” [50,
p. 28] He had expected that even at its present stage of development” his new fuzzy
method.
Fig. 14
(a): Joseph Goguen,
(b): George Lakoﬀand
(c): Ebrahim Mamdani
4PRUF is an acronym for “Possibilistic Relational Universal Fuzzy.”

564
R. Seising
can be applied rather eﬀectively to the formulation and approximate solution of a wide vari-
ety of practical problems, particularly in such ﬁelds as economics, management science,
psychology, linguistics, taxonomy, artiﬁcial intelligence, information retrieval, medicine and
biology. This is particularly true of those problem areas in these ﬁelds in which fuzzy algo-
rithms can be drawn upon to provide a means of description of ill-deﬁned concepts, relations,
and decision rules. [50, p. 44]
In an interview that Zadeh gave in 1994, he mentioned his surprise that Fuzzy
Logic was “embraced by engineers” and “used in industrial process controls and
in ‘smart’ consumer products such as hand-held camcorders that cancel out jitter-
ing and microwaves that cook your food perfectly at the touch of a single button.”
In that interview he also said that he had “expected people in the social sciences
– economics, psychology, philosophy, linguistics, politics, sociology, religion and
numerous other areas to pick up on it [Fuzzy Logic]. It’s been somewhat of a mys-
tery to me, why even to this day, so few social scientists have discovered how useful
it could be.” [57]5
However, it was the concept of fuzzy algorithms that fell on fertile ground ﬁrst:
Ebrahim H. Mamdani (1942–2010), Fig. 14 (c)),6 a professor of electrical engineer-
ing at Queen Mary College in London, had read Zadeh’s article [50] shortly after
it was published and he directed his doctoral student Sedrak Assilian to perform a
trial to realize a fuzzy system under laboratory conditions and he also pointed to this
paper in the article that he published together with Assilian after he had ﬁnished his
Ph. D thesis:
The true antecedent of the work described here is an outstanding paper by Zadeh (1973)
which lays the foundations of what we have termed linguistic synthesis … and which had
also been described by Zadeh as approximate reasoning (AR). In the 1973 paper Zadeh
shows how vague logical statements can be used to derive inferences (also vague) from vague
data. The paper suggests that this method is useful in the treatment of complex humanistic
systems. However, it was realized that this method could equally be applied to ‘hard’ systems
such as industrial plant controllers.” [60, p. 325]
This was the kick-oﬀfor the “Fuzzy-Boom” and Zadeh’s primary intention trailed
away for decades.
6 A Real-World Application Fuzzy System
The potential of the new techniques of fuzzy sets and fuzzy systems had stimulated
Mamdani to attempt the implementation of a real-world fuzzy system and Sedrak
Assilian designed a fuzzy algorithm to control a small steam engine (Fig. 15) within
a few days. The concepts of linguistic variables and the max-min composition were
suitable to establish fuzzy control rules because input, output and state of the steam
engine system range over fuzzy sets. Thus, Assilian and Mamdani designed the ﬁrst
5For Soft Computing methods Social Sciences see also [58].
6For more details on Abe Mamdani’s work see [59].

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
565
real fuzzy application when they controlled the system with the input variables heat
and throttle and the output variables pressure and speed (Fig. 16) by a fuzzy rule
base system.
It was an experimental study which became very popular. Immediately we had a steam
engine, and the idea was to control the steam engine. We started working at Friday, and – I do
not remember clearly – by Sunday it was working” he said in an interview in 2008 [61, p. 75].
In 1974, Assilian completed his Ph. D. thesis on this ﬁrst fuzzy control system;
unfortunately, no other facts about Assilian are available; he also does not appear in
later literature about Fuzzy Set Theory and its applications.
The entire system consisted of the combination of a steam engine and a boiler
(see Fig. 17). The steam was supposed to reach a certain predetermined pressure
within the boiler; this was achieved by regulating the temperature. The engine was
to run as consistently as possible at a particular piston speed, for which purpose a
throttle was installed. This was therefore a system with two inputs (heat supplied to
the boiler, engine throttle) and two outputs (pressure in the boiler, engine speed) (see
Fig. 16). These inputs and outputs range over fuzzy sets. Thus, Assilian and Mamdani
designed the ﬁrst real fuzzy system and also the ﬁrst real fuzzy application when they
controlled this system by a fuzzy rule base system.
Sensors constantly monitored the boiler and indicated the current pressure. If the
prevailing pressure corresponded to the set point value, then nothing needed be done.
If it deviated from the set point, then some action had to be taken, and this task was
to be assumed by an automatic fuzzy controller.
Fig. 15
Photograph of the “Fuzzy steam engine”, Queen Mary College, 1974, reprint courtesy of
Brian Gaines, see also: [62, p. 18]

566
R. Seising
Simple identiﬁcation tests on the plant proved that it is highly nonlinear with both
magnitude and polarity of the input variables. Therefore the plant possesses diﬀer-
ent characteristics at diﬀerent operating points, so that the direct digital controller
implemented for comparison purposes had to be returned (by trial and error) to give
the best performance each time the operating point was altered. [63, p. 2]
Assilian and Mamdani deﬁned six linguistic variables (four input and two output
variables):
1. PE (Pressure Error), deﬁned as the diﬀerence between the actual value
and the set point of the pressure in the boiler.
2. SE (Speed Error), deﬁned as the diﬀerence between the actual value
and the set point of the of the piston speed
3. CPE (Change in pressure error), deﬁned as the diﬀerence
between the actual value of PE and its most recent value
4. CSE (Change in speed error), deﬁned as the diﬀerence
between the actual value of SE and its most recent value
5. HC (Heat Change) (action variable, as the result of which a command occurs).
6. TC (Throttle Change) (action variable, as the result of which a command occurs).
Fig. 16
The process variables of the fuzzy steam engine, [62, p. 31]
Fig. 17
The system consisting of a steam engine and a boiler, [62, p. 18]

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
567
They introduced linguistic terms for the variables: PB (Positive Big), PM (Pos-
itive medium), PS (Positive Small), P0 (Positive, Zero), N0 (Negative Zero), NS
(Negative Small), NM (Negative Medium), and NB (Negative Big). The variables
were distributed over a number of points in accordance with the universe of dis-
course.
∙For the variables PE and SE there were 13 points, which ranged from the max-
imum negative error through zero to the maximum positive error, with the zero
being divided into a “negative zero error” N0 and a “positive zero error” P0 (“N0
– just below the set point … P0 – just above the set point” [63, p. 7f].
∙The variables CPE and CSE were similarly quantized.
∙The variable HC was ultimately quantized over 15 points.
∙Similarly, the variable TC was distributed over ﬁve points.
Mamdani and Assilian formed the fuzzy sets subjectively and then they deﬁned
24 rules as IF-THEN rules. Table 1 gives three rules as examples, represented as in
[63]. For the sake of simplicity, the authors of that work did not diﬀerentiate between
“positive zero” and “negative zero”.
Table 1
Examples of Mamdani’s and Assilian’s IF-THEN rules in [63]
Rule 1:
IF
the deviation in pressure is small and positive
AND
the deviation in pressure does not change much
THEN
reduce the supply of heat a little
IF PE is PS AND SE is N, THEN HC is NS
Rule 2:
IF
the deviation in pressure is approximately zero
AND
the deviation in pressure does not change much
THEN
do not change the supply of heat
IF PE is N AND SE is N, THEN HC is N
Rule 3:
IF
the deviation in pressure is small and positive
AND
the deviation in pressure is slowly increasing
THEN
reduce the supply of heat a little
IF PE is PS AND SE is PS, THEN HC is NS
These rule relationships were implemented as fuzzy relations for which Zadeh
had already indicated the max-min composition rule in his ﬁrst publication on Fuzzy
Sets. Additionally, a PDP 8/S digital computer [62, p. 17] calculated a corresponding
fuzzy set as a value for the output variable. This method can be represented graphi-
cally in the following way (see Fig. 18):

568
R. Seising
Fig. 18
Illustration of the application of the min-max rule based on [64, p. 161]
The sensors indicate sharp values for the input variables pressure deviation and
its change, whose membership values with respect to the corresponding fuzzy sets
can be read on the triangular membership functions. In the illustrated example for
rule 1, the membership value with respect to the fuzzy set pressure deviation PS is
0.2 and it is 0.4 with respect to the fuzzy set change in pressure deviation N. Today
this part of the fuzzy control process is known as “fuzziﬁcation”.
The max-min rule prescribes that the minimum of these two values is computed
ﬁrst. (In the example for rule 1 illustrated above, this value is 0.2). Accordingly,
after executing this rule alone, the output command was “Change heat supply NS”
and it had a membership value of 0.2. The result of rule 1 thus results in a triangu-
lar function that is truncated at the value 0.2 – a trapezoidal membership function.
However, rule 2 and rule 3 have also ﬁred and so they must be evaluated analogously
and parallel to rule 1. The ﬁnal membership function for the fuzzy set as a value of
the output variable change in pressure deviation is ultimately composed of the trape-
zoidal membership functions of the individual rule results. This composition occurs
according to the max-min rule by forming the maximum of the membership func-
tions of all three output fuzzy sets.
Just how was the output variable change in pressure deviation supposed to be
adjusted, though? For this a sharp (that is, crisp or non-fuzzy) value is required and
Mamdani and Assilian decided on a simple procedure:
Various considerations may inﬂuence the choice procedure depending on the particular
application and in our case eﬀectively that action is taken which has the largest member-
ship grade. It is possible of course that more than one peak of a ﬂat is obtained as illustrated
below [see Fig 19]:

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
569
Fig. 19
Illustration of the
selection of the centroid as a
defuzziﬁcation method
devised by Assilian and
Mamdani [63, p. 627]
Negative deviations signify a movement toward the set point, positive deviations signify a
movement away from the set point [60, p. 627].
“The particular procedure in our case takes the action indicated by the arrow,
which is midway between the two peaks or at the centre of the plateau.” [63, p. 5]
In his dissertation thesis entitled Artiﬁcial Intelligence in the Control of Real
Dynamic Systems that Assilian produced in response to this fuzzy control problem
[62], he wrote that the control strategy they had realized was one that a human oper-
ator could use to control a steam engine.
These control policies were established ﬁrst by imagining the entire state space (PE × CPE
× SE × CSE) to be divided into a number of areas, and second, writing down a control
policy for each of these areas. Obviously, the ﬁrst set of rules obtained in this manner does
not necessarily produce the best quality of control possible … [62, p. 135]
Figure 20 shows the “Fuzzy control instructions for heat-pressure loop of steam
engine” [60, p. 627]. This control algorithm was thus profoundly subjective. Not only
the algorithm but also the membership function had been designed subjectively. Yet
as Assilian and Mamdani managed to demonstrate, this Fuzzy Control (FC) sys-
tem exceeded the performance of conventional control systems in several ways (see
Fig. 21).
Fig. 20
FC commands for
the steam engine designed by
Assilian and Mamdani

570
R. Seising
∙Much less information is required for FC than for conventional control.
∙The verbal knowledge of human experts did not have to be mathematically exact
in order to be processed by the automatic control.
∙Errors were reduced little by little until the set point could be reached; digital
controllers “overshot” this target instead.
∙The FC system worked faster than a conventional control system; the possibility
of processing the parallel ﬁring of several rules at the same time shortened the
required control time.
With this fuzzy control of a steam engine – or more precisely a combination of a
boiler and a steam engine – the essential principles for the construction of an entire
class of fuzzy control systems were established and Mamdani went ahead. Already
in January 1976 he organized – together with Brian Gaines, then a professor of com-
puter science at Essex university – a Workshop on “Discrete Systems and Fuzzy
Reasoning” held at London’s Queen Mary College. At this workshop some similar
projects to control technical systems using fuzzy algorithms were presented, e.g. a
basic oxygen steel making process at the British Steel Corporation in Cambridge,
England [65], a sinter making plant at the British Steel Corporation in Middles-
borough, England [66, 67], and a pilot scale batch chemical process in the Warren
Spring Laboratory in Stevenage, England in [68, 69]. Some other FC investigations
of this time were a FC system to control a warm water plant in the Delft Technical
High School in the Netherlands [70] and a heat exchanger at the McMaster Univer-
sity in Canada.
The step forward from small laboratory systems to the ﬁrst large-scale commercial
fuzzy controlled system was taken very soon. The ﬁrst “big science” FC system was
built in Denmark by Jens-Jörgen Østergaard and Lauritz Peter Holmblad who joined
the company F. J. Smidth & Co. upon graduation from the Technical University of
Copenhagen. It was a system for the automatic control of a cement kiln. Attempts
Fig. 21
The result of the Assilian-Mamdani FC (◦) compared to a conventional controller.
(Dynamic Divergence Caching (DDC) algorithm damped (⊓⊔) and undamped (x)), [63, p. 6]

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
571
to automate cement production had always failed in the past because the process
of cement burning is highly complex, ovens do not behave linearly and only a few
measurements can be taken during the process [71]. The fuzzy cement kiln developed
by Holmblad and stergaard functioned very successfully and reliably, however. It
was the starting point of the “Fuzzy Boom” that started in the 1980s in Japan and
later pervaded the Western hemisphere. Many fuzzy applications, such as domestic
appliances, cameras and other devices appeared in the last two decades of the 20th
century. Of greater signiﬁcance, however, was the development of fuzzy process
controllers and fuzzy expert systems that served as trailblazers for scientiﬁc and
technological advancements of fuzzy sets and systems.
7 Fuzzy Sets in Humanities and Social Sciences
In 1969 Zadeh proposed his new theory of fuzzy sets to biologists: “The great
complexity of biological systems may well prove to be an insuperable block to the
achievement of a signiﬁcant measure of success in the application of conventional
mathematical techniques to the analysis of systems.” [72] “By ‘conventional mathe-
matical techniques’ in this statement, we mean mathematical approaches for which
we expect precise answers to well-chosen precise questions concerning a biologi-
cal system that has a high degree of relevance to its observed behaviour. Indeed,
the complexity of biological systems may force us to alter in radical ways our tradi-
tional approaches to the analysis of such systems. Thus, we may have to accept as
unavoidable a substantial degree of fuzziness in the description of the behaviour of
biological systems as well as in their characterization.” [72]
We ﬁnd great complexity not only in biological systems but also in social sciences
and humanities. At the end of the 1960s and for a greater audience two years later,
Zadeh wrote more generally: “What we still lack, and lack rather acutely, are methods
for dealing with systems which are too complex or too ill-deﬁned to admit of precise
analysis. Such systems pervade life sciences, social sciences, philosophy, economics,
psychology and many other ‘soft’ ﬁelds.” [16]
Zadeh was inspired by the remarkable human capability to perform a wide variety
of physical and mental tasks without any measurements and any computations”, e.
g. parking a car, playing golf, deciphering sloppy handwriting, and summarizing a
story. He distinguished between mechanical (or inanimate or man-made) systems at
one hand and humanistic systems at the other hand and he saw the following state of
the art in computer technology:
Unquestionably, computers have proved to be highly eﬀective in dealing with mechanistic
systems, that is, with inanimate systems whose behavior is governed by the laws of mechan-
ics, physics, chemistry and electromagnetism. Unfortunately, the same cannot be said about
humanistic systems, which – so far at least – have proved to be rather impervious to mathe-
matical analysis and computer simulation.

572
R. Seising
He deﬁned a “humanistic system” to be
a system whose behaviour is strongly inﬂuenced by human judgment, perception or emo-
tions. Examples of humanistic systems are: economic systems, political systems, legal sys-
tems, educational systems, etc. A single individual and his thought processes may also be
viewed as a humanistic system. [51, Part I, p. 200]
Zadeh summarized “that the use of computers has not shed much light on the basic
issues arising in philosophy, literature, law, politics, sociology and other human-
oriented ﬁelds. Nor have computers added signiﬁcantly to our understanding of
human thought processes-excerpting, perhaps, some examples to the contrary that
can be drawn from artiﬁcial intelligence and related ﬁelds.” [51, Part I, p. 200]
Thus, hard computing has been very successful in hard sciences but it could not
be that successful in humanistic systems in the ﬁeld of soft sciences. Therefore we
should open the ﬁeld of applications of soft computing to the soft sciences. This is
what Zadeh had in mind when he proposed the notion of soft computing:
I expected people in the social sciences-economics, psychology, philosophy, linguistics, pol-
itics, sociology, religion and numerous other areas to pick up on it. It’s been somewhat of
a mystery to me why even to this day, so few social scientists have discovered how useful
it could be. Instead, Fuzzy Logic was ﬁrst embraced by engineers and used in industrial
process controls and in ‘smart’ consumer products such as hand-held camcorders that can-
cel out jittering and microwaves that cook your food perfectly at the touch of a single button.
I didn’t expect it to play out this way back in 1965.” [57].
The ﬁeld of Soft Computing in Humanities and Social Sciences is at a turning
point. Not very long ago, the very label seemed a little bit odd. Soft Computing is
a technological ﬁeld while Humanities and Social Sciences fall at the other pole of
the academic ﬁeld. In recent years, however, this has changed. The strong distinction
between “science” and “humanities” has been criticized from many fronts and, at the
same time, increasing cooperation between the so-called “hard sciences” and “soft-
sciences” is taking place in a wide range of scientiﬁc projects dealing with very
complex and interdisciplinary topics [73].
In the last ﬁfteen years the area of Soft Computing has also experienced a gradual
rapprochement to disciplines in the Humanities and Social Sciences [58, 74].
8 Outlook: Computing with Words and Perceptions
Artiﬁcial Intelligence (AI) was born in the 1950s in the USA and spread to many
scientiﬁc and technological communities throughout the world. The history of AI
is a story of several successes but has lagged behind expectations. AI became a
ﬁeld of research to build computers and computer programs that act “intelligently”
although no human being controls those systems. AI methods became logic-based
to ﬁnd exact solutions. However, not all problems can be resolved with these meth-
ods. On the other hand, humans are able to resolve such tasks very well, as Lotﬁ
Zadeh mentioned in many speeches and articles over the last century. In conclusion,

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
573
Fig. 22
Perception-based
system modeling, [78]
he stated that “thinking machines” do not think as humans do. From the mid-1980s
he focused on “Making Computers Think like People” [75]. For this purpose, the
machine’s ability “to compute with numbers” was supplemented by an additional
ability that was similar to human thinking. In the 1990s he established Computing
with Words (CW) [76, 77] instead of exact computing with numbers, as a method
for reasoning and computing with perceptions based on the theory of fuzzy sets.
In his article “Fuzzy Logic = Computing with Words” in May 1996, he stated that
“the main contribution of fuzzy logic is a methodology for computing with words.
No other methodology serves this purpose.” [75, p. 103] Three years later he wrote
“From Computing with Numbers to Computing with Words – From Manipulation of
Measurements to Manipulation of Perceptions”, to show that a new Computational
Theory of Perceptions, or CTP for short, is based on the methodology of CW. In
CTP, words play the role of labels of perceptions and, more generally, perceptions
are expressed as propositions in natural language. [76, p. 105].
As we said already, he was inspired by the “remarkable human capability to per-
form a wide variety of physical and mental tasks without any measurements and
any computations. [...] Underlying this capability is the brain’s crucial ability to rea-
son with perceptions – perceptions of time, distance, speed, force, direction, shape,
intent, likelihood, truth and other attributes of physical and mental objects.” [77, p.
105]. Zadeh intended to establish a new dimension of artiﬁcial intelligence [78, p.
73]. He received an opportunity to propose these considerations concerning “A New
Direction in AI” to the AI community at the beginning of the new millennium, when
his manuscript was accepted for the AI Magazine issue in the spring of 2001 [78].
In this article he presented a new view on system theory, namely perception-based
system modeling: In “perception-based system modeling”, the input, the output and
the states are assumed to be perceptions (Fig. 22).
The 50th anniversary of a scientiﬁc theory is a good opportunity to cast a ret-
rospective look at its consequences and achievements. Many aspects of this history
are a matter of course, such as deﬁnitions of the theorys entities, theorems and pro-
tagonists of important developments. However, some are facts that were unknown
to most interested persons and also some specialists. The original research work on
the history of the theory of fuzzy sets, as presented in this chapter, shows that its
history cannot be comprehended without reﬂecting on the history of system theory.
Moreover, fuzzy set theory must be regarded as an inherent part of the history. This

574
R. Seising
deep connection is evidenced from the very beginning of Zadeh’s scientiﬁc all the
way up to his recent lectures and articles. With his varying views on system theory,
Computing with Words and the Computational Theory of Perceptions, he postulated
new directions for science and technology, in the ﬁelds of information science, com-
puter science, and artiﬁcial intelligence. Perhaps the lesson to be learned from this
history is that creating new views is one of the most eﬀective means of keeping a
scientiﬁc theory – such as fuzzy set theory – alive.
Acknowledgments The author wishes to thank LotﬁA. Zadeh for his generous help and unstinted
willingness to support the author’s historical project on the theory of fuzzy sets and systems during
the last 15 years. Many thanks go also to Claudio Moraga and Jim Bezdek who read this text and
gave important hints to improve it. Work leading to this chapter was partially supported by the
Foundation for the Advancement of Soft Computing Mieres, Asturias (Spain).
References
1. Seising R.: The Fuzziﬁcation of Systems. The Genesis of Fuzzy Set Theory and Its Initial
ApplicationsDevelopments up to the 1970s. Springer, Berlin (2007). A history of Fuzzy Set
Theory and the ways it was ﬁrst used. The book incorporates this genesis of the new theory into
the history of 20th century science and technology. Inﬂuences from philosophy, system theory
and cybernetics stemming from the earliest part of the 20th century are considered alongside
those of communication and control theory from mid-century
2. Zadeh, L.A., Desoer, C.A.: Linear System Theory: The State Space Approach. McGraw-Hill,
New York (1963). This textbook for engineers in research and development and applied math-
ematicians is landmark in the development of the state space approach. It concerns the tech-
nique’s application to systems described by diﬀerential equations
3. Zadeh, L.A., Polak, E.: System Theory, Bombay. McGraw-Hill, New Delhi (1969). This vol.
8 in the Inter-University electronics series is a collection of papers on system theory
4. Zadeh L.A.: The Concept of State in System Theory, in [3, p. 9–42]. In this book chapter Zadeh
presented the “state space approach” to System Theory
5. Zadeh L.A.: System theory. Columbia Eng. Q., 16–19, 34 (1954). Zadeh’s ﬁrst paper on System
theory in the New York student publication Columbia Engineering Quarterly
6. Zadeh, L.A.: Some basic problems in communication of information. New York Acad. Sci.
Series II 14(5), 201–204 (1952). Zadeh’s talk at the meeting of the Section of Mathematics
and Engineering of the New York Academy of Sciences on February 15, 1952
7. Zadeh, L.A., Miller, K.S.: Generalized ideal ﬁlters. J. Appl. Phys. 23(2), 223–228 (1952). In
this paper the authors established their general theory of linear signal transmission systems
with extensive use of modern mathematical methods: Fourier analysis as well as Hilbert space
and operator calculus
8. Zadeh, L.A.: Theory of ﬁltering. J. Soc. Ind. Appl. Math. 1, 35–51 (1953). In this article on
his general ﬁlter theory Zadeh emphasized a distinction between ideal and optimum ﬁlters.
The former are deﬁned as ﬁlters which achieve a perfect separation of signal and noise, but if
ideal ﬁltration is not possible, though, which is often the case when the signal is mixed with
noise, then one must accept that the ﬁltration can only be incomplete. In such cases, a ﬁlter
that delivers the best possible approximation of the desired signal and “a particular meaning”
of “best approximation” is used here – is called an optimum ﬁlter
9. Zadeh, L.A.: From circuit theory to system theory. Proc. IRE 50, 856–865 (1962). This article
was written for the anniversary edition of the Proceedings of the IRE appeared in May 1962
to mark the 50th year of the Institute of Radio Engineers (IRE). The article presents a brief

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
575
survey of the evolution of system theory, together with an exposition of some of its main con-
cepts, techniques and problems. It concerns problems and applications of system theory and its
relations to network theory, control theory, and information theory. The discussion is centered
on the notion of state and emphasizes the role played by state-space techniques
10. Bellman, R.E: Dynamic Programming. Princeton University Press, Princeton (1957). This is
Bellman’s introduction to the mathematical theory of multistage decision processes. Dynamic
programming describes the process of solving problems where one needs to ﬁnd the best deci-
sions one after another
11. Zadeh, L.A.: Fuzzy sets and systems. In: Fox, J. (ed.) System Theory. Microwave Research
Institute Symposia Series XV, pp. 29–37. Polytechnic Press, Brooklyn, New York (1965).
Zadeh’s contribution in the proceedings of the Symposium on System Theory (April 20-22,
1965) at the Polytechnic Institute in Brooklyn, When Zadeh gave this talk it was entitled “A
New View on System Theory”
12. Zadeh, L.A.: Fuzzy sets. Inf. Control 8, 338–353 (1965). This is the ﬁrst and seminal article
on fuzzy sets
13. Zadeh, L.A.: Fuzzy sets. ERL Report no. 64–44, University of California at Berkeley, 16 Nov
1964. Zadeh’s preprint of the article [12]
14. Bellman R.E., Kalaba, R.E., Zadeh L.A.: J. Math. Anal. Appl. 13, 1–7 (1966). This article was
published in 1966 but its text did appear already as a RAND memorandum two years before:
see [15]
15. Bellman R.E., Kalaba, R.E., Zadeh L.A: Abstraction and Pattern Classiﬁcation. Memoran-
dum RM-4307-PR, Santa Monica, California: The RAND Corporation, October 1964. Zadeh’s
ﬁrst paper on fuzzy sets and pattern recognition problems that appeared at ﬁrst as a RAND-
Corporation memorandum in. Even this paper has three co-authors it was written by Lotﬁ
Zadeh. It contains the ﬁrst deﬁnitions of the theory of fuzzy sets in a scientiﬁc text. Two years
later this paper appeared under the same title and authorship as a journal article, see [14]
16. Zadeh, L.A.: Towards a theory of fuzzy systems. In: Kalman R.E., DeClaris N. (eds.) Aspects
of Network and System Theory, pp. 469–490. Holt, Rinehart and Winston, New York (1971).
Zadeh’s contribution to an anthology on Network and System Theory
17. Black, M.: Vagueness. An exercise in logical analysis. Philos. Sci. 4, 427–455 (1937). In this
article the philosopher Max Black analyzed the nature of vagueness and the signiﬁcance this
concept might have for logic
18. Black, M.: Letter to LotﬁA. Zadeh at June 21, 1967. Private Archives of LotﬁA. Zadeh
19. Black, M.: Language and Philosophy. Cornell University Press, Ithaca (1949). This book is
a collection of Black’s articles on several topics in philosophy of language, semantics, and
semiotics. It also contents his article [17]
20. Black, M.: Reasoning with loose concepts. Dialogue 2(1), 1–12 (1963). This is Black’s second
paper on vague (or loose) concepts; for his ﬁrst article see [17]. Here he said that classical logic
is only applicable in a tentative, rough and ready way to our terrestrial world
21. Kant, I.: Kritik der reinen Vernunft. Meiner Verlag, Hamburg, [1781] (1998). One of the most
inﬂuential books in the history of philosophy, English Title: Critique of Pure Reason. “I do not
mean by this a critique of books and systems, but of the faculty of reason in general, in respect
of all knowledge after which it may strive independently of all experience.” (Preface to the ﬁrst
edition.)
22. Hertz, H.: Die Prinzipien der Mechanik in neuen Zusammenhange dargestellt. Drei Beiträge,
Ostwalds Klassiker der exakten Wissenschaften, Bd. 263) (1894), new ed.: Frankfurt am Main:
Verlag Harri Deutsch 1996), Hertz’s posthumously published approach to mechanics with a
philosophical introduction anticipated current discussions on the role of models in science.
English translation: The Principles of Mechanics Presented in a New Form. Dover, New York
(1956)
23. Wittgenstein, L.: Tractatus logico-philosophicus. Routledge & Kegan Paul, London. (First in
German: L. Wittgenstein: Logisch-Philosophische Abhandlung. Ostwalds Annalen der Natur-
philosophie, Band 14, Leipzig (1921). Wittgenstein’s ﬁrst book concerns the philosophical
problems which deal with the world, thought and language. Wittgenstein’s “solution” bases in
logic and in the nature of representation

576
R. Seising
24. Wittgenstein, L.: Werkausgabe in acht Bänden (1914–1916), new ed.: Frankfurt am Main:
Suhrkamp, Bd. 8, 1984. This is the German edition of Wittgenstein’s Collected Works
25. Kortabinśki, T.: Elementy teorii poznania, logiki formalnej i metodologii nauk. Lwów, Osso-
lineum (1929). In this book (English translation: “Gnosiology”, 1966) the Polish philosopher
Kortabinśki argued that a concept for a property is vague if the property may be the case by
grades
26. Ajdukiewicz, K.: On the problem of universals. Prz. Filozof. 38(1935), 219–234 (1935). In
this article the Polish philosopher Ajdukiewicz argued that “a term is vague if and only if its
use in a decidable context [..] will make the context undecidable in virtue of those [language]
rules”
27. Frege, G.: Begriﬀsschrift, eine der arithmetischen nachgebildete Formelsprache des reinen
Denkens, Halle (1879). This book (English Title: Concept-Script presents A Formal Language
for Pure Thought Modeled on that of Arithmetic (subtitle) as a second-order predicate calculus.
This is used to deﬁne mathematical concepts and to state and prove mathematically proposi-
tions
28. Frege, G.: Funktion und Begriﬀ. In: Frege, G. Funktion, Begriﬀ, Bedeutung (ed.: Patzig G.).
Vandenheoeck & Ruprecht, Gttingen, 1986, 1839 (1891). In this philosophical article, English
title “On Function and Concept”, Frege deﬁnes a concept as a monadic function whose value
is always a truth value
29. Frege, G.: Grundgesetze der Arithmetik, vol. 2, Hermann Pohle, Jena (1893–1903). In this
book, engl. title: The Foundations of Arithmetic: A Logic-Mathematical Enquiry into the Con-
cept of Number, Frege attempted to derive, by use of his symbolism, all of the laws of arith-
metic from axioms he asserted as logical. Most of these axioms were carried over from his
Begriﬀsschrift [27]
30. Russell, B.: Vagueness. Aust. J. Psychol. Philos. 1, 84–92 (1923). In this article the mathe-
matician and philosopher Bertrand Russell discusses some of the most important problems
concerning the nature of vagueness, its extension within the language, and its relation to truth
and logic
31. Menger, K.: Statistical metrics. Proc. Natl. Acad. Sci. USA 28, 535–537 (1942). In this short
paper Menger introduced his extension of the concept of a metrics to statistical metrics
32. Menger, K.: Probabilistic geometry. Proc. Natl. Acad. Sci. USA 37, 226–229 (1951). In this
short paper Menger used his concept of statistical metrics in geometry
33. Menger, K.: Probabilistic theories of relations. Proc. Natl. Acad. Sci. USA 37, 178180 (1951).
In this short paper Menger introduced the concept of probabilistic relations
34. Menger, K.: Ensembles ﬂous et fonctions aléatoires. Comptes Rendus Académie des Sciences
37, 226–229 (1951). In this paper that was written by Menger when he was a visiting professor
in Paris, he introduced the concept of “ensembles ﬂuos” or “hazy sets”. Today “ensembles
ﬂous” are the French name for fuzzy sets but Menger’s old concept of “ensembles ﬂous” was
still probabilistic and not identical with fuzzy sets
35. Menger, K.: Mathematical implications of Mach’s ideas: positivistic geometry, the clariﬁca-
tion of functional connections. In: Cohen, R.S., Seeger, R.J. (eds.) Ernst Mach, Physicist and
Philosopher, Boston Studies in the Philosophy of Science, vol. 6. Reidel, Dordrecht, pp. 107–
125 (1970). Reprint as: Menger K. Geometry and Positivism. A Probabilistic Microgeometry.
In: Menger K. Selected Papers in Logic and Foundations, Didactics, Economics. Dordrecht:
D. Reidel Publ. Comp., 225–234 (1979). Citation after the reprinted version. This article is the
printed form of Menger’s contribution to the symposium of the American Association for the
Advancement of Science, organized in 1966 to commemorate the 50th anniversary of Ernst
Machs death. In this contribution he compared his “microgeometry” or “probabilistic geom-
etry” (see [32]) with the theory of fuzzy sets. However, he mentioned the diﬀerence between
his concept of hazy sets and Zadeh’s concept of fuzzy sets because he wrote that Zadeh spoke
“of the degree rather than the probability of an element belonging to a set”
36. Wittgenstein, L.: Philosophical Investigations. Blackwell Publishing, Oxford (1953). In his sec-
ond book, published posthumously, Wittgenstein discusses several philosophical problems. On

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
577
the contrary to his Tractatus Logico-Philosophicus he claimed that most philosophical prob-
lems root in conceptual confusions surrounding language use
37. Goguen, J.A.: L-fuzzy sets. J. Math. Anal. Appl. 18, 145–174 (1967). This is Goguens ﬁrst
article on his generalization of fuzzy sets to “L-sets”; see also [38]
38. Goguen, J.A.: Categories of Fuzzy Sets: Applications of a Non-Cantorian Set Theory. Univer-
sity of California at Berkeley, Ph. D. Diss. Dept (1968). This is Goguen’s Ph. D. thesis. He
generalized fuzzy sets to “L-sets”. An L-set is a function that maps the fuzzy set carrier X into
a partially ordered set L; X →L
39. Goguen, J.A.: The logic of inexact concepts. Synthese 19, 325–373 (1969). In this article
Goguen interpreted the elements of L as “truth values”
40. Zadeh, L.A.: Fuzzy Languages and their Relation to Human and Machine Intelligence. In: Man
and Computer. Proceedings of the International Conference Bordeaux 1970, Karger: Basel, pp.
13–165 (1970). In this proceedings paper Zadeh’s thesis was that the diﬀerence between human
and mechanical intelligence lay in the ability of the human brain “an ability which present-day
digital computers do not possess—to think and reason in imprecise, non-quantitative terms”.
He said that humans could understand inexact instructions, whereas inputs for a computer had
to be deﬁned with precision. He suggested devising fuzzy languages which functioned such
that commands formulated in a language like this could also be processed and carried out by
future computers
41. Wee, W.G.: On a Generalization of Adaptive Algorithms and Applications of the Fuzzy Set
Concept to Pattern Classiﬁcation. Ph.D Thesis, Purdue University, Technical Report, vol. 67,
issue no 7 (1967). In his Ph D dissertation, written this work under King Sun Fu, Wee had
applied the fuzzy sets to iterative learning procedures for pattern classiﬁcation and he had
deﬁned a ﬁnite automaton based on Zadehs concept of the fuzzy relation as a model for learning
systems
42. Wee, W.G., Fu, K.S.: A formulation of fuzzy automata and its application as a model of learning
systems. IEEE T. Syst. Sci. Cyb. SSC- 5(3), 215–223 (1969)
43. Zadeh, L.A.: Fuzzy algorithms. Inf. Control 12, 99–102 (1968). In this article Zadeh introduced
the concept of fuzzy algorithms
44. Zadeh L.A.: Toward fuzziness in computer systems. Fuzzy Algorithms and Languages. In:
Boulaye, G.G. (ed.) Structure et Conception des Ordiinateures - Architecture and Design of
Digital Computers, École d’été de l’O.T.A.N., A N. A. T. O, pp. 9–18. Advanced Summer
Institute, Dunod, Paris (1969). In this contribution to a NATO summer school Zadeh gave a
view on his expectations on fuzziness in the area of computers
45. Turakainen, P.: On stochastic languages. Inf. Control 12(4), 304–313 (1968). This article
presents a concept of stochastic languages as an approximation to human languages using ran-
domizations in the productions
46. Lee, E.T., Zadeh, L.A.: Note on fuzzy languages. Inf. Sci. 1, 421–434 (1969). In this short paper
Zadeh and his Ph D student Lee present their program to extend non-fuzzy formal languages
to fuzzy languages
47. Hopcroft, J.E., Ullman J.D.: Formal Languages and their Relation to Automata. Addison Wes-
ley, Reading (1969). This is the authors’ seminal study of formal languages that constituted an
important subarea of computer science
48. Zadeh, L.A.: Similarity relations and fuzzy orderings. Inf. Sci. 3, 177–200 (1971). In this article
Zadeh deﬁned similarity relations as a generalization of the concept of equivalence relations
(reﬂexive symmetrical and transitive) and fuzzy orderings as transitive fuzzy relations
49. Zadeh, L.A.: Quantitative fuzzy semantics. Inf. Sci. 3, 159–176 (1971). In this article Zadeh
deﬁned a language as a fuzzy relation between a set of terms T = x and the universe of discourse
U = y. If a term x of T is given, then the membership function L(x, y) deﬁnes a set M(x) in U
with membership function: Mx(y) =L (x, y). Zadeh called the fuzzy set M(x) the meaning of the
term x; x is thus the name of M(x)
50. Zadeh, L.A.: Outline of a new approach to the analysis of complex systems and decision
processes. IEEE T. Syst. Man Cyb. SMC- 3(1), 28–44 (1973). In this article Zadeh treated
fuzzy algorithms and he also integrated the other fuzziﬁcations into a new approach that was

578
R. Seising
supposed to bring about a completely new form of system analysis based on his Fuzzy Set
Theory by using Linguistic Variables, Fuzzy If-Then-rules, and Fuzzy Algorithms
51. Zadeh, L.A.: The concept of a linguistic variable and its application to approximate
reasoning—I. Inf. Sci. 8, 199–249;—II, Inf. Sci. 8, 301–357;—III, Inf. Sci. 9, 43–80 (1975).
In this series of three articles Zadeh introduces his linguistic approach to Approximate Rea-
soning: By linguistic variables as variables whose values are words or sentences in a natural
or artiﬁcial language. He expected that the main applications of the linguistic approach lie in
the realm of humanistic systems, particularly in the ﬁelds of artiﬁcial intelligence, linguistics,
human decision processes, pattern recognition, psychology, law, medical diagnosis, informa-
tion retrieval, economics and related areas
52. Zadeh, L.A.: Fuzzy logic and approximate reasoning. Synthese 30, 407–428 (1975). Zadeh’s
article on the imprecise logical system, FL, in a philosophical journal. In FL the truth-values
are fuzzy sets of the unit interval with linguistic labels such as true, false but also not true, very
true, quite true, not very true and not very false, etc
53. Zadeh, L.A.: PRUF a meaning representation language for natural languages. Int. J. Man Mach.
Stud. 10, 395–460 (1978). In this article Zadeh describes the relation-manipulating language
Probabilistic Relational Universal Fuzzy (PRUF), to precisiate expressions in a natural lan-
guage; to exhibit their logical structure; and to provide a system for the characterization of
the meaning of a proposition by acting on a collection of fuzzy relations in a data base and
returning a possibility distribution
54. Rosch, E.: Natural categories. Cogn. Psychol. 4, 328–350 (1973). In this article the psycholo-
gist Eleanor Rosch demonstrated—based on a series of experiments in the 1970s—that when
people label an object, they rely on a comparison with what they regard as a prototype of the
category designated by that word but less on abstract deﬁnitions
55. Lakoﬀ, G.: Hedges. A study in meaning criteria and the logic of fuzzy concepts. J. Philos.
Logic 2, 458–508 (1973). Inﬂuenced by Rosch and Zadeh the linguist Lakoﬀemployed in this
paper “hedges” (meaning barriers) to categorize linguistic expressions. He also introduced in
this article the term “fuzzy logic”
56. Zadeh, L.A.: A fuzzy-set-theoretic interpretation of linguistic hedges. J. Cybern. 2, 4–34
(1972). Zadeh’s article on “linguistic operators”—e.g. very, more, more or less, much, essen-
tially, slightly etc., which he called “hedges”
57. Zadeh, L.A.: Interview with LotﬁZadeh, creator of fuzzy logic by Betty Blair. Azerbaijada Int.
2(4) (1994). An interesting interview on the history and the future of Fuzzy Sets and Systems
with the founder of this theory
58. Seising, R., Sanz, V. (eds.): Soft Computing in Humanities and Social Sciences. Springer,
Berlin (2012). This anthology presents a generous sampling of a wide array of authors and
subject matters from diﬀerent disciplines. Some of the contributors of the book belong to the
scientiﬁc and technical areas of Soft Computing while others come from various ﬁelds in the
humanities and social sciences such as Philosophy, History, Sociology or Economics
59. Seising, R.: The Experimenter and the Theoretician - Linguistic Synthesis to tell Machines
what to do. In: Trillas, E., Bonissone, P., Magdalena, L., Kacprycz, J. (eds.) Combining Exper-
imentation and Theory—A Homage to Abe Mamdani, pp. 329–358. Springer, Berlin (2012).
This book contribution concerns the life work of two pioneers of Fuzzy Sets and Systems,
Ebrahim H. Mamdani and LotﬁA. Zadeh. Mamdani initiated the development of practical
Fuzzy Control systems whereas Zadeh founded the theory of this ﬁeld. When they have been
asked to characterize their own role in science, the former characterized himself less close to
mathematics than the other
60. Mamdani, E.H.: Advances in the linguistic synthesis of fuzzy controllers. Int. J. Man Mach.
Stud. 8, 669–678 (1976). An article by Mamdani’s on the method of fuzzy control using the
example of the steam engine
61. Mamdani, E.H.: How a mouse crossed scientists mind a conversation with Ebrahim Mamdani.
JAMRIS 2(1), 74–76 (2008). An interesting interview on Mamdani’s view on science and
particular on Artiﬁcial Intelligence

The Genesis of Fuzzy Sets and Systems – Aspects in Science and Philosophy
579
62. Assilian, S.: Artiﬁcial intelligence in the control of real dynamic systems. Ph. D. Thesis Nr.
DX193553, University London (1974). Assilian’s Ph D Dissertation thesis on the ﬁrst fuzzy
control system
63. Mamdani, E.H., Assilian, S.: An experiment in linguistic synthesis with a fuzzy logic con-
troller. Int. J. Man Mach. Stud. 7(1), 1–13 (1975). The article on the fuzzy controlled steam
engine written by Mamdani and Asilian
64. McNeill, D., Freiberger, P.: Fuzzy Logic. Simon and Schuster, New York (1993). A book on
the history of fuzzy sets written by two journalists
65. Tong, R.M.: An Assessment of a Fuzzy Control Algorithm for a Nonlinear Multivariable sys-
tem. In: Proceedings of the Workshop on Discrete Systems and fuzzy Reasoning. Queen Mary
College, London (1976). This paper presents an early fuzzy control application system of in a
basic oxygen steel making process in England
66. Rutherford, D.A.: The Implementation and Evaluation of a Fuzzy Control Algorithmus for a
Sinter Plant. In: Mamdani, E.H., Gaines, B.R. (eds.) Discrete Systems and Fuzzy Reasoning,
EES-MMS-DSFR-76. Proceedings of the Workshop Queen Mary College, London (1976).
This paper presents an early fuzzy control application system of a sinter making plant in Eng-
land
67. Carter, A., Hague, M.J.: Fuzzy control of raw mix permeability at a sinter plant. In: Mamdani,
E.H., Gaines, B.R. (eds.) Discrete Systems and Fuzzy Reasoning, EES-MMSDSFR-76. Pro-
ceedings of the Workshop held at Queen Mary College, University of London (1976). This is
a presentation of a fuzzy controlled sinter making plant in England, see [66]
68. Kickert, W.J.M.: Analysis of a Fuzzy Logic Controller. Internal Report Queen Mary College,
London (1976). This paper presents a fuzzy controlled pilot scale batch chemical process in
England
69. King, P.J., Mamdani, E.H.: The Application of Fuzzy Control Systems to Industrial Processes.
In: Mamdani, E.H., Gaines, B.R. (eds.) Discrete Systems and Fuzzy Reasoning, EES-MMS-
DSFR-76. Proceedings of the Workshop held at Queen Mary College, University of London,
(1976). This paper presents another aspect of the fuzzy control system in [68]
70. Kickert, W.J.M., van Nautka Lemke H.R.: Application of fuzzy controller in a warm water
plant. Automatica 12, 301–308 (1976). This paper deals with a fuzzy control system to control
a warm water plant in the Netherlands
71. Holmblad, L.P., Østergaard, J.J.: Control of a cement Kiln by fuzzy logic. In: Gupta, M.M.,
Ragade, M.R.K., Yager, R.R. (eds.) Advances in Fuzzy Set Theory and Applications. North-
Holland Publishing Company, Amsterdam, New York, Oxford (1979). This article describes
the ﬁrst a commercial fuzzy control system for the automatic control of a cement kiln in Dan-
mark
72. Zadeh, L.A.: Biological Application of the Theory of Fuzzy Sets and Systems. In: Proctor,
L.D. (ed.).: Proc. International Symposium on Biocybernetics of the Central Nervous System,
pp. 199–206. Little, Brown and Comp., London (1969). Zadehs conference paper to advise life
scientists to use fuzzy set theory
73. Seising, R., Sanz, V.: Introduction. In: [58], 3–36 (2012). This introduction to the volume [58]
illuminates the relations between hard and soft sciences and hard and soft computing with each
other
74. Seising, R., Sanz, V.: (guest eds.). Soft computing in humanities and social sciences, special
issue. Fuzzy Sets Syst. 214, 1–96 (2013). This special issue of the journal concerns theoretical
research and practical applications of Fuzzy Sets and Systems in non-technical ﬁelds
75. Zadeh, L.A.: Making computers think like people. IEEE Spectr. 8, 26–32 (1984). In this arti-
cle Zadeh focused on the machines ability to “compute with numbers” that he intends to sup-
plement by an additional ability that is similar to human thinking. The “remarkable human
capability [of humans] to perform a wide variety of physical and mental tasks without any
measurements and any computations.”
76. Zadeh, L.A.: Fuzzy logic = computing with words. IEEE T. Fuzzy Syst. 4(2), 103–111 (1996).
Computing with Words (CW) is introduced in this article as a methodology in which words
are used in place of numbers for computing and reasoning. In CW, a word is viewed as a label

580
R. Seising
of a fuzzy set of points drawn together by similarity, with the fuzzy set playing the role of a
fuzzy constraint on a variable
77. Zadeh, L.A.: From computing with numbers to computing with words - from manipulation of
measurements to manipulation of perceptions. IEEE T Circuits Syst.-I: Fundam. Theory Appl.
45(1), 105–119 (1999). In this article Zadeh proposed Computing with Words (CW) based on
the theories of Fuzzy Sets and Systems and Fuzzy Logic and these methodologies instead of
exact Computing with numbers. In this article he wrote that “the main contribution of fuzzy
logic is a methodology for computing with words. No other methodology serves this purpose.”
78. Zadeh, L.A.: New Direction in AI. Toward a computational theory of perceptions. AI Mag.
22(1), 73–84 (2001). In this article Zadeh outlines the Computational Theory of Perceptions
(CTP). CTP as a new direction in AI is intended to be a capability to compute and reason with
perception-based information. Perceptions would be described by propositions drawn from a
natural language
Author Biography
Rudolf Seising born 1961 in Duisburg, Germany, obtained
his Ph.D. in Philosophy of Science and the German Habil-
itation in History of science from the Ludwig-Maximilians-
University in Munich after studies of Mathematics, Physics
and Philosophy at the Ruhr-University of Bochum (Germany).
Dr. Seising has been Scientiﬁc Assistant for computer sci-
ences at the University of the Armed Forces in Munich from
1988 to 1995 and for history of sciences at the same uni-
versity from 1995 to 2002. From 2002 to 2008 he was with
the Core unit for Medical Statistics and Informatics at the
University of Vienna Medical School, which in 2004 became
the Medical University of Vienna. Since 2005 he is College
Lecturer at the Faculty of History and Arts, at the Ludwig-
Maximilians-University Munich. From April to September 2008
he was acting as Professor for the History of science at the
Friedrich-Schiller-University Jena (Germany) and from September 2009 to March 2010 at the
Ludwig-Maximilians-University in Munich. He was Visiting Researcher )2008-2010) an Adjoint
Researcher (2010-2014) at the European Centre for Soft Computing in Mieres (Asturias),
Spain and he has been several times Visiting Scholar at the University of California, Berkeley.
Recently he is again acting as Professor for the History of science at the Friedrich-Schiller-
University Jena (Germany). Since 2004Dr. Seising is Chairman of the IFSA Special Interest
Group “History” and since 2007, of the EUSFLAT Working Group “Philosophical Founda-
tions”.In 2011he became member of the IEEE Computational Intelligence Society (CIS) His-
tory Committee, and since 2013 also of the IEEE CIS Fuzzy Technical Committee. In 2013 he
founded (with A. Sobrino, M. E. Tabacchi, and M. Pereira Fari?a the Online journal Archives
for the Philosophy and History of Soft Computing (www.aphsc.org). His main areas of research
comprise historical and philosophical foundations of science and technology. Among other
books he edited Views on Fuzzy Sets and Systems from Diﬀerent Perspectives. Philosophy and
Logic, Criticisims and Applications. (Springer 2009), (with V. Sanz) Soft Computing in Humani-
ties and Social Sciences (Springer 2012), (with E. Trillas, S. Termini, and C. Moraga) On Fuzzi-
ness. A Homage to LotﬁA. Zadeh - vols. I and II, (Springer 2013) and (with M. E. Tabacchi)
and Fuzziness and Medicine: Philosophical Reﬂections and Application Systems in Health Care.
A Companion Volume to Sadegh-Zadeh?s “Handbook on Analytical Philosophy of Medicine”,
(Springer 2013).

Fuzzy Logic in Speech
Technology - Introductory
and Overviewing Glimpses
Horia-Nicolai Teodorescu
Abstract The chapter critically reviews several applications of fuzzy logic and
fuzzy systems in speech technology, along the main directions of the ﬁled: speech
synthesis, speech recognition, and speech analysis. A brief incursion in the use of
mixed techniques, combining fuzzy logic, fuzzy classiﬁers and nonlinear dynamics
is included. A rich list of references complements the chapter.
1
Introduction
The applications of speech technology traditionally fall in three main classes:
speech recognition, speech synthesis, and speech analysis with applications in
psychology – including the more recently developed emotion analysis and related
sociology issues, linguistics (phonetics), and medicine – mainly speech pathology,
respiratory pathology, and dentistry. At least one class of applications, namely
speaker recognition, falls between the sub-domains of speech recognition and
speech analysis. Automatic speech synthesis has historically been the ﬁrst to be
developed, much before the use of computers, with important studies between 1920
and 1960, supported chieﬂy by the telephony industry. However, automatic speech
synthesis was successful only after the widespread introduction of microprocessors
and high volume memory chips. Speech recognition became a ﬁeld of systematic
research only after the introduction of mini- and micro-computers (PCs), and the
development was accelerated since the advent of digital signal processors (DSPs).
Again one of the main beneﬁciaries has been the automatic response systems in
telephony and later the intelligent phones. A steady development has seen speech
analysis for phonetics and those with medical applications (voice and speech
pathology). On the other hand, speech recognition covers numerous industrial,
H.-N. Teodorescu (✉)
Institute of Computer Science of the Romanian Academy, Gheorghe Asachi Technical
University of Iasi, Iasi, Romania
e-mail: hteodor@etti.tuiasi.ro
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_28
581

human computer interaction, and communication applications, including voice
command, voice data entry, and query-answer systems (Kasabov and Iliev 2000).
As side products, large repositories (speech databases), corpuses (databases
of annotated speech ﬁles), and ontologies were created (see for example Naphade
et al. 2006).
The use of fuzzy logic (FL) in speech technology follows the same main divi-
sions as speech technology. While one could expect that most researches would
apply fuzzy logic to speech recognition, this is not precisely the case. More
attention was paid to speech analysis and applications, at least by the count of
papers. This may be due that good methods for speech recognition based on
Markov chains were available early in the 1990 s, making fuzzy classiﬁcation less
attractive. The situation largely perpetuates until today. A recent search on the IEEE
Xplore revealed 45 journal papers including in their abstract the words “fuzzy” and
“speech” and 413 conference papers with the same key words. Out of the 45 journal
papers in this database shown that the topics of most papers fall into the categories
speech analysis and speech recognition. The distribution of papers per categories of
topics is shown in Fig. 1.
Fig. 1 Distribution of main topics of papers on speech and fuzzy published in IEEE journals, in %.
Analysis category includes the evaluation of speech quality and identiﬁcation of speaker.
Applications category includes speech-based control, andprocessing category includes segmentation
Fig. 2 Worldwide patents in the ﬁeld of speech technology using FL, according to the targeted
topic(s). Data based on a search on EspacenetTM (74 results found in the Worldwide database for
fuzzy speech in the title or abstract)
582
H.-N. Teodorescu

The Worldwide database on EspacenetTM contains (as at October 2014) 74
patents that include the terms “fuzzy” and “speech” in the title or abstract. Figure 2
illustrates the approximate distribution of worldwide patents using FL in speech
technology; the distribution is on topics that differ from those in Fig. 1 to better
reﬂect the categories found in patents. Some patents fall in two categories, while a
few others in the search results were marginally related to speech technology and
were omitted; therefore the sum is slightly smaller than 74, as obtained in the
search. Interestingly, in contrast to paper literature, by far the main application of
FL in speech technology is speech recognition – more than 40 % - while the topics
of speech processing and various applications maintain a high position.
The division into categories in Figs. 1 and 2 is subjective and partly imposed by
the topics dealt with in the papers in the database; readers may ﬁnd other classi-
ﬁcations of papers and patents more suitable for their purpose.
The number of articles that include in title and abstract both the words fuzzy and
speech in the ScienceDirect database is 114. Interestingly, the yearly number of
papers published in the period since 1995 in the ﬁeld and reported by ScienceDirect
remained essentially ﬂat, around 4 per year. This is a very low number, taking into
account that this publisher has about 20 journals that published papers in the ﬁeld,
including the ﬂagship journal Fuzzy Sets and Systems. A much higher number of
articles (including book chapters) is returned for the same keywords in the whole
articles by Springer Link: more than 11’000, out of them more than 3’700 articles,
but we estimate that about 10–30 % of them are actually related to the use of fuzzy
logic in speech processing, analysis, synthesis, and recognition. On the other hand,
a search using the European Patent search engine Espacenet detects 78 results found
in their Worldwide Database for the words fuzzy and speech in the title or abstract.
Most of these patents and patent applications are new, after 2000, and many of them
come from Asian inventors (Japan, China). Also, the result of search in US Patent
Collection for the terms fuzzy and speech, speciﬁcally in the claims, produces 79
patents, most of them registered after 2000.
This chapter is partly a review article with reference to a small sample of FL
techniques applied to speech technology of the main papers and to some research
tracks the author was involved in. The review is by no way exhaustive, however
efforts have been done to illustrate a wide range of topics covered by the literature,
including both journal and conference papers, moreover the patent literature,
striking a balance between the two categories of articles – a feature unfortunately
seldom respected in reviews.
In addition to reviewing the subject, the chapter describes in detail and gener-
alizes an approach which, according to the author opinion, deserves further
investigation in the future, namely the approach based on a generalization of fuzzy
information space and related fuzzy classiﬁcation based on representations in this
space. While not being a review in the proper sense, because of the vast number of
topics and approaches, the reviewing insights are critical in the sense that they
emphasize the limits in the results obtained so far by the application of fuzzy logic
to speech technology; also, the review discusses possible weaknesses and cir-
cumstances that motivated this situation.
Fuzzy Logic in Speech Technology …
583

The chapter is intended to help both experts in FL and in speech technology to
understand the other domain and we hope it will help postgraduate students and
young researchers to devise new research ﬁelds crossing the boundary from FL to
speech technology and vice versa.
For the beneﬁt of the audience from FL ﬁeld, we recall a few notions from
speech technology. Speech is either voiced or voiceless. In voiced speech, the vocal
folds vibrate almost periodically; the vibration frequency is named pitch, or
(number) zero formant, denoted by F0. The change in F0 value is the main con-
tributor to the representation of prosody. The basic voice waveform (signal) pro-
duced at the level of the vocal folds is further shaped by the larynx, buccal and
nasal cavities. The sound spectrum in voiced speech is due largely to this shaping,
which determines the main spectral components, named formants – the main being
denoted by F1 to F4. The ﬁrst two formants, F1 and F2, provide the main infor-
mation needed to recognize the voiced phonemes. All vowels and many consonants
are voiced phonemes. Some phonemes are unvoiced, that is, they are not produced
with the contribution of the vibration of the vocal folds. Examples of these
unvoiced phonemes are the plosive and the fricative consonants.
Beyond the discrete Fourier power spectrum {S(ωj)}j, typically reduced to the
values of the pitch (F0) and of the formants (F1···F4) and of their bandwidths,
speech segments are characterized by several easy to deﬁne and compute param-
eters, such as their energy, duration, “number of zero-crossings” (see Sect. 3) and
by the values in the descriptive statistics of these parameters, typically the average
value, spreading (standard deviation), minimum and maximum value, median, ﬁrst
and third quartiles, and range. After acquisition by a digital system, speech is
originally represented by a sequence of samples, fs t1
ð Þ, ⋯, s tn
ð Þ, ⋯g, where tn
represent time moments equally spaced. In typical representations, there are
between 16’000 and 96’000 such values per second, depending on the sampling
frequency used. Taking into account that a phoneme lasts between a few mili-
seconds (ms), for plosives to several tens of ms (for sustained, long vowels), the
initial processing for extracting the above parameters is performed on speech
segments of around 10 ms duration. Essentially, speech representation and pro-
cessing techniques belong to time series techniques – a ﬁeld familiar to scientists
from almost all branches of science. The computation on a ‘time window’ including
W samples of the energy is then performed as E = ∑k∈ws2
k, where the (abusive)
notation k ∊W means that the time moment tk is in the window W. Assuming that
the central sample of the time window (segment) is tn, we denote the window by Wn
and the corresponding energy by En. Similarly, in a second example, the range of
the signal samples in the window is deﬁned as rn = maxk∈Wn sk −mink∈Wn sk. This
phase of processing represents every window by a set of values of several
parameters, En, rn, ⋯. Each value of a parameter aggregates information from all
samples in the corresponding speech segment. Preserving the same order of the
parameters from one window to the next, at the end of this representation phase,
the segment is replaced by a vector of parameters, vn = ðF0n, F1n, ⋯, En, rn, ⋯Þ.
If the successive windows have their centers at a distance L ≫1, L ≤W=2,
584
H.-N. Teodorescu

the information in the speech segment may be signiﬁcantly compressed into the
vectors vn, because there is a single vector every L samples and we assume here that
the number of the components of the vector is much smaller than L. For example, if
L = W/10, there are 10 values of the representation vector for every speech segment.
In the next steps, the sequence
vn
f
gn of vectors representing the information in
the signal successive windows is used for further signal characterization. Again, one
can use the descriptive statistics of the components of the vector vn in the segment
and to create a new vector for every parameter. For example, for every 3 ms, using
windows of 3 ms width with the centers of successive windows every 0.1 ms, there
are 30 values of the vector vi = ðF0i, F1i, ⋯, Ei, ri, ⋯Þ. Then one can compute the
average of the corresponding values F0i in the segment corresponding to the index n
as F0n = F0 tn −1. 5 ms
ð
Þ + F0 tn −1. 4 ms
ð
Þ + ⋯+ F0 tn + 1. 5 ms
ð
Þ
ð
Þ=31. The set of
these new parameters can be used as ﬁnal representation of the signal in the window,
for example to determine if the signal is noise, unvoiced speech, or voiced speech.
The scope of this chapter is limited to providing a general view on the topic, and
a few insights that the author ﬁnds more promising or believes are good, yet easy to
understand examples of applications of FL to speech technology. Some minimal
knowledge of FL and rudimentary knowledge of signal processing are required,
rather at the level of terminology than at that of fundamental results. For basic
concepts and methods in speech technology, the reader is referred to books, such as
(Benesty, Sondhi and Huang 2008).
The next sub-section describes applications of FL to voice synthesis. Applica-
tions of fuzzy logic to voice processing and analysis are dealt with in the third
section, which also includes an extended sub-section on the detection of voiced,
unvoiced, speech, and noise segments, moreover a subsection on medical appli-
cations of speech analysis, while the fourth section deals with speech recognition.
The next (ﬁfth) section is a special incursion into the fuzzy information space
representation, which may be a good potential tool in both speech analysis and
speech recognition. This section includes a subsection of speaker recognition. The
sixth section overviews emotion estimation in speech, a currently important topic in
the ﬁeld. The ﬁnal section presents conclusions and potential future directions.
2
Applications of Fuzzy Logic to Voice Synthesis
One of the early papers on fuzzy logic applied to speech synthesis is (Raptis and
Carayannis 1997), who used fuzzy logic for rule-based formant speech synthesis.
In a study supported by research grants from the Romanian Academy during
1999 and 2002, Grigoras et al. (1999, 2000), Jitca et al. (2002) proposed a fuzzy
system for controlling the synthesis of the phonemes that are prone to multi-
deﬁnitions in rule-based speech synthesizers. The purpose was to control a Klatt-
type synthesizer (Klatt 1980) with the aim of improving the intelligibility and
Fuzzy Logic in Speech Technology …
585

naturalness of the speech produced by the basic Klatt synthesizer. The fuzzy system
controls the values (frequency) of the formants and their dynamics. The compu-
tation of the formant values takes into account the context of a sequence of two
syllables. The approach is exempliﬁed in that paper for the liquid phoneme l, for the
Romanian language, because of the difﬁculty of synthesizing this phoneme cor-
rectly. An increased intelligibility of the TtS (Text to Speech) synthesizer was
demonstrated using the FL controlled synthesizer. However, this type of application
fell into desuetude together with the fall of interest in Klatt synthesizers after 2000.
Also related to the topic of speech synthesis is the theme of articulatory system
modeling. In his paper, Brito (2009) proposed such a model based on a fuzzy-
genetic approach and conﬁrmed its validity for the Spanish vowels. Lin et al. (2004)
presented a text-to-speech (TtS) synthesis system based on FL, namely using
recurrent fuzzy neural network (RFNN) and fuzzy rules for prosodic phrase
structure in a concatenative synthesizer.
A few patents use FL for speech synthesizers. Tanaka and Masanobu (2000)
obtained the United States Patent 6,081,781 titled “Method and apparatus for
speech synthesis and program recorded medium”. Their synthesizer employs as
main idea fuzzy vector quantization for coding the spectrum envelope of speech
segments. Also, Coorman et al. (2007) were granted US Patent no. 7219060 for
“Speech synthesis using concatenation of speech waveforms”. These inventors used
only a form of characterization of the synthesized speech with linguistic degrees;
they name the related representation “fuzzy table”, but there is no true use of FL in
their patent.
We end this brief section signaling a recent patent that uses in a novel way the
context information for improving speech synthesis in TtS (text to speech) syn-
thesis. In the patent US2012221339, “Method, apparatus for synthesizing speech
and acoustic model training method for speech synthesis” the inventors Wang et al.
present a model-based speech synthesizer employing fuzzy context information for
improving speech synthesis; they describe the procedure as “generating fuzzy
context feature labels based on the plurality of candidate pronunciations and
probabilities thereof, and determining model parameters for the fuzzy context
feature labels based on acoustic model with fuzzy decision tree.”
Having advocated for a long time for the ambient-adaptive speech synthesis and
having proposed adaptation ways (Teodorescu et al. 1988), (Teodorescu 2001a),
(Teodorescu 2005) for creating stress rendering, emotion rendering, and relation-
ship rendering, we must agree that these issues remain a largely unchartered
territory. Also, little progresses were made in assessing speech quality, both
synthesized (Teodorescu, Feraru and Zbancioc 2009) and natural speech.
586
H.-N. Teodorescu

3
Applications of Fuzzy Logic to Voice Processing
and Analysis
FL for improving voice communications and general voice signal processing
There are numerous applications in speech technology, especially in speech anal-
ysis, where we see the use of fuzzy logic and fuzzy techniques as one of the primary
choices. For example, detecting speech disabilities is much more difﬁcult than
detecting Parkinson for example, because, in this case, Parkinson has a few and
quite speciﬁc signs, while speech disabilities have numerous causes and varied and
subtle ways to manifest them, with many yet unknown processes, imprecision in
knowledge, and variability. Hence, the use of FL and fuzzy classiﬁers may be a
good choice to deal with this problem. Other similarly difﬁcult problems are the
detection of dialects and sub-dialects in phonology and of the local variants,
especially in cases of small populations speaking a dialect or language variant, thus
making statistical methods only partly useful (due to the small sample effect).
Already in 1994,
Ndousse (1994), in one of the ﬁrst papers on FL in voice communications,
introduced a FL-based controller in asynchronous transfer mode (ATM) transmis-
sion of voice messages. The motivation of using FL is that “Typical voice cells,
characterized by a high degree of burstiness, complicate any attempt to use clas-
sical control theory in the design of an ATM cell rate controller.” The FL controlled
ATM management proposed by Ndousse is essentially a modiﬁed leaky bucket cell
rate control. Ndouse (1998) further details the method in the book chapter on Fuzzy
Expert Systems in ATM Networks.
Cheng and Chang (1996), starting from a paper by Murata et al. (reference 6 in
Cheng’s and Chang’s paper), provide a meticulous analysis of uncertainties in
ATM networks, saying that
“it is difﬁcult for a network to acquire complete statistics of input trafﬁc and, as a result, it is
not easy to accurately determine the effective thresholds or equivalent capacity in various
bursty trafﬁc ﬂow conditions of ATM networks; … therefore, the decision process is full of
uncertainty.”
Cheng and Chang relay on the work by Ndousse, discussed above, and on other
works, for their proposal of a FL-based trafﬁc controller that models an ATM
network. The FL-based trafﬁc controller includes a fuzzy bandwidth predictor
(which predicts the available equivalent capacity), a fuzzy admission controller, a
fuzzy congestion estimator, and a performance estimator. The fuzzy congestion
controller has three inputs, the queue length, the variation (change rate) of the queue
length, and an estimate of the cell loss probability. The fuzzy admission controller
inputs are the output of the fuzzy congestion controller, the estimate of the cell loss
probability, and the result of network resource estimation, determined by another
block. Only two linguistic degrees are used for the change rate, namely “positive”
and “negative”. Triangular and trapezoidal membership functions are used for the
inputs and singletons for the output. The simulation results reported are impressive:
Fuzzy Logic in Speech Technology …
587

about halving of the message blocking probability for both voice and video trafﬁc,
and almost null message blocking probability for either low- and high-bit-rate data
trafﬁc.
In another line of research, aimed to characterize and better understand speech
signals and processes, a speciﬁc approach in speech analysis and recognition was
presented in (Rodriguez et al. 2000), combining non-linear dynamic characteristics
of speech and fuzzy representation and classiﬁcation methods. Precisely, the
research aims to represent the dynamics of speech by temporal fuzzy sets and, based
on them, to represent the trajectories of the speech production dynamical system.
The application to speech analysis and potentially phoneme recognition relies on
determination of similarity measures of the corresponding temporal fuzzy sets.
In a single paper, Ciota (2001) considers FL for both improving the signal-to-
noise ratio (SNR) for speech signals and the decision-making process for whole-
word recognition. In tackling the ﬁrst issue, the author suggests a manner of
sampling the noise spectrum, but we are not convinced about the efﬁciency.
FL in detection of voiced, unvoiced, speech, and noise segments
Speech vs. non-speech segmentation play an essential role in optimizing the
communication bandwidth by suppressing transmission of un-voiced segments and
in reducing the overall noise level (Bouquin-Jeannes and Faucon 1994). Also,
voiced - unvoiced segmentation, that is, ﬁnding the segments of a recording where
the speech is vocalic (produced with the vibration of the vocal folds) and non--
vocalic helps improving the quality of communication and plays an essential role in
speech analysis and recognition.
Although the tasks of segmentation into speech and non-speech (noise) segments
and respectively into voiced and unvoiced segments for the speech segments look
elementary, numerous uncertainties in the process still hamper the solving of these
tasks by technical means. We will discuss in some more detail the topic of seg-
mentation using FL, because the topic is of large technical interest and because it is
narrower and allows us to include it in this brief chapter.
A VAD is an algorithm and the related application aimed at detecting, based on
the features that differentiate speech from other sounds, the temporal boundaries of
the segments of speech in a sound recording. The most obvious features relate to the
Fourier spectrum of the noise and respectively speech. However, the spectrum is
not a perfect indicator of the difference speech – ambient noise, because speech
includes consonant sounds that are, essentially, noise. The typical examples of
noise-like consonants are the unvoiced fricatives [s], [f], [sh], and the (partly)
voiced fricatives, as [z]. Typically, [f] and [s] have almost-uniform (i.e., white
noise) frequency spectrum. Also, plosive consonants, as [p], [d], [b] have the
characteristics of impulsive noise, thus being difﬁcult to differentiate from ambient
noise. The temporal boundaries of the words starting or ending with such consonant
sounds are difﬁcult to establish, even when isolated pronounced and even in low or
no noise conditions. For example, a VAD may have difﬁculty in detecting, under
continuous speech with words pronounced with no pause between them, that the
588
H.-N. Teodorescu

pronunciation of “suppress the sound” is a single segment of continuous speech.
Even at the middle of the words, unvoiced consonants may create errors in the
speech – non-speech segmentation and hence in VADs operation. Therefore, VADs
need more features than the Fourier spectrum (or a set of representative spectral
components) to properly operate.
Some of the features used are the amplitude of the sound and the so called
“number of zero crossings”, NZC, also named “zero crossing counts” or zero
crossing rate (ZCR). For speech, most parameters are computed on time windows
of 3 to 20 ms – time intervals that roughly correspond to the minimal duration of a
phoneme. Because of the use of time windows, the term of instantaneous value (of
the amplitude, for example) denotes another concept as the value of the current
sample of the signal. The (instantaneous) amplitude parameter is usually deﬁned as
the average value of the rectiﬁed (absolute) value of the sound signal in a time
window, where the window is centered on the time instant considered. The cor-
responding formula, with An the instantaneous amplitude at time moment n and sn+k
the signal sample amplitude at time moment n + k. is:
An =
1
2N + 1 ∑N
k = −Njsn + kj
The NZC is deﬁned as the number of zero-crossings per unit time (time win-
dow), taking into account the effect of the noise. When noise is not accounted for,
the computation of NZC is according to
if sn>0 and sn + 1<0
ð
ÞOR sn<0 and sn + 1>0
ð
Þ then
NZC←NZC + 1 increment the NZC counter
ð
Þ.
The incrementing is performed during a speciﬁed window, then the counter is
reset to 0; therefore, NZC has one computed value per window and is represented
by a time series, NZCm where m is the index of the time moment representing the
center of the window. Essentially, NZC informs of the main (lower) frequency
component in the signal, when such a component dominates the spectrum, or the
signal is low-pass ﬁltered. An alternative deﬁnition of NZC is (see for example
Köhler, Hennig, & Orglmeister (2003),
NZCn = ∑k∈Wn
sign sk
ð Þ −signðsk −1Þ
2

,
where sign represents the function signum (sign), Wn is the window centered at time
moment n and the sum is over all time moments in the window.
Noise occurs both in the recording circuits (microphone and ampliﬁer) and in the
analog to digital conversion (ADC) process, due to the ﬁnite resolution of the
conversion. The ambient noise adds to these internal noises. The effect of internal
noise on the NZC is a false, high increase of the NZC count. For avoiding at least
Fuzzy Logic in Speech Technology …
589

partly the effect of internal noise, the computation of the NZC uses thresholds θ that
are correlated with the expected internal noise level ν, according to
if sn>θ ν
ð Þ and sn + 1< −θ ν
ð Þ
ð
Þ OR sn< −θ ν
ð Þ and sn + 1>θ ν
ð Þ
ð
Þ then
NZC←NZC + 1 increment the NZC counter
ð
Þ.
On the other hand, the threshold thresholds θ cannot be larger than the voice
signal at its low levels (i.e., consonants). An optimal threshold can be determined
for speciﬁed noise conditions, while an adaptive threshold is needed for varying
ambient noise. The main problem of current VADs is the lack of robustness: their
performance strongly degrades for low signal to noise ratios (SNR), especially
when noise is not stationary and several types of noise occur. The choice of the
threshold could beneﬁt of the FL techniques, when the noise is not stationary or its
value is uncertain. This situation was documented by (Tian et al. 2003), who stress
that
“VAD approaches that use threshold… cannot achieve consistent accuracy since the mean-
value based and the histogram based threshold estimation algorithms are not robust. They
strongly depend on the percentage of voice and background noise in the estimate interval.”
Tian et al. used fuzzy clustering and Bayesian information for estimating the
thresholds for energy features, for VADs; they found the approach robust in non-
stationary environments. Increasing the information aggregated and used in the
decision making also improves the results. The so called higher order counts,
which represent NZC of ﬁltered signals, are also used as features in signal char-
acterization and recognition, for example see Petrantonakis and Hadjileontiadis
(2010). Again, the use of FL information aggregation and FL-based decision
making for utilizing the higher order counts was not studied yet, although it may
bring beneﬁts.
In several papers, Beritelli et al. proposed various VAD algorithms involving
FL. For example, Beritelli, Casale, and Cavallaro (1998, 1999) proposed a fuzzy
voice activity detection (FVAD) algorithm for telephony which, according to the
authors, because it is based on a new manner of pattern matching, using fuzzy logic,
achieves a very good immunity to noise. The authors claim that only six fuzzy rules
optimized by training using the “FuGeNeSys hybrid learning tool”, a supervised
training tool based on both NN and genetic algorithms (GA), able to generate
optimized fuzzy rules.
These authors used the features as in ITU-T Recommendation G.729, namely the
variation (difference) between the current temporal window of the NZC, ΔNZC,
and the average value; the variation of the (full band) energy, ΔEtot, the variation of
the energy in a speciﬁed low frequency band, ΔEL, and the spectral change, ΔS,
where the window is 10 ms. In the newer standard ITU-T G.729.1, which super-
sedes and improves G.729, the lower frequency band corresponds to 50–4000 Hz,
and the higher band to 4000–7000 Hz (IUT 2006); however, we are not aware of
any fuzzy implementation of VADs speciﬁcally for the newer IUT standard.
590
H.-N. Teodorescu

The VAD proposed by Beritelli et al. (2002) uses essentially a Sugeno-type FLS
(that is, crisp output values) with weighted mean (WM) defuzziﬁcation and with
Gaussian input membership functions. The rules of the system have variable
number of antecedents, with all antecedents connected by AND. Remarkably,
Beritelli et al. succeed to reduce the knowledge base needed to a set of a few rules,
to eliminate unnecessary antecedents in rules and to obtain very good results,
frequently outperforming other techniques.
Correct speech /non-speech segmentation is important in improving the quality of
hearing aids audition in noisy ambient by reducing noise between the non-speech
duration. The solution proposed by (Ramirez et al. 2004) does not employ fuzzy logics
and the authors show somewhat better results by their non-FL solution than those
previously obtained by Beritelli et al. (1998, 1999), Cavallaro et al. (1998). The same
paper compares results obtained by several methods, with the FL-based method
comparable, but not always better (under various SNRs and noise levels) than the other
solutions. However, because fuzzy logic systems (FLSs) generalize crisp systems and,
according to the representation theorems, FLSs can implement any nonlinear crisp
system, we can conﬁdently say that the application of FL can bring beneﬁt in this
application too. For further researches, we consider that several other parameters
should be used to increase the VAD performance and extensive use of pattern matching
applied for increasing the discrimination between speech and no-speech activity under
high noise levels. We also believe that nonlinear dynamics analysis, possibly combined
with FL as explained in this chapter may ﬁnd applications in VADs.
An interesting and powerful set of approaches mixing FL, nonlinear dynamics
analysis and various statistical tools (Bayesian information criterion, BIC, among
others) was recently introduced in a set of papers by Zhao et al. In the paper (Zhao
et al. 2011), a robust VAD was proposed based on multi-level Lempel-Ziv com-
plexity (MLZC) with the multiple thresholds determined by a combination of fuzzy
c-means (FCM) clustering and BIC. These authors ﬁnd that the VAD behaves
roobustly for SNR up to 10 dB, for various types of noises.
There are numerous other contributions to the VAD topic brought under back-
grounds not related to FL, including general statistical, perceptual, clustering, optimal
ﬁltering, and correlative settings. Compared to the number of papers using non-FL
approaches, those using FL in VADs are very few and not always fully convincing.
FL in medical applications of speech analysis
The subtle changes in voice, compared to typical voices of healthy subjects, as well
as signiﬁcant and rapid changes in a subject voice may be related to a large class of
pathologies. Indeed, voice production jointly and intimately relies on air ﬂux gen-
erated by the chest (lungs and chest muscles under neural control), vocal cords,
larynx, pharynx, buccal and nose cavities, neck and face muscles, teeth, and lips.
Any pathology of these parts of the body, including of their neural control, may
produce voice and speech changes ranging from light to dramatic. The corre-
sponding medical ﬁelds can beneﬁt for diagnostic purposes from tools of voice and
speech analysis. For example, low air ﬂux and volume may translate in low voice
Fuzzy Logic in Speech Technology …
591

volume, inability of maintaining sustained vowels, higher voice jitter, and degraded
prosody. Neurological disorders in the control of any of the parts contributig to
speech may produce changes in voice and prosody quality, hoarseness and jitter.
Anatomic and functional imperfections of the articulators (tongue, nose cavity,
buccal cavity, teeth, lips) are well known to change speech in a dramatic way.
Actually, numerous diagnostic techniques, devices and applications based on voice
analysis have been proposed in the last 20 years, including entirely new sub-domain,
as ‘gnathophony’ (a study of the relationship between the chewing apparatus and
speech production and deﬁciencies), some of them based on or connected to FL.
Subsequently, we exemplify some of these directions related to FL. Most uses of FL
in medical applications of speech regard diagnosis, as already stated, and mainly
data aggregation, classiﬁcation, and decision making in diagnosis.
Stylios et al. (2008) dealt with “voice quality assessment, including nasality of
speech, hoarseness, breathiness, voice tremor, strained voice, voice breaks, diplo-
phonia”, where the voice quality was determined using a fuzzy cognitive map
(FCM). These authors also fuzziﬁed the pitch using a FCM, to render the meaning
of low and high pitch, pitch breaks, and mono-pitch. The ﬁnal goal was to produce
an instrument for diagnosing of the dysarthria and apraxia of speech, using the
aggregation of the results of the two previous FCMs in a third one. The authored
argued that the results compare well to the diagnostics made by a language
pathologist; however, they studied only four patients.
A partly similar research was reported by Scherer et al. (2013), who studied
“fuzzy-input fuzzy-output support vector machines for robust voice quality clas-
siﬁcation”, but their aim was emotion and mood detection rather than pathologies
(see Section on Emotion detection in this chapter).
In a paper on the relationship between denture state and speech quality (Teod-
orescu and Feraru 2008), the authors suggest the use of FL for improving diagnostic
and also suggested a “high-pass” membership function for the degree of speech
discrimination as a voice quality indicator for diagnostic use. The membership
function was deﬁned based on Euclidean distances between phonemes. However,
these authors have left the issue open, with no checking of the capabilities of the
suggested fuzziﬁcation method.
We believe that FL has a strong potential in speech-related medical applications,
although it is not yet very well represented in the literature devoted to the ﬁeld of
speech pathology.
FL in phonetic segmentation
In the paper by Toledano, Crespo and Sardina (1998), a mixture of HMM and fuzzy
logic is used for improving the phonetic segmentation (phoneme boundary detec-
tion) quality. This task does not need to recognize the phonemes, but the transitions
from one to another. These authors used a two-stage segmentation algorithm, where
the FL was used only in the second stage, of decision making and reﬁning the
boundaries of the phonemes, based on time marks produced by a context-dependent
phonetic HMM recognizer and by a feature extraction block. The feature extraction
block characterizes the speech windows employing, among others, mean energy,
592
H.-N. Teodorescu

zero crossing rate and mean frequency. The decision on the phoneme boundaries is
made using the degree of conﬁdence produced conjointly by the feature extraction
bloc and the HMM one. The results of segmentations are very good, around 97 %,
which can be explained, according to the authors of that paper, by the use of
context-dependent models. Despite the good results, the research track of using FL
in decision making for phonetic segmentation was not followed by other papers.
4
FL in Speech and Speaker Recognition
The topic of speech and speaker recognition is too vast to deal it in any detail in a
single chapter, therefore this section of the overview remains sketchy.
Speech recognition
When FL is applied in speech recognition it was complementing and improving
established tools as hidden Markov models (HMMs). Some favorable results were
obtained with fuzzy NNs, but also limited in extent. In an early paper, Koo and Un
(1990), used a FL-based method “to smooth hidden Markov model parameters” and
obtained higher recognition scores and lower computational load with their method,
compared to the HMM methods using other smoothing techniques of that time.
Essentially, speech recognition is a pattern recognition process; it is approached
by rule-based methods or by statistical methods, the second class of methods having
higher performance and consequently being preferred nowadays (Burileanu et al.
2010). The development of statistical methods required and produced, as a side
effect, large databases and annotated speech corpuses, where annotations refer to
various levels, from voiced-unvoiced-no-voice segments, to phoneme (including di-
and tri-phones) and prosodic levels. These speech corpuses were then used to create
the so-called speech language models, essentially consisting in statistical repre-
sentations of the speech, from the level of phonemes and sub-phonemes, to the level
of word utterances and prosody. These pronunciation-related statistics are then
combined with the so called natural language (NL) language models, which include
the statistics of letters, syllables, words, etc., together with the corresponding
syntax. The development of statistical tools for analyzing speech, on the other hand,
allowed the automatic annotation of speech recording, which further facilitated the
creation of very large speech corpuses, comprising hundreds of hours of recordings
and millions of spoken words. Speech corpuses are often distinguished according to
their purposes: training of speech recognition systems, training of speaker recog-
nition or validation, emotion recognition, computer interfaces and answering
machines, or general purpose “spoken language repositories”, that may include tens
of millions of spoken words. The particularities of the corpuses depend on their
purposes. Almost any language has today several such corpuses; for example, at
least three corpuses were developed for the Romanian language (Cucu et al. 2014),
(Feararu et al. 2010) and a repository is currently under development.
Fuzzy Logic in Speech Technology …
593

Many if not most speech recognition systems in use today are based on hidden
Markov models (HMM) for the language and speech. For example (Burileanu et al.
2010) built a sequential architecture, based on HMMs, which uses MFCC coefﬁ-
cients or speech signal parameterization (for both training and decoding regimes),
acoustic modeling at the triphone-level, with an HMM trained for each phoneme,
and with each HMM having, for each triphone, ﬁve states (including one initial
state and one ﬁnal state, both nonemitting, and three emitting intermediary states).
Also, the system has a “left-right topology (where transitions towards remote states
and backwards transitions are not allowed) and a continuous output distribution
with weighted linear combination of Gaussian mixtures for each emitting state”
(Burileanu et al. 2010).
In several papers, FL was proposed to advance the HMMs used in speech
recognition. In an early development, Koo and Un (1990) proposed a fuzzy
smoothing of the HMM parameters in speech recognition. Cheok et al. (2001),
introduced novel generalized fuzzy hidden Markov model for speech recognition,
based on the fuzzy integral theory (Choquet integral). According to these authors,
the use of Choquet integral relaxes one of the two independence assumptions in
classical HMM theory. The author claim that, although FL typically require com-
putational-intensive algorithms, their method actually lowers the computation time.
Pal and Mitra, in their early paper in (1992), analyzed the use of multilayer
perceptrons and fuzzy sets in classiﬁcation applications. In this line, several
research groups endeavored to use FL in speech recognition either combining FL
and NNs or applying fuzzy rules and systems for decision and classiﬁcation.
(Kasabov and Iliev 2000), in a conference paper, deal with “adaptive speech rec-
ognition in a noisy environment (ASN).” Essentially, the authors describe “a system
based on the described method can store words and phrases spoken by the user and
subsequently recognize them when they are pronounced as connected words in a
noisy environment.” The system relies on a “speech recognition module that uses
evolving fuzzy neural networks (EFuNNs),” able to assign membership degrees to
noisy speech segments to non-noisy speech segments previously learned by the
system. For this purpose, the input vector of features and vectors of features stored
in memory are compared. The algorithm for training the “evolving fuzzy neural
network” (EFuNN) is also given. These authors registered a patent application for
the system, quoted in their paper as “A methodology and a system for adaptive
recognition in a noisy environment based on adaptive noise cancellation and
evolving fuzzy neural networks,”Preliminary Patent, University of Otago, 21
December 1999, New Zealand,” (Kasabov and Iliev, 1999).
In another early paper, Mills and Bowles (1996) applied fuzzy logic to improve
dynamic programming for speech recognition, for overcoming the difﬁculties of the
crisp dynamic programming under high noise. These authors proved that the
classiﬁcation accuracy was increased by the use of FL-based technique.
Also an early industrial interest in speech recognition based on FL is the patent US
5,040,215, titled Speech Recognition Apparatus Using Neural Network and Fuzzy
Logic, by inventors Amano et al. (1991), applied by Hitachi, Ltd., Japan. The same
team has partly described the invention in the conference paper (Amano et al. 1989).
594
H.-N. Teodorescu

The patent claims a rule-based phoneme recognition system, where FL is used in
decision making. The system employs pair-discrimination rules, that is aim to dis-
criminate among two phonemes for every pair of phonemes. This is essentially a
template matching based on FL.
In a closely connected, although different approach, (Shikano, Nakamura and
Abe 1991) the speech recognition is performed by matching the voice of speakers
to the speech of a reference, “standard” speaker based on codebook mapping. The
approach is not related to a speciﬁed speech representation such as HMM or
NN-based, but once the coding for the reference speaker is chosen, the same coding
must be used in the recognition. An advantage of the algorithm is the large range of
applications; for example, one performer speech can be transposed to impersonate
other speakers in games and movies.
Speech recognition relying on fuzzy NNs has been also studied extensively. As a
matter of example, in a book chapter and then in a paper, Melin et al. (2005, 2006)
use FL (type-2 fuzzy logic), NNs and GA for pattern matching and decision making
in the recognition process. However, the presented results are too few to derive
statistically relevant conclusions.
Although not in the proper domain of speech, but intimately related to it,
a special mention deserves the research by Temko, Macho, and Nadeu (2008), who
use information fusion based on FL for dealing with highly confusable non-speech
sounds. Examples of investigated sounds by these authors are cough and throat
produced sounds, laughter, sneeze, sniff, and yawn. These authors ﬁnd that fuzzy
measures and Choquet fuzzy integrals are suitable tools for fusing several infor-
mation sources, taking into account the importance of the sources. The results
obtained by fusing combinations of SVM, frequency-ﬁltered ﬁlter-bank energies
(FFBE), differential ΔFFBE (where Δ indicates the time derivative), and HMM,
according to combinations HMM-FFBE, HMM-ΔFFBE, HMM-FFBE+Δ FFBE,
then combinations of these combinations demonstrated an improvement of several
percentage points compared to the best performing of the above classiﬁers.
Also worth noting is the approach combining wavelets and fuzzy logic in speech
technology, illustrated by the papers by Avci and Akpolat (2006), who use “a
wavelet packet adaptive network based fuzzy inference system” and by Juang,
Cheng and Chen (2009), who address with similar tools the speech detection in
noisy conditions.
FL in speaker recognition
The speaker recognition ﬁeld is well illustrated by the paper by (Chibelushi et al.
1993),
“system that uses both acoustic speech and visual speech (motion of visible articulators). …
As an initial step towards this goal, voice has been used together with still face images; this
combination of vocal and facial information has resulted in better recognition accuracy than
from either of the two constituents individually.”
Fuzzy Logic in Speech Technology …
595

The authors, notice along with the literature that
“the features examined to accomplish each task are not valid for the other. Speciﬁcally, the
features needed to allow reliable phoneme identiﬁcation must contain detailed information
about the spectrum (high frequency resolution) and therefore have poor time resolution,
while the features with high time resolution needed to allow precise time mark positioning
have poor frequency resolution and therefore don’t allow accurate phoneme identiﬁcation.”
These authors divide the task in the two respective tasks and perform them in
parallel. At the same time, the two processes are developed as follows. On one side,
the “context-dependent phonetic HMM recognizer” is applied, followed by a
“context-dependent HMM training error cancellation” and “intermediate time
marks” are found. On a second, parallel processing channel, a feature extraction
process is applied to the voice signal, to determine in a second manner the
boundaries. Then, the two time boundary sets of results are input to a fuzzy logic-
based “post correction system” and the ﬁnal time marks are obtained with high
accuracy. With training, speaker-dependent results reach a precision of about 1 %
compared to the manual segmentation, for segments longer than 35 ms, and of
about 10 % for segments of duration 20 ms (see Fig. 4 in the quoted paper). Also,
for speaker-independent segmentation, the results are not degraded more than about
5 % (Fig. 5 in the quoted paper). Interestingly, the fuzzy rules in that paper directly
refer in their conclusions to the probabilities of transitions from one phoneme to the
next.
5
FL in Emotion Estimation
Emotion detection is a relatively new sub-ﬁeld in speech technology, which shows
a vivid dynamic, both in papers and patents.
It is somewhat surprising that fuzzy logic has not been more intensively applied
to emotion analysis in speech, and more broadly to speech prosody analysis, as
speech features are reputedly fuzzy. A notable exception is the early paper by
Massaro and Cohen (2000), who used a “fuzzy logical model of perception
(FLMP)” to characterize emotions in speech, and (De Gelder and Vroomen 2000).
In an early paper hugely cited, Cowie et al. (2001) proposed the correlated use of
clues from face image sequences and information from speech, in an intuitively
natural manner, for extracting relevant features in assessing emotions. Also quite
intuitively, they used a hybrid classiﬁcation system combining neural network and
FL to derive the emotion. It is worth mentioning that similar neuro-fuzzy approa-
ches have been developed later for emotion recognition based solely on face
expressions, see (Ioannou et al. 2005)
In a set of papers published both in conference and journal forms, Lee, Na-
rayanan, Grimm, Kroschel, and co-workers (Lee and Narayanan 2003, 2005;
Grimm, Kroschel and Narayanan 2007; Grimm et al. 2007), investigated several
techniques for emotion recognition in speech, ﬁrst extracting emotion primitives
596
H.-N. Teodorescu

(“valence, activation, and dominance”), including SVM and the corresponding
support vector regressions, and data-driven fuzzy inference systems. According to
these researches, for German speakers,
The results [obtained with SVMs and SVRs] were compared to a rule-based fuzzy logic
classiﬁer and a fuzzy k-nearest neighbor classiﬁer. SVR was found to give the best results
and to be suited well for emotion estimation yielding small classiﬁcation errors and high
correlation between estimates and reference. (from Grimm et al. 2007)
Recently, Zbancioc and Feraru (2012, 2012) have applied Fuzzy k-Nearest
Neighbor (FkNN) classiﬁers, FCM and WKNN algorithms to the estimation of
emotion in Romanian speech, on the SRoL corpus. In the approach utilizing FkNN
classiﬁers, the features vectors consisted of 17 parameters, namely F0, F1-F4
(average, dispersion, median for all formants), jitter and shimmer, all determined at
phoneme-level. These authors report an emotion recognition rate, at phoneme
(vowel) level, for four emotional states (joy, sadness, furry, and normal) between
61 % and 76, with FkNN, for the Romanian language corpus SRoL described in
(Feraru, Teodorescu and Zbancioc 2010). In the second paper, the same authors
apply a variant of Fuzzy C-Means (FCM) classiﬁers, the recurrent FCM, and
Weighted kNN (WkNN) classiﬁers to the same corpus, for emotion estimation.
They obtained similar results by the two techniques (WkNN and FCM), all results
being better than those obtained with kNN classiﬁers, for the same set of features.
Notable, these authors introduced a new classiﬁer, named FCMR algorithm (FCM
recurrent), which performs better than the typical FCM algorithms.
Related to emotion estimation in speech is the topic of human voice quality
estimation (Szekely et al. 2012), (Scherer et al. 2013). As (Scherer et al. 2013) put
it, “The dynamic use of voice qualities in spoken language can reveal useful
information on a speakers attitude, mood and affective states,” but notice that in
assessing the perceived voice quality, human experts frequently disagree. Focusing
on “a voice quality feature set that is suitable for differentiating voice qualities on a
tense to breathy dimension”, these authors introduce a fuzzy-input fuzzy-output
support vector machine (F2SVM) algorithm that was able to estimate the voice
quality of speech recording better than crisp SVMs. Szekely et al. also use fuzzy
SVM.
The meticulous research of Szekely et al. 2012 applied to detecting what the
authors name “target voice styles” in speech combines two classiﬁers, “namely a
fuzzy-output support vector machine (FSVM), and a Gaussian mixture model
(GMM)”. An agreement optimized ensemble (AOE) voting system then determines
the conﬁdence of the classiﬁcation results. Conﬁdence thresholds for the two
classiﬁers are determined by an expression based on relative agreement relA, taking
agreement. Target voice styles were automatically determined with accuracy close
to a remarkable 90 %.
Fuzzy Logic in Speech Technology …
597

6
An Incursion into the Fuzzy Information Space
Representation
Because of the less common approach in (Rodriguez et al. 2000) and the intricacy
of the matter and method, the issue deserves a few more explanations and a general
setting to potentiate further research in the area. The work was partly continued in
speech analysis but with no intervention of FL (Rodriguez+Brito). The foundations
of the matter are given in a paper devoted to establishing a method for signal
analysis in the deﬁned space of fuzzy information (Kosanovic, Chaparro, and
Sclabassi 1996), with envisaged speciﬁc applications to biomedical (EEG) signal
classiﬁcation. Subsequently, we reformulate the framework of a slightly general
approach based on fuzzy information.
Preliminaries. Consider a time-dependent process, as a bio-electric signal, or a
set of simultaneously acquired bioelectric signals, e.g., EEG or ECG signals. The
time variable is discretized, as in a sampling process. Assume that a set of features
xj, j = 1, . . . , m is derived from the acquired signal. Denote the time-dependent
vector of these features by xi = ðxijÞ = ðxi1, . . . , ximÞ, where the index i stands for the
time moment and the second index for the component of the vector. Assume that
there are n classes of signals, Ck, k = 1, . . , n. These classes may be statistical, that
is, at some speciﬁed moment of time, i, the signal has assigned a set of n proba-
bilities to belong to these classes, p xi∈Ck
ð
Þ. On the other hand, the classes may be
fuzzy categories, that is, signals belong to these classes with a certain degree of
conﬁdence, μ xi∈Ck
ð
Þ.
The assignment of a signal to a class is time-dependent in many non-stationary
processes. As a matter of example, the speech signal may represent one phoneme in
a brief time interval, while in shortly after it may represent another phoneme.
Therefore, we will assume that the probabilities, respectively the membership
functions, according to the case in hand, varies in time. Denote the time-dependent
probabilities
(membership
functions)
by
pik = p xi∈Ck
ð
Þ,
respectively
μik = μ xi∈Ck
ð
Þ.Then, for a speciﬁed classk, we obtain a time-dependent probability,
pkðiÞ = p xi∈Ck
ð
Þ, respectively the membership function μkðiÞ = μ xi∈Ck
ð
Þ. With the
discrete time seen as a variable, pk(i) becomes a distribution of probabilities over
the space of the discrete variable i, that is, over the set of integers, Z, or on a subset
of it, when the process has a ﬁnite duration. Similarly, μk(i) is a membership
function deﬁned on the set of integers, Z.
We name the vector of functions μ with the components μk, k = 1, …, n a fuzzy
dynamics. Recall that every component is a function, μk(i). We will say that the
fuzzy process (dynamics) is fuzzy periodic if it has an inﬁnite duration and the
dynamic membership functions μk(i) are periodical. A fuzzy dynamics is chaotic
when the evolution of at least one component μk(i) of the process is chaotic. Then,
the attractor of the process can be constructed in the space of the n variables of the
process.
Two ﬁnite duration processes can be compared at the level of their vectors
μ1 = ðμ1
1, . . . , μ1
nÞ and μ2. A simple method of comparison is to determine the
598
H.-N. Teodorescu

difference of the membership functions and to compute the average distance
between the respective vectors,
d μ1, μ2
ð
Þ = 1
N ∑N
i = 1d
μ1
1i, . . . , μ1
ni


, μ2
1i, . . . , μ2
ni




where N is the duration of the two processes, that is, the number of time moments
they are non-zero.
We will assume the classes are speciﬁed and their prototypes are known.
Example 1
Consider the EEG signal is monitored with 4 (active) electrodes and the signal is
sampled. The samples acquired on the four channels are denoted as sij, j = 1, 2, 3, 4.
Consider that the selected features to characterize the signals at every time moment
are the frequency fj and the amplitude Aj of the main spectral component in the
Fourier spectrum (determined on a speciﬁed window centered on the current
sample.) Consider that the EEG signals are assigned to one of the classes α, β, θ,
based on the two parameters1. Membership functions are deﬁned as functions of the
distance from the “central” (typical, prototype) frequency and amplitude of the
respective classes, namely
μα, j ið Þ = μα, j fj ti
ð Þ, AjðtiÞ


=
1
1 +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
fjðtiÞ −fαðtiÞ

2 + Aj ti
ð Þ −AαðtiÞ

2
q
.
The instantaneous average membership degree of the signals to the class α at
time moment i is
μα, 1 ið Þ + μα, 2 ið Þ + μα, 3 ið Þ + μα, 4 ið Þ


=4.
The overall membership degree is the average, over the recording duration, of
the instantaneous average degree.
Computing as above the average membership degrees to the classes α, β, θ, the
classiﬁcation is produced by choosing the class with the highest degree.
Example 2
Speech signal is acquired at a sample rate of 24 kHz and acoustically Fourier
analyzed. The result retains two features, the formants F1 and F2. The frequencies
of the formants are determined on windows of 3 ms during the utterance of vowel-
like sounds.2 Assuming a language with 15 vowel-like sounds,3 consider that the
prototypes of these vowel-type sounds in the formant space ðF1, F2Þ denoted by πv,
1In fact, only frequency is required in this case, but we pursue the two-parameter feature vector for
sake of a more complete example.
2A vowel-like sound is detected when there is a valid pitch, that is, when the vocal folds vibrate.
Pure consonants are produced without a pitch.
3Including [m], [n], [l], [r] etc.
Fuzzy Logic in Speech Technology …
599

where v∈a, e, i, o, u, m, n, . . .
f
g. For every time moment i determine for the
unknown sound its values for F1 and F2 and deﬁne its membership to a vowel class
as
μv ið Þ = μv F1 ti
ð Þ, F2ðtiÞ
ð
Þ =
1
1 +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
F1ðtiÞ −F1vðtiÞ
ð
Þ2 + F2 ti
ð Þ −F2vðtiÞ
ð
Þ2
q
.
For the entire duration of the sound,4 determine the average membership to each
class. The classiﬁcation result is that class that to which the sound has the largest
membership degree.
The above deﬁnitions of the membership functions work well for two classes
and in a one-dimensional space, but have several drawbacks when several classes
are involved and when their representation requires a multi-dimensional space.
Considering n classes with their prototypes denoted by πk x1k, x2k, . . . , xrk
ð
Þ, with
the classes overlapping at least two by two. Assuming that the classes occupy
(hyper)spheres, their intersections deﬁne (hyper)planes. Any point on such hyper-
planes has the same membership degree to the respective two classes, potentially a
degree close to 1. That situation is somewhat unintuitive and, when the membership
degrees are close to 1, not convenient for applications (discrimination). An alter-
native deﬁnition of the membership function could be
μk Q
ð Þ = 1 −d Q, πk
ð
Þ= max
j
d Q, πj


which satisfy the condition that μk Q = πk
ð
Þ = 1, but has the drawback of arbitrary
use of maxjd Q, πj


. A better deﬁnition is
μk Q
ð Þ = 1 −d Q, πk
ð
Þ= max
j
d πk, πj


This deﬁnition would allow for negative values, for very far apart Q points.
To correct, we can use
μk Q
ð Þ =
1 −
d Q, πk
ð
Þ
max
j
d πk, πj
ð
Þ for d Q, πk
ð
Þ≤max
j
d πk, πj


0 else
(
Still the deﬁnition inherits the disadvantage of the Euclidian distance, which
considers all directions equal. This is convenient when the classes have spherical
distributions, but not in the general case. For example, in the case of ellipsoidal
classes (as corresponding to the equal probability /Galton) ellipses for Gauss dis-
tributions, the weighting along the axes of the ellipsoids is justiﬁed, see Fig. 2.
4This requires a procedure for determining the boundaries of the sound.
600
H.-N. Teodorescu

Building the dynamic (temporal) membership functions. Consider a sampled
signal s(tn) that we wish to classify, where the signal is represented by several
parameters (features) computed on successive windows of speciﬁed duration. Then,
the distances to the prototypes are computed in the feature space. The above pro-
cedure produces time-indexed values of the membership functions to the speciﬁed
classes, μk(tn). These evolving (time-dependent) membership functions are repre-
sentations of the signal in the fuzzy dynamic space (also called information space).
Considering frames of the same length as above or of different durations, centered
on time moments tn, the dynamics represented by the vectors of values of the
membership functions can be contrasted or compared with prototype dynamics, in
view of ﬁnal classiﬁcation of the signal, for example for phoneme recognition.
Speciﬁcally, at time moment tn, the signal representation in the fuzzy dynamic
space is
μ1 tn
ð Þ, μ1 tn + 1
ð
Þ, . . . , μ1 tn + W
ð
Þ
ð
Þ, . . . , μm tn
ð Þ, μm tn + 1
ð
Þ, . . . , μm tn + W
ð
Þ
ð
Þ,
or
μ1, μ2, . . . , μm
ð
Þn
where m is the number of classes in the features space. Notice that the classes
used to compute the membership functions may be the same as the classes used in
the ﬁnal recognition process. The difference is that in the feature space, one creates
the membership functions point-wise (in time), while in the ﬁnal classiﬁcation, in
the fuzzy dynamics space, we work in matching dynamics (variations in time), not
values. For example, the classiﬁcation can be performed based on the minimal
distance between trajectories of the membership functions,
min
v ∑hd μh, μhν
ð
Þ
Matching. Beyond the matching by minimal distance, methods of matching
more speciﬁc to the nonlinear dynamic ﬁeld are available, for example Lyapunov
exponents and various fractal dimensions computed locally for a small number of
windows approximately corresponding to the average duration of phonemes. These
speciﬁc tools might help improve the overall recognition score and surely increases
the power of speech analysis.
An example of application of the technique reviewed in this section to speech
analysis is provided in (Rodriguez et al., 1997), while applications to the analysis of
various bio-electrical signals are presented in a series of papers by Kosanovic et al.
(cited series of papers, 1994–1996).
Fuzzy Logic in Speech Technology …
601

7
Conclusions and Potential Future Directions
The use of FL and FLSs in speech technology has seen a somewhat modest
development and mixed success, which are difﬁcult to explain else than by the large
amount of computations required by both speech characterization and fuzzy sys-
tems. Also, the early development of good statistical models, as HMMs may have
reduced the appetite to apply FL in speech recognition. Also, the advent of con-
catenative synthesizers and the transition from Klatt-type to concatenative-type
speech synthesis may have hampered the use of FL in synthesis for some time.
Interestingly, several patents have been applied, possibly indicating that FL was
more attractive in speech technology for industry than for academia.
Although the overall results of applying FL to speech technology are today
rather limited, we are conﬁdent that further researches can successfully involve the
use of FL in speech synthesis, including prosody synthesis, speech analysis with
applications to emotion detection and medical conditions, and in speech and
speaker recognition under noisy environment. We expect that the applications of
speech analysis in medicine and psychology are likely to beneﬁt of the use of FL
and neuro-fuzzy systems. However, for achieving more signiﬁcant results in speech
technology, FL needs to be used in its forms merged with second order m.f.s and
fuzzy probabilities.
What is striking for a reviewer who compares the processes of speech recog-
nition in humans, as currently known, and today algorithms for speech recognition
is the low parallelism (similarity) between them. In a review chapter on speech
perception, based on empirical ﬁndings in the medical and psychological literature,
the authors (Gierur & Pisoni 1988, pp. 253–276) say about speech perception
development in humans:
Children not only used but relied upon multiple cues in perception. Acoustic redundancies
were critical to the perception of the phonemic contrast. … Children assigned relative
weightings to the cues. … With development, children shift attention to more isolated,
discrete segmental cues.
None of these features can be assigned today to speech recognition systems, and
neither the development of these systems nor their evolving during training follows
a path similar to speech recognition by humans during their development from
children to adults. Notice that the relative weighting mentioned by Gierur & Pisoni
may have similarities with the information aggregation in fuzzy systems, or at least
may encourage the attempt to use FL to mimic human perception.
It is unclear what path the use of FL in speech technology will take in the context
of the trends in data analytics, Internet of things, and cloud computing. We may
expect that the extra computational effort required by the advanced and massive use
of FL merged with fuzzy probabilities, which may have hampered the extensive use
of FL in speech technology in the past, will be absorbed under these developments
and will produce FL-based instruments.
FL use in speech technology will increase only side-by-side to new commer-
cially-viable applications. While the application of FL in typical communication
602
H.-N. Teodorescu

technology was partly successful, FL has potential to become a major tool in
higher-intelligence applications, where man-machine interactions become ubiqui-
tous. Speech interpretation as a representation of the state of the subjects, including
emotion, intention, and mood, and of subjects’ view on their relationship with the
machine interlocutor [Teodorescu, 2005, Teodorescu, 2001a] might beneﬁt of FL,
and FL development may beneﬁt of researches in this broad ﬁeld. On the more
research-oriented side, we believe that phonetics and language studies, as well as
psychology and medical applications can tremendously proﬁt in the future from the
use of FL.
Acknowledgments The ﬁrst author acknowledges the help of Prof. Corneliu Burileanu,
Dr. Marius Zbancioc and Dr. Monica Feraru in reviewing a preliminary version of the chapter.
References
Amano, A., Aritsuka, T., Hataoka, N., Ichikawa, A.: On the use of neural networks and fuzzy logic
in speech recognition. In: IJCNN, International Joint Conference on Neural Networks, 1989,
pp. 301–305, vol. 1, 18–22 June 1989 Washington, DC, USA. doi:10.1109/IJCNN.1989.
118595
Amano, A., Ichikawa, A., Hataoka, N. (inventors): US 5,040,215 (date of patent Aug. 13, 1991),
Speech recognition apparatus using neural network and fuzzy logic, applied by Hitachi, Ltd
Avci, E., Akpolat, Z.H.: Speech recognition using a wavelet packet adaptive network based fuzzy
inference system. Expert Syst. Appl. 31, 495–503 (2006)
Benesty, J., Sondhi, M.M., Huang, Y. (eds.): Springer Handbook of Speech Processing,
1176 pp. Springer Science & Business Media, Berlin (2008)
Beritelli, F., Casale, S., Cavallaro, A.: A robust voice activity detector for wireless communi-
cations using soft computing. IEEE J. Sel. Areas Commun. 16(9), 1818–1829 (1998)
Beritelli, F., Casale, S., Cavallaro, A.: A multi-channel speech/silence detector based on time delay
estimation and fuzzy classiﬁcation. In: Proceedings of the 1999 IEEE International Conference
on Acoustics, Speech, and Signal Processing, 15–19 March 1999, vol. 1, pp. 93–96 Phoenix,
AZ. doi:10.1109/ICASSP.1999.758070
Beritelli, F., Casale, S., Ruggeri, G., Serrano, S.: Performance evaluation and comparison of g.729/
amr/fuzzy voice activity detectors. IEEE Signal Process. Lett. 9(3), 85–88 (2002)
Brito, J.A.: A fuzzy-genetic approach for the computational modeling of speech articulatory
processes. Saber (Venezuela) 21(3), 269–276 (2009)
Brito, J., Rodriguez, W.: Multipopulation genetic learning of midsagittal articulatory models for
speech synthesis. GrC 2006, pp. 166–169
Burileanu, C., Popescu, V., Buzo, A., Petrea, C.S., Ghelmez-Haneş, D.: Spontaneous speech
recognition for Romanian in spoken dialogue systems. Proc. Romanian Acad. Ser. A 11(1),
83–91 (2010)
Cheng, R.G., Chang, C.J.: Design of a fuzzy trafﬁc controller for ATM networks. IEEE/ACM
Trans. Netw. 4(3), 460–469 (1996)
Cavallaro, A., Beritelli, F., Casale, S.: A fuzzy logic-based speech detection algorithm for
communications in noisy environments. In: Proceedings of the 1998 IEEE International
Conference on Acoustics, Speech and Signal Processing, 1998, vol. 1, pp. 565–568, 12–15
May 1998, Seattle, WA. doi:10.1109/ICASSP.1998.674493
Chatterjee, A., Pulasinghe, K., Watanabe, K., Izumi, K.: A particle-swarm-optimized fuzzy-neural
network for voice-controlled robot systems. IEEE Trans. Ind. Electron. 52(6), 1478–1489
(2005)
Fuzzy Logic in Speech Technology …
603

Cheok, A.D., Chevalier, S., Kaynak, M., Sengupta, K.: Use of a novel generalized fuzzy hidden
Markov model for speech recognition. In: The 10th IEEE International Conference on Fuzzy
Systems, 2001, vol. 3, pp. 1207–1210, 02–05 Dec 2001 Melbourne, Victoria. doi:10.1109/
FUZZ.2001.1008874
Chibelushi, C.C., Mason, J.S., Deravi, R.: Integration of acoustic and visual speech for speaker
recognition. In: EUROSPEECH ‘93 Third European Conference on Speech Communication
and Technology, Berlin, Germany, 22–25 Sept 1993, pp. 157–160 (1993). http://www.isca-
speech.org/archive/eurospeech_1993/e93_0157.html
Ciota, Z.: Improvement of speech processing using fuzzy logic approach. In: Joint 9th IFSA World
Congress and 20th NAFIPS International Conference, 2001, vol. 2, 25–28 July 2001,
pp. 727–731, vol. 2, 25–28 July 2001, Vancouver, BC
Coorman, G et al.: US Patent no. 7219060, Speech synthesis using concatenation of speech
waveforms. 15 May 2007
Cowie, R., Douglas-Cowie, E., Tsapatsoulis, N., Votsis, G., Kollias, S., Fellenz, W., Taylor, J.G.:
Emotion recognition in human-computer interaction. IEEE Signal Process. Mag. 18(1), 32–80
(2001)
Cucu, H., Buzo, A., Besacier, L., Burileanu, C.: SMT-based ASR domain adaptation methods for
under-resourced languages: application to Romanian. Speech Commun. 56, 195–212 (2014)
Dutoit, T.: An Introduction to Text-to-Speech Synthesis. Kluwer Academic Publications,
Dordrecht (1997)
de Gelder, B., Vroomen, J.: The perception of emotions by ear and by eye. Cogn. Emot. 14(3),
289–311 (2000)
De Mori, R.: Use of fuzzy algorithms for phonetic and phonemic labeling of continuous speech.
IEEE Trans. Pattern Anal. Mach. Intell. PAMI-2(2), 136–148 (1980)
Erickson, D.: Expressive speech: Production, perception and application to speech synthesis.
Tutorial. Acoust. Sci. Technol 26(4), 317–325 (2005)
Fant, G., Liljencrants, J., Lin, Q.: A four parameter model of glottal ﬂow (Research Report
STL-QPSR 4, KTH), pp. 1–13. Stockholm, Royal Institute of Technology (1985)
Fant, G., Lin, Q.: Frequency domain interpretation and derivation of glottal glow parameter
(Research Report STL-QPSR 2-3, KTH), pp. 1–21. Stockholm, Royal Institute of Technology
(1988)
Feraru, S.M., Teodorescu, H.N., Zbancioc, M.D.: SRoL—Web-based resources for languages and
language technology e-learning. Int. J. Comput. Commun. Control 5(3), 301–313 (2010)
Gierur, J.A., Pisoni, D.B.: Speech perception, Chapter 9 in Handbook of Speech-Language
Pathology and Audiology, pp. 253–276. Decker Inc., New York (1988)
Gharavian, D., Sheikhan, M., Nazerieh, A., Garoucy, S.: Speech emotion recognition using FCBF
feature selection method and GA-optimized fuzzy ARTMAP neural network. Neural Comput.
Appl. 21(8), 2115–2126 (2012)
Grigoras, F., Teodorescu, H.N., Jain, L.C., Apopei, V., (1999). Fuzzy and knowledge-based
control for speech synthesis. In: ECC’99 CD-ROM Proceedings. Karlsruhe, Germany, VDI/
VDE Geselschaft (1999)
Grigoras, F., Apopei, V., Jitca, D., Teodorescu, H.N.: Conclusions from a research on soft-
computing rule-based speech synthesis for Romanian language. In: ECIT2000 CD-ROM
Proceedings. Iasi, Romania, Coda Press. (2000)
Grimm, M., Kroschel, K., Narayanan, S.: Support vector regression for automatic recognition of
spontaneous emotions in speech. In: ICASSP 2007, IEEE International Conference on
Acoustics, Speech and Signal Processing, 2007, vol. 4, 15–20 April 2007, pp. IV-1085–1088
ISSN 1520-6149, Honolulu, HI. doi:10.1109/ICASSP.2007.367262
Grimm, M., Kroschel, K., Mower, E., Narayanan, S., Primitives-based evaluation and estimation
of emotions in speech, Speech Commun. 49(10–11), 787–800 (2007). doi:10.1016/j.specom.
2007.01.010
Halavati, R., Shouraki, S.B., Zadeh, S.H.: Recognition of human speech phonemes using a novel
fuzzy approach. Appl. Soft Comput. 7(3), 828–839 (2007)
604
H.-N. Teodorescu

Ioannou, S.V., et al.: Emotion recognition through facial expression analysis based on a
neurofuzzy network. Neural Netw. 18(4), 423–435 (2005)
Iles, J., Ing-Simmons, N.: Rsynth 2.0, Text-to-Speech software. ftp://svr-ftp.eng.cam.ac.uk comp.
speech/sources (1994)
ITU-T G.729.1, G.729-based embedded variable bit-rate coder: An 8-32 kbit/s scalable wideband
coder bitstream interoperable with G.729. Telecommunication Standardization Sector of ITU,
Series G: Transmission Systems and Media, Digital Systems and Networks (05/2006)
Jitca, D., Teodorescu, H.N., Apopei, V., Grigoras, F.: Improved speech synthesis using fuzzy
methods. Int. J. Speech Technol. 5(3), 227–235 (2002) (Kluwer Academic Publishers)
Juang, C.-F., Cheng, C.-N., Chen, T.M.: Speech detection in noisy environments by wavelet
energy-based recurrent neural fuzzy network. Expert Syst. Appl. 36(1), 321–332 (2009)
Kandel, A., Teodorescu, H.-N., Arotaritei, D.: Analytic fuzzy RBF neural network. In: 1998
Conference
of
the
North
American
Fuzzy
Information
Processing
Society-NAFIPS,
pp. 281–285, IEEE
Kasabov, N., Iliev, G., A methodology and a system for adaptive recognition in a noisy
environment based on adaptive noise cancellation and evolving fuzzy neural networks.
Preliminary Patent, University of Otago, 21 Dec 1999, New Zealand
Kasabov, N., Iliev, G.: Hybrid system for robust recognition of noisy speech based on evolving
fuzzy neural networks and adaptive ﬁltering. In: Proceedings of the IEEE-INNS-ENNS
International Joint Conference on Neural Networks, 2000 (IJCNN 2000), vol. 5, 2000,
pp. 91–96, vol. 5, 24–27 July 2000, Como, Italy. doi:10.1109/IJCNN.2000.861440
Klatt, D.: Software for cascade/parallel formant synthesizer. J. Acoust. Soc. Am. 67, 971–995
(1980)
Klatt, D.: Review of text-to-speech conversion for English. J. Acoust. Soc. Am. 82, 737–793
(1987)
Köhler, B.-U., Hennig, C., Orglmeister, R.: QRS detection using zero crossing counts. Prog.
Biomed. Res. 8(3), 138–145 (2003)
Koo, J.M., Un, C.K.: Fuzzy smoothing of HMM parameters in speech recognition. Electron. Lett.
26(11), 743–744 (1990). doi:10.1049/el:19900485
Kosanovic, B.R., Chaparro, L.F., Sclabassi, R.J.: Signal analysis in fuzzy information space.
Fuzzy Sets Syst. 77, 49–62 (1996)
Kosanovic, B.R., Chaparro, L.F., Sclabassi, R.J.: Modeling of quasi-stationary signals using
temporal fuzzy sets and time-frequency distributions. In: Proceedings of the IEEE-SP
International Symposium on Time-Frequency and Time-Scale Analysis, Philadelphia, PA,
pp. 425–428, IEEE Press, New York (1994)
Kosanovic, B.R., Chaparro, L.F., Sclabassi, R.J.: Hidden process modeling. In: Proceedings
ICASSP-95, vol. 5, pp. 2935–2938, Detroit, MI, 8–12 May 1995, IEEE
Kosanovic, B.R., Chaparro, L.F., Sun, M., Sclabassi, R.J.: Physical system modeling using
temporal fuzzy sets. In: Proceedings of the International Joint Conference of NAFIPS/IFIS/
NASA ‘94, San Antonio, TX, pp. 429–433. IEEE Press, New York (1994)
Lee, C.M., Narayanan, S.S.: Toward detecting emotions in spoken dialogs. IEEE Trans. Speech
Audio Process. 13(2), 293–303 (2005)
Lee, C.M., Narayanan, S.: Emotion recognition using a data-driven fuzzy inference system. In:
The Proceedings of EUROSPEECH, Geneva, 2003, pp. 157–160 (Proceeding European Conf.
Speech Communication and Technology, 2003, EUROSPEECH 2003—INTERSPEECH
2003, 8th European Conference on Speech Communication and Technology), Geneva,
Switzerland, 1–4 Sept 2003
Lin, C.-T., Wu, R.-C., Chang, J.-Y., Liang, S.-F.: A novel prosodic-information synthesizer based
on recurrent fuzzy neural network for the chinese TTS system. IEEE Trans. Syst. Man Cybern.
—Part B Cybern. 34(1), 309–324 (2004)
Massaro, D.W., Cohen, M.M.: Fuzzy logical model of bimodal emotion perception: Comment on
“The perception of emotions by ear and by eye’’ by de Gelder and Vroomen. Cogn. Emot. 14
(3), 313–320 (2000)
Fuzzy Logic in Speech Technology …
605

Melin, P., Urias, J., Solano, D., Soto, M., Lopez, M., Castillo, O.: Voice recognition with neural
networks, type-2 fuzzy logic and genetic algorithms. Eng. Lett. 13, 2 EL_13_2_9 (Advance
online publication: 4 Aug 2006)
Melin, P., Castillo, O.: Voice recognition with neural networks, fuzzy logic and genetic
algorithms. In: Hybrid Intelligent Systems for Pattern Recognition Using Soft Computing.
Studies in Fuzziness and Soft Computing, vol. 172, pp. 223–240, 26 Feb 2005
Mills, P., Bowles, J.: Fuzzy logic enhanced symmetric dynamic programming for speech
recognition. In: Proc. Fifth IEEE International Conference on Fuzzy Systems, 1996, vol. 3,
8–11 Sept 1996, pp. 2013–2019 vol. 3, 08–11 Sept 1996, New Orleans, LA. doi:10.1109/
FUZZY.1996.552747
Ndousse, T.D.: Fuzzy neural control of voice cells in ATM networks. IEEE J. Sel. Areas Commun.
12(9), 1488–1494 (1994)
Naphade, M., Smith, J.R., Tesic, J., Shih-Fu Chang, Hsu, W., Kennedy, L., Hauptmann, A., Curtis,
J.: Large-scale concept ontology for multimedia. IEEE MultiMedia 13(3), 86–91 (2006)
Ndousse, T.D.: Fuzzy expert systems in a TM networks, pp. 229–284. In: Jain, L.C., Martin, N.M.
(eds.) Fusion of Neural Networks, Fuzzy Systems and Genetic Algorithms: Industrial
Applications. CRC Press, Boca Raton, USA (1998)
Oden, G.C., Massaro, D.W.: Integration of featural information in speech perception. Psychol.
Rev. 85(3), 172–191 (1978)
Pal, S.K., Majumder, D.D.: Fuzzy sets and decisionmaking approaches in vowel and speaker
recognition. IEEE Trans. Syst. Man Cybern. 7, 625–629 (1977)
Pal, S.K., Mitra, S.: Multilayer perceptron, fuzzy sets, and classiﬁcation. IEEE Trans. Neural
Netw. 3(5), 683–697 (1992)
Petrantonakis, P.C., Hadjileontiadis, L.J.: Emotion recognition from EEG using higher order
crossings. IEEE Trans. Inf. Technol. Biomed. 14(2), 186–197 (2010)
Ramirez, J., Segura, J.C., Benitez, C., de la Torre, A., Rubio, A.: Efﬁcient voice activity detection
algorithms using long-term speech information. Speech Commun. 42, 271–287 (2004)
Raptis, S., Carayannis, G.: Fuzzy logic for rule-based formant speech synthesis. In: EURO-
SPEECH‘97 Proceedings, Rhodes, Greece, vol. 3, pp. 1599–1602 (1997)
Rodriguez, W., Teodorescu, H.N., Grigoras, F., Kandel, A., Bunke, H.: A fuzzy information space
approach to speech signal non-linear analysis. Int. J. Intell. Syst. 15(4), 343–363 (2000)
Rodriguez, W., Kandel, A., Bunke, H.: 3D-curve similarity using fuzzy string matching. In:
Proceedings of the Sixth IEEE Int. Conference on Fuzzy Systems, 1997, vol. 1, pp. 79–82, 1–5
July 1997
Rodriguez, W., Last, M., Kandel, A., Bunke, H.: 3-Dimensional curve similarity using string
matching. Robot. Auton. Syst. 49(3–4, 31), 165–172 (2004)
Scherer, S., Kane, J., Gobl, C., Schwenker, F.: Investigating fuzzy-input fuzzy-output support
vector machines for robust voice quality classiﬁcation. Comput. Speech Lang. 27(1), 263–287
(2013) (Special issue on Paralinguistics in Naturalistic Speech and Language)
Shikano, K., Nakamura, S., Abe, M.: Speaker adaptation and voice conversion by codebook
mapping, In: IEEE International Symposium on Circuits and Systems, 1991, vol. 1,
pp. 594–597, 11–14 June 1991
Stylios, C.D., Georgopoulos, V.C., Malandraki, G.A., Chouliara, S.: Fuzzy cognitive map
architectures for medical decision support systems. Appl. Soft Comput. 8, 1243–1251 (2008)
Su, M.-C., Hsieh, C.-T., Chin, C.C.: A neuro-fuzzy approach to speech recognition without time
alignment. Fuzzy Sets Syst. 98(1), 33–41 (1998)
Szekely, E., Kane, J., Scherer, S., Gobl, C., Carson-Berndsen, J.: Detecting a targeted voice style
in an audiobook using voice quality features. ICASSP 2012, 4593–4596 (2012)
Tanaka, K., Masanobu, A.: United States Patent 6,081,781, Method and apparatus for speech
synthesis and program recorded medium. 27 June 2000
Temko, A., Macho, D., Nadeu, C.: Fuzzy integral based information fusion for classiﬁcation of
highly confusable non-speech sounds. Pattern Recognit. 41, 814–1823 (2008)
606
H.-N. Teodorescu

Teodorescu, H.N., Chelaru, M., Sofron, E., Adascalitei, A.: Adaptive speech synthesis. ITG-
Fachbericht 105, Digitale Sprach-verarbeitung—Prinzipien und Anwendungen. VDE-Verlag
GmBh, Berlin, pp. 183–188 (1988)
Teodorescu, H.N., Yamakawa, T.: Applications of chaotic systems: an emerging ﬁeld. Int. J. Intell.
Syst. 12(4), 251–253 (1997)
Teodorescu, H.N., Kandel, A., Schneider, M.: Fuzzy modeling and dynamics. Fuzzy Sets Syst.
106(1), 1–2 (1999)
Teodorescu, H.N.L., Kandel, A., Hall, L.O.: Report of research activities in fuzzy AI and medicine
at USFCSE. Artif. Intell. Med. 21(1–3), 177–183 (2001)
Teodorescu, H.-N.L.: Interrelationships, communication, semiotics, and artiﬁcial consciousness.
In: Kitamura, T. (ed.) What Should Be Computed to Understand and Model Brain Function?
FLSI Book Series, vol. 3, pp. 115–147. World Scientiﬁc, Singapore. ISBN 981-02-4518-1
(2001)
Teodorescu, H.N., Stoica, A, Mlynek, D., et al.: Nonlinear dynamics sensitivity analysis in
networks and applications to sensing. In: Filip, F.G., Dumitrache, I., Iliescu, S. (eds.) Large
Scale Systems: Theory and Applications 2001 (LSS’01), IFAC Symposia Series, pp. 333–338,
2002. Proceedings of the 9th IFAC Symposium on Large Scale Systems, Bucharest, Romania,
18–20 July 2001
Teodorescu, H.N.: A proposed theory in prosody generation and perception: the multi-dimensional
contextual integration principle of prosody. In: Burileanu, C. (ed.) Proc Sped 2005,
pp. 109–118. Romanian Academy Publ, Bucharest (2005)
Teodorescu, H.-N., Feraru, S.M.: A study on speech with manifest emotions. In: Matousek, V;
Mautner, P. (eds.) Text, Speech and Dialogue, Proceedings. Lecture Notes In Artiﬁcial
Intelligence, vol. 4629, pp. 254–261 (2007)
Teodorescu, H.-N., Feraru, M.: Analyzing emotions in spoken Romanian. Proc. Romanian Acad.
Ser. A-Math. Phys. Tech. Sci. Inf. Sci. 8(2), 161–168 (2007)
Teodorescu, H.N., Feraru, M.: Classiﬁcation in Gnathophonics—Preliminary Results. In:
Proceedings of the Second International Symposium on Electrical and Electronics Engineer-
ing—ISEEE-2008, Galati, Romania, 6 pp (2008)
Teodorescu, H.-N., Feraru, M., Zbancioc, M.: Assessing the quality of voice synthesizers. In:
Proceedings of the 5-th Conference on Speech Technology and Human-Computer Dialogue,
2009 SpeD’09, pp. 1–10 (2009)
Teodorescu, H.-N.: Characterization of nonlinear dynamic systems for engineering purposes—a
partial review. Int. J. Gen. Syst. 41(8), 805–825 (2012)
Tian, Y., Wu, J., Wang, Z., Lu, D.: Fuzzy clustering and Bayesian information criterion
based threshold estimation for robust voice activity detection. In: 2003 IEEE International
Conference on, Acoustics, Speech, and Signal Processing, Proceedings. (ICASSP ‘03), vol. 1,
6–10, pp. I-444–447 (2003)
Toledano, D.T., Rodríguez Crespo, M.A., Escalada Sardina J.G.: Trying to mimic human
segmentation of speech using HMM and fuzzy logic post-correction rules. In: The Third
ESCA/COCOSDA Workshop (ETRW) on Speech Synthesis. Blue Mountains, NSW Australia,
26–29 Nov 1998 SSW3-1998, pp. 207–212. http://www.isca-speech.org/archive_open/
archive_papers/ssw3/ssw3_207.pdf
Van Segbroeck, M., Van Hamme, H.: Robust speech recognition using missing data techniques in
the prospect domain and fuzzy masks. In: IEEE International Conference on Acoustics, Speech
and Signal Processing, 2008. ICASSP 2008, 4 April 2008, pp. 4393–4396, Las Vegas, NV.
doi:10.1109/ICASSP.2008.4518629
Wahyudi, W.A., Syazilawati, M.: Intelligent voice-based door access control system using
adaptive-network-based fuzzy inference systems (ANFIS) for building security. J. Comput.
Sci. 3(5), 274–280 (2007)
Wang, X., Lou, X., Li, J., Li, J. (Inventors): Method, apparatus for synthesizing speech and
acoustic model training method for speech synthesis. Patent application number 20120221339,
2012-08-30
Fuzzy Logic in Speech Technology …
607

Zbancioc, M., Feraru, M.: The analysis of the FCM and WKNN algorithms performance for the
emotional Corpus SROL, Adv. Electr. Comput. Eng. 12(3), 33–38 (2012). doi:10.4316/AECE.
2012.03005, http://www.aece.ro/abstractplus.php?year=2012&number=3&article=5
Zbancioc, M., Feraru, M.: Emotion recognition of the SROL Romanian database using fuzzy KNN
algorithm. In: International Symposium on Electronics and Telecommunications IEEE—
ISETC 2012 Tenth Edition, Timisoara, pp. 347–350, 15–16 Nov 2012. ISBN: 978-1-4673-
1177-9, http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6408133 (2012)
Zhao, H., Wang, G, Xu, C., Yu, F.: Voice activity detection method based on multivalued coarse-
graining Lempel-Ziv complexity. In: Comput. Sci. Inf. Syst. 8(3), 869–888 (2011) (US Patent
7,999,857, Aug. 16, 2011, F.E. Bonn, R.D. Adair, R.N. Peterson, D.D. Adair, Voice,
Lip-Reading, Face and Emotion Stress Analysis, Fuzzy Logic Intelligent Camera System)
Author Biography
Horia-Nicolai L. Teodorescu Ph.D., dr. h.c., senior member,
IEEE, is a corresponding member of the Romanian Academy
and director of the Institute of Computer Science of the
Romanian Academy – Iasi Branch. He is also a professor at
‘Gheorghe Asachi’ Technical University of Iasi and an associate
professor at ‘Al.I. Cuza’ University of Iasi, Romania, where he
teaches a class of Speech Technology at the post-graduate level.
He authored or co-authored about 300 papers and is an inventor
to more than 25 national and international patents. He co-edited
more than 10 volumes with major publishers (Springer, CRC-
Taylor & Francis, Kluwer, IEEE), and authored or co-authored
more than 10 books.
608
H.-N. Teodorescu

Fuzzy Sets: Towards the Scientiﬁc
Domestication of Imprecision
Enric Trillas
Abstract This paper considers fuzzy sets just as science tries to domesticate con-
cepts once abstracted from reality: identifying them with quantities, using these
quantities for building up mathematical models, escaping from just a formal logic
setting, and testing the models against reality before provisionally accepting them.
Its aim is that of trying to go towards a new experimental science of ‘the impre-
cise’; to something like a ‘physics of linguistic imprecision’. It contains a way for
looking at fuzzy sets that, if continued, could oﬀer a new perspective for seeing lin-
guistic imprecision, and whose possible value lies on the idea that several forms of
theorizing always can be better than a single one.
1 Introduction
From its inception by LotﬁA. Zadeh ﬁfty years ago [1], fuzzy sets are linked with the
management of imprecision in real, technological, problems. Imprecision permeates
natural language and common reasoning, and hence also the linguistic description of
the behavior of those systems that either cannot be described by equations, or such
a description is unknown. Imprecision, necessary for transmitting ideas in not too
long pieces of language, is an economic characteristic of natural language. Zadeh’s
paper was, after those by Max Black in 1937 and 1963 [27, 28], a true ﬁrst attempt
towards a domestication of linguistic imprecision, and his work from 1965 onwards
constitutes a relevant ‘corpus’ of doctrine for it.
In the Introduction at his 1965 seminal paper [1], and to separate from the very
beginning fuzzy sets from both sets and probability, Zadeh stated:
To professor LotﬁA. Zadeh, with a professional high admiration to his impressive work, and a
deep personal esteem for him.
E. Trillas (✉)
European Centre for Soft Computing, Mieres (Asturias), Spain
e-mail: enric.trillas@softcomputing.es
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_29
609

610
E. Trillas
“…such a framework [that of fuzzy sets] provides a natural way of dealing with
problems in which the source of imprecision is the absence of sharply deﬁned criteria
of class membership rather than the presence of random variables”.
Of course, this ‘class’ approach neither means that there is a lack of relationships
between sets and fuzzy sets, nor that the values of the membership function of a fuzzy
set can never come from some probabilistic computation, but that the genesis of
fuzzy sets neither lies in perfect classiﬁcation, nor in randomness. The genesis of
fuzzy sets is in language, where predicates (the words naming properties) usually
designate classes or collectives [2] as, for instance, the predicate young (Y) desig-
nates the class or collective of the ‘young Londoners’ ([Y]) when applied to London
inhabitants. The idea of a collective is well rooted in language.
By one side collectives should be mathematically represented to, for instance,
translating statements into formulas, and by the other side most of them should not
be confused with sets as it can be shown by using the Sorites’ methodology [3].
Additionally, it should be remarked that fuzzy sets are not collectives, but only a
partial representation of them depending on the directly available information on
the use of the predicate and, perhaps, on some reasonable hypotheses on such use.
Were, as it is usually done, a fuzzy set just seen as a function X →[0, 1] without
referring to a predicate or linguistic label, the function says nothing on any linguistic
subject. Fuzzy sets do translate into functions the current meaning of predicates on
X, and this paper goal is just to explain what a fuzzy set is, and how fuzzy sets allow
domesticating imprecision as it is usually done in science. Most of the applications of
Zadeh’s fuzzy sets do concern problems involving dynamical systems whose behav-
ior is described by means of linguistic imprecise rules, provided by an expert of the
type ‘If x is P and y is Q, then z is R’.
2 Measurable Predicates
Let X be a universe of discourse whose elements enjoy some property (p) named by
P. What follows will proceed under the following three naïve working hypotheses of
‘perceptive recognition’:
∙First: For all x in X it can be recognized if “x is P” or x veriﬁes p, can be, or cannot
be, stated.
∙Second: For some pairs of elements in X it can be recognized if one of them enjoys
p less or equal than the other.
∙Third: It can be recognized that, in the language, P originates in X a ‘collective’,
called that of the Ps.
If P veriﬁes the three hypotheses or conditions, it will be called “measurable” [4]
in X. Although it does not mean that they are for nothing, non-measurable predicates
will not be considered here. The ﬁrst question to answer is what it means to actually
measure a measurable predicate.

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
611
Let’s consider the empirically perceived binary relation ≤P deﬁned in X [4] by,
x ≤P y ⇔x shows P less or equal than y shows P,
provided that, at least. ﬁrst and second hypotheses hold. This relation reﬂects that
the elemental statement ‘x is P’ carries less or equal information than ‘y is P’ for
what concerns property p. Hence, the use of a measurable P in X endows it with
the simple mathematical structure given by the graph (X, ≤P), and it will be said
that such graph translates the primary or qualitative meaning of P in X [4]. In this
graph there can be maximal and minimal elements, that is, elements z ∈X such that
there are no elements x ∈X verifying, respectively, z ≤P x, or x ≤P z. Maximals can
be recognized by the lack of others showing more p, and minimals by the lack of
others showing less p. Provided there is only a maximal or a minimal, they are said
the maximum and the minimum, respectively. If z is maximal is when, classically,
is said that z is P is true, and if z is minimal that z is P is false; in the setting of
this paper the classical concepts ‘true’ and ‘false’ can be avoided, they are indeed
superﬂuous.
Notice that X does not need to be previously structured, and that the graph reﬂects
the intuitive idea that when speaking on something, one tries to introduce ‘some
order’ in the universe on which she/he is speaking of. This seems to be a necessary
ﬁrst step for reasoning. Notice that X represents a universe whatsoever; properties
can be observed in any kind of universe, and not only in those endowed with some
mathematical structure as it is with the predicate ‘prime’ on the set of natural num-
bers, and it is not with ‘hot’ in a set of cooper samples.
Notice also that the relation ≤P is not, in general, a total or linear one, that is,
there are often elements x and y not-comparable under ≤P, that is, neither verifying
x ≤P y, nor y ≤P x. If, for instance, the predicate white were applied to snow, namely
to a collection of snow’s samples, it will be measurable only provided it can be always
recognized that ‘this sample of snow is less white than that sample of snow’, sup-
posed the collection of samples is actually known. It is easy to imagine that for some
pair of samples it can be very diﬃcult to state if one of them is actually less white
than the other; if the collection of samples is large, almost surely the relation ≤white
will not be linear. Without well enough knowing the elements in the universe of dis-
course, it is very diﬃcult, if not impossible, to consider if a given predicate is or is
not measurable; previous information on X is basic for capturing a qualitative use of
P in X.
If the relation ≤P cannot be stated, the predicate will be said to be metaphysical
in X [4], and without any contempt against them, the consideration of metaphysical
predicates is not a goal of this paper. The kingdom of metaphysical predicates is
philosophy; science and technology are kingdoms of the measurable. It is not to be
forgotten the dictum induced from a 1883 lecture by Lord Kelvin [23], “if you cannot
measure it, it is not science”.
With all that, a measure of the extent up to which each x in X is P, veriﬁes p, or
carries the information provided by P, is a mapping [5] 𝜇P: X →[0, 1], such that:

612
E. Trillas
(1) If x ≤P y, then 𝜇P (x) ≤𝜇P (y), in the order of the real line,
(2) If z is a maximal in (X, ≤P), then 𝜇P (z) = 1,
(3) If z is a minimal in (X, ≤P), then 𝜇P (z) = 0.
Hence, a measure is but a morphism between the graph (X, ≤P) and the totally
ordered unit interval ([0, 1], ≤); it translates a ‘scarcely ordered’ graph into a linear
continuum, and it should be noticed that, in general, the three properties of a measure
are not suﬃcient to specify a single one. This is like what happens with the deﬁnition
of a measure of probability: it does not specify a single one, but more information on
the case is needed for such speciﬁcation. Notice that neither 𝜇P(x) = 1, nor 𝜇P(x) = 0
mean that x is, respectively, a maximal or a minimal; anyway, maximals are in the
set 𝜇−1
P (1), and minimals in 𝜇−1
P (0).
For instance, provided it could be established that 𝜇white is a measure for white,
to know a value 𝜇white(this snow’s sample), either more information, or a reasonable
hypothesis on the comparison of samples, will be needed. Of course, like for assign-
ing a probability to the event ‘six’ in the throw of a dice, it is necessary to know in
which surface the dice will land and if it is tricky, in the case of ‘snow’ some exper-
imentation, even imaginary or by doing a toy-experiment, is necessary for assigning
values to 𝜇white.
Example 1 If X = [0, 1000] and P = small, since it can be taken ≤small coincident
with the inverse ≥of the total order ≤of the real line (x is less small than y ⇔x ≥y),
with minimum 1000 and maximum 0, the measures of S = small are the mappings
𝜇S:[0, 1000] →[0, 1],
such that:
(1) 𝜇S is decreasing; (2) 𝜇S (0) = 1; 3) 𝜇S (1000) = 0,
of which there are many. For instance, provided it were known that the measure’s
variation should be linear, the only possible measure is 𝜇S(x) = 1 −x/1000, but if
it were known that it should be quadratic, then among the quadratic ones it can be
chosen 𝜇S (x) = (1 −x/1000)2. Provided it were known that the measure is piecewise
linear, then one of them can be chosen by selecting a number, say 250, such that 𝜇S(x)
= 1 if x ∈[0, 250], and 𝜇S (x) = (1000−x)/750 if x ∈(250, 1000]. Etc.
Hence, additional information on the variation of the measure should be known, or
supposed, to specify one of them. In any case, the measure will be suitable depending
on the correctness of such knowledge and/or hypotheses. Not only a good enough
knowledge of X, but also of the context on which P is applied to X, should be known
to capture the ‘current meaning’ of P in X.
Notice that if X is an interval [0, M] of the real line, and ≤P can be identiﬁed with
either ≤, or ≥, there is just a single linear measure 𝜇P (x) = ax + b since, in the ﬁrst
case it is 𝜇P (0) = 0 and 𝜇P (M) = 1, with which b = 0, a = 1/M, and 𝜇P (x) = x/M,

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
613
and in the second it is 𝜇P (0) = 1, 𝜇P (M) = 0, with which b = 1, a = -1/M, and 𝜇P
(x) = 1 - x/M. This is the former case in which M = 1000.
Once a measure 𝜇P is speciﬁed, the quantity (X, ≤P, 𝜇P) will be called a meaning
of P in X. It represents what is known on both the qualitative use of P in X, and the
context and, perhaps, also the purpose with which it is used. At the end, the concept
of meaning is here interpreted following the Wittgenstein’s idea that “he meaning of
a word is its use in language” [31]. Diﬀerent uses of the same P, given by diﬀerent
relations ≤P, or diﬀerent measures 𝜇P, generate diﬀerent meanings.
Once a measure is given, the new relation deﬁned by,
x ≤𝜇p y ⇔𝜇P(x) ≤𝜇P(y),
is not only reﬂexive, anti-symmetric and transitive (a partial order), but a total
one since it is always either 𝜇P(x) ≤𝜇P(y), or 𝜇P(x)≥𝜇P(y). This new relation ver-
iﬁes [4],
≤P ⊆≤𝜇p,
since: x ≤P y ⇒𝜇P (x) ≤𝜇P (y) ⇔x ≤𝜇p y. That is, the measure extends the
primary meaning; after actually measuring P its meaning grows in the diﬀerence-
set ≤𝜇p −≤P that, provided is non-empty, adds the information supplied by the
measure to the primary or qualitative meaning. In general, it can be said that the act
of measuring changes the primary meaning of the predicate.
As ≤P is not always a total, or linear, relation, both relations cannot coincide in all
cases, and when ≤P is total and coincides with either ≤𝜇p, or ≤−1
𝜇p , it is said that the
measure perfectly reﬂects [5] the primary meaning of P in X. This is, for instance,
the case of the former example in which ≤small = ≥= ≤−1
𝜇s . It is also the case with
the predicate big for which ≤big = ≤allows to specify a measure, for instance the
only linear one 𝜇big (x) = x/1000, and then ≤big = ≤𝜇big.
Remarks
(a) Provided ≤P is a preorder, that is, a reﬂexive and transitive relation, the relation
‘equally P than’, deﬁned by:
=P = ≤P ∩≤−1
P ,
under which it is [5],
x =P y ⇔x ≤P y & y ≤P x,
is a relation of equivalence. This equivalence gives the quotient-set X/ = P,
whose elements are the classes
[x] = {z ∈X; x =P z}

614
E. Trillas
consisting of those elements that are equally P than its representative. Provided
there is a measure 𝜇P, it is constant in each equivalence class, since:
z ∈[x] ⇒x ≤P z & z ≤P x ⇒𝜇P(x) ≤𝜇P(z) & 𝜇P(z) ≤𝜇P(x) ⇒𝜇P(z)
= 𝜇P(x).
Consequently, if the primary meaning ≤P is reﬂexive and transitive, once the
quotient set is partially ordered by
[x] ≤∗
P [y] ⇔x ≤P y,
a deﬁnition not dependent on the representatives of the classes, it can be deﬁned
a measure in X/ = P by
𝜇∗
P([x]) = 𝜇P(x).
All that gives the quantity (X / = P, ≤∗
P, 𝜇∗
P) that also can serve to deﬁne a mean-
ing of P in X. Obviously, the number of classes equals that of the diﬀerent values
the measure takes. For instance, in the case of the predicate big in [0, 1000], it
is x =big y ⇔x = y, and [x] = {x}, the classes are reduced to singletons. In
the case of predicate prime in the set of positive integers, there is just one class
consisting of all the prime numbers.
(b) In language predicates appear when they are applied to a ﬁrst universe of dis-
course, possibly after some previous trials consisting in observing and recogniz-
ing some property of its elements. For instance, large was perhaps used by the
ﬁrst time applied to stones, trees, mountains, lakes, etc., and further on passed
to other objects like rivers, roads, buildings, etc., and ﬁnally to numbers. When
applying an old predicate to new elements, the ‘history’ of its former uses is a
help for the new application.
In the same vein, a word P designating a property can be understood in diﬀer-
ent ways by a group of people. For instance if, in a measurable case, n persons
manage P in X each one with a meaning (X, ≤k
P, 𝜇k
P), 1≤k ≤n, and the inter-
section ≤P = ≤1
P ∩≤2
P … ∩≤n
P is not empty [5], the group can agree on the
common primary meaning ≤P, and the measure 𝜇P = min (𝜇1
P, 𝜇2
P , … , 𝜇n
P),
as the accepted common meaning of P in X. Provided the intersection of the n
relations ≤k
P were empty, or there were no agreement on it as a common use,
the group can accept either a partial non-empty intersection, or one of the indi-
vidual meanings. In other case, no common understanding of P in X is possible.
Possibly, it is the need of communication between humans that motivated and
motivates the acceptance of common meanings like those appearing in the dic-
tionaries. It also seems to be the starting point for negotiating.
(c) Since, in the praxis, a predicate P is usually managed in X by a measure once it
is speciﬁed, only the relation ≤𝜇p is known by the practitioner. For this reason,
≤𝜇p will be called the ‘working meaning’ of P in X. Notice also that usually is
not easy to know the full relation ≤P when, for instance, there is a big amount
of links between the elements in X; for this reason, sometimes 𝜇P is build up

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
615
only with the help of context information, and it is not well known if it veriﬁes
the properties of a measure. In this frequent praxis’ situation, the function 𝜇P
should be viewed as an approximation to a measure with an unknown error.
3 Fuzzy Sets
3.1
Once a quantity (X, ≤P, 𝜇P) reﬂects what is known on the use, or meaning, of P in
X, the following new terminology can be introduced [4]:
Fuzzy sets P, Q, etc., are entities mathematically created under the following rules:
1. P = Q ⇔(X, ≤P, 𝜇P) = (X, ≤Q, 𝜇Q)
2. P ⊆Q ⇔𝜇P(x) ≤𝜇Q(x), for all x in X
3. x ∈r P ⇔r = 𝜇P(x), read ‘x belongs to P with degree of membership r ∈[0, 1]’.
4. The predicate P is called the linguistic label of the fuzzy set P, and the measure
𝜇P its membership function. For short, P is called ‘the fuzzy set P’.
Example 2 If ([0, 1000], ≤, id/1000) is the quantity representing the use of the
predicate big in [0, 1000], it speciﬁes the fuzzy set B1, with membership function
𝜇1
big(x) = x/1000, with which it is 500 ∈0.5 B1, 250 ∈0.25 B1, 750 ∈0.75 B1, 0 ∈0 B1,
1000 ∈1 B1, etc.
The same predicate big can also be represented by the diﬀerent quantity ([0,
1000], ≤, (id/1000)2), specifying the fuzzy set B2, with membership function 𝜇2
big
(x) = x2/106, with which 500 ∈0.25 B2, 750 ∈0.5625 B2, etc., and that from x2/106 ≤
x/10 shows B2 ⊆B1 and B2 ≠B1.
This example shows that the collective [P], linguistically generated in X by a
predicate P [4], cannot be confused with those fuzzy sets specifying the predicate
from the information available on its current use. In this sense, the fuzzy sets P can be
seen as states of the collective [P] generated by the predicate; the collective consists
in all the states shown by the corresponding fuzzy sets. The former predicate big
in [0, 1000], for instance, generates a linguistic collective [big] consisting in states
like B1, B2, etc. The collective [big] can be seen as deﬁned by all the many actual
quantities deﬁned by the properties:
(1) If x ≤y, then 𝜇big (x) ≤𝜇big (y);
(2) 𝜇big (0) = 0;
(3) 𝜇big (1000) = 1,
once accepted that ≤big is coincidental with the linear order ≤of the real line.
That is, by all the functions 𝜇big: [0, 1000] →[0, 1] verifying (1) and under the
border conditions (2) and (3). The collective appears as the set of all solutions of a
constrained functional inequality, that is, of all measures of a measurable predicate,

616
E. Trillas
once accepted that ≤P describes its qualitative use. Each measure is speciﬁed by tak-
ing into account some contextual information on the particular use of the predicate;
meaning is not unique but context-dependent.
In language, measurable predicates are usually imprecise, or ﬂexible, that is,
accepting measures taking some values diﬀerent from 0 and 1. Whenever a pred-
icate is precise, rigid, or not imprecise, it only accepts measures with values in {0,
1}. In Fuzzy Logic precision is seen as a degenerate, isolate, case of imprecision.
3.2
Consider the typically rigid predicate, P = prime in the universe N of natural num-
bers. The use of P is known by stating the elemental statements “n is P” from the
if-and-only-if deﬁnition:
n is prime ⇔the only divisors of n are n and 1,
with which it is just the dichotomous possibility that, given a natural number p,
either p is prime, or p is not prime but composed. Set N is perfectly classiﬁed in the
two subsets of prime and composed numbers.
Without introducing a new iﬀdeﬁnition, given two natural numbers there is no
way of establishing if one of them is less prime than the other. Diﬀerent prime num-
bers are equally prime, composed numbers are not comparable by being one less
prime than the other, and given a prime and a composed the second is less prime
than the ﬁrst. For instance, the prime numbers 5 and 103 are just equally prime, the
numbers 12 and 24 are not comparable, and given 12 and 3, it is obviously 12 less
prime than 3. Hence, the relation ≤P is reduced to:
n ≤P m ⇔both n and m are primes, or n is composed and m is prime. Conse-
quently, since there is no initial way of distinguishing between prime numbers, the
only admissible measure is given by.
𝜇P(q) = 1 if q is prime, 𝜇P(q) = 0 if q is composed,
that actually veriﬁes n ≤P m ⇒𝜇P(n) ≤𝜇P(m), and preserves the axioms for
maximals and minimals.
3.3
Mutatis mutandis, a rigid predicate P on X denotes a property p such that the elements
of X either completely verify it, or not verify it at all: the use of P in X can be deﬁned
by an iﬀcondition: “x is P” ⇔such and such. Hence, X is completely classiﬁed in
those elements that fully satisfy p, and those that do not satisfy p at all. The rigid

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
617
predicate P is completely speciﬁed by the only set
P = {x ∈X; x fully satisﬁes p},
whose elements are the maximals for ≤P. Consequently, there is a single quantity
(X, ≤P, 𝜇P) with
(a) x ≤P y ⇔Either x, y ∈P, or x ∈Pc and y ∈P,
(b) 𝜇P(x) = 1 if x ∈P, and 𝜇P(x) = 0 if x ∈Pc,
with which it follows,
𝜇−1
P (1) = P, 𝜇−1
P (0) = Pc.
Thus, in the case of a rigid predicate P is
[P] = P = 𝜇−1
P (1),
the collective is reduced to the set specifying the predicate in X, that, at its turn,
coincides with the anti-image of 1 by the membership function, now reduced to be a
mapping X →{0, 1}. Rigid predicates are speciﬁed by a single fuzzy set that is just
a set, and sets are known in Fuzzy Logic as crisp sets.
Notwithstanding, in Fuzzy Logic there are used singular fuzzy sets whose use is
also degenerate; for instance, the fuzzy sets K speciﬁed in X by the constant functions
𝜇k (x ) = k ∈[0, 1],
Remarks
1. The denotation of a fuzzy set P will be sometimes shortened by just naming them
by its membership function 𝜇P.
2. The classical symbols ∈and ∉, do correspond to the new symbols ∈1 and ∈0,
respectively.
3. The constant fuzzy sets given by the constant membership functions 𝜇0 and 𝜇1
correspond to the sets ∅and X. The other constant fuzzy sets K, represented by
the membership functions 𝜇k, with 0 < k < 1, 𝜇k (x) = k, are neither with, nor
without points, that is, for no x in X is x ∈1K, or x ∈0K. In this sense, fuzzy sets
K are singular fuzzy sets.
4. The only set that is self-contradictory is the empty set since it is A ⊆Ac ⇔A
= ∅, but this is not the case with fuzzy sets. Once the concept of negation is
introduced by associating to each fuzzy set 𝜇P another 𝜇P’ = 𝜇notP, there can be
many fuzzy sets such that 𝜇P ≤𝜇P’. For instance, were 𝜇P’ = 1−𝜇P, it will be
𝜇P self-contradictory if and only if it is 𝜇P(x) ≤0.5 for all x in X, or 𝜇≤𝜇0.5,
and, even if there can be points x such that 𝜇P(x) = 0, that is x ∉P, since 0.5 <
1 there cannot exist any point x such that 𝜇P(x) = 1, no point x in X veriﬁes x ∈
P. As it will be shown latter on, no self-contradictory fuzzy set can have points
totally belonging to it. Perhaps, that the empty set ∅is self-contradictory, that it
has no a single element, is what makes it diﬃcult (for children) to accept it as a
true set.

618
E. Trillas
5. Points x such that x ∈P, with degree one of membership to P, are those recog-
nized as totally verifying the property named by the predicate P. They can be seen
as the prototypes for P in X. Analogously, those x such that x ∉P, with degree
zero of membership to P, are those recognized as non verifying at all the property
named by P, and can be seen as the anti-prototypes for P in X. Self-contradictory
fuzzy sets have no prototypes.
Both prototypes and anti-prototypes can help to evaluate, through comparison
with them, the degree of membership to P of the other elements in X. In this
sense, those fuzzy sets without prototypes or ant-prototypes can be seen, up to
some extent, as “rare” fuzzy sets since they are without, respectively, maximals
or minimals for the perceptive relation ≤P. For instance, provided the predicate
big were used in the set of positive real numbers R+, including zero, with ≤P =
≤, and the non-decreasing membership function 𝜇big (x) = x/(x2+1)1∕2, it is 𝜇big
(0) = 0, accordingly with the fact that 0 is the minimum for ≤, but for no x is
𝜇big(x) = 1, accordingly with the lack of maximals for ≤. In this example, big
has no prototypes in R+ = [0, + ∞). Nevertheless, since it is limx→+∞𝜇big (x) =
1, there are many x ∈R+ for which the corresponding value 𝜇big (x) is as closer
as wished to 1; these x could be called prototypes approximated up to the degree
1 −𝜇big (x).
6. It should be pointed out at once that specifying a imprecise predicate is a more
complex problem than that of specifying a precise one. This, of course, does
not mean that for precise predicates it is easy to recognize the elements with
membership one as it is, for instance, with the predicate transcendent in the real
line as the negation of algebraic.
4 Opposites and Negations
4.1
Predicates are (linguistic) terms, and also its opposites are terms, but their negations
are not so. For instance, the term big is in dictionaries, and so it is the opposite term
small, but it is neither the negation not big, nor the negation not small. If it has sense
to search for the opposites and the negations of big, what has no sense is to search
for an opposite of not big, unless accepting the rule ‘opposite of not P = not opposite
of P’, under which an opposite of not big will be not small.
If a predicate is measurable in X, are their opposites also measurable in X? Pro-
vided a predicate is speciﬁed in X by a quantity (X, ≤P, 𝜇P), is there a quantity
specifying an opposite, or antonym, aP?
In the case of big in [0, 10], it is ≤big = ≤, and a (big) = small is with ≤small = ≥:
x is less big than y ⇔y is less small than x,

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
619
accordingly with the same idea of opposition. Hence, since it can be always supposed
that a(aP) coincides with P, it is ≤aP = ≤−1
P , the qualitative behavior of aP in X is
described by the graph (X, ≤−1
P ), and aP is measurable.
To actually measure aP, and continuing with the idea that it is an opposite of P,
it can be taken a symmetry [6] sP : X →X, that is, a bijective mapping such that
s2
P = idX, verifying [x ≤P y ⇒sP (y) ≤P sP (x)], and deﬁning:
𝜇aP = 𝜇P◦sP
that is,
𝜇aP(x) = 𝜇P(sP(x)), for all x in X.
This mapping veriﬁes (shortening it by s):
(1) If x ≤aP y ⇔x ≤−1
P y ⇔y ≤P x ⇒s (x) ≤P s (y) ⇒𝜇P (s (x)) ≤𝜇P (s (y)) ⇔
𝜇aP (x) ≤𝜇aP (y),
(2) Since if z is a maximal for ≤aP, then s (z) is so for ≤P : 𝜇aP (z) = 𝜇P (s (z)) = 1.
(3) Since if z is a minimal for ≤aP, then s (z) is so for ≤P: 𝜇aP (z) = 𝜇(s (z)) = 0.
Hence, the quantity (X, ≤−1
P , 𝜇P ◦s) speciﬁes an antonym aP of P in X.
Notice that 𝜇a(aP) = 𝜇aP ◦saP = (𝜇P ◦sP) ◦saP = 𝜇P ◦(sP ◦saP) = 𝜇P provided
sP = saP. Hence, the antonym of the antonym actually coincides with the original
predicate provided the symmetries allowing to specify them can be taken as coinci-
dental.
For instance, with big in [0, 10], it is 𝜇small (x) = 𝜇big (10−x) = (10−x)/10 = 1−
x/10, with s (x) = 10−x that veriﬁes s2(x) = s (10−x) = x, and x ≤small y ⇔y ≤x
⇒10−x ≤10-y ⇔s (x) ≤s (y). With it, the maximum 10 for big is the maximum
s(10) = 0 for small, and the minimum 0 for big is the minimum s (0) = 10 for small:
𝜇small (0) = 1 and 𝜇small (10) = 0. Since it can be taken ssmall = s big = 10−id, it is
a(small) = a (a(big)) = big.
Of course, with each symmetry s in X the use of an opposite is reached, and the
question that remains is to know if these symmetries can be freely taken. The answer
is not. Examples like
‘If the bottle is full, it is not empty’,
with ‘full’ the antonym of ‘empty’, reveal the non reversible rule
If x is aP, then x is not P,
that can be translated into membership functions by
𝜇ap(x) ≤𝜇notP(x), for all x in X,
or

620
E. Trillas
𝜇ap ≤𝜇notP ⇔𝜇P◦s ≤𝜇notP,
showing that the symmetry s should respect this inequality. In the extreme case in
which aP coincides with not P and the sign equal holds in last functional inequality,
it is said that aP is a non-regular antonym.
Notice that the rule is not reversible since it is not, for instance, ‘If the bottle is
not empty, it is full’; in general, with the exception of a non-regular antonym, it is
not “If x is not P, then it is x is aP”. This is the beforehand case of big and small,
since with 𝜇notP (x) = 1−𝜇P(x), is 𝜇small (x) = 1−x/10 = 1−𝜇big (x) = 𝜇notbig (x).
Hence, supposing that not P is speciﬁed by
𝜇notP (x) = N(𝜇P (x)), for all x in X [7],
with a numerical function N : [0, 1] →[0,1], there should be kept the inequality
𝜇P◦s ≤N◦𝜇
constraining both mappings s and N.
4.2
For what concerns the negation not P = P’ of P, the only that can be presupposed in
general is that [4],
x ≤P′ y ⇒y ≤P x, that is, ≤P′⊆≤−1
P .
For instance, if it seems to be “x is less not hot than y” if and only if “y is less hot
than x”, there are more sophisticated cases in which there is not equivalence between
both conditionals since not always it is not (not P) coincidental with P, as it happens,
for instance, when not all that can be qualiﬁed as not P is completely known. This
is, in Prolog, the case of the ‘negation by failure’. Provided the primary meaning of
P’ in X can be described by a graph (X, ≤P′), are there eﬀective measures of it? The
answer is yes, but with some conditions on the maximals and the minimals.
Consider the mappings (negation functions) N: [0, 1] →[0, 1], such that,
1. N (0) = 1, and N (1) = 0
2. If a ≤b, then N (b) ≤N (a),
and deﬁne
𝜇P′ = NP◦𝜇P,
with a suitable function N. Then,

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
621
(a) x ≤P′ y ⇒y ≤P x ⇒𝜇P (y) ≤𝜇P (x) ⇒NP (𝜇P (x)) ≤NP(𝜇P (y)) ⇔𝜇P′ (x) ≤
𝜇P′ (y),
(b) Provided the minimals for ≤P′ are maximals for ≤P: 𝜇P′ (z) = NP (𝜇P (z)) = NP
(1) = 0,
(c) Provided the maximals for ≤P′ are minimals for ≤P: 𝜇P′ (z) = NP(0) = 1,
thus, under these hypotheses NP ◦𝜇P can be taken as a measure for P’. Notice that
provided it were ≤P′ = ≤−1
P , (b) and (c) will hold by the same reason they hold with
the opposite.
Were the ‘not’ involutive, that is, P coincidental with not (not P), function NP
should allow to have 𝜇(P′)′ = NP′ ◦𝜇P′ = NP′ ◦(NP ◦𝜇P) = (NP′ ◦NP) ◦𝜇P =
𝜇P, for which it suﬃces to have NP′ ◦NP = id[0,1], or NP′ = NP. Those negation
functions N verifying N2 = N ◦N = id[0,1], are called strong negation functions, or,
for short, strong negations, and can be characterized in the form [7],
N = 𝜑−1◦(1 −id[0,1])◦𝜑,
or
N (x) = 𝜑−1(1 −𝜑(x)), for all x in X,
with 𝜑: [0, 1] →[0, 1], strictly non-decreasing and such that 𝜑(0) = 0, and 𝜑
(1) = 1. That is, 𝜑is an auto-morphism of the ordered unit interval ([0, 1], ≤) of the
real line; obviously, strong negations are strictly decreasing functions and, hence,
continuous functions. The auto-morphism 𝜑is not unique, but there can be several
of them giving the same strong negation.
Notice that negation functions can verify,
N◦N ≤id[0,1], or id[0,1] ≤N◦N,
and that among them the only continuous are the strong negations N ◦N = id[0,1].
The ﬁrst negation functions are called ‘ordinary’ and the second ‘weak’; of course,
those that are both ordinary and weak are the strong ones. Provided N2 were not
comparable with the identity id[0,1], the negation function will be considered a rare
one.
With the order auto-morphisms,
𝜑(x) = ln (1 + 𝜏x𝜔)1∕𝜏, 𝜏> −1, 𝜔> 0,
where ln shortens logarithm in the base e, is obtained the bi-parametric family of
strong negations
N(x) = (1 −x𝜔∕1 + 𝜏x𝜔)1∕𝜔.
With 𝜔= 1 it appears the single-parameter family
N𝜏(X) = 1 −x∕1 + 𝜏x, 𝜏> −1,

622
E. Trillas
consisting of the only ‘rational’ strong negations, and called the Sugeno’s family of
strong negations. The only ‘linear’ strong negation is that given by 𝜏= 0, N0(x) =
1 −x.
It should be pointed out that in fuzzy logic only strong negations are managed,
since they never add discontinuities. Because of strong negations are continuous
negation functions, if 𝜇P is continuous it is also so 𝜇P′ = N ◦𝜇P, and if 𝜇P is discon-
tinuous at some points, 𝜇P′ is only discontinuous at the same points.
A strong negation N has a single ﬁxed point r ∈(0, 1), that is, such that N
(r) = r. To ﬁnd it, it suﬃces to solve the equation 𝜑−1 (1−𝜑(r)) = r ⇔1 = 2𝜑(r), or
r = 𝜑−1(1/2), and it is r ≠0 and r ≠1, since N (0) =1, and N(1)=0. For instance, in
the Sugeno’s family it is: N𝜏(r) = r ⇔1 −r = r(1 + 𝜏r) if 𝜏≠0, and 1−r = r if
𝜏= 0 ⇔r((1 + 𝜏)1∕2 −1)∕𝜏, if 𝜏≠0, and r = 1/2 if 𝜏= 0.
Hence, a fuzzy set 𝜇is self-contradictory if and only if
𝜇(x) ≤𝜇′(x) = N(𝜇(x)) ⇔𝜇(x) ≤𝜑−1(1∕2), for all x in X ⇔𝜇≤𝜇−1
𝜑(1∕2).
It follows that no self-contradictory fuzzy sets can have prototypes, although it can
have anti-prototypes. It should be pointed out that this result concerns a given strong
negation N, and that the fuzzy sets that are ‘absolutely’ non-self-contradictory, or
non-self-contradictory for all strong negation, are those 𝜇that, for any r ∈(0, 1),
there exist points x ∈X such that 𝜇(x) > r; for instance, those with prototypes also
called normalized fuzzy sets.
Notice that since it is N (0) = 1, and N (1) = 0, provided it were 𝜇P the membership
function of a crisp set P, it will be 𝜇P′ (x) = N (𝜇P(x)) ∈{0, 1}, changing 0 in 1,
and 1 in 0. That is, 𝜇P′ is also the membership function of the crisp set 1 −𝜇P, that
is, Pc. In this case, all the strong negations collapse in N0.
Examples in X = [0,1]
1. Deﬁne 𝜇∗
P (x) = 1−𝜇P (1−x). It is easy to check that:
∙There is no negation function N such that 𝜇∗
P = N o 𝜇P. That is, the operation
* is not decomposable, or functionally expressible.
∙𝜇∗
0 = 𝜇1, 𝜇∗
1 = 𝜇0
∙If 𝜇P ≤𝜇Q, then 𝜇∗
Q ≤𝜇∗
P
∙𝜇∗∗
P = 𝜇P
∙𝜇∗
P(x) = 1 ⇔𝜇P(1 −x) = 0.
Hence, the mapping given by 𝜇P →𝜇∗
P is a ‘symmetry’ for fuzzy sets that
seems to be a strong negation. But it is not so since it does not preserve the
complements of crisp sets: if P is a crisp set in [0, 1], it is x ∈Pc ⇔1−x
∈P∗, hence, for instance, it is [0, 0.8]c = (0.8, 1], but [0, 0.8]∗= [0, 0.2).
Consequently, the mapping * cannot deﬁne a negation for fuzzy sets.
Notice that the ﬁxed ‘points’ are those fuzzy sets 𝜇such that 𝜇(1−x) + 𝜇(x)
= 1 as, for instance, 𝜇= id[0,1], 𝜇= 1−id[0,1], [1/3, 2/3], and [0, 1].
2. The interval P = [0.4, 0.7] speciﬁes the precise predicate P = between 0.4 and 0.7,
and its negation is speciﬁed by the complement P’ = [0.4, 0.6]c = [0, 0.4] ∪[0.7,

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
623
1]. Nevertheless, since 𝜇aP (x) = 𝜇P (1−x) =1 ⇔0.3 ≤x ≤0.6, it is aP = [0.3,
0.6], not verifying aP ⊆P’ since, for instance, 0.5 is in aP but not in P’. Hence,
either the predicate P lacks an antonym, or it should be searched by means of a
symmetry diﬀerent from s (x) = 1−x, or the negation of P, not between 0.4 and
0.7, should be taken as the opposite. For instance, provided the Sugeno’s strong
negation with 𝜏= 2 could be taken as a suitable symmetry, it is
𝜇aP(x) = 𝜇P(1 −x∕1 + 2x) = 1 ⇔x ∈[1∕8, 1∕3],
gives aP = [1/8, 1/3] ⊆P’, and it can be taken aP = between 1/8 and 1/3.
It is neither sure that a suitable functionally expressible symmetry s : X →X
to express the opposite, nor a suitable functionally expressible negation function
N : [0, 1] →[0, 1] to express the negation, can always exist. Anyway, if existing,
as it is always supposed in the applications of fuzzy logic, they should verify the
‘compatibility condition’
𝜇◦s ≤N◦𝜇.
Examples like the last one with between 0.4 and 0.7, suggest the necessity of
doing some controlled experimentation in language for knowing which pairs (sP,
NP) are suitable at each case; of course, the case of mathematical language is the
less important one.
5 Conjunction and Disjunction
5.1
Let P and Q be two measurable predicates in the same universe of discourse X:
Provided ’P and Q’, and ’P or Q’, are deﬁned by:
∙x is (P and Q) ⇔(x is P) and (x is Q)
∙x is (P or Q) ⇔(x is P) or (x is Q),
are they measurable predicates?
Let’s try to answer this question on the following conditions,
(1) The primary meanings of ‘P and Q’, and ‘P or Q’, based on the primary mean-
ings of P and Q, respectively, are known. It will suﬃce for it to ﬁnd operations
F and G such that
≤PandQ= F(≤P, ≤Q), ≤PorQ= G(≤P, ≤Q).
(2) Once respective measures 𝜇P and 𝜇Q are given, it will suﬃce to ﬁnd functions
f and g, such that

624
E. Trillas
𝜇PandQ = f(𝜇P, 𝜇Q), 𝜇PorQ = g(𝜇P, 𝜇Q)
become measures for ‘P and Q’, and ‘P or Q’, respectively.
Functions f are called conjunctions, and functions g disjunctions.
5.2
Concerning the conjunction and, it seems to be also acceptable that at least [5], it is,
≤PandQ ⊆≤P ∩≤Q,
with which it is
x ≤PandQ y ⇒x ≤P y & x ≤Q y.
Then, it suﬃces to have monotonic binary operations ⋅in [0, 1] with neutral 1 and
absorbent 0, for continuing last formula by
𝜇P(x) ≤𝜇P (y) & 𝜇Q (x) ≤𝜇Q (y) ⇒𝜇P(x) ⋅𝜇Q (x) ≤𝜇P(y) ⋅𝜇Q (y) ⇔
𝜇PandQ(x) ≤𝜇PandQ(y),
and the mapping deﬁned by
𝜇PandQ = 𝜇P ⋅𝜇Q
veriﬁes the ﬁrst property of a measure for ‘P and Q’. Provided if z is a maximal
for ≤PandQ, it is so for both ≤P and ≤Q, follows 𝜇PandQ (z) = 1.1 = 1. Provided z
is a minimal for ≤PandQ it is so for, at least, one of the relations ≤P, ≤Q, and it is
analogously 𝜇PandQ (x) = 0. That is, under some reasonable hypotheses, 𝜇P ⋅𝜇Q is
a measure for ‘P and Q’.
Remarks
(a) The only that has been proven here is that there are ways of expressing the behav-
ior of ‘P and Q’ by a quantity; that on some conditions this predicate can be mea-
surable. Of course, it does not mean that in all cases 𝜇PandQ will be functionally
expressible by a numerical operation like the ⋅before considered [26].
(b) The particular case in which ≤PandQ = ≤P ∩≤Q, assures that if ≤P and ≤Q are
preorders also ≤PandQ is so.
(c) No hypotheses on the commutativity, associativity, etc., of the conjunction and
are necessary for just studying its measurability.
(d) Obviously, provided the fuzzy sets P and Q are crisp sets, 𝜇P ⋅𝜇Q gives the crisp
set P ∩Q, since the restriction of the operation ⋅to {0, 1} is min.

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
625
5.3
For what concerns the disjunction or, in the ﬁrst place it should be noticed that, in
general, it is not acceptable the identity ≤PorQ = ≤P ∪≤Q, since when both relations
≤P and ≤Q are preorders it can’t be assured that ≤PorQ is so [5]. The only acceptable
is the inclusion ≤P ∪≤Q ⊆≤PorQ, that shows ≤PorQ is not empty, and seems to
suggest the identiﬁcation of this last relation with the minimum one containing both
≤P and ≤Q, that is, to identify ≤PorQ with the direct-sum, ≤P
⨁≤Q, or closure
of both relations that, if both are preorders, is also a preorder. In this form, and for
instance, if there is a P-arc joining x and y, and a Q-arc joining y and z, there is a
(P or Q)-arc joining x and z. Were this identiﬁcation accepted, with any monotonic
binary operation + in [0, 1] with neutral 0, and absorbent 1, it can be said:
∙x ≤PorQ y ⇔x ≤P y or x ≤Q y ⇒𝜇P(x) ≤𝜇P(y) or 𝜇Q (x) ≤𝜇Q (y) ⇒𝜇P(x) +
𝜇Q(x) ≤𝜇P(y) + 𝜇Q(y).
∙If z is a minimal for ≤PorQ, and it is so for both ≤P and ≤Q : 𝜇P(z) + 𝜇Q (z) = 0
+ 0 = 0.
∙If z is a maximal for ≤PorQ, and it is so for, at least, one of the relations ≤P and
≤Q: 𝜇P(z) + 𝜇Q (z) = 1+something, or something +1, or 1 + 1 = 1.
That is, under some reasonable hypotheses, 𝜇P + 𝜇Q is a measure for ‘P or Q’.
Remarks
(a) The only that is proven here is that on some conditions from the measurability of
P and Q, it follows that of ‘P or Q’. What is not at all proven is that the measure
for ‘P or Q’ should be always functionally expressible [26] from those of P and
Q by a numerical operation like +.
(b) No hypotheses on the commutativity, associativity, etc., of the disjunction or are
necessary for just studying its measurability.
(c) Obviously, provided the fuzzy sets P and Q are crisp sets, 𝜇P + 𝜇Q gives P ∪
Q, since the restriction of + to {0, 1} is max.
(d) The operations ⋅and + are mappings
[0, 1]X × [0, 1]X →[0, 1]X,
corresponding, respectively, to the models ⋅= min, and + = max, but not con-
strained by properties like 𝜇⋅𝜇= 𝜇, and 𝜇+ 𝜇= 𝜇, typical of these two opera-
tions. As it is well known the family of t-norms, and that of t-conorms [8], verify
the basic properties of both operations but adding those of commutativity, asso-
ciativity, etc. , only necessary in some particular cases; among copulas [8] some
less restrictive examples can be found.
Since it is not always the case that (x is P and P) does coincide with (x is P),
it is convenient to have operations preserving such cases, as it is with the con-
tinuous t-norms called ordinal-sums [8] that, notwithstanding, are never used in
the applications since they have not too much simple mathematical expressions.

626
E. Trillas
The most popular continuous t-norms are, apart from min, the product (prod),
the Lukasiewicz (W), and their 𝜑-transforms:
∙prod𝜑(x,y) = 𝜑−1 (prod (𝜑(x), 𝜑(y))= 𝜑−1(𝜑(x) ⋅𝜑(y)),
∙W𝜑(x,y) = 𝜑−1 (W (𝜑(x), 𝜑(y)) = 𝜑−1 (max(0, 𝜑(x) + 𝜑(y)-1),
for all auto-morphism 𝜑of the ordered unit interval in the real line, like, for
instance,
∙𝜑(x) = xr, r a rational number, with which it is W𝜑(x, y) = (max(0, xr +
yr−1))1∕r, and prod𝜑= prod.
∙𝜑(x) = 2x/1+x, with which it is prod𝜑(x, y) = 2xy/(1+ y−x + xy).
Notice that prod (𝜇P(x), 𝜇P(x)) = 𝜇P(x)2 ≠𝜇P(x), except if the value of 𝜇P(x)
is 0 or 1. Analogously, W (𝜇P(x), 𝜇P(x)) = max(0, 2𝜇P(x)−1) ≠𝜇P(x), except
if the value of 𝜇P(x) is 0 or 1.
To know something on non decomposable connectives, and particularly on nega-
tions, see [26].
(e) It lacks to develop the study of the connectives and, or, when the predicates are
acting on diﬀerent universes of discourse.
6 Constrained, Qualiﬁed and Modiﬁed Predicates
6.1
Let (X, ≤P, 𝜇P) and (Y, ≤Q, 𝜇Q) two quantities representing the behavior of the
measurable predicates P in X, and Q in Y.
Each non-empty relation
R(P, Q) ⊆X(P) x Y(Q) : (x is P, y is Q) ∈R(P, Q),
allows to deﬁne the constrained predicate Q/P = ‘Q if P’, on X x Y, by [5],
(x, y) is Q/P ⇔(x is P, y is Q) ∈R(P, Q),
of which an example is obtained when the ﬁrst term, (x, y) is Q/P, is interpreted as
the conditional statement ‘If x is P, then y is Q’.
Provided Q/P is measurable by means of a quantity (X x Y, ≤Q∕P, 𝜇Q∕P), could it
be studied how to express 𝜇Q∕P by means of 𝜇P and 𝜇Q?
A measure 𝜇Q∕P is said to be functionally expressible or decomposable, if there
exists a function
J: [0, 1] × [0, 1] →[0, 1],

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
627
such that, 𝜇Q∕P (x,y) = J(𝜇P(x), 𝜇Q(y)) for all (x, y) ∈X x Y. Let’s see which prop-
erties can be required for J in several cases.
(a) Provided it were ≤Q∕P = ≤P x ≤Q, it will suﬃce that J be non-decreasing in its
two variables to have,
(x1, y1) ≤Q∕P (x2, y2) ⇔x1 ≤P x2 & y1 ≤Q y2 ⇒𝜇P(x1) ≤𝜇P(x2) & 𝜇Q(y1)
≤𝜇Q(y2) ⇒J(𝜇P(x1), 𝜇Q (y1)) ≤J(𝜇P(x2), 𝜇Q (y2)) ⇔𝜇Q∕P (x1,y1) ≤𝜇Q∕P
(x2,y2).
For what concerns the minimals and the maximals, it suﬃces that J also verify
J(0,0) = 1, and J(1,1) = 1. Since usually Q/P ≠P/Q, J will be non-commutative
in general; for instance,
J(x, y) = T(xr, y),
with a t-norm T (min, prod, W, …), and r > 0, is an example for this case and the
most used of these J in the applications of fuzzy logic, is with either T = min,
or T = prod, and r = 1. The t-norm W, as well as their 𝜑-transforms W𝜑, are
never used since they have zero-divisors: W𝜑(a, b) = 0 ⇔1 ≤𝜑(a) + 𝜑(b), and
then a rule ‘If x is P, then y is Q’ can verify the non desirable property 𝜇Q∕P(x,
y) = J(𝜇P(x), 𝜇Q(y)) = 0 with 𝜇P(x) > 0, and 𝜇Q(y) > 0: that is, the degree
up to which the rule holds is zero with both its antecedent and consequent with
a positive degree.
(b) Provided it were ≤Q∕P = ≤P x ≤−1
Q , it will suﬃce that J be non-decreasing in its
ﬁrst variable and decreasing in the second to have:
(x1, y1) ≤Q∕P (x2, y2) ⇔x1 ≤P x2 & y2 ≤Q y1 ⇒𝜇P(x1) ≤𝜇P(x2) & 𝜇Q(y2)
≤𝜇Q(y1) ⇒J(𝜇P(x1), 𝜇Q (y1)) ≤J(𝜇P(x2), 𝜇Q (y2)) ⇔𝜇Q∕P (x1,y1) ≤𝜇Q∕P
(x2,y2).
For what concerns the minimals and the maximals, it suﬃces that J also verify
J(0, 1) = J(1, 0) = J(1, 1) =1. An example is given by J(a, b) = max(a, 1−b).
(c) Provided it were ≤Q∕P = ≤−1
P
x ≤Q, it will suﬃce that J be decreasing in its
ﬁrst variable and non-decreasing in the second, with J(0, 0) = J(0, 1) = J(1,
1) = 1 and J(1,0) = 0. Example are given by J(a, b) = max(1−a, b), and J(a, b)
= 1−a(1−b), that correspond to the classically called ‘implication functions’.
Another example is given by the functions J(x, y) = Sup {z ∈[0, 1]; T(x, z) ≤
y} (called residuated implications), that are non functionally expressible unless
if T = W𝜑, and that come from the identity p + q’ = Sup {t; p ⋅t ≤q} of the
so-called classical material implication, that is only valid in complete Boolean
algebras.
Etc.

628
E. Trillas
6.2
Let it be (X, ≤P, 𝜇P) the quantity describing the use, or meaning, of a predicate P in
X, and the range 𝜇P(X) ⊆[0, 1], as well as a measurable predicate 𝜏acting on 𝜇P(X)
under a relation ≤𝜏= ≤, and a measure 𝜇𝜏. Deﬁne the qualiﬁed predicate ‘P is 𝜏’ in
X, by [5],
x is (P is 𝜏) ⇔𝜇P (x) is 𝜏,
and suppose ≤Pis𝜏⊆≤P.
On these conditions, 𝜇Pis𝜏= 𝜇𝜏o 𝜇P : X →[0, 1], veriﬁes:
x ≤Pis𝜏y ⇒x ≤P y ⇒𝜇P(x) ≤𝜇P(y) ⇒𝜇𝜏(𝜇P(x)) ≤𝜇𝜏(𝜇P(y)) ⇔(𝜇𝜏◦
𝜇P)(x) ≤(𝜇𝜏◦𝜇P)(y).
Provided the maximals and the minimals for ≤Pis𝜏do coincide with those of ≤P, and
𝜇𝜏(0)=0, 𝜇𝜏(1)=1, it follows the same corresponding values for them under 𝜇𝜏◦𝜇P;
in another case, it only holds that 𝜇Pis𝜏is non-decreasing. Hence, on this condition,
the quantity (X, ≤Pis𝜏, 𝜇𝜏o 𝜇P) mathematically describes the use of ‘P is 𝜏’ in X.
Examples
1. Let P = small in X = [0, 10], with ≤P=≥, and 𝜇P(x) = 1−x/10, with which
𝜇P(X) = [0, 1]. Let 𝜏=large in [0, 1] with 𝜇𝜏=≤, and 𝜇𝜏(x) = x, for all x in
[0,1].
Then, ‘P is 𝜏’ = small is large, behaves with the measure: 𝜇𝜏(1−x∕10) = 1−x∕10,
and it can be said that ‘small is large’ coincides with small.
2. In the former example, change X to [0, 1] with P = large, and 𝜇P(x) = 1 −x, and
Y to [0, 10] with 𝜏= small, and 𝜇𝜏(x) = x/10. Then ‘P is 𝜏’ = ‘large is small’ is
represented in [0, 1] by 𝜇𝜏(𝜇P(x)) = (1 −x)∕10.
6.3
A linguistic modiﬁer or hedge (m), is an adverb acting on the predicate P just in the
form mP, not to be confused with ‘P is m’. For instance, with P = short and m =
very, it is mP = very short, that has nothing to do with the linguistic absurd ‘short is
very’.
A characteristic distinguishing imprecise from precise predicates is that once P
and m are known, mP is immediately understandable. Instead, if P is precise, usually
mP needs a new deﬁnition to be understood, as it happens, for instance, with even and
very even in the set of integers. In principle, adverbs m modify but do not abruptly

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
629
change the meaning of a imprecise predicate, and this is a simple test to identify if
P is either precise or imprecise.
If P in X is with ≤P and 𝜇P, and m acts in 𝜇P(X) ⊆[0, 1] with ≤⊆≤m and a
measure 𝜇m, provided it is ≤mP ⊆≤P, it can be taken 𝜇mP = 𝜇m ◦𝜇P, since [5],
x ≤mP y ⇒x ≤P y ⇒𝜇P(x) ≤𝜇P(y) ⇒𝜇P(x) ≤m 𝜇P(y) ⇒𝜇m(𝜇P(x)) ≤
𝜇m(𝜇P(y)) ⇔(𝜇m◦𝜇P)(x) ≤(𝜇m◦𝜇P)(y).
Then, provided there is agreement between the corresponding maximals and min-
imals, mP is measured by this composition of functions.
Remarks
(a) Two important types of modiﬁers are:
∙Expansive modiﬁers, those such that id[0,1] ≤𝜇m
∙Contractive modiﬁers, those such that 𝜇m ≤id[0,1].
With the expansive it results 𝜇P ≤𝜇mP, and with the contractive 𝜇mP ≤𝜇P. This
is what happens with the Zadeh’s deﬁnitions
𝜇more or less(a) =
√
a, 𝜇very(a) = a2,
since in [0, 1] it is a2 ≤a ≤
√
a.
(b) There are again other modiﬁers.
∙Those called internal modiﬁers, like the antonym, in which the measure of P
is modiﬁed by a symmetry in the universe of discourse.
∙Those called external modiﬁers, like the negation, whose measure is modiﬁed
by a negation function ranging in the set [0, 1] where membership functions
take their values.
7 The Algebras of Fuzzy Sets
7.1
A function 𝜎∈[0, 1]X only can represent a fuzzy set provided there is a predicate P
acting in X, such that the quantity (X, ≤P, 𝜇P) is with 𝜇P = 𝜎. Hence, the elements
in the set [0, 1]X only ‘potentially are fuzzy sets’, analogously that the elements in
the set {0, 1}X are sets in the power-set 2X, by the equivalence:
A ∈2X ⇔It exists 𝜗∈{0, 1}Xsuch that A = 𝜗−1(1).
Thus, the consideration of the (abstract) algebraic structures that can be build up
from [0, 1]X, has sense for theoretically studying both the ‘algebras’ of fuzzy sets in

630
E. Trillas
themselves, and for doing practical computations once fuzzy sets are combined by
operations like the former ⋅, + , and ’, introduced in the last section. The operations to
be taken into account for obtaining such structures are traditionally ﬁxed by analogy
with what happens with sets, where the axiom of speciﬁcation states that all precise
predicates P, Q, etc., acting in X specify single subsets P, Q, etc., such that:
∙If ‘x is P and Q’, it is x ∈P ∩Q
∙If ‘x is P or Q’, it is x ∈P ∩Q
∙If ‘x is not P’, it is x ∈Pc
∙If ‘If x is P, then x is Q’, it is P ⊆Q,
with 𝜇P∩Q = min(𝜇P, 𝜇Q), 𝜇P∩Q = max(𝜇P, 𝜇Q), and 𝜇c
P = 1−𝜇P. Then with
fuzzy sets in [0, 1]X, it will be:
(1) P ∩Q (intersection or conjunction), deﬁned by 𝜇P∩Q = 𝜇P ⋅𝜇Q, with a suitable
binary operation ⋅: [0, 1]X x [0, 1]X →[0, 1]X
(2) P ∪Q (union or disjunction), deﬁned by 𝜇P∪Q = 𝜇P + 𝜇Q, with a suitable binary
operation + : [0, 1]X x [0, 1]X →[0, 1]X
(3) Pc (pseudo-complement or negation), deﬁned by 𝜇c
P = 𝜇
′
P, with a suitable unary
operation ‘: [0, 1]X →[0, 1]X
(4) P ⊆Q (inclusion), deﬁned by [𝜇P ≤𝜇Q ⇔𝜇P (x) ≤𝜇Q (x), for all x in X].
Hence, the algebras of fuzzy sets are ﬁve-tuples
([0, 1]X, ≤; ⋅, +, }),
restricted to verify some laws linking ≤, ⋅, +, and ‘. Although current fuzzy logic
mainly works under the hypothesis of decomposability, or functional expressibility
of the three operations ⋅, +, and ‘, it is not at all clear that such hypothesis can be
generally acceptable. This hypothesis has, notwithstanding, the great advantage of
allowing to translate into functional equations the study of the laws. For instance,
denoting by 𝜇r the ‘constant’ fuzzy sets the possible law of non-contradiction,
𝜇⋅𝜇′ = 𝜇0,
that with sets is P ∩Pc = ∅, is translated into the functional equation F(𝜇(x),
𝜇’(x)) = 0, for all 𝜇and x, and it suﬃces to solve the equation F(a, N(a)) = 0, provided
there exists numerical functions F: [0,1] × [0, 1] →[0, 1], and N: [0,1] →[0, 1] such
that
(𝜇⋅𝜎)(x) = F (𝜇(x), 𝜎(x)), and (𝜇′)(x) = N(𝜇(x)),
for all x in X. Of course, to solve such functional equation it is necessary to count
with some properties of F and N, among which continuity is essential. For instance,
if F is a continuous t-norm and N a strong negation, the solutions are F = W𝜑, and N
≤N𝜑. Analogously, the validity of the possible distributive law 𝜇⋅(𝜎+ 𝛾) = 𝜇⋅𝜎
+ 𝜇⋅𝛾, particularized with sets to P ∩(Q ∪R) = (P ∪Q) ∪(P ∩R), can be studied

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
631
by solving the functional equation F (a, G (b, c)) = G (F (a, b), F (a, c)), provided it
exists a second numerical function G: [0, 1] x]0,1] →[0, 1], such that
(𝜇+ 𝜎)(x) = G(𝜇(x), 𝜎(x)),
for all x ∈X. In the case F is non-decreasing in both variables, a solution is given
by G = max, regardless of which operation ⋅is taken. The other distributive law
𝜇+ (𝜎⋅𝛾) = (𝜇+ 𝜎) ⋅(𝜇+ 𝛾), holds with F = min, regardless of G but provided
it is non-decreasing; of course, both laws jointly hold whenever ⋅= min and + = max.
In the same vein, the validity of the possible laws
𝜇⋅𝜇= 𝜇+ 𝜇= 𝜇,
coming from the classical P ∩P = P ∪P = P, is reduced to solve the equations F(a,
a) = a, and G(a, a) = a, with obvious solutions F = min and G = max.
In the applications it is necessary to count with operations verifying some of these
laws. If, for instance, it is known that in the current universe of discourse it is always
“‘x is P’ or ‘x is not P”’, the operation + should be chosen among those for which the
law 𝜇+ 𝜇’ = 𝜇1 holds. On the contrary, for just theorizing and obtaining results of
a general validity, it is better to keep the algebra ([0, 1]X, ≤; ⋅, +, ‘) with a minimal
number of laws, even if sometimes more laws allowing to obtain new properties
should be added.
Notice that the relation ≤inherits all the properties of the ordering of the unit
interval, except linearity. It is a reﬂexive, anti-symmetric and transitive relation for
which there are fuzzy sets that are not comparable, that cannot be linked by it; is a
partial order. Hence ([0, 1]X, ≤) is a poset whose minimum element is 𝜇0, and whose
maximum is 𝜇1, that is 𝜇0 ≤𝜇≤𝜇1, for every fuzzy set 𝜇.
7.2
By deﬁnition, a Basic Algebra of fuzzy sets (BA) [9], 𝛥= ([0, 1]X, ≤; ⋅, + , ‘),
veriﬁes the following laws or axioms:
(1) If 𝜇≤𝜎, then 𝜎’ ≤𝜇’; 𝜇0’ = 𝜇1; 𝜇1’ = 𝜇0
(2) 𝜇⋅𝜇0 = 𝜇0 ⋅𝜇= 𝜇0; 𝜇⋅𝜇1 = 𝜇1 ⋅𝜇= 𝜇
𝜇+ 𝜇0= 𝜇0 + 𝜇= 𝜇; 𝜇+ 𝜇1= 𝜇1 + 𝜇= 𝜇1,
for all 𝜇in [0, 1]X.
(3) If 𝜇≤𝜎, then: 𝜇⋅𝛿≤𝜎⋅𝛿; 𝛿⋅𝜇≤𝛿⋅𝜎; 𝜇+ 𝛿≤𝜎+ 𝛿; 𝛿+ 𝜇≤𝛿+ 𝜎,
for all 𝛿in [0, 1]X,
(4) In 𝛥0= ({0, 1}X, ≤; ⋅, + , ‘) it is
𝜇⋅𝜎= min (𝜇, 𝜎); 𝜇+ 𝜎= max (𝜇, 𝜎); 𝜇’ = 1- 𝜇,
for all 𝜇, 𝜎∈{0, 1}X.
Because of the few and weak laws a BA enjoys, only the following properties can
be deductively proven.

632
E. Trillas
1. 𝛥0 is isomorphic to the power-set 2X once endowed with the classical operations
of intersection (∩), union (∪), and complement (c), 𝜇0 corresponds to the empty
set ∅, and 𝜇1 to the total X. 𝛥0 is a Boolean algebra.
2. Only with ⋅= min and + = max, is 𝛥a lattice. Provided the negation is strong
(𝜇“= 𝜇), 𝛥is a De Morgan-Kleene algebra since, in this case, it holds 𝜇⋅𝜇’ ≤𝜎
+ 𝜎’, for all 𝜇, 𝜎in [0, 1]X, and 𝛥0 consists in the Boolean elements of 𝛥.
Hence, a BA is neither a Ortho-lattice, nor a fortiori is a Boolean algebra.
3. It is always 𝜇⋅𝜎≤min (𝜇, 𝜎), and max (𝜇, 𝜎) ≤𝜇+ 𝜎.
Hence, it is:
𝜇⋅𝜎≤𝜇≤𝜇+ 𝜎; 𝜇⋅𝜎≤𝜎≤𝜇+ 𝜎.
In particular, it holds 𝜇⋅𝜇≤𝜇≤𝜇+ 𝜇, for all fuzzy set 𝜇.
4. It holds:
𝜇≤𝜎& 𝛼≤𝛽⇒𝜇⋅𝛼≤𝜎⋅𝛽, 𝜇+ 𝛼≤𝜎+ 𝛽.
5. It holds: 𝜇’ ⋅𝜎’ ≤(𝜇⋅𝜎)’; (𝜇+ 𝜎)’ ≤𝜇’ + 𝜎’.
6. If + = max, and regardless of ⋅and ‘, it holds:
(𝜇+ 𝜎)’ ≤𝜇’ ⋅𝜎’.
If ⋅= min, and regardless of + and ‘, it holds:
(𝜇⋅𝜎)’ ≤𝜇’ + 𝜎’.
7. With ⋅= min, and regardless of +, it holds:
𝜇+ 𝜎⋅𝛾= (𝜇+ 𝜎) ⋅(𝜇+ 𝛾).
8. With + = max, and regardless of ⋅, it holds:
𝜇⋅(𝜎+ 𝛾) = 𝜇⋅𝜎+ 𝜇⋅𝛾.
9. With the operations: 𝜇∧𝜎= (𝜇’ + 𝜎’)’, and 𝜇∨𝜎= (𝜇’ ⋅𝜎’)’, also 𝛥∗= ([0,
1]X, ≤; ∧, ∨, ‘) is a BA called the ‘dual’ of 𝛥, and that inherits all the additional
properties enjoyed by 𝛥.
Remark Since BAs are deﬁned by just a few and weak laws or axioms, only very
simple calculations are allowed in them, but what can be proven in their framework
is of a so very general character that will be preserved in case of considering oper-
ations ⋅, +, and ‘ , with more properties than those just deﬁning the BA. Since no
universal algebra of fuzzy sets can hold for all applications in all domains, as it is
with classical sets, it seems suitable the name ‘basic fuzzy algebra of fuzzy sets’ that
allow to add those properties that can be required at each case as it is, for instance,
the idempotency of the conjunction.

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
633
One of the weaknesses of Boolean and De Morgan algebras, as well as of Ortho-
modular lattices for the study of natural Language and ordinary reasoning, lies in the
big amount of axioms they enjoy and that make such algebraic structures too rigid
to aﬀord the typical ﬂexibility of human’s language and reasoning. These structures
own laws, like the commutative of ⋅, not always are in language where, for instance,
‘time’ often causes the failure of the commutative property for the conjunction ‘and’.
The world of human’s language and reasoning is very diﬀerent of those of artiﬁcial
languages and formal deductive reasoning.
At this respect, the Standard algebras of fuzzy sets [11] (those in which F is a
continuous t-norm, G a continuous t-conorm, and N a (continuous) strong negation),
show a lot of cases in which some Aristotelian forms are valid in one but not in
another algebra. For instance, the classical laws of duality,
(𝜇⋅𝜎)′ = 𝜇′ + 𝜎′, and (𝜇+ 𝜎)′ = 𝜇′ ⋅𝜎′,
are broken when the t-norm giving the conjunction, and the t-conorm giving the
disjunction, are not duals with respect to the strong negation giving the pseudo-
complement, as it is, for instance, with + = max, and ⋅= prod. It is also the case
that the von Neumann law of ‘perfect repartition’, 𝜇= 𝜇⋅𝜎+ 𝜇⋅𝜎’, only holds
provided the t-norm and the t-conorm are not dual [11].
In the words of Satosi Watanabe [12], a new land can be perhaps oﬀered to young
researchers, provided “a direct contact with the world of common sense which is the
mother earth of all knowledge” is not disdained at all.
8 Complex Measures and Type-Two Fuzzy Sets
8.1
A practical problem, that often appears with Zadeh’s fuzzy sets 𝜇∈[0, 1]X, is when
their values 𝜇(x) in [0, 1] only can be computed approximately. For instance, if it
is known that 𝜇(x) approaches
√
2/2 then, depending on the approximation needed
by the problem at hand, it could be taken a value either between 0.70 and 0.71, or
between 0.706 and 0.707. There is almost always a problem of uncertainty in the
practical management of fuzzy sets.
This problem causes a theoretical trouble since if, for instance, two fuzzy sets 𝜇
and 𝜎are such that 𝜇(x) = 𝜎(x) for all x in X-{x0}, and 𝜇(x0) < 𝜎(x0), it is not 𝜇
= 𝜎, but 𝜇≤𝜎with 𝜇≠𝜎. If X is with a large number of elements, X = [0, 1010]
for instance, the same comes if the diﬀerence between 𝜇and 𝜎is limited to a few
(say, eight hundred) points in X. It happens analogously when operating with several
fuzzy sets; for instance, with X = [0, 1], 𝜇(x)= x, 𝜎(x)= x2, and ⋅= product, it is
(𝜇⋅𝜎)(x) = x3, and taking the rounding 𝜇(𝜋/4) = 0.7854, and the rounding 𝜎(𝜋/4) =
0.6169, the value (𝜇⋅𝜎)(𝜋/4) will charge its rounding with the rounding of 𝜋/4 and

634
E. Trillas
𝜋2/4. Of course, this is nothing strange in practical computations where the analysis
of errors is well known. Nevertheless, the situation becomes worst when the value
𝜇(x) is just appreciated empirically without a previous model for the function 𝜇, in
which case the, let’s say, naked eye of the expert, only can appreciate that the value
𝜇(x) is between values 𝜇1(x) and 𝜇2(x). For these reasons it could be suitable to
change the unit interval to a set of intervals whose extremes belong to [0, 1].
There is another problem suggesting a change from Zadeh’s fuzzy sets to func-
tions whose values do not belong to the linearly ordered unit interval. If the qualita-
tive or primary meaning of P in X, is represented by a graph (X, ≤P) and the relation
≤P is not linear, as it often happens, once a measure 𝜇P is speciﬁed, the ‘working use’
given by ≤𝜇P cannot coincide with the primary meaning ≤P, but such coincidence
could be just possible provided ≤𝜇P were not linear. Something that only can come
from re-deﬁning fuzzy sets as functions taking their values in a non-linearly ordered
set. For instance, provided ≤P is a preorder, =P is an equivalence, the quotient set
inherits the partial order ≤∗
P between classes, and the measure ≤∗
P is that mentioned
in Sect. 2, it is:
x ≤P y ⇔[x] ≤∗
P [y] ⇔𝜇∗
P(x) ≤∗
P 𝜇∗
P(y),
showing that ≤𝜇∗
P perfectly reﬂects the primary meaning of P in X. This is at the
cost of avoiding a ‘numerical’ measuring of the extent up to which ‘x is P’, and
substituting it by a ‘qualitative’ type of measuring [32].
An additional view for changing the range [0, 1] by a partially ordered range,
is that with the ﬁrst the deﬁnition of a fuzzy set 𝜇P is too ‘crisp’, since very small
variations of some few values 𝜇P(x) change the fuzzy set to another one, as it is
shown at the beginning of this section, and of which there is no reason whatsoever
for believing that it does not represent a measure of the same predicate.
8.2
Without avoiding numerical measuring, but taking care of the cases like those in
Sect. 8.1, consider the complex unit square,
C = {a + ib; 0 ≤a ≤1 & 0 ≤b ≤1},
on which some well known measures used in both science and technology take their
values. With the usual ordering given by
a + ib ≤a∗+ ib∗⇔a ≤a∗& b ≤b∗,
(C, ≤) is a poset with minimum 0 = 0 + i0, and maximum 1 = 1 + i.

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
635
If P is used on X with a primary meaning given by ≤P, a complex measure of the
extent up to which x is P [4], is a mapping
𝜇P∶X →C, 𝜇P(x) = 𝜇1
P(x) + i𝜇2
P(x) ∈C,
verifying the axioms,
1. If x ≤P y, then 𝜇P(x) ≤𝜇P(y)
2. If z is a maximal under ≤P, it is 𝜇P(z) = 1
3. If z is a minimal under ≤P, it is 𝜇P(z) = 0
and the complex quantity (X, ≤P, 𝜇P) can be taken to represent the meaning of P in
X. Now the ‘working use’ ≤𝜇P is given by:
x ≤𝜇P y ⇔𝜇P (x) ≤𝜇P (y),
verifying ≤P ⊆≤𝜇P, and even without being sure that the equal sign can hold
between both relations, but since the second is not linear, there are more possibilities
of coincidence whenever the ﬁrst is not linear. Of course, were ≤P linear then no
coincidence is possible. Notice that 𝜇1
P and 𝜇2
P are in [0, 1]X. With complex measures
it is possible to consider fuzzy sets as functions in CX, and translating into it the same
axioms deﬁning a BA.
For instance, instead of the former fuzzy set 𝜇with value
√
2/2 in some point, it
can be taken a complex one 𝜇whose value in the same point is the complex number
0.70 + 0.71i. When in praxis it is not possible to determine values of 𝜇others than
with a certain approximation in the form 𝜇(x) ∈[𝜇1(x), 𝜇2(x)], it can be appropriate
to take a measure of the type 𝜇(x) = 𝜇1(x) + i𝜇2(x).
Notice that the set of the closed sub-intervals of [0, 1] may be endowed with the
order of C, and with it both sets, those of the sub-intervals of [0, 1] and C, are posets
with [0, 0] the minimum interval, and [0,1] the maximum one. Let X be the set of
those sub-intervals, and P a predicate on X such that
[a1, b1] ≤P [a2, b2] ⇔a1 ≤a2 & b1 ≤b2,
like, for instance, P = to the left, with ≤P indicating ‘less to the left than’. The function
𝜇([a, b]) = b + i (a+b),
veriﬁes:
(1) [a1, b1] ≤P [a2, b2] ⇒a1 ≤a2 & b1 ≤b2 ⇒a1 + b1 ≤a2 + b2 & b1 ≤b2 ⇒
b1 + i(a1 + b1) ≤b2 + i(a2 +b2) ⇔𝜇([a1, b1]) ≤𝜇([a2, b1]).
(2) ([0, 0]) = 0 + i0
(3) ([0,1]) = 1 + i (0+1) = 1 + i,

636
E. Trillas
and, thus 𝜇is a complex measure of P. Notice that if b2−b1 = r, and also a2−a1 = s, it
is 𝜇([a2, b2])−𝜇([a1, b1]) = (b2−b1) + i(a2−a1 + b2−b1) = r + 2si; hence, provided
r ≤e and s ≤e, it is r + 2si ≤e(1 + 2i).
Remarks
(a) The similarity between interval-valued and complex-valued fuzzy sets, reveals
that the more used case of the so-called ‘type-two fuzzy sets’ [13], the interval-
valued ones, are nothing else than complex fuzzy sets.
(b) For what concerns true type-two fuzzy sets, that is those corresponding to pred-
icates whose primary meaning only can be measured linguistically like, for
instance, ‘the degree up to which x is P is high, or 𝜇high(x)’, they correspond
to measures valued in [0, 1]X, that is, mappings 𝜇P: X →[0,1]X. Since ([0, 1]X,
≤) is a poset with minimum 𝜇0 and maximum 𝜇1, the deﬁnition of the three
axioms of a measure can be immediately reproduced. Since the ‘working mean-
ing’ ≤𝜇P will not be linear, type-two fuzzy sets also can oﬀer more opportunities
to perfectly reﬂect not linear primary meanings. Although the management of
true type-two fuzzy sets is more complicated than that of fuzzy sets, there is in
their favor the fact that fuzzy sets are, for what has been said, too crisp.
9 Conclusions
9.1
In science, and aside of its informal common use, the word ‘truth’, whose proper
setting of study as a concept is philosophy, deserves no particular attention. What
follows concerns some argumentation on why this is that.
‘Truth’ is an abstract concept whose mother-predicate is true, and that is applied
to statements in the form “‘x is P’ is true”. Thus, the minimal universes of discourse
to be considered for beginning with an analysis of true are the sets X(P) = {x is P; x
∈X}.
Provided T = true is measurable, that is, a graph (X(P), ≤T) is known, the problem
for grasping the full meaning of T in X(P), lies in obtaining a measure for it, that is,
a mapping 𝜇T: X(P) →[0, 1] verifying the three axioms of a measure in the graph:
(1) x is P is less T than y is P ⇔x is P ≤T y is P ⇒𝜇T(x is P) ≤𝜇T(y is P).
(2) ‘z is P’ is maximal under ≤T (‘z is P’ is completely true) ⇒𝜇T (z is P) = 1
(3) ‘z is P’ is minimal under ≤T (‘z is P’ is completely not true) ⇒𝜇T (z is P) = 0.
Since T is applied to the statements qualifying by P the elements in X, provided
it were (x is P) ≤T (y is P) ⇒x ≤P y, with the same maximals and minimals for
both relations, it is suﬃcient to express 𝜇T through an order auto-morphism 𝜑: [0,
1] →[0, 1]:
𝜇T (x is P) = 𝜑(𝜇P(x)),

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
637
since:
(1) (x is P) ≤T (y is P) ⇒x ≤P y ⇒𝜇P(x) ≤𝜇P(y) ⇒𝜑(𝜇P(x)) ≤𝜑(𝜇P(y)) ⇔𝜇T(x
is P) ≤𝜇T (y is P)
(2) If ‘z is P’ is a maximal, 𝜇T (z is P) = 𝜑(1)=1
(3) If ‘z is P’ is a minimal, 𝜇T (z is P) = 𝜑(0)=0.
Of course, the speciﬁcation of 𝜇T depends on that of 𝜑that will come from the
contextual information on the considered case. When it can be accepted 𝜑= id[0,1], it
results 𝜇T (x is P) = 𝜇P(x), that is what is classically done by accepting that the degree
up to which x is P is true coincides with the degree up to which x is P. Nevertheless,
if it should be taken, for instance, 𝜑(a) = a2, or 𝜑(a) =
√
a it will result, respectively,
𝜇T (x is P) ≤𝜇P(x), or 𝜇T (x is P) ≥𝜇P(x).
The opposite of true is false. Hence, for each symmetry s in X it can be deﬁned
𝜇false (x is P) = 𝜑(𝜇P(s(x)). In language, the concept of truth is actually managed
through the linguistic variable [29] with principal terms true and false: {true, false,
not true, not false, fairly true, fairly false, ...}.
9.2
The use of the predicate probable [14, 30], often appearing in the language of science
and actually used in it, deserves some attention since its use is only well known in the
setting of Ortho-modular lattices and, more particularly in Boolean algebras, with
the Kolmogorov’s type deﬁnition of a measure of probability once applied to ‘events’
represented by the elements in the Ortho-modular or Boolean lattice.
It should be noticed that the consideration of ‘events’ in a lattice, forces that the
conjunction, disjunction and negation of the statements naming the events, should be
represented, respectively, by a unique intersection, union and complement, provided
the statements are precise. This implies a limitation on the use of ‘probability’ in
language where imprecision and uncertainty are pervasive. For instance, this model
allows to speak of the probability of extracting a ball from a urn with ﬁfteen balls
of which six are red and nine are blue, but it is not applicable when concerning a
urn containing around a dozen of balls of which a few are red and several are blue,
a case that can happen when the urn is observed at some distance by naked eye and
it is impossible to know the exact numbers of balls.
In language, probable is directly applied to usual statements like ‘Is probable that
John is rich’. To do this, and to calculate a measure of how probable ‘John is rich’
is, it is necessary to grasp the meaning of P = probable, that is, to know a graph (X,
≤P), and a measure 𝜇P. But, in science the universe of discourse is supposed to be
an Ortho-modular or a Boolean lattice where the relation ≤P is understood as the
partial order ≤of the lattice [a ≤b ⇔a ⋅b = a ⇔a + b = b], with which it seems
evident that a ≤b implies a ≤P b, even if the reciprocal is not so evident, and hence
the equality ≤P = ≤, or ≤P = ⊆in the case of a Boolean algebra of sets, is just a
hypothesis.

638
E. Trillas
A measure of probability is a mapping p: LE(X) →[0, 1], where LE(X) is the
Ortho-modular lattice of events, verifying the axioms:
1. If x ≤y’, then p (x + y) = p(x) + p(y)
2. p(1) = 1,
from which it follows,
3. p(0) = 0
4. p(x’) = 1−p(x), for all x in LE(X)
5. If x ≤y, then p(x) ≤p(y),
with ⋅, +, and ‘, the three lattice operations, ≤the ‘natural’ order of the lattice, 1
its maximum, and 0 its minimum. Notice that just in the case of a Boolean algebra it
is [x ≤y’ ⇔x ⋅y = 0]. All that allows to state that p is a measure of probable when
the relation ≤probable coincides with the order of the universe that is either a proper
Ortho-modular lattice, or a Boolean algebra in particular, and it can be proved as
follows: Since in Ortho-modular lattices hold,
x ≤y ⇔y = x + x’ ⋅y, with x ≤x + y’ = (x’ ⋅y)′,
provided it were x ≤y, it will follow p(y) = p (x + x’⋅y) = p(x) + p(x’ ⋅y) ≥
p(x).
For weak structures, like the BA with ⋅= min, + = max, and ‘ = 1−id[0,1], there
is only an almost satisfactory theory [16] with which the (numerical) probability of
a fuzzy event can be computed, but until now there is no satisfactory theory for less
restrictive BA. It still lacks a clear theory in which the probability can take its values
in [0, 1]X, a theory that is necessary for a good understanding of the cases in language
in which [15] it is said, for instance, “the probability that John is rich is very low”
That is, a theory of linguistic probabilities.
Analogously to probability, the concepts of possibility, necessity, uncertainty and
imprecision, coming respectively from the mother-predicates possible, necessary,
uncertain and imprecise, still deserve a deep theoretical consideration through its
uses in language [17, 18].
9.3
As it was pointed out in Sect. 2, to specify a measure 𝜇P for a primary use ≤P of
a predicate P in X, some additional information is needed since the three axioms
deﬁning a measure are not suﬃcient to individuate a single measure. This infor-
mation only can come from the concrete use of P in X, that is, from the context
surrounding such use in which the purpose for it often plays an important role. For
instance, and even if the primary meaning can be ﬁxed, the measure of old will not
be the same if the contextual information is that old is used by people in a primary
school’s class, in a university class, or in an insurance company. It also will not be
the same provided the purpose of using old is purely descriptive, or it is in a fun

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
639
conversation. All this information, jointly with that furnished by the relation ≤P,
is actually summarized in the measure 𝜇P. Hence and for imprecise predicates, the
quantity (X, ≤P, 𝜇P) is context-dependent and purpose-driven. The speciﬁcation of
a predicate is strictly related with what surrounds the predicate’s use in the universe
of discourse, and except if the predicate is precise, or deﬁnable by means of ‘if and
only if’ conditions, the speciﬁcation is never unique. Up to some extent it happens
analogously with the connectives, the qualiﬁed predicates and the modiﬁers. All of
them should be speciﬁed accordingly with the contextual information available on
the corresponding problem.
Hence, solving a problem of fuzzy logic requires a correct design of all the terms
to be represented [19, 20], and specially when the problem concerns a dynamical
system. If from a theoretical view, fuzzy logic is a matter of degree, from a practical
point of view it is a matter of design. To wrongly design a single term, could imply
to pose in fuzzy terms a system diﬀerent from the current one and whose solutions
rarely will keep any actual relation with it. What it usually does a designer of fuzzy
systems?
In a ﬁrst place, the designer should work with the information actually available
on the involved terms that, more often than not, can be incomplete and, hence, neither
the total relation ≤P, nor a satisfactory measure 𝜇P will be obtained: The designed
fuzzy set 𝜇P is no more than an approximation to the measure, with an unknown
degree of approximation and giving an also approximate working meaning ≤𝜇P.
In a second place, the designer can be forced to add some working hypotheses
that can produce distortions in a model. For example, and as it is often the case,
if the designer accepts a piecewise linear character for the involved membership
functions, and, for the modiﬁer very accepts that it squares the membership function,
non piecewise linear functions are automatically introduced contrarily to the initial
hypothesis. Another hypothesis that is always accepted without any kind of testing
is that all the rules linguistically describing the behavior of a dynamical system are
represented by the same function J of those in Sect. 6.1.
The practitioner of fuzzy logic should have a good mastering of the design of
systems involving imprecision, and this requires a suﬃcient knowledge of the sev-
eral mathematical models that can be employed. Fuzzy systems should be carefully
designed.
9.4
Many questions still deserve a more deep study, and several are not just of a purely
theoretical character since their possible answers require a previous checking within
language and reasoning before being accepted (even in the provisory form typical of
science). Provided some of these answers come from a mathematical model, can it
be expected to found such model by just purely mathematical thinking?, or, should
it come from abstraction on the information furnished by reality?, that is, from data
captured through controlled processes of observation and experimentation within

640
E. Trillas
language and reasoning. The concept of a fuzzy set, once seen as a state of the col-
lective generated in language by its predicate or linguistic label, seems to be more a
scientiﬁc object than a purely logical one; and scientiﬁc objects are those typically
analyzed by science. Which should be the best working methodology to follow: just
theoretic thinking, like that of pure mathematics often based in second order abstrac-
tions and at once responding to theoretic interests, or mixed experimental and the-
oretic thinking like that of physics, where theoretic models should be tested against
an observed reality?
Is it possible to conduct a mathematical analysis of language and reasoning with-
out previously considering the nuances, dynamism and variability of the natural mat-
ter for such study? And, is not natural language plus common reasoning such natural
matter? Possibly fuzzy logic needs to evolve towards an experimental science of
‘language and reasoning’ with a methodology analogous to that of physics; towards
a kind of physics of language and reasoning, but with a working methodology based
on computer science [24].
Such a scientiﬁc-like modeling of language and reasoning, jointly with that on
how the human brain actually works, is one of the big challenges for science in the
XXI Century, and the starting point for both the continuation of the Leibniz’s ‘Cal-
culemus’, and the real possibility of having machines thinking like people. Such a
new way of studying imprecision and uncertainty in natural languages and common
reasoning, without missing that of ambiguity, is what the author hopes it will come
from the right now ﬁfty years old fuzzy sets. At the end, it is with fuzzy logic that
mathematical analysis actually begun into use for representing language and infer-
ence as people do. Like it happened before with the natural sciences where mathe-
matics shown, in the words of Eugene Wigner, ‘an unreasonable eﬀectiveness’ [21].
To end, let’s say that in the author’s view, Zadeh’s Computing with Words [22,
25], should be theoretically re-modeled by, for instance, departing from the view of
fuzzy sets presented in this paper, and being closely connected with natural language
and common reasoning. It is a nice challenge for young researchers to whom the
author wishes the ability of posing good questions conducting to fertile solutions
like professor Zadeh’s work is full of.
Acknowledgments This paper is partially funded by the Foundation for the Advancement of Soft
Computing (Asturias, Spain), and by the Government of Spain Project MICIIN/TIN 2011-29827-
C02-01.
References
1. Zadeh, L.A.: Fuzzy sets. Inf. Control 8, 338–353 (1965)
2. Trillas, E.: On the genesis of fuzzy sets. Agora 27(1), 7–33 (2008)
3. Trillas, E., García-Honrado, I.: A Layperson reﬂection on sorites. In: Seising, R., Tabacchi,
M.E. (eds.) Fuzziness and Medicine: Philosophical Reﬂections and Application Systems in
Health Care, pp. 217–231 (2013)

Fuzzy Sets: Towards the Scientiﬁc Domestication of Imprecision
641
4. Trillas, E., Moraga, C., Termini, S.: A Naïve Way of Looking at Fuzzy Sets, Forthcoming in
Fuzzy Sets and Systems (2015)
5. Trillas, E.: On a model for the meaning of predicates. In: Seising, R. (ed.) Views on Fuzzy Sets
and Systems from Diﬀerent Perspectives, pp. 175–205, Springer (2009)
6. Trillas, E., Moraga, C., Guadarrama, S., Cubillo, S., Castiñeira, E.: Computing with antonyms.
In: Nikravesh, M., et al., (eds.) Forging New Frontiers, vol. I, pp. 133–154, Springer, New York
(2007)
7. Trillas, E.: Sobre funciones de negación en la teoría de conjuntos difusos (in Spanish). Sto-
chastica III(1), 47–59 (1979)
8. Alsina, C., Frank, J.M., Schweizer, B.: Associative Functions. Triangular Norms and Copulas,
World Scientiﬁc, Singapore (2006)
9. Trillas, E.: A model for ‘Crisp Reasoning’ with fuzzy sets. Int. J. Intell. Syst. 27, 859–872
(2012)
10. Trillas, E., Alsina, C., Renedo, E.: On some schemes of reasoning in fuzzy logic. New Math.
Natural Comput. 7(3), 433–451 (2011)
11. Trillas, E., Alsina, C.: On standard theories of fuzzy sets with the law (𝜇⋅𝜎’)’ = 𝜎+ 𝜇’ ⋅𝜎’.
Int. J. Approximate Reasoning 37(2), 87–92 (2004)
12. Watanabe, S.: Knowing and Guessing. John Wiley & Sons, New York (1969)
13. Mendel, J.: Type-2 Fuzzy Sets and Systems: How to Learn About Them, eNewsletter Systems.
Man and Cybernetics Society, Issue 27 (2009)
14. See [5]
15. Nakama, T., Trillas, E., García-Honrado, I.: Axiomatic Investigation of fuzzy probabilities. In:
Seising, R., Sanz, V. (eds.) Soft Computing in Humanities and Social Sciences, pp. 125–140,
Springer, (2012)
16. Zadeh, L.A.: Probability measures of fuzzy events. J. Math. Anal. Appl. 23(2), 421–427 (1968)
17. Trillas, E.: Some uncertain reﬂections on uncertainty. Arch. Philos. Hist. Soft Comput. 1, 1–16
(2013)
18. Trillas, E.: Short Note: A Scrutiny on Imprecision, ECSC Research Report (available upon
request) (2014)
19. Trillas, E., Guadarrama, S.: Fuzzy representations need a careful design. Int. J. Gen. Syst.
39(3), 329–346 (2010)
20. Trillas, E., Moraga, C.: Reasons for a careful design of fuzzy sets. In: Proceedings IPMU-Milan
(2013)
21. Wigner, E.: The unreasonable eﬀectiveness of mathematics in the natural sciences. Commun.
Pure Appl. Math. 13/1 (1960)
22. Zadeh, L.A.: Computing with Words: Principal Concepts and Ideas. Springer, New York
(2012)
23. Thompson (Lord Kelvin), W.: Lecture on Electrical Units of Measurement, Popular Lectures
and Addresses, McMillan, London, vol. 1:73 (1889)
24. Mamdani, E.H., Trillas, E.: Correspondence between an experimentalist and a theoretician.
In: Trillas, E., et al. (ed.) Combining Experimentation and Theory, Springer, Berlin, pp. 1–18
(2012)
25. Mendel, J., Zadeh, L.A., Trillas, E., Yager, R., Lawry, J., Hagras, H., Guadarrama, S.: What
computing with words means to me (discussion forum). In: Computational Intelligence Mag-
azine IEEE, pp. 20–26 (2010)
26. Trillas, E., Pradera, A.: Non-functional fuzzy connectives: the case of negations. In: Proceed-
ings XI-ESTYLF (León), pp. 21–27 (2002)
27. Black, M.: Vagueness: an exercise in logical analysis. Philos. Sci. 4(4), 427–455 (1937)
28. Black, M.: Reasoning with loose concepts. Dialogue 2, 1–12 (1963)
29. Zadeh, L.A.: The concept of a linguistic variable and its application to approximate reasoning.
Inf. Sci. 8, 199–249 (1975)
30. Trillas, E.: On the words probable and improbable. Proc. IPMU (Madrid) II, 780–783 (2000)
31. Wittgenstein, L.: Philosophical Investigations. Basil Blackwell, Oxford (1958)
32. Goguen, J.A.: L-fuzzy sets. J. Math. Anal. Appl. 10, 145–174 (1967)

642
E. Trillas
Author Biography
Enric Trillas Ph.D. in Mathematics by the University of
Barcelona, got a chair in Mathematics at the Technical Uni-
versity of Catalonia in 1974, where he served as a full profes-
sor up to 1988 when he moved to the Department of Artiﬁ-
cial Intelligence at the Technical University of Madrid. From
2006 is an Emeritus Researcher at the European Centre for
Soft Computing, Mieres (Asturias) Spain. Formerly working
in Ordered Semigroups and Generalized Metrics, from 1976
is active in Fuzzy Logic, having received several international
distinctions and honors, as well as an H.C. Doctorate by the
Public University of Navarre, and published over 400 papers in
National/International journals, conferences and edited books.
His work in Fuzzy Logic began by studying fuzzy entropies
but, early in 1979, shifted to non-distributive logical connec-
tives, like it were the characterization of strong negations, the
introduction of t-norms and t-conorms, the ﬁrst analysis of the ‘Modus Ponens’ inequality, and
the introduction of T-Indistinguishabilities. After 1999 he mainly works in the analysis of con-
jecturing, representing the meaning of words, and the linguistic roots of fuzzy sets. Between
1983 and 1996 served in the Spanish Government, and holds several decorations from Spain
and other countries.

Type 1 and Full Type 2 Fuzzy System
Models
I. Burhan Türkşen
Abstract We ﬁrst present a brief review of the essentials fuzzy system models:
Namely (1) Zadeh’s rulebase model, (2) Takagi and Sugeno’s model which is partly
a rule base and partly a regression function and (3) Türkşen fuzzy regression
functions where a fuzzy regression function correspond to each fuzzy rule. Next we
review the well known FCM algorithm which lets one to extract Type 1 mem-
bership values from a given data set for the development of Type 1 fuzzy system
models as a foundation for the development of Full Type 2 fuzzy system models.
For this purpose, we provide an algorithm which lets one to generate Full Type 2
membership value distributions for a development of second order fuzzy system
models with our proposed second order data analysis. If required one can generate
Full Type 3,…, Full Type n fuzzy system models with an iterative execution of our
algorithm. We present our application results graphically for TD_Stockprice data
with respect to two validity indeces, namely: (1) Çelikyılmaz-Türkşen and
(2) Bezdek indeces.
1
Fuzzy System Models
Here ﬁrst, historically signiﬁcant fuzzy system model developments are reviewed in
order to identify their unique structures and to point out how they differ from each
other. Then we show the details of our FULL TYPE 2 Fuzzy System developments
with a new algorithm.
I. Burhan Türkşen (✉)
Department of Industrial Engineering, TOBB-Economy and Technology University,
Sögütözü Cad. no:43, 06560 Sögütözü, Ankara/Turkey
e-mail: bturksen@etu.edu.tr; turksen@mie.utoronto.ca
URL: http://www.mie.utoronto.ca/staff/proﬁles/turksen.html
I. Burhan Türkşen
Director-Knowledge/Intelligence Systems Laboratory, Department of Mechanical &
Industrial Engineering, University of Toronto, M5S 3G8 Toronto, ON, Canada
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_30
643

2
Type 1 Fuzzy Rule Base Models
The most commonly applied fuzzy system models are fuzzy rule bases. Here, we
only deal with Multi-Input Single Output (MISO) systems. Generally fuzzy system
models represent relationships between the input and output variables which are
expressed as a collection of IF-THEN rules that utilize linguistic labels, which are
represented with fuzzy sets. The general fuzzy rule base structure which is known
as Zadeh- Fuzzy Rule Base, Z-FRB, can be written as follows:
R: ALSO
c*
i = 1
IF antecedentiTHENconsequenti
ð
Þ,
where c* is the number of rules in a rule base either given by experts or it is
determined by a fuzzy clustering algorithm such as FCM, Fuzzy-C-Means (Bezdek
[1]) or IFC, Improved Fuzzy Clustering (Çelikyılmaz and Türkşen [2]). The fuzzy
rule base structures determined by various alternatives mainly differ in the repre-
sentation of the consequents. If the consequent is represented with fuzzy sets then
the fuzzy rule base is known as Zadeh [13] version which is originally applied by
Mamdani, et al., [3], and a modiﬁed version is proposed by Sugeno and Yasukawa,
SY-FRB, [6]. Whereas, if the consequents are represented with linear equations of
input variables, then the rule base structure is the Takagi-Sugeno Fuzzy Rule Base,
TS-FRB, [5] structure. These are the main models amongst others which we do not
review in this paper. In particular Zadeh Fuzzy Rule Bases, Z-FRB can be for-
malized as: R: ALSO
c*
i = 1
IF x ∈X isr Ai THEN y ∈Y isr Bi
ð
Þ
In general, let nv be the number of selected input variables in the system. Then,
the multidimensional antecedent, x, can be deﬁned as x = (x1,x2,…,xnv), where xj is
the jth input variable of the antecedent and the domain of x in X, can be deﬁned as
X = X1 × X1 × … × Xnv, Xj ⊆ℜ.
In particular, the Z-FRB structure can be expressed as follow, where the
multi-dimensional antecedent fuzzy subset of ith rule is Ai. This multi-dimensional
antecedent fuzzy subset determination eliminates the search for the appropriate
t-norm for the combination of antecedent fuzzy subsets with “AND”.
Thus, variations of Z-FRB are Sugeno-Yasukawa, SY-FRB, and Takagi-Sugeno
(TS-FRB) Fuzzy Rule Base structures:
SY −FRB
ð
Þ
R: ALSO
c*
i = 1
IF x ∈X isr Ai THEN y ∈Y isr Bi
ð
Þ
TS −FRB
ð
Þ
R: ALSO
c*
i = 1
ðIF antecedenti THEN yi = aixT + biÞ
where, antecedenti = x ∈X isr Ai, and ai = (ai,1,…, ai,NV) is the regression coef-
ﬁcient vector associated with the ith rule together with bi which is the scalar
associated with the ith rule. For these special cases of Z-FRB, again each degree of
644
I. Burhan Türkşen

ﬁring, di, associated with the-ith rule, is determined directly from the corresponding
ith multi-dimensional antecedent fuzzy subset Ai and applied to the consequent
fuzzy subset for the SY-FRB or to the classical ordinary regression for the case of
TS-FRB.
3
Fuzzy Regression Functions
There are a number of variations of the proposed Fuzzy Regression Functions. We
discuss here only one alternative in this paper, namely, Fuzzy Regression Functions
which we have proposed with LSE.
3.1 Fuzzy Regression Functions with LSE (FF-LSE)
In ordinary LSE (Least Square Estimation) method, the dependent variable, y, is
assumed to be a linear function of input, variables, x, plus an error component:
y = β0 + β1x1 + . . . + βnvxnv + ϵ
where y is the dependent output, xj’s are the explanatory variables input, for j = 1,
…, nv, nv is the number of selected inputs and ε is the independent error term which
is typically assumed to be normally distributed. The goal of the least squares
method is to obtain estimates of the unknown parameters, βj’s, j = 0,1,…, nv, which
indicate how a change in one of the independent variables affects the dependent
variable.
β = XTX

 −1XTy
The proposed generalization of LSE as FF-LSE (Fuzzy Functions with LSE,
more appropriately know as Fuzzy Regression Functions with LSE), requires
that a fuzzy clustering algorithm, such as FCM, or IFC be available to determine the
interactive (joint) membership values of input-output variables in each of the fuzzy
clusters that can be identiﬁed for a given training data set. Let (Xk,Yk), k = 1,…, nd,
be the set of observations in a training data set, such that Xk = (xjk | j = 1,…, nv).
First, one determines the optimal (m*, c*) pair for a particular performance mea-
sure, i.e., a cluster validity indeces such as Bezdek […], and Celikyılmaz and
Türkşen […] with an iterative search and an application of FCM or IFC algorithm,
where m is the level of fuzziness (in our experiments we usually take m = 1.4,
…,2.5), Ozkan and Turksen […]) and c is the number of clusters (in our experi-
ments we usually take c = 2,…,10). The well known FCM (Bezdek 1973) algo-
rithm can be stated as follows:
Type 1 and Full Type 2 Fuzzy System Models
645

min
JðU, VÞ = ∑
nd
k = 1
∑
c
i = 1
ðuikÞmð xk −vi
k
kÞA
s. t.
0 ≤uik ≤1, ∀i, k
∑
c
i = 1
uik = 1, ∀k
0 ≤∑
nd
k = 1
uik ≤nd, ∀i
,
where J is objective function to be minimized, ||.||A is a norm that speciﬁes a
distance based similarity between the data vector xk and a fuzzy cluster center vi. In
particular, A = I is the Euclidian Norm and A = C−1 is the Mahalonobis Norm, etc.
Once the optimal pair (m*, c*) is determined with the application of FCM
algorithm, and a cluster validity index, one next identiﬁes the
cluster centers for m = m* and c = 1,…,c* as:
vXjY, j
m*
= ðxc
1, j, xc
2, j, ⋯, xc
nv, j, yc
jÞ
From this, we identify the cluster centers of the input space again for m = m*
and c = 1,…,c* as: vX, j
m*
= ðxc
1, j, xc
2, j, ⋯, xc
nv, jÞ.
Next, one computes the normalized membership values of each vector of
observations in the training data set with the use of the cluster center values
determined in the previous step. There are generally two steps in this calculations:
First we determine the (local) optimum membership values uik‘s and then
determine µik ‘s that are above an α- cut in order to eliminate harmonics generated
by FCM as:
uik =
∑
c
j = 1
xk −vX, i
k
k
xk −vX, j


 
!
2
m −1
0
@
1
A
−1
, μik ≥α,
where µik denotes the membership value of the kth vector, k = 1,…,nd, in the ith rule,
i = 1,…,c* and xk denotes the kth vector and for all the input variables j = 1,…, nv,
in the input space. (2) Next, we normalize them as:
γijðxjÞ =
μijðxjÞ
∑
c
i0 = 1
μi0jðxjÞ
where γij is the normalized membership value of xj, j = 1,…, nv, in the ith rule,
i = 1,…,c*, which in turn will indicate the membership value that will constitute an
new input variable in our proposed scheme of function identiﬁcation for the
646
I. Burhan Türkşen

representation of ith cluster. Let Γi = ðγijji = 1, . . . , c*; j = 1, . . . , nvÞ be the mem-
bership values of X in the ith cluster, i.e., rule.
Next we determine a new augmented input matrix X for each of the clusters
which could take on several forms depending on which transformations of mem-
bership values we want to or need to include in our system structure identiﬁcation
for our intended system analyses. Examples of these are:
X′
i = 1, Γi, X
½
,
X′0
i = 1, Γ2
i , X


,
X′′′
i = 1, Γ2
i , Γm
i , expðΓiÞ, X


,
etc., where Xi′, Xi″, Xi″′ are the new input matrices to be used in least squares
estimation of a new system structure identiﬁcation where
Γi = ðγijji = 1, . . . , c*; j = 1, . . . , nvÞ.
The choice depends on whether we want to or need to include just the mem-
bership values or some of their transformations as new input variables in order to
obtain a best representation of a system behavior. In particular, this is done in order
to get a higher value of R2 to show that a better model is obtained for an appli-
cation. A new augmented input matrix, say Xi′, would look as shown below for the
special case of X = Xj, i.e., the matrix X is just a vector of a single variable, Xj = (xjk
|k = 1,…,nd) for the jth variable:
X′
ij = ½1, Γi, Xij =
1
γi1
xij1
⋮
⋮
⋮
1
γind
xijnd
2
4
3
5
Thus the fuzzy regression function, Yi = βi0 + βi1Γi + βi2Xij, that represents the ith
rule corresponding to the ith interactive (joint) cluster in space ðYi, Γi, XjÞ,
β*
i = ðXij′TX′
ijÞ −1ðXij′TYiÞ,
X′
ij = 1, Γi, Xij


.
Such that β*
i = ðβ*
i0, β*
i1, β*
i2Þ and the estimate of Yi would be obtained as
Y*
i = β*
i0 + β*
i1Γi + β*
i2Xij.
Within the proposed framework, the general form of the shape of a cluster can be
conceptually captured by a second order (cone) in the space of U × X × Y which
can be illustrated with a prototype shown in Fig. 1.
One usually determines Type 1 membership values with an application of FCM
[…] algorithm shown below:
Type 1 and Full Type 2 Fuzzy System Models
647

where Eq. 1 stated in the algorithm above is:
μðtÞ
ik =
∑
c
j = 1
d xk, υðt −1Þ
i


d xk, υðt −1Þ
j


0
@
1
A
2
m −1
2
64
3
75
−1
And Eq. 2 is:
γji
•
•
xji
0.0
y
Fig. 1 A Fuzzy cluster in U
× X × Y space
648
I. Burhan Türkşen

υðtÞ
i =
∑
n
k = 1
μðtÞ
ik

m
xk
	


∑
n
k = 1
μðtÞ
ik

m
, ∀i = 1, . . . , c
4
Generation of Full Type 2 Membership Values
For this purpose, we propose and hence introduce an new algorithm in order to
generate Full Type 2 membership value distribution from the results obtained with
an application of FCM which produce a Type 1 membership value distribution for
our studies of Full Type 2 investigations.
5
Full Type 2 Fuzziness i.e., Membership of Membership
Here we want to show how one determines the second order degree of fuzziness in
order to develop Full Type 2 fuzzy system models.
It should be noted that depending on where x ∈X is there may be more than one
second order membership value distribution.
6
Full Type 2 Fuzzy Set Extraction Algorithms
We propose the following Full Type 2 fuzzy set extraction algorithm from a given
data set called FT2FCM (Türkşen, 2012):
Full Type 2 Fuzzy Clustering Algorithm
Min j′ðU′ðUÞ, WÞ =
= ∑
nd
k = 1
∑
c′
i = 1
∑
1
l = 0
μμi xk
ð
Þ Z
ð Þ


μμi xk
ð
Þ Zl
ð Þ −μ̄ xk
ð
Þ Zl
ð Þ A
k



, k = 1, . . . , nd;
i = 1, . . . , c′
st. 0 ≤μμi xk
ð
ÞðZÞ ≤1
0 ≤μi xk
ð
Þ ≤1
0 ≤∑
nd
k = 1
μi xk
ð
Þ ≤nd
Type 1 and Full Type 2 Fuzzy System Models
649

μi xk
ð
Þ ϶ 0, 1
½
; μμiðxkÞðZÞ ϶ 0, 1
½
; l ϶ 0, 1
½

where j′ is the objective function to be minimized for a given xk ∈X, .k kA is a
norm, i.e., Euclidian or Mahalanobis, that speciﬁes a distance measure based on a
membership values for a given xk ∈X and its second order fuzzy cluster center
μ̄i xk
ð
Þ.
Next one computes the normalized membership values of these Full Type 2
membership values for each vector of membership values obtained in an initial
application of the original FCM or IFC algorithm in the ﬁrst stage.
There are generally two steps in these calculations:
We ﬁrst determine (local) optimum membership of membership values μμi xk
ð
Þ’s
and then apply an α-cut in order to eliminate the second order harmonics generated
by an application of FT2FCM as:
μμi xk
ð
Þ
xk ∈X =
∑
c′
i = 1
μμi xk
ð
Þ −μ̄i xk
ð
Þ


μμi xk
ð
Þ −μj xk
ð
Þ


 
!
2
m −1
2
4
3
5
−1
γ′
μμiðxkÞ = μμiðxkÞ xk ∈X
j
∑c′
i = 1 μμiðxkÞ
, μμiðxkÞ ≥α, γ′
μμiðxkÞ ≥α
where γ′
μμiðxkÞ denotes the membership values of the membership values of the kth
vector k = 1,..,nd in the ith rule, or ith fuzzy regression function (Türkşen, 2012)
and xk ∈X denotes the kth vector and for all the input variables, k = 1,.., nd in the
input space.
Recall that we are able to obtain the membership value distribution as:
X′
ij = ½1, Γi, Xij =
1
γi1
xi1
⋮
⋮
⋮
1
γind
xind
2
4
3
5
Γi = ðγikji = 1, . . . , c*; k = 1, . . . , ndÞ
Γi = γijji = 1, . . . , c*; j = 1, . . . , nd


Γi =
γ11
γ21
⋯γc*1
⋮
⋮
⋮
γ1nd
γ2nd
⋯γc*nd
2
4
3
5
We process each γi via our Full Type 2 clustering algorithm given above, called
FT2FCM, to determine Full Type 2 distribution for each cluster i, Γi = γijji = 1,

. . . , c*; j = 1, . . . , ndÞ.
Thus we apply to each Γi, ALGORITHM 2 given below to generate Full Type
2 membership, values, i.e., membership of membership.
650
I. Burhan Türkşen

7
Experimental Results
We present here our experimental results for TD_Stock Price Data set that is
available for all researchers on the internet.
Çelikyılmaz-Türkşen’s validity index results for TD_Stockprice data:
2
3
4
5
6
7
8
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
c
CVIFF
m=1.4
m=1.8
m=2.6
Type 1 and Full Type 2 Fuzzy System Models
651

Fuzzy classiﬁcation of TD_Stockprice data: (c* = 2,m* = 1.8)
Cluster-2 view for TD_Stockprice data:
-30
-20
-10
0
10
20
30
40
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
y
muik
cluster2
Çelikyılmaz-Türkşen’s Validity Index for µik data:
-30
-20
-10
0
10
20
30
40
0
0.2
0.4
0.6
0.8
1
y
muik
cluster2
cluster1
652
I. Burhan Türkşen

Cluster-2 results of TD-Stockprice data (c* = 2,m* = 1.8)
According Çelikyılmaz-Türkşen index, the suitable number of cluster
should be chosen as c’ = 2 (µik data is the membership values of ﬁrst study’s
cluster-2). Where c’ = 2, m’ = 1.8.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
muik
mulk (muik)
cluster2
cluster1
2
3
4
5
6
7
8
0
5
10
15
20
25
c
CVIFF
m=1.4
m=1.8
m=2.6
µlk(µik) for Cluster1 and 2 are shown above:
Type 1 and Full Type 2 Fuzzy System Models
653

A possible three cluster view:
TD_Stockprice Data set:
According to Bezdek’s validity index results (shown as follows), the suitable
number of cluster was chosen as c* = 3:
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
muik
mulk (muik)
2
3
4
5
6
7
8
0.1
0.15
0.2
0.25
0.3
0.35
c
Bezdek's validity index
m=1.4
m=1.8
654
I. Burhan Türkşen

Fuzzy classiﬁcation of TD_Stockprice data: (c* = 3,m* = 2.0)
-30
-20
-10
0
10
20
30
40
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
y
muik
cluster2
cluster1
cluster3
Cluster-2 view for TD_Stockprice data:
-30
-20
-10
0
10
20
30
40
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
y
muik
cluster2
Çelikyılmaz-Türkşen’s Validity Index for µik data:
Type 1 and Full Type 2 Fuzzy System Models
655

Cluster-2 results of TD-Stockprice data (c* = 3,m* = 2.0) for membership
of membership.
According Çelikyılmaz-Türkşen index, the suitable number of cluster
should be chosen as c’ = 2 (the µik data is the membership values of ﬁrst
study’s cluster-2). Where c’ = 2,m’ = 2.0.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
muik
mu'lk (muik)
cluster2
cluster1
2
3
4
5
6
7
8
0
2
4
6
8
10
12
14
16
c
CVIFF
m=1.4
m=1.8
m=2.6
656
I. Burhan Türkşen

A possible three cluster view:
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
muik
mu'lk (muik)
8
Conclusions
In this paper, we have ﬁrst review the essentials fuzzy system models: such as
(1) Zadeh’s rulebase model, (2) Takagi and Sugeno’s partly a rule base and partly a
regression function model and (3) Türkşen’s “Fuzzy Regression Functions” model
where a fuzzy regression function correspond to each fuzzy rule and thus a fuzzy
rule base is replaced with “Fuzzy Regression Functions” model. Next we review the
well known FCM algorithm which lets one to extract Type 1 membership values
from a given data set for the development of “Type 1” fuzzy system models as a
foundation for the development of “Full Type 2” fuzzy system models. For this
purpose, we provide an algorithm which lets one to generate Full Type 2 mem-
bership value distributions for a development of second order fuzzy system models
with our proposed second order data analysis. If required one can generate Full
Type 3,…, Full Type n fuzzy system models with an iterative execution of our
algorithm. Finally we present our results graphically for TD_Stockprice data with
respect to two validity indeces, namely: (1) Çelikyılmaz-Türkşen and (2) Bezdek
indeces. Based on our development, we expect in the future new results would be
obtained in “Full Type 3,…, Full Type n” fuzzy system model analyses.
Type 1 and Full Type 2 Fuzzy System Models
657

References
1. Bezdek, J.C.: Pattern recognition with fuzzy objective function algorithms. Plenum Press, New
York, 256 pp (1981)
2. Çelikyılmaz, A., Türkşen, I.B.: Validation criteria for enhanced fuzzy clustering. Pattern
Recogn. Lett. 29(2), 97–108 (2008)
3. Mamdani, E.M.: Application of fuzzy logic to approximate reasoning using linguistic systems.
Trans. Comput. 26,1182–1191 (1977)
4. Ozkan, I., Türkşen, I.B.: Upper and lower values for the level of fuzziness in FCM. Inf. Sci.
177(23), 5143–5152 (2007)
5. Takagi, T., Sugeno, M.: Fuzzy identiﬁcation of systems and its applications to modeling and
control. IEEE Trans. Syst. Man Cybern. SMC-15(1), 116–132 (1985)
6. Sugeno, M., Yasukawa, T.: A fuzzy logic based approach to qualitative modelling. IEEE
Trans. Fuzzy Syst. 1(1), 7–31 (1993)
7. Türkşen, I.B.: Non-speciﬁcity and interval valued fuzzy sets. Fuzzy Sets Syst. 80, 87–100
(1996)
8. Türkşen, I.B.: Type 2 Representation and Reasoning for CWW. Fuzzy Sets Syst. 127, 17–36
(2002)
9. Türkşen, I.B.: An Ontological and Epistemological Perspective of Fuzzy Set Theory.
Elsevier B.V., New York (2006)
10. Türkşen, I.B.: Meta-linguistic axioms as a foundation for computing with words. Inf. Sci. 177
(2), 332–359 (2007)
11. Türkşen, I.B.: Fuzzy functions with LSE. Int. J. Appl. Soft Comput. 8(3), 78–88 (2008)
12. Zadeh, L.A.: Fuzzy sets. Inf. Control 8, 338–353 (1965)
13. Zadeh, L.A.: The concept of a linguistic variable and its application to approximate
reasoning-I, II, III. Inf. Sci. 8, 199–245, 301–357, 9, 43–80 (1975)
Author Biography
Dr. I.B. Turksen is a Professor Emeritus of Industrial
Engineering in the Department of Mechanical & Industrial
Engineering at the University of Toronto. He received the B.S.
and M.S. degrees in Industrial Engineering and the Ph.D. degree
in Systems Management and Operations Research all from the
University of Pittsburgh, PA. He joined the Faculty of Applied
Science and Engineering at the University of Toronto and
became Full Professor in 1983. In 1984–1985 academic year, he
was a Visiting Professor at the Middle East Technical University
and Osaka Prefecture University. Since 1987, he has been
Director of the Knowledge / Intelligence Systems Laboratory.
During the 1991–1992 academic year, he was a Visiting
Research Professor at LIFE, Laboratory for International Fuzzy
Engineering, and the Chair of Fuzzy Theory at Tokyo Institute of
Technology. During 1996 academic year, he was Visiting
Research Professor at the University of South Florida, USA, and Bilkent University, Ankara,
Turkey. Since December 2005, he is appointed as the Head of Department of Industrial
Engineering at TOBB Economics and Technology University.
He was and/or is a member of the Editorial Boards of the following publications: Fuzzy Sets and
Systems, Approximate Reasoning, Decision Support Systems, Information Sciences, Fuzzy
658
I. Burhan Türkşen

Economic Review, Expert Systems and its Applications, Journal of Advanced Computational
Intelligence, Information Technology Management, Transactions on Operational Research, Fuzzy
Logic Reports and Letters, Encyclopedia of Computer Science and Technology, Failures and
Lessons Learned in Information Technology, Applied Soft Computing. He is the co-editor of
NATO-ASI Proceedings on Soft Computing and Computational Intelligence, and Editor of
NATO-ASI Proceedings on Computer Integrated Manufacturing as well co-editor of two special
issues of Robotics and Autonomous Systems.
He is a Fellow of IFSA and IEEE, and a member of IIE, CSIE, CORS, IFSA, NAFIPS, APEO,
APET, TORS, ACM, etc.
Dr. Turksen is the founding President of CSIE. He was Vice-President of IIE, General
Conference Chairman for IIE International Conference, and for NAFIPS in 1990. He served as
Co-Chairman of IFES’91 and Regional Chairman of World Congress on Expert Systems,
WCES’91, WCES’94, WCES’96 and WCES’98, Director of NATO-ASI’87 on Computer
Integrated Manufacturing and Co-Director of NATO-ASI’96 on Soft Computing and Computa-
tional Intelligence. He was General Conference Chairman for Intelligent Manufacturing Systems,
IMS’1998, IMS’2001, IMS’2003. He was the President of IFSA during 1997–2001 and Past
President of IFSA, International Fuzzy Systems Association during 2001–2003. Currently, he is
the President, CEO and CSO, of IIC, Information Intelligence Corporation.
He received the outstanding paper award from NAFIPS in 1986, “L.A. Zadeh Best Paper
Award” from Fuzzy Theory and Technology in 1995, “Science Award” from Middle East
Technical University, and an “Honorary Doctorate” from Sakarya University. He is a Foreign
Member, Academy of Modern Sciences.
His current research interests centre on the foundations of fuzzy sets and logics, measurement of
membership functions with experts, extraction of membership functions with fuzzy clustering and
fuzzy system modeling. His contributions include, in particular, Type 2 fuzzy knowledge
representation and reasoning, fuzzy truth tables, fuzzy normal forms, T-formalism which is a
modiﬁed and restricted Dempster’s multi-valued mapping, and system modeling applications for
intelligent manufacturing and processes, as well as for management decision support and
intelligent control.
He has published near 300 papers in scientiﬁc journals and conference proceedings.
His book entitled “An Ontological and Epistemological Perspective of Fuzzy Theory” was
published by Elsevier, The Netherlands, in January, 2006. (ISBN-10: 0-444-51891-6, ISBN-13:
978-0-444-51891-0).
Type 1 and Full Type 2 Fuzzy System Models
659

Complex Fuzzy Sets and Complex Fuzzy
Logic an Overview of Theory
and Applications
Dan E. Tamir, Naphtali D. Rishe and Abraham Kandel
Abstract Fuzzy Logic, introduced by Zadeh along with his introduction of fuzzy
sets, is a continuous multi-valued logic system. Hence, it is a generalization of the
classical logic and the classical discrete multi-valued logic (e.g. Łukasiewicz’
three/many-valued logic). Throughout the years Zadeh and other researches have
introduced extensions to the theory of fuzzy setts and fuzzy logic. Notable exten-
sions include linguistic variables, type-2 fuzzy sets, complex fuzzy numbers, and
Z-numbers. Another important extension to the theory, namely the concepts of
complex fuzzy logic and complex fuzzy sets, has been investigated by Kandel et al.
This extension provides the basis for control and inference systems relating to
complex phenomena that cannot be readily formalized via type-1 or type-2 fuzzy
sets. Hence, in recent years, several researchers have used the new formalism, often
in the context of hybrid neuro-fuzzy systems, to develop advanced complex fuzzy
logic-based inference applications. In this chapter we reintroduce the concept of
complex fuzzy sets and complex fuzzy logic and survey the current state of com-
plex fuzzy logic, complex fuzzy sets theory, and related applications.
Keywords Fuzzy set theory ⋅Fuzzy class theory ⋅Fuzzy logic ⋅Complex
fuzzy sets ⋅Complex fuzzy classes ⋅Complex fuzzy logic ⋅Neuro-fuzzy
systems
D.E. Tamir (✉)
Department of Computer Science, Texas State University San Marcos, Texas, USA
e-mail: dt19@txstate.edu
N.D. Rishe ⋅A. Kandel
School of Computing and Information Sciences, Florida International University,
Miami, USA
e-mail: rishe@cs.ﬁu.edu
A. Kandel
e-mail: Abraham.Kandel.FIU@gmail.com
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1_31
661

1
Introduction
The development of computers and the related attempt to automate human rea-
soning and inference have posed a challenge to researchers. Humans, and in many
cases machines, are not always operating under strict and well deﬁned two-valued
logic or discrete multi-valued logic. Their perception of sets and classes is not as
crisp as implied by the traditional set and class theory. To capture this perception,
L. A. Zadeh has introduced the theory of fuzzy sets and fuzzy logic [1–7]. The
seminal paper [1],1 published by Zadeh in 1965, ignited tremendous interest among
a large number of researchers. Following the introduction of the concepts of fuzzy
logic and set theory, several researchers, [8–10], have established an axiomatic
framework for these concepts.
The ﬁve decades that followed Zadhe’s pioneering work have produced exten-
sive research work and applications related to control theory [11, 12], artiﬁcial
intelligence [7, 13–15], inference, and reasoning [16, 17]. In recent years, fuzzy
logic has been applied in many areas, including fuzzy neural networks [18],
neuro-fuzzy systems and other bio-inspired fuzzy systems [19], clustering [20–22],
data mining [13, 23, 24], and software testing [25, 26]. In 1975 Zadeh introduced
the concept of linguistic variable and the induced concept of type-2 (type-n) fuzzy
sets [3, 27–30]. Other notable extensions to the theory of fuzzy sets and fuzzy logic
include complex fuzzy numbers [31], and Z-numbers [32].
Many natural phenomena are complex and cannot be modelled using
one-dimensional classes and/or one-dimensional variables. For example, in pattern
recognition, objects can be represented by a set of measurements and are regarded
as vectors in a multidimensional space. Often, it is not practical to assume that this
multidimensional information can be represented via a simple combination of
variables and operators on one-dimensional clauses. Speciﬁcally, consider a set of
values where each value is a member of a fuzzy set. This set, referred to as fuzzy set
of type-2, cannot be compactly represented by basic operations on fuzzy sets of
type-1 [3, 27–30]. This type of sets however, can be represented via complex
classes presented next.
Another important extension to the theory of fuzzy logic and fuzzy sets, namely
complex fuzzy logic (CFL) and complex fuzzy sets (CFS), has been developed by
Kandel and his coauthors [10, 33–36]. Moses et al. introduced an aggregation of
two fuzzy sets into one complex fuzzy set [33]. Next, Ramot et al. introduced the
concept of a complex degree of membership represented in polar coordinates,
where the amplitude is the degree of membership of an object in a CFS and the role
of the phase is to add information which is generally related to spatial or temporal
periodicity in the speciﬁc fuzzy set deﬁned by the amplitude component. They used
this formalism along with the theory of relations to establish the concept of CFL.
Finally, Tamir et al. developed an axiomatically-based CFL system and used CFL
1The ﬁrst documented reference by Zadeh to the concepts of Fuzzy Mathematics appeared in a
1962 paper.
662
D.E. Tamir et al.

to provide a new and general formalism of CFS. These formalisms signiﬁcantly
enhance the expressive power of type-1 and type-2 fuzzy sets [30, 37]. The suc-
cessive deﬁnitions of the theory of CFL and CFS represent an evolution from a
relatively naïve and restricted practice to a sound, well founded, practical, and
axiomatically-based form. In recent years, several researchers have used the new
formalism, often in the context of hybrid neuro-fuzzy systems to develop advanced
complex fuzzy logic-based inference applications.
There is a substantial difference between the deﬁnitions of complex fuzzy
numbers given by J. Buckley [31, 38–41] and the concept of complex fuzzy sets or
complex fuzzy logic. Buckley is concerned with generalizing the number theory
while the CFL and CFS theories are concerned with the generalization of fuzzy set
theory and fuzzy logic [10, 42, 43]. Complex fuzzy numbers have been utilized in
several numerical applications [44–46]. Yet, the concept of a complex fuzzy
number is different from the concept of complex fuzzy sets or complex fuzzy
classes. Recently, Zadeh introduced the concept of Z-numbers. A Z-number,
Z = ðA, BÞ, is an ordered pair of two fuzzy numbers. In this context A, provides a
restriction on a real-valued variable X and B is a restriction on the degree of
certainty that X is A [32]. Nevertheless, this concept is used to qualify the reliability
of fuzzy quantities rather than to deﬁne complex fuzzy sets [10, 36].
The present chapter includes an introduction to the succession of deﬁnitions of
CFL and CFS, concentrating on the axiomatic-based approach. In addition, the
chapter includes a survey the current state of research into complex fuzzy logic,
complex fuzzy set theory, and related applications.
The rest of the chapter is organized in the following way: Sect. 2 introduces the
axiomatic-based theory of fuzzy set and fuzzy logic. Section 3 surveys the theory of
complex
fuzzy
logic
and
complex
fuzzy
sets,
concentrating
on
the
axiomatically-based formulation of the theories. Section 4 includes a survey of
recent developments in the theory and applications of CFL and CFS. Finally,
Sect. 5 presents conclusions and directions for further research.
2
Fuzzy Logic and Set Theory
In 1965, L.A. Zadeh introduced the theory of fuzzy sets, where the degree of
membership of an item in a set can get any value in the interval [0, 1] rather than the
two values f∉, ∈g [1]. Additionally, he introduced the notion of fuzzy logic [1–4].
Fuzzy logic is a continuous (analog) multi-valued extension of classical logic where
propositions can get truth values in the interval [0, 1], and are not limited to one of
the two values {True, False} (or {0, 1}) [17]. These concepts can be considered as
an extension of the multi-valued logic proposed by Łukasiewicz [47]. The intro-
duction of the concepts of fuzzy sets and fuzzy logic was followed by extensive
research into fuzzy systems and their applications, related theories, and extensions
of the concept [1–4, 6, 13, 17, 19, 22, 26, 48–53]. One direction of research has
Complex Fuzzy Sets and Complex Fuzzy Logic …
663

concentrated on the formulation of an axiomatically-based foundation of fuzzy sets
and fuzzy logic [8–10, 54–63]. This is described next.
2.1 Axiomatic Fuzzy Logic
Several researchers presented an axiomatically-based formulation of fuzzy logic and
fuzzy set theory [8–10, 55, 56, 58]. In this section we brieﬂy review an axiomatic
framework that is founded on the basic fuzzy propositional and predicate logic
(BL), along with the fuzzy Łukasiewicz (Ł) and fuzzy product (Π) logical systems
[8–10, 55, 56, 58]. We refer to the propositional logic system as ŁΠ and to the ﬁrst
order predicate fuzzy logic system as ŁΠ∀.
Propositional Fuzzy Logic
Several axiom-based logical systems have been investigated [8–10, 55, 56, 58].
Běhounek et al. ([8]) use the ŁΠ/ ŁΠ∀as the basis for the deﬁnition of fuzzy class
theory (FCT). Our deﬁnition of complex propositional logic presented in Sect. 3
[10, 36], closely follows ŁΠ, the system used by Běhounek et al. For clarity, we
reintroduce some of the important notions, notations, and concepts from that paper.
A fuzzy proposition P can get any truth value in the real interval [0, 1], where ‘0’
denotes “False,” and ‘1’ denotes “True”. Furthermore, the relation ≤, over the
interval ½0, 1 implies a monotonically increasing ordering on the truth values
associated with the proposition. A fuzzy interpretation of a proposition P is an
assignment of a fuzzy truth value to P. Let P, Q and R denote fuzzy propositions
and let iðRÞ denote the fuzzy interpretation of R. Table 1, includes the basic con-
nectives of ŁΠ. Table 2 includes connectives that can be derived from the basic
connectives. The constant 0 is assumed and the constant 1 can be derived from 0
and the basic connectives.
Table 1 Basic ŁΠ
connectives
Operation
Interpretation
Ł-Implication
iðP →LQÞ = minð1, 1 −iðPÞ + iðQÞÞ
Π-Implication
iðP →∏QÞ = minð1, iðPÞ=iðQÞÞ
Π-Conjunction
iðP⊗QÞ = iðPÞ ⋅iðQÞ
Table 2 Derived ŁΠ
connectives
Operation
Interpretation
Ł-Negation
ið′PÞ = 1 −iðPÞ
Π-Delta
ΔðiðPÞÞ = 1 if iðPÞ = 1 else ΔðiðPÞÞ = 0
Equivalence
iðP ↔QÞ = iðP →LQÞ⊗iðQ →LPÞ
P ⊖Q
iðP ⊖QÞ = maxð0, iðPÞ −iðQÞÞ
664
D.E. Tamir et al.

Běhounek et al. use the basic and derived connectives along the truth constants
and the following set of axioms [8]:
(1) The Łukasiewicz set of axioms
(2) The product set of axioms
(3) The Łukasiewicz Delta axiom
(4) The Product Delta axiom
(5) The axiom:
R ⊗(P ⊖Q) ↔LðR⊗PÞ ⊖ðR⊗QÞ
ð1Þ
The rules of inference are:
(1) Modus ponens
(2) Product necessitation.
Reference [8] includes several theorems that follow from the deﬁnition of ŁΠ
propositional fuzzy logic. In the next section, we deﬁne the ŁΠ ﬁrst order predicate
fuzzy logic (ŁΠ∀).
First Order Predicate Fuzzy Logic
Following the classical logic, the ŁΠ ﬁrst order predicate fuzzy logic, referred to as
ŁΠ∀, extends the ŁΠ propositional fuzzy logic. The primitives include constants,
variables, arbitrary-arity functions and arbitrary-arity predicates. Formulae are
constructed using (1) the basic connectives deﬁned in Table 1; (2) derived con-
nectives, such as the connectives presented in Table 2; (3) the truth constants;
(4) the quantiﬁer ∀and (5) the identity sign “=”. The quantiﬁer ∃can be used to
abbreviate formulae derived from the basic primitives and connectives. A fuzzy
interpretation of a proposition Pðx1, . . . , xnÞ over a domain M is a mapping that
assigns a fuzzy truth value to each n-tuple of elements of M As in the case of ŁΠ,
we closely follow the system used in ref. [8].
Assuming that y can be substituted for x in P and x is not free in Q the following
axioms are used:
(1) Instances of the axioms of ŁΠ obtained through substitution
(2) Universal axiom I:
ð∀xÞPðxÞ →PðyÞ
ð2Þ
(3) Universal axiom II:
ð∀xÞðP →LQÞ →LðP →Lð∀xÞQÞ
ð3Þ
(4) Identity axiom I:
x = x
ð4Þ
Complex Fuzzy Sets and Complex Fuzzy Logic …
665

(5) Identity axiom II:
ðx = yÞ →ΔðPðxÞ ↔PðYÞÞ
ð5Þ
Modus ponens, product necessitation, and generalization are used for inference.
In the next section, we deﬁne propositional and ﬁrst order predicate CFL.
2.2 Axiomatic Fuzzy Class Theory
The axiomatic fuzzy logic can serve as a basis for establishing an axiomatic FCT.
Several variants of FCT exists, most of them use a similar approach and mainly
differ in the selection of the logic base. Another difference between various
approaches is the selection of class theory axioms [64]. Běhounek et al. present and
analyze a few variants of FCT. Ref. [8] presents an ŁΠ∀based FCT.
3
Complex Fuzzy Logic and Set Theory
The ﬁrst formalization of complex fuzzy sets and complex fuzzy logic investigated
by Kandel and his coauthors [35, 65] is a special case of the formalism presented by
Tamir et al. [10]. Hence, in this section only two formalisms for complex fuzzy sets
and complex fuzzy logic are considered: (1) the formal deﬁnitions provided by
Ramot et al. [33], (2) the generalization of these concepts developed by Tamir et al.
[10, 36, 43, 66].
3.1 Complex Fuzzy Sets (Ramot et al. [33] )
This section reviews the basic concepts and operations of complex fuzzy set as
deﬁned by Ramot et al. [34, 67]. According to Ramot et al., a complex fuzzy set
S on a universe of discourse U is a set deﬁned by a complex-valued grade of
membership function μsðxÞ [33, 34]:
μsðxÞ = rsðxÞejωsðxÞ
ð6Þ
where j =
ﬃﬃﬃﬃﬃﬃﬃﬃ
−1
p
. The function μsðxÞ maps U into the unit disc of the complex plane.
This deﬁnition utilizes polar representation of complex numbers along with con-
ventional fuzzy set deﬁnition; where rsðxÞ, the amplitude part of the grade of
member-ship, is a fuzzy function deﬁned in the interval [0, 1]. On the other hand,
ωsðxÞ is a real valued function standing for the phase part of the grade of
membership.
666
D.E. Tamir et al.

In the deﬁnition provided by Ramot, the absolute value, or the amplitude part of
the membership grade, behaves in the same way as in traditional fuzzy sets. Its
value is mapped into the interval [0, 1]. On the other hand, the phase component of
the expression is not a fuzzy function; it is a real valued function that can get any
real value. Furthermore, the grade of membership is not inﬂuenced by the phase.
The phase role is to add information which is generally related to spatial or tem-
poral periodicity in the speciﬁc fuzzy set deﬁned by the amplitude component. For
example, fuzzy information related to solar activity along with crisp information
that relates to the date of measurement of the solar activity [33]. Another example
where complex fuzzy set has an intuitive appeal comes from the stock market.
Intuitively, the periodicity of the stock market along with fuzzy set based estimate
of the current values of stocks can be represented by a complex grade of mem-
bership such as the one proposed by Ramot. The amplitude conveys the information
contained in a fuzzy set such as “strong stock” while the phase conveys a crisp
information about the current phase in the presumed stock market cycle.
Following the basic deﬁnition of complex-valued grade of membership function
Ramot et al. deﬁne the basic set operations such as complement, union, and
intersection. Each of these operations is deﬁned via a set of theorems [42].
3.2 Complex Fuzzy Logic (Ramot et al. [34])
There are several ways to deﬁne fuzzy logic, fuzzy inference, and fuzzy logic
system (FLS). One of these ways is to use fuzzy set theory to deﬁne fuzzy relations,
and then deﬁne logical operations, such as implication and negation, as well as
inference rules, as special types of relations on fuzzy sets. Alternatively, fuzzy logic
can be formalized as a direct generalization of classical logic. Under this “tradi-
tional” approach, notions that relate to the syntax and semantics of classical logic,
such as propositions, interpretation, and inference are used to deﬁne fuzzy logic.
Although the relations-based deﬁnition can be carefully formalized, it is generally
less rigorous than the traditional approach.
Ramot et al. use the ﬁrst approach [34]. They use the deﬁnition of complex fuzzy
relations to deﬁne complex fuzzy logic via the deﬁnition of logical operations.
Additionally, Ramot et al. restrict complex fuzzy logic to propositions of the form
‘X is A’ , where X is a variable that receives values x from a universal set U and A is
a complex fuzzy set on U. They use this type of propositions to introduce impli-
cations of the form ‘if X is A then Y is B’ . Finally, they use modus ponens to
produce a complex fuzzy inference system. Clearly their approach is limited due to
two facts: (1) they rely on complex fuzzy sets and relations to deﬁne CFL and
(2) their fuzzy inference system is limited to propositions on complex fuzzy sets.
These limitations are resolved via the axiomatically-based approach presented in
the next section.
Complex Fuzzy Sets and Complex Fuzzy Logic …
667

3.3 Generalized Complex Fuzzy Logic (Tamir et al. [10])
This section presents the generalized form of complex fuzzy logic investigated by
Tamir et al. [10].
Propositional and First Order Predicate Complex Fuzzy Logic
A complex fuzzy proposition P is a composition of two propositions each of which
can accept a truth value in the interval ½0, 1. In other words, the interpretation of a
complex fuzzy proposition is a pair of truth values from the Cartesian interval
½0, 1 × ½0, 1. Alternatively, the interpretation can be formulated as a mapping to
the unit circle. Formally a fuzzy interpretation of a complex fuzzy proposition P is
an assignment of fuzzy truth value of the form iðprÞ + j ⋅iðpiÞ or of the form
iðrðpÞÞejσiðθðpÞÞ, where σ is a scaling factor in the interval ð0, 2π, to P.
For example, consider a proposition of the form “x … A … B …,” along with the
deﬁnition of a linguistic variables and constants. Namely, a linguistic variable is a
variable whose domain of values is comprised of formal or natural language words
[3].
Generally,
a
linguistic
variable
is
related
to
a
fuzzy
set
such
as
fvery young male, young male, old male, very old maleg and can get any value
from the set. A linguistic constant has a ﬁxed and unmodiﬁed linguistic value, i.e. a
single word or phrase from a formal or natural language.
Thus, in a proposition of the form “ x … A … B … ,” where A and B are
linguistic
variables,
iðprÞ ðiðrðpÞÞÞ
can
be
assigned
to
the
term
A
and
iðpiÞ ðiðθðpÞÞÞ can be assigned to term B .
Propositional CFL extends the deﬁnition of propositional fuzzy logic and ﬁrst
order predicate CFL extends the notion of ﬁrst order predicate fuzzy logic. Nev-
ertheless, since propositional CFL is a special case of ﬁrst order predicate CFL, we
only present the formalism for ﬁrst order predicates CFL here.
Tables 3 and 4 present the basic and derived connectives of ŁΠ∀CFL. In
essence, the connectives are symmetric with respect to the real and imaginary parts
of the predicates.
Table 3 Basic ŁΠ∀CFL connectives
Operation
Interpretation
L-Implication
iðP →L QÞ = minð1, 1 −iðprÞ + iðqrÞÞ + j ⋅minð1, 1 −iðpiÞ + iðqiÞÞ
Π-Implication
iðP →∏QÞ = minð1, iðprÞ=iðqrÞ + j ⋅minð1, iðpiÞ=iðqiÞÞ
Π-Conjunction
iðP ⊗QÞ = iðprÞ ⋅iðqrÞÞ + j ⋅ðiðpiÞ ⋅iðqiÞÞ
Table 4 Derived ŁΠ∀CFL connectives
Operation
Interpretation
L-Negation
ið′PÞ = 1 + j1 −iðPÞ
Π-Delta
ΔðiðPÞÞ = if ði(PÞÞ = 1 + j1 else Δ ði(PÞÞ = 0 + j0
Equivalence
iðP ↔QÞ = iðPr →L QrÞ ⊗iðQr →L PrÞ + j ⋅iðPi →L QiÞ ⊗iðQi →L PiÞ
P ⊖Q
iðP ⊖QÞ = maxð0, iðprÞ −iðqrÞÞ + j ⋅maxð0, iðpiÞ −iðqiÞÞ
668
D.E. Tamir et al.

Following classical logic, ŁΠ∀CFL extends, ŁΠ CFL. The primitives include
constants, variables, arbitrary-arity functions and arbitrary-arity predicates. For-
mulae are constructed using the basic connectives deﬁned in Table 3, derived
connectives such as the connectives presented in Table 4, the truth constants, the
quantiﬁer ∀and the identity sign = The quantiﬁer ∃can be used to abbreviate
formulae derived from the basic primitives and connectives. A fuzzy interpretation
of
a
proposition
Pðx1 , . . . , xnÞ = Prðx1 , . . . , xnÞ + j . Piðx1 , . . . , xmÞ
over
a
domain M is a mapping that assigns a fuzzy truth value to each (n-tuple) × (m-
tuple) of elements of M. As in the case of ŁΠ fuzzy logic, we closely follow the
system used in ref [8].
The same axioms used for ﬁrst order predicate fuzzy logic are used for ﬁrst order
predicate complex fuzzy logic; Modus ponens as well as product necessitation, and
generalization are the rules of inference.
Complex Fuzzy Propositions and Inference Examples
Consider the following propositions:
1. P(x) ≡“x is a destructive hurricane with high surge”
2. Q(x) ≡“x is a destructive hurricane with fast moving center”
Let A be the term “destructive hurricane.” Let B be the term “high surge,” and
let C be the term “fast moving center.” Hence, P is of the form: “x is a A with B”
and Q is of the form “x is A with C” In this case, the terms “destructve hurricane,”
“high surge,” and “fast moving center,” are values assigned to the linguistic vari-
ables
A, B, C
f
g. Furthermore, the term “destructve hurricane” can get fuzzy truth
values (between 0 and 1) or fuzzy linguistic values such as: “catastriphic,” “dev-
astating,” and” disastrous.” Assume that the complex fuzzy interpretation (i.e., the
degree of conﬁdence or complex fuzzy truth value) of P is pr + jpi, while the
complex fuzzy interpretation of Q is qr + jqi. Thus, the truth value of “x is a dev-
astating hurricane” is pR, the truth value of “x is in a high surge” is pi, the truth
value of “ x is a catastriphic huricane” is qr, and the truth value of “x is a fast
moving center” is qi, Suppose that the term “moderate” stands for “non –
destructive” which stands for “NOT destructive,” the term “low” stands for “NOT
high,”, and the term “slow” stands for “NOT fast.” In this context, NOT is inter-
preted as the fuzzy negation operation. Note that this is not the only way to deﬁne
these linguistic terms and it is used to exemplify the expressive power and the
inference power of the logic. Then, the complex fuzzy interpretation of the noted
composite propositions is:
(1) f
′P


= ð1 −prÞ + jð1 −pIÞ
That is, ′P denotes the proposition:
“x is a moderate hurricane with a low surge.” The conﬁdence level in ′P is
ð1 −prÞ + jð1 −piÞ; where the fuzzy truth value of the term “x is a non –
destructive hurricane,” is ð1 −prÞ and the fuzzy truth value of the term “low
surge,” is ð1 −piÞ.
Complex Fuzzy Sets and Complex Fuzzy Logic …
669

(2)
′P →′Q = minð1, qr −prÞ + j × min 1, qi −pI
ð
Þ
Thus,
′P →′Q


denotes the proposition: If “x is a moderate hurricane with a
low surge”
THEN x is a moderate huricane with low moving center.” The truth values of
individual terms, as well as the truth value of ′P →′Q are calculated according
to Table 1.
(3) f P⊕′Q


= maxðpr, 1 −qrÞ + j × maxðpi, 1 −qiÞ.
That is, P⊕′Q


denotes a proposition such as: “x is a destructive hurricane
with high surge” OR
“x is a moderate huricane with slow moving center” The truth values of
individual terms, as well as the truth value of P⊕′Q are calculated according to
Table 1.
(4) f
′P⊗Q


= minð1 −pr, qrÞ + j × minð1 −pi, qiÞ
That is,
′P⊗Q


denotes the proposition “x is a moderate hurricane with low
surge” AND “x is a destructive huricane with fast moving center.”
The truth values of individual terms, as well as the truth value of ′P⊗Q are
calculated according to Table 1.
Complex Fuzzy Inference Example
Assume that the degree of conﬁdence in the proposition R = ′P deﬁned above is
rr + jri. Let S = ′Q and assume that the degree of conﬁdence in the fuzzy impli-
cation T = R →S is tr + jti. Then, using Modus ponens
R
R →S
S
one can infer S with a degree of conﬁdence min rr, tr
ð
Þ + j × min ri, ti
ð
Þ.
In other words if one is using:
“x is a non – destructive hurricane with a low surge”
IF “x is a non – destructive hurricane with a low surge” THEN
“x is non – destructive huricane with slow moving center”
“x is non – destructive huricane with slow moving center.”
Hence, using Modus ponens one can infer:
“x is moderate hurricane with slow moving center.” with a degree of conﬁdence
of min rr, tr
ð
Þ + j × min ri, ti
ð
Þ.
3.4 Generalized Complex Fuzzy Class Theory
(Tamir et al. [10])
The axiomatic fuzzy logic can serve as a basis for formal FCT. Similarly, axiomatic
based complex fuzzy logic can serve as the basis for formal deﬁnition of complex
fuzzy classes. In this section we provide a formulation of complex fuzzy class
theory (CFCT) that is based on the logic theory presented in Sect. 3.3.
670
D.E. Tamir et al.

The main components of FCT are:
(1) Variables
(a) Variables denoting objects (potentially complex objects)
(b) Variables denoting crisp sets, i.e. a universe of discourse and its subsets
(c) Variables denoting complex fuzzy classes of order 1
(d) Variables denoting complex fuzzy classes of order n, that is, complex
fuzzy classes of complex fuzzy classes of order n-1.
(2) The LΠ∀CFL system along with its variables, connectives, predicates, and
axioms as deﬁned in Sect. 3.3.
(3) Additional predicates
(a) A binary predicate ∈x, Γ
ð
Þ denoting membership of objects in complex
fuzzy classes and/or in crisp sets
(4) Additional Axioms
(a) Instances of the comprehension schema (further explained below)
ð∃ΓÞΔð∀xÞðx ∈Γ ↔P x
ð ÞÞ
ð7Þ
Where x is a complex fuzzy object, Γ is a complex fuzzy class, and PðÞ is
a complex fuzzy predicate.
(b) The axiom of extensionality
∀x
ð
ÞΔ x ∈Γ ↔x ∈Ψ
ð
Þ →Γ = Ψ
ð8Þ
Where, x is a complex fuzzy object, Γ is a complex fuzzy class, and PðÞ is a
complex fuzzy predicate.
Note that a grade of membership is not a part of the above speciﬁed terms; yet it
can be derived or deﬁned using these terms.
The comprehension schema is used to “construct” classes. It has the basic form
of: ð∀xÞðx ∈Γ ↔P x
ð ÞÞ. Intuitively, this schema refers to the class Γ of all the
objects x that satisfy the predicate PðÞ. Instances of this schema have the generic
form:ð∃ΓÞð∀xÞðx ∈Γ ↔P x
ð ÞÞ. Associated with this schema are comprehension
terms of the form: ∈fxjP x
ð Þ ↔P y
ð Þg. The Δ operation introduced in Eq. 8 is used
to produce precise instances of the extensionality schema and ensure the conser-
vatism of comprehension terms.
Fixing a standard model over the CFCT enables the deﬁnition of commonly used
terms, set operations, and deﬁnitions, as well as proving CFCT theorems. Some of
these elements are listed here:
(1) The complex characteristic function χx ∈Γ ≡χΓ and the grade of membership
function μx ∈Γ ≡μΓ
Complex Fuzzy Sets and Complex Fuzzy Logic …
671

(2) Complex class constants, α-cuts, iterated complements, and primitive binary
operations, such as union, intersection etc. These operations are constructed
using the schema OP Γ
ð Þ ≡fxjP x ∈Γ
ð
Þg. Table 5 lists some of these elements.
(3) Uniform and supreme relations deﬁned in ref. [8] enable the deﬁnition of fuzzy
class relations such as inclusion
(4) Theorems, primitive fuzzy class operations, and fuzzy class relations [8].
Following the axiomatically-based deﬁnition of grade of membership, Eqs. (9-
11) can be used as a basis for the deﬁnition of “membership grade based” com-
plement, union, and intersection.
Complex Fuzzy Classes and Connectives Examples
In order to provide a concrete example, we deﬁne the following complex fuzzy
classes using the comprehension schema. Let the universe of discourse be the set of
all the stocks that were available for trading on the opening of the New York stock
exchange (NYSE) market on January 5, 2015 along with a set of attributes related
to historical price performance of each of these stocks.
Consider the following complex propositions:
P(x) ≡“x is a volatile stock in a strong portfolio”
Q(x) ≡“x is a stock in a decline trend in a strong portfolio”
Then, the proposition: ð∃ΓÞΔð∀xÞðx ∈Γ ↔P x
ð Þ⊗Q x
ð Þ
ð
Þ, where x is any member
of the universe of discourse, deﬁnes a complex fuzzy class Γ that can be “described”
as the class of “volatile stocks in a decline trend in strong portfolios.” On the other
hand, the proposition ð∃ΓÞΔð∀xÞðx ∈Γ ↔
′P x
ð Þ ∨Q x
ð Þ


, where x is any member of
the universe of discourse, deﬁnes a complex fuzzy class Γ that can be “described” as
the class of “non – volatile stocks in a decline trend in strong portfolios.”
3.5 Pure Complex Fuzzy Classes
Often it is useful to deﬁne complex fuzzy sets via membership functions rather than
through axioms. To this end, Tamir et al. have introduced the concept of pure
complex fuzzy sets [42]. This concept is reviewed in this section.
Table 5 Derived primitive class operations
Term
Symbol
P
Comments
Empty complex class
Θ
0
Universal complex class
Φ
1
Strict complement
\Γ
∼
∼stands for Gödel (G) negation
Complex class
intersection
∩
⊕
⊕stands for a G, Ł, or Π conjunction T-norm
Complex class union
∪
∨
∨stands for a G, Ł, or Π disjunction
672
D.E. Tamir et al.

The Cartesian representation of the pure complex grade of membership is given
in the following way:
μ V, x
ð
Þ = μr V
ð Þ + jμi zð Þ
ð9Þ
Where μr V
ð Þ and μi zð Þ, the real and imaginary components of the pure complex
fuzzy grade of membership, are real value fuzzy grades of membership. That is,
μr V
ð Þ and μi zð Þ can get any value in the interval ½0, 1. The polar representation of
the pure complex grade of membership is given by:
μ V, x
ð
Þ = rðVÞejσϕðzÞ
ð10Þ
Where r V
ð Þ and ϕ zð Þ, the amplitude and phase components of the pure complex
fuzzy grade of membership, are real value fuzzy grades of membership. That is,
they can get any value in the interval ½0, 1. The scaling factor σ is in the interval
ð0, 2π. It is used to control the behavior of the phase within the unit circle
according to the speciﬁc application. Typical values of σ are f1, π
2 , π, 2πg.
The main difference between pure complex fuzzy grades of membership and the
complex fuzzy grade of membership proposed by Ramot et al. [33, 34] is that both
components of the membership grade are fuzzy functions that convey information
about a fuzzy set.
4
Recent Developments in the Theory and Applications
of CFL and CFS
In this section we review recent literature on complex fuzzy logic and complex
fuzzy sets. First we review papers that enhance the theoretical basis of CFL/CFS.
Next, we outline some of the recent reports on CFL/CFS related applications.
4.1 Advances in the Theoretical Foundations of CFL/CFS
Yager et al. have presented the idea of Pythagorean membership grades and the
related idea of Pythagorean fuzzy subsets [68]. They have focused on the negation
operation and its relationship to the Pythagorean Theorem. Additionally, they
examined the basic set operations for the case of Pythagorean fuzzy subsets. Yager
et al. further note that the idea of Pythagorean membership grades can provide an
interesting semantics for complex number-valued membership grades used in
complex fuzzy sets.
Greenﬁeld et al. ([69]) have compared and contrasted the FCS formalism pro-
posed by Ramot et al. ([33]) as well as the “innovation of pure complex fuzzy sets,
Complex Fuzzy Sets and Complex Fuzzy Logic …
673

proposed by Tamir et al. ([42])” with type-2 fuzzy sets [30, 37]. They have con-
centrated on the rationales, applications, deﬁnitions, and structures of these con-
structs. In addition, they have compared pure complex fuzzy sets with type-2 fuzzy
sets in relation to inference operations. They have concluded that complex fuzzy
sets and type-2 fuzzy sets differ in their roles and applications. They have identiﬁed
similarities between pure complex fuzzy sets and type-2 fuzzy sets; but concluded
that type-2 fuzzy sets were isomorphic to pure complex fuzzy sets.
Apolloni et al. propose to deﬁne and manage a complex fuzzy set by computing
its membership function using a few variables quantized into a few elementary
granules and elementary functions connecting the variables [70].
Guosheng et al. have introduced three complex fuzzy reasoning schemes:
Principal Axis, Phase Parameters, and Concurrence Reasoning Scheme [49]. They
have demonstrated that a variety of conjunction operators and implication operators
can be selected to compose the corresponding instances of complex reasoning
schemes.
Guangquan et al. have investigated various operation properties of complex
fuzzy relations [71]. They have deﬁned a distance measure for evaluating the
differences between the grades as well as the phases of two complex fuzzy relations.
Furthermore, they have used the distance measure to deﬁne δ-equalities of complex
fuzzy relations. Finally, they have examined fuzzy inference in the framework of δ-
equalities of complex fuzzy relations.
Tamir et al. have proposed a complex fuzzy logic (CFL) system that is based on
the extended Post multi-valued logic system (EPS) of order p > 2, and have dem-
onstrated its utility for reasoning with fuzzy facts and rules. The advantage of this
formalism is that it is discrete. Hence, it better ﬁts real time applications, digital
signal processing, and embedded systems that use integer processing units.
4.2 Applications of CFL/CFS
A group of researchers working along with Dick have developed the concept of
Adaptive Neuro Fuzzy Complex Inference System (ANCFIS) and explored related
applications by integrating complex fuzzy logic into Adaptive Neuro Fuzzy
Complex Inference System (ANFIS) [72].
Man et al. have extended the concept of ANFIS and introduced ACNFIS [73].
They have applied ANCFIS in time series forecasting. They compared ACNFIS to
three commonly cited time series datasets and demonstrated that ACNFIS was able
to accurately model relatively periodic data.
An extension of this work, including synthetic time series and several real-world
forecasting problems, is presented by Zhifei et al. [74]. They have found that
ANCFIS performs well on these problems and is also very parsimonious. Their
work demonstrates the utility of complex fuzzy logic on real-world problems.
Aghakhani et al. have developed an online learning algorithm for ACNFIS and
applied it to time series prediction [75]. Their experimental results show that the
674
D.E. Tamir et al.

online technique is comparable to existing results, although slightly inferior to the
off-line ANCFIS results.
Yazdanbaksh et al. applied ANCFIS to the problem of short-term forecasts of
Photovoltaic power generation [76]. They compared ANFIS and radial basis
function networks against ANCFIS. Their experimental results have demonstrated
that the ANCFIS based approach was more accurate in predicting power output on
a simulated solar cell. Additionally, in a recent paper Yazdanbaksh et al. presented a
recommended approach to determining input windows that balances the accuracy
and computation time [77].
Another group that is active in exploring CFL/CFS applications is led by Li [14,
78]. Li et al. have proposed a novel complex neuro-fuzzy autoregressive integrated
moving average (ARIMA) computing approach and applied it to the problem of
time-series forecasting [79]. They have found that their new formalism, referred to
as CNFS-ARIMA, has excellent nonlinear mapping capability for time-series
forecasting.
Additionally, Li et al. have presented a neuro-fuzzy approach using complex
fuzzy sets (CNFS) for the problem of knowledge discovery [80]. They have devised
a hybrid learning algorithm to evolve the CNFS for modeling accuracy, combining
artiﬁcial bee colony algorithm and recursive least squares estimator method. They
have tested the CNFS based approach in knowledge discovery through experi-
mentation, and concluded that the proposed approach outperforms comparable
approaches.
Another application of CNFS presented by Li et al. is adaptive image noise
cancelling [81]. Two cases of image restoration have been used to test the proposed
approach and have shown a good restoration quality. Additionally, Li et al. have
presented a hybrid learning method that enables efﬁcient and quick CNFS con-
vergence procedure and applied the hybrid learning based CNFS to the problem of
function approximation [14, 81]. They have concluded that the CNFS shows much
better performance than its traditional neuro-fuzzy counterpart and other compared
approaches.
Ma et al. applied complex fuzzy sets to the problem of multiple periodic factor
prediction (MPFP) [46]. They have developed a product-sum aggregation operator
(PSAO), which is a set of complex fuzzy sets. PSAO has been used to integrate
information with uncertainty and periodicity. Next, they have developed a
PSAO-based prediction (PSAOP) method to generate solutions for MPFP prob-
lems. The experimental results indicate that the proposed PSAOP method effec-
tively handles the uncertainty and periodicity in the information of multiple periodic
factors simultaneously and can generate accurate predictions for MPFP problems.
Tamir et al. have considered numerous applications of CFL [24, 36, 43, 66, 82,
83]. They have introduced several soft computing based methods and tools for
disaster mitigation [24] and epidemic crises prediction [83]. Additionally, they have
demonstrated the potential use of complex fuzzy graphs as well as incremental
fuzzy clustering in the context of complex and high order fuzzy logic systems.
Additionally, they have developed an axiomatic based framework for discrete
complex fuzzy logic and set theory [66].
Complex Fuzzy Sets and Complex Fuzzy Logic …
675

In [82] Tamir et al. have presented a complex fuzzy logic based inference system
used to account for the intricate relations between software engineering constraints
such as quality, software features, and development effort. The new model con-
centrates on the requirements speciﬁcations part of the software engineering pro-
cess. Moreover, the new model signiﬁcantly improves the expressive power and
inference capability of the soft computing component in a soft computing based
quantitative software engineering paradigm.
5
Conclusion
We have reviewed the theoretical basis of complex fuzzy logic and complex fuzzy
sets and the current state of related applications. We have surveyed the research
related to the underlying theory as well as recent applications of the theory in
complex fuzzy based algorithms and complex fuzzy inference systems.
The concepts of complex fuzzy logic and complex fuzzy sets have undergone an
evolutionary process since they were ﬁrst introduced [35]. The initial deﬁnitions
were practical but somewhat naïve and limited [33, 34, 65, 67]. The introduction of
axiomatically based approach ([10, 36, 43]) has enabled extending the concepts,
maintaining practicality, and providing a solid foundation for further theoretical
development. Several applications of the new theories have emerged; based on
recent reports this area of applications is gaining momentum.
There are numerous ﬁelds where fuzzy concepts interact in intricate ways, which
can be effectively captured by the semantics of FCL FCS, pure complex fuzzy
classes, and discrete signals. We plan to investigate some of these concepts in the
near future. Of particular interest are multimedia signals. Often, these signals are
represented using complex functions. On the other hand, due to noise, the pro-
cessing of such signals might require using complex fuzzy logic. We plan to assess
the utility of CFS, CFL, and complex fuzzy sets to the processing of signals in
certain noisy situations.
Acknowledgment This work is based in part upon work supported by the National Science
Foundation
under
grants
I/UCRC
IIP-1338922,
AIR
IIP-1237818,
SBIR
IIP-1330943,
III-Large
IIS-1213026,
MRI
(CNS-1429345,
CNS-0821345,
CNS-1126619),
and
CREST HRD-0833093 and by DHS S&T at TerraFly (http://terraﬂy.com) and the NSF CAKE
Center (http://cake.ﬁu.edu).
References
1. Zadeh, L.A.: Fuzzy sets. Inf. Control 8(1), 338–353 (1965)
2. Zadeh, L.A.: Fuzzy algorithms. Inf. Control 12(2), 94–102 (1968)
3. Zadeh, L.A.: The concept of a linguistic variable and its application to approximate reasoning -
part I. Inf. Sci. 7(1), 199–249 (1975)
676
D.E. Tamir et al.

4. Zadeh, L. A.: From computing with numbers to computing with words - from manipulation of
measurements to manipulation of perceptions. IEEE Trans. Circ. Syst. 45(1), 105–119 (1999)
5. Yager, R.R.: Fuzzy Sets and Applications: Selected Papers by L.A. Zadeh. Wiley, New York
(1987)
6. Kandel, A.: Fuzzy Mathematical Techniques with Applications. Addison Wesley, Boston
(1987)
7. Kosko, B.: Fuzzy logic. Sci. Am. 269(1), 76–81 (1993)
8. Běhounek, L., Cintula, P.: Fuzzy class theory. Fuzzy Sets Syst. 154(1), 34–55 (2005)
9. Tamir, D.E., Kandel, A.: An axiomatic approach to fuzzy set theory. Inf. Sci. 52(1), 75–83
(1990)
10. Tamir, D., Kandel, A.: Axiomatic theory of complex fuzzy logic and complex fuzzy classes.
Int. J. Comput. Commun. Control 6(3), 508–522 (2011)
11. Drianko, D., Hellendorf, H., Reinfrank, M.: An Introduction to Fuzzy Control. Springer,
London (1993)
12. Lee, C.C.: Fuzzy logic in control systems. IEEE Trans. Syst. Man Cybern. 20(2), 404–435
(1990)
13. De, S.P., Krishna, R.P.: A new approach to mining fuzzy databases using nearest neighbor
classiﬁcation by exploiting attribute hierarchies. Int. J. Intell. Syst. 19(12), 1277–1290 (2004)
14. Li, C., Chan, F.: Knowledge discovery by an intelligent approach using complex fuzzy sets.
In: Pan, J., Chen, S., Nguyen, N.T. (eds) Intelligent Information and Database Systems,
pp. 320–329. Springer, Berlin (2012)
15. Pedrycz, W., Gomide, F.: An Introduction to Fuzzy Sets Analysis and Design. MIT Press,
Massachusetts (1998)
16. Halpern, J.Y.: Reasoning about Uncertainty. MIT Press, Massachusetts (2003)
17. Klir, G.J., Tina, A.: Fuzzy Sets, Uncertainty, and Information. Prentice Hall, Upper Saddle
River (1988)
18. Lou, X., Hou, W., Li, Y., Wang, Z.: A fuzzy neural network model for predicting clothing
thermal comfort. Comput. Math Appl. 53(12), 1840–1846 (2007)
19. Constantin, V.: Fuzzy Logic and NeuroFuzzy Applications Explained. Prentice Hall, Upper
Saddle River (1995)
20. Aaron, B., Tamir, D.E., Rishe, N.D., Kandel, A.: Dynamic incremental fuzzy C-means
clustering. In Proceedings of The The Sixth International Conference on Pervasive Patterns
and Applications, pp. 28–37. Venice, Italy (2014)
21. Höppner, F., Klawonn, F., Kruse, R., Runkler, K.: Fuzzy Cluster Analysis: Methods for
Classiﬁcation, Data Analysis and Image Recognition. Wiley, New York (1999)
22. Tamir, D.E., Kandel, A.: The pyramid fuzzy C-means algorithm. Int. J. Comput. Intell.
Control 2(2), 65–77 (2010)
23. Hu, D., Li, H., Yu, X.: The Information content of fuzzy relations and fuzzy rules. Comput.
Math. Appl. 57, 202–216 (2009)
24. Kandel, A., Tamir, D.E., Rishe, N.D.: Fuzzy logic and data mining in disaster mitigation. In:
Teodorescu, H.N., Kirschenbaum, A., Cojocaru, S., Bruderlein, C. (eds.) Improving Disaster
Resilience and Mitigation - IT Means and Tools, pp. 167–186. Springer, Netherlands (2014)
25. Agarwal, D., Tamir, D.E., Last, M., Kandel, A.: A comparative study of software testing using
artiﬁcial neural networks and info-fuzzy networks. IEEE Trans. Syst. Man Cybern. 42(5),
1183–1193 (2012)
26. Last, M., Friedman, M., Kandel, A.: The data mining approach to automated software testing.
In: Proceedings of The Proceedings of the Ninth ACM International Conference on
Knowledge Discovery and Data Mining, pp. 388–396 (2003)
27. Zadeh, L.A.: The concept of a linguistic variable and its application to approximate reasoning -
part II. Inf. Sci. 7(1), 301–357 (1975)
28. Zadeh, L.A.: The concept of a linguistic variable and its application to approximate reasoning -
part III. Inf. Sci. 9(1), 43–80 (1975)
29. Karnik, N.N., Mendel, J.M., Liang, Q.: Type-2 fuzzy logic systems. IEEE Trans. Fuzzy Syst. 7
(6), 643–658 (1999)
Complex Fuzzy Sets and Complex Fuzzy Logic …
677

30. Qilian, L., Mendel, J.M.: Interval type-2 fuzzy logic systems. In: Proceedings of The The
Ninth IEEE International Conference on Fuzzy Systems, pp. 328–333 (2000)
31. Buckley, J.J.: Fuzzy complex numbers. Fuzzy Sets Syst. 33(1), 333–345 (1989)
32. Yager, R.R.: On a view of zadeh Z-numbers, vol. 299, pp. 90–101 (2012)
33. Ramot, D., Milo, R., Friedman, M., Kandel, A.: Complex fuzzy sets. IEEE Trans. Fuzzy Syst.
10(2), 171–186 (2002)
34. Ramot, D., Friedman, M., Langholz, G., Kandel, A.: Complex fuzzy logic. IEEE Trans. Fuzzy
Syst. 11(4), 450–461 (2003)
35. Moses, D., Degani, O., Teodorescu, H., Friedman, M., Kandel, A.: Linguistic coordinate
transformations for complex fuzzy sets. In: Proceedings of The IEEE International Conference
on Fuzzy Systems, pp. 1340–1345 (1999)
36. Tamir, D.E., Last, M., Kandel, A.: Complex fuzzy logic. In Seising, R., Trillas, E., Termini,
S., Moraga, C. (eds.) On Fuzziness, pp. 665–672. Springer, London (2013)
37. Karnik, N.N., Mendel, J.M., Qilian, L.: Type-2 fuzzy logic systems. IEEE Trans Fuzzy Syst. 7
(6), 643–658 (1999)
38. Buckley, J.J., Qu, Y.: Solving fuzzy equations: a new solution concept. Fuzzy Sets Syst. 41(1),
291–301 (1991)
39. Buckley, J.J., Qu, Y.: Solving linear and quadratic fuzzy equations. Fuzzy Sets Syst. 38(1),
43–59 (1990)
40. Buckley, J.J., Qu, Y.: Fuzzy complex analysis I: differentiation. Fuzzy Sets Syst. 41(1),
269–284 (1991)
41. Buckley, J.J.: Fuzzy complex analysis II: integration. Fuzzy Sets Syst. 49(1), 171–179 (1992)
42. Tamir, D.E., Kandel, A.: A new interpretation of complex membership grade. Int. J. Intell.
Syst. 26(4), 285–312 (2011)
43. Tamir, D.E., Last, M., Kandel, A.: The theory and applications of generalized complex fuzzy
propositional logic. In: Yager, R.R., Abbasov, A.M., Reformat, M.Z. Shahbazova, S.N. (eds.)
Soft Computing: State of the Art Theory and Novel Applications Springer Series on Studies in
Fuzziness and Soft Computing, pp. 177–192. Springer, Berlin (2013)
44. Zhang, G., Dillon, T.S., Cai, K., Ma, J., Lu, J.: Operation properties and delta equalities of
complex fuzzy sets. Int. J. Approximate Reasoning 50(8), 1227–1249 (2009)
45. Wu, C., Qiu, J.: Some remarks for fuzzy complex analysis. Fuzzy Sets Syst. 106(1), 231–238
(1999)
46. Ma, S., Peng, D., Li, D.: Fuzzy complex value measure and fuzzy complex value measurable
function. In: Cao, B., Zhang, C., Li, T. (eds.) Fuzzy Information and Engineering, pp. 187–192
(2009)
47. Łukasiewicz, J.: On three-valued logic. In: Borkowski, L. (ed.) Selected Works by Jan
Łukasiewicz (English Translation), pp. 87–88. North–Holland, Amsterdam (1970)
48. Guh, Y., Yang, M., Po, R., Lee, E.S.: Interval-valued fuzzy relation-based clustering with its
application to performance evaluation. Comput. Math Appl. 57(5), 841–849 (2009)
49. Guosheng, C., Jianwei, Y.: Complex fuzzy reasoning schemes. In: Proceedings of The Third
International Conference on Information and Computing, pp. 29–32 (2010)
50. Qiu, T., Chen, X., Liu, Q., Huang, H.: Granular computing approach to ﬁnding association
rules in relational database. Int. J. Intell. Syst. 25(2), 165–179 (2010)
51. Ronen, M., Shabtai, R., Guterman, H.: Hybrid model building methodology using
unsupervised fuzzy clustering and supervised neural networks. Biotechnol. Bioeng. 77(4),
420–429 (2002)
52. Tamir, D.E., Kandel, A.: Fuzzy semantic analysis and formal speciﬁcation of conceptual
knowledge. Inf. Sci. Intell. Syst. 82(3), 181–196 (1995)
53. Zimmermann, H.: Fuzzy Set Theory and its Applications. Kluwer Academic Publishers,
Massachusetts (2001)
54. Baaz, M., Hajek, P., Montagna, F., Veith, H.: Complexity of t-tautologies. Ann. Pure Appl.
Logic 113(1), 3–11 (2002)
55. Cintula, P.: Weakly implicative fuzzy logics. Arch. Math. Logic 45(6), 673–704 (2006)
56. Cintula, P.: Advances in LΠ and LΠ1/2 logics. Arch. Math. Logic 42(1), 449–468 (2003)
678
D.E. Tamir et al.

57. Hajek, P.: Arithmetical complexity of fuzzy logic - a survey. Soft. Comput. 9(1), 935–941
(2005)
58. Hájek, P.: Metamathematics of Fuzzy Logic. Kluwer Academic Publishers, Massachusetts
(19980
59. Hájek, P.: Fuzzy logic and arithmetical hierarchy. Fuzzy Sets Syst. 3(8), 359–363 (1995)
60. Montagna, F.: On the predicate logics of continuous t-norm BL-algebras. Arch. Math. Logic
44(1), 97–114 (2005)
61. Montagna, F.: Three complexity problems in quantiﬁed fuzzy logic. Stud. Logica. 68(1),
143–152 (2001)
62. Mundici, D., Cignoli, R., D’Ottaviano, I.M.L.: Algebraic Foundations of Many-Valued
Reasoning. Kluwer Academic Press, Massachusetts (1999)
63. She, Y., Wang, G.: An axiomatic approach of fuzzy rough sets based on residuated lattices.
Comput. Math Appl. 58(1), 189–201 (2009)
64. Fraenkel, A.A., Bar-Hillel, Y., Levy, A.: Foundations of Set Theory, 2nd edn. Elsevier,
Pennsylvania (1973)
65. Nguyen, H.T., Kandel, A., Kreinovich, V.: Complex fuzzy sets: towards new foundations. In:
Proceedings of The Proceedings of the IEEE International Conference on Fuzzy Systems,
pp. 1045–1048 (2000)
66. Tamir, D.E., Last, M., Teodorescu, N.H., Kandel, A.: Discrete complex fuzzy logic. In:
Proceedings of The Proceedings of the North American Fuzzy Information Processing Society,
pp. 1–6. California, USA (2012)
67. Dick, S.: Towards complex fuzzy logic. IEEE Trans. Fuzzy Syst. 13(1), 405–414 (2005)
68. Yager, R.R., Abbasov, A.M.: Pythagorean membership grades, complex numbers, and
decision making. Int. J. Intell. Syst. 28(5), 436–452 (2013)
69. Greenﬁeld, S., Chiclana, F.: Fuzzy in 3-D: contrasting complex fuzzy sets with type-2 fuzzy
sets. In: Proceedings of The Joint Annual Meeting IFSA World Congress and NAFIPS,
pp. 1237–1242 (2013)
70. Apolloni, B., Pedrycz, W., Bassis, S., Malchiodi, D.: Granular constructs. In: Apolloni, B.,
Pedrycz, W., Bassis, S. Malchiodi, D. (eds.) The Puzzle of Granular Computing, pp. 343–384.
Springer, Berlin (2008)
71. Guangquan, Z., Dillon, T.S., Kai-Yuan, C., Jun, M., Jie, L.: Delta-equalities of complex fuzzy
relations. In: Proceedings of The IEEE International 24th Conference on Advanced
Information Networking and Applications, pp. 1218–1224 (2010)
72. Chen, Z., Aghakhani, S., Man, J., Dick, S.: ANCFIS: a neuro-fuzzy architecture employing
complex fuzzy sets. IEEE Trans. Fuzzy Syst. 19(2), 305–322 (2009)
73. Man, J.Y., Chen, Z., Dick, S.: Towards inductive learning of complex fuzzy inference
systems. In: Proceedings of The Annual Meeting of the North American Fuzzy Information
Processing Society, pp. 415–420 (2007)
74. Zhifei, C., Aghakhani, S., Man, J., Dick, S.: ANCFIS: a neurofuzzy architecture employing
complex fuzzy sets. IEEE Int. Conf. Fuzzy Syst. 19(2), 305–322 (2011)
75. Aghakhani, S., Dick, S.: An on-line learning algorithm for complex fuzzy logic. In:
Proceedings of The The IEEE International Conference on Fuzzy Systems, pp. 1–7 (2010)
76. Yazdanbaksh, O., Krahn, A., Dick, S.: Predicting solar power output using complex fuzzy
logic. In: Proceedings of The Joint IFSA World Congress and NAFIPS Annual Meeting,
pp. 1243–1248 (2013)
77. Yazdanbakhsh, O., Dick, S.: Time-series forecasting via complex fuzzy logic, pp. 147–165
(2015)
78. Li, Y., Jang, T.Y.: Complex adaptive fuzzy inference systems. In: Proceedings of The
Proceedings of the Asian Conference on Soft Computing in Intelligent Systems and
Information Processing, pp. 551–556 (1996)
79. Li, C., Chiang, T.: Complex neurofuzzy ARIMA forecasting—a new approach using complex
fuzzy sets. IEEE Trans. Fuzzy Syst. 21(3), 567–584 (2013)
80. Li, C., Chiang, T.: Function approximation with complex neuro-fuzzy system using complex
fuzzy sets A new approach. New Gener. Comput. 29(3), 261–276 (2011)
Complex Fuzzy Sets and Complex Fuzzy Logic …
679

81. Li, C., Chan, F.: Complex-fuzzy adaptive image restoration an artiﬁcial-bee-colony-based
learning approach. In: Nguyen, N.T., Kim, C., Janiak, A. (eds.) Intelligent Information and
Database Systems, pp. 90–99. Springer, Berlin (2011)
82. Tamir, D.E., Mueller, C.J., Kandel, A.: Complex fuzzy logic reasoning based methodologies
for quantitative software engineering. In: Pedrycz, W., Succi, G., Sillitti, A. (eds.)
Computational Intelligence and Quantitative Software Engineering. Springer, Berlin (2015)
83. Tamir, D.E., Rishe, N.D., Last, M., Kandel, A.: Soft computing based epidemical crisis
prediction. In: Yager, R.R., Reformat, M.Z., Alajlan, N. (eds.) Intelligent Methods for
Cyberwarfare, pp. 43–76. Springer, Berlin (2014)
Authors Biography
Dan E. Tamir is an associate professor in the Department of
Computer Science, Texas State University, San Marcos, Texas
(2005 - to date). He obtained the PhD-CS from Florida State
University in1989, and the MS/BS-EE from Ben-Gurion Uni-
versity, Israel in 1983, 1986 respectively.
From 1996-2005, he managed applied research and design in
DSP
Core
technology
in
Motorola-SPS/Freescale.
From
1989-1996, he served as an assistant/associate professor in the
CS Department at Florida Tech. Between 1983-1986, he worked
in the applied research division, Tadiran, Israel.
Dr. Tamir is conducting research in the areas of data
compression and pattern recognition, complex fuzzy logic, and
effort based usability evaluation. Additional research areas
include signal processing and combinatorial optimization. He
is teaching graduate and undergraduate courses in formal
languages, computer architecture, multi-media programming, graphical user interfaces, and
computer graphics. He is supervising research and individual study courses with graduate and
undergraduate students; twenty eight students have completed their master’s thesis / PhD
dissertation under his supervision.
Dr. Tamir has published more than 90 refereed journal and confer-ence papers as well as four
book chapters in the areas of combinatorial optimization, computer vision, audio, image, and video
compression, human computer interaction, and pattern recognition. He has been a member of the
Israeli delegation to the MPEG committee and a Summer Fellow at NASA KSC.
Naphtali David Rishe
The Inaugural Outstanding University Professor
Eminent Chair Professor of Computer Science
Director, FIU High Performance Database Research Center
Director, NSF FIU-FAU-Dubna Industry/University Coopera-
tive Research Center for Advanced Knowledge Enablement
Director, NSF-FIU Center of Research Excellence in Science
and Technology
Dr. Rishe has authored 3 books on database design and
geography and edited 5 books on database management and high
performance computing. He is the inventor of 4 U.S. patents on
database querying, semantic database performance, Internet data
extraction, and computer medicine. Rishe has authored 300
680
D.E. Tamir et al.

papers in journals and proceedings on databases, software engineering, geographic information
systems, Internet, and life sciences. He was awarded over $55 million in research grants by
Government and Industry, including NASA, NSF, IBM, DoI, DHS, and USGS. Rishe is the
Founder and Director of the High Performance Database Research Center at FIU (HPDRC);
Director of the NSF Center for Research Excellence in Science and Technology at FIU (CREST)
and of the NSF International FIU-FAU-Dubna Industry-University Cooperative Research Center
for Advanced Knowledge Enablement (I/UCRC). Rishe is the inaugural FIU Outstanding
University Professor. Rishe’s TerraFly project has been extensively covered by worldwide press,
including the New York Times, USA Today, NPR, Science and Nature journals, and FOX TV
News. Rishe’s principal projects are TerraFly (a 50 TB database of aerial imagery and Web-based
GIS) and Medical Informatics.
High Performance Database Research Center, School of Computing and Information Sciences,
University Park, FIU ECS-243, Miami, FL 33199, USA. Telephone: +1-305-348-1706; fax:
+1-305-348-1707; http://hpdrc.ﬁu.edu; rishe@cs.ﬁu.edu.
Abraham Kandel Abraham Kandel received the B.S. degree in
Electrical Engineering from the Technion-Israel Institute of
Technology, Haifa, Israel, the M.S. degree in Electrical Engi-
neering from the University of California, Santa Barbara, and the
Ph.D. degree in Electrical Engineering and Computer Science
from the University of New Mexico, Albuquerque.
He
was
the
Chairman
of
the
Computer
Science
and
Engineering Department, University of South Florida during
1991-2003 and the Founding Chairman of the Computer Science
Department at the Florida State University during 1978 – 1991.
He was a Distinguished University Research Professor and
Endowed Eminent Scholar in Computer Science and Engineer-
ing at the University of South Florida, Tampa, and the Executive
Director of the National Institute for Applied Computational
Intelligence.
Dr. Kandel is the author or coauthor of more than 800 research papers for numerous
professional publications in Computer Science and Engineering. He is the author, coauthor, editor,
or coeditor of 51 text books and research monographs, with the most recent text “Calculus Light”
coauthored with Prof. M. Friedman and published in 2011 by Springer-Verlag. He is a member of
the editorial boards and advisory boards of several leading international journals in Computer
Science and Engineering.
Dr. Kandel is a Fellow of the Association for Computing Machinery, the Institute for Electrical
and Electronics Engineers, the New York Academy of Sciences, the American Association for the
Advancement of Science, and the International Fuzzy Systems Association. He was the recipient
of the Fulbright Senior Research Fellow Award at Tel-Aviv University during 2003-2004. In 2005
and 2013, he was selected by the Fulbright Foundation as a Fulbright Senior Specialist in applied
fuzzy logic and computational intelligence. Dr. Kandel is the recipient of numerous professional
awards, including the 2012 IEEE Computational Intelligence Society Fuzzy Systems Pioneer
Award.
Presently, Dr. Kandel is a Distinguished University Professor Emeritus at the University of
South Florida (retired in 2012), and since 2013 he is a Visit-ing Professor in the School of
Computing and Information Sciences at the Florida International University, Miami, Florida.
Complex Fuzzy Sets and Complex Fuzzy Logic …
681

Author Index
A
Abbasov, Ali M., 25
B
Banerjee, Romi, 417
Bordogna, Gloria, 267
Borgelt, Christian, 315
Bouchon-Meunier, Bernadette, 51
Braune, Christian, 315
Burhan Türkşen, I., 643
C
Colvin, Erin, 267
D
Ding, Liya, 65
Dong, Fangyan, 169
Duenyas, Shahaf, 363
E
Elmore, Paul, 483
F
Filev, Dimitar, 79
G
Gomide, Fernando, 107
Grabisch, Michel, 125
Gupta, Madan M., 153
H
Halpert, Hezi, 337
Hirota, Kaoru, 169
Hossein Zadeh, Parisa D., 519
K
Kacprzyk, Janusz, 183
Kandel, Abraham, 661
Klir, George J., 201
Kóczy, László T., 227
Kolmanovsky, Ilya, 79
Kosheleva, Olga, 297
Kosko, Bart, 245
Kraft, Donald H., 267
Kreinovich, Vladik, 297
Kruse, Rudolf, 315
L
Last, Mark, 337
Lee, Sang Wan, 39
Leite, Daniel, 107
Lesot, Marie-Jeanne, 315
Li, Kuwen, 459
Liu, Xiaogan, 65
M
Margaliot, Michael, 363
Marsala, Christophe, 51
Mendel, Jerry M., 389
N
Nguyen, Hung T., 403
P
Pal, Sankar K., 417
Pasi, Gabriella, 267
Pedrycz, Witold, 459
Petry, Frederick, 483
Pivert, Olivier, 499
R
Reformat, Marek, 459
Reformat, Marek Z., 519
Rishe, Naphtali D., 661
S
Seising, Rudolf, 537
Shahbazova, Shahnaz N., 25
© Springer International Publishing Switzerland 2015
D.E. Tamir et al. (eds.), Fifty Years of Fuzzy Logic and its Applications,
Studies in Fuzziness and Soft Computing 326,
DOI 10.1007/978-3-319-19683-1
683

Smits, Grégory, 499
Solo, Ashu M.G., 153
Sriboonchitta, Songsak, 403
T
Tamir, Dan E., 661
Teodorescu, Horia-Nicolai, 581
Tormási, Alex, 227
Trillas, Enric, 609
Y
Yager, Ronald, 79
Z
Zadeh, LotﬁA., 1
Zenn Bien, Z., 39
684
Author Index


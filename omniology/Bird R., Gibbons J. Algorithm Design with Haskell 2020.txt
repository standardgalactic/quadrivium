
ALGORITHM DESIGN WITH HASKELL
This book is devoted to ﬁve main principles of algorithm design: divide and
conquer, greedy algorithms, thinning, dynamic programming, and exhaustive
search. These principles are presented using Haskell, a purely functional language,
leading to simpler explanations and shorter programs than would be obtained
with imperative languages. Carefully selected examples, both new and standard,
reveal the commonalities and highlight the differences between algorithms. The
algorithm developments use equational reasoning where applicable, clarifying the
applicability conditions and correctness arguments. Every chapter concludes with
exercises (nearly 300 in total), each with complete answers, allowing the reader to
consolidate their understanding and apply the techniques to a range of problems.
The book serves students (both undergraduate and postgraduate), researchers,
teachers, and professionals who want to know more about what goes into a good
algorithm and how such algorithms can be expressed in purely functional terms.


ALGORITHM DESIGN WITH HASKELL
RICHARD BIRD
University of Oxford
JEREMY GIBBONS
University of Oxford

University Printing House, Cambridge CB2 8BS, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
314–321, 3rd Floor, Plot 3, Splendor Forum, Jasola District Centre, New Delhi – 110025, India
79 Anson Road, #06–04/06, Singapore 079906
Cambridge University Press is part of the University of Cambridge.
It furthers the University’s mission by disseminating knowledge in the pursuit of
education, learning, and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781108491617
DOI: 10.1017/9781108869041
© Richard Bird and Jeremy Gibbons 2020
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2020
Printed in the United Kingdom by TJ International Ltd, Padstow Cornwall
A catalogue record for this publication is available from the British Library.
ISBN 978-1-108-49161-7 Hardback
Cambridge University Press has no responsibility for the persistence or accuracy of
URLs for external or third-party internet websites referred to in this publication
and does not guarantee that any content on such websites is, or will remain,
accurate or appropriate.

For Stephen Gill (RB) and Sue Gibbons (JG).


Contents
Preface
page xiii
PART ONE
BASICS
1
1
Functional programming
5
1.1
Basic types and functions
5
1.2
Processing lists
7
1.3
Inductive and recursive deﬁnitions
9
1.4
Fusion
11
1.5
Accumulating and tupling
14
1.6
Chapter notes
16
References
16
Exercises
16
Answers
19
2
Timing
25
2.1
Asymptotic notation
25
2.2
Estimating running times
27
2.3
Running times in context
32
2.4
Amortised running times
34
2.5
Chapter notes
38
References
38
Exercises
38
Answers
40
3
Useful data structures
43
3.1
Symmetric lists
43

viii
Contents
3.2
Random-access lists
47
3.3
Arrays
51
3.4
Chapter notes
53
References
54
Exercises
54
Answers
56
PART TWO
DIVIDE AND CONQUER
59
4
Binary search
63
4.1
A one-dimensional search problem
63
4.2
A two-dimensional search problem
67
4.3
Binary search trees
73
4.4
Dynamic sets
81
4.5
Chapter notes
84
References
84
Exercises
85
Answers
87
5
Sorting
93
5.1
Quicksort
94
5.2
Mergesort
96
5.3
Heapsort
101
5.4
Bucketsort and Radixsort
102
5.5
Sorting sums
106
5.6
Chapter notes
110
References
110
Exercises
111
Answers
114
6
Selection
121
6.1
Minimum and maximum
121
6.2
Selection from one set
124
6.3
Selection from two sets
128
6.4
Selection from the complement of a set
132
6.5
Chapter notes
135
References
135
Exercises
135
Answers
137

Contents
ix
PART THREE
GREEDY ALGORITHMS
141
7
Greedy algorithms on lists
145
7.1
A generic greedy algorithm
145
7.2
Greedy sorting algorithms
147
7.3
Coin-changing
151
7.4
Decimal fractions in TEX
156
7.5
Nondeterministic functions and reﬁnement
161
7.6
Summary
165
7.7
Chapter notes
165
References
166
Exercises
166
Answers
170
8
Greedy algorithms on trees
177
8.1
Minimum-height trees
177
8.2
Huffman coding trees
187
8.3
Priority queues
196
8.4
Chapter notes
199
References
199
Exercises
199
Answers
201
9
Greedy algorithms on graphs
205
9.1
Graphs and spanning trees
205
9.2
Kruskal’s algorithm
208
9.3
Disjoint sets and the union–ﬁnd algorithm
211
9.4
Prim’s algorithm
215
9.5
Single-source shortest paths
219
9.6
Dijkstra’s algorithm
220
9.7
The jogger’s problem
224
9.8
Chapter notes
228
References
228
Exercises
229
Answers
231
PART FOUR
THINNING ALGORITHMS
237
10
Introduction to thinning
241
10.1
Theory
241
10.2
Paths in a layered network
244
10.3
Coin-changing revisited
248

x
Contents
10.4
The knapsack problem
252
10.5
A general thinning algorithm
255
10.6
Chapter notes
257
References
257
Exercises
257
Answers
261
11
Segments and subsequences
267
11.1
The longest upsequence
267
11.2
The longest common subsequence
270
11.3
A short segment with maximum sum
274
11.4
Chapter notes
280
References
281
Exercises
281
Answers
283
12
Partitions
289
12.1
Ways of generating partitions
289
12.2
Managing two bank accounts
291
12.3
The paragraph problem
294
12.4
Chapter notes
299
References
300
Exercises
300
Answers
303
PART FIVE
DYNAMIC PROGRAMMING
309
13
Eﬃcient recursions
313
13.1
Two numeric examples
313
13.2
Knapsack revisited
316
13.3
Minimum-cost edit sequences
319
13.4
Longest common subsequence revisited
322
13.5
The shuttle-bus problem
323
13.6
Chapter notes
326
References
326
Exercises
327
Answers
330
14
Optimum bracketing
335
14.1
A cubic-time algorithm
336
14.2
A quadratic-time algorithm
339
14.3
Examples
341

Contents
xi
14.4
Proof of monotonicity
345
14.5
Optimum binary search trees
347
14.6
The Garsia–Wachs algorithm
349
14.7
Chapter notes
358
References
358
Exercises
359
Answers
362
PART SIX
EXHAUSTIVE SEARCH
365
15
Ways of searching
369
15.1
Implicit search and the n-queens problem
369
15.2
Expressions with a given sum
376
15.3
Depth-ﬁrst and breadth-ﬁrst search
378
15.4
Lunar Landing
383
15.5
Forward planning
386
15.6
Rush Hour
389
15.7
Chapter notes
393
References
394
Exercises
395
Answers
398
16
Heuristic search
405
16.1
Searching with an optimistic heuristic
406
16.2
Searching with a monotonic heuristic
411
16.3
Navigating a warehouse
415
16.4
The 8-puzzle
419
16.5
Chapter notes
425
References
425
Exercises
426
Answers
428
Index
432


Preface
Our aim in this book is to provide an introduction to the principles of algorithm
design using a purely functional approach. Our language of choice is Haskell and all
the algorithms we design will be expressed as Haskell functions. Haskell has many
features for structuring function deﬁnitions, but we will use only a small subset of
them.
Using functions, rather than loops and assignment statements, to express algo-
rithms changes everything. First of all, an algorithm expressed as a function is
composed of other, more basic functions that can be studied separately and reused
in other algorithms. For instance, a sorting algorithm may be speciﬁed in terms of
building a tree of some kind and then ﬂattening it in some way. Functions that build
trees can be studied separately from functions that consume trees. Furthermore, the
properties of each of these basic functions and their relationship to others can be
captured with simple equational properties. As a result, one can talk and reason
about the ‘deep’ structure of an algorithm in a way that is not easily possible with
imperative code. To be sure, one can reason formally about imperative programs by
formulating their speciﬁcations in the predicate calculus, and using loop invariants
to prove they are correct. But, and this is the nub, one cannot easily reason about
the properties of an imperative program directly in terms of the language of its
code. Consequently, books on formal program design have a quite different tone
from those on algorithm design: they demand ﬂuency in both the predicate calculus
and the necessary imperative dictions. In contrast, many texts on algorithm design
traditionally present algorithms with a step-by-step commentary, and use informally
stated loop invariants to help one understand why the algorithm is correct.
With a functional approach there are no longer two separate languages to think
about, and one can happily calculate better versions of algorithms, or parts of
algorithms, by the straightforward process of equational reasoning. That, perhaps, is
the main contribution of this book. Although it contains a fair amount of equational
reasoning, we have tried to maintain a light touch. The plain fact of the matter is

xiv
Preface
that calculation is fun to do but boring to read – well, too much of it is. Although it
does not matter very much whether imperative algorithms are expressed in C or Java
or pseudo-code, the situation changes completely when algorithms are expressed
functionally.
Many of the problems considered in this book, especially in the later parts, begin
with a speciﬁcation of the task in hand, expressed as a composition of standard
functions such as maps, ﬁlters, and folds, as well as other functions such as perms for
computing all the permutations of a list, parts for computing all the partitions, and
mktrees for building all the trees of a particular kind. These component functions
are then combined, or fused, in various ways to construct a ﬁnal algorithm with the
required time complexity. A ﬁnal sorting algorithm may not refer to the underlying
tree, but the tree is still there in the structure of the algorithm. The notion of fusion
dominates the technical and mathematical aspects of the design process and is really
the driving force of the book.
The disadvantage for any author of taking a functional approach is that, be-
cause functional languages such as Haskell are not so well known as mainstream
procedural languages, one has to spend some time explaining them. That would
add substantially to the length of the book. The simple solution to this problem
is just to assume the necessary knowledge. There is a growing range of textbooks
on languages like Haskell, including our own Thinking Functionally with Haskell
(Cambridge University Press, 2014), and we will just assume the reader is familiar
with the necessary material. Indeed, the present book was designed as a companion
volume to the earlier book. A brief summary of what we do assume, and an even
briefer reprise of some essential ideas, is given in the ﬁrst chapter, but you will
probably not be able to learn enough about Haskell there to understand the rest
of the book. Even if you do know something about functional programming, but
not about how equational reasoning enters the picture (some books on functional
programming simply don’t mention equational reasoning), you will probably still
have to refer to our earlier book. In any case, the mathematics involved in equational
reasoning is neither new nor difﬁcult.
Books on algorithm design traditionally cover three broad areas: a collection of
design principles, a study of useful data structures, and a number of interesting and
intriguing algorithms that have been discovered over the centuries. Sometimes the
books are arranged by principles, sometimes by topic (such as graph algorithms, or
text algorithms), and sometimes by a mixture of both. This book mostly takes the
ﬁrst approach. It is devoted to ﬁve main design strategies underlying many effective
algorithms: divide and conquer, greedy algorithms, thinning algorithms, dynamic
programming, and exhaustive search. These are the design strategies that every
serious programmer should know. The middle strategy, on thinning algorithms,
is new, and serves in many problems as an alternative to dynamic programming.

Preface
xv
Each design strategy is allocated a part to itself, and the chapters on each strategy
cover a variety of algorithms from the well-known to the new. There is only a
little material on data structures – only as much as we need. In the ﬁrst part of the
book we do discuss some basic data structures, but we will also rely on some of
Haskell’s libraries of other useful ways of structuring data. One reason for doing so
is that we wanted the book not to be too voluminous; another reason is that there
does exist one text, Chris Okasaki’s Purely Functional Data Structures (Cambridge
University Press, 1998), that covers a lot of the material. Other books on functional
data structures have been published since we began writing this book, and more are
beginning to appear.
Another feature of this book is that, as well as some ﬁrm favourites, it describes a
number of algorithms that do not usually appear in books on algorithm design. Some
of these algorithms have been adapted, elaborated, and simpliﬁed from yet another
book published by Cambridge University Press: Pearls of Functional Algorithm
Design (2010). The reason for this novelty is simply to make the book entertaining
as well as instructive. Books on algorithm design are read, broadly speaking, by
three kinds of people: academics who need reference material, undergraduate or
graduate students on a course, and professional programmers simply for interest
and enjoyment. Most professional programmers do not design algorithms but just
take them from a library. Yet they too are a target audience for this book, because
sometimes professional programmers want to know more about what goes into a
good algorithm and how to think about them.
Algorithms in real life are a good deal more intricate than the ones presented in
this book. The shortest-path algorithm in a satellite navigation system is a good
deal more complicated than a shortest-path algorithm as presented in a textbook
on algorithm design. Real-life algorithms have to cope with the problems of scale,
with the effective use of a computer’s hardware, with user interfaces, and with many
other things that go into a well-designed and useful product. None of these aspects
is covered in the present book, nor indeed in most books devoted solely to the
principles of algorithm design.
There is another feature of this book that deserves mention: all exercises are
answered, if sometimes somewhat brieﬂy. The exercises form an integral part of
the text, and the questions and answers should be read even if the exercises are not
attempted. Rather than have a complete bibliography at the end of the book, each
chapter ends with references to (some of) the books and articles pertinent to the
chapter.
Most of the major programs in this book are available on the web site
www.cs.ox.ac.uk/publications/books/adwh

xvi
Preface
You can also use this site to see a list of all known errors, as well as report new ones.
We also welcome suggestions for improvement, including ideas for new exercises.
Acknowledgements
Preparation of this book has beneﬁted enormously from careful reading by Sue
Gibbons, Hsiang-Shang Ko, and Nicolas Wu. The manuscript was prepared using
the lhs2TEX system of Ralf Hinze and Andres L¨oh, which pretty-prints the Haskell
code and also allows it to be extracted and type-checked. The extracted code was
then tested using the wonderful QuickCheck tool developed by Koen Claessen and
John Hughes. Type-checking and QuickChecking the code has saved us from many
infelicities; any errors that remain are, of course, our own responsibility.
We also thank David Tranah and the team at Cambridge University Press for their
advice and hard work in the generation of the ﬁnal version of the text.
Richard Bird
Jeremy Gibbons

PART ONE
BASICS


3
What makes a good algorithm? There are as many answers to this question as there
are to the question of what makes a good cookbook recipe. Is the recipe clear and
easy to follow? Does the recipe use standard and well-understood techniques? Does
it use widely available ingredients? Is the preparation time reasonably short? Does it
involve many pots and pans and a lot of kitchen space? And so on and so on. Some
people when asked this question say that what is most important about a recipe
is whether the dish is attractive or not, a point we will try to bear in mind when
expressing our functional algorithms.
In the ﬁrst three chapters we review the ingredients we need for designing good
recipes for attractive algorithms in a functional kitchen, and describe the tools we
need for analysing their efﬁciency. Our functional language of choice is Haskell,
and the ingredients are Haskell functions. These ingredients and the techniques for
combining them are reviewed in the ﬁrst chapter. Be aware that the chapter is not
an introduction to Haskell; its main purpose is to outline what should be familiar
territory to the reader, or at least territory that the reader should feel comfortable
travelling in.
The second chapter concerns efﬁciency, speciﬁcally the running time of algo-
rithms. We will ignore completely the question of space efﬁciency, for the plain
fact of the matter is that executing a functional program can take up quite a lot of
kitchen space. There are methods for controlling the space used in evaluating a
functional expression, but we refer the reader to other books for their elaboration.
That chapter reviews asymptotic notation for stating running times, and explores
how recurrence relations, which are essentially recursive functions for determining
the running times of recursive functions, can be solved to give asymptotic estimates.
The chapter also introduces, albeit fairly brieﬂy, the notion of amortised running
times because it will be needed later in the book.
The ﬁnal chapter in this part introduces a small number of basic data structures
that will be needed at one or two places in the rest of the book. These are symmetric
lists, random-access lists, and purely functional arrays. Mostly we postpone discus-
sion of any data structure required to make an algorithm efﬁcient until the algorithm
itself is introduced, but these three form a coherent group that can be discussed
without having speciﬁc applications in mind.


Chapter 1
Functional programming
Haskell is a large and powerful language, brimming with clever ideas about how
to structure programs and possessing many bells and whistles. But in this book we
will use only a small subset of the host of available features. So, no Monads, no
Applicatives, no Foldables, and no Traversables. In this chapter we will spell out
what we do need to construct effective algorithms. Some of the material will be
revisited when particular problems are put under the microscope, so you should
regard the chapter primarily as a way to check your understanding of the basic ideas
of Haskell.
1.1 Basic types and functions
We will use only simple types, such as Booleans, characters, strings, numbers of
various kinds, and lists. Most of the functions we use can be found in Haskell’s
Standard Prelude (the Prelude library), or in the library Data.List. Be warned that
the deﬁnitions we give of some of these functions may not be exactly the deﬁnitions
given in these libraries: the library deﬁnitions are tuned for optimal performance
and ours for clarity. We will use type synonyms to improve readability, and data
declarations of new types, especially trees of various kinds. When necessary we
make use of simple type classes such as Eq, Ord, and Num, but we will not introduce
new ones. Haskell provides many kinds of number, including two kinds of integer,
Int and Integer, and two kinds of ﬂoating-point number, Float and Double. Elements
of Int are restricted in range, usually [−263,263) on 64-bit computers, though Haskell
compilers are only required to cover the range [−229,229). Elements of Integer are
unrestricted. We will rarely use the ﬂoating-point numbers provided by Float and
Double. In one or two places we will use Rational arithmetic, where a Rational
number is the ratio of two Integer values. Haskell does not have a type of natural

6
Functional programming
numbers,1 though the library Numeric.Natural does provide arbitrary-precision
ones. Instead, we will sometimes use the type synonym
type Nat = Int
Haskell cannot enforce the constraint that elements of Nat be natural numbers, and
we use the synonym purely to document intention. For example, we can assert
that length :: [a] →Nat because the length of a list, as deﬁned in the Prelude,
is a nonnegative element of Int. Haskell also provides unsigned numbers in the
Data.Word library. Elements of Word are unsigned numbers and can represent
natural numbers n in the range 0 ⩽n<264 on 64-bit machines. However, deﬁning
type Nat = Word would be inconvenient simply because we could not then assert
that length::[a] →Nat.
Most important for our purposes are the basic functions that manipulate lists.
Of these the most useful are map, ﬁlter, and folds of various kinds. Here is the
deﬁnition of map:
map::(a →b) →[a] →[b]
map f [ ]
= [ ]
map f (x:xs) = f x:map f xs
The function map applies its ﬁrst argument, a function, to every element of its
second argument, a list. The function ﬁlter is deﬁned as follows:
ﬁlter ::(a →Bool) →[a] →[a]
ﬁlter p [ ]
= [ ]
ﬁlter p (x:xs) = if p x then x:ﬁlter p xs else ﬁlter p xs
The function ﬁlter ﬁlters a list, retaining only those elements that satisfy the given
test. There are various fold functions on lists, most of which will be explained in
due course. Two of the important ones are foldr and foldl. The former is deﬁned as
follows:
foldr ::(a →b →b) →b →[a] →b
foldr f e [ ]
= e
foldr f e (x:xs) = f x (foldr f e xs)
The function foldr folds a list from right to left, starting with a value e and using a
binary operator ⊕to reduce the list to a single value. For example,
foldr (⊕) e [x,y,z] = x⊕(y⊕(z⊕e))
In particular, foldr (:) [ ] xs = xs for all lists xs, including inﬁnite lists. However, we
will not make much use of inﬁnite lists in what follows, except for idioms such as
1 In the documentation for the GHC libraries, there is the statement “It would be very natural to add a type
Natural providing an unbounded size unsigned integer, just as Prelude.Integer provides unbounded size signed
integers. We do not do that yet since there is no demand for it.” Maybe this book will create such a demand.

1.2 Processing lists
7
label::[a] →[(Nat,a)]
label xs = zip [0..] xs
As another example, we can write
length::[a] →Nat
length = foldr succ 0 where succ x n = n+1
The second main function, foldl, folds a list from left to right:
foldl::(b →a →b) →b →[a] →b
foldl f e [ ]
= e
foldl f e (x:xs) = foldl f (f e x) xs
Thus
foldl (⊕) e [x,y,z] = ((e⊕x)⊕y)⊕z
For example, we could also write
length::[a] →Nat
length = foldl succ 0 where succ n x = n+1
Note that foldl returns a well-deﬁned value only on ﬁnite lists; evaluation of foldl
on an inﬁnite list will never terminate. There is an alternative deﬁnition of foldl,
namely
foldl f e = foldr (ﬂip f) e·reverse
where ﬂip is a useful prelude function deﬁned by
ﬂip::(a →b →c) →b →a →c
ﬂip f x y = f y x
Since one can reverse a list in linear time, this deﬁnition is asymptotically as fast as
the former. However, it involves two traversals of the input, one to reverse it and the
second to fold it.
1.2 Processing lists
The difference between foldr and foldl prompts a general observation. When a
programmer brought up in the imperative programming tradition meets functional
programming for the ﬁrst time, they are likely to feel that many computations seem
to be carried out in the wrong order. Recursion has been described as the curious
process of reaching one’s goal by walking backwards towards it. Speciﬁcally, lists
often seem to be processed from right to left when the natural way surely appears to
be from left to right. Appeals to naturalness are often suspicious, and appearances
can be deceptive. We normally read an English sentence from left to right, but when
we encounter a phrase such as “a lovely little old French silver butter knife” the
adjectives have to be applied from right to left. If the knife was made of French

8
Functional programming
silver, but not necessarily made in France, we have to write “a lovely little old
French-silver butter knife” to avoid ambiguity. Mathematical expressions too are
usually understood from right to left, certainly those involving a chain of functional
compositions. As to deceptiveness, the deﬁnition
head = foldr (≪) ⊥where x ≪y = x
though a little strange is certainly correct and takes constant time. The evaluation
of foldr (≪), conceptually from right to left, is abandoned after the ﬁrst element is
encountered. Thus
head (x:xs) = foldr (≪) ⊥(x:xs)
= x ≪foldr (≪) ⊥xs
= x
The last step follows from the fact that Haskell is a lazy language in which eval-
uations are performed only when needed, so evaluation of ≪does not require
evaluation of its second argument.
Sometimes the direction of travel is important. For example, consider the follow-
ing two deﬁnitions of concat:
concat1,concat2 ::[[a]] →[a]
concat1 = foldr (++) [ ]
concat2 = foldl (++) [ ]
We have concat1 xss = concat2 xss for all ﬁnite lists xss (see Exercise 1.10), but
which deﬁnition is better? We will look at the precise running times of the two
functions in the following chapter, but here is one way to view the problem. Imagine
a long table on which there are a number of piles of documents. You have to assemble
these documents into one big pile ensuring that the correct order is maintained, so
the second pile (numbering from left to right) has to go under the ﬁrst pile, the third
pile under the second pile, and so on. You could start from left to right, picking up
the ﬁrst pile, putting it on top of the second pile, picking the combined pile up and
putting it on top of the third pile, and so on. Or you could start at the other end,
placing the penultimate pile on the last pile, the antepenultimate pile on top of that,
and so on (even English words are direction-biased: the words ‘ﬁrst’, ‘second’, and
‘third’ are simple, but ‘penultimate’ and ‘antepenultimate’ are not). The left to right
solution involves some heavy lifting, particularly at the last step when a big pile
of documents has to be lifted up and placed on the last pile, but the right to left
solution involves picking up only one pile at each step. So concat1 is potentially a
much more efﬁcient way to concatenate a list of lists than concat2.
Here is another example. Consider the problem of breaking a list of words into
a list of lines, ensuring that the width of each line is at most some given bound.
This problem is known as the paragraph problem, and there is a section devoted

1.3 Inductive and recursive deﬁnitions
9
to it in Chapter 12. It seems natural to process the input from left to right, adding
successive words to the end of the current line until no more words will ﬁt, in
which case a new line is started. This particular algorithm is a greedy one. There
are also non-greedy algorithms for the paragraph problem that process words from
right to left. Part Three of the book is devoted to the study of greedy algorithms.
Nevertheless, these two examples apart, the direction of travel is often unimportant.
The direction of travel is also related to another concept in algorithm design,
the notion of an online algorithm. An online algorithm is one that processes a list
without having the entire list available from the start. Instead, the list is regarded
as a potentially inﬁnite stream of values. Consequently, any online algorithm for
solving a problem for a given stream also has to solve the problem for every preﬁx
of the stream. And that means the stream has to be processed from left to right. In
contrast, an ofﬂine algorithm is one that is given the complete list to start with, and
can process the list in any order it wants. Online algorithms can usually be deﬁned
in terms of another basic Haskell function scanl, whose deﬁnition is as follows:
scanl::(b →a →b) →b →[a] →[b]
scanl f e [ ]
= [e]
scanl f e (x:xs) = e:scanl f (f e x) xs
For example,
scanl (⊕) e [x,y,z,...] = [e,e⊕x,(e⊕x)⊕y,((e⊕x)⊕y)⊕z,...]
In particular, scanl can be applied to an inﬁnite list, producing an inﬁnite list as
result.
1.3 Inductive and recursive deﬁnitions
While most functions make use of recursion, the nature of the recursion is different
in different functions. The functions map, ﬁlter, and foldr all make use of structural
recursion. That is, the recursion follows the structure of lists built from the empty
list [ ] and the cons constructor (:). There is one clause for the empty list and another,
recursive clause for x:xs in terms of the value of the function for xs. We will call
such deﬁnitions inductive deﬁnitions. Most inductive deﬁnitions can be expressed
as instances of foldr. For example, both map and ﬁlter can be so expressed (see the
exercises).
Here is another example, an inductive deﬁnition of the function perms that returns
a list of all the permutations of a list (we call it perms1 because later on we will
meet another deﬁnition, perms2):
perms1 [ ]
= [[ ]]
perms1 (x:xs) = [zs | ys ←perms1 xs,zs ←inserts x ys]

10
Functional programming
The permutations of a nonempty list are obtained by taking each permutation of
the tail of the list and returning all the ways the ﬁrst element can be inserted. The
function inserts is deﬁned by
inserts::a →[a] →[[a]]
inserts x [ ]
= [[x]]
inserts x (y:ys) = (x:y:ys):map (y:) (inserts x ys)
For example,
inserts 1 [2,3] = [[1,2,3],[2,1,3],[2,3,1]]
The deﬁnition of perms1 uses explicit recursion and a list comprehension, but
another way is to use a foldr:
perms1 = foldr step [[ ]] where step x xss = concatMap (inserts x) xss
The useful function concatMap is deﬁned by
concatMap::(a →[b]) →[a] →[b]
concatMap f = concat ·map f
Observe that since
step x xss = (concatMap·inserts) x xss
the deﬁnition of perms1 can be expressed even more brieﬂy as
perms1 = foldr (concatMap·inserts) [[ ]]
The idiom foldr (concatMap·steps) e will be used frequently in later chapters for
various deﬁnitions of steps and e, so keep the abbreviation in mind.
Here is another way of generating permutations, one that is recursive rather than
inductive:
perms2 [ ] = [[ ]]
perms2 xs = [x:zs | (x,ys) ←picks xs,zs ←perms2 ys]
picks::[a] →[(a,[a])]
picks [ ]
= [ ]
picks (x:xs) = (x,xs):[(y,x:ys) | (y,ys) ←picks xs]
The function picks picks an arbitrary element from a list in all possible ways,
returning both the element and what remains. The function perms2 computes a
permutation by picking an arbitrary element of a nonempty list as a ﬁrst element,
and following it with a permutation of the rest of the list.
The function perms2 uses a list comprehension, but an equivalent way is to write
perms2 [ ] = [[ ]]
perms2 xs = concatMap subperms (picks xs)
where subperms (x,ys) = map (x:) (perms2 ys)

1.4 Fusion
11
Expressing perms2 in this way rather than by a list comprehension helps with
equational reasoning, and also with the analysis of its running time. We will return
to both perms1 and perms2 in the following chapter.
The different styles, recursive or inductive, of the deﬁnitions of basic combinato-
rial functions, such as permutations, partitions, or subsequences, lead to different
kinds of ﬁnal algorithm. For example, divide-and-conquer algorithms are usually
recursive, while greedy and thinning algorithms are usually inductive. To appreciate
that there may be different algorithms for one and the same problem, one has to go
back to the deﬁnitions of the basic functions used in the speciﬁcation of the problem
and see if they can be deﬁned differently. For example, the inductive deﬁnition of
perms1 leads to Insertion sort, while the recursive deﬁnition of perms2 leads to Se-
lection sort. These two sorting algorithms will be introduced in the context of greedy
algorithms in Part Three. The general point is a key one for functional algorithm
design: different solutions for problems arise simply because there are different
but equally clear deﬁnitions of one or more of the basic functions describing the
solution.
While functional programming relies solely on recursion to deﬁne arbitrarily long
computations, imperative programming can also make use of loops of various kinds,
including while and until loops. We can deﬁne and use loops in Haskell too. For
example,
until::(a →Bool) →(a →a) →a →a
until p f x = if p x then x else until p f (f x)
is a recursive deﬁnition of the function until that repeatedly applies a function to a
value until the result satisﬁes some condition. We will encounter until again later in
the book. Given until we can deﬁne while by
while p = until (not ·p)
We can also deﬁne a functional version of simple for-loops in which a function is
applied to an argument a speciﬁed number of times (see the exercises).
1.4 Fusion
The most powerful technique for constructing efﬁcient algorithms lies in our ability
to fuse two computations together into one computation. Here are three simple
examples:
map f ·map g
= map (f ·g)
concatMap f ·map g = concatMap (f ·g)
foldr f e·map g
= foldr (f ·g) e

12
Functional programming
The ﬁrst equation says that the two-step process of applying one function to every
element of a list, and then applying a second function to every element of the
result, can be replaced by a one-step traversal in which the composition of the two
functions is applied to each element. The second equation is an instance of the ﬁrst
one, and the third is yet another example of when two traversals can be replaced by
a single traversal.
Here is an another example of a fusion law, one for you to solve:
foldr f e·concat = ????
Pause for a minute or so to try and complete the right-hand side. It is a good test of
your understanding of the material so far. But do not be discouraged if you cannot
ﬁnd the answer, because it is not too obvious and many experienced functional
programmers would fail to spot it. In a moment we will show how this particular
fusion rule follows from one single master rule. Indeed, that is how we ourselves
know the right-hand side, not by memorising it but by reconstructing it from the
master rule.
You probably paused for a short time, gave up and then read on. But you don’t
get away that easily. Try this simpler version ﬁrst:
foldr f e (xs++ys) = ????
Having answered this question, can you now answer the ﬁrst one?
The answers to both questions will be given shortly. The master fusion rule is the
fusion law of foldr. This law states that
h (foldr f e xs) = foldr g (h e) xs
for all ﬁnite lists xs provided
h (f x y) = g x (h y)
for all x and y. The proviso is called the fusion condition. Without one extra proviso,
the restriction to ﬁnite lists is necessary (see the exercises). The proof of the fusion
rule is by induction on the structure of a list. There are two cases, a base case and
an induction step. The base case is
h (foldr f e [ ])
=
{ deﬁnition of foldr }
h e
=
{ deﬁnition of foldr }
foldr g (h e) [ ]
The induction step is

1.4 Fusion
13
h (foldr f e (x:xs))
=
{ deﬁnition of foldr }
h (f x (foldr f e xs))
=
{ fusion condition }
g x (h (foldr f e xs))
=
{ induction }
g x (foldr g (h e) xs)
=
{ deﬁnition of foldr }
foldr g (h e) (x:xs)
This completes the induction and the proof of the fusion law of foldr.
Returning to our two problems, the answer to the easier one is
foldr f e (xs++ys) = foldr f (foldr f e ys) xs
For the more difﬁcult one we have concat = foldr (++) [ ], and the fusion law says
that
h (foldr (++) [ ] xss) = foldr g (h [ ]) xss
provided g satisﬁes
h (xs++ys) = g xs (h ys)
But h = foldr f e, and the solution to the easier problem says we can satisfy the
fusion condition by taking g = ﬂip (foldr f). Thus
foldr f e·concat = foldr (ﬂip (foldr f)) e
over ﬁnite lists. Well done if you got it.
Before ending the section, let us make a remark about styles of reasoning. The
proof of the fusion rule for foldr was carried out at the point-level, meaning that all
functions were fully applied to their arguments. It is also called point-wise reasoning.
Contrast this with the following proof:
map f ·ﬁlter p·concat
=
{ distributing ﬁlter over concat }
map f ·concat ·map (ﬁlter p)
=
{ distributing map over concat }
concat ·map (map f)·map (ﬁlter p)
=
{ property of map }
concat ·map (map f ·ﬁlter p)
=
{ deﬁnition of concatMap }
concatMap (map f ·ﬁlter p)
This calculation is carried out at the function level using functional composition
as the basic combining form. It is also called point-free reasoning (and sometimes

14
Functional programming
pointless reasoning by wags). When applicable, point-free reasoning is more attrac-
tive than point-wise reasoning, if only because there are fewer parentheses to write.
For this reason we often write things like
h·foldr f e = foldr g (h e)
without mentioning the list to which both sides are applied. However, as we said
above, without an additional proviso the fusion law is true only for ﬁnite lists. For
that reason we often state point-free equations with a rider “for all ﬁnite lists” or
“over ﬁnite lists”. We did exactly this at the end of the previous section, though it so
happens that the equation
foldr f e·concat = foldr (ﬂip (foldr f)) e
is true for all lists, inﬁnite as well as ﬁnite.
1.5 Accumulating and tupling
Sometimes a clean and simple deﬁnition has to be tweaked to make it efﬁcient. Here
is one rather artiﬁcial example. Given a list xss of lists of integers, consider the
problem of concatenating the shortest preﬁx of xss whose total sum is positive. If
no sum is positive, then the whole list is concatenated. Let collapse be the function
that carries out this process, so for example
collapse [[1],[−3],[2,4]]
= [1]
collapse [[−2,1],[−3],[2,4]] = [−2,1,−3,2,4]
collapse [[−2,1],[3],[2,4]]
= [−2,1,3]
The simplest way to deﬁne collapse is in terms of a helper function which accumu-
lates the required preﬁx in its ﬁrst argument:
collapse::[[Int]] →[Int]
collapse xss = help [ ] xss
help xs xss = if sum xs>0 ∨null xss then xs
else help (xs++head xss) (tail xss)
Ignore completely what this particular function might be useful for and concentrate
only on the fact that collapse appears to be doing a lot of work in recomputing sums.
As each list is constructed in the ﬁrst argument (the accumulating parameter) of
help, its sum is recomputed from scratch. We can do better by tupling each list with
its sum. Replace the deﬁnition of collapse with the following one:
collapse xss
= help (0,[ ]) (labelsum xss)
labelsum xss
= zip (map sum xss) xss
help (s,xs) xss
= if s>0 ∨null xss then xs
else help (cat (s,xs) (head xss)) (tail xss)
cat (s,xs) (t,ys) = (s+t,xs++ys)

1.5 Accumulating and tupling
15
Each list is paired with its sum and this pairing is threaded through the computation.
There are no sum computations in the revised deﬁnition of help but only in the
function labelsum. There is, however, a single + operation in the deﬁnition of cat.
In the worst case, the cost of computing these sums is now linear in the total length
of the input, whereas with the previous deﬁnition it was quadratic.
The remaining problem with collapse is the ++ operation in cat. The concatena-
tions are performed from left to right. For example,
collapse [[−5,3],[−2],[−4],[−4,1]] =
((([ ]++[−5,3])++[−2])++[−4])++[−4,1]
As we have seen, this is an inefﬁcient way to concatenate lists. One way to solve
the problem is to replace the accumulating list in the deﬁnition of help with an
accumulating function:
collapse xss
= (help (0,id) (labelsum xss)) [ ]
help (s,f) xss = if s>0 ∨null xss then f else help (s+t,f ·(xs++)) (tail xss)
where (t,xs) = head xss
At the end of the computation the accumulating function is applied to the empty list.
For example, we now have
collapse [[−5,3],[−2],[−4],[−4,1]]
= (([−5,3]++)·([−2]++)·([−4]++)·([−4,1]++)) [ ]
= [−5,3]++([−2]++([−4]++([−4,1]++[ ])))
The concatenation is now from right to left and is more efﬁcient. This trick of using
an accumulating function to achieve efﬁcient concatenation will appear again at
various places in the book.
The general point we want to make with this example is the idea of tupling,
whereby a computation can be made more efﬁcient by tupling values of interest
together and threading them through the computation. In that way such values do not
have to be recomputed from scratch each time. Tupling is, in fact, a simple version
of the idea of memoising the values of a function to save computing them more than
once. Memoisation will be treated more fully in Part Five on dynamic programming
algorithms. Generally, though not always, we will leave such tupling optimisations
to the last stage in the design of an efﬁcient algorithm because they add little by
way of understanding and can clutter and obscure the code. Premature optimisation
is the root of all evil in programming. We mention the tupling optimisation now
because it is used a number of times in the ﬁnal versions of some algorithms.
The twin techniques of accumulating parameters and tupling are useful devices
for improving the running time of algorithms, but the mother of all such devices is
fusion. Practically every algorithm in this book beneﬁts from fusion of some kind,
so if you take away anything from this chapter, make sure to include the two central

16
Functional programming
principles of good algorithm design: (i) formulate the problem in terms of basic,
well-understood ingredients; and (ii) fuse the components into a dish that is ﬁnally
ready to leave the kitchen.
1.6 Chapter notes
The particular version of Haskell used in this book is Haskell 8.0, released in May
2016. The website www.haskell.org shows you how to download the Haskell
Platform, a bundled system with lots of useful libraries. In addition to a com-
piler, the platform also provides an interpreter GHCi that we will use to illustrate
some computations. The website also contains a wealth of material about Haskell,
including a complete list of books on Haskell programming and a number of on-
line tutorials about various aspects and features of the language. The wikibook
en.wikibooks.org/wiki/Haskell also contains much useful information. Our
own book on functional programming in Haskell [1] gives some help on how the
use of a special function seq can be deployed to control the amount of space it takes
to evaluate (some) functional expressions.
The quote about premature optimisation being the root of all evil in programming
is due to Don Knuth [4], though it may have appeared earlier. The phrase “a
lovely little old rectangular green French silver whittling knife” was used by Mark
Forsyth [2] to show that adjectives in English absolutely have to be in the following
order: opinion, size, age, shape, colour, origin, material, purpose. Any deviation from
this order just sounds wrong. However, the linguistic device known as hyperbaton
changes the word order for rhetorical effect.
The technique of using an accumulating function to change the order in which
concatenations are performed was ﬁrst written up in [3], though it was used earlier
by a number of programmers.
References
[1]
Richard Bird. Thinking Functionally with Haskell. Cambridge University Press,
Cambridge, 2014.
[2]
Mark Forsyth. The Elements of Eloquence. Icon Books, London, 2013.
[3]
John Hughes. A novel representation of lists and its application to the function
“reverse”. Information Processing Letters, 22(3):141–144, 1986.
[4]
Donald E. Knuth. Structured programming with go to statements. ACM Computing
Surveys, 6(4):261–301, 1974.
Exercises
Exercise 1.1 Here are some other basic list-processing functions we will need:

Exercises
17
maximum, take, takeWhile, inits, splitAt, null, elem, zipWith,
minimum, drop, dropWhile, tails, span,
all,
(!!)
To check your understanding, just give appropriate types.
Exercise 1.2 Trawling through Data.List we discovered the function
uncons::[a] →Maybe (a,[a])
of whose existence we were quite unconscious. Guess the deﬁnition of uncons.
Exercise 1.3 The library Data.List does not provide functions
wrap
::a →[a]
unwrap::[a] →a
single ::[a] →Bool
for wrapping a value into a singleton list, unwrapping a singleton list into its sole
occupant, and testing a list for being a singleton. This is a pity, for the three functions
can be very useful on occasions and will appear a number of times in the rest of this
book. Give appropriate deﬁnitions.
Exercise 1.4 Write down a deﬁnition of reverse that takes linear time. One possi-
bility is to use a foldl.
Exercise 1.5 Express both map and ﬁlter as an instance of foldr.
Exercise 1.6 Express foldr f e·ﬁlter p as an instance of foldr.
Exercise 1.7 The function takeWhile returns the longest initial segment of a list all
of whose elements satisfy a given test. Moreover, its running time is proportional to
the length of the result, not the length of the input. Express takeWhile as an instance
of foldr, thereby demonstrating once again that a foldr need not process the whole
of its argument before terminating.
Exercise 1.8 The Data.List library contains a function dropWhileEnd which drops
the longest sufﬁx of a list all of whose elements satisfy a given Boolean test. For
example
dropWhileEnd even [1,4,3,6,2,4] = [1,4,3]
Deﬁne dropWhileEnd as an instance of foldr.
Exercise 1.9 An alternative deﬁnition of foldr is
foldr f e xs = if null xs then e else f (head xs) (foldr f e (tail xs))
Dually, an alternative deﬁnition of foldl is
foldl f e xs = if null xs then e else f (foldl f e (init xs)) (last xs)
where last and init are dual to head and tail. What is the problem with this deﬁnition
of foldl?

18
Functional programming
Exercise 1.10 Bearing the examples
foldr (⊕) e [x,y,z] = x⊕(y⊕(z⊕e))
foldl (⊕) e [x,y,z] = ((e⊕x)⊕y)⊕z
in mind, under what simple conditions on ⊕and e do we have
foldr (⊕) e xs = foldl (⊕) e xs
for all ﬁnite lists xs?
Exercise 1.11 Given a list of digits representing a natural number, construct a
function integer which converts the digits into that number. For example,
integer [1,4,8,4,9,3] = 148493
Next, given a list of digits representing a real number r in the range 0 ⩽r <
1, construct a function fraction which converts the digits into the corresponding
fraction. For example,
fraction [1,4,8,4,9,3] = 0.148493
Exercise 1.12 Complete the right-hand sides of
map (foldl f e)·inits = ????
map (foldr f e)·tails = ????
Exercise 1.13 Deﬁne the function
apply::Nat →(a →a) →a →a
that applies a function a speciﬁed number of times to a value.
Exercise 1.14 Can the function inserts associated with the inductive deﬁnition of
perms1 be expressed as an instance of foldr?
Exercise 1.15 Give a deﬁnition of remove for which
perms3 [ ] = [[ ]]
perms3 xs = [x:ys | x ←xs,ys ←perms3 (remove x xs)]
computes the permutations of a list. Is the ﬁrst clause necessary? What is the type
of perms3, and can one generate the permutations of a list of functions with this
deﬁnition?
Exercise 1.16 What extra condition is needed for the fusion law of foldr to be valid
over all lists, ﬁnite and inﬁnite?
Exercise 1.17 As stated, the fusion law for foldr requires the proviso
h (f x y) = g x (h y)
for all x and y. The proviso is actually too general. Can you spot what the necessary
and sufﬁcient fusion condition is? To help you, here is an example, admittedly

Answers
19
a rather artiﬁcial one, where a more restricted version of the fusion condition is
necessary. Deﬁne the function replace by
replace x = if even x then x else 0
We claim that replace·foldr f 0 = foldr f 0 on ﬁnite lists, where
f ::Int →Int →Int
f x y = 2×x+y
Prove this fact by using the more restricted proviso.
Exercise 1.18 We referred to the fusion rule of foldr as the master fusion rule, but
there is another master rule, the fusion rule for foldl. What is this rule?
Exercise 1.19 Is the following statement true or false? “The original deﬁnition of
collapse is more efﬁcient than the optimised versions in the best case, when the ﬁrst
preﬁx has positive sum, because the sums of the remaining lists are not required. In
the optimised version the sums of all the component lists are required.”
Exercise 1.20 Find a deﬁnition of op so that
concat xss = foldl op id xss [ ]
Exercise 1.21 A list of numbers is said to be steep if each number is greater than the
sum of the elements following it. Give a simple deﬁnition of the Boolean function
steep for determining whether a sequence of numbers is steep. What is the running
time and how can you improve it by tupling?
Answers
Answer 1.1 We have
maximum,minimum ::Ord a ⇒[a] →a
take,drop
::Nat →
[a] →[a]
takeWhile,dropWhile::(a →Bool) →[a] →[a]
inits,tails
::[a] →[[a]]
splitAt
::Nat →
[a] →([a],[a])
span
::(a →Bool) →[a] →([a],[a])
null
::
[a] →Bool
all
::(a →Bool) →[a] →Bool
elem
::Eq a ⇒a →
[a] →Bool
(!!)
::[a] →Nat →a
zipWith
::(a →b →c) →[a] →[b] →[c]
In Haskell 8.0 some of these functions have more general types. For example,
maximum,minimum::(Foldable t,Ord a) ⇒t a →a

20
Functional programming
The type class Foldable describes data structures that can be folded. For instance
Foldable t contains a method foldr of type
foldr ::(a →b →b) →b →t a →b
Lists are foldable and we will use foldr only on lists.
Answer 1.2 The function uncons is deﬁned by
uncons [ ]
= Nothing
uncons (x:xs) = Just (x,xs)
Answer 1.3 The simple deﬁnitions are
wrap x
= [x]
unwrap [x] = x
single [x]
= True
single
= False
Note that head and unwrap are different functions.
Answer 1.4 The deﬁnition is
reverse::[a] →[a]
reverse = foldl (ﬂip (:)) [ ]
Answer 1.5 We have
map f = foldr op [ ] where op x xs = f x:xs
ﬁlter p = foldr op [ ] where op x xs = if p x then x:xs else xs
Answer 1.6 We have foldr f e·ﬁlter p = foldr op e, where
op x y = if p x then f x y else y
Answer 1.7 We have
takeWhile::(a →Bool) →[a] →[a]
takeWhile p = foldr op [ ] where op x xs = if p x then x:xs else [ ]
For example,
takeWhile even [2,3,4,5]
= op 2 (takeWhile even [3,4,5])
= 2:takeWhile even [3,4,5]
= 2:op 3 (takeWhile even [4,5])
= 2:[ ]

Answers
21
Answer 1.8 We have
dropWhileEnd ::(a →Bool) →[a] →[a]
dropWhileEnd p = foldr op [ ]
where op x xs = if p x ∧null xs then [ ] else x:xs
Answer 1.9 While
head ::[a] →a
head (x:xs) = x
tail::[a] →[a]
tail (x:xs)
= xs
both take constant time, the dual functions
last ::[a] →a
last [x]
= x
last (x:xs) = last xs
init ::[a] →[a]
init [x]
= [ ]
init (x:xs) = x:init xs
both take linear time because the whole list has to be traversed. That makes the
alternative deﬁnition of foldl very inefﬁcient.
Answer 1.10 One simple condition is that ⊕is associative operation with identity
element e. For example, addition is an associative operation with identity element 0,
so foldr (+) 0 xs = foldl (+) 0 xs for all ﬁnite lists xs.
Answer 1.11 For the function that converts a decimal to an integer, a deﬁnition by
foldl is appropriate:
integer = foldl shiftl 0 where shiftl n d = 10×n+d
For the function that converts a decimal to a fraction, a deﬁnition by foldr is
appropriate:
fraction = foldr shiftr 0 where shiftr d x = (fromIntegral d +x)/10
The use of fromIntegral is necessary to convert a digit (an integer) to a ﬂoating-point
number since division is not deﬁned on integers.
Answer 1.12 We have
map (foldl f e)·inits = scanl f e
map (foldr f e)·tails = scanr f e
where scanr is a prelude function, dual to scanl. These results are known collectively
as the Scan Lemma and can be very useful in text-processing algorithms such as
those in Chapter 11.

22
Functional programming
Answer 1.13 There are two possible deﬁnitions:
apply 0 f = id
apply n f = f ·apply (n−1) f
apply 0 f = id
apply n f = apply (n−1) f ·f
Functional composition is an associative operation, so the deﬁnitions are equivalent.
Answer 1.14 Yes it can, although the deﬁnition is not immediately obvious:
inserts x = foldr step [[x]]
where step y yss = (x:y:ys):map (y:) yss
where ys = tail (head yss)
This deﬁnition relies on the fact that head (inserts x ys) = x:ys.
Answer 1.15 The function remove removes the ﬁrst occurrence of a given element
in a given list:
remove::Eq a ⇒a →[a] →[a]
remove x [ ]
= [ ]
remove x (y:ys) = if x == y then ys else y:remove x ys
The ﬁrst clause of perms3 is indeed necessary; without it we have perms3 [ ] = [ ].
From this one can show that perms3 returns the empty list for all arguments. The
type of perms3 is perms3 :: Eq a ⇒[a] →[[a]], so, no, one cannot generate the
permutations of a list of functions using this deﬁnition since functions cannot be
tested for equality.
Answer 1.16 We would have to show the validity of fusion when the input is the
undeﬁned list ⊥. Since foldr f e ⊥= ⊥we require that h has to be a strict function,
returning the undeﬁned value if the argument is undeﬁned.
Answer 1.17 Taking the example ﬁrst, the original fusion condition requires that
replace (2×x+y) = 2×x+replace y
which is not true if y is odd. But we do have
replace (f x (foldr f 0 xs)) = f x (replace (foldr f 0 xs))
because foldr f 0 xs is always an even number. The more general fusion law, which
we will call context-sensitive fusion, is that
h (foldr f e xs) = foldr g (h e) xs
for all ﬁnite lists xs provided that
h (f x (foldr f e xs)) = g x (h (foldr f e xs))
for all x and ﬁnite lists xs. Context-sensitive fusion will be needed in order to show
that some problems can be solved by greedy algorithms.

Answers
23
Answer 1.18 We have
h (foldl f e xs) = foldl g (h e) xs
for all ﬁnite lists xs provided that
h (f y x) = g (h y) x
for all y and x. The proof that this proviso is sufﬁcient is by induction, but we have to
be careful and ﬁrst generalise the induction hypothesis by replacing e by an arbitrary
value y:
h (foldl f y xs) = foldl g (h y) xs
Then we have
h (foldl f y (x:xs))
=
{ deﬁnition of foldl }
h (foldl f (f y x) xs)
=
{ induction }
foldl g (h (f y x)) xs
=
{ proviso }
foldl g (g (h y) x) xs
=
{ deﬁnition of foldl }
foldl g (h y) (x:xs)
The induction step would not be valid without the generalisation.
Answer 1.19 No, it is false. Haskell is a lazy language in which only those values
which contribute to the answer are computed. In the best case of collapse the
remaining sums are discarded so they are never computed.
Answer 1.20 We can take op f xs ys = f (xs ++ ys), though of course we cannot
concatenate an inﬁnite list of lists this way.
Answer 1.21 A simple deﬁnition:
steep [ ]
= True
steep (x:xs) = x>sum xs ∧steep xs
This deﬁnition computes sum on every tail of the list. Since computation of sum
takes linear time, computation of steep takes quadratic time. To obtain a linear-time
algorithm we can tuple sum and steep, leading to the deﬁnition
steep = snd ·faststeep
faststeep [ ]
= (0,True)
faststeep (x:xs) = (x+s,x>s ∧b) where (s,b) = faststeep xs


Chapter 2
Timing
The secret of a successful algorithm, like the secret of successful comedy, is all
about timing. In this chapter we review the tools needed for analysing the running
times of functional algorithms and illustrate their use on one or two examples.
The criteria for success should include space as well as time, but analysing the
space requirements of a functional algorithm can be a complicated process, so we
will ignore it almost entirely. The three tools we need are: asymptotic notation
for describing the growth of functions; recurrence relations for estimating running
times; and the notion of amortised running times.
2.1 Asymptotic notation
Asymptotic notation is used to compare the order of growth of functions without
worrying about the constants involved. There are three kinds of asymptotic notation:
Θ, O, and Ω (‘big theta’, ‘big omicron’, and ‘big omega’).
Let f and g be two functions taking natural numbers as arguments and returning
nonnegative results, not necessarily integers. We say that f is of order g, and write
f = Θ(g), if there are positive constants C and D and a number n0 such that
Cg(n) ⩽f(n) ⩽Dg(n)
for all n>n0. For example1
n (n+1)/2
= Θ(n2)
n3 +n2 +n log n
= Θ(n3)
n (1+1/2+1/3+···) = Θ(n log n)
The notation is abused to the extent that we write f(n) = Θ(g(n)) rather than the
more correct f = Θ(g). In particular, Θ(1) stands for an anonymous function whose
values lie between two positive constants. For instance, we can be conﬁdent that
1 In this book, except where otherwise stated, logarithms are taken in base two, so log n means log2 n.

26
Timing
taking the head of a list is a constant-time operation. Exactly what this constant is
does not really matter – well, as long as it is small. Instead we say that head takes
Θ(1) steps.
If we want only to put an upper bound on the values of a function, then we can
use O notation. We say that f is of order at most g, and write f = O(g), if there is a
positive constant C and a natural number n0 such that
f(n) ⩽Cg(n)
for all n>n0. In particular, O(1) stands for an anonymous function whose values
are bounded above by some positive constant. For example, the running time of
takeWhile on a list of length n is O(n) steps, assuming the test takes constant time.
In the worst case the running time is Θ(n) steps but in the best case, when the ﬁrst
element does not pass the test, the running time is Θ(1) steps.
A running time of O(n2) does not imply that the running time is not also O(n);
so to claim, for example, that a sorting algorithm is inefﬁcient because its running
time is O(n2) is mathematically illiterate. Instead we can use Ω notation. We say
that f is of order at least g, and write f = Ω(g), if there is a positive constant C and
a natural number n0 such that
f(n) ⩾Cg(n)
for all n ⩾n0. It follows that f = Θ(g) if and only if f = O(g) and f = Ω(g). Use of
Ω notation is therefore for putting lower bounds on the values of a function. It is
legitimate to assert that a sorting algorithm is inefﬁcient if its running time is Ω(n2)
in the worst case. As we will see in due course, this statement is correct as well as
legitimate, because there are sorting algorithms with superior running times.
Of the three kinds of asymptotic estimate, Θ notation and O notation are the ones
we will use the most. For example, we can estimate sums such as ∑n
k=1 k = Θ(n2)
and ∑n
k=1 k2 = Θ(n3) without bothering about the exact constants involved.
There are two dangerous bends one has to navigate with asymptotic notation.
Firstly, the equality sign in f = Θ(g) is not true equality with all the attendant
properties that equality entails. For instance, n2 = Θ(n2) and n2 +n = Θ(n2) does
not imply n2 = n2 +n. With Θ notation the equality goes one way, from the sharper
to the looser estimate. Another way to deﬁne Θ notation is to say that Θ(g) denotes
the set of all functions f with the stated property, and to write “f ∈Θ(g)” instead of
“f = Θ(g)”. However, use of one-way equality rather than inclusion is traditional
and there is no compelling reason to break from it. Consequently, we will never
write Θ(g) = f. We can, however, write
n2 +Θ(n) = Θ(n2)
for this assertion does make perfect mathematical sense.

2.2 Estimating running times
27
The second danger concerns reasoning about asymptotic notation. We cannot
reasonably assert, for example, that
Θ(1)+Θ(2)+···+Θ(n) = Θ(n2)
because the left-hand side does not have a clear meaning. But we can write
n
∑
i=1
Θ(i) = Θ(n2)
because there is only one occurrence of Θ on the left and we assume that it stands
for a single anonymous function of i.
We say that the running time of an algorithm is linear if it takes Θ(n) steps for
an input of size n, quadratic if it takes Θ(n2) steps, and so on. However, to claim
that an algorithm is linear should, strictly speaking, mean that it takes this time in
all cases. That is true for a function like reverse, but often the time taken differs in
different cases. Use of Θ notation is therefore usually conﬁned to one particular
case. For instance, we might say that an algorithm takes Θ(n2) steps in the worst
case, but only Θ(n) steps in the best case. Best- and worst-case times are rarely the
same. Most often we concentrate on the worst-case running time of an algorithm,
only occasionally mentioning the best case.
There are two other measures of an algorithm’s performance: the time it takes in
the average case, and a measure of how common the average case can be expected to
be. For any average-case analysis we have to assume some probability distribution
of the input values. For example, we could analyse the average case of a sorting
algorithm by assuming that the input is a permutation of 1 to n and that all such
permutations are equally likely. That may or may not be a sensible assumption
to make. Average-case analysis is a fascinating subject and can involve some
sophisticated mathematics, but in what follows we will almost always ignore this
measure of running time.
2.2 Estimating running times
So far we have used but not deﬁned the phrase “the running time of a function”. It
is, of course, a function of the input size that measures the number of basic steps
executed before the result is determined. The difﬁculty is that we simply do not
know what the notion of a basic step means, at least not without looking closely at
the details of a particular Haskell compiler and the architecture of the machine on
which the algorithm is executed. The simplest alternative, and the one we will use,
is to count reduction steps. Haskell evaluates an expression by reducing it to normal
form and printing the result. At each step some reducible expression, or redex, is
selected and simpliﬁed, by applying deﬁnitions supplied by the programmer or built-
in operations like +. However, not all reduction steps take exactly the same time,

28
Timing
nor does counting them take into account the time required to ﬁnd the next redex in
a large and possibly complicated expression, and so the number of reduction steps
is not a completely faithful measure of time. Alternatively, we could look at the
elapsed time between the start and ﬁnish points, but again such a measure depends
on the particular computer on which the function is evaluated. GHCi, a Haskell
interpreter that comes bundled with the Haskell Platform, does provide statistics of
performance if requested, including a measure of the elapsed time. Hugs, an earlier
interpreter for Haskell, counted reduction steps.
The second difﬁculty with estimating running times is that Haskell is a lazy
language and evaluates expressions only as far as necessary to obtain the required
value. For instance, in the evaluation of f (g x) it may or may not be the case that
g x is evaluated fully in order to determine the value computed by f. We will see an
example or two of this phenomenon in the following section. However, in most of
the algorithms in this book every subexpression will be fully evaluated at some point
in the computation, so the laziness of Haskell is not critical for timing purposes.
Instead we will assume eager evaluation for the purposes of timing. In particular,
if Tg(n) is an estimate of the number of reduction steps required to compute g on
an input of size n, for some appropriate deﬁnition of size, and g on such an input
returns a value of size m, and Tf (m) is similarly the number of steps required to
compute f on an input of size m, then the running time T(n) of the computation of
f ·g on an input of size n is given by
T(n) = Tg(n)+Tf (m)
Since lazy evaluation never requires more reduction steps than eager evaluation, any
upper bound of the running time of a function under eager evaluation will also be
an upper bound under lazy evaluation.
In order to count, or at least estimate, the number of reduction steps in the evalua-
tion of a recursive function, we need the idea of a recurrence relation. Associated
with every recursively deﬁned function is another recursively deﬁned function for
estimating the ﬁrst function’s running time. The deﬁnition of the second function is
usually referred to as a recurrence relation. Sometimes the relation is an equality
(=), sometimes an inequality (⩽or ⩾), depending on whether we are seeking exact,
upper, or lower bounds on the running time. To solve a recurrence relation means to
ﬁnd some way of expressing the function involved in a closed form.
For example, here is a simple recurrence relation for the running time T of some
algorithm as a function of the input size n:
T(n) = T(n−1)+Θ(n)
There is no real need to state the base case T(0) = Θ(1). The solution is given by
T(n) =
n
∑
k=0
Θ(k) = Θ(n2)

2.2 Estimating running times
29
The solution is not quite so obvious with the recurrence
T(n) = 2T(n−1)+Θ(n)
(2.1)
One way to solve (2.1) is to unfold it to see if a general pattern appears. Replacing
Θ(n) by cn to avoid tripping up over multiple Θs, we have
T(n) = cn+2T(n−1)
= cn+2(c(n−1)+2T(n−2))
= cn+2c(n−1)+4c(n−2)+···+2n−1 c+2n cT(0)
and so
T(n) = c
n−1
∑
k=0
2k (n−k)+2n cT(0)
= c
n
∑
k=1
k2n−k +2n cT(0)
= c2n
n
∑
k=1
k
2k +2n cT(0)
= Θ(2n)
since ∑n
k=1 k/2k <2. Strictly speaking, the above calculation solves only the recur-
rence
T(n) = 2T(n−1)+cn
However, (2.1) can be replaced by two recurrence relations:
T(n) ⩽2T(n−1)+c2 n
T(n) ⩾2T(n−1)+c1 n
and the reasoning above can be repeated with inequalities instead of equalities,
showing that T(n) = Ω(n) and T(n) = O(n), and thus T(n) = Θ(n). There is no real
harm in replacing Θ(f(n)) by c×f(n), and we shall continue to do so.
Next, consider the recurrence
T(n) = nT(n−1)+Θ(n)
This time we have
T(n) = cn+nT(n−1)
= cn+n(c(n−1)+(n−1)T(n−2))
= cn+cn(n−1)+cn(n−1)(n−2)+···
Hence
T(n) = c
n
∑
k=1
n!
(n−k)! = Θ(n!)
Later on, when we discuss divide-and-conquer algorithms, we will encounter other
recurrence relations and show how to solve them.

30
Timing
Let us now look at some speciﬁc examples of how to time a Haskell function.
Consider again the following two deﬁnitions of the function concat:
concat1 xss = foldr (++) [ ] xss
concat2 xss = foldl (++) [ ] xss
Since ++ is an associative operation with the empty list as identity element, these
two deﬁnitions are equivalent provided xss is a ﬁnite list. Let T1(m,n) and T2(m,n)
denote asymptotic estimates of the running times of the deﬁnitions when xss is a list
of length m consisting of lists each of length n. The total length of xss is therefore
mn. Under this assumption, the worst-case and the best-case running times coincide,
so we can give asymptotic estimates without focusing on different cases.
To estimate T1, it is best ﬁrst to rewrite the deﬁnition of concat1 in explicitly
recursive terms:
concat1 [ ]
= [ ]
concat1 (xs:xss) = xs++concat1 xss
The recursive deﬁnition of concat1 leads to the following deﬁnition of T1:
T1(0,n)
= Θ(1)
T1(m+1,n) = T1(m,n)+C(n,mn)
where C(m,n) is the time taken to concatenate a list of length m with a list of length
n. Here the base case is necessary. The cost of ++ is proportional to the length of the
ﬁrst argument, so C(m,n) = Θ(m). That means we can replace the second equation
in the deﬁnition of T1 by
T1(m+1,n) = T1(m,n)+Θ(n)
Now we have
T1(m,n) =
m
∑
k=0
Θ(n) = Θ(mn)
The running time of concat1 is therefore linear in the total length of the input, the
best we can expect. Turning to concat2, we again ﬁrst rewrite the deﬁnition as an
explicitly recursive function:
concat2 xss
= step [ ] xss
step ws [ ]
= ws
step ws (xs:xss) = step (ws++xs) xss
Here step is an abbreviation for foldl (++). That leads to the recurrence
T2(m,n)
= S(0,m,n)
S(k,0,n)
= Θ(1)
S(k,m+1,n) = S(k +n,m,n)+Θ(k)
where S(k,m,n) is the cost of evaluating step ws xss when ws has length k, and Θ(k)
accounts for the ++ operation in the recursive deﬁnition of step. We have

2.2 Estimating running times
31
S(k,m,n) =
m−1
∑
j=0
Θ(k +jn) = Θ(km+m2 n)
and so T2(m,n) = Θ(m2 n). The running time of concat2 is therefore not linear in the
total length of the input. Experiments with GHCi conﬁrm the difference in running
times:
> sum $ concat2 (replicate 2000 (replicate 100 1))
200000
(2.84 secs, 14,502,482,208 bytes)
> sum $ concat1 (replicate 2000 (replicate 100 1))
200000
(0.03 secs, 29,292,184 bytes)
By the way, experiments such as these are useful guides for guessing the running
time of a function. Try running the function on inputs of sizes n, 2n, 4n, and so on.
If the elapsed time doubles at each run, then the function takes linear time; if the
elapsed time is greater by a factor of four at each step, then the function probably
takes quadratic time; and if the time is greater by a factor of eight, then the function
takes cubic time. And so on.
Here is another example. Consider again the two permutation-generating func-
tions of the previous chapter:
perms1 = foldr (concatMap·inserts) [[ ]]
inserts x [ ]
= [[x]]
inserts x (y:ys) = (x:y:ys):map (y:) (inserts x ys)
perms2 [ ] = [[ ]]
perms2 xs = concatMap subperms (picks xs)
where subperms (x,ys) = map (x:) (perms2 ys)
picks [ ]
= [ ]
picks (x:xs) = (x,xs):[(y,x:ys) | (y,ys) ←picks xs]
Let T1(n) and T2(n) be the running times of perms1 and perms2 on a list of length n.
The recurrence relation for T1 satisﬁes
T1(n+1) = T1(n)+n!(I(n)+Θ(n2))
The function I(n) is the time to compute the list of insertions of a new element in
a permutation of length n. There are n+1 results, each of which is a list of length
n+1, and it takes Θ(n2) to concatenate them. Finally, there are n! permutations of
a list of length n, so the insertions are computed n! times. Now
I(n+1) = I(n)+Θ(n)
There are n+1 ways to add a new element to a list of length n, so it takes Θ(n) steps

32
Timing
to perform the map operations. That gives I(n) = Θ(n2), and I(n)n! = Θ((n+2)!),
so we have
T1(n+1) = T1(n)+Θ((n+2)!)
Hence
T1(n) =
n−1
∑
k=0
Θ((k +2)!) =
n+1
∑
k=2
Θ(k!)
But
n
∑
k=0
k! = n!

1+ 1
n +
1
n(n−1) +···+ 1
n!

= Θ(n!)
Therefore T1(n) = Θ((n+1)!) and so the running time of perms1 is proportional to
the total length of the output, namely n×n!.
Turning to T2(n), we have the recurrence relation
T2(n) = P(n)+n(T2(n−1)+Θ(n!))+Θ((n+1)!)
where P(n) is the time taken to compute picks. The second term accounts for the
total time spent computing n evaluations of subperms, and the ﬁnal term is the cost
of the concat operation, which takes linear time in the length of the output. The
function P(n) satisﬁes
P(n) = P(n−1)+Θ(n)
where the Θ(n) term accounts for the map operations implicit in the list comprehen-
sion. Thus
T2(n) = nT2(n−1)+Θ((n+1)!)
To solve this recurrence we can guess that T2(n) takes the form
T2(n) = f(n)n!
for some function f. Then we have
f(n)n! = nf(n−1)(n−1)!+Θ((n+1)!)
which on division by n! gives f(n) = f(n −1) + Θ(n). Hence f(n) = Θ(n2) and
T2(n) = Θ((n+2)!). Thus the running time of perms2 is a factor of n greater than
the running time of perms1.
2.3 Running times in context
As we said above, Haskell is a lazy language that evaluates expressions only as far
as necessary to obtain the answer. In this section we take a brief look at some of the
consequences of lazy evaluation. The material is not necessary for understanding
the rest of the book, so it can be skipped, especially at a ﬁrst reading.
The major point at issue is that we have to be careful about what running time

2.3 Running times in context
33
we actually assign to a given function when it occurs in a particular context. For
instance we have seen that evaluation of concat1 on a list of length m of lists all
of length n takes Θ(mn) steps. However, evaluating head · concat1 does not take
this time. In fact, evaluation of head ·concat1 on a nonempty list of nonempty lists
proceeds essentially as follows:
head (concat1 ((x:xs):xss))
= head ((x:xs)++concat1 xss)
= head (x:(xs++concat1 xss))
= x
There are only Θ(1) reduction steps. The running time of head·concat2 is, however,
Θ(m) steps because it takes this time to reduce
(([ ]++xs1)++xs2)++···++xsm
to xs1 ++··· before the head of xs1 can be extracted.
Here is another instructive example. Consider the functions inits and tails:
inits,tails::[a] →[[a]]
inits [ ]
= [[ ]]
inits (x:xs) = [ ]:map (x:) (inits xs)
tails [ ]
= [[ ]]
tails (x:xs) = (x:xs):tails xs
For instance
inits "abcd" = ["","a","ab","abc","abcd"]
tails "abcd" = ["abcd","bcd","cd","d",""]
The running times I(n) and T(n) of inits and tails satisfy
I(n+1) = I(n)+Θ(n)
T(n+1) = T(n)+Θ(1)
with solutions I(n) = Θ(n2) and T(n) = Θ(n). However, for both functions there
are Θ(n2) symbols in the output for a list of length n, so it takes this time to print
the result. The difference between the running times of inits and tails emerges when
we want to produce some function of the ﬁnal list rather than the ﬁnal list itself.
For instance one can count the number of sufﬁxes in linear time, but it requires
quadratic time to count the preﬁxes:
length $ tails [1..10000]
10001
(0.00 secs, 2,407,104 bytes)
length $ inits [1..10000]
10001
(1.39 secs, 5,901,977,160 bytes)

34
Timing
To compute the length of a list one only has to know the number of elements, not the
values of the elements themselves. The much greater space required in the second
evaluation is due to the fact that evaluation of inits builds up a long chain of maps:
[[ ],
map (1:) [[ ]],
map (1:) (map (2:) [[ ]]),
...
map (1:) (map (2:)...map (10000:) [[ ]])]
The values of these elements are not needed for computing the length of the result,
but nevertheless the unevaluated expressions have to be stored, which is why the
total amount of space required is about the square of the space needed for counting
the number of sufﬁxes. We will return to inits and tails in the following chapter.
Here is one more example. Consider the cost of computing length·perms1, where
perms1 was deﬁned in the previous section. Since only the number of permuta-
tions has to be found, no expression denoting an individual permutation has to be
evaluated and the running time, L(n) say, satisﬁes the recurrence relation
L(n+1) = L(n)+n!I(n)
where I(n) = Θ(n2), the same as before. The recurrence has the same asymptotic
solution as before, so it takes Θ((n+1)!) steps to compute the number of permuta-
tions. This time can be brought down to Θ(n!) steps by redeﬁning inserts. The trick
is to use an accumulating function:
inserts x
= help id x
help f x [ ]
= [f [x]]
help f x (y:ys) = f (x:y:ys):help (f ·(y:)) x ys
For example,
help id 1 [2,3]
= id [1,2,3]:help (2:) 1 [3]
= id [1,2,3]:(2:) [1,3]:help ((2:)·(3:)) 1 [ ]
= id [1,2,3]:(2:) [1,3]:(2:) ((3:) [1]):[ ]
It now takes only Θ(n) steps to count the number of insertions, so
L(n+1) = L(n)+Θ((n+1)!)
with the solution L(n) = Θ(n!). However, the total running time T1(n) of perms1 is
not affected by this change.
2.4 Amortised running times
Sometimes the total cost of a sequence of n operations is O(n) steps even though
the cost of an individual operation is not O(1). Consider the function

2.4 Amortised running times
35
build ::(a →a →Bool) →[a] →[a]
build p = foldr insert [ ] where insert x xs = x:dropWhile (p x) xs
For example, build (==) removes adjacent duplicates from a list:
build (==) [4,4,2,1,1,1,2,5] = [4,2,1,2,5]
The running time I(n) of insert on a list of length n is O(n) steps, assuming eval-
uation of p takes constant time. Hence the running time B(n) of build on a list of
length n satisﬁes
B(n+1) = B(n)+O(n)
with solution B(n) = O(n2). The running time of insert is certainly not constant
because dropWhile can take Ω(n) steps when applied to a list of length n. It therefore
appears that the assertion B(n) = O(n2) is the best we can say about the running
time of build p.
In fact the running time of build p is O(n) steps, not just O(n2) steps. To see
why, observe that each element added to the list can be dropped at most once. Thus
the total number of elements that can be dropped is at most the total number of
elements that can be added, namely n. That gives a total running time of O(n) steps.
The amortised cost of a single operation is obtained by dividing the total cost of
the operations by the number of such operations, namely n. The amortised cost of
insert in the computation of build is therefore O(1) steps. Note that no assumption
about probability distributions is involved in this analysis.
As another example, consider the following function which increments a binary
integer a given number of times:
bits::Int →[[Bit]]
bits n = take n (iterate inc [ ])
where inc [ ]
= [1]
inc (0:bs) = 1:bs
inc (1:bs) = 0:inc bs
The Standard Prelude function iterate generates an inﬁnite list:
iterate::(a →a) →a →[a]
iterate f x = x:iterate f (f x)
The function inc increments a binary integer, written in reverse order with the least
signiﬁcant bit ﬁrst: for example, inc 101 = 011 and inc 111 = 0001. How long does
it take to compute bits n? Since the running time of inc on a list of length k is Ω(k)
in the worst case (when all the bits are 1), it seems that the best we can say about the
total cost of n increments is that it takes O(n2) steps. Certainly it takes this time both
to compute and print the result, but we are concerned only with the computation
time. In fact the cost is O(n) steps. To see why, observe that in half of the cases only

36
Timing
the ﬁrst bit is changed; in a quarter of the cases only the ﬁrst two bits are changed;
in an eighth of the cases only the ﬁrst three bits are changed; and so on. Taking the
cost of changing a bit to be 1 step, the total cost is
n
2 + 2n
4 + 3n
8 +··· = O(n)
The amortised cost of each increment is therefore O(1) steps.
The most important use of amortised costs occurs when building data structures
of various kinds. We will see examples in the following chapter. Typical operations
on data structures include inserting an element into the structure, deleting an el-
ement, and perhaps merging two structures in some way. When each individual
operation is considered in isolation, upper and lower bounds can be given for the
cost of the operation. However, when some computation involves a sequence of n
such operations, whether they are grouped together or distributed throughout the
computation, the total cost as a function of n may be lower than the sum of the
individual estimates of the costs of each operation in the sequence.
The amortised costs of build p and a sequence of inc operations were obtained
using different insights, but there is a more uniform way of computing amortised
costs, not restricted to costs that are O(1). We will need this method in the following
chapter. To do so we ﬁrst change the cost model to one that counts the costs of
operations in terms of deﬁnite integers rather than asymptotic notation. For example
with build p we can charge a cost of 1 for each evaluation of p (remember p is
assumed to be a constant-time operation) and 1 for each cons operation. The actual
running times are proportional to these costs. Similarly, when a bit sequence begins
with exactly t bits set to 1, the cost of inc is deﬁned to be t +1.
Now suppose n successive applications of some function f applied to x0 produces
a sequence of values x0,x1,...,xn. Let C(xi) be the cost of computing f on input xi
and A(xi) the amortised cost. The aim is to show
n−1
∑
i=0
C(xi) ⩽
n−1
∑
i=0
A(xi)
(2.2)
In particular, if A(xi) = O(1), then the total cost of n operations is O(n).
To establish (2.2) we construct a function S, a ‘size’ function that returns nonneg-
ative integers, and show for some appropriate deﬁnition of A that
C(xi) ⩽S(xi)−S(xi+1)+A(xi)
(2.3)
for 0 ⩽i<n. In words, the cost of f on an input is bounded above by the difference
in sizes between the input and output, plus the amortised cost. The inequality (2.2)
can be summed, giving
n−1
∑
i=0
C(xi) ⩽S(x0)−S(xn)+
n−1
∑
i=0
A(xi)

2.4 Amortised running times
37
so (2.2) is certainly satisﬁed if S(x0) = 0.
Here are our two examples again. In the case of build p we take S(xs) = length xs.
Let C(xsi) be the cost of computing xsi+1, counting a cost of 1 for each p or cons
operation. We have
C(xsi) = length xsi −length xsi+1 +2
The cost is positive because if the output is longer than the input, it can only be so
by one element. Hence we can take A(xs) = 2.
In the case of inc we can deﬁne S(bs) to be the number of bits in bs that are set
to 1. If there are b1 bits set to 1 before an inc operation, including t initial bits, and
b2 bits set after the operation, then b2 = b1 −t +1, equivalently t +1 = b1 −b2 +2.
Hence
C(bsi) = S(bsi)−S(bsi+1)+2
and so A(bs) = 2.
Here is one more example. Consider the function
prune::([a] →Bool) →[a] →[a]
prune p = foldr cut [ ] where cut x xs = until done init (x:xs)
done xs = null xs ∨p xs
The value of cut x xs is the result of repeatedly dropping the last element of x:xs
until a list satisfying p is obtained. For example, if ordered is the test for whether a
sequence is in ascending order, we have
prune ordered [3,7,8,2,3] = [3,7,8]
This is obviously a very silly way to ﬁnd the longest ordered preﬁx of a list, but
never mind – only the running time is of interest. If evaluation of p on a list of
length k takes O(k) steps, then evaluation of cut on a list of length k takes O(k2)
steps. So it seems that the best we can say is that prune applied to a list of length n
takes O(n3) steps. In fact, prune takes O(n2) steps, which means that the amortised
running time of cut is O(n) steps.
To see why, suppose the result of cut on a list of length k1 is a list of length k2,
where 0 ⩽k2 ⩽k1 +1. Suppose we charge evaluation of each of done and init on a
list of length k as k units. Then init is performed k1 +1−k2 times with a total cost
of
(k1 +1)+k1 +(k1 −1)+···+(k2 +1) = (k1 +1)(k1 +2)
2
−k2 (k2 +1)
2
Since done is performed one more time than init, for a cost of k2, its total cost is
(k1 +1)(k1 +2)
2
−k2 (k2 −1)
2
Summing these two quantities and adding in 1 unit for the cons operation, we have
that the total cost of cut on a list of length k1 is

38
Timing
(k1 +1)(k1 +2)−k2
2 +1 = k2
1 −k2
2 +3(k1 +1)
We can therefore take
S(xsi) = (length xsi)2
A(xsi) = 3×(length xsi +1)
to satisfy (2.3). But no list can have length greater than n, so A(xsi) = O(n) and the
total cost of prune on a list of length n is O(n2) steps.
2.5 Chapter notes
As will be appreciated, a fair amount of combinatorial mathematics is involved in
the analysis of running times. We have mentioned sums and factorials, but later on
we will also need ﬂoors and ceilings, modulus operations, and binomial coefﬁcients.
The best source book for these concepts is [1]. The history of asymptotic notation is
discussed in [2] and also appears in [3]. To complete a quartet of books authored
by Donald Knuth, the inventor of the name ‘Analysis of Algorithms’, we also
recommend [4].
There are a number of algorithms for generating permutations, and a com-
prehensive review can be found in Section 7.2.1.2 of [5], yet another book by
Knuth. The method of choice used in Data.List is one that achieves maximal
laziness. For a fascinating discussion of this rather complicated deﬁnition, see
http://stackoverflow.com/questions/24484348/.
References
[1]
Ronald L. Graham, Donald E. Knuth, and Oren Patashnik. Concrete Mathematics.
Addison-Wesley, Reading, MA, second edition, 1994.
[2]
Donald E. Knuth. Big omicron and big omega and big theta. ACM SIGACT News,
8(2):18–23, 1976.
[3]
Donald E. Knuth. The Art of Computer Programming, volume 1: Fundamental
Algorithms. Addison-Wesley, Reading, MA, third edition, 1997.
[4]
Donald E. Knuth. Selected Papers on Analysis of Algorithms. Center for the Study of
Language and Information, Stanford University, CA, 2000.
[5]
Donald E. Knuth. The Art of Computer Programming, volume 4A: Combinatorial
Algorithms. Addison-Wesley, Reading, MA, 2011.
Exercises
Exercise 2.1 Is the assertion f(n) = O(1) the same as the assertion f(n) = Θ(1)?
Exercise 2.2 Are the following two assertions true?
O(f(n)×g(n)) = f(n)×O(g(n))
O(f(n)+g(n)) = f(n)+O(g(n))

Exercises
39
Exercise 2.3 Prove formally that (n + 1)2 = Θ(n2) by exhibiting the necessary
constants.
Exercise 2.4 What are the exact values of the sums ∑n
k=1 k and ∑n
k=1 k2?
Exercise 2.5 Some of the following are correct and some are wrong. Which are
which?
2n2 +3n = Θ(n2)
2n2 +3n = O(n3)
n log n
= O(n√n)
n+√n
= O(√n log n)
∑n
k=1 1/k = Θ(log n)
2log n
= O(n)
log(n!)
= Θ (n log n)
Exercise 2.6 Sums of the form ∑n
k=0 kxk for various x come up surprisingly often
in the analysis of running times. One way of ﬁnding the solution is to start with the
simpler geometric series
n
∑
k=0
xk = 1−xn+1
1−x
which is valid provided x ̸= 1, and to differentiate both sides with respect to x. Using
the fact that the derivative of a sum is the sum of its derivatives, carry out this
differentiation and hence estimate the sums ∑n
k=0 k2k and ∑n
k=0 k/2k.
Exercise 2.7 Using Θ notation, estimate the sum ∑n
k=1 k log k.
Exercise 2.8 Solve the recurrence relation
T(0,n) = Θ(n2)
T(m,n) = T(m−1,n)+Θ(m)
Exercise 2.9 Use the fusion law of foldr to simplify head ·concat1. Can the fusion
law of foldl be used to simplify head ·concat2?
Exercise 2.10 Analyse the running time of perms3, where
perms3 [ ] = [[ ]]
perms3 xs = concatMap subperms xs
where subperms x = map (x:) (perms3 (remove x xs))
Exercise 2.11 Do perms1, perms2, and perms3 all return the same ﬁrst list? If so,
what is it?
Exercise 2.12 Can the trick of using an accumulating function work with inits?

40
Timing
Exercise 2.13 Suppose you are given a list of n digits and you want to ﬁnd the
position, reading from left to right, of the ﬁrst digit d for which d ⩾5. If no such
digit occurs, then the result is some negative position, say −1. In the best case the
algorithm examines one digit and in the worst case all n digits. Assuming that every
sequence of n digits is equally likely, what is the average number of digits that have
to be examined?
Exercise 2.14 Using the function iterate, give a one-line deﬁnition of the function
tails1 that returns the nonempty sufﬁxes of a list.
Exercise 2.15 Consider the problem of maintaining a dynamic array. Apart from
inspecting and updating the elements, suppose that new elements can be added to
the array but only at the front. At some point the array, which is of a ﬁxed given size,
can become full. To solve this problem, a new array of double the size of the old
one can be allocated and all the existing elements copied into the upper half of the
new array, leaving space for further additions in the bottom half. Then we can carry
on until the new array becomes full, in which case the process is repeated. Show
that, in a sequence of add operations, each addition has an amortised cost of O(1).
Answers
Answer 2.1 Strictly speaking, no. We have f(n) = O(1) if there is a positive con-
stant C and an integer n0 such that f(n) ⩽C for all n > n0, while f(n) = Θ(n) if
there are positive constants C and D and an integer n0 such that D ⩽f(n) ⩽C for
all n>n0. Hence the function const 0 is O(1) but not Θ(1).
Answer 2.2 No, the ﬁrst one is true, but the second one is false. For example, take
f(n) = n2 and g(n) = 1. Then h(n) = O(n2 +1) holds if there exists a C and n0 such
that h(n) ⩽C(n2 +1) for all n>n0, but it does not follow that that there exists a C
and n0 such that h(n) ⩽n2 +C for all n>n0.
Answer 2.3 We have n2 ⩽(n + 1)2 ⩽4n2 for all n > 0. The second inequality
follows from the fact that 3n2 −2n−1 = (3n+1)(n−1), which is nonnegative if
n ⩾1.
Answer 2.4 We have
n
∑
k=1
k = n(n+1)
2
n
∑
k=1
k2 = n(n+1)(2n+1)
6
Answer 2.5 They are all true except for n + √n = O(√n log n). The last one,
log(n!) = Θ(n log n), is a crude form of Stirling’s approximation, which states that

Answers
41
n! =
√
2π n
n
e
n
(1+O(1/n))
Answer 2.6 Differentiating, we obtain
n
∑
k=0
kxk−1 = 1−(n+1)xn +nxn+1
(1−x)2
so
n
∑
k=0
kxk = x(1−(n+1)xn +nxn+1)
(1−x)2
Taking x = 2, we ﬁnd that the right-hand side is Θ(n2n), and taking x = 1/2 and
letting n tend to inﬁnity, the right-hand side tends to the value 2.
Answer 2.7 We have
n
∑
k=1
k log k ⩽log n
n
∑
k=1
k = O(n2 log n)
We also have
n
∑
k=1
k log k ⩾
n
∑
k=n/2
k log k ⩾log(n/2)
n
∑
k=n/2
k = Ω(n2 log n)
Hence the sum is Θ(n2 log n).
Answer 2.8 We have T(m,n) = Θ(mn+n2).
Answer 2.9 To simplify head ·foldr (++) [ ] we have to ﬁnd a function op1 so that
head (xs++ys) = op1 xs (head ys)
It is easy to see what the deﬁnition should be:
op1 xs y = if null xs then y else head xs
That gives
head ·concat1 = foldr op1 ⊥
since head [ ] = ⊥.
To simplify head · foldl (++) [ ] using the fusion law of foldl we have to ﬁnd a
function op2 so that
head (xs++ys) = op2 (head xs) ys
But, unless xs is known to be nonempty, no such op2 exists because we would have
to have op2 ⊥ys = head ys and op2 x ys = x.
Answer 2.10 We have
T(0) = Θ(1)
T(n) = n(R(n)+T(n−1)+Θ((n−1)!))+Θ((n+1)!)
where R(n) is the time needed for removing an element from a list of length n.

42
Timing
The ﬁrst term is the time needed to compute all evaluations of subperms. For each
evaluation we remove an element, costing R(n) steps, compute the permutations of
the resulting list, and then cons the element on the (n−1)! results. The ﬁnal term
accounts for the concatenations. Since nR(n) = O((n+1)!) we have
T(n) = nT(n−1)+Θ((n+1)!)
and, as we have seen, this leads to T(n) = Θ((n+2)!), a factor of n worse than the
total length of the output.
Answer 2.11 Yes. Applied to xs, all three methods return xs as the ﬁrst permutation
(in the case of perms3 it is required that the elements of xs can be compared with
equality).
Answer 2.12 Yes. Here is the deﬁnition:
inits = help id
where help f [ ]
= f [ ]:[ ]
help f (x:xs) = f [ ]:help (f ·(x:)) xs
It now takes linear time to compute length·inits.
Answer 2.13 Since exactly half the digits are greater than or equal to 5, the algo-
rithm inspects just one digit half of the time, two digits a quarter of the time, and so
on. Therefore in the average case ∑n
k=0 k/2k digits are inspected, which is 2 as we
saw in Exercise 2.6. The average case is therefore only twice as bad as the best case.
Answer 2.14 The deﬁnition is
tails1 = takeWhile (not ·null)·iterate tail
Answer 2.15 If the array has size 1 to begin with, the add operations take, in order,
times proportional to 1,2,1,4,1,1,1,8,.... The total cost of n add operations is
therefore
n+
p
∑
k=1
2k −1 ⩽n+2p+1
where 2p ⩽n<2p+1. Hence the amortised cost of an add operation is O(1) steps.
A similar situation occurs in Haskell. Periodically memory becomes full, and
computation is suspended while a garbage collection takes place. Thus a cons
operation is not actually guaranteed to take O(1) steps in all cases.

Chapter 3
Useful data structures
Most of the algorithms in this book can be implemented with acceptable efﬁciency
using just common-or-garden lists. One or two others require more specialised data
structures, such as binary search trees, heaps, and queues of various kinds. The
general philosophy of this book is not to consider data structures in isolation from
the algorithms that depend on them, so we postpone discussion of such structures to
the appropriate time and place. However, there is a small group of interrelated data
structures that we will introduce now. They are symmetric lists, random-access lists,
and arrays. Each is designed in its own way to overcome an obvious deﬁciency in
the running times of some of the basic operations on standard lists.
3.1 Symmetric lists
As we have seen, some operations on lists are lopsided with regard to efﬁciency:
while adding an element to the front of a list takes constant time, adding it to the rear
takes linear time. In what follows these two functions will be called cons and snoc;
the former is deﬁned by cons x xs = x : xs, and the latter by snoc x xs = xs ++ [x].
Similarly, while head and tail take constant time, last and init take linear time. The
data type known as symmetric lists overcomes this one-sidedness, guaranteeing
amortised constant time for all six operations. The basic idea is quite simple: break
the list into two and reverse the second half. In that way, a snoc can be implemented
as a cons on the second half, last as a head, and so on. The problem occurs with init
and tail when one attempts to remove an element; in some cases the list has ﬁrst to
be reorganised into two new halves.
Here are the details. A symmetric list is introduced as a pair of lists
type SymList a = ([a],[a])
with the understanding that the symmetric list (xs,ys) represents the standard list

44
Useful data structures
xs++reverse ys. That means we can convert back from symmetric lists to standard
lists by
fromSL::SymList a →[a]
fromSL (xs,ys) = xs++reverse ys
A function such as fromSL which converts a representation back into the structure
it is designed to represent is called an abstraction function. Using an abstraction
function we can capture the required relationship between the implementation of
an operation on the representing type and its ‘abstract’ deﬁnition with a simple
equation.
There is another aspect to the representation, the clever part that makes everything
ﬁt together. The two invariants
null xs ⇒null ys ∨single ys
null ys ⇒null xs ∨single xs
are maintained on symmetric lists (xs,ys). Here, single is the test for a singleton list.
In words, if one or other component is the empty list, then the other component has
to be either the empty list or a singleton list. The operations on symmetric lists both
exploit this representation invariant as well as maintain it.
Apart from fromSL there are six other operations we are going to implement on
symmetric lists; we will call them
consSL, snocSL, headSL, lastSL, tailSL, initSL
The implementations are designed to satisfy the six equations
cons x·fromSL = fromSL·consSL x
snoc x·fromSL = fromSL·snocSL x
tail·fromSL
= fromSL·tailSL
init ·fromSL
= fromSL·initSL
head ·fromSL = headSL
last ·fromSL
= lastSL
Here are the deﬁnitions of snocSL and lastSL:
snocSL::a →SymList a →SymList a
snocSL x (xs,ys) = if null xs then (ys,[x]) else (xs,x:ys)
lastSL::SymList a →a
lastSL (xs,ys)
= if null ys then head xs else head ys
Both of these deﬁnitions make use of, and maintain, the representation invariant. In
the case of snocSL we cannot just add x to the front of ys, because that would break
the invariant if xs happens to be the empty list and ys a singleton. But when xs is
empty we can return (ys,[x]), because ys is either empty or a singleton, and

3.1 Symmetric lists
45
[ ]++reverse [ ] ++[x] = [ ] ++reverse [x]
[ ]++reverse [y]++[x] = [y]++reverse [x]
In the case of lastSL, if ys is the empty list, then xs is either empty or a singleton
and, in the second case, the last element is the sole member of xs. We should really
have deﬁned lastSL to read
lastSL (xs,ys) = if null ys
then if null xs
then error "lastSL of empty list"
else head xs
else head ys
Otherwise we would get a confusing “head of empty list” error message when
trying to obtain the last element of an empty symmetric list. However, we will keep
the code simple by omitting error messages, using a simple ⊥instead. Once the
deﬁnitions of these two functions are understood, there should be no difﬁculty with
implementing the entirely dual functions consSL and headSL, so we will leave them
as exercises.
Two functions remain, and here things get interesting. The deﬁnition of tailSL is
as follows:
tailSL::SymList a →SymList a
tailSL (xs,ys)
| null xs
= if null ys then ⊥else nilSL
| single xs = (reverse vs,us)
| otherwise = (tail xs,ys)
where (us,vs) = splitAt (length ys div 2) ys
Let us look at the three cases. In the ﬁrst case, when xs is an empty list, the
representation invariant guarantees that ys is either the empty list or a singleton
list. If the former, then tailSL should really give a suitable error message rather
than simply returning ⊥. If ys is a singleton, then tailSL correctly returns the empty
symmetric list. The next easy case is the third one, in which xs is a list of length at
least two. Then we can simply drop the ﬁrst element of xs without destroying the
invariant. The most interesting case is the second one, when xs is a singleton list, so
ys can be a list of any length whatsoever. Here we split ys into two equal halves us
and vs, and then return the value (reverse vs,us). That’s correct because
[ ]++reverse (us++vs) = reverse vs++reverse us
The implementation of initSL is entirely dual to that of tailSL and we will leave it
as another exercise. The deﬁnition of nilSL is also left as an exercise.
Each of the operations apart from tailSL and initSL takes constant time. Although
tailSL and initSL can take linear time in the worst case, they both take amortised

46
Useful data structures
constant time. For the proof we employ the size method of the previous chapter. Con-
sider a sequence of n symmetric list operations producing a sequence x0,x1,...,xn
of symmetric lists, where we suppose x0 is the empty symmetric list ([ ],[ ]). Recall
that we have to construct a cost function C, a size function S, and an amortised
function A to satisfy
C(xi) ⩽S(xi)−S(xi+1)+A(xi)
(3.1)
For the size function S, we choose
S(xi) = abs (length xsi −length ysi)
where xi = (xsi,ysi) and abs is the function that returns the absolute value of a
number:
abs n = if n ⩾0 then n else−n
For the amortised time we choose A(xi) = 2. As to the costs of the individual
operations, we can charge a cost of 1 for each of the constant-time operations,
headSL, lastSL, consSL, and snocSL. Neither of the ﬁrst two change the symmetric
list, so (3.1) is satisﬁed for headSL and lastSL. Next consider snocSL, which, applied
to a symmetric list with component lengths (m,n), produces a symmetric list with
component lengths (n,1) if m = 0 and (m,n+1) if m ̸= 0. That means S increases or
decreases by 1 and so (3.1) is again satisﬁed. The same argument holds for consSL.
Finally, except in one case, both tailSL and initSL also increase or decrease S by at
most 1. The exceptional case is when one of xs or ys is a singleton and the other
has length k. In this case S has the value k −1 before the operation and at most 1
afterwards. Since k ⩽k −1−1+2 we can therefore charge k units for the cost of
the operation in this case, again satisfying (3.1).
For a fully serviceable library of symmetric list operations, we should of course
provide additional operations, such as nullSL and singleSL for testing whether a
symmetric list is empty or a singleton, and lengthSL for computing the length of a
symmetric list. These are left as exercises.
We will illustrate the use of symmetric lists on just one example. Consider again
the function inits from Chapter 2:
inits::[a] →[[a]]
inits [ ]
= [[ ]]
inits (x:xs) = [ ]:map (x:) (inits xs)
As we have seen, computing length·inits takes quadratic time. Can we ﬁnd some
other way of deﬁning inits so that this time is reduced to linear time? The deﬁnition
inits = map reverse·reverse·tails·reverse
achieves this aim but is unsatisfactory for another reason: what we really want is an
online algorithm for inits, so that given an inﬁnite list, inits returns an inﬁnite list

3.2 Random-access lists
47
of its ﬁnite preﬁxes. The above deﬁnition is not online because one cannot reverse
an inﬁnite list. A better deﬁnition, and the one given in Data.List in all essential
details, is to write
inits = map fromSL·scanl (ﬂip snocSL) nilSL
It still takes quadratic time to print all the preﬁxes, but only linear time to compute
length · inits (assuming of course that the input list is ﬁnite). There is another
deﬁnition of inits for which length · inits takes linear time, one that does not use
symmetric lists; we will leave that as an exercise.
Before leaving the subject of symmetric lists, we should mention that Haskell
provides an alternative method for providing efﬁcient list operations in the library
Data.Sequence. This library supports a number of operations on lists, including
those above. Instead of using the idea of representing a list as two component lists,
the library is based on 2-3 ﬁnger trees, a data structure that we will not discuss.
3.2 Random-access lists
Some algorithms, though not too many, rely on being able to retrieve the kth element
of a list for various k. Haskell provides a list-indexing operator (!!) for this purpose,
though we will rename it as fetch:
fetch::Nat →[a] →a
fetch k xs = if k == 0 then head xs else fetch (k −1) (tail xs)
Fetching the kth element of a list takes Θ(k) steps. In this section, and also in
the following one, we discuss two methods for making fetch more efﬁcient. In
the present section we describe a data structure known as a random-access list.
With random-access lists each of the operations cons, head, tail, and fetch takes
logarithmic time in the length of the list, that is, O(log n) steps for a list of length n.
While the performance of the ﬁrst three operations deteriorates, the last one is made
more efﬁcient. You pays your money and you makes your choice, as the saying goes.
Another important consequence of the representation is that, also in logarithmic
time, we can update an element at a speciﬁed position with a new element, an
operation that would take linear time with standard lists.
A random-access list is constructed out of two other data structures, the ﬁrst of
which is a binary tree:
data Tree a = Leaf a | Node (Tree a) (Tree a)
A tree is either a leaf containing a value, or a node consisting of two subtrees. The
size of a tree is the number of leaves in the tree:
size (Leaf x)
= 1
size (Node t1 t2) = size t1 +size t2

48
Useful data structures
Some operations on trees depend on knowing the size of a tree. Since we do not
want to recompute size from scratch each time, we can install its value in the tree,
redeﬁning Tree to read:
data Tree a = Leaf a | Node Nat (Tree a) (Tree a)
Provided size information is correctly installed each time we build a tree, we can
now deﬁne size as a selector function:
size::Tree a →Nat
size (Leaf x)
= 1
size (Node n
) = n
The function node, known as a smart constructor, constructs a Node ensuring that
size information is correctly installed:
node::Tree a →Tree a →Tree a
node t1 t2 = Node (size t1 +size t2) t1 t2
A binary tree can have many shapes and arbitrary sizes, but we are only going to
construct perfect binary trees in which all leaves have the same depth. For example,
t = Node 4 (Node 2 (Leaf 'a') (Leaf 'b')) (Node 2 (Leaf 'c') (Leaf 'd'))
is a perfect tree of size 4. All perfect trees have sizes of the form 2p for some p ⩾0.
We will see in due course how this perfection is guaranteed.
The second data structure is a sequence of perfect trees. But what we need is
not just an arbitrary list of trees, but a sequence of a special kind. The sequence is
designed to reﬂect the binary numerical representation described in the previous
chapter. Consider, for example, the number 6, which in (reversed) binary notation is
011 with the least signiﬁcant bit ﬁrst. The idea is to represent a six-element list, say
"abcdef", by a sequence
[Zero,
One (Node 2 (Leaf 'a') (Leaf 'b')),
One (Node 4 (Node 2 (Leaf 'c') (Leaf 'd'))
(Node 2 (Leaf 'e') (Leaf 'f')))]
Similarly, 5 is 101 in binary, and a ﬁve-element list, say "abcde", is represented by
[One (Leaf 'a'),
Zero,
One (Node 4 (Node 2 (Leaf 'b') (Leaf 'c'))
(Node 2 (Leaf 'd') (Leaf 'e')))]
An empty list can be represented by [ ]. We will not allow trailing zeros in random-
access lists, so the representations are unique.
Here, ﬁnally, is the deﬁnition of a random-access list:

3.2 Random-access lists
49
data Digit a
= Zero | One (Tree a)
type RAList a = [Digit a]
The abstraction function fromRA converts random-access lists into standard lists:
fromRA::RAList a →[a]
fromRA = concatMap from
where from Zero
= [ ]
from (One t) = fromT t
fromT ::Tree a →[a]
fromT (Leaf x)
= [x]
fromT (Node
t1 t2) = fromT t1 ++fromT t2
It is possible to make fromT more efﬁcient, but we leave that to the exercises.
The point of a random-access list is that we can skip over chunks of the list when
looking up an element at a speciﬁed location:
fetchRA::Nat →RAList a →a
fetchRA k (Zero:xs) = fetchRA k xs
fetchRA k (One t :xs) = if k <size t
then fetchT k t else fetchRA (k −size t) xs
fetchT ::Nat →Tree a →a
fetchT 0 (Leaf x) =
x
fetchT k (Node n t1 t2) = if k <m
then fetchT k t1 else fetchT (k −m) t2
where m = n div 2
The function fetchRA skips over trees whose elements have positions that are too
small, taking into account the number of elements it has skipped over. When a tree
is found that does contain a value at the desired position, the function fetchT is
invoked. Using the size information stored in a tree the required element can be
found either by searching the left subtree or the right subtree at each step. Provided
k is in the range 0 ⩽k <n when looking up the kth element in a list containing n
elements, we have
fetch k ·fromRA = fetchRA k
Furthermore, fetchRA takes O(log k) steps. To see this, suppose 2p ⩽k <2p+1. The
computation of fetchRA skips over p elements in O(p) steps, and then searches a
perfect binary tree of size 2p in a further O(p) steps. A better deﬁnition of fetchRA
would produce an “index too large” error message if n ⩽k, but we will leave that
deﬁnition as an exercise.
In addition to fetchRA and fromRA, ﬁve other basic operations are supported by
random-access lists:

50
Useful data structures
nullRA
::RAList a →Bool
nilRA
::RAList a
consRA
::a →RAList a →RAList a
unconsRA::RAList a →(a,RAList a)
updateRA ::Nat →a →RAList a →RAList a
The function nullRA tests whether a list is empty, nilRA returns an empty list, and
updateRA updates a random-access list at a speciﬁed location with a new value.
Its deﬁnition is similar to that of fetchRA and we will leave it as an exercise. The
deﬁnition of consRA stems directly from that of the inc operation of the previous
chapter:
inc [ ]
= [1]
inc (0:bs) = 1:bs
inc (1:bs) = 0:inc bs
Here is the deﬁnition of consRA:
consRA x xs = consT (Leaf x) xs
consT t1 [ ]
= [One t1]
consT t1 (Zero:xs)
= One t1 :xs
consT t1 (One t2 :xs) = Zero:consT (node t1 t2) xs
The deﬁnition of unconsRA follows that of the dec operation, which decrements a
binary counter:
dec [1]
= [ ]
dec (1:ds) = 0:ds
dec (0:ds) = 1:dec ds
Here is the deﬁnition of unconsRA:
unconsRA xs = (x,ys) where (Leaf x,ys) = unconsT xs
unconsT ::RAList a →(Tree a,RAList a)
unconsT (One t :xs) = if null xs then (t,[ ]) else (t,Zero:xs)
unconsT (Zero:xs) = (t1,One t2 :ys) where (Node
t1 t2,ys) = unconsT xs
The code is a little subtle. To illustrate the fact that unconsT xs always returns a leaf
as ﬁrst component when xs is a well-formed random-access list, it is instructive to
play through the example
[Zero,Zero,One t]
where t is the perfect tree of size 4 that ﬂattens to "abcd" from page 48. According
to the second clause of unconsT, the result is
(t1,One t2 :ys) where (Node
t1 t2,ys) = unconsT [Zero,One (tree "abcd")]
Again according to the second clause, the right-hand side returns

3.3 Arrays
51
(t3,One t4 :zs) where (Node
t3 t4,zs) = unconsT [One (tree "abcd")]
Finally, according to the ﬁrst clause of unconsT, we have
unconsT [One (tree "abcd")] = (tree "abcd",[ ])
That gives t3 = tree "ab", t4 = tree "cd" and zs = [ ]. Hence we have t1 = Leaf 'a',
t2 = Leaf 'b', and ys = [ ], and so
unconsT [Zero,Zero,One (tree "abcd")]
= (Leaf 'a',[One (Leaf 'b'),One (tree "cd")])
as required.
Given unconsRA, we can deﬁne headRA and tailRA quite simply (see the exer-
cises). As we have seen in the previous chapter, a sequence of n cons operations,
or n uncons operations, on an initially empty list takes O(n) steps, so considered
separately they take amortised constant time. But when they are mixed, the best we
can say is that they each take O(log n) steps. The lookup and update operations also
take this time. In the following section we look at a data structure in which a lookup
operation takes constant time, though an update operation goes from logarithmic
time to linear time.
3.3 Arrays
One of the main differences between functional and procedural algorithms is that
the former rely on lists as the basic carrier of information while the latter rely on
arrays. In functional algorithms input usually consists of a list of values, whereas
in procedural algorithms input values are usually assumed to be presented as the
elements of an array. For a procedural programmer array updates are destructive:
once an array is updated by changing the value at a particular index, the old array is
lost. In functional programming, data structures are persistent because any named
structure may be referred to at some other point in the computation and therefore has
to continue to exist. Consequently, any update operation, even at a single index, has
to be implemented by making a new copy of the whole array. Because they cannot be
changed but only copied, purely functional arrays are known as immutable arrays. It
is possible to get round this problem and allow mutable structures by encapsulating
the operations in a suitable monad, but we will not introduce monads in this book.
Wholesale or monolithic updates, on the other hand, are ﬁne. Changing all or
some of the entries at one go involves copying the array only once. Haskell provides
a number of such wholesale operations in the library Data.Array. The purpose of
this section is simply to describe the main functions in this library.
The type Array i e consists of arrays with indices of type i and elements of type e.
The basic operation for constructing arrays is a function

52
Useful data structures
array::Ix i ⇒(i,i) →[(i,e)] →Array i e
The type class Ix restricts what can be an index; usually this is an integer or a charac-
ter, types that can be converted into a contiguous range of values. The ﬁrst argument
to array is a pair of bounds, the lowest and highest indices in the array. The second
argument is an association list of index–value pairs. Building an array through array
takes linear time in the length of the association list and the size of the array.
A simple variant of array is listArray, which takes just a list of elements:
listArray::Ix i ⇒(i,i) →[e] →Array i e
listArray (l,r) xs = array (l,r) (zip [l..r] xs)
Finally, there is another way of building arrays, called accumArray, whose type
seems rather complicated:
accumArray::Ix i ⇒(e →v →e) →e →(i,i) →[(i,v)] →Array i e
The arguments are: an ‘accumulating’ function for transforming array entries
e and new values v into new entries; an initial entry for each index; a pair of
bounds for the array; and an association list of index–value pairs. The result of
accumArray f e (l,r) ivs is an array with bounds (l,r) and initial entries e every-
where, built by processing the association list ivs from left to right, combining old
entries and values into new entries using the accumulating function f. The process
takes linear time in the length of the association list, assuming that the accumulating
function take constant time. In symbols we have
accumArray f e (l,r) ivs =
array (l,r) [(j,foldl f e [v | (i,v) ←ivs,i == j]) | j ←[l..r]]
Well, nearly. In the Data.Array deﬁnition there is an added restriction on ivs, namely
that every index in ivs should lie in the speciﬁed range (l,r). If this condition is not
met, then the left-hand side returns an error while the right-hand side does not.
For example, we have
accumArray (+) 0 (1,3) [(1,20),(2,30),(1,40),(2,50)]
= array (1,3) [(1,60),(2,80),(3,0)]
accumArray (ﬂip (:)) [ ] ('A','C') [('A',"Apple"),('A',"Apricot")]
= array ('A','C') [('A',["Apricot","Apple"]),('B',[ ]),('C',[ ])]
As just one useful application of accumArray, suppose we are given a list of n
natural numbers, all in the range (0,m) for some m. We can sort this list in Θ(m+n)
steps in the following way:
sort ::Nat →[Nat] →[Nat]
sort m xs = concatMap copy (assocs a)
where a = accumArray (+) 0 (0,m) (zip xs (repeat 1))
copy (x,k) = replicate k x

3.4 Chapter notes
53
The function assocs :: Array i e →[(i,e)] returns the list of index–value pairs in
index order. The function elems, which can be deﬁned by
elems::Ix i ⇒Array i e →[e]
elems = map snd ·assocs
converts an array to a list of its elements in index order. Thus elems is the abstraction
function for converting arrays back into standard lists.
The good news is that with arrays the lookup function, (!), takes constant time.
For instance,
assocs xa = [(i,xa!i) | i ←range (bounds xa)]
takes time proportional to the size of the array. The Data.Array function bounds
returns the bounds of an array, and range enumerates the values between the lower
and upper bound.
The bad news, as we have said, is that array updates take linear time in the size of
the array. The update function is //, with type
(//)::Ix i ⇒Array i e →[(i,e)] →Array i e
Thus the operation xa//ies updates the array xa with the associations in ies. For
example,
foldl update (array (1,n) [ ]) (zip [1..n] xs)
where update xa (i,x) = xa//[(i,x)]
builds an array but takes Θ(n2) steps to do it, while the equivalent expression
array (1,n) (zip [1..n] xs)
takes Θ(n) steps.
In summary, indexing and wholesale operations are efﬁcient for arrays, while
individual updates are not. “We can remember it for you wholesale”, as Philip K.
Dick entitled one of his short stories (see [1]).
3.4 Chapter notes
The idea of modelling a symmetric list, also known as a double-ended queue or
deque, by a pair of lists has been thought of many times. It appears in Okasaki’s
book [5] on functional data structures, where the idea is attributed to Gries [2] and
Hood and Melville [3]. See also [4], which introduces the representation invariant
used above. Random-access lists, also known as one-sided ﬂexible arrays, are
discussed in Chapter 9 of [5]. That chapter also presents some alternative number
representations, including binary numbers constructed from 1s and 2s rather than
0s and 1s. Using such a representation, one can implement headRA to run in O(1)
worst-case time. The monolithic array operations of Data.Array were proposed by

54
Useful data structures
Philip Wadler in [6], although others had earlier suggested similar operations. The
Haskell Platform provides a number of other libraries for handling arrays, including
unboxed, mutable, and storable arrays.
References
[1]
Philip K. Dick. We can remember it for you wholesale. In The Collected Short
Stories of Philip K. Dick, Volume 2. Citadel Twilight, New York, 1990.
[2]
David Gries. The Science of Programming. Springer, New York, 1981.
[3]
Robert Hood and Robert Melville. Real-time queue operations in pure Lisp.
Information Processing Letters, 13(2):50–53, 1981.
[4]
Rob R. Hoogerwoord. A symmetric set of efﬁcient list operations. Journal of
Functional Programming, 2(4):294–303, 1992.
[5]
Chris Okasaki. Purely Functional Data Structures. Cambridge University Press,
Cambridge, 1998.
[6]
Philip L. Wadler. A new array operation. In J. F. Fasel and R. M. Keller, editors,
Graph Reduction, volume 279 of Lecture Notes in Computer Science, pages 328–333.
Springer-Verlag, Berlin, 1986.
Exercises
Exercise 3.1 Write down all the ways "abcd" can be represented as a symmetric
list. Give examples to show how each of these representations can be generated.
Exercise 3.2 Deﬁne the value nilSL that returns an empty symmetric list, and the
two functions nullSL and singleSL for testing whether a symmetric list is empty or
a singleton. Also, deﬁne lengthSL.
Exercise 3.3 Deﬁne the functions consSL and headSL.
Exercise 3.4 Deﬁne the function initSL.
Exercise 3.5 Implement dropWhileSL so that
dropWhile·fromSL = fromSL·dropWhileSL
Exercise 3.6 Deﬁne initsSL with the type
initsSL::SymList a →SymList (SymList a)
Write down the equation that expresses the relationship between fromSL, initsSL,
and inits.
Exercise 3.7 Give an online deﬁnition of inits that does not use symmetric lists for
which length·inits takes linear time.
Exercise 3.8 Estimate the running time of fromT when applied to a perfect tree of
size 2p, where fromT was deﬁned by

Exercises
55
fromT ::Tree a →[a]
fromT (Leaf x)
= [x]
fromT (Node
t1 t2) = fromT t1 ++fromT t2
One way to reduce the running time is to introduce a function
fromTs::[Tree a] →[a]
and deﬁne fromT t = fromTs [t]. Give an efﬁcient deﬁnition of fromTs. Variations
of this particular optimisation for ﬂattening a tree will be used a number of times in
the rest of the book.
Exercise 3.9 What change to the deﬁnition of fetchRA is needed to produce a
suitable error message when the index is too large?
Exercise 3.10 Give a deﬁnition of the function toRA::[a] →RAList a that converts
a list into a random-access list.
Exercise 3.11 Give a deﬁnition of updateRA.
Exercise 3.12 Following on from the previous exercise, give a one-line deﬁnition
of a function
(//)::RAList a →[(Nat,a)] →RAList a
so that xs//kxs is the result of carrying out a sequence of updates kxs on a random-
access list xs. The updates should be applied from left to right. Hint: both ﬂip and
the standard Haskell function
uncurry::(a →b →c) →(a,b) →c
uncurry f (x,y) = f x y
will be useful.
Exercise 3.13 Deﬁne headRA and tailRA.
Exercise 3.14 Suppose you want to deﬁne an array fa with bounds (0,n) whose
kth entry is k!, the factorial of k. Complete the deﬁnition
fa = listArray (0,n)????
in two different ways, one using scanl and one not. (Hint: for the second deﬁnition
use the fact that fa!i = i×fa!(i−1).)
Exercise 3.15 There is another function accum in Data.Array with the type
accum::Ix i ⇒(e →v →e) →Array i e →[(i,v)] →Array i e
This function takes an accumulating function, an array, and an association list. It
computes new array entries by combining elements from the association list with
the accumulating function. More precisely,
(accum f a ivs)!j = foldl f (a!j) [v | (i,v) ←ivs,i == j]
Deﬁne accumArray in terms of accum.

56
Useful data structures
Answers
Answer 3.1 There are three ways:
("a","dcb"),("ab","dc"),("abc","d")
We have, for example,
("a","dcb") = foldl (ﬂip snocSL) nilSL "abcd"
("abc","d") = foldr consSL nilSL "abcd"
("ab","dc") = consSL 'a' (snocSL 'd' (foldr consSL nilSL "bc"))
Answer 3.2 We have
nilSL::SymList a
nilSL = ([ ],[ ])
nullSL::SymList a →Bool
nullSL (xs,ys) = null xs ∧null ys
singleSL::SymList a →Bool
singleSL (xs,ys) = (null xs ∧single ys) ∨(null ys ∧single xs)
lengthSL::SymList a →Nat
lengthSL (xs,ys) = length xs+length ys
Answer 3.3 We have
consSL::a →SymList a →SymList a
consSL x (xs,ys) = if null ys then ([x],xs) else (x:xs,ys)
headSL::SymList a →a
headSL (xs,ys) = if null xs then head ys else head xs
Answer 3.4 We have
initSL::SymList a →SymList a
initSL (xs,ys)
| null ys
= if null xs then ⊥else nilSL
| single ys = (us,reverse vs)
| otherwise = (xs,tail ys)
where (us,vs) = splitAt (length xs div 2) xs
Answer 3.5 We have
dropWhileSL p xs
| nullSL xs
= nilSL
| p (headSL xs) = dropWhileSL p (tailSL xs)
| otherwise
= xs

Answers
57
Answer 3.6 We can deﬁne
initsSL xs = if nullSL xs
then snocSL xs nilSL
else snocSL xs (initsSL (initSL xs))
The relationship is
inits·fromSL = map fromSL·fromSL·initsSL
Answer 3.7 We have
inits = map reverse·scanl (ﬂip (:)) [ ]
Answer 3.8 We have
T(p) = 2T(p−1)+Θ(2p−1)
where the Θ(2p−1) term accounts for the concatenation. That gives T(p) = Θ(p2p).
The new deﬁnition is
fromT t = fromTs [t]
fromTs [ ]
= [ ]
fromTs (Leaf x:ts)
= x:fromTs ts
fromTs (Node
t1 t2 :ts) = fromTs (t1 :t2 :ts)
This deﬁnition has a running time of Θ(2p) steps. Another method is to use an
accumulating function.
Answer 3.9 Add a clause
fetchRA k [ ] = error "index too large"
Answer 3.10 We have
toRA::[a] →RAList a
toRA = foldr consRA nilRA
Answer 3.11 We have
updateRA k x (Zero:xs) = Zero:updateRA k x xs
updateRA k x (One t :xs) = if k <size t
then One (updateT k x t):xs
else One t :updateRA (k −size t) x xs
updateT ::Nat →a →Tree a →Tree a
updateT 0 x (Leaf y)
= Leaf x
updateT k x (Node n t1 t2)
= if k <m
then Node n (updateT k x t1) t2
else Node n t1 (updateT (k −m) x t2)
where m = n div 2

58
Useful data structures
Answer 3.12 We have
(//)::RAList a →[(Nat,a)] →RAList a
(//) = foldl (ﬂip (uncurry updateRA))
For example,
fromRA (toRA [0..3]//[(1,7),(2,3),(3,4),(2,8)]) = [0,7,8,4]
The intermediate updates are
[0,1,2,3], [0,7,2,3], [0,7,3,3], [0,7,3,4], [0,7,8,4]
If there are m updates on a random-access list of length n, the running time of // is
Θ(m log n) steps.
Answer 3.13 The deﬁnitions are
headRA xs = fst (unconsRA xs)
tailRA xs
= snd (unconsRA xs)
Answer 3.14 We have
fa = listArray (0,n) (scanl (×) 1 [1..n])
fa = listArray (0,n) (1:[i×fa!(i−1) | i ←[1..10]])
The listArray construction is not strict in the array elements, so recursive deﬁnitions
such as the one above are legitimate.
Answer 3.15 We have
accumArray f e bnds ivs = accum f (array bnds [(i,e) | i ←range bnds]) ivs

PART TWO
DIVIDE AND CONQUER


61
Divide and conquer (from the Latin divide et impera, and more accurately translated
as divide and rule) is the ﬁrst algorithm design technique we will study in depth.
Given a problem to solve, either solve it directly if its size is sufﬁciently small and
it is easy to do so, or else divide it into one or more subproblems, solve each of
these subproblems, and then combine the solutions to give a solution to the original
problem. Such a strategy covers pretty much everything about problem solving
in computer science, or mathematics, or life for that matter, but the feature that
makes it into a simple and effective computational tool is that each subproblem is
simply the original problem on an input of smaller size. Hence each subproblem is
solved by the same strategy. A divide-and-conquer algorithm is therefore essentially
recursive in nature.
Phrased this way, every functional algorithm that depends on explicit recursion
can be thought of as a divide-and-conquer algorithm. After all, one possible de-
composition of a problem of size n>0 is to divide it into a problem of size n−1
and a problem of size 1. For instance, an algorithm expressed as a foldr has essen-
tially this decomposition. But in a truly divide-and-conquer algorithm there are
two other important aspects. One is that each subproblem should have a size that
is some fraction of the input size, a fraction like n/2 or n/4. In many cases the
subproblems will have equal size, or as close to equal size as possible. A problem
of size n might therefore be divided into two subproblems each of size n/2, a very
common form of decomposition that we will meet later on. There are also examples
of divide-and-conquer algorithms in which the subproblems have different sizes, for
example one of size n/5 and the other of size 7×n/10. We will encounter such an
example in Chapter 6. The second important aspect is that the subproblems should
be independent of each other, so the work done in solving them is not duplicated.
Problems in which the subproblems overlap and have many sub-subproblems in
common can be tackled by the dynamic programming strategy, a topic we will take
up in Part Five.
Finally, because the subproblems are independent and can be solved concurrently
as well as sequentially, divide-and-conquer algorithms are highly suited to exploiting
parallelism. We will not pursue parallel programming in this book, but see Simon
Marlow’s book Parallel and Concurrent Programming in Haskell (O’Reilly, 2013),
for an excellent coverage of the topic.


Chapter 4
Binary search
Binary search is probably the simplest example of divide and conquer. A search
problem is solved by dividing it into two subproblems, each of size approximately
half the original. The distinguishing feature of binary search is that one of these
subproblems is trivial. In this chapter we introduce binary search by looking at two
examples that can proﬁtably use it, and then go on to encapsulate binary search as a
data structure, a binary search tree.
4.1 A one-dimensional search problem
In the ﬁrst problem we are given a strictly increasing function f from natural
numbers to natural numbers (so x <y ⇒f x <f y for all x and y) together with a
target number t. The object is to ﬁnd x, if it exists, such that t = f(x). Since f is
strictly increasing, there is at most one solution. Furthermore, x <f(x +1) if f is
strictly increasing, so the search can be conﬁned to the interval 0 ⩽x ⩽t (inclusive).
Recalling that Nat is a Haskell type synonym for Int, we have
search::(Nat →Nat) →Nat →[Nat]
search f t = [x | x ←[0..t],t == f x]
The result of search is either an empty list or a singleton list. The only assumption
we have really used about f is that t = f(x) ⇒0 ⩽x ⩽t for all x and t. This method,
which searches for a value incrementally in steps of one, is called linear search.
There are better methods than linear search for solving our problem, and we
are going to give two. In both methods the ﬁrst step is to make the search interval
explicit:
search f t = seek (0,t) where seek (a,b) = [x | x ←[a..b],t == f x]
The next step is to ﬁnd a better version of seek. If a > b, then seek (a,b) = [ ].
Otherwise, let m be any number in the range a ⩽m ⩽b. We then have

64
Binary search
seek (a,b) = [x | x ←[a..m−1],t == f x]++
[m | t == f m]++
[x | x ←[m+1..b],t == f x]
The key observation is that if t <f(m), then the last two lists are empty; if t = f(m),
then we are done; and if t >f(m), then the ﬁrst two lists are empty. Here we do use
the fact that f is increasing. Hence we can deﬁne
search::(Nat →Nat) →Nat →[Nat]
search f t = seek (0,t)
where seek (a,b) | a>b
= [ ]
| t <f m
= seek (a,m−1)
| t == f m = [m]
| otherwise = seek (m+1,b)
where m = choose (a,b)
It remains to choose m. The obvious choice to balance the two subproblems is to
take m = ⌊(a+b)/2⌋, the middle of the interval. In other words,
choose (a,b) = (a+b) div 2
This is binary search. A search problem is divided into a single subproblem of about
half the size. It is easy to appreciate that binary search takes logarithmic time in the
size of the interval being searched, because the interval halves at each step. Thus
search f t takes O(log t) steps. To be more precise we have to formulate and solve
the associated recurrence relation, but we will leave that discussion until after we
have dealt with the second method for solving our problem.
There are a number of aspects of the above deﬁnition of search that make another
solution worth exploration, not the least of which is the fact that the deﬁnition is
incorrect! For example, suppose f(n) = 2n. Then evaluation of search f 1024 returns
[ ] instead of the correct answer [10]. Pause for a moment to see if you can spot the
bug.
What has gone wrong is not the deﬁnition of search but its type. The ﬁrst step
requires evaluation of the test 1024<2512, and 2512 is a huge number, well beyond
the capabilities of limited-precision arithmetic. In fact, as an element of Nat, eval-
uation of 2512 returns 0, causing the test to incorrectly return False. The situation
can be remedied by changing Nat to Integer, but the numbers are still huge and the
calculations can be very time-consuming.
The second, minor problem with search is that f is evaluated twice at each step.
That is easily solved with a suitable local deﬁnition, but the fact still remains that in
the worst case there are three comparison tests at each step. Can we do better?
Yes, and here is the idea: we ﬁrst ﬁnd integers a and b such that f(a)<t ⩽f(b)

4.1 A one-dimensional search problem
65
and then search only the interval [a + 1..b]. If t ⩽f(0), then we can invent a
ﬁctitious value f(−1) = −∞and set (a,b) = (−1,0); otherwise we can ﬁnd a and b
by looking at the values of f for the numbers 1,2,4,8,... until a value p is found for
which f(2p−1)<t ⩽f(2p). Such a value is guaranteed to exist, because f is strictly
increasing. The function bound computes such an interval:
bound ::(Nat →Nat) →Nat →(Int,Nat)
bound f t = if t ⩽f 0 then (−1,0) else (b div 2,b)
where b = until done (×2) 1
done b = t ⩽f b
It takes p + 1 evaluations to compute bound f t when f(2p−1) < t ⩽f(2p). In the
worst case, when f = id, that gives O(log n) evaluations, but when f(n) = 2n, only
O(log(log n)) evaluations are required.
Now, to search the interval [a+1..b] we need only to ﬁnd the smallest x such
that t ⩽f(x). Such a value is guaranteed to exist because t ⩽f(b). That gives
search f t = if f x == t then [x] else [ ]
where x = smallest (bound f t)
smallest (a,b) = head [x | x ←[a+1..b],t ⩽f x]
The deﬁnition of smallest uses linear search, but, as we have seen above, a better
method is to split the interval: if a+1<b then for any m in the range a<m<b we
have
smallest (a,b) = head ([x | x ←[a+1..m],t ⩽f x]++
[x | x ←[m+1..b],t ⩽f x])
This time, if t ⩽f(m), then the ﬁrst list is not empty; otherwise it is. Hence we can
write
search::(Nat →Nat) →Nat →[Nat]
search f t = if f x == t then [x] else [ ] where
x = smallest (bound f t) f t
where
smallest (a,b) f t | a+1 == b = b
| t ⩽f m
= smallest (a,m) f t
| otherwise
= smallest (m,b) f t
where m = (a+b) div 2
This is our second version of binary search. We have made smallest a separate
top-level function because we will need it in the following section. Note that
smallest (a,b) f t is well deﬁned even if there is no x in the range a<x ⩽b such
that t ⩽f x; in this case the value returned is b. In this version of binary search
there is only one comparison involving f at each step, as compared with two in the

66
Binary search
worst case of the previous version. Moreover, search works with limited-precision
arithmetic. Note, ﬁnally, that f(a) is never evaluated during the algorithm, so the
ﬁctitious value f(−1) = −∞is never required.
To time this version, let T(n) denote the number of evaluations of f in the
computation of smallest (a,b) f t when interval (a,b) contains n numbers, so that
n = b−a+1. The fast and loose way to deﬁne T(n) is to write
T(2) = 0
T(n) = T(n/2)+1
To solve this recurrence, we can unfold it to give
T(n) = 1+T(n/2) = 2+T(n/4) = 3+T(n/8) = ··· = k +T(n/2k)
It follows that T(n) = k if n = 2k+1. If n is not a power of two, so 2k < n < 2k+1,
then we can appeal to the assumption that T(n) is an increasing function of n to
arrive at the estimate T(n) ⩽⌈log n⌉. If f takes constant time, then binary search
takes Θ(log t) steps.
Here is where we are playing fast and loose. For one thing, the subproblems do
not both have size n/2. If n is odd, then both subproblems have size ⌈(n+1)/2⌉,
while if n is even, then just one of the subproblems has this size. For another thing,
the sizes of intervals are natural numbers and T(n) is deﬁned only when n is a
natural number, so T(n/2) is not well-deﬁned. Finally, the assumption that when
the problem size increases the complexity cannot decrease is not always valid – it
depends on the algorithm. Neither of the ﬁrst two issues usually matters, especially
when we are after only asymptotic bounds, such as T(n) = Θ(log n). But sometimes
they do. This is certainly the case when we are after an exact number. For instance,
the exact number of evaluations of f in the worst case of smallest on an interval of
size n is given by the recurrence T(n) = T⌈(n+1)/2⌉+1 and T(2) = 0. The exact
solution turns out to be T(n) = ⌈log(n−1)⌉for 2 ⩽n (see Exercise 4.3). However,
in the main we will ignore ﬂoors and ceilings in recurrences, and carry on with fast
and loose reasoning.
Here is another recurrence relation that we will mention now, one that will
crop up frequently in the following chapter: T(n) = 2T(n/2)+Θ(n). To solve this
recurrence we unfold it, replacing Θ(n) by cn to avoid tripping up on multiple Θs.
Then we obtain
T(n) = cn+2T(n/2)
= cn+2(cn/2+2T(n/4))
= 2cn+4T(n/4)
= ...
= kcn+2k T(n/2k)
Supposing 2k−1 <n ⩽2k, so k = ⌈log n⌉, we obtain

4.2 A two-dimensional search problem
67
521
693
768
799
821
829
841
869
923
947
985
999
519
621
752
797
801
827
833
865
917
924
945
998
507
615
673
676
679
782
785
819
891
894
897
913
475
597
627
630
633
717
739
742
845
848
851
894
472
523
583
586
589
612
695
698
701
704
767
810
403
411
441
444
547
583
653
656
679
691
765
768
397
407
432
434
444
510
613
626
627
673
715
765
312
313
363
366
411
472
523
601
612
647
698
704
289
312
327
330
333
336
439
472
527
585
612
691
272
245
283
296
299
302
313
441
523
529
587
589
217
237
245
264
267
296
303
376
471
482
537
588
116
128
131
134
237
240
267
346
469
481
515
523
103
107
113
126
189
237
264
318
458
480
497
498
100
101
112
124
176
212
257
316
452
472
487
497
Figure 4.1 An example grid
T(n) = cn⌈log n⌉+Θ(2⌈log n⌉)
and so T(n) = Θ(n log n). Such a running time is sometimes called linearithmic, a
portmanteau word that combines linear and logarithmic. We will meet other more
difﬁcult recurrence relations in the following section.
4.2 A two-dimensional search problem
The second problem is much more interesting. This time we are given a function f
from pairs of natural numbers to natural numbers with the property that f is strictly
increasing in each argument. Given t, we have to ﬁnd all pairs (x,y) such that
f(x,y) = t. Unlike the one-dimensional case, there can be many solutions. To get a
feel for the problem, take a look at the grid in Figure 4.1. Positions on the grid are
given by Cartesian coordinates (x,y), where x is the column number and y is the row
number. The bottom-left element is at position (0,0) and the top-right element is at
position (11,13). What systematic procedure would you use to ﬁnd all the positions
that contain the number 472? Pause for a moment to answer this question.
Did you try to use binary search? After all, that is what the chapter is about. The
difﬁculty is that it is not easy to see exactly how to program the search in this two-
dimensional case. So we will start slowly and begin with the obvious generalisation
of one-dimensional search to a two-dimensional (t +1)×(t +1) grid:
search f t = [(x,y) | x ←[0..t],y ←[0..t],t == f(x,y)]
This method, which takes Θ(t2) steps, searches the grid upwards column by column,
starting at the leftmost column. Also it takes no account of the fact that searching a
column can be abandoned as soon as an (x,y) is found for which t ⩽f(x,y). There

68
Binary search
has to be a better way; we shall describe no fewer than four, including three versions
that employ binary search.
The ﬁrst improvement is to start at the top-left rather than the bottom-left corner:
search f t = [(x,y) | x ←[0..t],y ←[t,t −1..0],t == f(x,y)]
As in binary search, a more general version is obtained by making the search interval
explicit:
searchIn (a,b) f t = [(x,y) | x ←[a..t],y ←[b,b−1..0],t == f(x,y)]
Thus search = searchIn (0,t). Next, we examine the various cases that can arise.
First, it follows at once from the deﬁnition of searchIn that
searchIn (a,b) f t | a>t ∨b<0 = [ ]
Now suppose the search interval is not empty and f(a,b)<t. In this case column a
can be eliminated from further consideration since f(a,b′) ⩽f(a,b) for b′ ⩽b. That
means
searchIn (a,b) f t | f(a,b)<t = searchIn (a+1,b) f t
In the dual case, f(a,b) > t, row b can be eliminated since f(a′,b) ⩾f(a,b) for
a′ ⩾a. That means
searchIn (a,b) f t | f(a,b)>t = searchIn (a,b−1) f t
Finally, if f(a,b) = t, then both column a and row b can be eliminated since
f(a,b′)<f(a,b) if b′ <b and f(a′,b)>f(a,b) if a′ >a. It is only in this last case
that we use the fact that f is strictly increasing, rather than just weakly increasing,
in both arguments.
Putting the four cases together, and renaming (a,b) as (x,y), we arrive at
search f t = searchIn (0,t)
where searchIn (x,y) | x>t ∨y<0 = [ ]
| z<t
= searchIn (x+1,y)
| z == t
= (x,y):searchIn (x+1,y−1)
| z>t
= searchIn (x,y−1)
where z = f(x,y)
This method is known as saddleback search. It is fairly easy to see it requires only
Θ(t) evaluations of f. More precisely, suppose there is a p×q rectangle to search. In
the best case, when the search proceeds along the diagonal of the rectangle, ﬁnding
occurrences of t at each step, there are (p min q) evaluations of f. In the worst
case, when the search proceeds along the edges of the rectangle, there are p+q−1
evaluations of f. As just one example, with f(x,y) = x2 +3y and t = 20259, it takes
20402 evaluations of f to obtain the answer [(24,9)]. That is quite close to the best
case.
Saddleback search can be improved because starting with the corners (0,t) and

4.2 A two-dimensional search problem
69
z
x
y
(x1,y1)
(x2,y2)
Figure 4.2 A divide-and-conquer decomposition
(t,0) can be an overly pessimistic estimate of where the required values lie. Instead
we can use binary search to obtain better starting intervals. Recall from the previous
section that, provided t ⩽f b, the value of smallest (a,b) f t is the smallest x in the
range a<x ⩽b such that t ⩽f x. Hence, if we deﬁne
p = smallest (−1,t) (λy.f(0,y)) t
q = smallest (−1,t) (λx.f(x,0)) t
then we can start saddleback search with the corners (0,p) and (q,0). This version
of saddleback search takes Θ(log t) + Θ(p + q) steps. Since p and q may be sub-
stantially less than t, we can end up with a search that takes Θ(log t) steps. For
example, again with f(x,y) = x2 +3y and t = 20259, we have p = 10 and q = 143.
It now takes a total of only 181 evaluations of f (including those evaluations in the
two binary searches) to compute the answer, a substantial saving over the previous
version.
A third way to search a grid is to head for a proper divide-and-conquer solution,
looking at the middle element of the grid ﬁrst. After all, that would be the obvious
two-dimensional analogue of binary search. Suppose we have conﬁned the search to
a rectangle with top-left corner (x1,y1) and bottom-right corner (x2,y2). What if we
ﬁrst inspected the value f(x,y) where x = ⌊(x1 +x2)/2⌋and y = ⌊(y1 +y2)/2⌋? If
f(x,y)<t, we can throw away all elements of the lower-left rectangle. A picture of
this situation is given in Figure 4.2, in which f(x,y) = z<t and the shaded rectangles
are those we need to keep. Similarly, if f(x,y)>t the upper-right rectangle can be
discarded. And ﬁnally if f(x,y) = t, then both can be discarded.
This strategy will not, of course, maintain the property that the search space is
always a rectangle; instead we will have either two rectangles or an L-shape. We
can split an L-shape into two rectangles by making either a horizontal cut (as in

70
Binary search
the ﬁgure) or a vertical cut. We can then continue the search in both the smaller
rectangles. Without writing down the algorithm, let us see if this approach yields a
faster algorithm by looking at the associated recurrence relation. Consider an m×n
rectangle, and let T(m,n) denote the number of evaluations of f required to search
it in the worst case. If m = 0 or n = 0, there is nothing to search and T(m,n) = 0. If
m = 1 or n = 1, then the problem reduces to one-dimensional binary search and we
have
T(1,n) = 1+T(1,n/2)
T(m,1) = 1+T(m/2,1)
Otherwise, when m ⩾2 and n ⩾2, we can throw away a rectangle of size at least
m/2×n/2. If we make a horizontal cut, then we are left with two rectangles, one of
size m/2×n/2 and the other of size m/2×n. Hence
T(m,n) = 1+T(m/2,n/2)+T(m/2,n)
If we make a vertical cut, then we have
T(m,n) = 1+T(m/2,n/2)+T(m,n/2)
In order to reach a base case quickly, it is better to make a horizontal cut if m ⩽n,
and a vertical cut if m>n.
To solve these recurrences assume m and n are powers of two and deﬁne U by
U(i,j) = T(2i,2j). Supposing i ⩽j and making a horizontal cut, we therefore have
U(0,j)
= j
U(i+1,j+1) = 1+U(i,j)+U(i,j+1)
It is not easy to solve this recurrence, but we can make an educated guess and
assume that the solution is exponential in i. If we set U(i,j) = 2i f(i,j)−1 for some
function f, then we obtain
f(0,j)
= j+1
2f(i+1,j+1) = f(i,j)+f(i,j+1)
The second equation suggests another educated guess, namely that f is a linear
function of i and j. Setting f(i,j) = ai+bj+c, we obtain
bj+c
= j+1
2(a(i+1)+b(j+1)+c) = ai+bj+c+ai+b(j+1)+c
These equations are satisﬁed by taking a = −1/2, b = 1 and c = 1. Putting the pieces
together, we arrive at the solution
U(i,j) = 2i (j−i/2+1)−1
Setting i = log m and j = log n, we therefore have
T(m,n) = 2log m (log n−(log m)/2+1)−1 ⩽m log(2n/√m)
If m ⩾n we should make a vertical cut rather than a horizontal one; then we get an

4.2 A two-dimensional search problem
71
z
x
r
(x1,y1)
(x2,y2)
Figure 4.3 A two-dimensional divide-and-conquer decomposition
algorithm with at most n log(2m/√n) evaluations of f. In either case, if one of m
or n is much smaller than the other we get a better algorithm than saddleback search.
For example, again with f(x,y) = x2 +3y and t = 20259, this method needs only
96 evaluations of f to compute the answer, about half the number of the previous
version.
But we can do better still. As before, suppose we have conﬁned the search to a
rectangle with top-left corner (x1,y1) and bottom-right corner (x2,y2). Assume that
y1 −y2 ⩽x2 −x1, so there are at least as many columns as rows. Suppose we carry
out a binary search
x = smallest (x1 −1,x2) (λx.f(x,r)) t
along the middle row, r = ⌊(y1 +y2)/2⌋. Recall that x is the smallest x in the range
x1 ⩽x ⩽x2, if it exists, such that t ⩽f(x,r); otherwise x = x2. If t <f(x,r), then
we need continue the search only on the two rectangles ((x1,y1),(x −1,r + 1))
and ((x,r −1),(x2,y2)). Figure 4.3 shows a picture of this case, where z = f(x,r).
If f(x,r) = t, then we can cut out column x and continue the search on the two
rectangles ((x1,y1),(x−1,r +1)) and ((x+1,r −1),(x2,y2)). Finally, if f(x,r)>t,
so every entry in the row r is greater that t, then we can continue the search on the
single rectangle ((x1,r −1),(x2,y2)). The reasoning is dual if there are more rows
than columns. As a result, we can eliminate about half the elements of the array
with a logarithmic number of probes. The algorithm incorporating this method is
given in Figure 4.4.
As to the analysis, again let T(m,n) denote the number of evaluations of f required
to search an m×n rectangle. Suppose m ⩽n. In the best case, when each binary
search on a row returns the leftmost or rightmost element, we have
T(m,n) = log n+T(m/2,n)

72
Binary search
search f t = from (0,p) (q,0) where
p = smallest (−1,t) (λy.f(0,y)) t
q = smallest (−1,t) (λx.f(x,0)) t
from (x1,y1) (x2,y2)
| x2 <x1 ∨y1 <y2 = [ ]
| y1 −y2 ⩽x2 −x1 = row x
| otherwise
= col y
where
x = smallest (x1 −1,x2) (λx.f(x,r)) t
y = smallest (y2 −1,y1) (λy.f(c,y)) t
c = (x1 +x2) div 2
r = (y1 +y2) div 2
row x | z<t
= from (x1,y1) (x2,r +1)
| z == t = (x,r):from (x1,y1) (x−1,r +1)++from (x+1,r −1) (x2,y2)
| z>t
= from (x1,y1) (x−1,r +1)++from (x,r −1) (x2,y2)
where z = f (x,r)
col y | z<t
= from (c+1,y1) (x2,y2)
| z == t = (c,y):from (x1,y1) (c−1,y+1)++from (c+1,y−1) (x2,y2)
| z>t
= from (x1,y1) (c−1,y)++from (c+1,y−1) (x2,y2)
where z = f (c,y)
Figure 4.4 The ﬁnal program
with solution T(m,n) = Θ(log m × log n). In the worst case, when each binary
search returns the middle element, we have
T(m,n) = log n+2 T(m/2,n/2)
To solve this recurrence relation, again set U(i,j) = T(2i,2j). Then we have
U(i,j) =
i−1
∑
k=0
2k (j−k) = Θ(2i (j−i+1))
Hence T(m,n) = Θ(m log(1+n/m)). Dually, if n<m, we obtain a running time
of T(m,n) = Θ(n log(1 + m/n)). For our example function f(x,y) = x2 + 3y and
t = 20259, the ﬁnal program of Figure 4.4 needs only 72 evaluations of f to compute
the answer, about three-quarters of the previous best time.
These bounds are asymptotically optimal. Any algorithm for searching an m×n
rectangle has to perform at least
Ω(m log(1+n/m)+n log(1+m/n))
evaluations of f. This lower bound shows that when m = n we cannot do better
than Ω(m+n) comparisons. So saddleback search is the best possible method on a
square grid. But if m<n, then m ⩽n log(1+m/n) since x ⩽log(1+x) if 0 ⩽x ⩽1.
Thus when m ⩽n we have the lower bound Ω (m log(n/m)), and when m>n we
have the lower bound Ω (n log(m/n)).

4.3 Binary search trees
73
The proof of the lower bound depends on the decision tree associated with the
problem. The role of decision trees in putting a lower bound on the running time of a
problem will be explained at the end of the next section in the context of sorting. So,
it is perhaps better to read that section ﬁrst and then come back to what follows. But
here is the idea. Suppose there are A(m,n) different possible answers to the problem.
For example, A(1,1) = 2 because there are two possible outcomes, either an empty
list or a singleton list; and A(2,2) = 6 because the possible outcomes are one empty
list, four possible singleton lists, and one possible doubleton list. Each test of f(x,y)
has three possible outcomes, f(x,y)<t, f(x,y) = t, and f(x,y)>t, so the height h of
the ternary decision tree has to satisfy h ⩾log3 A(m,n). Provided we can estimate
A(m,n), this gives us a lower bound on the number of tests that have to be performed.
To estimate A(m,n), observe that each list of pairs (x,y) in the range 0 ⩽x<n
and 0 ⩽y<m with f(x,y) = z is in a one-to-one correspondence with a step-shaped
path from the top-left corner of the m×n rectangle to the bottom-right corner, in
which the value z appears at the inner corners of the steps. This step shape is not
necessarily the one traced by the function search. The path from the top-left to
bottom-right corner contains m down-moves and n right-moves in some order, so
the number of such paths is
m+n
n

(which is the same as
m+n
m

), so that is the value
of A(m,n).
Another way to calculate A(m,n) is to suppose there are k solutions. The required
value can appear in k rows in exactly
m
k

ways, and for each way there are
n
k

possible choices for the columns. Hence
A(m,n) =
m
∑
k=0
m
k
n
k

=
m+n
n

since the summation is an instance of Vandermonde’s convolution, see [7]. Taking
logarithms, we obtain the lower bound
log A(m,n) = Ω(m log(1+n/m)+n log(1+m/n))
which is the result given above.
4.3 Binary search trees
Binary search trees capture the essence of binary search as a data structure. The
trees are based on the following type:
data Tree a = Null | Node (Tree a) a (Tree a)
A tree either is the null tree or consists of a node, which has a left subtree, a node
value (also called its label), and a right subtree. This kind of tree is different from
the one used in the construction of random-access lists in the previous chapter, in
that values are stored at nodes rather than leaves. In general, trees can be classiﬁed

74
Binary search
according to the precise form of the branching structure, the location of information
in the tree, the presence or otherwise of subsidiary information, and the relationship
between the information stored in different parts of the tree. We will encounter other
kinds of tree in subsequent chapters.
The size of a tree is the number of labels it contains:
size::Tree a →Nat
size Null
= 0
size (Node l x r) = 1+size l+size r
The values in a tree can be turned into a list by the function ﬂatten:
ﬂatten::Tree a →[a]
ﬂatten Null
= [ ]
ﬂatten (Node l x r) = ﬂatten l++[x]++ﬂatten r
The running time of this deﬁnition of ﬂatten is not linear in the size of the tree,
an issue we have encountered before in Exercise 3.8. The solution is to use an
accumulating parameter (see the exercises).
By deﬁnition, a tree is a binary search tree if ﬂattening it returns a list of values
in strictly increasing order. Thus the label of a binary search tree is greater than any
label in its left subtree and smaller than any label in its right subtree.
The deﬁnition of a binary search tree can be modiﬁed in various ways. For
example, one can allow duplicate node labels, so that ﬂattening the tree produces
a list only in nondecreasing order. More useful in practice is to allow labels to be
records of some kind, with each record containing a key ﬁeld unique to that record.
The tree is ordered by key, so ﬂattening it produces a list of records in increasing
order of key. Such trees can be used to search dictionaries, in which the keys are
‘words’ of some kind, and the records contain information associated with a given
word.
Here is the counterpart of binary search in terms of records and key ﬁelds:
search::Ord k ⇒(a →k) →k →Tree a →Maybe a
search key k Null = Nothing
search key k (Node l x r)
| key x<k
= search key k r
| key x == k = Just x
| key x>k
= search key k l
The search returns Nothing if there is no record with the given key; otherwise
it returns Just x, the (unique) record with the given key. The tree is searched by
following either the left or the right subtree of a node depending on whether the key
at the node is greater than or less than the given key. In the worst case, the search
takes time proportional to the height of the tree, where

4.3 Binary search trees
75
height ::Tree a →Nat
height Null
= 0
height (Node l x r) = 1+max (height l) (height r)
Thus the search is guaranteed to take O(log n) steps for a tree of size n only if its
height is O(log n). Later on we will see how to ensure that the height of a tree is
logarithmic in its size.
Although two trees of the same size need not have the same height, the two
measures are not independent. The height h and size n of a tree satisfy the rela-
tionship h ⩽n<2h. In particular, h ⩾⌈log(n+1)⌉. The proof of this fundamental
relationship is by structural induction, a proof we leave as an exercise. By deﬁnition
a tree is balanced if the heights of the left and right subtrees of each node differ by
at most one. There are other deﬁnitions of what it means for a tree to be balanced,
but we will stick to this one. Although a balanced tree of size n need not have the
minimum possible height ⌈log(n+1)⌉, its height is always reasonably small. More
precisely, if t is a balanced tree of size n and height h, then we have
h ⩽1.4404 log(n+1)+Θ(1)
The proof of this result uses induction in rather an indirect way. Suppose H(n) is
the maximum possible height of a balanced tree of size n. Our objective is to put an
upper bound on H(n). We will do this by turning the problem around. Suppose S(h)
is the minimum possible size of a balanced tree of height h. Taking a tree of size n
and height H(n), we therefore have S(H(n)) ⩽n. Hence we can put an upper bound
on H(n) by putting a lower bound on S(n): if S(n) ⩾f(n), then n ⩾f(H(n)) and so
f −1(n) ⩾H(n).
Since Null is the only tree with height 0, it is clear that S(0) = 0. Similarly, there
is only one kind of tree with height 1, so S(1) = 1. The smallest possible balanced
tree with height h+2 has two balanced subtrees, one with height h+1 and the other
with height h. Hence
S(h+2) = S(h+1)+S(h)+1
It is at this point that induction comes in. A simple induction argument shows that
S(h) = ﬁb(h+2)−1, where ﬁb is the Fibonacci function. To complete the proof we
will need the following fact about the Fibonacci function, which can also be proved
by induction. Let ϕ and ψ be the two roots of the equation x2 −x−1 = 0, that is,
ϕ = (1+
√
5)/2 and ψ = (1−
√
5)/2. Then ﬁb(n) = (ϕn −ψn)/
√
5. Furthermore,
since ψn <1, we obtain that ﬁb(n)>(ϕn −1)/
√
5. Hence
(ϕH(n)+2 −1)/
√
5−1<ﬁb(H(n)+2)−1 = S(H(n)) ⩽n
Taking logarithms, we obtain
(H(n)+2) log ϕ <log(n+1)+Θ(1)
Since log ϕ >1/1.4404, the result now follows.

76
Binary search
We now turn to the task of building a balanced binary search tree from a list of
distinct values. One way to build a tree, though the result is not necessarily balanced,
is to partition the list into two lists, one containing those elements smaller than some
ﬁxed element, and those elements which are not. That leads to
mktree::Ord a ⇒[a] →Tree a
mktree [ ]
= Null
mktree (x:xs) = Node (mktree ys) x (mktree zs)
where (ys,zs) = partition (<x) xs
partition p xs = (ﬁlter p xs,ﬁlter (not ·p) xs)
In the best case, when partitioning splits a list of length n into two lists of lengths
n/2, the running time T(n) satisﬁes T(n+1) = 2T(n/2)+Θ(n). This recurrence, a
slight variant of one we have seen before, also has the solution T(n) = Θ(n log n).
In the worst case, when partitioning splits a list of length n into a list of length 0 and
a list of length n, the recurrence relation is T(n+1) = T(n)+Θ(n) with solution
T(n) = Θ(n2).
In order to construct an efﬁcient version of mktree that guarantees a balanced
tree, we need to maintain information about the heights of the subtrees of a node.
The way to do that is to modify the type Tree to read
data Tree a = Null | Node Nat (Tree a) a (Tree a)
The extra label in a node is the height of the tree. Thus, we have
height Null
= 0
height (Node h
) = h
We can build these augmented trees with the help of a smart constructor node,
deﬁned by
node::Tree a →a →Tree a →Tree a
node l x r = Node h l x r where h = 1+max (height l) (height r)
We will meet another smart constructor in a moment, and yet a third later on.
A balanced tree can be constructed by inserting values one by one into an initially
empty tree:
mktree::Ord a ⇒[a] →Tree a
mktree = foldr insert Null
The deﬁnition of insert starts off easily enough:
insert x Null = node Null x Null
insert x (Node h l y r)
| x<y
= balance (insert x l) y r
| x == y = Node h l y r
| y<x
= balance l y (insert x r)

4.3 Binary search trees
77
The value x is discarded if it is already present in the tree; otherwise x is inserted
into either the left subtree or the right subtree. But we cannot simply apply node to
the result since the result may not be a balanced tree. That is where balance comes
in. It is a second smart constructor, smarter than node in that it restores balance as
well as installing height information.
To implement balance we have to consider three cases. First of all, observe that
a single insertion can increase the height of a tree by at most one. That means
it is sufﬁcient to implement balance under the assumption that both subtrees are
balanced and that they differ in height by at most two. The easy case is when the
two subtrees differ in height by at most one. Then we can implement balance by
node. The other two cases are entirely symmetrical, so we shall consider just the
case when the left subtree has height two more than the right subtree, that is,
height l = height r +2
We have to inspect the subtrees of the left subtree, so let l have left subtree ll and
right subtree rl. In the ﬁrst case, suppose height rl ⩽height ll. Because l is assumed
to be balanced, all of the following four relationships hold:
height r = height l−2 = height ll−1 ⩽height rl ⩽height ll
In this case we can implement balance with a right rotation:
balance l x r = rotr (node l x r)
rotr (Node
(Node
ll y rl) x r) = node ll y (node rl x r)
Here is a picture of a right rotation:
x
y
r
ll
rl
=⇒
y
ll
x
rl
r
To check this leads to a balanced tree, we reason as follows:
abs (height ll−height (node rl x r))
=
{ deﬁnition of height }
abs (height ll−1−height rl max height r)
=
{ since height r ⩽height rl (see above) }
abs (height ll−1−height rl)
⩽
{ since height ll−1 ⩽height rl ⩽height ll (see above) }
1
Thus the tree on the right of the picture is indeed balanced.

78
Binary search
In the second case we have height ll < height rl. But again l is assumed to be
balanced, so
height ll+1 = height rl
In this case we have to inspect the subtrees of rl, so let lrl and rrl be the left and
right subtrees of rl. In this case all of the following relationships hold:
height r = height l−2 = height rl−1 = height ll = height lrl max height rrl
In this case we can implement balance with a left rotation followed by a right
rotation:
balance l x r = rotr (node (rotl l) x r)
rotl (Node
ll y (Node
lrl z rrl)) = node (node ll y lrl) z rrl
Here is the picture:
x
y
r
ll
z
lrl
rrl
=⇒
x
y
r
ll
z
lrl
rrl
x
To check this leads to a balanced tree, note that
balance l x r = rotr (node (node (node ll y lrl) z rrl) x r)
= node (node ll y lrl) z (node rrl x r)
We can then argue as follows:
abs (height (node ll y lrl)−height (node rrl x r))
=
{ deﬁnition of height }
abs (height ll max height lrl−height rrl max height r)
=
{ since height rrl ⩽height r (see above) }
abs (height ll max height lrl−height r)
=
{ since height lrl ⩽height ll (see above) }
abs (height ll−height r)
=
{ since height ll = height r (see above) }
0
The remaining case height r = height l+2 is treated in an entirely dual manner. To
give the complete deﬁnition of balance we will need a function bias, deﬁned by
bias::Tree a →Int
bias (Node
l x r) = height l−height r

4.3 Binary search trees
79
Then the full deﬁnition of balance is given by
balance::Tree a →a →Tree a →Tree a
balance t1 x t2
| abs (h1 −h2) ⩽1 = node
t1 x t2
| h1 == h2 +2
= rotateR t1 x t2
| h2 == h1 +2
= rotateL t1 x t2
where h1 = height t1; h2 = height t2
rotateR t1 x t2 = if 0 ⩽bias t1 then rotr (node t1 x t2)
else rotr (node (rotl t1) x t2)
rotateL t1 x t2 = if bias t2 ⩽0 then rotl (node t1 x t2)
else rotl (node t1 x (rotr t2))
The deﬁnition returns an error when applied to two trees whose heights differ by
more than two, but it is easy enough to deﬁne a balancing function that works for
two trees of any height. This function, which we will call gbalance, will be needed
in the following section. To compute gbalance t1 x t2, suppose ﬁrst that h1 >h2 +2.
In this case the subtrees r1,r2,... along the right spine of t1 can be traversed to ﬁnd
a subtree r = rk satisfying
0 ⩽height r −height t2 ⩽1
Such a tree is guaranteed to exist because the subtrees r1,r2,... decrease in height
by at least one and at most two at each step. Furthermore, if l is the left-sibling of r,
then
abs (height l−height (node r x t2)) ⩽2
because t1 is a balanced tree and abs (height l −height r) ⩽1. That means l and
node r x t2 can be combined with balance. Rebalancing can increase the height of a
tree by at most one, so further rebalancing up the tree maintains the precondition on
balance. The traversal is captured by balanceR, deﬁned by
balanceR::Set a →a →Set a →Set a
balanceR (Node
l y r) x t2 = if height r ⩾height t2 +2
then balance l y (balanceR r x t2)
else balance l y (node r x t2)
The situation is dual when h2 > h1 + 2 and is expressed by a function balanceL,
whose deﬁnition is left as an exercise. It is clear that balanceR t1 x t2 takes O(h1−h2)
steps, where h1 and h2 are the heights of t1 and t2. Dually, balanceL t1 x t2 takes
O(h2 −h1) steps.
With that, the complete deﬁnition of gbalance is as follows:

80
Binary search
gbalance::Set a →a →Set a →Set a
gbalance t1 x t2
| abs (h1 −h2) ⩽2 = balance
t1 x t2
| h1 >h2 +2
= balanceR t1 x t2
| h1 +2<h2
= balanceL t1 x t2
where h1 = height t1; h2 = height t2
Evaluation of balance certainly takes constant time, even though gbalance does not.
That means each insertion takes logarithmic time in the size of the tree, and building
the tree takes Θ(n log n) steps.
Can we build a binary search tree from a list of elements over an arbitrary ordered
type in better than Θ(n log n) steps? To answer this question, observe ﬁrst that any
information we can discover about the elements of the underlying type arises solely
as a result of comparison tests of the form x ⩽y or x == y between the elements.
That means it is sufﬁcient to put a lower bound to the number of such comparisons
required in the construction of the tree. If we can show that, say, Ω(f(n)) comparison
tests are needed in the worst case, then that is a lower bound on the total time to
build the tree. The argument is not valid when we are building a tree of integers or
words, since there may be cunning methods that avoid comparison tests altogether.
Now suppose the computation of mktree on a list of length n can be achieved with
B(n) comparison tests of the form x ⩽y. Then, since we can sort a list of elements
over an arbitrary ordered type by
sort ::(Ord a) ⇒[a] →[a]
sort = ﬂatten·mktree
and since ﬂatten involves no comparisons whatsoever, it follows that sorting a list
of elements can be achieved with B(n) comparisons.
We now put a lower bound to B(n). Every algorithm based on binary comparisons
can be associated with a certain tree, called a decision tree. The decision tree is a
binary tree whose labels are binary comparisons of the form x ⩽y. The left subtree
is a decision tree for the case x ⩽y and the right subtree is a decision tree for the case
x>y. The execution of any sorting algorithm based on binary comparisons traces
a path from the root of the decision tree to a leaf, each leaf being associated with
a unique permutation that sorts the list. Each permutation of the input determines
a sorted list, so there have to be at least n! leaves for an input of length n because
there are n! possible permutations that can sort the list. Since a binary tree of height
h has no more than 2h leaves, the decision tree has to have some height h such that
n! ⩽2h. Taking logarithms, that means h ⩾log(n!). To estimate the right-hand side,
we can use Stirling’s approximation (see Answer 2.5) to arrive at h = Ω(n log n).
But h estimates the total number of comparisons that may be needed to sort the list
in the worst case, so we have our lower bound B(n) = Ω(n log n). So, the answer

4.4 Dynamic sets
81
to the original question is: No, we cannot build a binary search tree in better than
Θ(n log n) steps.
4.4 Dynamic sets
Sets that can grow or shrink over time are called dynamic sets. Operations on such
sets include a membership test, adding a value to a set, and deleting an element
from a set. One may also want to take the union of two sets, or to split a set into two
sets, one containing those elements at most some given value, and the other those
elements greater than the value. As a special case of set union, one may also want
to combine two sets when it is known that the elements of the ﬁrst are all less than
any element of the second. Thus, splitting a set and then combining the results gives
back the original set.
In this section we will show how to implement these operations when sets are
represented by balanced binary search trees:
type Set a = Tree a
The membership test is a simple variant of the function search of the previous
section:
member ::Ord a ⇒a →Set a →Bool
member x Null
= False
member x (Node
l y r) | x<y
= member x l
| x == y = True
| x>y
= member x r
Insertion is implemented by the function insert of the previous section. The function
delete is more interesting:
delete::Ord a ⇒a →Set a →Set a
delete x Null
= Null
delete x (Node
l y r) | x<y
= balance (delete x l) y r
| x == y = combine l r
| x>y
= balance l y (delete x r)
Deleting a single value from a tree can reduce its height by at most one, so the smart
constructor balance can be used to restore balance. Recall that balance t1 x t2 was
deﬁned only for the case that the heights of t1 and t2 differed by at most two. That
leaves combine, which in effect has to concatenate two trees.
We will give two deﬁnitions of combine, the second of which generalises the ﬁrst.
In the ﬁrst deﬁnition, combine is deﬁned only for two balanced trees that differ in
height by at most one. This is certainly sufﬁcient for its use in delete. The easy case
is when one of the trees is null; in such a case we can simply return the other tree.

82
Binary search
When neither tree is null we have to ﬁnd an appropriate label for the combined tree,
and there are two sensible options: either take the leftmost label of the second tree,
or the rightmost label of the ﬁrst tree. We choose the former, deﬁning deleteMin by
deleteMin::Ord a ⇒Set a →(a,Set a)
deleteMin (Node
Null x r) = (x,r)
deleteMin (Node
l x r)
= (y,balance t x r) where (y,t) = deleteMin l
The function deleteMin returns the minimum element of a nonempty set, together
with the set that remains after deleting the minimum element. The function balance
can then be invoked to ensure that this set is balanced (see Exercise 4.15). Now we
can deﬁne combine by
combine::Ord a ⇒Set a →Set a →Set a
combine l Null = l
combine Null r = r
combine l r
= balance l x t where (x,t) = deleteMin r
The second deﬁnition of combine is exactly the same, except that balance is replaced
by the more general function gbalance of the previous section. Therefore combine
can be used to combine any two sets as long as all the elements of the ﬁrst set are
less than any element of the second set. Combining two sets of sizes m and n takes
O (log n+log m) steps.
The ﬁnal function we will implement is a function split with type
split ::Ord a ⇒a →Set a →(Set a,Set a)
The value of split x t is a pair of sets, the ﬁrst containing those elements of t which
are at most x, and the second those elements that are greater than x. Thus, combining
the two sets gives back the original set. In symbols,
split x xs = (ys,zs) ⇒combine ys zs = xs
Even more brieﬂy, uncurry combine·split x = id. For example, consider the 1973
two-volume edition of the Shorter Oxford English Dictionary. If soed is the set
of all the words in the dictionary, then the contents of each volume is given by
split "Markworthy" soed.
To deﬁne split we have to split a tree into pieces and then sew the pieces together
to make the ﬁnal pair of sets:
split x t = sew (pieces x t)
A piece consists of a tree minus one of its subtrees, so it consists of a label and
either a left or a right subtree:
data Piece a = LP (Set a) a | RP a (Set a)
A left piece LP l x is missing its right subtree, and a right piece RP x r is missing its
left subtree. The function pieces is deﬁned by

4.4 Dynamic sets
83
pieces::Ord a ⇒a →Set a →[Piece a]
pieces x t = addPiece t [ ] where
addPiece Null ps
= ps
addPiece (Node
l y r) ps | x<y = addPiece l (RP y r :ps)
| x ⩾y = addPiece r (LP l y:ps)
For example, pieces 9 t, where t is the tree
10
8
15
6
9
14
20
produces the list of three pieces
9
8
6
10
15
14
20
in which the missing tree is indicated by a dashed line. We can sew a list of pieces
together by
sew::[Piece a] →(Set a,Set a)
sew = foldl step (Null,Null)
where step (t1,t2) (LP t x) = (gbalance t x t1,t2)
step (t1,t2) (RP x t) = (t1,gbalance t2 x t)
For example, sewing the three pieces above produces the two trees
8
6
9
15
10
20
14
We claim that split x t takes O(h) steps, where h = height t. Certainly, pieces x t
takes this time, so we have to show that sew does too. If we deﬁne the height of a
piece to be the height of the tree associated with the piece, then pieces x t produces
a list of pieces whose heights h1,h2,...,hk are strictly increasing and bounded above
by h. For example, the pieces pictured above have heights 0,1,2. The total cost of
sew is proportional to

84
Binary search
(h1 −0)+(h2 −h1)+...+(hk −hk−1) ⩽h
because each call of gbalance t1 x t2 takes time proportional to the difference
between the heights of t1 and t2. Thus both piece and sew take logarithmic time in
the size of the sets, so split does too. We will need combine and a variant of split in
Chapter 14.
4.5 Chapter notes
According to Knuth [10], the ﬁrst publication to describe binary search (for the
special case n = 2k −1) appeared in 1946. The ﬁrst version that worked for all n was
not published until 1960. Binary search is easy to get wrong; see Bentley’s book [4]
for an interesting discussion of his experiences in getting professional programmers
to implement binary search. Saddleback search was so named by David Gries, see
[3, 6, 8], probably because the shape of the three-dimensional grid, with the smallest
element at the bottom left, the largest at the top right, and two wings, is a bit like an
equestrian saddle.
The functions sew and pieces of the ﬁnal section are closely related to a more
general way of taking a tree apart and joining two trees, called the Zipper; see [9].
Balanced binary trees are also called AVL trees, after their inventors Adelson-
Velski˘ı and Landis [1]; see also [10]. There are many other balanced tree schemes;
for example [5] describes red–black trees, and [2] describes a simple scheme based
on two operations, skew and split.
References
[1]
Georgy M. Adelson-Velski˘ı and Evgenii M. Landis. An algorithm for the organisation
of information. Soviet Mathematics – Doklady, 3(5):1259–1263, 1962. English
translation in Doklady Akademia Nauk SSSR, 146(2): 263–266.
[2]
Arne Andersson. Binary trees made simple. In F. Dehne, J. R. Sack, N. Santoro, and
S. Whitesides, editors, Workshop on Algorithm Design and Data Structures, volume
709 of Lecture Notes in Computer Science, pages 60–71, Springer-Verlag, Berlin,
1993.
[3]
Roland Backhouse. Program Construction and Veriﬁcation. Prentice-Hall, Hemel
Hempstead, 1986.
[4]
Jon Bentley. Programming Pearls. Addison-Wesley, Reading, MA, 1986.
[5]
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.
Introduction to Algorithms. MIT Press, Cambridge, MA, third edition, 2009.
[6]
Edsger W. Dijkstra. The saddleback search. EWD-934,
http://www.cs.utexas.edu/users/EWD/index09xx.html, 1985.
[7]
Ronald L. Graham, Donald E. Knuth, and Oren Patashnik. Concrete Mathematics.
Addison-Wesley, Reading, MA, second edition, 1994.
[8]
David Gries. The Science of Programming. Springer, New York, 1981.
[9]
G´erard Huet. The Zipper. Journal of Functional Programming, 7(5):549–554, 1997.
[10] Donald E. Knuth. The Art of Computer Programming, volume 3: Sorting and
Searching. Addison-Wesley, Reading, MA, second edition, 1998.

Exercises
85
Exercises
Exercise 4.1 The rule of ﬂoors states that for integers n and real numbers r we
have n ⩽⌊r⌋⇔n ⩽r. This useful rule will appear in a number of problems. Using
just the rule of ﬂoors (no case analysis) prove that for integers a and b we have
a<(a+b) div 2<b if and only if a+1<b.
The dual rule of ceilings states that for integers n and real numbers r we have
⌈r⌉⩽n ⇔r ⩽n. Using this rule prove that if h is an integer such that n<2h, then
⌈log(n+1)⌉⩽h. This result was stated in Section 4.3.
Exercise 4.2 Look again at the expression
head ([x | x ←[a+1..m],t ⩽f x]++[x | x ←[m+1..b],t ⩽f x])
where a<m<b and f(a)<t ⩽f(b). If nothing else is assumed about f, then we
cannot assert that the ﬁrst list is empty if f(m)<t. Nevertheless, the deﬁnition of
smallest (a,b) returns some value. What is it?
Exercise 4.3 Recall that the exact number T(n) of evaluations of f required in the
worst case of evaluating smallest (a,b) f t, where n = b −a + 1 is the number of
integers in the interval, satisﬁes T(2) = 0 and
T(n) = T(⌈(n+1)/2⌉)+1
for n>2. Use the rule of ceilings to show that T(n) = ⌈log(n−1)⌉.
Exercise 4.4 Following on from the previous question, given that f(a)<t ⩽f(b),
show that any algorithm for computing smallest (a,b) f t requires ⌈log(n −1)⌉
comparison tests of the form t ⩽f(x).
Exercise 4.5 What are the positions of 472 in the grid of Figure 4.1?
Exercise 4.6 With f(x,y) = x3 + y3, what is the result of saddleback search for
t = 1729? Does the ﬁnal algorithm return the same result?
Exercise 4.7 To obtain a linear-time deﬁnition of ﬂatten we can use an accumulat-
ing parameter. There is more than one way of doing so, but a simple method is to
introduce ﬂatcat deﬁned by
ﬂatcat ::Tree a →[a] →[a]
ﬂatcat t xs = ﬂatten t ++xs
We have ﬂatten t = ﬂatcat t [ ], so it remains to produce a recursive deﬁnition of
ﬂatcat that does not use ﬂatten or any ++ operations. Give details of the synthesis.

86
Binary search
Exercise 4.8 Prove by structural induction that
height t ⩽size t <2height t
for all binary trees t.
Exercise 4.9 The deﬁnition
partition p xs = (ﬁlter p xs,ﬁlter (not ·p) xs)
involves two traversals of its second argument. Give a deﬁnition of partition that
makes only one traversal of the input.
Exercise 4.10 Another way to build a binary tree for a list containing duplicates is
to build a tree of type Tree [a] in which node labels are lists of equal values. Show
how to build such a tree.
Exercise 4.11 Consider the recurrences
B(n+1) = 2B(n/2)+Θ(n)
W(n+1) = W(n)+Θ(n)
for the best and worst cases for building a binary search tree by partitioning the
input. Prove that B(n) = Θ(n log n) and W(n) = Θ(n2).
Exercise 4.12 Show that log(n!) = Ω(n log n) without using Stirling’s approxima-
tion.
Exercise 4.13 For the (second) deﬁnition of combine we have
ﬂatten (combine t1 t2) = ﬂatten t1 ++ﬂatten t2
Anticipating the following chapter, give a deﬁnition of merge for which
ﬂatten (union t1 t2) = merge (ﬂatten t1) (ﬂatten t2)
Exercise 4.14 One method of deﬁning union is to ﬂatten one of the trees and then
insert the elements one by one into the other tree:
union::Ord a ⇒Set a →Set a →Set a
union t1 t2 = foldr insert t1 (ﬂatten t2)
Supposing the ﬁrst tree has size m and the second tree has size n, how long does
union take?
Another method is to ﬂatten both trees, merge the results to obtain a sorted list,
and then to build a tree from the sorted list:
union t1 t2 = build (merge (ﬂatten t1) (ﬂatten t2))

Answers
87
We can build a tree from a sorted list in linear time if we bring in arrays. Here is the
ﬁrst line of the deﬁnition:
build xs = from (0,n) (listArray (0,n−1) xs) where n = length xs
(recall from Chapter 3 that the expression listArray (0,length xs −1) xs converts
a list of length n into an array whose indices run from 0 to n −1). Construct a
deﬁnition of from. How long does this method of deﬁning union take?
Exercise 4.15 Why is the use of balance justiﬁed in the deﬁnitions of deleteMin
and combine?
Exercise 4.16 Give the deﬁnition of balanceL.
Exercise 4.17 Suppose pair f (x,y) = (f x,f y). Using pair give a one-line, linear-
time deﬁnition of split x.
Answers
Answer 4.1 Here is the proof:
a<(a+b) div 2<b
⇔
{ deﬁnition of div }
a<⌊(a+b)/2⌋<b
⇔
{ arithmetic }
a+1 ⩽⌊(a+b)/2⌋<b
⇔
{ rule of ﬂoors (twice) }
a+1 ⩽(a+b)/2<b
⇔
{ arithmetic }
a+1<b
The rule of ﬂoors is used twice in the above proof, the second appeal being the
equivalent form: ⌊r⌋<x ⇔r <x.
For the second part we have n<2h ⇔n+1 ⩽2h ⇔log(n+1) ⩽h, and the result
follows by appeal to the rule of ceilings.
Answer 4.2 We have smallest (a,b) = x ⇒f(x)<t ⩽f(x+1).
Answer 4.3 The proof is by induction and for the induction step we have to show
⌈log(n−1)⌉= ⌈log(⌈(n+1)/2⌉−1)⌉+1
To do so, we can reason by indirect equality, showing that a = b by showing a ⩽k
if and only if b ⩽k for all k. Using the rule of ceilings on the left-hand side gives
⌈log(n−1)⌉⩽k ⇔n−1 ⩽2k. On the right-hand side we have

88
Binary search
⌈log(⌈(n+1)/2⌉−1)⌉+1 ⩽k
⇔
{ arithmetic }
⌈log(⌈(n+1)/2⌉−1)⌉⩽k −1
⇔
{ rule of ceilings }
log(⌈(n+1)/2⌉−1) ⩽k −1
⇔
{ arithmetic }
⌈(n+1)/2⌉−1 ⩽2k−1
⇔
{ arithmetic }
⌈(n+1)/2⌉⩽2k−1 +1
⇔
{ rule of ceilings }
(n+1)/2 ⩽2k−1 +1
⇔
{ arithmetic }
n−1 ⩽2k
establishing the result.
Answer 4.4 From the answer to Exercise 4.2, we have that smallest (a,b) f t can
return any one of b−a answers, namely those of the form f(x)<t ⩽f(x+1) for
some x in the range a ⩽x<b. A decision tree with internal nodes labelled with tests
of the form t ⩽f(x) therefore has to have a height h satisfying 2h ⩾b−a. Since
b−a = n−1 we have the lower bound h ⩾⌈log(n−1)⌉.
Answer 4.5 The positions are (0,9), (5,6), (7,5), and (9,0).
Answer 4.6 The four answers are (9,10), (10,9), (1,12), and (12,1). The only
issue is the order in which these four values are produced. With saddleback search
the answers are found in the order (12,1),(10,9),(9,10),(1,12), but they are listed
in reverse order. With the ﬁnal algorithm it depends on the order in which the
subrectangles are searched. It turns out that the ﬁnal algorithm produces the list
[(9,10),(1,12),(10,9),(12,1)].
Answer 4.7 Here is the recursive case:
ﬂatcat (Node l x r) xs
=
{ speciﬁcation of ﬂatcat }
ﬂatten (Node l x r)++xs
=
{ deﬁnition of ﬂatten }
ﬂatten l++[x]++ﬂatten r ++xs
=
{ speciﬁcation of ﬂatcat }
ﬂatten l++[x]++ﬂatcat r xs
=
{ speciﬁcation of ﬂatcat }
ﬂatcat l (x:ﬂatcat r xs)

Answers
89
Answer 4.8 There are two cases, depending on whether the tree is Null or a Node.
The former case is easy. For the latter, we will prove only the second inequality,
which can be written in the form size t ⩽2height t −1 since both size and height are
integers. We reason as follows:
size (node l x y)
=
{ deﬁnition of size }
size l+1+size r
⩽
{ induction hypothesis }
2height l −1+1+2height r −1
⩽
{ arithmetic }
21+max (height l) (height r) −1
⩽
{ deﬁnition of height }
2height t −1
Answer 4.9 We have partition p [ ] = ([ ],[ ]). Setting (ys,zs) = ﬁlter p xs gives
partition p (x:xs)
=
{ deﬁnition of partition }
(ﬁlter p (x:xs),ﬁlter (not ·p) (x:xs))
=
{ deﬁnition of ﬁlter }
if p x then (x:ys,zs) else (ys,x:zs)
from which we obtain
partition p xs = foldr op ([ ],[ ]) xs
where op x (ys,zs) = if p x then (x:ys,zs) else (ys,x:zs)
Answer 4.10 The best way of computing mktree is to partition the input into three
lists: those elements less than some given element, those elements equal to it, and
those elements greater than it:
partition3::Ord a ⇒a →[a] →([a],[a],[a])
partition3 y = foldr op ([ ],[ ],[ ])
where op x (us,vs,ws) | x<y
= (x:us,vs,ws)
| x == y = (us,x:vs,ws)
| x>y
= (us,vs,x:ws)
Now we can deﬁne
mktree::Ord a ⇒[a] →Tree [a]
mktree [ ] = Null
mktree xs = Node (mktree us) vs (mktree ws)
where (us,vs,ws) = partition3 (head xs) xs
This deﬁnition of partition3 will be needed in Chapter 6.

90
Binary search
Answer 4.11 By unfolding the recurrence for B we obtain
B(n+1) = cn+2B(n/2)
= 2cn+4B((n/2−1)/2)
⩽2cn+4B(n/4)
⩽...
⩽kcn+2k B(n/2k)
giving B(n) = O(n log n). We also have B(n) = Ω(n log n). Similarly,
W(n+1) = cn+W(n)
= cn+c (n−1)+W(n−1)
= cn+c (n−1)+···+c
= cn(n+1)/2
giving W(n) = Θ(n2).
Answer 4.12 For even n we have
n! ⩾n(n−1)(n−2)···(n/2) ⩾(n/2)n/2
so log(n!) ⩾(n/2) log(n/2) ⩾(n/4) log n for n ⩾4. The case for odd n is similar.
Answer 4.13 The deﬁnition is
merge::Ord a ⇒[a] →[a] →[a]
merge [ ] ys
= ys
merge xs [ ]
= xs
merge (x:xs) (y:ys) | x<y
= x:merge xs (y:ys)
| x == y = x:merge xs ys
| x>y
= y:merge (x:xs) ys
The function merge merges two sorted lists, removing duplicates. Merging two lists
of lengths m and n takes Θ(m+n) steps. We will return to merging in the following
chapter.
Answer 4.14 Inserting a new element into a balanced tree of size m takes Θ(log m)
steps and produces a tree of size m+1. Hence, if we do it for n elements, then the
number of steps is
log m+log(m+1)+···+log(m+n−1)
which is Θ ((m+n) log(m+n)) steps.
The deﬁnition of from is
from (l,r) xa =
if l == r then Null else node (from (l,m) xa) (xa!m) (from (m+1,r) xa)
where m = (l+r) div 2
This method of deﬁning union takes Θ(m+n) steps.

Answers
91
Answer 4.15 In the case of deleteMin, if l and r are two trees whose height differ-
ence is at most one and t is a tree whose height differs from that of l by at most one,
then the height difference of t and r is at most two, meeting the precondition on
balance. The argument for combine is similar.
Answer 4.16 We have
balanceL::Set a →a →Set a →Set a
balanceL t1 x (Node
l y r) = if height l ⩾height t1 +2
then balance (balanceL t1 x l) y r
else balance (node t1 x l) y r
Answer 4.17 We have split x = pair mktree·partition (⩽x)·ﬂatten.


Chapter 5
Sorting
If binary search is the simplest example of the divide-and-conquer strategy, then
sorting is arguably the most representative. By sorting we mean putting the elements
of a given list into nondecreasing order. In this chapter we consider two basic
divide-and-conquer algorithms for sorting. Nothing is assumed about the elements
of the input except that they can be compared under ⩽, so the type of sort is
sort :: Ord a ⇒[a] →[a]. In both algorithms the problem is divided into two
subproblems, each of about half the size of the original, which are then combined
to give the ﬁnal result. Together, the dividing and combining phases involve Θ(n)
comparisons on an input of length n, so the associated recurrence relation takes the
form
T(n) = 2T(n/2)+Θ(n)
which we have seen has the solution T(n) = Θ(n log n). As we have also seen at
the end of the previous chapter, this is asymptotically the best bound for a sorting
algorithm based on comparisons.
We will also consider one other comparison-based algorithm, and two more that
assume additional properties of the elements. All ﬁve algorithms have a common
theme, which is that sorting can be viewed as a two-stage process in which one ﬁrst
builds a tree of some kind and then ﬂattens it. In symbols,
sort = ﬂatten·mktree
It seems a bit wasteful of space to erect a potentially large data structure and then
demolish it, but under lazy evaluation the tree only exists in very small pieces at any
one moment. In any case, it is usually easy to synthesise another deﬁnition of sort
in which the tree no longer appears. Building a tree encapsulates the division stage
of a divide-and-conquer algorithm, while ﬂattening it captures the combining phase.
There are many different ways of sorting and it is not straightforward to say which
is best. When expressed functionally, some famous sorting algorithms have different
characteristics from when expressed as imperative code. What is a good algorithm

94
Sorting
in one setting may not be so good in the other. Naturally, we will concentrate only
on good functional sorting algorithms.
A good sorting algorithm should aim for four qualities, whose combination is
not always easy to achieve. First, it should be fast. Ideally, not only should it be
asymptotically optimal in the number of comparisons it makes, but the constants
involved in other operations should also be small. What would you prefer for sorting
a list of small size n, an algorithm that takes 2n2 steps or one that takes 1000n log n
steps? An algorithm with a blindingly fast performance on average, but a quadratic-
time performance in the worst case, might be acceptable. But then again it might
not. Second, the algorithm should be smooth, meaning that the more sorted the
input is, the faster the algorithm performs. In real life, large amounts of data are
unlikely to be in truly random order and a good algorithm should take advantage
of this fact. Third, the algorithm should be stable. When sorting records by key
values, records with equal keys should appear in the same order in the output as in
the input. One can always convert an unstable sorting algorithm into a stable one
by ﬁrst recording the position of each element in the list, then sorting the input by
key, keeping elements with equal keys in separate ‘buckets’. Then each bucket is
sorted by the positions of the elements in the original list. Finally, the positions
are discarded. Bucket sort is one of the algorithms we will describe later in the
chapter. Finally, a sorting algorithm should be compact, meaning that it should be
economical in its use of space as well as running time. This is much more difﬁcult
to achieve in a purely functional setting, especially a lazy one, and we will quietly
ignore the problem of compactness in what follows. In summary, sorting algorithms,
like cars, should be fast, smooth, stable, and compact.
5.1 Quicksort
Following on from the previous chapter, our ﬁrst sorting algorithm arises as a result
of ﬂattening a binary search tree. Here is the relevant data type again:
data Tree a = Null | Node (Tree a) a (Tree a)
The function mktree builds a tree:
mktree::Ord a ⇒[a] →Tree a
mktree [ ]
= Null
mktree (x:xs) = Node (mktree ys) x (mktree zs)
where (ys,zs) = partition (<x) xs
The function partition (<x) splits a list into two lists, comprising those elements
less than x and those not less than x:

5.1 Quicksort
95
partition::(a →Bool) →[a] →([a],[a])
partition p xs = foldr op ([ ],[ ]) xs
where op x (ys,zs) = if p x then (x:ys,zs) else (ys,x:zs)
The function ﬂatten ﬂattens a tree:
ﬂatten::Tree a →[a]
ﬂatten Null
= [ ]
ﬂatten (Node l x r) = ﬂatten l++[x]++ﬂatten r
Now deﬁne
qsort = ﬂatten·mktree
It is easy to eliminate the tree (see Exercise 5.2) and the result is one version of the
famous algorithm known as Quicksort:
qsort ::Ord a ⇒[a] →[a]
qsort [ ]
= [ ]
qsort (x:xs) = qsort ys++[x]++qsort zs
where (ys,zs) = partition (<x) xs
However, this version of qsort is not fast, not smooth, and not compact. But it is
stable. Stability is a result of the fact that partition does not change the order of
the elements in the input. It is not fast because in the worst case qsort requires
Θ(n2) comparisons to sort a list of length n. One worst case arises when the input
is already sorted, so qsort certainly isn’t smooth. The problem lies in the choice
of the partitioning element (or pivot) x, a choice that determines how equal in size
the two sublists produced by partition will be. Choosing the ﬁrst element of the
input as pivot can lead to two very unbalanced subproblems. Better is to choose
a random element of the input; better still is to choose the median element. But
ﬁnding the median takes time. We will consider two median-ﬁnding algorithms
in the following chapter. It is the case that qsort is fast on average, requiring only
Θ(n log n) steps with a small constant of proportionality. Finally, qsort as deﬁned
above can be very inefﬁcient in its use of space, requiring Θ(n2) units in the worst
case. Space efﬁciency can be improved by tweaking the deﬁnition of qsort but we
won’t go into details.
Nevertheless, Quicksort is a decent sorting algorithm when implemented in
an imperative setting. In such a setting, Quicksort can be formulated in terms of
mutable arrays rather than lists, and the partitioning phase can be carried out in
place, meaning that the input array can be used as working space for partitioning.
No other space, apart from the stack needed to implement the recursion, is required.
But the space-efﬁcient version sacriﬁces stability. We will not dwell further on the
merits and demerits of Quicksort, partly because the topic has been addressed in

96
Sorting
[1], and partly because there are better functional algorithms for sorting. Additional
aspects of Quicksort are taken up in the exercises.
5.2 Mergesort
Quicksort is a divide-and-conquer algorithm in which the hard work is done in the
dividing phase – the combining phase is just concatenation. By contrast, the next
sorting algorithm, Mergesort, performs more work in the combining phase and less
in the partition phase.
Again we begin with a tree. This time the tree is a slightly different species:
data Tree a = Null | Leaf a | Node (Tree a) (Tree a)
There are no constraints on the order of the elements in a tree and one can build a
tree for lists of arbitrary type. The ordering on the elements comes into play when
we ﬂatten a tree:
ﬂatten::Ord a ⇒Tree a →[a]
ﬂatten Null
= [ ]
ﬂatten (Leaf x)
= [x]
ﬂatten (Node t1 t2) = merge (ﬂatten t1) (ﬂatten t2)
The function merge merges two sorted lists into one:
merge::Ord a ⇒[a] →[a] →[a]
merge [ ]
ys
= ys
merge xs
[ ]
= xs
merge (x:xs) (y:ys) | x ⩽y
= x:merge xs (y:ys)
| otherwise = y:merge (x:xs) ys
Merging two lists of lengths m and n by merge requires at most m+n comparisons
(see the exercises). The cost of ﬂattening a tree depends on the number of leaves in
each subtree. If a tree of size n has two subtrees each of size n/2, then the number
of comparisons T(n) required satisﬁes
T(n) = 2T(n/2)+n
with solution T(n) = Θ(n log n). It follows that if we can build a tree for a list of
length n in this time, and the tree has the size-balanced property that each node has
two subtrees that differ in size by at most 1, then sorting a list of length n can be
done with Θ(n log n) comparisons.
Here is a divide-and-conquer algorithm to build a size-balanced tree:
mktree::[a] →Tree a
mktree [ ] = Null
mktree [x] = Leaf x
mktree xs = Node (mktree ys) (mktree zs) where (ys,zs) = halve xs

5.2 Mergesort
97
The function halve splits a list into two equal halves:
halve xs = (take m xs,drop m xs) where m = length xs div 2
This deﬁnition involves three traversals of the list, one to compute its length, and
two more to perform the splitting. No human would split up a list this way. Instead,
they would simply deal out the elements alternately into two piles:
halve = foldr op ([ ],[ ]) where op x (ys,zs) = (zs,x:ys)
This version of halve produces a different result, but the two sublists have the same
sizes as before and each is a subsequence of the input. For example,
halve [1..9] = ([2,4,6,8],[1,3,5,7,9])
Since the order of the elements in the tree is immaterial, this deﬁnition of halve is
perfectly adequate.
The running time of mktree satisﬁes essentially the same recurrence as ﬂatten
so it takes Θ(n log n) steps to build a tree. Clearly the tree has the size-balanced
property. Now if we deﬁne
msort = ﬂatten·mktree
then we obtain another famous algorithm called Mergesort. It is easy to eliminate
the tree, and the result is
msort [ ] = [ ]
msort [x] = [x]
msort xs = merge (msort ys) (msort zs)
where (ys,zs) = halve xs
Unlike Quicksort, Mergesort has a Θ(n log n) running time, so it is fast. It is stable
with the ﬁrst deﬁnition of halve, but not with the second deﬁnition. However,
Mergesort is not smooth, taking Θ(n log n) steps even when the input is already
sorted.
Returning to mktree, it is possible to build a size-balanced tree in linear time.
The method was covered in [1] as an example of the tupling technique, so we will
just sketch the idea and state the result. The idea is to avoid repeated halving, by
deﬁning mkpair n xs = (mktree (take n xs),drop n xs). A direct recursive deﬁnition
of mkpair can then be derived, leading to
mktree xs = fst (mkpair (length xs) xs)
mkpair 0 xs = (Null,xs)
mkpair 1 xs = (Leaf (head xs),tail xs)
mkpair n xs = (Node t1 t2,zs)
where (t1,ys) = mkpair m xs
(t2,zs) = mkpair (n−m) ys
m
= n div 2

98
Sorting
The running time T(n) of mkpair n satisﬁes T(n) = 2T(n/2)+Θ(1), with solution
T(n) = Θ(n).
There is another way of building a tree in linear time. Although the result will
not be a size-balanced tree, it will be good enough to ensure that the corresponding
version of Mergesort still has optimal asymptotic time complexity. The idea is to
switch from a divide-and-conquer scheme to a bottom-up scheme. First, the list
of elements is converted into a list of leaves. Provided this list is not empty or a
singleton list, it is then halved in size by combining adjacent pairs of trees into
larger trees. The halving process is repeated until only one tree is left:
mktree [ ] = Null
mktree xs = unwrap (until single (pairWith Node) (map Leaf xs))
pairWith f [ ]
= [ ]
pairWith f [x]
= [x]
pairWith f (x:y:xs) = f x y:pairWith f xs
The functions unwrap and single (and wrap, used below) were deﬁned in Exer-
cise 1.3. The running time T(n) of this version of mktree satisﬁes
T(n) = T(n/2)+Θ(n)
Thus T(n) = Θ(n), the same as before. If we use this bottom-up version of mktree
in the deﬁnition of msort, then we can synthesise another deﬁnition:
msort [ ] = [ ]
msort xs = unwrap (until single (pairWith merge) (map wrap xs))
This synthesis is more interesting than the previous one, and the details are provided
in the exercises. This version of Mergesort, called Bottom-up Mergesort (and some-
times Straight Mergesort), converts the input into a list of singleton lists and then
repeatedly merges those lists in pairs until a single list is left. To time this version of
msort assume that the length n of the input is a power of two. The ﬁrst pass involves
repeatedly merging two singleton lists, taking at most 2 × n/2 = n comparisons.
The second pass involves repeatedly merging two lists of length two, taking at most
4×n/4 = n comparisons, and so on. That gives a total of at most kn comparisons,
where n = 2k. For general n we have that Bottom-up Mergesort takes Θ(n log n)
comparisons.
Looking deeper into Bottom-up Mergesort, we can see that it is not essential to
start with a list of singleton lists. Instead, we could split the input into nondecreasing
runs and begin the merging process with that:
msort [ ] = [ ]
msort xs = unwrap (until single (pairWith merge) (runs xs))
The function runs splits a list into runs of nondecreasing values:

5.2 Mergesort
99
runs::Ord a ⇒[a] →[[a]]
runs = foldr op [ ]
where op x [ ]
= [[x]]
op x ((y:xs):xss) | x ⩽y
= (x:y:xs):xss
| otherwise = [x]:(y:xs):xss
The function runs processes the input from right to left. The next element is added
to the front of the current run if possible; otherwise it begins a new run. This version
of msort, called Smooth Mergesort (and sometimes Natural Mergesort), is smooth;
in particular, if the input is already sorted, it needs only Θ(n) comparisons to return
the input untouched. The Haskell Data.List function sort is similar, except that it
is more cunning and splits the input into both ascending runs and descending runs,
taking care to reverse the descending runs. Here is the deﬁnition:
runs [x]
= [[x]]
runs (x:y:xs)
| x ⩽y
= upruns y (x:) xs
| otherwise = dnruns y [x] xs
upruns x f [ ] = [f [x]]
upruns x f (y:ys)
| x ⩽y
= upruns y (f ·(x:)) ys
| otherwise = f [x]:runs (y:ys)
dnruns x xs [ ] = [x:xs]
dnruns x xs (y:ys)
| x>y
= dnruns y (x:xs) ys
| otherwise = (x:xs):runs (y:ys)
This time, runs processes the input from left to right. The second argument of both
upruns and dnruns is an accumulating parameter, a list in the case of dnruns and a
function in the case of upruns. The ﬁrst argument of dnruns is the minimum value
encountered so far, while the ﬁrst argument of upruns is the maximum. In the case
of dnruns, if the next value in the input is strictly smaller than the minimum, then
the minimum is added to the front of the run, and the new value becomes the current
minimum. Thus, the runs produced by dnruns are in strictly increasing order. In
the case of upruns, if the next value is no smaller than the maximum, then the
maximum is added, in effect, to the back of the run, and the next value becomes the
new maximum. Thus, the runs produced by upruns are in weakly increasing order.
The asymmetry between dnruns and upruns, the ﬁrst producing strictly increasing
runs and the second only weakly increasing runs, has as a consequence that the
resulting sorting algorithm is stable. We omit a formal proof, but simply give an
example. Using a, b, and c to indicate relative order in the original list, we have

100
Sorting
runs [6,4,3a,2a,2b,1a,1b,2c,1c,3b]
= [[2a,3a,4,6],[1a,2b],[1b,2c],[1c,3b]]
Merging these lists in pairs, and then merging the results, produces the sorted list
which is the required order for a stable sort.
Four versions of Mergesort have been covered in this section, all of which have
relatively short deﬁnitions. The ﬁnal one is probably the best, being fast, smooth,
and stable. As we said above, it is this deﬁnition of Mergesort that is provided
in the Haskell library Data.List. Well, not quite. The Haskell deﬁnition is more
general in that the comparison test is provided as an extra argument. To motivate the
deﬁnition, consider how you would sort a list into descending rather than ascending
order. All the algorithms above have tacitly assumed that sorting means sorting
into ascending order. One can of course deﬁne sortDown = reverse·sort, but this
deﬁnition involves another traversal and that adds overhead. There are other kinds
of comparison between elements that one can think of. For instance, we might
conceivably want to sort a list of numbers so that all the even ones come ﬁrst. To
achieve this generality, Data.List provides two more functions
sortBy ::(a →a →Ordering) →[a] →[a]
sortOn::Ord b ⇒(a →b) →[a] →[a]
The type Ordering is deﬁned in the Standard Prelude:
data Ordering = LT | EQ | GT
The function compare is a method in the type class Ord and has the type
compare::Ord a ⇒a →a →Ordering
For example, compare 3 4 = LT. As two examples,
sortBy compare [3,1,4]
= [1,3,4]
sortBy (ﬂip compare) [3,1,4] = [4,3,1]
The function sortBy cmp sorts according to the weird even–odd ordering described
above, where cmp is deﬁned by
cmp x y = compare (odd x,x) (odd y,y)
This deﬁnition exploits the fact that False<True in Haskell. For example
sortBy cmp [1..10] = [2,4,6,8,10,1,3,5,7,9]
The variant sortOn sorts according to the values of a given function. For example,
sortOn fst sorts a list of pairs in order of ﬁrst components. Exercise 5.12 asks you to
deﬁne this variant. We will need sortBy and sortOn later on in the book, but for the
while we continue with the assumption that sorting means sorting into ascending
order.

5.3 Heapsort
101
5.3 Heapsort
Our next sorting algorithm, Heapsort, involves a different kind of tree from that in
the previous section, one that is identical to the type of binary search trees except
that node labels are placed before the two subtrees:
data Tree a = Null | Node a (Tree a) (Tree a)
We can ﬂatten such a tree by
ﬂatten::Ord a ⇒Tree a →[a]
ﬂatten Null
= [ ]
ﬂatten (Node x u v) = x:merge (ﬂatten u) (ﬂatten v)
By deﬁnition a tree is a heap if ﬂattening it produces a list in nondecreasing order.
Thus a heap is a tree in which the value at a node is no larger than the values in
either of its subtrees. If the tree is size-balanced, then ﬂattening it takes Θ (n log n)
steps for a tree of size n.
It is easy enough to build a size-balanced heap in linearithmic time (i.e. O(n log n)
steps for a list of length n) – see Exercise 5.13. However, as we will now show, it is
possible to build such a heap in linear time. The idea is to compute mkheap as the
composition of two other functions: mktree, which builds a size-balanced tree, and
heapify, which reorganises the labels to ensure the heap condition. We will leave
the linear-time implementation of mktree as another exercise and concentrate on
heapify. The deﬁnition starts out easily enough:
heapify::Ord a ⇒Tree a →Tree a
heapify Null
= Null
heapify (Node x u v) = siftDown x (heapify u) (heapify v)
It remains to deﬁne siftDown. This function is another smart constructor, taking a
value and two heaps and building a heap by sifting the value downwards until the
heap property is restored:
siftDown::Ord a ⇒a →Tree a →Tree a →Tree a
siftDown x Null Null = Node x Null Null
siftDown x (Node y u v) Null
| x ⩽y
= Node x (Node y u v) Null
| otherwise = Node y (siftDown x u v) Null
siftDown x Null (Node y u v)
| x ⩽y
= Node x Null (Node y u v)
| otherwise = Node y Null (siftDown x u v)
siftDown x (Node y ul ur) (Node z vl vr)
| x ⩽min y z = Node x (Node y ul ur) (Node z vl vr)
| y ⩽min x z = Node y (siftDown x ul ur) (Node z vl vr)
| z ⩽min x y = Node z (Node y ul ur) (siftDown x vl vr)

102
Sorting
Note that heapify does not change the structure of a heap, so heapify returns a
size-balanced tree if given one. To show that heapify takes Θ(n) steps when applied
to a size-balanced tree t of size n, observe that siftDown is applied to every subtree
of t, including t itself, and in the worst case can take time proportional to the height
of the subtree. Suppose t has height h. Then t has one subtree of height h, at most
two subtrees of height h−1, at most four subtrees of height h−2, and so on. Hence
the total running time of heapify is proportional to at most
h+2(h−1)+4(h−2)+···+2h−1 =
h
∑
k=1
2h−k k = 2h
h
∑
k=1
k/2k <2h+1
Finally, a size-balanced tree of size n has height Θ(log n) (in fact it has the minimum
possible height ⌈log(n+1)⌉– see the exercises) and so T(n) = Θ(n).
Here, ﬁnally, is the deﬁnition of Heapsort:
hsort ::Ord a ⇒[a] →[a]
hsort = ﬂatten·heapify·mktree
A tree is built in Θ(n) steps, heapiﬁed in Θ(n) steps, and ﬂattened in Θ(n log n)
steps. Thus hsort takes Θ(n log n) steps. Unlike Quicksort or Mergesort, the tree
cannot be eliminated by fusing the component functions, so Heapsort necessarily
involves building a tree. In an imperative setting, when the input is given in an array,
the tree can be stored in the array by juggling with array indices, so Heapsort can be
made in-place. Heapsort is fast, but not smooth, stable, or compact. On random data
it turns out to be somewhat slower than the best version of Mergesort, so we still
have a champion. On the other hand, heaps are useful for other purposes, including
the implementation of priority queues, a topic we will take up in Chapter 8.
5.4 Bucketsort and Radixsort
We turn now to two completely different kinds of sorting algorithm. Neither is based
on comparisons between elements of some arbitrary type, so sorting no longer has
the type sort::Ord a ⇒[a] →[a]. Instead these two algorithms exploit the structure
of the elements to be sorted. To set the scene, consider sorting a list of words, where
a word is a list of alphabetic characters. That means sorting the words into lexical
order. We can use Mergesort for this purpose:
type Word = [Char]
sortWords::[Word] →[Word]
sortWords = msort
As we have seen, sorting a list of n words this way involves O(n log n) comparisons
between words. However, comparing two words does not take constant time. If there

5.4 Bucketsort and Radixsort
103
are at most k letters in each word, then comparing them will take Θ(k) character
comparisons in the worst case. That means the true cost of computing sortWords
is O(kn log n) steps. The two algorithms in this section reduce the cost to O(kn)
steps.
The way we will view the problem is to think of a word as containing ﬁelds of
information, the ﬁelds being the ﬁrst character of the word, the second character,
and so on. Similarly, an integer can be deﬁned in terms of the ﬁelds of its decimal
digits. These ﬁelds can be extracted by providing a list of total functions, which
we will call discriminators, each of which extracts one possible ﬁeld. For instance
with decimals of length k, there will be k discriminators, one for each decimal digit.
Given a list of discriminators, two elements x and y are lexically ordered if the
following test returns True:
ordered ::Ord b ⇒[a →b] →a →a →Bool
ordered [ ] x y
= True
ordered (d :ds) x y = (d x<d y) ∨((d x == d y) ∧ordered ds x y)
In this formulation of the problem, the ﬁelds themselves have to be elements from
some ordered type.
The obvious way to sort a list of words is to divide them into piles, or buck-
ets, according to their ﬁrst letter. Each bucket is then sorted in the same way,
but on the second letter, and so on. At the end of this process there will be lots
of little buckets, each containing a single word. These buckets then have to be
combined in the right order to give the ﬁnal sorted list. The simplest way to im-
plement this idea is in terms of a tree, and the kind of tree we will need is as
follows:
data Tree a = Leaf a | Node [Tree a]
This kind of tree is sometimes called a rose tree. A rose tree therefore is either a
leaf containing a value, or a node that can have an arbitrary list of subtrees. We can
build a rose tree by
mktree::(Bounded b,Enum b,Ord b) ⇒[a →b] →[a] →Tree [a]
mktree [ ] xs
= Leaf xs
mktree (d :ds) xs = Node (map (mktree ds) (ptn d xs))
A rose tree is built by partitioning the list into buckets according to the ﬁrst ﬁeld.
Each bucket is then converted into a tree by applying the algorithm recursively
to the remaining ﬁelds. Later on we will modify this deﬁnition in the interests of
efﬁciency. The reason for the type-class constraints on mktree will become apparent
in a moment.
The function ptn partitions a list into buckets according to the ﬁeld extracted by
the discriminator:

104
Sorting
ptn::(Bounded b,Enum b,Ord b) ⇒(a →b) →[a] →[[a]]
ptn d xs = [ﬁlter (λx.d x == m) xs | m ←rng]
where rng = [minBound..maxBound]
The deﬁnition of rng explains why the result type of a discriminator has to be both
bounded and enumerable. The deﬁnition of ptn is very inefﬁcient, and for two quite
separate reasons. Firstly, rng may be a very long list. For example, since Haskell’s
Char type represents all Unicode characters, the length of rng :: [Char] is over a
million. Secondly, the result of ptn is computed by repeatedly traversing the input.
If rng has length r and the input has length n, then ptn requires r ×n evaluations of
the discriminator. We will address this problem of efﬁciency later on.
Having built a tree, we can now ﬂatten it:
ﬂatten::Tree [a] →[a]
ﬂatten (Leaf xs) = xs
ﬂatten (Node ts) = concatMap ﬂatten ts
The resulting sorting algorithm is known as Bucketsort:
bsort ds xs = ﬂatten (mktree ds xs)
Well, not quite. In Bucketsort as traditionally presented there are no trees. We should
be familiar with this situation by now, and the next step is to eliminate the interme-
diate tree. The base case bsort [ ] xs = xs is easy. For the induction step we reason
bsort (d :ds) xs
=
{ deﬁnition of bsort }
ﬂatten (mktree (d :ds) xs)
=
{ deﬁnition of mktree }
ﬂatten (Node (map (mktree ds) (ptn d xs)))
=
{ deﬁnition of ﬂatten }
concatMap (ﬂatten·mktree ds) (ptn d xs)
=
{ deﬁnition of bsort }
concatMap (bsort ds) (ptn d xs)
Hence we have shown
bsort [ ] xs
= xs
bsort (d :ds) xs = concatMap (bsort ds) (ptn d xs)
So far, so good. But we can push the calculation one more step if we exploit a
simple but important fact, namely that
map (bsort ds) (ptn d xs) = ptn d (bsort ds xs)
Informally, this identity asserts that ptn is stable: partitioning a sorted sequence
yields a collection of sorted buckets. A detailed proof is given in the exercises.
Assuming it holds, we can continue:

5.4 Bucketsort and Radixsort
105
concatMap (bsort ds) (ptn d xs)
=
{ above stability property }
concat (ptn d (bsort ds xs))
And that gives us Radixsort:
rsort ::(Bounded b,Enum b,Ord b) ⇒[a →b] →[a] →[a]
rsort [ ] xs
= xs
rsort (d :ds) xs = concat (ptn d (rsort ds xs))
Whereas bsort sorts on the most signiﬁcant ﬁeld ﬁrst, rsort sorts on the most
signiﬁcant ﬁeld last. The difference is that in bsort the buckets have to be kept
separate; sorting each bucket means dividing each bucket into further buckets, and
so on. At the end there will be a long array of singleton buckets, which are only
then reassembled into the ﬁnal list. In rsort the buckets can be combined after each
pass. Indeed, Radixsort was used in the early days of sorting punched cards with
a mechanical sorter. The sorter was used to divide the cards into buckets on the
least signiﬁcant column of a card. The buckets were then carefully reassembled by
a human sorter into a single deck of cards without changing the order of any of the
cards in a single bucket or the order of the buckets themselves. The entire deck was
then replaced in the sorter and sorted again on the next least signiﬁcant digit, and so
on. A much simpler process for a human to carry out.
Let us now return to the problem of computing ptn efﬁciently. To avoid a po-
tentially huge range of values, most of which will probably not occur in a given
ﬁeld, it is best to make the range (l,u) of values in a ﬁeld explicit. We will assume
for simplicity that all ﬁelds have the same range. Second, we suppose that ﬁeld
elements are of a type that can be array indices, which is to say they are elements of
the type class Ix. Then we can avoid multiple traversals of the input by accumulating
the elements into an array:
ptn::Ix b ⇒(b,b) →(a →b) →[a] →[[a]]
ptn (l,u) d xs = elems xa
where xa = accumArray snoc [ ] (l,u) (zip (map d xs) xs)
snoc xs x = xs++[x]
It is important for the stability property above that ptn should ensure that the order
in each bucket is the same as in the input, which explains why array entries are
computed by adding a new value to the end of a list via snoc. But snoc is not a
constant-time operation, so building the array can take Θ(n2) steps in the worst case,
when all the elements go into the same slot. One solution is to use symmetric lists;
another is to insert elements in reverse order, and then to reverse each list when
extracting the array elements:

106
Sorting
ptn (l,u) d xs = map reverse (elems xa)
where xa = accumArray (ﬂip (:)) [ ] (l,u) (zip (map d xs) xs)
This version of ptn takes Θ(n) steps for an input of length n. Here is the revised
deﬁnition of rsort that uses the new ptn:
rsort ::Ix b ⇒(b,b) →[a →b] →[a] →[a]
rsort bb [ ] xs
= xs
rsort bb (d :ds) xs = concat (ptn bb d (rsort bb ds xs))
If there are k discriminators, then rsort takes Θ(kn) steps because there are k calls
of ptn, each of which takes Θ(n) steps, and one application of concat, which also
takes linear time.
Finally, let us specialise rsort to the case where the input is a nonempty list of
natural numbers. Each number is converted into a list of digits by applying show,
and then padded with leading zeros to ensure that each decimal has the same length.
We can deﬁne the discriminator functions by
discs::[Nat] →[Nat →Char]
discs xs = [λx.pad k (show x)!!i | i ←[0..k −1]]
where k = maximum (map (length·show) xs)
pad k xs = replicate (k −length xs) '0'++xs
And now we have
irsort ::[Nat] →[Nat]
irsort xs = rsort ('0','9') (discs xs) xs
How does irsort compare with the current champion, Smooth Mergesort? As one
experiment, we sorted a list of a million randomly generated integers, all in the
range (0,10000) – so there are ﬁve discriminators. Radixsort took 65% of the time
of Smooth Mergesort. In fact, Smooth Mergesort only begins to pull ahead when
there are eight or more discriminators.
5.5 Sorting sums
We end the chapter by taking a look at a famous unsolved problem connected with
sorting. Suppose A is some ordered type and + is some monotonic binary operation
on A, so
x ⩽x′ ∧y ⩽y′ ⇒x+y ⩽x′ +y′
In concrete code we take A to be a synonym for Integer and + to be numerical
addition. Consider the problem of computing sortsums, where
sortsums::[A] →[A] →[A]
sortsums xs ys = sort [x+y | x ←xs,y ←ys]

5.5 Sorting sums
107
Supposing both lists have length n, what is asymptotically the best possible running
time of any algorithm for computing sortsums? There are n2 sums, so sorting
involves Ω(n2 log n) comparisons on elements of A in the worst case. The bound
Ω(n2 log n) does not depend on + being monotonic but, even if it is, the bound is
the same. We will prove this fact below.
But now suppose we assume more about + and A, speciﬁcally that (A,+) is an
Abelian group. Thus + is associative and commutative, with an identity element we
will write as 0, and an operation negate such that x+negate x = 0. For example, the
integers form an Abelian group under addition. What is the best bound in this case?
The answer is that nobody knows. It cannot be better than O(n2) because it takes n2
steps to produce the answer, but there is still a gap between O(n2) and O(n2 log n).
What if additional properties of A were assumed? After all, integers have more
structure than just being an Abelian group under addition. Integers can be multiplied
as well as added – they form an algebraic ring. Does that help? Again, nobody
knows. It remains an open problem, some 40 years after it was ﬁrst posed in [6], as
to whether the total cost of computing sortsums can be reduced to O(n2) steps.
However, some progress has been made. In particular, Jean-Luc Lambert [9]
proved that, if (A,+) is an Abelian group, then sortsums can be computed with
O(n2) comparisons between elements of A. His algorithm is another nifty example
of divide and conquer, and we describe it below. However, Lambert’s algorithm
does require Cn2 log n additional operations, including other ‘housekeeping’ com-
parisons; moreover C is quite large. Thus the total running time does not beat the
O(n2 log n) bound.
Here is a proof that Ω(n2 log n) is a lower bound on sortsums when the only
assumption made is that + is monotonic. Suppose xs and ys are both sorted into
increasing order and consider the n×n matrix [[x+y | y ←ys] | x ←xs]. Each row
and column of the matrix is therefore in increasing order. The grid is of the same
kind that we saw in two-dimensional search in the previous chapter. The matrix
is an example of what is known as a standard Young tableau, and it follows from
Theorem H of Section 5.1.4 of [8] that there are precisely
E(n) = (n2)!
(2n−1)!
(n−1)!
(2n−2)!
(n−2)! ··· n!
0!

ways of assigning the values 1 to n2 to the elements of the matrix. Each such
assignment determines a potential permutation that can sort the input, so in the
associated decision tree there have to be at least E(n) leaves. Using the fact that
log E(n) = Ω (n2 log n), we conclude that at least this number of comparisons
between elements of A is required.
Now for the meat of the exercise. Lambert’s algorithm depends on two simple

108
Sorting
facts. Deﬁne the subtraction operation (−) by x−y = x+negate y. Then we have
x+y = x−negate y
(5.1)
and
x−y ⩽x′ −y′ ⇔x−x′ ⩽y−y′
(5.2)
Veriﬁcation of (5.1) is easy, but (5.2), which we leave as an exercise, requires all
the properties of an Abelian group. Here in outline is how (5.1) and (5.2) are used.
First, we use fact (5.1) to sort subtractions rather than sums:
sortsums xs ys = sortsubs xs (map negate ys)
sortsubs xs ys = sort [x−y | x ←xs,y ←ys]
Second, we use fact (5.2) to compute sortsubs xs ys by computing instead the
two lists xxs = sortsubs xs xs and yys = sortsubs ys ys. By using a divide-and-
conquer scheme, Lambert showed how the two lists xxs and yys can be com-
puted with only O(n2) comparisons between elements of A. The two lists can
be merged to give sortsubs xs ys – but crucially, without any further compar-
isons on elements of A. Since x −y ⩽x′ −y′ precisely in the case that x −x′
precedes y −y′ in the merged list, the merged list can be computed using prece-
dence comparisons only, comparisons between suitable integer labels, not elements
of A.
Let us deal ﬁrst with the merging step. We label values of type A with natural
numbers and change the deﬁnition of sortsubs to read
sortsubs xs ys = map fst (sortWith abs xis yis)
where xis = zip xs [0..n−1]
yis = zip ys [n..]
n
= length xs
abs = merge (sortsubs1 xis) (sortsubs1 yis)
The elements of the two lists xs and ys are given distinct labels, from 0 upwards.
With Label a type synonym for natural numbers, and Pair a synonym for pairs of
labels, the two component sorting functions in the new deﬁnition of sortsubs have
types
sortsubs1 ::[(A,Label)] →[(A,Pair)]
sortWith::[(A,Pair)] →[(A,Label)] →[(A,Label)] →[(A,Pair)]
The ﬁrst function, sortsubs1, is deﬁned in the ﬁrst instance by
sortsubs1 xis = sort (subs xis xis)
subs xis yis = [(x−y,(i,j)) | (x,i) ←xis,(y,j) ←yis]
Thus sortsubs1 sorts subtractions over a single labelled list, retaining label infor-
mation to show the origin of the subtraction. For example, the element (6,(11,3))
records that 6 is the result of subtracting the element in position 3 in xis from the

5.5 Sorting sums
109
element in position 11. As deﬁned, sortsubs1 takes O(n2 log n) steps. Below we
show how this can be reduced to O(n2) steps.
We deal with sortWith next. The deﬁnition, which uses both the Haskell function
sortBy and an array, is as follows:
sortWith abs xis yis = sortBy cmp (subs xis yis)
where cmp ( ,(i,k)) ( ,(j,l)) = compare (a!(i,j)) (a!(k,l))
a = array bs (zip labelPairs [0..])
labelPairs = map snd abs
bs = (minimum labelPairs,maximum labelPairs)
Now consider
abs = merge (sortsubs1 xis) (sortsubs1 yis)
Here map fst abs is a list of A-values in nondecreasing order, and map snd abs is
a list of corresponding pairs of labels. This second list is just what we need to
determine the result of a comparison test, for xi −yk ⩽xj −yl if and only if (i,j)
precedes (k,l) in the list labelPairs. We compute precedence information quickly
by creating an array indexed by pairs of labels whose entries are positive integers.
Although sortWith depends on comparisons, the comparisons are between pairs of
labels, not elements of A.
It remains to compute a better version of sortsubs1, and it is here that divide and
conquer enters the picture. We make use of the identity
sortsubs1 (xis++yis) (xis++yis) =
merge (merge (sortsubs1 xis xis) (sortsubs1 xis yis))
(merge (sortsubs1 yis xis) (sortsubs1 yis yis))
Two of the subterms, sortsubs1 xis xis and sortsubs1 yis yis, are computed recur-
sively, and the results are merged to give abs. Next, sortsubs1 xis yis is computed
via
sortsubs1 xis yis = sortWith abs (subs xis yis)
Finally, sortsubs1 yis xis can be computed quickly from sortsubs xis yis by negating
values, swapping labels, and reversing the list. The complete program is summarised
in Figure 5.1. Counting comparisons between elements of A only, the number
C(n) of comparisons to compute sortsubs1 on a list of length n satisﬁes C(n) =
2C(n/2)+Θ(n2) with solution C(n) = Θ(n2). However, the total time T(n) taken
to carry out the complete sorting is given by T(n) = 2T(n/2)+Θ(n2 log n), with
solution T(n) = Θ(n2 log n). The logarithmic factor can be removed from T(n) if
sortBy cmp in the deﬁnition of sortWith can be computed in quadratic time, but this
result remains elusive. In any case, the additional complexity arising from replacing
comparisons between elements of A by comparisons between integers makes the
algorithm very inefﬁcient in practice.

110
Sorting
sortsums xs ys = sortsubs xs (map negate ys)
sortsubs xs ys = map fst (sortWith abs xis yis)
where xis = zip xs [0..n−1]
yis = zip ys [n..]
n
= length xs
abs = merge (sortsubs1 xis) (sortsubs1 yis)
sortWith abs xis yis = sortBy cmp (subs xis yis)
where cmp ( ,(i,k)) ( ,(j,l)) = compare (a!(i,j)) (a!(k,l))
a = array bs (zip labelPairs [0..])
labelPairs = map snd abs
bs = (minimum labelPairs,maximum labelPairs)
subs xis yis = [(x−y,(i,j)) | (x,i) ←xis,(y,j) ←yis]
sortsubs1 [ ]
= [ ]
sortsubs1 [(x,i)] = [(0,(i,i))]
sortsubs1 xis
= merge abs (merge cs ds)
where abs = merge (sortsubs1 xis1) (sortsubs1 xis2)
cs = sortWith abs xis1 xis2
ds = reverse (map switch cs)
(xis1,xis2) = splitAt (length xis div 2) xis
switch (x,(i,j)) = (negate x,(j,i))
Figure 5.1 The complete program
5.6 Chapter notes
The ultimate source of information about sorting is Knuth’s comprehensive treatment
in [8]. Quicksort was invented by Tony Hoare, see [7]. Mergesort is even older
and goes back to John von Neumann, who ﬁrst suggested the method in 1945.
The Haskell implementation of Mergesort is attributed to Lennart Augustsson and
Thomas Nordin. Heapsort was discovered by J. W. J. Williams [10], although an
earlier version, simply called Treesort, was due to Robert W. Floyd. The derivation
of Radixsort is due to Jeremy Gibbons [5].
The problem of sorting sums is Problem 41 in the Open Problems Project [2],
a web resource devoted to recording open problems of interest to researchers
in computational geometry and related ﬁelds. The earliest known reference to
the problem is by Michael Fredman [4], who attributed the problem to Elwyn
Berlekamp. All these references consider the problem in terms of numbers rather
than Abelian groups, but the idea is the same.
References
[1]
Richard Bird. Thinking Functionally with Haskell. Cambridge University Press,
Cambridge, 2014.

Exercises
111
[2]
Erik D. Demaine, Joseph S. B. Mitchell, and Joseph O’Rourke. The open problems
project. http://cs.smith.edu/~jorourke/TOPP/, 2009.
[4]
Michael L. Fredman. How good is the information theory lower bound in sorting?
Theoretical Computer Science, 1(4):355–361, 1976.
[5]
Jeremy Gibbons. A pointless derivation of radix sort. Journal of Functional
Programming, 9(3):339–346, 1999.
[6]
Lawrence H. Harper, Thomas H. Payne, John E. Savage, and Ernst G. Straus. Sorting
x+y. Communications of the ACM, 18(6):347–349, 1975.
[7]
Charles A. R. Hoare. Quicksort. Computer Journal, 5(1):10–16, 1962.
[8]
Donald E. Knuth. The Art of Computer Programming, volume 3: Sorting and
Searching. Addison-Wesley, Reading, MA, second edition, 1998.
[9]
Jean-Luc Lambert. Sorting the sums (xi +yj) in O(n2) comparisons. Theoretical
Computer Science, 103(1):137–141, 1992.
[10] John W. J. Williams. Algorithm 232 – Heapsort. Communications of the ACM,
7(6):347–348, 1964.
Exercises
Exercise 5.1 Four species of tree have been described in this chapter. Name the
sorting algorithms associated with the following ﬁve tree structures, identifying the
odd one out:
data Tree a = Null | Leaf a | Node (Tree a) (Tree a)
data Tree a = Null | Node a (Tree a) (Tree a)
data Tree a = Null | Node a [Tree a]
data Tree a = Null | Node (Tree a) a (Tree a)
data Tree a = Leaf a | Node [Tree a]
Exercise 5.2 Starting with qsort = ﬂatten·mktree, where ﬂatten is as deﬁned in the
text, construct the deﬁnition of qsort (x:xs).
Exercise 5.3 Recall from the previous chapter the following deﬁnition of ﬂatten in
which the concatenation operations have been eliminated:
ﬂatten t = ﬂatcat t [ ]
ﬂatcat Null xs
= xs
ﬂatcat (Node l x r) xs = ﬂatcat l (x:ﬂatcat r xs)
Starting with qsort = ﬂatten · mktree, synthesise a version of Quicksort for this
version of ﬂatten.
Exercise 5.4 Would it be sensible to choose as pivot the middle element of the list,
that is, the one whose position is the middle position? How about the mean value,
or the median?
Exercise 5.5 Deﬁne minimum = head·qsort. How long does it take to compute the
minimum of a nonempty list this way?

112
Sorting
More generally, deﬁne select k xs = (qsort xs) !! k. Synthesise a more efﬁcient
deﬁnition. (This exercise will be answered in the following chapter.)
Exercise 5.6 Synthesise a space-efﬁcient version of qsort by introducing two accu-
mulating parameters:
qsort (x:xs) = help x xs [ ] [ ]
where help x xs us vs = qsort (us++ys)++[x]++qsort (vs++zs)
where (ys,zs) = partition (<x) xs
Your task is simply to obtain a recursive deﬁnition of help.
Exercise 5.7 The number of comparisons T(m,n) required by merge to merge two
lists of lengths m and n in the worst case satisﬁes
T(0,n) = 0
T(m,0) = 0
T(m,n) = 1+T(m−1,n) max T(m,n−1)
Prove that T(m,n) ⩽m+n.
Exercise 5.8 In the recursive deﬁnition of msort, are the two base cases both
necessary?
Exercise 5.9 Synthesise the bottom-up version of Mergesort. You will need the
following fusion law of until:
f ·until p g = until q h·f
provided that f ·g = h·f and p x ⇔q (f x) for all x.
Exercise 5.10 What common functions do the following two expressions represent?
ﬂip (foldl (λf x.(x:)·f) id) [ ]
ﬂip (foldr (λx f.f ·(x:)) id) [ ]
Exercise 5.11 A playing card can be represented by two characters, the ﬁrst being
one of the letters of SHDC (Spades, Hearts, Diamonds, Clubs) and the second one of
the letters of AKQJT98765432 (Ace, King, Queen, Jack, Ten, etc.). Bridge players
sort their 13 cards from left to right in the order given by these lists. For example,
[SA,SQ,S9,S8,S2,HK,H5,H3,H2,CA,CT,C7,C2]
Bridge players would describe this hand as “Five spades to the Ace-Queen, four
hearts to the King, void diamonds and four clubs to the Ace-Ten”. Be that as it may,
use sortBy to sort a Bridge hand into order.
Exercise 5.12 Using the Haskell Data.Ord function
comparing::Ord b ⇒(a →b) →a →a →Ordering
comparing f x y = compare (f x) (f y)

Exercises
113
give a simple deﬁnition of sortOn. Explain why this deﬁnition can be inefﬁcient,
and use tupling to give a better version.
Exercise 5.13 Give a divide-and-conquer algorithm for building a size-balanced
heap in linearithmic time.
Exercise 5.14 Given that we can build a heap in linearithmic time, it seems that
another way to deﬁne Heapsort is simply to deﬁne
hsort = ﬂatten·mkheap
In other words, build a heap and then ﬂatten it. Show the result of eliminating the
intermediate heap from this deﬁnition. Why did we not include this version of hsort
in the text?
Exercise 5.15 Show how to build a size-balanced tree, of the kind described in
Heapsort, in linear time. Start with
mktree [ ]
= Null
mktree (x:xs) = Node x (mktree (take m xs)) (mktree (drop m xs))
where m = length xs div 2
Exercise 5.16 Prove that a size-balanced tree, of the kind described in Heapsort,
has minimum possible height ⌈log(n+1)⌉, where n is the size of the tree.
Exercise 5.17 Suppose you are given a list of n integers, all of which lie in some
given range (0,m). Show how to sort the list in Θ(m+n) steps.
Exercise 5.18 Here is the stability property of Bucketsort again:
map (bsort ds) (ptn d xs) = ptn d (bsort ds xs)
(5.3)
The aim of this exercise is to prove (5.3) using the original deﬁnition of bsort as a
function that ﬂattens a tree. First, deﬁne the function
tmap::(a →b) →Tree a →Tree b
for mapping a function over a tree. This function will be needed below.
Next, use the following subsidiary claims to prove (5.3):
mktree ds·ﬁlter p
= tmap (ﬁlter p)·mktree ds
(5.4)
ﬂatten·tmap (ﬁlter p) = ﬁlter p·ﬂatten
(5.5)
Next, use the following claim to prove (5.4): provided p is a total predicate, we have
ptn d ·ﬁlter p = map (ﬁlter p)·ptn d
(5.6)
Now prove (5.6). You can assume that ﬁlters of total predicates commute, so
ﬁlter p·ﬁlter q = ﬁlter q·ﬁlter p
Finally, prove (5.5). That’s a lot of work for one exercise, but it does demonstrate
that the pages of explanation found in some textbooks as to why Radixsort works
can be reduced to simple calculation.

114
Sorting
Exercise 5.19 Specialise Radixsort to the problem of sorting a list of words, where
a word is made up of lower-case letters only.
Exercise 5.20 Prove that, if (A,+) is an Abelian group, then
x−y ⩽x′ −y′ ⇔x−x′ ⩽y−y′
Answers
Answer 5.1 Mergesort, Heapsort, odd one out, QuickSort, and Bucketsort.
Answer 5.2 We have
qsort (x:xs)
=
{ deﬁnition }
ﬂatten (mktree (x:xs))
=
{ deﬁnition of mktree with (ys,zs) = partition (<x) xs }
ﬂatten (Node (mktree ys) x (mktree zs))
=
{ deﬁnition of ﬂatten }
ﬂatten (mktree ys)++[x]++ﬂatten (mktree zs)
=
{ deﬁnition of qsort }
qsort ys++[x]++qsort zs
Answer 5.3 Let qcat xs ys = ﬂatcat (mktree xs) ys. Then we obtain
qsort xs = qcat xs [ ]
qcat [ ] ys
= ys
qcat (x:xs) ys = qcat us (x:qcat vs ys)
where (us,vs) = partition (<x) xs
Answer 5.4 No, choosing the middle element is as likely to produce two unbal-
anced lists as choosing the ﬁrst element. The only advantage of such a choice is
that if the input is already sorted into strictly increasing order, then qsort will take
Θ(n log n) steps rather than Θ(n2) steps. Choosing the mean value as pivot is better,
but of course the notion of a mean value depends on the input being a list of numbers,
so this method is not available for an arbitrary ordered type. Choosing the median
value as pivot would guarantee the list is evenly split, but of course such a choice
depends on there being an efﬁcient method for computing the median, a problem
that is addressed in the following chapter.
Answer 5.5 It could take quadratic time, for example when the input is in decreas-
ing order.

Answers
115
Answer 5.6 The base case
help x [ ] us vs = qsort us++[x]++qsort vs
is easy. For the recursive case assume y<x. We argue as follows:
help x (y:xs) us vs
=
{ with (ys,zs) as above }
qsort (us++y:ys)++[x]++qsort (vs++zs)
=
{ claim (see below) }
qsort (y:us++ys)++[x]++qsort (vs++zs)
=
{ deﬁnition of help }
help x (y:us) vs
The claim is that qsort is unchanged when the input is permuted. A dual calculation
in the remaining case yields
qsort [ ]
= [ ]
qsort (x:xs) = help xs [ ] [ ]
where help [ ] us vs
= qsort us++[x]++qsort vs
help (y:xs) us vs | x ⩽y
= help xs us (y:vs)
| otherwise = help xs (y:us) vs
Answer 5.7 The base cases T(0,n) = 0 ⩽n and T(m,0) = 0 ⩽m are immediate
and the induction step is
1+T(m−1,n) max T(m,n−1) ⩽1+(m−1+n) max (m+n−1) = m+n
In fact one can prove rather more: for m>0 and n>0 we have T(m,n) = m+n−1.
Answer 5.8 Oh yes. Since halve [x] = ([ ],[x]) the recursion would not terminate
if either clause were missing. It is an easy mistake to make. The ﬁrst two clauses of
sortsubs1 in Figure 5.1 are necessary for the same reason.
Answer 5.9 Arguing at the function level for a nonempty list, we have
ﬂatten·unwrap·until single (pairWith Node)·map Leaf
=
{ since ﬂatten·unwrap = unwrap·map ﬂatten }
unwrap·map ﬂatten·until single (pairWith Node)·map Leaf
=
{ fusion law of until (see below) }
unwrap·until single (pairWith merge)·map (ﬂatten·Leaf)
=
{ since ﬂatten·Leaf = wrap }
unwrap·until single (pairWith merge)·map wrap
The two fusion conditions are
single·map ﬂatten = single
map ﬂatten·pairWith Node = pairWith merge·map ﬂatten
Veriﬁcation of the conditions is omitted.

116
Sorting
Answer 5.10 In both cases the function is reverse. Of course, a shorter deﬁnition
is foldl (ﬂip (:)) [ ].
Answer 5.11 We have to deﬁne a comparison function cmp for playing cards. With
suit as a synonym for head, and rank a synonym for head · tail, and noting that
“SHDC” is in reverse alphabetical order, we can deﬁne
cmp c1 c2 = if suit c1 == suit c2
then compare (posn (rank c1)) (posn (rank c2))
else compare (suit c2) (suit c1)
posn r = head [i | (c,i) ←zip ranks [0..],c == r]
ranks = "AKQJT98765432"
Answer 5.12 We can deﬁne
sortOn f = sortBy (comparing f)
However, the value of f on one and the same argument may be computed many
times under this deﬁnition. A better method is to deﬁne
sortOn f xs = map snd (sortBy (comparing fst) (zip (map f xs) xs))
Even this deﬁnition is not as good as it might be because the list xs is traversed
twice in the ﬁnal term. A better deﬁnition still is
sortOn f = map snd ·sortBy (comparing fst)·map (λx.(f x,x))
This is essentially the deﬁnition given in Data.List.
Answer 5.13 The function mkheap is deﬁned by
mkheap::Ord a ⇒[a] →Tree a
mkheap [ ]
= Null
mkheap (x:xs) = Node y (mkheap ys) (mkheap zs)
where (y,ys,zs) = split (x:xs)
split ::Ord a ⇒[a] →(a,[a],[a])
split (x:xs) = foldr op (x,[ ],[ ]) xs
where op x (y,ys,zs) | x ⩽y
= (x,y:zs,ys)
| otherwise = (y,x:zs,ys)
The function split returns three components. The ﬁrst component is the smallest
element in the input, while the other two are lists, of as equal a size as possible.
Answer 5.14 Eliminating the tree gives
hsort [ ] = [ ]
hsort xs = y:merge (hsort ys) (hsort zs)
where (y,ys,zs) = split xs
But this is just another version of Mergesort.

Answers
117
Answer 5.15 The method is similar to the one used in Mergesort. Deﬁne
mkpair ::Nat →[a] →(Tree a,[a])
mkpair n xs = (mktree (take n xs),drop n xs)
mktree xs
= fst (mkpair (length xs) xs)
This time we obtain
mkpair 0 xs
= (Null,xs)
mkpair n (x:xs) = (Node x l r,zs)
where (l,ys) = mkpair m xs
(r,zs) = mkpair (n−1−m) ys
m = (n−1) div 2
Answer 5.16 A size-balanced tree of size n has height H(n), where H(0) = 0 and
H(n+1) = 1+H(⌈n/2⌉). The solution to this recurrence is H(n) = ⌈log(n+1)⌉.
The induction step follows from ⌈log(n+2)⌉= 1+⌈log(⌈n/2⌉+1)⌉, whose proof
is another application of the rule of ceilings.
Answer 5.17 The answer is simply to count the number of times each element
occurs. An array can be used to store the count of each element, and the ﬁnal sorted
output can then be read off from the array:
csort ::Nat →[Int] →[Int]
csort m xs = concat [replicate k x | (x,k) ←assocs a]
where a = accumArray (+) 0 (0,m) [(x,1) | x ←xs]
This is Countsort, and takes Θ(m+n) steps. It was also presented in Section 3.3.
Answer 5.18 We have
tmap f (Leaf x) = Leaf (f x)
tmap f (Node ts) = Node (map (tmap f) ts)
Here is the proof of (5.3):
map (bsort ds) (ptn d xs)
=
{ deﬁnition of ptn }
[bsort ds (ﬁlter ((== m)·d) xs) | m ←rng]
=
{ deﬁnition of bsort }
[(ﬂatten·mktree ds·ﬁlter ((== m)·d)) xs | m ←rng]
=
{ assumptions (5.4) and (5.5) }
[(ﬁlter ((== m)·d)·ﬂatten·mktree ds) xs | m ←rng]
=
{ deﬁnition of bsort }
[ﬁlter ((== m)·d) (bsort ds xs) | m ←rng]
=
{ deﬁnition of ptn }
ptn d (bsort ds xs)

118
Sorting
The proof of (5.4) is by induction over the discriminators. Unlike the proof above, it
is carried out in a point-free style. It is easy to show
mktree [ ]·ﬁlter p = tmap (ﬁlter p)·mktree [ ]
and that establishes the base case. For the induction step we reason
mktree (d :ds)·ﬁlter p
=
{ deﬁnition of mktree }
Node·map (mktree ds)·ptn d ·ﬁlter p
=
{ assumption (5.6) }
Node·map (mktree ds·ﬁlter p)·ptn d
=
{ induction }
Node·map (tmap (ﬁlter p)·mktree ds)·ptn d
=
{ deﬁnition of tmap }
tmap (ﬁlter p)·Node·map (mktree ds)·ptn d
=
{ deﬁnition of mktree }
tmap (ﬁlter p)·mktree (d :ds)
For the proof of (5.6) we reason
map (ﬁlter p) (ptn d xs)
=
{ deﬁnition of ptn }
[ﬁlter p (ﬁlter ((== m)·d) xs) | m ←rng]
=
{ ﬁlters of total predicates commute }
[ﬁlter ((== m)·d) (ﬁlter p xs) | m ←rng]
=
{ deﬁnition of ptn }
ptn (ﬁlter p xs)
Finally, we prove (5.5) by induction over the tree. The induction step is
ﬂatten (tmap (ﬁlter p) (Node ts))
=
{ deﬁnition of tmap }
ﬂatten (Node (map (tmap (ﬁlter p)) ts))
=
{ deﬁnition of ﬂatten }
concat (map (ﬂatten·tmap (ﬁlter p)) ts)
=
{ induction }
concat (map (ﬁlter p·ﬂatten) ts)
=
{ claim; see below }
ﬁlter p (concat (map ﬂatten ts))
=
{ deﬁnition of ﬂatten }
ﬁlter p (ﬂatten (Node ts))
The claim is that
concat ·map (ﬁlter p) = ﬁlter p·concat
But enough is enough, and we omit the proof.

Answers
119
Answer 5.19 One way:
wsort ::[Word] →[Word]
wsort [ ] = [ ]
wsort xss = rsort ('a','z') ds xss
where ds = [λxs.(xs++repeat 'a')!!i | i ←[0..k −1]]
k = maximum [length xs | xs ←xss]
Answer 5.20 The proof is as follows:
(x−y) ⩽(x′ −y′)
⇔
{ adding y to both sides }
(x−y)+y ⩽(x′ −y′)+y
⇔
{ associativity and commutativity }
x+(y−y) ⩽(y−y′)+x′
⇔
{ since y−y = 0 and x+0 = x }
x ⩽(y−y′)+x′
⇔
{ subtracting x′ from both sides }
(x−x′) ⩽(y−y′)


Chapter 6
Selection
This chapter describes four related problems in which a divide-and-conquer strategy
can be employed to good effect. All involve selection of some kind, whether from
one set, two sets, or the complement of a set. The primary example is the problem
of selecting the kth smallest element in a set, where k can be anything from 1 to n,
the size of the set. Finding the minimum element (the 1st smallest) or the maximum
element (the nth smallest) are special cases, as is the problem of ﬁnding the median
element (roughly, the ⌊n/2⌋th smallest, but see below). We shall also consider the
problem of selecting the kth smallest in the union of two given sets, and ﬁnding the
smallest number not in a given set of natural numbers. Efﬁcient solutions to these
problems depend on how the set is represented, whether by a list, with or without
duplicates, by an array, or by a tree of some kind.
6.1 Minimum and maximum
By way of a warm-up, let us start with the problem of computing the minimum and
maximum elements of a ﬁnite nonempty list. The standard deﬁnitions are
minimum,maximum::Ord a ⇒[a] →a
minimum = foldr1 min
maximum = foldr1 max
The prelude function foldr1, and its companion foldl1, are fold functions for
nonempty lists:
foldr1,foldl1::(a →a →a) →[a] →a
foldr1 f [x]
= x
foldr1 f (x:xs) = f x (foldr1 f xs)
foldl1 f (x:xs) = foldl f x xs
For example,

122
Selection
foldr1 (⊕) [w,x,y,z] = w⊕(x⊕(y⊕z))
foldl1 (⊕) [w,x,y,z] = ((w⊕x)⊕y)⊕z
The deﬁnition of minimum and maximum uses a foldr1, which processes the list
from right to left; we can also use a foldl1 and process from left to right because min
and max are associative operations. In either direction there are n−1 evaluations of
min or max for a list of length n. Each evaluation involves a single comparison, so
there are n−1 comparisons in total. This is the best one can achieve. Think of an
algorithm for determining the maximum as a tennis tournament between n players,
in which the outcome of a single match corresponds to the outcome of a single
comparison. Every player apart from the eventual winner must lose a match, so
there must be n−1 matches.
If we want both the minimum and maximum elements, then 2n−2 comparisons
are certainly sufﬁcient. The obvious method involves two passes over the input, but
it can be reduced to one pass by making use of the tupling law
(foldr f1 e1 xs,foldr f2 e2 xs) = foldr f (e1,e2) xs
where f x (y,z) = (f1 x y,f2 x z)
Although it is not true in general that
foldr1 f (x:xs) = foldr f x xs
we do have
minimum (x:xs) = foldr min x xs
maximum (x:xs) = foldr max x xs
because both min and max are commutative as well as associative. It follows that
minmax::Ord a ⇒[a] →(a,a)
minmax (x:xs) = foldr op (x,x) xs
where op x (y,z) = (min x y,max x z)
The number of comparisons can be reduced by rewriting op, using the fact that the
current minimum is never greater than the current maximum:
minmax::Ord a ⇒[a] →(a,a)
minmax (x:xs) = foldr op (x,x) xs
where op x (y,z) | x<y
= (x,z)
| z<x
= (y,x)
| otherwise = (y,z)
At each step the current minimum and maximum are updated according to whether
the new element is smaller than the current minimum, larger than the current
maximum, or in between. In the worst case there are two comparisons at each step,
giving 2n−2 comparisons in total, the same number as before. However, in the best

6.1 Minimum and maximum
123
case there are only n−1 comparisons. Examples of best-case and worst-case inputs
are set as exercises.
Here is a divide-and-conquer algorithm for the same problem:
minmax [x]
= (x,x)
minmax [x,y] = if x ⩽y then (x,y) else (y,x)
minmax xs
= (min a1 a2,max b1 b2)
where (a1,b1) = minmax ys
(a2,b2) = minmax zs
(ys,zs) = halve xs
The function halve was deﬁned in Section 5.2. The minimum and maximum element
in a singleton list coincide, so the answer can be computed with zero comparisons.
Otherwise, the input is divided into two equal halves, the results for both halves
are computed recursively, and the ﬁnal answer is obtained by comparing the two
minimums and the two maximums. The case of a doubleton list is treated separately
simply because the total number of comparisons can be reduced from two to one in
this case.
The total running time of this version of minmax goes up to Θ(n log n) steps,
which hardly seems to make it worthwhile. But, counting comparisons only, the
number C(n) of comparisons satisﬁes the recurrence relation
C(1) = 0
C(2) = 1
C(n) = C(⌊n/2⌋)+C(⌈n/2⌉)+2
This recurrence is difﬁcult to solve exactly, though it is easy enough to show that
C(n) = 3n/2−2 when n is a positive power of two. Thus the divide-and-conquer
algorithm can save a quarter of the comparisons.
In fact, the divide-and-conquer algorithm is not the best possible. For example,
C(12) = 18 but the minimum and maximum of 12 elements can be computed with
only 16 comparisons. To see why, here is an algorithm for minmax obtained by
switching to a bottom-up scheme:
minmax = unwrap·until single (pairWith op)·mkPairs
where op (a1,b1) (a2,b2) = (min a1 a2,max b1 b2)
pairWith f [ ]
= [ ]
pairWith f [x]
= [x]
pairWith f (x:y:xs) = f x y:pairWith f xs
mkPairs [ ]
= [ ]
mkPairs [x]
= [(x,x)]
mkPairs (x:y:xs) = if x ⩽y then (x,y):mkPairs xs else (y,x):mkPairs xs
With C(n) as the comparison count, we have

124
Selection
C(1) = 0
C(1) = 1
C(n) = ⌊n/2⌋+D(⌈n/2⌉)
where ⌊n/2⌋accounts for the number of comparisons to compute mkPairs and
D(1) = 0
D(n) = 2⌊n/2⌋+D(⌈n/2⌉)
It is easy enough to show that D(n) = 2(n−1), so C(n) = n+⌈n/2⌉−2. In particular
C(12) = 16. In fact, C(n) is also a lower bound on the number of comparisons
needed to compute the minimum and maximum of n elements in the worst case (see
Answer 6.7). Moreover, the total running time of minmax is Θ(n) steps. However,
the bottom-up algorithm is not recommended as a method for computing minmax
because the other constants involved are larger than with the naive version.
6.2 Selection from one set
By deﬁnition, the kth smallest element of a set (or a list without duplicates – see
Exercise 6.1) with n elements has exactly k −1 elements smaller than it, so the 1st
smallest is the smallest, and the nth smallest is the largest. The median m of a set
with an odd size n is an element of the set for which there are exactly ⌊n/2⌋elements
smaller than m and ⌊n/2⌋elements greater than m. When n is even there is a choice
of deﬁnition, and we pick the one that has ⌊n/2⌋−1 smaller elements and ⌊n/2⌋
larger elements. This deﬁnes what is sometimes called the lower median. Combining
the two cases, we have that the median of a set of size n has ⌊(n+1)/2⌋−1 smaller
elements and ⌈(n+1)/2⌉−1 greater elements.
When a set is represented by a list with no duplicates, the kth smallest appears at
position k −1 in a sorted list. Hence we can deﬁne
select ::Ord a ⇒Nat →[a] →a
select k xs = (sort xs)!!(k −1)
In particular,
median xs = select k xs where k = (length xs+1) div 2
The deﬁnitions of select and median make sense when xs contains duplicates, and
in what follows we will not assume that the elements of xs are all different. Since
sorting a list takes O(n log n) steps, the running time of select is O(n log n) steps.
This is an upper bound rather than an exact bound because, under lazy evaluation,
the whole list may not have to be sorted in order to retrieve the kth element – it
depends on the sorting algorithm. On the other hand, the running time is Ω(n) steps
because it takes this time just to inspect every element of the input. Can these lower
and upper bounds be made to coincide, that is, can select be computed in linear

6.2 Selection from one set
125
time? The answer is yes, and the algorithm is yet another clever example of divide
and conquer.
First, though, let us replace the function sort in the deﬁnition of select by a
modiﬁed version of qsort, the Quicksort algorithm described in the previous chapter:
qsort [ ] = [ ]
qsort xs = qsort us++vs++qsort ws
where (us,vs,ws) = partition3 (pivot xs) xs
Here pivot chooses some element of a list as the pivot and partition3, whose deﬁni-
tion was given in Answer 4.10, splits a list into three lists: those elements less than,
equal to, or greater than the pivot. Splitting a list into three sublists rather than two
is better whenever duplicate elements may be present.
We can synthesise a faster version of select by exploiting the following divide-
and-conquer property of the list-indexing operation:
(xs++ys)!!k = if k <n then xs!!k else ys!!(k −n) where n = length xs
Now we can reason for nonempty xs:
select k xs
=
{ deﬁnition }
qsort xs!!(k −1)
=
{ with (us,vs,ws) = partition3 (pivot xs) xs }
(qsort us++vs++qsort ws)!!(k −1)
=
{ assuming k −1<length us }
qsort us!!(k −1)
=
{ deﬁnition of select }
select k us
The other cases can be dealt with in a similar fashion, and we end up with the
following deﬁnition of select:
select ::Ord a ⇒Nat →[a] →a
select k xs
| k ⩽m
= select k us
| k ⩽m+n = vs!!(k −m−1)
| k >m+n = select (k −m−n) ws
where (us,vs,ws) = partition3 (pivot xs) xs
(m,n)
= (length us,length vs)
The middle list vs is not empty because it contains at least one copy of the pivot. Let
T(n) be the running time of this version of select on an input of length n. Assuming
partition3 splits the list into three lists, the ﬁrst and third having length at most
(n−1)/2, we have
T(n) ⩽T((n−1)/2)+Θ(n)

126
Selection
with solution T(n) = O(n). On the other hand, partition3 may return a very unequal
split, so the running time in the worst case satisﬁes
T(n) = T(n−1)+Θ(n)
with solution T(n) = Θ(n2). If the partitioning uses the median of the list as pivot,
then there will be an equal split and the result will be a linear-time algorithm for
select. But that involves ﬁnding the median in linear time, which is essentially what
we are trying to achieve in the ﬁrst place. The idea therefore seems circular and
without obvious merit.
However, it can be made to work. The wiggle room is that we do not have to
choose the actual median as the pivot: any value will do as long as the result of
partitioning is three lists, the sum of the lengths of the ﬁrst and third lists being some
proper fraction of the input. We shall see why later on. The method for choosing
pivot is a very cunning divide-and-conquer scheme. First, divide the input into
groups of ﬁve by applying group 5, where
group::Nat →[a] →[[a]]
group n [ ] = [ ]
group n xs = ys:group n zs where (ys,zs) = splitAt n xs
If the length of the input is not an exact multiple of ﬁve, there will be a trailing
group of shorter length. For example,
group 5 [1..12] = [[1..5],[6..10],[11,12]]
There are therefore ⌈n/5⌉groups. Computing group takes linear time.
Next, ﬁnd the median of each group by sorting each group and taking the middle
element of the result. That is, compute
medians = map (middle·sort)·group 5
where middle xs = xs!!((length xs+1) div 2−1)
For example, medians [1..12] = [3,8,11]. Computing medians also takes linear
time: sorting each group of ﬁve elements takes constant time, as does evaluation of
middle, and there are ⌈n/5⌉groups.
Finally, deﬁne pivot to select the median of these medians by applying the
algorithm for select recursively. That gives
pivot ::Ord a ⇒[a] →a
pivot [x] = x
pivot xs = median (medians xs)
where median xs = select ((length xs+1) div 2) xs
The clause pivot [x] = x has to be included as a special case: without it we would
have pivot [x] = select 1 [x], and computing the right-hand side would involve

6.2 Selection from one set
127
computing pivot [x] again, so the whole program would spin off into an inﬁnite
loop.
To estimate the running time T(n) of select in the worst case, observe that ﬁnding
the median of the medians takes T(⌈n/5⌉)+Θ(n) steps because pivot calls select
recursively on a list of length ⌈n/5⌉. The partitioning step takes Θ(n) steps on a
list of length n. To these times we have also to add in the running time to select
from either the ﬁrst or third list returned by partition3. We claim that each of these
lists has length at most 7n/10 for an input of length n. To appreciate why, here is a
picture for a particular input of length 28:
42
37
99
70
95
17
36
43
69
79
88
11
23
29
61
73
87
06
13
28
52
30
32
01
09
21
38
18
The columns are the groups of length 5, except for the last column. Each of these
groups has been sorted, and the columns have been arranged in increasing order
of their median. The median m = 29 of the medians has been highlighted. The
algorithm does not arrange the columns in this way; the picture is there just to
explain what we can say about how m partitions the list.
The key property of m is as follows. The bottom-left rectangle with upper-right
corner at m contains only elements less than or equal to m. The number of elements
in this rectangle, apart from m, is
3 ⌊(⌈n/5⌉+1)/2⌋−1 ⩾3n/10
(see Exercise 6.10). That means the number of elements greater than m is at most
7n/10. By reasoning similarly about the top-right rectangle with bottom-left corner
at m we have that the number of elements less than m is at most 7n/10. That means
that in the worst case select is called recursively on a list of size at most 7n/10.
Ignoring ﬂoors and ceilings, the total running time therefore satisﬁes
T(n) ⩽T(n/5)+T(7n/10)+cn
for some c. It is easy to show by induction that T(n) ⩽bn for some appropriately
chosen b. For the induction step, we have to show
bn/5+7bn/10+cn ⩽bn
This inequality is immediate on taking b = 10c. In summary, select can be computed
in linear time.

128
Selection
6.3 Selection from two sets
Continuing the theme of selection, consider the problem of computing select, where
this time we deﬁne
select ::Ord a ⇒Nat →[a] →[a] →a
select k as bs = (merge as bs)!!k
Thus select takes a number k and two sorted lists, and returns the element at position
k in the merged list. In particular, select 0 as bs returns the smallest element in
merge as bs.
How long does the computation of select take? Clearly, if both lists have length n,
then select takes O(n) steps because two lists can be merged in this time. However,
if they are given as two sorted arrays, or two balanced binary search trees, then
the time can be reduced to O(log n) steps. The result is a little surprising because
two arrays or two binary search trees certainly cannot be merged in less than
linear time. The faster algorithm is yet another example of divide and conquer,
and the proof that it works hinges on a subtle relationship between merging and
selection.
Here is the relationship: provided the arguments of merge are two sorted lists, we
have
merge (xs++[a]++ys) (us++[b]++vs)!!k
| a ⩽b ∧k ⩽p+q = merge (xs++[a]++ys) us!!k
| a ⩽b ∧k >p+q = merge ys (us++[b]++vs)!!(k −p−1)
| b ⩽a ∧k ⩽p+q = merge xs (us++[b]++vs)!!k
| b ⩽a ∧k >p+q = merge (xs++[a]++ys) vs!!(k −q−1)
where p = length xs;q = length us
The proof is given later on. Using the relationship, it is possible to derive the
following deﬁnition of select:
select k [ ] bs = bs!!k
select k as [ ] = as!!k
select k as bs | a ⩽b ∧k ⩽p+q = select k as us
-- line 3
| a ⩽b ∧k >p+q = select (k −p−1) ys bs
-- line 4
| b ⩽a ∧k ⩽p+q = select k xs bs
-- line 5
| b ⩽a ∧k >p+q = select (k −q−1) as vs
-- line 6
where p = (length as) div 2
q = (length bs) div 2
(xs,a:ys) = splitAt p as
(us,b:vs) = splitAt q bs
The derivation is similar to the one involving qsort in the previous section and we

6.3 Selection from two sets
129
won’t go into details. Instead, here is the trace of an example of select in which
k = 6:
p q k
as
bs
3 3 6
[1,4,4,7,8,11,15]
[2,5,9,11,15,16,20]
-- line 3
3 1 6
[1,4,4,7,8,11,15]
[2,5,9]
-- line 6
3 0 4
[1,4,4,7,8,11,15]
[9]
-- line 4
1 0 0
[8,11,15]
[9]
-- line 5
0 0 0
[8]
[9]
-- line 3
0 0 0
[8]
[ ]
The values of a and b have been underlined. The last column gives the line number
of the recursive call of select used for the next step. The ﬁnal value of select on the
two lists is 8.
Since select chooses the values of a and b to be the middle elements of the two
lists, half of one of the lists is discarded at each step. Ignoring completely the cost
of evaluating the local deﬁnitions, the running time of select therefore satisﬁes
T(m,n) = T(m,n/2) max T(m/2,n)+Θ(1)
with solution T(m,n) = Θ(log m+log n). Of course, evaluations of the local deﬁni-
tions take linear rather than constant time, so the true timing estimate is
T(m,n) = T(m,n/2) max T(m/2,n)+Θ(m+n)
with solution T(m,n) = Θ(m+n). The divide-and-conquer-algorithm is therefore
no faster than the naive version we started out with. The pay-off comes when the
two lists are given as two arrays or two binary search trees. Then we can arrange
that the cost of the local evaluations is indeed constant.
We will spell out the details for binary search trees, leaving arrays to the exercises.
Recall from Chapter 4 that a binary search tree is a tree of type
data Tree a = Null | Node Nat (Tree a) a (Tree a)
with the property that, provided the tree labels are values of an ordered type, ﬂatten-
ing a tree produces a list in ascending order. Recall also that the Nat component of
a Node contains the height of the tree. However, this time we need to assume that
the integer is not the height, but the size of the tree. Thus
size Null
= 0
size (Node s l x r) = s
Of course, installing such information takes time, but we will ignore the cost of
doing so. With that assumption, the task is to compute
select ::Ord a ⇒Nat →Tree a →Tree a →a
select k t1 t2 = merge (ﬂatten t1) (ﬂatten t2)!!k

130
Selection
The faster algorithm is given by
select k t1 Null = index t1 k
select k Null t2 = index t2 k
select k (Node h1 l1 a r1) (Node h2 l2 b r2)
| a ⩽b ∧k ⩽p+q = select k (Node h1 l1 a r1) l2
| a ⩽b ∧k >p+q = select (k −p−1) r1 (Node h2 l2 b r2)
| b ⩽a ∧k ⩽p+q = select k l1 (Node h2 l2 b r2)
| b ⩽a ∧k >p+q = select (k −q−1) (Node h1 l1 a r1) r2
where p = size l1;q = size l2
The function index is speciﬁed by
index t k = ﬂatten t !!k
It is easy to derive
index (Node
l x r) k
| k <p
= index l k
| k == p = x
| k >p
= index r (k −p−1)
where p = size l
and we leave details as an exercise.
Now for the tricky part, the proof of the relationship between merging and
selection. Recall that we have to simplify
merge (xs++[a]++ys) (us++[b]++vs)!!k
We will assume that a ⩽b; the case a ⩾b is entirely dual. Furthermore, let p be the
length of xs and q the length of us in what follows.
We will need to make use of two decomposition rules, one for list-indexing and
one for merging. The decomposition rule for indexing has been used before:
(xs++ys)!!k = if k <n then xs!!k else ys!!(k −n) where n = length xs
To state the decomposition rule for merge, ﬁrst deﬁne ≪= by
(≪=)::Ord a ⇒[a] →[a] →Bool
xs≪=ys = and [x ⩽y | x ←xs,y ←ys]
Thus xs ≪= ys holds if no element of xs is larger than any element of ys. The
decomposition rule for merge now states that
merge (xs++ys) (us++vs) = merge xs us++merge ys vs
provided xs≪=vs and us≪=ys. For the proof observe that xs≪=ys because the
list xs++ys is sorted. If xs≪=vs holds as well, then

6.3 Selection from two sets
131
xs≪=merge ys vs
Similarly, us≪=merge ys vs if us≪=ys. Hence, if both hold, then
merge xs us≪=merge ys vs
from which the decomposition rule for merge follows.
As well as the two decomposition rules, we will need two other observations.
First, suppose ys = ys1 ++ys2, where ys1 is the longest preﬁx of ys such that
xs++[a]++ys1 ≪=[b]++vs
Then we claim us≪=ys2. Either ys2 is empty, in which case the result is immediate,
or its ﬁrst element is greater than b, which means it is greater than any element in
us. As a consequence, we have
merge (xs++[a]++ys1 ++ys2) (us++[b]++vs)
= merge (xs++[a]++ys1) us++merge ys2 ([b]++vs)
by the decomposition rule for merge.
The second observation is dual. Suppose us = us1 ++us2, where us2 is the longest
sufﬁx of us such that
xs++[a]≪=us2 ++[b]++vs
then us1 ≪=ys. That means
merge (xs++[a]++ys) (us1 ++us2 ++[b]++vs)
= merge (xs++[a]) us1 ++merge ys (us2 ++[b]++vs)
We are now ready for the main calculation. Assume ﬁrst that k ⩽p+q. We reason
merge (xs++[a]++ys) (us++[b]++vs)!!k
=
{ choose ys1 and ys2 as above }
merge (xs++[a]++ys1 ++ys2) (us++[b]++vs)!!k
=
{ decomposition rule of merge; see above }
(merge (xs++[a]++ys1) us++merge ys2 ([b]++vs))!!k
=
{ assumption k ⩽p+q and decomposition rule of (!!) }
merge (xs++[a]++ys1) us!!k
=
{ decomposition rule of (!!) again }
(merge (xs++[a]++ys1) us++merge ys2 [ ])!!k
=
{ decomposition rule of merge again }
merge (xs++[a]++ys) us!!k
The second case is when k > p + q. Let q1 be the length of us1, where us1 is as
deﬁned above. Then we have
p+1+q1 ⩽p+1+q ⩽k

132
Selection
This time we reason
merge (xs++[a]++ys) (us++[b]++vs)!!k
=
{ choose us1 and us2 as above }
merge (xs++[a]++ys) (us1 ++us2 ++[b]++vs)!!k
=
{ decomposition property of merge; see above }
(merge (xs++[a]) us1 ++merge ys (us2 ++[b]++vs))!!k
=
{ assumption on k >p+q and decomposition rule of (!!) }
merge ys (us2 ++[b]++vs)!!(k −p−1−q1)
=
{ decomposition properties of (!!) and merge again }
merge ys (us++[b]++vs)!!(k −p−1)
The proof is complete.
6.4 Selection from the complement of a set
Sometimes we want to select from the complement of a set. For example, given a
set of ﬁve-letter words, we might want the (lexically) smallest ﬁve-letter word not
in the set. Or we might want the smallest natural number not in a given ﬁnite set of
natural numbers. The problem is a simpliﬁcation of a common programming task
in which the set represents objects currently in use and one wants to select some
object not in use, say the one with the smallest name. In this section we tackle the
natural number version of the problem, supposing the set is given as a list without
duplicates in no particular order. For example,
[08,23,09,00,12,11,01,10,13,07,41,04,14,21,05,17,03,19,02,06]
How would you go about ﬁnding the smallest natural number not in this list?
Here is the speciﬁcation of the problem:
select ::[Nat] →Nat
select xs = head ([0..]\\xs)
(\\)::Eq a ⇒[a] →[a] →[a]
xs\\ys = ﬁlter (/∈ys) xs
The value xs\\ys (pronounced ‘xs minus ys’) is what remains when every element
of ys is removed from xs. Evaluation of select on a list of length n takes Ω(n2) steps.
For example, evaluation of select [0..n−1] requires n+1 membership tests, the
total cost of which is n(n+1)/2 equality tests.
One idea that quickly springs to mind for improving the running time of select is
to sort the input. Since the order of the elements in the input is not material, we have
select xs = head ([0..]\\sort xs)

6.4 Selection from the complement of a set
133
Now that the right-hand argument to \\ is ordered, we can simply look for the ﬁrst
gap:
select xs
= searchFrom 0 (sort xs)
searchFrom k [ ]
= k
searchFrom k (x:xs) = if k == x then searchFrom (k +1) xs else k
This improves the running time of select to O(n log n) steps, assuming an asymptot-
ically optimal sorting algorithm is used. However, we can do better still and reduce
the time to Θ(n) steps. The key observation is that it is not necessary to sort all of
the input: only those elements that are at most n, the size of the set, need be sorted.
The reason is that not every number in {0,1,...,n} can be in the set: there are n+1
numbers in the former and only n in the latter. That means we can deﬁne
select xs = searchFrom 0 (sort (ﬁlter (⩽n) xs)) where n = length xs
Unlike general sorting, we can sort a list of n natural numbers, all of which are in the
range (0,n), in Θ(n) steps. For example, we can use Countsort (see Answer 5.17):
select xs = searchFrom 0 (csort n (ﬁlter (⩽n) xs))
where n = length xs
csort n xs = concat [replicate k x | (x,k) ←assocs a]
where a = accumArray (+) 0 (0,n) [(x,1) | x ←xs]
In fact there is no need to produce the sorted list: we can simply look for the ﬁrst
index with count 0:
select xs = length (takeWhile (̸= 0) (elems a))
where a = accumArray (+) 0 (0,n) [(x,1) | x ←xs,x ⩽n]
n = length xs
This algorithm does not depend on the input being a list without duplicates.
It is also possible to devise a linear-time divide-and-conquer solution that does not
make use of arrays. The idea is to decompose the lists into two equal-size sublists
and then compute the solution recursively by continuing with just one of the sublists,
the same strategy that was used with binary search. Assuming the decomposition
takes Θ(n) steps, we then obtain the recurrence relation
T(n) = T(n/2)+Θ(n)
for the running time T(n), with solution T(n) = Θ(n).
We can split xs by partitioning xs on a suitably chosen natural number b (the
choice will be made later on), using the function partition of Quicksort. With
(ys,zs) = partition (<b) xs we have
[0..]\\xs = ([0..b−1]\\ys)++([b..]\\zs)
Here is the proof:

134
Selection
[0..]\\xs
=
{ since [0..] = [0..b−1]++[b..] }
([0..b−1]++[b..])\\xs
=
{ since (as++bs)\\xs = (as\\xs)++(bs\\xs) }
([0..b−1]\\xs)++([b..]\\xs)
=
{ since as\\xs = (as\\ys)\\zs = (as\\zs)\\ys }
(([0..b−1]\\zs)\\ys)++(([b..]\\ys)\\zs))
=
{ since [0..b−1]\\zs = [0..b−1] and [b..]\\ys = [b..] }
([0..b−1]\\ys)++([b..]\\zs)
Next, since
head (as++bs) = if null as then head bs else head as
we obtain
select xs = if null ([0..b−1]\\ys)
then head ([b..]\\zs)
else head ([0..b−1]\\ys)
where (ys,zs) = partition (<b) xs
Now comes a second key observation. Since ys does not contain duplicates and
every element of ys is less than b, we have
null ([0..b−1]\\ys) = (length ys == b)
Inspection of the code for select suggests that we should generalise select to a
function, selectFrom say, deﬁned by
selectFrom::Nat →[Nat] →Nat
selectFrom a xs = head ([a..]\\xs)
under the invariant that no element of xs is smaller than a. Then, provided b is
chosen so that the lengths of both partitioned lists are at most half the length of the
original, the following recursive deﬁnition of select is well-founded:
select xs = selectFrom 0 xs
selectFrom a xs | null xs
= a
| length ys == b−a = selectFrom b zs
| otherwise
= selectFrom a ys
where (ys,zs) = partition (<b) xs
It remains to choose b. Clearly we want b>a, but we would also like to ensure the
lengths p and q of the two lists ys and zs are as equal as possible. The appropriate
choice to satisfy these requirements is b = a + 1 + ⌊n/2⌋, where n is the length
of xs. If n ̸= 0 and p<b−a, then p ⩽b−a−1 = ⌊n/2⌋, while if p = b−a, then
q = n −(b −a) = n −⌊n/2⌋−1 ⩽⌊n/2⌋. As a ﬁnal optimisation we can avoid
repeatedly computing length by tupling each list with its length. All of that leads to

6.5 Chapter notes
135
select xs = selectFrom 0 (length xs,xs)
selectFrom a (n,xs) | n == 0
= a
| l == b−a = selectFrom b (n−l,zs)
| otherwise = selectFrom a (l,ys)
where (ys,zs) = partition (<b) xs
b
= a+1+n div 2
l
= length ys
This solution also takes linear time.
6.5 Chapter notes
The lower bound on the number of comparisons required to compute minmax
(see Answer 6.7) is from Pohl [5]. The linear-time selection algorithm was ﬁrst
described by Blum et al. [2]. The selection algorithm derived from Quicksort is due
to Hoare [3]. It is still not known exactly how many comparisons are required to
ﬁnd the median; see Paterson [4]. The problems of selecting from the union of two
sets and from the complement of a set were treated in [1].
References
[1]
Richard Bird. Pearls of Functional Algorithm Design. Cambridge University Press,
Cambridge, 2010.
[2]
Manuel Blum, Robert W. Floyd, Vaughan Pratt, Ronald L. Rivest, and Robert E.
Tarjan. Time bounds for selection. Journal of Computer and System Sciences,
7(4):448–461, 1973.
[3]
Charles A. R. Hoare. Algorithm 63 (PARTITION) and Algorithm 65 (FIND).
Communications of the ACM, 4(7):321–322, 1961.
[4]
Michael S. Paterson. Progress in selection. In R. Karlsson and A. Lingas, editors,
Scandinavian Workshop on Algorithm Theory, volume 1097 of Lecture Notes in
Computer Science, pages 368–379. Springer-Verlag, Berlin, 1996.
[5]
Ira Pohl. A sorting problem and its complexity. Communications of the ACM,
15(6):462–464, 1972.
Exercises
Exercise 6.1 Does it make sense to deﬁne the kth smallest element of an arbitrary
list as an element with exactly k −1 elements smaller than it?
Exercise 6.2 Deﬁne combinators cross and pair so that
pair (foldr f1 e1,foldr f2 e2) = foldr (cross·pair (f1,f2)) (e1,e2)
Exercise 6.3 Describe two lists of length n for which the second deﬁnition of
minmax uses n−1 and 2n−2 comparisons, respectively.

136
Selection
Exercise 6.4 Show that in the divide-and-conquer algorithm for minmax the com-
parison count C(n) satisﬁes C(n) = 3n/2−2 when n is a positive power of two.
Exercise 6.5 Show that D(n) = 2(n−1) in the bottom-up algorithm for minmax.
Exercise 6.6 Suppose a set is given by a balanced binary search tree. Asymptoti-
cally, how long does it take to ﬁnd the minimum and maximum elements?
Exercise 6.7 Consider a particularly brutal tennis tournament with the twist that
the tournament has to determine both the best and the worst player. Initially all n
players are potential champions or potential losers, and the overlap between the
two groups is n. The best we can do is to play a match between two players in the
overlap, placing the winner in a potential champion category, and the loser in a
potential loser category. Assuming n is even, it follows that after n/2 matches the
overlap is reduced to zero. How would you complete the tournament and how many
matches are there?
Exercise 6.8 Show that n+⌈log n⌉−2 matches are sufﬁcient to determine the best
and second-best players in a tennis tournament involving n players.
Exercise 6.9 Are there any other ways apart from setting pivot [x] = x to ensure
the deﬁnition of the linear-time algorithm for select is well-founded?
Exercise 6.10 Use the rules of ﬂoors and ceilings to show that
3 ⌊(⌈n/5⌉+1)/2⌋−1 ⩾3n/10
No case analysis is allowed.
Exercise 6.11 Instead of grouping elements into blocks of 5, suppose we group
into blocks of 3. Does this lead to a linear-time algorithm for select? How about
grouping into blocks of 7?
Exercise 6.12 Counting only comparisons between list elements, how many com-
parisons are required to compute select 4 [1..7] when pivot chooses the ﬁrst element
of a list, and when pivot is as deﬁned in the linear-time version of select? For the
second question you can assume that sorting n elements for 3 ⩽n ⩽5 requires 3
comparisons for n = 3, 5 comparisons for n = 4, and 7 comparisons for n = 5.
Exercise 6.13 Is ≪= a transitive relation, that is, does xs≪=ys and ys≪=zs imply
xs≪=zs?
Exercise 6.14 Suppose xs++ys and us++vs are both sorted. Prove that xs≪=vs
or us≪=ys.
Exercise 6.15 Assuming arrays are indexed from 0, write down a deﬁnition of
select ::Ord a ⇒Nat →Array Nat a →Array Nat a →a

Answers
137
Answers
Answer 6.1 No. Consider the third smallest element of the list [1,2,2,3]. There is
no element with exactly two smaller elements.
Answer 6.2 We can deﬁne
pair ::(a →b,a →c) →a →(b,c)
pair (f,g) x = (f x,g x)
cross::(a →c,b →d) →(a,b) →(c,d)
cross (f,g) (x,y) = (f x,g y)
Then we have
(cross·pair (f,g)) x (y,z) = cross (pair (f,g) x) (y,z)
= cross (f x,g x) (y,z)
= (f x y,g x z)
Answer 6.3 One worst case is when the input is in ascending order. One best case
is when the input is in ascending order but with the ﬁrst and last elements swapped.
Answer 6.4 When n is a power of two we have C(n) = 2C(n/2)+2 and a simple
induction yields the answer.
Answer 6.5 The induction step is D(n) = 2⌊n/2⌋+2(⌈n/2⌉−1) = 2(n−1).
Answer 6.6 We have to ﬁnd the leftmost and rightmost elements in the tree, each
of which takes Θ(log n) steps.
Answer 6.7 After reducing the overlap to zero, the best one can do thereafter is
to play n/2−1 matches among the potential losers to determine the worst player,
and n/2−1 matches among the potential winners to determine the champion. That
comes to a total of 3n/2 −2 matches. A similar argument holds for odd n and
shows that ⌈3n/2⌉−2 matches are sufﬁcient, which is the same bound as for the
bottom-up algorithm for minmax.
The tennis tournament analogy can be used to show that ⌈3n/2⌉−2 matches are
necessary in the worst case to determine both the champion and the worst player.
We cannot, of course, construct a worst case, because that would depend on the
particular algorithm being executed. Instead we can use an adversarial argument.
In this scenario, the adversary chooses the answers to each comparison test asked
for by a particular algorithm as it runs in order to force a worst case. The only
restriction on the adversary is that the answers must be consistent with all previous
answers. Now, at any stage of the tournament there are four possible groups of
players: those that haven’t played any matches so far (group A), those that have
played some matches and never lost (group B); those that have played some matches
and never won (group C); and those that have both won and lost a match (group

138
Selection
D). Let the quadruple (a,b,c,d) denote the number of players in each of these
categories at some stage of the tournament. Any algorithm starts with (n,0,0,0)
and ends with (0,1,1,n−2). The adversary can always arrange that the answer to
each comparison test either leaves (a,b,c,d) unchanged or else produces one of the
following quadruples (as long as all values are nonnegative):
(a−2,b+1,c+1,d)
(a−1,b,c+1,d)
(a−1,b+1,c,d)
(a,b−1,c,d +1)
(a,b,c−1,d +1)
In the ﬁrst case, a match between two group A players – an AA match – will always
produce one extra member of group B and one extra member of group C. In the
second case, the adversary can arrange that an AB match produces only one extra
member of group C (by having the player in group B win). And so on, for each of
the ten cases AA, AB, AC, AD, BB, BC, BD, CC, CD, and DD. The ﬁnal step is to
consider the value k = 3a+2b+2c. At the start of the tournament k = 3n and at
the conclusion k = 4. But the value of k can decrease by at most two at each step,
so it takes at least ⌈(3n−4)/2⌉= ⌈3n/2⌉−2 matches to determine the outcome.
Answer 6.8 It requires n−1 matches to determine the best player. Any player who
lost to the eventual winner of the tournament may be the second-best player. Since
there are ⌈log n⌉players who lost to the eventual winner, a second tournament of
⌈log n⌉−1 matches can be played to determine the second-best player. That gives a
total of n+⌈log n⌉−2 matches. Using another adversarial argument, it can also be
shown that this number of matches is necessary.
Answer 6.9 Yes, either set select 1 [x] = x or set median [x] = x.
Answer 6.10 The proof goes as follows:
3k −1 ⩽3⌊(⌈n/5⌉+1)/2⌋−1
⇔
{ arithmetic of integers }
k ⩽⌊(⌈n/5⌉+1)/2⌋
⇔
{ rule of ﬂoors }
2k −1 ⩽⌈n/5⌉
⇔
{ arithmetic of integers }
2k −2<⌈n/5⌉
⇔
{ (contrapositive) rule of ceilings }
10k −10<n
⇔
{ arithmetic }
k ⩽(n+9)/10
Hence
3n/10 ⩽3(n+9)/10−1 ⩽3⌊(⌈n/5⌉+1)/2⌋−1

Answers
139
Answer 6.11 No, dividing into blocks of 3 will not give a linear-time algorithm.
We have
2⌊(⌈n/3⌉+1)/2⌋−1 ⩾n/3
so the associated recurrence relation is
T(n) = T(n/3)+T(2n/3)+Θ(n)
whose solution is Θ(n log n). However, dividing into blocks of 7 is okay because
the associated recurrence relation is
T(n) = T(n/7)+T(5n/7)+Θ(n)
whose solution is T(n) = Θ(n).
Answer 6.12 When the pivot is chosen to be the ﬁrst element of the list, the value
of select 4 [1..7] is obtained by computing partition3 p [p..7] for p = 1,2,3,4.
Partitioning a list of n elements requires n comparisons, so 7 + 6 + 5 + 4 = 22
comparisons are required.
To answer the second question, the calling structure with associated comparison
counts is given by
select 4 [1..7]
pivot [1..7]
sort [1..5]
(7 comparisons)
sort [6,7]
(1 comparisons)
select 1 [3,6]
(4 comparisons)
partition3 3 [1..7]
(7 comparisons)
select 1 [4..7]
(11 comparisons)
The counts, 4 and 11, for the recursive calls select 1 [3,6] and select 1 [4..7] of
select are given by
select 1 [3,6]
pivot [3,6]
sort [3,6]
(1 comparisons)
select 1 [3]
(1 comparisons)
partition3 3 [3,6]
(2 comparisons)
for a total count of 4 and
select 1 [4..7]
pivot [4..7]
sort [4..7]
(5 comparisons)
select 1 [5]
(1 comparisons)
partition3 5 [4..7]
(4 comparisons)
select 1 [4]
(1 comparisons)

140
Selection
for a total count of 11. Calls of the form select 1 [x] each take one comparison. That
gives a grand total of 30 comparisons.
Answer 6.13 No, not if ys is empty. Transitivity holds only for nonempty lists.
Answer 6.14 The result is immediate if any of the four lists is empty. Otherwise
either last xs ⩽head vs or last us ⩽head ys because if both are false, then
head vs<last xs ⩽head ys<last us
contradicting the assumption that us++vs is sorted.
Answer 6.15 To help understand the program below, suppose xa [0..n−1] denotes
an array of n elements. Then bounds xa = (0,n−1). The segment xa [lx..rx] of xa
has length rx−lx+1, and so is empty if lx = rx+1. The midpoint of the segment
is xa!p, where p = (lx+rx) div 2. The element at position k, counting from 0, is at
position lx+rx in the segment, and at position k +lx+ly in the result of merging
xs [lx..rx] with ya [ly..ry]. With that understood, the function
select ::Ord a ⇒Nat →Array Nat a →Array Nat a →a
is deﬁned by
select k xa ya = search k (bounds xa) (bounds ya) where
search k (lx,rx) (ly,ry)
| lx == rx+1
= ya!(ly+k)
| ly == ry+1
= xa!(lx+k)
| a ⩽b ∧k +lx+ly ⩽p+q = search k (lx,rx) (ly,q−1)
| a ⩽b ∧k +lx+ly>p+q = search (k +lx−p−1) (p+1,rx) (ly,ry)
| b ⩽a ∧k +lx+ly ⩽p+q = search k (lx,p−1) (ly,ry)
| b ⩽a ∧k +lx+ly>p+q = search (k +ly−q−1) (lx,rx) (q+1,ry)
where p = (lx+rx) div 2
q = (ly+ry) div 2
a = xa!p
b = ya!q

PART THREE
GREEDY ALGORITHMS


143
Many computational problems involve selecting some best candidate from a set of
possible candidates. Candidates can be lists, trees, layouts of a document, routes
in a network of roads, and so on. The best candidate may be the shortest list, the
least wasteful paragraph, or the quickest route. Even sorting can be regarded as
an optimisation problem; after all, the aim of a sorting algorithm is to ﬁnd some
permutation of the input that minimises the number of out-of-order elements. Greedy
sorting algorithms will be our ﬁrst topic in Chapter 7. In general there may be more
than one best candidate and the task is to ﬁnd just one of them.
The input to such problems is normally not the set of candidates but a list of
components out of which the candidates can be built. For example, the raw materials
may be a list of words that constitute the paragraph, a list of numbers that form
the fringe of a tree, or a list of towns and roads in a shortest-path algorithm. A
greedy algorithm solves such a problem in a step-by-step fashion by constructing a
single best partial candidate at each step. A partial candidate may be a fully formed
candidate for the components used so far in its construction, but it may be something
more general. The idea of a step-by-step algorithm is easy to grasp intuitively but
not so easy to formalise, especially in a purely functional language. For example, a
divide-and-conquer algorithm for sorting will minimise the number of out-of-order
elements but it is not, conceptually at least, a step-by-step algorithm. Finally, in most
of our examples a best candidate is one that minimises some notion of cost, so the
word ‘greedy’ may seem a little inappropriate. Perhaps ‘frugal’ or ‘parsimonious’
would be better adjectives. However, the name ‘greedy’ has become the standard
way of referring to these algorithms.
The idea of maintaining a single best candidate at each step will not always lead
to a best ﬁnal candidate. Suppose you are out walking on a hillside and wish to
climb to the highest point. If you are in a mist and cannot see where to go, you may
decide on the strategy of choosing to walk along a path of steepest ascent at each
step. That may work, but it may also lead to the top of a little hillock when there is
a much bigger hill in the background. The same is true of greedy algorithms (which
have also been called hillclimbing algorithms): you may get to a locally optimal
solution that is not globally optimal. We will say that a greedy algorithm works if it
does lead to a globally optimal solution.
Greedy algorithms can be tricky things. The trickiness is not in the algorithm
itself, which is usually quite short and easy to understand, but in the proof that
it does produce a best solution. With a greedy algorithm the correctness of the
program is less obvious than with, say, a sorting algorithm; after all we did not
spend any time in previous chapters on proving that the various sorting algorithms
actually did sort. The main difﬁculty with proving that a greedy algorithm works is
that for many problems equational reasoning is simply not up to the task and has

144
to be replaced by reasoning about reﬁnement. We will see what this entails in due
course.
In the following chapters we consider a number of greedy algorithms. Rather
than just giving the algorithm and proving it works, we will take a more struc-
tured approach, one that will pay dividends when we come to discuss dynamic
programming and exhaustive search algorithms. First we show how to deﬁne a
function candidates that generates the set of all possible candidates. This function
may be deﬁned recursively or by using a suitable higher-order function such as
foldr, until, or apply (a function that applies another function a given number of
times). Sometimes more than one style is available, each leading to an equally
clear deﬁnition, so it is a free choice as to which one to employ. Next we deﬁne
the selection criterion explicitly. This can also sometimes be done in various ways
because a given greedy algorithm may work for more than one cost function. The
choice of how the candidates are generated and how the cost function is deﬁned can
have a signiﬁcant effect on the ease with which the correctness of the algorithm is
proved. Finally, the generation and selection functions are combined, or fused, into
one function. When candidates is deﬁned as an instance of a standard higher-order
function such as foldr we can appeal to standard fusion conditions to carry out
the fusion step. We have already seen this two-stage process with sorting, when
algorithms were described in terms of building a tree and then ﬂattening it. The
deep structure of an algorithm is revealed by expressing it in terms of more basic
components and then combining them.

Chapter 7
Greedy algorithms on lists
This chapter deals with three problems in which the candidates are lists. The
problems are drawn from three different areas of computing, and appear to have
nothing in common. Nevertheless, they can all be solved by an appropriate greedy
algorithm and the method for obtaining the greedy algorithm is the same in all three
cases. The problems will repay careful study, so we will try to take things slowly.
To set the scene, we begin with an abstract formulation of the essential ingredients
behind a successful greedy algorithm.
7.1 A generic greedy algorithm
The following function mcc selects a candidate with minimum cost:
mcc::[Component] →Candidate
mcc = minWith cost ·candidates
This function is deﬁned as the composition of a function candidates that builds a
ﬁnite list of candidates out of a list of components, and a function minWith cost that
selects a candidate with minimum cost. The function minWith can be deﬁned in the
following way (alternatives are discussed in the exercises):
minWith::Ord b ⇒(a →b) →[a] →a
minWith f = foldr1 (smaller f)
where smaller f x y = if f x ⩽f y then x else y
The function foldr1 was introduced in the previous chapter. Since foldr1 returns
the undeﬁned value when applied to an empty list, so does minWith. Thus minWith
returns a well-deﬁned value only when applied to ﬁnite nonempty lists. If there is
more than one candidate with minimum cost, then the above deﬁnition selects the
ﬁrst such candidate on the list. Changing smaller to read
smaller f x y = if f x<f y then x else y

146
Greedy algorithms on lists
would mean that the last candidate with minimum cost is selected. Consequently,
the result returned by minWith depends on the precise order in which candidates are
generated. As will be appreciated by the end of the chapter, this fact will seriously
restrict our ability to reason equationally about greedy algorithms. The function
candidates takes a ﬁnite list of components, whatever they may be, and returns
a ﬁnite nonempty list of candidates. Candidate construction can be achieved in a
number of ways, but for the moment we will focus on one that uses foldr:
candidates::[Component] →[Candidate]
candidates xs = foldr step [c0] xs
where step x cs = concatMap (extend x) cs
Here c0 is some default partial candidate for an empty list of components. We could
have written
candidates = foldr (concatMap·extend) [c0]
but step is certainly a shorter name. The type of extend is
extend ::Component →Candidate →[Candidate]
This function takes a component and a candidate and returns a ﬁnite list of extended
candidates. The fully formed candidates are those constructed when all the compo-
nents have been processed. If the candidates were, say, the permutations of a list,
then c0 would be the empty list and extend x would be a list of all the ways x can be
inserted into a given permutation. For example,
extend 1 [2,4,3] = [[1,2,4,3],[2,1,4,3],[2,4,1,3],[2,4,3,1]]
It is assumed in what follows that extend x returns a nonempty ﬁnite list of candidates
for all x.
A greedy algorithm for computing mcc arises as the result of successfully fus-
ing minWith cost with candidates. Operationally speaking, instead of building the
complete list of candidates and then selecting a best one, we construct a single best
candidate at each step. We met the fusion rule for foldr in Chapter 1, but here it is
again: we have
h (foldr f e xs) = foldr g e′ xs
for all ﬁnite lists xs, provided e′ = h e and the fusion condition
h (f x y) = g x (h y)
holds for all x and y. For our problem, h = minWith cost and f = step, but g is
unknown. The fusion condition reads
minWith cost (step x cs) = gstep x (minWith cost cs)
for some function gstep (a ‘greedy step’). To see if it holds, and to discover gstep in
the process, we can reason as follows:

7.2 Greedy sorting algorithms
147
minWith cost (step x cs)
=
{ deﬁnition of step }
minWith cost (concatMap (extend x) cs)
=
{ distributive law (see below) }
minWith cost (map (minWith cost ·extend x) cs)
=
{ deﬁne gstep x = minWith cost ·extend x }
minWith cost (map (gstep x) cs)
=
{ greedy condition (see below) }
gstep x (minWith cost cs)
The distributive law used in the second step is the fact that
minWith f (concat xss) = minWith f (map (minWith f) xss)
provided xss is a ﬁnite list of ﬁnite nonempty lists. Equivalently,
minWith f (concatMap g xs) = minWith f (map (minWith f ·g) xs)
provided xs is a ﬁnite list and g returns ﬁnite nonempty lists. The proof of the
distributivity law is left as Exercise 7.3.
Summarising this short calculation, we have shown that
mcc = foldr gstep c0 where gstep x = minWith cost ·extend x
provided the following greedy condition holds:
minWith cost (map (gstep x) cs) = gstep x (minWith cost cs)
That all seems simple enough, so let’s look at some concrete examples.
7.2 Greedy sorting algorithms
Here is one speciﬁcation of the function sort that sorts a list into ascending order:
sort ::Ord a ⇒[a] →[a]
sort = minWith ic·perms
The function ic::Ord a ⇒[a] →Int, short for ‘inversion count’, counts the number
of inversions in a list. The notion of an inversion is one of the ﬁrst concepts that
arise in the study of the combinatorial properties of permutations. An inversion is a
pair of elements that are out of place, so (x,y) is an inversion if x appears before
y in the list but x>y. For example, ic [7,1,2,3] = 3 and ic [3,2,1,7] = 3. We can
deﬁne ic by
ic::Ord a ⇒[a] →Int
ic xs = length [(x,y) | (x,y) ←pairs xs,x>y]
pairs::[a] →[(a,a)]
pairs xs = [(x,y) | x:ys ←tails xs,y:zs ←tails ys]

148
Greedy algorithms on lists
The function pairs returns a list of all pairs of elements in a list in the order they
appear in the list, and ic counts the number of pairs for which the ﬁrst component
is greater than the second. A list with minimum inversion count has count 0 and
is a list in ascending order. Two distinct permutations cannot both be in ascending
order, so there is only one permutation of a list that minimises ic, namely the sorted
permutation.
The function perms can be deﬁned in various ways, including by a divide-and-
conquer algorithm – see the exercises. Here is the ﬁrst method used in Chapter 1:
perms::[a] →[[a]]
perms = foldr (concatMap·extend) [[ ]]
extend ::a →[a] →[[a]]
extend x [ ]
= [[x]]
extend x (y:xs) = (x:y:xs):map (y:) (extend x xs)
The function extend inserts a new element into a list in all possible positions. In
particular, the function gstep, where
gstep x = minWith ic·extend x
inserts a new element into a list so as to minimise the inversion count of the result.
For example,
gstep 6 [7,1,2,3] = [7,1,2,3,6]
gstep 6 [3,2,1,7] = [3,2,1,6,7]
The ﬁrst list has inversion count 4, while the second has inversion count 3. The
greedy condition for sort is the assertion
minWith ic (map (gstep x) xss) = gstep x (minWith ic xss)
for all x and xss. However, this assertion is false. Take xss = [[7,1,2,3],[3,2,1,7]].
We have
minWith ic xss = [7,1,2,3]
because both lists have inversion count 3 and minWith returns the ﬁrst list with the
smallest inversion count. Hence
gstep 6 (minWith ic xss) = [7,1,2,3,6]
with inversion count 4, while
minWith ic (map (gstep 6) xss) = [3,2,1,6,7]
with inversion count 3. The greedy condition therefore fails. Of course, the greedy
condition does hold if we swap the order of the two lists in xss, but what if xss were
a longer list of permutations? It is not clear that we can always reorder a list of
candidates to ensure that the greedy condition holds. In any case, such a step is
the wrong route to take, because candidate generation should be an independent

7.2 Greedy sorting algorithms
149
activity from ﬁnding one with minimum cost. We therefore appear to be well and
truly stuck.
There are three possible solvents to free us from this sticky situation. Two of
them will be described now, but the third and most important one will be left to the
end of the chapter.
The ﬁrst way of freeing ourselves is to use context-sensitive fusion. We discussed
this in Exercise 1.17, but here is the essential point again. Although the fusion
condition
h (f x y) = g x (h y)
for all x and y is sufﬁcient to establish the fusion rule for foldr, it is not a necessary
one. All that does have to be shown is that the fusion condition holds for all x and
all y of the form y = foldr f e xs. This version of the fusion condition is called
context-sensitive fusion. That means, in the case of sorting, that all we have in fact
to show is the context-sensitive fusion condition
minWith ic (map (gstep x) (perms xs)) = gstep x (minWith ic (perms xs))
for all x and xs. Luckily, this condition does hold. The proof follows from the fact
that there is a unique permutation that minimises ic, namely the ordered permutation
with inversion count 0, and that
ic xs = 0 ⇒ic (gstep x xs) = 0
Sometimes context-sensitive fusion is not enough to establish the greedy condition.
This happens when there is no unique candidate with minimum cost. The second
way of becoming unstuck is to change the cost function. Suppose you are climbing
some hills and want to reach a highest point, of which there maybe more than one.
At each step you can take the steepest ascending path, a strategy that may or may not
work. Nevertheless, it may also be the case that there is a unique point in the climb
with the best view, and that point is also a highest one. An alternative strategy at
each step is to take the path that best improves the view. The proof that this strategy
works may go through when the ﬁrst one does not. We will see many examples of
this trick in the following chapters.
In the case of sorting there is a simple alternative to ic, in fact an alternative that
can be accomplished at a stroke! It is to replace ic with id, the identity function. We
did not claim that the cost function had to return a single numerical value. We have
minWith id = minimum, so sort = minimum·perms. In words, the sorted permuta-
tion is the lexically least permutation. The context-sensitive greedy condition in this
case reads
minimum (map (gstep x) (perms xs)) = gstep x (minimum (perms xs))
where gstep x xs = minimum (extend x xs). As before, the greedy condition holds
because the sorted permutation is the unique permutation that minimises id, and

150
Greedy algorithms on lists
sorted xs ⇒sorted (gstep x xs)
When xs is a sorted list we can deﬁne gstep x xs by
gstep x [ ]
= [x]
gstep x (y:xs) = if x ⩽y then x:y:xs else y:gstep x xs
The result, namely sort = foldr gstep [ ], is a simple sorting algorithm usually known
as Insertion sort.
Insertion sort is not the only greedy sorting algorithm. Here is the other deﬁnition
of perms from Chapter 1:
perms [ ] = [[ ]]
perms xs = concatMap subperms (picks xs)
where subperms (x,ys) = map (x:) (perms ys)
picks [ ]
= [ ]
picks (x:xs) = (x,xs):[(y,x:ys) | (y,ys) ←picks xs]
The function picks picks an arbitrary element from a list in all possible ways,
returning both the element and what remains. This version of perms is deﬁned
recursively rather than through the use of foldr. Nevertheless, it is straightforward
to fuse minimum and perms. Firstly, we have
minimum (perms [ ]) = minimum [[ ]] = [ ]
Secondly, for nonempty xs we reason as follows:
minimum (perms xs)
=
{ above deﬁnition of perms }
minimum (concatMap subperms (picks xs))
=
{ distributive law }
minimum (map (minimum·subperms) (picks xs))
=
{ claim: see below }
minimum (subperms (minimum (picks xs)))
=
{ suppose (x,ys) = minimum (picks xs) }
minimum (subperms (x,ys))
=
{ deﬁnition of subperms }
minimum (map (x:) (perms ys))
=
{ since minimum·map (x:) = (x:)·minimum on nonemtpy lists }
x:minimum (perms ys)
The claim takes the form
minimum·map f = f ·minimum
where f = minimum·subperms. It is left as an exercise to show that the claim holds
if f is a monotonic function, that is, x ⩽y ⇒f x ⩽f y. To verify the claim for

7.3 Coin-changing
151
minimum·subperms, suppose (x1,ys1) and (x2,ys2) are two picks of the same list.
It is easy to check that
(x1,ys1) ⩽(x2,ys2) ⇒x1 :sort ys1 ⩽x2 :sort ys2
But, as we have seen, minimum (subperms (x,ys)) = x:sort ys so the claim follows.
We have therefore shown
sort [ ] = [ ]
sort xs = x:sort ys where (x,ys) = pick xs
where pick xs = minimum (picks xs). The function pick takes quadratic time, but it
can be implemented to take linear time, see Exercise 7.10. The result is another
well-known algorithm for sorting called Selection sort. Both Insertion sort and
Selection sort take quadratic time in the worst case, so they are not fast. But they
are simple.
7.3 Coin-changing
Our second problem is about giving change in coins. Suppose you were a cashier in
a supermarket and had to give 2.56 in change to a customer. How would you do it?
Pause for a moment to answer this question.
We cannot answer this question for you because we do not know your nationality
and the currency of your country (though we have assumed in the statement of the
question that it is a decimal currency). The denominations of the available coins
have to be known. In the United States the denominations are a penny (1c), a nickel
(5c), a dime (10c), a quarter (25c), a half-dollar (50c), and a dollar ($1). In the UK
they are 1p, 2p, 5p, 10p, 20p, 50p, £1, and £2. (Even 50 years after decimalisation,
there are no nicknames for UK coins.) Pre-decimalisation, the UK coinage system
was a very odd one, consisting of a halfpenny (0.5d), a penny (1d), a threepence
(3d), a sixpence (6d), a shilling (12d), a ﬂorin (24d), and a half-crown (30d). The
British coped with this system somehow, but fortunately it is now consigned to
history. Note that, whatever the system, there has to be a coin that allows change of
one unit of currency, be it a penny, a halfpenny, or a cent.1
We also cannot answer the question until you say what criterion you are adopting
for giving the change. Are you trying to minimise the number of coins in the change
or maybe the total weight? Although some people delight in carrying around lots
of loose change, the coins can weigh a lot in the pocket or handbag, so maybe
minimum weight should be the criterion to go for. The weights in grams of the UK
and US coins are given in the following table:
1 That is not strictly true. For example the smallest coin in Australia is 5c but one can buy items for, say, $9.99.
When paying by cash with a $10 note, the cashier rounds the amount down to $9.95 and gives 5c change.

152
Greedy algorithms on lists
1
2
5
10
20
25
50
100
200
UK
3.56
7.12
3.25
6.5
5.0
–
8.0
9.5
12.0
US
2.5
–
5.0
2.27
–
5.67
11.54
8.1
–
Inspection shows that for UK currency each coin of a given denomination weighs
no more than the value of the coin in smaller denominations. But that is not quite
enough to prove that minimising the number of coins also minimises the total weight.
In US currency two quarters weigh 11.34 grams, which is less than the weight of
a half-dollar (11.54 grams). And a quarter and a nickel together weigh 10.67 grams,
while three dimes weigh only 6.81 grams. Certainly in the United States minimising
the number of coins does not minimise their total weight. The statement is true for
UK currency, as one can check by exhaustive search (see the exercises).
There is an obvious greedy algorithm for minimising the number of coins: at
each step give the customer a coin of largest value as long as it is no larger than
the remaining amount. Cashiers regularly adopt such a strategy the world over. For
$2.56 that would mean ﬁve coins:
2×$1+1×50c+1×5c+1×1c
For £2.56 that would mean four coins:
1×£2+1×50p+1×5p+1×1p
Does the greedy algorithm work? The answer is no, not necessarily: it depends
on the coinage system. Before 1971 when decimalisation occurred in the UK, a
greedy algorithm for giving change of 48d would use three coins, a half-crown, a
shilling and a sixpence, whereas two ﬂorins would sufﬁce. Another reason to be
grateful for decimalisation. As a simpler example, with denominations [4,3,1] the
greedy algorithm would give three coins for a change of 6 units, while two coins of
denomination 3 would sufﬁce.
To specify the problem, suppose we are given a list of denominations in decreasing
order, ending with a denomination of 1. For example,
type Denom = Nat
type Tuple = [Nat]
usds,ukds::[Denom]
usds = [100,50,25,10,5,1]
ukds = [200,100,50,20,10,5,2,1]
By deﬁnition, a tuple is a list of natural numbers, of the same length as the list of
denominations, representing the given change (we prefer ‘tuple’ to ‘change’ simply
because ‘tuples’ reads better than ‘changes’). For example, [2,1,0,0,1,1] represents
$2.56 in US currency. The amount a tuple represents is given by
amount ::[Denom] →Tuple →Nat
amount ds cs = sum (zipWith (×) ds cs)

7.3 Coin-changing
153
The number of coins in a tuple, its count, is deﬁned by count = sum. We can now
deﬁne
mkchange::[Denom] →Nat →Tuple
mkchange ds = minWith count ·mktuples ds
The function mktuples gives all possible ways of making change for a given amount
with the given denominations. One simple deﬁnition is
mktuples::[Denom] →Nat →[Tuple]
mktuples [1] n = [[n]]
mktuples (d :ds) n = [c:cs | c ←[0..n div d],cs ←mktuples ds (n−c×d)]
By assumption the last coin has denomination 1, so to make up change of n using
just this coin we have to use n coins. Otherwise, for the next denomination d any
number c in the range 0 ⩽c ⩽⌊n/d⌋can be chosen. The rest of the computation is
a recursive call with the remaining denominations and the remaining amount n−cd.
Another reasonable deﬁnition of mktuples based on foldr is given as Exercise 7.14.
The function mktuples can return a long list of candidate tuples. For example,
length (mktuples usds 256) = 6620
length (mktuples ukds 256) = 223195
Hence computation with the above deﬁnition of mkchange is quite slow.
There is another important feature of mkchange to take into account: unlike the
case of sorting there may be more than one tuple with minimum count. For example,
take the denominations [7,3,1]. Then both [6,4,0] and [7,1,2] are tuples for 54
units of change with minimum count 10. The deﬁnition of minWith given in the
previous section chooses the ﬁrst tuple with minimum count in the list of candidates.
That means the above deﬁnition of mkchange returns [6,4,0] (why?) but the greedy
algorithm, as outlined in the preamble, chooses the second tuple [7,1,2]. These
results are different and again we seem to be stuck. One can resolve this difﬁculty
by modifying the deﬁnition of mktuples to produce the tuples in a different order.
But two wrongs do not make a right and this is not the path to take. One alternative,
as we have seen, is to change the cost function.
This time we replace minWith count by maxWith id. Thus we deﬁne
mkchange ds = maximum·mktuples ds
since maxWith id = maximum. Instead of choosing a tuple with minimum count,
mkchange chooses the lexically largest tuple. Whether or not the largest tuple is
also one with minimum count depends on the denominations of the coins. We return
to this essential point in a short while. Note that, while there may be more than one
tuple with minimum count, there is always a unique largest tuple.
So, let us calculate it. The base case
mkchange [1] n = [n]

154
Greedy algorithms on lists
is immediate. For the induction step we ﬁrst rewrite the deﬁnition of mktuples to
avoid an explicit list comprehension:
mktuples [1] n = [[n]]
mktuples (d :ds) n = concatMap (extend ds) [0..n div d]
where extend ds c = map (c:) (mktuples ds (n−c×d))
The translation is straightforward and details are omitted. The advantage of higher-
order functions such as concatMap over list comprehensions is that the rules of the
game can be stated more simply. In particular,
maximum (concatMap f xs) = maximum (map (maximum·f) xs)
maximum (map (x:) xs)
= x:maximum xs
for all ﬁnite nonempty lists. The ﬁrst law is an instance of the distributive law of the
previous section; as before it is valid only if f returns a ﬁnite nonempty list. That is
not a problem here because extend does return such a list. The second is not valid
if xs is the empty list (why?), but that is also not a problem here because mktuples
returns a nonempty list.
We now reason as follows:
mkchange (d :ds) n
=
{ deﬁnition }
maximum (mktuples (d :ds) n)
=
{ deﬁnition of mktuples with m = n div d }
maximum (concatMap (extend ds) [0..m])
=
{ ﬁrst law above }
maximum (map (maximum·extend ds) [0..m])
We continue with the inner term:
maximum (extend ds c)
=
{ deﬁnition of extend }
maximum (map (c:) (mktuples ds (n−c×d)))
=
{ second law above }
c:maximum (mktuples ds (n−c×d))
=
{ deﬁnition of mkchange }
c:mkchange ds (n−c×d)
Hence
maximum (map (maximum·extend ds) [0..m])
=
{ above }
maximum [c:mkchange ds (n−c×d) | c ←[0..m]]
=
{ deﬁnition of lexicographic maximum }
m:mkchange ds (n−m×d)

7.3 Coin-changing
155
That gives the greedy algorithm:
mkchange::[Denom] →Nat →Tuple
mkchange [1] n
= [n]
mkchange (d :ds) n = c:mkchange ds (n−c×d) where c = n div d
At each step the maximum number of coins of the next denomination is chosen. For
example,
mkchange ukds 256
= [1,0,1,0,0,1,0,1]
mkchange usds 256
= [2,1,0,0,1,1]
mkchange [7,3,1] 54 = [7,1,2]
All of the calculation above is valid for the earlier deﬁnition of mkchange in terms
of minimising count, except for the very last step.
Finally, but crucially, we revisit the question of when mkchange does produce a
tuple with minimum count. Equivalently, when is the lexically largest tuple also the
one with minimum count?
Let us prove this is the case for UK currency (the case of US currency is slightly
simpler and left as an exercise). Let [c8,c7,...,c1] be a tuple with minimum count
and [g8,g7,...,g1] be the tuple returned by the greedy algorithm, namely the lexically
largest tuple. The aim is to show that cj = gj for 1 ⩽j ⩽8, so the largest tuple for
UK currency is the unique tuple with minimum count. This is not necessarily true
for other currencies for which the greedy algorithm works. The amount A in the
change satisﬁes
A = 200c8 +100c7 +50c6 +20c5 +10c4 +5c3 +2c2 +c1
A = 200g8 +100g7 +50g6 +20g5 +10g4 +5g3 +2g2 +g1
We ﬁrst show that c1 = g1 and c2 = g2. First of all, 0 ⩽g1 < 2 for otherwise we
could increase g2 to obtain a lexically larger tuple. Also 0 ⩽c1 <2 for otherwise
we could increase c2 to obtain a larger tuple with a smaller count. The next step is
to prove that 0 ⩽2c2 +c1 <5 and 0 ⩽2g2 +g1 <5. This is done by showing that,
if 2c2 +c1 ⩾5, then there is a larger tuple for the same amount with the same or a
smaller count. Details are given in Exercise 7.15. The proof of the second inequality
is the same. As a result we have
A mod 5 = 2c2 +c1 = 2g2 +g1
and so 2(c2 −g2) = g1 −c1 <2. Hence c1 = g1 and c2 = g2. Next, setting
B = (A−(2c2 +c1))/5
we have
B = 40c8 +20c7 +10c6 +4c5 +2c4 +c3
B = 40g8 +20g7 +10g6 +4g5 +2g4 +g3

156
Greedy algorithms on lists
Now 0 ⩽g3 <2, for otherwise we could increase g4 to obtain a larger tuple. And
0 ⩽c3 <2, for otherwise there would be a tuple with smaller count. Hence
B mod 2 = c3 = g3
For the next step, set C = (B−c3)/2. Then
C = 20c8 +10c7 +5c6 +2c5 +c4
C = 20g8 +10g7 +5g6 +2g5 +g4
The same reasoning as in the ﬁrst step shows that c4 = g4 and c5 = g5. For the next
step, set D = (C −(2c5 +c4))/5, so
D = 4c8 +2c7 +c6
D = 4g8 +2g7 +g6
The same argument as in the second step shows c6 = g6. Setting E = (D−c6)/2
we have
E = 2c8 +c7
E = 2g8 +g7
and now we can repeat the argument in the ﬁrst step once again to show that c7 = g7
and c8 = g8.
There is no shortcut to this rather lengthy reasoning about currency; each denomi-
nation has to be dealt with separately. After all, the argument might break down only
with larger denominations. Essentially the same argument works for US currency.
It also works for denominations that are successive powers of some base or, more
generally, when each denomination is a multiple of the next lower denomination.
But there appears to be no simple characterisation of when it works in general.
7.4 Decimal fractions in TEX
Our third problem involving lists of numbers has to do with Knuth’s typesetting
system TEX, the system used to typeset this book. The source language of TEX is
decimal. For instance, one can use \hspace{0.2134156in} to get a space of that
width: |
|. But internally TEX uses integer arithmetic with all fractions expressed
as an integer multiple of 1/216 = 1/65536. For example, 0.2134156 is represented
by the integer 13986, as is the shorter fraction 0.21341. There is therefore the
problem of converting a decimal fraction to its closest internal representation and,
conversely, converting an internal representation to its shortest decimal fraction. In
either direction only limited-precision integer arithmetic is allowed, the arithmetic
of Int. The ﬁrst direction is easy but the other one involves a greedy algorithm.
Let us consider the external-to-internal problem ﬁrst. With Digit as a synonym
for Int restricted to digits d in the range 0 ⩽d <10, a decimal fraction representing

7.4 Decimal fractions in TEX
157
a real number r in the range 0 ⩽r <1 can be converted into a ﬂoating-point number
(see Exercise 1.11) by
fraction::[Digit] →Double
fraction = foldr shiftr 0
shiftr ::Digit →Double →Double
shiftr d r = (fromIntegral d +r)/10
For example, 0.d1d2d3 is converted into the real number
(d1 +(d2 +(d3 +0)/10)/10)/10 = d1
10 + d2
100 + d3
1000
The conversion function fromIntegral is needed in Haskell to convert an integer
(here a digit) into a ﬂoating-point number before it can be added to another ﬂoating-
point number. Such conversion functions obscure the arithmetic and from now on
we will silently ignore them in arithmetic reasoning.
The function scale converts the result r into the nearest multiple of 2−16, namely
⌊216 r +1/2⌋=
	217 r +1
2

Hence, since 217 = 131072, we can deﬁne
scale::Double →Int
scale r = ⌊(131072×r +1)/2⌋
The external-to-internal TEX problem is now speciﬁed by
intern::[Digit] →Int
intern = scale·fraction
Well and good, but intern uses fractional arithmetic to compute the result and the
requirement was to use only limited-precision integer arithmetic. So there is still a
problem to overcome.
The solution is to try to fuse scale and fraction into one function using the fusion
law of foldr. But this turns out not to be possible (see Exercise 7.19). The best we
can do is to decompose scale into two functions and fuse just one of them with
fraction. We have for all integers a and b, with b>0, and real x that
	x+a
b

=
	⌊x⌋+a
b

(7.1)
The proof, which uses the rule of ﬂoors (see Exercise 4.1), is left as another exercise.
In particular, taking x = 131072r, a = 1, and b = 2, it follows that
scale = halve·convert
where
halve n
= (n+1) div 2
convert r = ⌊131072×r⌋

158
Greedy algorithms on lists
It is possible to fuse convert and fraction. We have
convert ·foldr shiftr 0 = foldr shiftn 0
provided we can ﬁnd a function shiftn to satisfy the fusion condition
convert (shiftr d r) = shiftn d (convert r)
To discover shiftn we reason
convert (shiftr d r)
=
{ deﬁnitions }
⌊131072×(d +r)/10⌋
=
{ (7.1) }
(131072×d +⌊131072×r⌋) div 10
=
{ deﬁnition of convert }
(131072×d +convert r) div 10
Hence we can deﬁne
shiftn d n = (131072×d +n) div 10
We have shown that
intern::[Digit] →Int
intern = halve·foldr shiftn 0
That solves the external-to-internal problem. The largest integer that can arise during
this computation is at most 1310720, so Int arithmetic is sufﬁcient. Notice that we
have nowhere exploited any property of 131072 except that it was a positive integer.
But for 217 the algorithm can be optimised: except for the ﬁrst 17 digits, all the
other digits of the fraction can be discarded because they cannot affect the answer.
The proof is left as Exercise 7.20.
The other direction is to ﬁnd for a given n in the range 0 ⩽n<216 some shortest
decimal fraction whose internal representation is n. Again, only limited-precision
integer arithmetic is allowed. We know by now how to set up the problem:
extern::Int →[Digit]
extern n = minWith length (externs n)
where n is restricted to the range 0 ⩽n<216. Ideally, the function externs n should
return a list of all ﬁnite decimals whose internal value is n. The problem is that this is
an inﬁnite list, so any execution of extern would fail to terminate. For example, the
17-digit fraction 0.01525115966796875 and the 5-digit fraction 0.01526 both have
internal value 1000, and so does any fraction between these bounds. Sometimes, as
here, the set of candidates is inﬁnite, and selecting a best one, though expressible
mathematically, cannot be formulated as an executable expression.
One solution, as we have seen above, is to generate only decimals of length
at most 17. In fact, it is sufﬁcient to generate decimals of length at most 5 (see

7.4 Decimal fractions in TEX
159
Exercise 7.22). Indeed, in the ﬁrst implementation of TEX a decimal of exactly
length ﬁve was always chosen. But this choice proved unsatisfactory (a user who
asked for a 0.4-point rule was told that TEX had actually typeset a 0.39999-point
rule), so Knuth implemented a greedy algorithm.
Instead we will look at another way to generate a ﬁnite list of possible decimals,
one guaranteed to include all the shortest decimals. This method will lead to the
greedy algorithm. To determine the list, observe that a decimal ds is an element of
externs n if and only if scale (fraction ds) = n. Abbreviating 131072 to w in what
follows, we have
scale r = n ⇔2n−1 ⩽wr <2n+1
since scale r = ⌊(wr + 1)/2⌋. That suggests generalising externs to a function,
decimals say, that takes an interval as argument:
externs n = decimals (2n−1,2n+1)
where, provided a<b and b>0, the value of decimals (a,b) is any list of decimals
ds satisfying
a ⩽w×fraction ds<b
as long as it includes all the shortest decimals satisfying the constraint. To arrive at
a deﬁnition of decimals, observe ﬁrst that
a ⩽w×fraction [ ]<b ⇔a ⩽0<b
so we can set decimals (a,b) = [[ ]] if a ⩽0. Secondly, we have
a ⩽w×fraction (d :ds)<b
⇔
{ deﬁnition of fraction }
a ⩽w×shiftr d (fraction ds)<b
⇔
{ deﬁnition of shiftr, writing r = fraction ds }
a ⩽w(d +r)/10<b
⇔
{ arithmetic }
10a−wd ⩽wr <10b−wd
⇔
{ since 0 ⩽r <1 (and P ⇔P ∧Q if P ⇒Q) }
(10a/w−1<d <10b/w) ∧(10a−wd ⩽wr <10b−wd)
⇔
{ since d is an integer }
(⌊10a/w⌋⩽d ⩽⌊10b/w⌋) ∧(10a−wd ⩽wr <10b−wd)
⇔
{ since d is a digit }
(max 0 ⌊10a/w⌋⩽d ⩽min 9 ⌊10b/w⌋∧
(10a−wd ⩽wr <10b−wd)
Hence
a ⩽w×fraction (d :ds)<b
⇔
l ⩽d ⩽u ∧10a−wd ⩽fraction ds<10b−wd

160
Greedy algorithms on lists
where l = max 0 ⌊10a/w⌋and u = min 9 ⌊10b/w⌋. That suggests the following
deﬁnition of decimals:
decimals::(Int,Int) →[[Digit]]
decimals (a,b) =
if a ⩽0 then [[ ]]
else [d :ds | d ←[l..u],ds ←decimals (10×a−w×d,10×b−w×d)]
where w = 131072
l = 0 max ((10×a) div w)
u = 9 min ((10×b) div w)
Given this deﬁnition, we claim that externs n returns a list of all decimals ds such
that intern ds = n but intern ds′ < n for any proper preﬁx ds′ of ds. For the proof,
observe that the successive intervals generated by decimals (a,b) have lower bounds
a, 10a−wd1, 10(10a−wd1)−wd2,...
The kth term of this sequence is
10k a−w(10k−1 d1 +···+100 dk) = 10k (a−w×fraction ds)
Hence decimals (a,b) produces a list of the shortest ds such that a ⩽wr, where
r = fraction ds. Furthermore, 2n−1 ⩽wr if and only if n ⩽scale r, so externs n
produces decimals ds that scale to n but no preﬁx of ds does.
However, the above deﬁnition of decimals contains a bug that we have encoun-
tered before in the deﬁnition of binary search. The problem is that the numbers can
get quite large and the arithmetic of Int is not up to the job. Instead we have to move
over to Integer arithmetic and deﬁne decimals as a function with type
decimals::(Integer,Integer) →[[Digit]]
The reason for the bug and the necessary revisions of decimals and externs are left
as an exercise.
Now that we have ensured that externs returns a ﬁnite list, we can return to
consideration of extern. As in the coin-changing problem, there may be more than
one shortest fraction with the same internal representation. For example, both
0.05273 and 0.05274 are shortest fractions whose internal representation is 3456.
The above deﬁnition of extern returns the ﬁrst fraction while, as we will see, the
greedy algorithm returns the second. Once again the solution is to change the cost
function.
The revised deﬁnition of extern should cause no surprise:
extern = maximum·externs
As with coin-changing, we switch to selecting the lexically largest decimal fraction.
The proof that the largest fraction returned by externs is a shortest fraction is given
later on.

7.5 Nondeterministic functions and reﬁnement
161
We will omit the calculation that gives the following greedy algorithm:
extern::Int →[Digit]
extern n = decimal (2×n−1,2×n+1)
where
decimal::(Int,Int) →[Digit]
decimal (a,b) = if a ⩽0 then [ ]
else d :decimal (10×a−w×d,10×b−w×d)
where d = (10×b) div w
w = 131072
Note ﬁrst that Integer arithmetic has been replaced by Int arithmetic again. We
claim that b<w for all calls of decimal (a,b). With n<216 we have
2n+1 ⩽2(216 −1)+1 = 217 −1<217 = w
so the claim holds for the initial call. Furthermore
10b−w⌊10b/w⌋= 10b mod w<w
so the claim holds for recursive calls. With b<w we have 0 ⩽⌊10b/w⌋<10, so d
is always a valid digit.
It remains to show that the largest decimal fraction is also a shortest one. We do
this by showing that if ds1 and ds2 are two different decimals in decimals (a,b), then
ds1 <ds2 ⇒length ds2 ⩽length ds1. We saw above that, if decimals (a,b) produces
a decimal ds, then it cannot also produce a proper preﬁx of ds. Hence ds1 cannot be
a preﬁx of ds2. Now, by deﬁnition of lexical order, we have ds1 = us++d1 :vs1 and
ds2 = us++d2 :vs2, where d1 <d2. Let k be the length of us and n be the decimal
integer formed from the digits in us. It is easy to show that both d1 :vs1 and d2 :vs2
are in
decimals (10k ×a−131072×n,10k ×b−131072×n)
But d1 <d2, and that means [d2] is also in this list. And since [d2] and d2 :vs2 cannot
both be in the list unless vs2 is the empty list, we conclude that ds2 = us ++ [d2],
which is no longer than ds1.
7.5 Nondeterministic functions and reﬁnement
All three problems in this chapter have been successfully dealt with by changing
the cost function into another one that guarantees a linear order, so minimum and
maximum elements are unique. However, this device is not always possible. In
general, in order to establish a (context-sensitive) greedy condition of the form
gstep x (minWith cost (candidates xs))
= minWith cost (map (gstep x) (candidates x))

162
Greedy algorithms on lists
when there may be more than one candidate with minimum cost, we have to prove
the very strong property
cost c ⩽cost c′ ⇔cost (gstep x c) ⩽cost (gstep x c′)
(7.2)
for all candidates c and c′. To see why, observe that, if c is the ﬁrst candidate returned
by candidates with minimum cost, then gstep x c has to be the ﬁrst candidate with
minimum cost in the list of extended candidates. This follows from our deﬁnition of
minWith, which selects the ﬁrst element with minimum cost in a list of candidates.
To ensure that the extension of a candidate c′ earlier in the list has a larger cost, we
have to show that
cost c′ >cost c ⇒cost (gstep x c′)>cost (gstep x c)
(7.3)
for all candidates c and c′. To ensure that the extension of a candidate c′ later in the
list does not have a smaller cost, we have to show that
cost c ⩽cost c′ ⇒cost (gstep x c) ⩽cost (gstep x c′)
(7.4)
for all c and c′. The conjunction of (7.3) and (7.4) is (7.2).
The problem is that (7.2) is so strong that it rarely holds in practice. A similar
condition is needed if, say, minWith returned the last element in a list with minimum
cost. What we really need is a form of reasoning that allows us to establish the
necessary fusion condition from the simple monotonicity condition (7.4) alone,
and the plain fact of the matter is that equational reasoning with any deﬁnition of
minWith is simply not adequate to provide it.
It follows that we have to abandon equational reasoning, at least for a function
like minWith. One general approach is to replace our functional framework with a
relational one, and to reason instead about the inclusion of one relation in another.
But for our purposes this solution is way too drastic, more akin to a heart transplant
than a tube of solvent for occasional use. The alternative, if it can be made to
work smoothly, is to introduce a nondeterministic variant of minWith and to reason
about the reﬁnement of one expression by another instead of the equality of two
expressions.
Suppose we introduce MinWith cost as a nondeterministic function, speciﬁed by
the assertion that x is a possible value of MinWith cost xs precisely when xs is a
ﬁnite nonempty list of well-deﬁned values, x is an element of xs, and cost x ⩽cost y
for all elements y of xs. Note the initial capital letter: MinWith is not part of Haskell.
It is not our intention to extend Haskell with nondeterministic functions. Instead,
MinWith is simply there to extend our powers of speciﬁcation and will not appear
in any ﬁnal algorithm.
We will write x ←MinWith cost xs to mean that x is one possible element of
xs with minimum cost. The symbol ←is read as “is a reﬁnement of”. Think of
MinWith cost xs as the set of elements of xs with minimum cost and interpret ←

7.5 Nondeterministic functions and reﬁnement
163
as set membership. The situation is analogous to order notation, in which O(g(n))
is interpreted as a set of functions and the equality sign in f(n) = O(g(n)) as
set membership. For example, 1 ←MinWith cost [1,2] is a true assertion provided
cost 1 ⩽cost 2. On the other hand, neither MinWith cost [ ] nor MinWith cost [1,⊥,2]
is well-deﬁned.
More generally, if E1 and E2 are possibly nondeterministic expressions of the
same type T, then we will write E1 ←E2 to mean that
v ←E1 ⇒v ←E2
for all values v of type T. Thus the symbol ←in E1 ←E2 should be thought of as set
inclusion. The situation is analogous to an assertion such as 2n2 +O(n2) = O(n2)
in which the = sign really means set inclusion.
Next, suppose E and E1 are possibly nondeterministic expressions. Then we
interpret x ←E(E1) to mean that there exists a y such that y ←E1 and x ←E(y).
Consequently, we have
E1 ←E2 ⇒E(E1) ←E(E2)
Thus all expressions are monotonic under reﬁnement.
As an example, consider the greedy condition
gstep x (MinWith cost (candidates xs))
←MinWith cost (map (gstep x) (candidates xs))
First of all, this assertion is meaningful only if candidates xs is a ﬁnite nonempty
list of well-deﬁned values and gstep x returns well-deﬁned results on well-deﬁned
arguments. In such a case, the assertion
c ←gstep x (MinWith cost (candidates xs))
holds if c = gstep x c′ for some c′ such that c′ ←MinWith cost (candidates xs). The
greedy condition asserts that, for some candidate c′ in the list cs = candidates xs
with minimum cost, gstep x c′ is a candidate with minimum cost in map (gstep x) cs.
Unlike the previous version of the greedy condition, this assertion does follow from
the simple monotonicity condition (7.4). To spell out the details, suppose c′ is a
candidate in cs with minimum cost. We have only to show that
cost (gstep x c′) ⩽cost (gstep x c′′)
for all candidates c′′ in cs. But this follows at once from (7.4) and the assumption
cost c′ ⩽cost c′′.
Next, we deﬁne two nondeterministic expressions of the same type to be equal if
they both have the same set of reﬁnements. Thus
E1 = E2 ⇔E1 ←E2 ∧E2 ←E1
For example, consider the distributive law
MinWith cost (concat xss) = MinWith cost (map (MinWith cost) xss)

164
Greedy algorithms on lists
where xss is a ﬁnite nonempty list of ﬁnite nonempty lists. This is a law we deﬁnitely
want to hold. The equality sign here means that there is no reﬁnement of one side
that is not also a reﬁnement of the other side. We interpret the assertion
xs ←map (MinWith cost) xss
to mean that, if xss = [xs1,xs2,...,xsn] is a list of ﬁnite nonempty lists of well-
deﬁned values, then xs = [x1,x2,...,xn], where xj ←MinWith cost xsj. The proof
that the distributive law holds is left as Exercise 7.23.
What else do we want? Well, we certainly want a reﬁnement version of the fusion
law for foldr, namely that
foldr gstep c0 xs ←MinWith cost (foldr fstep [c0] xs)
for all ﬁnite lists xs provided
gstep x (MinWith cost ys) ←MinWith cost (fstep x ys)
for all x and all ys of the form ys = foldr fstep [c0] xs. Here is the proof of the fusion
law. The base case is immediate and the induction step is as follows:
foldr gstep c0 (x:xs)
=
{ deﬁnition of foldr }
gstep x (foldr gstep c0 xs)
←
{ induction and monotonicity of reﬁnement }
gstep x (MinWith cost (foldr fstep [c0] xs))
←
{ fusion condition }
MinWith cost (fstep x (foldr fstep [c0] xs))
=
{ deﬁnition of foldr }
MinWith (foldr fstep [c0] (x:xs))
Let us see what else we might need by redoing the calculation of the greedy
algorithm for mcc. This time we start with the speciﬁcation
mcc xs ←MinWith cost (candidates xs)
For the fusion condition we reason, with cs = candidates xs,
MinWith cost (fstep x cs)
=
{ with fstep = concatMap·extend }
MinWith cost (concatMap (extend x) cs)
=
{ distributive law }
MinWith cost (map (MinWith cost ·extend x) cs)
→
{ suppose gstep x xs ←MinWith cost (extend x xs) }
MinWith cost (map (gstep x) cs)
→
{ greedy condition }
gstep x (MinWith cost (candidates xs))
We write E1 →E2 as an alternative to E2 ←E1. The second step makes use of the

7.6 Summary
165
distributive law, and the third step makes use of the monotonicity of reﬁnement. As
we saw above, the greedy condition follows from (7.4).
We have introduced a single nondeterministic function MinWith cost, which
will be sufﬁcient for the following two chapters. In Part Four of the book, on
thinning algorithms, we will need another nondeterministic function, ThinBy, and
that function will be dealt with in the same way as MinWith, namely by simply
stating the valid rules of reasoning about reﬁnement.
7.6 Summary
Let us summarise the general points that emerge from this chapter:
1. A greedy algorithm arises from the successful fusion of a function that selects
a best candidate with a function that generates all candidates, or at least all
candidates that may turn out to be best ones.
2. The best candidate can sometimes be deﬁned in different ways. In hillwalking
terms, the highest point may also be the one with the best view and one can
choose to maximise the height or to maximise the view. In either case the result
is the same.
3. Sometimes the simple statement of the greedy condition is too strong because it
does not take context into account.
4. While it may be possible to prove that a context-sensitive fusion condition holds
in special cases, usually by changing the cost function, in general one may have
to replace reasoning with equality by reasoning with reﬁnement in order to prove
that a greedy algorithm works.
7.7 Chapter notes
Both Insertion sort and Selection sort are well-known sorting algorithms, though
they are not usually described as being greedy algorithms. Knuth starts his com-
prehensive text [6] with a study of inversions, and many interesting properties of
inversions can be found there.
The coin-changing problem has a long history. Recent references include [1, 5].
The TEX problem was ﬁrst discussed in [7], under the title “A simple program
whose proof isn’t”, and considered further in [3]. It is remarkable that both of these
problems succumbed to exactly the same calculation.
For further information about how to reason about nondeterminism in a functional
setting, see [4]. There are many articles about nondeterministic functions and
reﬁnement, and many ways of formalising these ideas. One way is to regard a
nondeterministic function as a relation, and reﬁnement as the inclusion of one

166
Greedy algorithms on lists
relation in another. The relational approach to programming, in a categorical setting,
is described in [2]; this book also contains the TEX problem as an example. Another
approach, which we have more or less followed above with minor syntactic changes,
is given in [8] and the earlier [9]. These two articles record the pitfalls one can
tumble into if sufﬁcient care is not taken.
References
[1]
Michal Adamaszek and Anna Niewiarowska. Combinatorics of the change-making
problem. European Journal of Combinatorics, 31(1):47–63, 2010.
[2]
Richard S. Bird and Oege de Moor. The Algebra of Programming. Prentice-Hall,
Hemel Hempstead, 1997.
[3]
Richard S. Bird. Two greedy algorithms. Journal of Functional Programming,
2(2):237–244, 1992.
[4]
Richard Bird and Florian Rabe. How to calculate with nondeterministic functions. In
G. Hutton, editor, Mathematics of Program Construction, volume 11825 of Lecture
Notes in Computer Science. Springer-Verlag, Cham, pages 138–154, 2019.
[5]
Xuan Cai. Canonical coin systems for change-making problems. In Hybrid Intelligent
Systems, IEEE, pages 499–504, 2009.
[6]
Donald E. Knuth. The Art of Computer Programming, volume 3: Sorting and
Searching. Addison-Wesley, Reading, MA, second edition, 1998.
[7]
Donald E. Knuth. A simple program whose proof isn’t. In W. H. J. Feijen, A. J.
M. van Gasteren, D. Gries, and J. Misra, editors, Beauty is Our Business: A Birthday
Salute to Edsger W. Dijkstra. Springer-Verlag, Berlin, 1990.
[8]
Joseph M. Morris and Malcolm Tyrrell. Dually nondeterministic functions. ACM
Transactions on Programming Languages and Systems, 30(6):34, 2008.
[9]
Joseph M. Morris and Alexander Bunkenburg. Speciﬁcational functions. ACM
Transactions on Programming Languages and Systems, 21(3):677–701, 1999.
Exercises
Exercise 7.1 The Data.List library provides a function
minimumBy::(a →a →Ordering) →[a] →a
Deﬁne minWith using minimumBy.
Exercise 7.2 Write down a deﬁnition of a function minsWith f that returns all
the elements of a ﬁnite nonempty list that minimise f. In particular the statement
x ←MinWith f xs can be read as x ∈minsWith f xs.
Exercise 7.3 Prove that, if f is associative, then
foldr1 f (xs++ys) = f (foldr1 f xs) (foldr1 f ys)
for all nonempty lists xs and ys. Hence show that
foldr1 f (concat xss) = foldr1 f (map (foldr1 f) xss)

Exercises
167
provided xss contains only nonempty lists. Finally, show that
minWith f (concat xss) = minWith f (map (minWith f) xss)
provided xss contains only nonempty lists.
Exercise 7.4 Write down a divide-and-conquer deﬁnition of perms.
Exercise 7.5 Why is the law
minimum·map (x:) = (x:)·minimum
not valid on empty lists?
Exercise 7.6 Show that minimum · map f = f · minimum on nonempty lists if f is
monotonic. Is the monotonicity condition necessary?
Exercise 7.7 Given that gstep x = minimum·extend x, derive a recursive deﬁnition
of gstep.
Exercise 7.8 With gstep as deﬁned in the previous question, show that
minimum (map (gstep x) xss) = gstep x (minimum xss)
provided all lists in xss have the same length. Give an example to show the condition
fails if xss can contain lists of different length.
Exercise 7.9 Suppose (x1,ys1) and (x2,ys2) are two picks of the same list. Show
that
(x1,ys1) ⩽(x2,ys2) ⇒x1 :sort ys1 ⩽x2 :sort ys2
Exercise 7.10 Write down a linear-time algorithm for computing pick.
Exercise 7.11 Consider evaluation of Insertion sort on the list [3,4,2,5,1]. Keep-
ing in mind that Haskell is a lazy language, continue the following sequence of
evaluation steps until the ﬁrst element of the result is obtained:
gstep 3 (gstep 4 (gstep 2 (gstep 5 (gstep 1 [ ]))))
gstep 3 (gstep 4 (gstep 2 (gstep 5 (1:[ ]))))
gstep 3 (gstep 4 (gstep 2 (1:gstep 5 [ ])))
Now answer the following questions. How long does it take to compute head ·isort
on a nonempty list? What is the precise sequence of comparisons made for sorting
[3,4,2,5,1]? Does Insertion sort actually work by inserting a new element into a
sorted list at each step?
Exercise 7.12 Explain why mkchange [7,3,1] 54 = [6,4,0], where
mkchange ds = minWith count ·mktuples ds
What change to the deﬁnition of mktuples would produce [7,1,2]?

168
Greedy algorithms on lists
Exercise 7.13 Here is the weight-based version of coin-changing:
type Weights = [Int]
weight ::Weights →Tuple →Int
weight ws cs = sum (zipWith (×) ws cs)
mkchangew::Weights →[Denom] →Nat →Tuple
mkchangew ws ds = minWith (weight ws)·mktuples ds
In the UK it is the case that minimising count also minimises weight. We could
prove this by simply carrying out an exhaustive test:
ukws = [1200,950,800,500,650,325,712,356]
test
= [n | n ←[1..200],mkchange ukds n ̸= mkchangew ukws ukds n]
We only need to check amounts up to £2. But test returns a nonempty list beginning
with 2 because
mkchange ukds 2
= [0,0,0,0,0,0,1,0]
mkchangew ukws ukds 2 = [0,0,0,0,0,0,0,2]
One 2p coin weighs the same as two 1p coins. What has gone wrong, and how can
the test be corrected?
Exercise 7.14 Express the function mktuples as an instance of foldr. (Hint: maintain
a list of pairs, where a pair consists of a tuple and a residual amount, and then at
the end select the ﬁrst component of a pair with a zero residue.) Write down the
associated greedy algorithm. We will use such a deﬁnition when we come to discuss
a thinning algorithm for the same problem.
Exercise 7.15 Consider the denominations [5,2,1] and a largest tuple [c3,c2,c1]
with minimum count. Show that, if 2 c2 +c1 ⩾5, then there is a larger tuple for the
same amount but with a smaller count. If the denominations were [4,3,1], do we
necessarily have 3 c2 +c1 <4?
Exercise 7.16 Consider the UR (United Region) currency whose denominations
are [100,50,20,15,5,2,1]. Explain carefully where the argument as to why the
greedy algorithm works for UK currency breaks down with UR currency. Does the
greedy algorithm work for UR currency?
Exercise 7.17 Prove that, if each denomination is a multiple of the next lower
denomination, then the greedy algorithm works.
Exercise 7.18 Prove (7.1) using the rule of ﬂoors.
Exercise 7.19 We calculated that
intern = halve·foldr shiftn 0

Exercises
169
Suppose halve · foldr shiftn 0 = foldr op 0 for some function op. The associated
fusion condition is
halve (shiftn d n) = op d (halve n)
for all n of the form n = foldr shiftn 0 ds. Using the rule of ﬂoors, we have
halve (shiftn d n) = (217 ×d +n+10) div 20
Now, since halve (2×n) = halve (2×n−1), the fusion condition requires that
(217 ×d +2×n+10) div 20 = (217 ×d +2×n+9) div 20
Your task is to ﬁnd a two-digit decimal ds with n = foldr shiftn 0 ds such that the
above statement is false for d = 0.
Exercise 7.20 Why can all but the ﬁrst 17 digits of the input be ignored?
Exercise 7.21 Why does the ﬁrst deﬁnition of decimals contain a bug? Hint: as we
said in the very ﬁrst chapter, Haskell does not guarantee that the type Int covers
a greater range than [−229,229). Would the bug still occur if a Haskell compiler
allowed the range [−263,263) for Int? Give the necessary revisions of decimals and
externs that solve the problem.
Exercise 7.22 For n<216 the integer D, where
D =
	
105 n
216 + 1
2

satisﬁes D<105 and so has at most 5 digits. Using this fact, show that extern n has
at most 5 digits.
Exercise 7.23 To verify the distributive law for MinWith cost, we have to show that,
if x ←MinWith (concat xss), then there exists a list xs such that
xs ←map (MinWith cost) xss ∧x ←MinWith cost xs
Conversely, we also have to show
xs ←map (MinWith cost) xss ∧x ←MinWith cost xs
⇒x ←MinWith cost (concat xss)
Prove these two claims.
Exercise 7.24 Suppose MCC xs = MinWith cost (candidates xs). Show that
foldr gstep e xs ←MCC xs
provided e ←MCC [ ] and
c ←MCC xs ⇒gstep x c ←MCC (x:xs)
for all candidates c, components x, and lists of components xs.

170
Greedy algorithms on lists
Exercise 7.25 Deﬁne Flip::Bool →Bool by
Flip x = MinWith (const 0) [x,not x]
Which of the following assertions are true?
id ←Flip
not ←Flip
not ·not = not
Flip·Flip = Flip
Answers
Answer 7.1 One simple deﬁnition:
minWith f = minimumBy cmp
where cmp x y = compare (f x) (f y)
A more efﬁcient deﬁnition:
minWith f = snd ·minimumBy cmp·map tuple
where tuple x = (f x,x)
cmp (x, ) (y, ) = compare x y
Answer 7.2 One simple deﬁnition:
minsWith f xs = [x | x ←xs,and [f x ⩽f y | y ←xs]]
A more efﬁcient deﬁnition:
minsWith f = map snd ·foldr step [ ]·map tuple
where tuple x = (f x,x)
step x [ ]
= [x]
step x (y:xs) | a<b
= [x]
| a == b = x:y:xs
| a>b
= y:xs
where a = fst x;b = fst y
Answer 7.3 We can prove
foldr1 f (xs++ys) = f (foldr1 f xs) (foldr1 f ys)
by induction on xs. The induction step is
foldr1 f (x:xs++ys)
=
{ deﬁnition of foldr1 }
f x (foldr1 f (xs++ys))
=
{ induction }
f x (f (foldr1 f xs) (foldr1 f ys))
=
{ associativity of f }
f (f x (foldr1 f xs)) (foldr1 f ys)
=
{ deﬁnition of foldr1 }
f (foldr1 f (x:xs)) (foldr1 ys)

Answers
171
The proof of
foldr1 f (concat xss) = foldr1 f (map (foldr1 f) xss)
is also by induction. The induction step is
foldr1 f (concat (xs:xss))
=
{ deﬁnition of concat }
foldr1 f (xs++concat xss)
=
{ above }
f (foldr1 f xs) (foldr1 f (concat xss))
=
{ induction }
f (foldr1 f xs) (foldr1 f (map (foldr1 f) xss))
=
{ deﬁnition of foldr1 }
foldr1 f (foldr1 f xs:map (foldr1 f) xss)
=
{ deﬁnition of map }
foldr1 (map (foldr1 f) (xs:xss))
The ﬁnal claim holds because smaller f is associative.
Answer 7.4 One deﬁnition is
perms::[a] →[[a]]
perms [ ] = [[ ]]
perms [x] = [[x]]
perms xs = concatMap interleave (cp yss zss)
where yss = perms ys
zss = perms zs
(ys,zs) = splitAt (length xs div 2) xs
cp::[a] →[b] →[(a,b)]
cp xs ys = [(x,y) | x ←xs,y ←ys]
interleave::([a],[a]) →[[a]]
interleave (xs,[ ])
= [xs]
interleave ([ ],ys)
= [ys]
interleave (x:xs,y:ys) = map (x:) (interleave (xs,y:ys))++
map (y:) (interleave (x:xs,ys))
Answer 7.5 We have
minimum (map (x:) [ ]) = ⊥
x:minimum [ ]
= x:⊥
This is a consequence of the fact that the (:) operation is not strict in Haskell.
Answer 7.6 The result clearly holds for a singleton list. For the induction step we
argue as follows:

172
Greedy algorithms on lists
minimum (map f (x:xs))
=
{ deﬁnition of map }
minimum (f x:map f xs)
=
{ deﬁnition of minimum }
min (f x) (minimum (map f xs))
=
{ induction }
min (f x) (f (minimum xs))
=
{ claim: min (f x) (f y) = f (min x y) }
f (min x (minimum xs))
=
{ deﬁnition of minimum }
f (minimum (x:xs))
The claim is equivalent to the condition that f is monotonic.
For the second question, suppose that a<b<c, f a<min (f b) (f c), and f c<f b.
Then f is not monotonic but, nevertheless,
minimum [f a,f b,f c] = f (minimum [a,b,c])
Answer 7.7 It is easy to show gstep x [ ] = [x]. For the induction step we argue
gstep x (y:xs)
=
{ deﬁnition of gstep }
minimum (extend x (y:xs))
=
{ deﬁnition of extend }
minimum ((x:y:xs):map (y:) (extend x xs))
=
{ deﬁnition of minimum }
min (x:y:xs) (minimum (map (y:) (extend x xs)))
=
{ since minimum·map (y:) = (y:)·minimum on nonempty lists }
min (x:y:xs) (y:minimum (extend x xs))
=
{ deﬁnition of gstep }
min (x:y:xs) (y:gstep x xs)
Hence we have the deﬁnition
gstep x [ ]
= [x]
gstep x (y:xs) = min (x:y:xs) (y:gstep x xs)
Answer 7.8 We show that gstep x is monotonic, that is,
as ⩽bs ⇒gstep x as ⩽gstep x bs
(7.5)
whenever as and bs have the same length. The proof is by induction. The claim is
immediate if both as and bs are the empty list. For the induction step, suppose a:as
and b:bs are two lists of the same length with a:as ⩽b:bs, so either a<b, or a = b
and as ⩽bs. If a<b, then

Answers
173
x:a:as<x:b:bs ∧a:gstep x as<b:gstep x bs
so gstep x (a:as)<gstep x (b:bs). In the case a = b and as ⩽bs we have
x:a:as ⩽x:a:bs ∧a:gstep x as ⩽a:gstep x bs
because, by induction, gstep x as ⩽gstep x bs.
For the second question, we have [ ]<[1], but
gstep 2 [ ] = [2]>[1,2] = gstep 2 [1]
Answer 7.9 Either x1 <x2, in which case the implication is immediate, or x1 = x2,
in which case ys1 and ys2 contain exactly the same elements and sorting these lists
produces the same result.
Answer 7.10 The deﬁnition is
pick [x]
= (x,[ ])
pick (x:xs) = if x ⩽y then (x,xs) else (y,x:ys) where (y,ys) = pick xs
Answer 7.11 The evaluation sequence is as follows:
gstep 3 (gstep 4 (gstep 2 (gstep 5 (gstep 1 [ ]))))
gstep 3 (gstep 4 (gstep 2 (gstep 5 (1:[ ]))))
gstep 3 (gstep 4 (gstep 2 (1:gstep 5 [ ])))
gstep 3 (gstep 4 (1:gstep 2 (gstep 5 [ ])))
gstep 3 (1:gstep 4 (gstep 2 (gstep 5 [ ])))
1:gstep 3 (gstep 4 (gstep 2 (gstep 5 [ ])))
In answer to the ﬁrst question, it takes Θ(n) steps to compute the head of Insertion
sort on a list of length n. In answer to the second question, the precise sequence of
comparisons is
(5,1) (2,1) (4,1) (3,1) (2,5) (4,2) (3,2) (4,5) (3,4) (4,5)
In answer to the third question, Insertion sort is not really sorting by insertion, at
least when evaluated lazily. It is more akin to a sorting algorithm known as Bubble
sort, though not exactly the same sequence of comparisons is performed. The lesson
here is that under lazy evaluation you don’t always get what you think you are
getting.
Answer 7.12 Because mktuples produces tuples in increasing lexical order. If we
change the deﬁnition to read
mktuples::[Denom] →Nat →[Tuple]
mktuples [1] n
= [[n]]
mktuples (d :ds) n = [c:cs | c ←[m,m−1..0],cs ←mktuples ds (n−c×d)]
where m = n div d
then mktuples would produce tuples in decreasing lexical order and we would have
mkchange [7,3,1] 54 = [7,1,2].

174
Greedy algorithms on lists
Answer 7.13 The culprits, once again, are the deﬁnitions of minWith cost and
mktuples. Since one 2p coin weighs exactly the same as two 1p coins, there is no
unique minimum-weight tuple. The test can be corrected by redeﬁning mktuples
as in the previous exercise to generate tuples in decreasing lexical order. Then test
does return the empty list.
Answer 7.14 Expressing mktuples in terms of foldr means processing the list of
denominations from right to left. It follows that, in order to process denominations
in decreasing order of value, we have to reverse given lists of currencies like ukds.
Thus we deﬁne
mktuples ds n = ﬁnish (foldr (concatMap·extend) [([ ],n)] (reverse ds))
where ﬁnish = map fst ·ﬁlter (λ(cs,r).r == 0)
extend d (cs,r) = [(cs++[c],r −c×d) | c ←[0..r div d]]
That leads to the greedy algorithm
mkchange ds n = fst (foldr gstep ([ ],n) (reverse ds))
where gstep d (cs,r) = (cs++[c],r −c×d) where c = r div d
The greedy algorithm can be calculated by fusing maximum with mktuples.
Answer 7.15 We have 2 c2 +c1 ⩾5 if c2 ⩾3 or if (c2,c1) = (2,1). In the ﬁrst case
we can increase c3 by one and replace (c2,c1) either by (c2 −3,1), if c1 = 0, or by
(c2 −2,0), if c1 = 1. In the second case we can increase c3 by one and set both c1
and c2 to zero. In each case this gives a larger tuple with a smaller count.
The answer to the second question is no, because the tuple [c3,2,0] has a smaller
count than [c3 +1,0,2].
Answer 7.16 Let [c7,c6,...,c1] be the optimal solution and let [g7,g6,...,g1] be the
greedy one, so
A = 100c7 +50c6 +20c5 +15c4 +5c3 +2c2 +c1
A = 100g7 +50g6 +20g5 +15g4 +5g3 +2g2 +g1
The same argument as in the text shows that c1 = g1 and c2 = g2. Next, with
B = (A−(2c2 +c1))/5 we have
B = 20c7 +10c6 +4c5 +3c4 +c3
B = 20g7 +10g6 +4g5 +3g4 +g3
But now the argument breaks down since we cannot show that c3 = g3. In UR
currency we have 1×20+2×5 as the greedy choice for 30 units, while 2×15 is
the same amount with one coin fewer.
Answer 7.17 Suppose, as in the previous solution, we have
A = dk ck +··· +d2 c2 +c1
A = dk gk +··· +d2 g2 +g1

Answers
175
Since d2 divides into each other denomination, we have A mod d2 = c1 = g1. Next,
with B = (A−c1)/d2 we have B mod d3 = c2 = g2. And so on.
Answer 7.18 We can reason as follows:
k ⩽⌊(x+a)/b⌋
⇔
{ rule of ﬂoors }
kb ⩽x+a
⇔
{ rule of ﬂoors }
kb−a ⩽⌊x⌋
⇔
{ rule of ﬂoors }
k ⩽⌊(⌊x⌋+a)/b⌋
Hence ⌊(x+a)/b⌋= ⌊(⌊x⌋+a)/b⌋.
Answer 7.19 Take ds = [0,7]. We have n = foldr shiftn 0 [0,7] = 9175 and
(217 ×0+2×9175+10) div 20 = 918
(217 ×0+2×9175+9) div 20 = 917
Answer 7.20 Let r = fraction ds and r′ = fraction (take 17 ds). Then ⌊1017 r⌋=
⌊1017 r′⌋. Furthermore,
	217 r +1
2

=
	1017 r +517
2×517

so scale r = scale r′ by (7.1). So, only 17 digits matter. The smallest internal
value is 0, which occurs only if the input decimal is strictly less than the decimal
0.00000762939453125, the value of 2−17. The largest internal value is 216 = 65536,
which occurs only if the input is greater than the decimal representation of 1−217,
namely 0.99999237060546875. Hence 17 digits are sometimes necessary.
Answer 7.21 The reason for the bug is that, since 10b−w⌊10a/w⌋⩾10(b−a),
the size of the interval argument to decimals can grow from 2 to 2 × 1017. Since
2×1017 ⩾229, the upper bound of an interval can exceed the range of Int. However,
2×1017 <263, so the problem does not arise with 64-bit computers. The revised
deﬁnition of externs is
externs::Int →[[Digit]]
externs n = decimals (2×n′ −1,2×n′ +1)
where n′ = fromIntegral n
The deﬁnition of decimals::(Integer,Integer) →[[Digit]] is as before except that
the term d :ds has to be replaced with fromInteger d :ds because digits are elements
of Int, not Integer.

176
Greedy algorithms on lists
Answer 7.22 The fraction D/105 produces the internal number n′, where

D
105 −n′
216
 ⩽2−17
We also have
D−105 n
216
 ⩽1
2
by deﬁnition of D. Now
|n−n′| ⩽
n−216 D/105+
n′ −216 D/105 ⩽215/105 +1/2<1
so n = n′.
Answer 7.23 Suppose xss = [xs1,...,xsn] is a ﬁnite nonempty list of ﬁnite nonempty
lists. If x ←MinWith cost (concat xss), then x is an element of some list xsi
with a cost that is no greater than any other element of concat xss. Suppose
xj ←MinWith cost xsj for each j ̸= i. Then the list xs = [x1,..,xi−1,x,xi+1,..xn]
is such that
xs ←map (MinWith cost) xss ∧x ←MinWith cost xs
Conversely, suppose xs = [x1,...,xn] satisﬁes
xs ←map (MinWith cost) xss
so xj ←MinWith cost xsj for each 1 ⩽j ⩽n. Now take x = xi for some i such that
xi ←MinWith cost xs. Then x ←MinWith cost (concat xss). The proof really only
relies on the fact that, if cost x ⩽cost y and cost y ⩽cost z, then cost x ⩽cost z.
Answer 7.24 The proof is by induction on xs. The base step is immediate, and for
the induction step we can argue
foldr gstep e (x:xs)
=
{ deﬁnition of foldr }
gstep x (foldr gstep e xs)
←
{ induction }
gstep x (MCC xs)
←
{ greedy condition }
MCC (x:xs)
This reasoning is valid for any deﬁnition of candidates. However, unlike the greedy
algorithm based on fusion, it gives no hint about how gstep may be deﬁned.
Answer 7.25 The assertion not ·not = not is, of course, false. The others are true,
including the last one because there is no reﬁnement of Flip·Flip that is not also a
reﬁnement of Flip and vice versa.

Chapter 8
Greedy algorithms on trees
The next two problems are about trees, so the greedy algorithms take place in a
wood rather than on a hillside. The problems concern the task of building a tree
with minimum cost, for two different deﬁnitions of cost. The ﬁrst problem is closely
related to the tree-building algorithms we have seen before in binary search and
sorting. The second problem, Huffman coding trees, is of practical importance in
compressing data effectively. Unlike the problems in the previous chapter, the two
greedy tree-building algorithms require us to reason about the nondeterministic
function MinWith in order to prove that they work.
8.1 Minimum-height trees
Throughout the chapter we ﬁx attention on one type of tree, called a leaf-labelled
tree:
data Tree a = Leaf a | Node (Tree a) (Tree a)
A leaf-labelled tree is therefore a binary tree with information stored only at the
leaves. Essentially this species of tree, though with an additional constructor Null,
was described in Section 5.2 on Mergesort.
The size of a leaf-labelled tree is the number of its leaves:
size::Tree a →Nat
size (Leaf x)
= 1
size (Node u v) = size u+size v
The height of a tree is deﬁned by
height (Leaf x)
= 0
height (Node u v) = 1+height u max height v
With a leaf-labelled tree of size n and height h we have the relationship h<n ⩽2h,
so h ⩾⌈log n⌉.

178
Greedy algorithms on trees
The fringe of a tree is the list of leaf labels in left-to-right order:
fringe::Tree a →[a]
fringe (Leaf x)
= [x]
fringe (Node u v) = fringe u++fringe v
Thus fringe is essentially the same function that we have previously called ﬂatten.
Note that the fringe of a tree is always a nonempty list.
Consider the problem of building a tree of minimum height with a given list as
fringe. We have already encountered two ways of solving this problem, both of
which can be implemented to take linear time. The ﬁrst solution is the divide-and-
conquer, or top-down, method of Section 5.2:
mktree::[a] →Tree a
mktree [x] = Leaf x
mktree xs = Node (mktree ys) (mktree zs)
where (ys,zs) = splitAt (length xs div 2) xs
This deﬁnition does not take linear time, but it is easy to convert it into one that
does. The trick, as we have seen in the treatment of Mergesort in Section 5.2, is to
avoid repeated halving by tupling. Second, we have the bottom-up method, also
described in Section 5.2:
mktree = unwrap·until single (pairWith Node)·map Leaf
These two ways of building a tree lead to different trees but both have minimum
height. To show that this property holds for the ﬁrst deﬁnition of mktree, let H(n)
denote the height of mktree for an input of length n. Then H satisﬁes the recurrence
H(1) = 0 and H(n) = 1+H(⌈n/2⌉) with solution H(n) = ⌈log n⌉(see Exercise 8.1),
the minimum height possible. The reason why the bottom-up method also produces
a minimum-height tree is left as another exercise.
Let us now change the problem slightly: given a nonempty list of natural numbers,
can we ﬁnd a linear-time algorithm for building a tree with minimum cost and the
given list as fringe, where
cost ::Tree Nat →Nat
cost (Leaf x)
= x
cost (Node u v) = 1+cost u max cost v
The function cost has the same deﬁnition as height except that the ‘height’ of a leaf
is the label value rather than 0. In fact, if each leaf is replaced by a tree whose height
is given by the label value, the problem is really of the following form: given a list
of trees together with their heights, can we ﬁnd a linear-time algorithm to combine
them into a single tree of minimum height without changing the shape or order of
the component trees? To appreciate the problem consider the two trees with the
same fringe

8.1 Minimum-height trees
179
1
2
3
4
3
5
6
and
1
2
3
4
3
4
5
in which each node is labelled with its cost. The tree on the left has cost 6, but the
tree on the right has minimum cost 5. It is not obvious how to construct a tree with
minimum cost, at least not efﬁciently, and that is where a greedy algorithm enters
the stage. We start off with a speciﬁcation and then calculate the algorithm.
The speciﬁcation is phrased as one of reﬁnement:
mct ::[Nat] →Tree Nat
mct xs ←MinWith cost (mktrees xs)
for ﬁnite nonempty lists xs, where mktrees xs is a list of all possible trees with fringe
xs. In words, mct xs is some element of mktrees xs with minimum cost.
The function mktrees can be deﬁned in a number of ways. We are going to give
two inductive deﬁnitions; other possibilities are discussed in the exercises. The ﬁrst
method is to deﬁne
mktrees::[a] →[Tree a]
mktrees [x]
= [Leaf x]
mktrees (x:xs) = concatMap (extend x) (mktrees xs)
The function extend returns a list of all the ways in which a new element can be
added as a leftmost leaf in a tree:
extend ::a →Tree a →[Tree a]
extend x (Leaf y)
= [Node (Leaf x) (Leaf y)]
extend x (Node u v) = [Node (Leaf x) (Node u v)]++
[Node u′ v | u′ ←extend x u]
For example, applying extend x to the tree
y
t1
t2
produces the three trees

180
Greedy algorithms on trees
x
y
t1
t2
x
y
t1
t2
x
y
t1
t2
We might have taken mktrees [ ] = [ ] and so deﬁned mktrees as an instance of foldr.
But MinWith is not deﬁned on an empty list and we have to restrict the input to
nonempty lists. The Haskell standard library does not provide a sufﬁciently general
fold function for nonempty lists (the function foldr1 is not quite general enough),
but if we deﬁne foldrn by
foldrn::(a →b →b) →(a →b) →[a] →b
foldrn f g [x]
= g x
foldrn f g (x:xs) = f x (foldrn f g xs)
then the deﬁnition of mktrees above can be recast in the form
mktrees = foldrn (concatMap·extend) (wrap·Leaf)
where wrap converts a value into a singleton list.
The second inductive way of building a tree is to ﬁrst build a forest, a list of trees:
type Forest a = [Tree a]
A forest can be ‘rolled up’ into a tree using
rollup::[Tree a] →Tree a
rollup = foldl1 Node
The function foldl1 is the Haskell prelude function for folding a nonempty list from
left to right. For example,
rollup [t1,t2,t3,t4] = Node (Node (Node t1 t2) t3) t4
The converse to rollup is the function spine, deﬁned by
spine::Tree a →[Tree a]
spine (Leaf x)
= [Leaf x]
spine (Node u v) = spine u++[v]
This function returns the leftmost leaf of a tree, followed by a list of the right
subtrees along the path from the leftmost leaf of the tree to the root. Provided the
ﬁrst tree in a forest ts is a leaf, we have
spine (rollup ts) = ts
We can now deﬁne
mktrees::[a] →[Tree a]
mktrees = map rollup·mkforests

8.1 Minimum-height trees
181
where mkforests builds the forests:
mkforests::[a] →[Forest a]
mkforests = foldrn (concatMap·extend) (wrap·wrap·Leaf)
extend ::a →Forest a →[Forest a]
extend x ts = [Leaf x:rollup (take k ts):drop k ts | k ←[1..length ts]]
The new version of extend is arguably simpler than the previous one. It works by
rolling up some initial segment of the forest into a tree and adding a new leaf as the
ﬁrst tree in the new forest. For example,
extend x [t1,t2,t3] = [[Leaf x,t1,t2,t3],
[Leaf x,Node t1 t2,t3],
[Leaf x,Node (Node t1 t2) t3]]
The two versions of mktrees are not the same function simply because they produce
the trees in a different order. We will come back to spine and rollup later on.
Let us now return to the ﬁrst deﬁnition of mktrees, the one expressed directly as
an instance of foldrn. To fuse the two component functions in the deﬁnition of mct
we can appeal to the fusion law of foldrn. The context-sensitive version of this law
states that
foldrn f2 g2 xs ←M (foldrn f1 g1 xs)
for all ﬁnite, nonempty lists xs, provided g2 x ←M (g1 x) and
f2 x (M (foldrn f1 g1 xs)) ←M (f1 x (foldrn f1 g1 xs))
For our problem, M = MinWith cost, f1 = concatMap·extend, and g1 = wrap·leaf.
Since Leaf x = MinWith cost [Leaf x], we can take g2 = Leaf. For the second fusion
condition we have to ﬁnd a function, gstep say, so that
gstep x (MinWith cost (mktrees xs))
←MinWith cost (concatMap (extend x) (mktrees xs))
As we saw at the end of the previous chapter, this condition is satisﬁed if the
monotonicity condition
cost t ⩽cost t′ ⇒cost (gstep x t) ⩽cost (gstep x t′)
holds for all trees t and t′ in mktrees xs. However, no such function gstep exists to
satisfy the monotonicity condition. Consider the two trees t1 and t2:
5
6
7
9
8
9
10
5
6
7
9
7
8
10

182
Greedy algorithms on trees
which, along with the three trees
5
6
7
9
7
10
11
5
6
7
9
11
10
8
5
6
7
9
10
11
12
are the ﬁve trees that can be built with fringe [5,6,7,9]. The subtrees of each tree
have been labelled with their costs, so both t1 and t2 have the minimum possible
cost 10. However, the monotonicity condition
cost t1 ⩽cost t2 ⇒cost (gstep x t1) ⩽cost (gstep x t2)
fails for any deﬁnition of gstep. Take, for example, x = 8. Adding 8 to t1 in the best
possible way gives a tree with minimum cost 11, while adding 8 to t2 in the best
possible way gives a tree with cost 10. So there is no way we can deﬁne a function
gstep for which the fusion condition holds. Once again we appear to be stuck, even
with a reﬁnement version of fusion.
The only way out of the wood is to change the cost function, and once again
lexical ordering comes to the rescue. Notice that the list of costs [10,8,7,5] reading
downwards along the left spine of t2 is lexically less than the costs [10,9,5] along
the left spine of t1. The lexical cost, lcost say, is deﬁned by
lcost ::Tree Nat →[Nat]
lcost = reverse·scanl1 op·map cost ·spine
where op x y = 1+(x max y)
The costs of the trees along the left spine are accumulated from left to right by
scanl1 op and then reversed. For example, spine t2 has tree costs [5,6,7,9] and
accumulation gives the list [5,7,8,10], which, when reversed, gives the lexical cost
of t2. Minimising lcost also minimises cost (why?), so we can revise the second
fusion condition to read
gstep x (MinWith lcost (mktrees xs))
←MinWith lcost (concatMap (extend x) (mktrees xs))
This time we can show
lcost t1 ⩽lcost t2 ⇒lcost (gstep x t1) ⩽lcost (gstep x t2)
where gstep is speciﬁed by
gstep x ts ←MinWith lcost (extend x ts)
To give a constructive deﬁnition of gstep and to prove that monotonicity holds,
consider the two trees of Figure 8.1 in which t1 is a leaf. The tree on the left is the
result of rolling up the forest [t1,t2,...,tn] into a single tree. The tree on the right is

8.1 Minimum-height trees
183
t1
t2
tn−1
tn
c2
cn−1
cn
x
cj
tj+1
tn
c′
j
c′
j+1
c′
n
t1
tj
Figure 8.1 Inserting x into a tree
obtained by adding x as a new leaf after rolling up the ﬁrst j elements of the forest.
The trees are labelled with cost information, so
c1 = cost t1
ck = 1+(ck−1 max cost tk)
for 2 ⩽k ⩽n. In particular, [c1,c2,...,cn] is strictly increasing. A similar deﬁnition
holds for the costs on the right:
c′j = 1+(x max cj)
c′k = 1+(c′k−1 max cost tk)
for j+1 ⩽k ⩽n. In particular, since adding a new leaf cannot reduce costs, we have
ck ⩽c′k for j ⩽k ⩽n.
The aim is to deﬁne gstep by choosing j to minimise [c′n,c′n−1,...,c′j,x]. For
example, consider the ﬁve trees [t1,t2,...,t5] with costs [5,2,4,9,6]. Then
[c1,c2,...,c5] = [5,6,7,10,11]
Take x = 8. There are ﬁve possible ways of adding x to the forest, namely by rolling
up j trees for 1 ⩽j ⩽5. Here they are, with costs on the left and accumulated costs
on the right:
[8,5,2,4,9,6] −→[8,9,10,11,12,13]
[8,6,4,9,6]
−→[8,9,10,11,12]
[8,7,9,6]
−→[8,9,10,11]
[8,10,6]
−→[8,11,12]
[8,11]
−→[8,12]
The forest which minimises lcost is the third one, whose lexical cost is the reverse
of [8,9,10,11].
We claim that the best choice of j is the smallest value in the range 1 ⩽j<n, if it
exists, such that
1+(x max cj) < cj+1
(8.1)

184
Greedy algorithms on trees
If no such j exists, then choose j = n. For example, with
[c1,c2,c3,c4,c5] = [5,6,7,10,11]
and x = 8, the smallest j satisfying (8.1) is j = 3, with the result
[x,1+(x max c3),c4,c5] = [8,9,10,11]
On the other hand, with x = 9 we have j = 5, with the result
[x,1+(x max c5)] = [9,12]
To prove (8.1), suppose the claim holds for both j and k, where 1 ⩽j<k <n. Then,
setting c′j = 1+(x max cj) and c′k = 1+(x max ck), the two sequences
as = [x,c′j,cj+1,...,ck−1,ck, ck+1,...,cn]
bs =
[x,
c′k,ck+1,...,cn]
are such that reverse as<reverse bs because ck <c′k. Hence, the smaller the value
of j, the lower is the cost.
To show that gstep x is monotonic with respect to lcost, suppose
lcost t1 = [cn,cn−1,...,c1]
lcost t2 = [dm,dm−1,...,d1]
where lcost t1 ⩽lcost t2. If these costs are equal, then so are the costs of adding a
new leaf to either tree. Otherwise, if lcost t1 <lcost t2 and we remove the common
preﬁx, say one of length k, then we are left with two trees t′1 and t′2 with
lcost t′1 = [cp,...,c1]
lcost t′2 = [dq,...,d1]
where p = n−k, q = m−k and cp <dq. It is sufﬁcient to show that
lcost (gstep x t′1) ⩽lcost (gstep x t′2)
Firstly, suppose (8.1) holds for t′1 and j<p. Then
lcost (gstep x t′1) = [cp,...,cj+1,1+(x max cj),x]
But cp < dq, and since gstep x t′2 can only increase the cost of t′2, we have in this
case that
lcost (gstep x t′1)<lcost t′2 ⩽lcost (gstep x t′2)
In the second case, suppose (8.1) does not hold for t′1. In this case
lcost (gstep x t′1) = [1+(x max cp),x]
Now, either 1+(x max cp)<dq, in which case
lcost (gstep x t′1)<lcost t′2 ⩽lcost (gstep x t′2)
or 1+(x max cp) ⩾dq, in which case x ⩾dq −1 and 1+(x max dq−1) ⩾dq. That
means that (8.1) does not hold for t′2 either, and so we have

8.1 Minimum-height trees
185
lcost (gstep x t′1) = [1+(x max cp),x]
⩽[1+(x max dq),x] = lcost (gstep x t′2)
That completes the proof of monotonicity.
The next task is to implement gtep. We can rewrite (8.1) by arguing
1+(x max cj)<cj+1
⇔1+(x max cj)<1+(cj max cost tj+1)
⇔(x max cj)<cost tj+1
Hence mct = foldrn gstep Leaf, where
gstep::Nat →Tree Nat →Tree Nat
gstep x = rollup·add x·spine
where add is deﬁned by
add x ts = Leaf x:join x ts
join x [u]
= [u]
join x (u:v:ts) = if x max cost u<cost v
then u:v:ts else join x (Node u v:ts)
However, instead of computing spines at each step and then rolling up the spine
again, we can roll up the forest at the end of the computation. What is wanted for
this step are functions hstep and g for which
foldrn gstep Leaf = rollup·foldrn hstep g
We can discover hstep and g by appealing to the fusion law for foldrn. Notice that
here we are applying the fusion law for foldrn in the anti-fusion, or ﬁssion direction,
splitting a fold into two parts.
Firstly, we require rollup·g = Leaf. Since rollup [Leaf x] = Leaf x, we can deﬁne
g by g = wrap·Leaf. Secondly, we want
rollup (hstep x ts) = gstep x (rollup ts)
for all x and all ts of the form ts = foldrn hstep g xs. Now,
gstep x (rollup ts)
=
{ deﬁnition of gstep }
rollup (add x (spine (rollup ts)))
=
{ provided the ﬁrst element of ts is a leaf }
rollup (add x ts)
Hence we can take hstep = add, provided the ﬁrst element of ts is a leaf. But
ts = foldrn add (wrap·Leaf) xs for some xs and it is immediate from the deﬁnition
of add that the ﬁrst element of ts is indeed a leaf.
We now have mct = rollup · foldrn add (wrap · Leaf). As a ﬁnal step, repeated
evaluations of cost can be eliminated by pairing each tree in the forest with its cost.
That leads to the ﬁnal algorithm

186
Greedy algorithms on trees
type Pair = (Tree Nat,Nat)
mct ::[Nat] →Tree Nat
mct = rollup·map fst ·foldrn hstep (wrap·leaf)
hstep::Nat →[Pair] →[Pair]
hstep x ts = leaf x:join x ts
join::Nat →[Pair] →[Pair]
join x [u]
= [u]
join x (u:v:ts) = if x max snd u<snd v
then u:v:ts else join x (node u v:ts)
The functions leaf and node are the smart constructors
leaf ::Nat →Pair
leaf x = (Leaf x,x)
node::Pair →Pair →Pair
node (u,c) (v,d) = (Node u v,1+c max d)
For example, the greedy algorithm applied to the list [5,3,1,4,2] produces the
forests
[Leaf 2]
[Leaf 4,Leaf 2]
[Leaf 1,Node (Leaf 4) (Leaf 2)]
[Leaf 3,Leaf 1,Node (Leaf 4) (Leaf 2)]
[Leaf 5,Node (Node (Leaf 3) (Leaf 1)) (Node (Leaf 4) (Leaf 2))]
The ﬁnal forest is then rolled up into the ﬁnal tree
5
3
1
4
2
with cost 7.
It remains to estimate the running time of mct. The critical measure is the number
of calls to join. We can prove by induction that any sequence of hstep operations
applied to a list of length n and returning a forest of length m involves at most
2n−m calls to join. The base case, n = 1 and m = 1, is obvious. For the induction
step, note that join applied to a list of length m′ and returning a list of length m is
called m′ −m times. Thus, using the induction step that hstep applied to a list of
length n−1 and returning a forest of length m′ involves at most 2(n−1)−m′ calls

8.2 Huﬀman coding trees
187
of join, we have hstep applied to a list of length n, and returning a forest of length
m involves at most
(2(n−1)−m′)+1+(m′ −m) ⩽2n−m
calls of join, establishing the induction. Hence the algorithm takes linear time.
Before leaving the problem of building a minimum-cost tree, we make one ﬁnal
remark. Observe that, when the input is a list consisting entirely of zeros, building
a minimum-cost tree means building a minimum-height tree. It follows that the
greedy algorithm, with minor changes, also works when the cost is the height of the
tree. The changes are left as an exercise.
8.2 Huﬀman coding trees
Our second example is Huffman coding trees. As older computer users know only
too well, it is often necessary to store ﬁles of information as compactly as possible.
Suppose the information to be stored is a text consisting of a sequence of characters.
Haskell uses Unicode internally for its Char data type, but the standard text I/O
functions assume that texts are sequences of 8-bit characters, so a text of n characters
contains 8n bits of information. Each character is represented by a ﬁxed-length
code, so the characters of a text can be recovered by decoding each successive group
of eight bits.
One idea for reducing the total number of bits required to code a text is to abandon
the notion of ﬁxed-length codes, and seek instead a coding scheme based on the
relative frequency of occurrence of the characters in the text. The basic idea is to
take a sample piece of text, estimate the number of times each character appears,
and choose short codes for the more frequent characters and longer codes for the
rarer ones. For example, if we take the codes
't' −→0
'e' −→10
'x' −→11
then “text” can be coded as the bit sequence 010110 of length 6. However, it is
important that codes are chosen in such a way as to ensure that the coded text can
be deciphered uniquely. To illustrate, suppose the codes had been
't' −→0
'e' −→10
'x' −→1
Under this scheme, “text” would be coded as the sequence 01010 of length 5.
However, the string “tee” would also be coded by 01010. Obviously this is not what

188
Greedy algorithms on trees
is wanted. The simplest way to prevent the problem arising is to choose codes so
that no code is a proper preﬁx of any other – a preﬁx-free code.
As well as requiring unique decipherability, we also want the coding to be optimal.
An optimal coding scheme is one that minimises the expected length of the coded
text. More precisely, if characters cj, for 1 ⩽j ⩽n, have frequencies of occurrence
pj, then we want to choose codes with lengths ℓj such that
n
∑
j=1
pj ℓj
is as small as possible.
One method for constructing an optimal code satisfying the preﬁx property is
called Huffman coding. Each character is stored in a leaf of a binary tree, the
structure of which is determined by the computed frequencies. The code for a
character c is the sequence of binary values describing the path in the tree to the
leaf containing c. For instance, with the tree
Node (Node (Leaf 'b') (Leaf 'e')) (Leaf 't')
the character ‘b’ is coded by 00, the character ‘e’ by 01, and the character ‘t’ by 1.
Clearly, such a scheme yields a preﬁx-free code.
There are four aspects to the problem of implementing Huffman coding: (i) col-
lecting information from a sample; (ii) building a binary tree; (iii) coding a text; and
(iv) decoding a bit sequence. We deal only with the problem of building a tree.
So, having analysed the sample, suppose we are given a list of pairs:
[(c1,w1),(c2,w2),...,(cn,wn)]
where for 1 ⩽j ⩽n the cj are the characters and the wj are positive integers,
called weights, indicating the frequencies of the characters in the text. The relative
frequency of character cj occurring is therefore wj/W, where W = ∑wj. We will
suppose w1 ⩽w2 ⩽··· ⩽wn, so that the weights are given in ascending order.
In terms of trees, the cost function we want to minimise can be deﬁned in the
following way. By deﬁnition, the depth of a leaf is the length of the path from the
root of the tree to the leaf. We can deﬁne the list of depths of the leaves in a tree by
depths::Tree a →[Nat]
depths = from 0
where from n (Leaf x)
= [n]
from n (Node u v) = from (n+1) u++from (n+1) v
Now introduce the types
type Weight = Nat
type Elem
= (Char,Weight)
type Cost
= Nat

8.2 Huﬀman coding trees
189
and deﬁne cost by
cost ::Tree Elem →Cost
cost t = sum [w×d | (( ,w),d) ←zip (fringe t) (depths t)]
It is left as an exercise to derive the following alternative deﬁnition of cost:
cost (Leaf e)
= 0
cost (Node u v) = cost u+cost v+weight u+weight v
weight ::Tree Elem →Nat
weight (Leaf (c,w)) = w
weight (Node u v)
= weight u+weight v
We might now follow the previous section and specify
huffman::[Elem] →Tree Elem
huffman ←MinWith cost ·mktrees
where mktrees builds all the trees with a given list as fringe. But this speciﬁcation
is too strong: it is not required that the input list be the fringe, only that some
permutation of it is. (However, in Chapter 14 we will consider a version of the
problem in which the input is required to be the fringe.) One way of correcting the
deﬁnition is to replace mktrees by concatMap mktrees · perms. Another way, and
the one we will pursue, is to design a new version of mktrees. This version will
construct all unordered binary trees. In an unordered binary tree the two children of
a node are regarded as a set of two trees rather than an ordered pair. Thus Node u v is
regarded as the same tree as Node v u. For example, there are 12 ordered binary trees
whose fringe is a permutation of [1,2,3], two trees for each of the six permutations,
but only three essentially different unordered trees:
Node (Node (Leaf 1) (Leaf 2)) (Leaf 3)
Node (Node (Leaf 1) (Leaf 3)) (Leaf 2)
Node (Node (Leaf 2) (Leaf 3)) (Leaf 1)
Each tree can be ﬂipped in three ways (ﬂipping the children of the top tree, the
children of the left subtree, or both) to give the 12 different ordered binary trees.
For Huffman coding it is sufﬁcient to consider unordered trees because two sibling
characters have the same codes except for the last bit and it does not matter which
sibling is on the left. To compute all the unordered Huffman trees we can start with
a list of leaves in weight order, and then repeatedly combine pairs of trees until a
single tree remains. The pairs are chosen in all possible ways and a combined pair
can be placed back in the list so as to maintain weight order. Thus, in an unordered
tree Node u v we can assume cost u ⩽cost v without loss of generality.
Here is an example to see the idea at work. Showing only the weights, consider
the following list of four trees in weight order:

190
Greedy algorithms on trees
[Leaf 3,Leaf 5,Leaf 8,Leaf 9]
As a ﬁrst step we can choose to combine the ﬁrst and third trees (among six possible
choices) to give
[Leaf 5,Leaf 9,Node (Leaf 3) (Leaf 8)]
The new tree, with weight 11, is placed last in the list to maintain weight order. As
the next step we can choose to combine the ﬁrst two trees (among three possible
choices), giving
[Node (Leaf 3) (Leaf 8),Node (Leaf 5) (Leaf 9)]
The next step is forced as there are only two trees left, and we end up with a singleton
tree
[Node (Node (Leaf 3) (Leaf 8)) (Node (Leaf 5) (Leaf 9))]
whose fringe is [3,8,5,9]. This bottom-up method for building trees will generate
6 × 3 = 18 trees in total, more than the total number of unordered trees on four
elements, because some trees, such as the one above, are generated twice (see the
exercises). However, the list of trees includes all that are needed.
Now for the details. We deﬁne
mktrees::[Elem] →[Tree Elem]
mktrees = map unwrap·mkforests·map Leaf
where mkforests builds the list of forests, each forest consisting of a singleton tree.
On way to deﬁne this function uses until:
mkforests::[Tree Elem] →[Forest Elem]
mkforests = until (all single) (concatMap combine)·wrap
The function mkforests takes a list of trees, turns them into a singleton list of forests
by applying wrap, and then repeatedly combines two trees in every possible way
until every forest is reduced to a single tree. Each singleton forest is then unwrapped
to give the ﬁnal list of trees. The function combine is deﬁned by
combine::Forest Elem →[Forest Elem]
combine ts = [insert (Node t1 t2) us | ((t1,t2),us) ←pairs ts]
pairs::[a] →[((a,a),[a])]
pairs xs = [((x,y),zs) | (x,ys) ←picks xs,(y,zs) ←picks ys]
The function picks was deﬁned in Chapter 1. The function insert, whose deﬁnition
is left as an exercise, inserts a tree into a list of trees so as to maintain weight order.
Hence combine selects, in all possible ways, a pair of trees from a forest, combines
them into a new tree, and inserts the new tree into the remaining trees.
Another way to deﬁne mkforests uses the function apply. Recall the answer to
Question 1.13, which gives the following deﬁnition of apply:

8.2 Huﬀman coding trees
191
apply::Nat →(a →a) →a →a
apply n f = if n == 0 then id else f ·apply (n−1) f
Thus apply n applies a function n times to a given value. The alternative deﬁnition
of mkforests is to write
mkforests::[Tree Elem] →[Forest Elem]
mkforests ts = apply (length ts−1) (concatMap combine) [ts]
The two deﬁnitions give the same result because at each step the number of trees
in each forest is reduced by one, so it takes exactly n−1 steps to reduce an initial
forest of n trees to a list of singleton forests.
Our problem now takes the form
huffman::[Elem] →Tree Elem
huffman ←MinWith cost ·mktrees
Since mktrees is deﬁned in terms of until, we will aim for a constructive deﬁnition
of huffman of the same form. The task is to ﬁnd a function gstep so that
unwrap (until single gstep (map Leaf xs)) ←MinWith cost (mktrees xs)
for all ﬁnite nonempty lists xs of type [Elem]. More generally, we will seek a
function gstep such that
unwrap (until single gstep ts) ←MinWith cost (map unwrap (mkforests ts))
for all ﬁnite nonempty lists of trees ts. Problems of this form will arise in the
following chapter too, so let us pause for a little more theory on greedy algorithms.
Another generic greedy algorithm
Suppose in this section that the list of candidates is given by a function
candidates::State →[Candidate]
for some type State. For Huffman coding, states are lists of trees and candidates are
trees:
candidates ts = map unwrap (mkforests ts)
For the problems in the following chapter, states are combinations of values.
The aim of this section is to give conditions for which the reﬁnement
extract (until ﬁnal gstep sx) ←MinWith cost (candidates sx)
(8.2)
holds for all states sx. The functions on the left have the following types:
gstep ::State →State
ﬁnal
::State →Bool
extract ::State →Candidate
In words, (8.2) states that repeatedly applying a greedy step to any initial state sx will

192
Greedy algorithms on trees
result in a ﬁnal state from which a candidate x can be extracted with the property that
x is a candidate in candidates sx with minimum cost. In order for the reﬁnement to
be meaningful, it is assumed that the left-hand side returns a well-deﬁned value for
any initial state. Unlike the formulation of a generic greedy algorithm in Section 7.1,
nothing is known about how the candidates are constructed.
For brevity in what follows, deﬁne
MCC sx
= MinWith cost (candidates sx)
mincost sx = minimum (map cost (candidates sx))
In particular, for all x in candidates sx we have
x ←MCC sx ⇔cost x = mincost sx
There are two conditions that ensure (8.2). The ﬁrst is
ﬁnal sx ⇒extract sx ←MCC sx
(8.3)
This condition holds for Huffman coding, when ﬁnal = single and extract = unwrap,
since map unwrap (mkforests [t]) = [t] and MinWith cost [t] = t.
The second condition is the greedy condition. We can state it in two ways. The
ﬁrst way is
not (ﬁnal sx) ⇒(∃x:x ←MCC (gstep sx) ∧x ←MCC sx)
(8.4)
In hillclimbing terms, the greedy condition asserts that, from any starting point not
already on top of the hill, there is some path to a highest point that starts out with a
greedy step.
The second way of stating the greedy condition appears to be stronger:
not (ﬁnal sx) ⇒MCC (gstep sx) ←MCC sx
(8.5)
However, with one extra proviso, (8.4) implies (8.5). The proviso is that applying
gstep to a state may reduce the number of ﬁnal candidates but will never introduce
new ones. In symbols,
candidates (gstep sx) ⊆candidates sx
(8.6)
Suppose x ←MCC (gstep sx) and x ←MCC sx. Then, by deﬁnition of MCC and
mincost, we have
mincost (gstep sx) = cost x = mincost sx
Now suppose y ←MCC (gstep sx), so y ∈candidates sx by (8.6). Then
cost y = mincost (gstep sx) = mincost sx
and so y ←MCC sx. Although, in general, E1 ←E2 is a stronger statement than one
that merely asserts there exists some value v such that v ←E1 ∧v ←E2, that is not
the case here.
To prove (8.2), suppose that k is the smallest integer – assumed to exist – for
which apply k gstep sx is a ﬁnal state. That means

8.2 Huﬀman coding trees
193
until ﬁnal gstep sx = apply k gstep sx
It follows that apply j gstep sx is not a ﬁnal state for 0 ⩽j<k, so, by the stronger
greedy condition, we have
MCC (apply (j+1) gstep sx) ←MCC (apply j gstep sx)
for 0 ⩽j<k. Hence MCC (apply k gstep sx) ←MCC sx. Furthermore, by (8.3) we
have
extract (apply k gstep sx) ←MCC (apply k gstep sx)
establishing (8.2).
This style of reasoning about greedy algorithms is very general. However, unlike
greedy algorithms derived by fusion, it gives no hint as to what form gstep might
take.
Huﬀman coding continued
Returning to Huffman coding, in which candidates are trees, it remains to deﬁne
gstep and to show that the greedy condition holds. For Huffman coding we have
MCC ts = MinWith cost (map unwrap (mkforests ts))
We take gstep to be the function that combines the two trees in the forest with
smallest weights. Since trees are kept in weight order, that means
gstep (t1 :t2 :ts) = insert (Node t1 t2) ts
For the greedy condition, let ts = [t1,t2,...,tn] be a list of trees in weight order, with
weights [w1,w2,...,wn]. The task is to construct a tree t for which
t ←MCC (gstep ts) ∧t ←MCC ts
Suppose t′ ←MCC ts. We construct t by applying tree surgery to t′. Every tree in
ts appears somewhere as a subtree of t′, so imagine that ti appears at depth di in t′
for 1 ⩽i ⩽n. Now, among the subtrees of t′, there will be a pair of sibling trees at
greatest depth. There may be more than one such pair, but there will be at least one.
Suppose two such trees are ti and tj and let d = di = dj. Then d1 ⩽d and d2 ⩽d.
Furthermore, ti and tj could have been chosen as the ﬁrst step in the construction
of t′. Without loss of generality, suppose w1 ⩽wi and w2 ⩽wj. Construct t by
swapping ti with t1 and tj with t2. Then t can be constructed by taking a greedy ﬁrst
step. Furthermore
cost t′ −cost t = d1 w1 +d2 w2 +d(wi +wj)−(d1 wi +d2 wj +d(w1 +w2))
= (d −d1)(wi −w1)+(d −d2)(wj −w2)
⩾0
But cost t′ is as small as possible, so cost t′ = cost t. Hence t ←MCC ts and
t ←MCC (gstep ts).

194
Greedy algorithms on trees
The same tree surgery can be used to show that the stronger greedy condition
holds by a direct argument. Suppose t ←MCC (gstep ts) but t is not a value in
MCC ts. That means there exists a tree t′ ←MCC ts with cost t′ <cost t. We now
get a contradiction by applying the surgical procedure to t′ to produce another tree
t′′ ←MCC (gstep ts) with cost t = cost t′′ ⩽cost t′.
Here is the greedy algorithm we have derived:
huffman es = unwrap (until single gstep (map Leaf es))
where gstep (t1 :t2 :ts) = insert (Node t1 t2) ts
However, simple as it is, the algorithm is not quite ready to leave the kitchen. There
are two sources of inefﬁciency. Firstly, the function insert recomputes weights at
each step, an inefﬁciency that can easily be brushed aside by tupling. The more
serious issue is that, while ﬁnding two trees of smallest weights is a constant-time
operation, inserting the combined tree back into the forest can take linear time in
the worst case. That means the greedy algorithm takes quadratic time in the worst
case. The ﬁnal step is to show how this can be reduced to linear time.
The key observation behind the linear-time algorithm is the fact that, in any
call of gstep, the argument to insert has a weight at least as large as any previous
argument. Suppose we combine two trees with weights w1 and w2 and, later on,
two trees with weights w3 and w4. We have w1 ⩽w2 ⩽w3 ⩽w4, and it follows
that w1 +w2 ⩽w3 +w4. This suggests maintaining the non-leaf trees as a simple
queue, whereby elements are added to the rear of the queue and removed only from
the front. Instead of maintaining a single list we therefore maintain two lists, the
ﬁrst being a list of leaves and the second a queue of node trees. Since elements are
never added to the ﬁrst list, but only removed from the front, the ﬁrst list could
also be a queue. But a simple list sufﬁces. We will call the ﬁrst list a stack simply
to distinguish it from the second one. At each step, gstep selects two lightest trees
from either the stack or the queue, combines them, and adds the result to the end
of the queue. At the end of the algorithm the queue will contain a single tree, the
greedy solution. Figure 8.2, which shows the weights only, gives an example of how
the method works out. The method is viable only if the various queue operations
take constant time. But we have already met symmetric lists in Chapter 3, which
satisfy the requirements exactly.
Here are the details. First we set up the type SQ of Stack-Queues:
type SQ a
= (Stack a,Queue a)
type Stack a = [a]
type Queue a = SymList a
Now we can deﬁne
huffman::[Elem] →Tree Elem
huffman = extractSQ·until singleSQ gstep·makeSQ·map leaf

8.2 Huﬀman coding trees
195
Stack of weights
Queue of combined weights
1, 2, 4, 4, 6, 9
4, 4, 6, 9
1+2
4, 6, 9
4+(1+2)
9
4+(1+2), 4+6
4+6, 9+(4+(1+2))
(4+6)+(9+(4+(1+2)))
Figure 8.2 Example of the stack and queue operations
The component functions on the right-hand side are deﬁned in terms of the type
type Pair = (Tree Elem,Weight)
of pairs of trees and weights. First of all, the functions leaf and node (needed in the
deﬁnition of gstep) are smart constructors that install weight information correctly:
leaf ::Elem →Pair
leaf (c,w) = (Leaf (c,w),w)
node::Pair →Pair →Pair
node (t1,w1) (t2,w2) = (Node t1 t2,w1 +w2)
Next, the function makeSQ initialises a Stack-Queue:
makeSQ::[Pair] →SQ Pair
makeSQ xs = (xs,nilSL)
Recall that the function nilSL returns an empty symmetric list.
Next, the function singleSQ determines whether a Stack-Queue is a singleton,
and extractSQ extracts the tree:
singleSQ::SQ a →Bool
singleSQ (xs,ys) = null xs ∧singleSL ys
extractSQ::SQ Pair →Tree Elem
extractSQ (xs,ys) = fst (headSL ys)
The function singleSL, whose deﬁnition is left as an exercise, tests for whether a
symmetric list is a singleton.
Finally, we deﬁne
gstep::SQ Pair →SQ Pair
gstep ps = add (node p1 p2) rs
where (p1,qs) = extractMin ps
(p2,rs) = extractMin qs
add ::Pair →SQ Pair →SQ Pair
add y (xs,ys) = (xs,snocSL y ys)

196
Greedy algorithms on trees
It remains to deﬁne extractMin for extracting a tree with minimum weight from a
Stack-Queue:
extractMin::SQ Pair →(Pair,SQ Pair)
extractMin (xs,ys)
| nullSL ys
= (head xs,(tail xs,ys))
| null xs
= (headSL ys,(xs,tailSL ys))
| snd x ⩽snd y = (x,(tail xs,ys))
| otherwise
= (y,(xs,tailSL ys))
where x = head xs; y = headSL ys
If both the stack and the queue are nonempty, then the tree with the smallest weight
from either list is selected. If one of the stack and the queue is empty, the selection
is made from the other component.
The linear-time algorithm for Huffman coding depends on the assumption that
the input is sorted into ascending order of weight. If this were not the case, then
O(n log n) steps have to be spent sorting. Strictly speaking, that means Huffman
coding actually takes O(n log n) steps. There is an alternative implementation of the
algorithm with this running time, and that is to use a priority queue. Priority queues
will be needed again, particularly in Part Six, so we will consider them now.
8.3 Priority queues
A priority queue is a data structure PQ for maintaining a list of values so that the
following two operations take at most logarithmic time in the length of the list:
insertQ ::Ord p ⇒a →p →PQ a p →PQ a p
deleteQ::Ord p ⇒PQ a p →((a,p),PQ a p)
The function insertQ takes a value and a priority and inserts the value into the queue
with the given priority. The function deleteQ takes a nonempty queue and extracts a
value whose priority is the smallest, returning the value and its associated priority,
together with the remaining queue. In a max-priority queue the function deleteQ
would extract a value with the largest priority.
As well as the two functions above, we also need some other functions on priority
queues, including
emptyQ ::PQ a p
nullQ
::PQ a p →Bool
addListQ::Ord p ⇒[(a,p)] →PQ a p →PQ a p
toListQ
::Ord p ⇒PQ a p →[(a,p)]
The constant emptyQ represents an empty queue, and nullQ tests for an empty
queue. The function addListQ adds a list of value-priority pairs in one fell swoop,

8.3 Priority queues
197
while toListQ returns a list of value-priority pairs in order of priority. The function
addListQ can be deﬁned in terms of insertQ (see the exercises).
One simple implementation of priority queues is to maintain the queue as a list in
ascending order of priority. But, as we have seen with Huffman’s algorithm, this
means that insertion is a linear-time operation. A better method is to use a heap,
similar to the heaps described in Section 5.3. Using a heap guarantees logarithmic
time for an insertion or deletion, as we will now see.
The relevant data type for heaps is the following:
data PQ a p = Null | Fork Rank a p (PQ a p) (PQ a p)
type Rank
= Nat
A queue is therefore a binary tree. (We use Fork as a constructor rather than Node
to avoid a name clash with Huffman trees, but continue to refer to ‘nodes’ rather
than ‘forks’.) The heap condition is that ﬂattening a queue returns a list of elements
in ascending order of priority:
toListQ::Ord p ⇒PQ a p →[(a,p)]
toListQ Null
= [ ]
toListQ (Fork
x p t1 t2) = (x,p):mergeOn snd (toListQ t1) (toListQ t2)
The deﬁnition of mergeOn is left as an exercise. Thus a queue is a heap in which
the element at a node has a priority that is no larger than the priorities in each of
its subtrees. Each node of the queue stores an additional piece of information, the
rank of that node. By deﬁnition, the rank of a tree is the length of the shortest path
in the tree from the root to a Null tree. A queue is not just a heap but a variety
called a leftist heap. A tree is leftist if the rank of the left subtree of any node is no
smaller than the rank of its right subtree. This property makes heaps taller on the
left, whence its name. A simple consequence of the leftist property is that the length
of the shortest path from the root of a tree to a Null is always along the right spine
of the tree. We leave it as an exercise to show that, for a tree of size n, this length is
at most ⌊log(n+1)⌋.
We can maintain rank information with the help of a smart constructor fork:
fork ::a →p →PQ a p →PQ a p →PQ a p
fork x p t1 t2
| r2 ⩽r1
= Fork (r2 +1) x p t1 t2
| otherwise = Fork (r1 +1) x p t2 t1
where r1 = rank t1; r2 = rank t2
rank ::PQ a p →Rank
rank Null
= 0
rank (Fork r
) = r

198
Greedy algorithms on trees
In order to maintain the leftist property, the two subtrees are swapped if the left
subtree has lower rank than the right subtree.
Two leftist heaps can be combined into one by the function combineQ, where
combineQ::Ord p ⇒PQ a p →PQ a p →PQ a p
combineQ Null t = t
combineQ t Null = t
combineQ (Fork k1 x1 p1 l1 r1) (Fork k2 x2 p2 l2 r2)
| p1 ⩽p2
= fork x1 p1 l1 (combineQ r1 (Fork k2 x2 p2 l2 r2))
| otherwise = fork x2 p2 l2 (combineQ (Fork k1 x1 p1 l1 r1) r2)
In the worst case, combineQ traverses the right spines of the two trees. Hence the
running time of combineQ on two leftist heaps of rank at most r is O(log r) steps.
Now we can deﬁne the insertion and deletion operations (the functions emptyQ and
nullQ are left as exercises):
insertQ::Ord p ⇒a →p →PQ a p →PQ a p
insertQ x p t = combineQ (fork x p Null Null) t
deleteQ::Ord p ⇒PQ a p →((a,p),PQ a p)
deleteQ (Fork
x p t1 t2) = ((x,p),combineQ t1 t2)
Both operations take logarithmic time in the size of the queue. Summarising, by
using a priority queue of n elements rather than an ordered list we can reduce the
time for an insertion to O(log n) steps rather than O(n) steps. The price paid for
this reduction is that the time to ﬁnd a smallest value goes up from O(1) steps to
O(log n) steps.
Finally, here is the implementation of Huffman’s algorithm using a priority queue:
huffman::[Elem] →Tree Elem
huffman = extract ·until singleQ gstep·makeQ·map leaf
extract ::PQ (Tree Elem) Weight →Tree Elem
extract = fst ·fst ·deleteQ
gstep::PQ (Tree Elem) Weight →PQ (Tree Elem) Int
gstep ps = insertQ t w rs
where (t,w)
= node p1 p2
(p1,qs) = deleteQ ps
(p2,rs) = deleteQ qs
makeQ::Ord p ⇒[(a,p)] →PQ a p
makeQ xs = addListQ xs emptyQ
singleQ::Ord p ⇒PQ a p →Bool
singleQ = nullQ·snd ·deleteQ

8.4 Chapter notes
199
This algorithm runs in O(n log n) steps without making the assumption that the
input is sorted by weight.
8.4 Chapter notes
The minimum-cost tree problem was ﬁrst described in [1]. Another way to build
a minimum-cost tree is to use either the Hu–Tucker [2] or the Garsia–Wachs algo-
rithm [5]. The Hu–Tucker algorithm applies because cost is a regular cost function
as deﬁned in [2]. But the best implementation of the Hu–Tucker algorithm takes
Θ(n log n) steps. The Garsia–Wachs algorithm will be discussed in Section 14.6.
Huffman’s algorithm is a ﬁrm favourite in the study of greedy algorithms. It ﬁrst
appeared in [3]. The linear-time greedy algorithm based on queues is described
in [4], which also shows how the algorithm can be generalised to deal with k-ary
trees rather than just binary trees. If one insists that the fringe of the tree is exactly
the given character–weight pairs in the order they are given, then the resulting tree,
called an alphabetic tree by Hu, can be built using the Garsia–Wachs algorithm.
There are many implementations of priority queues, including leftist heaps, skew
heaps, and maxiphobic heaps. All these can be found in [6, 7].
References
[1]
Richard Bird. Pearls of Functional Algorithm Design. Cambridge University Press,
Cambridge, 2010.
[2]
Te Chiang Hu. Combinatorial Algorithms. Addison-Wesley, Reading, MA, 1982.
[3]
David A. Huffman. A method for the construction of minimum-redundancy codes.
Proceedings of the IRE, 40(9):1098–1101, 1952.
[4]
Donald E. Knuth. The Art of Computer Programming, volume 1: Fundamental
Algorithms. Addison-Wesley, Reading, MA, third edition, 1997.
[5]
Donald E. Knuth. The Art of Computer Programming, volume 3: Sorting and
Searching. Addison-Wesley, Reading, MA, second edition, 1998.
[6]
Chris Okasaki. Purely Functional Data Structures. Cambridge University Press,
Cambridge, 1998.
[7]
Chris Okasaki. Fun with binary heap trees. In J. Gibbons and O. de Moor, editors,
The Fun of Programming, pages 1–16. Palgrave, Macmillan, Hampshire, 2003.
Exercises
Exercise 8.1 Consider the recurrence H(1) = 0 and H(n) = 1+H(⌈n/2⌉. Prove by
induction that H(n) = ⌈log n⌉.
Exercise 8.2 Prove that the bottom-up algorithm
mktree = unwrap·until single (pairWith Node)·map Leaf
of Section 8.1 produces a tree of minimum height.

200
Greedy algorithms on trees
Exercise 8.3 We claimed in Section 8.1 that minimising lcost also minimises cost.
Why is this true?
Exercise 8.4 Why is the claim rollup·spine = id not true for all possible lists of
trees?
Exercise 8.5 The (context-free) fusion rule for foldrn asserts that
foldrn f2 g2 xs ←M (foldrn f1 g1 xs)
for all ﬁnite lists xs, provided
g2 x
←M (g1 x)
f2 x (M y) ←M (f1 x y)
Prove this result.
Exercise 8.6 Specialise the ﬁnal greedy algorithm of Section 8.1 as suggested to
build a minimum-height tree.
Exercise 8.7 The function splits :: [a] →[([a],[a])] splits a list xs into all pairs
of lists (ys,zs) such that xs = ys ++zs. The function splitsn is similar, except that
it splits a list into pairs of nonempty lists. Give recursive deﬁnitions of splits and
splitsn.
Exercise 8.8 Using splitsn, give a recursive deﬁnition of the function mktrees of
Section 8.1. Write down a recurrence relation for the function T(n) that counts the
number of trees with n leaves. It can be shown that
T(n) = 1
n
2n−2
n−1

These values are called the Catalan numbers.
Exercise 8.9 Here is another way of deﬁning the function mktrees of Section 8.1,
one similar to that used in Huffman coding:
mktrees::[a] →[Tree a]
mktrees = map unwrap·until (all single) (concatMap combine)·
wrap·map Leaf
combine::Forest a →[Forest a]
combine xs = [ys++[Node x y]++zs | (ys,x:y:zs) ←splits xs]
The function combine combines two adjacent trees in a forest in all possible ways.
The process is repeated until only singleton forests remain, forests that consist of
just one tree. Finally the trees are extracted to give a list of trees. This method
may generate the same tree more than once, but all possible trees are nevertheless
produced. Write down the associated greedy algorithm for this version of mktrees
(no justiﬁcation is required).

Answers
201
Exercise 8.10 In Huffman coding, why does the second, recursive deﬁnition of
cost follow from the ﬁrst?
Exercise 8.11 Deﬁne the function insert used in Huffman’s algorithm.
Exercise 8.12 Give the two ways that the tree
[Node (Node (Leaf 3) (Leaf 8)) (Node (Leaf 5) (Leaf 9))]
can be generated from [Leaf 3,Leaf 5,Leaf 8,Leaf 9].
Exercise 8.13 The number of trees generated in the speciﬁcation of Huffman’s
algorithm is given for n ⩾2 by
n
2
n−1
2

···
2
2

Show that this number equals
n!(n−1)!
2n−1
Exercise 8.14 Deﬁne MCC k xs = MinWith cost (apply k fstep [xs]). Show that
apply k gstep xs ←MCC k xs
provided MCC k (gstep xs) ←MCC (k +1) xs.
Exercise 8.15 Deﬁne the function singleSL :: SymList a →Bool for determining
whether a symmetric list is a singleton.
Exercise 8.16 Deﬁne addListQ in terms of insertQ.
Exercise 8.17 Deﬁne mergeOn.
Exercise 8.18 Show that, for the trees considered in Section 8.3, a tree of size n
has rank at most ⌊log(n+1)⌋.
Exercise 8.19 Deﬁne emptyQ and nullQ.
Answers
Answer 8.1 The base case is immediate and the induction step follows from
⌈log n⌉= 1+⌈log⌈n/2⌉⌉
This equation can be proved by showing
⌈log n⌉⩽k ⇔1+⌈log⌈n/2⌉⌉⩽k
for any k. Both sides reduce to n ⩽2k by appeal to the rule of ceilings, establishing
the result.

202
Greedy algorithms on trees
Answer 8.2 For a list of length n the bottom-up algorithm builds a tree t whose left
child is a perfectly balanced binary tree with 2k leaves, where 2k <n ⩽2k+1. The
height of t is therefore k +1 = ⌈log n⌉, the smallest height possible.
Answer 8.3 Because cost = head ·lcost and
us ⩽vs ⇒head us ⩽head vs
Answer 8.4 The function spine returns the undeﬁned value on trees with an inﬁnite
spine, so the equation fails.
Answer 8.5 The base case is easy, and the induction step is
foldrn f2 g2 (x:xs)
=
{ deﬁnition of foldrn }
f2 x (foldrn f2 g2 xs)
←
{ induction }
f2 x (M (foldrn f1 g1 xs))
←
{ fusion condition }
M (f1 x (foldrn f1 g1 xs))
=
{ deﬁnition of foldrn }
M (foldrn f1 g1 (x:xs))
Answer 8.6 The algorithm is
greedy = rollup·map fst ·foldrn insert (wrap·leaf)
where insert x ts = leaf x:join ts
join [u]
= [u]
join (u:v:ts) = if snd u<snd v then u:v:ts else join (node u v:ts)
leaf x = (Leaf x,0)
Answer 8.7 The deﬁnitions are
splits [ ]
= [([ ],[ ])]
splits (x:xs) = ([ ],x:xs):[(x:ys,zs) | (ys,zs) ←splits xs]
splitsn [ ]
= [ ]
splitsn [x]
= [ ]
splitsn (x:xs) = ([x],xs):[(x:ys,zs) | (ys,zs) ←splitsn xs]
Answer 8.8 We have
mktrees [x] = [Leaf x]
mktrees xs = [Node u v | (ys,zs) ←splitsn xs,
u ←mktrees ys,v ←mktrees zs]
The recurrence relation is given by T(1) = 1 and, for n>1,

Answers
203
T(n) =
n−1
∑
k=1
T(k) T(n−k)
Answer 8.9 The greedy algorithm is
mct ::[Nat] →Tree Nat
mct = unwrap·until single combine·map Leaf
combine::Forest Nat →Forest Nat
combine ts = us++[Node u v]++vs
where (us,u:v:vs) = bestjoin ts
The omitted function bestjoin splits a forest into two sub-forests in which the
ﬁrst two trees of the second forest are trees whose combined cost is minimal. For
example, for the input [5,3,1,4,2,2] this version of mct produces the tree
5
3
1
4
2
2
4
5
6
3
7
In the case of a minimum-height tree this greedy algorithm can be simpliﬁed to give
the bottom-up algorithm described at the beginning of the chapter.
Answer 8.10 If the cost of tree u is ∑wi li and the cost of tree v is ∑w′i l′i, then the
cost of Node u v is
∑wi (li +1)+∑w′i (l′i +1) = cost u+cost v+weight u+weight v
Answer 8.11 We can implement insert by linear search, leading to
insert ::Tree Elem →Forest Elem →Forest Elem
insert t1 [ ]
= [t1]
insert t1 (t2 :ts) = if weight t1 ⩽weight t2 then t1 :t2 :ts else t2 :insert t1 ts
Answer 8.12 The same tree is generated either by combining Leaf 3 and Leaf 8 as
a ﬁrst step, followed by combining Leaf 5 and Leaf 9, or vice versa.
Answer 8.13 The proof is by induction. Both expressions equal 1 for n = 2, and
the induction step is an easy calculation.

204
Greedy algorithms on trees
Answer 8.14 The proof is by induction. The case k = 0 is immediate, and for the
induction step we can argue as follows:
apply (k +1) gstep xs
=
{ deﬁnition of apply }
apply k gstep (gstep xs)
←
{ induction }
MCC k (gstep xs)
←
{ given }
MCC (k +1) xs
Answer 8.15 The deﬁnition is
singleSL::SymList a →Bool
singleSL (xs,ys) = (null xs ∧single ys) ∨(null ys ∧single xs)
Answer 8.16 We can deﬁne
addListQ xs q = foldr (uncurry insertQ) q xs
Answer 8.17 We have
mergeOn::Ord b ⇒(a →b) →[a] →[a] →[a]
mergeOn key xs [ ] = xs
mergeOn key [ ] ys = ys
mergeOn key (x:xs) (y:ys)
| key x ⩽key y = x:mergeOn key xs (y:ys)
| otherwise
= y:mergeOn key (x:xs) ys
Answer 8.18 Essentially we have to show that a tree of rank r and size n satisﬁes
2r −1 ⩽n. Such a tree has one node at depth 0, two nodes at depth 1, and so on.
The tree also has 2r−1 nodes at depth r −1. All this is because the ﬁrst null node
does not appear until level r. Hence the size of the tree is at least
1+2+···+2r−1 = 2r −1
which establishes the claim.
Answer 8.19 We have
emptyQ = Null
nullQ Null = True
nullQ
= False

Chapter 9
Greedy algorithms on graphs
In this chapter we consider two problems for which the candidates are graphs, in fact
special forms of graph called spanning trees. The ﬁrst problem is about computing
a spanning tree with minimum cost for a connected graph, while the second is about
computing a spanning tree for a directed graph whose edges determine a shortest
path from a given starting vertex to all other vertices. All these terms are made
precise below. The shortest-paths algorithm is then used to solve another problem,
called the jogger’s problem, for computing a cyclic path with minimum total cost.
9.1 Graphs and spanning trees
We start with some terminology. There are two kinds of graph, a directed graph, also
called a digraph, and an undirected graph, just called a graph. Certain deﬁnitions
are slightly different for digraphs and graphs, so it is best to consider them as
separate though closely related species. The minimum-cost spanning tree problem
deals with graphs, while the shortest-paths problem deals with digraphs.
By deﬁnition, a digraph D is a pair (V,E), where V is a set of vertices, also called
nodes, and E is a set of edges. An edge consists of a pair of vertices (u,v), where u
is the source of the edge and v is the target. Such an edge is directed from u to v. In
a digraph it is possible to have loops – edges of the form (u,u). Because E is a set it
cannot contain an edge more than once, so there is at most one edge with the same
source and target. It follows that a digraph of n vertices cannot have more that n2
edges, or n(n−1) edges if there are no loops.
A graph G is also given by a pair (V,E) of vertices and edges, but this time each
edge is a set {u,v} of exactly two vertices. It follows that graphs cannot have loops.
For the purposes of representation we write (u,v) for this set of two vertices, but
(u,v) and (v,u) are considered to be the same edge. A graph of n vertices cannot
have more than n(n−1)/2 edges. In a sparse graph or digraph with n vertices and
e edges we have e = O(n), while in a dense graph or digraph we have e = Ω(n2).

206
Greedy algorithms on graphs
Certain algorithms are better tailored for sparse graphs, while others are better for
dense graphs.
For the purposes of this chapter we will need labelled graphs and digraphs.
By deﬁnition, a labelled graph or digraph is a graph in which each edge carries a
label, usually called its weight. For simplicity, weights are assumed to be integers.
The weight of an edge is recorded along with the edge, so both for graphs and for
digraphs the relevant type declarations are:
type Graph = ([Vertex],[Edge])
type Edge
= (Vertex,Vertex,Weight)
type Vertex = Int
type Weight = Int
We will also ﬁx on the deﬁnitions
nodes (vs,es)
= vs
edges (vs,es)
= es
source (u,v,w) = u
target (u,v,w) = v
weight (u,v,w) = w
The representation of a graph as lists of vertices and edges mirrors the mathematical
deﬁnition and is acceptable for many problems. For other problems an alternative
representation is superior. This is to view a graph as an adjacency function of type
Vertex →[(Vertex,Weight)]. The domain of this function is the set of vertices, and
for each vertex u the value of the function applied to u is a set of pairs (v,w) such
that (u,v,w) is a labelled edge. Assuming that vertices are named by integers in the
range 1 to n for some n, a simple implementation of the adjacency function is by an
array, so an alternative description of a graph is
type AdjArray = Array Vertex [(Vertex,Weight)]
We leave it as an exercise to convert between the two descriptions. The adjacency
array representation of a graph is used for some of the problems in Part Six.
A path in a graph or digraph is a sequence [v0,v1,...,vk] of vertices such that
(vj,vj+1) is an edge (directed from vj to vj+1 in the case of digraphs) for 0 ⩽j<k.
Such a path connects v0 and vk. A cycle in a graph or digraph is a path [v0,v1,...,v0]
whose edges and vertices are all distinct apart from the two endpoints. In a graph
the path [v0,v1,v0] is not a cycle because (v0,v1) and (v1,v0) are the same edge;
consequently, cycles in a graph have lengths greater than two. A graph or digraph
is acyclic if there are no cycles; a graph is connected if there is a path from every
vertex to every other vertex. In an acyclic graph there can be at most one path
between any two vertices.
A connected acyclic graph is called a tree, and a set of trees is called a forest:

9.1 Graphs and spanning trees
207
type Tree
= Graph
type Forest = [Tree]
Trees are therefore not a data type in the sense used in previous chapters, but merely
a synonym for a special kind of graph. Every graph can be decomposed into the set
of its connected components.
A spanning forest of a graph G = (V,E) is a disjoint set of trees
(V1,E1),(V2,E2),...,(Vk,Ek)
with V = 
1⩽i⩽k Vi and whose combined edges E′ = 
1⩽i⩽k Ei constitute a maximal
subset of E in the sense that no further edge of G can be added to E′ without creating
a cycle. If G is connected, then a spanning forest consists of a single spanning
tree. A spanning tree of a connected graph with n vertices has exactly n−1 edges
(why?). Finally, a minimum-cost spanning tree (MCST) of a connected graph G
is a spanning tree T of G in which the sum of the weights of the edges in T is as
small as possible. Our aim in this section is to ﬁnd efﬁcient methods for computing
a MCST of a connected graph. It is left as an exercise to generalise the solutions to
compute a minimum-cost spanning forest of a graph that is not connected.
To add some life to these deﬁnitions, consider a country with a given network of
towns and roads. The towns are the vertices and each road is an edge connecting
two towns. Each road can be travelled in either direction, so the graph is undirected.
We suppose there is at most one road connecting two given towns and the weight
associated with a road is its length. The network may be connected in that there is a
route (a path) between every two towns, but it may not. For instance, the country may
consist of several islands not connected by bridges. If all the towns are connected
by road, then a MCST is a network of roads with no cyclic routes1 and of minimum
total cost connecting all the towns.
Finding a MCST does not help with planning shortest routes. The path between
two towns in a MCST is not necessarily the shortest route between the two towns.
We will consider the problem of planning shortest routes in Section 9.5. Finding a
MCST also does not help with a superﬁcially similar problem in which one is given a
connected graph and a subset T of vertices, with the aim of ﬁnding a minimum-cost
tree that includes every vertex in T. This problem, called the Steiner tree problem,
is much more challenging and beyond the scope of this book.
Here is a speciﬁcation of the MCST problem, expressed in our standard way:
mcst ::Graph →Tree
mcst ←MinWith cost ·spats
The function cost returns the sum of the weights of the edges of the tree:
1 Though routes for cyclists are certainly allowed.

208
Greedy algorithms on graphs
cost ::Tree →Int
cost = sum·map weight ·edges
The function spats (short for ‘spanning trees’) generates all the spanning trees of a
given connected graph. For a graph (V,E) with n vertices, that means ﬁnding all
subsets of E of size n−1 which are both acyclic and connected. One way to deﬁne
spats is to add edges one by one into an initially empty set, ensuring at each step
that the set of edges is acyclic, that is, a forest. Only at the ﬁnal step when the last
edge is added is the forest guaranteed to coalesce into a single tree (provided of
course that the graph is connected). Another way is also to add edges one by one,
but to ensure at each step that the set of edges is both acyclic and connected, that is,
a tree. These two methods for generating spanning trees lead to two different greedy
algorithms, known respectively as Kruskal’s algorithm and Prim’s algorithm. Let us
examine each in turn.
9.2 Kruskal’s algorithm
In Kruskal’s algorithm, the deﬁnition of spats is very similar to the deﬁnition of
mktrees in Huffman’s algorithm, except that it works on a list of states rather than
a list of trees. Each state is a pair consisting of a forest and the list of edges from
which the next edge can be chosen:
type State = (Forest,[Edge])
spats::Graph →[Tree]
spats = map extract ·until (all done) (concatMap steps)·wrap·start
extract ::State →Tree
extract ([t], ) = t
done::State →Bool
done = single·fst
start ::Graph →State
start g = ([([v],[ ]) | v ←nodes g],edges g)
The starting state consists of a forest of trees, each of which is a graph with a
single vertex and no edges, and the full set of edges of the graph. A ﬁnal state is a
pair consisting of a singleton tree and the list of edges not used in its construction.
The function extract takes a ﬁnal state, discards the unused edges, and extracts the
spanning tree.
That leaves us with the deﬁnition of steps::State →[State], which takes a forest
and a list of edges and selects every possible edge that can be added to the forest
without creating a cycle. An edge can be so added if its endpoints belong to different

9.2 Kruskal’s algorithm
209
trees. The result is a forest in which the two trees are combined into one larger tree.
Hence we deﬁne
steps::State →[State]
steps (ts,es) = [(add e ts,es′) | (e,es′) ←picks es,safeEdge e ts]
Recall that the function picks::[a] →[(a,[a])] picks an element of a nonempty list
in all possible ways, returning both the element and the remaining list. The function
safeEdge is deﬁned by
safeEdge::Edge →Forest →Bool
safeEdge e ts = ﬁnd ts (source e) ̸= ﬁnd ts (target e)
ﬁnd ::Forest →Vertex →Tree
ﬁnd ts v = head [t | t ←ts,any (== v) (nodes t)]
The value ﬁnd ts v is the unique tree in the forest ts that contains v as one of its
vertices. Each ﬁnd operation can take Θ(n) steps in the worst case, since every vertex
of the graph may have to be inspected. A more efﬁcient deﬁnition is given later on.
Finally, the function add combines two trees and adds the result to the forest:
add ::Edge →Forest →Forest
add e ts = (nodes t1 ++nodes t2,e:edges t1 ++edges t2):rest
where t1
= ﬁnd ts (source e)
t2
= ﬁnd ts (target e)
rest = [t | t ←ts,t ̸= t1 ∧t ̸= t2]
It follows that each add operation, like ﬁnd, takes O(n) steps (ignoring tree compar-
isons). Again, a more efﬁcient deﬁnition is given later on.
The greedy algorithm for computing a minimum-cost spanning tree is obtained
by following the path mapped out by the theory in the previous chapter. First, deﬁne
MCC = MinWith cost ·map extract ·until (all done) (concatMap steps)·wrap
Recall that we have
extract (until done gstep sx) ←MCC sx
for all states sx, provided two conditions are satisﬁed. The ﬁrst one is
done sx ⇒extract sx ←MCC sx
This condition follows from the deﬁnition of MCC, since
extract ([t],es) = t ←MCC ([t],es)
The second condition is the greedy condition: there exists a tree t such that
t ←MCC (gstep sx) ∧t ←MCC sx
To verify the greedy condition we have to choose a deﬁnition of gstep. The obvious
choice is to deﬁne gstep to select a safe edge of minimum weight. Assuming the list
of edges is in ascending order of weight, we can deﬁne gstep by

210
Greedy algorithms on graphs
gstep::State →State
gstep (ts,e:es) = if t1 ̸= t2 then (ts′,es) else gstep (ts,es)
where t1
= ﬁnd ts (source e)
t2
= ﬁnd ts (target e)
ts′
= (nodes t1 ++nodes t2,e:edges t1 ++edges t2):rest
rest = [t | t ←ts,t ̸= t1 ∧t ̸= t2]
The function gstep selects the ﬁrst edge whose endpoints are in different trees t1
and t2, and combines t1 and t2 into one tree.
Now, to verify the greedy condition, consider a state sx = (ts,es) consisting of a
forest ts and a list es of unused edges. Let e be an element of es of lightest weight that
is a safe edge for ts. Suppose t ←MCC sx. If t contains the edge e, then t can always
be constructed by choosing e as a ﬁrst step. Hence t ←MCC (gstep sx) and the
greedy condition is satisﬁed. Otherwise, t does not contain e, and adding e to t would
create a (unique) cycle. Remove any edge e′ in the cycle and replace it with e. The
result is another spanning tree t′ with cost t′ ⩽cost t, because weight e ⩽weight e′.
Furthermore, since t′ contains e and t′ can be constructed by choosing e as a ﬁrst
step, we can take t = t′ to satisfy the greedy condition.
Hence one way to formulate Kruskal’s algorithm is as follows:
kruskal::Graph →Tree
kruskal = extract ·until done gstep·start
start g = ([([v],[ ]) | v ←nodes g],sortOn weight (edges g))
The function sortOn in the Haskell library Data.List appeared in Exercise 5.12.
There is another way to formulate the algorithm, which is to write
kruskal::Graph →Tree
kruskal g = extract (apply (n−1) gstep (start g))
where n = length (nodes g)
Given a connected graph with n vertices, we know that gstep will be applied exactly
n−1 times.
It remains to time the program. Suppose the graph has n vertices and e edges. As
the graph is assumed to be connected and there is at most one edge between any
two vertices, we have n−1 ⩽e ⩽n(n−1)/2. Sorting the edges takes O(e log e)
steps. As we have seen, each ﬁnd and add operation takes O(n) steps. In the worst
case, all e edges may have to be considered, so there are 2e calls of ﬁnd and n−1
calls of add, for a total running time of O(e log e+en+n2) = O(en) steps.
The bottleneck in this algorithm is the time complexity of ﬁnd and add. A faster
implementation of these functions makes use of a special data structure for comput-
ing with disjoint sets. We turn to this topic next.

9.3 Disjoint sets and the union–ﬁnd algorithm
211
9.3 Disjoint sets and the union–ﬁnd algorithm
The computationally expensive part of Kruskal’s algorithm lies in the maintenance
of a collection of disjoint sets, the vertices of the trees in the forest. Initially, each
vertex is in a set by itself. Each union operation reduces the number of disjoint
sets by one. The function ﬁnd has to discover which set in the collection contains a
given vertex. Rather than returning the whole set, we can deﬁne ﬁnd to return the
name of the set. The name of a set is some designated vertex in the set. Let DS be
some data type for maintaining disjoint sets of vertices v in the range 1 ⩽v ⩽n for
some n. What we need are the following three operations on DS, in which Name is
a synonym for Vertex:
startDS ::Nat →DS
ﬁndDS
::DS →Vertex →Name
unionDS::Name →Name →DS →DS
The function startDS takes a positive integer n and returns a collection of n singleton
sets, each containing a unique vertex v in the range 1 ⩽v ⩽n. The function ﬁndDS
takes a vertex v and returns the name of the set in the collection that contains v. The
function unionDS takes two different names and replaces the two named sets in the
collection by a single set with an appropriately chosen name, in fact the name of the
larger set.
Here is the implementation of Kruskal’s algorithm that uses these three functions.
The disjoint sets of vertices are separated out from the trees in the forest, and all the
tree edges are combined into one set. Thus we change the state to read
type State = (DS,[Edge],[Edge])
Then we can deﬁne
kruskal::Graph →Tree
kruskal g = extract (apply (n−1) gstep s)
where extract ( ,es, ) = (nodes g,es)
n = length (nodes g)
s = (startDS n,[ ],sortOn weight (edges g))
The revised deﬁnition of gstep is
gstep::State →State
gstep (ds,fs,e:es) = if n1 ̸= n2 then (unionDS n1 n2 ds,e:fs,es)
else gstep (ds,fs,es)
where n1 = ﬁndDS ds (source e)
n2 = ﬁndDS ds (target e)
In the simple implementation of Kruskal’s algorithm described above, the three
operations startDS, ﬁndDS, and unionDS can each be implemented to take O(n)

212
Greedy algorithms on graphs
steps. But we can do better with two other implementations, which we will call
implementations A and B. In implementation A, the function ﬁndDS takes O(log n)
steps in the worst case, while unionDS takes O(n) steps. Recalling that Kruskal’s
algorithm may require 2e calls of ﬁndDS and n−1 calls of unionDS, that means
a total running time of O(e log n+n2) steps, a signiﬁcant improvement on O(en)
steps. In implementation B, the function ﬁndDS takes O(log2 n) steps, but unionDS
takes only O(log n) steps. That means a total running time of O(e log2 n+n log n)
steps, again an improvement on O(en) steps.
These timing bounds are not the best that can be achieved: one can construct
implementations of ﬁndDS and unionDS so that a sequence of O(e) ﬁnd operations
and up to n−1 union operations takes O(e log n) steps. However, this implemen-
tation seems impossible to achieve in a purely functional setting because it relies
on mutable arrays with a constant-time update function. Although mutable data
structures can be handled with monadic programming, we choose not to do so. The
so-called Union–Find problem is a well-known example of a problem in which the
complexity of the best purely functional solution seems to be inferior to that of the
best imperative one.
Implementation A of DS also uses an array, but the array is an immutable one.
Recall the following three functions from Section 3.3:
listArray::Ix i ⇒(i,i) →[e] →Array i e
(!)
::Ix i ⇒Array i e →i →e
(//)
::Ix i ⇒Array i e →[(i,e)] →Array i e
The ﬁrst function constructs an array from a pair of bounds and a list of values
in index order, the second is the array lookup function, and the third is an update
function. Building an array takes linear time, a lookup takes constant time, but an
update takes linear time even for an update at a single position. We will use the
following tailored versions of the three operations above:
fromList ::[a] →Array Vertex a
fromList xs = listArray (1,length xs) xs
index::Array Vertex a →Vertex →a
index a v = a!v
update::Vertex →a →Array Vertex a →Array Vertex a
update v x a = a//[(v,x)]
Here is the deﬁnition of DS based on arrays:
type Size = Nat
data DS = DS {names::Array Vertex Vertex,sizes::Array Vertex Size}
The implementation consists of just two arrays, one for naming the sets in the
collection, and one for computing their sizes. The sets themselves can be determined

9.3 Disjoint sets and the union–ﬁnd algorithm
213
from the fact that two elements have the same name if and only if they are in the
same set.
The deﬁnition of startDS is
startDS::Nat →DS
startDS n = DS (fromList [1..n]) (fromList (replicate n 1))
Recall that we assume vertices are labelled from 1 to n for some n. Initially every
set is a singleton set of size 1. The name of a set is the value of its sole occupant. In
the general case, the name of a set is a value k such that
index (names ds) k = k
Each entry in the names array is either a name or a vertex whose entry is either a
name or another vertex with the same property. Thus we can ﬁnd the name of the
set containing a speciﬁed vertex by tracing back in the names array until an entry is
found that points to itself. That gives us the deﬁnition of ﬁndDS:
ﬁndDS::DS →Vertex →Name
ﬁndDS ds x = if x == y then x else ﬁndDS ds y
where y = index (names ds) x
The time complexity of this operation depends on how far away a vertex is from
the name of the set containing it. We will show how this distance is kept small in a
moment. Finally, unionDS is deﬁned by
unionDS::Name →Name →DS →DS
unionDS n1 n2 ds = DS ns ss
where (ns,ss) =
if s1 <s2
then (update n1 n2 (names ds),update n2 (s1 +s2) (sizes ds))
else (update n2 n1 (names ds),update n1 (s1 +s2) (sizes ds))
s1 = index (sizes ds) n1
s2 = index (sizes ds) n2
The ﬁrst two arguments of unionDS are different names, not arbitrary vertices. The
sizes of the sets corresponding to the two names are computed, and the smaller
set is absorbed into the larger by renaming the smaller set with the name of the
larger. Finally, the size of the larger set is increased accordingly. The sole but critical
purpose of maintaining size information is to ensure that the number of ﬁndDS
operations used in looking up the name of a set is as small as possible. If the ﬁrst
lookup does not yield the name of a set, it is because the set has been absorbed into
a larger one. A set of size 1 is absorbed into a set of size at least 1, which in turn is
absorbed into a set of size at least 2, which in turn is absorbed into a set of size 4,
and so on. It follows that, if there are k lookups in a search for the name of a set S,
then S has size at least 2k−1. That gives us the bound k ⩽⌊log n⌋+1.

214
Greedy algorithms on graphs
1
2
3
4
5
6
7
startDS 7
1
2
3
4
5
6
7
1
1
1
1
1
1
1
unionDS 1 2
1
1
3
4
5
6
7
2
1
1
1
1
1
1
unionDS 6 7
1
1
3
4
5
6
6
2
1
1
1
1
2
1
unionDS 3 6
1
1
6
4
5
6
6
2
1
1
1
1
3
1
unionDS 1 6
6
1
6
4
5
6
6
2
1
1
1
1
5
1
Figure 9.1 An example of Union–Find with seven vertices. After each operation,
the two rows show the resulting names and sizes.
An example of the use of these operations is given in Figure 9.1. Observe that the
second rows show the size correctly only for the name of the set; for example, after
unionDS 1 2 the set with name 1 has the correct size 2, but the size associated with
2 (which is no longer a name) remains 1. At the end of the four union operations we
have
map (ﬁndDS ds) [1..7] = [6,6,6,4,5,6,6]
Thus the set of disjoint sets is reduced to three sets: {1,2,3,6,7} with name 6 and
two singleton sets {4} and {5} with names 4 and 5, respectively. In particular, to
ﬁnd the name of the set containing 2 we have to evaluate ﬁndDS three times:
ﬁndDS ds 2 = ﬁndDS ds 1 = ﬁndDS ds 6 = 6
But the set containing 2 is a set of size 5, and ⌊log 5⌋+1 = 3, which is just what
the bound above predicts.
The second implementation, implementation B, of DS uses the data structure of
random-access lists from Chapter 3. This time we have
data DS = DS {names::RAList Vertex,sizes::RAList Size}
The deﬁnition of startDS is
startDS::Nat →DS
startDS n = DS (toRA [1..n]) (toRA (replicate n 1))
toRA::[a] →RAList a
toRA = foldr consRA nilRA
The deﬁnitions of ﬁndDS and unionDS remain the same, except for the changes

9.4 Prim’s algorithm
215
index xs x
= lookupRA (x−1) xs
update n1 n2 xs = updateRA (n1 −1) n2 xs
because positions in random-access lists are indexed from 0 rather than 1.
Now we can restate the various running times. The implementation of unionDS
involves two lookups and two updates, for a total of O(n) steps for implementa-
tion A, and O(log n) steps for implementation B. Implementation A of ﬁndDS takes
O(log n) steps, whereas implementation B takes O(log2 n) steps. With implemen-
tation A the total running time of Kruskal’s algorithm is O(n2) steps on a sparse
graph and O(n2 log n) steps on a dense graph. With implementation B the times
are O(n log2 n) steps for a sparse graph and O(n2 log2 n) steps for a dense graph. It
follows that implementation B is better for sparse graphs, while implementation A
is better for dense graphs.
9.4 Prim’s algorithm
The only difference between Prim’s algorithm and Kruskal’s algorithm is that a tree
is constructed at each step rather than a forest. Here is the revised deﬁnition of states
and spats:
type State = (Tree,[Edge])
spats::Graph →[Tree]
spats g = map fst (until (all done) (concatMap steps) [start g])
where done (t,es) = (length (nodes t) == length (nodes g))
start g = (([head (nodes g)],[ ]),edges g)
This time the starting state is deﬁned by arbitrarily selecting the ﬁrst vertex of g as
the initial tree. The function steps is virtually the same as in Kruskal’s algorithm,
namely
steps::State →[State]
steps (t,es) = [(add e t,es′) | (e,es′) ←picks es,safeEdge e t]
except for different deﬁnitions of add and safeEdge. This time, safeEdge determines
whether an edge has exactly one endpoint in the tree:
safeEdge::Edge →Tree →Bool
safeEdge e t = elem (source e) (nodes t) ̸= elem (target e) (nodes t)
The function add adds an edge to a tree:
add ::Edge →Tree →Tree
add e (vs,es) = if elem (source e) vs then (target e:vs,e:es)
else (source e:vs,e:es)
The greedy algorithm is derived in the same way as Kruskal’s algorithm. First of all,
deﬁne

216
Greedy algorithms on graphs
MCC = MinWith cost ·map fst ·until (all done) (concatMap steps)·wrap
We then have
extract (until done gstep sx) ←MCC sx
provided we can show that there exists a tree t for which
t ←MCC (gstep sx) ∧t ←MCC sx
As before, we can establish the greedy condition by deﬁning gstep to select a safe
edge of minimum weight. Assuming the list of edges is in ascending order of weight,
that means
gstep (t,e:es) = if safeEdge e t then (add e t,es) else keep e (gstep (t,es))
where keep e (t,es) = (t,e:es)
The function keep is needed because, unlike Kruskal’s algorithm, an edge that
cannot be added to a tree at one step could still be added at a later step when the
tree has grown some more.
The proof of the greedy condition is also very similar to that for Kruskal, but
it is worth spelling out the details. Consider an incomplete state sx = (t1,es), so
more edges can be added to t1, and let e be an element of es of lightest weight that
is a safe edge for t1. Without loss of generality, suppose source e is a vertex of t1
and target e is not. Now let t2 ←MCC sx. If t2 contains the edge e, then t2 can be
constructed by choosing e as a ﬁrst step. Hence t2 ←MCC (gstep sx) and we can
choose t = t2 to satisfy the greedy condition. Otherwise, t2 does not contain e and
adding e to t2 would create a (unique) cycle. This time we have to be more careful
in selecting an edge of t2 that can be replaced by e. Observe that among the edges of
the cycle there has to be an edge e′ such that source e′ is a vertex in t1 and target e′
is not. If this were not the case, then e would not be a safe edge for t1. Replacing e′
by e in t2 gives another tree t3 whose cost is no greater than cost t2. And now we
can take t = t3 to satisfy the greedy condition.
The greedy algorithm can be expressed in almost the same way as the ﬁrst version
of Kruskal’s algorithm:
prim::Graph →Tree
prim g = fst (until done gstep (start g))
where done (t,es) = (length (nodes t) == length (nodes g))
As an alternative we can write
prim g = fst (apply (n−1) gstep (start g))
where n = length (nodes g)
with a somewhat more efﬁcient deﬁnition of the termination condition. However,
the main problem with this version of Prim’s algorithm is that it is not very efﬁcient.
At step k, when the tree has k vertices and k −1 edges, the number of edges that

9.4 Prim’s algorithm
217
may have to be checked before ﬁnding a safe edge is O(e−k). That means gstep
takes O(k(e−k)) steps, because safeEdge takes O(k) steps. Summing over all steps
gives a running time of
n−1
∑
k=1
O(k(e−k)) =
n−1
∑
k=1
O(ke) = O(en2)
steps for Prim’s algorithm, compared with O(en) steps for the ﬁrst version of
Kruskal’s algorithm. The bound can be improved by using an efﬁcient implemen-
tation of sets with a membership test that takes logarithmic time. That reduces
the times for safeEdge and add to O(log k) steps, and the total time to O(en log n)
steps. But the result is still worse than Kruskal’s algorithm.
In fact, we can reduce the running time of Prim’s algorithm to O(n2) steps by
reducing the number of edges that have to be considered at each step. The idea is to
maintain for each vertex v off the tree at most one edge, that edge being one of least
weight that connects v to some tree vertex. When the tree is updated with a new
vertex, the candidate edges for the next step can be updated as well. The number
of candidate edges is therefore O(n) at each stage. The result will be a version of
Prim’s algorithm that takes O(n2) steps both for sparse and for dense graphs, which
is better than the O(e log n+n2) bound for Kruskal’s algorithm.
To implement the idea we need two arrays. First we suppose that vertices are
named by integers in the range 1 to n for some n, so they can be used as array
indices. States are redeﬁned to be
type State = (Links,[Vertex])
type Links = Array Vertex (Vertex,Weight)
The ﬁrst component of a state is now an array rather than a tree. The entry for a
vertex v not in the tree is a pair (u,w) for which the edge (u,v,w) is a lightest edge
linking v to any vertex u on the tree. The vertex u is called the parent of v. We
therefore deﬁne
parent ::Links →Vertex →Vertex
parent ls v = fst (ls!v)
weight ::Links →Vertex →Weight
weight ls v = snd (ls!v)
If there is no edge connecting v to a tree vertex, then the parent of v is v itself and
the associated weight is inﬁnitely large. Apart from the root, the parent of a vertex v
in the tree is the vertex of the tree to which v was linked when it was added to the
tree.
The second component of a state is a list of vertices, not edges. These are the
fresh vertices, vertices not yet on the tree. For example, in the following state the
tree vertices are [1,2,3,4,5] while [6,7,8] are fresh:

218
Greedy algorithms on graphs
1
2
3
4
5
6
7
8
7
15
11
4
2
5
The dashed line connecting 4 and 6 indicates that the lightest edge connecting the
vertex 6 to the tree is the edge (4,6,2); similarly, the lightest edge connecting 7 to
the tree is (5,7,5). Vertex 8 has no edges connecting it to the tree. In this state the
ﬁrst component is the array
1
2
3
4
5
6
7
8
parent
1
1
1
3
3
4
5
8
weight
0
7
15
11
4
2
5
∞
The second array is a ﬁxed one and is needed simply to be able to determine the
weight of an edge in constant time rather than having to search through the edges
each time:
type Weights = Array (Vertex,Vertex) Weight
We will leave the deﬁnition of weights::Graph →Weights as Exercise 9.9. The ﬁnal
version of Prim’s algorithm can now be expressed as follows:
prim::Graph →Tree
prim g = extract (apply (n−1) (gstep wa) (start n))
where n
= length (nodes g)
wa = weights g
In the initial state, all vertices are fresh and all except vertex 1 have inﬁnite weights:
start ::Nat →State
start n = (array (1,n) ((1,(1,0)):[(v,(v,maxInt)) | v ←[2..n]]),[1..n])
maxInt ::Int
maxInt = maxBound
The value maxInt, the largest possible element of Int, represents an inﬁnite weight.
Vertex 1 has zero weight and the default parent vertex for each entry is the vertex
itself. The function gstep is deﬁned by
gstep::Weights →State →State
gstep wa (ls,vs) = (ls′,vs′)
where ( ,v) = minimum [(weight ls v,v) | v ←vs]
vs′
= ﬁlter (̸= v) vs
ls′
= accum better ls [(u,(v,wa!(u,v))) | u ←vs′]
better (v1,w1) (v2,w2) = if w1 ⩽w2 then (v1,w1) else (v2,w2)

9.5 Single-source shortest paths
219
The function gstep selects a fresh vertex v closest to the tree and updates the links by
replacing each parent of a fresh vertex u with v and the weight of the edge (u,v) if
the replacement yields a lighter link to the tree. Finally, the function extract extracts
the ﬁnal tree from the ﬁnal state:
extract ::State →Tree
extract (ls, ) = (indices ls,[(u,v,w) | (v,(u,w)) ←assocs ls,v ̸= 1])
Each gstep operation takes O(n) steps, so the revised deﬁnition of prim therefore
takes O(n2) steps. As we will see in the following section, essentially the same
algorithm can be used for computing shortest paths on a directed graph.
9.5 Single-source shortest paths
We turn now to directed graphs and shortest paths. The notion of getting from one
point to another involves a direction of travel, so graphs with directed edges are an
appropriate basis for studying shortest routes. There is no loss in moving to digraphs
because a graph can always be modelled as a digraph by representing each edge
as two directed edges, each with the same weight. Typical of the problems we can
solve using a shortest-paths algorithm is: given a network of streets in a city that
may include one-way streets, what is the shortest route by car from one address to
another?
Normally, the cost of a route is the sum of the lengths (that is, the weights) of
the edges along the route, but there are examples where other aggregation functions
are required. For instance, if you are a hiker and the routes are footpaths, the best
route may be one with the shallowest uphill climb. Each footpath is associated
with a measure of its gradient and the cost of a route is the maximum of the
individual gradients along the path. In such a case, the best route for an unﬁt walker
is one that minimises this cost. As a dual example, some roads may have height
restrictions owing to bridges over the roads. Here the best route for the driver of a
high-sided vehicle is one that maximises the minimum of the heights of the bridges
along a route. In what follows we will focus on distances and their sums, but the
algorithm we will describe, a version of Dijkstra’s algorithm, is easily adapted to
other situations.
Finding a shortest path P from A to B necessarily involves ﬁnding a shortest path
from A to every node along P: shortest paths have shortest sub-paths. In the worst
case, the route to B may be discovered only after ﬁnding the routes from A to all
other nodes in the network. In other words, the algorithm may have to compute a
shortest-paths spanning tree (SPST) rooted at A. Note that a SPST is a different
animal from a MCST. In this section we will concentrate on ﬁnding a SPST for any
digraph for which there is a path from the given source vertex to every other vertex.

220
Greedy algorithms on graphs
The algorithm can be modiﬁed to terminate as soon as the shortest path to a given
destination is discovered.
Until now we have assumed nothing about edge weights except that, for simplicity,
they were integers. But from now on we need the assumption that no weight is
negative. With negative weights there is the possibility of having cycles with negative
costs, and that allows paths with inﬁnite negative costs. Some algorithms can cope
with negative weights (as long as there are no cycles with negative costs), but not
the algorithm we describe. We will see why we need this assumption later on.
Another feature of the problem is that, unlike the case of a MCST, the optimality
of a SPST rooted at A cannot be expressed in terms of a single numerical value.
The cost of a tree depends on the path costs from A to all other vertices on the tree.
The obvious way to state that one tree is no worse than another is to require that
the distances to every vertex in the ﬁrst tree are no greater than the corresponding
distances in the second. This requirement deﬁnes a preorder on trees but not a total
preorder.
A ﬁnal point to bear in mind is that the algorithm we will discuss is not the one
found in a car navigation system for computing real-life shortest routes. Actual road
networks are based on real distances, and adjacent towns in the network are more or
less closer than towns separated by long routes. That means certain heuristics can
be employed for ﬁnding shortest routes quickly. The resulting algorithm, called the
A* search algorithm, will be discussed in Chapter 16.
9.6 Dijkstra’s algorithm
Our shortest-paths spanning tree algorithm, a version of Dijkstra’s algorithm, uses
essentially the same deﬁnition of states as in the ﬁnal version of Prim’s algorithm,
except that edge weights in the links array are replaced by distances, where the
distance from the source vertex 1 to vertex v is the sum of the weights of the edges
along the path from 1 to v:
type State
= (Links,[Vertex])
type Links
= Array Vertex (Vertex,Distance)
type Distance = Int
parent ::Links →Vertex →Vertex
parent ls v = fst (ls!v)
distance::Links →Vertex →Distance
distance ls v = snd (ls!v)
For example, the state

9.6 Dijkstra’s algorithm
221
1
2
3
4
5
6
7
10
9
11
4
6
1
2
2
1
1
Figure 9.2 An example digraph
1
2
3
4
5
6
7
8
7
15
11
4
2
5
in which the fresh vertices are [6,7,8] is represented by the array
1
2
3
4
5
6
7
8
parent
1
1
1
3
3
4
5
8
distance
0
7
15
26
19
28
24
∞
In particular, the fresh vertex closest to the tree is vertex 7, with a distance from
vertex 1 of 15+4+5 = 24.
Except for one or two small changes, Dijkstra’s algorithm is identical to Prim’s
algorithm:
dijkstra::Graph →Tree
dijkstra g = extract (apply (n−1) (gstep wa) (start n))
where n
= length (nodes g)
wa = weights g
The function weights has to be deﬁned differently from how it was in Prim’s
algorithm because we are now dealing with a directed graph (see Exercise 9.9). The
functions start and extract are exactly the same as in Prim’s algorithm, and gstep is
deﬁned by
gstep::Weights →State →State
gstep wa (ls,vs) = (ls′,vs′)
where (d,v) = minimum [(distance ls v,v) | v ←vs]
vs′
= ﬁlter (̸= v) vs
ls′
= accum better ls [(u,(v,sum d (wa!(v,u)))) | u ←vs′]
where sum d w = if w == maxInt then maxInt else d +w
better (v1,d1) (v2,d2) = if d1 ⩽d2 then (v1,d1) else (v2,d2)

222
Greedy algorithms on graphs
vertex
1
2
3
4
5
6
1
1
2
3
4
5
6
0
∞
∞
∞
∞
∞
2
1
1
1
4
5
6
0
7
10
∞
∞
∞
3
1
1
2
2
5
6
0
7
9
16
∞
∞
6
1
1
2
2
3
3
0
7
9
16
13
10
5
1
1
2
2
6
3
0
7
9
16
11
10
4
1
1
2
5
6
3
0
7
9
12
11
10
Figure 9.3 A sequence of ﬁve greedy steps
Each application of gstep selects a fresh vertex v of minimum distance from the
source vertex 1. There are n−1 fresh vertices, so gstep is applied n−1 times. After
selecting v, the function gstep updates the parents and distances for each fresh vertex
u whenever there is a path to u going through v that is shorter. For instance, in the
example above, adding 7 as a new tree node, with distance 24, we may ﬁnd an edge
(7,6,1), so the distance from 1 to the fresh vertex 6 can be reduced to 24+1, which
is better than the current best distance 28. Note the necessity for the function sum
in the deﬁnition of gstep. The reason is that if (v,u) is not an edge, so its weight is
maxInt, then the new distance of u from the source vertex should also be maxInt.
That requires d +maxInt = maxInt for any ﬁnite distance d, an equation that does
not hold in Haskell.
The function extract extracts the spanning tree as a graph, but a better result is to
return the actual paths from the source node to each other vertex:
type Path = ([Vertex],Distance)
extract ::State →[Path]
extract (ls, ) = [(reverse (getPath ls v),distance ls v) | v ←indices ls]
getPath ls v
= if u == v then [u] else v:getPath ls u
where u = parent ls v
Let us walk through an example to show how Dijkstra’s algorithm works out in
practice. Consider the digraph of Figure 9.2 in which n = 6. There is a path from the
source vertex 1 to every other vertex, so it is possible to construct a SPST rooted at
vertex 1. Figure 9.3 shows the sequence of n−1 greedy steps. The vertex on the left
is the vertex found at the beginning of each step. The ﬁnal distances in Figure 9.3 are

9.6 Dijkstra’s algorithm
223
1
2
3
4
5
6
7
1
2
1
1
Figure 9.4 The shortest-paths spanning tree from vertex 1
the costs of the shortest paths from the source vertex 1 to all vertices. In particular,
the shortest route to vertex 4 has cost 12 and is along the path [1,2,3,6,5,4]. The
spanning tree is shown in Figure 9.4.
It remains to prove that Dijkstra’s algorithm works correctly. We did not give a
deﬁnition of the list of all shortest-paths spanning trees, so a proof based on the
generic greedy condition of the previous chapter is not available to us. Instead, we
give a direct proof. We show that, at each step, the distance recorded in the state for
every vertex on the tree is indeed the shortest distance from the source. In symbols,
if
(ls,vs) = apply k (gstep wa) (start n)
then for all tree vertices v (those not in vs), we have
distance ls v = shortest g v
where shortest g v is the cost of the shortest path in the graph g from the source
vertex to v. The proof of the claim is by induction on k. The base case k = 0 is
immediate since the only tree vertex is the source vertex 1 and the shortest path is
the empty path with distance 0. For the induction step, let v be the vertex selected
by gstep and let P be a path of shortest distance in the graph from the source vertex
to v. Such a path has to contain a fresh vertex because v itself is fresh. Suppose that
(x,y,w) is the ﬁrst edge in P for which y is fresh. Since x is a tree vertex, and the
distances to tree vertices are never changed once they are set, we have by induction
that
distance ls x = shortest g x
After selecting x, the function gstep updates the distances to each fresh vertex,
including y, and, since such distances are never increased, we have
distance ls y ⩽distance ls x+w = shortest g x+w = shortest g y
Hence, since computed distances are never shorter than the shortest possible distance,
we have distance ls y = shortest g y.
We can now reason

224
Greedy algorithms on graphs
distance ls v
⩽
{ deﬁnition of v as a closest fresh vertex as y is fresh }
distance ls y
=
{ above }
shortest g y
⩽
{ since P passes through y }
shortest g v
So, by the same argument as before, distance ls v = shortest g v. Note that it is in
the very last step that we exploit the fact that edge weights are not negative, so the
initial section of the path P to y cannot cost more than P itself.
We have introduced Dijkstra’s algorithm as a variant of Prim’s algorithm, but
there is another way of formulating Dijkstra’s algorithm, namely as a version of
breadth-ﬁrst search, a topic we will take up in Part Six (see Chapter 16 for details).
9.7 The jogger’s problem
Finally, here is one application of Dijkstra’s algorithm. Consider the plight of a
reluctant jogger who, while willing to undertake exercise, wishes to suffer as little
unpleasantness as possible. The jogger is confronted with a network of footpaths,
each of which possesses some nonnegative measure of undesirability, say its length.
Beginning at some speciﬁed point, called ‘home’, the jogger wishes to plan a circular
route, no footpath being traversed more than once, of minimum total undesirability.
We will suppose that the undesirability of a footpath is independent of the direction
of travel, so we are dealing with an undirected network of footpaths. Such a route
will be a cycle in the network, that is, a circular path consisting of distinct vertices
(footpath junctions) as well as distinct edges (why?).
Abstractly put, the problem is to determine, given a graph G = (V,E) and a
speciﬁed home vertex a, a cycle that begins and ends at a and is of minimum total
cost, where the cost of each individual footpath is some given positive value. Since
no footpath can be travelled more than once, there must be at least three different
edges in the cycle. In what follows we assume G is a connected graph and that such
a cycle exists. For example, the graph
1
2
3
4

9.7 The jogger’s problem
225
has ﬁve possible cycles from the source node 1, each of which can be travelled in
either direction: 1231,12341,1241,12431,1341.
A simple method for computing a minimum jog J in a graph G is to observe that
the path P deﬁned by J from a to the last vertex, say x, before returning to a via the
edge (x,a) has to be a shortest path from a to x in a modiﬁed graph G(x) in which
the edge between a and x is removed. If there were a shorter path, then there would
be a shorter cycle. That means Dijkstra’s algorithm can be used on G(x) to ﬁnd P,
provided that each undirected edge in G is replaced by two directed edges with the
same weight. A best possible jog can then be found by running Dijkstra’s algorithm
on all graphs G(x) for which x is incident on a. Since Dijkstra’s algorithm can take
Θ(n2) steps, this method can take Θ(n3) steps if there are Θ(n) edges incident on a.
The algorithm works equally well for both graphs and digraphs. Nevertheless, there
seems to be a lot of duplicated effort in the method, so it is sensible to ask whether
there is a way of using Dijkstra’s algorithm just once to solve the jogger’s problem.
The answer is yes, as we will now see.
Let T be a shortest-path spanning tree of a graph G rooted at vertex a. We are
going to show that there is some minimum jog J of G with the property that all the
constituent edges of J are in T except one. There has to be at least one such edge
since T is acyclic. This property is called the single-edge property.
Let J be a minimum jog with the fewest number of non-T edges. Suppose x is
the ﬁrst vertex in J such that the edge from x is not in T, and let y be the last vertex
such that the edge to y is not in T. The case x = a is not excluded, nor is y = a, but
x and y have to be different vertices. Since the graph is undirected, the roles of x
and y are dual. Here is a picture in which solid lines are paths in T:
a
x
y
Our aim is to show that the dashed line is a single non-T edge. Using the notation
(u···v)G to mean a path from u to v with vertices and edges in G, we have
J = (a···x)T (x···y)J (y···a)T
Since J is a cycle in which no vertex, apart from a, is repeated, x and y have no
common ancestor in T apart from a; in symbols,
(a···x)T ∩(y···a)T = (a)

226
Greedy algorithms on graphs
We now show that the assumption that there is some intermediate vertex z on the
path (x···y)J leads to a contradiction. Suppose such a z exists. Here is the picture:
a
z
x
y
Consider the jog J′, deﬁned by
J′ = (a···x)T (x···z)J (z···a)T
The jog J′ has fewer non-T edges than J. Furthermore, since T is a shortest-paths
spanning tree of an undirected graph, we have
cost | (z···a)T |⩽cost (z···y)J (y···a)T
Hence
cost J′ = cost (a···x)T +cost (x···z)J +cost (z···a)T
⩽cost (a···x)T +cost (x···z)J +cost (z···y)T +cost (y···a)T
⩽cost J
This contradicts the assumption that J is a shortest jog with the fewest non-T edges.
So no such vertex z exists.
Now we are ready to describe the algorithm. Suppose, as usual, that the home
vertex is vertex 1 and let T be a shortest-paths spanning tree with source vertex 1. It
follows from the single-edge property that we have to ﬁnd an edge e = (x,y), with
x<y, such that: (i) e is not an edge of T; (ii) e creates a cycle containing vertex 1
when added to T; and (iii) e minimises the sum of the distance in T from vertex 1
to vertex x, the weight of e, and the distance in T from vertex y to vertex 1 (which,
since the graph is undirected, is the same as the distance from 1 to y). In fact it is
not necessary to insist that (x,y) is a real edge, because if it is not then its weight
is inﬁnite and cannot minimise the sum. Call any pair (x,y) satisfying the ﬁrst two
properties a candidate pair.
We can identify a candidate pair by considering two cases. In the ﬁrst case x = 1,
so y cannot be connected directly to vertex 1 in T; that is, the parent of y in T cannot
be 1. In the second case, neither x nor y is vertex 1. In this case deﬁne the subtrees
of T to be those trees that result from deleting all edges of T incident on vertex 1.
In this case, (x,y) is a candidate pair if x and y belong to different subtrees. Call
the root of the subtree to which x belongs the root of x. Then a pair of vertices is a
candidate pair in the second case if they have different roots.

9.7 The jogger’s problem
227
For example, in the spanning tree
1
2
3
4
5
6
the roots of the two subtrees are 2 and 3, and the candidate pairs are (1,4), (1,5),
(1,6), (2,3), (2,4), (2,5), and (2,6). However, none of the pairs (3,6),(4,5),(5,6)
is a candidate, because these pairs have the same root, namely 3. The root of a vertex
can be computed from the links array constructed by Dijkstra’s algorithm:
root ::Links →Vertex →Vertex
root ls v = if p == 1 then v else root ls p
where p = parent ls v
A better method, left as an exercise, is to install a third component in the links
array, one that computes the root associated with each vertex, and to update this
component when processing vertices. In this way we can ensure that evaluation of
root takes constant time. Now we can deﬁne
candidate::Links →(Vertex,Vertex) →Bool
candidate ls (x,y) = if x == 1 then parent ls y ̸= 1 else root ls x ̸= root ls y
The jogger’s problem can now be solved by deﬁning
jog::Graph →[Edge]
jog g = getPath ls wa (bestEdge ls wa)
where ls = fst (apply (n−1) (gstep wa) (start n))
wa = weights g
n
= length (nodes g)
The functions gstep and start are the same as in Dijkstra’s algorithm, while weights
is the same as in Prim’s algorithm because the graph is undirected. The function
bestEdge is deﬁned by
bestEdge::Links →Weights →(Vertex,Vertex)
bestEdge ls wa =
minWith cost [(x,y) | x ←[1..n],y ←[x+1..n],candidate ls (x,y)]
where n = snd (bounds ls)
cost (x,y) = if w == maxInt then maxInt
else distance ls x+w+distance ls y
where w = wa!(x,y)
If (x,y) is not an edge, so its weight is maxInt, then cost (x,y) should also be maxInt.
The function getPath is deﬁned by

228
Greedy algorithms on graphs
getPath::Links →Weights →(Vertex,Vertex) →[Edge]
getPath ls wa (x,y) =
reverse (path x)++[(x,y,wa!(x,y))]++[(v,u,w) | (u,v,w) ←path y]
where path x = if x == 1 then [ ] else (p,x,wa!(p,x)):path p
where p = parent ls x
Building the array and determining the best candidate edge takes O(n2) steps, so
the jogger’s problem can be solved in this time.
9.8 Chapter notes
For an interesting history of the minimum-cost spanning tree problem, see [4]. The
four proofs of Cayley’s formula (see Answer 9.3) can be found in [1]. The Steiner
tree problem mentioned in the introduction is studied in [7].
A fast Union–Find algorithm is presented and analysed in [8]. Kruskal’s algorithm
was described in [5] and Prim’s algorithm in [6]. Prim’s algorithm should perhaps
be called Jarn´ık’s algorithm because it was invented earlier by Vojtˇech Jarn´ık in
1930 and rediscovered by Prim in 1957, and again by Dijkstra in 1959. Alternative
descriptions of these algorithms can be found in most textbooks on algorithm design.
Dijkstra’s shortest-paths algorithm was described in a short article in [3]. The
jogger’s problem is taken from [2], where a second version involving digraphs is
also discussed.
References
[1]
Martin Aigner and G¨unter M. Ziegler. Proofs from The Book. Springer-Verlag, Berlin,
third edition, 2004.
[2]
Richard S. Bird. The jogger’s problem. Information Processing Letters,
13(2):114–117, 1981.
[3]
Edsger W. Dijkstra. A note on two problems in connexion with graphs. Numerische
Mathematik, 1(1):269–271, 1959.
[4]
Ronald L. Graham and Pavol Hell. On the history of the minimum spanning tree
problem. Annals of the History of Computing, 7(1):43–57, 1985.
[5]
Joseph B. Kruskal. On the shortest spanning subtree of a graph and the traveling
salesman problem. Proceedings of the American Mathematical Society, 7(1):48–50,
1956.
[6]
Robert C. Prim. Shortest connection networks and some generalizations. Bell Systems
Technical Journal, 36(6):1389–1401, 1957.
[7]
Hans J¨urgen Pr¨omel and Angelika Steger. The Steiner Tree Problem. Springer-Verlag,
Berlin, 2002.
[8]
Robert E. Tarjan. Efﬁciency of a good but not linear set union algorithm. Journal of
the ACM, 22(2):215–225, 1975.

Exercises
229
Exercises
Exercise 9.1 Some quick questions on graphs and digraphs:
1. Why can a digraph of n vertices contain up to n2 edges, while a graph can contain
no more than n(n−1)/2 edges?
2. Can a digraph have a cycle of length two?
3. Why is it the case that in an acyclic graph there is at most one path between any
two vertices? Is this true of acyclic digraphs?
4. Can a labelled graph have more than one edge between two vertices?
5. Why is a spanning forest of a connected graph necessarily a spanning tree?
6. Why does a spanning tree of a connected graph of n vertices have exactly n−1
edges?
7. What is the maximum number of edges in a longest possible cycle of a graph of
n vertices?
Exercise 9.2 Assuming vertices are labelled from 1 to n, deﬁne functions
toAdj
::Graph →AdjArray
toGraph::AdjArray →Graph
for converting a digraph into its adjacency representation and vice versa.
Exercise 9.3 Draw all the spanning trees for the following graph:
Exercise 9.4 Assign weights to the edges AB and CD in the following graph to
show that the path from A to D in a MCST is not necessarily the shortest path from
A to D.
A
B
C
D
3
2
5
Exercise 9.5 Here is a possible divide-and-conquer algorithm for computing a
MCST. Divide the vertices V of the graph into two sets V1 and V2 that differ in size
by at most one. Let Ei be the set of edges whose endpoints are in Vi. Recursively
ﬁnd a MCST for G1 = (V1,E1) and G2 = (V2,E2). Finally, select a lightest edge,
one of whose endpoints is in V1 and the other in V2, and add it to the two MCSTs to
form a single MCST. Does this algorithm work?

230
Greedy algorithms on graphs
Exercise 9.6 The function steps in the speciﬁcation of Kruskal’s algorithm does
not discard an edge if it creates a cycle in a given forest, even though it will also
create a cycle in any subsequent forest. Write down a version of steps that does
discard such edges.
Exercise 9.7 What is the output of Kruskal’s algorithm if the input is not a con-
nected graph? Adapt the algorithm to ﬁnd the minimum-cost spanning forest of an
unconnected graph.
Exercise 9.8 Why is the test t1 ̸= t2 in the speciﬁcation of Kruskal’s algorithm
sufﬁcient to determine whether two trees in a forest are different? After all, the trees
t1 = ([1,2],[(1,2,3)] and t2 = ([2,1],[(1,2,3)] are the same tree but the test t1 ̸= t2
returns True. As a supplementary question, can the test be made more efﬁcient?
Exercise 9.9 Construct the function weights as used in Prim’s algorithm when the
input is an undirected graph. What is the deﬁnition when the input is a directed
graph?
Exercise 9.10 Consider the problem of ﬁnding a maximum-cost spanning tree. Is
there a greedy algorithm for this problem?
Exercise 9.11 Here is the speciﬁcation of a shortest-paths spanning tree:
spst ←MinWith cost ·spats
The function spats returns all spanning trees of a directed graph. Give a deﬁnition of
spats. To deﬁne cost, we need to compute the path from the source vertex to every
other vertex in the tree. Deﬁne a function
pathsFrom::Vertex →Tree →[Path]
where Path is a synonym for [Edge], such that pathsFrom 1 t returns the paths from
the source vertex 1 to every other vertex in the tree. Finally, deﬁne
cost ::Tree →[Distance]
so that cost t = [d2,...,dn], where dv is the distance from the source vertex 1 to
vertex v.
Exercise 9.12 To ﬁnd the shortest path between A and B, suppose we simultane-
ously compute shortest routes from A to various towns, and shortest routes from
various towns to B, stopping when some intermediate town C has been found in
both directions. Does this idea work?
Exercise 9.13 Give an example to show that Dijkstra’s algorithm does not work
with negative lengths even if there are no negative-length cycles.
Exercise 9.14 How would you modify Dijkstra’s algorithm to stop as soon as the
shortest path to a given vertex is found?

Answers
231
Exercise 9.15 In the jogger’s problem we can install a third component in the links
array to represent the root associated with each vertex:
type Links = Array Vertex (Vertex,Vertex,Distance)
type State = (Links,[Vertex])
parent ::Links →Vertex →Vertex
parent ls v = u where (u, , ) = ls!v
root ::Links →Vertex →Vertex
root ls v = r where ( ,r, ) = ls!v
distance::Links →Vertex →Distance
distance ls v = d where ( , ,d) = ls!v
The starting state is then given by
start ::Nat →State
start n =
(array (1,n) ((1,(1,1,0)):[(v,(v,v,maxInt)) | v ←[2..n]]),[1..n])
Give the modiﬁed deﬁnition of gstep::Weights →State →State.
Answers
Answer 9.1 Some quick answers:
1. Because in a digraph every vertex may contain an edge to every vertex, including
itself, while in a graph there are no edges from a vertex to itself, and at most one
edge between two vertices.
2. Yes, if the digraph contains both the edges (u,v) and (v,u).
3. Suppose there were two different paths between u and v. Let these two paths ﬁrst
meet at some vertex w after u, where w could be v. The two paths P and Q from
u to w contain no edge in common, so the path P followed by the reverse of Q
creates a cycle. In an acyclic digraph there can be many paths that connect two
vertices.
4. It is certainly possible to have both (u,v,w1) and (u,v,w2) as labelled edges
when w1 ̸= w2.
5. Because if the forest consisted of two trees there would be a vertex in each tree
connected by an edge (or the graph would not be connected), and so the edges in
the forest would not be a maximal set.
6. Because a tree with n nodes has exactly n−1 edges, a result that is easily proved
by induction.
7. A maximum-length cycle will pass through every vertex once apart from the two
endpoints, which gives a total number of n edges.

232
Greedy algorithms on graphs
Answer 9.2 For a directed graph g we can deﬁne
toAdj::Graph →AdjArray
toAdj g = accumArray (ﬂip (:)) [ ] (1,n) [(u,(v,w)) | (u,v,w) ←edges g]
where n = length (nodes g)
toGraph::AdjArray →Graph
toGraph a = (indices a,[(u,v,w) | (u,vws) ←assocs a,(v,w) ←vws])
For an undirected graph the last argument to accumArray has to be replaced by
[(u,(v,w)) | (u,v,w) ←edges g]++[(v,(u,w)) | (u,v,w) ←edges g]
Answer 9.3 There are 16 spanning trees:
In fact, the number of possible spanning trees for n vertices is given by Cayley’s
formula nn−2. Four proofs of this remarkable result are given in [1].
Answer 9.4 One fully labelled graph is as follows:
A
B
C
D
4
3
2
5
6
The shortest path from A to D has total length 9, while the path in the MCST has
length 10.
Answer 9.5 No. Take a triangle
A
B
C
1
2
10
Let V1 = [A] and V2 = [B,C]. The divide-and-conquer algorithm returns the edges
AB and BC with cost 11, but the MCST has edges AB and AC with cost 3.
Answer 9.6 Simply change the deﬁnition of steps to read
steps::State →[State]
steps (ts,es) = [(add e ts,es′) | e:es′ ←tails es,safeEdge e ts]

Answers
233
Answer 9.7 Kruskal’s algorithm will return an error because it attempts to take the
head of an empty list. The algorithm for ﬁnding a minimum-cost spanning forest
(MCSF) is as follows:
mcsf ::Graph →Forest
mcsf g = fst (until (null·snd) gstep s)
where s = ([([v],[ ]) | v ←nodes g],sortOn weight (edges g))
This time the algorithm searches all the unused edges and terminates when this list
is empty.
Answer 9.8 Because the test t1 ̸= t2 is applied only to two identical trees or to two
trees with disjoint sets of nodes. If, however, the two trees are identical, the test will
take linear time in the size of the trees. A faster deﬁnition is
notEqual t1 t2 = head (nodes t1) ̸= head (nodes t2)
Answer 9.9 One method is to set up an array with inﬁnite weights and then update
the array with the actual edge weights:
weights g = listArray ((1,1),(n,n)) (repeat maxInt)
//[((u,v),w) | (u,v,w) ←edges g]
//[((v,u),w) | (u,v,w) ←edges g]
where n = length (nodes g)
When the graph is directed the deﬁnition simpliﬁes to
weights::Graph →Array (Vertex,Vertex) Weight
weights g = listArray ((1,1),(n,n)) (repeat maxInt)
//[((u,v),w) | (u,v,w) ←edges g]
where n = length (nodes g)
Answer 9.10 Yes, both Kruskal’s and Prim’s algorithm can be adapted by negating
all the edge weights. In symbols,
maxWith cost = minWith newcost
where the new cost newcost is deﬁned by
newcost ::Tree →Int
newcost = sum·map (negate·weight)·edges
With Kruskal’s algorithm that means edges are listed in decreasing order of weight.
Answer 9.11 The deﬁnition of spats is exactly the same as in Prim’s algorithm but
with a modiﬁed deﬁnition of add and safeEdge to take account of the fact that edges
are directed:

234
Greedy algorithms on graphs
spats::Graph →[Tree]
spats g = map fst (apply (n−1) (concatMap steps) [s])
where n = length (nodes g)
s = (([head (nodes g)],[ ]),edges g)
steps::(Tree,[Edge]) →[(Tree,[Edge])]
steps (t,es) = [(add e t,es′) | (e,es′) ←picks es,safeEdge e t]
add ::Edge →Tree →Tree
add e (vs,es) = (target e:vs,e:es)
safeEdge::Edge →Tree →Bool
safeEdge e t = elem (source e) ns ∧not (elem (target e) ns)
where ns = nodes t
The deﬁnition of pathsFrom is
pathsFrom u t =
[ ]:[(u,v,w):es | (u′,v,w) ←edges t,u′ == u,es ←pathsFrom v t]
Finally, cost is deﬁned by
cost = map (sum·map weight)·sortOn (target ·last)·tail·pathsFrom 1
Answer 9.12 Not obviously. For example, consider the graph
A
B
C
D
E
5
2
5
4
2
The closest vertex from A is D with cost 2. The closest vertex to B is E with cost 2.
The next closest vertex from A is C with cost 5, and the next closest vertex to B is C
with cost 5. That gives the answer ACB with cost 10, but the path ADEB has cost 8.
Answer 9.13 A good example is the following digraph:
1
2
3
4
2
5
1
−4
1
The distances from 1 to the vertices [1,2,3,4] are [0,1,5,2], but the greedy algo-
rithm returns the following distances after three greedy steps (the parent array is not
shown):

Answers
235
1
2
3
4
start
0
∞
∞
∞
update 1
0
2
5
∞
update 2
0
2
5
3
update 4
0
2
5
3
The distances to vertices 2 and 4 are incorrectly calculated as 2 and 3.
Answer 9.14 Change the main deﬁnition to
dijkstra::Graph →Vertex →Path
dijkstra g v = path (until done (gstep wa) (start n))
where path (ls,vs) = (reverse (getPath ls v),distance ls v)
done (ls,vs) = v /∈vs
n = length (nodes g)
wa = weights g
Answer 9.15 The modiﬁed deﬁnition is
gstep::Weights →State →State
gstep wa (ls,vs) = (ls′,vs′) where
(d,v) = minimum [(distance ls v,v) | v ←vs]
vs′ = ﬁlter (̸= v) vs
ls′ = accum better ls [(u,(v,new u,sum d (wa!(v,u)))) | u ←vs′]
where sum d w = if w == maxInt then maxInt else d +w
better (v1,r1,d1) (v2,r2,d2) =
if d1 ⩽d2 then (v1,r1,d1) else (v2,r2,d2)
new u = if v == 1 then u else root ls v


PART FOUR
THINNING ALGORITHMS


239
We turn now to a powerful strategy for solving an optimisation problem when a
greedy algorithm is not possible. The strategy is called thinning, and an algorithm
that employs it a thinning algorithm.
The principle at work behind thinning is really quite simple: if maintaining a
single best candidate at each step is not guaranteed to deliver a best candidate
overall, then maybe one can get away with maintaining a subset of the candidates.
Provided we can quickly identify those partial candidates that can never grow into
fully ﬂedged best candidates, we can remove them from further consideration. The
key factor in the success of the enterprise is the size of the set that remains. When
the set of all possible candidates is exponential in the length of the input, we want
a subset that is much smaller, say one of linear or quadratic size. We have already
encountered one simple instance of thinning in the computation of externs in the
TEX problem of Chapter 7. There we thinned an inﬁnite set of candidates into a
ﬁnite one simply in order to make externs a computable function.
Thinning algorithms therefore sit between the extremes of greedy algorithms
and exhaustive search algorithms. However, with some exceptions, thinning has
not traditionally been suggested as a separate design technique in the algorithms
literature. Instead, problems that are susceptible to thinning have more often been
solved by a related technique, called dynamic programming, a topic we will pursue
in Part Five. As we will see, dynamic programming can be thought of as a gener-
alisation of the divide-and-conquer strategy. Although the parentage of thinning
algorithms and dynamic programming algorithms is different, both techniques can
often be applied to one and the same problem. What makes thinning important is
that many algorithms traditionally regarded as paradigms of dynamic programming
can be formulated, often more effectively, as thinning algorithms.


Chapter 10
Introduction to thinning
In this chapter we explore the basic theory of thinning and discuss three simple
examples of the idea. As with most of the algorithms we have seen so far, the
key step that makes thinning a viable design technique involves fusion. For fusion
to work, we will need to reason about reﬁnement and another nondeterministic
function, called ThinBy. The ﬁrst section enumerates the essential properties that we
need this function to possess. The chapter ends with a general thinning algorithm
that captures most of the essential points about how to introduce thinning.
10.1 Theory
The theory behind thinning algorithms is all about a nondeterministic function
ThinBy::(a →a →Bool) →[a] →[a]
This function takes a comparison function and a list as arguments and returns a list
as its result. It is speciﬁed by two properties: ﬁrstly, if ys is a possible output of the
expression ThinBy (≼) xs, that is, if
ys ←ThinBy (≼) xs
then ys is a subsequence of xs; and secondly, for every x in xs we can ﬁnd an element
y in ys such that y ≼x. In symbols,
ys ⊑xs ∧∀x ∈xs : ∃y ∈ys : y ≼x
where ys ⊑xs means that ys is a subsequence of xs. It is assumed throughout that ≼
is a preorder, a relation which is reﬂexive and transitive. But we do not assume ≼
is a total preorder; that is, we do not assume that for all x and y either x ≼y or y ≼x
holds. Any deﬁnition of the form
x ≼y = (cost x ⩽cost y)
for a total function cost :: Ord b ⇒a →b would mean that ≼is a total preorder.
Working with total preorders turns out to be too restrictive for the purposes of

242
Introduction to thinning
thinning, which is why we choose as our basic construct thinning by a comparison
function rather than thinning with a cost function.
Here is an example. Suppose ≼is deﬁned on pairs of numbers by
(a,b) ≼(c,d) = (a ⩾c) ∧(b ⩽d)
Then ≼is a preorder, in fact a partial order because it is also anti-symmetric,
meaning that x ≼y ∧y ≼x ⇒x = y for all x and y; but ≼is not a total preorder. For
example, (4,3) and (5,4) are not comparable under ≼. Now consider the expression
ThinBy (≼) [(1,2),(4,3),(2,3),(5,4),(3,1)]
This expression has four possible reﬁnements:
[(4,3),(5,4),(3,1)]
[(4,3),(2,3),(5,4),(3,1)]
[(1,2),(4,3),(5,4),(3,1)]
[(1,2),(4,3),(2,3),(5,4),(3,1)]
The most effective implementation of ThinBy would be to return a subsequence of
shortest length, but computing such a sequence (see the exercises) can involve a
quadratic number of evaluations of ≼. Instead we prefer sub-optimal implementa-
tions of ThinBy that take linear time. One legitimate but pointless implementation is
to take thinBy (≼) = id. However, the reﬁnement law id ←ThinBy (≼) is useful in
establishing other properties of ThinBy.
One sensible implementation of ThinBy is to deﬁne
thinBy (≼) = foldr bump [ ]
where bump x [ ]
= [x]
bump x (y:ys)
| x ≼y
= x:ys
| y ≼x
= y:ys
| otherwise = x:y:ys
This function processes a list from right to left. Each new element x can ‘bump’ the
current ﬁrst element y if x ≼y, or be bumped by y if y ≼x. Otherwise it is added to
the list. For example,
thinBy (≼) [(1,2),(4,3),(2,3),(5,4),(3,1)] = [(1,2),(4,3),(5,4),(3,1)]
In this example, thinning is more effective if the list elements are in ascending order
of ﬁrst component, or ascending order of second component:
thinBy (≼) [(1,2),(2,3),(3,1),(4,3),(5,4)] = [(3,1),(4,3),(5,4)]
thinBy (≼) [(3,1),(1,2),(2,3),(4,3),(5,4)] = [(3,1),(4,3),(5,4)]
We can maintain order when building candidates in a step-by-step manner by
merging sublists at each step rather than full-scale sorting. That is the primary
reason why we insist that thinning a list xs should return a subsequence of xs – the

10.1 Theory
243
relative order of the elements is not changed. There are other sensible deﬁnitions of
thinBy, including one that processes elements from left to right; see the exercises
for examples.
In addition to the identity law, there are six other basic laws about thinning, some
of which are more useful in calculations than others. Proofs of the laws are relegated
to the exercises so that we can concentrate here on what they say. The ﬁrst law is
that
ThinBy (≼) = ThinBy (≼)·ThinBy (≼)
In words, thinning a list twice has the same possible outcomes as thinning it once.
The law is interesting theoretically but not of much practical use.
By contrast, the next law is used as the very ﬁrst step in every derivation that
follows. It is called thin introduction, and it asserts that
MinWith cost = MinWith cost ·ThinBy (≼)
provided x ≼y ⇒cost x ⩽cost y. Thin introduction is the law that lets us restate an
optimisation problem as a problem about thinning.
The next law is called thin elimination:
wrap·MinWith cost ←ThinBy (≼)
provided cost x ⩽cost y ⇒x ≼y. Thin elimination is dual to thin introduction, and
so is its proviso.
The next law also makes an appearance in virtually every calculation about
thinning involving concat. It is the distributive law, and it states that
ThinBy (≼)·concat = ThinBy (≼)·concatMap (ThinBy (≼))
In words, one can thin the concatenation of a list of lists by thinning each list,
concatenating the results, and thinning again. Without the ﬁnal thinning, the law
would be only a reﬁnement. That is,
concatMap (ThinBy (≼)) ←ThinBy (≼)·concat
This version is not strong enough to be of much practical help.
The next law is the thin-map law, which comes in two ﬂavours. Firstly,
map f ·ThinBy (≼) ←ThinBy (≼)·map f
provided x ≼y ⇒f x ≼f y. Secondly,
ThinBy (≼)·map f ←map f ·ThinBy (≼)
provided f x ≼f y ⇒x ≼y. It follows that
map f ·ThinBy (≼) = ThinBy (≼)·map f
if x ≼y ⇔f x ≼f y. Appeal to the thin-map law often relies on context. For example,
map f ·ThinBy (≼)·ﬁlter p = ThinBy (≼)·map f ·ﬁlter p

244
Introduction to thinning
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
7
8
8
5
3
4
2
3
7
2
1
5
3
5
9
2
1
4
2
6
7
Figure 10.1 A layered network
provided p x ∧p y ⇒(x ≼y ⇔f x ≼f y). We will see an example of this context-
sensitive version in the following section.
The ﬁnal law is the thin-ﬁlter law:
ThinBy (≼)·ﬁlter p = ﬁlter p·ThinBy (≼)
provided (x ≼y ∧p y) ⇒p x.
We will come back to the theory of thinning after ﬁrst exploring some sample
problems to see what thinning can contribute to the study of efﬁcient functional
algorithms.
10.2 Paths in a layered network
Our ﬁrst problem is a shortest-paths problem. Consider the digraph in Figure 10.1.
Reading from top to bottom, the graph consists of a number of layers, each layer
consisting of a number of vertices and each edge going from one layer to the one
beneath. It so happens in the example that there are the same number of vertices
in each layer, but this is not a requirement. Each edge is given by a triple (u,v,w),
where u is the source vertex of the edge, v is the target vertex, and w is a numerical
weight, not necessarily positive. We assume that there is at least one path from some
vertex in the top layer to some vertex in the bottom layer (in the example there
are 27 such paths). The problem is to ﬁnd one with minimum total weight. For the
example the answer is the path [(4,7,2),(7,11,2),(11,16,3)] of total weight 7. It
is easy to see that Dijkstra’s algorithm can be used to solve this problem, at least if
the weights are nonnegative. Imagine another vertex U with zero-weight edges to
each of the vertices in the top layer, and another vertex V with zero-weight edges
from each of the vertices in the bottom layer. Then a shortest path from U to V

10.2 Paths in a layered network
245
includes a shortest path from the top layer to the bottom layer. Dijkstra’s algorithm
takes O(n2) steps, where n is the total number of vertices in the network, but it is
possible to reduce this time with a thinning algorithm.
To calculate the thinning algorithm, suppose the layered network is given by a
list of lists of edges, each list describing the edges between two adjacent layers:
type Net
= [[Edge]]
type Path
= [Edge]
type Edge
= (Vertex,Vertex,Weight)
type Vertex = Int
type Weight = Int
We will make use of the following selector functions:
source,target ::Edge →Vertex
source (u,v,w) = u
target (u,v,w) = v
weight ::Edge →Weight
weight (u,v,w) = w
Our problem is to compute mcp (a minimum-cost path), speciﬁed by
mcp::Net →Path
mcp ←MinWith cost ·paths
The cost function on paths is deﬁned by
cost ::Path →Int
cost = sum·map weight
The function paths can be deﬁned in terms of the Cartesian-product function cp:
cp::[[a]] →[[a]]
cp = foldr op [[ ]] where op xs yss = [x:ys | x ←xs,ys ←yss]
For example,
cp ["abc","de","f"] = ["adf","aef","bdf","bef","cdf","cef"]
We have
paths::Net →[Path]
paths = ﬁlter connected ·cp
where connected is the predicate
connected ::Path →Bool
connected [ ]
= True
connected (e:es) = linked e es ∧connected es
and linked is the predicate

246
Introduction to thinning
linked ::Edge →Path →Bool
linked e1 [ ]
= True
linked e1 (e2 :es) = target e1 == source e2
As a ﬁrst step we can fuse ﬁlter and cp to arrive at another deﬁnition of paths:
paths = foldr step [[ ]]
where step es ps = [e:p | e ←es,p ←ps,linked e p]
Details of the fusion step are left as an exercise. We can also rewrite step in the
equivalent form
step es ps = concat [cons e ps | e ←es]
where cons e ps = [e:p | p ←ps,linked e p]
Now we arrive at the heart of the problem. A greedy algorithm, one that maintains
a single path at each step, is not possible because the source of a minimum-cost
path at one level may not be among the target vertices of the edges at the next level
up. So we introduce thinning. The thin-introduction law says we can rewrite the
speciﬁcation as
mcp ←MinWith cost ·ThinBy (≼)·paths
provided we choose ≼so that p1 ≼p2 ⇒cost p1 ⩽cost p2. An appropriate choice
for ≼is the partial preorder
(≼)::Path →Path →Bool
p1 ≼p2 = source (head p1) == source (head p2) ∧cost p1 ⩽cost p2
In words, when building paths from bottom to top, there is no point in keeping a
path if there is another path with the same source vertex and lower cost.
The aim now is to fuse ThinBy (≼) and paths. That means ﬁnding a function
tstep so that the fusion condition
tstep es (ThinBy (≼) ps) ←ThinBy (≼) (step es ps)
holds. We can establish the fusion condition by arguing as follows:
ThinBy (≼) (step es ps)
=
{ deﬁnition of step }
ThinBy (≼) (concat [cons e ps | e ←es])
=
{ distributive law }
ThinBy (≼) (concat [ThinBy (≼) (cons e ps) | e ←es])
=
{ claim: see below }
ThinBy (≼) (concat [cons e (ThinBy (≼) ps) | e ←es])
=
{ deﬁnition of step }
ThinBy (≼) (step es (ThinBy (≼) ps))
→
{ deﬁning tstep es ps ←ThinBy (≼) (step es ps) }
tstep es (ThinBy (≼) ps)

10.2 Paths in a layered network
247
We have shown that
foldr tstep [[ ]] ←ThinBy (≼)·foldr step [[ ]]
where
tstep es ps ←ThinBy (≼) (step es ps)
The claim in the third step is the assertion
ThinBy (≼) (cons e ps) = cons e (ThinBy (≼) ps)
Here is the proof:
ThinBy (≼)·cons e
=
{ deﬁnition of cons }
ThinBy (≼)·map (e:)·ﬁlter (linked e)
=
{ thin-map law; see below }
map (e:)·ThinBy (≼)·ﬁlter (linked e)
=
{ thin-ﬁlter law; see below }
map (e:)·ﬁlter (linked e)·ThinBy (≼)
=
{ deﬁnition of cons }
cons e·ThinBy (≼)
The thin-ﬁlter law is justiﬁed because
p1 ≼p2 ∧linked e p2 ⇒linked e p1
The thin-map law is justiﬁed because
e:p1 ≼e:p2 ⇔p1 ≼p2
provided linked e p1 and linked e p2. The appeal to the thin-map law in the above
calculation therefore relies on context.
In summary, we have the ﬁnal algorithm
mcp = minWith cost ·foldr tstep [[ ]]
where tstep es ps = thinBy (≼) [e:p | e ←es,p ←ps,linked e p]
where minWith is some implementation of MinWith and thinBy is some suitable
implementation of ThinBy. As a further optimisation we can tuple paths with their
costs to avoid recomputation of cost.
There is one further and important optimisation. Thinning will be most effective if
each list of edges is sorted so that edges with the same source vertex appear together.
Then thinning with the deﬁnition of thinBy given in the ﬁrst section will produce
just one path for every source vertex. For example, in the network of Figure 10.1
the ﬁrst step will produce the four singleton paths
[[(9,13,4)],[(10,14,2)],[(11,16,3)],[(12,16,7)]]
Each additional step will also produce exactly four paths because each layer has
four vertices. As to the running time, observe that, because the number of paths

248
Introduction to thinning
maintained at each step is at most the number of vertices in the current layer, the
cost of each step is proportional to at most the product of the number of edges
between two layers and the number of vertices in the lower layer. If each layer has
no more than k vertices, then the running time is O(ek) steps, where e is the total
number of edges. If there are d layers, then e ⩽(d −1)k2. Furthermore, the total
number n of vertices is at most dk. The thinning algorithm therefore takes O(dk3)
steps, while Dijkstra’s algorithm takes O(d2 k2) steps. The thinning algorithm is
therefore superior when the network is deeper than it is wide. By renaming vertices
so that the vertices in each layer are labelled with 1 to k, and using an array to store
the best paths at each step, it is possible to shave a factor of k off this running time,
giving an optimal O(dk2) algorithm. This extension is left as Exercise 10.14.
10.3 Coin-changing revisited
For the next problem we revisit the coin-changing problem of Chapter 7. Recall that
the greedy algorithm is not guaranteed to produce the smallest number of coins for
all possible denominations. In particular, the greedy algorithm does not work for the
United Regions (UR) denominations (see Exercise 7.16). However, the UR is a rich
country and can afford automated change-giving systems. Which algorithm should
we design to guarantee a minimum number of coins is given for any possible set of
denominations?
One answer is a thinning algorithm. To set things up for a thinning step we need
to replace the recursive deﬁnition of mktuples given in Chapter 7 with a deﬁnition
using an appropriate higher-order function such as a fold of some kind. As we will
see in Part Five, working directly with recursive deﬁnitions leads to thinking about
dynamic programming solutions, but thinning typically involves a fusion step with
a higher-order function such as a fold. For compatibility with the other algorithms
in this chapter we choose foldr, so denominations are considered in order from right
to left. We still want to consider denominations in decreasing order of value, so we
take currencies in increasing order; for example
ukds = [1,2,5,10,20,50,100,200]
urds = [1,2,5,15,20,50,100]
Here are the relevant deﬁnitions:
type Denom = Nat
type Coin
= Nat
type Residue = Nat
type Count
= Nat
type Tuple
= ([Coin],Residue,Count)
And here are the selector functions we will need:

10.3 Coin-changing revisited
249
coins::Tuple →[Coin]
coins (cs, , ) = cs
residue::Tuple →Residue
residue ( ,r, ) = r
count ::Tuple →Count
count ( , ,k) = k
This time a tuple consists of three things: a list of coin counts [ck,ck−1,...,c1] for a
given list of denominations [d1,d2,...,dk], the residual amount r after giving these
coins in change, and a count of the number of coins used. The function mktuples is
redeﬁned as follows:
mktuples::Nat →[Denom] →[Tuple]
mktuples n = foldr (concatMap·extend) [([ ],n,0)]
extend ::Denom →Tuple →[Tuple]
extend d (cs,r,k) = [(cs++[c],r −c×d,k +c) | c ←[0..r div d]]
We start with no coins and a residue n, the amount of change required. At each
step the next lower denomination is considered, and every possible choice for a
number of coins of this denomination is considered. The new residue and count
are calculated and the algorithm proceeds to the next step. Evaluation of mktuples
returns many more values than the one in Chapter 7 because it returns all the partial
tuples, including those with a non-zero residue. For example,
length (mktuples 256 ukds) = 10640485
The function mkchange is now speciﬁed by
mkchange::Nat →[Denom] →[Coin]
mkchange n ←coins·MinWith cost ·mktuples n
where
cost ::Tuple →(Residue,Count)
cost t = (residue t,count t)
A candidate with minimum cost is one whose residue is as small as possible and,
among such candidates, one with minimum count. Since we are assuming there is a
denomination with value 1, there are candidates with zero residue, so a minimum-
cost candidate has zero residue and minimum count.
As in the layered network problem, we now introduce a thinning step, writing
mkchange n ←coins·MinWith cost ·ThinBy (≼)·mktuples n
where preorder ≼has to be chosen to satisfy
t1 ≼t2 ⇒cost t1 ⩽cost t2
The right choice of ≼is the following one:

250
Introduction to thinning
(≼)::Tuple →Tuple →Bool
t1 ≼t2 = (residue t1 == residue t2) ∧(count t1 ⩽count t2)
In words, there is no point in keeping a tuple in play if there is another tuple whose
residue is the same but whose count is smaller. That sounds reasonable, but it might
be thought that a stronger statement is true, namely that there is no point in keeping
a tuple if there is another tuple whose residue and count are both smaller. However,
this statement is false (see Exercise 10.16).
The aim now is to fuse ThinBy (≼) and mktuples. For this to work we need to
verify the fusion condition
tstep d (ThinBy (≼) ts) ←ThinBy (≼) (step d ts)
for some function tstep satisfying
tstep d ts ←ThinBy (≼) (step d ts)
That means we have to verify the condition
ThinBy (≼) (step d (ThinBy (≼) ts)) ←ThinBy (≼) (step d ts)
(10.1)
where step = concatMap·extend. Following exactly the same path as in the layered
network problem, we reason
ThinBy (≼) (step d ts)
=
{ deﬁnition of step }
ThinBy (≼) (concatMap (extend d) ts)
=
{ distributive law }
ThinBy (≼) (concatMap (ThinBy (≼)·extend d) ts)
However, the calculation can proceed no further because
ThinBy (≼)·extend d = extend d
The reason is that the tuples in extend d t have different residues and thinning can
never eliminate any tuples.
Instead, we have to back up and ﬁnd an alternative proof that (10.1) holds. For
this we need the key fact that, if t1 ≼t2, then
∀e2 ∈extend d t2 : ∃e1 ∈extend d t1 : e1 ≼e2
(10.2)
To prove (10.2), let t1 = (cs1,r,k1) and t2 = (cs2,r,k2), where t1 ≼t2 so k1 ⩽k2.
Suppose e2 = (cs2 ++[c],r −c×d,k2 +c). Then e1 = (cs1 ++[c],r −c×d,k1 +c)
is in extend d t1 and e1 ≼e2, establishing the result.
Now to prove (10.1), let us ←ThinBy (≼) ts and vs ←ThinBy (≼) (step d us).
We have to show that vs ←ThinBy (≼) (step d ts), that is,
vs ⊑step d ts ∧∀w ∈step d ts : (∃v ∈vs : v ≼w)
Recall that ⊑denotes the subsequence relation. For the ﬁrst conjunct, we can reason
as follows:

10.3 Coin-changing revisited
251
vs
⊑
{ deﬁnitions of vs and ThinBy }
step d us
⊑
{ since xs ⊑ys ⇒step d xs ⊑step d ys }
step d ts
For the second conjunct suppose w ∈extend d t, where t ∈ts. Since there exists
u ∈us with u ≼t, appeal to (10.2) says there exists e ∈extend d u with e ≼w. But
by deﬁnition of vs there exists a v ∈vs with v ≼e, so (10.1) follows on appeal to
the transitivity of ≼.
Summarising, we have shown that
foldr tstep [([ ],n,0)] ←ThinBy (≼)·mktuples n
where
tstep d ←ThinBy (≼)·concatMap (extend d)
As with the layered network problem, the thinning step will be more effective if
tuples with the same residue are brought together. This can be achieved by keeping
tuples in decreasing order of residue. Since extend produces tuples in this order, it
is sufﬁcient to deﬁne tstep by
tstep d = thinBy (≼)·mergeBy cmp·map (extend d)
where cmp t1 t2 = residue t1 ⩾residue t2
The deﬁnition of mergeBy::(a →a →Bool) →[[a]] →[a] is left as an exercise.
The complete algorithm now reads
mkchange::Nat →[Denom] →[Coin]
mkchange n = coins·minWith cost ·foldr tstep [([ ],n,0)]
The running time of mkchange is O(n2 k) steps, where n is the amount for which
change is required and k is the number of denominations. At each step the number
of candidates in play is at most n+1 because there is at most one candidate for each
residual amount r and 0 ⩽r ⩽n. A candidate with residue r has O(r) extensions, so
there can be O(n2) new candidates before thinning. Processing each denomination
therefore requires O(n2) steps, and there are k steps in total.
As a ﬁnal remark, the coin-changing problem can be thought of as an instance
of the layered network problem. Each layer contains one vertex for each residual
amount and for the denominations considered so far. The edges between the layers
correspond to the choices for the number of coins for the next denomination. For
example, with change 17 and denominations [1,2,5,10] the ﬁrst three layers of the
network are illustrated in Figure 10.2. The connection between the two problems is
no accident because all thinning algorithms involving a fold can be regarded as a
shortest-path problem on a directed acyclic graph of some kind. This connection
will be examined more closely later when we discuss dynamic programming.

252
Introduction to thinning
17
17
7
17
12
7
2
Figure 10.2 Coin-changing as a layered network
10.4 The knapsack problem
Our third and ﬁnal problem in this chapter is a famous one called the knapsack prob-
lem. This problem is usually given as a model instance of the dynamic programming
strategy, but we are going to give a thinning algorithm. The dynamic programming
solution (see Chapter 13) is more restrictive in that it depends on certain quantities
being integers.
Here is the setting. Suppose a thief comes to your room in the night bearing a
knapsack. Surveying the room, he discovers the following items:
item
value
weight
value/weight
Laptop
30
14
2.14
Television
67
31
2.16
Jewellery
19
8
2.38
CD collection
50
24
2.08
Each item here has an integer value and weight but, in general, values and weights
can be arbitrary positive real numbers. The thief would like to steal everything in the
room, but his knapsack can support only a limited weight. Assuming the maximum
weight the knapsack can hold is 50 units, what items should the thief steal in order
to maximise the total value of his haul?
He could decide to pack items in decreasing order of value. That gives
swag = Television+Laptop
(value 97, weight 45)
He could decide to pack items in ascending order of weight. That gives
swag = Jewellery+Laptop+CDs
(value 99, weight 46)
He could decide to pack in decreasing value/weight ratio. That gives
swag = Jewellery+Television
(value 86, weight 39)
Each of these strategies is, of course, a greedy strategy, trying to obtain a global
optimum by making a sequence of locally optimal decisions. For this example the
best strategy is the second one, but it is easy to give examples to show that packing

10.4 The knapsack problem
253
items in ascending order of weight is not always the best policy. In fact, there is no
greedy algorithm for the problem.
The scenario above is known as the 0/1 knapsack problem: either an item is
chosen or it is not. In the more general integer knapsack problem, the scenario
changes to a warehouse rather a room. The warehouse contains large numbers of
each individual item and the thief can choose an arbitrary number of each item
subject to the capacity of his knapsack. There is no greedy algorithm for this problem
either. There is, however, one version of the knapsack problem for which a greedy
algorithm does work. That is the fractional knapsack problem in which the items
are things like gold dust in which an arbitrary proportion of each item can be chosen.
In what follows we will concentrate on the 0/1 version of the knapsack problem,
leaving the other two as exercises.
We start off by deﬁning various types and selector functions:
type Name
= String
type Value
= Nat
type Weight
= Nat
type Item
= (Name,Value,Weight)
type Selection = ([Name],Value,Weight)
Each item has a name, a value, and a weight. A selection is a triple consisting of a
list of item names, the total value of the selection, and its total weight. We will need
the following three selector functions, the last two of which can be applied both to
items and to selections, and hence are given a polymorphic type:
name::Item →Name
name (n, , ) = n
value::(a,Value,Weight) →Value
value ( ,v, ) = v
weight ::(a,Value,Weight) →Weight
weight ( , ,w) = w
We can now specify swag (‘swag’ means money or goods taken by a thief) by
swag::Weight →[Item] →Selection
swag w ←MaxWith value·ﬁlter (within w)·selections
The nondeterministic function MaxWith cost is dual to MinWith cost in that the
possible reﬁnements are those with maximum cost rather than minimum cost. The
ﬁrst argument to swag is the maximum weight the knapsack can hold. The predicate
within w is deﬁned by
within::Weight →Selection →Bool
within w sn = weight sn ⩽w

254
Introduction to thinning
There are two reasonable ways to deﬁne selections. One way is to write
selections::[Item] →[Selection]
selections = foldr (concatMap·extend) [([ ],0,0)]
where extend i sn = [sn,add i sn]
add ::Item →Selection →Selection
add i (ns,v,w) = (name i:ns,value i+v,weight i+w)
The other way is left as Exercise 10.21. At each step we can extend a selection
either by omitting the next item or by including it. The function selections returns
all possible subsequences of the given list of items, so there are 2n selections for a
list of n items.
As a ﬁrst step, we fuse ﬁlter with selections to obtain a new function, which we
will call choices:
choices::Weight →[Item] →[Selection]
choices w = foldr (concatMap·extend) [([ ],0,0)]
where extend i sn = ﬁlter (within w) [sn,add i sn]
The function choices generates only those selections whose total weight is at most
the carrying capacity of the knapsack. This step alone can signiﬁcantly reduce the
number of selections that have to be considered, but we can do even better with a
thinning step. We rewrite the speciﬁcation as
swag w ←MaxWith value·ThinBy (≼)·choices w
where the appropriate choice here of preorder ≼is the following one:
(≼)::Selection →Selection →Bool
sn1 ≼sn2 = value sn1 ⩾value sn2 ∧weight sn1 ⩽weight sn2
In words, there is no point in keeping a selection from a list of items in play if
there is another selection from the same list with a greater value and a smaller
weight. We have sn1 ≼sn2 ⇒value sn1 ⩾value sn2, the necessary proviso for the
thin-introduction step in the case of MaxWith.
We can now fuse ThinBy and choices to arrive at the new deﬁnition
swag w = maxWith value·foldr tstep [([ ],0,0)]
where tstep i
= thinBy (≼)·concatMap (extend i)
extend i sn = ﬁlter (within w) [sn,add i sn]
The details are left as Exercise 10.20. As with the other thinning algorithms in this
chapter, the thinning step will be more effective if the selections are kept in order.
We can list selections either in decreasing order of value, or in increasing order of
weight. Since extend produces selections in increasing order of weight, we choose
the latter.
The result is the following algorithm for swag:

10.5 A general thinning algorithm
255
swag w = maxWith value·foldr tstep [([ ],0,0)]
where tstep i
= thinBy (≼)·mergeBy cmp·map (extend i)
extend i sn = ﬁlter (within w) [sn,add i sn]
cmp sn1 sn2 = weight sn1 ⩽weight sn2
This is our ﬁnal algorithm for the knapsack problem. As to its running time, suppose
that all weights are integers. Each thinning step brings selections with equal weights
together and eliminates all but one of them, thereby maintaining a list of at most
w + 1 selections, each with a different weight from 0 up to w. This list can be
computed in Θ(w) steps in the worst case. There are n items to process, so the
running time is O(nw) steps. That appears to make the algorithm a linear-time one.
However, if weights are arbitrary positive real numbers, then there is no guarantee
that only w + 1 selections are maintained at each step. In fact all 2n selections
might have to be kept, each with a different total weight and value. That means the
algorithm can be exponential in n for non-integral weights.
10.5 A general thinning algorithm
The last two examples seem very similar (even more so when Exercise 10.20 is
answered), so let’s end the chapter by solving an abstract problem that captures all
of the essential ideas behind thinning when candidates is expressed in the following
way:
candidates::[Data] →[Candidate]
candidates = foldr (concatMap·extend) [anon]
Here anon is some initial candidate.
Consider a speciﬁcation of the form
best ::[Data] →Candidate
best ←MinWith cost ·ﬁlter good ·candidates
There are four ritual steps in calculating a thinning algorithm to solve this problem.
The ﬁrst step is to fuse ﬁlter good with candidates. This step is possible if
good (extend d x) ⇒good x
In other words, if a candidate is bad, then no extension of the candidate can ever be
good. The candidate anon has to be good, otherwise there are no good candidates.
We can now reason
ﬁlter good ·concatMap (extend d)
=
{ since ﬁlter p·concatMap = concatMap (ﬁlter p) }
concatMap (ﬁlter good ·extend d)
=
{ assumption }
concatMap (ﬁlter good ·extend d)·ﬁlter good

256
Introduction to thinning
This establishes the fusion condition and so
ﬁlter good ·foldr (concatMap·extend) [anon] = foldr step [anon]
where
step d = concatMap (ﬁlter good ·extend d)
The second step is to introduce thinning. Suppose ≼is a comparison function for
which x ≼y ⇒cost x ⩽cost y for all good candidates x and y. We can then appeal
to the thin-introduction law to reﬁne the speciﬁcation of best to read
best ←MinWith cost ·ThinBy (≼)·foldr step [anon]
The third step is to fuse ThinBy (≼) and foldr. With
tstep d ←ThinBy (≼)·step d
we have
foldr tstep [anon] ←ThinBy (≼)·foldr step [anon]
provided the fusion condition
tstep d ·ThinBy (≼) ←ThinBy (≼)·step d
holds. With the speciﬁcation of tstep above, the proviso follows from
ThinBy (≼)·step d ·ThinBy (≼) ←ThinBy (≼)·step d
which, as we have seen in (10.1) of Section 10.3, follows from the assumption
x ≼y ⇒∀v ∈goodext d y : ∃u ∈goodext d x : u ≼v
where goodext d x = ﬁlter good (extend d x). As a result we have
best = minWith cost ·foldr step [anon]
where step d = thinBy (≼)·concatMap (ﬁlter good ·extend d)
The fourth and ﬁnal step is to make thinning more effective by keeping the candidates
in order. Suppose value is some function on candidates such that extend produces
new candidates in, say, increasing order of value. Then we have as the ﬁnal algorithm
best = minWith cost ·foldr step [anon]
where step d
= thinBy (≼)·mergeBy cmp·map (ﬁlter good ·extend d)
cmp x y = value x ⩽value y
It is possible, with more or less effort, to reformulate the three problems in this
chapter as instances of this general scheme, but the reformulation does not add
signiﬁcantly to the understanding of the three algorithms. What is important is that
the derivation of a thinning algorithm follows a more or less standard path.

10.6 Chapter notes
257
10.6 Chapter notes
The theory of thinning algorithms was described in [2], and developed further
by Sharon Curtis and Shin-Cheng Mu in their doctoral theses [3, 7]. The general
thinning theorem, along with a number of applications, appeared in [4] and [5]. The
knapsack problem has a long history, see [6], and a version of the thinning method
applied to this problem appears in [1].
References
[1]
Joachim H. Ahrens and Gerd Finke. Merging and sorting applied to the zero–one
knapsack problem. Operations Research, 23(6):1099–1109, 1975.
[2]
Richard S. Bird and Oege de Moor. The Algebra of Programming. Prentice-Hall,
Hemel Hempstead, 1997.
[3]
Sharon Curtis. A Relational Approach to Optimization Problems. DPhil thesis,
Oxford University Computing Laboratory, 1996. Technical Monograph PRG-122.
[4]
Oege de Moor. A generic program for sequential decision processes. In Programming
Languages: Implementations, Logics and Programs, volume 982 of Lecture Notes in
Computer Science, pages 1–23, Springer-Verlag, Berlin, 1995.
[5]
Oege de Moor. Dynamic programming as a software component. In Circuits, Systems,
Computers and Communications, IEEE, 1999. Invited talk.
[6]
Silvano Martello and Paolo Toth. Knapsack Problems: Algorithms and Computer
Implementations. John Wiley and Sons, Chichester, 1990.
[7]
Shin-Cheng Mu. A Calculational Approach to Program Inversion. DPhil thesis,
Oxford University Computing Laboratory, 2003. Research Report PRG-RR-04-03.
Exercises
Exercise 10.1 Is ThinBy (≼) [ ] well-deﬁned?
Exercise 10.2 Give a linear-time algorithm for thinBy (≼) that processes the list
from left to right.
Exercise 10.3 Here is a speciﬁcation of a version of thinBy that computes shortest
thinnings:
thinBy (≼) xs ←MinWith length (candidates (≼) xs)
Give a deﬁnition of candidates. You can assume a function subseqs::[a] →[[a]]
that returns all the subsequences of a sequence.
Exercise 10.4 Following on from the previous exercise, give a quadratic-time algo-
rithm for thinBy. No justiﬁcation is required.
Exercise 10.5 Is the reﬁnement law id ←ThinBy (≼) valid for all possible deﬁni-
tions of ≼?

258
Introduction to thinning
Exercise 10.6 Give an implementation thinBy of ThinBy for which the equation
thinBy (≼) = thinBy (≼)·thinBy (≼)
is false.
Exercise 10.7 The idempotency of ThinBy is captured as two reﬁnements:
ThinBy (≼) ←ThinBy (≼)·ThinBy (≼)
ThinBy (≼)·ThinBy (≼) ←ThinBy (≼)
The ﬁrst reﬁnement is easy. Why? For the second we have to show for all xs that,
if ys ←ThinBy (≼) xs and zs ←ThinBy (≼) ys, then zs ←ThinBy (≼) xs. Why is
this assertion true?
Exercise 10.8 The thin-introduction law is also captured as two reﬁnements:
MinWith cost ←MinWith cost ·ThinBy (≼)
MinWith cost ·ThinBy (≼) ←MinWith cost
The ﬁrst reﬁnement is easy. Why? Prove that the second reﬁnement holds under the
proviso x ≼y ⇒cost x ⩽cost y.
Exercise 10.9 Prove the thin-elimination law, namely that
wrap·MinWith cost ←ThinBy (≼)
provided cost x ⩽cost y ⇒x ≼y.
Exercise 10.10 Prove the thin-ﬁlter law, namely that
ﬁlter p·ThinBy (≼) = ThinBy (≼)·ﬁlter p
provided x ≼y ∧p y ⇒p x.
Exercise 10.11 Prove the thin-map law, namely that
map f ·ThinBy (≼) ←ThinBy (≼)·map f
provided x ≼y ⇒f x ≼f y.
Exercise 10.12 Give an example to show that the law
ThinBy (≼)·concat = concatMap (ThinBy (≼))
does not hold.
Exercise 10.13 For the layered network problem, prove that
ﬁlter connected ·foldr op [[ ]] = foldr step [[ ]]
where
op es ps
= [e:p | e ←es,p ←ps]
step es ps = [e:p | e ←es,p ←ps,linked e p]

Exercises
259
Exercise 10.14 Suppose in the layered network problem that there exists some k
such that the vertices in each layer are labelled with integers in the range 1 to k. This
can always be achieved by renaming the vertices, so there is no loss of generality in
the assumption. Then optimal paths from each vertex j can be stored as the jth ele-
ment of an array indexed from 1 to k. That idea leads to the following version of mcp:
mcp::Nat →Net →Path
mcp k = snd ·minWith fst ·elems·foldr step start
where start
= array (1,k) [(v,(0,[ ])) | v ←[1..k]]
step es pa = accumArray better initial (1,k) (map insert es)
where initial
= ...
insert (u,v,w) = ...
better (c1,p1) (c2,p2) = if c1 ⩽c2 then (c1,p1) else (c2,p2)
Complete the algorithm by giving the deﬁnitions of initial and insert.
Exercise 10.15 Give a deﬁnition of mergeBy::(a →a →Bool) →[[a]] →[a].
Exercise 10.16 In the coin-changing problem, suppose we had deﬁned ≼by
t1 ≼t2 = (residue t1 ⩽residue t2) ∧(count t1 ⩽count t2)
Now consider the list of tuples generated when the amount is 13 and the denom-
inations are [1,x,5,10]. Both ([1,0],3,1) and ([0,1],8,1) are on this list after
processing the denominations [5,10]. Thinning with ≼eliminates the latter. Why is
this wrong? (Hint: choose x.)
Exercise 10.17 The function extend in the coin-changing problem was deﬁned by
extend d (cs,r,k) = [(cs++[c],r −c×d,k +c) | c ←[0..r div d]]
How should mkchange be redeﬁned if the term cs ++ [c] is replaced by the more
efﬁcient c:cs?
Exercise 10.18 The ﬁnal algorithm for the coin-changing problem was
mkchange n = coins·minWith cost ·foldr tstep [([ ],n,0)]
What additional simpliﬁcation is possible?
Exercise 10.19 In the deﬁnition of choices in the knapsack problem we deﬁned the
local function
extend i sn = ﬁlter (within w) [sn,add i sn]
What minor optimisation to this deﬁnition is possible?
Exercise 10.20 To fuse ThinBy (≼) and choices in the knapsack problem, we have
to verify the fusion condition
ThinBy (≼) (step i (ThinBy (≼) sns)) ←ThinBy (≼) (step i sns)
where step i = concatMap·extend i. Prove that this condition holds.

260
Introduction to thinning
Exercise 10.21 There is another way of deﬁning the function selections, namely
selections::[Item] →[Selection]
selections = foldr step [([ ],0,0)]
where step i sns = sns++map (add i) sns
Write down the ﬁnal thinning algorithm for this version of selections.
Exercise 10.22 The knapsack problem was speciﬁed using MaxWith value. But
presumably the thief would prefer a selection that not only had the maximum value,
but also had minimum weight. How would you modify the speciﬁcation to take this
aspect into account?
Exercise 10.23 In the integer knapsack problem, selections have the type
type Selection = ([(Nat,Name)],Value,Weight)
Along with each item name there is a count of the number of times the item is
chosen. We can specify swag by
swag::Weight →[Item] →Selection
swag w ←extract ·MaxWith value·choices w
where extract retains only the chosen items:
extract ::Selection →Selection
extract (kns,v,w) = (ﬁlter nonzero kns,v,w)
where nonzero (k,n) = k ̸= 0
Deﬁne choices. Hence write down a thinning algorithm for the integer knapsack
problem.
Exercise 10.24 Why is it impossible to write down an executable speciﬁcation for
the fractional knapsack problem? It is, however, possible to show that a greedy
algorithm works. The method is to consider items in decreasing order of value-
to-weight ratio. At each step, the whole of the next item is chosen if the weight
constraint is satisﬁed, otherwise the maximum possible proportion of the next item
is chosen and the algorithm terminates. That way, up to the whole of the capacity of
the knapsack can be used. If all weights are integers, then it is possible to express
the greedy algorithm using rational arithmetic. Selections therefore have type
type Selection = ([(Rational,Name)],Value,Weight)
type Value = Rational
type Weight = Rational
Write down the deﬁnition of a function gswag that has the type
gswag::Weight →[Item] →[(Rational,Name)]
Hint: one answer is to sort the values in increasing order of value-to-weight ratio
and then to process from right to left.

Answers
261
Answers
Answer 10.1 Yes, we have [ ] = ThinBy (≼) [ ].
Answer 10.2 One possibility is
thinBy (≼) = foldl bump [ ]·reverse
where bump [ ] x
= [x]
bump (y:ys) x | x ≼y
= x:ys
| y ≼x
= y:ys
| otherwise = x:y:ys
Another method, which may not give the same answer, is to deﬁne
thinBy (≼) [ ] = [ ]
thinBy (≼) [x] = [x]
thinBy (≼) (x:y:xs)
| x ≼y
= thinBy (≼) (x:xs)
| y ≼x
= thinBy (≼) (y:xs)
| otherwise = x:thinBy (≼) (y:xs)
Answer 10.3 The straightforward deﬁnition of candidates is
candidates (≼) xs = [ys | ys ←subseqs xs,ok xs ys]
where ok xs ys = and [or [y ≼x | y ←ys] | x ←xs]
The prelude functions and and or return the conjunction and disjunction of a list of
Booleans.
Answer 10.4 One deﬁnition is
thinBy (≼) = foldr gstep [ ]
where gstep x ys = if any (≼x) ys then ys else x:ﬁlter (not ·(x ≼)) ys
where the prelude function any p is deﬁned by any p = or ·map p. This computes a
shortest thinning; but the proof that it does so is rather involved, and is omitted.
Answer 10.5 No, it holds only if ≼is reﬂexive.
Answer 10.6 The following deﬁnition of thinBy removes at most one element:
thinBy (≼) [ ]
= [ ]
thinBy (≼) [x]
= [x]
thinBy (≼) (x:y:xs) = if x ≼y then x:xs else x:thinBy (≼) (y:xs)
For example,
thinBy (⩽) [1,2,3] = [1,3]
thinBy (⩽) [2,1,3] = [2,1]
Thinning twice can remove two elements, so thinBy is not idempotent.

262
Introduction to thinning
Answer 10.7 The ﬁrst reﬁnement follows from the identity law and the monotonic-
ity of functional composition under reﬁnement. The second reﬁnement follows from
the transitivity of ≼and transitivity of ⊑, the subsequence relation.
Answer 10.8 The ﬁrst follows from the identity law. For the second we have to
show that ys ←ThinBy (≼) xs and y ←MinWith cost ys, then y ←MinWith cost xs.
This fact follows easily from the proviso.
Answer 10.9 We have to show that
x ←MinWith cost xs ⇒[x] ←ThinBy (≼) xs
This comes down to cost x ⩽cost y ⇒x ≼y for all y ∈xs, which is just the proviso.
Answer 10.10 We have to show that
ys ←ThinBy (≼) xs ∧zs = ﬁlter p ys
⇒zs ←ThinBy (≼) (ﬁlter p xs)
zs ←ThinBy (≼) (ﬁlter p xs)
⇒(∃ys : ys ←ThinBy (≼) xs ∧zs = ﬁlter p ys)
For the ﬁrst implication we have zs ⊑ys ⊑xs, since ⊑is transitive. Furthermore, it
follows from ys ←ThinBy (≼) xs and the proviso that, if x ∈xs and p x, then there
exists a y ∈ys such that y ≼x and p y. Hence y ∈zs, and so zs is a valid reﬁnement
of ThinBy (≼) (ﬁlter p xs).
For the second implication take ys to be the subsequence of xs consisting of zs
together with all the elements of xs not satisfying p. Thus zs = ﬁlter p ys. Suppose
x ∈xs; either p x holds, in which case there exists a z ∈zs such that z ≼x, or p x
does not hold, in which case x ∈ys and x ≼x. Hence ys is a valid reﬁnement of
ThinBy (≼) xs.
Answer 10.11 Suppose ys ←ThinBy (≼) xs. We have to show that
map f ys ←ThinBy (≼) (map f xs)
This follows from the proviso and the fact that
ys ⊑xs ⇒map f ys ⊑map f xs
The other thin-map law
ThinBy (≼)·map f ←map f ·ThinBy (≼)
is also straightforward.
Answer 10.12 Let x ≼y = (x ⩽y). Then
[1] ←ThinBy (≼) (concat [[1],[2]])
but
concat [ThinBy (≼) [1],ThinBy (≼) [2]] = [1,2]

Answers
263
Answer 10.13 We have to show
ﬁlter connected (op es ps) = step es (ﬁlter connected ps)
The proof is
ﬁlter connected (op es ps)
=
{ deﬁnition of op }
[e:p | e ←es,p ←ps,connected (e:p)]
=
{ deﬁnition of connected }
[e:p | e ←es,p ←ﬁlter connected ps,linked e p]
=
{ deﬁnition of step }
step es (ﬁlter connected ps)
Answer 10.14 We can deﬁne mcp by
mcp::Nat →Net →Path
mcp k = snd ·minWith fst ·elems·foldr step start
where start
= array (1,k) [(v,(0,[ ])) | v ←[1..k]]
step es pa = accumArray better initial (1,k) (map insert es)
where initial
= (maxInt,[ ])
insert (u,v,w) = (u,(add w c,(u,v,w):p))
where (c,p) = pa!v
better (c1,p1) (c2,p2) = if c1 ⩽c2 then (c1,p1) else (c2,p2)
The value maxInt and the function add are as deﬁned in Dijkstra’s algorithm:
maxInt ::Int
maxInt = maxBound
add w c = if c == maxInt then maxInt else w+c
Each call of step takes O(k+e), where e is the number of edges in the current layer,
so the total running time is O(dk2) as there are O(dk2) edges in total.
Answer 10.15 One deﬁnition is
mergeBy::(a →a →Bool) →[[a]] →[a]
mergeBy cmp = foldr merge [ ]
where merge xs [ ]
= xs
merge [ ] ys
= ys
merge (x:xs) (y:ys)
| cmp x y
= x:merge xs (y:ys)
| otherwise = y:merge (x:xs) ys
Answer 10.16 Because if x = 4 the tuple ([0,1,2],0,3) would not be produced and
the minimum-cost solution would not be found.

264
Introduction to thinning
Answer 10.17 Simply replace coins by reverse·coins.
Answer 10.18 Since tstep produces answers in decreasing order of residue, we can
write
mkchange n = coins·last ·foldr tstep [([ ],n,0)]
Answer 10.19 Since only selections that satisfy the capacity constraint are main-
tained, we could have deﬁned
extend i sn = sn:ﬁlter (within w) [add i sn]
Other deﬁnitions are of course possible.
Answer 10.20 A direct attack fails because ThinBy (≼)·extend i = extend i. Instead
we have to show that (10.2) holds, namely that, if sn1 ≼sn2, then
∀en2 ∈extend i sn2 : ∃en1 ∈extend i sn1 : en1 ≼en2
This comes down to the fact that add i sn1 is a valid choice if add i sn2 is, and that
sn1 ≼sn2 ⇒add i sn1 ≼add i sn2
Answer 10.21 The deﬁnition is
swag w = maxWith value·foldr tstep [([ ],0,0)]
where tstep i sns = thinBy (≼) (mergeBy cmp [sns,sns′])
where sns′ = ﬁlter (within w) (map (add i) sns)
cmp sn1 sn2 = weight sn1 ⩽weight sn2
Answer 10.22 One solution is to introduce a cost function
cost ::Selection →(Value,Weight)
cost sn = (value sn,negate (weight sn))
and replace maxWith value by maxWith cost. Another solution would be to replace
maxWith value by maxBy (≼), where
sn1 ≼sn2 = value sn1 <value sn2 ∨
(value sn1 == value sn2 ∧weight sn1 ⩾weight sn2)
That would involve a new maximisation function
maxBy::(a →a →Bool) →[a] →a
maxBy (≼) = foldr1 higher
where higher x y = if x ≼y then y else x

Answers
265
Answer 10.23 The deﬁnition is
choices::Weight →[Item] →[Selection]
choices w = foldr (concatMap·choose) [([ ],0,0)]
where choose i sn = [add k i sn | k ←[0..max]]
where max = (w−weight sn) div weight i
add ::Nat →Item →Selection →Selection
add k i (kns,v,w) = ((k,name i):kns,k ×value i+v,k ×weight i+w)
The function add k selects k copies of the next item, where k is constrained so that
the knapsack capacity is not exceeded. Note that values of k are chosen in increasing
order, so the weights of selections are in increasing order. The thinning algorithm is
then deﬁned by
swag w = extract ·maxWith value·foldr tstep [([ ],0,0)]
where tstep i
= thinBy (≼)·mergeBy cmp·map (choose i)
choose i sn = [add k i sn | k ←[0..max]]
where max = (w−weight sn) div weight i
cmp sn1 sn2 = weight sn1 ⩽weight sn2
Answer 10.24 In the fractional knapsack problem there is an inﬁnite, in fact an
uncountably inﬁnite, number of choices for each item, one for each real number x in
the range 0 ⩽x ⩽1. So no executable speciﬁcation is possible. When the weights
are integers, each choice is a rational number r in the range 0 ⩽r ⩽1, reducing the
number of choices to a countably inﬁnite number. The greedy algorithm is
gswag::Weight →[Item] →[(Rational,Name)]
gswag w = extract ·foldr (add w) ([ ],0,0)·sortBy cmp
extract ::Selection →[(Rational,Name)]
extract (rns, , ) = reverse rns
add ::Weight →Item →Selection →Selection
add w i (rns,vn,wn) = if wn == w then (rns,vn,wn)
else ((r,name i):rns,vn+r ×vi,wn+r ×wi)
where r = min 1 ((w−wn)/wi)
wi = fromIntegral (weight i)
vi = fromIntegral (value i)
cmp::Item →Item →Ordering
cmp i1 i2 = compare (value i1 ×weight i2) (value i2 ×weight i1)
For example, gswag 50 items returns the answer
[(1%1,"Jewellery"),(1%1,"TV"),(11%14,"Laptop")]
Quite how the thief steals eleven-fourteenths of a laptop we leave to your imagina-
tion.


Chapter 11
Segments and subsequences
By deﬁnition, a segment of a list is a contiguous subsequence of the list. Thus "arb"
is a segment of "barbara" while "bab" is a subsequence but not a contiguous one.
A segment that begins a list is called a preﬁx or an initial segment, and one that ends
a list a sufﬁx or tail segment. Segments are also called factors or substrings in the
literature, but we will reserve the word ‘segment’ for the contiguous subsequences.
A list can have an exponential number of subsequences but only a quadratic number
of segments.
Problems involving segments and subsequences abound in computing. For ex-
ample, they arise in genomics, text processing, data mining, and data compression.
Whole books have been written on ‘stringology’ and many interesting, subtle, and
useful algorithms have been discussed and analysed over the years. In this chapter
we conﬁne our attention to three simply stated problems, one involving segments
and two involving subsequences. The segment problem is the most complicated of
the three, so we will begin with the two problems about subsequences.
11.1 The longest upsequence
Given a sequence of elements from an ordered type, the function lus computes some
longest subsequence whose elements are in strictly increasing order (in other words,
a longest upsequence):
lus::Ord a ⇒[a] →[a]
lus ←MaxWith length·ﬁlter up·subseqs
For example, "lost" is a longest upsequence of "longest". The test up can be
deﬁned by
up::Ord a ⇒[a] →Bool
up xs = and (zipWith (<) xs (tail xs))

268
Segments and subsequences
The function subseqs can be deﬁned in a number of ways (see the exercises); here
are two, both based on foldr. The ﬁrst is to write
subseqs::[a] →[[a]]
subseqs = foldr step [[ ]]
where step x xss = xss++map (x:) xss
The second way is to write
subseqs::[a] →[[a]]
subseqs = foldr (concatMap·extend) [[ ]]
where extend x xs = [xs,x:xs]
The second method is essentially the one used in the deﬁnition of selections in the
knapsack problem of the previous chapter. For the sake of variety we will adopt
the ﬁrst deﬁnition. In either case, straightforward implementation of lus leads to an
algorithm with exponential time simply because there are an exponential number of
subsequences that have to be checked. Our aim is to do better; in fact there is an
O(n log n) time algorithm for the problem.
The ﬁrst step in the standard recipe is to fuse ﬁlter up and subseqs to arrive at
lus ←MaxWith length·foldr step [[ ]]
where step x xss = xss++map (x:) (ﬁlter (ok x) xss)
ok x ys
= null ys ∨x<head ys
Only upsequences are kept at each step. An element x can be added to the front of
an upsequence ys if either ys is the empty sequence or its ﬁrst element is greater
than x.
The next step is to see whether a greedy algorithm is possible. Can we keep
a single longest upsequence at each step? No, because while "ab" is the unique
longest upsequence of "xab", the longest upsequence of "uvwxab" is "uvwx", so
"x" cannot disappear from view and we need to keep more than one upsequence
in play. A similar argument holds if the input is processed from left to right, so
the failure is not due to use of foldr. The same argument shows that no obvious
divide-and-conquer algorithm is possible either: we can split the input into two
and compute a longest upsequence for each half, but these two upsequences do not
provide sufﬁcient information to determine a longest upsequence of the whole input.
That all means we need to keep more than one candidate in play. So we introduce a
thinning step:
lus ←MaxWith length·ThinBy (≼)·foldr step [[ ]]
We have to ensure
xs ≼ys ⇒length xs ⩾length ys
for the thin-introduction step to be valid, but what else do we need? Well, when

11.1 The longest upsequence
269
building upsequences from right to left, one upsequence is clearly better than another
if it is no shorter and its ﬁrst element, if it exists, is bigger. For instance, "jot" is
a better upsequence to keep in play than "dot" because the former allows more
symbols to be preﬁxed while maintaining an upsequence ("ijot" is an upsequence
but "idot" is not). We also want to keep the empty sequence as a candidate. That
all suggests deﬁning ≼by
[ ]
≼[ ]
= True
(x:xs) ≼[ ]
= False
[ ]
≼(y:ys) = False
(x:xs) ≼(y:ys) = x ⩾y ∧length xs ⩾length ys
The ﬁrst and fourth clauses ensure that ≼is reﬂexive and the length condition holds.
The next step is to fuse ThinBy (≼) and foldr step [[ ]]. To this end we reason as
follows:
ThinBy (≼) (step x xss)
=
{ deﬁnition of step }
ThinBy (≼) (xss++map (x:) (ﬁlter (ok x) xss))
=
{ distributive law of ThinBy }
ThinBy (≼) (ThinBy (≼) xss++ThinBy (≼) (map (x:) (ﬁlter (ok x) xss)))
→
{ thin-map law (see below) }
ThinBy (≼) (ThinBy (≼) xss++map (x:) (ThinBy (≼) (ﬁlter (ok x) xss)))
=
{ thin-ﬁlter law (see below) }
ThinBy (≼) (ThinBy (≼) xss++map (x:) (ﬁlter (ok x) (ThinBy (≼) xss)))
The thin-map and thin-ﬁlter laws rely on the facts that
xs ≼ys
⇒x:xs ≼x:ys
xs ≼ys ∧ok x ys ⇒ok x xs
whose proofs are left as exercises. Hence, deﬁning tstep by
tstep x xss = thinBy (≼) (step x xss)
we have
foldr tstep [[ ]] ←ThinBy (≼)·foldr step [[ ]]
and so lus ←MaxWith length·foldr tstep [[ ]]. Finally, the thinning process can be
made more effective by keeping subsequences in increasing order of length. That
all leads quite quickly to
lus = last ·foldr tstep [[ ]]
tstep x xss = thinBy (≼) (mergeBy cmp [xss,yss])
where yss = map (x:) (ﬁlter (ok x) xss)
cmp xs ys = length xs ⩽length ys
Ignoring length calculations, this version of lus takes O(nr) steps, where n is the

270
Segments and subsequences
length of the input and r is the length of the longest upsequence. At most r + 1
upsequences are kept in play at each stage and these can be updated in O(r) steps.
To discover the path for further optimisation we need to look more closely at the
computation. Observe that at a typical stage a list of upsequences [xs0,xs1,...,xsk] is
maintained in which xsj has length j, and head xsj >head xsj+1 for 1 ⩽j<k. After
applying tstep x to this list, we obtain a new list
[xs0,...,xsj,x:xsj,xsj+2,...,xsk]
where head xsj >x ⩾head xsj+1 (assuming the heads of xs0 and xsk+1 are inﬁnitely
large and inﬁnitely small, respectively). For example, since
foldr tstep [[ ]] "ripper" = ["","r","pr","ipr"]
we obtain
foldr tstep [[ ]] "kripper" = ["","r","pr","kpr"]
foldr tstep [[ ]] "cripper" = ["","r","pr","ipr","cipr"]
foldr tstep [[ ]] "tripper" = ["","t","pr","ipr"]
That means tstep can be redeﬁned to read
tstep x ([ ]:xss) = [ ]:search x [ ] xss
where search x xs [ ]
= [x:xs]
search x xs (ys:xss)
| head ys>x = ys:search x ys xss
| otherwise
= (x:xs):xss
This version of tstep ﬁnds the required insertion point by linear search from left
to right: the ﬁrst ys such that head ys ⩽x is replaced by x : xs, where xs is the
upsequence immediately preceding ys. If there is no such ys, then x : xs is added
to the end of the list, as in the "cripper" example above. Length calculations no
longer appear and the running time of lus is O(nr) steps. In an imperative setting,
the running time can be improved to O(n log r) steps by using an array and binary
search to locate the required insertion point. However, since the array also has to
be updated at each step, and array updates take linear time in a purely functional
setting, this solution does not improve the running time. The alternative is to use a
balanced binary search tree, but we will leave the details to Exercise 11.6.
11.2 The longest common subsequence
The problem of ﬁnding the longest common subsequence of two sequences has
many applications in computing, basically because such a subsequence is a useful
measure of how similar the two sequences are. In this section we consider a function
lcs::Eq a ⇒[a] →[a] →[a]

11.2 The longest common subsequence
271
so that lcs xs ys returns a longest common subsequence of xs and ys. The problem
is interesting because of the number of different ways to solve it. We begin with a
speciﬁcation of the problem, in fact with two speciﬁcations. The ﬁrst is to deﬁne
lcs xs ys ←MaxWith length (intersect (subseqs xs) (subseqs ys))
where intersect returns the common elements of two lists. The second speciﬁcation,
and the one we will use, is to deﬁne
lcs xs ←MaxWith length·ﬁlter (sub xs)·subseqs
where the test sub xs ys determines whether ys is a subsequence of xs:
sub xs
[ ]
= True
sub [ ]
(y:ys) = False
sub (x:xs) (y:ys) = if x == y then sub xs ys else sub xs (y:ys)
The ﬁrst speciﬁcation maintains symmetry between xs and ys, while the second
breaks it. The advantage of the second speciﬁcation is simply that it places us in
familiar territory for ferreting out a thinning algorithm.
For a functional programmer happy with recursion as their basic tool there is a
simple way to solve the problem, which is to write
lcs [ ]
ys
= [ ]
lcs xs
[ ]
= [ ]
lcs (x:xs) (y:ys) = if x == y then x:lcs xs ys
else longer (lcs (x:xs) ys) (lcs xs (y:ys))
The function longer returns the longer of two lists. This solution is an attractive one
because there are no subseqs, ﬁlter, or intersect operations, and it can be justiﬁed
by starting with the symmetric speciﬁcation of lcs and considering the various cases
that can arise. However, this solution takes exponential time, and the reason it does
so is because it involves computing the solutions to the same subproblems many
times over. The way to solve this problem is by dynamic programming, so we will
return to this solution in the next part of the book.
For a mathematician there is another way to solve the problem. Mathematicians
like to reduce problems with unknown solutions to problems with known solutions.
We can do that here. After the previous section we know that the function lus for
computing a longest upsequence can be solved reasonably efﬁciently and, with a
little bit of cleverness, we can compute lcs in terms of lus. The solution takes the
form
lcs xs ys = decode (lus (encode xs ys)) ys
We encode xs and ys as a single list over an ordered alphabet, solve the longest
upsequence problem on this encoded list, and then decode the result. Here is how

272
Segments and subsequences
we encode the two sequences. Suppose ys is the list
0
1
2
3
4
5
'b'
'a'
'a'
'b'
'c'
'a'
The positions of the elements are recorded above the elements. Let xs be the string
"baxca". For each letter in xs we record the positions in ys at which this letter
occurs, but in reverse order. Thus
posns 'b' = [3,0], posns 'a' = [5,2,1], posns 'x' = [ ],
posns 'c' = [4],
posns 'a' = [5,2,1]
The encoded string is the concatenation [3,0,5,2,1,4,5,2,1] of these positions. The
longest upsequence of this list is [0,1,4,5], which decodes to "baca", the longest
common subsequence of xs and ys. We leave it as an exercise to show why the trick
works, and also to supply the deﬁnitions of encode and decode. In the worst case
the encoded string can have length Θ(n2) when both inputs have length n, so the
computation of lus can take Θ(n2 log n) steps. A thinning approach can bring this
worst case time down to Θ(n2) steps.
The ﬁrst step in the standard recipe is to fuse ﬁlter (sub xs) and subseqs. The
success of this step relies on the fact that sub xs (y : ys) ⇒sub xs ys. In words, if
y:ys is a subsequence of xs, then so is ys. Here is the result of the fusion step:
lcs xs ←MaxWith length·foldr step [[ ]]
where step y yss = yss++ﬁlter (sub xs) (map (y:) yss)
Instead of ﬁltering at the end, we can ﬁlter at each step.
The next step is to check whether a greedy algorithm is possible. The longest
common subsequence of "abc" and "cab" is "ab", which cannot be extended
leftwards to the longest common subsequence of "abc" and "abcab", namely
"abc". So we cannot maintain a single subsequence at each step, and have to
introduce thinning. In order to determine which subsequences to keep we need to
know the position of each subsequence in xs. A subsequence can occur more than
once in a sequence; for example "ba" appears four times in "baabca", in positions
[0,1],[0,2],[0,5],[3,5]. When building subsequences from right to left, that is
adding elements to the front of subsequences, we want the position that is lexically
the largest, namely [3,5] in the above example. Such a choice gives the greatest
freedom in adding common elements to the front of the sequence. We do not need the
full position in calculations, but only the position of the ﬁrst element. So the position
of "ba" we want is 3, the rightmost position at which the last occurrence of "ba"
in "baabca" starts. The rightmost position of the empty sequence in "baabca" is
6. We will set the position of a sequence ys in a sequence xs to be −1 if ys is not
a subsequence of xs. The deﬁnition of position is left as an exercise.
A subsequence can be discarded if there is another subsequence whose length
and position are at least as large. Hence for ﬁxed xs we can deﬁne

11.2 The longest common subsequence
273
ys ≼zs = length ys ⩾length zs ∧position xs ys ⩾position xs zs
The thin-introduction law is now applicable, giving that lcs xs is a reﬁnement of
MaxWith length·ThinBy (≼)·foldr step [[ ]]
The next step is to fuse ThinBy with foldr. We can keep subsequences in increasing
order of position, and therefore in decreasing order of length, by merging at each
step. Moreover, the term ﬁlter (sub xs) can be removed from the computation if all
sequences with negative positions are discarded. That leads to
lcs xs = head ·foldr tstep [[ ]]
where tstep y yss = thinBy (≼) (mergeBy cmp [yss,zss])
where zss
= dropWhile negpos (map (y:) yss)
negpos ys = position xs ys<0
ys ≼zs
= length ys ⩾length zs ∧
position xs ys ⩾position xs zs
cmp ys zs = position xs ys ⩽position xs zs
The ﬁnal optimisation is to avoid multiple computations of position and length. To
this end we represent a subsequence us of xs by a quadruple (p,k,ws,us) in which
p = position xs us
k
= length us
ws = reverse (take p xs)
For example, the representation of "ba" as a subsequence of "baabca" is the
quadruple (3,2,"aab","ba"). The function cons x which replaces (x:) is deﬁned
by
cons x (p,k,ws,us) = (p−1−length as,k +1,tail bs,x:us)
where (as,bs) = span (̸= x) ws
For example,
cons 'b' (3,2,"aab","ba") = (0,3,"","bba")
cons 'x' (3,2,"aab","ba") = (−1,3,⊥,"bba")
If x:us is not a subsequence of xs, then the ﬁrst component is negative and the third
is undeﬁned. Now we can deﬁne
lcs xs = ext ·head ·foldr tstep start
where start
= [(length xs,0,reverse xs,[ ])]
tstep y yss = thinBy (≼) (mergeBy cmp [yss,zss])
where zss
= dropWhile negpos (map (cons y) yss)
negpos ys = psn ys<0
q1 ≼q2
= psn q1 ⩾psn q2 ∧lng q1 ⩾lng q2
cmp q1 q2 = psn q1 ⩽psn q2
where ext, psn, and lng are the selector functions

274
Segments and subsequences
ext (p,k,ws,us) = us
psn (p,k,ws,us) = p
lng (p,k,ws,us) = k
This algorithm takes O(mn) steps, where m and n are the lengths of xs and ys.
11.3 A short segment with maximum sum
Our third problem is easy to state but not so easy to solve, at least not with an
efﬁcient algorithm. Given a list of positive and negative integers, the problem is
simply to return a segment of the list that has the largest possible sum subject to the
segment not being too long. Thus we want to compute mss, where
mss::Nat →[Integer] →[Integer]
mss b ←MaxWith sum·ﬁlter (short b)·segments
and short is deﬁned by
short ::Nat →[a] →Bool
short b xs = (length xs ⩽b)
For example,
mss 3 [1,−2,3,0,−5,3,−2,3,−1] = [3,−2,3]
The function segments is deﬁned below. Straightforward computation of mss takes
O(bn) steps. There are Θ(bn) short segments in a list of length n, and we can gen-
erate all of them, along with their sums, in this time. Finding one with a maximum
sum takes linear time, so the algorithm takes O(bn) steps. However, b may be quite
large, and O(n) is a much better bound on the algorithm. The aim of this section is
to describe an algorithm with such a bound. The algorithm is interesting because a
signiﬁcant change of representation is required to achieve the desired efﬁciency, but
it is still basically a thinning algorithm.
First of all, here is one deﬁnition of segments:
segments::[a] →[[a]]
segments = concatMap inits·tails
The segments of the list are therefore obtained by taking all the preﬁxes of all the
sufﬁxes. As we will see, this leads to an algorithm that processes the input from
right to left. We could also have chosen to take all sufﬁxes of all preﬁxes, in which
case the algorithm proceeds from left to right. The functions inits and tails were
discussed in Chapter 2 and are provided in the library Data.List. Both functions
include the empty list as a preﬁx or sufﬁx, so the empty list appears n+1 times in
the segments of a list of length n. There are easy modiﬁcations to the deﬁnitions of
inits and tails that produce only nonempty segments. However, allowing the empty

11.3 A short segment with maximum sum
275
segment as a candidate means that a short segment with maximum sum in a list of
negative numbers is the empty sequence.
We can now reason
MaxWith sum·ﬁlter (short b)·segments
=
{ deﬁnition of segments }
MaxWith sum·ﬁlter (short b)·concatMap inits·tails
=
{ since ﬁlter p·concat = concat ·map (ﬁlter p) }
MaxWith sum·concatMap (ﬁlter (short b)·inits)·tails
=
{ distributive law }
MaxWith sum·map (MaxWith sum·ﬁlter (short b)·inits)·tails
→
{ with msp b ←MaxWith sum·ﬁlter (short b)·inits }
MaxWith sum·map (msp b)·tails
Summarising this calculation, we have shown that
mss b ←MaxWith sum·map (msp b)·tails
msp b ←MaxWith sum·ﬁlter (short b)·inits
The new function msp computes a short preﬁx with maximum sum. For example,
msp 4 [−2,4,4,−5,8,−2,3,1] = [−2,4,4]
msp 6 [−2,4,4,−5,8,−2,3,1] = [−2,4,4,−5,8]
The new form of mss suggests an appeal to the Scan Lemma, an essential tool when
dealing with problems involving segments. The Scan Lemma was mentioned in
Answer 1.12, but here it is again:
map (foldr op e)·tails = scanr op e
Applied to a list of length n, the left-hand side requires Θ(n2) applications of op,
while the right-hand side requires only Θ(n) applications. The function scanr is a
Haskell function in the library Data.List, whose deﬁnition is basically as follows:
scanr ::(a →b →b) →b →[a] →[b]
scanr op e [ ]
= [e]
scanr op e (x:xs) = op x (head ys):ys where ys = scanr op e xs
For example,
scanr (⊕) e [x,y] = [x⊕(y⊕e),y⊕e,e]
Later on, we will need the companion function scanl:
scanl::(b →a →b) →b →[a] →[b]
scanl op e [ ]
= [e]
scanl op e (x:xs) = e:scanl op (op e x) xs
For example,
scanl (⊕) e [x,y] = [e,e⊕x,(e⊕x)⊕y]

276
Segments and subsequences
The Scan Lemma suggests we look for a deﬁnition of msp as an instance of foldr.
Then we would obtain a deﬁnition of mss in terms of scanr. More precisely, if we
can ﬁnd a deﬁnition of msp in the form
msp b = foldr (op b) [ ]
then we can reﬁne mss to read
mss b ←MaxWith sum·scanr (op b) [ ]
As it happens there is such a deﬁnition of msp, but it doesn’t help:
msp b = foldr (op b) [ ] where op b x xs = msp b (x:xs)
This identity cannot serve as a legitimate Haskell deﬁnition of msp because it is
circular. In effect it states no more nor less than that msp b (x : xs) is a preﬁx of
x:msp b xs. Exercise 11.13 asks for a proof of this assertion.
Instead we will follow the standard thinning recipe. The ﬁrst step is to fuse
ﬁlter (short b) with inits, thereby producing only short preﬁxes. The function inits
can be expressed in terms of foldr:
inits::[a] →[[a]]
inits = foldr step [[ ]] where step x xss = [ ]:map (x:) xss
Since the elements of xss are lists in increasing order of length from 0 up to k, where
k is the length of xss, we have
ﬁlter (short b) (step x xss) = if length (last xss) == b
then [ ]:map (x:) (init xss)
else [ ]:map (x:) xss
In words, if adding a new element to the front of the list increases its length beyond
b, then we can simply cut out the last list. An appeal to the fusion law of foldr then
leads to
msp b ←MaxWith sum·foldr (op b) [[ ]]
where
op b x xss = [ ]:map (x:) (cut b xss)
cut b xss = if length (last xss) == b then init xss else xss
Later on we will see how to make the computation of cut more efﬁcient.
The next step is to introduce thinning, reﬁning msp to read
msp b ←MaxWith sum·ThinBy (≼)·foldr (op b) [[ ]]
An appropriate choice of preorder ≼is
xs ≼ys = (sum xs ⩾sum ys) ∧(length xs ⩽length ys)
In words, there is no point in keeping a preﬁx if there is another preﬁx that is shorter
and whose sum is at least as large. For example, optimal thinning of

11.3 A short segment with maximum sum
277
foldr (op 7) [[ ]] [−2,4,4,−5,8,−2,3,9]
produces the preﬁxes
[ ],[−2,4],[−2,4,4],[−2,4,4,−5,8],[−2,4,4,−5,8,−2,3]
of length at most 7 with sums 0,2,6,9,10. These preﬁxes are in increasing order of
length as well as increasing order of sum.
The next step, another appeal to fusion, is to thin at each step rather than just
once at the end. Thinning can be implemented by taking advantage of the fact that
the preﬁxes are in strictly increasing order of length and in strictly increasing order
of sum. This means we only have to delete a nonempty preﬁx if its sum is less than
or equal to zero. That gives
msp b
= last ·foldr (op b) [[ ]]
op b x xss = [ ]:thin (map (x:) (cut b xss))
thin
= dropWhile (λxs.sum xs ⩽0)
In other words, we cut from the end of the list to keep the preﬁxes short, and thin
from the front of the list to keep sums positive. The preﬁx with the largest sum is
the last preﬁx in the sequence. Now we can deﬁne
mss b = maxWith sum·map last ·scanr (op b) [[ ]]
However, this deﬁnition of op will not sufﬁce for the ﬁnal algorithm. Even ignoring
the cost of cutting and thinning, the map operations mean that computation of op
on a list of length k takes O(k) steps. In the worst case, when the input is a list of
positive numbers, we have k = b, so the total running time of mss is O(bn) steps
for an input of length n. That’s no better than before. The way to achieve a bound of
O(n) steps is by changing the representation of the list of preﬁxes.
The idea is simple enough: represent the list of preﬁxes by their differences. For
example, instead of maintaining the list
[[ ],[−2,4],[−2,4,4],[−2,4,4,−5,8],[−2,4,4,−5,8,−2,3]]
we maintain the partition [[−2,4],[4],[−5,8],[−2,3]] of the last element. More
precisely, suppose we deﬁne the abstraction function
abst ::[[a]] →[[a]]
abst = scanl (++) [ ]
Then
abst [[−2,4],[4],[−5,8],[−2,3]]
= [[ ],[−2,4],[−2,4,4],[−2,4,4,−5,8],[−2,4,4,−5,8,−2,3]]
In particular, last ·abst = concat. To effect the change in representation we need a
function, opR say, so that
abst (opR b x xss) = op b x (abst xss)

278
Segments and subsequences
Then, by the fusion law of foldr, we have
abst ·foldr (opR b) [ ] = foldr (op b) [[ ]]
since abst [ ] = [[ ]]. Note that we seek to apply the law in the anti-fusion or ﬁssion
direction, splitting the fold on the right into two functions. To deﬁne opR we need
the function
cutR b xss = if length (concat xss) == b then init xss else xss
as a replacement for cut. The function cutR satisﬁes
cut b (abst xss) = abst (cutR b xss)
We also need a replacement for thin, which we will call thinR. This function will
satisfy
[ ]:thin (map (x:) (abst xss)) = abst (thinR x xss)
We can now deﬁne opR by
opR b x xss = thinR x (cutR b xss)
Here is the proof that this choice works:
abst (opR b x xss)
=
{ deﬁnition of opR }
abst (thinR x (cutR b xss))
=
{ above property of thinR }
[ ]:thin (map (x:) (abst (cutR b xss)))
=
{ above property of cutR }
[ ]:thin (map (x:) (cut b (abst xss)))
=
{ deﬁnition of op }
op b x (abst xss)
Now, putting everything together, we have
mss b
=
{ deﬁnition of mss in terms of msp }
maxWith sum·map (msp b)·tails
=
{ deﬁnition of msp in terms of foldr }
maxWith sum·map (last ·foldr (op b) [[ ]])
=
{ deﬁnition of opR }
maxWith sum·map (last ·abst ·foldr (opR b) [ ])·tails
=
{ Scan Lemma }
maxWith sum·map (last ·abst)·scanR (opR b) [ ]
=
{ since last ·abst = concat }
maxWith sum·map concat ·scanR (opR b) [ ]
Hence

11.3 A short segment with maximum sum
279
mss b = maxWith sum·map concat ·scanr (opR b) [ ]
It remains to give the deﬁnition of thinR:
thinR x xss = add [x] xss
where add xs xss
| sum xs>0 = xs:xss
| null xss
= [ ]
| otherwise = add (xs++head xss) (tail xss)
For example,
add [−5] [[−2,3],[6],[−1,4]] = add [−5,−2,3] [[6],[−1,4]]
= add [−5,−2,3,6] [[−1,4]]
= [[−5,−2,3,6],[−1,4]]
If the current segment has positive sum, then it is added to the front of the list
of segments; otherwise it is concatenated with the next segment and the process
is repeated. If no segment has positive sum, then the empty list is returned. The
function add is similar to the function collapse we considered in Section 1.5 and
indeed was the inspiration for collapse.
The ﬁnal step is to ensure that all the length, concat, init, sum, and ++ opera-
tions are implemented efﬁciently. Firstly, we tuple partitions and segments with
their sums and lengths. Secondly, since partitions are processed at both ends, we
need symmetric lists (see Chapter 3) to ensure that init and cons operations take
constant time. Finally, to make segment concatenation efﬁcient, we introduce an
accumulating function. Here are the relevant deﬁnitions:
type Partition = (Sum,Length,SymList Segment)
type Segment = (Sum,Length,[Integer] →[Integer])
type Sum
= Integer
type Length
= Nat
We use the functions sumP, lenP, and segsP to extract the components of a partition,
and sumS, lenS, and segS to extract the components of a segment. The function opR
is replaced by opP, deﬁned by
opP b x xss = thinP x (cutP b xss)
where cutP is deﬁned by
cutP::Length →Partition →Partition
cutP b xss = if lenP xss == b then initP xss else xss
initP::Partition →Partition
initP (s,k,xss) = (s−t,k −m,initSL xss) where (t,m, ) = lastSL xss
and thinP is deﬁned by

280
Segments and subsequences
thinP::Integer →Partition →Partition
thinP x xss = add (x,1,([x]++)) xss
add ::Segment →Partition →Partition
add xs xss | sumS xs>0
= consP xs xss
| lenP xss == 0 = emptyP
| otherwise
= add (catS xs (headP xss)) (tailP xss)
The subsidiary functions are deﬁned by
consP::Segment →Partition →Partition
consP xs (s,k,xss) = (sumS xs+s,lenS xs+k,consSL xs xss)
emptyP::Partition
emptyP = (0,0,nilSL)
headP::Partition →Segment
headP xss = headSL (segsP xss)
tailP::Partition →Partition
tailP (s,k,xss) = (s−t,k −m,tailSL xss) where (t,m, ) = headSL xss
catS::Segment →Segment →Segment
catS (s,k,f) (t,m,g) = (s+t,k +m,f ·g)
The ﬁnal deﬁnition of mss is now given by
mss b = extract ·maxWith sumP·scanr (opP b) emptyP
extract ::Partition →[Integer]
extract = concatMap (ﬂip segS [ ])·fromSL·segsP
We have ﬂip segS [ ] xs = segS xs [ ], so the accumulating function of a segment is
applied to the empty list at the very end of the computation, and the results are
concatenated to produce the ﬁnal answer.
It remains to time the program. With the exception of add, all the other functions
appearing in opP take constant time. The function add takes an additional number
of steps proportional to the number of segments deleted. But the total number of
segments deleted cannot exceed the total number added, which is at most n for an
input of length n. Thus add takes amortised constant time. Computing extract can
take O(b) steps, so the total time for computing a short segment with maximum
sum is O(n+b) = O(n) steps, as we promised at the outset.
11.4 Chapter notes
There are a number of books on stringology, including [1, 2, 6]. All three of these
texts discuss the longest common subsequence problem and other related problems,
such as the edit-distance problem and the problem of optimal alignment. Gusﬁeld [6]

Exercises
281
describes the reduction of the longest common subsequence problem to the longest
upsequence problem used in this chapter. The upsequence problem is a favourite
example in formal program design for showing the use of loop invariants, and is
treated in [5, 4] as well as in a number of other places.
The maximum-sum short segment problem was discussed in [7]. Other problems
about ﬁnding segments with various properties are described in [3] and [8].
References
[1]
Maxime Crochemore, Christof Hancart, and Thierry Lecroq. Algorithms on Strings.
Cambridge University Press, Cambridge, 2007.
[2]
Maxime Crochemore and Wojciech Rytter. Jewels of Stringology. World Scientiﬁc
Publishing, Singapore, 2003.
[3]
Sharon Curtis and Shin-Cheng Mu. Calculating a linear-time solution to the
densest-segment problem. Journal of Functional Programming, 25:e22, 2015.
[4]
Edsger W. Dijkstra and Wim H. J. Feijen. A Method of Programming.
Addison-Wesley, Reading, MA, 1988.
[5]
David Gries. The Science of Programming. Springer, New York, 1981.
[6]
Dan Gusﬁeld. Algorithms on Strings, Trees, and Sequences. Cambridge University
Press, Cambridge, 1997.
[7]
Yaw-Ling Lin, Tao Jiang, and Kun-Mao Chao. Efﬁcient algorithms for locating the
length-constrained heaviest segments with applications to biomolecular sequence
analysis. Journal of Computer and System Sciences, 65(3):570–586, 2002.
[8]
Hans Zantema. Longest segment problems. Science of Computer Programming,
18(1):39–66, 1992.
Exercises
Exercise 11.1 Precisely how many segments and subsequences are there of a list
of n distinct elements? How many segments are there of length at most b?
Exercise 11.2 Write down a deﬁnition of subseqs that produces subsequences in
ascending order of length. No length calculations are allowed.
Exercise 11.3 With the deﬁnitions of ≼and ok given in the longest upsequence
problem, we claimed
xs ≼ys
⇒x:xs ≼x:ys
xs ≼ys ∧ok x ys ⇒ok x xs
Prove these claims.
Exercise 11.4 Can the deﬁnition of ≼in the longest upsequence problem be re-
placed by
xs ≼ys = length xs ⩾length ys ∧xs ⩾ys
or not?

282
Segments and subsequences
Exercise 11.5 Suppose we deﬁned an upsequence to be one whose elements are
only weakly increasing. Thus we change up to read
up xs = and (zipWith (⩽) xs (tail xs))
Write down a deﬁnition of tstep for which lwus = last ·foldr tstep [[ ]].
Exercise 11.6 As mentioned in the text, the longest upsequence problem can be
solved in O(n log r) steps by using a balanced binary search tree. The aim of the
following three exercises is to construct such a solution. The material depends on
Section 4.3 and Section 4.4, so reread those sections ﬁrst. Recall the deﬁnition
data Tree a = Null | Node Int (Tree a) a (Tree a)
from Section 4.3. A list xss = [xs0,xs1,...,xsk] of upsequences is represented by a
tree t of type Tree [a] such that ﬂatten t = xss. The leftmost value xs0 is the empty
sequence. As a warm-up exercise, deﬁne the function rmost that returns the last
entry xsk.
Exercise 11.7 Following on, the new deﬁnition of lus takes the form
lus::Ord a ⇒[a] →[a]
lus = rmost ·foldr update (Node 1 Null [ ] Null)
where update x t = modify x (split x t)
The value of split x t is a pair of trees, the ﬁrst of which is a tree whose labels consist
of the empty list and lists y:xs for which y>x, and the second is a tree whose labels
are lists y:xs for which y ⩽x. This function is deﬁned exactly as in Section 4.4:
split ::Ord a ⇒a →Tree [a] →(Tree [a],Tree [a])
split x t = sew (pieces x t [ ])
However, the deﬁnition of pieces is different. This time we have
pieces::Ord a ⇒a →Tree [a] →[Piece [a]] →[Piece [a]]
where, as in Section 4.4, we have
data Piece a = LP (Tree a) a | RP a (Tree a)
Recall that a left piece LP l x is missing its right subtree, and a right piece RP x r is
missing its left subtree. The deﬁnition of pieces x t ps is different because the labels
of t, apart from the leftmost label [ ], are in decreasing rather than increasing order.
Give the modiﬁed deﬁnition of pieces.
Exercise 11.8 The deﬁnition of sew is the same as in Section 4.4, so it remains to
deﬁne modify x (t1,t2). If t2 is not Null, then modify returns a tree that results from
combining t1 and a modiﬁed tree obtained from t2 by replacing the leftmost label of
t2 with x:xs, where xs is the rightmost label of t1. If t2 is Null, then a new node with
label x:xs is created. As a ﬁnal task, deﬁne modify in terms of the function combine
from Section 4.4.

Answers
283
Exercise 11.9 Write down the deﬁnitions of encode and decode for which
lcs xs ys = decode (lus (encode xs ys)) ys
Show that each upsequence of encode xs ys corresponds to a common subsequence
of xs and ys with the same length.
Exercise 11.10 One way of deﬁning the function position is by using a helper
function:
position xs ys = help (length xs) (reverse xs) (reverse ys)
Deﬁne help, making sure that the result is negative if ys is not a subsequence of xs.
Exercise 11.11 Recall that for a given xs the preorder ≼for the longest common
subsequence problem was deﬁned by
ys ≼zs = length ys ⩾length zs ∧position xs ys ⩾position xs zs
Show that
ys ≼zs ∧sub xs zs ⇒sub xs ys
ys ≼zs
⇒y:ys ≼y:zs
Hence justify the reﬁnement
tstep y (ThinBy (≼) yss) ←ThinBy (≼) (step y yss)
where tstep y yss ←ThinBy (≼) (step y yss).
Exercise 11.12 Express tails as an instance of scanr and inits as an instance of
scanl.
Exercise 11.13 Show that msp b (x:xs) is a preﬁx of x:msp b xs.
Exercise 11.14 A similar but much simpler problem about segments is to ﬁnd a
segment with maximum sum with no length restrictions:
mss ←MaxWith sum·segments
Write down a deﬁnition of msp for which
mss ←MaxWith sum·map msp·tails
Find a function step for which msp = foldr step [ ], and hence construct a simple
linear-time algorithm for mss.
Answers
Answer 11.1 Every element can be included or excluded in a subsequence, giving
2n subsequences in total. The number of nonempty segments of length j is n−j+1,
so the total number of nonempty segments is

284
Segments and subsequences
n
∑
j=1
(n−j+1) =
n
∑
j=1
j = n(n+1)/2
The number of nonempty segments of length at most b is
b
∑
j=1
(n−j+1) =
b−1
∑
j=0
(n−j) = bn−
b−1
∑
j=0
j = bn−b(b−1)/2
Answer 11.2 Perhaps the simplest method is to maintain a list of lists of subse-
quences, the ﬁrst list being all the subsequences of length 0, the second list all the
subsequences of length 1, and so on. This list can be updated as each new element is
processed, and then can be concatenated at the end of the computation. Thus we have
subseqs = concat ·foldr op [[[ ]]]
where
op::a →[[[a]]] →[[[a]]]
op x (xss:xsss) = xss:step x xss xsss
step x xss [ ]
= [map (x:) xss]
step x xss (yss:ysss) = (map (x:) xss++yss):step x yss ysss
Answer 11.3 We have
x:xs ≼x:ys
⇐
{ deﬁnition of ≼}
length xs ⩾length ys
⇐
{ deﬁnition of ≼}
xs ≼ys
The second claim is immediate if both xs and ys are the empty sequence. Otherwise
we can argue
u:us ≼v:vs ∧ok x (v:vs)
⇒
{ deﬁnition of ≼and ok }
u ⩾v ∧x<v
⇒
{ deﬁnition of ok }
ok x (u:us)
Answer 11.4 No. We have xs ≼[ ] for all xs, so the empty list would be removed
by any thinning step.
Answer 11.5 The only change is to replace > by ⩾:
tstep x (xs:xss) = xs:search xs x xss
where search xs x [ ]
= [x:xs]
search xs x (ys:xss) | head ys ⩾x = ys:search ys x xss
| otherwise
= (x:xs):xss

Answers
285
Answer 11.6 The deﬁnition of rmost is
rmost ::Tree [a] →[a]
rmost (Node
l xs Null) = xs
rmost (Node
l xs r)
= rmost r
Answer 11.7 The deﬁnition of pieces is
pieces x Null ps = ps
pieces x (Node
l xs r) ps
| null xs ∨(x<head xs) = pieces x r (LP l xs:ps)
| otherwise
= pieces x l (RP xs r :ps)
Answer 11.8 The deﬁnition of modify is
modify::a →(Tree [a],Tree [a]) →Tree [a]
modify x (t1,t2) = combine t1 (replace (x:rmost t1) t2)
replace::[a] →Tree [a] →Tree [a]
replace xs Null
= Node 1 Null xs Null
replace xs (Node h Null ys r) = Node h Null xs r
replace xs (Node h l ys r)
= Node h (replace xs l) ys r
Answer 11.9 Here are possible deﬁnitions:
encode xs ys = concatMap (posns ys) xs
posns ys x
= reverse [i | (i,y) ←zip [0..] ys,y == x]
decode us ys = pick us (zip [0..] ys)
where
pick [ ] pys
= [ ]
pick (u:us) ((p,y):pys) = if u == p then y:pick us pys
else pick (u:us) pys
Each upsequence of encode xs ys obviously decodes to a subsequence of ys of the
same length since any list of increasing positions in ys corresponds to a subsequence
of ys. Each upsequence also corresponds to a subsequence of xs, as we can see by
deﬁning
decode1 us xs ys = pick us [(posns ys x,x) | x ←xs]
where
pick [ ]
psxs
= [ ]
pick (u:us) ((ps,x):psxs) = if u ∈ps then x:pick us psxs
else pick (u:us) psxs
Then decode1 (lus (encode xs ys)) xs ys decodes to a subsequence of xs.

286
Segments and subsequences
Answer 11.10 The deﬁnition is
help p xs [ ]
= p
help p [ ] ys
= −1
help p (x:xs) (y:ys)
| x == y
= help (p−1) xs ys
| otherwise = help (p−1) xs (y:ys)
Answer 11.11 For the ﬁrst condition it is sufﬁcient to observe that
position xs ys ⩾position xs zs
implies that ys is a subsequence of xs if zs is.
For the second condition we can prove that
position xs ys ⩾position xs zs ⇒position xs (y:ys) ⩾position xs (y:zs)
by case analysis: either position xs (y:zs) = −1, in which case the result is immedi-
ate, or position xs (y:zs) ⩾0, in which case both y:zs and y:ys are subsequences
of xs and the position of y:ys is at least as large as the position of y:zs.
For the last part, we argue
ThinBy (≼) (step y yss)
=
{ deﬁnition of step }
ThinBy (≼) (yss++ﬁlter (sub xs) (map (y:) yss))
=
{ distributive law of ThinBy }
ThinBy (≼) (ThinBy (≼) yss++
ThinBy (≼) (ﬁlter (sub xs) (map (y:) yss)))
→
{ thin-ﬁlter law }
ThinBy (≼) (ThinBy (≼) yss++
ﬁlter (sub xs) (ThinBy (≼) (map (y:) yss)))
=
{ thin-map law }
ThinBy (≼) (ThinBy (≼) yss++
ﬁlter (sub xs) (map (y:) (ThinBy (≼) yss)))
→
{ given tstep y yss ←Thinby (≼) (step y yss) }
tstep y (ThinBy (≼) yss)
Answer 11.12 We have
tails = scanr (λx xs.[x]++xs) [ ]
inits = scanl (λxs x.xs++[x]) [ ]
Answer 11.13 Suppose msp b xs = ys and suppose to the contrary that x : ys is a
proper preﬁx of msp b (x:xs). That means
msp b (x:xs) = x:ys++zs
for some nonempty sequence zs. But

Answers
287
sum (ys++zs) = sum ys+sum zs ⩽sum ys
by deﬁnition of msp b xs, and
x+sum ys<x+sum ys+sum zs
by deﬁnition of msp b (x:xs), so sum zs is both positive and negative, giving rise to
a contradiction.
Answer 11.14 We have
msp ←MaxWith sum·inits
We can ﬁnd a greedy algorithm for msp, maintaining a preﬁx with maximum sum at
each step. Tupling sum computations, we then have
msp
= snd ·foldr step (0,[ ])
step x (s,xs) = if x+s>0 then (x+s,x:xs) else (0,[ ])
And now
mss = snd ·maxWith fst ·scanr step (0,[ ])


Chapter 12
Partitions
By deﬁnition, a partition of a nonempty list is a division of the list into nonempty
segments. For example, ["par","tit","i","on"] is one partition of the string
"partition". Partitions arise in a variety of problems. For instance, the segment
problem of the previous chapter involved partitioning the preﬁxes of a list to achieve
efﬁciency. In one version of Mergesort the input is partitioned into runs of non-
decreasing elements before merging. In operations research, the scheduling of a
sequence of activities can often be speciﬁed in terms of partitioning the activities.
Partitions also arise in various data-compression and text-processing algorithms. In
this chapter we will conﬁne ourselves to just two examples. The ﬁrst is a simple
scheduling problem, while the second involves breaking paragraphs into lines.
12.1 Ways of generating partitions
First of all, let us look at some of the ways we can generate all the partitions of a
list. A partition of a list of type [A] has type [[A]], so a list of partitions has type
[[[A]]]. To improve readability, we introduce the type synonyms
type Partition a = [Segment a]
type Segment a = [a]
A list of partitions now has the more readable type [Partition a]. By deﬁnition, xss
is a partition of xs just in the case that
concat xss = xs ∧all (not ·null) xss
In particular, the empty list is the only partition of the empty list. The following
recursive deﬁnition of parts can be derived from the speciﬁcation above:
parts::[a] →[Partition a]
parts [ ] = [[ ]]
parts xs = [ys:yss | (ys,zs) ←splits xs,yss ←parts zs]

290
Partitions
Each partition is generated by taking a nonempty preﬁx of the input list as the ﬁrst
segment, and then following it with a partition of the remaining sufﬁx. The function
splits splits a nonempty list xs into a pair of lists (ys,zs) such that ys is nonempty
and ys++zs = xs:
splits::[a] →[([a],[a])]
splits [ ]
= [ ]
splits (x:xs) = ([x],xs):[(x:ys,zs) | (ys,zs) ←splits xs]
There are other ways of deﬁning parts, including inductive deﬁnitions based on
either foldr or foldl. One deﬁnition of parts in terms of foldr is
parts::[a] →[Partition a]
parts = foldr (concatMap·extendl) [[ ]]
where extendl extends a partition on the left:
extendl::a →Partition a →[Partition a]
extendl x [ ] = [cons x [ ]]
extendl x p = [cons x p,glue x p]
cons,glue::a →Partition a →Partition a
cons x p
= [x]:p
glue x (s:p) = (x:s):p
The two ways of extending a nonempty partition with a new element on the left are
to start a new segment, or to ‘glue’ the element onto the ﬁrst segment, provided
such a segment exists.
The corresponding deﬁnition of parts in terms of foldl is
parts::[a] →[Partition a]
parts = foldl (ﬂip (concatMap·extendr)) [[ ]]
where, this time, extendr extends a partition on the right:
extendr ::a →Partition a →[Partition a]
extendr x [ ] = [snoc x [ ]]
extendr x p = [snoc x p,bind x p]
snoc,bind ::a →Partition a →Partition a
snoc x p = p++[[x]]
bind x p = init p++[last p++[x]]
The functions snoc and bind are the dual variants of cons and glue (bind has the
merit of being pronounceable, while eulg is not). Of course, snoc and bind do not
take constant time, but we can deal with that problem as and when the need arises.
It seems like a free choice as to whether to use a deﬁnition of parts in terms of
foldr or foldl, but for some problems the right choice is important. Many problems

12.2 Managing two bank accounts
291
about partitions ask for a partition in which all of its component segments satisfy
some property, ok say. Consider the task of proving that
ﬁlter (all ok)·parts = foldr (concatMap·okextendl) [[ ]]
where the deﬁnition of okextendl – the ok left-extensions – is
okextendl x = ﬁlter (ok ·head)·extendl x
The context-sensitive fusion condition is that
ﬁlter (all ok) (concatMap (extendl x) ps) =
concatMap (okextendl x) (ﬁlter (all ok) ps)
for all partitions ps of the same list. To prove it, one needs the assumption that ok
is sufﬁx-closed, meaning that, if ok (xs++ys) holds, then so does ok ys. Details are
left as an exercise. Dually, if we start out with the deﬁnition of parts in terms of
foldl, then the required assumption is that ok is preﬁx-closed, meaning that ok xs
holds if ok (xs++ys) does. Many predicates, including those used in the following
sections, are both preﬁx-closed and sufﬁx-closed, so there is a free choice of which
deﬁnition of parts to adopt. But sometimes only one of these properties holds, and
that dictates the choice of deﬁnition for parts.
12.2 Managing two bank accounts
Our ﬁrst problem is a simple example of a scheduling problem. It can be introduced
in the following way. A certain individual, whom we will call Zakia, has two
online bank accounts, a current account and a savings account. Zakia uses the
current account only for a ﬁxed and known sequence of transactions (deposits and
withdrawals), such as salary, standing orders, and utility bills. For security reasons,
Zakia never wants more than a certain amount C in her current account, where C
is some ﬁxed amount assumed to be at least as large as any single transaction. To
maintain this security condition, Zakia wants to set up an automatic sequence of
transfers between her current and deposit accounts so that at the beginning of each
group of transactions money can be transferred into or out of the current account
to cope with the next group of transactions. To minimise trafﬁc, Zakia wants the
number of such transfers to be as small as possible.
Abstractly stated, the problem is to ﬁnd a shortest partition of a list of positive
and negative integers into a list of safe segments. A segment [x1,x2,...,xk] is safe if
there is an amount r, the residue in the current account at the beginning of such a
sequence, such that all of the sums
r, r +x1, r +x1 +x2, ..., r +x1 +x2 +···+xk
lie between 0 and the given bound C. For example, if C = 100, the sequence
[−20,40,60,−30] is safe because we can take r = 20. But [40,−50,10,80,20]

292
Partitions
is not safe because r has to be at least 10 to cope with the ﬁrst withdrawal and
10+40−50+10+80+20 = 110, which is greater than C. It is left as an exercise
to show that, if a segment is safe, then so is every preﬁx and sufﬁx of the segment.
To simplify the safety condition, let m and n be the maximum and minimum of
the sums 0,x1,x1 +x2,...,x1 +x2 +···+xk, so n ⩽0 ⩽m. Then it is required that
there exists an r such that 0 ⩽r +n ⩽C and 0 ⩽r +m ⩽C. These two conditions
are equivalent to m ⩽C +n (see the exercises). Hence, supposing C is provided as
a global value c, we can deﬁne
safe::Segment Int →Bool
safe xs = maximum sums ⩽c+minimum sums
where sums = scanl (+) 0 xs
The function msp (a minimum safe partition) can now be speciﬁed by
msp::[Int] →Partition Int
msp ←MinWith length·ﬁlter (all safe)·parts
The function msp returns a partition, not the sequence of transfers that have to be
made between the two accounts. We will leave it as an exercise to show how the
transfers can be computed from the ﬁnal partition.
The ﬁrst step in the standard recipe is to fuse the ﬁlter operation with the gener-
ation of partitions. Since safe is both preﬁx-closed and sufﬁx-closed, we can use
either deﬁnition of parts. Choosing the deﬁnition of parts in terms of foldr, we
obtain
msp ←MinWith length·safeParts
where safeParts is deﬁned by
safeParts
= foldr (concatMap·safeExtendl) [[ ]]
safeExtendl x = ﬁlter (safe·head)·extendl x
At each step only safe partitions are computed. It is assumed that every singleton
transaction is safe, so a new transaction can always start a new segment. But it can
only be glued to a segment if the result is safe, which means that the segment itself
is also safe.
The next step in the recipe is to introduce thinning. Before doing so, we should
ﬁrst check whether or not a greedy algorithm is possible. Consider, for example, the
transactions [4,4,3,−3,5]. Taking C = 10, there are two safe partitions of shortest
length, namely [[4],[4,3,−3,5]] and [[4,4],[3,−3,5]]. While the former can be
extended to a safe partition [[5,4],[4,3,−3,5]] of length 2 by gluing 5, the second
one cannot, because [5,4,4] is not a safe segment. It follows that we cannot get
away with maintaining an arbitrary shortest safe partition. But that leaves open the
possibility of a greedy algorithm with a modiﬁed cost function
cost p = (length p,length (head p))

12.2 Managing two bank accounts
293
In words, we may be able to maintain a shortest partition whose ﬁrst segment is
also as short as possible. Such a deﬁnition would be perfectly acceptable because
minimising cost also minimises length. Recalling the standard calculation for a
greedy algorithm, we can reason
MinWith cost ·concatMap (safeExtendl x)
=
{ distributing MinWith cost }
MinWith cost ·map (MinWith cost ·safeExtendl x)
→
{ with add x ←MinWith cost ·safeExtendl x }
MinWith cost ·map (add x)
→
{ greedy condition (see below) }
add x·MinWith cost
The deﬁnition of add can be simpliﬁed to read
add x [ ]
= [[x]]
add x (s:p) = if safe (x:s) then (x:s):p else [x]:s:p
In words, a partition with a cheaper cost is obtained by gluing rather than starting a
new segment. The context-sensitive greedy condition holds if
cost p1 ⩽cost p2 ⇒cost (add x p1) ⩽cost (add x p2)
for any two partitions p1 and p2 of the same list, all of whose segments are safe.
To see whether or not the greedy condition holds, consider the four possible
values of q1 = add x p1 and q2 = add x p2, namely
q1 = cons x p1
q2 = cons x p2
(12.1)
q1 = cons x p1
q2 = glue x p2
(12.2)
q1 = glue x p1
q2 = cons x p2
(12.3)
q1 = glue x p1
q2 = glue x p2
(12.4)
Firstly, suppose |p1|<|p2|, where, for brevity, |p| abbreviates length p. Then |q1|<
|q2| except for case (12.2). But in this case we have |q1| ⩽|q2| and |head q1| <
|head q2|, and therefore cost q1 ⩽cost q2 for all values of q1 and q2.
Secondly, suppose |p1| = |p2| and |s1| ⩽|s2|, where s1 = head p1 and s2 = head p2.
By the assumption that p1 and p2 are partitions into safe segments of the same list,
it follows that s1 is a preﬁx of s2. Here case (12.2) cannot arise. In the remaining
three cases it is easy to check that cost q1 ⩽cost q2. So the greedy condition does
indeed hold.
That means the following greedy algorithm solves the bank accounts problem:
msp::[Int] →Partition Int
msp = foldr add [ ]
where add x [ ]
= [[x]]
add x (s:p) = if safe (x:s) then (x:s):p else [x]:s:p

294
Partitions
Ignoring the cost of computing safe, this is a linear-time algorithm. Computation of
safe can be made to take constant time by tupling and is left as an exercise.
The lesson to be learned from the bank accounts problem is that it is as well to
check whether a greedy algorithm is possible for a problem before embarking on an
attack by thinning. But, out of interest, suppose we had gone ahead with a thinning
strategy anyway. Then we would have
msp ←MinWith length·ThinBy (≼)·safeParts
where ≼has to be chosen so that
p1 ≼p2 ⇒length p1 ⩽length p2
A sensible choice of ≼is the partial preorder
p1 ≼p2 = length p1 ⩽length p2 ∧length (head p1) ⩽length (head p2)
With this choice one can establish the fusion condition
ThinBy (≼)·step x →ThinBy (≼)·step x·ThinBy (≼)
where step x = concatMap (safeExtendl x). Hence
msp = minWith length·foldr tstep [[ ]]
where tstep x = thinBy (≼)·concatMap (safeExtendl x)
This algorithm thins at each step. Moreover, one can prove by induction that, with
the deﬁnition of thinBy given in Chapter 8, at most two partitions are kept at each
stage. Therefore a thinning algorithm based on ≼will be almost as efﬁcient as the
greedy one.
There is another point of interest. Both the greedy algorithm and the thinning
algorithm may return a schedule in which transfers occur before they seem necessary.
For example, with C = 100 we obtain
msp [50,20,30,−10,40,−90,−20,60,70,−40,80]
= [[50],[20,30,−10,40,−90],[−20,60],[70],[−40,80]]
whereas the alternative solution
[[50,20,30,−10],[40,−90],[−20,60],[70,−40],[80]]
also has length ﬁve and might seem less suspicious to any tracking software em-
ployed by Zakia’s bank that might reasonably expect transfers to occur only when
necessary. Exercise 12.12 asks for a solution to this problem.
12.3 The paragraph problem
The paragraph problem is the problem of splitting a text into lines in the best
possible way. To begin with, we introduce the following type synonyms:

12.3 The paragraph problem
295
type Text = [Word]
type Word = [Char]
type Para = [Line]
type Line = [Word]
It is assumed that a text consists of a nonempty sequence of words, each word being
a nonempty sequence of non-space characters. A paragraph therefore consists of at
least one line.
The major constraint on paragraphs is that all lines have to ﬁt into a speciﬁed
width. For simplicity, we assume a single globally deﬁned value maxWidth that
gives the maximum width a line can possess. A reasonable generalisation, which
we will not pursue, is to allow different lines to have different maximum widths.
For example, paragraphs in newspapers often are arranged with varying widths to
ﬁt alongside pictures with varying contours. Instead we specify
para::Text →Para
para ←MinWith cost ·ﬁlter (all ﬁts)·parts
The function ﬁts determines whether a line will ﬁt into the required width:
ﬁts::Line →Bool
ﬁts line = width line ⩽maxWidth
width::Line →Nat
width = foldrn add length where add w n = length w+1+n
The function foldrn, a general fold over nonempty lists, was deﬁned in Chapter 8.
The width of a line consisting of a single word is the length of the word, while the
width of a line consisting of at least two words is the sum of the lengths of the words
plus the number of inter-word spaces. This deﬁnition is appropriate when every
character, including the space character, has the same width, but it can be adapted
to fonts in which characters have different widths. It is assumed that no single word
exceeds the maximum line width, so para is well-deﬁned for every input.
It remains to deﬁne cost and to choose a deﬁnition of parts either in terms of foldr
or in terms of foldl. The predicate ﬁts is both preﬁx-closed and sufﬁx-closed, so it
seems like a free choice. However, if we use foldr, then we can arrive at solutions
that, like Zakia’s bank accounts problem, allow short ﬁrst lines in order to ensure
longer subsequent lines. The appearance of such a paragraph might appear strange,
so we will use foldl instead.
That means we can fuse the ﬁltering with the generation of partitions to arrive at
para ←MinWith cost ·ﬁtParts
where
ﬁtParts = foldl (ﬂip (concatMap·ﬁtExtend)) [[ ]]
where ﬁtExtend x = ﬁlter (ﬁts·last)·extendr x

296
Partitions
Only those partitions whose lines ﬁt into the maximum width are generated at each
step.
Finally, how should we deﬁne the cost of a paragraph? There are at least ﬁve
reasonable answers. Firstly, we could deﬁne
cost1 = length
Here a best possible paragraph is one with the fewest lines. We could also deﬁne
cost2 = sum·map waste·init
where waste line = maxWidth−width line
Here the cost of a paragraph is the sum of the waste of each line, taken over all lines
except the very last (where wasted space does not detract from the appearance). A
third deﬁnition sums the squares of the wasted space:
cost3 = sum·map waste·init
where waste line = (optWidth−width line)2
The deﬁnition depends on another globally deﬁned constant optWidth, whose value
is at most maxWidth and which speciﬁes the optimum width of each line of a
paragraph. With this version, which is similar to the one used in TEX, lines that
deviate only a little from the optimum width are penalised less heavily than with
cost2. Finally, two more deﬁnitions of cost are
cost4 = foldr max 0·map waste·init
where waste line = maxWidth−width line
cost5 = foldr max 0·map waste·init
where waste line = (optWidth−width line)2
Here it is the maximum waste that is minimised. Use of foldr max 0 rather than
maximum is needed to ensure that the cost of a paragraph consisting of a single
line is zero. The last four deﬁnitions of cost assume that a paragraph is a nonempty
sequence of lines (init is undeﬁned on an empty list), but we can also set the cost of
an empty paragraph to zero.
There is an obvious greedy algorithm for the paragraph problem:
greedy = foldl add [ ]
where add [ ] w = snoc w [ ]
add p w = head (ﬁlter (ﬁts·last) [bind w p,snoc w p])
The algorithm works by adding each word to the end of the last line of the current
paragraph until no more words will ﬁt, in which case a new line is started. A more
efﬁcient version is discussed in the exercises. This algorithm is essentially the one
used by Microsoft Word and many other word processors.
So, for which deﬁnition of cost does the greedy algorithm work? The answer is

12.3 The paragraph problem
297
that cost has to satisfy two properties. Firstly, provided the result ﬁts, adding a new
word to the end of a line is never worse than starting a new line:
ﬁts (last (bind w p)) ⇒cost (bind w p) ⩽cost (snoc w p)
Secondly, as should be familiar by now, the greedy condition
cost p1 ⩽cost p2 ⇒cost (add p1 w) ⩽cost (add p2 w)
should hold. The greedy condition does not hold when the cost of a paragraph is
simply the number of lines (see the exercises), but it does if we strengthen this
measure by redeﬁning cost1 to read
cost1 p = (length p,width (last p))
That is to say, a best paragraph is one that minimises the number of lines and,
among such paragraphs, one that has a shortest last line. The proof is similar to the
one in the bank accounts problem. As in the previous proof, let q1 = add p1 w and
q2 = add p2 w. There are four possible cases:
q1 = bind w p1
q2 = bind w p2
(12.5)
q1 = bind w p1
q2 = snoc w p2
(12.6)
q1 = snoc w p1
q2 = bind w p2
(12.7)
q1 = snoc w p1
q2 = snoc w p2
(12.8)
Suppose cost1 p1 ⩽cost1 p2. Firstly, if |p1| < |p2|, where again |p| abbreviates
length p, then |q1|<|q2| except in case (12.7). But in case (12.7) we have
|q1| ⩽|q2| ∧width (last q1)<width (last q2)
which implies cost1 q1 <cost1 q2. Secondly, suppose
|p1| = |p2| ∧width (last p1) ⩽width (last p2)
Here, case (12.7) cannot arise. In cases (12.5) and (12.8) we have
|q1| = |q2| ∧width (last q1) = width (last q2)
while in case (12.6) we have |q1| < |q2|. So cost1 q1 ⩽cost1 q2 in all cases. The
greedy algorithm therefore minimises the number of lines in a paragraph.
The greedy algorithm also works for cost2, the cost function that sums the waste
of each line except the last. We claim that
cost1 p1 ⩽cost1 p2 ⇒cost2 p1 ⩽cost2 p2
For the proof, suppose p1 consists of the lines [l1,1,l1,2,...,l1,k], with w1, j as the
width of l1, j. Then, abbreviating maxWidth to M, we have
cost2 p1 = (M −w1,1)+(M −w1,2)+···+(M −w1,k−1)
= (k −1) M −(T −(w1,k +k −1))
where T is the total width of the text. Thus (T −(w1,k +k −1)) is the sum of the

298
Partitions
widths of all lines except the last because k −1 inter-word spaces are replaced by
newlines. Similarly, if p2 consists of the lines [l2,1,l2,2,...,l2,m], then
cost2 p2 = (m−1) M −(T −(w2,m +m−1))
Suppose cost1 p1 ⩽cost1 p2, so (k,w1,k) ⩽(m,w2,m). If k <m, then
cost2 p2 ⩾cost2 p1 +M +w2,m −w1,k >cost2 p1
because w1,k <M. If, on the other hand, k = m and w1,k ⩽w2,m, then
cost2 p2 = cost2 p1 +w2,m −w1,k ⩾cost2 p1
In either case we have cost2 p1 ⩽cost2 p2.
However, the greedy algorithm does not work for the other deﬁnitions of cost
described above. Take maxWidth = 10 and optWidth = 8 and consider the two
partitions
p1 = [[w6,w1],[w5,w3],[w4],[w7]]
p2 = [[w6],[w1,w5],[w3,w4],[w7]]
in which length wi = i for each word wi. The partition p1 is the one returned by the
greedy algorithm. We have
cost3 p1 = sum [(8−8)2,(8−9)2,(8−4)2] = 17
cost3 p2 = sum [(8−6)2,(8−7)2,(8−8)2] = 5
cost4 p1 = maximum [10−8,10−9,10−4] = 6
cost4 p2 = maximum [10−6,10−7,10−8] = 4
cost5 p1 = maximum [8−8,8−9,8−4]
= 4
cost5 p2 = maximum [8−6,8−7,8−8]
= 2
With all these measures of cost, p2 is a better partition than p1, so the greedy algo-
rithm does not lead to the best solution. That means we need a thinning algorithm
for these particular cost functions.
More generally, we will describe a thinning algorithm for any admissible cost
function, meaning that if
cost p1 ⩽cost p2 ∧width (last p1) = width (last p2)
then
cost (bind w p1) ⩽cost (bind w p2) ∧cost (snoc w p1) ⩽cost (snoc w p2)
As can easily be checked, all the cost functions introduced above are admissible
cost functions.
Suppose p1 and p2 satisfy these two conditions. Then for any completion
q2 = init p2 ++[last p2 ++[l0]]++[l1]++···++[lk]
of p2 to a full paragraph, there is a similar completion
q1 = init p1 ++[last p1 ++[l0]]++[l1]++···++[lk]

12.4 Chapter notes
299
of p1. Moreover, cost q1 ⩽cost q2. Hence the partial paragraph p2 can never lead to
a better solution than p1 and can be eliminated from the computation. Note carefully
that this conclusion depends on the last lines of p1 and p2 having equal widths: if
the last line of p1 had width smaller than that of p2, then every valid completion of
p2 remains a valid completion of p1, but the cost of the latter may not be smaller
than the cost of the former.
Taken together, all of this means thinning with ≼is appropriate, where
p1 ≼p2 = cost p1 ⩽cost p2 ∧width (last p1) == width (last p2)
However, instead of using thinWith (≼) we can customise the thinning step by
keeping the list of partitions ps in increasing order of width of last line. Then the
partitions in map (bind w) ps are also in this order. Moreover, the partitions in
map (snoc w) ps all have the same last line, the shortest one possible. Thinning this
list means retaining only the single partition
minWith cost (map (snoc w) ps)
when beginning a new line. Therefore thinning can be implemented by the following
deﬁnition:
para = minWith cost ·foldl tstep [[ ]]
where tstep [[ ]] w = [[[w]]]
tstep ps w = minWith cost (map (snoc w) ps):
ﬁlter (ﬁts·last) (map (bind w) ps)
It is easy to see that at most M = maxWidth partitions are kept in play at each step,
since no last line can have width more than M. We will leave it to the exercises
to show how to memoise cost and width, and how to implement snoc and bind
efﬁciently, so that tstep takes O(M) steps. Hence the paragraph problem for n words
takes O(M n) steps. It is possible with a more sophisticated algorithm to eliminate
the dependence of this bound on M for certain deﬁnitions of cost, but we will not
go into the details.
12.4 Chapter notes
The problem of managing two bank accounts is an updated reworking of the security
van problem invented by Hans Zantema and discussed in Section 7.5 of [2]. There
are many articles on the paragraph problem, including two, [1] and [3], written by
ourselves. In [3] it is shown how to remove the dependence on the maximum line
width in the running time for some deﬁnitions of cost. For a thorough discussion of
the line-breaking algorithm used in TEX see [4].

300
Partitions
References
[1]
Richard S. Bird. Transformational programming and the paragraph problem. Science
of Computer Programming, 6(2):159–189, 1986.
[2]
Richard S. Bird and Oege de Moor. The Algebra of Programming. Prentice-Hall,
Hemel Hempstead, 1997.
[3]
Oege de Moor and Jeremy Gibbons. Bridging the algorithm gap: A linear-time
functional program for paragraph formatting. Science of Computer Programming,
35(1):3–27, 1999.
[4]
Donald E. Knuth and Michael F. Plass. Breaking paragraphs into lines. Software:
Practice and Experience, 11(11):1119–1184, 1981.
Exercises
Exercise 12.1 How many partitions of a list of length n>0 are there?
Exercise 12.2 Why is the clause parts [ ] = [[ ]] necessary in the ﬁrst deﬁnition of
parts?
Exercise 12.3 Give another deﬁnition of parts in terms of foldr, one that at each
step does all the cons operations before the glue operations.
Exercise 12.4 Give the details of the proof that
ﬁlter (all ok)·parts = foldr (concatMap·okextendl) [[ ]]
provided ok is sufﬁx-closed. (Hint: it is probably best to express the fusion condition
in terms of list comprehensions.)
Exercise 12.5 Which of the following predicates on nonempty sequences of posi-
tive numbers are preﬁx-closed and which are sufﬁx-closed?
leftmin xs
= all (head xs ⩽) xs
rightmax xs = all (⩽last xs) xs
ordered xs = and (zipWith (⩽) xs (tail xs))
nomatch xs = and (zipWith (̸=) xs [0..])
Do each of these predicates hold for singleton lists?
Exercise 12.6 Suppose that n ⩽0 ⩽m. Show that
(∃r : 0 ⩽r +n ⩽C ∧0 ⩽r +m ⩽C) ⇔m ⩽C +n
Exercise 12.7 Show that the predicate safe in the bank accounts problem is both
preﬁx-closed and sufﬁx-closed.

Exercises
301
Exercise 12.8 Suppose C = 10. What is the value of msp [2,4,50,3] when msp is
the greedy algorithm for the bank accounts problem and when msp is deﬁned by the
original speciﬁcation?
Exercise 12.9 The function add in the bank accounts problem does not take con-
stant time because the safety test can take linear time. But we can represent a
partition p by a triple
(p,minimum (sums (head p)),maximum (sums (head p)))
where sums = scanl (+) 0. Write down a new deﬁnition of msp that does take linear
time.
Exercise 12.10 The function msp returns a partition, not the transfers that have to
be made to keep the current account in balance. Show how to deﬁne
transfers::Partition Int →[Int]
by computing a pair (n,r) of nonnegative numbers for each segment, where n is the
minimum that has to be in the current account to ensure the segment is safe and r is
the residue after the transactions in the segment.
Exercise 12.11 Consider the thinning algorithm for the bank accounts problem.
Suppose that at some point in the computation there are two partitions of the form
[y]:ys:p and (y:ys):p. This could happen as early as the second step, producing the
partitions [[y],[z]] and [[y,z]]. Show that adding in a new element x and thinning
the result will produce either a single partition, or two partitions of the above form.
Exercise 12.12 How can Zakia address the suspicious feature of the given solution
to the bank accounts problem, namely that transfers can occur before they are
absolutely necessary?
Exercise 12.13 The function runs used in Mergesort is speciﬁed by
runs::Ord a ⇒[a] →Partition a
runs ←MinWith length·ﬁlter (all ordered)·parts
Without looking back to the section on Mergesort, write down a greedy algorithm
for computing runs. Why does the greedy algorithm work?
Exercise 12.14 Show that the greedy condition fails when the cost of a paragraph
is simply the number of lines.
Exercise 12.15 The greedy algorithm for the paragraph problem can be made more
efﬁcient in two steps. This exercise deals with the ﬁrst step and the following
exercise with the second step. Consider the function help speciﬁed by

302
Partitions
p++help l ws = foldl add (p++[l]) ws
Prove that
greedy (w:ws) = help [w] ws
where help l [ ]
= [l]
help l (w:ws) = if width l′ ⩽maxWidth
then help l′ ws else l:help [w] ws
where l′ = l++[w]
Exercise 12.16 For the second step, memoise width and eliminate the concatenation
with the help of an accumulating function parameter.
Exercise 12.17 In the thinning version of the paragraph problem, can we replace
ﬁlter by takeWhile?
Exercise 12.18 Show that the cost functions described in the text for the paragraph
problem are all admissible.
Exercise 12.19 With some admissible cost functions, the thinning algorithm may
select a paragraph with minimum cost but whose length is not as short as possible.
How can this deﬁciency be overcome?
Exercise 12.20 The reﬁnement
snoc w·MinWith cost ←MinWith cost ·map (snoc w)
follows from the condition
cost p1 ⩽cost p2 ⇒cost (snoc w p1) ⩽cost (snoc w p2)
Does this condition hold for cost3?
Exercise 12.21 Suppose we had gone for a right-to-left thinning algorithm for the
paragraph problem, using a deﬁnition of parts based on foldr. This time a cost
function is admissible if
cost (glue w p1) ⩽cost (glue w p2) ∧cost (cons w p1) ⩽cost (cons w p2)
provided that
cost p1 ⩽cost p2 ∧width (head p1) = width (head p2)
As can be checked, all ﬁve cost functions introduced in the text are admissible in
this sense. Write down the associated thinning algorithm. Give an example to show
that the two different thinning algorithms produce different results for cost3.

Answers
303
Exercise 12.22 The ﬁnal exercise is to make the thinning algorithm for the para-
graph problem more efﬁcient. Setting rmr = reverse·map reverse, we can represent
a paragraph p by a triple
(rmr p,cost p,width (last p))
The last two components memoise cost and width, while the ﬁrst component means
that snoc and bind can be implemented in terms of cons and glue. More precisely,
we have
snoc w·rmr = rmr ·cons w
bind w·rmr = rmr ·glue w
Write down the resulting algorithm, assuming the cost function is cost3.
Answers
Answer 12.1 There are 2n−1 partitions.
Answer 12.2 Because with the single clause
parts xs = [ys:yss | (ys,zs) ←splits xs,yss ←parts zs]
we would have parts [ ] = [ ], from which it follows that parts xs = [ ] for all xs.
Answer 12.3 The deﬁnition is
parts = foldr step [[ ]]
where step x [[ ]] = [[[x]]]
step x ps = map (cons x) ps++map (glue x) ps
Answer 12.4 In terms of list comprehensions, the fusion condition takes the form
[p′ | p ←ps,p′ ←extendl x p,all ok p′]
= [p′ | p ←ps,all ok p,p′ ←extendl x p,ok (head p′)]
for all partitions ps of the same list. With the given deﬁnition of extendl, the fusion
condition follows if we can show that
[cons x p | p ←ps,all ok (cons x p)]
= [cons x p | p ←ps,all ok p ∧ok (head (cons x p))]
[glue x (s:p) | s:p ←ps,all ok (glue x (s:p))]
= [glue x (s:p) | s:p ←ps,all ok (s:p) ∧ok (head (glue x (s:p)))]
Since cons x p = [x]:p and
all ok ([x]:p) = all ok p ∧ok [x]
the ﬁrst condition holds. Since glue x (s:p) = (x:s):p and, provided ok is sufﬁx-
closed, we have

304
Partitions
all ok ((x:s):p) = all ok p ∧ok (x:s)
= all ok p ∧ok s ∧ok (x:s)
= all ok (s:p) ∧ok (x:s)
the second condition holds.
Answer 12.5 The predicates leftmin and nomatch are preﬁx-closed but not sufﬁx-
closed, while rightmax is sufﬁx-closed but not preﬁx-closed. Finally, ordered is
both preﬁx-closed and sufﬁx-closed. All predicates hold for singletons (in the case
of nomatch no positive integer is 0).
Answer 12.6 We can reason
(∃r : 0 ⩽r +n ⩽C ∧0 ⩽r +m ⩽C)
⇔
{ arithmetic }
(∃r : −n ⩽r ⩽C −n ∧−m ⩽r ⩽C −m)
⇔
{ arithmetic }
(∃r : max (−n) (−m) ⩽r ⩽min (C −n) (C −m))
⇔
{ assuming n ⩽0 ⩽m }
(∃r : −n ⩽r ⩽C −m)
⇔
{ logic }
m ⩽C +n
Answer 12.7 If all the sums r,r+x1,r+x1+x2,...,r+x1+x2+···+xk lie between
0 and C, then certainly every preﬁx of these sums does too. Taking r′ = r +x1, we
have that all the sums r′,r′ +x2,...,r′ +x2 +···+xk also lie between 0 and C, so
safe is sufﬁx-closed as well as preﬁx-closed.
Answer 12.8 For the greedy algorithm we have
msp [2,4,50,3] = [[2,4],[50],[3]]
but for the original speciﬁcation the answer is the undeﬁned value. Since the segment
[50] is not safe, there is no partition into safe segments.
Answer 12.9 We have
msp = part ·foldr add ([ ],0,0)
where
part (p,n,m) = p
add x pnm | null (part pnm)
= cons x pnm
| safe (glue x pnm) = glue x pnm
| otherwise
= cons x pnm
cons x (p,n,m)
= ([x]:p,min 0 x,max 0 x)
glue x (s:p,n,m) = ((x:s):p,min 0 (x+n),max 0 (x+m))
safe (p,n,m)
= max 0 (−n) ⩽min c (c−m)

Answers
305
Answer 12.10 The values (n,r) for the segments in a partition can be computed by
the function endpoints, where
endpoints::[Int] →(Int,Int)
endpoints xs = if n<0 then (−n,x−n) else (0,x)
where n
= minimum sums
x
= last sums
sums = scanl (+) 0 xs
For example,
map endpoints [[40,−85,55],[−32,79],[80],[−21,80]]
= [(45,55),(32,79),(0,80),(21,80)]
The current account has to have a balance of 45 to ensure the ﬁrst segment is safe.
At the end of the segment we can transfer 55−32 to the savings account to ensure a
credit of 32 for the next segment; and so on. Hence we can deﬁne
transfers = collect ·map endpoints
collect ::[(Int,Int)] →[Int]
collect xys = zipWith (−) (map fst xys++[0]) ([0]++map snd xys)
For example,
collect [(45,55),(32,79),(0,80),(21,80)] = [45,−23,−79,−59,−80]
Assuming a zero balance at the beginning, 45 has to be transferred to the current
account to ensure the ﬁrst segment is safe; the remaining amounts are what can be
transferred to the deposit account at the end of each segment, leaving a zero balance
in the current account at the end of all the transactions.
Answer 12.11 After adding x, there are three possible lists of partitions that can
result:
[[x]:[y]:ys:p,[x,y]:ys:p,[x]:(y:ys):p,(x:y:ys):p]
[[x]:[y]:ys:p,[x,y]:ys:p,[x]:(y:ys):p]
[[x]:[y]:ys:p,[x]:(y:ys):p]
Furthermore,
[x]:(y:ys):p ≼[x]:[y]:ys:p
[x]:(y:ys):p ≼[x,y]:ys:p
Hence, after thinning by thinBy the following partitions are left in each case:
[[x]:(y:ys):p,(x:y:ys):p]
[[x]:(y:ys):p]
[[x]:(y:ys):p]

306
Partitions
The ﬁrst pair of partitions also has the same form as in the question, so at most two
partitions are generated at each step.
Answer 12.12 The obvious answer is for Zakia to use a greedy algorithm that
processes from left to right:
msp = foldl add [ ]
where add [ ] x = [[x]]
add p x = head (ﬁlter (safe·last) [bind x p,snoc x p])
The answer to Exercise 12.15 shows how to make this version efﬁcient. The validity
of the left-to-right algorithm depends on the fact that safe is preﬁx-closed.
Answer 12.13 The deﬁnition is
runs::Ord a ⇒[a] →Partition a
runs = foldr add [ ]
where add x [ ]
= [[x]]
add x (s:p) = if ordered (x:s) then (x:s):p else [x]:s:p
The greedy algorithm works because exactly the same reasoning as in the bank
accounts problem applies, with safe replaced by ordered. Furthermore, the test in
the deﬁnition of add can be simpliﬁed to x ⩽head s.
Answer 12.14 Take maxWidth = 10 and consider the two paragraphs
p1 = [[6,1],[5,3],[4]]
p2 = [[6],[1,5],[3,4]]
both of which have the same length. We have
add 4 p1 = [[6,1],[5,3],[4,4]]
add 4 p2 = [[6],[1,5],[3,4],[4]]
so the greedy condition fails.
Answer 12.15 We have
foldl add (p++[l]) [ ] = p++[l]
so help l [ ] = [l]. Next, if add p w = bind w p, then we have
foldl add (p++[l]) (w:ws) = foldl add (p++[l++[w]]) ws
which shows that help l (w:ws) = help (l++[w]) ws. Finally, if add p w = snoc w p,
then
foldl add (p++[l]) (w:ws) = foldl add (p++[l]++[[w]]) ws
which shows that help l (w:ws) = l:help [w] ws.

Answers
307
Answer 12.16 The result is
greedy (w:ws) = help ((w:),length w) ws
where
help (f,d1) [ ]
= [f [ ]]
help (f,d1) (w:ws)
| d2 ⩽maxWidth = help (f ·(w:),d2) ws
| otherwise
= f [ ]:help ((w:),d) ws
where d2 = d1 +1+d;d = length w
Answer 12.17 Yes, the paragraphs are in increasing width of last line, so testing
can be abandoned as soon as a last line does not ﬁt.
Answer 12.18 The ﬁrst inequality holds because cost (bind w p) = cost p. For the
second inequality we have
cost (snoc w p) = cost p⊕waste (last p)
where ⊕is either + or max. The result follows because ⊕is monotonic and the
waste of a line depends only on its width.
Answer 12.19 Deﬁne a new cost function cost′ p = (cost p,length p). The function
cost′ is admissible if cost is.
Answer 12.20 No. Take the two paragraphs
p1 = [[6,1],[5,3],[4]]
p2 = [[6],[1,5],[3,4]]
whose costs, assuming maxWidth = 10 and optWidth = 8, are 1 and 5 respectively.
We have
cost3 (snoc 4 p1) = cost3 [[6,1],[5,3],[4],[4]] = 17
cost3 (snoc 4 p2) = cost3 [[6],[1,5],[3,4],[4]] = 5
Answer 12.21 The thinning algorithm is
para = minWith cost ·foldr tstep [[ ]]
where tstep w [[ ]] = [[[w]]]
tstep w ps
= cons w (minWith cost ps):
ﬁlter (ﬁts·head) (map (glue w) ps)
Take maxWidth = optWidth = 16. Here is just one example that shows different
outputs:

308
Partitions
Here is just
Here is just one
one example that
example that
shows different
shows different
outputs:
outputs:
The paragraph on the left was produced by the right-to-left algorithm, while the one
on the right was produced by the left-to-right one. The widths of the ﬁrst layout are
[12,16,15,8] while those of the second are [16,12,15,8] so the costs are the same.
Answer 12.22 The algorithm is
para = thePara·minWith cost ·thinparts
where
thePara (p, , ) = reverse (map reverse p)
cost ( ,c, )
= c
ok ( , ,k)
= k ⩽maxWidth
thinparts (w:ws) = foldl step (start w) ws
start w
= [([[w]],0,length w)]
step ps w
= minWith cost (map (snoc w) ps):
takeWhile ok (map (bind w) ps)
snoc w (p,c,k)
= (cons w p,c+(optWidth−k)2,length w)
bind w (p,c,k)
= (glue w p,c,k +1+length w)

PART FIVE
DYNAMIC PROGRAMMING


311
The term Dynamic Programming was coined by Richard Bellman in 1950 to de-
scribe his research into multi-stage decision processes. The word programming was
chosen as a synonym for planning to mean the process of determining the sequence
of decisions that have to be made, while dynamic suggested the evolution of the
system over time. These days, dynamic programming as a technique of algorithm
design means something much more speciﬁc. It involves a two-stage process in
which a problem, usually but not necessarily an optimisation problem, is formulated
in recursive terms and then some efﬁcient way of computing the solution is found.
Unlike a divide-and-conquer problem, the subproblems generated by the recursive
solution can overlap, so naive execution of the recursive algorithm will involve
solving the same subproblem many times over, possibly an exponential number of
times.
One way to understand the problem of overlap is to look at the dependency
graph associated with a recursive function. This is a directed graph whose vertices
represent function calls and whose directed edges show the dependency of each call
on recursive calls. While the dependency graph of a divide-and-conquer algorithm
is a tree of some kind with no shared vertices, the graph of a dynamic programming
algorithm is an acyclic directed graph, possibly with many shared vertices. A vertex
is shared if there is more than one incoming edge to the vertex.
The ﬁrst job in solving an optimisation problem by dynamic programming is
simply to obtain a recursive solution. As with thinning algorithms, the key step
is to exploit a suitable monotonicity condition. This condition enables an optimal
solution to a problem to be expressed in terms of optimal sub-solutions. When the
shape of the recursion is inductive, a thinning algorithm is appropriate; when it is
not, the techniques of dynamic programming come into play.
Having obtained a recursive description of the solution, there are basically two
ways to ensure that sub-solutions are not computed more than once. One is called
memoisation. Here the recursive, top-down structure of the computation is preserved
but sub-solutions are remembered and stored in a table for subsequent retrieval.
Thus at each recursive call one ﬁrst checks to see whether the call has been made
before, in which case the solution is retrieved from the table; otherwise the solution
is computed recursively and the result is stored.
The second method, and the one we will focus on, is called tabulation. Here,
the computation switches to a bottom-up scheme in which, by careful planning (or
‘programming’), the simplest partial results are computed ﬁrst, and then solutions to
larger subproblems are computed in an appropriate order until the complete solution
is obtained. For some problems, installing a tabulation can be viewed as the problem
of ﬁnding a shortest path in a suitable layered network derived from the dependency
graph. We considered the layered network problem in Chapter 10, and we will look
at it again in the following chapter.

312
Each approach, the top-down and bottom-up methods, has its advantages and dis-
advantages. Memoisation is in principle easy to install but does require a systematic
way of coding the arguments of the recursive function so that they can be used as
indices in a table, usually an array of some kind. These arguments also have to be
testable for equality. A top-down approach ensures that only those values actually
needed for the full computation are computed. Tabulation requires a more wholesale
change to the structure of the solution but, if the tabulation scheme is chosen well,
each solution can be determined easily from the solutions to the associated subprob-
lems. On the other hand, some simple tabulation schemes may involve computing
the solutions to subproblems not actually required for the full solution.
The aim of the next two chapters is to look at a number of problems for which
dynamic programming is a viable technique, and to examine the various kinds
of tabulation scheme that can arise. In imperative programming most tabulation
schemes involve arrays of various kinds, but in functional programming other
representations can prove superior.

Chapter 13
Eﬃcient recursions
In this chapter we introduce the essential ideas of dynamic programming by looking
at the recursive formulation of some simple problems, examining the dependency
graph associated with each recursion, and ﬁnding a suitable tabulation scheme
for implementing the recursion efﬁciently. Most problems for which dynamic
programming is appropriate are optimisation problems of one kind or another,
but the ﬁrst two problems, the Fibonacci function and the problem of computing
binomial coefﬁcients, are not. We will also give dynamic programming solutions
for the knapsack problem of Chapter 10 and the longest common subsequence
problem of Chapter 11. Two additional problems, the minimum-edit problem and
the shuttle-bus problem, are also described. All these examples illustrate the range
of possibilities for different tabulation schemes.
13.1 Two numeric examples
Perhaps the simplest example of a recursion that involves the same calculation being
repeated many times over is the Fibonacci function:
ﬁb::Nat →Integer
ﬁb n = if n ⩽1 then fromIntegral n else ﬁb (n−1)+ﬁb (n−2)
We use Integer arithmetic for the result since values of ﬁb grow large very quickly.
Direct evaluation of ﬁb on an argument n>1 involves ﬁb k evaluations of ﬁb on the
argument n−k for 1 ⩽k <n , so direct evaluation takes an exponential number of
steps (see Exercise 13.1).
The dependency graph of the computation of ﬁb for n = 7 is pictured in Fig-
ure 13.1. This is a directed acyclic graph with a single root, labelled with 7, and
directed edges from a node to the two recursive calls associated with the node.
One way of making the computation more efﬁcient is to use a one-dimensional
array:

314
Eﬃcient recursions
0
1
2
3
4
5
6
7
Figure 13.1 The dependency graph of ﬁb 7
ﬁb::Nat →Integer
ﬁb n = a!n
where a = tabulate f (0,n)
f i = if i ⩽1 then fromIntegral i else a!(i−1)+a!(i−2)
The function tabulate is deﬁned by
tabulate::Ix i ⇒(i →e) →(i,i) →Array i e
tabulate f bounds = array bounds [(x,f x) | x ←range bounds]
The declaration a = tabulate f (0,n) in the deﬁnition of ﬁb builds an array a whose
ith entry for i>1 is the unevaluated expression a!(i−1)+a!(i−2). Thus tabulate
takes linear time. Array entries are evaluated only when required, and then they are
evaluated at most once. Therefore the above deﬁnition of ﬁb takes linear time. We
will use tabulate again when tabulating with arrays.
However, using an array for the tabulation of ﬁb is overkill because at each step of
the computation only the two previous values of ﬁb are required. The table therefore
need consist of only two entries. This observation leads to the following simple
deﬁnition:
ﬁb::Nat →Integer
ﬁb n = fst (apply n step (0,1))
where step (a,b) = (b,a+b)
The ‘table’ consists of a pair of values. It is easy to show by induction that
apply n step (0,1) = (ﬁb n,ﬁb (n+1))
so the above program is correct. This solution also takes linear time. In fact there is
even a logarithmic-time algorithm for computing ﬁb, see Exercise 13.3.
The second example concerns computing binomial coefﬁcients. The standard
deﬁnition is, of course,
n
r

=
n!
r!(n−r)!
and can be easily implemented by
binom::(Nat,Nat) →Integer
binom (n,r) = fact n div (fact r ×fact (n−r))
where fact n = product [1..fromIntegral n]

13.1 Two numeric examples
315
(6,3)
(5,2)
(4,1)
(3,0)
(5,3)
(4,2)
(3,1)
(2,0)
(4,3)
(3,2)
(2,1)
(1,0)
(3,3)
(2,2)
(1,1)
Figure 13.2 Computation of binom (6,3)
We can also deﬁne binomial coefﬁcients recursively. If 0<r <n, then
n
r

=
n−1
r

+
n−1
r −1

Furthermore,
n
0

=
n
n

= 1
That leads to the following recursive deﬁnition of binom:
binom::(Nat,Nat) →Integer
binom (n,r) = if r == 0 ∨r == n then 1
else binom (n−1,r)+binom (n−1,r −1)
Like the Fibonacci function, this deﬁnition of binom can take exponential time if
executed directly.
The dependency graph for binom on the argument (6,3) is pictured in Figure 13.2.
It takes the form of a two-dimensional grid, so a simple tabulation scheme can be
based on a two-dimensional array:
binom::(Nat,Nat) →Integer
binom (n,r) = a!(n,r)
where a = tabulate f ((0,0),(n,r))
f (i,j) = if j == 0 ∨i == j then 1 else a!(i−1,j)+a!(i−1,j−1)
The function tabulate was deﬁned above. However, half of the entries, namely
those for (i,j) where i<j, consist of the undeﬁned value ⊥, so the array program is
wasteful of space.
A better solution can be based on a single list. Observe that the values of binom
for the grid in Figure 13.2 are given by

316
Eﬃcient recursions
20 10
4
1
10
6
3
1
4
3
2
1
1
1
1
1
and that each row consists of the running sums, reading from right to left, of the
elements in the row below it. That means we can deﬁne
binom (n,r) = head (apply (n−r) (scanr1 (+)) (replicate (r +1) 1))
The function scanr1, a variant of scanr and deﬁned only for nonempty lists, is
another Standard Prelude function whose values are illustrated by
scanr1 (⊕) [x1,x2,x3] = [x1 ⊕(x2 ⊕x3),x2 ⊕x3,x3]
This method takes r(n−r) additions to compute
n
r

, but no multiplications.
13.2 Knapsack revisited
For our next example of dynamic programming, let us take a second look at the
knapsack problem from Section 10.4. Recall the following declarations:
type Name
= String
type Value
= Nat
type Weight
= Nat
type Item
= (Name,Value,Weight)
type Selection
= ([Name],Value,Weight)
name (n, , )
= n
value ( ,v, )
= v
weight ( , ,w) = w
In Section 10.4 we speciﬁed swag by
swag::Weight →[Item] →Selection
swag w ←MaxWith value·choices w
where choices was deﬁned by a foldr. This time we deﬁne the choices recursively:
choices::Weight →[Item] →[Selection]
choices w [ ]
= [([ ],0,0)]
choices w (i:is) = if w<wi then choices w is
else choices w is++map (add i) (choices (w−wi) is)
where wi = weight i
add ::Item →Selection →Selection
add i (ns,v,w) = (name i:ns,value i+v,weight i+w)
Each item is considered in turn and, weight permitting, either added to the selection
or not.

13.2 Knapsack revisited
317
(5,4)
(5,3)
(5,2)
(5,1)
(5,0)
(4,0)
(3,2)
(3,1)
(3,0)
(2,3)
(2,2)
(2,1)
(2,0)
(1,1)
(1,0)
(0,2)
(0,1)
(0,0)
Figure 13.3 Knapsack with capacity 5, and four items with weights 3, 2, 2, 1
It is easy to show that the monotonicity condition
value sn1 ⩽value sn2 ⇒value (add i sn1) ⩽value (add i sn2)
holds. That means
add i·MaxWith value ←MaxWith value·map (add i)
Using this fact and the distributive law of MaxWith, an easy calculation gives us the
following recursive version of swag:
swag::Weight →[Item] →Selection
swag w [ ]
= ([ ],0,0)
swag w (i:is) = if w<wi then swag w is
else better (swag w is) (add i (swag (w−wi) is))
where wi = weight i
better ::Selection →Selection →Selection
better sn1 sn2 = if value sn1 ⩾value sn2 then sn1 else sn2
In words, if there are no items to choose from, then the result is the empty selection
with zero weight and zero value. Otherwise the choice is the better of packing the
next item, assuming the weight of the knapsack allows it, and not packing it. In
either case, the remaining selection is the best possible for the remaining items and
the remaining capacity.

318
Eﬃcient recursions
Suppose the carrying capacity of the knapsack is 5 and there are four items
to choose from, with weights 3,2,2,1. The dependency graph for this instance is
pictured in Figure 13.3. A pair (w,r) represents the problem of computing swag
when the capacity of the knapsack is w and the last r items are left to choose from.
In this instance there are only two shared values, at (3,1) and (0,1), but in general
there will be many more. One straightforward tabulation scheme is to use a two-
dimensional array. A more space-efﬁcient alternative is to reuse a one-dimensional
array, building the solution column by column from right to left, each column being
represented by the entries in a single array. Yet a third way is to recast the problem
as one of computing a path of maximum value in a layered network. Each layer
is a column in the dependency graph and the edges go from layer to layer. We
considered the layered network problem in Chapter 10, except that there we were
looking for a path of minimum cost rather than maximum value. If the capacity of
the knapsack is w and there are n items, then ﬁnding a best path will take O(nw)
steps, so the dynamic programming algorithm has the same asymptotic complexity
as the thinning algorithm of Chapter 10. However, the computational overhead in
recasting the knapsack problem as a layered network problem is quite large.
Instead, we will build the solution column by column from right to left, but
using a list rather than an array. For example, here is part of Figure 13.3 redrawn
horizontally to show the dependence of each column, now a row, on the one below
it. The row also shows the dependencies for (4,2) and (1,2), values not required in
the recursive solution:
(5,2)
(4,2)
(3,2)
(2,2)
(1,2)
(0,2)
(5,1)
(4,1)
(3,1)
(2,1)
(1,1)
(0,1)
Each new entry in a row depends on the previous entry in the same row and, possibly,
an entry further to the right. All these additional entries are shifted by the same
amount, namely the weight of the current item being considered. We can therefore
deﬁne
swag::Weight →[Item] →Selection
swag w = head ·foldr step start
where start
= replicate (w+1) ([ ],0,0)
step i row = zipWith better row (map (add i) (drop wi row))
++drop (w+1−wi) row
where wi = weight i
This solution is of comparable speed to the thinning algorithm of Section 10.4, and

13.3 Minimum-cost edit sequences
319
slightly faster than one based on a one-dimensional array, but it does depend on all
weights being integers, an assumption not needed in the thinning algorithm.
13.3 Minimum-cost edit sequences
Our next example of dynamic programming concerns another way of comparing
the similarity of two strings. One such measure, as we saw in Section 11.2, is the
length of the longest common subsequence of the two strings. Another measure is
to count the cost of transforming one string into the other by a sequence of simple
edit operations. There are various possible edit operations, but we will allow just
the following four:
• The operation Replace x y replaces the current character x in the ﬁrst string xs by
y and then moves on to the next character of xs. It is supposed that x and y are
different characters.
• The operation Copy x has the same effect as Replace x x.
• The operation Delete x deletes the current character x in xs and moves on to the
next character.
• The operation Insert y inserts a new character y before the current character of xs,
and then moves on to the next character.
These edit operations are encapsulated in the data type
data Op = Copy Char | Replace Char Char | Delete Char | Insert Char
The character being replaced, copied, or deleted from the ﬁrst string is made explicit
in the edit operation. In this way, the edit sequence transforming the second string
into the ﬁrst can be obtained by interchanging insert and delete operations and
swapping the arguments of a replace. One can also recover both the source and the
target string from the edit sequence alone. More precisely, we can deﬁne
reconstruct ::[Op] →([Char],[Char])
reconstruct = foldr step ([ ],[ ])
where step (Copy x)
(us,vs) = (x:us,x:vs)
step (Replace x y) (us,vs) = (x:us,y:vs)
step (Insert x)
(us,vs) = (us,x:vs)
step (Delete x)
(us,vs) = (x:us,vs)
Each edit operation has an associated cost. We suppose that the cost of a replace is
less than the combined costs of an insert and a delete, for otherwise there would be
no point in having a replace operation. Furthermore, the cost of a copy operation is
assumed to be zero; then two identical strings can be transformed into one another
with zero cost. Here is an example where the cost of an insert or delete is 2 units
and the cost of a replace is 3 units:

320
Eﬃcient recursions
i * n s t i t u t i o n *
c o n s t i t u e * * n t
3 2 0 0 0 0 0 0 3 2 2 0 2
The string "institution" is transformed into "constituent" by replacing i
by c, inserting an o, copying the next six characters, replacing t by e, deleting
the next two characters, copying n and ﬁnally inserting t. The two strings have
been aligned by using a * character to indicate insertions or deletions. The total
cost of this sequence of edits is 14 units, which is the smallest possible cost when
the individual edits are costed as above. The function cost yields the sum of the
individual edit costs:
cost ::[Op] →Nat
cost = sum·map ecost
ecost (Copy x)
= 0
ecost (Replace x y) = 3
ecost (Delete x)
= 2
ecost (Insert y)
= 2
The problem of computing mce (a minimum-cost edit) is now speciﬁed by
mce::[Char] →[Char] →[Op]
mce xs ys ←MinWith cost (edits xs ys)
The function edits returns all possible edit sequences:
edits::[Char] →[Char] →[[Op]]
edits xs [ ]
= [map Delete xs]
edits [ ] ys
= [map Insert ys]
edits (x:xs) (y:ys) = [pick x y:es | es ←edits xs ys]++
[Delete x:es | es ←edits xs (y:ys)]++
[Insert y:es | es ←edits (x:xs) ys]
pick x y = if x == y then Copy x else Replace x y
The primary monotonicity condition for this problem is that
cost es1 ⩽cost es2 ⇒cost (op:es1) ⩽cost (op:es2)
for all edit operations op, where es1 and es2 are edit sequences in edits xs ys. That
leads to the recursive formulation
mce xs [ ]
= map Delete xs
mce [ ] ys
= map Insert ys
mce (x:xs) (y:ys) = minWith cost [pick x y:mce xs ys,
Delete x:mce xs (y:ys),
Insert y:mce (x:xs) ys]
However, we can go one step further. Provided it is available, a Copy operation at

13.3 Minimum-cost edit sequences
321
a
b
c
a
b
a
c
Figure 13.4 Computation of mce "abca" "bac"
any step is always the best possible choice. The proof of this greedy condition is
left to Exercise 13.10. That means we can rewrite the third clause of mce to read:
mce (x:xs) (y:ys) = if x == y then Copy x:mce xs ys else
minWith cost [Replace x y:mce xs ys,
Delete x:mce xs (y:ys),
Insert y:mce (x:xs) ys]
The dependency graph for mce "abca" "bac" is pictured in Figure 13.4. There is a
single diagonal edge when two characters match; otherwise there are three edges.
It remains to implement a suitable tabulation scheme. As with the knapsack
problem, we can compute entries row by row from right to left:
mce xs ys = head (foldr (nextrow xs) (ﬁrstrow xs) ys)
The ﬁrst row of edit operations is given by ﬁrstrow = tails·map Delete. To see how
to deﬁne nextrow, observe that the next edit sequence to be added to the new row,
say at position i, depends on one of three values: (i) the edit sequence at position
i+1 of the new row (for a delete operation); (ii) the edit sequence at position i of
the previous row (for an insert operation); and (iii) the edit sequence at position
i + 1 of the previous row (for a replace operation). These last two values can be
obtained with the help of a zip, so we can deﬁne
nextrow::[Char] →Char →[[Op]] →[[Op]]
nextrow xs y row = foldr step [Insert y:last row] xes
where xes = zip3 xs row (tail row)
step (x,es1,es2) row = if x == y then (Copy x:es2)
:row else
minWith cost [ Replace x y:es2,
Delete x:head row,
Insert y:es1]
:row

322
Eﬃcient recursions
a
b
c
a
b
a
c
b
Figure 13.5 Computation of lcs "abca" "bacb"
Finally, cost computations should be memoised for efﬁciency, but we will leave
that as an exercise. In the worst case the time required to ﬁnd the edit sequence
with minimum cost is then Θ(mn) steps, where m and n are the lengths of the two
strings.
13.4 Longest common subsequence revisited
The above tabulation scheme for the minimum-cost edit sequence problem can be
adapted to the longest common subsequence problem. Recall the recursive deﬁnition
of lcs from Chapter 11:
lcs::Eq a ⇒[a] →[a] →[a]
lcs [ ] ys
= [ ]
lcs xs [ ]
= [ ]
lcs (x:xs) (y:ys) = if x == y then x:lcs xs ys
else longer (lcs (x:xs) ys) (lcs xs (y:ys))
The dependency graph for lcs "abca" "bacb" is pictured in Figure 13.5. There is a
single diagonal edge when two characters match; otherwise there are two edges. As
with mce, we can compute lcs row by row from right to left. This time, the ﬁrst row
of entries is a list of empty lists and the entries of a new row each depend on one
of three further entries, the same three entries as we had with mce. Hence we can
deﬁne
lcs xs = head ·foldr (nextrow xs) (ﬁrstrow xs)

13.5 The shuttle-bus problem
323
where
ﬁrstrow xs = replicate (length xs+1) [ ]
nextrow xs y row = foldr (step y) [[ ]] (zip3 xs row (tail row))
step y (x,cs1,cs2) row = if x == y then (x:cs2):row
else longer cs1 (head row):row
The time required to ﬁnd the longest common subsequence of two lists of lengths m
and n is Θ(mn) steps in the worst case, the same time as with the thinning algorithm.
13.5 The shuttle-bus problem
Our ﬁnal example in this chapter is another scheduling problem. Consider a shuttle-
bus that runs from an airport to a city centre. It takes on passengers only at the
airport, but it can drop them off at various points along the route. We suppose that
the possible stops are numbered from 0 (the airport) to n (the city centre). In the
interests of getting all passengers to their destinations as quickly as possible, the bus
driver is willing to make up to k intermediate stops. The problem is to program the
computer on board the bus to calculate a schedule of at most k intermediate stops
that minimises the total cost for a given group of passengers, where the cost to a
single passenger getting off at stop m is the absolute value of the difference between
the desired stop number and m.
All that is important about passengers is the number of them who wish to get off
at a particular stop, so we deﬁne
type Passengers = [(Count,Stop)]
type Count = Nat
type Stop
= Nat
For example, [(3,1),(10,2),(5,3),(15,4),(4,5),(10,8),(22,10)] is a possible pas-
senger list, indicating that three people want to get off at stop 1, ten at stop 2, and so
on. It is assumed that the passenger list is given in increasing order of stop number
and that the stops are numbered between 1 and n inclusive.
A schedule of k intermediate stops is a subsequence of 1,2,...,n−1 of length at
most k. However, it turns out to be computationally simpler to describe the journey
in terms not of stops but of the individual ‘legs’ of the journey, where a leg is a pair
of stops:
type Leg = (Stop,Stop)
For example, the subsequence [2,5,7] of three intermediate stops is represented
(assuming n = 10) by the sequence of legs [(0,2),(2,5),(5,7),(7,10)]. With this

324
Eﬃcient recursions
representation, the total cost to a list of passengers for a sequence of legs is deﬁned
by
cost ::Passengers →[Leg] →Nat
cost ps [ ]
= 0
cost ps ((x,y):ls) = legcost qs (x,y)+cost rs ls
where (qs,rs) = span (atmost y) ps
where
atmost y (c,s)
= s ⩽y
legcost ps (x,y) = sum [c×min (s−x) (y−s) | (c,s) ←ps]
The leg cost of (x,y) is the cost to all passengers who wish to get off after stop x but
before or at stop y. Clearly, the closer stop is the one with smaller cost. For example,
with the above passenger list and sequence of legs, the cost is
legcost [(3,1),(10,2)]
(0,2)+
legcost [(5,3),(15,4),(4,5)] (2,5)+
legcost [ ]
(5,7)+
legcost [(10,8),(22,10)]
(7,10)
which is 3 × (2 −1) + (5 × (3 −2) + 15 × (5 −4)) + 0 + 10 × (8 −7) = 33. In
particular, the ﬁve passengers who want to get off at stop 3 do best by walking there
from stop 2.
Now we can specify schedule by
schedule::Nat →Nat →Passengers →[Leg]
schedule n k ps ←MinWith (cost ps) (legs n k 0)
where legs returns the set of possible sequences of legs from a given position:
legs::Nat →Nat →Stop →[[Leg]]
legs n k x
| x == n
= [[ ]]
| k == 0
= [[(x,n)]]
| otherwise = [(x,y):ls | y ←[x+1..n],ls ←legs n (k −1) y]
When k = 0, the only possible leg is the one that goes straight from x to n without
making any intermediate stops; otherwise every possible leg beginning with x is
chosen.
The next step is to obtain a recursive deﬁnition of schedule. For the base cases
we have
MinWith (cost ps) (legs n k n) = MinWith (cost ps) [[ ]]
= [ ]
MinWith (cost ps) (legs n 0 x) = MinWith (cost ps) [[(x,n)]] = [(x,n)]

13.5 The shuttle-bus problem
325
For the recursive case of legs we can reason
cost ps ((x,y):ls)
=
{ deﬁnition of cost with (qs,rs) = span (atmost y) ls }
legcost qs (x,y)+cost rs ls
⩽
{ assuming cost rs ls ⩽cost rs ls′ }
legcost qs (x,y)+cost rs ls′
=
{ deﬁnition of cost }
cost ps (x,y):ls′
Hence
cost rs ls ⩽cost rs ls′ ⇒cost ps ((x,y):ls) ⩽cost ps ((x,y):ls′)
That leads to the following recursive deﬁnition of schedule:
schedule n k ps = process ps k 0 where
process ps k x
| x == n
= [ ]
| k == 0
= [(x,n)]
| otherwise = minWith (cost ps) [ (x,y):process (cut y ps) (k −1) y
| y ←[x+1..n]]
cut y = dropWhile (atmost y)
The next step is tabulation. Let (k,x) represent the call process (cut x ps) k x. Then
(k,x) depends on all of (k −1,x+1),(k −1,x+2),...,(k −1,n). This is a layered
recursion, so we can turn the problem into one of ﬁnding a shortest path in a layered
network. Alternatively, we can build a table row by row. Suppose we deﬁne
table ps k = [process (cut x ps) k x | x ←[0..n]]
In particular,
schedule n k ps = head (table ps k)
The bottom row of the table is given by
table ps 0 = [[(x,n)] | x ←[0..n−1]]++[[ ]]
It remains to show how table ps k is computed from table ps (k −1). The idea is to
deﬁne step so that
table ps k = step (table ps (k −1))
To this end, let ptails return the proper tails of a list, that is, all the tails except the
list itself:
ptails [ ]
= [ ]
ptails (x:xs) = xs:ptails xs
Then we can deﬁne

326
Eﬃcient recursions
step t = zipWith entry [0..n−1] (ptails t)++[[ ]]
entry x ts = minWith (cost (cut x ps)) (zipWith (:) [(x,y) | y ←[x+1..n]] ts)
Putting these pieces together, we arrive at the ﬁnal algorithm
schedule n k ps = head (apply k step start)
where
start
= [[(x,n)] | x ←[0..n−1]]++[[ ]]
step t
= zipWith entry [0..n−1] (ptails t)++[[ ]]
entry x ts = minWith (cost (cut x ps))
(zipWith (:) [(x,y) | y ←[x+1..n]] ts)
The algorithm can be made more efﬁcient in various ways, including by memoising
cost, but we will leave these optimisations as exercises.
13.6 Chapter notes
The story behind the term ‘dynamic programming’ is described in [4]. An early
account of dynamic programming by Bellman appears in his book [1]. Various
tabulation schemes for recursive programs are presented in [2]. The minimum edit
distance problem has applications in computational biology and is discussed in
most books on stringology, including [5]; see also [6, 8]. The shuttle-bus problem
appears, in different guises, in [3] and as an elevator problem in [7]. The Wikipedia
entry on dynamic programming contains a wealth of other examples.
References
[1]
Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ,
1957.
[2]
Richard S. Bird. Tabulation techniques for recursive programs. ACM Computing
Surveys, 12(4):403–417, 1980.
[3]
Eric V. Denardo. Dynamic Programming: Models and Applications. Prentice-Hall,
Upper Saddle River, NJ, 1982.
[4]
Stuart Dreyfus. Richard Bellman on the birth of dynamic programming. Operations
Research, 50(1):48–51, 2002.
[5]
Dan Gusﬁeld. Algorithms on Strings, Trees, and Sequences. Cambridge University
Press, Cambridge, 1997.
[6]
Gonzalo Navarro. A guided tour of approximate string matching. ACM Computing
Surveys, 33(1):31–88, 2001.
[7]
Steven S. Skiena and Miguel A. Revilla. Programming Challenges. Springer, New
York, 2003.
[8]
Robert A. Wagner and Michael J. Fischer. The string-to-string correction problem.
Journal of the ACM, 21(1):168–173, 1974.

Exercises
327
Exercises
Exercise 13.1 Let T(n) denote the number of additions in computing ﬁb from its
recursive deﬁnition. Given that ﬁb n = Θ(ϕn), where ϕ = (1+
√
5)/2 is the Golden
Ratio, prove that T(n) = Θ(ϕn).
Exercise 13.2 Give an efﬁcient one-line deﬁnition of the function ﬁbs that returns
the inﬁnite list of all the Fibonacci numbers.
Exercise 13.3 The following two identities hold for n>2:
ﬁb (2×n)
= ﬁb n×(2×ﬁb (n+1)−ﬁb n)
ﬁb (2×n+1) = ﬁb n×ﬁb n+ﬁb (n+1)×ﬁb (n+1)
Using these facts, show how to compute ﬁb n in O(log n) steps. As a hint, note that
the linear-time algorithm for ﬁb can be phrased in the form
ﬁb = fst ·foldr step (0,1)·unary
where step k (a,b) = (b,a+b)
unary n = [1..n]
The logarithmic version is obtained by modifying the deﬁnition of step and replacing
unary by binary, where binary returns the binary expansion of a number, least
signiﬁcant digit ﬁrst:
binary n = if n == 0 then [ ] else r :binary q
where (q,r) = n divMod 2
For example, binary 6 = [0,1,1].
Exercise 13.4 Consider the function
fob::Nat →Integer
fob n = if n ⩽2 then fromIntegral n else fob (n−1)+fob (n−3)
Show how to evaluate fob in linear time.
Exercise 13.5 The Stirling numbers can be deﬁned for 0 ⩽r ⩽n by the recurrence
stirling::(Nat,Nat) →Integer
stirling (n,r)
| r == n
= 1
| r == 0
= 0
| otherwise = fromIntegral r ×stirling (n−1,r)+stirling (n−1,r −1)
Give a suitable tabulation scheme for computing stirling efﬁciently.

328
Eﬃcient recursions
Exercise 13.6 An extreme form of dependency graph arises when every value
depends on every previous value, as in the function f, where
f n = if n == 0 then 1 else sum (map f [0..n−1])
How would you compute this particular recursion efﬁciently?
Exercise 13.7 Why does the monotonicity condition
value sn1 ⩽value sn2 ⇒value (add i sn1) ⩽value (add i sn2)
hold for the knapsack problem?
Exercise 13.8 Here is a solution to the knapsack problem based on a one-dimensional
array, similar to the one in the text except that values in each row go from left to
right:
swag w items = a!w
where
a = foldr step start items
start = listArray (0,w) (replicate (w+1) ([ ],0,0))
step item a = ...
Your task is to deﬁne step.
Exercise 13.9 Write down all the possible values of mce "abca" "bac".
Exercise 13.10 The purpose of this exercise is to establish the greedy condition for
the minimum-cost edit problem by showing that, at any point in the sequence, if
the two remaining strings begin with the same character, then starting with a copy
operation always leads to a best possible solution. Suppose a best sequence does
not begin with a copy, so it has to begin with either a delete or an insert (a replace is
not possible as the ﬁrst two characters are the same). The two situations are dual, so
suppose it begins with k delete operations, where k >0. Thereafter, there are three
possibilities for the next edit operation: a copy (if available), a replace, or an insert.
What alternative edit sequence beginning with a copy and with the same cost is
possible in the ﬁrst two cases? What alternative sequence beginning with a copy is
possible in the third case? The necessary assumption is that c ⩽r ⩽d +i, where c
is the cost of a copy, r the cost of a replace, d the cost of a delete, and i the cost of
an insert.
Exercise 13.11 We can memoise cost computations in the ﬁnal algorithm for the
edit sequence problem by pairing edit sequences with their costs. In particular, let
us introduce

Exercises
329
type Pair = (Nat,[Op])
Now the ﬁrst row is deﬁned by
ﬁrstrow::[Char] →[Pair]
ﬁrstrow xs = foldr nextentry [(0,[ ])] xs
where nextentry x row = cons (Delete x) (head row):row
where cons is deﬁned by
cons op (k,es) = (ecost op+k,op:es)
Write down the modiﬁed deﬁnition of nextrow (hint: it also uses cons) and hence
construct a new deﬁnition of mce.
Exercise 13.12 By deﬁnition a distance function d ::A×A →R+, where R+ is the
set of nonnegative real numbers, is a function with the following four properties:
1. d(x,y) ⩾0.
2. d(x,y) = 0 if and only if x = y.
3. d(x,y) = d(y,x).
4. d(x,y) ⩽d(x,z)+d(z,y) for all z.
Show that dist is a distance function, where
dist (xs,ys) = cost (mce xs ys)
For this reason the minimum-cost edit sequence problem is often referred to as the
edit distance problem.
Exercise 13.13 Let k denote the length of the longest common subsequence of xs
and ys. Show that
cost (mce xs ys) ⩽length xs+length ys−2×k
given that the only edit operations allowed are copy, insert, and delete, with costs 0,
1, and 1, respectively. Show that the inequality can be strengthened to an equality.
Exercise 13.14 It may seem in the light of Exercise 13.10 that a minimum edit
sequence can be obtained from a longest upsequence in the following way: partition
the two sequences according to their longest upsequence, giving
xs0 ++[x0]++ ··· ++xsn−1 ++[xn−1]++xsn
ys0 ++[x0]++ ··· ++ysn−1 ++[xn−1]++ysn
where [x0,...,xn−1] is the longest common subsequence. For example,
"bdacb" = "b"++"d"++"" ++"a"++"c"++"b"++""
"ddacc" = "" ++"d"++"d"++"a"++"c"++"" ++"c"
The sequences xsj and ysj have no characters in common, so their minimum edit
sequence can be determined by applying as many replace operations as possible,
followed by either a number of deletes or a number of inserts. Does this idea work?

330
Eﬃcient recursions
Exercise 13.15 To deﬁne an efﬁcient version of the shuttle-bus function schedule
we need to split up the passenger list by deﬁning
split n ps = [cut 0 ps,cut 1 ps,...,cut n ps]
Give a deﬁnition of split.
Now we can memoise cost computations by deﬁning
schedule::Nat →Nat →Passengers →[Leg]
schedule n k ps = extract (apply k step start) where
extract = snd ·head
start
= zipWith entry pss [0..n−1]++[(0,[ ])]
where entry ps x = (legcost ps (x,n),[(x,n)])
pss
= split n ps
step t
= ...
Each sequence of legs is paired with its cost. Deﬁne the local value step t.
Exercise 13.16 Recall the coin-changing problem of Section 7.3:
mkchange::[Denom] →Nat →Tuple
mkchange ds ←MinWith count ·mktuples ds
where count = sum and
mktuples [1] n
= [[n]]
mktuples (d :ds) n = concat [ map (c:) (mktuples ds (n−c×d))
| c ←[0..n div d]]
What is the monotonicity condition that yields a recursive deﬁnition of mkchange?
Write down the recursive deﬁnition and suggest a suitable tabulation scheme.
Answers
Answer 13.1 We have T(0) = T(1) = 0 and T(n) = T(n −1) + T(n −2) + 1 for
n ⩾2. By induction one can then show T(n) = ﬁb (n+1)−1. Since ﬁb n = Θ(ϕn),
where ϕ is the Golden Ratio, the result now follows.
Answer 13.2 We have
ﬁbs::[Integer]
ﬁbs = 0:1:zipWith (+) ﬁbs (tail ﬁbs)
Answer 13.3 We have
ﬁb = fst ·foldr step (0,1)·binary
where step k (a,b) = if k == 0 then (c,d) else (d,c+d)
where c = a×(2×b−a)
d = a×a+b×b

Answers
331
Answer 13.4 The solution is the same as the one for ﬁb except that we maintain
three values at each step:
fob n = fst3 (apply n step (0,1,2))
where step (a,b,c) = (b,c,a+c)
fst3 (a,b,c) = a
Answer 13.5 The simplest solution is to use a two-dimensional array:
stirling (n,r) = a!(n,r)
where a = tabulate f ((0,0),(n,r))
f (i,j) | i == j
= 1
| j == 0
= 0
| otherwise = fromIntegral j×a!(i−1,j)+a!(i−1,j−1)
As an alternative we can go for a solution with the same shape as binom:
stirling (n,r) = head (apply (n−r) step (replicate (r +1) 1))
where step row = scanr1 (+) (zipWith (×) [r′,r′ −1..0] row)
r′ = fromIntegral r
This method computes row (n,r),(n−1,r −1),...,(n−r,0) from the previous row
(n−1,r),(n−2,r−1),...,(n−r−1,0), starting with the row (r,r),...,(0,0), all of
whose entries are 1.
Answer 13.6 A trick question, because f n = 2n−1 for n ⩾1.
Answer 13.7 Because value (add i sn) = value i+value sn.
Answer 13.8 The deﬁnition is
step item a = a//[(j,next j item) | j ←[0..w]]
where next j i = if j<wi then a!j else better (a!j) (add i (a!(j−wi)))
where wi = weight i
The array-based solution has the same asymptotic complexity as the list-based
version but is slightly slower.
Answer 13.9 There are three answers, all with cost 6:
a b c a *
* a b c a
a b * c a
* b * a c
b a * c *
* b a c *
Answer 13.10 In the ﬁrst two cases we can start with a copy and k deletes, to
arrive at the same spot as k deletes followed by either a copy or a replace. Since
c ⩽r ⇒c + kd ⩽kd + r, the ﬁrst sequence gives an edit sequence with smaller
cost. In the third case we can start with a copy and k −1 deletes. This time, we have
c ⩽d +i ⇒c+(k −1)d ⩽kd +i, so again the ﬁrst sequence has smaller cost.

332
Eﬃcient recursions
Answer 13.11 The deﬁnition is basically the same as before except that (:) is
replaced by cons:
nextrow::[Char] →Char →[Pair] →[Pair]
nextrow xs y row = foldr step [cons (Insert y) (last row)] xes
where
xes = zip3 xs row (tail row)
step (x,es1,es2) row = if x == y then (cons (Copy x) es2):row else
minWith fst [cons (Insert y) es1,
cons (Replace x y) es2,
cons (Delete x) (head row)]:row
Now we have
mce xs ys = extract (foldr (nextrow xs) (ﬁrstrow xs) ys)
where extract = snd ·head
Answer 13.12 The ﬁrst two properties are immediate. Changing deletes into inserts
and vice versa, and swapping the arguments of a replace, we get an edit sequence
of the same cost for changing the second list into the ﬁrst, so the third property is
satisﬁed. For the fourth and ﬁnal property we have
cost (mce xs ys) ⩽cost (mce xs zs)+cost (mce zs ys)
because we can concatenate a minimum edit sequence turning xs into zs with a
minimum edit sequence turning zs into ys to get an edit sequence turning xs into ys.
Answer 13.13 Let zs be a longest common subsequence of xs and ys. Construct an
edit sequence that deletes all elements of xs not in zs, inserts all elements of ys not
in zs, and copies the common elements. The cost of this edit sequence is at most
(length xs−length zs)+(length ys−length zs)
and so a minimum cost edit sequence is also bounded by this quantity. To show
equality we have to prove there is no cheaper edit sequence. Given a minimum
sequence es, consider the string zs of length k that results from performing all the
deletes in es on xs. Since ys can be constructed from zs by applying insertions alone,
it follows that zs is a common subsequence of xs and ys. Hence
cost (mce xs ys) ⩾length xs+length ys−2×k
Since a longest common subsequence has length at least k, equality is established.
Answer 13.14 No, not as stated. For the example strings we would obtain
b d * a c b
* d d a c c
2 0 2 0 0 3
with cost 7. But a better edit is given by

Answers
333
b d a c b
d d a c c
3 0 0 0 3
with cost 6.
Answer 13.15 One sensible way of deﬁning split is as follows:
split n ps = scanl op ps [1..n]
where op qs x = dropWhile (atmost x) qs
The deﬁnition of step is
step t = zipWith3 entry pss [0..n−1] (ptails t)++[(0,[ ])]
entry ps x ts = minWith fst (zipWith cons [x+1..n] ts) where
cons y (c,ls) = (legcost (takeWhile (atmost y) ps) (x,y)+c,(x,y):ls)
Answer 13.16 The monotonicity condition is that
count cs1 ⩽count cs2 ⇒count (c:cs1) ⩽count (c:cs2)
That means
(c:)·MinWith count ←MinWith count ·map (c:)
Hence we can deﬁne
mkchange [1] n
= [n]
mkchange (d :ds) n = minWith count [ c:mkchange ds (n−c×d)
| c ←[0..n div d]]
Let (k,n) denote the call mkchange (drop k ds) n. Then (k,n) depends on all of
(k −1,n),(k −1,n−d),(k −1,n−2d),.... The recursion is layered, so we can use
the layered network algorithm for the tabulation scheme. Alternatively, one can
compute the partial solutions row by row.


Chapter 14
Optimum bracketing
A surprising variety of subtly different algorithms arises from the single idea of
trying to bracket an expression X1 ⊗X2 ⊗··· ⊗Xn in the best possible way. We
assume that ⊗is an associative operation, so the manner in which the brackets are
inserted does not affect the expression’s value. However, different bracketings may
have different costs, and the aim of the exercise is to ﬁnd one whose cost is as small
as possible. Depending on how the cost is deﬁned, ﬁnding the best solution may
take constant, linear, linearithmic, quadratic, or cubic time.
Here is a simple example; others will be given later on. Take ⊗to be matrix
multiplication, an associative operation but not in general a commutative one. The
cost of multiplying a p×q matrix by a q×r matrix is O(p×q×r) additions and
multiplications, and the result is a p × r matrix. Now consider the four matrices
X1,X2,X3,X4 with the following dimensions:
(10,20), (20,30), (30,5), (5,50)
With the cost taken as exactly p × q × r, the ﬁve possible ways of bracketing the
four matrices have costs 47500, 18000, 28500, 6500, and 10000, the best one being
(X1 ⊗(X2 ⊗X3))⊗X4 with cost
20×30×5+10×20×5+10×5×50 = 6500
There is no obvious method for bracketing the matrices to achieve minimum cost.
Greedy strategies, like doing the cheapest (or most expensive) multiplication ﬁrst, do
not work. However, as we will see later on, there is a fairly straightforward dynamic
programming algorithm to compute the best bracketing, one whose running time is
Θ(n3) steps for n matrices.
The right way of phrasing the bracketing problem is simply to ask for a leaf-
labelled binary tree of minimum cost with a given list as fringe. Each bracketing
corresponds to a particular tree. For simplicity, the elements at the leaves of the tree
are taken to be the sizes of the objects to be bracketed, not the objects themselves.
We will refer to such sizes as weights to avoid confusion with the use of size to

336
Optimum bracketing
describe the number of nodes in a tree. Problems like this were considered in
Chapter 8. In particular we looked at Huffman coding, which can be regarded as a
version of optimal bracketing in which ⊗is assumed to be commutative as well as
associative, so the fringe can be any permutation of the given list. In this chapter
we will also tackle the restricted version of Huffman coding without commutativity,
in which the fringe has to be exactly the given list. Another example that can be
solved using the techniques of this chapter is to ﬁnd an optimum binary search tree,
a problem we will tackle in Section 14.5.
14.1 A cubic-time algorithm
The problem is based on the following data type of binary trees:
data Tree a = Leaf a | Fork (Tree a) (Tree a)
We will refer to such trees as leaf trees to avoid confusion with other kinds of tree
we will need later on. A leaf tree will be displayed using parentheses. For example,
the leaf tree
5
6
7
1
2
3
4
is displayed as (((5 6) 7) ((1 (2 3)) 4)).
Given Weight as the type of weights, the function mct determines a tree with
minimum cost:
mct ::[Weight] →Tree Weight
mct ←MinWith cost ·mktrees
The function mktrees returns a list of all possible trees with a given fringe. Two
deﬁnitions were given in Chapter 8, but here is another way:
mktrees::[a] →[Tree a]
mktrees [w] = [ Leaf w]
mktrees ws = [ Fork t1 t2
| (us,vs) ←splitsn ws,t1 ←mktrees us,t2 ←mktrees vs]
The function splitsn (see Exercise 8.7) splits a list of length at least two into two
nonempty lists in all possible ways. The above recursive deﬁnition directs us towards

14.1 A cubic-time algorithm
337
a dynamic programming solution, while the inductive deﬁnitions of Chapter 8
suggest heading for a greedy or thinning algorithm. In any case, as mentioned in
Exercise 8.8, there are
2n−2
n−1
 1
n
trees with a fringe of length n, so all deﬁnitions of mktrees take exponential time.
It remains to deﬁne cost. There is a range of possible deﬁnitions, but we will only
consider cost functions that conform to the following general scheme:
type Cost = Nat
cost ::Tree Weight →Cost
cost (Leaf w)
= 0
cost (Fork t1 t2) = cost t1 +cost t2 +f (weight t1) (weight t2)
weight ::Tree Weight →Weight
weight (Leaf w)
= w
weight (Fork t1 t2) = g (weight t1) (weight t2)
Thus the cost of forming a tree is the sum of the costs of forming its two component
subtrees plus some function f of their weights. The weight of a leaf is the value at
the leaf, while the weight of a fork is some further function g of the weights of its
component trees. For the matrix multiplication problem we have the deﬁnitions
type Weight = (Nat,Nat)
f ::Weight →Weight →Cost
f (p,q) (q′,r) | q == q′ = p×q×r
g::Weight →Weight →Weight
g (p,q) (q′,r) | q == q′ = (p,r)
Other interesting instantiations of f and g will be given later on. We suppose
throughout that g is an associative operation, so two trees have the same weight
if they have the same fringe. This fact alone is sufﬁcient for us to write down a
recursive deﬁnition of mct and obtain a cubic-time solution by tabulation. Since the
weights of all trees in mktrees ws are the same, we have
cost u1 ⩽cost u2 ∧cost v1 ⩽cost v2 ⇒cost (Fork u1 v1) ⩽cost (Fork u2 v2)
where u1 and u2 are trees in mktrees us, and v1 and v2 are trees in mktrees vs. That
means we can reﬁne mct to read
mct [w] = Leaf w
mct ws = minWith cost [Fork (mct us) (mct vs) | (us,vs) ←splitsn ws]
Assuming cost takes constant time, the running time T(n) of this version of mct for
a fringe of length n satisﬁes

338
Optimum bracketing
T(n) =
n−1
∑
k=1
(T(k)+T(n−k))+Θ(n)
with solution T(n) = Θ(3n) (see Exercise 14.2).
The next task is to ﬁnd some suitable tabulation scheme. As a ﬁrst step we can
make the computation more efﬁcient by encoding a tree as a triple of values, the
cost of the tree, its weight, and the tree itself:
type Triple = (Cost,Weight,Tree Weight)
With cost, weight, and tree now returning the ﬁrst, second, and third components of
a triple, we have
mct ::[Weight] →Tree Weight
mct = tree·triple
triple::[Weight] →Triple
triple [w] = (0,w,Leaf w)
triple ws = minWith cost [fork (triple us) (triple vs) | (us,vs) ←splitsn ws]
fork ::Triple →Triple →Triple
fork (c1,w1,t1) (c2,w2,t2) = (c1 +c2 +f w1 w2,g w1 w2,Fork t1 t2)
The simplest way of implementing a tabulation scheme is to use a two-dimensional
array. The idea is to store the values of
table::(Int,Int) →[Weight] →Triple
table (i,j) = triple·drop (i−1)·take j
in an array. The value of table (i,j) is the solution when the inputs are the elements
of the segment wi,wi+1,...,wj for 1 ⩽i ⩽j ⩽n. The array-based algorithm takes
the form
mct ws = tree (table (1,n)) where
n = length ws
weights = listArray (1,n) ws
table (i,j)
| i == j = (0,weights!i,Leaf (weights!i))
| i<j
= minWith cost [fork (t !(i,k)) (t !(k +1,j)) | k ←[i..j−1]]
t = tabulate table ((1,1),(n,n))
The function tabulate was deﬁned in the previous chapter:
tabulate::Ix i ⇒(i →e) →(i,i) →Array i e
tabulate f bounds = array bounds [(x,f x) | x ←range bounds]
New entries to table are computed by looking up other entries in the array. Another
array, weights, is used solely for quick access to the given weights.

14.2 A quadratic-time algorithm
339
Assuming f and g take constant time, it takes Θ(j−i) steps to compute entry (i,j)
of the array for i ⩽j, so the total time T(n) is given by
T(n) =
n
∑
i=1
n
∑
j=i
Θ(j−i) = Θ(n3)
In summary, the above tabulation scheme requires cubic time and quadratic space.
The only assumption we made was that the function g for combining weights was
associative. We can do better if we suppose more about f and g, and that is the topic
of the following section.
14.2 A quadratic-time algorithm
It is possible to shave a factor of n off the running time if we make some more
assumptions about f and g. Let r(i,j) denote the location of the ﬁrst best split for
the segment wi,...,wj of the input. That is, if r = r(i,j), then r is the smallest integer
in the range i ⩽r <j such that (wi ...wr) (wr+1 ...wj) is the top-level split in a best
bracketing. We focus on the smallest r because our standard deﬁnition of minWith
happens to return the smallest best split, but the result below also holds when r
is the largest position for a best split. In either case, r(i,i) is undeﬁned because a
single value cannot be split. However, we can set r(i,i) = i for completeness.
The result we want to prove is that, under certain conditions on f and g, the
function r is monotonic; in symbols,
r(i,j−1) ⩽r(i,j) ⩽r(i+1,j)
(14.1)
for i < j. The proof is postponed to Section 14.4. That means we can revise the
tabulation of mct to manipulate quadruples of values, the ﬁrst component of which
records the position of a best split. With cost, weight, and tree now returning the
second, third, and fourth components of a quadruple, and root returning the ﬁrst,
we have
mct ws = tree (table (1,n)) where
n = length ws
weights = listArray (1,n) ws
table (i,j)
| i == j
= (i,0,weights!i,Leaf (weights!i))
| i+1 == j = fork i (t !(i,i)) (t !(j,j))
| i+1<j
= minWith cost [fork k (t !(i,k)) (t !(k +1,j))
| k ←[r(i,j−1)..r(i+1,j)]]
r(i,j) = root (t !(i,j))
t = tabulate table ((1,1),(n,n))
fork k ( ,c1,w1,t1) ( ,c2,w2,t2) = (k,c1 +c2 +f w1 w2,g w1 w2,Fork t1 t2)

340
Optimum bracketing
The case i+1 = j has to be treated separately (see Exercise 14.4). The monotonicity
of r is exploited in the third clause deﬁning table. It is immediate from the deﬁnition
that it takes Θ(r(i+1,j)−r(i,j−1)) steps to compute entry (i,j) of the table when
i+1<j. The total time T(n) needed to compute entry (1,n) can be estimated by
counting the cost of computing each entry of the table along each diagonal d, where
d = j−i:
T(n) = Θ(n)+
n−1
∑
d=2
n−d
∑
i=1
Θ(r(i+1,i+d)−r(i,i+d −1))
The ﬁrst two diagonals, d = 0 and d = 1, can be computed in Θ(n) steps. We have
n−d
∑
i=1
r(i+1,i+d)−r(i,i+d −1) = r(n−d +1,n)−r(1,d) = Θ(n)
since i ⩽r(i,j) ⩽j. That gives T(n) = Θ(n2).
It remains to give the conditions on f and g that ensure (14.1). In Section 14.4 we
will show that (14.1) follows from the quadrangle inequality (QI)
i ⩽i′ ⩽j ⩽j′ ⇒C(i,j)+C(i′,j′) ⩽C(i,j′)+C(i′,j)
(14.2)
where C(i,j) is the minimum cost of bracketing the segment wi,...,wj of the input
w1,...,wn. In words, the sum of the costs for two overlapping intervals is at most the
cost of the union of the intervals plus the cost of their intersection. The conditions on
f and g are those required to ensure the quadrangle inequality holds. The simplest
conditions are when f and g are the same function. It is possible to formulate
conditions when f ̸= g (see Exercise 14.9), but they are rather complicated and it is
difﬁcult to ﬁnd examples to satisfy them, so we will consider only the case f = g.
There are two conditions. Setting f = g = (•) for readability, the ﬁrst condition
is that the quadrangle inequality should also hold for the weight function
W(i,j) = wi •wi+1 •···•wj
Thus
i ⩽i′ ⩽j ⩽j′ ⇒W(i,j)+W(i′,j′) ⩽W(i,j′)+W(i′,j)
The second condition is that W should be monotonic in the sense that
i′ ⩽i ⩽j ⩽j′ ⇒W(i,j) ⩽W(i′,j′)
The monotonicity condition can be simpliﬁed (see the exercises) to read
A ⩽A•B ∧B ⩽A•B
for all weights A and B. Similarly, the quadrangle inequality can be simpliﬁed to
read
(A•B)+(B•C) ⩽(A•B•C)+B

14.3 Examples
341
for all weights A, B, and C. For example, with (•) = (+), the monotonicity and
QI conditions are immediate, provided sizes are nonnegative. With (•) = (×), the
QI condition is 0 ⩽B(A−1)(C −1), which holds if all weights are positive. The
monotonicity condition also holds if all weights are positive. However, the QI
condition fails for (•) = max, and the monotonicity condition for (•) = min.
We emphasise that these conditions are sufﬁcient for an O(n2) algorithm, but not
necessary conditions. Proof of (14.1) is a little complicated; for now we just accept
the result and move on to examples.
14.3 Examples
So far, we have a cubic-time algorithm if g is associative, and a quadratic-time one
if f = g and the monotonicity and QI conditions are satisﬁed. But it is also possible
to have a linear-time, or even constant-time, algorithm, depending on the values of
these two functions. To appreciate the range of possibilities, we will now look at a
number of instructive examples.
Concatenation. The ﬁrst example, an old friend, concerns the best way to concate-
nate a list of lists. It takes m steps to concatenate a list of length m with a list of
length n and the result is a list with length m+n, so we can take f = (≪), where
m ≪n = m, and g = (+). That means there is a cubic-time algorithm to determine
the best way of concatenating lists of lists.
However, there is a much simpler method. Suppose the lists are xsj for 1 ⩽j ⩽n.
The minimum possible cost of carrying out the concatenation is given by ∑n−1
j=1 xj,
where xj is the length of xsj. To see this, observe that each element of xsj for 1 ⩽j<n
has to be concatenated with some list to its right, contributing at least xj to the cost.
The minimum cost can be achieved by bracketing from the right. In other words, the
standard deﬁnition concat = foldr (++) [ ] is, as expected, the best possible way to
concatenate a list of lists. One can regard this solution as taking constant time since
no work has to be done in ﬁnding the best bracketing. Of course, it takes linear time
actually to build the tree. Essentially the same result, namely that bracketing from
the right is optimal, holds for f = (≪) and g = (×). The dual result, namely that
folding from the left is optimum, holds if f = (>>), where m>>n = n.
Adding numbers. Next, what is the best way of adding a list of decimal integers
together? Integer addition is commutative as well as associative, but we will ignore
this fact in what follows. Here the problem is to compute ∑n
k=1 xk, where xk is an
integer with dk digits. We will suppose that adding an m-digit integer to an n-digit
integer takes (m min n) steps and yields an integer of size (m max n). Thus f = min
and g = max. These estimates are not quite accurate for integer addition, owing

342
Optimum bracketing
to possible carries, so the claim below holds only when no carries are involved in
any addition. Since g is associative, there is a cubic-time solution. However, there
is a simple constant-time solution: any way of bracketing the additions is as good
as any other. We claim that the cost of any bracketing is the sum of the lengths
of the integers minus a maximum length. More precisely, let S(i,j) = ∑j
k=i dk and
M(i,j) = Maxj
k=i dk. Then we claim that the cost C(1,n) of adding n numbers is
C(1,n) = S(1,n)−M(1,n), irrespective of the bracketing. The proof is by induction.
For the base case we have
C(1,1) = 0 = S(1,1)−M(1,1)
since the cost of performing no additions is zero. For the induction step, we have
C(1,n)
=
{ assuming an initial split at position j }
C(1,j)+C(j+1,n)+(M(1,j) min M(j+1,n))
=
{ induction }
S(1,j)−M(1,j)+S(j+1,n)−M(j+1,n)+(M(1,j) min M(j+1,n))
=
{ deﬁnition of S }
S(1,n)−M(1,j)−M(j+1,n)+(M(1,j) min M(j+1,n))
=
{ arithmetic: x+y = x min y+x max y }
S(1,n)−(M(1,j) max M(j+1,n))
=
{ deﬁnition of M }
S(1,n)−M(1,n)
All ways of summing the numbers therefore have the same cost, so the way the
brackets are inserted is immaterial. It therefore takes no work to ﬁnd the solution,
though of course again it takes linear time to build the tree.
Multiplying numbers. Next, consider the cost of multiplying a list of decimal num-
bers together, assuming that multiplying an m-digit number by an n-digit number
takes exactly m × n multiplications and gives an answer of length m + n. Thus
f = (×) and g = (+). Again, these estimates are not quite accurate for integer mul-
tiplication, owing to possible carries. As in the case of addition, we can improve on
the cubic-time algorithm because any way of bracketing the multiplications has the
same cost as any other. To deﬁne this cost, let S(i,j) = ∑j
k=i dj and Q(i,j) = ∑j
k=i d2
k.
Then the common cost is given by
C(1,n) = (S(1,n)2 −Q(1,n))/2
The proof is by induction. The base case is
C(1,1) = 0 = (S(1,1)2 −Q(1,1))/2
The induction step is

14.3 Examples
343
3
6
2
1
5
Figure 14.1 The Amoeba Fight Show
C(1,n)
=
{ assuming an initial split at j }
C(1,j)+C(j+1,n)+S(1,j)S(j+1,n)
=
{ induction }
(S(1,j)2 −Q(1,j)+S(j+1,n)2 −Q(j+1,n))/2+S(1,j)S(j+1,n)
=
{ arithmetic: (x2 +y2)/2+xy = (x+y)2/2 }
(S(1,n)2 −Q(1,n))/2
Therefore the multiplications can be performed in any order. This is also a constant-
time solution.
Multiplying matrices. As we have seen, the situation changes when the objects to
be multiplied are matrices, not numbers. In this case the function r is not monotonic.
For example, take the four matrices M1,M2,M3,M4 with dimensions 2×3, 3×2,
2×10, and 10×1, respectively. As can easily be veriﬁed, the best order to compute
M1 M2 M3 is to parenthesise it as (M1 M2)M3 with root 2, while the best way to
compute M1 M2 M3 M4 is to parenthesise it as M1 (M2 (M3 M4)) with root 1. That
means that only the cubic-time dynamic programming solution applies. In fact there
is an O(n log n) solution for the matrix multiplication problem (see the chapter
notes) but it is too complicated to be described here.
Amoeba ﬁght show. This example owes its setting to [13]. Imagine a line of canni-
balistic amoebae, each separated from its neighbour by a sliding door, as shown
in Figure 14.1. Removing a door enables two neighbouring amoebae to ﬁght. The
winner of the ﬁght is always the heavier amoeba, which absorbs its lighter compan-
ion, increasing its weight in the process. The duration of the ﬁght is proportional to
the weight of the lighter amoeba. At the end of all the ﬁghts is a single fat amoeba
whose weight has been increased by the sum of all the losers. The comp`ere wants
the show to be over as quickly as possible, for fast audience turnover. What is the
best way of arranging the ﬁghts, that is, the best order for removing the sliding
doors?
More prosaically, we seek an optimum bracketing where the relevant deﬁnitions
are f = min and g = (+). There is therefore a cubic-time algorithm for the problem.
However, we can put a lower bound to the cost of a show: each amoeba except one
has to lose its life. That means the minimum cost is at least the sum of the weights

344
Optimum bracketing
of all the amoebae except a largest one. This bound can be achieved by the simple
expedient of letting a heaviest amoeba ﬁght at each step. The solution is not unique,
for the two ﬁghts ((((3 6) 2) 1) 5) and (3 (((6 2) 1) 5)) both have minimum
cost 11. One method for constructing a best bracketing is given by
mct xs = foldr Fork e (map Leaf ys)
where e = foldl Fork (Leaf z) (map Leaf zs)
(ys,z:zs) = span (̸= maximum xs) xs
We split a sequence into those elements before a (ﬁrst) maximum value and those
afterwards. For example, [3,6,2,1,5] is split into the two component lists [3]
and [6,2,1,5]. Note that the ﬁrst element of the second list is a maximum ele-
ment. The two lists are combined by folding from the left the elements in the
second list, and then folding the result from the right with the ﬁrst list. This al-
gorithm takes linear time. But it is not an example of a greedy algorithm, at least
not one built on the inductive deﬁnition of mktrees in Chapter 8. For example,
there are three trees over [2,1,7,3] with minimum cost, namely (2 (1 (7 3))),
(2 ((1 7) 3)), and ((2 (1 7)) 3), but none of them can be extended to the unique
solution ((((9 2) 1) 7) 3) for the fringe [9,2,1,7,3].
Restricted Huffman coding. The cost function for Huffman coding is given by
∑n
j=1 xj dj, where dj is the depth of the leaf containing xj. As we saw in Section 8.2,
the same cost function is given by taking f = g = (+) in the optimum bracketing
version of the problem. In the restricted version of Huffman coding the fringe of the
ﬁnal tree has to be exactly the list of elements in the input. In Huffman’s algorithm
the pair whose joint weight is the smallest is combined at each step, but that idea
does not work for the restricted version. For example, the best tree for the fringe
[10,13,9,14] is ((10 13) (9 14)), whose cost is 92, but combining the smallest
pair at each step would lead to the tree ((10 (13 9)) 14) with cost 100. Other ideas,
like choosing a split that best equalises the sum of weights in each half, also do
not work. However, the monotonicity and quadrangle inequality conditions hold,
so there is a quadratic-time algorithm for the problem. There is also another, quite
different algorithm for this particular instance, the Garsia–Wachs algorithm, which
we will discuss in Section 14.6. The Garsia–Wachs algorithm can be implemented
to take O(n log n) steps for an input of length n.
Cartesian sums. Consider the associative operation ⊕deﬁned by
xs⊕ys = [x+y | x ←xs,y ←ys]
This function arose in Section 5.5 in connection with sorting. The cost of computing
⊕on two lists of lengths m and n is m×n additions, and the result is a list of length
m×n, so we have f = g = (×). The problem is to combine a list of nonempty lists

14.4 Proof of monotonicity
345
of numbers with ⊕. As we have seen, the monotonicity and quadrangle inequality
conditions are satisﬁed for this problem, provided each list has a positive length, so
a best bracketing can certainly be found in quadratic time.
Boustrophedon product. Finally, consider an operation known as the boustrophe-
don product of two lists. Some combinatorial generation algorithms involve running
up and down one list in between generating successive elements of another list,
rather like the shuttle on a loom or an ox ploughing a ﬁeld. The word boustrophedon
means ‘ox-turning’ in ancient Greek. The boustrophedon product ⟨++⟩of two lists
can be deﬁned by
(⟨++⟩)::[a] →[a] →[a]
[ ]
⟨++⟩ys = ys
(x:xs) ⟨++⟩ys = ys++x:(xs ⟨++⟩reverse ys)
For example
[3,4] ⟨++⟩[0,1,2] = [0,1,2,3,2,1,0,4,0,1,2]
"abc" ⟨++⟩"xyz" = "xyzazyxbxyzczyx"
The function ⟨++⟩is associative, though this fact is not obvious. So, what is the
best way of computing the boustrophedon product of a list of lists? The cost of
computing ⟨++⟩for two lists of lengths m and n is proportional to the length of
the result, namely m+m×n+n. Thus f = g = (•), where m•n = m+m×n+n.
The monotonicity and quadrangle inequality conditions hold for this problem, so
there is a quadratic-time algorithm for computing the best way of bracketing the
boustrophedon product of a list of lists.
14.4 Proof of monotonicity
This section is devoted solely to the proof of (14.1). The result can be restated in
the form
r(i,j) ⩽r(i,j+1)
and
r(i,j) ⩽r(i+1,j)
(14.3)
where r(i,j) is the smallest integer in the range i ⩽r<j for which the best bracketing
for wi,...,wj begins with the split (wi ...wr) (wr+1 ...wj).
Let C(i,j) denote the minimum cost of bracketing wi,...,wj, and W(i,j) the weight
of the resulting expression. Thus W(i,j) = wi •wi+1 ···wj−1 •wj, where f = g = (•).
Deﬁne Ck(i,j) for i ⩽k <j by
Ck(i,j) = C(i,k)+C(k +1,j)+W(i,j)
Thus Ck(i,j) is the cost of the bracketing (wi,...,wk) (wk+1,...,wj). Now (14.3)
follows from the assertion that, if r is the smallest value in the range i ⩽r <j such
that C(i,j) = Cr(i,j), then

346
Optimum bracketing
i ⩽q<r ⇒Cr(i,j+1)<Cq(i,j+1)
i<q<r
⇒Cr(i+1,j)<Cq(i+1,j)
In turn, these assertions follow from
i ⩽q<r ⇒Cq(i,j)+Cr(i,j+1) ⩽Cq(i,j+1)+Cr(i,j)
(14.4)
i<q<r ⇒Cq(i,j)+Cr(i+1,j) ⩽Cq(i+1,j)+Cr(i,j)
(14.5)
By deﬁnition of r we have Cr(i,j)<Cq(i,j) for i ⩽q<r, so (14.4) and (14.5) give
0<Cq(i,j)−Cr(i,j) ⩽Cq(i,j+1)−Cr(i,j+1)
0<Cq(i,j)−Cr(i,j) ⩽Cq(i+1,j)−Cr(i+1,j)
In turn, (14.4) and (14.5) follow from the quadrangle inequality (14.2), namely
i ⩽i′ ⩽j ⩽j′ ⇒C(i,j)+C(i′,j′) ⩽C(i,j′)+C(i′,j)
Assuming (14.2), we can prove (14.4) by arguing
Cq(i,j)+Cr(i,j+1)
=
{ deﬁnition of Ck }
C(i,q)+C(q+1,j)+W(i,j)+C(i,r)+C(r +1,j+1)+W(i,j+1)
⩽
{ (14.2), as q+1 ⩽r +1<j<j+1 }
C(i,q)+C(q+1,j+1)+W(i,j+1)+C(i,r)+C(r +1,j)+W(i,j)
=
{ deﬁnition of Cq and Ck }
Cq(i,j+1)+Cr(i,j)
The proof of (14.5) is similar.
It remains to prove (14.2). The proof is by induction on j′−i. The claim is trivially
true when i = i′ or j = j′, so (14.2) holds when j′ −i ⩽1. For the induction step we
need to consider the cases i′ = j and i′ <j separately.
Case A: i<i′ = j<j′. In this case (14.2) reduces to
C(i,j)+C(j,j′) ⩽C(i,j′)
(14.6)
if i < j < j′. Suppose C(i,j′) = Cr(i,j′), where i ⩽r < j′. There are two subcases,
depending on whether r <j or j ⩽r. If r <j, then we reason
C(i,j)+C(j,j′)
⩽
{ since C(i,j) ⩽Cr(i,j) for i ⩽r <j }
C(i,r)+C(r +1,j)+W(i,j)+C(j,j′)
⩽
{ induction (14.6), since j′ −r −1<j′ −i as i<r +1 }
C(i,r)+C(r +1,j′)+W(i,j)
⩽
{ assumption; see below }
C(i,r)+C(r +1,j′)+W(i,j′)
=
{ deﬁnition of r }
C(i,j′)
The assumption on W is the case i = i′ of the monotonicity condition

14.5 Optimum binary search trees
347
i′ ⩽i ⩽j ⩽j′ ⇒W(i,j) ⩽W(i′,j′)
The case j ⩽r is handled in the same way and requires case j = j′ of the monotonicity
condition on W.
Case B: i<i′ <j<j′. In this case suppose the two terms on the right-hand side of
(14.2) are minimised at r and s, so
C(i′,j) = Cr(i′,j)
and
C(i,j′) = Cs(i,j′)
where i′ ⩽r < j and i ⩽s < j′. Again there are two symmetric subcases. If s ⩽r,
then we reason
C(i,j)+C(i′,j′)
⩽
{ deﬁnitions of r and s }
Cs(i,j)+Cr(i′,j′)
=
{ deﬁnition of Ck }
C(i,s)+C(s+1,j)+W(i,j)+C(i′,r)+C(r +1,j′)+W(i′,j′)
=
{ induction }
C(i,s)+C(s+1,j′)+W(i,j)+C(i′,r)+C(r +1,j)+W(i′,j′)
⩽
{ assumption; see below }
C(i,s)+C(s+1,j′)+W(i,j′)+C(i′,r)+C(r +1,j)+W(i′,j)
=
{ deﬁnition of Ck }
Cs(i,j′)+Cr(i′,j)
=
{ deﬁnition of r and s }
C(i,j′)+C(i′,j)
The assumption is just the quadrangle inequality condition on W. The case r ⩽s is
handled similarly and also requires the quadrangle inequality. This completes the
proof of (14.1).
14.5 Optimum binary search trees
We turn next to a close cousin of optimum bracketing, namely the problem of
building an optimum binary search tree. One way to build a binary search tree was
described in Section 4.3, where we showed how to balance a tree so that no search
takes more than logarithmic time. In practice, however, different keys have different
probabilities of occurring as the argument of a search. A better organisation would
be to have keys with a high frequency of occurring closer to the root. For example,
suppose we wanted to search for all occurrences of the nine-letter words in this
book, say for the purpose of preparing an index. It turns out that the word ‘algorithm’
appears much more frequently than ‘condition’ or ‘operation’, so that key should be
closer to the root of the tree.
Suppose we are given probabilities p1,p2,...,pn, expressed as integer frequency
counts, so that pj is the probability that the argument of a successful search is the

348
Optimum bracketing
value xj in a list x1,x2,...,xn of increasing values. Suppose q0,q1,...,qn is another
list so that qj is the probability that the argument of an unsuccessful search falls
between the two values xj and xj+1. By convention, q0 represents the probability
that the search argument is less than x1 and qn the probability that it is greater than
xn. We can install these values in a modiﬁed binary search tree in which Null nodes
are replaced by leaf nodes containing q-values, and internal nodes are augmented
with p-values.
Thus we deﬁne a binary search tree to be
data BST a = Leaf Nat | Node Nat (BST a) a (BST a)
For example, ignoring x-values, a simple example is
q0
p1
q1
p2
q2
p3
q3
The cost of this tree is 2q0 +2p1 +3q1 +3p2 +3q2 +p3 +q3, which is the scalar
product of the result of ﬂattening the tree and the depths of the nodes, where the
depths of non-leaf nodes are counted from 1 rather than 0. In general,
cost ::BST a →Nat
cost t = sum (zipWith (×) (ﬂatten t) (depths t))
where
ﬂatten::BST a →[Nat]
ﬂatten (Leaf q)
= [q]
ﬂatten (Node p l x r) = ﬂatten l++[p]++ﬂatten r
depths::BST a →[Nat]
depths = from 0
where from d (Leaf
)
= [d]
from d (Node
l
r) = from (d +1) l++[d +1]++from (d +1) r
We saw a similar deﬁnition of cost in Huffman coding. Moreover, as with the cost
function in Huffman coding, we can express cost recursively:
cost (Leaf q)
= 0
cost (Node p l x r) = cost l+cost r +weight (Node p l x r)
weight (Leaf q)
= q
weight (Node p l x r) = p+weight l+weight r
It follows that the cost C(i,j) of building a binary search tree with frequency counts

14.6 The Garsia–Wachs algorithm
349
pi+1,...,pj and qi,...,q j is given by
C(i,j) = Minj−1
k=i (C(i,k)+C(k +1,j))+w(i,j)
where w(i,j) = qi + pi+1 + ··· + q j−1 + pj + q j. The function w is monotonic and
satisﬁes the quadrangle inequality, so (14.1) holds and there is a quadratic-time
dynamic programming algorithm for constructing a binary search tree with minimum
cost.
14.6 The Garsia–Wachs algorithm
When the frequency counts pj are all zero, so only the costs of unsuccessful searches
matter, the problem of ﬁnding an optimum search tree is essentially the same as
that of the restricted version of Huffman coding in which the fringe has to be
exactly the given list. In turn, this is exactly the instance of optimum bracketing
in which f = g = (+). For these particular values of f and g there is another, quite
different algorithm for computing a tree with minimum cost. The algorithm is
known as the Garsia–Wachs algorithm and is fairly easy to describe – at least in
an unoptimised form – but even the best current proof of its correctness has some
tricky details, so we will omit it. References to published proofs are given in the
chapter notes.
The Garsia–Wachs algorithm is a two-stage process (see Exercise 14.14 as to why
two stages appear to be necessary). In the ﬁrst stage we build a tree from the given
list of weights, and in the second stage we rebuild it. With Weight as a synonym for
Int, we have
gwa::[Weight] →Tree Weight
gwa ws = rebuild ws (build ws)
With Label as another synonym for Int, the types of build and rebuild are
build
::[Weight] →Tree Label
rebuild ::[Weight] →Tree Label →Tree Weight
The result of build ws is a tree whose fringe is not ws but some permutation of
the labels [1..n], where n is the length of ws. The critical property of this tree
concerns the depths of its leaves. Suppose the depths are d1,d2,...,dn, where dj is
the depth of Leaf j. Then there is a tree with minimum cost and fringe ws in which
the depth of the leaf labelled with wj is dj. As an example, suppose build applied to
[27,16,11,70,21,31,65] produces the tree

350
Optimum bracketing
5
6
7
1
2
3
4
The list of depths in numerical order of leaf value is [3,4,4,2,3,3,2]. The claim,
which we will not prove, is that there is a minimum-cost tree for the given input
whose depths in fringe order constitute exactly this list, and that tree is
27
16
11
70
21
31
65
This tree can be obtained from the one above by a simple bottom-up algorithm.
The starting point is a list of pairs, the ﬁrst component of each pair being a leaf
containing the required label wj, and the second component being the depth dj. For
our example this is the list
(27,3) (16,4) (11,4) (70,2) (21,3) (31,3) (65,2)
in which a pair (w,d) represents (Leaf w,d). This list of pairs is reduced to a single
pair by repeatedly combining the ﬁrst two adjacent pairs in the list with the same
depth until only a single pair remains. When two pairs with a common depth are
combined, the depth is reduced by one. Thus for our example we get the sequence
of steps pictured in Figure 14.2, ending with a single tree and a ﬁnal depth of 0. This
process is not guaranteed to work for all lists of depths, but it does work for those
that result from the ﬁrst stage of the Garsia–Wachs algorithm. For example, the tree
((1 3) 2) has depths [2,1,2] in numerical order of leaf label, but no adjacent pair
has the same depth, so no pair can be combined, and the reduction process fails to
make progress.
The obvious way to implement this reduction process is by a function reduce:

14.6 The Garsia–Wachs algorithm
351
(27,3)
(16,4)
(11,4)
(70,2)
(21,3)
(31,3)
(65,2)
(27,3)
((16 11),3)
(70,2)
(21,3)
(31,3)
(65,2)
((27 (16 11)),2)
(70,2)
(21,3)
(31,3)
(65,2)
(((27 (16 11)) 70),1)
(21,3)
(31,3)
(65,2)
(((27 (16 11)) 70),1)
((21 31),2)
(65,2)
(((27 (16 11)) 70),1)
(((21 31) 65),1)
(((((27 (16 11)) 70) ((21 31) 65))),0)
Figure 14.2 Combining trees
reduce::[(Tree Label,Depth)] →Tree Label
reduce = extract ·until single step where
extract [(t, )] = t
step (x:y:xs) = if depth x == depth y then join x y:xs else x:step (y:xs)
join (t1,d) (t2, ) = (Fork t1 t2,d −1)
where Depth is a synonym for Int and depth = snd. The function step is applied
repeatedly until it produces a singleton list. However, this deﬁnition of reduce
can take quadratic time because step can take linear time. The inefﬁciency arises
because, if step ﬁnds the ﬁrst pair to be joined at positions k and k+1, then the next
call of step will repeat the unsuccessful search on the ﬁrst k −2 elements when it
could begin a new search at position k −1, the earliest position at which two depths
could be the same. One way to avoid the inefﬁciency is to use a foldl and a recursive
deﬁnition of step, redeﬁning reduce to read
reduce::[(Tree Label,Depth)] →Tree Label
reduce = extract ·foldl step [ ] where
extract [(t, )] = t
step [ ] y
= [y]
step (x:xs) y = if depth x == depth y then step xs (join x y) else y:x:xs
join (t1,d) (t2, ) = (Fork t1 t2,d −1)
The ﬁrst argument to step maintains the invariant that no two adjacent pairs on the
list have the same depth; this list is kept in reverse order for efﬁciency. To maintain
the invariant, step is called recursively whenever two pairs are joined. Each call of
step takes time proportional to the number of join operations, and there are exactly
n−1 of these operations in total, so reduce now takes linear time.
Having dealt with reduce, we can now deﬁne rebuild:
rebuild ::[Weight] →Tree Label →Tree Weight
rebuild ws = reduce·zip (map Leaf ws)·sortDepths
The function sortDepths sorts the depths of a tree into increasing order of label

352
Optimum bracketing
value. Since labels take the form [1,2,...,n], where n is the number of nodes in the
tree, sorting can be accomplished in linear time by using an array:
sortDepths::Tree Label →[Depth]
sortDepths t = elems (array (1,size t) (zip (fringe t) (depths t)))
The functions size, which counts the number of nodes in a tree, fringe and depths
can be computed in linear time, so sortDepths and rebuild each take linear time.
It remains to deal with the ﬁrst stage of the Garsia–Wachs algorithm, the function
build. This is where the intricacy of the algorithm resides. The plan of attack is to
develop a quadratic-time solution ﬁrst, and then improve it to a linearithmic one by
a suitable choice of data structure.
For input [w1,w2,...,wn], the starting point is a list
(0,w0), (1,w1), (2,w2), ..., (n,wn)
of pairs of leaves and weights, so (j,w) abbreviates (Leaf j,w). The ﬁrst pair (0,w0)
is a sentinel pair in which w0 = ∞. Use of a sentinel simpliﬁes the description of the
algorithm but is not essential (see the exercises). The following two steps are now
repeated until just two pairs remain, the sentinel pair and one other:
1. Given the current list [(0,w0),...,(tp,wp)], where p>1, ﬁnd the largest j in the
range 1 ⩽j < p such that wj−1 + wj ⩾wj + wj+1, equivalently, wj−1 ⩾wj+1.
Such a j is guaranteed to exist since w0 = ∞. Replace the two pairs (tj,wj) and
(tj+1,wj+1) by a single pair (t∗,w∗) = (Fork tj tj+1,wj +wj+1), giving a new list
(0,w0), (t1,w1), ..., (tj−1,wj−1), (t∗,w∗), (tj+2,wj+2), ..., (tp,wp)
2. Now move (t∗,w∗) to the right over all pairs (t,w) for which w<w∗.
At the end of this process there are just two pairs left, the sentinel and a second pair
whose ﬁrst component is the required tree.
Here is an example. Suppose we begin with the list
(0,∞), (1,10), (2,25), (3,31), (4,22), (5,13), (6,18), (7,45)
The ﬁrst pair to be combined is (5,13) and (6,18) (because 22 ⩾18). The result is
shifted zero places to the right, giving
(0,∞), (1,10), (2,25), (3,31), (4,22), ((5 6),31), (7,45)
The next pair to be combined is (4,22) and ((5 6),31) (because 31 ⩾31). The
result is shifted one place to the right, giving
(0,∞), (1,10), (2,25), (3,31), (7,45), ((4 (5 6)),53)
The next pair to be combined is (1,10) and (2,25), giving

14.6 The Garsia–Wachs algorithm
353
(0,∞), (3,31), ((1 2),35), (7,45), ((4 (5 6)),53)
The remaining three steps are similar in that they all involve combining the second
two pairs:
(0,∞), (7,45), ((4 (5 6)),53), ((3 (1 2)),66)
(0,∞), ((3 (1 2)),66), ((7 (4 (5 6))),98)
(0,∞), (((3 (1 2)) (7 (4 (5 6)))),164)
The ﬁrst component of the second pair is the ﬁnal tree. Note that the sentinel plays
a passive role and is never combined with another pair.
The obvious way to implement this algorithm is repeatedly to scan the whole
list from right to left at each step, looking for the largest j such that wj−1 ⩾wj+1.
However, a better way of organising the search stems from the following observation.
Say that a sequence w1,w2,... is two-sorted if w1 < w3 < w5 < ··· and w2 < w4 <
w6 <···. It follows from the deﬁnition of j in step 1 that the sequence wj,...,wp is
two-sorted. Suppose that the following sequence of weights is produced by step 2:
w0, w1, w2, ..., wj−1, wj+2, ..., wk−1, w∗, wk, ..., wp
Again, both wk,...,wp and wj+2,...,wk−1,w∗are two-sorted because wk−2 <w∗. Fur-
thermore, we know that wj+r <w∗⩽wk for 2 ⩽r <k −j. That means the next pair
to be combined is the ﬁrst one in the following list of three possibilities:
1. wk and wk+1, provided w∗⩾wk+1;
2. wj+2 and wj+3, provided wj−1 ⩾wj+3;
3. wi and wi+1, provided 1 ⩽i<j−1 and wi−1 ⩾wi+1.
These cases can be captured by expressing build in terms of foldr and a new function
step:
build ::[Weight] →Tree Label
build ws = extract (foldr step [ ] (zip (map Leaf [0..]) (inﬁnity:ws)))
where extract [ ,(t, )] = t
inﬁnity = sum ws
No weight arising during the algorithm can be greater than the sum of the input
weights, so this deﬁnition of inﬁnity is adequate. The function foldr step [ ] scans
the input from right to left, looking for the next pair to be combined. To deﬁne step,
we ﬁrst introduce
type Pair = (Tree Label,Weight)
weight ::Pair →Weight
weight (t,w) = w

354
Optimum bracketing
Then step is deﬁned by
step::Pair →[Pair] →[Pair]
step x (y:z:xs) | weight x<weight z = x:y:z:xs
| otherwise
= step x (insert (join y z) xs)
step x xs
= x:xs
join::Pair →Pair →Pair
join (t1,w1) (t2,w2) = (Fork t1 t2,w1 +w2)
insert ::Pair →[Pair] →[Pair]
insert x xs = ys++step x zs
where (ys,zs) = splitList x xs
splitList x xs = span (λy.weight y<weight x) xs
The function insert makes use of an instance splitList of the general utility function
span to ﬁnd the right place for a combined pair to be inserted, and calls step again
to deal with Case 1. The recursive call to step in the deﬁnition of step deals with
Case 2, and Case 3 is handled by the right-to-left search in foldr step [ ]. Note that
the second argument of both step and insert is always a two-sorted list, a fact we
will exploit later on.
In the worst case (see Exercise 14.13), the running time of build is quadratic
in the length of the input. That means that the algorithm is no better than the
dynamic programming algorithm seen earlier. The main culprit is the function
insert, which can take linear time in the worst case. If we could arrange that insert
took logarithmic time, then the total running time of the Garsia–Wachs algorithm
would be reduced to O(n log n) steps. Such an implementation is indeed possible,
because the second argument to insert is not an arbitrary list of pairs but one that is
two-sorted on second components.
The revised implementation is carried out in two stages. The ﬁrst stage is to
rewrite build in terms of a new data type List Pair, designed for representing lists of
pairs that are two-sorted on second components. The following six operations are to
be provided:
emptyL ::List a
nullL
::List a →Bool
consL
::a →List a →List a
deconsL::List a →(a,List a)
concatL ::List a →List a →List a
splitL
::Pair →List Pair →(List Pair,List Pair)
Most of these operations are self-explanatory. The ﬁrst ﬁve functions work for lists

14.6 The Garsia–Wachs algorithm
355
of any type, but the function splitL is speciﬁc to List Pair. This function is the
analogue of splitList used in the deﬁnition of insert.
The function build is replaced by a new version buildL, basically the same as
before except that certain list operations are replaced with List operations:
buildL::[Weight] →Tree Label
buildL ws = extractL (foldr stepL emptyL (start ws))
where start ws = zip (map Leaf [0..]) (inﬁnity:ws)
inﬁnity = sum ws
extractL::List Pair →Tree Label
extractL xs = t
where ( ,ys)
= deconsL xs
((t, ), ) = deconsL ys
stepL::Pair →List Pair →List Pair
stepL x xs = if nullL xs ∨nullL ys ∨weight x<weight z
then
consL x xs
else
stepL x (insertL (join y z) zs)
where (y,ys) = deconsL xs
(z,zs) = deconsL ys
insertL::Pair →List Pair →List Pair
insertL x xs = concatL ys (stepL x zs)
where (ys,zs) = splitL x xs
The second stage is to implement List so that the six operations above take at most
logarithmic time. Then buildL will take linearithmic time. There are various options,
and we choose an implementation based on a modiﬁcation of the balanced binary
search trees of Section 4.3, henceforth called search trees to distinguish them from
the leaf trees constructed by the algorithm. To motivate the modiﬁcation, consider
the search tree
6
8
12
10
14
15
20
whose nodes are labelled with pairs of leaf trees and their weights, although only the
weights are shown. Flattening this tree produces a list of weights which is two-sorted
but not sorted. Now suppose we want to insert a new pair with weight w into this
search tree. We cannot use straightforward binary search because the labels of the
search tree are not in increasing order of weight. Instead, as well as comparing w

356
Optimum bracketing
with the weight of the leaf tree at the root, we must also compare it with the weight
of the preceding leaf tree in the list. Only if w is greater than both these weights
can we continue by searching the right subtree; otherwise we have to search the left
subtree. In order to avoid repeatedly having to discover the weight of the preceding
leaf tree, we can install this tree at the root of a search tree. If there is no preceding
leaf tree, then we can artiﬁcially install a copy of the leaf tree. That leads to the tree
(6,6)
(6,8)
(8,12)
(12,10)
(10,14)
(14,15)
(15,20)
The data type List Pair is now introduced as an instance of
data List a = Null | Node Int (List a) (a,a) (List a)
in which nodes are labelled with pairs of values. As in Section 4.3, the ﬁrst label of
a Node records the height of the tree, which is needed in order to maintain balance.
The implementations of emptyL and nullL are immediate:
emptyL::List a
emptyL = Null
nullL::List a →Bool
nullL Null = True
nullL
= False
The operation consL adds a new pair as a leftmost element of a binary tree:
consL::a →List a →List a
consL x Null
= node Null (x,x) Null
consL x (Node
t1 (y,z) t2) = if nullL t1
then balance (consL x t1) (x,z) t2
else balance (consL x t1) (y,z) t2
For an empty tree t, the operation consL x t creates a new node with label (x,x). For
a nonempty tree t whose left subtree is empty (so y = z), consL x t creates a new
node with label (x,x) and, since x is now the preceding value of z, assigns (x,z) as
the new value at the root. Otherwise consL x is applied to the left subtree of t. The
deﬁnition makes use of the two smart constructors, node and balance, described in
Section 4.3. Recall that node is invoked only when the two trees have heights that
differ by at most one, and balance only when the two heights differ by at most two.

14.6 The Garsia–Wachs algorithm
357
Next, the function deconsL is deﬁned by
deconsL::List a →(a,List a)
deconsL (Node
t1 (x,y) t2) = if nullL t1 then (y,t2)
else (z,balance t3 (x,y) t2)
where (z,t3) = deconsL t1
This searches along the left spine of a tree to ﬁnd the ﬁrst element.
The next function is concatL, which is essentially the second version of the
function combine deﬁned in Section 4.3:
concatL::List a →List a →List a
concatL t1 Null = t1
concatL Null t2 = t2
concatL t1 t2
= gbalance t1 (x,y) t3
where x = lastL t1
(y,t3) = deconsL t2
The subsidiary function lastL returns the last value in a nonempty tree:
lastL::List a →a
lastL (Node
t1 (x,y) t2) = if nullL t2 then y else lastL t2
In the third clause of concatL the last value in t1 and the ﬁrst value in t2 are combined
as a new root. The deﬁnition of concatL makes use of the general rebalancing
function gbalance deﬁned in Section 4.3.
The ﬁnal function splitL is similar to the function split deﬁned in Section 4.4.
The difference is that splitL x t has to split the tree t into a pair of trees (t1,t2) in
which t1 consists of the initial segment of t whose weight components are all less
than weight x, and t2 is the remaining ﬁnal segment of t. To carry out this process,
we split a tree into pieces and then sew the pieces together again to make the ﬁnal
pair of trees. Thus
splitL::Pair →List Pair →(List Pair,List Pair)
splitL x t = sew (pieces x t)
The only difference between this deﬁnition of splitL and the deﬁnition of split in
Section 4.4 is in the deﬁnition of pieces:
data Piece a = LP (List a) (a,a) | RP (a,a) (List a)
pieces::Pair →List Pair →[Piece Pair]
pieces x t = addPiece t [ ] where
addPiece Null ps = ps
addPiece (Node
t1 (y,z) t2) ps = if weight x>max (weight y) (weight z)
then addPiece t2 (LP t1 (y,z):ps)
else addPiece t1 (RP (y,z) t2 :ps)

358
Optimum bracketing
As we saw in Section 4.4, splitL takes logarithmic time in the size of the tree. This
completes the deﬁnition of the Garsia–Wachs algorithm.
14.7 Chapter notes
The standard example of optimum bracketing, presented in most texts on algorithm
design, is matrix multiplication. In fact, there is an O(n log n) algorithm for the
multiplication of n matrices due to Hu and Shing, see [6, 7], though the details
are quite complicated. For an alternative method of tabulation that uses trees with
shared subtrees rather than arrays to record partial results, see [1, Chapter 21]. The
quadratic-time algorithm was ﬁrst described for the particular case of optimum
binary search trees in [11]. Our proof of the conditions under which r is monotonic
is an extension of the proof given by Yao in [14]. Yao’s paper also considers other
examples for which the monotonicity and QI conditions hold.
The amoeba ﬁght show example was ﬁrst described in [13]. For combinatorial
applications of the boustrophedon product, see [1, Chapter 28].
The Garsia–Wachs algorithm has an interesting history. It was ﬁrst discussed
in terms of restricted Huffman coding in [4], where a cubic-time algorithm was
proposed. As a special case of optimum binary search trees, this was reduced to
a quadratic-time algorithm in [11]. A different method was described by Hu and
Tucker in [8]; it is also presented in [5] and in the ﬁrst edition of [12]. According
to Knuth, “no simple proof [of the Hu–Tucker algorithm] is known, and it is quite
possible that no simple proof will ever be found.” Then along came a modiﬁcation
of the Hu–Tucker algorithm, the Garsia–Wachs algorithm [3], which was adopted in
the second edition of [12]. The best proof of its correctness, while still not exactly
simple, is discussed in [10]; see also [9]. A functional description of the Garsia–
Wachs algorithm in ML, though one that uses some non-pure functional techniques,
was given in [2]. All of these articles describe only the quadratic-time version of
the algorithm. There is a fairly short appendix to [3], written by Robert E. Tarjan,
that outlines how to implement the O(n log n) version. Knuth also mentions the
sub-quadratic algorithm in an exercise [12, Section 6.2.2, Exercise 45], which is
answered fairly cryptically on page 713. As far as we know, our description of an
optimal, purely functional implementation of the Garsia–Wachs algorithm is new.
References
[1]
Richard Bird. Pearls of Functional Algorithm Design. Cambridge University Press,
Cambridge, 2010.
[2]
Jean-Christophe Filliˆatre. A functional implementation of the Garsia-Wachs
algorithm. In ACM SIGPLAN Workshop on ML, pages 91–96, 2008.

Exercises
359
[3]
Adriano M. Garsia and Michelle L. Wachs. A new algorithm for minimum cost
binary trees. SIAM Journal on Computing, 6(4):622–642, 1977.
[4]
Edgar N. Gilbert and Edward F. Moore. Variable-length binary encodings. Bell
Systems Technical Journal, 38(4):933–967, 1959.
[5]
Te Chiang Hu. Combinatorial Algorithms. Addison-Wesley, Reading, MA, 1982.
[6]
Te Chiang Hu and Man-Tak Shing. Computation of matrix chain products, part I.
SIAM Journal on Computing, 11(2):362–373, 1982.
[7]
Te Chiang Hu and Man-Tak Shing. Computation of matrix chain products, part II.
SIAM Journal on Computing, 13(2):228–251, 1984.
[8]
Te Chiang Hu and Alan C. Tucker. Optimal computer search trees and variable-length
alphabetic codes. SIAM Journal on Applied Mathematics, 21(4):514–532, 1971.
[9]
Marek Karpinski, Lawrence L. Larmore, and Wojciech Rytter. Correctness of
constructing optimal alphabetic tree revisited. Theoretical Computer Science,
180(1–2):309–324, 1997.
[10] Jeffrey H. Kingston. A new proof of the Garsia-Wachs algorithm. Journal of
Algorithms, 9(1):129–136, 1988.
[11] Donald E. Knuth. Optimum binary search trees. Acta Informatica, 1(1):14–25, 1971.
[12] Donald E. Knuth. The Art of Computer Programming, volume 3: Sorting and
Searching. Addison-Wesley, Reading, MA, second edition, 1998.
[13] Lambert Meertens. Algorithmics: Towards programming as a mathematical activity.
In J. W. de Bakker, M. Hazewinkel, and J. K. Lenstra, editors, Mathematics and
Computer Science, volume 1 of CWI Monographs, pages 289––334. North-Holland
Publishing Company, Amsterdam, 1986.
[14] Frances F. Yao. Efﬁcient dynamic programming using quadrangle inequalities. In
ACM Symposium on the Theory of Computing, pages 429–435, 1980.
Exercises
Exercise 14.1 Write down the ﬁve ways of bracketing X1 ⊗X2 ⊗X3 ⊗X4. How
many ways of bracketing ﬁve Xs are there?
Exercise 14.2 Show that if T(1) = 1 and
T(n) = n+
n−1
∑
k=1
(T(k)+T(n−k))
then T(n) = (3n −1)/2.
Exercise 14.3 Suppose that the function cost associated with optimum bracketing
is generalised to read
cost (Leaf w)
= 0
cost (Fork t1 t2) = h (cost t1) (cost t2)+f (size t1) (size t2)
Under what conditions on h would the cubic-time algorithm in the text still work?
What might be an appropriate value of h if both subtrees could be computed in
parallel?

360
Optimum bracketing
Exercise 14.4 Suppose we replace the deﬁnition of table (i,j) in the quadratic
algorithm for mct by
table (i,j)
| i == j
= (i,0,weights!i,Leaf (weights!i))
| otherwise = minWith cost [ fork k (t !(i,k)) (t !(k +1,j))
| k ←[r(i,j−1)..r(i+1,j)]]
What goes wrong?
Exercise 14.5 Show that the monotonicity condition
i′ ⩽i ⩽j ⩽j′ ⇒S(i,j) ⩽S(i′,j′)
follows from A ⩽A•B and B ⩽A•B for all sizes A and B.
Exercise 14.6 Similarly, show that the quadrangle inequality
i ⩽i′ ⩽j ⩽j′ ⇒S(i,j)+S(i′,j′) ⩽S(i,j′)+S(i′,j)
follows from (A•B)+(B•C) ⩽(A•B•C)+B.
Exercise 14.7 Show that the quadrangle inequality fails for (•) = max.
Exercise 14.8 Verify the monotonicity and quadrangle inequality conditions when
m•n = m+m×n+n, where m and n are nonnegative.
Exercise 14.9 When f ̸= g the conditions that ensure (14.1) are as follows. With
f = (◦) and g = (•) the ﬁrst condition is that
A◦B ⩽A◦(B•X) ∧A◦B ⩽(X •A)◦B
This generalises the monotonicity condition. The generalisation of the QI condition
is more complicated. First of all, say that X is a right-factor of Y if Y = X or
Y = Z •X for some Z. Dually, X is a left-factor of Y if Y = X or Y = X •Z for some
Z. The two conditions are ﬁrstly that, if C •D is a right-factor of A•B, then
A◦B+C ◦(D•X) ⩽A◦(B•X)+C ◦D
and secondly that, if C •D is a left-factor of A•B, then
A◦B+(X •C)◦D ⩽(X •A)◦B+C ◦D
As we said in the text, it is difﬁcult to ﬁnd useful examples in which these conditions
are satisﬁed. Do they hold when m◦n = m and (•) = (+)? How about m◦n = m
and (•) = (×)?
Exercise 14.10 Given that the boustrophedon product ⟨++⟩satisﬁes the two equa-
tions
(xs++[x]++ys) ⟨++⟩zs = (xs ⟨++⟩zs)++[x]++(ys ⟨++⟩rev xs zs)
reverse (ys ⟨++⟩zs)
= reverse ys ⟨++⟩rev xs zs
where the function rev is deﬁned by

Exercises
361
rev xs zs = if even (length xs) then reverse zs else zs
prove that ⟨++⟩is associative.
Exercise 14.11 Suppose in Section 14.4 that r(i,j) is deﬁned to be the largest best
split rather than the smallest. Then (14.3) is proved by showing that Cq(i,j+1) ⩾
Cr(i,j+1) for i ⩽q<r, and Cq(i+1,j) ⩾Cr(i+1,j) for i+1 ⩽q<r. Prove that
these two facts follow from (14.4) and (14.5).
Exercise 14.12 Use of a special sentinel in the building phase of the Garsia–Wachs
algorithm is not essential, provided we change the deﬁnition of build to read
build = endstep·foldr step [ ]·zip (map Leaf [1..])
Give the deﬁnition of endstep.
Exercise 14.13 Consider the following input to build:
[k,k,k +1,k +1,...,2k −1,2k −1]
How long does build take?
Exercise 14.14 One may wonder whether the two stages of the Garsia–Wachs
algorithm are necessary. To show that they are, we can consider two obvious
simpliﬁcations, neither of which contains a labelling stage. One is to follow the
algorithm for Huffman coding by combining at each step two adjacent trees with
minimum combined weight. Ties can be broken arbitrarily by choosing the ﬁrst such
pair. However, unlike Huffman coding, the combined tree is not moved over other
trees in order to maintain the same fringe. Here are the steps for input [4,2,4,4,7]:
(4,4)
(2,2)
(4,4)
(4,4)
(7,7)
((4 2),6)
(4,4)
(4,4)
(7,7)
((4 2),6)
((4 4),8)
(7,7)
(((4 2) (4 4)),14)
(7,7)
((((4 2) (4 4)) 7),21)
The second simpliﬁcation is to start off with the same input as above, and to follow
step 1 of the function build, but again not to move the result. Here is the computation
for the same input as above:
(4,4)
(2,2)
(4,4)
(4,4)
(7,7)
(4,4)
((2 4),6)
(4,4)
(7,7)
((4 (2 4)),10)
(4,4)
(7,7)
((4 (2 4)),10)
((4 7),11)
(((4 (2 4)) (4 7)),21)
What are the costs of these two trees? Compute gwa [4,2,4,4,7] and show that it
has a lower cost.

362
Optimum bracketing
Exercise 14.15 Show that the Garsia–Wachs algorithm does not work when ◦and
• are both ×.
Answers
Answer 14.1 The ﬁve ways are
X1 ⊗(X2 ⊗(X3 ⊗X4))
X1 ⊗((X2 ⊗X3)⊗X4)
(X1 ⊗X2)⊗(X3 ⊗X4)
(X1 ⊗(X2 ⊗X3))⊗X4
((X1 ⊗X2)⊗X3)⊗X4
There are 14 ways to bracket ﬁve terms.
Answer 14.2 Rewriting the right-hand side gives us
T(n) = n+2
n−1
∑
k=1
T(k)
Setting T(n) = (f(n)−1)/2 to get rid of the n on the right, we have
f(n)−1
2
= 1+
n−1
∑
k=1
f(k)
which is solved by taking f(n) = 3n.
Answer 14.3 The condition x ⩽u ∧y ⩽v ⇒h x y ⩽h u v sufﬁces to ensure the
monotonicity condition
cost u1 ⩽cost u2 ∧cost v1 ⩽cost v2 ⇒cost (Node u1 v1) ⩽cost (Node u2 v2)
and therefore a recursive deﬁnition of mct. In a parallel setting we could take h to
return the maximum of two numbers.
Answer 14.4 In the case i+1 = j the result would be
minWith cost [fork i (t !(i,i)) (t !(i+1,i+1)),
fork (i+1) (t !(i+1,i+1)) (t !(i+2,i+1))]
But t !(i+2,i+1) is not deﬁned.
Answer 14.5 If i′ = i and j = j′ there is nothing to prove. If i′ = i and j<j′, then
monotonicity follows from A ⩽A•B, where A = S(i,j) and B = S(j+1,j′). Dually,
if i′ < i and j = j′, then monotonicity follows from B ⩽A • B, where A = S(i′,i)
and B = S(i + 1,j). Finally, if i′ < i and j < j′, then monotonicity follows from
B ⩽A•B•C, where A = S(i′,i), B = S(i+1,j), and C = S(j+1,j′). But this last
condition follows from the ﬁrst two.

Answers
363
Answer 14.6 If i′ = i or j = j′, then the result is immediate. Otherwise, we set
A = S(i,i′), B = S(i′ +1,j), and C = S(j+1,j′). Then the result follows from
(A•B)+(B•C) ⩽(A•B•C)+B
Answer 14.7 The quadrangle inequality reads
A max B+B max C ⩽A max B max C +B
But if B<C <A this simpliﬁes to A+C ⩽A+B, which is false.
Answer 14.8 Monotonicity holds because m ⩽m + mn + n and the quadrangle
inequality
a+ab+b+b+bc+c ⩽a+a(b+bc+c)+b+bc+c+b
simpliﬁes to 0 ⩽a(b+1)c, which also holds.
Answer 14.9 In the case (•) = (+), monotonicity simpliﬁes to A ⩽X +A, and the
remaining conditions simplify to A+X +C ⩽X +A+C. Both conditions hold for
nonnegative numbers. In the case (•) = (×), monotonicity simpliﬁes to A ⩽X A,
which holds, but the remaining conditions simplify to A + X C ⩽X A + C, which
does not hold for all A, X, and C.
Answer 14.10 The proof of
(xs ⟨++⟩ys) ⟨++⟩zs = xs ⟨++⟩(ys ⟨++⟩zs)
is by induction on xs. The base case is easy, and for the induction step we can argue
((x:xs) ⟨++⟩ys) ⟨++⟩zs
=
{ deﬁnition }
(ys++[x]++(xs ⟨++⟩reverse ys)) ⟨++⟩zs
=
{ ﬁrst equation, with rs = rev xs zs }
(ys ⟨++⟩zs)++[x]++((xs ⟨++⟩reverse ys) ⟨++⟩rs)
=
{ induction }
(ys ⟨++⟩zs)++[x]++(xs ⟨++⟩(reverse ys ⟨++⟩rs))
=
{ second equation }
(ys ⟨++⟩zs)++[x]++(xs ⟨++⟩(reverse (ys ⟨++⟩zs)))
=
{ deﬁnition }
(x:xs) ⟨++⟩(ys ⟨++⟩zs)
Answer 14.11 By deﬁnition of r, we have Cq(i,j) ⩾Cr(i,j) for i ⩽q<r, so (14.4)
gives
0 ⩽Cq(i,j)−Cr(i,j) ⩽Cq(i,j+1)−Cr(i,j+1)
and (14.5) gives
0 ⩽Cq(i,j)−Cr(i,j) ⩽Cq(i+1,j)−Cr(i+1,j)

364
Optimum bracketing
Answer 14.12 The deﬁnition is
endstep::[Pair] →Tree Label
endstep [(t, )]
= t
endstep (x:y:xs) = endstep (insert (join x y) xs)
Answer 14.13 The input is a two-sorted list, so the ﬁrst pair is combined at the ﬁrst
step, giving the list
[k +1,k +1,...,2k −1,2k −1,2k]
Again this list is two-sorted. It follows that build takes Θ(k2) steps in the worst case.
Answer 14.14 The cost of the ﬁrst tree is 49, and the cost of the second is 51. We
have
gwa [4,2,4,4,7] = ((4 (2 4)) (4 7))
with cost 48.
Answer 14.15 For example, with input [5,10,6,8,7] the Garsia–Wachs algorithm
produces the tree ((5 10) (7 (6 8))) with cost 17234, while an optimum tree is
(((5 10) 6) (8 7)) with cost 17206.

PART SIX
EXHAUSTIVE SEARCH


367
Sometimes there seems to be no better approach than to examine every possible
candidate in order to ﬁnd one with a particular property or to show that none exists.
That, in essence, is exhaustive search. Many of the algorithms we have met so far
started life as exhaustive search algorithms. By exploiting various monotonicity
conditions they were then transformed into more efﬁcient alternatives – greedy,
thinning, or dynamic-programming algorithms, algorithms whose running times
were typically a low-order polynomial in the size of the input.
However, for many problems, even quite simply stated ones, no algorithm with
a guaranteed polynomial running time is known. For example, there is no known
algorithm for determining the factors of a positive integer that takes polynomial time
in the number of its digits. There is such an algorithm for determining whether an
integer is prime or not, but the method is non-constructive and gives no hint of what
the potential divisors might be. The problems tackled in the remainder of this book
fall into a similar category of ignorance, and the algorithms we will describe all take
greater than polynomial time in the worst case. In fact most will take exponential
time.
The main problem with an exponential-time algorithm is that it severely limits
the sizes of problem instances that can be solved. Take, say, an algorithm whose
running time is Θ(2n) in the worst case. If we can improve the algorithm to one with
a thousand-fold increase in speed, then the size of problem that can be tackled in
the same allotted time increases from n to n+10, while a quadratic-time algorithm
allows an increase in problem size from n to about 30n. That is a big difference.
Even when faced with a potentially exhaustive search, there are still a number of
ways in which to squeeze as much efﬁciency as possible out of the process. One
avenue of attack is to arrange the generation of candidates in such a way that the
transition from one candidate to the next is as fast as possible. It may be possible
to postpone exploration of less likely paths in favour of those paths that some
heuristic deems to be more likely to lead to a solution. The choice of representation
of the candidates can be tuned to minimise the total amount of space required by
an exhaustive search. Finally, low-level implementations of the basic steps can
sometimes be found to make them as fast as possible. Functional languages are at a
disadvantage as far as the last two aspects are concerned because the use of space is
difﬁcult to control in a purely functional setting, and implementations that depend
on low-level memory operations often cannot be described without introducing
procedural features such as mutable arrays into the language.
Most of the problems we will discuss in the following two chapters deal with
games and puzzles of various kinds. Apart from being intriguing and fun to study,
puzzles provide a fertile ground for looking at exhaustive search, if only for the
reason that a good puzzle should be one in which there is no obvious route to a
solution.


Chapter 15
Ways of searching
As every good detective knows, an exhaustive search can be organised in different
ways. The present chapter introduces the two main variations, depth-ﬁrst search and
breadth-ﬁrst search. These different ways of searching are illustrated with the help
of games and puzzles of various kinds. Good detectives also know how to prioritise
certain lines of enquiry, in the hope that they will prove to be more fruitful than
others. One general way of doing so is embodied in heuristic search, a topic we will
consider in the following chapter. Another way is to formulate possible plans for
achieving a given goal and then to try each plan in turn until one is successful. One
example of a planning algorithm is considered in the ﬁnal section of this chapter.
We begin, however, with two examples in which the nature of the search is not made
explicit.
15.1 Implicit search and the n-queens problem
Sometimes the set of candidates can be described directly, so let us start out with
the simple idea of an exhaustive search based on the pattern
solutions = ﬁlter good ·candidates
The function candidates generates a list of possible candidates from some given
data, and the ﬁlter operation extracts those that are ‘good’. No particular search
method is explicit in this formulation because it depends on the precise way in
which the list of possible candidates is generated.
The pattern above returns all the good candidates, but to ﬁnd just one – assuming
of course that one exists – we can use the idiom
solution = head ·solutions
There is no loss of efﬁciency in extracting a single solution by this device because,
under lazy evaluation, only the ﬁrst element of solutions will ever be computed.
This simple idea, known as the list of successes technique, was recognised early

370
Ways of searching
on as a useful aspect of a lazy functional programming language such as Haskell.
However, as we will see below, it does not follow that the work for ﬁnding any
one of n possible solutions takes 1/n of the total time; depending on the precise
deﬁnition of candidates, it may take nearly as much time to ﬁnd the ﬁrst solution as
to ﬁnd all of them.
As a ﬁrst example we will look at a well-known puzzle whose history goes back
over 150 years. The puzzle is to arrange n queens on an n×n chessboard so that no
queen attacks any other. Each queen therefore has to be placed on the board in a
different row, column, and diagonal from any other queen. (Chess players refer to
ranks and ﬁles rather than rows and columns, but we will stick with the standard
matrix nomenclature with rows labelled from top to bottom and columns from left to
right.) The ﬁrst two constraints imply that any solution is necessarily a permutation
of the numbers 1 to n in which the jth element is the number of the column in which
the queen in row j is placed. For example, for the 8-queens problem there are 92
solutions, of which one is 15863724. The queen in the ﬁrst row is in column 1, the
queen in the second row is in column 5, and so on. Each solution is therefore a
permutation of 1 to n in which no queen attacks any other along a diagonal. Thus,
for each queen at position (r,q) there can be no other queen at any position (r′,q′)
for which r + q = r′ + q′ or r −q = r′ −q′. Each left diagonal (top left to bottom
right) is identiﬁed by coordinates with a common difference and each right diagonal
(top right to bottom left) by coordinates with a common sum. The diagonal safety
condition can be implemented by
safe::[Nat] →Bool
safe qs = check (zip [1..] qs)
check [ ]
= True
check ((r,q):rqs) = and [abs (q−q′) ̸= r′ −r | (r′,q′) ←rqs] ∧check rqs
Now we can simply write
queens::Nat →[[Nat]]
queens = ﬁlter safe·perms
where perms n generates all permutations of 1 to n. One efﬁcient deﬁnition of this
function was given in the very ﬁrst chapter:
perms n = foldr (concatMap·inserts) [[ ]] [1..n]
where inserts x [ ]
= [[x]]
inserts x (y:ys) = (x:y:ys):map (y:) (inserts x ys)
With very little effort it seems we have arrived at a reasonable program for solving
the puzzle. However, this is a very bad way of solving the problem. Generating the
permutations of 1 to n takes Θ(n×n!) steps, and each safety test takes Θ(n2) steps,

15.1 Implicit search and the n-queens problem
371
so the full algorithm takes Θ(n2 ×n!) steps because the safety test has to be applied
to n! permutations.
A better idea is to generate only those permutations that can be extended to safe
permutations. The idea is to exploit the following property of safe:
safe (qs++[q]) = safe qs ∧newDiag q qs
where
newDiag q qs = and [abs (q−q′) ̸= r −r′ | (r′,q′) ←zip [1..] qs]
where r = length qs+1
The test newDiag ensures that the next queen is placed on a fresh diagonal. How-
ever, it is difﬁcult to make use of this property with the above deﬁnition of perms
because new elements are inserted into the middle of previously generated partial
permutations. Instead we can use another deﬁnition of perms:
perms n = help n where
help 0 = [[ ]]
help r = [xs++[x] | xs ←help (r −1),x ←[1..n],notElem x xs]
The difference between this deﬁnition and the previous one is that each new element
is added to the end of a previous permutation, not somewhere in the middle. That
means we can fuse part of ﬁlter safe into the generation of permutations to arrive at
queens1 n = help n where
help 0 = [[ ]]
help r = [qs++[q] | qs ←help (r −1),q ←[1..n],
notElem q qs,newDiag1 (r,q) qs]
newDiag1 (r,q) qs = and [abs (q−q′) ̸= r −r′ | (r′,q′) ←zip [1..] qs]
The safety of previously placed queens is guaranteed by construction. The test
newDiag1 takes only Θ(n) steps and the resulting search is faster by a factor of n.
There is a dual solution in which the order of the generators is swapped and new
elements are added to the front of a previous permutation rather than to the rear:
queens2 n = help n where
help 0 = [[ ]]
help r = [q:qs | q ←[1..n],qs ←qss,notElem q qs,newDiag2 q qs]
where qss = help (r −1)
newDiag2 q qs = and [abs (q−q′) ̸= r′ −1 | (r′,q′) ←zip [2..] qs]
The computation of help (r −1) is brought out in a where clause, for otherwise it
would be recomputed for each possible placement. A revised version of newDiag is
needed because queens are now added to the front of a list.
The two functions queens1 and queens2 generate exactly the same solutions in
exactly the same order, so which is better? It might be thought that queens2 should

372
Ways of searching
be faster than queens1, if only because the operation of adding a queen to the front
of a list is a constant-time operation, while adding a queen to the rear takes linear
time. Indeed, the second version is faster when all the solutions are computed. But
the situation changes dramatically when we want just the ﬁrst solution. For example,
computing the ﬁrst element of queens2 9 is much slower than computing the ﬁrst
element of queens1 9. To see why, consider the ﬁrst solution 136824975 (out of 352
possible solutions). This solution is computed from left to right by queens1, and the
ﬁrst elements of the partial permutations are generated as follows:
1, 13, 135, 1352, 13524, 135249, 1357246, 13682497, 136824975
Most of the work takes place in generating the seventh and eighth partial permutation,
since 135249 cannot be extended on the right to a solution and neither can 1357246.
In each case that means more partial permutations have to be generated before
ﬁnding one that can be extended. The remaining partial permutations can be easily
extended and so require far less work.
Contrast this effort with that required by queens2, which computes from right to
left and generates exactly the same list of partial permutations. This time much more
work is done at each step in order to ﬁnd the next partial permutation that begins
with a 1. For example, the partial permutation 13524 has to be replaced by 35249 in
order to allow a 1 to be added to the front, and this involves generating about 400
intermediate permutations. The same phenomenon is exhibited at each step of the
process, causing queens2 to perform a lot more work than queens1 before it returns
the ﬁrst solution. Of course, it performs correspondingly less work in computing the
remaining solutions. The lesson of the story is that the order in which the choices
for the next move are made can signiﬁcantly inﬂuence the running time for ﬁnding
the ﬁrst solution.
Here is another solution to the n-queens problem, one in which the search strategy
is made explicit. The general idea, which we will revisit later when we discuss
depth-ﬁrst and breadth-ﬁrst search, is to reformulate the search in terms of two ﬁnite
sets, a set of states and a set of moves, and three functions
moves ::State →[Move]
move ::State →Move →State
solved ::State →Bool
The function moves determines the legal moves that can be made in a given state,
and move returns the state that results when a given move is made. The function
solved determines which states are a solution to the puzzle. Phrased in this way,
the problem is essentially one of searching a directed graph in which the vertices
represent states and the edges represent moves.
The following algorithm for listing the set of solved states works only under
certain assumptions, given below:

15.1 Implicit search and the n-queens problem
373
solutions::State →[State]
solutions t = search [t]
search::[State] →[State]
search [ ]
= [ ]
search (t :ts) = if solved t then t :search ts else search (succs t ++ts)
succs::State →[State]
succs t = [move t m | m ←moves t]
In words, if the current state is not a solved state, then its successors are added to
the front of the states waiting to be explored. This way of dealing with successor
states is typical of depth-ﬁrst search. Regarding the assumptions, the major one is
that the underlying graph is acyclic, for otherwise search would loop indeﬁnitely if
any state is repeated. The second assumption is that no further moves are possible
in any solved state, for otherwise some solved states would be missed, and the third
is that no state can be reached by more than one path, for otherwise some solved
states would be listed more than once.
These three assumptions are all satisﬁed in the n-queens problem, and we can
immediately install the deﬁnitions
type State = [Nat]
type Move = Nat
moves::State →[Move]
moves qs
= [q | q ←[1..n],notElem q qs,newDiag2 q qs]
move::State →Move →State
move qs q = q:qs
solved ::State →Bool
solved qs = (length qs == n)
The function newDiag2 was deﬁned above. A state is solved if it is a full permutation
of 1..n, and the set of moves consists of the legal positions at which the next queen
can be placed. The resulting algorithm is as fast as queens1 in ﬁnding the ﬁrst
solution.
The deﬁnition of search can be modiﬁed to count only the number of solutions.
Counting the number of solutions to the n-queens problem is a time-consuming
operation. For example, in an experiment in 2006 it took 26613 days of CPU time
to count the number of solutions to the 25-queens problem, which turns out to be
2207893435808352. No-one currently knows how many solutions there are when
n = 28. Nevertheless, we can try to put on speed with the algorithm above by using
a more compact representation of states. We will describe a representation that
uses three bit vectors. The three vectors determine which left diagonals, columns,
and right diagonals cannot be used for the next queen. For example, consider the

374
Ways of searching
5-queens problem, and suppose the last two rows have been ﬁlled in as follows,
with row 3 waiting to be ﬁlled:
·
·
·
·
·
·
Q
·
·
·
·
·
·
Q
·
The three vectors for this state are 11000, 01010, and 00100. The ﬁrst, 11000,
determines which left-to-right diagonals are attacked by a queen. We cannot place
a queen in either column 1 or column 2 because it would be under attack by an
existing queen along a left-to-right diagonal. The middle vector, 01010, determines
which columns are attacked, and the third, 00100 determines which right-to-left
diagonals are under attack. The columns that can be used for the next row are
calculated by taking the complement of the bitwise union of these three sequences:
complement (11000 .|. 01010 .|. 00100) = 00001
The bitwise union operator .|. and the complement function are taken from the
Haskell library Data.Bits, as are some further operations described below. The
result 00001 means we can place a queen only in column 5.
As another example, consider the possibilities for placing a queen in row 4 when
row 5 has a queen in column 4. Here the three relevant vectors are 00100, 00010,
and 00001. We have
complement (00100 .|. 00010 .|. 00001) = 11000
so a queen in row 4 can be placed only in columns 1 and 2. Suppose we choose
column 2 (as in the ﬁrst example), a choice which is represented by the bit vector
01000. We can then update the diagonal and column information by
shiftL (00100 .|. 01000) 1 = 11000
00010 .|. 01000
= 01010
shiftR (00001 .|. 01000) 1 = 00100
These three vectors appeared in the ﬁrst example. The operation shiftL shifts a bit
vector a designated number of places to the left, introducing trailing 0s. Similarly,
shiftR shifts a bit vector a designated number of places to the right, introducing
leading 0s. In each of the computations above the shift is by just one place. A state
is solved when all the bits in the column vector are 1.
Haskell provides a number of sizes for bit vectors in the Data.Word library,
including Word8, Word16, Word32, and Word64, each of which is a n-bit unsigned
integer type for n = 8,16, and so on. We will choose Word16, which will allow us
to solve the n-queens problem for n ⩽16. For n<16 we can use a mask to mask
out bits. For example, for n = 5 the mask would be a 16-bit vector all of whose bits
are 0 except for the last ﬁve bits, which are all 1. Numerically, the mask is a bit
representation of 2n −1 for 0 ⩽n ⩽16, so we can deﬁne

15.1 Implicit search and the n-queens problem
375
mask ::Word16
mask = 2n −1
Recomputing mask at every point would affect the efﬁciency of the search, so we
make it local to the complete counting algorithm:
type State = (Word16,Word16,Word16)
type Move = Word16
cqueens::Nat →Integer
cqueens n = search [(0,0,0)] where
search::[State] →Integer
search [ ]
= 0
search (t :ts) = if solved t then 1+search ts else search (succs t ++ts)
solved ::State →Bool
solved ( ,cls, ) = (cls == mask)
mask ::Word16
mask = 2n −1
succs::State →[State]
succs t = [move t b | b ←moves t]
move::State →Move →State
move (lds,cls,rds) m = (shiftL (lds .|. m) 1,cls .|. m,shiftR (rds .|. m) 1)
moves::State →[Move]
moves (lds,cls,rds) = bits (complement (lds .|. cls .|. rds) .&. mask)
The function bits extracts the bits from a vector as a sequence of bit vectors each
containing a single set bit:
bits::Word16 →[Move]
bits v = if v == 0 then [ ] else b:bits (v−b)
where b = v .&. negate v
See the exercises for an alternative, slightly less efﬁcient deﬁnition. For example,
bits 11010 = [00010,01000,10000]
The expression v .&. negate v, where .&. is bitwise conjunction, returns the least
signiﬁcant bit; for example
11010 .&. negate 11010 = 11010 .&. 00110 = 00010
Repeatedly subtracting the least signiﬁcant bit from the vector yields all the bits.
When the counting algorithm was compiled and run, it delivered the fact that the
16-queens problem has 14772512 solutions in close to a minute of CPU time.

376
Ways of searching
15.2 Expressions with a given sum
Here is a different kind of puzzle that can also be solved using the direct approach.
The problem involves constructing arithmetic expressions that evaluate to a given
sum. A simple version of the problem asks for a list of all the ways the operators ×
and + can be inserted into a list of digits 1 to 9 so as to make a total of 100. Two
such ways are
100 = 12+34+5×6+7+8+9
100 = 1+2×3+4+5+67+8+9
In this particular version of the problem, no parentheses are allowed in forming
expressions and, as usual, × binds more tightly than +. Here we can write
solutions::Nat →[Digit] →[Expr]
solutions n = ﬁlter (good n·value)·expressions
where expressions builds a list of all arithmetic expressions that can be formed from
a given list of digits, value delivers the value of such an expression, and good tests
whether the value is equal to a given target value.
Let’s consider expressions ﬁrst. Each expression is the sum of a list of terms,
each term is the product of a list of factors, and each factor is a nonempty list of
digits. For example, the expression
12+34+5×6+7+8+9
can be represented by the compound list
[[[1,2]], [[3,4]], [[5],[6]], [[7]], [[8]], [[9]]]
That means we can deﬁne expressions, terms, and factors just with the help of
suitable type synonyms:
type Expr
= [Term]
type Term
= [Factor]
type Factor = [Digit]
type Digit
= Nat
One simple way to deﬁne expressions follows the earlier deﬁnition of perms:
expressions::[Digit] →[Expr]
expressions = foldr (concatMap·glue) [[ ]]
glue::Digit →Expr →[Expr]
glue d [ ]
= [[[[d]]]]
glue d ((ds:fs):ts) = [((d :ds):fs):ts,([d]:ds:fs):ts,[[d]]:(ds:fs):ts]
To explain glue, observe that only one expression can be built from a single digit d,
namely [[[d]]]. An expression built from more than one digit can be decomposed
into a leading factor, ds say, which is part of a leading term ds:fs, and a remaining

15.2 Expressions with a given sum
377
expression, a list of terms ts. A new digit can be added to the front of an expression
in exactly three ways: by extending the leading factor with the new digit, by starting
a new factor, or by starting a new term. For example, 2×3+··· can be extended on
the left with a new digit 1 in one of the following three ways:
12×3+···
1×2×3+···
1+2×3+···
It is immediate from this deﬁnition that there are 6561 = 38 expressions one can
build from nine digits, indeed 3n−1 expressions from a list of n digits.
The function value can be implemented as a function valExpr, where
valExpr ::Expr →Nat
valExpr = sum·map valTerm
valTerm::Term →Nat
valTerm = product ·map valFact
valFact ::Factor →Nat
valFact = foldl op 0 where op n d = 10n+d
Finally, a good expression is one whose value is equal to the target value:
good ::Nat →Nat →Bool
good n v = (v == n)
Evaluating solutions 100 [1..9], and displaying the results in a suitable fashion,
yields the seven solutions
100 = 1×2×3+4+5+6+7+8×9
100 = 1+2+3+4+5+6+7+8×9
100 = 1×2×3×4+5+6+7×8+9
100 = 12+3×4+5+6+7×8+9
100 = 1+2×3+4+5+67+8+9
100 = 1×2+34+5+6×7+8+9
100 = 12+34+5×6+7+8+9
The computation does not take too long, as there are only 6561 possibilities to check.
However, on another day the target value may be much larger and there may be
many more digits, so let us see what we can do to optimise the search.
One obvious step is to memoise value computations to save recomputing values
from scratch each time. Better still, we can exploit a monotonicity condition to
achieve a partial fusion of the ﬁlter test into the generation of expressions. The
situation is exactly the same as with the n-queens problem. The key insight is that
expressions built out of positive digits, using just juxtaposition, ×, and +, have
values that are as least as large as their constituent expressions. A formal statement is

378
Ways of searching
given as an exercise. So we can pair expressions with their values and only generate
expressions whose values are at most the target value.
A technical difﬁculty is that we cannot determine the values of a new expression,
obtained by gluing a new digit to the front, from the values of the digit and expression
alone; we need the values of the leading factor and the leading term as well. So we
will deﬁne the component values to be
type Values = (Nat,Nat,Nat,Nat)
values::Expr →Values
values ((ds:fs):ts) = (10 ˆ length ds,valFact ds,valTerm fs,valExpr ts)
The additional ﬁrst component of this quadruple is included simply to make the
evaluation of valFact more efﬁcient. The value of an expression whose component
values are (p,f,t,e) is f ×t +e.
Here is the revised deﬁnition of solutions:
solutions::Nat →[Digit] →[Expr]
solutions n = map fst ·ﬁlter (good n)·expressions n
The function expressions n generates expressions whose value is at most n:
expressions::Nat →[Digit] →[(Expr,Values)]
expressions n = foldr (concatMap·glue) [([ ],⊥)]
where glue d = ﬁlter (ok n)·extend d
extend d ([ ], ) = [([[[d]]],(10,d,1,0))]
extend d ((ds:fs):ts,(p,f,t,e)) = [(((d :ds):fs):ts, (10×p,p×d +f,t,e)),
(([d]:ds:fs):ts,
(10,d,f ×t,e)),
([[d]]:(ds:fs):ts,(10,d,1,f ×t +e))]
Finally, the tests good and ok are deﬁned by
good n (ex,(p,f,t,e)) = (f ×t +e == n)
ok n
(ex,(p,f,t,e)) = (f ×t +e ⩽n)
The result is a program for solutions that is many times faster than the ﬁrst version.
15.3 Depth-ﬁrst and breadth-ﬁrst search
In Section 15.1 we implemented a simple version of depth-ﬁrst search that used
three functions:
moves ::State →[Move]
move ::State →Move →State
solved ::State →Bool
The search, which produced a list of all the solved states, was valid provided three
assumptions were satisﬁed, the main one being that the underlying digraph was

15.3 Depth-ﬁrst and breadth-ﬁrst search
379
acyclic. However, in many applications this assumption does not hold. It is perfectly
possible that some sequence of moves can lead to a state being repeated, so the
associated digraph will contain cycles. We will assume, though, that move t m ̸= t
for all states t and moves m, so the graph does not contain loops. The second
assumption, namely that ﬁnal states cannot be arrived at by more than one sequence
of moves, is not needed if we want to enumerate all the sequences of moves that
lead to solved states rather than the solved states themselves. The third assumption
was that no further moves are possible in a solved state, a reasonable restriction we
will continue to assume.
Let us therefore consider how to implement a function
solutions::State →[[Move]]
for computing all simple sequences of moves that lead to a solved state. A sequence
of moves is simple if no intermediate state is repeated during the moves. Without
this restriction the set of solutions could be inﬁnite. To maintain the restriction, we
need to remember both the sequence of moves in a path and the list of intermediate
states, including the initial state, that arises as a result of making the moves. Hence
we deﬁne
type Path = ([Move],[State])
where the second component of a path is a nonempty list of states. The simple
successors of a path are deﬁned by
succs::Path →[Path]
succs (ms,t :ts) = [ (ms++[m],t′ :t :ts)
| m ←moves t,let t′ = move t m,notElem t′ ts]
The intermediate states in a path are recorded from right to left. That means a path
leads to a ﬁnal state deﬁned by
ﬁnal::Path →State
ﬁnal = head ·snd
Next, the function paths takes a list of simple paths and produces all the possible
completions. Here are two ways to deﬁne paths:
paths1 ::[Path] →[Path]
paths1 = concat ·takeWhile (not ·null)·iterate (concatMap succs)
paths2 ::[Path] →[Path]
paths2 ps = concat [p:paths2 (succs p) | p ←ps]
In paths1 the list of paths is repeatedly extended by applying succs until no more
extensions are possible. Under this deﬁnition, the simple paths are generated in
ascending order of length. In paths2 each path is followed immediately by its

380
Ways of searching
successors, so paths are not necessarily produced in ascending order of length. We
will rewrite these two deﬁnitions in a moment. Now we can deﬁne solutions1 by
solutions1 ::State →[[Move]]
solutions1 = map fst ·ﬁlter (solved ·ﬁnal)·paths1 ·start
The initial state is converted into a singleton containing the empty path:
start ::State →[Path]
start t = [([ ],[t])]
The function paths1 enumerates all simple paths, and the result is ﬁltered for those
paths that lead to a solved state, which are then processed to produce the moves.
The deﬁnition of solutions2 is the same but with paths1 replaced by paths2.
The two deﬁnitions of paths can be rewritten with the help of a little calculation.
First consider the expression
exp = foldr f e·takeWhile p·iterate g
An easy calculation, left as an exercise, leads to the equivalent recursive deﬁnition
exp x = if p x then f x (exp (g x)) else e
Hence paths1 can be put in the form
paths1 ps = if null ps then [ ] else ps++paths1 (concatMap succs ps)
We can now show that
paths1 (ps++qs) = ps++paths1 (qs++concatMap succs ps)
for all ps and qs. The proof is by induction on ps. The base case is immediate, and
for the induction step we argue
paths1 (p:ps++qs)
=
{ deﬁnition of paths1 }
p:ps++qs++paths1 (concatMap succs (p:ps++qs))
=
{ deﬁnition of concatMap }
p:ps++qs++paths1 (succs p++concatMap succs (ps++qs))
=
{ introducing ps′ = ps++qs and qs′ = succs p }
p:ps′ ++paths1 (qs′ ++concatMap succs ps′)
=
{ induction, expanding the abbreviation }
p:paths1 (ps++qs++succs p)
=
{ introducing qs′′ = qs++succs p }
p:paths1 (ps++qs′′)
=
{ induction again, expanding the abbreviation }
p:ps++paths1 (qs++succs p++concatMap succs ps)
=
{ deﬁnition of concatMap }
p:ps++paths1 (qs++concatMap succs (p:ps))

15.3 Depth-ﬁrst and breadth-ﬁrst search
381
This completes the proof. In particular, setting (ps,qs) = ([ ],ps) we obtain
paths1 (p:ps) = p:paths1 (ps++succs p)
That means solutions1 can be rewritten in the form
solutions1 = search·start where
search [ ]
= [ ]
search ((ms,t :ts):ps)
| solved t
= ms:search ps
| otherwise = search (ps++succs (ms,t :ts))
The only assumption is that no moves are possible in solved states. This method is
known as breadth-ﬁrst search (BFS). In BFS, the frontier – the list of paths waiting
to be explored further – is maintained as a queue, with new entries added to the
rear of the queue. What the above calculation demonstrates is that BFS does indeed
produce solutions in ascending order of length of path.
Turning to paths2, we can reason
paths2 (p:ps)
=
{ deﬁnition of paths2 }
concat [p′ :paths2 (succs p′) | p′ ⩽p:ps]
=
{ deﬁnition of concat and paths2 }
p:paths2 (succs p)++paths2 ps
=
{ since concat (xss++yss) = concat xss++concat yss }
p:paths2 (succs p++ps)
Hence we obtain the following alternative deﬁnition of solutions2:
solutions2 = search·start where
search [ ]
= [ ]
search ((ms,t :ts):ps)
| solved t
= ms:search ps
| otherwise = search (succs (ms,t :ts)++ps)
This method is known as depth-ﬁrst search (DFS). This time, the frontier is managed
as a stack, with new entries added to the front of the stack. With DFS, the solutions
are not produced in ascending order of length, though all solutions will still be
produced. These two deﬁnitions of solutions are not quite the usual ways in which
DFS and BFS are described (see below), but it is instructive that both can be derived
from clear speciﬁcations.
The point about BFS producing solutions in order of length would seem to tip the
scales in favour of solutions1. But there is a downside: under BFS, the frontier can
be exponentially longer than under DFS. Suppose each state has K successors, and
the ﬁrst solved state occurs at level n, meaning there is a sequence of n moves that
leads to a solved state. Under DFS the frontier will increase in size by K at each step,

382
Ways of searching
so the ﬁnal frontier will have length K n. Under BFS, all the successors of all the
states at a distance of at most n away from the solved state will be queued up in the
frontier, so the frontier has a length of Kn. Consequently, BFS can use exponentially
more space than DFS. Worse, as deﬁned above it can also take exponentially longer
time, because computing (ps++succs p) takes time proportional to the length of ps.
One way to make the algorithm faster, though it will not reduce the space com-
plexity, is to use a dedicated Queue data type to ensure that adding elements to the
rear is a constant-time operation. Another alternative is to introduce an accumulating
parameter, deﬁning search1 by
search1 pss ps = search (ps++concat (reverse pss))
Then, after some simple calculation which we will again leave as an exercise, we
arrive at
solutions1 = search [ ]·start where
search [ ] [ ] = [ ]
search pss [ ] = search [ ] (concat (reverse pss))
search pss ((ms,t :ts):ps)
| solved t
= ms:search pss ps
| otherwise = search (succs (ms,t :ts):pss) ps
In fact, there is another version of search in which the accumulating parameter is a
list of paths rather than a list of lists of paths:
search qs [ ]
= if null qs then [ ] else search [ ] qs
search qs ((ms,t :ts):ps)
| solved t
= ms:search qs ps
| otherwise = search (succs (ms,t :ts)++qs) ps
This version has a different behaviour from the previous one, in that successive
frontiers are traversed alternately from left to right and from right to left, but the
solutions will still be produced in ascending order of length.
Each of the search functions considered above produces all the solutions. If just
one solution is required, then there is a further space-saving idea. The problem with
each of the previous searches is that a list of the intermediate states has to be kept
as part of each path in order to ensure that each path is a simple one, and this adds
signiﬁcantly to the total space required. By moving the membership test to the top
level, we can guarantee not only that each path is simple, but also that only one path
to a given state is maintained.
Here are the details. A path now consists of a sequence of moves and the ﬁnal
state that results, so the deﬁnition of succs has to be changed to read
succs (ms,t) = [(ms++[m],move t m) | m ←moves t]
Now we can deﬁne

15.4 Lunar Landing
383
solution1 ::State →Maybe [Move]
solution1 t = search [ ] [([ ],t)]
where search ts [ ]
= Nothing
search ts ((ms,t):ps)
| solved t
= Just ms
| elem t ts
= search ts ps
| otherwise = search (t :ts) (ps++succs (ms,t))
The ﬁrst argument to search is a list of visited states, states whose successors have
already been added to the frontier. Using a list means that the membership test can
take linear time. As an alternative we can make use of the efﬁcient set operations of
Section 4.4. The Haskell library Data.Set also provides the necessary operations, so
we can import it
import Data.Set (empty,insert,member)
and deﬁne
solution1 ::State →Maybe [Move]
solution1 t = search empty [([ ],t)]
where search ts [ ]
= Nothing
search ts ((ms,t):ps)
| solved t
= Just ms
| member t ts = search ts ps
| otherwise
= search (insert t ts) (ps++succs (ms,t))
This version of solution1 guarantees that member and insert operations both take
logarithmic time. This method of searching is what is usually given as the deﬁnition
of BFS. The companion function
solution2 ::State →Maybe [Move]
solution2 t = search empty [([ ],t)]
where search ts [ ]
= Nothing
search ts ((ms,t):ps)
| solved t
= Just ms
| member t ts = search ts ps
| otherwise
= search (insert t ts) (succs (ms,t)++ps)
is what is usually given as the deﬁnition of DFS. Neither function is suitable for
producing all solutions, but they will certainly produce one solution if one exists.
15.4 Lunar Landing
Let us now see how DFS and BFS can be put to work in solving another puzzle. This
one is called Lunar Landing (it is also known as Lunar Lockout) and is an addictive

384
Ways of searching
solitaire game invented by Hiroshi Yamamoto and publicised by Nob Yoshigahara,
the famous Japanese inventor of Rush Hour, a puzzle we will consider later on.
Although it can be played on boards of different shapes and sizes, the standard is a
5×5 square of cells, of which the centre cell is designated as an escape hatch. On
the board is a human astronaut and a number of bots, each occupying a single cell.
The aim of the game is to get the astronaut safely into the escape hatch. Both the
astronaut and the bots can move only horizontally or vertically. The catch is that
beyond the boundary of the board lies inﬁnite space, and no bot or human ever wants
to go there. Consequently, each move involves moving a piece as far as possible in
a straight line until it comes to rest next to another piece which is blocking the path
into inﬁnite space. The aim is to ﬁnd a sequence of moves that enables the astronaut
to land exactly on the escape hatch.
Here is an example board, in which the astronaut is piece number 0, there are ﬁve
bots numbered from 1 to 5, and the escape hatch is marked with a ×:
·
·
1
·
·
·
·
·
·
2
3
·
×
·
·
·
·
·
4
·
5
0
·
·
·
In this position only bots 3 and 5 can move; the astronaut and the remaining bots
would shoot off into inﬁnite space if moved. Bot 3 can move downwards one cell
and bot 5 upwards one cell. The longest sequence of moves involving bot 3 alone
is 3D 3R 3U 3R 3D. In words, bot 3 can move down, right, up, right, and down
until it ends up just above bot 4. Bot 5, on the other hand, can engage in an inﬁnite
sequence of moves. The two sequences of moves
5U 5R
5U 5R 5U 5R 5D 5L 5D 5R
both result in the same ﬁnal position in which bot 5 ends up to the left of bot 4. The
last six moves of the second sequence can be repeated ad inﬁnitum. Nevertheless,
there is a unique nine-move solution to the puzzle. Pause for a moment to see if you
can ﬁnd it.
The answer is the nine moves
5U 5R 5U 2L 2D 2L 0U 0R 0U
Bot 5 in three moves ends up below bot 1, then bot 2 in three moves ends up to the
right of bot 3, and ﬁnally the astronaut can escape in three more moves.

15.4 Lunar Landing
385
There is another solution involving 12 moves but only two pieces:
5U 5R 5U 5R 5D 5L 0U 0R 0U 0R 0D 0L
Notice that in this solution the astronaut passes over the escape hatch during her
third move, but only lands on it at the ﬁnal move. There are example boards for
which there are an inﬁnite number of solutions, so the associated digraph is cyclic.
The ﬁrst decision concerns how to represent the board. The obvious method is to
use Cartesian coordinates, but a more compact representation is to number the cells
as follows:
1
2
3
4
5
7
8
9
10
11
13
14
15
16
17
19
20
21
22
23
25
26
27
28
29
Cells that are a multiple of 6 represent the left and right borders, which will help in
determining the moves. The escape hatch is cell 15. A board is a list of occupied
cells with the ﬁrst cell, at position 0, naming the location of the astronaut. For
example, the board above is represented by the list [26,3,11,13,22,25]. Hence we
deﬁne
type Cell
= Nat
type Board = [Cell]
solved ::Board →Bool
solved b = (b!!0 == 15)
The next decision is how to represent moves. Rather than take a move to be a
named piece and a direction, we will represent a move by a named piece, its current
position, and the ﬁnishing point of the move:
type Name = Nat
type Move = (Name,Cell,Cell)
A move can be recast in terms of directions by showMove, where
showMove::Move →String
showMove (n,s,f) = show n++dir (s,f)
dir (s,f) = if abs (s−f) ⩾6 then (if s<f then "D" else "U")
else (if s<f then "R" else "L")
The function move is deﬁned by
move::Board →Move →Board
move b (n,s,f) = b1 ++f :b2 where (b1,
:b2) = splitAt n b
It remains only to deﬁne moves, which takes the form

386
Ways of searching
moves::Board →[Move]
moves b = [(n,s,f) | (n,s) ←zip [0..] b,f ←targets b s]
The function targets, which determines the destination cells of moves, is deﬁned in
terms of the four possible paths for moving a piece:
targets::Board →Cell →[Cell]
targets b c = concatMap try [ups c,downs c,lefts c,rights c]
where try cs | null ys
= [ ]
| null xs
= [ ]
| otherwise = [last xs]
where (xs,ys) = span (/∈b) cs
ups c
= [c−6,c−12..1]
downs c = [c+6,c+12..29]
lefts c
= [c−1,c−2..c−c mod 6+1]
rights c = [c+1,c+2..c−c mod 6+5]
Each of the various directions is examined in turn to see if there is a blocking piece
along the path. If there is, the cell adjacent to the blocker is a possible target for a
move. Putting these functions together, we can compute all the simple solutions for
a given board by
safeLandings = map (map showMove)·solutions
where solutions is, say, the breadth-ﬁrst version deﬁned in the previous section.
When solutions was run on the example board, it produced 25 simple solutions, of
which the ﬁrst two were those described above.
15.5 Forward planning
With both DFS and BFS we have basically the strategy of trying sequences of
random moves until ﬁnding one that works. For some games and puzzles, indeed
for some real-life problems, it is possible to improve on random search by suitable
forward planning. The subject of planning algorithms is a broad one, and we will
consider only one very simple situation. Suppose it is known that a certain sequence
ms of moves is sufﬁcient to take the starting state into a goal state. Such a sequence
of moves constitutes the game plan. Now, it may or may not be the case that the ﬁrst
move m in ms is a valid move in the starting state. If it is, then move m is made and
the algorithm carries on with the rest of the plan. If not, then it may be possible to
ﬁnd one or more lists of preparatory moves, each of which – provided they can be
carried out – leads to a state in which move m is a legal move. After making these
preparatory moves, the move m can then be made, in which case the rest of the plan
is carried out as before. However, some of these preparatory moves may, in turn,

15.5 Forward planning
387
require further preparatory moves to be made, so the planning process may have to
be repeated. In the case that no preparatory moves can be found for a given move
to be valid, a random move is made instead. It is because of this last possibility
that a planning algorithm should be thought of as an extension of, rather than an
alternative to, depth-ﬁrst or breadth-ﬁrst search.
Here is an example. Suppose you wanted to move a grand piano to an upstairs
room. One sensible game plan is ﬁrst to move the piano into the hallway, then to
lift the piano up the stairs, and ﬁnally to move the piano into the required room.
The ﬁrst step may not be possible because (i) the pathway to the door is blocked
by a chair, and (ii) the piano will not go through the door without removing its
legs. In such a case the preparatory moves would consist of, in either order, moving
the chair and removing the legs of the piano. The ﬁrst task, say moving the chair,
might be possible, but the second task would ﬁrst involve obtaining a suitably large
screwdriver for unscrewing the legs. Once the task of moving the piano into the
hallway is accomplished, the next step, lifting the piano up the stairs, may not be
possible without calling on the help of a number of friends to assist in the lifting.
And so on.
Here are the details. Abstractly, a plan is a sequence of moves:
type Plan = [Move]
The game plan is provided by a function
gameplan::State →Plan
The problem is solved in a given starting state by making the moves in gameplan.
An empty plan means success. Otherwise, if the ﬁrst move in the current plan can
be carried out, then the move is made and the plan proceeds with the remaining
moves. If it cannot, then we make use of a function
premoves::State →Move →[Plan]
for formulating additional plans. Given a state and a move, each alternative plan in
premoves should enable the move to be made, provided the moves in the plan are
executed ﬁrst. The ﬁrst move in each plan returned by premoves may in turn require
further preparatory moves to be made, so we have to form new plans by iterating
premoves:
newplans::State →Plan →[Plan]
newplans t [ ]
= [ ]
newplans t (m:ms) = if elem m (moves t) then [m:ms] else
concat [ newplans t (pms++m:ms)
| pms ←premoves t m,all (/∈ms) pms]
The result of newplans is a possibly empty list of nonempty but ﬁnite plans, the ﬁrst
move of which can be made in a given state. Plans cannot contain repeated moves.

388
Ways of searching
If, in order to make a certain move, a plan requires that move to be made ﬁrst, then
clearly the plan is cyclic and cannot be implemented.
Using just the two new functions newplans and gameplan, we can now formulate
a search based on an extended type of path and frontier:
type Path
= ([Move],State,Plan)
type Frontier = [Path]
This time, a path consists of the moves already made, the current state, and a plan
for the remaining moves. We can deﬁne the planning algorithm to have the same
structure as the time-efﬁcient version of breadth-ﬁrst search considered above:
psolve::State →Maybe [Move]
psolve t = psearch [ ] [ ] [([ ],t,gameplan t)] where
psearch::[State] →Frontier →Frontier →Maybe [Move]
psearch ts [ ] [ ] = Nothing
psearch ts qs [ ] = psearch ts [ ] qs
psearch ts qs ((ms,t,plan):ps)
| solved t
= Just ms
| elem t ts
= psearch ts qs ps
| otherwise = psearch (t :ts) (bsuccs (ms,t,plan)++qs)
(asuccs (ms,t,plan)++ps)
In psearch, all plans in the main frontier are tried ﬁrst in a depth-ﬁrst manner until
one of them succeeds or all fail. The function asuccs is deﬁned by
asuccs::Path →[Path]
asuccs (ms,t,plan) = [(ms++[m],move t m,p) | m:p ←newplans t plan]
In particular, if elem m (moves t), then
asuccs (ms,t,m:plan) = [(ms++[m],move t m,plan)]
If all plans fail, we can make some legal move at random and start again with a new
game plan. The function bsuccs is deﬁned by
bsuccs::Path →[Path]
bsuccs (ms,t, ) = [ (ms++[m],t′,gameplan t′)
| m ←moves t,let t′ = move t m]
Such additional plans are necessary for completeness: plans may fail even though
there is a solution. This is a consequence of the fact that plans are executed greedily
and moves that can be made are made. Note that, if newplans returns the empty list,
so does asuccs. In such a case, psolve reduces to simple breadth-ﬁrst search.

15.6 Rush Hour
389
Figure 15.1 A simple Rush Hour grid
15.6 Rush Hour
Let us now see how forward planning can help with another sliding-block puzzle.
This one is called Rush Hour, and is played on a 6×6 grid. Covering some of the
cells of the grid are cars and trucks, which are placed either horizontally or vertically.
Cars occupy two cells, while trucks occupy three. Horizontal vehicles can move left
or right and vertical vehicles up or down, provided their path is not obstructed by
another vehicle. One ﬁxed cell, three places down along the right-hand side of the
grid, is special, and is called the exit cell. One vehicle is also special. It is horizontal
and occupies cells to the left of the exit cell. The object of the game is simply to
move the special car to the exit cell.
A very simple starting grid, reminiscent of a real car-park situation, is pictured
in Figure 15.1. Down the middle of the grid is a line of cars, the fourth of which
has moved one place forwards. The special car, the third one, cannot exit the car
park because its path is impeded by a vertical truck. To enable the special car to exit,
the truck has to move two places down (which counts as two moves), which in turn
requires the fourth car to move back into line (one move). The puzzle therefore has a
fairly obvious ﬁve-move solution (the special car takes two moves to get to the exit).
There are nine possible starting moves on the grid – the ﬁrst car can move one step
left or right, the second car one step left, and so on – and breadth-ﬁrst search could
involve examining about 95 moves before ﬁnding the shortest ﬁve-move solution.
Simple planning, on the other hand, leads at once to the answer. Of course, most
starting grids that come with the puzzle are considerably more difﬁcult: there are
starting grids that take 93 moves to solve! Furthermore, as we will see, planning is
not guaranteed to ﬁnd a shortest solution.

390
Ways of searching
There are various ways to represent a grid, but we will take essentially the same
approach as in Lunar Landing and number the cells as follows:
1
2
3
4
5
6
8
9
10
11
12
13
15
16
17
18
19
20
22
23
24
25
26
27
29
30
31
32
33
34
36
37
38
39
40
41
The left and right borders are cells divisible by 7; the top border consists of cells
with negative numbers and the bottom border consists of cells with numbers greater
than 42. The exit cell is cell 20. A grid state can be deﬁned as a list of pairs of
cells, with each pair (r,f) satisfying r <f and representing the rear and front cells
occupied by a single vehicle. The vehicles in the grid are named implicitly by their
positions in the list, with the special vehicle being vehicle 0, so the ﬁrst pair of
numbers in the grid represents the cells occupied by vehicle 0, the second pair
vehicle 1, and so on. For example, the grid of Figure 15.1 is represented by
[(17,18), (3,4), (10,11), (12,26), (32,33), (38,39)]
This representation is captured by introducing the type synonyms
type Cell
= Nat
type Vehicle = (Cell,Cell)
type Grid
= [Vehicle]
The list of occupied cells in the grid can be constructed in increasing order by ﬁlling
in the intervals associated with each vehicle and merging the results:
occupied ::Grid →[Cell]
occupied = foldr merge [ ]·map ﬁll
ﬁll::Vehicle →[Cell]
ﬁll (r,f) = if horizontal (r,f) then [r..f ] else [r,r +7..f ]
horizontal::Vehicle →Bool
horizontal (r,f) = f −r <6
The next decision concerns the representation of moves. A simple representation is
to say that a move consists of a vehicle’s name and the target cell:
type Name = Nat
type Move = (Name,Cell)
For example, if a car occupies the cells (24,25) then the possible target cells are 23
and 26. The valid moves are deﬁned by
moves::Grid →[Move]
moves g = [(n,c) | (n,v) ←zip [0..] g,c ←steps v,notElem c (occupied g)]

15.6 Rush Hour
391
The function steps is deﬁned by
steps (r,f) = if horizontal (r,f)
then [c | c ←[f +1,r −1],c mod 7 ̸= 0]
else [c | c ←[f +7,r −7],0<c ∧c<42]
Each step involves moving a vehicle a step in one of two directions, left or right for
horizontal vehicles, and up or down for vertical vehicles. In each case the target of
the move has to be an unoccupied cell.
The function move is implemented by
move::Grid →Move →Grid
move g (n,c) = g1 ++[adjust v c]++g2
where (g1,v:g2) = splitAt n g
adjust ::Vehicle →Cell →Vehicle
adjust (r,f) c = if f <c then (c−f +r,c) else (c,c+f −r)
Finally, a puzzle is solved if the front of the special car is at the exit cell:
solved ::Grid →Bool
solved g = snd (head g) == 20
Having deﬁned moves, move, and solved, one can now implement a breadth-ﬁrst or
a depth-ﬁrst search following the standard recipe.
Turning to psolve, it seems we need only deﬁne gameplan and premoves. How-
ever, the deﬁnition of newplans given in the previous section needs to be modiﬁed
in order to work with Rush Hour. To see why, suppose the ﬁrst move in the current
plan is (0,19), moving the special car one step right from its initial position (17,18).
Assume further that cell 19 is currently blocked by a vehicle, so there is a need
for preparatory moves that move this vehicle out of the way. Now it is perfectly
possible that one of these preparatory moves is (0,16), moving the special vehicle
one step left. After executing these moves, we see that (0,19) is no longer a valid
move because it requires car 0 to move two steps forward.
To solve this problem, we will allow multi-step moves in plans, but expand them
to single-step moves before computing new plans. Thus we redeﬁne newplans to
read
newplans::Grid →Plan →[Plan]
newplans g [ ]
= [ ]
newplans g (m:ms) = mkplans (expand g m++ms)
where mkplans (m:ms) = if elem m (moves g) then [m:ms] else
concat [ newplans g (pms++m:ms)
| pms ←premoves g m,all (/∈ms) pms]
Each move is expanded into a sequence of legal moves before new plans are made.

392
Ways of searching
(1)
(2)
(3)
(4)
(5)
(6)
Figure 15.2 Six Rush Hour problems
Furthermore, premoves now returns a list of possibly multi-step moves rather than a
list of sequences of moves. The function expand is deﬁned by
expand ::Grid →Move →[Move]
expand g (n,c) = if horizontal (r,f)
then if f <c then [(n,d) | d ←[f +1..c]]
else [(n,d) | d ←[r −1,r −2..c]]
else if f <c then [(n,d) | d ←[f +7,f +14..c]]
else [(n,d) | d ←[r −7,r −17..c]]
where (r,f) = g!!n
Given the ability to make use of multi-step moves, we can deﬁne gameplan by
gameplan::Grid →Plan
gameplan g = [(0,20)]
To deﬁne premoves, observe that, if a move cannot be made it is because the target
cell is blocked by a vehicle, which therefore has to be moved out of the way. Each
additional plan therefore consists of a single, possibly multi-step move:
premoves::Grid →Move →[Plan]
premoves g (n,c) = [[m] | m ←freeingmoves c (blocker g c)]
blocker ::Grid →Cell →(Name,Vehicle)
blocker g c = head [(n,v) | (n,v) ←zip [0..] g,elem c (ﬁll v)]
The function blocker returns the name of the blocking vehicle and the cells occupied
by its front and rear. To deﬁne freeingmoves, observe that, if a vehicle with length

15.7 Chapter notes
393
Puzzle
bfsolve
moves
psolve
moves
dfsolve
moves
(1)
0.80s
34
0.08s
38
0.42s
1228
(2)
0.44s
18
0.03s
27
0.42s
2126
(3)
0.20s
55
0.12s
57
0.11s
812
(4)
16.83s
93
0.28s
121
17.27s
15542
(5)
4.14s
83
1.06s
119
3.47s
4794
(6)
0.78s
83
0.08s
89
0.27s
1323
Figure 15.3 Running times and move counts for the six Rush Hour problems
k is horizontal, then in order to free cell c we have to move the vehicle either
rightwards to cell c+k or leftwards to cell c−k. If the vehicle is vertical, then the
move is downwards to cell c+7k or upwards to c−7k. In each case the destination
cell has to be on the grid. For a horizontal vehicle (r,f) we have k = f −r+1, while
for a vertical vehicle k = (f −r)/7+1. Hence we can deﬁne
freeingmoves::Cell →(Name,Vehicle) →[Move]
freeingmoves c (n,(r,f)) =
if horizontal (r,f)
then [(n,j) | j ←[c−(f −r +1),c+(f −r +1)],a<j ∧j<b]
else [(n,j) | j ←[c−(f −r +7),c+(f −r +7)],0<j ∧j<42]
where a = r −r mod 7;b = f −f mod 7+7
This completes the planning algorithm for Rush Hour.
So, is psolve better than a BFS or a DFS solution, and if so, by how much?
Pictured in Figure 15.2 are six Rush Hour grids, the bottom three of which are
amongst the hardest known starting grids. Each puzzle was tackled using bfsolve
(a BFS to ﬁnd one solution), psolve, and also dfsolve (a DFS to ﬁnd one solution).
The computations were run using GHCi, with the results given in Figure 15.3. In
each case, psolve is faster than bfsolve, varying from a factor of two to a factor of
60 in the case of puzzle (4). On the other hand, in no case did psolve ﬁnd solutions
with the minimum number of moves. As can be seen from the table, dfsolve found
solutions with many more moves than necessary.
15.7 Chapter notes
The 8-queens puzzle was ﬁrst described in 1848; many mathematicians, including
Gauss, have worked on the problem. See [10], and also [4] which contains an explicit
formula for computing a single solution for the n-queens problem for all n ⩾4.
Using a massively parallel approach, the value of Q(27), the number of solutions
for the 27-queens problem, was found to be 234907967154122528 in September

394
Ways of searching
2016; see [7]. Other values of Q(n) appear as sequence A000170 in the On-line
Encyclopedia of Integer Sequences (OEIS), see [11]. The bit-vector approach was
ﬁrst described by Qiu Zongyan [12] and later rediscovered by Martin Richards [9].
The problem of computing expressions with a given sum appears in [1, Chapter 6].
Knuth [6, Section 7.2.1.6, Exercise 122] also discusses the problem, and gives
other variants of the problem, such as allowing parentheses and further arithmetic
operations.
Lunar Landing is available at www.thinkfun.com/products, as is Rush Hour.
Lunar Landing is also known as Lunar Lockout and the UFO puzzle. A computer
analysis of the puzzle on boards of different shapes and sizes can be found in [8].
The planning algorithm for Rush Hour was ﬁrst described in [1]. The complexity of
the problem is discussed in [3] and the hardest known starting grid is taken from
[2]. For more information on planning algorithms, consult [5].
References
[1]
Richard Bird. Pearls of Functional Algorithm Design. Cambridge University Press,
Cambridge, 2010.
[2]
S´ebastien Collette, Jean-Franc¸ois Raskin, and Fr´ed´eric Servais. On the symbolic
computation of the hardest conﬁgurations of the Rush Hour game. In Computers and
Games, volume 4630 of Lecture Notes in Computer Science, pages 220–233.
Springer-Verlag, Berlin, 2006.
[3]
Gary W. Flake and Eric B. Baum. Rush Hour is PSPACE-complete, or “Why you
should generously tip parking lot attendants”. Theoretical Computer Science,
270(1):895–911, 2002.
[4]
Eric J. Hoffman, J. C. Loessi, and Robert C. Moore. Construction for the solutions of
the m queens problem. Mathematics Magazine, 42(2):66–72, 1969.
[5]
Steven M. LaValle. Planning Algorithms. Cambridge University Press, Cambridge,
2006.
[6]
Donald E. Knuth. The Art of Computer Programming, volume 4A: Combinatorial
Algorithms. Addison-Wesley, Reading, MA, 2011.
[7]
Thomas B. Preusser and Matthias R. Engelhardt. Putting queens in carry chains,
No 27. Journal of Signal Processing Systems, 88(2):185–201, 2017.
[8]
John Rausch. Computer analysis of the UFO puzzle.
http://www.puzzleworld.org/puzzleworld/art/art02.htm, 1999.
[9]
Martin Richards. Backtracking algorithms in MCPL using bit patterns and recursion.
http://www.cl.cam.ac.uk/~mr10/backtrk.pdf, University of Cambridge
Computer Laboratory, 2009.
[10] Walter William Rouse Ball. Mathematical Recreations and Essays. Macmillan, New
York, 1960.
[11] Neil Sloane. The on-line encyclopedia of integer sequences. https://oeis.org/,
1996.
[12] Qiu Zongyan. Bit-vector encoding of n-queen problem. ACM SIGPLAN Notices,
37(2):68–70, 2002.

Exercises
395
Exercises
Exercise 15.1 What simple optimisation would make queens1 run faster?
Exercise 15.2 Consider the solution for the 4-queens problem based on the function
search. Write down the successive arguments of search up to the point that the ﬁrst
solution is found.
Exercise 15.3 In the solution to the n-queens problem based on bit vectors, there is
another, non-recursive deﬁnition of bits which uses the Data.Bits function bit. The
value of bit i is a bit vector with bit i set to 1 and all other bits set to 0. Give the
alternative deﬁnition as a list comprehension.
Exercise 15.4 In Section 15.2 the deﬁnition of solutions 100 [1..9] used limited-
precision integers. Why is this justiﬁed?
Exercise 15.5 Consider again the two functions solutions and solutions1 for com-
puting the number of ways a list of digits can be combined to give a target value.
Recall that the latter is an optimised version of the former based on the monotonicity
of expressions built out of × and +. What would you expect the value of
solutions 100 [0..9] == solutions1 100 [0..9]
to be?
Exercise 15.6 Express the condition that expressions built out of juxtaposition, ×,
and + never decrease the value of an expression more formally as a property of
glue. Does the condition hold when the digit 0 is allowed in expressions?
Exercise 15.7 If we allowed decimal points in expressions, then there are other
ways of making 100, including
100 = 1×.2+.3+45+6.7×8+.9
100 = 1×23×4+5.6+.7+.8+.9
100 = 1×23×4+5+.6+.7+.8+.9
In Haskell, .6 isn’t a legal expression and one has to write 0.6 instead, but never
mind. Are there any other ways? Write a program to ﬁnd out. As a hint, there
are seven ways to extend 2 × 3 + ··· on the left with a new digit 1, six ways to
extend .2×3+···, and ﬁve ways to extend 2.3×4+···. Base the program on the
following type synonyms:
type Expr
= [Term]
type Term
= [Factor]
type Factor = ([Digit],[Digit])
A factor (xs,ys) contains the digits xs before the decimal point and the digits ys
after the point. Either xs or ys can be the empty list but not both.

396
Ways of searching
Exercise 15.8 Supposing we allowed exponentials in expressions, there is at least
one other way to make 100:
100 = 1+2 ˆ 3+4×5+6+7×8+9
Are there any other ways? Write a program to ﬁnd out. Again, no parentheses are
allowed. Base the program on the following type synonyms:
type Expr
= [Term]
type Term
= [Expo]
type Expo
= [Factor]
type Factor = [Digit]
type Digit
= Integer
Here a digit is an Integer and the values of expressions are Integer values because
the numbers involved can exceed the range of ﬁxed-precision arithmetic. Each term
is now a product of a nonempty list of exponentials; for example
12 ˆ 3×4 ˆ 5+6×7
would be represented by
[[[[1,2],[3]], [[4],[5]]], [[[6]],[[7]]]]
Assume that exponentiation associates to the left, so that, for example, 2 ˆ 3 ˆ 2 = 64.
(In Haskell, exponentiation associates to the right, but solving the problem with
this order of association would involve such huge numbers that the program would
crash.)
Exercise 15.9 Given that
exp = foldr f e·takeWhile p·iterate g
show that
exp x = if p x then f x (exp (g x)) else e
Exercise 15.10 Recall that in order to optimise BFS we deﬁned
search1 pss ps = search (ps++concat (reverse pss))
Calculate search1 pss (p:ps), assuming the ﬁrst state in p is not a solved state.
Exercise 15.11 Given is a permutation of [0..n]. The aim is to sort the permutation
into increasing order using only moves that interchange 0 with any neighbour that
is at most two positions away. For example, [3,0,4,1,2] can produce [0,3,4,1,2],
[3,4,0,1,2], and [3,1,4,0,2] in a single step. Can the aim always be achieved?
Write a breadth-ﬁrst search to ﬁnd the shortest sequence of moves. (Hint: one
possible representation of states and moves is
type State = (Nat,Array Nat Nat)
type Move = Nat

Exercises
397
1
2
3
0
4
Figure 15.4 A Rush Hour grid
The ﬁrst component of a state is the location of 0 in the array, and a move is an
integer giving the target position for 0.)
Exercise 15.12 Imagine a row of differently sized jugs, given in ascending order of
their capacity. Initially all jugs are empty except for the last, which is full to the brim
with water. The object is to get to a situation in which one or more jugs contains
exactly a given target amount of water. A move in the puzzle consists of ﬁlling one
jug with water from another jug, or emptying one jug into another. (Water cannot
simply be discarded.) Suppose cap is a given array that determines the capacity of
each jug, and target is a given integer target. Decide on the representation of states
and moves and give the functions moves, move, and solved. Hence use breadth-ﬁrst
search to ﬁnd the unique shortest solution for the particular instance of three jugs,
with capacities 3, 5, and 8, and a target amount of 4.
Exercise 15.13 There are m elves and m dwarves on a river bank. There is also a
rowing boat that can take them to the other side of the river. All elves can row, but
only n of the dwarves can. The boat can contain up to p passengers, one of whom
has to be a rower. The problem is to transport the elves and dwarves safely to the
other side in the shortest number of trips, where a trip is safe if the dwarves on
either side of the river, or in the boat, never outnumber the elves. For the avoidance
of doubt, the boat empties completely before new passengers get on. The exercise
simply asks for a suitable way to model states and for the deﬁnitions of moves,
move, and solved.
Exercise 15.14 Write a function showMoves::[Move] →String for Lunar Landing
so that, for example, the moves 5U 5R 5U 2L 2D 2L 0U 0R 0U are recorded as
5URU 2LDL 0URU. The Haskell Data.List function groupBy may prove useful.

398
Ways of searching
Exercise 15.15 Consider the Rush Hour grid g in Figure 15.4. Using directional
rather than cell-based notation, the value of gameplan for this grid is 0RRR, meaning
the special car 0 has to move three places to the right. By listing the appropriate
values of premoves, determine the value of newplans g gameplan.
Answers
Answer 15.1 Just reverse the lists in help and use newDiag2:
queens n = map reverse (help n)
where help 0 = [[ ]]
help r = [q:qs | qs ←help (r −1),q ←[1..n],
notElem q qs,newDiag2 q qs]
Answer 15.2 The successive arguments of search are
[[ ]]
[[1],[2],[3],[4]]
[[3,1],[4,1],[2],[3],[4]]
[[4,1],[2],[3],[4]]
[[2,4,1],[2],[3],[4]]
[[2],[3],[4]]
[[4,2],[3],[4]]
[[1,4,2],[3],[4]]
[[3,1,4,2],[3],[4]]
Answer 15.3 One non-recursive deﬁnition of bits is
bits::Word16 →[Word16]
bits v = [b | b ←map bit [0..15],v .&. b == b]
Answer 15.4 The largest value of an expression is 123456789, which is less than
229, so Int-arithmetic is adequate. Of course, when the input is longer than nine
digits, we should use Integer arithmetic.
Answer 15.5 You would probably expect the answer to be True, but you would be
wrong. The left-hand side returns 17 solutions, while the right-hand side returns
only 14. One answer returned by solutions but not by solutions1 is
100 = 0×1+2×3+4+5+6+7+8×9
The reason for the discrepancy is that the monotonicity condition fails when the
digit 0 is allowed in expressions. In particular,
101 = 1+2×3+4+5+6+7+8×9

Answers
399
but when this expression is extended by the digit 0, the result can drop in value.
Answer 15.6 The monotonicity condition can be expressed by
elem y (glue d x) ⇒value x ⩽value y
The proof follows from the fact that, if x and y are positive integers, then the larger
of x and y is no greater than any of the expressions 10 x + y, 10 y + x, x + y, or
x×y. The claim does not hold when zero values are allowed, because x ⩽x×0 is
false for positive x. It also does not hold when exponentiation is allowed, because
x ⩽y ˆ x is false unless y>1. It also does not hold when decimal points are allowed
in expressions.
Answer 15.7 First of all, the seven ways of extending 2×3+··· are
.12×3+···
12×3+···
1.2×3+···
.1×2×3+···
1×2×3...
.1+2×3+···
1+2×3+···
The monotonicity condition fails when decimal points are allowed, so only the naive
program works. We will just give the modiﬁed version of glue, which is
glue::Digit →Expr →[Expr]
glue d [ ] = [[[([d],[ ])]],[[([ ],[d])]]]
glue d (((xs,ys):fs):ts)
| null xs
= (((xs,d :ys):fs):ts):rest
| null ys
= [(([ ],d :xs):fs):ts,(([d],xs):fs):ts]++rest
| otherwise = rest
where rest = [((d :xs,ys):fs):ts,
(([d],[ ]):(xs,ys):fs):ts,
(([ ],[d]):(xs,ys):fs):ts,
[([d],[ ])]:((xs,ys):fs):ts,
[([ ],[d])]:((xs,ys):fs):ts]
It turns out that there are 198 ways to make 100 when decimal points are allowed.
Answer 15.8 The monotonicity condition fails when exponentiation is allowed, so
only the naive program works. Here is a modiﬁed version of glue:
glue::Digit →Expr →[Expr]
glue d [ ]
= [[[[[d]]]]]
glue d (((ds:fs):es):ts) = [(((d :ds):fs):es):ts,
(([d]:(ds:fs)):es):ts,
([[d]]:((ds:fs):es)):ts,
[[[d]]]:(((ds:fs):es):ts)]
An expression can be extended on the left in four ways.

400
Ways of searching
It turns out that there are just three ways to make a century using exponentiation:
100 = 1 ˆ 23+4+5×6+7×8+9
100 = 1 ˆ 2 ˆ 3+4+5×6+7×8+9
100 = 1+2 ˆ 3+4×5+6+7×8+9
Answer 15.9 We have
exp x
=
{ deﬁnition of iterate }
foldr f e (takewhile p (x:iterate g (g x)))
=
{ deﬁnition of takeWhile, assuming p x }
foldr f e (x:takeWhile p (iterate g (g x)))
=
{ deﬁnition of foldr }
f x (foldr f e (takeWhile p (iterate g (g x))))
=
{ deﬁnition of exp }
f x (exp (g x))
On the other hand, exp x = e if p x is false.
Answer 15.10 The calculation is as follows, assuming the ﬁrst state in p is not a
solved state:
search1 pss (p:ps)
=
{ deﬁnition of search1 }
search (p:ps++concat (reverse pss))
=
{ deﬁnition of search, given assumption }
search (ps++concat (reverse pss)++succs p)
=
{ deﬁnition of concat and reverse }
search (ps++concat (reverse (succs p:pss)))
=
{ deﬁnition of search1 }
search1 (succs p:pss) ps
Answer 15.11 Yes, the aim can always be achieved. One way is ﬁrst to get the
largest element into its ﬁnal position and then apply the same method recursively,
leaving the largest element untouched. To get the largest element into its ﬁnal
position, ﬁrst position 0 one place to the right of the largest element. For example,
counting positions from 1, the single move 2 converts [4,1,3,0,2] to [4,0,1,3,2],
while the two moves 4 and 3 convert [3,0,1,4,2] to [3,4,0,1,2]. Let j be the
position of the largest element. Repeatedly apply the moves j, j + 2, j + 1, j + 3,
and so on, followed by a ﬁnal move n −1, to shufﬂe the largest element to the
right. Continuing the example, the moves 2, 4, 3, and 5 convert [3,4,0,1,2] to
[3,1,2,4,0], which, followed by move 4, yields [3,1,2,0,4].

Answers
401
The relevant deﬁnitions for a breadth-ﬁrst search are
start ::[Nat] →State
start xs = (hole x,x)
where x = listArray (1,length xs) xs
hole x = head [j | j ←[1..],x!j == 0]
moves::State →[Move]
moves (j,x) = [k | k ←[j−1,j−2,j+1,j+2],a ⩽k ∧k ⩽b]
where (a,b) = bounds x
move::State →Move →State
move (j,x) k = (k,x//[(j,x!k),(k,x!j)])
solved ::State →Bool
solved (j,x) = sorted (elems x)
where sorted xs = and (zipWith (⩽) xs (tail xs))
For example,
solution (start [4,1,3,0,2]) = Just [3,1,2,4,3,5,4,2,1]
A total of nine moves sorts the numbers.
Answer 15.12 A simple representation is to use an array of naturals for states and
two naturals, the source and destination jugs, for the moves:
type State = Array Nat Nat
type Move = (Nat,Nat)
The possible moves in a given state consist of a pair of distinct integers in which the
source jug is nonempty and the target jug is not full:
moves::State →[Move]
moves t = [(j,k) | j ←indices t,k ←indices t,j ̸= k,0<t !j,t !k <cap!k]
The puzzle is solved when the target value appears in the array:
solved ::State →Bool
solved t = elem target (elems t)
Finally, to determine the result of a move, observe that the total quantity of water in
the two jugs remains the same, and either the source is emptied or the target is ﬁlled
to its capacity. That leads to
move::State →Move →State
move x (j,k) = if t ⩽c then x//[(j,0),(k,t)] else x//[(j,t −c),(k,c)]
where t = x!j+x!k;c = cap!k
The unique solution of length six for the three-jugs problem is
[(3,2), (2,1), (1,3), (2,1), (3,2), (2,1)]

402
Ways of searching
Answer 15.13 Data for the problem is deﬁned by three numbers (m,n,p), where m
is the total number of elves (the same as the number of dwarves), n is the number of
dwarves who can row, and p is the maximum number of passengers allowed in a
boat:
type Data = (Nat,Nat,Nat)
One possible deﬁnition of a state is a quadruple
type State = (Bool,Nat,Nat,Nat)
In a state (b,e,d,r) the boolean b is True if the boat is empty and at the left bank
and False if it is empty and at the right bank. The values (e,d,r) are the numbers
of elves, the number of non-rowing dwarves, and the number of dwarves who can
row, on the left bank of the river, so the corresponding values on the right bank are
(m−e,m−n−d,n−r). Assuming everyone is initially on the left bank, the initial
state is
start ::Data →State
start (m,n,p) = (True,m,m−n,n)
The puzzle is solved if nobody is left on the left bank:
solved ::State →Bool
solved t = (t == (False,0,0,0))
A state is safe if the dwarves never outnumber the elves on either bank. If (e,d,r)
are the numbers on the left bank, then we require
(e == 0 ∨e ⩾d +r) ∧(m−e == 0 ∨m−e ⩾m−(d +r))
which simpliﬁes to
safe::Nat →State →Bool
safe m (b,e,d,r) = (e == 0 ∨e == m ∨e == d +r)
A move consists of the number of elves, non-rowing dwarves, and dwarves who can
row, representing the passengers carried on the boat:
type Move = (Nat,Nat,Nat)
A move is legal if it contains at most p people, at least one rower, and if the dwarves
do not outnumber the elves:
legal::Nat →Move →Bool
legal p (x,y,z) = x+y+z ⩽p ∧(x ⩾1 ∨z ⩾1) ∧(x == 0 ∨x ⩾y+z)
The function move is now deﬁned by
move::State →Move →State
move (True,e,d,r) (x,y,z) = (False,e−x,d −y,r −z)
move (False,e,d,r) (x,y,z) = (True,e+x,d +y,r +z)

Answers
403
A move consists of the boat travelling from one side of the river to the other, and
emptying all passengers onto the river bank. The function moves is deﬁned by
moves::Data →State →[Move]
moves (m,n,p) t@(b,e,d,r)
= [(x,y,z) | x ←[0..i],y ←[0..j],z ←[0..k],
legal p (x,y,z) ∧safe m (move t (x,y,z))]
where (i,j,k) = if b then (e,d,r) else (m−e,m−n−d,n−r)
For example, the (3,1,2) problem has four solutions, each involving 13 crossings
in total.
Answer 15.14 One possibility:
showMoves::[Move] →[String]
showMoves = map showMove·groupBy sameName
where sameName m1 m2 = name m1 == name m2
name (n, , )
= n
showMove ms
= show (name (head ms))++
concatMap dir [(s,f) | ( ,s,f) ←ms]
Answer 15.15 We have, again writing moves in expanded, directional form:
premoves g 0R = [[1DDD]]
premoves g 1D = [[4L]]
premoves g 4L = [[3U],[3DD]]
premoves g 3U = [[2R]]
premoves g 2R = [[1DD]]
But the move 1DD repeats part of 1DDD, so this plan is rejected. We are left with a
single plan, namely
newplans g gameplan = [3DD,4L,1DDD,0RRR]
All moves in this plan can be carried out, leading to the solution.


Chapter 16
Heuristic search
In the search methods we have looked at so far, we have always chosen the ﬁrst
path on the frontier for expansion, the only difference between breadth-ﬁrst and
depth-ﬁrst search being the order in which we added newly formed paths to the
frontier. In heuristic search, we make use of a given estimate of how likely it is
that each path will lead to a good result, and we choose next the one with the best
expectation. The hope is that, if the estimate is reasonably accurate, then we will
ﬁnd an optimum path more quickly. With heuristic search the frontier is managed as
a priority queue in which the priorities are estimates of how good a path is. At each
step the path with the highest priority is chosen for further expansion. Heuristic
search is useful only when searching for a single solution to a problem.
The primary example of heuristic search is the problem of ﬁnding a route between
two towns in a network of roads. The cost of getting to the ﬁnal destination from a
given town can be estimated as the straight-line distance between the two towns,
the distance a crow would have to ﬂy. No real route could have a shorter distance,
so this is an optimistic estimate. The choice of the next partial route for further
exploration will be one that minimises the sum of the cost of the route so far and
the estimate of how much further there is to travel. Contrast this with Dijkstra’s
algorithm, which always explores a partial route whose cost is the minimum so far,
ignoring any estimate for completing the journey. Route-ﬁnding algorithms have
many applications in artiﬁcial intelligence, including robotics, games, and puzzles.
We will take a look at some examples later on.
Heuristic search is usually described using the terminology of graphs and edges
rather than states and moves. We will assume throughout this chapter that graphs
consist of a ﬁnite number of vertices and directed edges, and that the cost (or weight)
of each edge is always a positive number. We describe two closely related algorithms
for carrying out heuristic search, each of which depends on a different assumption
about the estimating function. We revisit the necessary operations of priority queues,

406
Heuristic search
and also describe a new structure, a priority search queue, that can help to improve
the running time of the search.
16.1 Searching with an optimistic heuristic
By deﬁnition, an estimating function or heuristic is a function h from vertices to
costs such that h(v) estimates the cost of getting from vertex v to a closest goal (in
general, there may be a number of possible goals rather than one single destination).
Such a function is said to be optimistic if it never overestimates the actual cost. In
symbols, if H(v) is the minimum cost of any path from vertex v to a closest goal,
then h(v) ⩽H(v) for all vertices v. If there is no path from v to a goal, then h(v) is
unconstrained. An optimistic heuristic is also called an admissible heuristic. In this
section we give two algorithms that work whenever the heuristic is optimistic and
there is a path from the source to a goal.
The ﬁrst algorithm is a very basic form of heuristic search, which we will call T*
search simply because the underlying algorithm is really a tree search. Here are the
types we need, with the exception of Vertex, which depends on the application:
type Cost
= Nat
type Graph
= Vertex →[(Vertex,Cost)]
type Heuristic = Vertex →Cost
type Path
= ([Vertex],Cost)
We assume that a graph is given not as a list of vertices and edges, but as a function
from vertices to lists of adjacent vertices together with the associated edge costs.
This function corresponds to the function moves of the previous chapter, except that
now we assume each move from one state to another is associated with a certain
cost. A path is a list of vertices along with the cost of the path. For efﬁciency, paths
will be constructed in reverse order, so the endpoint of a path is the ﬁrst element in
the list of vertices:
end ::Path →Vertex
end = head ·fst
cost ::Path →Cost
cost = snd
extract ::Path →Path
extract (vs,c) = (reverse vs,c)
In terms of states and moves, a path would be a triple consisting of a list of moves,
an end state, and the cost of the moves. We will also make use of the following
operations on priority queues from Section 8.3:

16.1 Searching with an optimistic heuristic
407
insertQ
::Ord p ⇒a →p →PQ a p →PQ a p
addListQ::Ord p ⇒[(a,p)] →PQ a p →PQ a p
deleteQ ::Ord p ⇒PQ a p →((a,p),PQ a p)
emptyQ ::PQ a p
nullQ
::PQ a p →Bool
To recap: insertQ adds a new value with a given priority to the queue; addListQ adds
a list of value–priority pairs to an existing queue; deleteQ deletes a value with the
lowest priority from the queue, returning the value, its priority, and the remaining
queue; emptyQ is the empty queue; and nullQ is a test for whether the queue is
empty or not. In what follows we will not need deleteQ to return the priority of a
value, so we introduce the variant
removeQ::Ord p ⇒PQ a p →(a,PQ a p)
removeQ q1 = (x,q2) where ((x, ),q2) = deleteQ q1
Here now is the deﬁnition of tstar:
tstar ::Graph →Heuristic →(Vertex →Bool) →Vertex →Maybe Path
tstar g h goal source = tsearch start
where start = insertQ ([source],0) (h source) emptyQ
tsearch ps | nullQ ps
= Nothing
| goal (end p) = Just (extract p)
| otherwise
= tsearch rs
where (p,qs) = removeQ ps
rs
= addListQ (succs g h p) qs
As inputs to tstar we have a graph, a heuristic function, a test for whether a vertex
is a goal or not, and the source vertex. The frontier is maintained as a priority queue
of paths and their costs, initially containing the single path [source] with cost 0 and
priority h(source). If the queue is not empty, then a path with the lowest estimate of
how much it costs to complete the journey is selected. If the selected path ends at a
goal node, then that path is the result; otherwise its successor paths are added to the
queue. The subsidiary function succs returns a list of possible successor paths:
succs::Graph →Heuristic →Path →[(Path,Cost)]
succs g h (u:vs,c) = [((v:u:vs,c+d),c+d +h v) | (v,d) ←g u]
Note carefully that the priority of a new path is not simply the estimate of how far
away the endpoint is from a goal, but the sum of the cost of getting to the endpoint
and the estimate of the remaining cost. It is left as an exercise to show that taking
the estimate alone as the priority can lead to a solution that is not the shortest.
The tstar algorithm is not a very satisfactory one. One fundamental ﬂaw is that it
is not guaranteed to terminate. For example, consider the graph

408
Heuristic search
A
B
C
1
with source vertex A and an isolated goal vertex C. Instead of terminating with
Nothing, the function tstar goes into an inﬁnite loop, constructing longer and longer
paths A, AB, ABA, ABAB, and so on, in a fruitless attempt to ﬁnd the goal. A similar
phenomenon occurs with the graph
A
B
C
1
100
and the optimistic heuristic h = const 0. Only after 100 oscillations between A and
B will tstar discover the path ABC with cost 101. Even worse, with the graph
A
B
C
D
1
100
1
tstar will oscillate about 250 times between A, B, and D before ﬁnding the ﬁnal path.
Hence tstar can be very inefﬁcient. We will remedy both these problems below.
Nevertheless, provided there is a path from the source to a goal, tstar will ﬁnd one
with minimum cost. The only provisos are that h be optimistic and that edge costs
are positive numbers. Say a path from the source vertex s is a good path if it can be
completed to a path with minimum cost. We show that at each step of tstar there is a
good path on the frontier and, moreover, some good path will eventually be selected
for further expansion at a subsequent step. The claim is clearly true initially. For the
induction step, suppose p is a good path on the frontier, with endpoint v say. Let
c(p) be the cost of p, so
c(p)+h(v) ⩽c(p)+H(v) = H(s)
since h is optimistic. Recall that H(v) is the minimum cost of any path from v to a
goal and s is the starting vertex. Suppose some bad path q, with endpoint u say, is
chosen at the next step, so
c(q)+h(u) ⩽c(p)+h(v) ⩽H(s)
However, u cannot be a goal state, for otherwise
c(q)+h(u) = c(q)+0>H(s)
since q is a bad path. The ﬁnal step of the proof is to observe that bad paths cannot
be added indeﬁnitely to the frontier before a good path is selected for expansion.

16.1 Searching with an optimistic heuristic
409
A
B
C
D
5
5
2
2
Figure 16.1 A simple graph
Let δ >0 be the minimum cost of any edge (recall that graphs are ﬁnite, so there
are ﬁnitely many edges). A path of length k therefore has cost at least kδ, so no bad
path of length greater than H(s)/δ can be added to the frontier before a good path
is selected for expansion.
Here is an example to show tstar at work. Consider the graph of Figure 16.1, in
which A is the source vertex and D is the goal. Suppose h is the optimistic heuristic
A
B
C
D
h
9
1
5
0
The successive queue entries in priority order are as follows (paths are written in
the normal left-to-right direction):
A (0+9)
AB (5+1), AC (2+5)
AC (2+5), ABD (10+0)
ACB (4+1), ABD (10+0)
ACBD (9+0), ABD (10+0)
The algorithm starts off with the single queue entry A (0 + 9), where the ﬁrst
component of the sum is the distance from the source vertex A and the second
component is the heuristic value. Subsequent paths in the queue are as above.
Although the non-optimal path ABD is inserted into the queue at the second step,
it is never selected; instead the ﬁnal path returned by the algorithm is ABCD with
cost 9. As this example demonstrates, tstar works best when the underlying graph
is acyclic.
The obvious way to remedy the fact that tstar may not terminate is to maintain a
second argument that records which vertices have already been visited, meaning
their successors have been added to the queue. That way, no vertex is processed
more than once. After all, this was exactly what was done in depth-ﬁrst and breadth-
ﬁrst search. However, the idea does not work: it is possible that a second path to the
same vertex with a smaller cost, and hence a better estimate, may be found later on,

410
Heuristic search
so vertices may have to be processed more than once. For example, in Figure 16.1
the vertex B is visited twice in order to discover the second, shorter path to D.
One solution to the problem is to maintain a ﬁnite map from vertices to path
costs instead of a set containing the vertices that have already been visited. If a
new path to a vertex is discovered with a lower cost, then the path can be explored
further. Otherwise the path can be abandoned. Another solution, involving a stronger
assumption about the heuristic function h, is left to the following section.
The ﬁnite map can be implemented as a simple association list of vertex–cost
pairs, but a more efﬁcient alternative is to make use of the Haskell library Data.Map
and the three operations
empty ::Ord k ⇒Map k a
lookup::Ord k ⇒k →Map k a →Maybe a
insert ::Ord k ⇒k →a →Map k a →Map k a
The last two operations take logarithmic rather than linear time. To avoid name
clashes, we use a qualiﬁed import:
import qualiﬁed Data.Map as M
Here now is the deﬁnition of a revised search, the algorithm known as A* search:
astar ::Graph →Heuristic →(Vertex →Bool) →Vertex →Maybe Path
astar g h goal source = asearch M.empty start
where start = insertQ ([source],0) (h source) emptyQ
asearch vcmap ps | nullQ ps
= Nothing
| goal (end p)
= Just (extract p)
| better p vcmap = asearch vcmap qs
| otherwise
= asearch (add p vcmap) rs
where (p,qs) = removeQ ps
rs
= addListQ (succs g h p) qs
better ::Path →M.Map Vertex Cost →Bool
better (v:vs,c) vcmap = query (M.lookup v vcmap)
where query Nothing = False
query (Just c′) = c′ ⩽c
add ::Path →M.Map Vertex Cost →M.Map Vertex Cost
add (v:vs,c) vcmap = M.insert v c vcmap
The additional argument vcmap to asearch is a ﬁnite map of vertex–cost pairs. The
test better determines of a path whether or not another path to the same endpoint
but with a smaller cost has already been found. If so, the path can be abandoned.
The operation M.lookup looks up a vertex in the ﬁnite map, returning Nothing if
there is no binding, and the associated cost if there is. The function better returns

16.2 Searching with a monotonic heuristic
411
True just in the case that there is such an associated cost and it is no larger than
the given cost. The function add adds a new vertex–cost pair, or overwrites an old
binding with the same vertex and the new cost.
Let us ﬁrst prove that astar terminates for all inputs. There are a ﬁnite number of
simple paths in a ﬁnite graph, paths that do not contain repeated vertices. Because
the edge weights are positive numbers, no non-simple path can have a smaller
cost than the corresponding simple path to the same destination. Let M denote the
maximum cost taken over all simple paths, of which there are a ﬁnite number. Then
any vertex can be processed at most M times during the computation because each
processing step requires a path with a strictly smaller cost than before. It follows
that astar terminates after M n steps at the very most, where n is the number of
vertices in the graph. If no path from the source to the target has been found, then
no such path exists, and the algorithm correctly returns Nothing.
To show that astar terminates with a minimum-cost path from the source to a
goal, assuming there is one, we can follow the proof of the correctness of tstar. We
only have to show that at every step there is a good path on the frontier. Say that
a path p with endpoint v is open if p is on the frontier and there is no entry (v,c)
recorded by the ﬁnite map with c ⩽c(p). Otherwise, say that p is closed. Open
paths are candidates for further expansion, while closed paths are not.
Let P = [v0,v1,...,vn] be an optimal path from the source v0 to a goal vn and
let Pj denote the initial segment [v0,v1,...,vj] for 0 ⩽j<n. We show that at each
step there is an open path p with endpoint vj for some j and such that c(p) = c(Pj).
Hence p can be completed to an optimal path. The assertion holds at the very ﬁrst
step because P0 is open. Otherwise, let D be the set of vertices vi for which there
is a closed path q from v0 to vi on the frontier with c(q) = c(Pi). The set D is not
empty, because it contains v0. Let vi be the vertex with largest index in D and set
j = i+1. Deﬁne p to be the path q followed by the single edge (vi,vj) with cost c.
Then p is an open path and
c(p) = c(q)+c = c(Pi)+c = c(Pj)
That completes the proof that astar correctly returns an optimal solution.
16.2 Searching with a monotonic heuristic
Now we turn to a second solution to the problem with tstar. This time we need to
assume more about the heuristic function h, namely that it is monotonic. A heuristic
h is monotonic if h(u) ⩽c+h(v) for every edge (u,v,c) of the graph, where c is
the cost of the edge. Provided h(v) = 0 for every goal vertex v, it is the case that
a monotonic heuristic is optimistic; we leave the proof as an exercise. We do not
need a ﬁnite map in the case of a monotonic heuristic because, as we will see below,

412
Heuristic search
no vertex is processed more than once. It is therefore sufﬁcient to keep a set of the
processed vertices. A simple list would do, but it is more efﬁcient to use the set
operations of Section 4.4. Alternatively, we can use the Haskell library Data.Set,
which contains the operations
empty
::Ord a ⇒Set a
member ::Ord a ⇒a →Set a →Bool
insert
::Ord a ⇒a →Set a →Set a
The functions member and insert take logarithmic time. To avoid name clashes, we
use a qualiﬁed import:
import qualiﬁed Data.Set as S
Under the assumption that h is monotonic, the following monotonic search algo-
rithm mstar will ﬁnd an optimum path to a goal, provided one exists:
mstar ::Graph →Heuristic →(Vertex →Bool) →Vertex →Maybe Path
mstar g h goal source = msearch S.empty start
where start = insertQ ([source],0) (h source) emptyQ
msearch vs ps | nullQ ps
= Nothing
| goal (end p) = Just (extract p)
| seen (end p) = msearch vs qs
| otherwise
= msearch (S.insert (end p) vs) rs
where seen v = S.member v vs
(p,qs) = removeQ ps
rs
= addListQ (succs g h vs p) qs
This variation on A* search is the one most like breadth-ﬁrst or depth-ﬁrst search,
in that there is a simple set vs to record vertices that have been visited to ensure that
no vertex is ever processed more than once. We show below that, once a path p to a
vertex v has been found, then p has the minimum cost of any path from the source
to v, so no further paths to v need be considered. The modiﬁed deﬁnition of succs
reads
succs::Graph →Heuristic →S.Set Vertex →Path →[(Path,Cost)]
succs g h vs p = [extend p v d | (v,d) ←g (end p),not (S.member v vs)]
where extend (vs,c) v d = ((v:vs,c+d),c+d +h v)
This is more efﬁcient than the previous version because a successor path is never
added to the frontier if its endpoint has already been processed.
For the proof that mstar works correctly, suppose path p to vertex v was found
before another path p′ to v. We have to show that c(p) ⩽c(p′). Let q′ be the initial
segment of p′ that was on the frontier when p was selected; let q′ end at vertex u
and let r be the continuation of q′ that begins at u and constitutes p′. Then

16.2 Searching with a monotonic heuristic
413
c(p)
⩽
{ since p was selected in favour of q′ }
c(q′)+h(u)−h(v)
=
{ by deﬁnition of path costs }
c(p′)−c(r)+h(u)−h(v)
⩽
{ since h is monotonic and r is a path from u to v }
c(p′)
The last step makes use of a generalisation of monotonicity, namely that, if r is
a path from u to v, then h(u) ⩽c(r)+h(v). We leave the proof as an exercise. In
conclusion, mstar returns an optimal solution if one exists.
The example of Figure 16.1 shows that mstar can return a non-optimal solution
if h is optimistic but not monotonic. The heuristic function
A
B
C
D
h
9
1
5
0
is not monotonic: the edge from C to B has cost 2 but h(C)>2+h(B). As before,
the algorithm starts off with the single entry A (0+9). In the next step the queue
has two entries AB (5+1), AC (2+5). The path with the lowest priority is AB, so
the next queue is ABD (10+0), AC (2+5). The next path to be expanded is AC
and, since B has already been processed, the next queue consists of the single entry
ABD (10+0), which is the ﬁnal non-optimal result.
Here is a more elaborate example that illustrates another aspect of the mstar
algorithm:
A
B
C
D
E
F
3
10
20
20
5
8
20
2
10
6
1
The source node is A and the single goal is F. Suppose h is the monotonic function
A
B
C
D
E
F
h
10
10
5
5
0
0
The ﬁrst queue has the single entry A (0+10). Vertex A is added to the visited list,
and the next queue is
AB (3+10), AC (10+5), AE (20+0), AD (20+5)

414
Heuristic search
Vertex B is added to the visited list, and the next queue is
ABC (8+5), AC (10+5), ABD (11+5), AE (20+0), ABE (23+0),
AD (20+5)
The queue contains redundant entries, since there are two paths to each of C, D, and
E, only one of which in each case will ever be further explored. We will see how to
deal with redundancy later on. Note also that the revised deﬁnition of succs means
that the additional path ABA is not added to the queue because A has already been
visited. Vertex C is added to the visited list, and the next queue is
AC (10+5), ABCD (10+5), ABD (11+5), ABCE (18+0), AE (20+0),
ABE (23+0), AD (20+5)
There are now three paths to each of D and E on the queue, two of which are
redundant. The path with the lowest priority is AC with cost 10, but this path is
abandoned since C has been added to the visited list and a better path ABC with
cost 8 has already been found. Instead, ABCD is selected, D is added to the visited
list, and the next queue is
ABD (11+5), ABCDE (16+0), ABCE (18+0), AE (20+0),
ABE (23+0), AD (20+5)
There are four remaining paths to E on the queue, three of which are redundant.
The (ﬁrst) path with the lowest priority is ABD, but this is rejected since D is on
the visited list. Instead the path ABCDE is chosen, and after one more step the ﬁnal
path ABCDEF with cost 17 is returned.
As can be appreciated from this example, it is possible for many redundant entries
to be added to the queue. Depending on the connectivity of the graph, that can lead
to a good deal of unnecessary computation, adding entries to the queue only to
delete them again at a later stage.
One solution to this problem is to employ a more reﬁned data structure than a
priority queue, called a priority search queue (PSQ). In a PSQ there are values and
priorities but also keys. The idea is that there is at most one value in a PSQ with a
given key. In the present example, values are paths, along with their costs, and keys
are the end vertices of paths. The ﬁve queue operations, adapted to priority search
queues, are as follows:
insertQ
::(Ord k,Ord p) ⇒(a →k) →a →p →PSQ a k p →PSQ a k p
addListQ::(Ord k,Ord p) ⇒(a →k) →[(a,p)] →PSQ a k p →PSQ a k p
deleteQ ::(Ord k,Ord p) ⇒(a →k) →PSQ a k p →((a,p),PSQ a k p)
emptyQ ::PSQ a k p
nullQ
::PSQ a k p →Bool
The type PSQ has three parameters: values, keys, and priorities. The ﬁrst three
functions, insertQ, addListQ, and deleteQ, take an extra argument, a function for

16.3 Navigating a warehouse
415
Figure 16.2 A warehouse with obstacles
extracting keys from values. For both astar and mstar the key function is end for
extracting the endpoint of a path. The function insertQ works as follows: if there
is no value with the given key in the queue, then the value is added to the queue
along with its priority. If there is such a value, only the value with the smaller
priority is kept. The function addListQ takes a key function and a list of value–
priority pairs, and inserts them into a queue as before. The remaining functions
are also as before. The three main queue operations each take logarithmic time in
the size of the queue. The astar and mstar algorithms are unchanged except for
an additional argument to the queue operations. It is beyond our scope to go into
the details of how to implement priority search queues, but see the chapter notes
for references. In fact, the Haskell libraries in the Hackage repository provide a
number of implementations of PSQs, including PSQueue and psqueues, though the
functions provided are slightly different in each case.
16.3 Navigating a warehouse
We will give two illustrations of A* search, the ﬁrst of which concerns the problem
of navigating around a warehouse ﬁlled with obstacles. This is the task that would
face an autonomous vehicle which has to ﬁnd a path from a given starting point in
the warehouse to a given destination, taking care to avoid collisions. For example,
consider the warehouse shown in Figure 16.2, which contains a haphazard collec-
tion of unit-sized boxes. What is wanted is a path from the top-left corner of the
warehouse to the bottom-right corner. Sections of the path have to be straight lines
starting at one grid point and ending at another, avoiding any box along the way.

416
Heuristic search
Figure 16.3 The continuous line shows an optimal ﬁxed-angle path with cost
25.07. The dashed line is a variable-angle path with cost 23.64 and the dotted path
is an optimal variable-angle path with cost 21.48.
Different solutions are available depending on precisely how the vehicle is al-
lowed to move between grid points. The most restrictive rule is that it can move at
each step only horizontally or vertically to an adjacent grid point. A more relaxed
rule would allow the vehicle to move diagonally at a 45 degree angle, so each grid
point has up to eight rather than four neighbours. In either of these scenarios the
target of each edge that makes up a path has to be unoccupied by a box. Finally, the
vehicle may be allowed in one step to move from any one grid point to any other,
turning through an arbitrary angle to do so. Not only has the target to be unoccupied
by a box, the step has to avoid touching any line segment that forms part of the
perimeter of a box. Figure 16.3 shows three such solutions. Firstly, the continuous
line describes a path that moves only from one grid point to a neighbouring one,
allowing diagonal moves. The cost of this path is the sum of the costs of the edges,
where an edge costs 1 for a horizontal or vertical move and
√
2 for a diagonal move,
so distances are Euclidean. The path consists of 18 straight moves and 5 diagonal
moves, for a total distance of 18 + 5
√
2 = 25.07. There are other paths with the
same minimum cost, but we will leave ﬁnding them as an exercise. The other two
paths are both variable-angle paths obtained by different means. The dotted line
shows a path in which every point on the grid that is visible to a starting point is
a possible neighbour. Finally, the dashed line is a path obtained by smoothing the
ﬁxed-angle path in a manner described below.
The layout of a warehouse can be described in terms of a grid. The points on a grid
of size m×n are deﬁned by coordinates (x,y), where 1 ⩽x ⩽m and 1 ⩽y ⩽n, with
the four lines x = 0, x = m+1, y = 0, and y = n+1 acting as barriers. Obstacles are

16.3 Navigating a warehouse
417
made up of unit-size boxes each occupying four grid points. The vertex identifying
each box is, say, its top-left corner. Hence we deﬁne
type Coord = Nat
type Vertex = (Coord,Coord)
type Box
= Vertex
type Grid
= (Nat,Nat,[Box])
boxes::Grid →[Box]
boxes ( , ,bs) = bs
The four corners of a box are given by
corners::Box →[Vertex]
corners (x,y) = [(x,y),(x+1,y),(x+1,y−1),(x,y−1)]
In the ﬁxed-angle solution, the neighbours of a grid point are any of its eight adjacent
grid points that are not boundary points or points occupied by a box. The function
neighbours can be deﬁned in a number of ways, including by using a ﬁxed array:
type Graph = Vertex →[Vertex]
neighbours::Grid →Graph
neighbours grid = ﬁlter (free grid)·adjacents
adjacents::Vertex →[Vertex]
adjacents (x,y) = [(x−1,y−1),(x−1,y),(x−1,y+1),
(x,
y−1),
(x,
y+1),
(x+1,y−1),(x+1,y),(x+1,y+1)]
free::Grid →Vertex →Bool
free (m,n,bs) = (a!)
where a = listArray ((0,0),(m+1,n+1)) (repeat True)
//[((x,y),False) | x ←[0..m+1],y ←[0,n+1]]
//[((x,y),False) | x ←[0,m+1],y ←[1..n]]
//[((x,y),False) | b ←bs,(x,y) ←corners b]
Recall that // is the array update function. A grid point is free if it is not on a
horizontal or vertical border and not occupied by a box. Use of an array means that
the neighbours of a grid point can be computed in constant time.
The ﬁxed-angle path can now be computed using the function fpath, a modiﬁca-
tion of mstar of the previous section. This time we will need the deﬁnitions
type Dist = Float
type Path = ([Vertex],Dist)
end ::Path →Vertex
end = head ·fst

418
Heuristic search
extract ::Path →Path
extract (vs,d) = (reverse vs,d)
We can now deﬁne fpath by
fpath::Grid →Vertex →Vertex →Maybe Path
fpath grid source target = mstar (neighbours grid) source target
Since the heuristic function is ﬁxed and there is only one goal, the type of mstar
changes to
mstar ::Graph →Vertex →Vertex →Maybe Path
The three arguments to mstar now consist of a graph, a source vertex, and a target
vertex. We will not write out the modiﬁed deﬁnition of mstar because the only
real difference is a new version of succs, which now takes the target vertex as an
argument instead of the heuristic function:
succs::Graph →Vertex →S.Set Vertex →Path →[(Path,Dist)]
succs g target visited p =
[extend p v | v ←g (end p),not (S.member v visited)]
where extend (u:vs,d) v = ((v:u:vs,dv),dv+dist v target)
where dv = d +dist u v
Deﬁnition of the Euclidean distance function dist is left as an exercise. The heuristic
function is monotonic, so fpath is guaranteed to ﬁnd a shortest path under the stated
travel restrictions if such a path exists.
Computing a variable-angle path involves an additional complication: not only
does the endpoint of each segment of the path have to be unoccupied, but also
the segment itself cannot cross any border of any box. Such a crossing may be
somewhere between two grid points. For example, only the two endpoints of the
path segment in
are grid points, but we have to ensure that no border edge of any box crosses the
segment. A detailed implementation is postponed to the exercises, but suppose we
have a function
visible::Grid →Segment →Bool
where
type Segment = (Vertex,Vertex)
that determines of a grid and a segment whether or not the segment is unimpeded

16.4 The 8-puzzle
419
by a box. Then the variable-angle path of Figure 16.3 that arises by smoothing the
ﬁxed-angle path is computed by
vpath::Grid →Vertex →Vertex →Maybe Path
vpath grid source target =
mstar (neighbours grid) (visible grid) source target
where this time mstar has the type
mstar ::Graph →(Segment →Bool) →Vertex →Vertex →Maybe Path
and is deﬁned in the same way as before except for a new deﬁnition of succs:
succs g vtest target vs p =
[extend p w | w ←g (end p),not (S.member w vs)]
where extend (v:vs,d) w = if not (null vs) ∧vtest (u,w)
then ((w:vs,du),du+dist w target)
else ((w:v:vs,dw),dw+dist w target)
where u
= head vs
du = d −dist u v+dist u w
dw = d +dist v w
The extra argument of succs is the visibility test vtest = visible grid. Each time a
successor w of a vertex v is added to the list we check that the parent u of v, if
it exists, is visible to w. If it is, then the vertex v is removed from the path, and
the added edge proceeds directly from u to w. Such a smoothing step will never
increase the cost of a path and may decrease it. The running time of this algorithm
is proportional to the ﬁxed-angle path version except for the additional time spent
evaluating visibility checks.
Finally, the optimal variable-angle path of Figure 16.3 can be obtained as an
instance of fpath that takes the neighbours of a grid point to be all points on the grid
visible to it:
neighbours (m,n,bs) (x1,y1) =
[(x2,y2) | x2 ←[1..m],y2 ←[1..n],visible (m,n,bs) ((x1,y1),(x2,y2))]
However, this method is costly and only realistic for small grids.
16.4 The 8-puzzle
The 8-puzzle is an example of a class of problems known as sliding-block puzzles. It
is a smaller version of the famous 15-puzzle popularised by Sam Loyd in the 1880s.
The puzzle consists of eight tiles arranged in a 3×3 grid with one empty space (the
15-puzzle is identical except there are 15 tiles on a 4×4 grid). An example is shown
in Figure 16.4. Tiles are numbered from 1 to 8, and any tile adjacent to the empty
space can be moved into it. The aim is to get from some given initial grid to a given

420
Heuristic search
1
4
7
2
5
6
8
3
7
8
4
5
6
1
2
3
Figure 16.4 An initial grid of the 8-puzzle and the required ﬁnal grid
ﬁnal grid, such as the one pictured on the right of the ﬁgure. In other sliding-block
puzzles the tiles can be of different sizes and may be coloured rather than numbered.
The aim is to slide the pieces around to get to some pleasing ﬁnal arrangement from
a given starting point.
The popularity of the 15-puzzle arose partly because Loyd asked for a solution
to an impossible problem: he gave an initial grid for which there was no sequence
of moves that could lead to the ﬁnal grid. In fact, whatever the ﬁnal grid happens
to be, only half the initial grids are solvable. The same holds true for the 8-puzzle.
The proof turns on three ideas, the ﬁrst of which is the parity of a permutation. The
parity of a permutation is the parity of its inversion count, a concept introduced in
Section 7.2. The inversion count is the number of pairs of elements of a permutation
p that are out of place, namely when i<j and p(i)>p(j). If we imagine the empty
space to be a 0, then the ﬁnal permutation 123456780 of Figure 16.4 has an inversion
count of 8, while the initial permutation 083256147 has a count of 14. So both
permutations have even parity.
Next, a transposition of a permutation is when any two different elements are
swapped. We claim that any transposition changes the parity of the permutation. To
see this, consider the transposition (i,j), where without loss of generality we suppose
that i occurs before j in the list p(1),p(2),...,p(n). Let s be the segment between i
and j and let Li be the number of elements of s less than i, and Bi the number of
elements bigger than i; similarly for L j and Bj. It follows that Li +Bi = L j +Bj = m,
where m is the length of s. The inversion count c of the permutation p can now be
expressed as
c = c0 +Li +Bj
where c0 is the contribution to the inversion count of elements outside the segment
s. The inversion count after the transposition is given by
c′ = c0 +Lj +Bi ±1
where the ﬁnal term is positive if i<j and negative if i>j. That means
c′ = (c−Li −Bj)+Lj +Bi ±1 = c−2Li +2Lj ±1

16.4 The 8-puzzle
421
so c′ is odd if c is even and vice versa. This shows in particular that if the initial and
ﬁnal permutations are both even, then the ﬁrst can lead to the second only with an
even number of moves.
The third idea involves Manhattan distances. The Manhattan distance of a tile is
the number of vertical and horizontal moves required to place the tile in its correct
place in the ﬁnal grid. For example, the Manhattan distance of each of the ﬁve tiles
1, 2, 4, 7, 8 in the ﬁrst grid of Figure 16.4 is two, because each tile has to move two
places to reach its ﬁnal resting place. The three remaining tiles, tiles 3, 5, and 6,
each have a Manhattan distance of zero, and the Manhattan distance of the empty
space is four. Each move in the puzzle corresponds to a transposition of the empty
space with a tile, and each transposition changes both the parity of the permutation
and the parity of the Manhattan distance of the empty space.
It follows that starting with an EE permutation (even parity of permutation and
even parity of the Manhattan distance of the empty space) can only lead to an
OO or an EE permutation and never to an OE or EO one. Similarly an OE or EO
permutation can never lead to a permutation in which the two parities are the same.
The ﬁnal step in the proof is to observe that the four classes, EE, OE, EO, and OO,
divide the set of permutations into equal sizes. For the proof, consider the row in
which the empty space occurs. Transposing the two tiles in the row changes the
parity of the permutation but not the Manhattan distance of the empty space. This
produces a bijection between OE and EE and a bijection between EO and OO, so
they must all be of equal size.
The next task is to consider what heuristic functions might be useful in solving
the 8-puzzle. At least half a dozen functions have been proposed in the literature, but
we will discuss just two of the best known. The ﬁrst is to take h(g) simply to be the
number of tiles in the grid g that are out of place. As we have seen, in Figure 16.4
there are ﬁve out-of-place tiles, so h(g) = 5. The function h is monotonic (see the
exercises), so mstar is guaranteed to ﬁnd a shortest path.
The second function is the sum of the Manhattan distances of the tiles from
their current positions to their ﬁnal resting places (but not including the Manhattan
distance of the blank space). For our example we have h(g) = 10 because the
Manhattan distance of each out-of-place tile is 2. The Manhattan heuristic is a
reﬁnement of the out-of-place heuristic in that it takes account of how out of place
each tile is. The Manhattan heuristic is also monotonic.
The next decision concerns the representation of the grid. An obvious choice is a
3×3 array, but this representation can be quite wasteful of space, so we will go for
a more compact one. The idea is to encode a permutation such as 083256147 as a
string of digits, more speciﬁcally as an element of Text, a time- and space-efﬁcient
encoding of Unicode text deﬁned in the Haskell library Data.Text. Since many of

422
Heuristic search
the functions imported by this library have names that clash with Standard Prelude
functions, we import the library as a qualiﬁed module:
import qualiﬁed Data.Text as T
We deﬁne a state of the grid by
type Position = Nat
type State
= (T.Text,Position)
perm::State →String
perm (xs,j) = T.unpack xs
posn0::State →Position
posn0 (xs,j) = j
The position component of a state is the position of the empty space 0 in the
permutation encoded by the text component, a number between 0 and 8. The
function unpack unpacks a text into a string. The two states of our running example
are therefore represented by
istate,fstate::State
istate = (T.pack "083256147",0)
fstate = (T.pack "123456780",8)
where pack ::String →Text is another library function in Data.Text.
Each move can shift the position of the empty space to one of its vertical or
horizontal neighbours. An efﬁcient deﬁnition of moves is given by
type Move = Nat
moves::State →[Move]
moves st = moveTable!(posn0 st)
moveTable::Array Nat [Nat]
moveTable = listArray (0,8) [[1,3],
[0,2,4],
[1,5],
[0,4,6],[1,3,5,7],[2,4,8],
[3,7],
[4,6,8],
[5,7]]
The array moveTable lists the neighbours of grid points explicitly. For example, the
neighbours of grid point 0 are 1 and 3, while grid point 4 has neighbours 1, 3, 5,
and 7. The function move can be deﬁned by
move::State →Move →State
move (xs,i) j = (T.replace ty t0 (T.replace t0 tx (T.replace tx ty xs)),j)
where t0 = T.singleton '0'
ty = T.singleton '?'
tx = T.singleton (T.index xs j)
This is a three-step process in which the character x at position j in the text is

16.4 The 8-puzzle
423
replaced by some new character ‘?’, the empty space by x, and ﬁnally ‘?’ by the
empty space. The functions replace and index are also library functions of the
module Data.Text, as is singleton, which converts a single character into a text.
To solve a given instance we should ﬁrst check whether a solution is possible. We
will leave it as an exercise to deﬁne the two functions
icparity ::State →Bool
mhparity::State →State →Bool
where icparity returns True if the parity of the inversion count is even, and mhparity
returns True if the Manhattan distance of the empty space in the initial state to its
resting place in the ﬁnal state is even. We can then deﬁne
possible::State →State →Bool
possible is fs = (mhparity is fs == (icparity is == icparity fs))
That is, a solution is possible if either the Manhattan parity is even and the inversion
parities agree, or the Manhattan parity is odd and the inversion parities disagree.
This uses the fact that the Manhattan distance of the empty space in the ﬁnal state is
zero and therefore has even parity.
Here now is the deﬁnition of the out-of-place heuristic:
type Heuristic = State →State →Nat
h1 ::Heuristic
h1 is fs = length (ﬁlter p (zip (perm is) (perm fs)))
where p (c,d) = c ̸= '0' ∧c ̸= d
The two permutations are aligned and the number of out-of-place tiles is counted.
In order to deﬁne the Manhattan heuristic, we will need the coordinates of the
grid points, which we can take to be
(0,0)
(0,1)
(0,2)
(1,0)
(1,1)
(1,2)
(2,0)
(2,1)
(2,2)
The coordinates of a state are given by a list of coordinates in tile order, showing
which coordinate position each tile occupies. For example, with the permutation
083256147 these coordinates are
[(2,0), (1,0), (0,2), (2,1), (1,1), (1,2), (2,2), (0,1)]
Thus tile 1 occupies position (2,0), tile 2 occupies position (1,0), and so on. If we
introduce the type synonym
type Coord = (Nat,Nat)
then the coordinates in tile order are given by

424
Heuristic search
coords::State →[Coord]
coords = tail·map snd ·sort ·addCoords
where addCoords st = zip (perm st) gridpoints
gridpoints
= map (divMod3) [0..8]
Each tile is associated with its coordinate position by addCoords, the result is sorted
into tile order, and the tiles are discarded. The leading position, the position of the
empty space, is also dropped.
Now we can deﬁne the Manhattan heuristic by
h2 ::Heuristic
h2 is fs = sum (zipWith d (coords is) (coords fs))
where d (x0,y0) (x1,y1) = abs (x0 −x1)+abs (y0 −y1)
The mstar algorithm maintains a queue of paths, where a path is deﬁned by
type Path = ([Move],Nat,State)
key::Path →State
key (ms,k,st) = st
Each path records a sequence of moves, the length of the sequence, and the ﬁnal
state at the end of the moves. The algorithm makes use of priority search queues
and the library Data.Set as used in Section 16.2:
mstar ::Heuristic →State →State →Maybe [Move]
mstar h istate fstate =
if possible istate fstate then msearch S.empty start else Nothing
where start = insertQ key ([ ],0,istate) (h istate fstate) emptyQ
msearch vs ps | st == fstate
= Just (reverse ms)
| S.member st vs = msearch vs qs
| otherwise
= msearch (S.insert st vs) rs
where ((ms,k,st),qs) = removeQ key ps
rs = addListQ key (succs h fstate (ms,k,st) vs) qs
The revised deﬁnition of succs is
succs::Heuristic →State →Path →S.Set State →[(Path,Nat)]
succs h fstate (ms,k,st) vs
= [ ((m:ms,k +1,st′),k +1+h st′ fstate)
| m ←moves st,let st′ = move st m,not (S.member st′ vs)]
Both the out-of-place and the Manhattan heuristics ﬁnd a solution much more
quickly than breadth-ﬁrst search, with the Manhattan heuristic proving superior in
many examples. Here for comparison purposes are typical running times with GHCi
and numbers of steps, where bfsolve computes a solution using a simple breadth-ﬁrst
search, in each case returning the same solution [3,6,7,8,5,4,1,0,3,6,7,4,5,8]:

16.5 Chapter notes
425
time
moves
bfsolve
0.60s
6450
mstar h1
0.02s
138
mstar h2
0.01s
35
From the initial state 032871456, there is an even more dramatic improvement in
computing the solution [1,4,3,6,7,4,1,0,3,4,5,2,1,4,5,8,7,6,3,0,1,4,7,8]:
time
moves
bfsolve
920.01s
312963
mstar h1
3.37s
15765
mstar h2
0.41s
2032
16.5 Chapter notes
The ﬁrst description of A* search was given in 1968, see [2], as part of a project for
building a robot that could plan its own actions. A deﬁnitive study of the algorithm
was later given in [1]. A general study of heuristics and how to choose good ones can
be found in Judea Pearl’s book [6]. Applications of heuristic techniques to problems
in artiﬁcial intelligence can be found in [7], among many other books. Priority
search queues are described in [3]. The variable-angle path planning algorithms are
taken from [5]. The 15-puzzle was invented by Noyes P. Chapman, not Sam Loyd,
and a proof that only a half of the initial positions were solvable was ﬁrst given in
1879, see [4].
References
[1]
Rina Dechter and Judea Pearl. Generalised best-ﬁt search strategies and the
optimality of A*. Journal of the ACM, 32(3):505–536, 1985.
[2]
Peter E. Hart, Nils J. Nilsson, and Bertram Raphael. A formal basis for the heuristic
determination of minimum cost paths. IEEE Transactions on Systems Science and
Cybernetics, 4(2):100–107, 1968.
[3]
Ralf Hinze. A simple implementation technique for priority search queues. In B. C.
Pierce and X. Leroy, editors, ACM International Conference on Functional
Programming, pages 110–121, 2001.
[4]
William W. Johnson and William E. Story. Notes on the 15-puzzle. American Journal
of Mathematics, 2(4):397–404, 1879.
[5]
Alex Nash and Sven Koenig. Any-angle path planning. Artiﬁcial Intelligence
Magazine, 34(4):85–107, 2013.
[6]
Judea Pearl. Heuristics: Intelligent Search Strategies for Computer Problem Solving.
Addison-Wesley, Reading, MA, 1984.
[7]
Stuart J. Russell and Peter Norvig. Artiﬁcial Intelligence: A Modern Approach.
Prentice-Hall, Upper Saddle River, NJ, third edition, 2003.

426
Heuristic search
Exercises
Exercise 16.1 Is the function H well-deﬁned if edge costs are not constrained to be
positive? Is H well-deﬁned if edge costs are positive but not necessarily integers?
Exercise 16.2 Consider the graph
A
B
C
0
5
and the optimistic heuristic h(A) = h(B) = 4 and h(C) = 0. Will tstar ﬁnd the path
ABC?
Exercise 16.3 Why is Dijkstra’s algorithm a special case of A* search?
Exercise 16.4 Give a simple graph to show that tstar does not always return a
shortest path if the priority is just the heuristic estimate for completing the journey.
Exercise 16.5 The function insert of Data.Map has type
insert ::Ord k ⇒k →a →Map k a →Map k a
What assumption has been made about the use of insert in astar and mstar?
Exercise 16.6 Is the heuristic that returns the straight-line distance between a town
and the closest goal a monotonic heuristic?
Exercise 16.7 Let the minimum edge cost be c. Is the constant function h(v) = c
optimistic?
Exercise 16.8 Show that, if h(v) = 0 for every goal vertex v, then h is optimistic if
h is monotonic.
Exercise 16.9 Deﬁne f(p) = c(p)+h(v), where v is the endpoint of path p. Show
that, if p is a preﬁx of q, then f(p) ⩽f(q).
Exercise 16.10 How many other ﬁxed-angle paths with 18 straight moves and 5
diagonal moves are there in the grid of Figure 16.3?
Exercise 16.11 Deﬁne the function dist used in the warehouse problem.
Exercise 16.12 Determining whether or not two arbitrary line segments intersect
is a fundamental task in computational geometry. The full algorithm is a little
complicated, as a number of different cases have to be considered. However, in the
warehouse situation the task can be simpliﬁed somewhat, even though a number of
cases still have to be distinguished. First of all, what does one have to show if the
segment of the path under construction is horizontal, vertical, or a diagonal slope at
an angle of 45 degrees?

Exercises
427
Exercise 16.13 Following on from the previous question, in the remaining case we
have to check that the endpoint of the segment is free from obstruction, and that no
border of any box crosses the segment. The borders of the boxes in a grid can be
deﬁned by
borders::Grid →[Segment]
borders = concatMap (edges·corners)·boxes
where edges [u,v,w,x] = [(u,v),(w,v),(x,w),(x,u)]
However, testing all border segments for whether they cross a given segment s will
involve border segments that may be far away from s. It is better to ﬁlter the borders
for those that are near s in some suitable sense. What is a suitable deﬁnition of near?
Exercise 16.14 Following on, the deﬁnition of visible now takes the form
visible::Grid →Segment →Bool
visible g s | hseg s
= all (free g) (ypoints s)
| vseg s
= all (free g) (xpoints s)
| dseg s
= all (free g) (dpoints s)
| eseg s
= all (free g) (epoints s)
| otherwise = free g (snd s) ∧all (not ·crosses s) es
where es = ﬁlter (near s) (borders g)
A segment satisﬁes hseg if it is horizontal, vseg if it is vertical, dseg if the sum of
the two coordinates of the endpoints of the segment is the same, so the diagonal is
left to right, and eseg if the difference of the coordinates is the same. Write suitable
deﬁnitions of the remaining functions, apart from crosses.
Exercise 16.15 It remains to deﬁne crosses. For this we need to determine the
orientation of a triangle. The function
orientation::Segment →Vertex →Int
orientation ((x1,y1),(x2,y2)) (x,y)
= signum ((x−x1)×(y2 −y1)−(x2 −x1)×(y−y1))
returns −1 if the orientation of the triangle ABC, where A = (x1,y1), B = (x2,y2),
and C = (x,y), is anti-clockwise, +1 if ABC is clockwise, and 0 if the points A,
B, and C are collinear. For example, in Figure 16.5 the orientation of ABC is anti-
clockwise and ABD is clockwise. Thus, if CD is a border of some box, then the
segment crosses it. On the other hand, the segment does not cross EF even though
ABE and ABF have opposite orientations. Why will the crossing test not be applied
to EF? Hence deﬁne crosses.

428
Heuristic search
A
B
C
D
E
F
Figure 16.5 ABC and ABE are anti-clockwise, while ABD and ABF are clockwise
Exercise 16.16 Consider the 3-puzzle, in which there are three tiles and a blank in a
2×2 grid. Which of the 24 possible initial states are solvable if the ﬁnal permutation
of the tiles is 1230?
Exercise 16.17 Show that the out-of-place heuristic h for the 8-puzzle is monotonic.
Show also that this would not be true if we counted whether or not the blank tile
was out of place.
Exercise 16.18 Show that the Manhattan heuristic is monotonic, but only because
the distance of the blank tile is not included in the sum.
Exercise 16.19 Deﬁne the functions icparity and mhparity, where icparity returns
True if the parity of the inversion count is even, and mhparity returns True if the
Manhattan distance of the blank tile in the initial state to its resting place in the ﬁnal
state is even.
Answers
Answer 16.1 Not if the graph has cycles with negative costs. As to the second part,
yes, but not if the graph is inﬁnite. Imagine an inﬁnite graph with a single source s,
a single target t, and an inﬁnite number of other vertices vi. There is an edge from s
to vi with cost 1/i and an edge from each vi to t with cost 1. In this case H(s) is not
well-deﬁned.
Answer 16.2 No. After two steps the queue contains ABC (5 + 0),ABA (0 + 4).
The path ABA is chosen for further expansion, and this leads to an inﬁnite loop. The
above graph is disallowed if all edge costs are positive, which is why the assumption
that edge costs are positive is necessary.
Answer 16.3 Because Dijkstra’s algorithm is the special case of A* search when
the heuristic function is h = const 0. This function is both optimistic and monotonic

Answers
429
(provided edge costs are positive), so Dijkstra’s algorithm is also a special case of
M* search.
Answer 16.4 A graph with three vertices sufﬁces:
A
B
C
2
2
5
The source vertex is A and the goal vertex is C. We take h(A) = 4, h(B) = 2, and
h(C) = 0. Tree search will select the path AC with cost 5, but the shortest path is
ABC with cost 4.
Answer 16.5 That Ord Vertex.
Answer 16.6 Yes, in a metric space, monotonicity amounts to the triangle inequality
being satisﬁed.
Answer 16.7 No. If v is a goal, then h(v) = c while H(v) = 0.
Answer 16.8 Let [v,v1,v2,...,vn] be a shortest path from v to a goal vn, with edge
costs [c1,c2,...,cn]. If h is monotonic, then
h(v) ⩽c1 +c2 +···+cn +h(vn) = H(v)
since h(vn) = 0. Note the necessity of h returning 0 on any goal vertex.
Answer 16.9 It is sufﬁcient to show that this holds when p′ is p without its last
vertex. Suppose u is the endpoint of p′ and the edge (u,v) has cost d. Then
f(p′) = c(p′)+h(u) ⩽c(p′)+d +h(v) = f(p)
by monotonicity.
Answer 16.10 We counted 16 paths in total.
Answer 16.11 We have
dist ::Vertex →Vertex →Dist
dist (x1,y1) (x2,y2) = sqrt (fromIntegral (sqr (x2 −x1)+sqr (y2 −y1)))
where sqr x = x×x
Answer 16.12 In each of these three cases the segment goes only through grid
points, so we have to check only that all these points are unimpeded by boxes.

430
Heuristic search
Answer 16.13 We need only consider borders that lie within the rectangular area
determined by s. Hence
near ::Segment →Segment →Bool
near ((x1,y1),(x2,y2)) ((x3,y3),(x4,y4)) = min x1 x2 ⩽x3 ∧x4 ⩽max x1 x2 ∧
min y1 y2 ⩽y3 ∧y4 ⩽max y1 y2
Answer 16.14 We can write
hseg ((x1,y1),(x2,y2)) = x1 == x2
vseg ((x1,y1),(x2,y2)) = y1 == y2
dseg ((x1,y1),(x2,y2)) = x1 +y1 == x2 +y2
eseg ((x1,y1),(x2,y2)) = x1 −y1 == x2 −y2
and also
ypoints ((x1,y1),(x2,y2)) = [(x1,y) | y ←[min y1 y2 ..max y1 y2]]
xpoints ((x1,y1),(x2,y2)) = [(x,y1) | x ←[min x1 x2 ..max x1 x2]]
dpoints ((x1,y1),(x2,y2)) = [(x,x1 +y1 −x) | x ←[min x1 x2 ..max x1 x2]]
epoints ((x1,y1),(x2,y2)) = [(x1 −y1 +y,y) | y ←[min y1 y2 ..max y1 y2]]
Answer 16.15 The segment EF lies outside the rectangle determined by AB and
is therefore excluded from consideration by the near test. The function crosses is
deﬁned by
crosses s (v1,v2) = orientation s v1 ×orientation s v2 ⩽0
Either the two vertices v1 and v2 straddle the segment s, or one of them lies on the
segment.
Answer 16.16 The ﬁnal state is of type EO, so only initial states of type EO or OE
are solvable. These are the 12 permutations:
0123 0231 0312 1023 1203 1230 2031 2301 2310 3012 3102 3120
Answer 16.17 Let u be a state of the 8-puzzle and v be the state that results when
any tile t is exchanged with the blank tile. Such a move changes the value of h(u)
by +1 if t was in its correct place, by 0 if neither the starting point nor the endpoint
of t is its correct place, or −1 if the endpoint of t is its correct place. In all cases we
have h(u) ⩽1+h(v), given that the cost of a move is 1.
If we counted whether or not the empty space was out of place, then a move
could reduce the value of h(u) by 2 if both t and the empty space were now in their
correct places. But that requires h(u) ⩽h(u)−1, which is impossible.

Answers
431
Answer 16.18 A similar argument to the previous question applies. Moving a tile
can increase or decrease the value of the heuristic by 1 (and by −2 if the empty
space is included in the sum and both tiles are in their correct places after the swap).
Answer 16.19 We have
icparity::State →Bool
icparity = even·ic·perm
mhparity::State →State →Bool
mhparity is fs = even (dist (posn0 is) (posn0 fs))
where dist i j = abs (x0 −x1)+abs (y0 −y1)
where (x0,y0) = i divMod 3
(x1,y1) = j divMod 3
The function ic for computing the inversion count was deﬁned in Section 7.2.

Index
Abelian group, 107, 114
abs, 46
abstraction function, 44, 49, 53
accumulating parameter, 14–16, 85, 99, 112, 382
accumulating function, 15, 16, 34, 39, 57, 99, 279,
280, 302
acyclic graph, 206, 229, 231
directed, see directed acyclic graph
Adamaszek, Michal, 166
Adelson-Velski˘ı, Georgy M., 84
adversarial argument, 137, 138
Ahrens, Joachim H., 257
Aigner, Martin, 228
algebraic ring, 107
all, 17, 19
amoeba ﬁght show, 343, 358
amortised complexity, 34–38, 40, 42, 46, 51
and, 261
Andersson, Arne, 84
Applicative class, 5
apply, 18, 144, 190
array, 51–53, 55, 58
accum, 55
accumArray, 52, 105
array, 51, 109
assocs, 53
elems, 53, 105
immutable, 51, 212
listArray, 52, 212
mutable, 95, 212, 367
association list, 52
associative operation, 21, 122
A*, 220, 410–412, 415, 425, 426, 428
asymptotic notation, 25–27, 38
Augustsson, Lennart, 110
average-case complexity, 27
AVL tree, 84
Backhouse, Roland, 84
balanced tree, 75–81, 84, 87, 91, 96, 97, 101–102, 113,
117, 270, 282, 355
Ball, Walter William Rouse, 394
Baum, Eric B., 394
Bellman, Richard, 311, 326
Bentley, Jon, 84
Berlekamp, Elwyn, 110
big omega, 25
big omicron, 25, 38–41
big theta, 25, 38–41
binary comparisons, 80
binary search, 63–67, 85, 177, 270
binary search tree, 63, 73–81, 85–86, 88–90, 94, 270,
282
optimum, 336, 347–349, 358
binary tree, 47, 96, 177, 188, 197, 335
perfect, see perfect binary tree
search, see binary search tree
unordered, see unordered binary tree
binomial coefﬁcient, 314–316
bit vector, 373–375, 394, 395, 398
Blum, Manuel, 135
Bool type, 5
bottom-up algorithm, 98, 123, 178, 199, 203, 311
Bottom-up Mergesort, see Mergesort
Bounded class, 104
boustrophedon product, 345, 358, 360, 363
breadth-ﬁrst search, 369, 372, 378–386, 389, 393,
396–397, 400–401, 405, 409, 412, 424
Bridge hand, 112
Bubble sort, 173
Bucketsort, 102–106, 113–114, 117–118
Bunkenburg, Alexander, 166
Cai, Xuan, 166
Catalan numbers, 200
Chao, Kun-Mao, 281
Chapman, Noyes P., 425
coin-changing, 151–156, 165, 167–168, 173–175,
248–251, 259, 263–264, 330, 333
collapse, 14
Collette, S´ebastien, 394
commutative operation, 122

Index
433
compact sorting, 94
compare, 100, 109
comparing, 112
complexity
average-case, see average-case complexity
worst-case, see worst-case complexity
concat, 8, 30
concatMap, 10
concrete mathematics, 38
connected graph, 206, 229–231
cons, 9, 43
Cormen, Thomas H., 84
Countsort, 117, 133
Crochemore, Maxime, 281
Curtis, Sharon, 257, 281
cycle, 206, 224, 229–231, 428
data compression, 289
data declarations, 5
data structure, 36
destructive, 51
persistent, 51
Data.Array library, 51, 55, 105
Data.Bits library, 374, 395
Data.List library, 5, 17, 47, 99, 397
Data.Map library, 410, 426, 429
Data.Ord library, 112
Data.Sequence library, 47
Data.Set library, 412
Data.Text library, 421
Data.Word library, 6, 374
Dechter, Rina, 425
decimal fractions, 156–161, 165, 168–169, 175–176,
239
decision tree, 73, 80
Demaine, Erik D., 111
Denardo, Eric V., 326
dependency graph, 311, 313–333
depth-ﬁrst search, 369, 372, 373, 378–386, 393, 396,
400, 405, 409, 412
de Moor, Oege, 166, 257, 300
Dick, Philip K., 54
dictionary, 74
digraph, see directed graph
Dijkstra’s algorithm, 220–225, 228, 230, 233–235, 244,
248, 263, 405, 426, 428
Dijkstra, Edsger W., 84, 228, 281
directed acyclic graph, 244, 251, 311, 313, 373, 379
directed graph, 205, 230, 372
acyclic, see directed acyclic graph
discriminator function, 103, 106
disjoint sets, see union–ﬁnd
distributive law, 147, 163, 166, 169, 170, 176, 275, 293,
317
of thinning, see thinning, distributive law
divide-and-conquer, 11, 29, 61–140, 167, 171, 178,
229, 232, 239, 268, 311
Double type, 5
Dreyfus, Stuart, 326
drop, 17, 19
dropWhile, 17, 19, 34
dropWhileEnd, 17
dynamic array, 40, 42
dynamic programming, 239, 248, 252, 271, 311–364,
367
dynamic set, 81–84, 86–87, 90–91
eager evaluation, 28
8-puzzle, see sliding-block puzzle
elem, 17, 19
Engelhardt, Matthias R., 394
Enum class, 104
EQ, 100
Eq class, 5, 22
equational reasoning, xiii, xiv, 146, 162
error messages, 45, 49, 55, 57
Euclidean distance, 416, 418
exhaustive search, 239, 367–431
expression problem, 376–378, 395–396, 398–400
extend, 148
factor, see segment
Feijen, Wim H. J., 281
fetch, 47
ﬁb, 75, 313
Fibonacci, 75, 313–314, 327, 330–331
15-puzzle, see sliding-block puzzle
Filliˆatre, Jean-Christophe, 359
ﬁlter, 6, 17, 20
Finke, Gerd, 257
Fischer, Michael J., 326
ﬁssion, 185, 278
Flake, Gary W., 394
ﬂatten, 55, 74, 85, 95, 96, 101, 104, 111
ﬂip, 7
Float type, 5
ﬂoating-point numbers, 5
Floyd, Robert W., 110, 135
Foldable class, 5, 20
foldl, 7, 17, 18, 290
foldl1, 121, 180
foldr, 6, 17, 18, 20, 144, 146, 168, 174, 248, 268, 290
foldr1, 121, 145, 180
foldrn, 180, 295
folds, 6–9
forest, 206
spanning, see spanning forest
Forsyth, Mark, 16
Fredman, Michael L., 110, 111
fringe (of a tree), 178, 189, 335, 336
functional composition, 8, 13
fusion, xiv, 11–14, 144, 241, 246, 248, 254–256, 268,
269, 272, 273, 276, 277, 292, 295, 371, 377
condition, 12, 146, 158, 162, 181, 182, 246, 250,
256, 259, 264, 294, 300
context-sensitive, 18, 22, 149, 181, 291
law for foldl, 19, 23, 39, 41
law for foldrn, 181, 185, 200

434
Index
law for foldr, 12, 18, 39, 41, 146, 157, 164, 168, 175,
276, 278
law for until, 112, 115
Garsia, Adriano M., 359
Garsia–Wachs algorithm, 199, 344, 349–358, 361–362,
364
Gauss, Carl Friedrich, 393
GHCi, 16, 28, 31, 393, 424
Gilbert, Edgar N., 359
Golden Ratio, 75, 327, 330
Graham, Ronald L., 38, 84, 228
graph, 205–235
acyclic, see acyclic graph
adjacency representation, 206, 229, 232, 406
connected, see connected graph
directed, see directed graph
directed acyclic, see directed acyclic graph
labelled, see labelled graph
undirected, see undirected graph
greedy algorithm, 9, 11, 22, 143–235, 239, 246, 248,
260, 265, 268, 272, 287, 292–294, 296–298, 301,
304, 306, 335, 344, 367
greedy condition, 161, 192–194, 216, 293, 297, 301,
306, 321, 328
Gries, David, 54, 84, 281
group, 126
GT, 100
Gusﬁeld, Dan, 280, 281, 326
halve, 97
Hancart, Christof, 281
Harper, Lawrence H., 111
Hart, Peter E., 425
Haskell, xiii, 3, 5, 16, 104, see also GHCi, Hugs
Haskell Platform, 16
head, 20
heap, 101, 197
condition, 197
leftist, see leftist heap
maxiphobic, see maxiphobic heap
skew, see skew heap
Heapsort, 101–102, 110, 113–114, 116–117
height (of a tree), 74, 76, 86, 89, 177
Hell, Pavol, 228
heuristic
admissible, see optimistic heuristic
monotonic, see monotonic heuristic
optimistic, see optimistic heuristic
heuristic search, 369, 405–431
hillclimbing algorithm, 143
Hinze, Ralf, 425
Hoare, Charles A. R., 110, 111, 135
Hoffman, Eric J., 394
Hood, Robert, 54
Hoogerwoord, Rob R., 54
Hu, Te Chiang, 199, 358, 359
Hu–Tucker algorithm, 199, 358
Huet, G´erard, 84
Huffman coding, 187–196, 200–201, 203–204, 208,
336, 344, 348, 349, 358
Huffman, David A., 199
Hughes, John, 16
Hugs, 28
hyperbaton, 16
identity element, 21
imperative programming, 7
implicit search, 369–375
in-place algorithm, 95, 102
incrementing a binary integer, 35, 50
indirect equality, 87
induction, 75
inductive deﬁnitions, 9
inﬁnite list, 6, 9, 14, 18, 47
inﬁnite loop, 127
inits, 17, 19, 33, 39, 42, 46, 274, 283, 286
Insertion sort, 11, 150, 165, 167, 173
Int type, 5, 169, 175, 218, 398
Integer type, 5, 313, 398
inversion count, 147–149, 165, 420, 423, 428, 431
iterate, 35
Ix class, 52, 105
Jarn´ık, Vojtˇech, 228
Jiang, Tao, 281
jogger’s problem, 224–228, 231, 235
Johnson, William W., 425
Karpinski, Marek, 359
Kingston, Jeffrey H., 359
knapsack problem, 252–255, 259–260, 264–265, 268,
313, 316–319, 328, 331
0/1, 253
fractional, 253, 260, 265
integer, 253, 260, 265
Knuth, Donald E., 16, 38, 84, 110, 111, 165, 166, 199,
300, 358, 359, 394
Koenig, Sven, 425
Kruskal’s algorithm, 208–215, 228, 230, 232–233
Kruskal, Joseph B., 228
label, 6
labelled graph, 206, 229
Lambert’s algorithm, 107
Lambert, Jean-Luc, 107, 111
Landis, Evgenii M., 84
Larmore, Lawrence L., 359
LaValle, Steven M., 394
layered network, 244–248, 251, 258–259, 262–263,
311, 318
lazy evaluation, 8, 23, 28, 32, 93, 111, 124, 173
Lecroq, Thierry, 281
leftist heap, 197
Leiserson, Charles E., 84
length, 7
lexical order, 102, 103, 132, 149, 153, 155, 160, 161,
173, 174, 182, 272
lhs2TEX, xvi
Lin, Yaw-Ling, 281

Index
435
linear search, 63, 203, 270
linear time, 27
linearithmic time, 67, 101, 113
list comprehension, 10
list indexing, 125
list of successes, 369
Loessi, J. C., 394
logarithmic time, 64
longest common subsequence, 270–274, 280–283,
285–286, 313, 319, 322–323, 329, 332–333
longest upsequence, 267–270, 281–285, 329
loop invariant, xiii
lower median, 124
Loyd, Sam, 419, 425
LT, 100
Lunar Landing, 383–386, 390, 394, 397, 403
Lunar Lockout, see Lunar Landing
Manhattan distance, 421, 423, 428, 431
map, 6, 17, 20
Marlow, Simon, 61
Martello, Silvano, 257
maxBound, 104
maximum, 17, 19, 121
maximum-sum segment, 283, 287
maximum-sum short segment, 281
maxInt, 218, 263
maxiphobic heap, 199
median, 121, 124, 126
Meertens, Lambert, 359
Melville, Robert, 54
memoisation, 15, 311
merge, 90, 96, 109, 128
Mergesort, 96–100, 110, 112, 114–115, 177, 178, 289,
301
Bottom-up, 98, 112, 115
Natural, 99
Smooth, 99, 106
Straight, 98
minBound, 104
minimum, 17, 19, 121
minimum edit distance, 313, 319–322, 326, 328–329,
331–332
minimum-cost spanning tree, 205, 207, 228
minimum-height tree, 177–187, 199–203
minimum-sum short segment, 274–280
MinWith, 162
minWith, 145
Mitchell, Joseph S. B., 111
Monad class, 5
monad, 51
monolithic updates, 51
monotonic heuristic, 410–415, 426, 428–429
monotonicity, 106, 107, 150, 162, 163, 167, 172, 184,
262, 307, 311, 317, 320, 328, 330, 333, 339–341,
345–347, 349, 358, 360–363, 367, 377, 395, 398,
399
Moore, Edward. F, 359
Moore, Robert C., 394
Morris, Joseph M., 166
Mu, Shin-Cheng, 257, 281
n-queens problem, 369–375, 377, 393, 395, 398
Nash, Alex, 425
Nat type, 6
Natural Mergesort, see Mergesort
Navarro, Gonzalo, 326
navigating a warehouse, 415–419, 426–427, 429–430
Niewiarowska, Anna, 166
Nilsson, Nils J., 425
nondeterministic function, 161–165, 169–170, 176,
241
Nordin, Thomas, 110
normal form, 27
Norvig, Peter, 425
null, 17, 19
Num class, 5
Numeric.Natural library, 6
O, see big omicron
O’Rourke, Joseph, 111
ofﬂine algorithm, 9
Okasaki, Chris, xv, 54, 199
Ω, see big omega
online algorithm, 9, 46
Open Problems Project, 110
optimistic heuristic, 405–411, 426, 428–429
optimum bracketing, 335–364, see also amoeba ﬁght
show, boustrophedon product
addition, 341
cartesian sums, 344
concatenation, 341
Huffman coding (restricted), 344
matrix multiplication, 335, 337, 343, 358
multiplication, 342
or, 261
Ord class, 5, 93
Ordering type, 100
paragraph problem, 8, 294–299, 301–303, 306–308
partial order, 242
partition, 277, 289–308
partition (by a predicate), 76, 86, 89, 94
partition3 (into three parts), 89, 125
parts (all partitions), xiv, 289–291, 300, 303
ptn (into buckets), 103, 105
partition, 133
Patashnik, Oren, 38, 84
Paterson, Michael S., 135
path, 206
Payne, Thomas H., 111
Pearl, Judea, 425
perfect binary tree, 48
permutation, 9–11, 31, 34, 38, 39, 41, 42, 80, 147, 396,
420
perms, xiv, 9, 10, 18, 148, 167, 370, 371
picks, 10, 150, 190, 209
planning, 369, 386–393, 397–398, 403, 425
Plass, Michael F., 300

436
Index
Pohl, Ira, 135
point-free reasoning, 14
point-wise reasoning, 13
Pratt, Vaughan, 135
preﬁx, see inits
preﬁx-closed, 291, 292, 295, 300, 304, 306
preﬁx-free code, 188
Prelude library, 5
preorder, 241, 246, 249, 254, 269, 272, 276, 281, 283,
294, 299
Preusser, Thomas B., 394
Prim’s algorithm, 215–219, 228, 230–233
Prim, Robert C., 228
priority queue, 102, 196–199, 201, 204, 405, 406
priority search queue, 406, 414, 425
Pr¨omel, Hans J¨urgen, 228
ptails, 325
quadrangle inequality, 340, 345, 347, 349, 358, 360,
363
quadratic time, 27
queens problem, see n-queens problem
queue, 194, 381
priority, see priority queue
QuickCheck, xvi
Quicksort, 94–96, 110–112, 114–115, 125, 133
Rabe, Florian, 166
Radixsort, 102–106, 110, 113–114, 117–119
random-access list, 47–51, 54–55, 57–58, 214
Raphael, Bertram, 425
Raskin, Jean-Franc¸ois, 394
Rational type, 5
Rausch, John, 394
recurrence relation, 28–32
recursion, 7
red–black tree, 84
reduction steps, 27
reﬁnement, 144, 161–165, 241
representation invariant, 44
reverse, 17, 20, 112
Revilla, Miguel A., 326
Richards, Martin, 394
ring, see algebraic ring
Rivest, Ronald L., 84, 135
rose tree, 103
rotation, 77, 78
rule of ceilings, 85, 87, 117, 201
rule of ﬂoors, 85, 87, 157, 168, 169, 175
running time, 27
runs, 98
Rush Hour, 384, 389–394, 397, 403
Russell, Stuart J., 425
Rytter, Wojciech, 281, 359
Saddleback search, 68–69, 84
Savage, John E., 111
Scan Lemma, 21, 275, 276
scanl, 9, 275, 283, 301
scanr, 21, 275, 283
scanr1, 316
scheduling problem, 291–294, 299–301, 304–306
search
binary, see binary search
linear, see linear search
two-dimensional, see two-dimensional search
segment, 267, 274–281, 283, 286–287, 289
initial, 267, 290
tail, 267, 290
Selection, 121–140
Selection sort, 11, 151, 165
Servais, Frederic, 394
set
combination, 81–82
deletion, 81
insertion, 81
membership, 81
splitting, 81–84
union, 81–82, 86
Shing, Man-Tak, 358, 359
shortest paths, 219–228, 230–231, 233–251
shortest-paths spanning tree, 219, 220, 226, 230
show, 106
shuttle-bus problem, 313, 323–326, 330, 333
single, 17, 20, 98
size (of a tree), 74, 86, 89, 97, 177, 335
skew heap, 199
skew tree, 84
Skiena, Steven S., 326
sliding-block puzzle, 419–425, 428, 430–431
Sloane, Neil, 394
smart constructor, 48, 76, 77, 101, 186, 195, 197
Smooth Mergesort, see Mergesort
smooth sorting, 94, 99, 100
snoc, 43, 105
sortBy, 100, 112, 116
sorting, 52, 80, 86, 93–119, 143, 147–151, 177, 396,
400, see also Bubble sort, Bucketsort, compact
sorting, Countsort, Heapsort, Insertion sort,
Mergesort, Quicksort, Radixsort, Selection sort,
smooth sorting, stable sorting
sorting sums, 106–110, 114, 119, 344
sortOn, 100, 113, 116, 210
span, 17, 19
spanning forest, 207, 229, 230, 233
spanning tree, 205–219, 229–230, 232–233
minimum-cost, see minimum-cost spanning tree
shortest-paths, see shortest-paths spanning tree
spine (of a tree), 180
splitAt, 17, 19
stable sorting, 94, 95, 97, 99, 100, 104, 105
stack, 381
steep, 19, 23
Steger, Angelika, 228
Stein, Clifford, 84
step-by-step algorithm, 143
Stirling number, 327, 331
Stirling’s approximation, 40, 80, 86

Index
437
Story, William E., 425
Straight Mergesort, see Mergesort
Straus, Ernst G., 111
stream, see inﬁnite list
strict function, 22, 58, 171
strictly increasing function, 63
stringology, 267, 280, 326
structural recursion, 9
subseqs, 257
subsequence, 241, 242, 250, 267–274, 281–286
substring, see segment
sufﬁx, see tails
sufﬁx-closed, 291, 292, 295, 300, 304
symmetric list, 43–47, 54, 56–57, 105, 194, 201, 279
tabulation, 311, 313–333, 338
tails, 17, 19, 33, 40, 42, 274, 283, 286
proper, see ptails
take, 17, 19
takeWhile, 17, 19, 20, 26
Tarjan, Robert E., 135, 228, 358
tennis tournament, 122
TEX problem, see decimal fractions
text processing, 289
Θ, see big theta
ThinBy, 241
thinning
distributive law, 243, 246, 250, 258, 262, 269, 286
elimination law, 243, 258, 262
ﬁlter law, 244, 247, 258, 262, 269, 286
idempotency law, 243, 258, 261, 262
identity law, 242, 257, 261, 262
introduction law, 243, 246, 254, 256, 258, 262, 268,
273
map law, 243, 247, 258, 262, 269, 286
thinning algorithm, 11, 239–308, 367
3-puzzle, see sliding-block puzzle
top-down algorithm, 178, 311
Toth, Paolo, 257
transposition, 420
Traversable class, 5
tree, 206, 311
AVL, see AVL tree
balanced, see balanced tree
binary, see binary tree
decision, see decision tree
red–black, see red–black tree
rose, see rose tree
skew, see skew tree
spanning, see spanning tree
triangle inequality, 429
T*, 406–409, 411, 426, 429
Tucker, Alan C., 358, 359
tupling, 14–16, 97
law, 122
two-dimensional search, 67–73, 85, 88, 107
two-sorted, 353–355, 364
type class, 5
type synonym, 5
Tyrrell, Malcolm, 166
uncons, 17, 20
uncurry, 55, 82, 204
undeﬁned value (⊥), 8, 22, 45
undirected graph, 205, 230
Unicode, 104, 421
union, 90
union–ﬁnd, 211–215, 228
unordered binary tree, 189
until, 11, 98, 144
unwrap, 17, 20, 98
Vandermonde’s convolution, 73
von Neumann, John, 110
Wachs, Michelle L., 359
Wadler, Philip L., 54
Wagner, Robert A., 326
while, 11
Williams, John W. J., 110, 111
worst-case complexity, 27
wrap, 17, 20, 98, 180
Yamamoto, Hiroshi, 384
Yao, Frances F., 358, 359
Yoshigahara, Nobuyuki, 384
Young tableau, 107
Zantema, Hans, 281, 299
Ziegler, G¨unter M., 228
Zipper, 84
zipWith, 17, 19
Zongyan, Qiu, 394


Machine	  Learning	  Specializa0on	  
Multiple Regression: 
Linear regression with multiple features 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
1 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
2	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
y 
x 
ŷ 
ŵ 

Machine	  Learning	  Specializa0on	  
3	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Simple linear regression model 
yi = w0+w1 xi + εi 
f(x) = w0+w1 x 

Machine	  Learning	  Specializa0on	  
More complex functions 
of a single input 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
Polynomial regression 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
6	  
Fit data with a line or … ?  
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
You show 
your friend 
your analysis 

Machine	  Learning	  Specializa0on	  
7	  
Fit data with a line or … ?  
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Dude, it’s 
not a linear 
relationship! 

Machine	  Learning	  Specializa0on	  
8	  
What about a quadratic function? 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Dude, it’s 
not a linear 
relationship! 

Machine	  Learning	  Specializa0on	  
9	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
f(x) = w0 + w1
 x+ w2
 x2 
What about a quadratic function? 

Machine	  Learning	  Specializa0on	  
10	  
Even higher order polynomial 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
f(x) = w0 + w1
 x+ w2
 x2 + … + wp
 xp  

Machine	  Learning	  Specializa0on	  
11	  
Polynomial regression 
Model: 
yi = w0 + w1
 xi+ w2
 xi
2 + … + wp
 xi
p + εi 
 
 
 
 
feature 1 = 1 (constant) 
feature 2 = x 
feature 3 = x2 
… 
feature p+1 = xp 
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
treat as diﬀerent features 
parameter 1 = w0 
parameter 2 = w1 
parameter 3 = w2 
… 
parameter p+1 = wp 
 

Machine	  Learning	  Specializa0on	  
Other functions of one input 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
13	  
11.5
12.0
12.5
13.0
13.5
14.0
Month
log(Price)
1997−01
1999−01
2001−01
2003−01
2005−01
2007−01
2009−01
2011−01
2013−01
Motivating application: 
Detrending time series 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
House sales recorded monthly 
yi = $ of ith house sale 
ti = month of ith house sale 

Machine	  Learning	  Specializa0on	  
14	  
11.5
12.0
12.5
13.0
13.5
14.0
Month
log(Price)
1997−01
1999−01
2001−01
2003−01
2005−01
2007−01
2009−01
2011−01
2013−01
Trends over time 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
On average, house prices 
tend to increase with time 
 

Machine	  Learning	  Specializa0on	  
15	  
11.5
12.0
12.5
13.0
13.5
14.0
Month
log(Price)
1997−01
1999−01
2001−01
2003−01
2005−01
2007−01
2009−01
2011−01
2013−01
Seasonality 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Most	  houses	  listed	  in	  summer	  
+	  	  
Good	  houses	  sell	  quickly 
Few	  homes	  listed	  in	  Nov./Dec.	  
+	  	  
Transac0ons	  oSen	  leSover	  inventory	  
or	  special	  circumstances 

Machine	  Learning	  Specializa0on	  
16	  
An example detrending 
Model: 
yi = w0 + w1
 ti + w2
 sin(2πti / 12  -  Φ) + εi 
 
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Linear trend 
Seasonal component =  
Sinusoid with period 12  
(resets annually) 
Unknown phase/shift 
Trigonometric identity: sin(a-b)=sin(a)cos(b)-cos(a)sin(b) 
à sin(2πti / 12  -  Φ) = sin(2πti / 12)cos(Φ) - cos(2πti / 12)sin(Φ)  

Machine	  Learning	  Specializa0on	  
17	  
An example detrending 
Equivalently,  
yi = w0 + w1
 ti + w2
 sin(2πti / 12)  
                        + w3
 cos(2πti / 12) + εi 
 
 
feature 1 = 1 (constant) 
feature 2 = t 
feature 3 = sin(2πt/12) 
feature 4 = cos(2πt/12) 
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
18	  
11.5
12.0
12.5
13.0
13.5
14.0
Month
log(Price)
1997−01
1999−01
2001−01
2003−01
2005−01
2007−01
2009−01
2011−01
2013−01
Detrended housing data 
Fit polynomial trend and  
sinusoidal seasonal component 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
5th order polynomial 
+  
sin/cos basis 

Machine	  Learning	  Specializa0on	  
19	  
Zoom in… 
Fit polynomial trend and  
sinusoidal seasonal component 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
5th order polynomial 
+  
sin/cos basis 

Machine	  Learning	  Specializa0on	  
20	  
Other examples of seasonality 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Weather modeling 
(e.g., temperature, rainfall) 
Demand forecasting 
(e.g., jacket purchases) 
Motion capture data 
Flu monitoring 

Machine	  Learning	  Specializa0on	  
More generally… 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
22	  
Generic basis expansion 
Model: 
yi = w0h0(xi) + w1
 h1(xi) + … + wD
 hD(xi)+ εi 
     
    =      wj
 hj(xi) + εi 
 
feature 1 = h0(x)…often 1 
feature 2 = h1(x)… e.g., x 
feature 3 = h2(x)… e.g., x2 or sin(2πx/12) 
… 
feature p+1 = hp(x)… e.g., xp
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
jth regression coeﬃcient  
           or weight 
jth feature 
D
X
j=0

Machine	  Learning	  Specializa0on	  
23	  
Generic basis expansion 
Model: 
yi = w0h0(xi) + w1
 h1(xi) + … + wD
 hD(xi)+ εi 
     
    =      wj
 hj(xi) + εi 
 
feature 1 = h0(x)…often 1 (constant) 
feature 2 = h1(x)… e.g., x 
feature 3 = h2(x)… e.g., x2 or sin(2πx/12) 
… 
feature D+1 = hD(x)… e.g., xp
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
D
X
j=0

Machine	  Learning	  Specializa0on	  
24	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
y 
x 
ŷ 
ŵ 
x 
h(x) 

Machine	  Learning	  Specializa0on	  
Incorporating multiple inputs 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
26	  
Predictions just based on  
house size 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Only 1 bathroom! 
Not same as my  
3 bathrooms 

Machine	  Learning	  Specializa0on	  
27	  
Add more inputs 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x[1] 
y 
x[2] 
f(x) = w0 + w1
 sq.ft. 
       + w2
 #bath 

Machine	  Learning	  Specializa0on	  
28	  
Many possible inputs 
- Square feet 
- # bathrooms 
- # bedrooms 
- Lot size 
- Year built 
- … 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
29	  
Reading your mind 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
very sad 
very happy 
Features are 
brain region 
intensities 

Machine	  Learning	  Specializa0on	  
30	  
General notation 
Output: y 
Inputs: x = (x[1],x[2],…, x[d]) 
 
 
Notational conventions: 
 x[j] = jth input (scalar) 
 hj(x) = jth feature (scalar) 
 xi = input of ith data point (vector) 
 xi[j] = jth input of ith data point (scalar) 
 
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
d-dim vector 
scalar 

Machine	  Learning	  Specializa0on	  
31	  
Simple hyperplane 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Model: 
yi = w0 + w1
 xi[1] + … + wd
 xi[d] + εi 
     
feature 1 = 1 
feature 2 = x[1] … e.g., sq. ft. 
feature 3 = x[2] … e.g., #bath 
… 
feature d+1 = x[d] … e.g., lot size 

Machine	  Learning	  Specializa0on	  
32	  
More generically… 
D-dimensional curve 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Model: 
yi = w0 h0(xi) + w1
 h1(xi) + … + wD
 hD(xi) + εi 
 
    =      wj
 hj(xi) + εi 
     
 
feature 1 = h0(x) … e.g., 1 
feature 2 = h1(x) … e.g., x[1] = sq. ft. 
feature 3 = h2(x) … e.g., x[2] = #bath 
  
  
 
              or, log(x[7]) x[2] = log(#bed) x #bath 
… 
feature D+1 = hD(x) … some other function of x[1],…, x[d] 
D
X
j=0

Machine	  Learning	  Specializa0on	  
33	  
More on notation 
# observations (xi,yi) : N 
# inputs x[j] : d 
# features hj(x) : D 
 
 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
Interpreting the ﬁtted function 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
35	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Interpreting the coeﬃcients – 
Simple linear regression 
ŷ = ŵ0 + ŵ1
 x 
 
1 sq. ft. 
predicted 
change in $ 

Machine	  Learning	  Specializa0on	  
36	  
ﬁx 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Interpreting the coeﬃcients – 
Two linear features 
square feet 
(sq.ft.) 
price ($) 
x[1] 
y 
x[2] 
ŷ = ŵ0 + ŵ1
 x[1] + ŵ2
 x[2] 
 

Machine	  Learning	  Specializa0on	  
37	  
ﬁx 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Interpreting the coeﬃcients – 
Two linear features 
ŷ = ŵ0 + ŵ1
 x[1] + ŵ2
 x[2] 
 
# bathrooms 
price ($) 
x[2] 
1 bathroom 
predicted 
change in $ 
y 
For ﬁxed  
# sq.ft.! 

Machine	  Learning	  Specializa0on	  
38	  
ﬁx 
ﬁx 
ﬁx 
ﬁx 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Interpreting the coeﬃcients – 
Multiple linear features 
ŷ = ŵ0 + ŵ1
 x[1] + …+ŵj
 x[j] + … + ŵd
 x[d] 
 
 
square feet 
(sq.ft.) 
price ($) 
x[1] 
y 
x[2] 

Machine	  Learning	  Specializa0on	  
39	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Interpreting the coeﬃcients- 
Polynomial regression 
ŷ = ŵ0 + ŵ1x +… + ŵj
 xj + … + ŵp
 xp 
 
 
square feet 
(sq.ft.) 
price ($) 
x 
y 
Can’t hold 
other features 
ﬁxed! 

Machine	  Learning	  Specializa0on	  
Fitting D-dimensional curves 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
41	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
ŵ 
y 
ŷ 
h(x) 

Machine	  Learning	  Specializa0on	  
Step 1: 
Rewrite the regression model 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
43	  
Rewrite in matrix notation 
For observation i 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
 
     
yi =      wj
 hj(xi) + εi 
 
 
    =   
  
 
 + εi  
  
   
yi 
D
X
j=0
=  
  
       + εi  
 

Machine	  Learning	  Specializa0on	  
44	  
Rewrite in matrix notation 
For all observations together 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
= 
+ 

Machine	  Learning	  Specializa0on	  
Step 2: 
Compute the cost 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
46	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
y 
h(x) 
ŷ 
ŵ 

Machine	  Learning	  Specializa0on	  
47	  
“Cost” of using a given line 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Residual sum of squares (RSS) 
RSS(w0,w1) =      (yi-[w0+w1xi])2 

Machine	  Learning	  Specializa0on	  
48	  
RSS for multiple regression 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
y 
RSS(w) =      (yi-                 )2 
x[1] 
x[2] 
ŷi  = 

Machine	  Learning	  Specializa0on	  
49	  
RSS in matrix notation 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
RSS(w) =      (yi- h(xi)Tw)2 
 
  
 =  
Why? (part 1) 

Machine	  Learning	  Specializa0on	  
50	  
RSS in matrix notation 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
RSS(w) =      (yi- h(xi)Tw)2 
 
  
 = (y-Hw)T(y-Hw) 
residual1 
residual2 
residual3 
… 
residualN  
residual1 residual2 residual3 
… 
residualN 
=  
Why? (part 2) 

Machine	  Learning	  Specializa0on	  
Step 3: 
Take the gradient 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
52	  
Gradient of RSS 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
RSS(w) =    [(y-Hw)T(y-Hw)] 
  
 = -2HT(y-Hw) 
  Δ 
  Δ 
Why? By analogy to 1D case:  

Machine	  Learning	  Specializa0on	  
Step 4, Approach 1: 
Set the gradient = 0 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
54	  
Closed-form solution 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
RSS(w) = -2HT(y-Hw) = 0 
  Δ 
Solve for w: 

Machine	  Learning	  Specializa0on	  
55	  
Closed-form solution 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
ŵ = ( HTH )-1 HTy 
Invertible if: 
 
 
Complexity of inverse: 

Machine	  Learning	  Specializa0on	  
Step 4, Approach 2: 
Gradient descent 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
57	  
Gradient descent 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
 
while not converged 
 w(t+1) ß w(t) - η   RSS(w(t))  
  Δ 
-2HT(y-Hw) 

Machine	  Learning	  Specializa0on	  
58	  
Feature-by-feature update 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
RSS(w) =      (yi- h(xi)Tw)2 
Partial with respect to wj  
wj
(t+1) ß wj 
(t) - η(                       ) 
Update to jth feature weight: 

Machine	  Learning	  Specializa0on	  
59	  
Interpreting elementwise 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
wj
(t+1) ß wj
(t) + 2η    hj(xi)(yi-ŷi(w(t)))  
Update to jth feature weight: 
square feet 
(sq.ft.) 
price ($) 
y 
x[1] 
x[2] 

Machine	  Learning	  Specializa0on	  
60	  
Summary of gradient descent 
for multiple regression 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
init w(1)=0 (or randomly, or smartly), t=1 
while ||    RSS(w(t))|| > ε  
 for j=0,…,D 
 partial[j] =-2    hj(xi)(yi-ŷi(w(t))) 
 wj
(t+1) ß wj
(t) – η partial[j] 
     t ß t + 1 
 
  Δ 

Machine	  Learning	  Specializa0on	  
62	  
An extremely useful algorithm 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
Summary for  
multiple linear regression 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
64	  
What you can do now… 
• 
Describe polynomial regression 
• 
Detrend a time series using trend and seasonal components 
• 
Write a regression model using multiple inputs or features 
thereof 
• 
Cast both polynomial regression and regression with multiple 
inputs as regression with multiple features 
• 
Calculate a goodness-of-ﬁt metric (e.g., RSS) 
• 
Estimate model parameters of a general multiple regression 
model to minimize RSS: 
-  In closed form 
-  Using an iterative gradient descent algorithm 
• 
Interpret the coeﬃcients of a non-featurized multiple 
regression ﬁt 
• 
Exploit the estimated model to form predictions 
• 
Explain applications of multiple regression beyond house 
price modeling 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  


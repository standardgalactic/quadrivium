Vector Calculus (Maths 214)
Theodore Voronov
January 20, 2003
Contents
1
Recollection of diﬀerential calculus in Rn
3
1.1
Points and vectors
. . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Velocity vector
. . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.3
Diﬀerential of a function . . . . . . . . . . . . . . . . . . . . .
9
1.4
Changes of coordinates . . . . . . . . . . . . . . . . . . . . . .
15
2
Line integrals and 1-forms
20
3
Algebra of forms
24
3.1
Jacobian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
3.2
Rules of exterior multiplication
. . . . . . . . . . . . . . . . .
26
4
Exterior derivative
27
4.1
Dependence of line integrals on paths . . . . . . . . . . . . . .
27
4.2
Exterior derivative: construction . . . . . . . . . . . . . . . . .
27
4.3
Main properties and examples of calculation . . . . . . . . . .
28
5
Stokes’s theorem
29
5.1
Integration of k-forms
. . . . . . . . . . . . . . . . . . . . . .
29
5.2
Stokes’s theorem: statement and examples . . . . . . . . . . .
34
5.3
A proof for a simple case . . . . . . . . . . . . . . . . . . . . .
39
6
Classical integral theorems
41
6.1
Forms corresponding to a vector ﬁeld . . . . . . . . . . . . . .
41
6.2
The Ostrogradski–Gauss and classical Stokes theorems
. . . .
46
Introduction
Vector calculus develops on some ideas that you have learned from elementary
multivariate calculus. Our main task is to develop the geometric tools. The
central notion of this course is that of a diﬀerential form (shortly, form).

THEODORE VORONOV
Example 1. The expressions
2dx + 5dy −dz
and
dxdy + exdydz
are examples of diﬀerential forms.
In fact, the former expression above is an example of what is called a
“1-form”, while the latter is an example of a “2-form”. (You can guess what
1 and 2 stand for.)
You will learn the precise deﬁnition of a form pretty soon; meanwhile
I will give some more examples in order to demonstrate that to a certain
extent this object is already familiar.
Example 2. In the usual integral over a segment in R, e.g.,
Z 2π
0
sin x dx,
the expression sin x dx is a 1-form on [0, 2π] (or on R).
Example 3. The total diﬀerential of a function in R3 (if you know what it
is),
df = ∂f
∂x dx + ∂f
∂y dy + ∂f
∂z dz,
is a 1-form in R3.
Example 4. When you integrate a function over a bounded domain in the
plane:
Z
D
f(x, y) dxdy
the expression under the integral, f(x, y) dxdy, is a 2-form in D.
We can conclude that a form is a linear combination of diﬀerentials or
their products. Of course, we need to know the algebraic rules of handling
these products. This will be discussed in due time.
When we will learn how to handle forms, this, in particular, will help us
a lot with integrals.
The central statement about forms is the so-called ‘general (or general-
ized) Stokes theorem’. You should be familiar with what turns out to be
some of its instances:
2

VECTOR CALCULUS. Fall 2002
Example 5. In elementary multivariate calculus Green’s formula in the
plane is considered:
I
C
Pdx + Qdy =
ZZ
D
µ∂Q
∂x −∂P
∂y
¶
dxdy,
where D is a domain bounded by a contour C. (The symbol
H
is used for
integrals over “closed contours”.)
Example 6. The Newton–Leibniz formula or the “fundamental theorem
of calculus”:
F(b) −F(a) =
Z b
a
F ′(x) dx.
Here the boundary of a segment [a, b] consists of two points b, a. The dif-
ference F(b) −F(a) should be regarded as an “integral” over these points
(taken with appropriate signs).
The generalized Stokes theorem embraces the two statements above as
well as many others, which have various traditional names attached to them.
It reads as follows:
Theorem.
I
∂M
ω =
Z
M
dω.
Here ω is a diﬀerential form, M is an “oriented manifold with boundary”,
dω is the “exterior diﬀerential” of ω, ∂M is the “boundary” of M. Or, rather,
we shall consider a version of this theorem with M replaced by a so-called
“chain” and ∂M replaced by the “boundary” of this chain.
Our task will be to make a precise meaning of these notions.
Remark. “Vector calculus” is the name for this course, ﬁrstly, because vec-
tors play an important role in it, and, secondly, because of a tradition. In
expositions that are now obsolete, the central place was occupied by vector
ﬁelds in “space” (that is, R3) or in the “plane” (that is, R2).
Approach
based on forms clariﬁes and simpliﬁes things enormously. It allows to gener-
alize the calculus to arbitrary Rn (and even further to general diﬀerentiable
manifolds). The methods of the theory of diﬀerential forms nowadays are
used almost everywhere in mathematics and its applications, in particular in
physics and in engineering.
1
Recollection of diﬀerential calculus in Rn
1.1
Points and vectors
Let us recall that Rn is the set of arrays of real numbers of length n:
Rn = {(x1, x2, . . . , xn) | xi ∈R, i = 1, . . . , n}.
(1)
3

THEODORE VORONOV
Here the superscript i is not a power, but simply an index. We interpret
the elements of Rn as points of an “n-dimensional space”. For points we use
boldface letters (or the underscore, in hand-writing): x = (x1, x2, . . . , xn) or
x = (x1, x2, . . . , xn). The numbers xi are called the coordinates of the point
x. Of course, we can use letters other than x, e.g., a, b or y, to denote
points. Sometimes we also use capital letters like A, B, C, . . . , P, Q, . . .. A
lightface letter with an index (e.g., yi) is a generic notation for a coordinate
of the corresponding point.
Example 1.1. a = (2, 5, −3) ∈R3, x = (x, y, z, t) ∈R4, P = (1, −1) ∈R2
are points in R3, R4, R2, respectively. Here a1 = 2, a2 = 5, a3 = −5; x1 = x,
x2 = y, x3 = z, x4 = t; P 1 = 1, P 2 = −1. Notice that coordinates can be
ﬁxed numbers or variables.
In the examples, Rn often will be R1, R2 or R3 (maybe R4), but our theory
is good for any n. We shall often use the “standard” coordinates x, y, z in
R3 instead of x1, x2, x3.
Elements on Rn can also be interpreted as vectors. This you should know
from linear algebra. Vectors can be added and multiplied by numbers. There
is a distinguished vector “zero”: 0 = (0, . . . , 0).
Example 1.2. For a = (0, 1, 2) and b = (2, 3, −2) we have a+b = (0, 1, 2)+
(2, 3, −2) = (2, 4, 0). Also, 5a = 5(0, 1, 2) = (5, 1, 10).
All the expected properties are satisﬁed (e.g., the commutative and as-
sociative laws for the addition, the distributive law for the multiplication by
numbers).
Vectors are also denoted by letters with an arrow: −→a = (a1, a2, . . . , an) ∈
Rn. We refer to coordinates of vectors also as to their components.
For a time being the distinction of points and vectors is only mental.
We want to introduce two operations involving points and vectors.
Deﬁnition 1.1. For a point x and a vector a (living in the same Rn),
we deﬁne their sum, which is a point (by deﬁnition), as x + a := (x1 +
a1, x2 + a2, . . . , xn + an).
For two points x and y in Rn, we deﬁne their
diﬀerence as a vector (by deﬁnition), denoted either as y −x or −→
xy, and
y −x = −→
xy := (y1 −x1, y2 −x2, . . . , yn −xn).
Example 1.3. Let A = (1, 2, 3), B = (−1, 0, 7). Then −→
AB = (−2, −2, 4).
(From the viewpoint of arrays, the operations introduced above are no
diﬀerent from the addition or subtraction of vectors. The diﬀerence comes
from our mental distinction of points and vectors.)
“Addition of points” or “multiplication of a point by a number” are not
deﬁned. Please note this.
4

VECTOR CALCULUS. Fall 2002
Remark 1.1. Both points and vectors are represented by the same type of
arrays in Rn. Their distinction will become very important later.
The most important properties of the addition of a point and a vector,
and of the subtraction of two points, are contained in the formulae
−→
AA = 0,
−→
AB + −−→
BC = −→
AC;
(2)
if P + a = Q, then a = −→
PQ.
(3)
They reﬂect our intuitive understanding of vectors as “directed segments”.
Example 1.4. Consider the point O = (0, . . . , 0) ∈Rn. For an arbitrary
vector r, the coordinates of the point x = O + r are equal to the respective
coordinates of the vector r: x = (x1, . . . , xn) and r = (x1, . . . , xn).
The vector r such as in the example is called the position vector or the
radius-vector of the point x. (Or, in greater detail: r is the radius-vector
of x w.r.t. an origin O.) Points are frequently speciﬁed by their radius-
vectors. This presupposes the choice of O as the “standard origin”. (There
is a temptation to identify points with their radius-vectors, which we will
resist in view of the remark above.)
Let us summarize. We have considered Rn and interpreted its elements
in two ways: as points and as vectors. Hence we may say that we dealing
with the two copies of Rn:
Rn = {points},
Rn = {vectors}
Operations with vectors: multiplication by a number, addition. Operations
with points and vectors: adding a vector to a point (giving a point), sub-
tracting two points (giving a vector).
Rn treated in this way is called an n-dimensional aﬃne space. (An “ab-
stract” aﬃne space is a pair of sets, the set of points and the set of vectors so
that the operations as above are deﬁned axiomatically.) Notice that vectors
in an aﬃne space are also known as “free vectors”. Intuitively, they are not
ﬁxed at points and “ﬂoat freely” in space. Later, with the introduction of
so-called curvilinear coordinates, we will see the necessity of “ﬁxing” vectors.
From Rn considered as an aﬃne space we can proceed in two opposite
directions:
Rn as a Euclidean space ⇐Rn as an aﬃne space ⇒Rn as a manifold
What does it mean? Going to the left means introducing some extra
structure which will make the geometry richer. Going to the right means
forgetting about part of the aﬃne structure; going further in this direction
will lead us to the so-called “smooth (or diﬀerentiable) manifolds”.
The theory of diﬀerential forms does not require any extra geometry. So
our natural direction is to the right. The Euclidean structure, however, is
useful for examples and applications. So let us say a few words about it:
5

THEODORE VORONOV
Remark 1.2. Euclidean geometry. In Rn considered as an aﬃne space we
can already do a good deal of geometry. For example, we can consider lines
and planes, and quadric surfaces like an ellipsoid. However, we cannot discuss
such things as “lengths”, “angles” or “areas” and “volumes”. To be able to
do so, we have to introduce some more deﬁnitions, making Rn a Euclidean
space. Namely, we deﬁne the length of a vector a = (a1, . . . , an) to be
|a| :=
p
(a1)2 + . . . + (an)2.
(4)
After that we can also deﬁne distances between points as follows:
d(A, B) := |−→
AB|.
(5)
One can check that the distance so deﬁned possesses natural properties that
we expect: is it always non-negative and equals zero only for coinciding
points; the distance from A to B is the same as that from B to A (symmetry);
also, for three points, A, B and C, we have d(A, B) ⩽d(A, C)+d(C, B) (the
“triangle inequality”). To deﬁne angles, we ﬁrst introduce the scalar product
of two vectors
(a, b) := a1b1 + . . . + anbn.
(6)
Thus |a| =
p
(a, a). The scalar product is also denoted by a dot: a · b =
(a, b), and hence is often referred to as the “dot product”. Now, for nonzero
vectors we deﬁne the angle between them by the equality
cos α := (a, b)
|a||b|.
(7)
The angle itself is deﬁned up to an integral multiple of 2π. For this deﬁnition
to be consistent we have to ensure that the r.h.s. of (7) does not exceed 1
by the absolute value. This follows from the inequality
(a, b)2 ⩽|a|2|b|2
(8)
known as the Cauchy–Bunyakovsky–Schwarz inequality (various combina-
tions of these three names are applied in diﬀerent books). One of the ways of
proving (8) is to consider the scalar square of the linear combination a + tb,
where t ∈R. As (a + tb, a + tb) ⩾0 is a quadratic polynomial in t which is
never negative, its discriminant must be less or equal zero. Writing this ex-
plicitly yields (8) (check!). The triangle inequality for distances also follows
from the inequality (8).
1.2
Velocity vector
The most important example of vectors for us is their occurrence as velocity
vectors of parametrized curves.
Consider a map t 7→x(t) from an open
interval of the real line to Rn. Such map is called a parametrized curve or a
path. We will often omit the word “parametrized”.
6

VECTOR CALCULUS. Fall 2002
Remark 1.3. There is another meaning of the word “curve” when it is used
for a set of points line a straight line or a circle. A parametrized curve is a
map, not a set of points. One can visualize it as a set of points given by its
image plus a law according to which this set is travelled along in “time”.
Example 1.5. A straight line l in Rn can be speciﬁed by a point on l line
and a nonzero vector in the direction of l. Hence we can make it into a
parametrized curve by introducing the equation
x(t) = x0 + tv.
In the coordinates we have xi = xi
0+tvi. Here t runs over R (inﬁnite interval)
if we want to obtain the whole line, not just a segment.
Example 1.6. A straight line in R3 in the direction of the vector v = (1, 0, 2)
through the point x0 = (1, 1, 1):
x(t) = (1, 1, 1) + t(1, 0, 2)
or
x = 1 + t
y = 1
z = 1 + 2t.
Example 1.7. The graph of the function y = x2 (a parabola in R2) can be
made a parametrized curve by introducing a parameter t as
x = t
y = t2.
Example 1.8. The following parametrized curve:
x = cos t
y = sin t,
where t ∈R, describes a unit circle with center at the origin, which we go
around inﬁnitely many times (with constant speed) if t ∈R. If we specify
some interval (a, b) ⊂[0, 2π], then we obtain just an arc of the circle.
Deﬁnition 1.2. The velocity vector (or, shortly, the velocity) of a curve x(t)
is the vector denoted ˙x(t) or dx/dt, where
˙x(t) = dx
dt := lim
h→0
x(t + h) −x(t)
h
.
(9)
7

THEODORE VORONOV
Notice that the diﬀerence x(t+h)−x(t) is a vector, so the velocity vector
is indeed a vector in Rn. It is convenient to visualize ˙x(t) as being attached
to the corresponding point x(t). As the directed segment x(t + h) −x(t) lies
on a secant, the velocity vector lies on the tangent line to our curve at the
point x(t) (“the limit position of secants through the point x(t)”). From the
deﬁnition immediately follows that
˙x(t) =
µdx1
dt , . . . , dxn
dt
¶
(10)
in the coordinates. (A curve is smooth if the velocity vector exists. In the
sequel we shall use smooth curves without special explication.)
Example 1.9. For a straight line parametrized as in Example 1.5 we get
x(t + h) −x(t) = x0 + (t + h)v −x0 −tv = hv, hence ˙x = v (a constant
vector).
Example 1.10. In Example 1.6 we get ˙x = (1, 0, 2).
Example 1.11. In Example 1.7 we get ˙x(t) = (1, 2t). It is instructive to
sketch a picture of the curve and plot the velocity vectors at t = 0, 1, −1, 2, −2,
drawing them as attached to the corresponding points.
Example 1.12. In Example 1.8 we get ˙x(t) = (−sin t, cos t). Again, it is
instructive to sketch a picture. (Plot the velocity vectors at t = 0, π
4, π
2, 3π
4 , π.)
Example 1.13. Consider the parametrized curve x = 2 cos t, y = 2 sin t, z =
t in R3 (representing a round helix). Then
˙x = (−2 sin t, 2 cos t, 1).
(Make a sketch!)
The velocity vector is a feature of a parametrized curve as a map, not of
its image (a “physical” curve as a set of points in space). If we will change
the parametrization, the velocity will change:
Example 1.14. In Example 1.8 we can introduce a new parameter s so that
t = 5s. Hence
x = cos 5s
y = sin 5s
will be the new equation of the curve. Then
dx
ds = (−5 sin 5s, 5 cos 5s) = 5 dx
dt .
8

VECTOR CALCULUS. Fall 2002
In general, for an arbitrary curve t 7→x(t) we obtain
dx
ds = dt
ds
dx
dt
(11)
if we introduce a new parameter s so that t = t(s) is a function of s. We
always assume that the change of parameter is invertible and dt/ds ̸= 0.
Notice that the velocity is only changed by the multiplication by a nonzero
scalar factor, hence its direction is not changed (only the “speed” with which
we move along the curve changes). In particular, the tangent line to a curve
does not depend on parametrization.
1.3
Diﬀerential of a function
Formally, the diﬀerential of a function is the following expression:
df = df
dx dx
(12)
for functions of one variable and
df = ∂f
∂x1 dx1 + . . . + ∂f
∂xn dxn
(13)
for functions of many variables. Now we want to explain the meaning of the
diﬀerential.
Let us start from a function f : (a, b) →R deﬁned on an interval of the
real line. We shall revisit the notion of the diﬀerentiability. Fix a point x; we
want to know how the value of the function changes when we move from x to
some other point x + h. In other words, we consider an increment ∆x = h of
the independent variable and we study the corresponding increment of our
function: ∆f = f(x + h) −f(x). It depends on x and on h. For “good”
functions we expect that ∆f is small for small ∆x.
Deﬁnition 1.3. A function f is diﬀerentiable at x if ∆f is “approximately
linear” in ∆x; precisely:
f(x + h) −f(x) = k · h + α(h)h
(14)
where α(h) →0 when h →0.
This can be illustrated using the graph of the function f. The coeﬃcient
k is the slope of the tangent line to the graph at the point x. The linear
function of the increment h appearing in (14) is called the diﬀerential of f
at x:
df(x)(h) = k · h = k · ∆x.
(15)
9

THEODORE VORONOV
In other words, df(x)(h) is the “main (linear) part” of the increment ∆f (at
the point x) when h →0. Approximately ∆f ≈df when ∆x = h is small.
The coeﬃcient k is exactly the derivative of f at x. Notice that dx = ∆x.
Hence
df = k · dx
(16)
where k is the derivative. (We suppressed x in the notation for df.) Thus
the common notation df/dx for the derivative can be understood directly as
the ratio of the diﬀerentials.
This deﬁnition of diﬀerentiability for functions of a single variable is equiv-
alent to the one where the derivative comes ﬁrst and the diﬀerential is deﬁned
later. It is worth teaching yourself to think in terms of diﬀerentials.
Example 1.15. Diﬀerentials of elementary functions:
d(xn) = nxn−1 dx
d(ex) = ex dx
d(ln x) = dx
x
d(sin x) = cos x dx,
etc.
The same approach works for functions of many variables.
Consider
f : U →R where U ⊂Rn.
Fix a point x ∈U.
The main diﬀerence
from functions of a single variable is that the increment of x is now a vector:
h = (h1, . . . , hn). Consider ∆f = f(x + h) −f(x) for various h ∈Rn. For
this to make sense at least for small h we need the domain U where f is
deﬁned to be open, i.e. containing a small ball around x (for every x ∈U).
Deﬁnition 1.4. A function f : U →R is diﬀerentiable at x if
f(x + h) −f(x) = A(h) + α(h)|h|
(17)
where A(h) = A1h1+. . .+Anhn is a linear function of h and α(h) →0 when
h →0. (The function A, of course, depends on x.) The linear function A(h)
is called the diﬀerential of f at x. Notation: df or df(x), so df(x)(h) = A(h).
The value of df on a vector h is also called the derivative of f along h and
denoted ∂hf(x) = df(x)(h).
Example 1.16. Let f(x) = (x1)2 + (x2)2 in R2. Choose x = (1, 2). Then
df(x)(h) = 2h1 + 4h2 (check!).
Example 1.17. Consider h = ei = (0, . . . , 0, 1, 0, . . . , 0) (the i-th standard
basis vector in Rn). The derivative ∂eif = df(x)(ei) = Ai is called the partial
derivative w.r.t. xi. The standard notation:
df(x)(ei) =: ∂f
∂xi(x).
(18)
10

VECTOR CALCULUS. Fall 2002
From Deﬁnition 1.4 immediately follows that partial derivatives are just the
usual derivatives of a function of a single variable if we allow only one coor-
dinate xi to change and ﬁx all other coordinates.
Example 1.18. Consider the function f(x) = xi (the i-th coordinate). The
linear function dxi (the diﬀerential of xi) applied to an arbitrary vector h is
simply hi.
From these examples follows that we can rewrite df as
df = ∂f
∂x1 dx1 + . . . + ∂f
∂xn dxn ,
(19)
which is the standard form. Once again: the partial derivatives in (19) are
just the coeﬃcients (depending on x); dx1, dx2, . . . are linear functions giving
on an arbitrary vector h its coordinates h1, h2, . . ., respectively. Hence
df(x)(h) = ∂hf(x) = ∂f
∂x1 h1 + . . . + ∂f
∂xn hn.
(20)
Theorem 1.1. Suppose we have a parametrized curve t 7→x(t) passing
through x0 ∈Rn at t = t0 and with the velocity vector ˙x(t0) = v. Then
d f(x(t))
dt
(t0) = ∂vf(x0) = df(x0)(v).
(21)
Proof. Indeed, consider a small increment of the parameter t: t0 7→t0 + ∆t.
We want to ﬁnd the increment of f(x(t)). We have
x0 7→x(t0 + ∆t) = x0 + v · ∆t + α(∆t)∆t
where ∆t →0. On the other hand, we have
f(x0 + h) −f(x0) = df(x0)(h) + β(h)|h|
for an arbitrary vector h, where β(h) →0 when h →0.
Combining it
together, for the increment of f(x(t)) we obtain
f(x(t0 + ∆t)) −f(x0) =
df(x0)
¡
v · ∆t + α(∆t)∆t
¢
+ β(v · ∆t + α(∆t)∆t) · |v∆t + α(∆t)∆t| =
df(x0)(v) · ∆t + γ(∆t)∆t
for a certain γ(∆t) such that γ(∆t) →0 when ∆t →0 (we used the linearity
of df(x0)). By the deﬁnition, this means that the derivative of f(x(t)) at
t = t0 is exactly df(x0)(v).
11

THEODORE VORONOV
The statement of the theorem can be expressed by a simple formula:
d f(x(t))
dt
= ∂f
∂x1 ˙x1 + . . . + ∂f
∂xn ˙xn.
(22)
Theorem 1.1 gives another approach to diﬀerentials: to calculate the value
of df at a point x0 on a given vector v one can take an arbitrary curve passing
through x0 at t0 with v as the velocity vector at t0 and calculate the usual
derivative of f(x(t)) at t = t0.
Theorem 1.2. For functions f, g: U →R, where U ⊂Rn,
d(f + g) = df + dg
(23)
d(fg) = df · g + f · dg
(24)
Proof. We can prove this either directly from Deﬁnition 1.4 or using for-
mula (21). Consider an arbitrary point x0 and an arbitrary vector v stretch-
ing from it.
Let a curve x(t) be such that x(t0) = x0 and ˙x(t0) = v.
Hence d(f + g)(x0)(v) = d
dt(f(x(t)) + g(x(t))) at t = t0 and d(fg)(x0)(v) =
d
dt(f(x(t))g(x(t))) at t = t0. Formulae (23) and (24) then immediately follow
from the corresponding formulae for the usual derivative (check!).
Now, almost without change the theory generalizes to functions taking
values in Rm instead of R. The only diﬀerence is that now the diﬀerential of
a map F : U →Rm at a point x will be a linear function taking vectors in
Rn to vectors in Rm (instead of R). For an arbitrary vector h ∈Rn,
F (x + h) = F (x) + dF (x)(h) + β(h)|h|
(25)
where β(h) →0 when h →0. We have dF = (dF 1, . . . , dF m) and
dF = ∂F
∂x1 dx1 + . . . + ∂F
∂xn dxn =


∂F 1
∂x1
. . .
∂F 1
∂xn
. . .
. . .
. . .
∂F m
∂x1
. . .
∂F m
∂xn




dx1
. . .
dxn

.
(26)
In this matrix notation we have to write vectors as vector-columns.
Theorem 1.1 generalizes as follows.
Theorem 1.3. For an arbitrary parametrized curve x(t) in Rn, the diﬀer-
ential of a map F : U →Rm (where U ⊂Rn) maps the velocity vector ˙x(t)
to the velocity vector of the curve F (x(t)) in Rm:
d F (x(t))
dt
= dF (x(t))( ˙x(t)).
(27)
12

VECTOR CALCULUS. Fall 2002
Proof. By the deﬁnition of the velocity vector,
x(t + ∆t) = x(t) + ˙x(t) · ∆t + α(∆t)∆t
(28)
where α(∆t) →0 when ∆t →0. By the deﬁnition of the diﬀerential,
F (x + h) = F (x) + dF (x)(h) + β(h)|h|
(29)
where β(h) →0 when h →0. Plugging (28) into (29), we obtain
F (x(t+∆t)) = F
¡
x+ ˙x(t) · ∆t + α(∆t)∆t
|
{z
}
h
¢
= F (x)+dF (x)
¡ ˙x(t)∆t+α(∆t)∆t
¢
+
β
¡ ˙x(t)∆t+α(∆t)∆t
¢
·
¯¯ ˙x(t)∆t+α(∆t)∆t
¯¯ = F (x)+dF (x) ˙x(t)·∆t+γ(∆t)∆t
for some γ(∆t) →0 when ∆t →0. This precisely means that dF (x) ˙x(t) is
the velocity vector of F (x).
As every vector attached to a point can be viewed as the velocity vector of
some curve passing through this point, this theorem gives a clear geometric
picture of dF as a linear map on vectors.
Theorem 1.4 (Chain rule for diﬀerentials). Suppose we have two maps
F : U →V and G: V →W, where U ⊂Rn, V ⊂Rm, W ⊂Rp (open
domains). Let F : x 7→y = F (x). Then the diﬀerential of the composite
map G ◦F : U →W is the composition of the diﬀerentials of F and G:
d(G ◦F )(x) = dG(y) ◦dF (x).
(30)
Proof. We can use the description of the diﬀerential given by Theorem 1.3.
Consider a curve x(t) in Rn with the velocity vector ˙x. Basically, we need
to know to which vector in Rp it is taken by d(G ◦F ). By Theorem 1.3, it is
the velocity vector to the curve (G ◦F )(x(t)) = G(F (x(t))). By the same
theorem, it equals the image under dG of the velocity vector to the curve
F (x(t)) in Rm. Applying the theorem once again, we see that the velocity
vector to the curve F (x(t)) is the image under dF of the vector ˙x(t). Hence
d(G ◦F )( ˙x) = dG(dF ( ˙x)) for an arbitrary vector ˙x (we suppressed points
from the notation), which is exactly the desired formula (30).
Corollary 1.1. If we denote coordinates in Rn by (x1, . . . , xn) and in Rm by
(y1, . . . , ym), and write
dF = ∂F
∂x1 dx1 + . . . + ∂F
∂xn dxn
(31)
dG = ∂G
∂y1 dy1 + . . . + ∂G
∂ym dym,
(32)
13

THEODORE VORONOV
then the chain rule can be expressed as follows:
d(G ◦F ) = ∂G
∂y1 dF 1 + . . . + ∂G
∂ym dF m,
(33)
where dF i are taken from (31). In other words, to get d(G ◦F ) we have to
substitute into (32) the expression for dyi = dF i from (31).
This can also be expressed by the following matrix formula:
d(G ◦F ) =


∂G1
∂y1
. . .
∂G1
∂ym
. . .
. . .
. . .
∂Gp
∂y1
. . .
∂Gp
∂ym




∂F 1
∂x1
. . .
∂F 1
∂xn
. . .
. . .
. . .
∂F m
∂x1
. . .
∂F m
∂xn




dx1
. . .
dxn

,
(34)
i.e.
if dG and dF are expressed by matrices of partial derivatives, then
d(G◦F ) is expressed by the product of these matrices. This is often written
as


∂z1
∂x1
. . .
∂z1
∂xn
. . .
. . .
. . .
∂zp
∂x1
. . .
∂zp
∂xn

=


∂z1
∂y1
. . .
∂z1
∂ym
. . .
. . .
. . .
∂zp
∂y1
. . .
∂zp
∂ym




∂y1
∂x1
. . .
∂y1
∂xn
. . .
. . .
. . .
∂ym
∂x1
. . .
∂ym
∂xn

,
(35)
or
∂zµ
∂xa =
m
X
i=1
∂zµ
∂yi
∂yi
∂xa,
(36)
where it is assumed that the dependence of y ∈Rm on x ∈Rn is given by
the map F , the dependence of z ∈Rp on y ∈Rm is given by the map G,
and the dependence of z ∈Rp on x ∈Rn is given by the composition G ◦F .
Experience shows that it is much more easier to work in terms of diﬀer-
entials than in terms of partial derivatives.
Example 1.19. d cos2 x = 2 cos x d cos x = −2 cos x sin x dx = −sin 2x dx.
Example 1.20. d ex2 = ex2 d(x2) = 2xex2 dx.
Example 1.21. d(x2 + y2 + z2) = d(x2) + d(y2) + d(z2) = 2x dx + 2y dy +
2z dz = 2(x dx + y dy + z dz).
Example 1.22. d (t, t2, t3) = (dt, d(t2), d(t3)) = (dt, 2t dt, 3t2 dt) = (1, 2t, 3t2) dt.
Example 1.23. d (x+y, x−y) = (d(x+y), d(x−y)) = (dx+dy, dx−dy) =
(dx, dx) + (dy, −dy) = (1, 1) dx + (1, −1) dy.
Remark 1.4. The deﬁnition of the diﬀerential involves objects like α(h)|h|
and the notion of the limit in Rn.
At the ﬁrst glance it may seem that
the theory essentially relies on a Euclidean structure in Rn (the concept of
“length”, as deﬁned in Remark 1.2). However, it is not so. One can check
that the notions of the limit as well as the conditions like “a function of the
form α(h)|h| where α(h) →0 when h →0” are equivalent for all reasonable
deﬁnitions of “length” in Rn, hence are intrinsic for Rn and do not depend
on any extra structure.
14

VECTOR CALCULUS. Fall 2002
1.4
Changes of coordinates
Consider points x = (x1, . . . , xn) ∈Rn. From now on we will call the numbers
xi the “standard coordinates” of the point x. The reason is that we can use
other coordinates to specify points. Before we give a general deﬁnition, let
us consider some examples.
Example 1.24. “New coordinates” x′, y′ are introduced in R2 by the for-
mulae
(
x′ = 2x −y
y′ = x + y.
For example, if a point x has the standard coordinates x = 1, y = 2, then the
new coordinates of the same point will be x′ = 0, y′ = 3. We can, conversely,
express x, y in terms of x′y′:





x = 1
3(x′ + y′)
y = 1
3(−x′ + 2y′).
The geometric meaning of such change of coordinates is very simple. Re-
call that the standard coordinates of a point x coincide with the components
of the radius-vector r = −→
Ox: r = xe1 + ye2, where e1 = (1, 0), e2 = (0, 1)
is the standard basis. “New” coordinates as above correspond to a diﬀerent
choice of a basis: r = x′e′
1 + y′e′
2.
Problem 1.1. Find e′
1 and e′
2 for the example above from the condition
that xe1 + ye2 = x′e′
1 + y′e′
2 for all points (x, y and x′, y′ are related by the
formulae above).
Another way to get new coordinates in Rn is to choose a “new origin”
instead of O = (0, . . . , 0).
Example 1.25. Deﬁne x′, y′ in R2 by the formulae
(
x′ = x + 5
y′ = y −3.
Then the “old origin” O will have the new coordinates x′ = 5, y′ = −3.
Conversely, we can ﬁnd the “new origin” O′, i.e., the point that has the new
coordinates x′ = 0, y′ = 0: its old coordinates are x = −5, y = 3.
Now we will consider a non-linear change of coordinates.
Example 1.26. Consider in R2 the polar coordinates (r, ϕ) so that
(
x = r cos ϕ
y = r sin ϕ.
(37)
15

THEODORE VORONOV
Let r ⩾0. Then
r =
p
x2 + y2.
(38)
As for the angle ϕ, for (x, y) ̸= (0, 0) it can be expressed up to integral
multiples of 2π via the inverse trigonometric functions. Note that ϕ is not
deﬁned at the origin. The correspondence between (x, y) and (r, ϕ) cannot
be made one-to-one in the whole plane. To deﬁne ϕ uniquely and not just
up to 2π, we have to cut out a ray starting from the origin (then we can
count angles from that ray). For example, if we cut out the positive ray of
the x-axis, then we can choose ϕ ∈(0, 2π) and express ϕ as
ϕ = arccos
x
p
x2 + y2.
(39)
Hence formulae (37) and (38,39) give mutually inverse maps F : V →U and
G: U →V , where U = R2 \ {(x, 0) | x ⩾0} is a domain of the (x, y)-plane
and V = {(r, ϕ) | r > 0, ϕ ∈(0, 2π)} is a domain of the (r, ϕ)-plane. Notice
that we can diﬀerentiate both maps inﬁnitely many times.
We shall call a map smooth if it is inﬁnitely diﬀerentiable, i.e., if there
exist partial derivatives of all orders.
Modelling on the examples above, we are going to give a general deﬁni-
tion of a “coordinate system” (or “system of coordinates”). Notice that the
example of polar coordinates shows that a “system of coordinates” should
be deﬁned for a domain of Rn, not for the whole Rn (in general).
Deﬁnition 1.5. Consider an open domain U ⊂Rn.
Consider also an-
other copy of Rn, denoted for distinction Rn
y, with the standard coordinates
(y1 . . . , yn). A system of coordinates in the open domain U is given by a map
F : V →U, where V ⊂Rn
y is an open domain of Rn
y, such that the following
three conditions are satisﬁed:
(1) F is smooth;
(2) F is invertible;
(3) F −1 : U →V is also smooth.
The coordinates of a point x ∈U in this system are the standard coordinates
of F −1(x) ∈Rn
y.
In other words,
F : (y1 . . . , yn) 7→x = x(y1 . . . , yn).
(40)
Here the variables (y1 . . . , yn) are the “new” coordinates of the point x.
The standard coordinates in Rn are a particular case when the map F
is identical.
In Examples 1.24 and 1.25 we have maps R2
(x′,y′) →R2; in
Example 1.26 we have a map R2
(r,ϕ) ⊃V →U ⊂R2.
16

VECTOR CALCULUS. Fall 2002
Remark 1.5. Coordinate systems as introduced in Deﬁnition 1.5 are of-
ten called “curvilinear”. They, of course, also embrace “linear” changes of
coordinates like those in Examples 1.24 and 1.25.
Remark 1.6. There are plenty of examples of particular coordinate systems.
The reason why they are introduced is that a “good choice” of coordinates
can substantially simplify a problem. For example, polar coordinates in the
plane are very useful in planar problems with a rotational symmetry.
What happens with vectors when we pass to general coordinate systems
in domains of Rn?
Clearly, operations with points like taking diﬀerence, −→
AB = B−A, cannot
survive nonlinear maps involved in the deﬁnition of curvilinear coordinates.
The hint on what to do is the notion of the velocity vector. The slogan is:
“Every vector is a velocity vector for some curve.” Hence we have to ﬁgure
out how to handle velocity vectors. For this we return ﬁrst to the example
of polar coordinates.
Example 1.27. Consider a curve in R2 speciﬁed in polar coordinates as
x(t): r = r(t), ϕ = ϕ(t).
(41)
How to ﬁnd the velocity ˙x? We can simply use the chain rule. The map
t 7→x(t) can be considered as the composition of the maps t 7→(r(t), ϕ(t)),
(r, ϕ) 7→x(r, ϕ). Then, by the chain rule, we have
˙x = dx
dt = ∂x
∂r
dr
dt + ∂x
∂ϕ
dϕ
dt = ∂x
∂r ˙r + ∂x
∂ϕ ˙ϕ.
(42)
Here ˙r and ˙ϕ are scalar coeﬃcients depending on t, whence the partial deriva-
tives ∂x/∂r, ∂x/∂ϕ are vectors depending on point in R2. We can compare
this with the formula in the “standard” coordinates: ˙x = e1 ˙x+e2 ˙y. Consider
the vectors ∂x/∂r, ∂x/∂ϕ. Explicitly we have
∂x
∂r = (cos ϕ, sin ϕ)
(43)
∂x
∂ϕ = (−r sin ϕ, r cos ϕ),
(44)
from where it follows that these vectors make a basis at all points except for
the origin (where r = 0). It is instructive to sketch a picture, drawing vectors
corresponding to a point as starting from that point. Notice that ∂x/∂r,
∂x/∂ϕ are, respectively, the velocity vectors for the curves r 7→x(r, ϕ)
(ϕ = ϕ0 ﬁxed) and ϕ 7→x(r, ϕ) (r = r0 ﬁxed). We can conclude that for
an arbitrary curve given in polar coordinates the velocity vector will have
components ( ˙r, ˙ϕ) if as a basis we take er := ∂x/∂r, eϕ := ∂x/∂ϕ:
˙x = er ˙r + eϕ ˙ϕ.
(45)
17

THEODORE VORONOV
A characteristic feature of the basis er, eϕ is that it is not “constant” but
depends on point. Vectors “stuck to points” when we consider curvilinear
coordinates.
Example 1.28. Question: given a vector v = 2er + 3eϕ, express it in
standard coordinates. The question is ill-deﬁned unless we specify a point
in R2 at which we consider our vector. Indeed, if we simply plug er, eϕ
from (43), (44), we get v = 2(cos ϕ, sin ϕ) + 3(−r sin ϕ, r cos ϕ), something
with variable coeﬃcients. Until we specify r, ϕ, i.e., specify a point in the
plane, we cannot get numbers!
Problem 1.2. Check that solving (43), (44) for the standard basis e1, e2
we obtain
e1 = 1
r (er r cos ϕ −eϕ sin ϕ)
(46)
e1 = 1
r (er r sin ϕ + eϕ cos ϕ).
(47)
After considering these examples, we can say how vectors should be han-
dled using arbitrary (curvilinear) coordinates. Suppose we are given a system
of coordinates in a domain U ⊂Rn. We shall denote the coordinates in this
system simply by x1, . . . , xn. (So xi no longer stand for the “standard” co-
ordinates!) Then:
(1) there appears a “variable basis” associated with this coordinate sys-
tem, which we denote ei = ∂x
∂xi;
(2) vectors are attached to points; every vector is speciﬁed by components
w.r.t. the basis ei;
(3) if we change coordinates from xi to xi′, then the basis transforms
according (formally) to the chain rule:
ei =
X
ei′ ∂xi′
∂xi ,
(48)
with coeﬃcients depending on point;
(4) the components of vectors at each point transform accordingly.
It exactly the transformation law with variable coeﬃcients that make us
consider the basis ei associated with a coordinate system as “variable” and
attach vectors to points.
This new approach to vectors is compatible with our original approach
when we treated vectors and points simply as arrays and vectors were not
attached to points.
Example 1.29. Suppose xi are the standard coordinates in Rn so that x =
(x1, . . . , xn). Then we can understand ei = ∂x/∂xi straightforwardly and by
diﬀerentiation get at each place either 1 or 0 depending on whether we diﬀer-
entiate ∂xj/∂xi for j = ior not: hence e1 = (1, 0, . . . , 0), e2 = (0, 1, 0, . . . , 0),
. . . , evn = (0, . . . , 0, 1). From the general rule we have recovered the standard
basis in Rn!
18

VECTOR CALCULUS. Fall 2002
Remark 1.7. The “aﬃne structure” in Rn, i.e., the operations with points
and vectors described in Section 1.1 and in particular the possibility to con-
sider vectors independently of points (“free” vectors) is preserved under a
special class of changes of coordinates, namely, those similar to Examples 1.24
and 1.25 (linear transformations and parallel translations).
With this new understanding of vectors, we can deﬁne the velocity vector
for a parametrized curve speciﬁed w.r.t. arbitrary coordinates as xi = xi(t)
as the vector ˙x := ei ˙xi.
Proposition 1.1. The velocity vector has the same appearance in all coor-
dinate systems.
Proof. Follows directly from the chain rule and the transformation law for
the basis ei.
In particular, the elements of the basis ei = ∂x/∂xi (originally, a formal
notation) can be understood directly as the velocity vectors of the coordinate
lines xi 7→x(x1, . . . , xn) (all coordinates but xi are ﬁxed).
Now, what happens with the diﬀerentials of maps?
Since we now know how to handle velocities in arbitrary coordinates, the
best way to treat the diﬀerential of a map F : Rn →Rm is by its action on
the velocity vectors. By deﬁnition, we set
dF (x0): dx(t)
dt
(t0) 7→dF (x(t))
dt
(t0).
(49)
Now dF (x0) is a linear map that takes vectors attached to a point x0 ∈Rn
to vectors attached to the point F (x)) ∈Rm. Using Theorem 1.3 backwards,
we obtain
dF = ∂F
∂x1 dx1 + . . . + ∂F
∂xn dxn =
¡
e1, . . . , em
¢


∂F 1
∂x1
. . .
∂F 1
∂xn
. . .
. . .
. . .
∂F m
∂x1
. . .
∂F m
∂xn




dx1
. . .
dxn

,
(50)
as before, — but now these formulae are valid in all coordinate systems.
In particular, for the diﬀerential of a function we always have
df = ∂f
∂x1 dx1 + . . . + ∂f
∂xn dxn,
(51)
where xi are arbitrary coordinates. The form of the diﬀerential does not
change when we perform a change of coordinates.
Example 1.30. Consider the following function in R2: f = r2 = x2 + y2.
We want to calculate its diﬀerential in the polar coordinates. We shall use
19

THEODORE VORONOV
two methods: directly in the polar coordinates and working ﬁrst in the co-
ordinates x, y and then transforming to r, ϕ. Directly in polars, we simply
have df = 2rdr. Now, by the second method we get
df = ∂f
∂x dx + ∂f
∂y dy = 2xdx + 2ydy.
Plugging dx = cos ϕ dr −r sin ϕ dϕ and dy = sin ϕ dr + r cos ϕ dϕ, and mul-
tiplying through, after simpliﬁcation we obtain the same result df = dr.
Remark 1.8. The relation between vectors and diﬀerentials of functions
(considered as linear functions on vectors, at a given point) remain valid in
all coordinate systems. The diﬀerential df at a point x is a linear function
on vectors attached to x. In particular, for dxi we have
dxi(ej) = dxi
µ ∂x
∂xj
¶
= ∂xi
∂xj = δi
j
(52)
(the second equality because the value of df on a velocity vector is the deriva-
tive of f w.r.t. the parameter on the curve). Hence the diﬀerentials of the
coordinates dxi form a basis (in the space of linear functions on vectors)
“dual” to the basis ej.
Problem 1.3. Consider spherical coordinates in R3. Find the basis er, eθ,
eϕ associated with it (in terms of the standard basis). Find the diﬀerentials
dr, dθ, dϕ. Do the same for cylindrical coordinates.
2
Line integrals and 1-forms
We are already acquainted with examples of 1-forms. Let us give a formal
deﬁnition.
Deﬁnition 2.1. A linear diﬀerential form or, shortly, a 1-form in an open
domain U ⊂Rn is an expression of the form
ω = ω1 dx1 + . . . + ωn dxn,
where ωi are functions. Here x1, . . . , xn denote some coordinates in U.
Greek letters like ω, σ, as well as capital Latin letters like A, E, are
traditionally used for denoting 1-forms.
Example 2.1. x dy −y dx, 2dx −(x + z) dy + xy dz are examples of 1-forms
in R2 and R3.
Example 2.2. The diﬀerential of a function in Rn is a 1-form:
df = ∂f
∂x1 dx1 + . . . + ∂f
∂xn dxn.
20

VECTOR CALCULUS. Fall 2002
(Notice that not every 1-form is df for some function f.
We will see
examples later.)
Though Deﬁnition 2.1 makes use of some (arbitrary) coordinate system,
the notion of a 1-form is independent of coordinates. There are at least two
ways to explain this.
Firstly, if we change coordinates, we will obtain again a 1-form (i.e., an
expression of the same type).
Example 2.3. Consider a 1-form in R2 given in the standard coordinates:
A = −y dx + x dy.
In the polar coordinates we will have x = r cos ϕ, y = r sin ϕ, hence
dx = cos ϕ dr −r sin ϕ dϕ
dy = sin ϕ dr + r cos ϕ dϕ.
Substituting into A, we get A = −r sin ϕ(cos ϕ dr−r sin ϕ dϕ)+r cos ϕ(sin ϕ dr+
r cos ϕ dϕ) = r2(sin2 ϕ + cos2 ϕ) dϕ = r2dϕ. Hence
A = r2dϕ
is the formula for A in the polar coordinates. In particular, we see that this
is again a 1-form, a linear combination of the diﬀerentials of coordinates with
functions as coeﬃcients.
Secondly, in a more conceptual way, we can deﬁne a 1-form in a domain
U as a linear function on vectors at every point of U:
ω(v) = ω1v1 + . . . + ωnvn,
(53)
if v = P ei vi, where ei = ∂x/∂xi. Recall that the diﬀerentials of functions
were deﬁned as linear functions on vectors (at every point), and
dxi(ej) = dxi
µ ∂x
∂xj
¶
= δi
j
(54)
at every point x. Remark: if we need to show explicitly the dependence on
point, we write ω(x)(v) (similarly as we did for diﬀerentials). There is an
alternative notation:
⟨ω, v⟩= ω(v) = ω1v1 + . . . + ωnvn,
(55)
which is sometimes more convenient. (Notice angle brackets; do not confuse
it with a scalar product of vectors, which is deﬁned for an Euclidean space.)
21

THEODORE VORONOV
Example 2.4. At the point x = (1, 2, −1) ∈R3 (we are using the standard
coordinates) calculate ⟨ω, v⟩if ω = x dx+y dy +z dz and v = 3e1 −5e3. We
have
⟨ω(x), v⟩= ⟨dx + 2dy −dz, 3e1 −5e3⟩= 3 −(−5) = 8.
The main purpose for which we need 1-forms is integration.
Suppose we are given a path, i.e., a parametrized curve in Rn. Denote it
γ : xi = xi(t), t ∈[a, b] ⊂R. Consider a 1-form ω.
Deﬁnition 2.2. The integral of ω over γ is
Z
γ
ω =
Z
γ
ω1 dx1 + . . . + ωn dxn :=
Z b
a
­
ω(x(t)), ˙x(t)
®
dt =
Z b
a
µ
ω1(x(t)) dx1
dt + . . . + ωn(x(t)) dxn
dt
¶
dt.
(56)
Integrals of 1-forms are also called line integrals. For a line integral we
need two ingredients: a path of integration and a 1-form. The integral de-
pends on both.
Example 2.5. Consider a “constant” 1-form E = 2dx−3dy in R2. Let γ be
the following path: x = t, y = 1−t, where t ∈[0, 1]. (It represents a straight
line segment [PQ] where P = (0, 1), Q = (1, 0).) To calculate
R
γ E, we ﬁrst
ﬁnd the velocity vector: ˙x = (1, −1) = e1 −e2 (constant, in this case). Next,
we take the value of E on ˙x: ⟨E, ˙x⟩= ⟨2dx −3dy, e1 −e2⟩= 2 −3(−1) = 5.
Hence,
Z
γ
E =
Z 1
0
⟨E, ˙x⟩dt =
Z 1
0
5dt = 5.
Remark 2.1. A practical way of calculating line integrals is based on the
following shortcut: the expression ⟨ω(x(t)), ˙x(t)⟩dt is simply a 1-form on
[a, b] ⊂R obtained from ω by substituting xi = xi(t) as functions of t given
by the path γ. We have to substitute both in the arguments of ωi(x) and
in the diﬀerentials dxi expanding them as the diﬀerentials of functions of t.
The resulting 1-form on [a, b] depends on both ω and γ, and is denoted γ∗ω:
γ∗ω =
µX
ωi(x(t)) dxi
dt (t)
¶
dt ,
(57)
so that
Z
γ
ω =
Z b
a
γ∗ω .
(58)
Example 2.6. Find the integral of the 1-form A = x dy −y dx over the path
γ : x = t, y = t2, t ∈[0, 2]. Considering x, y as functions of t (given by the
path γ), we can calculate their diﬀerentials: dx = dt, dy = 2t dt. Hence
Z
γ
A =
Z 2
0
γ∗A =
Z 2
0
¡
t(2t dt) −t2 dt
¢
=
Z 2
0
t2 dt = t3
3
¯¯¯¯
2
0
= 8
3
22

VECTOR CALCULUS. Fall 2002
Deﬁnition 2.3. An orientation of a path γ : x = x(t) is given by the direc-
tion of the velocity vector ˙x.
Suppose we change parametrization of a path γ. That means we consider
a new path γ′ : [a′, b′] →Rn obtained from γ by a substitution t = t(t′).
We use t′ to denote the new parameter. Let us assume that dt/dt′ ̸= 0, i.e.,
the function t′ 7→t = t(t′) is monotonous. If it increases, then the velocity
vectors dx/dt and dx/dt′ have the same direction; if it decreases, they have
the opposite directions. Recall that
dx
dt′ = dx
dt · dt
dt′.
(59)
The most important statement concerning line integrals is the following the-
orem.
Theorem 2.1. For arbitrary 1-form ω and path γ, the integral
R
γ ω does not
change if we change parametrization of γ provided the orientation remains
the same.
Proof. Consider
­
ω(x(t)), dx
dt
®
and
­
ω(x(t(t′))), dx
dt′
®
. As
¿
ω(x(t(t′))), dx
dt′
À
=
¿
ω(x(t(t′))), dx
dt
À
· dt
dt′,
we can use the familiar formula
Z b
a
f(t) dt =
Z b′
a′ f(t(t′)) · dt
dt′ dt′
valid if
dt
dt′ > 0, and the statement immediately follows.
If we change orientation to the opposite, then the integral changes sign.
This corresponds to the formula
Z b
a
f(t) dt = −
Z a
b
f(t) dt
in the calculus of a single variable.
Independence of parametrization allows us to deﬁne line integrals over
more general objects. We can consider integrals over any “contours” consist-
ing of pieces which can be represented by parametrized curves; such contours
can have “angles” and not necessarily be connected. We simply add inte-
grals over pieces. All that we need to calculate the integral of a 1-form over
a contour is an orientation of the contour, i.e., an orientation for every piece
that can be represented by a parametrized curve.
23

THEODORE VORONOV
Example 2.7. Consider in R2 a contour ABCD consisting of the segments
[AB], [BC], [CD], where A = (−2, 0), B = (−2, 4), C = (2, 4), D = (2, 0)
(this is an upper part of the boundary of the square ABCD). The orientation
is given by the order of vertices ABCD. Calculate
R
ABCD ω for ω = (x +
y) dx + y dy. The integral is the sum of the three integrals over the segments
[AB], [BC] and [CD]. As parameters we can take y for [AB] and [CD], and
x for [BC]. We have
Z
[AB]
ω =
Z 4
0
y dy = y2
2
¯¯¯¯
4
0
= 8
Z
[BC]
ω =
Z 2
−2
(x + 4) dx = (x + 4)2
2
¯¯¯¯
2
−2
= 18 −2 = 16
Z
[CD]
ω =
Z 0
4
y dy = −8
Hence
Z
ABCD
ω =
Z
[AB]
ω +
Z
[BC]
ω +
Z
[CD]
ω = 8 + 16 −8 = 16.
Notice that the integrals over vertical segments cancel each other.
Example 2.8. Calculate the integral of the form ω = dz over the perimeter
of the triangle ABC in R3 (orientation is given by the order of vertices), if
A = (1, 0, 0), B = (0, 2, 0), C = (0, 0, 3). We can parameterize the sides of
the triangle as follows:
[AB]: x = 1 −t, y = 2t, z = 0,
t ∈[0, 1]
[BC]: x = 0, y = 2 −2t, z = 3t,
t ∈[0, 1]
[CA]: x = t, y = 0, z = −3t,
t ∈[0, 1].
Hence
Z
[AB]
dz = 0,
Z
[BC]
dz =
Z 1
0
3dt = 3,
Z
[CA]
dz =
Z 1
0
(−3dt) = −3;
and the integral in question is I = 0 + 3 −3 = 0.
3
Algebra of forms
3.1
Jacobian
Recall the formula for the transformation of variables in the double integral:
Z
D
f(x, y) dxdy = ±
Z
D
f(x(u, v), y(u, v)) J(u, v) dudv
(60)
24

VECTOR CALCULUS. Fall 2002
where
J = D(x, y)
D(u, v) = det ∂(x, y)
∂(u, v)
(61)
is called the Jacobian of the transformation of variables. Here
∂(x, y)
∂(u, v) =
µ ∂x
∂u
∂x
∂v
∂y
∂u
∂y
∂v
¶
(62)
denotes the matrix of partial derivatives. The sign ± in (60) is the sign of
the Jacobian.
Example 3.1. For polar coordinates r, ϕ where x = r cos ϕ, y = r sin ϕ, we
have dx = cos ϕ dr −r sin ϕ dϕ, dy = sin ϕ dr + r cos ϕ dϕ, hence
D(x, y)
D(r, ϕ) =
¯¯¯¯
cos ϕ
−r sin ϕ
sin ϕ
r cos ϕ
¯¯¯¯ = r.
(63)
Thus we can write dx dy = r dr dϕ.
Problem 3.1. Calculate the area of a disk of radius R in two ways: using
the standard coordinates and using polar coordinates. By “area” we mean
R
D dxdy. (Here D = {(x, y) | −
√
R2 −x2 ⩽y ⩽
√
R2 −x2, −R ⩽x ⩽R} =
{(r, ϕ) | 0 ⩽r ⩽R, 0 ⩽ϕ ⩽2π}.)
We can ask ourselves the following question: is there a way of getting the
formula with the Jacobian by multiplying the formulae for the diﬀerentials?
Or, to put it diﬀerently: is it possible to understand dx dy as an actual
product of the diﬀerentials dx and dy so that the formula like dx dy = r dr dϕ
in the above example comes naturally?
The answer is “yes”. The rules that we have to adopt for the multiplica-
tion of dr, dϕ are as follows:
dr dr = 0, dr dϕ = −dϕ dr, dϕ dϕ = 0.
(64)
Indeed, if we calculate according to these rules, we get: dx dy = (cos ϕ dr −
r sin ϕ dϕ)(sin ϕ dr+r cos ϕ dϕ) = cos ϕ sin ϕ dr dr+r cos2 ϕ dr dϕ−r sin2 ϕ dϕ dr−
r2 sin ϕ cos ϕ dϕ dϕ = r cos2 ϕ dr dϕ−r sin2 ϕ (−dr dϕ) = r dr dϕ, as required.
(We also assumed the usual distributivity w.r.t. addition.) These are the only
rules that lead to the correct answer.
Notice that as a corollary we get similar rules for the products of dx and
dy:
dx dx = 0, dx dy = −dy dx, dy dy = 0
(65)
(check!).
25

THEODORE VORONOV
More generally, we can check that in this way we will get the general
formula for arbitrary changes of variables in R2: if x = x(u, v), y = y(u, v),
then
dx = ∂x
∂u du + ∂x
∂v dv
(66)
dy = ∂y
∂u du + ∂y
∂v dv,
(67)
and we get
dx dy =
µ∂x
∂u du + ∂x
∂v dv
¶ µ∂y
∂u du + ∂y
∂v dv
¶
=
∂x
∂u
∂y
∂v du dv + ∂x
∂v
∂y
∂u dv du =
µ∂x
∂u
∂y
∂v −∂x
∂v
∂y
∂u
¶
du dv = J du dv
(we used du dv = −dv du; there are no terms with du du = 0 and dv dv = 0).
The multiplication that we have just introduced is called the “exterior
multiplication”. The characteristic feature of the “exterior” rules is that the
product of two diﬀerentials changes sign if we change the order. In particular,
the exterior product of any diﬀerential with itself must vanish. Notice that a
product like dx dy is a new object compared to dx and dy: it does not equal
any of 1-forms, and it is the kind of expression that stands under the sign of
integral when we integrate over a two-dimensional region.
Now, if we consider Rn, a similar formula with the Jacobian is valid
Z
D
f dx1 . . . dxn = ±
Z
D
f J du1 . . . dun
(68)
where
J = D(x1, . . . , xn)
D(u1, . . . , un) = det ∂(x1, . . . , xn)
∂(u1, . . . , un).
(69)
How to handle it? Can we understand it in the same way as the formula in
two dimensions? It turns out that all we have to do is to extend the exterior
multiplication from just two to an arbitrary number of factors. We discuss
it in the next section.
3.2
Rules of exterior multiplication
Deﬁnition of forms in Rn. Deﬁnition of the exterior product. Examples.
Example: translation; linear change; x + xy.
Eﬀect of maps.
Jacobian obtained from n-forms.
26

VECTOR CALCULUS. Fall 2002
Remark 3.1. As well as dxi as a linear function on vectors gives the i-th
coordinate: dxi(h) = hi, the exterior product dxidxj can be understood as a
function on pairs of vectors giving the determinant
dxidxj(h1, h2) =
¯¯¯¯
hi
1
hi
2
hj
1
hj
2
¯¯¯¯ = hi
1hj
2 −hj
1hi
2 ,
(70)
and similarly for the triple exterior products like dxidxjdxk, etc. We will not
pursue it further.
4
Exterior derivative
Want to get d: Ωk →Ωk+1
4.1
Dependence of line integrals on paths
Consider paths in Rn
Theorem 4.1. The integral of df does not depend on path (provided the
endpoints are ﬁxed).
Closed paths that are boundaries: additivity of integral. Small path.
Let us explore how
R
γ ω depend on path in general. For this we shall
consider a “small” closed path and calculate the integral over it....
d for 1-forms
4.2
Exterior derivative: construction
Consider diﬀerential forms deﬁned in some open domain U ⊂Rn. In the
sequel we omit further references to U and simply write Ωk for Ωk(U). An
axiomatic deﬁnition of d is given by the following theorem:
Theorem 4.2. There is a unique operation d: Ωk →Ωk+1 for all k =
0, 1, . . . , n possessing the following properties:
d(a ω + b σ) = a dω + b dσ
(71)
for any k-forms ω and σ (where a, b are constants);
d(ωσ) = (dω) dσ + (−1)kω (dσ)
(72)
for any k-form ω and l-form σ; on functions
df =
X ∂f
∂xi dxi
(73)
is the usual diﬀerential, and
d(df) = 0
(74)
for any function f.
27

THEODORE VORONOV
Proof. Let us assume that an operator d satisfying these properties exists.
By induction we deduce that d(dxi1 . . . dxik) = 0. Hence for an arbitrary
k-form ω =
P
i1<...<ik
ωi1...ik dxi1 . . . dxik we arrive at the formula
dω =
X
i1<...<ik
dωi1...ik dxi1 . . . dxik.
(This establishes the uniqueness part of the theorem.) Now we take this
formula as a deﬁnition. By a direct check we can show that it will satisfy
all required properties. For example, to show that d(df) = 0 for an arbitrary
function f, we apply the above formula to df and obtain
d(df) = d
µX ∂f
∂xi dxi
¶
=
X
∂2f
∂xj∂xidxj dxi =
−
X
∂2f
∂xi∂xj dxi dxj = −
X
∂2f
∂xj∂xidxj dxi = −d(df)
(we swapped the partial derivatives using their commutativity and the dif-
ferentials in the exterior product dxj dxi getting the minus sign, and then
renamed the summation indices). Thus d(df) = 0, as required. (This estab-
lishes the existence part of the theorem.)
The formula
dω =
X
i1<...<ik
dωi1...ik dxi1 . . . dxik
is the working formula for the calculation of d. In many cases, though, it
is easier to calculate directly using the basic properties of d. On the other
hand, one can deduce “general” explicit expressions in particular cases (like
1-forms).
4.3
Main properties and examples of calculation
Example 4.1. Suppose ω is a (n −1)-form is Rn. We can write it as
ω =
X
ωi dx1 . . . dxi−1dxi+1 . . . dxn.
(75)
Then we easily obtain
dω =
X ∂ωi
∂xi dx1 . . . dxn.
(76)
Exact and closed forms
Theorem 4.3. d ◦F ∗= F ∗◦d
28

VECTOR CALCULUS. Fall 2002
5
Stokes’s theorem
The Stokes theorem is one of the most fundamental results that you will learn
from the university mathematics course. It has the form of the equality
Z
C
dω =
Z
∂C
ω,
where ω ∈Ωk−1(U) is a (k −1)-form, C a “k-dimensional contour” (to be
deﬁned) and ∂C the boundary of this contour (to be deﬁned, as well).
There are two cases already familiar.
The Newton–Leibniz formula (or the “main theorem of integral calculus”)
is for k = 1. Then k −1 = 0, and ω = f is just a function. If we take a path
γ : [0, 1] →U as a contour, then
Z
γ
df = f(Q) −f(P)
(77)
where P = γ(0), Q = γ(1). The r.h.s. should be considered as an “integral
of a 0-form” over the boundary of the path γ, which is a formal linear combi-
nation of its endpoints: Q−P; i.e., we treat f(Q)−f(P) as the “integral” of
f over Q−P. The integral of a function (as a 0-form) over a point is deﬁned
as the value at this point. It is essential that the path has the orientation
(direction) “from P to Q”.
The Green formula in the plane is the case k = 2 (hence k −1 = 1) and
n = 2. Then ω = ω1 dx1 + ω2 dx2. For a bounded domain D ⊂R2 with the
boundary being a closed curve C, we have the formula
Z
D
µ∂ω2
∂x1 −∂ω1
∂x2
¶
dx1 dx2 =
Z
C
¡
ω1 dx1 + ω2 dx2¢
.
(78)
Under the integral at the l.h.s. stands precisely the diﬀerential dω. Again,
it is important that the domain D and its boundary C are given coherent
orientations.
In this section we shall deﬁne all notions involved in Stokes’s theorem,
discuss its applications and give an idea of a proof.
5.1
Integration of k-forms
We are going to deﬁne integrals like
R
C ω where ω ∈Ω(U), U ⊂Rn is an
open domain, and C is a “k-dimensional contour”. (Such “contours” will be
called “chains”.) This will be achieved in several steps.
Step 1.
The “contours” are supposed to be oriented. So we have to
introduce the notion of an orientation. Let D ⊂Rk be some bounded domain.
What is an “orientation” of D? Consider examples.
29

THEODORE VORONOV
k = 0. D is just a point. An orientation of a point is simply a sign
(plus or minus) assigned to this point. For example, in the Newton–Leibniz
formula above we meet the expression f(Q) −f(P); it should be treated as
an integral of a 0-form f over the formal diﬀerence Q−P, i.e., over the points
P and Q where to P we assign +1 and to Q we assign −1.
k = 1. D = [a, b] is a segment. An orientation is a (choice of) direction:
from a to b or from b to a.
k = 2. We have D a domain in R2. An orientation here is a sense of
rotation, which we may describe as “counterclockwise” or “clockwise”. It
is important to realize that these terms have no absolute meaning: if we
look at a sketch of our domain from the other side of the paper, what was
“counterclockwise” will be “clockwise”, and vice versa. The labels by which
we distinguish the two possible orientations of D are relative; important is
that there are exactly two of them. (We assume that D is connected; other-
wise each connected component can be assigned one of the two orientations
independently.) How one can specify an orientation? A general method is
to choose a basis of vectors e1, e2 (at a point of D) and by “moving” it to
other points of D get a basis at all points continuously depending on a point.
Then the sense of rotation at every point will be given as from e1 to e2. It
is clear that if we slightly deform the basis (move the vectors e1, e2 at each
point slightly) or multiply it by a matrix with a positive determinant, the
sense of rotation deﬁned by e1, e2 will not change. In particular, if we have a
coordinate system x1, x2 in D and take the basis e1 = ∂x/∂x1, e2 = ∂x/∂x2,
then any two coordinate systems deﬁne the same orientation if and only if
the Jacobian of the transformation of coordinates is positive, and they deﬁne
the opposite orientations if the Jacobian is negative.
Example 5.1. Consider in R2 the standard coordinates x, y and the polar
coordinates r, ϕ so that x = r cos ϕ, y = r sin ϕ. Then
J = D(x, y)
D(r, ϕ) =
¯¯¯¯
cos ϕ
−r sin ϕ
sin ϕ
r cos ϕ
¯¯¯¯ = r > 0,
(79)
hence the coordinate systems x, y and r, ϕ deﬁne the same orientation in
the plane. The order of coordinates is very important: if we consider ϕ, r
instead (the ϕ coordinate considered as the ﬁrst and the r coordinate con-
sidered as the second), then the columns of the determinant in (79) will be
swapped and the Jacobian will change sign. In general, if we swap two co-
ordinates, then the orientation changes to the oppositive. Also, if we change
ϕ to ϕ′ = −ϕ (i.e., begin to count the polar angles “clockwise” instead of
“counterclockwise”), then the second column will change sign, again giving
a negative Jacobian.
k > 2. In this general case we deﬁne an orientation in D ⊂Rk using a
basis e1, . . . , ek at every point of D (we assume that the vectors of the basis
30

VECTOR CALCULUS. Fall 2002
smoothly depend on point). Two bases e1, . . . , ek and g1, . . . , gk are said to
be equivalent or deﬁning the same orientation if and only if gi = Pk
j=1 ejAj
i
with det A > 0. Notice that every two bases in a vector space diﬀer by a
linear transformation with an invertible matrix. The point is that the matrix
should have a positive determinant for the bases in question to deﬁne the
same orientation. Notice that the matrix A is invertible, hence det A is never
zero. It follows that if we slightly perturb the basis, i.e., take A close to the
identity matrix E, or consider a continuous family of transformations starting
at the identity, then the determinant cannot change sign (as det E = 1 and
passing through 0 is impossible), so the orientation will not change.
In particular, it is possible to take the bases corresponding to coordi-
nate systems in D. Then any two coordinate systems, say, x1, . . . , xk and
y1, . . . , yk deﬁne the same orientation in D if
J = D(x1, . . . , xk)
D(y1, . . . , yk) =
¯¯¯¯¯¯¯
∂x1
∂y1
. . .
∂x1
∂yk
. . .
. . .
. . .
∂xk
∂y1
. . .
∂xk
∂yk
¯¯¯¯¯¯¯
> 0,
(80)
and they deﬁne the opposite orientations if the Jacobian is negative. (This
deﬁnition applied to k = 1 and k = 2 embraces the cases considered sep-
arately above. It turns out that only the case k = 0 requires an ad hoc
deﬁnition.)
Problem 5.1. For spherical coordinates in R3 ﬁnd out which order of coor-
dinates: r, ϕ, θ or r, θ, ϕ gives the same orientation as x, y, z.
Step 2.
Now we shall deﬁne the integral of an n-form over a bounded
domain D ⊂Rn. Consider ω ∈Ωn(Rn). Let x1, . . . , xn be some coordinates
in D. Since there is just one independent n-fold product of the diﬀerentials
dxi here, the form ω has a very simple appearance:
ω = f(x1, . . . , xn) dx1 . . . dxn,
(81)
where f is a function of the coordinates. We deﬁne the integral of an n-form
ω over an n-dimensional domain D as
Z
D
ω =
Z
D
f(x1, . . . , xn) dx1 . . . dxn,
(82)
where at the r.h.s. stands the usual multiple integral as deﬁned in calculus
courses (via partitions and limits of integral sums). Let us recall the theorem
about the change of variables in multiple integrals. If we introduce “new”
coordinates y1, . . . , yn in the domain D, then
Z
D
f(x1, . . . , xn) dx1 . . . dxn =
±
Z
D
f(x1(y1, . . . , yn), . . . , xn(y1, . . . , yn)) J dy1 . . . dyn.
(83)
31

THEODORE VORONOV
Here J = J(y1, . . . , yn) is the Jacobian D(x1, . . . , xn)/D(y1, . . . , yn). Forget-
ting for a moment about the sign ± in (83), we recognize the l.h.s. as
R
D ω
(deﬁned using the coordinates x1, . . . , xn) and the expression under the inte-
gral in the r.h.s. as exactly the form ω written in the coordinates y1, . . . , yn.
Indeed, the rules of the exterior multiplication are devised precisely to give
dx1 . . . dxn = J dy1 . . . dyn when xi are expressed as functions of yj (see
Section 3.1). It follows that the integral in the r.h.s. is the integral of ω
calculated in the coordinates yj. Hence, up to a sign, we see that the inte-
gral
R
D ω does not depend on a choice of coordinates in D. Now, what is
the meaning of the sign? Notice that ± = sign J. We obtain the following
theorem.
Theorem 5.1. The integral of an n-form ω over D ⊂Rn does not depend on
a choice of coordinates in D provided the orientation is ﬁxed. If we change
orientation, the integral changes sign.
Example 5.2. Let I2 = {(x, y) | 0 ⩽x, y ⩽1} be the unit square in R2. Let
an orientation be given by the coordinates x, y. Then
Z
I2 dx dy = −
Z
I2 dy dx = 1.
(84)
Deﬁne an oriented domain as a domain D with a chosen orientation. Let
−D denote the same domain with the opposite orientation. Hence, we can
integrate n-forms over oriented domains in Rn, and
Z
−D
ω = −
Z
D
ω.
(85)
In the above we assumed that n > 0. For n = 0, we deﬁne the integral of
a 0-form in R0, which just a number (R0 is a single point!), to be this number
if the point is taken with + or minus this number if the point is taken with
−(see above on an orientation for k = 0).
Step 3.
Now we want to learn how to integrate k-forms in Rn. We have
to explain ﬁrst over which objects forms will be integrated. Bear in mind the
analogy with k = 1. We deﬁne a k-path or a k-dimensional path) in Rn or
in U ⊂Rn (an open domain) as a smooth map Γ: D →U, where D ⊂Rk
is a bounded domain. (Recall that for k = 1, a path or a “1-path” is a map
[a, b] →U.) A “parametrization” of a k-path Γ is a choice of coordinates in
D. For any ω ∈Ωk(U) deﬁne the integral of a k-form over a k-path
Z
Γ
ω :=
Z
D
Γ∗ω.
(86)
Here Γ∗: Ωk(U) →Ωk(D) is the pull-back map. The integral at the r.h.s. is
the integral of a k-form over a bounded domain in Rk. It does not depend
32

VECTOR CALCULUS. Fall 2002
on a choice of coordinates as long as we do not change the orientation. An
orientation of D will be called an orientation of the k-path Γ. It follows
that the integral of k-forms is well-deﬁned on oriented k-paths. If we agree
to denote by Γ an oriented k-path and by −Γ the same k-path with the
opposite orientation, then we have
Z
−Γ
ω = −
Z
Γ
ω.
(87)
Since we can integrate over k-paths so that the integral depends only on
orientation and not a parametrization, we can extend the integral to any
objects that can be “cut” into pieces representable by k-paths, — provided
the orientations on the pieces are ﬁxed. Following the 1-dimensional exam-
ple, we deﬁne a k-chain (or a k-dimensional chain) in U as a formal linear
combination of oriented k-paths:
C = a1Γ1 + a2Γ2 + . . . + arΓr,
(88)
where ai ∈R and Γi are k-dimensional paths with chosen orientations. When
we take linear combinations we agree that −(−Γ) = Γ, where the minus sign
in the brackets means taking the opposite orientation (so that the “formal”
multiplication by −1 coincides with changing the orientation). We deﬁne the
integral of a k-form in U ⊂Rn over a k-chain as the sum of integrals over
k-paths:
Z
C
ω =
Z
a1Γ1+...+arΓr
ω := a1
Z
Γ1
ω + . . . + ar
Z
Γr
ω.
(89)
Remark 5.1. The standard deﬁnitions of chains are more restrictive than the
one given above. We have not speciﬁed bounded domains D ⊂Rk that are
allowed in k-paths. Such ﬂexibility is convenient for practical calculations,
but makes it diﬃcult to advance with general theorems.
For theoretical
purposes, as the experience shows, it is better to consider chains made of
k-paths deﬁned as maps Γ: D →U where D is a standard domain like the
unit k-cube (cubical chains) or the standard k-simplex (simplicial chains).
Remark 5.2. To integrate over a k-dimensional surface, like a sphere, we
have to represent it by a chain. First of all, a surface should be oriented, i.e.,
an orientation should be chosen at every tangent plane by a basis continu-
ously depending on a point. If it is not possible, then the integral cannot be
deﬁned. An oriented surface can be cut into pieces representable by oriented
k-paths, so that the whole surface can be replaced by their sum (a chain).
The integral over an oriented surface is deﬁned as the integral over this chain.
Clearly, the integral will not depend on a particular cutting into pieces, i.e.,
a representation of the surface by a chain.
33

THEODORE VORONOV
Example 5.3. Calculate the integral of the 2-form x dx dy + (2 + 3y) dx dz
over the 2-path Γ: D →R3, Γ: (u, v) 7→(u, v, u2+v2), and D: 0 ⩽u, v ⩽1.
We ﬁnd Γ∗ω = u du dv + (2 + 3v) du d(u2 + v2) = (u + 2 + 6v2) du dv. Hence
Z
Γ
ω =
Z
D
(u+2+6v2) du dv =
Z 1
0
Z 1
0
(u+2+6v2) du dv =
Z 1
0
(u+2+2) du = 41
2.
Example 5.4. Calculate the integral of the 2-form ω = z dx dy over the
the unit sphere with center O = (0, 0, 0) in R3. Denote this surface S2. An
orientation is ﬁxed by the basis e1 = (1, 0, 0), e2 = (0, 1, 0) at the point
(0, 0, 1) (the “north pole”). At all other points of the sphere we can get a
basis deﬁning the orientation by dragging to these points the given basis at
(0, 0, 1) (so that it remains tangent to the sphere). We have to represent
the sphere by a chain. One way of doing this is to cut it into the upper
hemisphere and the lower hemisphere.
Consider the 2-paths Γ+ and Γ−,
where Γ+ : D →R3, (u, v) 7→(u, v, +
√
1 −u2 −v2), and Γ−: D →R3,
(u, v) 7→(u, v, −
√
1 −u2 −v2).
Here D = {u2 + v2 ⩽1} (for both 2-
paths). Now, what about the orientation? One can see that at the south pole
(0, 0, −1) the given orientation of the sphere S2 is opposite to that speciﬁed
by e1, e2 at this point (check!). Hence for Γ+ we should take the orientation
by the coordinates u, v, and for Γ−the opposite orientation, so the sphere is
represented by the 2-chain C = Γ+ −Γ−. We have
Z
S2 ω =
Z
C
ω =
Z
Γ+
ω −
Z
Γ−
ω =
Z
D
Γ∗
+ω −
Z
D
Γ∗
−ω =
Z
D
√
1 −u2 −v2 du dv −
Z
D
(−
√
1 −u2 −v2 du dv) =
2
Z
D
√
1 −u2 −v2 du dv
(we skip the calculation of the remaining double integral).
Problem 1. Calculate the integral
R
S2 ω using spherical coordinates (i.e.,
representing the sphere by the chain consisting of a single 2-path: x =
cos ϕ sin θ, y = sin ϕ sin θ, z = cos θ, 0 ⩽ϕ ⩽2π, 0 ⩽θ ⩽π) and check
that you will get the same answer.
5.2
Stokes’s theorem: statement and examples
Theorem 5.2 (Stokes’s theorem for chains). Let U ⊂Rn be an open
domain. For every ω ∈Ωk−1(U) and every k-chain C in U,
Z
∂C
ω =
Z
C
dω,
(90)
where ∂C is the boundary of C.
34

VECTOR CALCULUS. Fall 2002
The only notion that we have not discussed yet is the “boundary of a
chain”.
Remark 5.3. The theorem remains true if we replace U ⊂Rn by any
“smooth n-dimensional manifold”. Smooth manifolds are sets that locally
look like open domains of Rn. All calculus, including the theory of diﬀeren-
tial forms, can be extended to smooth manifolds.
Remark 5.4. In the statement of the theorem, the chain C can be replaced
by an “oriented submanifold with boundary”, e.g., a closed disk.
The boundary of a k-chain C is a (k −1)-chain deﬁned as follows. If
C = a1Γ1 + . . . + arΓr, we set ∂C = a1 ∂Γ1 + . . . + ar ∂Γr. Hence we have to
deﬁne the chain ∂Γ for an oriented k-path Γ. Loosely speaking, if Γ: D →U
and an orientation of D is ﬁxed, then ∂Γ is given by the map Γ restricted to
∂D, where ∂D is the boundary of the domain D ⊂Rk in the natural sense,
endowed with the orientation compatible with that of D.
Example 5.5. Suppose D = [a, b] ⊂R and the orientation is from a to b.
Then the boundary of D consists of a and b, and in view of the Newton–
Leibniz formula it is natural to take a with “minus” and b with “plus”. We
can consider ∂[a, b] as the chain b −a (formal diﬀerence! not the subtraction
of numbers).
Example 5.6. Suppose D ⊂R2 is the unit disk with center at the origin ori-
ented by the coordinates x, y. Then ∂D is the unit circle with the orientation
counterclockwise.
Example 5.7. Suppose D ⊂R2 is the unit square {(x, y) | 0 ⩽x, y ⩽1}
oriented by the coordinates x, y. Then ∂D consists of four segments: [OA],
[AB], [BC], [CO] with the orientations from O to A, from A to B, from B
to C, from C to O. Here O = (0, 0), A = (1, 0), B = (1, 1), C = (0, 1). We
can write ∂D = [OA] + [AB] + [BC] + [CO].
Example 5.8. Suppose D ⊂R3 is the unit ball with center at the origin
oriented by the coordinates x, y, z. Then ∂D is the unit sphere with the
orientation given by the basis e2, e3 at the point (1, 0, 0). Here e1, e2, e3 is
the standard basis (corresponding to x, y, z).
Problem 5.2. Show that the same orientation on the sphere is given at the
point (0, 0, 1) by the basis e1, e2. (Drag the vectors e2, e3 from (1, 0, 0) to
(0, 0, 1) along the meridian and see what you will get there.)
We see that the problem is that in some cases (like for the disk or the
ball above), the boundary of D is not naturally deﬁned as a chain, though it
may be cut into pieces making up a chain. Here is exactly the point where
35

THEODORE VORONOV
restricting by some standard domains like a unit cube has a great advantage
(see Remark 5.1).
How to induce an orientation on ∂D by a given orientation for D? Notice
that at the points of the boundary it is always deﬁned the direction “inwards”
the domain as well as the opposite direction “outwards” the domain. More
precisely, for a vector not tangent to the boundary we can always tell if it
points “inwards” or “outwards”.
Example 5.9. For the ball x2 + y2 + z2 ⩽1, the basis vector e3 at the
point (0, 0, 1) (the north pole) points outwards and at the point (0, 0, −1)
(the south pole) it points inwards.
Deﬁnition 5.1. Suppose an orientation of a domain D ⊂Rk is given by
some basis e1, . . . , ek. Then the induced orientation for ∂D is given by a
basis g1, . . . , gk−1 (consisting of vectors tangent to ∂D) such that
(e1, . . . , ek) ∼(n, g1, . . . , gk−1),
where n is any vector pointing outwards. Here ∼means the equivalence of
bases, i.e., that they deﬁne the same orientation.
Problem 5.3. Check that in the examples above the orientation of the
boundary is given by this rule.
Example 5.10. Consider in R2 a domain between two circles (not necessarily
concentric). Check that if the domain is oriented in such a way that at the
outer circle the induced orientation is counterclockwise, then at the inner
circle it is clockwise, and vice versa.
Theorem 5.2 is valid for any chains where the boundary (with the induced
orientation) is understood as explained above. It is also valid for “subman-
ifolds” and their boundaries (as we have already mentioned), and again the
orientation on the boundary is given as in Deﬁnition 5.1.
Proposition 5.1. For any chain, ∂(∂C) = 0.
This is easy to prove for chains based on some particular standard do-
mains, like cubical chains or simplicial chains. A chain C such that ∂C = 0
is called a closed chain or a cycle. An example of a 1-cycle is a closed path.
Any oriented surface without boundary, like a sphere, can be cut into pieces
making up a cycle. In particular, any boundary, i.e., a chain which is the
boundary of some other chain is a cycle. It is customary to denote integrals
over cycles by
H
. In this notation the Stokes theorem reads
I
∂C
ω =
Z
C
dω.
(91)
The Stokes theorem has the following immediate corollaries:
36

VECTOR CALCULUS. Fall 2002
Corollary 5.1. The integral of a closed form over a boundary is zero: if
dω = 0, then
I
∂C
ω = 0.
The integral of an exact form over any cycle is zero: if ∂C = 0, then
I
C
dω = 0.
Corollary 5.2. If two k-chains, C and C′, bound a (k + 1)-chain, i.e.,
C −C′ = ∂W for some (k + 1)-chain W, then
Z
C
ω =
Z
C′ ω
for any k-form ω.
Consider some elementary examples.
Example 5.11. Let ω = a dx + b dy in R2, where a, b are constants. Then
dω = 0, hence
I
∂D
ω = 0
for every bounded domain D ⊂R2.
Example 5.12. Consider the 1-form
ω = 1
2(x dy −y dy)
(92)
in R2. Then dω = dx dy. Hence for any bounded domain D,
I
∂D
ω =
Z
D
dx dy = “area of D”.
(We shall discuss areas at greater detail in the next section.) The contour in
the l.h.s. should be oriented counterclockwise.
The form from the previous example can be re-written in polar coordi-
nates as
ω = 1
2(x dy −y dy) = 1
2r2 dϕ
(93)
(check!). Hence, integration of 1
2r2 dϕ over the boundary of a domain gives
its area.
Example 5.13. The area of a sector of angle ∆ϕ of a disk of radius R equals
1
2
I
∂D
r2 dϕ = 1
2 R2∆ϕ
(94)
(the integrals over the two radii are zero, only the integral over the circular
arc gives an input). In particular, the area of the whole disk (∆ϕ = 2π) is
πR2.
37

THEODORE VORONOV
Example 5.14. Consider
d ln r = dr
r = x dx + y dy
x2 + y2
.
(95)
Since this form is exact in R2 \ {(0)}, the integral of it over any cycle in
R2 \ {(0)} vanishes.
Example 5.15. Consider the 1-form
dϕ = d arctan y
x = x dy −y dx
x2 + y2
.
(96)
By Corollary 5.2, the integral of this form is the same for all closed paths
that go around the origin once, and equals 2π. In general,
I
C
dϕ = 2πn,
n ∈Z,
(97)
if a cycle C “goes around the origin n times” (example: x = cos nt, y = sin nt,
t ∈[0, 2π]). If a cycle does not go around the origin, the integral is zero.
Notice that n can be negative, and n = 0 for a cycle that goes around the
origin once and then goes back once (one can show that every such cycle is
a boundary).
Example 5.16. Consider in Rn the following (n −1)-form:
ω = x1 dx2dx3 . . . dxn −x2 dx1dx3 . . . dxn + . . . + (−1)n−1xn dx1dx2 . . . dxn−1
((x1)2 + . . . + (xn)2)n/2
.
(98)
For n = 3 it is the form
ω = x dy dz −y dx dz + z dx dy
(x2 + y2 + z2)3/2
(99)
and for n = 2 it is the form dϕ considered in Example 5.15. One can show
that dω = 0 for all n. On the other hand, if SR stands for the sphere of
radius R with center at the origin O = (0, . . . , 0), then
I
SR
ω = An.
(100)
where An ̸= 0 is a constant independent of R. Indeed, the restriction of ω to
SR coincides with the restriction of the form ω′ = R−n(x1 dx2dx3 . . . dxn −
x2 dx1dx3 . . . dxn + . . . + (−1)n−1xn dx1dx2 . . . dxn−1) deﬁned at all points of
Rn. Since dω′ = R−nn dx1 . . . dxn, by the Stokes theorem we get
I
SR
ω =
I
SR
ω′ =
Z
BR
R−nn dx1 . . . dxn = n R−n vol BR = n vol B1,
(101)
where BR denotes the ball of radius R in Rn.
In particular, the form (98) is a closed but not exact form in Rn \ O
(otherwise the integral over any cycle would be zero).
38

VECTOR CALCULUS. Fall 2002
5.3
A proof for a simple case
We shall give a proof of the Stokes theorem for cubical chains. In fact, the
statement for this case implies the statement in general, but we will not prove
this.
Notice ﬁrst that by linearity the general Stokes formula for chains can be
reduced to the formula for a single k-path:
I
∂Γ
ω =
Z
Γ
ω.
(102)
Since ∂Γ is nothing but the map Γ: D →U restricted to the boundary of
the domain D, we have the l.h.s. in (102) equal to
I
∂D
Γ∗ω
and the r.h.s. equal to
Z
D
Γ∗dω =
Z
D
d(Γ∗ω),
since d◦F ∗= F ∗◦d for any map F (Theorem 4.3). Hence we have to prove
that
I
∂D
Γ∗ω =
Z
D
d(Γ∗ω)
(103)
for a (k −1)-form Γ∗ω in D ⊂Rk.
We can conclude that to prove Stokes’s theorem it would suﬃce to prove
it just for a bounded domain in Rn and (n −1)-forms in it:
I
∂D
ω =
Z
D
dω.
(104)
This is one of the strengths of the theory of forms — due to pullbacks and the
fact that d commutes with pullbacks. (In obsolete expositions not making
use of forms, proofs of statements like Stokes’s theorem are much harder
exactly because of the lack of pullbacks.) To make (104) more explicit, we
can write ω as P ωi dx1 . . . dxi−1dxi+1 . . . dxn. Then we can use
Remark 5.5. There is a problem of singling out a class of domains D in
Rn for which the Stokes theorem is valid. We do not want to do it pre-
cisely. Instead let us just mention that this class should contain all natural
examples, like cubes, balls, and any bounded domains speciﬁed by a ﬁnite
set of inequalities fi(x) ⩾0 in Rn. A good example is the domain between
two graphs of functions xn = f(x1, . . . , xn−1) in Rn where the argument
(x1, . . . , xn−1) runs over a similar type region in Rn−1. In all such cases it is
possible to deﬁne clearly ∂D, the induced orientation for it and the integral
39

THEODORE VORONOV
of any (n −1)-form. It seems intuitively clear, — in fact, this is a statement
of a topological nature proved in the so-called homology theory, — that every
D as above can be cut into “cubical” pieces, i.e., that can be made cubes by
changes of variables, so that the sum of the boundaries of all cubes gives a
chain representing the boundary of D, with the induced orientation, and all
faces that are not on ∂D enter the formula with opposite orientations, hence
are cancelled.
Example 5.17. Consider a square in R2 oriented by coordinates x, y. Cut it
into four smaller squares and consider them with the same orientation. Take
the boundary of each of the smaller squares (with the induced orientation),
and check that all “internal” sides appear twice with the opposite orienta-
tions, hence in the sum they cancel, leaving only the sides subdividing the
sides of the original square, with the correct orientations.
Now we shall prove the Stokes formula for a cube. It directly implies the
theorem for all cubical chains. From the above remark it also follows that
it implies the theorem for all other chains (and actually for manifolds with
boundary).
Let Qn denote the standard unit n-cube in Rn, i.e.,
Qn = {x ∈Rn | 0 ⩽xi ⩽1 for all i}.
(105)
The boundary of Qn consists of 2n sides, which we can treat as embeddings
of the standard unit (n −1)-cube Qn−1. Hence ∂Qn is a cubical chain, and
the boundary of every cubical chain (made on standard unit cubes) is again
a cubical chain of the same type. So it is a convenient class of chains, closed
under the action of ∂.
Let us describe the orientations induced on the sides of Qn. (For all n
we take the orientation of Qn by the coordinates x1, . . . , xn.) Fix some i =
1, . . . , n. By setting xi to 1 or to 0 we get two sides, which we denote j±
i where
plus denotes xi = 1 and minus denotes xi = 0. We have the embeddings
j±
i : Qn−1 →Qn where x1 = y1, . . . , xi−1 = yi−1, xi+1 = yi, . . . , xn = yn−1
and xi = 0 or xi = 1 (we denoted by yi the coordinates on Qn−1). The
standard orientation of Qn−1 corresponds to the orientation by the basis
e1, . . . , ei−1, ei+1, . . . , en, i.e., the standard basis in Rn with the i-th vector
omitted.
Notice that ei points outwards for j+
i
and inwards for j−
i .
To
get the induced orientation on j±
i
we have to compare the orientation of
±ei, e1, . . . , ei−1, ei+1, . . . , en with that of e1, . . . , en. It follows that we have
to take (−1)i−1 for j+
i and −(−1)i−1 for j−
i . We have arrived at a statement:
Proposition 5.2. The boundary of the cube Qn is given by the formula:
∂Qn =
n
X
i=1
(−1)i−1(j+
i −j−
i ).
(106)
40

VECTOR CALCULUS. Fall 2002
Problem 5.4. Check that ∂(∂Qn) = 0 as a cubical chain (made of maps
Qn−2 →Qn).
Discussion: particular cases, recollection of the meaning of d and exterior
product.
6
Classical integral theorems
Historical remark.
6.1
Forms corresponding to a vector ﬁeld
In this section we consider Rn as a Euclidean space (see Remark 1.2). In
particular, in the standard coordinates the scalar product of vectors X and
Y , which we denote (X, Y ) or X · Y , is given by the formula
(X, Y ) = X · Y = X1Y 1 + . . . + XnY n =
X
XiY i.
(107)
We call coordinates in which the scalar product is given by (107) Cartesian
coordinates. In particular, the standard coordinates are Cartesian. It is not
diﬃcult to see that all Cartesian coordinates are obtained from the standard
coordinates by changes of variables of the form
yi =
X
Ai
jxj + bi
(108)
where the matrix A = (Ai
j) is orthogonal.
In an arbitrary coordinate system the expression for the scalar product
will be more complicated.
Example 6.1. Consider polar coordinates in R2. To them correspond the
basis er, eϕ:
er = (cos ϕ, sin ϕ)
(109)
eϕ = (−r sin ϕ, r cos ϕ).
(110)
For vectors X = X1er + X2eϕ, Y = X1er + X2eϕ we obtain
(X, Y ) = (X1er + X2eϕ, X1er + X2eϕ) =
X1Y 1(er, er) + X1Y 2(er, eϕ) + X2Y 1(er, eϕ) + X2Y 2(eϕ, eϕ).
Since from (109),(110) we have (er, er) = 1, (er, eϕ) = 0, and (eϕ, eϕ) = r2,
we ﬁnally obtain
(X, Y ) = X1Y 1 + r2 X2Y 2.
(111)
This is the formula for the scalar product in polar coordinates.
41

THEODORE VORONOV
In general, if X = P Xiei in the basis ei = ∂x
∂xi associated with a coor-
dinate system x1, . . . , xn, and similarly for Y , then
(X, Y ) = X · Y =
X
XiY jgij
(112)
where gij = (ei, ej). These coeﬃcients in general depend on point, and the
scalar product makes sense only for vectors attached to the same point.
Example 6.2. In polar coordinates the matrix (gij) is
(gij) =
µ1
0
0
r2
¶
.
(113)
Problem 6.1. For spherical coordinates r, θ, ϕ in R3 where
x = r sin θ cos ϕ
(114)
y = r sin θ sin ϕ
(115)
z = r cos θ
(116)
calculate pairwise scalar products of er, eθ, eϕ and show that
(gij) =


1
0
0
0
r2
0
0
0
r2 sin2 θ

,
(117)
so
(X, Y ) = X1Y 1 + r2 X2Y 2 + r2 sin2 θ X3Y 3.
(118)
After these preliminaries we can pass to the main topic. Consider vector
ﬁelds in Rn, i.e., smooth functions associating with every point x in some
open domain U ⊂Rn a vector X(x) attached to this point: x 7→X(x).
We can visualize this as arrows attached to all points of U and smoothly
varying with points. Treating them as velocity vectors, we obtain a picture
of a vector ﬁeld as a “ﬂow” in U, so that each vector X(x) represents the
velocity of the ﬂow at the point x. This hydrodynamical picture is very
helpful.
With every vector ﬁeld X in Rn are associated two diﬀerential forms:
(1) a 1-form denoted X · dr, called the circulation form of X,
(2) an (n −1)-form denoted X · dS, called the ﬂux form of X.
Before giving precise deﬁnitions, let us give a rough idea. Suppose we
visualize X as a ﬂow of some ﬂuid. If γ is a path (or a 1-chain), it is natural
to look for the measure of ﬂuid that circulates along γ in a unit of time.
Likewise, for an (n −1)-dimensional surface in Rn (or an (n −1)-chain) it
is natural to look for the measure of ﬂuid that passes across the surface in
a unit of time. The answers to these questions are given by the integrals of
the forms X ·dr and X ·dS respectively, hence the names. (“Flux” in Latin
means “ﬂow”.)
42

VECTOR CALCULUS. Fall 2002
Deﬁnition 6.1. The circulation form corresponding to a vector ﬁeld X,
notation: X · dr (or X · dx), is a 1-form that on every vector Y takes the
value (X, Y ), the scalar product of X and Y :
⟨X · dr, Y ⟩= (X, Y ).
(119)
We immediately conclude that in Cartesian coordinates, if X = P Xiei,
then
X · dr =
X
Xi dxi.
(120)
Indeed, the l.h.s. of (119) is always P ωiY i in arbitrary coordinates (where
X · dr = ω = P ωi dxi). Comparing with (107), we get (120). In general
coordinates we similarly obtain
X · dr =
X
i,j
gijXi dxj
(121)
where gij = (ei, ej).
Example 6.3. In the plane, if X is given in Cartesian coordinates x, y as
X = X1e1 + X2e2, then
X · dr = X1 dx + X2 dy.
If X is given in polar coordinates r, ϕ as X = X1er + X2eϕ (the coeﬃcients
X1, X2 now have a diﬀerent meaning), then
X · dr = X1 dr + r2 X2 dϕ.
The correspondence between vectors ﬁelds and 1-forms given by X 7→
X · dr is invertible. Any 1-form ω = P ωi dxi is the circulation form for a
unique vector ﬁeld X. All we have to do is to solve the equation (121) for
the coeﬃcients Xi. We obtain
X =
X
i,j
gijωj ei
(122)
where gij (with upper indices) are the coeﬃcients of the inverse matrix for
(gij).
An important example is given by the notion of the gradient of a
function.
Deﬁnition 6.2. The gradient of a function f, notation: grad f, is the vector
ﬁeld corresponding to the 1-form df:
grad f · dr = df.
(123)
Example 6.4. In Cartesian coordinates in Rn
grad f = ∂f
∂x1 e1 + . . . + ∂f
∂xn en.
(124)
43

THEODORE VORONOV
Example 6.5. In polar coordinates in R2
grad f = ∂f
∂r er + 1
r2
∂f
∂ϕ eϕ.
(125)
Example 6.6. In spherical coordinates in R3
grad f = ∂f
∂r er + 1
r2
∂f
∂θ eθ +
1
r2 sin2 θ
∂f
∂ϕ eϕ.
(126)
We see that while df has a universal form in all coordinate systems, the
expression for grad f depends on particular coordinates.
Now we shall deﬁne the ﬂux form X · dS. To do so, we have to make
a digression and discuss volumes in a Euclidean spaces. Let x1, . . . , xn be
Cartesian coordinates.
The volume of any bounded domain D ⊂Rn is
deﬁned as
vol D :=
Z
D
dx1 . . . dxn.
(127)
It is clear that the deﬁnition does not depend on a choice of Cartesian coor-
dinates if the orientation is not changed. (Indeed, the Jacobian is det A for A
in (108), which is +1 or −1, since A is orthogonal.) Hence we have actually
deﬁned the “oriented volume” of an oriented domain D, the absolute value
of which is the usual volume. For n = 2 volume is called area. (For n = 1
“volume” is length.)
Example 6.7. Let Π(a, b) be a parallelogram spanned by vectors a and b
in R2. Then by a direct calculation of the integral (check!)
area Π(a, b) =
¯¯¯¯
a1
a2
b1
b2
¯¯¯¯
(128)
if a = a1e1 + a2e2, b = b1e1 + b2e2 in Cartesian coordinates.
Obviously, this generalizes to an arbitrary n: in Cartesian coordinates
the volume of parallelipiped in Rn spanned by vectors a1, . . . , an is
vol Π(a1, . . . , an) =
¯¯¯¯¯¯
a1
1
. . .
an
1
. . .
. . .
. . .
a1
n
. . .
an
n
¯¯¯¯¯¯
.
(129)
It is possible, starting from (128), (129) to give an “intrinsic” expression
for this volume, entirely in terms of lengths of the vectors a1, . . . , an and
angles between them, i.e., in terms of the pairwise scalar products of ai.
Lemma 6.1. For all n,
(vol Π(a1, . . . , an))2 =
¯¯¯¯¯¯
a1 · a1
. . .
a1 · an
. . .
. . .
. . .
an · a1
. . .
an · an
¯¯¯¯¯¯
.
(130)
At the r.h.s. stands the determinant made of all pairwise scalar products of
ai.
44

VECTOR CALCULUS. Fall 2002
This lemma can be proved by induction in n.
It is clear for n = 1
(|a|2 = a · a). For the inductive step one can notice that both sides does not
change if to one of the vectors is added a linear combination of the others.
We skip the remaining details.
Problem 6.2. Verify (130) directly for n = 2, by explicitly calculating the
determinant in the r.h.s. in Cartesian coordinates.
The usefulness of Lemma 6.1 is double. First, it gives us a coordinate-free
formula for the volume of a parallelipiped (and the area of a parallelogram).
Second, because it does not involve coordinates, it is applicable to a system of
k vectors in n-dimensional space for k ⩽n (as any such system is contained
in a k-dimensional subspace). It gives in this case a formula for a k-volume
in n-space.
For example, it gives a formula for the area of an arbitrary
parallelogram in R3.
Corollary 6.1. In arbitrary coordinates the volume of a domain D is
vol D :=
Z
D
√g dx1 . . . dxn
(131)
where g = det(gij), and gij = ei · ej.
Indeed, when we calculate the integral in (131), the terms in the integral
sum are the volumes of small parallelipipeds spanned by the vectors e1, . . . , en
multiplied by ∆x1 . . . ∆xn (small increments). We calculate each of the small
volumes using formula (130) and add. Passing to the limit gives (131).
Hence the n-form
dV := √g dx1 . . . dxn
(132)
is the volume form. Its integral over a bounded domain gives the volume of
the domain. In particular, in any Cartesian coordinates we have g = 1, and
we return to the original deﬁnition of volume.
Similarly we can introduce a “k-dimensional area” (or “k-dimensional
volume”) element dS for any k-dimensional surface in Rn. Standard nota-
tion: dS (regardless of k). It is a k-form that “lives” on a surface, i.e., is
a k-form written in terms of parameters on a surface.
For any choice of
parametrization, say, by variables u1, . . . , uk, the area element dS has the
universal appearance
dS :=
√
h du1 . . . duk
(133)
if we denote by h the determinant made of pairwise scalar products of the
vectors ei := ∂x
∂ui.
Example 6.8. For the sphere of radius R with center at O in R3
dS = R2 sin θdθ dϕ.
45

THEODORE VORONOV
Remark 6.1. Volume element dV and area element dS are not diﬀerentials
of any “V ” or “S”, in spite of their notation. This notation is traditional
and has a meaning of a “small element” giving vol or area after integration.
Now we can deﬁne the ﬂux form X · dS for an arbitrary vector ﬁeld X
in Rn.
Deﬁnition 6.3. The ﬂux form corresponding to a vector ﬁeld X, notation:
X · dS, is an (n −1)-form in Cartesian coordinates equal to
X·dS = X1 dx2 dx3 . . . dxn−X2 dx1 dx3 . . . dxn+. . .+(−1)n−1Xn dx1 dx2 . . . dxn−1,
(134)
Proposition 6.1. The pull-back of the ﬂux form X ·dS for a vector ﬁeld X
in Rn by any (n −1)-path Γ: (u1, . . . , un−1) 7→x(u1, . . . , un−1) equals
vol Π(X, e1, . . . , en−1) du1 . . . dun−1.
where ei =
∂x
∂ui, and vol Π(a, . . . , c) denotes the volume of a parallelogram
spanned by vectors a, . . . , c.
Proof for n = 3. Let Γ: x = x(u, v) be the 2-path in question. In Cartesian
coordinates we have (since Γ∗(dy dz) = (∂uy ∂vz −∂vy ∂uz) du dv, etc.):
Γ∗(X · dS) = Γ∗(X1 dy dz −X2 dx dz + X3 dx dy) =
X1
¯¯¯¯
∂uy
∂uz
∂vy
∂vz
¯¯¯¯ du dv −X2
¯¯¯¯
∂ux
∂uz
∂vx
∂vz
¯¯¯¯ du dv + X3
¯¯¯¯
∂ux
∂uy
∂vx
∂vy
¯¯¯¯ du dv =
¯¯¯¯¯¯
X1
X2
X3
∂ux
∂uy
∂uz
∂vx
∂vy
∂vz
¯¯¯¯¯¯
du dv = vol Π(X, eu, ev) du dv.
(Similar proof works in any Rn.)
.................................
div
rotor
calculation in arb coord
ex in polar coordinates
6.2
The Ostrogradski–Gauss and classical Stokes the-
orems
The specialization of the general Stokes theorem for the ﬂux form X ·dS and
the circulation form X ·dr gives two classical integral theorems traditionally
associated with the names of Ostrogradski, Gauss, and Stokes.
Let us ﬁx some orientation in Rn. This makes it possible to consider
integrals of n-forms over any bounded domains D ⊂Rn without ambiguity
in sign.
46

VECTOR CALCULUS. Fall 2002
Deﬁnition 6.4. Let S be an oriented surface of dimension n −1 in Rn or
an (n −1)-chain. The ﬂux of a vector ﬁeld X through S is deﬁned as the
integral of the ﬂux form X · dS over S:
Z
S
X · dS.
The general Stokes theorem and the deﬁnitions of X · dS and div X
immediately imply
Theorem 6.1 (Ostrogradski–Gauss theorem). The ﬂux of a vector ﬁeld
X through the boundary of any bounded domain D ⊂Rn equals the volume
integral of the divergence of X:
I
∂D
X · dS =
Z
D
div X dV.
(135)
Written in Cartesian coordinates, equation (135) up to a change in no-
tation coincides with the version (104) of the general Stokes theorem (which
makes sense without any Euclidean structure). Peculiar for a Euclidean space
is the possibility to formulate it in terms of a vector ﬁeld.
There is an extra statement giving an interpretation of the l.h.s. of (135)
that helps to better understand its geometrical meaning. We need the notion
of a unit normal for this.
Notice that for any (n −1)-dimensional surface S in Rn at every point
of S we can consider a unit normal (a normal vector of unit length) n. It is
deﬁned up to a sign: ±n. However, if the surface is oriented, the direction of
n is deﬁned uniquely. Recall that we have ﬁxed an orientation in the ambient
space. The condition is that the basis n, g1, . . . , gn−1 gives the orientation of
Rn if the basis g1, . . . , gn−1 gives the orientation of the surface S. Conversely,
if n is given, this ﬁxes an orientation of S, from the chosen orientation of
Rn.
Example 6.9. If S = ∂D with the induced orientation (see Deﬁnition 5.1),
then n must point outwards. Hence n is the outward normal, and Deﬁni-
tion 5.1 is often referred to as the “outward normal rule”.
Example 6.10. In R3, if a piece of a surface is given in the parametric
form as x = x(u, v), then the parameters u, v deﬁne an orientation of the
surface via the basis eu = ∂x
∂u, ev = ∂x
∂v of the tangent plane. The unit normal
corresponding to this orientation is given by
n = eu × ev
|eu × ev|.
(136)
Indeed, the rule deﬁning the cross product is that a, b, a × b should give a
positive basis, and this is equivalent to a × b, a, b giving a positive basis.
47

THEODORE VORONOV
(Actually, this is valid for arbitrary dimension n if the notion of a “cross
product” is suitably generalized, so that it takes as arguments n −1 vectors
instead of 2 vectors in R3.)
Proposition 6.2. The restriction of the ﬂux form X·dS to any oriented sur-
face of dimension n−1 in Rn equals (X ·n) dS where n is the corresponding
unit normal and dS is the area element.
Proof (for n = 3). Immediately follows from Proposition 6.1 and the formula
for the normal vector.
It follows that for the same magnitude of X, the “elementary ” ﬂux of
X through a surface S near some point is maximal for X normal to S, and
equals zero for X tangent to S, which is exactly our intuitive picture of a
“ﬂow across the surface S”.
Example 6.11. Find the ﬂux of a “constant ﬂow” along the x-axis,
X = a e1
across a unit square in the plane Pα passing through the y-axis with an
orientation speciﬁed by a unit normal n = (cos α, 0, sin α) (so the plane is
at angle α with the z-axis). By Proposition 6.2, X · dS = (a cos α) dS; thus
the ﬂux is a cos α. It takes the maximal value a when α = 0, and when we
rotate the plane the ﬂux decreases to 0 for α = π/2, becomes negative, and
takes the value −a for α = π, when the orientation is “opposite to the ﬂow”.
Example 6.12. Consider the ﬂux of the vector ﬁeld
E = −r
r3
(137)
in R3 (the “Coulomb force”) through the sphere of radius R oriented by
the outward normal.
The Ostrogradski–Gauss theorem is not applicable
because E is not deﬁned at the origin O.
(There is a trick overcoming
this, see Example 5.16.)
Using Proposition 6.2 we can evaluate the ﬂux
easily. Indeed, as r points in the direction of the outward normal, we have
(E · n) dS = −RR−3 dS = −R−2 dS (as r = R on the sphere). Hence
I
SR
E · dS =
I
SR
(E · n) dS = −R−2
I
SR
dS = −R−2 area SR = −4π. (138)
We see that remarkably the ﬂux does not depend on radius. The explanation
is that the form −r−3r · dS is closed, or, equivalently, that div (−r−3r) = 0,
for r ̸= 0, in R3.
48

VECTOR CALCULUS. Fall 2002
From the Ostrogradski–Gauss theorem follows the “integral deﬁnition” of
the divergence:
div X(x0) = lim
D→x0
H
∂D X · dS
vol D
.
(139)
Here x0 ∈D and D →x0 means that the domain D “shrinks” to a point
x0. Thus the divergence at x0 measures the intensity of a “source” of the
ﬂow at the point x0. If it is negative, the “source” is actually a “sink”. All
these concepts come from the hydrodynamical interpretation.
Another statement following from the general Stokes theorem and which
gave to it the name is the “classical Stokes theorem”.
In Cartesian coordinates curl X =
¯¯¯¯¯¯
e1
e2
e3
∂1
∂2
∂2
X1
X2
X3
¯¯¯¯¯¯
(this works only in R3).
Theorem 6.2 (Classical Stokes theorem). The circulation of a vector
ﬁeld over the boundary of any oriented surface or 2-chain C in R3 equals the
ﬂux of the curl of X through C:
I
∂C
X · dr =
Z
C
curl X · dS.
49


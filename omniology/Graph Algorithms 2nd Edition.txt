

Graph Algorithms, 2nd Edition
Shimon Even’s Graph Algorithms, published in 1979, was a seminal introductory book
on algorithms read by everyone engaged in the ﬁeld. This thoroughly revised second
edition, withaforewordbyRichardM. KarpandnotesbyAndrewV. Goldberg, continues
the exceptional presentation from the ﬁrst edition and explains algorithms in formal but
simple language with a direct and intuitive presentation.
The material covered by the book begins with basic material, including graphs and
shortest paths, trees, depth-ﬁrst search, and breadth-ﬁrst search. The main part of the
book is devoted to network ﬂows and applications of network ﬂows. The book ends with
two chapters on planar graphs and on testing graph planarity.
S H I M O N E V E N (1935–2004) was a pioneering researcher on graph algorithms and
cryptography. He was a highly inﬂuential educator who played a major role in establish-
ing computer science education in Israel at the Weizmann Institute and the Technion.
He served as a source of professional inspiration and as a role model for generations
of students and researchers. He is the author of Algorithmic Combinatorics (1973) and
Graph Algorithms (1979).


Graph Algorithms
2nd Edition
SHIMON EVEN
Edited by
GUY EVEN
Tel-Aviv University

C A M B R I D G E U N I V E R S I T Y P R E S S
Cambridge, New York, Melbourne, Madrid, Cape Town,
Singapore, São Paulo, Delhi, Tokyo, Mexico City
Cambridge University Press
32 Avenue of the Americas, New York, NY 10013-2473, USA
www.cambridge.org
Information on this title: www.cambridge.org/9780521736534
© Shimon Even 1979
© Shimon Even and Guy Even 2012
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First edition published 1979 by Computer Science Press
Second edition published 2012
Printed in the United States of America
A catalog record for this publication is available from the British Library.
Library of Congress Cataloging in Publication Data
ISBN 978-0-521-51718-8 Hardback
ISBN 978-0-521-73653-4 Paperback
Cambridge University Press has no responsibility for the persistence or accuracy of URLs
for external or third-party Internet Web sites referred to in this publication and does not
guarantee that any content on such Web sites is, or will remain, accurate or appropriate.

Contents
Foreword by Richard M. Karp
pagevii
Preface to the Second Edition
ix
Preface to the First Edition
xi
1
Paths in Graphs
1
1.1
Introduction to Graph Theory
1
1.2
Computer Representation of Graphs
3
1.3
Euler Graphs
6
1.4
De Bruijn Sequences
9
1.5
Shortest-Path Algorithms
11
1.6
Problems
22
2
Trees
29
2.1
Tree Deﬁnitions
29
2.2
Minimum Spanning Tree
31
2.3
Cayley’s Theorem
34
2.4
Directed Tree Deﬁnitions
37
2.5
The Inﬁnity Lemma
39
2.6
Problems
42
3
Depth-First Search
46
3.1
DFS of Undirected Graphs
46
3.2
Algorithm for Nonseparable Components
52
3.3
DFS on Directed Graphs
57
3.4
Strongly Connected Components of a Digraph
58
3.5
Problems
62
4
Ordered Trees
65
4.1
Uniquely Decipherable Codes
65
v

vi
Contents
4.2
Positional Trees and Huffman’s Optimization Problem
69
4.3
Application of the Huffman Tree to Sort-by-Merge
Techniques
75
4.4
Catalan Numbers
77
4.5
Problems
82
5
Flow in Networks
85
5.1
Introduction
85
5.2
The Algorithm of Ford and Fulkerson
87
5.3
The Dinitz Algorithm
94
5.4
Networks with Upper and Lower Bounds
102
5.5
Problems
109
5.6
Notes by Andrew Goldberg
115
6
Applications of Network Flow Techniques
117
6.1
Zero-One Network Flow
117
6.2
Vertex Connectivity of Graphs
121
6.3
Connectivity of Digraphs and Edge Connectivity
129
6.4
Maximum Matching in Bipartite Graphs
135
6.5
Two Problems on PERT Digraphs
137
6.6
Problems
141
7
Planar Graphs
146
7.1
Bridges and Kuratowski’s Theorem
146
7.2
Equivalence
157
7.3
Euler’s Theorem
158
7.4
Duality
159
7.5
Problems
164
8
Testing Graph Planarity
168
8.1
Introduction
168
8.2
The Path Addition Algorithm of Hopcroft and Tarjan
169
8.3
Computing an st-Numbering
177
8.4
The Vertex Addition Algorithm of Lempel, Even, and
Cederbaum
179
8.5
Problems
185
Index
187

Foreword
In Appreciation of Shimon Even
Shimon was a great computer scientist who inspired generations of Israeli
stutents and young researchers, including many future leaders of theoretical
computer science.
He was a master at creating combinatorial algorithms, constructions, and
proofs. He always sought the simplest and most lucid solutions. Because he
neverallowedhimselftouseaknowntheoremunlessheunderstooditsproof,his
discoveries were often based on original methods. His lectures were legendary
for their clarity.
Shimon was devoted to his family, generous to his colleagues, and freely
available to the students in his classes.
He expressed his views forcefully and with complete honesty. He expected
honesty in return, and reserved his disapproval for those who tried to obfuscate
or mislead.
Shimon had an unending supply of interesting anecdotes, and would laugh
uproariously at good jokes, including his own.
In sum, he was a great and unforgettable man and a great scientist, and his
name has a permanent place in the annals of theoretical computer science.
Richard M. Karp
Berkeley, April 2011
vii


Preface to the Second Edition
My father, Shimon Even, died on May 1, 2004. In the year prior to his illness,
he began revising this book. He used to tell me with great satisfaction whenever
he completed the revision of a chapter. To his surprise, he often discovered
that, after twenty-ﬁve years, he preferred to present the material differently (the
ﬁrst edition was published in 1979). Unfortunately, he only managed to revise
Chapters 1, 2, 3, and 5. These revised chapters appear in this edition. However,
since the material in Chapters 9 and 10 on NP-completeness is well covered in
a few other books, we decided to omit these chapters from the second edition.
Therefore, the second edition contains only the ﬁrst eight chapters.
As I was reading the manuscript for the second edition, my father’s deep voice
resonated clearly in my mind. Not only his voice, but also his passion for teach-
ing, for elegant explanations, and, most importantly, for distilling the essence.
As an exceptional teacher, he used his voice and his physique to reinforce his
arguments. His smile revealed how happy he was to have the opportunity to
tell newcomers about this wonderful topic. One cannot overvalue the power of
such enthusiasm. Luckily, this enthusiasm is conveyed in this book.
Many people tell me (with a smile) about being introduced to the topic of
algorithms through this book. I believe the source of their smiles is its outstand-
ing balance between clarity and preciseness. When one writes mathematical
text, it is very easy to get carried away with the desire to be precise. The written
letter is long lasting, and being aware that one’s text leaves certain gaps requires
boldness. For my father this task was trivial. The audience he had in mind con-
sisted simply of himself. He wrote as he would have wanted the material to be
presented to him. This meant that he elaborated where he needed to, and he did
not hesitate to skim over details he felt comfortable with. As a child, I recall
seeing him prepare for class by reading a chapter from his book. I asked him:
ix

x
Preface to the Second Edition
“Why are you reading your own book? Presumably, you know what is there.”
“True,” he replied, “but I don’t remember!”
This second edition would have never been completed without Oded
Goldreich. Oded Goldreich began by convincing me to prepare the second edi-
tion. Then he put me in touch with Lauren Cowles from Cambridge University
Press. Finally, he continuously encouraged me to complete this project. It took
almost seven years! There is no good excuse for it. We all know how such a task
can be pushed aside by more “urgent” and “demanding” tasks. Apparently, it
took me some time to realize how important this task was, and that it could not
be completed without a coordinated effort. Only after I recruited Lotem Kaplan
to do the typesetting of the unrevised chapters and complete the missing ﬁgure
and index terms did this project begin to progress seriously. I am truly grateful
to Oded for his insistence, to Lotem for her assistance, and to Lauren for her
kind patience.
Finally, I wish to thank Richard M. Karp, an old friend of my father’s, for his
foreword. I also wish to thank Andrew Goldberg, the expert in network ﬂow
algorithms, for the notes he contributed in Chapter 5. These notes outline the
major developments in the algorithms for maximum ﬂow that have taken place
since the ﬁrst edition of this book was published.
Guy Even
Tel-Aviv, March 2011

Preface to the First Edition
Graph theory has long been recognized as one of the more useful mathematical
subjects for the computer science student to master. The approachthat is natural
to computer science is the algorithmic one; our interest is not so much in the
existenceproofsorenumerationtechniquesasitisin ﬁnding efﬁcientalgorithms
for solving relevant problems or, alternatively, in showing evidence that no such
algorithm exists. Although algorithmic graph theory was started by Euler, if not
earlier, its development in the last ten years has been dramatic and revolutionary.
Much of the material in Chapters 3, 5, 6, 8, 9, and 10 is less than ten years old.
This book is meant to be a textbook for an upper-level undergraduate, or a
graduate course. It is the result of my experience in teaching such a course
numerous times, since 1967, at Harvard, the Weizmann Institute of Science,
Tel-Aviv University, the University of California at Berkeley, and the Tech-
nion. There is more than enough material for a one-semester course; I am sure
that most teachers will have to omit parts of the book. If the course is for
undergraduates, Chapters 1 to 5 provide enough material, and even then, the
teacher may choose to omit a few sections, such as 2.6, 2.7, 3.3, and 3.4.1
Chapter 7 consists of classical nonalgorithmic studies of planar graphs, which
are necessary in order to understand the tests of planarity, described in Chapter
8; it may be assigned as a preparatory reading assignment. The mathematical
background needed for understanding Chapters 1 to 8 includes some knowl-
edge of set theory, combinatorics, and algebra, which the computer science
student usually masters during his freshman year through courses on discrete
mathematics and on linear algebra. However, the student also needs to know
something about data structures and programming techniques, or he may not
The ﬁrst edition was published in 1979 (G.E.).
1 Sections 2.6 and 2.7 were removed from the second edition by Shimon Even.
xi

xii
Preface to the First Edition
appreciate the algorithmic side or may miss the complexity considerations. It
is my experience that after two courses in programming, students have the nec-
essary knowledge. However, in order to follow Chapters 9 and 10,2 additional
background is necessary, namely, in theory of computation. Speciﬁcally, the
student should know about Turing machines and Church’s thesis.
The book is self-contained. No previous knowledge is needed beyond the
general background just described. No comments such as “the rest of the proof
is left to the reader” or “this is beyond the scope of this book” are ever made.
Some unproved results are mentioned, with a reference, but are not used later
in the book.
At the end of each chapter, there are a few problems teachers can use for
homework assignments. The teacher is advised to use them discriminately,
since some of them may be too hard for his students.
I would like to thank some of my past colleagues for our joint work and for the
inﬂuence they have had on my work, and therefore on this book: I. Cederbaum,
M. R. Garey, J. E. Hopcroft, R. M. Karp, A. Lempel, A. Pnuely, A. Shamir,
and R. E. Tarjan. Also, I would like to thank some of my former Ph.D. students
for all that I have learned from them: O. Kariv, A. Itai, Y. Perl, M. Rodeh,
and Y. Shiloach. Finally, I would like to thank E. Horowitz for his continuing
encouragement.
S.E., Techinion, Haifa, Israel
2 Chapters 9 and 10 are not included in the second edition.

1
Paths in Graphs
1.1 Introduction to Graph Theory
A graph G(V,E) is a structure consisting of a set of vertices V = {v1,v2,...}
and a set of edges E = {e1,e2,...}; each edge e has two endpoints, which are
vertices, and they are not necessarily distinct.
Unless otherwise stated, both V and E are assumed to be ﬁnite. In this case
we say that G is ﬁnite.
For example, consider the graph in Figure 1.1. Here, V = {v1,v2,v3,v4,v5},
E = {e1,e2,e3,e4,e5}. The endpoints of e2 are v1 and v2. Alternatively, we say
that e2 is incident on v1 and v2. The edges e4 and e5 have the same endpoints
and are therefore called parallel. Both endpoints of e1 are the same – v1; such
an edge is called a self-loop.
The degree of a vertex v, d(v), is the number of times v is used as an endpoint.
Clearly, a self-loop uses its endpoint twice. Thus, in our example, d(v1) = 4,
d(v2) = 3.Also, a vertexwhose degreeis zerois called isolated.In our example,
v3 is isolated since d(v3) = 0.
Lemma 1.1 In a ﬁnite graph the number of vertices of odd degree is even.
Proof: Let |V| and |E| be the number of vertices and edges, respectively. It is
easy to see that
|V|

i=1
d(vi) = 2 · |E|,
since each edge contributes two to the left-hand side: one to the degree of each
of its two endpoints if they are distinct; and two to the degree of its endpoint if
the edge is a self-loop. For the left-hand side to sum up to an even number, the
number of odd terms must be even.
■
1

2
1 Paths in Graphs
e3
v1
e1
e2
v2
v3
v4
e5
e4
v5
Figure 1.1: Example of a graph.
The notation u e v means that the edge e is incident on vertices u and v. In
this case we also say that e connects vertices u and v, or that vertices u and v
are adjacent.
A path, P, is a sequence of vertices and edges, interweaved in the following
way: P starts with a vertex, say v0, followed by an edge e1 incident to v0,
followed by the other endpoint v1 of e1, and so on. We write
P : v0 e1 v1 e2 v2 ···
If P is ﬁnite, it ends with a vertex, say vl. We call v0 the start-vertex of P and
vl the end-vertex of P. The number of edge appearances in P, l, is called the
length of P. If l = 0, then P is said to be empty, but it has a start-vertex and
an end-vertex, which are identical. (We shall not use the term “path” unless a
start-vertex exists.)
In a path, edges and vertices may appear more than once, unless otherwise
stated. If no vertex appears more than once, and therefore no edge can appear
more than once, the path is called simple.
A circuit, C, is a ﬁnite path in which the start and end vertices are identical.
However, an empty path is not considered a circuit. By deﬁnition, the start and
end verticesofacircuitarethesame,and ifthereisno otherrepetitionofavertex,
thecircuitiscalled simple.However,acircuitoflength two,a e b e a,where
the same edge, e, appears twice, is not considered simple. (For a longer circuit,
it is superﬂuous to state that if it is simple, then no edge appears more than
once.) A self-loop is a simple circuit of length one.
If for every two vertices u and v of a graph G, there is a (ﬁnite) path that starts
in u and ends in v, then G is said to be connected.
A digraph or directed graph G(V,E) is deﬁned similarly to a graph, except
that the pair of endpoints of every edge is now ordered. If the ordered pair of

1.2 Computer Representation of Graphs
3
endpoints of a (directed) edge e is (u,v), we write
u e
−→v.
The vertex u is called the start-vertex of e; and the vertex, v, the end-vertex of
e. The edge e is said to be directed from u to v. Edges with the same start-vertex
and the same end-vertex are called parallel. If u ̸= v, u e1
−→v and v e2
−→u, then
e1 and e2 are antiparallel. An edge u −→u is called a self-loop.
The out-degree dout(v) of vertex v is the number of (directed) edges having
v as their start-vertex; in-degree din(v) is similarly deﬁned. Clearly, for every
ﬁnite digraph G(V,E),

v∈V
din(v) =

v∈V
dout(v).
A directed path is similar to a path in an undirected graph; if the sequence of
edges is e1,e2,··· then for every i ⩾1, the end-vertex of ei is the start-vertex of
ei+1. The directed path is simple if no vertex appears on it more than once. A
ﬁnite directed path C is a directed circuit if the start-vertex and end-vertex of C
are the same. If C consists of one edge, it is a self-loop. As stated, the start and
end vertices of C are identical, but if there is no other repetition of a vertex, C
is simple. A digraph is said to be strongly connected if, for every ordered pair
of vertices (u,v) there is a directed path which starts at u and ends in v.
1.2 Computer Representation of Graphs
To understand the time and space complexities of graph algorithms one needs
to know how graphs are represented in the computers memory. In this section
two of the most common methods of graph representation are brieﬂy described.
Let us consider graphs and digraphs that have no parallel edges. For such
graphs, the speciﬁcation of the two endpoints is sufﬁcient to specify the edge;
for digraphs, the speciﬁcation of the start-vertex and the end-vertexis sufﬁcient.
Thus, we can represent such a graph or digraph of n vertices by an n×n matrix
M, where Mij = 1 if there is an edge connecting vertex vi to vj, and Mij = 0, if
not. Clearly, in the case of (undirected) graphs, Mij = 1 implies that Mji = 1;
or in other words, M is symmetric. But in the case of digraphs,any n×n matrix
of zeros and ones is possible. This matrix is called the adjacency matrix.
Given the adjacency matrix M of a graph, one can computed(vi) by counting
the number of ones in the i-th row, except that a one on the main diagonal
representsaself-loop and contributestwo to thecount.Foradigraph,thenumber

4
1 Paths in Graphs
of ones in the i-th row is equal to dout(vi), and the number of ones in the j-th
column is equal to din(vj).
The adjacency matrix is not an efﬁcient representation of the graphif the graph
is sparse; namely, the number of edges is signiﬁcantly smaller than n2. In these
cases, it is more efﬁcient to use the incidence lists representation, described
later. We use this representation, which also allows parallel edges, in this book
unless stated otherwise.
A vertex array is used. For each vertex v, it lists v’s incident edges and a
pointer indicating the current edge. The incidence list may simply be an array
or may be a linked list. Initially, the pointer points to the ﬁrst edge on the list.
Also, we use an edge array, which tells us for each edge its two endpoints (or
start-vertex and end-vertex, in the case of a digraph).
Assume we want an algorithm TRACE(s,P), such that given a ﬁnite graph
G(V,E) and a start-vertex s ∈V traces a maximal path P that starts at s and does
not use any edge more than once. Note that by “maximal” we do not mean that
the resulting path, P, will be the longest possible; we only mean that P cannot
be extended, that is, there are no unused incident edges at the end-vertex.
We can trace a path starting at s by taking the ﬁrst edge e1 on the incidence list
of s, marking e1 as “used” in the edge array, and looking up its other endpoint
v1 (which is s if e1 is a self-loop). Next, use the vertex array to ﬁnd the pointer
to the current edge on the list of v1. Scan the incidence list of v1 for the ﬁrst
unused edge, take it, and so on. If the scanning hits the last edge and it is used,
TRACE(s,P) halts. A PASCAL-like description of TRACE(s,P) is presented
in Algorithm 1.1. Here is a list of the data structures it uses:
(i) A vertex table such that, for every v ∈V, it includes the following:
– A list of the edges incident on v, which ends with NIL
– A pointer N(v) to the current item in this list. Initially, N(v) points to
the ﬁrst edge on the list (or to NIL, if the list is empty).
(ii) An edge table such that every e ∈E consists of the following:
– The two endpoints of e
– A ﬂag that indicates whether e is used or unused. Initially, all edges are
unused.
(iii) An array (or linked list) P of edges that is initially empty, and when
TRACE(s,P) halts, will contain the resulting path.
Notice that in each application of the “while” loop of TRACE (lines 2–10 in
Algorithm 1.1), either N(v) is moved to the next item on the incidence list of
v (line 4), or lines 6–10 are applied, but not both. The number of times line

1.2 Computer Representation of Graphs
5
Procedure TRACE(s,P)
1
v ←s
2
while N(v) points to an edge (and not to NIL) do
3
if N(v) points to a used edge do
4
change N(v) to point to the next item on the list
5
else do
6
e ←N(v)
7
change the ﬂag of e to used
8
add e to the end of P
9
use the edge table to ﬁnd the second endpoint of e, say u
10
v ←u
Algorithm 1.1: The TRACE algorithm.
4 is applied is clearly O(|E|). The number of times lines 6–10 are applied is
also O(|E|), since the ﬂag of an unused edge changes to used, and each of these
lines takes time bounded by a constant to run. Thus, the time complexity of
TRACE is O(|E| ).1 (In fact, if the length of the resulting P is L then the time
complexity is O(l); this follows from the fact that each edge that joins P can
“cause a waste” of computing time only twice: once when it joins P and, at
most, once again by its appearance on the incidence list of the adjacent vertex.)
If one uses the adjacency matrix representation, in the worst case, the tracing
algorithm takes time (and space) complexity Ω(|V |2).2 And if |E| << |V |2, as is
the case for sparse graphs, the complexity is reduced by using the incidence-list
representation. Since in most applications, the graphs are sparse, we prefer to
use the incidence-list representation.
Note that in our discussions of complexity, we assume that the word length of
our computeris sufﬁcient to store the names of our atomic components:vertices
and edges. If one does not make this assumption, then one may have to allow
Ω(log(|E| + |V|)) bits to represent the atomic components, and to multiply the
complexities by this factor.
1 f(x) is O(g(x)) if there are two constants k1 and k2, such that for every x, f(x)⩽k1·
g(x)+k2.
2 f(x) is Ω(g(x)) if there are two constants k3 and k4, such that for every x, f(x)⩽k3 ·
g(x)+k4.

6
1 Paths in Graphs
1.3 Euler Graphs
An Euler path of a ﬁnite undirected graph G(V,E) is a path such that every
edge of G appears on it once. Therefore, the length of an Euler path is |E|. If G
has an Euler path, then it is called an Euler graph.
Theorem 1.1 A ﬁnite (undirected) connected graph is an Euler graph if and
only if exactly two vertices are of odd degree or all vertices are of even degree.
In the latter case, every Euler path of the graph is a circuit, and in the former
case, none is.
As an immediate conclusion of Theorem 1.1 we observe that none of the
graphs in Figure 1.2 is an Euler graph because both have four vertices of odd
degree. The graph shown in Figure 1.2(a) is the famous Königsberg bridge
problem solved by Euler in 1736. The graph shown in Figure 1.2(b) is a common
misleading puzzle of the type “draw without lifting your pen from the paper.”
Proof: It is clear that if a graph has an Euler path that is not a circuit, then
the start-vertex and the end-vertex of the path are of odd degree, while all the
other vertices are of even degree. Also, if a graph has an Euler circuit, then all
vertices are of even degree.
Assume now that G is a ﬁnite graph with exactly two vertices of odd degree,
a and b. We now describe an algorithm (A), which will ﬁnd an Euler path from
a to b.
First, trace a maximal path P, as in the previous section, starting at vertex a.
Since G is ﬁnite, the algorithm halts, producing a path. But as soon as the path
emanates from a, one of the edges incident to a is used, and a’s residual degree
becomes even. Thus, every time a is reentered, there is an unused edge to leave
(a)
(b)
Figure 1.2: Non-Eulerian graphs.

1.3 Euler Graphs
7
a by. This proves that P cannot end in a. Similarly, if vertex v ∈V \{a,b}, then
P cannot end in v. It follows that P ends in b.
If P contains all the edges of G, we are done. If not, we make the following
observations:
• The residual degree of every vertex is even.
• There is an unused edge incident on some vertex v that is on P. To see that
this must be so, let u e w be an unused edge. If either u or w is on P, we
are done. If not, since G is connected, there is a path Q from a to u. There
must be unused edges on Q. Going from a on Q, the ﬁrst unused edge we
encounter ﬁts the bill.
Now, trace a maximal path P ′ in the residual graph, which consists of the
set V of vertices and all edges of E that are not in P. Start P ′ at v. Since all
vertices of the residual graph are of even degree, P ′ ends in v (and is therefore
a circuit). Next, combine P and P ′ to form one path from a to b as follows:
Follow P until it enters v. Now, incorporate P ′, and then follow the remainder
of P.
Repeat, incorporating additional circuits into the present path as long as there
are unused edges. Since the graph is ﬁnite, this process will terminate, yielding
an Euler path.
If all vertices of the graph are of even degree, the ﬁrst traced path can start at
any vertex, and will be a circuit. The remainder of the algorithm is similar to
to this process.
■
In the case of digraphs, a directed Euler path is a directed path in which every
edge appears once. A directed Euler circuit is a directed Euler path for which
the start and end vertices are identical. In addition, digraph is called Euler if it
has a directed Euler path (or circuit).
The underlying (undirected) graph of a digraph is the graph resulting from
the digraph if the direction of the edges is ignored. Thus, the underlying graph
of the digraph in Figure 1.3(a) is shown in Figure 1.3(b).
(b)
(a)
Figure 1.3: A digraph and its underlying graph.

8
1 Paths in Graphs
Theorem 1.2 A ﬁnite digraph is an Euler digraph if and only if its underlying
graph is connected and one of the following two conditions holds:
(i) There is one vertex a such that dout(a) = din(a)+1,and anothervertex b
such that dout(b)+1 = din(b), while for every other vertex v, dout(v) =
din(v).
(ii) For every vertex v, dout(v) = din(v).
In the former case, every directed Euler path starts at a and ends in b. In the
latter, every directed Euler path is a directed Euler circuit.
The proof of Theorem 1.2 is along the same lines as the proof of Theorem 1.1,
and is therefore not repeated here.
Let us make now a few comments about the complexity of the algorithm A
for ﬁnding an Euler path, as described in the proof of Theorem 1.1. Our purpose
is to show that the time complexity of the algorithm is O(|E|).
Assume G(V,E) is presented in the incidence list’s data structure. The main
path P and the detour P ′ will be represented by linked lists, where each item
on the list represents an edge.
In the vertex table, we add for each vertex v the following two items:
(i) A ﬂag that indicates whether v is already on the main path P or the detour
P ′. Initially, this ﬂag is “unvisited.”
(ii) For every visited vertex v, there is a pointer E(v) to the location on the path
of the edge through which v was ﬁrst encountered. Initially, for every v,
E(v) =NIL.
We shall also use a list L of visited vertices. Each vertex enters L once, when
its ﬂag is changed from “unvisited” to “visited.”
A starts by running TRACE(a,P), updating the vertices’ ﬂags, and E(v) for
each newly visited vertex v. Next, the following loop is applied:
If L is empty, A halts. If not, take a vertex v from L, and remove v from L.
Use TRACE(v,P ′) to produce P ′. Look up edge E(v), recording the location
of the edge e it is linked to. Change this link to point to the ﬁrst edge on P ′.
Now, let the last edge of P ′ point to e.
Note that when TRACE(v,P ′) terminates, v has no unused incident edges.
This explains why we can remove v from L.
Now that P ′ has been incorporated into P, the loop is repeated.
It is not hard to see that both the time and space complexities of A are O(|E|).

1.4 De Bruijn Sequences
9
1.4 De Bruijn Sequences
Let Σ = {0,1,...,σ −1} be an alphabet of σ letters. Clearly, there are L = σn
different words of length n over Σ. A de Bruijn sequence is a (circular) sequence
a0a1 ···aL−1 over Σ such that for every word w of length n over Σ there exists
a (unique) 0 ⩽j < L such that
ajaj+1 ···aj+n−1 = w,
where the computation of the indexes is modulo L.
The most important case is that of σ = 2. Binary de Bruijn sequences are of
great importance in coding theory and can be generated by shift registers. (See
Golomb, 1967, on the subject.) In this section we discuss the existence of de
Bruijn sequences for every σ and every n.
For that purpose let us deﬁne the de Bruijn digraph Gσ,n(V,E) as follows:
(i) V = Σn−1; i.e., the set of all σn−1 words of length n −1 over Σ.
(ii) E = Σn.
(iii) The directed edge b1b2 ···bn starts at vertex b1b2 ···bn−1 and ends in
vertex b2b3 ···bn.
Digraphs G2,3 and G2,4 are shown in Figure 1.4. G3,2 is shown in Figure 1.5.
Observe that if w1,w2 ∈Σn, then w2 can follow w1 in a de Bruijn sequence
only if in Gσ,n the edge w2 starts at the vertex in which w1 ends. It follows
that there is a de Bruijn sequence for σ and n if and only if there is a directed
Euler circuit in Gσ,n.
For example, consider the directed Euler circuit of G2,3, which consists of the
following sequence of directed edges:
000, 001, 011, 111, 110, 101, 010, 100.
The corresponding de Bruijn sequence, 00011101, follows by reading the ﬁrst
letter of each word (edge) in the circuit.
Theorem 1.3 For every σ and n, Gσ,n has a directed Euler circuit.
Proof: To use Theorem 1.2 we have to show that the underlying undirected
graph of Gσ,n is connected and that for every vertex v, dout(v) = din(v).
Let us show that Gσ,n is strongly connected. This implies that its underlying
undirected graph is connected.

10
1 Paths in Graphs
00
01
11
10
000
001
011
111
110
100
010
101
000
001
011
111
110
100
010
101
0000
0001
0011
1100
0101
1010
1011
1101
1001
0010
0100
1000
1111
0111
1110
0110
G2,3
G2,4
Figure 1.4: Examples of de Bruijn digraphs for σ = 2.
0
1
2
00
11
21
01
20
22
02
10
12
Figure 1.5: G3,2
Let b1b2 ···bn−1 and c1c2 ···cn−1 be any two vertices. The directed path
b1b2 ···bn−1c1, b2b3 ···bn−1c1c2, ...,bn−1c1 ···cn−1
is of length n −1, it starts at vertex b1b2 ···bn−1 and ends in vertex
c1c2 ···cn−1, showing that Gσ,n is strongly connected. (Observe that this
directed path is not necessarily simple; it may use vertices and edges more
than once.)
Now, observethat for each vertex v = b1b2 ···bn−1, every outgoing edgeis of
the form b1b2 ···bn−1c, where c can be any of the σ letters. Thus, dout(v) = σ.

1.5 Shortest-Path Algorithms
11
Asimilarstatementholdsfor din(v).Thus,thecondition thatdout(v) = din(v)
holds, uniformly.
■
Corollary 1.1 For every σ and n there exists a de Bruijn sequence.
1.5 Shortest-Path Algorithms
In general, the shortest-path problem is concerned with ﬁnding shortest paths
between vertices. Many interesting problems arise, and their variety depends on
the type of graph in our application and the exact question we want to answer.
Some of the characteristics that may help in deﬁning the exact problem are as
follows:
(i) The graph is ﬁnite or inﬁnite.
(ii) The graph is undirected or directed.
(iii) The edges are all of length one, or all lengths are nonnegative, or negative
lengths are allowed.
(iv) We may be interested in shortest paths from a given vertex to another, or
from a given vertex to all the other vertices, or from each vertex to all the
other vertices.
(v) We may be interested in ﬁnding just one path, or in all paths, or in counting
the number of shortest paths.
Clearly, this section will deal only with a few of all possible problems. We
attempt to describe the most important techniques.
It is assumed that we are given a graph (or digraph) G(V,E) and a length
function l : E 
→R. Namely, if e is an edge, then l(e) is the length of e. The
length of a path is the sum of the lengths of edges on it. The distance from
vertex u to vertex v, d(u,v), is the length of a shortest path from u to v if there
are paths from u to v, and is inﬁnite if no such path exists.
In most algorithms described in this section, the scenario is as follows: We
assume that there is a designated vertex s ∈V, called the source. We denote by
δ(v) the value d(s,v). The algorithm will assign a label λ(v) to some (or all)
vertices v. Thus prove that when the algorithm halts, λ(v) = δ(v).
1.5.1 Breadth-First Search
Let us start with the case that G is ﬁnite and undirected, l(e) = 1 for every
edge e, and s ∈V is a designated source. Our goal is to compute δ(v) for every
vertex v.

12
1 Paths in Graphs
Procedure BFS(G,s;λ)
1
empty Q and put s in Q
2
λ(s) ←0
3
T ←{s}
4
while Q ̸= ∅do
5
remove the ﬁrst vertex, u, from Q
6
for every edge u
v, if v /∈T, do
7
T ←T ∪{v}
8
λ(v) ←λ(u) + 1
9
put v in Q
Algorithm 1.2: The BFS algorithm.
The natural and simple algorithm that follows was ﬁrst suggested by
Moore 1957, and was later called Breadth-First Search, or BFS. Intuitively, all
it does is the following:
We start by assigning λ(v) ←∞for every v ∈V. We then proceed in waves.
In wave i ⩾0 a set W(i) of vertices is assigned a ﬁnite label λ(v) ←i, and
no vertex with a ﬁnite label will ever be relabeled. In wave 0, λ(s) ←0, and
therefore W(0) = {s}. As long as W(i) ̸= ∅, i is incremented, for every edge
u
v, such that λ(u) = i−1 and λ(v) = ∞, assign λ(v) ←i and put v ∈W(i).
In the Pascal-like algorithm, described in Algorithm 1.2, we use a slightly
different presentation; our reasons for doing so are discussed later. In this pre-
sentation we do not use the sets of waves, nor is there a running index i. Instead,
weuseaqueueQofverticesand asetT ofvertices.Theﬁrstvertextoberemoved
from Q is s, and for every edge incident on s, the adjacent vertex, if “new,”3
is labeled 1, joins T, and is put on Q. (These are the vertices of W(1).) Next,
vertices (that would have been in W(1)) are removed from Q, and their new
neighbors4 are labeled 2, join T, are put in Q, and so on.
Let us say that a vertex v is accessible from u if there is a path from u to v.
Theorem 1.4 Algorithm BFS assigns every vertex v, which is accessible from
s, a label λ(v) and λ(v) = δ(v).
3 Not in T.
4 Adjacent vertices.

1.5 Shortest-Path Algorithms
13
Proof: First, let us prove that if v is accessible from s, then it gets a (ﬁnite)
label λ(v), and λ(v) ⩽δ(v). This is proved by induction on the value of δ(v).
The basis is established by Line 2, where s is labeled 0.
Now, assume the claim holds for vertices whose distance from s is less than
i, and let us prove it for vertex v for which δ(v) = i.
There is a path P from s to v of length i. The vertex u that precedes v on
P, satisﬁes δ(u) = i −1.5 Thus, by the inductive hypothesis, u is labeled, and
λ(u) ⩽i −1. When u is removed from Q, the edge between u and v on P is
examined. If at that time v is not yet labeled, it gets a label λ(v) ⩽i, proving
the claim. If v is already labeled, by the fact that waves exit Q according to
nondecreasing order, λ(v) cannot be higher than i.
It is easy to see that if a vertex v gets a label λ(v), then there is a path of
length λ(v) from s to v. Such a path can be traced back from v to s by the edges
through which vertices have been labeled. This proves that λ(v) ⩾δ(v). Putting
the two inequalities together, the theorem follows.
■
The foregoing discussion holds for digraphs as well. The only change one
has to make in Algorithm 1.2 is to replace “u
v” by “u −→v.”
In the case of inﬁnite graphs (digraphs), one can still use a modiﬁcation of
BFS to solve the problem, provided the following conditions hold:
(i) Since it is impossible to store the entire input graph, there should be an
algorithm that provides the data of the vertex table of a speciﬁed vertex,
when such is requested. Clearly, the degrees (out-degrees) of the vertices
must be ﬁnite.
(ii) There is a designated-target vertex t, which is accessible from s, and all we
want to do is to ﬁnd δ(t) (and δ(v) for every v, such that δ(v) < δ(t)).
All one needs to change in Algorithm 1.2 is Line 4, as follows: “while Q ̸= ∅
and t /∈T do.” Note that the description of BFS as in Algorithm 1.2 does not
require the prelabeling of all vertices by ∞, which is an impossible task if G
has inﬁnitely many vertices.
Let us discuss the time complexity of BFS for ﬁnite graphs (digraphs). We
can represent T by an array of length |V| in which the i’th position is 0 if vi /∈T
and one if vi ∈T. Looking up whether v ∈T or changing the i’th position takes
constant time, but the performance of Line 3 takes Ω(|V|) time. In addition,
note that each edge is examined at most twice once from each of its endpoints,
5 It is easy to prove that every subpath of a shortest path is itself a shortest path between its two
ends. This is sometimes referred to as the “principle of dynamic programming.”

14
1 Paths in Graphs
and when the edge is examined it may cause a computation that takes constant
time. Thus, the time complexity of BFS is O(|V| + |E|).
Finally, let us comment on how one can use the data generated by BFS to
trace a shortest path from s to a vertex v ∈T. This can be done as described
in the last paragraph of the proof of Theorem 1.4. To ﬁnd the edge by which
a vertex has been labeled, one can add to the vertex table an item that carries
this information; initially, the value of this item is NIL for every vertex, and
when a vertex joins T, the item is updated. This shows that the time complexity
remains essentially unchanged.
1.5.2 Dijkstra’s Algorithm
In this subsection, we shall ﬁrst assume that we are given a ﬁnite digraph
G(V,E); there is a source vertex s, and every (directed) edge e has a nonnegative
length l(e) ⩾0. Our task is to compute δ(v) for every vertex v. Later, we shall
discuss other cases in which similar algorithms apply.
If all edge lengths are positive integers, it may seem that by replacing each
edge e by a directed path that goes through l(e) new edges, all of length 1, and
l(e)−1 new(intermediate)vertices,theproblemismapped tothecasedescribed
in the previous subsection, and BFS can be used to solve it. Although, this is
a valid statement, from a computer-science point of view, it is a bad idea. The
reason is that it takes logl(e) bits to represent l(e), and the foregoing suggested
transformation introduces l(e) −1 new vertices and edges. This blows up the
length of the input data exponentially. The Dijkstra Algorithm (Dijkstra 1959,
vol.1) avoids this blowup and keeps the complexity bounded polynomially in
terms of the length of the input data.
The Dijkstra algorithm is presented in Algorithm 1.3 in Pascal-like style.
Two sets of vertices are used: T is the set of temporarily labeled vertices; that
is, vertices for which λ has been assigned but its value is still subject to change.
P is the set of permanently labeled vertices. Vertices neither in T, nor in P,
have not been labeled yet. A vertex v, for which λ(v) is minimum, among the
vertices in T, is chosen in Line 5. This vertex moves from T to P, and every
edge e outgoing from v is examined. If the end-vertex u of e is in T, then its
label is lowered in Line 10, if e enables such an improvement. If u is new, it
gets a label in Line 12, and joins T in Line 13.
Lemma 1.2 When procedure DIJKSTRA is applied to any ﬁnite digraph G and
start-vertex s, it halts.

1.5 Shortest-Path Algorithms
15
Procedure DIJKSTRA(G,s,l;λ)
1
λ(s) ←0
2
T ←{s}
3
P ←∅
4
while T ̸= ∅do
5
choose a vertex v ∈T for which λ(v) is minimum
6
T ←T \ {v}
7
P ←P ∪{v}
8
for every v e
−→u do
9
if u ∈T then do
10
λ(u) ←min{λ(u), λ(v) + l(e)}
11
else, if u /∈P then do
12
λ(u) ←λ(v) + l(e)
13
T ←T ∪{u}
Algorithm 1.3: The Dijkstra algorithm.
Proof: Every vertex enters T at most once, and every vertex chosen in Line 5
leaves T. The performance of Line 5 takes at most O(|V|) time, and the sum
total time to perform Lines 8–13, for all chosen vertices, is O(|E|), since no
edge is examined more than once. After performing Line 5 |V| times, T must
be empty, and the procedure halts.
■
It follows that the time complexity of Dijkstra’s algorithm is O(|V|2 + |E|).
We shall return to this issue.
Lemma 1.3 During the computation of Dijkstra’s algorithm, every vertex
accessible from s gets a label.
Proof: Let P be a directed path from s to v. If v does not get a label, let u be
the ﬁrst unlabeled vertex along P when the algorithm halts. Consider the edge
w e
−→u on P. Since w has been labeled, eventually, it is chosen to leave T,
and by the edge e, u gets labeled. A contradiction.
■
Lemma 1.4 At any time during the computation of Dijkstra’s algorithm, if a
vertex v is labeled λ(v), then there is a directed path from s to v whose length
is λ(v).

16
1 Paths in Graphs
Proof: By induction on the time during which an assignment or reassignment
of a label takes place. The ﬁrst assignment is in Line 1, and indeed, there is a
path of length 0 from s to itself, that is, the empty path.
Now, look at an assignment, or reassignment, that occurs at time τ, and
assume all previous assignments satisfy the claim. Assume vertex u is assigned
a label at time τ (for the ﬁrst time) in Line 12. Thus, the label of v at time
τ was assigned earlier. By the inductive hypothesis, there is a directed path
from vertex s to v of length λ(v), and this path, appended with e, is a path of
length λ(u) from s to u. A similar argument holds in case of a reassignment,
per Line 10.
■
Note that Lemma 1.4 implies that for every labeled vertex v (and at any time),
λ(v) ⩾δ(v).
(1.1)
Theorem 1.5 When Dijkstra’s algorithm halts, for every vertex v accessible
from s,
λ(v) = δ(v).
Proof: If v is accessible from s, then by Lemma 1.3, v has an assigned label
λ(v). By Equation 1.1, λ(v) ⩾δ(v). It remains to show that λ(v) ⩽δ(v).
The proof is by induction on the order in which vertices join the set P.
Let P be a shortest directed path from s to v. Let τ be the time when vertex
v is chosen to join P. At that time, s is in P and v is not. Thus, there must be
an edge u
e
−→w on P such that u ∈P and w /∈P. Consider the following
sequence of claims:
• Since the length of edges is nonnegative, the subpath of P, from w to v, is
of a nonnegative length. Thus,
δ(v) ⩾δ(w).
• Since every subpath of a shortest path is shortest from its start-vertex to its
end-vertex, the subpath of P from s to w is shortest, and its length consists
of the length of a shortest path from s to u plus l(e), that is,
δ(w) = δ(u) + l(e).
• Since u has joined P before time τ, by the inductive hypothesis, δ(u) ⩾λ(u).
Thus,
δ(u) + l(e) ⩾λ(u) + l(e).

1.5 Shortest-Path Algorithms
17
• Since u has joined P before time τ, all its outgoing edges, including e, have
been examined, as per Lines 8–13. Thus, at time τ, λ(w) has been assigned,
and
λ(u) + l(e) ⩾λ(w).
• However, at time τ, v has been chosen to join P, not w. Thus,
λ(w) ⩾λ(v).
These foregoing ﬁve relations imply that, at time τ,
δ(v) ⩾λ(v).
■
As we have seen, a simple implementation of the Dijkstra algorithm is of time
complexityO(|V|2+|E|). For sparse graphs the bulk of the computationis in the
Ω(|V|) applications of Line 5, where many of them may take Ω(|V|) time each.
If one uses a heap to store the vertices of T (see, e.g., Cormen et al. [4]), then
the complexity is reduced to O(|E| · log|V|). If one uses Fibonacci heaps (see,
e.g., [5] or [4]), then the complexity is reduced further to O(|V| · log|V|+ |E|).
In case of undirected graphs, the Dijkstra algorithm is applicable; the only
change one should make is to replace “v e
−→u” by “v e u” in Line 8. Alter-
natively, transform the given graph to a digraph by replacing each undirected
edge with a pair of antiparallel directed edges of the same length, and apply
Dijkstra’s algorithm as is.
If there is a target vertex t, and one does not want to continue the computation
after t has been put in P, then one should replace Line 4 with “while T ̸= ∅and
t /∈P do.” This change makes the algorithm applicable for inﬁnite graphs,
provided t is accessible from s, the out-degrees of the vertices are ﬁnite, and
the number of vertices v, for which δ(v) ⩽δ(t), is ﬁnite.
Note that Dijkstra’s algorithm may not be applicable if there are edges of
negative length. This is the case even if the graph is directed and there are no
directed circuits whose length is negative.
1.5.3 The Ford Algorithm
In this subsection we assume that the given digraph, G(V,E), is ﬁnite, every
(directed) edge e has a length l(e), which may be negative. We are also given
a source vertex s. Our task is to compute δ(v) for every vertex v.

18
1 Paths in Graphs
Procedure gen-FORD(G,s,l;λ)
1
for every v ∈V do
2
λ(v) ←∞
3
λ(s) ←0
4
while there is an edge u e
−→v such that λ(u) is ﬁnite and λ(v) >
λ(u) + l(e) do
5
λ(v) ←λ(u) + l(e)
Algorithm 1.4: The generic Ford algorithm.
Let us call a directed circuit negative if its length is negative. Notice that if
there is a negative circuit C, and it is accessible from s, then the distance from
s to the vertices on C is not deﬁned; for every real number r, one may take a
path from s to one of C’s vertices and go around C sufﬁciently many times,
to build up a path of length less than r. But, if there are no negative circuits
accessible from s, then either v is not accessible from s, and then δ(v) = ∞, or
only simple paths from s to v need to be considered. The number of such paths
is ﬁnite, and therefore, δ(v) is well deﬁned. Thus, we conclude that δ(·) is well
deﬁned.
The generic Ford algorithm [6, 7],6 described in Algorithm 1.4, computes for
every vertex v, a value λ(v). As we shall see, if there are no negative circuits
accessible from s, the procedure will terminate, and upon termination, for every
vertex v, λ(v) = δ(v).
Lemma 1.5 While running gen-FORD, if λ(v) is ﬁnite then there is a directed
path from s to v whose length is λ(v).
The proof is similar to that of Lemma 1.4.
Lemma 1.5 holds even if there are negative circuits. However, if there are
no such circuits, the path traced in the proof cannot return to a vertex visited
earlier. For if it does, then by going around the directed circuit, a vertex has
improved its own label. This implies that the sum of the edge lengths of the
circuit is negative. Therefore, we have:
6 Sometimes called the Bellman-Ford algorithm.

1.5 Shortest-Path Algorithms
19
Lemma 1.6 If the digraph has no accessible negative circuits and if, while
running, gen-FORD λ(v) is ﬁnite, then there is a simple directed path from s
to v whose length is λ(v).
Under the conditions of Lemma 1.6, since each new assignment of λ(·)
corresponds to a new simple directed path from s, and since the number of
simple directed paths (from s) is ﬁnite, we conclude that under these conditions
procedure gen-FORD must terminate, and is therefore, an algorithm.
Lemma 1.7 For a digraph with no accessible negative circuits, upon termina-
tion of the Ford algorithm, λ(v) = δ(v) for every vertex v.
Proof: If v is not accessible from s, then both λ(v) and δ(v) are equal to ∞
and the claim holds.
If v is accessible from s, by Lemma 1.5, λ(v) ⩾δ(v). It remains to be shown
that λ(v) ⩽δ(v).
Let P be a shortest path from s to v, where
P : s = u0 e1
−→u1 e2
−→u2 ···ul−1 el
−→ul = v.
We prove, by induction on 0 ⩽i ⩽l, that λ(ui) ⩽δ(ui). The claim clearly
holds for u0, by Line 3 and the fact that labels never increase.
Let i be the least value for which λ(ui) > δ(ui). Since every section of a
shortest path is shortest between its two endpoints, the length of the subpath
of P, from u0 to ui is δ(ui) and δ(ui) = δ(ui−1) + l(ei). By the choice of i,
λ(ui−1) ⩽δ(ui−1). It follows that
λ(ui) > δ(ui) = δ(ui−1) + l(ei) ⩾λ(ui−1) + l(ei).
Thus, the algorithm should not have terminated.
■
In spite of the fact that gen-FORD is a valid algorithm, the lack of determinism
in the choice of the order in which the edges are observed in Line 4 may be
abused to cause the algorithmto take exponentialtime. (See Johnson [8].) There
is a simple remedy: Order the edges of the digraph – any order will do – and
perform Line 4 by scanning the edges in this order. Once the scan is complete,
repeat it until there is no improvement in a complete scan. This procedure,
adv-FORD, is described in Algorithm 1.5, where it is assumed that
E = {e1, e2, ..., em}.

20
1 Paths in Graphs
Procedure adv-FORD(G(V,E),s,l;λ)
1
for every v ∈V do
2
λ(v) ←∞
3
λ(s) ←0
4
repeat
5
Flag ←False
6
for every 1 ⩽i ⩽m do
7
let u ei
−→v
8
if λ(u) is ﬁnite and λ(v) > λ(u) + l(ei) then do
9
λ(v) ←λ(u) + l(ei)
10
Flag ←True
11
until Flag = False
Algorithm 1.5: The advanced Ford algorithm.
Theorem 1.6 If the digraph has no accessible negativecircuits, then procedure
adv-FORD terminates in O(|V|·|E|) time, and when it terminates, λ(v) = δ(v)
for every vertex v.
Proof: Letusproveby induction on kthatforeveryvertexv,ifthereisashortest
path from s to v, which consists of k edges, then after the k-th application of
the loop (Lines 4-11), or if procedure adv-FORD halts earlier, λ(v) = δ(v).
For k = 0, the only applicable vertex is s, and Line 3 establishes the claim.
Assume now that the claim holds for 0 ⩽k ⩽j and show that it holds for
k = j + 1.
If procedure adv-FORD terminates before the j+1-st application of the loop,
the claim follows from Lemma 1.7. Let v be a vertex such that there is a shortest
path, P, from s to v that consists of j + 1 edges. (If there is also a shortest path
from s to v which consists of less edges, then there is nothing to prove.) Let
u e
−→v be the last edge in P. Since the subpath of P from s to u is a shortest
path to u, and since it consists of j edges, by the inductive hypothesis, after the
j-th application of the loop, λ(u) = δ(u). In the j+1-st application of the loop,
when e is checked, λ(v) gets the value δ(v), if it has not had that value already.
That proves the claim.
If v is accessible from s, and since there are no accessible negative circuits,
a shortest path from s to v is simple and consists |V|−1 edges, or fewer. Thus,

1.5 Shortest-Path Algorithms
21
during the |V|-th application of the loop no vertex improves its label, and the
procedure halts. Since the time complexity of the loop is O(|E|), the whole
procedure takes O(|V| · |E|) time.
■
A simple conclusion of the proof of Theorem 1.6 is that, if adv-FORD does
not halt after the V-th application of the loop, then there must be an accessible
negative circuit. The algorithm can easily be modiﬁed to detect the existence
of negative circuits in digraphs in time O(|V| · |E|).
1.5.4 The Floyd Algorithm
As in the previous subsection, assume we are given a ﬁnite digraph G(V,E) and
a length function l : E 
→R.7 We shall assume that there are no negativecircuits
in G. Our aim is to compute a complete distance table; that is, to compute the
distance δ(u,v), from vertex u to vertex v, for every (ordered) pair of u and v.
We shall assume that there are no parallel edges; if there is more than one
edge from u to v, then we can remove them all, except for one of the shortest.
Self-loops are also superﬂuous, but we shall allow them, and as we shall see,
the algorithm can be used to check if there are negative circuits, including the
case of negative self-loops.
If all edges were of nonnegative lengths, we could have used Dijkstra’s algo-
rithm from every vertex, and the complexity, in the simple implementation,
would have been O(|V|3). If there are negative edges, however, this cannot
be done. If we use Ford’s algorithm from every vertex, the complexity is
O(|V|2 · |E|), and for dense graphs, that can take Ω(|V|4) time.
Floyd’s algorithm, [9], presented bellow, achieves the goal in time complexity
O(|V|3).
Let us assume that V = {1, 2, ..., n} and for every 0 ⩽k ⩽n let δk be an
n × n matrix.8 δk(i,j) stands for the length of a shortest path from vertex i to
vertex j, among all paths which do not go through vertices k+1, k+2, ..., n
as intermediate vertices.
The Floyd algorithm is described in Algorithm 1.6.
It is easy to see that the time complexity of the Floyd algorithm is O(n3); there
are n applications of the outer loop (Lines 9–11),and in each of its applications,
there are n2 applications of Line 11.
The proof of validity is also easy, by induction on k. A shortest path from i
to j, among paths that do not go through vertices higher than k, either does not
7 R denotes the set of real numbers.
8 We will show that only one matrix is necessary, but for didactic reasons, let us start with n + 1
matrices.

22
1 Paths in Graphs
Procedure FLOYD(G(V,E),l;δn)
1
for every 1 ⩽i ⩽n do
2
if there is a self-loop i e
−→i and l(e) < 0 then do
3
δ0(i,i) ←l(e)
4
else δ0(i,i) ←0
5
for every 1 ⩽i,j ⩽n such that i ̸= j do
6
if there is an edge i e
−→j then do
7
δ0(i,j) ←l(e)
8
else δ0(i,j) ←∞
9
for every k, starting with k = 1 and ending with k = n do
10
for every 1 ⩽i,j ⩽n do
11
δk(i,j) ←min{δk−1(i,j), δk−1(i,k) + δk−1(k,j)}
Algorithm 1.6: The Floyd algorithm.
go through vertex k, and is therefore equal to δk−1(i,j), or does go through k
and is therefore equal to δk−1(i,k) + δk−1(k,j).
However, the space complexity of the algorithm, as stated in Algorithm 1.6,
is Ω(n3), since there are n matrices of size n × n each. It is easy to see that
there is no need to keep previous matrices. Two matrices sufﬁce: The previous
one and the one being computed. In fact, one matrix δ will do, where some
of the entries are as in δk−1; and some, as in δk. (Observe that a shortest path
from i to k, or from k to j, never needs to go through k, since there are no
negative circuits.) Thus, in fact, the space complexity can be reduced to O(n2),
by dropping the superscript indexes of δ.
Finally, one can use Floyd’s algorithm to check whether there are negativecir-
cuits in the digraph. Simply apply the algorithm, and in the end, check whether
there is an i for which δ(i,i) < 0. If so, there is a negative circuit.
1.6 Problems
Problem 1.1 Prove that if a connected (undirected) ﬁnite graph has exactly 2k
vertices of odd degree, then the set of edges can be partitioned into k paths such
that every edge is used exactly once. Is the condition of connectivity necessary
or can it be replaced by a weaker condition?

1.6 Problems
23
Figure 1.6: A graph for Problem 1.4.
Problem 1.2 Let G(V,E) be an undirected ﬁnite circular Euler graph; that is,
G is connected and for every v ∈V, d(v) is even. A vertex s is called universal
if every application of TRACE(s,G), no matter how the edges are ordered in
the incidence lists, produces an Euler circuit.
Prove that s is universal if and only if s appears in every simple circuit
of G.9
Problem 1.3 Let G(V,E) be a ﬁnite digraph such that for every v ∈V, din(v) =
dout(v). Also, assume that the exits from v are labeled 1, 2, ..., dout(v).
Consider a tour in G, which starts at a given vertex s. Every time a vertex
v is visited, the next exit is chosen to leave, starting with exit number 1 and
continuing cyclically. However, the tour stops if s is reached and all its exits
have been taken.
Prove that the tour stops and that every edge has been used at most once.10
Problem 1.4 A Hamilton path (circuit) is a simple path (circuit) in which every
vertex of the graph appears exactly once.
Prove that the graph shown in Figure 1.6 has no Hamilton path or circuit.
Problem 1.5 Prove that in every completely connected digraph (a digraph in
which every two vertices are connected by exactly one directed edge in one of
the two possible directions), there is always a directed Hamilton path. (Hint:
Prove by induction on the number of vertices.)
Problem 1.6 Prove that a directed Hamilton circuit of the de Bruijn digraph,
Gσ,n, corresponds to a directed Euler circuit of Gσ,n−1. Is it true that Gσ,n
always has a directed Hamilton circuit?
9 See[10].
10 More about such tours and ﬁnding Euler circuits can be found in [11].

24
1 Paths in Graphs
...
...
a
c
b
Figure 1.7: A switch for Problem 1.7.
Problem 1.7 In the following, assume that G(V,E) is a ﬁnite undirected graph,
with no parallel edges and no self-loops.
(i) Describe an algorithm which attempts to ﬁnd a Hamilton circuit in G by
working with a partial simple path. If the path cannot be extended in either
direction, then try to close it into a simple circuit by the edge between its
endpoints, if it exists, or by a switch, as suggested by Figure 1.7, where
edges a and b are added and c is deleted. Once a circuit is formed, look
for an edge from one of its vertices to a new vertex, and open the circuit
to a now longer simple path, and so on.
(ii) Prove that if for every two vertices u and v, d(u) + d(v) ⩾n, where
n = |V|, then the algorithm never fails to produce a Hamilton circuit.
(iii) Deduce Dirac’s Theorem [12]: “If for every vertex v, d(v) ⩾n
2 , then G
has a Hamilton circuit.”
Problem 1.8 Describe an algorithm for ﬁnding the number of shortest paths
from s to t, after the BFS algorithm has been performed.
Problem 1.9 A digraph is called acyclic if there are no directed circuits.11 Let
G(V,E) be a ﬁnite acyclic digraph. A bijection f : V 
→{1, 2, ..., n}, where
n = |V|, is called a topological sorting if for every edge u −→v, f(u) < f(v).
Consider the procedure described in Algorithm 1.7. A queue Q of vertices is
used, which is initially empty.
Prove that this is an algorithm, that it computes a topological sorting and that
its time complexity is O(|V| + |E|).
Problem 1.10 Show that the Dijkstra algorithm is not applicable if there are
negative edges, even if the digraph is acyclic.
11 Sometimes called DAG, for directed acyclic graph.

1.6 Problems
25
Procedure TOPO.SORT(G;f)
1
for every v ∈V compute din(v)
2
for every v ∈V do
3
if din(v) = 0 then put v in Q
4
i ←1
5
while Q ̸= ∅do
6
remove the ﬁrst vertex u from Q
7
f(u) ←i
8
i ←i + 1
9
for every edge u −→v do
10
din(v) ←din(v) −1
11
if din(v) = 0 then put v in Q
Algorithm 1.7: Topological sorting.
Problem1.11 In the Dijkstra algorithm,assume the sequenceof vertices which
join P, in this order, is s = v1, v2, .... Prove that the sequence λ(v1), λ(v2), ...
is nondecreasing.
Problem 1.12 Assume G(V,E) is a ﬁnite digraph, l : E 
→R a length function,
and assume the length of every directed circuit is positive. Also, assume s ∈V
is the source, V ′ is the set of vertices accessible from, s and δ : V ′ 
→R is the
distance function.
We want to compute the function ν : V ′ 
→Z, where ν(v) is the number of
shortest paths from s to v.
(i) Let H(V ′,E′) be a subgraph of G, where E′ is the set of edges u
e
−→v
such that δ(v) = δ(u) + l(e). Prove that H is acyclic.
(ii) Show how a modiﬁcation of the topological sorting algorithm, applied to
H, can compute ν. What is the complexity of this algorithm?
Problem 1.13 Prove that a connected undirected graph G is orientable (by
giving each edge some direction) into a strongly connected digraph if and only
if each edge of G is in some simple circuit in G.
Problem 1.14 Prove that if the digraph G(V,E), with edge lengths l : E 
→R,
has a negative circuit, then there is a vertex i on this circuit such that in the
matrix δ computed by Floyd’s algorithm, δ(i,i) < 0.

26
1 Paths in Graphs
Procedure WARSHALL(G(V,E);T)
1
for every 1 ⩽i,j ⩽n do
2
if there is an edge i −→j in G then do
3
T(i,j) ←1
4
else T(i,j) ←0
5
for every k, starting with k = 1 and ending with k = n do
6
for every 1 ⩽i,j ⩽n do
7
T(i,j) ←max{T(i,j), T(i,k) · T(k,j)}
Algorithm 1.8: The Warshall algorithm.
Problem 1.15 The transitive closure of a digraphG(V,E) is a digraphT(V,ET)
such that there is an edge u −→v in ET if and only if there is anonempty directed
path from u to v in G.
Show how BFS can be used to construct T in time O(|V| · |E|).
Problem 1.16 This problem is about Warshall’s algorithm [13] for the
computation of the transitive closure.
Given a digraph G(V,E), where V = {1, 2, ..., n}, we want to compute an
n × n matrix T, such that
T(i,j) =
 1 if there is a nonempty directed path in G from i to j
0 otherwise
Warshall’s algorithm is described in Algorithm 1.8.
(i) What is the complexity of Warshall’s algorithm? Compare it with the
repeated BFS above.
(ii) Prove the validity of Warshall’s algorithm. (Hint of one possible proof:
Consider a (simple) path from i to j and the order in which the intermediate
vertices on it are processed in the loop of Lines 5–7.)
(iii) Show that there is a close relationship between Floyd’s algorithm and
Warshall’s: δ(i,j) is ﬁnite if and only if T(i,j) = 1.
Problem 1.17 This problem is about Dantzig’s algorithm [14] for computing
all distances in a ﬁnite digraph, like Floyd’s algorithm.

1.6 Problems
27
Procedure Dantzig(G(V,E),l;δn)
1
for every 1 ⩽i,j,k ⩽n do
2
δk(i,j) ←∞
3
δ1(1,1) ←l(1,1)
4
for every k, starting with k = 2 and ending with k = n do
5
for every 1 ⩽i < k do
6
δk(i,k) ←min1⩽j<k{δk−1(i,j) + l(j,k)}
7
δk(k,i) ←min1⩽j<k{l(k,j) + δk−1(j,i)}
8
for every 1 ⩽i,j < k do
9
δk(i,j) ←min{δk−1(i,j), δk(i,k) + δk(k,j)}
Algorithm 1.9: The Dantzig algorithm.
For 1 ⩽i,j ⩽k ⩽n, let δk(i,j) denote the length of a shortest path from i to
j among paths that do not use vertices higher than k. The algorithm computes
δk(i,j) for all i, j and k.
Set l(i,j) as follows:
l(i,j) =









l(e)
if i ̸= j and there is an edge i e
−→j
∞
if i ̸= j and there is no edge i −→j
min{0, l(e)} if i = j and there is an edge i e
−→i
0
if i = j and there is no edge i −→i
The Dantzig algorithm is described in Algorithm 1.9.
(i) Show that Dantzig’s algorithm is valid.
(ii) How can negative circuits be detected?
(iii) What is the time complexity of this algorithm?
(iv) What is the space complexity of the algorithm? Can it be reduced?
Problem 1.18 Describe an algorithm whose input is a ﬁnite,strongly connected
digraph G(V,E), and it determines whether there is a directed circuit whose
length (in terms of the number of edges in it) is odd. The algorithm should be
of time complexity O(|E|). (Hint: Does the fact that for some pair of vertices,
a and b, there are two directed paths from a to b, one of odd length and one of
even length, imply the existence of an odd circuit?)

28
1 Paths in Graphs
Problem 1.19 Let an undirected connected ﬁnite graph G(V,E) be the road
map of a country, where each edge represents a road, and each vertex represents
an intersection. Let h : E 
→R+ be a function such that h(e) speciﬁes the
maximum height of vehicles allowed on the road represented by e. Also, let
c ∈V be a speciﬁed vertex (such as the capital of the country).
Write an algorithm that, when given the data above, computes for each vertex
v the maximum height of vehicles which can travel from c to v.
Make sure the time complexity of your algorithm is O(|V|2).
(Hint: A proper modiﬁcation of Dijkstra’s algorithm can solve the problem.)
Bibliography
[1] Golomb, S. W., Shift Register Sequences, Holden-Day, 1967.
[2] Moore, E. F., “The Shortest Path Through a Maze,” Proc. Iternat. Symp. Switching
Th., 1957, Part II, Harvard Univ. Press, 1959, pp. 285–292.
[3] Dijkstra, E. W., “A Note on Two Problems in Connexion with Graphs,” Numerische
Math., Vol. 1,1959, pp. 269–271.
[4] Cormen, T. H., C. E. Leiserson, and R. L. Rivest, Introduction to Algorithms, The
MIT Press, 1990.
[5] Fredman, M. L., and R. E. Tarjan, “Fibonacci Heaps and Their Uses in Improved
Network Optimization Algorithms,” J. of the ACM, Vol. 34, No. 3, 1987, pp. 596–
615.
[6] Ford, L. R., Jr., “Network Flow Theory,” The Rand Corporation, P-923, August,
1956.
[7] Ford, L. R., Jr. and D. R. Fulkerson, D. R., Flows in Networks, Princeton University
Press, 1962, Chap. III, Sec. 5.
[8] Johnson, D. B., “A Note on Dijkstra’s Shortest Path Algorithm,” J. of the ACM,
Vol. 20, No. 3, 1973, pp. 385–388.
[9] Floyd, R. W., “Algorithm 97: Shortest Path,” Comm. of the ACM, Vol. 5, 1962,
p. 345.
[10] Ore, O., “A Problem Regarding the Tracing of Graphs,” Elem. Math., Vol. 6, 1951,
pp. 49–53.
[11] Bhatt, S., D. Greenberg, S. Even, and R. Tayar, “Traversing Directed Eulerian
Mazes,” J. of Graph Algorithms and Applications, Vol. 6, No. 2, 2002, pp. 157–173.
[12] Dirac, G. A., “Connectivity Theorems for Graphs,” Quart. J. of Math., Ser. (2),
Vol. 3, 1952, pp. 171–174.
[13] Warshall, S., “A Theorem on Boolean Matrices,” J. of the ACM, Vol. 9, No. 1, 1962,
pp. 11–12.
[14] Dantzig, G. B., “All Shortest Routes in a Graph,” Oper. Res. House, Stanford
University Tech. Rep. 66–3, November 1966.

2
Trees
2.1 Tree Deﬁnitions
Let G(V,E) be an undirected, ﬁnite, or inﬁnite graph. We say that G is circuit-
free if there are no simple circuits in G. G is called a tree if it is connected and
circuit-free.
Theorem 2.1 The following four conditions are equivalent:
(a) G is a tree.
(b) G is circuit-free, but if any new edge is added to G, a simple circuit is
formed.
(c) G has no self-loops, and for every two vertices, there is a unique simple
path connecting them.
(d) G is connected, but if any edge is deleted, the connectivity of G is
interrupted.
Proof: We shall prove that (a) ⇒(b) ⇒(c) ⇒(d) ⇒(a).
(a) ⇒(b): We assume that G is connected and circuit-free. Let a e b be
a new edge. If a = b then e forms a self-loop, and therefore a simple circuit
exists. If a ̸= b, there is a simple path in G (without e) connecting a and b. The
addition of e creates a simple circuit.
(b) ⇒(c): We assume that G is circuit-free and that no edge can be added to
G without creating a simple circuit. Clearly, G has no self-loops.
Let a and b be any two vertices of G. If there is no path connecting them,
then we can add an edge between a and b without creating a simple circuit.
Thus, G must be connected.
Moreover, if there are two simple paths, P and P ′, connecting a and b, then
there must be a simple circuit in G. To see that, assume that
P : a = v0 e1 v1 e2 v2 ···vl−1 el vl = b,
29

30
2 Trees
and
P ′ : a = v′
0
e′
1 v′
1
e′
2 v′
2 ···v′
l′−1
e′
l′ v′
l′ = b.
Since both paths are simple, one cannot be the beginning of the other. Let i be
the ﬁrst index for which ei ̸= e′
i. That is, the two paths split at vi−1 = v′
i−1. Let
v be the ﬁrst vertex on P, after the split, which is also on P ′. Thus, for some
j,k ⩾i, v = vj = v′
k. The subpath of P, between vi−1 and vj, and the subpath
of P ′, between v′
i−1 and v‘k, form a simple circuit.
(c) ⇒(d): We assume that G has no self-loops and that for every two vertices,
there is a unique simple path connecting them. Thus, G is connected.
Assume now that we delete an edge a e b from G. Since G has no self-
loops, a ̸= b. If there is still a (simple) path in (V,E\{e}) connecting a and b,
then in (V,E), there are two simple paths connecting a and b. A contradiction.
(d) ⇒(a): We assume that G is connected and that no edge can be deleted
without interrupting the connectivity.
If G has a simple circuit, any edge on this circuit can be deleted without
interrupting the connectivity. Thus, G is circuit-free.
■
There are two, more common ways to deﬁne a ﬁnite tree. These are presented
in the following theorem:
Theorem 2.2 Let G(V,E) be a ﬁnite graph and n = |V|. The following three
conditions are equivalent:
(a) G is a tree.
(b) G is circuit-free and |E| = n −1.
(c) G is connected and |E| = n −1.
Proof: For n = 1 the theorem is trivial. Assume n ⩾2. We shall prove that (a)
⇒(b) ⇒(c) ⇒(a).
(a) ⇒(b): Let us prove, by induction on n, that if G(V,E) is a tree, then
|E| = n −1. This statement clearly holds for n = 1. Assume that it is true for
all n < m, and let G be a tree with m vertices.
Delete from G any edge e. By condition (d) of Theorem 2.1, the resulting
graph is not connected any more, and has two disjoint connected components.
Each of these components is circuit-free and is therefore a tree. By the inductive
hypothesis,each componenthas one edge less than its number of vertices. Thus,
together they have m−2 edges. Add e back, and the number of edges is m−1.
(b) ⇒(c): We assume that G is circuit-free and has n −1 edges. Let us ﬁrst
show that G has at least two vertices of degree 1.

2.2 Minimum Spanning Tree
31
Choose any edge u e v; there is at least one edge, since the number of
edges is n−1 and n ⩾2. Also, u ̸= v since G is circuit-free. As in the TRACE
procedure, walk on new edges, starting from u. Since the number of edges is
ﬁnite, this walk must end, say, at vertex t1. Also, the traced path is simple,
or a simple circuit would have been formed. Thus, d(t1) = 1. A similar walk,
starting from v, yields another vertex, t2, and d(t2) = 1, as well. Thus, there
are two vertices of degree 1.
Now, the proof that G is connected proceeds by induction on the number of
vertices, n. Obviously, the statement holds for n = 1. Assume that it holds for
n = m−1, and let G be a circuit-free graph with m vertices and m−1 edges.
Eliminate from G a vertex v of degree 1 and its incident edge. The resulting
graph is still circuit-free and has m−1 vertices and m−2 edges. Thus, by the
inductive hypothesis it is connected. Therefore, G is connected, as well.
(c) ⇒(a): Assume that G is connected and has n −1 edges. As long as G
has simple circuits, we can eliminate edges (without eliminating vertices) and
maintain the connectivity. When this process terminates, the resulting graph is
a tree, and, by (a) ⇒(b), it has n −1 edges. This, however, is the number of
edges we started with. Thus, no edge has been eliminated, and therefore G is
circuit-free.
■
Let us call a vertex whose degree is 1, a leaf. A corollary of Theorem 2.2 and
the statement proved in the (b) ⇒(c) part of its proof is the following:
Corollary 2.1 A ﬁnite tree with more than one vertex has at least two leaves.
2.2 Minimum Spanning Tree
A graph G′(V ′,E′) is called a subgraph of a graph G(V,E), if V ′ ⊂V and
E′ ⊂E.1
Assume G(V,E) is a ﬁnite, connected (undirected)graph and each edge e ∈E
has a known length l(e) > 0. Assume that we want to ﬁnd a connected subgraph
G′(V,E′), whose total length, 
e∈E l(e), is minimum; or, in other words, we
want to remove from G a subset of edges whose total length is maximum, and
yet, the resulting subgraph remains connected. Such a subgraph is a tree. For
G′ is assumed to be connected, and since its total length is minimum, none of
its edges can be removed without destroying its connectivity. By Theorem 2.1
(see part (d)) G′ is a tree. A subgraph of G that contains all of its vertices and
1 Note that an arbitrary choice of V ′ ⊂V and E′ ⊂E may not yield a subgraph, simply
because it may not be a graph; that is, some of the endpoints of edges in E′ may not be in V ′.

32
2 Trees
Procedure PRIM(G,l;T)
1
for every v ∈V do
2
λ(v) ←∞
3
choose a vertex s ∈V
4
λ(s) ←0
5
ε(s) ←∅
6
TEMP ←V
7
T ←∅
8
while TEMP ̸= ∅do
9
choose a vertex v ∈TEMP for which λ(v) is minimum
10
TEMP ←TEMP \ {v}
11
T ←T ∪ε(v)
12
for every v e u do
13
if u ∈TEMP and λ(u) > l(e) then do
14
λ(u) ←l(e)
15
ε(u) ←{e}
Algorithm 2.1: The Prim algorithm.
is a tree is called a spanning tree of G. Thus, our problem is that of ﬁnding a
minimum (length) spanning tree (MST) of G.
There are several known algorithms for constructing an MST. We describe
here the Prim algorithm [1]. Additional algorithms are discussed intheproblems
section in the end of the chapter.
The Prim algorithm is described in Algorithm 2.1.2
In the following discussion, we shall use T to denote a subgraph of G, which
is a tree, and the set of edges in this tree. TEMP denotes the set of vertices, not
yet in T.
The idea of the algorithm is to “grow” T, by starting with a tree with one
vertex, s, and in each step adding a new leaf, v, to T, until all vertices have
joined it. Thus, v is chosen in such a way that the edge connecting it to T (the
single edge in the set ε(v)) is of minimum length among edges which connect
a vertex of T to a vertex of TEMP.
2 The structure of the algorithm is similar to that of Dijkstra, but historically, Prim’s algorithm
precedes it.

2.2 Minimum Spanning Tree
33
When a new vertex, v, joins T, all its incident edges, v e u are checked. If
u is not a vertex of T yet (this is equivalent to the fact that u ∈TEMP), and if
l(e) is shorter than the presently known shortest edge (if any) that connects u
to a vertex of T, then the value of λ(u) is updated to be l(e), and the edge e is
recorded in (the set) ε(u).
Since G is connected, when PRIM halts, the set of edges in T constitutes a
spanning tree. The complexity of the algorithm is similar to that of Dijkstra’s
algorithm; see Section 1.5.2.
Before we discuss the validity of the algorithm, let us deﬁne the concept of a
cut.
Let S ⊂V and ¯S = V \S. The cut (S;¯S) is the set of edges with one endpoint
in S and the other in ¯S.
Theorem 2.3 The tree T produced by Prim’s algorithm is a Minimum Spanning
Tree of G.
Proof: Let us prove that for every T produced while the algorithm is running,
there is an MST, Topt, of G, such that T is a subgraph of Topt. This implies that
the ﬁnal T is an MST. The proof is by induction on ν, the number of vertices
in T.
Initially, ν = 1 and there are no edges in T. Thus, the claim holds trivially.
Assume now that the claim holds for ν < |V|, and let us prove it for ν + 1.
By the inductive hypothesis, there is an MST, Topt, such that T is a subgraph
of Topt. If the newly added edge, e, is such that e ∈Topt, then the claim holds
for T ∪{e} as well, and we are done.
If e /∈Topt, consider the graph H(V,Topt ∪{e}). By Theorem 2.1 part (b),
there is a simple circuit C in H. Also, let S be the set of vertices in T and consider
the cut (S;¯S) of G. There are at least two edges of C in (S;¯S). One of them is
e, and assume another is e′. Now consider the graph H′(V,(Topt ∪{e})\{e′}).
Since an edge of a simple circuit has been removed, H′ remains connected, and
the number of edges is |V|−1. By Theorem 2.2 part (c), H′ is a spanning tree.
By the choice of v, λ(v) is minimum (among values of λ for vertices in TEMP),
and the edge e ∈ε(v) satisﬁes l(e) = λ(v) ⩽l(e′). Thus, the total length of H′
is less than or equal to that of Topt, and is therefore an MST.
■
The analogous problem for digraphs, namely, that of ﬁnding a subset of the
edges E′ whose total length is minimum among those for which (V,E′) is
a strongly connected subgraph, is much harder. In fact, even the case where
l(e) = 1 for all edges is hard. This is discussed in Chapter 10.

34
2 Trees
2
3
1
2
3
1
2
3
1
Figure 2.1: The spanning trees on three named vertices.
2.3 Cayley’s Theorem
In a later section we shall consider the question of the number of spanning trees
of a given graph. Here we consider the more restricted, and yet interesting,
problem of the number of trees one can construct on a given set of vertices,
V = {1, 2, ..., n}.
For n = 2, there is only one tree that one can construct, consisting of an edge
between the two vertices. For n = 3, there are three possible trees, as shown
in Figure 2.1. The reader can verify, by exhausting all cases, that for n = 4 the
number of trees is 16. The following theorem is due to Cayley [3]. Warning:
We shall use the integers 1, 2, ..., n in three different meanings. As names of
vertices, as integers, and as letters of an alphabet.
Theorem 2.4 The number of spanning trees for n distinct vertices is nn−2.
The remainderof this section describes a proofdue to Prüfer [4]. (For a survey
of various proofs see Moon [5].)
Assume V = {1, 2, ..., n}. Let us display a bijection between the set of
the spanning trees and the nn−2 words of length n −2 over the alphabet
{1, 2, ..., n}. The mapping from the set of spanning trees to the corresponding
set of words is deﬁned by an algorithm which is described in Algorithm 2.2.
For example, assume that n = 6 and T is as shown in Figure 2.2. We now
apply TREEtoWORD. We start with the given T and an empty template for
a word w of 4 letters. This is depicted in the ﬁrst line of Figure 2.3. T has 3
leaves, and vertex 2 has the least name. Therefore, vertex 2 and its incident
edge, 2
4, are removed. The ﬁrst letter in the word w is 4, as shown in the
second line of Figure 2.3. The next leaf to be removed is 3, and a2 = 1, and
so on. After four steps, the tree consists of two vertices (4 and 6) and an edge
between them, while w = 4164.
By Corollary 2.1, Line (2) of TREEtoWORD algorithm can always be per-
formed. Note that when a leaf is removed, the remaining graph is still a tree. It
follows that for every tree of n vertices, a word w of length n−2 is produced.
Since algorithm TREEtoWORD is deterministic, it deﬁnes a mapping f from

2.3 Cayley’s Theorem
35
Procedure TREEtoWORD(T(V,E);w = a1a2 ···an−2)
1
for every i starting with 1 and ending with n −2 do
2
choose a leaf v whose name is minimum among the leaves of T
3
let v e u be its incident edge
4
ai ←u
5
remove e and v from T
Algorithm 2.2: The TREEtoWORD algorithm. Mapping a spanning
tree T to a word w.
2
5
4
6
1
3
Figure 2.2: T: An example of a spanning tree with six vertices.
trees to words. It remains to be shown that no word is produced by two different
trees, and that for every word w there is a tree T such that f(T) = w.
Notice that TREEtoWORD is insensitive to the nature of the set of vertices,
V, of T, as long as the names of the vertices are distinct and an order is deﬁned
on these names. We shall assume that V is a set of integers, not necessarily
{1, 2, ..., n}.
Lemma 2.1 If in T a vertex v has a degree d(v), then in w = f(T) the letter v
appears d(v) −1 times.
Proof: When each of the edges incident on v are removed, except the last one,
one writes the letter v in w. Thus, v appears d(v)−1 times in w. The last edge
may not be removed at all, if v is one of the two vertices which remain when
TREEtoWORD halts. And if v’s last edge is removed, then v is the removed
leaf, and its neighbor, not v, is written in w.
■
Lemma 2.2 For every word w = a1a2 ···an−2 over an alphabet V, where
|V| = n, there is a unique tree T whose vertices are V and f(T) = w.

36
2 Trees
2
4
6
1
3
5
4
6
1
3
4
5
4
6
1
4
5
1
6
4
6
4
5
1
6
4
6
4
1
4
Figure 2.3: Applying TREEtoWORD to the the example tree T.
Proof: By induction on n ⩾2. For n = 2, w is the empty word and if V = {u, v},
there is only one tree whose set of vertices is V. TREEtoWORD does nothing
(leaving T intact, and w empty).
Now assume the claim holds for n −1 and let us prove it for n. Let w =
a1a2 ···an−2 be a word over an alphabet V, where |V| = n.
Since the alphabet has n letters, while there are n −2 appearances of letters
in w, there are at least two letters missing from w. Let v be the least letter that
does not appear in w. Now consider the word w′ = a2a3 ···an−2, with the
alphabet V ′ = V \ {v}. By the inductive hypothesis, there is a unique tree T ′
whose set of vertices is V ′, such that f(T ′) = w′. Notice that a1 appears in w,
and therefore a1 ̸= v, and thus a1 ∈V ′.
By Lemma 2.1, the set of leaves of T ′ is equal to the set of elements of V ′
that do not appear as letters in w′. Thus, v is less than every leaf of T ′, with the
possible exception of a1.
Now deﬁne T to be the tree of n vertices that results from T ′ by adding to it
the vertex v and an edge connecting v to a1. Clearly T is a tree, and a1 is not a
leaf of T, even if it has been a leaf of T ′. It follows that v is the least leaf of T.
Thus, if one applies TREEtoWORD to T, the ﬁrst letter assigned to the word
TREEtoWORD builds is indeed a1, v and its incident edge are removed, and
the tree on which TREEtoWORD continues is T ′. By the inductive hypothesis,
T ′ is the unique tree that produces w′. We conclude that f(T) = w.

2.4 Directed Tree Deﬁnitions
37
Since v is the least letter missing from w, and since a1 is the ﬁrst letter of w,
every tree that produces w must have an edge v
a1. Thus, T is the unique
tree that produces w.
■
2.4 Directed Tree Deﬁnitions
A digraph G(V,E) is said to have a root r if r ∈V and every vertex v ∈V is
reachable from r; that is, there is a directed path that starts in r and ends in v.
A digraph (ﬁnite or inﬁnite) is called a directed tree if it has a root and its
underlying undirected graph is a tree.
Theorem 2.5 Assume G is a digraph. The following ﬁve conditions are
equivalent:
(a) G is a directed tree.
(b) G has a root from which there is a unique directed path to every vertex.
(c) G has a root r for which din(r) = 0, and for every other vertex v, din(v)=1.
(d) G has a root and the deletion of any edge (but no vertices) interrupts this
condition.
(e) The underlying undirected graph of G is connected and G has one vertex
r for which din(r) = 0, while for every other vertex v, din(v) = 1.
Proof: We prove that (a) ⇒(b) ⇒(c) ⇒(d) ⇒(e) ⇒(a).
(a) ⇒(b): We assume that G has a root, say r, and its underlying undirected
graph G′ is a tree. Thus, by Theorem 2.1 part (c), there is a unique simple path
from r to every vertex in G′. Also, G′ is circuit-free. Thus, a directed path from
r to a vertex v, in G, must be simple and unique.
(b) ⇒(c): Here, we assume that G has a root, say r, and a unique directed
path from it to every vertex v. First, let us show that din(r) = 0.
Assume there is an edge u e
−→r. There is a directed path from r to u, and
it can be continued, via e, back to r. Thus, in addition to the empty path from
r to itself (containing no edges), there is one more path, in contradiction to the
assumption of the path uniqueness.
Now, we have to show that if v ̸= r, then din(v) = 1. Clearly, din(v) > 0, for
it must be reachable from r. If din(v) > 1, then there are at least two edges,
say, v1 e1
−→v and v2 e2
−→v. Since there is a directed path, P1, from r to v1, and
a directed path, P2, from r to v2, by adding e1 to P1 and e2 to P2, we get two
different directed paths from r to v, contradicting the uniqueness assumption.
(Note that the two paths are different, even if v1 = v2.)

38
2 Trees
(c) ⇒(d): This proof is trivial, for the deletion of any edge u e
−→v will make
v unreachable from r.
(d) ⇒(e): We assume that G has a root, say r, and the deletion of any edge
interrupts this condition.
Clearly, din(r) = 0, for any edge entering r can be deleted without inter-
rupting the condition that r is a root. Also, if v ̸= r, then din(v) > 0, since v is
reachable from r. If din(v) > 1, let v1 e1
−→v and v2 e2
−→v be two edges entering
v. Let P be a simple directed path from r to v. At least one of the edges e1 and
e2 is not used in P. This edge can be deleted without interrupting the fact that
r is a root. Thus, din(v) = 1.
(e) ⇒(a): We assume that the underlying undirected graph of G, G′, is
connected, din(r) = 0, and for v ̸= r, din(v) = 1. First let us prove that r is a
root of G.
Let P ′ be a simple (undirected) path connecting r and v in G′. This must
correspond to a directed path, P, from r to v in G, for if any of the edges points
in the wrong direction, it would imply either that din(r) > 0 or that for some
u, din(u) > 1.
Now, assume G′ has a simple circuit, C′. In G, all corresponding edges must
be directed in the same circular direction, or there would be a vertex v for which
din(v) ⩾2. Thus, r is not one of the vertices of C, the directed circuit of G
which corresponds to C′. Now, let P be a shortest directed path from r to a
vertex of C, say it is u. Then, din(u) must be at least 2. A contradiction. Thus,
G′ is circuit-free, and is therefore an undirected tree.
■
In case of ﬁnite digraphs one more useful deﬁnition of a directed tree is
possible:
Theorem 2.6 A ﬁnite digraph G(V,E) is a directed tree if and only if its
underlying undirected graph, G′, is circuit-free, one of its vertices, r, satisﬁes
din(r) = 0, and for all other vertices v, din(v) = 1.
Proof: The “only if” part follows directly from the deﬁnition of a directed tree
and Theorem 2.5 part (c).
To prove the “if” part we ﬁrst observe that the number of edges is |V| −1.
Thus, by Theorem 2.2, (b) ⇒(c), G′ is connected. Thus, by Theorem 2.5, (e)
⇒(a), G is a directed tree.
■

2.5 The Inﬁnity Lemma
39
...
Figure 2.4: An example of an inﬁnite arbitrated digraph with no root.
Let us say that a digraph is arbitrated (Berge [6] calls it quasi strongly con-
nected) if for every two vertices, v1 and v2, there is a vertex v called an arbiter
of v1 and v2, such that there are directed paths from v to v1 and from v to v2.
There are inﬁnite digraphs which are arbitrated but do not have a root. For exam-
ple, see the digraph of Figure 2.4. However, for ﬁnite digraphs the following
theorem holds:
Theorem 2.7 If a ﬁnite digraph is arbitrated then it has a root.
Proof: Let G(V,E) be a ﬁnite arbitrated digraph, where V = {1, 2, ..., n}.
Let us prove, by induction, that every set {1, 2, ..., m}, where m ⩽n, has an
arbiter; i.e., a vertex am such that every 1 ⩽i ⩽m is reachable from am. By
deﬁnition, a2 exists. Assume am−1 exists. Let am, be the arbiter of am−1 and
m. Since am−1 is reachable from am, and every 1 ⩽i ⩽m −1 is reachable
from am−1, every 1 ⩽i ⩽m −1 is also reachable from am.
■
Thus, for ﬁnite digraphs, the condition that it has a root, as in Theorem 2.5
parts (a), (b), (c), and (d), can be replaced by it being arbitrated.
2.5 The Inﬁnity Lemma
The following is known as König’s Inﬁnity Lemma [7]:
Theorem 2.8 If G is an inﬁnite digraph with a root r, and every vertex has a
ﬁnite out-degree, then G has an inﬁnite directed path starting in r.
Before we present our proof, let us point out the necessity of the ﬁniteness of
the out-degrees of the vertices. For, if we allow a single vertex to be of inﬁnite
out-degree, the conclusion does not follow. Consider the digraph of Figure 2.5.
The root r is connected to vertices v1
1, v2
1, v3
1, ..., where vk
1 is the second vertex
on a directed path of length k. It is clear this directed tree is inﬁnite, and yet
it has no inﬁnite path. Furthermore, the replacement of the condition of ﬁnite
degrees by the condition that for every k, the tree has a path of length k, does
not work either, as the same example shows.

40
2 Trees
r
...
v 1
1
v 2
1
v 3
1
v 3
2
v 3
3
v 2
2
Figure 2.5: An example of an inﬁnite digraph with no inﬁnite path.
Proof: 3 First, let us restrict our attention to a directed tree T which is an inﬁnite
subgraph of G. T’s root is r. All vertices of distance 1 away from r in G are also
of distance 1 away from r in T. In general, if a vertex v is of distance δ away
from r in G, it is also of distance δ away from r in T; all the edges entering v in
G are now dropped, except one which connects a vertex of distance δ−1 to v.
It is sufﬁcient to show that in T there is an inﬁnite directed path from r. Clearly,
since T is a subgraph of G, all its vertices are of ﬁnite out-degrees too.
In T, r has inﬁnitely many descendants (vertices reachable from r). Since r is
of ﬁnite out-degree,at least one of its sons (the vertices reachable via one edge),
say r1, must have inﬁnitely many descendants. One of r1’s sons has inﬁnitely
many descendants, too, and so we continue to construct an inﬁnite directed path
r, r1, r2, ....
■
Despite the seeming simplicity of the theorem, it is useful. For example,
imagine we conduct a search on a directed tree of ﬁnite degrees (but it is not
known that there is a bound on the degrees). If it is known that the tree has no
inﬁnite directed path, then the theorem assures us that the tree is ﬁnite and our
search will terminate.
An interesting application of Theorem 2.8 was made by Wang [8]. Consider
the problem of tiling the plane with square tiles, all of the same size (Wang
calls the tiles “dominoes”). There is a ﬁnite number of tile families. The sides
of the tiles are labeled by letters of an alphabet, and all tiles of one family have
the same labels and are indistinguishable. Tiles may not be rotated or reﬂected,
and the labels are speciﬁed for their north side, south side, and so on. There
3 We use a naive approach to set theory and thus ignore the applications of the axiom of choice.

2.5 The Inﬁnity Lemma
41
a
b
c
c
a
a
a
d
a
d
b
c
b
a
b
a
Figure 2.6: A set of families of tiles.
c
a
a
a
a
b
c
a
d
a
d
b
d
a
d
b
a
b
c
a
c
b
a
b
Figure 2.7: A torus constructed from the tile families depicted in Figure 2.6.
is an inﬁnite supply of tiles of each family. Tiles may abut one another if the
joining sides have the same labels. For example, if the tile families are as shown
in Figure 2.6, then we can construct the “torus,” as shown in Figure 2.7. Now,
by repeating this torus inﬁnitely many times horizontally and vertically, we can
tile the whole plane.
Wang proved that if it is possible to tile the upper right quadrant of the plane
with a given ﬁnite set of t tile families, then it is possible to tile the whole plane.
The reader should realize that a southwest shift of the upper-right tiled quadrant
cannot be used to cover the whole plane. In fact, if the number of tile families is
not restricted to be ﬁnite, one can ﬁnd sets of families for which the upper-right
quadrant is tileable, whereas the whole plane is not. (See Problem 2.14.)
Consider the following directed tree T: The root r is connected to vertices,
each representing a tile family, that is, a 1 × 1 square, tiled with the tile of that
family. Thus, the out-degree of r is t. For every k ⩾1 and every legitimate way
of tiling a (2k + 1) × (2k + 1) square, there is a vertex in T; its father is the
vertex which represents the tiling of a (2k −1) × (2k −1) square, identical to
the center part of the tiled square represented by the son.
Now, if the upper-right quadrant is tilable, then T has inﬁnitely many vertices.
However, a vertex representing a certain tiling of a (2k −1) × (2k −1) square
has at most t8k sons. Since the out-degree of each vertex is ﬁnite (although it
may not be bounded),Theorem 2.8 implies that there is an inﬁnite directed path
in T. Such a path describes a way to tile the whole plane.

42
2 Trees
2.6 Problems
Problem 2.1 Let T1(V,E1) and T2(V,E2) be two spanning trees of G(V,E).
Prove that for every α ∈E1 \E2 there is a β ∈E2 \E1 such that each of the sets
(E1 \ {α}) ∪{β}
(E2 \ {β}) ∪{α}
deﬁnes a spanning tree.
Problem 2.2 Let G(V,E) be a ﬁnite connected graph and l : E 
→R. Describe
an algorithm for ﬁnding a maximum length spanning tree of G; explain why it
is valid and analyze its complexity.
Problem 2.3 Algorithm 2.3 describes the Kruskal [2] algorithm for computing
an MST of a ﬁnite connected undirected graph G(V,E) with a length function
l : E 
→R. The set of edges in the resulting tree is T. Prove that the resulting T
is indeed an MST and analyze the complexity of the algorithm.
Problem 2.4 Describe an algorithm that is similar to Kruskal’s, but instead of
adding edges, from light to heavy, as long as they do not create a simple circuit,
it deletes edges, from heavy to light, as long as connectivity is maintained.
Prove its validity and analyze its complexity.
Problem 2.5 Let G(V,E) be a ﬁnite undirected graph, with a length function
l : E 
→R. Also, T is an MST of G.
Procedure KRUSKAL(G(V,E),l;T)
1
sort the set E into E = {e1, e2, ..., e|E|}, so that if i < j, then l(ei) ⩽
l(ej)
2
T ←∅
3
for i, starting with i = 1 and ending with i = |E| do
4
if (V,T ∪{ei}) is circuit-free then
5
T ←T ∪{ei}
Algorithm 2.3: The Kruskal algorithm.

2.6 Problems
43
A new edge, e, is added, of length l(e). The following is a sketch of an
algorithm for mending the tree to be an MST of the new graph: Add e to T. Let
e′ be an edge of maximum length in the simple circuit that forms. If e′ ̸= e,
then remove e′. Otherwise, leave T intact.
Prove the validity of this algorithm and compare its time complexity with
computing an MST anew.
Problem 2.6 Let G(V,E) be a ﬁnite undirected graph, with a length function
l : E 
→R. Also, T is an MST of G.
An edge e ∈E is removed from G to form G′. The following is a sketch of
an algorithm for mending the tree to be an MST of G′: If e /∈T, do nothing.
Otherwise, let S and ¯S be the two sets of vertices in the two connected compo-
nents of T \{e}. If in G′, the cut (S;¯S) = ∅then G′ has no MST. Otherwise, let
e′ be of minimum length in (S;¯S). The new MST is (T \ {e}) ∪{e′}.
Prove the validity of this algorithm and compare its time complexity with
computing an MST anew.
Problem 2.7 Compute the number of trees that can be built on n, given labeled
vertices with unlabeled edges, in such a way that one speciﬁed vertex is of
degree k.
Problem 2.8 Let V = {1, 2, ..., n} and for each 1 ⩽i ⩽n d(i) is a positive
integer. Prove that if
n

i=1
d(i) = 2n −2,
then there exists a tree T(V,E) in which for every i, the degree of i is d(i). How
many such trees are there if the edges have no names?
Problem 2.9 What is the number of trees that one can build with n labeled
vertices and m = n −1 labeled edges? Prove that the number of trees that can
be built with m ⩾2 labeled edges (and no labels on the vertices) is (m+1)m−2.
Explain why the condition that m ⩾2 is necessary.4
Problem 2.10 A digraph which has no directed circuits is called a DAG
(directed acyclic graph). One wants an algorithm that checks whether a given
ﬁnite DAG, G(V,E), has a root.
One way to do this is ﬁrst to check that there is only one vertex r, such that
din(r) = 0, and then check if all vertices are reachable from r. Explain why
this is valid, and prove that the time complexity of such an algorithm is O(|E|).
4 This problem was inspired by an unpublished report of S. Golomb and A. Lempel, and a
comment made by A. Pnueli.

44
2 Trees
Problem 2.11 Given a ﬁnite digraph G(V,E). Describe an algorithm which
runs in time O(|V|) and checks whether G is a directed tree.
Problem 2.12 Prove that, if G is an inﬁnite undirected connected graph whose
vertices are of ﬁnite degrees, then every vertex of G is the start vertex of some
simple inﬁnite path.
Problem 2.13 Show that, if rotation or ﬂipping of tiles is allowed, then the
question of tiling the plane becomes trivial.
Problem 2.14 Consider the following (inﬁnite) set of tile families: For every
ordered pair of positive integers (i,j), there is a tile family with a label i−1 in
the West, i in the East, j−1 in the South, and j in the North. Prove that one can
tile the upper-right quadrant with this set of families, but not the whole plane.
Problem 2.15 Let T be an undirected tree with n vertices. We want to invest
O(n) time, labeling the graph in a way that will allow one to ﬁnd a (minimum)
path between any two vertices thatareofdistanceδapart, intimeO(δ). Describe
both a preparatory algorithm and an algorithm for ﬁnding the path once the two
vertices are speciﬁed.
Problem 2.16 A clique is a simple undirected graph such that for every two
vertices there is an edge connecting them. Let G(V,E) be a clique and T(V,E′)
a spanning tree of G. Prove that the complement of T (= (V,E′)) is either
connected or consists of one isolated vertex, while the remaining vertices form
a clique.
Problem 2.17 Let G(V,E) be an undirectedﬁnite connectedgraph,where each
edge e has a given length l(e) > 0 and s is a designated vertex. Also, let δ(v)
denote the distance from s to v.
(a) Explain why every vertex v ̸= s, has an incident edge u e v, such that
δ(v) = δ(u) + l(e).
(b) Show that if one such edge is chosen for each vertex v ̸= s, then the set of
these edges forms a spanning tree of G.
(c) Show that such a tree is not necessarily an MST.
Bibliography
[1] Prim, R.C., “Shortest Connection Networks and Some Generalizations,” Bell
System Tech. J., Vol. 36, 1957, pp. 1389–1401.
[2] Kruskal, J.B., “On the Shortest Spanning Subtree of a Graph and the Traveling
Salesman Problem,” Proc. of the Amer. Math. Society, Vol. 7, 1956, pp. 48–50.

2.6 Problems
45
[3] Cayley, A., “A Theorem on Trees,” Quart. J. Math., Vol. 23, pp. 376–378. Also in
Collected Papers, Vol. 13, Cambridge, 1897, pp. 26–28.
[4] Prüfer, H., “Neuer Beweise eines Satzes über Permutationen,” Arch. Math. Phys.,
Vol. 27, 1918, pp. 742–744.
[5] Moon, J.W., “Various Proofs of Cayley’s Formula for Counting Trees,” A Seminar
on Graph Theory, F. Harary (ed.), Holt, Rinehart and Winston, 1967, pp. 70–78.
[6] Berge, C., and A. Ghouila-Houri,
Programming, Games and Transportation
Networks, Wiley, 1965, Sec. 7.4.
[7] König, D.,
Theorie der endlichen und unendlichen Graphen, Liepzig, 1936.
Reprinted by Chelsea, 1950.
[8] Wang, H., “Proving Theorems by Pattern Recognition,”
Bell System Tech. J.,
Vol. 40, 1961, pp. 1–41.

3
Depth-First Search
3.1 DFS of Undirected Graphs
The depth-ﬁrst search (DFS) technique is a method of scanning a ﬁnite,
undirected graph. Since the publication of the papers of Hopcroft and Tar-
jan [4, 6], DFS has been widely recognized as a powerful technique for solving
variousgraph problems.However,the algorithmhas been known since the nine-
teenth century as a technique for threading mazes. See, for example, Lucas’
report of Trémaux’s work [5]. Another algorithm, which was suggested later
by Tarry [7], is just as good for threading mazes, and in fact, DFS is a special
case of it. But the additional structure of DFS is what makes the technique so
useful.
3.1.1 Trémaux’s Algorithm
Assume one is given a ﬁnite, connected graph G(V,E), which we will also refer
to as the maze. Starting in one of the vertices, one wants to “walk” along the
edges, from vertex to vertex, visit all vertices, and halt. We seek an algorithm
that will guarantee that the whole graph will be scanned without wandering
too long in the maze, and that the procedure will allow one to recognize when
the task is done. However, before one starts walking in the maze, one does not
know anything about its structure, and therefore, no preplanning is possible. So,
decisions about where to go next must be made one by one as one goes along.
We will use “markers,” which will be placed in the maze to help one to
recognize that one has returned to a place visited earlier and to make later
decisions on where to go next. Let us mark the passages, namely theconnections
of the edges to vertices. If the graph is presented by incidence lists, then we
can think of each of an edge’s two appearances in the incidence lists of its two
endpoints as its two passages. It sufﬁces to use two types of markers: F for the
ﬁrst passage used to enter the vertex, and E for any other passage used to leave
46

3.1 DFS of Undirected Graphs
47
Procedure TRÉMAUX(G,s)
1
v ←s
2
while there is an unmarked passage in v or v has a passage marked F do
3
if there is an unmarked passage to edge v e u, then do
4
mark the passage of e at v by E
5
if u has no marked passages then do
6
mark the passage of e at u by F
7
v ←u
8
else mark the passage of e at u by E
9
else (there is a passage in v marked F) do
10
use the passage marked F to move to the neighboring vertex u
11
v ←u
Algorithm 3.1: The Trémaux algorithm.
s
a
b
c
E
E
E
F
E
E
E
F
E
E
Figure 3.1: An example of running Trémaux’s algorithm.
the vertex. No marker is ever erased or changed. There is no use of memory
other than the markers on the passages and the stage of the algorithm one is has
reached; in other words, the moves are controlled by a ﬁnite state automaton.
As we shall prove later, the algorithm described in Algorithm 3.1 will terminate
in the original starting vertex s, after scanning each edge once in each direction.
Let us demonstratethe algorithmon the graph shown in Figure 3.1. The initial
value of v, the place where we are situated, or the center of activity, is s. All
passages are unlabeled.We choose one, mark it E and traverse the edge.Its other
endpoint is a (u = a). None of its passages are marked, therefore we mark the

48
3 Depth-First Search
passage through which a has been entered by F, the new center of activity is
a (v = a), and we are back in Line 2. Since a has two unmarked passages,
assume that we choose the one leading to b. The passage at a is marked E and
the one at b is marked F since b is new, etc. The complete excursion is shown
in Figure 3.1 by the dashed line.
Lemma 3.1 Trémaux’s algorithm never allows an edge to be traversed twice
in the same direction.
Proof:1 If a passage is used as to exit a vertex and enter an edge), then either it
is being marked E in the process, and thus the edge is never traversed again in
this direction, or the passage is already marked F. It remains to be shown that
no passage marked F is ever reused for entering the edge.
Let u e v be the ﬁrst edge to be traversed twice in the same direction, from
u to v. The passage of e, at u, must be labeled F. Since s has no passages
marked F, u ̸= s. Vertex u has been left d(u) + 1 times; once through each of
the passages marked E and twice through e. Thus, u must have been entered
d(u) + 1 times and some edge been used twice to enter u, before e is used for
the second time. A contradiction.
■
An immediate corollary of Lemma 3.1 is that the process described by Tré-
maux’s algorithm will always terminate. Clearly, it can only terminate in s,
since every other visited vertex has an F passage. Therefore, all we need to
prove is that, upon termination, the whole graph has been scanned.
Lemma 3.2 Upon termination of Trémaux’s algorithm, every edge of the graph
has been traversed once in each direction.
Proof: Let us state the proposition differently: For every vertex, all its incident
edges have been traversed in both directions.
First, consider the start vertex s. Since the algorithm has terminated, all the
incident edges of s have been traversed from s outward. Thus, s has been left
d(s) times, and since we end up in s, it has also been entered d(s) times.
However, by Lemma 3.1 no edge is traversed more than once in the same
direction. Therefore, every edge incident to s has been traversed once in each
direction. Let S be the set of vertices for which the statement that each of their
incident edges has been traversed once in each direction holds. Since s ∈S,
S ̸= ∅. Assume V ̸= S. By the connectivity of the graph there must be edges
1 The following terminology is used in the proof. A passage from a node u to an edge v e u is
used as an exit if the algorithm exits v via the passage. In such a case, the passage is used to
enter the edge. (G.E.)

3.1 DFS of Undirected Graphs
49
connecting vertices of S with V \ S. All these edges have been traversed once
in each direction. Let e be the ﬁrst edge to be traversed from a vertex v ∈S to
u ∈V \ S. Clearly, the passage of e, at u, is marked F. Since this passage has
been entered, all other passages of u must have been marked E. Thus, each of
u’s incident edges has been traversed outward. The search has not started in u
and has not ended in u. Therefore, u has been entered d(u) times, and each of
its incident edges has been traversed inward. A contradiction, since u belongs
in S.
■
Observe that the loop (Lines 2–10) is applied at most once for every passage,
and thenumberofcomputationalstepsin each application oftheloop isbounded
by a constant. Thus, the time complexity is O(|E|).
3.1.2 The Hopcroft-Tarjan Version of DFS
The Hopcroft and Tarjan version of DFS is essentially the same as Trémaux’s,
except that they number the vertices from 1 to n(= |V|) in the order in which
they are discovered. This is not necessary, as we have seen, for scanning the
graph, but the numbering is useful in applying the algorithm for more advanced
tasks. Let us denote the number assigned to vertex v by k(v). Also, instead of
marking passages, they mark edges as “used,” and instead of using the F mark to
indicate the edge through which the vertex was discovered and through which
it is left for the last time, let us record for each vertex v other than s the vertex
f(v) from which v has been discovered. Then f(v) is called the father of v; this
name will be justiﬁed later. DFS is described in Algorithm 3.2.
Since this algorithm is just a simple variation of the previous one, our proof
that the whole (connected) graph will be scanned, each edge once in each
direction, still applies. Here, in Line 11, if k(u) ̸= 0, then u is not a new vertex,
and v, the center of activity, does not change. This is equivalent to the scanning
of e from v to u and back to v, as was done in Trémaux’s version. Also, moving
our center of activity from v to f(v) (Line 16) correspondsto traversing the edge
v
f(v), in this direction. Thus, the whole algorithm is of time complexity
O(|E|), namely, linear in the size of the graph.
Now that we have applied DFS to a ﬁnite and connected G(V,E), let us
consider the set of edges E′, consisting of all edges of the form f(v)
v
through which new vertices have been discovered. Also, direct each such edge
from f(v) to v. The digraph (V,E′) is called the DFS tree. This name is justiﬁed
by the following Lemma:
Lemma 3.3 The digraph (V,E′) deﬁned above is a directed tree with root s.

50
3 Depth-First Search
Procedure DFS(G(V,E),s;k(·),f(·))
1
for every e ∈E mark e “new”
2
for every u ∈V do
3
k(u) ←0
4
f(u) ←NIL
5
v ←s
6
k(s) ←1
7
i ←2
8
while v has a new incident edge or f(v) ̸= NIL do
9
if v has a new incident edge v e u then do
10
mark e “old”
11
if k(u) = 0 (u is a new vertex) then do
12
f(u) ←v
13
k(u) ←i
14
i ←i + 1
15
v ←u
16
else v ←f(v)
Algorithm 3.2: DFS.
Proof: Since f(s) = NIL, din(s) = 0. For every other vertex v, din(v) = 1.
Now one can ﬁnd a directed path
s = v0 e1
−→v1 e2
−→v2 ··· vl = v
from s to any vertex v, where for every 1 ⩽i ⩽l, ei is the directed edge from
f(vi) to vi, by starting from v and tracing the path backwards. Since for every i,
f(vi) was discovered before vi, the directed path is simple. Also, every vertex,
other than s, has a deﬁned father, and therefore the only vertex in which this
backwards search can end is s.
By Theorem 2.5, Part (c), (V,E′) is a directed tree.
■
Clearly, if one ignores the edge directions, (V,E′) is a spanning tree of G.
In a directed tree, vertex u is called an ancestor of v, and v is called a
descendant of u if there is a directed path from u to v.
The following very useful lemma is due to Hopcroft and Tarjan [4, 6]:

3.1 DFS of Undirected Graphs
51
Lemma 3.4 Let (V,E′) be the undirected version of a DFS tree, T, of G(V,E).
If an edge a e b is in E \ E′, then either a is an ancestor of b or a is a
descendant of b in T.
Proof: Without loss of generality, assume that k(a) < k(b). In the DFS algo-
rithm, the center of activity (v in the algorithm) moves only along the edges of
the tree (V,E′). If b is not a descendant of a, and since a is discovered before
b, the center of activity must ﬁrst move from a to some ancestor of a before it
moves up to b. However, we backtrack from a (in Line 16 of Algorithm 3.2)
only when all a’s incident edges are “old.” This means that e is “old.” Thus, b
is already discovered. A contradiction.
■
Let us call the edges of the DFS tree tree edges, and all other edges of the
graph, back edges. The justiﬁcation for this name is in Lemma 3.4; every nontree
edge connects some vertex back to one of its ancestors.
Consider, as an example, the graph shown in Figure 3.2. We apply DFS to this
graph, starting with s = c, and assume that we discover vertices d, e, f, g, b, a,
in this order. The resulting vertex numbers, tree edges, and back edges are shown
in Figure 3.3, where the tree edges are shown as solid lines and are directed
from low to high, and the back edges are shown as dashed lines and are directed
from high to low. In both cases the direction of the edge indicates the direction
in which the edge was scanned ﬁrst. For tree edges this is the deﬁned direction,
and for back edges we can prove it as follows: Assume v e u is a back edge,
and u is an ancestor of v. The edge e could not have been scanned ﬁrst from u,
for if v was new at that time, then e would have been a tree edge, and if v has
already been discovered (after u), then the center of activity could have been in
u only if we have backtracked from v, and this means that e has already been
scanned from v.
a
b
c
d
e
f
g
e1
e2
e3
e5
e6
e8
e9
e4
e7
Figure 3.2: A graph to which DFS is applied.

52
3 Depth-First Search
c
1
d
2
e
3
b
6
f
4
a
7
g
5
e6
e7
e5
e3
e2
e1
e8
e9
e4
Figure 3.3: The DFS tree edges and back edges.
3.2 Algorithm for Nonseparable Components
A connected graph G(V,E) is said to have a separation vertex v (sometimes
also called an articulation point) if there exist vertices a and b, distinct from
v, such that every path connecting a and b passes through v. In this case, we
also say that v separates a from b. A graph that has a separation vertex is called
separable, and one that has none is called nonseparable.
Let V ′ ⊂V. The induced subgraph G′(V ′,E′) is called a nonseparable com-
ponent if G′ is nonseparable, and if for every V ′ ⊊V ′′ ⊂V the induced
subgraph G′′(V ′′,E′′) is separable. For example, in the graph shown in
Figure 3.2, the subsets {a,b}, {b,c,d} and {d,e,f,g} induce the nonsepara-
ble components of the graph. If a graph G(V,E) contains no separation vertex,
then clearly the whole G is a nonseparable component. However, if v is a
separation vertex, then V \ {v} can be partitioned into {V1, V2, ..., Vk} such
that V1 ∪V2 ∪··· ∪Vk = V \ {v}, and if i ̸= j, then Vi ∩Vj = ∅. Also, two
vertices a and b are in the same Vi if and only if there is a path connecting
them that does not include v. Thus, no nonseparable component can contain
vertices from more than one Vi. We can next consider each of the subgraphs
induced by Vi ∪{v} and continue to partition them into smaller parts, if they
are separable. Eventually, we end up with nonseparable parts. This shows that

3.2 Algorithm for Nonseparable Components
53
no two nonseparable components can share more than one vertex because each
such vertex is a separating vertex. Also, every simple circuit must lie entirely in
one nonseparable component. Now, let us discuss how DFS can help to detect
separating vertices.
Let the lowpoint of v, L(v), be the least number, k(u), of a vertex u that can
be reached from v by a, possible empty, directed path consisting of tree edges,
followed by at most one back edge. Clearly L(v) ⩽k(v), for we can use the
empty path from v to itself. Also, if a nonempty path is used, then its last edge
is a back edge, for a directed path of tree edges leads to vertices higher than v.
For example, in the graph of Figure 3.2, with the DFS as shown in Figure 3.3,
the lowpoints are as follows: L(a) = 7, L(b) = L(c) = L(d) = 1 and L(e) =
L(f) = L(g) = 2.
Lemma 3.5 Let G be a graph whose vertices have been numbered by DFS. If
u −→v is a tree edge, k(u) > 1, and L(v) ⩾k(u), then u is a separating vertex
of G.
Proof: Let S be the set of vertices on the path from the root r (k(r) = 1)
to u, including r but not including u, and let T be the set of vertices on the
subtree rooted at v, including v (i.e., all descendants of v, including v itself). By
Lemma 3.4, there cannot be any edge connecting a vertex of T with any vertex
of V \ (S ∪{u} ∪T). Also, if there is any edge connecting a vertex t ∈T with
a vertex s ∈S, then the edge t
s is a back edge and clearly k(s) < k(u).
Now, L(v) ⩽k(s), since one can take the tree edges from v to t, followed by
t
s. Thus, L(v) < k(u), contradicting the hypothesis. Thus, u separates the
S vertices from the T vertices and is, therefore, a separating vertex.
■
Lemma 3.6 Let G(V,E) be a graph whose vertices have been numbered by
DFS. If u is a separating vertex, and k(u) > 1, then there exists a tree edge
u −→v such that L(v) ⩾k(u).
Proof: Since u is a separating vertex, there is a partition of V \ {u} into
V1, V2, ... , Vm such that m ⩾2, and if i ̸= j, then all paths from a ver-
tex of Vi to a vertex of Vj, pass through u. Since k(u) > 1, the the search has
not started in u. Let us assume that it starts in r, and w.l.o.g. r ∈V1. The center
of activity of the DFS must pass through u. Let u −→v be the ﬁrst tree edge
for which v /∈V1. W.l.o.g. assume v ∈V2. Since there are no edges connecting
vertices of V2 with vertices of V \ (V2 ∪{u}), L(v) ⩾k(u).
■

54
3 Depth-First Search
Lemma 3.7 Let G(V,E) be a graph whose vertices have been numbered by
DFS, starting with r (k(r) = 1). Vertex r is a separating vertex if and only if
there are at least two tree edges out of r.
Proof: Assume r is a separating vertex. Let V1, V2, ... , Vm be a partition
of V \ {r} such that m ⩾2, and if i ̸= j, then all paths from a vertex of Vi to a
vertex of Vj, pass through u. Therefore,no path in the tree starting with r −→v,
v ∈Vi, can lead to a vertex of Vj where j ̸= i. Thus, there are at least two tree
edges out of r.
Now, assume r −→v1 and r −→v2 are two tree edges out of r. Let T be the
set of vertices in the subtree rooted at v1. By Lemma 3.4, there are no edges
connecting vertices of T with vertices of V \ (T ∪{r}). Thus, r separates T
from the rest of the graph, which is not empty, since it includes at least the
vertex v2.
■
Let C1, C2, ... ,Cm be the nonseparable components of a connected graph
G(V,E), and let s1, s2, ... ,sp be its separating vertices. Let us deﬁne ˜G( ˜V, ˜E),
the superstructure of G(V,E), as follows:
˜V = {s1, s2, ... ,sp} ∪{C1, C2, ... ,Cm},
˜E = {si
Cj | si is a vertex of Cj in G}.
By the observations we made in the beginning of the section, ˜G is a tree. By
Corollary 2.1, if m > 1, then there must be at least two leaves in ˜G. However,
the degree of a separating vertex si in ˜G is greater than 1. Thus, there are at
least two leaf components in G, each containing only one separating vertex.
By Lemma 3.2, the whole graph will be explored by the DFS. Now, assume
the search starts in a vertex r that is not a separating vertex. Even if it is in one of
the leaf components, eventually, we will enter another leaf component C, say,
via its separating vertex s and an edge s −→v. By Lemma 3.6, L(v) ⩾k(s),
and if L(v) is known when we backtrack from v to s, then by using Lemma 3.5,
we can detect that s is a separating vertex. Also, as far as the component C is
concerned, from the time C is entered via s −→v until it is entirely explored,
we can think of the algorithm as running on C alone, with s as the starting
vertex. Thus, by Lemma 3.7, there is only one tree edge from s to other vertices
of C, and all other vertices of C are descendants of v and are therefore explored
after v is discovered and before the backtrack from v to s. This suggests the
use of a stack (pushdown store) for producing the vertices of the component.
We store the vertices in the stack in the order that they are discovered. If on
backtracking from v to f(v), we discover that f(v) is a separating vertex, we

3.2 Algorithm for Nonseparable Components
55
read off all vertices from the top of the stack down to and including v. All these
vertices, plus f(v), (which is not removed at this point from the stack even if it
is the next on top) constitute a component. This, in effect, removes the leaf C
from the tree ˜G, and if its adjacent separating vertex s has now d(s) = 1, then we
may assume that it is removed too. The new superstructure is again a tree, and
the same process will repeat itself, detecting and trimming one leaf at a time
until only one component is left when the DFS terminates.
If the search starts in a separating vertex r, then all but the components con-
taining r are detected and produced as before. All components that contain r,
except the last one, are detected by Lemma 3.7: Each time we backtrack into r,
on r −→v, if r still has additional unexplored incident edges, then we conclude
that r is a separating vertex, and the vertices on the stack above, including v,
plus r, constitute a component.
Finally, when the search is about to end, since we are going to backtrack
to r, and r has no new incident edges, all vertices on the stack form the last
component, although no separating vertex is discovered at this point.
The remaining problem is that of computing L(v) in time; that is, its value
should be known by the time we backtrack from v. If v is a leaf of the DFS
tree, then L(v) is the least element in the following set: {k(u) | u = v or v
u is a back edge}. Let us assign L(v) = k(v) immediately when v is discovered,
and as each back edge v
u is explored,let us assign L(v) = min{L(v),k(u)}.
Clearly, by the time we backtrack from v, all the back edges have been explored,
and L(v) has the right value. If v is not a leaf of theDFS tree, then L(v) is the least
element in the following set: {k(u) | u = v or v
u is a back edge} ∪{L(u) |
v −→uis a tree edge}.When webacktrack from v,wehavealready backtracked
from all its sons earlier, and therefore already know their lowpoint. Thus, in
addition to what we do for a tree leaf, it sufﬁces to do the following: When we
backtrack from u to v = f(u), we assign L(v) = min{L(v),L(u)}.
Putting together the ideas described above leads to the algorithm represented
in Algorithm 3.3. In this representation, L(·) is the lowpoint function, and
S is a stack of vertices. The remaining variables are as in DFS. For the given
undirected, connected, and ﬁnite graphG(V,E), |V| > 1, thealgorithm produces
the set of separating vertices and a list of its nonseparable components, both of
which are assumed to be initially empty. Note that just before the last backtrack
into s, Lines 26–27 produce the last nonseparable component, which may be
the entire V if G has no separating vertices.
Although this algorithm is more complicated then the original DFS, its time
complexity is still O(|E|). This follows easily from the fact that each edge is
still scanned exactly once in each direction. The number of operations per edge

56
3 Depth-First Search
Procedure NONSEPARABLE(G(V,E),s;set of separating vertices,
list of nonseparable components)
1
for every e ∈E mark e “new”
2
for every u ∈V do
3
k(u) ←0
4
f(u) ←NIL
5
v ←s
6
k(s) ←1
7
i ←2
8
vacate S
9
push s into S
10
while v has a new incident edge or f(v) ̸= NIL do
11
if v has a new incident edge v e u then do
12
mark e “old”
13
if k(u) = 0 (u is a new vertex) then do
14
push u into S
15
f(u) ←v
16
k(u) ←i
17
L(u) ←i
18
i ←i + 1
19
v ←u
20
else (u is old) do
21
L(v) ←min{L(v),k(u)}
22
else (f(v) is deﬁned)
23
if L(v) ⩾k(f(v)) then do
24
if f(v) ̸= s or s has a new incident edge, then do
25
add f(v) to the set of separating vertices
26
pop vertices from S down to and including v
27
the set of popped vertices, with f(v), is an element
of the set of nonseparable components
28
else (L(v) < k(f(v))) then do
29
L(f(v)) ←min{L(f(v)),L(v)}
30
v ←f(v)
Algorithm 3.3: Using DFS to ﬁnd the separating vertices and
nonseparable components.

3.3 DFS on Directed Graphs
57
is bounded by a constant, except when a nonseparable component is produced.
Each vertex is pushed into S once, and popped once. Thus, the total time to
produce the components is O|V|.
3.3 DFS on Directed Graphs
Running DFS on digraphs is similar to runningit on undirected graphs. We start
scanning from a new vertex and will scan all vertices (and edges) that can be
reached from it via directed paths. A new scan is started from a new vertex, as
long as the previous search has left unscanned vertices. Thus, the fact that the
whole graph is scanned is trivial.
Assuming the given ﬁnite digraph is G(V,E), where |V|= n. Again, we assign
a number k(u) to every vertex u, where k : V 
→{1,2,...,n} is a bijection. If
a vertex u is ﬁrst discovered by scanning an edge v −→u, then we assign
f(u) = v. Here, too, v denotes the center of activity.
The algorithm is described in Algorithm 3.4. It is easy to see that the time
complexity is O(|V| + |E|).
Let us call a vertex v ripe if all its outgoing edges are old and the center of
activity is at v. When v is ripe, and if f(v) ̸= NIL, then we backtrack to f(v).
Otherwise, we return to Line 6 in Algorithm 3.4.
Let us denote by Tu the directed subtree whose root is u, and all vertices that
are discovered from the time u is discovered to the time u is ripe are in it.2 The
edges of Tu are of the form f(w) e
−→w, where both f(w) and w are in Tu, and
w has been discovered via e.
Lemma 3.8 If vertex w is new when u is discovered, and if at that time there is
a directed path from u to w such that all its intermediate vertices (and edges)
are new, then w is in Tu.
Proof: By contradiction. Assume that w is as in the premise of the Lemma, but
w is not in Tu. Let P be a directed path from u to w such that all its intermediate
vertices are new when u is discovered, and let b be the ﬁrst vertex on P that
does not belong to Tu. Let a e
−→b be the edge on P that enters b.
Since a ∈Tu, and it eventually becomes ripe, e must have been investigated,
as in Line 11 of Algorithm 3.4, and b has been discovered and belongs to Tu.
A contradiction.
■
2 Note that u is not necessarily a root of a search in the sense that it may not have been picked in
Line 7, and may belong to some Tv for v ̸= u.

58
3 Depth-First Search
Procedure Directed-DFS(G(V,E),s;k(·),f(·))
1
for every e ∈E mark e “new”
2
for every u ∈V do
3
k(u) ←0
4
f(u) ←NIL
5
i ←1
6
while there is a vertex u for which k(u) = 0 do
7
let v be such a vertex
8
k(v) ←i
9
i ←i + 1
10
while v has a new outgoing edge or f(v) ̸= NIL do
11
if v has a new outgoing edge v e
−→w, then do
12
mark e “old”
13
if k(w) = 0 (u is a new vertex) then do
14
f(w) ←v
15
k(w) ←i
16
i ←i + 1
17
v ←w
18
else v ←f(v)
Algorithm 3.4: DFS on a directed graph.
3.4 Strongly Connected Components of a Digraph
Let G(V,E) be a ﬁnite digraph. Let us deﬁne the relation ∼, a subset of V ×V,
in the following way: For x,y ∈V, x ∼y if there is a directed path from x to y
and also a directed path from y to x.
The relation ∼is easily seen to be reﬂexive, symmetric, and transitive. Thus,
it is an equivalence relation. An equivalence class of this relation is called a
strongly connected component or, in short, a strong component, and if there is
only one equivalence class, then G is said to be strongly connected.
The super-structure of G, ˜G( ˜V, ˜E) is a digraph constructed as follows:
• ˜V is the set of strong components of V.
• ˜E = {X e
−→Y | there exists an edge x e
−→y in E such that x ∈X and y ∈Y}.

3.4 Strongly Connected Components of a Digraph
59
Observe that ˜G is a DAG (directed acyclic graph). A strong component C is
called a source if there are no edges that enter C in ˜G. A sink component is
similarly deﬁned.
Given a digraph G(V,E), our purpose is to describe an efﬁcient algorithm for
ﬁnding its strong components. The ﬁrst linear-time algorithm that achieved this
goal was presented by Tarjan [6]. However, we present an algorithm attributed
to Kosaraju and Sharir [1], which is simpler to explain.3
The algorithm consists of three phases:
Phase 1: Run a DFS-A on G, in which vertices are numbered h : V 
→
{1,2, ... ,n} in the order in which they become ripe. This is described
in Algorithm 3.5.
Phase 2: Reverse the direction of all edges of G(V,E) to obtain GR(V,ER).
Phase 3: Run a DFS-B on GR, where each time a new search begins, it is
started in the new vertex v for which h(v) is maximum; the set of
vertices found in a search is declared to be a strong component of G.
This is described in Algorithm 3.6.
Observe that the strong components of GR are identical to those of G. Also,
the algorithm consists of three phases, each of time complexity O(|V| + |E|).
Thus, the whole algorithm is of time complexity O(|V| + |E|).
Lemma 3.9 The function h(·), produced by DFS-A, satisﬁes the following
condition: For every u ∈V,
h(u) = max
w∈Tu{h(w)} .
Proof: Follows from the fact that u is the last vertex in Tu to become ripe. ■
Lemma 3.10 Let C be a strong component of G, and let u be the vertex of
C for which h(u) is maximum. It follows that u is the ﬁrst vertex of C to be
discovered in DFS-A.
Proof: If a is the ﬁrst vertex of C to be discovered in DFS-A, then by
Lemma 3.8, u ∈Ta. By Lemma 3.9, h(a) ⩾h(u). Since h is a bijection,
and h(u) is maximum in C, it follows that a = u.
■
Corollary 3.1 If the premise of Lemma 3.10 holds, then all vertices of C are
in Tu.
3 The algorithm of Tarjan uses DFS once, while that of Kosaraju and Sharir uses two runs of DFS.

60
3 Depth-First Search
Procedure DFS-A(G(V,E);h(·))
1
for every e ∈E mark e “new”
2
for every u ∈V do
3
f(u) ←NIL
4
mark u “new”
5
i ←1
6
while there are new vertices do
7
let v be such a vertex
8
mark v “old”
9
while v has new outgoing edges or f(v) ̸= NIL do
10
if v has new outgoing edges, then do
11
let v e
−→w be a new edge
12
mark e “old”
13
if w is new then do
14
mark w “old”
15
f(w) ←v
16
v ←w
17
else (f(v) ̸= NIL) then do
18
h(v) ←i
19
i ←i + 1
20
v ←f(v)
21
h(v) ←i
22
i ←i + 1
Algorithm 3.5: DFS-A.
Lemma 3.11 Assume DFS-A was applied to G and vertex v was assigned the
highest value of h(v). The strong component C to which v belongs is a source
component.
Proof: By contradiction. Assume there is an edge a
e
−→b, such that a /∈C
and b ∈C.
Let r be the root, chosen in Line 7, of the search in which a is discovered.
Then r /∈C, for otherwise, there would be a directed path from C, which starts
in r, to a, to b, and thus, a belongs to C, contradicting the assumption that
a /∈C. We conclude that r ̸= v.

3.4 Strongly Connected Components of a Digraph
61
Procedure DFS-B(GR(V,ER),h(·);strong components of G)
1
for every e ∈ER mark e “new”
2
for every u ∈V do
3
f(u) ←NIL
4
mark u “new”
5
while there are new vertices do
6
let v be the new vertex for which h(v) is maximum
7
S ←{v}
8
mark v “old”
9
while v has new outgoing edges or f(v) ̸= NIL do
10
if v has new outgoing edges, then do
11
let v e
−→w be a new edge
12
mark e “old”
13
if w is new then do
14
mark w “old”
15
f(w) ←v
16
S ←S ∪{w}
17
v ←w
18
else (f(v) ̸= NIL) then do
19
v ←f(v)
20
print: “The set S is a strong component”
Algorithm 3.6: DFS-B.
Observe that the last vertexto serve as a root of a search is the vertex for which
h(·) is maximum. Thus, v is a root of a search. Also, Tr is constructed before v
is discovered. But when r is chosen to be a root of a search, all vertices on the
path from r to a, to b, to v, are new. Thus, by Lemma 3.8, v ∈Tr, contradicting
the fact that v is a root of a new search.
■
Since the component C, containing the vertex v for which h(v) is maximum,
is a source component of G, it is a sink component of GR. It follows that in
DFS-B, since the root of the ﬁrst search is v, all vertices of C, and none else,
are discovered in this search, and indeed, this set S is declared to be a strong
component.

62
3 Depth-First Search
Procedure TARRY(G,s)
1
v ←s
2
while there is an unmarked passage in v or v has a passage marked F do
3
if there is an unmarked passage to edge v e u then do
4
mark the passage of e at v by E
5
if u has no marked passages then do
6
mark the passage of e at u by F
7
v ←u
8
else (there is a passage in v marked F) do
9
use the passage marked F to move to the neighboring vertex u
10
v ←u
Algorithm 3.7: The Tarry algorithm.
Now, assume we remove from G all vertices of the ﬁrst declared component
and their incident (outgoing)edges to form a directed subgraphG′. The remain-
ing directed subforest, of the original DFS-A forest of G, and the values of h(·)
for the remaining vertices are legitimate in the sense that one can run DFS-A
on G′ to yield exactly this forest and assignments of h(·). It follows that the
next declared strong component is valid too. By induction, all declared strong
components are valid as well.
3.5 Problems
Problem 3.1 Tarry’s algorithm [7] is like Trémaux’s, with the following
change. Upon reaching an old vertex u, one moves the center of activity to
that vertex anyway, instead of insisting on marking the passage through which
one has just reached u by E and not moving the center of activity to u. Tarry’s
algorithmis described in Algorithm3.7. Provethat Tarry’s algorithmterminates
after all edges of G have been traversed, once in each direction.
Problem 3.2 Consider the set of edges which upon termination of Tarry’s
algorithm (see Problem 3.1) have one endpoint marked E and the other marked
F. Also, assume these edges are now directed from E to F.
(i) Prove that this set of edges form a directed spanning tree of G, with root s.
(ii) DoesastatementlikethatofLemma3.4hold in thiscase?Proveordisprove.

3.5 Problems
63
Problem 3.3 Fraenkel [2, 3] showed that the number of edge traversals can
sometimes be reduced, in comparison to the Tarry algorithm (see Problem 3.1),
if the use of a two-way counter is allowed. Each time a new vertex is entered,
the counter is incremented.When it is realized that all incident edges of a vertex
have been traversed at least in one direction, the counter is decremented. If the
counter reaches the start value, the search is terminated. (One can return to s
via the passages marked F.)
Write an algorithm that realizes this idea. (Hint: An additional mark that
temporarily marks the passages used to reenter a vertex is used.) Prove the
validity of your algorithm. Show that forsomegraphs the algorithm will traverse
each edgeexactly onceforothers;thesavingsdependson thechoiceofpassages.
Yet there are graphs for which the algorithm can save only one traversal, even
if we do not insist on returning to s.
Problem 3.4 Assume that G is drawn in the plane in such a way that no two
edges cross. Show how Trémaux’s algorithm can be modiﬁed in such a way
that the whole scanning path never crosses itself.
Problem 3.5 In an undirected graph G, a set of vertices K is called a clique if
every two vertices of K are connected by an edge. Prove that, in the spanning
(directed) tree resulting from running DFS on a ﬁnite and connected G, all
vertices of a clique appear on one directed path. Do they necessarily appear
consecutively on the path? Justify your answer.
Problem 3.6 Prove or disprove the following claim: If C is a simple circuit in
an undirected, ﬁnite, and connected graph G to which DFS is applied, then all
vertices of C appear on one directed path of the (directed) DFS tree. Does you
answer change if C is an induced circuit? (A circuit C is an induced circuit in
G if G does not contain an edge between two nonadjacent vertices in C.)
Problem 3.7 Prove that if C is a directed circuit of a ﬁnite digraph to which
DFS is applied and v is a vertex on C for which k(v) is minimum, then v is a
root of a subtree of the resulting directed forest, and all vertices of C are in this
subtree.
Problem 3.8 An edge e of a connected undirected graph G is called a bridge if
the deletion of e destroys G’s connectivity. Describe an algorithm to compute
all bridges of a given ﬁnite G. (Hint: There are two ways to solve the problem.
In the ﬁrst solution, one modiﬁes the graph and uses the algorithm for ﬁnding
the separating vertices. In the second, one modiﬁes the algorithm and applies
the modiﬁed algorithm to G.)

64
3 Depth-First Search
Problem 3.9 (This problem was suggested by Silvio Micali.) Let G be a
connected graph.
• Prove that a vertex u ̸= s is a separating vertex of G if and only if, upon
termination of DFS on G, there is a tree edge u −→v for which there is no
back edge x −→y such that x is a descendant of v and y is a proper ancestor
of u.
• Describe an algorithm, of time complexity O(|E|), which detects the sepa-
rating vertices of G without numbering the vertices and, therefore, without
the use of lowpoint. (Hint: For every back edge x −→y, mark all tree edges
on the path from y to x; proceed from x to y until an already marked edge
is encountered or the edge tree outgoing from y is reached. The latter edge
is not marked. The back edges are considered by rescanning G again, while
remembering which edges are tree edges and which are back edges, only that
in the second scan the back edges are processed ﬁrst. When all this is over,
v ̸= s is a separating vertex if and only if there is an unmarked tree edge out
of v. Also note that a third scan can be used to produce the nonseparable
components.)
Problem 3.10 (This problem was suggested by Alessandro Tescari.) Show that
the algorithm for nonseparable components can be simpliﬁed by adding a new
vertex r and a new edge r
s, and starting the search at r. Do we still need
Lemma 3.7?
Bibliography
[1] J.E. Hopcroft, A.V. Aho, and J.D. Ullman. Data Structures and Algorithms.
Addison-Wesley, 1983.
[2] A.S. Fraenkel. “Economic traversal of labyrinths.” Math. Mag., 43(1):125–130,
1970.
[3] A.S. Fraenkel. “Economic traversal of labyrinths (correction).” Math. Mag., 44,
1971.
[4] J. Hopcroft and R. Tarjan. “Algorithm 447: Efﬁcient algorithms for graph
manipulation.” Comm. of the ACM, 16:372–378, 1973.
[5] E. Lucas. Récreations Mathématiques. Paris, 1882.
[6] R. Tarjan. “Depth-ﬁrst search and linear graph algorithms.” SIAM J. on Computing,
1:146–160, 1972.
[7] G. Tarry. “Le problème des labyrinthes.” Nouvelles Annales de Math., 14:187,
1895.

4
Ordered Trees
4.1 Uniquely Decipherable Codes
Let Σ = {0,1,...,σ −1}. We call Σ an alphabet and its elements are called
letters; the number of letters in Σ is σ. (Except for this numerical use of σ,
the “numerical” value of the letters is ignored; they are just “meaningless”
characters. We use the numerals just because they are convenient characters.)
A ﬁnite sequence a1a2 ···al, where ai is a letter, is called a word whose length
is l. We denote the length of a word w by l(w). A set of (nonempty and
distinct) words is called a code. For example, the code {102,21,00} consists of
three code-words: one code-word of length 3 and two code-words of length 2;
the alphabet is {0,1,2} and consists of three letters. Such an alphabet is called
“ternary”.
Let c1,c2,...,ck be code-words. The message c1c2 ···ck is the word resulting
from the concatenation of the code-word c1 with c2, and so on. For example, if
c1 = 00, c2 = 21, and c3 = 00, then c1c2c3 = 002100.
A code C over Σ (i.e., the code-words of C consist of letters in Σ) is said to
be uniquely decipherable (UD) if every message constructed from code-words
of C can be broken down into code-words of C in only one way. For example,
the code {01,0,10} is not UD because the message 010 can be parsed in two
ways: 0,10 and 01,0.
Our ﬁrst goal is to describe a test for deciding whether a given code C is UD.
This test is an improvement on a test of Sardinas and Patterson [1] and can be
found in Gallager’s book [2].
If s,p, and w are words, and ps = w, then p is is called a preﬁx of w and s
is called a sufﬁx of w. We say that a word w is nonempty if l(w) > 0.
A nonempty word t is called a tail if there exist two messages c1c2 ···cm and
c′
1c′
2 ···c′
n with the following properties:
65

66
4 Ordered Trees
(1) ci,1 ⩽i ⩽m, and c′
j,1 ⩽j ⩽n are code-words, and c1 ̸= c′
1;
(2) t is a sufﬁx of c′
n;
(3) c1c2 ···cmt = c′
1c′
2 ···c′
n.
Lemma 4.1 A code C is UD if and only if no tail is a code-word.
Proof: If a code-word c is a tail then, by deﬁnition, there exist two messages
c1c2 ···cm and c′
1c′
2 ···c′
n that satisfy c1c2 ···cmc = c′
1c′
2 ···c′
n, while c1 ̸= c′
1.
Thus, there are two different ways to parse this message, and C is not UD.
If C is not UD, then there exist messages that can be parsed in more than
one way. Let µ be such an ambiguous message, whose length is minimum: µ =
c1c2 ···ck = c′
1c′
2 ···c′
n; i.e. all the ci-s and cj-s are code-words and c1 ̸= c′
1.
Now, without loss of generality we can assume that ck is a sufﬁx of c′
n (or
change sides). Thus, ck is a tail.
■
The followingalgorithm generates all the tails. If a code is a tail, the algorithm
terminates with a negative answer.
Algorithm for UD:
(1) For every two code-words, ci and cj (i ̸= j), do the following:
a. If ci = cj, halt; C is not UD.
b. If for some word s, either cis = cj or ci = cjs, put s in the set of tails.
(2) For every tail t and every code-word c do the following:
a. If t = c, halt; C is not UD.
b. If some word s, either ts = c or cs = t, put s in the set of tails.
(3) Halt; C is UD.
Clearly, in Step (1), the words declared to be tails are indeed tails. In step
(2), since t is already known to be a tail, there exist code-words c1,c2,...,cm
and c′
1,c′
2,...,c′
n such that c1c2 ···cmt = c′
1c′
2 ···c′
n. Now, if ts = c, then
c1c2 ···cmc = c′
1c′
2 ···c′
ns, and therefore s is a tail; and if cs = t, then
c1c2 ···cmcs = c′
1c′
2 ···c′
n and s is a tail.
Next, if the algorithm halts in (3), we want to show that all the tails have been
produced. Once this is established, it is easy to see that the conclusion that C
is UD follows; each tail has been checked, in Line (2a.), whether it is equal to
a code-word, and no such equality has been found. By lemma 4.1, the code C
is UD.
For every t let m(t) = c1c2 ···cm be a shortest message such that
c1c2 ···cmt = c′
1c′
2 ···c′
n and t is a sufﬁx of c′
n. We prove by induction on

4.1 Uniquely Decipherable Codes
67
the length of m(t) that t is produced. If m(t) = 1, then t is produced by ((1)b.),
since m = n = 1.
Now assume that all tails p for which m(p) < m(t) have been produced.
Since t is a sufﬁx of c′
n, let p denote the word such that pt = c′
n. Therefore,
c1c2 ···cm = c′
1c′
2 ···c′
n−1p, and p is a tail.
If p = cm, then cmt = c′
n, and t is produced in Step (1).
If p is a sufﬁx of cm, then p is a tail. Also, m(p) is shorter then m(t). By
the inductive hypothesis, p has been produced. When Step (2b) is applied to
pt = c′
n (with p as a tail and c′
n as a code-word), the tail t is produced.
If cm is a sufﬁx of p, then cmt is a sufﬁx of c′
n, and therefore, cmt is a
tail. Also, m(cmt) = c1c2 ···cm−1 and is shorter than m(t). By the inductive
hypothesis, cmt has been produced. When Step (2b) is applied to the tail cmt
and code-word cm, the tail t is produced.
This proves that the algorithm halts with the right answer.
Let the code consist of n words, and l be the maximum length of a code-word.
Step (1) takes at most O(n2 · l) elementary operations. The number of tails is
at most O(n · l). Thus, Step (2) takes at most O(n2l2) elementary operations.
Therefore,thewholealgorithmisoftimecomplexity O(n2l2).Otheralgorithms
of the same complexity can be found in [3] and [4]; these tests are extendible
to test for additional properties [5, 6, 7].
Theorem 4.1 Let C = {c1,c2,...,cn} be a UD code over an alphabet of σ
letters. If li = l(ci), i = 1,2,...,n, then
n

i=1
σ−li ⩽1.
(4.1)
The left-hand side of 4.1 is called the characteristic sum of C; clearly, it
characterizes the vector (l1,l2,...,ln), rather than C. The inequality 4.1is called
the characteristic sum condition. The theoremwas ﬁrst proved by McMillan [8].
The following proof is due to Karush [9].
Proof: Let e be a positive integer
 n

i=1
σ−li
e
=
n

i1=1
n

i2=1
···
n

ie=1
σ−(li1+li2+···+lie ).
There is a unique term, on the right-hand side, for each of the ne messages of
e code-words.Let us denoteby N(e,j) the numberof messages of e code-words

68
4 Ordered Trees
whose length is j. It follows that
n

i1=1
n

i2=1
···
n

ie=1
σ−(li1+li2+···+lie) =
eˆl

j=e
N(e,j) · σ−j,
where ˆl is the maximum length of a code-word. Since C is UD, no two
messages can be equal. Thus, N(e,j) ⩽σj. We now have
eˆl

j=e
N(e,j) · σ−j ⩽
eˆl

j=e
σj · σ−j ⩽e · ˆl.
We conclude that for all e ⩾1,
 n

i=1
σ−li
e
⩽e · ˆl.
This implies 4.1.
■
A code C is said to be preﬁx if no code-word is a preﬁx of another. For
example, the code {00,10,11,100,110} is not preﬁx, since 10 is a preﬁx of
100; the code {00,10,11,010,011} is a preﬁx. A preﬁx code has no tails and
is therefore UD. In fact, it is very easy to parse the messages: Reading the
messages from left to right, as soon as we read a code-word, we know that
it is the ﬁrst code-word of the message, since it cannot be the beginning of
another code-word. Therefore, in most applications, preﬁx codes are used. The
following theorem, due to Kraft [10], in a sense shows us that we do not need
nonpreﬁx codes.
Theorem 4.2 If the vector of integers, (l1,l2,...,ln), satisﬁes
n

i=1
σ−li ⩽1,
(4.2)
then there exists a preﬁx code C = {c1,c2,...,cn} over the alphabet of σ letters
such that li = l(ci).
Proof: Let λ1 < λ2 < ··· < λm be integers such that each li is equal to one of
the λj-s and each λj is equal to at least one of the li-s. Let kj be the number of

4.2 Positional Trees and Huffman’s Optimization Problem
69
lj-s that are equal to λj. We have to show that there exists a preﬁx code C such
that the number of code-words of length λj is kj.
Clearly, 4.2 implies that
m

j=1
kjσ−λj ⩽1
(4.3)
We prove by induction on r that for every 1 ⩽r ⩽m there exists a preﬁx
code Cr such that, for every 1 ⩽j ⩽r, the number of its code-words of
length λj is kj.
First assume that r = 1. Inequality 4.3 implies that k1σ−λ1 ⩽1, or k1 ⩽σλ1.
Since there are σλ1 distinct words of length λ1, we can assign any k1 of them
to constitute C1.
Now, assume Cr exists. If r < m then 4.3 implies that
r+1

j=1
kjσ−λj ⩽1.
Multiplying both sides by σλr+1 yields
r+1

j=1
kjσλr+1−λj ⩽σλr+1,
which is equivalent to
kr+1 ⩽σλr+1 −
r

j=1
kjσλr+1−λj.
(4.4)
The number of distinct words of length λr+1, a preﬁx of which of length λj
is a code-word in Cr, equals kj · σλr+1−λj. Thus, 4.4 implies that among the
σλr+1 words of length λr+1, there are at least kr+1 words, none of which has a
preﬁx in Cr. The enlarged set of code-words is Cr+1.
■
This proof suggests an algorithm for the construction of a code with a given
vector of code-word length. We return later to the question of preﬁx code
construction, but ﬁrst we introduce positional trees.
4.2 Positional Trees and Huffman’s Optimization Problem
A positional σ-tree (or when σ is known, a positional tree) is a directed tree
with the following property: Each edge out of a vertex v is associated with one

70
4 Ordered Trees
00
0
01
011
100
101
10
1
λ
0
0
0
1
1
1
0
1
Figure 4.1: A positional 2-tree.
of the letters of the alphabet Σ = {0,1,...,σ −1}; different edges, out of v, are
associated with different letters. It follows that the number of edges out of a
vertex is at most σ, but may be less; in fact, a leaf has none.
We associate with each vertex v the word consisting of the sequence of letters
associated with the edges on the path from the root r to v. For example, consider
the binary tree (positional 2-tree) of Figure 4.1, where the associated word is
written in each vertex. (λ denotes the empty word.)
Clearly, the set of words associated with the leaves of a positional tree is a pre-
ﬁx code. Also, every preﬁx code canbe described by a positional tree inthis way.
The level of a vertex v of a tree is the length of the directed path from the root
to v; it is equal to the length of the word associated with v.
Our next goal is to describe a construction of an optimum code in a sense that
we discuss shortly. It is described here as a communication problem, as it was
viewed by Huffman [11], who solved it. In the next section, we shall describe
one more application of this optimization technique.
Assume that words over a source alphabet of n letters have to be transmitted
over a channel that can transfer one letter of the alphabet Σ = {0,1,...,σ−1} at
a time, and σ < n. We want to construct a code over Σ with n code-words and
associate a code-word with each source letter. A word over the source alphabet
is translated into a message over the code, by concatenating the code-words that
correspond to the source letters in the same order as they appear in the source
word. This message can now be transmitted through the channel. Clearly, the
code must be UD.

4.2 Positional Trees and Huffman’s Optimization Problem
71
Assume further that the source letters have given probabilities p1,p2,...pn of
appearance and that the choice of the next letter in the source word is indepen-
dent of its previous letters. If the vector of code-word lengths is (l1,l2,...,ln),
then the average code-word length, l, is given by
¯l =
n

i=1
pili .
(4.5)
We want to ﬁnd a code for which ¯lis minimum in order to minimize the expected
length of the message.
Since the code must be UD, by Theorem 4.1, the vector of code-word lengths
must satisfy the characteristic sum condition. This implies, by Theorem 4.2,
that a preﬁx code with the same vector of code-word lengths exists. Therefore,
in seeking an optimumcode, for which ¯l is minimum,we may restrict our search
to preﬁx codes. In fact, all we have to do is ﬁnd a vector of code-word lengths
for which ¯l is minimum, among the vectors that satisfy the characteristic sum
condition.
First, let us assume that p1 ⩾p2 ⩾··· ⩾pn. This is easily achieved by sorting
the probabilities. We ﬁrst demonstrate Huffman’s construction for the binary
case (σ = 2). Assume the probabilities are 0.6, 0.2, 0.05, 0.05, 0.03, 0.03, 0.03,
0.01. We write this list as our top row (see Figure 4.2). We add the last (and
therefore least) two numbers, and insert the sum in the proper place to maintain
the nonincreasingorder. We repeat this operation until we get a vector with only
two probabilities. Now, we assign each probability a word-length 1, and start
working our way back up by assigning each of the probabilities of the previous
step its length in the present step, if it is not one of the last two, and each of the
two last probabilities of the previous step is assigned a length that is larger by
one than the length assigned to their sum in the present step.
Once the vector of code-word lengths is found, a preﬁx code can be assigned
to it by the technique of the proof of Theorem 4.2. (An efﬁcient implementation
is discussed in Problem 4.6.) Alternatively, the back-up procedure can produce
a preﬁx code directly. Instead of assigning lengths to the last two probabilities,
we assign the two words of length one: 0 and 1. As we back up from a present
step in which a word is already assigned to each probability, to the previous
step, the rule is as follows: All but the last two probabilities of the previous
step are assigned the same words as in the present step. The last two probabil-
ities are assigned c0 and c1, where c is the word assigned to their sum in the
present step.

72
4 Ordered Trees
0.6
1
2
4
4
5
5
5
5
1
2
4
4
4
5
5
1
2
4
4
4
4
1
2
3
4
4
1
2
3
3
1
2
1
1
2
0.2
0.05
0.05
0.03
0.03
0.03
0.01
0.6
0.2
0.05
0.05
0.04
0.03
0.03
0.6
0.2
0.06
0.05
0.05
0.04
0.6
0.2
0.09
0.06
0.05
0.6
0.2
0.11
0.09
0.6
0.2
0.2
0.6
0.4
Figure 4.2: A demonstration of Huffman’s construction for the binary case.
In the general case, when σ ⩾2, we add in each step the last d probabilities
of the present vector of probabilities; if n is the number of probabilities of this
vector then d is given by
1 < d ⩽σ and n ≡d
mod (σ −1)
(4.6)
After the ﬁrst step, the length of the vector, n′, satisﬁes n′ ≡1 mod (σ−1),
and will be equal to one, mod (σ −1), from there on. The reason for this
rule is that we should end up with exactly σ probabilities, each to be assigned
length 1. Now, σ ≡1 mod (σ−1), and since in each ordinary step the number
of probabilities is reduced by σ −1, we want n ≡1 mod (σ −1). In case this
condition is not satisﬁed by the given n, we correct it in the ﬁrst step as is
done by our rule. Our next goal is to prove that this indeed leads to an optimum
assignment of a vector of code-word lengths.
Lemma 4.2 If C = {c1,c2,...,cn} is an optimum preﬁx code for the probabili-
ties p1,p2,...,pn, then pi > pj implies that l(ci) ⩽l(cj).
Proof: Assume l(ci) > l(cj). Make the following switch: Assign ci to proba-
bility pj, and cj to pi; all other assignments remain unchanged.Let ˜l denote the
average code-word length of the new assignment, while ¯l denotes the previous

4.2 Positional Trees and Huffman’s Optimization Problem
73
one. By (4.5), we have
¯l −l = [pi · l(ci) + pj · l(cj)] −[pi · l(cj) + pj · l(ci)]
= (pi −pj)(l(ci) −l(cj)) > 0
contradicting the assumption that ¯l is minimum.
■
Lemma 4.3 There exists an optimum preﬁx code for the probabilities p1 ⩾
p2 ⩾··· ⩾pn such that the positional tree that represents it has the following
properties:
(1) All the internal vertices of the tree, except possibly one internal vertex v,
have exactly σ sons.
(2) Vertex v has 1 < ρ ⩽σ sons, where n ≡ρ mod (σ −1).
(3) Vertex v of ((1)) is on the lowest level that contains internal vertices, and
its sons are assigned to pn−ρ+1,pn−ρ+2,...,pn.
Proof: Let T be a positional tree representing an optimum preﬁx code. If there
exists an internal vertex u that is not on the lowest level of T containing internal
vertices and it has less than σ sons, then we can perform the following change
in T: Remove one of the leaves of T from its lowest level and assign to the
probability a new son of u. The resulting tree, and therefore its corresponding
preﬁx code, has a smaller average code-word length. A contradiction. Thus, we
conclude that no such internal vertex u exists.
If there are internal vertices on the lowest level of internal vertices that have
less than σ sons, choose one of them, say v. Now eliminate sons from v and
attach their probabilities to new sons of the others, so that their number of sons
is σ. Clearly, such a change does not change the average length, and the tree
remains optimum. If, before ﬁlling in all the missing sons, v has no more sons,
we can use v as a leaf and assign to it one of the probabilities from the lowest
level, thus creating a new tree that is better than T. A contradiction. Thus, we
never run out of sons of v to be transferred to other lacking internal vertices
on the same level. Also, when this process ends, v is the only lacking internal
vertex (proving (1)), and its number of remaining sons must be greater than
one, or its son can be removed and its probability attached to v. This proves
that the number of sons of v, ρ, satisﬁes 1 < ρ ⩽σ.
If v’s ρ sons are removed, the new tree has n′ = n −ρ + 1 leaves and is full
(i.e., every internal vertex has exactly σ sons). In such a tree, the number of
leaves, n′, satisﬁes n′ ≡1, mod (σ−1). This is easily proved by induction on

74
4 Ordered Trees
the numberof internal vertices. Thus, n−ρ+1 ≡1 mod (σ−1), and therefore
n ≡ρ mod (σ −1), proving (2).
We have already shown that v is on the lowest level of T that contains internal
vertices, and the number of its sons is ρ. By Lemma 4.2, we know that the least
ρ probabilities are assigned to leaves of the lowest level of T. If they are not
sons of v, we can exchange sons of v with sons of other internal vertices on
this level, to bring all the least probabilities to v without changing the average
length.
■
For a given alphabet size σ and probabilities p1 ⩾p2 ⩾··· ⩾pn, let
θσ(p1,p2,...,pn) be the set of all σ-ary positional trees with n leaves, assigned
with the probabilities p1,p2,...,pn in such a way that pn−d+1,pn−d+2,...,pn
(see Equation 4.6) are assigned, in this order, to the ﬁrst d sons of a vertex v,
which has no other sons. By Lemma 4.3, θσ(p1,p2,...,pn) contains at least
one optimum tree. Thus, we may restrict our search for an optimum tree to
θσ(p1,p2,...,pn).
Lemma 4.4 There is a one to one correspondence between θσ(p1,p2,...,pn)
and the set of σ-ary positional trees, with n −d + 1 leaves assigned with
p1,p2,...,pn−d,p′, where p′ =
n

i=n−d+1
pi. The average word-length ¯l of the
preﬁx code, represented by a tree T of θσ(p1,p2,...,pn), and the average code-
word-length ¯l′ of the preﬁx code represented by the tree T ′, which corresponds
to T, satisfy
¯l = ¯l′ + p′.
(4.7)
Proof: The tree T ′ which corresponds to T is achieved as follows: Let v be the
father of the leaves assigned pn−d+1,pn−d+2,...,pn. Remove all the sons of
v and assign p′ to it.
It is easy to see that two different trees T1 and T2 in θσ(p1,p2,...,pn) will
yield two different trees T ′
1 and T ′
2, and that every σ-ary tree T ′ with n−d+1
leaves assigned p1,p2,...,pn−d,p′, is the image of some T; establishing the
correspondence.
Letli denotetheleveloftheleafassignedpiinT.Clearly ln−d+1 = ln−d+2 =
··· = ln. Thus,
¯l =
n−d

i=1
pi · li + ln ·
n

i=n−d+1
pi=
n−d

i=1
pi · li + ln · p′
=
n−d

i=1
pi · li+(ln −1) · p′ + p′ = ¯l′ + p′.
■

4.3 Application of the Huffman Tree to Sort-by-Merge Techniques
75
Lemma 4.4 suggests a recursive approach to ﬁnd an optimum T. For ¯l to be
minimum, ¯l′ must be minimum. Thus, let us ﬁrst ﬁnd an optimum T ′, and then
ﬁnd T by attaching d sons to the vertex of T ′ assigned p′; these d sons are
assigned pn−d+1,pn−d+2,...,pn. This is exactly what is done in Huffman’s
procedure, thus proving its validity.
It is easy to implement Huffman’s algorithm in time complexity O(n2). First,
we sort the probabilities, and after each addition, the resulting probability is
inserted in a proper place. Each such insertion takes at most O(n) steps, and
the number of insertions is ⌈(n−σ)/(σ−1)⌉. Thus, the whole forward process
is of time complexity O(n2). The back up process is O(n) if pointers are left
in the forward process to indicate the probabilities of which it is composed.
However,thetimecomplexity can bereducedtoO(nlogn).Onewayofdoing
it is the following: First sort the probabilities. This can be done in O(nlogn)
steps [14]. The sorted probabilities are put on a queue S1 in a non-increasing
order from left to right. A second queue, S2, initially empty, is used too. In the
general step, we repeatedly take the least probability of the two (or one, if one
of the queues is empty) appearing at the right hand side ends of the two queues,
and add up d of them. The result, p′, is inserted at the left hand side end of
S2. The process ends when after adding d probabilities both queues are empty.
This adding process and the back up are O(n). Thus, the whole algorithm is
O(nlogn).
Theconstruction ofan optimumpreﬁx code,when thecostofthelettersarenot
equal is discussed in Reference [12]; the case of alphabetic preﬁx codes, where
the words must maintain lexicographically the order of the given probabilities,
is discussed in Reference [13]. These references give additional references to
previous work.
4.3 Application of the Huffman Tree to Sort-by-Merge Techniques
Assume that we have n items, and there is an order deﬁned between them. For
ease of presentation, let us assume that the items are the integers 1,2,...,n,
and the order is “less than.” Assume that we want to organize the numbers in
nondecreasing order, where initially they are put in L lists, A1,A2,...,AL. Each
Ai is assumed to be ordered already. Our method of building larger lists from
smaller ones is as follows: Let B1,B2,...,Bm be any m existing lists. We read
the ﬁrst, and therefore least, number in each of the lists, take the least number
among them away from its list, and put it as the ﬁrst number of the merged list.
The list from which we took the ﬁrst number is now shorter by one. We repeat
this operation on the same m lists until they merge into one. Clearly, some of

76
4 Ordered Trees
1
5
4
3
2
5
2
4
3
1
3
4
1
Figure 4.3: An example of a sort-by-merge described by a positional tree.
the lists become empty before others, but since this depends on the structure of
the lists, we only know that the general step of ﬁnding the least number among
m numbers (or less) and its transfer to a new list is repeated b1 +b2 +···+bm
times, where bi is the number of numbers in Bi.
The number m is dictated by our equipment or decided upon in some other
way. However, we shall assume that its value is ﬁxed and predetermined. In
fact, in most cases m = 2.
The whole procedure can be described by a positional tree. Consider the
example shown in Figure 4.3, where m = 2. First, we merge the list ⟨3⟩with
⟨1,4⟩. Next, we merge ⟨2,5⟩with ⟨1,3,4⟩. The original lists, A1,A2,...,AL,
correspond to the leaves of the tree. The number of transfers can be computed
as follows: Let ai be the number of numbers in Ai, and li be the level of the
list Ai in the tree. The number of elementary merge operations is then
L

i=1
ai · li.
(4.8)
Burge [15] observed that the attempt to ﬁnd a positional m-ary that mini-
mizes (4.8) is similar to that of the minimum average word-length problem
solved by Huffman. The fact that the Huffman construction is in terms of prob-
abilities does not matter, since the fact that p1 + p2 + ··· + pL = 1 is never
used in the construction or its validity proof. Let us demonstrate the implied
procedure by the following example:

4.4 Catalan Numbers
77
9
8
8
7
6
5
5
4
6
6
3
3
1
2
2
2
2
2
2
2
2
2
2
2
10
9
8
8
7
6
6
6
5
5
1
1
2
2
2
2
2
2
2
2
22
10
9
8
8
7
6
1
1
1
2
2
2
2
29
22
10
9
1
1
1
1
Figure 4.4: An example of sort-by-merge for L = 12 and m = 4.
29
22
10
70
9
8
8
7
6
6
6
5
5
4
3
3
Figure 4.5: The positional tree describing the example in Figure 4.4.
Assume L = 12 and m = 4; the bi’s are given in nonincreasing order:
9,8,8,7,6,6,6,5,5,4,3,3. Since L ≡0(mod 3), according to (4.6), d = 3.
Thus, in the ﬁrst step we merge the last three lists to form a list of length 10,
which is now put in the ﬁrst place (see Figure 4.4). From there on, we merge
each time the four lists of least length. The whole merge procedure is described
in the tree shown in Figure 4.5.
4.4 Catalan Numbers
The set of well-formed sequences of parentheses is deﬁned by the following
recursive deﬁnition:
(1) The empty sequence is well formed.
(2) If A and B are well-formed sequences, so is AB (the concatenation of A
and B).
(3) If A is well formed, so is (A).

78
4 Ordered Trees
(4) There are no other well-formed sequences.
For example, (()(())) is well formed; (()))(() is not.
Lemma 4.5 A sequence of (left and right) parentheses is well formed if and
only if it contains an even number of parentheses, half of which are left, and the
other half, right. Also as we read the sequence from left to right, the number of
right parentheses never exceeds the number of left parentheses.
Proof: First let us prove the “only if” part. Since the construction of every well-
formed sequence starts with no parentheses (the empty sequence), and each time
we add on parentheses (Step 3) there is one left and one right, it is clear that
there are n left parentheses and n right parentheses. Now, assume that for every
well-formed sequence of m left and m right parentheses where m < n, it is
true that as we read it from left to right the number of right parentheses never
exceeds the number of left parentheses. If the last step in the construction of
our sequence was (2), then since A is a well-formed sequence, as we read from
left to right, as long as we still read A, the condition is satisﬁed. When we are
between A and B, the count of left and right parentheses equalizes. From there
on, the balance of left and right is safe, since B is well formed and contains less
than n parentheses. If the last step in the construction of our sequence was (3),
since A satisﬁes the condition, so does (A).
Now, we shall prove the “if” part, again by induction on the number of paren-
theses. (Here, as before, the basis of the induction is trivial.) Assume that the
statement holds for all sequences of m left and m right parentheses, if m < n,
and we are given a sequence of n left and n right parentheses that satisﬁes
the condition. Clearly, if after reading 2m symbols of it from left to right, the
number of left and right parentheses is equal, and if m < n, then this subse-
quence, A, by the inductive hypothesis, is well formed. Now, the remainder
of our sequence, B, must satisfy the condition, too, and again by the inductive
hypothesis is well formed. Thus, by Step (2), AB is well formed. If there is no
such nonempty subsequence A, which leaves a nonempty B, then as we read
from left to right, the number of right parentheses, after reading one symbol
and before reading the whole sequence, is strictly less then the number of left
parentheses. Thus, if we delete the ﬁrst symbol, which is a “(”, and the last,
which is a “)”, the remainder sequence, A, still satisﬁes the condition, and
by the inductive hypothesis, is well formed. By Step (3) our sequence is well
formed too.
■

4.4 Catalan Numbers
79
We shall now show a one-to-one correspondence between the non-well-
formed sequences of n left and n right parentheses, and all sequences of n−1
left parentheses and n + 1 right parentheses.
Let p1p2 ···p2n be a sequence of n left and n right parentheses that is not
well formed. By Lemma 4.5, there is a preﬁx of it that contains more right
parentheses than left parenthesis. Let j be the least integer such that the number
of right parentheses exceeds the number of left parentheses in the subsequence
p1p2 ···pj. Clearly, the number of right parentheses is then one larger than the
number of left parentheses, or j is not the least index to satisfy the condition.
Now, invert all pi’s for i > j from left parentheses to right parentheses, and from
right parentheses to left parentheses. Clearly, the number of left parentheses is
now n −1, and the number of right parentheses is now n + 1.
Conversely, given any sequence p1p2 ···p2n of n −1 left parentheses and
n+1 right parentheses, let j be the ﬁrst index such that p1p2 ···pj contains one
right parenthesis more than left parentheses.If we now invertall the parentheses
in the section pj+1pj+2 ···p2n from left to right and from right to left, we get
a sequence of n left and n right parentheses which is not well formed. This
transformation is the inverse of the one in the previous paragraph. Thus, the
one-to-one correspondence is established.
The number of sequences of n −1 left and n + 1 right parentheses is
 2n
n −1

,
for we can choose the places for the left parentheses, and the remaining places
will have right parentheses. Thus, the number of well-formed sequences of
length n is
2n
n

−
 2n
n −1

=
1
1 + n
2n
n

.
(4.9)
These numbers are called Catalan numbers.
An ordered tree is a directed tree such that for each internal vertex there is
a deﬁned order of its sons. Clearly, every positional tree is ordered, but the
converse does not hold: In the case of ordered trees there are no predetermined
“potential” sons; only the order of the sons counts, not their position, and there
is no limit on the number of sons.
An ordered forest is a sequence of ordered trees. We usually draw a forest
with all the roots on one horizontal line. The sons of a vertex are drawn from
left to right in their given order. For example, the forest shown in Figure 4.6
consists of three ordered trees whose roots are A, B, and C.

80
4 Ordered Trees
B
A
C
((( )( )))
(( )( )( ))
((( )( ))( ))
(( )( ))
(( )( ))
( )
( )
( )
( )
( )
( )
( )
( )
Figure 4.6: An example of three ordered trees.
There is a natural correspondence between well-formed sequences of n pairs
of parentheses and ordered forests of n vertices. Let us label each leaf with
the sequence (). Every vertex whose sons are labeled w1,w2,...,ws is labeled
with the concatenation (w1w2 ···ws); clearly, the order of the labels is in the
order of the sons. Finally, once the roots are labeled x1,x2,...,xr, the sequence
corresponding to the forest is the concatenation x1x2 ···xn. For example, the
sequencecorrespondingto the forest of Figure 4.6 is ((()())())(()()())((()())).The
inverse transformation clearly exists, and thus the one-to-one correspondence
is established. Therefore, the number of ordered forests of n vertices is given
by (4.9).
We now describe a one-to-one correspondence between ordered forests and
positional binary trees. The leftmost root of the forest is the root of the binary
tree. The leftmost son of the vertex in the forest is the left son of the vertex in
the binary tree. The next brother on the right, or, in the case of a root, the next
root on the right is the right son in the binary tree. For example, see Figure 4.7,
where an ordered forest and its corresponding binary tree are drawn. Again, it
is clear that this is a one-to-one correspondence, and therefore the number of
positional binary trees with n vertices is given by (4.9).
There is yet another combinatorial enumeration that is directly related to
these.
A stack is a storage device that can be described as follows. Suppose that n
cars travel on a narrow one-way street where no passing is possible. This leads
into a narrow two-way street on which the cars can park or back up to enter
another narrow one-way street (see Figure 4.8). Our problem is to ﬁnd how may

4.4 Catalan Numbers
81
A
D
E
J
K
J
K
E
D
A
F
B
G
H
L
M
I
C
F
G
H
B
C
I
L
M
Figure 4.7: An ordered forest and its corresponding binary tree.
Figure 4.8: An example of a stack of cars.
permutations of the cars can be realized from input to output if we assume that
the cars enter in the natural order.
The order of operations in the stack is fully described by the sequence of
drive-in and drive-out operations. There is no need to specify which car drives
in, for it must be the ﬁrst one on the leading-in present queue; also, the only one
that can drive out is the top one in the stack. If we denote a drive-in operation
by “(”, and a drive-out operation by “)”, the whole procedure is described by a
well-formed sequence of n pairs of parentheses.
The sequence must be well formed,by Lemma 4.5, since the number of drive-
out operations can never exceed the number of drive-in operations. Also, every
well-formed sequence of n pairs of parentheses deﬁnes a realizable sequence
of operations, since, again by Lemma 4.5, a drive-out is never instructed when

82
4 Ordered Trees
the stack is empty. Also, different sequences yield different permutations.Thus,
the number of permutations on n cars realizable by a stack is given by (4.9).
Let us now consider the problem of ﬁnding the number of full binary trees.
Denote the number of leaves of a binary tree T by L(T), and the number of
internal vertices by I(T). It is easy to prove, by induction on the number of
leaves, that L(T) = I(T) + 1. Also, if all leaves of T are removed, the resulting
tree of I(T) vertices is a positional binary tree T ′. Clearly, different T-s will
yield different T ′-s, and one can reconstruct T from T ′ by attaching two leaves
to each leaf of T ′, and one leaf (son) to each vertex that in T ′ has only one son.
Thus, the number of full binary trees of n vertices is equal to the number of
positional binary trees of (n −1)/2 vertices. By (4.9) this number is
2
n + 1
n −1
n−1
2

.
4.5 Problems
Problem 4.1 Prove the following theorem: A position between two letters in
a message m over a UD code C is a separation between two code-words if
and only if both the preﬁx and the sufﬁx of m up to this position are messages
over C.
Problem 4.2 Use the result of Problem 4.1 to construct an efﬁcient algorithm
for parsing a message m over a UD code C in order to ﬁnd the words which
compose m. (Hint: scan m from left to right, and mark all the positions such
that the preﬁx m up to them is a message. Repeat from right to left to mark
positions corresponding to sufﬁxes that are messages.)
Problem 4.3 Test the following codes for UD:
(1) {00,10,11,100,110},
(2) {1,10,001,0010,00000,100001},
(3) {1,00,101,010}.
Problem 4.4 Construct a preﬁx binary code, with minimum average
code-word length, which consists of ten words whose probabilities are
0.2,0.18,0.12,0.1,0.1,0.08,0.06,0.06,0.06,0.04. Repeat the construction for
σ = 3 and 4.
Problem 4.5 Prove that, if a preﬁx code corresponds to a full positional tree,
then its characteristic sum is equal to 1.

4.5 Problems
83
Problem 4.6 Prove that, if the word-length vector (l1,l2,...,ln) satisﬁes the
characteristic sum condition, and if l1 ⩽l2 ⩽··· ⩽ln, then there exists a
positional tree with n leaves whose levels are the given li’s, and the order of
the leaves from left to right is as in the vector.
Problem 4.7 A code is called exhaustive if every word over the alphabet is the
beginning of some message over the code. Prove the following:
(1) If a code is preﬁx, and its characteristic sum is 1, then the code is exhaustive.
(2) If a code is UD and exhaustive, then it is preﬁx and its characteristic sum
is 1.
Problem 4.8 Construct the ordered forest, the positional binary tree and the
permutation through a stack that corresponds to the following well-formed
sequence of ten pairs of parentheses:
(()(()()))((()()())).
Problem 4.9 A direct method for computing the number of positional binary
trees of n vertices through the use of a generating function goes as follows: Let
bn be the number of trees of n vertices. Deﬁne b0 = 1 and deﬁne the function
B(x) = b0 + b1x + b2x2 + ··· .
(1) Prove that bn = b0 · bn−1 + ··· + bn−1 · b0.
(2) Prove that xB2(x) −B(x) + 1 = 0.
(3) Use the formula
(1 + a)
1
2 = 1 +
1
2
1!a +
1
2( 1
2 −1)
2!
a2 +
1
2( 1
2 −1)( 1
2 −2)
3!
a3 + ···
to prove that
bn =
1
n + 1
2n
n

.
Bibliography
[1] Sardinas, A. A., and Patterson, G. W., “A Necessary and Sufﬁcient Condition for
the Unique Decomposition of Coded Messages,” IRE Convention Record, Part 8,
1953, pp. 104–108.
[2] Gallager, R. G., Information Theory and Reliable Communication, John Wiley,
1968. Problem 3.4, page 512.

84
4 Ordered Trees
[3] Levenshtein, V. I., “Certain Properties of Code Systems,” Dokl. Akad. Nauk,
SSSR, Vol. 140, No. 6, Oct. 1961, pp. 1274–1277. English translation: Soviet
Physics, “Doklady,” Vol. 6, April 1962, pp. 858–860.
[4] Even, S., “Test for Unique Decipherability,” IEEE Trans. on Infor. Th., Vol. IT-9,
No. 2, April 1963, pp. 109–112.
[5] Levenshtein, V. I., “Self-Adaptive Automata for Coding Messages,” Dokl. Akad.
Nauk, SSSR, Vol 140, Dec. 1961, pp. 1320–1323. English translation: Soviet
Physics, “Doklady,” Vol. 6, June 1962, pp. 1042–1045.
[6] Markov, Al. A., “On Alphabet Coding,” Dokl. Akad. Nauk, SSSR, Vol. 139, July
1961, pp. 560–561. English translation: Soviet Pysics, “Doklady,” Vol. 6, Jan.
1962, pp. 553–554.
[7] Even, S., “Test for Synchronizability of Finite Automata and Variable Length
Codes,” IEEE Trans. on Infor. Th., Vol. IT-10, No. 3, July 1964, pp. 185–189.
[8] McMillan, B., “Two Inequalities Implied by Unique Decipherability,” IRE Tran.
on Infor. Th., Vol. IT-2, 1956, pp. 115–116.
[9] Karush, J., “A Simple Proof of an Inequality of McMillan,” IRE Tran. on Infor.
Th., Vol. IT-7, 1961, pp. 118.
[10] Kraft, L. G., “A Device for Quantizing, Grouping and Coding Amplitude
Modulated Pulses,” M.S. Thesis, Dept. of E.E., M.I.T.
[11] Huffman, D. A.,“AMethodfortheConstructionofMinimumRedundancyCodes,”
Proc. IRE, Vol. 40, No. 10, 1952, pp. 1098–1101.
[12] Perl, Y., Garey, M. R., and Even, S., “Efﬁcient Generation of Optimal Preﬁx Code:
Equiprobable Words Using Unequal Cost Letters,” J.ACM, Vol. 22, No. 2, April
1975, pp. 202–214.
[13] Itai, A., “Optimal Alphabetic Trees,” SIAM J. Comput., Vol. 5, No. 1, March 1976,
pp. 9–18.
[14] Knuth, D. E., The Art of Computer Programming, Vol. 3: Sorting and Searching,
Addison-Wesley, 1973.
[15] Burge, W. H., “Sorting, Trees, and Measures of Order,” Infor. and Control, Vol. 1,
1958, pp. 181–197.

5
Flow in Networks
5.1 Introduction
A network N(G,s,t,c) consists of the following data:
• A ﬁnite digraph G(V,E) with no self-loops and no parallel edges.1
• Two vertices s and t are speciﬁed; s is called the source; and t, the sink.2
• The capacity function, c : E →R+. The positive real number, c(e), is called
the capacity of edge e.
For every vertex v ∈V, let α(v) denote the set of edges that enter v in G.
Similarly, let β(v) denote the set of edges that emanate from v.
A ﬂow function, f : E →R, is an assignment of a real number f(e) to each
edge e such that the following two conditions hold:
The Edge Rule. For every edge e ∈E, 0 ⩽f(e) ⩽c(e).
The Vertex Rule. For every vertex v ∈V \ {s,t},

e∈α(v)
f(e) =

e∈β(v)
f(e).
The total ﬂow F, of f in N, is deﬁned by
F =

e∈α(t)
f(e) −

e∈β(t)
f(e) .
(5.1)
Namely, F is the net sum of ﬂow into the sink.
1 Self-loops are useless in this context, and parallel edges can be replaced with one edge whose
capacity is the sum of the capacities of the parallel edges.
2 The source is not necessarily a graphical source; i.e., it may have incoming edges. Similarly,
the sink is not necessarily a graphical sink.
85

86
5 Flow in Networks
In the next two sections, we shall discuss methods for computing a ﬂow
function f for which F is maximum.
Given a set S ⊂V, let ¯S = V \S. In the following, we shall discuss sets S, such
that s ∈S and t ∈¯S. Also, (S;¯S) denotes the set of edges which are directed
from a vertex in S to a vertex in ¯S; this set of edges is called a forward cut. The
set (¯S;S) is similarly deﬁned, and is called a backward cut. The union of (S;¯S)
and (¯S;S) is called the cut deﬁned by S.
By deﬁnition, the total ﬂow F is measured at the sink. Our purpose is to show
that F can be measured at any cut.
Lemma 5.1 For every {s} ⊂S ⊂(V \ {t}) and every ﬂow function f of total
ﬂow F, the following holds:
F =

e∈(S; ¯S)
f(e) −

e∈( ¯S;S)
f(e) .
(5.2)
Proof: By the vertex rule, for every v ∈(V \ {s,t}),
0 =

e∈α(v)
f(e) −

e∈β(v)
f(e) .
(5.3)
Also, consider again Equation 5.1:
F =

e∈α(t)
f(e) −

e∈β(t)
f(e) .
Now, add the equations, as in Equation 5.3, for every v ∈(¯S \ {t}), as well as
Equation 5.1. The aggregate equation has F on the l.h.s.3
In order to see what happens on the r.h.s., consider an edge x e
−→y. If both
x and y belong to S then f(e) does not appear on the r.h.s. of the aggregate
equation at all, in agreement with Equation 5.2. If both x and y belong to ¯S
then f(e) appears twice on the r.h.s. of the aggregate equation; once positively,
in the equation for y, and once negatively, in the equation for x. Thus, it is
canceled out in the summation, again in agreement with Equation 5.2. If x ∈S
and y ∈¯S then f(e) appears on the r.h.s. of the aggregate equation, as part of
Equation 5.3 for y, positively, and in no equation for other vertices included
in the summation. In this case e ∈(S;¯S), and again we have agreement with
Equation 5.2. Finally, if x ∈¯S and y ∈S, f(e) appears negatively on the r.h.s.
3 Left-hand side.

5.2 The Algorithm of Ford and Fulkerson
87
of the aggregate equation, as part of Equation 5.3 for x, and again this agrees
with Equation 5.2, since e ∈(¯S;S).
■
Let us denoteby c(S) the capacity of the cut determinedby S, which is deﬁned
as follows:
c(S) =

e∈(S; ¯S)
c(e) .
(5.4)
Lemma 5.2 For every ﬂow function f, with total ﬂow F, and every {s} ⊂S ⊂
(V \ {t}), the following inequality holds:
F ⩽c(S) .
(5.5)
Proof: By Lemma 5.1,
F =

e∈(S; ¯S)
f(e) −

e∈( ¯S;S)
f(e) .
By the edge rule, for every edge e, 0 ⩽f(e) ⩽c(e). Thus,
F ⩽

e∈(S; ¯S)
c(e) −0 .
Finally, by Equation 5.4, F ⩽c(S).
■
The following is a very important corollary of Lemma 5.2. It allows us to
detect that a given total ﬂow F is maximum, and the capacity of a given cut,
deﬁned by S, is minimum.
Corollary 5.1 If F and S satisfy Equation 5.5 by equality, then F is maximum
and the cut deﬁned by S is of minimum capacity.
5.2 The Algorithm of Ford and Fulkerson
Ford and Fulkerson [7] suggested the use of augmentingpaths to changea given
ﬂow function f in order to increase the total ﬂow. A procedure for ﬁnding an
augmenting path, if one exists, is used. If an augmenting path is found, then it is
used to increase the total ﬂow. If no augmenting path is found, then the present
ﬂow is declared maximum and the process terminates.
If the direction of the edges is ignored, an augmenting path P is a simple path
from s to t. It is used to add a quantity ∆> 0 to the total ﬂow by pushing along

88
5 Flow in Networks
P ∆additional units of ﬂow. To do this, if an edge e of P is directed in the
direction from s to t, c(e) ⩾f(e)+∆must hold. But if e of P is directed in the
opposite direction, we must be able to reduce its ﬂow from f(e) to f(e) −∆.
Thus, f(e) ⩾∆must hold.
In attempt to ﬁnd an augmenting path for a given ﬂow, a labeling procedure is
used. We ﬁrst label s. Then, as long as there is an unlabeled vertex v for which
an augmenting path from s to v is found, v is labeled. If t is labeled, then an
augmenting path has been found and the labeling process is terminated.
Every vertex v is assigned a label λ(v), which is one of the following:
• λ(s) = ∞; only vertex s gets this label, and it is the only label s gets.
• λ(v) = NIL; in this case, we say that v has no label.
• λ(v) = (e,σ), where e is the edge through which v has been assigned its label,
σ = + if e has been used forwardly and σ = −if e has been used backwardly.
An edge u e v is said to be useful from u to v if λ(u) ̸= NIL, λ(v) = NIL
and one of the following conditions holds:
• u e
−→v and f(e) < c(e). In this case, we say that e is forwardly useful.
• v e
−→u and f(e) > 0. In this case, we say that e is backwardly useful.
Algorithm 5.1 describes the labeling procedure, named LABEL. The input
of LABEL consists of the network N and the present ﬂow function f. λ(·) is
deﬁned for all vertices. The procedure may modify these labels. Thus, λ(·) is
both an input and output. The output of LABEL includes a function ∆(·), which
is deﬁned on a subset of E, and its value is a positive real number.
Procedure LABEL(N,f,λ,∆)
1
while there is an edge u e v which is useful from u to v and
λ(t) = NIL do
2
if e is forwardly useful then do
3
λ(v) ←(e,+)
4
∆(e) ←c(e) −f(e)
5
if e is backwardly useful then do
6
λ(v) ←(e,−)
7
∆(e) ←f(e)
Algorithm 5.1: The labeling procedure.

5.2 The Algorithm of Ford and Fulkerson
89
Procedure AUGMENT(G,λ,∆,f)
1
empty Q
2
∆←∞
3
v ←t (v is called the center of activity)
4
while v ̸= s do
5
put λ(v) into Q
6
let λ(v) = (e,σ)
7
∆←min{∆,∆(e)}
8
let e be u e v
9
v ←u
10
while Q is not empty do
11
remove the ﬁrst label, (e,σ), from Q
12
if σ = + then do
13
f(e) ←f(e) + ∆
14
else (σ = −) do
15
f(e) ←f(e) −∆
Algorithm 5.2: The augmenting procedure.
If λ(t) ̸= NIL when LABEL is terminated, then an augmenting path exists.
The augmenting path is found in the AUGMENT procedure and used to change
f and thus, increment F. This is presented in Algorithm 5.2.
The input to AUGMENT consists of G(V,E), λ(·) and ∆(·). The ﬂow function
f is both an input and an output. A queue Q of vertex labels is used, as well as
a real ∆– these variables are internal.
The Ford and Fulkerson procedure is described in Algorithm 5.3 and uses
LABEL and AUGMENT as subroutines. Initially, f is any legitimate ﬂow in
the network N, and in the absence of better ideas, one can use f(e) = 0 for
every edge e. Thus, the input is N and f is both an input and an output.
Note that, if for an edge e ∈α(s), initially f(e) = 0, then f(e) is neverchanged.
The same is true for an edge e ∈β(t).
As an example, consider the networks shown in Figure 5.1. Next to each edge
e we write c(e),f(e), in this order. We assume a zero initial ﬂow everywhere.
A ﬁrst wave of label propagation might be as follows: s is labeled, e2 is used to
label c, e6 is used to label d, e4 is used to label a, e3 used to label b and ﬁnally,

90
5 Flow in Networks
Procedure FORD-FULKERSON(N,f)
1
for every v ∈V do
2
λ(v) ←NIL
3
λ(s) ←∞
4
call LABEL(N,f,λ,∆)
5
while λ(t) ̸= NIL do
6
call AUGMENT(G,λ,∆,f)
7
for every v ∈V \ {s} do
8
λ(v) ←NIL
9
call LABEL(N,f,λ,∆)
10
print “The present ﬂow f is maximum”
Algorithm 5.3: The Ford-Fulkerson procedure.
s
a
b
c
d
t
15,0
12,0
7,0
4,0
10,0
5,0
10,0
3,0
e1
e3
e6
e4
e7
e8
e2
e5
Figure 5.1: An example of a network.
e7 is used to label t. The path is
s e2
−→c e6
−→d e4
−→a e3
−→b e7
−→t ,
∆= 4, and the new ﬂow is shown in Figure 5.2. The second augmenting path
may be
s e1
−→a e3
−→b e5
−→c e6
−→d e8
−→t ,
∆= 3 and the new ﬂow is shown in Figure 5.3. The third augmenting path
may be
s e1
−→a e3
−→b e7
−→t ,
∆= 3 and the new ﬂow is shown in Figure 5.4. Up to now only forward labeling

5.2 The Algorithm of Ford and Fulkerson
91
s
a
b
c
d
t
15,0
12,4
7,4
4,4
10,4
5,4
10,0
3,0
e1
e3
e6
e4
e5
e7
e8
e2
Figure 5.2: The example after the ﬁrst augmenting path.
s
a
b
c
d
t
15,3
12,7
7,4
4,4
10,7
5,4
10,3
3,3
e1
e3
e6
e4
e5
e7
e8
e2
Figure 5.3: The example after the second augmenting path.
s
a
b
c
d
t
15,6
12,10
7,7
4,4
10,7
10,3
3,3
5,4
e1
e3
e6
e4
e5
e7
e8
e2
Figure 5.4: The example after the third augmenting path.
has been used. In the next application of LABEL, we can still label vertex a
via e1; and vertex b, via e3, both of which are forward labeling, but no further
forward labeling exists. However, e4 is useful backwardly, and through it one

92
5 Flow in Networks
s
a
b
c
d
t
12,10
7,7
10,7
10,7
5,0
3,3
15,10
4,4
e1
e3
e6
e4
e5
e7
e8
e2
Figure 5.5: The example after the fourth augmenting path.
can label d. Now one can use e8 to label t. The augmenting path is
s e1
−→a e4
←−d e8
−→t ,
∆= 4, and the new ﬂow is shown in Figure 5.5. Now, the total ﬂow, F, is equal
14. The next application of LABEL does not reach t. The set of labeled vertices
S is {s,a,b} and the forward cut, (S;¯S) consists of the edges e2, e5, and e7.
All of these edges are saturated; that is, f(e) = c(e). The backward cut, (¯S;S),
consists of one edge, e4, and its ﬂow is 0. Thus, F = c(S), and by Corollary 5.1,
F is maximum and c(S) is minimum.
It is easy to see that the ﬂow produced by the algorithm remains legitimate
throughout. The deﬁnitions of ∆(·) and ∆guaranty that forwardly used edges
will not be overﬂowed, that is, f(e) ⩽c(e), and that backward edges will not
be underﬂowed, that is, f(e) ⩾0. Also, since ∆units of ﬂow are are pushed
from s to t on a path, the incoming ﬂow will remain equal to the outgoing ﬂow
in every vertex v ∈V \ {s,t}.
Assuming the Ford and Fulkerson procedure halts, the last labeling process
has not reached t. As above, let S be the set of vertices labeled in the last
application of the labeling process. If e ∈(S;¯S), then f(e) = c(e), since e is not
forwardly useful. If e ∈(¯S,S), then f(e) = 0, since e is not backwardly useful.
By Lemma 5.1,
F =

e∈(S; ¯S)
f(e) −

e∈( ¯S;S)
f(e) =

e∈(S; ¯S)
c(e) −

e∈( ¯S;S)
0 = c(S) .
By Corollary 5.1, F is maximum and c(S) is minimum.
The question of whether the Ford and Fulkerson procedure will always halt
remains to be discussed. Note ﬁrst a very important property of the procedure:

5.2 The Algorithm of Ford and Fulkerson
93
If the initial ﬂow is integral, for example, zero everywhere, and if all capacities
are integers, then the algorithm never introduces fractions. The algorithm adds
and subtracts, but it never divides. Also, if t is labeled, the resulting augmenting
path is used to increase the total ﬂow by at least one unit. Since there is an upper
bound on the total ﬂow (any cut), the process must terminate.
Ford and Fulkerson showed that their procedure may fail if the capacities are
allowed to be irrational numbers. Their counterexample ([7, p. 21]) displays an
inﬁnite sequence of ﬂow augmentations.The ﬂow converges(in inﬁnitely many
steps) to a value which is one-fourth of the maximum total ﬂow. We shall not
present their example here; it is fairly complex, and as the reader will shortly
discover, it is not as important any more.
One could have arguedthat, for all practical purposes,we may assume that the
algorithm is sure to halt. This follows from the fact that our computations are
usually through a ﬁxed radix (decimal, binary, etc.) number representation with
a bound on the number of digits used; in other words, all ﬁgures are multiples
of a ﬁxed quantum and the termination proof works here as it does for integers.
However, a simple example shows the weakness of this argument. Consider the
network shown in Figure 5.6.
Assume that M is a very large integer. If the algorithm starts with f(e) = 0
for every e, and alternately uses
s −→a −→b −→t
and
s −→b ←−a −→t
a
b
s
t
M
M
M
M
1
Figure 5.6: An example which demonstrating the possibility that the
Ford and Fulkerson procedure may be inefﬁcient.

94
5 Flow in Networks
as augmenting paths, it will use 2M augmentations before F = 2M is achieved.
This is exponential time in terms of the length of the input data, since it takes
only ⌈log2(M + 1)⌉bits to represent M.
Edmonds and Karp [4] were ﬁrst to overcome this problem. They showed
that if Breadth-First Search (BFS) is used in the labeling algorithm and thus,
every augmentingpath is a shortest one, the algorithm terminates in O(|V|·|E|2)
time, regardless of the capacities. (Here, of course, we assume that our com-
puter can handle, in one step, any real number.) In the next section we shall
present the more advanced work of Dinitz (see [2]); his algorithm has time
complexity O(|V|2 · |E|). A signiﬁcantly more efﬁcient algorithm was pre-
sented by Goldberg and Tarjan [9], but this algorithm is not described in this
book.
The existence of the algorithm of Edmonds and Karp, orthat of Dinitz, assures
that if one proceeds according to a proper strategy in the labeling procedure,
the algorithm is guaranteed to halt (in polynomial time). When it does, the total
ﬂow is maximum, and the cut indicated is minimum, thus providing the max
ﬂow min-cut theorem:
Theorem 5.1 Every network has a maximum total ﬂow which is equal to the
capacity of a cut for which the capacity is minimum.
5.3 The Dinitz Algorithm
As in the Ford and Fulkerson algorithm, the Dinitz algorithm4 starts with some
legitimate ﬂow function f and improves it. When no improvement is possible
the algorithm halts, and the total ﬂow, F, is maximum.
Given a network N(G(V,E),s,t,c) and a ﬂow function f, let us deﬁne the
secondary network N′(G′(V,E′),s,t,c′) as follows:
• For every e ∈E such that f(e) < c(e), e ∈E′. Also, c′(e) = c(e)−f(e). (Such
an edge is called forwardly useful and c′(e) is called its residual capacity.)
• For every e ∈E, a e
−→b such that f(e) > 0, there is an edge b e′
−→a in E′.
Also, c′(e′) = f(e). (Such an edge is called backwardly useful.)
Note that if 0 < f(e) < c(e), then e gives rise to two antiparallel edges in
G′, since it is useful both in the forward and the backward directions. Thus,
|E| ⩽|E′| ⩽2|E|.
4 In [3], Yeﬁm Dinitz tells the story of his algorithm, and the differences between his version and
the one presented here (G.E.).

5.3 The Dinitz Algorithm
95
Procedure LAYER(N′,Vi, vertex labels,T)
1
T ←∅
2
while there is an edge u −→v in N′, u ∈Vi and v is not labeled, do
3
T ←T ∪{v}
4
label v
Algorithm 5.4: Finding the next layer in the BFS of N′.
Before we go into a detailed description of the Dinitz algorithm, here is an
abstract of its structure:
The Dinitz algorithm proceeds in phases. In each phase the current f is used
to produce the corresponding secondary network N′. A layered network, N′′,
is then produced by a BFS on N′, starting from the source s. If the sink t is
not reached, the whole algorithm halts, and the current ﬂow is maximum. If t
is reached, a maximal (or blocking) ﬂow, f′′, is found in N′′. The ﬂow f is then
augmented, by using f′′, and a new phase is launched.
We now present a detailed description of the Dinitz algorithm and later discuss
its validity and time complexity.
The construction of N′′, from N′, uses the BFS layers. The production of the
layers is described in the subroutine LAYERS, which in turn uses a subroutine
LAYER, described in Algorithm 5.4. The input to LAYER is the secondary
network N′, the previous layer (a set of vertices) Vi, and the vertex labels. The
set of labeled verﬁces can change when LAYER is run (since additional vertices
may get labeled), and is therefore also an output. The set of vertices that may
become the next layer is provided by the output T.
The LAYERS subroutine is described in Algorithm 5.5. The input to LAYERS
consist of N′. The output is l, and if t is reached, the layers are speciﬁed in
V0, V1, ... Vl. Obviously, l ⩽n −1.
If t has been reached, the layered network N′′(G′′(V ′′,E′′),s,t,c′′) is
constructed as follows:
• V ′′ = ∪l
i=0Vi,
• E′′
i = {u e′′
−→v | u ∈Vi, v ∈Vi+1, e′′ ∈E′},
• E′′ = ∪l−1
i=0E′′
i ,
• for every e′′ ∈E′′ c′′(e′′) = c′(e′′).

96
5 Flow in Networks
Procedure LAYERS(N′;V0,V1, ... Vn−1,l)
Local variables: T a set of vertices, vertex labels.
1
label every v ∈V “unlabeled”
2
label s
3
V0 ←{s}
4
i ←0
5
repeat
6
call LAYER(N′,Vi, “vertex labels′′,T)
7
i ←i + 1
8
Vi ←T
9
until t ∈T or T = ∅
10
if t ∈T then do
11
l ←i
12
Vl ←{t}
13
else (T = ∅) declare: “f is a maximum ﬂow.”
Algorithm 5.5: Finding the BFS Layers of N′.
s
t
a
b
c
d
1,1
1,1
1,1
1,0
1,0
1,0
1,0
V0
V1
V2
V3
Figure 5.7: An example of a maximal ﬂow in a layered network that
is not maximum.
Next, we construct a maximal, or blocking ﬂow in N′′. A maximal ﬂow f′′
is a legitimate ﬂow in N′′ such that no augmenting path of length l exists in
the presence of f′′. A maximal ﬂow may not be maximum, as demonstrated in
Figure 5.7. The total ﬂow is 1. This ﬂow is not maximum, since there is a ﬂow

5.3 The Dinitz Algorithm
97
Procedure FIND-PATH(N′′,blocking-labels,S)
1
empty S
2
push (NIL,s) into S
3
while S is not empty and t is not the right element in the top pair of S do
4
let (x,v) be the top pair on S
5
if there is an unblocked edge v e′
−→v′, then do
6
push (e′,v′) into S
7
else do
8
if x ̸= NIL then label x ’blocked’
9
pop (x,v) from S
Algorithm 5.6: Finding an augmenting path of length l in N′′.
for which the total is 2. Yet it is maximal, since on every directed path of length
3 from s to t, there is at least one saturated edge.
It is easier to ﬁnd maximal ﬂow than maximum ﬂow, since the ﬂow in edges
never has to decrease. In other words, no backward labeling of vertices is
necessary.
The procedure MAXIMAL to ﬁnd a maximal ﬂow in a given layered network
N′′ starts with f′′(e′′) = 0 for every e′′ ∈E′′. It uses two subroutines: FIND-
PATH and INCREASE-FLOW. Initially, all edges are marked “unblocked”.
When an edge becomes saturated, its label changes to “blocked.” Also, if we
conclude that the ﬂow in u e′′
−→v cannot increase since all paths from v to t are
blocked, we change the label of e′′ to “blocked.”
FIND-PATH is described in Algorithm 5.6. Its input is N′′. The blocking-
labels are both an inputand an output,and the stackS is anoutput.The procedure
uses a DFS strategy. As long as there is an unblocked edge to continue on, we
keep going, and store the sequence of the edges and their end-points in a stack
S. Thus, S is a stack of pairs, such as (e,v), where v is the vertex that e enters.
If t is reached, the sequence of edges stored in S is a directed augmenting path
from s to t of length l. If there are no unblocked edges out of the vertex we
have reached, we pop the top edge (and its endpoint) from S, change its label
to ’blocked’ and backtrack to the previous vertex on the path. If we are stuck
at s, the search terminates with no edges in S.
The procedure INCREASE-FLOW is described in Algorithm 5.7. The pro-
cedure uses two local variables: S′ is a stack of vertices, and ∆is a real number.

98
5 Flow in Networks
Procedure INCREASE-FLOW(N′′,S,f′′,blocking-labels)
1
empty S′
2
∆←∞
3
while the pair on top of S is not (NIL,s) do
4
pop the top pair (e,v) from S
5
∆←min{∆, c′′(e) −f′′(e)}
6
push e into S′
7
while S′ is not empty do
8
pop the top edge e from S′
9
f′′(e) ←f′′(e) + ∆
10
if f′′(e) = c′′(e) then mark e ’blocked’
Algorithm 5.7: Increasing the ﬂow in the edges of the augmenting
path in N′′.
N′′ and S are the input, while f′′ and the blocking-labels are both an input and
an output. The augmenting path stored in S is scanned twice. During the ﬁrst
scan (Lines 3–6), the minimum residual capacity of the edges on the augment-
ing path is computed and recorded in ∆, and the augmenting path is restored
S′. In the second scan (Lines 7–10), the ﬂow in every edge of the augmenting
path is increased by ∆, and if an edge becomes saturated, its label is changed
from “unblocked” to “blocked”. Clearly, at least one edge on the path becomes
saturated.
The procedure MAXIMAL described in Algorithm 5.8 ﬁnds a maximal ﬂow,
f′′, in a given layered network N′′. The stack S and the blocking-labels are
internal variables. Finally, we are ready to present the Dinitz algorithm, as
described in Algorithm 5.9. Note that the input to DINITZ is the network N,
and the output is a maximum ﬂow f, in N. All other variables used in the
procedure, such as N′, N′′ and all their components, including l, and f′′, are
internal variables.
Lemma 5.3 If procedure DINITZ halts, the resulting f is a legitimate ﬂow
in N.
Proof: First, observe that in each phase the ﬂow f′′ is legitimate in N′′. The
edge rule is observed since ∆is chosen in INCREASE-FLOW in a way that

5.3 The Dinitz Algorithm
99
Procedure MAXIMAL(N′′,f′′)
1
for every e ∈E′′ do
2
label e ’unblocked’
3
f′′(e) ←0
4
run FIND-PATH(N′′, blocking-labels,S)
5
while S is not empty then do
6
run INCREASE-FLOW(N′′,S,f′′,blocking-labels)
7
run FIND-PATH(N′′,blocking-labels,S)
Algorithm 5.8: Constructing a maximal ﬂow in a layered network N′′.
Procedure DINITZ(N;f)
1
for every e ∈E do
2
f(e) = 0
3
N′ ←N
4
run LAYERS(N′;V0,V1,...,Vn−1,l)
5
while f has not been declared to be maximum do
6
construct N′′
7
run MAXIMAL(N′′;f′′)
8
for every e′ ∈E′′ do
9
if e′ corresponds to a forward edge e ∈E then do
10
f(e) ←f(e) + f′′(e′)
11
else (u e′
−→v ∈N′′, but v e
−→u ∈N) do
12
f(e) ←f(e) −f′′(e′)
13
construct N′ from (N,f)
14
run LAYERS(N′;V0,V1,...,Vn−1,l)
Algorithm 5.9: The Dinitz algorithm.
ensures that no edge is overﬂowed. The vertex rule is observed, since the ﬂow
is built up by augmenting paths.
By the deﬁnition of c′ and thus, c′′, when f is changed by adding (subtracting)
f′′ to (from) the previous f, the edge rule is maintained in the edges of G. Also,

100
5 Flow in Networks
since f is the superposition of two ﬂows, each of which observes the vertex rule,
so does their sum.
■
Lemma 5.4 If procedure DINITZ halts, the resulting f is a maximum ﬂow
in N.
Proof: The proof here is very similar to the one in the Ford and Fulkerson
algorithm. Consider the last phase, in which vertex t is not reached in procedure
LAYERS (see Line 13). Let S be the union of V0, V1,..., Vi, where Vi is the
last (nonempty) layer before T = ∅is encountered. Now consider the cuts (S;¯S)
and (¯S;S) in G. Every edge u e
−→v in (S;¯S) is saturated, or else e is useful from
u ∈Vi to v, and T is not empty. Also, for every edge u e
−→v in (¯S;S), f(e) = 0,
or e is useful from v ∈Vi to u, and again, T is not empty. The remainder of the
argument is identical to that of the Ford-Fulkerson case.
■
Lemma 5.5 In each phase, except the last, the ﬂow f′′ is maximal in N′′.
Proof: Initially, all edges of N′′ are unblocked (see Algorithm 5.8). An edge
u e
−→v is marked “blocked” in two cases:
• e is saturated. See Line 10 of Algorithm 5.7.
• There is no unblocked edge out of v. See Line 8 of Table 5.6.
In either case, when an edge is blocked, it is already known that no additional
augmenting path through itispossible.Thus,whenalledgesoutofsareblocked,
the ﬂow f′′ is maximal. This event is detected by procedure FIND-PATH (see
Algorithm 5.6) by the emptiness of S.
■
Lemma 5.6 The running time of each phase is O(|V| · |E|).
Proof: The time it takes to construct N′′ is O(|E|), since it is essentially a BFS
on N′.
Note that in Algorithm 5.6 we traverse edges (see Lines 5–6). If we backtrack
to a vertex other than s (Lines 7–9), an edge is marked blocked.
The number of consecutive edge traversals between two blockings of edges
is bounded by l for the following reasons:
• If l traversals are performed consecutively without any backtracking, then
t has been reached and an augmenting path is stored in S. Procedure
INCREASE-FLOW is called and at least one edge becomes saturated and
is marked blocked.

5.3 The Dinitz Algorithm
101
• It is possible that we have backtracked into a vertex other than s. In that case,
in less than l consecutive traversals, either another backtrack occurs or t is
reached, and as in the previous case, at least one edge is marked “blocked”.
Since the number of edges in N′′ is O(|E|), and since every edge can be
blocked at most once, the total number of edge traversals is O(|V| · |E|).
It is not hard to see that the time of all other operations in proce-
dure MAXIMAL is bounded by a constant times the number of edge
traversals.
■
Note that the length of a layered network N′′ was denoted by l. Since we
need to compare lengths of the layered networks of consecutive phases, let us
denote by lk the length of the layered network of the k-th phase, k ⩾1.
Lemma 5.7 If the (k + 1)-st phase is not the last, then lk+1 > lk.
Proof: There is a path P of length lk+1 in the layered network of the (k+1)-st
phase, which starts with s and ends with t:
P : s = v0 e1
−→v1 e2
−→v2 ··· vlk+1−1
elk+1
−→vlk+1 = t .
First, let us assume that all the vertices of Pappear in the k-th layered network.
Let Vj be the j-th layer of the k-th layered network. We claim that if va ∈Vb,
then a ⩾b. This is proved by induction on a. For a = 0 (v0 = s), the claim is
obviously true. Now, assume va+1 ∈Vc.
By the inductive hypothesis, (a ⩾b), and if b+1 ⩾c, then a+1 ⩾b+1 ⩾c,
proving the inductive step. However, if b + 1 < c, then the edge ea+1 has not
been used in the k-th phase since it is not even in the k-th layered network,
where all edges are between adjacent layers. If ea+1 has not been used in the
k-th layered network and is useful from va to va+1 in the beginning of phase
k + 1, then it has been useful from va to va+1 in the beginning of phase k, as
well. Thus, va+1 cannot belong to Vc since by the algorithm it belongs to a
previous layer of the k-th network.
Now,in particular,t = vlk+1 ∈Vk.Therefore,lk+1 ⩾lk.Also,equality cannot
hold because in this case, the entire P is in the k-th layered network, and if all
its edges are still useful in the beginning of phase k + 1, then the ﬁnal ﬂow f′′
of phase k was not maximal. This proves the claim of the lemma in the case
that all vertices of P are in the k-th layered network.
If not all the vertices of P appear in the kth layered network, then let va
ea+1
−→
va+1 be the ﬁrst edge of P such that for some b, va ∈Vb, but va+1 is not in the

102
5 Flow in Networks
k-th layered network. Thus, ea+1 was not used in phase k. Since ea+1 is useful
in the beginning of phase k + 1, it was also useful in the beginning of phase k.
The only possible reason for va+1 not to belong to Vb+1 is that b+1 = lk and
va+1 ̸= t. Since t = vlk+1, it follows that a+1 < lk+1. By the argument of the
previous paragraph, a ⩾b. Thus, lk = b + 1 ⩽a + 1 < lk+1.
■
Corollary 5.2 The number of phases is bounded by |V|.
Proof: Clearly, l1 ⩾1. By Lemma 5.7, for the k-th phase, if it is not the last,
lk ⩾k. Since k ⩽lk ⩽|V| −1, the number of phases, including the last, is
bounded by |V|.
■
Theorem 5.2 The Dinitz algorithm terminates in time O(|V|2|E|) and yields a
maximum ﬂow.
Proof: By Corollary 5.2, the number of phases is bounded by |V|. By
Lemma 5.6, each phase requires O(|V| · |E|) time. Thus, the whole algo-
rithm takes O(|V|2|E|) time to terminate. By Lemma 5.4, the resulting ﬂow
is maximum in N.
■
Theorem 5.3 (The max-ﬂow min-cut theorem) Every ﬁnite network has a
maximum ﬂow and a minimum cut, and their values are equal.
Proof: By Theorem 5.2 there is a max-ﬂow, and by the proof of Lemma 5.4,
there is a cut of the same value. By Corollary 5.1, this cut is of minimum
value.
■
5.4 Networks with Upper and Lower Bounds
In the previous sections, we have assumed that the ﬂow in each edge is bounded
from above by the capacity of the edge, but that the lower bound on the ﬂow in
every edge is zero. The signiﬁcance of this assumption is that the assignment
of f(e) = 0, for every edge e, deﬁnes a legitimate ﬂow, and the algorithm for
improving the ﬂow can be started with this zero ﬂow.
In this section, in addition to the upper bound c(e) on the ﬂow in e, we assume
that the ﬂow is also boundedfrom below by b(e). Thus, the edge rule is changed
as follows: The ﬂow f(e), in every edge e, must satisfy
b(e) ⩽f(e) ⩽c(e).
(5.6)
The vertex rule remains unchanged.
Thus, the problem of ﬁnding a maximum ﬂow in a given network
N(G(V,E),s,t,b,c) is divided into two subproblems. First, check whether N

5.4 Networks with Upper and Lower Bounds
103
s
t
a
0,1
2,3
Figure 5.8: An example of a network which has no legitimate ﬂow.
has legitimate ﬂows, and if the answer is positive, ﬁnd one. Second, increase
this initial ﬂow and ﬁnd a maximum ﬂow.
A simple example of a network that has no legitimate ﬂow is shown in
Figure 5.8. Here, next to each edge e we write b(e),c(e).
The following method for testing whether a given network N has a legitimate
ﬂow function is due to Ford and Fulkerson [7]. It reduces the problem to one
of determining a maximum ﬂow in an auxiliary network ˜N( ˜G( ˜V, ˜E), ˜s,˜t, ˜c), in
which all lower bounds are zero. Here, ˜s is called the auxiliary source, and ˜t
is called the auxiliary sink. In the auxiliary ﬂow problem, s and t are not the
source and the sink anymore and must satisfy the vertex rule. We shall show
that the original N has legitimate ﬂows if and only if the maximum ﬂow in ˜N
satisﬁes a condition to be speciﬁed shortly.
˜N is deﬁned as follows:
(i) ˜V = V ∪{˜s,˜t}, where ˜s and ˜t are two new vertices.
(ii) ˜E = E ∪S ∪T ∪{e′,e′′}, where S, T and {e′,e′′} are sets of new edges
deﬁned as follows:
S = {˜s −→v | v ∈V},
T = {v −→˜t | v ∈V},
and s e′
−→t and t e′′
−→s.
(iii) c : ˜E →R⩾0∪{∞} is deﬁned separately for each of the four sets of edges:
– For e ∈E,
˜c(e) = c(e) −b(e).
(5.7)
– For ˜s σ
−→v, ˜c(σ) = 
e∈α(v) b(e).
– For v τ
−→˜t, ˜c(τ) = 
e∈β(v) b(e).
– ˜c(e′) = ∞and ˜c(e′′) = ∞.
Let us demonstrate this construction on the network shown in Figure 5.9. The
auxiliary network is shown in Figure 5.10. The upper bounds are shown next
to the edges to which they apply.
Now we can use the Ford and Fulkerson or the Dinitz algorithm to ﬁnd a
maximum ﬂow in the auxiliary network. It is left to the reader to verify that the

104
5 Flow in Networks
s
t
w
x
y
z
5,7
1,3
3,5
0,10
2,6
2,4
1,3
2,8
Figure 5.9: An example of a network for which we want to determine
whether it has a legitimate ﬂow.
s
t
w
x
y
z
1
2
4
4
0
1
0
3
5
5
2
2
2
2
2
2
6
10
4
5
˜s
˜t
∞
∞
Figure 5.10: The auxiliary network of the example network.
maximum total ﬂow in the auxiliary network of our example is 16. We return
to this example shortly.
Theorem 5.4 The original network N has a legitimate ﬂow if and only if the
maximum ﬂow of the auxiliary network ˜N saturates all the edges emanating
from ˜s; that is, all edges of S.
Clearly, if all edges of S are saturated, then so are all edges of T . This follows
from the fact that every edge e ∈E contributes b(e) to the capacity of one edge

5.4 Networks with Upper and Lower Bounds
105
in S and of one edge in T . Thus, the sum of capacities of edges of S is equal
to the sum of capacities of edges of T .
Proof: First, we prove the sufﬁciency ofthe condition.Assume that a maximum
ﬂow function ˜fof ˜Nsaturates a11 edges of S. Deﬁne the following ﬂow function
f, for N: For every e ∈E,
f(e) = b(e) + ˜f(e).
(5.8)
Since ˜f satisﬁes the edge rule in ˜N, it follows that
0 ⩽˜f(e) ⩽˜c(e).
(5.9)
Thus,
b(e) ⩽b(e) + ˜f(e) ⩽b(e) + ˜c(e).
By Equations 5.8 and 5.7, one gets
b(e) ⩽f(e) ⩽c(e) ,
(5.10)
and f satisﬁes the edge rule.
Now, let v ∈V \{s,t}. We remind the reader that α(v) is the set of edges that
enter v in G, and β(v) is the set of edges that emanate from v in G. Let ˜s σ
−→v
be the new edge that enters v in ˜G, and v τ
−→˜t be the new edge that emanates
from v in ˜G. Since ˜f satisﬁes the vertex rule in ˜N, we have

e∈α(v)
˜f(e) + ˜f(σ) =

e∈β(v)
˜f(e) + ˜f(τ) .
(5.11)
By the assumption, σ and τ are saturated. Thus,

e∈α(v)
˜f(e) + ˜c(σ) =

e∈β(v)
˜f(e) + ˜c(τ) .
By the deﬁnitions of ˜c(σ) and ˜c(τ), it follows that

e∈α(v)
˜f(e) +

e∈α(v)
b(e) =

e∈β(v)
˜f(e) +

e∈β(v)
b(e) .
By Equation 5.8 and merging the sums on each side of the equation, one gets

e∈α(v)
f(e) =

e∈β(v)
f(e) ,
(5.12)

106
5 Flow in Networks
and the vertex rule is proved. Thus, f is legitimate in N.
Next, we prove the necessity of the condition. Assume there is a legitimate
ﬂow f in N. Let us show that there is a maximum ﬂow ˜f in ˜N for which all
edges of S are saturated.
The steps of the previous proof are reversible, with minor modiﬁcations. For
an edge e ∈E, use Equation 5.8 to deﬁne ˜f(e). For an edge σ ∈S, deﬁne
˜f(σ) = ˜c(σ), and for τ ∈T deﬁne ˜f(τ) = ˜c(τ). Finally, if F ⩾0 is the total ﬂow
according to f, deﬁne ˜f(e′′) = F and ˜f(e′) = 0, and if F < 0, deﬁne ˜f(e′′) = 0
and ˜f(e′) = −F.
To show that ˜f observes the edge rule, all we need to do is show it for e ∈E;
obviously, in the remaining edges, ˜f is deﬁned in a way which satisﬁes the edge
rule. Since f is legitimate in N, it satisﬁes Equation 5.10. By Equations 5.8 and
5.7, one concludes that Equation 5.9 holds.
To show that ˜f observes the vertex rule, ﬁrst consider a vertex v ∈V \ {s,t}.
Since f observes the vertex rule in N, Equation 5.12 holds. By reversing the
steps of the proof above, one gets Equation 5.11, which implies that v satisﬁes
the vertex rule.
Finally, for vertex s, ˜f satisﬁes the vertex rule for the following reasons. In N,
s may not be balanced. If F > 0, then the total outgoing ﬂow is F units greater
than the total incoming ﬂow. The total outgoing ﬂow in ˜N in edges of β(s) and
in τ is as it is in N, and the total incoming ﬂow in ˜N in edges of α(s) and in σ
is also as in N. However, ˜f(e′′) causes the incoming and outgoing total ﬂows
to balance. Similar arguments show that the total ﬂow in t is balanced as well,
and that both s and t are balanced when F < 0.
■
Let us demonstrate the technique for establishing whether the network has a
legitimateﬂow,andﬁnding oneinthecasetheanswerispositive,onourexample
(Figure 5.9). First, we apply the Dinitz algorithm to the auxiliary network of
Figure 5.10 and end up with the ﬂow, as in Figure 5.11. The maximum ﬂow
saturates all edges that emanate from ˜s, and we conclude that the original
network has a legitimate ﬂow. We use Equation 5.8 to deﬁne a legitimate ﬂow
in the original network; this is shown in Figure 5.12. (Next to each edge e we
write b(e),c(e), and f(e), in this order.)
Once a legitimate ﬂow f has been found in N, we turn to the question of
optimizing it. First, let us consider the question of maximizing the total ﬂow.
OnecanusetheFordandFulkersonalgorithm,exceptthatthebackwardlabeling
must be redeﬁned as follows. An edge v e
−→u is backwardly useful for labeling
vertex v, if

5.4 Networks with Upper and Lower Bounds
107
s
t
w
x
y
z
1, 1
0, 0
3, 3
5, 5
5, 5
2, 2
2, 0
2, 2
2, 0
2, 2
10, 2
2, 1
4, 0
6, 2
5, 5
1, 1
2, 2
4, 4
0, 0
4, 4
˜s
˜t
∞, 5
∞, 0
Figure 5.11: A maximumﬂow in the auxiliary network of the example
network.
s
t
y
z
w
x
1,3,3
5,7,5
3,5,3
2,4,2
1,3,2
0,10,2
2,6,2
2,8,4
Figure 5.12: A legitimate ﬂow in the original example network.
(i) u is labeled and v is not,
(ii) f(e) > b(e).
The label that v gets is (e,−). In this case, Line 7 of the procedure LABEL (see
Algorithm 5.1) is changed to
∆(e) ←f(e) −b(e).

108
5 Flow in Networks
With this exception, the algorithm is exactly as described in Section 5.2. The
proof that the ﬂow is maximum when the algorithm terminates is similar, but
we need to redeﬁne the capacity of a cut determined by S (see Equation 5.4) as
follows:
c(S) =

e∈(S; ¯S)
c(e) −

e∈( ¯S;S)
b(e) .
It is easy to prove that a statement just like Lemma 5.2 still holds; namely,
F ⩽c(S) .
Now, the set of labeled vertices S, when the algorithm terminates, satisﬁes this
inequality by equality. Thus, the ﬂow is maximum, and the indicated cut is
minimum.
Clearly, the Dinitz algorithm can also be used. It is left as an exercise for the
reader to make the necessary changes.
In certain applications, what we want is a minimum ﬂow, that is, a legitimate
ﬂow function f for which the total ﬂow F is minimum. Consider a network
N(G(V,E),b,c), where function b : E →R speciﬁes the lower bounds on the
ﬂow in the edges, and c : E →R speciﬁes the upper bounds on the ﬂow in the
edges. Let s,t ∈V be any two vertices. Denote by f : E →R a legitimate ﬂow
function of the network (G(V,E),s,t,b,c), where s plays the role of the source
and t plays the role of the sink. Also, let Fs,t be the corresponding total ﬂow.
Clearly, f is legitimate in (G(V,E),t,s,b,c) as well, where the roles of the
source and the sink are reversed. Also,
Fs,t = −Ft,s .
It follows that f is a minimum ﬂow in (G(V,E),s,t,b,c) if and only if f is a
maximum ﬂow in (G(V,E),t,s,b,c). Thus, our techniques solve the problem
of minimizing the total ﬂow by simply exchanging the roles of s and t.
For a set {s} ⊂S ⊂V \ {t}, let us deﬁne c(S), (the value of the cut (S;¯S) for
minimum ﬂow purposes) as follows:
c(S) =

e∈(S; ¯S)
b(e) −

e∈( ¯S;S)
c(e) .
Clearly, c(S) = −c(¯S). Now, assume f is a maximum ﬂow in (G(V,E),t,s,b,c).
By the max-ﬂow min-cut theorem (Theorem 5.3), which also applies to net-
works with lower and upper bounds, provided they have legitimate ﬂows, there

5.5 Problems
109
is a cut deﬁned by some {s} ⊂S ⊂V \ {t}, such that
Ft,s = c(¯S) .
Thus,
−Fs,t = −c(S) ,
and
Fs,t = c(S) .
One can prove and use a lemma similar to Lemma 5.2, with the inequality
reversed. Thus, c(S) is maximum. It follows that for networks with lower and
upper bounds, which have legitimate ﬂows, a min-ﬂow max-cut theorem holds.
5.5 Problems
Problem 5.1 Find a maximum ﬂow in the network shown below.
s
a
b
c
d
e
f
t
3
2
9
6
2
7
5
7
1
2
4
6
The number next to each edge is its capacity. Show a minimum cut. How
many minimum cuts can you ﬁnd?
Problem 5.2 In the following network x1, x2, x3, are all sources (of the same
commodity). The supply available at x1 is 5, at x2 is 10, and at x3 is 5. The

110
5 Flow in Networks
a
b
c
d
7
9
4
3
3
7
3
2
5
8
8
1
6
8
1
2
4
2
7
x1
x2
x3
y1
y2
y3
vertices y1, y2, y3, are all sinks. The demand required at y1 is 5, at y2 is 10,
and at y3 is 5. Find out whether all requirements can be met simultaneously.
(Hint: One way of solving this type of problem is to reduce it to the familiar
one-source one-sink format. Introduce auxiliary source s and sink t. Connect s
to xi through a directed edge of capacity equal to xi’s supply. Connect each yi
to t through a directed edge of capacity equal to yi’s demand. Find a maximum
ﬂow in the resulting network, and observe whether all demands are met.)
Problem 5.3 In the following network, in addition to the capacities of the
edges, each vertex other than s and t has an upper bound on the ﬂow that may
s
t
5
4
5
a
5
b
c
5
d
e
4
f
3
3
2
9
6
7
2
7
5
3
4
2
6
1

5.5 Problems
111
ﬂow through it. These vertex capacities are written below the vertex labels.
Find a maximum ﬂow for this network. (Hint: One way of solving this type
of problem is to replace each vertex v by two vertices v′ and v′′ with an edge
v′
e
−→v′′, where c(e) is the upper bound on the ﬂow through v. All edges that
previously entered v now enter v′, and all edges that previously emanated from
v now emanate from v′′.)
Problem 5.4
(1) Describe an alternative labeling procedure,like that of Ford and Fulkerson,
for maximizing the ﬂow, except that the labeling starts at t, and if it reaches
s, an augmenting path is found.
(2) Demonstrate your algorithm on the following network:
s
t
a
c
b
d
4
5
6
4
4
4
2
7
6
(3) Describe a method of locating an edge with the property that increasing
its capacity increases the maximum ﬂow in the network. (Hint: One way
of doing this is to use both source-to-sink and sink-to-source labelings.)
Demonstrate your method on the network above.
(4) Does an edge like this always exist? Prove your claim.
Problem 5.5 In a network N(G(V,E),s,t,c), there are two sets, {s} ⊂S1 ⊂
V \ {t} and {s} ⊂S2 ⊂V \ {t}, and each of them deﬁnes a minimum cut. Prove
that each of S1 ∪S2 and S1 ∩S2 deﬁnes a minimum cut as well.
Problem 5.6 Let f be a maximum ﬂow in N(G(V,E),s,t,c), where all edge
capacitiesarepositiveintegers,andforeverye ∈E,f(e)isanonnegativeinteger.

112
5 Flow in Networks
The capacity of u
e
−→v is reduced by one unit. It is necessary to ﬁnd a
maximum ﬂow in the new network.
Describe an algorithm to achieve this goal whose time complexity is O(|E|).
(Hint: Without loss of generality, assume f(e) = c(e). If there is a directed
circuit via e that carries ﬂow, reduce the ﬂow on every edge of the circuit by
one unit. If not, ﬁrst locate a directed path from s to t via e that carries ﬂow,
and reduce the ﬂow in every edge of the path by one unit, and then look for an
augmenting path from s to t in the new network.)
Problem 5.7 Let G(V,E) be a ﬁnite directed graph without parallel edges.
Describe an algorithm that is a modiﬁcation of the algorithm of Ford and Fulk-
erson for ﬁnding a maximum ﬂow in a given network N(G,s,t,c), except that,
in each stage, one looks for an augmenting path for which ∆, the value by
which the ﬂow is increased, is maximum. The time complexity of ﬁnding each
such path should be O(|V|2). (Comment: This algorithm can be shown to be
polynomial; see [4].)
Problem 5.8 In the following network, the capacities of the edges are written
next to them. Use the Dinitz algorithmto ﬁnd its maximumﬂow when the initial
ﬂow is zero everywhere. How many phases are there in which t is reached?
s
t
2
2
2
2
2
2
2
2
2
2
2
2
1
1
Problem 5.9 A ﬂow in a network is said to have circuits if there is at least one
directed circuit such that on all its edges the ﬂow is positive. Such a circular
ﬂow is superﬂuous since it contributes nothing to the total ﬂow.

5.5 Problems
113
• Show that if we start with zero ﬂow everywhere and use the Dinitz algorithm
to ﬁnd a maximum ﬂow, we may end up with a ﬂow that has circuits. (Hint:
Consider the graph depicted in Figure 5.13.)
s
t
Figure 5.13: Hint for Problem 5.9.
• Describe an O(|E|2) algorithm to remove all circular ﬂow from a given ﬂow
function.
Problem 5.10 Let N(G(V,E),c) be a network where G is a ﬁnite directed
graph, and c is a capacity function on the edges. For every x,y ∈V, let Fxy
denote the maximum ﬂow in case x is the source and y is the sink.
Prove that for every three vertices u,v,w ∈V, Fuw ⩾min{Fuv,Fvw}.
Problem 5.11 Prove that in a network with a lower bound b(e) ⩾0 for every
edge e, but no upper bound (c(e) = ∞), there is a legitimate ﬂow if and only
if for every edge e, for which b(e) > 0, either e is in a directed circuit or e is
in a directed path from s to t or from t to s. Show that in such a network, a
legitimate ﬂow can be found in time O(|V| · |E|).
Problem 5.12 Find a minimum ﬂow from s to t for the network of Problem 5.1,
where the numbers next to the edges are now assumed to be lower bounds, and
there are no upper bounds.
Problem 5.13 The two networks shown below have both lower and upper
bounds on the ﬂow through the edges. Which of the two networks has no
legitimate ﬂow? Find both a maximum ﬂow and a minimum ﬂow if a legit-
imate ﬂow exists. If no legitimate ﬂow exists, display a set of vertices that

114
5 Flow in Networks
includes neither the source nor the sink and is required to “produce” ﬂow or to
“absorb” it.5
s
t
3,5
2,6
9,12
3,4
1,2
4,6
2,4
3,6
s
t
3,5
2,6
5,10
3,4
1,2
4,6
2,4
3,6
(a)
(b)
Problem 5.14 Prove that a network with lower and upper bounds on the ﬂow
in the edges has no legitimate ﬂow if and only if there exists a set of vertices
which includes the neither source nor the sink and is required to “produce” ﬂow
or to “absorb” it.
5 A set of vertices A ⊂V \ {s,t} is required to absorb ﬂow if

e∈( ¯A;A)
b(e) >

e∈(A; ¯A)
c(e) .

5.6 Notes by Andrew Goldberg
115
5.6 Notes by Andrew Goldberg
The augmenting path algorithm is due to Ford and Fulkerson [7, 6]. Dinitz [2]
developed the blocking ﬂow algorithm, which runs in polynomial time. The
observation that augmenting along a shortest path leads to a polynomial-time
algorithm has been made independentlyby Edmondsand Karp [4]. A more efﬁ-
cient, O(|V|3), variant of the blocking ﬂow algorithm based on preﬂows is due
to Karzanov [12]. Sleator and Tarjan [14] used the dynamic tree data structure
to improve this bound to O(|V||E|log|V|). This has been further improved to
O(|V||E|log(|V|2/|E|)) by Goldberg and Tarjan [10].
Goldberg and Tarjan [9] developed the push-relabel method that uses pre-
ﬂows and the push operation similar to Karzanov’s algorithm. However, instead
of building a layered network, the algorithm uses the relabel operation for
ﬁne-grains updates of vertex distances. The push-relabel method leads to the
best currently known, strongly polynomial bound of King et al. [13]. This
bounds comes very close, but does not quite achieve, O(|V||E|). The push-
relabel algorithm is also highly practical when used in combination with
efﬁciency-enhancing heuristics [1].
Karzanov [11] and, independently, Even and Tarjan [5] have shown that
on unit capacity networks, the blocking ﬂow algorithm algorithm runs in
O(min((|E|1/2,|V|2/3)|E|) time. This leaves a polynomial gap between the
general and the unit-capacity cases.
When talking about shortest augmenting paths, one has to assign lengths
for residual edges. All polynomial-time algorithms mentioned above use the
unit length function. Edmonds and Karp [4] observed that one can use
other length functions. However, for a long time nobody was able to use
this observation to improve the time bounds. Goldberg and Rao [8] use the
binary length function (zero for high- and one for low-capacity edges) to get
an O(min((|E|1/2,|V|2/3)|E|log |V|2
|E| logU) bound for networks with integral
capacities in the range [1,U]. This closes the gap between the general and the
unit-capacity case.
Bibliography
[1] B. V. Cherkassky and A. V. Goldberg. “On Implementing Push-Relabel Method
for the Maximum Flow Problem.” Algorithmica, 19:390–410, 1997.
[2] E. A. Dinic. “Algorithm for Solution of a Problem of Maximum Flow in Networks
with Power Estimation.” Soviet Math. Dokl., 11:1277–1280, 1970.

116
5 Flow in Networks
[3] Yeﬁm Dinitz. Dinitz’ Algorithm: The original version and Even’s version. In Oded
Goldreich, Arnold L. Rosenberg, and Alan L. Selman, editors, Essays in Memory
of Shimon Even, Vol. 3895 of Lecture Notes in Computer Science, pp. 218–240.
Springer, 2006.
[4] J. Edmonds and R. M. Karp. “Theoretical Improvements in Algorithmic Efﬁciency
for Network Flow Problems.” J. Assoc. Comput. Mach., 19:248–264, 1972.
[5] S. Even and R. E. Tarjan. “Network Flow and Testing Graph Connectivity.” SIAM
J. Comput., 4:507–518, 1975.
[6] L. R. Ford, Jr. and D. R. Fulkerson. “Maximal Flow Through a Network.”
Canadian Journal of Math., 8:399–404, 1956.
[7] L. R. Ford, Jr. and D. R. Fulkerson. Flows in Networks. Princeton University Press,
1962.
[8] A. V. Goldberg and S. Rao. “Beyond the Flow Decomposition Barrier.” J. Assoc.
Comput. Mach., 45:753–782, 1998.
[9] A. V. Goldberg and R. E. Tarjan. “A New Approach to the Maximum Flow
Problem.” J. Assoc. Comput. Mach., 35:921–940, 1988.
[10] A. V. Goldberg and R. E. Tarjan. “Finding Minimum-Cost Circulations by
Successive Approximation.” Math. of OR, 15:430–466, 1990.
[11] A. V. Karzanov. “O nakhozhdenii maksimal’nogo potoka v setyakh spetsial’nogo
vida i nekotorykh prilozheniyakh.” In Matematicheskie Voprosy Upravleniya
Proizvodstvom, volume 5. Moscow State University Press, 1973. In Russian; title
translation: “On Finding Maximum Flows in Networks with Special Structure and
Some Applications.”
[12] A. V. Karzanov. “Determining the Maximal Flow in a Network by the Method of
Preﬂows.” Soviet Math. Dok., 15:434–437, 1974.
[13] V. King, S. Rao, and R. Tarjan. “A Faster Deterministic Maximum Flow
Algorithm.” J. Algorithms, 17:447–474, 1994.
[14] D. D. Sleator and R. E. Tarjan. “A Data Structure for Dynamic Trees.” J. Comput.
System Sci., 26:362–391, 1983.

6
Applications of Network Flow Techniques
6.1 Zero-One Network Flow
Several combinatorial problems can be solved through network ﬂow techniques.
In the networks we get, the capacity of all the edges is one. To get better
algorithms with lower time complexities, we need to study these network ﬂow
problems. We follow here the work of Even and Tarjan [1].
Consider a maximum ﬂow problem where for every edge e of G(V,E),
c(e) = 1.
The ﬁrst observation is that in the Dinitz algorithm for maximal ﬂow in a
layered network, each time we ﬁnd a path, all the edges on it become blocked;
in case the last edge leads to a dead end, we backtrack on this edge, and it
becomes blocked. Thus, the total number of edge traversals is bounded by |E|,
and the whole phase is of time complexity O(|E|). Since the number of phases
is bounded by |V|, the Dinitz algorithm for maximum ﬂow is of complexity
O(|V| · |E|).
Our ﬁrst goal is to prove a better bound yet: O(|E|3/2). However, we need to
prepare a few results beforehand.
Let G(V,E) be a 0-1 network in which c(e) = 1 for all e ∈E with some
integral legal ﬂow function f. Deﬁne ˜G(V, ˜E) as follows:
(i) If u e
−→v in G, and f(e) = 0, then e ∈˜E.
(ii) If u
e
←−v in G, and f(e) = 1, then u e′
−→v is in ˜G. Clearly, e′ is a new
edge that corresponds to e.
Thus, |E| = |˜E|. Clearly, the useful edges of the layered network that is con-
structed for G with present ﬂow f, with their direction of usefulness, are all
edges of ˜G.
117

118
6 Applications of Network Flow Techniques
Let us denote by (SI ¯S)G, where s ∈S, t /∈S, and ¯S = V −S, the set of edges
that emanate from a vertex of S and enter a vertex of ¯S, and let c(S,G) be the
capacity of the corresponding cut in G. Also, let M be the total maximum ﬂow
in G, while F is the total present ﬂow.
Lemma 6.1
˜M = M −F.
Proof: Let S be a subset of V such that s ∈S and t /∈S. The deﬁnition of ˜G
implies that
c(S, ˜G) = |(S;¯S) ˜G| =

e∈(S; ¯S)G
(1 −f(e)) +

e∈( ¯S;S)G
f(e).
However,
F =

e∈(S; ¯S)G
f(e) −

e∈( ¯S;S)G
f(e).
Thus,
c(S, ˜G) = |(S;¯S)G| −F = c(S,G) −F.
This implies that the minimum cut of G corresponds to the minimum cut
of ˜G; that is, is deﬁned by the same S. By the max-ﬂow min-cut theorem
(Theorem 5.1), the capacity of a minimum cut of ˜G is ˜M (the maximum total
ﬂow in ˜G). Thus, the lemma follows.
■
Lemma 6.2 The length of the layered network for the 0-1 network deﬁned by
G(V,E) (with a given s and t) and zero ﬂow everywhere is at most |E|/M.
Proof: We remind the reader that Vi is the set of vertices of the i-th layer of
the layered network, and Ei is the set of edges from Vi−1 to Vi. Since f(e) = 0
for every e ∈E, the useful directions are all forward. Thus, every Ei is equal to
(S;¯S)G, where S = V0 ∪V1 ∪... ∪Vi−1. Thus, by Lemma 5.1,
M ⩽|Ei|.
(6.1)
Summing up (6.1), for every i = 1,2,...,l where l is the length of the layered
network, we get l · M ⩽|E|, or
l ⩽|E|/M.
■
Theorem 6.1 For 0-1 networks, Dinitz’s algorithm is of time complexity
O(|E|3/2).

6.1 Zero-One Network Flow
119
Proof: If M ⩽|E|1/2, then the number of phases is bounded by |E|1/2, and
the result follows. Otherwise, consider the phase during which the total ﬂow
reaches M −|E|1/2. The total ﬂow F in G(V,E) when the layered network for
this phase is constructed satisﬁes
F < M −|E|1/2.
This layered network is identical with the one constructed for ˜G, with zero ﬂow
everywhere. Thus, by Lemma 6.1;
˜M = M −F > |E|1/2.
By Lemma 6.2, the length l of this layered network satisﬁes
l ⩽|E|/ ˜M < |E|/|E|1/2 = |E|1/2.
Therefore, the number of phases up to this point is at most |E|1/2 −1, and since
the numberof additionalphases to completion is at most |E|1/2, the total number
of phases is at most 2|E|1/2.
■
A 0-1 network is of type 1 if it has no parallel edges. For such a network we
can prove another upper bound on the time complexity. First, we prove a lemma
similar to Lemma 6.2.
Lemma 6.3 Let G(V,E) deﬁne a 0-1 network of type 1, with maximum total
ﬂow M from s to t. The length l of the ﬁrst layered network, when the ﬂow is
zero everywhere, is at most 2|V|/M1/2.
Proof: Let Vi be the set of vertices of the i-th layer. Since there are no parallel
edges, the set of edges, Ei+1, from Vi to Vi+1 in the layered network satisﬁes
|Ei+1| ⩽|Vi| · |Vi+1| for every i = 0,1,...,l −1. Since each |Ei| is the capacity
of a cut, we get that
M ⩽|Vi| · |Vi+1|.
Thus, either Vi ⩾M1/2 or |Vi+1| ⩾M1/2. Clearly,
|V| ⩾
ℓ

i=0
|Vi| ⩾
l + 1
2

· M1/2.
Thus,
|V|
M1/2 ⩾
l + 1
2

⩾ℓ
2,

120
6 Applications of Network Flow Techniques
and
2|V|
M1/2 ⩾ℓ,
and the lemma follows.
■
Theorem6.2 For0-1 networksoftype1,Dinitz’salgorithmhastimecomplexity
O(|V|2/3 · |E|).
Proof: If M ⩽|V|2/3, the result follows immediately. Let F be the total ﬂow
when the layered network, for the phase during which the total ﬂow reaches the
value M−|V|2/3, is constructed. This layered network is identical with the ﬁrst
layered network for ˜G with zero ﬂow everywhere. ˜G may not be of type 1 since
it may have parallel edges, but it can have at most two parallel edges from one
vertex to another; if e1 and e2 are antiparallel in G, f(e1) = 0 and f(e2) = 1,
then in ˜G there are two parallel edges: e1 and e2. A result similar to Lemma 6.3
yields that
l < 23/2|V|/ ˜M1/2.
Since ˜M = M −F > M −(M −|V|2/3) = |V|2/3, we get
l < 23/2|V|
|V|1/3 = 23/2 · |V|2/3.
Thus, the number of phases up to this point is O(|V|2/3). Since the number of
phases from here to completion is at most |V|2/3, the total number of phases is
O(|V|2/3).
■
In certain applications, the networks that arise satisfy the condition that for
each vertex other than s or t, there is either only one edge emanating from it or
only one edge entering it. Such 0-1 networks are called type 2.
Lemma 6.4 Let the 0-1 network deﬁned by G(V,E) be of type 2, with maximum
total ﬂow M from s to t. The length l of the ﬁrst layered network, when the ﬂow
is zero everywhere, is at most (|V| −2)/M + 1.
Proof: The structure of G implies that a max-ﬂow in G can be decomposed
into vertex-disjoint directed paths from s to t; that is, no two of these paths
share any vertices, except their common start-vertex s and end-vertex t. (The
ﬂow may imply some directed circuits that are vertex-disjoint from each other
and from the paths above, except possibly at s or t. These circuits are of no
interest to us). The number of these paths is equal to M. Let λ be the length of a

6.2 Vertex Connectivity of Graphs
121
shortest of these paths. Thus, each of the paths uses at least λ −1 intermediate
vertices. We have
M · (λ −1) ⩽|V| −2,
which implies λ ⩽(|V| −2)/M + 1. However, l ⩽λ. Thus, the lemma
follows.
■
Lemma 6.5 If the 0-1 network deﬁned by G is of type 2, and if the present ﬂow
function is f, then the corresponding ˜G also deﬁnes a type 2, 0-1 network.
Proof: Clearly ˜G deﬁnes a 0-1 network. What remains to be shown is that in
˜G, for every vertex v, there is either one emanating edge or only one entering
edge. If there is no ﬂow through v (per f), then, in ˜G, v has exactly the same
incident edges, and the condition continues to hold. If the ﬂow going through v
is 1, (clearly, it cannot be more) assume that it enters via e1 and leaves via e2.
In ˜G, neither of these two edges appears, but two edges e′
1 and e′
2 are added,
which have directions opposite to e1 and e2, respectively. The other edges of G
that are incident to v remain intact in ˜G. Thus, the numbers of incoming edges
and outgoing edges of v remain the same. Since G is of type 2, so is ˜G.
■
Theorem 6.3 For a 0-1 network of type 2, Dinitz’s algorithm is of time
complexity O(|V|1/2 · |E|).
Proof: If M ⩽|V|1/2, then the number of phases is bounded by |V|1/2, and
the result follows. Otherwise, consider the phase during which the total ﬂow
reaches the value M −|V|1/2. Therefore, the layered network for this phase is
constructed when F < M −|V|1/2. This layered network is identical with the
ﬁrst for ˜G, with zero ﬂow everywhere. Also, by Lemma 6.5, ˜G is of type 2. Thus,
by Lemma 6.4, the length l of the layered network is at most (|V|−2)/ ˜M+1.
Now, ˜M = M −F > M −(M −|V|1/2) = |V|1/2. Thus,
l ⩽|V| −2
|V|1/2 + 1 = O(|V|1/2).
Therefore, the number of phases up to this one is at most O(|V|1/2). Since the
number of phases to completion is at most |V|1/2 more, the total number of
phases is at most O(|V|1/2).
■
6.2 Vertex Connectivity of Graphs
Intuitively, the connectivity of a graph is the minimum number of elements
whose removal from the graph disconnect it. There are four cases. We may

122
6 Applications of Network Flow Techniques
discuss undirected graphs or digraphs; we may discuss the elimination of edges
or vertices. We start with the problem of determining the vertex-connectivity
of an undirected graph. The other cases, which are simpler, are discussed in the
next section.
Let G(V,E) be a ﬁnite undirected graph, with no self-loops and no parallel
edges. A set of vertices, S, is called an (a,b) vertex separator if {a,b} ⊆V \S,
and every path connecting a and b passes through at least one vertex of S.
Clearly, if a and b are connected by an edge, no (a,b) vertex separator exists.
Let a—
̸
b mean that there is no such edge. In this case, let N(a,b) be the
least cardinality of an (a,b) vertex separator. Also, let p(a,b) be the maximum
number of pairwise vertex-disjoint paths connecting a and b in G; clearly, all
these paths share the two end-vertices, but no other vertex appears on more than
one of them.
Theorem 6.4 If a—
̸
b then N(a,b) = p(a,b).
This is one of the variations of Menger’s theorem [2]. It is not only rem-
iniscent of the max-ﬂow min-cut theorem, but can be proved by it. Dantzig
and Fulkerson [3] pointed out how this can be done, and we shall follow their
approach.
Proof: Construct a digraph ¯G( ¯V, ¯E) as follows. For every v ∈V put two vertices
v′ and v′′ in ¯V with an edge v′ ev
−→v′′. For every edge u e v in G, put two
edges u′′ e′
−→v′ and v′′ e′′
−→u′ in ¯G. Deﬁne now a network, with digraph ¯G,
source a′′, sink b′, unit capacities for all the edges of the ev type (let us call
them internal edges), and inﬁnite capacity for all the edges of the e′ and e′′
type (called external edges). For example, in Figure 6.1(b) the network for G,
as shown in Figure 6.1(a), is demonstrated.
We now claim that p(a,b) is equal to the total maximum ﬂow F (from a′′ to
b′) in the corresponding network. First, assume we have p(a,b) vertex disjoint
paths from a to b in G. Each such path, a e1 v1 e2 v2 e3 ... el−1 vl−1 el b,
indicates a direct path in ¯G:
a′′ e′
1
−→v′
1
ev1
−→v′′
1
e′
2
−→v′
2
ev2
−→v′′
2
e′
3
−→...
e′
l−1
−→v′
l−1
evl−1
−→v′′
l−1
e′
l
−→b′
These directed paths are vertex-disjoint, and each can be used to ﬂow one
unit from a′′ to b′. Thus,
F ⩾p(a,b).

6.2 Vertex Connectivity of Graphs
123
a
(a)
(b)
b
∞
∞
∞
b′
b′′
1
1
1
1
∞
∞
∞
∞
∞
∞
∞
a′
a′′
Figure 6.1: Construction in the proof of Theorem 6.4
Next, assume f is a ﬂow function which achieves a maximum total ﬂow F in
the network. We may assume that f(e) is either zero or one, for every e ∈¯E.
This follows from the fact that one can use the Ford and Fulkerson algorithm,
or the Dinitz algorithm, in which the ﬂow is always integral. Also, the edges
with inﬁnite capacity enter a vertex with a single outgoing edge whose capacity
is one and which must satisfy the conversation rule (C2), or they emanate from
a vertex with only one incoming edge of unit capacity which is subject to C2;
thus, the ﬂow through them is actually bounded from above by one. (We have
assigned them inﬁnite capacity for convenience reasons, which will become
clear shortly.) Therefore, the total ﬂow F can be decomposed to paths, each
describing the way that one unit reaches b′ from a′′. These paths are vertex-
disjoint, since the ﬂow through v′ or v′′, if v /∈{a,b}, is bounded by one. Each
indicates a path in G. These F paths in G are vertex-disjoint too. Thus,
F ⩽p(a,b).
We conclude that F = p(a,b).
By the max-ﬂow min-cut theorem, F is equal to the capacity c(S) of some cut
deﬁned by some S ⊂¯V, such that a′′ ∈S and b′ /∈S. Since
c(S) =

e∈(S; ¯S)
c(e),
the set (S;¯S) consists of internal edges only. Now, every directed path from
a′′ to b′ in ¯G uses at least one edge of (S;¯S). Thus, every path from a to b
in G uses at least one vertex v such that ev ∈(S;¯S). Therefore, the set R =
{v | v ∈V and e′
v ∈(S;¯S)} is an (a,b) vertex separator. Clearly |R| = c(S).

124
6 Applications of Network Flow Techniques
Thus, we have an (a,b) vertex separator whose cardinality is F. Proving that
N(a,b) ⩽F ⩽p(a,b).
Finally, it is easy to see that N(a,b) ⩽p(a,b), since every path from a
to b uses at least one vertex of the separator, and no two paths can use the
same one.
■
The algorithm suggested in the proof, for ﬁnding N(a,b), when the Dinitz
algorithm is used to solve the network problem, is of time complexity
O(|V|1/2·|E|). This results from the following considerations. The number of
vertices in ¯G is 2|V|; the number of edges is |V|+2|E|. Assuming |E| ⩾|V|, we
have | ¯V| = O(|V|) and |¯E| = O(|E|). Since we can assign unit capacity to all the
edges without changing the maximum total ﬂow, the network is of type 2. By
Theorem 6.3, the algorithm is of time complexity O(|V|1/2 · |E|). We can even
ﬁnd a minimum (a,b) vertex separator as follows: Once the ﬂow is maximum,
change the capacity of the external edges back to ∞and apply the construction
of the layered network. The set of vertices which appear also in this layered
network, S, deﬁnes a minimum cut which consists of internal edges only. Let
R be the vertices of G which correspond to the internal edges in (S;¯S). R is a
minimum (a,b) vertex separator in G. This addition work is of time complexity
O(|E|).
The vertex connectivity, c, of an undirected graph G(V,E) is deﬁned as
follows:
(i) If G is completely connected, (i.e., every two vertices are connected by an
edge), then c = |V| −1.
(ii) If G is not completely connected, then
c = min
a—
̸
b
N(a,b).
Lemma 6.6 If G is not completely connected, then
min
a—
̸
b
p(a,b) = min
a,b p(a,b);
namely, the smallest value of p(a,b) occurs also for some two vertices a and
b that are not connected by an edge.
Proof: Let a,b be a pair of vertices such that a e b and p(a,b) is minimum
over all pairs of vertices of the graph. Let G′ be the graph obtained from G by
dropping e. Clearly, the number of vertex disjoint paths connecting a and b in

6.2 Vertex Connectivity of Graphs
125
G′, p′(a,b), satisﬁes
p′(a,b) = p(a,b) −1.
Also, since a—
̸
b in G′, then by Theorem 6.4, there is an (a,b) vertex separator
R in G′ such that p′(a,b) = |R|.
If |R| = |V| −2, then p(a,b) = |V| −1, and p(a,b) cannot be the least of all
{p(u,v) | u,v ∈V}, since for any u—
̸
v, p(u,v) ⩽|V|−2. Hence, |R| < |V|−2.
Therefore, there must be some vertex v ∈V −(R∪{a,b}). Now, without loss of
generality, we may assume that R is also an (a,v) vertex separator (or exchange
a and b). Thus, a—
̸
v in G and R ∪{b} is an (a,v) vertex separator in G. We
now have
p(a,v) ⩽|R| + 1 = p(a,b),
and the lemma follows.
■
Theorem 6.5 c = mina,bp(a,b).
Proof: If G is completely connected, then for every two vertices a and b,
p(a,b) = |V| −1, and the theorem holds. If G is not completely connected,
then, by deﬁnition,
c = min
a—
̸
b
N(a,b).
By Theorem 6.4, mina—
̸
b N(a,b) = mina—
̸
b p(a,b). Now by Lemma 6.6,
mina—
̸
b p(a,b) = mina,b p(a,b).
■
We can use the intermediate result,
c = min
a—
̸
b
p(a,b),
to compute the vertex connectivity of G with time complexity O(|V|5/2 · |E|).
However, a slightly better bound can be obtained.
Lemma 6.7 c ⩽2|E|/|V|.
Proof: The vertex (or edge) connectivity of a graph cannot exceed the degree
of any vertex. Thus,
c ⩽min
v d(v).
Also,

v
d(v) = 2 · |E|.
Thus, minv d(v) ⩽2 · |E|/|V|, and the lemma follows.
■

126
6 Applications of Network Flow Techniques
Procedure VERTEX-CONNECTIVITY(V,E)
1
Order the vertices v1,v2,...,v|V| in such a way that v1 —
̸
v for some v.
2
γ ←∞
3
i ←1
4
while i ⩽γ do
5
for every v such that vi —
̸
v do
6
γ ←min{γ,N(vi,v)}
7
return γ.
Algorithm 6.1: Constructing the vertex connectivity of a graph G =
(V,E) that is not completely connected.
A procedure to ﬁnd the vertex connectivity c of a graph G that is not
completely connected is listed in Algorithm 6.1.
Theorem6.6 TheprocedureVERTEX-CONNECTIVITYterminateswith γ=c.
Proof: Clearly, after the ﬁrst computation of N(v1,v) for some v1 —
̸
v,
γ satisﬁes
c ⩽γ ⩽|V| −2.
(6.2)
From there on, γ can only decrease, but (6.2) still holds. Thus, for some
k ⩽|V| −1, the procedure will terminate. When it does, k ⩾γ + 1 ⩾c + 1.
By deﬁnition, c equal to the cardinality of a minimum vertex separator R of
G. Thus, at least one of the vertices v1,v2,...,vk is not in R, say vi. R separates
the remaining vertices into at least two sets, such that each path from a vertex
of one set to a vertex of another passes through at least on vertex of R. Thus,
there exists a vertex v such that N(vi,v) ⩽|R| = c, and therefore γ ⩽c.
■
Clearly, the time complexity of this procedure is O(c · |V|3/2 · |E|). By
Lemma 6.7, this is bounded by O(|V1/2 · |E|2).
If c = 0, then G is not connected.We can use DFS (or BFS) to test whetherthis
is the case in O(|E|) time. If c = 1, then G is separable, and as we saw in Section
3.2, this can be tested also by DFS in O(|E|) time. This algorithm determines
also whether c ⩾2, that is, whether it is nonseparable.Before we discuss testing

6.2 Vertex Connectivity of Graphs
127
for a given k, whether c ⩾k, let us consider the following interesting theorem
about equivalent conditions for G to be nonseparable.1
Theorem 6.7 Let G(V,E) be an undirected graph with |V| > 2 and no isolated
vertices. 2 The following six statements are equivalent.
1. G is nonseparable.
2. For every two vertices x and y there exists a simple circuit which goes
through both.
3. For every two edges e1 and e2 there exists a simple circuit which goes through
both.
4. For every two vertices x and y and an edge e there exists a simple path from
x to y which goes through e.
5. For every three vertices x, y and z there exists a simple path from x to z
which goes through y.
6. For every three vertices x, y and z there exists a simple path from x to z
which avoids
Proof: First we prove that (1) is equivalent to (2).
(1) ⇒(2): Since G is nonseparable, c ⩾2. By Theorem 6.5, for every two
vertices x and y p(x,y) ⩾2; thus, there is a simple circuit that goes through x
and y.
(2) ⇒(1): There cannot exist a separation vertex in G, since every two vertices
lie on some common simple circuit.
Next, let us show that (1) and (3) are equivalent.
(1) ⇒(3): From G construct G′ as follows. Remove the edges u1 e1
−→v1
and u2 e2
−→v2 (without removing any vertices). Add two new vertices, x and
y, and four new edges: u1
x
v1, u2
y
v2. Clearly, none of the
old vertices become separation vertices by this change. Also, x cannot be a
separation vertex, or either u1 or v1 are separation vertices in G. (Here, |V| > 2
is used.) Thus, G′ is nonseparable. Hence, by the equivalence of (1) and (2),
G′ satisﬁes (2). Therefore, there exists a simple circuit in G′ that goes through
x and y. This circuit indicates a circuit through e1 and e2 in G.
(3) ⇒(1): Let x and y be any two vertices. Since G has no isolated vertices,
there is an edge e1 incident to x and an edge e2 incident to y. (If e1 = e2, choose
any other edge to replace e2; the replacement need not even be incident to y; the
replacement exists since there is at least one other vertex, and it is not isolated.)
1 Many authors use the term “biconnected” to mean nonseparable. I prefer to call a graph
biconnected if c = 2.
2 Namely, for every v ∈V,d(v) > 0. G has been assumed to have no self-loops.

128
6 Applications of Network Flow Techniques
By (3) there is a simple circuit through e1 and e2, and therefore a circuit through
x and y. Thus, (2) holds, and (1) follows.
Now, let us prove that (3) ⇒(4) ⇒(5) ⇒(6) ⇒((3)).
(3) ⇒(4): Since (3) holds, the graph G is nonseparable. Add a new edge
x e′
y, if such does no exist already in G. Clearly, the new graph G′, is still
nonseparable. By the equivalence of (1) and (3), G′ satisfy statement (3). Thus,
there is a simple circuit which goes through e and e′ in G′. Therefore, there is
a simple path in G from x to y through e.
(4) ⇒(5): Let e be an edge incident to vertex y; such an edge exists, since
there are no isolated vertices in G. By (4), there is a simple path from x to z
through e. Thus, this path goes through y.
(5) ⇒(6): Let p be a simple path which goes from x to y through z; such a
path exists, since (5) holds for every three vertices. The ﬁrst part of p, from x
to z does not pass through y.
(6) ⇒(1): If (6) holds, then there cannot be any separation vertex in G.
■
Let us now return to the problem of testing the vertex connectivity of a given
graph G; that is, testing whether c is greater than or equal to a given positive
integer k. We have already seen that for k = 1 and 2, there is an O(|E|) algorithm.
Hopcroft and Tarjan [4] showed that k = 3 can also be tested in linear time, but
their algorithm is quite complicated and does not seem to generalize for higher
values of k. Let us present a method suggested by Kleitman [5] and improved
by Even [6].
Let L = {v1,v2,...,vl} be a subset of V, where l ⩾k. Deﬁne ˜G as follows: ˜G
includes all the vertices and edges of G. In addition, it includes a new vertex
s connected by an edge to each of the vertices of L; ˜G is called the auxiliary
graph.
Lemma 6.8 Let u ∈V −L. If p(vi,u) ⩾k in G, for every vi ∈L, then, in ˜G,
p(s,u) ⩾k.
Proof: Assume not. Then p(s,u) < k. By Theorem 6.4, there exists a (s,u)
vertex separator S in ˜G such that |S| < k. Let R be the set of vertices such that all
paths in ˜G from s to v ∈R pass through at least one vertex of S. Clearly, vi /∈R,
since vi is connected by an edge to s. However, since l ⩾k > |S|, there exists
some 1 ⩽i ⩽l such that vi /∈S. All paths from vi to u go through vertices of
S. Thus, p(vi,u) ⩽|S| < k, contradicting the assumption.
■
Let V = {v1,v2,...,vn}. Let j be the least integer such that for some i < j,
p(vi,vj) < k in G.

6.3 Connectivity of Digraphs and Edge Connectivity
129
Lemma 6.9 Let j be as deﬁned above and ˜G be the auxiliary graph for L =
{v1,v2,...,vj−1}. In ˜G, p(s,vj) < k
Proof: Consider a minimum (vi,vj) vertex separator S. By Theorem 6.4,
|S|<k. Let R be the set of all vertices v ∈V such that all the paths from vi
to v in G pass through vertices of S. Clearly, vj ∈R. If for some j′ < j, vj′ ∈R,
then p(i,j′) ⩽|S| < k, and the choice of j is wrong. Thus, vj is the least vertex
in R (i.e. the vertex for which the subscript is minimum). Hence, L ∩R = ∅.
Thus, in ˜G, S is an (s,vj) vertex separator, and p(s,vj) < k.
■
The following algorithm determines whether the vertex connectivity of a
given undirected graph G(V,E), where V = {v1,v2,...,vn}, is at least k.
1. For every i and j such that 1 ⩽i < j ⩽k, check whether p(vi,vj) ⩾k. If
for some i and j this test fails then halt; G’s connectivity is less than k.
2. For every j such that k+1 ⩽j ⩽n, form ˜G (with L = {v1,v2,...,vj−1}), and
check whether, in ˜G, p(s,vj) ⩾k. If for some j this test fails, then halt; G’s
connectivity is less than k.
3. Halt; the connectivity of G is at least k.
The proof for the algorithm’s validity is as follows: If G’s connectivity is at
least k, then clearly no failure willbedetectedinStep(6.2). Also, by Lemma 6.8,
no failure will occur in Step (6.2), and the algorithm will halt in Step (6.2) with
the right conclusion. If G’s connectivity is less than k, and it is not detected
directly in Step (6.2) then, by Lemma 6.9, it will be detected in Step (6.2).
Step (6.2) takes O(k3 · |E|) steps, since we have to solve k(k −1)/2 ﬂow
problems. In each we have to ﬁnd k augmenting paths, and each path takes
O(|E|) steps to ﬁnd.
Step (6.2) takes O(k · |V| · |E|) steps, since we have to solve |V| −k ﬂow
problems, again for each up to total ﬂow k.
Thus, if k ⩽|V|1/2 then the time complexity of the algorithm is O(k·|V|·|E|).
The readers who are familiar with the interesting result of Gomory and Hu [7]
for ﬁnding all |V|·(|V|−1) total ﬂows, for all source-sink pairs in an undirected
network by solving only |V|−1 network ﬂow problems, should notice that this
technique is of no help in our problem. The reason for that is that even if G is
undirected, the network we get for vertex connectivity testing is directed.
6.3 Connectivity of Digraphs and Edge Connectivity
First, let us consider the problem of vertex-connectivity of a digraph G(V,E).
The deﬁnition of an (a,b) vertex separator is the same as in the undirected case,

130
6 Applications of Network Flow Techniques
except that now all we are looking at are directed paths from a to b; i.e., an
(a,b) vertex separator is a set of vertices S such that {a,b} ∩S = ∅and every
directed path from a to b passes through at least one vertex of S. Accordingly,
N(a,b) and p(a,b) are deﬁned. The theorem analogous to Theorem 6.4, still
holds, except that the algorithm is even simpler: For every edge u e
−→v in G
there is only one edge u′′ e′
−→v′ in ¯G.
The vertex connectivity, c, of a digraph G(V,E) is deﬁned as follows:
(i) If G is completely connected, (i.e., for every two vertices a and b, there
are edges a −→b and b −→a), then c = |V| −1.
(ii) If G is not completely connected, then
c = min
a—
̸
b
N(a,b).
The lemma analogous to Lemma 6.6 still holds, and the proof goes along
the same lines. Also, the theorem analogous to Theorem 6.5 holds, and the
complexity it yields is the same. If G has no parallel edges, a statement like
Lemma 6.7 holds, and the procedure and the proof of its validity (Theorem 6.6)
extend to the directed case, except that for each vi, we compute both N(vi,v)
and N(v,vi).
The algorithm for testing k connectivity extends also to the directed case, and
again all we need to change is that whenever p(a,b) was computed, we now
have to compute both p(a,b) and p(b,a).
Let us nowconsider the case of edgeconnectivity both in graphs and digraphs.
Let G(V,E) be an undirected graph. A set of edges, T, is called an (a,b) edge
separator if every path from a to b passes through at least one edge of T. Let
M(a,b) be the least cardinality of an (a,b) edge separator. Let p(a,b) be now
the maximum number of edge disjoint paths which connect a with b.
Theorem 6.8 M(a,b) = p(a,b).
The proof is similar to that of Theorem 6.4, only simpler. There is no need
to split vertices. Thus, in ¯G, ¯V = V. We still represent each edge u
v of
G by two edges u e′
−→v and v e′′
−→u in ¯G. There is no loss of generality in
assuming that the ﬂow function in ¯G satisﬁes the condition that either f(e′) = 0
or f(e′′) = 0; for if f(e′) = f(e′′) = 1 then replacing both by 0 does not change
the total ﬂow. The rest of the proof raises no difﬁculties.
The edge connectivity, c, of a graph G is deﬁned by c = mina,b M(a,b). By
Theorem 6.8 and its proof, we can ﬁnd c by the network ﬂow technique. The

6.3 Connectivity of Digraphs and Edge Connectivity
131
networks we get are of type 1. Both Theorem 6.1 and Theorem 6.2 apply. Thus,
each network ﬂow problem is solvable by Dinitz’s algorithm with complexity
O(min{|E|3/2,|V|2/3 · |E|}).
Let T be a minimum edge separator in G; that is, |T| = c. Let v be any vertex
of G. For every vertex v′, on the other side of T, M(v,v′) = c. Thus, in order
to determine c, we can use
c =
min
v′∈V−{v}M(v,v′).
We need to solve at most |V|−1 network ﬂow problems. Thus, the complexity
of the algorithm is O(|V| · |E| · min{|E|1/2,|V|2/3}).
In the case of edge connectivity of digraphs, we need to consider directed
paths. The deﬁnition of an (a,b) edge separator is accordingly a set of edges,
T, such that every directed path from a to b uses at least one edge of T. The
deﬁnition of p(a,b) again uses directed paths, and the proof of the statement
analogousto Theorem6.8 is the easiest of all, since ¯G is now G with no changes.
In the deﬁnition of c, the edge connectivity, we need the following change:
c = min{M(a,b) | (a,b) ∈V × V},
namely, we need to consider all ordered pairs of vertices.
The networks we get are still of type 1andthecomplexity of each is still O(|E|·
min{|E|1/2,|V|2/3}). The approach of testing for one vertex v, both M(v,v′) and
M(v′,v) for all v′ ∈V −{v} still works, to yield the same complexity: O(|V| ·
|E| · min{|E|1/2,|V|2/3}). However, the same result, with an improvement only
in the constant coefﬁcient follows from the following interesting observation of
Schnorr[8], which applies both to the directed and undirected edgeconnectivity
problems.
Lemma 6.10 Let v1,v2,...,vn be a circular ordering (i.e. vn+1 = v1) of the
vertices of a digraph G. The edge connectivity, c, of G satisﬁes
c = min
1⩽i⩽nM(vi,vi+1).
Proof: Let T be a minimum edge separator in G. That means that there are two
vertices a and b such that T is an (a,b) edge separator. Deﬁne
L = {v | there is a directed path from a to v which avoids T}.
R = {v | there is no directed path from a to v which avoids T}.

132
6 Applications of Network Flow Techniques
Clearly, L ∪R = V and L ∩R = ∅. Let l ∈L and r ∈R. T is an (l,r) edge
separator; for if it is not, then r belongs in L. Therefore, M(l,r) ⩽|T|. Since
T is a minimum edge separator, M(l,r) = |T|. Now neither L nor R are empty,
since they contain a and b, respectively.
Consider now the circular ordering of V. There must be an i, 1 ⩽i ⩽n, such
that vi ∈L and vi+1 ∈R. Hence, the result.
■
In the case of graphs and digraphs we can test for k connectivity, easily, in
time complexity O(k·|V|·|E|). Instead of running each network ﬂow problem
to completion, we terminate it when the total ﬂow reaches k. Each augmenting
path takes O(|E|) time and there are |V| ﬂow problems. As we can see, testing
for k edge connectivity is much easier than for k vertex connectivity.The reason
is that vertices cannot participate in the separating set which consists of edges.
We can also use this approach to determine the edge connectivity, c, in time
O(c · |V| · |E|). We run all the |V| network ﬂow problems in parallel, one aug-
menting path for each network in turn. When no augmenting path exists in any
of the |V| problems, we terminate. The cost increase is only in space, since we
need to store all |V| problems simultaneously. One can use binary search on c to
avoid this increase in space requirements, but in this case the time complexity
is O(c · |V| · |E| · logc).
We conclude our discussion of edge connectivity with a very powerful
theorem of Edmonds [9]. The proof presented here is due to Lovász [10].
Theorem 6.9 Let a be a vertex of a digraph G(V,E) and minv∈V−{a}
M(a,v) = k. There are k edge-disjoint directed spanning trees of G rooted
at a.
Proof: Thetheoremtrivially holdsfor k =1.Weprovethetheorembyinduction
on k. Let us denote by δG(S) the number of edges in (S;¯S) in G. If H is a
subgraph of G then G −H is the digraph resulting from the deletion of all the
edges of H from G.
Clearly, the condition that minv∈V−{a} M(v,a) ⩾k is equivalent to the
statement that, for every S ⊂V, S ̸= V and a ∈S, δG(S) ⩾k.
Let F(V ′,E′) be a subgraph of G such that
(i) F is a directed tree rooted at a (which is not necessarily spanning);
(ii) For every S ⊂V, S ̸= V and a ∈S, δG−F(S) ⩾k −1.
If F is spanning directed tree then we get the result immediately; by the
inductive hypothesis there are k−1 edge-disjoint spanning trees rooted at a in
G −F, and F is one more.

6.3 Connectivity of Digraphs and Edge Connectivity
133
The crux of the proof is to show that if F is not spanning then an edge of the
set (V ′; ¯V ′) can be added to F, to increase its number of vertices by one and
still satisfy both (i) and (ii).
Consider the following three conditions on a subset of vertices, S:
(1) a ∈S,
(2) S ∪V ′ ̸= V,
(3) δG−F(S) = k −1.
Let us show that if no such S exists, then one can add any edge e ∈(V ′; ¯V ′)
to F. Clearly, F+e satisﬁes (i). Now, if (ii) does not hold, then there exists an S
such that S ̸= V, a ∈S, and δG−(F+e)(S) < k−1. It follows that δG−F(S)<k.
Now, by (ii), δG−F(S) ⩾k −1. Thus, δG−F(S) = k −1, and S satisﬁes condi-
tion ((3)). Let u and v be vertices such that u e
−→v. Since δG−(F+e)(S) < k−1
and δG−F(S) = k −1, v /∈S. Also, v /∈V ′. Thus, S ∪V ′ ̸= V, satisfying
condition ((2)). Therefore, S satisﬁes all three conditions; A contradiction.
Now, let A be a maximal 3 set of vertices that satisﬁes ((1)), ((2)), and ((3)).
Since the edges of F all enter vertices of V ′,
δG−F(A ∪V ′) = δG(A ∪V ′) ⩾k.
By condition ((3)),
δG−F(A ∪V ′) > δG−F(A).
The inequality implies that there exists an edge x e
−→y that belongs to (A ∪
V ′;A ∪V ′) and does not belong to (A; ¯A) in G −F. Hence, x ∈¯A ∩V ′ and
y ∈¯A∩¯V ′. Clearly, F+e satisﬁes (i). It remains to be shown that it satisﬁes (ii).
Let S ⊂V, S ̸= V and a ∈S. If e /∈(S;¯S), then
δG−(F+e)(S) = δG−F(S) ⩾k −1.
Assume e ∈(S;¯S). It is not hard to prove that for every two subsets of V, S,
and A,
δG−F(S ∪A) + δG−F(S ∩A) ⩽δG−F(S) + δG−F(A),
by considering the sets of edges connecting S ∩A, S ∩¯A, ¯S ∩A, and ¯S ∩¯A.
Now, δG−F(A) = k −1 and δG−F(S ∩A) ⩾k −1. Therefore,
δG−F(S ∪A) ⩽δG−F(S).
3 Namely, no larger set that contains A has all three properties.

134
6 Applications of Network Flow Techniques
Since x ∈S and x /∈A, S ⊈A; namely, S∪A is larger than A. Also, y ∈¯S, y ∈¯A,
and y ∈¯V ′. Thus, (S∪A)∪V ′ ̸= V. By the maximality of A, δG−F(S∪A) ⩾k.
This implies that δG−F(S)⩾k; thereforeδG−(F+e)(S)⩾k−1, proving (ii). ■
The proof provides an algorithm for ﬁnding k edge-disjoint directed trees
rooted at a. We look for a tree F such that minv∈V−{a}M(a,v) ⩾k −1 in
G −F, by adding to F one edge at a time. For each candidate edge e, we have
to check whether minv∈V−{a}M(a,v) ⩾k −1 in G −(F + e). This can be
done by solving |V| −1 network ﬂow problems, each of complexity O(k · |E|).
Thus, the test for each candidate edge is O(k · |V| · |E|). No edge needs be
considered more than once in the construction of F, yielding the timecomplexity
O(k · |V| · |E|2). Since we repeat the construction k times, the whole algorithm
is of time complexity O(k2 · |V| · |E|2).
The following theorem was conjectured by Y. Shiloach and proved by Even,
Garey, and Tarjan [11].
Theorem 6.10 Let G(V,E) be a digraph whose edge connectivity is at least k.
For every two vertices, u and v, and every l, 0 ⩽l ⩽k, there are l directed paths
from u to v and k −l directed paths from v to u, which are all edge disjoint.
Proof: Construct an auxiliary graph ¯G by adding to G a new vertex a, l parallel
edges from a to u, and k −l parallel edges from a to v. Let us ﬁrst show that
in ¯G minw∈V M(a,w) = k.
If minw∈V M(a,w) < k, then there exists a set S such that S ⊂V ∪{a}, a ∈S,
and |(S;¯S)| < k in ¯G. Clearly, S ̸= {a} for |({a};V)| = k. Let x ∈S −{a}. Thus,
M(x,y) < k for every y ∈¯S, and the same also holds in G, since G is a sub-
graph of ¯G. This contradicts the assumption that in G, the edge connectivity is
at least k.
Now, by Theorem 6.9, there are k edge-disjoint directed spanning trees, in
¯G, rooted at a. Exactly one edge out of a appears in each tree. Thus, each
of the trees that uses an edge a −→u contains a directed path from u to v,
and each of the trees that uses an edge a −→v contains a directed path from
v to u. All these paths are, clearly, edge disjoint.
■
Corollary 6.1 If the edge connectivity of a digraph is at least 2, then for every
two vertices u and v there exists a directed circuit that goes through u and v
in which no edge appears more than once.
Proof: Use k = 2, l = 1 in Theorem 6.10.
■
It is interesting to note that no such easy result exists in the case of vertex
connectivity and a simple directed circuit through given two vertices. In [11],

6.4 Maximum Matching in Bipartite Graphs
135
a digraph with vertex connectivity 5 is shown such that for every two of its
vertices there is no simple directed circuit that passes through both. The author
does not know whether any vertex connectivity will guarantee the existence of
a simple directed circuit through any two vertices.
6.4 Maximum Matching in Bipartite Graphs
A set of edges, M, of a graph G(V,E) with no self-loops, is called a matching
if every vertex is incident to at most one edge of M. The problem of ﬁnding a
maximum matching was ﬁrst solved in polynomial time by Edmonds [12]. The
best known result of Even and Kariv [13] is O(|V|2.5). These algorithms are too
complicated to be included here, and they do not use network ﬂow techniques.
An easier problem is to ﬁnd a maximum matching in a bipartite graph, that is,
a graph in which V = X∪Y, X∩Y = ∅, and each edge has one end-vertex in X
and one in Y. This problem is also known as the marriage problem. We present
here its solution via network ﬂow and show that its complexity is O(|V|1/2 ·|E|).
This result was ﬁrst achieved by Hopcroft and Karp [14].
Let us construct a network N(G). Its digraph ¯G( ¯V, ¯E) is deﬁned as follows:
¯V = {s,t} ∪V,
¯E = {s −→x|x ∈X} ∪{y −→t|y ∈Y} ∪{x −→y|x
y in G}.
Let c(s −→x) = c(y −→t) = 1 for every x ∈X and y ∈Y. For every edge
x e
−→y let c(e) = ∞. (This inﬁnite capacity is deﬁned in order to simplify our
proof of Theorem 6.12. Actually, since there is only one edge entering x, with
unit capacity, the ﬂow in x −→y is bounded by 1.) The source is s and the
sink is t. For example, consider the bipartite graph G shown in Figure 6.2(a).
Its corresponding network N(G) is shown in Figure 6.2(b).
Theorem 6.11 The number of edges in a maximum matching of a bipartite
graph G is equal to the maximum ﬂow, F, in its corresponding network, N(G).
Proof: Let M be a maximum matching. For each edge x −→y of M, use the
directed path s −→x −→y −→t to ﬂow one unit from s to t. Clearly, all these
paths are vertex-disjoint. Thus, F ⩾|M|.
Let f be a ﬂow function on N(G), which is integral. (There is no loss of
generality here. As we saw, in Chapter 5, every network with integral capacities
has a maximum integral ﬂow.) All the directed paths connecting s and t are of
the form s −→x −→y −→t. If such a path is used to ﬂow (one unit) from s

136
6 Applications of Network Flow Techniques
x1
x3
x4
x5
x2
y1
y2
(a)
(b)
y3
y4
∞
1
1
1
1
1
1
1
1
1
∞
∞
∞
∞
∞
∞
∞
s
t
x1
x3
x4
x5
x2
y1
y2
y3
y4
Figure 6.2: Construction of N(G).
∞,1
∞,1
1,1
1,1
1,1
1,1
1,1
∞,1
∞,0
∞,0
∞,0
∞,1
∞,0
1,1
1,1
1,1
1,1
s
t
x1
x3
x4
x5
x2
y1
y2
y3
y4
x1
x3
x4
x5
x2
y1
y2
y3
y4
(a)
(b)
Figure 6.3: Maximum ﬂow found by Dinitz’s algorithm and its
corresponding matching.
to t, then no other edge x −→y′ or x′ −→y can carry ﬂow, since there is only
one edge s −→x and its capacity is one, and the same is true for y −→t. Thus,
the set of edges x −→y, for which f(x −→y) = 1, constitutes a matching in
G. Thus, |M| ⩾F.
■
The proof indicates how the network ﬂow solution can yield a maximum
matching. for our example, a maximum ﬂow, found by Dinitz’s algorithm is
shown in Figure 3(a) and its corresponding matching is shown in Figure 3(b).
The algorithm described in the proof constructs a type 2 network; therefore, by
Theorem 6.3, its running time is O(|V|1/2 · |E|).

6.5 Two Problems on PERT Digraphs
137
Next, let us show that one can also use the max-ﬂow min-cut theorem to prove
a theorem of Hall’s [15]. For every A ⊆X, let Γ(A) denote the set of vertices
(all in Y) that are connected by an edge to a vertex of A. A matching M, is
called complete if |M| = |X|.
Theorem 6.12 A bipartite graph G has a complete matching M if and only if
for every A ⊂X, |Γ(A)| ⩾|A|.
Proof: Clearly, if G has a complete matching M, then each x has a unique
“mate” in Y. Thus, for every A ⊂X, |Γ(A)| ⩾|A|.
Assume now that G does not have a complete matching. Let S be the set of
labeled vertices (in the Ford and Fulkerson algorithm or the Dinitz algorithm)
upon termination. Clearly, the maximumtotal ﬂow is equal to |M|, but |M| < |X|.
Let A = X ∩S. Since all the edges of the type x −→y are of inﬁnite capacity,
Γ(A) ⊂S. Also, no vertex of Y \Γ(A) is labeled, since there is no edge from a
labeled vertex to it. We have
(S;¯S) = ({s};X −A) ∪(Γ(A);{t}).
Since (S;¯S) = |M| < |X|, we get
|X −A| + |Γ(A)| < |X|,
which implies |Γ(A)| < |A|.
■
6.5 Two Problems on PERT Digraphs
The Program Evaluation and Review Technique, commonly abbreviated PERT,
is a model for project management. A PERT digraph is a ﬁnite digraph G(V,E)
with the following properties:
(i) There is a vertex s, called the start-vertex, and a vertex t(̸= s), called the
termination vertex.
(ii) G has no directed circuits.
(iii) Every vertex v ∈V \ {s,t} is on some directed path from s to t.
A PERT digraph has the following interpretation: Every edge represents a
process. Recall that α(v) denotes the set of edges that enter v; β(v) denotes
the set of edges that emanate from v. All the processes which are represented
by edges of β(s) can be started right away. For every vertex v, the process
represented by edges of β(v) can be started when all the processes represented
by edges of α(v) are completed.

138
6 Applications of Network Flow Techniques
Our ﬁrst problem deals with the question of how soon can the whole project
can be completed; that is, what is the shortest time, from the moment the pro-
cesses represented by β(s) are started, until all the processes represented by
α(t) are completed. We assume that the resources for running the processes are
unlimited. For this problem to be well deﬁned, let us assume that each e ∈E has
an assigned length l(e), which speciﬁes the time it takes to execute the process
represented by e. The minimum completion time can be found by the following
algorithm:
1. Assign s the label 0 (λ(s) ←0). All other vertices are “unlabeled.”
2. Find a vertex v such that v is unlabeled and all edges of α(v) emanate from
labeled vertices. Assign
λ(v) ←max
e∈α(v){λ(u) + l(e)}.
3. If v = t, halt; λ(t) is the minimum completion time. Otherwise, go to
Step (2).
In Step (2), the existence of a vertex v such that all the edges of α(v) emanate
from labeled vertices is guaranteed by conditions (i) and (iii): If no unlabeled
vertex satisﬁes the condition, then for every unlabeled vertex v, there is an
incoming edge which emanates from another unlabeled vertex. By repeatedly
tracing back these edges, one ﬁnds a directed circuit. Thus, if no such vertex is
found, then we conclude that either (i) or (ii) does not hold.
It is easy to prove by induction on the order of labeling, that λ(v) is the
minimum time in which all processes represented by the edges of α(v) can be
completed.
The time complexity of the algorithm can be kept down to O(|E|) as follows:
For each vertex v we keep count of its incoming edges from unlabeled vertices;
this count is initially set to din(v); each time a vertex u gets labeled, we use
the list β(u) to decrease the count for all v such that u −→v, accordingly; once
the count of a vertex v reaches 0, it enters a queue of vertices to be labeled.
Once the algorithm terminates, by going back from t to s, via the edge that
determined the label of the vertex, we can trace a longest path from s to t. Such
a path is called critical.4 Clearly, there may be more than one critical path. If
one wants to shorten the completion time, λ(t), then on each critical path at
least one edge length must be shortened.
4 The whole process is sometimes called the critical path method (CPM).

6.5 Two Problems on PERT Digraphs
139
s
t
Figure 6.4: Example of a PERT digraph.
Next, we shall consider another problem concerning PERT digraphs, where
there is no reference to edge lengths. Assume that each of the processes, repre-
sented by the edges, uses one processor for its execution. The question is: How
many processors do we need to be sure that no execution will ever be delayed
because of a shortage of processors? We want to avoid such a delay without
relying on the values of l(e)’s either because they are unknown or because they
vary from time to time.
Let us solve a minimum ﬂow problem in the network whose digraph is G,
source s, sink t, lower bound b(e) = 1 for all e ∈E and no upper bound (i.e.,
c(e) = ∞for all e ∈E). Condition (6.5) assures the existence of a legal ﬂow
(see Problem 6.5).
For example, consider the PERT digraph of Figure 6.4. The minimum ﬂow
(which in this case is unique) is shown in Figure 6.5(a), where a maximum cut
is shown too.
A set of edges is called concurrent if for no two edges in the set there is a
directed path that passes through both. Now, let T be the set of vertices that
are labeled in the last attempt to ﬁnd an augmenting path from t to s. Clearly,
t ∈T and s /∈T. The set of edges (¯T;T) is a maximum cut; there are no edges
in (T;¯T), for there is no upper bound on the ﬂow in the edges, and any such
edge would enable to continue the labeling of vertices. Thus, the set (¯T;T) is
concurrent.
If S is a set of concurrent edges, then the number of processors required is,
at least |S|. This can be seen by assigning the edges of S a very large length;
and all the others, a short length. Since no directed path leads from one edge
of S to another, they all will be operative simultaneously. This implies that the
number of processors required is at least |(¯T;T)|.

140
6 Applications of Network Flow Techniques
s
t
s
t
3
3
3
3
2
1
2
(a)
2
1
1
1
1
2
2
2
2
2
1
1
1
1
1
1
(b)
Figure 6.5: (a) The minimum ﬂow in the PERT; (b) decompositions
of the ﬂow into the PERT into F directed paths.
However, the ﬂow can be decomposed into F directed paths from s to t, where
F is the minimum total ﬂow, such that every edge is on at least one such (since
f(e) ⩾1 for every e ∈E). This is demonstratedfor our examplein Figure 6.5(b).
We can now assign to each processor all the edges of one such path. Each such
processor executes the processes represented by the edges of the path in the
order in which they appear on the path. If one process is assigned to more than
one processor, then one of them executes while the others are idle. It follows
that whenever a process that corresponds to u −→v is executable (because all
the processes which correspond to α(u) have been executed), the processor to
which this process is assigned is available for its execution. Thus, F processors
are sufﬁcient for our purpose.
Since F = |(¯T;T)|, by the min-ﬂowmax-cuttheorem,the numberof processors
thus assigned is minimum.
The complexity of this procedureis as follows: We can ﬁnd a legal initial ﬂow
in time O(|V|·|E|), by tracing for each edge a directed path from s to t via this
edge, and ﬂow through it one unit. This path is found by starting from the edge,
and going forward and backward from it untils and t are reached. Next,we solve

6.6 Problems
141
a maximum ﬂow problem, from t to s. Thus, the complexity of the whole pro-
cedure is dominated by the complexity of solving one maximum ﬂow problem.
6.6 Problems
Problem 6.1 Let G(V,E) be an acyclic ﬁnite digraph.5 We wish to ﬁnd a
minimum number of directed vertex-disjoint paths that cover all the vertices;
that is, every vertex is on exactly one path. The paths may start anywhere and
end anywhere, and their lengths are not restricted in any way. A path may be
of zero length; that is, it may consist of one vertex.
1. Describe an algorithm for achieving this goal, and make it as efﬁcient as
possible. (Hint. Form a network as follows:
V ′ = {s,t} ∪{x1,x2,...,x|V|} ∪{y1,y2,...,y|V|}.
E′ = {s −→xi | 1 ⩽i ⩽|V|} ∪{yi −→t | 1 ⩽i ⩽|V|}
∪{xi −→yj | vi −→vj in G}.
The capacity of all edges is 1.
Show that the minimumnumberof paths that cover V in G is equal to |V|−F,
where F is the maximum total ﬂow of the network.)
2. Is the condition that G is acyclic essential for the validity of your algorithm?
Explain.
3. Give the best upper bound you can on the time complexity of your algorithm.
Problem 6.2 This problem is similar to Problem 6.1, except that the paths are
not required to be vertex- (or edge-) disjoint.
1. Describe an algorithm for ﬁnding a minimum number of covering paths.
(Hint. Form a network as follows:
V ′ = {s,t} ∪{x1,x2,...,x|V|} ∪{y1,y2,...,y|V|}.
E′ = {s −→xi | 1 ⩽i ⩽|V|} ∪{yi −→t | 1 ⩽i ⩽|V|}
∪{xi −→yi | 1 ⩽i ⩽|V|} ∪{yi −→xj | vi −→vj in G}.
The lower bound of each xi −→yi edge is 1.
The lower bound of all other edges is 0.
The upper bound of all the edges is ∞. Find a minimum ﬂow from s to t.)
5 A digraph is called “acyclic” if it has no directed circuits.

142
6 Applications of Network Flow Techniques
2. Is the condition that G is acyclic essential for the validity of your algorithm?
Explain.
3. Give the best upper bound you can on the time complexity of your algorithm.
(Hint. O(|V| · |E|) is achievable.)
4. Two vertices u and v are called concurrent if no directed path exists from
u to v or from v to u. A set of concurrent vertices is such that every two in
the set are concurrent. Prove that the minimum number of paths that cover
the vertices of G is equal to the maximum number of concurrent vertices.
(This is Dilworth’s Theorem [16].)
Problem 6.3 1. Let G(X,Y,E) be a ﬁnite bipartite graph. Describe an efﬁcient
algorithm for ﬁnding a minimum set of edges such that each vertex is an
end-vertex of at least one of the edges in the set.
2. Discuss the time complexity of your algorithm.
3. Prove that the size of a minimum set of edges which cover the ver-
tices (as in (1)) is equal to the maximum size of an independent set of
vertices of G.6
Problem 6.4 1. Prove that if G(X,Y,E) is a complete bipartite graph (i.e., for
every two vertices x ∈X and y ∈Y, there is an edge x
y) then the vertex
connectivity of G is
c(G) = min{|X|,|Y|}.
2. Prove that for every k, there exists a graph G such that c(G) ⩾k and G has
no Hamilton path. (See Problem 1.4.)
Problem 6.5 Let M be a matching of a bipartite graph. Prove that there exists
a maximum matching M′ such that every vertex matched in M is matched also
in M′.
Problem 6.6 Let G(V,E) be a ﬁnite acyclic digraph with exactly one vertex
s for which din(s) = 0 and exactly one vertex t for which dout = 0. We say
that the edge a −→b is greater than the edge c −→d if and only if there is a
directed path in G from b to c. A set of edges is called a slice if no edge in it is
greater than another and it is maximal; no other set of edges with this property
contains it. Prove that the following three conditions on a set of edges, P, are
equivalent:
6 A set of vertices of a graph is called independent if there is no edge between two vertices of the
set.

6.6 Problems
143
1. P is a slice.
2. P is an (s,t) edge separator in which no edge is greater than any other.
3. P = (S;¯S) for some {s} ⊂S ⊂V −{t} such that (¯S;S) = ∅.
Problem 6.7 (The problem of a system of distinct representatives [SDR]). Let
S1,S2,...,Sm be ﬁnite sets. A set {e1,e2,...,em} is called an SDR if for every
1 ⩽i ⩽m, ei ∈Si.
1. Describe an efﬁcient algorithm for ﬁnding an SDR, if one exists. (Hint.
Deﬁne a bipartite graph and solve a matching problem.)
2. Prove that an SDR exists if and only if the union of any 1 ⩽k ⩽m of the
sets contains at least k elements.
Problem 6.8 Let π1 and π2 be two partitions of a set of m elements, each
containing exactly r disjoint subsets. We want to ﬁnd a set of r elements such
that each of the subsets of π1 and π2 is represented.
1. Describe an efﬁcient algorithm to determine whether there is such a set of r
representatives.
2. State a necessary and sufﬁcient condition for the existence of such a set,
similar to Theorem 6.12.
Problem6.9 LetG(V,E)beacompletely connecteddigraph (seeProblem1.5);
it is called classiﬁable if V can be partitioned into two nonempty classes, A and
B, such that all the edges connecting between them are directed from A to B.
Let V = {v1,v2,...,vn} where the vertices satisfy
dout(v1) ⩽dout(v2) ⩽... ⩽dout(vn).
Prove that G is classiﬁable if and only if there exists a k < n such that
k

i=1
dout(vi) =
k
2

.
Problem 6.10 Let S be a set of people such that |S| ⩾4. We assume that
acquaintance is a mutual relationship. Prove that if, in every subset of four
people, there is one who knows all the others, then there is someone in S who
knows everybody.
Problem 6.11 In the acyclic digraph shown in Figure 6.6, there are both AND
vertices (designated by ) and OR vertices (designated by ). As in a PERT
network, the edges represent processes, and the edge length is the time the

144
6 Applications of Network Flow Techniques
t
s
4
5
6
3
3
5
4
6
2
6
2
5
4
3
3
5
6
1
Figure 6.6: Graph for Problem 6.11.
process requires. The processes represented by the edges that emanate from
an AND (OR) vertex can be started when all (at least one of) the incoming
processes are (is) completed. Describe an algorithm for ﬁnding the minimum
time from start (s) to reach termination (which depends on the type of t). Apply
youralgorithmon the given network.What is the complexityof youralgorithm?
Problem 6.12 Consider Problem 6.11 on digraphs that are not necessarily
acyclic. Show how to modify the algorithm to solve the problem or conclude
that there is no solution.
Problem 6.13 In a school are n boys and n girls. Each boy knows exactly k
girls (1 ⩽k ⩽n), and each girl knows exactly k boys. Prove that if “knowing"
is mutual, then all the boys and girls can participate in one dance, where every
pair of dancers (a boy and a girl) know each other. Also show that it is always
true that k consecutive dances can be organized so that everyone will dance
once with everyone he or she knows.
Problem 6.14 Prove or disprove the following claim: If the vertex connectivity
of a digraph is at least 2, then for every three vertices x,y, and z, there exists a
simple directed path from x to z via y.
Problem 6.15 Let G be a ﬁnite undirected graph whose edge connectivity is
at least 2. For each one of the following claims, determine whether it is true or
false. Justify your answer.
1. For every three vertices x,y, and z there exists a path in which no edge
appears more than once, from x to z via y.
2. For every three vertices x,y, and z there exists a path in which no edge
appears more than once, from x to z, which avoids y.

6.6 Problems
145
Bibliography
[1] Even, S., Tarjan, R. E., “Network Flow and Testing Connectivity,” SIAM J. on
comput., Vol. 4, 1975, pp. 507–518.
[2] Menger, K., “Zur Aligemeinen Kurventheorie,” Fund. Math., Vol. 10, 1927,
pp. 96–115.
[3] Dantzig, G. B., Fulkerson, D. R., “On the Max-Flow Min-Cut Theorem of
Networks,” Linear Inequalities and Related Systems, Annals of Math. Study 38,
Princeton University Press, 1956, pp. 215–221.
[4] Hopcroft, J., and Tarjan, R. E., “Dividing a Graph into Triconnected Components”,
SIAM J. on Comput., Vol. 2, 1973, pp. 135–158.
[5] Kleitman, D. J., “Methods for Investigating Connectivity of Large Graphs,” IEEE
Trans. on Circuit Theory, CT-16, 1969, pp. 232–233.
[6] Even, S., “Algorithm for Determining whether the Connectivity of a Graph is at
Least k,” Siam J. on Comput., Vol. 4, 1977, pp. 393–396.
[7] Gomory, R. E., and Hu, T. C., “Multi-Terminal Network Flows,” J. of SIAM,
Vol. 9, 1961, pp. 551–570.
[8] Schnorr, C. P., “Multiterminal Network Flow and Connectivity in Unsymmetrical
Networks,” Department of Applied Math, University of Frankfurt, October 1977.
[9] Edmonds, J., “Edge-Disjoint Branchings,” in Combinatorial Algorithms, Courant
Inst. Sci. Symp. 9, R. Rustin, Ed., Algorithmics Press Inc., 1973, pp. 91–96.
[10] Lovász, L., “On Two Minimax Theorems in Graph Theory,” Journal of Combina-
torial Theory, Series B, Vol. 21:2, 1976, pp. 96–103.
[11] Even, S., Garey, M. R., and Tarjan, R. E., “A Note on Connectivity and Circuits
in Directed Graphs,” unpublished manuscript (1977).
[12] Edmonds, J., “Paths, Trees, and Flowers,” Canadian J. of Math., Vol. 17, 1965,
pp. 449–467.
[13] Even, S., and Kariv, O., “An O(n2.5) Algorithm for Maximum Matching in General
Graphs,” 16th Annual Symposium on Foundations of Computer Science, IEEE,
1975, pp. 100–112.
[14] Hopcroft, J., and Karp, R. M., “An O(n5/2) Algorithm for Maximum Matching
in Bipartite Graphs,” SIAM J. on Comput., 1975, pp. 225–231.
[15] Hall, P., “On Representation of Subsets,” J. London Math. Soc. Vol. 10, 1935,
pp. 26–30.
[16] Dilworth, R. P., “A Decomposition Theorem for Partially Ordered Sets,” Ann.
Math., Vol. 51, 1950, pp. 161–166.

7
Planar Graphs
7.1 Bridges and Kuratowski’s Theorem
Consider a graph drawn in the plane in such a way that each vertex is repre-
sented by a point; each edge is represented by a continuous line connecting
the two points that represent its end vertices, and no two lines, which represent
edges, share any points, except in their ends. Such a drawing is called a plane
graph. If a graph G has a representation in the plane that is a plane graph then
it is said to be planar.
In this chapter, we shall discuss some of the classical work concerning planar
graphs. The question of efﬁciently testing whether a given ﬁnite graph is planar
is discussed in the next chapter.
Let S be a set of vertices of a nonseparable graph G(V,E). Consider the
partition of the set V −S into classes, such that two vertices are in the same
class if and only if there is a path connecting them that does not use any vertex
of S. Each such class K deﬁnes a component as follows: The component is a
subgraph H(V ′,E′), where V ′ ⊃K. In addition, V ′ includes all the vertices of S
that are connected by an edge to a vertex of K, in G. Also, E′ contains all edges
of G that have at least one end-vertex in K. An edge u e v, where both u and
v are in S, deﬁnes a singular component ({u,v},{e}). Clearly, two components
share no edges, and the only vertices they can share are elements of S. The
vertices of a component that are elements of S are called its attachments.
In our study we usually use a set S, which is the set of vertices of a simple
circuit C. In this case, we call the components bridges; the edges of C are not
considered bridges.
For example, consider the plane graph over the vertices a,...,k shown in
Figure 7.1. The edges of the plane graph are 1,...,18. Let C be the outside
boundary:
146

7.1 Bridges and Kuratowski’s Theorem
147
c
k
d
b
i
j
a
g
f
e
h
6
3
4
8
10
9
7
12
11
14
15
16
18
17
5
2
1
13
Figure 7.1: Example of a plane graph.
C = a 1 b 2 c 3 ... 6 g 7 a.
The circuit C partitions V \ C into two classes: {i,j,h} and {k}. The set of
bridges is
({e,g},{8}),
({i,j,h,a,e,g},{9,10,11,12,13,14}),
({a,e},{15}), and
({k,b,c,d},{16,17,18}).
The ﬁrst and third bridges are singular.
Lemma 7.1 Let B be a bridge, and a1,a2,a3, three of its attachments.
There exists a vertex v, not an attachment, for which there are three vertex-
disjoint paths in B: P1(v,a1),P2(v,a2) and P3(v,a3). (P(a,b) denotes a path
connecting a to b).
Proof: Let a1 e1 v1,a2 e2 v2, and a3 e3 v3 be edges of B. If any of the
vi’s (i = 1,2,3) is an attachment, then the corresponding edge is a singular
component and is not part of B. Thus, vi ∈K, where K is the class that deﬁnes
B. Hence, there is a simple path P ′(v1,v2) that uses vertices of K only; if v1= v2,
this path is empty. Also, there is a simple path P ′′(v3,v1) that uses vertices of K
only. Let v be the ﬁrst vertexofP ′′(v3,v1) that is also on P ′. Now, let P1(v,a1) be
the part of P ′ that leads from v to v1, concatenated with v1
a1; let P2(v,a2)
be the part of P ′that leads from v to v2, concatenated with v2
a2; let P3(v,a3)
be the part of P ′′ that leads from v to v3, concatenated with v3
a3. It is easy
to see that these paths are disjoint.
■

148
7 Planar Graphs
Let C be a simple circuit of a nonseparable graph G, and B1,B2,...,Bk be
the bridges with respect to C. We say that Bi and Bj interlace if at least one of
the following conditions holds:
1. There are two attachments of Bi, a and b, and two attachments of Bj, c and
d, such that all four are distinct and appear on C in the order a,c,b,d.
2. There are three attachments common to Bi and Bj.
For each bridge Bi, consider the subgraph C+Bi. If any of these graphs is not
planar, then clearly G is not planar. Now, assume all these subgraphs are planar.
In every plane realization of G, C outlines a contour that divides the plane into
two disjoint parts: its inside and outside. Each bridge must lie entirely in one
of these parts. Clearly, if two bridges interlace they cannot be on the same side
of C. Thus, in every plane realization of G, the set of bridges is partitioned into
two sets: those drawn inside C and those drawn outside it. No two bridges in
the same set interlace.
Lemma 7.2 If B1,B2,...,Bm is set of bridges of a nonseparable graph G with
respect to a simple circuit C and the following two conditions are satisﬁed:
1. for every 1 ⩽i ⩽n, C + Bi is planar, and
2. no two bridges interlace,
then C+B1 +B2 +...+Bm has a plane realization in which all these bridges
are inside C.
Proof: We shall only outline the proof. As we go clockwise around C there
must be a bridge Bi such that we encounter all its attachments in some order:
a1,a2,...,at, and no attachment of any other bridge appears between a1 and at
on C. Such a bridge can be found by starting from any attachment a of B1 and
going around, say, clockwise. If before encountering all the attachments of B1
we encounter an attachment of another bridge Bi, then all Bi’s attachments are
between consecutive attachments of B1 that may also belong to Bi. We repeat
the same process on Bi, and so on. Since the number of bridges is ﬁnite, and
those discarded will not “interfere" with the new ones, the process will yield
the desired bridge.
This observation allows a proof by induction. First Bi is drawn, and since no
other bridge uses any of the vertices of C between a1 and at, we can take C′
to be the circuit that describes the part of C from at to a1, clockwise, and the
boundary of Bi from a1 to at to form a simple circuit C′, whose inside is so far
empty. The remaining bridges are also bridges of C′ and, clearly, satisfy (7.2)
and (7.2) with respect to C′.
■

7.1 Bridges and Kuratowski’s Theorem
149
Corollary 7.1 Let G be a nonseparable graph; and C, a simple circuit in G. In
this case G is planar if and only if the bridges B1,B2,...,Bk of G, with respect
to C, satisfy the following conditions:
1. For every 1 ⩽i ⩽k, C + Bi is planar.
2. The set of bridges can be partitioned into two subsets, such that no two
bridges in the same subset interlace.
Let us introduce the coloring of graphs. Here, we consider only 2-coloring
of vertices. A graph G(V,E) is said to be 2-colorable if V can be partitioned
into V1 and V2 in such a way that there is no edge in G with two end vertices
in V1(V2). (Obviously, a 2-colorable graph is bipartite, and vice versa. It is
customary to use the term “bipartite" if the partition is given, and “2-colorable"
if one exists.) The following theorem is due to König [1].
Theorem 7.1 A graph G is 2-colorable if and only if it has no odd length
circuits.
Proof: It is easy to see that if a graph has an odd length circuit, then it is not
2-colorable.In order to provethe converse, we may assume that G is connected,
for if each component is 2-colorable, then the whole graph is 2-colorable.
Let v be any vertex. Let us perform BFS (see Section 1.5.1) starting from v.
There cannot be an edge u
w in G if u and w belong to the same layer, that
is, are the same distance from v. For if such an edge exists then we can display
an odd length circuit as follows: Let P1(v,u) be a shortest path from v to u.
P2(v,w) is deﬁned similarly and is of the same length. Let x be the last vertex
that is common to P1 and P2. The part of P1 from x to u, and the part of P2 from
x to w are of equal length, and together with u
w, they form an odd length
simple circuit.
Now, we can color all the vertices of even distance from v with one color and
all the vertices of odd distance from v with a second color.
■
We can use the concept of 2-colorability to decide whether the bridges
B1,B2,...,Bk, with respect to a simple circuit C, can be partitioned into two
pairwise noninterlacing subsets, as follows: Construct a graph G′ whose ver-
tices are the bridges B1,B2,...,Bk, and two vertices are connected by an edge
if and only if the corresponding bridges in G interlace. Now test whether the
graph G′ is 2-colorable, by giving one vertex color 1 and using some search
technique, such as DFS or BFS to color alternate vertices with different colors

150
7 Planar Graphs
a1
b1
b2
a2
a3
b3
a
(b)
(a)
e
b
d
c
e1
e5
e2
e3
e4
Figure 7.2: The graphs of Kuratowski: (a) K3,3 and (b) K5.
out of {1,2}. If no contradiction arises, a coloring is obtained and thus, a parti-
tion. If a contradiction occurs, there is no 2-coloring, and therefore no partition
of the bridges; in this case, the graph is nonplanar, by Corollary 7.1.
Let us now introduce the graphs of Kuratowski [2]: K3,3 and K5. They are
shown in Figure 7.2(a) and (b), respectively.
K5 is completely connected graph of ﬁve vertices, or a clique of ﬁve vertices.
K3,3 is a completely connected bipartite graph with three vertices on each side.
Lemma 7.3 Neither K3,3 nor K5 is planar.
Proof: First, let us consider K5, and its circuit C: a
b
c
d
e
a. Clearly, there are ﬁve bridges, all singular, corresponding to the edges
e1,e2,e3,e4, and e5. Let us construct G′ as in the preceding discussion. It is
shown in Figure 7.3. For example, the bridge e1 interlaces with e5 and e2, and
so on. Since G′ contains an odd circuit, by Theorem 7.1, it is not 2-colorable,
and the set of bridges of K5 with respect to C is not partitionable. Thus, by
Corollary 7.1, K5 is not planar.
In the case of K3,3, take C: a1
b1
a2
b2
a3
b3
a1.
The bridges a1
b2, a2
b3 and a3
b1 form a triangle in the
corresponding G′.
■
Before we take on Kuratowski’s theorem, we need a few more deﬁnitions and
a lemma.
Let G(V,E) be a ﬁnite nonseparable plane graph with |V| > 2. A face of G is
a maximal part of the plane such that, for every two points x and y in it, there is
a continuous line from x and y that does not share any point with the realization
of G. The contour of each face is a simple circuit of G; if any of these circuits
is not simple, then G is separable. Each of these circuits is called a window.

7.1 Bridges and Kuratowski’s Theorem
151
e1
e5
e2
e4
e3
Figure 7.3: G′ for the proof of Lemma 7.3.
One of the faces is of inﬁnite area. It is called the external face, and its window
is the external window. It is not hard to show that for every window W, there
exists another plane realization, G′, of the same graph, which has the same set
of windows, but in G′, the window W is external. First, draw the graph on a
sphere, maintaining the windows; this can be achieved by projecting each point
of the plane vertically up to the surface on a sphere whose center is in the plane
and its intersecting circle with the plane encircles G. Next, place the sphere on
a plane that is tangent to it, in such a way that a point in the face whose contour
is W is the “north pole,” that is, furthest from the plane. Project each point P
of the sphere (other than the “north pole”) to the plane by a straight line that
starts from the “north pole” and goes through P. The graph is now drawn in the
plane, and W is the external window.
Lemma 7.4 Let G(V,E) be a 2-vertex connected graph with a separating pair
a,b. Let H1,H2,...,Hm be the components with respect to {a,b}. G is planar
if and only if for every 1 ⩽i ⩽m, Hi + (a e b) is planar.
By Hi +(a e b) we mean the subgraph obtained by adding the edge a e b
to Hi.1
Proof: In each Hi, there is a path from a to b, or G is not 2-connected. Also
m > 1. Thus, for each Hi we can ﬁnd a path P from a to b in one of the other
components. If G is planar, so is Hi +P, and therefore Hi +(a e b) is planar.
Now assume that each Hi + (a e b) is planar. For each of these realization
we can assume that e is on the external window. Thus, a planar realization of
G exists, as demonstrated in Figure 7.4 in the case of m = 3.
■
1 Since G is 2-vertex connected, it follows that a,b ∈Hi. By deﬁnition, the component Hi
does not contain e, even if e ∈E. Thus, we add e to Hi regardless of whether e ∈E.

152
7 Planar Graphs
a
b
b
b
a
a
a
b
H1
H1
H2
H3
H2
H3
Figure 7.4: A demonstration of a planar realization of G, with m = 3,
for the proof of Lemma 7.4.
Two graphs are said to be homeomorphic if both can be obtained from the
same graph by subdividing edges, that is, an edge is replaced by a simple path
whose intermediate vertices are all new.2 Clearly, if two graphs are homeomor-
phic, then either both are planar or both are not. We are now ready to state
Kuratowski’s Theorem [2].
Theorem 7.2 A graph G is non-planar if and only if there is a subgraph of G
which is homeomorphic to either K3,3 or K5.
Proof: If G has a subgraph H that is homeomorphic to either K3,3 or K5, then
by Lemma 7.3, H is nonplanar, and therefore G is nonplanar. The converse is
much harder to prove. We prove it by contradiction.
Let G be a graph that is nonplanar and which does not contain a subgraph
that is homeomorphic to one of Kuratowski’s graphs; in addition, let us assume
that among such graphs, G has the minimum number of edges.
First, let us show that the vertex connectivity of G is at least 3. Clearly, G is
connected and nonseparable,orthenumberofitsedgesisnotminimum.Assume
that it has a separating pair {a,b}, and let H1,H2,...,Hm be the components
2 Two graphs G1(V1,E1) and G2(V2,E2) are said to be isomorphic if there are bijections
f : V1 →V2 and g : E1 →E2 such that f(u) g(e) f(v) in G2 for every edge u e v in
G1. Clearly, G1 is planar if and only if G2 is. Thus, we are not interested in the particular
names of the vertices or edges, and we distinguish between graphs only up to isomorphism.

7.1 Bridges and Kuratowski’s Theorem
153
with respect to {a,b}, where m > 1. By Lemma 7.4, there exists an 1 ⩽i ⩽m
for which Hi +(a
b) is nonplanar. Clearly Hi +(a
b) does not contain
a subgraph that is homeomorphic to one of Kuratowski’s graphs either. This
contradicts the assumption that G has the minimum number of edges.
We now omit an edge a0 e0 b0 from G. The resulting graph, G0, is planar.
Since the connectivity of G is at least 3, G0 is nonseparable. By Theorem 6.7,
there is a simple circuit in G0 that goes through a0 and b0. Let ˆG0 be a plane
realization of G0 and C be a simple circuit that goes througha0 and b0, such that
C encircles the maximum number of faces of all such circuits in all the plane
realizations of G0. Note that the selection of ˆG0 and C is done simultaneously,
and not successively. Assuming u and v are vertices on C, let C[u,v] be the part
of C going from u to v, clockwise. C(u,v) is deﬁned similarly, but the vertices
u and v are excluded.
Consider now the external bridges of C in ˆG0. If such a bridge, B, has two
attachments either on C[a0,b0] or C[b0,a0], then C is not maximum. To see
this, assume that B has two attachments, a and b, in C[a0,b0]. There is a simple
path P(a,b) connecting a to b via the edges and vertices of B, which is disjoint
from C, and is therefore exterior to C. Form C′ by adding to P the path C[b,a].
C′ goes through a0 and b0, and the interior of C is either completely included
in the interior of C′ or in the exterior (see Figure 7.5). In the ﬁrst case, C′ has
a larger interior than C, in ˆG0. In the later case, we have to ﬁnd another plane
realization that is similar to G0, but the exterior of C′ is now the interior. In
either case, the maximality of the choice of ˆG0 and C is contradicted.Thus, each
P(a,b)
P(a,b)
a0
a
b
(b)
(a)
b0
a0
a
b
b0
Figure 7.5: (a) The interior of the circuit C is contained in the interior
of C′; (b) the interior of the circuit C is contained in the exterior
of C′.

154
7 Planar Graphs
external bridge has at most one attachment in C[a0,b0] and C[b0,a0]. Since an
external bridge must have at least two attachments, it follows that neither a0
nor b0 can be an attachment of an external bridge.
It follows that each external bridge has at most two attachments. Since the
vertex connectivity is at least 3,we concludethatall external bridges are singular
(i.e., consist of a single edge).
Finally, there is at least one such external singular bridge; otherwise, one
could draw the edge e0 outside C, to yield a planar realization of G. Every
external singular bridge interlaces with a0 e0 b0 since the attachments are on
C(a0,b0) and C(b0,a0).
Similarly, there must be an internal bridge, B∗, which prevents the drawing of
e0 inside, and which cannot be transferred outside; that is, B∗interlaces with an
external singular bridge, say, a1 e1 b1. The situation is schematically shown
in Figure 7.6. We divide the argument to two cases according to whether B∗
has any attachment other than a0,b0,a1,b1.
Case 1: B∗has an attachment a2 other than a0,b0,a1,b1. Without loss of
generality wemay assumethata2ison C(a1,a0).SinceB∗preventsthedrawing
of e0, it must have an attachment on C(a0,b0). Since B∗interlaces e1, it must
have an attachment on C(b1,a1).
Case 1.1: B∗has an attachment b2 on C(b1,b0). In B∗, there is a path P
connecting a2 with b2. The situation is shown in Figure 7.7. The subgraph of
a1
b1
b0
a0
e1
B″
Figure 7.6: The bridge B∗in the proof of Theorem 7.2.

7.1 Bridges and Kuratowski’s Theorem
155
a1
a2
a0
e1
b1
P
b2
b0
e0
Figure 7.7: Case 1.1 in the proof of Theorem 7.2.
G shown in Figure 7.7 is homeomorphic to K3,3, where a1,a0, and b2 play the
role of the upper vertices of Figure 7.2(a), and a2,b1 and b0 play the role of
the lower vertices.
Case 1.2: B∗has no attachments on C(b1,b0). Thus, B∗has one attachment b′
2
on C(a0,b1); that is, it may be b1 but not a0. Also, B∗has an attachment b′′
2 on
C[b0,a1). By Lemma 7.1, there exists a vertex v and three vertex-disjoint paths
in B∗: P1(v,a2),P2(v,b′
2), and P3(v,b′′
2 ). The situation is shown in Figure 7.8.
If we erase from the subgraph of G, shown in Figure 7.8 the edges in the path
C[b1,b0] and all its intermediate vertices, the resulting subgraph is homeomor-
phic to K3,3: Vertices a2,b′
2, and b′′
2 play the role of the upper vertices; and
a0,a1, and v, the lower vertices.
Case 2: B∗has no attachments other than a0,b0,a1,b1. In this case all four
must be attachments; for, if a0 or b0 are not, then B∗and e1 do not interlace; if
a1 or b1 are not, then B∗does not prevent the drawing of e0.
Case 2.1: There is a vertex v, in B∗, from which there are four disjoint
paths in B∗: P1(v,a0),P2(v,b0),P3(v,a1), and P4(v,b1). This case is shown
in Figure 7.9, and the shown subgraph is clearly homeomorphic to K5.
Case 2.2: No vertex as in Case 2.1 exists. Let P0(a0,b0) and P1(a1,b1) be two
simple paths in B∗. Let c1 be the ﬁrst vertex on P1 that is common with P0, and
let c2 be the last on P1 that is common with P0. We use only the ﬁrst part, A,

156
7 Planar Graphs
e1
a0
b′2
b″2
b1
a2
a1
b0
v
e0
P2
P1
P3
Figure 7.8: Case 1.2 in the proof of Theorem 7.2.
e1
a0
b1
a1
b0
e0
v
Figure 7.9: Case 2.1 in the proof of Theorem 7.2.
of P1, connecting a1 and c1; and the last part, B, connecting c2 with b1. The
pertaining subgraph of G is now shown in Figure 7.10, and is homeomorphic
to K3,3 after the edges in C[a0,b1] and C[b0,a1] and all their intermediate
vertices are erased: Vertices a0,b1, and c1 play the role of the upper vertices;
and b0,a1, and c2, the lower. (If c1 is closer to a0 than c2, then we erase C[a1,a0]
and C[b1,b0], instead, and the upper vertices are a0,a1 and c2.)
■
Kuratowski’s theorem provides a necessary and sufﬁcient condition for a
graph to be planar. However,it does not yield an efﬁcient algorithmfor planarity

7.2 Equivalence
157
e1
a0
b1
a1
b0
e0
c1
c2
A
B
P0
Figure 7.10: Case 2.2. in the proof of Theorem 7.2.
testing. The obvious procedure, that of trying for all subsets of ﬁve vertices to
see whether there are ten vertex disjoint paths connecting all pairs, or for every
pairs of three and three vertices whether there are nine paths, suffers from two
shortcomings. First there are
	|V|
choices of ﬁve-sets and 1/2 ·
	|V|
3

·
	|V|−3
3

choices of three, and three vertices; this alone is O(|V|6). But what is worse,
we have no efﬁcient way to look for the disjoint paths; this problem may, in
fact, be exponential.
Fortunately, there are O(|E|) tests, as we shall see in the next chapter, for
testing whether a given graph is planar.
7.2 Equivalence
Let ˆG1 and ˆG2 be two plane realizations of the graph G. We say that ˆG1 and ˆG2
are equivalent if every window of one of them is also a window in the other.
G may be 2-connected and have nonequivalent plane realization; for example,
see Figure 7.11
Let us restrict our discussion to planar ﬁnite graphs with no parallel edges
and no self-loops. Our aim is to show that if the vertex connectivity of G, c(G),
is at least three then the plane realization of G is unique up to equivalence.
Lemma 7.5 A planar nonseparable graph G is 2-connected if there is a plane
realization of it, ˆG, and one of its windows has more than one bridge.
Proof: If C is a window of ˆG with more than one bridge, then all C’s bridges
are external. Therefore,no two bridges interlace. As in the ﬁrst paragraph of the

158
7 Planar Graphs
1
2
4
6
7
5
3
8
1
2
3
4
5
6
8
7
Figure 7.11: Two nonequivalent plane realizations of the same graph.
proof of Lemma 7.2, there exists a bridge B whose attachments can be ordered
a1,a2,...at, and no attachments of any other bridge appear on C(a1,at). It
is easy to see that {a1,at} is a separating pair; it separates the vertices of B
and C(a1,at) from the set of vertices of all other bridges and C(at,a1), where
neither set can be empty since G has no parallel edges.
■
Theorem 7.3 If G is a plane graph with no parallel edges and no self-loops and
if its vertex connectivity, c(G), is at least 3, then every two plane realizations
of G are equivalent.
Proof: Assume that G has two plane realizations ˆG1 and ˆG2 that are not equiv-
alent. Without loss of generality we may assume that there is a window C in ˆG1
that is not a window in ˆG2. Therefore, C has at least two bridges; one interior and
one exterior. By Lemma 7.5, and since C is a window in ˆG1, G is 2-connected.
A contradiction, since c(G) ⩾3.
■
7.3 Euler’s Theorem
The following theorem is due to Euler.
Theorem 7.4 Let G(V,E) be a nonempty connected plane graph. The number
of faces, f, satisﬁes
|V| + f −|E| = 2.
(7.1)

7.4 Duality
159
Proof: By induction on |E|. If |E| = 0, then G consists of one vertex and there is
one face, and Equation 7.1 holds. Assume that the theorem holds for all graphs
with m = |E|. Let G(V,E) be a connected plane graph with m + 1 edges. If
G contains a circuit, then we can remove one of its edges. The resulting plane
graph is connected and has m edges and, therefore, by the inductive hypothesis,
satisﬁes Equation 7.1. Adding back the edge increases the number of faces by
one and the number of edges by one, and thus Equation 7.1 is maintained. If G
contains no circuits, then it is a tree. By Corollary 2.1, it has at least two leaves.
Removing a leaf and its incident edge yields a connected graph with one less
edge and one less vertex, which satisﬁes Equation 7.1. Therefore, G satisﬁes
Equation 7.1 too.
■
The theorem implies that all connected plane graphs with |V| vertices and |E|
edges have the same number of faces. One can draw many conclusions from
the theorem. Some of them are the following:
Corollary 7.2 If G(V,E) is a connected plane graph with no parallel edges,
no self-loops and |V| > 2, then
|E| ⩽3|V| −6.
(7.2)
Proof: Since there are no parallel edges, every window consists of at least
three edges. Each edge appears on the windows of two faces, or twice on the
window of one face. Thus, 3 · f ⩽2 · |E|. By Equation 7.1, |E| = |V| + f −2.
Thus, |E| ⩽|V| + 2/3|E| −2, and (7.2) follows.
■
Corollary 7.3 Every connected plane graph with no parallel edges and no
self-loops has at least one vertex whose degree is 5 or less.
Proof: Assumethecontrary;thatis,thedegreeofeveryvertexisatleast6.Thus,
2 · |E| ⩾6 · |V|; note that each edge is counted in each of its two end-vertices.
This contradicts (7.2).
■
7.4 Duality
Let G(V,E) be a ﬁnite undirected and connected graph. A set K ⊆E is called a
cutset if it is a minimal separating set of edges; that is, the removal of K from
G interrupts its connectivity, but no proper subset of K does it. It is easy to
see that a cutset separates G into two connected components: Consider ﬁrst the
removal of K−{e}, where e ∈K. G remains connected. Now remove e. Clearly,
G breaks into two components.

160
7 Planar Graphs
x6
x1
x5
x3
x4
x2
y3
y2
y4
y5
y1
y6
z5
z2
z6
z1
z4
z3
Figure 7.12: (a) A connected graph G1; (b) G2, a dual of G1; (c) G3,
also a dual of G1.
The graph G2(V2,E2) is said to be the dual of a connected graph G1(V1,E1)
if there is a 1 −1 correspondence f : E1 →E2, such that a set of edges S forms
a simple circuit in G1 if and only if f(S) (the corresponding set of edges in
G2) forms a cutset in G2. Consider the graph G1 shown in Figure 7.12 (a). G2
shown in Figure 7.12(b) is a dual of G1, but so is G3, shown in Figure 7.12(c),
as the reader can verify by considering all (six) simple circuits of G1 and all
cutsets of G2, or G3.
A contraction of an edge x e y of a graph G(V,E) is the following operation:
Delete the edge e, and merge x with y. The new contracted graph, G′, has one
less edge and one less vertex, if x ̸= y. Clearly, if G is connected, so is G′. Also,
graph G′ is a contraction of G if by repeated contractions we can construct G′
from G.
Lemma 7.6 If a connected graph G1has a dual and G′
1is a connected subgraph
of G1, then G′
1 has a dual.
Proof: We can get G′
1 from G1 by a sequence of two kinds of deletions:

7.4 Duality
161
1. A deletion of an edge e of the present graph, which is not in G′
1, and whose
deletion does not interrupt the connectivity of the present graph.
2. A deletion of a leaf of the present graph,which is not a vertex of G′
1, together
with its incident edge.
We want to show that each of the resulting graphs, starting with G1 and ending
with G′
1, has a dual.
Let G be one of these graphs, except the last, and its dual be Gd. First consider
a deletion of type (7.4), of an edge e. Construct f(e) in Gd, to get Gdc. If C
is a simple circuit in G −e, then clearly it cannot use e, and therefore it is a
circuit in G too. The set of edges of C is denoted by S. Thus, f(S) is a cutset of
Gd, and it does not include f(e). Thus, the end vertices of f(e) are in the same
component of Gd with respect to f(S). It follows that f(S) is a cutset of Gdc
too. If K is a cutset of Gdc, then it is a cutset of Gd too. Thus, f−1(K) form a
simple circuit C′ in G. However, f(e) is not in K, and therefore e is not in C′.
Hence, C′ is a simple circuit of G −e.
Next, consider a deletion of type (7.4) of a leaf v and its incident edge e.
Clearly, e, plays no role in a circuit. Thus, f(e) cannot be a part of a cutset in Gd.
Hence, f(e) is a self-loop. The deletion of v and e form G, and the contraction
of f(e) in Gd (which effectively, only deletes f(e) from Gd), does not change
the sets of simple circuits in G and cutsets in Gd, and the correspondence is
maintained.
■
Lemma 7.7 Let G be a connected graph and e1,e2 be two of its edges, neither
of which is a self loop. If for every cutset, either both edges are in it or both are
not, then e1 and e2 are parallel edges.
Proof: If e1 and e2 are not parallel, then there exists a spanning tree which
includes both. (Such a tree can be found by contracting both edges and ﬁnding
a spanning tree of the contracted graph.) The edge e1 separates the tree into
two connected components whose sets of vertices are S and ¯S. The set of edges
between S and ¯S in G is a cutset that includes e1 and does not include e2. A
contradiction.
■
Lemma 7.8 Let G be a connected graph with a dual Gd, and let f be the
1 −1 correspondence of their edges. If u e1 x1 e2 x2 e3 ...xl−1 el v is a
simple path or circuit in G such that x1,x2,...,xl−1 are all of degree two, then
f(e1),f(e2),...,f(el) are parallel edges in Gd.

162
7 Planar Graphs
Proof: Every circuit of G that contains one ei,1 ⩽i ⩽l, contains all the rest.
Thus, in Gd, if one edge,f(ei), is in a cutset then all the rest are. By Lemma. 7.7,
they are parallel edges.
■
Lemma 7.9 If a connected graph G1 has a dual, and G2 is homeomorphic to
G1, then G2 has a dual.
Proof: If an edge x
e
y of G1 is replaced in G2 by a path x e1
v1 e2 v2
... el y, then in the dual f(e) is replaced by parallel edges:
f(e1),f(e2),...,f(el). If a path of G1 is replaced in G2 by an edge e, then the
edges of the dual that correspond to the edges of the path are all parallel (by
Lemma 7.8) and can be replaced by a single edge f(e). It is easy to see that
every circuit which uses an edge e, when it is replaced by a path, will use all
the edges of the path instead, while in the dual, every cutset that uses f(e) will
use all the parallel edges which replace it. Thus, the correspondence of circuits
and cutsets is maintained.
■
Theorem 7.5 A (connected) graph has a dual if and only if it is planar.
Proof: Assume G1(V1,E1) is a planar graph and ˆG1 is a plane realization of
it. Choose a point pi in each face Fi of ˆG1. Let V2 be the set of these points.
Since G1 is connected, the boundary of every face is a circuit (not necessarily
simple) which we call its window. Let Wi be the window of Fi, and assume
it consists of l edges. We can ﬁnd l curved lines, all starting in pi, but no two
share any other point, such that each of the lines ends on one of the l edges
of Wi, one line per edge. If a separating edge3 e appears on Wi, then clearly
it appears twice. In this case there will be two lines from pi hitting e from
both directions. These two lines can be made to end in the same point on e,
thus creating a closed curve that goes through pi and crosses e. If e is not a
separating edge, then it appears on two windows, say, Wi and Wj. In this case,
we can make the line from pi to e meet e at the same point as does the line from
pj to e, to form a line connecting pi with pj, which crosses e. None of the set of
lines thus formed crosses another, and we have one per edge of ˆG1. Now deﬁne
ˆG2(V2,E2) as follows: The set of lines connecting the pi’s is a representation
of E2. The 1 −1 correspondence f : E1 →E2 is deﬁned as follows: f(e) is the
edge of ˆG2 which crosses e. Clearly, ˆG2 is a plane graph that is a realization
of a graph G2(V2,E2). It remains to show that there is a 1 −1 correspondence
of the simple circuits of G1 to the cutsets of G2. The construction described
3 An edge whose removal from G1 interrupts its connectivity.

7.4 Duality
163
Figure 7.13: For the proof of Theorem 7.5.
above is demonstrated in Figure 7.13, where ˆG1 is shown in solid lines, and ˆG2
is shown in dashed lines.
Let C be a simple circuit of G1. Clearly, in ˆG1, C describes a simple closed
curve in the plane. There must be at least one vertex of ˆG2 inside this circuit,
since at least one edge of ˆG2 crosses the circuit, and it crosses exactly once.
The same argument also applies to the outside. This implies that f(S), where S
is the set of edges of C, forms a separating set of G2. Let us postpone the proof
of the minimality of f(S) for a little while.
Now let K be a cutset of G2. Let T and ¯T be the sets of vertices of the two
components of G2 formed by the deletion of K. The set of faces of ˆG1 that
correspond to the vertices of T, forms a continuous region, but not the whole
plane. The minimality of K implies that the boundary of this region is a circuit
in G1, whose edges correspond to K. Thus, we have shown that every simple
circuit of G1 corresponds to a separating set of G2, and every cutset of G2
corresponds to a circuit of G1.
Now let us handle the minimality. If S is the set of edges of a simple circuit C
of G1 and f(S) is not a cutset of G2, then there is a proper subset of f(S) which
is a cutset, say K. Therefore, f−1(K) is a circuit of G1. However, f−1(K) ⊊S. A
contradiction to the assumption that C is simple. The proof that if K is a cutset
then f−1(K) is a simple circuit is similar.
This completes the proof that the connected planar graph has a dual. We turn
to the proof that a nonplanar graph has no dual. Here, we follow the proof of
Parson [3].
First, let us show that neither K3,3 nor K5 have a dual. In K3,3, the shortest
circuit is of length four, and for every two edges there exists a simple circuit
that one but does not use the other. Thus, in its dual no cutset consists of less

164
7 Planar Graphs
than four edges and there are no parallel edges. Therefore, the degree of each
vertex is at least 4 and there must be at least ﬁve vertices. The number of edges
is therefore at least (5 · 4/2) = 10, while K3,3 has 9 edges. Thus, K3,3 has no
dual. In the case of K5, it has ten edges, ten simple circuits of length three and no
shorter circuits, and for every two edges there is a simple circuit that uses one
but not the other. Thus, the dual must have ten edges, ten cutsets of three edges,
no cutset of lesser size and therefore the degree of every vertex is at least three.
Also, there are no parallel edges in the dual. If the dual has ﬁve vertices, then it is
K5 itself (ten edges and no parallel edges), but K5 has no cutsets of three edges. If
the dual has seven vertices or more, then it has at least eleven edges (⌈7·3/2⌉).
Thus, the dual must have six vertices. Since it has ten clusters of three edges,
there is one that separates S from ¯S, where neither consists of a single vertex.
If |S| = 2, then it contains a vertex whose degree is 2. If |S| = |¯S| = 3, then the
maximum number of edges in the dual is nine. Thus, K5 has no dual.
Now, if G is a nonplanar graph with a dual, then by Kuratowski’s theorem
(Theorem 7.2) it contains a subgraph G′ that is homeomorphic to either K3,3 or
K5. By Lemma 7.6, G′ also has a dual. By Lemma 7.9, either K3,3 or K5 has a
dual. A contradiction. Thus, no nonplanar graph has a dual.
■
Theorem 7.5provides another necessary and sufﬁcient condition forplanarity,
in addition to Kuratowski’s theorem. However, neither has been shown to be
useful for testing planarity.
There are many facts about duality that we have not discussed. Among them
are the following:
1. If Gd is a dual of G, then G is a dual of Gd.
2. A 3-connected planar graph has a unique dual.
The interested reader can ﬁnd additional information and references in the
books of Harary [4], Ore [5], and Wilson [6].
7.5 Problems
Problem 7.1 The purpose of this problem is to prove a variation of Kura-
towski’s theorem.
1. Prove that if G is a connected graph and v1,v2,v3 are three ver-
tices, then there exists a vertex v and three (vertex) disjoint paths
P1(v,v1),P2(v,v2),P3(v,v3), one of which may be empty.
2. Prove that if G is a connected graph and S is a set of four vertices, then
either there is a vertex v with four disjoint paths to the members of S or there

7.5 Problems
165
are two vertices u and v such that two members of S are connected to u by
paths, two to v, and u is connected to v; all ﬁve paths are vertex-disjoint.
3. Show that if a graph is contractible to K3,3, then it has a subgraph
homeomorphic to K3,3.
4. Show that if a graph is contractible to K5, then either it has a subgraph that
is homeomorphic to K5 or it has a subgraph that is contractible to K3,3.
5. Prove the theorem: A graph is nonplanar if and only if it contains a subgraph
that is contractible to K3,3 or K5.
Problem 7.2 Show a graph that is nonplanar but not contractible to either K3,3
or K5. Does it contradict the result of Problem 7.1(7.1)?
Problem7.3 UseKuratowski’stheoremto provethatthePetersengraph,shown
in Figure 7.14, is nonplanar.
Figure 7.14: Peterson graph.
Problem 7.4 Is the graph depicted in Figure 7.15 planar? Justify your answer.
Figure 7.15: Is this graph planar?

166
7 Planar Graphs
Problem 7.5 A plane graph is called triangular if each of its windows is a
triangle. Let Ni be the number of vertices whose degree is i.
1. Prove that every plane graph with three or more vertices that has no self-
loops and no parallel edges can be made triangular by adding new edges
without creating any self-loops or parallel edges. (This process is called
triangulation.)
2. Let G be a triangular plane graph as in part 1, with |V| > 3. Prove that
N1 = N2 = 0.
3. Let G be a triangular plane graph as in part 1. Prove that
12 = 3 · N3 + 2 · N4 + N5 −N7 −2 · N8 −3 · N9 −4 · N10 ...
4. Prove that in a graph, in part 1, if there are no vertices of degree 3 or 4, then
there are at least tweleve vertices of degree 5.
5. Prove that if G is a planar graph with no self-loops or parallel edges, and
|V| > 3, then it has at least 4 vertices with degrees less than 6, and if N3 =
N4 = 0, then N5 ⩾12.
6. Prove that if the vertex connectivity, c(G), of a graph G(V,E) is at least ﬁve,
then |V| ⩾12.
7. Prove that if G is planar, then c(G) < 6.
Problem 7.6 Prove that if G1(V1,E1) and G2(V2,E2) are homeomorphic, then
|E1| −|V1| = |E2| −|V2|.
Problem 7.7 Show a triangular plane graph without parallel edges that is not
Hamiltonian.
Problem 7.8 Let G be a plane graph. The plane graph G∗, which is constructed
by the procedure in the ﬁrst part of the proof of Theorem 7.5, is called its
geometric dual.
Prove that if G∗is the geometric dual of G, then G is the geometric dual
of G∗.

7.5 Problems
167
Bibliography
[1] König, D., Theorie der endlichen und unendlichen Graphen. Leipzig, 1936.
Reprinted Chelsea, 1950.
[2] Kuratowski, K., “Sur le problème des Courbes Gauches en Topologie,” Fund. Math.,
Vol. 15, 1930, pp. 217–283.
[3] Parson, T. D., “On Planar Graphs,” Am. Math. Monthly, Vol. 78, No. 2, 1971,
pp. 176–178.
[4] Harary, F., Graph Theory, Addison Wesley, 1969.
[5] Ore, O., The Four-Color Problem, Academic Press, 1976.
[6] Wilson, R. J., Introduction to Graph Theory, Longman, 1972.

8
Testing Graph Planarity
8.1 Introduction
There are two known planarity testing algorithms that have been shown to be
realizable in a way that achieves linear time (O(|V|)). The idea in both is to
follow the decisions to be made during the planar construction of the graph,
piece by piece, as to the relative location of the various pieces. The construction
is not carried out explicitly because there are difﬁculties, such as the crowding
of elements into a relatively small portion of the area allocated, which, as yet,
we do not know to avoid. Also, an explicit drawing of the graph is not necessary,
as we shall see, to decide whether such a drawing is possible. We shall imagine
that such a realization is being carried out, but will only decide where the various
pieces are laid relative to each other, and not their exact shape. Such decisions
may change later to make a place for later additions of pieces. In both cases, it
was shown that the algorithm terminates within (O(|V|)) steps, and if it fails to
ﬁnd a “realization,” then none exists.
The ﬁrst algorithm starts by ﬁnding a simple circuit and adding toit one simple
path at a time. Each such new path connects two old vertices via new edges
and vertices. (Whole pieces are sometimes ﬂipped over, around some line).
Thus, we call it the path addition algorithm. The basic ideas were suggested
by various authors, such as Auslander and Parter [1] and Goldstein [2], but the
algorithm in its present form, both from the graph-theoretic point of view, and
complexity point of view, is the contribution of Hopcroft and Tarjan [3]. They
were ﬁrst to show that planarity testing can be done in linear time.
The second algorithm adds one vertex in each step. Previously drawn edges
incident to this vertex are connected to it, and new edges incident to it are
drawn and their other endpoints are left unconnected. (Here, too, sometimes
whole pieces have to be ﬂipped around or permuted). The algorithm is due to
Lempel, Even, and Cederbaum [4]. It consists of two parts. The ﬁrst part was
168

8.2 The Path Addition Algorithm of Hopcroft and Tarjan
169
shown to be linearly realizable by Even and Tarjan [5]; the second part was
shown to be linearly realizable by Booth and Leuker [6]. We call this algorithm
the vertex addition algorithm.
Each of these algorithms can be divided into its graph-theoretic part and
its data structures and their manipulation. The algorithms are fairly complex
and a complete description and proof would require a much more elaborate
exposition. Thus, since this is a book on graphs, and not on programming, I
have chosen to describe in full the graph-theoretic aspects of both algorithms
and to only brieﬂy describe the details of the data manipulation techniques.
An attempt is made to convince the reader that the algorithms work, but in
order to see the details which make it linear one will have to refer to the papers
mentioned above.
Throughout this chapter, for reasons explained in Chapter 7, we shall assume
that G(V,E) is a ﬁnite undirected graph with no parallel edges and no self-
loops. We shall also assume that G is nonseparable. The ﬁrst thing that we can
do is check whether |E| ⩽3 · |V| −6. By Corollary 7.1, if this condition does
not hold, then G is nonplanar. Thus, we can restrict our algorithm to the cases
where |E| = O(|V|).
8.2 The Path Addition Algorithm of Hopcroft and Tarjan
The algorithm starts with a DFS of G. We assume that G is nonseparable. Thus,
we drop from the DFS the steps for testing nonseparability. However, we still
need the lowpoint function, to be denoted now L1(v). In addition, we need the
second lowpoint function, L2(v), to be deﬁned as follows: Let S(v) be the set
of values k(u) of vertices u reachable from descendants of v by a single back
edge. Clearly, L1(v) = min{{k(v)} ∪S(v)}.
Deﬁne
L2(v) = min{{k(v)} ∪[S(v) −{L1(v)}]}.
Let us now rewrite the DFS in order to compute these functions (we denote
an assignment statement, “x gets the value of y” by x := y):
1. Mark all the edges “unused.” For every v ∈V, let k(v) := 0. Let i := 0 and
v := s (the vertex s is where we choose to start the DFS).
2. i := i + 1,k(v) := i,L1(v) := i,L2(v) := i.
3. If v has no unused incident edges, go to Step (8.2).
4. Choose an unused incident edge v e u. Mark e “used.” If k(u) ̸= 0 then
do the following:
If k(u) < L1(v), then L2(v) := L1(v),L1(v) := k(u).

170
8 Testing Graph Planarity
If k(u) > L1(v), then L2(v) := min{L2(v),k(u)}.
Go to Step (8.2).
Otherwise (k(u) = 0), let f(u) := v, v := u and go to Step (8.2).
5. If k(v) = 1, halt.
6. (k(v) > 1; we backtrack).
If L1(v) < L1(f(v)) then L2(f(v)) := min{L2(v),L1(f(v))},L1(f(v))
:= L1(v).
If L1(v) = L1(f(v)), then L2(f(v)) := min{L2(v),L2(f(v))}.
Otherwise (L1(v) > L1(f(v))),L2(f(v)) := min{L1(v),L2(f(v))}.
v := f(v).
Go to Step (8.2).
From now on we refer to the vertices by their k(v) number; that is, we change
the name of v to k(v).
Let A(v) be the adjacency list of v; that is, the list of edges incident from v.
We remind the reader that after the DFS each of the edges is directed; the tree
edges are directed from low to high, and the back edges are directed from high
to low. Thus, each edge appears once in the adjacency lists. Now, we want to
reorder the adjacency lists, but ﬁrst, we must deﬁne an order on the edges. Let
the value φ(e) of an edge u e
−→v be deﬁned as follows:
φ(e) =







2 · v
if u e
−→v is a back edge.
2 · L1(v)
if u e
−→v is a tree edge and L2(v) ⩾u.
2 · L1(v) + 1 if u e
−→v is a tree edge and L2(v) < u.
Next, we order the edges in each adjacency list to be in nondecreasing order
with respectto φ.[Thiscan bedoneon O(|V|)timeasfollows.First,computefor
each edge it’s φ value. Prepare 2·|V|+1 buckets, numbered 1,2,...,2·|V|+1.
Put each e into the φ(e) bucket. Now, empty the buckets in order, ﬁrst bucket
number 1, then 2 and so on, putting the edges taken out into the proper new
adjacency lists, in the order that they are taken out of the buckets.]
The new adjacency lists are now used, in a second run of a DFS algorithm, to
generate the paths, one by one. In this second DFS, vertices are not renumbered,
and there is no need to recompute f(v), L1(v) or L2(v). The tree remains the
same, although the vertices may not be visited in the same order. The paths
ﬁnding algorithm is as follows:
1. Mark all edges “unused” and let v := 1.
2. Start the circuit C; its ﬁrst vertex is 1.

8.2 The Path Addition Algorithm of Hopcroft and Tarjan
171
3. Let the ﬁrst unused edge in A(v) be v e
−→u.
If u ̸= 1, then mark e as used, add u to C, update v := u, and repeat
Step (3). Otherwise, (u = 1), C is closed. Output C.
4. If v = 1, halt.
5. If every edge in A(v) is used, then update v := f(v) and go to Step (4).
6. Start a path P with v as its ﬁrst vertex.
7. Let v e
−→u be the ﬁrst unused edge in A(v).
If e is a back edge (u < v), terminate P with u, output P, and go to (5).
8. Otherwise, (e is a tree edge). Add u to P, update v := u, and go to Step (7).
Lemma 8.1 The paths ﬁnding algorithm ﬁnds ﬁrst a circuit C that consists of
a path from 1 (the root) to some vertex v, via tree edges, and a back edge from
v to 1.
Proof: Let 1 −→u be the ﬁrst edge of the tree to be traced (in the ﬁrst appli-
cation of Step (3)). We assume that G is nonseparable, and |V| > 2. Thus, by
Lemma 3.7, this edge is the only tree edge out of 1, and u = 2. Also, 2 has some
descendants, other than itself. Clearly, 2
3 is a tree edge. By Lemma 3.5,
L1(3) < 2, that is, L1(3) = 1. Thus, L1(2) = 1. The reordering of the adjacency
lists assures that the ﬁrst path to be chosen out of 1 will lead back to 1 as
claimed.
■
Lemma 8.2 Each generated path P is simple, and it contains exactly two ver-
tices in common with previously generated paths; they are the ﬁrst vertex, f,
and the last, l.
Proof: The edge scanning during the paths ﬁnding algorithm is in a DFS
manner, in accord with the structure of the tree (but not necessarily in the same
scanning order of vertices). Thus, a path starts from some (old) vertex f, goes
along tree edges, via intermediate vertices which are all new, and ends with
a back edge which leads to l. Since back edges always lead to ancestors, l is
old. Also, by the reordering of the adjacency lists and the assumption that G is
nonseparable, l must be lower than f. Thus, the path is simple.
■
Let Sv denote the set of descendants of v, including v itself.
Lemma 8.3 Let f and l be the ﬁrst and last vertices of a generated path P and
f −→v be its ﬁrst edge.
1. If v ̸= l then L1(v) = l.
2. l is the lowest vertex reachable from Sf via a back edge that has not been
used in any path yet.

172
8 Testing Graph Planarity
Proof: Let us partition Sf−{f} into two parts, α and β, as follows. A descendant
u belongs to α if and only if when the construction of P begins, we have already
backtracked from u. Clearly, f ∈β and all the back edges out of α have been
scanned already. Let u be the lowest vertex reachable via an unused back edge
from β. Clearly, the ﬁrst remaining (unused) edge of A(f) is the beginning of a
directed path to u, which is either an unused back edge from f to u or a path of
tree edges, via vertices of β, followed by a single back edge to u. Thus, u = l,
and the lemma follows.
■
Lemma 8.4 Let P1 and P2 be two generated paths whose ﬁrst and last ver-
tices are f1,l1 and f2,l2, respectively. If P1 is generated before P2 and f2 is a
descendant of f1 then l1 ⩽l2.
Proof: The lemma follows immediately from Lemma 8.3.
■
So far, we have not made any use of L2(v). However, Lemma 8.5 relies on it.
Lemma 8.5 Let P1: f e1
−→v1 −→... −→l and P2: f e2
−→v2 −→... −→l be
two generated paths, where P1 is generated before P2. If v1 ̸= l and L2(v1) < f,
then v2 ̸= l and L2(v2) < f.
Proof: By the deﬁnition of φ(e), φ(e1) = 2 · l + 1. If v2 = l or L2(v2) ⩾f,
then φ(e2) = 2 · l, and e2 should appear in A(f) before e1. A contradiction. ■
Let C be 1 −→v1 −→v2 −→... −→vn −→l. Clearly 1 < v1 < v2 < ...< vn.
Consider now the bridges of G with respect to C.
Lemma 8.6 Let B be a nonsingular bridge of G with respect to C, whose
highest attachment is vi. There exists a tree edge vi
e
−→u that belongs to B,
and all other edges of B with endpoints on C are back edges.
Proof: The lemma follows from the fact that the paths ﬁnding algorithm is
a DFS. First C is found. We then backtrack from a vertex vj only if all its
descendants have been scanned. No internal part of B can be scanned before
we backtrack into vi. There must be a tree edge vi −→u, where u belongs to
B, for the following reasons. If all the edges of B, incident to vi are back edges,
they all must come from descendants or go to ancestors of vi (see Lemma 3.4).
An edge from vi to one of its ancestors (which must be on C) is a singular
bridge and is not part of B. An edge from a descendant w of vi implies that w
cannot be in B, for it has been scanned already, and we have observed that no
internal part of B can be scanned before we backtrack into vi. If any other edge
vk
x of B is also a tree edge, then, by deﬁnition of a bridge, there is a path

8.2 The Path Addition Algorithm of Hopcroft and Tarjan
173
connecting u and x that is vertex-disjoint from C. Along this path there is at
least one edge that contradicts Lemma 3.4.
■
Corollary 8.1 Once a bridge B is entered, it is completely traced before
it is left.
Proof: By Lemma 8.6, there is only one edge through which B is entered.
Since eventually the whole graph is scanned, and no edge is scanned twice in
the same direction, the corollary follows.
■
Assuming that C and the bridges explored from vertices higher than vi have
already been explored and drawn in the plane. Lemma 8.7 provides a test for
whether the next generated path could be drawn inside; the answer is negative,
even if the path itself can be placed inside, but it is already clear that the whole
bridge to which it belongs cannot be placed there.
Lemma 8.7 Let vi −→u1 −→u2 −→... −→ul(= vj) be the ﬁrst path, P, of
the bridge B to be generated by the paths ﬁnding algorithm. P can be drawn
inside (outside) C if there is no back edge w −→vk drawn inside (outside)
for which j < k < i. If there is such an edge, then B cannot be drawn inside
(outside).
Proof: The sufﬁciency is immediate. If there is no back edge drawn inside C,
that enters the path, vj −→vj+1 −→··· −→vi in one of its internal vertices,
then there cannot be any inside edge incident to these vertices, since bridges to
be explored from vertices lower than vi have not been scanned yet. Thus, P can
be drawn inside if it is placed sufﬁciently close to C.
Now, assume there is a back edge w −→vk, drawn inside, for which j < k < i.
Let P ′ be the directed path from vp to vk whose last edge is the back edge
w −→vk. Clearly, vp is on C and p ⩾i; P ′ is not necessarily generated in one
piece by the path-ﬁnding algorithm, if it is not the ﬁrst path to be generated in
the bridge B′ to which it belongs.
Case 1: p > i. The bridges B and B′ interlace by part (i) of the deﬁnition of
interlacement. Thus, B cannot be drawn inside.
Case 2: p = i. Let P ′′ be the ﬁrst path of B′ to be generated, P ′′ : vi −→x1 −→
x2 −→... −→vq. By Lemma 8.4, q ⩽j, since B′ is explored before B.
Case 2.1: q < j. Since vi and vj are attachments of B, vk and vq are attachments
of B′ and q < j < k < i, the two bridges interlace. Thus, B cannot be drawn
inside.

174
8 Testing Graph Planarity
Case 2.2: q = j. P ′′ cannot consist of a single edge, for in this case, it is a
singular bridge and vk is not one of its attachments. Also, L2(x1) < vk. Thus,
L2(x1) < vi. By Lemma 8.5, u1 ̸= vj and L2(u1) < vi. This implies that B and
B′ interlace by either part (i) or part (ii) of the deﬁnition of interlacement, and
B cannot be drawn inside.
■
The algorithm assumes that the ﬁrst path of the new bridge B is drawn inside
C. Now, we use the results of Corollary 7.1, Theorem 7.1 and the discussion
that follows it, to decide whether the part of the graph explored so far is planar,
assuming that C+B is planar. By Lemma 8.7, we ﬁnd which previous bridges
interlace with B. The part of the graph explored so far is planar if and only if
the set of its bridges can be partitioned into two sets such that no two bridges in
the same set interlace. If the answer is negative, the algorithm halts, declaring
that graph nonplanar. If the answer is positive, we still have to check whether
C + B is planar.
Let the ﬁrst path of B be P: vi −→u1 −→u2 −→... −→vj. We now have a
circuit C′ consisting of C[vj,vi] and P. The rest of C is an outside path P ′, with
respect to C′, and it consists of C[vi,1] and C[1,vj]. The graph B + C[vj,vi]
may have bridges with respect to C′, but none of them has all its attachments
on C[vj,vi], for such a bridge is also a bridge of G with respect to C, and is not
a part of B.
Thus, no bridge of C′, with attachments of C(vj,vi) may be drawn outside
C′, since it interlaces with P ′. We conclude that C + B is planar if and only
if B + C[vj,vi] is planar and its bridges can be partitioned into an inside set
and an outside set so that the outside set contains no bridge with attachments
in C(vj,vi). The planarity of B + C[vj,vi] is tested by applying the algorithm
recursively,using theestablished vertex numbering L1,L2,ffunctionsand order-
ing of the adjacency lists. If B + C[vj,vi] is found to be nonplanar, clearly G
is nonplanar. If it is found to be planar, we check whether all its bridges with
attachments in C(vj,vi) can be placed inside. If so, C + B is planar; if not,
G is nonplanar.
Hopcroft and Tarjan devised a simple tool for deciding whether the set of
bridgescan be partitionedproperly.It consists of three stacks Π1,Π2, and Π3. Of
these Π1 contains in nondecreasing order (the greatest entry on top) the vertices
of C(1,vi) (where vi is the last vertex of C into which we have backtraced),
which are attachments of bridges placed inside C in the present partition of the
bridges. A vertex may appear on Π1 several times if there are several back edges
into it from inside bridges. Π2 is similarly deﬁned for outside bridges. Both Π1

8.2 The Path Addition Algorithm of Hopcroft and Tarjan
175
and Π2 are doubly linked lists, in order to enable switching over of complete
sections from one of them to the other, investing, per section, time bounded by
some constant. Π3 consists of pairs of pointers to entries in Π1 and Π2. Its task
will be described shortly.
Let S be a maximal set of bridges, explored up to now, such that a decision
for any one of these bridges as to the side it is in, implies a decision for all the
rest. (S corresponds to a connected component of the graph of bridges, as in the
discussion following Theorem 7.1.) Let the set of entries in Π1 and Π2, which
correspond to back edges from the bridges of S, be called a block.
Lemma 8.8 Let K be a block, whose highest element is vh and lowest element
is vl. If vp is an entry in Π1 or Π2, then vp belongs to K if vl < vp < vh.
Proof: We prove the lemma by induction on the order in which the bridges are
explored. At ﬁrst, both Π1 and Π2 are empty and the lemma is vacuously true.
The lemma trivially holds after one bridge is explored.
Assume the lemma is true up to the exploration of the ﬁrst path P of the
new bridge B, where P : vi −→... −→vj. If there is no vertex vk on Π1 or
Π2 such that vj < vk < vi then clearly the attachments of B (in C(1,vi)) form
a mew block (assuming C + B is planar), and the lemma holds. However, if
there are vertices of Π1 or Π2 in between vj and vi, then by Lemma 8.7, the
bridges they are attachments of all interlace with B. Thus, the old blocks which
these attachments belong to, must now be merged into one new block with
the attachments of B (in C(1,vi)). Now, let vl be the lowest vertex of the new
block and vh be the highest. Clearly, vl was the lowest vertex of some old block
whose highest vertexwas v′
h, and v′
h > vj. Thus, by the inductivehypothesis,no
attachment of another block could be between vl and v′
h and therefore cannot
be in this region after the merger. Also, all the attachments in between vj and
vh are in the new block since they are attachments of bridges which interlace
with B. Thus, all the entries of Π1 or Π2 which are in between vl and vh belong
to the new block.
■
Corollary 8.2 The entries of one block appear consecutively on Π1(Π2).
Thus, when we consider the ﬁrst path P : vi −→... −→vj of the new bridge
B in order to decide whether it can be drawn inside, we check the top entries
t1 and t2 of Π1 and Π2, respectively.
If vj ⩾t1 and vj ⩾t2, then no merger is necessary; the attachments of B (in
C(1,vi)) are entered as a block (if C + B is found to be planar) on top of Π1.
If vj < t1 and vj ⩾t2, then we ﬁrst join all the blocks for which their highest
entry is higher than vj. To this end there is no need to check Π2, since vj ⩾t2;

176
8 Testing Graph Planarity
however, several blocks still may merge. Next, switch the sections of the new
block, that is, the section on Π1 exchanges places with the section on Π2. Finally,
place the attachments of B in nondecreasing order on top of Π1; these entries
join the new block.
If vj ⩾t1 and vj < t2, then again we join all blocks whose highest element is
greater than vj; only the sections on Π2 need to be checked. The attachments
of B join the same block are placed on top of Π1.
If vj < t1 and vj < t2, then all blocks whose highest element is greater than vj
are joined into one new block. As we join the blocks, we examine them one by
one. If the highest entry in the section on Π1 is higher than vj, then we switch the
sections. If it is still higher, then we halt and declare the graph nonplanar. If all
these switches succeed, then the mergeris completed by adding the attachments
of B on top of Π1.
In order to handle the sections switching, the examination of their tops and
mergers efﬁciently, we use a third stack, Π3. It consists of pairs of pointers, one
pair (x,y) per block; x points to the lowest entry of the section in Π1; and y, to
the lowest entry of the section in Π2. (If the section is empty, then the pointer’s
value is 0). Two adjacent blocks are joined by simply discarding the pair of the
top one. When several blocks have to be joined together upon the drawing of
the ﬁrst path of a new bridge, only the pair of the lowest of these blocks need
remain, except when one of its entries is 0. In this case, the lowest nonzero entry
of the pairs above it on the same side, if any such entry exists, takes its place.
When we enter a recursive step, a special “end of stack” marker E is placed on
top of Π2, and the three stacks are used as in the main algorithm. If the recursive
step ends successfully, we ﬁrst attempt to switch sections for each of the blocks
with a nonempty section on Π2, above the top most E. If we fail to expose E,
then C + B is nonplanar and we halt. Otherwise, all the blocks created during
the recursion are joined to the one which includes vj (the end vertex of the ﬁrst
path of B). The exposed E, on top of Π2, is removed, and we continue with the
previous level of recursion. When we backtrack into a vertex vi, all occurrences
of vi are removed from the top of Π1 and Π2, together with pairs of pointers of
Π3 which point to removed entries on both Π1 and Π2. (Technically, instead of
pointing to an occurrence of vi, we point to 0 and pairs (0,0) are removed).
Theorem 8.1 The complexity of the path addition algorithm is O(|V|).
Proof: As in the closing remarks of Section 8.1, we can assume |E| = O(|V|).
The DFS and the reordering of the adjacency lists have been shown to be O(|V|).
Each edge in the paths ﬁnding algorithm is used again, once in each direction.
The total number of entries in the stacks Π1 and Π2 is bounded by the number of

8.3 Computing an st-Numbering
177
back edges (|E|−|V|+1), and is therefore O(|V|). After each section switching
the number of blocks is reduced by one; thus the total work invested in section
switchings is O(|V|).
■
8.3 Computing an st-Numbering
In this section, we deﬁne an st-numbering and describe a linear time algorithm
to compute it. Thus numbering is necessary for the vertex addition algorithm,
for testing planarity, of Lempel, Even, and Cederbaum.
Given any edge s
t of a nonseparable graph G(V,E), a 1-1 function
g : V →{1,2,...,|V|} is called an st-numbering if the following conditions are
satisﬁed:
1. g(s) = 1.
2. g(t) = |V| (= n).
3. For every v ∈V −{s,t} there are adjacent vertices u and w such that g(u) <
g(v) < g(w).
Lempel, Even, and Cederbaum showed that, for every nonseparable graph
and every edge s
t, there exists an st-numbering. The algorithm described
here, following the work of Even and Tarjan [5], achieves this goal in linear
time.
The algorithm starts with a DFS whose ﬁrst vertex is t and ﬁrst edge is
t
s. (i.e., k(t) = 1 and k(s) = 2). This DFS computes for each vertex v, its
DFS number, k(v), its father, f(v), and its lowpoint L(v) and distinguishes tree
edges from back edges. This information is used in the paths-ﬁnding algorithm
to be described next, which is different from the one used in the path addition
algorithm.
Initially, s, t, and the edge connecting them are marked “old,” and all the
other edges and vertices are marked “new.” The path-ﬁnding algorithm starts
from a given vertex v and ﬁnds a path from it. This path may be directed from
v or into v.
1. If there is a “new” back edge v e
−→w (in this case k(w) < k(v)), then do
the following:
Mark e “old.”
The path is v e
−→w.
Halt.
2. If there is a “new” tree edge v e
−→w (in this case k(w) > k(v)), then do the
following:

178
8 Testing Graph Planarity
Trace a path whose ﬁrst edge is e, and from there it follows a path that deﬁned
L(w), that is, it goes up the tree and ends with a back edge into a vertex u
such that k(u) = L(w). All vertices and edges on the path are marked “old.”
Halt.
3. If there is a “new” back edge w e
−→v (in this case k(w) > k(v)), then do
the following:
Start the path with e (going backwards on it) and continue backwards via
tree edges until you encounter an “old” vertex. All vertices and edges on the
path are marked “old.” Halt.
4. (All edges incident to v are “old”). The path produced is empty. Halt.
Lemma 8.9 If the path ﬁnding algorithm is always applied from an “old”
vertex v ̸= t, then all the ancestors of an “old” vertex are “old” too.
Proof: By induction on the number of applications of the path ﬁnding algo-
rithm. Clearly, before the ﬁrst application, the only ancestor of s is t, and it is
old. Assuming the statement is true up to the present application, it is easy to
see that if any of the four steps is applicable, the statement continues to hold
after its application.
■
Corollary 8.3 If G is nonseparable and under the condition of Lemma 8.9,
each application of the path-ﬁndingalgorithm from an “old” vertex v produces
a path, through “new” vertices and edges, to another “old” vertex, or in case
all edges incident to v are “old”, it returns the empty path.
Proof: The only case which requires a discussion is when case (2) of the path
ﬁnding algorithm is applied. Since G is nonseparable, by Lemma 3.5, L(w) <
k(v). Thus, the path ends “bellow” v, in one of its ancestors. By Lemma 8.9,
this ancestor is “old.”
■
We are now ready to present the algorithm which produces an st-numbering.
It uses a stack S that initially contains only t and s, s on top of t.
1. i ←1.
2. Let v be the top vertex on S. Remove v from S. If v = t then g(t) ←i and
halt.
3. (v ̸= t) Apply the path ﬁnding algorithm to v. If the path is empty, then
g(v) ←i, i ←i + 1 and go to Step (2).
4. (The path is not empty) Let the path be v
u1
u2
...
ul
w.
Put ul,ul−1,...,u2,u1,v on S in this order (v comes out on top), and go to
Step (2).

8.4 The Vertex Addition Algorithm of Lempel, Even, and Cederbaum 179
Theorem 8.2 The algorithm above computes an st-numbering for every
nonseparable graph G(V,E).
Proof: First, we make a few observations about the algorithm:
1. No vertex ever appears in two or more places on S at the same time.
2. Once a vertex v is placed on S, nothing under v receives a number until v
does.
3. A vertex is permanently removed from S only after all its incident edges
become “old.”
Next, we want to show that each vertex v is placed on S before t is removed.
Since t and s are placed on S initially, the statement needs to be proved for
v ̸= s, t only. Since G is nonseparable, there exists a simple path from s to v
which does not pass through t (see Theorem 6.7 part (??)). Let this path be
s = u1
u2
···
ul = v. Let m be the ﬁrst index such that um is not
placed on S. Since um−1 is placed on S, t can be removed only after um−1
(fact (ii)), and um−1 is removed only after all its incident edges are “old” (fact
(ii)). Thus, um must be placed on S before t is removed.
It remains to be shown that the algorithm computes an st-numbering.
Since each vertex is placed on S and eventually is removed, each vertex v
gets a number g(v). Clearly, g(s) = 1, for it is the ﬁrst to be removed. After
each assignment i is incremented. Thus, g(t) = |V|. Every other vertex v is
placed on S, for the ﬁrst time, as an intermediate vertex on a path. Thus, there is
an adjacent vertex stored below it, and an adjacent vertex stored above it. The
one above it (by fact (ii)) gets a lower number and the one below it, a higher
number.
■
It is easy to see that the whole algorithm is of time complexity O(|E|): First,
the DFS is O(|E|). The total time spent on path ﬁnding is also O(|E|) since
no edge is used more than once. The total number of operations in the main
algorithm is bounded also by O(|E|) because the number of stack insertions is
exactly |E| + 1.
8.4 The Vertex Addition Algorithm of Lempel, Even, and Cederbaum
In this section, we assume that G(V,E) is a nonseparable graph whose vertices
are st-numbered. From now on, we shall refer to the vertices by their st-number.
Thus, V = {1,2,...,n}. Also, the edges are now directed from low to high.
A (graphical) source of a digraph is a vertex v such that din(v) = 0; a (graph-
ical) sink is a vertex v such that dout(v) = 0. (Do not confuse with the source

180
8 Testing Graph Planarity
2
(b)
(a)
3
2
1
3
4
5
6
1
4
6
5
6
5
4
Figure 8.1: (a) An example of a digraph; (b) B3 of the digraph from (a).
and sink of a network. The source of a network is not necessarily a (graphical)
source, etc.) Clearly, vertex 1 is a source of G and vertex n is a sink. Further-
more, due to the st-numbering, no other vertex is either a source or a sink. Let
Vk = {1,2,...,k}. Then, Gk(Vk,Ek) is the digraph induced by Vk, that is, Ek
consists of all the edges of G whose endpoints are both in Vk.
If G is planar, let ˆG be a plane realization of G. It contains a plane realization
of Gk. The following simple lemma reveals the reason for the st-numbering.
Lemma 8.10 If ˆGk is a plane realization of Gk contained in a plane digraph
ˆG, then all the edges and vertices of ˆG −ˆGk are drawn in one face of ˆGk.
Proof: Assume that a face F of ˆGk contains vertices of V −Vk. Since all the
vertices on F’s window are lower than the vertices in F, the highest vertex in F
must be a sink. Since G has only one sink, only one such face is possible.
■
Let Bk be the following digraph. Gk is a subgraph of Bk. In addition, Bk
contains all the edges of G which emanate from vertices of Vk and enter in
G, vertices of V −Vk. These edges are called virtual edges, and the leaves
they enter in Bk are called virtual vertices. These vertices are labeled as their
counterpartsin G, but they are kept separate; that is, there may be several virtual
vertices with the same label, each with exactly one entering edge. For example,
consider the digraph shown in Figure 8.1(a). B3 of this digraph is shown in
Figure 8.1(b).
By Lemma 8.10, we can assume that if G is planar, then there exists a plane
realization of Bk in which all the virtual edges are drawn in the outside face.
Furthermore, since 1 e
−→n is always an edge in G, if k < n then vertex 1 is
on the outside window and one of the virtual edges is e. In this case, we can

8.4 The Vertex Addition Algorithm of Lempel, Even, and Cederbaum 181
1
4
4
5
5
6
6
2
3
Figure 8.2: A bush form of B3, of our example.
draw Bk in the following form: Vertex 1 is drawn at the bottom level. All the
virtual vertices appear on one horizontal line. The remaining vertices of Gk are
drawn in such a way that vertices with higher names are drawn higher. Such a
realization is called a bush form. A bush form of B3, of our example, is shown
in Figure 8.2.
In fact, Lemma 8.10 implies that, if G is planar, then there exists a bush form
of Bk such that all the virtual vertices with labels k + 1 appear next to each
other on the horizontal line.
The algorithm proceeds by successively “drawing” B1,B2,...,Bn−1 and G.
If in the realization of Bk all the virtual vertices labeled k + 1 are next to each
other, then it is easy to draw Bk+1: One joins all the virtual vertices labeled
k + 1 into one vertex and “pulls” it down from the horizontal line. Now all the
edges of G that emanate from k + 1 are added, and their other endpoints are
labeled properly and placed in an arbitrary order on the horizontal line, in the
space evacuated by the former virtual vertices labeled k + 1.
However, a difﬁculty arises. Indeed, the discussion up to now guarantees that
if G is planar, then there exists a sequence of bush forms such that each one is
“grown” from the previous one. But since we do not have a plane realization
of G, we may put the virtual vertices, out of k + 1, in a “wrong” order. It is
necessary to show that this does not matter; namely, by simple transformations
it will be possible later to correct the “mistake.”
Lemma 8.11 Assume v is a separation vertex of Bk. If v > 1, then exactly one
component of Bk, with respect to v, contains vertices lower than v.

182
8 Testing Graph Planarity
Note that here we ignore the direction of the edges, and the lemma is actually
concerned with the undirected underlying graph of Bk.
Proof: The st-numbering implies that for every vertex u there exists a path
from 1 to u such that all the vertices on the path are less than u. Thus, if u < v
then there is a path from 1 to u that does not pass through v. Therefore, 1 and
u are in the same component.
■
Lemma 8.11 implies that a separation vertex v of Bk is the lowest vertex in
each of the components, except the one which contains 1 (in case v > 1). Each
of these components is a sub-bush; that is, it has the same structure as a bush
form, except that its lowest vertex is v rather than 1. These subbushes can be
permuted around v in any of the p! permutations, in case the number of these
subbushes is p. In addition, each of the subbushes can be ﬂipped over. These
transformations maintain the bush form. It is our purpose to show that if ˆB1
k
and ˆB2
k are bush forms of a planar Bk, then through a sequence of permutations
and ﬂippings, one can change ˆB1
k into a ˆB3
k such that the virtual vertices of Bk
appear in ˆB2
k and ˆB3
k in the same order.
For efﬁciency reasons, to be become clear to those readers who will study the
implementation through P Q-trees, we assume that when a subbush is ﬂipped,
smaller subbushes of other separation vertices of the component are not ﬂipped
by this action. For example, consider the bush form shown in Figure 8.3(a). The
bush form of Figure 8.3(b) is achieved by permuting about 1, Figure 8.3(c) by
ﬂipping about 1 and Figure 8.3(d) by ﬂipping about 2.
Lemma 8.12 Let H be a maximal nonseparable component of Bk and
y1,y2,...,ym be the vertices of H that are also endpoints of edges of Bk −H.
In every bush form ˆBk, all the y’s are on the outside window of H and in the
same order, except that the orientation may be reversed.
Proof: Since ˆBk is a bush form, all the y’s are on the outside face of H. Assume
there are two bush forms ˆB1
k and ˆB2
k in which the realizations of H are ˆH1 and
ˆH2, respectively. If the y’s do not appear in the same order on the outside
windows of ˆH1 and ˆH2, then there are two y’s, yi and yj which are next to each
other in ˆH1 but not in ˆH2 (see Fig. 8.4). Therefore, in ˆH2, there are two other
y’s, yk and yl which interpose between yi and yj on the two paths between
them on the outside window of ˆH2. However, from ˆH1 we see that there are
two paths, P1[yi,yj] and P2[yk,yl], which are completely disjoint. These two
paths cannot exist simultaneously in ˆH2. A contradiction.
■

8.4 The Vertex Addition Algorithm of Lempel, Even, and Cederbaum 183
7
2
3
1
4
5
6
9
8
9
8
9
2
3
1
5
7
6
4
9
5
8
4
9
3
2
1
9
8
6
7
9
(a)
(b)
(c)
(d)
8
9
9
8
8
9
8
9
9
8
9
8
9
5
8
4
9
7
6
3
2
1
8
9
Figure 8.3: (a) A bush form; (b) a bush form obtained by permuting (a)
about vertex 1; (c) a bush form obtained by ﬂipping (a) about vertex 1;
(d) a bush form obtained by ﬂipping (c) about vertex 2.
yi
yi
yk
yj
yl
yj
ˆH1
ˆH 2
Figure 8.4: Proof of Lemma 8.12.
Theorem 8.3 If ˆB1
k and ˆB2
k are bush forms of the same Bk, then there exists a
sequence of permutations and ﬂippings that transforms ˆB1
k into ˆB3
k, such that
in ˆB2
k and ˆB3
k, the virtual vertices appear in the same order.
Proof: By induction on the size of bush or subbush forms.1 Clearly, if each
of the two (sub-)bushes consists of only one vertex and one virtual vertex,
then the statement is trivial. Let v be the lowest vertex in the (sub-)bushes
1 Size the number of vertices.

184
8 Testing Graph Planarity
ˆB1 and ˆB2 of the same B. If v is a separation vertex, then the components of
B appear as subbushes in ˆB1 and ˆB2. If they are not in the same order, it is
possible, by permuting them in ˆB1, to put them in the same order as in ˆB2. By
the inductive hypothesis, there is a sequence of permutations and ﬂippings that
will change each subbush of ˆB1 to have the same order of its virtual vertice as
in its counterpart in ˆB2, and therefore the theorem follows.
If v is not a separating vertex then let H be the maximal nonseparable compo-
nent of B which contains v. In ˆB1( ˆB2) there is a planar realization ˆH1( ˆH2) of H.
The vertices y1,y2,...,ym of H, which are also endpoints of edges of B −H,
by Lemma 8.12, must appear on the outside window of H in the same order, up
to orientation. If the orientation of the y’s in ˆH1 is opposite to that of ˆH2, ﬂip
the (sub-)bush ˆB1 about v. Now, each of the y’s is the lowest vertex of some
subbush of B, and these subbushes appear in (the new) ˆB1 and ˆB2 in the same
order. By the inductive hypothesis, each of these subbushes can be transformed
by a sequence of permutations and ﬂippings to have its virtual vertices in the
same order as its counterpart in ˆB2.
■
Corollary 8.4 If G is planar and ˆBk is a bush form of Bk, then there exists a
sequence of permutations and ﬂippings which transforms ˆBk into a ˆB′
k in which
all the virtual vertices labeled k + 1 appear together on the horizontal line.
It remains to be shown how one decides which permutation or ﬂipping to
apply, how to represent the necessary information, without actually drawing
bush forms, and how to do it all efﬁciently. Lempel, Even, and Cederbaum
described a method that uses a representation of the pertinent information by
proper expressions. However, a better representation was suggested by Booth
and Leuker. They invented a data structure, called PQ-trees, through which the
algorithm can be run in linear time. PQ-trees were used to solve other problems
of interest; see [6].
I shall not describePQ-trees in detail. The description in [6] is long (30 pages)
although not hard to follow. It involves ideas of data structure manipulation, but
almost no graph theory. The following is a brief and incomplete description.
A PQ-tree is a directed ordered tree, with three types of vertices: P-vertices,
Q-vertices and leaves. Each P-vertex or Q-vertex, has at least one son. The
sons of a P-vertex, which in our application represents a separating vertex v of
Bk, may be permuted into any new order. Each sons and its subtree, represents
a subbush. A Q-vertex represents a maximal nonseparable component, and its
sons, which represent the y’s, may not be permuted, but their order can be
reversed. The leaves represent the virtual vertices.

8.5 Problems
185
The attempt to gather all the leaves labeled k + 1 into an unbroken run, is
done from sons to fathers, starting with the leaves labeled k + 1. Through a
technique of template matching, vertices are modiﬁed while the k + 1 labeled
leaves are bunched together. Only the smallest subtree containing all the k+1 –
labeled leaves is scanned. All these leaves, if successfully gathered, are merged
into one P-vertex, and its sons represent the virtual edges out of k + 1. This
procedure is repeated until k + 1 = n.
8.5 Problems
Problem 8.1 Demonstrate the path-addition algorithm on the Peterson graph
(see Problem 7.3). Show the data for all the steps: the DFS for numbering
the vertices, deﬁning the tree and computing L1 and L2. The Φ function on
the edges. The sorting of the adjacency lists. Use the path-ﬁnding algorithm
in the new DFS to decompose the graph into C and a sequence of paths. Use
Π1, Π2, Π3 and end-of-stack markers to carry out all the recursive steps up to
planarity decision.
Problem 8.2 Repeat the path-addition planarity test, as in Problem 8.1, for the
graph depicted in Figure 8.5.
Figure 8.5: A graph for Problem 8.2.
Problem 8.3 Demonstrate the vertex-addition planarity test on the Peterson
graph. Show the steps for the DFS, the st-numbering,and the sequence of bush
forms.
Problem 8.4 Repeat the vertex-addition planarity test for the graph of
Problem 8.2.
Problem 8.5 Show that if a graph is nonplanar,then a subgraph homeomorphic
to one of the Kuratowski’s graphs can be found in O(|V|2). (Hints: Only O(|V|)
edges need to be considered. Delete edges if their deletion does not make the
graph planar. What is left?)

186
8 Testing Graph Planarity
Bibliography
[1] Auslander, L., and Parter, S. V., “On Embedding Graphs in the Plane,” J. Math. and
Mech., Vol. 10, No. 3, May 1961, pp. 517–523.
[2] Goldstein, A. J., “An Efﬁcient and Constructive Algorithm for Testing Whether a
Graph Can Be Embedded in a Plane,” Graph and Combinatorics Conf., Contract
No. NONR 1858-(21), Ofﬁce of Naval Research Logistics Proj., Dept. of Math.,
Princeton Univ., May 16–18, 1963, 2 pp.
[3] Hopcroft, J., and Tarjan, R., “Efﬁcient Planarity Testing,” JACM, Vol. 21, No. 4,
Oct. 1974, pp. 549–568.
[4] Lempel, A., Even, S., and Cederbaum, I., “An Algorithm for Planarity Test-
ing of Graphs,” Theory of Graphs, International Symposium, Rome, July, 1966.
P. Rosenstiehl, Ed., Gordon and Breach, N.Y. 1967, pp. 215–232.
[5] Even, S., and Tarjan, R. E., “Computing and st-numbering,” Th. Comp. Sci., Vol. 2,
1976, pp. 339–344.
[6] Booth, K. S., and Lueker, G. S., “Testing for the Consecutive Ones Property, Interval
Graphs, and Graph Planarity Using PQ-tree Algorithms,” J. of Comp. and Sys.
Sciences, Vol. 13, 1976, pp. 335–379.

Index
acyclic digraph, 141
adjacency list, 170
adjacency matrix, 3
attachments, 146
augment procedure, 89
augmenting path, 87
backward cut, 86
BFS, see breadth-ﬁrst search
biconnected, 127
bipartite graph, 135
Breadth-ﬁrst search, 11
bridge, 63
bridges, 146
interlace, 148
capacity, 85
capacity function, 85
capacity of a cut, 87
residual capacity, 94
circuit, 2
simple, 2
classiﬁable, 143
clique, 44, 63
code, 65
word length, 65
alphabet, 65
characteristic sum, 67
characteristic sum condition, 67
exhaustive, 83
letters, 65
message, 65
preﬁx, 65, 68
sufﬁx, 65
tail, 65
uniquely decipherable, 65
word, 65
coloring, 149
component, 146
nonseparable, 52
singular component, 146
strong, 58
superstructure, 54
concurrent set of edges, 139
connectivity
vertex, 121, 124
contraction, 160
critical path, 138
cut, 33, 86
cutset, 159
Dantzig algorithm, 27
De Bruijn sequence, 9
depth-ﬁrst search, 46
directed, 57
lowpoint, 53
number, 49
tree, 49
back edges, 51
tree edges, 51
DFS, see depth-ﬁrst search
digraph, see graph
Dijkstra algorithm, 14
Dinitz algorithm, 94
187

188
Index
edge, 1
antiparallel, 3
edge array, 4
parallel, 3
self-loop, 3
edge rule, 102
edge separator, 130
Euler, 6
ﬂow
absorb, 114
blocking, 95
maximal, 95
ﬂow function, 85
Floyd algorithm, 21
Ford algorithm, 17
Ford-Fulkerson algorithm, 87
forward cut, 86
geometric dual, 166
graph, 1
2-colorable, 149
acyclic, 24
bipartite, 149
circuit-free, 29
connected, 2
directed (digraph), 2
arbitrated, 38
dual, 160
Euler graph, 6
planar, 146
sparse, 4
strongly connected, 3, 58
triangular, 166
triangulation, 166
underlying graph, 7
homeomorphic, 152
incidence list, 4
independent set, 142
isomorphic, 152
label
labeling procedure, 88
leaf, 31
marriage problem, 135
matching, 135
complete, 137
max-ﬂow min-cut theorem, 94
Menger’s Theorem, 122
network, 85
0-1, 117
0-1 type 1, 119
0-1 type 2, 120
auxiliary network, 103
layered network, 95
secondary network, 94
nonseparable, 126
path, 2
directed, 3
directed Euler path, 7
Euler path, 6
Hamilton, 23
shortest, 11
simple, 2
PERT digraph, 137
Petersen graph, 165
planar
PQ-trees, 184
block, 175
bush form, 181
equivalent, 157
face, 150
external, 151
path addition, 168
second lowpoint function, 169
sink, 179
source, 179
st-numbering, 177
vetex addition, 169
virtual edges, 180
virtual vertices, 180
window, 150
plane graph, 146
Prim algorithm, 32
root, 37
separating edge, 162
separation vertex, 52
sink, 85
auxiliary sink, 103
slice, 142
source, 85
auxiliary source, 103
spanning tree, 32
subgraph, 31

Index
189
topological sorting, 24
total ﬂow, 85
Trémaux’s algorithm, 46
tree, 29
directed, 37
useful
backwardly useful, 88
forwardly useful, 88
useful edge, 88
vertex, 1
degree, 1
in-degree, 3
out-degree, 3
sink, 58
source, 58
vertex array, 4
vertex separator, 122, 130
Warshall’s Algorithm, 26
Warshall’s algorithm, 26



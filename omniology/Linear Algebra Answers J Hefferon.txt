Answers to Exercises
Linear Algebra
Jim Hefferon
¡2
1
¢
¡1
3
¢
¯¯¯¯
1
2
3
1
¯¯¯¯
¡2
1
¢
x1 · ¡1
3
¢
¯¯¯¯
x · 1
2
x · 3
1
¯¯¯¯
¡2
1
¢
¡6
8
¢
¯¯¯¯
6
2
8
1
¯¯¯¯

Notation
R, R+, Rn
real numbers, reals greater than 0, n-tuples of reals
N
natural numbers: {0, 1, 2, . . .}
C
complex numbers
{. . .
¯¯ . . .}
set of . . . such that . . .
(a .. b), [a .. b]
interval (open or closed) of reals between a and b
⟨. . .⟩
sequence; like a set but order matters
V, W, U
vector spaces
⃗v, ⃗w
vectors
⃗0, ⃗0V
zero vector, zero vector of V
B, D
bases
En = ⟨⃗e1, . . . , ⃗en⟩
standard basis for Rn
⃗β,⃗δ
basis vectors
RepB(⃗v)
matrix representing the vector
Pn
set of n-th degree polynomials
Mn×m
set of n×m matrices
[S]
span of the set S
M ⊕N
direct sum of subspaces
V ∼= W
isomorphic spaces
h, g
homomorphisms, linear maps
H, G
matrices
t, s
transformations; maps from a space to itself
T, S
square matrices
RepB,D(h)
matrix representing the map h
hi,j
matrix entry from row i, column j
|T|
determinant of the matrix T
R(h), N (h)
rangespace and nullspace of the map h
R∞(h), N∞(h)
generalized rangespace and nullspace
Lower case Greek alphabet
name
character
name
character
name
character
alpha
α
iota
ι
rho
ρ
beta
β
kappa
κ
sigma
σ
gamma
γ
lambda
λ
tau
τ
delta
δ
mu
µ
upsilon
υ
epsilon
ϵ
nu
ν
phi
φ
zeta
ζ
xi
ξ
chi
χ
eta
η
omicron
o
psi
ψ
theta
θ
pi
π
omega
ω
Cover. This is Cramer’s Rule for the system x1 + 2x2 = 6, 3x1 + x2 = 8. The size of the ﬁrst box is the
determinant shown (the absolute value of the size is the area). The size of the second box is x1 times that, and
equals the size of the ﬁnal box. Hence, x1 is the ﬁnal determinant divided by the ﬁrst determinant.

These are answers to the exercises in Linear Algebra by J. Hefferon. Corrections or comments are
very welcome, email to jimjoshua.smcvt.edu
An answer labeled here as, for instance, One.II.3.4, matches the question numbered 4 from the ﬁrst
chapter, second section, and third subsection. The Topics are numbered separately.


Contents
Chapter One: Linear Systems
4
Subsection One.I.1: Gauss’ Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
Subsection One.I.2: Describing the Solution Set . . . . . . . . . . . . . . . . . . . . . . .
10
Subsection One.I.3: General = Particular + Homogeneous . . . . . . . . . . . . . . . . .
14
Subsection One.II.1: Vectors in Space
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
Subsection One.II.2: Length and Angle Measures . . . . . . . . . . . . . . . . . . . . . .
20
Subsection One.III.1: Gauss-Jordan Reduction
. . . . . . . . . . . . . . . . . . . . . . .
25
Subsection One.III.2: Row Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
Topic: Computer Algebra Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
Topic: Input-Output Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
Topic: Accuracy of Computations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
Topic: Analyzing Networks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
Chapter Two: Vector Spaces
36
Subsection Two.I.1: Deﬁnition and Examples . . . . . . . . . . . . . . . . . . . . . . . .
37
Subsection Two.I.2: Subspaces and Spanning Sets
. . . . . . . . . . . . . . . . . . . . .
40
Subsection Two.II.1: Deﬁnition and Examples . . . . . . . . . . . . . . . . . . . . . . . .
46
Subsection Two.III.1: Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
Subsection Two.III.2: Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
Subsection Two.III.3: Vector Spaces and Linear Systems . . . . . . . . . . . . . . . . . .
61
Subsection Two.III.4: Combining Subspaces . . . . . . . . . . . . . . . . . . . . . . . . .
66
Topic: Fields
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
Topic: Crystals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
Topic: Dimensional Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
Chapter Three: Maps Between Spaces
73
Subsection Three.I.1: Definition and Examples
. . . . . . . . . . . . . . . . . . . . . . .
75
Subsection Three.I.2: Dimension Characterizes Isomorphism . . . . . . . . . . . . . . . .
83
Subsection Three.II.1: Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
Subsection Three.II.2: Rangespace and Nullspace . . . . . . . . . . . . . . . . . . . . . .
90
Subsection Three.III.1: Representing Linear Maps with Matrices . . . . . . . . . . . . .
95
Subsection Three.III.2: Any Matrix Represents a Linear Map . . . . . . . . . . . . . . .
103
Subsection Three.IV.1: Sums and Scalar Products
. . . . . . . . . . . . . . . . . . . . .
107
Subsection Three.IV.2: Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . .
108
Subsection Three.IV.3: Mechanics of Matrix Multiplication
. . . . . . . . . . . . . . . .
113
Subsection Three.IV.4: Inverses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
Subsection Three.V.1: Changing Representations of Vectors . . . . . . . . . . . . . . . .
121
Subsection Three.V.2: Changing Map Representations . . . . . . . . . . . . . . . . . . .
125
Subsection Three.VI.1: Orthogonal Projection Into a Line . . . . . . . . . . . . . . . . .
128
Subsection Three.VI.2: Gram-Schmidt Orthogonalization
. . . . . . . . . . . . . . . . .
131
Subsection Three.VI.3: Projection Into a Subspace . . . . . . . . . . . . . . . . . . . . .
138
Topic: Line of Best Fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
144
Topic: Geometry of Linear Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148
Topic: Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
Topic: Orthonormal Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
158
Chapter Four: Determinants
159
Subsection Four.I.1: Exploration
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
161
Subsection Four.I.2: Properties of Determinants . . . . . . . . . . . . . . . . . . . . . . .
163
Subsection Four.I.3: The Permutation Expansion . . . . . . . . . . . . . . . . . . . . . .
166
Subsection Four.I.4: Determinants Exist . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
Subsection Four.II.1: Determinants as Size Functions . . . . . . . . . . . . . . . . . . . .
170
Subsection Four.III.1: Laplace’s Expansion
. . . . . . . . . . . . . . . . . . . . . . . . .
173
Topic: Cramer’s Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
176

4
Linear Algebra, by Hefferon
Topic: Speed of Calculating Determinants . . . . . . . . . . . . . . . . . . . . . . . . . .
177
Topic: Projective Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
178
Chapter Five: Similarity
180
Subsection Five.II.1: Deﬁnition and Examples . . . . . . . . . . . . . . . . . . . . . . . .
181
Subsection Five.II.2: Diagonalizability . . . . . . . . . . . . . . . . . . . . . . . . . . . .
184
Subsection Five.II.3: Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . .
188
Subsection Five.III.1: Self-Composition
. . . . . . . . . . . . . . . . . . . . . . . . . . .
192
Subsection Five.III.2: Strings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
194
Subsection Five.IV.1: Polynomials of Maps and Matrices . . . . . . . . . . . . . . . . . .
198
Subsection Five.IV.2: Jordan Canonical Form . . . . . . . . . . . . . . . . . . . . . . . .
205
Topic: Method of Powers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212
Topic: Stable Populations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212
Topic: Linear Recurrences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212
Chapter One: Linear Systems
213
Subsection One.I.1: Gauss’ Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
Subsection One.I.2: Describing the Solution Set . . . . . . . . . . . . . . . . . . . . . . .
220
Subsection One.I.3: General = Particular + Homogeneous . . . . . . . . . . . . . . . . .
224
Subsection One.II.1: Vectors in Space
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
227
Subsection One.II.2: Length and Angle Measures . . . . . . . . . . . . . . . . . . . . . .
230
Subsection One.III.1: Gauss-Jordan Reduction
. . . . . . . . . . . . . . . . . . . . . . .
235
Subsection One.III.2: Row Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
Topic: Computer Algebra Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241
Topic: Input-Output Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
Topic: Accuracy of Computations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
Topic: Analyzing Networks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
244
Chapter Two: Vector Spaces
246
Subsection Two.I.1: Deﬁnition and Examples . . . . . . . . . . . . . . . . . . . . . . . .
247
Subsection Two.I.2: Subspaces and Spanning Sets
. . . . . . . . . . . . . . . . . . . . .
250
Subsection Two.II.1: Deﬁnition and Examples . . . . . . . . . . . . . . . . . . . . . . . .
256
Subsection Two.III.1: Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
Subsection Two.III.2: Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
268
Subsection Two.III.3: Vector Spaces and Linear Systems . . . . . . . . . . . . . . . . . .
271
Subsection Two.III.4: Combining Subspaces . . . . . . . . . . . . . . . . . . . . . . . . .
276
Topic: Fields
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279
Topic: Crystals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
280
Topic: Dimensional Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
281
Chapter Three: Maps Between Spaces
283
Subsection Three.I.1: Definition and Examples
. . . . . . . . . . . . . . . . . . . . . . .
285
Subsection Three.I.2: Dimension Characterizes Isomorphism . . . . . . . . . . . . . . . .
293
Subsection Three.II.1: Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
295
Subsection Three.II.2: Rangespace and Nullspace . . . . . . . . . . . . . . . . . . . . . .
300
Subsection Three.III.1: Representing Linear Maps with Matrices . . . . . . . . . . . . .
305
Subsection Three.III.2: Any Matrix Represents a Linear Map . . . . . . . . . . . . . . .
313
Subsection Three.IV.1: Sums and Scalar Products
. . . . . . . . . . . . . . . . . . . . .
317
Subsection Three.IV.2: Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . .
318
Subsection Three.IV.3: Mechanics of Matrix Multiplication
. . . . . . . . . . . . . . . .
323
Subsection Three.IV.4: Inverses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
326
Subsection Three.V.1: Changing Representations of Vectors . . . . . . . . . . . . . . . .
331
Subsection Three.V.2: Changing Map Representations . . . . . . . . . . . . . . . . . . .
335
Subsection Three.VI.1: Orthogonal Projection Into a Line . . . . . . . . . . . . . . . . .
338
Subsection Three.VI.2: Gram-Schmidt Orthogonalization
. . . . . . . . . . . . . . . . .
341
Subsection Three.VI.3: Projection Into a Subspace . . . . . . . . . . . . . . . . . . . . .
348
Topic: Line of Best Fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
354

Answers to Exercises
5
Topic: Geometry of Linear Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
358
Topic: Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
361
Topic: Orthonormal Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
368
Chapter Four: Determinants
369
Subsection Four.I.1: Exploration
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
371
Subsection Four.I.2: Properties of Determinants . . . . . . . . . . . . . . . . . . . . . . .
373
Subsection Four.I.3: The Permutation Expansion . . . . . . . . . . . . . . . . . . . . . .
376
Subsection Four.I.4: Determinants Exist . . . . . . . . . . . . . . . . . . . . . . . . . . .
378
Subsection Four.II.1: Determinants as Size Functions . . . . . . . . . . . . . . . . . . . .
380
Subsection Four.III.1: Laplace’s Expansion
. . . . . . . . . . . . . . . . . . . . . . . . .
383
Topic: Cramer’s Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
386
Topic: Speed of Calculating Determinants . . . . . . . . . . . . . . . . . . . . . . . . . .
387
Topic: Projective Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
388
Chapter Five: Similarity
390
Subsection Five.II.1: Deﬁnition and Examples . . . . . . . . . . . . . . . . . . . . . . . .
391
Subsection Five.II.2: Diagonalizability . . . . . . . . . . . . . . . . . . . . . . . . . . . .
394
Subsection Five.II.3: Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . .
398
Subsection Five.III.1: Self-Composition
. . . . . . . . . . . . . . . . . . . . . . . . . . .
402
Subsection Five.III.2: Strings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
404
Subsection Five.IV.1: Polynomials of Maps and Matrices . . . . . . . . . . . . . . . . . .
408
Subsection Five.IV.2: Jordan Canonical Form . . . . . . . . . . . . . . . . . . . . . . . .
415
Topic: Method of Powers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
422
Topic: Stable Populations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
422
Topic: Linear Recurrences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
422


Chapter One: Linear Systems
Subsection One.I.1: Gauss’ Method
One.I.1.16
Gauss’ method can be performed in diﬀerent ways, so these simply exhibit one possible
way to get the answer.
(a) Gauss’ method
−(1/2)ρ1+ρ2
−→
2x +
3y =
13
−(5/2)y = −15/2
gives that the solution is y = 3 and x = 2.
(b) Gauss’ method here
−3ρ1+ρ2
−→
ρ1+ρ3
x
−z = 0
y + 3z = 1
y
= 4
−ρ2+ρ3
−→
x
−
z = 0
y +
3z = 1
−3z = 3
gives x = −1, y = 4, and z = −1.
One.I.1.17
(a) Gaussian reduction
−(1/2)ρ1+ρ2
−→
2x +
2y =
5
−5y = −5/2
shows that y = 1/2 and x = 2 is the unique solution.
(b) Gauss’ method
ρ1+ρ2
−→
−x + y = 1
2y = 3
gives y = 3/2 and x = 1/2 as the only solution.
(c) Row reduction
−ρ1+ρ2
−→
x −3y + z = 1
4y + z = 13
shows, because the variable z is not a leading variable in any row, that there are many solutions.
(d) Row reduction
−3ρ1+ρ2
−→
−x −y =
1
0 = −1
shows that there is no solution.
(e) Gauss’ method
ρ1↔ρ4
−→
x + y −z = 10
2x −2y + z = 0
x
+ z = 5
4y + z = 20
−2ρ1+ρ2
−→
−ρ1+ρ3
x +
y −z =
10
−4y + 3z = −20
−y + 2z = −5
4y + z =
20
−(1/4)ρ2+ρ3
−→
ρ2+ρ4
x +
y −
z =
10
−4y +
3z = −20
(5/4)z =
0
4z =
0
gives the unique solution (x, y, z) = (5, 5, 0).
(f) Here Gauss’ method gives
−(3/2)ρ1+ρ3
−→
−2ρ1+ρ4
2x
+
z +
w =
5
y
−
w =
−1
−(5/2)z −(5/2)w = −15/2
y
−
w =
−1
−ρ2+ρ4
−→
2x
+
z +
w =
5
y
−
w =
−1
−(5/2)z −(5/2)w = −15/2
0 =
0
which shows that there are many solutions.
One.I.1.18
(a) From x = 1 −3y we get that 2(1 −3y) + y = −3, giving y = 1.
(b) From x = 1 −3y we get that 2(1 −3y) + 2y = 0, leading to the conclusion that y = 1/2.
Users of this method must check any potential solutions by substituting back into all the equations.

8
Linear Algebra, by Hefferon
One.I.1.19
Do the reduction
−3ρ1+ρ2
−→
x −y =
1
0 = −3 + k
to conclude this system has no solutions if k ̸= 3 and if k = 3 then it has inﬁnitely many solutions. It
never has a unique solution.
One.I.1.20
Let x = sin α, y = cos β, and z = tan γ:
2x −y + 3z = 3
4x + 2y −2z = 10
6x −3y + z = 9
−2ρ1+ρ2
−→
−3ρ1+ρ3
2x −y +
3z = 3
4y −
8z = 4
−8z = 0
gives z = 0, y = 1, and x = 2. Note that no α satisﬁes that requirement.
One.I.1.21
(a) Gauss’ method
−3ρ1+ρ2
−→
−ρ1+ρ3
−2ρ1+ρ4
x −3y =
b1
10y = −3b1 + b2
10y = −b1 + b3
10y = −2b1 + b4
−ρ2+ρ3
−→
−ρ2+ρ4
x −3y =
b1
10y =
−3b1 + b2
0 = 2b1 −b2 + b3
0 = b1 −b2 + b4
shows that this system is consistent if and only if both b3 = −2b1 + b2 and b4 = −b1 + b2.
(b) Reduction
−2ρ1+ρ2
−→
−ρ1+ρ3
x1 +
2x2 + 3x3 =
b1
x2 −3x3 = −2b1 + b2
−2x2 + 5x3 = −b1 + b3
2ρ2+ρ3
−→
x1 + 2x2 + 3x3 =
b1
x2 −3x3 =
−2b1 + b2
−x3 = −5b1 + +2b2 + b3
shows that each of b1, b2, and b3 can be any real number — this system always has a unique solution.
One.I.1.22
This system with more unknowns than equations
x + y + z = 0
x + y + z = 1
has no solution.
One.I.1.23
Yes. For example, the fact that the same reaction can be performed in two diﬀerent ﬂasks
shows that twice any solution is another, diﬀerent, solution (if a physical reaction occurs then there
must be at least one nonzero solution).
One.I.1.24
Because f(1) = 2, f(−1) = 6, and f(2) = 3 we get a linear system.
1a + 1b + c = 2
1a −1b + c = 6
4a + 2b + c = 3
Gauss’ method
−ρ1+ρ2
−→
−4ρ1+ρ2
a +
b + c =
2
−2b
=
4
−2b −3c = −5
−ρ2+ρ3
−→
a +
b +
c =
2
−2b
=
4
−3c = −9
shows that the solution is f(x) = 1x2 −2x + 3.
One.I.1.25
(a) Yes, by inspection the given equation results from −ρ1 + ρ2.
(b) No. The given equation is satisﬁed by the pair (1, 1). However, that pair does not satisfy the
ﬁrst equation in the system.
(c) Yes. To see if the given row is c1ρ1 + c2ρ2, solve the system of equations relating the coeﬃcients
of x, y, z, and the constants:
2c1 + 6c2 =
6
c1 −3c2 = −9
−c1 + c2 =
5
4c1 + 5c2 = −2
and get c1 = −3 and c2 = 2, so the given row is −3ρ1 + 2ρ2.
One.I.1.26
If a ̸= 0 then the solution set of the ﬁrst equation is {(x, y)
¯¯ x = (c −by)/a}. Taking y = 0
gives the solution (c/a, 0), and since the second equation is supposed to have the same solution set,
substituting into it gives that a(c/a) + d · 0 = e, so c = e. Then taking y = 1 in x = (c −by)/a gives
that a((c −b)/a) + d · 1 = e, which gives that b = d. Hence they are the same equation.
When a = 0 the equations can be diﬀerent and still have the same solution set: e.g., 0x + 3y = 6
and 0x + 6y = 12.

Answers to Exercises
9
One.I.1.27
We take three cases: that a ̸= 0, that a = 0 and c ̸= 0, and that both a = 0 and c = 0.
For the ﬁrst, we assume that a ̸= 0. Then the reduction
−(c/a)ρ1+ρ2
−→
ax +
by =
j
(−cb
a + d)y = −cj
a + k
shows that this system has a unique solution if and only if −(cb/a) + d ̸= 0; remember that a ̸= 0
so that back substitution yields a unique x (observe, by the way, that j and k play no role in the
conclusion that there is a unique solution, although if there is a unique solution then they contribute
to its value). But −(cb/a)+d = (ad−bc)/a and a fraction is not equal to 0 if and only if its numerator
is not equal to 0. Thus, in this ﬁrst case, there is a unique solution if and only if ad −bc ̸= 0.
In the second case, if a = 0 but c ̸= 0, then we swap
cx + dy = k
by = j
to conclude that the system has a unique solution if and only if b ̸= 0 (we use the case assumption that
c ̸= 0 to get a unique x in back substitution). But — where a = 0 and c ̸= 0 — the condition “b ̸= 0”
is equivalent to the condition “ad −bc ̸= 0”. That ﬁnishes the second case.
Finally, for the third case, if both a and c are 0 then the system
0x + by = j
0x + dy = k
might have no solutions (if the second equation is not a multiple of the ﬁrst) or it might have inﬁnitely
many solutions (if the second equation is a multiple of the ﬁrst then for each y satisfying both equations,
any pair (x, y) will do), but it never has a unique solution. Note that a = 0 and c = 0 gives that
ad −bc = 0.
One.I.1.28
Recall that if a pair of lines share two distinct points then they are the same line. That’s
because two points determine a line, so these two points determine each of the two lines, and so they
are the same line.
Thus the lines can share one point (giving a unique solution), share no points (giving no solutions),
or share at least two points (which makes them the same line).
One.I.1.29
For the reduction operation of multiplying ρi by a nonzero real number k, we have that
(s1, . . . , sn) satisﬁes this system
a1,1x1 + a1,2x2 + · · · + a1,nxn = d1
...
kai,1x1 + kai,2x2 + · · · + kai,nxn = kdi
...
am,1x1 + am,2x2 + · · · + am,nxn = dm
if and only if
a1,1s1 + a1,2s2 + · · · + a1,nsn = d1
...
and kai,1s1 + kai,2s2 + · · · + kai,nsn = kdi
...
and am,1s1 + am,2s2 + · · · + am,nsn = dm
by the deﬁnition of ‘satisﬁes’. But, because k ̸= 0, that’s true if and only if
a1,1s1 + a1,2s2 + · · · + a1,nsn = d1
...
and ai,1s1 + ai,2s2 + · · · + ai,nsn = di
...
and am,1s1 + am,2s2 + · · · + am,nsn = dm
(this is straightforward cancelling on both sides of the i-th equation), which says that (s1, . . . , sn)

10
Linear Algebra, by Hefferon
solves
a1,1x1 + a1,2x2 + · · · + a1,nxn = d1
...
ai,1x1 + ai,2x2 + · · · + ai,nxn = di
...
am,1x1 + am,2x2 + · · · + am,nxn = dm
as required.
For the pivot operation kρi + ρj, we have that (s1, . . . , sn) satisﬁes
a1,1x1 + · · · +
a1,nxn =
d1
...
ai,1x1 + · · · +
ai,nxn =
di
...
(kai,1 + aj,1)x1 + · · · + (kai,n + aj,n)xn = kdi + dj
...
am,1x1 + · · · +
am,nxn = dm
if and only if
a1,1s1 + · · · + a1,nsn = d1
...
and ai,1s1 + · · · + ai,nsn = di
...
and (kai,1 + aj,1)s1 + · · · + (kai,n + aj,n)sn = kdi + dj
...
and am,1s1 + am,2s2 + · · · + am,nsn = dm
again by the deﬁnition of ‘satisﬁes’. Subtract k times the i-th equation from the j-th equation (re-
mark: here is where i ̸= j is needed; if i = j then the two di’s above are not equal) to get that the
previous compound statement holds if and only if
a1,1s1 + · · · + a1,nsn = d1
...
and ai,1s1 + · · · + ai,nsn = di
...
and (kai,1 + aj,1)s1 + · · · + (kai,n + aj,n)sn
−(kai,1s1 + · · · + kai,nsn) = kdi + dj −kdi
...
and am,1s1 + · · · + am,nsn = dm
which, after cancellation, says that (s1, . . . , sn) solves
a1,1x1 + · · · + a1,nxn = d1
...
ai,1x1 + · · · + ai,nxn = di
...
aj,1x1 + · · · + aj,nxn = dj
...
am,1x1 + · · · + am,nxn = dm
as required.
One.I.1.30
Yes, this one-equation system:
0x + 0y = 0
is satisﬁed by every (x, y) ∈R2.

Answers to Exercises
11
One.I.1.31
Yes. This sequence of operations swaps rows i and j
ρi+ρj
−→
−ρj+ρi
−→
ρi+ρj
−→
−1ρi
−→
so the row-swap operation is redundant in the presence of the other two.
One.I.1.32
Swapping rows is reversed by swapping back.
a1,1x1 + · · · + a1,nxn = d1
...
am,1x1 + · · · + am,nxn = dm
ρi↔ρj
−→
ρj↔ρi
−→
a1,1x1 + · · · + a1,nxn = d1
...
am,1x1 + · · · + am,nxn = dm
Multiplying both sides of a row by k ̸= 0 is reversed by dividing by k.
a1,1x1 + · · · + a1,nxn = d1
...
am,1x1 + · · · + am,nxn = dm
kρi
−→
(1/k)ρi
−→
a1,1x1 + · · · + a1,nxn = d1
...
am,1x1 + · · · + am,nxn = dm
Adding k times a row to another is reversed by adding −k times that row.
a1,1x1 + · · · + a1,nxn = d1
...
am,1x1 + · · · + am,nxn = dm
kρi+ρj
−→
−kρi+ρj
−→
a1,1x1 + · · · + a1,nxn = d1
...
am,1x1 + · · · + am,nxn = dm
Remark: observe for the third case that if we were to allow i = j then the result wouldn’t hold.
3x + 2y = 7
2ρ1+ρ1
−→
9x + 6y = 21
−2ρ1+ρ1
−→
−9x −6y = −21
One.I.1.33
Let p, n, and d be the number of pennies, nickels, and dimes. For variables that are real
numbers, this system
p + n +
d = 13
p + 5n + 10d = 83
−ρ1+ρ2
−→
p + n + d = 13
4n + 9d = 70
has inﬁnitely many solutions. However, it has a limited number of solutions in which p, n, and d are
non-negative integers. Running through d = 0, . . . , d = 8 shows that (p, n, d) = (3, 4, 6) is the only
sensible solution.
One.I.1.34
Solving the system
(1/3)(a + b + c) + d = 29
(1/3)(b + c + d) + a = 23
(1/3)(c + d + a) + b = 21
(1/3)(d + a + b) + c = 17
we obtain a = 12, b = 9, c = 3, d = 21. Thus the second item, 21, is the correct answer.
One.I.1.35
This is how the answer was given in the cited source.
A comparison of the units and
hundreds columns of this addition shows that there must be a carry from the tens column. The tens
column then tells us that A < H, so there can be no carry from the units or hundreds columns. The
ﬁve columns then give the following ﬁve equations.
A + E = W
2H = A + 10
H = W + 1
H + T = E + 10
A + 1 = T
The ﬁve linear equations in ﬁve unknowns, if solved simultaneously, produce the unique solution: A =
4, T = 5, H = 7, W = 6 and E = 2, so that the original example in addition was 47474+5272 = 52746.
One.I.1.36
This is how the answer was given in the cited source. Eight commissioners voted for B.
To see this, we will use the given information to study how many voters chose each order of A, B, C.
The six orders of preference are ABC, ACB, BAC, BCA, CAB, CBA; assume they receive a, b,
c, d, e, f votes respectively. We know that
a + b + e = 11
d + e + f = 12
a + c + d = 14

12
Linear Algebra, by Hefferon
from the number preferring A over B, the number preferring C over A, and the number preferring B
over C. Because 20 votes were cast, we also know that
c + d + f = 9
a + b + c = 8
b + e + f = 6
from the preferences for B over A, for A over C, and for C over B.
The solution is a = 6, b = 1, c = 1, d = 7, e = 4, and f = 1. The number of commissioners voting
for B as their ﬁrst choice is therefore c + d = 1 + 7 = 8.
Comments. The answer to this question would have been the same had we known only that at least
14 commissioners preferred B over C.
The seemingly paradoxical nature of the commissioners’s preferences (A is preferred to B, and B is
preferred to C, and C is preferred to A), an example of “non-transitive dominance”, is not uncommon
when individual choices are pooled.
One.I.1.37
This is how the answer was given in the cited source. We have not used “dependent” yet;
it means here that Gauss’ method shows that there is not a unique solution. If n ≥3 the system is
dependent and the solution is not unique. Hence n < 3. But the term “system” implies n > 1. Hence
n = 2. If the equations are
ax + (a + d)y = a + 2d
(a + 3d)x + (a + 4d)y = a + 5d
then x = −1, y = 2.
Subsection One.I.2: Describing the Solution Set
One.I.2.15
(a) 2
(b) 3
(c) −1
(d) Not deﬁned.
One.I.2.16
(a) 2×3
(b) 3×2
(c) 2×2
One.I.2.17
(a)


5
1
5


(b)
µ
20
−5
¶
(c)


−2
4
0


(d)
µ
41
52
¶
(e) Not deﬁned.
(f)


12
8
4


One.I.2.18
(a) This reductionµ
3
6
18
1
2
6
¶
(−1/3)ρ1+ρ2
−→
µ
3
6
18
0
0
0
¶
leaves x leading and y free. Making y the parameter, we have x = 6 −2y so the solution set is
{
µ
6
0
¶
+
µ
−2
1
¶
y
¯¯ y ∈R}.
(b) This reduction
µ
1
1
1
1
−1
−1
¶
−ρ1+ρ2
−→
µ
1
1
1
0
−2
−2
¶
gives the unique solution y = 1, x = 0. The solution set is
{
µ
0
1
¶
}.
(c) This use of Gauss’ method


1
0
1
4
1
−1
2
5
4
−1
5
17


−ρ1+ρ2
−→
−4ρ1+ρ3


1
0
1
4
0
−1
1
1
0
−1
1
1

−ρ2+ρ3
−→


1
0
1
4
0
−1
1
1
0
0
0
0


leaves x1 and x2 leading with x3 free. The solution set is
{


4
−1
0

+


−1
1
1

x3
¯¯ x3 ∈R}.

Answers to Exercises
13
(d) This reduction


2
1
−1
2
2
0
1
3
1
−1
0
0


−ρ1+ρ2
−→
−(1/2)ρ1+ρ3


2
1
−1
2
0
−1
2
1
0
−3/2
1/2
−1

(−3/2)ρ2+ρ3
−→


2
1
−1
2
0
−1
2
1
0
0
−5/2
−5/2


shows that the solution set is a singleton set.
{


1
1
1

}
(e) This reduction is easy


1
2
−1
0
3
2
1
0
1
4
1
−1
1
1
1

−2ρ1+ρ2
−→
−ρ1+ρ3


1
2
−1
0
3
0
−3
2
1
−2
0
−3
2
1
−2

−ρ2+ρ3
−→


1
2
−1
0
3
0
−3
2
1
−2
0
0
0
0
0


and ends with x and y leading, while z and w are free. Solving for y gives y = (2 + 2z + w)/3 and
substitution shows that x + 2(2 + 2z + w)/3 −z = 3 so x = (5/3) −(1/3)z −(2/3)w, making the
solution set
{




5/3
2/3
0
0



+




−1/3
2/3
1
0



z +




−2/3
1/3
0
1



w
¯¯ z, w ∈R}.
(f) The reduction


1
0
1
1
4
2
1
0
−1
2
3
1
1
0
7

−2ρ1+ρ2
−→
−3ρ1+ρ3


1
0
1
1
4
0
1
−2
−3
−6
0
1
−2
−3
−5

−ρ2+ρ3
−→


1
0
1
1
4
0
1
−2
−3
−6
0
0
0
0
1


shows that there is no solution — the solution set is empty.
One.I.2.19
(a) This reduction
µ
2
1
−1
1
4
−1
0
3
¶
−2ρ1+ρ2
−→
µ
2
1
−1
1
0
−3
2
1
¶
ends with x and y leading while z is free. Solving for y gives y = (1−2z)/(−3), and then substitution
2x + (1 −2z)/(−3) −z = 1 shows that x = ((4/3) + (1/3)z)/2. Hence the solution set is
{


2/3
−1/3
0

+


1/6
2/3
1

z
¯¯ z ∈R}.
(b) This application of Gauss’ method


1
0
−1
0
1
0
1
2
−1
3
1
2
3
−1
7

−ρ1+ρ3
−→


1
0
−1
0
1
0
1
2
−1
3
0
2
4
−1
6

−2ρ2+ρ3
−→


1
0
−1
0
1
0
1
2
−1
3
0
0
0
1
0


leaves x, y, and w leading. The solution set is
{




1
3
0
0



+




1
−2
1
0



z
¯¯ z ∈R}.
(c) This row reduction




1
−1
1
0
0
0
1
0
1
0
3
−2
3
1
0
0
−1
0
−1
0




−3ρ1+ρ3
−→




1
−1
1
0
0
0
1
0
1
0
0
1
0
1
0
0
−1
0
−1
0




−ρ2+ρ3
−→
ρ2+ρ4




1
−1
1
0
0
0
1
0
1
0
0
0
0
0
0
0
0
0
0
0




ends with z and w free. The solution set is
{




0
0
0
0



+




−1
0
1
0



z +




−1
−1
0
1



w
¯¯ z, w ∈R}.

14
Linear Algebra, by Hefferon
(d) Gauss’ method done in this way
µ
1
2
3
1
−1
1
3
−1
1
1
1
3
¶
−3ρ1+ρ2
−→
µ
1
2
3
1
−1
1
0
−7
−8
−2
4
0
¶
ends with c, d, and e free. Solving for b shows that b = (8c + 2d −4e)/(−7) and then substitution
a + 2(8c + 2d −4e)/(−7) + 3c + 1d −1e = 1 shows that a = 1 −(5/7)c −(3/7)d −(1/7)e and so the
solution set is
{






1
0
0
0
0






+






−5/7
−8/7
1
0
0






c +






−3/7
−2/7
0
1
0






d +






−1/7
4/7
0
0
1






e
¯¯ c, d, e ∈R}.
One.I.2.20
For each problem we get a system of linear equations by looking at the equations of
components.
(a) k = 5
(b) The second components show that i = 2, the third components show that j = 1.
(c) m = −4, n = 2
One.I.2.21
For each problem we get a system of linear equations by looking at the equations of
components.
(a) Yes; take k = −1/2.
(b) No; the system with equations 5 = 5 · j and 4 = −4 · j has no solution.
(c) Yes; take r = 2.
(d) No. The second components give k = 0. Then the third components give j = 1. But the ﬁrst
components don’t check.
One.I.2.22
This system has 1 equation. The leading variable is x1, the other variables are free.
{





−1
1
...
0




x2 + · · · +





−1
0
...
1




xn
¯¯ x1, . . . , xn ∈R}
One.I.2.23
(a) Gauss’ method here gives


1
2
0
−1
a
2
0
1
0
b
1
1
0
2
c


−2ρ1+ρ2
−→
−ρ1+ρ3


1
2
0
−1
a
0
−4
1
2
−2a + b
0
−1
0
3
−a + c


−(1/4)ρ2+ρ3
−→


1
2
0
−1
a
0
−4
1
2
−2a + b
0
0
−1/4
5/2
−(1/2)a −(1/4)b + c

,
leaving w free. Solve: z = 2a + b −4c + 10w, and −4y = −2a + b −(2a + b −4c + 10w) −2w so
y = a −c + 3w, and x = a −2(a −c + 3w) + w = −a + 2c −5w. Therefore the solution set is this.
{




−a + 2c
a −c
2a + b −4c
0



+




−5
3
10
1



w
¯¯ w ∈R}
(b) Plug in with a = 3, b = 1, and c = −2.
{




−7
5
15
0



+




−5
3
10
1



w
¯¯ w ∈R}
One.I.2.24
Leaving the comma out, say by writing a123, is ambiguous because it could mean a1,23 or
a12,3.
One.I.2.25
(a)




2
3
4
5
3
4
5
6
4
5
6
7
5
6
7
8




(b)




1
−1
1
−1
−1
1
−1
1
1
−1
1
−1
−1
1
−1
1





Answers to Exercises
15
One.I.2.26
(a)


1
4
2
5
3
6


(b)
µ
2
1
−3
1
¶
(c)
µ
5
10
10
5
¶
(d)
¡1
1
0¢
One.I.2.27
(a) Plugging in x = 1 and x = −1 gives
a + b + c = 2
a −b + c = 6
−ρ1+ρ2
−→
a +
b + c = 2
−2b
= 4
so the set of functions is {f(x) = (4 −c)x2 −2x + c
¯¯ c ∈R}.
(b) Putting in x = 1 gives
a + b + c = 2
so the set of functions is {f(x) = (2 −b −c)x2 + bx + c
¯¯ b, c ∈R}.
One.I.2.28
On plugging in the ﬁve pairs (x, y) we get a system with the ﬁve equations and six unknowns
a, . . . , f. Because there are more unknowns than equations, if no inconsistency exists among the
equations then there are inﬁnitely many solutions (at least one variable will end up free).
But no inconsistency can exist because a = 0, . . . , f = 0 is a solution (we are only using this zero
solution to show that the system is consistent — the prior paragraph shows that there are nonzero
solutions).
One.I.2.29
(a) Here is one — the fourth equation is redundant but still OK.
x + y −z + w = 0
y −z
= 0
2z + 2w = 0
z + w = 0
(b) Here is one.
x + y −z + w = 0
w = 0
w = 0
w = 0
(c) This is one.
x + y −z + w = 0
x + y −z + w = 0
x + y −z + w = 0
x + y −z + w = 0
One.I.2.30
This is how the answer was given in the cited source.
(a) Formal solution of the system yields
x = a3 −1
a2 −1
y = −a2 + a
a2 −1 .
If a + 1 ̸= 0 and a −1 ̸= 0, then the system has the single solution
x = a2 + a + 1
a + 1
y =
−a
a + 1.
If a = −1, or if a = +1, then the formulas are meaningless; in the ﬁrst instance we arrive at the
system
½−x + y = 1
x −y = 1
which is a contradictory system. In the second instance we have
½
x + y = 1
x + y = 1
which has an inﬁnite number of solutions (for example, for x arbitrary, y = 1 −x).
(b) Solution of the system yields
x = a4 −1
a2 −1
y = −a3 + a
a2 −1 .
Here, is a2 −1 ̸= 0, the system has the single solution x = a2 + 1, y = −a. For a = −1 and a = 1,
we obtain the systems
½−x + y = −1
x −y =
1
½x + y = 1
x + y = 1
both of which have an inﬁnite number of solutions.

16
Linear Algebra, by Hefferon
One.I.2.31
This is how the answer was given in the cited source. Let u, v, x, y, z be the volumes
in cm3 of Al, Cu, Pb, Ag, and Au, respectively, contained in the sphere, which we assume to be
not hollow. Since the loss of weight in water (speciﬁc gravity 1.00) is 1000 grams, the volume of the
sphere is 1000 cm3. Then the data, some of which is superﬂuous, though consistent, leads to only 2
independent equations, one relating volumes and the other, weights.
u +
v +
x +
y +
z = 1000
2.7u + 8.9v + 11.3x + 10.5y + 19.3z = 7558
Clearly the sphere must contain some aluminum to bring its mean speciﬁc gravity below the speciﬁc
gravities of all the other metals. There is no unique result to this part of the problem, for the amounts
of three metals may be chosen arbitrarily, provided that the choices will not result in negative amounts
of any metal.
If the ball contains only aluminum and gold, there are 294.5 cm3 of gold and 705.5 cm3 of aluminum.
Another possibility is 124.7 cm3 each of Cu, Au, Pb, and Ag and 501.2 cm3 of Al.
Subsection One.I.3: General = Particular + Homogeneous
One.I.3.15
For the arithmetic to these, see the answers from the prior subsection.
(a) The solution set is
{
µ
6
0
¶
+
µ
−2
1
¶
y
¯¯ y ∈R}.
Here the particular solution and the solution set for the associated homogeneous system are
µ
6
0
¶
and
{
µ
−2
1
¶
y
¯¯ y ∈R}.
(b) The solution set is
{
µ
0
1
¶
}.
The particular solution and the solution set for the associated homogeneous system are
µ
0
1
¶
and
{
µ
0
0
¶
}
(c) The solution set is
{


4
−1
0

+


−1
1
1

x3
¯¯ x3 ∈R}.
A particular solution and the solution set for the associated homogeneous system are


4
−1
0


and
{


−1
1
1

x3
¯¯ x3 ∈R}.
(d) The solution set is a singleton
{


1
1
1

}.
A particular solution and the solution set for the associated homogeneous system are


1
1
1


and
{


0
0
0

t
¯¯ t ∈R}.
(e) The solution set is
{




5/3
2/3
0
0



+




−1/3
2/3
1
0



z +




−2/3
1/3
0
1



w
¯¯ z, w ∈R}.
A particular solution and the solution set for the associated homogeneous system are




5/2
2/3
0
0




and
{




−1/3
2/3
1
0



z +




−2/3
1/3
0
1



w
¯¯ z, w ∈R}.

Answers to Exercises
17
(f) This system’s solution set is empty. Thus, there is no particular solution. The solution set of the
associated homogeneous system is
{




−1
2
1
0



z +




−1
3
0
1



w
¯¯ z, w ∈R}.
One.I.3.16
The answers from the prior subsection show the row operations.
(a) The solution set is
{


2/3
−1/3
0

+


1/6
2/3
1

z
¯¯ z ∈R}.
A particular solution and the solution set for the associated homogeneous system are


2/3
−1/3
0


and
{


1/6
2/3
1

z
¯¯ z ∈R}.
(b) The solution set is
{




1
3
0
0



+




1
−2
1
0



z
¯¯ z ∈R}.
A particular solution and the solution set for the associated homogeneous system are




1
3
0
0




and
{




1
−2
1
0



z
¯¯ z ∈R}.
(c) The solution set is
{




0
0
0
0



+




−1
0
1
0



z +




−1
−1
0
1



w
¯¯ z, w ∈R}.
A particular solution and the solution set for the associated homogeneous system are




0
0
0
0




and
{




−1
0
1
0



z +




−1
−1
0
1



w
¯¯ z, w ∈R}.
(d) The solution set is
{






1
0
0
0
0






+






−5/7
−8/7
1
0
0






c +






−3/7
−2/7
0
1
0






d +






−1/7
4/7
0
0
1






e
¯¯ c, d, e ∈R}.
A particular solution and the solution set for the associated homogeneous system are






1
0
0
0
0






and
{






−5/7
−8/7
1
0
0






c +






−3/7
−2/7
0
1
0






d +






−1/7
4/7
0
0
1






e
¯¯ c, d, e ∈R}.
One.I.3.17
Just plug them in and see if they satisfy all three equations.
(a) No.
(b) Yes.
(c) Yes.
One.I.3.18
Gauss’ method on the associated homogeneous system gives


1
−1
0
1
0
2
3
−1
0
0
0
1
1
1
0

−2ρ1+ρ2
−→


1
−1
0
1
0
0
5
−1
−2
0
0
1
1
1
0

−(1/5)ρ2+ρ3
−→


1
−1
0
1
0
0
5
−1
−2
0
0
0
6/5
7/5
0



18
Linear Algebra, by Hefferon
so this is the solution to the homogeneous problem:
{




−5/6
1/6
−7/6
1



w
¯¯ w ∈R}.
(a) That vector is indeed a particular solution, so the required general solution is
{




0
0
0
4



+




−5/6
1/6
−7/6
1



w
¯¯ w ∈R}.
(b) That vector is a particular solution so the required general solution is
{




−5
1
−7
10



+




−5/6
1/6
−7/6
1



w
¯¯ w ∈R}.
(c) That vector is not a solution of the system since it does not satisfy the third equation. No such
general solution exists.
One.I.3.19
The ﬁrst is nonsingular while the second is singular. Just do Gauss’ method and see if the
echelon form result has non-0 numbers in each entry on the diagonal.
One.I.3.20
(a) Nonsingular:
−ρ1+ρ2
−→
µ
1
2
0
1
¶
ends with each row containing a leading entry.
(b) Singular:
3ρ1+ρ2
−→
µ
1
2
0
0
¶
ends with row 2 without a leading entry.
(c) Neither. A matrix must be square for either word to apply.
(d) Singular.
(e) Nonsingular.
One.I.3.21
In each case we must decide if the vector is a linear combination of the vectors in the
set.
(a) Yes. Solve
c1
µ
1
4
¶
+ c2
µ
1
5
¶
=
µ
2
3
¶
with
µ
1
1
2
4
5
3
¶
−4ρ1+ρ2
−→
µ
1
1
2
0
1
−5
¶
to conclude that there are c1 and c2 giving the combination.
(b) No. The reduction


2
1
−1
1
0
0
0
1
1

−(1/2)ρ1+ρ2
−→


2
1
−1
0
−1/2
1/2
0
1
1

2ρ2+ρ3
−→


2
1
−1
0
−1/2
1/2
0
0
2


shows that
c1


2
1
0

+ c2


1
0
1

=


−1
0
1


has no solution.
(c) Yes. The reduction


1
2
3
4
1
0
1
3
2
3
4
5
0
1
0

−4ρ1+ρ3
−→


1
2
3
4
1
0
1
3
2
3
0
−3
−12
−15
−4

3ρ2+ρ3
−→


1
2
3
4
1
0
1
3
2
3
0
0
−3
−9
5



Answers to Exercises
19
shows that there are inﬁnitely many ways
{




c1
c2
c3
c4



=




−10
8
−5/3
0



+




−9
7
−3
1



c4
¯¯ c4 ∈R}
to write


1
3
0

= c1


1
0
4

+ c2


2
1
5

+ c3


3
3
0

+ c4


4
2
1

.
(d) No. Look at the third components.
One.I.3.22
Because the matrix of coeﬃcients is nonsingular, Gauss’ method ends with an echelon form
where each variable leads an equation. Back substitution gives a unique solution.
(Another way to see the solution is unique is to note that with a nonsingular matrix of coeﬃcients
the associated homogeneous system has a unique solution, by deﬁnition. Since the general solution is
the sum of a particular solution with each homogeneous solution, the general solution has (at most)
one element.)
One.I.3.23
In this case the solution set is all of Rn, and can be expressed in the required form
{c1





1
0
...
0




+ c2





0
1
...
0




+ · · · + cn





0
0
...
1





¯¯ c1, . . . , cn ∈R}.
One.I.3.24
Assume ⃗s,⃗t ∈Rn and write
⃗s =



s1
...
sn



and
⃗t =



t1
...
tn


.
Also let ai,1x1 + · · · + ai,nxn = 0 be the i-th equation in the homogeneous system.
(a) The check is easy:
ai,1(s1 + t1) + · · · + ai,n(sn + tn)
=
(ai,1s1 + · · · + ai,nsn) + (ai,1t1 + · · · + ai,ntn)
=
0 + 0.
(b) This one is similar:
ai,1(3s1) + · · · + ai,n(3sn) = 3(ai,1s1 + · · · + ai,nsn) = 3 · 0 = 0.
(c) This one is not much harder:
ai,1(ks1 + mt1) + · · · + ai,n(ksn + mtn)
=
k(ai,1s1 + · · · + ai,nsn) + m(ai,1t1 + · · · + ai,ntn)
=
k · 0 + m · 0.
What is wrong with that argument is that any linear combination of the zero vector yields the zero
vector again.
One.I.3.25
First the proof.
Gauss’ method will use only rationals (e.g., −(m/n)ρi +ρj). Thus the solution set can be expressed
using only rational numbers as the components of each vector.
Now the particular solution is all
rational.
There are inﬁnitely many (rational vector) solutions if and only if the associated homogeneous sys-
tem has inﬁnitely many (real vector) solutions. That’s because setting any parameters to be rationals
will produce an all-rational solution.
Subsection One.II.1: Vectors in Space

20
Linear Algebra, by Hefferon
One.II.1.1
(a)
µ
2
1
¶
(b)
µ
−1
2
¶
(c)


4
0
−3


(d)


0
0
0


One.II.1.2
(a) No, their canonical positions are diﬀerent.
µ
1
−1
¶
µ
0
3
¶
(b) Yes, their canonical positions are the same.

1
−1
3


One.II.1.3
That line is this set.
{




−2
1
1
0



+




7
9
−2
4



t
¯¯ t ∈R}
Note that this system
−2 + 7t = 1
1 + 9t = 0
1 −2t = 2
0 + 4t = 1
has no solution. Thus the given point is not in the line.
One.II.1.4
(a) Note that




2
2
2
0



−




1
1
5
−1



=




1
1
−3
1








3
1
0
4



−




1
1
5
−1



=




2
0
−5
5




and so the plane is this set.
{




1
1
5
−1



+




1
1
−3
1



t +




2
0
−5
5



s
¯¯ t, s ∈R}
(b) No; this system
1 + 1t + 2s = 0
1 + 1t
= 0
5 −3t −5s = 0
−1 + 1t + 5s = 0
has no solution.
One.II.1.5
The vector


2
0
3


is not in the line. Because


2
0
3

−


−1
0
−4

=


3
0
7


that plane can be described in this way.
{


−1
0
−4

+ m


1
1
2

+ n


3
0
7

¯¯ m, n ∈R}
One.II.1.6
The points of coincidence are solutions of this system.
t
= 1 + 2m
t + s = 1 + 3k
t + 3s =
4m

Answers to Exercises
21
Gauss’ method


1
0
0
−2
1
1
1
−3
0
1
1
3
0
−4
0

−ρ1+ρ2
−→
−ρ1+ρ3


1
0
0
−2
1
0
1
−3
2
0
0
3
0
−2
−1

−3ρ2+ρ3
−→


1
0
0
−2
1
0
1
−3
2
0
0
0
9
−8
−1


gives k = −(1/9) + (8/9)m, so s = −(1/3) + (2/3)m and t = 1 + 2m. The intersection is this.
{


1
1
0

+


0
3
0

(−1
9 + 8
9m) +


2
0
4

m
¯¯ m ∈R} = {


1
2/3
0

+


2
8/3
4

m
¯¯ m ∈R}
One.II.1.7
(a) The system
1 =
1
1 + t =
3 + s
2 + t = −2 + 2s
gives s = 6 and t = 8, so this is the solution set.
{


1
9
10

}
(b) This system
2 + t =
0
t = s + 4w
1 −t = 2s + w
gives t = −2, w = −1, and s = 2 so their intersection is this point.


0
−2
3


One.II.1.8
(a) The vector shown
is not the result of doubling


2
0
0

+


−0.5
1
0

· 1
instead it is


2
0
0

+


−0.5
1
0

· 2 =


1
2
0


which has a parameter twice as large.
(b) The vector
is not the result of adding
(


2
0
0

+


−0.5
1
0

· 1) + (


2
0
0

+


−0.5
0
1

· 1)

22
Linear Algebra, by Hefferon
instead it is


2
0
0

+


−0.5
1
0

· 1 +


−0.5
0
1

· 1 =


1
1
1


which adds the parameters.
One.II.1.9
The “if” half is straightforward. If b1 −a1 = d1 −c1 and b2 −a2 = d2 −c2 then
p
(b1 −a1)2 + (b2 −a2)2 =
p
(d1 −c1)2 + (d2 −c2)2
so they have the same lengths, and the slopes are just as easy:
b2 −a2
b1 −a1
= d2 −c2
d1 −a1
(if the denominators are 0 they both have undeﬁned slopes).
For “only if”, assume that the two segments have the same length and slope (the case of un-
deﬁned slopes is easy; we will do the case where both segments have a slope m).
Also assume,
without loss of generality, that a1 < b1 and that c1 < d1.
The ﬁrst segment is (a1, a2)(b1, b2) =
{(x, y)
¯¯ y = mx + n1, x ∈[a1..b1]} (for some intercept n1) and the second segment is (c1, c2)(d1, d2) =
{(x, y)
¯¯ y = mx + n2, x ∈[c1..d1]} (for some n2). Then the lengths of those segments are
p
(b1 −a1)2 + ((mb1 + n1) −(ma1 + n1))2 =
p
(1 + m2)(b1 −a1)2
and, similarly,
p
(1 + m2)(d1 −c1)2. Therefore, |b1−a1| = |d1−c1|. Thus, as we assumed that a1 < b1
and c1 < d1, we have that b1 −a1 = d1 −c1.
The other equality is similar.
One.II.1.10
We shall later deﬁne it to be a set with one element — an “origin”.
One.II.1.11
This is how the answer was given in the cited source. The vector triangle is as follows, so
⃗w = 3
√
2 from the north west.
-
@
@
@
@
R
⃗w
¡
¡
¡
¡

One.II.1.12
Euclid no doubt is picturing a plane inside of R3. Observe, however, that both R1 and
R3 also satisfy that deﬁnition.
Subsection One.II.2: Length and Angle Measures
One.II.2.10
(a)
√
32 + 12 =
√
10
(b)
√
5
(c)
√
18
(d) 0
(e)
√
3
One.II.2.11
(a) arccos(9/
√
85) ≈0.22 radians
(b) arccos(8/
√
85) ≈0.52 radians
(c) Not deﬁned.
One.II.2.12
We express each displacement as a vector (rounded to one decimal place because that’s
the accuracy of the problem’s statement) and add to ﬁnd the total displacement (ignoring the curvature
of the earth).
µ0.0
1.2
¶
+
µ 3.8
−4.8
¶
+
µ4.0
0.1
¶
+
µ3.3
5.6
¶
=
µ11.1
2.1
¶
The distance is
√
11.12 + 2.12 ≈11.3.
One.II.2.13
Solve (k)(4) + (1)(3) = 0 to get k = −3/4.
One.II.2.14
The set
{


x
y
z

¯¯ 1x + 3y −1z = 0}
can also be described with parameters in this way.
{


−3
1
0

y +


1
0
1

z
¯¯ y, z ∈R}

Answers to Exercises
23
One.II.2.15
(a) We can use the x-axis.
arccos((1)(1) + (0)(1)
√
1
√
2
) ≈0.79 radians
(b) Again, use the x-axis.
arccos((1)(1) + (0)(1) + (0)(1)
√
1
√
3
) ≈0.96 radians
(c) The x-axis worked before and it will work again.
arccos((1)(1) + · · · + (0)(1)
√
1√n
) = arccos( 1
√n)
(d) Using the formula from the prior item, limn→∞arccos(1/√n) = π/2 radians.
One.II.2.16
Clearly u1u1 + · · · + unun is zero if and only if each ui is zero.
So only ⃗0 ∈Rn is
perpendicular to itself.
One.II.2.17
Assume that ⃗u,⃗v, ⃗w ∈Rn have components u1, . . . , un, v1, . . . , wn.
(a) Dot product is right-distributive.
(⃗u + ⃗v)
⃗w = [



u1
...
un


+



v1
...
vn


]



w1
...
wn



=



u1 + v1
...
un + vn






w1
...
wn



= (u1 + v1)w1 + · · · + (un + vn)wn
= (u1w1 + · · · + unwn) + (v1w1 + · · · + vnwn)
= ⃗u
⃗w + ⃗v
⃗w
(b) Dot product is also left distributive: ⃗w (⃗u + ⃗v) = ⃗w ⃗u + ⃗w ⃗v. The proof is just like the prior
one.
(c) Dot product commutes.



u1
...
un






v1
...
vn


= u1v1 + · · · + unvn = v1u1 + · · · + vnun =



v1
...
vn






u1
...
un



(d) Because ⃗u ⃗v is a scalar, not a vector, the expression (⃗u ⃗v)
⃗w makes no sense; the dot product
of a scalar and a vector is not deﬁned.
(e) This is a vague question so it has many answers. Some are (1) k(⃗u ⃗v) = (k⃗u) ⃗v and k(⃗u ⃗v) =
⃗u (k⃗v), (2) k(⃗u ⃗v) ̸= (k⃗u) (k⃗v) (in general; an example is easy to produce), and (3) ∥k⃗v ∥= k2∥⃗v ∥
(the connection between norm and dot product is that the square of the norm is the dot product of
a vector with itself).
One.II.2.18
(a) Verifying that (k⃗x) ⃗y = k(⃗x ⃗y) = ⃗x (k⃗y) for k ∈R and ⃗x, ⃗y ∈Rn is easy. Now, for
k ∈R and ⃗v, ⃗w ∈Rn, if ⃗u = k⃗v then ⃗u ⃗v = (k⃗u) ⃗v = k(⃗v ⃗v), which is k times a nonnegative real.
The ⃗v = k⃗u half is similar (actually, taking the k in this paragraph to be the reciprocal of the k
above gives that we need only worry about the k = 0 case).
(b) We ﬁrst consider the ⃗u ⃗v ≥0 case. From the Triangle Inequality we know that ⃗u ⃗v = ∥⃗u ∥∥⃗v ∥if
and only if one vector is a nonnegative scalar multiple of the other. But that’s all we need because
the ﬁrst part of this exercise shows that, in a context where the dot product of the two vectors
is positive, the two statements ‘one vector is a scalar multiple of the other’ and ‘one vector is a
nonnegative scalar multiple of the other’, are equivalent.
We ﬁnish by considering the ⃗u
⃗v < 0 case. Because 0 < |⃗u
⃗v| = −(⃗u
⃗v) = (−⃗u)
⃗v and
∥⃗u ∥∥⃗v ∥= ∥−⃗u ∥∥⃗v ∥, we have that 0 < (−⃗u) ⃗v = ∥−⃗u ∥∥⃗v ∥. Now the prior paragraph applies to
give that one of the two vectors −⃗u and ⃗v is a scalar multiple of the other. But that’s equivalent to
the assertion that one of the two vectors ⃗u and ⃗v is a scalar multiple of the other, as desired.
One.II.2.19
No. These give an example.
⃗u =
µ
1
0
¶
⃗v =
µ
1
0
¶
⃗w =
µ
1
1
¶

24
Linear Algebra, by Hefferon
One.II.2.20
We prove that a vector has length zero if and only if all its components are zero.
Let ⃗u ∈Rn have components u1, . . . , un. Recall that the square of any real number is greater than
or equal to zero, with equality only when that real is zero. Thus ∥⃗u ∥2 = u12 + · · · + un2 is a sum of
numbers greater than or equal to zero, and so is itself greater than or equal to zero, with equality if
and only if each ui is zero. Hence ∥⃗u ∥= 0 if and only if all the components of ⃗u are zero.
One.II.2.21
We can easily check that
¡x1 + x2
2
, y1 + y2
2
¢
is on the line connecting the two, and is equidistant from both. The generalization is obvious.
One.II.2.22
Assume that ⃗v ∈Rn has components v1, . . . , vn. If ⃗v ̸= ⃗0 then we have this.
sµ
v1
√
v12 + · · · + vn2
¶2
+ · · · +
µ
vn
√
v12 + · · · + vn2
¶2
=
sµ
v12
v12 + · · · + vn2
¶
+ · · · +
µ
vn2
v12 + · · · + vn2
¶
= 1
If ⃗v = ⃗0 then ⃗v/∥⃗v ∥is not deﬁned.
One.II.2.23
For the ﬁrst question, assume that ⃗v ∈Rn and r ≥0, take the root, and factor.
∥r⃗v ∥=
p
(rv1)2 + · · · + (rvn)2 =
p
r2(v12 + · · · + vn2 = r∥⃗v ∥
For the second question, the result is r times as long, but it points in the opposite direction in that
r⃗v + (−r)⃗v = ⃗0.
One.II.2.24
Assume that ⃗u,⃗v ∈Rn both have length 1. Apply Cauchy-Schwartz: |⃗u ⃗v| ≤∥⃗u ∥∥⃗v ∥= 1.
To see that ‘less than’ can happen, in R2 take
⃗u =
µ
1
0
¶
⃗v =
µ
0
1
¶
and note that ⃗u ⃗v = 0. For ‘equal to’, note that ⃗u ⃗u = 1.
One.II.2.25
Write
⃗u =



u1
...
un



⃗v =



v1
...
vn



and then this computation works.
∥⃗u + ⃗v ∥2 + ∥⃗u −⃗v ∥2 = (u1 + v1)2 + · · · + (un + vn)2
+ (u1 −v1)2 + · · · + (un −vn)2
= u1
2 + 2u1v1 + v1
2 + · · · + un
2 + 2unvn + vn
2
+ u1
2 −2u1v1 + v1
2 + · · · + un
2 −2unvn + vn
2
= 2(u1
2 + · · · + un
2) + 2(v1
2 + · · · + vn
2)
= 2∥⃗u ∥2 + 2∥⃗v ∥2
One.II.2.26
We will prove this demonstrating that the contrapositive statement holds: if ⃗x ̸= ⃗0 then
there is a ⃗y with ⃗x ⃗y ̸= 0.
Assume that ⃗x ∈Rn. If ⃗x ̸= ⃗0 then it has a nonzero component, say the i-th one xi. But the
vector ⃗y ∈Rn that is all zeroes except for a one in component i gives ⃗x ⃗y = xi. (A slicker proof just
considers ⃗x ⃗x.)
One.II.2.27
Yes; we can prove this by induction.
Assume that the vectors are in some Rk. Clearly the statement applies to one vector. The Triangle
Inequality is this statement applied to two vectors. For an inductive step assume the statement is true
for n or fewer vectors. Then this
∥⃗u1 + · · · + ⃗un + ⃗un+1∥≤∥⃗u1 + · · · + ⃗un∥+ ∥⃗un+1∥
follows by the Triangle Inequality for two vectors. Now the inductive hypothesis, applied to the ﬁrst
summand on the right, gives that as less than or equal to ∥⃗u1∥+ · · · + ∥⃗un∥+ ∥⃗un+1∥.

Answers to Exercises
25
One.II.2.28
By deﬁnition
⃗u ⃗v
∥⃗u ∥∥⃗v ∥= cos θ
where θ is the angle between the vectors. Thus the ratio is | cos θ|.
One.II.2.29
So that the statement ‘vectors are orthogonal iﬀtheir dot product is zero’ has no excep-
tions.
One.II.2.30
The angle between (a) and (b) is found (for a, b ̸= 0) with
arccos(
ab
√
a2√
b2 ).
If a or b is zero then the angle is π/2 radians. Otherwise, if a and b are of opposite signs then the
angle is π radians, else the angle is zero radians.
One.II.2.31
The angle between ⃗u and ⃗v is acute if ⃗u ⃗v > 0, is right if ⃗u ⃗v = 0, and is obtuse if
⃗u ⃗v < 0. That’s because, in the formula for the angle, the denominator is never negative.
One.II.2.32
Suppose that ⃗u,⃗v ∈Rn. If ⃗u and ⃗v are perpendicular then
∥⃗u + ⃗v ∥2 = (⃗u + ⃗v) (⃗u + ⃗v) = ⃗u ⃗u + 2 ⃗u ⃗v + ⃗v ⃗v = ⃗u ⃗u + ⃗v ⃗v = ∥⃗u ∥2 + ∥⃗v ∥2
(the third equality holds because ⃗u ⃗v = 0).
One.II.2.33
Where ⃗u,⃗v ∈Rn, the vectors ⃗u + ⃗v and ⃗u −⃗v are perpendicular if and only if 0 =
(⃗u +⃗v) (⃗u −⃗v) = ⃗u ⃗u −⃗v ⃗v, which shows that those two are perpendicular if and only if ⃗u ⃗u = ⃗v ⃗v.
That holds if and only if ∥⃗u ∥= ∥⃗v ∥.
One.II.2.34
Suppose ⃗u ∈Rn is perpendicular to both ⃗v ∈Rn and ⃗w ∈Rn. Then, for any k, m ∈R
we have this.
⃗u (k⃗v + m⃗w) = k(⃗u ⃗v) + m(⃗u
⃗w) = k(0) + m(0) = 0
One.II.2.35
We will show something more general: if ∥⃗z1∥= ∥⃗z2∥for ⃗z1,⃗z2 ∈Rn, then ⃗z1 + ⃗z2 bisects
the angle between ⃗z1 and ⃗z2
©©©
©
*
¢
¢
¢¢
¡
¡
¡
¡
¡

¢
¢
¢¢
©©©
©
gives
©©©
©
¢
¢
¢¢
¡
¡
¡
¡
¡
¢
¢
¢¢
©©©
©
′
′
′′
′′′
′′′
(we ignore the case where ⃗z1 and ⃗z2 are the zero vector).
The ⃗z1 + ⃗z2 = ⃗0 case is easy. For the rest, by the deﬁnition of angle, we will be done if we show
this.
⃗z1 (⃗z1 + ⃗z2)
∥⃗z1∥∥⃗z1 + ⃗z2∥= ⃗z2 (⃗z1 + ⃗z2)
∥⃗z2∥∥⃗z1 + ⃗z2∥
But distributing inside each expression gives
⃗z1 ⃗z1 + ⃗z1 ⃗z2
∥⃗z1∥∥⃗z1 + ⃗z2∥
⃗z2 ⃗z1 + ⃗z2 ⃗z2
∥⃗z2∥∥⃗z1 + ⃗z2∥
and ⃗z1 ⃗z1 = ∥⃗z1∥= ∥⃗z2∥= ⃗z2 ⃗z2, so the two are equal.
One.II.2.36
We can show the two statements together. Let ⃗u,⃗v ∈Rn, write
⃗u =



u1
...
un



⃗v =



v1
...
vn



and calculate.
cos θ =
ku1v1 + · · · + kunvn
q
(ku1)2 + · · · + (kun)2p
b1
2 + · · · + bn
2 = k
|k|
⃗u · ⃗v
∥⃗u ∥∥⃗v ∥= ±
⃗u ⃗v
∥⃗u ∥∥⃗v ∥
One.II.2.37
Let
⃗u =



u1
...
un


,
⃗v =



v1
...
vn



⃗w =



w1
...
wn




26
Linear Algebra, by Hefferon
and then
⃗u
¡
k⃗v + m⃗w
¢
=



u1
...
un



¡



kv1
...
kvn


+



mw1
...
mwn



¢
=



u1
...
un






kv1 + mw1
...
kvn + mwn



= u1(kv1 + mw1) + · · · + un(kvn + mwn)
= ku1v1 + mu1w1 + · · · + kunvn + munwn
= (ku1v1 + · · · + kunvn) + (mu1w1 + · · · + munwn)
= k(⃗u ⃗v) + m(⃗u
⃗w)
as required.
One.II.2.38
For x, y ∈R+, set
⃗u =
µ√x
√y
¶
⃗v =
µ√y
√x
¶
so that the Cauchy-Schwartz inequality asserts that (after squaring)
(√x√y + √y√x)2 ≤(√x√x + √y√y)(√y√y + √x√x)
(2√x√y)2 ≤(x + y)2
√xy ≤x + y
2
as desired.
One.II.2.39
This is how the answer was given in the cited source. The actual velocity ⃗v of the wind
is the sum of the ship’s velocity and the apparent velocity of the wind. Without loss of generality we
may assume ⃗a and ⃗b to be unit vectors, and may write
⃗v = ⃗v1 + s⃗a = ⃗v2 + t⃗b
where s and t are undetermined scalars. Take the dot product ﬁrst by ⃗a and then by ⃗b to obtain
s −t⃗a ⃗b = ⃗a (⃗v2 −⃗v1)
s⃗a ⃗b −t = ⃗b (⃗v2 −⃗v1)
Multiply the second by ⃗a ⃗b, subtract the result from the ﬁrst, and ﬁnd
s = [⃗a −(⃗a ⃗b)⃗b] (⃗v2 −⃗v1)
1 −(⃗a ⃗b)2
.
Substituting in the original displayed equation, we get
⃗v = ⃗v1 + [⃗a −(⃗a ⃗b)⃗b] (⃗v2 −⃗v1)⃗a
1 −(⃗a ⃗b)2
.
One.II.2.40
We use induction on n.
In the n = 1 base case the identity reduces to
(a1b1)2 = (a1
2)(b1
2) −0
and clearly holds.
For the inductive step assume that the formula holds for the 0, . . . , n cases. We will show that it

Answers to Exercises
27
then holds in the n + 1 case. Start with the right-hand side
¡
X
1≤j≤n+1
aj
2¢¡
X
1≤j≤n+1
bj
2¢
−
X
1≤k<j≤n+1
¡
akbj −ajbk
¢2
=
£
(
X
1≤j≤n
aj
2) + an+1
2¤£
(
X
1≤j≤n
bj
2) + bn+1
2¤
−
£
X
1≤k<j≤n
¡
akbj −ajbk
¢2 +
X
1≤k≤n
¡
akbn+1 −an+1bk
¢2¤
=
¡ X
1≤j≤n
aj
2¢¡ X
1≤j≤n
bj
2¢
+
X
1≤j≤n
bj
2an+1
2 +
X
1≤j≤n
aj
2bn+1
2 + an+1
2bn+1
2
−
£
X
1≤k<j≤n
¡
akbj −ajbk
¢2 +
X
1≤k≤n
¡
akbn+1 −an+1bk
¢2¤
=
¡ X
1≤j≤n
aj
2¢¡ X
1≤j≤n
bj
2¢
−
X
1≤k<j≤n
¡
akbj −ajbk
¢2
+
X
1≤j≤n
bj
2an+1
2 +
X
1≤j≤n
aj
2bn+1
2 + an+1
2bn+1
2
−
X
1≤k≤n
¡
akbn+1 −an+1bk
¢2
and apply the inductive hypothesis
=
¡ X
1≤j≤n
ajbj
¢2 +
X
1≤j≤n
bj
2an+1
2 +
X
1≤j≤n
aj
2bn+1
2 + an+1
2bn+1
2
−
£ X
1≤k≤n
ak
2bn+1
2 −2
X
1≤k≤n
akbn+1an+1bk +
X
1≤k≤n
an+1
2bk
2¤
=
¡ X
1≤j≤n
ajbj
¢2 −2
¡ X
1≤k≤n
akbn+1an+1bk
¢
+ an+1
2bn+1
2
=
£¡ X
1≤j≤n
ajbj
¢
+ an+1bn+1
¤2
to derive the left-hand side.
Subsection One.III.1: Gauss-Jordan Reduction
One.III.1.7
These answers show only the Gauss-Jordan reduction. With it, describing the solution
set is easy.
(a)
µ
1
1
2
1
−1
0
¶
−ρ1+ρ2
−→
µ
1
1
2
0
−2
−2
¶
−(1/2)ρ2
−→
µ
1
1
2
0
1
1
¶
−ρ2+ρ1
−→
µ
1
0
1
0
1
1
¶
(b)
µ
1
0
−1
4
2
2
0
1
¶
−2ρ1+ρ2
−→
µ
1
0
−1
4
0
2
2
−7
¶
(1/2)ρ2
−→
µ
1
0
−1
4
0
1
1
−7/2
¶
(c)µ3
−2
1
6
1
1/2
¶
−2ρ1+ρ2
−→
µ3
−2
1
0
5
−3/2
¶
(1/3)ρ1
−→
(1/5)ρ2
µ1
−2/3
1/3
0
1
−3/10
¶
(2/3)ρ2+ρ1
−→
µ1
0
2/15
0
1
−3/10
¶
(d) A row swap here makes the arithmetic easier.


2
−1
0
−1
1
3
−1
5
0
1
2
5

−(1/2)ρ1+ρ2
−→


2
−1
0
−1
0
7/2
−1
11/2
0
1
2
5

ρ2↔ρ3
−→


2
−1
0
−1
0
1
2
5
0
7/2
−1
11/2


−(7/2)ρ2+ρ3
−→


2
−1
0
−1
0
1
2
5
0
0
−8
−12


(1/2)ρ1
−→
−(1/8)ρ2


1
−1/2
0
−1/2
0
1
2
5
0
0
1
3/2


−2ρ3+ρ2
−→


1
−1/2
0
−1/2
0
1
0
2
0
0
1
3/2

(1/2)ρ2+ρ1
−→


1
0
0
1/2
0
1
0
2
0
0
1
3/2



28
Linear Algebra, by Hefferon
One.III.1.8
Use Gauss-Jordan reduction.
(a)
−(1/2)ρ1+ρ2
−→
µ
2
1
0
5/2
¶
(1/2)ρ1
−→
(2/5)ρ2
µ
1
1/2
0
1
¶
−(1/2)ρ2+ρ1
−→
µ
1
0
0
1
¶
(b)
−2ρ1+ρ2
−→
ρ1+ρ3


1
3
1
0
−6
2
0
0
−2

−(1/6)ρ2
−→
−(1/2)ρ3


1
3
1
0
1
−1/3
0
0
1

(1/3)ρ3+ρ2
−→
−ρ3+ρ1


1
3
0
0
1
0
0
0
1

−3ρ2+ρ1
−→


1
0
0
0
1
0
0
0
1


(c)
−ρ1+ρ2
−→
−3ρ1+ρ3


1
0
3
1
2
0
4
−1
0
3
0
4
−1
−2
−4

−ρ2+ρ3
−→


1
0
3
1
2
0
4
−1
0
3
0
0
0
−2
−7


(1/4)ρ2
−→
−(1/2)ρ3


1
0
3
1
2
0
1
−1/4
0
3/4
0
0
0
1
7/2

−ρ3+ρ1
−→


1
0
3
0
−3/2
0
1
−1/4
0
3/4
0
0
0
1
7/2


(d)
ρ1↔ρ3
−→


1
5
1
5
0
0
5
6
0
1
3
2

ρ2↔ρ3
−→


1
5
1
5
0
1
3
2
0
0
5
6

(1/5)ρ3
−→


1
5
1
5
0
1
3
2
0
0
1
6/5


−3ρ3+ρ2
−→
−ρ3+ρ1


1
5
0
19/5
0
1
0
−8/5
0
0
1
6/5

−5ρ2+ρ1
−→


1
0
0
59/5
0
1
0
−8/5
0
0
1
6/5


One.III.1.9
For the “Gauss” halves, see the answers to Exercise 19.
(a) The “Jordan” half goes this way.
(1/2)ρ1
−→
−(1/3)ρ2
µ
1
1/2
−1/2
1/2
0
1
−2/3
−1/3
¶
−(1/2)ρ2+ρ1
−→
µ
1
0
−1/6
2/3
0
1
−2/3
−1/3
¶
The solution set is this
{


2/3
−1/3
0

+


1/6
2/3
1

z
¯¯ z ∈R}
(b) The second half is
ρ3+ρ2
−→


1
0
−1
0
1
0
1
2
0
3
0
0
0
1
0


so the solution is this.
{




1
3
0
0



+




1
−2
1
0



z
¯¯ z ∈R}
(c) This Jordan half
ρ2+ρ1
−→




1
0
1
1
0
0
1
0
1
0
0
0
0
0
0
0
0
0
0
0




gives
{




0
0
0
0



+




−1
0
1
0



z +




−1
−1
0
1



w
¯¯ z, w ∈R}
(of course, the zero vector could be omitted from the description).
(d) The “Jordan” half
−(1/7)ρ2
−→
µ
1
2
3
1
−1
1
0
1
8/7
2/7
−4/7
0
¶
−2ρ2+ρ1
−→
µ
1
0
5/7
3/7
1/7
1
0
1
8/7
2/7
−4/7
0
¶
ends with this solution set.
{






1
0
0
0
0






+






−5/7
−8/7
1
0
0






c +






−3/7
−2/7
0
1
0






d +






−1/7
4/7
0
0
1






e
¯¯ c, d, e ∈R}

Answers to Exercises
29
One.III.1.10
Routine Gauss’ method gives one:
−3ρ1+ρ2
−→
−(1/2)ρ1+ρ3


2
1
1
3
0
1
−2
−7
0
9/2
1/2
7/2

−(9/2)ρ2+ρ3
−→


2
1
1
3
0
1
−2
−7
0
0
19/2
35


and any cosmetic change, like multiplying the bottom row by 2,


2
1
1
3
0
1
−2
−7
0
0
19
70


gives another.
One.III.1.11
In the cases listed below, we take a, b ∈R. Thus, some canonical forms listed below
actually include inﬁnitely many cases. In particular, they includes the cases a = 0 and b = 0.
(a)
µ
0
0
0
0
¶
,
µ
1
a
0
0
¶
,
µ
0
1
0
0
¶
,
µ
1
0
0
1
¶
(b)
µ
0
0
0
0
0
0
¶
,
µ
1
a
b
0
0
0
¶
,
µ
0
1
a
0
0
0
¶
,
µ
0
0
1
0
0
0
¶
,
µ
1
0
a
0
1
b
¶
,
µ
1
a
0
0
0
1
¶
,
µ
0
1
0
0
0
1
¶
(c)


0
0
0
0
0
0

,


1
a
0
0
0
0

,


0
1
0
0
0
0

,


1
0
0
1
0
0


(d)


0
0
0
0
0
0
0
0
0

,


1
a
b
0
0
0
0
0
0

,


0
1
a
0
0
0
0
0
0

,


0
0
1
0
0
0
0
0
0

,


1
0
a
0
1
b
0
0
0

,


1
a
0
0
0
1
0
0
0

,


1
0
0
0
1
0
0
0
1


One.III.1.12
A nonsingular homogeneous linear system has a unique solution. So a nonsingular matrix
must reduce to a (square) matrix that is all 0’s except for 1’s down the upper-left to lower-right diagonal,
e.g.,
µ
1
0
0
1
¶
,
or


1
0
0
0
1
0
0
0
1

,
etc.
One.III.1.13
(a) The ρi ↔ρi operation does not change A.
(b) For instance,
µ
1
2
3
4
¶
−ρ1+ρ1
−→
µ
0
0
3
4
¶
ρ1+ρ1
−→
µ
0
0
3
4
¶
leaves the matrix changed.
(c) If i ̸= j then









...
ai,1
· · ·
ai,n
...
aj,1
· · ·
aj,n
...









kρi+ρj
−→









...
ai,1
· · ·
ai,n
...
kai,1 + aj,1
· · ·
kai,n + aj,n
...









−kρi+ρj
−→









...
ai,1
· · ·
ai,n
...
−kai,1 + kai,1 + aj,1
· · ·
−kai,n + kai,n + aj,n
...









does indeed give A back. (Of course, if i = j then the third matrix would have entries of the form
−k(kai,j + ai,j) + kai,j + ai,j.)
Subsection One.III.2: Row Equivalence
One.III.2.11
Bring each to reduced echelon form and compare.

30
Linear Algebra, by Hefferon
(a) The ﬁrst gives
−4ρ1+ρ2
−→
µ
1
2
0
0
¶
while the second gives
ρ1↔ρ2
−→
µ
1
2
0
1
¶
−2ρ2+ρ1
−→
µ
1
0
0
1
¶
The two reduced echelon form matrices are not identical, and so the original matrices are not row
equivalent.
(b) The ﬁrst is this.
−3ρ1+ρ2
−→
−5ρ1+ρ3


1
0
2
0
−1
−5
0
−1
−5

−ρ2+ρ3
−→


1
0
2
0
−1
−5
0
0
0

−ρ2
−→


1
0
2
0
1
5
0
0
0


The second is this.
−2ρ1+ρ3
−→


1
0
2
0
2
10
0
0
0

(1/2)ρ2
−→


1
0
2
0
1
5
0
0
0


These two are row equivalent.
(c) These two are not row equivalent because they have diﬀerent sizes.
(d) The ﬁrst,
ρ1+ρ2
−→
µ
1
1
1
0
3
3
¶
(1/3)ρ2
−→
µ
1
1
1
0
1
1
¶
−ρ2+ρ1
−→
µ
1
0
0
0
1
1
¶
and the second.
ρ1↔ρ2
−→
µ2
2
5
0
3
−1
¶
(1/2)ρ1
−→
(1/3)ρ2
µ1
1
5/2
0
1
−1/3
¶
−ρ2+ρ1
−→
µ1
0
17/6
0
1
−1/3
¶
These are not row equivalent.
(e) Here the ﬁrst is
(1/3)ρ2
−→
µ
1
1
1
0
0
1
¶
−ρ2+ρ1
−→
µ
1
1
0
0
0
1
¶
while this is the second.
ρ1↔ρ2
−→
µ
1
−1
1
0
1
2
¶
ρ2+ρ1
−→
µ
1
0
3
0
1
2
¶
These are not row equivalent.
One.III.2.12
First, the only matrix row equivalent to the matrix of all 0’s is itself (since row operations
have no eﬀect).
Second, the matrices that reduce to
µ
1
a
0
0
¶
have the form
µ
b
ba
c
ca
¶
(where a, b, c ∈R).
Next, the matrices that reduce to
µ
0
1
0
0
¶
have the form
µ
0
a
0
b
¶
(where a, b ∈R).
Finally, the matrices that reduce to
µ
1
0
0
1
¶
are the nonsingular matrices. That’s because a linear system for which this is the matrix of coeﬃcients
will have a unique solution, and that is the deﬁnition of nonsingular. (Another way to say the same
thing is to say that they fall into none of the above classes.)

Answers to Exercises
31
One.III.2.13
(a) They have the form
µ
a
0
b
0
¶
where a, b ∈R.
(b) They have this form (for a, b ∈R).
µ
1a
2a
1b
2b
¶
(c) They have the form
µ
a
b
c
d
¶
(for a, b, c, d ∈R) where ad −bc ̸= 0. (This is the formula that determines when a 2×2 matrix is
nonsingular.)
One.III.2.14
Inﬁnitely many. For instance, in µ1
k
0
0
¶
each k ∈R gives a diﬀerent class.
One.III.2.15
No. Row operations do not change the size of a matrix.
One.III.2.16
(a) A row operation on a zero matrix has no eﬀect. Thus each zero matrix is alone in
its row equivalence class.
(b) No. Any nonzero entry can be rescaled.
One.III.2.17
Here are two.
µ1
1
0
0
0
1
¶
and
µ1
0
0
0
0
1
¶
One.III.2.18
Any two n×n nonsingular matrices have the same reduced echelon form, namely the
matrix with all 0’s except for 1’s down the diagonal.





1
0
0
0
1
0
...
0
0
1





Two 2×2 singular matrices need not be row equivalent.
µ
1
1
0
0
¶
and
µ
1
0
0
0
¶
One.III.2.19
Since there is one and only one reduced echelon form matrix in each class, we can just
list the possible reduced echelon form matrices.
For that list, see the answer for Exercise 11.
One.III.2.20
(a) If there is a linear relationship where c0 is not zero then we can subtract c0⃗β0 and
divide both sides by c0 to get ⃗β0 as a linear combination of the others. (Remark. If there are no
others — if the relationship is, say, ⃗0 = 3 · ⃗0 — then the statement is still true because zero is by
deﬁnition the sum of the empty set of vectors.)
If ⃗β0 is a combination of the others ⃗β0 = c1⃗β1 + · · · + cn⃗βn then subtracting ⃗β0 from both sides
gives a relationship where one of the coeﬃcients is nonzero, speciﬁcally, the coeﬃcient is −1.
(b) The ﬁrst row is not a linear combination of the others for the reason given in the proof: in the
equation of components from the column containing the leading entry of the ﬁrst row, the only
nonzero entry is the leading entry from the ﬁrst row, so its coeﬃcient must be zero. Thus, from the
prior part of this question, the ﬁrst row is in no linear relationship with the other rows. Hence, to
see if the second row can be in a linear relationship with the other rows, we can leave the ﬁrst row
out of the equation. But now the argument just applied to the ﬁrst row will apply to the second
row. (Technically, we are arguing by induction here.)
One.III.2.21
(a) As in the base case we will argue that ℓ2 isn’t less than k2 and that it also isn’t
greater. To obtain a contradiction, assume that ℓ2 ≤k2 (the k2 ≤ℓ2 case, and the possibility that
either or both is a zero row, are left to the reader). Consider the i = 2 version of the equation
that gives each row of B as a linear combination of the rows of D. Focus on the ℓ1-th and ℓ2-th
component equations.
b2,ℓ1 = c2,1d1,ℓ1 + c2,2d2,ℓ1 + · · · + c2,mdm,ℓ1b2,ℓ2
= c2,1d1,ℓ2 + c2,2d2,ℓ2 + · · · + c2,mdm,ℓ2

32
Linear Algebra, by Hefferon
The ﬁrst of these equations shows that c2,1 is zero because δ1,ℓ1 is not zero, but since both matrices
are in echelon form, each of the entries d2,ℓ1, . . . , dm,ℓ1, and b2,ℓ1 is zero. Now, with the second
equation, b2,ℓ2 is nonzero as it leads its row, c2,1 is zero by the prior sentence, and each of d3,ℓ2,
. . . , dm,ℓ2 is zero because D is in echelon form and we’ve assumed that ℓ2 ≤k2. Thus, this second
equation shows that d2,ℓ2 is nonzero and so k2 ≤ℓ2. Therefore k2 = ℓ2.
(b) For the inductive step assume that ℓ1 = k1, . . . , ℓj = kj (where 1 ≤j < m); we will show that
implies ℓj+1 = kj+1.
We do the ℓj+1 ≤kj+1 < ∞case here — the other cases are then easy. Consider the ρj+1 version
of the vector equation:
¡0
. . .
0
βj+1,ℓj1
. . .
βj+1,n
¢
=
cj+1,1
¡
0
. . .
δ1,k1
. . .
δ1,kj
. . .
δ1,kj+1
. . .
δ1,km
. . .
¢
...
+ cj+1,j
¡
0
. . .
0
. . .
δj,kj
. . .
δj,kj+1
. . .
δj,km
. . .
¢
+ cj+1,j+1
¡
0
. . .
0
. . .
0
. . .
δj+1,kj+1
. . .
δj+1,km
. . .
¢
...
+ cj+1,m
¡0
. . .
0
. . .
0
. . .
0
. . .
δm,km
. . .¢
Knowing that ℓ1 = k1, . . . , ℓj = kj, consider the ℓ1-th, . . . , ℓj-th component equations.
0 = cj+1,1δ1,k1 + cj+1,2 · 0 + · · · + cj+1,j · 0 + cj+1,j+1 · 0 + · · · + cj+1,m · 0
0 = cj+1,1δ1,k2 + cj+1,2δ2,kj · · · + cj+1,j · 0 + cj+1,j+1 · 0 + · · · + cj+1,m · 0
...
0 = cj+1,1δ1,kj + cj+1,2δ2,k2 · · · + cj+1,jδj,kj + cj+1,j+1 · 0 + · · · + cj+1,m · 0
We can conclude that cj+1,1, . . . , cj+1,j are all zero.
Now look at the ℓj+1-th component equation:
βj+1,ℓj+1 = cj+1,j+1δj+1,ℓj+1 + cj+1,j+2δj+2,ℓj+1 + · · · + cj+1,mδm,ℓj+1.
Because D is in echelon form and because ℓj+1 ≤kj+1, each of δj+2,ℓj+1, . . . , δm,ℓj+1 is zero. But
βj+1,ℓj+1 is nonzero since it leads its row, and so δj+1,ℓj+1 is nonzero.
Conclusion: kj+1 ≤ℓj+1 and so kj+1 = ℓj+1.
(c) From the prior answer, we know that for any echelon form matrix, if this relationship holds
among the non-zero rows:
ρi = c1ρ1 + · · · + ci−1ρi−1 + ci+1ρi+1 + · · · + cnρn
(where c1, . . . , cn ∈R) then c1,. . . , ci−1 must all be zero (in the i = 1 case we don’t know any of the
scalars are zero).
To derive a contradiction suppose the above relationship exists and let ℓi be the column index
of the leading entry of ρi. Consider the equation of ℓi-th components:
ρi,ℓi = ci+1ρi+1,ℓi + · · · + cnρn,ℓi
and observe that because the matrix is in echelon form each of ρi+1,ℓi, . . . , ρn,ℓi is zero. But that’s
a contradiction as ρi,ℓi is nonzero since it leads the i-th row.
Hence the linear relationship supposed to exist among the rows is not possible.
One.III.2.22
(a) The inductive step is to show that if the statement holds on rows 1 through r then
it also holds on row r + 1. That is, we assume that ℓ1 = k1, and ℓ2 = k2, . . . , and ℓr = kr, and we
will show that ℓr+1 = kr+1 also holds (for r in 1 .. m −1).
(b) Lemma 2.3 gives the relationship βr+1 = sr+1,1δ1 + sr+2,2δ2 + · · · + sr+1,mδm between rows.
Inside of those rows, consider the relationship between entries in column ℓ1 = k1. Because r +1 > 1,
the row βr+1 has a zero in that entry (the matrix B is in echelon form), while the row δ1 has
a nonzero entry in column k1 (it is, by deﬁnition of k1, the leading entry in the ﬁrst row of D).
Thus, in that column, the above relationship among rows resolves to this equation among numbers:
0 = sr+1,1 · d1,k1, with d1,k1 ̸= 0. Therefore sr+1,1 = 0.
With sr+1,1 = 0, a similar argument shows that sr+1,2 = 0. With those two, another turn gives
that sr+1,3 = 0. That is, inside of the larger induction argument used to prove the entire lemma is
here an subargument by induction that shows sr+1,j = 0 for all j in 1 .. r. (We won’t write out the
details since it is just like the induction done in Exercise 21.)

Answers to Exercises
33
(c) First, ℓr+1 < kr+1 is impossible. In the columns of D to the left of column kr+1 the entries are
are all zeroes as dr+1,kr+1 leads the row k + 1) and so if ℓk+1 < kk+1 then the equation of entries
from column ℓk+1 would be br+1,ℓr+1 = sr+1,1 · 0 + · · · + sr+1,m · 0, but br+1,ℓr+1 isn’t zero since it
leads its row. A symmetric argument shows that kr+1 < ℓr+1 also is impossible.
One.III.2.23
The zero rows could have nonzero coeﬃcients, and so the statement would not be true.
One.III.2.24
We know that 4s + c + 10d = 8.45 and that 3s + c + 7d = 6.30, and we’d like to know
what s + c + d is. Fortunately, s + c + d is a linear combination of 4s + c + 10d and 3s + c + 7d. Calling
the unknown price p, we have this reduction.


4
1
10
8.45
3
1
7
6.30
1
1
1
p

−(3/4)ρ1+ρ2
−→
−(1/4)ρ1+ρ3


4
1
10
8.45
0
1/4
−1/2
−0.037 5
0
3/4
−3/2
p −2.112 5

−3ρ2+ρ3
−→


4
1
10
8.45
0
1/4
−1/2
−0.037 5
0
0
0
p −2.00


The price paid is $2.00.
One.III.2.25
If multiplication of a row by zero were allowed then Lemma 2.6 would not hold. That
is, where
µ
1
3
2
1
¶
0ρ2
−→
µ
1
3
0
0
¶
all the rows of the second matrix can be expressed as linear combinations of the rows of the ﬁrst, but
the converse does not hold. The second row of the ﬁrst matrix is not a linear combination of the rows
of the second matrix.
One.III.2.26
(1)
An easy answer is this:
0 = 3.
For a less wise-guy-ish answer, solve the system:
µ3
−1
8
2
1
3
¶
−(2/3)ρ1+ρ2
−→
µ3
−1
8
0
5/3
−7/3
¶
gives y = −7/5 and x = 11/5. Now any equation not satisﬁed by (−7/5, 11/5) will do, e.g.,
5x + 5y = 3.
(2)
Every equation can be derived from an inconsistent system. For instance, here is how to derive
“3x + 2y = 4” from “0 = 5”. First,
0 = 5
(3/5)ρ1
−→0 = 3
xρ1
−→0 = 3x
(validity of the x = 0 case is separate but clear). Similarly, 0 = 2y. Ditto for 0 = 4. But now,
0 + 0 = 0 gives 3x + 2y = 4.
One.III.2.27
Deﬁne linear systems to be equivalent if their augmented matrices are row equivalent.
The proof that equivalent systems have the same solution set is easy.
One.III.2.28
(a) The three possible row swaps are easy, as are the three possible rescalings. One of
the six possible pivots is kρ1 + ρ2:

1
2
3
k · 1 + 3
k · 2 + 0
k · 3 + 3
1
4
5


and again the ﬁrst and second columns add to the third. The other ﬁve pivots are similar.
(b) The obvious conjecture is that row operations do not change linear relationships among columns.
(c) A case-by-case proof follows the sketch given in the ﬁrst item.
Topic: Computer Algebra Systems
1
(a) The commands
> A:=array( [[40,15],
[-50,25]] );
> u:=array([100,50]);
> linsolve(A,u);
yield the answer [1, 4].
(b) Here there is a free variable:

34
Linear Algebra, by Hefferon
> A:=array( [[7,0,-7,0],
[8,1,-5,2],
[0,1,-3,0],
[0,3,-6,-1]] );
> u:=array([0,0,0,0]);
> linsolve(A,u);
prompts the reply [ t1, 3 t1, t1, 3 t1].
2
These are easy to type in. For instance, the ﬁrst
> A:=array( [[2,2],
[1,-4]] );
> u:=array([5,0]);
> linsolve(A,u);
gives the expected answer of [2, 1/2]. The others are entered similarly.
(a) The answer is x = 2 and y = 1/2.
(b) The answer is x = 1/2 and y = 3/2.
(c) This system has inﬁnitely many solutions. In the ﬁrst subsection, with z as a parameter, we got
x = (43−7z)/4 and y = (13−z)/4. Maple responds with [−12+7 t1, t1, 13−4 t1], for some reason
preferring y as a parameter.
(d) There is no solution to this system. When the array A and vector u are given to Maple and it
is asked to linsolve(A,u), it returns no result at all, that is, it responds with no solutions.
(e) The solutions is (x, y, z) = (5, 5, 0).
(f) There are many solutions. Maple gives [1, −1 + t1, 3 −t1, t1].
3
As with the prior question, entering these is easy.
(a) This system has inﬁnitely many solutions. In the second subsection we gave the solution set as
{
µ
6
0
¶
+
µ
−2
1
¶
y
¯¯ y ∈R}
and Maple responds with [6 −2 t1, t1].
(b) The solution set has only one member
{
µ
0
1
¶
}
and Maple has no trouble ﬁnding it [0, 1].
(c) This system’s solution set is inﬁnite
{


4
−1
0

+


−1
1
1

x3
¯¯ x3 ∈R}
and Maple gives [ t1, −t1 + 3, −t1 + 4].
(d) There is a unique solution
{


1
1
1

}
and Maple gives [1, 1, 1].
(e) This system has inﬁnitely many solutions; in the second subsection we described the solution set
with two parameters
{




5/3
2/3
0
0



+




−1/3
2/3
1
0



z +




−2/3
1/3
0
1



w
¯¯ z, w ∈R}
as does Maple [3 −2 t1 + t2, t1, t2, −2 + 3 t1 −2 t2].
(f) The solution set is empty and Maple replies to the linsolve(A,u) command with no returned
solutions.
4
In response to this prompting
> A:=array( [[a,c],
[b,d]] );
> u:=array([p,q]);
> linsolve(A,u);

Answers to Exercises
35
Maple thought for perhaps twenty seconds and gave this reply.
£
−−d p + q c
−b c + a d, −b p + a q
−b c + a d
¤
Topic: Input-Output Analysis
1
These answers were given by Octave.
(a) With the external use of steel as 17 789 and the external use of autos as 21 243, we get s = 25 952,
a = 30 312.
(b) s = 25 857, a = 30 596
(c) s = 25 984, a = 30 597
2
Octave gives these answers.
(a) s = 24 244, a = 30 307
(b) s = 24 267, a = 30 673
3
(a) These are the equations.
(11.79/18.69)s −(1.28/4.27)a = 11.56
−(0/18.69)s + (9.87/4.27)a = 11.35
Octave gives s = 20.66 and a = 16.41.
(b) These are the ratios.
1947
by steel
by autos
use of steel
0.63
0.09
use of autos
0.00
0.69
1958
by steel
by autos
use of steel
0.79
0.09
use of autos
0.00
0.70
(c) Octave gives (in billions of 1947 dollars) s = 24.82 and a = 23.63. In billions of 1958 dollars that
is s = 32.26 and a = 30.71.
Topic: Accuracy of Computations
1
Sceintiﬁc notation is convienent to express the two-place restriction. We have .25 × 102 + .67 × 100 =
.25 × 102. The 2/3 has no apparent eﬀect.
2
The reduction
−3ρ1+ρ2
−→
x + 2y =
3
−8 = −7.992
gives a solution of (x, y) = (1.002, 0.999).
3
(a) The fully accurate solution is that x = 10 and y = 0.
(b) The four-digit conclusion is quite diﬀerent.
−(.3454/.0003)ρ1+ρ2
−→
µ
.0003
1.556
1.569
0
1789
−1805
¶
=⇒x = 10460, y = −1.009
4
(a) For the ﬁrst one, ﬁrst, (2/3) −(1/3) is .666 666 67 −.333 333 33 = .333 333 34 and so (2/3) +
((2/3) −(1/3)) = .666 666 67 + .333 333 34 = 1.000 000 0. For the other one, ﬁrst ((2/3) + (2/3)) =
.666 666 67 + .666 666 67 = 1.333 333 3 and so ((2/3) + (2/3)) −(1/3) = 1.333 333 3 −.333 333 33 =
.999 999 97.
(b) The ﬁrst equation is .333 333 33 · x + 1.000 000 0 · y = 0 while the second is .666 666 67 · x +
2.000 000 0 · y = 0.
5
(a) This calculation
−(2/3)ρ1+ρ2
−→
−(1/3)ρ1+ρ3


3
2
1
6
0
−(4/3) + 2ε
−(2/3) + 2ε
−2 + 4ε
0
−(2/3) + 2ε
−(1/3) −ε
−1 + ε


−(1/2)ρ2+ρ3
−→


3
2
1
6
0
−(4/3) + 2ε
−(2/3) + 2ε
−2 + 4ε
0
ε
−2ε
−ε


gives a third equation of y−2z = −1. Substituting into the second equation gives ((−10/3)+6ε)·z =
(−10/3) + 6ε so z = 1 and thus y = 1. With those, the ﬁrst equation says that x = 1.

36
Linear Algebra, by Hefferon
(b) The solution with two digits kept


.30 × 101
.20 × 101
.10 × 101
.60 × 101
.10 × 101
.20 × 10−3
.20 × 10−3
.20 × 101
.30 × 101
.20 × 10−3
−.10 × 10−3
.10 × 101


−(2/3)ρ1+ρ2
−→
−(1/3)ρ1+ρ3


.30 × 101
.20 × 101
.10 × 101
.60 × 101
0
−.13 × 101
−.67 × 100
−.20 × 101
0
−.67 × 100
−.33 × 100
−.10 × 101


−(.67/1.3)ρ2+ρ3
−→


.30 × 101
.20 × 101
.10 × 101
.60 × 101
0
−.13 × 101
−.67 × 100
−.20 × 101
0
0
.15 × 10−2
.31 × 10−2


comes out to be z = 2.1, y = 2.6, and x = −.43.
Topic: Analyzing Networks
1
(a) The total resistance is 7 ohms. With a 9 volt potential, the ﬂow will be 9/7 amperes. Inciden-
tally, the voltage drops will then be: 27/7 volts across the 3 ohm resistor, and 18/7 volts across each
of the two 2 ohm resistors.
(b) One way to do this network is to note that the 2 ohm resistor on the left has a voltage drop
across it of 9 volts (and hence the ﬂow through it is 9/2 amperes), and the remaining portion on
the right also has a voltage drop of 9 volts, and so is analyzed as in the prior item.
We can also use linear systems.
−→
i0
i1 ↓
−→
i2
i3
←−
Using the variables from the diagram we get a linear system
i0 −i1 −i2
= 0
i1 + i2 −i3 = 0
2i1
= 9
7i2
= 9
which yields the unique solution i1 = 81/14, i1 = 9/2, i2 = 9/7, and i3 = 81/14.
Of course, the ﬁrst and second paragraphs yield the same answer. Esentially, in the ﬁrst para-
graph we solved the linear system by a method less systematic than Gauss’ method, solving for some
of the variables and then substituting.
(c) Using these variables
−→
i0
i1 ↓
−→
i2
−→
i3
i4 ↓
i5
←−
i6
←−
one linear system that suﬃces to yield a unique solution is this.
i0 −i1 −i2
= 0
i2 −i3 −i4
= 0
i3 + i4 −i5
= 0
i1
+ i5 −i6 = 0
3i1
= 9
3i2
+ 2i4 + 2i5
= 9
3i2 + 9i3
+ 2i5
= 9

Answers to Exercises
37
(The last three equations come from the circuit involving i0-i1-i6, the circuit involving i0-i2-i4-i5-
i6, and the circuit with i0-i2-i3-i5-i6.)
Octave gives i0 = 4.35616, i1 = 3.00000, i2 = 1.35616,
i3 = 0.24658, i4 = 1.10959, i5 = 1.35616, i6 = 4.35616.
2
(a) Using the variables from the earlier analysis,
i0 −
i1 −i2 = 0
−i0 +
i1 + i2 = 0
5i1
= 20
8i2 = 20
−5i1 + 8i2 = 0
The current ﬂowing in each branch is then is i2 = 20/8 = 2.5, i1 = 20/5 = 4, and i0 = 13/2 = 6.5, all
in amperes. Thus the parallel portion is acting like a single resistor of size 20/(13/2) ≈3.08 ohms.
(b) A similar analysis gives that is i2 = i1 = 20/8 = 4 and i0 = 40/8 = 5 amperes. The equivalent
resistance is 20/5 = 4 ohms.
(c) Another analysis like the prior ones gives is i2 = 20/r2, i1 = 20/r1, and i0 = 20(r1+r2)/(r1r2), all
in amperes. So the parallel portion is acting like a single resistor of size 20/i1 = r1r2/(r1 +r2) ohms.
(This equation is often stated as: the equivalent resistance r satisﬁes 1/r = (1/r1) + (1/r2).)
3
(a) The circuit looks like this.
(b) The circuit looks like this.
4
Not yet done.
5
(a) An adaptation is: in any intersection the ﬂow in equals the ﬂow out. It does seem reasonable
in this case, unless cars are stuck at an intersection for a long time.
(b) We can label the ﬂow in this way.
Shelburne St
Willow
Winooski Ave
west
east
Jay Ln
Because 50 cars leave via Main while 25 cars enter, i1 −25 = i2. Similarly Pier’s in/out balance
means that i2 = i3 and North gives i3 + 25 = i1. We have this system.
i1 −i2
=
25
i2 −i3 = 0
−i1
+ i3 = −25

38
Linear Algebra, by Hefferon
(c) The row operations ρ1 + ρ2 and rho2 + ρ3 lead to the conclusion that there are inﬁnitely many
solutions. With i3 as the parameter,
{


25 + i3
i3
i3

¯¯ i3 ∈R}
of course, since the problem is stated in number of cars, we might restrict i3 to be a natural number.
(d) If we picture an initially-empty circle with the given input/output behavior, we can superimpose
a z3-many cars circling endlessly to get a new solution.
(e) A suitable restatement might be: the number of cars entering the circle must equal the number
of cars leaving. The reasonableness of this one is not as clear. Over the ﬁve minute time period it
could easily work out that a half dozen more cars entered than left, although the into/out of table
in the problem statement does have that this property is satisﬁed. In any event it is of no help in
getting a unique solution since for that we need to know the number of cars circling endlessly.
6
(a) Here is a variable for each unknown block; each known block has the ﬂow shown.
75
65
55
40
50
30
70
80
5
i1
i2
i3
i4
i7
i5
i6
We apply Kirchoﬀ’s principle that the ﬂow into the intersection of Willow and Shelburne must equal
the ﬂow out to get i1 + 25 = i2 + 125. Doing the intersections from right to left and top to bottom
gives these equations.
i1 −i2
=
10
−i1
+
i3
=
15
i2
+ i4
=
5
−i3 −i4
+
i6
= −50
i5
−i7 = −10
−i6 + i7 =
30
The row operation ρ1 + ρ2 followed by ρ2 + ρ3 then ρ3 + ρ4 and ρ4 + ρ5 and ﬁnally ρ5 + ρ6 result in
this system.
i1 −
i2
=
10
−i2 + i3
=
25
i3 + i4 −
i5
=
30
−i5 +
i6
= −20
−i6 + i7 = −30
0 =
0
Since the free variables are i4 and i7 we take them as parameters.
i6 = i7 −30
i5 = i6 + 20 = (i7 −30) + 20 = i7 −10
i3 = −i4 + i5 + 30 = −i4 + (i7 −10) + 30 = −i4 + i7 + 20
i2 = i3 −25 = (−i4 + i7 + 20) −25 = −i4 + i7 −5
i1 = i2 + 10 = (−i4 + i7 −5) + 10 = −i4 + i7 + 5
()
Obviously i4 and i7 have to be positive, and in fact the ﬁrst equation shows that i7 must be at least
30. If we start with i7, then the i2 equation shows that 0 ≤i4 ≤i7 −5.
(b) We cannot take i7 to be zero or else i6 will be negative (this would mean cars going the wrong
way on the one-way street Jay). We can, however, take i7 to be as small as 30, and then there are
many suitable i4’s. For instance, the solution
(i1, i2, i3, i4, i5, i6, i7) = (35, 25, 50, 0, 20, 0, 30)
results from choosing i4 = 0.

Chapter Two: Vector Spaces
Subsection Two.I.1: Deﬁnition and Examples
Two.I.1.18
(a) 0 + 0x + 0x2 + 0x3
(b)
µ
0
0
0
0
0
0
0
0
¶
(c) The constant function f(x) = 0
(d) The constant function f(n) = 0
Two.I.1.19
(a) 3 + 2x −x2
(b)
µ
−1
+1
0
−3
¶
(c) −3ex + 2e−x
Two.I.1.20
Most of the conditions are easy to check; use Example 1.3 as a guide. Here are some
comments.
(a) This is just like Example 1.3; the zero element is 0 + 0x.
(b) The zero element of this space is the 2×2 matrix of zeroes.
(c) The zero element is the vector of zeroes.
(d) Closure of addition involves noting that the sum




x1
y1
z1
w1



+




x2
y2
z2
w2



=




x1 + x2
y1 + y2
z1 + z2
w1 + w2




is in L because (x1+x2)+(y1+y2)−(z1+z2)+(w1+w2) = (x1+y1−z1+w1)+(x2+y2−z2+w2) = 0+0.
Closure of scalar multiplication is similar. Note that the zero element, the vector of zeroes, is in L.
Two.I.1.21
In each item the set is called Q. For some items, there are other correct ways to show that
Q is not a vector space.
(a) It is not closed under addition; it fails to meet condition (1).


1
0
0

,


0
1
0

∈Q


1
1
0

̸∈Q
(b) It is not closed under addition.

1
0
0

,


0
1
0

∈Q


1
1
0

̸∈Q
(c) It is not closed under addition.
µ0
1
0
0
¶
,
µ1
1
0
0
¶
∈Q
µ1
2
0
0
¶
̸∈Q
(d) It is not closed under scalar multiplication.
1 + 1x + 1x2 ∈Q
−1 · (1 + 1x + 1x2) ̸∈Q
(e) It is empty, violating condition (4).
Two.I.1.22
The usual operations (v0 + v1i) + (w0 + w1i) = (v0 + w0) + (v1 + w1)i and r(v0 + v1i) =
(rv0) + (rv1)i suﬃce. The check is easy.
Two.I.1.23
No, it is not closed under scalar multiplication since, e.g., π · (1) is not a rational number.
Two.I.1.24
The natural operations are (v1x + v2y + v3z) + (w1x + w2y + w3z) = (v1 + w1)x + (v2 +
w2)y + (v3 + w3)z and r · (v1x + v2y + v3z) = (rv1)x + (rv2)y + (rv3)z. The check that this is a vector
space is easy; use Example 1.3 as a guide.
Two.I.1.25
The ‘+’ operation is not commutative (that is, condition (2) is not met); producing two
members of the set witnessing this assertion is easy.

40
Linear Algebra, by Hefferon
Two.I.1.26
(a) It is not a vector space.
(1 + 1) ·


1
0
0

̸=


1
0
0

+


1
0
0


(b) It is not a vector space.
1 ·


1
0
0

̸=


1
0
0


Two.I.1.27
For each “yes” answer, you must give a check of all the conditions given in the deﬁni-
tion of a vector space. For each “no” answer, give a speciﬁc example of the failure of one of the
conditions.
(a) Yes.
(b) Yes.
(c) No, it is not closed under addition.
The vector of all 1/4’s, when added to itself, makes a
nonmember.
(d) Yes.
(e) No, f(x) = e−2x + (1/2) is in the set but 2 · f is not (that is, condition (6) fails).
Two.I.1.28
It is a vector space. Most conditions of the deﬁnition of vector space are routine; we here
check only closure. For addition, (f1 + f2) (7) = f1(7) + f2(7) = 0 + 0 = 0. For scalar multiplication,
(r · f) (7) = rf(7) = r0 = 0.
Two.I.1.29
We check Deﬁnition 1.1.
First, closure under ‘+’ holds because the product of two positive reals is a positive real. The
second condition is satisﬁed because real multiplication commutes. Similarly, as real multiplication
associates, the third checks. For the fourth condition, observe that multiplying a number by 1 ∈R+
won’t change the number. Fifth, any positive real has a reciprocal that is a positive real.
The sixth, closure under ‘·’, holds because any power of a positive real is a positive real. The
seventh condition is just the rule that vr+s equals the product of vr and vs. The eight condition says
that (vw)r = vrwr. The ninth condition asserts that (vr)s = vrs. The ﬁnal condition says that v1 = v.
Two.I.1.30
(a) No: 1 · (0, 1) + 1 · (0, 1) ̸= (1 + 1) · (0, 1).
(b) No; the same calculation as the prior answer shows a contition in the deﬁnition of a vector
space that is violated. Another example of a violation of the conditions for a vector space is that
1 · (0, 1) ̸= (0, 1).
Two.I.1.31
It is not a vector space since it is not closed under addition, as (x2) + (1 + x −x2) is not
in the set.
Two.I.1.32
(a) 6
(b) nm
(c) 3
(d) To see that the answer is 2, rewrite it as
{
µ
a
0
b
−a −b
¶ ¯¯ a, b ∈R}
so that there are two parameters.
Two.I.1.33
A vector space (over R) consists of a set V along with two operations ‘⃗+’ and ‘⃗·’ subject
to these conditions. Where ⃗v, ⃗w ∈V , (1) their vector sum ⃗v ⃗+ ⃗w is an element of V . If ⃗u,⃗v, ⃗w ∈V
then (2) ⃗v ⃗+ ⃗w = ⃗w ⃗+ ⃗v and (3) (⃗v ⃗+ ⃗w) ⃗+ ⃗u = ⃗v ⃗+ (⃗w ⃗+ ⃗u). (4) There is a zero vector ⃗0 ∈V such that
⃗v ⃗+⃗0 = ⃗v for all ⃗v ∈V . (5) Each ⃗v ∈V has an additive inverse ⃗w ∈V such that ⃗w ⃗+⃗v = ⃗0. If r, s are
scalars, that is, members of R), and ⃗v, ⃗w ∈V then (6) each scalar multiple r ·⃗v is in V . If r, s ∈R and
⃗v, ⃗w ∈V then (7) (r + s) ·⃗v = r ·⃗v ⃗+ s ·⃗v, and (8) r⃗· (⃗v + ⃗w) = r⃗·⃗v + r⃗· ⃗w, and (9) (rs)⃗·⃗v = r⃗· (s⃗·⃗v),
and (10) 1⃗· ⃗v = ⃗v.
Two.I.1.34
(a) Let V be a vector space, assume that ⃗v ∈V , and assume that ⃗w ∈V is the additive
inverse of ⃗v so that ⃗w + ⃗v = ⃗0. Because addition is commutative, ⃗0 = ⃗w + ⃗v = ⃗v + ⃗w, so therefore ⃗v
is also the additive inverse of ⃗w.
(b) Let V be a vector space and suppose ⃗v,⃗s,⃗t ∈V . The additive inverse of ⃗v is −⃗v so ⃗v + ⃗s = ⃗v +⃗t
gives that −⃗v + ⃗v + ⃗s = −⃗v + ⃗v + ⃗t, which says that ⃗0 + ⃗s = ⃗0 + ⃗t and so ⃗s = ⃗t.
Two.I.1.35
Addition is commutative, so in any vector space, for any vector ⃗v we have that ⃗v = ⃗v +⃗0 =
⃗0 + ⃗v.

Answers to Exercises
41
Two.I.1.36
It is not a vector space since addition of two matrices of unequal sizes is not deﬁned, and
thus the set fails to satisfy the closure condition.
Two.I.1.37
Each element of a vector space has one and only one additive inverse.
For, let V be a vector space and suppose that ⃗v ∈V . If ⃗w1, ⃗w2 ∈V are both additive inverses of
⃗v then consider ⃗w1 + ⃗v + ⃗w2. On the one hand, we have that it equals ⃗w1 + (⃗v + ⃗w2) = ⃗w1 + ⃗0 = ⃗w1.
On the other hand we have that it equals (⃗w1 + ⃗v) + ⃗w2 = ⃗0 + ⃗w2 = ⃗w2. Therefore, ⃗w1 = ⃗w2.
Two.I.1.38
(a) Every such set has the form {r · ⃗v + s · ⃗w
¯¯ r, s ∈R} where either or both of ⃗v, ⃗w may
be ⃗0. With the inherited operations, closure of addition (r1⃗v + s1 ⃗w) + (r2⃗v + s2 ⃗w) = (r1 + r2)⃗v +
(s1 + s2)⃗w and scalar multiplication c(r⃗v + s⃗w) = (cr)⃗v + (cs)⃗w are easy. The other conditions are
also routine.
(b) No such set can be a vector space under the inherited operations because it does not have a zero
element.
Two.I.1.39
Assume that ⃗v ∈V is not ⃗0.
(a) One direction of the if and only if is clear: if r = 0 then r · ⃗v = ⃗0. For the other way, let r be a
nonzero scalar. If r⃗v = ⃗0 then (1/r) · r⃗v = (1/r) ·⃗0 shows that ⃗v = ⃗0, contrary to the assumption.
(b) Where r1, r2 are scalars, r1⃗v = r2⃗v holds if and only if (r1 −r2)⃗v = ⃗0. By the prior item, then
r1 −r2 = 0.
(c) A nontrivial space has a vector ⃗v ̸= ⃗0. Consider the set {k · ⃗v
¯¯ k ∈R}. By the prior item this
set is inﬁnite.
(d) The solution set is either trivial, or nontrivial. In the second case, it is inﬁnite.
Two.I.1.40
Yes. A theorem of ﬁrst semester calculus says that a sum of diﬀerentiable functions is
diﬀerentiable and that (f +g)′ = f ′+g′, and that a multiple of a diﬀerentiable function is diﬀerentiable
and that (r · f)′ = r f ′.
Two.I.1.41
The check is routine. Note that ‘1’ is 1 + 0i and the zero elements are these.
(a) (0 + 0i) + (0 + 0i)x + (0 + 0i)x2
(b)
µ
0 + 0i
0 + 0i
0 + 0i
0 + 0i
¶
Two.I.1.42
Notably absent from the deﬁnition of a vector space is a distance measure.
Two.I.1.43
(a) A small rearrangement does the trick.
(⃗v1 + (⃗v2 + ⃗v3)) + ⃗v4 = ((⃗v1 + ⃗v2) + ⃗v3) + ⃗v4
= (⃗v1 + ⃗v2) + (⃗v3 + ⃗v4)
= ⃗v1 + (⃗v2 + (⃗v3 + ⃗v4))
= ⃗v1 + ((⃗v2 + ⃗v3) + ⃗v4)
Each equality above follows from the associativity of three vectors that is given as a condition in
the deﬁnition of a vector space. For instance, the second ‘=’ applies the rule (⃗w1 + ⃗w2) + ⃗w3 =
⃗w1 + (⃗w2 + ⃗w3) by taking ⃗w1 to be ⃗v1 + ⃗v2, taking ⃗w2 to be ⃗v3, and taking ⃗w3 to be ⃗v4.
(b) The base case for induction is the three vector case. This case ⃗v1 + (⃗v2 + ⃗v3) = (⃗v1 + ⃗v2) + ⃗v3 is
required of any triple of vectors by the deﬁnition of a vector space.
For the inductive step, assume that any two sums of three vectors, any two sums of four vectors,
. . . , any two sums of k vectors are equal no matter how the sums are parenthesized. We will show
that any sum of k + 1 vectors equals this one ((· · · ((⃗v1 + ⃗v2) + ⃗v3) + · · · ) + ⃗vk) + ⃗vk+1.
Any parenthesized sum has an outermost ‘+’. Assume that it lies between ⃗vm and ⃗vm+1 so the
sum looks like this.
(· · · ⃗v1 · · ·⃗vm · · · ) + (· · · ⃗vm+1 · · ·⃗vk+1 · · · )
The second half involves fewer than k + 1 additions, so by the inductive hypothesis we can re-
parenthesize it so that it reads left to right from the inside out, and in particular, so that its
outermost ‘+’ occurs right before ⃗vk+1.
= (· · · ⃗v1 · · · ⃗vm · · · ) + ((· · · (⃗vm+1 + ⃗vm+2) + · · · + ⃗vk) + ⃗vk+1)
Apply the associativity of the sum of three things
= (( · · · ⃗v1 · · · ⃗vm · · · ) + ( · · · (⃗vm+1 + ⃗vm+2) + · · · ⃗vk)) + ⃗vk+1
and ﬁnish by applying the inductive hypothesis inside these outermost parenthesis.

42
Linear Algebra, by Hefferon
Two.I.1.44
(a) We outline the check of the conditions from Deﬁnition 1.1.
Additive closure holds because if a0 + a1 + a2 = 0 and b0 + b1 + b2 = 0 then
(a0 + a1x + a2x2) + (b0 + b1x + b2x2) = (a0 + b0) + (a1 + b1)x + (a2 + b2)x2
is in the set since (a0 + b0) + (a1 + b1) + (a2 + b2) = (a0 + a1 + a2) + (b0 + b1 + b2) is zero. The
second through ﬁfth conditions are easy.
Closure under scalar multiplication holds because if a0 + a1 + a2 = 0 then
r · (a0 + a1x + a2x2) = (ra0) + (ra1)x + (ra2)x2
is in the set as ra0 + ra1 + ra2 = r(a0 + a1 + a2) is zero. The remaining conditions here are also
easy.
(b) This is similar to the prior answer.
(c) Call the vector space V . We have two implications: left to right, if S is a subspace then it is closed
under linear combinations of pairs of vectors and, right to left, if a nonempty subset is closed under
linear combinations of pairs of vectors then it is a subspace. The left to right implication is easy;
we here sketch the other one by assuming S is nonempty and closed, and checking the conditions of
Deﬁnition 1.1.
First, to show closure under addition, if ⃗s1,⃗s2 ∈S then ⃗s1 + ⃗s2 ∈S as ⃗s1 + ⃗s2 = 1 · ⃗s1 + 1 · ⃗s2.
Second, for any ⃗s1,⃗s2 ∈S, because addition is inherited from V , the sum ⃗s1 + ⃗s2 in S equals the
sum ⃗s1 + ⃗s2 in V and that equals the sum ⃗s2 + ⃗s1 in V and that in turn equals the sum ⃗s2 + ⃗s1 in
S. The argument for the third condition is similar to that for the second. For the fourth, suppose
that ⃗s is in the nonempty set S and note that 0 · ⃗s = ⃗0 ∈S; showing that the ⃗0 of V acts under the
inherited operations as the additive identity of S is easy. The ﬁfth condition is satisﬁed because for
any ⃗s ∈S closure under linear combinations shows that the vector 0 · ⃗0 + (−1) · ⃗s is in S; showing
that it is the additive inverse of ⃗s under the inherited operations is routine.
The proofs for the remaining conditions are similar.
Subsection Two.I.2: Subspaces and Spanning Sets
Two.I.2.20
By Lemma 2.9, to see if each subset of M2×2 is a subspace, we need only check if it is
nonempty and closed.
(a) Yes, it is easily checked to be nonempty and closed. This is a parametrization.
{a
µ1
0
0
0
¶
+ b
µ0
0
0
1
¶ ¯¯ a, b ∈R}
By the way, the parametrization also shows that it is a subspace, it is given as the span of the
two-matrix set, and any span is a subspace.
(b) Yes; it is easily checked to be nonempty and closed. Alternatively, as mentioned in the prior
answer, the existence of a parametrization shows that it is a subspace. For the parametrization, the
condition a + b = 0 can be rewritten as a = −b. Then we have this.
{
µ
−b
0
0
b
¶ ¯¯ b ∈R} = {b
µ
−1
0
0
1
¶ ¯¯ b ∈R}
(c) No. It is not closed under addition. For instance,
µ
5
0
0
0
¶
+
µ
5
0
0
0
¶
=
µ
10
0
0
0
¶
is not in the set. (This set is also not closed under scalar multiplication, for instance, it does not
contain the zero matrix.)
(d) Yes.
{b
µ−1
0
0
1
¶
+ c
µ0
1
0
0
¶ ¯¯ b, c ∈R}
Two.I.2.21
No, it is not closed. In particular, it is not closed under scalar multiplication because it
does not contain the zero polynomial.
Two.I.2.22
(a) Yes, solving the linear system arising from
r1


1
0
0

+ r2


0
0
1

=


2
0
1


gives r1 = 2 and r2 = 1.

Answers to Exercises
43
(b) Yes; the linear system arising from r1(x2) + r2(2x + x2) + r3(x + x3) = x −x3
2r2 + r3 =
1
r1 + r2
=
0
r3 = −1
gives that −1(x2) + 1(2x + x2) −1(x + x3) = x −x3.
(c) No; any combination of the two given matrices has a zero in the upper right.
Two.I.2.23
(a) Yes; it is in that span since 1 · cos2 x + 1 · sin2 x = f(x).
(b) No, since r1 cos2 x + r2 sin2 x = 3 + x2 has no scalar solutions that work for all x. For instance,
setting x to be 0 and π gives the two equations r1 · 1 + r2 · 0 = 3 and r1 · 1 + r2 · 0 = 3 + π2, which
are not consistent with each other.
(c) No; consider what happens on setting x to be π/2 and 3π/2.
(d) Yes, cos(2x) = 1 · cos2(x) −1 · sin2(x).
Two.I.2.24
(a) Yes, for any x, y, z ∈R this equation
r1


1
0
0

+ r2


0
2
0

+ r3


0
0
3

=


x
y
z


has the solution r1 = x, r2 = y/2, and r3 = z/3.
(b) Yes, the equation
r1


2
0
1

+ r2


1
1
0

+ r3


0
0
1

=


x
y
z


gives rise to this
2r1 + r2
= x
r2
= y
r1
+ r3 = z
−(1/2)ρ1+ρ3
−→
(1/2)ρ2+ρ3
−→
2r1 + r2
= x
r2
= y
r3 = −(1/2)x + (1/2)y + z
so that, given any x, y, and z, we can compute that r3 = (−1/2)x + (1/2)y + z, r2 = y, and
r1 = (1/2)x −(1/2)y.
(c) No. In particular, the vector


0
0
1


cannot be gotten as a linear combination since the two given vectors both have a third component
of zero.
(d) Yes. The equation
r1


1
0
1

+ r2


3
1
0

+ r3


−1
0
0

+ r4


2
1
5

=


x
y
z


leads to this reduction.


1
3
−1
2
x
0
1
0
1
y
1
0
0
5
z

−ρ1+ρ3
−→
3ρ2+ρ3
−→


1
3
−1
2
x
0
1
0
1
y
0
0
1
6
−x + 3y + z


We have inﬁnitely many solutions. We can, for example, set r4 to be zero and solve for r3, r2, and
r1 in terms of x, y, and z by the usual methods of back-substitution.
(e) No. The equation
r1


2
1
1

+ r2


3
0
1

+ r3


5
1
2

+ r4


6
0
2

=


x
y
z


leads to this reduction.


2
3
5
6
x
1
0
1
0
y
1
1
2
2
z

−(1/2)ρ1+ρ2
−→
−(1/2)ρ1+ρ3
−(1/3)ρ2+ρ3
−→


2
3
5
6
x
0
−3/2
−3/2
−3
−(1/2)x + y
0
0
0
0
−(1/3)x −(1/3)y + z


This shows that not every three-tall vector can be so expressed. Only the vectors satisfying the
restriction that −(1/3)x −(1/3)y + z = 0 are in the span. (To see that any such vector is indeed
expressible, take r3 and r4 to be zero and solve for r1 and r2 in terms of x, y, and z by back-
substitution.)

44
Linear Algebra, by Hefferon
Two.I.2.25
(a) {
¡
c
b
c
¢ ¯¯ b, c ∈R} = {b
¡
0
1
0
¢
+ c
¡
1
0
1
¢ ¯¯ b, c ∈R} The obvious choice
for the set that spans is {
¡0
1
0¢
,
¡1
0
1¢
}.
(b) {
µ
−d
b
c
d
¶ ¯¯ b, c, d ∈R} = {b
µ
0
1
0
0
¶
+ c
µ
0
0
1
0
¶
+ d
µ
−1
0
0
1
¶ ¯¯ b, c, d ∈R} One set that spans
this space consists of those three matrices.
(c) The system
a + 3b
= 0
2a
−c −d = 0
gives b = −(c + d)/6 and a = (c + d)/2. So one description is this.
{c
µ
1/2
−1/6
1
0
¶
+ d
µ
1/2
−1/6
0
1
¶ ¯¯ c, d ∈R}
That shows that a set spanning this subspace consists of those two matrices.
(d) The a = 2b −c gives {(2b −c) + bx + cx3 ¯¯ b, c ∈R} = {b(2 + x) + c(−1 + x3)
¯¯ b, c ∈R}. So the
subspace is the span of the set {2 + x, −1 + x3}.
(e) The set {a + bx + cx2 ¯¯ a + 7b + 49c = 0} parametrized as {b(−7 + x) + c(−49 + x2)
¯¯ b, c ∈R}
has the spanning set {−7 + x, −49 + x2}.
Two.I.2.26
Each answer given is only one out of many possible.
(a) We can parametrize in this way
{


x
0
z

¯¯ x, z ∈R} = {x


1
0
0

+ z


0
0
1

¯¯ x, z ∈R}
giving this for a spanning set.
{


1
0
0

,


0
0
1

}
(b) Parametrize it with {y


−2/3
1
0

+ z


−1/3
0
1

¯¯ y, z ∈R} to get {


−2/3
1
0

,


−1/3
0
1

}.
(c) {




1
−2
1
0



,




−1/2
0
0
1



}
(d) Parametrize the description as {−a1 + a1x + a3x2 + a3x3 ¯¯ a1, a3 ∈R} to get {−1 + x, x2 + x3}.
(e) {1, x, x2, x3, x4}
(f) {
µ
1
0
0
0
¶
,
µ
0
1
0
0
¶
,
µ
0
0
1
0
¶
,
µ
0
0
0
1
¶
}
Two.I.2.27
Technically, no. Subspaces of R3 are sets of three-tall vectors, while R2 is a set of two-tall
vectors. Clearly though, R2 is “just like” this subspace of R3.
{


x
y
0

¯¯ x, y ∈R}
Two.I.2.28
Of course, the addition and scalar multiplication operations are the ones inherited from
the enclosing space.
(a) This is a subspace. It is not empty as it contains at least the two example functions given. It is
closed because if f1, f2 are even and c1, c2 are scalars then we have this.
(c1f1 + c2f2) (−x) = c1 f1(−x) + c2 f2(−x) = c1 f1(x) + c2 f2(x) = (c1f1 + c2f2) (x)
(b) This is also a subspace; the check is similar to the prior one.
Two.I.2.29
It can be improper. If ⃗v = ⃗0 then this is a trivial subspace. At the opposite extreme, if
the vector space is R1 and ⃗v ̸= ⃗0 then the subspace is all of R1.
Two.I.2.30
No, such a set is not closed. For one thing, it does not contain the zero vector.
Two.I.2.31
No. The only subspaces of R1 are the space itself and its trivial subspace. Any subspace S
of R that contains a nonzero member ⃗v must contain the set of all of its scalar multiples {r · ⃗v
¯¯ r ∈R}.
But this set is all of R.

Answers to Exercises
45
Two.I.2.32
Item (1) is checked in the text.
Item (2) has ﬁve conditions. First, for closure, if c ∈R and ⃗s ∈S then c ·⃗s ∈S as c ·⃗s = c ·⃗s + 0 ·⃗0.
Second, because the operations in S are inherited from V , for c, d ∈R and ⃗s ∈S, the scalar product
(c + d) · ⃗s in S equals the product (c + d) · ⃗s in V , and that equals c · ⃗s + d · ⃗s in V , which equals
c · ⃗s + d · ⃗s in S.
The check for the third, fourth, and ﬁfth conditions are similar to the second conditions’s check
just given.
Two.I.2.33
An exercise in the prior subsection shows that every vector space has only one zero vector
(that is, there is only one vector that is the additive identity element of the space). But a trivial space
has only one element and that element must be this (unique) zero vector.
Two.I.2.34
As the hint suggests, the basic reason is the Linear Combination Lemma from the ﬁrst
chapter. For the full proof, we will show mutual containment between the two sets.
The ﬁrst containment [[S]] ⊇[S] is an instance of the more general, and obvious, fact that for any
subset T of a vector space, [T] ⊇T.
For the other containment, that [[S]] ⊆[S], take m vectors from [S], namely c1,1⃗s1,1+· · ·+c1,n1⃗s1,n1,
. . . , c1,m⃗s1,m + · · · + c1,nm⃗s1,nm, and note that any linear combination of those
r1(c1,1⃗s1,1 + · · · + c1,n1⃗s1,n1) + · · · + rm(c1,m⃗s1,m + · · · + c1,nm⃗s1,nm)
is a linear combination of elements of S
= (r1c1,1)⃗s1,1 + · · · + (r1c1,n1)⃗s1,n1 + · · · + (rmc1,m)⃗s1,m + · · · + (rmc1,nm)⃗s1,nm
and so is in [S]. That is, simply recall that a linear combination of linear combinations (of members
of S) is a linear combination (again of members of S).
Two.I.2.35
(a) It is not a subspace because these are not the inherited operations. For one thing,
in this space,
0 ·


x
y
z

=


1
0
0


while this does not, of course, hold in R3.
(b) We can combine the argument showing closure under addition with the argument showing closure
under scalar multiplication into one single argument showing closure under linear combinations of
two vectors. If r1, r2, x1, x2, y1, y2, z1, z2 are in R then
r1


x1
y1
z1

+ r2


x2
y2
z2

=


r1x1 −r1 + 1
r1y1
r1z1

+


r2x2 −r2 + 1
r2y2
r2z2

=


r1x1 −r1 + r2x2 −r2 + 1
r1y1 + r2y2
r1z1 + r2z2


(note that the deﬁnition of addition in this space is that the ﬁrst components combine as (r1x1−r1+
1)+(r2x2−r2+1)−1, so the ﬁrst component of the last vector does not say ‘+2’). Adding the three
components of the last vector gives r1(x1−1+y1+z1)+r2(x2−1+y2+z2)+1 = r1·0+r2·0+1 = 1.
Most of the other checks of the conditions are easy (although the oddness of the operations keeps
them from being routine). Commutativity of addition goes like this.


x1
y1
z1

+


x2
y2
z2

=


x1 + x2 −1
y1 + y2
z1 + z2

=


x2 + x1 −1
y2 + y1
z2 + z1

=


x2
y2
z2

+


x1
y1
z1


Associativity of addition has
(


x1
y1
z1

+


x2
y2
z2

) +


x3
y3
z3

=


(x1 + x2 −1) + x3 −1
(y1 + y2) + y3
(z1 + z2) + z3


while


x1
y1
z1

+ (


x2
y2
z2

+


x3
y3
z3

) =


x1 + (x2 + x3 −1) −1
y1 + (y2 + y3)
z1 + (z2 + z3)


and they are equal. The identity element with respect to this addition operation works this way


x
y
z

+


1
0
0

=


x + 1 −1
y + 0
z + 0

=


x
y
z



46
Linear Algebra, by Hefferon
and the additive inverse is similar.


x
y
z

+


−x + 2
−y
−z

=


x + (−x + 2) −1
y −y
z −z

=


1
0
0


The conditions on scalar multiplication are also easy. For the ﬁrst condition,
(r + s)


x
y
z

=


(r + s)x −(r + s) + 1
(r + s)y
(r + s)z


while
r


x
y
z

+ s


x
y
z

=


rx −r + 1
ry
rz

+


sx −s + 1
sy
sz

=


(rx −r + 1) + (sx −s + 1) −1
ry + sy
rz + sz


and the two are equal. The second condition compares
r · (


x1
y1
z1

+


x2
y2
z2

) = r ·


x1 + x2 −1
y1 + y2
z1 + z2

=


r(x1 + x2 −1) −r + 1
r(y1 + y2)
r(z1 + z2)


with
r


x1
y1
z1

+ r


x2
y2
z2

=


rx1 −r + 1
ry1
rz1

+


rx2 −r + 1
ry2
rz2

=


(rx1 −r + 1) + (rx2 −r + 1) −1
ry1 + ry2
rz1 + rz2


and they are equal. For the third condition,
(rs)


x
y
z

=


rsx −rs + 1
rsy
rsz


while
r(s


x
y
z

) = r(


sx −s + 1
sy
sz

) =


r(sx −s + 1) −r + 1
rsy
rsz


and the two are equal. For scalar multiplication by 1 we have this.
1 ·


x
y
z

=


1x −1 + 1
1y
1z

=


x
y
z


Thus all the conditions on a vector space are met by these two operations.
Remark. A way to understand this vector space is to think of it as the plane in R3
P = {


x
y
z

¯¯ x + y + z = 0}
displaced away from the origin by 1 along the x-axis. Then addition becomes: to add two members
of this space,


x1
y1
z1

,


x2
y2
z2


(such that x1 + y1 + z1 = 1 and x2 + y2 + z2 = 1) move them back by 1 to place them in P and add
as usual,


x1 −1
y1
z1

+


x2 −1
y2
z2

=


x1 + x2 −2
y1 + y2
z1 + z2


(in P)
and then move the result back out by 1 along the x-axis.


x1 + x2 −1
y1 + y2
z1 + z2

.
Scalar multiplication is similar.

Answers to Exercises
47
(c) For the subspace to be closed under the inherited scalar multiplication, where ⃗v is a member of
that subspace,
0 · ⃗v =


0
0
0


must also be a member.
The converse does not hold. Here is a subset of R3 that contains the origin
{


0
0
0

,


1
0
0

}
(this subset has only two elements) but is not a subspace.
Two.I.2.36
(a) (⃗v1 + ⃗v2 + ⃗v3) −(⃗v1 + ⃗v2) = ⃗v3
(b) (⃗v1 + ⃗v2) −(⃗v1) = ⃗v2
(c) Surely, ⃗v1.
(d) Taking the one-long sum and subtracting gives (⃗v1) −⃗v1 = ⃗0.
Two.I.2.37
Yes; any space is a subspace of itself, so each space contains the other.
Two.I.2.38
(a) The union of the x-axis and the y-axis in R2 is one.
(b) The set of integers, as a subset of R1, is one.
(c) The subset {⃗v} of R2 is one, where ⃗v is any nonzero vector.
Two.I.2.39
Because vector space addition is commutative, a reordering of summands leaves a linear
combination unchanged.
Two.I.2.40
We always consider that span in the context of an enclosing space.
Two.I.2.41
It is both ‘if’ and ‘only if’.
For ‘if’, let S be a subset of a vector space V and assume ⃗v ∈S satisﬁes ⃗v = c1⃗s1 + · · · + cn⃗sn
where c1, . . . , cn are scalars and ⃗s1, . . . ,⃗sn ∈S. We must show that [S ∪{⃗v}] = [S].
Containment one way, [S] ⊆[S ∪{⃗v}] is obvious. For the other direction, [S ∪{⃗v}] ⊆[S], note
that if a vector is in the set on the left then it has the form d0⃗v + d1⃗t1 + · · · + dm⃗tm where the d’s are
scalars and the ⃗t ’s are in S. Rewrite that as d0(c1⃗s1 + · · · + cn⃗sn) + d1⃗t1 + · · · + dm⃗tm and note that
the result is a member of the span of S.
The ‘only if’ is clearly true — adding ⃗v enlarges the span to include at least ⃗v.
Two.I.2.42
(a) Always.
Assume that A, B are subspaces of V . Note that their intersection is not empty as both contain
the zero vector. If ⃗w,⃗s ∈A ∩B and r, s are scalars then r⃗v + s⃗w ∈A because each vector is in A
and so a linear combination is in A, and r⃗v + s⃗w ∈B for the same reason. Thus the intersection is
closed. Now Lemma 2.9 applies.
(b) Sometimes (more precisely, only if A ⊆B or B ⊆A).
To see the answer is not ‘always’, take V to be R3, take A to be the x-axis, and B to be the
y-axis. Note that
µ
1
0
¶
∈A and
µ
0
1
¶
∈B
but
µ
1
0
¶
+
µ
0
1
¶
̸∈A ∪B
as the sum is in neither A nor B.
The answer is not ‘never’ because if A ⊆B or B ⊆A then clearly A ∪B is a subspace.
To show that A∪B is a subspace only if one subspace contains the other, we assume that A ̸⊆B
and B ̸⊆A and prove that the union is not a subspace. The assumption that A is not a subset
of B means that there is an ⃗a ∈A with ⃗a ̸∈B. The other assumption gives a ⃗b ∈B with ⃗b ̸∈A.
Consider ⃗a +⃗b. Note that sum is not an element of A or else (⃗a +⃗b) −⃗a would be in A, which it is
not. Similarly the sum is not an element of B. Hence the sum is not an element of A ∪B, and so
the union is not a subspace.
(c) Never. As A is a subspace, it contains the zero vector, and therefore the set that is A’s complement
does not. Without the zero vector, the complement cannot be a vector space.
Two.I.2.43
The span of a set does not depend on the enclosing space. A linear combination of vectors
from S gives the same sum whether we regard the operations as those of W or as those of V , because
the operations of W are inherited from V .

48
Linear Algebra, by Hefferon
Two.I.2.44
It is; apply Lemma 2.9. (You must consider the following. Suppose B is a subspace of a
vector space V and suppose A ⊆B ⊆V is a subspace. From which space does A inherit its operations?
The answer is that it doesn’t matter — A will inherit the same operations in either case.)
Two.I.2.45
(a) Always; if S ⊆T then a linear combination of elements of S is also a linear combi-
nation of elements of T.
(b) Sometimes (more precisely, if and only if S ⊆T or T ⊆S).
The answer is not ‘always’ as is shown by this example from R3
S = {


1
0
0

,


0
1
0

},
T = {


1
0
0

,


0
0
1

}
because of this.


1
1
1

∈[S ∪T]


1
1
1

̸∈[S] ∪[T]
The answer is not ‘never’ because if either set contains the other then equality is clear. We
can characterize equality as happening only when either set contains the other by assuming S ̸⊆T
(implying the existence of a vector ⃗s ∈S with ⃗s ̸∈T) and T ̸⊆S (giving a ⃗t ∈T with ⃗t ̸∈S), noting
⃗s + ⃗t ∈[S ∪T], and showing that ⃗s + ⃗t ̸∈[S] ∪[T].
(c) Sometimes.
Clearly [S ∩T] ⊆[S]∩[T] because any linear combination of vectors from S ∩T is a combination
of vectors from S and also a combination of vectors from T.
Containment the other way does not always hold. For instance, in R2, take
S = {
µ
1
0
¶
,
µ
0
1
¶
},
T = {
µ
2
0
¶
}
so that [S] ∩[T] is the x-axis but [S ∩T] is the trivial subspace.
Characterizing exactly when equality holds is tough. Clearly equality holds if either set contains
the other, but that is not ‘only if’ by this example in R3.
S = {


1
0
0

,


0
1
0

},
T = {


1
0
0

,


0
0
1

}
(d) Never, as the span of the complement is a subspace, while the complement of the span is not (it
does not contain the zero vector).
Two.I.2.46
Call the subset S. By Lemma 2.9, we need to check that [S] is closed under linear combi-
nations. If c1⃗s1 + · · · + cn⃗sn, cn+1⃗sn+1 + · · · + cm⃗sm ∈[S] then for any p, r ∈R we have
p · (c1⃗s1 + · · · + cn⃗sn) + r · (cn+1⃗sn+1 + · · · + cm⃗sm) = pc1⃗s1 + · · · + pcn⃗sn + rcn+1⃗sn+1 + · · · + rcm⃗sm
which is an element of [S]. (Remark. If the set S is empty, then that ‘if . . . then . . . ’ statement is
vacuously true.)
Two.I.2.47
For this to happen, one of the conditions giving the sensibleness of the addition and scalar
multiplication operations must be violated. Consider R2 with these operations.
µx1
y1
¶
+
µx2
y2
¶
=
µ0
0
¶
r
µx
y
¶
=
µ0
0
¶
The set R2 is closed under these operations. But it is not a vector space.
1 ·
µ
1
1
¶
̸=
µ
1
1
¶
Subsection Two.II.1: Deﬁnition and Examples
Two.II.1.18
For each of these, when the subset is independent it must be proved, and when the subset
is dependent an example of a dependence must be given.

Answers to Exercises
49
(a) It is dependent. Considering
c1


1
−3
5

+ c2


2
2
4

+ c3


4
−4
14

=


0
0
0


gives rise to this linear system.
c1 + 2c2 + 4c3 = 0
−3c1 + 2c2 −4c3 = 0
5c1 + 4c2 + 14c3 = 0
Gauss’ method


1
2
4
0
−3
2
−4
0
5
4
14
0


3ρ1+ρ2
−→
−5ρ1+ρ3
(3/4)ρ2+ρ3
−→


1
2
4
0
0
8
8
0
0
0
0
0


yields a free variable, so there are inﬁnitely many solutions. For an example of a particular depen-
dence we can set c3 to be, say, 1. Then we get c2 = −1 and c1 = −2.
(b) It is dependent. The linear system that arises here


1
2
3
0
7
7
7
0
7
7
7
0

−7ρ1+ρ2
−→
−7ρ1+ρ3
−ρ2+ρ3
−→


1
2
3
0
0
−7
−14
0
0
0
0
0


has inﬁnitely many solutions.
We can get a particular solution by taking c3 to be, say, 1, and
back-substituting to get the resulting c2 and c1.
(c) It is linearly independent. The system


0
1
0
0
0
0
−1
4
0

ρ1↔ρ2
−→
ρ3↔ρ1
−→


−1
4
0
0
1
0
0
0
0


has only the solution c1 = 0 and c2 = 0. (We could also have gotten the answer by inspection — the
second vector is obviously not a multiple of the ﬁrst, and vice versa.)
(d) It is linearly dependent. The linear system


9
2
3
12
0
9
0
5
12
0
0
1
−4
−1
0


has more unknowns than equations, and so Gauss’ method must end with at least one variable free
(there can’t be a contradictory equation because the system is homogeneous, and so has at least the
solution of all zeroes). To exhibit a combination, we can do the reduction
−ρ1+ρ2
−→
(1/2)ρ2+ρ3
−→


9
2
3
12
0
0
−2
2
0
0
0
0
−3
−1
0


and take, say, c4 = 1. Then we have that c3 = −1/3, c2 = −1/3, and c1 = −31/27.
Two.II.1.19
In the cases of independence, that must be proved. Otherwise, a speciﬁc dependence must
be produced. (Of course, dependences other than the ones exhibited here are possible.)
(a) This set is independent. Setting up the relation c1(3−x+9x2)+c2(5−6x+3x2)+c3(1+1x−5x2) =
0 + 0x + 0x2 gives a linear system


3
5
1
0
−1
−6
1
0
9
3
−5
0

(1/3)ρ1+ρ2
−→
−3ρ1+ρ3
3ρ2
−→
−(12/13)ρ2+ρ3
−→


3
5
1
0
0
−13
4
0
0
0
−128/13
0


with only one solution: c1 = 0, c2 = 0, and c3 = 0.
(b) This set is independent. We can see this by inspection, straight from the deﬁnition of linear
independence. Obviously neither is a multiple of the other.
(c) This set is linearly independent. The linear system reduces in this way


2
3
4
0
1
−1
0
0
7
2
−3
0

−(1/2)ρ1+ρ2
−→
−(7/2)ρ1+ρ3
−(17/5)ρ2+ρ3
−→


2
3
4
0
0
−5/2
−2
0
0
0
−51/5
0


to show that there is only the solution c1 = 0, c2 = 0, and c3 = 0.

50
Linear Algebra, by Hefferon
(d) This set is linearly dependent. The linear system


8
0
2
8
0
3
1
2
−2
0
3
2
2
5
0


must, after reduction, end with at least one variable free (there are more variables than equations,
and there is no possibility of a contradictory equation because the system is homogeneous). We can
take the free variables as parameters to describe the solution set. We can then set the parameter to
a nonzero value to get a nontrivial linear relation.
Two.II.1.20
Let Z be the zero function Z(x) = 0, which is the additive identity in the vector space
under discussion.
(a) This set is linearly independent. Consider c1 · f(x) + c2 · g(x) = Z(x). Plugging in x = 1 and
x = 2 gives a linear system
c1 · 1 +
c2 · 1 = 0
c1 · 2 + c2 · (1/2) = 0
with the unique solution c1 = 0, c2 = 0.
(b) This set is linearly independent. Consider c1 · f(x) + c2 · g(x) = Z(x) and plug in x = 0 and
x = π/2 to get
c1 · 1 + c2 · 0 = 0
c1 · 0 + c2 · 1 = 0
which obviously gives that c1 = 0, c2 = 0.
(c) This set is also linearly independent. Considering c1 · f(x) + c2 · g(x) = Z(x) and plugging in
x = 1 and x = e
c1 · e + c2 · 0 = 0
c1 · ee + c2 · 1 = 0
gives that c1 = 0 and c2 = 0.
Two.II.1.21
In each case, that the set is independent must be proved, and that it is dependent must
be shown by exihibiting a speciﬁc dependence.
(a) This set is dependent. The familiar relation sin2(x)+cos2(x) = 1 shows that 2 = c1 ·(4 sin2(x))+
c2 · (cos2(x)) is satisﬁed by c1 = 1/2 and c2 = 2.
(b) This set is independent. Consider the relationship c1 · 1 + c2 · sin(x) + c3 · sin(2x) = 0 (that ‘0’
is the zero function). Taking x = 0, x = π/2 and x = π/4 gives this system.
c1
= 0
c1 +
c2
= 0
c1 + (
√
2/2)c2 + c3 = 0
whose only solution is c1 = 0, c2 = 0, and c3 = 0.
(c) By inspection, this set is independent. Any dependence cos(x) = c · x is not possible since the
cosine function is not a multiple of the identity function (we are applying Corollary 1.17).
(d) By inspection, we spot that there is a dependence. Because (1 + x)2 = x2 + 2x + 1, we get that
c1 · (1 + x)2 + c2 · (x2 + 2x) = 3 is satisﬁed by c1 = 3 and c2 = −3.
(e) This set is dependent. The easiest way to see that is to recall the triginometric relationship
cos2(x) −sin2(x) = cos(2x).
(Remark.
A person who doesn’t recall this, and tries some x’s,
simply never gets a system leading to a unique solution, and never gets to conclude that the set is
independent. Of course, this person might wonder if they simply never tried the right set of x’s, but
a few tries will lead most people to look instead for a dependence.)
(f) This set is dependent, because it contains the zero object in the vector space, the zero polynomial.
Two.II.1.22
No, that equation is not a linear relationship. In fact this set is independent, as the
system arising from taking x to be 0, π/6 and π/4 shows.
Two.II.1.23
To emphasize that the equation 1 · ⃗s + (−1) · ⃗s = ⃗0 does not make the set dependent.
Two.II.1.24
We have already showed this: the Linear Combination Lemma and its corollary state
that in an echelon form matrix, no nonzero row is a linear combination of the others.
Two.II.1.25
(a) Assume that the set {⃗u,⃗v, ⃗w} is linearly independent, so that any relationship d0⃗u+
d1⃗v + d2 ⃗w = ⃗0 leads to the conclusion that d0 = 0, d1 = 0, and d2 = 0.
Consider the relationship c1(⃗u) + c2(⃗u + ⃗v) + c3(⃗u + ⃗v + ⃗w) = ⃗0. Rewrite it to get (c1 + c2 +
c3)⃗u + (c2 + c3)⃗v + (c3)⃗w = ⃗0. Taking d0 to be c1 + c2 + c3, taking d1 to be c2 + c3, and taking d2

Answers to Exercises
51
to be c3 we have this system.
c1 + c2 + c3 = 0
c2 + c3 = 0
c3 = 0
Conclusion: the c’s are all zero, and so the set is linearly independent.
(b) The second set is dependent
1 · (⃗u −⃗v) + 1 · (⃗v −⃗w) + 1 · (⃗w −⃗u) = ⃗0
whether or not the ﬁrst set is independent.
Two.II.1.26
(a) A singleton set {⃗v} is linearly independent if and only if ⃗v ̸= ⃗0. For the ‘if’ direction,
with ⃗v ̸= ⃗0, we can apply Lemma 1.4 by considering the relationship c · ⃗v = ⃗0 and noting that the
only solution is the trivial one: c = 0. For the ‘only if’ direction, just recall that Example 1.11 shows
that {⃗0} is linearly dependent, and so if the set {⃗v} is linearly independent then ⃗v ̸= ⃗0.
(Remark. Another answer is to say that this is the special case of Lemma 1.16 where S = ∅.)
(b) A set with two elements is linearly independent if and only if neither member is a multiple of the
other (note that if one is the zero vector then it is a multiple of the other, so this case is covered).
This is an equivalent statement: a set is linearly dependent if and only if one element is a multiple
of the other.
The proof is easy.
A set {⃗v1,⃗v2} is linearly dependent if and only if there is a relationship
c1⃗v1 + c2⃗v2 = ⃗0 with either c1 ̸= 0 or c2 ̸= 0 (or both). That holds if and only if ⃗v1 = (−c2/c1)⃗v2 or
⃗v2 = (−c1/c2)⃗v1 (or both).
Two.II.1.27
This set is linearly dependent set because it contains the zero vector.
Two.II.1.28
The ‘if’ half is given by Lemma 1.14. The converse (the ‘only if’ statement) does not
hold. An example is to consider the vector space R2 and these vectors.
⃗x =
µ1
0
¶
,
⃗y =
µ0
1
¶
,
⃗z =
µ1
1
¶
Two.II.1.29
(a) The linear system arising from
c1


1
1
0

+ c2


−1
2
0

=


0
0
0


has the unique solution c1 = 0 and c2 = 0.
(b) The linear system arising from
c1


1
1
0

+ c2


−1
2
0

=


3
2
0


has the unique solution c1 = 8/3 and c2 = −1/3.
(c) Suppose that S is linearly independent. Suppose that we have both ⃗v = c1⃗s1 + · · · + cn⃗sn and
⃗v = d1⃗t1 + · · · + dm⃗tm (where the vectors are members of S). Now,
c1⃗s1 + · · · + cn⃗sn = ⃗v = d1⃗t1 + · · · + dm⃗tm
can be rewritten in this way.
c1⃗s1 + · · · + cn⃗sn −d1⃗t1 −· · · −dm⃗tm = ⃗0
Possibly some of the ⃗s ’s equal some of the ⃗t ’s; we can combine the associated coeﬃcients (i.e., if
⃗si = ⃗tj then · · · + ci⃗si + · · · −dj⃗tj −· · · can be rewritten as · · · + (ci −dj)⃗si + · · · ). That equation
is a linear relationship among distinct (after the combining is done) members of the set S. We’ve
assumed that S is linearly independent, so all of the coeﬃcients are zero. If i is such that ⃗si does
not equal any ⃗tj then ci is zero. If j is such that ⃗tj does not equal any ⃗si then dj is zero. In the
ﬁnal case, we have that ci −dj = 0 and so ci = dj.
Therefore, the original two sums are the same, except perhaps for some 0 · ⃗si or 0 ·⃗tj terms that
we can neglect.
(d) This set is not linearly independent:
S = {
µ
1
0
¶
,
µ
2
0
¶
} ⊂R2
and these two linear combinations give the same result
µ
0
0
¶
= 2 ·
µ
1
0
¶
−1 ·
µ
2
0
¶
= 4 ·
µ
1
0
¶
−2 ·
µ
2
0
¶

52
Linear Algebra, by Hefferon
Thus, a linearly dependent set might have indistinct sums.
In fact, this stronger statement holds: if a set is linearly dependent then it must have the property
that there are two distinct linear combinations that sum to the same vector. Brieﬂy, where c1⃗s1 +
· · · + cn⃗sn = ⃗0 then multiplying both sides of the relationship by two gives another relationship. If
the ﬁrst relationship is nontrivial then the second is also.
Two.II.1.30
In this ‘if and only if’ statement, the ‘if’ half is clear — if the polynomial is the zero
polynomial then the function that arises from the action of the polynomial must be the zero function
x 7→0. For ‘only if’ we write p(x) = cnxn + · · · + c0. Plugging in zero p(0) = 0 gives that c0 = 0.
Taking the derivative and plugging in zero p′(0) = 0 gives that c1 = 0. Similarly we get that each ci
is zero, and p is the zero polynomial.
Two.II.1.31
The work in this section suggests that an n-dimensional non-degenerate linear surface
should be deﬁned as the span of a linearly independent set of n vectors.
Two.II.1.32
(a) For any a1,1, . . . , a2,4,
c1
µ
a1,1
a2,1
¶
+ c2
µ
a1,2
a2,2
¶
+ c3
µ
a1,3
a2,3
¶
+ c4
µ
a1,4
a2,4
¶
=
µ
0
0
¶
yields a linear system
a1,1c1 + a1,2c2 + a1,3c3 + a1,4c4 = 0
a2,1c1 + a2,2c2 + a2,3c3 + a2,4c4 = 0
that has inﬁnitely many solutions (Gauss’ method leaves at least two variables free). Hence there
are nontrivial linear relationships among the given members of R2.
(b) Any set ﬁve vectors is a superset of a set of four vectors, and so is linearly dependent.
With three vectors from R2, the argument from the prior item still applies, with the slight change
that Gauss’ method now only leaves at least one variable free (but that still gives inﬁntely many
solutions).
(c) The prior item shows that no three-element subset of R2 is independent. We know that there
are two-element subsets of R2 that are independent — one is
{
µ
1
0
¶
,
µ
0
1
¶
}
and so the answer is two.
Two.II.1.33
Yes; here is one.
{


1
0
0

,


0
1
0

,


0
0
1

,


1
1
1

}
Two.II.1.34
Yes. The two improper subsets, the entire set and the empty subset, serve as examples.
Two.II.1.35
In R4 the biggest linearly independent set has four vectors. There are many examples of
such sets, this is one.
{




1
0
0
0



,




0
1
0
0



,




0
0
1
0



,




0
0
0
1



}
To see that no set with ﬁve or more vectors can be independent, set up
c1




a1,1
a2,1
a3,1
a4,1



+ c2




a1,2
a2,2
a3,2
a4,2



+ c3




a1,3
a2,3
a3,3
a4,3



+ c4




a1,4
a2,4
a3,4
a4,4



+ c5




a1,5
a2,5
a3,5
a4,5



=




0
0
0
0




and note that the resulting linear system
a1,1c1 + a1,2c2 + a1,3c3 + a1,4c4 + a1,5c5 = 0
a2,1c1 + a2,2c2 + a2,3c3 + a2,4c4 + a2,5c5 = 0
a3,1c1 + a3,2c2 + a3,3c3 + a3,4c4 + a3,5c5 = 0
a4,1c1 + a4,2c2 + a4,3c3 + a4,4c4 + a4,5c5 = 0
has four equations and ﬁve unknowns, so Gauss’ method must end with at least one c variable free,
so there are inﬁnitely many solutions, and so the above linear relationship among the four-tall vectors
has more solutions than just the trivial solution.
The smallest linearly independent set is the empty set.
The biggest linearly dependent set is R4. The smallest is {⃗0}.

Answers to Exercises
53
Two.II.1.36
(a) The intersection of two linearly independent sets S∩T must be linearly independent
as it is a subset of the linearly independent set S (as well as the linearly independent set T also, of
course).
(b) The complement of a linearly independent set is linearly dependent as it contains the zero vector.
(c) We must produce an example. One, in R2, is
S = {
µ
1
0
¶
}
and
T = {
µ
2
0
¶
}
since the linear dependence of S1 ∪S2 is easily seen.
(d) The union of two linearly independent sets S ∪T is linearly independent if and only if their
spans have a trivial intersection [S] ∩[T] = {⃗0}. To prove that, assume that S and T are linearly
independent subsets of some vector space.
For the ‘only if’ direction, assume that the intersection of the spans is trivial [S] ∩[T] = {⃗0}.
Consider the set S ∪T. Any linear relationship c1⃗s1 + · · · + cn⃗sn + d1⃗t1 + · · · + dm⃗tm = ⃗0 gives
c1⃗s1 + · · · + cn⃗sn = −d1⃗t1 −· · · −dm⃗tm. The left side of that equation sums to a vector in [S], and
the right side is a vector in [T]. Therefore, since the intersection of the spans is trivial, both sides
equal the zero vector. Because S is linearly independent, all of the c’s are zero. Because T is linearly
independent, all of the d’s are zero. Thus, the original linear relationship among members of S ∪T
only holds if all of the coeﬃcients are zero. That shows that S ∪T is linearly independent.
For the ‘if’ half we can make the same argument in reverse.
If the union S ∪T is linearly
independent, that is, if the only solution to c1⃗s1 + · · · + cn⃗sn + d1⃗t1 + · · · + dm⃗tm = ⃗0 is the trivial
solution c1 = 0, . . . , dm = 0, then any vector ⃗v in the intersection of the spans ⃗v = c1⃗s1+· · ·+cn⃗sn =
−d1⃗t1 −· · · = dm⃗tm must be the zero vector because each scalar is zero.
Two.II.1.37
(a) We do induction on the number of vectors in the ﬁnite set S.
The base case is that S has no elements. In this case S is linearly independent and there is
nothing to check — a subset of S that has the same span as S is S itself.
For the inductive step assume that the theorem is true for all sets of size n = 0, n = 1, . . . , n = k
in order to prove that it holds when S has n = k+1 elements. If the k+1-element set S = {⃗s0, . . . ,⃗sk}
is linearly independent then the theorem is trivial, so assume that it is dependent. By Corollary 1.17
there is an ⃗si that is a linear combination of other vectors in S. Deﬁne S1 = S −{⃗si} and note that
S1 has the same span as S by Lemma 1.1. The set S1 has k elements and so the inductive hypothesis
applies to give that it has a linearly independent subset with the same span. That subset of S1 is
the desired subset of S.
(b) Here is a sketch of the argument. The induction argument details have been left out.
If the ﬁnite set S is empty then there is nothing to prove. If S = {⃗0} then the empty subset will
do.
Otherwise, take some nonzero vector ⃗s1 ∈S and deﬁne S1 = {⃗s1}. If [S1] = [S] then this proof
is ﬁnished by noting that S1 is linearly independent.
If not, then there is a nonzero vector ⃗s2 ∈S −[S1] (if every ⃗s ∈S is in [S1] then [S1] = [S]).
Deﬁne S2 = S1 ∪{⃗s2}. If [S2] = [S] then this proof is ﬁnished by using Theorem 1.17 to show that
S2 is linearly independent.
Repeat the last paragraph until a set with a big enough span appears. That must eventually
happen because S is ﬁnite, and [S] will be reached at worst when every vector from S has been used.
Two.II.1.38
(a) Assuming ﬁrst that a ̸= 0,
x
µ
a
c
¶
+ y
µ
b
d
¶
=
µ
0
0
¶
gives
ax + by = 0
cx + dy = 0
−(c/a)ρ1+ρ2
−→
ax +
by = 0
(−(c/a)b + d)y = 0
which has a solution if and only if 0 ̸= −(c/a)b + d = (−cb + ad)/d (we’ve assumed in this case that
a ̸= 0, and so back substitution yields a unique solution).
The a = 0 case is also not hard — break it into the c ̸= 0 and c = 0 subcases and note that in
these cases ad −bc = 0 · d −bc.
Comment. An earlier exercise showed that a two-vector set is linearly dependent if and only if
either vector is a scalar multiple of the other. That can also be used to make the calculation.

54
Linear Algebra, by Hefferon
(b) The equation
c1


a
d
g

+ c2


b
e
h

+ c3


c
f
i

=


0
0
0


gives rise to a homogeneous linear system. We proceed by writing it in matrix form and applying
Gauss’ method.
We ﬁrst reduce the matrix to upper-triangular. Assume that a ̸= 0.
(1/a)ρ1
−→


1
b/a
c/a
0
d
e
f
0
g
h
i
0


−dρ1+ρ2
−→
−gρ1+ρ3


1
b/a
c/a
0
0
(ae −bd)/a
(af −cd)/a
0
0
(ah −bg)/a
(ai −cg)/a
0


(a/(ae−bd))ρ2
−→


1
b/a
c/a
0
0
1
(af −cd)/(ae −bd)
0
0
(ah −bg)/a
(ai −cg)/a
0


(where we’ve assumed for the moment that ae−bd ̸= 0 in order to do the row reduction step). Then,
under the assumptions, we get this.
((ah−bg)/a)ρ2+ρ3
−→


1
b
a
c
a
0
0
1
af−cd
ae−bd
0
0
0
aei+bgf+cdh−hfa−idb−gec
ae−bd
0


shows that the original system is nonsingular if and only if the 3, 3 entry is nonzero. This fraction
is deﬁned because of the ae −bd ̸= 0 assumption, and it will equal zero if and only if its numerator
equals zero.
We next worry about the assumptions. First, if a ̸= 0 but ae −bd = 0 then we swap


1
b/a
c/a
0
0
0
(af −cd)/a
0
0
(ah −bg)/a
(ai −cg)/a
0


ρ2↔ρ3
−→


1
b/a
c/a
0
0
(ah −bg)/a
(ai −cg)/a
0
0
0
(af −cd)/a
0


and conclude that the system is nonsingular if and only if either ah −bg = 0 or af −cd = 0. That’s
the same as asking that their product be zero:
ahaf −ahcd −bgaf + bgcd = 0
ahaf −ahcd −bgaf + aegc = 0
a(haf −hcd −bgf + egc) = 0
(in going from the ﬁrst line to the second we’ve applied the case assumption that ae −bd = 0 by
substituting ae for bd). Since we are assuming that a ̸= 0, we have that haf −hcd −bgf + egc = 0.
With ae −bd = 0 we can rewrite this to ﬁt the form we need: in this a ̸= 0 and ae −bd = 0 case,
the given system is nonsingular when haf −hcd −bgf + egc −i(ae −bd) = 0, as required.
The remaining cases have the same character. Do the a = 0 but d ̸= 0 case and the a = 0 and
d = 0 but g ̸= 0 case by ﬁrst swapping rows and then going on as above. The a = 0, d = 0, and
g = 0 case is easy — a set with a zero vector is linearly dependent, and the formula comes out to
equal zero.
(c) It is linearly dependent if and only if either vector is a multiple of the other. That is, it is not
independent iﬀ


a
d
g

= r ·


b
e
h


or


b
e
h

= s ·


a
d
g


(or both) for some scalars r and s. Eliminating r and s in order to restate this condition only in
terms of the given letters a, b, d, e, g, h, we have that it is not independent — it is dependent — iﬀ
ae −bd = ah −gb = dh −ge.
(d) Dependence or independence is a function of the indices, so there is indeed a formula (although
at ﬁrst glance a person might think the formula involves cases: “if the ﬁrst component of the ﬁrst
vector is zero then . . . ”, this guess turns out not to be correct).
Two.II.1.39
Recall that two vectors from Rn are perpendicular if and only if their dot product is
zero.
(a) Assume that ⃗v and ⃗w are perpendicular nonzero vectors in Rn, with n > 1. With the linear
relationship c⃗v + d⃗w = ⃗0, apply ⃗v to both sides to conclude that c · ∥⃗v∥2 + d · 0 = 0. Because ⃗v ̸= ⃗0
we have that c = 0. A similar application of ⃗w shows that d = 0.

Answers to Exercises
55
(b) Two vectors in R1 are perpendicular if and only if at least one of them is zero.
We deﬁne R0 to be a trivial space, and so both ⃗v and ⃗w are the zero vector.
(c) The right generalization is to look at a set {⃗v1, . . . ,⃗vn} ⊆Rk of vectors that are mutually
orthogonal (also called pairwise perpendicular): if i ̸= j then ⃗vi is perpendicular to ⃗vj. Mimicing the
proof of the ﬁrst item above shows that such a set of nonzero vectors is linearly independent.
Two.II.1.40
(a) This check is routine.
(b) The summation is inﬁnite (has inﬁnitely many summands). The deﬁnition of linear combination
involves only ﬁnite sums.
(c) No nontrivial ﬁnite sum of members of {g, f0, f1, . . .} adds to the zero object: assume that
c0 · (1/(1 −x)) + c1 · 1 + · · · + cn · xn = 0
(any ﬁnite sum uses a highest power, here n).
Multiply both sides by 1 −x to conclude that
each coeﬃcient is zero, because a polynomial describes the zero function only when it is the zero
polynomial.
Two.II.1.41
It is both ‘if’ and ‘only if’.
Let T be a subset of the subspace S of the vector space V . The assertion that any linear relationship
c1⃗t1 + · · · + cn⃗tn = ⃗0 among members of T must be the trivial relationship c1 = 0, . . . , cn = 0 is a
statement that holds in S if and only if it holds in V , because the subspace S inherits its addition and
scalar multiplication operations from V .
Subsection Two.III.1: Basis
Two.III.1.16
By Theorem 1.12, each is a basis if and only if each vector in the space can be given in
a unique way as a linear combination of the given vectors.
(a) Yes this is a basis. The relation
c1


1
2
3

+ c2


3
2
1

+ c3


0
0
1

=


x
y
z


gives


1
3
0
x
2
2
0
y
3
1
1
z

−2ρ1+ρ2
−→
−3ρ1+ρ3
−2ρ2+ρ3
−→


1
3
0
x
0
−4
0
−2x + y
0
0
1
x −2y + z


which has the unique solution c3 = x −2y + z, c2 = x/2 −y/4, and c1 = −x/2 + 3y/4.
(b) This is not a basis. Setting it up as in the prior item
c1


1
2
3

+ c2


3
2
1

=


x
y
z


gives a linear system whose solution


1
3
x
2
2
y
3
1
z

−2ρ1+ρ2
−→
−3ρ1+ρ3
−2ρ2+ρ3
−→


1
3
x
0
−4
−2x + y
0
0
x −2y + z


is possible if and only if the three-tall vector’s components x, y, and z satisfy x −2y + z = 0. For
instance, we can ﬁnd the coeﬃcients c1 and c2 that work when x = 1, y = 1, and z = 1. However,
there are no c’s that work for x = 1, y = 1, and z = 2. Thus this is not a basis; it does not span the
space.
(c) Yes, this is a basis. Setting up the relationship leads to this reduction


0
1
2
x
2
1
5
y
−1
1
0
z

ρ1↔ρ3
−→
2ρ1+ρ2
−→
−(1/3)ρ2+ρ3
−→


−1
1
0
z
0
3
5
y + 2z
0
0
1/3
x −y/3 −2z/3


which has a unique solution for each triple of components x, y, and z.

56
Linear Algebra, by Hefferon
(d) No, this is not a basis. The reduction


0
1
1
x
2
1
3
y
−1
1
0
z

ρ1↔ρ3
−→
2ρ1+ρ2
−→
(−1/3)ρ2+ρ3
−→


−1
1
0
z
0
3
3
y + 2z
0
0
0
x −y/3 −2z/3


which does not have a solution for each triple x, y, and z. Instead, the span of the given set includes
only those three-tall vectors where x = y/3 + 2z/3.
Two.III.1.17
(a) We solve
c1
µ1
1
¶
+ c2
µ−1
1
¶
=
µ1
2
¶
with
µ
1
−1
1
1
1
2
¶
−ρ1+ρ2
−→
µ
1
−1
1
0
2
1
¶
and conclude that c2 = 1/2 and so c1 = 3/2. Thus, the representation is this.
RepB(
µ
1
2
¶
) =
µ
3/2
1/2
¶
B
(b) The relationship c1 · (1) + c2 · (1 + x) + c3 · (1 + x + x2) + c4 · (1 + x + x2 + x3) = x2 + x3 is easily
solved by eye to give that c4 = 1, c3 = 0, c2 = −1, and c1 = 0.
RepD(x2 + x3) =




0
−1
0
1




D
(c) RepE4(




0
−1
0
1



) =




0
−1
0
1




E4
Two.III.1.18
A natural basis is ⟨1, x, x2⟩. There are bases for P2 that do not contain any polynomials
of degree one or degree zero. One is ⟨1 + x + x2, x + x2, x2⟩. (Every basis has at least one polynomial
of degree two, though.)
Two.III.1.19
The reduction
µ1
−4
3
−1
0
2
−8
6
−2
0
¶
−2ρ1+ρ2
−→
µ1
−4
3
−1
0
0
0
0
0
0
¶
gives that the only condition is that x1 = 4x2 −3x3 + x4. The solution set is
{




4x2 −3x3 + x4
x2
x3
x4




¯¯ x2, x3, x4 ∈R} = {x2




4
1
0
0



+ x3




−3
0
1
0



+ x4




1
0
0
1




¯¯ x2, x3, x4 ∈R}
and so the obvious candidate for the basis is this.
⟨




4
1
0
0



,




−3
0
1
0



,




1
0
0
1



⟩
We’ve shown that this spans the space, and showing it is also linearly independent is routine.
Two.III.1.20
There are many bases. This is a natural one.
⟨
µ
1
0
0
0
¶
,
µ
0
1
0
0
¶
,
µ
0
0
1
0
¶
,
µ
0
0
0
1
¶
⟩
Two.III.1.21
For each item, many answers are possible.
(a) One way to proceed is to parametrize by expressing the a2 as a combination of the other two
a2 = 2a1 + a0. Then a2x2 + a1x + a0 is (2a1 + a0)x2 + a1x + a0 and
{(2a1 + a0)x2 + a1x + a0
¯¯ a1, a0 ∈R} = {a1 · (2x2 + x) + a0 · (x2 + 1)
¯¯ a1, a0 ∈R}
suggests ⟨2x2 +x, x2 +1⟩. This only shows that it spans, but checking that it is linearly independent
is routine.
(b) Parametrize {
¡
a
b
c
¢ ¯¯ a + b = 0} to get {
¡
−b
b
c
¢ ¯¯ b, c ∈R}, which suggests using the
sequence ⟨
¡
−1
1
0
¢
,
¡
0
0
1
¢
⟩.
We’ve shown that it spans, and checking that it is linearly
independent is easy.

Answers to Exercises
57
(c) Rewriting
{
µ
a
b
0
2b
¶ ¯¯ a, b ∈R} = {a ·
µ
1
0
0
0
¶
+ b ·
µ
0
1
0
2
¶ ¯¯ a, b ∈R}
suggests this for the basis.
⟨
µ
1
0
0
0
¶
,
µ
0
1
0
2
¶
⟩
Two.III.1.22
We will show that the second is a basis; the ﬁrst is similar. We will show this straight
from the deﬁnition of a basis, because this example appears before Theorem 1.12.
To see that it is linearly independent, we set up c1 · (cos θ −sin θ) + c2 · (2 cos θ + 3 sin θ) =
0 cos θ + 0 sin θ. Taking θ = 0 and θ = π/2 gives this system
c1 · 1 + c2 · 2 = 0
c1 · (−1) + c2 · 3 = 0
ρ1+ρ2
−→
c1 + 2c2 = 0
+ 5c2 = 0
which shows that c1 = 0 and c2 = 0.
The calculation for span is also easy; for any x, y ∈R, we have that c1 ·(cos θ −sin θ)+c2 ·(2 cos θ +
3 sin θ) = x cos θ + y sin θ gives that c2 = x/5 + y/5 and that c1 = 3x/5 −2y/5, and so the span is the
entire space.
Two.III.1.23
(a) Asking which a0 + a1x + a2x2 can be expressed as c1 · (1 + x) + c2 · (1 + 2x) gives
rise to three linear equations, describing the coeﬃcients of x2, x, and the constants.
c1 + c2 = a0
c1 + 2c2 = a1
0 = a2
Gauss’ method with back-substitution shows, provided that a2 = 0, that c2 = −a0 + a1 and
c1 = 2a0 −a1.
Thus, with a2 = 0, we can compute appropriate c1 and c2 for any a0 and a1.
So the span is the entire set of linear polynomials {a0 + a1x
¯¯ a0, a1 ∈R}. Parametrizing that set
{a0 · 1 + a1 · x
¯¯ a0, a1 ∈R} suggests a basis ⟨1, x⟩(we’ve shown that it spans; checking linear inde-
pendence is easy).
(b) With
a0 + a1x + a2x2 = c1 · (2 −2x) + c2 · (3 + 4x2) = (2c1 + 3c2) + (−2c1)x + (4c2)x2
we get this system.
2c1 + 3c2 = a0
−2c1
= a1
4c2 = a2
ρ1+ρ2
−→
(−4/3)ρ2+ρ3
−→
2c1 + 3c2 = a0
3c2 = a0 + a1
0 = (−4/3)a0 −(4/3)a1 + a2
Thus, the only quadratic polynomials a0 + a1x + a2x2 with associated c’s are the ones such that 0 =
(−4/3)a0−(4/3)a1+a2. Hence the span is {(−a1 + (3/4)a2) + a1x + a2x2 ¯¯ a1, a2 ∈R}. Parametriz-
ing gives {a1 · (−1 + x) + a2 · ((3/4) + x2)
¯¯ a1, a2 ∈R}, which suggests ⟨−1+x, (3/4)+x2⟩(checking
that it is linearly independent is routine).
Two.III.1.24
(a) The subspace is {a0 + a1x + a2x2 + a3x3 ¯¯ a0 + 7a1 + 49a2 + 343a3 = 0}. Rewrit-
ing a0 = −7a1 −49a2 −343a3 gives {(−7a1 −49a2 −343a3) + a1x + a2x2 + a3x3 ¯¯ a1, a2, a3 ∈R},
which, on breaking out the parameters, suggests ⟨−7 + x, −49 + x2, −343 + x3⟩for the basis (it is
easily veriﬁed).
(b) The given subspace is the collection of cubics p(x) = a0 +a1x+a2x2 +a3x3 such that a0 +7a1 +
49a2 + 343a3 = 0 and a0 + 5a1 + 25a2 + 125a3 = 0. Gauss’ method
a0 + 7a1 + 49a2 + 343a3 = 0
a0 + 5a1 + 25a2 + 125a3 = 0
−ρ1+ρ2
−→
a0 +
7a1 + 49a2 + 343a3 = 0
−2a1 −24a2 −218a3 = 0
gives that a1 = −12a2 −109a3 and that a0 = 35a2 + 420a3. Rewriting (35a2 + 420a3) + (−12a2 −
109a3)x + a2x2 + a3x3 as a2 · (35 −12x + x2) + a3 · (420 −109x + x3) suggests this for a basis
⟨35 −12x + x2, 420 −109x + x3⟩. The above shows that it spans the space. Checking it is linearly
independent is routine. (Comment. A worthwhile check is to verify that both polynomials in the
basis have both seven and ﬁve as roots.)
(c) Here there are three conditions on the cubics, that a0 + 7a1 + 49a2 + 343a3 = 0, that a0 + 5a1 +
25a2 + 125a3 = 0, and that a0 + 3a1 + 9a2 + 27a3 = 0. Gauss’ method
a0 + 7a1 + 49a2 + 343a3 = 0
a0 + 5a1 + 25a2 + 125a3 = 0
a0 + 3a1 + 9a2 + 27a3 = 0
−ρ1+ρ2
−→
−ρ1+ρ3
−2ρ2+ρ3
−→
a0 +
7a1 + 49a2 + 343a3 = 0
−2a1 −24a2 −218a3 = 0
8a2 + 120a3 = 0

58
Linear Algebra, by Hefferon
yields the single free variable a3, with a2 = −15a3, a1 = 71a3, and a0 = −105a3. The parametriza-
tion is this.
{(−105a3) + (71a3)x + (−15a3)x2 + (a3)x3 ¯¯ a3 ∈R} = {a3 · (−105 + 71x −15x2 + x3)
¯¯ a3 ∈R}
Therefore, a natural candidate for the basis is ⟨−105 + 71x −15x2 + x3⟩. It spans the space by
the work above. It is clearly linearly independent because it is a one-element set (with that single
element not the zero object of the space). Thus, any cubic through the three points (7, 0), (5, 0),
and (3, 0) is a multiple of this one. (Comment. As in the prior question, a worthwhile check is to
verify that plugging seven, ﬁve, and three into this polynomial yields zero each time.)
(d) This is the trivial subspace of P3. Thus, the basis is empty ⟨⟩.
Remark. The polynomial in the third item could alternatively have been derived by multiplying out
(x −7)(x −5)(x −3).
Two.III.1.25
Yes. Linear independence and span are unchanged by reordering.
Two.III.1.26
No linearly independent set contains a zero vector.
Two.III.1.27
(a) To show that it is linearly independent, note that d1(c1⃗β1)+d2(c2⃗β2)+d3(c3⃗β3) = ⃗0
gives that (d1c1)⃗β1 + (d2c2)⃗β2 + (d3c3)⃗β3 = ⃗0, which in turn implies that each dici is zero. But with
ci ̸= 0 that means that each di is zero. Showing that it spans the space is much the same; because
⟨⃗β1, ⃗β2, ⃗β3⟩is a basis, and so spans the space, we can for any ⃗v write ⃗v = d1⃗β1 + d2⃗β2 + d3⃗β3, and
then ⃗v = (d1/c1)(c1⃗β1) + (d2/c2)(c2⃗β2) + (d3/c3)(c3⃗β3).
If any of the scalars are zero then the result is not a basis, because it is not linearly independent.
(b) Showing that ⟨2⃗β1, ⃗β1 + ⃗β2, ⃗β1 + ⃗β3⟩is linearly independent is easy. To show that it spans the
space, assume that ⃗v = d1⃗β1 + d2⃗β2 + d3⃗β3. Then, we can represent the same ⃗v with respect to
⟨2⃗β1, ⃗β1 + ⃗β2, ⃗β1 + ⃗β3⟩in this way ⃗v = (1/2)(d1 −d2 −d3)(2⃗β1) + d2(⃗β1 + ⃗β2) + d3(⃗β1 + ⃗β3).
Two.III.1.28
Each forms a linearly independent set if ⃗v is ommitted. To preserve linear independence,
we must expand the span of each. That is, we must determine the span of each (leaving ⃗v out), and
then pick a ⃗v lying outside of that span. Then to ﬁnish, we must check that the result spans the entire
given space. Those checks are routine.
(a) Any vector that is not a multiple of the given one, that is, any vector that is not on the line
y = x will do here. One is ⃗v = ⃗e1.
(b) By inspection, we notice that the vector ⃗e3 is not in the span of the set of the two given vectors.
The check that the resulting set is a basis for R3 is routine.
(c) For any member of the span {c1 · (x) + c2 · (1 + x2)
¯¯ c1, c2 ∈R}, the coeﬃcient of x2 equals the
constant term. So we expand the span if we add a quadratic without this property, say, ⃗v = 1 −x2.
The check that the result is a basis for P2 is easy.
Two.III.1.29
To show that each scalar is zero, simply subtract c1⃗β1+· · ·+ck⃗βk−ck+1⃗βk+1−· · ·−cn⃗βn =
⃗0. The obvious generalization is that in any equation involving only the ⃗β’s, and in which each ⃗β appears
only once, each scalar is zero. For instance, an equation with a combination of the even-indexed basis
vectors (i.e., ⃗β2, ⃗β4, etc.) on the right and the odd-indexed basis vectors on the left also gives the
conclusion that all of the coeﬃcients are zero.
Two.III.1.30
No; no linearly independent set contains the zero vector.
Two.III.1.31
Here is a subset of R2 that is not a basis, and two diﬀerent linear combinations of its
elements that sum to the same vector.
{
µ
1
2
¶
,
µ
2
4
¶
}
2 ·
µ
1
2
¶
+ 0 ·
µ
2
4
¶
= 0 ·
µ
1
2
¶
+ 1 ·
µ
2
4
¶
Thus, when a subset is not a basis, it can be the case that its linear combinations are not unique.
But just because a subset is not a basis does not imply that its combinations must be not unique.
For instance, this set
{
µ
1
2
¶
}
does have the property that
c1 ·
µ
1
2
¶
= c2 ·
µ
1
2
¶
implies that c1 = c2. The idea here is that this subset fails to be a basis because it fails to span the
space; the proof of the theorem establishes that linear combinations are unique if and only if the subset
is linearly independent.

Answers to Exercises
59
Two.III.1.32
(a) Describing the vector space as
{
µ
a
b
b
c
¶ ¯¯ a, b, c ∈R}
suggests this for a basis.
⟨
µ
1
0
0
0
¶
,
µ
0
0
0
1
¶
,
µ
0
1
1
0
¶
⟩
Veriﬁcation is easy.
(b) This is one possible basis.
⟨


1
0
0
0
0
0
0
0
0

,


0
0
0
0
1
0
0
0
0

,


0
0
0
0
0
0
0
0
1

,


0
1
0
1
0
0
0
0
0

,


0
0
1
0
0
0
1
0
0

,


0
0
0
0
0
1
0
1
0

⟩
(c) As in the prior two questions, we can form a basis from two kinds of matrices. First are the
matrices with a single one on the diagonal and all other entries zero (there are n of those matrices).
Second are the matrices with two opposed oﬀ-diagonal entries are ones and all other entries are
zeros. (That is, all entries in M are zero except that mi,j and mj,i are one.)
Two.III.1.33
(a) Any four vectors from R3 are linearly related because the vector equation
c1


x1
y1
z1

+ c2


x2
y2
z2

+ c3


x3
y3
z3

+ c4


x4
y4
z4

=


0
0
0


gives rise to a linear system
x1c1 + x2c2 + x3c3 + x4c4 = 0
y1c1 + y2c2 + y3c3 + y4c4 = 0
z1c1 + z2c2 + z3c3 + z4c4 = 0
that is homogeneous (and so has a solution) and has four unknowns but only three equations, and
therefore has nontrivial solutions. (Of course, this argument applies to any subset of R3 with four
or more vectors.)
(b) Given x1, . . . , z2,
S = {


x1
y1
z1

,


x2
y2
z2

}
to decide which vectors


x
y
z


are in the span of S, set up
c1


x1
y1
z1

+ c2


x2
y2
z2

=


x
y
z


and row reduce the resulting system.
x1c1 + x2c2 = x
y1c1 + y2c2 = y
z1c1 + z2c2 = z
There are two variables c1 and c2 but three equations, so when Gauss’ method ﬁnishes, on the
bottom row there will be some relationship of the form 0 = m1x + m2y + m3z. Hence, vectors in
the span of the two-element set S must satisfy some restriction. Hence the span is not all of R3.
Two.III.1.34
We have (using these peculiar operations with care)
{


1 −y −z
y
z

¯¯ y, z ∈R} = {


−y + 1
y
0

+


−z + 1
0
z

¯¯ y, z ∈R} = {y ·


0
1
0

+ z ·


0
0
1

¯¯ y, z ∈R}
and so a natural candidate for a basis is this.
⟨


0
1
0

,


0
0
1

⟩

60
Linear Algebra, by Hefferon
To check linear independence we set up
c1


0
1
0

+ c2


0
0
1

=


1
0
0


(the vector on the right is the zero object in this space). That yields the linear system
(−c1 + 1) + (−c2 + 1) −1 = 1
c1
= 0
c2
= 0
with only the solution c1 = 0 and c2 = 0. Checking the span is similar.
Subsection Two.III.2: Dimension
Two.III.2.14
One basis is ⟨1, x, x2⟩, and so the dimension is three.
Two.III.2.15
The solution set is
{




4x2 −3x3 + x4
x2
x3
x4




¯¯ x2, x3, x4 ∈R}
so a natural basis is this
⟨




4
1
0
0



,




−3
0
1
0



,




1
0
0
1



⟩
(checking linear independence is easy). Thus the dimension is three.
Two.III.2.16
For this space
{
µ
a
b
c
d
¶ ¯¯ a, b, c, d ∈R} = {a ·
µ
1
0
0
0
¶
+ · · · + d ·
µ
0
0
0
1
¶ ¯¯ a, b, c, d ∈R}
this is a natural basis.
⟨
µ
1
0
0
0
¶
,
µ
0
1
0
0
¶
,
µ
0
0
1
0
¶
,
µ
0
0
0
1
¶
⟩
The dimension is four.
Two.III.2.17
(a) As in the prior exercise, the space M2×2 of matrices without restriction has this
basis
⟨
µ
1
0
0
0
¶
,
µ
0
1
0
0
¶
,
µ
0
0
1
0
¶
,
µ
0
0
0
1
¶
⟩
and so the dimension is four.
(b) For this space
{
µ
a
b
c
d
¶ ¯¯ a = b −2c and d ∈R} = {b ·
µ
1
1
0
0
¶
+ c ·
µ
−2
0
1
0
¶
+ d ·
µ
0
0
0
1
¶ ¯¯ b, c, d ∈R}
this is a natural basis.
⟨
µ
1
1
0
0
¶
,
µ
−2
0
1
0
¶
,
µ
0
0
0
1
¶
⟩
The dimension is three.
(c) Gauss’ method applied to the two-equation linear system gives that c = 0 and that a = −b.
Thus, we have this description
{
µ
−b
b
0
d
¶ ¯¯ b, d ∈R} = {b ·
µ
−1
1
0
0
¶
+ d ·
µ
0
0
0
1
¶ ¯¯ b, d ∈R}
and so this is a natural basis.
⟨
µ
−1
1
0
0
¶
,
µ
0
0
0
1
¶
⟩
The dimension is two.

Answers to Exercises
61
Two.III.2.18
The bases for these spaces are developed in the answer set of the prior subsection.
(a) One basis is ⟨−7 + x, −49 + x2, −343 + x3⟩. The dimension is three.
(b) One basis is ⟨35 −12x + x2, 420 −109x + x3⟩so the dimension is two.
(c) A basis is {−105 + 71x −15x2 + x3}. The dimension is one.
(d) This is the trivial subspace of P3 and so the basis is empty. The dimension is zero.
Two.III.2.19
First recall that cos 2θ = cos2 θ −sin2 θ, and so deletion of cos 2θ from this set leaves
the span unchanged. What’s left, the set {cos2 θ, sin2 θ, sin 2θ}, is linearly independent (consider the
relationship c1 cos2 θ + c2 sin2 θ + c3 sin 2θ = Z(θ) where Z is the zero function, and then take θ = 0,
θ = π/4, and θ = π/2 to conclude that each c is zero). It is therefore a basis for its span. That shows
that the span is a dimension three vector space.
Two.III.2.20
Here is a basis
⟨(1 + 0i, 0 + 0i, . . . , 0 + 0i), (0 + 1i, 0 + 0i, . . . , 0 + 0i), (0 + 0i, 1 + 0i, . . . , 0 + 0i), . . .⟩
and so the dimension is 2 · 47 = 94.
Two.III.2.21
A basis is
⟨


1
0
0
0
0
0
0
0
0
0
0
0
0
0
0

,


0
1
0
0
0
0
0
0
0
0
0
0
0
0
0

, . . . ,


0
0
0
0
0
0
0
0
0
0
0
0
0
0
1

⟩
and thus the dimension is 3 · 5 = 15.
Two.III.2.22
In a four-dimensional space a set of four vectors is linearly independent if and only if
it spans the space. The form of these vectors makes linear independence easy to show (look at the
equation of fourth components, then at the equation of third components, etc.).
Two.III.2.23
(a) The diagram for P2 has four levels. The top level has the only three-dimensional
subspace, P2 itself.
The next level contains the two-dimensional subspaces (not just the linear
polynomials; any two-dimensional subspace, like those polynomials of the form ax2 +b). Below that
are the one-dimensional subspaces. Finally, of course, is the only zero-dimensional subspace, the
trivial subspace.
(b) For M2×2, the diagram has ﬁve levels, including subspaces of dimension four through zero.
Two.III.2.24
(a) One
(b) Two
(c) n
Two.III.2.25
We need only produce an inﬁnite linearly independent set. One is ⟨f1, f2, . . .⟩where
fi : R →R is
fi(x) =
(
1
if x = i
0
otherwise
the function that has value 1 only at x = i.
Two.III.2.26
Considering a function to be a set, speciﬁcally, a set of ordered pairs (x, f(x)), then the
only function with an empty domain is the empty set. Thus this is a trivial vector space, and has
dimension zero.
Two.III.2.27
Apply Corollary 2.8.
Two.III.2.28
A plane has the form {⃗p + t1⃗v1 + t2⃗v2
¯¯ t1, t2 ∈R}. (The ﬁrst chapter also calls this a
‘2-ﬂat’, and contains a discussion of why this is equivalent to the description often taken in Calculus
as the set of points (x, y, z) subject to a condition of the form ax + by + cz = d). When the plane
passes through the origin we can take the particular vector ⃗p to be ⃗0. Thus, in the language we have
developed in this chapter, a plane through the origin is the span of a set of two vectors.
Now for the statement. Asserting that the three are not coplanar is the same as asserting that no
vector lies in the span of the other two — no vector is a linear combination of the other two. That’s
simply an assertion that the three-element set is linearly independent.
By Corollary 2.12, that’s
equivalent to an assertion that the set is a basis for R3.
Two.III.2.29
Let the space V be ﬁnite dimensional. Let S be a subspace of V .
(a) The empty set is a linearly independent subset of S. By Corollary 2.10, it can be expanded to a
basis for the vector space S.
(b) Any basis for the subspace S is a linearly independent set in the superspace V . Hence it can be
expanded to a basis for the superspace, which is ﬁnite dimensional. Therefore it has only ﬁnitely
many members.

62
Linear Algebra, by Hefferon
Two.III.2.30
It ensures that we exhaust the ⃗β’s. That is, it justiﬁes the ﬁrst sentence of the last
paragraph.
Two.III.2.31
Let BU be a basis for U and let BW be a basis for W. The set BU ∪BW is linearly
dependent as it is a six member subset of the ﬁve-dimensional space R5. Thus some member of BW is
in the span of BU, and thus U ∩W is more than just the trivial space {⃗0 }.
Generalization: if U, W are subspaces of a vector space of dimension n and if dim(U)+dim(W) > n
then they have a nontrivial intersection.
Two.III.2.32
First, note that a set is a basis for some space if and only if it is linearly independent,
because in that case it is a basis for its own span.
(a) The answer to the question in the second paragraph is “yes” (implying “yes” answers for both
questions in the ﬁrst paragraph). If BU is a basis for U then BU is a linearly independent subset of
W. Apply Corollary 2.10 to expand it to a basis for W. That is the desired BW .
The answer to the question in the third paragraph is “no”, which implies a “no” answer to the
question of the fourth paragraph. Here is an example of a basis for a superspace with no sub-basis
forming a basis for a subspace: in W = R2, consider the standard basis E2. No sub-basis of E2 forms
a basis for the subspace U of R2 that is the line y = x.
(b) It is a basis (for its span) because the intersection of linearly independent sets is linearly inde-
pendent (the intersection is a subset of each of the linearly independent sets).
It is not, however, a basis for the intersection of the spaces. For instance, these are bases for R2:
B1 = ⟨
µ
1
0
¶
,
µ
0
1
¶
⟩
and
B2 = ⟨
µ
2
0
¶
,
µ
0
2
¶
⟩
and R2 ∩R2 = R2, but B1 ∩B2 is empty. All we can say is that the intersection of the bases is a
basis for a subset of the intersection of the spaces.
(c) The union of bases need not be a basis: in R2
B1 = ⟨
µ
1
0
¶
,
µ
1
1
¶
⟩
and
B2 = ⟨
µ
1
0
¶
,
µ
0
2
¶
⟩
have a union B1 ∪B2 that is not linearly independent. A necessary and suﬃcient condition for a
union of two bases to be a basis
B1 ∪B2 is linearly independent
⇐⇒
[B1 ∩B2] = [B1] ∩[B2]
it is easy enough to prove (but perhaps hard to apply).
(d) The complement of a basis cannot be a basis because it contains the zero vector.
Two.III.2.33
(a) A basis for U is a linearly independent set in W and so can be expanded via
Corollary 2.10 to a basis for W. The second basis has at least as many members as the ﬁrst.
(b) One direction is clear: if V = W then they have the same dimension. For the converse, let BU
be a basis for U. It is a linearly independent subset of W and so can be expanded to a basis for W.
If dim(U) = dim(W) then this basis for W has no more members than does BU and so equals BU.
Since U and W have the same bases, they are equal.
(c) Let W be the space of ﬁnite-degree polynomials and let U be the subspace of polynomails that
have only even-powered terms {a0 + a1x2 + a2x4 + · · · + anx2n ¯¯ a0, . . . , an ∈R}. Both spaces have
inﬁnite dimension, but U is a proper subspace.
Two.III.2.34
The possibilities for the dimension of V are 0, 1, n −1, and n.
To see this, ﬁrst consider the case when all the coordinates of ⃗v are equal.
⃗v =





z
z
...
z





Then σ(⃗v) = ⃗v for every permutation σ, so V is just the span of ⃗v, which has dimension 0 or 1 according
to whether ⃗v is ⃗0 or not.
Now suppose not all the coordinates of ⃗v are equal; let x and y with x ̸= y be among the coordinates
of ⃗v. Then we can ﬁnd permutations σ1 and σ2 such that
σ1(⃗v) =







x
y
a3
...
an







and
σ2(⃗v) =







y
x
a3
...
an








Answers to Exercises
63
for some a3, . . . , an ∈R. Therefore,
1
y −x
¡
σ1(⃗v) −σ2(⃗v)
¢
=







−1
1
0
...
0







is in V . That is, ⃗e2 −⃗e1 ∈V , where ⃗e1, ⃗e2, . . . , ⃗en is the standard basis for Rn. Similarly, ⃗e3 −⃗e2,
. . . , ⃗en −⃗e1 are all in V . It is easy to see that the vectors ⃗e2 −⃗e1, ⃗e3 −⃗e2, . . . , ⃗en −⃗e1 are linearly
independent (that is, form a linearly independent set), so dim V ≥n −1.
Finally, we can write
⃗v = x1⃗e1 + x2⃗e2 + · · · + xn⃗en
= (x1 + x2 + · · · + xn)⃗e1 + x2(⃗e2 −⃗e1) + · · · + xn(⃗en −⃗e1)
This shows that if x1 + x2 + · · · + xn = 0 then ⃗v is in the span of ⃗e2 −⃗e1, . . . , ⃗en −⃗e1 (that is, is in
the span of the set of those vectors); similarly, each σ(⃗v) will be in this span, so V will equal this span
and dim V = n −1. On the other hand, if x1 + x2 + · · · + xn ̸= 0 then the above equation shows that
⃗e1 ∈V and thus ⃗e1, . . . ,⃗en ∈V , so V = Rn and dim V = n.
Subsection Two.III.3: Vector Spaces and Linear Systems
Two.III.3.16
(a)
µ2
3
1
1
¶
(b)
µ2
1
1
3
¶
(c)


1
6
4
7
3
8


(d)
¡
0
0
0
¢
(e)
µ−1
−2
¶
Two.III.3.17
(a) Yes. To see if there are c1 and c2 such that c1 ·
¡
2
1
¢
+ c2 ·
¡
3
1
¢
=
¡
1
0
¢
we
solve
2c1 + 3c2 = 1
c1 + c2 = 0
and get c1 = −1 and c2 = 1. Thus the vector is in the row space.
(b) No. The equation c1
¡
0
1
3
¢
+ c2
¡
−1
0
1
¢
+ c3
¡
−1
2
7
¢
=
¡
1
1
1
¢
has no solution.


0
−1
−1
1
1
0
2
1
3
1
7
1

ρ1↔ρ2
−→
−3ρ1+ρ2
−→
ρ2+ρ3
−→


1
0
2
1
0
−1
−1
1
0
0
0
−1


Thus, the vector is not in the row space.
Two.III.3.18
(a) No. To see if there are c1, c2 ∈R such that
c1
µ
1
1
¶
+ c2
µ
1
1
¶
=
µ
1
3
¶
we can use Gauss’ method on the resulting linear system.
c1 + c2 = 1
c1 + c2 = 3
−ρ1+ρ2
−→
c1 + c2 = 1
0 = 2
There is no solution and so the vector is not in the column space.
(b) Yes. From this relationship
c1


1
2
1

+ c2


3
0
−3

+ c3


1
4
3

=


1
0
0


we get a linear system that, when Gauss’ method is applied,


1
3
1
1
2
0
4
0
1
−3
−3
0

−2ρ1+ρ2
−→
−ρ1+ρ3
−ρ2+ρ3
−→


1
3
1
1
0
−6
2
−2
0
0
−6
1


yields a solution. Thus, the vector is in the column space.

64
Linear Algebra, by Hefferon
Two.III.3.19
A routine Gaussian reduction




2
0
3
4
0
1
1
−1
3
1
0
2
1
0
−4
1




−(3/2)ρ1+ρ3
−→
−(1/2)ρ1+ρ4
−ρ2+ρ3
−→
−ρ3+ρ4
−→




2
0
3
4
0
1
1
−1
0
0
−11/2
−3
0
0
0
0




suggests this basis ⟨
¡2
0
3
4¢
,
¡0
1
1
−1¢
,
¡0
0
−11/2
−3¢
⟩.
Another, perhaps more convenient procedure, is to swap rows ﬁrst,
ρ1↔ρ4
−→
−3ρ1+ρ3
−→
−2ρ1+ρ4
−ρ2+ρ3
−→
−ρ3+ρ4
−→




1
0
−4
−1
0
1
1
−1
0
0
11
6
0
0
0
0




leading to the basis ⟨
¡
1
0
−4
−1
¢
,
¡
0
1
1
−1
¢
,
¡
0
0
11
6
¢
⟩.
Two.III.3.20
(a) This reduction
−(1/2)ρ1+ρ2
−→
−(1/2)ρ1+ρ3
−(1/3)ρ2+ρ3
−→


2
1
3
0
−3/2
1/2
0
0
4/3


shows that the row rank, and hence the rank, is three.
(b) Inspection of the columns shows that that the others are multiples of the ﬁrst (inspection of the
rows shows the same thing). Thus the rank is one.
Alternatively, the reduction

1
−1
2
3
−3
6
−2
2
−4

−3ρ1+ρ2
−→
2ρ1+ρ3


1
−1
2
0
0
0
0
0
0


shows the same thing.
(c) This calculation


1
3
2
5
1
1
6
4
3

−5ρ1+ρ2
−→
−6ρ1+ρ3
−ρ2+ρ3
−→


1
3
2
0
−14
−9
0
0
0


shows that the rank is two.
(d) The rank is zero.
Two.III.3.21
(a) This reduction



1
3
−1
3
1
4
2
1




ρ1+ρ2
−→
−ρ1+ρ3
−2ρ1+ρ4
−(1/6)ρ2+ρ3
−→
(5/6)ρ2+ρ4




1
3
0
6
0
0
0
0




gives ⟨
¡1
3¢
,
¡0
6¢
⟩.
(b) Transposing and reducing


1
2
1
3
1
−1
1
−3
−3

−3ρ1+ρ2
−→
−ρ1+ρ3


1
2
1
0
−5
−4
0
−5
−4

−ρ2+ρ3
−→


1
2
1
0
−5
−4
0
0
0


and then transposing back gives this basis.
⟨


1
2
1

,


0
−5
−4

⟩
(c) Notice ﬁrst that the surrounding space is given as P3, not P2. Then, taking the ﬁrst polynomial
1 + 1 · x + 0 · x2 + 0 · x3 to be “the same” as the row vector
¡
1
1
0
0
¢
, etc., leads to


1
1
0
0
1
0
−1
0
3
2
−1
0

−ρ1+ρ2
−→
−3ρ1+ρ3
−ρ2+ρ3
−→


1
1
0
0
0
−1
−1
0
0
0
0
0


which yields the basis ⟨1 + x, −x −x2⟩.
(d) Here “the same” gives


1
0
1
3
1
−1
1
0
3
2
1
4
−1
0
−5
−1
−1
−9

−ρ1+ρ2
−→
ρ1+ρ3
2ρ2+ρ3
−→


1
0
1
3
1
−1
0
0
2
−1
0
5
0
0
0
0
0
0



Answers to Exercises
65
leading to this basis.
⟨
µ1
0
1
3
1
−1
¶
,
µ 0
0
2
−1
0
5
¶
⟩
Two.III.3.22
Only the zero matrices have rank of zero. The only matrices of rank one have the form



k1 · ρ
...
km · ρ



where ρ is some nonzero row vector, and not all of the ki’s are zero. (Remark. We can’t simply say
that all of the rows are multiples of the ﬁrst because the ﬁrst row might be the zero row. Another
Remark. The above also applies with ‘column’ replacing ‘row’.)
Two.III.3.23
If a ̸= 0 then a choice of d = (c/a)b will make the second row be a multiple of the ﬁrst,
speciﬁcally, c/a times the ﬁrst. If a = 0 and b = 0 then any non-0 choice for d will ensure that the
second row is nonzero. If a = 0 and b ̸= 0 and c = 0 then any choice for d will do, since the matrix
will automatically have rank one (even with the choice of d = 0). Finally, if a = 0 and b ̸= 0 and c ̸= 0
then no choice for d will suﬃce because the matrix is sure to have rank two.
Two.III.3.24
The column rank is two. One way to see this is by inspection — the column space consists
of two-tall columns and so can have a dimension of at least two, and we can easily ﬁnd two columns
that together form a linearly independent set (the fourth and ﬁfth columns, for instance). Another
way to see this is to recall that the column rank equals the row rank, and to perform Gauss’ method,
which leaves two nonzero rows.
Two.III.3.25
We apply Theorem 3.13. The number of columns of a matrix of coeﬃcients A of a linear
system equals the number n of unknowns. A linear system with at least one solution has at most one
solution if and only if the space of solutions of the associated homogeneous system has dimension zero
(recall: in the ‘General = Particular+Homogeneous’ equation ⃗v = ⃗p+⃗h, provided that such a ⃗p exists,
the solution ⃗v is unique if and only if the vector ⃗h is unique, namely ⃗h = ⃗0). But that means, by the
theorem, that n = r.
Two.III.3.26
The set of columns must be dependent because the rank of the matrix is at most ﬁve
while there are nine columns.
Two.III.3.27
There is little danger of their being equal since the row space is a set of row vectors
while the column space is a set of columns (unless the matrix is 1×1, in which case the two spaces
must be equal).
Remark. Consider
A =
µ
1
3
2
6
¶
and note that the row space is the set of all multiples of
¡
1
3
¢
while the column space consists of
multiples of
µ
1
2
¶
so we also cannot argue that the two spaces must be simply transposes of each other.
Two.III.3.28
First, the vector space is the set of four-tuples of real numbers, under the natural oper-
ations. Although this is not the set of four-wide row vectors, the diﬀerence is slight — it is “the same”
as that set. So we will treat the four-tuples like four-wide vectors.
With that, one way to see that (1, 0, 1, 0) is not in the span of the ﬁrst set is to note that this
reduction


1
−1
2
−3
1
1
2
0
3
−1
6
−6

−ρ1+ρ2
−→
−3ρ1+ρ3
−ρ2+ρ3
−→


1
−1
2
−3
0
2
0
3
0
0
0
0


and this one




1
−1
2
−3
1
1
2
0
3
−1
6
−6
1
0
1
0




−ρ1+ρ2
−→
−3ρ1+ρ3
−ρ1+ρ4
−ρ2+ρ3
−→
−(1/2)ρ2+ρ4
ρ3↔ρ4
−→




1
−1
2
−3
0
2
0
3
0
0
−1
3/2
0
0
0
0




yield matrices diﬀering in rank. This means that addition of (1, 0, 1, 0) to the set of the ﬁrst three
four-tuples increases the rank, and hence the span, of that set. Therefore (1, 0, 1, 0) is not already in
the span.

66
Linear Algebra, by Hefferon
Two.III.3.29
It is a subspace because it is the column space of the matrix


3
2
4
1
0
−1
2
2
5


of coeﬃcients. To ﬁnd a basis for the column space,
{c1


3
1
2

+ c2


2
0
2

+ c3


4
−1
5

¯¯ c1, c2, c3 ∈R}
we take the three vectors from the spanning set, transpose, reduce,


3
1
2
2
0
2
4
−1
5

−(2/3)ρ1+ρ2
−→
−(4/3)ρ1+ρ3
−(7/2)ρ2+ρ3
−→


3
1
2
0
−2/3
2/3
0
0
0


and transpose back to get this.
⟨


3
1
2

,


0
−2/3
2/3

⟩
Two.III.3.30
This can be done as a straightforward calculation.
(rA + sB)trans =



ra1,1 + sb1,1
. . .
ra1,n + sb1,n
...
...
ram,1 + sbm,1
. . .
ram,n + sbm,n



trans
=



ra1,1 + sb1,1
. . .
ram,1 + sbm,1
...
ra1,n + sb1,n
. . .
ram,n + sbm,n



=



ra1,1
. . .
ram,1
...
ra1,n
. . .
ram,n


+



sb1,1
. . .
sbm,1
...
sb1,n
. . .
sbm,n



= rAtrans + sBtrans
Two.III.3.31
(a) These reductions give diﬀerent bases.
µ1
2
0
1
2
1
¶
−ρ1+ρ2
−→
µ1
2
0
0
0
1
¶
µ1
2
0
1
2
1
¶
−ρ1+ρ2
−→
2ρ2
−→
µ1
2
0
0
0
2
¶
(b) An easy example is this.
µ
1
2
1
3
1
4
¶


1
2
1
3
1
4
0
0
0


This is a less simplistic example.
µ
1
2
1
3
1
4
¶




1
2
1
3
1
4
2
4
2
4
3
5




(c) Assume that A and B are matrices with equal row spaces. Construct a matrix C with the rows
of A above the rows of B, and another matrix D with the rows of B above the rows of A.
C =
µ
A
B
¶
D =
µ
B
A
¶
Observe that C and D are row-equivalent (via a sequence of row-swaps) and so Gauss-Jordan reduce
to the same reduced echelon form matrix.
Because the row spaces are equal, the rows of B are linear combinations of the rows of A so
Gauss-Jordan reduction on C simply turns the rows of B to zero rows and thus the nonzero rows
of C are just the nonzero rows obtained by Gauss-Jordan reducing A. The same can be said for
the matrix D — Gauss-Jordan reduction on D gives the same non-zero rows as are produced by
reduction on B alone.
Therefore, A yields the same nonzero rows as C, which yields the same
nonzero rows as D, which yields the same nonzero rows as B.

Answers to Exercises
67
Two.III.3.32
It cannot be bigger.
Two.III.3.33
The number of rows in a maximal linearly independent set cannot exceed the number
of rows. A better bound (the bound that is, in general, the best possible) is the minimum of m and
n, because the row rank equals the column rank.
Two.III.3.34
Because the rows of a matrix A are turned into the columns of Atrans the dimension of
the row space of A equals the dimension of the column space of Atrans. But the dimension of the row
space of A is the rank of A and the dimension of the column space of Atrans is the rank of Atrans. Thus
the two ranks are equal.
Two.III.3.35
False. The ﬁrst is a set of columns while the second is a set of rows.
This example, however,
A =
µ
1
2
3
4
5
6
¶
,
Atrans =


1
4
2
5
3
6


indicates that as soon as we have a formal meaning for “the same”, we can apply it here:
Columnspace(A) = [{
µ
1
4
¶
,
µ
2
5
¶
,
µ
3
6
¶
}]
while
Rowspace(Atrans) = [{
¡1
4¢
,
¡2
5¢
,
¡3
6¢
}]
are “the same” as each other.
Two.III.3.36
No. Here, Gauss’ method does not change the column space.
µ
1
0
3
1
¶
−3ρ1+ρ2
−→
µ
1
0
0
1
¶
Two.III.3.37
A linear system
c1⃗a1 + · · · + cn⃗an = ⃗d
has a solution if and only if ⃗d is in the span of the set {⃗a1, . . . ,⃗an}. That’s true if and only if the
column rank of the augmented matrix equals the column rank of the matrix of coeﬃcients. Since rank
equals the column rank, the system has a solution if and only if the rank of its augmented matrix
equals the rank of its matrix of coeﬃcients.
Two.III.3.38
(a) Row rank equals column rank so each is at most the minimum of the number of
rows and columns. Hence both can be full only if the number of rows equals the number of columns.
(Of course, the converse does not hold: a square matrix need not have full row rank or full column
rank.)
(b) If A has full row rank then, no matter what the right-hand side, Gauss’ method on the augmented
matrix ends with a leading one in each row and none of those leading ones in the furthest right column
(the “augmenting” column). Back substitution then gives a solution.
On the other hand, if the linear system lacks a solution for some right-hand side it can only be
because Gauss’ method leaves some row so that it is all zeroes to the left of the “augmenting” bar
and has a nonzero entry on the right. Thus, if A does not have a solution for some right-hand sides,
then A does not have full row rank because some of its rows have been eliminated.
(c) The matrix A has full column rank if and only if its columns form a linearly independent set.
That’s equivalent to the existence of only the trivial linear relationship.
(d) The matrix A has full column rank if and only if the set of its columns is linearly independent,
and so forms a basis for its span. That’s equivalent to the existence of a unique linear representation
of all vectors in that span.
Two.III.3.39
Instead of the row spaces being the same, the row space of B would be a subspace
(possibly equal to) the row space of A.
Two.III.3.40
Clearly rank(A) = rank(−A) as Gauss’ method allows us to multiply all rows of a matrix
by −1. In the same way, when k ̸= 0 we have rank(A) = rank(kA).
Addition is more interesting. The rank of a sum can be smaller than the rank of the summands.
µ
1
2
3
4
¶
+
µ
−1
−2
−3
−4
¶
=
µ
0
0
0
0
¶
The rank of a sum can be bigger than the rank of the summands.
µ
1
2
0
0
¶
+
µ
0
0
3
4
¶
=
µ
1
2
3
4
¶

68
Linear Algebra, by Hefferon
But there is an upper bound (other than the size of the matrices).
In general, rank(A + B) ≤
rank(A) + rank(B).
To prove this, note that Gaussian elimination can be performed on A + B in either of two ways:
we can ﬁrst add A to B and then apply the appropriate sequence of reduction steps
(A + B)
step1
−→· · ·
stepk
−→echelon form
or we can get the same results by performing step1 through stepk separately on A and B, and then
adding. The largest rank that we can end with in the second case is clearly the sum of the ranks.
(The matrices above give examples of both possibilities, rank(A + B) < rank(A) + rank(B) and
rank(A + B) = rank(A) + rank(B), happening.)
Subsection Two.III.4: Combining Subspaces
Two.III.4.20
With each of these we can apply Lemma 4.15.
(a) Yes. The plane is the sum of this W1 and W2 because for any scalars a and b
µ
a
b
¶
=
µ
a −b
0
¶
+
µ
b
b
¶
shows that the general vector is a sum of vectors from the two parts. And, these two subspaces are
(diﬀerent) lines through the origin, and so have a trivial intersection.
(b) Yes. To see that any vector in the plane is a combination of vectors from these parts, consider
this relationship.
µ
a
b
¶
= c1
µ
1
1
¶
+ c2
µ
1
1.1
¶
We could now simply note that the set
{
µ
1
1
¶
,
µ
1
1.1
¶
}
is a basis for the space (because it is clearly linearly independent, and has size two in R2), and thus
ther is one and only one solution to the above equation, implying that all decompositions are unique.
Alternatively, we can solve
c1 +
c2 = a
c1 + 1.1c2 = b
−ρ1+ρ2
−→
c1 +
c2 =
a
0.1c2 = −a + b
to get that c2 = 10(−a + b) and c1 = 11a −10b, and so we have
µ
a
b
¶
=
µ
11a −10b
11a −10b
¶
+
µ
−10a + 10b
1.1 · (−10a + 10b)
¶
as required. As with the prior answer, each of the two subspaces is a line through the origin, and
their intersection is trivial.
(c) Yes. Each vector in the plane is a sum in this way
µ
x
y
¶
=
µ
x
y
¶
+
µ
0
0
¶
and the intersection of the two subspaces is trivial.
(d) No. The intersection is not trivial.
(e) No. These are not subspaces.
Two.III.4.21
With each of these we can use Lemma 4.15.
(a) Any vector in R3 can be decomposed as this sum.


x
y
z

=


x
y
0

+


0
0
z


And, the intersection of the xy-plane and the z-axis is the trivial subspace.
(b) Any vector in R3 can be decomposed as


x
y
z

=


x −z
y −z
0

+


z
z
z


and the intersection of the two spaces is trivial.

Answers to Exercises
69
Two.III.4.22
It is. Showing that these two are subspaces is routine. To see that the space is the direct
sum of these two, just note that each member of P2 has the unique decomposition m + nx + px2 =
(m + px2) + (nx).
Two.III.4.23
To show that they are subspaces is routine. We will argue they are complements with
Lemma 4.15. The intersection E ∩O is trivial because the only polynomial satisfying both conditions
p(−x) = p(x) and p(−x) = −p(x) is the zero polynomial. To see that the entire space is the sum of
the subspaces E + O = Pn, note that the polynomials p0(x) = 1, p2(x) = x2, p4(x) = x4, etc., are in
E and also note that the polynomials p1(x) = x, p3(x) = x3, etc., are in O. Hence any member of Pn
is a combination of members of E and O.
Two.III.4.24
Each of these is R3.
(a) These are broken into lines for legibility.
W1 + W2 + W3, W1 + W2 + W3 + W4, W1 + W2 + W3 + W5, W1 + W2 + W3 + W4 + W5,
W1 + W2 + W4, W1 + W2 + W4 + W5, W1 + W2 + W5,
W1 + W3 + W4, W1 + W3 + W5, W1 + W3 + W4 + W5,
W1 + W4, W1 + W4 + W5,
W1 + W5,
W2 + W3 + W4, W2 + W3 + W4 + W5,
W2 + W4, W2 + W4 + W5,
W3 + W4, W3 + W4 + W5,
W4 + W5
(b) W1 ⊕W2 ⊕W3, W1 ⊕W4, W1 ⊕W5, W2 ⊕W4, W3 ⊕W4
Two.III.4.25
Clearly each is a subspace. The bases Bi = ⟨xi⟩for the subspaces, when concatenated,
form a basis for the whole space.
Two.III.4.26
It is W2.
Two.III.4.27
True by Lemma 4.8.
Two.III.4.28
Two distinct direct sum decompositions of R4 are easy to ﬁnd. Two such are W1 =
[{⃗e1,⃗e2}] and W2 = [{⃗e3,⃗e4}], and also U1 = [{⃗e1}] and U2 = [{⃗e2,⃗e3,⃗e4}]. (Many more are possible,
for example R4 and its trivial subspace.)
In contrast, any partition of R1’s single-vector basis will give one basis with no elements and another
with a single element. Thus any decomposition involves R1 and its trivial subspace.
Two.III.4.29
Set inclusion one way is easy: {⃗w1 + · · · + ⃗wk
¯¯ ⃗wi ∈Wi} is a subset of [W1 ∪. . . ∪Wk]
because each ⃗w1 + · · · + ⃗wk is a sum of vectors from the union.
For the other inclusion, to any linear combination of vectors from the union apply commutativity
of vector addition to put vectors from W1 ﬁrst, followed by vectors from W2, etc. Add the vectors
from W1 to get a ⃗w1 ∈W1, add the vectors from W2 to get a ⃗w2 ∈W2, etc. The result has the desired
form.
Two.III.4.30
One example is to take the space to be R3, and to take the subspaces to be the xy-plane,
the xz-plane, and the yz-plane.
Two.III.4.31
Of course, the zero vector is in all of the subspaces, so the intersection contains at least
that one vector..
By the deﬁnition of direct sum the set {W1, . . . , Wk} is independent and so no
nonzero vector of Wi is a multiple of a member of Wj, when i ̸= j. In particular, no nonzero vector
from Wi equals a member of Wj.
Two.III.4.32
It can contain a trivial subspace; this set of subspaces of R3 is independent: {{⃗0}, x-axis}.
No nonzero vector from the trivial space {⃗0} is a multiple of a vector from the x-axis, simply because
the trivial space has no nonzero vectors to be candidates for such a multiple (and also no nonzero
vector from the x-axis is a multiple of the zero vector from the trivial subspace).
Two.III.4.33
Yes. For any subspace of a vector space we can take any basis ⟨⃗ω1, . . . , ⃗ωk⟩for that
subspace and extend it to a basis ⟨⃗ω1, . . . , ⃗ωk, ⃗βk+1, . . . , ⃗βn⟩for the whole space. Then the complemen
of the original subspace has this for a basis: ⟨⃗βk+1, . . . , ⃗βn⟩.
Two.III.4.34
(a) It must. Any member of W1 + W2 can be written ⃗w1 + ⃗w2 where ⃗w1 ∈W1 and
⃗w2 ∈W2. As S1 spans W1, the vector ⃗w1 is a combination of members of S1. Similarly ⃗w2 is a
combination of members of S2.

70
Linear Algebra, by Hefferon
(b) An easy way to see that it can be linearly independent is to take each to be the empty set. On
the other hand, in the space R1, if W1 = R1 and W2 = R1 and S1 = {1} and S2 = {2}, then their
union S1 ∪S2 is not independent.
Two.III.4.35
(a) The intersection and sum are
{
µ
0
0
c
0
¶ ¯¯ c ∈R}
{
µ
0
b
c
d
¶ ¯¯ b, c, d ∈R}
which have dimensions one and three.
(b) We write BU∩W for the basis for U ∩W, we write BU for the basis for U, we write BW for the
basis for W, and we write BU+W for the basis under consideration.
To see that that BU+W spans U + W, observe that any vector c⃗u + d⃗w from U + W can be
written as a linear combination of the vectors in BU+W , simply by expressing ⃗u in terms of BU and
expressing ⃗w in terms of BW .
We ﬁnish by showing that BU+W is linearly independent. Consider
c1⃗µ1 + · · · + cj+1⃗β1 + · · · + cj+k+p⃗ωp = ⃗0
which can be rewritten in this way.
c1⃗µ1 + · · · + cj⃗µj = −cj+1⃗β1 −· · · −cj+k+p⃗ωp
Note that the left side sums to a vector in U while right side sums to a vector in W, and thus both
sides sum to a member of U ∩W. Since the left side is a member of U ∩W, it is expressible in terms
of the members of BU∩W , which gives the combination of ⃗µ’s from the left side above as equal to
a combination of ⃗β’s. But, the fact that the basis BU is linearly independent shows that any such
combination is trivial, and in particular, the coeﬃcients c1, . . . , cj from the left side above are all
zero. Similarly, the coeﬃcients of the ⃗ω’s are all zero. This leaves the above equation as a linear
relationship among the ⃗β’s, but BU∩W is linearly independent, and therefore all of the coeﬃcients
of the ⃗β’s are also zero.
(c) Just count the basis vectors in the prior item: dim(U + W) = j + k + p, and dim(U) = j + k, and
dim(W) = k + p, and dim(U ∩W) = k.
(d) We know that dim(W1 + W2) = dim(W1) + dim(W2) −dim(W1 ∩W2). Because W1 ⊆W1 + W2,
we know that W1 + W2 must have dimension greater than that of W1, that is, must have dimension
eight, nine, or ten.
Substituting gives us three possibilities 8 = 8 + 8 −dim(W1 ∩W2) or 9 =
8 + 8 −dim(W1 ∩W2) or 10 = 8 + 8 −dim(W1 ∩W2). Thus dim(W1 ∩W2) must be either eight,
seven, or six. (Giving examples to show that each of these three cases is possible is easy, for instance
in R10.)
Two.III.4.36
Expand each Si to a basis Bi for Wi. The concatenation of those bases B1
⌢· · ·
⌢Bk is
a basis for V and thus its members form a linearly independent set. But the union S1 ∪· · · ∪Sk is a
subset of that linearly independent set, and thus is itself linearly independent.
Two.III.4.37
(a) Two such are these. µ
1
2
2
3
¶
µ
0
1
−1
0
¶
For the antisymmetric one, entries on the diagonal must be zero.
(b) A square symmetric matrix equals its transpose.
A square antisymmetric matrix equals the
negative of its transpose.
(c) Showing that the two sets are subspaces is easy. Suppose that A ∈Mn×n. To express A as a
sum of a symmetric and an antisymmetric matrix, we observe that
A = (1/2)(A + Atrans) + (1/2)(A −Atrans)
and note the ﬁrst summand is symmetric while the second is antisymmetric. Thus Mn×n is the
sum of the two subspaces. To show that the sum is direct, assume a matrix A is both symmetric
A = Atrans and antisymmetric A = −Atrans. Then A = −A and so all of A’s entries are zeroes.
Two.III.4.38
Assume that ⃗v ∈(W1 ∩W2) + (W1 ∩W3). Then ⃗v = ⃗w2 + ⃗w3 where ⃗w2 ∈W1 ∩W2 and
⃗w3 ∈W1 ∩W3. Note that ⃗w2, ⃗w3 ∈W1 and, as a subspace is closed under addition, ⃗w2 + ⃗w3 ∈W1.
Thus ⃗v = ⃗w2 + ⃗w3 ∈W1 ∩(W2 + W3).
This example proves that the inclusion may be strict: in R2 take W1 to be the x-axis, take W2 to
be the y-axis, and take W3 to be the line y = x. Then W1 ∩W2 and W1 ∩W3 are trivial and so their
sum is trivial. But W2 + W3 is all of R2 so W1 ∩(W2 + W3) is the x-axis.

Answers to Exercises
71
Two.III.4.39
It happens when at least one of W1, W2 is trivial. But that is the only way it can happen.
To prove this, assume that both are non-trivial, select nonzero vectors ⃗w1, ⃗w2 from each, and
consider ⃗w1 + ⃗w2. This sum is not in W1 because ⃗w1 + ⃗w2 = ⃗v ∈W1 would imply that ⃗w2 = ⃗v −⃗w1 is
in W1, which violates the assumption of the independence of the subspaces. Similarly, ⃗w1 + ⃗w2 is not
in W2. Thus there is an element of V that is not in W1 ∪W2.
Two.III.4.40
(a) The set
{
µ
v1
v2
¶ ¯¯
µ
v1
v2
¶ µ
x
0
¶
= 0 for all x ∈R}
is easily seen to be the y-axis.
(b) The yz-plane.
(c) The z-axis.
(d) Assume that U is a subspace of some Rn. Because U ⊥contains the zero vector, since that vector
is perpendicular to everything, we need only show that the orthocomplement is closed under linear
combinations of two elements. If ⃗w1, ⃗w2 ∈U ⊥then ⃗w1
⃗u = 0 and ⃗w2
⃗u = 0 for all ⃗u ∈U. Thus
(c1 ⃗w1 + c2 ⃗w2)
⃗u = c1(⃗w1
⃗u) + c2(⃗w2
⃗u) = 0 for all ⃗u ∈U and so U ⊥is closed under linear
combinations.
(e) The only vector orthogonal to itself is the zero vector.
(f) This is immediate.
(g) To prove that the dimensions add, it suﬃces by Corollary 4.13 and Lemma 4.15 to show that
U ∩U ⊥is the trivial subspace {⃗0}. But this is one of the prior items in this problem.
Two.III.4.41
Yes. The left-to-right implication is Corollary 4.13. For the other direction, assume that
dim(V ) = dim(W1) + · · · + dim(Wk). Let B1, . . . , Bk be bases for W1, . . . , Wk. As V is the sum of the
subspaces, any ⃗v ∈V can be written ⃗v = ⃗w1 + · · · + ⃗wk and expressing each ⃗wi as a combination of
vectors from the associated basis Bi shows that the concatenation B1
⌢· · ·
⌢Bk spans V . Now, that
concatenation has dim(W1) + · · · + dim(Wk) members, and so it is a spanning set of size dim(V ). The
concatenation is therefore a basis for V . Thus V is the direct sum.
Two.III.4.42
No. The standard basis for R2 does not split into bases for the complementary subspaces
the line x = y and the line x = −y.
Two.III.4.43
(a) Yes, W1 + W2 = W2 + W1 for all subspaces W1, W2 because each side is the span
of W1 ∪W2 = W2 ∪W1.
(b) This one is similar to the prior one — each side of that equation is the span of (W1 ∪W2)∪W3 =
W1 ∪(W2 ∪W3).
(c) Because this is an equality between sets, we can show that it holds by mutual inclusion. Clearly
W ⊆W + W. For W + W ⊆W just recall that every subset is closed under addition so any sum of
the form ⃗w1 + ⃗w2 is in W.
(d) In each vector space, the identity element with respect to subspace addition is the trivial subspace.
(e) Neither of left or right cancelation needs to hold. For an example, in R3 take W1 to be the
xy-plane, take W2 to be the x-axis, and take W3 to be the y-axis.
Two.III.4.44
(a) They are equal because for each, V is the direct sum if and only if each ⃗v ∈V can
be written in a unique way as a sum ⃗v = ⃗w1 + ⃗w2 and ⃗v = ⃗w2 + ⃗w1.
(b) They are equal because for each, V is the direct sum if and only if each ⃗v ∈V can be written in
a unique way as a sum of a vector from each ⃗v = (⃗w1 + ⃗w2) + ⃗w3 and ⃗v = ⃗w1 + (⃗w2 + ⃗w3).
(c) Any vector in R3 can be decomposed uniquely into the sum of a vector from each axis.
(d) No. For an example, in R2 take W1 to be the x-axis, take W2 to be the y-axis, and take W3 to
be the line y = x.
(e) In any vector space the trivial subspace acts as the identity element with respect to direct sum.
(f) In any vector space, only the trivial subspace has a direct-sum inverse (namely, itself). One way
to see this is that dimensions add, and so increase.
Topic: Fields
1
These checks are all routine; most consist only of remarking that property is so familiar that it does
not need to be proved.

72
Linear Algebra, by Hefferon
2
For both of these structures, these checks are all routine. As with the prior question, most of the
checks consist only of remarking that property is so familiar that it does not need to be proved.
3
There is no multiplicative inverse for 2 so the integers do not satisfy condition (5).
4
These checks can be done by listing all of the possibilities. For instance, to verify the commutativity
of addition, that a + b = b + a, we can easily check it for all possible pairs a, b, because there are
only four such pairs. Similarly, for associativity, there are only eight triples a, b, c, and so the check
is not too long. (There are other ways to do the checks, in particular, a reader may recognize these
operations as arithmetic ‘mod 2’.)
5
These will do.
+
0
1
2
0
0
1
2
1
1
2
0
2
2
0
1
·
0
1
2
0
0
0
0
1
0
1
2
2
0
2
1
As in the prior item, the check that they satisfy the conditions can be done by listing all of the cases,
although this way of checking is somewhat long (making use of commutativity is helpful in shortening
the work).
Topic: Crystals
1
Each fundamental unit is 3.34 × 10−10 cm, so there are about 0.1/(3.34 × 10−10) such units. That
gives 2.99 × 108, so there are something like 300, 000, 000 (three hundred million) units.
2
(a) We solve
c1
µ1.42
0
¶
+ c2
µ1.23
0.71
¶
=
µ5.67
3.14
¶
=⇒
1.42c1 + 1.23c2 = 5.67
0.71c2 = 3.14
to get c2 =≈4.42 and c1 ≈0.16.
(b) Here is the point located in the lattice. In the picture on the left, superimposed on the unit cell
are the two basis vectors ⃗β1 and ⃗β2, and a box showing the oﬀset of 0.16⃗β1 + 4.42⃗β2. The picture on
the right shows where that appears inside of the crystal lattice, taking as the origin the lower left
corner of the hexagon in the lower left.
So this point is in the next column of hexagons over, and either one hexagon up or two hexagons
up, depending on how you count them.
(c) This second basis
⟨
µ1.42
0
¶
,
µ 0
1.42
¶
⟩
makes the computation easier
c1
µ
1.42
0
¶
+ c2
µ
0
1.42
¶
=
µ
5.67
3.14
¶
=⇒
1.42c1
= 5.67
1.42c2 = 3.14
(we get c2 ≈2.21 and c1 ≈3.99), but it doesn’t seem to have to do much with the physical structure
that we are studying.
3
In terms of the basis the locations of the corner atoms are (0, 0, 0), (1, 0, 0), . . . , (1, 1, 1). The locations
of the face atoms are (0.5, 0.5, 1), (1, 0.5, 0.5), (0.5, 1, 0.5), (0, 0.5, 0.5), (0.5, 0, 0.5), and (0.5, 0.5, 0). The
locations of the atoms a quarter of the way down from the top are (0.75, 0.75, 0.75) and (0.25, 0.25, 0.25).
The atoms a quarter of the way up from the bottom are at (0.75, 0.25, 0.25) and (0.25, 0.75, 0.25).
Converting to ˚Angstroms is easy.

Answers to Exercises
73
4
(a) 195.08/6.02 × 1023 = 3.239 × 10−22
(b) 4
(c) 4 · 3.239 × 10−22 = 1.296 × 10−21
(d) 1.296 × 10−21/21.45 = 6.042 × 10−23 cubic centimeters
(e) 3.924 × 10−8 centimeters.
(f) ⟨


3.924 × 10−8
0
0

,


0
3.924 × 10−8
0

,


0
0
3.924 × 10−8

⟩
Topic: Dimensional Analysis
1
(a) This relationship
(L1M 0T 0)p1(L1M 0T 0)p2(L1M 0T −1)p3(L0M 0T 0)p4(L1M 0T −2)p5(L0M 0T 1)p6 = L0M 0T 0
gives rise to this linear system
p1 + p2 +
p3 + p5
= 0
0 = 0
−p3 −2p5 + p6 = 0
(note that there is no restriction on p4). The natural parametrization uses the free variables to give
p3 = −2p5 + p6 and p1 = −p2 + p5 −p6. The resulting description of the solution set
{








p1
p2
p3
p4
p5
p6








= p2








−1
1
0
0
0
0








+ p4








0
0
0
1
0
0








+ p5








1
0
−2
0
1
0








+ p6








−1
0
1
0
0
1








¯¯ p2, p4, p5, p6 ∈R}
gives {y/x, θ, xt/v02, v0t/x} as a complete set of dimensionless products (recall that “complete” in
this context does not mean that there are no other dimensionless products; it simply means that the
set is a basis). This is, however, not the set of dimensionless products that the question asks for.
There are two ways to proceed. The ﬁrst is to ﬁddle with the choice of parameters, hoping to
hit on the right set. For that, we can do the prior paragraph in reverse. Converting the given
dimensionless products gt/v0, gx/v2
0, gy/v2
0, and θ into vectors gives this description (note the ?’s
where the parameters will go).
{








p1
p2
p3
p4
p5
p6








= ?








0
0
−1
0
1
1








+ ?








1
0
−2
0
1
0








+ ?








0
1
−2
0
1
0








+ p4








0
0
0
1
0
0








¯¯ p2, p4, p5, p6 ∈R}
The p4 is already in place. Examining the rows shows that we can also put in place p6, p1, and p2.
The second way to proceed, following the hint, is to note that the given set is of size four in
a four-dimensional vector space and so we need only show that it is linearly independent. That
is easily done by inspection, by considering the sixth, ﬁrst, second, and fourth components of the
vectors.
(b) The ﬁrst equation can be rewritten
gx
v02 = gt
v0
cos θ
so that Buckingham’s function is f1(Π1, Π2, Π3, Π4) = Π2 −Π1 cos(Π4). The second equation can
be rewritten
gy
v02 = gt
v0
sin θ −1
2
µgt
v0
¶2
and Buckingham’s function here is f2(Π1, Π2, Π3, Π4) = Π3 −Π1 sin(Π4) + (1/2)Π1
2.

74
Linear Algebra, by Hefferon
2
We consider
(L0M 0T −1)p1(L1M −1T 2)p2(L−3M 0T 0)p3(L0M 1T 0)p4 = (L0M 0T 0)
which gives these relations among the powers.
p2 −3p3
= 0
−p2
+ p4 = 0
−p1 + 2p2
= 0
ρ1↔ρ3
−→
ρ2+ρ3
−→
−p1 + 2p2
= 0
−p2
+ p4 = 0
−3p3 + p4 = 0
This is the solution space (because we wish to express k as a function of the other quantities, p2 is
taken as the parameter).
{




2
1
1/3
1



p2
¯¯ p2 ∈R}
Thus, Π1 = ν2kN 1/3m is the dimensionless combination, and we have that k equals ν−2N −1/3m−1
times a constant (the function ˆf is constant since it has no arguments).
3
(a) Setting
(L2M 1T −2)p1(L0M 0T −1)p2(L3M 0T 0)p3 = (L0M 0T 0)
gives this
2p1
+ 3p3 = 0
p1
= 0
−2p1 −p2
= 0
which implies that p1 = p2 = p3 = 0. That is, among quantities with these dimensional formulas,
the only dimensionless product is the trivial one.
(b) Setting
(L2M 1T −2)p1(L0M 0T −1)p2(L3M 0T 0)p3(L−3M 1T 0)p4 = (L0M 0T 0)
gives this.
2p1
+ 3p3 −3p4 = 0
p1
+ p4 = 0
−2p1 −p2
= 0
(−1/2)ρ1+ρ2
−→
ρ1+ρ3
ρ2↔ρ3
−→
2p1
+
3p3 −
3p4 = 0
−p2 +
3p3 −
3p4 = 0
(−3/2)p3 + (5/2)p4 = 0
Taking p1 as parameter to express the torque gives this description of the solution set.
{




1
−2
−5/3
−1



p1
¯¯ p1 ∈R}
Denoting the torque by τ, the rotation rate by r, the volume of air by V , and the density of air by
d we have that Π1 = τr−2V −5/3d−1, and so the torque is r2V 5/3d times a constant.
4
(a) These are the dimensional formulas.
quantity
dimensional
formula
speed of the wave v
L1M 0T −1
separation of the dominoes d
L1M 0T 0
height of the dominoes h
L1M 0T 0
acceleration due to gravity g
L1M 0T −2
(b) The relationship
(L1M 0T −1)p1(L1M 0T 0)p2(L1M 0T 0)p3(L1M 0T −2)p4 = (L0M 0T 0)
gives this linear system.
p1 + p2 + p3 + p4 = 0
0 = 0
−p1
−2p4 = 0
ρ1+ρ4
−→
p1 + p2 + p3 + p4 = 0
p2 + p3 −p4 = 0
Taking p3 and p4 as parameters, the solution set is described in this way.
{




0
−1
1
0



p3 +




−2
1
0
1



p4
¯¯ p3, p4 ∈R}
That gives {Π1 = h/d, Π2 = dg/v2} as a complete set.

Answers to Exercises
75
(c) Buckingham’s Theorem says that v2 = dg · ˆf(h/d), and so, since g is a constant, if h/d is ﬁxed
then v is proportional to
√
d .
5
Checking the conditions in the deﬁnition of a vector space is routine.
6
(a) The dimensional formula of the circumference is L, that is, L1M 0T 0. The dimensional formula
of the area is L2.
(b) One is C + A = 2πr + πr2.
(c) One example is this formula relating the the length of arc subtended by an angle to the radius
and the angle measure in radians: ℓ−rθ = 0. Both terms in that formula have dimensional formula
L1. The relationship holds for some unit systems (inches and radians, for instance) but not for all
unit systems (inches and degrees, for instance).


Chapter Three: Maps Between Spaces
Subsection Three.I.1: Definition and Examples
Three.I.1.10
(a) Call the map f.
¡a
b¢
f
7−→
µ
a
b
¶
It is one-to-one because if f sends two members of the domain to the same image, that is, if
f
¡¡
a
b
¢¢
= f
¡¡
c
d
¢¢
, then the deﬁnition of f gives that
µ
a
b
¶
=
µ
c
d
¶
and since column vectors are equal only if they have equal components, we have that a = c and that
b = d. Thus, if f maps two row vectors from the domain to the same column vector then the two
row vectors are equal:
¡
a
b
¢
=
¡
c
d
¢
.
To show that f is onto we must show that any member of the codomain R2 is the image under
f of some row vector. That’s easy;
µ
x
y
¶
is f
¡¡
x
y
¢¢
.
The computation for preservation of addition is this.
f
¡¡
a
b
¢
+
¡
c
d
¢¢
= f
¡¡
a + c
b + d
¢¢
=
µ
a + c
b + d
¶
=
µ
a
b
¶
+
µ
c
d
¶
= f
¡¡
a
b
¢¢
+ f
¡¡
c
d
¢¢
The computation for preservation of scalar multiplication is similar.
f
¡
r ·
¡
a
b
¢¢
= f
¡¡
ra
rb
¢¢
=
µra
rb
¶
= r ·
µa
b
¶
= r · f
¡¡
a
b
¢¢
(b) Denote the map from Example 1.2 by f. To show that it is one-to-one, assume that f(a0 +a1x+
a2x2) = f(b0 + b1x + b2x2). Then by the deﬁnition of the function,


a0
a1
a2

=


b0
b1
b2


and so a0 = b0 and a1 = b1 and a2 = b2. Thus a0 + a1x + a2x2 = b0 + b1x + b2x2, and consequently
f is one-to-one.
The function f is onto because there is a polynomial sent to


a
b
c


by f, namely, a + bx + cx2.
As for structure, this shows that f preserves addition
f
¡
(a0 + a1x + a2x2) + (b0 + b1x + b2x2)
¢
= f
¡
(a0 + b0) + (a1 + b1)x + (a2 + b2)x2 ¢
=


a0 + b0
a1 + b1
a2 + b2


=


a0
a1
a2

+


b0
b1
b2


= f(a0 + a1x + a2x2) + f(b0 + b1x + b2x2)

78
Linear Algebra, by Hefferon
and this shows
f( r(a0 + a1x + a2x2) ) = f( (ra0) + (ra1)x + (ra2)x2 )
=


ra0
ra1
ra2


= r ·


a0
a1
a2


= r f(a0 + a1x + a2x2)
that it preserves scalar multiplication.
Three.I.1.11
These are the images.
(a)
µ
5
−2
¶
(b)
µ
0
2
¶
(c)
µ
−1
1
¶
To prove that f is one-to-one, assume that it maps two linear polynomials to the same image
f(a1 + b1x) = f(a2 + b2x). Then
µ
a1 −b1
b1
¶
=
µ
a2 −b2
b2
¶
and so, since column vectors are equal only when their components are equal, b1 = b2 and a1 = a2.
That shows that the two linear polynomials are equal, and so f is one-to-one.
To show that f is onto, note that this member of the codomain
µ
s
t
¶
is the image of this member of the domain (s + t) + tx.
To check that f preserves structure, we can use item (2) of Lemma 1.9.
f (c1 · (a1 + b1x) + c2 · (a2 + b2x)) = f ((c1a1 + c2a2) + (c1b1 + c2b2)x)
=
µ
(c1a1 + c2a2) −(c1b1 + c2b2)
c1b1 + c2b2
¶
= c1 ·
µ
a1 −b1
b1
¶
+ c2 ·
µ
a2 −b2
b2
¶
= c1 · f(a1 + b1x) + c2 · f(a2 + b2x)
Three.I.1.12
To verify it is one-to-one, assume that f1(c1x + c2y + c3z) = f1(d1x + d2y + d3z). Then
c1 + c2x + c3x2 = d1 + d2x + d3x2 by the deﬁnition of f1.
Members of P2 are equal only when
they have the same coeﬃcients, so this implies that c1 = d1 and c2 = d2 and c3 = d3. Therefore
f1(c1x + c2y + c3z) = f1(d1x + d2y + d3z) implies that c1x + c2y + c3z = d1x + d2y + d3z, and so f1 is
one-to-one.
To verify that it is onto, consider an arbitrary member of the codomain a1 +a2x+a3x2 and observe
that it is indeed the image of a member of the domain, namely, it is f1(a1x+a2y +a3z). (For instance,
0 + 3x + 6x2 = f1(0x + 3y + 6z).)
The computation checking that f1 preserves addition is this.
f1 ( (c1x + c2y + c3z) + (d1x + d2y + d3z) ) = f1 ( (c1 + d1)x + (c2 + d2)y + (c3 + d3)z )
= (c1 + d1) + (c2 + d2)x + (c3 + d3)x2
= (c1 + c2x + c3x2) + (d1 + d2x + d3x2)
= f1(c1x + c2y + c3z) + f1(d1x + d2y + d3z)
The check that f1 preserves scalar multiplication is this.
f1( r · (c1x + c2y + c3z) ) = f1( (rc1)x + (rc2)y + (rc3)z )
= (rc1) + (rc2)x + (rc3)x2
= r · (c1 + c2x + c3x2)
= r · f1(c1x + c2y + c3z)
Three.I.1.13
(a) No; this map is not one-to-one. In particular, the matrix of all zeroes is mapped
to the same image as the matrix of all ones.

Answers to Exercises
79
(b) Yes, this is an isomorphism.
It is one-to-one:
if f(
µa1
b1
c1
d1
¶
) = f(
µa2
b2
c2
d2
¶
) then




a1 + b1 + c1 + d1
a1 + b1 + c1
a1 + b1
a1



=




a2 + b2 + c2 + d2
a2 + b2 + c2
a2 + b2
a2




gives that a1 = a2, and that b1 = b2, and that c1 = c2, and that d1 = d2.
It is onto, since this shows




x
y
z
w



= f(
µ
w
z −w
y −z
x −y
¶
)
that any four-tall vector is the image of a 2×2 matrix.
Finally, it preserves combinations
f( r1 ·
µ
a1
b1
c1
d1
¶
+ r2 ·
µ
a2
b2
c2
d2
¶
) = f(
µ
r1a1 + r2a2
r1b1 + r2b2
r1c1 + r2c2
r1d1 + r2d2
¶
)
=




r1a1 + · · · + r2d2
r1a1 + · · · + r2c2
r1a1 + · · · + r2b2
r1a1 + r2a2




= r1 ·




a1 + · · · + d1
a1 + · · · + c1
a1 + b1
a1



+ r2 ·




a2 + · · · + d2
a2 + · · · + c2
a2 + b2
a2




= r1 · f(
µ
a1
b1
c1
d1
¶
) + r2 · f(
µ
a2
b2
c2
d2
¶
)
and so item (2) of Lemma 1.9 shows that it preserves structure.
(c) Yes, it is an isomorphism.
To show that it is one-to-one, we suppose that two members of the domain have the same image
under f.
f(
µ
a1
b1
c1
d1
¶
) = f(
µ
a2
b2
c2
d2
¶
)
This gives, by the deﬁnition of f, that c1 + (d1 + c1)x + (b1 + a1)x2 + a1x3 = c2 + (d2 + c2)x + (b2 +
a2)x2 + a2x3 and then the fact that polynomials are equal only when their coeﬃcients are equal
gives a set of linear equations
c1 = c2
d1 + c1 = d2 + c2
b1 + a1 = b2 + a2
a1 = a2
that has only the solution a1 = a2, b1 = b2, c1 = c2, and d1 = d2.
To show that f is onto, we note that p + qx + rx2 + sx3 is the image under f of this matrix.
µ
s
r −s
p
q −p
¶
We can check that f preserves structure by using item (2) of Lemma 1.9.
f(r1 ·
µ
a1
b1
c1
d1
¶
+ r2 ·
µ
a2
b2
c2
d2
¶
) = f(
µ
r1a1 + r2a2
r1b1 + r2b2
r1c1 + r2c2
r1d1 + r2d2
¶
)
= (r1c1 + r2c2) + (r1d1 + r2d2 + r1c1 + r2c2)x
+ (r1b1 + r2b2 + r1a1 + r2a2)x2 + (r1a1 + r2a2)x3
= r1 ·
¡
c1 + (d1 + c1)x + (b1 + a1)x2 + a1x3¢
+ r2 ·
¡
c2 + (d2 + c2)x + (b2 + a2)x2 + a2x3¢
= r1 · f(
µ
a1
b1
c1
d1
¶
) + r2 · f(
µ
a2
b2
c2
d2
¶
)

80
Linear Algebra, by Hefferon
(d) No, this map does not preserve structure. For instance, it does not send the zero matrix to the
zero polynomial.
Three.I.1.14
It is one-to-one and onto, a correspondence, because it has an inverse (namely, f −1(x) =
3√x). However, it is not an isomorphism. For instance, f(1) + f(1) ̸= f(1 + 1).
Three.I.1.15
Many maps are possible. Here are two.
¡a
b¢
7→
µ
b
a
¶
and
¡a
b¢
7→
µ
2a
b
¶
The veriﬁcations are straightforward adaptations of the others above.
Three.I.1.16
Here are two.
a0 + a1x + a2x2 7→


a1
a0
a2


and
a0 + a1x + a2x2 7→


a0 + a1
a1
a2


Veriﬁcation is straightforward (for the second, to show that it is onto, note that


s
t
u


is the image of (s −t) + tx + ux2).
Three.I.1.17
The space R2 is not a subspace of R3 because it is not a subset of R3. The two-tall
vectors in R2 are not members of R3.
The natural isomorphism ι: R2 →R3 (called the injection map) is this.
µ
x
y
¶
ι
7−→


x
y
0


This map is one-to-one because
f(
µx1
y1
¶
) = f(
µx2
y2
¶
)
implies


x1
y1
0

=


x2
y2
0


which in turn implies that x1 = x2 and y1 = y2, and therefore the initial two two-tall vectors are equal.
Because


x
y
0

= f(
µx
y
¶
)
this map is onto the xy-plane.
To show that this map preserves structure, we will use item (2) of Lemma 1.9 and show
f(c1 ·
µx1
y1
¶
+ c2 ·
µx2
y2
¶
) = f(
µc1x1 + c2x2
c1y1 + c2y2
¶
) =


c1x1 + c2x2
c1y1 + c2y2
0


= c1 ·


x1
y1
0

+ c2 ·


x2
y2
0

= c1 · f(
µ
x1
y1
¶
) + c2 · f(
µ
x2
y2
¶
)
that it preserves combinations of two vectors.
Three.I.1.18
Here are two:





r1
r2
...
r16




7→


r1
r2
. . .
. . .
r16


and





r1
r2
...
r16




7→





r1
r2
...
...
r16





Veriﬁcation that each is an isomorphism is easy.
Three.I.1.19
When k is the product k = mn, here is an isomorphism.



r1
r2
. . .
...
. . .
rm·n


7→





r1
r2
...
rm·n





Checking that this is an isomorphism is easy.

Answers to Exercises
81
Three.I.1.20
If n ≥1 then Pn−1 ∼= Rn. (If we take P−1 and R0 to be trivial vector spaces, then the
relationship extends one dimension lower.) The natural isomorphism between them is this.
a0 + a1x + · · · + an−1xn−1 7→





a0
a1
...
an−1





Checking that it is an isomorphism is straightforward.
Three.I.1.21
This is the map, expanded.
f(a0 + a1x + a2x2 + a3x3 + a4x4 + a5x5) = a0 + a1(x −1) + a2(x −1)2 + a3(x −1)3
+ a4(x −1)4 + a5(x −1)5
= a0 + a1(x −1) + a2(x2 −2x + 1)
+ a3(x3 −3x2 + 3x −1)
+ a4(x4 −4x3 + 6x2 −4x + 1)
+ a5(x5 −5x4 + 10x3 −10x2 + 5x −1)
= (a0 −a1 + a2 −a3 + a4 −a5)
+ (a1 −2a2 + 3a3 −4a4 + 5a5)x
+ (a2 −3a3 + 6a4 −10a5)x2 + (a3 −4a4 + 10a5)x3
+ (a4 −5a5)x4 + a5x5
This map is a correspondence because it has an inverse, the map p(x) 7→p(x + 1).
To ﬁnish checking that it is an isomorphism, we apply item (2) of Lemma 1.9 and show that it
preserves linear combinations of two polynomials. Brieﬂy, the check goes like this.
f(c · (a0 + a1x + · · · + a5x5) + d · (b0 + b1x + · · · + b5x5))
= · · · = (ca0 −ca1 + ca2 −ca3 + ca4 −ca5 + db0 −db1 + db2 −db3 + db4 −db5) + · · · + (ca5 + db5)x5
= · · · = c · f(a0 + a1x + · · · + a5x5) + d · f(b0 + b1x + · · · + b5x5)
Three.I.1.22
No vector space has the empty set underlying it. We can take ⃗v to be the zero vector.
Three.I.1.23
Yes; where the two spaces are {⃗a} and {⃗b}, the map sending ⃗a to ⃗b is clearly one-to-one
and onto, and also preserves what little structure there is.
Three.I.1.24
A linear combination of n = 0 vectors adds to the zero vector and so Lemma 1.8 shows
that the three statements are equivalent in this case.
Three.I.1.25
Consider the basis ⟨1⟩for P0 and let f(1) ∈R be k. For any a ∈P0 we have that
f(a) = f(a · 1) = af(1) = ak and so f’s action is multiplication by k. Note that k ̸= 0 or else the map
is not one-to-one. (Incidentally, any such map a 7→ka is an isomorphism, as is easy to check.)
Three.I.1.26
In each item, following item (2) of Lemma 1.9, we show that the map preserves structure
by showing that the it preserves linear combinations of two members of the domain.
(a) The identity map is clearly one-to-one and onto. For linear combinations the check is easy.
id(c1 · ⃗v1 + c2 · ⃗v2) = c1⃗v1 + c2⃗v2 = c1 · id(⃗v1) + c2 · id(⃗v2)
(b) The inverse of a correspondence is also a correspondence (as stated in the appendix), so we need
only check that the inverse preserves linear combinations. Assume that ⃗w1 = f(⃗v1) (so f −1(⃗w1) = ⃗v1)
and assume that ⃗w2 = f(⃗v2).
f −1(c1 · ⃗w1 + c2 · ⃗w2) = f −1¡
c1 · f(⃗v1) + c2 · f(⃗v2)
¢
= f −1( f
¡
c1⃗v1 + c2⃗v2)
¢
= c1⃗v1 + c2⃗v2
= c1 · f −1(⃗w1) + c2 · f −1(⃗w2)
(c) The composition of two correspondences is a correspondence (as stated in the appendix), so we
need only check that the composition map preserves linear combinations.
g ◦f
¡
c1 · ⃗v1 + c2 · ⃗v2
¢
= g
¡
f(c1⃗v1 + c2⃗v2)
¢
= g
¡
c1 · f(⃗v1) + c2 · f(⃗v2)
¢
= c1 · g
¡
f(⃗v1)) + c2 · g(f(⃗v2)
¢
= c1 · g ◦f (⃗v1) + c2 · g ◦f (⃗v2)

82
Linear Algebra, by Hefferon
Three.I.1.27
One direction is easy: by deﬁnition, if f is one-to-one then for any ⃗w ∈W at most one
⃗v ∈V has f(⃗v ) = ⃗w, and so in particular, at most one member of V is mapped to ⃗0W . The proof
of Lemma 1.8 does not use the fact that the map is a correspondence and therefore shows that any
structure-preserving map f sends ⃗0V to ⃗0W .
For the other direction, assume that the only member of V that is mapped to ⃗0W is ⃗0V . To show
that f is one-to-one assume that f(⃗v1) = f(⃗v2). Then f(⃗v1) −f(⃗v2) = ⃗0W and so f(⃗v1 −⃗v2) = ⃗0W .
Consequently ⃗v1 −⃗v2 = ⃗0V , so ⃗v1 = ⃗v2, and so f is one-to-one.
Three.I.1.28
We will prove something stronger — not only is the existence of a dependence preserved
by isomorphism, but each instance of a dependence is preserved, that is,
⃗vi = c1⃗v1 + · · · + ci−1⃗vi−1 + ci+1⃗vi+1 + · · · + ck⃗vk
⇐⇒f(⃗vi) = c1f(⃗v1) + · · · + ci−1f(⃗vi−1) + ci+1f(⃗vi+1) + · · · + ckf(⃗vk).
The =⇒direction of this statement holds by item (3) of Lemma 1.9. The ⇐= direction holds by
regrouping
f(⃗vi) = c1f(⃗v1) + · · · + ci−1f(⃗vi−1) + ci+1f(⃗vi+1) + · · · + ckf(⃗vk)
= f(c1⃗v1 + · · · + ci−1⃗vi−1 + ci+1⃗vi+1 + · · · + ck⃗vk)
and applying the fact that f is one-to-one, and so for the two vectors ⃗vi and c1⃗v1 + · · · + ci−1⃗vi−1 +
ci+1f⃗vi+1 + · · · + ckf(⃗vk to be mapped to the same image by f, they must be equal.
Three.I.1.29
(a) This map is one-to-one because if ds(⃗v1) = ds(⃗v2) then by deﬁnition of the map,
s · ⃗v1 = s · ⃗v2 and so ⃗v1 = ⃗v2, as s is nonzero. This map is onto as any ⃗w ∈R2 is the image of
⃗v = (1/s) · ⃗w (again, note that s is nonzero). (Another way to see that this map is a correspondence
is to observe that it has an inverse: the inverse of ds is d1/s.)
To ﬁnish, note that this map preserves linear combinations
ds(c1 · ⃗v1 + c2 · ⃗v2) = s(c1⃗v1 + c2⃗v2) = c1s⃗v1 + c2s⃗v2 = c1 · ds(⃗v1) + c2 · ds(⃗v2)
and therefore is an isomorphism.
(b) As in the prior item, we can show that the map tθ is a correspondence by noting that it has an
inverse, t−θ.
That the map preserves structure is geometrically easy to see. For instance, adding two vectors
and then rotating them has the same eﬀect as rotating ﬁrst and then adding. For an algebraic
argument, consider polar coordinates: the map tθ sends the vector with endpoint (r, φ) to the vector
with endpoint (r, φ+θ). Then the familiar trigonometric formulas cos(φ+θ) = cos φ cos θ−sin φ sin θ
and sin(φ+θ) = sin φ cos θ+cos φ sin θ show how to express the map’s action in the usual rectangular
coordinate system. µx
y
¶
=
µr cos φ
r sin φ
¶
tθ
7−→
µr cos(φ + θ)
r sin(φ + θ)
¶
=
µx cos θ −y sin θ
x sin θ + y cos θ
¶
Now the calculation for preservation of addition is routine.
µx1 + x2
y1 + y2
¶
tθ
7−→
µ(x1 + x2) cos θ −(y1 + y2) sin θ
(x1 + x2) sin θ + (y1 + y2) cos θ
¶
=
µx1 cos θ −y1 sin θ
x1 sin θ + y1 cos θ
¶
+
µx2 cos θ −y2 sin θ
x2 sin θ + y2 cos θ
¶
The calculation for preservation of scalar multiplication is similar.
(c) This map is a correspondence because it has an inverse (namely, itself).
As in the last item, that the reﬂection map preserves structure is geometrically easy to see: adding
vectors and then reﬂecting gives the same result as reﬂecting ﬁrst and then adding, for instance.
For an algebraic proof, suppose that the line ℓhas slope k (the case of a line with undeﬁned slope
can be done as a separate, but easy, case). We can follow the hint and use polar coordinates: where
the line ℓforms an angle of φ with the x-axis, the action of fℓis to send the vector with endpoint
(r cos θ, r sin θ) to the one with endpoint (r cos(2φ −θ), r sin(2φ −θ)).
fℓ
7−→
θ
φ
φ −(θ −φ)
To convert to rectangular coordinates, we will use some trigonometric formulas, as we did in the
prior item. First observe that cos φ and sin φ can be determined from the slope k of the line. This
picture
x
kx
x
√
1 + k2
θ

Answers to Exercises
83
gives that cos φ = 1/
√
1 + k2 and sin φ = k/
√
1 + k2. Now,
cos(2φ −θ) = cos(2φ) cos θ + sin(2φ) sin θ
=
¡
cos2 φ −sin2 φ
¢
cos θ + (2 sin φ cos φ) sin θ
=
µ
(
1
√
1 + k2 )2 −(
k
√
1 + k2 )2
¶
cos θ +
µ
2
k
√
1 + k2
1
√
1 + k2
¶
sin θ
=
µ1 −k2
1 + k2
¶
cos θ +
µ
2k
1 + k2
¶
sin θ
and thus the ﬁrst component of the image vector is this.
r · cos(2φ −θ) = 1 −k2
1 + k2 · x +
2k
1 + k2 · y
A similar calculation shows that the second component of the image vector is this.
r · sin(2φ −θ) =
2k
1 + k2 · x −1 −k2
1 + k2 · y
With this algebraic description of the action of fℓ
µ
x
y
¶
fℓ
7−→
µ
(1 −k2/1 + k2) · x + (2k/1 + k2) · y
(2k/1 + k2) · x −(1 −k2/1 + k2) · y
¶
checking that it preserves structure is routine.
Three.I.1.30
First, the map p(x) 7→p(x + k) doesn’t count because it is a version of p(x) 7→p(x −k).
Here is a correct answer (many others are also correct): a0+a1x+a2x2 7→a2+a0x+a1x2. Veriﬁcation
that this is an isomorphism is straightforward.
Three.I.1.31
(a) For the ‘only if’ half, let f : R1 →R1 to be an isomorphism. Consider the basis
⟨1⟩⊆R1. Designate f(1) by k. Then for any x we have that f(x) = f(x · 1) = x · f(1) = xk, and
so f’s action is multiplication by k. To ﬁnish this half, just note that k ̸= 0 or else f would not be
one-to-one.
For the ‘if’ half we only have to check that such a map is an isomorphism when k ̸= 0. To check
that it is one-to-one, assume that f(x1) = f(x2) so that kx1 = kx2 and divide by the nonzero factor
k to conclude that x1 = x2. To check that it is onto, note that any y ∈R1 is the image of x = y/k
(again, k ̸= 0). Finally, to check that such a map preserves combinations of two members of the
domain, we have this.
f(c1x1 + c2x2) = k(c1x1 + c2x2) = c1kx1 + c2kx2 = c1f(x1) + c2f(x2)
(b) By the prior item, f’s action is x 7→(7/3)x. Thus f(−2) = −14/3.
(c) For the ‘only if’ half, assume that f : R2 →R2 is an automorphism. Consider the standard basis
E2 for R2. Let
f(⃗e1) =
µ
a
c
¶
and
f(⃗e2) =
µ
b
d
¶
.
Then the action of f on any vector is determined by by its action on the two basis vectors.
f(
µx
y
¶
) = f(x · ⃗e1 + y · ⃗e2) = x · f(⃗e1) + y · f(⃗e2) = x ·
µa
c
¶
+ y ·
µb
d
¶
=
µax + by
cx + dy
¶
To ﬁnish this half, note that if ad −bc = 0, that is, if f(⃗e2) is a multiple of f(⃗e1), then f is not
one-to-one.
For ‘if’ we must check that the map is an isomorphism, under the condition that ad −bc ̸= 0.
The structure-preservation check is easy; we will here show that f is a correspondence. For the
argument that the map is one-to-one, assume this.
f(
µ
x1
y1
¶
) = f(
µ
x2
y2
¶
)
and so
µ
ax1 + by1
cx1 + dy1
¶
=
µ
ax2 + by2
cx2 + dy2
¶
Then, because ad −bc ̸= 0, the resulting system
a(x1 −x2) + b(y1 −y2) = 0
c(x1 −x2) + d(y1 −y2) = 0
has a unique solution, namely the trivial one x1 −x2 = 0 and y1 −y2 = 0 (this follows from the
hint).
The argument that this map is onto is closely related — this system
ax1 + by1 = x
cx1 + dy1 = y

84
Linear Algebra, by Hefferon
has a solution for any x and y if and only if this set
{
µ
a
c
¶
,
µ
b
d
¶
}
spans R2, i.e., if and only if this set is a basis (because it is a two-element subset of R2), i.e., if and
only if ad −bc ̸= 0.
(d)
f(
µ
0
−1
¶
) = f(
µ
1
3
¶
−
µ
1
4
¶
) = f(
µ
1
3
¶
) −f(
µ
1
4
¶
) =
µ
2
−1
¶
−
µ
0
1
¶
=
µ
2
−2
¶
Three.I.1.32
There are many answers; two are linear independence and subspaces.
To show that if a set {⃗v1, . . . ,⃗vn} is linearly independent then its image {f(⃗v1), . . . , f(⃗vn)} is also
linearly independent, consider a linear relationship among members of the image set.
0 = c1f(⃗v1) + · · · + cnf( ⃗vn) = f(c1⃗v1) + · · · + f(cn ⃗vn) = f(c1⃗v1 + · · · + cn ⃗vn)
Because this map is an isomorphism, it is one-to-one. So f maps only one vector from the domain to
the zero vector in the range, that is, c1⃗v1 +· · ·+cn⃗vn equals the zero vector (in the domain, of course).
But, if {⃗v1, . . . ,⃗vn} is linearly independent then all of the c’s are zero, and so {f(⃗v1), . . . , f(⃗vn)} is
linearly independent also. (Remark. There is a small point about this argument that is worth mention.
In a set, repeats collapse, that is, strictly speaking, this is a one-element set: {⃗v,⃗v}, because the things
listed as in it are the same thing. Observe, however, the use of the subscript n in the above argument.
In moving from the domain set {⃗v1, . . . ,⃗vn} to the image set {f(⃗v1), . . . , f(⃗vn)}, there is no collapsing,
because the image set does not have repeats, because the isomorphism f is one-to-one.)
To show that if f : V →W is an isomorphism and if U is a subspace of the domain V then the set
of image vectors f(U) = {⃗w ∈W
¯¯ ⃗w = f(⃗u) for some ⃗u ∈U} is a subspace of W, we need only show
that it is closed under linear combinations of two of its members (it is nonempty because it contains
the image of the zero vector). We have
c1 · f(⃗u1) + c2 · f(⃗u2) = f(c1⃗u1) + f(c2⃗u2) = f(c1⃗u1 + c2⃗u2)
and c1⃗u1 + c2⃗u2 is a member of U because of the closure of a subspace under combinations. Hence the
combination of f(⃗u1) and f(⃗u2) is a member of f(U).
Three.I.1.33
(a) The association
⃗p = c1⃗β1 + c2⃗β2 + c3⃗β3
RepB(·)
7−→


c1
c2
c3


is a function if every member ⃗p of the domain is associated with at least one member of the codomain,
and if every member ⃗p of the domain is associated with at most one member of the codomain. The
ﬁrst condition holds because the basis B spans the domain — every ⃗p can be written as at least one
linear combination of ⃗β’s. The second condition holds because the basis B is linearly independent —
every member ⃗p of the domain can be written as at most one linear combination of the ⃗β’s.
(b) For the one-to-one argument, if RepB(⃗p) = RepB(⃗q), that is, if RepB(p1⃗β1 + p2⃗β2 + p3⃗β3) =
RepB(q1⃗β1 + q2⃗β2 + q3⃗β3) then


p1
p2
p3

=


q1
q2
q3


and so p1 = q1 and p2 = q2 and p3 = q3, which gives the conclusion that ⃗p = ⃗q. Therefore this map
is one-to-one.
For onto, we can just note that


a
b
c


equals RepB(a⃗β1 +b⃗β2 +c⃗β3), and so any member of the codomain R3 is the image of some member
of the domain P2.
(c) This map respects addition and scalar multiplication because it respects combinations of two
members of the domain (that is, we are using item (2) of Lemma 1.9): where ⃗p = p1⃗β1 +p2⃗β2 +p3⃗β3

Answers to Exercises
85
and ⃗q = q1⃗β1 + q2⃗β2 + q3⃗β3, we have this.
RepB(c · ⃗p + d · ⃗q) = RepB( (cp1 + dq1)⃗β1 + (cp2 + dq2)⃗β2 + (cp3 + dq3)⃗β3 )
=


cp1 + dq1
cp2 + dq2
cp3 + dq3


= c ·


p1
p2
p3

+ d ·


q1
q2
q3


= RepB(⃗p) + RepB(⃗q)
(d) Use any basis B for P2 whose ﬁrst two members are x + x2 and 1 −x, say B = ⟨x + x2, 1 −x, 1⟩.
Three.I.1.34
See the next subsection.
Three.I.1.35
(a) Most of the conditions in the deﬁnition of a vector space are routine. We here
sketch the veriﬁcation of part (1) of that deﬁnition.
For closure of U × W, note that because U and W are closed, we have that ⃗u1 + ⃗u2 ∈U and
⃗w1 + ⃗w2 ∈W and so (⃗u1 + ⃗u2, ⃗w1 + ⃗w2) ∈U × W. Commutativity of addition in U × W follows
from commutativity of addition in U and W.
(⃗u1, ⃗w1) + (⃗u2, ⃗w2) = (⃗u1 + ⃗u2, ⃗w1 + ⃗w2) = (⃗u2 + ⃗u1, ⃗w2 + ⃗w1) = (⃗u2, ⃗w2) + (⃗u1, ⃗w1)
The check for associativity of addition is similar. The zero element is (⃗0U,⃗0W ) ∈U × W and the
additive inverse of (⃗u, ⃗w) is (−⃗u, −⃗w).
The checks for the second part of the deﬁnition of a vector space are also straightforward.
(b) This is a basis
⟨(1,
µ0
0
¶
), (x,
µ0
0
¶
), (x2,
µ0
0
¶
), (1,
µ1
0
¶
), (1,
µ0
1
¶
) ⟩
because there is one and only one way to represent any member of P2 × R2 with respect to this set;
here is an example.
(3 + 2x + x2,
µ5
4
¶
) = 3 · (1,
µ0
0
¶
) + 2 · (x,
µ0
0
¶
) + (x2,
µ0
0
¶
) + 5 · (1,
µ1
0
¶
) + 4 · (1,
µ0
1
¶
)
The dimension of this space is ﬁve.
(c) We have dim(U × W) = dim(U) + dim(W) as this is a basis.
⟨(⃗µ1,⃗0W ), . . . , (⃗µdim(U),⃗0W ), (⃗0U, ⃗ω1), . . . , (⃗0U, ⃗ωdim(W ))⟩
(d) We know that if V = U ⊕W then each ⃗v ∈V can be written as ⃗v = ⃗u + ⃗w in one and only one
way. This is just what we need to prove that the given function an isomorphism.
First, to show that f is one-to-one we can show that if f ((⃗u1, ⃗w1)) = ((⃗u2, ⃗w2)), that is, if
⃗u1 + ⃗w1 = ⃗u2 + ⃗w2 then ⃗u1 = ⃗u2 and ⃗w1 = ⃗w2. But the statement ‘each ⃗v is such a sum in only
one way’ is exactly what is needed to make this conclusion. Similarly, the argument that f is onto
is completed by the statement that ‘each ⃗v is such a sum in at least one way’.
This map also preserves linear combinations
f( c1 · (⃗u1, ⃗w1) + c2 · (⃗u2, ⃗w2) ) = f( (c1⃗u1 + c2⃗u2, c1 ⃗w1 + c2 ⃗w2) )
= c1⃗u1 + c2⃗u2 + c1 ⃗w1 + c2 ⃗w2
= c1⃗u1 + c1 ⃗w1 + c2⃗u2 + c2 ⃗w2
= c1 · f( (⃗u1, ⃗w1) ) + c2 · f( (⃗u2, ⃗w2) )
and so it is an isomorphism.
Subsection Three.I.2: Dimension Characterizes Isomorphism
Three.I.2.8
Each pair of spaces is isomorphic if and only if the two have the same dimension. We can,
when there is an isomorphism, state a map, but it isn’t strictly necessary.
(a) No, they have diﬀerent dimensions.
(b) No, they have diﬀerent dimensions.

86
Linear Algebra, by Hefferon
(c) Yes, they have the same dimension. One isomorphism is this.
µ
a
b
c
d
e
f
¶
7→



a
...
f



(d) Yes, they have the same dimension. This is an isomorphism.
a + bx + · · · + fx5 7→
µa
b
c
d
e
f
¶
(e) Yes, both have dimension 2k.
Three.I.2.9
(a) RepB(3 −2x) =
µ
5
−2
¶
(b)
µ
0
2
¶
(c)
µ
−1
1
¶
Three.I.2.10
They have diﬀerent dimensions.
Three.I.2.11
Yes, both are mn-dimensional.
Three.I.2.12
Yes, any two (nondegenerate) planes are both two-dimensional vector spaces.
Three.I.2.13
There are many answers, one is the set of Pk (taking P−1 to be the trivial vector space).
Three.I.2.14
False (except when n = 0). For instance, if f : V →Rn is an isomorphism then mul-
tiplying by any nonzero scalar, gives another, diﬀerent, isomorphism.
(Between trivial spaces the
isomorphisms are unique; the only map possible is ⃗0V 7→0W .)
Three.I.2.15
No.
A proper subspace has a strictly lower dimension than it’s superspace; if U is
a proper subspace of V then any linearly independent subset of U must have fewer than dim(V )
members or else that set would be a basis for V , and U wouldn’t be proper.
Three.I.2.16
Where B = ⟨⃗β1, . . . , ⃗βn⟩, the inverse is this.



c1
...
cn


7→c1⃗β1 + · · · + cn⃗βn
Three.I.2.17
All three spaces have dimension equal to the rank of the matrix.
Three.I.2.18
We must show that if ⃗a = ⃗b then f(⃗a) = f(⃗b). So suppose that a1⃗β1 + · · · + an⃗βn =
b1⃗β1 + · · · + bn⃗βn. Each vector in a vector space (here, the domain space) has a unique representation
as a linear combination of basis vectors, so we can conclude that a1 = b1, . . . , an = bn. Thus,
f(⃗a) =



a1
...
an


=



b1
...
bn


= f(⃗b)
and so the function is well-deﬁned.
Three.I.2.19
Yes, because a zero-dimensional space is a trivial space.
Three.I.2.20
(a) No, this collection has no spaces of odd dimension.
(b) Yes, because Pk ∼= Rk+1.
(c) No, for instance, M2×3 ∼= M3×2.
Three.I.2.21
One direction is easy: if the two are isomorphic via f then for any basis B ⊆V , the set
D = f(B) is also a basis (this is shown in Lemma 2.3). The check that corresponding vectors have the
same coordinates: f(c1⃗β1 + · · · + cn⃗βn) = c1f(⃗β1) + · · · + cnf(⃗βn) = c1⃗δ1 + · · · + cn⃗δn is routine.
For the other half, assume that there are bases such that corresponding vectors have the same
coordinates with respect to those bases.
Because f is a correspondence, to show that it is an
isomorphism, we need only show that it preserves structure.
Because RepB(⃗v ) = RepD(f(⃗v )),
the map f preserves structure if and only if representations preserve addition: RepB(⃗v1 + ⃗v2) =
RepB(⃗v1) + RepB(⃗v2) and scalar multiplication: RepB(r ·⃗v ) = r · RepB(⃗v ) The addition calculation is
this: (c1+d1)⃗β1+· · ·+(cn+dn)⃗βn = c1⃗β1+· · ·+cn⃗βn+d1⃗β1+· · ·+dn⃗βn, and the scalar multiplication
calculation is similar.
Three.I.2.22
(a) Pulling the deﬁnition back from R4 to P3 gives that a0 + a1x + a2x2 + a3x3 is
orthogonal to b0 + b1x + b2x2 + b3x3 if and only if a0b0 + a1b1 + a2b2 + a3b3 = 0.

Answers to Exercises
87
(b) A natural deﬁnition is this.
D(




a0
a1
a2
a3



) =




a1
2a2
3a3
0




Three.I.2.23
Yes.
Assume that V is a vector space with basis B = ⟨⃗β1, . . . , ⃗βn⟩and that W is another vector space
such that the map f : B →W is a correspondence. Consider the extension ˆf : V →W of f.
ˆf(c1⃗β1 + · · · + cn⃗βn) = c1f(⃗β1) + · · · + cnf(⃗βn).
The map ˆf is an isomorphism.
First, ˆf is well-deﬁned because every member of V has one and only one representation as a linear
combination of elements of B.
Second, ˆf is one-to-one because every member of W has only one representation as a linear combi-
nation of elements of ⟨f(⃗β1), . . . , f(⃗βn)⟩. That map ˆf is onto because every member of W has at least
one representation as a linear combination of members of ⟨f(⃗β1), . . . , f(⃗βn)⟩.
Finally, preservation of structure is routine to check.
For instance, here is the preservation of
addition calculation.
ˆf( (c1⃗β1 + · · · + cn⃗βn) + (d1⃗β1 + · · · + dn⃗βn) ) = ˆf( (c1 + d1)⃗β1 + · · · + (cn + dn)⃗βn )
= (c1 + d1)f(⃗β1) + · · · + (cn + dn)f(⃗βn)
= c1f(⃗β1) + · · · + cnf(⃗βn) + d1f(⃗β1) + · · · + dnf(⃗βn)
= ˆf(c1⃗β1 + · · · + cn⃗βn) + + ˆf(d1⃗β1 + · · · + dn⃗βn).
Preservation of scalar multiplication is similar.
Three.I.2.24
Because V1 ∩V2 = {⃗0V } and f is one-to-one we have that f(V1) ∩f(V2) = {⃗0U}. To
ﬁnish, count the dimensions: dim(U) = dim(V ) = dim(V1) + dim(V2) = dim(f(V1)) + dim(f(V2)), as
required.
Three.I.2.25
Rational numbers have many representations, e.g., 1/2 = 3/6, and the numerators can
vary among representations.
Subsection Three.II.1: Deﬁnition
Three.II.1.17
(a) Yes. The veriﬁcation is straightforward.
h(c1 ·


x1
y1
z1

+ c2 ·


x2
y2
z2

) = h(


c1x1 + c2x2
c1y1 + c2y2
c1z1 + c2z2

)
=
µ
c1x1 + c2x2
c1x1 + c2x2 + c1y1 + c2y2 + c1z1 + c2z2
¶
= c1 ·
µ
x1
x1 + y1 + z1
¶
+ c2 ·
µ
x2
c2 + y2 + z2
¶
= c1 · h(


x1
y1
z1

) + c2 · h(


x2
y2
z2

)
(b) Yes. The veriﬁcation is easy.
h(c1 ·


x1
y1
z1

+ c2 ·


x2
y2
z2

) = h(


c1x1 + c2x2
c1y1 + c2y2
c1z1 + c2z2

)
=
µ
0
0
¶
= c1 · h(


x1
y1
z1

) + c2 · h(


x2
y2
z2

)

88
Linear Algebra, by Hefferon
(c) No. An example of an addition that is not respected is this.
h(


0
0
0

+


0
0
0

) =
µ
1
1
¶
̸= h(


0
0
0

) + h(


0
0
0

)
(d) Yes. The veriﬁcation is straightforward.
h(c1 ·


x1
y1
z1

+ c2 ·


x2
y2
z2

) = h(


c1x1 + c2x2
c1y1 + c2y2
c1z1 + c2z2

)
=
µ
2(c1x1 + c2x2) + (c1y1 + c2y2)
3(c1y1 + c2y2) −4(c1z1 + c2z2)
¶
= c1 ·
µ
2x1 + y1
3y1 −4z1
¶
+ c2 ·
µ
2x2 + y2
3y2 −4z2
¶
= c1 · h(


x1
y1
z1

) + c2 · h(


x2
y2
z2

)
Three.II.1.18
For each, we must either check that linear combinations are preserved, or give an
example of a linear combination that is not.
(a) Yes. The check that it preserves combinations is routine.
h(r1 ·
µ
a1
b1
c1
d1
¶
+ r2 ·
µ
a2
b2
c2
d2
¶
) = h(
µ
r1a1 + r2a2
r1b1 + r2b2
r1c1 + r2c2
r1d1 + r2d2
¶
)
= (r1a1 + r2a2) + (r1d1 + r2d2)
= r1(a1 + d1) + r2(a2 + d2)
= r1 · h(
µ
a1
b1
c1
d1
¶
) + r2 · h(
µ
a2
b2
c2
d2
¶
)
(b) No. For instance, not preserved is multiplication by the scalar 2.
h(2 ·
µ
1
0
0
1
¶
) = h(
µ
2
0
0
2
¶
) = 4
while
2 · h(
µ
1
0
0
1
¶
) = 2 · 1 = 2
(c) Yes. This is the check that it preserves combinations of two members of the domain.
h(r1 ·
µa1
b1
c1
d1
¶
+ r2 ·
µa2
b2
c2
d2
¶
) = h(
µr1a1 + r2a2
r1b1 + r2b2
r1c1 + r2c2
r1d1 + r2d2
¶
)
= 2(r1a1 + r2a2) + 3(r1b1 + r2b2) + (r1c1 + r2c2) −(r1d1 + r2d2)
= r1(2a1 + 3b1 + c1 −d1) + r2(2a2 + 3b2 + c2 −d2)
= r1 · h(
µ
a1
b1
c1
d1
¶
+ r2 · h(
µ
a2
b2
c2
d2
¶
)
(d) No. An example of a combination that is not preserved is this.
h(
µ
1
0
0
0
¶
+
µ
1
0
0
0
¶
) = h(
µ
2
0
0
0
¶
) = 4
while
h(
µ
1
0
0
0
¶
) + h(
µ
1
0
0
0
¶
) = 1 + 1 = 2
Three.II.1.19
The check that each is a homomorphisms is routine. Here is the check for the diﬀeren-
tiation map.
d
dx(r · (a0 + a1x + a2x2 + a3x3) + s · (b0 + b1x + b2x2 + b3x3))
= d
dx((ra0 + sb0) + (ra1 + sb1)x + (ra2 + sb2)x2 + (ra3 + sb3)x3)
= (ra1 + sb1) + 2(ra2 + sb2)x + 3(ra3 + sb3)x2
= r · (a1 + 2a2x + 3a3x2) + s · (b1 + 2b2x + 3b3x2)
= r · d
dx(a0 + a1x + a2x2 + a3x3) + s · d
dx(b0 + b1x + b2x2 + b3x3)
(An alternate proof is to simply note that this is a property of diﬀerentiation that is familar from
calculus.)
These two maps are not inverses as this composition does not act as the identity map on this
element of the domain.
1 ∈P3
d/dx
7−→0 ∈P2
R
7−→0 ∈P3

Answers to Exercises
89
Three.II.1.20
Each of these projections is a homomorphism. Projection to the xz-plane and to the
yz-plane are these maps.


x
y
z

7→


x
0
z




x
y
z

7→


0
y
z


Projection to the x-axis, to the y-axis, and to the z-axis are these maps.


x
y
z

7→


x
0
0




x
y
z

7→


0
y
0




x
y
z

7→


0
0
z


And projection to the origin is this map.


x
y
z

7→


0
0
0


Veriﬁcation that each is a homomorphism is straightforward. (The last one, of course, is the zero
transformation on R3.)
Three.II.1.21
The ﬁrst is not onto; for instance, there is no polynomial that is sent the constant
polynomial p(x) = 1. The second is not one-to-one; both of these members of the domain
µ1
0
0
0
¶
and
µ0
0
0
1
¶
are mapped to the same member of the codomain, 1 ∈R.
Three.II.1.22
Yes; in any space id(c · ⃗v + d · ⃗w) = c · ⃗v + d · ⃗w = c · id(⃗v) + d · id(⃗w).
Three.II.1.23
(a) This map does not preserve structure since f(1 + 1) = 3, while f(1) + f(1) = 2.
(b) The check is routine.
f(r1 ·
µ
x1
y1
¶
+ r2 ·
µ
x2
y2
¶
) = f(
µ
r1x1 + r2x2
r1y1 + r2y2
¶
)
= (r1x1 + r2x2) + 2(r1y1 + r2y2)
= r1 · (x1 + 2y1) + r2 · (x2 + 2y2)
= r1 · f(
µ
x1
y1
¶
) + r2 · f(
µ
x2
y2
¶
)
Three.II.1.24
Yes. Where h: V →W is linear, h(⃗u −⃗v) = h(⃗u + (−1) · ⃗v) = h(⃗u) + (−1) · h(⃗v) =
h(⃗u) −h(⃗v).
Three.II.1.25
(a) Let ⃗v ∈V be represented with respect to the basis as ⃗v = c1⃗β1 +· · ·+cn⃗βn. Then
h(⃗v) = h(c1⃗β1 + · · · + cn⃗βn) = c1h(⃗β1) + · · · + cnh(⃗βn) = c1 ·⃗0 + · · · + cn ·⃗0 = ⃗0.
(b) This argument is similar to the prior one. Let ⃗v ∈V be represented with respect to the basis as
⃗v = c1⃗β1 + · · · + cn⃗βn. Then h(c1⃗β1 + · · · + cn⃗βn) = c1h(⃗β1) + · · · + cnh(⃗βn) = c1⃗β1 + · · · + cn⃗βn = ⃗v.
(c) As above, only c1h(⃗β1) + · · · + cnh(⃗βn) = c1r⃗β1 + · · · + cnr⃗βn = r(c1⃗β1 + · · · + cn⃗βn) = r⃗v.
Three.II.1.26
That it is a homomorphism follows from the familiar rules that the logarithm of a
product is the sum of the logarithms ln(ab) = ln(a) + ln(b) and that the logarithm of a power is the
multiple of the logarithm ln(ar) = r ln(a). This map is an isomorphism because it has an inverse,
namely, the exponential map, so it is a correspondence, and therefore it is an isomorphism.
Three.II.1.27
Where ˆx = x/2 and ˆy = y/3, the image set is
{
µ
ˆx
ˆy
¶ ¯¯ (2ˆx)2
4
+ (3ˆy)2
9
= 1} = {
µ
ˆx
ˆy
¶ ¯¯ ˆx2 + ˆy2 = 1}
the unit circle in the ˆxˆy-plane.
Three.II.1.28
The circumference function r 7→2πr is linear. Thus we have 2π · (rearth + 6) −2π ·
(rearth) = 12π. Observe that it takes the same amount of extra rope to raise the circle from tightly
wound around a basketball to six feet above that basketball as it does to raise it from tightly wound
around the earth to six feet above the earth.

90
Linear Algebra, by Hefferon
Three.II.1.29
Verifying that it is linear is routine.
h(c1 ·


x1
y1
z1

+ c2 ·


x2
y2
z2

) = h(


c1x1 + c2x2
c1y1 + c2y2
c1z1 + c2z2

)
= 3(c1x1 + c2x2) −(c1y1 + c2y2) −(c1z1 + c2z2)
= c1 · (3x1 −y1 −z1) + c2 · (3x2 −y2 −z2)
= c1 · h(


x1
y1
z1

) + c2 · h(


x2
y2
z2

)
The natural guess at a generalization is that for any ﬁxed ⃗k ∈R3 the map ⃗v 7→⃗v ⃗k is linear. This
statement is true. It follows from properties of the dot product we have seen earlier: (⃗v+⃗u) ⃗k = ⃗v ⃗k+⃗u ⃗k
and (r⃗v) ⃗k = r(⃗v ⃗k). (The natural guess at a generalization of this generalization, that the map from
Rn to R whose action consists of taking the dot product of its argument with a ﬁxed vector ⃗k ∈Rn is
linear, is also true.)
Three.II.1.30
Let h: R1 →R1 be linear. A linear map is determined by its action on a basis, so ﬁx the
basis ⟨1⟩for R1. For any r ∈R1 we have that h(r) = h(r · 1) = r · h(1) and so h acts on any argument
r by multiplying it by the constant h(1). If h(1) is not zero then the map is a correspondence — its
inverse is division by h(1) — so any nontrivial transformation of R1 is an isomorphism.
This projection map is an example that shows that not every transformation of Rn acts via multi-
plication by a constant when n > 1, including when n = 2.





x1
x2
...
xn




7→





x1
0
...
0





Three.II.1.31
(a) Where c and d are scalars, we have this.
h(c ·



x1
...
xn


+ d ·



y1
...
yn


) = h(



cx1 + dy1
...
cxn + dyn


)
=



a1,1(cx1 + dy1) + · · · + a1,n(cxn + dyn)
...
am,1(cx1 + dy1) + · · · + am,n(cxn + dyn)



= c ·



a1,1x1 + · · · + a1,nxn
...
am,1x1 + · · · + am,nxn


+ d ·



a1,1y1 + · · · + a1,nyn
...
am,1y1 + · · · + am,nyn



= c · h(



x1
...
xn


) + d · h(



y1
...
yn


)
(b) Each power i of the derivative operator is linear because of these rules familiar from calculus.
di
dxi ( f(x) + g(x) ) = di
dxi f(x) + di
dxi g(x)
and
di
dxi r · f(x) = r · di
dxi f(x)
Thus the given map is a linear transformation of Pn because any linear combination of linear maps
is also a linear map.
Three.II.1.32
(This argument has already appeared, as part of the proof that isomorphism is an equiv-
alence.) Let f : U →V and g: V →W be linear. For any ⃗u1, ⃗u2 ∈U and scalars c1, c2 combinations
are preserved.
g ◦f(c1⃗u1 + c2⃗u2) = g( f(c1⃗u1 + c2⃗u2) ) = g( c1f(⃗u1) + c2f(⃗u2) )
= c1 · g(f(⃗u1)) + c2 · g(f(⃗u2)) = c1 · g ◦f(⃗u1) + c2 · g ◦f(⃗u2)
Three.II.1.33
(a) Yes. The set of ⃗w ’s cannot be linearly independent if the set of ⃗v ’s is linearly
dependent because any nontrivial relationship in the domain ⃗0V = c1⃗v1 + · · · + cn⃗vn would give a

Answers to Exercises
91
nontrivial relationship in the range f(⃗0V ) = ⃗0W = f(c1⃗v1 + · · · + cn⃗vn) = c1f(⃗v1) + · · · + cnf(⃗vn) =
c1 ⃗w + · · · + cn ⃗wn.
(b) Not necessarily. For instance, the transformation of R2 given by
µx
y
¶
7→
µx + y
x + y
¶
sends this linearly independent set in the domain to a linearly dependent image.
{⃗v1,⃗v2} = {
µ
1
0
¶
,
µ
1
1
¶
} 7→{
µ
1
1
¶
,
µ
2
2
¶
} = {⃗w1, ⃗w2}
(c) Not necessarily. An example is the projection map π: R3 →R2


x
y
z

7→
µ
x
y
¶
and this set that does not span the domain but maps to a set that does span the codomain.
{


1
0
0

,


0
1
0

}
π
7−→{
µ
1
0
¶
,
µ
0
1
¶
}
(d) Not necessarily. For instance, the injection map ι: R2 →R3 sends the standard basis E2 for the
domain to a set that does not span the codomain. (Remark. However, the set of ⃗w’s does span the
range. A proof is easy.)
Three.II.1.34
Recall that the entry in row i and column j of the transpose of M is the entry mj,i
from row j and column i of M. Now, the check is routine.
[r ·




...
· · ·
ai,j
· · ·
...



+ s ·




...
· · ·
bi,j
· · ·
...



]
trans
=




...
· · ·
rai,j + sbi,j
· · ·
...




trans
=




...
· · ·
raj,i + sbj,i
· · ·
...




= r ·




...
· · ·
aj,i
· · ·
...



+ s ·




...
· · ·
bj,i
· · ·
...




= r ·




...
· · ·
aj,i
· · ·
...




trans
+ s ·




...
· · ·
bj,i
· · ·
...




trans
The domain is Mm×n while the codomain is Mn×m.
Three.II.1.35
(a) For any homomorphism h: Rn →Rm we have
h(ℓ) = {h(t · ⃗u + (1 −t) · ⃗v)
¯¯ t ∈[0..1]} = {t · h(⃗u) + (1 −t) · h(⃗v)
¯¯ t ∈[0..1]}
which is the line segment from h(⃗u) to h(⃗v).
(b) We must show that if a subset of the domain is convex then its image, as a subset of the range, is
also convex. Suppose that C ⊆Rn is convex and consider its image h(C). To show h(C) is convex
we must show that for any two of its members, ⃗d1 and ⃗d2, the line segment connecting them
ℓ= {t · ⃗d1 + (1 −t) · ⃗d2
¯¯ t ∈[0..1]}
is a subset of h(C).
Fix any member ˆt · ⃗d1 + (1 −ˆt) · ⃗d2 of that line segment. Because the endpoints of ℓare in the
image of C, there are members of C that map to them, say h(⃗c1) = ⃗d1 and h(⃗c2) = ⃗d2. Now, where
ˆt is the scalar that is ﬁxed in the ﬁrst sentence of this paragraph, observe that h(ˆt·⃗c1 +(1−ˆt)·⃗c2) =
ˆt · h(⃗c1) + (1 −ˆt) · h(⃗c2) = ˆt · ⃗d1 + (1 −ˆt) · ⃗d2 Thus, any member of ℓis a member of h(C), and so
h(C) is convex.

92
Linear Algebra, by Hefferon
Three.II.1.36
(a) For ⃗v0,⃗v1 ∈Rn, the line through ⃗v0 with direction ⃗v1 is the set {⃗v0 + t · ⃗v1
¯¯ t ∈R}.
The image under h of that line {h(⃗v0 + t · ⃗v1)
¯¯ t ∈R} = {h(⃗v0) + t · h(⃗v1)
¯¯ t ∈R} is the line through
h(⃗v0) with direction h(⃗v1). If h(⃗v1) is the zero vector then this line is degenerate.
(b) A k-dimensional linear surface in Rn maps to a (possibly degenerate) k-dimensional linear surface
in Rm. The proof is just like that the one for the line.
Three.II.1.37
Suppose that h: V →W is a homomorphism and suppose that S is a subspace of
V .
Consider the map ˆh: S →W deﬁned by ˆh(⃗s) = h(⃗s).
(The only diﬀerence between ˆh and h
is the diﬀerence in domain.) Then this new map is linear: ˆh(c1 · ⃗s1 + c2 · ⃗s2) = h(c1⃗s1 + c2⃗s2) =
c1h(⃗s1) + c2h(⃗s2) = c1 · ˆh(⃗s1) + c2 · ˆh(⃗s2).
Three.II.1.38
This will appear as a lemma in the next subsection.
(a) The range is nonempty because V is nonempty. To ﬁnish we need to show that it is closed under
combinations. A combination of range vectors has the form, where ⃗v1, . . . ,⃗vn ∈V ,
c1 · h(⃗v1) + · · · + cn · h(⃗vn) = h(c1⃗v1) + · · · + h(cn⃗vn) = h(c1 · ⃗v1 + · · · + cn · ⃗vn),
which is itself in the range as c1 · ⃗v1 + · · · + cn · ⃗vn is a member of domain V . Therefore the range is
a subspace.
(b) The nullspace is nonempty since it contains ⃗0V , as ⃗0V maps to ⃗0W . It is closed under linear com-
binations because, where ⃗v1, . . . ,⃗vn ∈V are elements of the inverse image set {⃗v ∈V
¯¯ h(⃗v) = ⃗0W },
for c1, . . . , cn ∈R
⃗0W = c1 · h(⃗v1) + · · · + cn · h(⃗vn) = h(c1 · ⃗v1 + · · · + cn · ⃗vn)
and so c1 · ⃗v1 + · · · + cn · ⃗vn is also in the inverse image of ⃗0W .
(c) This image of U nonempty because U is nonempty.
For closure under combinations, where
⃗u1, . . . , ⃗un ∈U,
c1 · h(⃗u1) + · · · + cn · h(⃗un) = h(c1 · ⃗u1) + · · · + h(cn · ⃗un) = h(c1 · ⃗u1 + · · · + cn · ⃗un)
which is itself in h(U) as c1 · ⃗u1 + · · · + cn · ⃗un is in U. Thus this set is a subspace.
(d) The natural generalization is that the inverse image of a subspace of is a subspace.
Suppose that X is a subspace of W. Note that ⃗0W ∈X so the set {⃗v ∈V
¯¯ h(⃗v) ∈X} is not
empty. To show that this set is closed under combinations, let ⃗v1, . . . ,⃗vn be elements of V such that
h(⃗v1) = ⃗x1, . . . , h(⃗vn) = ⃗xn and note that
h(c1 · ⃗v1 + · · · + cn · ⃗vn) = c1 · h(⃗v1) + · · · + cn · h(⃗vn) = c1 · ⃗x1 + · · · + cn · ⃗xn
so a linear combination of elements of h−1(X) is also in h−1(X).
Three.II.1.39
No; the set of isomorphisms does not contain the zero map (unless the space is trivial).
Three.II.1.40
If ⟨⃗β1, . . . , ⃗βn⟩doesn’t span the space then the map needn’t be unique. For instance,
if we try to deﬁne a map from R2 to itself by specifying only that ⃗e1 is sent to itself, then there is
more than one homomorphism possible; both the identity map and the projection map onto the ﬁrst
component ﬁt this condition.
If we drop the condition that ⟨⃗β1, . . . , ⃗βn⟩is linearly independent then we risk an inconsistent
speciﬁcation (i.e, there could be no such map). An example is if we consider ⟨⃗e2,⃗e1, 2⃗e1⟩, and try
to deﬁne a map from R2 to itself that sends ⃗e2 to itself, and sends both ⃗e1 and 2⃗e1 to ⃗e1.
No
homomorphism can satisfy these three conditions.
Three.II.1.41
(a) Brieﬂy, the check of linearity is this.
F(r1 · ⃗v1 + r2 · ⃗v2) =
µ
f1(r1⃗v1 + r2⃗v2)
f2(r1⃗v1 + r2⃗v2)
¶
= r1
µ
f1(⃗v1)
f2(⃗v1)
¶
+ r2
µ
f1(⃗v2)
f2(⃗v2)
¶
= r1 · F(⃗v1) + r2 · F(⃗v2)
(b) Yes. Let π1 : R2 →R1 and π2 : R2 →R1 be the projections
µx
y
¶
π1
7−→x
and
µx
y
¶
π2
7−→y
onto the two axes.
Now, where f1(⃗v) = π1(F(⃗v)) and f2(⃗v) = π2(F(⃗v)) we have the desired
component functions.
F(⃗v) =
µ
f1(⃗v)
f2(⃗v)
¶
They are linear because they are the composition of linear functions, and the fact that the compois-
tion of linear functions is linear was shown as part of the proof that isomorphism is an equivalence
relation (alternatively, the check that they are linear is straightforward).
(c) In general, a map from a vector space V to an Rn is linear if and only if each of the component
functions is linear. The veriﬁcation is as in the prior item.

Answers to Exercises
93
Subsection Three.II.2: Rangespace and Nullspace
Three.II.2.22
First, to answer whether a polynomial is in the nullspace, we have to consider it as a
member of the domain P3. To answer whether it is in the rangespace, we consider it as a member of
the codomain P4. That is, for p(x) = x4, the question of whether it is in the rangespace is sensible but
the question of whether it is in the nullspace is not because it is not even in the domain.
(a) The polynomial x3 ∈P3 is not in the nullspace because h(x3) = x4 is not the zero polynomial in
P4. The polynomial x3 ∈P4 is in the rangespace because x2 ∈P3 is mapped by h to x3.
(b) The answer to both questions is, “Yes, because h(0) = 0.” The polynomial 0 ∈P3 is in the
nullspace because it is mapped by h to the zero polynomial in P4. The polynomial 0 ∈P4 is in the
rangespace because it is the image, under h, of 0 ∈P3.
(c) The polynomial 7 ∈P3 is not in the nullspace because h(7) = 7x is not the zero polynomial in
P4. The polynomial x3 ∈P4 is not in the rangespace because there is no member of the domain
that when multiplied by x gives the constant polynomial p(x) = 7.
(d) The polynomial 12x −0.5x3 ∈P3 is not in the nullspace because h(12x −0.5x3) = 12x2 −0.5x4.
The polynomial 12x −0.5x3 ∈P4 is in the rangespace because it is the image of 12 −0.5x2.
(e) The polynomial 1 + 3x2 −x3 ∈P3 is not in the nullspace because h(1 + 3x2 −x3) = x + 3x3 −x4.
The polynomial 1 + 3x2 −x3 ∈P4 is not in the rangespace because of the constant term.
Three.II.2.23
(a) The nullspace is
N (h) = {
µ
a
b
¶
∈R2 ¯¯ a + ax + ax2 + 0x3 = 0 + 0x + 0x2 + 0x3} = {
µ
0
b
¶ ¯¯ b ∈R}
while the rangespace is
R(h) = {a + ax + ax2 ∈P3
¯¯ a, b ∈R} = {a · (1 + x + x2)
¯¯ a ∈R}
and so the nullity is one and the rank is one.
(b) The nullspace is this.
N (h) = {
µa
b
c
d
¶ ¯¯ a + d = 0} = {
µ−d
b
c
d
¶ ¯¯ b, c, d ∈R}
The rangespace
R(h){a + d
¯¯ a, b, c, d ∈R}
is all of R (we can get any real number by taking d to be 0 and taking a to be the desired number).
Thus, the nullity is three and the rank is one.
(c) The nullspace is
N (h) = {
µ
a
b
c
d
¶ ¯¯ a + b + c = 0 and d = 0} = {
µ
−b −c
b
c
0
¶ ¯¯ b, c ∈R}
while the rangespace is R(h) = {r + sx2 ¯¯ r, s ∈R}. Thus, the nullity is two and the rank is two.
(d) The nullspace is all of R3 so the nullity is three. The rangespace is the trivial subspace of R4 so
the rank is zero.
Three.II.2.24
For each, use the result that the rank plus the nullity equals the dimension of the
domain.
(a) 0
(b) 3
(c) 3
(d) 0
Three.II.2.25
Because
d
dx (a0 + a1x + · · · + anxn) = a1 + 2a2x + 3a3x2 + · · · + nanxn−1
we have this.
N ( d
dx) = {a0 + · · · + anxn ¯¯ a1 + 2a2x + · · · + nanxn−1 = 0 + 0x + · · · + 0xn−1}
= {a0 + · · · + anxn ¯¯ a1 = 0, and a2 = 0, . . . , an = 0}
= {a0 + 0x + 0x2 + · · · + 0xn ¯¯ a0 ∈R}
In the same way,
N ( dk
dxk ) = {a0 + a1x + · · · + anxn ¯¯ a0, . . . , ak−1 ∈R}
for k ≤n.

94
Linear Algebra, by Hefferon
Three.II.2.26
The shadow of a scalar multiple is the scalar multiple of the shadow.
Three.II.2.27
(a) Setting a0 + (a0 + a1)x + (a2 + a3)x3 = 0 + 0x + 0x2 + 0x3 gives a0 = 0 and
a0 + a1 = 0 and a2 + a3 = 0, so the nullspace is {−a3x2 + a3x3 ¯¯ a3 ∈R}.
(b) Setting a0 + (a0 + a1)x + (a2 + a3)x3 = 2 + 0x + 0x2 −x3 gives that a0 = 2, and a1 = −2,
and a2 + a3 = −1. Taking a3 as a parameter, and renaming it a3 = a gives this set description
{2 −2x + (−1 −a)x2 + ax3 ¯¯ a ∈R} = {(2 −2x −x2) + a · (−x2 + x3)
¯¯ a ∈R}.
(c) This set is empty because the range of h includes only those polynomials with a 0x2 term.
Three.II.2.28
All inverse images are lines with slope −2.
2x + y = 1
2x + y = −3
2x + y = 0
Three.II.2.29
These are the inverses.
(a) a0 + a1x + a2x2 + a3x3 7→a0 + a1x + (a2/2)x2 + (a3/3)x3
(b) a0 + a1x + a2x2 + a3x3 7→a0 + a2x + a1x2 + a3x3
(c) a0 + a1x + a2x2 + a3x3 7→a3 + a0x + a1x2 + a2x3
(d) a0 + a1x + a2x2 + a3x3 7→a0 + (a1 −a0)x + (a2 −a1)x2 + (a3 −a2)x3
For instance, for the second one, the map given in the question sends 0 + 1x + 2x2 + 3x3 7→0 + 2x +
1x2 + 3x3 and then the inverse above sends 0 + 2x + 1x2 + 3x3 7→0 + 1x + 2x2 + 3x3. So this map is
actually self-inverse.
Three.II.2.30
For any vector space V , the nullspace
{⃗v ∈V
¯¯ 2⃗v = ⃗0}
is trivial, while the rangespace
{⃗w ∈V
¯¯ ⃗w = 2⃗v for some ⃗v ∈V }
is all of V , because every vector ⃗w is twice some other vector, speciﬁcally, it is twice (1/2)⃗w. (Thus,
this transformation is actually an automorphism.)
Three.II.2.31
Because the rank plus the nullity equals the dimension of the domain (here, ﬁve), and
the rank is at most three, the possible pairs are: (3, 2), (2, 3), (1, 4), and (0, 5). Coming up with linear
maps that show that each pair is indeed possible is easy.
Three.II.2.32
No (unless Pn is trivial), because the two polynomials f0(x) = 0 and f1(x) = 1 have
the same derivative; a map must be one-to-one to have an inverse.
Three.II.2.33
The nullspace is this.
{a0 + a1x + · · · + anxn ¯¯ a0(1) + a12 (12) + · · · +
an
n + 1(1n+1) = 0}
= {a0 + a1x + · · · + anxn ¯¯ a0 + (a1/2) + · · · + (an+1/n + 1) = 0}
Thus the nullity is n.
Three.II.2.34
(a) One direction is obvious: if the homomorphism is onto then its range is the
codomain and so its rank equals the dimension of its codomain. For the other direction assume
that the map’s rank equals the dimension of the codomain. Then the map’s range is a subspace of
the codomain, and has dimension equal to the dimension of the codomain. Therefore, the map’s
range must equal the codomain, and the map is onto. (The ‘therefore’ is because there is a linearly
independent subset of the range that is of size equal to the dimension of the codomain, but any such
linearly independent subset of the codomain must be a basis for the codomain, and so the range
equals the codomain.)
(b) By Theorem 2.21, a homomorphism is one-to-one if and only if its nullity is zero. Because rank
plus nullity equals the dimension of the domain, it follows that a homomorphism is one-to-one if
and only if its rank equals the dimension of its domain. But this domain and codomain have the
same dimension, so the map is one-to-one if and only if it is onto.
Three.II.2.35
We are proving that h: V →W is nonsingular if and only if for every linearly indepen-
dent subset S of V the subset h(S) = {h(⃗s)
¯¯ ⃗s ∈S} of W is linearly independent.

Answers to Exercises
95
One half is easy — by Theorem 2.21, if h is singular then its nullspace is nontrivial (contains more
than just the zero vector). So, where ⃗v ̸= ⃗0V is in that nullspace, the singleton set {⃗v } is independent
while its image {h(⃗v)} = {⃗0W } is not.
For the other half, assume that h is nonsingular and so by Theorem 2.21 has a trivial nullspace.
Then for any ⃗v1, . . . ,⃗vn ∈V , the relation
⃗0W = c1 · h(⃗v1) + · · · + cn · h(⃗vn) = h(c1 · ⃗v1 + · · · + cn · ⃗vn)
implies the relation c1 · ⃗v1 + · · · + cn · ⃗vn = ⃗0V . Hence, if a subset of V is independent then so is its
image in W.
Remark. The statement is that a linear map is nonsingular if and only if it preserves independence
for all sets (that is, if a set is independent then its image is also independent). A singular map may
well preserve some independent sets. An example is this singular map from R3 to R2.


x
y
z

7→
µ
x + y + z
0
¶
Linear independence is preserved for this set
{


1
0
0

} 7→{
µ
1
0
¶
}
and (in a somewhat more tricky example) also for this set
{


1
0
0

,


0
1
0

} 7→{
µ
1
0
¶
}
(recall that in a set, repeated elements do not appear twice). However, there are sets whose indepen-
dence is not preserved under this map;
{


1
0
0

,


0
2
0

} 7→{
µ
1
0
¶
,
µ
2
0
¶
}
and so not all sets have independence preserved.
Three.II.2.36
(We use the notation from Theorem 1.9.) Fix a basis ⟨⃗β1, . . . , ⃗βn⟩for V and a basis
⟨⃗w1, . . . , ⃗wk⟩for W. If the dimension k of W is less than or equal to the dimension n of V then the
theorem gives a linear map from V to W determined in this way.
⃗β1 7→⃗w1, . . . , ⃗βk 7→⃗wk
and
⃗βk+1 7→⃗wk, . . . , ⃗βn 7→⃗wk
We need only to verify that this map is onto.
Any member of W can be written as a linear combination of basis elements c1 · ⃗w1 + · · · + ck · ⃗wk.
This vector is the image, under the map described above, of c1 · ⃗β1 + · · · + ck · ⃗βk + 0 · ⃗βk+1 · · · + 0 · ⃗βn.
Thus the map is onto.
Three.II.2.37
By assumption, h is not the zero map and so a vector ⃗v ∈V exists that is not in the
nullspace. Note that ⟨h(⃗v)⟩is a basis for R, because it is a size one linearly independent subset of R.
Consequently h is onto, as for any r ∈R we have r = c · h(⃗v) for some scalar c, and so r = h(c⃗v).
Thus the rank of h is one. Because the nullity is given as n, the dimension of the domain of h (the
vector space V ) is n + 1. We can ﬁnish by showing {⃗v, ⃗β1, . . . , ⃗βn} is linearly independent, as it is a
size n + 1 subset of a dimension n + 1 space. Because {⃗β1, . . . , ⃗βn} is linearly independent we need
only show that ⃗v is not a linear combination of the other vectors. But c1⃗β1 + · · · + cn⃗βn = ⃗v would
give −⃗v + c1⃗β1 + · · · + cn⃗βn = ⃗0 and applying h to both sides would give a contradiction.
Three.II.2.38
Yes. For the transformation of R2 given by
µx
y
¶
h
7−→
µ0
x
¶
we have this.
N (h) = {
µ
0
y
¶ ¯¯ y ∈R} = R(h)
Remark. We will see more of this in the ﬁfth chapter.

96
Linear Algebra, by Hefferon
Three.II.2.39
This is a simple calculation.
h([S]) = {h(c1⃗s1 + · · · + cn⃗sn)
¯¯ c1, . . . , cn ∈R and ⃗s1, . . . ,⃗sn ∈S}
= {c1h(⃗s1) + · · · + cnh(⃗sn)
¯¯ c1, . . . , cn ∈R and ⃗s1, . . . ,⃗sn ∈S}
= [h(S)]
Three.II.2.40
(a) We will show that the two sets are equal h−1(⃗w) = {⃗v + ⃗n
¯¯ ⃗n ∈N (h)} by mutual
inclusion.
For the {⃗v + ⃗n
¯¯ ⃗n ∈N (h)} ⊆h−1(⃗w) direction, just note that h(⃗v + ⃗n) = h(⃗v) +
h(⃗n) equals ⃗w, and so any member of the ﬁrst set is a member of the second. For the h−1(⃗w) ⊆
{⃗v + ⃗n
¯¯ ⃗n ∈N (h)} direction, consider ⃗u ∈h−1(⃗w). Because h is linear, h(⃗u) = h(⃗v) implies that
h(⃗u −⃗v) = ⃗0. We can write ⃗u −⃗v as ⃗n, and then we have that ⃗u ∈{⃗v + ⃗n
¯¯ ⃗n ∈N (h)}, as desired,
because ⃗u = ⃗v + (⃗u −⃗v).
(b) This check is routine.
(c) This is immediate.
(d) For the linearity check, brieﬂy, where c, d are scalars and ⃗x, ⃗y ∈Rn have components x1, . . . , xn
and y1, . . . , yn, we have this.
h(c · ⃗x + d · ⃗y) =



a1,1(cx1 + dy1) + · · · + a1,n(cxn + dyn)
...
am,1(cx1 + dy1) + · · · + am,n(cxn + dyn)



=



a1,1cx1 + · · · + a1,ncxn
...
am,1cx1 + · · · + am,ncxn


+



a1,1dy1 + · · · + a1,ndyn
...
am,1dy1 + · · · + am,ndyn



= c · h(⃗x) + d · h(⃗y)
The appropriate conclusion is that General = Particular + Homogeneous.
(e) Each power of the derivative is linear because of the rules
dk
dxk (f(x) + g(x)) = dk
dxk f(x) + dk
dxk g(x)
and
dk
dxk rf(x) = r dk
dxk f(x)
from calculus.
Thus the given map is a linear transformation of the space because any linear
combination of linear maps is also a linear map by Lemma 1.16. The appropriate conclusion is
General = Particular + Homogeneous, where the associated homogeneous diﬀerential equation has
a constant of 0.
Three.II.2.41
Because the rank of t is one, the rangespace of t is a one-dimensional set. Taking ⟨h(⃗v)⟩
as a basis (for some appropriate ⃗v), we have that for every ⃗w ∈V , the image h(⃗w) ∈V is a multiple
of this basis vector — associated with each ⃗w there is a scalar c⃗w such that t(⃗w) = c⃗wt(⃗v). Apply t to
both sides of that equation and take r to be ct(⃗v)
t ◦t(⃗w) = t(c⃗w · t(⃗v)) = c⃗w · t ◦t(⃗v) = c⃗w · ct(⃗v) · t(⃗v) = c⃗w · r · t(⃗v) = r · c⃗w · t(⃗v) = r · t(⃗w)
to get the desired conclusion.
Three.II.2.42
Fix a basis ⟨⃗β1, . . . , ⃗βn⟩for V . We shall prove that this map
h
Φ
7−→



h(⃗β1)
...
h(⃗βn)



is an isomorphism from V ∗to Rn.
To see that Φ is one-to-one, assume that h1 and h2 are members of V ∗such that Φ(h1) = Φ(h2).
Then



h1(⃗β1)
...
h1(⃗βn)


=



h2(⃗β1)
...
h2(⃗βn)



and consequently, h1(⃗β1) = h2(⃗β1), etc. But a homomorphism is determined by its action on a basis,
so h1 = h2, and therefore Φ is one-to-one.
To see that Φ is onto, consider



x1
...
xn




Answers to Exercises
97
for x1, . . . , xn ∈R. This function h from V to R
c1⃗β1 + · · · + cn⃗βn
h
7−→c1x1 + · · · + cnxn
is easily seen to be linear, and to be mapped by Φ to the given vector in Rn, so Φ is onto.
The map Φ also preserves structure: where
c1⃗β1 + · · · + cn⃗βn
h1
7−→c1h1(⃗β1) + · · · + cnh1(⃗βn)
c1⃗β1 + · · · + cn⃗βn
h2
7−→c1h2(⃗β1) + · · · + cnh2(⃗βn)
we have
(r1h1 + r2h2)(c1⃗β1 + · · · + cn⃗βn) = c1(r1h1(⃗β1) + r2h2(⃗β1)) + · · · + cn(r1h1(⃗βn) + r2h2(⃗βn))
= r1(c1h1(⃗β1) + · · · + cnh1(⃗βn)) + r2(c1h2(⃗β1) + · · · + cnh2(⃗βn))
so Φ(r1h1 + r2h2) = r1Φ(h1) + r2Φ(h2).
Three.II.2.43
Let h: V →W be linear and ﬁx a basis ⟨⃗β1, . . . , ⃗βn⟩for V . Consider these n maps from
V to W
h1(⃗v) = c1 · h(⃗β1),
h2(⃗v) = c2 · h(⃗β2),
. . .
, hn(⃗v) = cn · h(⃗βn)
for any ⃗v = c1⃗β1 + · · · + cn⃗βn. Clearly h is the sum of the hi’s. We need only check that each hi is
linear: where ⃗u = d1⃗β1 + · · · + dn⃗βn we have hi(r⃗v + s⃗u) = rci + sdi = rhi(⃗v) + shi(⃗u).
Three.II.2.44
Either yes (trivially) or no (nearly trivially).
If V ‘is homomorphic to’ W is taken to mean there is a homomorphism from V into (but not
necessarily onto) W, then every space is homomorphic to every other space as a zero map always
exists.
If V ‘is homomorphic to’ W is taken to mean there is an onto homomorphism from V to W then the
relation is not an equivalence. For instance, there is an onto homomorphism from R3 to R2 (projection
is one) but no homomorphism from R2 onto R3 by Corollary 2.17, so the relation is not reﬂexive.∗
Three.II.2.45
That they form the chains is obvious. For the rest, we show here that R(tj+1) = R(tj)
implies that R(tj+2) = R(tj+1). Induction then applies.
Assume that R(tj+1) = R(tj).
Then t: R(tj+1) →R(tj+2) is the same map, with the same
domain, as t: R(tj) →R(tj+1). Thus it has the same range: R(tj+2) = R(tj+1).
Subsection Three.III.1: Representing Linear Maps with Matrices
Three.III.1.11
(a)


1 · 2 + 3 · 1 + 1 · 0
0 · 2 + (−1) · 1 + 2 · 0
1 · 2 + 1 · 1 + 0 · 0

=


5
−1
3


(b) Not deﬁned.
(c)


0
0
0


Three.III.1.12
(a)
µ
2 · 4 + 1 · 2
3 · 4 −(1/2) · 2
¶
=
µ
10
11
¶
(b)
µ
4
1
¶
(c) Not deﬁned.
Three.III.1.13
Matrix-vector multiplication gives rise to a linear system.
2x + y + z = 8
y + 3z = 4
x −y + 2z = 4
Gaussian reduction shows that z = 1, y = 1, and x = 3.
Three.III.1.14
Here are two ways to get the answer.
First, obviously 1 −3x + 2x2 = 1 · 1 −3 · x + 2 · x2, and so we can apply the general property of
preservation of combinations to get h(1−3x+2x2) = h(1·1−3·x+2·x2) = 1·h(1)−3·h(x)+2·h(x2) =
1 · (1 + x) −3 · (1 + 2x) + 2 · (x −x3) = −2 −3x −2x3.
The other way uses the computation scheme developed in this subsection. Because we know where
these elements of the space go, we consider this basis B = ⟨1, x, x2⟩for the domain. Arbitrarily, we
can take D = ⟨1, x, x2, x3⟩as a basis for the codomain. With those choices, we have that
RepB,D(h) =




1
1
0
1
2
1
0
0
0
0
0
−1




B,D
∗More information on equivalence relations is in the appendix.

98
Linear Algebra, by Hefferon
and, as
RepB(1 −3x + 2x2) =


1
−3
2


B
the matrix-vector multiplication calculation gives this.
RepD(h(1 −3x + 2x2)) =




1
1
0
1
2
1
0
0
0
0
0
−1




B,D


1
−3
2


B
=




−2
−3
0
−2




D
Thus, h(1 −3x + 2x2) = −2 · 1 −3 · x + 0 · x2 −2 · x3 = −2 −3x −2x3, as above.
Three.III.1.15
Again, as recalled in the subsection, with respect to Ei, a column vector represents
itself.
(a) To represent h with respect to E2, E3 we take the images of the basis vectors from the domain,
and represent them with respect to the basis for the codomain.
RepE3( h(⃗e1) ) = RepE3(


2
2
0

) =


2
2
0


RepE3( h(⃗e2) ) = RepE3(


0
1
−1

) =


0
1
−1


These are adjoined to make the matrix.
RepE2,E3(h) =


2
0
2
1
0
−1


(b) For any ⃗v in the domain R2,
RepE2(⃗v) = RepE2(
µ
v1
v2
¶
) =
µ
v1
v2
¶
and so
RepE3( h(⃗v) ) =


2
0
2
1
0
−1


µ
v1
v2
¶
=


2v1
2v1 + v2
−v2


is the desired representation.
Three.III.1.16
(a) We must ﬁrst ﬁnd the image of each vector from the domain’s basis, and then
represent that image with respect to the codomain’s basis.
RepB(d 1
dx ) =




0
0
0
0




RepB(d x
dx ) =




1
0
0
0




RepB(d x2
dx ) =




0
2
0
0




RepB(d x3
dx ) =




0
0
3
0




Those representations are then adjoined to make the matrix representing the map.
RepB,B( d
dx) =




0
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0




(b) Proceeding as in the prior item, we represent the images of the domain’s basis vectors
RepB(d 1
dx ) =




0
0
0
0




RepB(d x
dx ) =




1
0
0
0




RepB(d x2
dx ) =




0
1
0
0




RepB(d x3
dx ) =




0
0
1
0




and adjoin to make the matrix.
RepB,D( d
dx) =




0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0




Three.III.1.17
For each, we must ﬁnd the image of each of the domain’s basis vectors, represent
each image with respect to the codomain’s basis, and then adjoin those representations to get the
matrix.

Answers to Exercises
99
(a) The basis vectors from the domain have these images
1 7→0
x 7→1
x2 7→2x
. . .
and these images are represented with respect to the codomain’s basis in this way.
RepB(0) =









0
0
0
...









RepB(1) =









1
0
0
...









RepB(2x) =









0
2
0
...









. . .
RepB(nxn−1) =









0
0
0
...
n
0









The matrix
RepB,B( d
dx) =







0
1
0
. . .
0
0
0
2
. . .
0
...
0
0
0
. . .
n
0
0
0
. . .
0







has n + 1 rows and columns.
(b) Once the images under this map of the domain’s basis vectors are determined
1 7→x
x 7→x2/2
x2 7→x3/3
. . .
then they can be represented with respect to the codomain’s basis
RepBn+1(x) =







0
1
0
...







RepBn+1(x2/2) =







0
0
1/2
...







. . .
RepBn+1(xn+1/(n + 1)) =







0
0
0
...
1/(n + 1)







and put together to make the matrix.
RepBn,Bn+1(
Z
) =







0
0
. . .
0
0
1
0
. . .
0
0
0
1/2
. . .
0
0
...
0
0
. . .
0
1/(n + 1)







(c) The images of the basis vectors of the domain are
1 7→1
x 7→1/2
x2 7→1/3
. . .
and they are represented with respect to the codomain’s basis as
RepE1(1) = 1
RepE1(1/2) = 1/2
. . .
so the matrix is
RepB,E1(
Z
) =
¡
1
1/2
· · ·
1/n
1/(n + 1)
¢
(this is an 1×(n + 1) matrix).
(d) Here, the images of the domain’s basis vectors are
1 7→1
x 7→3
x2 7→9
. . .
and they are represented in the codomain as
RepE1(1) = 1
RepE1(3) = 3
RepE1(9) = 9
. . .
and so the matrix is this.
RepB,E1(
Z 1
0
) =
¡
1
3
9
· · ·
3n¢
(e) The images of the basis vectors from the domain are
1 7→1
x 7→x + 1 = 1 + x
x2 7→(x + 1)2 = 1 + 2x + x2
x3 7→(x + 1)3 = 1 + 3x + 3x2 + x3
. . .
which are represented as
RepB(1) =









1
0
0
0
...
0









RepB(1 + x) =









1
1
0
0
...
0









RepB(1 + 2x + x2) =









1
2
1
0
...
0









. . .

100
Linear Algebra, by Hefferon
The resulting matrix
RepB,B(
Z
) =








1
1
1
1
. . .
1
0
1
2
3
. . .
¡n
2
¢
0
0
1
3
. . .
¡n
3
¢
...
0
0
0
. . .
1








is Pascal’s triangle (recall that
¡n
r
¢
is the number of ways to choose r things, without order and
without repetition, from a set of size n).
Three.III.1.18
Where the space is n-dimensional,
RepB,B(id) =





1
0 . . .
0
0
1 . . .
0
...
0
0 . . .
1





B,B
is the n×n identity matrix.
Three.III.1.19
Taking this as the natural basis
B = ⟨⃗β1, ⃗β2, ⃗β3, ⃗β4⟩= ⟨
µ
1
0
0
0
¶
,
µ
0
1
0
0
¶
,
µ
0
0
1
0
¶
,
µ
0
0
0
1
¶
⟩
the transpose map acts in this way
⃗β1 7→⃗β1
⃗β2 7→⃗β3
⃗β3 7→⃗β2
⃗β4 7→⃗β4
so that representing the images with respect to the codomain’s basis and adjoining those column
vectors together gives this.
RepB,B(trans) =




1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1




B,B
Three.III.1.20
(a) With respect to the basis of the codomain, the images of the members of the
basis of the domain are represented as
RepB(⃗β2) =




0
1
0
0




RepB(⃗β3) =




0
0
1
0




RepB(⃗β4) =




0
0
0
1




RepB(⃗0) =




0
0
0
0




and consequently, the matrix representing the transformation is this.




0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
0




(b)




0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0




(c)




0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0




Three.III.1.21
(a) The picture of ds : R2 →R2 is this.
⃗u
⃗v
ds(⃗u)
ds(⃗v)
ds
−→

Answers to Exercises
101
This map’s eﬀect on the vectors in the standard basis for the domain is
µ
1
0
¶
ds
7−→
µ
s
0
¶
µ
0
1
¶
ds
7−→
µ
0
s
¶
and those images are represented with respect to the codomain’s basis (again, the standard basis)
by themselves.
RepE2(
µ
s
0
¶
) =
µ
s
0
¶
RepE2(
µ
0
s
¶
) =
µ
0
s
¶
Thus the representation of the dilation map is this.
RepE2,E2(ds) =
µ
s
0
0
s
¶
(b) The picture of fℓ: R2 →R2 is this.
fℓ
7−→
Some calculation (see Exercise I.29) shows that when the line has slope k
µ
1
0
¶
fℓ
7−→
µ
(1 −k2)/(1 + k2)
2k/(1 + k2)
¶
µ
0
1
¶
fℓ
7−→
µ
2k/(1 + k2)
−(1 −k2)/(1 + k2)
¶
(the case of a line with undeﬁned slope is separate but easy) and so the matrix representing reﬂection
is this.
RepE2,E2(fℓ) =
1
1 + k2 ·
µ
1 −k2
2k
2k
−(1 −k2)
¶
Three.III.1.22
Call the map t: R2 →R2.
(a) To represent this map with respect to the standard bases, we must ﬁnd, and then represent, the
images of the vectors ⃗e1 and ⃗e2 from the domain’s basis. The image of ⃗e1 is given.
One way to ﬁnd the image of ⃗e2 is by eye — we can see this.
µ
1
1
¶
−
µ
1
0
¶
=
µ
0
1
¶
t
7−→
µ
2
0
¶
−
µ
−1
0
¶
=
µ
3
0
¶
A more systemmatic way to ﬁnd the image of ⃗e2 is to use the given information to represent the
transformation, and then use that representation to determine the image. Taking this for a basis,
C = ⟨
µ1
1
¶
,
µ1
0
¶
⟩
the given information says this.
RepC,E2(t)
µ
2
−1
0
0
¶
As
RepC(⃗e2) =
µ
1
−1
¶
C
we have that
RepE2(t(⃗e2)) =
µ2
−1
0
0
¶
C,E2
µ 1
−1
¶
C
=
µ3
0
¶
E2
and consequently we know that t(⃗e2) = 3 · ⃗e1 (since, with respect to the standard basis, this vector
is represented by itself). Therefore, this is the representation of t with respect to E2, E2.
RepE2,E2(t) =
µ
−1
3
0
0
¶
E2,E2
(b) To use the matrix developed in the prior item, note that
RepE2(
µ
0
5
¶
) =
µ
0
5
¶
E2
and so we have this is the representation, with respect to the codomain’s basis, of the image of the
given vector.
RepE2(t(
µ
0
5
¶
)) =
µ
−1
3
0
0
¶
E2,E2
µ
0
5
¶
E2
=
µ
15
0
¶
E2
Because the codomain’s basis is the standard one, and so vectors in the codomain are represented
by themselves, we have this.
t(
µ
0
5
¶
) =
µ
15
0
¶

102
Linear Algebra, by Hefferon
(c) We ﬁrst ﬁnd the image of each member of B, and then represent those images with respect to
D. For the ﬁrst step, we can use the matrix developed earlier.
RepE2(
µ
1
−1
¶
) =
µ
−1
3
0
0
¶
E2,E2
µ
1
−1
¶
E2
=
µ
−4
0
¶
E2
so
t(
µ
1
−1
¶
) =
µ
−4
0
¶
Actually, for the second member of B there is no need to apply the matrix because the problem
statement gives its image.
t(
µ
1
1
¶
) =
µ
2
0
¶
Now representing those images with respect to D is routine.
RepD(
µ
−4
0
¶
) =
µ
−1
2
¶
D
and
RepD(
µ
2
0
¶
) =
µ
1/2
−1
¶
D
Thus, the matrix is this.
RepB,D(t) =
µ−1
1/2
2
−1
¶
B,D
(d) We know the images of the members of the domain’s basis from the prior item.
t(
µ 1
−1
¶
) =
µ−4
0
¶
t(
µ1
1
¶
) =
µ2
0
¶
We can compute the representation of those images with respect to the codomain’s basis.
RepB(
µ−4
0
¶
) =
µ−2
−2
¶
B
and
RepB(
µ2
0
¶
) =
µ1
1
¶
B
Thus this is the matrix.
RepB,B(t) =
µ
−2
1
−2
1
¶
B,B
Three.III.1.23
(a) The images of the members of the domain’s basis are
⃗β1 7→h(⃗β1)
⃗β2 7→h(⃗β2)
. . .
⃗βn 7→h(⃗βn)
and those images are represented with respect to the codomain’s basis in this way.
Reph(B)( h(⃗β1) ) =





1
0
...
0





Reph(B)( h(⃗β2) ) =





0
1
...
0





. . .
Reph(B)( h(⃗βn) ) =





0
0
...
1





Hence, the matrix is the identity.
RepB,h(B)(h) =





1
0
. . .
0
0
1
0
...
0
0
1





(b) Using the matrix in the prior item, the representation is this.
Reph(B)( h(⃗v) ) =



c1
...
cn



h(B)
Three.III.1.24
The product





h1,1
. . .
h1,i
. . .
h1,n
h2,1
. . .
h2,i
. . .
h2,n
...
hm,1
. . .
hm,i
. . .
h1,n













0
...
1
...
0








=





h1,i
h2,i
...
hm,i





gives the i-th column of the matrix.
Three.III.1.25
(a) The images of the basis vectors for the domain are cos x
d/dx
7−→−sin x and sin x
d/dx
7−→
cos x. Representing those with respect to the codomain’s basis (again, B) and adjoining the repre-
sentations gives this matrix.
RepB,B( d
dx) =
µ
0
1
−1
0
¶
B,B

Answers to Exercises
103
(b) The images of the vectors in the domain’s basis are ex d/dx
7−→ex and e2x d/dx
7−→2e2x. Representing
with respect to the codomain’s basis and adjoining gives this matrix.
RepB,B( d
dx) =
µ
1
0
0
2
¶
B,B
(c) The images of the members of the domain’s basis are 1
d/dx
7−→0, x
d/dx
7−→1, ex d/dx
7−→ex, and xex d/dx
7−→
ex + xex. Representing these images with respect to B and adjoining gives this matrix.
RepB,B( d
dx) =




0
1
0
0
0
0
0
0
0
0
1
1
0
0
0
1




B,B
Three.III.1.26
(a) It is the set of vectors of the codomain represented with respect to the codomain’s
basis in this way.
{
µ
1
0
0
0
¶ µ
x
y
¶ ¯¯ x, y ∈R} = {
µ
x
0
¶ ¯¯ x, y ∈R}
As the codomain’s basis is E2, and so each vector is represented by itself, the range of this transfor-
mation is the x-axis.
(b) It is the set of vectors of the codomain represented in this way.
{
µ
0
0
3
2
¶ µ
x
y
¶ ¯¯ x, y ∈R} = {
µ
0
3x + 2y
¶ ¯¯ x, y ∈R}
With respect to E2 vectors represent themselves so this range is the y axis.
(c) The set of vectors represented with respect to E2 as
{
µ
a
b
2a
2b
¶ µ
x
y
¶ ¯¯ x, y ∈R} = {
µ
ax + by
2ax + 2by
¶ ¯¯ x, y ∈R} = {(ax + by) ·
µ
1
2
¶ ¯¯ x, y ∈R}
is the line y = 2x, provided either a or b is not zero, and is the set consisting of just the origin if
both are zero.
Three.III.1.27
Yes, for two reasons.
First, the two maps h and ˆh need not have the same domain and codomain. For instance,
µ
1
2
3
4
¶
represents a map h: R2 →R2 with respect to the standard bases that sends
µ1
0
¶
7→
µ1
3
¶
and
µ0
1
¶
7→
µ2
4
¶
and also represents a ˆh: P1 →R2 with respect to ⟨1, x⟩and E2 that acts in this way.
1 7→
µ
1
3
¶
and
x 7→
µ
2
4
¶
The second reason is that, even if the domain and codomain of h and ˆh coincide, diﬀerent bases
produce diﬀerent maps. An example is the 2×2 identity matrix
I =
µ
1
0
0
1
¶
which represents the identity map on R2 with respect to E2, E2. However, with respect to E2 for the
domain but the basis D = ⟨⃗e2,⃗e1⟩for the codomain, the same matrix I represents the map that swaps
the ﬁrst and second components
µ
x
y
¶
7→
µ
y
x
¶
(that is, reﬂection about the line y = x).
Three.III.1.28
We mimic Example 1.1, just replacing the numbers with letters.
Write B as ⟨⃗β1, . . . , ⃗βn⟩and D as ⟨⃗δ1, . . . ,⃗δm⟩. By deﬁnition of representation of a map with respect
to bases, the assumption that
RepB,D(h) =



h1,1
. . .
h1,n
...
...
hm,1
. . .
hm,n




104
Linear Algebra, by Hefferon
means that h(⃗βi) = hi,1⃗δ1 + · · · + hi,n⃗δn. And, by the deﬁnition of the representation of a vector with
respect to a basis, the assumption that
RepB(⃗v) =



c1
...
cn



means that ⃗v = c1⃗β1 + · · · + cn⃗βn. Substituting gives
h(⃗v) = h(c1 · ⃗β1 + · · · + cn · ⃗βn)
= c1 · h(⃗β1) + · · · + cn · ⃗βn
= c1 · (h1,1⃗δ1 + · · · + hm,1⃗δm) + · · · + cn · (h1,n⃗δ1 + · · · + hm,n⃗δm)
= (h1,1c1 + · · · + h1,ncn) · ⃗δ1 + · · · + (hm,1c1 + · · · + hm,ncn) · ⃗δm
and so h(⃗v) is represented as required.
Three.III.1.29
(a) The picture is this.
The images of the vectors from the domain’s basis


1
0
0

7→


1
0
0




0
1
0

7→


0
cos θ
−sin θ




0
0
1

7→


0
sin θ
cos θ


are represented with respect to the codomain’s basis (again, E3) by themselves, so adjoining the
representations to make the matrix gives this.
RepE3,E3(rθ) =


1
0
0
0
cos θ
sin θ
0
−sin θ
cos θ


(b) The picture is similar to the one in the prior answer. The images of the vectors from the domain’s
basis


1
0
0

7→


cos θ
0
sin θ




0
1
0

7→


0
1
0




0
0
1

7→


−sin θ
0
cos θ


are represented with respect to the codomain’s basis E3 by themselves, so this is the matrix.


cos θ
0
−sin θ
0
1
0
sin θ
0
cos θ


(c) To a person standing up, with the vertical z-axis, a rotation of the xy-plane that is clockwise
proceeds from the positive y-axis to the positive x-axis. That is, it rotates opposite to the direction
in Example 1.8. The images of the vectors from the domain’s basis


1
0
0

7→


cos θ
−sin θ
0




0
1
0

7→


sin θ
cos θ
0




0
0
1

7→


0
0
1


are represented with respect to E3 by themselves, so the matrix is this.


cos θ
sin θ
0
−sin θ
cos θ
0
0
0
1


(d)




cos θ
sin θ
0
0
−sin θ
cos θ
0
0
0
0
1
0
0
0
0
1




Three.III.1.30
(a) Write BU as ⟨⃗β1, . . . , ⃗βk⟩and then BV as ⟨⃗β1, . . . , ⃗βk, ⃗βk+1, . . . , ⃗βn⟩. If
RepBU (⃗v) =



c1
...
ck



so that ⃗v = c1 · ⃗β1 + · · · + ck · ⃗βk

Answers to Exercises
105
then,
RepBV (⃗v) =










c1
...
ck
0
...
0










because ⃗v = c1 · ⃗β1 + · · · + ck · ⃗βk + 0 · ⃗βk+1 + · · · + 0 · ⃗βn.
(b) We must ﬁrst decide what the question means. Compare h: V →W with its restriction to the
subspace h↾U : U →W. The rangespace of the restriction is a subspace of W, so ﬁx a basis Dh(U)
for this rangespace and extend it to a basis DV for W. We want the relationship between these two.
RepBV ,DV (h)
and
RepBU,Dh(U)(h↾U)
The answer falls right out of the prior item: if
RepBU,Dh(U)(h↾U) =



h1,1
. . .
h1,k
...
...
hp,1
. . .
hp,k



then the extension is represented in this way.
RepBV ,DV (h) =










h1,1
. . .
h1,k
h1,k+1
. . .
h1,n
...
...
hp,1
. . .
hp,k
hp,k+1
. . .
hp,n
0
. . .
0
hp+1,k+1
. . .
hp+1,n
...
...
0
. . .
0
hm,k+1
. . .
hm,n










(c) Take Wi to be the span of {h(⃗β1), . . . , h(⃗βi)}.
(d) Apply the answer from the second item to the third item.
(e) No. For instance πx : R2 →R2, projection onto the x axis, is represented by these two upper-
triangular matrices
RepE2,E2(πx) =
µ
1
0
0
0
¶
and
RepC,E2(πx) =
µ
0
1
0
0
¶
where C = ⟨⃗e2,⃗e1⟩.
Subsection Three.III.2: Any Matrix Represents a Linear Map
Three.III.2.9
(a) Yes; we are asking if there are scalars c1 and c2 such that
c1
µ
2
2
¶
+ c2
µ
1
5
¶
=
µ
1
−3
¶
which gives rise to a linear system
2c1 + c2 =
1
2c1 + 5c2 = −3
−ρ1+ρ2
−→
2c1 + c2 =
1
4c2 = −4
and Gauss’ method produces c2 = −1 and c1 = 1. That is, there is indeed such a pair of scalars and
so the vector is indeed in the column space of the matrix.
(b) No; we are asking if there are scalars c1 and c2 such that
c1
µ
4
2
¶
+ c2
µ
−8
−4
¶
=
µ
0
1
¶
and one way to proceed is to consider the resulting linear system
4c1 −8c2 = 0
2c1 −4c2 = 1
that is easily seen to have no solution. Another way to proceed is to note that any linear combination
of the columns on the left has a second component half as big as its ﬁrst component, but the vector
on the right does not meet that criterion.

106
Linear Algebra, by Hefferon
(c) Yes; we can simply observe that the vector is the ﬁrst column minus the second. Or, failing that,
setting up the relationship among the columns
c1


1
1
−1

+ c2


−1
1
−1

+ c3


1
−1
1

=


2
0
0


and considering the resulting linear system
c1 −c2 + c3 = 2
c1 + c2 −c3 = 0
−c1 −c2 + c3 = 0
−ρ1+ρ2
−→
ρ1+ρ3
c1 −
c2 + c3 =
2
2c2 −2c3 = −2
−2c2 + 2c3 =
2
ρ2+ρ3
−→
c1 −c2 + c3 =
2
2c2 −2c3 = −2
0 =
0
gives the additional information (beyond that there is at least one solution) that there are inﬁnitely
many solutions. Parametizing gives c2 = −1 + c3 and c1 = 1, and so taking c3 to be zero gives a
particular solution of c1 = 1, c2 = −1, and c3 = 0 (which is, of course, the observation made at the
start).
Three.III.2.10
As described in the subsection, with respect to the standard bases, representations are
transparent, and so, for instance, the ﬁrst matrix describes this map.


1
0
0

=


1
0
0


E3
7→
µ
1
0
¶
E2
=
µ
1
0
¶


0
1
0

7→
µ
1
1
¶


0
0
1

7→
µ
3
4
¶
So, for this ﬁrst one, we are asking whether thare are scalars such that
c1
µ
1
0
¶
+ c2
µ
1
1
¶
+ c3
µ
3
4
¶
=
µ
1
3
¶
that is, whether the vector is in the column space of the matrix.
(a) Yes. We can get this conclusion by setting up the resulting linear system and applying Gauss’
method, as usual. Another way to get it is to note by inspection of the equation of columns that
taking c3 = 3/4, and c1 = −5/4, and c2 = 0 will do. Still a third way to get this conclusion is to
note that the rank of the matrix is two, which equals the dimension of the codomain, and so the
map is onto — the range is all of R2 and in particular includes the given vector.
(b) No; note that all of the columns in the matrix have a second component that is twice the ﬁrst,
while the vector does not. Alternatively, the column space of the matrix is
{c1
µ
2
4
¶
+ c2
µ
0
0
¶
+ c3
µ
3
6
¶ ¯¯ c1, c2, c3 ∈R} = {c
µ
1
2
¶ ¯¯ c ∈R}
(which is the fact already noted, but was arrived at by calculation rather than inspiration), and the
given vector is not in this set.
Three.III.2.11
(a) The ﬁrst member of the basis
µ
0
1
¶
=
µ
1
0
¶
B
is mapped to
µ
1/2
−1/2
¶
D
which is this member of the codomain.
1
2 ·
µ
1
1
¶
−1
2 ·
µ
1
−1
¶
=
µ
0
1
¶
(b) The second member of the basis is mapped
µ
1
0
¶
=
µ
0
1
¶
B
7→
µ
(1/2
1/2
¶
D
to this member of the codomain.
1
2 ·
µ
1
1
¶
+ 1
2 ·
µ
1
−1
¶
=
µ
1
0
¶
(c) Because the map that the matrix represents is the identity map on the basis, it must be the
identity on all members of the domain. We can come to the same conclusion in another way by
considering
µ
x
y
¶
=
µ
y
x
¶
B

Answers to Exercises
107
which is mapped to
µ(x + y)/2
(x −y)/2
¶
D
which represents this member of R2.
x + y
2
·
µ
1
1
¶
+ x −y
2
·
µ
1
−1
¶
=
µ
x
y
¶
Three.III.2.12
A general member of the domain, represented with respect to the domain’s basis as
a cos θ + b sin θ =
µ
a
a + b
¶
B
is mapped to
µ
0
a
¶
D
representing
0 · (cos θ + sin θ) + a · (cos θ)
and so the linear map represented by the matrix with respect to these bases
a cos θ + b sin θ 7→a cos θ
is projection onto the ﬁrst component.
Three.III.2.13
Denote the given basis of P2 by B. Then application of the linear map is represented
by matrix-vector addition. Thus, the ﬁrst vector in E3 is mapped to the element of P2 represented
with respect to B by


1
3
0
0
1
0
1
0
1




1
0
0

=


1
0
1


and that element is 1 + x. The other two images of basis vectors are calculated similarly.


1
3
0
0
1
0
1
0
1




0
1
0

=


3
1
0

= RepB(4 + x2)


1
3
0
0
1
0
1
0
1




0
0
1

=


0
0
1

= RepB(x)
We can thus decide if 1 + 2x is in the range of the map by looking for scalars c1, c2, and c3 such that
c1 · (1) + c2 · (1 + x2) + c3 · (x) = 1 + 2x
and obviously c1 = 1, c2 = 0, and c3 = 1 suﬃce. Thus it is in the range, and in fact it is the image of
this vector.
1 ·


1
0
0

+ 0 ·


0
1
0

+ 1 ·


0
0
1


Three.III.2.14
Let the matrix be G, and suppose that it rperesents g: V →W with respect to bases
B and D. Because G has two columns, V is two-dimensional. Because G has two rows, W is two-
dimensional. The action of g on a general member of the domain is this.
µx
y
¶
B
7→
µ x + 2y
3x + 6y
¶
D
(a) The only representation of the zero vector in the codomain is
RepD(⃗0) =
µ
0
0
¶
D
and so the set of representations of members of the nullspace is this.
{
µ
x
y
¶
B
¯¯ x + 2y = 0 and 3x + 6y = 0} = {y ·
µ
−1/2
1
¶
D
¯¯ y ∈R}
(b) The representation map RepD : W →R2 and its inverse are isomorphisms, and so preserve the
dimension of subspaces. The subspace of R2 that is in the prior item is one-dimensional. Therefore,
the image of that subspace under the inverse of the representation map — the nullspace of G, is also
one-dimensional.
(c) The set of representations of members of the rangespace is this.
{
µ
x + 2y
3x + 6y
¶
D
¯¯ x, y ∈R} = {k ·
µ
1
3
¶
D
¯¯ k ∈R}
(d) Of course, Theorem 2.3 gives that the rank of the map equals the rank of the matrix, which is
one. Alternatively, the same argument that was used above for the nullspace gives here that the
dimension of the rangespace is one.

108
Linear Algebra, by Hefferon
(e) One plus one equals two.
Three.III.2.15
No, the rangespaces may diﬀer. Example 2.2 shows this.
Three.III.2.16
Recall that the represention map
V
RepB
7−→Rn
is an isomorphism.
Thus, its inverse map Rep−1
B : Rn →V is also an isomorphism.
The desired
transformation of Rn is then this composition.
Rn Rep−1
B
7−→V
RepD
7−→Rn
Because a composition of isomorphisms is also an isomorphism, this map RepD ◦Rep−1
B is an isomor-
phism.
Three.III.2.17
Yes. Consider
H =
µ
1
0
0
1
¶
representing a map from R2 to R2. With respect to the standard bases B1 = E2, D1 = E2 this matrix
represents the identity map. With respect to
B2 = D2 = ⟨
µ1
1
¶
,
µ 1
−1
¶
⟩
this matrix again represents the identity. In fact, as long as the starting and ending bases are equal —
as long as Bi = Di — then the map represented by H is the identity.
Three.III.2.18
This is immediate from Corollary 2.6.
Three.III.2.19
The ﬁrst map
µ
x
y
¶
=
µ
x
y
¶
E2
7→
µ
3x
2y
¶
E2
=
µ
3x
2y
¶
stretches vectors by a factor of three in the x direction and by a factor of two in the y direction. The
second map
µ
x
y
¶
=
µ
x
y
¶
E2
7→
µ
x
0
¶
E2
=
µ
x
0
¶
projects vectors onto the x axis. The third
µ
x
y
¶
=
µ
x
y
¶
E2
7→
µ
y
x
¶
E2
=
µ
y
x
¶
interchanges ﬁrst and second components (that is, it is a reﬂection about the line y = x). The last
µ
x
y
¶
=
µ
x
y
¶
E2
7→
µ
x + 3y
y
¶
E2
=
µ
x + 3y
y
¶
stretches vectors parallel to the y axis, by an amount equal to three times their distance from that
axis (this is a skew.)
Three.III.2.20
(a) This is immediate from Theorem 2.3.
(b) Yes. This is immediate from the prior item.
To give a speciﬁc example, we can start with E3 as the basis for the domain, and then we require
a basis D for the codomain R3. The matrix H gives the action of the map as this


1
0
0

=


1
0
0


E3
7→


1
2
0


D


0
1
0

=


0
1
0


E3
7→


0
0
1


D


0
0
1

=


0
0
1


E3
7→


0
0
0


D
and there is no harm in ﬁnding a basis D so that
RepD(


1
0
0

) =


1
2
0


D
and
RepD(


0
1
0

) =


0
0
1


D
that is, so that the map represented by H with respect to E3, D is projection down onto the xy plane.
The second condition gives that the third member of D is ⃗e2. The ﬁrst condition gives that the ﬁrst
member of D plus twice the second equals ⃗e1, and so this basis will do.
D = ⟨


0
−1
0

,


1/2
1/2
0

,


0
1
0

⟩

Answers to Exercises
109
Three.III.2.21
(a) Recall that the representation map RepB : V →Rn is linear (it is actually an
isomorphism, but we do not need that it is one-to-one or onto here). Considering the column vector
x to be a n×1 matrix gives that the map from Rn to R that takes a column vector to its dot product
with ⃗x is linear (this is a matrix-vector product and so Theorem 2.1 applies). Thus the map under
consideration h⃗x is linear because it is the composistion of two linear maps.
⃗v 7→RepB(⃗v) 7→⃗x · RepB(⃗v)
(b) Any linear map g: V →R is represented by some matrix
¡
g1
g2
· · ·
gn
¢
(the matrix has n columns because V is n-dimensional and it has only one row because R is one-
dimensional). Then taking ⃗x to be the column vector that is the transpose of this matrix
⃗x =



g1
...
gn



has the desired action.
⃗v =



v1
...
vn


7→



g1
...
gn






v1
...
vn


= g1v1 + · · · + gnvn
(c) No. If ⃗x has any nonzero entries then h⃗x cannot be the zero map (and if ⃗x is the zero vector then
h⃗x can only be the zero map).
Three.III.2.22
See the following section.
Subsection Three.IV.1: Sums and Scalar Products
Three.IV.1.7
(a)
µ
7
0
6
9
1
6
¶
(b)
µ
12
−6
−6
6
12
18
¶
(c)
µ
4
2
0
6
¶
(d)
µ
−1
28
2
1
¶
(e) Not deﬁned.
Three.IV.1.8
Represent the domain vector ⃗v ∈V and the maps g, h: V →W with respect to bases
B, D in the usual way.
(a) The representation of (g + h) (⃗v) = g(⃗v) + h(⃗v)
¡
(g1,1v1 + · · · + g1,nvn)⃗δ1 + · · · + (gm,1v1 + · · · + gm,nvn)⃗δm
¢
+
¡
(h1,1v1 + · · · + h1,nvn)⃗δ1 + · · · + (hm,1v1 + · · · + hm,nvn)⃗δm
¢
regroups
= ((g1,1 + h1,1)v1 + · · · + (g1,1 + h1,n)vn) · ⃗δ1 + · · · + ((gm,1 + hm,1)v1 + · · · + (gm,n + hm,n)vn) · ⃗δm
to the entry-by-entry sum of the representation of g(⃗v) and the representation of h(⃗v).
(b) The representation of (r · h) (⃗v) = r ·
¡
h(⃗v)
¢
r ·
¡
(h1,1v1 + h1,2v2 + · · · + h1,nvn)⃗δ1 + · · · + (hm,1v1 + hm,2v2 + · · · + hm,nvn)⃗δm
¢
= (rh1,1v1 + · · · + rh1,nvn) · ⃗δ1 + · · · + (rhm,1v1 + · · · + rhm,nvn) · ⃗δm
is the entry-by-entry multiple of r and the representation of h.
Three.IV.1.9
First, each of these properties is easy to check in an entry-by-entry way. For example,
writing
G =



g1,1
. . .
g1,n
...
...
gm,1
. . .
gm,n



H =



h1,1
. . .
h1,n
...
...
hm,1
. . .
hm,n



then, by deﬁnition we have
G + H =



g1,1 + h1,1
. . .
g1,n + h1,n
...
...
gm,1 + hm,1
. . .
gm,n + hm,n



H + G =



h1,1 + g1,1
. . .
h1,n + g1,n
...
...
hm,1 + gm,1
. . .
hm,n + gm,n



and the two are equal since their entries are equal gi,j + hi,j = hi,j + gi,j. That is, each of these is easy
to check by using Deﬁnition 1.3 alone.
However, each property is also easy to understand in terms of the represented maps, by applying
Theorem 1.5 as well as the deﬁnition.

110
Linear Algebra, by Hefferon
(a) The two maps g +h and h+g are equal because g(⃗v)+h(⃗v) = h(⃗v)+g(⃗v), as addition is commu-
tative in any vector space. Because the maps are the same, they must have the same representative.
(b) As with the prior answer, except that here we apply that vector space addition is associative.
(c) As before, except that here we note that g(⃗v) + z(⃗v) = g(⃗v) +⃗0 = g(⃗v).
(d) Apply that 0 · g(⃗v) = ⃗0 = z(⃗v).
(e) Apply that (r + s) · g(⃗v) = r · g(⃗v) + s · g(⃗v).
(f) Apply the prior two items with r = 1 and s = −1.
(g) Apply that r · (g(⃗v) + h(⃗v)) = r · g(⃗v) + r · h(⃗v).
(h) Apply that (rs) · g(⃗v) = r · (s · g(⃗v)).
Three.IV.1.10
For any V, W with bases B, D, the (appropriately-sized) zero matrix represents this
map.
⃗β1 7→0 · ⃗δ1 + · · · + 0 · ⃗δm
· · ·
⃗βn 7→0 · ⃗δ1 + · · · + 0 · ⃗δm
This is the zero map.
There are no other matrices that represent only one map. For, suppose that H is not the zero
matrix. Then it has a nonzero entry; assume that hi,j ̸= 0. With respect to bases B, D, it represents
h1 : V →W sending
⃗βj 7→h1,j⃗δ1 + · · · + hi,j⃗δi + · · · + hm,j⃗δm
and with respcet to B, 2 · D it also represents h2 : V →W sending
⃗βj 7→h1,j · (2⃗δ1) + · · · + hi,j · (2⃗δi) + · · · + hm,j · (2⃗δm)
(the notation 2·D means to double all of the members of D). These maps are easily seen to be unequal.
Three.IV.1.11
Fix bases B and D for V and W, and consider RepB,D : L(V, W) →Mm×n associating
each linear map with the matrix representing that map h 7→RepB,D(h). From the prior section we
know that (under ﬁxed bases) the matrices correspond to linear maps, so the representation map is
one-to-one and onto. That it preserves linear operations is Theorem 1.5.
Three.IV.1.12
Fix bases and represent the transformations with 2×2 matrices. The space of matrices
M2×2 has dimension four, and hence the above six-element set is linearly dependent. By the prior
exercise that extends to a dependence of maps.
(The misleading part is only that there are six
transformations, not ﬁve, so that we have more than we need to give the existence of the dependence.)
Three.IV.1.13
That the trace of a sum is the sum of the traces holds because both trace(H + G) and
trace(H) + trace(G) are the sum of h1,1 + g1,1 with h2,2 + g2,2, etc. For scalar multiplication we have
trace(r · H) = r · trace(H); the proof is easy. Thus the trace map is a homomorphism from Mn×n to
R.
Three.IV.1.14
(a) The i, j entry of (G + H)trans is gj,i + hj,i. That is also the i, j entry of Gtrans +
Htrans.
(b) The i, j entry of (r · H)trans is rhj,i, which is also the i, j entry of r · Htrans.
Three.IV.1.15
(a) For H + Htrans, the i, j entry is hi,j + hj,i and the j, i entry of is hj,i + hi,j. The
two are equal and thus H + Htrans is symmetric.
Every symmetric matrix does have that form, since it can be written H = (1/2) · (H + Htrans).
(b) The set of symmetric matrices is nonempty as it contains the zero matrix.
Clearly a scalar
multiple of a symmetric matrix is symmetric. A sum H +G of two symmetric matrices is symmetric
because hi,j + gi,j = hj,i + gj,i (since hi,j = hj,i and gi,j = gj,i). Thus the subset is nonempty and
closed under the inherited operations, and so it is a subspace.
Three.IV.1.16
(a) Scalar multiplication leaves the rank of a matrix unchanged except that multi-
plication by zero leaves the matrix with rank zero. (This follows from the ﬁrst theorem of the book,
that multiplying a row by a nonzero scalar doesn’t change the solution set of the associated linear
system.)
(b) A sum of rank n matrices can have rank less than n. For instance, for any matrix H, the sum
H + (−1) · H has rank zero.
A sum of rank n matrices can have rank greater than n. Here are rank one matrices that sum
to a rank two matrix.
µ
1
0
0
0
¶
+
µ
0
0
0
1
¶
=
µ
1
0
0
1
¶

Answers to Exercises
111
Subsection Three.IV.2: Matrix Multiplication
Three.IV.2.14
(a)
µ
0
15.5
0
−19
¶
(b)
µ
2
−1
−1
17
−1
−1
¶
(c) Not deﬁned.
(d)
µ
1
0
0
1
¶
Three.IV.2.15
(a)
µ
1
−2
10
4
¶
(b)
µ
1
−2
10
4
¶ µ
−2
3
−4
1
¶
=
µ
6
1
−36
34
¶
(c)
µ
−18
17
−24
16
¶
(d)
µ
1
−1
2
0
¶ µ
−18
17
−24
16
¶
=
µ
6
1
−36
34
¶
Three.IV.2.16
(a) Yes.
(b) Yes.
(c) No.
(d) No.
Three.IV.2.17
(a) 2×1
(b) 1×1
(c) Not deﬁned.
(d) 2×2
Three.IV.2.18
We have
h1,1 · (g1,1y1 + g1,2y2) + h1,2 · (g2,1y1 + g2,2y2) + h1,3 · (g3,1y1 + g3,2y2) = d1
h2,1 · (g1,1y1 + g1,2y2) + h2,2 · (g2,1y1 + g2,2y2) + h2,3 · (g3,1y1 + g3,2y2) = d2
which, after expanding and regrouping about the y’s yields this.
(h1,1g1,1 + h1,2g2,1 + h1,3g3,1)y1 + (h1,1g1,2 + h1,2g2,2 + h1,3g3,2)y2 = d1
(h2,1g1,1 + h2,2g2,1 + h2,3g3,1)y1 + (h2,1g1,2 + h2,2g2,2 + h2,3g3,2)y2 = d2
The starting system, and the system used for the substitutions, can be expressed in matrix language.
µh1,1
h1,2
h1,3
h2,1
h2,2
h2,3
¶ 

x1
x2
x3

= H


x1
x2
x3

=
µd1
d2
¶


g1,1
g1,2
g2,1
g2,2
g3,1
g3,2


µy1
y2
¶
= G
µy1
y2
¶
=


x1
x2
x3


With this, the substitution is ⃗d = H⃗x = H(G⃗y) = (HG)⃗y.
Three.IV.2.19
Technically, no. The dot product operation yields a scalar while the matrix product
yields a 1×1 matrix. However, we usually will ignore the distinction.
Three.IV.2.20
The action of d/dx on B is 1 7→0, x 7→1, x2 7→2x, . . . and so this is its (n+1)×(n+1)
matrix representation.
RepB,B( d
dx) =







0
1
0
0
0
0
2
0
...
0
0
0
n
0
0
0
0







The product of this matrix with itself is deﬁned because the matrix is square.







0
1
0
0
0
0
2
0
...
0
0
0
n
0
0
0
0







2
=









0
0
2
0
0
0
0
0
6
0
...
0
0
0
n(n −1)
0
0
0
0
0
0
0
0









The map so represented is the composition
p
d
dx
7−→d p
dx
d
dx
7−→d2 p
dx2
which is the second derivative operation.
Three.IV.2.21
It is true for all one-dimensional spaces. Let f and g be transformations of a one-
dimensional space. We must show that g ◦f (⃗v) = f ◦g (⃗v) for all vectors. Fix a basis B for the space
and then the transformations are represented by 1×1 matrices.
F = RepB,B(f) =
¡
f1,1
¢
G = RepB,B(g) =
¡
g1,1
¢
Therefore, the compositions can be represented as GF and FG.
GF = RepB,B(g ◦f) =
¡
g1,1f1,1
¢
FG = RepB,B(f ◦g) =
¡
f1,1g1,1
¢
These two matrices are equal and so the compositions have the same eﬀect on each vector in the space.
Three.IV.2.22
It would not represent linear map composition; Theorem 2.6 would fail.

112
Linear Algebra, by Hefferon
Three.IV.2.23
Each follows easily from the associated map fact. For instance, p applications of the
transformation h, following q applications, is simply p + q applications.
Three.IV.2.24
Although these can be done by going through the indices, they are best understood
in terms of the represented maps. That is, ﬁx spaces and bases so that the matrices represent linear
maps f, g, h.
(a) Yes; we have both r · (g ◦h) (⃗v) = r · g( h(⃗v) ) = (r · g) ◦h (⃗v) and g ◦(r · h) (⃗v) = g( r · h(⃗v) ) =
r · g(h(⃗v)) = r · (g ◦h) (⃗v) (the second equality holds because of the linearity of g).
(b) Both answers are yes. First, f◦(rg+sh) and r·(f◦g)+s·(f◦h) both send ⃗v to r·f(g(⃗v))+s·f(h(⃗v));
the calculation is as in the prior item (using the linearity of f for the ﬁrst one). For the other,
(rf + sg) ◦h and r · (f ◦h) + s · (g ◦h) both send ⃗v to r · f(h(⃗v)) + s · g(h(⃗v)).
Three.IV.2.25
We have not seen a map interpretation of the transpose operation, so we will verify
these by considering the entries.
(a) The i, j entry of GHtrans is the j, i entry of GH, which is the dot product of the j-th row of G
and the i-th column of H. The i, j entry of HtransGtrans is the dot product of the i-th row of Htrans
and the j-th column of Gtrans, which is the the dot product of the i-th column of H and the j-th
row of G. Dot product is commutative and so these two are equal.
(b) By the prior item each equals its transpose, e.g., (HHtrans)trans = HtranstransHtrans = HHtrans.
Three.IV.2.26
Consider rx, ry : R3 →R3 rotating all vectors π/2 radians counterclockwise about the
x and y axes (counterclockwise in the sense that a person whose head is at ⃗e1 or ⃗e2 and whose feet are
at the origin sees, when looking toward the origin, the rotation as counterclockwise).
Rotating rx ﬁrst and then ry is diﬀerent than rotating ry ﬁrst and then rx. In particular, rx(⃗e3) = −⃗e2
so ry ◦rx(⃗e3) = −⃗e2, while ry(⃗e3) = ⃗e1 so rx ◦ry(⃗e3) = ⃗e1, and hence the maps do not commute.
Three.IV.2.27
It doesn’t matter (as long as the spaces have the appropriate dimensions).
For associativity, suppose that F is m×r, that G is r ×n, and that H is n×k. We can take
any r dimensional space, any m dimensional space, any n dimensional space, and any k dimensional
space — for instance, Rr, Rm, Rn, and Rk will do. We can take any bases A, B, C, and D, for those
spaces. Then, with respect to C, D the matrix H represents a linear map h, with respect to B, C the
matrix G represents a g, and with respect to A, B the matrix F represents an f. We can use those
maps in the proof.
The second half is done similarly, except that G and H are added and so we must take them to
represent maps with the same domain and codomain.
Three.IV.2.28
(a) The product of rank n matrices can have rank less than or equal to n but not
greater than n.
To see that the rank can fall, consider the maps πx, πy : R2 →R2 projecting onto the axes. Each
is rank one but their composition πx◦πy, which is the zero map, is rank zero. That can be translated
over to matrices representing those maps in this way.
RepE2,E2(πx) · RepE2,E2(πy) =
µ
1
0
0
0
¶ µ
0
0
0
1
¶
=
µ
0
0
0
0
¶
To prove that the product of rank n matrices cannot have rank greater than n, we can apply the
map result that the image of a linearly dependent set is linearly dependent. That is, if h: V →W
and g: W →X both have rank n then a set in the range R(g ◦h) of size larger than n is the image
under g of a set in W of size larger than n and so is linearly dependent (since the rank of h is n).
Now, the image of a linearly dependent set is dependent, so any set of size larger than n in the range
is dependent. (By the way, observe that the rank of g was not mentioned. See the next part.)
(b) Fix spaces and bases and consider the associated linear maps f and g. Recall that the dimension
of the image of a map (the map’s rank) is less than or equal to the dimension of the domain, and
consider the arrow diagram.
V
f
7−→
R(f)
g
7−→
R(g ◦f)
First, the image of R(f) must have dimension less than or equal to the dimension of R(f), by the
prior sentence. On the other hand, R(f) is a subset of the domain of g, and thus its image has

Answers to Exercises
113
dimension less than or equal the dimension of the domain of g. Combining those two, the rank of a
composition is less than or equal to the minimum of the two ranks.
The matrix fact follows immediately.
Three.IV.2.29
The ‘commutes with’ relation is reﬂexive and symmetric. However, it is not transi-
tive: for instance, with
G =
µ
1
2
3
4
¶
H =
µ
1
0
0
1
¶
J =
µ
5
6
7
8
¶
G commutes with H and H commutes with J, but G does not commute with J.
Three.IV.2.30
(a) Either of these.


x
y
z


πx
7−→


x
0
0


πy
7−→


0
0
0




x
y
z


πy
7−→


0
y
0


πx
7−→


0
0
0


(b) The composition is the ﬁfth derivative map d5/dx5 on the space of fourth-degree polynomials.
(c) With respect to the natural bases,
RepE3,E3(πx) =


1
0
0
0
0
0
0
0
0


RepE3,E3(πy) =


0
0
0
0
1
0
0
0
0


and their product (in either order) is the zero matrix.
(d) Where B = ⟨1, x, x2, x3, x4⟩,
RepB,B( d2
dx2 ) =






0
0
2
0
0
0
0
0
6
0
0
0
0
0
12
0
0
0
0
0
0
0
0
0
0






RepB,B( d3
dx3 ) =






0
0
0
6
0
0
0
0
0
24
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0






and their product (in either order) is the zero matrix.
Three.IV.2.31
Note that (S + T)(S −T) = S2 −ST + TS −T 2, so a reasonable try is to look at
matrices that do not commute so that −ST and TS don’t cancel: with
S =
µ
1
2
3
4
¶
T =
µ
5
6
7
8
¶
we have the desired inequality.
(S + T)(S −T) =
µ
−56
−56
−88
−88
¶
S2 −T 2 =
µ
−60
−68
−76
−84
¶
Three.IV.2.32
Because the identity map acts on the basis B as ⃗β1 7→⃗β1, . . . , ⃗βn 7→⃗βn, the represen-
tation is this.







1
0
0
0
0
1
0
0
0
0
1
0
...
0
0
0
1







The second part of the question is obvious from Theorem 2.6.
Three.IV.2.33
Here are four solutions.
T =
µ
±1
0
0
±1
¶
Three.IV.2.34
(a) The vector space M2×2 has dimension four. The set {T 4, . . . , T, I} has ﬁve ele-
ments and thus is linearly dependent.
(b) Where T is n×n, generalizing the argument from the prior item shows that there is such a
polynomial of degree n2 or less, since {T n2, . . . , T, I} is a n2+1-member subset of the n2-dimensional
space Mn×n.
(c) First compute the powers
T 2 =
µ
1/2
−
√
3/2
√
3/2
1/2
¶
T 3 =
µ
0
−1
1
0
¶
T 4 =
µ
−1/2
−
√
3/2
√
3/2
−1/2
¶

114
Linear Algebra, by Hefferon
(observe that rotating by π/6 three times results in a rotation by π/2, which is indeed what T 3
represents). Then set c4T 4 + c3T 3 + c2T 2 + c1T + c0I equal to the zero matrix
µ
−1/2
−
√
3/2
√
3/2
−1/2
¶
c4 +
µ
0
−1
1
0
¶
c3 +
µ
1/2
−
√
3/2
√
3/2
1/2
¶
c2 +
µ√
3/2
−1/2
1/2
√
3/2
¶
c1 +
µ
1
0
0
1
¶
c0
=
µ
0
0
0
0
¶
to get this linear system.
−(1/2)c4
+
(1/2)c2 + (
√
3/2)c1 + c0 = 0
−(
√
3/2)c4 −c3 −(
√
3/2)c2 −
(1/2)c1
= 0
(
√
3/2)c4 + c3 + (
√
3/2)c2 +
(1/2)c1
= 0
−(1/2)c4
+
(1/2)c2 + (
√
3/2)c1 + c0 = 0
Apply Gaussian reduction.
−ρ1+ρ4
−→
ρ2+ρ3
−→
−(1/2)c4
+
(1/2)c2 + (
√
3/2)c1 + c0 = 0
−(
√
3/2)c4 −c3 −(
√
3/2)c2 −
(1/2)c1
= 0
0 = 0
0 = 0
−
√
3ρ1+ρ2
−→
−(1/2)c4
+ (1/2)c2 + (
√
3/2)c1 +
c0 = 0
−c3 −
√
3c2 −
2c1 −
√
3c0 = 0
0 = 0
0 = 0
Setting c4, c3, and c2 to zero makes c1 and c0 also come out to be zero so no degree one or degree
zero polynomial will do. Setting c4 and c3 to zero (and c2 to one) gives a linear system
(1/2) + (
√
3/2)c1 +
c0 = 0
−
√
3 −
2c1 −
√
3c0 = 0
that can be solved with c1 = −
√
3 and c0 = 1. Conclusion: the polynomial m(x) = x2 −
√
3x + 1 is
minimal for the matrix T.
Three.IV.2.35
The check is routine:
a0 + a1x + · · · + anxn
s
7−→a0x + a1x2 + · · · + anxn+1 d/dx
7−→a0 + 2a1x + · · · + (n + 1)anxn
while
a0 + a1x + · · · + anxn d/dx
7−→a1 + · · · + nanxn−1
s
7−→a1x + · · · + anxn
so that under the map (d/dx ◦s) −(s ◦d/dx) we have a0 + a1x + · · · + anxn 7→a0 + a1x + · · · + anxn.
Three.IV.2.36
(a) Tracing through the remark at the end of the subsection gives that the i, j entry
of (FG)H is this
s
X
t=1
¡ r
X
k=1
fi,kgk,t
¢
ht,j =
s
X
t=1
r
X
k=1
(fi,kgk,t)ht,j =
s
X
t=1
r
X
k=1
fi,k(gk,tht,j)
=
r
X
k=1
s
X
t=1
fi,k(gk,tht,j) =
r
X
k=1
fi,k
¡ s
X
t=1
gk,tht,j
¢
(the ﬁrst equality comes from using the distributive law to multiply through the h’s, the second
equality is the associative law for real numbers, the third is the commutative law for reals, and the
fourth equality follows on using the distributive law to factor the f’s out), which is the i, j entry of
F(GH).
(b) The k-th component of h(⃗v) is
n
X
j=1
hk,jvj
and so the i-th component of g ◦h (⃗v) is this
r
X
k=1
gi,k
¡ n
X
j=1
hk,jvj
¢
=
r
X
k=1
n
X
j=1
gi,khk,jvj =
r
X
k=1
n
X
j=1
(gi,khk,j)vj
=
n
X
j=1
r
X
k=1
(gi,khk,j)vj =
n
X
j=1
(
r
X
k=1
gi,khk,j) vj

Answers to Exercises
115
(the ﬁrst equality holds by using the distributive law to multiply the g’s through, the second equality
represents the use of associativity of reals, the third follows by commutativity of reals, and the fourth
comes from using the distributive law to factor the v’s out).
Subsection Three.IV.3: Mechanics of Matrix Multiplication
Three.IV.3.23
(a) The second matrix has its ﬁrst row multiplied by 3 and its second row multiplied
by 0.
µ
3
6
0
0
¶
(b) The second matrix has its ﬁrst row multiplied by 4 and its second row multiplied by 2.
µ
4
8
6
8
¶
(c) The second matrix undergoes the pivot operation of replacing the second row with −2 times the
ﬁrst row added to the second.
µ
1
2
1
0
¶
(d) The ﬁrst matrix undergoes the column operation of: the second column is replaced by −1 times
the ﬁrst column plus the second.
µ
1
1
3
1
¶
(e) The ﬁrst matrix has its columns swapped.µ2
1
4
3
¶
Three.IV.3.24
(a) The incidence matrix is this (e.g, the ﬁrst row shows that there is only one
connection including Burlington, the road to Winooski).






0
0
0
0
1
0
0
1
1
1
0
1
0
1
0
0
1
1
0
0
1
1
0
0
0






(b) Because these are two-way roads, any road connecting city i to city j gives a connection between
city j and city i.
(c) The square of the incidence matrix tells how cities are connected by trips involving two roads.
Three.IV.3.25
The pay due each person appears in the matrix product of the two arrays.
Three.IV.3.26
The product is the identity matrix (recall that cos2 θ + sin2 θ = 1). An explanation
is that the given matrix represents, with respect to the standard bases, a rotation in R2 of θ radians
while the transpose represents a rotation of −θ radians. The two cancel.
Three.IV.3.27
The set of diagonal matrices is nonempty as the zero matrix is diagonal. Clearly it is
closed under scalar multiples and sums. Therefore it is a subspace. The dimension is n; here is a basis.
{





1
0
. . .
0
0
...
0
0
0




, . . . ,





0
0
. . .
0
0
...
0
0
1




}
Three.IV.3.28
No. In P1, with respect to the unequal bases B = ⟨1, x⟩and D = ⟨1 + x, 1 −x⟩, the
identity transformation is represented by by this matrix.
RepB,D(id) =
µ
1/2
1/2
1/2
−1/2
¶
B,D

116
Linear Algebra, by Hefferon
Three.IV.3.29
For any scalar r and square matrix H we have (rI)H = r(IH) = rH = r(HI) =
(Hr)I = H(rI).
There are no other such matrices; here is an argument for 2×2 matrices that is easily extended to
n×n. If a matrix commutes with all others then it commutes with this unit matrix.
µ
0
a
0
c
¶
=
µ
a
b
c
d
¶ µ
0
1
0
0
¶
=
µ
0
1
0
0
¶ µ
a
b
c
d
¶
=
µ
c
d
0
0
¶
From this we ﬁrst conclude that the upper left entry a must equal its lower right entry d. We also
conclude that the lower left entry c is zero. The argument for the upper right entry b is similar.
Three.IV.3.30
It is false; these two don’t commute.
µ
1
0
0
0
¶
µ
0
0
1
0
¶
Three.IV.3.31
A permutation matrix has a single one in each row and column, and all its other entries
are zeroes. Fix such a matrix. Suppose that the i-th row has its one in its j-th column. Then no
other row has its one in the j-th column; every other row has a zero in the j-th column. Thus the dot
product of the i-th row and any other row is zero.
The i-th row of the product is made up of the dot products of the i-th row of the matrix and the
columns of the transpose. By the last paragraph, all such dot products are zero except for the i-th
one, which is one.
Three.IV.3.32
The generalization is to go from the ﬁrst and second rows to the i1-th and i2-th rows.
Row i of GH is made up of the dot products of row i of G and the columns of H. Thus if rows i1 and
i2 of G are equal then so are rows i1 and i2 of GH.
Three.IV.3.33
If the product of two diagonal matrices is deﬁned — if both are n×n — then the product
of the diagonals is the diagonal of the products: where G, H are equal-sized diagonal matrices, GH is
all zeros except each that i, i entry is gi,ihi,i.
Three.IV.3.34
One way to produce this matrix from the identity is to use the column operations of
ﬁrst multiplying the second column by three, and then adding the negative of the resulting second
column to the ﬁrst.
µ
1
0
0
1
¶
−→
µ
1
0
0
3
¶
−→
µ
1
0
−3
3
¶
Column operations, in contrast with row operations) are written from left to right, so doing the above
two operations is expressed with this matrix product.
µ1
0
0
3
¶ µ 1
0
−1
1
¶
Remark.
Alternatively, we could get the required matrix with row operations.
Starting with the
identity, ﬁrst adding the negative of the ﬁrst row to the second, and then multiplying the second row
by three will work. Because successive row operations are written as matrix products from right to
left, doing these two row operations is expressed with: the same matrix product.
Three.IV.3.35
The i-th row of GH is made up of the dot products of the i-th row of G with the
columns of H. The dot product of a zero row with a column is zero.
It works for columns if stated correctly: if H has a column of zeros then GH (if deﬁned) has a
column of zeros. The proof is easy.
Three.IV.3.36
Perhaps the easiest way is to show that each n×m matrix is a linear combination of
unit matrices in one and only one way:
c1



1
0
. . .
0
0
...


+ · · · + cn,m



0
0
. . .
...
0
. . .
1


=



a1,1
a1,2
. . .
...
an,1
. . .
an,m



has the unique solution c1 = a1,1, c2 = a1,2, etc.
Three.IV.3.37
Call that matrix F. We have
F 2 =
µ
2
1
1
1
¶
F 3 =
µ
3
2
2
1
¶
F 4 =
µ
5
3
3
2
¶
In general,
F n =
µ
fn+1
fn
fn
fn−1
¶

Answers to Exercises
117
where fi is the i-th Fibonacci number fi = fi−1 + fi−2 and f0 = 0, f1 = 1, which is veriﬁed by
induction, based on this equation.
µ
fi−1
fi−2
fi−2
fi−3
¶ µ
1
1
1
0
¶
=
µ
fi
fi−1
fi−1
fi−2
¶
Three.IV.3.38
Chapter Five gives a less computational reason — the trace of a matrix is the second
coeﬃcient in its characteristic polynomial — but for now we can use indices. We have
trace(GH) = (g1,1h1,1 + g1,2h2,1 + · · · + g1,nhn,1)
+ (g2,1h1,2 + g2,2h2,2 + · · · + g2,nhn,2)
+ · · · + (gn,1h1,n + gn,2h2,n + · · · + gn,nhn,n)
while
trace(HG) = (h1,1g1,1 + h1,2g2,1 + · · · + h1,ngn,1)
+ (h2,1g1,2 + h2,2g2,2 + · · · + h2,ngn,2)
+ · · · + (hn,1g1,n + hn,2g2,n + · · · + hn,ngn,n)
and the two are equal.
Three.IV.3.39
A matrix is upper triangular if and only if its i, j entry is zero whenever i > j.
Thus, if G, H are upper triangular then hi,j and gi,j are zero when i > j. An entry in the product
pi,j = gi,1h1,j + · · · + gi,nhn,j is zero unless at least some of the terms are nonzero, that is, unless for
at least some of the summands gi,rhr,j both i ≤r and r ≤j. Of course, if i > j this cannot happen
and so the product of two upper triangular matrices is upper triangular. (A similar argument works
for lower triangular matrices.)
Three.IV.3.40
The sum along the i-th row of the product is this.
pi,1 + · · · + pi,n = (hi,1g1,1 + hi,2g2,1 + · · · + hi,ngn,1)
+ (hi,1g1,2 + hi,2g2,2 + · · · + hi,ngn,2)
+ · · · + (hi,1g1,n + hi,2g2,n + · · · + hi,ngn,n)
= hi,1(g1,1 + g1,2 + · · · + g1,n)
+ hi,2(g2,1 + g2,2 + · · · + g2,n)
+ · · · + hi,n(gn,1 + gn,2 + · · · + gn,n)
= hi,1 · 1 + · · · + hi,n · 1
= 1
Three.IV.3.41
Matrices representing (say, with respect to E2, E2 ⊂R2) the maps that send
⃗β1
h
7−→⃗β1
⃗β2
h
7−→⃗0
and
⃗β1
g
7−→⃗β2
⃗β2
g
7−→⃗0
will do.
Three.IV.3.42
The combination is to have all entries of the matrix be zero except for one (possibly)
nonzero entry in each row and column. Such a matrix can be written as the product of a permutation
matrix and a diagonal matrix, e.g.,


0
4
0
2
0
0
0
0
−5

=


0
1
0
1
0
0
0
0
1




4
0
0
0
2
0
0
0
−5


and its action is thus to rescale the rows and permute them.
Three.IV.3.43
(a) Each entry pi,j = gi,1h1,j + · · · + g1,rhr,1 takes r multiplications and there are
m · n entries. Thus there are m · n · r multiplications.
(b) Let H1 be 5×10, let H2 be 10×20, let H3 be 20×5, let H4 be 5×1. Then, using the formula
from the prior part,
this association
uses this many multiplications
((H1H2)H3)H4
1000 + 500 + 25 = 1525
(H1(H2H3))H4
1000 + 250 + 25 = 1275
(H1H2)(H3H4)
1000 + 100 + 100 = 1200
H1(H2(H3H4))
100 + 200 + 50 = 350
H1((H2H3)H4)
1000 + 50 + 50 = 1100

118
Linear Algebra, by Hefferon
shows which is cheapest.
(c) This is reported by Knuth as an improvement by S. Winograd of a formula due to V. Strassen:
where w = aA −(a −c −d)(A −C + D),
µ
a
b
c
d
¶ µ
A
B
C
D
¶
=
µ
aA + bB
w + (c + d)(C −A) + (a + b −c −d)D
w + (a −c)(D −C) −d(A −B −C + D)
w + (a −c)(D −C) + (c + d)(C −A)
¶
takes seven multiplications and ﬁfteen additions (save the intermediate results).
Three.IV.3.44
This is how the answer was given in the cited source. No, it does not. Let A and B
represent, with respect to the standard bases, these transformations of R3.


x
y
z


a
7−→


x
y
0




x
y
z


a
7−→


0
x
y


Observe that


x
y
z

abab
7−→


0
0
0


but


x
y
z

baba
7−→


0
0
x

.
Three.IV.3.45
This is how the answer was given in the cited source.
(a) Obvious.
(b) If AtransA⃗x = ⃗0 then ⃗y · ⃗y = 0 where ⃗y = A⃗x. Hence ⃗y = ⃗0 by (a).
The converse is obvious.
(c) By (b), A⃗x1,. . . ,A⃗xn are linearly independent iﬀAtransA⃗x1,. . . , AtransA⃗vn are linearly indepen-
dent.
(d) We have col rank(A) = col rank(AtransA) = dim {Atrans(A⃗x)
¯¯ all ⃗x} ≤dim {Atrans⃗y
¯¯ all ⃗y} =
col rank(Atrans). Thus also col rank(Atrans) ≤col rank(Atranstrans) and so we have col rank(A) =
col rank(Atrans) = row rank(A).
Three.IV.3.46
This is how the answer was given in the cited source. Let ⟨⃗z1, . . . ,⃗zk⟩be a basis for
R(A) ∩N (A) (k might be 0). Let ⃗x1, . . . , ⃗xk ∈V be such that A⃗xi = ⃗zi. Note {A⃗x1, . . . , A⃗xk}
is linearly independent, and extend to a basis for R(A): A⃗x1, . . . , A⃗xk, A⃗xk+1, . . . , A⃗xr1 where r1 =
dim(R(A)).
Now take ⃗x ∈V . Write
A⃗x = a1(A⃗x1) + · · · + ar1(A⃗xr1)
and so
A2⃗x = a1(A2⃗x1) + · · · + ar1(A2⃗xr1).
But A⃗x1, . . . , A⃗xk ∈N (A), so A2⃗x1 = ⃗0, . . . , A2⃗xk = ⃗0 and we now know
A2⃗xk+1, . . . , A2⃗xr1
spans R(A2).
To see {A2⃗xk+1, . . . , A2⃗xr1} is linearly independent, write
bk+1A2⃗xk+1 + · · · + br1A2⃗xr1 = ⃗0
A[bk+1A⃗xk+1 + · · · + br1A⃗xr1] = ⃗0
and, since bk+1A⃗xk+1 + · · · + br1A⃗xr1 ∈N (A) we get a contradiction unless it is ⃗0 (clearly it is in
R(A), but A⃗x1, . . . , A⃗xk is a basis for R(A) ∩N (A)).
Hence dim(R(A2)) = r1 −k = dim(R(A)) −dim(R(A) ∩N (A)).
Subsection Three.IV.4: Inverses

Answers to Exercises
119
Three.IV.4.13
Here is one way to proceed.
ρ1↔ρ2
−→


1
0
1
0
1
0
0
3
−1
1
0
0
1
−1
0
0
0
1

−ρ1+ρ3
−→


1
0
1
0
1
0
0
3
−1
1
0
0
0
−1
−1
0
−1
1


(1/3)ρ2+ρ3
−→


1
0
1
0
1
0
0
3
−1
1
0
0
0
0
−4/3
1/3
−1
1


(1/3)ρ2
−→
−(3/4)ρ3


1
0
1
0
1
0
0
1
−1/3
1/3
0
0
0
0
1
−1/4
3/4
−3/4


(1/3)ρ3+ρ2
−→
−ρ3+ρ1


1
0
0
1/4
1/4
3/4
0
1
0
1/4
1/4
−1/4
0
0
1
−1/4
3/4
−3/4


Three.IV.4.14
(a) Yes, it has an inverse: ad −bc = 2 · 1 −1 · (−1) ̸= 0.
(b) Yes.
(c) No.
Three.IV.4.15
(a)
1
2 · 1 −1 · (−1) ·
µ
1
−1
1
2
¶
= 1
3 ·
µ
1
−1
1
2
¶
=
µ
1/3
−1/3
1/3
2/3
¶
(b)
1
0 · (−3) −4 · 1 ·
µ
−3
−4
−1
0
¶
=
µ
3/4
1
1/4
0
¶
(c) The prior question shows that no inverse exists.
Three.IV.4.16
(a) The reduction is routine.
µ
3
1
1
0
0
2
0
1
¶
(1/3)ρ1
−→
(1/2)ρ2
µ
1
1/3
1/3
0
0
1
0
1/2
¶
−(1/3)ρ2+ρ1
−→
µ
1
0
1/3
−1/6
0
1
0
1/2
¶
This answer agrees with the answer from the check.
µ3
1
0
2
¶−1
=
1
3 · 2 −0 · 1 ·
µ2
−1
0
3
¶
= 1
6 ·
µ2
−1
0
3
¶
(b) This reduction is easy.
µ
2
1/2
1
0
3
1
0
1
¶
−(3/2)ρ1+ρ2
−→
µ
2
1/2
1
0
0
1/4
−3/2
1
¶
(1/2)ρ1
−→
4ρ2
µ
1
1/4
1/2
0
0
1
−6
4
¶
−(1/4)ρ2+ρ1
−→
µ
1
0
2
−1
0
1
−6
4
¶
The check agrees.
1
2 · 1 −3 · (1/2) ·
µ
1
−1/2
−3
2
¶
= 2 ·
µ
1
−1/2
−3
2
¶
(c) Trying the Gauss-Jordan reduction
µ
2
−4
1
0
−1
2
0
1
¶
(1/2)ρ1+ρ2
−→
µ
2
−4
1
0
0
0
1/2
1
¶
shows that the left side won’t reduce to the identity, so no inverse exists. The check ad −bc =
2 · 2 −(−4) · (−1) = 0 agrees.
(d) This produces an inverse.


1
1
3
1
0
0
0
2
4
0
1
0
−1
1
0
0
0
1

ρ1+ρ3
−→


1
1
3
1
0
0
0
2
4
0
1
0
0
2
3
1
0
1

−ρ2+ρ3
−→


1
1
3
1
0
0
0
2
4
0
1
0
0
0
−1
1
−1
1


(1/2)ρ2
−→
−ρ3


1
1
3
1
0
0
0
1
2
0
1/2
0
0
0
1
−1
1
−1

−2ρ3+ρ2
−→
−3ρ3+ρ1


1
1
0
4
−3
3
0
1
0
2
−3/2
2
0
0
1
−1
1
−1


−ρ2+ρ1
−→


1
0
0
2
−3/2
1
0
1
0
2
−3/2
2
0
0
1
−1
1
−1



120
Linear Algebra, by Hefferon
(e) This is one way to do the reduction.


0
1
5
1
0
0
0
−2
4
0
1
0
2
3
−2
0
0
1

ρ3↔ρ1
−→


2
3
−2
0
0
1
0
−2
4
0
1
0
0
1
5
1
0
0


(1/2)ρ2+ρ3
−→


2
3
−2
0
0
1
0
−2
4
0
1
0
0
0
7
1
1/2
0


(1/2)ρ1
−→
−(1/2)ρ2
(1/7)ρ3


1
3/2
−1
0
0
1/2
0
1
−2
0
−1/2
0
0
0
1
1/7
1/14
0


2ρ3+ρ2
−→
ρ3+ρ1


1
3/2
0
1/7
1/14
1/2
0
1
0
2/7
−5/14
0
0
0
1
1/7
1/14
0

−(3/2)ρ2+ρ1
−→


1
0
0
−2/7
17/28
1/2
0
1
0
2/7
−5/14
0
0
0
1
1/7
1/14
0


(f) There is no inverse.


2
2
3
1
0
0
1
−2
−3
0
1
0
4
−2
−3
0
0
1

−(1/2)ρ1+ρ2
−→
−2ρ1+ρ3


2
2
3
1
0
0
0
−3
−9/2
−1/2
1
0
0
−6
−9
−2
0
1


−2ρ2+ρ3
−→


2
2
3
1
0
0
0
−3
−9/2
−1/2
1
0
0
0
0
−1
−2
1


As a check, note that the third column of the starting matrix is 3/2 times the second, and so it is
indeed singular and therefore has no inverse.
Three.IV.4.17
We can use Corollary 4.12.
1
1 · 5 −2 · 3 ·
µ 5
−3
−2
1
¶
=
µ−5
3
2
−1
¶
Three.IV.4.18
(a) The proof that the inverse is r−1H−1 = (1/r) · H−1 (provided, of course, that
the matrix is invertible) is easy.
(b) No. For one thing, the fact that H + G has an inverse doesn’t imply that H has an inverse or
that G has an inverse. Neither of these matrices is invertible but their sum is.
µ
1
0
0
0
¶
µ
0
0
0
1
¶
Another point is that just because H and G each has an inverse doesn’t mean H +G has an inverse;
here is an example.
µ1
0
0
1
¶
µ−1
0
0
−1
¶
Still a third point is that, even if the two matrices have inverses, and the sum has an inverse, doesn’t
imply that the equation holds:
µ
2
0
0
2
¶−1
=
µ
1/2
0
0
1/2
¶−1
µ
3
0
0
3
¶−1
=
µ
1/3
0
0
1/3
¶−1
but
µ
5
0
0
5
¶−1
=
µ
1/5
0
0
1/5
¶−1
and (1/2)+(1/3) does not equal 1/5.
Three.IV.4.19
Yes: T k(T −1)k = (TT · · · T) · (T −1T −1 · · · T −1) = T k−1(TT −1)(T −1)k−1 = · · · = I.
Three.IV.4.20
Yes, the inverse of H−1 is H.
Three.IV.4.21
One way to check that the ﬁrst is true is with the angle sum formulas from trigonom-
etry.µ
cos(θ1 + θ2)
−sin(θ1 + θ2)
sin(θ1 + θ2)
cos(θ1 + θ2)
¶
=
µ
cos θ1 cos θ2 −sin θ1 sin θ2
−sin θ1 cos θ2 −cos θ1 sin θ2
sin θ1 cos θ2 + cos θ1 sin θ2
cos θ1 cos θ2 −sin θ1 sin θ2
¶
=
µ
cos θ1
−sin θ1
sin θ1
cos θ1
¶ µ
cos θ2
−sin θ2
sin θ2
cos θ2
¶
Checking the second equation in this way is similar.
Of course, the equations can be not just checked but also understood by recalling that tθ is the
map that rotates vectors about the origin through an angle of θ radians.

Answers to Exercises
121
Three.IV.4.22
There are two cases. For the ﬁrst case we assume that a is nonzero. Then
−(c/a)ρ1+ρ2
−→
µ
a
b
1
0
0
−(bc/a) + d
−c/a
1
¶
=
µ
a
b
1
0
0
(ad −bc)/a
−c/a
1
¶
shows that the matrix is invertible (in this a ̸= 0 case) if and only if ad −bc ̸= 0. To ﬁnd the inverse,
we ﬁnish with the Jordan half of the reduction.
(1/a)ρ1
−→
(a/ad−bc)ρ2
µ
1
b/a
1/a
0
0
1
−c/(ad −bc)
a/(ad −bc)
¶
−(b/a)ρ2+ρ1
−→
µ
1
0
d/(ad −bc)
−b/(ad −bc)
0
1
−c/(ad −bc)
a/(ad −bc)
¶
The other case is the a = 0 case. We swap to get c into the 1, 1 position.
ρ1↔ρ2
−→
µ
c
d
0
1
0
b
1
0
¶
This matrix is nonsingular if and only if both b and c are nonzero (which, under the case assumption
that a = 0, holds if and only if ad −bc ̸= 0). To ﬁnd the inverse we do the Jordan half.
(1/c)ρ1
−→
(1/b)ρ2
µ
1
d/c
0
1/c
0
1
1/b
0
¶
−(d/c)ρ2+ρ1
−→
µ
1
0
−d/bc
1/c
0
1
1/b
0
¶
(Note that this is what is required, since a = 0 gives that ad −bc = −bc).
Three.IV.4.23
With H a 2×3 matrix, in looking for a matrix G such that the combination HG acts
as the 2×2 identity we need G to be 3×2. Setting up the equation
µ1
0
1
0
1
0
¶ 

m
n
p
q
r
s

=
µ1
0
0
1
¶
and solving the resulting linear system
m
+r
= 1
n
+s = 0
p
= 0
q
= 1
gives inﬁnitely many solutions.
{








m
n
p
q
r
s








=








1
0
0
1
0
0








+ r ·








−1
0
0
0
1
0








+ s ·








0
−1
0
0
0
1








¯¯ r, s ∈R}
Thus H has inﬁnitely many right inverses.
As for left inverses, the equation
µa
b
c
d
¶ µ1
0
1
0
1
0
¶
=


1
0
0
0
1
0
0
0
1


gives rise to a linear system with nine equations and four unknowns.
a
= 1
b
= 0
a
= 0
c
= 0
d
= 1
c
= 0
e
= 0
f = 0
e
= 1
This system is inconsistent (the ﬁrst equation conﬂicts with the third, as do the seventh and ninth)
and so there is no left inverse.
Three.IV.4.24
With respect to the standard bases we have
RepE2,E3(η) =


1
0
0
1
0
0



122
Linear Algebra, by Hefferon
and setting up the equation to ﬁnd the matrix inverse
µa
b
c
d
e
f
¶ 

1
0
0
1
0
0

=
µ1
0
0
1
¶
= RepE2,E2(id)
gives rise to a linear system.
a
= 1
b
= 0
d
= 0
e = 1
There are inﬁnitely many solutions in a, . . . , f to this system because two of these variables are entirely
unrestricted
{








a
b
c
d
e
f








=








1
0
0
0
1
0








+ c ·








0
0
1
0
0
0








+ f ·








0
0
0
0
0
1








¯¯ c, f ∈R}
and so there are inﬁnitely many solutions to the matrix equation.
{
µ
1
0
c
0
1
f
¶ ¯¯ c, f ∈R}
With the bases still ﬁxed at E2, E2, for instance taking c = 2 and f = 3 gives a matrix representing
this map.


x
y
z


f2,3
7−→
µ
x + 2z
y + 3z
¶
The check that f2,3 ◦η is the identity map on R2 is easy.
Three.IV.4.25
By Lemma 4.3 it cannot have inﬁnitely many left inverses, because a matrix with both
left and right inverses has only one of each (and that one of each is one of both — the left and right
inverse matrices are equal).
Three.IV.4.26
The associativity of matrix multiplication gives on the one hand H−1(HG) = H−1Z =
Z, and on the other that H−1(HG) = (H−1H)G = IG = G.
Three.IV.4.27
Multiply both sides of the ﬁrst equation by H.
Three.IV.4.28
Checking that when I −T is multiplied on both sides by that expression (assuming
that T 4 is the zero matrix) then the result is the identity matrix is easy. The obvious generalization
is that if T n is the zero matrix then (I −T)−1 = I + T + T 2 + · · · + T n−1; the check again is easy.
Three.IV.4.29
The powers of the matrix are formed by taking the powers of the diagonal entries.
That is, D2 is all zeros except for diagonal entries of d1,1
2, d2,2
2, etc. This suggests deﬁning D0 to be
the identity matrix.
Three.IV.4.30
Assume that B is row equivalent to A and that A is invertible. Because they are
row-equivalent, there is a sequence of row steps to reduce one to the other. That reduction can be
done with matrices, for instance, A can be changed by row operations to B as B = Rn · · · R1A. This
equation gives B as a product of invertible matrices and by Lemma 4.5 then, B is also invertible.
Three.IV.4.31
(a) See the answer to Exercise 28.
(b) We will show that both conditions are equivalent to the condition that the two matrices be
nonsingular.
As T and S are square and their product is deﬁned, they are equal-sized, say n×n. Consider
the TS = I half. By the prior item the rank of I is less than or equal to the minimum of the rank
of T and the rank of S. But the rank of I is n, so the rank of T and the rank of S must each be n.
Hence each is nonsingular.
The same argument shows that ST = I implies that each is nonsingular.
Three.IV.4.32
Inverses are unique, so we need only show that it works. The check appears above as
Exercise 31.
Three.IV.4.33
(a) See the answer for Exercise 25.
(b) See the answer for Exercise 25.

Answers to Exercises
123
(c) Apply the ﬁrst part to I = AA−1 to get I = Itrans = (AA−1)
trans = (A−1)
transAtrans.
(d) Apply the prior item with Atrans = A, as A is symmetric.
Three.IV.4.34
For the answer to the items making up the ﬁrst half, see Exercise 30. For the proof
in the second half, assume that A is a zero divisor so there is a nonzero matrix B with AB = Z (or
else BA = Z; this case is similar), If A is invertible then A−1(AB) = (A−1A)B = IB = B but also
A−1(AB) = A−1Z = Z, contradicting that B is nonzero.
Three.IV.4.35
No, there are at least four.
µ
±1
0
0
±1
¶
Three.IV.4.36
It is not reﬂexive since, for instance,
H =
µ
1
0
0
2
¶
is not a two-sided inverse of itself. The same example shows that it is not transitive. That matrix has
this two-sided inverse
G =
µ
1
0
0
1/2
¶
and while H is a two-sided inverse of G and G is a two-sided inverse of H, we know that H is not
a two-sided inverse of H. However, the relation is symmetric: if G is a two-sided inverse of H then
GH = I = HG and therefore H is also a two-sided inverse of G.
Three.IV.4.37
This is how the answer was given in the cited source. Let A be m×m, non-singular,
with the stated property. Let B be its inverse. Then for n ≤m,
1 =
m
X
r=1
δnr =
m
X
r=1
m
X
s=1
bnsasr =
m
X
s=1
m
X
r=1
bnsasr = k
m
X
s=1
bns
(A is singular if k = 0).
Subsection Three.V.1: Changing Representations of Vectors
Three.V.1.6
For the matrix to change bases from D to E2 we need that RepE2(id(⃗δ1)) = RepE2(⃗δ1)
and that RepE2(id(⃗δ2)) = RepE2(⃗δ2). Of course, the representation of a vector in R2 with respect to
the standard basis is easy.
RepE2(⃗δ1) =
µ
2
1
¶
RepE2(⃗δ2) =
µ
−2
4
¶
Concatenating those two together to make the columns of the change of basis matrix gives this.
RepD,E2(id) =
µ
2
−2
1
4
¶
The change of basis matrix in the other direction can be gotten by calculating RepD(id(⃗e1)) = RepD(⃗e1)
and RepD(id(⃗e2)) = RepD(⃗e2) (this job is routine) or it can be found by taking the inverse of the above
matrix. Because of the formula for the inverse of a 2×2 matrix, this is easy.
RepE2,D(id) = 1
10 ·
µ
4
2
−1
2
¶
=
µ
4/10
2/10
−1/10
2/10
¶
Three.V.1.7
In each case, the columns RepD(id(⃗β1)) = RepD(⃗β1) and RepD(id(⃗β2)) = RepD(⃗β2) are
concatenated to make the change of basis matrix RepB,D(id).
(a)
µ
0
1
1
0
¶
(b)
µ
2
−1/2
−1
1/2
¶
(c)
µ
1
1
2
4
¶
(d)
µ
1
−1
−1
2
¶
Three.V.1.8
One way to go is to ﬁnd RepB(⃗δ1) and RepB(⃗δ2), and then concatenate them into the
columns of the desired change of basis matrix. Another way is to ﬁnd the inverse of the matrices that
answer Exercise 7.
(a)
µ0
1
1
0
¶
(b)
µ1
1
2
4
¶
(c)
µ 2
−1/2
−1
1/2
¶
(d)
µ2
1
1
1
¶
Three.V.1.9
The columns vector representations RepD(id(⃗β1)) = RepD(⃗β1), and RepD(id(⃗β2)) =
RepD(⃗β2), and RepD(id(⃗β3)) = RepD(⃗β3) make the change of basis matrix RepB,D(id).

124
Linear Algebra, by Hefferon
(a)


0
0
1
1
0
0
0
1
0


(b)


1
−1
0
0
1
−1
0
0
1


(c)


1
−1
1/2
1
1
−1/2
0
2
0


E.g., for the ﬁrst column of the ﬁrst matrix, 1 = 0 · x2 + 1 · 1 + 0 · x.
Three.V.1.10
A matrix changes bases if and only if it is nonsingular.
(a) This matrix is nonsingular and so changes bases. Finding to what basis E2 is changed means
ﬁnding D such that
RepE2,D(id) =
µ5
0
0
4
¶
and by the deﬁnition of how a matrix represents a linear map, we have this.
RepD(id(⃗e1)) = RepD(⃗e1) =
µ
5
0
¶
RepD(id(⃗e2)) = RepD(⃗e2) =
µ
0
4
¶
Where
D = ⟨
µx1
y1
¶
,
µx2
y2
¶
⟩
we can either solve the system
µ
1
0
¶
= 5
µ
x1
y1
¶
+ 0
µ
x2
y1
¶
µ
0
1
¶
= 0
µ
x1
y1
¶
+ 4
µ
x2
y1
¶
or else just spot the answer (thinking of the proof of Lemma 1.4).
D = ⟨
µ
1/5
0
¶
,
µ
0
1/4
¶
⟩
(b) Yes, this matrix is nonsingular and so changes bases. To calculate D, we proceed as above with
D = ⟨
µ
x1
y1
¶
,
µ
x2
y2
¶
⟩
to solve
µ
1
0
¶
= 2
µ
x1
y1
¶
+ 3
µ
x2
y1
¶
and
µ
0
1
¶
= 1
µ
x1
y1
¶
+ 1
µ
x2
y1
¶
and get this.
D = ⟨
µ
−1
3
¶
,
µ
1
−2
¶
⟩
(c) No, this matrix does not change bases because it is nonsingular.
(d) Yes, this matrix changes bases because it is nonsingular. The calculation of the changed-to basis
is as above.
D = ⟨
µ
1/2
−1/2
¶
,
µ
1/2
1/2
¶
⟩
Three.V.1.11
This question has many diﬀerent solutions. One way to proceed is to make up any basis
B for any space, and then compute the appropriate D (necessarily for the same space, of course).
Another, easier, way to proceed is to ﬁx the codomain as R3 and the codomain basis as E3. This way
(recall that the representation of any vector with respect to the standard basis is just the vector itself),
we have this.
B = ⟨


3
2
0

,


1
−1
0

,


4
1
4

⟩
D = E3
Three.V.1.12
Checking that B = ⟨2 sin(x) + cos(x), 3 cos(x)⟩is a basis is routine. Call the natural
basis D. To compute the change of basis matrix RepB,D(id) we must ﬁnd RepD(2 sin(x)+cos(x)) and
RepD(3 cos(x)), that is, we need x1, y1, x2, y2 such that these equations hold.
x1 · sin(x) + y1 · cos(x) = 2 sin(x) + cos(x)
x2 · sin(x) + y2 · cos(x) = 3 cos(x)
Obviously this is the answer.
RepB,D(id) =
µ
2
0
1
3
¶
For the change of basis matrix in the other direction we could look for RepB(sin(x)) and RepB(cos(x))
by solving these.
w1 · (2 sin(x) + cos(x)) + z1 · (3 cos(x)) = sin(x)
w2 · (2 sin(x) + cos(x)) + z2 · (3 cos(x)) = cos(x)

Answers to Exercises
125
An easier method is to ﬁnd the inverse of the matrix found above.
RepD,B(id) =
µ
2
0
1
3
¶−1
= 1
6 ·
µ
3
0
−1
2
¶
=
µ
1/2
0
−1/6
1/3
¶
Three.V.1.13
We start by taking the inverse of the matrix, that is, by deciding what is the inverse to
the map of interest.
RepD,E2(id)RepD,E2(id)−1 =
1
−cos2(2θ) −sin2(2θ) ·
µ
−cos(2θ)
−sin(2θ)
−sin(2θ)
cos(2θ)
¶
=
µ
cos(2θ)
sin(2θ)
sin(2θ)
−cos(2θ)
¶
This is more tractable than the representation the other way because this matrix is the concatenation
of these two column vectors
RepE2(⃗δ1) =
µ
cos(2θ)
sin(2θ)
¶
RepE2(⃗δ2) =
µ
sin(2θ)
−cos(2θ)
¶
and representations with respect to E2 are transparent.
⃗δ1 =
µ
cos(2θ)
sin(2θ)
¶
⃗δ2 =
µ
sin(2θ)
−cos(2θ)
¶
This pictures the action of the map that transforms D to E2 (it is, again, the inverse of the map that
is the answer to this question). The line lies at an angle θ to the x axis.
f
7−→
⃗δ1 =
³
cos(2θ)
sin(2θ)
´
⃗δ2 =
³
sin(2θ)
−cos(2θ)
´
⃗e1
⃗e2
This map reﬂects vectors over that line. Since reﬂections are self-inverse, the answer to the question
is: the original map reﬂects about the line through the origin with angle of elevation θ. (Of course, it
does this to any basis.)
Three.V.1.14
The appropriately-sized identity matrix.
Three.V.1.15
Each is true if and only if the matrix is nonsingular.
Three.V.1.16
What remains to be shown is that left multiplication by a reduction matrix represents
a change from another basis to B = ⟨⃗β1, . . . , ⃗βn⟩.
Application of a row-multiplication matrix Mi(k) translates a representation with respect to the
basis ⟨⃗β1, . . . , k⃗βi, . . . , ⃗βn⟩to one with respect to B, as here.
⃗v = c1 · ⃗β1 + · · · + ci · (k⃗βi) + · · · + cn · ⃗βn 7→c1 · ⃗β1 + · · · + (kci) · ⃗βi + · · · + cn · ⃗βn = ⃗v
Applying a row-swap matrix Pi,j translates a representation with respect to ⟨⃗β1, . . . , ⃗βj, . . . , ⃗βi, . . . , ⃗βn⟩
to one with respect to ⟨⃗β1, . . . , ⃗βi, . . . , ⃗βj, . . . , ⃗βn⟩. Finally, applying a row-combination matrix Ci,j(k)
changes a representation with respect to ⟨⃗β1, . . . , ⃗βi + k⃗βj, . . . , ⃗βj, . . . , ⃗βn⟩to one with respect to B.
⃗v = c1 · ⃗β1 + · · · + ci · (⃗βi + k⃗βj) + · · · + cj ⃗βj + · · · + cn · ⃗βn
7→c1 · ⃗β1 + · · · + ci · ⃗βi + · · · + (kci + cj) · ⃗βj + · · · + cn · ⃗βn = ⃗v
(As in the part of the proof in the body of this subsection, the various conditions on the row operations,
e.g., that the scalar k is nonzero, assure that these are all bases.)
Three.V.1.17
Taking H as a change of basis matrix H = RepB,En(id), its columns are



h1,i
...
hn,i


= RepEn(id(⃗βi)) = RepEn(⃗βi)
and, because representations with respect to the standard basis are transparent, we have this.



h1,i
...
hn,i


= ⃗βi
That is, the basis is the one composed of the columns of H.

126
Linear Algebra, by Hefferon
Three.V.1.18
(a) We can change the starting vector representation to the ending one through a
sequence of row operations. The proof tells us what how the bases change. We start by swapping
the ﬁrst and second rows of the representation with respect to B to get a representation with resepect
to a new basis B1.
RepB1(1 −x + 3x2 −x3) =




1
0
1
2




B1
B1 = ⟨1 −x, 1 + x, x2 + x3, x2 −x3⟩
We next add −2 times the third row of the vector representation to the fourth row.
RepB3(1 −x + 3x2 −x3) =




1
0
1
0




B2
B2 = ⟨1 −x, 1 + x, 3x2 −x3, x2 −x3⟩
(The third element of B2 is the third element of B1 minus −2 times the fourth element of B1.) Now
we can ﬁnish by doubling the third row.
RepD(1 −x + 3x2 −x3) =




1
0
2
0




D
D = ⟨1 −x, 1 + x, (3x2 −x3)/2, x2 −x3⟩
(b) Here are three diﬀerent approaches to stating such a result. The ﬁrst is the assertion: where
V is a vector space with basis B and ⃗v ∈V is nonzero, for any nonzero column vector ⃗z (whose
number of components equals the dimension of V ) there is a change of basis matrix M such that
M · RepB(⃗v) = ⃗z.
The second possible statement: for any (n-dimensional) vector space V and
any nonzero vector ⃗v ∈V , where ⃗z1,⃗z2 ∈Rn are nonzero, there are bases B, D ⊂V such that
RepB(⃗v) = ⃗z1 and RepD(⃗v) = ⃗z2. The third is: for any nonzero ⃗v member of any vector space (of
dimension n) and any nonzero column vector (with n components) there is a basis such that ⃗v is
represented with respect to that basis by that column vector.
The ﬁrst and second statements follow easily from the third. The ﬁrst follows because the third
statement gives a basis D such that RepD(⃗v) = ⃗z and then RepB,D(id) is the desired M. The second
follows from the third because it is just a doubled application of it.
A way to prove the third is as in the answer to the ﬁrst part of this question. Here is a sketch.
Represent ⃗v with respect to any basis B with a column vector ⃗z1. This column vector must have
a nonzero component because ⃗v is a nonzero vector. Use that component in a sequence of row
operations to convert ⃗z1 to ⃗z. (This sketch could be ﬁlled out as an induction argument on the
dimension of V .)
Three.V.1.19
This is the topic of the next subsection.
Three.V.1.20
A change of basis matrix is nonsingular and thus has rank equal to the number of its
columns. Therefore its set of columns is a linearly independent subset of size n in Rn and it is thus a
basis. The answer to the second half is also ‘yes’; all implications in the prior sentence reverse (that
is, all of the ‘if . . . then . . . ’ parts of the prior sentence convert to ‘if and only if’ parts).
Three.V.1.21
In response to the ﬁrst half of the question, there are inﬁnitely many such matrices.
One of them represents with respect to E2 the transformation of R2 with this action.
µ
1
0
¶
7→
µ
4
0
¶
µ
0
1
¶
7→
µ
0
−1/3
¶
The problem of specifying two distinct input/output pairs is a bit trickier. The fact that matrices have
a linear action precludes some possibilities.
(a) Yes, there is such a matrix. These conditions
µ
a
b
c
d
¶ µ
1
3
¶
=
µ
1
1
¶
µ
a
b
c
d
¶ µ
2
−1
¶
=
µ
−1
−1
¶
can be solved
a + 3b
=
1
c + 3d =
1
2a −b
= −1
2c −d = −1

Answers to Exercises
127
to give this matrix.
µ−2/7
3/7
−2/7
3/7
¶
(b) No, because
2 ·
µ1
3
¶
=
µ2
6
¶
but
2 ·
µ1
1
¶
̸=
µ−1
−1
¶
no linear action can produce this eﬀect.
(c) A suﬃcient condition is that {⃗v1,⃗v2} be linearly independent, but that’s not a necessary condition.
A necessary and suﬃcient condition is that any linear dependences among the starting vectors appear
also among the ending vectors. That is,
c1⃗v1 + c2⃗v2 = ⃗0
implies
c1 ⃗w1 + c2 ⃗w2 = ⃗0.
The proof of this condition is routine.
Subsection Three.V.2: Changing Map Representations
Three.V.2.10
(a) Yes, each has rank two.
(b) Yes, they have the same rank.
(c) No, they have diﬀerent ranks.
Three.V.2.11
We need only decide what the rank of each is.
(a)
µ
1
0
0
0
0
0
¶
(b)


1
0
0
0
0
1
0
0
0
0
1
0


Three.V.2.12
Recall the diagram and the formula.
R2
w.r.t. B
t
−−−−→
T
R2
w.r.t. D
id
y
id
y
R2
w.r.t. ˆ
B
t
−−−−→
ˆT
R2
w.r.t. ˆ
D
ˆT = RepD, ˆ
D(id) · T · Rep ˆ
B,B(id)
(a) These two
µ1
1
¶
= 1 ·
µ−1
0
¶
+ 1 ·
µ2
1
¶
µ 1
−1
¶
= (−3) ·
µ−1
0
¶
+ (−1) ·
µ2
1
¶
show that
RepD, ˆ
D(id) =
µ
1
−3
1
−1
¶
and similarly these twoµ
0
1
¶
= 0 ·
µ
1
0
¶
+ 1 ·
µ
0
1
¶
µ
1
1
¶
= 1 ·
µ
1
0
¶
+ 1 ·
µ
0
1
¶
give the other nonsinguar matrix.
Rep ˆ
B,B(id) =
µ
0
1
1
1
¶
Then the answer is this.
ˆT =
µ
1
−3
1
−1
¶ µ
1
2
3
4
¶ µ
0
1
1
1
¶
=
µ
−10
−18
−2
−4
¶
Although not strictly necessary, a check is reassuring. Arbitrarily ﬁxing
⃗v =
µ
3
2
¶
we have that
RepB(⃗v) =
µ
3
2
¶
B
µ
1
2
3
4
¶
B,D
µ
3
2
¶
B
=
µ
7
17
¶
D
and so t(⃗v) is this.
7 ·
µ
1
1
¶
+ 17 ·
µ
1
−1
¶
=
µ
24
−10
¶

128
Linear Algebra, by Hefferon
Doing the calculation with respect to ˆB, ˆD starts with
Rep ˆ
B(⃗v) =
µ−1
3
¶
ˆ
B
µ−10
−18
−2
−4
¶
ˆ
B, ˆ
D
µ−1
3
¶
ˆ
B
=
µ−44
−10
¶
ˆ
D
and then checks that this is the same result.
−44 ·
µ
−1
0
¶
−10 ·
µ
2
1
¶
=
µ
24
−10
¶
(b) These two
µ
1
1
¶
= 1
3 ·
µ
1
2
¶
+ 1
3 ·
µ
2
1
¶
µ
1
−1
¶
= −1 ·
µ
1
2
¶
+ 1 ·
µ
2
1
¶
show that
RepD, ˆ
D(id) =
µ
1/3
−1
1/3
1
¶
and these two
µ
1
2
¶
= 1 ·
µ
1
0
¶
+ 2 ·
µ
0
1
¶
µ
1
0
¶
= −1 ·
µ
1
0
¶
+ 0 ·
µ
0
1
¶
show this.
Rep ˆ
B,B(id) =
µ
1
1
2
0
¶
With those, the conversion goes in this way.
ˆT =
µ
1/3
−1
1/3
1
¶ µ
1
2
3
4
¶ µ
1
1
2
0
¶
=
µ
−28/3
−8/3
38/3
10/3
¶
As in the prior item, a check provides some conﬁdence that this calculation was performed without
mistakes. We can for instance, ﬁx the vector
⃗v =
µ
−1
2
¶
(this is selected for no reason, out of thin air). Now we have
RepB(⃗v) =
µ
−1
2
¶
µ
1
2
3
4
¶
B,D
µ
−1
2
¶
B
=
µ
3
5
¶
D
and so t(⃗v) is this vector.
3 ·
µ
1
1
¶
+ 5 ·
µ
1
−1
¶
=
µ
8
−2
¶
With respect to ˆB, ˆD we ﬁrst calculate
Rep ˆ
B(⃗v) =
µ 1
−2
¶
µ−28/3
−8/3
38/3
10/3
¶
ˆ
B, ˆ
D
µ 1
−2
¶
ˆ
B
=
µ−4
6
¶
ˆ
D
and, sure enough, that is the same result for t(⃗v).
−4 ·
µ
1
2
¶
+ 6 ·
µ
2
1
¶
=
µ
8
−2
¶
Three.V.2.13
Where H and ˆH are m×n, the matrix P is m×m while Q is n×n.
Three.V.2.14
Any n×n matrix is nonsingular if and only if it has rank n, that is, by Theorem 2.6, if
and only if it is matrix equivalent to the n×n matrix whose diagonal is all ones.
Three.V.2.15
If PAQ = I then QPAQ = Q, so QPA = I, and so QP = A−1.
Three.V.2.16
By the deﬁnition following Example 2.2, a matrix M is diagonalizable if it represents
M = RepB,D(t) a transformation with the property that there is some basis ˆB such that Rep ˆ
B, ˆ
B(t)
is a diagonal matrix — the starting and ending bases must be equal. But Theorem 2.6 says only that
there are ˆB and ˆD such that we can change to a representation Rep ˆ
B, ˆ
D(t) and get a diagonal matrix.
We have no reason to suspect that we could pick the two ˆB and ˆD so that they are equal.
Three.V.2.17
Yes. Row rank equals column rank, so the rank of the transpose equals the rank of the
matrix. Same-sized matrices with equal ranks are matrix equivalent.
Three.V.2.18
Only a zero matrix has rank zero.

Answers to Exercises
129
Three.V.2.19
For reﬂexivity, to show that any matrix is matrix equivalent to itself, take P and Q to
be identity matrices. For symmetry, if H1 = PH2Q then H2 = P −1H1Q−1 (inverses exist because P
and Q are nonsingular). Finally, for transitivity, assume that H1 = P2H2Q2 and that H2 = P3H3Q3.
Then substitution gives H1 = P2(P3H3Q3)Q2 = (P2P3)H3(Q3Q2). A product of nonsingular matrices
is nonsingular (we’ve shown that the product of invertible matrices is invertible; in fact, we’ve shown
how to calculate the inverse) and so H1 is therefore matrix equivalent to H3.
Three.V.2.20
By Theorem 2.6, a zero matrix is alone in its class because it is the only m×n of rank
zero. No other matrix is alone in its class; any nonzero scalar product of a matrix has the same rank
as that matrix.
Three.V.2.21
There are two matrix-equivalence classes of 1×1 matrices — those of rank zero and those
of rank one. The 3×3 matrices fall into four matrix equivalence classes.
Three.V.2.22
For m×n matrices there are classes for each possible rank: where k is the minimum
of m and n there are classes for the matrices of rank 0, 1, . . . , k. That’s k + 1 classes. (Of course,
totaling over all sizes of matrices we get inﬁnitely many classes.)
Three.V.2.23
They are closed under nonzero scalar multiplication, since a nonzero scalar multiple of
a matrix has the same rank as does the matrix. They are not closed under addition, for instance,
H + (−H) has rank zero.
Three.V.2.24
(a) We have
RepB,E2(id) =
µ1
−1
2
−1
¶
RepE2,B(id) = RepB,E2(id)−1 =
µ1
−1
2
−1
¶−1
=
µ−1
1
−2
1
¶
and thus the answer is this.
RepB,B(t) =
µ
1
−1
2
−1
¶ µ
1
1
3
−1
¶ µ
−1
1
−2
1
¶
=
µ
−2
0
−5
2
¶
As a quick check, we can take a vector at random
⃗v =
µ
4
5
¶
giving
RepE2(⃗v) =
µ4
5
¶
µ1
1
3
−1
¶ µ4
5
¶
=
µ9
7
¶
= t(⃗v)
while the calculation with respect to B, B
RepB(⃗v) =
µ 1
−3
¶
µ−2
0
−5
2
¶
B,B
µ 1
−3
¶
B
=
µ −2
−11
¶
B
yields the same result.
−2 ·
µ
1
2
¶
−11 ·
µ
−1
−1
¶
=
µ
9
7
¶
(b) We have
R2
w.r.t. E2
t
−−−−→
T
R2
w.r.t. E2
id
y
id
y
R2
w.r.t. B
t
−−−−→
ˆT
R2
w.r.t. B
RepB,B(t) = RepE2,B(id) · T · RepB,E2(id)
and, as in the ﬁrst item of this question
RepB,E2(id) =
³
⃗β1
· · ·
⃗βn
´
RepE2,B(id) = RepB,E2(id)−1
so, writing Q for the matrix whose columns are the basis vectors, we have that RepB,B(t) = Q−1TQ.
Three.V.2.25
(a) The adapted form of the arrow diagram is this.
Vw.r.t. B1
h
−−−−→
H
Ww.r.t. D
id
yQ
id
yP
Vw.r.t. B2
h
−−−−→
ˆ
H
Ww.r.t. D
Since there is no need to change bases in W (or we can say that the change of basis matrix P is the
identity), we have RepB2,D(h) = RepB1,D(h) · Q where Q = RepB2,B1(id).

130
Linear Algebra, by Hefferon
(b) Here, this is the arrow diagram.
Vw.r.t. B
h
−−−−→
H
Ww.r.t. D1
id
yQ
id
yP
Vw.r.t. B
h
−−−−→
ˆ
H
Ww.r.t. D2
We have that RepB,D2(h) = P · RepB,D1(h) where P = RepD1,D2(id).
Three.V.2.26
(a) Here is the arrow diagram, and a version of that diagram for inverse functions.
Vw.r.t. B
h
−−−−→
H
Ww.r.t. D
id
yQ
id
yP
Vw.r.t. ˆ
B
h
−−−−→
ˆ
H
Ww.r.t. ˆ
D
Vw.r.t. B
h−1
←−−−−
H−1
Ww.r.t. D
id
yQ
id
yP
Vw.r.t. ˆ
B
h−1
←−−−−
ˆ
H−1
Ww.r.t. ˆ
D
Yes, the inverses of the matrices represent the inverses of the maps. That is, we can move from the
lower right to the lower left by moving up, then left, then down. In other words, where ˆH = PHQ
(and P, Q invertible) and H, ˆH are invertible then ˆH−1 = Q−1H−1P −1.
(b) Yes; this is the prior part repeated in diﬀerent terms.
(c) No, we need another assumption: if H represents h with respect to the same starting as ending
bases B, B, for some B then H2 represents h◦h. As a speciﬁc example, these two matrices are both
rank one and so they are matrix equivalent
µ
1
0
0
0
¶
µ
0
0
1
0
¶
but the squares are not matrix equivalent — the square of the ﬁrst has rank one while the square of
the second has rank zero.
(d) No. These two are not matrix equivalent but have matrix equivalent squares.
µ
0
0
0
0
¶
µ
0
0
1
0
¶
Three.V.2.27
(a) The deﬁnition is suggested by the appropriate arrow diagram.
Vw.r.t. B1
t
−−−−→
T
Vw.r.t. B1
id
y
id
y
Vw.r.t. B2
t
−−−−→
ˆT
Vw.r.t. B2
Call matrices T, ˆT similar if there is a nonsingular matrix P such that ˆT = P −1TP.
(b) Take P −1 to be P and take P to be Q.
(c) This is as in Exercise 19. Reﬂexivity is obvious: T = I−1TI. Symmetry is also easy:
ˆT =
P −1TP implies that T = P ˆTP −1 (multiply the ﬁrst equation from the right by P −1 and from
the left by P).
For transitivity, assume that T1 = P2
−1T2P2 and that T2 = P3
−1T3P3.
Then
T1 = P2
−1(P3
−1T3P3)P2 = (P2
−1P3
−1)T3(P3P2) and we are ﬁnished on noting that P3P2 is an
invertible matrix with inverse P2
−1P3
−1.
(d) Assume that ˆT = P −1TP. For the squares:
ˆT 2 = (P −1TP)(P −1TP) = P −1T(PP −1)TP =
P −1T 2P. Higher powers follow by induction.
(e) These two are matrix equivalent but their squares are not matrix equivalent.
µ
1
0
0
0
¶
µ
0
0
1
0
¶
By the prior item, matrix similarity and matrix equivalence are thus diﬀerent.
Subsection Three.VI.1: Orthogonal Projection Into a Line
Three.VI.1.7
Each is a straightforward application of the formula from Deﬁnition 1.1.

Answers to Exercises
131
(a)
µ
2
1
¶ µ
3
−2
¶
µ 3
−2
¶ µ 3
−2
¶ ·
µ
3
−2
¶
= 4
13 ·
µ
3
−2
¶
=
µ
12/13
−8/13
¶
(b)
µ
2
1
¶ µ
3
0
¶
µ3
0
¶ µ3
0
¶ ·
µ
3
0
¶
= 2
3 ·
µ
3
0
¶
=
µ
2
0
¶
(c)


1
1
4




1
2
−1




1
2
−1




1
2
−1


·


1
2
−1

= −1
6 ·


1
2
−1

=


−1/6
−1/3
1/6


(d)


1
1
4




3
3
12




3
3
12




3
3
12


·


3
3
12

=
1
3 ·


3
3
12

=


1
1
4


Three.VI.1.8
(a)


2
−1
4




−3
1
−3




−3
1
−3




−3
1
−3


·


−3
1
−3

= −19
19 ·


−3
1
−3

=


3
−1
3


(b) Writing the line as {c ·
µ
1
3
¶ ¯¯ c ∈R} gives this projection.
µ
−1
−1
¶ µ
1
3
¶
µ
1
3
¶ µ
1
3
¶ ·
µ1
3
¶
= −4
10 ·
µ1
3
¶
=
µ−2/5
−6/5
¶
Three.VI.1.9




1
2
1
3








−1
1
−1
1








−1
1
−1
1








−1
1
−1
1




·




−1
1
−1
1



= 3
4 ·




−1
1
−1
1



=




−3/4
3/4
−3/4
3/4




Three.VI.1.10
(a)
µ
1
2
¶ µ
3
1
¶
µ3
1
¶ µ3
1
¶ ·
µ
3
1
¶
= 1
2 ·
µ
3
1
¶
=
µ
3/2
1/2
¶
(b)
µ
0
4
¶ µ
3
1
¶
µ3
1
¶ µ3
1
¶ ·
µ
3
1
¶
= 2
5 ·
µ
3
1
¶
=
µ6/5
2/5
¶
In general the projection is this.
µ
x1
x2
¶ µ
3
1
¶
µ
3
1
¶ µ
3
1
¶ ·
µ
3
1
¶
= 3x1 + x2
10
·
µ
3
1
¶
=
µ
(9x1 + 3x2)/10
(3x1 + x2)/10
¶
The appropriate matrix is this.
µ
9/10
3/10
3/10
1/10
¶
Three.VI.1.11
Suppose that ⃗v1 and ⃗v2 are nonzero and orthogonal. Consider the linear relationship
c1⃗v1 + c2⃗v2 = ⃗0. Take the dot product of both sides of the equation with ⃗v1 to get that
⃗v1 (c1⃗v1 + c2⃗v2) = c1 · (⃗v1 ⃗v1) + c2 · (⃗v1 ⃗v2) = c1 · (⃗v1 ⃗v1) + c2 · 0 = c1 · (⃗v1 ⃗v1)
is equal to ⃗v1 ⃗0 = ⃗0. With the assumption that ⃗v1 is nonzero, this gives that c1 is zero. Showing that
c2 is zero is similar.

132
Linear Algebra, by Hefferon
Three.VI.1.12
(a) If the vector ⃗v is in the line then the orthogonal projection is ⃗v. To verify this
by calculation, note that since ⃗v is in the line we have that ⃗v = c⃗v · ⃗s for some scalar c⃗v.
⃗v ⃗s
⃗s ⃗s · ⃗s = c⃗v · ⃗s ⃗s
⃗s ⃗s
· ⃗s = c⃗v · ⃗s ⃗s
⃗s ⃗s · ⃗s = c⃗v · 1 · ⃗s = ⃗v
(Remark. If we assume that ⃗v is nonzero then the above is simpliﬁed on taking ⃗s to be ⃗v.)
(b) Write c⃗p⃗s for the projection proj[⃗s ](⃗v). Note that, by the assumption that ⃗v is not in the line,
both ⃗v and ⃗v −c⃗p⃗s are nonzero. Note also that if c⃗p is zero then we are actually considering the
one-element set {⃗v }, and with ⃗v nonzero, this set is necessarily linearly independent. Therefore, we
are left considering the case that c⃗p is nonzero.
Setting up a linear relationship
a1(⃗v) + a2(⃗v −c⃗p⃗s) = ⃗0
leads to the equation (a1 + a2) ·⃗v = a2c⃗p ·⃗s. Because ⃗v isn’t in the line, the scalars a1 + a2 and a2c⃗p
must both be zero. The c⃗p = 0 case is handled above, so the remaining case is that a2 = 0, and this
gives that a1 = 0 also. Hence the set is linearly independent.
Three.VI.1.13
If ⃗s is the zero vector then the expression
proj[⃗s ](⃗v) = ⃗v ⃗s
⃗s ⃗s · ⃗s
contains a division by zero, and so is undeﬁned. As for the right deﬁnition, for the projection to lie in
the span of the zero vector, it must be deﬁned to be ⃗0.
Three.VI.1.14
Any vector in Rn is the projection of some other into a line, provided that the dimension
n is greater than one. (Clearly, any vector is the projection of itself into a line containing itself; the
question is to produce some vector other than ⃗v that projects to ⃗v.)
Suppose that ⃗v ∈Rn with n > 1. If ⃗v ̸= ⃗0 then we consider the line ℓ= {c⃗v
¯¯ c ∈R} and if ⃗v = ⃗0
we take ℓto be any (nondegenerate) line at all (actually, we needn’t distinguish between these two
cases — see the prior exercise). Let v1, . . . , vn be the components of ⃗v; since n > 1, there are at least
two. If some vi is zero then the vector ⃗w = ⃗ei is perpendicular to ⃗v. If none of the components is zero
then the vector ⃗w whose components are v2, −v1, 0, . . . , 0 is perpendicular to ⃗v. In either case, observe
that ⃗v + ⃗w does not equal ⃗v, and that ⃗v is the projection of ⃗v + ⃗w into ℓ.
(⃗v + ⃗w) ⃗v
⃗v ⃗v
· ⃗v =
¡⃗v ⃗v
⃗v ⃗v + ⃗w ⃗v
⃗v ⃗v
¢
· ⃗v = ⃗v ⃗v
⃗v ⃗v · ⃗v = ⃗v
We can dispose of the remaining n = 0 and n = 1 cases. The dimension n = 0 case is the trivial
vector space, here there is only one vector and so it cannot be expressed as the projection of a diﬀerent
vector. In the dimension n = 1 case there is only one (nondegenerate) line, and every vector is in it,
hence every vector is the projection only of itself.
Three.VI.1.15
The proof is simply a calculation.
∥⃗v ⃗s
⃗s ⃗s · ⃗s ∥= |⃗v ⃗s
⃗s ⃗s| · ∥⃗s ∥= |⃗v ⃗s |
∥⃗s ∥2 · ∥⃗s ∥= |⃗v ⃗s |
∥⃗s ∥
Three.VI.1.16
Because the projection of ⃗v into the line spanned by ⃗s is
⃗v ⃗s
⃗s ⃗s · ⃗s
the distance squared from the point to the line is this (a vector dotted with itself ⃗w ⃗w is written ⃗w2).
∥⃗v −⃗v ⃗s
⃗s ⃗s · ⃗s ∥2 = ⃗v ⃗v −⃗v (⃗v ⃗s
⃗s ⃗s · ⃗s) −(⃗v ⃗s
⃗s ⃗s · ⃗s ) ⃗v + (⃗v ⃗s
⃗s ⃗s · ⃗s )2
= ⃗v ⃗v −2 · (⃗v ⃗s
⃗s ⃗s) · ⃗v ⃗s + (⃗v ⃗s
⃗s ⃗s) · ⃗s ⃗s
= (⃗v ⃗v ) · (⃗s ⃗s ) −2 · (⃗v ⃗s )2 + (⃗v ⃗s )2
⃗s ⃗s
= (⃗v ⃗v )(⃗s ⃗s ) −(⃗v ⃗s )2
⃗s ⃗s
Three.VI.1.17
Because square root is a strictly increasing function, we can minimize d(c) = (cs1 −
v1)2+(cs2−v2)2 instead of the square root of d. The derivative is dd/dc = 2(cs1−v1)·s1+2(cs2−v2)·s2.
Setting it equal to zero 2(cs1 −v1) · s1 + 2(cs2 −v2) · s2 = c · (2s2
1 + 2s2
2) −(v1s1 + v2s2) = 0 gives the
only critical point.
c = v1s1 + v2s2
s12 + s22
= ⃗v ⃗s
⃗s ⃗s

Answers to Exercises
133
Now the second derivative with respect to c
d2 d
dc2 = 2s1
2 + 2s2
2
is strictly positive (as long as neither s1 nor s2 is zero, in which case the question is trivial) and so the
critical point is a minimum.
The generalization to Rn is straightforward. Consider dn(c) = (cs1 −v1)2 + · · · + (csn −vn)2, take
the derivative, etc.
Three.VI.1.18
The Cauchy-Schwartz inequality |⃗v ⃗s | ≤∥⃗v ∥· ∥⃗s ∥gives that this fraction
∥⃗v ⃗s
⃗s ⃗s · ⃗s ∥= |⃗v ⃗s
⃗s ⃗s| · ∥⃗s ∥= |⃗v ⃗s |
∥⃗s ∥2 · ∥⃗s ∥= |⃗v ⃗s |
∥⃗s ∥
when divided by ∥⃗v ∥is less than or equal to one. That is, ∥⃗v ∥is larger than or equal to the fraction.
Three.VI.1.19
Write c⃗s for ⃗q, and calculate: (⃗v c⃗s/c⃗s c⃗s ) · c⃗s = (⃗v ⃗s/⃗s ⃗s ) · ⃗s.
Three.VI.1.20
(a) Fixing
⃗s =
µ
1
1
¶
as the vector whose span is the line, the formula gives this action,
µ
x
y
¶
7→
µ
x
y
¶ µ
1
1
¶
µ1
1
¶ µ1
1
¶ ·
µ
1
1
¶
= x + y
2
·
µ
1
1
¶
=
µ
(x + y)/2
(x + y)/2
¶
which is the eﬀect of this matrix.
µ1/2
1/2
1/2
1/2
¶
(b) Rotating the entire plane π/4 radians clockwise brings the y = x line to lie on the x-axis. Now
projecting and then rotating back has the desired eﬀect.
Three.VI.1.21
The sequence need not settle down. With
⃗a =
µ
1
0
¶
⃗b =
µ
1
1
¶
the projections are these.
⃗v1 =
µ
1/2
1/2
¶
,
⃗v2 =
µ
1/2
0
¶
,
⃗v3 =
µ
1/4
1/4
¶
,
. . .
This sequence doesn’t repeat.
Subsection Three.VI.2: Gram-Schmidt Orthogonalization
Three.VI.2.9
(a)
⃗κ1 =
µ
1
1
¶
⃗κ2 =
µ
2
1
¶
−proj[⃗κ1](
µ
2
1
¶
) =
µ
2
1
¶
−
µ
2
1
¶ µ
1
1
¶
µ
1
1
¶ µ
1
1
¶ ·
µ
1
1
¶
=
µ
2
1
¶
−3
2 ·
µ
1
1
¶
=
µ
1/2
−1/2
¶

134
Linear Algebra, by Hefferon
(b)
⃗κ1 =
µ0
1
¶
⃗κ2 =
µ
−1
3
¶
−proj[⃗κ1](
µ
−1
3
¶
) =
µ
−1
3
¶
−
µ
−1
3
¶ µ
0
1
¶
µ
0
1
¶ µ
0
1
¶ ·
µ
0
1
¶
=
µ
−1
3
¶
−3
1 ·
µ
0
1
¶
=
µ
−1
0
¶
(c)
⃗κ1 =
µ0
1
¶
⃗κ2 =
µ
−1
0
¶
−proj[⃗κ1](
µ
−1
0
¶
) =
µ
−1
0
¶
−
µ
−1
0
¶ µ
0
1
¶
µ
0
1
¶ µ
0
1
¶ ·
µ
0
1
¶
=
µ
−1
0
¶
−0
1 ·
µ
0
1
¶
=
µ
−1
0
¶
The corresponding orthonormal bases for the three parts of this question are these.
⟨
µ
1/
√
2
1/
√
2
¶
,
µ √
2/2
−
√
2/2
¶
⟩
⟨
µ
0
1
¶
,
µ
−1
0
¶
⟩
⟨
µ
0
1
¶
,
µ
−1
0
¶
⟩
Three.VI.2.10
(a)
⃗κ1 =


2
2
2


⃗κ2 =


1
0
−1

−proj[⃗κ1](


1
0
−1

) =


1
0
−1

−


1
0
−1




2
2
2




2
2
2




2
2
2


·


2
2
2

=


1
0
−1

−0
12 ·


2
2
2

=


1
0
−1


⃗κ3 =


0
3
1

−proj[⃗κ1](


0
3
1

) −proj[⃗κ2](


0
3
1

) =


0
3
1

−


0
3
1




2
2
2




2
2
2




2
2
2


·


2
2
2

−


0
3
1




1
0
−1




1
0
−1




1
0
−1


·


1
0
−1


=


0
3
1

−8
12 ·


2
2
2

−−1
2 ·


1
0
−1

=


−5/6
5/3
−5/6



Answers to Exercises
135
(b)
⃗κ1 =


1
−1
0


⃗κ2 =


0
1
0

−proj[⃗κ1](


0
1
0

) =


0
1
0

−


0
1
0




1
−1
0




1
−1
0




1
−1
0


·


1
−1
0

=


0
1
0

−−1
2 ·


1
−1
0

=


1/2
1/2
0


⃗κ3 =


2
3
1

−proj[⃗κ1](


2
3
1

) −proj[⃗κ2](


2
3
1

)
=


2
3
1

−


2
3
1




1
−1
0




1
−1
0




1
−1
0


·


1
−1
0

−


2
3
1




1/2
1/2
0




1/2
1/2
0




1/2
1/2
0


·


1/2
1/2
0


=


2
3
1

−−1
2 ·


1
−1
0

−5/2
1/2 ·


1/2
1/2
0

=


0
0
1


The corresponding orthonormal bases for the two parts of this question are these.
⟨


1/
√
3
1/
√
3
1/
√
3

,


1/
√
2
0
−1/
√
2

,


−1/
√
6
2/
√
6
−1/
√
6

⟩
⟨


1/
√
2
−1/
√
2
0

,


1/
√
2
1/
√
2
0




0
0
1

⟩
Three.VI.2.11
The given space can be parametrized in this way.
{


x
y
z

¯¯ x = y −z} = {


1
1
0

· y +


−1
0
1

· z
¯¯ y, z ∈R}
So we take the basis
⟨


1
1
0

,


−1
0
1

⟩
apply the Gram-Schmidt process
⃗κ1 =


1
1
0


⃗κ2 =


−1
0
1

−proj[⃗κ1](


−1
0
1

) =


−1
0
1

−


−1
0
1




1
1
0




1
1
0




1
1
0


·


1
1
0

=


−1
0
1

−−1
2 ·


1
1
0

=


−1/2
1/2
1


and then normalize.
⟨


1/
√
2
1/
√
2
0

,


−1/
√
6
1/
√
6
2/
√
6

⟩
Three.VI.2.12
Reducing the linear system
x −y −z + w = 0
x
+ z
= 0
−ρ1+ρ2
−→
x −y −z + w = 0
y + 2z −w = 0

136
Linear Algebra, by Hefferon
and parametrizing gives this description of the subspace.
{




−1
−2
1
0



· z +




0
1
0
1



· w
¯¯ z, w ∈R}
So we take the basis,
⟨




−1
−2
1
0



,




0
1
0
1



⟩
go through the Gram-Schmidt process
⃗κ1 =




−1
−2
1
0




⃗κ2 =




0
1
0
1



−proj[⃗κ1](




0
1
0
1



) =




0
1
0
1



−




0
1
0
1








−1
−2
1
0








−1
−2
1
0








−1
−2
1
0




·




−1
−2
1
0



=




0
1
0
1



−−2
6 ·




−1
−2
1
0



=




−1/3
1/3
1/3
1




and ﬁnish by normalizing.
⟨




−1/
√
6
−2/
√
6
1/
√
6
0



,




−
√
3/6
√
3/6
√
3/6
√
3/2



⟩
Three.VI.2.13
A linearly independent subset of Rn is a basis for its own span. Apply Theorem 2.7.
Remark. Here’s why the phrase ‘linearly independent’ is in the question. Dropping the phrase
would require us to worry about two things. The ﬁrst thing to worry about is that when we do the
Gram-Schmidt process on a linearly dependent set then we get some zero vectors. For instance, with
S = {
µ
1
2
¶
,
µ
3
6
¶
}
we would get this.
⃗κ1 =
µ
1
2
¶
⃗κ2 =
µ
3
6
¶
−proj[⃗κ1](
µ
3
6
¶
) =
µ
0
0
¶
This ﬁrst thing is not so bad because the zero vector is by deﬁnition orthogonal to every other vector, so
we could accept this situation as yielding an orthogonal set (although it of course can’t be normalized),
or we just could modify the Gram-Schmidt procedure to throw out any zero vectors. The second thing
to worry about if we drop the phrase ‘linearly independent’ from the question is that the set might be
inﬁnite. Of course, any subspace of the ﬁnite-dimensional Rn must also be ﬁnite-dimensional so only
ﬁnitely many of its members are linearly independent, but nonetheless, a “process” that examines the
vectors in an inﬁnite set one at a time would at least require some more elaboration in this question.
A linearly independent subset of Rn is automatically ﬁnite — in fact, of size n or less — so the ‘linearly
independent’ phrase obviates these concerns.
Three.VI.2.14
The process leaves the basis unchanged.
Three.VI.2.15
(a) The argument is as in the i = 3 case of the proof of Theorem 2.7. The dot
product
⃗κi
³
⃗v −proj[⃗κ1](⃗v ) −· · · −proj[⃗vk](⃗v )
´
can be written as the sum of terms of the form −⃗κi proj[⃗κj](⃗v ) with j ̸= i, and the term ⃗κi (⃗v −
proj[⃗κi](⃗v )). The ﬁrst kind of term equals zero because the ⃗κ’s are mutually orthogonal. The other
term is zero because this projection is orthogonal (that is, the projection deﬁnition makes it zero:
⃗κi (⃗v −proj[⃗κi](⃗v )) = ⃗κi ⃗v −⃗κi ((⃗v ⃗κi)/(⃗κi ⃗κi)) · ⃗κi equals, after all of the cancellation is done,
zero).

Answers to Exercises
137
(b) The vector ⃗v is shown in black and the vector proj[⃗κ1](⃗v ) + proj[⃗v2](⃗v ) = 1 · ⃗e1 + 2 · ⃗e2 is in gray.
The vector ⃗v −(proj[⃗κ1](⃗v ) + proj[⃗v2](⃗v )) lies on the dotted line connecting the black vector to the
gray one, that is, it is orthogonal to the xy-plane.
(c) This diagram is gotten by following the hint.
The dashed triangle has a right angle where the gray vector 1 · ⃗e1 + 2 · ⃗e2 meets the vertical dashed
line ⃗v −(1 · ⃗e1 + 2 · ⃗e2); this is what was proved in the ﬁrst item of this question. The Pythagorean
theorem then gives that the hypoteneuse — the segment from ⃗v to any other vector — is longer than
the vertical dashed line.
More formally, writing proj[⃗κ1](⃗v ) + · · · + proj[⃗vk](⃗v ) as c1 · ⃗κ1 + · · · + ck · ⃗κk, consider any other
vector in the span d1 · ⃗κ1 + · · · + dk · ⃗κk. Note that
⃗v −(d1 · ⃗κ1 + · · · + dk · ⃗κk)
=
¡
⃗v −(c1 · ⃗κ1 + · · · + ck · ⃗κk)
¢
+
¡
(c1 · ⃗κ1 + · · · + ck · ⃗κk) −(d1 · ⃗κ1 + · · · + dk · ⃗κk)
¢
and that
¡
⃗v −(c1 · ⃗κ1 + · · · + ck · ⃗κk)
¢ ¡
(c1 · ⃗κ1 + · · · + ck · ⃗κk) −(d1 · ⃗κ1 + · · · + dk · ⃗κk)
¢
= 0
(because the ﬁrst item shows the ⃗v −(c1 · ⃗κ1 + · · · + ck · ⃗κk) is orthogonal to each ⃗κ and so it is
orthogonal to this linear combination of the ⃗κ’s). Now apply the Pythagorean Theorem (i.e., the
Triangle Inequality).
Three.VI.2.16
One way to proceed is to ﬁnd a third vector so that the three together make a basis
for R3, e.g.,
⃗β3 =


1
0
0


(the second vector is not dependent on the third because it has a nonzero second component, and the
ﬁrst is not dependent on the second and third because of its nonzero third component), and then apply

138
Linear Algebra, by Hefferon
the Gram-Schmidt process.
⃗κ1 =


1
5
−1


⃗κ2 =


2
2
0

−proj[⃗κ1](


2
2
0

) =


2
2
0

−


2
2
0




1
5
−1




1
5
−1




1
5
−1


·


1
5
−1


=


2
2
0

−12
27 ·


1
5
−1

=


14/9
−2/9
4/9


⃗κ3 =


1
0
0

−proj[⃗κ1](


1
0
0

) −proj[⃗κ2](


1
0
0

)
=


1
0
0

−


1
0
0




1
5
−1




1
5
−1




1
5
−1


·


1
5
−1

−


1
0
0




14/9
−2/9
4/9




14/9
−2/9
4/9




14/9
−2/9
4/9


·


14/9
−2/9
4/9


=


1
0
0

−1
27 ·


1
5
−1

−7
12 ·


14/9
−2/9
4/9

=


1/18
−1/18
−4/18


The result ⃗κ3 is orthogonal to both ⃗κ1 and ⃗κ2. It is therefore orthogonal to every vector in the span
of the set {⃗κ1,⃗κ2}, including the two vectors given in the question.
Three.VI.2.17
(a) The representation can be done by eye.
µ
2
3
¶
= 3 ·
µ
1
1
¶
+ (−1) ·
µ
1
0
¶
RepB(⃗v ) =
µ
3
−1
¶
B
The two projections are also easy.
proj[⃗β1](
µ2
3
¶
) =
µ
2
3
¶ µ
1
1
¶
µ
1
1
¶ µ
1
1
¶ ·
µ1
1
¶
= 5
2 ·
µ1
1
¶
proj[⃗β2](
µ2
3
¶
) =
µ
2
3
¶ µ
1
0
¶
µ
1
0
¶ µ
1
0
¶ ·
µ1
0
¶
= 2
1 ·
µ1
0
¶
(b) As above, the representation can be done by eye
µ
2
3
¶
= (5/2) ·
µ
1
1
¶
+ (−1/2) ·
µ
1
−1
¶
and the two projections are easy.
proj[⃗β1](
µ
2
3
¶
) =
µ
2
3
¶ µ
1
1
¶
µ
1
1
¶ µ
1
1
¶·
µ
1
1
¶
= 5
2·
µ
1
1
¶
proj[⃗β2](
µ
2
3
¶
) =
µ
2
3
¶ µ
1
−1
¶
µ
1
−1
¶ µ
1
−1
¶·
µ
1
−1
¶
= −1
2 ·
µ
1
−1
¶
Note the recurrence of the 5/2 and the −1/2.
(c) Represent ⃗v with respect to the basis
RepK(⃗v ) =



r1
...
rk



so that ⃗v = r1⃗κ1 + · · · + rk⃗κk. To determine ri, take the dot product of both sides with ⃗κi.
⃗v ⃗κi = (r1⃗κ1 + · · · + rk⃗κk) ⃗κi = r1 · 0 + · · · + ri · (⃗κi ⃗κi) + · · · + rk · 0
Solving for ri yields the desired coeﬃcient.
(d) This is a restatement of the prior item.
Three.VI.2.18
First, ∥⃗v ∥2 = 42 + 32 + 22 + 12 = 50.

Answers to Exercises
139
(a) c1 = 4
(b) c1 = 4, c2 = 3
(c) c1 = 4, c2 = 3, c3 = 2, c4 = 1
For the proof, we will do only the k = 2 case because the completely general case is messier but no
more enlightening. We follow the hint (recall that for any vector ⃗w we have ∥⃗w ∥2 = ⃗w
⃗w).
0 ≤
µ
⃗v −
¡ ⃗v ⃗κ1
⃗κ1 ⃗κ1
· ⃗κ1 + ⃗v ⃗κ2
⃗κ2 ⃗κ2
· ⃗κ2
¢¶ µ
⃗v −
¡ ⃗v ⃗κ1
⃗κ1 ⃗κ1
· ⃗κ1 + ⃗v ⃗κ2
⃗κ2 ⃗κ2
· ⃗κ2
¢¶
= ⃗v ⃗v −2 · ⃗v
µ ⃗v ⃗κ1
⃗κ1 ⃗κ1
· ⃗κ1 + ⃗v ⃗κ2
⃗κ2 ⃗κ2
· ⃗κ2
¶
+
µ ⃗v ⃗κ1
⃗κ1 ⃗κ1
· ⃗κ1 + ⃗v ⃗κ2
⃗κ2 ⃗κ2
· ⃗κ2
¶ µ ⃗v ⃗κ1
⃗κ1 ⃗κ1
· ⃗κ1 + ⃗v ⃗κ2
⃗κ2 ⃗κ2
· ⃗κ2
¶
= ⃗v ⃗v −2 ·
µ ⃗v ⃗κ1
⃗κ1 ⃗κ1
· (⃗v ⃗κ1) + ⃗v ⃗κ2
⃗κ2 ⃗κ2
· (⃗v ⃗κ2)
¶
+
µ
( ⃗v ⃗κ1
⃗κ1 ⃗κ1
)2 · (⃗κ1 ⃗κ1) + ( ⃗v ⃗κ2
⃗κ2 ⃗κ2
)2 · (⃗κ2 ⃗κ2)
¶
(The two mixed terms in the third part of the third line are zero because ⃗κ1 and ⃗κ2 are orthogonal.)
The result now follows on gathering like terms and on recognizing that ⃗κ1
⃗κ1 = 1 and ⃗κ2
⃗κ2 = 1
because these vectors are given as members of an orthonormal set.
Three.VI.2.19
It is true, except for the zero vector. Every vector in Rn except the zero vector is in a
basis, and that basis can be orthogonalized.
Three.VI.2.20
The 3×3 case gives the idea. The set
{


a
d
g

,


b
e
h

,


c
f
i

}
is orthonormal if and only if these nine conditions all hold
¡a
d
g¢


a
d
g

= 1
¡a
d
g¢


b
e
h

= 0
¡a
d
g¢


c
f
i

= 0
¡
b
e
h
¢


a
d
g

= 0
¡
b
e
h
¢


b
e
h

= 1
¡
b
e
h
¢


c
f
i

= 0
¡
c
f
i
¢


a
d
g

= 0
¡
c
f
i
¢


b
e
h

= 0
¡
c
f
i
¢


c
f
i

= 1
(the three conditions in the lower left are redundant but nonetheless correct). Those, in turn, hold if
and only if


a
d
g
b
e
h
c
f
i




a
b
c
d
e
f
g
h
i

=


1
0
0
0
1
0
0
0
1


as required.
This is an example, the inverse of this matrix is its transpose.


1/
√
2
1/
√
2
0
−1/
√
2
1/
√
2
0
0
0
1


Three.VI.2.21
If the set is empty then the summation on the left side is the linear combination of the
empty set of vectors, which by deﬁnition adds to the zero vector. In the second sentence, there is not
such i, so the ‘if . . . then . . . ’ implication is vacuously true.
Three.VI.2.22
(a) Part of the induction argument proving Theorem 2.7 checks that ⃗κi is in the
span of ⟨⃗β1, . . . , ⃗βi⟩. (The i = 3 case in the proof illustrates.) Thus, in the change of basis matrix
RepK,B(id), the i-th column RepB(⃗κi) has components i + 1 through k that are zero.
(b) One way to see this is to recall the computational procedure that we use to ﬁnd the inverse. We
write the matrix, write the identity matrix next to it, and then we do Gauss-Jordan reduction. If the
matrix starts out upper triangular then the Gauss-Jordan reduction involves only the Jordan half
and these steps, when performed on the identity, will result in an upper triangular inverse matrix.
Three.VI.2.23
For the inductive step, we assume that for all j in [1..i], these three conditions are true
of each ⃗κj: (i) each ⃗κj is nonzero, (ii) each ⃗κj is a linear combination of the vectors ⃗β1, . . . , ⃗βj, and

140
Linear Algebra, by Hefferon
(iii) each ⃗κj is orthogonal to all of the ⃗κm’s prior to it (that is, with m < j). With those inductive
hypotheses, consider ⃗κi+1.
⃗κi+1 = ⃗βi+1 −proj[⃗κ1](βi+1) −proj[⃗κ2](βi+1) −· · · −proj[⃗κi](βi+1)
= ⃗βi+1 −βi+1 ⃗κ1
⃗κ1 ⃗κ1
· ⃗κ1 −βi+1 ⃗κ2
⃗κ2 ⃗κ2
· ⃗κ2 −· · · −βi+1 ⃗κi
⃗κi ⃗κi
· ⃗κi
By the inductive assumption (ii) we can expand each ⃗κj into a linear combination of ⃗β1, . . . , ⃗βj
= ⃗βi+1 −
⃗βi+1 ⃗κ1
⃗κ1 ⃗κ1
· ⃗β1
−
⃗βi+1 ⃗κ2
⃗κ2 ⃗κ2
·
³
linear combination of ⃗β1, ⃗β2
´
−· · · −
⃗βi+1 ⃗κi
⃗κi ⃗κi
·
³
linear combination of ⃗β1, . . . , ⃗βi
´
The fractions are scalars so this is a linear combination of linear combinations of ⃗β1, . . . , ⃗βi+1. It is
therefore just a linear combination of ⃗β1, . . . , ⃗βi+1. Now, (i) it cannot sum to the zero vector because
the equation would then describe a nontrivial linear relationship among the ⃗β’s that are given as
members of a basis (the relationship is nontrivial because the coeﬃcient of ⃗βi+1 is 1). Also, (ii) the
equation gives ⃗κi+1 as a combination of ⃗β1, . . . , ⃗βi+1. Finally, for (iii), consider ⃗κj ⃗κi+1; as in the i = 3
case, the dot product of ⃗κj with ⃗κi+1 = ⃗βi+1 −proj[⃗κ1](⃗βi+1) −· · · −proj[⃗κi](⃗βi+1) can be rewritten to
give two kinds of terms, ⃗κj
³
⃗βi+1 −proj[⃗κj](⃗βi+1)
´
(which is zero because the projection is orthogonal)
and ⃗κj
proj[⃗κm](⃗βi+1) with m ̸= j and m < i + 1 (which is zero because by the hypothesis (iii) the
vectors ⃗κj and ⃗κm are orthogonal).
Subsection Three.VI.3: Projection Into a Subspace
Three.VI.3.10
(a) When bases for the subspaces
BM = ⟨
µ
1
−1
¶
⟩
BN = ⟨
µ
2
−1
¶
⟩
are concatenated
B = BM
⌢BN = ⟨
µ
1
−1
¶
,
µ
2
−1
¶
⟩
and the given vector is representedµ
3
−2
¶
= 1 ·
µ
1
−1
¶
+ 1 ·
µ
2
−1
¶
then the answer comes from retaining the M part and dropping the N part.
projM,N(
µ
3
−2
¶
) =
µ
1
−1
¶
(b) When the bases
BM = ⟨
µ
1
1
¶
⟩
BN⟨
µ
1
−2
¶
⟩
are concatenated, and the vector is represented,
µ
1
2
¶
= (4/3) ·
µ
1
1
¶
−(1/3) ·
µ
1
−2
¶
then retaining only the M part gives this answer.
projM,N(
µ
1
2
¶
) =
µ
4/3
4/3
¶
(c) With these bases
BM = ⟨


1
−1
0

,


0
0
1

⟩
BN = ⟨


1
0
1

⟩

Answers to Exercises
141
the representation with respect to the concatenation is this.


3
0
1

= 0 ·


1
−1
0

−2 ·


0
0
1

+ 3 ·


1
0
1


and so the projection is this.
projM,N(


3
0
1

) =


0
0
−2


Three.VI.3.11
As in Example 3.5, we can simplify the calculation by just ﬁnding the space of vectors
perpendicular to all the the vectors in M’s basis.
(a) Parametrizing to get
M = {c ·
µ
−1
1
¶ ¯¯ c ∈R}
gives that
M ⊥{
µ
u
v
¶ ¯¯ 0 =
µ
u
v
¶ µ
−1
1
¶
} = {
µ
u
v
¶ ¯¯ 0 = −u + v}
Parametrizing the one-equation linear system gives this description.
M ⊥= {k ·
µ
1
1
¶ ¯¯ k ∈R}
(b) As in the answer to the prior part, M can be described as a span
M = {c ·
µ3/2
1
¶ ¯¯ c ∈R}
BM = ⟨
µ3/2
1
¶
⟩
and then M ⊥is the set of vectors perpendicular to the one vector in this basis.
M ⊥= {
µu
v
¶ ¯¯ (3/2) · u + 1 · v = 0} = {k ·
µ−2/3
1
¶ ¯¯ k ∈R}
(c) Parametrizing the linear requirement in the description of M gives this basis.
M = {c ·
µ
1
1
¶ ¯¯ c ∈R}
BM = ⟨
µ
1
1
¶
⟩
Now, M ⊥is the set of vectors perpendicular to (the one vector in) BM.
M ⊥= {
µu
v
¶ ¯¯ u + v = 0} = {k ·
µ−1
1
¶ ¯¯ k ∈R}
(By the way, this answer checks with the ﬁrst item in this question.)
(d) Every vector in the space is perpendicular to the zero vector so M ⊥= Rn.
(e) The appropriate description and basis for M are routine.
M = {y ·
µ0
1
¶ ¯¯ y ∈R}
BM = ⟨
µ0
1
¶
⟩
Then
M ⊥= {
µ
u
v
¶ ¯¯ 0 · u + 1 · v = 0} = {k ·
µ
1
0
¶ ¯¯ k ∈R}
and so (y-axis)⊥= x-axis.
(f) The description of M is easy to ﬁnd by parametrizing.
M = {c ·


3
1
0

+ d ·


1
0
1

¯¯ c, d ∈R}
BM = ⟨


3
1
0

,


1
0
1

⟩
Finding M ⊥here just requires solving a linear system with two equations
3u + v
= 0
u
+ w = 0
−(1/3)ρ1+ρ2
−→
3u +
v
= 0
−(1/3)v + w = 0
and parametrizing.
M ⊥= {k ·


−1
3
1

¯¯ k ∈R}

142
Linear Algebra, by Hefferon
(g) Here, M is one-dimensional
M = {c ·


0
−1
1

¯¯ c ∈R}
BM = ⟨


0
−1
1

⟩
and as a result, M ⊥is two-dimensional.
M ⊥= {


u
v
w

¯¯ 0 · u −1 · v + 1 · w = 0} = {j ·


1
0
0

+ k ·


0
1
1

¯¯ j, k ∈R}
Three.VI.3.12
(a) Parametrizing the equation leads to this basis for P.
BP = ⟨


1
0
3

,


0
1
2

⟩
(b) Because R3 is three-dimensional and P is two-dimensional, the complement P ⊥must be a line.
Anyway, the calculation as in Example 3.5
P ⊥= {


x
y
z

¯¯
µ
1
0
3
0
1
2
¶ 

x
y
z

=
µ
0
0
¶
}
gives this basis for P ⊥.
BP ⊥= ⟨


3
2
−1

⟩
(c)


1
1
2

= (5/14) ·


1
0
3

+ (8/14) ·


0
1
2

+ (3/14) ·


3
2
−1


(d) projP (


1
1
2

) =


5/14
8/14
31/14


(e) The matrix of the projection


1
0
0
1
3
2

¡µ1
0
3
0
1
2
¶ 

1
0
0
1
3
2

¢−1
µ1
0
3
0
1
2
¶
=


1
0
0
1
3
2


µ10
6
6
5
¶−1 µ1
0
3
0
1
2
¶
= 1
14


5
−6
3
−6
10
2
3
2
13


when applied to the vector, yields the expected result.
1
14


5
−6
3
−6
10
2
3
2
13




1
1
2

=


5/14
8/14
31/14


Three.VI.3.13
(a) Parametrizing gives this.
M = {c ·
µ−1
1
¶ ¯¯ c ∈R}
For the ﬁrst way, we take the vector spanning the line M to be
⃗s =
µ−1
1
¶
and the Deﬁnition 1.1 formula gives this.
proj[⃗s ](
µ
1
−3
¶
) =
µ 1
−3
¶ µ−1
1
¶
µ
−1
1
¶ µ
−1
1
¶ ·
µ
−1
1
¶
= −4
2 ·
µ
−1
1
¶
=
µ
2
−2
¶
For the second way, we ﬁx
BM = ⟨
µ
−1
1
¶
⟩

Answers to Exercises
143
and so (as in Example 3.5 and 3.6, we can just ﬁnd the vectors perpendicular to all of the members
of the basis)
M ⊥= {
µ
u
v
¶ ¯¯ −1 · u + 1 · v = 0} = {k ·
µ
1
1
¶ ¯¯ k ∈R}
BM ⊥= ⟨
µ
1
1
¶
⟩
and representing the vector with respect to the concatenation gives this.
µ
1
−3
¶
= −2 ·
µ
−1
1
¶
−1 ·
µ
1
1
¶
Keeping the M part yields the answer.
projM,M ⊥(
µ
1
−3
¶
) =
µ
2
−2
¶
The third part is also a simple calculation (there is a 1×1 matrix in the middle, and the inverse
of it is also 1×1)
A
¡
AtransA
¢−1 Atrans =
µ
−1
1
¶ µ¡
−1
1
¢ µ
−1
1
¶¶−1 ¡
−1
1
¢
=
µ
−1
1
¶ ¡
2
¢−1 ¡
−1
1
¢
=
µ
−1
1
¶ ¡
1/2
¢ ¡
−1
1
¢
=
µ
−1
1
¶ ¡
−1/2
1/2
¢
=
µ
1/2
−1/2
−1/2
1/2
¶
which of course gives the same answer.
projM(
µ
1
−3
¶
) =
µ
1/2
−1/2
−1/2
1/2
¶ µ
1
−3
¶
=
µ
2
−2
¶
(b) Parametrization gives this.
M = {c ·


−1
0
1

¯¯ c ∈R}
With that, the formula for the ﬁrst way gives this.


0
1
2




−1
0
1




−1
0
1




−1
0
1


·


−1
0
1

= 2
2 ·


−1
0
1

=


−1
0
1


To proceed by the second method we ﬁnd M ⊥,
M ⊥= {


u
v
w

¯¯ −u + w = 0} = {j ·


1
0
1

+ k ·


0
1
0

¯¯ j, k ∈R}
ﬁnd the representation of the given vector with respect to the concatenation of the bases BM and
BM ⊥


0
1
2

= 1 ·


−1
0
1

+ 1 ·


1
0
1

+ 1 ·


0
1
0


and retain only the M part.
projM(


0
1
2

) = 1 ·


−1
0
1

=


−1
0
1


Finally, for the third method, the matrix calculation
A
¡
AtransA
¢−1 Atrans =


−1
0
1

¡¡
−1
0
1
¢


−1
0
1

¢−1 ¡
−1
0
1
¢
=


−1
0
1

¡
2
¢−1 ¡
−1
0
1
¢
=


−1
0
1

¡
1/2
¢ ¡
−1
0
1
¢
=


−1
0
1

¡
−1/2
0
1/2
¢
=


1/2
0
−1/2
0
0
0
−1/2
0
1/2


followed by matrix-vector multiplication
projM(


0
1
2

)


1/2
0
−1/2
0
0
0
−1/2
0
1/2




0
1
2

=


−1
0
1


gives the answer.

144
Linear Algebra, by Hefferon
Three.VI.3.14
No, a decomposition of vectors ⃗v = ⃗m +⃗n into ⃗m ∈M and ⃗n ∈N does not depend on
the bases chosen for the subspaces — this was shown in the Direct Sum subsection.
Three.VI.3.15
The orthogonal projection of a vector into a subspace is a member of that subspace.
Since a trivial subspace has only one member, ⃗0, the projection of any vector must equal ⃗0.
Three.VI.3.16
The projection into M along N of a ⃗v ∈M is ⃗v. Decomposing ⃗v = ⃗m + ⃗n gives ⃗m = ⃗v
and ⃗n = ⃗0, and dropping the N part but retaining the M part results in a projection of ⃗m = ⃗v.
Three.VI.3.17
The proof of Lemma 3.7 shows that each vector ⃗v ∈Rn is the sum of its orthogonal
projections onto the lines spanned by the basis vectors.
⃗v = proj[⃗κ1](⃗v ) + · · · + proj[⃗κn](⃗v ) = ⃗v ⃗κ1
⃗κ1 ⃗κ1
· ⃗κ1 + · · · + ⃗v ⃗κn
⃗κn ⃗κn
· ⃗κn
Since the basis is orthonormal, the bottom of each fraction has ⃗κi ⃗κi = 1.
Three.VI.3.18
If V = M ⊕N then every vector can be decomposed uniquely as ⃗v = ⃗m + ⃗n. For all ⃗v
the map p gives p(⃗v) = ⃗m if and only if ⃗v −p(⃗v) = ⃗n, as required.
Three.VI.3.19
Let ⃗v be perpendicular to every ⃗w ∈S. Then ⃗v
(c1 ⃗w1 + · · · + cn ⃗wn) = ⃗v
(c1 ⃗w1) +
· · · + ⃗v (cn
⃗wn) = c1(⃗v
⃗w1) + · · · + cn(⃗v
⃗wn) = c1 · 0 + · · · + cn · 0 = 0.
Three.VI.3.20
True; the only vector orthogonal to itself is the zero vector.
Three.VI.3.21
This is immediate from the statement in Lemma 3.7 that the space is the direct sum
of the two.
Three.VI.3.22
The two must be equal, even only under the seemingly weaker condition that they yield
the same result on all orthogonal projections. Consider the subspace M spanned by the set {⃗v1,⃗v2}.
Since each is in M, the orthogonal projection of ⃗v1 into M is ⃗v1 and the orthogonal projection of ⃗v2
into M is ⃗v2. For their projections into M to be equal, they must be equal.
Three.VI.3.23
(a) We will show that the sets are mutually inclusive, M ⊆(M ⊥)⊥and (M ⊥)⊥⊆M.
For the ﬁrst, if ⃗m ∈M then by the deﬁnition of the perp operation, ⃗m is perpendicular to every
⃗v ∈M ⊥, and therefore (again by the deﬁnition of the perp operation) ⃗m ∈(M ⊥)⊥. For the other
direction, consider ⃗v ∈(M ⊥)⊥. Lemma 3.7’s proof shows that Rn = M ⊕M ⊥and that we can give
an orthogonal basis for the space ⟨⃗κ1, . . . ,⃗κk,⃗κk+1, . . . ,⃗κn⟩such that the ﬁrst half ⟨⃗κ1, . . . ,⃗κk⟩is a
basis for M and the second half is a basis for M ⊥. The proof also checks that each vector in the
space is the sum of its orthogonal projections onto the lines spanned by these basis vectors.
⃗v = proj[⃗κ1](⃗v ) + · · · + proj[⃗κn](⃗v )
Because ⃗v ∈(M ⊥)⊥, it is perpendicular to every vector in M ⊥, and so the projections in the second
half are all zero. Thus ⃗v = proj[⃗κ1](⃗v ) + · · · + proj[⃗κk](⃗v ), which is a linear combination of vectors
from M, and so ⃗v ∈M. (Remark. Here is a slicker way to do the second half: write the space both
as M ⊕M ⊥and as M ⊥⊕(M ⊥)⊥. Because the ﬁrst half showed that M ⊆(M ⊥)⊥and the prior
sentence shows that the dimension of the two subspaces M and (M ⊥)⊥are equal, we can conclude
that M equals (M ⊥)⊥.)
(b) Because M ⊆N, any ⃗v that is perpendicular to every vector in N is also perpendicular to every
vector in M. But that sentence simply says that N ⊥⊆M ⊥.
(c) We will again show that the sets are equal by mutual inclusion. The ﬁrst direction is easy; any ⃗v
perpendicular to every vector in M +N = {⃗m + ⃗n
¯¯ ⃗m ∈M, ⃗n ∈N} is perpendicular to every vector
of the form ⃗m + ⃗0 (that is, every vector in M) and every vector of the form ⃗0 + ⃗n (every vector in
N), and so (M + N)⊥⊆M ⊥∩N ⊥. The second direction is also routine; any vector ⃗v ∈M ⊥∩N ⊥
is perpendicular to any vector of the form c⃗m + d⃗n because ⃗v (c⃗m + d⃗n) = c · (⃗v
⃗m) + d · (⃗v ⃗n) =
c · 0 + d · 0 = 0.
Three.VI.3.24
(a) The representation of


v1
v2
v3


f
7−→1v1 + 2v2 + 3v3
is this.
RepE3,E1(f) =
¡
1
2
3
¢
By the deﬁnition of f
N (f) = {


v1
v2
v3

¯¯ 1v1 + 2v2 + 3v3 = 0} = {


v1
v2
v3

¯¯


1
2
3




v1
v2
v3

= 0}

Answers to Exercises
145
and this second description exactly says this.
N (f)⊥= [{


1
2
3

}]
(b) The generalization is that for any f : Rn →R there is a vector ⃗h so that



v1
...
vn



f
7−→h1v1 + · · · + hnvn
and ⃗h ∈N (f)⊥. We can prove this by, as in the prior item, representing f with respect to the
standard bases and taking ⃗h to be the column vector gotten by transposing the one row of that
matrix representation.
(c) Of course,
RepE3,E2(f) =
µ1
2
3
4
5
6
¶
and so the nullspace is this set.
N (f){


v1
v2
v3

¯¯
µ
1
2
3
4
5
6
¶ 

v1
v2
v3

=
µ
0
0
¶
}
That description makes clear that


1
2
3

,


4
5
6

∈N (f)⊥
and since N (f)⊥is a subspace of Rn, the span of the two vectors is a subspace of the perp of the
nullspace. To see that this containment is an equality, take
M = [{


1
2
3

}]
N = [{


4
5
6

}]
in the third item of Exercise 23, as suggested in the hint.
(d) As above, generalizing from the speciﬁc case is easy: for any f : Rn →Rm the matrix H repre-
senting the map with respect to the standard bases describes the action



v1
...
vn



f
7−→



h1,1v1 + h1,2v2 + · · · + h1,nvn
...
hm,1v1 + hm,2v2 + · · · + hm,nvn



and the description of the nullspace gives that on transposing the m rows of H
⃗h1 =





h1,1
h1,2
...
h1,n




, . . .⃗hm =





hm,1
hm,2
...
hm,n





we have N (f)⊥= [{⃗h1, . . . ,⃗hm}]. (In [Strang 93], this space is described as the transpose of the
row space of H.)
Three.VI.3.25
(a) First note that if a vector ⃗v is already in the line then the orthogonal projection
gives ⃗v itself. One way to verify this is to apply the formula for projection into the line spanned by
a vector ⃗s, namely (⃗v ⃗s/⃗s ⃗s) · ⃗s. Taking the line as {k · ⃗v
¯¯ k ∈R} (the ⃗v = ⃗0 case is separate but
easy) gives (⃗v ⃗v/⃗v ⃗v) · ⃗v, which simpliﬁes to ⃗v, as required.
Now, that answers the question because after once projecting into the line, the result projℓ(⃗v) is
in that line. The prior paragraph says that projecting into the same line again will have no eﬀect.
(b) The argument here is similar to the one in the prior item. With V = M ⊕N, the projection of
⃗v = ⃗m+⃗n is projM,N(⃗v ) = ⃗m. Now repeating the projection will give projM,N(⃗m) = ⃗m, as required,
because the decomposition of a member of M into the sum of a member of M and a member of N
is ⃗m = ⃗m +⃗0. Thus, projecting twice into M along N has the same eﬀect as projecting once.

146
Linear Algebra, by Hefferon
(c) As suggested by the prior items, the condition gives that t leaves vectors in the rangespace
unchanged, and hints that we should take ⃗β1, . . . , ⃗βr to be basis vectors for the range, that is, that
we should take the range space of t for M (so that dim(M) = r). As for the complement, we write
N for the nullspace of t and we will show that V = M ⊕N.
To show this, we can show that their intersection is trivial M ∩N = {⃗0} and that they sum to the
entire space M + N = V . For the ﬁrst, if a vector ⃗m is in the rangespace then there is a ⃗v ∈V with
t(⃗v) = ⃗m, and the condition on t gives that t(⃗m) = (t◦t) (⃗v) = t(⃗v) = ⃗m, while if that same vector is
also in the nullspace then t(⃗m) = ⃗0 and so the intersection of the rangespace and nullspace is trivial.
For the second, to write an arbitrary ⃗v as the sum of a vector from the rangespace and a vector from
the nullspace, the fact that the condition t(⃗v) = t(t(⃗v)) can be rewritten as t(⃗v −t(⃗v)) = ⃗0 suggests
taking ⃗v = t(⃗v) + (⃗v −t(⃗v)).
So we are ﬁnished on taking a basis B = ⟨⃗β1, . . . , ⃗βn⟩for V where ⟨⃗β1, . . . , ⃗βr⟩is a basis for the
rangespace M and ⟨⃗βr+1, . . . , ⃗βn⟩is a basis for the nullspace N.
(d) Every projection (as deﬁned in this exercise) is a projection into its rangespace and along its
nullspace.
(e) This also follows immediately from the third item.
Three.VI.3.26
For any matrix M we have that (M −1)
trans = (M trans)−1, and for any two matrices
M, N we have that MN trans = N transM trans (provided, of course, that the inverse and product are
deﬁned). Applying these two gives that the matrix equals its transpose.
¡
A(AtransA)−1Atrans¢trans = (Atranstrans)(
¡
(AtransA)−1¢trans)(Atrans)
= (Atranstrans)(
¡
(AtransA)
trans¢−1)(Atrans) = A(AtransAtranstrans)−1Atrans = A(AtransA)−1Atrans
Topic: Line of Best Fit
Data on the progression of the world’s records (taken from the Runner’s World web site) is below.
1
As with the ﬁrst example discussed above, we are trying to ﬁnd a best m to “solve” this system.
8m = 4
16m = 9
24m = 13
32m = 17
40m = 20
Projecting into the linear subspace gives this






4
9
13
17
20












8
16
24
32
40












8
16
24
32
40












8
16
24
32
40






·






8
16
24
32
40






= 1832
3520 ·






8
16
24
32
40






so the slope of the line of best ﬁt is approximately 0.52.
0
10
20
30
40
0
5
10
15
20

Answers to Exercises
147
Progression of Men’s Mile Record
time
name
date
4:52.0
Cadet Marshall (GBR)
02Sep52
4:45.0
Thomas Finch (GBR)
03Nov58
4:40.0
Gerald Surman (GBR)
24Nov59
4:33.0
George Farran (IRL)
23May62
4:29 3/5
Walter Chinnery (GBR)
10Mar68
4:28 4/5
William Gibbs (GBR)
03Apr68
4:28 3/5
Charles Gunton (GBR)
31Mar73
4:26.0
Walter Slade (GBR)
30May74
4:24 1/2
Walter Slade (GBR)
19Jun75
4:23 1/5
Walter George (GBR)
16Aug80
4:19 2/5
Walter George (GBR)
03Jun82
4:18 2/5
Walter George (GBR)
21Jun84
4:17 4/5
Thomas Conneﬀ(USA)
26Aug93
4:17.0
Fred Bacon (GBR)
06Jul95
4:15 3/5
Thomas Conneﬀ(USA)
28Aug95
4:15 2/5
John Paul Jones (USA)
27May11
4:14.4
John Paul Jones (USA)
31May13
4:12.6
Norman Taber (USA)
16Jul15
4:10.4
Paavo Nurmi (FIN)
23Aug23
4:09 1/5
Jules Ladoumegue (FRA)
04Oct31
4:07.6
Jack Lovelock (NZL)
15Jul33
4:06.8
Glenn Cunningham (USA)
16Jun34
4:06.4
Sydney Wooderson (GBR)
28Aug37
4:06.2
Gunder Hagg (SWE)
01Jul42
4:04.6
Gunder Hagg (SWE)
04Sep42
4:02.6
Arne Andersson (SWE)
01Jul43
4:01.6
Arne Andersson (SWE)
18Jul44
4:01.4
Gunder Hagg (SWE)
17Jul45
3:59.4
Roger Bannister (GBR)
06May54
3:58.0
John Landy (AUS)
21Jun54
3:57.2
Derek Ibbotson (GBR)
19Jul57
3:54.5
Herb Elliott (AUS)
06Aug58
3:54.4
Peter Snell (NZL)
27Jan62
3:54.1
Peter Snell (NZL)
17Nov64
3:53.6
Michel Jazy (FRA)
09Jun65
3:51.3
Jim Ryun (USA)
17Jul66
3:51.1
Jim Ryun (USA)
23Jun67
3:51.0
Filbert Bayi (TAN)
17May75
3:49.4
John Walker (NZL)
12Aug75
3:49.0
Sebastian Coe (GBR)
17Jul79
3:48.8
Steve Ovett (GBR)
01Jul80
3:48.53
Sebastian Coe (GBR)
19Aug81
3:48.40
Steve Ovett (GBR)
26Aug81
3:47.33
Sebastian Coe (GBR)
28Aug81
3:46.32
Steve Cram (GBR)
27Jul85
3:44.39
Noureddine Morceli (ALG)
05Sep93
3:43.13
Hicham el Guerrouj (MOR)
07Jul99
Progression of Men’s 1500 Meter Record
time
name
date
4:09.0
John Bray (USA)
30May00
4:06.2
Charles Bennett (GBR)
15Jul00
4:05.4
James Lightbody (USA)
03Sep04
3:59.8
Harold Wilson (GBR)
30May08
3:59.2
Abel Kiviat (USA)
26May12
3:56.8
Abel Kiviat (USA)
02Jun12
3:55.8
Abel Kiviat (USA)
08Jun12
3:55.0
Norman Taber (USA)
16Jul15
3:54.7
John Zander (SWE)
05Aug17
3:53.0
Paavo Nurmi (FIN)
23Aug23
3:52.6
Paavo Nurmi (FIN)
19Jun24
3:51.0
Otto Peltzer (GER)
11Sep26
3:49.2
Jules Ladoumegue (FRA)
05Oct30
3:49.0
Luigi Beccali (ITA)
17Sep33
3:48.8
William Bonthron (USA)
30Jun34
3:47.8
Jack Lovelock (NZL)
06Aug36
3:47.6
Gunder Hagg (SWE)
10Aug41
3:45.8
Gunder Hagg (SWE)
17Jul42
3:45.0
Arne Andersson (SWE)
17Aug43
3:43.0
Gunder Hagg (SWE)
07Jul44
3:42.8
Wes Santee (USA)
04Jun54
3:41.8
John Landy (AUS)
21Jun54
3:40.8
Sandor Iharos (HUN)
28Jul55
3:40.6
Istvan Rozsavolgyi (HUN)
03Aug56
3:40.2
Olavi Salsola (FIN)
11Jul57
3:38.1
Stanislav Jungwirth (CZE)
12Jul57
3:36.0
Herb Elliott (AUS)
28Aug58
3:35.6
Herb Elliott (AUS)
06Sep60
3:33.1
Jim Ryun (USA)
08Jul67
3:32.2
Filbert Bayi (TAN)
02Feb74
3:32.1
Sebastian Coe (GBR)
15Aug79
3:31.36
Steve Ovett (GBR)
27Aug80
3:31.24
Sydney Maree (usa)
28Aug83
3:30.77
Steve Ovett (GBR)
04Sep83
3:29.67
Steve Cram (GBR)
16Jul85
3:29.46
Said Aouita (MOR)
23Aug85
3:28.86
Noureddine Morceli (ALG)
06Sep92
3:27.37
Noureddine Morceli (ALG)
12Jul95
3:26.00
Hicham el Guerrouj (MOR)
14Jul98
Progression of Women’s Mile Record
time
name
date
6:13.2
Elizabeth Atkinson (GBR)
24Jun21
5:27.5
Ruth Christmas (GBR)
20Aug32
5:24.0
Gladys Lunn (GBR)
01Jun36
5:23.0
Gladys Lunn (GBR)
18Jul36
5:20.8
Gladys Lunn (GBR)
08May37
5:17.0
Gladys Lunn (GBR)
07Aug37
5:15.3
Evelyne Forster (GBR)
22Jul39
5:11.0
Anne Oliver (GBR)
14Jun52
5:09.8
Enid Harding (GBR)
04Jul53
5:08.0
Anne Oliver (GBR)
12Sep53
5:02.6
Diane Leather (GBR)
30Sep53
5:00.3
Edith Treybal (ROM)
01Nov53
5:00.2
Diane Leather (GBR)
26May54
4:59.6
Diane Leather (GBR)
29May54
4:50.8
Diane Leather (GBR)
24May55
4:45.0
Diane Leather (GBR)
21Sep55
4:41.4
Marise Chamberlain (NZL)
08Dec62
4:39.2
Anne Smith (GBR)
13May67
4:37.0
Anne Smith (GBR)
03Jun67
4:36.8
Maria Gommers (HOL)
14Jun69
4:35.3
Ellen Tittel (FRG)
20Aug71
4:34.9
Glenda Reiser (CAN)
07Jul73
4:29.5
Paola Pigni-Cacchi (ITA)
08Aug73
4:23.8
Natalia Marasescu (ROM)
21May77
4:22.1
Natalia Marasescu (ROM)
27Jan79
4:21.7
Mary Decker (USA)
26Jan80
4:20.89
Lyudmila Veselkova (SOV)
12Sep81
4:18.08
Mary Decker-Tabb (USA)
09Jul82
4:17.44
Maricica Puica (ROM)
16Sep82
4:15.8
Natalya Artyomova (SOV)
05Aug84
4:16.71
Mary Decker-Slaney (USA)
21Aug85
4:15.61
Paula Ivan (ROM)
10Jul89
4:12.56
Svetlana Masterkova (RUS)
14Aug96

148
Linear Algebra, by Hefferon
2
With this input
A =







1
1852.71
1
1858.88
...
...
1
1985.54
1
1993.71







b =







292.0
285.0
...
226.32
224.39







(the dates have been rounded to months, e.g., for a September record, the decimal .71 ≈(8.5/12) was
used), Maple responded with an intercept of b = 994.8276974 and a slope of m = −0.3871993827.
1850
1900
1950
2000
220
240
260
280
3
With this input (the years are zeroed at 1900)
A :=







1
.38
1
.54
......
1
92.71
1
95.54







b =







249.0
246.2
...
208.86
207.37







(the dates have been rounded to months, e.g., for a September record, the decimal .71 ≈(8.5/12) was
used), Maple gives an intercept of b = 243.1590327 and a slope of m = −0.401647703. The slope given
in the body of this Topic for the men’s mile is quite close to this.
1900 1920 1940 1960 1980 2000
200
210
220
230
240
250
4
With this input (the years are zeroed at 1900)
A =







1
21.46
1
32.63
...
...
1
89.54
1
96.63







b =







373.2
327.5
...
255.61
252.56







(the dates have been rounded to months, e.g., for a September record, the decimal .71 ≈(8.5/12) was
used), MAPLE gave an intercept of b = 378.7114894 and a slope of m = −1.445753225.
1900
1920
1940
1960
1980
2000
220
240
260
280
300
320
340
360
380

Answers to Exercises
149
5
These are the equations of the lines for men’s and women’s mile (the vertical intercept term of the
equation for the women’s mile has been adjusted from the answer above, to zero it at the year 0,
because that’s how the men’s mile equation was done).
y = 994.8276974 −0.3871993827x
y = 3125.6426 −1.445753225x
Obviously the lines cross. A computer program is the easiest way to do the arithmetic: MuPAD gives
x = 2012.949004 and y = 215.4150856 (215 seconds is 3 minutes and 35 seconds). Remark. Of course
all of this projection is highly dubious — for one thing, the equation for the women is inﬂuenced by
the quite slow early times — but it is nonetheless fun.
1850
1900
1950
2000
220
240
260
280
300
320
340
360
380
6
(a) A computer algebra system like MAPLE or MuPAD will give an intercept of b = 4259/1398 ≈
3.239628 and a slope of m = −71/2796 ≈−0.025393419 Plugging x = 31 into the equation yields
a predicted number of O-ring failures of y = 2.45 (rounded to two places). Plugging in y = 4 and
solving gives a temperature of x = −29.94◦F.
(b) On the basis of this information
A =







1
53
1
75
...
1
80
1
81







b =







3
2
...
0
0







MAPLE gives the intercept b = 187/40 = 4.675 and the slope m = −73/1200 ≈−0.060833. Here,
plugging x = 31 into the equation predicts y = 2.79 O-ring failures (rounded to two places). Plugging
in y = 4 failures gives a temperature of x = 11◦F.
40
50
60
70
80
0
1
2
3
7
(a) The plot is nonlinear.
0
2
4
6
0
5
10
15
20
(b) Here is the plot.

150
Linear Algebra, by Hefferon
0
2
4
6
−0.5
0
0.5
1
There is perhaps a jog up between planet 4 and planet 5.
(c) This plot seems even more linear.
0
2
4
6
8
−0.5
0
0.5
1
(d) With this input
A =










1
1
1
2
1
3
1
4
1
6
1
7
1
8










b =










−0.40893539
−0.1426675
0
0.18184359
0.71600334
0.97954837
1.2833012










MuPAD gives that the intercept is b = −0.6780677466 and the slope is m = 0.2372763818.
(e) Plugging x = 9 into the equation y = −0.6780677466 + 0.2372763818x from the prior item gives
that the log of the distance is 1.4574197, so the expected distance is 28.669472. The actual distance
is about 30.003.
(f) Plugging x = 10 into the same equation gives that the log of the distance is 1.6946961, so the
expected distance is 49.510362. The actual distance is about 39.503.
8
(a) With this input
A =












1
306
1
329
1
356
1
367
1
396
1
427
1
415
1
424












b =












975
969
948
910
890
906
900
899












MAPLE gives the intercept b = 34009779/28796 ≈1181.0591 and the slope m = −19561/28796 ≈
−0.6793.
300 320 340 360 380 400 420
900
920
940
960
980

Answers to Exercises
151
Topic: Geometry of Linear Maps
1
(a) To represent H, recall that rotation counterclockwise by θ radians is represented with respect
to the standard basis in this way.
RepE2,E2(h) =
µ
cos θ
−sin θ
sin θ
cos θ
¶
A clockwise angle is the negative of a counterclockwise one.
RepE2,E2(h) =
µ
cos(−π/4)
−sin(−π/4)
sin(−π/4)
cos(−π/4)
¶
=
µ √
2/2
√
2/2
−
√
2/2
√
2/2
¶
This Gauss-Jordan reduction
ρ1+ρ2
−→
µ√
2/2
√
2/2
0
√
2
¶
(2/
√
2)ρ1
−→
(1/
√
2)ρ2
µ1
1
0
1
¶
−ρ2+ρ1
−→
µ1
0
0
1
¶
produces the identity matrix so there is no need for column-swapping operations to end with a
partial-identity.
(b) The reduction is expressed in matrix multiplication as
µ
1
−1
0
1
¶ µ
2/
√
2
0
0
1/
√
2
¶ µ
1
0
1
1
¶
H = I
(note that composition of the Gaussian operations is performed from right to left).
(c) Taking inverses
H =
µ
1
0
−1
1
¶ µ√
2/2
0
0
√
2
¶ µ
1
1
0
1
¶
|
{z
}
P
I
gives the desired factorization of H (here, the partial identity is I, and Q is trivial, that is, it is also
an identity matrix).
(d) Reading the composition from right to left (and ignoring the identity matrices as trivial) gives
that H has the same eﬀect as ﬁrst performing this skew
⃗u
⃗v
h(⃗u)
h(⃗v)
µx
y
¶
7→
µx + y
y
¶
−→
followed by a dilation that multiplies all ﬁrst components by
√
2/2 (this is a “shrink” in that
√
2/2 ≈0.707) and all second components by
√
2, followed by another skew.
⃗u
⃗v
h(⃗u)
h(⃗v)
µ
x
y
¶
7→
µ
x
−x + y
¶
−→
For instance, the eﬀect of H on the unit vector whose angle with the x-axis is π/3 is this.

152
Linear Algebra, by Hefferon
³√
3/2
1/2
´
³
(
√
3 + 1)/2
1/2
´
³√
2(
√
3 + 1)/2
√
2/2
´
³√
2(
√
3 + 1)/4
√
2(1 −
√
3)/4
´
µ
x
y
¶
7→
µ
x + y
y
¶
−→
µ
x
y
¶
7→
µ
(
√
2/2)x
√
2y
¶
−→
µ
x
y
¶
7→
µ
x
−x + y
¶
−→
Verifying that the resulting vector has unit length and forms an angle of −π/6 with the x-axis is
routine.
2
We will ﬁrst represent the map with a matrix H, perform the row operations and, if needed, column
operations to reduce it to a partial-identity matrix. We will then translate that into a factorization
H = PBQ. Subsitituting into the general matrix
RepE2,E2(rθ)
µcos θ
−sin θ
sin θ
cos θ
¶
gives this representation.
RepE2,E2(r2π/3)
µ
−1/2
−
√
3/2
√
3/2
−1/2
¶
Gauss’ method is routine.
√
3ρ1+ρ2
−→
µ
−1/2
−
√
3/2
0
−2
¶
−2ρ1
−→
(−1/2)ρ2
µ
1
√
3
0
1
¶
−
√
3ρ2+ρ1
−→
µ
1
0
0
1
¶
That translates to a matrix equation in this way.
µ
1
−
√
3
0
1
¶ µ
−2
0
0
−1/2
¶ µ 1
0
√
3
1
¶ µ
−1/2
−
√
3/2
√
3/2
−1/2
¶
= I
Taking inverses to solve for H yields this factorization.
µ−1/2
−
√
3/2
√
3/2
−1/2
¶
=
µ
1
0
−
√
3
1
¶ µ−1/2
0
0
−2
¶ µ
1
√
3
0
1
¶
I
3
This Gaussian reduction
−3ρ1+ρ2
−→
−ρ1+ρ3


1
2
1
0
0
−3
0
0
1

(1/3)ρ2+ρ3
−→


1
2
1
0
0
−3
0
0
0

(−1/3)ρ2
−→


1
2
1
0
0
1
0
0
0

−ρ2+ρ1
−→


1
2
0
0
0
1
0
0
0


gives the reduced echelon form of the matrix. Now the two column operations of taking −2 times the
ﬁrst column and adding it to the second, and then of swapping columns two and three produce this
partial identity.
B =


1
0
0
0
1
0
0
0
0


All of that translates into matrix terms as: where
P =


1
−1
0
0
1
0
0
0
1




1
0
0
0
−1/3
0
0
0
1




1
0
0
0
1
0
0
1/3
1




1
0
0
0
1
0
−1
0
1




1
0
0
−3
1
0
0
0
1


and
Q =


1
−2
0
0
1
0
0
0
1




0
1
0
1
0
0
0
0
1


the given matrix factors as PBQ.

Answers to Exercises
153
4
Represent it with respect to the standard bases E1, E1, then the only entry in the resulting 1×1 matrix
is the scalar k.
5
We can show this by induction on the number of components in the vector. In the n = 1 base case
the only permutation is the trivial one, and the map
¡
x1
¢
7→
¡
x1
¢
is indeed expressible as a composition of swaps — as zero swaps. For the inductive step we assume
that the map induced by any permutation of fewer than n numbers can be expressed with swaps only,
and we consider the map induced by a permutation p of n numbers.





x1
x2
...
xn




7→





xp(1)
xp(2)
...
xp(n)





Consider the number i such that p(i) = n. The map










x1
x2
...
xi
...
xn










ˆp
7−→










xp(1)
xp(2)
...
xp(n)
...
xn










will, when followed by the swap of the i-th and n-th components, give the map p. Now, the inductive
hypothesis gives that ˆp is achievable as a composition of swaps.
6
(a) A line is a subset of Rn of the form {⃗v = ⃗u + t · ⃗w
¯¯ t ∈R}. The image of a point on that line
is h(⃗v) = h(⃗u + t · ⃗w) = h(⃗u) + t · h(⃗w), and the set of such vectors, as t ranges over the reals, is a
line (albeit, degenerate if h(⃗w) = ⃗0).
(b) This is an obvious extension of the prior argument.
(c) If the point B is between the points A and C then the line from A to C has B in it. That is,
there is a t ∈(0 .. 1) such that ⃗b = ⃗a + t · (⃗c −⃗a) (where B is the endpoint of ⃗b, etc.). Now, as in the
argument of the ﬁrst item, linearity shows that h(⃗b) = h(⃗a) + t · h(⃗c −⃗a).
7
The two are inverse. For instance, for a ﬁxed x ∈R, if f ′(x) = k (with k ̸= 0) then (f −1)′(x) = 1/k.
x
f(x)
f−1(f(x))
Topic: Markov Chains
1
(a) With this ﬁle coin.m
# Octave function for Markov coin game.
p is chance of going down.
function w = coin(p,v)
q = 1-p;
A=[1,p,0,0,0,0;
0,0,p,0,0,0;
0,q,0,p,0,0;
0,0,q,0,p,0;
0,0,0,q,0,0;
0,0,0,0,q,1];
w = A * v;
endfunction

154
Linear Algebra, by Hefferon
This Octave session produced the output given here.
octave:1> v0=[0;0;0;1;0;0]
v0 =
0
0
0
1
0
0
octave:2> p=.5
p = 0.50000
octave:3> v1=coin(p,v0)
v1 =
0.00000
0.00000
0.50000
0.00000
0.50000
0.00000
octave:4> v2=coin(p,v1)
v2 =
0.00000
0.25000
0.00000
0.50000
0.00000
0.25000
This continued for too many steps to list here.
octave:26> v24=coin(p,v23)
v24 =
0.39600
0.00276
0.00000
0.00447
0.00000
0.59676
(b) Using these formulas
p1(n + 1) = 0.5 · p2(n)
p2(n + 1) = 0.5 · p1(n) + 0.5 · p3(n)
p3(n + 1) = 0.5 · p2(n) + 0.5 · p4(n)
p5(n + 1) = 0.5 · p4(n)
and these initial conditions








p0(0)
p1(0)
p2(0)
p3(0)
p4(0)
p5(0)








=








0
0
0
1
0
0








we will prove by induction that when n is odd then p1(n) = p3(n) = 0 and when n is even then
p2(n) = p4(n) = 0. Note ﬁrst that this is true in the n = 0 base case by the initial conditions. For
the inductive step, suppose that it is true in the n = 0, n = 1, . . . , n = k cases and consider the
n = k + 1 case. If k + 1 is odd then the two
p1(k + 1) = 0.5 · p2(k) = 0.5 · 0 = 0
p3(k + 1) = 0.5 · p2(k) + 0.5 · p4(k) = 0.5 · 0 + 0.5 · 0 = 0
follow from the inductive hypothesis that p2(k) = p4(k) = 0 since k is even. The case where k + 1
is even is similar.
(c) We can use, say, n = 100. This Octave session
octave:1> B=[1,.5,0,0,0,0;
>
0,0,.5,0,0,0;
>
0,.5,0,.5,0,0;
>
0,0,.5,0,.5,0;

Answers to Exercises
155
>
0,0,0,.5,0,0;
>
0,0,0,0,.5,1];
octave:2> B100=B**100
B100 =
1.00000
0.80000
0.60000
0.40000
0.20000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.20000
0.40000
0.60000
0.80000
1.00000
octave:3> B100*[0;1;0;0;0;0]
octave:4> B100*[0;1;0;0;0;0]
octave:5> B100*[0;0;0;1;0;0]
octave:6> B100*[0;1;0;0;0;0]
yields these outputs.
starting with:
$1
$2
$3
$4
s0(100)
s1(100)
s2(100)
s3(100)
s4(100)
s5(100)
0.80000
0.00000
0.00000
0.00000
0.00000
0.20000
0.60000
0.00000
0.00000
0.00000
0.00000
0.40000
0.40000
0.00000
0.00000
0.00000
0.00000
0.60000
0.20000
0.00000
0.00000
0.00000
0.00000
0.80000
2
(a) From these equations
(1/6)s1(n) +
0s2(n) +
0s3(n) +
0s4(n) +
0s5(n) +
0s6(n) = s1(n + 1)
(1/6)s1(n) + (2/6)s2(n) +
0s3(n) +
0s4(n) +
0s5(n) +
0s6(n) = s2(n + 1)
(1/6)s1(n) + (1/6)s2(n) + (3/6)s3(n) +
0s4(n) +
0s5(n) +
0s6(n) = s3(n + 1)
(1/6)s1(n) + (1/6)s2(n) + (1/6)s3(n) + (4/6)s4(n) +
0s5(n) +
0s6(n) = s4(n + 1)
(1/6)s1(n) + (1/6)s2(n) + (1/6)s3(n) + (1/6)s4(n) + (5/6)s5(n) +
0s6(n) = s5(n + 1)
(1/6)s1(n) + (1/6)s2(n) + (1/6)s3(n) + (1/6)s4(n) + (1/6)s5(n) + (6/6)s6(n) = s6(n + 1)
We get this transition matrix. 







1/6
0
0
0
0
0
1/6
2/6
0
0
0
0
1/6
1/6
3/6
0
0
0
1/6
1/6
1/6
4/6
0
0
1/6
1/6
1/6
1/6
5/6
0
1/6
1/6
1/6
1/6
1/6
6/6








(b) This is the Octave session, with outputs edited out and condensed into the table at the end.
octave:1>
F=[1/6,
0,
0,
0,
0,
0;
>
1/6,
2/6, 0,
0,
0,
0;
>
1/6,
1/6, 3/6, 0,
0,
0;
>
1/6,
1/6, 1/6, 4/6, 0,
0;
>
1/6,
1/6, 1/6, 1/6, 5/6, 0;
>
1/6,
1/6, 1/6, 1/6, 1/6, 6/6];
octave:2> v0=[1;0;0;0;0;0]
octave:3> v1=F*v0
octave:4> v2=F*v1
octave:5> v3=F*v2
octave:6> v4=F*v3
octave:7> v5=F*v4
These are the results.
1
2
3
4
5
1
0
0
0
0
0
0.16667
0.16667
0.16667
0.16667
0.16667
0.16667
0.027778
0.083333
0.138889
0.194444
0.250000
0.305556
0.0046296
0.0324074
0.0879630
0.1712963
0.2824074
0.4212963
0.00077160
0.01157407
0.05015432
0.13503086
0.28472222
0.51774691
0.00012860
0.00398663
0.02713477
0.10043724
0.27019033
0.59812243

156
Linear Algebra, by Hefferon
3
(a) It does seem reasonable that, while the ﬁrm’s present location should strongly inﬂuence where
it is next time (for instance, whether it stays), any locations in the prior stages should have little
inﬂuence. That is, while a company may move or stay because of where it is, it is unlikely to move
or stay because of where it was.
(b) This Octave session has been edited, with the outputs put together in a table at the end.
octave:1> M=[.787,0,0,.111,.102;
>
0,.966,.034,0,0;
>
0,.063,.937,0,0;
>
0,0,.074,.612,.314;
>
.021,.009,.005,.010,.954]
M =
0.78700
0.00000
0.00000
0.11100
0.10200
0.00000
0.96600
0.03400
0.00000
0.00000
0.00000
0.06300
0.93700
0.00000
0.00000
0.00000
0.00000
0.07400
0.61200
0.31400
0.02100
0.00900
0.00500
0.01000
0.95400
octave:2> v0=[.025;.025;.025;.025;.900]
octave:3> v1=M*v0
octave:4> v2=M*v1
octave:5> v3=M*v2
octave:6> v4=M*v3
is summarized in this table.
⃗p0
⃗p1
⃗p2
⃗p3
⃗p4






0.025000
0.025000
0.025000
0.025000
0.900000












0.114250
0.025000
0.025000
0.299750
0.859725












0.210879
0.025000
0.025000
0.455251
0.825924












0.300739
0.025000
0.025000
0.539804
0.797263












0.377920
0.025000
0.025000
0.582550
0.772652






(c) This is a continuation of the Octave session from the prior item.
octave:7> p0=[.0000;.6522;.3478;.0000;.0000]
octave:8> p1=M*p0
octave:9> p2=M*p1
octave:10> p3=M*p2
octave:11> p4=M*p3
This summarizes the output.
⃗p0
⃗p1
⃗p2
⃗p3
⃗p4






0.00000
0.65220
0.34780
0.00000
0.00000












0.00000
0.64185
0.36698
0.02574
0.00761












0.0036329
0.6325047
0.3842942
0.0452966
0.0151277












0.0094301
0.6240656
0.3999315
0.0609094
0.0225751












0.016485
0.616445
0.414052
0.073960
0.029960






(d) This is more of the same Octave session.
octave:12> M50=M**50
M50 =
0.03992
0.33666
0.20318
0.02198
0.37332
0.00000
0.65162
0.34838
0.00000
0.00000
0.00000
0.64553
0.35447
0.00000
0.00000
0.03384
0.38235
0.22511
0.01864
0.31652
0.04003
0.33316
0.20029
0.02204
0.37437
octave:13> p50=M50*p0
p50 =
0.29024
0.54615
0.54430
0.32766
0.28695
octave:14> p51=M*p50
p51 =
0.29406
0.54609

Answers to Exercises
157
0.54442
0.33091
0.29076
This is close to a steady state.
4
(a) This is the relevant system of equations.
(1 −2p) · sU(n) +
p · tA(n) +
p · tB(n)
= sU(n + 1)
p · sU(n) + (1 −2p) · tA(n)
= tA(n + 1)
p · sU(n)
+ (1 −2p) · tB(n)
= tB(n + 1)
p · tA(n)
+ sA(n)
= sA(n + 1)
p · tB(n)
+ sB(n) = sB(n + 1)
Thus we have this.





1 −2p
p
p
0
0
p
1 −2p
0
0
0
p
0
1 −2p
0
0
0
p
0
1
0
0
0
p
0
1












sU(n)
tA(n)
tB(n)
sA(n)
sB(n)






=






sU(n + 1)
tA(n + 1)
tB(n + 1)
sA(n + 1)
sB(n + 1)






(b) This is the Octave code, with the output removed.
octave:1> T=[.5,.25,.25,0,0;
>
.25,.5,0,0,0;
>
.25,0,.5,0,0;
>
0,.25,0,1,0;
>
0,0,.25,0,1]
T =
0.50000
0.25000
0.25000
0.00000
0.00000
0.25000
0.50000
0.00000
0.00000
0.00000
0.25000
0.00000
0.50000
0.00000
0.00000
0.00000
0.25000
0.00000
1.00000
0.00000
0.00000
0.00000
0.25000
0.00000
1.00000
octave:2> p0=[1;0;0;0;0]
octave:3> p1=T*p0
octave:4> p2=T*p1
octave:5> p3=T*p2
octave:6> p4=T*p3
octave:7> p5=T*p4
Here is the output. The probability of ending at sA is about 0.23.
⃗p0
⃗p1
⃗p2
⃗p3
⃗p4
⃗p5
sU
tA
tB
sA
sB
1
0
0
0
0
0.50000
0.25000
0.25000
0.00000
0.00000
0.375000
0.250000
0.250000
0.062500
0.062500
0.31250
0.21875
0.21875
0.12500
0.12500
0.26562
0.18750
0.18750
0.17969
0.17969
0.22656
0.16016
0.16016
0.22656
0.22656
(c) With this ﬁle as learn.m
# Octave script file for learning model.
function w = learn(p)
T = [1-2*p,p,
p,
0, 0;
p,
1-2*p,0,
0, 0;
p,
0,
1-2*p,0, 0;
0,
p,
0,
1, 0;
0,
0,
p,
0, 1];
T5 = T**5;
p5 = T5*[1;0;0;0;0];
w = p5(4);
endfunction
issuing the command octave:1> learn(.20) yields ans = 0.17664.
(d) This Octave session
octave:1> x=(.01:.01:.50)’;
octave:2> y=(.01:.01:.50)’;
octave:3> for i=.01:.01:.50
>
y(100*i)=learn(i);

158
Linear Algebra, by Hefferon
>
endfor
octave:4> z=[x, y];
octave:5> gplot z
yields this plot. There is no threshold value — no probability above which the curve rises sharply.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
line 1
5
(a) From these equations
0.90 · pT (n) + 0.01 · pC(n) = pT (n + 1)
0.10 · pT (n) + 0.99 · pC(n) = pC(n + 1)
we get this matrix.
µ0.90
0.01
0.10
0.99
¶ µpT (n)
pC(n)
¶
=
µpT (n + 1)
pC(n + 1)
¶
(b) This is the result from Octave.
n = 0
1
2
3
4
5
0.30000
0.70000
0.27700
0.72300
0.25653
0.74347
0.23831
0.76169
0.22210
0.77790
0.20767
0.79233
6
7
8
9
10
0.19482
0.80518
0.18339
0.81661
0.17322
0.82678
0.16417
0.83583
0.15611
0.84389
(c) This is the sT = 0.2 result.
n = 0
1
2
3
4
5
0.20000
0.80000
0.18800
0.81200
0.17732
0.82268
0.16781
0.83219
0.15936
0.84064
0.15183
0.84817
6
7
8
9
10
0.14513
0.85487
0.13916
0.86084
0.13385
0.86615
0.12913
0.87087
0.12493
0.87507
(d) Although the probability vectors start 0.1 apart, they end only 0.032 apart. So they are alike.
6
These are the p = .55 vectors,

Answers to Exercises
159
n = 0
n = 1
n = 2
n = 3
n = 4
n = 5
n = 6
n = 7
0-0
1-0
0-1
2-0
1-1
0-2
3-0
2-1
1-2
0-3
4-0
3-1
2-2
1-3
0-4
4-1
3-2
2-3
1-4
4-2
3-3
2-4
4-3
3-4
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.55000
0.45000
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.30250
0.49500
0.20250
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.16638
0.40837
0.33412
0.09112
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.09151
0.29948
0.36754
0.20047
0.04101
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.09151
0
0
0
0.04101
0.16471
0.33691
0.27565
0.09021
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.09151
0
0
0
0.04101
0.16471
0
0
0.09021
0.18530
0.30322
0.12404
0
0
0
0
0
0
0
0
0
0
0
0
0.09151
0
0
0
0.04101
0.16471
0
0
0.09021
0.18530
0
0.12404
0.16677
0.13645
and these are the p = .60 vectors.
n = 0
n = 1
n = 2
n = 3
n = 4
n = 5
n = 6
n = 7
0-0
1-0
0-1
2-0
1-1
0-2
3-0
2-1
1-2
0-3
4-0
3-1
2-2
1-3
0-4
4-1
3-2
2-3
1-4
4-2
3-3
2-4
4-3
3-4
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.60000
0.40000
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.36000
0.48000
0.16000
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.21600
0.43200
0.28800
0.06400
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.12960
0.34560
0.34560
0.15360
0.02560
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.12960
0
0
0
0.02560
0.20736
0.34560
0.23040
0.06144
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.12960
0
0
0
0.02560
0.20736
0
0
0.06144
0.20736
0.27648
0.09216
0
0
0
0
0
0
0
0
0
0
0
0
0.12960
0
0
0
0.02560
0.20736
0
0
0.06144
0.20736
0
0.09216
0.16589
0.11059
(a) The script from the computer code section can be easily adapted.
# Octave script file to compute chance of World Series outcomes.
function w = markov(p,v)
q = 1-p;
A=[0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 0-0
p,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 1-0
q,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 0-1_
0,p,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 2-0
0,q,p,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 1-1
0,0,q,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 0-2__
0,0,0,p,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 3-0

160
Linear Algebra, by Hefferon
0,0,0,q,p,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 2-1
0,0,0,0,q,p, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 1-2_
0,0,0,0,0,q, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 0-3
0,0,0,0,0,0, p,0,0,0,1,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 4-0
0,0,0,0,0,0, q,p,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 3-1__
0,0,0,0,0,0, 0,q,p,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 2-2
0,0,0,0,0,0, 0,0,q,p,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 1-3
0,0,0,0,0,0, 0,0,0,q,0,0, 0,0,1,0,0,0, 0,0,0,0,0,0;
# 0-4_
0,0,0,0,0,0, 0,0,0,0,0,p, 0,0,0,1,0,0, 0,0,0,0,0,0;
# 4-1
0,0,0,0,0,0, 0,0,0,0,0,q, p,0,0,0,0,0, 0,0,0,0,0,0;
# 3-2
0,0,0,0,0,0, 0,0,0,0,0,0, q,p,0,0,0,0, 0,0,0,0,0,0;
# 2-3__
0,0,0,0,0,0, 0,0,0,0,0,0, 0,q,0,0,0,0, 1,0,0,0,0,0;
# 1-4
0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,p,0, 0,1,0,0,0,0;
# 4-2
0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,q,p, 0,0,0,0,0,0;
# 3-3_
0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,q, 0,0,0,1,0,0;
# 2-4
0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,p,0,1,0;
# 4-3
0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,q,0,0,1]; # 3-4
v7 = (A**7) * v;
w = v7(11)+v7(16)+v7(20)+v7(23)
endfunction
Using this script, we get that when the American League has a p = 0.55 probability of winning
each game then their probability of winning the ﬁrst-to-win-four series is 0.60829.
When their
probability of winning any one game is p = 0.6 then their probability of winning the series is
0.71021.
(b) This Octave session
octave:1> v0=[1;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0];
octave:2> x=(.01:.01:.99)’;
octave:3> y=(.01:.01:.99)’;
octave:4> for i=.01:.01:.99
>
y(100*i)=markov(i,v0);
>
endfor
octave:5> z=[x, y];
octave:6> gplot z
yields this graph. By eye we judge that if p > 0.7 then the team is close to assurred of the series.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1
line 1
7
(a) They must satisfy this condition because the total probability of a state transition (including
back to the same state) is 100%.
(b) See the answer to the third item.
(c) We will do the 2×2 case; bigger-sized cases are just notational problems. This product
µ
a1,1
a1,2
a2,1
a2,2
¶ µ
b1,1
b1,2
b2,1
b2,2
¶
=
µ
a1,1b1,1 + a1,2b2,1
a1,1b1,2 + a1,2b2,2
a2,1b1,1 + a2,2b2,1
a2,1b1,2 + a2,2b2,2
¶
has these two column sums
(a1,1b1,1 +a1,2b2,1)+(a2,1b1,1 +a2,2b2,1) = (a1,1 +a2,1)·b1,1 +(a1,2 +a2,2)·b2,1 = 1·b1,1 +1·b2,1 = 1
and
(a1,1b1,2 +a1,2b2,2)+(a2,1b1,2 +a2,2b2,2) = (a1,1 +a2,1)·b1,2 +(a1,2 +a2,2)·b2,2 = 1·b1,2 +1·b2,2 = 1
as required.

Answers to Exercises
161
Topic: Orthonormal Matrices
1
(a) Yes.
(b) No, the columns do not have length one.
(c) Yes.
2
Some of these are nonlinear, because they involve a nontrivial translation.
(a)
µ
x
y
¶
7→
µ
x · cos(π/6) −y · sin(π/6)
x · sin(π/6) + y · cos(π/6)
¶
+
µ
0
1
¶
=
µ
x · (
√
3/2) −y · (1/2) + 0
x · (1/2) + y · cos(
√
3/2) + 1
¶
(b) The line y = 2x makes an angle of arctan(2/1) with the x-axis. Thus sin θ = 2/
√
5 and cos θ =
1/
√
5.
µ
x
y
¶
7→
µ
x · (1/
√
5) −y · (2/
√
5)
x · (2/
√
5) + y · (1/
√
5)
¶
(c)
µ
x
y
¶
7→
µ
x · (1/
√
5) −y · (−2/
√
5)
x · (−2/
√
5) + y · (1/
√
5)
¶
+
µ
1
1
¶
=
µ
x/
√
5 + 2y/
√
5 + 1
−2x/
√
5 + y/
√
5 + 1
¶
3
(a) Let f be distance-preserving and consider f −1. Any two points in the codomain can be written
as f(P1) and f(P2). Because f is distance-preserving, the distance from f(P1) to f(P2) equals the
distance from P1 to P2. But this is exactly what is required for f −1 to be distance-preserving.
(b) Any plane ﬁgure F is congruent to itself via the identity map id: R2 →R2, which is obviously
distance-preserving. If F1 is congruent to F2 (via some f) then F2 is congruent to F1 via f −1, which
is distance-preserving by the prior item. Finally, if F1 is congruent to F2 (via some f) and F2 is
congruent to F3 (via some g) then F1 is congruent to F3 via g ◦f, which is easily checked to be
distance-preserving.
4
The ﬁrst two components of each are ax + cy + e and bx + dy + f.
5
(a) The Pythagorean Theorem gives that three points are colinear if and only if (for some ordering of
them into P1, P2, and P3), dist(P1, P2) + dist(P2, P3) = dist(P1, P3). Of course, where f is distance-
preserving, this holds if and only if dist(f(P1), f(P2)) + dist(f(P2), f(P3)) = dist(f(P1), f(P3)),
which, again by Pythagoras, is true if and only if f(P1), f(P2), and f(P3) are colinear.
The argument for betweeness is similar (above, P2 is between P1 and P3).
If the ﬁgure F is a triangle then it is the union of three line segments P1P2, P2P3, and P1P3.
The prior two paragraphs together show that the property of being a line segment is invariant. So
f(F) is the union of three line segments, and so is a triangle.
A circle C centered at P and of radius r is the set of all points Q such that dist(P, Q) = r.
Applying the distance-preserving map f gives that the image f(C) is the set of all f(Q) subject to
the condition that dist(P, Q) = r. Since dist(P, Q) = dist(f(P), f(Q)), the set f(C) is also a circle,
with center f(P) and radius r.
(b) Here are two that are easy to verify: (i) the property of being a right triangle, and (ii) the
property of two lines being parallel.
(c) One that was mentioned in the section is the ‘sense’ of a ﬁgure. A triangle whose vertices read
clockwise as P1, P2, P3 may, under a distance-preserving map, be sent to a triangle read P1, P2, P3
counterclockwise.


Chapter Four: Determinants
Subsection Four.I.1: Exploration
Four.I.1.1
(a) 4
(b) 3
(c) −12
Four.I.1.2
(a) 6
(b) 21
(c) 27
Four.I.1.3
For the ﬁrst, apply the formula in this section, note that any term with a d, g, or h is zero,
and simplify. Lower-triangular matrices work the same way.
Four.I.1.4
(a) Nonsingular, the determinant is −1.
(b) Nonsingular, the determinant is −1.
(c) Singular, the determinant is 0.
Four.I.1.5
(a) Nonsingular, the determinant is 3.
(b) Singular, the determinant is 0.
(c) Singular, the determinant is 0.
Four.I.1.6
(a) det(B) = det(A) via −2ρ1 + ρ2
(b) det(B) = −det(A) via ρ2 ↔ρ3
(c) det(B) = (1/2) · det(A) via (1/2)ρ2
Four.I.1.7
Using the formula for the determinant of a 3×3 matrix we expand the left side
1 · b · c2 + 1 · c · a2 + 1 · a · b2 −b2 · c · 1 −c2 · a · 1 −a2 · b · 1
and by distributing we expand the right side.
(bc −ba −ac + a2) · (c −b) = c2b −b2c −bac + b2a −ac2 + acb + a2c −a2b
Now we can just check that the two are equal. (Remark. This is the 3×3 case of Vandermonde’s
determinant which arises in applications).
Four.I.1.8
This equation
0 = det(
µ
12 −x
4
8
8 −x
¶
) = 64 −20x + x2 = (x −16)(x −4)
has roots x = 16 and x = 4.
Four.I.1.9
We ﬁrst reduce the matrix to echelon form. To begin, assume that a ̸= 0 and that ae−bd ̸=
0.
(1/a)ρ1
−→


1
b/a
c/a
d
e
f
g
h
i


−dρ1+ρ2
−→
−gρ1+ρ3


1
b/a
c/a
0
(ae −bd)/a
(af −cd)/a
0
(ah −bg)/a
(ai −cg)/a


(a/(ae−bd))ρ2
−→


1
b/a
c/a
0
1
(af −cd)/(ae −bd)
0
(ah −bg)/a
(ai −cg)/a


This step ﬁnishes the calculation.
((ah−bg)/a)ρ2+ρ3
−→


1
b/a
c/a
0
1
(af −cd)/(ae −bd)
0
0
(aei + bgf + cdh −hfa −idb −gec)/(ae −bd)


Now assuming that a ̸= 0 and ae −bd ̸= 0, the original matrix is nonsingular if and only if the 3, 3
entry above is nonzero. That is, under the assumptions, the original matrix is nonsingular if and only
if aei + bgf + cdh −hfa −idb −gec ̸= 0, as required.
We ﬁnish by running down what happens if the assumptions that were taken for convienence in
the prior paragraph do not hold. First, if a ̸= 0 but ae −bd = 0 then we can swap


1
b/a
c/a
0
0
(af −cd)/a
0
(ah −bg)/a
(ai −cg)/a

ρ2↔ρ3
−→


1
b/a
c/a
0
(ah −bg)/a
(ai −cg)/a
0
0
(af −cd)/a



164
Linear Algebra, by Hefferon
and conclude that the matrix is nonsingular if and only if either ah −bg = 0 or af −cd = 0. The
condition ‘ah−bg = 0 or af −cd = 0’ is equivalent to the condition ‘(ah−bg)(af −cd) = 0’. Multiplying
out and using the case assumption that ae −bd = 0 to substitute ae for bd gives this.
0 = ahaf −ahcd −bgaf + bgcd = ahaf −ahcd −bgaf + aegc = a(haf −hcd −bgf + egc)
Since a ̸= 0, we have that the matrix is nonsingular if and only if haf −hcd−bgf +egc = 0. Therefore,
in this a ̸= 0 and ae−bd = 0 case, the matrix is nonsingular when haf −hcd−bgf +egc−i(ae−bd) = 0.
The remaining cases are routine. Do the a = 0 but d ̸= 0 case and the a = 0 and d = 0 but g ̸= 0
case by ﬁrst swapping rows and then going on as above. The a = 0, d = 0, and g = 0 case is easy —
that matrix is singular since the columns form a linearly dependent set, and the determinant comes
out to be zero.
Four.I.1.10
Figuring the determinant and doing some algebra gives this.
0 = y1x + x2y + x1y2 −y2x −x1y −x2y1
(x2 −x1) · y = (y2 −y1) · x + x2y1 −x1y2
y = y2 −y1
x2 −x1
· x + x2y1 −x1y2
x2 −x1
Note that this is the equation of a line (in particular, in contains the familiar expression for the slope),
and note that (x1, y1) and (x2, y2) satisfy it.
Four.I.1.11
(a) The comparison with the formula given in the preamble to this section is easy.
(b) While it holds for 2×2 matrices
µ
h1,1
h1,2
h1,1
h2,1
h2,2
h2,1
¶
= h1,1h2,2 + h1,2h2,1
−h2,1h1,2 −h2,2h1,1
= h1,1h2,2 −h1,2h2,1
it does not hold for 4×4 matrices. An example is that this matrix is singular because the second
and third rows are equal




1
0
0
1
0
1
1
0
0
1
1
0
−1
0
0
1




but following the scheme of the mnemonic does not give zero.




1
0
0
1
1
0
0
0
1
1
0
0
1
1
0
1
1
0
0
1
1
−1
0
0
1
−1
0
0



= 1 + 0 + 0 + 0
−(−1) −0 −0 −0
Four.I.1.12
The determinant is (x2y3 −x3y2)⃗e1 + (x3y1 −x1y3)⃗e2 + (x1y2 −x2y1)⃗e3. To check per-
pendicularity, we check that the dot product with the ﬁrst vector is zero


x1
x2
x3




x2y3 −x3y2
x3y1 −x1y3
x1y2 −x2y1

= x1x2y3 −x1x3y2 + x2x3y1 −x1x2y3 + x1x3y2 −x2x3y1 = 0
and the dot product with the second vector is also zero.


y1
y2
y3




x2y3 −x3y2
x3y1 −x1y3
x1y2 −x2y1

= x2y1y3 −x3y1y2 + x3y1y2 −x1y2y3 + x1y2y3 −x2y1y3 = 0
Four.I.1.13
(a) Plug and chug: the determinant of the product is this
det(
µ
a
b
c
d
¶ µ
w
x
y
z
¶
) = det(
µ
aw + by
ax + bz
cw + dy
cx + dz
¶
)
= acwx + adwz + bcxy + bdyz
−acwx −bcwz −adxy −bdyz
while the product of the determinants is this.
det(
µ
a
b
c
d
¶
) · det(
µ
w
x
y
z
¶
) = (ad −bc) · (wz −xy)
Veriﬁcation that they are equal is easy.

Answers to Exercises
165
(b) Use the prior item.
That similar matrices have the same determinant is immediate from the above two: det(PTP −1) =
det(P) · det(T) · det(P −1).
Four.I.1.14
One way is to count these areas
y1
y2
x2
x1
A
B
C
D
E
F
by taking the area of the entire rectangle and subtracting the area of A the upper-left rectangle, B
the upper-middle triangle, D the upper-right triangle, C the lower-left triangle, E the lower-middle
triangle, and F the lower-right rectangle (x1+x2)(y1+y2)−x2y1−(1/2)x1y1−(1/2)x2y2−(1/2)x2y2−
(1/2)x1y1 −x2y1. Simpliﬁcation gives the determinant formula.
This determinant is the negative of the one above; the formula distinguishes whether the second
column is counterclockwise from the ﬁrst.
Four.I.1.15
The computation for 2×2 matrices, using the formula quoted in the preamble, is easy. It
does also hold for 3×3 matrices; the computation is routine.
Four.I.1.16
No. Recall that constants come out one row at a time.
det(
µ
2
4
2
6
¶
) = 2 · det(
µ
1
2
2
6
¶
) = 2 · 2 · det(
µ
1
2
1
3
¶
)
This contradicts linearity (here we didn’t need S, i.e., we can take S to be the zero matrix).
Four.I.1.17
Bring out the c’s one row at a time.
Four.I.1.18
There are no real numbers θ that make the matrix singular because the determinant of the
matrix cos2 θ +sin2 θ is never 0, it equals 1 for all θ. Geometrically, with respect to the standard basis,
this matrix represents a rotation of the plane through an angle of θ. Each such map is one-to-one —
for one thing, it is invertible.
Four.I.1.19
This is how the answer was given in the cited source. Let P be the sum of the three
positive terms of the determinant and −N the sum of the three negative terms. The maximum value
of P is
9 · 8 · 7 + 6 · 5 · 4 + 3 · 2 · 1 = 630.
The minimum value of N consistent with P is
9 · 6 · 1 + 8 · 5 · 2 + 7 · 4 · 3 = 218.
Any change in P would result in lowering that sum by more than 4. Therefore 412 the maximum value
for the determinant and one form for the determinant is
¯¯¯¯¯¯
9
4
2
3
8
6
5
1
7
¯¯¯¯¯¯
.
Subsection Four.I.2: Properties of Determinants
Four.I.2.7
(a)
¯¯¯¯¯¯
3
1
2
3
1
0
0
1
4
¯¯¯¯¯¯
=
¯¯¯¯¯¯
3
1
2
0
0
−2
0
1
4
¯¯¯¯¯¯
= −
¯¯¯¯¯¯
3
1
2
0
1
4
0
0
−2
¯¯¯¯¯¯
= 6
(b)
¯¯¯¯¯¯¯¯
1
0
0
1
2
1
1
0
−1
0
1
0
1
1
1
0
¯¯¯¯¯¯¯¯
=
¯¯¯¯¯¯¯¯
1
0
0
1
0
1
1
−2
0
0
1
1
0
1
1
−1
¯¯¯¯¯¯¯¯
=
¯¯¯¯¯¯¯¯
1
0
0
1
0
1
1
−2
0
0
1
1
0
0
0
1
¯¯¯¯¯¯¯¯
= 1

166
Linear Algebra, by Hefferon
Four.I.2.8
(a)
¯¯¯¯
2
−1
−1
−1
¯¯¯¯ =
¯¯¯¯
2
−1
0
−3/2
¯¯¯¯ = −3;
(b)
¯¯¯¯¯¯
1
1
0
3
0
2
5
2
2
¯¯¯¯¯¯
=
¯¯¯¯¯¯
1
1
0
0
−3
2
0
−3
2
¯¯¯¯¯¯
=
¯¯¯¯¯¯
1
1
0
0
−3
2
0
0
0
¯¯¯¯¯¯
= 0
Four.I.2.9
When is the determinant not zero?
¯¯¯¯¯¯¯¯
1
0
1
−1
0
1
−2
0
1
0
k
0
0
0
1
−1
¯¯¯¯¯¯¯¯
=
¯¯¯¯¯¯¯¯
1
0
1
−1
0
1
−2
0
0
0
k −1
1
0
0
1
−1
¯¯¯¯¯¯¯¯
Obviously, k = 1 gives nonsingularity and hence a nonzero determinant. If k ̸= 1 then we get echelon
form with a (−1/k −1)ρ3 + ρ4 pivot.
=
¯¯¯¯¯¯¯¯
1
0
1
−1
0
1
−2
0
0
0
k −1
1
0
0
0
−1 −(1/k −1)
¯¯¯¯¯¯¯¯
Multiplying down the diagonal gives (k −1)(−1 −(1/k −1)) = −(k −1) −1 = −k. Thus the matrix
has a nonzero determinant, and so the system has a unique solution, if and only if k ̸= 0.
Four.I.2.10
(a) Property (2) of the deﬁnition of determinants applies via the swap ρ1 ↔ρ3.
¯¯¯¯¯¯
h3,1
h3,2
h3,3
h2,1
h2,2
h2,3
h1,1
h1,2
h1,3
¯¯¯¯¯¯
= −
¯¯¯¯¯¯
h1,1
h1,2
h1,3
h2,1
h2,2
h2,3
h3,1
h3,2
h3,3
¯¯¯¯¯¯
(b) Property (3) applies.
¯¯¯¯¯¯
−h1,1
−h1,2
−h1,3
−2h2,1
−2h2,2
−2h2,3
−3h3,1
−3h3,2
−3h3,3
¯¯¯¯¯¯
= (−1) · (−2) · (−3) ·
¯¯¯¯¯¯
h1,1
h1,2
h1,3
h2,1
h2,2
h2,3
h3,1
h3,2
h3,3
¯¯¯¯¯¯
= (−6) ·
¯¯¯¯¯¯
h1,1
h1,2
h1,3
h2,1
h2,2
h2,3
h3,1
h3,2
h3,3
¯¯¯¯¯¯
(c)
¯¯¯¯¯¯
h1,1 + h3,1
h1,2 + h3,2
h1,3 + h3,3
h2,1
h2,2
h2,3
5h3,1
5h3,2
5h3,3
¯¯¯¯¯¯
= 5 ·
¯¯¯¯¯¯
h1,1 + h3,1
h1,2 + h3,2
h1,3 + h3,3
h2,1
h2,2
h2,3
h3,1
h3,2
h3,3
¯¯¯¯¯¯
= 5 ·
¯¯¯¯¯¯
h1,1
h1,2
h1,3
h2,1
h2,2
h2,3
h3,1
h3,2
h3,3
¯¯¯¯¯¯
Four.I.2.11
A diagonal matrix is in echelon form, so the determinant is the product down the diagonal.
Four.I.2.12
It is the trivial subspace.
Four.I.2.13
Pivoting by adding the second row to the ﬁrst gives a matrix whose ﬁrst row is x + y + z
times its third row.
Four.I.2.14
(a)
¡
1
¢
,
µ
1
−1
−1
1
¶
,


1
−1
1
−1
1
−1
1
−1
1


(b) The determinant in the 1×1 case is 1. In every other case the second row is the negative of the
ﬁrst, and so matrix is singular and the determinant is zero.
Four.I.2.15
(a)
¡
2
¢
,
µ
2
3
3
4
¶
,


2
3
4
3
4
5
4
5
6


(b) The 1×1 and 2×2 cases yield these.
¯¯2
¯¯ = 2
¯¯¯¯
2
3
3
4
¯¯¯¯ = −1
And n×n matrices with n ≥3 are singular, e.g.,
¯¯¯¯¯¯
2
3
4
3
4
5
4
5
6
¯¯¯¯¯¯
= 0
because twice the second row minus the ﬁrst row equals the third row. Checking this is routine.

Answers to Exercises
167
Four.I.2.16
This one
A = B =
µ
1
2
3
4
¶
is easy to check.
|A + B| =
¯¯¯¯
2
4
6
8
¯¯¯¯ = −8
|A| + |B| = −2 −2 = −4
By the way, this also gives an example where scalar multiplication is not preserved |2 · A| ̸= 2 · |A|.
Four.I.2.17
No, we cannot replace it. Remark 2.2 shows that the four conditions after the replacement
would conﬂict — no function satisﬁes all four.
Four.I.2.18
A upper-triangular matrix is in echelon form.
A lower-triangular matrix is either singular or nonsingular. If it is singular then it has a zero on
its diagonal and so its determinant (namely, zero) is indeed the product down its diagonal. If it is
nonsingular then it has no zeroes on its diagonal, and can be reduced by Gauss’ method to echelon
form without changing the diagonal.
Four.I.2.19
(a) The properties in the deﬁnition of determinant show that |Mi(k)| = k, |Pi,j| = −1,
and |Ci,j(k)| = 1.
(b) The three cases are easy to check by recalling the action of left multiplication by each type of
matrix.
(c) If TS is invertible (TS)M = I then the associative property of matrix multiplication T(SM) = I
shows that T is invertible. So if T is not invertible then neither is TS.
(d) If T is singular then apply the prior answer: |TS| = 0 and |T|·|S| = 0·|S| = 0. If T is not singular
then it can be written as a product of elementary matrices |TS| = |Er · · · E1S| = |Er| · · · |E1| · |S| =
|Er · · · E1||S| = |T||S|.
(e) 1 = |I| = |T · T −1| = |T||T −1|
Four.I.2.20
(a) We must show that if
T
kρi+ρj
−→
ˆT
then d(T) = |TS|/|S| = | ˆTS|/|S| = d( ˆT).
We will be done if we show that pivoting ﬁrst and
then multiplying to get ˆTS gives the same result as multiplying ﬁrst to get TS and then pivoting
(because the determinant |TS| is unaﬀected by the pivot so we’ll then have | ˆTS| = |TS|, and hence
d( ˆT) = d(T)). That argument runs: after adding k times row i of TS to row j of TS, the j, p entry
is (kti,1 + tj,1)s1,p + · · · + (kti,r + tj,r)sr,p, which is the j, p entry of ˆTS.
(b) We need only show that swapping T
ρi↔ρj
−→ˆT and then multiplying to get ˆTS gives the same result
as multiplying T by S and then swapping (because, as the determinant |TS| changes sign on the
row swap, we’ll then have | ˆTS| = −|TS|, and so d( ˆT) = −d(T)). That argument runs just like the
prior one.
(c) Not surprisingly by now, we need only show that multiplying a row by a nonzero scalar T
kρi
−→ˆT
and then computing ˆTS gives the same result as ﬁrst computing TS and then multiplying the row
by k (as the determinant |TS| is rescaled by k the multiplication, we’ll have | ˆTS| = k|TS|, so
d( ˆT) = k d(T)). The argument runs just as above.
(d) Clear.
(e) Because we’ve shown that d(T) is a determinant and that determinant functions (if they exist)
are unique, we have that so |T| = d(T) = |TS|/|S|.
Four.I.2.21
We will ﬁrst argue that a rank r matrix has a r×r submatrix with nonzero determinant.
A rank r matrix has a linearly independent set of r rows. A matrix made from those rows will have
row rank r and thus has column rank r. Conclusion: from those r rows can be extracted a linearly
independent set of r columns, and so the original matrix has a r×r submatrix of rank r.
We ﬁnish by showing that if r is the largest such integer then the rank of the matrix is r. We need
only show, by the maximality of r, that if a matrix has a k×k submatrix of nonzero determinant then
the rank of the matrix is at least k. Consider such a k×k submatrix. Its rows are parts of the rows
of the original matrix, clearly the set of whole rows is linearly independent. Thus the row rank of the
original matrix is at least k, and the row rank of a matrix equals its rank.
Four.I.2.22
A matrix with only rational entries can be reduced with Gauss’ method to an echelon
form matrix using only rational arithmetic. Thus the entries on the diagonal must be rationals, and
so the product down the diagonal is rational.

168
Linear Algebra, by Hefferon
Four.I.2.23
This is how the answer was given in the cited source. The value (1−a4)3 of the determinant
is independent of the values B, C, D. Hence operation (e) does not change the value of the determinant
but merely changes its appearance. Thus the element of likeness in (a), (b), (c), (d), and (e) is only
that the appearance of the principle entity is changed. The same element appears in (f) changing the
name-label of a rose, (g) writing a decimal integer in the scale of 12, (h) gilding the lily, (i) whitewashing
a politician, and (j) granting an honorary degree.
Subsection Four.I.3: The Permutation Expansion
Four.I.3.14
(a) This matrix is singular.
¯¯¯¯¯¯
1
2
3
4
5
6
7
8
9
¯¯¯¯¯¯
= (1)(5)(9) |Pφ1| + (1)(6)(8) |Pφ2| + (2)(4)(9) |Pφ3|
+ (2)(6)(7) |Pφ4| + (3)(4)(8) |Pφ5| + (7)(5)(3) |Pφ6|
= 0
(b) This matrix is nonsingular.
¯¯¯¯¯¯
2
2
1
3
−1
0
−2
0
5
¯¯¯¯¯¯
= (2)(−1)(5) |Pφ1| + (2)(0)(0) |Pφ2| + (2)(3)(5) |Pφ3|
+ (2)(0)(−2) |Pφ4| + (1)(3)(0) |Pφ5| + (−2)(−1)(1) |Pφ6|
= −42
Four.I.3.15
(a) Gauss’ method gives this
¯¯¯¯
2
1
3
1
¯¯¯¯ =
¯¯¯¯
2
1
0
−1/2
¯¯¯¯ = −1
and permutation expansion gives this.
¯¯¯¯
2
1
3
1
¯¯¯¯ =
¯¯¯¯
2
0
0
1
¯¯¯¯ +
¯¯¯¯
0
1
3
0
¯¯¯¯ = (2)(1)
¯¯¯¯
1
0
0
1
¯¯¯¯ + (1)(3)
¯¯¯¯
0
1
1
0
¯¯¯¯ = −1
(b) Gauss’ method gives this
¯¯¯¯¯¯
0
1
4
0
2
3
1
5
1
¯¯¯¯¯¯
= −
¯¯¯¯¯¯
1
5
1
0
2
3
0
1
4
¯¯¯¯¯¯
= −
¯¯¯¯¯¯
1
5
1
0
2
3
0
0
5/2
¯¯¯¯¯¯
= −5
and the permutation expansion gives this.
¯¯¯¯¯¯
0
1
4
0
2
3
1
5
1
¯¯¯¯¯¯
= (0)(2)(1) |Pφ1| + (0)(3)(5) |Pφ2| + (1)(0)(1) |Pφ3|
+ (1)(3)(1) |Pφ4| + (4)(0)(5) |Pφ5| + (1)(2)(0) |Pφ6|
= −5
Four.I.3.16
Following Example 3.6 gives this.
¯¯¯¯¯¯
t1,1
t1,2
t1,3
t2,1
t2,2
t2,3
t3,1
t3,2
t3,3
¯¯¯¯¯¯
= t1,1t2,2t3,3 |Pφ1| + t1,1t2,3t3,2 |Pφ2|
+ t1,2t2,1t3,3 |Pφ3| + t1,2t2,3t3,1 |Pφ4|
+ t1,3t2,1t3,2 |Pφ5| + t1,3t2,2t3,1 |Pφ6|
= t1,1t2,2t3,3(+1) + t1,1t2,3t3,2(−1)
+ t1,2t2,1t3,3(−1) + t1,2t2,3t3,1(+1)
+ t1,3t2,1t3,2(+1) + t1,3t2,2t3,1(−1)
Four.I.3.17
This is all of the permutations where φ(1) = 1
φ1 = ⟨1, 2, 3, 4⟩
φ2 = ⟨1, 2, 4, 3⟩
φ3 = ⟨1, 3, 2, 4⟩
φ4 = ⟨1, 3, 4, 2⟩
φ5 = ⟨1, 4, 2, 3⟩
φ6 = ⟨1, 4, 3, 2⟩
the ones where φ(1) = 1
φ7 = ⟨2, 1, 3, 4⟩
φ8 = ⟨2, 1, 4, 3⟩
φ9 = ⟨2, 3, 1, 4⟩
φ10 = ⟨2, 3, 4, 1⟩
φ11 = ⟨2, 4, 1, 3⟩
φ12 = ⟨2, 4, 3, 1⟩

Answers to Exercises
169
the ones where φ(1) = 3
φ13 = ⟨3, 1, 2, 4⟩
φ14 = ⟨3, 1, 4, 2⟩
φ15 = ⟨3, 2, 1, 4⟩
φ16 = ⟨3, 2, 4, 1⟩
φ17 = ⟨3, 4, 1, 2⟩
φ18 = ⟨3, 4, 2, 1⟩
and the ones where φ(1) = 4.
φ19 = ⟨4, 1, 2, 3⟩
φ20 = ⟨4, 1, 3, 2⟩
φ21 = ⟨4, 2, 1, 3⟩
φ22 = ⟨4, 2, 3, 1⟩
φ23 = ⟨4, 3, 1, 2⟩
φ24 = ⟨4, 3, 2, 1⟩
Four.I.3.18
Each of these is easy to check.
(a)
permutation
φ1
φ2
inverse
φ1
φ2
(b)
permutation
φ1
φ2
φ3
φ4
φ5
φ6
inverse
φ1
φ2
φ3
φ5
φ4
φ6
Four.I.3.19
For the ‘if’ half, the ﬁrst condition of Deﬁnition 3.2 follows from taking k1 = k2 = 1 and
the second condition follows from taking k2 = 0.
The ‘only if’ half also routine. From f(⃗ρ1, . . . , k1⃗v1 + k2⃗v2, . . . , ⃗ρn) the ﬁrst condition of Deﬁni-
tion 3.2 gives = f(⃗ρ1, . . . , k1⃗v1, . . . , ⃗ρn) + f(⃗ρ1, . . . , k2⃗v2, . . . , ⃗ρn) and the second condition, applied
twice, gives the result.
Four.I.3.20
To get a nonzero term in the permutation expansion we must use the 1, 2 entry and the
4, 3 entry. Having ﬁxed on those two we must also use the 2, 1 entry and the the 3, 4 entry. The signum
of ⟨2, 1, 4, 3⟩is +1 because from




0
1
0
0
1
0
0
0
0
0
0
1
0
0
1
0




the two rwo swaps ρ1 ↔ρ2 and ρ3 ↔ρ4 will produce the identity matrix.
Four.I.3.21
They would all double.
Four.I.3.22
For the second statement, given a matrix, transpose it, swap rows, and transpose back.
The result is swapped columns, and the determinant changes by a factor of −1. The third statement
is similar: given a matrix, transpose it, apply multilinearity to what are now rows, and then transpose
back the resulting matrices.
Four.I.3.23
An n×n matrix with a nonzero determinant has rank n so its columns form a basis for
Rn.
Four.I.3.24
False.
¯¯¯¯
1
−1
1
1
¯¯¯¯ = 2
Four.I.3.25
(a) For the column index of the entry in the ﬁrst row there are ﬁve choices. Then, for
the column index of the entry in the second row there are four choices (the column index used in the
ﬁrst row cannot be used here). Continuing, we get 5 · 4 · 3 · 2 · 1 = 120. (See also the next question.)
(b) Once we choose the second column in the ﬁrst row, we can choose the other entries in 4·3·2·1 = 24
ways.
Four.I.3.26
n · (n −1) · · · 2 · 1 = n!
Four.I.3.27
In |A| = |Atrans| = | −A| = (−1)n|A| the exponent n must be even.
Four.I.3.28
Showing that no placement of three zeros suﬃces is routine. Four zeroes does suﬃce; put
them all in the same row or column.
Four.I.3.29
The n = 3 case shows what to do. The pivot operations of −x1ρ2 + ρ3 and −x1ρ1 + ρ2
give this.
¯¯¯¯¯¯
1
1
1
x1
x2
x3
x2
1
x2
2
x2
3
¯¯¯¯¯¯
=
¯¯¯¯¯¯
1
1
1
x1
x2
x3
0
(−x1 + x2)x2
(−x1 + x3)x3
¯¯¯¯¯¯
=
¯¯¯¯¯¯
1
1
1
0
−x1 + x2
−x1 + x3
0
(−x1 + x2)x2
(−x1 + x3)x3
¯¯¯¯¯¯
Then the pivot operation of x2ρ2 + ρ3 gives the desired result.
=
¯¯¯¯¯¯
1
1
1
0
−x1 + x2
−x1 + x3
0
0
(−x1 + x3)(−x2 + x3)
¯¯¯¯¯¯
= (x2 −x1)(x3 −x1)(x3 −x2)

170
Linear Algebra, by Hefferon
Four.I.3.30
Let T be n×n, let J be p×p, and let K be q×q. Apply the permutation expansion formula
|T| =
X
permutations φ
t1,φ(1)t2,φ(2) . . . tn,φ(n) |Pφ|
Because the upper right of T is all zeroes, if a φ has at least one of p+1, . . . , n among its ﬁrst p column
numbers φ(1), . . . , φ(p) then the term arising from φ is 0 (e.g., if φ(1) = n then t1,φ(1)t2,φ(2) . . . tn,φ(n)
is 0). So the above formula reduces to a sum over all permutations with two halves: ﬁrst 1, . . . , p are
rearranged, and after that comes a permutation of p+1, . . . , p+q. To see this gives |J|·|K|, distribute.
·
X
perms φ1
of 1,...,p
t1,φ1(1) · · · tp,φ1(p) |Pφ1|
¸
·
·
X
perms φ2
of p+1,...,p+q
tp+1,φ2(p+1) · · · tp+q,φ2(p+q) |Pφ2|
¸
Four.I.3.31
The n = 3 case shows what happens.
|T −rI| =
¯¯¯¯¯¯
t1,1 −x
t1,2
t1,3
t2,1
t2,2 −x
t2,3
t3,1
t3,2
t3,3 −x
¯¯¯¯¯¯
Each term in the permutation expansion has three factors drawn from entries in the matrix (e.g.,
(t1,1 −x)(t2,2 −x)(t3,3 −x) and (t1,1 −x)(t2,3)(t3,2)), and so the determinant is expressible as a
polynomial in x of degree 3. Such a polynomial has at most 3 roots.
In general, the permutation expansion shows that the determinant can be written as a sum of
terms, each with n factors, giving a polynomial of degree n. A polynomial of degree n has at most n
roots.
Four.I.3.32
This is how the answer was given in the cited source. When two rows of a determinant are
interchanged, the sign of the determinant is changed. When the rows of a three-by-three determinant
are permuted, 3 positive and 3 negative determinants equal in absolute value are obtained. Hence the
9! determinants fall into 9!/6 groups, each of which sums to zero.
Four.I.3.33
This is how the answer was given in the cited source. When the elements of any column
are subtracted from the elements of each of the other two, the elements in two of the columns of the
derived determinant are proportional, so the determinant vanishes. That is,
¯¯¯¯¯¯
2
1
x −4
4
2
x −3
6
3
x −10
¯¯¯¯¯¯
=
¯¯¯¯¯¯
1
x −3
−1
2
x −1
−2
3
x −7
−3
¯¯¯¯¯¯
=
¯¯¯¯¯¯
x −2
−1
−2
x + 1
−2
−4
x −4
−3
−6
¯¯¯¯¯¯
= 0.
Four.I.3.34
This is how the answer was given in the cited source. Let
a
b
c
d
e
f
g
h
i
have magic sum N = S/3. Then
N = (a + e + i) + (d + e + f) + (g + e + c)
−(a + d + g) −(c + f + i) = 3e
and S = 9e. Hence, adding rows and columns,
D =
¯¯¯¯¯¯
a
b
c
d
e
f
g
h
i
¯¯¯¯¯¯
=
¯¯¯¯¯¯
a
b
c
d
e
f
3e
3e
3e
¯¯¯¯¯¯
=
¯¯¯¯¯¯
a
b
3e
d
e
3e
3e
3e
9e
¯¯¯¯¯¯
=
¯¯¯¯¯¯
a
b
e
d
e
e
1
1
1
¯¯¯¯¯¯
S.
Four.I.3.35
This is how the answer was given in the cited source. Denote by Dn the determinant in
question and by ai,j the element in the i-th row and j-th column. Then from the law of formation of
the elements we have
ai,j = ai,j−1 + ai−1,j,
a1,j = ai,1 = 1.
Subtract each row of Dn from the row following it, beginning the process with the last pair of rows.
After the n −1 subtractions the above equality shows that the element ai,j is replaced by the element
ai,j−1, and all the elements in the ﬁrst column, except a1,1 = 1, become zeroes. Now subtract each
column from the one following it, beginning with the last pair. After this process the element ai,j−1
is replaced by ai−1,j−1, as shown in the above relation. The result of the two operations is to replace
ai,j by ai−1,j−1, and to reduce each element in the ﬁrst row and in the ﬁrst column to zero. Hence
Dn = Dn+i and consequently
Dn = Dn−1 = Dn−2 = · · · = D2 = 1.

Answers to Exercises
171
Subsection Four.I.4: Determinants Exist
Four.I.4.10
This is the permutation expansion of the determinant of a 2×2 matrix
¯¯¯¯
a
b
c
d
¯¯¯¯ = ad ·
¯¯¯¯
1
0
0
1
¯¯¯¯ + bc ·
¯¯¯¯
0
1
1
0
¯¯¯¯
and the permutation expansion of the determinant of its transpose.
¯¯¯¯
a
c
b
d
¯¯¯¯ = ad ·
¯¯¯¯
1
0
0
1
¯¯¯¯ + cb ·
¯¯¯¯
0
1
1
0
¯¯¯¯
As with the 3×3 expansions described in the subsection, the permutation matrices from corresponding
terms are transposes (although this is disguised by the fact that each is self-transpose).
Four.I.4.11
Each of these is easy to check.
(a)
permutation
φ1
φ2
inverse
φ1
φ2
(b)
permutation
φ1
φ2
φ3
φ4
φ5
φ6
inverse
φ1
φ2
φ3
φ5
φ4
φ6
Four.I.4.12
(a) sgn(φ1) = +1, sgn(φ2) = −1
(b) sgn(φ1) = +1, sgn(φ2) = −1, sgn(φ3) = −1, sgn(φ4) = +1, sgn(φ5) = +1, sgn(φ6) = −1
Four.I.4.13
The pattern is this.
i
1
2
3
4
5
6
. . .
sgn(φi)
+1
−1
−1
+1
+1
−1
. . .
So to ﬁnd the signum of φn!, we subtract one n! −1 and look at the remainder on division by four. If
the remainder is 1 or 2 then the signum is −1, otherwise it is +1. For n > 4, the number n! is divisible
by four, so n! −1 leaves a remainder of −1 on division by four (more properly said, a remainder or 3),
and so the signum is +1. The n = 1 case has a signum of +1, the n = 2 case has a signum of −1 and
the n = 3 case has a signum of −1.
Four.I.4.14
(a) Permutations can be viewed as one-one and onto maps φ: {1, . . . , n} →{1, . . . , n}.
Any one-one and onto map has an inverse.
(b) If it always takes an odd number of swaps to get from Pφ to the identity, then it always takes
an odd number of swaps to get from the identity to Pφ (any swap is reversible).
(c) This is the ﬁrst question again.
Four.I.4.15
If φ(i) = j then φ−1(j) = i. The result now follows on the observation that Pφ has a 1 in
entry i, j if and only if φ(i) = j, and Pφ−1 has a 1 in entry j, i if and only if φ−1(j) = i,
Four.I.4.16
This does not say that m is the least number of swaps to produce an identity, nor does it
say that m is the most. It instead says that there is a way to swap to the identity in exactly m steps.
Let ιj be the ﬁrst row that is inverted with respect to a prior row and let ιk be the ﬁrst row giving
that inversion. We have this interval of rows.













...
ιk
ιr1
...
ιrs
ιj
...













j < k < r1 < · · · < rs
Swap.













...
ιj
ιr1
...
ιrs
ιk
...














172
Linear Algebra, by Hefferon
The second matrix has one fewer inversion because there is one fewer inversion in the interval (s vs.
s + 1) and inversions involving rows outside the interval are not aﬀected.
Proceed in this way, at each step reducing the number of inversions by one with each row swap.
When no inversions remain the result is the identity.
The contrast with Corollary 4.6 is that the statement of this exercise is a ‘there exists’ statement:
there exists a way to swap to the identity in exactly m steps. But the corollary is a ‘for all’ statement:
for all ways to swap to the identity, the parity (evenness or oddness) is the same.
Four.I.4.17
(a) First, g(φ1) is the product of the single factor 2 −1 and so g(φ1) = 1. Second, g(φ2)
is the product of the single factor 1 −2 and so g(φ2) = −1.
(b)
permutation φ
φ1
φ2
φ3
φ4
φ5
φ6
g(φ)
2
−2
−2
2
2
−2
(c) Note that φ(j) −φ(i) is negative if and only if ιφ(j) and ιφ(i) are in an inversion of their usual
order.
Subsection Four.II.1: Determinants as Size Functions
Four.II.1.8
For each, ﬁnd the determinant and take the absolute value.
(a) 7
(b) 0
(c) 58
Four.II.1.9
Solving
c1


3
3
1

+ c2


2
6
1

+ c3


1
0
5

=


4
1
2


gives the unique solution c3 = 11/57, c2 = −40/57 and c1 = 99/57. Because c1 > 1, the vector is not
in the box.
Four.II.1.10
Move the parallelepiped to start at the origin, so that it becomes the box formed by
⟨
µ
3
0
¶
,
µ
2
1
¶
⟩
and now the absolute value of this determinant is easily computed as 3.
¯¯¯¯
3
2
0
1
¯¯¯¯ = 3
Four.II.1.11
(a) 3
(b) 9
(c) 1/9
Four.II.1.12
Express each transformation with respect to the standard bases and ﬁnd the determi-
nant.
(a) 6
(b) −1
(c) −5
Four.II.1.13
The starting area is 6 and the matrix changes sizes by −14. Thus the area of the image
is 84.
Four.II.1.14
By a factor of 21/2.
Four.II.1.15
For a box we take a sequence of vectors (as described in the remark, the order in which
the vectors are taken matters), while for a span we take a set of vectors. Also, for a box subset of Rn
there must be n vectors; of course for a span there can be any number of vectors. Finally, for a box
the coeﬃcients t1, . . . , tn are restricted to the interval [0..1], while for a span the coeﬃcients are free
to range over all of R.
Four.II.1.16
That picture is drawn to mislead. The picture on the left is not the box formed by two
vectors. If we slide it to the origin then it becomes the box formed by this sequence.
⟨
µ
0
1
¶
,
µ
2
0
¶
⟩
Then the image under the action of the matrix is the box formed by this sequence.
⟨
µ
1
1
¶
,
µ
4
0
¶
⟩
which has an area of 4.
Four.II.1.17
Yes to both. For instance, the ﬁrst is |TS| = |T| · |S| = |S| · |T| = |ST|.

Answers to Exercises
173
Four.II.1.18
(a) If it is deﬁned then it is (32) · (2) · (2−2) · (3).
(b) |6A3 + 5A2 + 2A| = |A| · |6A2 + 5A + 2I|.
Four.II.1.19
¯¯¯¯
cos θ
−sin θ
sin θ
cos θ
¯¯¯¯ = 1
Four.II.1.20
No, for instance the determinant of
T =
µ
2
0
0
1/2
¶
is 1 so it preserves areas, but the vector T⃗e1 has length 2.
Four.II.1.21
It is zero.
Four.II.1.22
Two of the three sides of the triangle are formed by these vectors.


2
2
2

−


1
2
1

=


1
0
1




3
−1
4

−


1
2
1

=


2
−3
3


One way to ﬁnd the area of this triangle is to produce a length-one vector orthogonal to these two.
From these two relations


1
0
1

·


x
y
z

=


0
0
0




2
−3
3

·


x
y
z

=


0
0
0


we get a system
x
+ z = 0
2x −3y + 3z = 0
−2ρ1+ρ2
−→
x
+ z = 0
−3y + z = 0
with this solution set.
{


−1
1/3
1

z
¯¯ z ∈R},
A solution of length one is this.
1
p
19/9


−1
1/3
1


Thus the area of the triangle is the absolute value of this determinant.
¯¯¯¯¯¯
1
2
−3/
√
19
0
−3
1/
√
19
1
3
3/
√
19
¯¯¯¯¯¯
= −12/
√
19
Four.II.1.23
(a) Because the image of a linearly dependent set is linearly dependent, if the vectors
forming S make a linearly dependent set, so that |S| = 0, then the vectors forming t(S) make a
linearly dependent set, so that |TS| = 0, and in this case the equation holds.
(b) We must check that if T
kρi+ρj
−→ˆT then d(T) = |TS|/|S| = | ˆTS|/|S| = d( ˆT). We can do this by
checking that pivoting ﬁrst and then multiplying to get ˆTS gives the same result as multiplying ﬁrst
to get TS and then pivoting (because the determinant |TS| is unaﬀected by the pivot so we’ll then
have that | ˆTS| = |TS| and hence that d( ˆT) = d(T)). This check runs: after adding k times row i of
TS to row j of TS, the j, p entry is (kti,1 + tj,1)s1,p + · · · + (kti,r + tj,r)sr,p, which is the j, p entry
of ˆTS.
(c) For the second property, we need only check that swapping T
ρi↔ρj
−→ˆT and then multiplying to get
ˆTS gives the same result as multiplying T by S ﬁrst and then swapping (because, as the determinant
|TS| changes sign on the row swap, we’ll then have | ˆTS| = −|TS|, and so d( ˆT) = −d(T)). This
ckeck runs just like the one for the ﬁrst property.
For the third property, we need only show that performing T
kρi
−→ˆT and then computing ˆTS
gives the same result as ﬁrst computing TS and then performing the scalar multiplication (as the
determinant |TS| is rescaled by k, we’ll have | ˆTS| = k|TS| and so d( ˆT) = k d(T)). Here too, the
argument runs just as above.
The fourth property, that if T is I then the result is 1, is obvious.
(d) Determinant functions are unique, so |TS|/|S| = d(T) = |T|, and so |TS| = |T||S|.

174
Linear Algebra, by Hefferon
Four.II.1.24
Any permutation matrix has the property that the transpose of the matrix is its inverse.
For the implication, we know that |Atrans| = |A|. Then 1 = |A·A−1| = |A·Atrans| = |A|·|Atrans| =
|A|2.
The converse does not hold; here is an example.
µ3
1
2
1
¶
Four.II.1.25
Where the sides of the box are c times longer, the box has c3 times as many cubic units
of volume.
Four.II.1.26
If H = P −1GP then |H| = |P −1||G||P| = |P −1||P||G| = |P −1P||G| = |G|.
Four.II.1.27
(a) The new basis is the old basis rotated by π/4.
(b) ⟨
µ
−1
0
¶
,
µ
0
−1
¶
⟩, ⟨
µ
0
−1
¶
,
µ
1
0
¶
⟩
(c) In each case the determinant is +1 (these bases are said to have positive orientation).
(d) Because only one sign can change at a time, the only other cycle possible is
· · · −→
µ
+
+
¶
−→
µ
+
−
¶
−→
µ
−
−
¶
−→
µ
−
+
¶
−→· · · .
Here each associated determinant is −1 (such bases are said to have a negative orientation).
(e) There is one positively oriented basis ⟨(1)⟩and one negatively oriented basis ⟨(−1)⟩.
(f) There are 48 bases (6 half-axis choices are possible for the ﬁrst unit vector, 4 for the second, and
2 for the last). Half are positively oriented like the standard basis on the left below, and half are
negatively oriented like the one on the right
⃗e1
⃗e2
⃗e3
⃗
β1
⃗
β2
⃗
β3
In R3 positive orientation is sometimes called ‘right hand orientation’ because if a person’s right
hand is placed with the ﬁngers curling from ⃗e1 to ⃗e2 then the thumb will point with ⃗e3.
Four.II.1.28
We will compare det(⃗s1, . . . ,⃗sn) with det(t(⃗s1), . . . , t(⃗sn)) to show that the second diﬀers
from the ﬁrst by a factor of |T|. We represent the ⃗s ’s with respect to the standard bases
RepEn(⃗si) =





s1,i
s2,i
...
sn,i





and then we represent the map application with matrix-vector multiplication
RepEn( t(⃗si) ) =





t1,1
t1,2
. . .
t1,n
t2,1
t2,2
. . .
t2,n
...
tn,1
tn,2
. . .
tn,n










s1,j
s2,j
...
sn,j





= s1,j





t1,1
t2,1
...
tn,1




+ s2,j





t1,2
t2,2
...
tn,2




+ · · · + sn,j





t1,n
t2,n
...
tn,n





= s1,j⃗t1 + s2,j⃗t2 + · · · + sn,j⃗tn
where ⃗ti is column i of T. Then det(t(⃗s1), . . . , t(⃗sn)) equals det(s1,1⃗t1+s2,1⃗t2+. . .+sn,1⃗tn, . . . , s1,n⃗t1+
s2,n⃗t2+. . .+sn,n⃗tn).
As in the derivation of the permutation expansion formula, we apply multilinearity, ﬁrst splitting
along the sum in the ﬁrst argument
det(s1,1⃗t1, . . . , s1,n⃗t1 + s2,n⃗t2 + · · · + sn,n⃗tn) + · · · + det(sn,1⃗tn, . . . , s1,n⃗t1 + s2,n⃗t2 + · · · + sn,n⃗tn)
and then splitting each of those n summands along the sums in the second arguments, etc. We end
with, as in the derivation of the permutation expansion, nn summand determinants, each of the form
det(si1,1⃗ti1, si2,2⃗ti2, . . . , sin,n⃗tin). Factor out each of the si,j’s = si1,1si2,2 . . . sin,n·det(⃗ti1,⃗ti2, . . . , ⃗tin).

Answers to Exercises
175
As in the permutation expansion derivation, whenever two of the indices in i1, . . . , in are equal
then the determinant has two equal arguments, and evaluates to 0. So we need only consider the cases
where i1, . . . , in form a permutation of the numbers 1, . . . , n. We thus have
det(t(⃗s1), . . . , t(⃗sn)) =
X
permutations φ
sφ(1),1 . . . sφ(n),n det(⃗tφ(1), . . . ,⃗tφ(n)).
Swap the columns in det(⃗tφ(1), . . . ,⃗tφ(n)) to get the matrix T back, which changes the sign by a factor
of sgn φ, and then factor out the determinant of T.
=
X
φ
sφ(1),1 . . . sφ(n),n det(⃗t1, . . . ,⃗tn) · sgn φ = det(T)
X
φ
sφ(1),1 . . . sφ(n),n · sgn φ.
As in the proof that the determinant of a matrix equals the determinant of its transpose, we commute
the s’s so they are listed by ascending row number instead of by ascending column number (and we
substitute sgn(φ−1) for sgn(φ)).
= det(T)
X
φ
s1,φ−1(1) . . . sn,φ−1(n) · sgn φ−1 = det(T) det(⃗s1,⃗s2, . . . ,⃗sn)
Four.II.1.29
(a) An algebraic check is easy.
0 = xy2 + x2y3 + x3y −x3y2 −xy3 −x2y = x · (y2 −y3) + y · (x3 −x2) + x2y3 −x3y2
simpliﬁes to the familiar form
y = x · (x3 −x2)/(y3 −y2) + (x2y3 −x3y2)/(y3 −y2)
(the y3 −y2 = 0 case is easily handled).
For geometric insight, this picture shows that the box formed by the three vectors. Note that all
three vectors end in the z = 1 plane. Below the two vectors on the right is the line through (x2, y2)
and (x3, y3).
³x
y
1
´
³x2
y2
1
´
³x3
y3
1
´
The box will have a nonzero volume unless the triangle formed by the ends of the three is degenerate.
That only happens (assuming that (x2, y3) ̸= (x3, y3)) if (x, y) lies on the line through the other two.
(b) This is how the answer was given in the cited source. The altitude through (x1, y1) of a triangle
with vertices (x1, y1) (x2, y2) and (x3, y3) is found in the usual way from the normal form of the
above:
1
p
(x2 −x3)2 + (y2 −y3)2
¯¯¯¯¯¯
x1
x2
x3
y1
y2
y3
1
1
1
¯¯¯¯¯¯
.
Another step shows the area of the triangle to be
1
2
¯¯¯¯¯¯
x1
x2
x3
y1
y2
y3
1
1
1
¯¯¯¯¯¯
.
This exposition reveals the modus operandi more clearly than the usual proof of showing a collection
of terms to be identitical with the determinant.
(c) This is how the answer was given in the cited source. Let
D =
¯¯¯¯¯¯
x1
x2
x3
y1
y2
y3
1
1
1
¯¯¯¯¯¯
then the area of the triangle is (1/2)|D|. Now if the coordinates are all integers, then D is an integer.
Subsection Four.III.1: Laplace’s Expansion

176
Linear Algebra, by Hefferon
Four.III.1.13
(a) (−1)2+3
¯¯¯¯
1
0
0
2
¯¯¯¯ = −2
(b) (−1)3+2
¯¯¯¯
1
2
−1
3
¯¯¯¯ = −5
(c) (−1)4
¯¯¯¯
−1
1
0
2
¯¯¯¯ = −2
Four.III.1.14
(a) 3 · (+1)
¯¯¯¯
2
2
3
0
¯¯¯¯ + 0 · (−1)
¯¯¯¯
1
2
−1
0
¯¯¯¯ + 1 · (+1)
¯¯¯¯
1
2
−1
3
¯¯¯¯ = −13
(b) 1 · (−1)
¯¯¯¯
0
1
3
0
¯¯¯¯ + 2 · (+1)
¯¯¯¯
3
1
−1
0
¯¯¯¯ + 2 · (−1)
¯¯¯¯
3
0
−1
3
¯¯¯¯ = −13
(c) 1 · (+1)
¯¯¯¯
1
2
−1
3
¯¯¯¯ + 2 · (−1)
¯¯¯¯
3
0
−1
3
¯¯¯¯ + 0 · (+1)
¯¯¯¯
3
0
1
2
¯¯¯¯ = −13
Four.III.1.15
adj(T) =


T1,1
T2,1
T3,1
T1,2
T2,2
T3,2
T1,3
T2,3
T3,3

=









+
¯¯¯¯
5
6
8
9
¯¯¯¯ −
¯¯¯¯
2
3
8
9
¯¯¯¯ +
¯¯¯¯
2
3
5
6
¯¯¯¯
−
¯¯¯¯
4
6
7
9
¯¯¯¯ +
¯¯¯¯
1
3
7
9
¯¯¯¯ −
¯¯¯¯
1
3
4
6
¯¯¯¯
+
¯¯¯¯
4
5
7
8
¯¯¯¯ −
¯¯¯¯
1
2
7
8
¯¯¯¯ +
¯¯¯¯
1
2
4
5
¯¯¯¯









=


−3
6
−3
6
−12
6
−3
6
−3


Four.III.1.16
(a)


T1,1
T2,1
T3,1
T1,2
T2,2
T3,2
T1,3
T2,3
T3,3

=









¯¯¯¯
0
2
0
1
¯¯¯¯
−
¯¯¯¯
1
4
0
1
¯¯¯¯
¯¯¯¯
1
4
0
2
¯¯¯¯
−
¯¯¯¯
−1
2
1
1
¯¯¯¯
¯¯¯¯
2
4
1
1
¯¯¯¯
−
¯¯¯¯
2
4
−1
2
¯¯¯¯
¯¯¯¯
−1
0
1
0
¯¯¯¯
−
¯¯¯¯
2
1
1
0
¯¯¯¯
¯¯¯¯
2
1
−1
0
¯¯¯¯









=


0
−1
2
3
−2
−8
0
1
1


(b) The minors are 1×1:
µ
T1,1
T2,1
T1,2
T2,2
¶
=
Ã ¯¯4
¯¯
−
¯¯−1
¯¯
−
¯¯2
¯¯
¯¯3
¯¯
!
=
µ
4
1
−2
3
¶
.
(c)
µ
0
−1
−5
1
¶
(d)


T1,1
T2,1
T3,1
T1,2
T2,2
T3,2
T1,3
T2,3
T3,3

=









¯¯¯¯
0
3
8
9
¯¯¯¯
−
¯¯¯¯
4
3
8
9
¯¯¯¯
¯¯¯¯
4
3
0
3
¯¯¯¯
−
¯¯¯¯
−1
3
1
9
¯¯¯¯
¯¯¯¯
1
3
1
9
¯¯¯¯
−
¯¯¯¯
1
3
−1
3
¯¯¯¯
¯¯¯¯
−1
0
1
8
¯¯¯¯
−
¯¯¯¯
1
4
1
8
¯¯¯¯
¯¯¯¯
1
4
−1
0
¯¯¯¯









=


−24
−12
12
12
6
−6
−8
−4
4


Four.III.1.17
(a) (1/3) ·


0
−1
2
3
−2
−8
0
1
1

=


0
−1/3
2/3
1
−2/3
−8/3
0
1/3
1/3


(b) (1/14) ·
µ
4
1
−2
3
¶
=
µ
2/7
1/14
−1/7
3/14
¶
(c) (1/ −5) ·
µ
0
−1
−5
1
¶
=
µ
0
1/5
1
−1/5
¶
(d) The matrix has a zero determinant, and so has no inverse.
Four.III.1.18




T1,1
T2,1
T3,1
T4,1
T1,2
T2,2
T3,2
T4,2
T1,3
T2,3
T3,3
T4,3
T1,4
T2,4
T3,4
T4,4



=




4
−3
2
−1
−3
6
−4
2
2
−4
6
−3
−1
2
−3
4




Four.III.1.19
The determinant
¯¯¯¯
a
b
c
d
¯¯¯¯
expanded on the ﬁrst row gives a · (+1)|d| + b · (−1)|c| = ad −bc (note the two 1×1 minors).
Four.III.1.20
The determinant of


a
b
c
d
e
f
g
h
i


is this.
a ·
¯¯¯¯
e
f
h
i
¯¯¯¯ −b ·
¯¯¯¯
d
f
g
i
¯¯¯¯ + c ·
¯¯¯¯
d
e
g
h
¯¯¯¯ = a(ei −fh) −b(di −fg) + c(dh −eg)

Answers to Exercises
177
Four.III.1.21
(a)
µ
T1,1
T2,1
T1,2
T2,2
¶
=
µ ¯¯t2,2
¯¯
−
¯¯t1,2
¯¯
−
¯¯t2,1
¯¯
¯¯t1,1
¯¯
¶
=
µ
t2,2
−t1,2
−t2,1
t1,1
¶
(b) (1/t1,1t2,2 −t1,2t2,1) ·
µ
t2,2
−t1,2
−t2,1
t1,1
¶
Four.III.1.22
No. Here is a determinant whose value
¯¯¯¯¯¯
1
0
0
0
1
0
0
0
1
¯¯¯¯¯¯
= 1
dosn’t equal the result of expanding down the diagonal.
1 · (+1)
¯¯¯¯
1
0
0
1
¯¯¯¯ + 1 · (+1)
¯¯¯¯
1
0
0
1
¯¯¯¯ + 1 · (+1)
¯¯¯¯
1
0
0
1
¯¯¯¯ = 3
Four.III.1.23
Consider this diagonal matrix.
D =







d1
0
0
. . .
0
d2
0
0
0
d3
...
dn







If i ̸= j then the i, j minor is an (n −1)×(n −1) matrix with only n −2 nonzero entries, because both
di and dj are deleted. Thus, at least one row or column of the minor is all zeroes, and so the cofactor
Di,j is zero. If i = j then the minor is the diagonal matrix with entries d1, . . . , di−1, di+1, . . . , dn. Its
determinant is obviously (−1)i+j = (−1)2i = 1 times the product of those.
adj(D) =





d2 · · · dn
0
0
0
d1d3 · · · dn
0
...
d1 · · · dn−1





By the way, Theorem 1.9 provides a slicker way to derive this conclusion.
Four.III.1.24
Just note that if S = T trans then the cofactor Sj,i equals the cofactor Ti,j because
(−1)j+i = (−1)i+j and because the minors are the transposes of each other (and the determinant of a
transpose equals the determinant of the matrix).
Four.III.1.25
It is false; here is an example.
T =


1
2
3
4
5
6
7
8
9


adj(T) =


−3
6
−3
6
−12
6
−3
6
−3


adj(adj(T)) =


0
0
0
0
0
0
0
0
0


Four.III.1.26
(a) An example
M =


1
2
3
0
4
5
0
0
6


suggests the right answer.
adj(M) =


M1,1
M2,1
M3,1
M1,2
M2,2
M3,2
M1,3
M2,3
M3,3

=








¯¯¯¯
4
5
0
6
¯¯¯¯
−
¯¯¯¯
2
3
0
6
¯¯¯¯
¯¯¯¯
2
3
4
5
¯¯¯¯
−
¯¯¯¯
0
5
0
6
¯¯¯¯
¯¯¯¯
1
3
0
6
¯¯¯¯
−
¯¯¯¯
1
3
0
5
¯¯¯¯
¯¯¯¯
0
4
0
0
¯¯¯¯
−
¯¯¯¯
1
2
0
0
¯¯¯¯
¯¯¯¯
1
2
0
4
¯¯¯¯








=


24
−12
−2
0
6
−5
0
0
4


The result is indeed upper triangular.
A check of this is detailed but not hard. The entries in the upper triangle of the adjoint are
Ma,b where a > b. We need to verify that the cofactor Ma,b is zero if a > b. With a > b, row a and
column b of M,










m1,1
. . .
m1,b
m2,1
. . .
m2,b
...
...
ma,1
. . .
ma,b
. . .
ma,n
...
mn,b











178
Linear Algebra, by Hefferon
when deleted, leave an upper triangular minor, because entry i, j of the minor is either entry i, j
of M (this happens if a > i and b > j; in this case i < j implies that the entry is zero) or it is
entry i, j + 1 of M (this happens if i < a and j > b; in this case, i < j implies that i < j + 1,
which implies that the entry is zero), or it is entry i + 1, j + 1 of M (this last case happens when
i > a and j > b; obviously here i < j implies that i + 1 < j + 1 and so the entry is zero). Thus the
determinant of the minor is the product down the diagonal. Observe that the a −1, a entry of M is
the a −1, a −1 entry of the minor (it doesn’t get deleted because the relation a > b is strict). But
this entry is zero because M is upper triangular and a −1 < a. Therefore the cofactor is zero, and
the adjoint is upper triangular. (The lower triangular case is similar.)
(b) This is immediate from the prior part, by Corollary 1.11.
Four.III.1.27
We will show that each determinant can be expanded along row i. The argument for
column j is similar.
Each term in the permutation expansion contains one and only one entry from each row. As in
Example 1.1, factor out each row i entry to get |T| = ti,1 · ˆTi,1 + · · · + ti,n · ˆTi,n, where each ˆTi,j is a
sum of terms not containing any elements of row i. We will show that ˆTi,j is the i, j cofactor.
Consider the i, j = n, n case ﬁrst:
tn,n · ˆTn,n = tn,n ·
X
φ
t1,φ(1)t2,φ(2) . . . tn−1,φ(n−1) sgn(φ)
where the sum is over all n-permutations φ such that φ(n) = n. To show that ˆTi,j is the minor Ti,j,
we need only show that if φ is an n-permutation such that φ(n) = n and σ is an n −1-permutation
with σ(1) = φ(1), . . . , σ(n −1) = φ(n −1) then sgn(σ) = sgn(φ). But that’s true because φ and σ
have the same number of inversions.
Back to the general i, j case. Swap adjacent rows until the i-th is last and swap adjacent columns
until the j-th is last.
Observe that the determinant of the i, j-th minor is not aﬀected by these
adjacent swaps because inversions are preserved (since the minor has the i-th row and j-th column
omitted).
On the other hand, the sign of |T| and ˆTi,j is changed n −i plus n −j times.
Thus
ˆTi,j = (−1)n−i+n−j|Ti,j| = (−1)i+j|Ti,j|.
Four.III.1.28
This is obvious for the 1×1 base case.
For the inductive case, assume that the determinant of a matrix equals the determinant of its
transpose for all 1×1, . . . , (n−1)×(n−1) matrices. Expanding on row i gives |T| = ti,1Ti,1+. . . +ti,nTi,n
and expanding on column i gives |T trans| = t1,i(T trans)1,i +· · ·+tn,i(T trans)n,i Since (−1)i+j = (−1)j+i
the signs are the same in the two summations. Since the j, i minor of T trans is the transpose of the i, j
minor of T, the inductive hypothesis gives |(T trans)i,j| = |Ti,j|.
Four.III.1.29
This is how the answer was given in the cited source. Denoting the above determinant
by Dn, it is seen that D2 = 1, D3 = 2. It remains to show that Dn = Dn−1 + Dn−2, n ≥4. In Dn
subtract the (n −3)-th column from the (n −1)-th, the (n −4)-th from the (n −2)-th, . . . , the ﬁrst
from the third, obtaining
Fn =
¯¯¯¯¯¯¯¯¯¯
1
−1
0
0
0
0
. . .
1
1
−1
0
0
0
. . .
0
1
1
−1
0
0
. . .
0
0
1
1
−1
0
. . .
.
.
.
.
.
.
. . .
¯¯¯¯¯¯¯¯¯¯
.
By expanding this determinant with reference to the ﬁrst row, there results the desired relation.
Topic: Cramer’s Rule
1
(a) x = 1, y = −3
(b) x = −2, y = −2
2
z = 1
3
Determinants are unchanged by pivots, including column pivots, so det(Bi) = det(⃗a1, . . . , x1⃗a1+· · ·+
xi⃗ai + · · · + xn⃗an, . . . ,⃗an) is equal to det(⃗a1, . . . , xi⃗ai, . . . ,⃗an) (use the operation of taking −x1 times
the ﬁrst column and adding it to the i-th column, etc.). That is equal to xi · det(⃗a1, . . . ,⃗ai, . . . ,⃗an) =
xi · det(A), as required.

Answers to Exercises
179
4
Because the determinant of A is nonzero, Cramer’s Rule applies and shows that xi = |Bi|/1. Since
Bi is a matrix of integers, its determinant is an integer.
5
The solution of
ax +by = e
cx +dy = f
is
x = ed −fb
ad −bc
y = af −ec
ad −bc
provided of course that the denominators are not zero.
6
Of course, singular systems have |A| equal to zero, but the inﬁnitely many solutions case is charac-
terized by the fact that all of the |Bi| are zero as well.
7
We can consider the two nonsingular cases together with this system
x1 + 2x2 = 6
x1 + 2x2 = c
where c = 6 of course yields inﬁnitely many solutions, and any other value for c yields no solutions.
The corresponding vector equation
x1 ·
µ1
1
¶
+ x2 ·
µ2
2
¶
=
µ6
c
¶
gives a picture of two overlapping vectors. Both lie on the line y = x. In the c = 6 case the vector on
the right side also lies on the line y = x but in any other case it does not.
Topic: Speed of Calculating Determinants
1
(a) Under Octave, rank(rand(5)) ﬁnds the rank of a 5×5 matrix whose entries are (uniformily
distributed) in the interval [0..1). This loop which runs the test 5000 times
octave:1> for i=1:5000
> if rank(rand(5))<5 printf("That’s one."); endif
> endfor
produces (after a few seconds) returns the prompt, with no output.
The Octave script
function elapsed_time = detspeed (size)
a=rand(size);
tic();
for i=1:10
det(a);
endfor
elapsed_time=toc();
endfunction
lead to this session.
octave:1> detspeed(5)
ans = 0.019505
octave:2> detspeed(15)
ans = 0.0054691
octave:3> detspeed(25)
ans = 0.0097431
octave:4> detspeed(35)
ans = 0.017398
(b) Here is the data (rounded a bit), and the graph.
matrix rows
15
25
35
45
55
65
75
85
95
time per ten
0.0034
0.0098
0.0675
0.0285
0.0443
0.0663
0.1428
0.2282
0.1686
(This data is from an average of twenty runs of the above script, because of the possibility that
the randomly chosen matrix happens to take an unusually long or short time. Even so, the timing
cannot be relied on too heavily; this is just an experiment.)

180
Linear Algebra, by Hefferon
20
40
60
80
100
0
0.05
0.1
0.15
0.2
2
The number of operations depends on exactly how the operations are carried out.
(a) The determinant is −11. To row reduce takes a single pivot with two multiplications (−5/2
times 2 plus 5, and −5/2 times 1 plus −3) and the product down the diagonal takes one more
multiplication. The permutation expansion takes two multiplications (2 times −3 and 5 times 1).
(b) The determinant is −39. Counting the operations is routine.
(c) The determinant is 4.
3
One way to get started is to compare these under Octave: det(rand(10));, versus det(hilb(10));,
versus det(eye(10));, versus det(zeroes(10));. You can time them as in tic(); det(rand(10));
toc().
4
This is a simple one.
DO 5 ROW=1, N
PIVINV=1.0/A(ROW,ROW)
DO 10 I=ROW+1, N
DO 20 J=I, N
A(I,J)=A(I,J)-PIVINV*A(ROW,J)
20 CONTINUE
10 CONTINUE
5 CONTINUE
5
Yes, because the J is in the innermost loop.
Topic: Projective Geometry
1
From the dot product
0 =


1
0
0

¡
L1
L2
L3
¢
= L1
we get that the equation is L1 = 0.
2
(a) This determinant
0 =
¯¯¯¯¯¯
1
4
x
2
5
y
3
6
z
¯¯¯¯¯¯
= −3x + 6y −3z
shows that the line is L =
¡
−3
6
−3
¢
.
(b)


−3
6
−3


3
The line incident on
u =


u1
u2
u3


v =


v1
v2
v3


can be found from this determinant equation.
0 =
¯¯¯¯¯¯
u1
v1
x
u2
v2
y
u3
v3
z
¯¯¯¯¯¯
= (u2v3 −u3v2) · x + (u3v1 −u1v3) · y + (u1v2 −u2v1) · z
The equation for the point incident on two lines is the same.

Answers to Exercises
181
4
If p1, p2, p3, and q1, q2, q3 are two triples of homogeneous coordinates for p then the two column
vectors are in proportion, that is, lie on the same line through the origin. Similarly, the two row vectors
are in proportion.
k ·


p1
p2
p3

=


q1
q2
q3


m ·
¡
L1
L2
L3
¢
=
¡
M1
M2
M3
¢
Then multiplying gives the answer (km) · (p1L1 + p2L2 + p3L3) = q1M1 + q2M2 + q3M3 = 0.
5
The picture of the solar eclipse — unless the image plane is exactly perpendicular to the line from
the sun through the pinhole — shows the circle of the sun projecting to an image that is an ellipse.
(Another example is that in many pictures in this Topic, the circle that is the sphere’s equator is drawn
as an ellipse, that is, is seen by a viewer of the drawing as an ellipse.)
The solar eclipse picture also shows the converse. If we picture the projection as going from left to
right through the pinhole then the ellipse I projects through P to a circle S.
6
A spot on the unit sphere


p1
p2
p3


is non-equatorial if and only if p3 ̸= 0. In that case it corresponds to this point on the z = 1 plane


p1/p3
p2/p3
1


since that is intersection of the line containing the vector and the plane.
7
(a) Other pictures are possible, but this is one.
T0
U0
V0
T1
U1
V1
V2
U2
T2
The intersections T0U1 ∩T1U0 = V2, T0V1 ∩T1V0 = U2, and U0V1 ∩U1V0 = T2 are labeled so that
on each line is a T, a U, and a V .
(b) The lemma used in Desargue’s Theorem gives a basis B with respect to which the points have
these homogeneous coordinate vectors.
RepB(⃗t0) =


1
0
0


RepB(⃗t1) =


0
1
0


RepB(⃗t2) =


0
0
1


RepB(⃗v0) =


1
1
1


(c) First, any U0 on T0V0
RepB(⃗u0) = a


1
0
0

+ b


1
1
1

=


a + b
b
b


has homogeneous coordinate vectors of this form


u0
1
1


(u0 is a parameter; it depends on where on the T0V0 line the point U0 is, but any point on that line
has a homogeneous coordinate vector of this form for some u0 ∈R). Similarly, U2 is on T1V0
RepB(⃗u2) = c


0
1
0

+ d


1
1
1

=


d
c + d
d


and so has this homogeneous coordinate vector.

1
u2
1



182
Linear Algebra, by Hefferon
Also similarly, U1 is incident on T2V0
RepB(⃗u1) = e


0
0
1

+ f


1
1
1

=


f
f
e + f


and has this homogeneous coordinate vector. 

1
1
u1


(d) Because V1 is T0U2 ∩U0T2 we have this.
g


1
0
0

+ h


1
u2
1

= i


u0
1
1

+ j


0
0
1


=⇒
g + h = iu0
hu2 = i
h = i + j
Substituting hu2 for i in the ﬁrst equation


hu0u2
hu2
h


shows that V1 has this two-parameter homogeneous coordinate vector.


u0u2
u2
1


(e) Since V2 is the intersection T0U1 ∩T1U0
k


1
0
0

+ l


1
1
u1

= m


0
1
0

+ n


u0
1
1


=⇒
k + l = nu0
l = m + n
lu1 = n
and substituting lu1 for n in the ﬁrst equation

lu0u1
l
lu1


gives that V2 has this two-parameter homogeneous coordinate vector.


u0u1
1
u1


(f) Because V1 is on the T1U1 line its homogeneous coordinate vector has the form
p


0
1
0

+ q


1
1
u1

=


q
p + q
qu1


(∗)
but a previous part of this question established that V1’s homogeneous coordinate vectors have the
form


u0u2
u2
1


and so this a homogeneous coordinate vector for V1.


u0u1u2
u1u2
u1


(∗∗)
By (∗) and (∗∗), there is a relationship among the three parameters: u0u1u2 = 1.
(g) The homogeneous coordinate vector of V2 can be written in this way.


u0u1u2
u2
u1u2

=


1
u2
u1u2


Now, the T2U2 line consists of the points whose homogeneous coordinates have this form.
r


0
0
1

+ s


1
u2
1

=


s
su2
r + s


Taking s = 1 and r = u1u2 −1 shows that the homogeneous coordinate vectors of V2 have this form.

Chapter Five: Similarity
Subsection Five.II.1: Deﬁnition and Examples
Five.II.1.4
One way to proceed is left to right.
PSP −1 =
µ
4
2
−3
2
¶ µ
1
3
−2
−6
¶ µ
2/14
−2/14
3/14
4/14
¶
=
µ
0
0
−7
−21
¶ µ
2/14
−2/14
3/14
4/14
¶
=
µ
0
0
−11/2
−5
¶
Five.II.1.5
(a) Because the matrix (2) is 1×1, the matrices P and P −1 are also 1×1 and so where
P = (p) the inverse is P −1 = (1/p). Thus P(2)P −1 = (p)(2)(1/p) = (2).
(b) Yes: recall that scalar multiples can be brought out of a matrix P(cI)P −1 = cPIP −1 = cI. By
the way, the zero and identity matrices are the special cases c = 0 and c = 1.
(c) No, as this example shows.
µ
1
−2
−1
1
¶ µ
−1
0
0
−3
¶ µ
−1
−2
−1
−1
¶
=
µ
−5
−4
2
1
¶
Five.II.1.6
Gauss’ method shows that the ﬁrst matrix represents maps of rank two while the second
matrix represents maps of rank three.
Five.II.1.7
(a) Because t is described with the members of B, ﬁnding the matrix representation is
easy:
RepB(t(x2)) =


0
1
1


B
RepB(t(x)) =


1
0
−1


B
RepB(t(1)) =


0
0
3


B
gives this.
RepB,B(t)


0
1
0
1
0
0
1
−1
3


(b) We will ﬁnd t(1), t(1 + x), and t(1 + x + x2, to ﬁnd how each is represented with respect
to D.
We are given that t(1) = 3, and the other two are easy to see: t(1 + x) = x2 + 2 and
t(1 + x + x2) = x2 + x + 3. By eye, we get the representation of each vector
RepD(t(1)) =


3
0
0


D
RepD(t(1 + x)) =


2
−1
1


D
RepD(t(1 + x + x2)) =


2
0
1


D
and thus the representation of the map.
RepD,D(t) =


3
2
2
0
−1
0
0
1
1


(c) The diagram, adapted for this T and S,
Vw.r.t. D
t
−−−−→
S
Vw.r.t. D
id
yP
id
yP
Vw.r.t. B
t
−−−−→
T
Vw.r.t. B
shows that P = RepD,B(id).
P =


0
0
1
0
1
1
1
1
1



184
Linear Algebra, by Hefferon
Five.II.1.8
One possible choice of the bases is
B = ⟨
µ1
2
¶
,
µ−1
1
¶
⟩
D = E2 = ⟨
µ1
0
¶
,
µ0
1
¶
⟩
(this B is suggested by the map description). To ﬁnd the matrix T = RepB,B(t), solve the relations
c1
µ
1
2
¶
+ c2
µ
−1
1
¶
=
µ
3
0
¶
ˆc1
µ
1
2
¶
+ ˆc2
µ
−1
1
¶
=
µ
−1
2
¶
to get c1 = 1, c2 = −2, ˆc1 = 1/3 and ˆc2 = 4/3.
RepB,B(t) =
µ 1
1/3
−2
4/3
¶
Finding RepD,D(t) involves a bit more computation. We ﬁrst ﬁnd t(⃗e1). The relation
c1
µ
1
2
¶
+ c2
µ
−1
1
¶
=
µ
1
0
¶
gives c1 = 1/3 and c2 = −2/3, and so
RepB(⃗e1) =
µ 1/3
−2/3
¶
B
making
RepB(t(⃗e1)) =
µ
1
1/3
−2
4/3
¶
B,B
µ
1/3
−2/3
¶
B
=
µ
1/9
−14/9
¶
B
and hence t acts on the ﬁrst basis vector ⃗e1 in this way.
t(⃗e1) = (1/9) ·
µ
1
2
¶
−(14/9) ·
µ
−1
1
¶
=
µ
5/3
−4/3
¶
The computation for t(⃗e2) is similar. The relation
c1
µ
1
2
¶
+ c2
µ
−1
1
¶
=
µ
0
1
¶
gives c1 = 1/3 and c2 = 1/3, so
RepB(⃗e1) =
µ1/3
1/3
¶
B
making
RepB(t(⃗e1)) =
µ
1
1/3
−2
4/3
¶
B,B
µ
1/3
1/3
¶
B
=
µ
4/9
−2/9
¶
B
and hence t acts on the second basis vector ⃗e2 in this way.
t(⃗e2) = (4/9) ·
µ
1
2
¶
−(2/9) ·
µ
−1
1
¶
=
µ
2/3
2/3
¶
Therefore
RepD,D(t) =
µ 5/3
2/3
−4/3
2/3
¶
and these are the change of basis matrices.
P = RepB,D(id) =
µ
1
−1
2
1
¶
P −1 =
¡
RepB,D(id)
¢−1 =
µ
1
−1
2
1
¶−1
=
µ
1/3
1/3
−2/3
1/3
¶
The check of these computations is routine.
µ
1
−1
2
1
¶ µ
1
1/3
−2
4/3
¶ µ
1/3
1/3
−2/3
1/3
¶
=
µ
5/3
2/3
−4/3
2/3
¶
Five.II.1.9
The only representation of a zero map is a zero matrix, no matter what the pair of bases
RepB,D(z) = Z, and so in particular for any single basis B we have RepB,B(z) = Z. The case of the
identity is related, but slightly diﬀerent: the only representation of the identity map, with respect to
any B, B, is the identity RepB,B(id) = I. (Remark: of course, we have seen examples where B ̸= D
and RepB,D(id) ̸= I — in fact, we have seen that any nonsingular matrix is a representation of the
identity map with respect to some B, D.)
Five.II.1.10
No. If A = PBP −1 then A2 = (PBP −1)(PBP −1) = PB2P −1.
Five.II.1.11
Matrix similarity is a special case of matrix equivalence (if matrices are similar then they
are matrix equivalent) and matrix equivalence preserves nonsingularity.

Answers to Exercises
185
Five.II.1.12
A matrix is similar to itself; take P to be the identity matrix: IPI−1 = IPI = P.
If T is similar to S then T = PSP −1 and so P −1TP = S. Rewrite this as S = (P −1)T(P −1)−1 to
conclude that S is similar to T.
If T is similar to S and S is similar to U then T = PSP −1 and S = QUQ−1.
Then T =
PQUQ−1P −1 = (PQ)U(PQ)−1, showing that T is similar to U.
Five.II.1.13
Let fx and fy be the reﬂection maps (sometimes called ‘ﬂip’s). For any bases B and D,
the matrices RepB,B(fx) and RepD,D(fy) are similar. First note that
S = RepE2,E2(fx) =
µ
1
0
0
−1
¶
T = RepE2,E2(fy) =
µ
−1
0
0
1
¶
are similar because the second matrix is the representation of fx with respect to the basis A = ⟨⃗e2,⃗e1⟩:
µ
1
0
0
−1
¶
= P
µ
−1
0
0
1
¶
P −1
where P = RepA,E2(id).
R2
w.r.t. A
fx
−−−−→
T
V R2
w.r.t. A
id
yP
id
yP
R2
w.r.t. E2
fx
−−−−→
S
R2
w.r.t. E2
Now the conclusion follows from the transitivity part of Exercise 12.
To ﬁnish without relying on that exercise, write RepB,B(fx) = QTQ−1 = QRepE2,E2(fx)Q−1 and
RepD,D(fy) = RSR−1 = RRepE2,E2(fy)R−1. Using the equation in the ﬁrst paragraph, the ﬁrst of
these two becomes RepB,B(fx) = QPRepE2,E2(fy)P −1Q−1 and rewriting the second of these two as
R−1 · RepD,D(fy) · R = RepE2,E2(fy) and substituting gives the desired relationship
RepB,B(fx) = QPRepE2,E2(fy)P −1Q−1
= QPR−1 · RepD,D(fy) · RP −1Q−1 = (QPR−1) · RepD,D(fy) · (QPR−1)−1
Thus the matrices RepB,B(fx) and RepD,D(fy) are similar.
Five.II.1.14
We must show that if two matrices are similar then they have the same determinant and
the same rank. Both determinant and rank are properties of matrices that we have already shown to
be preserved by matrix equivalence. They are therefore preserved by similarity (which is a special case
of matrix equivalence: if two matrices are similar then they are matrix equivalent).
To prove the statement without quoting the results about matrix equivalence, note ﬁrst that rank
is a property of the map (it is the dimension of the rangespace) and since we’ve shown that the rank of
a map is the rank of a representation, it must be the same for all representations. As for determinants,
|PSP −1| = |P| · |S| · |P −1| = |P| · |S| · |P|−1 = |S|.
The converse of the statement does not hold; for instance, there are matrices with the same de-
terminant that are not similar. To check this, consider a nonzero matrix with a determinant of zero.
It is not similar to the zero matrix, the zero matrix is similar only to itself, but they have they same
determinant. The argument for rank is much the same.
Five.II.1.15
The matrix equivalence class containing all n×n rank zero matrices contains only a single
matrix, the zero matrix. Therefore it has as a subset only one similarity class.
In contrast, the matrix equivalence class of 1 × 1 matrices of rank one consists of those 1 × 1
matrices (k) where k ̸= 0. For any basis B, the representation of multiplication by the scalar k is
RepB,B(tk) = (k), so each such matrix is alone in its similarity class. So this is a case where a matrix
equivalence class splits into inﬁnitely many similarity classes.
Five.II.1.16
Yes, these are similar
µ
1
0
0
3
¶
µ
3
0
0
1
¶
since, where the ﬁrst matrix is RepB,B(t) for B = ⟨⃗β1, ⃗β2⟩, the second matrix is RepD,D(t) for D =
⟨⃗β2, ⃗β1⟩.
Five.II.1.17
The k-th powers are similar because, where each matrix represents the map t, the k-
th powers represent tk, the composition of k-many t’s. (For instance, if T = reptB, B then T 2 =
RepB,B(t ◦t).)

186
Linear Algebra, by Hefferon
Restated more computationally, if T = PSP −1 then T 2 = (PSP −1)(PSP −1) = PS2P −1. Induc-
tion extends that to all powers.
For the k ≤0 case, suppose that S is invertible and that T = PSP −1. Note that T is invertible:
T −1 = (PSP −1)−1 = PS−1P −1, and that same equation shows that T −1 is similar to S−1. Other
negative powers are now given by the ﬁrst paragraph.
Five.II.1.18
In conceptual terms, both represent p(t) for some transformation t. In computational
terms, we have this.
p(T) = cn(PSP −1)n + · · · + c1(PSP −1) + c0I
= cnPSnP −1 + · · · + c1PSP −1 + c0I
= PcnSnP −1 + · · · + Pc1SP −1 + Pc0P −1
= P(cnSn + · · · + c1S + c0)P −1
Five.II.1.19
There are two equivalence classes, (i) the class of rank zero matrices, of which there is
one: C1 = {(0)}, and (2) the class of rank one matrices, of which there are inﬁnitely many: C2 =
{(k)
¯¯ k ̸= 0}.
Each 1×1 matrix is alone in its similarity class. That’s because any transformation of a one-
dimensional space is multiplication by a scalar tk : V →V given by ⃗v 7→k · ⃗v. Thus, for any basis
B = ⟨⃗β⟩, the matrix representing a transformation tk with respect to B, B is (RepB(tk(⃗β))) = (k).
So, contained in the matrix equivalence class C1 is (obviously) the single similarity class consisting
of the matrix (0). And, contained in the matrix equivalence class C2 are the inﬁnitely many, one-
member-each, similarity classes consisting of (k) for k ̸= 0.
Five.II.1.20
No. Here is an example that has two pairs, each of two similar matrices:
µ
1
−1
1
2
¶ µ
1
0
0
3
¶ µ
2/3
1/3
−1/3
1/3
¶
=
µ
5/3
−2/3
−4/3
7/3
¶
and
µ
1
−2
−1
1
¶ µ
−1
0
0
−3
¶ µ
−1
−2
−1
−1
¶
=
µ
−5
−4
2
1
¶
(this example is mostly arbitrary, but not entirely, because the the center matrices on the two left sides
add to the zero matrix). Note that the sums of these similar matrices are not similar
µ
1
0
0
3
¶
+
µ
−1
0
0
−3
¶
=
µ
0
0
0
0
¶
µ
5/3
−2/3
−4/3
7/3
¶
+
µ
−5
−4
2
1
¶
̸=
µ
0
0
0
0
¶
since the zero matrix is similar only to itself.
Five.II.1.21
If N = P(T −λI)P −1 then N = PTP −1 −P(λI)P −1. The diagonal matrix λI commutes
with anything, so P(λI)P −1 = PP −1(λI) = λI. Thus N = PTP −1 −λI and consequently N + λI =
PTP −1. (So not only are they similar, in fact they are similar via the same P.)
Subsection Five.II.2: Diagonalizability
Five.II.2.6
Because the basis vectors are chosen arbitrarily, many diﬀerent answers are possible. How-
ever, here is one way to go; to diagonalize
T =
µ
4
−2
1
1
¶
take it as the representation of a transformation with respect to the standard basis T = RepE2,E2(t)
and look for B = ⟨⃗β1, ⃗β2⟩such that
RepB,B(t) =
µ
λ1
0
0
λ2
¶
that is, such that t(⃗β1) = λ1 and t(⃗β2) = λ2.
µ
4
−2
1
1
¶
⃗β1 = λ1 · ⃗β1
µ
4
−2
1
1
¶
⃗β2 = λ2 · ⃗β2
We are looking for scalars x such that this equation
µ
4
−2
1
1
¶ µ
b1
b2
¶
= x ·
µ
b1
b2
¶

Answers to Exercises
187
has solutions b1 and b2, which are not both zero. Rewrite that as a linear system
(4 −x) · b1 +
−2 · b2 = 0
1 · b1 + (1 −x) · b2 = 0
If x = 4 then the ﬁrst equation gives that b2 = 0, and then the second equation gives that b1 = 0. The
case where both b’s are zero is disallowed so we can assume that x ̸= 4.
(−1/(4−x))ρ1+ρ2
−→
(4 −x) · b1 +
−2 · b2 = 0
((x2 −5x + 6)/(4 −x)) · b2 = 0
Consider the bottom equation. If b2 = 0 then the ﬁrst equation gives b1 = 0 or x = 4. The b1 = b2 = 0
case is disallowed. The other possibility for the bottom equation is that the numerator of the fraction
x2 −5x + 6 = (x −2)(x −3) is zero. The x = 2 case gives a ﬁrst equation of 2b1 −2b2 = 0, and so
associated with x = 2 we have vectors whose ﬁrst and second components are equal:
⃗β1 =
µ1
1
¶
(so
µ4
−2
1
1
¶ µ1
1
¶
= 2 ·
µ1
1
¶
, and λ1 = 2).
If x = 3 then the ﬁrst equation is b1 −2b2 = 0 and so the associated vectors are those whose ﬁrst
component is twice their second:
⃗β2 =
µ
2
1
¶
(so
µ
4
−2
1
1
¶ µ
2
1
¶
= 3 ·
µ
2
1
¶
, and so λ2 = 3).
This picture
R2
w.r.t. E2
t
−−−−→
T
R2
w.r.t. E2
id
y
id
y
R2
w.r.t. B
t
−−−−→
D
R2
w.r.t. B
shows how to get the diagonalization.
µ
2
0
0
3
¶
=
µ
1
2
1
1
¶−1 µ
4
−2
1
1
¶ µ
1
2
1
1
¶
Comment. This equation matches the T = PSP −1 deﬁnition under this renaming.
T =
µ
2
0
0
3
¶
P =
µ
1
2
1
1
¶−1
P −1 =
µ
1
2
1
1
¶
S =
µ
4
−2
1
1
¶
Five.II.2.7
(a) Setting up
µ
−2
1
0
2
¶ µ
b1
b2
¶
= x ·
µ
b1
b2
¶
=⇒
(−2 −x) · b1 +
b2 = 0
(2 −x) · b2 = 0
gives the two possibilities that b2 = 0 and x = 2. Following the b2 = 0 possibility leads to the ﬁrst
equation (−2 −x)b1 = 0 with the two cases that b1 = 0 and that x = −2. Thus, under this ﬁrst
possibility, we ﬁnd x = −2 and the associated vectors whose second component is zero, and whose
ﬁrst component is free.
µ
−2
1
0
2
¶ µ
b1
0
¶
= −2 ·
µ
b1
0
¶
⃗β1 =
µ
1
0
¶
Following the other possibility leads to a ﬁrst equation of −4b1+b2 = 0 and so the vectors associated
with this solution have a second component that is four times their ﬁrst component.
µ
−2
1
0
2
¶ µ
b1
4b1
¶
= 2 ·
µ
b1
4b1
¶
⃗β2 =
µ
1
4
¶
The diagonalization is this. µ
1
1
0
4
¶−1 µ
−2
1
0
2
¶ µ
1
1
0
4
¶−1 µ
−2
0
0
2
¶
(b) The calculations are like those in the prior part.
µ5
4
0
1
¶ µb1
b2
¶
= x ·
µb1
b2
¶
=⇒
(5 −x) · b1 +
4 · b2 = 0
(1 −x) · b2 = 0
The bottom equation gives the two possibilities that b2 = 0 and x = 1.
Following the b2 = 0
possibility, and discarding the case where both b2 and b1 are zero, gives that x = 5, associated with
vectors whose second component is zero and whose ﬁrst component is free.
⃗β1 =
µ
1
0
¶

188
Linear Algebra, by Hefferon
The x = 1 possibility gives a ﬁrst equation of 4b1 + 4b2 = 0 and so the associated vectors have a
second component that is the negative of their ﬁrst component.
⃗β1 =
µ
1
−1
¶
We thus have this diagonalization.
µ
1
1
0
−1
¶−1 µ
5
4
0
1
¶ µ
1
1
0
−1
¶
=
µ
5
0
0
1
¶
Five.II.2.8
For any integer p,



d1
0
0
...
dn



p
=



dp
1
0
0
...
dp
n


.
Five.II.2.9
These two are not similar
µ
0
0
0
0
¶
µ
1
0
0
1
¶
because each is alone in its similarity class.
For the second half, these
µ2
0
0
3
¶
µ3
0
0
2
¶
are similar via the matrix that changes bases from ⟨⃗β1, ⃗β2⟩to ⟨⃗β2, ⃗β1⟩. (Question. Are two diagonal
matrices similar if and only if their diagonal entries are permutations of each other’s?)
Five.II.2.10
Contrast these two.
µ
2
0
0
1
¶
µ
2
0
0
0
¶
The ﬁrst is nonsingular, the second is singular.
Five.II.2.11
To check that the inverse of a diagonal matrix is the diagonal matrix of the inverses, just
multiply.





a1,1
0
0
a2,2
...
an,n










1/a1,1
0
0
1/a2,2
...
1/an,n





(Showing that it is a left inverse is just as easy.)
If a diagonal entry is zero then the diagonal matrix is singular; it has a zero determinant.
Five.II.2.12
(a) The check is easy.
µ
1
1
0
−1
¶ µ
3
2
0
1
¶
=
µ
3
3
0
−1
¶
µ
3
3
0
−1
¶ µ
1
1
0
−1
¶−1
=
µ
3
0
0
1
¶
(b) It is a coincidence, in the sense that if T = PSP −1 then T need not equal P −1SP. Even in the
case of a diagonal matrix D, the condition that D = PTP −1 does not imply that D equals P −1TP.
The matrices from Example 2.2 show this.
µ
1
2
1
1
¶ µ
4
−2
1
1
¶
=
µ
6
0
5
−1
¶
µ
6
0
5
−1
¶ µ
1
2
1
1
¶−1
=
µ
−6
12
−6
11
¶
Five.II.2.13
The columns of the matrix are chosen as the vectors associated with the x’s. The exact
choice, and the order of the choice was arbitrary. We could, for instance, get a diﬀerent matrix by
swapping the two columns.
Five.II.2.14
Diagonalizing and then taking powers of the diagonal matrix shows that
µ
−3
1
−4
2
¶k
= 1
3
µ
−1
1
−4
4
¶
+ (−2
3 )k
µ
4
−1
4
−1
¶
.
Five.II.2.15
(a)
µ
1
1
0
−1
¶−1 µ
1
1
0
0
¶ µ
1
1
0
−1
¶
=
µ
1
0
0
0
¶
(b)
µ
1
1
1
−1
¶−1 µ
0
1
1
0
¶ µ
1
1
0
−1
¶
=
µ
1
0
0
−1
¶

Answers to Exercises
189
Five.II.2.16
Yes, ct is diagonalizable by the ﬁnal theorem of this subsection.
No, t+s need not be diagonalizable. Intuitively, the problem arises when the two maps diagonalize
with respect to diﬀerent bases (that is, when they are not simultaneously diagonalizable). Speciﬁcally,
these two are diagonalizable but their sum is not:
µ
1
1
0
0
¶
µ
−1
0
0
0
¶
(the second is already diagonal; for the ﬁrst, see Exercise 15). The sum is not diagonalizable because
its square is the zero matrix.
The same intuition suggests that t ◦s is not be diagonalizable. These two are diagonalizable but
their product is not:
µ
1
0
0
0
¶
µ
0
1
1
0
¶
(for the second, see Exercise 15).
Five.II.2.17
If
P
µ1
c
0
1
¶
P −1 =
µa
0
0
b
¶
then
P
µ
1
c
0
1
¶
=
µ
a
0
0
b
¶
P
so
µ
p
q
r
s
¶ µ
1
c
0
1
¶
=
µ
a
0
0
b
¶ µ
p
q
r
s
¶
µ
p
cp + q
r
cr + s
¶
=
µ
ap
aq
br
bs
¶
The 1, 1 entries show that a = 1 and the 1, 2 entries then show that pc = 0. Since c ̸= 0 this means
that p = 0. The 2, 1 entries show that b = 1 and the 2, 2 entries then show that rc = 0. Since c ̸= 0
this means that r = 0. But if both p and r are 0 then P is not invertible.
Five.II.2.18
(a) Using the formula for the inverse of a 2×2 matrix gives this.
µa
b
c
d
¶ µ1
2
2
1
¶
·
1
ad −bc ·
µ d
−b
−c
a
¶
=
1
ad −bc
µad + 2bd −2ac −bc
−ab −2b2 + 2a2 + ab
cd + 2d2 −2c2 −cd
−bc −2bd + 2ac + ad
¶
Now pick scalars a, . . . , d so that ad −bc ̸= 0 and 2d2 −2c2 = 0 and 2a2 −2b2 = 0. For example,
these will do.
µ
1
1
1
−1
¶ µ
1
2
2
1
¶
· 1
−2 ·
µ
−1
−1
−1
1
¶
= 1
−2
µ
−6
0
0
2
¶
(b) As above,
µ
a
b
c
d
¶ µ
x
y
y
z
¶
·
1
ad −bc ·
µ
d
−b
−c
a
¶
=
1
ad −bc
µ
adx + bdy −acy −bcz
−abx −b2y + a2y + abz
cdx + d2y −c2y −cdz
−bcx −bdy + acy + adz
¶
we are looking for scalars a, . . . , d so that ad −bc ̸= 0 and −abx −b2y + a2y + abz = 0 and
cdx + d2y −c2y −cdz = 0, no matter what values x, y, and z have.
For starters, we assume that y ̸= 0, else the given matrix is already diagonal. We shall use that
assumption because if we (arbitrarily) let a = 1 then we get
−bx −b2y + y + bz = 0
(−y)b2 + (z −x)b + y = 0
and the quadratic formula gives
b = −(z −x) ±
p
(z −x)2 −4(−y)(y)
−2y
y ̸= 0
(note that if x, y, and z are real then these two b’s are real as the discriminant is positive). By the
same token, if we (arbitrarily) let c = 1 then
dx + d2y −y −dz = 0
(y)d2 + (x −z)d −y = 0
and we get here
d = −(x −z) ±
p
(x −z)2 −4(y)(−y)
2y
y ̸= 0

190
Linear Algebra, by Hefferon
(as above, if x, y, z ∈R then this discriminant is positive so a symmetric, real, 2×2 matrix is similar
to a real diagonal matrix).
For a check we try x = 1, y = 2, z = 1.
b = 0 ± √0 + 16
−4
= ∓1
d = 0 ± √0 + 16
4
= ±1
Note that not all four choices (b, d) = (+1, +1), . . . , (−1, −1) satisfy ad −bc ̸= 0.
Subsection Five.II.3: Eigenvalues and Eigenvectors
Five.II.3.20
(a) This
0 =
¯¯¯¯
10 −x
−9
4
−2 −x
¯¯¯¯ = (10 −x)(−2 −x) −(−36)
simpliﬁes to the characteristic equation x2 −8x + 16 = 0. Because the equation factors into (x −4)2
there is only one eigenvalue λ1 = 4.
(b) 0 = (1 −x)(3 −x) −8 = x2 −4x −5; λ1 = 5, λ2 = −1
(c) x2 −21 = 0; λ1 =
√
21, λ2 = −
√
21
(d) x2 = 0; λ1 = 0
(e) x2 −2x + 1 = 0; λ1 = 1
Five.II.3.21
(a) The characteristic equation is (3 −x)(−1 −x) = 0. Its roots, the eigenvalues, are
λ1 = 3 and λ2 = −1. For the eigenvectors we consider this equation.
µ3 −x
0
8
−1 −x
¶ µb1
b2
¶
=
µ0
0
¶
For the eigenvector associated with λ1 = 3, we consider the resulting linear system.
0 · b1 +
0 · b2 = 0
8 · b1 + −4 · b2 = 0
The eigenspace is the set of vectors whose second component is twice the ﬁrst component.
{
µ
b2/2
b2
¶ ¯¯ b2 ∈C}
µ
3
0
8
−1
¶ µ
b2/2
b2
¶
= 3 ·
µ
b2/2
b2
¶
(Here, the parameter is b2 only because that is the variable that is free in the above system.) Hence,
this is an eigenvector associated with the eigenvalue 3.
µ
1
2
¶
Finding an eigenvector associated with λ2 = −1 is similar. This system
4 · b1 + 0 · b2 = 0
8 · b1 + 0 · b2 = 0
leads to the set of vectors whose ﬁrst component is zero.
{
µ
0
b2
¶ ¯¯ b2 ∈C}
µ
3
0
8
−1
¶ µ
0
b2
¶
= −1 ·
µ
0
b2
¶
And so this is an eigenvector associated with λ2.µ
0
1
¶
(b) The characteristic equation is
0 =
¯¯¯¯
3 −x
2
−1
−x
¯¯¯¯ = x2 −3x + 2 = (x −2)(x −1)
and so the eigenvalues are λ1 = 2 and λ2 = 1. To ﬁnd eigenvectors, consider this system.
(3 −x) · b1 + 2 · b2 = 0
−1 · b1 −x · b2 = 0
For λ1 = 2 we get
1 · b1 + 2 · b2 = 0
−1 · b1 −2 · b2 = 0

Answers to Exercises
191
leading to this eigenspace and eigenvector.
{
µ
−2b2
b2
¶ ¯¯ b2 ∈C}
µ
−2
1
¶
For λ2 = 1 the system is
2 · b1 + 2 · b2 = 0
−1 · b1 −1 · b2 = 0
leading to this.
{
µ
−b2
b2
¶ ¯¯ b2 ∈C}
µ
−1
1
¶
Five.II.3.22
The characteristic equation
0 =
¯¯¯¯
−2 −x
−1
5
2 −x
¯¯¯¯ = x2 + 1
has the complex roots λ1 = i and λ2 = −i. This system
(−2 −x) · b1 −
1 · b2 = 0
5 · b1
(2 −x) · b2 = 0
For λ1 = i Gauss’ method gives this reduction.
(−2 −i) · b1 −
1 · b2 = 0
5 · b1 −(2 −i) · b2 = 0
(−5/(−2−i))ρ1+ρ2
−→
(−2 −i) · b1 −1 · b2 = 0
0 = 0
(For the calculation in the lower right get a common denominator
5
−2 −i −(2 −i) =
5
−2 −i −−2 −i
−2 −i · (2 −i) = 5 −(−5)
−2 −i
to see that it gives a 0 = 0 equation.) These are the resulting eigenspace and eigenvector.
{
µ(1/(−2 −i))b2
b2
¶ ¯¯ b2 ∈C}
µ1/(−2 −i)
1
¶
For λ2 = −i the system
(−2 + i) · b1 −
1 · b2 = 0
5 · b1 −(2 + i) · b2 = 0
(−5/(−2+i))ρ1+ρ2
−→
(−2 + i) · b1 −1 · b2 = 0
0 = 0
leads to this.
{
µ
(1/(−2 + i))b2
b2
¶ ¯¯ b2 ∈C}
µ
1/(−2 + i)
1
¶
Five.II.3.23
The characteristic equation is
0 =
¯¯¯¯¯¯
1 −x
1
1
0
−x
1
0
0
1 −x
¯¯¯¯¯¯
= (1 −x)2(−x)
and so the eigenvalues are λ1 = 1 (this is a repeated root of the equation) and λ2 = 0. For the rest,
consider this system.
(1 −x) · b1 +
b2 +
b3 = 0
−x · b2 +
b3 = 0
(1 −x) · b3 = 0
When x = λ1 = 1 then the solution set is this eigenspace.
{


b1
0
0

¯¯ b1 ∈C}
When x = λ2 = 0 then the solution set is this eigenspace.
{


−b2
b2
0

¯¯ b2 ∈C}
So these are eigenvectors associated with λ1 = 1 and λ2 = 0.


1
0
0




−1
1
0



192
Linear Algebra, by Hefferon
Five.II.3.24
(a) The characteristic equation is
0 =
¯¯¯¯¯¯
3 −x
−2
0
−2
3 −x
0
0
0
5 −x
¯¯¯¯¯¯
= x3 −11x2 + 35x −25 = (x −1)(x −5)2
and so the eigenvalues are λ1 = 1 and also the repeated eigenvalue λ2 = 5. To ﬁnd eigenvectors,
consider this system.
(3 −x) · b1 −
2 · b2
= 0
−2 · b1 + (3 −x) · b2
= 0
(5 −x) · b3 = 0
For λ1 = 1 we get
2 · b1 −2 · b2
= 0
−2 · b1 + 2 · b2
= 0
4 · b3 = 0
leading to this eigenspace and eigenvector.
{


b2
b2
0

¯¯ b2 ∈C}


1
1
0


For λ2 = 1 the system is
−2 · b1 −2 · b2
= 0
−2 · b1 −2 · b2
= 0
0 · b3 = 0
leading to this.
{


−b2
b2
0

+


0
0
b3

¯¯ b2, b3 ∈C}


−1
1
0

,


0
0
1


(b) The characteristic equation is
0 =
¯¯¯¯¯¯
−x
1
0
0
−x
1
4
−17
8 −x
¯¯¯¯¯¯
= −x3 + 8x2 −17x + 4 = −1 · (x −4)(x2 −4x + 1)
and the eigenvalues are λ1 = 4 and (by using the quadratic equation) λ2 = 2+
√
3 and λ3 = 2 −
√
3.
To ﬁnd eigenvectors, consider this system.
−x · b1 +
b2
= 0
−x · b2 +
b3 = 0
4 · b1 −17 · b2 + (8 −x) · b3 = 0
Substituting x = λ1 = 4 gives the system
−4 · b1 +
b2
= 0
−4 · b2 +
b3 = 0
4 · b1 −17 · b2 + 4 · b3 = 0
ρ1+ρ3
−→
−4 · b1 +
b2
= 0
−4 · b2 +
b3 = 0
−16 · b2 + 4 · b3 = 0
−4ρ2+ρ3
−→
−4 · b1 +
b2
= 0
−4 · b2 + b3 = 0
0 = 0
leading to this eigenspace and eigenvector.
V4 = {


(1/16) · b3
(1/4) · b3
b3

¯¯ b2 ∈C}


1
4
16


Substituting x = λ2 = 2 +
√
3 gives the system
(−2 −
√
3) · b1 +
b2
= 0
(−2 −
√
3) · b2 +
b3 = 0
4 · b1 −
17 · b2 + (6 −
√
3) · b3 = 0
(−4/(−2−
√
3))ρ1+ρ3
−→
(−2 −
√
3) · b1 +
b2
= 0
(−2 −
√
3) · b2 +
b3 = 0
+ (−9 −4
√
3) · b2 + (6 −
√
3) · b3 = 0
(the middle coeﬃcient in the third equation equals the number (−4/(−2−
√
3))−17; ﬁnd a common
denominator of −2 −
√
3 and then rationalize the denominator by multiplying the top and bottom
of the frsction by −2 +
√
3)
((9+4
√
3)/(−2−
√
3))ρ2+ρ3
−→
(−2 −
√
3) · b1 +
b2
= 0
(−2 −
√
3) · b2 + b3 = 0
0 = 0

Answers to Exercises
193
which leads to this eigenspace and eigenvector.
V2+
√
3 = {


(1/(2 +
√
3)2) · b3
(1/(2 +
√
3)) · b3
b3

¯¯ b3 ∈C}


(1/(2 +
√
3)2)
(1/(2 +
√
3))
1


Finally, substituting x = λ3 = 2 −
√
3 gives the system
(−2 +
√
3) · b1 +
b2
= 0
(−2 +
√
3) · b2 +
b3 = 0
4 · b1 −
17 · b2 + (6 +
√
3) · b3 = 0
(−4/(−2+
√
3))ρ1+ρ3
−→
(−2 +
√
3) · b1 +
b2
= 0
(−2 +
√
3) · b2 +
b3 = 0
(−9 + 4
√
3) · b2 + (6 +
√
3) · b3 = 0
((9−4
√
3)/(−2+
√
3))ρ2+ρ3
−→
(−2 +
√
3) · b1 +
b2
= 0
(−2 +
√
3) · b2 + b3 = 0
0 = 0
which gives this eigenspace and eigenvector.
V2−
√
3 = {


(1/(2 +
√
3)2) · b3
(1/(2 −
√
3)) · b3
b3

¯¯ b3 ∈C}


(1/(−2 +
√
3)2)
(1/(−2 +
√
3))
1


Five.II.3.25
With respect to the natural basis B = ⟨1, x, x2⟩the matrix representation is this.
RepB,B(t) =


5
6
2
0
−1
−8
1
0
−2


Thus the characteristic equation
0 =


5 −x
6
2
0
−1 −x
−8
1
0
−2 −x

= (5 −x)(−1 −x)(−2 −x) −48 −2 · (−1 −x)
is 0 = −x3 + 2x2 + 15x −36 = −1 · (x + 4)(x −3)2. To ﬁnd the associated eigenvectors, consider this
system.
(5 −x) · b1 +
6 · b2 +
2 · b3 = 0
(−1 −x) · b2 −
8 · b3 = 0
b1
+ (−2 −x) · b3 = 0
Plugging in x = λ1 = 4 gives
b1 +
6 · b2 + 2 · b3 = 0
−5 · b2 −8 · b3 = 0
b1
−6 · b3 = 0
−ρ1+ρ2
−→
b1 +
6 · b2 + 2 · b3 = 0
−5 · b2 −8 · b3 = 0
−6 · b2 −8 · b3 = 0
−ρ1+ρ2
−→
b1 +
6 · b2 + 2 · b3 = 0
−5 · b2 −8 · b3 = 0
−6 · b2 −8 · b3 = 0
Five.II.3.26
λ = 1,
µ
0
0
0
1
¶
and
µ
2
3
1
0
¶
, λ = −2,
µ
−1
0
1
0
¶
, λ = −1,
µ
−2
1
1
0
¶
Five.II.3.27
Fix the natural basis B = ⟨1, x, x2, x3⟩. The map’s action is 1 7→0, x 7→1, x2 7→2x, and
x3 7→3x2 and its representation is easy to compute.
T = RepB,B(d/dx) =




0
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0




B,B
We ﬁnd the eigenvalues with this computation.
0 = |T −xI| =
¯¯¯¯¯¯¯¯
−x
1
0
0
0
−x
2
0
0
0
−x
3
0
0
0
−x
¯¯¯¯¯¯¯¯
= x4
Thus the map has the single eigenvalue λ = 0. To ﬁnd the associated eigenvectors, we solve




0
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0




B,B




b1
b2
b3
b4




B
= 0 ·




b1
b2
b3
b4




B
=⇒
b2 = 0, b3 = 0, b4 = 0

194
Linear Algebra, by Hefferon
to get this eigenspace.
{




b1
0
0
0




B
¯¯ b1 ∈C} = {b1 + 0 · x + 0 · x2 + 0 · x3 ¯¯ b1 ∈C} = {b1
¯¯ b1 ∈C}
Five.II.3.28
The determinant of the triangular matrix T −xI is the product down the diagonal, and
so it factors into the product of the terms ti,i −x.
Five.II.3.29
Just expand the determinant of T −xI.
¯¯¯¯
a −x
c
b
d −x
¯¯¯¯ = (a −x)(d −x) −bc = x2 + (−a −d) · x + (ad −bc)
Five.II.3.30
Any two representations of that transformation are similar, and similar matrices have the
same characteristic polynomial.
Five.II.3.31
(a) Yes, use λ = 1 and the identity map.
(b) Yes, use the transformation that multiplies by λ.
Five.II.3.32
If t(⃗v) = λ · ⃗v then ⃗v 7→⃗0 under the map t −λ · id.
Five.II.3.33
The characteristic equation
0 =
¯¯¯¯
a −x
b
c
d −x
¯¯¯¯ = (a −x)(d −x) −bc
simpliﬁes to x2 + (−a −d) · x + (ad −bc). Checking that the values x = a + b and x = a −c satisfy the
equation (under the a + b = c + d condition) is routine.
Five.II.3.34
Consider an eigenspace Vλ. Any ⃗w ∈Vλ is the image ⃗w = λ · ⃗v of some ⃗v ∈Vλ (namely,
⃗v = (1/λ)· ⃗w). Thus, on Vλ (which is a nontrivial subspace) the action of t−1 is t−1(⃗w) = ⃗v = (1/λ)· ⃗w,
and so 1/λ is an eigenvalue of t−1.
Five.II.3.35
(a) We have (cT + dI)⃗v = cT⃗v + dI⃗v = cλ⃗v + d⃗v = (cλ + d) · ⃗v.
(b) Suppose that S = PTP −1 is diagonal.
Then P(cT + dI)P −1 = P(cT)P −1 + P(dI)P −1 =
cPTP −1 + dI = cS + dI is also diagonal.
Five.II.3.36
The scalar λ is an eigenvalue if and only if the transformation t −λid is singular. A
transformation is singular if and only if it is not an isomorphism (that is, a transformation is an
isomorphism if and only if it is nonsingular).
Five.II.3.37
(a) Where the eigenvalue λ is associated with the eigenvector ⃗x then Ak⃗x = A · · · A⃗x =
Ak−1λ⃗x = λAk−1⃗x = · · · = λk⃗x. (The full details can be put in by doing induction on k.)
(b) The eigenvector associated wih λ might not be an eigenvector associated with µ.
Five.II.3.38
No. These are two same-sized, equal rank, matrices with diﬀerent eigenvalues.
µ
1
0
0
1
¶
µ
1
0
0
2
¶
Five.II.3.39
The characteristic polynomial has an odd power and so has at least one real root.
Five.II.3.40
The characteristic polynomial x3 −5x2 + 6x has distinct roots λ1 = 0, λ2 = −2, and
λ3 = −3. Thus the matrix can be diagonalized into this form.


0
0
0
0
−2
0
0
0
−3


Five.II.3.41
We must show that it is one-to-one and onto, and that it respects the operations of matrix
addition and scalar multiplication.
To show that it is one-to-one, suppose that tP (T) = tP (S), that is, suppose that PTP −1 = PSP −1,
and note that multiplying both sides on the left by P −1 and on the right by P gives that T = S. To
show that it is onto, consider S ∈Mn×n and observe that S = tP (P −1SP).
The map tP preserves matrix addition since tP (T + S) = P(T + S)P −1 = (PT + PS)P −1 =
PTP −1 + PSP −1 = tP (T + S) follows from properties of matrix multiplication and addition that we
have seen. Scalar multiplication is similar: tP (cT) = P(c · T)P −1 = c · (PTP −1) = c · tP (T).
Five.II.3.42
This is how the answer was given in the cited source. If the argument of the characteristic
function of A is set equal to c, adding the ﬁrst (n −1) rows (columns) to the nth row (column) yields
a determinant whose nth row (column) is zero. Thus c is a characteristic root of A.

Answers to Exercises
195
Subsection Five.III.1: Self-Composition
Five.III.1.8
For the zero transformation, no matter what the space, the chain of rangespaces is V ⊃
{⃗0} = {⃗0} = · · · and the chain of nullspaces is {⃗0} ⊂V = V = · · · . For the identity transformation
the chains are V = V = V = · · · and {⃗0} = {⃗0} = · · · .
Five.III.1.9
(a) Iterating t0 twice a + bx + cx2 7→b + cx2 7→cx2 gives
a + bx + cx2
t2
0
7−→cx2
and any higher power is the same map. Thus, while R(t0) is the space of quadratic polynomials
with no linear term {p + rx2 ¯¯ p, r ∈C}, and R(t2
0) is the space of purely-quadratic polynomials
{rx2 ¯¯ r ∈C}, this is where the chain stabilizes R∞(t0) = {rx2 ¯¯ n ∈C}. As for nullspaces, N (t0)
is the space of purely-linear quadratic polynomials {qx
¯¯ q ∈C}, and N (t2
0) is the space of quadratic
polynomials with no x2 term {p + qx
¯¯ p, q ∈C}, and this is the end N∞(t0) = N (t2
0).
(b) The second power
µ
a
b
¶
t1
7−→
µ
0
a
¶
t1
7−→
µ
0
0
¶
is the zero map. Consequently, the chain of rangespaces
R2 ⊃{
µ
0
p
¶ ¯¯ p ∈C} ⊃{⃗0 } = · · ·
and the chain of nullspaces
{⃗0 } ⊂{
µ
q
0
¶ ¯¯ q ∈C} ⊂R2 = · · ·
each has length two. The generalized rangespace is the trivial subspace and the generalized nullspace
is the entire space.
(c) Iterates of this map cycle around
a + bx + cx2
t2
7−→b + cx + ax2
t2
7−→c + ax + bx2
t2
7−→a + bx + cx2 · · ·
and the chains of rangespaces and nullspaces are trivial.
P2 = P2 = · · ·
{⃗0 } = {⃗0 } = · · ·
Thus, obviously, generalized spaces are R∞(t2) = P2 and N∞(t2) = {⃗0 }.
(d) We have


a
b
c

7→


a
a
b

7→


a
a
a

7→


a
a
a

7→· · ·
and so the chain of rangespaces
R3 ⊃{


p
p
r

¯¯ p, r ∈C} ⊃{


p
p
p

¯¯ p ∈C} = · · ·
and the chain of nullspaces
{⃗0 } ⊂{


0
0
r

¯¯ r ∈C} ⊂{


0
q
r

¯¯ q, r ∈C} = · · ·
each has length two. The generalized spaces are the ﬁnal ones shown above in each chain.
Five.III.1.10
Each maps x 7→t(t(t(x))).
Five.III.1.11
Recall that if W is a subspace of V then any basis BW for W can be enlarged to make
a basis BV for V . From this the ﬁrst sentence is immediate. The second sentence is also not hard: W
is the span of BW and if W is a proper subspace then V is not the span of BW , and so BV must have
at least one vector more than does BW .
Five.III.1.12
It is both ‘if’ and ‘only if’. We have seen earlier that a linear map is nonsingular if and
only if it preserves dimension, that is, if the dimension of its range equals the dimension of its domain.
With a transformation t: V →V that means that the map is nonsingular if and only if it is onto:
R(t) = V (and thus R(t2) = V , etc).

196
Linear Algebra, by Hefferon
Five.III.1.13
The nullspaces form chains because because if ⃗v ∈N (tj) then tj(⃗v) = ⃗0 and tj+1(⃗v) =
t( tj(⃗v) ) = t(⃗0) = ⃗0 and so ⃗v ∈N (tj+1).
Now, the “further” property for nullspaces follows from that fact that it holds for rangespaces,
along with the prior exercise. Because the dimension of R(tj) plus the dimension of N (tj) equals the
dimension n of the starting space V , when the dimensions of the rangespaces stop decreasing, so do
the dimensions of the nullspaces. The prior exercise shows that from this point k on, the containments
in the chain are not proper — the nullspaces are equal.
Five.III.1.14
(Of course, many examples are correct, but here is one.) An example is the shift operator
on triples of reals (x, y, z) 7→(0, x, y). The nullspace is all triples that start with two zeros. The map
stabilizes after three iterations.
Five.III.1.15
The diﬀerentiation operator d/dx: P1 →P1 has the same rangespace as nullspace. For
an example of where they are disjoint — except for the zero vector — consider an identity map (or any
nonsingular map).
Subsection Five.III.2: Strings
Five.III.2.17
Three. It is at least three because ℓ2( (1, 1, 1) ) = (0, 0, 1) ̸= ⃗0. It is at most three because
(x, y, z) 7→(0, x, y) 7→(0, 0, x) 7→(0, 0, 0).
Five.III.2.18
(a) The domain has dimension four. The map’s action is that any vector in the space
c1·⃗β1+c2·⃗β2+c3·⃗β3+c4·⃗β4 is sent to c1·⃗β2+c2·⃗0+c3·⃗β4+c4·⃗0 = c1·⃗β3+c3·⃗β4. The ﬁrst application
of the map sends two basis vectors ⃗β2 and ⃗β4 to zero, and therefore the nullspace has dimension two
and the rangespace has dimension two. With a second application, all four basis vectors are sent to
zero and so the nullspace of the second power has dimension four while the rangespace of the second
power has dimension zero. Thus the index of nilpotency is two. This is the canonical form.




0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0




(b) The dimension of the domain of this map is six.
For the ﬁrst power the dimension of the
nullspace is four and the dimension of the rangespace is two. For the second power the dimension of
the nullspace is ﬁve and the dimension of the rangespace is one. Then the third iteration results in
a nullspace of dimension six and a rangespace of dimension zero. The index of nilpotency is three,
and this is the canonical form.








0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0








(c) The dimension of the domain is three, and the index of nilpotency is three. The ﬁrst power’s
null space has dimension one and its range space has dimension two. The second power’s null space
has dimension two and its range space has dimension one. Finally, the third power’s null space has
dimension three and its range space has dimension zero. Here is the canonical form matrix.


0
0
0
1
0
0
0
1
0


Five.III.2.19
By Lemma 1.3 the nullity has grown as large as possible by the n-th iteration where n
is the dimension of the domain. Thus, for the 2×2 matrices, we need only check whether the square
is the zero matrix. For the 3×3 matrices, we need only check the cube.
(a) Yes, this matrix is nilpotent because its square is the zero matrix.
(b) No, the square is not the zero matrix.
µ
3
1
1
3
¶2
=
µ
10
6
6
10
¶

Answers to Exercises
197
(c) Yes, the cube is the zero matrix. In fact, the square is zero.
(d) No, the third power is not the zero matrix.


1
1
4
3
0
−1
5
2
7


3
=


206
86
304
26
8
26
438
180
634


(e) Yes, the cube of this matrix is the zero matrix.
Another way to see that the second and fourth matrices are not nilpotent is to note that they are
nonsingular.
Five.III.2.20
The table os calculations
p
N p
N (N p)
1






0
1
1
0
1
0
0
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0






{






r
u
−u −v
u
v






¯¯ r, u, v ∈C}
2






0
0
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0






{






r
s
−u −v
u
v






¯¯ r, s, u, v ∈C}
2
–zero matrix–
C5
gives these requirements of the string basis: three basis vectors are sent directly to zero, one more
basis vector is sent to zero by a second application, and the ﬁnal basis vector is sent to zero by a third
application. Thus, the string basis has this form.
⃗β1 7→⃗β2 7→⃗β3 7→⃗0
⃗β4 7→⃗0
⃗β5 7→⃗0
From that the canonical form is immediate.






0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0






Five.III.2.21
(a) The canonical form has a 3×3 block and a 2×2 block






0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
1
0






corresponding to the length three string and the length two string in the basis.
(b) Assume that N is the representation of the underlying map with respect to the standard basis.
Let B be the basis to which we will change. By the similarity diagram
C2
w.r.t. E2
n
−−−−→
N
C2
w.r.t. E2
id
yP
id
yP
C2
w.r.t. B
n
−−−−→C2
w.r.t. B
we have that the canonical form matrix is PNP −1 where
P −1 = RepB,E5(id) =






1
0
0
0
0
0
1
0
1
0
1
0
1
0
0
0
0
1
1
1
0
0
0
0
1







198
Linear Algebra, by Hefferon
and P is the inverse of that.
P = RepE5,B(id) = (P −1)−1 =






1
0
0
0
0
−1
1
1
−1
1
−1
0
1
0
0
1
0
−1
1
−1
0
0
0
0
1






(c) The calculation to check this is routine.
Five.III.2.22
(a) The calculation
p
N p
N (N p)
1
µ1/2
−1/2
1/2
−1/2
¶
{
µu
u
¶ ¯¯ u ∈C}
2
–zero matrix–
C2
shows that any map represented by the matrix must act on the string basis in this way
⃗β1 7→⃗β2 7→⃗0
because the nullspace after one application has dimension one and exactly one basis vector, ⃗β2, is
sent to zero. Therefore, this representation with respect to ⟨⃗β1, ⃗β2⟩is the canonical form.
µ0
0
1
0
¶
(b) The calculation here is similar to the prior one.
p
N p
N (N p)
1


0
0
0
0
−1
1
0
−1
1


{


u
v
v

¯¯ u, v ∈C}
2
–zero matrix–
C3
The table shows that the string basis is of the form
⃗β1 7→⃗β2 7→⃗0
⃗β3 7→⃗0
because the nullspace after one application of the map has dimension two — ⃗β2 and ⃗β3 are both
sent to zero — and one more iteration results in the additional vector being brought to zero.
(c) The calculation
p
N p
N (N p)
1


−1
1
−1
1
0
1
1
−1
1


{


u
0
−u

¯¯ u ∈C}
2


1
0
1
0
0
0
−1
0
−1


{


u
v
−u

¯¯ u, v ∈C}
3
–zero matrix–
C3
shows that any map represented by this basis must act on a string basis in this way.
⃗β1 7→⃗β2 7→⃗β3 7→⃗0
Therefore, this is the canonical form.


0
0
0
1
0
0
0
1
0


Five.III.2.23
A couple of examples
µ
0
0
1
0
¶ µ
a
b
c
d
¶
=
µ
0
0
a
b
¶


0
0
0
1
0
0
0
1
0




a
b
c
d
e
f
g
h
i

=


0
0
0
a
b
c
d
e
f


suggest that left multiplication by a block of subdiagonal ones shifts the rows of a matrix downward.
Distinct blocks




0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0








a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p



=




0
0
0
0
a
b
c
d
0
0
0
0
i
j
k
l




act to shift down distinct parts of the matrix.
Right multiplication does an analgous thing to columns. See Exercise 17.

Answers to Exercises
199
Five.III.2.24
Yes.
Generalize the last sentence in Example 2.9.
As to the index, that same last
sentence shows that the index of the new matrix is less than or equal to the index of ˆN, and reversing
the roles of the two matrices gives inequality in the other direction.
Another answer to this question is to show that a matrix is nilpotent if and only if any associated
map is nilpotent, and with the same index. Then, because similar matrices represent the same map,
the conclusion follows. This is Exercise 30 below.
Five.III.2.25
Observe that a canonical form nilpotent matrix has only zero eigenvalues; e.g., the
determinant of this lower-triangular matrix


−x
0
0
1
−x
0
0
1
−x


is (−x)3, the only root of which is zero. But similar matrices have the same eigenvalues and every
nilpotent matrix is similar to one in canonical form.
Another way to see this is to observe that a nilpotent matrix sends all vectors to zero after some
number of iterations, but that conﬂicts with an action on an eigenspace ⃗v 7→λ⃗v unless λ is zero.
Five.III.2.26
No, by Lemma 1.3 for a map on a two-dimensional space, the nullity has grown as large
as possible by the second iteration.
Five.III.2.27
The index of nilpotency of a transformation can be zero only when the vector starting
the string must be ⃗0, that is, only when V is a trivial space.
Five.III.2.28
(a) Any member ⃗w of the span can be written as a linear combination ⃗w = c0 ·⃗v + c1 ·
t(⃗v) + · · · + ck−1 · tk−1(⃗v). But then, by the linearity of the map, t(⃗w) = c0 · t(⃗v) + c1 · t2(⃗v) + · · · +
ck−2 · tk−1(⃗v) + ck−1 ·⃗0 is also in the span.
(b) The operation in the prior item, when iterated k times, will result in a linear combination of
zeros.
(c) If ⃗v = ⃗0 then the set is empty and so is linearly independent by deﬁnition. Otherwise write
c1⃗v + · · · + ck−1tk−1(⃗v) = ⃗0 and apply tk−1 to both sides. The right side gives ⃗0 while the left side
gives c1tk−1(⃗v); conclude that c1 = 0. Continue in this way by applying tk−2 to both sides, etc.
(d) Of course, t acts on the span by acting on this basis as a single, k-long, t-string.









0
0
0
0
. . .
0
0
1
0
0
0
. . .
0
0
0
1
0
0
. . .
0
0
0
0
1
0
0
0
...
0
0
0
0
1
0









Five.III.2.29
We must check that B ∪ˆC ∪{⃗v1, . . . ,⃗vj} is linearly independent where B is a t-string
basis for R(t), where ˆC is a basis for N (t), and where t(⃗v1) = ⃗β1, . . . , t(⃗vi = ⃗βi. Write
⃗0 = c1,−1⃗v1 + c1,0⃗β1 + c1,1t(⃗β1) + · · · + c1,h1th1(⃗⃗β1) + c2,−1⃗v2 + · · · + cj,hithi(⃗βi)
and apply t.
⃗0 = c1,−1⃗β1 + c1,0t(⃗β1) + · · · + c1,h1−1th1(⃗⃗β1) + c1,h1⃗0 + c2,−1⃗β2 + · · · + ci,hi−1thi(⃗βi) + ci,hi⃗0
Conclude that the coeﬃcients c1,−1, . . . , c1,hi−1, c2,−1, . . . , ci,hi−1 are all zero as B ∪ˆC is a basis.
Substitute back into the ﬁrst displayed equation to conclude that the remaining coeﬃcients are zero
also.
Five.III.2.30
For any basis B, a transformation n is nilpotent if and only if N = RepB,B(n) is a
nilpotent matrix. This is because only the zero matrix represents the zero map and so nj is the zero
map if and only if N j is the zero matrix.
Five.III.2.31
It can be of any size greater than or equal to one. To have a transformation that is
nilpotent of index four, whose cube has rangespace of dimension k, take a vector space, a basis for

200
Linear Algebra, by Hefferon
that space, and a transformation that acts on that basis in this way.
⃗β1
7→
⃗β2
7→
⃗β3
7→⃗β4 7→⃗0
⃗β5
7→
⃗β6
7→
⃗β7
7→⃗β8 7→⃗0
...
⃗β4k−3 7→⃗β4k−2 7→⃗β4k−1 7→⃗β4k 7→⃗0
...
–possibly other, shorter, strings–
So the dimension of the rangespace of T 3 can be as large as desired. The smallest that it can be is
one — there must be at least one string or else the map’s index of nilpotency would not be four.
Five.III.2.32
These two have only zero for eigenvalues
µ
0
0
0
0
¶
µ
0
0
1
0
¶
but are not similar (they have diﬀerent canonical representatives, namely, themselves).
Five.III.2.33
A simple reordering of the string basis will do. For instance, a map that is assoicated
with this string basis
⃗β1 7→⃗β2 7→⃗0
is represented with respect to B = ⟨⃗β1, ⃗β2⟩by this matrix
µ
0
0
1
0
¶
but is represented with respect to B = ⟨⃗β2, ⃗β1⟩in this way.
µ
0
1
0
0
¶
Five.III.2.34
Let t: V →V be the transformation. If rank(t) = nullity(t) then the equation rank(t)+
nullity(t) = dim(V ) shows that dim(V ) is even.
Five.III.2.35
For the matrices to be nilpotent they must be square. For them to commute they must
be the same size. Thus their product and sum are deﬁned.
Call the matrices A and B. To see that AB is nilpotent, multiply (AB)2 = ABAB = AABB =
A2B2, and (AB)3 = A3B3, etc., and, as A is nilpotent, that product is eventually zero.
The sum is similar; use the Binomial Theorem.
Five.III.2.36
Some experimentation gives the idea for the proof. Expansion of the second power
t2
S(T) = S(ST −TS) −(ST −TS)S = S2 −2STS + TS2
the third power
t3
S(T) = S(S2 −2STS + TS2) −(S2 −2STS + TS2)S
= S3T −3S2TS + 3STS2 −TS3
and the fourth power
t4
S(T) = S(S3T −3S2TS + 3STS2 −TS3) −(S3T −3S2TS + 3STS2 −TS3)S
= S4T −4S3TS + 6S2TS2 −4STS3 + TS4
suggest that the expansions follow the Binomial Theorem. Verifying this by induction on the power
of tS is routine. This answers the question because, where the index of nilpotency of S is k, in the
expansion of t2k
S
t2k
S (T) =
X
0≤i≤2k
(−1)i
µ2k
i
¶
SiTS2k−i
for any i at least one of the Si and S2k−i has a power higher than k, and so the term gives the zero
matrix.
Five.III.2.37
Use the geometric series: I −N k+1 = (I −N)(N k + N k−1 + · · · + I). If N k+1 is the zero
matrix then we have a right inverse for I −N. It is also a left inverse.
This statement is not ‘only if’ since µ1
0
0
1
¶
−
µ−1
0
0
−1
¶
is invertible.

Answers to Exercises
201
Subsection Five.IV.1: Polynomials of Maps and Matrices
Five.IV.1.13
For each, the minimal polynomial must have a leading coeﬃcient of 1 and Theorem 1.8,
the Cayley-Hamilton Theorem, says that the minimal polynomial must contain the same linear factors
as the characteristic polynomial, although possibly of lower degree but not of zero degree.
(a) The possibilities are m1(x) = x −3, m2(x) = (x −3)2, m3(x) = (x −3)3, and m4(x) = (x −3)4.
Note that the 8 has been dropped because a minimal polynomial must have a leading coeﬃcient of
one. The ﬁrst is a degree one polynomial, the second is degree two, the third is degree three, and
the fourth is degree four.
(b) The possibilities are m1(x) = (x+1)(x−4), m2(x) = (x+1)2(x−4), and m3(x) = (x+1)3(x−4).
The ﬁrst is a quadratic polynomial, that is, it has degree two. The second has degree three, and the
third has degree four.
(c) We have m1(x) = (x −2)(x −5), m2(x) = (x −2)2(x −5), m3(x) = (x −2)(x −5)2, and
m4(x) = (x −2)2(x −5)2. They are polynomials of degree two, three, three, and four.
(d) The possiblities are m1(x) = (x + 3)(x −1)(x −2), m2(x) = (x + 3)2(x −1)(x −2), m3(x) =
(x + 3)(x −1)(x −2)2, and m4(x) = (x + 3)2(x −1)(x −2)2. The degree of m1 is three, the degree
of m2 is four, the degree of m3 is four, and the degree of m4 is ﬁve.
Five.IV.1.14
In each case we will use the method of Example 1.12.
(a) Because T is triangular, T −xI is also triangular
T −xI =


3 −x
0
0
1
3 −x
0
0
0
4 −x


the characteristic polynomial is easy c(x) = |T −xI| = (3−x)2(4−x) = −1·(x−3)2(x−4). There are
only two possibilities for the minimal polynomial, m1(x) = (x−3)(x−4) and m2(x) = (x−3)2(x−4).
(Note that the characteristic polynomial has a negative sign but the minimal polynomial does not
since it must have a leading coeﬃcient of one). Because m1(T) is not the zero matrix
(T −3I)(T −4I) =


0
0
0
1
0
0
0
0
1




−1
0
0
1
−1
0
0
0
0

=


0
0
0
−1
0
0
0
0
0


the minimal polynomial is m(x) = m2(x).
(T −3I)2(T −4I) = (T −3I) ·
¡
(T −3I)(T −4I)
¢
=


0
0
0
1
0
0
0
0
1




0
0
0
−1
0
0
0
0
0

=


0
0
0
0
0
0
0
0
0


(b) As in the prior item, the fact that the matrix is triangular makes computation of the characteristic
polynomial easy.
c(x) = |T −xI| =
¯¯¯¯¯¯
3 −x
0
0
1
3 −x
0
0
0
3 −x
¯¯¯¯¯¯
= (3 −x)3 = −1 · (x −3)3
There are three possibilities for the minimal polynomial m1(x) = (x −3), m2(x) = (x −3)2, and
m3(x) = (x −3)3. We settle the question by computing m1(T)
T −3I =


0
0
0
1
0
0
0
0
0


and m2(T).
(T −3I)2 =


0
0
0
1
0
0
0
0
0




0
0
0
1
0
0
0
0
0

=


0
0
0
0
0
0
0
0
0


Because m2(T) is the zero matrix, m2(x) is the minimal polynomial.
(c) Again, the matrix is triangular.
c(x) = |T −xI| =
¯¯¯¯¯¯
3 −x
0
0
1
3 −x
0
0
1
3 −x
¯¯¯¯¯¯
= (3 −x)3 = −1 · (x −3)3

202
Linear Algebra, by Hefferon
Again, there are three possibilities for the minimal polynomial m1(x) = (x −3), m2(x) = (x −3)2,
and m3(x) = (x −3)3. We compute m1(T)
T −3I =


0
0
0
1
0
0
0
1
0


and m2(T)
(T −3I)2 =


0
0
0
1
0
0
0
1
0




0
0
0
1
0
0
0
1
0

=


0
0
0
0
0
0
1
0
0


and m3(T).
(T −3I)3 = (T −3I)2(T −3I) =


0
0
0
0
0
0
1
0
0




0
0
0
1
0
0
0
1
0

=


0
0
0
0
0
0
0
0
0


Therefore, the minimal polynomial is m(x) = m3(x) = (x −3)3.
(d) This case is also triangular, here upper triangular.
c(x) = |T −xI| =
¯¯¯¯¯¯
2 −x
0
1
0
6 −x
2
0
0
2 −x
¯¯¯¯¯¯
= (2 −x)2(6 −x) = −(x −2)2(x −6)
There are two possibilities for the minimal polynomial, m1(x) = (x −2)(x −6) and m2(x) =
(x −2)2(x −6). Computation shows that the minimal polynomial isn’t m1(x).
(T −2I)(T −6I) =


0
0
1
0
4
2
0
0
0




−4
0
1
0
0
2
0
0
−4

=


0
0
−4
0
0
0
0
0
0


It therefore must be that m(x) = m2(x) = (x −2)2(x −6). Here is a veriﬁcation.
(T −2I)2(T −6I) = (T −2I) ·
¡
(T −2I)(T −6I)
¢
=


0
0
1
0
4
2
0
0
0




0
0
−4
0
0
0
0
0
0

=


0
0
0
0
0
0
0
0
0


(e) The characteristic polynomial is
c(x) = |T −xI| =
¯¯¯¯¯¯
2 −x
2
1
0
6 −x
2
0
0
2 −x
¯¯¯¯¯¯
= (2 −x)2(6 −x) = −(x −2)2(x −6)
and there are two possibilities for the minimal polynomial, m1(x) = (x −2)(x −6) and m2(x) =
(x −2)2(x −6). Checking the ﬁrst one
(T −2I)(T −6I) =


0
2
1
0
4
2
0
0
0




−4
2
1
0
0
2
0
0
−4

=


0
0
0
0
0
0
0
0
0


shows that the minimal polynomial is m(x) = m1(x) = (x −2)(x −6).
(f) The characteristic polynomial is this.
c(x) = |T −xI| =
¯¯¯¯¯¯¯¯¯¯
−1 −x
4
0
0
0
0
3 −x
0
0
0
0
−4
−1 −x
0
0
3
−9
−4
2 −x
−1
1
5
4
1
4 −x
¯¯¯¯¯¯¯¯¯¯
= (x −3)3(x + 1)2
There are a number of possibilities for the minimal polynomial, listed here by ascending degree:
m1(x) = (x −3)(x + 1), m1(x) = (x −3)2(x + 1), m1(x) = (x −3)(x + 1)2, m1(x) = (x −3)3(x + 1),
m1(x) = (x −3)2(x + 1)2, and m1(x) = (x −3)3(x + 1)2. The ﬁrst one doesn’t pan out
(T −3I)(T + 1I) =






−4
4
0
0
0
0
0
0
0
0
0
−4
−4
0
0
3
−9
−4
−1
−1
1
5
4
1
1












0
4
0
0
0
0
4
0
0
0
0
−4
0
0
0
3
−9
−4
3
−1
1
5
4
1
5






=






0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−4
−4
0
−4
−4
4
4
0
4
4







Answers to Exercises
203
but the second one does.
(T −3I)2(T + 1I) = (T −3I)
¡
(T −3I)(T + 1I)
¢
=






−4
4
0
0
0
0
0
0
0
0
0
−4
−4
0
0
3
−9
−4
−1
−1
1
5
4
1
1












0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−4
−4
0
−4
−4
4
4
0
4
4






=






0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0






The minimal polynomial is m(x) = (x −3)2(x + 1).
Five.IV.1.15
Its characteristic polynomial has complex roots.
¯¯¯¯¯¯
−x
1
0
0
−x
1
1
0
−x
¯¯¯¯¯¯
= (1 −x) · (x −(−1
2 +
√
3
2 i)) · (x −(−1
2 −
√
3
2 i))
As the roots are distinct, the characteristic polynomial equals the minimal polynomial.
Five.IV.1.16
We know that Pn is a dimension n + 1 space and that the diﬀerentiation operator is
nilpotent of index n+1 (for instance, taking n = 3, P3 = {c3x3 + c2x2 + c1x + c0
¯¯ c3, . . . , c0 ∈C} and
the fourth derivative of a cubic is the zero polynomial). Represent this operator using the canonical
form for nilpotent transformations.







0
0
0
. . .
0
1
0
0
0
0
1
0
...
0
0
0
1
0







This is an (n + 1)×(n + 1) matrix with an easy characteristic polynomial, c(x) = xn+1. (Remark: this
matrix is RepB,B(d/dx) where B = ⟨xn, nxn−1, n(n−1)xn−2, . . . , n!⟩.) To ﬁnd the minimal polynomial
as in Example 1.12 we consider the powers of T −0I = T. But, of course, the ﬁrst power of T that is
the zero matrix is the power n + 1. So the minimal polynomial is also xn+1.
Five.IV.1.17
Call the matrix T and suppose that it is n×n. Because T is triangular, and so T −xI
is triangular, the characteristic polynomial is c(x) = (x −λ)n. To see that the minimal polynomial is
the same, consider T −λI.







0
0
0
. . .
0
1
0
0
. . .
0
0
1
0
...
0
0
. . .
1
0







Recognize it as the canonical form for a transformation that is nilpotent of degree n; the power (T −λI)j
is zero ﬁrst when j is n.
Five.IV.1.18
The n = 3 case provides a hint. A natural basis for P3 is B = ⟨1, x, x2, x3⟩. The action
of the transformation is
1 7→1
x 7→x + 1
x2 7→x2 + 2x + 1
x3 7→x3 + 3x2 + 3x + 1
and so the representation RepB,B(t) is this upper triangular matrix.




1
1
1
1
0
1
2
3
0
0
1
3
0
0
0
1





204
Linear Algebra, by Hefferon
Because it is triangular, the fact that the characteristic polynomial is c(x) = (x −1)4 is clear. For the
minimal polynomial, the candidates are m1(x) = (x −1),
T −1I =




0
1
1
1
0
0
2
3
0
0
0
3
0
0
0
0




m2(x) = (x −1)2,
(T −1I)2 =




0
0
2
6
0
0
0
6
0
0
0
0
0
0
0
0




m3(x) = (x −1)3,
(T −1I)3 =




0
0
0
6
0
0
0
0
0
0
0
0
0
0
0
0




and m4(x) = (x −1)4. Because m1, m2, and m3 are not right, m4 must be right, as is easily veriﬁed.
In the case of a general n, the representation is an upper triangular matrix with ones on the diagonal.
Thus the characteristic polynomial is c(x) = (x−1)n+1. One way to verify that the minimal polynomial
equals the characteristic polynomial is argue something like this: say that an upper triangular matrix
is 0-upper triangular if there are nonzero entries on the diagonal, that it is 1-upper triangular if the
diagonal contains only zeroes and there are nonzero entries just above the diagonal, etc. As the above
example illustrates, an induction argument will show that, where T has only nonnegative entries, T j
is j-upper triangular. That argument is left to the reader.
Five.IV.1.19
The map twice is the same as the map once: π ◦π = π, that is, π2 = π and so the
minimal polynomial is of degree at most two since m(x) = x2 −x will do. The fact that no linear
polynomial will do follows from applying the maps on the left and right side of c1 · π + c0 · id = z
(where z is the zero map) to these two vectors.


0
0
1




1
0
0


Thus the minimal polynomial is m.
Five.IV.1.20
This is one answer.


0
0
0
1
0
0
0
0
0


Five.IV.1.21
The x must be a scalar, not a matrix.
Five.IV.1.22
The characteristic polynomial of
T =
µ
a
b
c
d
¶
is (a −x)(d −x) −bc = x2 −(a + d)x + (ad −bc). Substitute
µa
b
c
d
¶2
−(a + d)
µa
b
c
d
¶
+ (ad −bc)
µ1
0
0
1
¶
=
µ
a2 + bc
ab + bd
ac + cd
bc + d2
¶
−
µ
a2 + ad
ab + bd
ac + cd
ad + d2
¶
+
µ
ad −bc
0
0
ad −bc
¶
and just check each entry sum to see that the result is the zero matrix.
Five.IV.1.23
By the Cayley-Hamilton theorem the degree of the minimal polynomial is less than or
equal to the degree of the characteristic polynomial, n. Example 1.6 shows that n can happen.
Five.IV.1.24
Suppose that t’s only eigenvalue is zero. Then the characteristic polynomial of t is xn.
Because t satisﬁes its characteristic polynomial, it is a nilpotent map.
Five.IV.1.25
A minimal polynomial must have leading coeﬃcient 1, and so if the minimal polynomial
of a map or matrix were to be a degree zero polynomial then it would be m(x) = 1. But the identity
map or matrix equals the zero map or matrix only on a trivial vector space.

Answers to Exercises
205
So in the nontrivial case the minimal polynomial must be of degree at least one. A zero map or
matrix has minimal polynomial m(x) = x, and an identity map or matrix has minimal polynomial
m(x) = x −1.
Five.IV.1.26
The polynomial can be read geometrically to say “a 60◦rotation minus two rotations of
30◦equals the identity.”
Five.IV.1.27
For a diagonal matrix
T =





t1,1
0
0
t2,2
...
tn,n





the characteristic polynomial is (t1,1 −x)(t2,2 −x) · · · (tn,n −x). Of course, some of those factors may
be repeated, e.g., the matrix might have t1,1 = t2,2. For instance, the characteristic polynomial of
D =


3
0
0
0
3
0
0
0
1


is (3 −x)2(1 −x) = −1 · (x −3)2(x −1).
To form the minimal polynomial, take the terms x −ti,i, throw out repeats, and multiply them
together.
For instance, the minimal polynomial of D is (x −3)(x −1).
To check this, note ﬁrst
that Theorem 1.8, the Cayley-Hamilton theorem, requires that each linear factor in the characteristic
polynomial appears at least once in the minimal polynomial. One way to check the other direction —
that in the case of a diagonal matrix, each linear factor need appear at most once — is to use a matrix
argument. A diagonal matrix, multiplying from the left, rescales rows by the entry on the diagonal.
But in a product (T −t1,1I) · · · , even without any repeat factors, every row is zero in at least one of
the factors.
For instance, in the product
(D −3I)(D −1I) = (D −3I)(D −1I)I =


0
0
0
0
0
0
0
0
−2




2
0
0
0
2
0
0
0
0




1
0
0
0
1
0
0
0
1


because the ﬁrst and second rows of the ﬁrst matrix D −3I are zero, the entire product will have a
ﬁrst row and second row that are zero. And because the third row of the middle matrix D −1I is zero,
the entire product has a third row of zero.
Five.IV.1.28
This subsection starts with the observation that the powers of a linear transformation
cannot climb forever without a “repeat”, that is, that for some power n there is a linear relationship
cn · tn + · · · + c1 · t + c0 · id = z where z is the zero transformation. The deﬁnition of projection is
that for such a map one linear relationship is quadratic, t2 −t = z. To ﬁnish, we need only consider
whether this relationship might not be minimal, that is, are there projections for which the minimal
polynomial is constant or linear?
For the minimal polynomial to be constant, the map would have to satisfy that c0 · id = z, where
c0 = 1 since the leading coeﬃcient of a minimal polynomial is 1. This is only satisﬁed by the zero
transformation on a trivial space. This is indeed a projection, but not a very interesting one.
For the minimal polynomial of a transformation to be linear would give c1 · t + c0 · id = z where
c1 = 1.
This equation gives t = −c0 · id.
Coupling it with the requirement that t2 = t gives
t2 = (−c0)2 · id = −c0 · id, which gives that c0 = 0 and t is the zero transformation or that c0 = 1 and
t is the identity.
Thus, except in the cases where the projection is a zero map or an identity map, the minimal
polynomial is m(x) = x2 −x.
Five.IV.1.29
(a) This is a property of functions in general, not just of linear functions. Suppose
that f and g are one-to-one functions such that f ◦g is deﬁned. Let f ◦g(x1) = f ◦g(x2), so that
f(g(x1)) = f(g(x2)). Because f is one-to-one this implies that g(x1) = g(x2). Because g is also
one-to-one, this in turn implies that x1 = x2. Thus, in summary, f ◦g(x1) = f ◦g(x2) implies that
x1 = x2 and so f ◦g is one-to-one.
(b) If the linear map h is not one-to-one then there are unequal vectors ⃗v1, ⃗v2 that map to the same
value h(⃗v1) = h(⃗v2). Because h is linear, we have ⃗0 = h(⃗v1) −h(⃗v2) = h(⃗v1 −⃗v2) and so ⃗v1 −⃗v2 is
a nonzero vector from the domain that is mapped by h to the zero vector of the codomain (⃗v1 −⃗v2
does not equal the zero vector of the domain because ⃗v1 does not equal ⃗v2).

206
Linear Algebra, by Hefferon
(c) The minimal polynomial m(t) sends every vector in the domain to zero and so it is not one-to-one
(except in a trivial space, which we ignore). By the ﬁrst item of this question, since the composition
m(t) is not one-to-one, at least one of the components t −λi is not one-to-one. By the second item,
t −λi has a nontrivial nullspace. Because (t −λi)(⃗v) = ⃗0 holds if and only if t(⃗v) = λi · ⃗v, the
prior sentence gives that λi is an eigenvalue (recall that the deﬁnition of eigenvalue requires that
the relationship hold for at least one nonzero ⃗v).
Five.IV.1.30
This is false. The natural example of a non-diagonalizable transformation works here.
Consider the transformation of C2 represented with respect to the standard basis by this matrix.
N =
µ
0
1
0
0
¶
The characteristic polynomial is c(x) = x2. Thus the minimal polynomial is either m1(x) = x or
m2(x) = x2. The ﬁrst is not right since N −0 · I is not the zero matrix, thus in this example the
minimal polynomial has degree equal to the dimension of the underlying space, and, as mentioned, we
know this matrix is not diagonalizable because it is nilpotent.
Five.IV.1.31
Let A and B be similar A = PBP −1. From the facts that
An = (PBP −1)n = (PBP −1)(PBP −1) · · · (PBP −1)
= PB(P −1P)B(P −1P) · · · (P −1P)BP −1 = PBnP −1
and c · A = c · (PBP −1) = P(c · B)P −1 follows the required fact that for any polynomial function f
we have f(A) = P f(B) P −1. For instance, if f(x) = x2 + 2x + 3 then
A2 + 2A + 3I = (PBP −1)2 + 2 · PBP −1 + 3 · I
= (PBP −1)(PBP −1) + P(2B)P −1 + 3 · PP −1 = P(B2 + 2B + 3I)P −1
shows that f(A) is similar to f(B).
(a) Taking f to be a linear polynomial we have that A −xI is similar to B −xI. Similar matrices
have equal determinants (since |A| = |PBP −1| = |P| · |B| · |P −1| = 1 · |B| · 1 = |B|). Thus the
characteristic polynomials are equal.
(b) As P and P −1 are invertible, f(A) is the zero matrix when, and only when, f(B) is the zero
matrix.
(c) They cannot be similar since they don’t have the same characteristic polynomial. The charac-
teristic polynomial of the ﬁrst one is x2 −4x −3 while the characteristic polynomial of the second
is x2 −5x + 5.
Five.IV.1.32
Suppose that m(x) = xn + mn−1xn−1 + · · · + m1x + m0 is minimal for T.
(a) For the ‘if’ argument, because T n + · · · + m1T + m0I is the zero matrix we have that I =
(T n+· · ·+m1T)/(−m0) = T·(T n−1+· · ·+m1I)/(−m0) and so the matrix (−1/m0)·(T n−1+· · ·+m1I)
is the inverse of T. For ‘only if’, suppose that m0 = 0 (we put the n = 1 case aside but it is easy) so
that T n + · · · + m1T = (T n−1 + · · · + m1I)T is the zero matrix. Note that T n−1 + · · · + m1I is not
the zero matrix because the degree of the minimal polynomial is n. If T −1 exists then multiplying
both (T n−1 + · · · + m1I)T and the zero matrix from the right by T −1 gives a contradiction.
(b) If T is not invertible then the constant term in its minimal polynomial is zero. Thus,
T n + · · · + m1T = (T n−1 + · · · + m1I)T = T(T n−1 + · · · + m1I)
is the zero matrix.
Five.IV.1.33
(a) For the inductive step, assume that Lemma 1.7 is true for polynomials of degree
i, . . . , k −1 and consider a polynomial f(x) of degree k. Factor f(x) = k(x −λ1)q1 · · · (x −λℓ)qℓand
let k(x −λ1)q1−1 · · · (x −λℓ)qℓbe cn−1xn−1 + · · · + c1x + c0. Substitute:
k(t −λ1)q1 ◦· · · ◦(t −λℓ)qℓ(⃗v) = (t −λ1) ◦(t −λ1)q1 ◦· · · ◦(t −λℓ)qℓ(⃗v)
= (t −λ1) (cn−1tn−1(⃗v) + · · · + c0⃗v)
= f(t)(⃗v)
(the second equality follows from the inductive hypothesis and the third from the linearity of t).
(b) One example is to consider the squaring map s: R →R given by s(x) = x2. It is nonlinear. The
action deﬁned by the polynomial f(t) = t2 −1 changes s to f(s) = s2 −1, which is this map.
x
s2−1
7−→s ◦s(x) −1 = x4 −1
Observe that this map diﬀers from the map (s −1) ◦(s + 1); for instance, the ﬁrst map takes x = 5
to 624 while the second one takes x = 5 to 675.

Answers to Exercises
207
Five.IV.1.34
Yes. Expand down the last column to check that xn + mn−1xn−1 + · · · + m1x + m0 is
plus or minus the determinant of this.







−x
0
0
m0
0
1 −x
0
m1
0
0
1 −x
m2
...
1 −x
mn−1







Subsection Five.IV.2: Jordan Canonical Form
Five.IV.2.17
We are required to check that
µ
3
0
1
3
¶
= N + 3I = PTP −1 =
µ
1/2
1/2
−1/4
1/4
¶ µ
2
−1
1
4
¶ µ
1
−2
1
2
¶
That calculation is easy.
Five.IV.2.18
(a) The characteristic polynomial is c(x) = (x−3)2 and the minimal polynomial is the
same.
(b) The characteristic polynomial is c(x) = (x + 1)2. The minimal polynomial is m(x) = x + 1.
(c) The characteristic polynomial is c(x) = (x + (1/2))(x −2)2 and the minimal polynomial is the
same.
(d) The characteristic polynomial is c(x) = (x −3)3 The minimal polynomial is the same.
(e) The characteristic polynomial is c(x) = (x −3)4. The minimal polynomial is m(x) = (x −3)2.
(f) The characteristic polynomial is c(x) = (x+4)2(x−4)2 and the minimal polynomial is the same.
(g) The characteristic polynomial is c(x) = (x −2)2(x −3)(x −5) and the minimal polynomial is
m(x) = (x −2)(x −3)(x −5).
(h) The characteristic polynomial is c(x) = (x −2)2(x −3)(x −5) and the minimal polynomial is the
same.
Five.IV.2.19
(a) The transformation t −3 is nilpotent (that is, N∞(t −3) is the entire space) and
it acts on a string basis via two strings, ⃗β1 7→⃗β2 7→⃗β3 7→⃗β4 7→⃗0 and ⃗β5 7→⃗0. Consequently, t −3
can be represented in this canonical form.
N3 =






0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0






and therefore T is similar to this this canonical form matrix.
J3 = N3 + 3I =






3
0
0
0
0
1
3
0
0
0
0
1
3
0
0
0
0
1
3
0
0
0
0
0
3






(b) The restriction of the transformation s + 1 is nilpotent on the subspace N∞(s + 1), and the
action on a string basis is given as ⃗β1 7→⃗0. The restriction of the transformation s −2 is nilpotent
on the subspace N∞(s −2), having the action on a string basis of ⃗β2 7→⃗β3 7→⃗0 and ⃗β4 7→⃗β5 7→⃗0.
Consequently the Jordan form is this






−1
0
0
0
0
0
2
0
0
0
0
1
2
0
0
0
0
0
2
0
0
0
0
1
2






(note that the blocks are arranged with the least eigenvalue ﬁrst).

208
Linear Algebra, by Hefferon
Five.IV.2.20
For each, because many choices of basis are possible, many other answers are possible.
Of course, the calculation to check if an answer gives that PTP −1 is in Jordan form is the arbiter of
what’s correct.
(a) Here is the arrow diagram.
C3
w.r.t. E3
t
−−−−→
T
C3
w.r.t. E3
id
yP
id
yP
C3
w.r.t. B
t
−−−−→
J
C3
w.r.t. B
The matrix to move from the lower left to the upper left is this.
P −1 =
¡
RepE3,B(id)
¢−1 = RepB,E3(id) =


1
−2
0
1
0
1
−2
0
0


The matrix P to move from the upper right to the lower right is the inverse of P −1.
(b) We want this matrix and its inverse.
P −1 =


1
0
3
0
1
4
0
−2
0


(c) The concatenation of these bases for the generalized null spaces will do for the basis for the entire
space.
B−1 = ⟨






−1
0
0
1
0






,






−1
0
−1
0
1






⟩
B3 = ⟨






1
1
−1
0
0






,






0
0
0
−2
2






,






−1
−1
1
2
0






⟩
The change of basis matrices are this one and its inverse.
P −1 =






−1
−1
1
0
−1
0
0
1
0
−1
0
−1
−1
0
1
1
0
0
−2
2
0
1
0
2
0






Five.IV.2.21
The general procedure is to factor the characteristic polynomial c(x) = (x −λ1)p1(x −
λ2)p2 · · · to get the eigenvalues λ1, λ2, etc. Then, for each λi we ﬁnd a string basis for the action of the
transformation t −λi when restricted to N∞(t −λi), by computing the powers of the matrix T −λiI
and ﬁnding the associated null spaces, until these null spaces settle down (do not change), at which
point we have the generalized null space. The dimensions of those null spaces (the nullities) tell us
the action of t −λi on a string basis for the generalized null space, and so we can write the pattern of
subdiagonal ones to have Nλi. From this matrix, the Jordan block Jλi associated with λi is immediate
Jλi = Nλi + λiI. Finally, after we have done this for each eigenvalue, we put them together into the
canonical form.
(a) The characteristic polynomial of this matrix is c(x) = (−10 −x)(10 −x) + 100 = x2, so it has
only the single eigenvalue λ = 0.
power p
(T + 0 · I)p
N ((t −0)p)
nullity
1
Ã
−10
4
−25
10
!
{
Ã
2y/5
y
!
¯¯ y ∈C}
1
2
Ã
0
0
0
0
!
C2
2
(Thus, this transformation is nilpotent: N∞(t −0) is the entire space). From the nullities we know
that t’s action on a string basis is ⃗β1 7→⃗β2 7→⃗0. This is the canonical form matrix for the action of
t −0 on N∞(t −0) = C2
N0 =
µ
0
0
1
0
¶
and this is the Jordan form of the matrix.
J0 = N0 + 0 · I =
µ
0
0
1
0
¶

Answers to Exercises
209
Note that if a matrix is nilpotent then its canonical form equals its Jordan form.
We can ﬁnd such a string basis using the techniques of the prior section.
B = ⟨
µ1
0
¶
,
µ−10
−25
¶
⟩
The ﬁrst basis vector has been taken so that it is in the null space of t2 but is not in the null space
of t. The second basis vector is the image of the ﬁrst under t.
(b) The characteristic polynomial of this matrix is c(x) = (x+1)2, so it is a single-eigenvalue matrix.
(That is, the generalized null space of t + 1 is the entire space.) We have
N (t + 1) = {
µ
2y/3
y
¶ ¯¯ y ∈C}
N ((t + 1)2) = C2
and so the action of t + 1 on an associated string basis is ⃗β1 7→⃗β2 7→⃗0. Thus,
N−1 =
µ
0
0
1
0
¶
the Jordan form of T is
J−1 = N−1 + −1 · I =
µ
−1
0
1
−1
¶
and choosing vectors from the above null spaces gives this string basis (many other choices are
possible).
B = ⟨
µ
1
0
¶
,
µ
6
9
¶
⟩
(c) The characteristic polynomial c(x) = (1 −x)(4 −x)2 = −1 · (x −1)(x −4)2 has two roots and
they are the eigenvalues λ1 = 1 and λ2 = 4.
We handle the two eigenvalues separately. For λ1, the calculation of the powers of T −1I yields
N (t −1) = {


0
y
0

¯¯ y ∈C}
and the null space of (t −1)2 is the same. Thus this set is the generalized null space N∞(t −1).
The nullities show that the action of the restriction of t −1 to the generalized null space on a string
basis is ⃗β1 7→⃗0.
A similar calculation for λ2 = 4 gives these null spaces.
N (t −4) = {


0
z
z

¯¯ z ∈C}
N ((t −4)2) = {


y −z
y
z

¯¯ y, z ∈C}
(The null space of (t −4)3 is the same, as it must be because the power of the term associated with
λ2 = 4 in the characteristic polynomial is two, and so the restriction of t −2 to the generalized null
space N∞(t −2) is nilpotent of index at most two — it takes at most two applications of t −2 for
the null space to settle down.) The pattern of how the nullities rise tells us that the action of t −4
on an associated string basis for N∞(t −4) is ⃗β2 7→⃗β3 7→⃗0.
Putting the information for the two eigenvalues together gives the Jordan form of the transfor-
mation t.


1
0
0
0
4
0
0
1
4


We can take elements of the nullspaces to get an appropriate basis.
B = B1
⌢B4 = ⟨


0
1
0

,


1
0
1

,


0
5
5

⟩
(d) The characteristic polynomial is c(x) = (−2 −x)(4 −x)2 = −1 · (x + 2)(x −4)2.
For the eigenvalue λ−2, calculation of the powers of T + 2I yields this.
N (t + 2) = {


z
z
z

¯¯ z ∈C}
The null space of (t + 2)2 is the same, and so this is the generalized null space N∞(t + 2). Thus the
action of the restriction of t + 2 to N∞(t + 2) on an associated string basis is ⃗β1 7→⃗0.

210
Linear Algebra, by Hefferon
For λ2 = 4, computing the powers of T −4I yields
N (t −4) = {


z
−z
z

¯¯ z ∈C}
N ((t −4)2) = {


x
−z
z

¯¯ x, z ∈C}
and so the action of t −4 on a string basis for N∞(t −4) is ⃗β2 7→⃗β3 7→⃗0.
Therefore the Jordan form is


−2
0
0
0
4
0
0
1
4


and a suitable basis is this.
B = B−2
⌢B4 = ⟨


1
1
1

,


0
−1
1

,


−1
1
−1

⟩
(e) The characteristic polynomial of this matrix is c(x) = (2 −x)3 = −1 · (x −2)3. This matrix has
only a single eigenvalue, λ = 2. By ﬁnding the powers of T −2I we have
N (t −2) = {


−y
y
0

¯¯ y ∈C}
N ((t −2)2) = {


−y −(1/2)z
y
z

¯¯ y, z ∈C}
N ((t −2)3) = C3
and so the action of t −2 on an associated string basis is ⃗β1 7→⃗β2 7→⃗β3 7→⃗0. The Jordan form is
this


2
0
0
1
2
0
0
1
2


and one choice of basis is this.
B = ⟨


0
1
0

,


7
−9
4

,


−2
2
0

⟩
(f) The characteristic polynomial c(x) = (1 −x)3 = −(x −1)3 has only a single root, so the matrix
has only a single eigenvalue λ = 1. Finding the powers of T −1I and calculating the null spaces
N (t −1) = {


−2y + z
y
z

¯¯ y, z ∈C}
N ((t −1)2) = C3
shows that the action of the nilpotent map t −1 on a string basis is ⃗β1 7→⃗β2 7→⃗0 and ⃗β3 7→⃗0.
Therefore the Jordan form is
J =


1
0
0
1
1
0
0
0
1


and an appropriate basis (a string basis associated with t −1) is this.
B = ⟨


0
1
0

,


2
−2
−2

,


1
0
1

⟩
(g) The characteristic polynomial is a bit large for by-hand calculation, but just manageable c(x) =
x4 −24x3 + 216x2 −864x + 1296 = (x −6)4. This is a single-eigenvalue map, so the transformation
t −6 is nilpotent. The null spaces
N (t−6) = {




−z −w
−z −w
z
w




¯¯ z, w ∈C}
N ((t−6)2) = {




x
−z −w
z
w




¯¯ x, z, w ∈C}
N ((t−6)3) = C4
and the nullities show that the action of t −6 on a string basis is ⃗β1 7→⃗β2 7→⃗β3 7→⃗0 and ⃗β4 7→⃗0.
The Jordan form is




6
0
0
0
1
6
0
0
0
1
6
0
0
0
0
6





Answers to Exercises
211
and ﬁnding a suitable string basis is routine.
B = ⟨




0
0
0
1



,




2
−1
−1
2



,




3
3
−6
3



,




−1
−1
1
0



⟩
Five.IV.2.22
There are two eigenvalues, λ1 = −2 and λ2 = 1. The restriction of t + 2 to N∞(t + 2)
could have either of these actions on an associated string basis.
⃗β1 7→⃗β2 7→⃗0
⃗β1 7→⃗0
⃗β2 7→⃗0
The restriction of t −1 to N∞(t −1) could have either of these actions on an associated string basis.
⃗β3 7→⃗β4 7→⃗0
⃗β3 7→⃗0
⃗β4 7→⃗0
In combination, that makes four possible Jordan forms, the two ﬁrst actions, the second and ﬁrst, the
ﬁrst and second, and the two second actions.




−2
0
0
0
1
−2
0
0
0
0
1
0
0
0
1
1








−2
0
0
0
0
−2
0
0
0
0
1
0
0
0
1
1








−2
0
0
0
1
−2
0
0
0
0
1
0
0
0
0
1








−2
0
0
0
0
−2
0
0
0
0
1
0
0
0
0
1




Five.IV.2.23
The restriction of t + 2 to N∞(t + 2) can have only the action ⃗β1 7→⃗0. The restriction
of t −1 to N∞(t −1) could have any of these three actions on an associated string basis.
⃗β2 7→⃗β3 7→⃗β4 7→⃗0
⃗β2 7→⃗β3 7→⃗0
⃗β4 7→⃗0
⃗β2 7→⃗0
⃗β3 7→⃗0
⃗β4 7→⃗0
Taken together there are three possible Jordan forms, the one arising from the ﬁrst action by t −1
(along with the only action from t + 2), the one arising from the second action, and the one arising
from the third action. 



−2
0
0
0
0
1
0
0
0
1
1
0
0
0
1
1








−2
0
0
0
0
1
0
0
0
1
1
0
0
0
0
1








−2
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1




Five.IV.2.24
The action of t + 1 on a string basis for N∞(t + 1) must be ⃗β1 7→⃗0. Because of the
power of x −2 in the minimal polynomial, a string basis for t −2 has length two and so the action of
t −2 on N∞(t −2) must be of this form.
⃗β2 7→⃗β3 7→⃗0
⃗β4 7→⃗0
Therefore there is only one Jordan form that is possible.




−1
0
0
0
0
2
0
0
0
1
2
0
0
0
0
2




Five.IV.2.25
There are two possible Jordan forms. The action of t + 1 on a string basis for N∞(t + 1)
must be ⃗β1 7→⃗0. There are two actions for t −2 on a string basis for N∞(t −2) that are possible with
this characteristic polynomial and minimal polynomial.
⃗β2 7→⃗β3 7→⃗0
⃗β4 7→⃗β5 7→⃗0
⃗β2 7→⃗β3 7→⃗0
⃗β4 7→⃗0
⃗β5 7→⃗0
The resulting Jordan form matrics are these.






−1
0
0
0
0
0
2
0
0
0
0
1
2
0
0
0
0
0
2
0
0
0
0
1
2












−1
0
0
0
0
0
2
0
0
0
0
1
2
0
0
0
0
0
2
0
0
0
0
0
2







212
Linear Algebra, by Hefferon
Five.IV.2.26
(a) The characteristic polynomial is c(x) = x(x −1). For λ1 = 0 we have
N (t −0) = {
µ−y
y
¶ ¯¯ y ∈C}
(of course, the null space of t2 is the same). For λ2 = 1,
N (t −1) = {
µ
x
0
¶ ¯¯ x ∈C}
(and the null space of (t −1)2 is the same). We can take this basis
B = ⟨
µ
1
−1
¶
,
µ
1
0
¶
⟩
to get the diagonalization.
µ
1
1
−1
0
¶−1 µ
1
1
0
0
¶ µ
1
1
−1
0
¶
=
µ
0
0
0
1
¶
(b) The characteristic polynomial is c(x) = x2 −1 = (x + 1)(x −1). For λ1 = −1,
N (t + 1) = {
µ
−y
y
¶ ¯¯ y ∈C}
and the null space of (t + 1)2 is the same. For λ2 = 1
N (t −1) = {
µy
y
¶ ¯¯ y ∈C}
and the null space of (t −1)2 is the same. We can take this basis
B = ⟨
µ
1
−1
¶
,
µ
1
1
¶
⟩
to get a diagonalization.
µ1
1
1
−1
¶−1 µ0
1
1
0
¶ µ 1
1
−1
1
¶
=
µ−1
0
0
1
¶
Five.IV.2.27
The transformation d/dx: P3 →P3 is nilpotent. Its action on B = ⟨x3, 3x2, 6x, 6⟩is
x3 7→3x2 7→6x 7→6 7→0. Its Jordan form is its canonical form as a nilpotent matrix.
J =




0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
0




Five.IV.2.28
Yes. Each has the characteristic polynomial (x + 1)2. Calculations of the powers of
T1 + 1 · I and T2 + 1 · I gives these two.
N (t1 + 1) = {
µ
y/2
y
¶ ¯¯ y ∈C}
N (t2 + 1) = {
µ
0
y
¶ ¯¯ y ∈C}
(Of course, for each the null space of the square is the entire space.) The way that the nullities rise
shows that each is similar to this Jordan form matrix
µ
−1
0
1
−1
¶
and they are therefore similar to each other.
Five.IV.2.29
Its characteristic polynomial is c(x) = x2 + 1 which has complex roots x2 + 1 = (x +
i)(x −i). Because the roots are distinct, the matrix is diagonalizable and its Jordan form is that
diagonal matrix.
µ
−i
0
0
i
¶
To ﬁnd an associated basis we compute the null spaces.
N (t + i) = {
µ
−iy
y
¶ ¯¯ y ∈C}
N (t −i) = {
µ
iy
y
¶ ¯¯ y ∈C}
For instance,
T + i · I =
µ
i
−1
1
i
¶

Answers to Exercises
213
and so we get a description of the null space of t + i by solving this linear system.
ix −y = 0
x + iy = 0
iρ1+ρ2
−→
ix −y = 0
0 = 0
(To change the relation ix = y so that the leading variable x is expressed in terms of the free variable
y, we can multiply both sides by −i.)
As a result, one such basis is this.
B = ⟨
µ
−i
1
¶
,
µ
i
1
¶
⟩
Five.IV.2.30
We can count the possible classes by counting the possible canonical representatives,
that is, the possible Jordan form matrices. The characteristic polynomial must be either c1(x) =
(x + 3)2(x −4) or c2(x) = (x + 3)(x −4)2. In the c1 case there are two possible actions of t + 3 on a
string basis for N∞(t + 3).
⃗β1 7→⃗β2 7→⃗0
⃗β1 7→⃗0
⃗β2 7→⃗0
There are two associated Jordan form matrices.


−3
0
0
1
−3
0
0
0
4




−3
0
0
0
−3
0
0
0
4


Similarly there are two Jordan form matrices that could arise out of c2.


−3
0
0
0
4
0
0
1
4




−3
0
0
0
4
0
0
0
4


So in total there are four possible Jordan forms.
Five.IV.2.31
Jordan form is unique. A diagonal matrix is in Jordan form. Thus the Jordan form of
a diagonalizable matrix is its diagonalization. If the minimal polynomial has factors to some power
higher than one then the Jordan form has subdiagonal 1’s, and so is not diagonal.
Five.IV.2.32
One example is the transformation of C that sends x to −x.
Five.IV.2.33
Apply Lemma 2.7 twice; the subspace is t −λ1 invariant if and only if it is t invariant,
which in turn holds if and only if it is t −λ2 invariant.
Five.IV.2.34
False; these two 4×4 matrices each have c(x) = (x −3)4 and m(x) = (x −3)2.




3
0
0
0
1
3
0
0
0
0
3
0
0
0
1
3








3
0
0
0
1
3
0
0
0
0
3
0
0
0
0
3




Five.IV.2.35
(a) The characteristic polynomial is this.
¯¯¯¯
a −x
b
c
d −x
¯¯¯¯ = (a −x)(d −x) −bc = ad −(a + d)x + x2 −bc = x2 −(a + d)x + (ad −bc)
Note that the determinant appears as the constant term.
(b) Recall that the characteristic polynomial |T −xI| is invariant under similarity. Use the permu-
tation expansion formula to show that the trace is the negative of the coeﬃcient of xn−1.
(c) No, there are matrices T and S that are equivalent S = PTQ (for some nonsingular P and Q)
but that have diﬀerent traces. An easy example is this.
PTQ =
µ
2
0
0
1
¶ µ
1
0
0
1
¶ µ
1
0
0
1
¶
=
µ
2
0
0
1
¶
Even easier examples using 1×1 matrices are possible.
(d) Put the matrix in Jordan form. By the ﬁrst item, the trace is unchanged.
(e) The ﬁrst part is easy; use the third item. The converse does not hold: this matrix
µ
1
0
0
−1
¶
has a trace of zero but is not nilpotent.
Five.IV.2.36
Suppose that BM is a basis for a subspace M of some vector space. Implication one way
is clear; if M is t invariant then in particular, if ⃗m ∈BM then t(⃗m) ∈M. For the other implication,
let BM = ⟨⃗β1, . . . , ⃗βq⟩and note that t(⃗m) = t(m1⃗β1 + · · · + mq⃗βq) = m1t(⃗β1) + · · · + mqt(⃗βq) is in M
as any subspace is closed under linear combinations.

214
Linear Algebra, by Hefferon
Five.IV.2.37
Yes, the intersection of t invariant subspaces is t invariant. Assume that M and N are
t invariant. If ⃗v ∈M ∩N then t(⃗v) ∈M by the invariance of M and t(⃗v) ∈N by the invariance of N.
Of course, the union of two subspaces need not be a subspace (remember that the x- and y-axes
are subspaces of the plane R2 but the union of the two axes fails to be closed under vector addition,
for instance it does not contain ⃗e1 +⃗e2.) However, the union of invariant subsets is an invariant subset;
if ⃗v ∈M ∪N then ⃗v ∈M or ⃗v ∈N so t(⃗v) ∈M or t(⃗v) ∈N.
No, the complement of an invariant subspace need not be invariant. Consider the subspace
{
µ
x
0
¶ ¯¯ x ∈C}
of C2 under the zero transformation.
Yes, the sum of two invariant subspaces is invariant. The check is easy.
Five.IV.2.38
One such ordering is the dictionary ordering. Order by the real component ﬁrst, then
by the coeﬃcient of i. For instance, 3 + 2i < 4 + 1i but 4 + 1i < 4 + 2i.
Five.IV.2.39
The ﬁrst half is easy — the derivative of any real polynomial is a real polynomial of lower
degree. The answer to the second half is ‘no’; any complement of Pj(R) must include a polynomial of
degree j + 1, and the derivative of that polynomial is in Pj(R).
Five.IV.2.40
For the ﬁrst half, show that each is a subspace and then observe that any polynomial
can be uniquely written as the sum of even-powered and odd-powered terms (the zero polynomial is
both). The answer to the second half is ‘no’: x2 is even while 2x is odd.
Five.IV.2.41
Yes. If RepB,B(t) has the given block form, take BM to be the ﬁrst j vectors of B, where
J is the j×j upper left submatrix. Take BN to be the remaining k vectors in B. Let M and N be the
spans of BM and BN. Clearly M and N are complementary. To see M is invariant (N works the same
way), represent any ⃗m ∈M with respect to B, note the last k components are zeroes, and multiply by
the given block matrix. The ﬁnal k components of the result are zeroes, so that result is again in M.
Five.IV.2.42
Put the matrix in Jordan form. By non-singularity, there are no zero eigenvalues on the
diagonal. Ape this example:


9
0
0
1
9
0
0
0
4

=


3
0
0
1/6
3
0
0
0
2


2
to construct a square root. Show that it holds up under similarity: if S2 = T then (PSP −1)(PSP −1) =
PTP −1.
Topic: Method of Powers
1
(a) The largest eigenvalue is 4.
(b) The largest eigenvalue is 2.
3
(a) The largest eigenvalue is 3.
(b) The largest eigenvalue is −3.
5
In theory, this method would produce λ2. In practice, however, rounding errors in the computation
introduce components in the direction of ⃗v1, and so the method will still produce λ1, although it may
take somewhat longer than it would have taken with a more fortunate choice of initial vector.
6
Instead of using ⃗vk = T⃗vk−1, use T −1⃗vk = ⃗vk−1.
Topic: Stable Populations

Answers to Exercises
215
Topic: Linear Recurrences
1
(a) We express the relation in matrix form.
µ
5
−6
1
0
¶ µ
f(n)
f(n −1)
¶
=
µ
f(n + 1)
f(n)
¶
The characteristic equation of the matrix
¯¯¯¯
5 −λ
−6
1
−λ
¯¯¯¯ = λ2 −5λ + 6
has roots of 2 and 3. Any function of the form f(n) = c12n + c23n satisﬁes the recurrence.
(b) This is like the prior part, but simpler. The matrix expression of the relation is
¡
4
¢ ¡
f(n)
¢
=
¡
f(n + 1)
¢
and the characteristic equation of the matrix
¯¯4 −λ
¯¯ = 4 −λ
has the single root 4. Any function of the form f(n) = c4n satisﬁes this recurrence.
(c) In matrix form the relation

6
7
6
1
0
0
0
1
0




f(n)
f(n −1)
f(n −2)

=


f(n + 1)
f(n)
f(n −1)


gives this characteristic equation.
¯¯¯¯¯¯
6 −λ
7
6
1
−λ
0
0
1
−λ
¯¯¯¯¯¯
= −λ3 −6λ2 + 7λ + 6


Chapter One: Linear Systems
Subsection One.I.1: Gauss’ Method
One.I.1.16
Gauss’ method can be performed in diﬀerent ways, so these simply exhibit one possible
way to get the answer.
(a) Gauss’ method
−(1/2)ρ1+ρ2
−→
2x +
3y =
13
−(5/2)y = −15/2
gives that the solution is y = 3 and x = 2.
(b) Gauss’ method here
−3ρ1+ρ2
−→
ρ1+ρ3
x
−z = 0
y + 3z = 1
y
= 4
−ρ2+ρ3
−→
x
−
z = 0
y +
3z = 1
−3z = 3
gives x = −1, y = 4, and z = −1.
One.I.1.17
(a) Gaussian reduction
−(1/2)ρ1+ρ2
−→
2x +
2y =
5
−5y = −5/2
shows that y = 1/2 and x = 2 is the unique solution.
(b) Gauss’ method
ρ1+ρ2
−→
−x + y = 1
2y = 3
gives y = 3/2 and x = 1/2 as the only solution.
(c) Row reduction
−ρ1+ρ2
−→
x −3y + z = 1
4y + z = 13
shows, because the variable z is not a leading variable in any row, that there are many solutions.
(d) Row reduction
−3ρ1+ρ2
−→
−x −y =
1
0 = −1
shows that there is no solution.
(e) Gauss’ method
ρ1↔ρ4
−→
x + y −z = 10
2x −2y + z = 0
x
+ z = 5
4y + z = 20
−2ρ1+ρ2
−→
−ρ1+ρ3
x +
y −z =
10
−4y + 3z = −20
−y + 2z = −5
4y + z =
20
−(1/4)ρ2+ρ3
−→
ρ2+ρ4
x +
y −
z =
10
−4y +
3z = −20
(5/4)z =
0
4z =
0
gives the unique solution (x, y, z) = (5, 5, 0).
(f) Here Gauss’ method gives
−(3/2)ρ1+ρ3
−→
−2ρ1+ρ4
2x
+
z +
w =
5
y
−
w =
−1
−(5/2)z −(5/2)w = −15/2
y
−
w =
−1
−ρ2+ρ4
−→
2x
+
z +
w =
5
y
−
w =
−1
−(5/2)z −(5/2)w = −15/2
0 =
0
which shows that there are many solutions.
One.I.1.18
(a) From x = 1 −3y we get that 2(1 −3y) + y = −3, giving y = 1.
(b) From x = 1 −3y we get that 2(1 −3y) + 2y = 0, leading to the conclusion that y = 1/2.
Users of this method must check any potential solutions by substituting back into all the equations.

218
Linear Algebra, by Hefferon
One.I.1.19
Do the reduction
−3ρ1+ρ2
−→
x −y =
1
0 = −3 + k
to conclude this system has no solutions if k ̸= 3 and if k = 3 then it has inﬁnitely many solutions. It
never has a unique solution.
One.I.1.20
Let x = sin α, y = cos β, and z = tan γ:
2x −y + 3z = 3
4x + 2y −2z = 10
6x −3y + z = 9
−2ρ1+ρ2
−→
−3ρ1+ρ3
2x −y +
3z = 3
4y −
8z = 4
−8z = 0
gives z = 0, y = 1, and x = 2. Note that no α satisﬁes that requirement.
One.I.1.21
(a) Gauss’ method
−3ρ1+ρ2
−→
−ρ1+ρ3
−2ρ1+ρ4
x −3y =
b1
10y = −3b1 + b2
10y = −b1 + b3
10y = −2b1 + b4
−ρ2+ρ3
−→
−ρ2+ρ4
x −3y =
b1
10y =
−3b1 + b2
0 = 2b1 −b2 + b3
0 = b1 −b2 + b4
shows that this system is consistent if and only if both b3 = −2b1 + b2 and b4 = −b1 + b2.
(b) Reduction
−2ρ1+ρ2
−→
−ρ1+ρ3
x1 +
2x2 + 3x3 =
b1
x2 −3x3 = −2b1 + b2
−2x2 + 5x3 = −b1 + b3
2ρ2+ρ3
−→
x1 + 2x2 + 3x3 =
b1
x2 −3x3 =
−2b1 + b2
−x3 = −5b1 + +2b2 + b3
shows that each of b1, b2, and b3 can be any real number — this system always has a unique solution.
One.I.1.22
This system with more unknowns than equations
x + y + z = 0
x + y + z = 1
has no solution.
One.I.1.23
Yes. For example, the fact that the same reaction can be performed in two diﬀerent ﬂasks
shows that twice any solution is another, diﬀerent, solution (if a physical reaction occurs then there
must be at least one nonzero solution).
One.I.1.24
Because f(1) = 2, f(−1) = 6, and f(2) = 3 we get a linear system.
1a + 1b + c = 2
1a −1b + c = 6
4a + 2b + c = 3
Gauss’ method
−ρ1+ρ2
−→
−4ρ1+ρ2
a +
b + c =
2
−2b
=
4
−2b −3c = −5
−ρ2+ρ3
−→
a +
b +
c =
2
−2b
=
4
−3c = −9
shows that the solution is f(x) = 1x2 −2x + 3.
One.I.1.25
(a) Yes, by inspection the given equation results from −ρ1 + ρ2.
(b) No. The given equation is satisﬁed by the pair (1, 1). However, that pair does not satisfy the
ﬁrst equation in the system.
(c) Yes. To see if the given row is c1ρ1 + c2ρ2, solve the system of equations relating the coeﬃcients
of x, y, z, and the constants:
2c1 + 6c2 =
6
c1 −3c2 = −9
−c1 + c2 =
5
4c1 + 5c2 = −2
and get c1 = −3 and c2 = 2, so the given row is −3ρ1 + 2ρ2.
One.I.1.26
If a ̸= 0 then the solution set of the ﬁrst equation is {(x, y)
¯¯ x = (c −by)/a}. Taking y = 0
gives the solution (c/a, 0), and since the second equation is supposed to have the same solution set,
substituting into it gives that a(c/a) + d · 0 = e, so c = e. Then taking y = 1 in x = (c −by)/a gives
that a((c −b)/a) + d · 1 = e, which gives that b = d. Hence they are the same equation.
When a = 0 the equations can be diﬀerent and still have the same solution set: e.g., 0x + 3y = 6
and 0x + 6y = 12.

Answers to Exercises
219
One.I.1.27
We take three cases: that a ̸= 0, that a = 0 and c ̸= 0, and that both a = 0 and c = 0.
For the ﬁrst, we assume that a ̸= 0. Then the reduction
−(c/a)ρ1+ρ2
−→
ax +
by =
j
(−cb
a + d)y = −cj
a + k
shows that this system has a unique solution if and only if −(cb/a) + d ̸= 0; remember that a ̸= 0
so that back substitution yields a unique x (observe, by the way, that j and k play no role in the
conclusion that there is a unique solution, although if there is a unique solution then they contribute
to its value). But −(cb/a)+d = (ad−bc)/a and a fraction is not equal to 0 if and only if its numerator
is not equal to 0. Thus, in this ﬁrst case, there is a unique solution if and only if ad −bc ̸= 0.
In the second case, if a = 0 but c ̸= 0, then we swap
cx + dy = k
by = j
to conclude that the system has a unique solution if and only if b ̸= 0 (we use the case assumption that
c ̸= 0 to get a unique x in back substitution). But — where a = 0 and c ̸= 0 — the condition “b ̸= 0”
is equivalent to the condition “ad −bc ̸= 0”. That ﬁnishes the second case.
Finally, for the third case, if both a and c are 0 then the system
0x + by = j
0x + dy = k
might have no solutions (if the second equation is not a multiple of the ﬁrst) or it might have inﬁnitely
many solutions (if the second equation is a multiple of the ﬁrst then for each y satisfying both equations,
any pair (x, y) will do), but it never has a unique solution. Note that a = 0 and c = 0 gives that
ad −bc = 0.
One.I.1.28
Recall that if a pair of lines share two distinct points then they are the same line. That’s
because two points determine a line, so these two points determine each of the two lines, and so they
are the same line.
Thus the lines can share one point (giving a unique solution), share no points (giving no solutions),
or share at least two points (which makes them the same line).
One.I.1.29
For the reduction operation of multiplying ρi by a nonzero real number k, we have that
(s1, . . . , sn) satisﬁes this system
a1,1x1 + a1,2x2 + · · · + a1,nxn = d1
...
kai,1x1 + kai,2x2 + · · · + kai,nxn = kdi
...
am,1x1 + am,2x2 + · · · + am,nxn = dm
if and only if
a1,1s1 + a1,2s2 + · · · + a1,nsn = d1
...
and kai,1s1 + kai,2s2 + · · · + kai,nsn = kdi
...
and am,1s1 + am,2s2 + · · · + am,nsn = dm
by the deﬁnition of ‘satisﬁes’. But, because k ̸= 0, that’s true if and only if
a1,1s1 + a1,2s2 + · · · + a1,nsn = d1
...
and ai,1s1 + ai,2s2 + · · · + ai,nsn = di
...
and am,1s1 + am,2s2 + · · · + am,nsn = dm
(this is straightforward cancelling on both sides of the i-th equation), which says that (s1, . . . , sn)

220
Linear Algebra, by Hefferon
solves
a1,1x1 + a1,2x2 + · · · + a1,nxn = d1
...
ai,1x1 + ai,2x2 + · · · + ai,nxn = di
...
am,1x1 + am,2x2 + · · · + am,nxn = dm
as required.
For the pivot operation kρi + ρj, we have that (s1, . . . , sn) satisﬁes
a1,1x1 + · · · +
a1,nxn =
d1
...
ai,1x1 + · · · +
ai,nxn =
di
...
(kai,1 + aj,1)x1 + · · · + (kai,n + aj,n)xn = kdi + dj
...
am,1x1 + · · · +
am,nxn = dm
if and only if
a1,1s1 + · · · + a1,nsn = d1
...
and ai,1s1 + · · · + ai,nsn = di
...
and (kai,1 + aj,1)s1 + · · · + (kai,n + aj,n)sn = kdi + dj
...
and am,1s1 + am,2s2 + · · · + am,nsn = dm
again by the deﬁnition of ‘satisﬁes’. Subtract k times the i-th equation from the j-th equation (re-
mark: here is where i ̸= j is needed; if i = j then the two di’s above are not equal) to get that the
previous compound statement holds if and only if
a1,1s1 + · · · + a1,nsn = d1
...
and ai,1s1 + · · · + ai,nsn = di
...
and (kai,1 + aj,1)s1 + · · · + (kai,n + aj,n)sn
−(kai,1s1 + · · · + kai,nsn) = kdi + dj −kdi
...
and am,1s1 + · · · + am,nsn = dm
which, after cancellation, says that (s1, . . . , sn) solves
a1,1x1 + · · · + a1,nxn = d1
...
ai,1x1 + · · · + ai,nxn = di
...
aj,1x1 + · · · + aj,nxn = dj
...
am,1x1 + · · · + am,nxn = dm
as required.
One.I.1.30
Yes, this one-equation system:
0x + 0y = 0
is satisﬁed by every (x, y) ∈R2.

Answers to Exercises
221
One.I.1.31
Yes. This sequence of operations swaps rows i and j
ρi+ρj
−→
−ρj+ρi
−→
ρi+ρj
−→
−1ρi
−→
so the row-swap operation is redundant in the presence of the other two.
One.I.1.32
Swapping rows is reversed by swapping back.
a1,1x1 + · · · + a1,nxn = d1
...
am,1x1 + · · · + am,nxn = dm
ρi↔ρj
−→
ρj↔ρi
−→
a1,1x1 + · · · + a1,nxn = d1
...
am,1x1 + · · · + am,nxn = dm
Multiplying both sides of a row by k ̸= 0 is reversed by dividing by k.
a1,1x1 + · · · + a1,nxn = d1
...
am,1x1 + · · · + am,nxn = dm
kρi
−→
(1/k)ρi
−→
a1,1x1 + · · · + a1,nxn = d1
...
am,1x1 + · · · + am,nxn = dm
Adding k times a row to another is reversed by adding −k times that row.
a1,1x1 + · · · + a1,nxn = d1
...
am,1x1 + · · · + am,nxn = dm
kρi+ρj
−→
−kρi+ρj
−→
a1,1x1 + · · · + a1,nxn = d1
...
am,1x1 + · · · + am,nxn = dm
Remark: observe for the third case that if we were to allow i = j then the result wouldn’t hold.
3x + 2y = 7
2ρ1+ρ1
−→
9x + 6y = 21
−2ρ1+ρ1
−→
−9x −6y = −21
One.I.1.33
Let p, n, and d be the number of pennies, nickels, and dimes. For variables that are real
numbers, this system
p + n +
d = 13
p + 5n + 10d = 83
−ρ1+ρ2
−→
p + n + d = 13
4n + 9d = 70
has inﬁnitely many solutions. However, it has a limited number of solutions in which p, n, and d are
non-negative integers. Running through d = 0, . . . , d = 8 shows that (p, n, d) = (3, 4, 6) is the only
sensible solution.
One.I.1.34
Solving the system
(1/3)(a + b + c) + d = 29
(1/3)(b + c + d) + a = 23
(1/3)(c + d + a) + b = 21
(1/3)(d + a + b) + c = 17
we obtain a = 12, b = 9, c = 3, d = 21. Thus the second item, 21, is the correct answer.
One.I.1.35
This is how the answer was given in the cited source.
A comparison of the units and
hundreds columns of this addition shows that there must be a carry from the tens column. The tens
column then tells us that A < H, so there can be no carry from the units or hundreds columns. The
ﬁve columns then give the following ﬁve equations.
A + E = W
2H = A + 10
H = W + 1
H + T = E + 10
A + 1 = T
The ﬁve linear equations in ﬁve unknowns, if solved simultaneously, produce the unique solution: A =
4, T = 5, H = 7, W = 6 and E = 2, so that the original example in addition was 47474+5272 = 52746.
One.I.1.36
This is how the answer was given in the cited source. Eight commissioners voted for B.
To see this, we will use the given information to study how many voters chose each order of A, B, C.
The six orders of preference are ABC, ACB, BAC, BCA, CAB, CBA; assume they receive a, b,
c, d, e, f votes respectively. We know that
a + b + e = 11
d + e + f = 12
a + c + d = 14

222
Linear Algebra, by Hefferon
from the number preferring A over B, the number preferring C over A, and the number preferring B
over C. Because 20 votes were cast, we also know that
c + d + f = 9
a + b + c = 8
b + e + f = 6
from the preferences for B over A, for A over C, and for C over B.
The solution is a = 6, b = 1, c = 1, d = 7, e = 4, and f = 1. The number of commissioners voting
for B as their ﬁrst choice is therefore c + d = 1 + 7 = 8.
Comments. The answer to this question would have been the same had we known only that at least
14 commissioners preferred B over C.
The seemingly paradoxical nature of the commissioners’s preferences (A is preferred to B, and B is
preferred to C, and C is preferred to A), an example of “non-transitive dominance”, is not uncommon
when individual choices are pooled.
One.I.1.37
This is how the answer was given in the cited source. We have not used “dependent” yet;
it means here that Gauss’ method shows that there is not a unique solution. If n ≥3 the system is
dependent and the solution is not unique. Hence n < 3. But the term “system” implies n > 1. Hence
n = 2. If the equations are
ax + (a + d)y = a + 2d
(a + 3d)x + (a + 4d)y = a + 5d
then x = −1, y = 2.
Subsection One.I.2: Describing the Solution Set
One.I.2.15
(a) 2
(b) 3
(c) −1
(d) Not deﬁned.
One.I.2.16
(a) 2×3
(b) 3×2
(c) 2×2
One.I.2.17
(a)


5
1
5


(b)
µ
20
−5
¶
(c)


−2
4
0


(d)
µ
41
52
¶
(e) Not deﬁned.
(f)


12
8
4


One.I.2.18
(a) This reductionµ
3
6
18
1
2
6
¶
(−1/3)ρ1+ρ2
−→
µ
3
6
18
0
0
0
¶
leaves x leading and y free. Making y the parameter, we have x = 6 −2y so the solution set is
{
µ
6
0
¶
+
µ
−2
1
¶
y
¯¯ y ∈R}.
(b) This reduction
µ
1
1
1
1
−1
−1
¶
−ρ1+ρ2
−→
µ
1
1
1
0
−2
−2
¶
gives the unique solution y = 1, x = 0. The solution set is
{
µ
0
1
¶
}.
(c) This use of Gauss’ method


1
0
1
4
1
−1
2
5
4
−1
5
17


−ρ1+ρ2
−→
−4ρ1+ρ3


1
0
1
4
0
−1
1
1
0
−1
1
1

−ρ2+ρ3
−→


1
0
1
4
0
−1
1
1
0
0
0
0


leaves x1 and x2 leading with x3 free. The solution set is
{


4
−1
0

+


−1
1
1

x3
¯¯ x3 ∈R}.

Answers to Exercises
223
(d) This reduction


2
1
−1
2
2
0
1
3
1
−1
0
0


−ρ1+ρ2
−→
−(1/2)ρ1+ρ3


2
1
−1
2
0
−1
2
1
0
−3/2
1/2
−1

(−3/2)ρ2+ρ3
−→


2
1
−1
2
0
−1
2
1
0
0
−5/2
−5/2


shows that the solution set is a singleton set.
{


1
1
1

}
(e) This reduction is easy


1
2
−1
0
3
2
1
0
1
4
1
−1
1
1
1

−2ρ1+ρ2
−→
−ρ1+ρ3


1
2
−1
0
3
0
−3
2
1
−2
0
−3
2
1
−2

−ρ2+ρ3
−→


1
2
−1
0
3
0
−3
2
1
−2
0
0
0
0
0


and ends with x and y leading, while z and w are free. Solving for y gives y = (2 + 2z + w)/3 and
substitution shows that x + 2(2 + 2z + w)/3 −z = 3 so x = (5/3) −(1/3)z −(2/3)w, making the
solution set
{




5/3
2/3
0
0



+




−1/3
2/3
1
0



z +




−2/3
1/3
0
1



w
¯¯ z, w ∈R}.
(f) The reduction


1
0
1
1
4
2
1
0
−1
2
3
1
1
0
7

−2ρ1+ρ2
−→
−3ρ1+ρ3


1
0
1
1
4
0
1
−2
−3
−6
0
1
−2
−3
−5

−ρ2+ρ3
−→


1
0
1
1
4
0
1
−2
−3
−6
0
0
0
0
1


shows that there is no solution — the solution set is empty.
One.I.2.19
(a) This reduction
µ
2
1
−1
1
4
−1
0
3
¶
−2ρ1+ρ2
−→
µ
2
1
−1
1
0
−3
2
1
¶
ends with x and y leading while z is free. Solving for y gives y = (1−2z)/(−3), and then substitution
2x + (1 −2z)/(−3) −z = 1 shows that x = ((4/3) + (1/3)z)/2. Hence the solution set is
{


2/3
−1/3
0

+


1/6
2/3
1

z
¯¯ z ∈R}.
(b) This application of Gauss’ method


1
0
−1
0
1
0
1
2
−1
3
1
2
3
−1
7

−ρ1+ρ3
−→


1
0
−1
0
1
0
1
2
−1
3
0
2
4
−1
6

−2ρ2+ρ3
−→


1
0
−1
0
1
0
1
2
−1
3
0
0
0
1
0


leaves x, y, and w leading. The solution set is
{




1
3
0
0



+




1
−2
1
0



z
¯¯ z ∈R}.
(c) This row reduction




1
−1
1
0
0
0
1
0
1
0
3
−2
3
1
0
0
−1
0
−1
0




−3ρ1+ρ3
−→




1
−1
1
0
0
0
1
0
1
0
0
1
0
1
0
0
−1
0
−1
0




−ρ2+ρ3
−→
ρ2+ρ4




1
−1
1
0
0
0
1
0
1
0
0
0
0
0
0
0
0
0
0
0




ends with z and w free. The solution set is
{




0
0
0
0



+




−1
0
1
0



z +




−1
−1
0
1



w
¯¯ z, w ∈R}.

224
Linear Algebra, by Hefferon
(d) Gauss’ method done in this way
µ
1
2
3
1
−1
1
3
−1
1
1
1
3
¶
−3ρ1+ρ2
−→
µ
1
2
3
1
−1
1
0
−7
−8
−2
4
0
¶
ends with c, d, and e free. Solving for b shows that b = (8c + 2d −4e)/(−7) and then substitution
a + 2(8c + 2d −4e)/(−7) + 3c + 1d −1e = 1 shows that a = 1 −(5/7)c −(3/7)d −(1/7)e and so the
solution set is
{






1
0
0
0
0






+






−5/7
−8/7
1
0
0






c +






−3/7
−2/7
0
1
0






d +






−1/7
4/7
0
0
1






e
¯¯ c, d, e ∈R}.
One.I.2.20
For each problem we get a system of linear equations by looking at the equations of
components.
(a) k = 5
(b) The second components show that i = 2, the third components show that j = 1.
(c) m = −4, n = 2
One.I.2.21
For each problem we get a system of linear equations by looking at the equations of
components.
(a) Yes; take k = −1/2.
(b) No; the system with equations 5 = 5 · j and 4 = −4 · j has no solution.
(c) Yes; take r = 2.
(d) No. The second components give k = 0. Then the third components give j = 1. But the ﬁrst
components don’t check.
One.I.2.22
This system has 1 equation. The leading variable is x1, the other variables are free.
{





−1
1
...
0




x2 + · · · +





−1
0
...
1




xn
¯¯ x1, . . . , xn ∈R}
One.I.2.23
(a) Gauss’ method here gives


1
2
0
−1
a
2
0
1
0
b
1
1
0
2
c


−2ρ1+ρ2
−→
−ρ1+ρ3


1
2
0
−1
a
0
−4
1
2
−2a + b
0
−1
0
3
−a + c


−(1/4)ρ2+ρ3
−→


1
2
0
−1
a
0
−4
1
2
−2a + b
0
0
−1/4
5/2
−(1/2)a −(1/4)b + c

,
leaving w free. Solve: z = 2a + b −4c + 10w, and −4y = −2a + b −(2a + b −4c + 10w) −2w so
y = a −c + 3w, and x = a −2(a −c + 3w) + w = −a + 2c −5w. Therefore the solution set is this.
{




−a + 2c
a −c
2a + b −4c
0



+




−5
3
10
1



w
¯¯ w ∈R}
(b) Plug in with a = 3, b = 1, and c = −2.
{




−7
5
15
0



+




−5
3
10
1



w
¯¯ w ∈R}
One.I.2.24
Leaving the comma out, say by writing a123, is ambiguous because it could mean a1,23 or
a12,3.
One.I.2.25
(a)




2
3
4
5
3
4
5
6
4
5
6
7
5
6
7
8




(b)




1
−1
1
−1
−1
1
−1
1
1
−1
1
−1
−1
1
−1
1





Answers to Exercises
225
One.I.2.26
(a)


1
4
2
5
3
6


(b)
µ
2
1
−3
1
¶
(c)
µ
5
10
10
5
¶
(d)
¡1
1
0¢
One.I.2.27
(a) Plugging in x = 1 and x = −1 gives
a + b + c = 2
a −b + c = 6
−ρ1+ρ2
−→
a +
b + c = 2
−2b
= 4
so the set of functions is {f(x) = (4 −c)x2 −2x + c
¯¯ c ∈R}.
(b) Putting in x = 1 gives
a + b + c = 2
so the set of functions is {f(x) = (2 −b −c)x2 + bx + c
¯¯ b, c ∈R}.
One.I.2.28
On plugging in the ﬁve pairs (x, y) we get a system with the ﬁve equations and six unknowns
a, . . . , f. Because there are more unknowns than equations, if no inconsistency exists among the
equations then there are inﬁnitely many solutions (at least one variable will end up free).
But no inconsistency can exist because a = 0, . . . , f = 0 is a solution (we are only using this zero
solution to show that the system is consistent — the prior paragraph shows that there are nonzero
solutions).
One.I.2.29
(a) Here is one — the fourth equation is redundant but still OK.
x + y −z + w = 0
y −z
= 0
2z + 2w = 0
z + w = 0
(b) Here is one.
x + y −z + w = 0
w = 0
w = 0
w = 0
(c) This is one.
x + y −z + w = 0
x + y −z + w = 0
x + y −z + w = 0
x + y −z + w = 0
One.I.2.30
This is how the answer was given in the cited source.
(a) Formal solution of the system yields
x = a3 −1
a2 −1
y = −a2 + a
a2 −1 .
If a + 1 ̸= 0 and a −1 ̸= 0, then the system has the single solution
x = a2 + a + 1
a + 1
y =
−a
a + 1.
If a = −1, or if a = +1, then the formulas are meaningless; in the ﬁrst instance we arrive at the
system
½−x + y = 1
x −y = 1
which is a contradictory system. In the second instance we have
½
x + y = 1
x + y = 1
which has an inﬁnite number of solutions (for example, for x arbitrary, y = 1 −x).
(b) Solution of the system yields
x = a4 −1
a2 −1
y = −a3 + a
a2 −1 .
Here, is a2 −1 ̸= 0, the system has the single solution x = a2 + 1, y = −a. For a = −1 and a = 1,
we obtain the systems
½−x + y = −1
x −y =
1
½x + y = 1
x + y = 1
both of which have an inﬁnite number of solutions.

226
Linear Algebra, by Hefferon
One.I.2.31
This is how the answer was given in the cited source. Let u, v, x, y, z be the volumes
in cm3 of Al, Cu, Pb, Ag, and Au, respectively, contained in the sphere, which we assume to be
not hollow. Since the loss of weight in water (speciﬁc gravity 1.00) is 1000 grams, the volume of the
sphere is 1000 cm3. Then the data, some of which is superﬂuous, though consistent, leads to only 2
independent equations, one relating volumes and the other, weights.
u +
v +
x +
y +
z = 1000
2.7u + 8.9v + 11.3x + 10.5y + 19.3z = 7558
Clearly the sphere must contain some aluminum to bring its mean speciﬁc gravity below the speciﬁc
gravities of all the other metals. There is no unique result to this part of the problem, for the amounts
of three metals may be chosen arbitrarily, provided that the choices will not result in negative amounts
of any metal.
If the ball contains only aluminum and gold, there are 294.5 cm3 of gold and 705.5 cm3 of aluminum.
Another possibility is 124.7 cm3 each of Cu, Au, Pb, and Ag and 501.2 cm3 of Al.
Subsection One.I.3: General = Particular + Homogeneous
One.I.3.15
For the arithmetic to these, see the answers from the prior subsection.
(a) The solution set is
{
µ
6
0
¶
+
µ
−2
1
¶
y
¯¯ y ∈R}.
Here the particular solution and the solution set for the associated homogeneous system are
µ
6
0
¶
and
{
µ
−2
1
¶
y
¯¯ y ∈R}.
(b) The solution set is
{
µ
0
1
¶
}.
The particular solution and the solution set for the associated homogeneous system are
µ
0
1
¶
and
{
µ
0
0
¶
}
(c) The solution set is
{


4
−1
0

+


−1
1
1

x3
¯¯ x3 ∈R}.
A particular solution and the solution set for the associated homogeneous system are


4
−1
0


and
{


−1
1
1

x3
¯¯ x3 ∈R}.
(d) The solution set is a singleton
{


1
1
1

}.
A particular solution and the solution set for the associated homogeneous system are


1
1
1


and
{


0
0
0

t
¯¯ t ∈R}.
(e) The solution set is
{




5/3
2/3
0
0



+




−1/3
2/3
1
0



z +




−2/3
1/3
0
1



w
¯¯ z, w ∈R}.
A particular solution and the solution set for the associated homogeneous system are




5/2
2/3
0
0




and
{




−1/3
2/3
1
0



z +




−2/3
1/3
0
1



w
¯¯ z, w ∈R}.

Answers to Exercises
227
(f) This system’s solution set is empty. Thus, there is no particular solution. The solution set of the
associated homogeneous system is
{




−1
2
1
0



z +




−1
3
0
1



w
¯¯ z, w ∈R}.
One.I.3.16
The answers from the prior subsection show the row operations.
(a) The solution set is
{


2/3
−1/3
0

+


1/6
2/3
1

z
¯¯ z ∈R}.
A particular solution and the solution set for the associated homogeneous system are


2/3
−1/3
0


and
{


1/6
2/3
1

z
¯¯ z ∈R}.
(b) The solution set is
{




1
3
0
0



+




1
−2
1
0



z
¯¯ z ∈R}.
A particular solution and the solution set for the associated homogeneous system are




1
3
0
0




and
{




1
−2
1
0



z
¯¯ z ∈R}.
(c) The solution set is
{




0
0
0
0



+




−1
0
1
0



z +




−1
−1
0
1



w
¯¯ z, w ∈R}.
A particular solution and the solution set for the associated homogeneous system are




0
0
0
0




and
{




−1
0
1
0



z +




−1
−1
0
1



w
¯¯ z, w ∈R}.
(d) The solution set is
{






1
0
0
0
0






+






−5/7
−8/7
1
0
0






c +






−3/7
−2/7
0
1
0






d +






−1/7
4/7
0
0
1






e
¯¯ c, d, e ∈R}.
A particular solution and the solution set for the associated homogeneous system are






1
0
0
0
0






and
{






−5/7
−8/7
1
0
0






c +






−3/7
−2/7
0
1
0






d +






−1/7
4/7
0
0
1






e
¯¯ c, d, e ∈R}.
One.I.3.17
Just plug them in and see if they satisfy all three equations.
(a) No.
(b) Yes.
(c) Yes.
One.I.3.18
Gauss’ method on the associated homogeneous system gives


1
−1
0
1
0
2
3
−1
0
0
0
1
1
1
0

−2ρ1+ρ2
−→


1
−1
0
1
0
0
5
−1
−2
0
0
1
1
1
0

−(1/5)ρ2+ρ3
−→


1
−1
0
1
0
0
5
−1
−2
0
0
0
6/5
7/5
0



228
Linear Algebra, by Hefferon
so this is the solution to the homogeneous problem:
{




−5/6
1/6
−7/6
1



w
¯¯ w ∈R}.
(a) That vector is indeed a particular solution, so the required general solution is
{




0
0
0
4



+




−5/6
1/6
−7/6
1



w
¯¯ w ∈R}.
(b) That vector is a particular solution so the required general solution is
{




−5
1
−7
10



+




−5/6
1/6
−7/6
1



w
¯¯ w ∈R}.
(c) That vector is not a solution of the system since it does not satisfy the third equation. No such
general solution exists.
One.I.3.19
The ﬁrst is nonsingular while the second is singular. Just do Gauss’ method and see if the
echelon form result has non-0 numbers in each entry on the diagonal.
One.I.3.20
(a) Nonsingular:
−ρ1+ρ2
−→
µ
1
2
0
1
¶
ends with each row containing a leading entry.
(b) Singular:
3ρ1+ρ2
−→
µ
1
2
0
0
¶
ends with row 2 without a leading entry.
(c) Neither. A matrix must be square for either word to apply.
(d) Singular.
(e) Nonsingular.
One.I.3.21
In each case we must decide if the vector is a linear combination of the vectors in the
set.
(a) Yes. Solve
c1
µ
1
4
¶
+ c2
µ
1
5
¶
=
µ
2
3
¶
with
µ
1
1
2
4
5
3
¶
−4ρ1+ρ2
−→
µ
1
1
2
0
1
−5
¶
to conclude that there are c1 and c2 giving the combination.
(b) No. The reduction


2
1
−1
1
0
0
0
1
1

−(1/2)ρ1+ρ2
−→


2
1
−1
0
−1/2
1/2
0
1
1

2ρ2+ρ3
−→


2
1
−1
0
−1/2
1/2
0
0
2


shows that
c1


2
1
0

+ c2


1
0
1

=


−1
0
1


has no solution.
(c) Yes. The reduction


1
2
3
4
1
0
1
3
2
3
4
5
0
1
0

−4ρ1+ρ3
−→


1
2
3
4
1
0
1
3
2
3
0
−3
−12
−15
−4

3ρ2+ρ3
−→


1
2
3
4
1
0
1
3
2
3
0
0
−3
−9
5



Answers to Exercises
229
shows that there are inﬁnitely many ways
{




c1
c2
c3
c4



=




−10
8
−5/3
0



+




−9
7
−3
1



c4
¯¯ c4 ∈R}
to write


1
3
0

= c1


1
0
4

+ c2


2
1
5

+ c3


3
3
0

+ c4


4
2
1

.
(d) No. Look at the third components.
One.I.3.22
Because the matrix of coeﬃcients is nonsingular, Gauss’ method ends with an echelon form
where each variable leads an equation. Back substitution gives a unique solution.
(Another way to see the solution is unique is to note that with a nonsingular matrix of coeﬃcients
the associated homogeneous system has a unique solution, by deﬁnition. Since the general solution is
the sum of a particular solution with each homogeneous solution, the general solution has (at most)
one element.)
One.I.3.23
In this case the solution set is all of Rn, and can be expressed in the required form
{c1





1
0
...
0




+ c2





0
1
...
0




+ · · · + cn





0
0
...
1





¯¯ c1, . . . , cn ∈R}.
One.I.3.24
Assume ⃗s,⃗t ∈Rn and write
⃗s =



s1
...
sn



and
⃗t =



t1
...
tn


.
Also let ai,1x1 + · · · + ai,nxn = 0 be the i-th equation in the homogeneous system.
(a) The check is easy:
ai,1(s1 + t1) + · · · + ai,n(sn + tn)
=
(ai,1s1 + · · · + ai,nsn) + (ai,1t1 + · · · + ai,ntn)
=
0 + 0.
(b) This one is similar:
ai,1(3s1) + · · · + ai,n(3sn) = 3(ai,1s1 + · · · + ai,nsn) = 3 · 0 = 0.
(c) This one is not much harder:
ai,1(ks1 + mt1) + · · · + ai,n(ksn + mtn)
=
k(ai,1s1 + · · · + ai,nsn) + m(ai,1t1 + · · · + ai,ntn)
=
k · 0 + m · 0.
What is wrong with that argument is that any linear combination of the zero vector yields the zero
vector again.
One.I.3.25
First the proof.
Gauss’ method will use only rationals (e.g., −(m/n)ρi +ρj). Thus the solution set can be expressed
using only rational numbers as the components of each vector.
Now the particular solution is all
rational.
There are inﬁnitely many (rational vector) solutions if and only if the associated homogeneous sys-
tem has inﬁnitely many (real vector) solutions. That’s because setting any parameters to be rationals
will produce an all-rational solution.
Subsection One.II.1: Vectors in Space

230
Linear Algebra, by Hefferon
One.II.1.1
(a)
µ
2
1
¶
(b)
µ
−1
2
¶
(c)


4
0
−3


(d)


0
0
0


One.II.1.2
(a) No, their canonical positions are diﬀerent.
µ
1
−1
¶
µ
0
3
¶
(b) Yes, their canonical positions are the same.

1
−1
3


One.II.1.3
That line is this set.
{




−2
1
1
0



+




7
9
−2
4



t
¯¯ t ∈R}
Note that this system
−2 + 7t = 1
1 + 9t = 0
1 −2t = 2
0 + 4t = 1
has no solution. Thus the given point is not in the line.
One.II.1.4
(a) Note that




2
2
2
0



−




1
1
5
−1



=




1
1
−3
1








3
1
0
4



−




1
1
5
−1



=




2
0
−5
5




and so the plane is this set.
{




1
1
5
−1



+




1
1
−3
1



t +




2
0
−5
5



s
¯¯ t, s ∈R}
(b) No; this system
1 + 1t + 2s = 0
1 + 1t
= 0
5 −3t −5s = 0
−1 + 1t + 5s = 0
has no solution.
One.II.1.5
The vector


2
0
3


is not in the line. Because


2
0
3

−


−1
0
−4

=


3
0
7


that plane can be described in this way.
{


−1
0
−4

+ m


1
1
2

+ n


3
0
7

¯¯ m, n ∈R}
One.II.1.6
The points of coincidence are solutions of this system.
t
= 1 + 2m
t + s = 1 + 3k
t + 3s =
4m

Answers to Exercises
231
Gauss’ method


1
0
0
−2
1
1
1
−3
0
1
1
3
0
−4
0

−ρ1+ρ2
−→
−ρ1+ρ3


1
0
0
−2
1
0
1
−3
2
0
0
3
0
−2
−1

−3ρ2+ρ3
−→


1
0
0
−2
1
0
1
−3
2
0
0
0
9
−8
−1


gives k = −(1/9) + (8/9)m, so s = −(1/3) + (2/3)m and t = 1 + 2m. The intersection is this.
{


1
1
0

+


0
3
0

(−1
9 + 8
9m) +


2
0
4

m
¯¯ m ∈R} = {


1
2/3
0

+


2
8/3
4

m
¯¯ m ∈R}
One.II.1.7
(a) The system
1 =
1
1 + t =
3 + s
2 + t = −2 + 2s
gives s = 6 and t = 8, so this is the solution set.
{


1
9
10

}
(b) This system
2 + t =
0
t = s + 4w
1 −t = 2s + w
gives t = −2, w = −1, and s = 2 so their intersection is this point.


0
−2
3


One.II.1.8
(a) The vector shown
is not the result of doubling


2
0
0

+


−0.5
1
0

· 1
instead it is


2
0
0

+


−0.5
1
0

· 2 =


1
2
0


which has a parameter twice as large.
(b) The vector
is not the result of adding
(


2
0
0

+


−0.5
1
0

· 1) + (


2
0
0

+


−0.5
0
1

· 1)

232
Linear Algebra, by Hefferon
instead it is


2
0
0

+


−0.5
1
0

· 1 +


−0.5
0
1

· 1 =


1
1
1


which adds the parameters.
One.II.1.9
The “if” half is straightforward. If b1 −a1 = d1 −c1 and b2 −a2 = d2 −c2 then
p
(b1 −a1)2 + (b2 −a2)2 =
p
(d1 −c1)2 + (d2 −c2)2
so they have the same lengths, and the slopes are just as easy:
b2 −a2
b1 −a1
= d2 −c2
d1 −a1
(if the denominators are 0 they both have undeﬁned slopes).
For “only if”, assume that the two segments have the same length and slope (the case of un-
deﬁned slopes is easy; we will do the case where both segments have a slope m).
Also assume,
without loss of generality, that a1 < b1 and that c1 < d1.
The ﬁrst segment is (a1, a2)(b1, b2) =
{(x, y)
¯¯ y = mx + n1, x ∈[a1..b1]} (for some intercept n1) and the second segment is (c1, c2)(d1, d2) =
{(x, y)
¯¯ y = mx + n2, x ∈[c1..d1]} (for some n2). Then the lengths of those segments are
p
(b1 −a1)2 + ((mb1 + n1) −(ma1 + n1))2 =
p
(1 + m2)(b1 −a1)2
and, similarly,
p
(1 + m2)(d1 −c1)2. Therefore, |b1−a1| = |d1−c1|. Thus, as we assumed that a1 < b1
and c1 < d1, we have that b1 −a1 = d1 −c1.
The other equality is similar.
One.II.1.10
We shall later deﬁne it to be a set with one element — an “origin”.
One.II.1.11
This is how the answer was given in the cited source. The vector triangle is as follows, so
⃗w = 3
√
2 from the north west.
-
@
@
@
@
R
⃗w
¡
¡
¡
¡

One.II.1.12
Euclid no doubt is picturing a plane inside of R3. Observe, however, that both R1 and
R3 also satisfy that deﬁnition.
Subsection One.II.2: Length and Angle Measures
One.II.2.10
(a)
√
32 + 12 =
√
10
(b)
√
5
(c)
√
18
(d) 0
(e)
√
3
One.II.2.11
(a) arccos(9/
√
85) ≈0.22 radians
(b) arccos(8/
√
85) ≈0.52 radians
(c) Not deﬁned.
One.II.2.12
We express each displacement as a vector (rounded to one decimal place because that’s
the accuracy of the problem’s statement) and add to ﬁnd the total displacement (ignoring the curvature
of the earth).
µ0.0
1.2
¶
+
µ 3.8
−4.8
¶
+
µ4.0
0.1
¶
+
µ3.3
5.6
¶
=
µ11.1
2.1
¶
The distance is
√
11.12 + 2.12 ≈11.3.
One.II.2.13
Solve (k)(4) + (1)(3) = 0 to get k = −3/4.
One.II.2.14
The set
{


x
y
z

¯¯ 1x + 3y −1z = 0}
can also be described with parameters in this way.
{


−3
1
0

y +


1
0
1

z
¯¯ y, z ∈R}

Answers to Exercises
233
One.II.2.15
(a) We can use the x-axis.
arccos((1)(1) + (0)(1)
√
1
√
2
) ≈0.79 radians
(b) Again, use the x-axis.
arccos((1)(1) + (0)(1) + (0)(1)
√
1
√
3
) ≈0.96 radians
(c) The x-axis worked before and it will work again.
arccos((1)(1) + · · · + (0)(1)
√
1√n
) = arccos( 1
√n)
(d) Using the formula from the prior item, limn→∞arccos(1/√n) = π/2 radians.
One.II.2.16
Clearly u1u1 + · · · + unun is zero if and only if each ui is zero.
So only ⃗0 ∈Rn is
perpendicular to itself.
One.II.2.17
Assume that ⃗u,⃗v, ⃗w ∈Rn have components u1, . . . , un, v1, . . . , wn.
(a) Dot product is right-distributive.
(⃗u + ⃗v)
⃗w = [



u1
...
un


+



v1
...
vn


]



w1
...
wn



=



u1 + v1
...
un + vn






w1
...
wn



= (u1 + v1)w1 + · · · + (un + vn)wn
= (u1w1 + · · · + unwn) + (v1w1 + · · · + vnwn)
= ⃗u
⃗w + ⃗v
⃗w
(b) Dot product is also left distributive: ⃗w (⃗u + ⃗v) = ⃗w ⃗u + ⃗w ⃗v. The proof is just like the prior
one.
(c) Dot product commutes.



u1
...
un






v1
...
vn


= u1v1 + · · · + unvn = v1u1 + · · · + vnun =



v1
...
vn






u1
...
un



(d) Because ⃗u ⃗v is a scalar, not a vector, the expression (⃗u ⃗v)
⃗w makes no sense; the dot product
of a scalar and a vector is not deﬁned.
(e) This is a vague question so it has many answers. Some are (1) k(⃗u ⃗v) = (k⃗u) ⃗v and k(⃗u ⃗v) =
⃗u (k⃗v), (2) k(⃗u ⃗v) ̸= (k⃗u) (k⃗v) (in general; an example is easy to produce), and (3) ∥k⃗v ∥= k2∥⃗v ∥
(the connection between norm and dot product is that the square of the norm is the dot product of
a vector with itself).
One.II.2.18
(a) Verifying that (k⃗x) ⃗y = k(⃗x ⃗y) = ⃗x (k⃗y) for k ∈R and ⃗x, ⃗y ∈Rn is easy. Now, for
k ∈R and ⃗v, ⃗w ∈Rn, if ⃗u = k⃗v then ⃗u ⃗v = (k⃗u) ⃗v = k(⃗v ⃗v), which is k times a nonnegative real.
The ⃗v = k⃗u half is similar (actually, taking the k in this paragraph to be the reciprocal of the k
above gives that we need only worry about the k = 0 case).
(b) We ﬁrst consider the ⃗u ⃗v ≥0 case. From the Triangle Inequality we know that ⃗u ⃗v = ∥⃗u ∥∥⃗v ∥if
and only if one vector is a nonnegative scalar multiple of the other. But that’s all we need because
the ﬁrst part of this exercise shows that, in a context where the dot product of the two vectors
is positive, the two statements ‘one vector is a scalar multiple of the other’ and ‘one vector is a
nonnegative scalar multiple of the other’, are equivalent.
We ﬁnish by considering the ⃗u
⃗v < 0 case. Because 0 < |⃗u
⃗v| = −(⃗u
⃗v) = (−⃗u)
⃗v and
∥⃗u ∥∥⃗v ∥= ∥−⃗u ∥∥⃗v ∥, we have that 0 < (−⃗u) ⃗v = ∥−⃗u ∥∥⃗v ∥. Now the prior paragraph applies to
give that one of the two vectors −⃗u and ⃗v is a scalar multiple of the other. But that’s equivalent to
the assertion that one of the two vectors ⃗u and ⃗v is a scalar multiple of the other, as desired.
One.II.2.19
No. These give an example.
⃗u =
µ
1
0
¶
⃗v =
µ
1
0
¶
⃗w =
µ
1
1
¶

234
Linear Algebra, by Hefferon
One.II.2.20
We prove that a vector has length zero if and only if all its components are zero.
Let ⃗u ∈Rn have components u1, . . . , un. Recall that the square of any real number is greater than
or equal to zero, with equality only when that real is zero. Thus ∥⃗u ∥2 = u12 + · · · + un2 is a sum of
numbers greater than or equal to zero, and so is itself greater than or equal to zero, with equality if
and only if each ui is zero. Hence ∥⃗u ∥= 0 if and only if all the components of ⃗u are zero.
One.II.2.21
We can easily check that
¡x1 + x2
2
, y1 + y2
2
¢
is on the line connecting the two, and is equidistant from both. The generalization is obvious.
One.II.2.22
Assume that ⃗v ∈Rn has components v1, . . . , vn. If ⃗v ̸= ⃗0 then we have this.
sµ
v1
√
v12 + · · · + vn2
¶2
+ · · · +
µ
vn
√
v12 + · · · + vn2
¶2
=
sµ
v12
v12 + · · · + vn2
¶
+ · · · +
µ
vn2
v12 + · · · + vn2
¶
= 1
If ⃗v = ⃗0 then ⃗v/∥⃗v ∥is not deﬁned.
One.II.2.23
For the ﬁrst question, assume that ⃗v ∈Rn and r ≥0, take the root, and factor.
∥r⃗v ∥=
p
(rv1)2 + · · · + (rvn)2 =
p
r2(v12 + · · · + vn2 = r∥⃗v ∥
For the second question, the result is r times as long, but it points in the opposite direction in that
r⃗v + (−r)⃗v = ⃗0.
One.II.2.24
Assume that ⃗u,⃗v ∈Rn both have length 1. Apply Cauchy-Schwartz: |⃗u ⃗v| ≤∥⃗u ∥∥⃗v ∥= 1.
To see that ‘less than’ can happen, in R2 take
⃗u =
µ
1
0
¶
⃗v =
µ
0
1
¶
and note that ⃗u ⃗v = 0. For ‘equal to’, note that ⃗u ⃗u = 1.
One.II.2.25
Write
⃗u =



u1
...
un



⃗v =



v1
...
vn



and then this computation works.
∥⃗u + ⃗v ∥2 + ∥⃗u −⃗v ∥2 = (u1 + v1)2 + · · · + (un + vn)2
+ (u1 −v1)2 + · · · + (un −vn)2
= u1
2 + 2u1v1 + v1
2 + · · · + un
2 + 2unvn + vn
2
+ u1
2 −2u1v1 + v1
2 + · · · + un
2 −2unvn + vn
2
= 2(u1
2 + · · · + un
2) + 2(v1
2 + · · · + vn
2)
= 2∥⃗u ∥2 + 2∥⃗v ∥2
One.II.2.26
We will prove this demonstrating that the contrapositive statement holds: if ⃗x ̸= ⃗0 then
there is a ⃗y with ⃗x ⃗y ̸= 0.
Assume that ⃗x ∈Rn. If ⃗x ̸= ⃗0 then it has a nonzero component, say the i-th one xi. But the
vector ⃗y ∈Rn that is all zeroes except for a one in component i gives ⃗x ⃗y = xi. (A slicker proof just
considers ⃗x ⃗x.)
One.II.2.27
Yes; we can prove this by induction.
Assume that the vectors are in some Rk. Clearly the statement applies to one vector. The Triangle
Inequality is this statement applied to two vectors. For an inductive step assume the statement is true
for n or fewer vectors. Then this
∥⃗u1 + · · · + ⃗un + ⃗un+1∥≤∥⃗u1 + · · · + ⃗un∥+ ∥⃗un+1∥
follows by the Triangle Inequality for two vectors. Now the inductive hypothesis, applied to the ﬁrst
summand on the right, gives that as less than or equal to ∥⃗u1∥+ · · · + ∥⃗un∥+ ∥⃗un+1∥.

Answers to Exercises
235
One.II.2.28
By deﬁnition
⃗u ⃗v
∥⃗u ∥∥⃗v ∥= cos θ
where θ is the angle between the vectors. Thus the ratio is | cos θ|.
One.II.2.29
So that the statement ‘vectors are orthogonal iﬀtheir dot product is zero’ has no excep-
tions.
One.II.2.30
The angle between (a) and (b) is found (for a, b ̸= 0) with
arccos(
ab
√
a2√
b2 ).
If a or b is zero then the angle is π/2 radians. Otherwise, if a and b are of opposite signs then the
angle is π radians, else the angle is zero radians.
One.II.2.31
The angle between ⃗u and ⃗v is acute if ⃗u ⃗v > 0, is right if ⃗u ⃗v = 0, and is obtuse if
⃗u ⃗v < 0. That’s because, in the formula for the angle, the denominator is never negative.
One.II.2.32
Suppose that ⃗u,⃗v ∈Rn. If ⃗u and ⃗v are perpendicular then
∥⃗u + ⃗v ∥2 = (⃗u + ⃗v) (⃗u + ⃗v) = ⃗u ⃗u + 2 ⃗u ⃗v + ⃗v ⃗v = ⃗u ⃗u + ⃗v ⃗v = ∥⃗u ∥2 + ∥⃗v ∥2
(the third equality holds because ⃗u ⃗v = 0).
One.II.2.33
Where ⃗u,⃗v ∈Rn, the vectors ⃗u + ⃗v and ⃗u −⃗v are perpendicular if and only if 0 =
(⃗u +⃗v) (⃗u −⃗v) = ⃗u ⃗u −⃗v ⃗v, which shows that those two are perpendicular if and only if ⃗u ⃗u = ⃗v ⃗v.
That holds if and only if ∥⃗u ∥= ∥⃗v ∥.
One.II.2.34
Suppose ⃗u ∈Rn is perpendicular to both ⃗v ∈Rn and ⃗w ∈Rn. Then, for any k, m ∈R
we have this.
⃗u (k⃗v + m⃗w) = k(⃗u ⃗v) + m(⃗u
⃗w) = k(0) + m(0) = 0
One.II.2.35
We will show something more general: if ∥⃗z1∥= ∥⃗z2∥for ⃗z1,⃗z2 ∈Rn, then ⃗z1 + ⃗z2 bisects
the angle between ⃗z1 and ⃗z2
©©©
©
*
¢
¢
¢¢
¡
¡
¡
¡
¡

¢
¢
¢¢
©©©
©
gives
©©©
©
¢
¢
¢¢
¡
¡
¡
¡
¡
¢
¢
¢¢
©©©
©
′
′
′′
′′′
′′′
(we ignore the case where ⃗z1 and ⃗z2 are the zero vector).
The ⃗z1 + ⃗z2 = ⃗0 case is easy. For the rest, by the deﬁnition of angle, we will be done if we show
this.
⃗z1 (⃗z1 + ⃗z2)
∥⃗z1∥∥⃗z1 + ⃗z2∥= ⃗z2 (⃗z1 + ⃗z2)
∥⃗z2∥∥⃗z1 + ⃗z2∥
But distributing inside each expression gives
⃗z1 ⃗z1 + ⃗z1 ⃗z2
∥⃗z1∥∥⃗z1 + ⃗z2∥
⃗z2 ⃗z1 + ⃗z2 ⃗z2
∥⃗z2∥∥⃗z1 + ⃗z2∥
and ⃗z1 ⃗z1 = ∥⃗z1∥= ∥⃗z2∥= ⃗z2 ⃗z2, so the two are equal.
One.II.2.36
We can show the two statements together. Let ⃗u,⃗v ∈Rn, write
⃗u =



u1
...
un



⃗v =



v1
...
vn



and calculate.
cos θ =
ku1v1 + · · · + kunvn
q
(ku1)2 + · · · + (kun)2p
b1
2 + · · · + bn
2 = k
|k|
⃗u · ⃗v
∥⃗u ∥∥⃗v ∥= ±
⃗u ⃗v
∥⃗u ∥∥⃗v ∥
One.II.2.37
Let
⃗u =



u1
...
un


,
⃗v =



v1
...
vn



⃗w =



w1
...
wn




236
Linear Algebra, by Hefferon
and then
⃗u
¡
k⃗v + m⃗w
¢
=



u1
...
un



¡



kv1
...
kvn


+



mw1
...
mwn



¢
=



u1
...
un






kv1 + mw1
...
kvn + mwn



= u1(kv1 + mw1) + · · · + un(kvn + mwn)
= ku1v1 + mu1w1 + · · · + kunvn + munwn
= (ku1v1 + · · · + kunvn) + (mu1w1 + · · · + munwn)
= k(⃗u ⃗v) + m(⃗u
⃗w)
as required.
One.II.2.38
For x, y ∈R+, set
⃗u =
µ√x
√y
¶
⃗v =
µ√y
√x
¶
so that the Cauchy-Schwartz inequality asserts that (after squaring)
(√x√y + √y√x)2 ≤(√x√x + √y√y)(√y√y + √x√x)
(2√x√y)2 ≤(x + y)2
√xy ≤x + y
2
as desired.
One.II.2.39
This is how the answer was given in the cited source. The actual velocity ⃗v of the wind
is the sum of the ship’s velocity and the apparent velocity of the wind. Without loss of generality we
may assume ⃗a and ⃗b to be unit vectors, and may write
⃗v = ⃗v1 + s⃗a = ⃗v2 + t⃗b
where s and t are undetermined scalars. Take the dot product ﬁrst by ⃗a and then by ⃗b to obtain
s −t⃗a ⃗b = ⃗a (⃗v2 −⃗v1)
s⃗a ⃗b −t = ⃗b (⃗v2 −⃗v1)
Multiply the second by ⃗a ⃗b, subtract the result from the ﬁrst, and ﬁnd
s = [⃗a −(⃗a ⃗b)⃗b] (⃗v2 −⃗v1)
1 −(⃗a ⃗b)2
.
Substituting in the original displayed equation, we get
⃗v = ⃗v1 + [⃗a −(⃗a ⃗b)⃗b] (⃗v2 −⃗v1)⃗a
1 −(⃗a ⃗b)2
.
One.II.2.40
We use induction on n.
In the n = 1 base case the identity reduces to
(a1b1)2 = (a1
2)(b1
2) −0
and clearly holds.
For the inductive step assume that the formula holds for the 0, . . . , n cases. We will show that it

Answers to Exercises
237
then holds in the n + 1 case. Start with the right-hand side
¡
X
1≤j≤n+1
aj
2¢¡
X
1≤j≤n+1
bj
2¢
−
X
1≤k<j≤n+1
¡
akbj −ajbk
¢2
=
£
(
X
1≤j≤n
aj
2) + an+1
2¤£
(
X
1≤j≤n
bj
2) + bn+1
2¤
−
£
X
1≤k<j≤n
¡
akbj −ajbk
¢2 +
X
1≤k≤n
¡
akbn+1 −an+1bk
¢2¤
=
¡ X
1≤j≤n
aj
2¢¡ X
1≤j≤n
bj
2¢
+
X
1≤j≤n
bj
2an+1
2 +
X
1≤j≤n
aj
2bn+1
2 + an+1
2bn+1
2
−
£
X
1≤k<j≤n
¡
akbj −ajbk
¢2 +
X
1≤k≤n
¡
akbn+1 −an+1bk
¢2¤
=
¡ X
1≤j≤n
aj
2¢¡ X
1≤j≤n
bj
2¢
−
X
1≤k<j≤n
¡
akbj −ajbk
¢2
+
X
1≤j≤n
bj
2an+1
2 +
X
1≤j≤n
aj
2bn+1
2 + an+1
2bn+1
2
−
X
1≤k≤n
¡
akbn+1 −an+1bk
¢2
and apply the inductive hypothesis
=
¡ X
1≤j≤n
ajbj
¢2 +
X
1≤j≤n
bj
2an+1
2 +
X
1≤j≤n
aj
2bn+1
2 + an+1
2bn+1
2
−
£ X
1≤k≤n
ak
2bn+1
2 −2
X
1≤k≤n
akbn+1an+1bk +
X
1≤k≤n
an+1
2bk
2¤
=
¡ X
1≤j≤n
ajbj
¢2 −2
¡ X
1≤k≤n
akbn+1an+1bk
¢
+ an+1
2bn+1
2
=
£¡ X
1≤j≤n
ajbj
¢
+ an+1bn+1
¤2
to derive the left-hand side.
Subsection One.III.1: Gauss-Jordan Reduction
One.III.1.7
These answers show only the Gauss-Jordan reduction. With it, describing the solution
set is easy.
(a)
µ
1
1
2
1
−1
0
¶
−ρ1+ρ2
−→
µ
1
1
2
0
−2
−2
¶
−(1/2)ρ2
−→
µ
1
1
2
0
1
1
¶
−ρ2+ρ1
−→
µ
1
0
1
0
1
1
¶
(b)
µ
1
0
−1
4
2
2
0
1
¶
−2ρ1+ρ2
−→
µ
1
0
−1
4
0
2
2
−7
¶
(1/2)ρ2
−→
µ
1
0
−1
4
0
1
1
−7/2
¶
(c)µ3
−2
1
6
1
1/2
¶
−2ρ1+ρ2
−→
µ3
−2
1
0
5
−3/2
¶
(1/3)ρ1
−→
(1/5)ρ2
µ1
−2/3
1/3
0
1
−3/10
¶
(2/3)ρ2+ρ1
−→
µ1
0
2/15
0
1
−3/10
¶
(d) A row swap here makes the arithmetic easier.


2
−1
0
−1
1
3
−1
5
0
1
2
5

−(1/2)ρ1+ρ2
−→


2
−1
0
−1
0
7/2
−1
11/2
0
1
2
5

ρ2↔ρ3
−→


2
−1
0
−1
0
1
2
5
0
7/2
−1
11/2


−(7/2)ρ2+ρ3
−→


2
−1
0
−1
0
1
2
5
0
0
−8
−12


(1/2)ρ1
−→
−(1/8)ρ2


1
−1/2
0
−1/2
0
1
2
5
0
0
1
3/2


−2ρ3+ρ2
−→


1
−1/2
0
−1/2
0
1
0
2
0
0
1
3/2

(1/2)ρ2+ρ1
−→


1
0
0
1/2
0
1
0
2
0
0
1
3/2



238
Linear Algebra, by Hefferon
One.III.1.8
Use Gauss-Jordan reduction.
(a)
−(1/2)ρ1+ρ2
−→
µ
2
1
0
5/2
¶
(1/2)ρ1
−→
(2/5)ρ2
µ
1
1/2
0
1
¶
−(1/2)ρ2+ρ1
−→
µ
1
0
0
1
¶
(b)
−2ρ1+ρ2
−→
ρ1+ρ3


1
3
1
0
−6
2
0
0
−2

−(1/6)ρ2
−→
−(1/2)ρ3


1
3
1
0
1
−1/3
0
0
1

(1/3)ρ3+ρ2
−→
−ρ3+ρ1


1
3
0
0
1
0
0
0
1

−3ρ2+ρ1
−→


1
0
0
0
1
0
0
0
1


(c)
−ρ1+ρ2
−→
−3ρ1+ρ3


1
0
3
1
2
0
4
−1
0
3
0
4
−1
−2
−4

−ρ2+ρ3
−→


1
0
3
1
2
0
4
−1
0
3
0
0
0
−2
−7


(1/4)ρ2
−→
−(1/2)ρ3


1
0
3
1
2
0
1
−1/4
0
3/4
0
0
0
1
7/2

−ρ3+ρ1
−→


1
0
3
0
−3/2
0
1
−1/4
0
3/4
0
0
0
1
7/2


(d)
ρ1↔ρ3
−→


1
5
1
5
0
0
5
6
0
1
3
2

ρ2↔ρ3
−→


1
5
1
5
0
1
3
2
0
0
5
6

(1/5)ρ3
−→


1
5
1
5
0
1
3
2
0
0
1
6/5


−3ρ3+ρ2
−→
−ρ3+ρ1


1
5
0
19/5
0
1
0
−8/5
0
0
1
6/5

−5ρ2+ρ1
−→


1
0
0
59/5
0
1
0
−8/5
0
0
1
6/5


One.III.1.9
For the “Gauss” halves, see the answers to Exercise 19.
(a) The “Jordan” half goes this way.
(1/2)ρ1
−→
−(1/3)ρ2
µ
1
1/2
−1/2
1/2
0
1
−2/3
−1/3
¶
−(1/2)ρ2+ρ1
−→
µ
1
0
−1/6
2/3
0
1
−2/3
−1/3
¶
The solution set is this
{


2/3
−1/3
0

+


1/6
2/3
1

z
¯¯ z ∈R}
(b) The second half is
ρ3+ρ2
−→


1
0
−1
0
1
0
1
2
0
3
0
0
0
1
0


so the solution is this.
{




1
3
0
0



+




1
−2
1
0



z
¯¯ z ∈R}
(c) This Jordan half
ρ2+ρ1
−→




1
0
1
1
0
0
1
0
1
0
0
0
0
0
0
0
0
0
0
0




gives
{




0
0
0
0



+




−1
0
1
0



z +




−1
−1
0
1



w
¯¯ z, w ∈R}
(of course, the zero vector could be omitted from the description).
(d) The “Jordan” half
−(1/7)ρ2
−→
µ
1
2
3
1
−1
1
0
1
8/7
2/7
−4/7
0
¶
−2ρ2+ρ1
−→
µ
1
0
5/7
3/7
1/7
1
0
1
8/7
2/7
−4/7
0
¶
ends with this solution set.
{






1
0
0
0
0






+






−5/7
−8/7
1
0
0






c +






−3/7
−2/7
0
1
0






d +






−1/7
4/7
0
0
1






e
¯¯ c, d, e ∈R}

Answers to Exercises
239
One.III.1.10
Routine Gauss’ method gives one:
−3ρ1+ρ2
−→
−(1/2)ρ1+ρ3


2
1
1
3
0
1
−2
−7
0
9/2
1/2
7/2

−(9/2)ρ2+ρ3
−→


2
1
1
3
0
1
−2
−7
0
0
19/2
35


and any cosmetic change, like multiplying the bottom row by 2,


2
1
1
3
0
1
−2
−7
0
0
19
70


gives another.
One.III.1.11
In the cases listed below, we take a, b ∈R. Thus, some canonical forms listed below
actually include inﬁnitely many cases. In particular, they includes the cases a = 0 and b = 0.
(a)
µ
0
0
0
0
¶
,
µ
1
a
0
0
¶
,
µ
0
1
0
0
¶
,
µ
1
0
0
1
¶
(b)
µ
0
0
0
0
0
0
¶
,
µ
1
a
b
0
0
0
¶
,
µ
0
1
a
0
0
0
¶
,
µ
0
0
1
0
0
0
¶
,
µ
1
0
a
0
1
b
¶
,
µ
1
a
0
0
0
1
¶
,
µ
0
1
0
0
0
1
¶
(c)


0
0
0
0
0
0

,


1
a
0
0
0
0

,


0
1
0
0
0
0

,


1
0
0
1
0
0


(d)


0
0
0
0
0
0
0
0
0

,


1
a
b
0
0
0
0
0
0

,


0
1
a
0
0
0
0
0
0

,


0
0
1
0
0
0
0
0
0

,


1
0
a
0
1
b
0
0
0

,


1
a
0
0
0
1
0
0
0

,


1
0
0
0
1
0
0
0
1


One.III.1.12
A nonsingular homogeneous linear system has a unique solution. So a nonsingular matrix
must reduce to a (square) matrix that is all 0’s except for 1’s down the upper-left to lower-right diagonal,
e.g.,
µ
1
0
0
1
¶
,
or


1
0
0
0
1
0
0
0
1

,
etc.
One.III.1.13
(a) The ρi ↔ρi operation does not change A.
(b) For instance,
µ
1
2
3
4
¶
−ρ1+ρ1
−→
µ
0
0
3
4
¶
ρ1+ρ1
−→
µ
0
0
3
4
¶
leaves the matrix changed.
(c) If i ̸= j then









...
ai,1
· · ·
ai,n
...
aj,1
· · ·
aj,n
...









kρi+ρj
−→









...
ai,1
· · ·
ai,n
...
kai,1 + aj,1
· · ·
kai,n + aj,n
...









−kρi+ρj
−→









...
ai,1
· · ·
ai,n
...
−kai,1 + kai,1 + aj,1
· · ·
−kai,n + kai,n + aj,n
...









does indeed give A back. (Of course, if i = j then the third matrix would have entries of the form
−k(kai,j + ai,j) + kai,j + ai,j.)
Subsection One.III.2: Row Equivalence
One.III.2.11
Bring each to reduced echelon form and compare.

240
Linear Algebra, by Hefferon
(a) The ﬁrst gives
−4ρ1+ρ2
−→
µ
1
2
0
0
¶
while the second gives
ρ1↔ρ2
−→
µ
1
2
0
1
¶
−2ρ2+ρ1
−→
µ
1
0
0
1
¶
The two reduced echelon form matrices are not identical, and so the original matrices are not row
equivalent.
(b) The ﬁrst is this.
−3ρ1+ρ2
−→
−5ρ1+ρ3


1
0
2
0
−1
−5
0
−1
−5

−ρ2+ρ3
−→


1
0
2
0
−1
−5
0
0
0

−ρ2
−→


1
0
2
0
1
5
0
0
0


The second is this.
−2ρ1+ρ3
−→


1
0
2
0
2
10
0
0
0

(1/2)ρ2
−→


1
0
2
0
1
5
0
0
0


These two are row equivalent.
(c) These two are not row equivalent because they have diﬀerent sizes.
(d) The ﬁrst,
ρ1+ρ2
−→
µ
1
1
1
0
3
3
¶
(1/3)ρ2
−→
µ
1
1
1
0
1
1
¶
−ρ2+ρ1
−→
µ
1
0
0
0
1
1
¶
and the second.
ρ1↔ρ2
−→
µ2
2
5
0
3
−1
¶
(1/2)ρ1
−→
(1/3)ρ2
µ1
1
5/2
0
1
−1/3
¶
−ρ2+ρ1
−→
µ1
0
17/6
0
1
−1/3
¶
These are not row equivalent.
(e) Here the ﬁrst is
(1/3)ρ2
−→
µ
1
1
1
0
0
1
¶
−ρ2+ρ1
−→
µ
1
1
0
0
0
1
¶
while this is the second.
ρ1↔ρ2
−→
µ
1
−1
1
0
1
2
¶
ρ2+ρ1
−→
µ
1
0
3
0
1
2
¶
These are not row equivalent.
One.III.2.12
First, the only matrix row equivalent to the matrix of all 0’s is itself (since row operations
have no eﬀect).
Second, the matrices that reduce to
µ
1
a
0
0
¶
have the form
µ
b
ba
c
ca
¶
(where a, b, c ∈R).
Next, the matrices that reduce to
µ
0
1
0
0
¶
have the form
µ
0
a
0
b
¶
(where a, b ∈R).
Finally, the matrices that reduce to
µ
1
0
0
1
¶
are the nonsingular matrices. That’s because a linear system for which this is the matrix of coeﬃcients
will have a unique solution, and that is the deﬁnition of nonsingular. (Another way to say the same
thing is to say that they fall into none of the above classes.)

Answers to Exercises
241
One.III.2.13
(a) They have the form
µ
a
0
b
0
¶
where a, b ∈R.
(b) They have this form (for a, b ∈R).
µ
1a
2a
1b
2b
¶
(c) They have the form
µ
a
b
c
d
¶
(for a, b, c, d ∈R) where ad −bc ̸= 0. (This is the formula that determines when a 2×2 matrix is
nonsingular.)
One.III.2.14
Inﬁnitely many. For instance, in µ1
k
0
0
¶
each k ∈R gives a diﬀerent class.
One.III.2.15
No. Row operations do not change the size of a matrix.
One.III.2.16
(a) A row operation on a zero matrix has no eﬀect. Thus each zero matrix is alone in
its row equivalence class.
(b) No. Any nonzero entry can be rescaled.
One.III.2.17
Here are two.
µ1
1
0
0
0
1
¶
and
µ1
0
0
0
0
1
¶
One.III.2.18
Any two n×n nonsingular matrices have the same reduced echelon form, namely the
matrix with all 0’s except for 1’s down the diagonal.





1
0
0
0
1
0
...
0
0
1





Two 2×2 singular matrices need not be row equivalent.
µ
1
1
0
0
¶
and
µ
1
0
0
0
¶
One.III.2.19
Since there is one and only one reduced echelon form matrix in each class, we can just
list the possible reduced echelon form matrices.
For that list, see the answer for Exercise 11.
One.III.2.20
(a) If there is a linear relationship where c0 is not zero then we can subtract c0⃗β0 and
divide both sides by c0 to get ⃗β0 as a linear combination of the others. (Remark. If there are no
others — if the relationship is, say, ⃗0 = 3 · ⃗0 — then the statement is still true because zero is by
deﬁnition the sum of the empty set of vectors.)
If ⃗β0 is a combination of the others ⃗β0 = c1⃗β1 + · · · + cn⃗βn then subtracting ⃗β0 from both sides
gives a relationship where one of the coeﬃcients is nonzero, speciﬁcally, the coeﬃcient is −1.
(b) The ﬁrst row is not a linear combination of the others for the reason given in the proof: in the
equation of components from the column containing the leading entry of the ﬁrst row, the only
nonzero entry is the leading entry from the ﬁrst row, so its coeﬃcient must be zero. Thus, from the
prior part of this question, the ﬁrst row is in no linear relationship with the other rows. Hence, to
see if the second row can be in a linear relationship with the other rows, we can leave the ﬁrst row
out of the equation. But now the argument just applied to the ﬁrst row will apply to the second
row. (Technically, we are arguing by induction here.)
One.III.2.21
(a) As in the base case we will argue that ℓ2 isn’t less than k2 and that it also isn’t
greater. To obtain a contradiction, assume that ℓ2 ≤k2 (the k2 ≤ℓ2 case, and the possibility that
either or both is a zero row, are left to the reader). Consider the i = 2 version of the equation
that gives each row of B as a linear combination of the rows of D. Focus on the ℓ1-th and ℓ2-th
component equations.
b2,ℓ1 = c2,1d1,ℓ1 + c2,2d2,ℓ1 + · · · + c2,mdm,ℓ1b2,ℓ2
= c2,1d1,ℓ2 + c2,2d2,ℓ2 + · · · + c2,mdm,ℓ2

242
Linear Algebra, by Hefferon
The ﬁrst of these equations shows that c2,1 is zero because δ1,ℓ1 is not zero, but since both matrices
are in echelon form, each of the entries d2,ℓ1, . . . , dm,ℓ1, and b2,ℓ1 is zero. Now, with the second
equation, b2,ℓ2 is nonzero as it leads its row, c2,1 is zero by the prior sentence, and each of d3,ℓ2,
. . . , dm,ℓ2 is zero because D is in echelon form and we’ve assumed that ℓ2 ≤k2. Thus, this second
equation shows that d2,ℓ2 is nonzero and so k2 ≤ℓ2. Therefore k2 = ℓ2.
(b) For the inductive step assume that ℓ1 = k1, . . . , ℓj = kj (where 1 ≤j < m); we will show that
implies ℓj+1 = kj+1.
We do the ℓj+1 ≤kj+1 < ∞case here — the other cases are then easy. Consider the ρj+1 version
of the vector equation:
¡0
. . .
0
βj+1,ℓj1
. . .
βj+1,n
¢
=
cj+1,1
¡
0
. . .
δ1,k1
. . .
δ1,kj
. . .
δ1,kj+1
. . .
δ1,km
. . .
¢
...
+ cj+1,j
¡
0
. . .
0
. . .
δj,kj
. . .
δj,kj+1
. . .
δj,km
. . .
¢
+ cj+1,j+1
¡
0
. . .
0
. . .
0
. . .
δj+1,kj+1
. . .
δj+1,km
. . .
¢
...
+ cj+1,m
¡0
. . .
0
. . .
0
. . .
0
. . .
δm,km
. . .¢
Knowing that ℓ1 = k1, . . . , ℓj = kj, consider the ℓ1-th, . . . , ℓj-th component equations.
0 = cj+1,1δ1,k1 + cj+1,2 · 0 + · · · + cj+1,j · 0 + cj+1,j+1 · 0 + · · · + cj+1,m · 0
0 = cj+1,1δ1,k2 + cj+1,2δ2,kj · · · + cj+1,j · 0 + cj+1,j+1 · 0 + · · · + cj+1,m · 0
...
0 = cj+1,1δ1,kj + cj+1,2δ2,k2 · · · + cj+1,jδj,kj + cj+1,j+1 · 0 + · · · + cj+1,m · 0
We can conclude that cj+1,1, . . . , cj+1,j are all zero.
Now look at the ℓj+1-th component equation:
βj+1,ℓj+1 = cj+1,j+1δj+1,ℓj+1 + cj+1,j+2δj+2,ℓj+1 + · · · + cj+1,mδm,ℓj+1.
Because D is in echelon form and because ℓj+1 ≤kj+1, each of δj+2,ℓj+1, . . . , δm,ℓj+1 is zero. But
βj+1,ℓj+1 is nonzero since it leads its row, and so δj+1,ℓj+1 is nonzero.
Conclusion: kj+1 ≤ℓj+1 and so kj+1 = ℓj+1.
(c) From the prior answer, we know that for any echelon form matrix, if this relationship holds
among the non-zero rows:
ρi = c1ρ1 + · · · + ci−1ρi−1 + ci+1ρi+1 + · · · + cnρn
(where c1, . . . , cn ∈R) then c1,. . . , ci−1 must all be zero (in the i = 1 case we don’t know any of the
scalars are zero).
To derive a contradiction suppose the above relationship exists and let ℓi be the column index
of the leading entry of ρi. Consider the equation of ℓi-th components:
ρi,ℓi = ci+1ρi+1,ℓi + · · · + cnρn,ℓi
and observe that because the matrix is in echelon form each of ρi+1,ℓi, . . . , ρn,ℓi is zero. But that’s
a contradiction as ρi,ℓi is nonzero since it leads the i-th row.
Hence the linear relationship supposed to exist among the rows is not possible.
One.III.2.22
(a) The inductive step is to show that if the statement holds on rows 1 through r then
it also holds on row r + 1. That is, we assume that ℓ1 = k1, and ℓ2 = k2, . . . , and ℓr = kr, and we
will show that ℓr+1 = kr+1 also holds (for r in 1 .. m −1).
(b) Lemma 2.3 gives the relationship βr+1 = sr+1,1δ1 + sr+2,2δ2 + · · · + sr+1,mδm between rows.
Inside of those rows, consider the relationship between entries in column ℓ1 = k1. Because r +1 > 1,
the row βr+1 has a zero in that entry (the matrix B is in echelon form), while the row δ1 has
a nonzero entry in column k1 (it is, by deﬁnition of k1, the leading entry in the ﬁrst row of D).
Thus, in that column, the above relationship among rows resolves to this equation among numbers:
0 = sr+1,1 · d1,k1, with d1,k1 ̸= 0. Therefore sr+1,1 = 0.
With sr+1,1 = 0, a similar argument shows that sr+1,2 = 0. With those two, another turn gives
that sr+1,3 = 0. That is, inside of the larger induction argument used to prove the entire lemma is
here an subargument by induction that shows sr+1,j = 0 for all j in 1 .. r. (We won’t write out the
details since it is just like the induction done in Exercise 21.)

Answers to Exercises
243
(c) First, ℓr+1 < kr+1 is impossible. In the columns of D to the left of column kr+1 the entries are
are all zeroes as dr+1,kr+1 leads the row k + 1) and so if ℓk+1 < kk+1 then the equation of entries
from column ℓk+1 would be br+1,ℓr+1 = sr+1,1 · 0 + · · · + sr+1,m · 0, but br+1,ℓr+1 isn’t zero since it
leads its row. A symmetric argument shows that kr+1 < ℓr+1 also is impossible.
One.III.2.23
The zero rows could have nonzero coeﬃcients, and so the statement would not be true.
One.III.2.24
We know that 4s + c + 10d = 8.45 and that 3s + c + 7d = 6.30, and we’d like to know
what s + c + d is. Fortunately, s + c + d is a linear combination of 4s + c + 10d and 3s + c + 7d. Calling
the unknown price p, we have this reduction.


4
1
10
8.45
3
1
7
6.30
1
1
1
p

−(3/4)ρ1+ρ2
−→
−(1/4)ρ1+ρ3


4
1
10
8.45
0
1/4
−1/2
−0.037 5
0
3/4
−3/2
p −2.112 5

−3ρ2+ρ3
−→


4
1
10
8.45
0
1/4
−1/2
−0.037 5
0
0
0
p −2.00


The price paid is $2.00.
One.III.2.25
If multiplication of a row by zero were allowed then Lemma 2.6 would not hold. That
is, where
µ
1
3
2
1
¶
0ρ2
−→
µ
1
3
0
0
¶
all the rows of the second matrix can be expressed as linear combinations of the rows of the ﬁrst, but
the converse does not hold. The second row of the ﬁrst matrix is not a linear combination of the rows
of the second matrix.
One.III.2.26
(1)
An easy answer is this:
0 = 3.
For a less wise-guy-ish answer, solve the system:
µ3
−1
8
2
1
3
¶
−(2/3)ρ1+ρ2
−→
µ3
−1
8
0
5/3
−7/3
¶
gives y = −7/5 and x = 11/5. Now any equation not satisﬁed by (−7/5, 11/5) will do, e.g.,
5x + 5y = 3.
(2)
Every equation can be derived from an inconsistent system. For instance, here is how to derive
“3x + 2y = 4” from “0 = 5”. First,
0 = 5
(3/5)ρ1
−→0 = 3
xρ1
−→0 = 3x
(validity of the x = 0 case is separate but clear). Similarly, 0 = 2y. Ditto for 0 = 4. But now,
0 + 0 = 0 gives 3x + 2y = 4.
One.III.2.27
Deﬁne linear systems to be equivalent if their augmented matrices are row equivalent.
The proof that equivalent systems have the same solution set is easy.
One.III.2.28
(a) The three possible row swaps are easy, as are the three possible rescalings. One of
the six possible pivots is kρ1 + ρ2:

1
2
3
k · 1 + 3
k · 2 + 0
k · 3 + 3
1
4
5


and again the ﬁrst and second columns add to the third. The other ﬁve pivots are similar.
(b) The obvious conjecture is that row operations do not change linear relationships among columns.
(c) A case-by-case proof follows the sketch given in the ﬁrst item.
Topic: Computer Algebra Systems
1
(a) The commands
> A:=array( [[40,15],
[-50,25]] );
> u:=array([100,50]);
> linsolve(A,u);
yield the answer [1, 4].
(b) Here there is a free variable:

244
Linear Algebra, by Hefferon
> A:=array( [[7,0,-7,0],
[8,1,-5,2],
[0,1,-3,0],
[0,3,-6,-1]] );
> u:=array([0,0,0,0]);
> linsolve(A,u);
prompts the reply [ t1, 3 t1, t1, 3 t1].
2
These are easy to type in. For instance, the ﬁrst
> A:=array( [[2,2],
[1,-4]] );
> u:=array([5,0]);
> linsolve(A,u);
gives the expected answer of [2, 1/2]. The others are entered similarly.
(a) The answer is x = 2 and y = 1/2.
(b) The answer is x = 1/2 and y = 3/2.
(c) This system has inﬁnitely many solutions. In the ﬁrst subsection, with z as a parameter, we got
x = (43−7z)/4 and y = (13−z)/4. Maple responds with [−12+7 t1, t1, 13−4 t1], for some reason
preferring y as a parameter.
(d) There is no solution to this system. When the array A and vector u are given to Maple and it
is asked to linsolve(A,u), it returns no result at all, that is, it responds with no solutions.
(e) The solutions is (x, y, z) = (5, 5, 0).
(f) There are many solutions. Maple gives [1, −1 + t1, 3 −t1, t1].
3
As with the prior question, entering these is easy.
(a) This system has inﬁnitely many solutions. In the second subsection we gave the solution set as
{
µ
6
0
¶
+
µ
−2
1
¶
y
¯¯ y ∈R}
and Maple responds with [6 −2 t1, t1].
(b) The solution set has only one member
{
µ
0
1
¶
}
and Maple has no trouble ﬁnding it [0, 1].
(c) This system’s solution set is inﬁnite
{


4
−1
0

+


−1
1
1

x3
¯¯ x3 ∈R}
and Maple gives [ t1, −t1 + 3, −t1 + 4].
(d) There is a unique solution
{


1
1
1

}
and Maple gives [1, 1, 1].
(e) This system has inﬁnitely many solutions; in the second subsection we described the solution set
with two parameters
{




5/3
2/3
0
0



+




−1/3
2/3
1
0



z +




−2/3
1/3
0
1



w
¯¯ z, w ∈R}
as does Maple [3 −2 t1 + t2, t1, t2, −2 + 3 t1 −2 t2].
(f) The solution set is empty and Maple replies to the linsolve(A,u) command with no returned
solutions.
4
In response to this prompting
> A:=array( [[a,c],
[b,d]] );
> u:=array([p,q]);
> linsolve(A,u);

Answers to Exercises
245
Maple thought for perhaps twenty seconds and gave this reply.
£
−−d p + q c
−b c + a d, −b p + a q
−b c + a d
¤
Topic: Input-Output Analysis
1
These answers were given by Octave.
(a) With the external use of steel as 17 789 and the external use of autos as 21 243, we get s = 25 952,
a = 30 312.
(b) s = 25 857, a = 30 596
(c) s = 25 984, a = 30 597
2
Octave gives these answers.
(a) s = 24 244, a = 30 307
(b) s = 24 267, a = 30 673
3
(a) These are the equations.
(11.79/18.69)s −(1.28/4.27)a = 11.56
−(0/18.69)s + (9.87/4.27)a = 11.35
Octave gives s = 20.66 and a = 16.41.
(b) These are the ratios.
1947
by steel
by autos
use of steel
0.63
0.09
use of autos
0.00
0.69
1958
by steel
by autos
use of steel
0.79
0.09
use of autos
0.00
0.70
(c) Octave gives (in billions of 1947 dollars) s = 24.82 and a = 23.63. In billions of 1958 dollars that
is s = 32.26 and a = 30.71.
Topic: Accuracy of Computations
1
Sceintiﬁc notation is convienent to express the two-place restriction. We have .25 × 102 + .67 × 100 =
.25 × 102. The 2/3 has no apparent eﬀect.
2
The reduction
−3ρ1+ρ2
−→
x + 2y =
3
−8 = −7.992
gives a solution of (x, y) = (1.002, 0.999).
3
(a) The fully accurate solution is that x = 10 and y = 0.
(b) The four-digit conclusion is quite diﬀerent.
−(.3454/.0003)ρ1+ρ2
−→
µ
.0003
1.556
1.569
0
1789
−1805
¶
=⇒x = 10460, y = −1.009
4
(a) For the ﬁrst one, ﬁrst, (2/3) −(1/3) is .666 666 67 −.333 333 33 = .333 333 34 and so (2/3) +
((2/3) −(1/3)) = .666 666 67 + .333 333 34 = 1.000 000 0. For the other one, ﬁrst ((2/3) + (2/3)) =
.666 666 67 + .666 666 67 = 1.333 333 3 and so ((2/3) + (2/3)) −(1/3) = 1.333 333 3 −.333 333 33 =
.999 999 97.
(b) The ﬁrst equation is .333 333 33 · x + 1.000 000 0 · y = 0 while the second is .666 666 67 · x +
2.000 000 0 · y = 0.
5
(a) This calculation
−(2/3)ρ1+ρ2
−→
−(1/3)ρ1+ρ3


3
2
1
6
0
−(4/3) + 2ε
−(2/3) + 2ε
−2 + 4ε
0
−(2/3) + 2ε
−(1/3) −ε
−1 + ε


−(1/2)ρ2+ρ3
−→


3
2
1
6
0
−(4/3) + 2ε
−(2/3) + 2ε
−2 + 4ε
0
ε
−2ε
−ε


gives a third equation of y−2z = −1. Substituting into the second equation gives ((−10/3)+6ε)·z =
(−10/3) + 6ε so z = 1 and thus y = 1. With those, the ﬁrst equation says that x = 1.

246
Linear Algebra, by Hefferon
(b) The solution with two digits kept


.30 × 101
.20 × 101
.10 × 101
.60 × 101
.10 × 101
.20 × 10−3
.20 × 10−3
.20 × 101
.30 × 101
.20 × 10−3
−.10 × 10−3
.10 × 101


−(2/3)ρ1+ρ2
−→
−(1/3)ρ1+ρ3


.30 × 101
.20 × 101
.10 × 101
.60 × 101
0
−.13 × 101
−.67 × 100
−.20 × 101
0
−.67 × 100
−.33 × 100
−.10 × 101


−(.67/1.3)ρ2+ρ3
−→


.30 × 101
.20 × 101
.10 × 101
.60 × 101
0
−.13 × 101
−.67 × 100
−.20 × 101
0
0
.15 × 10−2
.31 × 10−2


comes out to be z = 2.1, y = 2.6, and x = −.43.
Topic: Analyzing Networks
1
(a) The total resistance is 7 ohms. With a 9 volt potential, the ﬂow will be 9/7 amperes. Inciden-
tally, the voltage drops will then be: 27/7 volts across the 3 ohm resistor, and 18/7 volts across each
of the two 2 ohm resistors.
(b) One way to do this network is to note that the 2 ohm resistor on the left has a voltage drop
across it of 9 volts (and hence the ﬂow through it is 9/2 amperes), and the remaining portion on
the right also has a voltage drop of 9 volts, and so is analyzed as in the prior item.
We can also use linear systems.
−→
i0
i1 ↓
−→
i2
i3
←−
Using the variables from the diagram we get a linear system
i0 −i1 −i2
= 0
i1 + i2 −i3 = 0
2i1
= 9
7i2
= 9
which yields the unique solution i1 = 81/14, i1 = 9/2, i2 = 9/7, and i3 = 81/14.
Of course, the ﬁrst and second paragraphs yield the same answer. Esentially, in the ﬁrst para-
graph we solved the linear system by a method less systematic than Gauss’ method, solving for some
of the variables and then substituting.
(c) Using these variables
−→
i0
i1 ↓
−→
i2
−→
i3
i4 ↓
i5
←−
i6
←−
one linear system that suﬃces to yield a unique solution is this.
i0 −i1 −i2
= 0
i2 −i3 −i4
= 0
i3 + i4 −i5
= 0
i1
+ i5 −i6 = 0
3i1
= 9
3i2
+ 2i4 + 2i5
= 9
3i2 + 9i3
+ 2i5
= 9

Answers to Exercises
247
(The last three equations come from the circuit involving i0-i1-i6, the circuit involving i0-i2-i4-i5-
i6, and the circuit with i0-i2-i3-i5-i6.)
Octave gives i0 = 4.35616, i1 = 3.00000, i2 = 1.35616,
i3 = 0.24658, i4 = 1.10959, i5 = 1.35616, i6 = 4.35616.
2
(a) Using the variables from the earlier analysis,
i0 −
i1 −i2 = 0
−i0 +
i1 + i2 = 0
5i1
= 20
8i2 = 20
−5i1 + 8i2 = 0
The current ﬂowing in each branch is then is i2 = 20/8 = 2.5, i1 = 20/5 = 4, and i0 = 13/2 = 6.5, all
in amperes. Thus the parallel portion is acting like a single resistor of size 20/(13/2) ≈3.08 ohms.
(b) A similar analysis gives that is i2 = i1 = 20/8 = 4 and i0 = 40/8 = 5 amperes. The equivalent
resistance is 20/5 = 4 ohms.
(c) Another analysis like the prior ones gives is i2 = 20/r2, i1 = 20/r1, and i0 = 20(r1+r2)/(r1r2), all
in amperes. So the parallel portion is acting like a single resistor of size 20/i1 = r1r2/(r1 +r2) ohms.
(This equation is often stated as: the equivalent resistance r satisﬁes 1/r = (1/r1) + (1/r2).)
3
(a) The circuit looks like this.
(b) The circuit looks like this.
4
Not yet done.
5
(a) An adaptation is: in any intersection the ﬂow in equals the ﬂow out. It does seem reasonable
in this case, unless cars are stuck at an intersection for a long time.
(b) We can label the ﬂow in this way.
Shelburne St
Willow
Winooski Ave
west
east
Jay Ln
Because 50 cars leave via Main while 25 cars enter, i1 −25 = i2. Similarly Pier’s in/out balance
means that i2 = i3 and North gives i3 + 25 = i1. We have this system.
i1 −i2
=
25
i2 −i3 = 0
−i1
+ i3 = −25

248
Linear Algebra, by Hefferon
(c) The row operations ρ1 + ρ2 and rho2 + ρ3 lead to the conclusion that there are inﬁnitely many
solutions. With i3 as the parameter,
{


25 + i3
i3
i3

¯¯ i3 ∈R}
of course, since the problem is stated in number of cars, we might restrict i3 to be a natural number.
(d) If we picture an initially-empty circle with the given input/output behavior, we can superimpose
a z3-many cars circling endlessly to get a new solution.
(e) A suitable restatement might be: the number of cars entering the circle must equal the number
of cars leaving. The reasonableness of this one is not as clear. Over the ﬁve minute time period it
could easily work out that a half dozen more cars entered than left, although the into/out of table
in the problem statement does have that this property is satisﬁed. In any event it is of no help in
getting a unique solution since for that we need to know the number of cars circling endlessly.
6
(a) Here is a variable for each unknown block; each known block has the ﬂow shown.
75
65
55
40
50
30
70
80
5
i1
i2
i3
i4
i7
i5
i6
We apply Kirchoﬀ’s principle that the ﬂow into the intersection of Willow and Shelburne must equal
the ﬂow out to get i1 + 25 = i2 + 125. Doing the intersections from right to left and top to bottom
gives these equations.
i1 −i2
=
10
−i1
+
i3
=
15
i2
+ i4
=
5
−i3 −i4
+
i6
= −50
i5
−i7 = −10
−i6 + i7 =
30
The row operation ρ1 + ρ2 followed by ρ2 + ρ3 then ρ3 + ρ4 and ρ4 + ρ5 and ﬁnally ρ5 + ρ6 result in
this system.
i1 −
i2
=
10
−i2 + i3
=
25
i3 + i4 −
i5
=
30
−i5 +
i6
= −20
−i6 + i7 = −30
0 =
0
Since the free variables are i4 and i7 we take them as parameters.
i6 = i7 −30
i5 = i6 + 20 = (i7 −30) + 20 = i7 −10
i3 = −i4 + i5 + 30 = −i4 + (i7 −10) + 30 = −i4 + i7 + 20
i2 = i3 −25 = (−i4 + i7 + 20) −25 = −i4 + i7 −5
i1 = i2 + 10 = (−i4 + i7 −5) + 10 = −i4 + i7 + 5
()
Obviously i4 and i7 have to be positive, and in fact the ﬁrst equation shows that i7 must be at least
30. If we start with i7, then the i2 equation shows that 0 ≤i4 ≤i7 −5.
(b) We cannot take i7 to be zero or else i6 will be negative (this would mean cars going the wrong
way on the one-way street Jay). We can, however, take i7 to be as small as 30, and then there are
many suitable i4’s. For instance, the solution
(i1, i2, i3, i4, i5, i6, i7) = (35, 25, 50, 0, 20, 0, 30)
results from choosing i4 = 0.

Chapter Two: Vector Spaces
Subsection Two.I.1: Deﬁnition and Examples
Two.I.1.18
(a) 0 + 0x + 0x2 + 0x3
(b)
µ
0
0
0
0
0
0
0
0
¶
(c) The constant function f(x) = 0
(d) The constant function f(n) = 0
Two.I.1.19
(a) 3 + 2x −x2
(b)
µ
−1
+1
0
−3
¶
(c) −3ex + 2e−x
Two.I.1.20
Most of the conditions are easy to check; use Example 1.3 as a guide. Here are some
comments.
(a) This is just like Example 1.3; the zero element is 0 + 0x.
(b) The zero element of this space is the 2×2 matrix of zeroes.
(c) The zero element is the vector of zeroes.
(d) Closure of addition involves noting that the sum




x1
y1
z1
w1



+




x2
y2
z2
w2



=




x1 + x2
y1 + y2
z1 + z2
w1 + w2




is in L because (x1+x2)+(y1+y2)−(z1+z2)+(w1+w2) = (x1+y1−z1+w1)+(x2+y2−z2+w2) = 0+0.
Closure of scalar multiplication is similar. Note that the zero element, the vector of zeroes, is in L.
Two.I.1.21
In each item the set is called Q. For some items, there are other correct ways to show that
Q is not a vector space.
(a) It is not closed under addition; it fails to meet condition (1).


1
0
0

,


0
1
0

∈Q


1
1
0

̸∈Q
(b) It is not closed under addition.

1
0
0

,


0
1
0

∈Q


1
1
0

̸∈Q
(c) It is not closed under addition.
µ0
1
0
0
¶
,
µ1
1
0
0
¶
∈Q
µ1
2
0
0
¶
̸∈Q
(d) It is not closed under scalar multiplication.
1 + 1x + 1x2 ∈Q
−1 · (1 + 1x + 1x2) ̸∈Q
(e) It is empty, violating condition (4).
Two.I.1.22
The usual operations (v0 + v1i) + (w0 + w1i) = (v0 + w0) + (v1 + w1)i and r(v0 + v1i) =
(rv0) + (rv1)i suﬃce. The check is easy.
Two.I.1.23
No, it is not closed under scalar multiplication since, e.g., π · (1) is not a rational number.
Two.I.1.24
The natural operations are (v1x + v2y + v3z) + (w1x + w2y + w3z) = (v1 + w1)x + (v2 +
w2)y + (v3 + w3)z and r · (v1x + v2y + v3z) = (rv1)x + (rv2)y + (rv3)z. The check that this is a vector
space is easy; use Example 1.3 as a guide.
Two.I.1.25
The ‘+’ operation is not commutative (that is, condition (2) is not met); producing two
members of the set witnessing this assertion is easy.

250
Linear Algebra, by Hefferon
Two.I.1.26
(a) It is not a vector space.
(1 + 1) ·


1
0
0

̸=


1
0
0

+


1
0
0


(b) It is not a vector space.
1 ·


1
0
0

̸=


1
0
0


Two.I.1.27
For each “yes” answer, you must give a check of all the conditions given in the deﬁni-
tion of a vector space. For each “no” answer, give a speciﬁc example of the failure of one of the
conditions.
(a) Yes.
(b) Yes.
(c) No, it is not closed under addition.
The vector of all 1/4’s, when added to itself, makes a
nonmember.
(d) Yes.
(e) No, f(x) = e−2x + (1/2) is in the set but 2 · f is not (that is, condition (6) fails).
Two.I.1.28
It is a vector space. Most conditions of the deﬁnition of vector space are routine; we here
check only closure. For addition, (f1 + f2) (7) = f1(7) + f2(7) = 0 + 0 = 0. For scalar multiplication,
(r · f) (7) = rf(7) = r0 = 0.
Two.I.1.29
We check Deﬁnition 1.1.
First, closure under ‘+’ holds because the product of two positive reals is a positive real. The
second condition is satisﬁed because real multiplication commutes. Similarly, as real multiplication
associates, the third checks. For the fourth condition, observe that multiplying a number by 1 ∈R+
won’t change the number. Fifth, any positive real has a reciprocal that is a positive real.
The sixth, closure under ‘·’, holds because any power of a positive real is a positive real. The
seventh condition is just the rule that vr+s equals the product of vr and vs. The eight condition says
that (vw)r = vrwr. The ninth condition asserts that (vr)s = vrs. The ﬁnal condition says that v1 = v.
Two.I.1.30
(a) No: 1 · (0, 1) + 1 · (0, 1) ̸= (1 + 1) · (0, 1).
(b) No; the same calculation as the prior answer shows a contition in the deﬁnition of a vector
space that is violated. Another example of a violation of the conditions for a vector space is that
1 · (0, 1) ̸= (0, 1).
Two.I.1.31
It is not a vector space since it is not closed under addition, as (x2) + (1 + x −x2) is not
in the set.
Two.I.1.32
(a) 6
(b) nm
(c) 3
(d) To see that the answer is 2, rewrite it as
{
µ
a
0
b
−a −b
¶ ¯¯ a, b ∈R}
so that there are two parameters.
Two.I.1.33
A vector space (over R) consists of a set V along with two operations ‘⃗+’ and ‘⃗·’ subject
to these conditions. Where ⃗v, ⃗w ∈V , (1) their vector sum ⃗v ⃗+ ⃗w is an element of V . If ⃗u,⃗v, ⃗w ∈V
then (2) ⃗v ⃗+ ⃗w = ⃗w ⃗+ ⃗v and (3) (⃗v ⃗+ ⃗w) ⃗+ ⃗u = ⃗v ⃗+ (⃗w ⃗+ ⃗u). (4) There is a zero vector ⃗0 ∈V such that
⃗v ⃗+⃗0 = ⃗v for all ⃗v ∈V . (5) Each ⃗v ∈V has an additive inverse ⃗w ∈V such that ⃗w ⃗+⃗v = ⃗0. If r, s are
scalars, that is, members of R), and ⃗v, ⃗w ∈V then (6) each scalar multiple r ·⃗v is in V . If r, s ∈R and
⃗v, ⃗w ∈V then (7) (r + s) ·⃗v = r ·⃗v ⃗+ s ·⃗v, and (8) r⃗· (⃗v + ⃗w) = r⃗·⃗v + r⃗· ⃗w, and (9) (rs)⃗·⃗v = r⃗· (s⃗·⃗v),
and (10) 1⃗· ⃗v = ⃗v.
Two.I.1.34
(a) Let V be a vector space, assume that ⃗v ∈V , and assume that ⃗w ∈V is the additive
inverse of ⃗v so that ⃗w + ⃗v = ⃗0. Because addition is commutative, ⃗0 = ⃗w + ⃗v = ⃗v + ⃗w, so therefore ⃗v
is also the additive inverse of ⃗w.
(b) Let V be a vector space and suppose ⃗v,⃗s,⃗t ∈V . The additive inverse of ⃗v is −⃗v so ⃗v + ⃗s = ⃗v +⃗t
gives that −⃗v + ⃗v + ⃗s = −⃗v + ⃗v + ⃗t, which says that ⃗0 + ⃗s = ⃗0 + ⃗t and so ⃗s = ⃗t.
Two.I.1.35
Addition is commutative, so in any vector space, for any vector ⃗v we have that ⃗v = ⃗v +⃗0 =
⃗0 + ⃗v.

Answers to Exercises
251
Two.I.1.36
It is not a vector space since addition of two matrices of unequal sizes is not deﬁned, and
thus the set fails to satisfy the closure condition.
Two.I.1.37
Each element of a vector space has one and only one additive inverse.
For, let V be a vector space and suppose that ⃗v ∈V . If ⃗w1, ⃗w2 ∈V are both additive inverses of
⃗v then consider ⃗w1 + ⃗v + ⃗w2. On the one hand, we have that it equals ⃗w1 + (⃗v + ⃗w2) = ⃗w1 + ⃗0 = ⃗w1.
On the other hand we have that it equals (⃗w1 + ⃗v) + ⃗w2 = ⃗0 + ⃗w2 = ⃗w2. Therefore, ⃗w1 = ⃗w2.
Two.I.1.38
(a) Every such set has the form {r · ⃗v + s · ⃗w
¯¯ r, s ∈R} where either or both of ⃗v, ⃗w may
be ⃗0. With the inherited operations, closure of addition (r1⃗v + s1 ⃗w) + (r2⃗v + s2 ⃗w) = (r1 + r2)⃗v +
(s1 + s2)⃗w and scalar multiplication c(r⃗v + s⃗w) = (cr)⃗v + (cs)⃗w are easy. The other conditions are
also routine.
(b) No such set can be a vector space under the inherited operations because it does not have a zero
element.
Two.I.1.39
Assume that ⃗v ∈V is not ⃗0.
(a) One direction of the if and only if is clear: if r = 0 then r · ⃗v = ⃗0. For the other way, let r be a
nonzero scalar. If r⃗v = ⃗0 then (1/r) · r⃗v = (1/r) ·⃗0 shows that ⃗v = ⃗0, contrary to the assumption.
(b) Where r1, r2 are scalars, r1⃗v = r2⃗v holds if and only if (r1 −r2)⃗v = ⃗0. By the prior item, then
r1 −r2 = 0.
(c) A nontrivial space has a vector ⃗v ̸= ⃗0. Consider the set {k · ⃗v
¯¯ k ∈R}. By the prior item this
set is inﬁnite.
(d) The solution set is either trivial, or nontrivial. In the second case, it is inﬁnite.
Two.I.1.40
Yes. A theorem of ﬁrst semester calculus says that a sum of diﬀerentiable functions is
diﬀerentiable and that (f +g)′ = f ′+g′, and that a multiple of a diﬀerentiable function is diﬀerentiable
and that (r · f)′ = r f ′.
Two.I.1.41
The check is routine. Note that ‘1’ is 1 + 0i and the zero elements are these.
(a) (0 + 0i) + (0 + 0i)x + (0 + 0i)x2
(b)
µ
0 + 0i
0 + 0i
0 + 0i
0 + 0i
¶
Two.I.1.42
Notably absent from the deﬁnition of a vector space is a distance measure.
Two.I.1.43
(a) A small rearrangement does the trick.
(⃗v1 + (⃗v2 + ⃗v3)) + ⃗v4 = ((⃗v1 + ⃗v2) + ⃗v3) + ⃗v4
= (⃗v1 + ⃗v2) + (⃗v3 + ⃗v4)
= ⃗v1 + (⃗v2 + (⃗v3 + ⃗v4))
= ⃗v1 + ((⃗v2 + ⃗v3) + ⃗v4)
Each equality above follows from the associativity of three vectors that is given as a condition in
the deﬁnition of a vector space. For instance, the second ‘=’ applies the rule (⃗w1 + ⃗w2) + ⃗w3 =
⃗w1 + (⃗w2 + ⃗w3) by taking ⃗w1 to be ⃗v1 + ⃗v2, taking ⃗w2 to be ⃗v3, and taking ⃗w3 to be ⃗v4.
(b) The base case for induction is the three vector case. This case ⃗v1 + (⃗v2 + ⃗v3) = (⃗v1 + ⃗v2) + ⃗v3 is
required of any triple of vectors by the deﬁnition of a vector space.
For the inductive step, assume that any two sums of three vectors, any two sums of four vectors,
. . . , any two sums of k vectors are equal no matter how the sums are parenthesized. We will show
that any sum of k + 1 vectors equals this one ((· · · ((⃗v1 + ⃗v2) + ⃗v3) + · · · ) + ⃗vk) + ⃗vk+1.
Any parenthesized sum has an outermost ‘+’. Assume that it lies between ⃗vm and ⃗vm+1 so the
sum looks like this.
(· · · ⃗v1 · · ·⃗vm · · · ) + (· · · ⃗vm+1 · · ·⃗vk+1 · · · )
The second half involves fewer than k + 1 additions, so by the inductive hypothesis we can re-
parenthesize it so that it reads left to right from the inside out, and in particular, so that its
outermost ‘+’ occurs right before ⃗vk+1.
= (· · · ⃗v1 · · · ⃗vm · · · ) + ((· · · (⃗vm+1 + ⃗vm+2) + · · · + ⃗vk) + ⃗vk+1)
Apply the associativity of the sum of three things
= (( · · · ⃗v1 · · · ⃗vm · · · ) + ( · · · (⃗vm+1 + ⃗vm+2) + · · · ⃗vk)) + ⃗vk+1
and ﬁnish by applying the inductive hypothesis inside these outermost parenthesis.

252
Linear Algebra, by Hefferon
Two.I.1.44
(a) We outline the check of the conditions from Deﬁnition 1.1.
Additive closure holds because if a0 + a1 + a2 = 0 and b0 + b1 + b2 = 0 then
(a0 + a1x + a2x2) + (b0 + b1x + b2x2) = (a0 + b0) + (a1 + b1)x + (a2 + b2)x2
is in the set since (a0 + b0) + (a1 + b1) + (a2 + b2) = (a0 + a1 + a2) + (b0 + b1 + b2) is zero. The
second through ﬁfth conditions are easy.
Closure under scalar multiplication holds because if a0 + a1 + a2 = 0 then
r · (a0 + a1x + a2x2) = (ra0) + (ra1)x + (ra2)x2
is in the set as ra0 + ra1 + ra2 = r(a0 + a1 + a2) is zero. The remaining conditions here are also
easy.
(b) This is similar to the prior answer.
(c) Call the vector space V . We have two implications: left to right, if S is a subspace then it is closed
under linear combinations of pairs of vectors and, right to left, if a nonempty subset is closed under
linear combinations of pairs of vectors then it is a subspace. The left to right implication is easy;
we here sketch the other one by assuming S is nonempty and closed, and checking the conditions of
Deﬁnition 1.1.
First, to show closure under addition, if ⃗s1,⃗s2 ∈S then ⃗s1 + ⃗s2 ∈S as ⃗s1 + ⃗s2 = 1 · ⃗s1 + 1 · ⃗s2.
Second, for any ⃗s1,⃗s2 ∈S, because addition is inherited from V , the sum ⃗s1 + ⃗s2 in S equals the
sum ⃗s1 + ⃗s2 in V and that equals the sum ⃗s2 + ⃗s1 in V and that in turn equals the sum ⃗s2 + ⃗s1 in
S. The argument for the third condition is similar to that for the second. For the fourth, suppose
that ⃗s is in the nonempty set S and note that 0 · ⃗s = ⃗0 ∈S; showing that the ⃗0 of V acts under the
inherited operations as the additive identity of S is easy. The ﬁfth condition is satisﬁed because for
any ⃗s ∈S closure under linear combinations shows that the vector 0 · ⃗0 + (−1) · ⃗s is in S; showing
that it is the additive inverse of ⃗s under the inherited operations is routine.
The proofs for the remaining conditions are similar.
Subsection Two.I.2: Subspaces and Spanning Sets
Two.I.2.20
By Lemma 2.9, to see if each subset of M2×2 is a subspace, we need only check if it is
nonempty and closed.
(a) Yes, it is easily checked to be nonempty and closed. This is a parametrization.
{a
µ1
0
0
0
¶
+ b
µ0
0
0
1
¶ ¯¯ a, b ∈R}
By the way, the parametrization also shows that it is a subspace, it is given as the span of the
two-matrix set, and any span is a subspace.
(b) Yes; it is easily checked to be nonempty and closed. Alternatively, as mentioned in the prior
answer, the existence of a parametrization shows that it is a subspace. For the parametrization, the
condition a + b = 0 can be rewritten as a = −b. Then we have this.
{
µ
−b
0
0
b
¶ ¯¯ b ∈R} = {b
µ
−1
0
0
1
¶ ¯¯ b ∈R}
(c) No. It is not closed under addition. For instance,
µ
5
0
0
0
¶
+
µ
5
0
0
0
¶
=
µ
10
0
0
0
¶
is not in the set. (This set is also not closed under scalar multiplication, for instance, it does not
contain the zero matrix.)
(d) Yes.
{b
µ−1
0
0
1
¶
+ c
µ0
1
0
0
¶ ¯¯ b, c ∈R}
Two.I.2.21
No, it is not closed. In particular, it is not closed under scalar multiplication because it
does not contain the zero polynomial.
Two.I.2.22
(a) Yes, solving the linear system arising from
r1


1
0
0

+ r2


0
0
1

=


2
0
1


gives r1 = 2 and r2 = 1.

Answers to Exercises
253
(b) Yes; the linear system arising from r1(x2) + r2(2x + x2) + r3(x + x3) = x −x3
2r2 + r3 =
1
r1 + r2
=
0
r3 = −1
gives that −1(x2) + 1(2x + x2) −1(x + x3) = x −x3.
(c) No; any combination of the two given matrices has a zero in the upper right.
Two.I.2.23
(a) Yes; it is in that span since 1 · cos2 x + 1 · sin2 x = f(x).
(b) No, since r1 cos2 x + r2 sin2 x = 3 + x2 has no scalar solutions that work for all x. For instance,
setting x to be 0 and π gives the two equations r1 · 1 + r2 · 0 = 3 and r1 · 1 + r2 · 0 = 3 + π2, which
are not consistent with each other.
(c) No; consider what happens on setting x to be π/2 and 3π/2.
(d) Yes, cos(2x) = 1 · cos2(x) −1 · sin2(x).
Two.I.2.24
(a) Yes, for any x, y, z ∈R this equation
r1


1
0
0

+ r2


0
2
0

+ r3


0
0
3

=


x
y
z


has the solution r1 = x, r2 = y/2, and r3 = z/3.
(b) Yes, the equation
r1


2
0
1

+ r2


1
1
0

+ r3


0
0
1

=


x
y
z


gives rise to this
2r1 + r2
= x
r2
= y
r1
+ r3 = z
−(1/2)ρ1+ρ3
−→
(1/2)ρ2+ρ3
−→
2r1 + r2
= x
r2
= y
r3 = −(1/2)x + (1/2)y + z
so that, given any x, y, and z, we can compute that r3 = (−1/2)x + (1/2)y + z, r2 = y, and
r1 = (1/2)x −(1/2)y.
(c) No. In particular, the vector


0
0
1


cannot be gotten as a linear combination since the two given vectors both have a third component
of zero.
(d) Yes. The equation
r1


1
0
1

+ r2


3
1
0

+ r3


−1
0
0

+ r4


2
1
5

=


x
y
z


leads to this reduction.


1
3
−1
2
x
0
1
0
1
y
1
0
0
5
z

−ρ1+ρ3
−→
3ρ2+ρ3
−→


1
3
−1
2
x
0
1
0
1
y
0
0
1
6
−x + 3y + z


We have inﬁnitely many solutions. We can, for example, set r4 to be zero and solve for r3, r2, and
r1 in terms of x, y, and z by the usual methods of back-substitution.
(e) No. The equation
r1


2
1
1

+ r2


3
0
1

+ r3


5
1
2

+ r4


6
0
2

=


x
y
z


leads to this reduction.


2
3
5
6
x
1
0
1
0
y
1
1
2
2
z

−(1/2)ρ1+ρ2
−→
−(1/2)ρ1+ρ3
−(1/3)ρ2+ρ3
−→


2
3
5
6
x
0
−3/2
−3/2
−3
−(1/2)x + y
0
0
0
0
−(1/3)x −(1/3)y + z


This shows that not every three-tall vector can be so expressed. Only the vectors satisfying the
restriction that −(1/3)x −(1/3)y + z = 0 are in the span. (To see that any such vector is indeed
expressible, take r3 and r4 to be zero and solve for r1 and r2 in terms of x, y, and z by back-
substitution.)

254
Linear Algebra, by Hefferon
Two.I.2.25
(a) {
¡
c
b
c
¢ ¯¯ b, c ∈R} = {b
¡
0
1
0
¢
+ c
¡
1
0
1
¢ ¯¯ b, c ∈R} The obvious choice
for the set that spans is {
¡0
1
0¢
,
¡1
0
1¢
}.
(b) {
µ
−d
b
c
d
¶ ¯¯ b, c, d ∈R} = {b
µ
0
1
0
0
¶
+ c
µ
0
0
1
0
¶
+ d
µ
−1
0
0
1
¶ ¯¯ b, c, d ∈R} One set that spans
this space consists of those three matrices.
(c) The system
a + 3b
= 0
2a
−c −d = 0
gives b = −(c + d)/6 and a = (c + d)/2. So one description is this.
{c
µ
1/2
−1/6
1
0
¶
+ d
µ
1/2
−1/6
0
1
¶ ¯¯ c, d ∈R}
That shows that a set spanning this subspace consists of those two matrices.
(d) The a = 2b −c gives {(2b −c) + bx + cx3 ¯¯ b, c ∈R} = {b(2 + x) + c(−1 + x3)
¯¯ b, c ∈R}. So the
subspace is the span of the set {2 + x, −1 + x3}.
(e) The set {a + bx + cx2 ¯¯ a + 7b + 49c = 0} parametrized as {b(−7 + x) + c(−49 + x2)
¯¯ b, c ∈R}
has the spanning set {−7 + x, −49 + x2}.
Two.I.2.26
Each answer given is only one out of many possible.
(a) We can parametrize in this way
{


x
0
z

¯¯ x, z ∈R} = {x


1
0
0

+ z


0
0
1

¯¯ x, z ∈R}
giving this for a spanning set.
{


1
0
0

,


0
0
1

}
(b) Parametrize it with {y


−2/3
1
0

+ z


−1/3
0
1

¯¯ y, z ∈R} to get {


−2/3
1
0

,


−1/3
0
1

}.
(c) {




1
−2
1
0



,




−1/2
0
0
1



}
(d) Parametrize the description as {−a1 + a1x + a3x2 + a3x3 ¯¯ a1, a3 ∈R} to get {−1 + x, x2 + x3}.
(e) {1, x, x2, x3, x4}
(f) {
µ
1
0
0
0
¶
,
µ
0
1
0
0
¶
,
µ
0
0
1
0
¶
,
µ
0
0
0
1
¶
}
Two.I.2.27
Technically, no. Subspaces of R3 are sets of three-tall vectors, while R2 is a set of two-tall
vectors. Clearly though, R2 is “just like” this subspace of R3.
{


x
y
0

¯¯ x, y ∈R}
Two.I.2.28
Of course, the addition and scalar multiplication operations are the ones inherited from
the enclosing space.
(a) This is a subspace. It is not empty as it contains at least the two example functions given. It is
closed because if f1, f2 are even and c1, c2 are scalars then we have this.
(c1f1 + c2f2) (−x) = c1 f1(−x) + c2 f2(−x) = c1 f1(x) + c2 f2(x) = (c1f1 + c2f2) (x)
(b) This is also a subspace; the check is similar to the prior one.
Two.I.2.29
It can be improper. If ⃗v = ⃗0 then this is a trivial subspace. At the opposite extreme, if
the vector space is R1 and ⃗v ̸= ⃗0 then the subspace is all of R1.
Two.I.2.30
No, such a set is not closed. For one thing, it does not contain the zero vector.
Two.I.2.31
No. The only subspaces of R1 are the space itself and its trivial subspace. Any subspace S
of R that contains a nonzero member ⃗v must contain the set of all of its scalar multiples {r · ⃗v
¯¯ r ∈R}.
But this set is all of R.

Answers to Exercises
255
Two.I.2.32
Item (1) is checked in the text.
Item (2) has ﬁve conditions. First, for closure, if c ∈R and ⃗s ∈S then c ·⃗s ∈S as c ·⃗s = c ·⃗s + 0 ·⃗0.
Second, because the operations in S are inherited from V , for c, d ∈R and ⃗s ∈S, the scalar product
(c + d) · ⃗s in S equals the product (c + d) · ⃗s in V , and that equals c · ⃗s + d · ⃗s in V , which equals
c · ⃗s + d · ⃗s in S.
The check for the third, fourth, and ﬁfth conditions are similar to the second conditions’s check
just given.
Two.I.2.33
An exercise in the prior subsection shows that every vector space has only one zero vector
(that is, there is only one vector that is the additive identity element of the space). But a trivial space
has only one element and that element must be this (unique) zero vector.
Two.I.2.34
As the hint suggests, the basic reason is the Linear Combination Lemma from the ﬁrst
chapter. For the full proof, we will show mutual containment between the two sets.
The ﬁrst containment [[S]] ⊇[S] is an instance of the more general, and obvious, fact that for any
subset T of a vector space, [T] ⊇T.
For the other containment, that [[S]] ⊆[S], take m vectors from [S], namely c1,1⃗s1,1+· · ·+c1,n1⃗s1,n1,
. . . , c1,m⃗s1,m + · · · + c1,nm⃗s1,nm, and note that any linear combination of those
r1(c1,1⃗s1,1 + · · · + c1,n1⃗s1,n1) + · · · + rm(c1,m⃗s1,m + · · · + c1,nm⃗s1,nm)
is a linear combination of elements of S
= (r1c1,1)⃗s1,1 + · · · + (r1c1,n1)⃗s1,n1 + · · · + (rmc1,m)⃗s1,m + · · · + (rmc1,nm)⃗s1,nm
and so is in [S]. That is, simply recall that a linear combination of linear combinations (of members
of S) is a linear combination (again of members of S).
Two.I.2.35
(a) It is not a subspace because these are not the inherited operations. For one thing,
in this space,
0 ·


x
y
z

=


1
0
0


while this does not, of course, hold in R3.
(b) We can combine the argument showing closure under addition with the argument showing closure
under scalar multiplication into one single argument showing closure under linear combinations of
two vectors. If r1, r2, x1, x2, y1, y2, z1, z2 are in R then
r1


x1
y1
z1

+ r2


x2
y2
z2

=


r1x1 −r1 + 1
r1y1
r1z1

+


r2x2 −r2 + 1
r2y2
r2z2

=


r1x1 −r1 + r2x2 −r2 + 1
r1y1 + r2y2
r1z1 + r2z2


(note that the deﬁnition of addition in this space is that the ﬁrst components combine as (r1x1−r1+
1)+(r2x2−r2+1)−1, so the ﬁrst component of the last vector does not say ‘+2’). Adding the three
components of the last vector gives r1(x1−1+y1+z1)+r2(x2−1+y2+z2)+1 = r1·0+r2·0+1 = 1.
Most of the other checks of the conditions are easy (although the oddness of the operations keeps
them from being routine). Commutativity of addition goes like this.


x1
y1
z1

+


x2
y2
z2

=


x1 + x2 −1
y1 + y2
z1 + z2

=


x2 + x1 −1
y2 + y1
z2 + z1

=


x2
y2
z2

+


x1
y1
z1


Associativity of addition has
(


x1
y1
z1

+


x2
y2
z2

) +


x3
y3
z3

=


(x1 + x2 −1) + x3 −1
(y1 + y2) + y3
(z1 + z2) + z3


while


x1
y1
z1

+ (


x2
y2
z2

+


x3
y3
z3

) =


x1 + (x2 + x3 −1) −1
y1 + (y2 + y3)
z1 + (z2 + z3)


and they are equal. The identity element with respect to this addition operation works this way


x
y
z

+


1
0
0

=


x + 1 −1
y + 0
z + 0

=


x
y
z



256
Linear Algebra, by Hefferon
and the additive inverse is similar.


x
y
z

+


−x + 2
−y
−z

=


x + (−x + 2) −1
y −y
z −z

=


1
0
0


The conditions on scalar multiplication are also easy. For the ﬁrst condition,
(r + s)


x
y
z

=


(r + s)x −(r + s) + 1
(r + s)y
(r + s)z


while
r


x
y
z

+ s


x
y
z

=


rx −r + 1
ry
rz

+


sx −s + 1
sy
sz

=


(rx −r + 1) + (sx −s + 1) −1
ry + sy
rz + sz


and the two are equal. The second condition compares
r · (


x1
y1
z1

+


x2
y2
z2

) = r ·


x1 + x2 −1
y1 + y2
z1 + z2

=


r(x1 + x2 −1) −r + 1
r(y1 + y2)
r(z1 + z2)


with
r


x1
y1
z1

+ r


x2
y2
z2

=


rx1 −r + 1
ry1
rz1

+


rx2 −r + 1
ry2
rz2

=


(rx1 −r + 1) + (rx2 −r + 1) −1
ry1 + ry2
rz1 + rz2


and they are equal. For the third condition,
(rs)


x
y
z

=


rsx −rs + 1
rsy
rsz


while
r(s


x
y
z

) = r(


sx −s + 1
sy
sz

) =


r(sx −s + 1) −r + 1
rsy
rsz


and the two are equal. For scalar multiplication by 1 we have this.
1 ·


x
y
z

=


1x −1 + 1
1y
1z

=


x
y
z


Thus all the conditions on a vector space are met by these two operations.
Remark. A way to understand this vector space is to think of it as the plane in R3
P = {


x
y
z

¯¯ x + y + z = 0}
displaced away from the origin by 1 along the x-axis. Then addition becomes: to add two members
of this space,


x1
y1
z1

,


x2
y2
z2


(such that x1 + y1 + z1 = 1 and x2 + y2 + z2 = 1) move them back by 1 to place them in P and add
as usual,


x1 −1
y1
z1

+


x2 −1
y2
z2

=


x1 + x2 −2
y1 + y2
z1 + z2


(in P)
and then move the result back out by 1 along the x-axis.


x1 + x2 −1
y1 + y2
z1 + z2

.
Scalar multiplication is similar.

Answers to Exercises
257
(c) For the subspace to be closed under the inherited scalar multiplication, where ⃗v is a member of
that subspace,
0 · ⃗v =


0
0
0


must also be a member.
The converse does not hold. Here is a subset of R3 that contains the origin
{


0
0
0

,


1
0
0

}
(this subset has only two elements) but is not a subspace.
Two.I.2.36
(a) (⃗v1 + ⃗v2 + ⃗v3) −(⃗v1 + ⃗v2) = ⃗v3
(b) (⃗v1 + ⃗v2) −(⃗v1) = ⃗v2
(c) Surely, ⃗v1.
(d) Taking the one-long sum and subtracting gives (⃗v1) −⃗v1 = ⃗0.
Two.I.2.37
Yes; any space is a subspace of itself, so each space contains the other.
Two.I.2.38
(a) The union of the x-axis and the y-axis in R2 is one.
(b) The set of integers, as a subset of R1, is one.
(c) The subset {⃗v} of R2 is one, where ⃗v is any nonzero vector.
Two.I.2.39
Because vector space addition is commutative, a reordering of summands leaves a linear
combination unchanged.
Two.I.2.40
We always consider that span in the context of an enclosing space.
Two.I.2.41
It is both ‘if’ and ‘only if’.
For ‘if’, let S be a subset of a vector space V and assume ⃗v ∈S satisﬁes ⃗v = c1⃗s1 + · · · + cn⃗sn
where c1, . . . , cn are scalars and ⃗s1, . . . ,⃗sn ∈S. We must show that [S ∪{⃗v}] = [S].
Containment one way, [S] ⊆[S ∪{⃗v}] is obvious. For the other direction, [S ∪{⃗v}] ⊆[S], note
that if a vector is in the set on the left then it has the form d0⃗v + d1⃗t1 + · · · + dm⃗tm where the d’s are
scalars and the ⃗t ’s are in S. Rewrite that as d0(c1⃗s1 + · · · + cn⃗sn) + d1⃗t1 + · · · + dm⃗tm and note that
the result is a member of the span of S.
The ‘only if’ is clearly true — adding ⃗v enlarges the span to include at least ⃗v.
Two.I.2.42
(a) Always.
Assume that A, B are subspaces of V . Note that their intersection is not empty as both contain
the zero vector. If ⃗w,⃗s ∈A ∩B and r, s are scalars then r⃗v + s⃗w ∈A because each vector is in A
and so a linear combination is in A, and r⃗v + s⃗w ∈B for the same reason. Thus the intersection is
closed. Now Lemma 2.9 applies.
(b) Sometimes (more precisely, only if A ⊆B or B ⊆A).
To see the answer is not ‘always’, take V to be R3, take A to be the x-axis, and B to be the
y-axis. Note that
µ
1
0
¶
∈A and
µ
0
1
¶
∈B
but
µ
1
0
¶
+
µ
0
1
¶
̸∈A ∪B
as the sum is in neither A nor B.
The answer is not ‘never’ because if A ⊆B or B ⊆A then clearly A ∪B is a subspace.
To show that A∪B is a subspace only if one subspace contains the other, we assume that A ̸⊆B
and B ̸⊆A and prove that the union is not a subspace. The assumption that A is not a subset
of B means that there is an ⃗a ∈A with ⃗a ̸∈B. The other assumption gives a ⃗b ∈B with ⃗b ̸∈A.
Consider ⃗a +⃗b. Note that sum is not an element of A or else (⃗a +⃗b) −⃗a would be in A, which it is
not. Similarly the sum is not an element of B. Hence the sum is not an element of A ∪B, and so
the union is not a subspace.
(c) Never. As A is a subspace, it contains the zero vector, and therefore the set that is A’s complement
does not. Without the zero vector, the complement cannot be a vector space.
Two.I.2.43
The span of a set does not depend on the enclosing space. A linear combination of vectors
from S gives the same sum whether we regard the operations as those of W or as those of V , because
the operations of W are inherited from V .

258
Linear Algebra, by Hefferon
Two.I.2.44
It is; apply Lemma 2.9. (You must consider the following. Suppose B is a subspace of a
vector space V and suppose A ⊆B ⊆V is a subspace. From which space does A inherit its operations?
The answer is that it doesn’t matter — A will inherit the same operations in either case.)
Two.I.2.45
(a) Always; if S ⊆T then a linear combination of elements of S is also a linear combi-
nation of elements of T.
(b) Sometimes (more precisely, if and only if S ⊆T or T ⊆S).
The answer is not ‘always’ as is shown by this example from R3
S = {


1
0
0

,


0
1
0

},
T = {


1
0
0

,


0
0
1

}
because of this.


1
1
1

∈[S ∪T]


1
1
1

̸∈[S] ∪[T]
The answer is not ‘never’ because if either set contains the other then equality is clear. We
can characterize equality as happening only when either set contains the other by assuming S ̸⊆T
(implying the existence of a vector ⃗s ∈S with ⃗s ̸∈T) and T ̸⊆S (giving a ⃗t ∈T with ⃗t ̸∈S), noting
⃗s + ⃗t ∈[S ∪T], and showing that ⃗s + ⃗t ̸∈[S] ∪[T].
(c) Sometimes.
Clearly [S ∩T] ⊆[S]∩[T] because any linear combination of vectors from S ∩T is a combination
of vectors from S and also a combination of vectors from T.
Containment the other way does not always hold. For instance, in R2, take
S = {
µ
1
0
¶
,
µ
0
1
¶
},
T = {
µ
2
0
¶
}
so that [S] ∩[T] is the x-axis but [S ∩T] is the trivial subspace.
Characterizing exactly when equality holds is tough. Clearly equality holds if either set contains
the other, but that is not ‘only if’ by this example in R3.
S = {


1
0
0

,


0
1
0

},
T = {


1
0
0

,


0
0
1

}
(d) Never, as the span of the complement is a subspace, while the complement of the span is not (it
does not contain the zero vector).
Two.I.2.46
Call the subset S. By Lemma 2.9, we need to check that [S] is closed under linear combi-
nations. If c1⃗s1 + · · · + cn⃗sn, cn+1⃗sn+1 + · · · + cm⃗sm ∈[S] then for any p, r ∈R we have
p · (c1⃗s1 + · · · + cn⃗sn) + r · (cn+1⃗sn+1 + · · · + cm⃗sm) = pc1⃗s1 + · · · + pcn⃗sn + rcn+1⃗sn+1 + · · · + rcm⃗sm
which is an element of [S]. (Remark. If the set S is empty, then that ‘if . . . then . . . ’ statement is
vacuously true.)
Two.I.2.47
For this to happen, one of the conditions giving the sensibleness of the addition and scalar
multiplication operations must be violated. Consider R2 with these operations.
µx1
y1
¶
+
µx2
y2
¶
=
µ0
0
¶
r
µx
y
¶
=
µ0
0
¶
The set R2 is closed under these operations. But it is not a vector space.
1 ·
µ
1
1
¶
̸=
µ
1
1
¶
Subsection Two.II.1: Deﬁnition and Examples
Two.II.1.18
For each of these, when the subset is independent it must be proved, and when the subset
is dependent an example of a dependence must be given.

Answers to Exercises
259
(a) It is dependent. Considering
c1


1
−3
5

+ c2


2
2
4

+ c3


4
−4
14

=


0
0
0


gives rise to this linear system.
c1 + 2c2 + 4c3 = 0
−3c1 + 2c2 −4c3 = 0
5c1 + 4c2 + 14c3 = 0
Gauss’ method


1
2
4
0
−3
2
−4
0
5
4
14
0


3ρ1+ρ2
−→
−5ρ1+ρ3
(3/4)ρ2+ρ3
−→


1
2
4
0
0
8
8
0
0
0
0
0


yields a free variable, so there are inﬁnitely many solutions. For an example of a particular depen-
dence we can set c3 to be, say, 1. Then we get c2 = −1 and c1 = −2.
(b) It is dependent. The linear system that arises here


1
2
3
0
7
7
7
0
7
7
7
0

−7ρ1+ρ2
−→
−7ρ1+ρ3
−ρ2+ρ3
−→


1
2
3
0
0
−7
−14
0
0
0
0
0


has inﬁnitely many solutions.
We can get a particular solution by taking c3 to be, say, 1, and
back-substituting to get the resulting c2 and c1.
(c) It is linearly independent. The system


0
1
0
0
0
0
−1
4
0

ρ1↔ρ2
−→
ρ3↔ρ1
−→


−1
4
0
0
1
0
0
0
0


has only the solution c1 = 0 and c2 = 0. (We could also have gotten the answer by inspection — the
second vector is obviously not a multiple of the ﬁrst, and vice versa.)
(d) It is linearly dependent. The linear system


9
2
3
12
0
9
0
5
12
0
0
1
−4
−1
0


has more unknowns than equations, and so Gauss’ method must end with at least one variable free
(there can’t be a contradictory equation because the system is homogeneous, and so has at least the
solution of all zeroes). To exhibit a combination, we can do the reduction
−ρ1+ρ2
−→
(1/2)ρ2+ρ3
−→


9
2
3
12
0
0
−2
2
0
0
0
0
−3
−1
0


and take, say, c4 = 1. Then we have that c3 = −1/3, c2 = −1/3, and c1 = −31/27.
Two.II.1.19
In the cases of independence, that must be proved. Otherwise, a speciﬁc dependence must
be produced. (Of course, dependences other than the ones exhibited here are possible.)
(a) This set is independent. Setting up the relation c1(3−x+9x2)+c2(5−6x+3x2)+c3(1+1x−5x2) =
0 + 0x + 0x2 gives a linear system


3
5
1
0
−1
−6
1
0
9
3
−5
0

(1/3)ρ1+ρ2
−→
−3ρ1+ρ3
3ρ2
−→
−(12/13)ρ2+ρ3
−→


3
5
1
0
0
−13
4
0
0
0
−128/13
0


with only one solution: c1 = 0, c2 = 0, and c3 = 0.
(b) This set is independent. We can see this by inspection, straight from the deﬁnition of linear
independence. Obviously neither is a multiple of the other.
(c) This set is linearly independent. The linear system reduces in this way


2
3
4
0
1
−1
0
0
7
2
−3
0

−(1/2)ρ1+ρ2
−→
−(7/2)ρ1+ρ3
−(17/5)ρ2+ρ3
−→


2
3
4
0
0
−5/2
−2
0
0
0
−51/5
0


to show that there is only the solution c1 = 0, c2 = 0, and c3 = 0.

260
Linear Algebra, by Hefferon
(d) This set is linearly dependent. The linear system


8
0
2
8
0
3
1
2
−2
0
3
2
2
5
0


must, after reduction, end with at least one variable free (there are more variables than equations,
and there is no possibility of a contradictory equation because the system is homogeneous). We can
take the free variables as parameters to describe the solution set. We can then set the parameter to
a nonzero value to get a nontrivial linear relation.
Two.II.1.20
Let Z be the zero function Z(x) = 0, which is the additive identity in the vector space
under discussion.
(a) This set is linearly independent. Consider c1 · f(x) + c2 · g(x) = Z(x). Plugging in x = 1 and
x = 2 gives a linear system
c1 · 1 +
c2 · 1 = 0
c1 · 2 + c2 · (1/2) = 0
with the unique solution c1 = 0, c2 = 0.
(b) This set is linearly independent. Consider c1 · f(x) + c2 · g(x) = Z(x) and plug in x = 0 and
x = π/2 to get
c1 · 1 + c2 · 0 = 0
c1 · 0 + c2 · 1 = 0
which obviously gives that c1 = 0, c2 = 0.
(c) This set is also linearly independent. Considering c1 · f(x) + c2 · g(x) = Z(x) and plugging in
x = 1 and x = e
c1 · e + c2 · 0 = 0
c1 · ee + c2 · 1 = 0
gives that c1 = 0 and c2 = 0.
Two.II.1.21
In each case, that the set is independent must be proved, and that it is dependent must
be shown by exihibiting a speciﬁc dependence.
(a) This set is dependent. The familiar relation sin2(x)+cos2(x) = 1 shows that 2 = c1 ·(4 sin2(x))+
c2 · (cos2(x)) is satisﬁed by c1 = 1/2 and c2 = 2.
(b) This set is independent. Consider the relationship c1 · 1 + c2 · sin(x) + c3 · sin(2x) = 0 (that ‘0’
is the zero function). Taking x = 0, x = π/2 and x = π/4 gives this system.
c1
= 0
c1 +
c2
= 0
c1 + (
√
2/2)c2 + c3 = 0
whose only solution is c1 = 0, c2 = 0, and c3 = 0.
(c) By inspection, this set is independent. Any dependence cos(x) = c · x is not possible since the
cosine function is not a multiple of the identity function (we are applying Corollary 1.17).
(d) By inspection, we spot that there is a dependence. Because (1 + x)2 = x2 + 2x + 1, we get that
c1 · (1 + x)2 + c2 · (x2 + 2x) = 3 is satisﬁed by c1 = 3 and c2 = −3.
(e) This set is dependent. The easiest way to see that is to recall the triginometric relationship
cos2(x) −sin2(x) = cos(2x).
(Remark.
A person who doesn’t recall this, and tries some x’s,
simply never gets a system leading to a unique solution, and never gets to conclude that the set is
independent. Of course, this person might wonder if they simply never tried the right set of x’s, but
a few tries will lead most people to look instead for a dependence.)
(f) This set is dependent, because it contains the zero object in the vector space, the zero polynomial.
Two.II.1.22
No, that equation is not a linear relationship. In fact this set is independent, as the
system arising from taking x to be 0, π/6 and π/4 shows.
Two.II.1.23
To emphasize that the equation 1 · ⃗s + (−1) · ⃗s = ⃗0 does not make the set dependent.
Two.II.1.24
We have already showed this: the Linear Combination Lemma and its corollary state
that in an echelon form matrix, no nonzero row is a linear combination of the others.
Two.II.1.25
(a) Assume that the set {⃗u,⃗v, ⃗w} is linearly independent, so that any relationship d0⃗u+
d1⃗v + d2 ⃗w = ⃗0 leads to the conclusion that d0 = 0, d1 = 0, and d2 = 0.
Consider the relationship c1(⃗u) + c2(⃗u + ⃗v) + c3(⃗u + ⃗v + ⃗w) = ⃗0. Rewrite it to get (c1 + c2 +
c3)⃗u + (c2 + c3)⃗v + (c3)⃗w = ⃗0. Taking d0 to be c1 + c2 + c3, taking d1 to be c2 + c3, and taking d2

Answers to Exercises
261
to be c3 we have this system.
c1 + c2 + c3 = 0
c2 + c3 = 0
c3 = 0
Conclusion: the c’s are all zero, and so the set is linearly independent.
(b) The second set is dependent
1 · (⃗u −⃗v) + 1 · (⃗v −⃗w) + 1 · (⃗w −⃗u) = ⃗0
whether or not the ﬁrst set is independent.
Two.II.1.26
(a) A singleton set {⃗v} is linearly independent if and only if ⃗v ̸= ⃗0. For the ‘if’ direction,
with ⃗v ̸= ⃗0, we can apply Lemma 1.4 by considering the relationship c · ⃗v = ⃗0 and noting that the
only solution is the trivial one: c = 0. For the ‘only if’ direction, just recall that Example 1.11 shows
that {⃗0} is linearly dependent, and so if the set {⃗v} is linearly independent then ⃗v ̸= ⃗0.
(Remark. Another answer is to say that this is the special case of Lemma 1.16 where S = ∅.)
(b) A set with two elements is linearly independent if and only if neither member is a multiple of the
other (note that if one is the zero vector then it is a multiple of the other, so this case is covered).
This is an equivalent statement: a set is linearly dependent if and only if one element is a multiple
of the other.
The proof is easy.
A set {⃗v1,⃗v2} is linearly dependent if and only if there is a relationship
c1⃗v1 + c2⃗v2 = ⃗0 with either c1 ̸= 0 or c2 ̸= 0 (or both). That holds if and only if ⃗v1 = (−c2/c1)⃗v2 or
⃗v2 = (−c1/c2)⃗v1 (or both).
Two.II.1.27
This set is linearly dependent set because it contains the zero vector.
Two.II.1.28
The ‘if’ half is given by Lemma 1.14. The converse (the ‘only if’ statement) does not
hold. An example is to consider the vector space R2 and these vectors.
⃗x =
µ1
0
¶
,
⃗y =
µ0
1
¶
,
⃗z =
µ1
1
¶
Two.II.1.29
(a) The linear system arising from
c1


1
1
0

+ c2


−1
2
0

=


0
0
0


has the unique solution c1 = 0 and c2 = 0.
(b) The linear system arising from
c1


1
1
0

+ c2


−1
2
0

=


3
2
0


has the unique solution c1 = 8/3 and c2 = −1/3.
(c) Suppose that S is linearly independent. Suppose that we have both ⃗v = c1⃗s1 + · · · + cn⃗sn and
⃗v = d1⃗t1 + · · · + dm⃗tm (where the vectors are members of S). Now,
c1⃗s1 + · · · + cn⃗sn = ⃗v = d1⃗t1 + · · · + dm⃗tm
can be rewritten in this way.
c1⃗s1 + · · · + cn⃗sn −d1⃗t1 −· · · −dm⃗tm = ⃗0
Possibly some of the ⃗s ’s equal some of the ⃗t ’s; we can combine the associated coeﬃcients (i.e., if
⃗si = ⃗tj then · · · + ci⃗si + · · · −dj⃗tj −· · · can be rewritten as · · · + (ci −dj)⃗si + · · · ). That equation
is a linear relationship among distinct (after the combining is done) members of the set S. We’ve
assumed that S is linearly independent, so all of the coeﬃcients are zero. If i is such that ⃗si does
not equal any ⃗tj then ci is zero. If j is such that ⃗tj does not equal any ⃗si then dj is zero. In the
ﬁnal case, we have that ci −dj = 0 and so ci = dj.
Therefore, the original two sums are the same, except perhaps for some 0 · ⃗si or 0 ·⃗tj terms that
we can neglect.
(d) This set is not linearly independent:
S = {
µ
1
0
¶
,
µ
2
0
¶
} ⊂R2
and these two linear combinations give the same result
µ
0
0
¶
= 2 ·
µ
1
0
¶
−1 ·
µ
2
0
¶
= 4 ·
µ
1
0
¶
−2 ·
µ
2
0
¶

262
Linear Algebra, by Hefferon
Thus, a linearly dependent set might have indistinct sums.
In fact, this stronger statement holds: if a set is linearly dependent then it must have the property
that there are two distinct linear combinations that sum to the same vector. Brieﬂy, where c1⃗s1 +
· · · + cn⃗sn = ⃗0 then multiplying both sides of the relationship by two gives another relationship. If
the ﬁrst relationship is nontrivial then the second is also.
Two.II.1.30
In this ‘if and only if’ statement, the ‘if’ half is clear — if the polynomial is the zero
polynomial then the function that arises from the action of the polynomial must be the zero function
x 7→0. For ‘only if’ we write p(x) = cnxn + · · · + c0. Plugging in zero p(0) = 0 gives that c0 = 0.
Taking the derivative and plugging in zero p′(0) = 0 gives that c1 = 0. Similarly we get that each ci
is zero, and p is the zero polynomial.
Two.II.1.31
The work in this section suggests that an n-dimensional non-degenerate linear surface
should be deﬁned as the span of a linearly independent set of n vectors.
Two.II.1.32
(a) For any a1,1, . . . , a2,4,
c1
µ
a1,1
a2,1
¶
+ c2
µ
a1,2
a2,2
¶
+ c3
µ
a1,3
a2,3
¶
+ c4
µ
a1,4
a2,4
¶
=
µ
0
0
¶
yields a linear system
a1,1c1 + a1,2c2 + a1,3c3 + a1,4c4 = 0
a2,1c1 + a2,2c2 + a2,3c3 + a2,4c4 = 0
that has inﬁnitely many solutions (Gauss’ method leaves at least two variables free). Hence there
are nontrivial linear relationships among the given members of R2.
(b) Any set ﬁve vectors is a superset of a set of four vectors, and so is linearly dependent.
With three vectors from R2, the argument from the prior item still applies, with the slight change
that Gauss’ method now only leaves at least one variable free (but that still gives inﬁntely many
solutions).
(c) The prior item shows that no three-element subset of R2 is independent. We know that there
are two-element subsets of R2 that are independent — one is
{
µ
1
0
¶
,
µ
0
1
¶
}
and so the answer is two.
Two.II.1.33
Yes; here is one.
{


1
0
0

,


0
1
0

,


0
0
1

,


1
1
1

}
Two.II.1.34
Yes. The two improper subsets, the entire set and the empty subset, serve as examples.
Two.II.1.35
In R4 the biggest linearly independent set has four vectors. There are many examples of
such sets, this is one.
{




1
0
0
0



,




0
1
0
0



,




0
0
1
0



,




0
0
0
1



}
To see that no set with ﬁve or more vectors can be independent, set up
c1




a1,1
a2,1
a3,1
a4,1



+ c2




a1,2
a2,2
a3,2
a4,2



+ c3




a1,3
a2,3
a3,3
a4,3



+ c4




a1,4
a2,4
a3,4
a4,4



+ c5




a1,5
a2,5
a3,5
a4,5



=




0
0
0
0




and note that the resulting linear system
a1,1c1 + a1,2c2 + a1,3c3 + a1,4c4 + a1,5c5 = 0
a2,1c1 + a2,2c2 + a2,3c3 + a2,4c4 + a2,5c5 = 0
a3,1c1 + a3,2c2 + a3,3c3 + a3,4c4 + a3,5c5 = 0
a4,1c1 + a4,2c2 + a4,3c3 + a4,4c4 + a4,5c5 = 0
has four equations and ﬁve unknowns, so Gauss’ method must end with at least one c variable free,
so there are inﬁnitely many solutions, and so the above linear relationship among the four-tall vectors
has more solutions than just the trivial solution.
The smallest linearly independent set is the empty set.
The biggest linearly dependent set is R4. The smallest is {⃗0}.

Answers to Exercises
263
Two.II.1.36
(a) The intersection of two linearly independent sets S∩T must be linearly independent
as it is a subset of the linearly independent set S (as well as the linearly independent set T also, of
course).
(b) The complement of a linearly independent set is linearly dependent as it contains the zero vector.
(c) We must produce an example. One, in R2, is
S = {
µ
1
0
¶
}
and
T = {
µ
2
0
¶
}
since the linear dependence of S1 ∪S2 is easily seen.
(d) The union of two linearly independent sets S ∪T is linearly independent if and only if their
spans have a trivial intersection [S] ∩[T] = {⃗0}. To prove that, assume that S and T are linearly
independent subsets of some vector space.
For the ‘only if’ direction, assume that the intersection of the spans is trivial [S] ∩[T] = {⃗0}.
Consider the set S ∪T. Any linear relationship c1⃗s1 + · · · + cn⃗sn + d1⃗t1 + · · · + dm⃗tm = ⃗0 gives
c1⃗s1 + · · · + cn⃗sn = −d1⃗t1 −· · · −dm⃗tm. The left side of that equation sums to a vector in [S], and
the right side is a vector in [T]. Therefore, since the intersection of the spans is trivial, both sides
equal the zero vector. Because S is linearly independent, all of the c’s are zero. Because T is linearly
independent, all of the d’s are zero. Thus, the original linear relationship among members of S ∪T
only holds if all of the coeﬃcients are zero. That shows that S ∪T is linearly independent.
For the ‘if’ half we can make the same argument in reverse.
If the union S ∪T is linearly
independent, that is, if the only solution to c1⃗s1 + · · · + cn⃗sn + d1⃗t1 + · · · + dm⃗tm = ⃗0 is the trivial
solution c1 = 0, . . . , dm = 0, then any vector ⃗v in the intersection of the spans ⃗v = c1⃗s1+· · ·+cn⃗sn =
−d1⃗t1 −· · · = dm⃗tm must be the zero vector because each scalar is zero.
Two.II.1.37
(a) We do induction on the number of vectors in the ﬁnite set S.
The base case is that S has no elements. In this case S is linearly independent and there is
nothing to check — a subset of S that has the same span as S is S itself.
For the inductive step assume that the theorem is true for all sets of size n = 0, n = 1, . . . , n = k
in order to prove that it holds when S has n = k+1 elements. If the k+1-element set S = {⃗s0, . . . ,⃗sk}
is linearly independent then the theorem is trivial, so assume that it is dependent. By Corollary 1.17
there is an ⃗si that is a linear combination of other vectors in S. Deﬁne S1 = S −{⃗si} and note that
S1 has the same span as S by Lemma 1.1. The set S1 has k elements and so the inductive hypothesis
applies to give that it has a linearly independent subset with the same span. That subset of S1 is
the desired subset of S.
(b) Here is a sketch of the argument. The induction argument details have been left out.
If the ﬁnite set S is empty then there is nothing to prove. If S = {⃗0} then the empty subset will
do.
Otherwise, take some nonzero vector ⃗s1 ∈S and deﬁne S1 = {⃗s1}. If [S1] = [S] then this proof
is ﬁnished by noting that S1 is linearly independent.
If not, then there is a nonzero vector ⃗s2 ∈S −[S1] (if every ⃗s ∈S is in [S1] then [S1] = [S]).
Deﬁne S2 = S1 ∪{⃗s2}. If [S2] = [S] then this proof is ﬁnished by using Theorem 1.17 to show that
S2 is linearly independent.
Repeat the last paragraph until a set with a big enough span appears. That must eventually
happen because S is ﬁnite, and [S] will be reached at worst when every vector from S has been used.
Two.II.1.38
(a) Assuming ﬁrst that a ̸= 0,
x
µ
a
c
¶
+ y
µ
b
d
¶
=
µ
0
0
¶
gives
ax + by = 0
cx + dy = 0
−(c/a)ρ1+ρ2
−→
ax +
by = 0
(−(c/a)b + d)y = 0
which has a solution if and only if 0 ̸= −(c/a)b + d = (−cb + ad)/d (we’ve assumed in this case that
a ̸= 0, and so back substitution yields a unique solution).
The a = 0 case is also not hard — break it into the c ̸= 0 and c = 0 subcases and note that in
these cases ad −bc = 0 · d −bc.
Comment. An earlier exercise showed that a two-vector set is linearly dependent if and only if
either vector is a scalar multiple of the other. That can also be used to make the calculation.

264
Linear Algebra, by Hefferon
(b) The equation
c1


a
d
g

+ c2


b
e
h

+ c3


c
f
i

=


0
0
0


gives rise to a homogeneous linear system. We proceed by writing it in matrix form and applying
Gauss’ method.
We ﬁrst reduce the matrix to upper-triangular. Assume that a ̸= 0.
(1/a)ρ1
−→


1
b/a
c/a
0
d
e
f
0
g
h
i
0


−dρ1+ρ2
−→
−gρ1+ρ3


1
b/a
c/a
0
0
(ae −bd)/a
(af −cd)/a
0
0
(ah −bg)/a
(ai −cg)/a
0


(a/(ae−bd))ρ2
−→


1
b/a
c/a
0
0
1
(af −cd)/(ae −bd)
0
0
(ah −bg)/a
(ai −cg)/a
0


(where we’ve assumed for the moment that ae−bd ̸= 0 in order to do the row reduction step). Then,
under the assumptions, we get this.
((ah−bg)/a)ρ2+ρ3
−→


1
b
a
c
a
0
0
1
af−cd
ae−bd
0
0
0
aei+bgf+cdh−hfa−idb−gec
ae−bd
0


shows that the original system is nonsingular if and only if the 3, 3 entry is nonzero. This fraction
is deﬁned because of the ae −bd ̸= 0 assumption, and it will equal zero if and only if its numerator
equals zero.
We next worry about the assumptions. First, if a ̸= 0 but ae −bd = 0 then we swap


1
b/a
c/a
0
0
0
(af −cd)/a
0
0
(ah −bg)/a
(ai −cg)/a
0


ρ2↔ρ3
−→


1
b/a
c/a
0
0
(ah −bg)/a
(ai −cg)/a
0
0
0
(af −cd)/a
0


and conclude that the system is nonsingular if and only if either ah −bg = 0 or af −cd = 0. That’s
the same as asking that their product be zero:
ahaf −ahcd −bgaf + bgcd = 0
ahaf −ahcd −bgaf + aegc = 0
a(haf −hcd −bgf + egc) = 0
(in going from the ﬁrst line to the second we’ve applied the case assumption that ae −bd = 0 by
substituting ae for bd). Since we are assuming that a ̸= 0, we have that haf −hcd −bgf + egc = 0.
With ae −bd = 0 we can rewrite this to ﬁt the form we need: in this a ̸= 0 and ae −bd = 0 case,
the given system is nonsingular when haf −hcd −bgf + egc −i(ae −bd) = 0, as required.
The remaining cases have the same character. Do the a = 0 but d ̸= 0 case and the a = 0 and
d = 0 but g ̸= 0 case by ﬁrst swapping rows and then going on as above. The a = 0, d = 0, and
g = 0 case is easy — a set with a zero vector is linearly dependent, and the formula comes out to
equal zero.
(c) It is linearly dependent if and only if either vector is a multiple of the other. That is, it is not
independent iﬀ


a
d
g

= r ·


b
e
h


or


b
e
h

= s ·


a
d
g


(or both) for some scalars r and s. Eliminating r and s in order to restate this condition only in
terms of the given letters a, b, d, e, g, h, we have that it is not independent — it is dependent — iﬀ
ae −bd = ah −gb = dh −ge.
(d) Dependence or independence is a function of the indices, so there is indeed a formula (although
at ﬁrst glance a person might think the formula involves cases: “if the ﬁrst component of the ﬁrst
vector is zero then . . . ”, this guess turns out not to be correct).
Two.II.1.39
Recall that two vectors from Rn are perpendicular if and only if their dot product is
zero.
(a) Assume that ⃗v and ⃗w are perpendicular nonzero vectors in Rn, with n > 1. With the linear
relationship c⃗v + d⃗w = ⃗0, apply ⃗v to both sides to conclude that c · ∥⃗v∥2 + d · 0 = 0. Because ⃗v ̸= ⃗0
we have that c = 0. A similar application of ⃗w shows that d = 0.

Answers to Exercises
265
(b) Two vectors in R1 are perpendicular if and only if at least one of them is zero.
We deﬁne R0 to be a trivial space, and so both ⃗v and ⃗w are the zero vector.
(c) The right generalization is to look at a set {⃗v1, . . . ,⃗vn} ⊆Rk of vectors that are mutually
orthogonal (also called pairwise perpendicular): if i ̸= j then ⃗vi is perpendicular to ⃗vj. Mimicing the
proof of the ﬁrst item above shows that such a set of nonzero vectors is linearly independent.
Two.II.1.40
(a) This check is routine.
(b) The summation is inﬁnite (has inﬁnitely many summands). The deﬁnition of linear combination
involves only ﬁnite sums.
(c) No nontrivial ﬁnite sum of members of {g, f0, f1, . . .} adds to the zero object: assume that
c0 · (1/(1 −x)) + c1 · 1 + · · · + cn · xn = 0
(any ﬁnite sum uses a highest power, here n).
Multiply both sides by 1 −x to conclude that
each coeﬃcient is zero, because a polynomial describes the zero function only when it is the zero
polynomial.
Two.II.1.41
It is both ‘if’ and ‘only if’.
Let T be a subset of the subspace S of the vector space V . The assertion that any linear relationship
c1⃗t1 + · · · + cn⃗tn = ⃗0 among members of T must be the trivial relationship c1 = 0, . . . , cn = 0 is a
statement that holds in S if and only if it holds in V , because the subspace S inherits its addition and
scalar multiplication operations from V .
Subsection Two.III.1: Basis
Two.III.1.16
By Theorem 1.12, each is a basis if and only if each vector in the space can be given in
a unique way as a linear combination of the given vectors.
(a) Yes this is a basis. The relation
c1


1
2
3

+ c2


3
2
1

+ c3


0
0
1

=


x
y
z


gives


1
3
0
x
2
2
0
y
3
1
1
z

−2ρ1+ρ2
−→
−3ρ1+ρ3
−2ρ2+ρ3
−→


1
3
0
x
0
−4
0
−2x + y
0
0
1
x −2y + z


which has the unique solution c3 = x −2y + z, c2 = x/2 −y/4, and c1 = −x/2 + 3y/4.
(b) This is not a basis. Setting it up as in the prior item
c1


1
2
3

+ c2


3
2
1

=


x
y
z


gives a linear system whose solution


1
3
x
2
2
y
3
1
z

−2ρ1+ρ2
−→
−3ρ1+ρ3
−2ρ2+ρ3
−→


1
3
x
0
−4
−2x + y
0
0
x −2y + z


is possible if and only if the three-tall vector’s components x, y, and z satisfy x −2y + z = 0. For
instance, we can ﬁnd the coeﬃcients c1 and c2 that work when x = 1, y = 1, and z = 1. However,
there are no c’s that work for x = 1, y = 1, and z = 2. Thus this is not a basis; it does not span the
space.
(c) Yes, this is a basis. Setting up the relationship leads to this reduction


0
1
2
x
2
1
5
y
−1
1
0
z

ρ1↔ρ3
−→
2ρ1+ρ2
−→
−(1/3)ρ2+ρ3
−→


−1
1
0
z
0
3
5
y + 2z
0
0
1/3
x −y/3 −2z/3


which has a unique solution for each triple of components x, y, and z.

266
Linear Algebra, by Hefferon
(d) No, this is not a basis. The reduction


0
1
1
x
2
1
3
y
−1
1
0
z

ρ1↔ρ3
−→
2ρ1+ρ2
−→
(−1/3)ρ2+ρ3
−→


−1
1
0
z
0
3
3
y + 2z
0
0
0
x −y/3 −2z/3


which does not have a solution for each triple x, y, and z. Instead, the span of the given set includes
only those three-tall vectors where x = y/3 + 2z/3.
Two.III.1.17
(a) We solve
c1
µ1
1
¶
+ c2
µ−1
1
¶
=
µ1
2
¶
with
µ
1
−1
1
1
1
2
¶
−ρ1+ρ2
−→
µ
1
−1
1
0
2
1
¶
and conclude that c2 = 1/2 and so c1 = 3/2. Thus, the representation is this.
RepB(
µ
1
2
¶
) =
µ
3/2
1/2
¶
B
(b) The relationship c1 · (1) + c2 · (1 + x) + c3 · (1 + x + x2) + c4 · (1 + x + x2 + x3) = x2 + x3 is easily
solved by eye to give that c4 = 1, c3 = 0, c2 = −1, and c1 = 0.
RepD(x2 + x3) =




0
−1
0
1




D
(c) RepE4(




0
−1
0
1



) =




0
−1
0
1




E4
Two.III.1.18
A natural basis is ⟨1, x, x2⟩. There are bases for P2 that do not contain any polynomials
of degree one or degree zero. One is ⟨1 + x + x2, x + x2, x2⟩. (Every basis has at least one polynomial
of degree two, though.)
Two.III.1.19
The reduction
µ1
−4
3
−1
0
2
−8
6
−2
0
¶
−2ρ1+ρ2
−→
µ1
−4
3
−1
0
0
0
0
0
0
¶
gives that the only condition is that x1 = 4x2 −3x3 + x4. The solution set is
{




4x2 −3x3 + x4
x2
x3
x4




¯¯ x2, x3, x4 ∈R} = {x2




4
1
0
0



+ x3




−3
0
1
0



+ x4




1
0
0
1




¯¯ x2, x3, x4 ∈R}
and so the obvious candidate for the basis is this.
⟨




4
1
0
0



,




−3
0
1
0



,




1
0
0
1



⟩
We’ve shown that this spans the space, and showing it is also linearly independent is routine.
Two.III.1.20
There are many bases. This is a natural one.
⟨
µ
1
0
0
0
¶
,
µ
0
1
0
0
¶
,
µ
0
0
1
0
¶
,
µ
0
0
0
1
¶
⟩
Two.III.1.21
For each item, many answers are possible.
(a) One way to proceed is to parametrize by expressing the a2 as a combination of the other two
a2 = 2a1 + a0. Then a2x2 + a1x + a0 is (2a1 + a0)x2 + a1x + a0 and
{(2a1 + a0)x2 + a1x + a0
¯¯ a1, a0 ∈R} = {a1 · (2x2 + x) + a0 · (x2 + 1)
¯¯ a1, a0 ∈R}
suggests ⟨2x2 +x, x2 +1⟩. This only shows that it spans, but checking that it is linearly independent
is routine.
(b) Parametrize {
¡
a
b
c
¢ ¯¯ a + b = 0} to get {
¡
−b
b
c
¢ ¯¯ b, c ∈R}, which suggests using the
sequence ⟨
¡
−1
1
0
¢
,
¡
0
0
1
¢
⟩.
We’ve shown that it spans, and checking that it is linearly
independent is easy.

Answers to Exercises
267
(c) Rewriting
{
µ
a
b
0
2b
¶ ¯¯ a, b ∈R} = {a ·
µ
1
0
0
0
¶
+ b ·
µ
0
1
0
2
¶ ¯¯ a, b ∈R}
suggests this for the basis.
⟨
µ
1
0
0
0
¶
,
µ
0
1
0
2
¶
⟩
Two.III.1.22
We will show that the second is a basis; the ﬁrst is similar. We will show this straight
from the deﬁnition of a basis, because this example appears before Theorem 1.12.
To see that it is linearly independent, we set up c1 · (cos θ −sin θ) + c2 · (2 cos θ + 3 sin θ) =
0 cos θ + 0 sin θ. Taking θ = 0 and θ = π/2 gives this system
c1 · 1 + c2 · 2 = 0
c1 · (−1) + c2 · 3 = 0
ρ1+ρ2
−→
c1 + 2c2 = 0
+ 5c2 = 0
which shows that c1 = 0 and c2 = 0.
The calculation for span is also easy; for any x, y ∈R, we have that c1 ·(cos θ −sin θ)+c2 ·(2 cos θ +
3 sin θ) = x cos θ + y sin θ gives that c2 = x/5 + y/5 and that c1 = 3x/5 −2y/5, and so the span is the
entire space.
Two.III.1.23
(a) Asking which a0 + a1x + a2x2 can be expressed as c1 · (1 + x) + c2 · (1 + 2x) gives
rise to three linear equations, describing the coeﬃcients of x2, x, and the constants.
c1 + c2 = a0
c1 + 2c2 = a1
0 = a2
Gauss’ method with back-substitution shows, provided that a2 = 0, that c2 = −a0 + a1 and
c1 = 2a0 −a1.
Thus, with a2 = 0, we can compute appropriate c1 and c2 for any a0 and a1.
So the span is the entire set of linear polynomials {a0 + a1x
¯¯ a0, a1 ∈R}. Parametrizing that set
{a0 · 1 + a1 · x
¯¯ a0, a1 ∈R} suggests a basis ⟨1, x⟩(we’ve shown that it spans; checking linear inde-
pendence is easy).
(b) With
a0 + a1x + a2x2 = c1 · (2 −2x) + c2 · (3 + 4x2) = (2c1 + 3c2) + (−2c1)x + (4c2)x2
we get this system.
2c1 + 3c2 = a0
−2c1
= a1
4c2 = a2
ρ1+ρ2
−→
(−4/3)ρ2+ρ3
−→
2c1 + 3c2 = a0
3c2 = a0 + a1
0 = (−4/3)a0 −(4/3)a1 + a2
Thus, the only quadratic polynomials a0 + a1x + a2x2 with associated c’s are the ones such that 0 =
(−4/3)a0−(4/3)a1+a2. Hence the span is {(−a1 + (3/4)a2) + a1x + a2x2 ¯¯ a1, a2 ∈R}. Parametriz-
ing gives {a1 · (−1 + x) + a2 · ((3/4) + x2)
¯¯ a1, a2 ∈R}, which suggests ⟨−1+x, (3/4)+x2⟩(checking
that it is linearly independent is routine).
Two.III.1.24
(a) The subspace is {a0 + a1x + a2x2 + a3x3 ¯¯ a0 + 7a1 + 49a2 + 343a3 = 0}. Rewrit-
ing a0 = −7a1 −49a2 −343a3 gives {(−7a1 −49a2 −343a3) + a1x + a2x2 + a3x3 ¯¯ a1, a2, a3 ∈R},
which, on breaking out the parameters, suggests ⟨−7 + x, −49 + x2, −343 + x3⟩for the basis (it is
easily veriﬁed).
(b) The given subspace is the collection of cubics p(x) = a0 +a1x+a2x2 +a3x3 such that a0 +7a1 +
49a2 + 343a3 = 0 and a0 + 5a1 + 25a2 + 125a3 = 0. Gauss’ method
a0 + 7a1 + 49a2 + 343a3 = 0
a0 + 5a1 + 25a2 + 125a3 = 0
−ρ1+ρ2
−→
a0 +
7a1 + 49a2 + 343a3 = 0
−2a1 −24a2 −218a3 = 0
gives that a1 = −12a2 −109a3 and that a0 = 35a2 + 420a3. Rewriting (35a2 + 420a3) + (−12a2 −
109a3)x + a2x2 + a3x3 as a2 · (35 −12x + x2) + a3 · (420 −109x + x3) suggests this for a basis
⟨35 −12x + x2, 420 −109x + x3⟩. The above shows that it spans the space. Checking it is linearly
independent is routine. (Comment. A worthwhile check is to verify that both polynomials in the
basis have both seven and ﬁve as roots.)
(c) Here there are three conditions on the cubics, that a0 + 7a1 + 49a2 + 343a3 = 0, that a0 + 5a1 +
25a2 + 125a3 = 0, and that a0 + 3a1 + 9a2 + 27a3 = 0. Gauss’ method
a0 + 7a1 + 49a2 + 343a3 = 0
a0 + 5a1 + 25a2 + 125a3 = 0
a0 + 3a1 + 9a2 + 27a3 = 0
−ρ1+ρ2
−→
−ρ1+ρ3
−2ρ2+ρ3
−→
a0 +
7a1 + 49a2 + 343a3 = 0
−2a1 −24a2 −218a3 = 0
8a2 + 120a3 = 0

268
Linear Algebra, by Hefferon
yields the single free variable a3, with a2 = −15a3, a1 = 71a3, and a0 = −105a3. The parametriza-
tion is this.
{(−105a3) + (71a3)x + (−15a3)x2 + (a3)x3 ¯¯ a3 ∈R} = {a3 · (−105 + 71x −15x2 + x3)
¯¯ a3 ∈R}
Therefore, a natural candidate for the basis is ⟨−105 + 71x −15x2 + x3⟩. It spans the space by
the work above. It is clearly linearly independent because it is a one-element set (with that single
element not the zero object of the space). Thus, any cubic through the three points (7, 0), (5, 0),
and (3, 0) is a multiple of this one. (Comment. As in the prior question, a worthwhile check is to
verify that plugging seven, ﬁve, and three into this polynomial yields zero each time.)
(d) This is the trivial subspace of P3. Thus, the basis is empty ⟨⟩.
Remark. The polynomial in the third item could alternatively have been derived by multiplying out
(x −7)(x −5)(x −3).
Two.III.1.25
Yes. Linear independence and span are unchanged by reordering.
Two.III.1.26
No linearly independent set contains a zero vector.
Two.III.1.27
(a) To show that it is linearly independent, note that d1(c1⃗β1)+d2(c2⃗β2)+d3(c3⃗β3) = ⃗0
gives that (d1c1)⃗β1 + (d2c2)⃗β2 + (d3c3)⃗β3 = ⃗0, which in turn implies that each dici is zero. But with
ci ̸= 0 that means that each di is zero. Showing that it spans the space is much the same; because
⟨⃗β1, ⃗β2, ⃗β3⟩is a basis, and so spans the space, we can for any ⃗v write ⃗v = d1⃗β1 + d2⃗β2 + d3⃗β3, and
then ⃗v = (d1/c1)(c1⃗β1) + (d2/c2)(c2⃗β2) + (d3/c3)(c3⃗β3).
If any of the scalars are zero then the result is not a basis, because it is not linearly independent.
(b) Showing that ⟨2⃗β1, ⃗β1 + ⃗β2, ⃗β1 + ⃗β3⟩is linearly independent is easy. To show that it spans the
space, assume that ⃗v = d1⃗β1 + d2⃗β2 + d3⃗β3. Then, we can represent the same ⃗v with respect to
⟨2⃗β1, ⃗β1 + ⃗β2, ⃗β1 + ⃗β3⟩in this way ⃗v = (1/2)(d1 −d2 −d3)(2⃗β1) + d2(⃗β1 + ⃗β2) + d3(⃗β1 + ⃗β3).
Two.III.1.28
Each forms a linearly independent set if ⃗v is ommitted. To preserve linear independence,
we must expand the span of each. That is, we must determine the span of each (leaving ⃗v out), and
then pick a ⃗v lying outside of that span. Then to ﬁnish, we must check that the result spans the entire
given space. Those checks are routine.
(a) Any vector that is not a multiple of the given one, that is, any vector that is not on the line
y = x will do here. One is ⃗v = ⃗e1.
(b) By inspection, we notice that the vector ⃗e3 is not in the span of the set of the two given vectors.
The check that the resulting set is a basis for R3 is routine.
(c) For any member of the span {c1 · (x) + c2 · (1 + x2)
¯¯ c1, c2 ∈R}, the coeﬃcient of x2 equals the
constant term. So we expand the span if we add a quadratic without this property, say, ⃗v = 1 −x2.
The check that the result is a basis for P2 is easy.
Two.III.1.29
To show that each scalar is zero, simply subtract c1⃗β1+· · ·+ck⃗βk−ck+1⃗βk+1−· · ·−cn⃗βn =
⃗0. The obvious generalization is that in any equation involving only the ⃗β’s, and in which each ⃗β appears
only once, each scalar is zero. For instance, an equation with a combination of the even-indexed basis
vectors (i.e., ⃗β2, ⃗β4, etc.) on the right and the odd-indexed basis vectors on the left also gives the
conclusion that all of the coeﬃcients are zero.
Two.III.1.30
No; no linearly independent set contains the zero vector.
Two.III.1.31
Here is a subset of R2 that is not a basis, and two diﬀerent linear combinations of its
elements that sum to the same vector.
{
µ
1
2
¶
,
µ
2
4
¶
}
2 ·
µ
1
2
¶
+ 0 ·
µ
2
4
¶
= 0 ·
µ
1
2
¶
+ 1 ·
µ
2
4
¶
Thus, when a subset is not a basis, it can be the case that its linear combinations are not unique.
But just because a subset is not a basis does not imply that its combinations must be not unique.
For instance, this set
{
µ
1
2
¶
}
does have the property that
c1 ·
µ
1
2
¶
= c2 ·
µ
1
2
¶
implies that c1 = c2. The idea here is that this subset fails to be a basis because it fails to span the
space; the proof of the theorem establishes that linear combinations are unique if and only if the subset
is linearly independent.

Answers to Exercises
269
Two.III.1.32
(a) Describing the vector space as
{
µ
a
b
b
c
¶ ¯¯ a, b, c ∈R}
suggests this for a basis.
⟨
µ
1
0
0
0
¶
,
µ
0
0
0
1
¶
,
µ
0
1
1
0
¶
⟩
Veriﬁcation is easy.
(b) This is one possible basis.
⟨


1
0
0
0
0
0
0
0
0

,


0
0
0
0
1
0
0
0
0

,


0
0
0
0
0
0
0
0
1

,


0
1
0
1
0
0
0
0
0

,


0
0
1
0
0
0
1
0
0

,


0
0
0
0
0
1
0
1
0

⟩
(c) As in the prior two questions, we can form a basis from two kinds of matrices. First are the
matrices with a single one on the diagonal and all other entries zero (there are n of those matrices).
Second are the matrices with two opposed oﬀ-diagonal entries are ones and all other entries are
zeros. (That is, all entries in M are zero except that mi,j and mj,i are one.)
Two.III.1.33
(a) Any four vectors from R3 are linearly related because the vector equation
c1


x1
y1
z1

+ c2


x2
y2
z2

+ c3


x3
y3
z3

+ c4


x4
y4
z4

=


0
0
0


gives rise to a linear system
x1c1 + x2c2 + x3c3 + x4c4 = 0
y1c1 + y2c2 + y3c3 + y4c4 = 0
z1c1 + z2c2 + z3c3 + z4c4 = 0
that is homogeneous (and so has a solution) and has four unknowns but only three equations, and
therefore has nontrivial solutions. (Of course, this argument applies to any subset of R3 with four
or more vectors.)
(b) Given x1, . . . , z2,
S = {


x1
y1
z1

,


x2
y2
z2

}
to decide which vectors


x
y
z


are in the span of S, set up
c1


x1
y1
z1

+ c2


x2
y2
z2

=


x
y
z


and row reduce the resulting system.
x1c1 + x2c2 = x
y1c1 + y2c2 = y
z1c1 + z2c2 = z
There are two variables c1 and c2 but three equations, so when Gauss’ method ﬁnishes, on the
bottom row there will be some relationship of the form 0 = m1x + m2y + m3z. Hence, vectors in
the span of the two-element set S must satisfy some restriction. Hence the span is not all of R3.
Two.III.1.34
We have (using these peculiar operations with care)
{


1 −y −z
y
z

¯¯ y, z ∈R} = {


−y + 1
y
0

+


−z + 1
0
z

¯¯ y, z ∈R} = {y ·


0
1
0

+ z ·


0
0
1

¯¯ y, z ∈R}
and so a natural candidate for a basis is this.
⟨


0
1
0

,


0
0
1

⟩

270
Linear Algebra, by Hefferon
To check linear independence we set up
c1


0
1
0

+ c2


0
0
1

=


1
0
0


(the vector on the right is the zero object in this space). That yields the linear system
(−c1 + 1) + (−c2 + 1) −1 = 1
c1
= 0
c2
= 0
with only the solution c1 = 0 and c2 = 0. Checking the span is similar.
Subsection Two.III.2: Dimension
Two.III.2.14
One basis is ⟨1, x, x2⟩, and so the dimension is three.
Two.III.2.15
The solution set is
{




4x2 −3x3 + x4
x2
x3
x4




¯¯ x2, x3, x4 ∈R}
so a natural basis is this
⟨




4
1
0
0



,




−3
0
1
0



,




1
0
0
1



⟩
(checking linear independence is easy). Thus the dimension is three.
Two.III.2.16
For this space
{
µ
a
b
c
d
¶ ¯¯ a, b, c, d ∈R} = {a ·
µ
1
0
0
0
¶
+ · · · + d ·
µ
0
0
0
1
¶ ¯¯ a, b, c, d ∈R}
this is a natural basis.
⟨
µ
1
0
0
0
¶
,
µ
0
1
0
0
¶
,
µ
0
0
1
0
¶
,
µ
0
0
0
1
¶
⟩
The dimension is four.
Two.III.2.17
(a) As in the prior exercise, the space M2×2 of matrices without restriction has this
basis
⟨
µ
1
0
0
0
¶
,
µ
0
1
0
0
¶
,
µ
0
0
1
0
¶
,
µ
0
0
0
1
¶
⟩
and so the dimension is four.
(b) For this space
{
µ
a
b
c
d
¶ ¯¯ a = b −2c and d ∈R} = {b ·
µ
1
1
0
0
¶
+ c ·
µ
−2
0
1
0
¶
+ d ·
µ
0
0
0
1
¶ ¯¯ b, c, d ∈R}
this is a natural basis.
⟨
µ
1
1
0
0
¶
,
µ
−2
0
1
0
¶
,
µ
0
0
0
1
¶
⟩
The dimension is three.
(c) Gauss’ method applied to the two-equation linear system gives that c = 0 and that a = −b.
Thus, we have this description
{
µ
−b
b
0
d
¶ ¯¯ b, d ∈R} = {b ·
µ
−1
1
0
0
¶
+ d ·
µ
0
0
0
1
¶ ¯¯ b, d ∈R}
and so this is a natural basis.
⟨
µ
−1
1
0
0
¶
,
µ
0
0
0
1
¶
⟩
The dimension is two.

Answers to Exercises
271
Two.III.2.18
The bases for these spaces are developed in the answer set of the prior subsection.
(a) One basis is ⟨−7 + x, −49 + x2, −343 + x3⟩. The dimension is three.
(b) One basis is ⟨35 −12x + x2, 420 −109x + x3⟩so the dimension is two.
(c) A basis is {−105 + 71x −15x2 + x3}. The dimension is one.
(d) This is the trivial subspace of P3 and so the basis is empty. The dimension is zero.
Two.III.2.19
First recall that cos 2θ = cos2 θ −sin2 θ, and so deletion of cos 2θ from this set leaves
the span unchanged. What’s left, the set {cos2 θ, sin2 θ, sin 2θ}, is linearly independent (consider the
relationship c1 cos2 θ + c2 sin2 θ + c3 sin 2θ = Z(θ) where Z is the zero function, and then take θ = 0,
θ = π/4, and θ = π/2 to conclude that each c is zero). It is therefore a basis for its span. That shows
that the span is a dimension three vector space.
Two.III.2.20
Here is a basis
⟨(1 + 0i, 0 + 0i, . . . , 0 + 0i), (0 + 1i, 0 + 0i, . . . , 0 + 0i), (0 + 0i, 1 + 0i, . . . , 0 + 0i), . . .⟩
and so the dimension is 2 · 47 = 94.
Two.III.2.21
A basis is
⟨


1
0
0
0
0
0
0
0
0
0
0
0
0
0
0

,


0
1
0
0
0
0
0
0
0
0
0
0
0
0
0

, . . . ,


0
0
0
0
0
0
0
0
0
0
0
0
0
0
1

⟩
and thus the dimension is 3 · 5 = 15.
Two.III.2.22
In a four-dimensional space a set of four vectors is linearly independent if and only if
it spans the space. The form of these vectors makes linear independence easy to show (look at the
equation of fourth components, then at the equation of third components, etc.).
Two.III.2.23
(a) The diagram for P2 has four levels. The top level has the only three-dimensional
subspace, P2 itself.
The next level contains the two-dimensional subspaces (not just the linear
polynomials; any two-dimensional subspace, like those polynomials of the form ax2 +b). Below that
are the one-dimensional subspaces. Finally, of course, is the only zero-dimensional subspace, the
trivial subspace.
(b) For M2×2, the diagram has ﬁve levels, including subspaces of dimension four through zero.
Two.III.2.24
(a) One
(b) Two
(c) n
Two.III.2.25
We need only produce an inﬁnite linearly independent set. One is ⟨f1, f2, . . .⟩where
fi : R →R is
fi(x) =
(
1
if x = i
0
otherwise
the function that has value 1 only at x = i.
Two.III.2.26
Considering a function to be a set, speciﬁcally, a set of ordered pairs (x, f(x)), then the
only function with an empty domain is the empty set. Thus this is a trivial vector space, and has
dimension zero.
Two.III.2.27
Apply Corollary 2.8.
Two.III.2.28
A plane has the form {⃗p + t1⃗v1 + t2⃗v2
¯¯ t1, t2 ∈R}. (The ﬁrst chapter also calls this a
‘2-ﬂat’, and contains a discussion of why this is equivalent to the description often taken in Calculus
as the set of points (x, y, z) subject to a condition of the form ax + by + cz = d). When the plane
passes through the origin we can take the particular vector ⃗p to be ⃗0. Thus, in the language we have
developed in this chapter, a plane through the origin is the span of a set of two vectors.
Now for the statement. Asserting that the three are not coplanar is the same as asserting that no
vector lies in the span of the other two — no vector is a linear combination of the other two. That’s
simply an assertion that the three-element set is linearly independent.
By Corollary 2.12, that’s
equivalent to an assertion that the set is a basis for R3.
Two.III.2.29
Let the space V be ﬁnite dimensional. Let S be a subspace of V .
(a) The empty set is a linearly independent subset of S. By Corollary 2.10, it can be expanded to a
basis for the vector space S.
(b) Any basis for the subspace S is a linearly independent set in the superspace V . Hence it can be
expanded to a basis for the superspace, which is ﬁnite dimensional. Therefore it has only ﬁnitely
many members.

272
Linear Algebra, by Hefferon
Two.III.2.30
It ensures that we exhaust the ⃗β’s. That is, it justiﬁes the ﬁrst sentence of the last
paragraph.
Two.III.2.31
Let BU be a basis for U and let BW be a basis for W. The set BU ∪BW is linearly
dependent as it is a six member subset of the ﬁve-dimensional space R5. Thus some member of BW is
in the span of BU, and thus U ∩W is more than just the trivial space {⃗0 }.
Generalization: if U, W are subspaces of a vector space of dimension n and if dim(U)+dim(W) > n
then they have a nontrivial intersection.
Two.III.2.32
First, note that a set is a basis for some space if and only if it is linearly independent,
because in that case it is a basis for its own span.
(a) The answer to the question in the second paragraph is “yes” (implying “yes” answers for both
questions in the ﬁrst paragraph). If BU is a basis for U then BU is a linearly independent subset of
W. Apply Corollary 2.10 to expand it to a basis for W. That is the desired BW .
The answer to the question in the third paragraph is “no”, which implies a “no” answer to the
question of the fourth paragraph. Here is an example of a basis for a superspace with no sub-basis
forming a basis for a subspace: in W = R2, consider the standard basis E2. No sub-basis of E2 forms
a basis for the subspace U of R2 that is the line y = x.
(b) It is a basis (for its span) because the intersection of linearly independent sets is linearly inde-
pendent (the intersection is a subset of each of the linearly independent sets).
It is not, however, a basis for the intersection of the spaces. For instance, these are bases for R2:
B1 = ⟨
µ
1
0
¶
,
µ
0
1
¶
⟩
and
B2 = ⟨
µ
2
0
¶
,
µ
0
2
¶
⟩
and R2 ∩R2 = R2, but B1 ∩B2 is empty. All we can say is that the intersection of the bases is a
basis for a subset of the intersection of the spaces.
(c) The union of bases need not be a basis: in R2
B1 = ⟨
µ
1
0
¶
,
µ
1
1
¶
⟩
and
B2 = ⟨
µ
1
0
¶
,
µ
0
2
¶
⟩
have a union B1 ∪B2 that is not linearly independent. A necessary and suﬃcient condition for a
union of two bases to be a basis
B1 ∪B2 is linearly independent
⇐⇒
[B1 ∩B2] = [B1] ∩[B2]
it is easy enough to prove (but perhaps hard to apply).
(d) The complement of a basis cannot be a basis because it contains the zero vector.
Two.III.2.33
(a) A basis for U is a linearly independent set in W and so can be expanded via
Corollary 2.10 to a basis for W. The second basis has at least as many members as the ﬁrst.
(b) One direction is clear: if V = W then they have the same dimension. For the converse, let BU
be a basis for U. It is a linearly independent subset of W and so can be expanded to a basis for W.
If dim(U) = dim(W) then this basis for W has no more members than does BU and so equals BU.
Since U and W have the same bases, they are equal.
(c) Let W be the space of ﬁnite-degree polynomials and let U be the subspace of polynomails that
have only even-powered terms {a0 + a1x2 + a2x4 + · · · + anx2n ¯¯ a0, . . . , an ∈R}. Both spaces have
inﬁnite dimension, but U is a proper subspace.
Two.III.2.34
The possibilities for the dimension of V are 0, 1, n −1, and n.
To see this, ﬁrst consider the case when all the coordinates of ⃗v are equal.
⃗v =





z
z
...
z





Then σ(⃗v) = ⃗v for every permutation σ, so V is just the span of ⃗v, which has dimension 0 or 1 according
to whether ⃗v is ⃗0 or not.
Now suppose not all the coordinates of ⃗v are equal; let x and y with x ̸= y be among the coordinates
of ⃗v. Then we can ﬁnd permutations σ1 and σ2 such that
σ1(⃗v) =







x
y
a3
...
an







and
σ2(⃗v) =







y
x
a3
...
an








Answers to Exercises
273
for some a3, . . . , an ∈R. Therefore,
1
y −x
¡
σ1(⃗v) −σ2(⃗v)
¢
=







−1
1
0
...
0







is in V . That is, ⃗e2 −⃗e1 ∈V , where ⃗e1, ⃗e2, . . . , ⃗en is the standard basis for Rn. Similarly, ⃗e3 −⃗e2,
. . . , ⃗en −⃗e1 are all in V . It is easy to see that the vectors ⃗e2 −⃗e1, ⃗e3 −⃗e2, . . . , ⃗en −⃗e1 are linearly
independent (that is, form a linearly independent set), so dim V ≥n −1.
Finally, we can write
⃗v = x1⃗e1 + x2⃗e2 + · · · + xn⃗en
= (x1 + x2 + · · · + xn)⃗e1 + x2(⃗e2 −⃗e1) + · · · + xn(⃗en −⃗e1)
This shows that if x1 + x2 + · · · + xn = 0 then ⃗v is in the span of ⃗e2 −⃗e1, . . . , ⃗en −⃗e1 (that is, is in
the span of the set of those vectors); similarly, each σ(⃗v) will be in this span, so V will equal this span
and dim V = n −1. On the other hand, if x1 + x2 + · · · + xn ̸= 0 then the above equation shows that
⃗e1 ∈V and thus ⃗e1, . . . ,⃗en ∈V , so V = Rn and dim V = n.
Subsection Two.III.3: Vector Spaces and Linear Systems
Two.III.3.16
(a)
µ2
3
1
1
¶
(b)
µ2
1
1
3
¶
(c)


1
6
4
7
3
8


(d)
¡
0
0
0
¢
(e)
µ−1
−2
¶
Two.III.3.17
(a) Yes. To see if there are c1 and c2 such that c1 ·
¡
2
1
¢
+ c2 ·
¡
3
1
¢
=
¡
1
0
¢
we
solve
2c1 + 3c2 = 1
c1 + c2 = 0
and get c1 = −1 and c2 = 1. Thus the vector is in the row space.
(b) No. The equation c1
¡
0
1
3
¢
+ c2
¡
−1
0
1
¢
+ c3
¡
−1
2
7
¢
=
¡
1
1
1
¢
has no solution.


0
−1
−1
1
1
0
2
1
3
1
7
1

ρ1↔ρ2
−→
−3ρ1+ρ2
−→
ρ2+ρ3
−→


1
0
2
1
0
−1
−1
1
0
0
0
−1


Thus, the vector is not in the row space.
Two.III.3.18
(a) No. To see if there are c1, c2 ∈R such that
c1
µ
1
1
¶
+ c2
µ
1
1
¶
=
µ
1
3
¶
we can use Gauss’ method on the resulting linear system.
c1 + c2 = 1
c1 + c2 = 3
−ρ1+ρ2
−→
c1 + c2 = 1
0 = 2
There is no solution and so the vector is not in the column space.
(b) Yes. From this relationship
c1


1
2
1

+ c2


3
0
−3

+ c3


1
4
3

=


1
0
0


we get a linear system that, when Gauss’ method is applied,


1
3
1
1
2
0
4
0
1
−3
−3
0

−2ρ1+ρ2
−→
−ρ1+ρ3
−ρ2+ρ3
−→


1
3
1
1
0
−6
2
−2
0
0
−6
1


yields a solution. Thus, the vector is in the column space.

274
Linear Algebra, by Hefferon
Two.III.3.19
A routine Gaussian reduction




2
0
3
4
0
1
1
−1
3
1
0
2
1
0
−4
1




−(3/2)ρ1+ρ3
−→
−(1/2)ρ1+ρ4
−ρ2+ρ3
−→
−ρ3+ρ4
−→




2
0
3
4
0
1
1
−1
0
0
−11/2
−3
0
0
0
0




suggests this basis ⟨
¡2
0
3
4¢
,
¡0
1
1
−1¢
,
¡0
0
−11/2
−3¢
⟩.
Another, perhaps more convenient procedure, is to swap rows ﬁrst,
ρ1↔ρ4
−→
−3ρ1+ρ3
−→
−2ρ1+ρ4
−ρ2+ρ3
−→
−ρ3+ρ4
−→




1
0
−4
−1
0
1
1
−1
0
0
11
6
0
0
0
0




leading to the basis ⟨
¡
1
0
−4
−1
¢
,
¡
0
1
1
−1
¢
,
¡
0
0
11
6
¢
⟩.
Two.III.3.20
(a) This reduction
−(1/2)ρ1+ρ2
−→
−(1/2)ρ1+ρ3
−(1/3)ρ2+ρ3
−→


2
1
3
0
−3/2
1/2
0
0
4/3


shows that the row rank, and hence the rank, is three.
(b) Inspection of the columns shows that that the others are multiples of the ﬁrst (inspection of the
rows shows the same thing). Thus the rank is one.
Alternatively, the reduction

1
−1
2
3
−3
6
−2
2
−4

−3ρ1+ρ2
−→
2ρ1+ρ3


1
−1
2
0
0
0
0
0
0


shows the same thing.
(c) This calculation


1
3
2
5
1
1
6
4
3

−5ρ1+ρ2
−→
−6ρ1+ρ3
−ρ2+ρ3
−→


1
3
2
0
−14
−9
0
0
0


shows that the rank is two.
(d) The rank is zero.
Two.III.3.21
(a) This reduction



1
3
−1
3
1
4
2
1




ρ1+ρ2
−→
−ρ1+ρ3
−2ρ1+ρ4
−(1/6)ρ2+ρ3
−→
(5/6)ρ2+ρ4




1
3
0
6
0
0
0
0




gives ⟨
¡1
3¢
,
¡0
6¢
⟩.
(b) Transposing and reducing


1
2
1
3
1
−1
1
−3
−3

−3ρ1+ρ2
−→
−ρ1+ρ3


1
2
1
0
−5
−4
0
−5
−4

−ρ2+ρ3
−→


1
2
1
0
−5
−4
0
0
0


and then transposing back gives this basis.
⟨


1
2
1

,


0
−5
−4

⟩
(c) Notice ﬁrst that the surrounding space is given as P3, not P2. Then, taking the ﬁrst polynomial
1 + 1 · x + 0 · x2 + 0 · x3 to be “the same” as the row vector
¡
1
1
0
0
¢
, etc., leads to


1
1
0
0
1
0
−1
0
3
2
−1
0

−ρ1+ρ2
−→
−3ρ1+ρ3
−ρ2+ρ3
−→


1
1
0
0
0
−1
−1
0
0
0
0
0


which yields the basis ⟨1 + x, −x −x2⟩.
(d) Here “the same” gives


1
0
1
3
1
−1
1
0
3
2
1
4
−1
0
−5
−1
−1
−9

−ρ1+ρ2
−→
ρ1+ρ3
2ρ2+ρ3
−→


1
0
1
3
1
−1
0
0
2
−1
0
5
0
0
0
0
0
0



Answers to Exercises
275
leading to this basis.
⟨
µ1
0
1
3
1
−1
¶
,
µ 0
0
2
−1
0
5
¶
⟩
Two.III.3.22
Only the zero matrices have rank of zero. The only matrices of rank one have the form



k1 · ρ
...
km · ρ



where ρ is some nonzero row vector, and not all of the ki’s are zero. (Remark. We can’t simply say
that all of the rows are multiples of the ﬁrst because the ﬁrst row might be the zero row. Another
Remark. The above also applies with ‘column’ replacing ‘row’.)
Two.III.3.23
If a ̸= 0 then a choice of d = (c/a)b will make the second row be a multiple of the ﬁrst,
speciﬁcally, c/a times the ﬁrst. If a = 0 and b = 0 then any non-0 choice for d will ensure that the
second row is nonzero. If a = 0 and b ̸= 0 and c = 0 then any choice for d will do, since the matrix
will automatically have rank one (even with the choice of d = 0). Finally, if a = 0 and b ̸= 0 and c ̸= 0
then no choice for d will suﬃce because the matrix is sure to have rank two.
Two.III.3.24
The column rank is two. One way to see this is by inspection — the column space consists
of two-tall columns and so can have a dimension of at least two, and we can easily ﬁnd two columns
that together form a linearly independent set (the fourth and ﬁfth columns, for instance). Another
way to see this is to recall that the column rank equals the row rank, and to perform Gauss’ method,
which leaves two nonzero rows.
Two.III.3.25
We apply Theorem 3.13. The number of columns of a matrix of coeﬃcients A of a linear
system equals the number n of unknowns. A linear system with at least one solution has at most one
solution if and only if the space of solutions of the associated homogeneous system has dimension zero
(recall: in the ‘General = Particular+Homogeneous’ equation ⃗v = ⃗p+⃗h, provided that such a ⃗p exists,
the solution ⃗v is unique if and only if the vector ⃗h is unique, namely ⃗h = ⃗0). But that means, by the
theorem, that n = r.
Two.III.3.26
The set of columns must be dependent because the rank of the matrix is at most ﬁve
while there are nine columns.
Two.III.3.27
There is little danger of their being equal since the row space is a set of row vectors
while the column space is a set of columns (unless the matrix is 1×1, in which case the two spaces
must be equal).
Remark. Consider
A =
µ
1
3
2
6
¶
and note that the row space is the set of all multiples of
¡
1
3
¢
while the column space consists of
multiples of
µ
1
2
¶
so we also cannot argue that the two spaces must be simply transposes of each other.
Two.III.3.28
First, the vector space is the set of four-tuples of real numbers, under the natural oper-
ations. Although this is not the set of four-wide row vectors, the diﬀerence is slight — it is “the same”
as that set. So we will treat the four-tuples like four-wide vectors.
With that, one way to see that (1, 0, 1, 0) is not in the span of the ﬁrst set is to note that this
reduction


1
−1
2
−3
1
1
2
0
3
−1
6
−6

−ρ1+ρ2
−→
−3ρ1+ρ3
−ρ2+ρ3
−→


1
−1
2
−3
0
2
0
3
0
0
0
0


and this one




1
−1
2
−3
1
1
2
0
3
−1
6
−6
1
0
1
0




−ρ1+ρ2
−→
−3ρ1+ρ3
−ρ1+ρ4
−ρ2+ρ3
−→
−(1/2)ρ2+ρ4
ρ3↔ρ4
−→




1
−1
2
−3
0
2
0
3
0
0
−1
3/2
0
0
0
0




yield matrices diﬀering in rank. This means that addition of (1, 0, 1, 0) to the set of the ﬁrst three
four-tuples increases the rank, and hence the span, of that set. Therefore (1, 0, 1, 0) is not already in
the span.

276
Linear Algebra, by Hefferon
Two.III.3.29
It is a subspace because it is the column space of the matrix


3
2
4
1
0
−1
2
2
5


of coeﬃcients. To ﬁnd a basis for the column space,
{c1


3
1
2

+ c2


2
0
2

+ c3


4
−1
5

¯¯ c1, c2, c3 ∈R}
we take the three vectors from the spanning set, transpose, reduce,


3
1
2
2
0
2
4
−1
5

−(2/3)ρ1+ρ2
−→
−(4/3)ρ1+ρ3
−(7/2)ρ2+ρ3
−→


3
1
2
0
−2/3
2/3
0
0
0


and transpose back to get this.
⟨


3
1
2

,


0
−2/3
2/3

⟩
Two.III.3.30
This can be done as a straightforward calculation.
(rA + sB)trans =



ra1,1 + sb1,1
. . .
ra1,n + sb1,n
...
...
ram,1 + sbm,1
. . .
ram,n + sbm,n



trans
=



ra1,1 + sb1,1
. . .
ram,1 + sbm,1
...
ra1,n + sb1,n
. . .
ram,n + sbm,n



=



ra1,1
. . .
ram,1
...
ra1,n
. . .
ram,n


+



sb1,1
. . .
sbm,1
...
sb1,n
. . .
sbm,n



= rAtrans + sBtrans
Two.III.3.31
(a) These reductions give diﬀerent bases.
µ1
2
0
1
2
1
¶
−ρ1+ρ2
−→
µ1
2
0
0
0
1
¶
µ1
2
0
1
2
1
¶
−ρ1+ρ2
−→
2ρ2
−→
µ1
2
0
0
0
2
¶
(b) An easy example is this.
µ
1
2
1
3
1
4
¶


1
2
1
3
1
4
0
0
0


This is a less simplistic example.
µ
1
2
1
3
1
4
¶




1
2
1
3
1
4
2
4
2
4
3
5




(c) Assume that A and B are matrices with equal row spaces. Construct a matrix C with the rows
of A above the rows of B, and another matrix D with the rows of B above the rows of A.
C =
µ
A
B
¶
D =
µ
B
A
¶
Observe that C and D are row-equivalent (via a sequence of row-swaps) and so Gauss-Jordan reduce
to the same reduced echelon form matrix.
Because the row spaces are equal, the rows of B are linear combinations of the rows of A so
Gauss-Jordan reduction on C simply turns the rows of B to zero rows and thus the nonzero rows
of C are just the nonzero rows obtained by Gauss-Jordan reducing A. The same can be said for
the matrix D — Gauss-Jordan reduction on D gives the same non-zero rows as are produced by
reduction on B alone.
Therefore, A yields the same nonzero rows as C, which yields the same
nonzero rows as D, which yields the same nonzero rows as B.

Answers to Exercises
277
Two.III.3.32
It cannot be bigger.
Two.III.3.33
The number of rows in a maximal linearly independent set cannot exceed the number
of rows. A better bound (the bound that is, in general, the best possible) is the minimum of m and
n, because the row rank equals the column rank.
Two.III.3.34
Because the rows of a matrix A are turned into the columns of Atrans the dimension of
the row space of A equals the dimension of the column space of Atrans. But the dimension of the row
space of A is the rank of A and the dimension of the column space of Atrans is the rank of Atrans. Thus
the two ranks are equal.
Two.III.3.35
False. The ﬁrst is a set of columns while the second is a set of rows.
This example, however,
A =
µ
1
2
3
4
5
6
¶
,
Atrans =


1
4
2
5
3
6


indicates that as soon as we have a formal meaning for “the same”, we can apply it here:
Columnspace(A) = [{
µ
1
4
¶
,
µ
2
5
¶
,
µ
3
6
¶
}]
while
Rowspace(Atrans) = [{
¡1
4¢
,
¡2
5¢
,
¡3
6¢
}]
are “the same” as each other.
Two.III.3.36
No. Here, Gauss’ method does not change the column space.
µ
1
0
3
1
¶
−3ρ1+ρ2
−→
µ
1
0
0
1
¶
Two.III.3.37
A linear system
c1⃗a1 + · · · + cn⃗an = ⃗d
has a solution if and only if ⃗d is in the span of the set {⃗a1, . . . ,⃗an}. That’s true if and only if the
column rank of the augmented matrix equals the column rank of the matrix of coeﬃcients. Since rank
equals the column rank, the system has a solution if and only if the rank of its augmented matrix
equals the rank of its matrix of coeﬃcients.
Two.III.3.38
(a) Row rank equals column rank so each is at most the minimum of the number of
rows and columns. Hence both can be full only if the number of rows equals the number of columns.
(Of course, the converse does not hold: a square matrix need not have full row rank or full column
rank.)
(b) If A has full row rank then, no matter what the right-hand side, Gauss’ method on the augmented
matrix ends with a leading one in each row and none of those leading ones in the furthest right column
(the “augmenting” column). Back substitution then gives a solution.
On the other hand, if the linear system lacks a solution for some right-hand side it can only be
because Gauss’ method leaves some row so that it is all zeroes to the left of the “augmenting” bar
and has a nonzero entry on the right. Thus, if A does not have a solution for some right-hand sides,
then A does not have full row rank because some of its rows have been eliminated.
(c) The matrix A has full column rank if and only if its columns form a linearly independent set.
That’s equivalent to the existence of only the trivial linear relationship.
(d) The matrix A has full column rank if and only if the set of its columns is linearly independent,
and so forms a basis for its span. That’s equivalent to the existence of a unique linear representation
of all vectors in that span.
Two.III.3.39
Instead of the row spaces being the same, the row space of B would be a subspace
(possibly equal to) the row space of A.
Two.III.3.40
Clearly rank(A) = rank(−A) as Gauss’ method allows us to multiply all rows of a matrix
by −1. In the same way, when k ̸= 0 we have rank(A) = rank(kA).
Addition is more interesting. The rank of a sum can be smaller than the rank of the summands.
µ
1
2
3
4
¶
+
µ
−1
−2
−3
−4
¶
=
µ
0
0
0
0
¶
The rank of a sum can be bigger than the rank of the summands.
µ
1
2
0
0
¶
+
µ
0
0
3
4
¶
=
µ
1
2
3
4
¶

278
Linear Algebra, by Hefferon
But there is an upper bound (other than the size of the matrices).
In general, rank(A + B) ≤
rank(A) + rank(B).
To prove this, note that Gaussian elimination can be performed on A + B in either of two ways:
we can ﬁrst add A to B and then apply the appropriate sequence of reduction steps
(A + B)
step1
−→· · ·
stepk
−→echelon form
or we can get the same results by performing step1 through stepk separately on A and B, and then
adding. The largest rank that we can end with in the second case is clearly the sum of the ranks.
(The matrices above give examples of both possibilities, rank(A + B) < rank(A) + rank(B) and
rank(A + B) = rank(A) + rank(B), happening.)
Subsection Two.III.4: Combining Subspaces
Two.III.4.20
With each of these we can apply Lemma 4.15.
(a) Yes. The plane is the sum of this W1 and W2 because for any scalars a and b
µ
a
b
¶
=
µ
a −b
0
¶
+
µ
b
b
¶
shows that the general vector is a sum of vectors from the two parts. And, these two subspaces are
(diﬀerent) lines through the origin, and so have a trivial intersection.
(b) Yes. To see that any vector in the plane is a combination of vectors from these parts, consider
this relationship.
µ
a
b
¶
= c1
µ
1
1
¶
+ c2
µ
1
1.1
¶
We could now simply note that the set
{
µ
1
1
¶
,
µ
1
1.1
¶
}
is a basis for the space (because it is clearly linearly independent, and has size two in R2), and thus
ther is one and only one solution to the above equation, implying that all decompositions are unique.
Alternatively, we can solve
c1 +
c2 = a
c1 + 1.1c2 = b
−ρ1+ρ2
−→
c1 +
c2 =
a
0.1c2 = −a + b
to get that c2 = 10(−a + b) and c1 = 11a −10b, and so we have
µ
a
b
¶
=
µ
11a −10b
11a −10b
¶
+
µ
−10a + 10b
1.1 · (−10a + 10b)
¶
as required. As with the prior answer, each of the two subspaces is a line through the origin, and
their intersection is trivial.
(c) Yes. Each vector in the plane is a sum in this way
µ
x
y
¶
=
µ
x
y
¶
+
µ
0
0
¶
and the intersection of the two subspaces is trivial.
(d) No. The intersection is not trivial.
(e) No. These are not subspaces.
Two.III.4.21
With each of these we can use Lemma 4.15.
(a) Any vector in R3 can be decomposed as this sum.


x
y
z

=


x
y
0

+


0
0
z


And, the intersection of the xy-plane and the z-axis is the trivial subspace.
(b) Any vector in R3 can be decomposed as


x
y
z

=


x −z
y −z
0

+


z
z
z


and the intersection of the two spaces is trivial.

Answers to Exercises
279
Two.III.4.22
It is. Showing that these two are subspaces is routine. To see that the space is the direct
sum of these two, just note that each member of P2 has the unique decomposition m + nx + px2 =
(m + px2) + (nx).
Two.III.4.23
To show that they are subspaces is routine. We will argue they are complements with
Lemma 4.15. The intersection E ∩O is trivial because the only polynomial satisfying both conditions
p(−x) = p(x) and p(−x) = −p(x) is the zero polynomial. To see that the entire space is the sum of
the subspaces E + O = Pn, note that the polynomials p0(x) = 1, p2(x) = x2, p4(x) = x4, etc., are in
E and also note that the polynomials p1(x) = x, p3(x) = x3, etc., are in O. Hence any member of Pn
is a combination of members of E and O.
Two.III.4.24
Each of these is R3.
(a) These are broken into lines for legibility.
W1 + W2 + W3, W1 + W2 + W3 + W4, W1 + W2 + W3 + W5, W1 + W2 + W3 + W4 + W5,
W1 + W2 + W4, W1 + W2 + W4 + W5, W1 + W2 + W5,
W1 + W3 + W4, W1 + W3 + W5, W1 + W3 + W4 + W5,
W1 + W4, W1 + W4 + W5,
W1 + W5,
W2 + W3 + W4, W2 + W3 + W4 + W5,
W2 + W4, W2 + W4 + W5,
W3 + W4, W3 + W4 + W5,
W4 + W5
(b) W1 ⊕W2 ⊕W3, W1 ⊕W4, W1 ⊕W5, W2 ⊕W4, W3 ⊕W4
Two.III.4.25
Clearly each is a subspace. The bases Bi = ⟨xi⟩for the subspaces, when concatenated,
form a basis for the whole space.
Two.III.4.26
It is W2.
Two.III.4.27
True by Lemma 4.8.
Two.III.4.28
Two distinct direct sum decompositions of R4 are easy to ﬁnd. Two such are W1 =
[{⃗e1,⃗e2}] and W2 = [{⃗e3,⃗e4}], and also U1 = [{⃗e1}] and U2 = [{⃗e2,⃗e3,⃗e4}]. (Many more are possible,
for example R4 and its trivial subspace.)
In contrast, any partition of R1’s single-vector basis will give one basis with no elements and another
with a single element. Thus any decomposition involves R1 and its trivial subspace.
Two.III.4.29
Set inclusion one way is easy: {⃗w1 + · · · + ⃗wk
¯¯ ⃗wi ∈Wi} is a subset of [W1 ∪. . . ∪Wk]
because each ⃗w1 + · · · + ⃗wk is a sum of vectors from the union.
For the other inclusion, to any linear combination of vectors from the union apply commutativity
of vector addition to put vectors from W1 ﬁrst, followed by vectors from W2, etc. Add the vectors
from W1 to get a ⃗w1 ∈W1, add the vectors from W2 to get a ⃗w2 ∈W2, etc. The result has the desired
form.
Two.III.4.30
One example is to take the space to be R3, and to take the subspaces to be the xy-plane,
the xz-plane, and the yz-plane.
Two.III.4.31
Of course, the zero vector is in all of the subspaces, so the intersection contains at least
that one vector..
By the deﬁnition of direct sum the set {W1, . . . , Wk} is independent and so no
nonzero vector of Wi is a multiple of a member of Wj, when i ̸= j. In particular, no nonzero vector
from Wi equals a member of Wj.
Two.III.4.32
It can contain a trivial subspace; this set of subspaces of R3 is independent: {{⃗0}, x-axis}.
No nonzero vector from the trivial space {⃗0} is a multiple of a vector from the x-axis, simply because
the trivial space has no nonzero vectors to be candidates for such a multiple (and also no nonzero
vector from the x-axis is a multiple of the zero vector from the trivial subspace).
Two.III.4.33
Yes. For any subspace of a vector space we can take any basis ⟨⃗ω1, . . . , ⃗ωk⟩for that
subspace and extend it to a basis ⟨⃗ω1, . . . , ⃗ωk, ⃗βk+1, . . . , ⃗βn⟩for the whole space. Then the complemen
of the original subspace has this for a basis: ⟨⃗βk+1, . . . , ⃗βn⟩.
Two.III.4.34
(a) It must. Any member of W1 + W2 can be written ⃗w1 + ⃗w2 where ⃗w1 ∈W1 and
⃗w2 ∈W2. As S1 spans W1, the vector ⃗w1 is a combination of members of S1. Similarly ⃗w2 is a
combination of members of S2.

280
Linear Algebra, by Hefferon
(b) An easy way to see that it can be linearly independent is to take each to be the empty set. On
the other hand, in the space R1, if W1 = R1 and W2 = R1 and S1 = {1} and S2 = {2}, then their
union S1 ∪S2 is not independent.
Two.III.4.35
(a) The intersection and sum are
{
µ
0
0
c
0
¶ ¯¯ c ∈R}
{
µ
0
b
c
d
¶ ¯¯ b, c, d ∈R}
which have dimensions one and three.
(b) We write BU∩W for the basis for U ∩W, we write BU for the basis for U, we write BW for the
basis for W, and we write BU+W for the basis under consideration.
To see that that BU+W spans U + W, observe that any vector c⃗u + d⃗w from U + W can be
written as a linear combination of the vectors in BU+W , simply by expressing ⃗u in terms of BU and
expressing ⃗w in terms of BW .
We ﬁnish by showing that BU+W is linearly independent. Consider
c1⃗µ1 + · · · + cj+1⃗β1 + · · · + cj+k+p⃗ωp = ⃗0
which can be rewritten in this way.
c1⃗µ1 + · · · + cj⃗µj = −cj+1⃗β1 −· · · −cj+k+p⃗ωp
Note that the left side sums to a vector in U while right side sums to a vector in W, and thus both
sides sum to a member of U ∩W. Since the left side is a member of U ∩W, it is expressible in terms
of the members of BU∩W , which gives the combination of ⃗µ’s from the left side above as equal to
a combination of ⃗β’s. But, the fact that the basis BU is linearly independent shows that any such
combination is trivial, and in particular, the coeﬃcients c1, . . . , cj from the left side above are all
zero. Similarly, the coeﬃcients of the ⃗ω’s are all zero. This leaves the above equation as a linear
relationship among the ⃗β’s, but BU∩W is linearly independent, and therefore all of the coeﬃcients
of the ⃗β’s are also zero.
(c) Just count the basis vectors in the prior item: dim(U + W) = j + k + p, and dim(U) = j + k, and
dim(W) = k + p, and dim(U ∩W) = k.
(d) We know that dim(W1 + W2) = dim(W1) + dim(W2) −dim(W1 ∩W2). Because W1 ⊆W1 + W2,
we know that W1 + W2 must have dimension greater than that of W1, that is, must have dimension
eight, nine, or ten.
Substituting gives us three possibilities 8 = 8 + 8 −dim(W1 ∩W2) or 9 =
8 + 8 −dim(W1 ∩W2) or 10 = 8 + 8 −dim(W1 ∩W2). Thus dim(W1 ∩W2) must be either eight,
seven, or six. (Giving examples to show that each of these three cases is possible is easy, for instance
in R10.)
Two.III.4.36
Expand each Si to a basis Bi for Wi. The concatenation of those bases B1
⌢· · ·
⌢Bk is
a basis for V and thus its members form a linearly independent set. But the union S1 ∪· · · ∪Sk is a
subset of that linearly independent set, and thus is itself linearly independent.
Two.III.4.37
(a) Two such are these. µ
1
2
2
3
¶
µ
0
1
−1
0
¶
For the antisymmetric one, entries on the diagonal must be zero.
(b) A square symmetric matrix equals its transpose.
A square antisymmetric matrix equals the
negative of its transpose.
(c) Showing that the two sets are subspaces is easy. Suppose that A ∈Mn×n. To express A as a
sum of a symmetric and an antisymmetric matrix, we observe that
A = (1/2)(A + Atrans) + (1/2)(A −Atrans)
and note the ﬁrst summand is symmetric while the second is antisymmetric. Thus Mn×n is the
sum of the two subspaces. To show that the sum is direct, assume a matrix A is both symmetric
A = Atrans and antisymmetric A = −Atrans. Then A = −A and so all of A’s entries are zeroes.
Two.III.4.38
Assume that ⃗v ∈(W1 ∩W2) + (W1 ∩W3). Then ⃗v = ⃗w2 + ⃗w3 where ⃗w2 ∈W1 ∩W2 and
⃗w3 ∈W1 ∩W3. Note that ⃗w2, ⃗w3 ∈W1 and, as a subspace is closed under addition, ⃗w2 + ⃗w3 ∈W1.
Thus ⃗v = ⃗w2 + ⃗w3 ∈W1 ∩(W2 + W3).
This example proves that the inclusion may be strict: in R2 take W1 to be the x-axis, take W2 to
be the y-axis, and take W3 to be the line y = x. Then W1 ∩W2 and W1 ∩W3 are trivial and so their
sum is trivial. But W2 + W3 is all of R2 so W1 ∩(W2 + W3) is the x-axis.

Answers to Exercises
281
Two.III.4.39
It happens when at least one of W1, W2 is trivial. But that is the only way it can happen.
To prove this, assume that both are non-trivial, select nonzero vectors ⃗w1, ⃗w2 from each, and
consider ⃗w1 + ⃗w2. This sum is not in W1 because ⃗w1 + ⃗w2 = ⃗v ∈W1 would imply that ⃗w2 = ⃗v −⃗w1 is
in W1, which violates the assumption of the independence of the subspaces. Similarly, ⃗w1 + ⃗w2 is not
in W2. Thus there is an element of V that is not in W1 ∪W2.
Two.III.4.40
(a) The set
{
µ
v1
v2
¶ ¯¯
µ
v1
v2
¶ µ
x
0
¶
= 0 for all x ∈R}
is easily seen to be the y-axis.
(b) The yz-plane.
(c) The z-axis.
(d) Assume that U is a subspace of some Rn. Because U ⊥contains the zero vector, since that vector
is perpendicular to everything, we need only show that the orthocomplement is closed under linear
combinations of two elements. If ⃗w1, ⃗w2 ∈U ⊥then ⃗w1
⃗u = 0 and ⃗w2
⃗u = 0 for all ⃗u ∈U. Thus
(c1 ⃗w1 + c2 ⃗w2)
⃗u = c1(⃗w1
⃗u) + c2(⃗w2
⃗u) = 0 for all ⃗u ∈U and so U ⊥is closed under linear
combinations.
(e) The only vector orthogonal to itself is the zero vector.
(f) This is immediate.
(g) To prove that the dimensions add, it suﬃces by Corollary 4.13 and Lemma 4.15 to show that
U ∩U ⊥is the trivial subspace {⃗0}. But this is one of the prior items in this problem.
Two.III.4.41
Yes. The left-to-right implication is Corollary 4.13. For the other direction, assume that
dim(V ) = dim(W1) + · · · + dim(Wk). Let B1, . . . , Bk be bases for W1, . . . , Wk. As V is the sum of the
subspaces, any ⃗v ∈V can be written ⃗v = ⃗w1 + · · · + ⃗wk and expressing each ⃗wi as a combination of
vectors from the associated basis Bi shows that the concatenation B1
⌢· · ·
⌢Bk spans V . Now, that
concatenation has dim(W1) + · · · + dim(Wk) members, and so it is a spanning set of size dim(V ). The
concatenation is therefore a basis for V . Thus V is the direct sum.
Two.III.4.42
No. The standard basis for R2 does not split into bases for the complementary subspaces
the line x = y and the line x = −y.
Two.III.4.43
(a) Yes, W1 + W2 = W2 + W1 for all subspaces W1, W2 because each side is the span
of W1 ∪W2 = W2 ∪W1.
(b) This one is similar to the prior one — each side of that equation is the span of (W1 ∪W2)∪W3 =
W1 ∪(W2 ∪W3).
(c) Because this is an equality between sets, we can show that it holds by mutual inclusion. Clearly
W ⊆W + W. For W + W ⊆W just recall that every subset is closed under addition so any sum of
the form ⃗w1 + ⃗w2 is in W.
(d) In each vector space, the identity element with respect to subspace addition is the trivial subspace.
(e) Neither of left or right cancelation needs to hold. For an example, in R3 take W1 to be the
xy-plane, take W2 to be the x-axis, and take W3 to be the y-axis.
Two.III.4.44
(a) They are equal because for each, V is the direct sum if and only if each ⃗v ∈V can
be written in a unique way as a sum ⃗v = ⃗w1 + ⃗w2 and ⃗v = ⃗w2 + ⃗w1.
(b) They are equal because for each, V is the direct sum if and only if each ⃗v ∈V can be written in
a unique way as a sum of a vector from each ⃗v = (⃗w1 + ⃗w2) + ⃗w3 and ⃗v = ⃗w1 + (⃗w2 + ⃗w3).
(c) Any vector in R3 can be decomposed uniquely into the sum of a vector from each axis.
(d) No. For an example, in R2 take W1 to be the x-axis, take W2 to be the y-axis, and take W3 to
be the line y = x.
(e) In any vector space the trivial subspace acts as the identity element with respect to direct sum.
(f) In any vector space, only the trivial subspace has a direct-sum inverse (namely, itself). One way
to see this is that dimensions add, and so increase.
Topic: Fields
1
These checks are all routine; most consist only of remarking that property is so familiar that it does
not need to be proved.

282
Linear Algebra, by Hefferon
2
For both of these structures, these checks are all routine. As with the prior question, most of the
checks consist only of remarking that property is so familiar that it does not need to be proved.
3
There is no multiplicative inverse for 2 so the integers do not satisfy condition (5).
4
These checks can be done by listing all of the possibilities. For instance, to verify the commutativity
of addition, that a + b = b + a, we can easily check it for all possible pairs a, b, because there are
only four such pairs. Similarly, for associativity, there are only eight triples a, b, c, and so the check
is not too long. (There are other ways to do the checks, in particular, a reader may recognize these
operations as arithmetic ‘mod 2’.)
5
These will do.
+
0
1
2
0
0
1
2
1
1
2
0
2
2
0
1
·
0
1
2
0
0
0
0
1
0
1
2
2
0
2
1
As in the prior item, the check that they satisfy the conditions can be done by listing all of the cases,
although this way of checking is somewhat long (making use of commutativity is helpful in shortening
the work).
Topic: Crystals
1
Each fundamental unit is 3.34 × 10−10 cm, so there are about 0.1/(3.34 × 10−10) such units. That
gives 2.99 × 108, so there are something like 300, 000, 000 (three hundred million) units.
2
(a) We solve
c1
µ1.42
0
¶
+ c2
µ1.23
0.71
¶
=
µ5.67
3.14
¶
=⇒
1.42c1 + 1.23c2 = 5.67
0.71c2 = 3.14
to get c2 =≈4.42 and c1 ≈0.16.
(b) Here is the point located in the lattice. In the picture on the left, superimposed on the unit cell
are the two basis vectors ⃗β1 and ⃗β2, and a box showing the oﬀset of 0.16⃗β1 + 4.42⃗β2. The picture on
the right shows where that appears inside of the crystal lattice, taking as the origin the lower left
corner of the hexagon in the lower left.
So this point is in the next column of hexagons over, and either one hexagon up or two hexagons
up, depending on how you count them.
(c) This second basis
⟨
µ1.42
0
¶
,
µ 0
1.42
¶
⟩
makes the computation easier
c1
µ
1.42
0
¶
+ c2
µ
0
1.42
¶
=
µ
5.67
3.14
¶
=⇒
1.42c1
= 5.67
1.42c2 = 3.14
(we get c2 ≈2.21 and c1 ≈3.99), but it doesn’t seem to have to do much with the physical structure
that we are studying.
3
In terms of the basis the locations of the corner atoms are (0, 0, 0), (1, 0, 0), . . . , (1, 1, 1). The locations
of the face atoms are (0.5, 0.5, 1), (1, 0.5, 0.5), (0.5, 1, 0.5), (0, 0.5, 0.5), (0.5, 0, 0.5), and (0.5, 0.5, 0). The
locations of the atoms a quarter of the way down from the top are (0.75, 0.75, 0.75) and (0.25, 0.25, 0.25).
The atoms a quarter of the way up from the bottom are at (0.75, 0.25, 0.25) and (0.25, 0.75, 0.25).
Converting to ˚Angstroms is easy.

Answers to Exercises
283
4
(a) 195.08/6.02 × 1023 = 3.239 × 10−22
(b) 4
(c) 4 · 3.239 × 10−22 = 1.296 × 10−21
(d) 1.296 × 10−21/21.45 = 6.042 × 10−23 cubic centimeters
(e) 3.924 × 10−8 centimeters.
(f) ⟨


3.924 × 10−8
0
0

,


0
3.924 × 10−8
0

,


0
0
3.924 × 10−8

⟩
Topic: Dimensional Analysis
1
(a) This relationship
(L1M 0T 0)p1(L1M 0T 0)p2(L1M 0T −1)p3(L0M 0T 0)p4(L1M 0T −2)p5(L0M 0T 1)p6 = L0M 0T 0
gives rise to this linear system
p1 + p2 +
p3 + p5
= 0
0 = 0
−p3 −2p5 + p6 = 0
(note that there is no restriction on p4). The natural parametrization uses the free variables to give
p3 = −2p5 + p6 and p1 = −p2 + p5 −p6. The resulting description of the solution set
{








p1
p2
p3
p4
p5
p6








= p2








−1
1
0
0
0
0








+ p4








0
0
0
1
0
0








+ p5








1
0
−2
0
1
0








+ p6








−1
0
1
0
0
1








¯¯ p2, p4, p5, p6 ∈R}
gives {y/x, θ, xt/v02, v0t/x} as a complete set of dimensionless products (recall that “complete” in
this context does not mean that there are no other dimensionless products; it simply means that the
set is a basis). This is, however, not the set of dimensionless products that the question asks for.
There are two ways to proceed. The ﬁrst is to ﬁddle with the choice of parameters, hoping to
hit on the right set. For that, we can do the prior paragraph in reverse. Converting the given
dimensionless products gt/v0, gx/v2
0, gy/v2
0, and θ into vectors gives this description (note the ?’s
where the parameters will go).
{








p1
p2
p3
p4
p5
p6








= ?








0
0
−1
0
1
1








+ ?








1
0
−2
0
1
0








+ ?








0
1
−2
0
1
0








+ p4








0
0
0
1
0
0








¯¯ p2, p4, p5, p6 ∈R}
The p4 is already in place. Examining the rows shows that we can also put in place p6, p1, and p2.
The second way to proceed, following the hint, is to note that the given set is of size four in
a four-dimensional vector space and so we need only show that it is linearly independent. That
is easily done by inspection, by considering the sixth, ﬁrst, second, and fourth components of the
vectors.
(b) The ﬁrst equation can be rewritten
gx
v02 = gt
v0
cos θ
so that Buckingham’s function is f1(Π1, Π2, Π3, Π4) = Π2 −Π1 cos(Π4). The second equation can
be rewritten
gy
v02 = gt
v0
sin θ −1
2
µgt
v0
¶2
and Buckingham’s function here is f2(Π1, Π2, Π3, Π4) = Π3 −Π1 sin(Π4) + (1/2)Π1
2.

284
Linear Algebra, by Hefferon
2
We consider
(L0M 0T −1)p1(L1M −1T 2)p2(L−3M 0T 0)p3(L0M 1T 0)p4 = (L0M 0T 0)
which gives these relations among the powers.
p2 −3p3
= 0
−p2
+ p4 = 0
−p1 + 2p2
= 0
ρ1↔ρ3
−→
ρ2+ρ3
−→
−p1 + 2p2
= 0
−p2
+ p4 = 0
−3p3 + p4 = 0
This is the solution space (because we wish to express k as a function of the other quantities, p2 is
taken as the parameter).
{




2
1
1/3
1



p2
¯¯ p2 ∈R}
Thus, Π1 = ν2kN 1/3m is the dimensionless combination, and we have that k equals ν−2N −1/3m−1
times a constant (the function ˆf is constant since it has no arguments).
3
(a) Setting
(L2M 1T −2)p1(L0M 0T −1)p2(L3M 0T 0)p3 = (L0M 0T 0)
gives this
2p1
+ 3p3 = 0
p1
= 0
−2p1 −p2
= 0
which implies that p1 = p2 = p3 = 0. That is, among quantities with these dimensional formulas,
the only dimensionless product is the trivial one.
(b) Setting
(L2M 1T −2)p1(L0M 0T −1)p2(L3M 0T 0)p3(L−3M 1T 0)p4 = (L0M 0T 0)
gives this.
2p1
+ 3p3 −3p4 = 0
p1
+ p4 = 0
−2p1 −p2
= 0
(−1/2)ρ1+ρ2
−→
ρ1+ρ3
ρ2↔ρ3
−→
2p1
+
3p3 −
3p4 = 0
−p2 +
3p3 −
3p4 = 0
(−3/2)p3 + (5/2)p4 = 0
Taking p1 as parameter to express the torque gives this description of the solution set.
{




1
−2
−5/3
−1



p1
¯¯ p1 ∈R}
Denoting the torque by τ, the rotation rate by r, the volume of air by V , and the density of air by
d we have that Π1 = τr−2V −5/3d−1, and so the torque is r2V 5/3d times a constant.
4
(a) These are the dimensional formulas.
quantity
dimensional
formula
speed of the wave v
L1M 0T −1
separation of the dominoes d
L1M 0T 0
height of the dominoes h
L1M 0T 0
acceleration due to gravity g
L1M 0T −2
(b) The relationship
(L1M 0T −1)p1(L1M 0T 0)p2(L1M 0T 0)p3(L1M 0T −2)p4 = (L0M 0T 0)
gives this linear system.
p1 + p2 + p3 + p4 = 0
0 = 0
−p1
−2p4 = 0
ρ1+ρ4
−→
p1 + p2 + p3 + p4 = 0
p2 + p3 −p4 = 0
Taking p3 and p4 as parameters, the solution set is described in this way.
{




0
−1
1
0



p3 +




−2
1
0
1



p4
¯¯ p3, p4 ∈R}
That gives {Π1 = h/d, Π2 = dg/v2} as a complete set.

Answers to Exercises
285
(c) Buckingham’s Theorem says that v2 = dg · ˆf(h/d), and so, since g is a constant, if h/d is ﬁxed
then v is proportional to
√
d .
5
Checking the conditions in the deﬁnition of a vector space is routine.
6
(a) The dimensional formula of the circumference is L, that is, L1M 0T 0. The dimensional formula
of the area is L2.
(b) One is C + A = 2πr + πr2.
(c) One example is this formula relating the the length of arc subtended by an angle to the radius
and the angle measure in radians: ℓ−rθ = 0. Both terms in that formula have dimensional formula
L1. The relationship holds for some unit systems (inches and radians, for instance) but not for all
unit systems (inches and degrees, for instance).


Chapter Three: Maps Between Spaces
Subsection Three.I.1: Definition and Examples
Three.I.1.10
(a) Call the map f.
¡a
b¢
f
7−→
µ
a
b
¶
It is one-to-one because if f sends two members of the domain to the same image, that is, if
f
¡¡
a
b
¢¢
= f
¡¡
c
d
¢¢
, then the deﬁnition of f gives that
µ
a
b
¶
=
µ
c
d
¶
and since column vectors are equal only if they have equal components, we have that a = c and that
b = d. Thus, if f maps two row vectors from the domain to the same column vector then the two
row vectors are equal:
¡
a
b
¢
=
¡
c
d
¢
.
To show that f is onto we must show that any member of the codomain R2 is the image under
f of some row vector. That’s easy;
µ
x
y
¶
is f
¡¡
x
y
¢¢
.
The computation for preservation of addition is this.
f
¡¡
a
b
¢
+
¡
c
d
¢¢
= f
¡¡
a + c
b + d
¢¢
=
µ
a + c
b + d
¶
=
µ
a
b
¶
+
µ
c
d
¶
= f
¡¡
a
b
¢¢
+ f
¡¡
c
d
¢¢
The computation for preservation of scalar multiplication is similar.
f
¡
r ·
¡
a
b
¢¢
= f
¡¡
ra
rb
¢¢
=
µra
rb
¶
= r ·
µa
b
¶
= r · f
¡¡
a
b
¢¢
(b) Denote the map from Example 1.2 by f. To show that it is one-to-one, assume that f(a0 +a1x+
a2x2) = f(b0 + b1x + b2x2). Then by the deﬁnition of the function,


a0
a1
a2

=


b0
b1
b2


and so a0 = b0 and a1 = b1 and a2 = b2. Thus a0 + a1x + a2x2 = b0 + b1x + b2x2, and consequently
f is one-to-one.
The function f is onto because there is a polynomial sent to


a
b
c


by f, namely, a + bx + cx2.
As for structure, this shows that f preserves addition
f
¡
(a0 + a1x + a2x2) + (b0 + b1x + b2x2)
¢
= f
¡
(a0 + b0) + (a1 + b1)x + (a2 + b2)x2 ¢
=


a0 + b0
a1 + b1
a2 + b2


=


a0
a1
a2

+


b0
b1
b2


= f(a0 + a1x + a2x2) + f(b0 + b1x + b2x2)

288
Linear Algebra, by Hefferon
and this shows
f( r(a0 + a1x + a2x2) ) = f( (ra0) + (ra1)x + (ra2)x2 )
=


ra0
ra1
ra2


= r ·


a0
a1
a2


= r f(a0 + a1x + a2x2)
that it preserves scalar multiplication.
Three.I.1.11
These are the images.
(a)
µ
5
−2
¶
(b)
µ
0
2
¶
(c)
µ
−1
1
¶
To prove that f is one-to-one, assume that it maps two linear polynomials to the same image
f(a1 + b1x) = f(a2 + b2x). Then
µ
a1 −b1
b1
¶
=
µ
a2 −b2
b2
¶
and so, since column vectors are equal only when their components are equal, b1 = b2 and a1 = a2.
That shows that the two linear polynomials are equal, and so f is one-to-one.
To show that f is onto, note that this member of the codomain
µ
s
t
¶
is the image of this member of the domain (s + t) + tx.
To check that f preserves structure, we can use item (2) of Lemma 1.9.
f (c1 · (a1 + b1x) + c2 · (a2 + b2x)) = f ((c1a1 + c2a2) + (c1b1 + c2b2)x)
=
µ
(c1a1 + c2a2) −(c1b1 + c2b2)
c1b1 + c2b2
¶
= c1 ·
µ
a1 −b1
b1
¶
+ c2 ·
µ
a2 −b2
b2
¶
= c1 · f(a1 + b1x) + c2 · f(a2 + b2x)
Three.I.1.12
To verify it is one-to-one, assume that f1(c1x + c2y + c3z) = f1(d1x + d2y + d3z). Then
c1 + c2x + c3x2 = d1 + d2x + d3x2 by the deﬁnition of f1.
Members of P2 are equal only when
they have the same coeﬃcients, so this implies that c1 = d1 and c2 = d2 and c3 = d3. Therefore
f1(c1x + c2y + c3z) = f1(d1x + d2y + d3z) implies that c1x + c2y + c3z = d1x + d2y + d3z, and so f1 is
one-to-one.
To verify that it is onto, consider an arbitrary member of the codomain a1 +a2x+a3x2 and observe
that it is indeed the image of a member of the domain, namely, it is f1(a1x+a2y +a3z). (For instance,
0 + 3x + 6x2 = f1(0x + 3y + 6z).)
The computation checking that f1 preserves addition is this.
f1 ( (c1x + c2y + c3z) + (d1x + d2y + d3z) ) = f1 ( (c1 + d1)x + (c2 + d2)y + (c3 + d3)z )
= (c1 + d1) + (c2 + d2)x + (c3 + d3)x2
= (c1 + c2x + c3x2) + (d1 + d2x + d3x2)
= f1(c1x + c2y + c3z) + f1(d1x + d2y + d3z)
The check that f1 preserves scalar multiplication is this.
f1( r · (c1x + c2y + c3z) ) = f1( (rc1)x + (rc2)y + (rc3)z )
= (rc1) + (rc2)x + (rc3)x2
= r · (c1 + c2x + c3x2)
= r · f1(c1x + c2y + c3z)
Three.I.1.13
(a) No; this map is not one-to-one. In particular, the matrix of all zeroes is mapped
to the same image as the matrix of all ones.

Answers to Exercises
289
(b) Yes, this is an isomorphism.
It is one-to-one:
if f(
µa1
b1
c1
d1
¶
) = f(
µa2
b2
c2
d2
¶
) then




a1 + b1 + c1 + d1
a1 + b1 + c1
a1 + b1
a1



=




a2 + b2 + c2 + d2
a2 + b2 + c2
a2 + b2
a2




gives that a1 = a2, and that b1 = b2, and that c1 = c2, and that d1 = d2.
It is onto, since this shows




x
y
z
w



= f(
µ
w
z −w
y −z
x −y
¶
)
that any four-tall vector is the image of a 2×2 matrix.
Finally, it preserves combinations
f( r1 ·
µ
a1
b1
c1
d1
¶
+ r2 ·
µ
a2
b2
c2
d2
¶
) = f(
µ
r1a1 + r2a2
r1b1 + r2b2
r1c1 + r2c2
r1d1 + r2d2
¶
)
=




r1a1 + · · · + r2d2
r1a1 + · · · + r2c2
r1a1 + · · · + r2b2
r1a1 + r2a2




= r1 ·




a1 + · · · + d1
a1 + · · · + c1
a1 + b1
a1



+ r2 ·




a2 + · · · + d2
a2 + · · · + c2
a2 + b2
a2




= r1 · f(
µ
a1
b1
c1
d1
¶
) + r2 · f(
µ
a2
b2
c2
d2
¶
)
and so item (2) of Lemma 1.9 shows that it preserves structure.
(c) Yes, it is an isomorphism.
To show that it is one-to-one, we suppose that two members of the domain have the same image
under f.
f(
µ
a1
b1
c1
d1
¶
) = f(
µ
a2
b2
c2
d2
¶
)
This gives, by the deﬁnition of f, that c1 + (d1 + c1)x + (b1 + a1)x2 + a1x3 = c2 + (d2 + c2)x + (b2 +
a2)x2 + a2x3 and then the fact that polynomials are equal only when their coeﬃcients are equal
gives a set of linear equations
c1 = c2
d1 + c1 = d2 + c2
b1 + a1 = b2 + a2
a1 = a2
that has only the solution a1 = a2, b1 = b2, c1 = c2, and d1 = d2.
To show that f is onto, we note that p + qx + rx2 + sx3 is the image under f of this matrix.
µ
s
r −s
p
q −p
¶
We can check that f preserves structure by using item (2) of Lemma 1.9.
f(r1 ·
µ
a1
b1
c1
d1
¶
+ r2 ·
µ
a2
b2
c2
d2
¶
) = f(
µ
r1a1 + r2a2
r1b1 + r2b2
r1c1 + r2c2
r1d1 + r2d2
¶
)
= (r1c1 + r2c2) + (r1d1 + r2d2 + r1c1 + r2c2)x
+ (r1b1 + r2b2 + r1a1 + r2a2)x2 + (r1a1 + r2a2)x3
= r1 ·
¡
c1 + (d1 + c1)x + (b1 + a1)x2 + a1x3¢
+ r2 ·
¡
c2 + (d2 + c2)x + (b2 + a2)x2 + a2x3¢
= r1 · f(
µ
a1
b1
c1
d1
¶
) + r2 · f(
µ
a2
b2
c2
d2
¶
)

290
Linear Algebra, by Hefferon
(d) No, this map does not preserve structure. For instance, it does not send the zero matrix to the
zero polynomial.
Three.I.1.14
It is one-to-one and onto, a correspondence, because it has an inverse (namely, f −1(x) =
3√x). However, it is not an isomorphism. For instance, f(1) + f(1) ̸= f(1 + 1).
Three.I.1.15
Many maps are possible. Here are two.
¡a
b¢
7→
µ
b
a
¶
and
¡a
b¢
7→
µ
2a
b
¶
The veriﬁcations are straightforward adaptations of the others above.
Three.I.1.16
Here are two.
a0 + a1x + a2x2 7→


a1
a0
a2


and
a0 + a1x + a2x2 7→


a0 + a1
a1
a2


Veriﬁcation is straightforward (for the second, to show that it is onto, note that


s
t
u


is the image of (s −t) + tx + ux2).
Three.I.1.17
The space R2 is not a subspace of R3 because it is not a subset of R3. The two-tall
vectors in R2 are not members of R3.
The natural isomorphism ι: R2 →R3 (called the injection map) is this.
µ
x
y
¶
ι
7−→


x
y
0


This map is one-to-one because
f(
µx1
y1
¶
) = f(
µx2
y2
¶
)
implies


x1
y1
0

=


x2
y2
0


which in turn implies that x1 = x2 and y1 = y2, and therefore the initial two two-tall vectors are equal.
Because


x
y
0

= f(
µx
y
¶
)
this map is onto the xy-plane.
To show that this map preserves structure, we will use item (2) of Lemma 1.9 and show
f(c1 ·
µx1
y1
¶
+ c2 ·
µx2
y2
¶
) = f(
µc1x1 + c2x2
c1y1 + c2y2
¶
) =


c1x1 + c2x2
c1y1 + c2y2
0


= c1 ·


x1
y1
0

+ c2 ·


x2
y2
0

= c1 · f(
µ
x1
y1
¶
) + c2 · f(
µ
x2
y2
¶
)
that it preserves combinations of two vectors.
Three.I.1.18
Here are two:





r1
r2
...
r16




7→


r1
r2
. . .
. . .
r16


and





r1
r2
...
r16




7→





r1
r2
...
...
r16





Veriﬁcation that each is an isomorphism is easy.
Three.I.1.19
When k is the product k = mn, here is an isomorphism.



r1
r2
. . .
...
. . .
rm·n


7→





r1
r2
...
rm·n





Checking that this is an isomorphism is easy.

Answers to Exercises
291
Three.I.1.20
If n ≥1 then Pn−1 ∼= Rn. (If we take P−1 and R0 to be trivial vector spaces, then the
relationship extends one dimension lower.) The natural isomorphism between them is this.
a0 + a1x + · · · + an−1xn−1 7→





a0
a1
...
an−1





Checking that it is an isomorphism is straightforward.
Three.I.1.21
This is the map, expanded.
f(a0 + a1x + a2x2 + a3x3 + a4x4 + a5x5) = a0 + a1(x −1) + a2(x −1)2 + a3(x −1)3
+ a4(x −1)4 + a5(x −1)5
= a0 + a1(x −1) + a2(x2 −2x + 1)
+ a3(x3 −3x2 + 3x −1)
+ a4(x4 −4x3 + 6x2 −4x + 1)
+ a5(x5 −5x4 + 10x3 −10x2 + 5x −1)
= (a0 −a1 + a2 −a3 + a4 −a5)
+ (a1 −2a2 + 3a3 −4a4 + 5a5)x
+ (a2 −3a3 + 6a4 −10a5)x2 + (a3 −4a4 + 10a5)x3
+ (a4 −5a5)x4 + a5x5
This map is a correspondence because it has an inverse, the map p(x) 7→p(x + 1).
To ﬁnish checking that it is an isomorphism, we apply item (2) of Lemma 1.9 and show that it
preserves linear combinations of two polynomials. Brieﬂy, the check goes like this.
f(c · (a0 + a1x + · · · + a5x5) + d · (b0 + b1x + · · · + b5x5))
= · · · = (ca0 −ca1 + ca2 −ca3 + ca4 −ca5 + db0 −db1 + db2 −db3 + db4 −db5) + · · · + (ca5 + db5)x5
= · · · = c · f(a0 + a1x + · · · + a5x5) + d · f(b0 + b1x + · · · + b5x5)
Three.I.1.22
No vector space has the empty set underlying it. We can take ⃗v to be the zero vector.
Three.I.1.23
Yes; where the two spaces are {⃗a} and {⃗b}, the map sending ⃗a to ⃗b is clearly one-to-one
and onto, and also preserves what little structure there is.
Three.I.1.24
A linear combination of n = 0 vectors adds to the zero vector and so Lemma 1.8 shows
that the three statements are equivalent in this case.
Three.I.1.25
Consider the basis ⟨1⟩for P0 and let f(1) ∈R be k. For any a ∈P0 we have that
f(a) = f(a · 1) = af(1) = ak and so f’s action is multiplication by k. Note that k ̸= 0 or else the map
is not one-to-one. (Incidentally, any such map a 7→ka is an isomorphism, as is easy to check.)
Three.I.1.26
In each item, following item (2) of Lemma 1.9, we show that the map preserves structure
by showing that the it preserves linear combinations of two members of the domain.
(a) The identity map is clearly one-to-one and onto. For linear combinations the check is easy.
id(c1 · ⃗v1 + c2 · ⃗v2) = c1⃗v1 + c2⃗v2 = c1 · id(⃗v1) + c2 · id(⃗v2)
(b) The inverse of a correspondence is also a correspondence (as stated in the appendix), so we need
only check that the inverse preserves linear combinations. Assume that ⃗w1 = f(⃗v1) (so f −1(⃗w1) = ⃗v1)
and assume that ⃗w2 = f(⃗v2).
f −1(c1 · ⃗w1 + c2 · ⃗w2) = f −1¡
c1 · f(⃗v1) + c2 · f(⃗v2)
¢
= f −1( f
¡
c1⃗v1 + c2⃗v2)
¢
= c1⃗v1 + c2⃗v2
= c1 · f −1(⃗w1) + c2 · f −1(⃗w2)
(c) The composition of two correspondences is a correspondence (as stated in the appendix), so we
need only check that the composition map preserves linear combinations.
g ◦f
¡
c1 · ⃗v1 + c2 · ⃗v2
¢
= g
¡
f(c1⃗v1 + c2⃗v2)
¢
= g
¡
c1 · f(⃗v1) + c2 · f(⃗v2)
¢
= c1 · g
¡
f(⃗v1)) + c2 · g(f(⃗v2)
¢
= c1 · g ◦f (⃗v1) + c2 · g ◦f (⃗v2)

292
Linear Algebra, by Hefferon
Three.I.1.27
One direction is easy: by deﬁnition, if f is one-to-one then for any ⃗w ∈W at most one
⃗v ∈V has f(⃗v ) = ⃗w, and so in particular, at most one member of V is mapped to ⃗0W . The proof
of Lemma 1.8 does not use the fact that the map is a correspondence and therefore shows that any
structure-preserving map f sends ⃗0V to ⃗0W .
For the other direction, assume that the only member of V that is mapped to ⃗0W is ⃗0V . To show
that f is one-to-one assume that f(⃗v1) = f(⃗v2). Then f(⃗v1) −f(⃗v2) = ⃗0W and so f(⃗v1 −⃗v2) = ⃗0W .
Consequently ⃗v1 −⃗v2 = ⃗0V , so ⃗v1 = ⃗v2, and so f is one-to-one.
Three.I.1.28
We will prove something stronger — not only is the existence of a dependence preserved
by isomorphism, but each instance of a dependence is preserved, that is,
⃗vi = c1⃗v1 + · · · + ci−1⃗vi−1 + ci+1⃗vi+1 + · · · + ck⃗vk
⇐⇒f(⃗vi) = c1f(⃗v1) + · · · + ci−1f(⃗vi−1) + ci+1f(⃗vi+1) + · · · + ckf(⃗vk).
The =⇒direction of this statement holds by item (3) of Lemma 1.9. The ⇐= direction holds by
regrouping
f(⃗vi) = c1f(⃗v1) + · · · + ci−1f(⃗vi−1) + ci+1f(⃗vi+1) + · · · + ckf(⃗vk)
= f(c1⃗v1 + · · · + ci−1⃗vi−1 + ci+1⃗vi+1 + · · · + ck⃗vk)
and applying the fact that f is one-to-one, and so for the two vectors ⃗vi and c1⃗v1 + · · · + ci−1⃗vi−1 +
ci+1f⃗vi+1 + · · · + ckf(⃗vk to be mapped to the same image by f, they must be equal.
Three.I.1.29
(a) This map is one-to-one because if ds(⃗v1) = ds(⃗v2) then by deﬁnition of the map,
s · ⃗v1 = s · ⃗v2 and so ⃗v1 = ⃗v2, as s is nonzero. This map is onto as any ⃗w ∈R2 is the image of
⃗v = (1/s) · ⃗w (again, note that s is nonzero). (Another way to see that this map is a correspondence
is to observe that it has an inverse: the inverse of ds is d1/s.)
To ﬁnish, note that this map preserves linear combinations
ds(c1 · ⃗v1 + c2 · ⃗v2) = s(c1⃗v1 + c2⃗v2) = c1s⃗v1 + c2s⃗v2 = c1 · ds(⃗v1) + c2 · ds(⃗v2)
and therefore is an isomorphism.
(b) As in the prior item, we can show that the map tθ is a correspondence by noting that it has an
inverse, t−θ.
That the map preserves structure is geometrically easy to see. For instance, adding two vectors
and then rotating them has the same eﬀect as rotating ﬁrst and then adding. For an algebraic
argument, consider polar coordinates: the map tθ sends the vector with endpoint (r, φ) to the vector
with endpoint (r, φ+θ). Then the familiar trigonometric formulas cos(φ+θ) = cos φ cos θ−sin φ sin θ
and sin(φ+θ) = sin φ cos θ+cos φ sin θ show how to express the map’s action in the usual rectangular
coordinate system. µx
y
¶
=
µr cos φ
r sin φ
¶
tθ
7−→
µr cos(φ + θ)
r sin(φ + θ)
¶
=
µx cos θ −y sin θ
x sin θ + y cos θ
¶
Now the calculation for preservation of addition is routine.
µx1 + x2
y1 + y2
¶
tθ
7−→
µ(x1 + x2) cos θ −(y1 + y2) sin θ
(x1 + x2) sin θ + (y1 + y2) cos θ
¶
=
µx1 cos θ −y1 sin θ
x1 sin θ + y1 cos θ
¶
+
µx2 cos θ −y2 sin θ
x2 sin θ + y2 cos θ
¶
The calculation for preservation of scalar multiplication is similar.
(c) This map is a correspondence because it has an inverse (namely, itself).
As in the last item, that the reﬂection map preserves structure is geometrically easy to see: adding
vectors and then reﬂecting gives the same result as reﬂecting ﬁrst and then adding, for instance.
For an algebraic proof, suppose that the line ℓhas slope k (the case of a line with undeﬁned slope
can be done as a separate, but easy, case). We can follow the hint and use polar coordinates: where
the line ℓforms an angle of φ with the x-axis, the action of fℓis to send the vector with endpoint
(r cos θ, r sin θ) to the one with endpoint (r cos(2φ −θ), r sin(2φ −θ)).
fℓ
7−→
θ
φ
φ −(θ −φ)
To convert to rectangular coordinates, we will use some trigonometric formulas, as we did in the
prior item. First observe that cos φ and sin φ can be determined from the slope k of the line. This
picture
x
kx
x
√
1 + k2
θ

Answers to Exercises
293
gives that cos φ = 1/
√
1 + k2 and sin φ = k/
√
1 + k2. Now,
cos(2φ −θ) = cos(2φ) cos θ + sin(2φ) sin θ
=
¡
cos2 φ −sin2 φ
¢
cos θ + (2 sin φ cos φ) sin θ
=
µ
(
1
√
1 + k2 )2 −(
k
√
1 + k2 )2
¶
cos θ +
µ
2
k
√
1 + k2
1
√
1 + k2
¶
sin θ
=
µ1 −k2
1 + k2
¶
cos θ +
µ
2k
1 + k2
¶
sin θ
and thus the ﬁrst component of the image vector is this.
r · cos(2φ −θ) = 1 −k2
1 + k2 · x +
2k
1 + k2 · y
A similar calculation shows that the second component of the image vector is this.
r · sin(2φ −θ) =
2k
1 + k2 · x −1 −k2
1 + k2 · y
With this algebraic description of the action of fℓ
µ
x
y
¶
fℓ
7−→
µ
(1 −k2/1 + k2) · x + (2k/1 + k2) · y
(2k/1 + k2) · x −(1 −k2/1 + k2) · y
¶
checking that it preserves structure is routine.
Three.I.1.30
First, the map p(x) 7→p(x + k) doesn’t count because it is a version of p(x) 7→p(x −k).
Here is a correct answer (many others are also correct): a0+a1x+a2x2 7→a2+a0x+a1x2. Veriﬁcation
that this is an isomorphism is straightforward.
Three.I.1.31
(a) For the ‘only if’ half, let f : R1 →R1 to be an isomorphism. Consider the basis
⟨1⟩⊆R1. Designate f(1) by k. Then for any x we have that f(x) = f(x · 1) = x · f(1) = xk, and
so f’s action is multiplication by k. To ﬁnish this half, just note that k ̸= 0 or else f would not be
one-to-one.
For the ‘if’ half we only have to check that such a map is an isomorphism when k ̸= 0. To check
that it is one-to-one, assume that f(x1) = f(x2) so that kx1 = kx2 and divide by the nonzero factor
k to conclude that x1 = x2. To check that it is onto, note that any y ∈R1 is the image of x = y/k
(again, k ̸= 0). Finally, to check that such a map preserves combinations of two members of the
domain, we have this.
f(c1x1 + c2x2) = k(c1x1 + c2x2) = c1kx1 + c2kx2 = c1f(x1) + c2f(x2)
(b) By the prior item, f’s action is x 7→(7/3)x. Thus f(−2) = −14/3.
(c) For the ‘only if’ half, assume that f : R2 →R2 is an automorphism. Consider the standard basis
E2 for R2. Let
f(⃗e1) =
µ
a
c
¶
and
f(⃗e2) =
µ
b
d
¶
.
Then the action of f on any vector is determined by by its action on the two basis vectors.
f(
µx
y
¶
) = f(x · ⃗e1 + y · ⃗e2) = x · f(⃗e1) + y · f(⃗e2) = x ·
µa
c
¶
+ y ·
µb
d
¶
=
µax + by
cx + dy
¶
To ﬁnish this half, note that if ad −bc = 0, that is, if f(⃗e2) is a multiple of f(⃗e1), then f is not
one-to-one.
For ‘if’ we must check that the map is an isomorphism, under the condition that ad −bc ̸= 0.
The structure-preservation check is easy; we will here show that f is a correspondence. For the
argument that the map is one-to-one, assume this.
f(
µ
x1
y1
¶
) = f(
µ
x2
y2
¶
)
and so
µ
ax1 + by1
cx1 + dy1
¶
=
µ
ax2 + by2
cx2 + dy2
¶
Then, because ad −bc ̸= 0, the resulting system
a(x1 −x2) + b(y1 −y2) = 0
c(x1 −x2) + d(y1 −y2) = 0
has a unique solution, namely the trivial one x1 −x2 = 0 and y1 −y2 = 0 (this follows from the
hint).
The argument that this map is onto is closely related — this system
ax1 + by1 = x
cx1 + dy1 = y

294
Linear Algebra, by Hefferon
has a solution for any x and y if and only if this set
{
µ
a
c
¶
,
µ
b
d
¶
}
spans R2, i.e., if and only if this set is a basis (because it is a two-element subset of R2), i.e., if and
only if ad −bc ̸= 0.
(d)
f(
µ
0
−1
¶
) = f(
µ
1
3
¶
−
µ
1
4
¶
) = f(
µ
1
3
¶
) −f(
µ
1
4
¶
) =
µ
2
−1
¶
−
µ
0
1
¶
=
µ
2
−2
¶
Three.I.1.32
There are many answers; two are linear independence and subspaces.
To show that if a set {⃗v1, . . . ,⃗vn} is linearly independent then its image {f(⃗v1), . . . , f(⃗vn)} is also
linearly independent, consider a linear relationship among members of the image set.
0 = c1f(⃗v1) + · · · + cnf( ⃗vn) = f(c1⃗v1) + · · · + f(cn ⃗vn) = f(c1⃗v1 + · · · + cn ⃗vn)
Because this map is an isomorphism, it is one-to-one. So f maps only one vector from the domain to
the zero vector in the range, that is, c1⃗v1 +· · ·+cn⃗vn equals the zero vector (in the domain, of course).
But, if {⃗v1, . . . ,⃗vn} is linearly independent then all of the c’s are zero, and so {f(⃗v1), . . . , f(⃗vn)} is
linearly independent also. (Remark. There is a small point about this argument that is worth mention.
In a set, repeats collapse, that is, strictly speaking, this is a one-element set: {⃗v,⃗v}, because the things
listed as in it are the same thing. Observe, however, the use of the subscript n in the above argument.
In moving from the domain set {⃗v1, . . . ,⃗vn} to the image set {f(⃗v1), . . . , f(⃗vn)}, there is no collapsing,
because the image set does not have repeats, because the isomorphism f is one-to-one.)
To show that if f : V →W is an isomorphism and if U is a subspace of the domain V then the set
of image vectors f(U) = {⃗w ∈W
¯¯ ⃗w = f(⃗u) for some ⃗u ∈U} is a subspace of W, we need only show
that it is closed under linear combinations of two of its members (it is nonempty because it contains
the image of the zero vector). We have
c1 · f(⃗u1) + c2 · f(⃗u2) = f(c1⃗u1) + f(c2⃗u2) = f(c1⃗u1 + c2⃗u2)
and c1⃗u1 + c2⃗u2 is a member of U because of the closure of a subspace under combinations. Hence the
combination of f(⃗u1) and f(⃗u2) is a member of f(U).
Three.I.1.33
(a) The association
⃗p = c1⃗β1 + c2⃗β2 + c3⃗β3
RepB(·)
7−→


c1
c2
c3


is a function if every member ⃗p of the domain is associated with at least one member of the codomain,
and if every member ⃗p of the domain is associated with at most one member of the codomain. The
ﬁrst condition holds because the basis B spans the domain — every ⃗p can be written as at least one
linear combination of ⃗β’s. The second condition holds because the basis B is linearly independent —
every member ⃗p of the domain can be written as at most one linear combination of the ⃗β’s.
(b) For the one-to-one argument, if RepB(⃗p) = RepB(⃗q), that is, if RepB(p1⃗β1 + p2⃗β2 + p3⃗β3) =
RepB(q1⃗β1 + q2⃗β2 + q3⃗β3) then


p1
p2
p3

=


q1
q2
q3


and so p1 = q1 and p2 = q2 and p3 = q3, which gives the conclusion that ⃗p = ⃗q. Therefore this map
is one-to-one.
For onto, we can just note that


a
b
c


equals RepB(a⃗β1 +b⃗β2 +c⃗β3), and so any member of the codomain R3 is the image of some member
of the domain P2.
(c) This map respects addition and scalar multiplication because it respects combinations of two
members of the domain (that is, we are using item (2) of Lemma 1.9): where ⃗p = p1⃗β1 +p2⃗β2 +p3⃗β3

Answers to Exercises
295
and ⃗q = q1⃗β1 + q2⃗β2 + q3⃗β3, we have this.
RepB(c · ⃗p + d · ⃗q) = RepB( (cp1 + dq1)⃗β1 + (cp2 + dq2)⃗β2 + (cp3 + dq3)⃗β3 )
=


cp1 + dq1
cp2 + dq2
cp3 + dq3


= c ·


p1
p2
p3

+ d ·


q1
q2
q3


= RepB(⃗p) + RepB(⃗q)
(d) Use any basis B for P2 whose ﬁrst two members are x + x2 and 1 −x, say B = ⟨x + x2, 1 −x, 1⟩.
Three.I.1.34
See the next subsection.
Three.I.1.35
(a) Most of the conditions in the deﬁnition of a vector space are routine. We here
sketch the veriﬁcation of part (1) of that deﬁnition.
For closure of U × W, note that because U and W are closed, we have that ⃗u1 + ⃗u2 ∈U and
⃗w1 + ⃗w2 ∈W and so (⃗u1 + ⃗u2, ⃗w1 + ⃗w2) ∈U × W. Commutativity of addition in U × W follows
from commutativity of addition in U and W.
(⃗u1, ⃗w1) + (⃗u2, ⃗w2) = (⃗u1 + ⃗u2, ⃗w1 + ⃗w2) = (⃗u2 + ⃗u1, ⃗w2 + ⃗w1) = (⃗u2, ⃗w2) + (⃗u1, ⃗w1)
The check for associativity of addition is similar. The zero element is (⃗0U,⃗0W ) ∈U × W and the
additive inverse of (⃗u, ⃗w) is (−⃗u, −⃗w).
The checks for the second part of the deﬁnition of a vector space are also straightforward.
(b) This is a basis
⟨(1,
µ0
0
¶
), (x,
µ0
0
¶
), (x2,
µ0
0
¶
), (1,
µ1
0
¶
), (1,
µ0
1
¶
) ⟩
because there is one and only one way to represent any member of P2 × R2 with respect to this set;
here is an example.
(3 + 2x + x2,
µ5
4
¶
) = 3 · (1,
µ0
0
¶
) + 2 · (x,
µ0
0
¶
) + (x2,
µ0
0
¶
) + 5 · (1,
µ1
0
¶
) + 4 · (1,
µ0
1
¶
)
The dimension of this space is ﬁve.
(c) We have dim(U × W) = dim(U) + dim(W) as this is a basis.
⟨(⃗µ1,⃗0W ), . . . , (⃗µdim(U),⃗0W ), (⃗0U, ⃗ω1), . . . , (⃗0U, ⃗ωdim(W ))⟩
(d) We know that if V = U ⊕W then each ⃗v ∈V can be written as ⃗v = ⃗u + ⃗w in one and only one
way. This is just what we need to prove that the given function an isomorphism.
First, to show that f is one-to-one we can show that if f ((⃗u1, ⃗w1)) = ((⃗u2, ⃗w2)), that is, if
⃗u1 + ⃗w1 = ⃗u2 + ⃗w2 then ⃗u1 = ⃗u2 and ⃗w1 = ⃗w2. But the statement ‘each ⃗v is such a sum in only
one way’ is exactly what is needed to make this conclusion. Similarly, the argument that f is onto
is completed by the statement that ‘each ⃗v is such a sum in at least one way’.
This map also preserves linear combinations
f( c1 · (⃗u1, ⃗w1) + c2 · (⃗u2, ⃗w2) ) = f( (c1⃗u1 + c2⃗u2, c1 ⃗w1 + c2 ⃗w2) )
= c1⃗u1 + c2⃗u2 + c1 ⃗w1 + c2 ⃗w2
= c1⃗u1 + c1 ⃗w1 + c2⃗u2 + c2 ⃗w2
= c1 · f( (⃗u1, ⃗w1) ) + c2 · f( (⃗u2, ⃗w2) )
and so it is an isomorphism.
Subsection Three.I.2: Dimension Characterizes Isomorphism
Three.I.2.8
Each pair of spaces is isomorphic if and only if the two have the same dimension. We can,
when there is an isomorphism, state a map, but it isn’t strictly necessary.
(a) No, they have diﬀerent dimensions.
(b) No, they have diﬀerent dimensions.

296
Linear Algebra, by Hefferon
(c) Yes, they have the same dimension. One isomorphism is this.
µ
a
b
c
d
e
f
¶
7→



a
...
f



(d) Yes, they have the same dimension. This is an isomorphism.
a + bx + · · · + fx5 7→
µa
b
c
d
e
f
¶
(e) Yes, both have dimension 2k.
Three.I.2.9
(a) RepB(3 −2x) =
µ
5
−2
¶
(b)
µ
0
2
¶
(c)
µ
−1
1
¶
Three.I.2.10
They have diﬀerent dimensions.
Three.I.2.11
Yes, both are mn-dimensional.
Three.I.2.12
Yes, any two (nondegenerate) planes are both two-dimensional vector spaces.
Three.I.2.13
There are many answers, one is the set of Pk (taking P−1 to be the trivial vector space).
Three.I.2.14
False (except when n = 0). For instance, if f : V →Rn is an isomorphism then mul-
tiplying by any nonzero scalar, gives another, diﬀerent, isomorphism.
(Between trivial spaces the
isomorphisms are unique; the only map possible is ⃗0V 7→0W .)
Three.I.2.15
No.
A proper subspace has a strictly lower dimension than it’s superspace; if U is
a proper subspace of V then any linearly independent subset of U must have fewer than dim(V )
members or else that set would be a basis for V , and U wouldn’t be proper.
Three.I.2.16
Where B = ⟨⃗β1, . . . , ⃗βn⟩, the inverse is this.



c1
...
cn


7→c1⃗β1 + · · · + cn⃗βn
Three.I.2.17
All three spaces have dimension equal to the rank of the matrix.
Three.I.2.18
We must show that if ⃗a = ⃗b then f(⃗a) = f(⃗b). So suppose that a1⃗β1 + · · · + an⃗βn =
b1⃗β1 + · · · + bn⃗βn. Each vector in a vector space (here, the domain space) has a unique representation
as a linear combination of basis vectors, so we can conclude that a1 = b1, . . . , an = bn. Thus,
f(⃗a) =



a1
...
an


=



b1
...
bn


= f(⃗b)
and so the function is well-deﬁned.
Three.I.2.19
Yes, because a zero-dimensional space is a trivial space.
Three.I.2.20
(a) No, this collection has no spaces of odd dimension.
(b) Yes, because Pk ∼= Rk+1.
(c) No, for instance, M2×3 ∼= M3×2.
Three.I.2.21
One direction is easy: if the two are isomorphic via f then for any basis B ⊆V , the set
D = f(B) is also a basis (this is shown in Lemma 2.3). The check that corresponding vectors have the
same coordinates: f(c1⃗β1 + · · · + cn⃗βn) = c1f(⃗β1) + · · · + cnf(⃗βn) = c1⃗δ1 + · · · + cn⃗δn is routine.
For the other half, assume that there are bases such that corresponding vectors have the same
coordinates with respect to those bases.
Because f is a correspondence, to show that it is an
isomorphism, we need only show that it preserves structure.
Because RepB(⃗v ) = RepD(f(⃗v )),
the map f preserves structure if and only if representations preserve addition: RepB(⃗v1 + ⃗v2) =
RepB(⃗v1) + RepB(⃗v2) and scalar multiplication: RepB(r ·⃗v ) = r · RepB(⃗v ) The addition calculation is
this: (c1+d1)⃗β1+· · ·+(cn+dn)⃗βn = c1⃗β1+· · ·+cn⃗βn+d1⃗β1+· · ·+dn⃗βn, and the scalar multiplication
calculation is similar.
Three.I.2.22
(a) Pulling the deﬁnition back from R4 to P3 gives that a0 + a1x + a2x2 + a3x3 is
orthogonal to b0 + b1x + b2x2 + b3x3 if and only if a0b0 + a1b1 + a2b2 + a3b3 = 0.

Answers to Exercises
297
(b) A natural deﬁnition is this.
D(




a0
a1
a2
a3



) =




a1
2a2
3a3
0




Three.I.2.23
Yes.
Assume that V is a vector space with basis B = ⟨⃗β1, . . . , ⃗βn⟩and that W is another vector space
such that the map f : B →W is a correspondence. Consider the extension ˆf : V →W of f.
ˆf(c1⃗β1 + · · · + cn⃗βn) = c1f(⃗β1) + · · · + cnf(⃗βn).
The map ˆf is an isomorphism.
First, ˆf is well-deﬁned because every member of V has one and only one representation as a linear
combination of elements of B.
Second, ˆf is one-to-one because every member of W has only one representation as a linear combi-
nation of elements of ⟨f(⃗β1), . . . , f(⃗βn)⟩. That map ˆf is onto because every member of W has at least
one representation as a linear combination of members of ⟨f(⃗β1), . . . , f(⃗βn)⟩.
Finally, preservation of structure is routine to check.
For instance, here is the preservation of
addition calculation.
ˆf( (c1⃗β1 + · · · + cn⃗βn) + (d1⃗β1 + · · · + dn⃗βn) ) = ˆf( (c1 + d1)⃗β1 + · · · + (cn + dn)⃗βn )
= (c1 + d1)f(⃗β1) + · · · + (cn + dn)f(⃗βn)
= c1f(⃗β1) + · · · + cnf(⃗βn) + d1f(⃗β1) + · · · + dnf(⃗βn)
= ˆf(c1⃗β1 + · · · + cn⃗βn) + + ˆf(d1⃗β1 + · · · + dn⃗βn).
Preservation of scalar multiplication is similar.
Three.I.2.24
Because V1 ∩V2 = {⃗0V } and f is one-to-one we have that f(V1) ∩f(V2) = {⃗0U}. To
ﬁnish, count the dimensions: dim(U) = dim(V ) = dim(V1) + dim(V2) = dim(f(V1)) + dim(f(V2)), as
required.
Three.I.2.25
Rational numbers have many representations, e.g., 1/2 = 3/6, and the numerators can
vary among representations.
Subsection Three.II.1: Deﬁnition
Three.II.1.17
(a) Yes. The veriﬁcation is straightforward.
h(c1 ·


x1
y1
z1

+ c2 ·


x2
y2
z2

) = h(


c1x1 + c2x2
c1y1 + c2y2
c1z1 + c2z2

)
=
µ
c1x1 + c2x2
c1x1 + c2x2 + c1y1 + c2y2 + c1z1 + c2z2
¶
= c1 ·
µ
x1
x1 + y1 + z1
¶
+ c2 ·
µ
x2
c2 + y2 + z2
¶
= c1 · h(


x1
y1
z1

) + c2 · h(


x2
y2
z2

)
(b) Yes. The veriﬁcation is easy.
h(c1 ·


x1
y1
z1

+ c2 ·


x2
y2
z2

) = h(


c1x1 + c2x2
c1y1 + c2y2
c1z1 + c2z2

)
=
µ
0
0
¶
= c1 · h(


x1
y1
z1

) + c2 · h(


x2
y2
z2

)

298
Linear Algebra, by Hefferon
(c) No. An example of an addition that is not respected is this.
h(


0
0
0

+


0
0
0

) =
µ
1
1
¶
̸= h(


0
0
0

) + h(


0
0
0

)
(d) Yes. The veriﬁcation is straightforward.
h(c1 ·


x1
y1
z1

+ c2 ·


x2
y2
z2

) = h(


c1x1 + c2x2
c1y1 + c2y2
c1z1 + c2z2

)
=
µ
2(c1x1 + c2x2) + (c1y1 + c2y2)
3(c1y1 + c2y2) −4(c1z1 + c2z2)
¶
= c1 ·
µ
2x1 + y1
3y1 −4z1
¶
+ c2 ·
µ
2x2 + y2
3y2 −4z2
¶
= c1 · h(


x1
y1
z1

) + c2 · h(


x2
y2
z2

)
Three.II.1.18
For each, we must either check that linear combinations are preserved, or give an
example of a linear combination that is not.
(a) Yes. The check that it preserves combinations is routine.
h(r1 ·
µ
a1
b1
c1
d1
¶
+ r2 ·
µ
a2
b2
c2
d2
¶
) = h(
µ
r1a1 + r2a2
r1b1 + r2b2
r1c1 + r2c2
r1d1 + r2d2
¶
)
= (r1a1 + r2a2) + (r1d1 + r2d2)
= r1(a1 + d1) + r2(a2 + d2)
= r1 · h(
µ
a1
b1
c1
d1
¶
) + r2 · h(
µ
a2
b2
c2
d2
¶
)
(b) No. For instance, not preserved is multiplication by the scalar 2.
h(2 ·
µ
1
0
0
1
¶
) = h(
µ
2
0
0
2
¶
) = 4
while
2 · h(
µ
1
0
0
1
¶
) = 2 · 1 = 2
(c) Yes. This is the check that it preserves combinations of two members of the domain.
h(r1 ·
µa1
b1
c1
d1
¶
+ r2 ·
µa2
b2
c2
d2
¶
) = h(
µr1a1 + r2a2
r1b1 + r2b2
r1c1 + r2c2
r1d1 + r2d2
¶
)
= 2(r1a1 + r2a2) + 3(r1b1 + r2b2) + (r1c1 + r2c2) −(r1d1 + r2d2)
= r1(2a1 + 3b1 + c1 −d1) + r2(2a2 + 3b2 + c2 −d2)
= r1 · h(
µ
a1
b1
c1
d1
¶
+ r2 · h(
µ
a2
b2
c2
d2
¶
)
(d) No. An example of a combination that is not preserved is this.
h(
µ
1
0
0
0
¶
+
µ
1
0
0
0
¶
) = h(
µ
2
0
0
0
¶
) = 4
while
h(
µ
1
0
0
0
¶
) + h(
µ
1
0
0
0
¶
) = 1 + 1 = 2
Three.II.1.19
The check that each is a homomorphisms is routine. Here is the check for the diﬀeren-
tiation map.
d
dx(r · (a0 + a1x + a2x2 + a3x3) + s · (b0 + b1x + b2x2 + b3x3))
= d
dx((ra0 + sb0) + (ra1 + sb1)x + (ra2 + sb2)x2 + (ra3 + sb3)x3)
= (ra1 + sb1) + 2(ra2 + sb2)x + 3(ra3 + sb3)x2
= r · (a1 + 2a2x + 3a3x2) + s · (b1 + 2b2x + 3b3x2)
= r · d
dx(a0 + a1x + a2x2 + a3x3) + s · d
dx(b0 + b1x + b2x2 + b3x3)
(An alternate proof is to simply note that this is a property of diﬀerentiation that is familar from
calculus.)
These two maps are not inverses as this composition does not act as the identity map on this
element of the domain.
1 ∈P3
d/dx
7−→0 ∈P2
R
7−→0 ∈P3

Answers to Exercises
299
Three.II.1.20
Each of these projections is a homomorphism. Projection to the xz-plane and to the
yz-plane are these maps.


x
y
z

7→


x
0
z




x
y
z

7→


0
y
z


Projection to the x-axis, to the y-axis, and to the z-axis are these maps.


x
y
z

7→


x
0
0




x
y
z

7→


0
y
0




x
y
z

7→


0
0
z


And projection to the origin is this map.


x
y
z

7→


0
0
0


Veriﬁcation that each is a homomorphism is straightforward. (The last one, of course, is the zero
transformation on R3.)
Three.II.1.21
The ﬁrst is not onto; for instance, there is no polynomial that is sent the constant
polynomial p(x) = 1. The second is not one-to-one; both of these members of the domain
µ1
0
0
0
¶
and
µ0
0
0
1
¶
are mapped to the same member of the codomain, 1 ∈R.
Three.II.1.22
Yes; in any space id(c · ⃗v + d · ⃗w) = c · ⃗v + d · ⃗w = c · id(⃗v) + d · id(⃗w).
Three.II.1.23
(a) This map does not preserve structure since f(1 + 1) = 3, while f(1) + f(1) = 2.
(b) The check is routine.
f(r1 ·
µ
x1
y1
¶
+ r2 ·
µ
x2
y2
¶
) = f(
µ
r1x1 + r2x2
r1y1 + r2y2
¶
)
= (r1x1 + r2x2) + 2(r1y1 + r2y2)
= r1 · (x1 + 2y1) + r2 · (x2 + 2y2)
= r1 · f(
µ
x1
y1
¶
) + r2 · f(
µ
x2
y2
¶
)
Three.II.1.24
Yes. Where h: V →W is linear, h(⃗u −⃗v) = h(⃗u + (−1) · ⃗v) = h(⃗u) + (−1) · h(⃗v) =
h(⃗u) −h(⃗v).
Three.II.1.25
(a) Let ⃗v ∈V be represented with respect to the basis as ⃗v = c1⃗β1 +· · ·+cn⃗βn. Then
h(⃗v) = h(c1⃗β1 + · · · + cn⃗βn) = c1h(⃗β1) + · · · + cnh(⃗βn) = c1 ·⃗0 + · · · + cn ·⃗0 = ⃗0.
(b) This argument is similar to the prior one. Let ⃗v ∈V be represented with respect to the basis as
⃗v = c1⃗β1 + · · · + cn⃗βn. Then h(c1⃗β1 + · · · + cn⃗βn) = c1h(⃗β1) + · · · + cnh(⃗βn) = c1⃗β1 + · · · + cn⃗βn = ⃗v.
(c) As above, only c1h(⃗β1) + · · · + cnh(⃗βn) = c1r⃗β1 + · · · + cnr⃗βn = r(c1⃗β1 + · · · + cn⃗βn) = r⃗v.
Three.II.1.26
That it is a homomorphism follows from the familiar rules that the logarithm of a
product is the sum of the logarithms ln(ab) = ln(a) + ln(b) and that the logarithm of a power is the
multiple of the logarithm ln(ar) = r ln(a). This map is an isomorphism because it has an inverse,
namely, the exponential map, so it is a correspondence, and therefore it is an isomorphism.
Three.II.1.27
Where ˆx = x/2 and ˆy = y/3, the image set is
{
µ
ˆx
ˆy
¶ ¯¯ (2ˆx)2
4
+ (3ˆy)2
9
= 1} = {
µ
ˆx
ˆy
¶ ¯¯ ˆx2 + ˆy2 = 1}
the unit circle in the ˆxˆy-plane.
Three.II.1.28
The circumference function r 7→2πr is linear. Thus we have 2π · (rearth + 6) −2π ·
(rearth) = 12π. Observe that it takes the same amount of extra rope to raise the circle from tightly
wound around a basketball to six feet above that basketball as it does to raise it from tightly wound
around the earth to six feet above the earth.

300
Linear Algebra, by Hefferon
Three.II.1.29
Verifying that it is linear is routine.
h(c1 ·


x1
y1
z1

+ c2 ·


x2
y2
z2

) = h(


c1x1 + c2x2
c1y1 + c2y2
c1z1 + c2z2

)
= 3(c1x1 + c2x2) −(c1y1 + c2y2) −(c1z1 + c2z2)
= c1 · (3x1 −y1 −z1) + c2 · (3x2 −y2 −z2)
= c1 · h(


x1
y1
z1

) + c2 · h(


x2
y2
z2

)
The natural guess at a generalization is that for any ﬁxed ⃗k ∈R3 the map ⃗v 7→⃗v ⃗k is linear. This
statement is true. It follows from properties of the dot product we have seen earlier: (⃗v+⃗u) ⃗k = ⃗v ⃗k+⃗u ⃗k
and (r⃗v) ⃗k = r(⃗v ⃗k). (The natural guess at a generalization of this generalization, that the map from
Rn to R whose action consists of taking the dot product of its argument with a ﬁxed vector ⃗k ∈Rn is
linear, is also true.)
Three.II.1.30
Let h: R1 →R1 be linear. A linear map is determined by its action on a basis, so ﬁx the
basis ⟨1⟩for R1. For any r ∈R1 we have that h(r) = h(r · 1) = r · h(1) and so h acts on any argument
r by multiplying it by the constant h(1). If h(1) is not zero then the map is a correspondence — its
inverse is division by h(1) — so any nontrivial transformation of R1 is an isomorphism.
This projection map is an example that shows that not every transformation of Rn acts via multi-
plication by a constant when n > 1, including when n = 2.





x1
x2
...
xn




7→





x1
0
...
0





Three.II.1.31
(a) Where c and d are scalars, we have this.
h(c ·



x1
...
xn


+ d ·



y1
...
yn


) = h(



cx1 + dy1
...
cxn + dyn


)
=



a1,1(cx1 + dy1) + · · · + a1,n(cxn + dyn)
...
am,1(cx1 + dy1) + · · · + am,n(cxn + dyn)



= c ·



a1,1x1 + · · · + a1,nxn
...
am,1x1 + · · · + am,nxn


+ d ·



a1,1y1 + · · · + a1,nyn
...
am,1y1 + · · · + am,nyn



= c · h(



x1
...
xn


) + d · h(



y1
...
yn


)
(b) Each power i of the derivative operator is linear because of these rules familiar from calculus.
di
dxi ( f(x) + g(x) ) = di
dxi f(x) + di
dxi g(x)
and
di
dxi r · f(x) = r · di
dxi f(x)
Thus the given map is a linear transformation of Pn because any linear combination of linear maps
is also a linear map.
Three.II.1.32
(This argument has already appeared, as part of the proof that isomorphism is an equiv-
alence.) Let f : U →V and g: V →W be linear. For any ⃗u1, ⃗u2 ∈U and scalars c1, c2 combinations
are preserved.
g ◦f(c1⃗u1 + c2⃗u2) = g( f(c1⃗u1 + c2⃗u2) ) = g( c1f(⃗u1) + c2f(⃗u2) )
= c1 · g(f(⃗u1)) + c2 · g(f(⃗u2)) = c1 · g ◦f(⃗u1) + c2 · g ◦f(⃗u2)
Three.II.1.33
(a) Yes. The set of ⃗w ’s cannot be linearly independent if the set of ⃗v ’s is linearly
dependent because any nontrivial relationship in the domain ⃗0V = c1⃗v1 + · · · + cn⃗vn would give a

Answers to Exercises
301
nontrivial relationship in the range f(⃗0V ) = ⃗0W = f(c1⃗v1 + · · · + cn⃗vn) = c1f(⃗v1) + · · · + cnf(⃗vn) =
c1 ⃗w + · · · + cn ⃗wn.
(b) Not necessarily. For instance, the transformation of R2 given by
µx
y
¶
7→
µx + y
x + y
¶
sends this linearly independent set in the domain to a linearly dependent image.
{⃗v1,⃗v2} = {
µ
1
0
¶
,
µ
1
1
¶
} 7→{
µ
1
1
¶
,
µ
2
2
¶
} = {⃗w1, ⃗w2}
(c) Not necessarily. An example is the projection map π: R3 →R2


x
y
z

7→
µ
x
y
¶
and this set that does not span the domain but maps to a set that does span the codomain.
{


1
0
0

,


0
1
0

}
π
7−→{
µ
1
0
¶
,
µ
0
1
¶
}
(d) Not necessarily. For instance, the injection map ι: R2 →R3 sends the standard basis E2 for the
domain to a set that does not span the codomain. (Remark. However, the set of ⃗w’s does span the
range. A proof is easy.)
Three.II.1.34
Recall that the entry in row i and column j of the transpose of M is the entry mj,i
from row j and column i of M. Now, the check is routine.
[r ·




...
· · ·
ai,j
· · ·
...



+ s ·




...
· · ·
bi,j
· · ·
...



]
trans
=




...
· · ·
rai,j + sbi,j
· · ·
...




trans
=




...
· · ·
raj,i + sbj,i
· · ·
...




= r ·




...
· · ·
aj,i
· · ·
...



+ s ·




...
· · ·
bj,i
· · ·
...




= r ·




...
· · ·
aj,i
· · ·
...




trans
+ s ·




...
· · ·
bj,i
· · ·
...




trans
The domain is Mm×n while the codomain is Mn×m.
Three.II.1.35
(a) For any homomorphism h: Rn →Rm we have
h(ℓ) = {h(t · ⃗u + (1 −t) · ⃗v)
¯¯ t ∈[0..1]} = {t · h(⃗u) + (1 −t) · h(⃗v)
¯¯ t ∈[0..1]}
which is the line segment from h(⃗u) to h(⃗v).
(b) We must show that if a subset of the domain is convex then its image, as a subset of the range, is
also convex. Suppose that C ⊆Rn is convex and consider its image h(C). To show h(C) is convex
we must show that for any two of its members, ⃗d1 and ⃗d2, the line segment connecting them
ℓ= {t · ⃗d1 + (1 −t) · ⃗d2
¯¯ t ∈[0..1]}
is a subset of h(C).
Fix any member ˆt · ⃗d1 + (1 −ˆt) · ⃗d2 of that line segment. Because the endpoints of ℓare in the
image of C, there are members of C that map to them, say h(⃗c1) = ⃗d1 and h(⃗c2) = ⃗d2. Now, where
ˆt is the scalar that is ﬁxed in the ﬁrst sentence of this paragraph, observe that h(ˆt·⃗c1 +(1−ˆt)·⃗c2) =
ˆt · h(⃗c1) + (1 −ˆt) · h(⃗c2) = ˆt · ⃗d1 + (1 −ˆt) · ⃗d2 Thus, any member of ℓis a member of h(C), and so
h(C) is convex.

302
Linear Algebra, by Hefferon
Three.II.1.36
(a) For ⃗v0,⃗v1 ∈Rn, the line through ⃗v0 with direction ⃗v1 is the set {⃗v0 + t · ⃗v1
¯¯ t ∈R}.
The image under h of that line {h(⃗v0 + t · ⃗v1)
¯¯ t ∈R} = {h(⃗v0) + t · h(⃗v1)
¯¯ t ∈R} is the line through
h(⃗v0) with direction h(⃗v1). If h(⃗v1) is the zero vector then this line is degenerate.
(b) A k-dimensional linear surface in Rn maps to a (possibly degenerate) k-dimensional linear surface
in Rm. The proof is just like that the one for the line.
Three.II.1.37
Suppose that h: V →W is a homomorphism and suppose that S is a subspace of
V .
Consider the map ˆh: S →W deﬁned by ˆh(⃗s) = h(⃗s).
(The only diﬀerence between ˆh and h
is the diﬀerence in domain.) Then this new map is linear: ˆh(c1 · ⃗s1 + c2 · ⃗s2) = h(c1⃗s1 + c2⃗s2) =
c1h(⃗s1) + c2h(⃗s2) = c1 · ˆh(⃗s1) + c2 · ˆh(⃗s2).
Three.II.1.38
This will appear as a lemma in the next subsection.
(a) The range is nonempty because V is nonempty. To ﬁnish we need to show that it is closed under
combinations. A combination of range vectors has the form, where ⃗v1, . . . ,⃗vn ∈V ,
c1 · h(⃗v1) + · · · + cn · h(⃗vn) = h(c1⃗v1) + · · · + h(cn⃗vn) = h(c1 · ⃗v1 + · · · + cn · ⃗vn),
which is itself in the range as c1 · ⃗v1 + · · · + cn · ⃗vn is a member of domain V . Therefore the range is
a subspace.
(b) The nullspace is nonempty since it contains ⃗0V , as ⃗0V maps to ⃗0W . It is closed under linear com-
binations because, where ⃗v1, . . . ,⃗vn ∈V are elements of the inverse image set {⃗v ∈V
¯¯ h(⃗v) = ⃗0W },
for c1, . . . , cn ∈R
⃗0W = c1 · h(⃗v1) + · · · + cn · h(⃗vn) = h(c1 · ⃗v1 + · · · + cn · ⃗vn)
and so c1 · ⃗v1 + · · · + cn · ⃗vn is also in the inverse image of ⃗0W .
(c) This image of U nonempty because U is nonempty.
For closure under combinations, where
⃗u1, . . . , ⃗un ∈U,
c1 · h(⃗u1) + · · · + cn · h(⃗un) = h(c1 · ⃗u1) + · · · + h(cn · ⃗un) = h(c1 · ⃗u1 + · · · + cn · ⃗un)
which is itself in h(U) as c1 · ⃗u1 + · · · + cn · ⃗un is in U. Thus this set is a subspace.
(d) The natural generalization is that the inverse image of a subspace of is a subspace.
Suppose that X is a subspace of W. Note that ⃗0W ∈X so the set {⃗v ∈V
¯¯ h(⃗v) ∈X} is not
empty. To show that this set is closed under combinations, let ⃗v1, . . . ,⃗vn be elements of V such that
h(⃗v1) = ⃗x1, . . . , h(⃗vn) = ⃗xn and note that
h(c1 · ⃗v1 + · · · + cn · ⃗vn) = c1 · h(⃗v1) + · · · + cn · h(⃗vn) = c1 · ⃗x1 + · · · + cn · ⃗xn
so a linear combination of elements of h−1(X) is also in h−1(X).
Three.II.1.39
No; the set of isomorphisms does not contain the zero map (unless the space is trivial).
Three.II.1.40
If ⟨⃗β1, . . . , ⃗βn⟩doesn’t span the space then the map needn’t be unique. For instance,
if we try to deﬁne a map from R2 to itself by specifying only that ⃗e1 is sent to itself, then there is
more than one homomorphism possible; both the identity map and the projection map onto the ﬁrst
component ﬁt this condition.
If we drop the condition that ⟨⃗β1, . . . , ⃗βn⟩is linearly independent then we risk an inconsistent
speciﬁcation (i.e, there could be no such map). An example is if we consider ⟨⃗e2,⃗e1, 2⃗e1⟩, and try
to deﬁne a map from R2 to itself that sends ⃗e2 to itself, and sends both ⃗e1 and 2⃗e1 to ⃗e1.
No
homomorphism can satisfy these three conditions.
Three.II.1.41
(a) Brieﬂy, the check of linearity is this.
F(r1 · ⃗v1 + r2 · ⃗v2) =
µ
f1(r1⃗v1 + r2⃗v2)
f2(r1⃗v1 + r2⃗v2)
¶
= r1
µ
f1(⃗v1)
f2(⃗v1)
¶
+ r2
µ
f1(⃗v2)
f2(⃗v2)
¶
= r1 · F(⃗v1) + r2 · F(⃗v2)
(b) Yes. Let π1 : R2 →R1 and π2 : R2 →R1 be the projections
µx
y
¶
π1
7−→x
and
µx
y
¶
π2
7−→y
onto the two axes.
Now, where f1(⃗v) = π1(F(⃗v)) and f2(⃗v) = π2(F(⃗v)) we have the desired
component functions.
F(⃗v) =
µ
f1(⃗v)
f2(⃗v)
¶
They are linear because they are the composition of linear functions, and the fact that the compois-
tion of linear functions is linear was shown as part of the proof that isomorphism is an equivalence
relation (alternatively, the check that they are linear is straightforward).
(c) In general, a map from a vector space V to an Rn is linear if and only if each of the component
functions is linear. The veriﬁcation is as in the prior item.

Answers to Exercises
303
Subsection Three.II.2: Rangespace and Nullspace
Three.II.2.22
First, to answer whether a polynomial is in the nullspace, we have to consider it as a
member of the domain P3. To answer whether it is in the rangespace, we consider it as a member of
the codomain P4. That is, for p(x) = x4, the question of whether it is in the rangespace is sensible but
the question of whether it is in the nullspace is not because it is not even in the domain.
(a) The polynomial x3 ∈P3 is not in the nullspace because h(x3) = x4 is not the zero polynomial in
P4. The polynomial x3 ∈P4 is in the rangespace because x2 ∈P3 is mapped by h to x3.
(b) The answer to both questions is, “Yes, because h(0) = 0.” The polynomial 0 ∈P3 is in the
nullspace because it is mapped by h to the zero polynomial in P4. The polynomial 0 ∈P4 is in the
rangespace because it is the image, under h, of 0 ∈P3.
(c) The polynomial 7 ∈P3 is not in the nullspace because h(7) = 7x is not the zero polynomial in
P4. The polynomial x3 ∈P4 is not in the rangespace because there is no member of the domain
that when multiplied by x gives the constant polynomial p(x) = 7.
(d) The polynomial 12x −0.5x3 ∈P3 is not in the nullspace because h(12x −0.5x3) = 12x2 −0.5x4.
The polynomial 12x −0.5x3 ∈P4 is in the rangespace because it is the image of 12 −0.5x2.
(e) The polynomial 1 + 3x2 −x3 ∈P3 is not in the nullspace because h(1 + 3x2 −x3) = x + 3x3 −x4.
The polynomial 1 + 3x2 −x3 ∈P4 is not in the rangespace because of the constant term.
Three.II.2.23
(a) The nullspace is
N (h) = {
µ
a
b
¶
∈R2 ¯¯ a + ax + ax2 + 0x3 = 0 + 0x + 0x2 + 0x3} = {
µ
0
b
¶ ¯¯ b ∈R}
while the rangespace is
R(h) = {a + ax + ax2 ∈P3
¯¯ a, b ∈R} = {a · (1 + x + x2)
¯¯ a ∈R}
and so the nullity is one and the rank is one.
(b) The nullspace is this.
N (h) = {
µa
b
c
d
¶ ¯¯ a + d = 0} = {
µ−d
b
c
d
¶ ¯¯ b, c, d ∈R}
The rangespace
R(h){a + d
¯¯ a, b, c, d ∈R}
is all of R (we can get any real number by taking d to be 0 and taking a to be the desired number).
Thus, the nullity is three and the rank is one.
(c) The nullspace is
N (h) = {
µ
a
b
c
d
¶ ¯¯ a + b + c = 0 and d = 0} = {
µ
−b −c
b
c
0
¶ ¯¯ b, c ∈R}
while the rangespace is R(h) = {r + sx2 ¯¯ r, s ∈R}. Thus, the nullity is two and the rank is two.
(d) The nullspace is all of R3 so the nullity is three. The rangespace is the trivial subspace of R4 so
the rank is zero.
Three.II.2.24
For each, use the result that the rank plus the nullity equals the dimension of the
domain.
(a) 0
(b) 3
(c) 3
(d) 0
Three.II.2.25
Because
d
dx (a0 + a1x + · · · + anxn) = a1 + 2a2x + 3a3x2 + · · · + nanxn−1
we have this.
N ( d
dx) = {a0 + · · · + anxn ¯¯ a1 + 2a2x + · · · + nanxn−1 = 0 + 0x + · · · + 0xn−1}
= {a0 + · · · + anxn ¯¯ a1 = 0, and a2 = 0, . . . , an = 0}
= {a0 + 0x + 0x2 + · · · + 0xn ¯¯ a0 ∈R}
In the same way,
N ( dk
dxk ) = {a0 + a1x + · · · + anxn ¯¯ a0, . . . , ak−1 ∈R}
for k ≤n.

304
Linear Algebra, by Hefferon
Three.II.2.26
The shadow of a scalar multiple is the scalar multiple of the shadow.
Three.II.2.27
(a) Setting a0 + (a0 + a1)x + (a2 + a3)x3 = 0 + 0x + 0x2 + 0x3 gives a0 = 0 and
a0 + a1 = 0 and a2 + a3 = 0, so the nullspace is {−a3x2 + a3x3 ¯¯ a3 ∈R}.
(b) Setting a0 + (a0 + a1)x + (a2 + a3)x3 = 2 + 0x + 0x2 −x3 gives that a0 = 2, and a1 = −2,
and a2 + a3 = −1. Taking a3 as a parameter, and renaming it a3 = a gives this set description
{2 −2x + (−1 −a)x2 + ax3 ¯¯ a ∈R} = {(2 −2x −x2) + a · (−x2 + x3)
¯¯ a ∈R}.
(c) This set is empty because the range of h includes only those polynomials with a 0x2 term.
Three.II.2.28
All inverse images are lines with slope −2.
2x + y = 1
2x + y = −3
2x + y = 0
Three.II.2.29
These are the inverses.
(a) a0 + a1x + a2x2 + a3x3 7→a0 + a1x + (a2/2)x2 + (a3/3)x3
(b) a0 + a1x + a2x2 + a3x3 7→a0 + a2x + a1x2 + a3x3
(c) a0 + a1x + a2x2 + a3x3 7→a3 + a0x + a1x2 + a2x3
(d) a0 + a1x + a2x2 + a3x3 7→a0 + (a1 −a0)x + (a2 −a1)x2 + (a3 −a2)x3
For instance, for the second one, the map given in the question sends 0 + 1x + 2x2 + 3x3 7→0 + 2x +
1x2 + 3x3 and then the inverse above sends 0 + 2x + 1x2 + 3x3 7→0 + 1x + 2x2 + 3x3. So this map is
actually self-inverse.
Three.II.2.30
For any vector space V , the nullspace
{⃗v ∈V
¯¯ 2⃗v = ⃗0}
is trivial, while the rangespace
{⃗w ∈V
¯¯ ⃗w = 2⃗v for some ⃗v ∈V }
is all of V , because every vector ⃗w is twice some other vector, speciﬁcally, it is twice (1/2)⃗w. (Thus,
this transformation is actually an automorphism.)
Three.II.2.31
Because the rank plus the nullity equals the dimension of the domain (here, ﬁve), and
the rank is at most three, the possible pairs are: (3, 2), (2, 3), (1, 4), and (0, 5). Coming up with linear
maps that show that each pair is indeed possible is easy.
Three.II.2.32
No (unless Pn is trivial), because the two polynomials f0(x) = 0 and f1(x) = 1 have
the same derivative; a map must be one-to-one to have an inverse.
Three.II.2.33
The nullspace is this.
{a0 + a1x + · · · + anxn ¯¯ a0(1) + a12 (12) + · · · +
an
n + 1(1n+1) = 0}
= {a0 + a1x + · · · + anxn ¯¯ a0 + (a1/2) + · · · + (an+1/n + 1) = 0}
Thus the nullity is n.
Three.II.2.34
(a) One direction is obvious: if the homomorphism is onto then its range is the
codomain and so its rank equals the dimension of its codomain. For the other direction assume
that the map’s rank equals the dimension of the codomain. Then the map’s range is a subspace of
the codomain, and has dimension equal to the dimension of the codomain. Therefore, the map’s
range must equal the codomain, and the map is onto. (The ‘therefore’ is because there is a linearly
independent subset of the range that is of size equal to the dimension of the codomain, but any such
linearly independent subset of the codomain must be a basis for the codomain, and so the range
equals the codomain.)
(b) By Theorem 2.21, a homomorphism is one-to-one if and only if its nullity is zero. Because rank
plus nullity equals the dimension of the domain, it follows that a homomorphism is one-to-one if
and only if its rank equals the dimension of its domain. But this domain and codomain have the
same dimension, so the map is one-to-one if and only if it is onto.
Three.II.2.35
We are proving that h: V →W is nonsingular if and only if for every linearly indepen-
dent subset S of V the subset h(S) = {h(⃗s)
¯¯ ⃗s ∈S} of W is linearly independent.

Answers to Exercises
305
One half is easy — by Theorem 2.21, if h is singular then its nullspace is nontrivial (contains more
than just the zero vector). So, where ⃗v ̸= ⃗0V is in that nullspace, the singleton set {⃗v } is independent
while its image {h(⃗v)} = {⃗0W } is not.
For the other half, assume that h is nonsingular and so by Theorem 2.21 has a trivial nullspace.
Then for any ⃗v1, . . . ,⃗vn ∈V , the relation
⃗0W = c1 · h(⃗v1) + · · · + cn · h(⃗vn) = h(c1 · ⃗v1 + · · · + cn · ⃗vn)
implies the relation c1 · ⃗v1 + · · · + cn · ⃗vn = ⃗0V . Hence, if a subset of V is independent then so is its
image in W.
Remark. The statement is that a linear map is nonsingular if and only if it preserves independence
for all sets (that is, if a set is independent then its image is also independent). A singular map may
well preserve some independent sets. An example is this singular map from R3 to R2.


x
y
z

7→
µ
x + y + z
0
¶
Linear independence is preserved for this set
{


1
0
0

} 7→{
µ
1
0
¶
}
and (in a somewhat more tricky example) also for this set
{


1
0
0

,


0
1
0

} 7→{
µ
1
0
¶
}
(recall that in a set, repeated elements do not appear twice). However, there are sets whose indepen-
dence is not preserved under this map;
{


1
0
0

,


0
2
0

} 7→{
µ
1
0
¶
,
µ
2
0
¶
}
and so not all sets have independence preserved.
Three.II.2.36
(We use the notation from Theorem 1.9.) Fix a basis ⟨⃗β1, . . . , ⃗βn⟩for V and a basis
⟨⃗w1, . . . , ⃗wk⟩for W. If the dimension k of W is less than or equal to the dimension n of V then the
theorem gives a linear map from V to W determined in this way.
⃗β1 7→⃗w1, . . . , ⃗βk 7→⃗wk
and
⃗βk+1 7→⃗wk, . . . , ⃗βn 7→⃗wk
We need only to verify that this map is onto.
Any member of W can be written as a linear combination of basis elements c1 · ⃗w1 + · · · + ck · ⃗wk.
This vector is the image, under the map described above, of c1 · ⃗β1 + · · · + ck · ⃗βk + 0 · ⃗βk+1 · · · + 0 · ⃗βn.
Thus the map is onto.
Three.II.2.37
By assumption, h is not the zero map and so a vector ⃗v ∈V exists that is not in the
nullspace. Note that ⟨h(⃗v)⟩is a basis for R, because it is a size one linearly independent subset of R.
Consequently h is onto, as for any r ∈R we have r = c · h(⃗v) for some scalar c, and so r = h(c⃗v).
Thus the rank of h is one. Because the nullity is given as n, the dimension of the domain of h (the
vector space V ) is n + 1. We can ﬁnish by showing {⃗v, ⃗β1, . . . , ⃗βn} is linearly independent, as it is a
size n + 1 subset of a dimension n + 1 space. Because {⃗β1, . . . , ⃗βn} is linearly independent we need
only show that ⃗v is not a linear combination of the other vectors. But c1⃗β1 + · · · + cn⃗βn = ⃗v would
give −⃗v + c1⃗β1 + · · · + cn⃗βn = ⃗0 and applying h to both sides would give a contradiction.
Three.II.2.38
Yes. For the transformation of R2 given by
µx
y
¶
h
7−→
µ0
x
¶
we have this.
N (h) = {
µ
0
y
¶ ¯¯ y ∈R} = R(h)
Remark. We will see more of this in the ﬁfth chapter.

306
Linear Algebra, by Hefferon
Three.II.2.39
This is a simple calculation.
h([S]) = {h(c1⃗s1 + · · · + cn⃗sn)
¯¯ c1, . . . , cn ∈R and ⃗s1, . . . ,⃗sn ∈S}
= {c1h(⃗s1) + · · · + cnh(⃗sn)
¯¯ c1, . . . , cn ∈R and ⃗s1, . . . ,⃗sn ∈S}
= [h(S)]
Three.II.2.40
(a) We will show that the two sets are equal h−1(⃗w) = {⃗v + ⃗n
¯¯ ⃗n ∈N (h)} by mutual
inclusion.
For the {⃗v + ⃗n
¯¯ ⃗n ∈N (h)} ⊆h−1(⃗w) direction, just note that h(⃗v + ⃗n) = h(⃗v) +
h(⃗n) equals ⃗w, and so any member of the ﬁrst set is a member of the second. For the h−1(⃗w) ⊆
{⃗v + ⃗n
¯¯ ⃗n ∈N (h)} direction, consider ⃗u ∈h−1(⃗w). Because h is linear, h(⃗u) = h(⃗v) implies that
h(⃗u −⃗v) = ⃗0. We can write ⃗u −⃗v as ⃗n, and then we have that ⃗u ∈{⃗v + ⃗n
¯¯ ⃗n ∈N (h)}, as desired,
because ⃗u = ⃗v + (⃗u −⃗v).
(b) This check is routine.
(c) This is immediate.
(d) For the linearity check, brieﬂy, where c, d are scalars and ⃗x, ⃗y ∈Rn have components x1, . . . , xn
and y1, . . . , yn, we have this.
h(c · ⃗x + d · ⃗y) =



a1,1(cx1 + dy1) + · · · + a1,n(cxn + dyn)
...
am,1(cx1 + dy1) + · · · + am,n(cxn + dyn)



=



a1,1cx1 + · · · + a1,ncxn
...
am,1cx1 + · · · + am,ncxn


+



a1,1dy1 + · · · + a1,ndyn
...
am,1dy1 + · · · + am,ndyn



= c · h(⃗x) + d · h(⃗y)
The appropriate conclusion is that General = Particular + Homogeneous.
(e) Each power of the derivative is linear because of the rules
dk
dxk (f(x) + g(x)) = dk
dxk f(x) + dk
dxk g(x)
and
dk
dxk rf(x) = r dk
dxk f(x)
from calculus.
Thus the given map is a linear transformation of the space because any linear
combination of linear maps is also a linear map by Lemma 1.16. The appropriate conclusion is
General = Particular + Homogeneous, where the associated homogeneous diﬀerential equation has
a constant of 0.
Three.II.2.41
Because the rank of t is one, the rangespace of t is a one-dimensional set. Taking ⟨h(⃗v)⟩
as a basis (for some appropriate ⃗v), we have that for every ⃗w ∈V , the image h(⃗w) ∈V is a multiple
of this basis vector — associated with each ⃗w there is a scalar c⃗w such that t(⃗w) = c⃗wt(⃗v). Apply t to
both sides of that equation and take r to be ct(⃗v)
t ◦t(⃗w) = t(c⃗w · t(⃗v)) = c⃗w · t ◦t(⃗v) = c⃗w · ct(⃗v) · t(⃗v) = c⃗w · r · t(⃗v) = r · c⃗w · t(⃗v) = r · t(⃗w)
to get the desired conclusion.
Three.II.2.42
Fix a basis ⟨⃗β1, . . . , ⃗βn⟩for V . We shall prove that this map
h
Φ
7−→



h(⃗β1)
...
h(⃗βn)



is an isomorphism from V ∗to Rn.
To see that Φ is one-to-one, assume that h1 and h2 are members of V ∗such that Φ(h1) = Φ(h2).
Then



h1(⃗β1)
...
h1(⃗βn)


=



h2(⃗β1)
...
h2(⃗βn)



and consequently, h1(⃗β1) = h2(⃗β1), etc. But a homomorphism is determined by its action on a basis,
so h1 = h2, and therefore Φ is one-to-one.
To see that Φ is onto, consider



x1
...
xn




Answers to Exercises
307
for x1, . . . , xn ∈R. This function h from V to R
c1⃗β1 + · · · + cn⃗βn
h
7−→c1x1 + · · · + cnxn
is easily seen to be linear, and to be mapped by Φ to the given vector in Rn, so Φ is onto.
The map Φ also preserves structure: where
c1⃗β1 + · · · + cn⃗βn
h1
7−→c1h1(⃗β1) + · · · + cnh1(⃗βn)
c1⃗β1 + · · · + cn⃗βn
h2
7−→c1h2(⃗β1) + · · · + cnh2(⃗βn)
we have
(r1h1 + r2h2)(c1⃗β1 + · · · + cn⃗βn) = c1(r1h1(⃗β1) + r2h2(⃗β1)) + · · · + cn(r1h1(⃗βn) + r2h2(⃗βn))
= r1(c1h1(⃗β1) + · · · + cnh1(⃗βn)) + r2(c1h2(⃗β1) + · · · + cnh2(⃗βn))
so Φ(r1h1 + r2h2) = r1Φ(h1) + r2Φ(h2).
Three.II.2.43
Let h: V →W be linear and ﬁx a basis ⟨⃗β1, . . . , ⃗βn⟩for V . Consider these n maps from
V to W
h1(⃗v) = c1 · h(⃗β1),
h2(⃗v) = c2 · h(⃗β2),
. . .
, hn(⃗v) = cn · h(⃗βn)
for any ⃗v = c1⃗β1 + · · · + cn⃗βn. Clearly h is the sum of the hi’s. We need only check that each hi is
linear: where ⃗u = d1⃗β1 + · · · + dn⃗βn we have hi(r⃗v + s⃗u) = rci + sdi = rhi(⃗v) + shi(⃗u).
Three.II.2.44
Either yes (trivially) or no (nearly trivially).
If V ‘is homomorphic to’ W is taken to mean there is a homomorphism from V into (but not
necessarily onto) W, then every space is homomorphic to every other space as a zero map always
exists.
If V ‘is homomorphic to’ W is taken to mean there is an onto homomorphism from V to W then the
relation is not an equivalence. For instance, there is an onto homomorphism from R3 to R2 (projection
is one) but no homomorphism from R2 onto R3 by Corollary 2.17, so the relation is not reﬂexive.∗
Three.II.2.45
That they form the chains is obvious. For the rest, we show here that R(tj+1) = R(tj)
implies that R(tj+2) = R(tj+1). Induction then applies.
Assume that R(tj+1) = R(tj).
Then t: R(tj+1) →R(tj+2) is the same map, with the same
domain, as t: R(tj) →R(tj+1). Thus it has the same range: R(tj+2) = R(tj+1).
Subsection Three.III.1: Representing Linear Maps with Matrices
Three.III.1.11
(a)


1 · 2 + 3 · 1 + 1 · 0
0 · 2 + (−1) · 1 + 2 · 0
1 · 2 + 1 · 1 + 0 · 0

=


5
−1
3


(b) Not deﬁned.
(c)


0
0
0


Three.III.1.12
(a)
µ
2 · 4 + 1 · 2
3 · 4 −(1/2) · 2
¶
=
µ
10
11
¶
(b)
µ
4
1
¶
(c) Not deﬁned.
Three.III.1.13
Matrix-vector multiplication gives rise to a linear system.
2x + y + z = 8
y + 3z = 4
x −y + 2z = 4
Gaussian reduction shows that z = 1, y = 1, and x = 3.
Three.III.1.14
Here are two ways to get the answer.
First, obviously 1 −3x + 2x2 = 1 · 1 −3 · x + 2 · x2, and so we can apply the general property of
preservation of combinations to get h(1−3x+2x2) = h(1·1−3·x+2·x2) = 1·h(1)−3·h(x)+2·h(x2) =
1 · (1 + x) −3 · (1 + 2x) + 2 · (x −x3) = −2 −3x −2x3.
The other way uses the computation scheme developed in this subsection. Because we know where
these elements of the space go, we consider this basis B = ⟨1, x, x2⟩for the domain. Arbitrarily, we
can take D = ⟨1, x, x2, x3⟩as a basis for the codomain. With those choices, we have that
RepB,D(h) =




1
1
0
1
2
1
0
0
0
0
0
−1




B,D
∗More information on equivalence relations is in the appendix.

308
Linear Algebra, by Hefferon
and, as
RepB(1 −3x + 2x2) =


1
−3
2


B
the matrix-vector multiplication calculation gives this.
RepD(h(1 −3x + 2x2)) =




1
1
0
1
2
1
0
0
0
0
0
−1




B,D


1
−3
2


B
=




−2
−3
0
−2




D
Thus, h(1 −3x + 2x2) = −2 · 1 −3 · x + 0 · x2 −2 · x3 = −2 −3x −2x3, as above.
Three.III.1.15
Again, as recalled in the subsection, with respect to Ei, a column vector represents
itself.
(a) To represent h with respect to E2, E3 we take the images of the basis vectors from the domain,
and represent them with respect to the basis for the codomain.
RepE3( h(⃗e1) ) = RepE3(


2
2
0

) =


2
2
0


RepE3( h(⃗e2) ) = RepE3(


0
1
−1

) =


0
1
−1


These are adjoined to make the matrix.
RepE2,E3(h) =


2
0
2
1
0
−1


(b) For any ⃗v in the domain R2,
RepE2(⃗v) = RepE2(
µ
v1
v2
¶
) =
µ
v1
v2
¶
and so
RepE3( h(⃗v) ) =


2
0
2
1
0
−1


µ
v1
v2
¶
=


2v1
2v1 + v2
−v2


is the desired representation.
Three.III.1.16
(a) We must ﬁrst ﬁnd the image of each vector from the domain’s basis, and then
represent that image with respect to the codomain’s basis.
RepB(d 1
dx ) =




0
0
0
0




RepB(d x
dx ) =




1
0
0
0




RepB(d x2
dx ) =




0
2
0
0




RepB(d x3
dx ) =




0
0
3
0




Those representations are then adjoined to make the matrix representing the map.
RepB,B( d
dx) =




0
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0




(b) Proceeding as in the prior item, we represent the images of the domain’s basis vectors
RepB(d 1
dx ) =




0
0
0
0




RepB(d x
dx ) =




1
0
0
0




RepB(d x2
dx ) =




0
1
0
0




RepB(d x3
dx ) =




0
0
1
0




and adjoin to make the matrix.
RepB,D( d
dx) =




0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0




Three.III.1.17
For each, we must ﬁnd the image of each of the domain’s basis vectors, represent
each image with respect to the codomain’s basis, and then adjoin those representations to get the
matrix.

Answers to Exercises
309
(a) The basis vectors from the domain have these images
1 7→0
x 7→1
x2 7→2x
. . .
and these images are represented with respect to the codomain’s basis in this way.
RepB(0) =









0
0
0
...









RepB(1) =









1
0
0
...









RepB(2x) =









0
2
0
...









. . .
RepB(nxn−1) =









0
0
0
...
n
0









The matrix
RepB,B( d
dx) =







0
1
0
. . .
0
0
0
2
. . .
0
...
0
0
0
. . .
n
0
0
0
. . .
0







has n + 1 rows and columns.
(b) Once the images under this map of the domain’s basis vectors are determined
1 7→x
x 7→x2/2
x2 7→x3/3
. . .
then they can be represented with respect to the codomain’s basis
RepBn+1(x) =







0
1
0
...







RepBn+1(x2/2) =







0
0
1/2
...







. . .
RepBn+1(xn+1/(n + 1)) =







0
0
0
...
1/(n + 1)







and put together to make the matrix.
RepBn,Bn+1(
Z
) =







0
0
. . .
0
0
1
0
. . .
0
0
0
1/2
. . .
0
0
...
0
0
. . .
0
1/(n + 1)







(c) The images of the basis vectors of the domain are
1 7→1
x 7→1/2
x2 7→1/3
. . .
and they are represented with respect to the codomain’s basis as
RepE1(1) = 1
RepE1(1/2) = 1/2
. . .
so the matrix is
RepB,E1(
Z
) =
¡
1
1/2
· · ·
1/n
1/(n + 1)
¢
(this is an 1×(n + 1) matrix).
(d) Here, the images of the domain’s basis vectors are
1 7→1
x 7→3
x2 7→9
. . .
and they are represented in the codomain as
RepE1(1) = 1
RepE1(3) = 3
RepE1(9) = 9
. . .
and so the matrix is this.
RepB,E1(
Z 1
0
) =
¡
1
3
9
· · ·
3n¢
(e) The images of the basis vectors from the domain are
1 7→1
x 7→x + 1 = 1 + x
x2 7→(x + 1)2 = 1 + 2x + x2
x3 7→(x + 1)3 = 1 + 3x + 3x2 + x3
. . .
which are represented as
RepB(1) =









1
0
0
0
...
0









RepB(1 + x) =









1
1
0
0
...
0









RepB(1 + 2x + x2) =









1
2
1
0
...
0









. . .

310
Linear Algebra, by Hefferon
The resulting matrix
RepB,B(
Z
) =








1
1
1
1
. . .
1
0
1
2
3
. . .
¡n
2
¢
0
0
1
3
. . .
¡n
3
¢
...
0
0
0
. . .
1








is Pascal’s triangle (recall that
¡n
r
¢
is the number of ways to choose r things, without order and
without repetition, from a set of size n).
Three.III.1.18
Where the space is n-dimensional,
RepB,B(id) =





1
0 . . .
0
0
1 . . .
0
...
0
0 . . .
1





B,B
is the n×n identity matrix.
Three.III.1.19
Taking this as the natural basis
B = ⟨⃗β1, ⃗β2, ⃗β3, ⃗β4⟩= ⟨
µ
1
0
0
0
¶
,
µ
0
1
0
0
¶
,
µ
0
0
1
0
¶
,
µ
0
0
0
1
¶
⟩
the transpose map acts in this way
⃗β1 7→⃗β1
⃗β2 7→⃗β3
⃗β3 7→⃗β2
⃗β4 7→⃗β4
so that representing the images with respect to the codomain’s basis and adjoining those column
vectors together gives this.
RepB,B(trans) =




1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1




B,B
Three.III.1.20
(a) With respect to the basis of the codomain, the images of the members of the
basis of the domain are represented as
RepB(⃗β2) =




0
1
0
0




RepB(⃗β3) =




0
0
1
0




RepB(⃗β4) =




0
0
0
1




RepB(⃗0) =




0
0
0
0




and consequently, the matrix representing the transformation is this.




0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
0




(b)




0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0




(c)




0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0




Three.III.1.21
(a) The picture of ds : R2 →R2 is this.
⃗u
⃗v
ds(⃗u)
ds(⃗v)
ds
−→

Answers to Exercises
311
This map’s eﬀect on the vectors in the standard basis for the domain is
µ
1
0
¶
ds
7−→
µ
s
0
¶
µ
0
1
¶
ds
7−→
µ
0
s
¶
and those images are represented with respect to the codomain’s basis (again, the standard basis)
by themselves.
RepE2(
µ
s
0
¶
) =
µ
s
0
¶
RepE2(
µ
0
s
¶
) =
µ
0
s
¶
Thus the representation of the dilation map is this.
RepE2,E2(ds) =
µ
s
0
0
s
¶
(b) The picture of fℓ: R2 →R2 is this.
fℓ
7−→
Some calculation (see Exercise I.29) shows that when the line has slope k
µ
1
0
¶
fℓ
7−→
µ
(1 −k2)/(1 + k2)
2k/(1 + k2)
¶
µ
0
1
¶
fℓ
7−→
µ
2k/(1 + k2)
−(1 −k2)/(1 + k2)
¶
(the case of a line with undeﬁned slope is separate but easy) and so the matrix representing reﬂection
is this.
RepE2,E2(fℓ) =
1
1 + k2 ·
µ
1 −k2
2k
2k
−(1 −k2)
¶
Three.III.1.22
Call the map t: R2 →R2.
(a) To represent this map with respect to the standard bases, we must ﬁnd, and then represent, the
images of the vectors ⃗e1 and ⃗e2 from the domain’s basis. The image of ⃗e1 is given.
One way to ﬁnd the image of ⃗e2 is by eye — we can see this.
µ
1
1
¶
−
µ
1
0
¶
=
µ
0
1
¶
t
7−→
µ
2
0
¶
−
µ
−1
0
¶
=
µ
3
0
¶
A more systemmatic way to ﬁnd the image of ⃗e2 is to use the given information to represent the
transformation, and then use that representation to determine the image. Taking this for a basis,
C = ⟨
µ1
1
¶
,
µ1
0
¶
⟩
the given information says this.
RepC,E2(t)
µ
2
−1
0
0
¶
As
RepC(⃗e2) =
µ
1
−1
¶
C
we have that
RepE2(t(⃗e2)) =
µ2
−1
0
0
¶
C,E2
µ 1
−1
¶
C
=
µ3
0
¶
E2
and consequently we know that t(⃗e2) = 3 · ⃗e1 (since, with respect to the standard basis, this vector
is represented by itself). Therefore, this is the representation of t with respect to E2, E2.
RepE2,E2(t) =
µ
−1
3
0
0
¶
E2,E2
(b) To use the matrix developed in the prior item, note that
RepE2(
µ
0
5
¶
) =
µ
0
5
¶
E2
and so we have this is the representation, with respect to the codomain’s basis, of the image of the
given vector.
RepE2(t(
µ
0
5
¶
)) =
µ
−1
3
0
0
¶
E2,E2
µ
0
5
¶
E2
=
µ
15
0
¶
E2
Because the codomain’s basis is the standard one, and so vectors in the codomain are represented
by themselves, we have this.
t(
µ
0
5
¶
) =
µ
15
0
¶

312
Linear Algebra, by Hefferon
(c) We ﬁrst ﬁnd the image of each member of B, and then represent those images with respect to
D. For the ﬁrst step, we can use the matrix developed earlier.
RepE2(
µ
1
−1
¶
) =
µ
−1
3
0
0
¶
E2,E2
µ
1
−1
¶
E2
=
µ
−4
0
¶
E2
so
t(
µ
1
−1
¶
) =
µ
−4
0
¶
Actually, for the second member of B there is no need to apply the matrix because the problem
statement gives its image.
t(
µ
1
1
¶
) =
µ
2
0
¶
Now representing those images with respect to D is routine.
RepD(
µ
−4
0
¶
) =
µ
−1
2
¶
D
and
RepD(
µ
2
0
¶
) =
µ
1/2
−1
¶
D
Thus, the matrix is this.
RepB,D(t) =
µ−1
1/2
2
−1
¶
B,D
(d) We know the images of the members of the domain’s basis from the prior item.
t(
µ 1
−1
¶
) =
µ−4
0
¶
t(
µ1
1
¶
) =
µ2
0
¶
We can compute the representation of those images with respect to the codomain’s basis.
RepB(
µ−4
0
¶
) =
µ−2
−2
¶
B
and
RepB(
µ2
0
¶
) =
µ1
1
¶
B
Thus this is the matrix.
RepB,B(t) =
µ
−2
1
−2
1
¶
B,B
Three.III.1.23
(a) The images of the members of the domain’s basis are
⃗β1 7→h(⃗β1)
⃗β2 7→h(⃗β2)
. . .
⃗βn 7→h(⃗βn)
and those images are represented with respect to the codomain’s basis in this way.
Reph(B)( h(⃗β1) ) =





1
0
...
0





Reph(B)( h(⃗β2) ) =





0
1
...
0





. . .
Reph(B)( h(⃗βn) ) =





0
0
...
1





Hence, the matrix is the identity.
RepB,h(B)(h) =





1
0
. . .
0
0
1
0
...
0
0
1





(b) Using the matrix in the prior item, the representation is this.
Reph(B)( h(⃗v) ) =



c1
...
cn



h(B)
Three.III.1.24
The product





h1,1
. . .
h1,i
. . .
h1,n
h2,1
. . .
h2,i
. . .
h2,n
...
hm,1
. . .
hm,i
. . .
h1,n













0
...
1
...
0








=





h1,i
h2,i
...
hm,i





gives the i-th column of the matrix.
Three.III.1.25
(a) The images of the basis vectors for the domain are cos x
d/dx
7−→−sin x and sin x
d/dx
7−→
cos x. Representing those with respect to the codomain’s basis (again, B) and adjoining the repre-
sentations gives this matrix.
RepB,B( d
dx) =
µ
0
1
−1
0
¶
B,B

Answers to Exercises
313
(b) The images of the vectors in the domain’s basis are ex d/dx
7−→ex and e2x d/dx
7−→2e2x. Representing
with respect to the codomain’s basis and adjoining gives this matrix.
RepB,B( d
dx) =
µ
1
0
0
2
¶
B,B
(c) The images of the members of the domain’s basis are 1
d/dx
7−→0, x
d/dx
7−→1, ex d/dx
7−→ex, and xex d/dx
7−→
ex + xex. Representing these images with respect to B and adjoining gives this matrix.
RepB,B( d
dx) =




0
1
0
0
0
0
0
0
0
0
1
1
0
0
0
1




B,B
Three.III.1.26
(a) It is the set of vectors of the codomain represented with respect to the codomain’s
basis in this way.
{
µ
1
0
0
0
¶ µ
x
y
¶ ¯¯ x, y ∈R} = {
µ
x
0
¶ ¯¯ x, y ∈R}
As the codomain’s basis is E2, and so each vector is represented by itself, the range of this transfor-
mation is the x-axis.
(b) It is the set of vectors of the codomain represented in this way.
{
µ
0
0
3
2
¶ µ
x
y
¶ ¯¯ x, y ∈R} = {
µ
0
3x + 2y
¶ ¯¯ x, y ∈R}
With respect to E2 vectors represent themselves so this range is the y axis.
(c) The set of vectors represented with respect to E2 as
{
µ
a
b
2a
2b
¶ µ
x
y
¶ ¯¯ x, y ∈R} = {
µ
ax + by
2ax + 2by
¶ ¯¯ x, y ∈R} = {(ax + by) ·
µ
1
2
¶ ¯¯ x, y ∈R}
is the line y = 2x, provided either a or b is not zero, and is the set consisting of just the origin if
both are zero.
Three.III.1.27
Yes, for two reasons.
First, the two maps h and ˆh need not have the same domain and codomain. For instance,
µ
1
2
3
4
¶
represents a map h: R2 →R2 with respect to the standard bases that sends
µ1
0
¶
7→
µ1
3
¶
and
µ0
1
¶
7→
µ2
4
¶
and also represents a ˆh: P1 →R2 with respect to ⟨1, x⟩and E2 that acts in this way.
1 7→
µ
1
3
¶
and
x 7→
µ
2
4
¶
The second reason is that, even if the domain and codomain of h and ˆh coincide, diﬀerent bases
produce diﬀerent maps. An example is the 2×2 identity matrix
I =
µ
1
0
0
1
¶
which represents the identity map on R2 with respect to E2, E2. However, with respect to E2 for the
domain but the basis D = ⟨⃗e2,⃗e1⟩for the codomain, the same matrix I represents the map that swaps
the ﬁrst and second components
µ
x
y
¶
7→
µ
y
x
¶
(that is, reﬂection about the line y = x).
Three.III.1.28
We mimic Example 1.1, just replacing the numbers with letters.
Write B as ⟨⃗β1, . . . , ⃗βn⟩and D as ⟨⃗δ1, . . . ,⃗δm⟩. By deﬁnition of representation of a map with respect
to bases, the assumption that
RepB,D(h) =



h1,1
. . .
h1,n
...
...
hm,1
. . .
hm,n




314
Linear Algebra, by Hefferon
means that h(⃗βi) = hi,1⃗δ1 + · · · + hi,n⃗δn. And, by the deﬁnition of the representation of a vector with
respect to a basis, the assumption that
RepB(⃗v) =



c1
...
cn



means that ⃗v = c1⃗β1 + · · · + cn⃗βn. Substituting gives
h(⃗v) = h(c1 · ⃗β1 + · · · + cn · ⃗βn)
= c1 · h(⃗β1) + · · · + cn · ⃗βn
= c1 · (h1,1⃗δ1 + · · · + hm,1⃗δm) + · · · + cn · (h1,n⃗δ1 + · · · + hm,n⃗δm)
= (h1,1c1 + · · · + h1,ncn) · ⃗δ1 + · · · + (hm,1c1 + · · · + hm,ncn) · ⃗δm
and so h(⃗v) is represented as required.
Three.III.1.29
(a) The picture is this.
The images of the vectors from the domain’s basis


1
0
0

7→


1
0
0




0
1
0

7→


0
cos θ
−sin θ




0
0
1

7→


0
sin θ
cos θ


are represented with respect to the codomain’s basis (again, E3) by themselves, so adjoining the
representations to make the matrix gives this.
RepE3,E3(rθ) =


1
0
0
0
cos θ
sin θ
0
−sin θ
cos θ


(b) The picture is similar to the one in the prior answer. The images of the vectors from the domain’s
basis


1
0
0

7→


cos θ
0
sin θ




0
1
0

7→


0
1
0




0
0
1

7→


−sin θ
0
cos θ


are represented with respect to the codomain’s basis E3 by themselves, so this is the matrix.


cos θ
0
−sin θ
0
1
0
sin θ
0
cos θ


(c) To a person standing up, with the vertical z-axis, a rotation of the xy-plane that is clockwise
proceeds from the positive y-axis to the positive x-axis. That is, it rotates opposite to the direction
in Example 1.8. The images of the vectors from the domain’s basis


1
0
0

7→


cos θ
−sin θ
0




0
1
0

7→


sin θ
cos θ
0




0
0
1

7→


0
0
1


are represented with respect to E3 by themselves, so the matrix is this.


cos θ
sin θ
0
−sin θ
cos θ
0
0
0
1


(d)




cos θ
sin θ
0
0
−sin θ
cos θ
0
0
0
0
1
0
0
0
0
1




Three.III.1.30
(a) Write BU as ⟨⃗β1, . . . , ⃗βk⟩and then BV as ⟨⃗β1, . . . , ⃗βk, ⃗βk+1, . . . , ⃗βn⟩. If
RepBU (⃗v) =



c1
...
ck



so that ⃗v = c1 · ⃗β1 + · · · + ck · ⃗βk

Answers to Exercises
315
then,
RepBV (⃗v) =










c1
...
ck
0
...
0










because ⃗v = c1 · ⃗β1 + · · · + ck · ⃗βk + 0 · ⃗βk+1 + · · · + 0 · ⃗βn.
(b) We must ﬁrst decide what the question means. Compare h: V →W with its restriction to the
subspace h↾U : U →W. The rangespace of the restriction is a subspace of W, so ﬁx a basis Dh(U)
for this rangespace and extend it to a basis DV for W. We want the relationship between these two.
RepBV ,DV (h)
and
RepBU,Dh(U)(h↾U)
The answer falls right out of the prior item: if
RepBU,Dh(U)(h↾U) =



h1,1
. . .
h1,k
...
...
hp,1
. . .
hp,k



then the extension is represented in this way.
RepBV ,DV (h) =










h1,1
. . .
h1,k
h1,k+1
. . .
h1,n
...
...
hp,1
. . .
hp,k
hp,k+1
. . .
hp,n
0
. . .
0
hp+1,k+1
. . .
hp+1,n
...
...
0
. . .
0
hm,k+1
. . .
hm,n










(c) Take Wi to be the span of {h(⃗β1), . . . , h(⃗βi)}.
(d) Apply the answer from the second item to the third item.
(e) No. For instance πx : R2 →R2, projection onto the x axis, is represented by these two upper-
triangular matrices
RepE2,E2(πx) =
µ
1
0
0
0
¶
and
RepC,E2(πx) =
µ
0
1
0
0
¶
where C = ⟨⃗e2,⃗e1⟩.
Subsection Three.III.2: Any Matrix Represents a Linear Map
Three.III.2.9
(a) Yes; we are asking if there are scalars c1 and c2 such that
c1
µ
2
2
¶
+ c2
µ
1
5
¶
=
µ
1
−3
¶
which gives rise to a linear system
2c1 + c2 =
1
2c1 + 5c2 = −3
−ρ1+ρ2
−→
2c1 + c2 =
1
4c2 = −4
and Gauss’ method produces c2 = −1 and c1 = 1. That is, there is indeed such a pair of scalars and
so the vector is indeed in the column space of the matrix.
(b) No; we are asking if there are scalars c1 and c2 such that
c1
µ
4
2
¶
+ c2
µ
−8
−4
¶
=
µ
0
1
¶
and one way to proceed is to consider the resulting linear system
4c1 −8c2 = 0
2c1 −4c2 = 1
that is easily seen to have no solution. Another way to proceed is to note that any linear combination
of the columns on the left has a second component half as big as its ﬁrst component, but the vector
on the right does not meet that criterion.

316
Linear Algebra, by Hefferon
(c) Yes; we can simply observe that the vector is the ﬁrst column minus the second. Or, failing that,
setting up the relationship among the columns
c1


1
1
−1

+ c2


−1
1
−1

+ c3


1
−1
1

=


2
0
0


and considering the resulting linear system
c1 −c2 + c3 = 2
c1 + c2 −c3 = 0
−c1 −c2 + c3 = 0
−ρ1+ρ2
−→
ρ1+ρ3
c1 −
c2 + c3 =
2
2c2 −2c3 = −2
−2c2 + 2c3 =
2
ρ2+ρ3
−→
c1 −c2 + c3 =
2
2c2 −2c3 = −2
0 =
0
gives the additional information (beyond that there is at least one solution) that there are inﬁnitely
many solutions. Parametizing gives c2 = −1 + c3 and c1 = 1, and so taking c3 to be zero gives a
particular solution of c1 = 1, c2 = −1, and c3 = 0 (which is, of course, the observation made at the
start).
Three.III.2.10
As described in the subsection, with respect to the standard bases, representations are
transparent, and so, for instance, the ﬁrst matrix describes this map.


1
0
0

=


1
0
0


E3
7→
µ
1
0
¶
E2
=
µ
1
0
¶


0
1
0

7→
µ
1
1
¶


0
0
1

7→
µ
3
4
¶
So, for this ﬁrst one, we are asking whether thare are scalars such that
c1
µ
1
0
¶
+ c2
µ
1
1
¶
+ c3
µ
3
4
¶
=
µ
1
3
¶
that is, whether the vector is in the column space of the matrix.
(a) Yes. We can get this conclusion by setting up the resulting linear system and applying Gauss’
method, as usual. Another way to get it is to note by inspection of the equation of columns that
taking c3 = 3/4, and c1 = −5/4, and c2 = 0 will do. Still a third way to get this conclusion is to
note that the rank of the matrix is two, which equals the dimension of the codomain, and so the
map is onto — the range is all of R2 and in particular includes the given vector.
(b) No; note that all of the columns in the matrix have a second component that is twice the ﬁrst,
while the vector does not. Alternatively, the column space of the matrix is
{c1
µ
2
4
¶
+ c2
µ
0
0
¶
+ c3
µ
3
6
¶ ¯¯ c1, c2, c3 ∈R} = {c
µ
1
2
¶ ¯¯ c ∈R}
(which is the fact already noted, but was arrived at by calculation rather than inspiration), and the
given vector is not in this set.
Three.III.2.11
(a) The ﬁrst member of the basis
µ
0
1
¶
=
µ
1
0
¶
B
is mapped to
µ
1/2
−1/2
¶
D
which is this member of the codomain.
1
2 ·
µ
1
1
¶
−1
2 ·
µ
1
−1
¶
=
µ
0
1
¶
(b) The second member of the basis is mapped
µ
1
0
¶
=
µ
0
1
¶
B
7→
µ
(1/2
1/2
¶
D
to this member of the codomain.
1
2 ·
µ
1
1
¶
+ 1
2 ·
µ
1
−1
¶
=
µ
1
0
¶
(c) Because the map that the matrix represents is the identity map on the basis, it must be the
identity on all members of the domain. We can come to the same conclusion in another way by
considering
µ
x
y
¶
=
µ
y
x
¶
B

Answers to Exercises
317
which is mapped to
µ(x + y)/2
(x −y)/2
¶
D
which represents this member of R2.
x + y
2
·
µ
1
1
¶
+ x −y
2
·
µ
1
−1
¶
=
µ
x
y
¶
Three.III.2.12
A general member of the domain, represented with respect to the domain’s basis as
a cos θ + b sin θ =
µ
a
a + b
¶
B
is mapped to
µ
0
a
¶
D
representing
0 · (cos θ + sin θ) + a · (cos θ)
and so the linear map represented by the matrix with respect to these bases
a cos θ + b sin θ 7→a cos θ
is projection onto the ﬁrst component.
Three.III.2.13
Denote the given basis of P2 by B. Then application of the linear map is represented
by matrix-vector addition. Thus, the ﬁrst vector in E3 is mapped to the element of P2 represented
with respect to B by


1
3
0
0
1
0
1
0
1




1
0
0

=


1
0
1


and that element is 1 + x. The other two images of basis vectors are calculated similarly.


1
3
0
0
1
0
1
0
1




0
1
0

=


3
1
0

= RepB(4 + x2)


1
3
0
0
1
0
1
0
1




0
0
1

=


0
0
1

= RepB(x)
We can thus decide if 1 + 2x is in the range of the map by looking for scalars c1, c2, and c3 such that
c1 · (1) + c2 · (1 + x2) + c3 · (x) = 1 + 2x
and obviously c1 = 1, c2 = 0, and c3 = 1 suﬃce. Thus it is in the range, and in fact it is the image of
this vector.
1 ·


1
0
0

+ 0 ·


0
1
0

+ 1 ·


0
0
1


Three.III.2.14
Let the matrix be G, and suppose that it rperesents g: V →W with respect to bases
B and D. Because G has two columns, V is two-dimensional. Because G has two rows, W is two-
dimensional. The action of g on a general member of the domain is this.
µx
y
¶
B
7→
µ x + 2y
3x + 6y
¶
D
(a) The only representation of the zero vector in the codomain is
RepD(⃗0) =
µ
0
0
¶
D
and so the set of representations of members of the nullspace is this.
{
µ
x
y
¶
B
¯¯ x + 2y = 0 and 3x + 6y = 0} = {y ·
µ
−1/2
1
¶
D
¯¯ y ∈R}
(b) The representation map RepD : W →R2 and its inverse are isomorphisms, and so preserve the
dimension of subspaces. The subspace of R2 that is in the prior item is one-dimensional. Therefore,
the image of that subspace under the inverse of the representation map — the nullspace of G, is also
one-dimensional.
(c) The set of representations of members of the rangespace is this.
{
µ
x + 2y
3x + 6y
¶
D
¯¯ x, y ∈R} = {k ·
µ
1
3
¶
D
¯¯ k ∈R}
(d) Of course, Theorem 2.3 gives that the rank of the map equals the rank of the matrix, which is
one. Alternatively, the same argument that was used above for the nullspace gives here that the
dimension of the rangespace is one.

318
Linear Algebra, by Hefferon
(e) One plus one equals two.
Three.III.2.15
No, the rangespaces may diﬀer. Example 2.2 shows this.
Three.III.2.16
Recall that the represention map
V
RepB
7−→Rn
is an isomorphism.
Thus, its inverse map Rep−1
B : Rn →V is also an isomorphism.
The desired
transformation of Rn is then this composition.
Rn Rep−1
B
7−→V
RepD
7−→Rn
Because a composition of isomorphisms is also an isomorphism, this map RepD ◦Rep−1
B is an isomor-
phism.
Three.III.2.17
Yes. Consider
H =
µ
1
0
0
1
¶
representing a map from R2 to R2. With respect to the standard bases B1 = E2, D1 = E2 this matrix
represents the identity map. With respect to
B2 = D2 = ⟨
µ1
1
¶
,
µ 1
−1
¶
⟩
this matrix again represents the identity. In fact, as long as the starting and ending bases are equal —
as long as Bi = Di — then the map represented by H is the identity.
Three.III.2.18
This is immediate from Corollary 2.6.
Three.III.2.19
The ﬁrst map
µ
x
y
¶
=
µ
x
y
¶
E2
7→
µ
3x
2y
¶
E2
=
µ
3x
2y
¶
stretches vectors by a factor of three in the x direction and by a factor of two in the y direction. The
second map
µ
x
y
¶
=
µ
x
y
¶
E2
7→
µ
x
0
¶
E2
=
µ
x
0
¶
projects vectors onto the x axis. The third
µ
x
y
¶
=
µ
x
y
¶
E2
7→
µ
y
x
¶
E2
=
µ
y
x
¶
interchanges ﬁrst and second components (that is, it is a reﬂection about the line y = x). The last
µ
x
y
¶
=
µ
x
y
¶
E2
7→
µ
x + 3y
y
¶
E2
=
µ
x + 3y
y
¶
stretches vectors parallel to the y axis, by an amount equal to three times their distance from that
axis (this is a skew.)
Three.III.2.20
(a) This is immediate from Theorem 2.3.
(b) Yes. This is immediate from the prior item.
To give a speciﬁc example, we can start with E3 as the basis for the domain, and then we require
a basis D for the codomain R3. The matrix H gives the action of the map as this


1
0
0

=


1
0
0


E3
7→


1
2
0


D


0
1
0

=


0
1
0


E3
7→


0
0
1


D


0
0
1

=


0
0
1


E3
7→


0
0
0


D
and there is no harm in ﬁnding a basis D so that
RepD(


1
0
0

) =


1
2
0


D
and
RepD(


0
1
0

) =


0
0
1


D
that is, so that the map represented by H with respect to E3, D is projection down onto the xy plane.
The second condition gives that the third member of D is ⃗e2. The ﬁrst condition gives that the ﬁrst
member of D plus twice the second equals ⃗e1, and so this basis will do.
D = ⟨


0
−1
0

,


1/2
1/2
0

,


0
1
0

⟩

Answers to Exercises
319
Three.III.2.21
(a) Recall that the representation map RepB : V →Rn is linear (it is actually an
isomorphism, but we do not need that it is one-to-one or onto here). Considering the column vector
x to be a n×1 matrix gives that the map from Rn to R that takes a column vector to its dot product
with ⃗x is linear (this is a matrix-vector product and so Theorem 2.1 applies). Thus the map under
consideration h⃗x is linear because it is the composistion of two linear maps.
⃗v 7→RepB(⃗v) 7→⃗x · RepB(⃗v)
(b) Any linear map g: V →R is represented by some matrix
¡
g1
g2
· · ·
gn
¢
(the matrix has n columns because V is n-dimensional and it has only one row because R is one-
dimensional). Then taking ⃗x to be the column vector that is the transpose of this matrix
⃗x =



g1
...
gn



has the desired action.
⃗v =



v1
...
vn


7→



g1
...
gn






v1
...
vn


= g1v1 + · · · + gnvn
(c) No. If ⃗x has any nonzero entries then h⃗x cannot be the zero map (and if ⃗x is the zero vector then
h⃗x can only be the zero map).
Three.III.2.22
See the following section.
Subsection Three.IV.1: Sums and Scalar Products
Three.IV.1.7
(a)
µ
7
0
6
9
1
6
¶
(b)
µ
12
−6
−6
6
12
18
¶
(c)
µ
4
2
0
6
¶
(d)
µ
−1
28
2
1
¶
(e) Not deﬁned.
Three.IV.1.8
Represent the domain vector ⃗v ∈V and the maps g, h: V →W with respect to bases
B, D in the usual way.
(a) The representation of (g + h) (⃗v) = g(⃗v) + h(⃗v)
¡
(g1,1v1 + · · · + g1,nvn)⃗δ1 + · · · + (gm,1v1 + · · · + gm,nvn)⃗δm
¢
+
¡
(h1,1v1 + · · · + h1,nvn)⃗δ1 + · · · + (hm,1v1 + · · · + hm,nvn)⃗δm
¢
regroups
= ((g1,1 + h1,1)v1 + · · · + (g1,1 + h1,n)vn) · ⃗δ1 + · · · + ((gm,1 + hm,1)v1 + · · · + (gm,n + hm,n)vn) · ⃗δm
to the entry-by-entry sum of the representation of g(⃗v) and the representation of h(⃗v).
(b) The representation of (r · h) (⃗v) = r ·
¡
h(⃗v)
¢
r ·
¡
(h1,1v1 + h1,2v2 + · · · + h1,nvn)⃗δ1 + · · · + (hm,1v1 + hm,2v2 + · · · + hm,nvn)⃗δm
¢
= (rh1,1v1 + · · · + rh1,nvn) · ⃗δ1 + · · · + (rhm,1v1 + · · · + rhm,nvn) · ⃗δm
is the entry-by-entry multiple of r and the representation of h.
Three.IV.1.9
First, each of these properties is easy to check in an entry-by-entry way. For example,
writing
G =



g1,1
. . .
g1,n
...
...
gm,1
. . .
gm,n



H =



h1,1
. . .
h1,n
...
...
hm,1
. . .
hm,n



then, by deﬁnition we have
G + H =



g1,1 + h1,1
. . .
g1,n + h1,n
...
...
gm,1 + hm,1
. . .
gm,n + hm,n



H + G =



h1,1 + g1,1
. . .
h1,n + g1,n
...
...
hm,1 + gm,1
. . .
hm,n + gm,n



and the two are equal since their entries are equal gi,j + hi,j = hi,j + gi,j. That is, each of these is easy
to check by using Deﬁnition 1.3 alone.
However, each property is also easy to understand in terms of the represented maps, by applying
Theorem 1.5 as well as the deﬁnition.

320
Linear Algebra, by Hefferon
(a) The two maps g +h and h+g are equal because g(⃗v)+h(⃗v) = h(⃗v)+g(⃗v), as addition is commu-
tative in any vector space. Because the maps are the same, they must have the same representative.
(b) As with the prior answer, except that here we apply that vector space addition is associative.
(c) As before, except that here we note that g(⃗v) + z(⃗v) = g(⃗v) +⃗0 = g(⃗v).
(d) Apply that 0 · g(⃗v) = ⃗0 = z(⃗v).
(e) Apply that (r + s) · g(⃗v) = r · g(⃗v) + s · g(⃗v).
(f) Apply the prior two items with r = 1 and s = −1.
(g) Apply that r · (g(⃗v) + h(⃗v)) = r · g(⃗v) + r · h(⃗v).
(h) Apply that (rs) · g(⃗v) = r · (s · g(⃗v)).
Three.IV.1.10
For any V, W with bases B, D, the (appropriately-sized) zero matrix represents this
map.
⃗β1 7→0 · ⃗δ1 + · · · + 0 · ⃗δm
· · ·
⃗βn 7→0 · ⃗δ1 + · · · + 0 · ⃗δm
This is the zero map.
There are no other matrices that represent only one map. For, suppose that H is not the zero
matrix. Then it has a nonzero entry; assume that hi,j ̸= 0. With respect to bases B, D, it represents
h1 : V →W sending
⃗βj 7→h1,j⃗δ1 + · · · + hi,j⃗δi + · · · + hm,j⃗δm
and with respcet to B, 2 · D it also represents h2 : V →W sending
⃗βj 7→h1,j · (2⃗δ1) + · · · + hi,j · (2⃗δi) + · · · + hm,j · (2⃗δm)
(the notation 2·D means to double all of the members of D). These maps are easily seen to be unequal.
Three.IV.1.11
Fix bases B and D for V and W, and consider RepB,D : L(V, W) →Mm×n associating
each linear map with the matrix representing that map h 7→RepB,D(h). From the prior section we
know that (under ﬁxed bases) the matrices correspond to linear maps, so the representation map is
one-to-one and onto. That it preserves linear operations is Theorem 1.5.
Three.IV.1.12
Fix bases and represent the transformations with 2×2 matrices. The space of matrices
M2×2 has dimension four, and hence the above six-element set is linearly dependent. By the prior
exercise that extends to a dependence of maps.
(The misleading part is only that there are six
transformations, not ﬁve, so that we have more than we need to give the existence of the dependence.)
Three.IV.1.13
That the trace of a sum is the sum of the traces holds because both trace(H + G) and
trace(H) + trace(G) are the sum of h1,1 + g1,1 with h2,2 + g2,2, etc. For scalar multiplication we have
trace(r · H) = r · trace(H); the proof is easy. Thus the trace map is a homomorphism from Mn×n to
R.
Three.IV.1.14
(a) The i, j entry of (G + H)trans is gj,i + hj,i. That is also the i, j entry of Gtrans +
Htrans.
(b) The i, j entry of (r · H)trans is rhj,i, which is also the i, j entry of r · Htrans.
Three.IV.1.15
(a) For H + Htrans, the i, j entry is hi,j + hj,i and the j, i entry of is hj,i + hi,j. The
two are equal and thus H + Htrans is symmetric.
Every symmetric matrix does have that form, since it can be written H = (1/2) · (H + Htrans).
(b) The set of symmetric matrices is nonempty as it contains the zero matrix.
Clearly a scalar
multiple of a symmetric matrix is symmetric. A sum H +G of two symmetric matrices is symmetric
because hi,j + gi,j = hj,i + gj,i (since hi,j = hj,i and gi,j = gj,i). Thus the subset is nonempty and
closed under the inherited operations, and so it is a subspace.
Three.IV.1.16
(a) Scalar multiplication leaves the rank of a matrix unchanged except that multi-
plication by zero leaves the matrix with rank zero. (This follows from the ﬁrst theorem of the book,
that multiplying a row by a nonzero scalar doesn’t change the solution set of the associated linear
system.)
(b) A sum of rank n matrices can have rank less than n. For instance, for any matrix H, the sum
H + (−1) · H has rank zero.
A sum of rank n matrices can have rank greater than n. Here are rank one matrices that sum
to a rank two matrix.
µ
1
0
0
0
¶
+
µ
0
0
0
1
¶
=
µ
1
0
0
1
¶

Answers to Exercises
321
Subsection Three.IV.2: Matrix Multiplication
Three.IV.2.14
(a)
µ
0
15.5
0
−19
¶
(b)
µ
2
−1
−1
17
−1
−1
¶
(c) Not deﬁned.
(d)
µ
1
0
0
1
¶
Three.IV.2.15
(a)
µ
1
−2
10
4
¶
(b)
µ
1
−2
10
4
¶ µ
−2
3
−4
1
¶
=
µ
6
1
−36
34
¶
(c)
µ
−18
17
−24
16
¶
(d)
µ
1
−1
2
0
¶ µ
−18
17
−24
16
¶
=
µ
6
1
−36
34
¶
Three.IV.2.16
(a) Yes.
(b) Yes.
(c) No.
(d) No.
Three.IV.2.17
(a) 2×1
(b) 1×1
(c) Not deﬁned.
(d) 2×2
Three.IV.2.18
We have
h1,1 · (g1,1y1 + g1,2y2) + h1,2 · (g2,1y1 + g2,2y2) + h1,3 · (g3,1y1 + g3,2y2) = d1
h2,1 · (g1,1y1 + g1,2y2) + h2,2 · (g2,1y1 + g2,2y2) + h2,3 · (g3,1y1 + g3,2y2) = d2
which, after expanding and regrouping about the y’s yields this.
(h1,1g1,1 + h1,2g2,1 + h1,3g3,1)y1 + (h1,1g1,2 + h1,2g2,2 + h1,3g3,2)y2 = d1
(h2,1g1,1 + h2,2g2,1 + h2,3g3,1)y1 + (h2,1g1,2 + h2,2g2,2 + h2,3g3,2)y2 = d2
The starting system, and the system used for the substitutions, can be expressed in matrix language.
µh1,1
h1,2
h1,3
h2,1
h2,2
h2,3
¶ 

x1
x2
x3

= H


x1
x2
x3

=
µd1
d2
¶


g1,1
g1,2
g2,1
g2,2
g3,1
g3,2


µy1
y2
¶
= G
µy1
y2
¶
=


x1
x2
x3


With this, the substitution is ⃗d = H⃗x = H(G⃗y) = (HG)⃗y.
Three.IV.2.19
Technically, no. The dot product operation yields a scalar while the matrix product
yields a 1×1 matrix. However, we usually will ignore the distinction.
Three.IV.2.20
The action of d/dx on B is 1 7→0, x 7→1, x2 7→2x, . . . and so this is its (n+1)×(n+1)
matrix representation.
RepB,B( d
dx) =







0
1
0
0
0
0
2
0
...
0
0
0
n
0
0
0
0







The product of this matrix with itself is deﬁned because the matrix is square.







0
1
0
0
0
0
2
0
...
0
0
0
n
0
0
0
0







2
=









0
0
2
0
0
0
0
0
6
0
...
0
0
0
n(n −1)
0
0
0
0
0
0
0
0









The map so represented is the composition
p
d
dx
7−→d p
dx
d
dx
7−→d2 p
dx2
which is the second derivative operation.
Three.IV.2.21
It is true for all one-dimensional spaces. Let f and g be transformations of a one-
dimensional space. We must show that g ◦f (⃗v) = f ◦g (⃗v) for all vectors. Fix a basis B for the space
and then the transformations are represented by 1×1 matrices.
F = RepB,B(f) =
¡
f1,1
¢
G = RepB,B(g) =
¡
g1,1
¢
Therefore, the compositions can be represented as GF and FG.
GF = RepB,B(g ◦f) =
¡
g1,1f1,1
¢
FG = RepB,B(f ◦g) =
¡
f1,1g1,1
¢
These two matrices are equal and so the compositions have the same eﬀect on each vector in the space.
Three.IV.2.22
It would not represent linear map composition; Theorem 2.6 would fail.

322
Linear Algebra, by Hefferon
Three.IV.2.23
Each follows easily from the associated map fact. For instance, p applications of the
transformation h, following q applications, is simply p + q applications.
Three.IV.2.24
Although these can be done by going through the indices, they are best understood
in terms of the represented maps. That is, ﬁx spaces and bases so that the matrices represent linear
maps f, g, h.
(a) Yes; we have both r · (g ◦h) (⃗v) = r · g( h(⃗v) ) = (r · g) ◦h (⃗v) and g ◦(r · h) (⃗v) = g( r · h(⃗v) ) =
r · g(h(⃗v)) = r · (g ◦h) (⃗v) (the second equality holds because of the linearity of g).
(b) Both answers are yes. First, f◦(rg+sh) and r·(f◦g)+s·(f◦h) both send ⃗v to r·f(g(⃗v))+s·f(h(⃗v));
the calculation is as in the prior item (using the linearity of f for the ﬁrst one). For the other,
(rf + sg) ◦h and r · (f ◦h) + s · (g ◦h) both send ⃗v to r · f(h(⃗v)) + s · g(h(⃗v)).
Three.IV.2.25
We have not seen a map interpretation of the transpose operation, so we will verify
these by considering the entries.
(a) The i, j entry of GHtrans is the j, i entry of GH, which is the dot product of the j-th row of G
and the i-th column of H. The i, j entry of HtransGtrans is the dot product of the i-th row of Htrans
and the j-th column of Gtrans, which is the the dot product of the i-th column of H and the j-th
row of G. Dot product is commutative and so these two are equal.
(b) By the prior item each equals its transpose, e.g., (HHtrans)trans = HtranstransHtrans = HHtrans.
Three.IV.2.26
Consider rx, ry : R3 →R3 rotating all vectors π/2 radians counterclockwise about the
x and y axes (counterclockwise in the sense that a person whose head is at ⃗e1 or ⃗e2 and whose feet are
at the origin sees, when looking toward the origin, the rotation as counterclockwise).
Rotating rx ﬁrst and then ry is diﬀerent than rotating ry ﬁrst and then rx. In particular, rx(⃗e3) = −⃗e2
so ry ◦rx(⃗e3) = −⃗e2, while ry(⃗e3) = ⃗e1 so rx ◦ry(⃗e3) = ⃗e1, and hence the maps do not commute.
Three.IV.2.27
It doesn’t matter (as long as the spaces have the appropriate dimensions).
For associativity, suppose that F is m×r, that G is r ×n, and that H is n×k. We can take
any r dimensional space, any m dimensional space, any n dimensional space, and any k dimensional
space — for instance, Rr, Rm, Rn, and Rk will do. We can take any bases A, B, C, and D, for those
spaces. Then, with respect to C, D the matrix H represents a linear map h, with respect to B, C the
matrix G represents a g, and with respect to A, B the matrix F represents an f. We can use those
maps in the proof.
The second half is done similarly, except that G and H are added and so we must take them to
represent maps with the same domain and codomain.
Three.IV.2.28
(a) The product of rank n matrices can have rank less than or equal to n but not
greater than n.
To see that the rank can fall, consider the maps πx, πy : R2 →R2 projecting onto the axes. Each
is rank one but their composition πx◦πy, which is the zero map, is rank zero. That can be translated
over to matrices representing those maps in this way.
RepE2,E2(πx) · RepE2,E2(πy) =
µ
1
0
0
0
¶ µ
0
0
0
1
¶
=
µ
0
0
0
0
¶
To prove that the product of rank n matrices cannot have rank greater than n, we can apply the
map result that the image of a linearly dependent set is linearly dependent. That is, if h: V →W
and g: W →X both have rank n then a set in the range R(g ◦h) of size larger than n is the image
under g of a set in W of size larger than n and so is linearly dependent (since the rank of h is n).
Now, the image of a linearly dependent set is dependent, so any set of size larger than n in the range
is dependent. (By the way, observe that the rank of g was not mentioned. See the next part.)
(b) Fix spaces and bases and consider the associated linear maps f and g. Recall that the dimension
of the image of a map (the map’s rank) is less than or equal to the dimension of the domain, and
consider the arrow diagram.
V
f
7−→
R(f)
g
7−→
R(g ◦f)
First, the image of R(f) must have dimension less than or equal to the dimension of R(f), by the
prior sentence. On the other hand, R(f) is a subset of the domain of g, and thus its image has

Answers to Exercises
323
dimension less than or equal the dimension of the domain of g. Combining those two, the rank of a
composition is less than or equal to the minimum of the two ranks.
The matrix fact follows immediately.
Three.IV.2.29
The ‘commutes with’ relation is reﬂexive and symmetric. However, it is not transi-
tive: for instance, with
G =
µ
1
2
3
4
¶
H =
µ
1
0
0
1
¶
J =
µ
5
6
7
8
¶
G commutes with H and H commutes with J, but G does not commute with J.
Three.IV.2.30
(a) Either of these.


x
y
z


πx
7−→


x
0
0


πy
7−→


0
0
0




x
y
z


πy
7−→


0
y
0


πx
7−→


0
0
0


(b) The composition is the ﬁfth derivative map d5/dx5 on the space of fourth-degree polynomials.
(c) With respect to the natural bases,
RepE3,E3(πx) =


1
0
0
0
0
0
0
0
0


RepE3,E3(πy) =


0
0
0
0
1
0
0
0
0


and their product (in either order) is the zero matrix.
(d) Where B = ⟨1, x, x2, x3, x4⟩,
RepB,B( d2
dx2 ) =






0
0
2
0
0
0
0
0
6
0
0
0
0
0
12
0
0
0
0
0
0
0
0
0
0






RepB,B( d3
dx3 ) =






0
0
0
6
0
0
0
0
0
24
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0






and their product (in either order) is the zero matrix.
Three.IV.2.31
Note that (S + T)(S −T) = S2 −ST + TS −T 2, so a reasonable try is to look at
matrices that do not commute so that −ST and TS don’t cancel: with
S =
µ
1
2
3
4
¶
T =
µ
5
6
7
8
¶
we have the desired inequality.
(S + T)(S −T) =
µ
−56
−56
−88
−88
¶
S2 −T 2 =
µ
−60
−68
−76
−84
¶
Three.IV.2.32
Because the identity map acts on the basis B as ⃗β1 7→⃗β1, . . . , ⃗βn 7→⃗βn, the represen-
tation is this.







1
0
0
0
0
1
0
0
0
0
1
0
...
0
0
0
1







The second part of the question is obvious from Theorem 2.6.
Three.IV.2.33
Here are four solutions.
T =
µ
±1
0
0
±1
¶
Three.IV.2.34
(a) The vector space M2×2 has dimension four. The set {T 4, . . . , T, I} has ﬁve ele-
ments and thus is linearly dependent.
(b) Where T is n×n, generalizing the argument from the prior item shows that there is such a
polynomial of degree n2 or less, since {T n2, . . . , T, I} is a n2+1-member subset of the n2-dimensional
space Mn×n.
(c) First compute the powers
T 2 =
µ
1/2
−
√
3/2
√
3/2
1/2
¶
T 3 =
µ
0
−1
1
0
¶
T 4 =
µ
−1/2
−
√
3/2
√
3/2
−1/2
¶

324
Linear Algebra, by Hefferon
(observe that rotating by π/6 three times results in a rotation by π/2, which is indeed what T 3
represents). Then set c4T 4 + c3T 3 + c2T 2 + c1T + c0I equal to the zero matrix
µ
−1/2
−
√
3/2
√
3/2
−1/2
¶
c4 +
µ
0
−1
1
0
¶
c3 +
µ
1/2
−
√
3/2
√
3/2
1/2
¶
c2 +
µ√
3/2
−1/2
1/2
√
3/2
¶
c1 +
µ
1
0
0
1
¶
c0
=
µ
0
0
0
0
¶
to get this linear system.
−(1/2)c4
+
(1/2)c2 + (
√
3/2)c1 + c0 = 0
−(
√
3/2)c4 −c3 −(
√
3/2)c2 −
(1/2)c1
= 0
(
√
3/2)c4 + c3 + (
√
3/2)c2 +
(1/2)c1
= 0
−(1/2)c4
+
(1/2)c2 + (
√
3/2)c1 + c0 = 0
Apply Gaussian reduction.
−ρ1+ρ4
−→
ρ2+ρ3
−→
−(1/2)c4
+
(1/2)c2 + (
√
3/2)c1 + c0 = 0
−(
√
3/2)c4 −c3 −(
√
3/2)c2 −
(1/2)c1
= 0
0 = 0
0 = 0
−
√
3ρ1+ρ2
−→
−(1/2)c4
+ (1/2)c2 + (
√
3/2)c1 +
c0 = 0
−c3 −
√
3c2 −
2c1 −
√
3c0 = 0
0 = 0
0 = 0
Setting c4, c3, and c2 to zero makes c1 and c0 also come out to be zero so no degree one or degree
zero polynomial will do. Setting c4 and c3 to zero (and c2 to one) gives a linear system
(1/2) + (
√
3/2)c1 +
c0 = 0
−
√
3 −
2c1 −
√
3c0 = 0
that can be solved with c1 = −
√
3 and c0 = 1. Conclusion: the polynomial m(x) = x2 −
√
3x + 1 is
minimal for the matrix T.
Three.IV.2.35
The check is routine:
a0 + a1x + · · · + anxn
s
7−→a0x + a1x2 + · · · + anxn+1 d/dx
7−→a0 + 2a1x + · · · + (n + 1)anxn
while
a0 + a1x + · · · + anxn d/dx
7−→a1 + · · · + nanxn−1
s
7−→a1x + · · · + anxn
so that under the map (d/dx ◦s) −(s ◦d/dx) we have a0 + a1x + · · · + anxn 7→a0 + a1x + · · · + anxn.
Three.IV.2.36
(a) Tracing through the remark at the end of the subsection gives that the i, j entry
of (FG)H is this
s
X
t=1
¡ r
X
k=1
fi,kgk,t
¢
ht,j =
s
X
t=1
r
X
k=1
(fi,kgk,t)ht,j =
s
X
t=1
r
X
k=1
fi,k(gk,tht,j)
=
r
X
k=1
s
X
t=1
fi,k(gk,tht,j) =
r
X
k=1
fi,k
¡ s
X
t=1
gk,tht,j
¢
(the ﬁrst equality comes from using the distributive law to multiply through the h’s, the second
equality is the associative law for real numbers, the third is the commutative law for reals, and the
fourth equality follows on using the distributive law to factor the f’s out), which is the i, j entry of
F(GH).
(b) The k-th component of h(⃗v) is
n
X
j=1
hk,jvj
and so the i-th component of g ◦h (⃗v) is this
r
X
k=1
gi,k
¡ n
X
j=1
hk,jvj
¢
=
r
X
k=1
n
X
j=1
gi,khk,jvj =
r
X
k=1
n
X
j=1
(gi,khk,j)vj
=
n
X
j=1
r
X
k=1
(gi,khk,j)vj =
n
X
j=1
(
r
X
k=1
gi,khk,j) vj

Answers to Exercises
325
(the ﬁrst equality holds by using the distributive law to multiply the g’s through, the second equality
represents the use of associativity of reals, the third follows by commutativity of reals, and the fourth
comes from using the distributive law to factor the v’s out).
Subsection Three.IV.3: Mechanics of Matrix Multiplication
Three.IV.3.23
(a) The second matrix has its ﬁrst row multiplied by 3 and its second row multiplied
by 0.
µ
3
6
0
0
¶
(b) The second matrix has its ﬁrst row multiplied by 4 and its second row multiplied by 2.
µ
4
8
6
8
¶
(c) The second matrix undergoes the pivot operation of replacing the second row with −2 times the
ﬁrst row added to the second.
µ
1
2
1
0
¶
(d) The ﬁrst matrix undergoes the column operation of: the second column is replaced by −1 times
the ﬁrst column plus the second.
µ
1
1
3
1
¶
(e) The ﬁrst matrix has its columns swapped.µ2
1
4
3
¶
Three.IV.3.24
(a) The incidence matrix is this (e.g, the ﬁrst row shows that there is only one
connection including Burlington, the road to Winooski).






0
0
0
0
1
0
0
1
1
1
0
1
0
1
0
0
1
1
0
0
1
1
0
0
0






(b) Because these are two-way roads, any road connecting city i to city j gives a connection between
city j and city i.
(c) The square of the incidence matrix tells how cities are connected by trips involving two roads.
Three.IV.3.25
The pay due each person appears in the matrix product of the two arrays.
Three.IV.3.26
The product is the identity matrix (recall that cos2 θ + sin2 θ = 1). An explanation
is that the given matrix represents, with respect to the standard bases, a rotation in R2 of θ radians
while the transpose represents a rotation of −θ radians. The two cancel.
Three.IV.3.27
The set of diagonal matrices is nonempty as the zero matrix is diagonal. Clearly it is
closed under scalar multiples and sums. Therefore it is a subspace. The dimension is n; here is a basis.
{





1
0
. . .
0
0
...
0
0
0




, . . . ,





0
0
. . .
0
0
...
0
0
1




}
Three.IV.3.28
No. In P1, with respect to the unequal bases B = ⟨1, x⟩and D = ⟨1 + x, 1 −x⟩, the
identity transformation is represented by by this matrix.
RepB,D(id) =
µ
1/2
1/2
1/2
−1/2
¶
B,D

326
Linear Algebra, by Hefferon
Three.IV.3.29
For any scalar r and square matrix H we have (rI)H = r(IH) = rH = r(HI) =
(Hr)I = H(rI).
There are no other such matrices; here is an argument for 2×2 matrices that is easily extended to
n×n. If a matrix commutes with all others then it commutes with this unit matrix.
µ
0
a
0
c
¶
=
µ
a
b
c
d
¶ µ
0
1
0
0
¶
=
µ
0
1
0
0
¶ µ
a
b
c
d
¶
=
µ
c
d
0
0
¶
From this we ﬁrst conclude that the upper left entry a must equal its lower right entry d. We also
conclude that the lower left entry c is zero. The argument for the upper right entry b is similar.
Three.IV.3.30
It is false; these two don’t commute.
µ
1
0
0
0
¶
µ
0
0
1
0
¶
Three.IV.3.31
A permutation matrix has a single one in each row and column, and all its other entries
are zeroes. Fix such a matrix. Suppose that the i-th row has its one in its j-th column. Then no
other row has its one in the j-th column; every other row has a zero in the j-th column. Thus the dot
product of the i-th row and any other row is zero.
The i-th row of the product is made up of the dot products of the i-th row of the matrix and the
columns of the transpose. By the last paragraph, all such dot products are zero except for the i-th
one, which is one.
Three.IV.3.32
The generalization is to go from the ﬁrst and second rows to the i1-th and i2-th rows.
Row i of GH is made up of the dot products of row i of G and the columns of H. Thus if rows i1 and
i2 of G are equal then so are rows i1 and i2 of GH.
Three.IV.3.33
If the product of two diagonal matrices is deﬁned — if both are n×n — then the product
of the diagonals is the diagonal of the products: where G, H are equal-sized diagonal matrices, GH is
all zeros except each that i, i entry is gi,ihi,i.
Three.IV.3.34
One way to produce this matrix from the identity is to use the column operations of
ﬁrst multiplying the second column by three, and then adding the negative of the resulting second
column to the ﬁrst.
µ
1
0
0
1
¶
−→
µ
1
0
0
3
¶
−→
µ
1
0
−3
3
¶
Column operations, in contrast with row operations) are written from left to right, so doing the above
two operations is expressed with this matrix product.
µ1
0
0
3
¶ µ 1
0
−1
1
¶
Remark.
Alternatively, we could get the required matrix with row operations.
Starting with the
identity, ﬁrst adding the negative of the ﬁrst row to the second, and then multiplying the second row
by three will work. Because successive row operations are written as matrix products from right to
left, doing these two row operations is expressed with: the same matrix product.
Three.IV.3.35
The i-th row of GH is made up of the dot products of the i-th row of G with the
columns of H. The dot product of a zero row with a column is zero.
It works for columns if stated correctly: if H has a column of zeros then GH (if deﬁned) has a
column of zeros. The proof is easy.
Three.IV.3.36
Perhaps the easiest way is to show that each n×m matrix is a linear combination of
unit matrices in one and only one way:
c1



1
0
. . .
0
0
...


+ · · · + cn,m



0
0
. . .
...
0
. . .
1


=



a1,1
a1,2
. . .
...
an,1
. . .
an,m



has the unique solution c1 = a1,1, c2 = a1,2, etc.
Three.IV.3.37
Call that matrix F. We have
F 2 =
µ
2
1
1
1
¶
F 3 =
µ
3
2
2
1
¶
F 4 =
µ
5
3
3
2
¶
In general,
F n =
µ
fn+1
fn
fn
fn−1
¶

Answers to Exercises
327
where fi is the i-th Fibonacci number fi = fi−1 + fi−2 and f0 = 0, f1 = 1, which is veriﬁed by
induction, based on this equation.
µ
fi−1
fi−2
fi−2
fi−3
¶ µ
1
1
1
0
¶
=
µ
fi
fi−1
fi−1
fi−2
¶
Three.IV.3.38
Chapter Five gives a less computational reason — the trace of a matrix is the second
coeﬃcient in its characteristic polynomial — but for now we can use indices. We have
trace(GH) = (g1,1h1,1 + g1,2h2,1 + · · · + g1,nhn,1)
+ (g2,1h1,2 + g2,2h2,2 + · · · + g2,nhn,2)
+ · · · + (gn,1h1,n + gn,2h2,n + · · · + gn,nhn,n)
while
trace(HG) = (h1,1g1,1 + h1,2g2,1 + · · · + h1,ngn,1)
+ (h2,1g1,2 + h2,2g2,2 + · · · + h2,ngn,2)
+ · · · + (hn,1g1,n + hn,2g2,n + · · · + hn,ngn,n)
and the two are equal.
Three.IV.3.39
A matrix is upper triangular if and only if its i, j entry is zero whenever i > j.
Thus, if G, H are upper triangular then hi,j and gi,j are zero when i > j. An entry in the product
pi,j = gi,1h1,j + · · · + gi,nhn,j is zero unless at least some of the terms are nonzero, that is, unless for
at least some of the summands gi,rhr,j both i ≤r and r ≤j. Of course, if i > j this cannot happen
and so the product of two upper triangular matrices is upper triangular. (A similar argument works
for lower triangular matrices.)
Three.IV.3.40
The sum along the i-th row of the product is this.
pi,1 + · · · + pi,n = (hi,1g1,1 + hi,2g2,1 + · · · + hi,ngn,1)
+ (hi,1g1,2 + hi,2g2,2 + · · · + hi,ngn,2)
+ · · · + (hi,1g1,n + hi,2g2,n + · · · + hi,ngn,n)
= hi,1(g1,1 + g1,2 + · · · + g1,n)
+ hi,2(g2,1 + g2,2 + · · · + g2,n)
+ · · · + hi,n(gn,1 + gn,2 + · · · + gn,n)
= hi,1 · 1 + · · · + hi,n · 1
= 1
Three.IV.3.41
Matrices representing (say, with respect to E2, E2 ⊂R2) the maps that send
⃗β1
h
7−→⃗β1
⃗β2
h
7−→⃗0
and
⃗β1
g
7−→⃗β2
⃗β2
g
7−→⃗0
will do.
Three.IV.3.42
The combination is to have all entries of the matrix be zero except for one (possibly)
nonzero entry in each row and column. Such a matrix can be written as the product of a permutation
matrix and a diagonal matrix, e.g.,


0
4
0
2
0
0
0
0
−5

=


0
1
0
1
0
0
0
0
1




4
0
0
0
2
0
0
0
−5


and its action is thus to rescale the rows and permute them.
Three.IV.3.43
(a) Each entry pi,j = gi,1h1,j + · · · + g1,rhr,1 takes r multiplications and there are
m · n entries. Thus there are m · n · r multiplications.
(b) Let H1 be 5×10, let H2 be 10×20, let H3 be 20×5, let H4 be 5×1. Then, using the formula
from the prior part,
this association
uses this many multiplications
((H1H2)H3)H4
1000 + 500 + 25 = 1525
(H1(H2H3))H4
1000 + 250 + 25 = 1275
(H1H2)(H3H4)
1000 + 100 + 100 = 1200
H1(H2(H3H4))
100 + 200 + 50 = 350
H1((H2H3)H4)
1000 + 50 + 50 = 1100

328
Linear Algebra, by Hefferon
shows which is cheapest.
(c) This is reported by Knuth as an improvement by S. Winograd of a formula due to V. Strassen:
where w = aA −(a −c −d)(A −C + D),
µ
a
b
c
d
¶ µ
A
B
C
D
¶
=
µ
aA + bB
w + (c + d)(C −A) + (a + b −c −d)D
w + (a −c)(D −C) −d(A −B −C + D)
w + (a −c)(D −C) + (c + d)(C −A)
¶
takes seven multiplications and ﬁfteen additions (save the intermediate results).
Three.IV.3.44
This is how the answer was given in the cited source. No, it does not. Let A and B
represent, with respect to the standard bases, these transformations of R3.


x
y
z


a
7−→


x
y
0




x
y
z


a
7−→


0
x
y


Observe that


x
y
z

abab
7−→


0
0
0


but


x
y
z

baba
7−→


0
0
x

.
Three.IV.3.45
This is how the answer was given in the cited source.
(a) Obvious.
(b) If AtransA⃗x = ⃗0 then ⃗y · ⃗y = 0 where ⃗y = A⃗x. Hence ⃗y = ⃗0 by (a).
The converse is obvious.
(c) By (b), A⃗x1,. . . ,A⃗xn are linearly independent iﬀAtransA⃗x1,. . . , AtransA⃗vn are linearly indepen-
dent.
(d) We have col rank(A) = col rank(AtransA) = dim {Atrans(A⃗x)
¯¯ all ⃗x} ≤dim {Atrans⃗y
¯¯ all ⃗y} =
col rank(Atrans). Thus also col rank(Atrans) ≤col rank(Atranstrans) and so we have col rank(A) =
col rank(Atrans) = row rank(A).
Three.IV.3.46
This is how the answer was given in the cited source. Let ⟨⃗z1, . . . ,⃗zk⟩be a basis for
R(A) ∩N (A) (k might be 0). Let ⃗x1, . . . , ⃗xk ∈V be such that A⃗xi = ⃗zi. Note {A⃗x1, . . . , A⃗xk}
is linearly independent, and extend to a basis for R(A): A⃗x1, . . . , A⃗xk, A⃗xk+1, . . . , A⃗xr1 where r1 =
dim(R(A)).
Now take ⃗x ∈V . Write
A⃗x = a1(A⃗x1) + · · · + ar1(A⃗xr1)
and so
A2⃗x = a1(A2⃗x1) + · · · + ar1(A2⃗xr1).
But A⃗x1, . . . , A⃗xk ∈N (A), so A2⃗x1 = ⃗0, . . . , A2⃗xk = ⃗0 and we now know
A2⃗xk+1, . . . , A2⃗xr1
spans R(A2).
To see {A2⃗xk+1, . . . , A2⃗xr1} is linearly independent, write
bk+1A2⃗xk+1 + · · · + br1A2⃗xr1 = ⃗0
A[bk+1A⃗xk+1 + · · · + br1A⃗xr1] = ⃗0
and, since bk+1A⃗xk+1 + · · · + br1A⃗xr1 ∈N (A) we get a contradiction unless it is ⃗0 (clearly it is in
R(A), but A⃗x1, . . . , A⃗xk is a basis for R(A) ∩N (A)).
Hence dim(R(A2)) = r1 −k = dim(R(A)) −dim(R(A) ∩N (A)).
Subsection Three.IV.4: Inverses

Answers to Exercises
329
Three.IV.4.13
Here is one way to proceed.
ρ1↔ρ2
−→


1
0
1
0
1
0
0
3
−1
1
0
0
1
−1
0
0
0
1

−ρ1+ρ3
−→


1
0
1
0
1
0
0
3
−1
1
0
0
0
−1
−1
0
−1
1


(1/3)ρ2+ρ3
−→


1
0
1
0
1
0
0
3
−1
1
0
0
0
0
−4/3
1/3
−1
1


(1/3)ρ2
−→
−(3/4)ρ3


1
0
1
0
1
0
0
1
−1/3
1/3
0
0
0
0
1
−1/4
3/4
−3/4


(1/3)ρ3+ρ2
−→
−ρ3+ρ1


1
0
0
1/4
1/4
3/4
0
1
0
1/4
1/4
−1/4
0
0
1
−1/4
3/4
−3/4


Three.IV.4.14
(a) Yes, it has an inverse: ad −bc = 2 · 1 −1 · (−1) ̸= 0.
(b) Yes.
(c) No.
Three.IV.4.15
(a)
1
2 · 1 −1 · (−1) ·
µ
1
−1
1
2
¶
= 1
3 ·
µ
1
−1
1
2
¶
=
µ
1/3
−1/3
1/3
2/3
¶
(b)
1
0 · (−3) −4 · 1 ·
µ
−3
−4
−1
0
¶
=
µ
3/4
1
1/4
0
¶
(c) The prior question shows that no inverse exists.
Three.IV.4.16
(a) The reduction is routine.
µ
3
1
1
0
0
2
0
1
¶
(1/3)ρ1
−→
(1/2)ρ2
µ
1
1/3
1/3
0
0
1
0
1/2
¶
−(1/3)ρ2+ρ1
−→
µ
1
0
1/3
−1/6
0
1
0
1/2
¶
This answer agrees with the answer from the check.
µ3
1
0
2
¶−1
=
1
3 · 2 −0 · 1 ·
µ2
−1
0
3
¶
= 1
6 ·
µ2
−1
0
3
¶
(b) This reduction is easy.
µ
2
1/2
1
0
3
1
0
1
¶
−(3/2)ρ1+ρ2
−→
µ
2
1/2
1
0
0
1/4
−3/2
1
¶
(1/2)ρ1
−→
4ρ2
µ
1
1/4
1/2
0
0
1
−6
4
¶
−(1/4)ρ2+ρ1
−→
µ
1
0
2
−1
0
1
−6
4
¶
The check agrees.
1
2 · 1 −3 · (1/2) ·
µ
1
−1/2
−3
2
¶
= 2 ·
µ
1
−1/2
−3
2
¶
(c) Trying the Gauss-Jordan reduction
µ
2
−4
1
0
−1
2
0
1
¶
(1/2)ρ1+ρ2
−→
µ
2
−4
1
0
0
0
1/2
1
¶
shows that the left side won’t reduce to the identity, so no inverse exists. The check ad −bc =
2 · 2 −(−4) · (−1) = 0 agrees.
(d) This produces an inverse.


1
1
3
1
0
0
0
2
4
0
1
0
−1
1
0
0
0
1

ρ1+ρ3
−→


1
1
3
1
0
0
0
2
4
0
1
0
0
2
3
1
0
1

−ρ2+ρ3
−→


1
1
3
1
0
0
0
2
4
0
1
0
0
0
−1
1
−1
1


(1/2)ρ2
−→
−ρ3


1
1
3
1
0
0
0
1
2
0
1/2
0
0
0
1
−1
1
−1

−2ρ3+ρ2
−→
−3ρ3+ρ1


1
1
0
4
−3
3
0
1
0
2
−3/2
2
0
0
1
−1
1
−1


−ρ2+ρ1
−→


1
0
0
2
−3/2
1
0
1
0
2
−3/2
2
0
0
1
−1
1
−1



330
Linear Algebra, by Hefferon
(e) This is one way to do the reduction.


0
1
5
1
0
0
0
−2
4
0
1
0
2
3
−2
0
0
1

ρ3↔ρ1
−→


2
3
−2
0
0
1
0
−2
4
0
1
0
0
1
5
1
0
0


(1/2)ρ2+ρ3
−→


2
3
−2
0
0
1
0
−2
4
0
1
0
0
0
7
1
1/2
0


(1/2)ρ1
−→
−(1/2)ρ2
(1/7)ρ3


1
3/2
−1
0
0
1/2
0
1
−2
0
−1/2
0
0
0
1
1/7
1/14
0


2ρ3+ρ2
−→
ρ3+ρ1


1
3/2
0
1/7
1/14
1/2
0
1
0
2/7
−5/14
0
0
0
1
1/7
1/14
0

−(3/2)ρ2+ρ1
−→


1
0
0
−2/7
17/28
1/2
0
1
0
2/7
−5/14
0
0
0
1
1/7
1/14
0


(f) There is no inverse.


2
2
3
1
0
0
1
−2
−3
0
1
0
4
−2
−3
0
0
1

−(1/2)ρ1+ρ2
−→
−2ρ1+ρ3


2
2
3
1
0
0
0
−3
−9/2
−1/2
1
0
0
−6
−9
−2
0
1


−2ρ2+ρ3
−→


2
2
3
1
0
0
0
−3
−9/2
−1/2
1
0
0
0
0
−1
−2
1


As a check, note that the third column of the starting matrix is 3/2 times the second, and so it is
indeed singular and therefore has no inverse.
Three.IV.4.17
We can use Corollary 4.12.
1
1 · 5 −2 · 3 ·
µ 5
−3
−2
1
¶
=
µ−5
3
2
−1
¶
Three.IV.4.18
(a) The proof that the inverse is r−1H−1 = (1/r) · H−1 (provided, of course, that
the matrix is invertible) is easy.
(b) No. For one thing, the fact that H + G has an inverse doesn’t imply that H has an inverse or
that G has an inverse. Neither of these matrices is invertible but their sum is.
µ
1
0
0
0
¶
µ
0
0
0
1
¶
Another point is that just because H and G each has an inverse doesn’t mean H +G has an inverse;
here is an example.
µ1
0
0
1
¶
µ−1
0
0
−1
¶
Still a third point is that, even if the two matrices have inverses, and the sum has an inverse, doesn’t
imply that the equation holds:
µ
2
0
0
2
¶−1
=
µ
1/2
0
0
1/2
¶−1
µ
3
0
0
3
¶−1
=
µ
1/3
0
0
1/3
¶−1
but
µ
5
0
0
5
¶−1
=
µ
1/5
0
0
1/5
¶−1
and (1/2)+(1/3) does not equal 1/5.
Three.IV.4.19
Yes: T k(T −1)k = (TT · · · T) · (T −1T −1 · · · T −1) = T k−1(TT −1)(T −1)k−1 = · · · = I.
Three.IV.4.20
Yes, the inverse of H−1 is H.
Three.IV.4.21
One way to check that the ﬁrst is true is with the angle sum formulas from trigonom-
etry.µ
cos(θ1 + θ2)
−sin(θ1 + θ2)
sin(θ1 + θ2)
cos(θ1 + θ2)
¶
=
µ
cos θ1 cos θ2 −sin θ1 sin θ2
−sin θ1 cos θ2 −cos θ1 sin θ2
sin θ1 cos θ2 + cos θ1 sin θ2
cos θ1 cos θ2 −sin θ1 sin θ2
¶
=
µ
cos θ1
−sin θ1
sin θ1
cos θ1
¶ µ
cos θ2
−sin θ2
sin θ2
cos θ2
¶
Checking the second equation in this way is similar.
Of course, the equations can be not just checked but also understood by recalling that tθ is the
map that rotates vectors about the origin through an angle of θ radians.

Answers to Exercises
331
Three.IV.4.22
There are two cases. For the ﬁrst case we assume that a is nonzero. Then
−(c/a)ρ1+ρ2
−→
µ
a
b
1
0
0
−(bc/a) + d
−c/a
1
¶
=
µ
a
b
1
0
0
(ad −bc)/a
−c/a
1
¶
shows that the matrix is invertible (in this a ̸= 0 case) if and only if ad −bc ̸= 0. To ﬁnd the inverse,
we ﬁnish with the Jordan half of the reduction.
(1/a)ρ1
−→
(a/ad−bc)ρ2
µ
1
b/a
1/a
0
0
1
−c/(ad −bc)
a/(ad −bc)
¶
−(b/a)ρ2+ρ1
−→
µ
1
0
d/(ad −bc)
−b/(ad −bc)
0
1
−c/(ad −bc)
a/(ad −bc)
¶
The other case is the a = 0 case. We swap to get c into the 1, 1 position.
ρ1↔ρ2
−→
µ
c
d
0
1
0
b
1
0
¶
This matrix is nonsingular if and only if both b and c are nonzero (which, under the case assumption
that a = 0, holds if and only if ad −bc ̸= 0). To ﬁnd the inverse we do the Jordan half.
(1/c)ρ1
−→
(1/b)ρ2
µ
1
d/c
0
1/c
0
1
1/b
0
¶
−(d/c)ρ2+ρ1
−→
µ
1
0
−d/bc
1/c
0
1
1/b
0
¶
(Note that this is what is required, since a = 0 gives that ad −bc = −bc).
Three.IV.4.23
With H a 2×3 matrix, in looking for a matrix G such that the combination HG acts
as the 2×2 identity we need G to be 3×2. Setting up the equation
µ1
0
1
0
1
0
¶ 

m
n
p
q
r
s

=
µ1
0
0
1
¶
and solving the resulting linear system
m
+r
= 1
n
+s = 0
p
= 0
q
= 1
gives inﬁnitely many solutions.
{








m
n
p
q
r
s








=








1
0
0
1
0
0








+ r ·








−1
0
0
0
1
0








+ s ·








0
−1
0
0
0
1








¯¯ r, s ∈R}
Thus H has inﬁnitely many right inverses.
As for left inverses, the equation
µa
b
c
d
¶ µ1
0
1
0
1
0
¶
=


1
0
0
0
1
0
0
0
1


gives rise to a linear system with nine equations and four unknowns.
a
= 1
b
= 0
a
= 0
c
= 0
d
= 1
c
= 0
e
= 0
f = 0
e
= 1
This system is inconsistent (the ﬁrst equation conﬂicts with the third, as do the seventh and ninth)
and so there is no left inverse.
Three.IV.4.24
With respect to the standard bases we have
RepE2,E3(η) =


1
0
0
1
0
0



332
Linear Algebra, by Hefferon
and setting up the equation to ﬁnd the matrix inverse
µa
b
c
d
e
f
¶ 

1
0
0
1
0
0

=
µ1
0
0
1
¶
= RepE2,E2(id)
gives rise to a linear system.
a
= 1
b
= 0
d
= 0
e = 1
There are inﬁnitely many solutions in a, . . . , f to this system because two of these variables are entirely
unrestricted
{








a
b
c
d
e
f








=








1
0
0
0
1
0








+ c ·








0
0
1
0
0
0








+ f ·








0
0
0
0
0
1








¯¯ c, f ∈R}
and so there are inﬁnitely many solutions to the matrix equation.
{
µ
1
0
c
0
1
f
¶ ¯¯ c, f ∈R}
With the bases still ﬁxed at E2, E2, for instance taking c = 2 and f = 3 gives a matrix representing
this map.


x
y
z


f2,3
7−→
µ
x + 2z
y + 3z
¶
The check that f2,3 ◦η is the identity map on R2 is easy.
Three.IV.4.25
By Lemma 4.3 it cannot have inﬁnitely many left inverses, because a matrix with both
left and right inverses has only one of each (and that one of each is one of both — the left and right
inverse matrices are equal).
Three.IV.4.26
The associativity of matrix multiplication gives on the one hand H−1(HG) = H−1Z =
Z, and on the other that H−1(HG) = (H−1H)G = IG = G.
Three.IV.4.27
Multiply both sides of the ﬁrst equation by H.
Three.IV.4.28
Checking that when I −T is multiplied on both sides by that expression (assuming
that T 4 is the zero matrix) then the result is the identity matrix is easy. The obvious generalization
is that if T n is the zero matrix then (I −T)−1 = I + T + T 2 + · · · + T n−1; the check again is easy.
Three.IV.4.29
The powers of the matrix are formed by taking the powers of the diagonal entries.
That is, D2 is all zeros except for diagonal entries of d1,1
2, d2,2
2, etc. This suggests deﬁning D0 to be
the identity matrix.
Three.IV.4.30
Assume that B is row equivalent to A and that A is invertible. Because they are
row-equivalent, there is a sequence of row steps to reduce one to the other. That reduction can be
done with matrices, for instance, A can be changed by row operations to B as B = Rn · · · R1A. This
equation gives B as a product of invertible matrices and by Lemma 4.5 then, B is also invertible.
Three.IV.4.31
(a) See the answer to Exercise 28.
(b) We will show that both conditions are equivalent to the condition that the two matrices be
nonsingular.
As T and S are square and their product is deﬁned, they are equal-sized, say n×n. Consider
the TS = I half. By the prior item the rank of I is less than or equal to the minimum of the rank
of T and the rank of S. But the rank of I is n, so the rank of T and the rank of S must each be n.
Hence each is nonsingular.
The same argument shows that ST = I implies that each is nonsingular.
Three.IV.4.32
Inverses are unique, so we need only show that it works. The check appears above as
Exercise 31.
Three.IV.4.33
(a) See the answer for Exercise 25.
(b) See the answer for Exercise 25.

Answers to Exercises
333
(c) Apply the ﬁrst part to I = AA−1 to get I = Itrans = (AA−1)
trans = (A−1)
transAtrans.
(d) Apply the prior item with Atrans = A, as A is symmetric.
Three.IV.4.34
For the answer to the items making up the ﬁrst half, see Exercise 30. For the proof
in the second half, assume that A is a zero divisor so there is a nonzero matrix B with AB = Z (or
else BA = Z; this case is similar), If A is invertible then A−1(AB) = (A−1A)B = IB = B but also
A−1(AB) = A−1Z = Z, contradicting that B is nonzero.
Three.IV.4.35
No, there are at least four.
µ
±1
0
0
±1
¶
Three.IV.4.36
It is not reﬂexive since, for instance,
H =
µ
1
0
0
2
¶
is not a two-sided inverse of itself. The same example shows that it is not transitive. That matrix has
this two-sided inverse
G =
µ
1
0
0
1/2
¶
and while H is a two-sided inverse of G and G is a two-sided inverse of H, we know that H is not
a two-sided inverse of H. However, the relation is symmetric: if G is a two-sided inverse of H then
GH = I = HG and therefore H is also a two-sided inverse of G.
Three.IV.4.37
This is how the answer was given in the cited source. Let A be m×m, non-singular,
with the stated property. Let B be its inverse. Then for n ≤m,
1 =
m
X
r=1
δnr =
m
X
r=1
m
X
s=1
bnsasr =
m
X
s=1
m
X
r=1
bnsasr = k
m
X
s=1
bns
(A is singular if k = 0).
Subsection Three.V.1: Changing Representations of Vectors
Three.V.1.6
For the matrix to change bases from D to E2 we need that RepE2(id(⃗δ1)) = RepE2(⃗δ1)
and that RepE2(id(⃗δ2)) = RepE2(⃗δ2). Of course, the representation of a vector in R2 with respect to
the standard basis is easy.
RepE2(⃗δ1) =
µ
2
1
¶
RepE2(⃗δ2) =
µ
−2
4
¶
Concatenating those two together to make the columns of the change of basis matrix gives this.
RepD,E2(id) =
µ
2
−2
1
4
¶
The change of basis matrix in the other direction can be gotten by calculating RepD(id(⃗e1)) = RepD(⃗e1)
and RepD(id(⃗e2)) = RepD(⃗e2) (this job is routine) or it can be found by taking the inverse of the above
matrix. Because of the formula for the inverse of a 2×2 matrix, this is easy.
RepE2,D(id) = 1
10 ·
µ
4
2
−1
2
¶
=
µ
4/10
2/10
−1/10
2/10
¶
Three.V.1.7
In each case, the columns RepD(id(⃗β1)) = RepD(⃗β1) and RepD(id(⃗β2)) = RepD(⃗β2) are
concatenated to make the change of basis matrix RepB,D(id).
(a)
µ
0
1
1
0
¶
(b)
µ
2
−1/2
−1
1/2
¶
(c)
µ
1
1
2
4
¶
(d)
µ
1
−1
−1
2
¶
Three.V.1.8
One way to go is to ﬁnd RepB(⃗δ1) and RepB(⃗δ2), and then concatenate them into the
columns of the desired change of basis matrix. Another way is to ﬁnd the inverse of the matrices that
answer Exercise 7.
(a)
µ0
1
1
0
¶
(b)
µ1
1
2
4
¶
(c)
µ 2
−1/2
−1
1/2
¶
(d)
µ2
1
1
1
¶
Three.V.1.9
The columns vector representations RepD(id(⃗β1)) = RepD(⃗β1), and RepD(id(⃗β2)) =
RepD(⃗β2), and RepD(id(⃗β3)) = RepD(⃗β3) make the change of basis matrix RepB,D(id).

334
Linear Algebra, by Hefferon
(a)


0
0
1
1
0
0
0
1
0


(b)


1
−1
0
0
1
−1
0
0
1


(c)


1
−1
1/2
1
1
−1/2
0
2
0


E.g., for the ﬁrst column of the ﬁrst matrix, 1 = 0 · x2 + 1 · 1 + 0 · x.
Three.V.1.10
A matrix changes bases if and only if it is nonsingular.
(a) This matrix is nonsingular and so changes bases. Finding to what basis E2 is changed means
ﬁnding D such that
RepE2,D(id) =
µ5
0
0
4
¶
and by the deﬁnition of how a matrix represents a linear map, we have this.
RepD(id(⃗e1)) = RepD(⃗e1) =
µ
5
0
¶
RepD(id(⃗e2)) = RepD(⃗e2) =
µ
0
4
¶
Where
D = ⟨
µx1
y1
¶
,
µx2
y2
¶
⟩
we can either solve the system
µ
1
0
¶
= 5
µ
x1
y1
¶
+ 0
µ
x2
y1
¶
µ
0
1
¶
= 0
µ
x1
y1
¶
+ 4
µ
x2
y1
¶
or else just spot the answer (thinking of the proof of Lemma 1.4).
D = ⟨
µ
1/5
0
¶
,
µ
0
1/4
¶
⟩
(b) Yes, this matrix is nonsingular and so changes bases. To calculate D, we proceed as above with
D = ⟨
µ
x1
y1
¶
,
µ
x2
y2
¶
⟩
to solve
µ
1
0
¶
= 2
µ
x1
y1
¶
+ 3
µ
x2
y1
¶
and
µ
0
1
¶
= 1
µ
x1
y1
¶
+ 1
µ
x2
y1
¶
and get this.
D = ⟨
µ
−1
3
¶
,
µ
1
−2
¶
⟩
(c) No, this matrix does not change bases because it is nonsingular.
(d) Yes, this matrix changes bases because it is nonsingular. The calculation of the changed-to basis
is as above.
D = ⟨
µ
1/2
−1/2
¶
,
µ
1/2
1/2
¶
⟩
Three.V.1.11
This question has many diﬀerent solutions. One way to proceed is to make up any basis
B for any space, and then compute the appropriate D (necessarily for the same space, of course).
Another, easier, way to proceed is to ﬁx the codomain as R3 and the codomain basis as E3. This way
(recall that the representation of any vector with respect to the standard basis is just the vector itself),
we have this.
B = ⟨


3
2
0

,


1
−1
0

,


4
1
4

⟩
D = E3
Three.V.1.12
Checking that B = ⟨2 sin(x) + cos(x), 3 cos(x)⟩is a basis is routine. Call the natural
basis D. To compute the change of basis matrix RepB,D(id) we must ﬁnd RepD(2 sin(x)+cos(x)) and
RepD(3 cos(x)), that is, we need x1, y1, x2, y2 such that these equations hold.
x1 · sin(x) + y1 · cos(x) = 2 sin(x) + cos(x)
x2 · sin(x) + y2 · cos(x) = 3 cos(x)
Obviously this is the answer.
RepB,D(id) =
µ
2
0
1
3
¶
For the change of basis matrix in the other direction we could look for RepB(sin(x)) and RepB(cos(x))
by solving these.
w1 · (2 sin(x) + cos(x)) + z1 · (3 cos(x)) = sin(x)
w2 · (2 sin(x) + cos(x)) + z2 · (3 cos(x)) = cos(x)

Answers to Exercises
335
An easier method is to ﬁnd the inverse of the matrix found above.
RepD,B(id) =
µ
2
0
1
3
¶−1
= 1
6 ·
µ
3
0
−1
2
¶
=
µ
1/2
0
−1/6
1/3
¶
Three.V.1.13
We start by taking the inverse of the matrix, that is, by deciding what is the inverse to
the map of interest.
RepD,E2(id)RepD,E2(id)−1 =
1
−cos2(2θ) −sin2(2θ) ·
µ
−cos(2θ)
−sin(2θ)
−sin(2θ)
cos(2θ)
¶
=
µ
cos(2θ)
sin(2θ)
sin(2θ)
−cos(2θ)
¶
This is more tractable than the representation the other way because this matrix is the concatenation
of these two column vectors
RepE2(⃗δ1) =
µ
cos(2θ)
sin(2θ)
¶
RepE2(⃗δ2) =
µ
sin(2θ)
−cos(2θ)
¶
and representations with respect to E2 are transparent.
⃗δ1 =
µ
cos(2θ)
sin(2θ)
¶
⃗δ2 =
µ
sin(2θ)
−cos(2θ)
¶
This pictures the action of the map that transforms D to E2 (it is, again, the inverse of the map that
is the answer to this question). The line lies at an angle θ to the x axis.
f
7−→
⃗δ1 =
³
cos(2θ)
sin(2θ)
´
⃗δ2 =
³
sin(2θ)
−cos(2θ)
´
⃗e1
⃗e2
This map reﬂects vectors over that line. Since reﬂections are self-inverse, the answer to the question
is: the original map reﬂects about the line through the origin with angle of elevation θ. (Of course, it
does this to any basis.)
Three.V.1.14
The appropriately-sized identity matrix.
Three.V.1.15
Each is true if and only if the matrix is nonsingular.
Three.V.1.16
What remains to be shown is that left multiplication by a reduction matrix represents
a change from another basis to B = ⟨⃗β1, . . . , ⃗βn⟩.
Application of a row-multiplication matrix Mi(k) translates a representation with respect to the
basis ⟨⃗β1, . . . , k⃗βi, . . . , ⃗βn⟩to one with respect to B, as here.
⃗v = c1 · ⃗β1 + · · · + ci · (k⃗βi) + · · · + cn · ⃗βn 7→c1 · ⃗β1 + · · · + (kci) · ⃗βi + · · · + cn · ⃗βn = ⃗v
Applying a row-swap matrix Pi,j translates a representation with respect to ⟨⃗β1, . . . , ⃗βj, . . . , ⃗βi, . . . , ⃗βn⟩
to one with respect to ⟨⃗β1, . . . , ⃗βi, . . . , ⃗βj, . . . , ⃗βn⟩. Finally, applying a row-combination matrix Ci,j(k)
changes a representation with respect to ⟨⃗β1, . . . , ⃗βi + k⃗βj, . . . , ⃗βj, . . . , ⃗βn⟩to one with respect to B.
⃗v = c1 · ⃗β1 + · · · + ci · (⃗βi + k⃗βj) + · · · + cj ⃗βj + · · · + cn · ⃗βn
7→c1 · ⃗β1 + · · · + ci · ⃗βi + · · · + (kci + cj) · ⃗βj + · · · + cn · ⃗βn = ⃗v
(As in the part of the proof in the body of this subsection, the various conditions on the row operations,
e.g., that the scalar k is nonzero, assure that these are all bases.)
Three.V.1.17
Taking H as a change of basis matrix H = RepB,En(id), its columns are



h1,i
...
hn,i


= RepEn(id(⃗βi)) = RepEn(⃗βi)
and, because representations with respect to the standard basis are transparent, we have this.



h1,i
...
hn,i


= ⃗βi
That is, the basis is the one composed of the columns of H.

336
Linear Algebra, by Hefferon
Three.V.1.18
(a) We can change the starting vector representation to the ending one through a
sequence of row operations. The proof tells us what how the bases change. We start by swapping
the ﬁrst and second rows of the representation with respect to B to get a representation with resepect
to a new basis B1.
RepB1(1 −x + 3x2 −x3) =




1
0
1
2




B1
B1 = ⟨1 −x, 1 + x, x2 + x3, x2 −x3⟩
We next add −2 times the third row of the vector representation to the fourth row.
RepB3(1 −x + 3x2 −x3) =




1
0
1
0




B2
B2 = ⟨1 −x, 1 + x, 3x2 −x3, x2 −x3⟩
(The third element of B2 is the third element of B1 minus −2 times the fourth element of B1.) Now
we can ﬁnish by doubling the third row.
RepD(1 −x + 3x2 −x3) =




1
0
2
0




D
D = ⟨1 −x, 1 + x, (3x2 −x3)/2, x2 −x3⟩
(b) Here are three diﬀerent approaches to stating such a result. The ﬁrst is the assertion: where
V is a vector space with basis B and ⃗v ∈V is nonzero, for any nonzero column vector ⃗z (whose
number of components equals the dimension of V ) there is a change of basis matrix M such that
M · RepB(⃗v) = ⃗z.
The second possible statement: for any (n-dimensional) vector space V and
any nonzero vector ⃗v ∈V , where ⃗z1,⃗z2 ∈Rn are nonzero, there are bases B, D ⊂V such that
RepB(⃗v) = ⃗z1 and RepD(⃗v) = ⃗z2. The third is: for any nonzero ⃗v member of any vector space (of
dimension n) and any nonzero column vector (with n components) there is a basis such that ⃗v is
represented with respect to that basis by that column vector.
The ﬁrst and second statements follow easily from the third. The ﬁrst follows because the third
statement gives a basis D such that RepD(⃗v) = ⃗z and then RepB,D(id) is the desired M. The second
follows from the third because it is just a doubled application of it.
A way to prove the third is as in the answer to the ﬁrst part of this question. Here is a sketch.
Represent ⃗v with respect to any basis B with a column vector ⃗z1. This column vector must have
a nonzero component because ⃗v is a nonzero vector. Use that component in a sequence of row
operations to convert ⃗z1 to ⃗z. (This sketch could be ﬁlled out as an induction argument on the
dimension of V .)
Three.V.1.19
This is the topic of the next subsection.
Three.V.1.20
A change of basis matrix is nonsingular and thus has rank equal to the number of its
columns. Therefore its set of columns is a linearly independent subset of size n in Rn and it is thus a
basis. The answer to the second half is also ‘yes’; all implications in the prior sentence reverse (that
is, all of the ‘if . . . then . . . ’ parts of the prior sentence convert to ‘if and only if’ parts).
Three.V.1.21
In response to the ﬁrst half of the question, there are inﬁnitely many such matrices.
One of them represents with respect to E2 the transformation of R2 with this action.
µ
1
0
¶
7→
µ
4
0
¶
µ
0
1
¶
7→
µ
0
−1/3
¶
The problem of specifying two distinct input/output pairs is a bit trickier. The fact that matrices have
a linear action precludes some possibilities.
(a) Yes, there is such a matrix. These conditions
µ
a
b
c
d
¶ µ
1
3
¶
=
µ
1
1
¶
µ
a
b
c
d
¶ µ
2
−1
¶
=
µ
−1
−1
¶
can be solved
a + 3b
=
1
c + 3d =
1
2a −b
= −1
2c −d = −1

Answers to Exercises
337
to give this matrix.
µ−2/7
3/7
−2/7
3/7
¶
(b) No, because
2 ·
µ1
3
¶
=
µ2
6
¶
but
2 ·
µ1
1
¶
̸=
µ−1
−1
¶
no linear action can produce this eﬀect.
(c) A suﬃcient condition is that {⃗v1,⃗v2} be linearly independent, but that’s not a necessary condition.
A necessary and suﬃcient condition is that any linear dependences among the starting vectors appear
also among the ending vectors. That is,
c1⃗v1 + c2⃗v2 = ⃗0
implies
c1 ⃗w1 + c2 ⃗w2 = ⃗0.
The proof of this condition is routine.
Subsection Three.V.2: Changing Map Representations
Three.V.2.10
(a) Yes, each has rank two.
(b) Yes, they have the same rank.
(c) No, they have diﬀerent ranks.
Three.V.2.11
We need only decide what the rank of each is.
(a)
µ
1
0
0
0
0
0
¶
(b)


1
0
0
0
0
1
0
0
0
0
1
0


Three.V.2.12
Recall the diagram and the formula.
R2
w.r.t. B
t
−−−−→
T
R2
w.r.t. D
id
y
id
y
R2
w.r.t. ˆ
B
t
−−−−→
ˆT
R2
w.r.t. ˆ
D
ˆT = RepD, ˆ
D(id) · T · Rep ˆ
B,B(id)
(a) These two
µ1
1
¶
= 1 ·
µ−1
0
¶
+ 1 ·
µ2
1
¶
µ 1
−1
¶
= (−3) ·
µ−1
0
¶
+ (−1) ·
µ2
1
¶
show that
RepD, ˆ
D(id) =
µ
1
−3
1
−1
¶
and similarly these twoµ
0
1
¶
= 0 ·
µ
1
0
¶
+ 1 ·
µ
0
1
¶
µ
1
1
¶
= 1 ·
µ
1
0
¶
+ 1 ·
µ
0
1
¶
give the other nonsinguar matrix.
Rep ˆ
B,B(id) =
µ
0
1
1
1
¶
Then the answer is this.
ˆT =
µ
1
−3
1
−1
¶ µ
1
2
3
4
¶ µ
0
1
1
1
¶
=
µ
−10
−18
−2
−4
¶
Although not strictly necessary, a check is reassuring. Arbitrarily ﬁxing
⃗v =
µ
3
2
¶
we have that
RepB(⃗v) =
µ
3
2
¶
B
µ
1
2
3
4
¶
B,D
µ
3
2
¶
B
=
µ
7
17
¶
D
and so t(⃗v) is this.
7 ·
µ
1
1
¶
+ 17 ·
µ
1
−1
¶
=
µ
24
−10
¶

338
Linear Algebra, by Hefferon
Doing the calculation with respect to ˆB, ˆD starts with
Rep ˆ
B(⃗v) =
µ−1
3
¶
ˆ
B
µ−10
−18
−2
−4
¶
ˆ
B, ˆ
D
µ−1
3
¶
ˆ
B
=
µ−44
−10
¶
ˆ
D
and then checks that this is the same result.
−44 ·
µ
−1
0
¶
−10 ·
µ
2
1
¶
=
µ
24
−10
¶
(b) These two
µ
1
1
¶
= 1
3 ·
µ
1
2
¶
+ 1
3 ·
µ
2
1
¶
µ
1
−1
¶
= −1 ·
µ
1
2
¶
+ 1 ·
µ
2
1
¶
show that
RepD, ˆ
D(id) =
µ
1/3
−1
1/3
1
¶
and these two
µ
1
2
¶
= 1 ·
µ
1
0
¶
+ 2 ·
µ
0
1
¶
µ
1
0
¶
= −1 ·
µ
1
0
¶
+ 0 ·
µ
0
1
¶
show this.
Rep ˆ
B,B(id) =
µ
1
1
2
0
¶
With those, the conversion goes in this way.
ˆT =
µ
1/3
−1
1/3
1
¶ µ
1
2
3
4
¶ µ
1
1
2
0
¶
=
µ
−28/3
−8/3
38/3
10/3
¶
As in the prior item, a check provides some conﬁdence that this calculation was performed without
mistakes. We can for instance, ﬁx the vector
⃗v =
µ
−1
2
¶
(this is selected for no reason, out of thin air). Now we have
RepB(⃗v) =
µ
−1
2
¶
µ
1
2
3
4
¶
B,D
µ
−1
2
¶
B
=
µ
3
5
¶
D
and so t(⃗v) is this vector.
3 ·
µ
1
1
¶
+ 5 ·
µ
1
−1
¶
=
µ
8
−2
¶
With respect to ˆB, ˆD we ﬁrst calculate
Rep ˆ
B(⃗v) =
µ 1
−2
¶
µ−28/3
−8/3
38/3
10/3
¶
ˆ
B, ˆ
D
µ 1
−2
¶
ˆ
B
=
µ−4
6
¶
ˆ
D
and, sure enough, that is the same result for t(⃗v).
−4 ·
µ
1
2
¶
+ 6 ·
µ
2
1
¶
=
µ
8
−2
¶
Three.V.2.13
Where H and ˆH are m×n, the matrix P is m×m while Q is n×n.
Three.V.2.14
Any n×n matrix is nonsingular if and only if it has rank n, that is, by Theorem 2.6, if
and only if it is matrix equivalent to the n×n matrix whose diagonal is all ones.
Three.V.2.15
If PAQ = I then QPAQ = Q, so QPA = I, and so QP = A−1.
Three.V.2.16
By the deﬁnition following Example 2.2, a matrix M is diagonalizable if it represents
M = RepB,D(t) a transformation with the property that there is some basis ˆB such that Rep ˆ
B, ˆ
B(t)
is a diagonal matrix — the starting and ending bases must be equal. But Theorem 2.6 says only that
there are ˆB and ˆD such that we can change to a representation Rep ˆ
B, ˆ
D(t) and get a diagonal matrix.
We have no reason to suspect that we could pick the two ˆB and ˆD so that they are equal.
Three.V.2.17
Yes. Row rank equals column rank, so the rank of the transpose equals the rank of the
matrix. Same-sized matrices with equal ranks are matrix equivalent.
Three.V.2.18
Only a zero matrix has rank zero.

Answers to Exercises
339
Three.V.2.19
For reﬂexivity, to show that any matrix is matrix equivalent to itself, take P and Q to
be identity matrices. For symmetry, if H1 = PH2Q then H2 = P −1H1Q−1 (inverses exist because P
and Q are nonsingular). Finally, for transitivity, assume that H1 = P2H2Q2 and that H2 = P3H3Q3.
Then substitution gives H1 = P2(P3H3Q3)Q2 = (P2P3)H3(Q3Q2). A product of nonsingular matrices
is nonsingular (we’ve shown that the product of invertible matrices is invertible; in fact, we’ve shown
how to calculate the inverse) and so H1 is therefore matrix equivalent to H3.
Three.V.2.20
By Theorem 2.6, a zero matrix is alone in its class because it is the only m×n of rank
zero. No other matrix is alone in its class; any nonzero scalar product of a matrix has the same rank
as that matrix.
Three.V.2.21
There are two matrix-equivalence classes of 1×1 matrices — those of rank zero and those
of rank one. The 3×3 matrices fall into four matrix equivalence classes.
Three.V.2.22
For m×n matrices there are classes for each possible rank: where k is the minimum
of m and n there are classes for the matrices of rank 0, 1, . . . , k. That’s k + 1 classes. (Of course,
totaling over all sizes of matrices we get inﬁnitely many classes.)
Three.V.2.23
They are closed under nonzero scalar multiplication, since a nonzero scalar multiple of
a matrix has the same rank as does the matrix. They are not closed under addition, for instance,
H + (−H) has rank zero.
Three.V.2.24
(a) We have
RepB,E2(id) =
µ1
−1
2
−1
¶
RepE2,B(id) = RepB,E2(id)−1 =
µ1
−1
2
−1
¶−1
=
µ−1
1
−2
1
¶
and thus the answer is this.
RepB,B(t) =
µ
1
−1
2
−1
¶ µ
1
1
3
−1
¶ µ
−1
1
−2
1
¶
=
µ
−2
0
−5
2
¶
As a quick check, we can take a vector at random
⃗v =
µ
4
5
¶
giving
RepE2(⃗v) =
µ4
5
¶
µ1
1
3
−1
¶ µ4
5
¶
=
µ9
7
¶
= t(⃗v)
while the calculation with respect to B, B
RepB(⃗v) =
µ 1
−3
¶
µ−2
0
−5
2
¶
B,B
µ 1
−3
¶
B
=
µ −2
−11
¶
B
yields the same result.
−2 ·
µ
1
2
¶
−11 ·
µ
−1
−1
¶
=
µ
9
7
¶
(b) We have
R2
w.r.t. E2
t
−−−−→
T
R2
w.r.t. E2
id
y
id
y
R2
w.r.t. B
t
−−−−→
ˆT
R2
w.r.t. B
RepB,B(t) = RepE2,B(id) · T · RepB,E2(id)
and, as in the ﬁrst item of this question
RepB,E2(id) =
³
⃗β1
· · ·
⃗βn
´
RepE2,B(id) = RepB,E2(id)−1
so, writing Q for the matrix whose columns are the basis vectors, we have that RepB,B(t) = Q−1TQ.
Three.V.2.25
(a) The adapted form of the arrow diagram is this.
Vw.r.t. B1
h
−−−−→
H
Ww.r.t. D
id
yQ
id
yP
Vw.r.t. B2
h
−−−−→
ˆ
H
Ww.r.t. D
Since there is no need to change bases in W (or we can say that the change of basis matrix P is the
identity), we have RepB2,D(h) = RepB1,D(h) · Q where Q = RepB2,B1(id).

340
Linear Algebra, by Hefferon
(b) Here, this is the arrow diagram.
Vw.r.t. B
h
−−−−→
H
Ww.r.t. D1
id
yQ
id
yP
Vw.r.t. B
h
−−−−→
ˆ
H
Ww.r.t. D2
We have that RepB,D2(h) = P · RepB,D1(h) where P = RepD1,D2(id).
Three.V.2.26
(a) Here is the arrow diagram, and a version of that diagram for inverse functions.
Vw.r.t. B
h
−−−−→
H
Ww.r.t. D
id
yQ
id
yP
Vw.r.t. ˆ
B
h
−−−−→
ˆ
H
Ww.r.t. ˆ
D
Vw.r.t. B
h−1
←−−−−
H−1
Ww.r.t. D
id
yQ
id
yP
Vw.r.t. ˆ
B
h−1
←−−−−
ˆ
H−1
Ww.r.t. ˆ
D
Yes, the inverses of the matrices represent the inverses of the maps. That is, we can move from the
lower right to the lower left by moving up, then left, then down. In other words, where ˆH = PHQ
(and P, Q invertible) and H, ˆH are invertible then ˆH−1 = Q−1H−1P −1.
(b) Yes; this is the prior part repeated in diﬀerent terms.
(c) No, we need another assumption: if H represents h with respect to the same starting as ending
bases B, B, for some B then H2 represents h◦h. As a speciﬁc example, these two matrices are both
rank one and so they are matrix equivalent
µ
1
0
0
0
¶
µ
0
0
1
0
¶
but the squares are not matrix equivalent — the square of the ﬁrst has rank one while the square of
the second has rank zero.
(d) No. These two are not matrix equivalent but have matrix equivalent squares.
µ
0
0
0
0
¶
µ
0
0
1
0
¶
Three.V.2.27
(a) The deﬁnition is suggested by the appropriate arrow diagram.
Vw.r.t. B1
t
−−−−→
T
Vw.r.t. B1
id
y
id
y
Vw.r.t. B2
t
−−−−→
ˆT
Vw.r.t. B2
Call matrices T, ˆT similar if there is a nonsingular matrix P such that ˆT = P −1TP.
(b) Take P −1 to be P and take P to be Q.
(c) This is as in Exercise 19. Reﬂexivity is obvious: T = I−1TI. Symmetry is also easy:
ˆT =
P −1TP implies that T = P ˆTP −1 (multiply the ﬁrst equation from the right by P −1 and from
the left by P).
For transitivity, assume that T1 = P2
−1T2P2 and that T2 = P3
−1T3P3.
Then
T1 = P2
−1(P3
−1T3P3)P2 = (P2
−1P3
−1)T3(P3P2) and we are ﬁnished on noting that P3P2 is an
invertible matrix with inverse P2
−1P3
−1.
(d) Assume that ˆT = P −1TP. For the squares:
ˆT 2 = (P −1TP)(P −1TP) = P −1T(PP −1)TP =
P −1T 2P. Higher powers follow by induction.
(e) These two are matrix equivalent but their squares are not matrix equivalent.
µ
1
0
0
0
¶
µ
0
0
1
0
¶
By the prior item, matrix similarity and matrix equivalence are thus diﬀerent.
Subsection Three.VI.1: Orthogonal Projection Into a Line
Three.VI.1.7
Each is a straightforward application of the formula from Deﬁnition 1.1.

Answers to Exercises
341
(a)
µ
2
1
¶ µ
3
−2
¶
µ 3
−2
¶ µ 3
−2
¶ ·
µ
3
−2
¶
= 4
13 ·
µ
3
−2
¶
=
µ
12/13
−8/13
¶
(b)
µ
2
1
¶ µ
3
0
¶
µ3
0
¶ µ3
0
¶ ·
µ
3
0
¶
= 2
3 ·
µ
3
0
¶
=
µ
2
0
¶
(c)


1
1
4




1
2
−1




1
2
−1




1
2
−1


·


1
2
−1

= −1
6 ·


1
2
−1

=


−1/6
−1/3
1/6


(d)


1
1
4




3
3
12




3
3
12




3
3
12


·


3
3
12

=
1
3 ·


3
3
12

=


1
1
4


Three.VI.1.8
(a)


2
−1
4




−3
1
−3




−3
1
−3




−3
1
−3


·


−3
1
−3

= −19
19 ·


−3
1
−3

=


3
−1
3


(b) Writing the line as {c ·
µ
1
3
¶ ¯¯ c ∈R} gives this projection.
µ
−1
−1
¶ µ
1
3
¶
µ
1
3
¶ µ
1
3
¶ ·
µ1
3
¶
= −4
10 ·
µ1
3
¶
=
µ−2/5
−6/5
¶
Three.VI.1.9




1
2
1
3








−1
1
−1
1








−1
1
−1
1








−1
1
−1
1




·




−1
1
−1
1



= 3
4 ·




−1
1
−1
1



=




−3/4
3/4
−3/4
3/4




Three.VI.1.10
(a)
µ
1
2
¶ µ
3
1
¶
µ3
1
¶ µ3
1
¶ ·
µ
3
1
¶
= 1
2 ·
µ
3
1
¶
=
µ
3/2
1/2
¶
(b)
µ
0
4
¶ µ
3
1
¶
µ3
1
¶ µ3
1
¶ ·
µ
3
1
¶
= 2
5 ·
µ
3
1
¶
=
µ6/5
2/5
¶
In general the projection is this.
µ
x1
x2
¶ µ
3
1
¶
µ
3
1
¶ µ
3
1
¶ ·
µ
3
1
¶
= 3x1 + x2
10
·
µ
3
1
¶
=
µ
(9x1 + 3x2)/10
(3x1 + x2)/10
¶
The appropriate matrix is this.
µ
9/10
3/10
3/10
1/10
¶
Three.VI.1.11
Suppose that ⃗v1 and ⃗v2 are nonzero and orthogonal. Consider the linear relationship
c1⃗v1 + c2⃗v2 = ⃗0. Take the dot product of both sides of the equation with ⃗v1 to get that
⃗v1 (c1⃗v1 + c2⃗v2) = c1 · (⃗v1 ⃗v1) + c2 · (⃗v1 ⃗v2) = c1 · (⃗v1 ⃗v1) + c2 · 0 = c1 · (⃗v1 ⃗v1)
is equal to ⃗v1 ⃗0 = ⃗0. With the assumption that ⃗v1 is nonzero, this gives that c1 is zero. Showing that
c2 is zero is similar.

342
Linear Algebra, by Hefferon
Three.VI.1.12
(a) If the vector ⃗v is in the line then the orthogonal projection is ⃗v. To verify this
by calculation, note that since ⃗v is in the line we have that ⃗v = c⃗v · ⃗s for some scalar c⃗v.
⃗v ⃗s
⃗s ⃗s · ⃗s = c⃗v · ⃗s ⃗s
⃗s ⃗s
· ⃗s = c⃗v · ⃗s ⃗s
⃗s ⃗s · ⃗s = c⃗v · 1 · ⃗s = ⃗v
(Remark. If we assume that ⃗v is nonzero then the above is simpliﬁed on taking ⃗s to be ⃗v.)
(b) Write c⃗p⃗s for the projection proj[⃗s ](⃗v). Note that, by the assumption that ⃗v is not in the line,
both ⃗v and ⃗v −c⃗p⃗s are nonzero. Note also that if c⃗p is zero then we are actually considering the
one-element set {⃗v }, and with ⃗v nonzero, this set is necessarily linearly independent. Therefore, we
are left considering the case that c⃗p is nonzero.
Setting up a linear relationship
a1(⃗v) + a2(⃗v −c⃗p⃗s) = ⃗0
leads to the equation (a1 + a2) ·⃗v = a2c⃗p ·⃗s. Because ⃗v isn’t in the line, the scalars a1 + a2 and a2c⃗p
must both be zero. The c⃗p = 0 case is handled above, so the remaining case is that a2 = 0, and this
gives that a1 = 0 also. Hence the set is linearly independent.
Three.VI.1.13
If ⃗s is the zero vector then the expression
proj[⃗s ](⃗v) = ⃗v ⃗s
⃗s ⃗s · ⃗s
contains a division by zero, and so is undeﬁned. As for the right deﬁnition, for the projection to lie in
the span of the zero vector, it must be deﬁned to be ⃗0.
Three.VI.1.14
Any vector in Rn is the projection of some other into a line, provided that the dimension
n is greater than one. (Clearly, any vector is the projection of itself into a line containing itself; the
question is to produce some vector other than ⃗v that projects to ⃗v.)
Suppose that ⃗v ∈Rn with n > 1. If ⃗v ̸= ⃗0 then we consider the line ℓ= {c⃗v
¯¯ c ∈R} and if ⃗v = ⃗0
we take ℓto be any (nondegenerate) line at all (actually, we needn’t distinguish between these two
cases — see the prior exercise). Let v1, . . . , vn be the components of ⃗v; since n > 1, there are at least
two. If some vi is zero then the vector ⃗w = ⃗ei is perpendicular to ⃗v. If none of the components is zero
then the vector ⃗w whose components are v2, −v1, 0, . . . , 0 is perpendicular to ⃗v. In either case, observe
that ⃗v + ⃗w does not equal ⃗v, and that ⃗v is the projection of ⃗v + ⃗w into ℓ.
(⃗v + ⃗w) ⃗v
⃗v ⃗v
· ⃗v =
¡⃗v ⃗v
⃗v ⃗v + ⃗w ⃗v
⃗v ⃗v
¢
· ⃗v = ⃗v ⃗v
⃗v ⃗v · ⃗v = ⃗v
We can dispose of the remaining n = 0 and n = 1 cases. The dimension n = 0 case is the trivial
vector space, here there is only one vector and so it cannot be expressed as the projection of a diﬀerent
vector. In the dimension n = 1 case there is only one (nondegenerate) line, and every vector is in it,
hence every vector is the projection only of itself.
Three.VI.1.15
The proof is simply a calculation.
∥⃗v ⃗s
⃗s ⃗s · ⃗s ∥= |⃗v ⃗s
⃗s ⃗s| · ∥⃗s ∥= |⃗v ⃗s |
∥⃗s ∥2 · ∥⃗s ∥= |⃗v ⃗s |
∥⃗s ∥
Three.VI.1.16
Because the projection of ⃗v into the line spanned by ⃗s is
⃗v ⃗s
⃗s ⃗s · ⃗s
the distance squared from the point to the line is this (a vector dotted with itself ⃗w ⃗w is written ⃗w2).
∥⃗v −⃗v ⃗s
⃗s ⃗s · ⃗s ∥2 = ⃗v ⃗v −⃗v (⃗v ⃗s
⃗s ⃗s · ⃗s) −(⃗v ⃗s
⃗s ⃗s · ⃗s ) ⃗v + (⃗v ⃗s
⃗s ⃗s · ⃗s )2
= ⃗v ⃗v −2 · (⃗v ⃗s
⃗s ⃗s) · ⃗v ⃗s + (⃗v ⃗s
⃗s ⃗s) · ⃗s ⃗s
= (⃗v ⃗v ) · (⃗s ⃗s ) −2 · (⃗v ⃗s )2 + (⃗v ⃗s )2
⃗s ⃗s
= (⃗v ⃗v )(⃗s ⃗s ) −(⃗v ⃗s )2
⃗s ⃗s
Three.VI.1.17
Because square root is a strictly increasing function, we can minimize d(c) = (cs1 −
v1)2+(cs2−v2)2 instead of the square root of d. The derivative is dd/dc = 2(cs1−v1)·s1+2(cs2−v2)·s2.
Setting it equal to zero 2(cs1 −v1) · s1 + 2(cs2 −v2) · s2 = c · (2s2
1 + 2s2
2) −(v1s1 + v2s2) = 0 gives the
only critical point.
c = v1s1 + v2s2
s12 + s22
= ⃗v ⃗s
⃗s ⃗s

Answers to Exercises
343
Now the second derivative with respect to c
d2 d
dc2 = 2s1
2 + 2s2
2
is strictly positive (as long as neither s1 nor s2 is zero, in which case the question is trivial) and so the
critical point is a minimum.
The generalization to Rn is straightforward. Consider dn(c) = (cs1 −v1)2 + · · · + (csn −vn)2, take
the derivative, etc.
Three.VI.1.18
The Cauchy-Schwartz inequality |⃗v ⃗s | ≤∥⃗v ∥· ∥⃗s ∥gives that this fraction
∥⃗v ⃗s
⃗s ⃗s · ⃗s ∥= |⃗v ⃗s
⃗s ⃗s| · ∥⃗s ∥= |⃗v ⃗s |
∥⃗s ∥2 · ∥⃗s ∥= |⃗v ⃗s |
∥⃗s ∥
when divided by ∥⃗v ∥is less than or equal to one. That is, ∥⃗v ∥is larger than or equal to the fraction.
Three.VI.1.19
Write c⃗s for ⃗q, and calculate: (⃗v c⃗s/c⃗s c⃗s ) · c⃗s = (⃗v ⃗s/⃗s ⃗s ) · ⃗s.
Three.VI.1.20
(a) Fixing
⃗s =
µ
1
1
¶
as the vector whose span is the line, the formula gives this action,
µ
x
y
¶
7→
µ
x
y
¶ µ
1
1
¶
µ1
1
¶ µ1
1
¶ ·
µ
1
1
¶
= x + y
2
·
µ
1
1
¶
=
µ
(x + y)/2
(x + y)/2
¶
which is the eﬀect of this matrix.
µ1/2
1/2
1/2
1/2
¶
(b) Rotating the entire plane π/4 radians clockwise brings the y = x line to lie on the x-axis. Now
projecting and then rotating back has the desired eﬀect.
Three.VI.1.21
The sequence need not settle down. With
⃗a =
µ
1
0
¶
⃗b =
µ
1
1
¶
the projections are these.
⃗v1 =
µ
1/2
1/2
¶
,
⃗v2 =
µ
1/2
0
¶
,
⃗v3 =
µ
1/4
1/4
¶
,
. . .
This sequence doesn’t repeat.
Subsection Three.VI.2: Gram-Schmidt Orthogonalization
Three.VI.2.9
(a)
⃗κ1 =
µ
1
1
¶
⃗κ2 =
µ
2
1
¶
−proj[⃗κ1](
µ
2
1
¶
) =
µ
2
1
¶
−
µ
2
1
¶ µ
1
1
¶
µ
1
1
¶ µ
1
1
¶ ·
µ
1
1
¶
=
µ
2
1
¶
−3
2 ·
µ
1
1
¶
=
µ
1/2
−1/2
¶

344
Linear Algebra, by Hefferon
(b)
⃗κ1 =
µ0
1
¶
⃗κ2 =
µ
−1
3
¶
−proj[⃗κ1](
µ
−1
3
¶
) =
µ
−1
3
¶
−
µ
−1
3
¶ µ
0
1
¶
µ
0
1
¶ µ
0
1
¶ ·
µ
0
1
¶
=
µ
−1
3
¶
−3
1 ·
µ
0
1
¶
=
µ
−1
0
¶
(c)
⃗κ1 =
µ0
1
¶
⃗κ2 =
µ
−1
0
¶
−proj[⃗κ1](
µ
−1
0
¶
) =
µ
−1
0
¶
−
µ
−1
0
¶ µ
0
1
¶
µ
0
1
¶ µ
0
1
¶ ·
µ
0
1
¶
=
µ
−1
0
¶
−0
1 ·
µ
0
1
¶
=
µ
−1
0
¶
The corresponding orthonormal bases for the three parts of this question are these.
⟨
µ
1/
√
2
1/
√
2
¶
,
µ √
2/2
−
√
2/2
¶
⟩
⟨
µ
0
1
¶
,
µ
−1
0
¶
⟩
⟨
µ
0
1
¶
,
µ
−1
0
¶
⟩
Three.VI.2.10
(a)
⃗κ1 =


2
2
2


⃗κ2 =


1
0
−1

−proj[⃗κ1](


1
0
−1

) =


1
0
−1

−


1
0
−1




2
2
2




2
2
2




2
2
2


·


2
2
2

=


1
0
−1

−0
12 ·


2
2
2

=


1
0
−1


⃗κ3 =


0
3
1

−proj[⃗κ1](


0
3
1

) −proj[⃗κ2](


0
3
1

) =


0
3
1

−


0
3
1




2
2
2




2
2
2




2
2
2


·


2
2
2

−


0
3
1




1
0
−1




1
0
−1




1
0
−1


·


1
0
−1


=


0
3
1

−8
12 ·


2
2
2

−−1
2 ·


1
0
−1

=


−5/6
5/3
−5/6



Answers to Exercises
345
(b)
⃗κ1 =


1
−1
0


⃗κ2 =


0
1
0

−proj[⃗κ1](


0
1
0

) =


0
1
0

−


0
1
0




1
−1
0




1
−1
0




1
−1
0


·


1
−1
0

=


0
1
0

−−1
2 ·


1
−1
0

=


1/2
1/2
0


⃗κ3 =


2
3
1

−proj[⃗κ1](


2
3
1

) −proj[⃗κ2](


2
3
1

)
=


2
3
1

−


2
3
1




1
−1
0




1
−1
0




1
−1
0


·


1
−1
0

−


2
3
1




1/2
1/2
0




1/2
1/2
0




1/2
1/2
0


·


1/2
1/2
0


=


2
3
1

−−1
2 ·


1
−1
0

−5/2
1/2 ·


1/2
1/2
0

=


0
0
1


The corresponding orthonormal bases for the two parts of this question are these.
⟨


1/
√
3
1/
√
3
1/
√
3

,


1/
√
2
0
−1/
√
2

,


−1/
√
6
2/
√
6
−1/
√
6

⟩
⟨


1/
√
2
−1/
√
2
0

,


1/
√
2
1/
√
2
0




0
0
1

⟩
Three.VI.2.11
The given space can be parametrized in this way.
{


x
y
z

¯¯ x = y −z} = {


1
1
0

· y +


−1
0
1

· z
¯¯ y, z ∈R}
So we take the basis
⟨


1
1
0

,


−1
0
1

⟩
apply the Gram-Schmidt process
⃗κ1 =


1
1
0


⃗κ2 =


−1
0
1

−proj[⃗κ1](


−1
0
1

) =


−1
0
1

−


−1
0
1




1
1
0




1
1
0




1
1
0


·


1
1
0

=


−1
0
1

−−1
2 ·


1
1
0

=


−1/2
1/2
1


and then normalize.
⟨


1/
√
2
1/
√
2
0

,


−1/
√
6
1/
√
6
2/
√
6

⟩
Three.VI.2.12
Reducing the linear system
x −y −z + w = 0
x
+ z
= 0
−ρ1+ρ2
−→
x −y −z + w = 0
y + 2z −w = 0

346
Linear Algebra, by Hefferon
and parametrizing gives this description of the subspace.
{




−1
−2
1
0



· z +




0
1
0
1



· w
¯¯ z, w ∈R}
So we take the basis,
⟨




−1
−2
1
0



,




0
1
0
1



⟩
go through the Gram-Schmidt process
⃗κ1 =




−1
−2
1
0




⃗κ2 =




0
1
0
1



−proj[⃗κ1](




0
1
0
1



) =




0
1
0
1



−




0
1
0
1








−1
−2
1
0








−1
−2
1
0








−1
−2
1
0




·




−1
−2
1
0



=




0
1
0
1



−−2
6 ·




−1
−2
1
0



=




−1/3
1/3
1/3
1




and ﬁnish by normalizing.
⟨




−1/
√
6
−2/
√
6
1/
√
6
0



,




−
√
3/6
√
3/6
√
3/6
√
3/2



⟩
Three.VI.2.13
A linearly independent subset of Rn is a basis for its own span. Apply Theorem 2.7.
Remark. Here’s why the phrase ‘linearly independent’ is in the question. Dropping the phrase
would require us to worry about two things. The ﬁrst thing to worry about is that when we do the
Gram-Schmidt process on a linearly dependent set then we get some zero vectors. For instance, with
S = {
µ
1
2
¶
,
µ
3
6
¶
}
we would get this.
⃗κ1 =
µ
1
2
¶
⃗κ2 =
µ
3
6
¶
−proj[⃗κ1](
µ
3
6
¶
) =
µ
0
0
¶
This ﬁrst thing is not so bad because the zero vector is by deﬁnition orthogonal to every other vector, so
we could accept this situation as yielding an orthogonal set (although it of course can’t be normalized),
or we just could modify the Gram-Schmidt procedure to throw out any zero vectors. The second thing
to worry about if we drop the phrase ‘linearly independent’ from the question is that the set might be
inﬁnite. Of course, any subspace of the ﬁnite-dimensional Rn must also be ﬁnite-dimensional so only
ﬁnitely many of its members are linearly independent, but nonetheless, a “process” that examines the
vectors in an inﬁnite set one at a time would at least require some more elaboration in this question.
A linearly independent subset of Rn is automatically ﬁnite — in fact, of size n or less — so the ‘linearly
independent’ phrase obviates these concerns.
Three.VI.2.14
The process leaves the basis unchanged.
Three.VI.2.15
(a) The argument is as in the i = 3 case of the proof of Theorem 2.7. The dot
product
⃗κi
³
⃗v −proj[⃗κ1](⃗v ) −· · · −proj[⃗vk](⃗v )
´
can be written as the sum of terms of the form −⃗κi proj[⃗κj](⃗v ) with j ̸= i, and the term ⃗κi (⃗v −
proj[⃗κi](⃗v )). The ﬁrst kind of term equals zero because the ⃗κ’s are mutually orthogonal. The other
term is zero because this projection is orthogonal (that is, the projection deﬁnition makes it zero:
⃗κi (⃗v −proj[⃗κi](⃗v )) = ⃗κi ⃗v −⃗κi ((⃗v ⃗κi)/(⃗κi ⃗κi)) · ⃗κi equals, after all of the cancellation is done,
zero).

Answers to Exercises
347
(b) The vector ⃗v is shown in black and the vector proj[⃗κ1](⃗v ) + proj[⃗v2](⃗v ) = 1 · ⃗e1 + 2 · ⃗e2 is in gray.
The vector ⃗v −(proj[⃗κ1](⃗v ) + proj[⃗v2](⃗v )) lies on the dotted line connecting the black vector to the
gray one, that is, it is orthogonal to the xy-plane.
(c) This diagram is gotten by following the hint.
The dashed triangle has a right angle where the gray vector 1 · ⃗e1 + 2 · ⃗e2 meets the vertical dashed
line ⃗v −(1 · ⃗e1 + 2 · ⃗e2); this is what was proved in the ﬁrst item of this question. The Pythagorean
theorem then gives that the hypoteneuse — the segment from ⃗v to any other vector — is longer than
the vertical dashed line.
More formally, writing proj[⃗κ1](⃗v ) + · · · + proj[⃗vk](⃗v ) as c1 · ⃗κ1 + · · · + ck · ⃗κk, consider any other
vector in the span d1 · ⃗κ1 + · · · + dk · ⃗κk. Note that
⃗v −(d1 · ⃗κ1 + · · · + dk · ⃗κk)
=
¡
⃗v −(c1 · ⃗κ1 + · · · + ck · ⃗κk)
¢
+
¡
(c1 · ⃗κ1 + · · · + ck · ⃗κk) −(d1 · ⃗κ1 + · · · + dk · ⃗κk)
¢
and that
¡
⃗v −(c1 · ⃗κ1 + · · · + ck · ⃗κk)
¢ ¡
(c1 · ⃗κ1 + · · · + ck · ⃗κk) −(d1 · ⃗κ1 + · · · + dk · ⃗κk)
¢
= 0
(because the ﬁrst item shows the ⃗v −(c1 · ⃗κ1 + · · · + ck · ⃗κk) is orthogonal to each ⃗κ and so it is
orthogonal to this linear combination of the ⃗κ’s). Now apply the Pythagorean Theorem (i.e., the
Triangle Inequality).
Three.VI.2.16
One way to proceed is to ﬁnd a third vector so that the three together make a basis
for R3, e.g.,
⃗β3 =


1
0
0


(the second vector is not dependent on the third because it has a nonzero second component, and the
ﬁrst is not dependent on the second and third because of its nonzero third component), and then apply

348
Linear Algebra, by Hefferon
the Gram-Schmidt process.
⃗κ1 =


1
5
−1


⃗κ2 =


2
2
0

−proj[⃗κ1](


2
2
0

) =


2
2
0

−


2
2
0




1
5
−1




1
5
−1




1
5
−1


·


1
5
−1


=


2
2
0

−12
27 ·


1
5
−1

=


14/9
−2/9
4/9


⃗κ3 =


1
0
0

−proj[⃗κ1](


1
0
0

) −proj[⃗κ2](


1
0
0

)
=


1
0
0

−


1
0
0




1
5
−1




1
5
−1




1
5
−1


·


1
5
−1

−


1
0
0




14/9
−2/9
4/9




14/9
−2/9
4/9




14/9
−2/9
4/9


·


14/9
−2/9
4/9


=


1
0
0

−1
27 ·


1
5
−1

−7
12 ·


14/9
−2/9
4/9

=


1/18
−1/18
−4/18


The result ⃗κ3 is orthogonal to both ⃗κ1 and ⃗κ2. It is therefore orthogonal to every vector in the span
of the set {⃗κ1,⃗κ2}, including the two vectors given in the question.
Three.VI.2.17
(a) The representation can be done by eye.
µ
2
3
¶
= 3 ·
µ
1
1
¶
+ (−1) ·
µ
1
0
¶
RepB(⃗v ) =
µ
3
−1
¶
B
The two projections are also easy.
proj[⃗β1](
µ2
3
¶
) =
µ
2
3
¶ µ
1
1
¶
µ
1
1
¶ µ
1
1
¶ ·
µ1
1
¶
= 5
2 ·
µ1
1
¶
proj[⃗β2](
µ2
3
¶
) =
µ
2
3
¶ µ
1
0
¶
µ
1
0
¶ µ
1
0
¶ ·
µ1
0
¶
= 2
1 ·
µ1
0
¶
(b) As above, the representation can be done by eye
µ
2
3
¶
= (5/2) ·
µ
1
1
¶
+ (−1/2) ·
µ
1
−1
¶
and the two projections are easy.
proj[⃗β1](
µ
2
3
¶
) =
µ
2
3
¶ µ
1
1
¶
µ
1
1
¶ µ
1
1
¶·
µ
1
1
¶
= 5
2·
µ
1
1
¶
proj[⃗β2](
µ
2
3
¶
) =
µ
2
3
¶ µ
1
−1
¶
µ
1
−1
¶ µ
1
−1
¶·
µ
1
−1
¶
= −1
2 ·
µ
1
−1
¶
Note the recurrence of the 5/2 and the −1/2.
(c) Represent ⃗v with respect to the basis
RepK(⃗v ) =



r1
...
rk



so that ⃗v = r1⃗κ1 + · · · + rk⃗κk. To determine ri, take the dot product of both sides with ⃗κi.
⃗v ⃗κi = (r1⃗κ1 + · · · + rk⃗κk) ⃗κi = r1 · 0 + · · · + ri · (⃗κi ⃗κi) + · · · + rk · 0
Solving for ri yields the desired coeﬃcient.
(d) This is a restatement of the prior item.
Three.VI.2.18
First, ∥⃗v ∥2 = 42 + 32 + 22 + 12 = 50.

Answers to Exercises
349
(a) c1 = 4
(b) c1 = 4, c2 = 3
(c) c1 = 4, c2 = 3, c3 = 2, c4 = 1
For the proof, we will do only the k = 2 case because the completely general case is messier but no
more enlightening. We follow the hint (recall that for any vector ⃗w we have ∥⃗w ∥2 = ⃗w
⃗w).
0 ≤
µ
⃗v −
¡ ⃗v ⃗κ1
⃗κ1 ⃗κ1
· ⃗κ1 + ⃗v ⃗κ2
⃗κ2 ⃗κ2
· ⃗κ2
¢¶ µ
⃗v −
¡ ⃗v ⃗κ1
⃗κ1 ⃗κ1
· ⃗κ1 + ⃗v ⃗κ2
⃗κ2 ⃗κ2
· ⃗κ2
¢¶
= ⃗v ⃗v −2 · ⃗v
µ ⃗v ⃗κ1
⃗κ1 ⃗κ1
· ⃗κ1 + ⃗v ⃗κ2
⃗κ2 ⃗κ2
· ⃗κ2
¶
+
µ ⃗v ⃗κ1
⃗κ1 ⃗κ1
· ⃗κ1 + ⃗v ⃗κ2
⃗κ2 ⃗κ2
· ⃗κ2
¶ µ ⃗v ⃗κ1
⃗κ1 ⃗κ1
· ⃗κ1 + ⃗v ⃗κ2
⃗κ2 ⃗κ2
· ⃗κ2
¶
= ⃗v ⃗v −2 ·
µ ⃗v ⃗κ1
⃗κ1 ⃗κ1
· (⃗v ⃗κ1) + ⃗v ⃗κ2
⃗κ2 ⃗κ2
· (⃗v ⃗κ2)
¶
+
µ
( ⃗v ⃗κ1
⃗κ1 ⃗κ1
)2 · (⃗κ1 ⃗κ1) + ( ⃗v ⃗κ2
⃗κ2 ⃗κ2
)2 · (⃗κ2 ⃗κ2)
¶
(The two mixed terms in the third part of the third line are zero because ⃗κ1 and ⃗κ2 are orthogonal.)
The result now follows on gathering like terms and on recognizing that ⃗κ1
⃗κ1 = 1 and ⃗κ2
⃗κ2 = 1
because these vectors are given as members of an orthonormal set.
Three.VI.2.19
It is true, except for the zero vector. Every vector in Rn except the zero vector is in a
basis, and that basis can be orthogonalized.
Three.VI.2.20
The 3×3 case gives the idea. The set
{


a
d
g

,


b
e
h

,


c
f
i

}
is orthonormal if and only if these nine conditions all hold
¡a
d
g¢


a
d
g

= 1
¡a
d
g¢


b
e
h

= 0
¡a
d
g¢


c
f
i

= 0
¡
b
e
h
¢


a
d
g

= 0
¡
b
e
h
¢


b
e
h

= 1
¡
b
e
h
¢


c
f
i

= 0
¡
c
f
i
¢


a
d
g

= 0
¡
c
f
i
¢


b
e
h

= 0
¡
c
f
i
¢


c
f
i

= 1
(the three conditions in the lower left are redundant but nonetheless correct). Those, in turn, hold if
and only if


a
d
g
b
e
h
c
f
i




a
b
c
d
e
f
g
h
i

=


1
0
0
0
1
0
0
0
1


as required.
This is an example, the inverse of this matrix is its transpose.


1/
√
2
1/
√
2
0
−1/
√
2
1/
√
2
0
0
0
1


Three.VI.2.21
If the set is empty then the summation on the left side is the linear combination of the
empty set of vectors, which by deﬁnition adds to the zero vector. In the second sentence, there is not
such i, so the ‘if . . . then . . . ’ implication is vacuously true.
Three.VI.2.22
(a) Part of the induction argument proving Theorem 2.7 checks that ⃗κi is in the
span of ⟨⃗β1, . . . , ⃗βi⟩. (The i = 3 case in the proof illustrates.) Thus, in the change of basis matrix
RepK,B(id), the i-th column RepB(⃗κi) has components i + 1 through k that are zero.
(b) One way to see this is to recall the computational procedure that we use to ﬁnd the inverse. We
write the matrix, write the identity matrix next to it, and then we do Gauss-Jordan reduction. If the
matrix starts out upper triangular then the Gauss-Jordan reduction involves only the Jordan half
and these steps, when performed on the identity, will result in an upper triangular inverse matrix.
Three.VI.2.23
For the inductive step, we assume that for all j in [1..i], these three conditions are true
of each ⃗κj: (i) each ⃗κj is nonzero, (ii) each ⃗κj is a linear combination of the vectors ⃗β1, . . . , ⃗βj, and

350
Linear Algebra, by Hefferon
(iii) each ⃗κj is orthogonal to all of the ⃗κm’s prior to it (that is, with m < j). With those inductive
hypotheses, consider ⃗κi+1.
⃗κi+1 = ⃗βi+1 −proj[⃗κ1](βi+1) −proj[⃗κ2](βi+1) −· · · −proj[⃗κi](βi+1)
= ⃗βi+1 −βi+1 ⃗κ1
⃗κ1 ⃗κ1
· ⃗κ1 −βi+1 ⃗κ2
⃗κ2 ⃗κ2
· ⃗κ2 −· · · −βi+1 ⃗κi
⃗κi ⃗κi
· ⃗κi
By the inductive assumption (ii) we can expand each ⃗κj into a linear combination of ⃗β1, . . . , ⃗βj
= ⃗βi+1 −
⃗βi+1 ⃗κ1
⃗κ1 ⃗κ1
· ⃗β1
−
⃗βi+1 ⃗κ2
⃗κ2 ⃗κ2
·
³
linear combination of ⃗β1, ⃗β2
´
−· · · −
⃗βi+1 ⃗κi
⃗κi ⃗κi
·
³
linear combination of ⃗β1, . . . , ⃗βi
´
The fractions are scalars so this is a linear combination of linear combinations of ⃗β1, . . . , ⃗βi+1. It is
therefore just a linear combination of ⃗β1, . . . , ⃗βi+1. Now, (i) it cannot sum to the zero vector because
the equation would then describe a nontrivial linear relationship among the ⃗β’s that are given as
members of a basis (the relationship is nontrivial because the coeﬃcient of ⃗βi+1 is 1). Also, (ii) the
equation gives ⃗κi+1 as a combination of ⃗β1, . . . , ⃗βi+1. Finally, for (iii), consider ⃗κj ⃗κi+1; as in the i = 3
case, the dot product of ⃗κj with ⃗κi+1 = ⃗βi+1 −proj[⃗κ1](⃗βi+1) −· · · −proj[⃗κi](⃗βi+1) can be rewritten to
give two kinds of terms, ⃗κj
³
⃗βi+1 −proj[⃗κj](⃗βi+1)
´
(which is zero because the projection is orthogonal)
and ⃗κj
proj[⃗κm](⃗βi+1) with m ̸= j and m < i + 1 (which is zero because by the hypothesis (iii) the
vectors ⃗κj and ⃗κm are orthogonal).
Subsection Three.VI.3: Projection Into a Subspace
Three.VI.3.10
(a) When bases for the subspaces
BM = ⟨
µ
1
−1
¶
⟩
BN = ⟨
µ
2
−1
¶
⟩
are concatenated
B = BM
⌢BN = ⟨
µ
1
−1
¶
,
µ
2
−1
¶
⟩
and the given vector is representedµ
3
−2
¶
= 1 ·
µ
1
−1
¶
+ 1 ·
µ
2
−1
¶
then the answer comes from retaining the M part and dropping the N part.
projM,N(
µ
3
−2
¶
) =
µ
1
−1
¶
(b) When the bases
BM = ⟨
µ
1
1
¶
⟩
BN⟨
µ
1
−2
¶
⟩
are concatenated, and the vector is represented,
µ
1
2
¶
= (4/3) ·
µ
1
1
¶
−(1/3) ·
µ
1
−2
¶
then retaining only the M part gives this answer.
projM,N(
µ
1
2
¶
) =
µ
4/3
4/3
¶
(c) With these bases
BM = ⟨


1
−1
0

,


0
0
1

⟩
BN = ⟨


1
0
1

⟩

Answers to Exercises
351
the representation with respect to the concatenation is this.


3
0
1

= 0 ·


1
−1
0

−2 ·


0
0
1

+ 3 ·


1
0
1


and so the projection is this.
projM,N(


3
0
1

) =


0
0
−2


Three.VI.3.11
As in Example 3.5, we can simplify the calculation by just ﬁnding the space of vectors
perpendicular to all the the vectors in M’s basis.
(a) Parametrizing to get
M = {c ·
µ
−1
1
¶ ¯¯ c ∈R}
gives that
M ⊥{
µ
u
v
¶ ¯¯ 0 =
µ
u
v
¶ µ
−1
1
¶
} = {
µ
u
v
¶ ¯¯ 0 = −u + v}
Parametrizing the one-equation linear system gives this description.
M ⊥= {k ·
µ
1
1
¶ ¯¯ k ∈R}
(b) As in the answer to the prior part, M can be described as a span
M = {c ·
µ3/2
1
¶ ¯¯ c ∈R}
BM = ⟨
µ3/2
1
¶
⟩
and then M ⊥is the set of vectors perpendicular to the one vector in this basis.
M ⊥= {
µu
v
¶ ¯¯ (3/2) · u + 1 · v = 0} = {k ·
µ−2/3
1
¶ ¯¯ k ∈R}
(c) Parametrizing the linear requirement in the description of M gives this basis.
M = {c ·
µ
1
1
¶ ¯¯ c ∈R}
BM = ⟨
µ
1
1
¶
⟩
Now, M ⊥is the set of vectors perpendicular to (the one vector in) BM.
M ⊥= {
µu
v
¶ ¯¯ u + v = 0} = {k ·
µ−1
1
¶ ¯¯ k ∈R}
(By the way, this answer checks with the ﬁrst item in this question.)
(d) Every vector in the space is perpendicular to the zero vector so M ⊥= Rn.
(e) The appropriate description and basis for M are routine.
M = {y ·
µ0
1
¶ ¯¯ y ∈R}
BM = ⟨
µ0
1
¶
⟩
Then
M ⊥= {
µ
u
v
¶ ¯¯ 0 · u + 1 · v = 0} = {k ·
µ
1
0
¶ ¯¯ k ∈R}
and so (y-axis)⊥= x-axis.
(f) The description of M is easy to ﬁnd by parametrizing.
M = {c ·


3
1
0

+ d ·


1
0
1

¯¯ c, d ∈R}
BM = ⟨


3
1
0

,


1
0
1

⟩
Finding M ⊥here just requires solving a linear system with two equations
3u + v
= 0
u
+ w = 0
−(1/3)ρ1+ρ2
−→
3u +
v
= 0
−(1/3)v + w = 0
and parametrizing.
M ⊥= {k ·


−1
3
1

¯¯ k ∈R}

352
Linear Algebra, by Hefferon
(g) Here, M is one-dimensional
M = {c ·


0
−1
1

¯¯ c ∈R}
BM = ⟨


0
−1
1

⟩
and as a result, M ⊥is two-dimensional.
M ⊥= {


u
v
w

¯¯ 0 · u −1 · v + 1 · w = 0} = {j ·


1
0
0

+ k ·


0
1
1

¯¯ j, k ∈R}
Three.VI.3.12
(a) Parametrizing the equation leads to this basis for P.
BP = ⟨


1
0
3

,


0
1
2

⟩
(b) Because R3 is three-dimensional and P is two-dimensional, the complement P ⊥must be a line.
Anyway, the calculation as in Example 3.5
P ⊥= {


x
y
z

¯¯
µ
1
0
3
0
1
2
¶ 

x
y
z

=
µ
0
0
¶
}
gives this basis for P ⊥.
BP ⊥= ⟨


3
2
−1

⟩
(c)


1
1
2

= (5/14) ·


1
0
3

+ (8/14) ·


0
1
2

+ (3/14) ·


3
2
−1


(d) projP (


1
1
2

) =


5/14
8/14
31/14


(e) The matrix of the projection


1
0
0
1
3
2

¡µ1
0
3
0
1
2
¶ 

1
0
0
1
3
2

¢−1
µ1
0
3
0
1
2
¶
=


1
0
0
1
3
2


µ10
6
6
5
¶−1 µ1
0
3
0
1
2
¶
= 1
14


5
−6
3
−6
10
2
3
2
13


when applied to the vector, yields the expected result.
1
14


5
−6
3
−6
10
2
3
2
13




1
1
2

=


5/14
8/14
31/14


Three.VI.3.13
(a) Parametrizing gives this.
M = {c ·
µ−1
1
¶ ¯¯ c ∈R}
For the ﬁrst way, we take the vector spanning the line M to be
⃗s =
µ−1
1
¶
and the Deﬁnition 1.1 formula gives this.
proj[⃗s ](
µ
1
−3
¶
) =
µ 1
−3
¶ µ−1
1
¶
µ
−1
1
¶ µ
−1
1
¶ ·
µ
−1
1
¶
= −4
2 ·
µ
−1
1
¶
=
µ
2
−2
¶
For the second way, we ﬁx
BM = ⟨
µ
−1
1
¶
⟩

Answers to Exercises
353
and so (as in Example 3.5 and 3.6, we can just ﬁnd the vectors perpendicular to all of the members
of the basis)
M ⊥= {
µ
u
v
¶ ¯¯ −1 · u + 1 · v = 0} = {k ·
µ
1
1
¶ ¯¯ k ∈R}
BM ⊥= ⟨
µ
1
1
¶
⟩
and representing the vector with respect to the concatenation gives this.
µ
1
−3
¶
= −2 ·
µ
−1
1
¶
−1 ·
µ
1
1
¶
Keeping the M part yields the answer.
projM,M ⊥(
µ
1
−3
¶
) =
µ
2
−2
¶
The third part is also a simple calculation (there is a 1×1 matrix in the middle, and the inverse
of it is also 1×1)
A
¡
AtransA
¢−1 Atrans =
µ
−1
1
¶ µ¡
−1
1
¢ µ
−1
1
¶¶−1 ¡
−1
1
¢
=
µ
−1
1
¶ ¡
2
¢−1 ¡
−1
1
¢
=
µ
−1
1
¶ ¡
1/2
¢ ¡
−1
1
¢
=
µ
−1
1
¶ ¡
−1/2
1/2
¢
=
µ
1/2
−1/2
−1/2
1/2
¶
which of course gives the same answer.
projM(
µ
1
−3
¶
) =
µ
1/2
−1/2
−1/2
1/2
¶ µ
1
−3
¶
=
µ
2
−2
¶
(b) Parametrization gives this.
M = {c ·


−1
0
1

¯¯ c ∈R}
With that, the formula for the ﬁrst way gives this.


0
1
2




−1
0
1




−1
0
1




−1
0
1


·


−1
0
1

= 2
2 ·


−1
0
1

=


−1
0
1


To proceed by the second method we ﬁnd M ⊥,
M ⊥= {


u
v
w

¯¯ −u + w = 0} = {j ·


1
0
1

+ k ·


0
1
0

¯¯ j, k ∈R}
ﬁnd the representation of the given vector with respect to the concatenation of the bases BM and
BM ⊥


0
1
2

= 1 ·


−1
0
1

+ 1 ·


1
0
1

+ 1 ·


0
1
0


and retain only the M part.
projM(


0
1
2

) = 1 ·


−1
0
1

=


−1
0
1


Finally, for the third method, the matrix calculation
A
¡
AtransA
¢−1 Atrans =


−1
0
1

¡¡
−1
0
1
¢


−1
0
1

¢−1 ¡
−1
0
1
¢
=


−1
0
1

¡
2
¢−1 ¡
−1
0
1
¢
=


−1
0
1

¡
1/2
¢ ¡
−1
0
1
¢
=


−1
0
1

¡
−1/2
0
1/2
¢
=


1/2
0
−1/2
0
0
0
−1/2
0
1/2


followed by matrix-vector multiplication
projM(


0
1
2

)


1/2
0
−1/2
0
0
0
−1/2
0
1/2




0
1
2

=


−1
0
1


gives the answer.

354
Linear Algebra, by Hefferon
Three.VI.3.14
No, a decomposition of vectors ⃗v = ⃗m +⃗n into ⃗m ∈M and ⃗n ∈N does not depend on
the bases chosen for the subspaces — this was shown in the Direct Sum subsection.
Three.VI.3.15
The orthogonal projection of a vector into a subspace is a member of that subspace.
Since a trivial subspace has only one member, ⃗0, the projection of any vector must equal ⃗0.
Three.VI.3.16
The projection into M along N of a ⃗v ∈M is ⃗v. Decomposing ⃗v = ⃗m + ⃗n gives ⃗m = ⃗v
and ⃗n = ⃗0, and dropping the N part but retaining the M part results in a projection of ⃗m = ⃗v.
Three.VI.3.17
The proof of Lemma 3.7 shows that each vector ⃗v ∈Rn is the sum of its orthogonal
projections onto the lines spanned by the basis vectors.
⃗v = proj[⃗κ1](⃗v ) + · · · + proj[⃗κn](⃗v ) = ⃗v ⃗κ1
⃗κ1 ⃗κ1
· ⃗κ1 + · · · + ⃗v ⃗κn
⃗κn ⃗κn
· ⃗κn
Since the basis is orthonormal, the bottom of each fraction has ⃗κi ⃗κi = 1.
Three.VI.3.18
If V = M ⊕N then every vector can be decomposed uniquely as ⃗v = ⃗m + ⃗n. For all ⃗v
the map p gives p(⃗v) = ⃗m if and only if ⃗v −p(⃗v) = ⃗n, as required.
Three.VI.3.19
Let ⃗v be perpendicular to every ⃗w ∈S. Then ⃗v
(c1 ⃗w1 + · · · + cn ⃗wn) = ⃗v
(c1 ⃗w1) +
· · · + ⃗v (cn
⃗wn) = c1(⃗v
⃗w1) + · · · + cn(⃗v
⃗wn) = c1 · 0 + · · · + cn · 0 = 0.
Three.VI.3.20
True; the only vector orthogonal to itself is the zero vector.
Three.VI.3.21
This is immediate from the statement in Lemma 3.7 that the space is the direct sum
of the two.
Three.VI.3.22
The two must be equal, even only under the seemingly weaker condition that they yield
the same result on all orthogonal projections. Consider the subspace M spanned by the set {⃗v1,⃗v2}.
Since each is in M, the orthogonal projection of ⃗v1 into M is ⃗v1 and the orthogonal projection of ⃗v2
into M is ⃗v2. For their projections into M to be equal, they must be equal.
Three.VI.3.23
(a) We will show that the sets are mutually inclusive, M ⊆(M ⊥)⊥and (M ⊥)⊥⊆M.
For the ﬁrst, if ⃗m ∈M then by the deﬁnition of the perp operation, ⃗m is perpendicular to every
⃗v ∈M ⊥, and therefore (again by the deﬁnition of the perp operation) ⃗m ∈(M ⊥)⊥. For the other
direction, consider ⃗v ∈(M ⊥)⊥. Lemma 3.7’s proof shows that Rn = M ⊕M ⊥and that we can give
an orthogonal basis for the space ⟨⃗κ1, . . . ,⃗κk,⃗κk+1, . . . ,⃗κn⟩such that the ﬁrst half ⟨⃗κ1, . . . ,⃗κk⟩is a
basis for M and the second half is a basis for M ⊥. The proof also checks that each vector in the
space is the sum of its orthogonal projections onto the lines spanned by these basis vectors.
⃗v = proj[⃗κ1](⃗v ) + · · · + proj[⃗κn](⃗v )
Because ⃗v ∈(M ⊥)⊥, it is perpendicular to every vector in M ⊥, and so the projections in the second
half are all zero. Thus ⃗v = proj[⃗κ1](⃗v ) + · · · + proj[⃗κk](⃗v ), which is a linear combination of vectors
from M, and so ⃗v ∈M. (Remark. Here is a slicker way to do the second half: write the space both
as M ⊕M ⊥and as M ⊥⊕(M ⊥)⊥. Because the ﬁrst half showed that M ⊆(M ⊥)⊥and the prior
sentence shows that the dimension of the two subspaces M and (M ⊥)⊥are equal, we can conclude
that M equals (M ⊥)⊥.)
(b) Because M ⊆N, any ⃗v that is perpendicular to every vector in N is also perpendicular to every
vector in M. But that sentence simply says that N ⊥⊆M ⊥.
(c) We will again show that the sets are equal by mutual inclusion. The ﬁrst direction is easy; any ⃗v
perpendicular to every vector in M +N = {⃗m + ⃗n
¯¯ ⃗m ∈M, ⃗n ∈N} is perpendicular to every vector
of the form ⃗m + ⃗0 (that is, every vector in M) and every vector of the form ⃗0 + ⃗n (every vector in
N), and so (M + N)⊥⊆M ⊥∩N ⊥. The second direction is also routine; any vector ⃗v ∈M ⊥∩N ⊥
is perpendicular to any vector of the form c⃗m + d⃗n because ⃗v (c⃗m + d⃗n) = c · (⃗v
⃗m) + d · (⃗v ⃗n) =
c · 0 + d · 0 = 0.
Three.VI.3.24
(a) The representation of


v1
v2
v3


f
7−→1v1 + 2v2 + 3v3
is this.
RepE3,E1(f) =
¡
1
2
3
¢
By the deﬁnition of f
N (f) = {


v1
v2
v3

¯¯ 1v1 + 2v2 + 3v3 = 0} = {


v1
v2
v3

¯¯


1
2
3




v1
v2
v3

= 0}

Answers to Exercises
355
and this second description exactly says this.
N (f)⊥= [{


1
2
3

}]
(b) The generalization is that for any f : Rn →R there is a vector ⃗h so that



v1
...
vn



f
7−→h1v1 + · · · + hnvn
and ⃗h ∈N (f)⊥. We can prove this by, as in the prior item, representing f with respect to the
standard bases and taking ⃗h to be the column vector gotten by transposing the one row of that
matrix representation.
(c) Of course,
RepE3,E2(f) =
µ1
2
3
4
5
6
¶
and so the nullspace is this set.
N (f){


v1
v2
v3

¯¯
µ
1
2
3
4
5
6
¶ 

v1
v2
v3

=
µ
0
0
¶
}
That description makes clear that


1
2
3

,


4
5
6

∈N (f)⊥
and since N (f)⊥is a subspace of Rn, the span of the two vectors is a subspace of the perp of the
nullspace. To see that this containment is an equality, take
M = [{


1
2
3

}]
N = [{


4
5
6

}]
in the third item of Exercise 23, as suggested in the hint.
(d) As above, generalizing from the speciﬁc case is easy: for any f : Rn →Rm the matrix H repre-
senting the map with respect to the standard bases describes the action



v1
...
vn



f
7−→



h1,1v1 + h1,2v2 + · · · + h1,nvn
...
hm,1v1 + hm,2v2 + · · · + hm,nvn



and the description of the nullspace gives that on transposing the m rows of H
⃗h1 =





h1,1
h1,2
...
h1,n




, . . .⃗hm =





hm,1
hm,2
...
hm,n





we have N (f)⊥= [{⃗h1, . . . ,⃗hm}]. (In [Strang 93], this space is described as the transpose of the
row space of H.)
Three.VI.3.25
(a) First note that if a vector ⃗v is already in the line then the orthogonal projection
gives ⃗v itself. One way to verify this is to apply the formula for projection into the line spanned by
a vector ⃗s, namely (⃗v ⃗s/⃗s ⃗s) · ⃗s. Taking the line as {k · ⃗v
¯¯ k ∈R} (the ⃗v = ⃗0 case is separate but
easy) gives (⃗v ⃗v/⃗v ⃗v) · ⃗v, which simpliﬁes to ⃗v, as required.
Now, that answers the question because after once projecting into the line, the result projℓ(⃗v) is
in that line. The prior paragraph says that projecting into the same line again will have no eﬀect.
(b) The argument here is similar to the one in the prior item. With V = M ⊕N, the projection of
⃗v = ⃗m+⃗n is projM,N(⃗v ) = ⃗m. Now repeating the projection will give projM,N(⃗m) = ⃗m, as required,
because the decomposition of a member of M into the sum of a member of M and a member of N
is ⃗m = ⃗m +⃗0. Thus, projecting twice into M along N has the same eﬀect as projecting once.

356
Linear Algebra, by Hefferon
(c) As suggested by the prior items, the condition gives that t leaves vectors in the rangespace
unchanged, and hints that we should take ⃗β1, . . . , ⃗βr to be basis vectors for the range, that is, that
we should take the range space of t for M (so that dim(M) = r). As for the complement, we write
N for the nullspace of t and we will show that V = M ⊕N.
To show this, we can show that their intersection is trivial M ∩N = {⃗0} and that they sum to the
entire space M + N = V . For the ﬁrst, if a vector ⃗m is in the rangespace then there is a ⃗v ∈V with
t(⃗v) = ⃗m, and the condition on t gives that t(⃗m) = (t◦t) (⃗v) = t(⃗v) = ⃗m, while if that same vector is
also in the nullspace then t(⃗m) = ⃗0 and so the intersection of the rangespace and nullspace is trivial.
For the second, to write an arbitrary ⃗v as the sum of a vector from the rangespace and a vector from
the nullspace, the fact that the condition t(⃗v) = t(t(⃗v)) can be rewritten as t(⃗v −t(⃗v)) = ⃗0 suggests
taking ⃗v = t(⃗v) + (⃗v −t(⃗v)).
So we are ﬁnished on taking a basis B = ⟨⃗β1, . . . , ⃗βn⟩for V where ⟨⃗β1, . . . , ⃗βr⟩is a basis for the
rangespace M and ⟨⃗βr+1, . . . , ⃗βn⟩is a basis for the nullspace N.
(d) Every projection (as deﬁned in this exercise) is a projection into its rangespace and along its
nullspace.
(e) This also follows immediately from the third item.
Three.VI.3.26
For any matrix M we have that (M −1)
trans = (M trans)−1, and for any two matrices
M, N we have that MN trans = N transM trans (provided, of course, that the inverse and product are
deﬁned). Applying these two gives that the matrix equals its transpose.
¡
A(AtransA)−1Atrans¢trans = (Atranstrans)(
¡
(AtransA)−1¢trans)(Atrans)
= (Atranstrans)(
¡
(AtransA)
trans¢−1)(Atrans) = A(AtransAtranstrans)−1Atrans = A(AtransA)−1Atrans
Topic: Line of Best Fit
Data on the progression of the world’s records (taken from the Runner’s World web site) is below.
1
As with the ﬁrst example discussed above, we are trying to ﬁnd a best m to “solve” this system.
8m = 4
16m = 9
24m = 13
32m = 17
40m = 20
Projecting into the linear subspace gives this






4
9
13
17
20












8
16
24
32
40












8
16
24
32
40












8
16
24
32
40






·






8
16
24
32
40






= 1832
3520 ·






8
16
24
32
40






so the slope of the line of best ﬁt is approximately 0.52.
0
10
20
30
40
0
5
10
15
20

Answers to Exercises
357
Progression of Men’s Mile Record
time
name
date
4:52.0
Cadet Marshall (GBR)
02Sep52
4:45.0
Thomas Finch (GBR)
03Nov58
4:40.0
Gerald Surman (GBR)
24Nov59
4:33.0
George Farran (IRL)
23May62
4:29 3/5
Walter Chinnery (GBR)
10Mar68
4:28 4/5
William Gibbs (GBR)
03Apr68
4:28 3/5
Charles Gunton (GBR)
31Mar73
4:26.0
Walter Slade (GBR)
30May74
4:24 1/2
Walter Slade (GBR)
19Jun75
4:23 1/5
Walter George (GBR)
16Aug80
4:19 2/5
Walter George (GBR)
03Jun82
4:18 2/5
Walter George (GBR)
21Jun84
4:17 4/5
Thomas Conneﬀ(USA)
26Aug93
4:17.0
Fred Bacon (GBR)
06Jul95
4:15 3/5
Thomas Conneﬀ(USA)
28Aug95
4:15 2/5
John Paul Jones (USA)
27May11
4:14.4
John Paul Jones (USA)
31May13
4:12.6
Norman Taber (USA)
16Jul15
4:10.4
Paavo Nurmi (FIN)
23Aug23
4:09 1/5
Jules Ladoumegue (FRA)
04Oct31
4:07.6
Jack Lovelock (NZL)
15Jul33
4:06.8
Glenn Cunningham (USA)
16Jun34
4:06.4
Sydney Wooderson (GBR)
28Aug37
4:06.2
Gunder Hagg (SWE)
01Jul42
4:04.6
Gunder Hagg (SWE)
04Sep42
4:02.6
Arne Andersson (SWE)
01Jul43
4:01.6
Arne Andersson (SWE)
18Jul44
4:01.4
Gunder Hagg (SWE)
17Jul45
3:59.4
Roger Bannister (GBR)
06May54
3:58.0
John Landy (AUS)
21Jun54
3:57.2
Derek Ibbotson (GBR)
19Jul57
3:54.5
Herb Elliott (AUS)
06Aug58
3:54.4
Peter Snell (NZL)
27Jan62
3:54.1
Peter Snell (NZL)
17Nov64
3:53.6
Michel Jazy (FRA)
09Jun65
3:51.3
Jim Ryun (USA)
17Jul66
3:51.1
Jim Ryun (USA)
23Jun67
3:51.0
Filbert Bayi (TAN)
17May75
3:49.4
John Walker (NZL)
12Aug75
3:49.0
Sebastian Coe (GBR)
17Jul79
3:48.8
Steve Ovett (GBR)
01Jul80
3:48.53
Sebastian Coe (GBR)
19Aug81
3:48.40
Steve Ovett (GBR)
26Aug81
3:47.33
Sebastian Coe (GBR)
28Aug81
3:46.32
Steve Cram (GBR)
27Jul85
3:44.39
Noureddine Morceli (ALG)
05Sep93
3:43.13
Hicham el Guerrouj (MOR)
07Jul99
Progression of Men’s 1500 Meter Record
time
name
date
4:09.0
John Bray (USA)
30May00
4:06.2
Charles Bennett (GBR)
15Jul00
4:05.4
James Lightbody (USA)
03Sep04
3:59.8
Harold Wilson (GBR)
30May08
3:59.2
Abel Kiviat (USA)
26May12
3:56.8
Abel Kiviat (USA)
02Jun12
3:55.8
Abel Kiviat (USA)
08Jun12
3:55.0
Norman Taber (USA)
16Jul15
3:54.7
John Zander (SWE)
05Aug17
3:53.0
Paavo Nurmi (FIN)
23Aug23
3:52.6
Paavo Nurmi (FIN)
19Jun24
3:51.0
Otto Peltzer (GER)
11Sep26
3:49.2
Jules Ladoumegue (FRA)
05Oct30
3:49.0
Luigi Beccali (ITA)
17Sep33
3:48.8
William Bonthron (USA)
30Jun34
3:47.8
Jack Lovelock (NZL)
06Aug36
3:47.6
Gunder Hagg (SWE)
10Aug41
3:45.8
Gunder Hagg (SWE)
17Jul42
3:45.0
Arne Andersson (SWE)
17Aug43
3:43.0
Gunder Hagg (SWE)
07Jul44
3:42.8
Wes Santee (USA)
04Jun54
3:41.8
John Landy (AUS)
21Jun54
3:40.8
Sandor Iharos (HUN)
28Jul55
3:40.6
Istvan Rozsavolgyi (HUN)
03Aug56
3:40.2
Olavi Salsola (FIN)
11Jul57
3:38.1
Stanislav Jungwirth (CZE)
12Jul57
3:36.0
Herb Elliott (AUS)
28Aug58
3:35.6
Herb Elliott (AUS)
06Sep60
3:33.1
Jim Ryun (USA)
08Jul67
3:32.2
Filbert Bayi (TAN)
02Feb74
3:32.1
Sebastian Coe (GBR)
15Aug79
3:31.36
Steve Ovett (GBR)
27Aug80
3:31.24
Sydney Maree (usa)
28Aug83
3:30.77
Steve Ovett (GBR)
04Sep83
3:29.67
Steve Cram (GBR)
16Jul85
3:29.46
Said Aouita (MOR)
23Aug85
3:28.86
Noureddine Morceli (ALG)
06Sep92
3:27.37
Noureddine Morceli (ALG)
12Jul95
3:26.00
Hicham el Guerrouj (MOR)
14Jul98
Progression of Women’s Mile Record
time
name
date
6:13.2
Elizabeth Atkinson (GBR)
24Jun21
5:27.5
Ruth Christmas (GBR)
20Aug32
5:24.0
Gladys Lunn (GBR)
01Jun36
5:23.0
Gladys Lunn (GBR)
18Jul36
5:20.8
Gladys Lunn (GBR)
08May37
5:17.0
Gladys Lunn (GBR)
07Aug37
5:15.3
Evelyne Forster (GBR)
22Jul39
5:11.0
Anne Oliver (GBR)
14Jun52
5:09.8
Enid Harding (GBR)
04Jul53
5:08.0
Anne Oliver (GBR)
12Sep53
5:02.6
Diane Leather (GBR)
30Sep53
5:00.3
Edith Treybal (ROM)
01Nov53
5:00.2
Diane Leather (GBR)
26May54
4:59.6
Diane Leather (GBR)
29May54
4:50.8
Diane Leather (GBR)
24May55
4:45.0
Diane Leather (GBR)
21Sep55
4:41.4
Marise Chamberlain (NZL)
08Dec62
4:39.2
Anne Smith (GBR)
13May67
4:37.0
Anne Smith (GBR)
03Jun67
4:36.8
Maria Gommers (HOL)
14Jun69
4:35.3
Ellen Tittel (FRG)
20Aug71
4:34.9
Glenda Reiser (CAN)
07Jul73
4:29.5
Paola Pigni-Cacchi (ITA)
08Aug73
4:23.8
Natalia Marasescu (ROM)
21May77
4:22.1
Natalia Marasescu (ROM)
27Jan79
4:21.7
Mary Decker (USA)
26Jan80
4:20.89
Lyudmila Veselkova (SOV)
12Sep81
4:18.08
Mary Decker-Tabb (USA)
09Jul82
4:17.44
Maricica Puica (ROM)
16Sep82
4:15.8
Natalya Artyomova (SOV)
05Aug84
4:16.71
Mary Decker-Slaney (USA)
21Aug85
4:15.61
Paula Ivan (ROM)
10Jul89
4:12.56
Svetlana Masterkova (RUS)
14Aug96

358
Linear Algebra, by Hefferon
2
With this input
A =







1
1852.71
1
1858.88
...
...
1
1985.54
1
1993.71







b =







292.0
285.0
...
226.32
224.39







(the dates have been rounded to months, e.g., for a September record, the decimal .71 ≈(8.5/12) was
used), Maple responded with an intercept of b = 994.8276974 and a slope of m = −0.3871993827.
1850
1900
1950
2000
220
240
260
280
3
With this input (the years are zeroed at 1900)
A :=







1
.38
1
.54
......
1
92.71
1
95.54







b =







249.0
246.2
...
208.86
207.37







(the dates have been rounded to months, e.g., for a September record, the decimal .71 ≈(8.5/12) was
used), Maple gives an intercept of b = 243.1590327 and a slope of m = −0.401647703. The slope given
in the body of this Topic for the men’s mile is quite close to this.
1900 1920 1940 1960 1980 2000
200
210
220
230
240
250
4
With this input (the years are zeroed at 1900)
A =







1
21.46
1
32.63
...
...
1
89.54
1
96.63







b =







373.2
327.5
...
255.61
252.56







(the dates have been rounded to months, e.g., for a September record, the decimal .71 ≈(8.5/12) was
used), MAPLE gave an intercept of b = 378.7114894 and a slope of m = −1.445753225.
1900
1920
1940
1960
1980
2000
220
240
260
280
300
320
340
360
380

Answers to Exercises
359
5
These are the equations of the lines for men’s and women’s mile (the vertical intercept term of the
equation for the women’s mile has been adjusted from the answer above, to zero it at the year 0,
because that’s how the men’s mile equation was done).
y = 994.8276974 −0.3871993827x
y = 3125.6426 −1.445753225x
Obviously the lines cross. A computer program is the easiest way to do the arithmetic: MuPAD gives
x = 2012.949004 and y = 215.4150856 (215 seconds is 3 minutes and 35 seconds). Remark. Of course
all of this projection is highly dubious — for one thing, the equation for the women is inﬂuenced by
the quite slow early times — but it is nonetheless fun.
1850
1900
1950
2000
220
240
260
280
300
320
340
360
380
6
(a) A computer algebra system like MAPLE or MuPAD will give an intercept of b = 4259/1398 ≈
3.239628 and a slope of m = −71/2796 ≈−0.025393419 Plugging x = 31 into the equation yields
a predicted number of O-ring failures of y = 2.45 (rounded to two places). Plugging in y = 4 and
solving gives a temperature of x = −29.94◦F.
(b) On the basis of this information
A =







1
53
1
75
...
1
80
1
81







b =







3
2
...
0
0







MAPLE gives the intercept b = 187/40 = 4.675 and the slope m = −73/1200 ≈−0.060833. Here,
plugging x = 31 into the equation predicts y = 2.79 O-ring failures (rounded to two places). Plugging
in y = 4 failures gives a temperature of x = 11◦F.
40
50
60
70
80
0
1
2
3
7
(a) The plot is nonlinear.
0
2
4
6
0
5
10
15
20
(b) Here is the plot.

360
Linear Algebra, by Hefferon
0
2
4
6
−0.5
0
0.5
1
There is perhaps a jog up between planet 4 and planet 5.
(c) This plot seems even more linear.
0
2
4
6
8
−0.5
0
0.5
1
(d) With this input
A =










1
1
1
2
1
3
1
4
1
6
1
7
1
8










b =










−0.40893539
−0.1426675
0
0.18184359
0.71600334
0.97954837
1.2833012










MuPAD gives that the intercept is b = −0.6780677466 and the slope is m = 0.2372763818.
(e) Plugging x = 9 into the equation y = −0.6780677466 + 0.2372763818x from the prior item gives
that the log of the distance is 1.4574197, so the expected distance is 28.669472. The actual distance
is about 30.003.
(f) Plugging x = 10 into the same equation gives that the log of the distance is 1.6946961, so the
expected distance is 49.510362. The actual distance is about 39.503.
8
(a) With this input
A =












1
306
1
329
1
356
1
367
1
396
1
427
1
415
1
424












b =












975
969
948
910
890
906
900
899












MAPLE gives the intercept b = 34009779/28796 ≈1181.0591 and the slope m = −19561/28796 ≈
−0.6793.
300 320 340 360 380 400 420
900
920
940
960
980

Answers to Exercises
361
Topic: Geometry of Linear Maps
1
(a) To represent H, recall that rotation counterclockwise by θ radians is represented with respect
to the standard basis in this way.
RepE2,E2(h) =
µ
cos θ
−sin θ
sin θ
cos θ
¶
A clockwise angle is the negative of a counterclockwise one.
RepE2,E2(h) =
µ
cos(−π/4)
−sin(−π/4)
sin(−π/4)
cos(−π/4)
¶
=
µ √
2/2
√
2/2
−
√
2/2
√
2/2
¶
This Gauss-Jordan reduction
ρ1+ρ2
−→
µ√
2/2
√
2/2
0
√
2
¶
(2/
√
2)ρ1
−→
(1/
√
2)ρ2
µ1
1
0
1
¶
−ρ2+ρ1
−→
µ1
0
0
1
¶
produces the identity matrix so there is no need for column-swapping operations to end with a
partial-identity.
(b) The reduction is expressed in matrix multiplication as
µ
1
−1
0
1
¶ µ
2/
√
2
0
0
1/
√
2
¶ µ
1
0
1
1
¶
H = I
(note that composition of the Gaussian operations is performed from right to left).
(c) Taking inverses
H =
µ
1
0
−1
1
¶ µ√
2/2
0
0
√
2
¶ µ
1
1
0
1
¶
|
{z
}
P
I
gives the desired factorization of H (here, the partial identity is I, and Q is trivial, that is, it is also
an identity matrix).
(d) Reading the composition from right to left (and ignoring the identity matrices as trivial) gives
that H has the same eﬀect as ﬁrst performing this skew
⃗u
⃗v
h(⃗u)
h(⃗v)
µx
y
¶
7→
µx + y
y
¶
−→
followed by a dilation that multiplies all ﬁrst components by
√
2/2 (this is a “shrink” in that
√
2/2 ≈0.707) and all second components by
√
2, followed by another skew.
⃗u
⃗v
h(⃗u)
h(⃗v)
µ
x
y
¶
7→
µ
x
−x + y
¶
−→
For instance, the eﬀect of H on the unit vector whose angle with the x-axis is π/3 is this.

362
Linear Algebra, by Hefferon
³√
3/2
1/2
´
³
(
√
3 + 1)/2
1/2
´
³√
2(
√
3 + 1)/2
√
2/2
´
³√
2(
√
3 + 1)/4
√
2(1 −
√
3)/4
´
µ
x
y
¶
7→
µ
x + y
y
¶
−→
µ
x
y
¶
7→
µ
(
√
2/2)x
√
2y
¶
−→
µ
x
y
¶
7→
µ
x
−x + y
¶
−→
Verifying that the resulting vector has unit length and forms an angle of −π/6 with the x-axis is
routine.
2
We will ﬁrst represent the map with a matrix H, perform the row operations and, if needed, column
operations to reduce it to a partial-identity matrix. We will then translate that into a factorization
H = PBQ. Subsitituting into the general matrix
RepE2,E2(rθ)
µcos θ
−sin θ
sin θ
cos θ
¶
gives this representation.
RepE2,E2(r2π/3)
µ
−1/2
−
√
3/2
√
3/2
−1/2
¶
Gauss’ method is routine.
√
3ρ1+ρ2
−→
µ
−1/2
−
√
3/2
0
−2
¶
−2ρ1
−→
(−1/2)ρ2
µ
1
√
3
0
1
¶
−
√
3ρ2+ρ1
−→
µ
1
0
0
1
¶
That translates to a matrix equation in this way.
µ
1
−
√
3
0
1
¶ µ
−2
0
0
−1/2
¶ µ 1
0
√
3
1
¶ µ
−1/2
−
√
3/2
√
3/2
−1/2
¶
= I
Taking inverses to solve for H yields this factorization.
µ−1/2
−
√
3/2
√
3/2
−1/2
¶
=
µ
1
0
−
√
3
1
¶ µ−1/2
0
0
−2
¶ µ
1
√
3
0
1
¶
I
3
This Gaussian reduction
−3ρ1+ρ2
−→
−ρ1+ρ3


1
2
1
0
0
−3
0
0
1

(1/3)ρ2+ρ3
−→


1
2
1
0
0
−3
0
0
0

(−1/3)ρ2
−→


1
2
1
0
0
1
0
0
0

−ρ2+ρ1
−→


1
2
0
0
0
1
0
0
0


gives the reduced echelon form of the matrix. Now the two column operations of taking −2 times the
ﬁrst column and adding it to the second, and then of swapping columns two and three produce this
partial identity.
B =


1
0
0
0
1
0
0
0
0


All of that translates into matrix terms as: where
P =


1
−1
0
0
1
0
0
0
1




1
0
0
0
−1/3
0
0
0
1




1
0
0
0
1
0
0
1/3
1




1
0
0
0
1
0
−1
0
1




1
0
0
−3
1
0
0
0
1


and
Q =


1
−2
0
0
1
0
0
0
1




0
1
0
1
0
0
0
0
1


the given matrix factors as PBQ.

Answers to Exercises
363
4
Represent it with respect to the standard bases E1, E1, then the only entry in the resulting 1×1 matrix
is the scalar k.
5
We can show this by induction on the number of components in the vector. In the n = 1 base case
the only permutation is the trivial one, and the map
¡
x1
¢
7→
¡
x1
¢
is indeed expressible as a composition of swaps — as zero swaps. For the inductive step we assume
that the map induced by any permutation of fewer than n numbers can be expressed with swaps only,
and we consider the map induced by a permutation p of n numbers.





x1
x2
...
xn




7→





xp(1)
xp(2)
...
xp(n)





Consider the number i such that p(i) = n. The map










x1
x2
...
xi
...
xn










ˆp
7−→










xp(1)
xp(2)
...
xp(n)
...
xn










will, when followed by the swap of the i-th and n-th components, give the map p. Now, the inductive
hypothesis gives that ˆp is achievable as a composition of swaps.
6
(a) A line is a subset of Rn of the form {⃗v = ⃗u + t · ⃗w
¯¯ t ∈R}. The image of a point on that line
is h(⃗v) = h(⃗u + t · ⃗w) = h(⃗u) + t · h(⃗w), and the set of such vectors, as t ranges over the reals, is a
line (albeit, degenerate if h(⃗w) = ⃗0).
(b) This is an obvious extension of the prior argument.
(c) If the point B is between the points A and C then the line from A to C has B in it. That is,
there is a t ∈(0 .. 1) such that ⃗b = ⃗a + t · (⃗c −⃗a) (where B is the endpoint of ⃗b, etc.). Now, as in the
argument of the ﬁrst item, linearity shows that h(⃗b) = h(⃗a) + t · h(⃗c −⃗a).
7
The two are inverse. For instance, for a ﬁxed x ∈R, if f ′(x) = k (with k ̸= 0) then (f −1)′(x) = 1/k.
x
f(x)
f−1(f(x))
Topic: Markov Chains
1
(a) With this ﬁle coin.m
# Octave function for Markov coin game.
p is chance of going down.
function w = coin(p,v)
q = 1-p;
A=[1,p,0,0,0,0;
0,0,p,0,0,0;
0,q,0,p,0,0;
0,0,q,0,p,0;
0,0,0,q,0,0;
0,0,0,0,q,1];
w = A * v;
endfunction

364
Linear Algebra, by Hefferon
This Octave session produced the output given here.
octave:1> v0=[0;0;0;1;0;0]
v0 =
0
0
0
1
0
0
octave:2> p=.5
p = 0.50000
octave:3> v1=coin(p,v0)
v1 =
0.00000
0.00000
0.50000
0.00000
0.50000
0.00000
octave:4> v2=coin(p,v1)
v2 =
0.00000
0.25000
0.00000
0.50000
0.00000
0.25000
This continued for too many steps to list here.
octave:26> v24=coin(p,v23)
v24 =
0.39600
0.00276
0.00000
0.00447
0.00000
0.59676
(b) Using these formulas
p1(n + 1) = 0.5 · p2(n)
p2(n + 1) = 0.5 · p1(n) + 0.5 · p3(n)
p3(n + 1) = 0.5 · p2(n) + 0.5 · p4(n)
p5(n + 1) = 0.5 · p4(n)
and these initial conditions








p0(0)
p1(0)
p2(0)
p3(0)
p4(0)
p5(0)








=








0
0
0
1
0
0








we will prove by induction that when n is odd then p1(n) = p3(n) = 0 and when n is even then
p2(n) = p4(n) = 0. Note ﬁrst that this is true in the n = 0 base case by the initial conditions. For
the inductive step, suppose that it is true in the n = 0, n = 1, . . . , n = k cases and consider the
n = k + 1 case. If k + 1 is odd then the two
p1(k + 1) = 0.5 · p2(k) = 0.5 · 0 = 0
p3(k + 1) = 0.5 · p2(k) + 0.5 · p4(k) = 0.5 · 0 + 0.5 · 0 = 0
follow from the inductive hypothesis that p2(k) = p4(k) = 0 since k is even. The case where k + 1
is even is similar.
(c) We can use, say, n = 100. This Octave session
octave:1> B=[1,.5,0,0,0,0;
>
0,0,.5,0,0,0;
>
0,.5,0,.5,0,0;
>
0,0,.5,0,.5,0;

Answers to Exercises
365
>
0,0,0,.5,0,0;
>
0,0,0,0,.5,1];
octave:2> B100=B**100
B100 =
1.00000
0.80000
0.60000
0.40000
0.20000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.00000
0.20000
0.40000
0.60000
0.80000
1.00000
octave:3> B100*[0;1;0;0;0;0]
octave:4> B100*[0;1;0;0;0;0]
octave:5> B100*[0;0;0;1;0;0]
octave:6> B100*[0;1;0;0;0;0]
yields these outputs.
starting with:
$1
$2
$3
$4
s0(100)
s1(100)
s2(100)
s3(100)
s4(100)
s5(100)
0.80000
0.00000
0.00000
0.00000
0.00000
0.20000
0.60000
0.00000
0.00000
0.00000
0.00000
0.40000
0.40000
0.00000
0.00000
0.00000
0.00000
0.60000
0.20000
0.00000
0.00000
0.00000
0.00000
0.80000
2
(a) From these equations
(1/6)s1(n) +
0s2(n) +
0s3(n) +
0s4(n) +
0s5(n) +
0s6(n) = s1(n + 1)
(1/6)s1(n) + (2/6)s2(n) +
0s3(n) +
0s4(n) +
0s5(n) +
0s6(n) = s2(n + 1)
(1/6)s1(n) + (1/6)s2(n) + (3/6)s3(n) +
0s4(n) +
0s5(n) +
0s6(n) = s3(n + 1)
(1/6)s1(n) + (1/6)s2(n) + (1/6)s3(n) + (4/6)s4(n) +
0s5(n) +
0s6(n) = s4(n + 1)
(1/6)s1(n) + (1/6)s2(n) + (1/6)s3(n) + (1/6)s4(n) + (5/6)s5(n) +
0s6(n) = s5(n + 1)
(1/6)s1(n) + (1/6)s2(n) + (1/6)s3(n) + (1/6)s4(n) + (1/6)s5(n) + (6/6)s6(n) = s6(n + 1)
We get this transition matrix. 







1/6
0
0
0
0
0
1/6
2/6
0
0
0
0
1/6
1/6
3/6
0
0
0
1/6
1/6
1/6
4/6
0
0
1/6
1/6
1/6
1/6
5/6
0
1/6
1/6
1/6
1/6
1/6
6/6








(b) This is the Octave session, with outputs edited out and condensed into the table at the end.
octave:1>
F=[1/6,
0,
0,
0,
0,
0;
>
1/6,
2/6, 0,
0,
0,
0;
>
1/6,
1/6, 3/6, 0,
0,
0;
>
1/6,
1/6, 1/6, 4/6, 0,
0;
>
1/6,
1/6, 1/6, 1/6, 5/6, 0;
>
1/6,
1/6, 1/6, 1/6, 1/6, 6/6];
octave:2> v0=[1;0;0;0;0;0]
octave:3> v1=F*v0
octave:4> v2=F*v1
octave:5> v3=F*v2
octave:6> v4=F*v3
octave:7> v5=F*v4
These are the results.
1
2
3
4
5
1
0
0
0
0
0
0.16667
0.16667
0.16667
0.16667
0.16667
0.16667
0.027778
0.083333
0.138889
0.194444
0.250000
0.305556
0.0046296
0.0324074
0.0879630
0.1712963
0.2824074
0.4212963
0.00077160
0.01157407
0.05015432
0.13503086
0.28472222
0.51774691
0.00012860
0.00398663
0.02713477
0.10043724
0.27019033
0.59812243

366
Linear Algebra, by Hefferon
3
(a) It does seem reasonable that, while the ﬁrm’s present location should strongly inﬂuence where
it is next time (for instance, whether it stays), any locations in the prior stages should have little
inﬂuence. That is, while a company may move or stay because of where it is, it is unlikely to move
or stay because of where it was.
(b) This Octave session has been edited, with the outputs put together in a table at the end.
octave:1> M=[.787,0,0,.111,.102;
>
0,.966,.034,0,0;
>
0,.063,.937,0,0;
>
0,0,.074,.612,.314;
>
.021,.009,.005,.010,.954]
M =
0.78700
0.00000
0.00000
0.11100
0.10200
0.00000
0.96600
0.03400
0.00000
0.00000
0.00000
0.06300
0.93700
0.00000
0.00000
0.00000
0.00000
0.07400
0.61200
0.31400
0.02100
0.00900
0.00500
0.01000
0.95400
octave:2> v0=[.025;.025;.025;.025;.900]
octave:3> v1=M*v0
octave:4> v2=M*v1
octave:5> v3=M*v2
octave:6> v4=M*v3
is summarized in this table.
⃗p0
⃗p1
⃗p2
⃗p3
⃗p4






0.025000
0.025000
0.025000
0.025000
0.900000












0.114250
0.025000
0.025000
0.299750
0.859725












0.210879
0.025000
0.025000
0.455251
0.825924












0.300739
0.025000
0.025000
0.539804
0.797263












0.377920
0.025000
0.025000
0.582550
0.772652






(c) This is a continuation of the Octave session from the prior item.
octave:7> p0=[.0000;.6522;.3478;.0000;.0000]
octave:8> p1=M*p0
octave:9> p2=M*p1
octave:10> p3=M*p2
octave:11> p4=M*p3
This summarizes the output.
⃗p0
⃗p1
⃗p2
⃗p3
⃗p4






0.00000
0.65220
0.34780
0.00000
0.00000












0.00000
0.64185
0.36698
0.02574
0.00761












0.0036329
0.6325047
0.3842942
0.0452966
0.0151277












0.0094301
0.6240656
0.3999315
0.0609094
0.0225751












0.016485
0.616445
0.414052
0.073960
0.029960






(d) This is more of the same Octave session.
octave:12> M50=M**50
M50 =
0.03992
0.33666
0.20318
0.02198
0.37332
0.00000
0.65162
0.34838
0.00000
0.00000
0.00000
0.64553
0.35447
0.00000
0.00000
0.03384
0.38235
0.22511
0.01864
0.31652
0.04003
0.33316
0.20029
0.02204
0.37437
octave:13> p50=M50*p0
p50 =
0.29024
0.54615
0.54430
0.32766
0.28695
octave:14> p51=M*p50
p51 =
0.29406
0.54609

Answers to Exercises
367
0.54442
0.33091
0.29076
This is close to a steady state.
4
(a) This is the relevant system of equations.
(1 −2p) · sU(n) +
p · tA(n) +
p · tB(n)
= sU(n + 1)
p · sU(n) + (1 −2p) · tA(n)
= tA(n + 1)
p · sU(n)
+ (1 −2p) · tB(n)
= tB(n + 1)
p · tA(n)
+ sA(n)
= sA(n + 1)
p · tB(n)
+ sB(n) = sB(n + 1)
Thus we have this.





1 −2p
p
p
0
0
p
1 −2p
0
0
0
p
0
1 −2p
0
0
0
p
0
1
0
0
0
p
0
1












sU(n)
tA(n)
tB(n)
sA(n)
sB(n)






=






sU(n + 1)
tA(n + 1)
tB(n + 1)
sA(n + 1)
sB(n + 1)






(b) This is the Octave code, with the output removed.
octave:1> T=[.5,.25,.25,0,0;
>
.25,.5,0,0,0;
>
.25,0,.5,0,0;
>
0,.25,0,1,0;
>
0,0,.25,0,1]
T =
0.50000
0.25000
0.25000
0.00000
0.00000
0.25000
0.50000
0.00000
0.00000
0.00000
0.25000
0.00000
0.50000
0.00000
0.00000
0.00000
0.25000
0.00000
1.00000
0.00000
0.00000
0.00000
0.25000
0.00000
1.00000
octave:2> p0=[1;0;0;0;0]
octave:3> p1=T*p0
octave:4> p2=T*p1
octave:5> p3=T*p2
octave:6> p4=T*p3
octave:7> p5=T*p4
Here is the output. The probability of ending at sA is about 0.23.
⃗p0
⃗p1
⃗p2
⃗p3
⃗p4
⃗p5
sU
tA
tB
sA
sB
1
0
0
0
0
0.50000
0.25000
0.25000
0.00000
0.00000
0.375000
0.250000
0.250000
0.062500
0.062500
0.31250
0.21875
0.21875
0.12500
0.12500
0.26562
0.18750
0.18750
0.17969
0.17969
0.22656
0.16016
0.16016
0.22656
0.22656
(c) With this ﬁle as learn.m
# Octave script file for learning model.
function w = learn(p)
T = [1-2*p,p,
p,
0, 0;
p,
1-2*p,0,
0, 0;
p,
0,
1-2*p,0, 0;
0,
p,
0,
1, 0;
0,
0,
p,
0, 1];
T5 = T**5;
p5 = T5*[1;0;0;0;0];
w = p5(4);
endfunction
issuing the command octave:1> learn(.20) yields ans = 0.17664.
(d) This Octave session
octave:1> x=(.01:.01:.50)’;
octave:2> y=(.01:.01:.50)’;
octave:3> for i=.01:.01:.50
>
y(100*i)=learn(i);

368
Linear Algebra, by Hefferon
>
endfor
octave:4> z=[x, y];
octave:5> gplot z
yields this plot. There is no threshold value — no probability above which the curve rises sharply.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
line 1
5
(a) From these equations
0.90 · pT (n) + 0.01 · pC(n) = pT (n + 1)
0.10 · pT (n) + 0.99 · pC(n) = pC(n + 1)
we get this matrix.
µ0.90
0.01
0.10
0.99
¶ µpT (n)
pC(n)
¶
=
µpT (n + 1)
pC(n + 1)
¶
(b) This is the result from Octave.
n = 0
1
2
3
4
5
0.30000
0.70000
0.27700
0.72300
0.25653
0.74347
0.23831
0.76169
0.22210
0.77790
0.20767
0.79233
6
7
8
9
10
0.19482
0.80518
0.18339
0.81661
0.17322
0.82678
0.16417
0.83583
0.15611
0.84389
(c) This is the sT = 0.2 result.
n = 0
1
2
3
4
5
0.20000
0.80000
0.18800
0.81200
0.17732
0.82268
0.16781
0.83219
0.15936
0.84064
0.15183
0.84817
6
7
8
9
10
0.14513
0.85487
0.13916
0.86084
0.13385
0.86615
0.12913
0.87087
0.12493
0.87507
(d) Although the probability vectors start 0.1 apart, they end only 0.032 apart. So they are alike.
6
These are the p = .55 vectors,

Answers to Exercises
369
n = 0
n = 1
n = 2
n = 3
n = 4
n = 5
n = 6
n = 7
0-0
1-0
0-1
2-0
1-1
0-2
3-0
2-1
1-2
0-3
4-0
3-1
2-2
1-3
0-4
4-1
3-2
2-3
1-4
4-2
3-3
2-4
4-3
3-4
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.55000
0.45000
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.30250
0.49500
0.20250
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.16638
0.40837
0.33412
0.09112
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.09151
0.29948
0.36754
0.20047
0.04101
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.09151
0
0
0
0.04101
0.16471
0.33691
0.27565
0.09021
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.09151
0
0
0
0.04101
0.16471
0
0
0.09021
0.18530
0.30322
0.12404
0
0
0
0
0
0
0
0
0
0
0
0
0.09151
0
0
0
0.04101
0.16471
0
0
0.09021
0.18530
0
0.12404
0.16677
0.13645
and these are the p = .60 vectors.
n = 0
n = 1
n = 2
n = 3
n = 4
n = 5
n = 6
n = 7
0-0
1-0
0-1
2-0
1-1
0-2
3-0
2-1
1-2
0-3
4-0
3-1
2-2
1-3
0-4
4-1
3-2
2-3
1-4
4-2
3-3
2-4
4-3
3-4
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.60000
0.40000
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.36000
0.48000
0.16000
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.21600
0.43200
0.28800
0.06400
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.12960
0.34560
0.34560
0.15360
0.02560
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.12960
0
0
0
0.02560
0.20736
0.34560
0.23040
0.06144
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.12960
0
0
0
0.02560
0.20736
0
0
0.06144
0.20736
0.27648
0.09216
0
0
0
0
0
0
0
0
0
0
0
0
0.12960
0
0
0
0.02560
0.20736
0
0
0.06144
0.20736
0
0.09216
0.16589
0.11059
(a) The script from the computer code section can be easily adapted.
# Octave script file to compute chance of World Series outcomes.
function w = markov(p,v)
q = 1-p;
A=[0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 0-0
p,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 1-0
q,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 0-1_
0,p,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 2-0
0,q,p,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 1-1
0,0,q,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 0-2__
0,0,0,p,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 3-0

370
Linear Algebra, by Hefferon
0,0,0,q,p,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 2-1
0,0,0,0,q,p, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 1-2_
0,0,0,0,0,q, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 0-3
0,0,0,0,0,0, p,0,0,0,1,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 4-0
0,0,0,0,0,0, q,p,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 3-1__
0,0,0,0,0,0, 0,q,p,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 2-2
0,0,0,0,0,0, 0,0,q,p,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0;
# 1-3
0,0,0,0,0,0, 0,0,0,q,0,0, 0,0,1,0,0,0, 0,0,0,0,0,0;
# 0-4_
0,0,0,0,0,0, 0,0,0,0,0,p, 0,0,0,1,0,0, 0,0,0,0,0,0;
# 4-1
0,0,0,0,0,0, 0,0,0,0,0,q, p,0,0,0,0,0, 0,0,0,0,0,0;
# 3-2
0,0,0,0,0,0, 0,0,0,0,0,0, q,p,0,0,0,0, 0,0,0,0,0,0;
# 2-3__
0,0,0,0,0,0, 0,0,0,0,0,0, 0,q,0,0,0,0, 1,0,0,0,0,0;
# 1-4
0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,p,0, 0,1,0,0,0,0;
# 4-2
0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,q,p, 0,0,0,0,0,0;
# 3-3_
0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,q, 0,0,0,1,0,0;
# 2-4
0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,p,0,1,0;
# 4-3
0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,q,0,0,1]; # 3-4
v7 = (A**7) * v;
w = v7(11)+v7(16)+v7(20)+v7(23)
endfunction
Using this script, we get that when the American League has a p = 0.55 probability of winning
each game then their probability of winning the ﬁrst-to-win-four series is 0.60829.
When their
probability of winning any one game is p = 0.6 then their probability of winning the series is
0.71021.
(b) This Octave session
octave:1> v0=[1;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0];
octave:2> x=(.01:.01:.99)’;
octave:3> y=(.01:.01:.99)’;
octave:4> for i=.01:.01:.99
>
y(100*i)=markov(i,v0);
>
endfor
octave:5> z=[x, y];
octave:6> gplot z
yields this graph. By eye we judge that if p > 0.7 then the team is close to assurred of the series.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1
line 1
7
(a) They must satisfy this condition because the total probability of a state transition (including
back to the same state) is 100%.
(b) See the answer to the third item.
(c) We will do the 2×2 case; bigger-sized cases are just notational problems. This product
µ
a1,1
a1,2
a2,1
a2,2
¶ µ
b1,1
b1,2
b2,1
b2,2
¶
=
µ
a1,1b1,1 + a1,2b2,1
a1,1b1,2 + a1,2b2,2
a2,1b1,1 + a2,2b2,1
a2,1b1,2 + a2,2b2,2
¶
has these two column sums
(a1,1b1,1 +a1,2b2,1)+(a2,1b1,1 +a2,2b2,1) = (a1,1 +a2,1)·b1,1 +(a1,2 +a2,2)·b2,1 = 1·b1,1 +1·b2,1 = 1
and
(a1,1b1,2 +a1,2b2,2)+(a2,1b1,2 +a2,2b2,2) = (a1,1 +a2,1)·b1,2 +(a1,2 +a2,2)·b2,2 = 1·b1,2 +1·b2,2 = 1
as required.

Answers to Exercises
371
Topic: Orthonormal Matrices
1
(a) Yes.
(b) No, the columns do not have length one.
(c) Yes.
2
Some of these are nonlinear, because they involve a nontrivial translation.
(a)
µ
x
y
¶
7→
µ
x · cos(π/6) −y · sin(π/6)
x · sin(π/6) + y · cos(π/6)
¶
+
µ
0
1
¶
=
µ
x · (
√
3/2) −y · (1/2) + 0
x · (1/2) + y · cos(
√
3/2) + 1
¶
(b) The line y = 2x makes an angle of arctan(2/1) with the x-axis. Thus sin θ = 2/
√
5 and cos θ =
1/
√
5.
µ
x
y
¶
7→
µ
x · (1/
√
5) −y · (2/
√
5)
x · (2/
√
5) + y · (1/
√
5)
¶
(c)
µ
x
y
¶
7→
µ
x · (1/
√
5) −y · (−2/
√
5)
x · (−2/
√
5) + y · (1/
√
5)
¶
+
µ
1
1
¶
=
µ
x/
√
5 + 2y/
√
5 + 1
−2x/
√
5 + y/
√
5 + 1
¶
3
(a) Let f be distance-preserving and consider f −1. Any two points in the codomain can be written
as f(P1) and f(P2). Because f is distance-preserving, the distance from f(P1) to f(P2) equals the
distance from P1 to P2. But this is exactly what is required for f −1 to be distance-preserving.
(b) Any plane ﬁgure F is congruent to itself via the identity map id: R2 →R2, which is obviously
distance-preserving. If F1 is congruent to F2 (via some f) then F2 is congruent to F1 via f −1, which
is distance-preserving by the prior item. Finally, if F1 is congruent to F2 (via some f) and F2 is
congruent to F3 (via some g) then F1 is congruent to F3 via g ◦f, which is easily checked to be
distance-preserving.
4
The ﬁrst two components of each are ax + cy + e and bx + dy + f.
5
(a) The Pythagorean Theorem gives that three points are colinear if and only if (for some ordering of
them into P1, P2, and P3), dist(P1, P2) + dist(P2, P3) = dist(P1, P3). Of course, where f is distance-
preserving, this holds if and only if dist(f(P1), f(P2)) + dist(f(P2), f(P3)) = dist(f(P1), f(P3)),
which, again by Pythagoras, is true if and only if f(P1), f(P2), and f(P3) are colinear.
The argument for betweeness is similar (above, P2 is between P1 and P3).
If the ﬁgure F is a triangle then it is the union of three line segments P1P2, P2P3, and P1P3.
The prior two paragraphs together show that the property of being a line segment is invariant. So
f(F) is the union of three line segments, and so is a triangle.
A circle C centered at P and of radius r is the set of all points Q such that dist(P, Q) = r.
Applying the distance-preserving map f gives that the image f(C) is the set of all f(Q) subject to
the condition that dist(P, Q) = r. Since dist(P, Q) = dist(f(P), f(Q)), the set f(C) is also a circle,
with center f(P) and radius r.
(b) Here are two that are easy to verify: (i) the property of being a right triangle, and (ii) the
property of two lines being parallel.
(c) One that was mentioned in the section is the ‘sense’ of a ﬁgure. A triangle whose vertices read
clockwise as P1, P2, P3 may, under a distance-preserving map, be sent to a triangle read P1, P2, P3
counterclockwise.


Chapter Four: Determinants
Subsection Four.I.1: Exploration
Four.I.1.1
(a) 4
(b) 3
(c) −12
Four.I.1.2
(a) 6
(b) 21
(c) 27
Four.I.1.3
For the ﬁrst, apply the formula in this section, note that any term with a d, g, or h is zero,
and simplify. Lower-triangular matrices work the same way.
Four.I.1.4
(a) Nonsingular, the determinant is −1.
(b) Nonsingular, the determinant is −1.
(c) Singular, the determinant is 0.
Four.I.1.5
(a) Nonsingular, the determinant is 3.
(b) Singular, the determinant is 0.
(c) Singular, the determinant is 0.
Four.I.1.6
(a) det(B) = det(A) via −2ρ1 + ρ2
(b) det(B) = −det(A) via ρ2 ↔ρ3
(c) det(B) = (1/2) · det(A) via (1/2)ρ2
Four.I.1.7
Using the formula for the determinant of a 3×3 matrix we expand the left side
1 · b · c2 + 1 · c · a2 + 1 · a · b2 −b2 · c · 1 −c2 · a · 1 −a2 · b · 1
and by distributing we expand the right side.
(bc −ba −ac + a2) · (c −b) = c2b −b2c −bac + b2a −ac2 + acb + a2c −a2b
Now we can just check that the two are equal. (Remark. This is the 3×3 case of Vandermonde’s
determinant which arises in applications).
Four.I.1.8
This equation
0 = det(
µ
12 −x
4
8
8 −x
¶
) = 64 −20x + x2 = (x −16)(x −4)
has roots x = 16 and x = 4.
Four.I.1.9
We ﬁrst reduce the matrix to echelon form. To begin, assume that a ̸= 0 and that ae−bd ̸=
0.
(1/a)ρ1
−→


1
b/a
c/a
d
e
f
g
h
i


−dρ1+ρ2
−→
−gρ1+ρ3


1
b/a
c/a
0
(ae −bd)/a
(af −cd)/a
0
(ah −bg)/a
(ai −cg)/a


(a/(ae−bd))ρ2
−→


1
b/a
c/a
0
1
(af −cd)/(ae −bd)
0
(ah −bg)/a
(ai −cg)/a


This step ﬁnishes the calculation.
((ah−bg)/a)ρ2+ρ3
−→


1
b/a
c/a
0
1
(af −cd)/(ae −bd)
0
0
(aei + bgf + cdh −hfa −idb −gec)/(ae −bd)


Now assuming that a ̸= 0 and ae −bd ̸= 0, the original matrix is nonsingular if and only if the 3, 3
entry above is nonzero. That is, under the assumptions, the original matrix is nonsingular if and only
if aei + bgf + cdh −hfa −idb −gec ̸= 0, as required.
We ﬁnish by running down what happens if the assumptions that were taken for convienence in
the prior paragraph do not hold. First, if a ̸= 0 but ae −bd = 0 then we can swap


1
b/a
c/a
0
0
(af −cd)/a
0
(ah −bg)/a
(ai −cg)/a

ρ2↔ρ3
−→


1
b/a
c/a
0
(ah −bg)/a
(ai −cg)/a
0
0
(af −cd)/a



374
Linear Algebra, by Hefferon
and conclude that the matrix is nonsingular if and only if either ah −bg = 0 or af −cd = 0. The
condition ‘ah−bg = 0 or af −cd = 0’ is equivalent to the condition ‘(ah−bg)(af −cd) = 0’. Multiplying
out and using the case assumption that ae −bd = 0 to substitute ae for bd gives this.
0 = ahaf −ahcd −bgaf + bgcd = ahaf −ahcd −bgaf + aegc = a(haf −hcd −bgf + egc)
Since a ̸= 0, we have that the matrix is nonsingular if and only if haf −hcd−bgf +egc = 0. Therefore,
in this a ̸= 0 and ae−bd = 0 case, the matrix is nonsingular when haf −hcd−bgf +egc−i(ae−bd) = 0.
The remaining cases are routine. Do the a = 0 but d ̸= 0 case and the a = 0 and d = 0 but g ̸= 0
case by ﬁrst swapping rows and then going on as above. The a = 0, d = 0, and g = 0 case is easy —
that matrix is singular since the columns form a linearly dependent set, and the determinant comes
out to be zero.
Four.I.1.10
Figuring the determinant and doing some algebra gives this.
0 = y1x + x2y + x1y2 −y2x −x1y −x2y1
(x2 −x1) · y = (y2 −y1) · x + x2y1 −x1y2
y = y2 −y1
x2 −x1
· x + x2y1 −x1y2
x2 −x1
Note that this is the equation of a line (in particular, in contains the familiar expression for the slope),
and note that (x1, y1) and (x2, y2) satisfy it.
Four.I.1.11
(a) The comparison with the formula given in the preamble to this section is easy.
(b) While it holds for 2×2 matrices
µ
h1,1
h1,2
h1,1
h2,1
h2,2
h2,1
¶
= h1,1h2,2 + h1,2h2,1
−h2,1h1,2 −h2,2h1,1
= h1,1h2,2 −h1,2h2,1
it does not hold for 4×4 matrices. An example is that this matrix is singular because the second
and third rows are equal




1
0
0
1
0
1
1
0
0
1
1
0
−1
0
0
1




but following the scheme of the mnemonic does not give zero.




1
0
0
1
1
0
0
0
1
1
0
0
1
1
0
1
1
0
0
1
1
−1
0
0
1
−1
0
0



= 1 + 0 + 0 + 0
−(−1) −0 −0 −0
Four.I.1.12
The determinant is (x2y3 −x3y2)⃗e1 + (x3y1 −x1y3)⃗e2 + (x1y2 −x2y1)⃗e3. To check per-
pendicularity, we check that the dot product with the ﬁrst vector is zero


x1
x2
x3




x2y3 −x3y2
x3y1 −x1y3
x1y2 −x2y1

= x1x2y3 −x1x3y2 + x2x3y1 −x1x2y3 + x1x3y2 −x2x3y1 = 0
and the dot product with the second vector is also zero.


y1
y2
y3




x2y3 −x3y2
x3y1 −x1y3
x1y2 −x2y1

= x2y1y3 −x3y1y2 + x3y1y2 −x1y2y3 + x1y2y3 −x2y1y3 = 0
Four.I.1.13
(a) Plug and chug: the determinant of the product is this
det(
µ
a
b
c
d
¶ µ
w
x
y
z
¶
) = det(
µ
aw + by
ax + bz
cw + dy
cx + dz
¶
)
= acwx + adwz + bcxy + bdyz
−acwx −bcwz −adxy −bdyz
while the product of the determinants is this.
det(
µ
a
b
c
d
¶
) · det(
µ
w
x
y
z
¶
) = (ad −bc) · (wz −xy)
Veriﬁcation that they are equal is easy.

Answers to Exercises
375
(b) Use the prior item.
That similar matrices have the same determinant is immediate from the above two: det(PTP −1) =
det(P) · det(T) · det(P −1).
Four.I.1.14
One way is to count these areas
y1
y2
x2
x1
A
B
C
D
E
F
by taking the area of the entire rectangle and subtracting the area of A the upper-left rectangle, B
the upper-middle triangle, D the upper-right triangle, C the lower-left triangle, E the lower-middle
triangle, and F the lower-right rectangle (x1+x2)(y1+y2)−x2y1−(1/2)x1y1−(1/2)x2y2−(1/2)x2y2−
(1/2)x1y1 −x2y1. Simpliﬁcation gives the determinant formula.
This determinant is the negative of the one above; the formula distinguishes whether the second
column is counterclockwise from the ﬁrst.
Four.I.1.15
The computation for 2×2 matrices, using the formula quoted in the preamble, is easy. It
does also hold for 3×3 matrices; the computation is routine.
Four.I.1.16
No. Recall that constants come out one row at a time.
det(
µ
2
4
2
6
¶
) = 2 · det(
µ
1
2
2
6
¶
) = 2 · 2 · det(
µ
1
2
1
3
¶
)
This contradicts linearity (here we didn’t need S, i.e., we can take S to be the zero matrix).
Four.I.1.17
Bring out the c’s one row at a time.
Four.I.1.18
There are no real numbers θ that make the matrix singular because the determinant of the
matrix cos2 θ +sin2 θ is never 0, it equals 1 for all θ. Geometrically, with respect to the standard basis,
this matrix represents a rotation of the plane through an angle of θ. Each such map is one-to-one —
for one thing, it is invertible.
Four.I.1.19
This is how the answer was given in the cited source. Let P be the sum of the three
positive terms of the determinant and −N the sum of the three negative terms. The maximum value
of P is
9 · 8 · 7 + 6 · 5 · 4 + 3 · 2 · 1 = 630.
The minimum value of N consistent with P is
9 · 6 · 1 + 8 · 5 · 2 + 7 · 4 · 3 = 218.
Any change in P would result in lowering that sum by more than 4. Therefore 412 the maximum value
for the determinant and one form for the determinant is
¯¯¯¯¯¯
9
4
2
3
8
6
5
1
7
¯¯¯¯¯¯
.
Subsection Four.I.2: Properties of Determinants
Four.I.2.7
(a)
¯¯¯¯¯¯
3
1
2
3
1
0
0
1
4
¯¯¯¯¯¯
=
¯¯¯¯¯¯
3
1
2
0
0
−2
0
1
4
¯¯¯¯¯¯
= −
¯¯¯¯¯¯
3
1
2
0
1
4
0
0
−2
¯¯¯¯¯¯
= 6
(b)
¯¯¯¯¯¯¯¯
1
0
0
1
2
1
1
0
−1
0
1
0
1
1
1
0
¯¯¯¯¯¯¯¯
=
¯¯¯¯¯¯¯¯
1
0
0
1
0
1
1
−2
0
0
1
1
0
1
1
−1
¯¯¯¯¯¯¯¯
=
¯¯¯¯¯¯¯¯
1
0
0
1
0
1
1
−2
0
0
1
1
0
0
0
1
¯¯¯¯¯¯¯¯
= 1

376
Linear Algebra, by Hefferon
Four.I.2.8
(a)
¯¯¯¯
2
−1
−1
−1
¯¯¯¯ =
¯¯¯¯
2
−1
0
−3/2
¯¯¯¯ = −3;
(b)
¯¯¯¯¯¯
1
1
0
3
0
2
5
2
2
¯¯¯¯¯¯
=
¯¯¯¯¯¯
1
1
0
0
−3
2
0
−3
2
¯¯¯¯¯¯
=
¯¯¯¯¯¯
1
1
0
0
−3
2
0
0
0
¯¯¯¯¯¯
= 0
Four.I.2.9
When is the determinant not zero?
¯¯¯¯¯¯¯¯
1
0
1
−1
0
1
−2
0
1
0
k
0
0
0
1
−1
¯¯¯¯¯¯¯¯
=
¯¯¯¯¯¯¯¯
1
0
1
−1
0
1
−2
0
0
0
k −1
1
0
0
1
−1
¯¯¯¯¯¯¯¯
Obviously, k = 1 gives nonsingularity and hence a nonzero determinant. If k ̸= 1 then we get echelon
form with a (−1/k −1)ρ3 + ρ4 pivot.
=
¯¯¯¯¯¯¯¯
1
0
1
−1
0
1
−2
0
0
0
k −1
1
0
0
0
−1 −(1/k −1)
¯¯¯¯¯¯¯¯
Multiplying down the diagonal gives (k −1)(−1 −(1/k −1)) = −(k −1) −1 = −k. Thus the matrix
has a nonzero determinant, and so the system has a unique solution, if and only if k ̸= 0.
Four.I.2.10
(a) Property (2) of the deﬁnition of determinants applies via the swap ρ1 ↔ρ3.
¯¯¯¯¯¯
h3,1
h3,2
h3,3
h2,1
h2,2
h2,3
h1,1
h1,2
h1,3
¯¯¯¯¯¯
= −
¯¯¯¯¯¯
h1,1
h1,2
h1,3
h2,1
h2,2
h2,3
h3,1
h3,2
h3,3
¯¯¯¯¯¯
(b) Property (3) applies.
¯¯¯¯¯¯
−h1,1
−h1,2
−h1,3
−2h2,1
−2h2,2
−2h2,3
−3h3,1
−3h3,2
−3h3,3
¯¯¯¯¯¯
= (−1) · (−2) · (−3) ·
¯¯¯¯¯¯
h1,1
h1,2
h1,3
h2,1
h2,2
h2,3
h3,1
h3,2
h3,3
¯¯¯¯¯¯
= (−6) ·
¯¯¯¯¯¯
h1,1
h1,2
h1,3
h2,1
h2,2
h2,3
h3,1
h3,2
h3,3
¯¯¯¯¯¯
(c)
¯¯¯¯¯¯
h1,1 + h3,1
h1,2 + h3,2
h1,3 + h3,3
h2,1
h2,2
h2,3
5h3,1
5h3,2
5h3,3
¯¯¯¯¯¯
= 5 ·
¯¯¯¯¯¯
h1,1 + h3,1
h1,2 + h3,2
h1,3 + h3,3
h2,1
h2,2
h2,3
h3,1
h3,2
h3,3
¯¯¯¯¯¯
= 5 ·
¯¯¯¯¯¯
h1,1
h1,2
h1,3
h2,1
h2,2
h2,3
h3,1
h3,2
h3,3
¯¯¯¯¯¯
Four.I.2.11
A diagonal matrix is in echelon form, so the determinant is the product down the diagonal.
Four.I.2.12
It is the trivial subspace.
Four.I.2.13
Pivoting by adding the second row to the ﬁrst gives a matrix whose ﬁrst row is x + y + z
times its third row.
Four.I.2.14
(a)
¡
1
¢
,
µ
1
−1
−1
1
¶
,


1
−1
1
−1
1
−1
1
−1
1


(b) The determinant in the 1×1 case is 1. In every other case the second row is the negative of the
ﬁrst, and so matrix is singular and the determinant is zero.
Four.I.2.15
(a)
¡
2
¢
,
µ
2
3
3
4
¶
,


2
3
4
3
4
5
4
5
6


(b) The 1×1 and 2×2 cases yield these.
¯¯2
¯¯ = 2
¯¯¯¯
2
3
3
4
¯¯¯¯ = −1
And n×n matrices with n ≥3 are singular, e.g.,
¯¯¯¯¯¯
2
3
4
3
4
5
4
5
6
¯¯¯¯¯¯
= 0
because twice the second row minus the ﬁrst row equals the third row. Checking this is routine.

Answers to Exercises
377
Four.I.2.16
This one
A = B =
µ
1
2
3
4
¶
is easy to check.
|A + B| =
¯¯¯¯
2
4
6
8
¯¯¯¯ = −8
|A| + |B| = −2 −2 = −4
By the way, this also gives an example where scalar multiplication is not preserved |2 · A| ̸= 2 · |A|.
Four.I.2.17
No, we cannot replace it. Remark 2.2 shows that the four conditions after the replacement
would conﬂict — no function satisﬁes all four.
Four.I.2.18
A upper-triangular matrix is in echelon form.
A lower-triangular matrix is either singular or nonsingular. If it is singular then it has a zero on
its diagonal and so its determinant (namely, zero) is indeed the product down its diagonal. If it is
nonsingular then it has no zeroes on its diagonal, and can be reduced by Gauss’ method to echelon
form without changing the diagonal.
Four.I.2.19
(a) The properties in the deﬁnition of determinant show that |Mi(k)| = k, |Pi,j| = −1,
and |Ci,j(k)| = 1.
(b) The three cases are easy to check by recalling the action of left multiplication by each type of
matrix.
(c) If TS is invertible (TS)M = I then the associative property of matrix multiplication T(SM) = I
shows that T is invertible. So if T is not invertible then neither is TS.
(d) If T is singular then apply the prior answer: |TS| = 0 and |T|·|S| = 0·|S| = 0. If T is not singular
then it can be written as a product of elementary matrices |TS| = |Er · · · E1S| = |Er| · · · |E1| · |S| =
|Er · · · E1||S| = |T||S|.
(e) 1 = |I| = |T · T −1| = |T||T −1|
Four.I.2.20
(a) We must show that if
T
kρi+ρj
−→
ˆT
then d(T) = |TS|/|S| = | ˆTS|/|S| = d( ˆT).
We will be done if we show that pivoting ﬁrst and
then multiplying to get ˆTS gives the same result as multiplying ﬁrst to get TS and then pivoting
(because the determinant |TS| is unaﬀected by the pivot so we’ll then have | ˆTS| = |TS|, and hence
d( ˆT) = d(T)). That argument runs: after adding k times row i of TS to row j of TS, the j, p entry
is (kti,1 + tj,1)s1,p + · · · + (kti,r + tj,r)sr,p, which is the j, p entry of ˆTS.
(b) We need only show that swapping T
ρi↔ρj
−→ˆT and then multiplying to get ˆTS gives the same result
as multiplying T by S and then swapping (because, as the determinant |TS| changes sign on the
row swap, we’ll then have | ˆTS| = −|TS|, and so d( ˆT) = −d(T)). That argument runs just like the
prior one.
(c) Not surprisingly by now, we need only show that multiplying a row by a nonzero scalar T
kρi
−→ˆT
and then computing ˆTS gives the same result as ﬁrst computing TS and then multiplying the row
by k (as the determinant |TS| is rescaled by k the multiplication, we’ll have | ˆTS| = k|TS|, so
d( ˆT) = k d(T)). The argument runs just as above.
(d) Clear.
(e) Because we’ve shown that d(T) is a determinant and that determinant functions (if they exist)
are unique, we have that so |T| = d(T) = |TS|/|S|.
Four.I.2.21
We will ﬁrst argue that a rank r matrix has a r×r submatrix with nonzero determinant.
A rank r matrix has a linearly independent set of r rows. A matrix made from those rows will have
row rank r and thus has column rank r. Conclusion: from those r rows can be extracted a linearly
independent set of r columns, and so the original matrix has a r×r submatrix of rank r.
We ﬁnish by showing that if r is the largest such integer then the rank of the matrix is r. We need
only show, by the maximality of r, that if a matrix has a k×k submatrix of nonzero determinant then
the rank of the matrix is at least k. Consider such a k×k submatrix. Its rows are parts of the rows
of the original matrix, clearly the set of whole rows is linearly independent. Thus the row rank of the
original matrix is at least k, and the row rank of a matrix equals its rank.
Four.I.2.22
A matrix with only rational entries can be reduced with Gauss’ method to an echelon
form matrix using only rational arithmetic. Thus the entries on the diagonal must be rationals, and
so the product down the diagonal is rational.

378
Linear Algebra, by Hefferon
Four.I.2.23
This is how the answer was given in the cited source. The value (1−a4)3 of the determinant
is independent of the values B, C, D. Hence operation (e) does not change the value of the determinant
but merely changes its appearance. Thus the element of likeness in (a), (b), (c), (d), and (e) is only
that the appearance of the principle entity is changed. The same element appears in (f) changing the
name-label of a rose, (g) writing a decimal integer in the scale of 12, (h) gilding the lily, (i) whitewashing
a politician, and (j) granting an honorary degree.
Subsection Four.I.3: The Permutation Expansion
Four.I.3.14
(a) This matrix is singular.
¯¯¯¯¯¯
1
2
3
4
5
6
7
8
9
¯¯¯¯¯¯
= (1)(5)(9) |Pφ1| + (1)(6)(8) |Pφ2| + (2)(4)(9) |Pφ3|
+ (2)(6)(7) |Pφ4| + (3)(4)(8) |Pφ5| + (7)(5)(3) |Pφ6|
= 0
(b) This matrix is nonsingular.
¯¯¯¯¯¯
2
2
1
3
−1
0
−2
0
5
¯¯¯¯¯¯
= (2)(−1)(5) |Pφ1| + (2)(0)(0) |Pφ2| + (2)(3)(5) |Pφ3|
+ (2)(0)(−2) |Pφ4| + (1)(3)(0) |Pφ5| + (−2)(−1)(1) |Pφ6|
= −42
Four.I.3.15
(a) Gauss’ method gives this
¯¯¯¯
2
1
3
1
¯¯¯¯ =
¯¯¯¯
2
1
0
−1/2
¯¯¯¯ = −1
and permutation expansion gives this.
¯¯¯¯
2
1
3
1
¯¯¯¯ =
¯¯¯¯
2
0
0
1
¯¯¯¯ +
¯¯¯¯
0
1
3
0
¯¯¯¯ = (2)(1)
¯¯¯¯
1
0
0
1
¯¯¯¯ + (1)(3)
¯¯¯¯
0
1
1
0
¯¯¯¯ = −1
(b) Gauss’ method gives this
¯¯¯¯¯¯
0
1
4
0
2
3
1
5
1
¯¯¯¯¯¯
= −
¯¯¯¯¯¯
1
5
1
0
2
3
0
1
4
¯¯¯¯¯¯
= −
¯¯¯¯¯¯
1
5
1
0
2
3
0
0
5/2
¯¯¯¯¯¯
= −5
and the permutation expansion gives this.
¯¯¯¯¯¯
0
1
4
0
2
3
1
5
1
¯¯¯¯¯¯
= (0)(2)(1) |Pφ1| + (0)(3)(5) |Pφ2| + (1)(0)(1) |Pφ3|
+ (1)(3)(1) |Pφ4| + (4)(0)(5) |Pφ5| + (1)(2)(0) |Pφ6|
= −5
Four.I.3.16
Following Example 3.6 gives this.
¯¯¯¯¯¯
t1,1
t1,2
t1,3
t2,1
t2,2
t2,3
t3,1
t3,2
t3,3
¯¯¯¯¯¯
= t1,1t2,2t3,3 |Pφ1| + t1,1t2,3t3,2 |Pφ2|
+ t1,2t2,1t3,3 |Pφ3| + t1,2t2,3t3,1 |Pφ4|
+ t1,3t2,1t3,2 |Pφ5| + t1,3t2,2t3,1 |Pφ6|
= t1,1t2,2t3,3(+1) + t1,1t2,3t3,2(−1)
+ t1,2t2,1t3,3(−1) + t1,2t2,3t3,1(+1)
+ t1,3t2,1t3,2(+1) + t1,3t2,2t3,1(−1)
Four.I.3.17
This is all of the permutations where φ(1) = 1
φ1 = ⟨1, 2, 3, 4⟩
φ2 = ⟨1, 2, 4, 3⟩
φ3 = ⟨1, 3, 2, 4⟩
φ4 = ⟨1, 3, 4, 2⟩
φ5 = ⟨1, 4, 2, 3⟩
φ6 = ⟨1, 4, 3, 2⟩
the ones where φ(1) = 1
φ7 = ⟨2, 1, 3, 4⟩
φ8 = ⟨2, 1, 4, 3⟩
φ9 = ⟨2, 3, 1, 4⟩
φ10 = ⟨2, 3, 4, 1⟩
φ11 = ⟨2, 4, 1, 3⟩
φ12 = ⟨2, 4, 3, 1⟩

Answers to Exercises
379
the ones where φ(1) = 3
φ13 = ⟨3, 1, 2, 4⟩
φ14 = ⟨3, 1, 4, 2⟩
φ15 = ⟨3, 2, 1, 4⟩
φ16 = ⟨3, 2, 4, 1⟩
φ17 = ⟨3, 4, 1, 2⟩
φ18 = ⟨3, 4, 2, 1⟩
and the ones where φ(1) = 4.
φ19 = ⟨4, 1, 2, 3⟩
φ20 = ⟨4, 1, 3, 2⟩
φ21 = ⟨4, 2, 1, 3⟩
φ22 = ⟨4, 2, 3, 1⟩
φ23 = ⟨4, 3, 1, 2⟩
φ24 = ⟨4, 3, 2, 1⟩
Four.I.3.18
Each of these is easy to check.
(a)
permutation
φ1
φ2
inverse
φ1
φ2
(b)
permutation
φ1
φ2
φ3
φ4
φ5
φ6
inverse
φ1
φ2
φ3
φ5
φ4
φ6
Four.I.3.19
For the ‘if’ half, the ﬁrst condition of Deﬁnition 3.2 follows from taking k1 = k2 = 1 and
the second condition follows from taking k2 = 0.
The ‘only if’ half also routine. From f(⃗ρ1, . . . , k1⃗v1 + k2⃗v2, . . . , ⃗ρn) the ﬁrst condition of Deﬁni-
tion 3.2 gives = f(⃗ρ1, . . . , k1⃗v1, . . . , ⃗ρn) + f(⃗ρ1, . . . , k2⃗v2, . . . , ⃗ρn) and the second condition, applied
twice, gives the result.
Four.I.3.20
To get a nonzero term in the permutation expansion we must use the 1, 2 entry and the
4, 3 entry. Having ﬁxed on those two we must also use the 2, 1 entry and the the 3, 4 entry. The signum
of ⟨2, 1, 4, 3⟩is +1 because from




0
1
0
0
1
0
0
0
0
0
0
1
0
0
1
0




the two rwo swaps ρ1 ↔ρ2 and ρ3 ↔ρ4 will produce the identity matrix.
Four.I.3.21
They would all double.
Four.I.3.22
For the second statement, given a matrix, transpose it, swap rows, and transpose back.
The result is swapped columns, and the determinant changes by a factor of −1. The third statement
is similar: given a matrix, transpose it, apply multilinearity to what are now rows, and then transpose
back the resulting matrices.
Four.I.3.23
An n×n matrix with a nonzero determinant has rank n so its columns form a basis for
Rn.
Four.I.3.24
False.
¯¯¯¯
1
−1
1
1
¯¯¯¯ = 2
Four.I.3.25
(a) For the column index of the entry in the ﬁrst row there are ﬁve choices. Then, for
the column index of the entry in the second row there are four choices (the column index used in the
ﬁrst row cannot be used here). Continuing, we get 5 · 4 · 3 · 2 · 1 = 120. (See also the next question.)
(b) Once we choose the second column in the ﬁrst row, we can choose the other entries in 4·3·2·1 = 24
ways.
Four.I.3.26
n · (n −1) · · · 2 · 1 = n!
Four.I.3.27
In |A| = |Atrans| = | −A| = (−1)n|A| the exponent n must be even.
Four.I.3.28
Showing that no placement of three zeros suﬃces is routine. Four zeroes does suﬃce; put
them all in the same row or column.
Four.I.3.29
The n = 3 case shows what to do. The pivot operations of −x1ρ2 + ρ3 and −x1ρ1 + ρ2
give this.
¯¯¯¯¯¯
1
1
1
x1
x2
x3
x2
1
x2
2
x2
3
¯¯¯¯¯¯
=
¯¯¯¯¯¯
1
1
1
x1
x2
x3
0
(−x1 + x2)x2
(−x1 + x3)x3
¯¯¯¯¯¯
=
¯¯¯¯¯¯
1
1
1
0
−x1 + x2
−x1 + x3
0
(−x1 + x2)x2
(−x1 + x3)x3
¯¯¯¯¯¯
Then the pivot operation of x2ρ2 + ρ3 gives the desired result.
=
¯¯¯¯¯¯
1
1
1
0
−x1 + x2
−x1 + x3
0
0
(−x1 + x3)(−x2 + x3)
¯¯¯¯¯¯
= (x2 −x1)(x3 −x1)(x3 −x2)

380
Linear Algebra, by Hefferon
Four.I.3.30
Let T be n×n, let J be p×p, and let K be q×q. Apply the permutation expansion formula
|T| =
X
permutations φ
t1,φ(1)t2,φ(2) . . . tn,φ(n) |Pφ|
Because the upper right of T is all zeroes, if a φ has at least one of p+1, . . . , n among its ﬁrst p column
numbers φ(1), . . . , φ(p) then the term arising from φ is 0 (e.g., if φ(1) = n then t1,φ(1)t2,φ(2) . . . tn,φ(n)
is 0). So the above formula reduces to a sum over all permutations with two halves: ﬁrst 1, . . . , p are
rearranged, and after that comes a permutation of p+1, . . . , p+q. To see this gives |J|·|K|, distribute.
·
X
perms φ1
of 1,...,p
t1,φ1(1) · · · tp,φ1(p) |Pφ1|
¸
·
·
X
perms φ2
of p+1,...,p+q
tp+1,φ2(p+1) · · · tp+q,φ2(p+q) |Pφ2|
¸
Four.I.3.31
The n = 3 case shows what happens.
|T −rI| =
¯¯¯¯¯¯
t1,1 −x
t1,2
t1,3
t2,1
t2,2 −x
t2,3
t3,1
t3,2
t3,3 −x
¯¯¯¯¯¯
Each term in the permutation expansion has three factors drawn from entries in the matrix (e.g.,
(t1,1 −x)(t2,2 −x)(t3,3 −x) and (t1,1 −x)(t2,3)(t3,2)), and so the determinant is expressible as a
polynomial in x of degree 3. Such a polynomial has at most 3 roots.
In general, the permutation expansion shows that the determinant can be written as a sum of
terms, each with n factors, giving a polynomial of degree n. A polynomial of degree n has at most n
roots.
Four.I.3.32
This is how the answer was given in the cited source. When two rows of a determinant are
interchanged, the sign of the determinant is changed. When the rows of a three-by-three determinant
are permuted, 3 positive and 3 negative determinants equal in absolute value are obtained. Hence the
9! determinants fall into 9!/6 groups, each of which sums to zero.
Four.I.3.33
This is how the answer was given in the cited source. When the elements of any column
are subtracted from the elements of each of the other two, the elements in two of the columns of the
derived determinant are proportional, so the determinant vanishes. That is,
¯¯¯¯¯¯
2
1
x −4
4
2
x −3
6
3
x −10
¯¯¯¯¯¯
=
¯¯¯¯¯¯
1
x −3
−1
2
x −1
−2
3
x −7
−3
¯¯¯¯¯¯
=
¯¯¯¯¯¯
x −2
−1
−2
x + 1
−2
−4
x −4
−3
−6
¯¯¯¯¯¯
= 0.
Four.I.3.34
This is how the answer was given in the cited source. Let
a
b
c
d
e
f
g
h
i
have magic sum N = S/3. Then
N = (a + e + i) + (d + e + f) + (g + e + c)
−(a + d + g) −(c + f + i) = 3e
and S = 9e. Hence, adding rows and columns,
D =
¯¯¯¯¯¯
a
b
c
d
e
f
g
h
i
¯¯¯¯¯¯
=
¯¯¯¯¯¯
a
b
c
d
e
f
3e
3e
3e
¯¯¯¯¯¯
=
¯¯¯¯¯¯
a
b
3e
d
e
3e
3e
3e
9e
¯¯¯¯¯¯
=
¯¯¯¯¯¯
a
b
e
d
e
e
1
1
1
¯¯¯¯¯¯
S.
Four.I.3.35
This is how the answer was given in the cited source. Denote by Dn the determinant in
question and by ai,j the element in the i-th row and j-th column. Then from the law of formation of
the elements we have
ai,j = ai,j−1 + ai−1,j,
a1,j = ai,1 = 1.
Subtract each row of Dn from the row following it, beginning the process with the last pair of rows.
After the n −1 subtractions the above equality shows that the element ai,j is replaced by the element
ai,j−1, and all the elements in the ﬁrst column, except a1,1 = 1, become zeroes. Now subtract each
column from the one following it, beginning with the last pair. After this process the element ai,j−1
is replaced by ai−1,j−1, as shown in the above relation. The result of the two operations is to replace
ai,j by ai−1,j−1, and to reduce each element in the ﬁrst row and in the ﬁrst column to zero. Hence
Dn = Dn+i and consequently
Dn = Dn−1 = Dn−2 = · · · = D2 = 1.

Answers to Exercises
381
Subsection Four.I.4: Determinants Exist
Four.I.4.10
This is the permutation expansion of the determinant of a 2×2 matrix
¯¯¯¯
a
b
c
d
¯¯¯¯ = ad ·
¯¯¯¯
1
0
0
1
¯¯¯¯ + bc ·
¯¯¯¯
0
1
1
0
¯¯¯¯
and the permutation expansion of the determinant of its transpose.
¯¯¯¯
a
c
b
d
¯¯¯¯ = ad ·
¯¯¯¯
1
0
0
1
¯¯¯¯ + cb ·
¯¯¯¯
0
1
1
0
¯¯¯¯
As with the 3×3 expansions described in the subsection, the permutation matrices from corresponding
terms are transposes (although this is disguised by the fact that each is self-transpose).
Four.I.4.11
Each of these is easy to check.
(a)
permutation
φ1
φ2
inverse
φ1
φ2
(b)
permutation
φ1
φ2
φ3
φ4
φ5
φ6
inverse
φ1
φ2
φ3
φ5
φ4
φ6
Four.I.4.12
(a) sgn(φ1) = +1, sgn(φ2) = −1
(b) sgn(φ1) = +1, sgn(φ2) = −1, sgn(φ3) = −1, sgn(φ4) = +1, sgn(φ5) = +1, sgn(φ6) = −1
Four.I.4.13
The pattern is this.
i
1
2
3
4
5
6
. . .
sgn(φi)
+1
−1
−1
+1
+1
−1
. . .
So to ﬁnd the signum of φn!, we subtract one n! −1 and look at the remainder on division by four. If
the remainder is 1 or 2 then the signum is −1, otherwise it is +1. For n > 4, the number n! is divisible
by four, so n! −1 leaves a remainder of −1 on division by four (more properly said, a remainder or 3),
and so the signum is +1. The n = 1 case has a signum of +1, the n = 2 case has a signum of −1 and
the n = 3 case has a signum of −1.
Four.I.4.14
(a) Permutations can be viewed as one-one and onto maps φ: {1, . . . , n} →{1, . . . , n}.
Any one-one and onto map has an inverse.
(b) If it always takes an odd number of swaps to get from Pφ to the identity, then it always takes
an odd number of swaps to get from the identity to Pφ (any swap is reversible).
(c) This is the ﬁrst question again.
Four.I.4.15
If φ(i) = j then φ−1(j) = i. The result now follows on the observation that Pφ has a 1 in
entry i, j if and only if φ(i) = j, and Pφ−1 has a 1 in entry j, i if and only if φ−1(j) = i,
Four.I.4.16
This does not say that m is the least number of swaps to produce an identity, nor does it
say that m is the most. It instead says that there is a way to swap to the identity in exactly m steps.
Let ιj be the ﬁrst row that is inverted with respect to a prior row and let ιk be the ﬁrst row giving
that inversion. We have this interval of rows.













...
ιk
ιr1
...
ιrs
ιj
...













j < k < r1 < · · · < rs
Swap.













...
ιj
ιr1
...
ιrs
ιk
...














382
Linear Algebra, by Hefferon
The second matrix has one fewer inversion because there is one fewer inversion in the interval (s vs.
s + 1) and inversions involving rows outside the interval are not aﬀected.
Proceed in this way, at each step reducing the number of inversions by one with each row swap.
When no inversions remain the result is the identity.
The contrast with Corollary 4.6 is that the statement of this exercise is a ‘there exists’ statement:
there exists a way to swap to the identity in exactly m steps. But the corollary is a ‘for all’ statement:
for all ways to swap to the identity, the parity (evenness or oddness) is the same.
Four.I.4.17
(a) First, g(φ1) is the product of the single factor 2 −1 and so g(φ1) = 1. Second, g(φ2)
is the product of the single factor 1 −2 and so g(φ2) = −1.
(b)
permutation φ
φ1
φ2
φ3
φ4
φ5
φ6
g(φ)
2
−2
−2
2
2
−2
(c) Note that φ(j) −φ(i) is negative if and only if ιφ(j) and ιφ(i) are in an inversion of their usual
order.
Subsection Four.II.1: Determinants as Size Functions
Four.II.1.8
For each, ﬁnd the determinant and take the absolute value.
(a) 7
(b) 0
(c) 58
Four.II.1.9
Solving
c1


3
3
1

+ c2


2
6
1

+ c3


1
0
5

=


4
1
2


gives the unique solution c3 = 11/57, c2 = −40/57 and c1 = 99/57. Because c1 > 1, the vector is not
in the box.
Four.II.1.10
Move the parallelepiped to start at the origin, so that it becomes the box formed by
⟨
µ
3
0
¶
,
µ
2
1
¶
⟩
and now the absolute value of this determinant is easily computed as 3.
¯¯¯¯
3
2
0
1
¯¯¯¯ = 3
Four.II.1.11
(a) 3
(b) 9
(c) 1/9
Four.II.1.12
Express each transformation with respect to the standard bases and ﬁnd the determi-
nant.
(a) 6
(b) −1
(c) −5
Four.II.1.13
The starting area is 6 and the matrix changes sizes by −14. Thus the area of the image
is 84.
Four.II.1.14
By a factor of 21/2.
Four.II.1.15
For a box we take a sequence of vectors (as described in the remark, the order in which
the vectors are taken matters), while for a span we take a set of vectors. Also, for a box subset of Rn
there must be n vectors; of course for a span there can be any number of vectors. Finally, for a box
the coeﬃcients t1, . . . , tn are restricted to the interval [0..1], while for a span the coeﬃcients are free
to range over all of R.
Four.II.1.16
That picture is drawn to mislead. The picture on the left is not the box formed by two
vectors. If we slide it to the origin then it becomes the box formed by this sequence.
⟨
µ
0
1
¶
,
µ
2
0
¶
⟩
Then the image under the action of the matrix is the box formed by this sequence.
⟨
µ
1
1
¶
,
µ
4
0
¶
⟩
which has an area of 4.
Four.II.1.17
Yes to both. For instance, the ﬁrst is |TS| = |T| · |S| = |S| · |T| = |ST|.

Answers to Exercises
383
Four.II.1.18
(a) If it is deﬁned then it is (32) · (2) · (2−2) · (3).
(b) |6A3 + 5A2 + 2A| = |A| · |6A2 + 5A + 2I|.
Four.II.1.19
¯¯¯¯
cos θ
−sin θ
sin θ
cos θ
¯¯¯¯ = 1
Four.II.1.20
No, for instance the determinant of
T =
µ
2
0
0
1/2
¶
is 1 so it preserves areas, but the vector T⃗e1 has length 2.
Four.II.1.21
It is zero.
Four.II.1.22
Two of the three sides of the triangle are formed by these vectors.


2
2
2

−


1
2
1

=


1
0
1




3
−1
4

−


1
2
1

=


2
−3
3


One way to ﬁnd the area of this triangle is to produce a length-one vector orthogonal to these two.
From these two relations


1
0
1

·


x
y
z

=


0
0
0




2
−3
3

·


x
y
z

=


0
0
0


we get a system
x
+ z = 0
2x −3y + 3z = 0
−2ρ1+ρ2
−→
x
+ z = 0
−3y + z = 0
with this solution set.
{


−1
1/3
1

z
¯¯ z ∈R},
A solution of length one is this.
1
p
19/9


−1
1/3
1


Thus the area of the triangle is the absolute value of this determinant.
¯¯¯¯¯¯
1
2
−3/
√
19
0
−3
1/
√
19
1
3
3/
√
19
¯¯¯¯¯¯
= −12/
√
19
Four.II.1.23
(a) Because the image of a linearly dependent set is linearly dependent, if the vectors
forming S make a linearly dependent set, so that |S| = 0, then the vectors forming t(S) make a
linearly dependent set, so that |TS| = 0, and in this case the equation holds.
(b) We must check that if T
kρi+ρj
−→ˆT then d(T) = |TS|/|S| = | ˆTS|/|S| = d( ˆT). We can do this by
checking that pivoting ﬁrst and then multiplying to get ˆTS gives the same result as multiplying ﬁrst
to get TS and then pivoting (because the determinant |TS| is unaﬀected by the pivot so we’ll then
have that | ˆTS| = |TS| and hence that d( ˆT) = d(T)). This check runs: after adding k times row i of
TS to row j of TS, the j, p entry is (kti,1 + tj,1)s1,p + · · · + (kti,r + tj,r)sr,p, which is the j, p entry
of ˆTS.
(c) For the second property, we need only check that swapping T
ρi↔ρj
−→ˆT and then multiplying to get
ˆTS gives the same result as multiplying T by S ﬁrst and then swapping (because, as the determinant
|TS| changes sign on the row swap, we’ll then have | ˆTS| = −|TS|, and so d( ˆT) = −d(T)). This
ckeck runs just like the one for the ﬁrst property.
For the third property, we need only show that performing T
kρi
−→ˆT and then computing ˆTS
gives the same result as ﬁrst computing TS and then performing the scalar multiplication (as the
determinant |TS| is rescaled by k, we’ll have | ˆTS| = k|TS| and so d( ˆT) = k d(T)). Here too, the
argument runs just as above.
The fourth property, that if T is I then the result is 1, is obvious.
(d) Determinant functions are unique, so |TS|/|S| = d(T) = |T|, and so |TS| = |T||S|.

384
Linear Algebra, by Hefferon
Four.II.1.24
Any permutation matrix has the property that the transpose of the matrix is its inverse.
For the implication, we know that |Atrans| = |A|. Then 1 = |A·A−1| = |A·Atrans| = |A|·|Atrans| =
|A|2.
The converse does not hold; here is an example.
µ3
1
2
1
¶
Four.II.1.25
Where the sides of the box are c times longer, the box has c3 times as many cubic units
of volume.
Four.II.1.26
If H = P −1GP then |H| = |P −1||G||P| = |P −1||P||G| = |P −1P||G| = |G|.
Four.II.1.27
(a) The new basis is the old basis rotated by π/4.
(b) ⟨
µ
−1
0
¶
,
µ
0
−1
¶
⟩, ⟨
µ
0
−1
¶
,
µ
1
0
¶
⟩
(c) In each case the determinant is +1 (these bases are said to have positive orientation).
(d) Because only one sign can change at a time, the only other cycle possible is
· · · −→
µ
+
+
¶
−→
µ
+
−
¶
−→
µ
−
−
¶
−→
µ
−
+
¶
−→· · · .
Here each associated determinant is −1 (such bases are said to have a negative orientation).
(e) There is one positively oriented basis ⟨(1)⟩and one negatively oriented basis ⟨(−1)⟩.
(f) There are 48 bases (6 half-axis choices are possible for the ﬁrst unit vector, 4 for the second, and
2 for the last). Half are positively oriented like the standard basis on the left below, and half are
negatively oriented like the one on the right
⃗e1
⃗e2
⃗e3
⃗
β1
⃗
β2
⃗
β3
In R3 positive orientation is sometimes called ‘right hand orientation’ because if a person’s right
hand is placed with the ﬁngers curling from ⃗e1 to ⃗e2 then the thumb will point with ⃗e3.
Four.II.1.28
We will compare det(⃗s1, . . . ,⃗sn) with det(t(⃗s1), . . . , t(⃗sn)) to show that the second diﬀers
from the ﬁrst by a factor of |T|. We represent the ⃗s ’s with respect to the standard bases
RepEn(⃗si) =





s1,i
s2,i
...
sn,i





and then we represent the map application with matrix-vector multiplication
RepEn( t(⃗si) ) =





t1,1
t1,2
. . .
t1,n
t2,1
t2,2
. . .
t2,n
...
tn,1
tn,2
. . .
tn,n










s1,j
s2,j
...
sn,j





= s1,j





t1,1
t2,1
...
tn,1




+ s2,j





t1,2
t2,2
...
tn,2




+ · · · + sn,j





t1,n
t2,n
...
tn,n





= s1,j⃗t1 + s2,j⃗t2 + · · · + sn,j⃗tn
where ⃗ti is column i of T. Then det(t(⃗s1), . . . , t(⃗sn)) equals det(s1,1⃗t1+s2,1⃗t2+. . .+sn,1⃗tn, . . . , s1,n⃗t1+
s2,n⃗t2+. . .+sn,n⃗tn).
As in the derivation of the permutation expansion formula, we apply multilinearity, ﬁrst splitting
along the sum in the ﬁrst argument
det(s1,1⃗t1, . . . , s1,n⃗t1 + s2,n⃗t2 + · · · + sn,n⃗tn) + · · · + det(sn,1⃗tn, . . . , s1,n⃗t1 + s2,n⃗t2 + · · · + sn,n⃗tn)
and then splitting each of those n summands along the sums in the second arguments, etc. We end
with, as in the derivation of the permutation expansion, nn summand determinants, each of the form
det(si1,1⃗ti1, si2,2⃗ti2, . . . , sin,n⃗tin). Factor out each of the si,j’s = si1,1si2,2 . . . sin,n·det(⃗ti1,⃗ti2, . . . , ⃗tin).

Answers to Exercises
385
As in the permutation expansion derivation, whenever two of the indices in i1, . . . , in are equal
then the determinant has two equal arguments, and evaluates to 0. So we need only consider the cases
where i1, . . . , in form a permutation of the numbers 1, . . . , n. We thus have
det(t(⃗s1), . . . , t(⃗sn)) =
X
permutations φ
sφ(1),1 . . . sφ(n),n det(⃗tφ(1), . . . ,⃗tφ(n)).
Swap the columns in det(⃗tφ(1), . . . ,⃗tφ(n)) to get the matrix T back, which changes the sign by a factor
of sgn φ, and then factor out the determinant of T.
=
X
φ
sφ(1),1 . . . sφ(n),n det(⃗t1, . . . ,⃗tn) · sgn φ = det(T)
X
φ
sφ(1),1 . . . sφ(n),n · sgn φ.
As in the proof that the determinant of a matrix equals the determinant of its transpose, we commute
the s’s so they are listed by ascending row number instead of by ascending column number (and we
substitute sgn(φ−1) for sgn(φ)).
= det(T)
X
φ
s1,φ−1(1) . . . sn,φ−1(n) · sgn φ−1 = det(T) det(⃗s1,⃗s2, . . . ,⃗sn)
Four.II.1.29
(a) An algebraic check is easy.
0 = xy2 + x2y3 + x3y −x3y2 −xy3 −x2y = x · (y2 −y3) + y · (x3 −x2) + x2y3 −x3y2
simpliﬁes to the familiar form
y = x · (x3 −x2)/(y3 −y2) + (x2y3 −x3y2)/(y3 −y2)
(the y3 −y2 = 0 case is easily handled).
For geometric insight, this picture shows that the box formed by the three vectors. Note that all
three vectors end in the z = 1 plane. Below the two vectors on the right is the line through (x2, y2)
and (x3, y3).
³x
y
1
´
³x2
y2
1
´
³x3
y3
1
´
The box will have a nonzero volume unless the triangle formed by the ends of the three is degenerate.
That only happens (assuming that (x2, y3) ̸= (x3, y3)) if (x, y) lies on the line through the other two.
(b) This is how the answer was given in the cited source. The altitude through (x1, y1) of a triangle
with vertices (x1, y1) (x2, y2) and (x3, y3) is found in the usual way from the normal form of the
above:
1
p
(x2 −x3)2 + (y2 −y3)2
¯¯¯¯¯¯
x1
x2
x3
y1
y2
y3
1
1
1
¯¯¯¯¯¯
.
Another step shows the area of the triangle to be
1
2
¯¯¯¯¯¯
x1
x2
x3
y1
y2
y3
1
1
1
¯¯¯¯¯¯
.
This exposition reveals the modus operandi more clearly than the usual proof of showing a collection
of terms to be identitical with the determinant.
(c) This is how the answer was given in the cited source. Let
D =
¯¯¯¯¯¯
x1
x2
x3
y1
y2
y3
1
1
1
¯¯¯¯¯¯
then the area of the triangle is (1/2)|D|. Now if the coordinates are all integers, then D is an integer.
Subsection Four.III.1: Laplace’s Expansion

386
Linear Algebra, by Hefferon
Four.III.1.13
(a) (−1)2+3
¯¯¯¯
1
0
0
2
¯¯¯¯ = −2
(b) (−1)3+2
¯¯¯¯
1
2
−1
3
¯¯¯¯ = −5
(c) (−1)4
¯¯¯¯
−1
1
0
2
¯¯¯¯ = −2
Four.III.1.14
(a) 3 · (+1)
¯¯¯¯
2
2
3
0
¯¯¯¯ + 0 · (−1)
¯¯¯¯
1
2
−1
0
¯¯¯¯ + 1 · (+1)
¯¯¯¯
1
2
−1
3
¯¯¯¯ = −13
(b) 1 · (−1)
¯¯¯¯
0
1
3
0
¯¯¯¯ + 2 · (+1)
¯¯¯¯
3
1
−1
0
¯¯¯¯ + 2 · (−1)
¯¯¯¯
3
0
−1
3
¯¯¯¯ = −13
(c) 1 · (+1)
¯¯¯¯
1
2
−1
3
¯¯¯¯ + 2 · (−1)
¯¯¯¯
3
0
−1
3
¯¯¯¯ + 0 · (+1)
¯¯¯¯
3
0
1
2
¯¯¯¯ = −13
Four.III.1.15
adj(T) =


T1,1
T2,1
T3,1
T1,2
T2,2
T3,2
T1,3
T2,3
T3,3

=









+
¯¯¯¯
5
6
8
9
¯¯¯¯ −
¯¯¯¯
2
3
8
9
¯¯¯¯ +
¯¯¯¯
2
3
5
6
¯¯¯¯
−
¯¯¯¯
4
6
7
9
¯¯¯¯ +
¯¯¯¯
1
3
7
9
¯¯¯¯ −
¯¯¯¯
1
3
4
6
¯¯¯¯
+
¯¯¯¯
4
5
7
8
¯¯¯¯ −
¯¯¯¯
1
2
7
8
¯¯¯¯ +
¯¯¯¯
1
2
4
5
¯¯¯¯









=


−3
6
−3
6
−12
6
−3
6
−3


Four.III.1.16
(a)


T1,1
T2,1
T3,1
T1,2
T2,2
T3,2
T1,3
T2,3
T3,3

=









¯¯¯¯
0
2
0
1
¯¯¯¯
−
¯¯¯¯
1
4
0
1
¯¯¯¯
¯¯¯¯
1
4
0
2
¯¯¯¯
−
¯¯¯¯
−1
2
1
1
¯¯¯¯
¯¯¯¯
2
4
1
1
¯¯¯¯
−
¯¯¯¯
2
4
−1
2
¯¯¯¯
¯¯¯¯
−1
0
1
0
¯¯¯¯
−
¯¯¯¯
2
1
1
0
¯¯¯¯
¯¯¯¯
2
1
−1
0
¯¯¯¯









=


0
−1
2
3
−2
−8
0
1
1


(b) The minors are 1×1:
µ
T1,1
T2,1
T1,2
T2,2
¶
=
Ã ¯¯4
¯¯
−
¯¯−1
¯¯
−
¯¯2
¯¯
¯¯3
¯¯
!
=
µ
4
1
−2
3
¶
.
(c)
µ
0
−1
−5
1
¶
(d)


T1,1
T2,1
T3,1
T1,2
T2,2
T3,2
T1,3
T2,3
T3,3

=









¯¯¯¯
0
3
8
9
¯¯¯¯
−
¯¯¯¯
4
3
8
9
¯¯¯¯
¯¯¯¯
4
3
0
3
¯¯¯¯
−
¯¯¯¯
−1
3
1
9
¯¯¯¯
¯¯¯¯
1
3
1
9
¯¯¯¯
−
¯¯¯¯
1
3
−1
3
¯¯¯¯
¯¯¯¯
−1
0
1
8
¯¯¯¯
−
¯¯¯¯
1
4
1
8
¯¯¯¯
¯¯¯¯
1
4
−1
0
¯¯¯¯









=


−24
−12
12
12
6
−6
−8
−4
4


Four.III.1.17
(a) (1/3) ·


0
−1
2
3
−2
−8
0
1
1

=


0
−1/3
2/3
1
−2/3
−8/3
0
1/3
1/3


(b) (1/14) ·
µ
4
1
−2
3
¶
=
µ
2/7
1/14
−1/7
3/14
¶
(c) (1/ −5) ·
µ
0
−1
−5
1
¶
=
µ
0
1/5
1
−1/5
¶
(d) The matrix has a zero determinant, and so has no inverse.
Four.III.1.18




T1,1
T2,1
T3,1
T4,1
T1,2
T2,2
T3,2
T4,2
T1,3
T2,3
T3,3
T4,3
T1,4
T2,4
T3,4
T4,4



=




4
−3
2
−1
−3
6
−4
2
2
−4
6
−3
−1
2
−3
4




Four.III.1.19
The determinant
¯¯¯¯
a
b
c
d
¯¯¯¯
expanded on the ﬁrst row gives a · (+1)|d| + b · (−1)|c| = ad −bc (note the two 1×1 minors).
Four.III.1.20
The determinant of


a
b
c
d
e
f
g
h
i


is this.
a ·
¯¯¯¯
e
f
h
i
¯¯¯¯ −b ·
¯¯¯¯
d
f
g
i
¯¯¯¯ + c ·
¯¯¯¯
d
e
g
h
¯¯¯¯ = a(ei −fh) −b(di −fg) + c(dh −eg)

Answers to Exercises
387
Four.III.1.21
(a)
µ
T1,1
T2,1
T1,2
T2,2
¶
=
µ ¯¯t2,2
¯¯
−
¯¯t1,2
¯¯
−
¯¯t2,1
¯¯
¯¯t1,1
¯¯
¶
=
µ
t2,2
−t1,2
−t2,1
t1,1
¶
(b) (1/t1,1t2,2 −t1,2t2,1) ·
µ
t2,2
−t1,2
−t2,1
t1,1
¶
Four.III.1.22
No. Here is a determinant whose value
¯¯¯¯¯¯
1
0
0
0
1
0
0
0
1
¯¯¯¯¯¯
= 1
dosn’t equal the result of expanding down the diagonal.
1 · (+1)
¯¯¯¯
1
0
0
1
¯¯¯¯ + 1 · (+1)
¯¯¯¯
1
0
0
1
¯¯¯¯ + 1 · (+1)
¯¯¯¯
1
0
0
1
¯¯¯¯ = 3
Four.III.1.23
Consider this diagonal matrix.
D =







d1
0
0
. . .
0
d2
0
0
0
d3
...
dn







If i ̸= j then the i, j minor is an (n −1)×(n −1) matrix with only n −2 nonzero entries, because both
di and dj are deleted. Thus, at least one row or column of the minor is all zeroes, and so the cofactor
Di,j is zero. If i = j then the minor is the diagonal matrix with entries d1, . . . , di−1, di+1, . . . , dn. Its
determinant is obviously (−1)i+j = (−1)2i = 1 times the product of those.
adj(D) =





d2 · · · dn
0
0
0
d1d3 · · · dn
0
...
d1 · · · dn−1





By the way, Theorem 1.9 provides a slicker way to derive this conclusion.
Four.III.1.24
Just note that if S = T trans then the cofactor Sj,i equals the cofactor Ti,j because
(−1)j+i = (−1)i+j and because the minors are the transposes of each other (and the determinant of a
transpose equals the determinant of the matrix).
Four.III.1.25
It is false; here is an example.
T =


1
2
3
4
5
6
7
8
9


adj(T) =


−3
6
−3
6
−12
6
−3
6
−3


adj(adj(T)) =


0
0
0
0
0
0
0
0
0


Four.III.1.26
(a) An example
M =


1
2
3
0
4
5
0
0
6


suggests the right answer.
adj(M) =


M1,1
M2,1
M3,1
M1,2
M2,2
M3,2
M1,3
M2,3
M3,3

=








¯¯¯¯
4
5
0
6
¯¯¯¯
−
¯¯¯¯
2
3
0
6
¯¯¯¯
¯¯¯¯
2
3
4
5
¯¯¯¯
−
¯¯¯¯
0
5
0
6
¯¯¯¯
¯¯¯¯
1
3
0
6
¯¯¯¯
−
¯¯¯¯
1
3
0
5
¯¯¯¯
¯¯¯¯
0
4
0
0
¯¯¯¯
−
¯¯¯¯
1
2
0
0
¯¯¯¯
¯¯¯¯
1
2
0
4
¯¯¯¯








=


24
−12
−2
0
6
−5
0
0
4


The result is indeed upper triangular.
A check of this is detailed but not hard. The entries in the upper triangle of the adjoint are
Ma,b where a > b. We need to verify that the cofactor Ma,b is zero if a > b. With a > b, row a and
column b of M,










m1,1
. . .
m1,b
m2,1
. . .
m2,b
...
...
ma,1
. . .
ma,b
. . .
ma,n
...
mn,b











388
Linear Algebra, by Hefferon
when deleted, leave an upper triangular minor, because entry i, j of the minor is either entry i, j
of M (this happens if a > i and b > j; in this case i < j implies that the entry is zero) or it is
entry i, j + 1 of M (this happens if i < a and j > b; in this case, i < j implies that i < j + 1,
which implies that the entry is zero), or it is entry i + 1, j + 1 of M (this last case happens when
i > a and j > b; obviously here i < j implies that i + 1 < j + 1 and so the entry is zero). Thus the
determinant of the minor is the product down the diagonal. Observe that the a −1, a entry of M is
the a −1, a −1 entry of the minor (it doesn’t get deleted because the relation a > b is strict). But
this entry is zero because M is upper triangular and a −1 < a. Therefore the cofactor is zero, and
the adjoint is upper triangular. (The lower triangular case is similar.)
(b) This is immediate from the prior part, by Corollary 1.11.
Four.III.1.27
We will show that each determinant can be expanded along row i. The argument for
column j is similar.
Each term in the permutation expansion contains one and only one entry from each row. As in
Example 1.1, factor out each row i entry to get |T| = ti,1 · ˆTi,1 + · · · + ti,n · ˆTi,n, where each ˆTi,j is a
sum of terms not containing any elements of row i. We will show that ˆTi,j is the i, j cofactor.
Consider the i, j = n, n case ﬁrst:
tn,n · ˆTn,n = tn,n ·
X
φ
t1,φ(1)t2,φ(2) . . . tn−1,φ(n−1) sgn(φ)
where the sum is over all n-permutations φ such that φ(n) = n. To show that ˆTi,j is the minor Ti,j,
we need only show that if φ is an n-permutation such that φ(n) = n and σ is an n −1-permutation
with σ(1) = φ(1), . . . , σ(n −1) = φ(n −1) then sgn(σ) = sgn(φ). But that’s true because φ and σ
have the same number of inversions.
Back to the general i, j case. Swap adjacent rows until the i-th is last and swap adjacent columns
until the j-th is last.
Observe that the determinant of the i, j-th minor is not aﬀected by these
adjacent swaps because inversions are preserved (since the minor has the i-th row and j-th column
omitted).
On the other hand, the sign of |T| and ˆTi,j is changed n −i plus n −j times.
Thus
ˆTi,j = (−1)n−i+n−j|Ti,j| = (−1)i+j|Ti,j|.
Four.III.1.28
This is obvious for the 1×1 base case.
For the inductive case, assume that the determinant of a matrix equals the determinant of its
transpose for all 1×1, . . . , (n−1)×(n−1) matrices. Expanding on row i gives |T| = ti,1Ti,1+. . . +ti,nTi,n
and expanding on column i gives |T trans| = t1,i(T trans)1,i +· · ·+tn,i(T trans)n,i Since (−1)i+j = (−1)j+i
the signs are the same in the two summations. Since the j, i minor of T trans is the transpose of the i, j
minor of T, the inductive hypothesis gives |(T trans)i,j| = |Ti,j|.
Four.III.1.29
This is how the answer was given in the cited source. Denoting the above determinant
by Dn, it is seen that D2 = 1, D3 = 2. It remains to show that Dn = Dn−1 + Dn−2, n ≥4. In Dn
subtract the (n −3)-th column from the (n −1)-th, the (n −4)-th from the (n −2)-th, . . . , the ﬁrst
from the third, obtaining
Fn =
¯¯¯¯¯¯¯¯¯¯
1
−1
0
0
0
0
. . .
1
1
−1
0
0
0
. . .
0
1
1
−1
0
0
. . .
0
0
1
1
−1
0
. . .
.
.
.
.
.
.
. . .
¯¯¯¯¯¯¯¯¯¯
.
By expanding this determinant with reference to the ﬁrst row, there results the desired relation.
Topic: Cramer’s Rule
1
(a) x = 1, y = −3
(b) x = −2, y = −2
2
z = 1
3
Determinants are unchanged by pivots, including column pivots, so det(Bi) = det(⃗a1, . . . , x1⃗a1+· · ·+
xi⃗ai + · · · + xn⃗an, . . . ,⃗an) is equal to det(⃗a1, . . . , xi⃗ai, . . . ,⃗an) (use the operation of taking −x1 times
the ﬁrst column and adding it to the i-th column, etc.). That is equal to xi · det(⃗a1, . . . ,⃗ai, . . . ,⃗an) =
xi · det(A), as required.

Answers to Exercises
389
4
Because the determinant of A is nonzero, Cramer’s Rule applies and shows that xi = |Bi|/1. Since
Bi is a matrix of integers, its determinant is an integer.
5
The solution of
ax +by = e
cx +dy = f
is
x = ed −fb
ad −bc
y = af −ec
ad −bc
provided of course that the denominators are not zero.
6
Of course, singular systems have |A| equal to zero, but the inﬁnitely many solutions case is charac-
terized by the fact that all of the |Bi| are zero as well.
7
We can consider the two nonsingular cases together with this system
x1 + 2x2 = 6
x1 + 2x2 = c
where c = 6 of course yields inﬁnitely many solutions, and any other value for c yields no solutions.
The corresponding vector equation
x1 ·
µ1
1
¶
+ x2 ·
µ2
2
¶
=
µ6
c
¶
gives a picture of two overlapping vectors. Both lie on the line y = x. In the c = 6 case the vector on
the right side also lies on the line y = x but in any other case it does not.
Topic: Speed of Calculating Determinants
1
(a) Under Octave, rank(rand(5)) ﬁnds the rank of a 5×5 matrix whose entries are (uniformily
distributed) in the interval [0..1). This loop which runs the test 5000 times
octave:1> for i=1:5000
> if rank(rand(5))<5 printf("That’s one."); endif
> endfor
produces (after a few seconds) returns the prompt, with no output.
The Octave script
function elapsed_time = detspeed (size)
a=rand(size);
tic();
for i=1:10
det(a);
endfor
elapsed_time=toc();
endfunction
lead to this session.
octave:1> detspeed(5)
ans = 0.019505
octave:2> detspeed(15)
ans = 0.0054691
octave:3> detspeed(25)
ans = 0.0097431
octave:4> detspeed(35)
ans = 0.017398
(b) Here is the data (rounded a bit), and the graph.
matrix rows
15
25
35
45
55
65
75
85
95
time per ten
0.0034
0.0098
0.0675
0.0285
0.0443
0.0663
0.1428
0.2282
0.1686
(This data is from an average of twenty runs of the above script, because of the possibility that
the randomly chosen matrix happens to take an unusually long or short time. Even so, the timing
cannot be relied on too heavily; this is just an experiment.)

390
Linear Algebra, by Hefferon
20
40
60
80
100
0
0.05
0.1
0.15
0.2
2
The number of operations depends on exactly how the operations are carried out.
(a) The determinant is −11. To row reduce takes a single pivot with two multiplications (−5/2
times 2 plus 5, and −5/2 times 1 plus −3) and the product down the diagonal takes one more
multiplication. The permutation expansion takes two multiplications (2 times −3 and 5 times 1).
(b) The determinant is −39. Counting the operations is routine.
(c) The determinant is 4.
3
One way to get started is to compare these under Octave: det(rand(10));, versus det(hilb(10));,
versus det(eye(10));, versus det(zeroes(10));. You can time them as in tic(); det(rand(10));
toc().
4
This is a simple one.
DO 5 ROW=1, N
PIVINV=1.0/A(ROW,ROW)
DO 10 I=ROW+1, N
DO 20 J=I, N
A(I,J)=A(I,J)-PIVINV*A(ROW,J)
20 CONTINUE
10 CONTINUE
5 CONTINUE
5
Yes, because the J is in the innermost loop.
Topic: Projective Geometry
1
From the dot product
0 =


1
0
0

¡
L1
L2
L3
¢
= L1
we get that the equation is L1 = 0.
2
(a) This determinant
0 =
¯¯¯¯¯¯
1
4
x
2
5
y
3
6
z
¯¯¯¯¯¯
= −3x + 6y −3z
shows that the line is L =
¡
−3
6
−3
¢
.
(b)


−3
6
−3


3
The line incident on
u =


u1
u2
u3


v =


v1
v2
v3


can be found from this determinant equation.
0 =
¯¯¯¯¯¯
u1
v1
x
u2
v2
y
u3
v3
z
¯¯¯¯¯¯
= (u2v3 −u3v2) · x + (u3v1 −u1v3) · y + (u1v2 −u2v1) · z
The equation for the point incident on two lines is the same.

Answers to Exercises
391
4
If p1, p2, p3, and q1, q2, q3 are two triples of homogeneous coordinates for p then the two column
vectors are in proportion, that is, lie on the same line through the origin. Similarly, the two row vectors
are in proportion.
k ·


p1
p2
p3

=


q1
q2
q3


m ·
¡
L1
L2
L3
¢
=
¡
M1
M2
M3
¢
Then multiplying gives the answer (km) · (p1L1 + p2L2 + p3L3) = q1M1 + q2M2 + q3M3 = 0.
5
The picture of the solar eclipse — unless the image plane is exactly perpendicular to the line from
the sun through the pinhole — shows the circle of the sun projecting to an image that is an ellipse.
(Another example is that in many pictures in this Topic, the circle that is the sphere’s equator is drawn
as an ellipse, that is, is seen by a viewer of the drawing as an ellipse.)
The solar eclipse picture also shows the converse. If we picture the projection as going from left to
right through the pinhole then the ellipse I projects through P to a circle S.
6
A spot on the unit sphere


p1
p2
p3


is non-equatorial if and only if p3 ̸= 0. In that case it corresponds to this point on the z = 1 plane


p1/p3
p2/p3
1


since that is intersection of the line containing the vector and the plane.
7
(a) Other pictures are possible, but this is one.
T0
U0
V0
T1
U1
V1
V2
U2
T2
The intersections T0U1 ∩T1U0 = V2, T0V1 ∩T1V0 = U2, and U0V1 ∩U1V0 = T2 are labeled so that
on each line is a T, a U, and a V .
(b) The lemma used in Desargue’s Theorem gives a basis B with respect to which the points have
these homogeneous coordinate vectors.
RepB(⃗t0) =


1
0
0


RepB(⃗t1) =


0
1
0


RepB(⃗t2) =


0
0
1


RepB(⃗v0) =


1
1
1


(c) First, any U0 on T0V0
RepB(⃗u0) = a


1
0
0

+ b


1
1
1

=


a + b
b
b


has homogeneous coordinate vectors of this form


u0
1
1


(u0 is a parameter; it depends on where on the T0V0 line the point U0 is, but any point on that line
has a homogeneous coordinate vector of this form for some u0 ∈R). Similarly, U2 is on T1V0
RepB(⃗u2) = c


0
1
0

+ d


1
1
1

=


d
c + d
d


and so has this homogeneous coordinate vector.

1
u2
1



392
Linear Algebra, by Hefferon
Also similarly, U1 is incident on T2V0
RepB(⃗u1) = e


0
0
1

+ f


1
1
1

=


f
f
e + f


and has this homogeneous coordinate vector. 

1
1
u1


(d) Because V1 is T0U2 ∩U0T2 we have this.
g


1
0
0

+ h


1
u2
1

= i


u0
1
1

+ j


0
0
1


=⇒
g + h = iu0
hu2 = i
h = i + j
Substituting hu2 for i in the ﬁrst equation


hu0u2
hu2
h


shows that V1 has this two-parameter homogeneous coordinate vector.


u0u2
u2
1


(e) Since V2 is the intersection T0U1 ∩T1U0
k


1
0
0

+ l


1
1
u1

= m


0
1
0

+ n


u0
1
1


=⇒
k + l = nu0
l = m + n
lu1 = n
and substituting lu1 for n in the ﬁrst equation

lu0u1
l
lu1


gives that V2 has this two-parameter homogeneous coordinate vector.


u0u1
1
u1


(f) Because V1 is on the T1U1 line its homogeneous coordinate vector has the form
p


0
1
0

+ q


1
1
u1

=


q
p + q
qu1


(∗)
but a previous part of this question established that V1’s homogeneous coordinate vectors have the
form


u0u2
u2
1


and so this a homogeneous coordinate vector for V1.


u0u1u2
u1u2
u1


(∗∗)
By (∗) and (∗∗), there is a relationship among the three parameters: u0u1u2 = 1.
(g) The homogeneous coordinate vector of V2 can be written in this way.


u0u1u2
u2
u1u2

=


1
u2
u1u2


Now, the T2U2 line consists of the points whose homogeneous coordinates have this form.
r


0
0
1

+ s


1
u2
1

=


s
su2
r + s


Taking s = 1 and r = u1u2 −1 shows that the homogeneous coordinate vectors of V2 have this form.

Chapter Five: Similarity
Subsection Five.II.1: Deﬁnition and Examples
Five.II.1.4
One way to proceed is left to right.
PSP −1 =
µ
4
2
−3
2
¶ µ
1
3
−2
−6
¶ µ
2/14
−2/14
3/14
4/14
¶
=
µ
0
0
−7
−21
¶ µ
2/14
−2/14
3/14
4/14
¶
=
µ
0
0
−11/2
−5
¶
Five.II.1.5
(a) Because the matrix (2) is 1×1, the matrices P and P −1 are also 1×1 and so where
P = (p) the inverse is P −1 = (1/p). Thus P(2)P −1 = (p)(2)(1/p) = (2).
(b) Yes: recall that scalar multiples can be brought out of a matrix P(cI)P −1 = cPIP −1 = cI. By
the way, the zero and identity matrices are the special cases c = 0 and c = 1.
(c) No, as this example shows.
µ
1
−2
−1
1
¶ µ
−1
0
0
−3
¶ µ
−1
−2
−1
−1
¶
=
µ
−5
−4
2
1
¶
Five.II.1.6
Gauss’ method shows that the ﬁrst matrix represents maps of rank two while the second
matrix represents maps of rank three.
Five.II.1.7
(a) Because t is described with the members of B, ﬁnding the matrix representation is
easy:
RepB(t(x2)) =


0
1
1


B
RepB(t(x)) =


1
0
−1


B
RepB(t(1)) =


0
0
3


B
gives this.
RepB,B(t)


0
1
0
1
0
0
1
−1
3


(b) We will ﬁnd t(1), t(1 + x), and t(1 + x + x2, to ﬁnd how each is represented with respect
to D.
We are given that t(1) = 3, and the other two are easy to see: t(1 + x) = x2 + 2 and
t(1 + x + x2) = x2 + x + 3. By eye, we get the representation of each vector
RepD(t(1)) =


3
0
0


D
RepD(t(1 + x)) =


2
−1
1


D
RepD(t(1 + x + x2)) =


2
0
1


D
and thus the representation of the map.
RepD,D(t) =


3
2
2
0
−1
0
0
1
1


(c) The diagram, adapted for this T and S,
Vw.r.t. D
t
−−−−→
S
Vw.r.t. D
id
yP
id
yP
Vw.r.t. B
t
−−−−→
T
Vw.r.t. B
shows that P = RepD,B(id).
P =


0
0
1
0
1
1
1
1
1



394
Linear Algebra, by Hefferon
Five.II.1.8
One possible choice of the bases is
B = ⟨
µ1
2
¶
,
µ−1
1
¶
⟩
D = E2 = ⟨
µ1
0
¶
,
µ0
1
¶
⟩
(this B is suggested by the map description). To ﬁnd the matrix T = RepB,B(t), solve the relations
c1
µ
1
2
¶
+ c2
µ
−1
1
¶
=
µ
3
0
¶
ˆc1
µ
1
2
¶
+ ˆc2
µ
−1
1
¶
=
µ
−1
2
¶
to get c1 = 1, c2 = −2, ˆc1 = 1/3 and ˆc2 = 4/3.
RepB,B(t) =
µ 1
1/3
−2
4/3
¶
Finding RepD,D(t) involves a bit more computation. We ﬁrst ﬁnd t(⃗e1). The relation
c1
µ
1
2
¶
+ c2
µ
−1
1
¶
=
µ
1
0
¶
gives c1 = 1/3 and c2 = −2/3, and so
RepB(⃗e1) =
µ 1/3
−2/3
¶
B
making
RepB(t(⃗e1)) =
µ
1
1/3
−2
4/3
¶
B,B
µ
1/3
−2/3
¶
B
=
µ
1/9
−14/9
¶
B
and hence t acts on the ﬁrst basis vector ⃗e1 in this way.
t(⃗e1) = (1/9) ·
µ
1
2
¶
−(14/9) ·
µ
−1
1
¶
=
µ
5/3
−4/3
¶
The computation for t(⃗e2) is similar. The relation
c1
µ
1
2
¶
+ c2
µ
−1
1
¶
=
µ
0
1
¶
gives c1 = 1/3 and c2 = 1/3, so
RepB(⃗e1) =
µ1/3
1/3
¶
B
making
RepB(t(⃗e1)) =
µ
1
1/3
−2
4/3
¶
B,B
µ
1/3
1/3
¶
B
=
µ
4/9
−2/9
¶
B
and hence t acts on the second basis vector ⃗e2 in this way.
t(⃗e2) = (4/9) ·
µ
1
2
¶
−(2/9) ·
µ
−1
1
¶
=
µ
2/3
2/3
¶
Therefore
RepD,D(t) =
µ 5/3
2/3
−4/3
2/3
¶
and these are the change of basis matrices.
P = RepB,D(id) =
µ
1
−1
2
1
¶
P −1 =
¡
RepB,D(id)
¢−1 =
µ
1
−1
2
1
¶−1
=
µ
1/3
1/3
−2/3
1/3
¶
The check of these computations is routine.
µ
1
−1
2
1
¶ µ
1
1/3
−2
4/3
¶ µ
1/3
1/3
−2/3
1/3
¶
=
µ
5/3
2/3
−4/3
2/3
¶
Five.II.1.9
The only representation of a zero map is a zero matrix, no matter what the pair of bases
RepB,D(z) = Z, and so in particular for any single basis B we have RepB,B(z) = Z. The case of the
identity is related, but slightly diﬀerent: the only representation of the identity map, with respect to
any B, B, is the identity RepB,B(id) = I. (Remark: of course, we have seen examples where B ̸= D
and RepB,D(id) ̸= I — in fact, we have seen that any nonsingular matrix is a representation of the
identity map with respect to some B, D.)
Five.II.1.10
No. If A = PBP −1 then A2 = (PBP −1)(PBP −1) = PB2P −1.
Five.II.1.11
Matrix similarity is a special case of matrix equivalence (if matrices are similar then they
are matrix equivalent) and matrix equivalence preserves nonsingularity.

Answers to Exercises
395
Five.II.1.12
A matrix is similar to itself; take P to be the identity matrix: IPI−1 = IPI = P.
If T is similar to S then T = PSP −1 and so P −1TP = S. Rewrite this as S = (P −1)T(P −1)−1 to
conclude that S is similar to T.
If T is similar to S and S is similar to U then T = PSP −1 and S = QUQ−1.
Then T =
PQUQ−1P −1 = (PQ)U(PQ)−1, showing that T is similar to U.
Five.II.1.13
Let fx and fy be the reﬂection maps (sometimes called ‘ﬂip’s). For any bases B and D,
the matrices RepB,B(fx) and RepD,D(fy) are similar. First note that
S = RepE2,E2(fx) =
µ
1
0
0
−1
¶
T = RepE2,E2(fy) =
µ
−1
0
0
1
¶
are similar because the second matrix is the representation of fx with respect to the basis A = ⟨⃗e2,⃗e1⟩:
µ
1
0
0
−1
¶
= P
µ
−1
0
0
1
¶
P −1
where P = RepA,E2(id).
R2
w.r.t. A
fx
−−−−→
T
V R2
w.r.t. A
id
yP
id
yP
R2
w.r.t. E2
fx
−−−−→
S
R2
w.r.t. E2
Now the conclusion follows from the transitivity part of Exercise 12.
To ﬁnish without relying on that exercise, write RepB,B(fx) = QTQ−1 = QRepE2,E2(fx)Q−1 and
RepD,D(fy) = RSR−1 = RRepE2,E2(fy)R−1. Using the equation in the ﬁrst paragraph, the ﬁrst of
these two becomes RepB,B(fx) = QPRepE2,E2(fy)P −1Q−1 and rewriting the second of these two as
R−1 · RepD,D(fy) · R = RepE2,E2(fy) and substituting gives the desired relationship
RepB,B(fx) = QPRepE2,E2(fy)P −1Q−1
= QPR−1 · RepD,D(fy) · RP −1Q−1 = (QPR−1) · RepD,D(fy) · (QPR−1)−1
Thus the matrices RepB,B(fx) and RepD,D(fy) are similar.
Five.II.1.14
We must show that if two matrices are similar then they have the same determinant and
the same rank. Both determinant and rank are properties of matrices that we have already shown to
be preserved by matrix equivalence. They are therefore preserved by similarity (which is a special case
of matrix equivalence: if two matrices are similar then they are matrix equivalent).
To prove the statement without quoting the results about matrix equivalence, note ﬁrst that rank
is a property of the map (it is the dimension of the rangespace) and since we’ve shown that the rank of
a map is the rank of a representation, it must be the same for all representations. As for determinants,
|PSP −1| = |P| · |S| · |P −1| = |P| · |S| · |P|−1 = |S|.
The converse of the statement does not hold; for instance, there are matrices with the same de-
terminant that are not similar. To check this, consider a nonzero matrix with a determinant of zero.
It is not similar to the zero matrix, the zero matrix is similar only to itself, but they have they same
determinant. The argument for rank is much the same.
Five.II.1.15
The matrix equivalence class containing all n×n rank zero matrices contains only a single
matrix, the zero matrix. Therefore it has as a subset only one similarity class.
In contrast, the matrix equivalence class of 1 × 1 matrices of rank one consists of those 1 × 1
matrices (k) where k ̸= 0. For any basis B, the representation of multiplication by the scalar k is
RepB,B(tk) = (k), so each such matrix is alone in its similarity class. So this is a case where a matrix
equivalence class splits into inﬁnitely many similarity classes.
Five.II.1.16
Yes, these are similar
µ
1
0
0
3
¶
µ
3
0
0
1
¶
since, where the ﬁrst matrix is RepB,B(t) for B = ⟨⃗β1, ⃗β2⟩, the second matrix is RepD,D(t) for D =
⟨⃗β2, ⃗β1⟩.
Five.II.1.17
The k-th powers are similar because, where each matrix represents the map t, the k-
th powers represent tk, the composition of k-many t’s. (For instance, if T = reptB, B then T 2 =
RepB,B(t ◦t).)

396
Linear Algebra, by Hefferon
Restated more computationally, if T = PSP −1 then T 2 = (PSP −1)(PSP −1) = PS2P −1. Induc-
tion extends that to all powers.
For the k ≤0 case, suppose that S is invertible and that T = PSP −1. Note that T is invertible:
T −1 = (PSP −1)−1 = PS−1P −1, and that same equation shows that T −1 is similar to S−1. Other
negative powers are now given by the ﬁrst paragraph.
Five.II.1.18
In conceptual terms, both represent p(t) for some transformation t. In computational
terms, we have this.
p(T) = cn(PSP −1)n + · · · + c1(PSP −1) + c0I
= cnPSnP −1 + · · · + c1PSP −1 + c0I
= PcnSnP −1 + · · · + Pc1SP −1 + Pc0P −1
= P(cnSn + · · · + c1S + c0)P −1
Five.II.1.19
There are two equivalence classes, (i) the class of rank zero matrices, of which there is
one: C1 = {(0)}, and (2) the class of rank one matrices, of which there are inﬁnitely many: C2 =
{(k)
¯¯ k ̸= 0}.
Each 1×1 matrix is alone in its similarity class. That’s because any transformation of a one-
dimensional space is multiplication by a scalar tk : V →V given by ⃗v 7→k · ⃗v. Thus, for any basis
B = ⟨⃗β⟩, the matrix representing a transformation tk with respect to B, B is (RepB(tk(⃗β))) = (k).
So, contained in the matrix equivalence class C1 is (obviously) the single similarity class consisting
of the matrix (0). And, contained in the matrix equivalence class C2 are the inﬁnitely many, one-
member-each, similarity classes consisting of (k) for k ̸= 0.
Five.II.1.20
No. Here is an example that has two pairs, each of two similar matrices:
µ
1
−1
1
2
¶ µ
1
0
0
3
¶ µ
2/3
1/3
−1/3
1/3
¶
=
µ
5/3
−2/3
−4/3
7/3
¶
and
µ
1
−2
−1
1
¶ µ
−1
0
0
−3
¶ µ
−1
−2
−1
−1
¶
=
µ
−5
−4
2
1
¶
(this example is mostly arbitrary, but not entirely, because the the center matrices on the two left sides
add to the zero matrix). Note that the sums of these similar matrices are not similar
µ
1
0
0
3
¶
+
µ
−1
0
0
−3
¶
=
µ
0
0
0
0
¶
µ
5/3
−2/3
−4/3
7/3
¶
+
µ
−5
−4
2
1
¶
̸=
µ
0
0
0
0
¶
since the zero matrix is similar only to itself.
Five.II.1.21
If N = P(T −λI)P −1 then N = PTP −1 −P(λI)P −1. The diagonal matrix λI commutes
with anything, so P(λI)P −1 = PP −1(λI) = λI. Thus N = PTP −1 −λI and consequently N + λI =
PTP −1. (So not only are they similar, in fact they are similar via the same P.)
Subsection Five.II.2: Diagonalizability
Five.II.2.6
Because the basis vectors are chosen arbitrarily, many diﬀerent answers are possible. How-
ever, here is one way to go; to diagonalize
T =
µ
4
−2
1
1
¶
take it as the representation of a transformation with respect to the standard basis T = RepE2,E2(t)
and look for B = ⟨⃗β1, ⃗β2⟩such that
RepB,B(t) =
µ
λ1
0
0
λ2
¶
that is, such that t(⃗β1) = λ1 and t(⃗β2) = λ2.
µ
4
−2
1
1
¶
⃗β1 = λ1 · ⃗β1
µ
4
−2
1
1
¶
⃗β2 = λ2 · ⃗β2
We are looking for scalars x such that this equation
µ
4
−2
1
1
¶ µ
b1
b2
¶
= x ·
µ
b1
b2
¶

Answers to Exercises
397
has solutions b1 and b2, which are not both zero. Rewrite that as a linear system
(4 −x) · b1 +
−2 · b2 = 0
1 · b1 + (1 −x) · b2 = 0
If x = 4 then the ﬁrst equation gives that b2 = 0, and then the second equation gives that b1 = 0. The
case where both b’s are zero is disallowed so we can assume that x ̸= 4.
(−1/(4−x))ρ1+ρ2
−→
(4 −x) · b1 +
−2 · b2 = 0
((x2 −5x + 6)/(4 −x)) · b2 = 0
Consider the bottom equation. If b2 = 0 then the ﬁrst equation gives b1 = 0 or x = 4. The b1 = b2 = 0
case is disallowed. The other possibility for the bottom equation is that the numerator of the fraction
x2 −5x + 6 = (x −2)(x −3) is zero. The x = 2 case gives a ﬁrst equation of 2b1 −2b2 = 0, and so
associated with x = 2 we have vectors whose ﬁrst and second components are equal:
⃗β1 =
µ1
1
¶
(so
µ4
−2
1
1
¶ µ1
1
¶
= 2 ·
µ1
1
¶
, and λ1 = 2).
If x = 3 then the ﬁrst equation is b1 −2b2 = 0 and so the associated vectors are those whose ﬁrst
component is twice their second:
⃗β2 =
µ
2
1
¶
(so
µ
4
−2
1
1
¶ µ
2
1
¶
= 3 ·
µ
2
1
¶
, and so λ2 = 3).
This picture
R2
w.r.t. E2
t
−−−−→
T
R2
w.r.t. E2
id
y
id
y
R2
w.r.t. B
t
−−−−→
D
R2
w.r.t. B
shows how to get the diagonalization.
µ
2
0
0
3
¶
=
µ
1
2
1
1
¶−1 µ
4
−2
1
1
¶ µ
1
2
1
1
¶
Comment. This equation matches the T = PSP −1 deﬁnition under this renaming.
T =
µ
2
0
0
3
¶
P =
µ
1
2
1
1
¶−1
P −1 =
µ
1
2
1
1
¶
S =
µ
4
−2
1
1
¶
Five.II.2.7
(a) Setting up
µ
−2
1
0
2
¶ µ
b1
b2
¶
= x ·
µ
b1
b2
¶
=⇒
(−2 −x) · b1 +
b2 = 0
(2 −x) · b2 = 0
gives the two possibilities that b2 = 0 and x = 2. Following the b2 = 0 possibility leads to the ﬁrst
equation (−2 −x)b1 = 0 with the two cases that b1 = 0 and that x = −2. Thus, under this ﬁrst
possibility, we ﬁnd x = −2 and the associated vectors whose second component is zero, and whose
ﬁrst component is free.
µ
−2
1
0
2
¶ µ
b1
0
¶
= −2 ·
µ
b1
0
¶
⃗β1 =
µ
1
0
¶
Following the other possibility leads to a ﬁrst equation of −4b1+b2 = 0 and so the vectors associated
with this solution have a second component that is four times their ﬁrst component.
µ
−2
1
0
2
¶ µ
b1
4b1
¶
= 2 ·
µ
b1
4b1
¶
⃗β2 =
µ
1
4
¶
The diagonalization is this. µ
1
1
0
4
¶−1 µ
−2
1
0
2
¶ µ
1
1
0
4
¶−1 µ
−2
0
0
2
¶
(b) The calculations are like those in the prior part.
µ5
4
0
1
¶ µb1
b2
¶
= x ·
µb1
b2
¶
=⇒
(5 −x) · b1 +
4 · b2 = 0
(1 −x) · b2 = 0
The bottom equation gives the two possibilities that b2 = 0 and x = 1.
Following the b2 = 0
possibility, and discarding the case where both b2 and b1 are zero, gives that x = 5, associated with
vectors whose second component is zero and whose ﬁrst component is free.
⃗β1 =
µ
1
0
¶

398
Linear Algebra, by Hefferon
The x = 1 possibility gives a ﬁrst equation of 4b1 + 4b2 = 0 and so the associated vectors have a
second component that is the negative of their ﬁrst component.
⃗β1 =
µ
1
−1
¶
We thus have this diagonalization.
µ
1
1
0
−1
¶−1 µ
5
4
0
1
¶ µ
1
1
0
−1
¶
=
µ
5
0
0
1
¶
Five.II.2.8
For any integer p,



d1
0
0
...
dn



p
=



dp
1
0
0
...
dp
n


.
Five.II.2.9
These two are not similar
µ
0
0
0
0
¶
µ
1
0
0
1
¶
because each is alone in its similarity class.
For the second half, these
µ2
0
0
3
¶
µ3
0
0
2
¶
are similar via the matrix that changes bases from ⟨⃗β1, ⃗β2⟩to ⟨⃗β2, ⃗β1⟩. (Question. Are two diagonal
matrices similar if and only if their diagonal entries are permutations of each other’s?)
Five.II.2.10
Contrast these two.
µ
2
0
0
1
¶
µ
2
0
0
0
¶
The ﬁrst is nonsingular, the second is singular.
Five.II.2.11
To check that the inverse of a diagonal matrix is the diagonal matrix of the inverses, just
multiply.





a1,1
0
0
a2,2
...
an,n










1/a1,1
0
0
1/a2,2
...
1/an,n





(Showing that it is a left inverse is just as easy.)
If a diagonal entry is zero then the diagonal matrix is singular; it has a zero determinant.
Five.II.2.12
(a) The check is easy.
µ
1
1
0
−1
¶ µ
3
2
0
1
¶
=
µ
3
3
0
−1
¶
µ
3
3
0
−1
¶ µ
1
1
0
−1
¶−1
=
µ
3
0
0
1
¶
(b) It is a coincidence, in the sense that if T = PSP −1 then T need not equal P −1SP. Even in the
case of a diagonal matrix D, the condition that D = PTP −1 does not imply that D equals P −1TP.
The matrices from Example 2.2 show this.
µ
1
2
1
1
¶ µ
4
−2
1
1
¶
=
µ
6
0
5
−1
¶
µ
6
0
5
−1
¶ µ
1
2
1
1
¶−1
=
µ
−6
12
−6
11
¶
Five.II.2.13
The columns of the matrix are chosen as the vectors associated with the x’s. The exact
choice, and the order of the choice was arbitrary. We could, for instance, get a diﬀerent matrix by
swapping the two columns.
Five.II.2.14
Diagonalizing and then taking powers of the diagonal matrix shows that
µ
−3
1
−4
2
¶k
= 1
3
µ
−1
1
−4
4
¶
+ (−2
3 )k
µ
4
−1
4
−1
¶
.
Five.II.2.15
(a)
µ
1
1
0
−1
¶−1 µ
1
1
0
0
¶ µ
1
1
0
−1
¶
=
µ
1
0
0
0
¶
(b)
µ
1
1
1
−1
¶−1 µ
0
1
1
0
¶ µ
1
1
0
−1
¶
=
µ
1
0
0
−1
¶

Answers to Exercises
399
Five.II.2.16
Yes, ct is diagonalizable by the ﬁnal theorem of this subsection.
No, t+s need not be diagonalizable. Intuitively, the problem arises when the two maps diagonalize
with respect to diﬀerent bases (that is, when they are not simultaneously diagonalizable). Speciﬁcally,
these two are diagonalizable but their sum is not:
µ
1
1
0
0
¶
µ
−1
0
0
0
¶
(the second is already diagonal; for the ﬁrst, see Exercise 15). The sum is not diagonalizable because
its square is the zero matrix.
The same intuition suggests that t ◦s is not be diagonalizable. These two are diagonalizable but
their product is not:
µ
1
0
0
0
¶
µ
0
1
1
0
¶
(for the second, see Exercise 15).
Five.II.2.17
If
P
µ1
c
0
1
¶
P −1 =
µa
0
0
b
¶
then
P
µ
1
c
0
1
¶
=
µ
a
0
0
b
¶
P
so
µ
p
q
r
s
¶ µ
1
c
0
1
¶
=
µ
a
0
0
b
¶ µ
p
q
r
s
¶
µ
p
cp + q
r
cr + s
¶
=
µ
ap
aq
br
bs
¶
The 1, 1 entries show that a = 1 and the 1, 2 entries then show that pc = 0. Since c ̸= 0 this means
that p = 0. The 2, 1 entries show that b = 1 and the 2, 2 entries then show that rc = 0. Since c ̸= 0
this means that r = 0. But if both p and r are 0 then P is not invertible.
Five.II.2.18
(a) Using the formula for the inverse of a 2×2 matrix gives this.
µa
b
c
d
¶ µ1
2
2
1
¶
·
1
ad −bc ·
µ d
−b
−c
a
¶
=
1
ad −bc
µad + 2bd −2ac −bc
−ab −2b2 + 2a2 + ab
cd + 2d2 −2c2 −cd
−bc −2bd + 2ac + ad
¶
Now pick scalars a, . . . , d so that ad −bc ̸= 0 and 2d2 −2c2 = 0 and 2a2 −2b2 = 0. For example,
these will do.
µ
1
1
1
−1
¶ µ
1
2
2
1
¶
· 1
−2 ·
µ
−1
−1
−1
1
¶
= 1
−2
µ
−6
0
0
2
¶
(b) As above,
µ
a
b
c
d
¶ µ
x
y
y
z
¶
·
1
ad −bc ·
µ
d
−b
−c
a
¶
=
1
ad −bc
µ
adx + bdy −acy −bcz
−abx −b2y + a2y + abz
cdx + d2y −c2y −cdz
−bcx −bdy + acy + adz
¶
we are looking for scalars a, . . . , d so that ad −bc ̸= 0 and −abx −b2y + a2y + abz = 0 and
cdx + d2y −c2y −cdz = 0, no matter what values x, y, and z have.
For starters, we assume that y ̸= 0, else the given matrix is already diagonal. We shall use that
assumption because if we (arbitrarily) let a = 1 then we get
−bx −b2y + y + bz = 0
(−y)b2 + (z −x)b + y = 0
and the quadratic formula gives
b = −(z −x) ±
p
(z −x)2 −4(−y)(y)
−2y
y ̸= 0
(note that if x, y, and z are real then these two b’s are real as the discriminant is positive). By the
same token, if we (arbitrarily) let c = 1 then
dx + d2y −y −dz = 0
(y)d2 + (x −z)d −y = 0
and we get here
d = −(x −z) ±
p
(x −z)2 −4(y)(−y)
2y
y ̸= 0

400
Linear Algebra, by Hefferon
(as above, if x, y, z ∈R then this discriminant is positive so a symmetric, real, 2×2 matrix is similar
to a real diagonal matrix).
For a check we try x = 1, y = 2, z = 1.
b = 0 ± √0 + 16
−4
= ∓1
d = 0 ± √0 + 16
4
= ±1
Note that not all four choices (b, d) = (+1, +1), . . . , (−1, −1) satisfy ad −bc ̸= 0.
Subsection Five.II.3: Eigenvalues and Eigenvectors
Five.II.3.20
(a) This
0 =
¯¯¯¯
10 −x
−9
4
−2 −x
¯¯¯¯ = (10 −x)(−2 −x) −(−36)
simpliﬁes to the characteristic equation x2 −8x + 16 = 0. Because the equation factors into (x −4)2
there is only one eigenvalue λ1 = 4.
(b) 0 = (1 −x)(3 −x) −8 = x2 −4x −5; λ1 = 5, λ2 = −1
(c) x2 −21 = 0; λ1 =
√
21, λ2 = −
√
21
(d) x2 = 0; λ1 = 0
(e) x2 −2x + 1 = 0; λ1 = 1
Five.II.3.21
(a) The characteristic equation is (3 −x)(−1 −x) = 0. Its roots, the eigenvalues, are
λ1 = 3 and λ2 = −1. For the eigenvectors we consider this equation.
µ3 −x
0
8
−1 −x
¶ µb1
b2
¶
=
µ0
0
¶
For the eigenvector associated with λ1 = 3, we consider the resulting linear system.
0 · b1 +
0 · b2 = 0
8 · b1 + −4 · b2 = 0
The eigenspace is the set of vectors whose second component is twice the ﬁrst component.
{
µ
b2/2
b2
¶ ¯¯ b2 ∈C}
µ
3
0
8
−1
¶ µ
b2/2
b2
¶
= 3 ·
µ
b2/2
b2
¶
(Here, the parameter is b2 only because that is the variable that is free in the above system.) Hence,
this is an eigenvector associated with the eigenvalue 3.
µ
1
2
¶
Finding an eigenvector associated with λ2 = −1 is similar. This system
4 · b1 + 0 · b2 = 0
8 · b1 + 0 · b2 = 0
leads to the set of vectors whose ﬁrst component is zero.
{
µ
0
b2
¶ ¯¯ b2 ∈C}
µ
3
0
8
−1
¶ µ
0
b2
¶
= −1 ·
µ
0
b2
¶
And so this is an eigenvector associated with λ2.µ
0
1
¶
(b) The characteristic equation is
0 =
¯¯¯¯
3 −x
2
−1
−x
¯¯¯¯ = x2 −3x + 2 = (x −2)(x −1)
and so the eigenvalues are λ1 = 2 and λ2 = 1. To ﬁnd eigenvectors, consider this system.
(3 −x) · b1 + 2 · b2 = 0
−1 · b1 −x · b2 = 0
For λ1 = 2 we get
1 · b1 + 2 · b2 = 0
−1 · b1 −2 · b2 = 0

Answers to Exercises
401
leading to this eigenspace and eigenvector.
{
µ
−2b2
b2
¶ ¯¯ b2 ∈C}
µ
−2
1
¶
For λ2 = 1 the system is
2 · b1 + 2 · b2 = 0
−1 · b1 −1 · b2 = 0
leading to this.
{
µ
−b2
b2
¶ ¯¯ b2 ∈C}
µ
−1
1
¶
Five.II.3.22
The characteristic equation
0 =
¯¯¯¯
−2 −x
−1
5
2 −x
¯¯¯¯ = x2 + 1
has the complex roots λ1 = i and λ2 = −i. This system
(−2 −x) · b1 −
1 · b2 = 0
5 · b1
(2 −x) · b2 = 0
For λ1 = i Gauss’ method gives this reduction.
(−2 −i) · b1 −
1 · b2 = 0
5 · b1 −(2 −i) · b2 = 0
(−5/(−2−i))ρ1+ρ2
−→
(−2 −i) · b1 −1 · b2 = 0
0 = 0
(For the calculation in the lower right get a common denominator
5
−2 −i −(2 −i) =
5
−2 −i −−2 −i
−2 −i · (2 −i) = 5 −(−5)
−2 −i
to see that it gives a 0 = 0 equation.) These are the resulting eigenspace and eigenvector.
{
µ(1/(−2 −i))b2
b2
¶ ¯¯ b2 ∈C}
µ1/(−2 −i)
1
¶
For λ2 = −i the system
(−2 + i) · b1 −
1 · b2 = 0
5 · b1 −(2 + i) · b2 = 0
(−5/(−2+i))ρ1+ρ2
−→
(−2 + i) · b1 −1 · b2 = 0
0 = 0
leads to this.
{
µ
(1/(−2 + i))b2
b2
¶ ¯¯ b2 ∈C}
µ
1/(−2 + i)
1
¶
Five.II.3.23
The characteristic equation is
0 =
¯¯¯¯¯¯
1 −x
1
1
0
−x
1
0
0
1 −x
¯¯¯¯¯¯
= (1 −x)2(−x)
and so the eigenvalues are λ1 = 1 (this is a repeated root of the equation) and λ2 = 0. For the rest,
consider this system.
(1 −x) · b1 +
b2 +
b3 = 0
−x · b2 +
b3 = 0
(1 −x) · b3 = 0
When x = λ1 = 1 then the solution set is this eigenspace.
{


b1
0
0

¯¯ b1 ∈C}
When x = λ2 = 0 then the solution set is this eigenspace.
{


−b2
b2
0

¯¯ b2 ∈C}
So these are eigenvectors associated with λ1 = 1 and λ2 = 0.


1
0
0




−1
1
0



402
Linear Algebra, by Hefferon
Five.II.3.24
(a) The characteristic equation is
0 =
¯¯¯¯¯¯
3 −x
−2
0
−2
3 −x
0
0
0
5 −x
¯¯¯¯¯¯
= x3 −11x2 + 35x −25 = (x −1)(x −5)2
and so the eigenvalues are λ1 = 1 and also the repeated eigenvalue λ2 = 5. To ﬁnd eigenvectors,
consider this system.
(3 −x) · b1 −
2 · b2
= 0
−2 · b1 + (3 −x) · b2
= 0
(5 −x) · b3 = 0
For λ1 = 1 we get
2 · b1 −2 · b2
= 0
−2 · b1 + 2 · b2
= 0
4 · b3 = 0
leading to this eigenspace and eigenvector.
{


b2
b2
0

¯¯ b2 ∈C}


1
1
0


For λ2 = 1 the system is
−2 · b1 −2 · b2
= 0
−2 · b1 −2 · b2
= 0
0 · b3 = 0
leading to this.
{


−b2
b2
0

+


0
0
b3

¯¯ b2, b3 ∈C}


−1
1
0

,


0
0
1


(b) The characteristic equation is
0 =
¯¯¯¯¯¯
−x
1
0
0
−x
1
4
−17
8 −x
¯¯¯¯¯¯
= −x3 + 8x2 −17x + 4 = −1 · (x −4)(x2 −4x + 1)
and the eigenvalues are λ1 = 4 and (by using the quadratic equation) λ2 = 2+
√
3 and λ3 = 2 −
√
3.
To ﬁnd eigenvectors, consider this system.
−x · b1 +
b2
= 0
−x · b2 +
b3 = 0
4 · b1 −17 · b2 + (8 −x) · b3 = 0
Substituting x = λ1 = 4 gives the system
−4 · b1 +
b2
= 0
−4 · b2 +
b3 = 0
4 · b1 −17 · b2 + 4 · b3 = 0
ρ1+ρ3
−→
−4 · b1 +
b2
= 0
−4 · b2 +
b3 = 0
−16 · b2 + 4 · b3 = 0
−4ρ2+ρ3
−→
−4 · b1 +
b2
= 0
−4 · b2 + b3 = 0
0 = 0
leading to this eigenspace and eigenvector.
V4 = {


(1/16) · b3
(1/4) · b3
b3

¯¯ b2 ∈C}


1
4
16


Substituting x = λ2 = 2 +
√
3 gives the system
(−2 −
√
3) · b1 +
b2
= 0
(−2 −
√
3) · b2 +
b3 = 0
4 · b1 −
17 · b2 + (6 −
√
3) · b3 = 0
(−4/(−2−
√
3))ρ1+ρ3
−→
(−2 −
√
3) · b1 +
b2
= 0
(−2 −
√
3) · b2 +
b3 = 0
+ (−9 −4
√
3) · b2 + (6 −
√
3) · b3 = 0
(the middle coeﬃcient in the third equation equals the number (−4/(−2−
√
3))−17; ﬁnd a common
denominator of −2 −
√
3 and then rationalize the denominator by multiplying the top and bottom
of the frsction by −2 +
√
3)
((9+4
√
3)/(−2−
√
3))ρ2+ρ3
−→
(−2 −
√
3) · b1 +
b2
= 0
(−2 −
√
3) · b2 + b3 = 0
0 = 0

Answers to Exercises
403
which leads to this eigenspace and eigenvector.
V2+
√
3 = {


(1/(2 +
√
3)2) · b3
(1/(2 +
√
3)) · b3
b3

¯¯ b3 ∈C}


(1/(2 +
√
3)2)
(1/(2 +
√
3))
1


Finally, substituting x = λ3 = 2 −
√
3 gives the system
(−2 +
√
3) · b1 +
b2
= 0
(−2 +
√
3) · b2 +
b3 = 0
4 · b1 −
17 · b2 + (6 +
√
3) · b3 = 0
(−4/(−2+
√
3))ρ1+ρ3
−→
(−2 +
√
3) · b1 +
b2
= 0
(−2 +
√
3) · b2 +
b3 = 0
(−9 + 4
√
3) · b2 + (6 +
√
3) · b3 = 0
((9−4
√
3)/(−2+
√
3))ρ2+ρ3
−→
(−2 +
√
3) · b1 +
b2
= 0
(−2 +
√
3) · b2 + b3 = 0
0 = 0
which gives this eigenspace and eigenvector.
V2−
√
3 = {


(1/(2 +
√
3)2) · b3
(1/(2 −
√
3)) · b3
b3

¯¯ b3 ∈C}


(1/(−2 +
√
3)2)
(1/(−2 +
√
3))
1


Five.II.3.25
With respect to the natural basis B = ⟨1, x, x2⟩the matrix representation is this.
RepB,B(t) =


5
6
2
0
−1
−8
1
0
−2


Thus the characteristic equation
0 =


5 −x
6
2
0
−1 −x
−8
1
0
−2 −x

= (5 −x)(−1 −x)(−2 −x) −48 −2 · (−1 −x)
is 0 = −x3 + 2x2 + 15x −36 = −1 · (x + 4)(x −3)2. To ﬁnd the associated eigenvectors, consider this
system.
(5 −x) · b1 +
6 · b2 +
2 · b3 = 0
(−1 −x) · b2 −
8 · b3 = 0
b1
+ (−2 −x) · b3 = 0
Plugging in x = λ1 = 4 gives
b1 +
6 · b2 + 2 · b3 = 0
−5 · b2 −8 · b3 = 0
b1
−6 · b3 = 0
−ρ1+ρ2
−→
b1 +
6 · b2 + 2 · b3 = 0
−5 · b2 −8 · b3 = 0
−6 · b2 −8 · b3 = 0
−ρ1+ρ2
−→
b1 +
6 · b2 + 2 · b3 = 0
−5 · b2 −8 · b3 = 0
−6 · b2 −8 · b3 = 0
Five.II.3.26
λ = 1,
µ
0
0
0
1
¶
and
µ
2
3
1
0
¶
, λ = −2,
µ
−1
0
1
0
¶
, λ = −1,
µ
−2
1
1
0
¶
Five.II.3.27
Fix the natural basis B = ⟨1, x, x2, x3⟩. The map’s action is 1 7→0, x 7→1, x2 7→2x, and
x3 7→3x2 and its representation is easy to compute.
T = RepB,B(d/dx) =




0
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0




B,B
We ﬁnd the eigenvalues with this computation.
0 = |T −xI| =
¯¯¯¯¯¯¯¯
−x
1
0
0
0
−x
2
0
0
0
−x
3
0
0
0
−x
¯¯¯¯¯¯¯¯
= x4
Thus the map has the single eigenvalue λ = 0. To ﬁnd the associated eigenvectors, we solve




0
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0




B,B




b1
b2
b3
b4




B
= 0 ·




b1
b2
b3
b4




B
=⇒
b2 = 0, b3 = 0, b4 = 0

404
Linear Algebra, by Hefferon
to get this eigenspace.
{




b1
0
0
0




B
¯¯ b1 ∈C} = {b1 + 0 · x + 0 · x2 + 0 · x3 ¯¯ b1 ∈C} = {b1
¯¯ b1 ∈C}
Five.II.3.28
The determinant of the triangular matrix T −xI is the product down the diagonal, and
so it factors into the product of the terms ti,i −x.
Five.II.3.29
Just expand the determinant of T −xI.
¯¯¯¯
a −x
c
b
d −x
¯¯¯¯ = (a −x)(d −x) −bc = x2 + (−a −d) · x + (ad −bc)
Five.II.3.30
Any two representations of that transformation are similar, and similar matrices have the
same characteristic polynomial.
Five.II.3.31
(a) Yes, use λ = 1 and the identity map.
(b) Yes, use the transformation that multiplies by λ.
Five.II.3.32
If t(⃗v) = λ · ⃗v then ⃗v 7→⃗0 under the map t −λ · id.
Five.II.3.33
The characteristic equation
0 =
¯¯¯¯
a −x
b
c
d −x
¯¯¯¯ = (a −x)(d −x) −bc
simpliﬁes to x2 + (−a −d) · x + (ad −bc). Checking that the values x = a + b and x = a −c satisfy the
equation (under the a + b = c + d condition) is routine.
Five.II.3.34
Consider an eigenspace Vλ. Any ⃗w ∈Vλ is the image ⃗w = λ · ⃗v of some ⃗v ∈Vλ (namely,
⃗v = (1/λ)· ⃗w). Thus, on Vλ (which is a nontrivial subspace) the action of t−1 is t−1(⃗w) = ⃗v = (1/λ)· ⃗w,
and so 1/λ is an eigenvalue of t−1.
Five.II.3.35
(a) We have (cT + dI)⃗v = cT⃗v + dI⃗v = cλ⃗v + d⃗v = (cλ + d) · ⃗v.
(b) Suppose that S = PTP −1 is diagonal.
Then P(cT + dI)P −1 = P(cT)P −1 + P(dI)P −1 =
cPTP −1 + dI = cS + dI is also diagonal.
Five.II.3.36
The scalar λ is an eigenvalue if and only if the transformation t −λid is singular. A
transformation is singular if and only if it is not an isomorphism (that is, a transformation is an
isomorphism if and only if it is nonsingular).
Five.II.3.37
(a) Where the eigenvalue λ is associated with the eigenvector ⃗x then Ak⃗x = A · · · A⃗x =
Ak−1λ⃗x = λAk−1⃗x = · · · = λk⃗x. (The full details can be put in by doing induction on k.)
(b) The eigenvector associated wih λ might not be an eigenvector associated with µ.
Five.II.3.38
No. These are two same-sized, equal rank, matrices with diﬀerent eigenvalues.
µ
1
0
0
1
¶
µ
1
0
0
2
¶
Five.II.3.39
The characteristic polynomial has an odd power and so has at least one real root.
Five.II.3.40
The characteristic polynomial x3 −5x2 + 6x has distinct roots λ1 = 0, λ2 = −2, and
λ3 = −3. Thus the matrix can be diagonalized into this form.


0
0
0
0
−2
0
0
0
−3


Five.II.3.41
We must show that it is one-to-one and onto, and that it respects the operations of matrix
addition and scalar multiplication.
To show that it is one-to-one, suppose that tP (T) = tP (S), that is, suppose that PTP −1 = PSP −1,
and note that multiplying both sides on the left by P −1 and on the right by P gives that T = S. To
show that it is onto, consider S ∈Mn×n and observe that S = tP (P −1SP).
The map tP preserves matrix addition since tP (T + S) = P(T + S)P −1 = (PT + PS)P −1 =
PTP −1 + PSP −1 = tP (T + S) follows from properties of matrix multiplication and addition that we
have seen. Scalar multiplication is similar: tP (cT) = P(c · T)P −1 = c · (PTP −1) = c · tP (T).
Five.II.3.42
This is how the answer was given in the cited source. If the argument of the characteristic
function of A is set equal to c, adding the ﬁrst (n −1) rows (columns) to the nth row (column) yields
a determinant whose nth row (column) is zero. Thus c is a characteristic root of A.

Answers to Exercises
405
Subsection Five.III.1: Self-Composition
Five.III.1.8
For the zero transformation, no matter what the space, the chain of rangespaces is V ⊃
{⃗0} = {⃗0} = · · · and the chain of nullspaces is {⃗0} ⊂V = V = · · · . For the identity transformation
the chains are V = V = V = · · · and {⃗0} = {⃗0} = · · · .
Five.III.1.9
(a) Iterating t0 twice a + bx + cx2 7→b + cx2 7→cx2 gives
a + bx + cx2
t2
0
7−→cx2
and any higher power is the same map. Thus, while R(t0) is the space of quadratic polynomials
with no linear term {p + rx2 ¯¯ p, r ∈C}, and R(t2
0) is the space of purely-quadratic polynomials
{rx2 ¯¯ r ∈C}, this is where the chain stabilizes R∞(t0) = {rx2 ¯¯ n ∈C}. As for nullspaces, N (t0)
is the space of purely-linear quadratic polynomials {qx
¯¯ q ∈C}, and N (t2
0) is the space of quadratic
polynomials with no x2 term {p + qx
¯¯ p, q ∈C}, and this is the end N∞(t0) = N (t2
0).
(b) The second power
µ
a
b
¶
t1
7−→
µ
0
a
¶
t1
7−→
µ
0
0
¶
is the zero map. Consequently, the chain of rangespaces
R2 ⊃{
µ
0
p
¶ ¯¯ p ∈C} ⊃{⃗0 } = · · ·
and the chain of nullspaces
{⃗0 } ⊂{
µ
q
0
¶ ¯¯ q ∈C} ⊂R2 = · · ·
each has length two. The generalized rangespace is the trivial subspace and the generalized nullspace
is the entire space.
(c) Iterates of this map cycle around
a + bx + cx2
t2
7−→b + cx + ax2
t2
7−→c + ax + bx2
t2
7−→a + bx + cx2 · · ·
and the chains of rangespaces and nullspaces are trivial.
P2 = P2 = · · ·
{⃗0 } = {⃗0 } = · · ·
Thus, obviously, generalized spaces are R∞(t2) = P2 and N∞(t2) = {⃗0 }.
(d) We have


a
b
c

7→


a
a
b

7→


a
a
a

7→


a
a
a

7→· · ·
and so the chain of rangespaces
R3 ⊃{


p
p
r

¯¯ p, r ∈C} ⊃{


p
p
p

¯¯ p ∈C} = · · ·
and the chain of nullspaces
{⃗0 } ⊂{


0
0
r

¯¯ r ∈C} ⊂{


0
q
r

¯¯ q, r ∈C} = · · ·
each has length two. The generalized spaces are the ﬁnal ones shown above in each chain.
Five.III.1.10
Each maps x 7→t(t(t(x))).
Five.III.1.11
Recall that if W is a subspace of V then any basis BW for W can be enlarged to make
a basis BV for V . From this the ﬁrst sentence is immediate. The second sentence is also not hard: W
is the span of BW and if W is a proper subspace then V is not the span of BW , and so BV must have
at least one vector more than does BW .
Five.III.1.12
It is both ‘if’ and ‘only if’. We have seen earlier that a linear map is nonsingular if and
only if it preserves dimension, that is, if the dimension of its range equals the dimension of its domain.
With a transformation t: V →V that means that the map is nonsingular if and only if it is onto:
R(t) = V (and thus R(t2) = V , etc).

406
Linear Algebra, by Hefferon
Five.III.1.13
The nullspaces form chains because because if ⃗v ∈N (tj) then tj(⃗v) = ⃗0 and tj+1(⃗v) =
t( tj(⃗v) ) = t(⃗0) = ⃗0 and so ⃗v ∈N (tj+1).
Now, the “further” property for nullspaces follows from that fact that it holds for rangespaces,
along with the prior exercise. Because the dimension of R(tj) plus the dimension of N (tj) equals the
dimension n of the starting space V , when the dimensions of the rangespaces stop decreasing, so do
the dimensions of the nullspaces. The prior exercise shows that from this point k on, the containments
in the chain are not proper — the nullspaces are equal.
Five.III.1.14
(Of course, many examples are correct, but here is one.) An example is the shift operator
on triples of reals (x, y, z) 7→(0, x, y). The nullspace is all triples that start with two zeros. The map
stabilizes after three iterations.
Five.III.1.15
The diﬀerentiation operator d/dx: P1 →P1 has the same rangespace as nullspace. For
an example of where they are disjoint — except for the zero vector — consider an identity map (or any
nonsingular map).
Subsection Five.III.2: Strings
Five.III.2.17
Three. It is at least three because ℓ2( (1, 1, 1) ) = (0, 0, 1) ̸= ⃗0. It is at most three because
(x, y, z) 7→(0, x, y) 7→(0, 0, x) 7→(0, 0, 0).
Five.III.2.18
(a) The domain has dimension four. The map’s action is that any vector in the space
c1·⃗β1+c2·⃗β2+c3·⃗β3+c4·⃗β4 is sent to c1·⃗β2+c2·⃗0+c3·⃗β4+c4·⃗0 = c1·⃗β3+c3·⃗β4. The ﬁrst application
of the map sends two basis vectors ⃗β2 and ⃗β4 to zero, and therefore the nullspace has dimension two
and the rangespace has dimension two. With a second application, all four basis vectors are sent to
zero and so the nullspace of the second power has dimension four while the rangespace of the second
power has dimension zero. Thus the index of nilpotency is two. This is the canonical form.




0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0




(b) The dimension of the domain of this map is six.
For the ﬁrst power the dimension of the
nullspace is four and the dimension of the rangespace is two. For the second power the dimension of
the nullspace is ﬁve and the dimension of the rangespace is one. Then the third iteration results in
a nullspace of dimension six and a rangespace of dimension zero. The index of nilpotency is three,
and this is the canonical form.








0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0








(c) The dimension of the domain is three, and the index of nilpotency is three. The ﬁrst power’s
null space has dimension one and its range space has dimension two. The second power’s null space
has dimension two and its range space has dimension one. Finally, the third power’s null space has
dimension three and its range space has dimension zero. Here is the canonical form matrix.


0
0
0
1
0
0
0
1
0


Five.III.2.19
By Lemma 1.3 the nullity has grown as large as possible by the n-th iteration where n
is the dimension of the domain. Thus, for the 2×2 matrices, we need only check whether the square
is the zero matrix. For the 3×3 matrices, we need only check the cube.
(a) Yes, this matrix is nilpotent because its square is the zero matrix.
(b) No, the square is not the zero matrix.
µ
3
1
1
3
¶2
=
µ
10
6
6
10
¶

Answers to Exercises
407
(c) Yes, the cube is the zero matrix. In fact, the square is zero.
(d) No, the third power is not the zero matrix.


1
1
4
3
0
−1
5
2
7


3
=


206
86
304
26
8
26
438
180
634


(e) Yes, the cube of this matrix is the zero matrix.
Another way to see that the second and fourth matrices are not nilpotent is to note that they are
nonsingular.
Five.III.2.20
The table os calculations
p
N p
N (N p)
1






0
1
1
0
1
0
0
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0






{






r
u
−u −v
u
v






¯¯ r, u, v ∈C}
2






0
0
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0






{






r
s
−u −v
u
v






¯¯ r, s, u, v ∈C}
2
–zero matrix–
C5
gives these requirements of the string basis: three basis vectors are sent directly to zero, one more
basis vector is sent to zero by a second application, and the ﬁnal basis vector is sent to zero by a third
application. Thus, the string basis has this form.
⃗β1 7→⃗β2 7→⃗β3 7→⃗0
⃗β4 7→⃗0
⃗β5 7→⃗0
From that the canonical form is immediate.






0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0






Five.III.2.21
(a) The canonical form has a 3×3 block and a 2×2 block






0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
1
0






corresponding to the length three string and the length two string in the basis.
(b) Assume that N is the representation of the underlying map with respect to the standard basis.
Let B be the basis to which we will change. By the similarity diagram
C2
w.r.t. E2
n
−−−−→
N
C2
w.r.t. E2
id
yP
id
yP
C2
w.r.t. B
n
−−−−→C2
w.r.t. B
we have that the canonical form matrix is PNP −1 where
P −1 = RepB,E5(id) =






1
0
0
0
0
0
1
0
1
0
1
0
1
0
0
0
0
1
1
1
0
0
0
0
1







408
Linear Algebra, by Hefferon
and P is the inverse of that.
P = RepE5,B(id) = (P −1)−1 =






1
0
0
0
0
−1
1
1
−1
1
−1
0
1
0
0
1
0
−1
1
−1
0
0
0
0
1






(c) The calculation to check this is routine.
Five.III.2.22
(a) The calculation
p
N p
N (N p)
1
µ1/2
−1/2
1/2
−1/2
¶
{
µu
u
¶ ¯¯ u ∈C}
2
–zero matrix–
C2
shows that any map represented by the matrix must act on the string basis in this way
⃗β1 7→⃗β2 7→⃗0
because the nullspace after one application has dimension one and exactly one basis vector, ⃗β2, is
sent to zero. Therefore, this representation with respect to ⟨⃗β1, ⃗β2⟩is the canonical form.
µ0
0
1
0
¶
(b) The calculation here is similar to the prior one.
p
N p
N (N p)
1


0
0
0
0
−1
1
0
−1
1


{


u
v
v

¯¯ u, v ∈C}
2
–zero matrix–
C3
The table shows that the string basis is of the form
⃗β1 7→⃗β2 7→⃗0
⃗β3 7→⃗0
because the nullspace after one application of the map has dimension two — ⃗β2 and ⃗β3 are both
sent to zero — and one more iteration results in the additional vector being brought to zero.
(c) The calculation
p
N p
N (N p)
1


−1
1
−1
1
0
1
1
−1
1


{


u
0
−u

¯¯ u ∈C}
2


1
0
1
0
0
0
−1
0
−1


{


u
v
−u

¯¯ u, v ∈C}
3
–zero matrix–
C3
shows that any map represented by this basis must act on a string basis in this way.
⃗β1 7→⃗β2 7→⃗β3 7→⃗0
Therefore, this is the canonical form.


0
0
0
1
0
0
0
1
0


Five.III.2.23
A couple of examples
µ
0
0
1
0
¶ µ
a
b
c
d
¶
=
µ
0
0
a
b
¶


0
0
0
1
0
0
0
1
0




a
b
c
d
e
f
g
h
i

=


0
0
0
a
b
c
d
e
f


suggest that left multiplication by a block of subdiagonal ones shifts the rows of a matrix downward.
Distinct blocks




0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0








a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p



=




0
0
0
0
a
b
c
d
0
0
0
0
i
j
k
l




act to shift down distinct parts of the matrix.
Right multiplication does an analgous thing to columns. See Exercise 17.

Answers to Exercises
409
Five.III.2.24
Yes.
Generalize the last sentence in Example 2.9.
As to the index, that same last
sentence shows that the index of the new matrix is less than or equal to the index of ˆN, and reversing
the roles of the two matrices gives inequality in the other direction.
Another answer to this question is to show that a matrix is nilpotent if and only if any associated
map is nilpotent, and with the same index. Then, because similar matrices represent the same map,
the conclusion follows. This is Exercise 30 below.
Five.III.2.25
Observe that a canonical form nilpotent matrix has only zero eigenvalues; e.g., the
determinant of this lower-triangular matrix


−x
0
0
1
−x
0
0
1
−x


is (−x)3, the only root of which is zero. But similar matrices have the same eigenvalues and every
nilpotent matrix is similar to one in canonical form.
Another way to see this is to observe that a nilpotent matrix sends all vectors to zero after some
number of iterations, but that conﬂicts with an action on an eigenspace ⃗v 7→λ⃗v unless λ is zero.
Five.III.2.26
No, by Lemma 1.3 for a map on a two-dimensional space, the nullity has grown as large
as possible by the second iteration.
Five.III.2.27
The index of nilpotency of a transformation can be zero only when the vector starting
the string must be ⃗0, that is, only when V is a trivial space.
Five.III.2.28
(a) Any member ⃗w of the span can be written as a linear combination ⃗w = c0 ·⃗v + c1 ·
t(⃗v) + · · · + ck−1 · tk−1(⃗v). But then, by the linearity of the map, t(⃗w) = c0 · t(⃗v) + c1 · t2(⃗v) + · · · +
ck−2 · tk−1(⃗v) + ck−1 ·⃗0 is also in the span.
(b) The operation in the prior item, when iterated k times, will result in a linear combination of
zeros.
(c) If ⃗v = ⃗0 then the set is empty and so is linearly independent by deﬁnition. Otherwise write
c1⃗v + · · · + ck−1tk−1(⃗v) = ⃗0 and apply tk−1 to both sides. The right side gives ⃗0 while the left side
gives c1tk−1(⃗v); conclude that c1 = 0. Continue in this way by applying tk−2 to both sides, etc.
(d) Of course, t acts on the span by acting on this basis as a single, k-long, t-string.









0
0
0
0
. . .
0
0
1
0
0
0
. . .
0
0
0
1
0
0
. . .
0
0
0
0
1
0
0
0
...
0
0
0
0
1
0









Five.III.2.29
We must check that B ∪ˆC ∪{⃗v1, . . . ,⃗vj} is linearly independent where B is a t-string
basis for R(t), where ˆC is a basis for N (t), and where t(⃗v1) = ⃗β1, . . . , t(⃗vi = ⃗βi. Write
⃗0 = c1,−1⃗v1 + c1,0⃗β1 + c1,1t(⃗β1) + · · · + c1,h1th1(⃗⃗β1) + c2,−1⃗v2 + · · · + cj,hithi(⃗βi)
and apply t.
⃗0 = c1,−1⃗β1 + c1,0t(⃗β1) + · · · + c1,h1−1th1(⃗⃗β1) + c1,h1⃗0 + c2,−1⃗β2 + · · · + ci,hi−1thi(⃗βi) + ci,hi⃗0
Conclude that the coeﬃcients c1,−1, . . . , c1,hi−1, c2,−1, . . . , ci,hi−1 are all zero as B ∪ˆC is a basis.
Substitute back into the ﬁrst displayed equation to conclude that the remaining coeﬃcients are zero
also.
Five.III.2.30
For any basis B, a transformation n is nilpotent if and only if N = RepB,B(n) is a
nilpotent matrix. This is because only the zero matrix represents the zero map and so nj is the zero
map if and only if N j is the zero matrix.
Five.III.2.31
It can be of any size greater than or equal to one. To have a transformation that is
nilpotent of index four, whose cube has rangespace of dimension k, take a vector space, a basis for

410
Linear Algebra, by Hefferon
that space, and a transformation that acts on that basis in this way.
⃗β1
7→
⃗β2
7→
⃗β3
7→⃗β4 7→⃗0
⃗β5
7→
⃗β6
7→
⃗β7
7→⃗β8 7→⃗0
...
⃗β4k−3 7→⃗β4k−2 7→⃗β4k−1 7→⃗β4k 7→⃗0
...
–possibly other, shorter, strings–
So the dimension of the rangespace of T 3 can be as large as desired. The smallest that it can be is
one — there must be at least one string or else the map’s index of nilpotency would not be four.
Five.III.2.32
These two have only zero for eigenvalues
µ
0
0
0
0
¶
µ
0
0
1
0
¶
but are not similar (they have diﬀerent canonical representatives, namely, themselves).
Five.III.2.33
A simple reordering of the string basis will do. For instance, a map that is assoicated
with this string basis
⃗β1 7→⃗β2 7→⃗0
is represented with respect to B = ⟨⃗β1, ⃗β2⟩by this matrix
µ
0
0
1
0
¶
but is represented with respect to B = ⟨⃗β2, ⃗β1⟩in this way.
µ
0
1
0
0
¶
Five.III.2.34
Let t: V →V be the transformation. If rank(t) = nullity(t) then the equation rank(t)+
nullity(t) = dim(V ) shows that dim(V ) is even.
Five.III.2.35
For the matrices to be nilpotent they must be square. For them to commute they must
be the same size. Thus their product and sum are deﬁned.
Call the matrices A and B. To see that AB is nilpotent, multiply (AB)2 = ABAB = AABB =
A2B2, and (AB)3 = A3B3, etc., and, as A is nilpotent, that product is eventually zero.
The sum is similar; use the Binomial Theorem.
Five.III.2.36
Some experimentation gives the idea for the proof. Expansion of the second power
t2
S(T) = S(ST −TS) −(ST −TS)S = S2 −2STS + TS2
the third power
t3
S(T) = S(S2 −2STS + TS2) −(S2 −2STS + TS2)S
= S3T −3S2TS + 3STS2 −TS3
and the fourth power
t4
S(T) = S(S3T −3S2TS + 3STS2 −TS3) −(S3T −3S2TS + 3STS2 −TS3)S
= S4T −4S3TS + 6S2TS2 −4STS3 + TS4
suggest that the expansions follow the Binomial Theorem. Verifying this by induction on the power
of tS is routine. This answers the question because, where the index of nilpotency of S is k, in the
expansion of t2k
S
t2k
S (T) =
X
0≤i≤2k
(−1)i
µ2k
i
¶
SiTS2k−i
for any i at least one of the Si and S2k−i has a power higher than k, and so the term gives the zero
matrix.
Five.III.2.37
Use the geometric series: I −N k+1 = (I −N)(N k + N k−1 + · · · + I). If N k+1 is the zero
matrix then we have a right inverse for I −N. It is also a left inverse.
This statement is not ‘only if’ since µ1
0
0
1
¶
−
µ−1
0
0
−1
¶
is invertible.

Answers to Exercises
411
Subsection Five.IV.1: Polynomials of Maps and Matrices
Five.IV.1.13
For each, the minimal polynomial must have a leading coeﬃcient of 1 and Theorem 1.8,
the Cayley-Hamilton Theorem, says that the minimal polynomial must contain the same linear factors
as the characteristic polynomial, although possibly of lower degree but not of zero degree.
(a) The possibilities are m1(x) = x −3, m2(x) = (x −3)2, m3(x) = (x −3)3, and m4(x) = (x −3)4.
Note that the 8 has been dropped because a minimal polynomial must have a leading coeﬃcient of
one. The ﬁrst is a degree one polynomial, the second is degree two, the third is degree three, and
the fourth is degree four.
(b) The possibilities are m1(x) = (x+1)(x−4), m2(x) = (x+1)2(x−4), and m3(x) = (x+1)3(x−4).
The ﬁrst is a quadratic polynomial, that is, it has degree two. The second has degree three, and the
third has degree four.
(c) We have m1(x) = (x −2)(x −5), m2(x) = (x −2)2(x −5), m3(x) = (x −2)(x −5)2, and
m4(x) = (x −2)2(x −5)2. They are polynomials of degree two, three, three, and four.
(d) The possiblities are m1(x) = (x + 3)(x −1)(x −2), m2(x) = (x + 3)2(x −1)(x −2), m3(x) =
(x + 3)(x −1)(x −2)2, and m4(x) = (x + 3)2(x −1)(x −2)2. The degree of m1 is three, the degree
of m2 is four, the degree of m3 is four, and the degree of m4 is ﬁve.
Five.IV.1.14
In each case we will use the method of Example 1.12.
(a) Because T is triangular, T −xI is also triangular
T −xI =


3 −x
0
0
1
3 −x
0
0
0
4 −x


the characteristic polynomial is easy c(x) = |T −xI| = (3−x)2(4−x) = −1·(x−3)2(x−4). There are
only two possibilities for the minimal polynomial, m1(x) = (x−3)(x−4) and m2(x) = (x−3)2(x−4).
(Note that the characteristic polynomial has a negative sign but the minimal polynomial does not
since it must have a leading coeﬃcient of one). Because m1(T) is not the zero matrix
(T −3I)(T −4I) =


0
0
0
1
0
0
0
0
1




−1
0
0
1
−1
0
0
0
0

=


0
0
0
−1
0
0
0
0
0


the minimal polynomial is m(x) = m2(x).
(T −3I)2(T −4I) = (T −3I) ·
¡
(T −3I)(T −4I)
¢
=


0
0
0
1
0
0
0
0
1




0
0
0
−1
0
0
0
0
0

=


0
0
0
0
0
0
0
0
0


(b) As in the prior item, the fact that the matrix is triangular makes computation of the characteristic
polynomial easy.
c(x) = |T −xI| =
¯¯¯¯¯¯
3 −x
0
0
1
3 −x
0
0
0
3 −x
¯¯¯¯¯¯
= (3 −x)3 = −1 · (x −3)3
There are three possibilities for the minimal polynomial m1(x) = (x −3), m2(x) = (x −3)2, and
m3(x) = (x −3)3. We settle the question by computing m1(T)
T −3I =


0
0
0
1
0
0
0
0
0


and m2(T).
(T −3I)2 =


0
0
0
1
0
0
0
0
0




0
0
0
1
0
0
0
0
0

=


0
0
0
0
0
0
0
0
0


Because m2(T) is the zero matrix, m2(x) is the minimal polynomial.
(c) Again, the matrix is triangular.
c(x) = |T −xI| =
¯¯¯¯¯¯
3 −x
0
0
1
3 −x
0
0
1
3 −x
¯¯¯¯¯¯
= (3 −x)3 = −1 · (x −3)3

412
Linear Algebra, by Hefferon
Again, there are three possibilities for the minimal polynomial m1(x) = (x −3), m2(x) = (x −3)2,
and m3(x) = (x −3)3. We compute m1(T)
T −3I =


0
0
0
1
0
0
0
1
0


and m2(T)
(T −3I)2 =


0
0
0
1
0
0
0
1
0




0
0
0
1
0
0
0
1
0

=


0
0
0
0
0
0
1
0
0


and m3(T).
(T −3I)3 = (T −3I)2(T −3I) =


0
0
0
0
0
0
1
0
0




0
0
0
1
0
0
0
1
0

=


0
0
0
0
0
0
0
0
0


Therefore, the minimal polynomial is m(x) = m3(x) = (x −3)3.
(d) This case is also triangular, here upper triangular.
c(x) = |T −xI| =
¯¯¯¯¯¯
2 −x
0
1
0
6 −x
2
0
0
2 −x
¯¯¯¯¯¯
= (2 −x)2(6 −x) = −(x −2)2(x −6)
There are two possibilities for the minimal polynomial, m1(x) = (x −2)(x −6) and m2(x) =
(x −2)2(x −6). Computation shows that the minimal polynomial isn’t m1(x).
(T −2I)(T −6I) =


0
0
1
0
4
2
0
0
0




−4
0
1
0
0
2
0
0
−4

=


0
0
−4
0
0
0
0
0
0


It therefore must be that m(x) = m2(x) = (x −2)2(x −6). Here is a veriﬁcation.
(T −2I)2(T −6I) = (T −2I) ·
¡
(T −2I)(T −6I)
¢
=


0
0
1
0
4
2
0
0
0




0
0
−4
0
0
0
0
0
0

=


0
0
0
0
0
0
0
0
0


(e) The characteristic polynomial is
c(x) = |T −xI| =
¯¯¯¯¯¯
2 −x
2
1
0
6 −x
2
0
0
2 −x
¯¯¯¯¯¯
= (2 −x)2(6 −x) = −(x −2)2(x −6)
and there are two possibilities for the minimal polynomial, m1(x) = (x −2)(x −6) and m2(x) =
(x −2)2(x −6). Checking the ﬁrst one
(T −2I)(T −6I) =


0
2
1
0
4
2
0
0
0




−4
2
1
0
0
2
0
0
−4

=


0
0
0
0
0
0
0
0
0


shows that the minimal polynomial is m(x) = m1(x) = (x −2)(x −6).
(f) The characteristic polynomial is this.
c(x) = |T −xI| =
¯¯¯¯¯¯¯¯¯¯
−1 −x
4
0
0
0
0
3 −x
0
0
0
0
−4
−1 −x
0
0
3
−9
−4
2 −x
−1
1
5
4
1
4 −x
¯¯¯¯¯¯¯¯¯¯
= (x −3)3(x + 1)2
There are a number of possibilities for the minimal polynomial, listed here by ascending degree:
m1(x) = (x −3)(x + 1), m1(x) = (x −3)2(x + 1), m1(x) = (x −3)(x + 1)2, m1(x) = (x −3)3(x + 1),
m1(x) = (x −3)2(x + 1)2, and m1(x) = (x −3)3(x + 1)2. The ﬁrst one doesn’t pan out
(T −3I)(T + 1I) =






−4
4
0
0
0
0
0
0
0
0
0
−4
−4
0
0
3
−9
−4
−1
−1
1
5
4
1
1












0
4
0
0
0
0
4
0
0
0
0
−4
0
0
0
3
−9
−4
3
−1
1
5
4
1
5






=






0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−4
−4
0
−4
−4
4
4
0
4
4







Answers to Exercises
413
but the second one does.
(T −3I)2(T + 1I) = (T −3I)
¡
(T −3I)(T + 1I)
¢
=






−4
4
0
0
0
0
0
0
0
0
0
−4
−4
0
0
3
−9
−4
−1
−1
1
5
4
1
1












0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−4
−4
0
−4
−4
4
4
0
4
4






=






0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0






The minimal polynomial is m(x) = (x −3)2(x + 1).
Five.IV.1.15
Its characteristic polynomial has complex roots.
¯¯¯¯¯¯
−x
1
0
0
−x
1
1
0
−x
¯¯¯¯¯¯
= (1 −x) · (x −(−1
2 +
√
3
2 i)) · (x −(−1
2 −
√
3
2 i))
As the roots are distinct, the characteristic polynomial equals the minimal polynomial.
Five.IV.1.16
We know that Pn is a dimension n + 1 space and that the diﬀerentiation operator is
nilpotent of index n+1 (for instance, taking n = 3, P3 = {c3x3 + c2x2 + c1x + c0
¯¯ c3, . . . , c0 ∈C} and
the fourth derivative of a cubic is the zero polynomial). Represent this operator using the canonical
form for nilpotent transformations.







0
0
0
. . .
0
1
0
0
0
0
1
0
...
0
0
0
1
0







This is an (n + 1)×(n + 1) matrix with an easy characteristic polynomial, c(x) = xn+1. (Remark: this
matrix is RepB,B(d/dx) where B = ⟨xn, nxn−1, n(n−1)xn−2, . . . , n!⟩.) To ﬁnd the minimal polynomial
as in Example 1.12 we consider the powers of T −0I = T. But, of course, the ﬁrst power of T that is
the zero matrix is the power n + 1. So the minimal polynomial is also xn+1.
Five.IV.1.17
Call the matrix T and suppose that it is n×n. Because T is triangular, and so T −xI
is triangular, the characteristic polynomial is c(x) = (x −λ)n. To see that the minimal polynomial is
the same, consider T −λI.







0
0
0
. . .
0
1
0
0
. . .
0
0
1
0
...
0
0
. . .
1
0







Recognize it as the canonical form for a transformation that is nilpotent of degree n; the power (T −λI)j
is zero ﬁrst when j is n.
Five.IV.1.18
The n = 3 case provides a hint. A natural basis for P3 is B = ⟨1, x, x2, x3⟩. The action
of the transformation is
1 7→1
x 7→x + 1
x2 7→x2 + 2x + 1
x3 7→x3 + 3x2 + 3x + 1
and so the representation RepB,B(t) is this upper triangular matrix.




1
1
1
1
0
1
2
3
0
0
1
3
0
0
0
1





414
Linear Algebra, by Hefferon
Because it is triangular, the fact that the characteristic polynomial is c(x) = (x −1)4 is clear. For the
minimal polynomial, the candidates are m1(x) = (x −1),
T −1I =




0
1
1
1
0
0
2
3
0
0
0
3
0
0
0
0




m2(x) = (x −1)2,
(T −1I)2 =




0
0
2
6
0
0
0
6
0
0
0
0
0
0
0
0




m3(x) = (x −1)3,
(T −1I)3 =




0
0
0
6
0
0
0
0
0
0
0
0
0
0
0
0




and m4(x) = (x −1)4. Because m1, m2, and m3 are not right, m4 must be right, as is easily veriﬁed.
In the case of a general n, the representation is an upper triangular matrix with ones on the diagonal.
Thus the characteristic polynomial is c(x) = (x−1)n+1. One way to verify that the minimal polynomial
equals the characteristic polynomial is argue something like this: say that an upper triangular matrix
is 0-upper triangular if there are nonzero entries on the diagonal, that it is 1-upper triangular if the
diagonal contains only zeroes and there are nonzero entries just above the diagonal, etc. As the above
example illustrates, an induction argument will show that, where T has only nonnegative entries, T j
is j-upper triangular. That argument is left to the reader.
Five.IV.1.19
The map twice is the same as the map once: π ◦π = π, that is, π2 = π and so the
minimal polynomial is of degree at most two since m(x) = x2 −x will do. The fact that no linear
polynomial will do follows from applying the maps on the left and right side of c1 · π + c0 · id = z
(where z is the zero map) to these two vectors.


0
0
1




1
0
0


Thus the minimal polynomial is m.
Five.IV.1.20
This is one answer.


0
0
0
1
0
0
0
0
0


Five.IV.1.21
The x must be a scalar, not a matrix.
Five.IV.1.22
The characteristic polynomial of
T =
µ
a
b
c
d
¶
is (a −x)(d −x) −bc = x2 −(a + d)x + (ad −bc). Substitute
µa
b
c
d
¶2
−(a + d)
µa
b
c
d
¶
+ (ad −bc)
µ1
0
0
1
¶
=
µ
a2 + bc
ab + bd
ac + cd
bc + d2
¶
−
µ
a2 + ad
ab + bd
ac + cd
ad + d2
¶
+
µ
ad −bc
0
0
ad −bc
¶
and just check each entry sum to see that the result is the zero matrix.
Five.IV.1.23
By the Cayley-Hamilton theorem the degree of the minimal polynomial is less than or
equal to the degree of the characteristic polynomial, n. Example 1.6 shows that n can happen.
Five.IV.1.24
Suppose that t’s only eigenvalue is zero. Then the characteristic polynomial of t is xn.
Because t satisﬁes its characteristic polynomial, it is a nilpotent map.
Five.IV.1.25
A minimal polynomial must have leading coeﬃcient 1, and so if the minimal polynomial
of a map or matrix were to be a degree zero polynomial then it would be m(x) = 1. But the identity
map or matrix equals the zero map or matrix only on a trivial vector space.

Answers to Exercises
415
So in the nontrivial case the minimal polynomial must be of degree at least one. A zero map or
matrix has minimal polynomial m(x) = x, and an identity map or matrix has minimal polynomial
m(x) = x −1.
Five.IV.1.26
The polynomial can be read geometrically to say “a 60◦rotation minus two rotations of
30◦equals the identity.”
Five.IV.1.27
For a diagonal matrix
T =





t1,1
0
0
t2,2
...
tn,n





the characteristic polynomial is (t1,1 −x)(t2,2 −x) · · · (tn,n −x). Of course, some of those factors may
be repeated, e.g., the matrix might have t1,1 = t2,2. For instance, the characteristic polynomial of
D =


3
0
0
0
3
0
0
0
1


is (3 −x)2(1 −x) = −1 · (x −3)2(x −1).
To form the minimal polynomial, take the terms x −ti,i, throw out repeats, and multiply them
together.
For instance, the minimal polynomial of D is (x −3)(x −1).
To check this, note ﬁrst
that Theorem 1.8, the Cayley-Hamilton theorem, requires that each linear factor in the characteristic
polynomial appears at least once in the minimal polynomial. One way to check the other direction —
that in the case of a diagonal matrix, each linear factor need appear at most once — is to use a matrix
argument. A diagonal matrix, multiplying from the left, rescales rows by the entry on the diagonal.
But in a product (T −t1,1I) · · · , even without any repeat factors, every row is zero in at least one of
the factors.
For instance, in the product
(D −3I)(D −1I) = (D −3I)(D −1I)I =


0
0
0
0
0
0
0
0
−2




2
0
0
0
2
0
0
0
0




1
0
0
0
1
0
0
0
1


because the ﬁrst and second rows of the ﬁrst matrix D −3I are zero, the entire product will have a
ﬁrst row and second row that are zero. And because the third row of the middle matrix D −1I is zero,
the entire product has a third row of zero.
Five.IV.1.28
This subsection starts with the observation that the powers of a linear transformation
cannot climb forever without a “repeat”, that is, that for some power n there is a linear relationship
cn · tn + · · · + c1 · t + c0 · id = z where z is the zero transformation. The deﬁnition of projection is
that for such a map one linear relationship is quadratic, t2 −t = z. To ﬁnish, we need only consider
whether this relationship might not be minimal, that is, are there projections for which the minimal
polynomial is constant or linear?
For the minimal polynomial to be constant, the map would have to satisfy that c0 · id = z, where
c0 = 1 since the leading coeﬃcient of a minimal polynomial is 1. This is only satisﬁed by the zero
transformation on a trivial space. This is indeed a projection, but not a very interesting one.
For the minimal polynomial of a transformation to be linear would give c1 · t + c0 · id = z where
c1 = 1.
This equation gives t = −c0 · id.
Coupling it with the requirement that t2 = t gives
t2 = (−c0)2 · id = −c0 · id, which gives that c0 = 0 and t is the zero transformation or that c0 = 1 and
t is the identity.
Thus, except in the cases where the projection is a zero map or an identity map, the minimal
polynomial is m(x) = x2 −x.
Five.IV.1.29
(a) This is a property of functions in general, not just of linear functions. Suppose
that f and g are one-to-one functions such that f ◦g is deﬁned. Let f ◦g(x1) = f ◦g(x2), so that
f(g(x1)) = f(g(x2)). Because f is one-to-one this implies that g(x1) = g(x2). Because g is also
one-to-one, this in turn implies that x1 = x2. Thus, in summary, f ◦g(x1) = f ◦g(x2) implies that
x1 = x2 and so f ◦g is one-to-one.
(b) If the linear map h is not one-to-one then there are unequal vectors ⃗v1, ⃗v2 that map to the same
value h(⃗v1) = h(⃗v2). Because h is linear, we have ⃗0 = h(⃗v1) −h(⃗v2) = h(⃗v1 −⃗v2) and so ⃗v1 −⃗v2 is
a nonzero vector from the domain that is mapped by h to the zero vector of the codomain (⃗v1 −⃗v2
does not equal the zero vector of the domain because ⃗v1 does not equal ⃗v2).

416
Linear Algebra, by Hefferon
(c) The minimal polynomial m(t) sends every vector in the domain to zero and so it is not one-to-one
(except in a trivial space, which we ignore). By the ﬁrst item of this question, since the composition
m(t) is not one-to-one, at least one of the components t −λi is not one-to-one. By the second item,
t −λi has a nontrivial nullspace. Because (t −λi)(⃗v) = ⃗0 holds if and only if t(⃗v) = λi · ⃗v, the
prior sentence gives that λi is an eigenvalue (recall that the deﬁnition of eigenvalue requires that
the relationship hold for at least one nonzero ⃗v).
Five.IV.1.30
This is false. The natural example of a non-diagonalizable transformation works here.
Consider the transformation of C2 represented with respect to the standard basis by this matrix.
N =
µ
0
1
0
0
¶
The characteristic polynomial is c(x) = x2. Thus the minimal polynomial is either m1(x) = x or
m2(x) = x2. The ﬁrst is not right since N −0 · I is not the zero matrix, thus in this example the
minimal polynomial has degree equal to the dimension of the underlying space, and, as mentioned, we
know this matrix is not diagonalizable because it is nilpotent.
Five.IV.1.31
Let A and B be similar A = PBP −1. From the facts that
An = (PBP −1)n = (PBP −1)(PBP −1) · · · (PBP −1)
= PB(P −1P)B(P −1P) · · · (P −1P)BP −1 = PBnP −1
and c · A = c · (PBP −1) = P(c · B)P −1 follows the required fact that for any polynomial function f
we have f(A) = P f(B) P −1. For instance, if f(x) = x2 + 2x + 3 then
A2 + 2A + 3I = (PBP −1)2 + 2 · PBP −1 + 3 · I
= (PBP −1)(PBP −1) + P(2B)P −1 + 3 · PP −1 = P(B2 + 2B + 3I)P −1
shows that f(A) is similar to f(B).
(a) Taking f to be a linear polynomial we have that A −xI is similar to B −xI. Similar matrices
have equal determinants (since |A| = |PBP −1| = |P| · |B| · |P −1| = 1 · |B| · 1 = |B|). Thus the
characteristic polynomials are equal.
(b) As P and P −1 are invertible, f(A) is the zero matrix when, and only when, f(B) is the zero
matrix.
(c) They cannot be similar since they don’t have the same characteristic polynomial. The charac-
teristic polynomial of the ﬁrst one is x2 −4x −3 while the characteristic polynomial of the second
is x2 −5x + 5.
Five.IV.1.32
Suppose that m(x) = xn + mn−1xn−1 + · · · + m1x + m0 is minimal for T.
(a) For the ‘if’ argument, because T n + · · · + m1T + m0I is the zero matrix we have that I =
(T n+· · ·+m1T)/(−m0) = T·(T n−1+· · ·+m1I)/(−m0) and so the matrix (−1/m0)·(T n−1+· · ·+m1I)
is the inverse of T. For ‘only if’, suppose that m0 = 0 (we put the n = 1 case aside but it is easy) so
that T n + · · · + m1T = (T n−1 + · · · + m1I)T is the zero matrix. Note that T n−1 + · · · + m1I is not
the zero matrix because the degree of the minimal polynomial is n. If T −1 exists then multiplying
both (T n−1 + · · · + m1I)T and the zero matrix from the right by T −1 gives a contradiction.
(b) If T is not invertible then the constant term in its minimal polynomial is zero. Thus,
T n + · · · + m1T = (T n−1 + · · · + m1I)T = T(T n−1 + · · · + m1I)
is the zero matrix.
Five.IV.1.33
(a) For the inductive step, assume that Lemma 1.7 is true for polynomials of degree
i, . . . , k −1 and consider a polynomial f(x) of degree k. Factor f(x) = k(x −λ1)q1 · · · (x −λℓ)qℓand
let k(x −λ1)q1−1 · · · (x −λℓ)qℓbe cn−1xn−1 + · · · + c1x + c0. Substitute:
k(t −λ1)q1 ◦· · · ◦(t −λℓ)qℓ(⃗v) = (t −λ1) ◦(t −λ1)q1 ◦· · · ◦(t −λℓ)qℓ(⃗v)
= (t −λ1) (cn−1tn−1(⃗v) + · · · + c0⃗v)
= f(t)(⃗v)
(the second equality follows from the inductive hypothesis and the third from the linearity of t).
(b) One example is to consider the squaring map s: R →R given by s(x) = x2. It is nonlinear. The
action deﬁned by the polynomial f(t) = t2 −1 changes s to f(s) = s2 −1, which is this map.
x
s2−1
7−→s ◦s(x) −1 = x4 −1
Observe that this map diﬀers from the map (s −1) ◦(s + 1); for instance, the ﬁrst map takes x = 5
to 624 while the second one takes x = 5 to 675.

Answers to Exercises
417
Five.IV.1.34
Yes. Expand down the last column to check that xn + mn−1xn−1 + · · · + m1x + m0 is
plus or minus the determinant of this.







−x
0
0
m0
0
1 −x
0
m1
0
0
1 −x
m2
...
1 −x
mn−1







Subsection Five.IV.2: Jordan Canonical Form
Five.IV.2.17
We are required to check that
µ
3
0
1
3
¶
= N + 3I = PTP −1 =
µ
1/2
1/2
−1/4
1/4
¶ µ
2
−1
1
4
¶ µ
1
−2
1
2
¶
That calculation is easy.
Five.IV.2.18
(a) The characteristic polynomial is c(x) = (x−3)2 and the minimal polynomial is the
same.
(b) The characteristic polynomial is c(x) = (x + 1)2. The minimal polynomial is m(x) = x + 1.
(c) The characteristic polynomial is c(x) = (x + (1/2))(x −2)2 and the minimal polynomial is the
same.
(d) The characteristic polynomial is c(x) = (x −3)3 The minimal polynomial is the same.
(e) The characteristic polynomial is c(x) = (x −3)4. The minimal polynomial is m(x) = (x −3)2.
(f) The characteristic polynomial is c(x) = (x+4)2(x−4)2 and the minimal polynomial is the same.
(g) The characteristic polynomial is c(x) = (x −2)2(x −3)(x −5) and the minimal polynomial is
m(x) = (x −2)(x −3)(x −5).
(h) The characteristic polynomial is c(x) = (x −2)2(x −3)(x −5) and the minimal polynomial is the
same.
Five.IV.2.19
(a) The transformation t −3 is nilpotent (that is, N∞(t −3) is the entire space) and
it acts on a string basis via two strings, ⃗β1 7→⃗β2 7→⃗β3 7→⃗β4 7→⃗0 and ⃗β5 7→⃗0. Consequently, t −3
can be represented in this canonical form.
N3 =






0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0






and therefore T is similar to this this canonical form matrix.
J3 = N3 + 3I =






3
0
0
0
0
1
3
0
0
0
0
1
3
0
0
0
0
1
3
0
0
0
0
0
3






(b) The restriction of the transformation s + 1 is nilpotent on the subspace N∞(s + 1), and the
action on a string basis is given as ⃗β1 7→⃗0. The restriction of the transformation s −2 is nilpotent
on the subspace N∞(s −2), having the action on a string basis of ⃗β2 7→⃗β3 7→⃗0 and ⃗β4 7→⃗β5 7→⃗0.
Consequently the Jordan form is this






−1
0
0
0
0
0
2
0
0
0
0
1
2
0
0
0
0
0
2
0
0
0
0
1
2






(note that the blocks are arranged with the least eigenvalue ﬁrst).

418
Linear Algebra, by Hefferon
Five.IV.2.20
For each, because many choices of basis are possible, many other answers are possible.
Of course, the calculation to check if an answer gives that PTP −1 is in Jordan form is the arbiter of
what’s correct.
(a) Here is the arrow diagram.
C3
w.r.t. E3
t
−−−−→
T
C3
w.r.t. E3
id
yP
id
yP
C3
w.r.t. B
t
−−−−→
J
C3
w.r.t. B
The matrix to move from the lower left to the upper left is this.
P −1 =
¡
RepE3,B(id)
¢−1 = RepB,E3(id) =


1
−2
0
1
0
1
−2
0
0


The matrix P to move from the upper right to the lower right is the inverse of P −1.
(b) We want this matrix and its inverse.
P −1 =


1
0
3
0
1
4
0
−2
0


(c) The concatenation of these bases for the generalized null spaces will do for the basis for the entire
space.
B−1 = ⟨






−1
0
0
1
0






,






−1
0
−1
0
1






⟩
B3 = ⟨






1
1
−1
0
0






,






0
0
0
−2
2






,






−1
−1
1
2
0






⟩
The change of basis matrices are this one and its inverse.
P −1 =






−1
−1
1
0
−1
0
0
1
0
−1
0
−1
−1
0
1
1
0
0
−2
2
0
1
0
2
0






Five.IV.2.21
The general procedure is to factor the characteristic polynomial c(x) = (x −λ1)p1(x −
λ2)p2 · · · to get the eigenvalues λ1, λ2, etc. Then, for each λi we ﬁnd a string basis for the action of the
transformation t −λi when restricted to N∞(t −λi), by computing the powers of the matrix T −λiI
and ﬁnding the associated null spaces, until these null spaces settle down (do not change), at which
point we have the generalized null space. The dimensions of those null spaces (the nullities) tell us
the action of t −λi on a string basis for the generalized null space, and so we can write the pattern of
subdiagonal ones to have Nλi. From this matrix, the Jordan block Jλi associated with λi is immediate
Jλi = Nλi + λiI. Finally, after we have done this for each eigenvalue, we put them together into the
canonical form.
(a) The characteristic polynomial of this matrix is c(x) = (−10 −x)(10 −x) + 100 = x2, so it has
only the single eigenvalue λ = 0.
power p
(T + 0 · I)p
N ((t −0)p)
nullity
1
Ã
−10
4
−25
10
!
{
Ã
2y/5
y
!
¯¯ y ∈C}
1
2
Ã
0
0
0
0
!
C2
2
(Thus, this transformation is nilpotent: N∞(t −0) is the entire space). From the nullities we know
that t’s action on a string basis is ⃗β1 7→⃗β2 7→⃗0. This is the canonical form matrix for the action of
t −0 on N∞(t −0) = C2
N0 =
µ
0
0
1
0
¶
and this is the Jordan form of the matrix.
J0 = N0 + 0 · I =
µ
0
0
1
0
¶

Answers to Exercises
419
Note that if a matrix is nilpotent then its canonical form equals its Jordan form.
We can ﬁnd such a string basis using the techniques of the prior section.
B = ⟨
µ1
0
¶
,
µ−10
−25
¶
⟩
The ﬁrst basis vector has been taken so that it is in the null space of t2 but is not in the null space
of t. The second basis vector is the image of the ﬁrst under t.
(b) The characteristic polynomial of this matrix is c(x) = (x+1)2, so it is a single-eigenvalue matrix.
(That is, the generalized null space of t + 1 is the entire space.) We have
N (t + 1) = {
µ
2y/3
y
¶ ¯¯ y ∈C}
N ((t + 1)2) = C2
and so the action of t + 1 on an associated string basis is ⃗β1 7→⃗β2 7→⃗0. Thus,
N−1 =
µ
0
0
1
0
¶
the Jordan form of T is
J−1 = N−1 + −1 · I =
µ
−1
0
1
−1
¶
and choosing vectors from the above null spaces gives this string basis (many other choices are
possible).
B = ⟨
µ
1
0
¶
,
µ
6
9
¶
⟩
(c) The characteristic polynomial c(x) = (1 −x)(4 −x)2 = −1 · (x −1)(x −4)2 has two roots and
they are the eigenvalues λ1 = 1 and λ2 = 4.
We handle the two eigenvalues separately. For λ1, the calculation of the powers of T −1I yields
N (t −1) = {


0
y
0

¯¯ y ∈C}
and the null space of (t −1)2 is the same. Thus this set is the generalized null space N∞(t −1).
The nullities show that the action of the restriction of t −1 to the generalized null space on a string
basis is ⃗β1 7→⃗0.
A similar calculation for λ2 = 4 gives these null spaces.
N (t −4) = {


0
z
z

¯¯ z ∈C}
N ((t −4)2) = {


y −z
y
z

¯¯ y, z ∈C}
(The null space of (t −4)3 is the same, as it must be because the power of the term associated with
λ2 = 4 in the characteristic polynomial is two, and so the restriction of t −2 to the generalized null
space N∞(t −2) is nilpotent of index at most two — it takes at most two applications of t −2 for
the null space to settle down.) The pattern of how the nullities rise tells us that the action of t −4
on an associated string basis for N∞(t −4) is ⃗β2 7→⃗β3 7→⃗0.
Putting the information for the two eigenvalues together gives the Jordan form of the transfor-
mation t.


1
0
0
0
4
0
0
1
4


We can take elements of the nullspaces to get an appropriate basis.
B = B1
⌢B4 = ⟨


0
1
0

,


1
0
1

,


0
5
5

⟩
(d) The characteristic polynomial is c(x) = (−2 −x)(4 −x)2 = −1 · (x + 2)(x −4)2.
For the eigenvalue λ−2, calculation of the powers of T + 2I yields this.
N (t + 2) = {


z
z
z

¯¯ z ∈C}
The null space of (t + 2)2 is the same, and so this is the generalized null space N∞(t + 2). Thus the
action of the restriction of t + 2 to N∞(t + 2) on an associated string basis is ⃗β1 7→⃗0.

420
Linear Algebra, by Hefferon
For λ2 = 4, computing the powers of T −4I yields
N (t −4) = {


z
−z
z

¯¯ z ∈C}
N ((t −4)2) = {


x
−z
z

¯¯ x, z ∈C}
and so the action of t −4 on a string basis for N∞(t −4) is ⃗β2 7→⃗β3 7→⃗0.
Therefore the Jordan form is


−2
0
0
0
4
0
0
1
4


and a suitable basis is this.
B = B−2
⌢B4 = ⟨


1
1
1

,


0
−1
1

,


−1
1
−1

⟩
(e) The characteristic polynomial of this matrix is c(x) = (2 −x)3 = −1 · (x −2)3. This matrix has
only a single eigenvalue, λ = 2. By ﬁnding the powers of T −2I we have
N (t −2) = {


−y
y
0

¯¯ y ∈C}
N ((t −2)2) = {


−y −(1/2)z
y
z

¯¯ y, z ∈C}
N ((t −2)3) = C3
and so the action of t −2 on an associated string basis is ⃗β1 7→⃗β2 7→⃗β3 7→⃗0. The Jordan form is
this


2
0
0
1
2
0
0
1
2


and one choice of basis is this.
B = ⟨


0
1
0

,


7
−9
4

,


−2
2
0

⟩
(f) The characteristic polynomial c(x) = (1 −x)3 = −(x −1)3 has only a single root, so the matrix
has only a single eigenvalue λ = 1. Finding the powers of T −1I and calculating the null spaces
N (t −1) = {


−2y + z
y
z

¯¯ y, z ∈C}
N ((t −1)2) = C3
shows that the action of the nilpotent map t −1 on a string basis is ⃗β1 7→⃗β2 7→⃗0 and ⃗β3 7→⃗0.
Therefore the Jordan form is
J =


1
0
0
1
1
0
0
0
1


and an appropriate basis (a string basis associated with t −1) is this.
B = ⟨


0
1
0

,


2
−2
−2

,


1
0
1

⟩
(g) The characteristic polynomial is a bit large for by-hand calculation, but just manageable c(x) =
x4 −24x3 + 216x2 −864x + 1296 = (x −6)4. This is a single-eigenvalue map, so the transformation
t −6 is nilpotent. The null spaces
N (t−6) = {




−z −w
−z −w
z
w




¯¯ z, w ∈C}
N ((t−6)2) = {




x
−z −w
z
w




¯¯ x, z, w ∈C}
N ((t−6)3) = C4
and the nullities show that the action of t −6 on a string basis is ⃗β1 7→⃗β2 7→⃗β3 7→⃗0 and ⃗β4 7→⃗0.
The Jordan form is




6
0
0
0
1
6
0
0
0
1
6
0
0
0
0
6





Answers to Exercises
421
and ﬁnding a suitable string basis is routine.
B = ⟨




0
0
0
1



,




2
−1
−1
2



,




3
3
−6
3



,




−1
−1
1
0



⟩
Five.IV.2.22
There are two eigenvalues, λ1 = −2 and λ2 = 1. The restriction of t + 2 to N∞(t + 2)
could have either of these actions on an associated string basis.
⃗β1 7→⃗β2 7→⃗0
⃗β1 7→⃗0
⃗β2 7→⃗0
The restriction of t −1 to N∞(t −1) could have either of these actions on an associated string basis.
⃗β3 7→⃗β4 7→⃗0
⃗β3 7→⃗0
⃗β4 7→⃗0
In combination, that makes four possible Jordan forms, the two ﬁrst actions, the second and ﬁrst, the
ﬁrst and second, and the two second actions.




−2
0
0
0
1
−2
0
0
0
0
1
0
0
0
1
1








−2
0
0
0
0
−2
0
0
0
0
1
0
0
0
1
1








−2
0
0
0
1
−2
0
0
0
0
1
0
0
0
0
1








−2
0
0
0
0
−2
0
0
0
0
1
0
0
0
0
1




Five.IV.2.23
The restriction of t + 2 to N∞(t + 2) can have only the action ⃗β1 7→⃗0. The restriction
of t −1 to N∞(t −1) could have any of these three actions on an associated string basis.
⃗β2 7→⃗β3 7→⃗β4 7→⃗0
⃗β2 7→⃗β3 7→⃗0
⃗β4 7→⃗0
⃗β2 7→⃗0
⃗β3 7→⃗0
⃗β4 7→⃗0
Taken together there are three possible Jordan forms, the one arising from the ﬁrst action by t −1
(along with the only action from t + 2), the one arising from the second action, and the one arising
from the third action. 



−2
0
0
0
0
1
0
0
0
1
1
0
0
0
1
1








−2
0
0
0
0
1
0
0
0
1
1
0
0
0
0
1








−2
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1




Five.IV.2.24
The action of t + 1 on a string basis for N∞(t + 1) must be ⃗β1 7→⃗0. Because of the
power of x −2 in the minimal polynomial, a string basis for t −2 has length two and so the action of
t −2 on N∞(t −2) must be of this form.
⃗β2 7→⃗β3 7→⃗0
⃗β4 7→⃗0
Therefore there is only one Jordan form that is possible.




−1
0
0
0
0
2
0
0
0
1
2
0
0
0
0
2




Five.IV.2.25
There are two possible Jordan forms. The action of t + 1 on a string basis for N∞(t + 1)
must be ⃗β1 7→⃗0. There are two actions for t −2 on a string basis for N∞(t −2) that are possible with
this characteristic polynomial and minimal polynomial.
⃗β2 7→⃗β3 7→⃗0
⃗β4 7→⃗β5 7→⃗0
⃗β2 7→⃗β3 7→⃗0
⃗β4 7→⃗0
⃗β5 7→⃗0
The resulting Jordan form matrics are these.






−1
0
0
0
0
0
2
0
0
0
0
1
2
0
0
0
0
0
2
0
0
0
0
1
2












−1
0
0
0
0
0
2
0
0
0
0
1
2
0
0
0
0
0
2
0
0
0
0
0
2







422
Linear Algebra, by Hefferon
Five.IV.2.26
(a) The characteristic polynomial is c(x) = x(x −1). For λ1 = 0 we have
N (t −0) = {
µ−y
y
¶ ¯¯ y ∈C}
(of course, the null space of t2 is the same). For λ2 = 1,
N (t −1) = {
µ
x
0
¶ ¯¯ x ∈C}
(and the null space of (t −1)2 is the same). We can take this basis
B = ⟨
µ
1
−1
¶
,
µ
1
0
¶
⟩
to get the diagonalization.
µ
1
1
−1
0
¶−1 µ
1
1
0
0
¶ µ
1
1
−1
0
¶
=
µ
0
0
0
1
¶
(b) The characteristic polynomial is c(x) = x2 −1 = (x + 1)(x −1). For λ1 = −1,
N (t + 1) = {
µ
−y
y
¶ ¯¯ y ∈C}
and the null space of (t + 1)2 is the same. For λ2 = 1
N (t −1) = {
µy
y
¶ ¯¯ y ∈C}
and the null space of (t −1)2 is the same. We can take this basis
B = ⟨
µ
1
−1
¶
,
µ
1
1
¶
⟩
to get a diagonalization.
µ1
1
1
−1
¶−1 µ0
1
1
0
¶ µ 1
1
−1
1
¶
=
µ−1
0
0
1
¶
Five.IV.2.27
The transformation d/dx: P3 →P3 is nilpotent. Its action on B = ⟨x3, 3x2, 6x, 6⟩is
x3 7→3x2 7→6x 7→6 7→0. Its Jordan form is its canonical form as a nilpotent matrix.
J =




0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
0




Five.IV.2.28
Yes. Each has the characteristic polynomial (x + 1)2. Calculations of the powers of
T1 + 1 · I and T2 + 1 · I gives these two.
N (t1 + 1) = {
µ
y/2
y
¶ ¯¯ y ∈C}
N (t2 + 1) = {
µ
0
y
¶ ¯¯ y ∈C}
(Of course, for each the null space of the square is the entire space.) The way that the nullities rise
shows that each is similar to this Jordan form matrix
µ
−1
0
1
−1
¶
and they are therefore similar to each other.
Five.IV.2.29
Its characteristic polynomial is c(x) = x2 + 1 which has complex roots x2 + 1 = (x +
i)(x −i). Because the roots are distinct, the matrix is diagonalizable and its Jordan form is that
diagonal matrix.
µ
−i
0
0
i
¶
To ﬁnd an associated basis we compute the null spaces.
N (t + i) = {
µ
−iy
y
¶ ¯¯ y ∈C}
N (t −i) = {
µ
iy
y
¶ ¯¯ y ∈C}
For instance,
T + i · I =
µ
i
−1
1
i
¶

Answers to Exercises
423
and so we get a description of the null space of t + i by solving this linear system.
ix −y = 0
x + iy = 0
iρ1+ρ2
−→
ix −y = 0
0 = 0
(To change the relation ix = y so that the leading variable x is expressed in terms of the free variable
y, we can multiply both sides by −i.)
As a result, one such basis is this.
B = ⟨
µ
−i
1
¶
,
µ
i
1
¶
⟩
Five.IV.2.30
We can count the possible classes by counting the possible canonical representatives,
that is, the possible Jordan form matrices. The characteristic polynomial must be either c1(x) =
(x + 3)2(x −4) or c2(x) = (x + 3)(x −4)2. In the c1 case there are two possible actions of t + 3 on a
string basis for N∞(t + 3).
⃗β1 7→⃗β2 7→⃗0
⃗β1 7→⃗0
⃗β2 7→⃗0
There are two associated Jordan form matrices.


−3
0
0
1
−3
0
0
0
4




−3
0
0
0
−3
0
0
0
4


Similarly there are two Jordan form matrices that could arise out of c2.


−3
0
0
0
4
0
0
1
4




−3
0
0
0
4
0
0
0
4


So in total there are four possible Jordan forms.
Five.IV.2.31
Jordan form is unique. A diagonal matrix is in Jordan form. Thus the Jordan form of
a diagonalizable matrix is its diagonalization. If the minimal polynomial has factors to some power
higher than one then the Jordan form has subdiagonal 1’s, and so is not diagonal.
Five.IV.2.32
One example is the transformation of C that sends x to −x.
Five.IV.2.33
Apply Lemma 2.7 twice; the subspace is t −λ1 invariant if and only if it is t invariant,
which in turn holds if and only if it is t −λ2 invariant.
Five.IV.2.34
False; these two 4×4 matrices each have c(x) = (x −3)4 and m(x) = (x −3)2.




3
0
0
0
1
3
0
0
0
0
3
0
0
0
1
3








3
0
0
0
1
3
0
0
0
0
3
0
0
0
0
3




Five.IV.2.35
(a) The characteristic polynomial is this.
¯¯¯¯
a −x
b
c
d −x
¯¯¯¯ = (a −x)(d −x) −bc = ad −(a + d)x + x2 −bc = x2 −(a + d)x + (ad −bc)
Note that the determinant appears as the constant term.
(b) Recall that the characteristic polynomial |T −xI| is invariant under similarity. Use the permu-
tation expansion formula to show that the trace is the negative of the coeﬃcient of xn−1.
(c) No, there are matrices T and S that are equivalent S = PTQ (for some nonsingular P and Q)
but that have diﬀerent traces. An easy example is this.
PTQ =
µ
2
0
0
1
¶ µ
1
0
0
1
¶ µ
1
0
0
1
¶
=
µ
2
0
0
1
¶
Even easier examples using 1×1 matrices are possible.
(d) Put the matrix in Jordan form. By the ﬁrst item, the trace is unchanged.
(e) The ﬁrst part is easy; use the third item. The converse does not hold: this matrix
µ
1
0
0
−1
¶
has a trace of zero but is not nilpotent.
Five.IV.2.36
Suppose that BM is a basis for a subspace M of some vector space. Implication one way
is clear; if M is t invariant then in particular, if ⃗m ∈BM then t(⃗m) ∈M. For the other implication,
let BM = ⟨⃗β1, . . . , ⃗βq⟩and note that t(⃗m) = t(m1⃗β1 + · · · + mq⃗βq) = m1t(⃗β1) + · · · + mqt(⃗βq) is in M
as any subspace is closed under linear combinations.

424
Linear Algebra, by Hefferon
Five.IV.2.37
Yes, the intersection of t invariant subspaces is t invariant. Assume that M and N are
t invariant. If ⃗v ∈M ∩N then t(⃗v) ∈M by the invariance of M and t(⃗v) ∈N by the invariance of N.
Of course, the union of two subspaces need not be a subspace (remember that the x- and y-axes
are subspaces of the plane R2 but the union of the two axes fails to be closed under vector addition,
for instance it does not contain ⃗e1 +⃗e2.) However, the union of invariant subsets is an invariant subset;
if ⃗v ∈M ∪N then ⃗v ∈M or ⃗v ∈N so t(⃗v) ∈M or t(⃗v) ∈N.
No, the complement of an invariant subspace need not be invariant. Consider the subspace
{
µ
x
0
¶ ¯¯ x ∈C}
of C2 under the zero transformation.
Yes, the sum of two invariant subspaces is invariant. The check is easy.
Five.IV.2.38
One such ordering is the dictionary ordering. Order by the real component ﬁrst, then
by the coeﬃcient of i. For instance, 3 + 2i < 4 + 1i but 4 + 1i < 4 + 2i.
Five.IV.2.39
The ﬁrst half is easy — the derivative of any real polynomial is a real polynomial of lower
degree. The answer to the second half is ‘no’; any complement of Pj(R) must include a polynomial of
degree j + 1, and the derivative of that polynomial is in Pj(R).
Five.IV.2.40
For the ﬁrst half, show that each is a subspace and then observe that any polynomial
can be uniquely written as the sum of even-powered and odd-powered terms (the zero polynomial is
both). The answer to the second half is ‘no’: x2 is even while 2x is odd.
Five.IV.2.41
Yes. If RepB,B(t) has the given block form, take BM to be the ﬁrst j vectors of B, where
J is the j×j upper left submatrix. Take BN to be the remaining k vectors in B. Let M and N be the
spans of BM and BN. Clearly M and N are complementary. To see M is invariant (N works the same
way), represent any ⃗m ∈M with respect to B, note the last k components are zeroes, and multiply by
the given block matrix. The ﬁnal k components of the result are zeroes, so that result is again in M.
Five.IV.2.42
Put the matrix in Jordan form. By non-singularity, there are no zero eigenvalues on the
diagonal. Ape this example:


9
0
0
1
9
0
0
0
4

=


3
0
0
1/6
3
0
0
0
2


2
to construct a square root. Show that it holds up under similarity: if S2 = T then (PSP −1)(PSP −1) =
PTP −1.
Topic: Method of Powers
1
(a) The largest eigenvalue is 4.
(b) The largest eigenvalue is 2.
3
(a) The largest eigenvalue is 3.
(b) The largest eigenvalue is −3.
5
In theory, this method would produce λ2. In practice, however, rounding errors in the computation
introduce components in the direction of ⃗v1, and so the method will still produce λ1, although it may
take somewhat longer than it would have taken with a more fortunate choice of initial vector.
6
Instead of using ⃗vk = T⃗vk−1, use T −1⃗vk = ⃗vk−1.
Topic: Stable Populations

Answers to Exercises
425
Topic: Linear Recurrences
1
(a) We express the relation in matrix form.
µ
5
−6
1
0
¶ µ
f(n)
f(n −1)
¶
=
µ
f(n + 1)
f(n)
¶
The characteristic equation of the matrix
¯¯¯¯
5 −λ
−6
1
−λ
¯¯¯¯ = λ2 −5λ + 6
has roots of 2 and 3. Any function of the form f(n) = c12n + c23n satisﬁes the recurrence.
(b) This is like the prior part, but simpler. The matrix expression of the relation is
¡
4
¢ ¡
f(n)
¢
=
¡
f(n + 1)
¢
and the characteristic equation of the matrix
¯¯4 −λ
¯¯ = 4 −λ
has the single root 4. Any function of the form f(n) = c4n satisﬁes this recurrence.
(c) In matrix form the relation

6
7
6
1
0
0
0
1
0




f(n)
f(n −1)
f(n −2)

=


f(n + 1)
f(n)
f(n −1)


gives this characteristic equation.
¯¯¯¯¯¯
6 −λ
7
6
1
−λ
0
0
1
−λ
¯¯¯¯¯¯
= −λ3 −6λ2 + 7λ + 6


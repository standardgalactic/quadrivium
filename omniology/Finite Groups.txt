Chapter 1
Group Representations
Deﬁnition 1.1 A representation α of a group G in a vector space V over k is
deﬁned by a homomorphism
α : G →GL(V ).
The degree of the representation is the dimension of the vector space:
deg α = dimk V.
Remarks:
1. Recall that GL(V )—the general linear group on V —is the group of invert-
ible (or non-singular) linear maps t : V →V .
2. We shall be concerned almost exclusively with representations of ﬁnite de-
gree, that is, in ﬁnite-dimensional vector spaces; and these will almost al-
ways be vector spaces over R or C. Therefore, to avoid repetition, let us
agree to use the term ‘representation’ to mean representation of ﬁnite de-
gree over R or C, unless the contrary is explicitly stated.
Furthermore, in this ﬁrst Part we shall be concerned almost exclusively with
ﬁnite groups; so let us also agree that the term ‘group’ will mean ﬁnite group
in this Part, unless the contrary is stated.
3. Suppose {e1, . . . , en} is a basis for V . Then each linear map t : V →V is
deﬁned (with respect to this basis) by an n × n-matrix T. Explicitly,
tej =
X
i
Tijei;
424–I
1–1

424–I
1–2
or in terms of coordinates,




x1
...
xn



7→T




x1
...
xn




Thus a representation in V can be deﬁned by a homomorphism
α : G →GL(n, k),
where GL(n, k) denotes the group of non-singular n×n-matrices over k. In
other words, α is deﬁned by giving matrices A(g) for each g ∈G, satisfying
the conditions
A(gh) = A(g)A(h)
for all g, h ∈G; and also
A(e) = I.
4. There is another way of looking at group representations which is almost
always more fruitful than the rather abstract deﬁnition we started with.
Recall that a group is said to act on the set X if we have a map
G × X →X : (g, x) 7→gx
satisfying
(a) (gh)x) = g(hx),
(b) ex = x.
Now suppose X = V is a vector space. Then we can say that G acts linearly
on V if in addition
(c) g(u + v) = gu + gv,
(d) g(ρv) = ρ(gv).
Each representation α of G in V deﬁnes a linear action of G on V , by
gv = α(g)v;
and every such action arises from a representation in this way.
Thus the notions of representation and linear action are completely equiv-
alent. We can use whichever we ﬁnd more convenient in a given case.

424–I
1–3
5. There are two other ways of looking at group representations, completely
equivalent to the deﬁnition but expressing slightly different points of view.
Firstly, we may speak of the vector space V , with the action of G on it.
as a G-space. For those familiar with category theory, this would be the
categorical approach. Representation theory, from this point of view, is the
study of the category of G-spaces and G-maps, where a G-map
t : U →V
from one G-space to another is a linear map preserving the action of G, ie
satisfying
t(gu) = g(tu)
(g ∈G, u ∈U).
6. Secondly, and ﬁnally, mathematical physicists often speak—strikingly—of
the vector space V carrying the representation α.
Examples:
1. Recall that the dihedral group D4 is the symmetry group of a square ABCD
A
B
C
D
O
x
y
Figure 1.1: The natural representation of D4
(Figure ??). Let us take coordinates Ox, Oy as shown through the centre O
of the square. Then
D4 = {e, r, r2, r3, c, d, x, y},
where r is the rotation about O through π/2 (sending A to B), while c, d, x, y
are the reﬂections in AC, BD, Ox, Oy respectively.

424–I
1–4
By deﬁnition a symmetry g ∈D4 is an isometry of the plane E2 sending
the square into itself. Evidently g must send O into itself, and so gives rise
to a linear map
A(g) : R2 →R2.
The map
g 7→A(g) ∈GL(2, R)
deﬁnes a 2-dimensional representation ρ of D4 over R. We may describe
this as the natural 2-dimensional representation of D4.
(Evidently the symmetry group G of any bounded subset S ⊂En will have
a similar ‘natural’ representation in Rn.)
The representation ρ is given in matrix terms by
e 7→
 
1
0
0
1
!
, r 7→
 
0
−1
1
0
!
, r2 7→
 
−1
0
0
−1
!
, r3 7→
 
0
1
−1
0
!
,
c 7→
 
0
1
1
0
!
, d 7→
 
0
−1
−1
0
!
, x 7→
 
1
0
0
−1
!
, y 7→
 
−1
0
0
1
!
.
Each group relation is represented in a corresponding matrix equation, eg
cd = r2 =⇒
 
0
1
1
0
!  
0
−1
−1
0
!
=
 
−1
0
0
−1
!
.
The representation ρ is faithful, ie the homomorphism deﬁning it is injec-
tive. Thus a relation holds in D4 if and only if the corresponding matrix
equation is true. However, representations are not necessarily faithful, and
in general the implication is only one way.
Every ﬁnite-dimensional representation can be expressed in matrix form in
this way, after choosing a basis for the vector space carrying the representa-
tion. However, while such matrix representations are reassuringly concrete,
they are impractical except in the lowest dimensions. Better just to keep at
the back of one’s mind that a representation could be expressed in this way.
2. Suppose G acts on the set X:
(g, x) 7→gx.
Let
C(X) = C(X, k)
denote the space of maps
f : X →k.

424–I
1–5
Then G acts linearly on C(X)—and so deﬁnes a representation ρ of G—by
gf(x) = f(g−1x).
(We need g−1 rather than g on the right to satisfy the rule
g(hf) = (gh)f.
It is fortunate that the relation
(gh)−1 = h−1g−1
enables us to correct the order reversal. We shall often have occasion to
take advantage of this, particularly when dealing—as here—with spaces of
functions.)
Now suppose that X is ﬁnite; say
X = {x1, . . . , xn}.
Then
deg ρ = n = ∥X∥,
the number of elements in X. For the functions
ey(x) =
(
1 if x = y,
0 otherwise.
(ie the characteristic functions of the 1-point subsets) form a basis for C(X).
Also
gex = egx,
since
gey(x) = ey(g−1x)
=



1
if g−1x = y
0
if g−1x ̸= y
=



1
if x = gy
0
if x ̸= gy
Thus
g 7→P(g)

424–I
1–6
where P = P(g) is the matrix with entries
Pxy =
(
1 if y = gx,
0 otherwise.
Notice that P is a permutation matrix, ie there is just one 1 in each row and
column, all other entries being 0. We call a representation that arises from
the action of a group on a set in this way a permutational representation.
As an illustration, consider the natural action of S(3) on the set {a, b, c}.
This yields a 3-dimensional representation ρ of S(3), under which
(abc)
7→



0
1
0
0
0
1
1
0
0



(ab)
7→



0
1
0
1
0
0
0
0
1



(These two instances actually deﬁne the representation, since (abc) and (ab)
generate S(3).)
3. A 1-dimensional representation α of a group G over k = R or C is just a
homomorphism
α : G →k×,
where k× denotes the multiplicative group on the set k \ {0}. For
GL(1, k) = k×,
since we can identify the 1 × 1-matrix [x] with its single entry x.
We call the 1-dimensional representation deﬁned by the identity homomor-
phism
g 7→1
(for all g ∈G) the trivial representation of G, and denote it by 1.
In a 1-dimensional representation, each group element is represented by a
number. Since these numbers commute, the study of 1-dimensional repre-
sentations is much simpler than those of higher dimension.
In general, when investigating the representations of a group G, we start by
determining all its 1-dimensional representations.
Recall that two elements g, h ∈G are said to be conjugate if
h = xgx−1

424–I
1–7
for some third element x ∈G. Suppose α is a 1-dimensional representation
of G. Then
α(h)
=
α(x)α(g)α(x−1)
=
α(x)α(g)α(x)−1
=
α(g)α(x)α(x)−1
=
α(g),
since the numbers α(x), α(g) commute. It follows that a 1-dimensional
representation is constant on each conjugacy class of G.
Consider the group S3. This has 3 classes (we shall usually abbreviate ‘con-
jugacy class’ to class):
{1}, {(abc), (acb)}, {(bc), (ca), (ab)}.
Let us write
s = (abc), t = (bc).
Then (assuming k = C)
s3 = 1
=⇒
α(s)3 = 1 =⇒α(s) = 1, ω or ω2,
t2 = 1
=⇒
α(s)2 = 1 =⇒α(t) = ±1.
But
tst−1 = s2.
It follows that
α(t)α(s)α(t)−1 = α(s)2,
from which we deduce that
α(s) = 1.
It follows that S3 has just two 1-dimensional representations: the trivial
representation
1 : g 7→1,
and the parity representation
ϵ : g 7→
(
1
if g is even,
−1
if g is odd.
4. The corresponding result is true for all the symmetric groups Sn (for n ≥2);
Sn has just two 1-dimensional representations, the trivial representation 1
and the parity representation ϵ.
To see this, let us recall two facts about Sn.

424–I
1–8
(a) The transpositions τ = (xy) generate Sn, ie each permutation g ∈Sn
is expressible (not uniquely) as a product of transpositions
g = τ1 · · · τr.
(b) The transpositions are all conjugate.
(This is a particular case of the general fact that two permutations in
Sn are conjugate if and only if they are of the same cyclic type, ie they
have the same number of cycles of each length.)
It follows from (1) that a 1-dimensional representation of Sn is completely
determined by its values on the transpositions. It follows from (2) that the
representation is constant on the transpositions. Finally, since each trans-
position τ satisﬁes τ 2 = 1 it follows that this constant value is ±1. Thus
there can only be two 1-dimensional representations of Sn; the ﬁrst takes the
value 1 on the transpositions, and so is 1 everywhere; the second takes the
value -1 on the transpositions, and takes the value (−1)r on the permutation
g = τ1 · · · τr.
Thus Sn has just two 1-dimensional representations; the trivial representa-
tion 1 and the parity representation ϵ.
5. Let’s look again at the dihedral group D4, ie the symmetry group of the
square ABCD. Let r denote the rotation through π/2, taking A into B; and
let c denote the reﬂection in AC.
It is readily veriﬁed that r and c generate D4, ie each element g ∈D4 is
expressible as a word in r and c (eg g = r2cr). This follows for example
from Lagrange’s Theorem. The subgroup generated by r and c contains at
least the 5 elements 1, r, r2, r3, c, and so must be the whole group. (We shall
sometimes denote the identity element in a group by 1, while at other times
we shall use e or I.)
It is also easy to see that r and c satisfy the relations
r4 = 1, c2 = 1, rc = cr3.
In fact these are deﬁning relations for D4, ie every relation between r and c
can be derived from these 3.
We can express this in the form
D4 = ⟨r, c : r4 = c2 = 1, rc = cr3⟩.

424–I
1–9
Now suppose α is a 1-dimensional representation of D4. Then we must
have
α(r)4 = α(c)2 = 1, α(r)α(c) = α(c)α(r)3.
From the last relation
α(r)2 = 1.
Thus there are just 4 possibilities
α(r) = ±1, α(c) = ±1.
It is readily veriﬁed that all 4 of these satisfy the 3 deﬁning relations for s
and t. It follows that each deﬁnes a homomorphism
α : D4 →k×.
We conclude that D4 has just 4 1-dimensional representations.
6. We look now at some examples from chemistry and physics. It should be
emphasized, ﬁrstly that the theory is completely independent of these ex-
amples, which can safely be ignored; and secondly, that we are not on oath
when speaking of physics. It would be inappropriate to delve too deeply
here into the physical basis for the examples we give.
Figure 1.2: The methane molecule
First let us look at the methane molecule CH4. In its stable state the 4
hydrogen atoms are situated at the vertices of a regular tetrahedron, with
the single carbon atom at its centroid (Figure ??).
The molecule evidently has symmetry group S4, being invariant under per-
mutations of the 4 hydrogen atoms.
Now suppose the molecule is vibrating about this stable position. We sup-
pose that the carbon atom at the centroid remains ﬁxed. (We shall return to
this point later.) Thus the conﬁguration of the molecule at any moment is
deﬁned by the displacement of the 4 hydrogen atoms, say
Xi = (xi1, xi2, xi3)
(i = 1, 2, 3, 4).

424–I
1–10
Since the centroid remains ﬁxed,
X
i
xij = 0
(j = 1, 2, 3).
This reduces the original 12 degrees of freedom to 9.
Now let us assume further that the angular momentum also remains 0, ie
the molecule is not slowly rotating. This imposes a further 3 conditions on
the xij, leaving 6 degrees of freedom for the 12 ‘coordinates’ xij. Mathe-
matically, the coordinates are constrained to lie in a 6-dimensional space.
In other words we can ﬁnd 6 ‘generalized coordinates’ q1, . . . , q6 — chosen
so that q1 = q2 = · · · = q6 = 0 at the point of equilibrium — such that each
of the xij is expressible in terms of the qk:
xij = xij(q1, . . . , q6).
The motion of the molecule is governed by the Euler-Lagrange equations
d
dt
 ∂K
∂˙qk
!
= −∂V
∂qk
where K is the kinetic energy of the system, and V its potential energy.
(These equations were developed for precisely this purpose, to express the
motion of a system whose conﬁguration is deﬁned by generalized coordi-
nates.)
The kinetic energy of the system is given in terms of the mass m of the
hydrogen atom by
K = 1
2m
X
i,j
˙x2
ij
On substituting
˙xij = ∂xij
∂q1
˙q1 + · · · + ∂xij
∂q6
˙q6,
we see that
K = K( ˙q1, . . . , ˙q6),
where K is a positive-deﬁnite quadratic form. Although the coefﬁcients
of this quadratic form are actually functions of q1, . . . , q6, we may suppose
them constant since we are dealing with small vibrations.
The potential energy of the system, which we may take to have minimal
value 0 at the stable position, is given to second order by some positive-
deﬁnite quadratic form Q in the qk:
V = Q(q1, . . . , q6) + . . . .

424–I
1–11
While we could explicitly choose the coordinates qk, and determine the ki-
netic energy K, the potential energy form Q evidently depends on the forces
holding the molecule together. Fortunately, we can say a great deal about the
vibrational modes of the molecule without knowing anything about these
forces.
Since these two forms are positive-deﬁnite, we can simultaneously diag-
onalize them, ie we can ﬁnd new generalized coordinates z1, . . . , z6 such
that
K = ˙z2
1 + · · · + ˙z2
6,
V = ω2
1z2
1 + · · · + ω2
6z2
6.
The Euler-Lagrange equations now give
¨zi = −ω2
i zi
(i = 1, . . . , 6).
Thus the motion is made up of 6 independent harmonic oscillations, with
frequencies ω1, . . . , ω6.
As usual when studying harmonic or wave motion, life is easier if we allow
complex solutions (of which the ‘real’ solutions will be the real part). Each
harmonic oscillation then has 1 degree of freedom:
zj = Cjeiωjt.
The set of all solutions of these equations (ie all possible vibrations of the
system) thus forms a 6-dimensional solution-space V .
So far we have made no use of the S4-symmetry of the CH4 molecule. But
now we see that this symmetry group acts on the solution space V , which
thus carries a representation, ρ say, of S4. Explicitly, suppose π ∈S4 is
a permutation of the 4 H atoms. This permutation is ‘implemented’ by
a unique spatial isometry Π. (For example, the permutation (123)(4) is
effected by rotation through 1/3 of a revolution about the axis joining the C
atom to the 4th H atom.)
But now if we apply this isometry Π to any vibration v(t) we obtain a new
vibration Πv(t). In this way the permutation π acts on the solution-space
V .
In general, the symmetry group G of the physical conﬁguration will act on
the solution-space V .
The fundamental result in the representation theory of a ﬁnite group G (as
we shall establish) is that every representation ρ of G splits into parts, each

424–I
1–12
corresponding to a ‘simple’ representation of G. Each ﬁnite group has a ﬁ-
nite number of such simple representations, which thus serve as the ‘atoms’
out of which every representation of G is constructed. (There is a close
analogy with the Fundamental Theorem of Arithmetic, that every natural
number is uniquely expressible as a product of primes.)
The group S4 (as we shall ﬁnd) has 5 simple representations, of dimensions
1, 1, 2, 3, 3. Our 6-dimensional representation must be the ‘sum’ of some of
these.
It is not hard to see that there is just one 1-dimensional mode (up to a scalar
multiple) corresponding to a ‘pulsing’ of the molecule in which the 4 H
atoms move in and out (in ‘sync’) along the axes joining them to the cen-
tral C atom. (Recall that S4 has just two 1-dimensional representations: the
trivial representation, under which each permutation leaves everything un-
changed, and the parity representation, in which even permutations leave
things unchanged, while odd permutations reverse them. In our case, the
4 atoms must move in the same way under the trivial representation, while
their motion is reversed under an odd permutation. The latter is impossible.
For by considering the odd permutation (12)(3)(4) we deduce that the ﬁrst
atom is moving out while the second moves in; while under the action of
the even permutation (12)(34) the ﬁrst and second atoms must move in and
out together.)
We conclude (not rigorously, it should be emphasized!) that
ρ = 1 + α + β
where 1 denotes the trivial representation of S4, α is the unique 2-dimensional
representation, and β is one of the two 3-dimensional representations.
Thus without any real work we’ve deduced quite a lot about the vibrations
of CH4.
Each of these 3 modes has a distinct frequency. To see that, note that our
system — and in fact any similar non-relativistic system — has a time sym-
metry corresponding to the additive group R. For if (zj(t) : 1 ≤j ≤6) is
one solution then (zj(t + c)) is also a solution for any constant c ∈R.
The simple representations of R are just the 1-dimensional representations
t 7→eiωt.
(We shall see that the simple representations of an abelian group are always
1-dimensional.) In effect, Fourier analysis — the splitting of a function

424–I
1–13
or motion into parts corresponding to different frequencies — is just the
representation theory of R.
The actions of S4 and R on the solution space commute, giving a represen-
tation of the product group S4 × R.
As we shall see, the simple representations of a product group G × H arise
from simple representations of G and H: ρ = σ × τ. In the present case we
must have
ρ = 1 × E(ω1) + α × E(ω2) + β × E(ω3),
where ω1, ω2, ω3 are the frequencies of the 3 modes.
If the symmetry is slightly broken, eg by placing the vibrating molecule in
a magnetic ﬁeld, these ‘degenerate’ frequencies will split, so that 6 frequen-
cies will be seen: ω′
1, ω′
2, ω′′
2, ω′
3, ω′′
3, ω′′′
3 , where eg ω′
2 and ω′′
2 are close to ω.
This is the origin of ‘multiple lines’ in spectroscopy.
The concept of broken symmetry has become one of the corner-stones of
mathematical physics. In ‘grand uniﬁed theories’ distinct particles are seen
as identical (like our 4 H atoms) under some large symmetry group, whose
action is ‘broken’ in our actual universe.
7. Vibrations of a circular drum. [?]. Consider a circular elastic membrane.
The motion of the membrane is determined by the function
z(x, y, t)
(x2 + y2 ≤r2)
where z is the height of the point of the drum at position (x, y).
It is not hard to establish that under small vibrations this function will satisfy
the wave equation
T
 ∂2z
∂x2 + ∂2z
∂y2
!
= ρ∂2z
∂t2 ,
where T is the tension of the membrane and ρ its mass per unit area. This
may be written
 ∂2z
∂x2 + ∂2z
∂y2
!
= 1
c2
∂2z
∂t2 ,
where c = (T/ρ)1/2 is the speed of the wave motion.
The conﬁguration has O(2) symmetry, where O(2) is the group of 2-dimensional
isometries leaving the centre O ﬁxed, consisting of the rotations about O
and the reﬂections in lines through O.

424–I
1–14
Although this group is not ﬁnite, it is compact. As we shall see, the repre-
sentation theory of compact groups is essentially identical to the ﬁnite the-
ory; the main difference being that a compact group has a countable inﬁnity
of simple representations.
For example, the group O(2) has the trivial representation 1, and an inﬁnity
of representations R(1), R(2), . . . , each of dimension 2.
The circular drum has corresponding modes M(0), M(1), M(2), . . . , each
with its characteristic frequency. As in our last example, after taking time
symmetry into account, the solution-space carries a representation ρ of the
product group O(2) × R, which splits into
1 × E(ω0) + R(1) × E(ω1) + R(2) × E(ω2) + · · · .
8. In the last example but one, we considered the 4 hydrogen atoms in the
methane molecule as particles, or solid balls. But now let us consider a
single hydrogen atom, consisting of an electron moving in the ﬁeld of a
massive central proton.
According to classical non-relativistic quantum mechanics [?], the state of
the electron (and so of the atom) is determined by a wave function ψ(x, y, z, t),
whose evolution is determined by Schr¨odinger’s equation
iℏ∂ψ
∂t = Hψ.
Here H is the hamiltonian operator, given by
Hψ = −ℏ2
2m∇2ψ + V (r)ψ,
where m is the mass of the electron, V (t) is its potential energy, ℏis
Planck’s constant, and
∇2ψ = ∂2ψ
∂x2 + ∂2ψ
∂y2 + ∂2ψ
∂z2 .
Thus Schrodinger’s equation reads, in full,
iℏ∂ψ
∂t = −ℏ2
2m
 ∂2ψ
∂x2 + ∂2ψ
∂y2 + ∂2ψ
∂z2
!
−e2
r ψ.
The essential point is that this is a linear differential equation, whose solu-
tions therefore form a vector space, the solution-space.

424–I
1–15
We regard the central proton as ﬁxed at O. (A more accurate account might
take O to be the centre of mass of the system.) The system is invariant under
the orthogonal group O(3), consisting of all isometries — that is, distance-
preserving transformations — which leave O ﬁxed. Thus the solution space
carries a representation of the compact group O(3).
This group is a product-group:
O(3) = SO(3) × C2,
where C2 = {I, J} (J denoting reﬂection in O), while SO(3) is the sub-
group of orientation-preserving isometries. In fact, each such isometry is a
rotation about some axis, so SO(3) is group of rotations in 3 dimensions.
The rotation group SO(3) has simple representations D0, D1, D2, . . . of
dimensions 1, 3, 5, . . . . To each of these corresponds a mode of the hy-
drogen atom, with a particular frequency ω and corresponding energy level
E = ℏω.
These energy levels are seen in the spectroscope, although the spectral lines
of hydrogen actually correspond to differences between energy levels, since
they arise from photons given off when the energy level changes.
This idea — considering the space of atomic wave functions as a repre-
sentation of SO(3) gave the ﬁrst explanation of the periodic table of the
elements, proposed many years before by Mendeleev on purely empirical
grounds [?].
The discussion above ignores the spin of the electron. In fact representation
theory hints strongly at the existence of spin, since the ‘double-covering’
SU(2) of SO(3) adds the ‘spin representations’ D1/2, D3/2, . . . of dimen-
sions 2, 4, . . . to the sequence above, as we shall see.
Finally, it is worth noting that quantum theory (as also electrodynamics) are
linear theories, where the Principle of Superposition rules. Thus the appli-
cation of representation theory is exact, and not an approximation restricted
to small vibrations, as in classical mechanical systems like the methane
molecule, or the drum.
9. The classiﬁcation of elementary particles.
[?]. Consider an elementary
particle E, eg an electron, in relativistic quantum theory. The possible states
of E again correspond to the points of a vector space V . More precisely,
they correspond to the points of the projective space P(V ) formed by the
rays, or 1-dimensional subspaces, of V . For the wave functions ψ and ρψ
correspond to the same state of E.

424–I
1–16
The state space V is now acted on by the Poincar´e group E(1, 3) formed by
the isometries of Minkowski space-time. It follows that V carries a repre-
sentation of E(1, 3).
Each elementary particle corresponds to a simple representation of the
Poincar´e group E(1, 3). This group is not compact. It is however a Lie
group; and — as we shall see — a different approach to representation the-
ory, based on Lie algebras, allows much of the theory to be extended to this
case.
A last remark. One might suppose, from its reliance on linearity, that rep-
resentation theory would have no rˆole to play in curved space-time. But
that is far from true. Even if the underlying topological space is curved, the
vector and tensor ﬁelds on such a space preserve their linear structure. (So
one can, for example, superpose vector ﬁelds on a sphere.) Thus represen-
tation theory can still be applied; and in fact, the so-called gauge theories
introduced in the search for a uniﬁed ‘theory of everything’ are of precisely
this kind.

Bibliography
[1] C. A. Coulson. Waves. Oliver and Boyd, 1955.
[2] Richard Feynman. Lecture Notes on Physics III. Addison-Wesley, 196.
[3] D. J. Simms. Lie Groups and Quantum Mechanics. Springer, 1968.
[4] Hermann Weyl. The Theory of Groups and Quantum Mechanics. Dover,
1950.
424–I
1–17

BIBLIOGRAPHY
424–I
1–18
Exercises
All representations are over C, unless the contrary is stated.
In Exercises 01–11 determine all 1-dimensional representations of the given group.
1 ∗C2
2 ∗∗C3
3 ∗∗Cn
4 ∗∗D2
5 ∗∗D3
6 ∗∗∗Dn
7 ∗∗∗Q8
8 ∗∗∗A4
9 ∗∗∗∗An
10 ∗∗Z
11 ∗∗∗∗D∞= ⟨r, s : s2 = 1, rsr = s⟩
Suppose G is a group; and suppose g, h ∈G. The element [g, h] = ghg−1h−1 is
called the commutator of g and h. The subgroup G′ ≡[G, G] is generated by all
commutators in G is called the commutator subgroup, or derived group of G.
12 ∗∗∗Show that G′ lies in the kernel of any 1-dimensional representation ρ of G,
ie ρ(g) acts trivially if g ∈G′.
13 ∗∗∗Show that G′ is a normal subgroup of G, and that G/G′ is abelian. Show
moreover that if K is a normal subgroup of G then G/K is abelian if and only if
G′ ⊂K. [In other words, G′ is the smallest normal subgroup such that G/G′ is
abelian.)
14 ∗∗Show that the 1-dimensional representations of G form an abelian group
G∗under multiplication. [Nb: this notation G∗is normally only used when G is
abelian.]
15 ∗∗Show that C∗
n ∼= Cn.
16 ∗∗∗Show that for any 2 groups G, H
(G × H)∗= G∗× H∗.
17 ∗∗∗∗By using the Structure Theorem on Finite Abelian Groups (stating that
each such group is expressible as a product of cyclic groups) or otherwise, show
that
A∗∼= A
for any ﬁnite abelian group A.
18 ∗∗Suppose Θ : G →H is a homomorphism of groups. Then each representa-
tion α of H deﬁnes a representation Θα of G.
19 ∗∗∗Show that the 1-dimensional representations of G and of G/G′ are in one-
one correspondence.
In Exercises 20–24 determine the derived group G′ of the given group G.
20 ∗∗∗Cn
21 ∗∗∗∗Dn
22 ∗∗Z
23 ∗∗∗∗D∞
24 ∗∗∗Q8
25 ∗∗∗Sn
26 ∗∗∗A4
27 ∗∗∗∗An

Chapter 2
Equivalent Representations
Every mathematical theory starts from some notion of equivalence—an agree-
ment not to distinguish between objects that ‘look the same’ in some sense.
Deﬁnition 2.1 Suppose α, β are two representations of G in the vector spaces
U, V over k. We say that α and β are equivalent, and we write α = β, if U and V
are isomorphic G-spaces.
In other words, we can ﬁnd a linear map
t : U →V
which preserves the action of G, ie
t(gu) = g(tu)
for all g ∈G, u ∈U.
Remarks:
1. Suppose α and β are given in matrix form:
α : g 7→A(g),
β : g 7→B(g).
If α = β, then U and V are isomorphic, and so in particular dim α = dim β,
ie the matrices A(g) and B(g) are of the same size.
Suppose the linear map t : U →V is given by the matrix P. Then the
condition t(gu) = g(tu) gives
B(g) = PA(g)P −1
for each g ∈G. This is the condition in matrix terms for two representations
to be equivalent.
424–I
2–1

424–I
2–2
2. Recall that two n × n matrices S, T are said to be similar if there exists a
non-singular (invertible) matrix P such that
T = PSP −1.
A necessary condition for this is that A, B have the same eigenvalues. For
the characteristic equations of two similar matrices are identical:
det

PSP −1 −λI

=
det P det(S −λI) det P −1
=
det(S −λI).
3. In general this condition is necessary but not sufﬁcient. For example, the
matrices
 
1
0
0
1
!
,
 
1
1
0
1
!
have the same eigenvalues 1,1, but are not similar. (No matrix is similar to
the identity matrix I except I itself.)
However, there is one important case, or particular relevance to us, where
the converse is true. Let us recall a result from linear algebra.
An n × n complex matrix A is diagonalisable if and only if it satisﬁes a
separable polynomial equation, ie one without repeated roots.
It is easy to see that if A is diagonalisable then it satisﬁes a separable equa-
tion. For if
A ∼









λ1
...
λ1
λ2
...









then A satisﬁes the separable equation
m(x) ≡(x −λ1)(x −λ2) · · · = 0.
The converse is less obvious. Suppose A satisﬁes the polynomial equation
p(x) ≡(x −λ1) · · · (x −λr) = 0
with λ1, . . . , λr distinct. Consider the expression of 1/p(x) as a sum of
partial fractions:
1
p(x) =
a1
x −λ1
+ · · · +
ar
x −λr
.

424–I
2–3
Multiplying across,
1 = a1Q1(x) + · · · + arQr(x),
where
Qi(x) =
Y
j̸=i
(x −λj) = p(x)
x −λi
.
Substituting x = A,
I = a1Q1(A) + · · · + arQr(A).
Applying each side to the vector v ∈V ,
v
=
a1Q1(A)v + · · · + arQr(A)v
=
v1 + · · · + vr,
say. The vector vi is an eigenvector of A with eigenvalue λi, since
(A −λi)vi = aip(A)v = 0.
Thus every vector is expressible as a sum of eigenvectors. In other words
the eigenvectors of A span the space.
But that is precisely the condition for A to be diagonalisable. For we can
ﬁnd a basis for V consisting of eigenvectors, and with respect to this basis
A will be diagonal.
4. It is important to note that while each matrix A(g) is diagonalisable sepa-
rately, we cannot in general diagonalise all the A(g) simultaneously. That
would imply that the A(g) commuted, which is certainly not the case in
general.
5. However, we can show that if A1, A2, . . . is a set of commuting matrices
then they can be diagonalised simultaneously if and only if they can be
diagonalised separately.
To see this, suppose λ is an eigenvalue of A1. Let
E = {v : A1v = λv}
be the corresponding eigenspace. Then E is stable under all the Ai, since
v ∈E =⇒A1(Aiv) = AiA1v = λAiv =⇒Aiv ∈E.

424–I
2–4
Thus we have reduced the problem to the simultaneous diagonalisation of
the restrictions of A2, A3, . . . to the eigenspaces of A1. A simple inductive
argument on the degree of the Ai yields the result.
In our case, this means that we can diagonalise some (or all) of our repre-
sentation matrices
A(g1), A(g2), . . .
if and only it these matrices commute.
This is perhaps best seen as a result on the representations of abelian groups,
which we shall meet later.
6. To summarise, two representations α, β are certainly not equivalent if A(g), B(g)
have different eigenvalues for some g ∈G.
Suppose to the contrary that A(g), B(g) have the same eigenvalues for all
g ∈G. Then as we have seen
A(g) ∼B(g)
for all g, ie
B(g) = P(g)A(g)P(g)−1
for some invertible matrix P(g).
Remarkably, we shall see that if this is so for all g ∈G, then in fact α and
β are equivalent. In other words, if such a matrix P(g) exists for all g then
we can ﬁnd a matrix P independent of g such that
B(g) = PA(g)P −1
for all g ∈G.
7. Suppose A ∼B, ie
B = PAP −1.
We can interpret this as meaning that A and B represent the same linear
transformation, under the change of basis deﬁned by P.
Thus we can think of two equivalent representations as being, if effect, the
same representation looked at from two points of view, that is, taking two
different bases for the representation-space.
Example: Let us look again at the natural 2-dimensional real representation ρ of
the symmetry group D4 of the square ABCD. Recall that when we took coordi-
nates with respect to axes Ox, Oy bisecting DA, AB, ρ took the matrix form
s 7→
 
0
−1
1
0
!
c 7→
 
0
1
1
0
!
,

424–I
2–5
where s is the rotation through a right-angle (sending A to B), and c is the reﬂec-
tion in AC.
Now suppose we choose instead the axes OA, OB. Then we obtain the equiv-
alent representation
s 7→
 
0
−1
1
0
!
c 7→
 
1
0
0
−1
!
.
We observe that c has the same eigenvalues, ±1, in both cases.
Since we have identiﬁed equivalent representations, it makes sense to ask for
all the representations of a given group G of dimension d, say. What we have to
do in such a case is to give a list of d-dimensional representations, prove that every
d-dimensional representation is equivalent to one of them, and show also that no
two of the representations are equivalent.
It isn’t at all obvious that the number of such representations is ﬁnite, even
after we have identiﬁed equivalent representations. We shall see later that this is
so: a ﬁnite group G has only a ﬁnite number of representations of each dimension.
Example: Let us ﬁnd all the 2-dimensional representations over C of
S3 = ⟨s, t : s3 = t2 = 1, st = ts2⟩,
that is, all 2-dimensional representations up to equivalence.
Suppose α is a representation of S(3) in the 2-dimensional vector space V .
Consider the eigenvectors of s. There are 2 possibilities:
1. s has an eigenvector e with eigenvalue λ ̸= 1. Since s3 = 1, it follows that
λ3 = 1, ie λ = ω or ω2.
Now let f = te. Then
sf = ste = ts2e = λ2te = λ2f.
Thus f is also an eigenvector of s, although now with eigenvector λ2.
Since e and f are eigenvectors corresponding to different eigenvalues, they
must be linearly independent, and therefore span (and in fact form a basis
for) V :
V = ⟨e, f⟩.
Since se = λe, sf = λ2f, we see that s is represented with respect to this
basis by the matrix
s 7→
 
λ
0
0
λ2
!
.

424–I
2–6
On the other hand, te = f, tf = t2e = e, and so
t 7→
 
0
1
1
0
!
.
The 2 cases λ = ω, ω2 give the representations
α :
s 7→
 
ω
0
0
ω2
!
,
t 7→
 
0
1
1
0
!
;
β :
s 7→
 
ω2
0
0
ω
!
,
t 7→
 
0
1
1
0
!
;
In fact these 2 representations are equivalent,
α = β,
since one is got from the other by the swapping the basis elements: e, f 7→
f, e.
2. The alternative possibility is that both eigenvalues of s are equal to 1. In
that case, since s is diagonalisable, it follows that
s 7→I =
 
1
0
0
1
!
with respect to some basis. But then it follows that this remains the case
with respect to every basis: s is always represented by the matrix I.
In particular, s is always diagonal. So if we diagonalise c—as we know we
can—then we will simultaneously diagonalise s and c, and so too all the
elements of D4.
Suppose
s 7→
 
1
0
0
1
!
,
t 7→
 
λ
0
0
µ
!
.
Then it is evident that
s 7→1, t 7→λ
and
s 7→1, t 7→µ
will deﬁne two 1-dimensional representations of S3. But we know these
representations; there are just 2 of them. In combination, these will give 4

424–I
2–7
2-dimensional representations of S3. However, two of these will be equiv-
alent. The 1-dimensional representations 1 and ϵ give the 2-dimensional
representation
s 7→
 
1
0
0
1
!
,
t 7→
 
1
0
0
−1
!
.
(Later we shall denote this representation by 1 + ϵ, and call it the sum of 1
and ϵ.)
On the other hand, ϵ and 1 in the opposite order give the representation
s 7→
 
1
0
0
1
!
,
t 7→
 
−1
0
0
1
!
.
This is equivalent to the previous case, one being taken into the other by the
change of coordinates (x, y) 7→(y, x). (In other words, ϵ + 1 = 1 + ϵ.)
We see from this that we obtain just 3 2-dimensional representations of S3
in this way (in the notation above they will be 1 + 1, 1 + ϵ and ϵ + ϵ).
Adding the single 2-dimensional representation from the ﬁrst case, we con-
clude that S3 has just 4 2-dimensional representations.
It is easy to see that no 2 of these 4 representations are equivalent, by consid-
ering the eigenvalues of s and c in the 4 cases.

424–I
2–8
Exercises
All representations are over C, unless the contrary is stated.
In Exercises 01–15 determine all 2-dimensional representations (up to equiva-
lence) of the given group.
1 ∗∗C2
2 ∗∗C3
3 ∗∗Cn
4 ∗∗∗D2
5 ∗∗∗D4
6 ∗∗∗D5
7 ∗∗∗∗Dn
8 ∗∗∗S3
9 ∗∗∗∗S4
10 ∗∗∗∗∗Sn
11 ∗∗∗∗A4
12 ∗∗∗∗∗An
13 ∗∗∗Q8
14 ∗∗Z
15 ∗∗∗∗D∞
16 ∗∗∗Show that a real matrix A ∈Mat(n, R) is diagonalisable over R if and
only if its minimal polynomial has distinct roots, all of which are real.
17 ∗∗∗Show that a rational matrix A ∈Mat(n, Q) is diagonalisable over Q if and
only if its minimal polynomial has distinct roots, all of which are rational.
18 ∗∗∗∗If 2 real matrices A, B ∈Mat(n, R) are similar over C, are they necessar-
ily similar over R, ie can we ﬁnd a matrix P ∈GL(n, R) such that B = PAP −1?
19 ∗∗∗∗If 2 rational matrices A, B ∈Mat(n, Q) are similar over C, are they
necessarily similar over Q?
20 ∗∗∗∗∗If 2 integral matrices A, B ∈Mat(n, Z) are similar over C, are they
necessarily similar over Z, ie can we ﬁnd an integral matrix P ∈GL(n, Z) with
integral inverse, such that B = PAP −1?
The matrix A ∈Mat(n, k) is said to be semisimple if its minimal polynomial has
distinct roots. It is said to be nilpotent if Ar = 0 for some r > 0.
21 ∗∗∗Show that a matrix A ∈Mat(n, k) cannot be both semisimple and nilpo-
tent, unless A = 0.
22 ∗∗∗Show that a polynomial p(x) has distinct roots if and only if
gcd (p(x), p′(x)) = 1.
23 ∗∗∗∗Show that every matrix A ∈Mat(n, C) is uniquely expressible in the
form
A = S + N,
where S is semisimple, N is nilpotent, and
SN = NS.
(We call S and N the semisimple and nilpotent parts of A.)
24 ∗∗∗∗Show that S and N are expressible as polynomials in A.

424–I
2–9
25 ∗∗∗∗Suppose the matrix B ∈Mat(n, C) commutes with all matrices that
commute with A, ie
AX = XA =⇒BX = XB.
Show that B is expressible as a polynomial in A.

Chapter 3
Simple Representations
Deﬁnition 3.1 The representation α of G in the vector space V over k is said to
be simple if no proper subspace of V is stable under G.
In other words, α is simple if it has the following property: if U is a subspace
of V such that
g ∈G, u ∈U =⇒gu ∈U
then either U = 0 or U = V .
Proposition 3.1
1. A 1-dimensional representation over k is necessarily sim-
ple.
2. If α is a simple representation of G over k then
dim α ≤∥G∥.
Proof ▶(1) is evident since a 1-dimensional space has no proper subspaces, stable
or otherwise.
For (2), suppose α is a simple representation of G in V . Take any v ̸= 0 in V ,
and consider the set of all transforms gv of V . Let U be the subspace spanned by
these:
U = ⟨gv : g ∈G⟩.
Each g ∈G permutes the transforms of v, since
g(hv) = (gh)v.
It follows that g sends U into itself. Thus U is stable under G. Since α is simple,
by hypothesis,
V = U.
424–I
3–1

424–I
3–2
But since U is spanned by the ∥G∥transforms of v,
dim V = dim U ≤∥G∥.
◀
Remark: This result can be greatly improved, as we shall see. If k = C—the case
of greatest interest to us—then we shall prove that
dim α ≤∥G∥
1
2
for any simple representation α.
We may as well announce now the full result. Suppose G is a ﬁnite group.
Then we shall show (in due course) that
1. The number of simple representations of G over C is equal to the number s
of conjugacy classes in G;
2. The dimensions of the simple representations σ1, . . . , σs of G over C satisfy
the relation
dim2 σ1 + · · · + dim2 σs = ∥G∥.
3. The dimension each simple representation σi divides the order of the group:
dim σi | ∥G∥.
Of course we cannot use these results in any proof; and in fact we will not
even use them in examples. But at least they provide a useful check on our work.
Examples:
1. The ﬁrst stage in studying the representation theory of a group G is to de-
termine the simple representations of G.
Let us agree henceforth to adopt the convention that if the scalar ﬁeld k is
not explicitly mentioned, then we may take it that k = C.
We normally start our search for simple representations by listing the 1-
dimensional representations. In this case we know that S3 has just 2 1-
dimensional representations, the trivial representation 1, and the parity rep-
resentation ϵ.
Now suppose that α is a simple representation of S3 of dimension > 1.
Recall that
S3 = ⟨s, t : s3 = t2 = 1, /; st = ts2⟩,
where s = (abc), /; t = (ab).

424–I
3–3
Let e be an eigenvector of s. Thus
se = λe,
where
s3 = 1 =⇒λ3 = 1 =⇒λ = 1, ω, or ω2.
Let
f = te.
Then
sf = ste = ts2e = λ2te = λ2f.
Thus f is also an eigenvector of s, but with eigenvalue λ2.
Now consider the subspace
U = ⟨e, f⟩
spanned by e and f. Then U is stable under s and t, and so under S3. For
se = λe, sf = λ2f, te = f, tf = t2e = e.
It follows, since α is simple, that
V = U.
So we have shown, in particular, that the simple representations of S3 can
only have dimension 1 or 2.
Let us consider the 3 possible values for λ:
(a) λ = ω. In this case the representation takes the matrix form
s 7→
 
ω
0
0
ω2
!
,
t 7→
 
0
1
1
0
!
.
(b) λ = ω2. In this case the representation takes the matrix form
s 7→
 
ω2
0
0
ω
!
,
t 7→
 
0
1
1
0
!
.
But this is the same representation as the ﬁrst, since the coordinate
swap (x, y) 7→(y, x) takes one into the other.

424–I
3–4
(c) λ = 1. In this case
se = e, sf = f =⇒sv = v for all v ∈V .
In other words s acts as the identity on V . It follows that s is repre-
sented by the matrix I with respect to any basis of V .
(More generally, is g ∈G is represented by a scalar multiple ρI of
the identity with respect to one basis, then it is represented by ρI with
respect to every basis; because
P(ρI)P −1 = ρI,
if you like.)
So in this case we can turn to t, leaving s to ‘look after itself’. Let e
be an eigenvector of t. Then the 1-dimensional space
U = ⟨e⟩
is stable under S3, since
se = e, /; te = ±e.
Since α is simple, it follows that V = U, ie V is 1-dimensional, con-
trary to hypothesis.
We conclude that S3 has just 3 simple representations
1, ϵ and α,
of dimensions 1, 1 and 2, given by
1 :
s 7→1, /; t 7→1
ϵ :
s 7→1, /; t 7→−1
α :
s 7→
 
ω
0
0
ω2
!
,
t 7→
 
0
1
1
0
!
.
2. Now let us determine the simple representations (over C) of the quaternion
group
Q8 = ⟨s, t : s4 = 1, s2 = t2, st = ts3⟩,
where s = i, /; t = j. (It is best to forget at this point that one of the
elements of Q8 is called −1, and another i, since otherwise we shall fall
into endless confusion.)

424–I
3–5
We know that Q8 has four 1-dimensional representations, given by
s 7→±1, t 7→±1.
Suppose α is a simple representation of Q8 in V , of dimension > 1. Let e
be an eigenvector of s:
se = λe,
where
s4 = 1 =⇒λ = ±1, ±i.
Let
te = f.
Then
sf = ste = ts3e = λ3te = λ3f.
So as in the previous example, f is also an eigenvector of s, but with eigen-
value λ3.
Again, as in that example, the subspace
U = ⟨e, f⟩
is stable under Q8, since
se = λe, sf = λ3f, te = f, tf = t2e = s2e = λ2e.
So V = U, and {e, f} is a basis for V . With respect to this basis our
representation takes the form
s 7→
 
λ
0
0
λ3
!
,
t 7→
 
0
λ2
1
0
!
,
where λ = ±1, ±i.
If λ = 1 this representation is not simple, since the 1-dimensional subspace
⟨(1, 1)⟩
is stable under Q8. (This is the same argument as before. Every vector is an
eigenvector of s, so we can ﬁnd a simultaneous eigenvector by taking any
eigenvector of t.)
The same argument holds if λ = −1, since s is represented by −I with
respect to one basis, and so also with respect to any basis. Again, the sub-
space
⟨(1, 1)⟩

424–I
3–6
is stable under Q8, contradicting our assumption that the representation is
simple, and of dimension > 1.
We are left with the cases λ = ±i. In fact these are equivalent. For if
λ = −i, then f is an s-eigenvector with eigenvalue λ3 = i. So taking f in
place of e we may assume that λ = i.
We conclude that Q8 has just 5 simple representations, of dimensions 1,1,1,1,2,
given by
1 :
s 7→1, /; t 7→1
µ :
s 7→1, /; t 7→−1
ν :
s 7→−1, /; t 7→1
ρ :
s 7→−1, /; t 7→−1
α :
s 7→
 
i
0
0
−i
!
,
t 7→
 
0
−1
1
0
!
.
We end by considering a very important case: abelian (or commutative) groups.
Proposition 3.2 A simple representation of a ﬁnite abelian group over C is nec-
essarily 1-dimensional.
Proof ▶Suppose a ∈A. Let λ be an eigenvalue of a, and let
E(λ) = {v ∈V : av = λv}.
be the corresponding eigenspace.
Then E(λ) is stable under A. For
b ∈A, v ∈E(λ)
=⇒
a(bv) = (ab)v = (ba)v = b(av) = b(λv) = λ(bv)
=⇒
bv ∈E(λ).
Thus E(λ) is stable under b, and so under A. But since V is simple, by hypothesis,
it follows that
E(λ) = V.
In other words a acts as a scalar multiple of the identity:
a = λI.
It follows that every subspace of V is stable under a. Since that is true for each
a ∈A, we conclude that every subspace of V is stable under A. Therefore, since
α is simple, V has no proper subspaces. But that is only true if dim V = 1.
◀

424–I
3–7
Example: Consider the group
D2 = {1, a, b, c : a2 = b2 = c2 = 1, bc = cb = a, ca = ac = b, ab = ca = c}.
This has just four 1-dimensional representations, as shown in the following table.
1
a
b
c
1
1
1
1
1
µ
1
1
−1
−1
ν
1
−1
1
−1
ρ
1
−1
−1
1

Chapter 4
The Arithmetic of Representations
4.1
Addition
Representations can be added and multiplied, like numbers; and the usual
laws of arithmetic hold. There is even a conjugacy operation, analogous to
complex conjugation.
Deﬁnition 4.1 Suppose α, β are representations of G in the vector spaces U, V
over k. Then α + β is the representation of G in U L V deﬁned by the action
g(u ⊕v) = gu ⊕gv.
Remarks:
1. Recall that U L V is the cartesian product of U and V , where however we
write u ⊕v rather than (u, v). The structure of a vector space is deﬁned on
this set in the natural way.
2. Note that α + β is only deﬁned when α, β are representations of the same
group G over the same scalar ﬁeld k.
3. Suppose α, β are given in matrix form
α : g 7→A(g),
β : g 7→B(g).
Then α + β is the representation
g 7→
 
A(g)
0
0
B(g)
!
424–I
4–1

4.2. MULTIPLICATION
424–I
4–2
Example: Let us look again at the 2-dimensional representations γ1, γ2, γ3 of S3
over C deﬁned in Chapter 2
γ1 : s 7→
 
ω
0
0
ω2
!
, t 7→
 
1
0
0
1
!
,
γ2 : s 7→
 
ω
0
0
ω2
!
, t 7→
 
1
0
0
−1
!
,
γ3 : s 7→
 
ω
0
0
ω2
!
, t 7→
 
−1
0
0
−1
!
.
We see now that
γ1 = 1 + 1, γ2 = 1 + ϵ, γ3 = ϵ + ϵ,
where 1 is the trivial 1-dimensional representation of S3, and ϵ is the 1-dimensional
parity representation
s 7→1,
t 7→−1.
(We can safely write 1 + 1 = 2, ϵ + ϵ = 2ϵ.)
Proposition 4.1
1. dim(α + β) = dim α + dim β;
2. β + α = α + β;
3. α + (β + γ) = (α + β) + γ.
Proof ▶These are all immediate. For example, the second part follows from the
natural isomorphism
V
M
U →U
M
V : v ⊕u 7→u ⊕v.
◀
4.2
Multiplication
Deﬁnition 4.2 Suppose α, β are representations of G in the vector spaces U, V
over k. Then αβ is the representation of G in U N V deﬁned by the action
g(u1 ⊗v1 + · · · + ur ⊗vr) = gu1 ⊗gv1 + · · · gur ⊗gvr.
Remarks:

4.2. MULTIPLICATION
424–I
4–3
1. The tensor product U N V of 2 vector spaces U and V may be unfamiliar.
Each element of U
N V is expressible as a ﬁnite sum
u1 ⊗v1 + · · · + ur ⊗vr.
If U has basis {e1, . . . , em} and V has basis {f1, . . . , fn} then the mn ele-
ments
ui ⊗vj
(i = 1, . . . , m; j = 1, . . . , n)
form a basis for U N V . In particular
dim(U ⊗V ) = dim U dim V.
(It is a common mistake to suppose that every element of U N V is express-
ible in the form u ⊗v. That is not so; the general element requires a ﬁnite
sum.)
Formally, the tensor product is deﬁned as the set of formal sums
u1 ⊗v1 + · · · + ur ⊗vr,
where 2 sums deﬁne the same element if one can be derived from the other
by applying the rules
(u1+u2)⊗v⊗u1⊗v+u2⊗v, u⊗(v1+v2)⊗u⊗v1+u⊗v2, (ρu)⊗v = u⊗(ρv).
The structure of a vector space is deﬁned on this set in the natural way.
2. As with α+β, αβ is only deﬁned when α, β are representations of the same
group G over the same scalar ﬁeld k.
3. It is important not to write α × β for αβ, as we shall attach a different
meaning to α × β later.
4. Suppose α, β are given in matrix form
α : g 7→A(g),
β : g 7→B(g).
Then αβ is the representation
αβ : g 7→A(g) ⊗B(g).
But what do we mean by the tensor product S ⊗T of 2 square matrices
S, T? If S = sij is an m × m-matrix, and T = tkl is an n × n-matrix, then

4.2. MULTIPLICATION
424–I
4–4
S ⊗T is the mn × mn-matrix whose rows and columns are indexed by the
pairs (i, k) where 1 ≤i ≤m, 1 ≤k ≤n, with matrix entries
(S ⊗T)(i,k)(j,l) = SijTkl.
To write out this matrix S ⊗T we must order the index-pairs. Let us settle
for the ‘lexicographic order’
(1, 1), (1, 2), . . . , (1, n), (2, 1), . . . , (2, n), . . . , (m, 1), . . . , (m, n).
(In fact the ordering does not matter for our purposes. For if we choose a
different ordering of the rows, then we shall have to make the same change
in the ordering of the columns; and this double change simply corresponds
to a change of basis in the underlying vector space, leading to a similar
matrix to S ⊗T.)
Example: Consider the 2-dimensional representation α of S3 over C
α : s 7→
 
ω
0
0
ω2
!
,
t 7→
 
0
1
1
0
!
We shall determine the 4-dimensional representation α2 = αα. (The notation α2
causes no problems.) We have
α2 : s 7→
 
ω
0
0
ω2
! O  
ω
0
0
ω2
!
,
t 7→
 
0
1
1
0
! O  
0
1
1
0
!
It is simply (!) a matter of working out these 2 tensor products. In fact
 
ω
0
0
ω2
! O  
ω
0
0
ω2
!
=





ω · ω
ω · 0
0 · ω
0 · 0
ω · 0
ω · ω2
0 · 0
0 · ω2
0 · ω
0 · 0
ω2 · ω
ω2 · 0
0 · 0
0 · ω2
ω2 · 0
ω2 · ω2





=





ω2
0
0
0
0
1
0
0
0
0
1
0
0
0
0
ω




,
while
 
0
1
1
0
! O  
0
1
1
0
!
=





0
0
0
1
0
0
1
0
0
1
0
0
1
0
0
0




,

4.2. MULTIPLICATION
424–I
4–5
We can simplify this by the change of coordinates (x, y, z, t) 7→(y, z, t, x). This
will give the equivalent representation (which we may still denote by αβ):
αβ : s 7→





1
0
0
0
0
1
0
0
0
0
ω
0
0
0
0
ω2




,
t 7→





0
1
0
0
1
0
0
0
0
0
0
1
0
0
1
0





But now we see that this splits into 2 2-dimensional representations, the second of
which is α itself:
α2 = β + α,
where β is the representation
β : s 7→
 
1
0
0
1
!
,
t 7→
 
0
1
1
0
!
The representation β can be split further. That is evident if we note that since
s is represented by I, we can diagonalise t without affecting s. Since t has eigen-
values ±1, this must yield the representation
β : s 7→
 
1
0
0
1
!
,
t 7→
 
1
0
0
−1
!
Concretely, the change of coordinates (x, y) 7→(x + y, x −y) brings this about.)
Thus
β = 1 + ϵ,
and so
α2 = 1 + ϵ + α.
(We hasten to add that this kind of matrix manipulation is not an essential part
of representation theory! We shall rapidly develop techniques which will enable
us to dispense with matrices altogether.)
Proposition 4.2
1. dim(αβ) = dim α dim β;
2. βα = αβ;
3. α(βγ) = (αβ)γ;
4. α(β + γ) = αβ + αγ;
5. 1α = α.

4.3. CONJUGACY
424–I
4–6
All these results, again, are immediate consequences of ‘canonical isomor-
phisms’ which it would be tedious to explicate.
We have seen that the representations of G over k can be added and multiplied.
They almost form a ring—only subtraction is missing. In fact if we introduce
‘virtual representations’ α−β (where α, β are representations) then we will indeed
obtain a ring
R(G) = R(G, k),
the representation-ring if G over k. (By convention if k is omitted then we assume
that k = C.)
We shall see later that
α + β = α + γ =⇒β = γ.
It follows that nothing is lost in passing from representations to R(G); if α = β
in R(G) then α = β in ‘real life’.
4.3
Conjugacy
Deﬁnition 4.3 Suppose α = is a representation of G in the vector space V over
k. Then α∗is the representation of G in the dual vector space U ∗deﬁned by the
action
(gπ)(v) = π(g−1v)
(g ∈G, π ∈V ∗, v ∈V )
Remarks:
1. Recall that the dual vector space V ∗is the space of linear functionals
π : V →k.
To any basis {e1, . . . , en} of V there corresponds a dual basis {π1, . . . , πn}
of V ∗, where
πj(ei) =
(
1
if i = j
0
otherwise
2. Suppose α is given in matrix form
α : g 7→A(g).
Then α∗is the representation
g 7→

A(g)−1′ ,

4.3. CONJUGACY
424–I
4–7
where T ′ denotes the transpose of T. Notice the mysterious way in which
the inverse and transpose, each of which is ‘contravariant’, ie
(RS)−1 = S−1T −1,
(RS)′ = S′R′,
combine to give the required property

(RS)−1′ = (R−1)′(S−1)′.
Example: Consider α∗, where α is the 2-dimensional representation of S3 over C
considered above. By the rule above, α∗is given by
α∗: s 7→
 
ω2
0
0
ω
!
,
t 7→
 
0
1
1
0
!
.
It is easy to see that swapping the coordinates, (x, y) 7→(y, x), gives
α∗= α
Many of the representations we shall meet will share this property of self-conjugacy.
Proposition 4.3
1. dim(α∗) = dim α;
2. (α∗)∗= α;
3. (α + β)∗= α∗+ β∗.
4. (αβ)∗= α∗β∗.
5. 1∗= 1.
Summary: We have deﬁned the representation ring R(G) of a group G, and
shown that it carries a conjugacy operation α 7→α∗.

Chapter 5
Semisimple Representations
Deﬁnition 5.1 The represenation α of G is said to be semisimple if it is express-
ible as a sum of simple representations:
α = σ1 + · · · + σr.
Example: Consider the permutation representation ρ of S3 in k3. (It doesn’t matter
for the following argument if k = R or C.)
Recall that
g(x1, x2, x3) = (xg−11, xg−12, xg−13).
We have seen that k3 has 2 proper stable subspaces:
U = {(x, x, x) : x ∈k},
W = {(x1, x2, x3) : x1 + x2 + x3 = 0}.
U has dimension 1, with basis {(1, 1, 1)}; W has dimension 2, with basis {(1, −1, 0), (−1, 0, 1)}.
Evidently
U ∩V = 0.
Recall that a sum U + V of vector subspaces is direct,
U + V = U ⊕V,
if (and only if) U ∩V = 0. So it follows here, by considering dimensions, that
k3 = U
M
W.
The representation on U is the trivial representation 1. Thus
ρ = 1 + α,
where α is the representation of S3 in W.
424–I
5–1

424–I
5–2
We can see that α is simple as follows. Suppose V ⊂W is stable under S3,
where V ̸= 0. Take any element v ̸= 0 in V : say
v = (x, y, z)
(x + y + z = 0).
The coefﬁcients x, y, z cannot all be equal. Suppose x ̸= y. Then
(12)v = (y, x, z) ∈V ;
and so
v −(12)v = (x −y, y −x, 0) = (x −y)(1, −1, 0) ∈V.
Hence
(1, −1, 0) ∈V.
It follows that
(−1, 0, 1) = (132)(1, −1, 0) ∈V
also. But these 2 elements generate W; hence
V = W.
So we have shown that W is a simple S3-space, whence the corresponding repre-
sentation α is simple.
We conclude that the representation
ρ = 1 + α
is a sum of simple representations, and so is semisimple.
It is easy to see that U and W are the only subspaces of k3 stable under S3,
apart from 0 and the whole space. So it is evident that the splitting U ⊕V is
unique. In general this is not so; in fact we shall show later that there is a unique
split into simple subspaces if and only if the representations corresponding to
these subspaces are distinct. (So in this case the split is unique because 1 ̸= α.)
However the simple representations that appear are unique. This fact, which we
shall prove in the next chapter, is the foundation stone of representation theory.
Most of the time we do not need to look behind a representation at the un-
derlying representation-space. But sometimes we do; and the following results
should help to clarify the structure of semisimple representation-spaces.
Proposition 5.1 Suppose V is a sum (not necessarily direct) of simple subspaces:
V = S1 + · · · + Sr.
Then V is semisimple.

424–I
5–3
Proof ▶Since S2 is simple,
S1 ∩S2 = 0 or S2.
In the former case
S1 + S2 = S1
M
S2;
in the latter case S2 ⊂S1 and so
S1 + S2 = S1.
Repeating the argument with S1 + S2 in place of S1, and S3 in place of S2,
(S1 + S2) ∩S3 = 0 or S3,
since S3 is simple. In the former case
S1 + S2 + S3 = (S1 + S2)
M
S3;
in the latter case S3 ⊂S1 + S2 and so
S1 + S2 + S3 = S1 + S2.
Combining this with the previous step
S1 + S2 + S3 = S1
M
S2
M
S3 or S1
M
S3 or S1
M
S2 or S1.
Continuing in this style, at the ith step, since Si is simple,
S1 + · · · + Si = (S1 + · · · + Si−1)
M
Si or S1 + · · · + Si−1.
We conclude, ﬁnally, that
V = S1 + · · · + Sr = Si1
M
· · · Sis,
where {Si1, . . . , Sis} is a subset of {S1, . . . , Sr}.
◀
Remark: The subset {Si1, . . . , Sis} depends in general on the order in which we
take S1, . . . , Sr. In particular, since Si1 = S1, we can always specify that any one
of S1, . . . , Sn appears in the direct sum.
Proposition 5.2 The following 2 properties of the G-space V are equivalent:
1. V is semisimple;

424–I
5–4
2. each stable subspace U ⊂V has at least one complementary stable sub-
space W, ie
V = U
M
W.
Proof ▶Suppose ﬁrst that V is semisimple, say
V = S1
M
· · ·
M
Sr.
Let us follow the proof of the preceding proposition, but starting with U rather
than S1. Thus our ﬁrst step is to note that since S1 is simple,
U + S1 = U
M
S1 or U.
Continuing as before, we conclude that
V = U
M
Si1
M
· · ·
M
Sis,
from which the result follows, with
W = Si1
M
· · ·
M
Sis.
Now suppose that condition (2) holds. Since V is ﬁnite-dimensional, we can
ﬁnd a stable subspace S1 of minimal dimension. Evidently S1 is simple; and by
our hypothesis
V = S1
M
W1.
Now let us ﬁnd a stable subspace S2 of W1 of minimal dimension. As before,
this subspace is simple; and
S1 ∩S2 ⊂S1 ∩W = 0,
so that
S1 + S2 = S1
M
S2.
Applying the hypothesis again to this space, we can ﬁnd a stable complement
W2:
V = S1
M
S2
M
W2.
Continuing in this way, since V is ﬁnite-dimensional we must conclude with
an expression for V as a direct sum of simple subspaces:
V = S1
M
· · ·
M
Sr.
Hence V is semisimple.
◀
Remark: This Proposition gives an alternative deﬁnition of semisimplicity: V
is semisimple if every stable subspace U ⊂V posseses a complementary sta-
ble subspace W. This alternative deﬁnition allows us to extend the concept of
semisimplicity to inﬁnite-dimensional representations.

424–I
5–5
Exercises
In Exercises 01–15 calculate eX for the given matrix X:
1. Show that any commuting set of diagonalisable matrices can be simultane-
ously diagonalised. Hence show that any representation of a ﬁnite abelian
group
2. Show that for all n the natural representation ρ of Sn in kn is semisimple.
3. If T ∈GL(n, k) then the map
Z →GL(n, k) : m 7→T m
deﬁnes a representation τ of the inﬁnite abelian group Z.
Show that if k = C then τ is semisimple if and only if T is semisimple.
4. Prove the same result when k = R.
5. Suppose k = GF(2) = {0, 1}, the ﬁnite ﬁeld with 2 elements. Show that
the representation of C2 = {e, g} given by
g 7→
 
1
1
0
1
!
is not semisimple.

Chapter 6
Every Representation of a Finite
Group is Semisimple
Theorem 6.1 (Maschke’s Theorem) Suppose α is a representation of the ﬁnite
group G over k, where k = R or C. Then α is semisimple.
Proof ▶Suppose α is a representation on V . We take the alternative deﬁnition of
semisimplicity: every stable subspace U ⊂V must have a stable complement W.
Our idea is to construct an invariant positive-deﬁnite form P on V . (By ‘form’
we mean here quadratic form if k = R, or hermitian form if k = C.) Then we can
take W to be the orthogonal complement of U with respect to this form:
W = U ⊥= {v ∈V : P(u, v) = 0 for all u ∈U}.
We can construct such a form by taking any positive-deﬁnite form Q, and
averaging it over the group:
P(u, v) =
1
∥G∥
X
g∈G
Q(gu, gv).
(It’s not really necessary to divide by the order of the group; we do it because the
idea of ‘averaging over the group’ occurs in other contexts.)
It is easy to see that the resulting form is invariant:
P(gu, gv)
=
1
∥G∥
X
h∈G
Q(hgu, hgv)
=
1
∥G∥
X
h∈G
Q(hu, hv)
=
P(u, v)
424–I
6–1

424–I
6–2
since hg runs over the group as h does so.
It is a straightforward matter to verify that if P is invariant and U is stable then
so is U ⊥. Writing ⟨u, v⟩for P(u, v),
g ∈G, w ∈U ⊥
=⇒
⟨u, w⟩= 0 ∀u ∈U
=⇒
⟨gu, gw⟩= ⟨u, w⟩= 0 ∀u ∈U
=⇒
⟨u, gw⟩= ⟨g(g−1u), w⟩= 0 ∀u ∈U
=⇒
gw ∈U ⊥.
◀
Examples:
1. Consider the representation of S3 in R3.
There is an obvious invariant
quadratic form—as is often the case—namely
x2
1 + x2
2 + x2
3.
But as an exercise in averaging, let us take the positive-deﬁnite form
Q(x1, x2, x3) = 2x2
1 −2x1x2 + 3x2
2 + x2
3.
Then
Q (e(x1, x2, x3))
=
Q(x1, x2, x3) = 2x2
1 −2x1x2 + 3x2
2 + x2
3
Q ((23)(x1, x2, x3))
=
Q(x1, x3, x2) = 2x2
1 −2x1x3 + 3x2
3 + x2
2
Q ((13)(x1, x2, x3))
=
Q(x3, x2, x1) = 2x2
3 −2x3x2 + 3x2
2 + x2
1
Q ((12)(x1, x2, x3))
=
Q(x2, x1, x3) = 2x2
2 −2x2x1 + 3x2
1 + x2
3
Q ((123)(x1, x2, x3))
=
Q(x3, x1, x2) = 2x2
3 −2x3x1 + 3x2
1 + x2
2
Q ((132)(x1, x2, x3))
=
Q(x2, x3, x1) = 2x2
2 −2x2x3 + 3x2
3 + x2
1
Adding, and dividing by 6,
P(x1, x2, x3)
=
2

x2
1 + x2
2 + x2
3

−2
3 (x2x3 + x1x3 + x1x2)
=
7
3

x2
1 + x2
2 + x2
3

−1
3 (x1 + x2 + x3)2 .
The corresponding inner product is given by
⟨(x1, x2, x3), (y1, y2, y3)⟩= 2(x1y1+x2y2+x3y3)−1
3(x2y3+x3y2+x3y1+x1y3+x1y2+x2y1)

424–I
6–3
To see how this is used, let
U = {(x, x, x) : x ∈R}.
Evidently U is stable. Its orthogonal complement with respect to the form
above is
U ⊥
=
{(x1, x2, x3) : ⟨(1, 1, 1), (x1, x2, x3)⟩= 0}
=
{(x1, x2, x3) : 4
3(x1 + x2 + x3) = 0}
=
{(x1, x2, x3) : x1 + x2 + x3 = 0},
which is just the complement we found before. This is not surprising since—
as we observed earlier—U and U ⊥are the only proper stable subspaces of
R3.
2. For an example using hermitian forms, consider the simple representation
of D4 over C deﬁned by
s 7→
 
i
0
0
−i
!
,
t 7→
 
0
1
1
0
!
.
Again, there is an obvious invariant hermitian form, namely
|x2
1 + x2
2| = x1x1 + x2x2.
But this will not give us much exercise.
The general hermitian form on C2 is
a¯xx + b¯yy + c¯xy + ¯c¯yx
(a, b ∈R, c ∈C)
Let us take
Q(x, y) = 2¯xx + ¯yy −i¯xy + i¯yx.
Note that
D4 = {e, s, s2, s3, t, ts, ts2, ts3}.
For these 8 elements are certainly distinct, eg
s2 = ts3 =⇒ts = 1 =⇒s = t.

424–I
6–4
Now
Q (e(x, y))
=
Q(x, y) = 2¯xx + ¯yy −i¯xy + i¯yx,
Q (s(x, y))
=
Q(ix, −iy) = 2¯xx + ¯yy + i¯xy −i¯yx,
Q

s2(x, y)

=
Q(−x, −y) = 2¯xx + ¯yy −i¯xy + i¯yx,
Q

s3(x, y)

=
Q(−ix, iy) = 2¯xx + ¯yy + i¯xy −i¯yx,
Q (t(x, y))
=
Q(y, x) = ¯xx + 2¯yy + i¯xy −i¯yx,
Q (ts(x, y))
=
Q(iy, −ix) = ¯xx + 2¯yy −i¯xy + i¯yx,
Q

ts2(x, y)

=
Q(−y, −x) = ¯xx + 2¯yy + i¯xy −i¯yx,
Q

ts3(x, y)

=
Q(−iy, ix) = ¯xx + 2¯yy −i¯xy + i¯yx.
Averaging,
P(x, y)
=
1
8
X
g ∈D4Q(g(x, y))
=
3
2(¯xx + ¯yy)
It is no coincidence that we have ended up with a scalar multiple of |x|2 +
|y|2. For it is easy to see that a simple G-space carries a unique invariant
hermitian form, up to a scalar multiple. Suppose P, Q were 2 such forms.
Let λ be an eigenvalue of Q with respect to P, ie a solution of
det(A −λB) = 0,
where A, B are the matrices of P, Q. Then the corresponding eigenspace
E = {v : Av = λBv}
would be stable under G.
The alternative proof of Maschke’s Theory below may be preferred by the
algebraically-minded. It has the advantage of extending to scalar ﬁelds other than
R and C. Against that, it lacks the intuitive appeal of the earlier proof.
Alternative proof ▶Recall that a projection p : V →V is a linear map satisfying
the relation
p2 = p
(ie p is idempotent).
If p is a projection then so is 1 −p:
(1 −p)2 = 1 −2p + p2 = 1 −2p + p = 1 −p.

424–I
6–5
The projections (p, 1 −p) deﬁne a splitting of V into a direct sum
V = im p
M
im(1 −p).
Note that
v ∈im p ⇐⇒pv = v.
Note also that
im(1 −p) = ker p,
since
v = (1 −p)w =⇒pv = (p −p2)w = 0,
while
pv = 0 =⇒v = (1 −p)v.
Thus the splitting can equally well be written
V = im p
M
ker p.
Conversely, every splitting
V = U
M
W
arises from a projection p in this way: if
v = u + w
(u ∈U, w ∈W)
then we set
pv = u.
(Although the projection p is often referred to as ‘the projection onto U’ it
depends on W as well as U. In general there are an inﬁnity of projections onto U,
corresponding to the inﬁnity of complements W. When there is a positive-deﬁnite
form on V —quadratic or hermitian, according as k = R or C—then one of these
projections is distinguished: namely the ‘orthogonal projection’ corresponding to
the splitting
V = U
M
U ⊥.
But we are not assuming the existence of such a form at the moment.)
Now suppose U is a stable subspace of V . Choose any complementary sub-
space W:
V = U
M
W.
In general W will not be stable under G. Our task is to ﬁnd a stable complemen-
tary subspace W0:
V = U
M
W = U
M
W0.

424–I
6–6
Let p be the projection onto U with complement W. We know that U is stable
under G, ie
g ∈G, u ∈U =⇒gu ∈U.
Thus
g ∈G, u = pv =⇒pgu = gu =⇒pgpv = gpv.
Since this holds for all v ∈V ,
pgp = gp
for all g ∈G. Conversely, if this is so then U = im p is stable.
By the same argument, W = im(1 −p) will be stable if and only if
(1 −p)g(1 −p) = g(1 −p)
for all g ∈G. This reduces to
pgp = pg.
Both U and W are stable if and only if
gp = pg.
For in that case
pgp = p(gp) = p(pg) = p2g = pg = gp.
Now
gp = pg ⇐⇒g−1pg = p.
In other words, p deﬁnes a splitting into stable subspaces if and only if it is invari-
ant under G.
In general, we can construct an invariant element by averaging over G. Let us
therefore set
P =
1
∥G∥
X
g∈G
g−1pg.
This will certainly be invariant under G:
g−1Pg
=
1
∥G∥
X
h∈G
g−1h−1phg
=
1
∥G∥
X
h∈G
(hg)−1p(hg)−1
=
1
∥G∥
X
h∈G
h−1ph−1
=
P,
since hg runs over G as h does so.

424–I
6–7
What is less obvious is that P is a projection, and in fact a projection onto U.
To see that, note that
u ∈U =⇒gu ∈U =⇒p(gu) = gu.
Hence by addition
u ∈U =⇒Pu = u.
Conversely,
v ∈V =⇒pgv ∈U =⇒gpgv ∈U.
So by addition
v ∈V =⇒Pv ∈U.
These 2 results imply that P 2 = P, and that P projects onto U.
◀
Remarks:
1. We can show directly that P is a projection, as follows:
P 2
=
1
∥G∥2
X
g,h
g−1pgh−1ph
=
1
∥G∥2
X
g,h
g−1gh−1ph
=
1
∥G∥2
X
g,h
h−1ph
=
1
∥G∥
X
h
h−1ph
=
P.
Two projections p, q project onto the same (ﬁrst) subspace if
qp = p, pq = q.
So to prove that P projects onto the same subspace U as p, we must show
that Pp = p and pP = P. These follow in much the same way:
Pp
=
1
∥G∥
X
g
g−1pgp
=
1
∥G∥
X
g
g−1gp
=
p,

424–I
6–8
pP
=
1
∥G∥
X
g
pg−1pg
=
1
∥G∥
X
g
g−1pg
=
P.
2. Both proofs of Maschke’s Theorem rely on the same idea: obtaining an
invariant element (in the ﬁrst proof, an invariant form; in the second, and
invariant projection) by averaging over transforms of a non-invariant ele-
ment.
In general, if V is a G-space (in other words, we have a representation of G
in V ) then the invariant elements form a subspace
V G = {v ∈V : gv = v∀g ∈G}.
The averaging operation deﬁnes a projection of V onto V G:
v 7→
1
∥G∥
X
g
gv.
Clearly V G is a stable subspace of V . Thus if V is simple, either V G = 0
or V G = V . In the ﬁrst case, all averages vanish. In the second case, the
representation in V is trivial, and so V must be 1-dimensional.
3. It is worth noting that our alternative proof works in any scalar ﬁeld k,
provided ∥G∦= 0 in k. Thus it even works over the ﬁnite ﬁeld GF(pn),
unless p | ∥G∥.
Of course we are not considering such modular representations (as rep-
resentations over ﬁnite ﬁelds are known); but our argument shows that
semisimplicity still holds unless the characteristic p if the scalar ﬁeld di-
vides the order of the group.

Chapter 7
Uniqueness and the Intertwining
Number
Deﬁnition 7.1 Suppose α, β are representations of G over k in the vector spaces
U, V respectively. The intertwining number I(α, β) is deﬁned to be the dimension
of the space of G-maps t : U →V ,
I(α, β) = dim homG(U, V ).
Remarks:
1. A G-map t : U →V is a linear map which preserves the action of G:
t(gu) = g(tu)
(g ∈G, u ∈G).
These G-maps evidently form a vector space over k.
2. The intertwining number will remain somewhat abstract until we give a
formula for it (in terms of characters) in Chapter . But intuitively I(α, β)
measures how much the representations α, β have in common.
3. The intertwining number of ﬁnite-dimensional representations is certainly
ﬁnite, as the following result shows.
Proposition 7.1 We have
I(α, β) ≤dim α dim β.
Proof ▶The space hom(U, V ) of all linear maps t : U →V has dimension
dim U dim V , since we can represent each such map by an m × n-matrix, where
m = dim U, n = dim V .
424–I
7–1

424–I
7–2
The result follows, since
homG(U, V ) ⊂hom(U, V ).
◀
Proposition 7.2 Suppose α, β are simple representations over k. Then
I(α, β) =
(
0
if α ̸= β,
≥1
if α = β.
Proof ▶Suppose α, β are representations in U, V , respectively; and suppose
t : U →V
is a G-map. Then the subspaces
ker t = {u ∈U : tu = 0} and im t = {v ∈V : ∃u ∈U, tu = v}
are both stable under G. Thus
u ∈ker t
=⇒
tu = 0
=⇒
t(gu) = g(tu) = 0
=⇒
gu ∈ker t,
while
v ∈im t
=⇒
v = tu
=⇒
t(gu) = g(tu) = gv
=⇒
gv ∈im t.
But since U and V are both simple, by hypothesis, it follows that
ker t = 0 or U,
im t = 0 or V.
Now ker t = U =⇒t = 0, and im t = 0 =⇒t = 0. So if t ̸= 0,
ker t = 0,
im t = V.
But in this case t is an isomorphism of G-spaces, and so α = β.
On the other hand, if α = β then (by the deﬁnition of equivalent representa-
tions) there exists a G-isomorphis t : U →V , and so I(α, β) ≥1.
◀
When k = C we can be more precise.

424–I
7–3
Proposition 7.3 If α is a simple representation over C then
I(α, α) = 1.
Proof ▶Suppose V carries the representation α. We have to show that
dim homG(V, V ) = 1.
Since the identity map 1 : V →V is certainly a G-map, we have to show that
every G-map t : V →V is a scalar multiple ρ1 of the identity.
Let λ be an eigenvector of t. Then the corresponding eigenspace
E = E(λ) = {v ∈V : tv = λv}
is stable under G. For
g ∈G, v ∈E =⇒t(gv) = g(tv) = λgv =⇒gv ∈E.
Since α is simple, this implies that E = V , ie
t = λ1.
◀
Proposition 7.4 Suppose α, β, γ are representations over k. Then
1. I(α + β, γ) = I(α, γ) + I(β, γ);
2. I(α, β + γ) = I(α, β) + I(α, γ);
3. I(αβ, γ) = I(α, β∗γ).
Proof ▶Suppose α, β, γ are representations in U, V, W respectively. The ﬁrst 2
results are immediate, arising from the more-or-less self-evident isomorphisms
hom(U
M
V, W)
∼=
hom(U, W)
M
hom(V, W)
hom(U, V
M
W)
∼=
hom(U, V )
M
hom(U, W).
Take the ﬁrst. This expresses the fact that a linear map
t : U
M
V →W
can be deﬁned by giving 2 linear maps
t1 : U →W, t2 : V →W.

424–I
7–4
In fact t1 is the restriction of t to U ⊂U L V , and t2 the restriction of t to
V ⊂U
L V ; and
t(u ⊕v) = t1u ⊕t2v.
In much the same way, the second result expresses the fact that a linear map
t : U →V
M
W
can be deﬁned by giving 2 linear maps
t1 : U →V, t2 : U →W.
In fact
t1 = π1t,
t2 = π2t,
where π1, π2 are the projections of U L V onto V, W respectively; and
tu = t1u ⊕t2u.
The third result, although following from a similar ‘natural equivalence’
hom(U
O
V, W) ∼= hom(U, V ∗O
W),
where
V ∗= hom(V, k),
is rather more difﬁcult to establish.
We can divide the task in two. First, there is a natural equivalence
hom(U, hom(V, W)) ∼= hom(U
O
V, W).
For this, note that there is a 1–1 correspondence between linear maps b : U N V →
W and bilinear maps
B : U × V →W.
(This is sometimes taken as the deﬁnition of U N V .) So we have to show how
such a bilinear map B(u, v) gives rise to a linear map
t : U →hom(V, W).
But that is evident:
t(u)(v) = B(u, v).
It is a straightforward matter to verify that every such linear map t arises in this
way from a unique bilinear map B.

424–I
7–5
It remains to show that
hom(V, W) ∼= V ∗O
W.
For this, note ﬁrst that both sides are ‘additive functors’ in W, ie
hom(V, W1
M
W2)
=
hom(V, W1)
M
hom(V, W2),
V ∗O
(W1
M
W2)
=
(V ∗O
W1)
M
(V ∗O
W2).
This allows us to reduce the problem, by expressing W as a sum of 1-dimensional
subspaces, to the case where W is 1-dimensional. In that case, we may take
W = k; so the result to be proved is
hom(V, k) ∼= V ∗O
k.
But there is a natural isomorphism
U
O
k ∼= U
for every vector space U. So our result reduces to the tautology V ∗∼= V ∗.
It’s a straightforward (if tedious) matter to verify that these isomorphisms are
all compatible with the actions of the group G. In particular the G-invariant ele-
ments on each side correspond:
homG(U
M
V, W)
∼=
homG(U, W)
M
homG(V, W),
homG(U, V
M
W)
∼=
homG(U, V )
M
homG(U, W),
homG(U
O
V, W)
∼=
homG(U, V ∗O
W).
The 3 results follow on taking the dimensions of each side.
◀
Theorem 7.1 The expression for a semisimple representation α as a sum of sim-
ple parts
α = σ1 + · · · + σr
is unique up to order.
Proof ▶Suppose σ is a simple representation of G over k. We can use the inter-
twining number to compute the number of times, m say, that σ occurs amongst
the σi. For
I(σ, α)
=
I(σ, σ1) + · · · + I(σ, σr)
=
mI(σ, σ),

424–I
7–6
since only those summands for which σi = σ will contribute to the sum. Thus
m = I(σ, α)
I(σ, σ).
It follows that σ will occur the same number m times in every expression for α
as a sum of simple parts. Hence two such expressions can only differ in the order
of their summands.
◀
Although the expression
α = σ1 + · · · + σr
for the representation α is unique, the corresponding splitting
V = U1
M
· · ·
M
Ur
of the representation-space is not in general unique. It’s perfectly possible for 2
different expressions for V as a direct sum of simple G-subspaces to give rise to
the same expression for α: say
V = U1
M
· · ·
M
Ur,
V = W1
M
· · ·
M
Wr
where Ui and Wi both carry the representation σi.
For example, consider the trivial representation α = 1 + 1 of a group G in the
2-dimensional space V = k2. Every subspace of V is stable under G; so if we
choose any 2 different 1-dimensional subspaces U, W ⊂V , we will have
V = U
M
W.
However, the splitting of V into isotypic components is unique, as we shall
see.
Deﬁnition 7.2 The representation α, and the underlying representation-space V ,
are said to be isotypic of type σ, where σ is a simple representation, if
α = eσ = σ + · · · + σ.
In other words, σ is the only simple representation appearing in α.
Proposition 7.5 Suppose V is a G-space.
1. If V is isotypic of type σ then so is every G-subspace U ⊂V .
2. If U, W ⊂V are isotypic of type σ then so is U + W.

424–I
7–7
Proof ▶These results follow easily from the Uniqueness Theorem. But it is useful
to give an independent proof, since we can use this to construct an alternative
proof of the Uniqueness Theorem.
Lemma 7.1 Suppose
V = U1 + · · · + Ur
is an expression for the G-space V as a sum of simple spaces; and suppose the
subspace U ⊂V is also simple. Then U is isomorphic (as a G-space) to one of
the summands:
U ∼= Ui
for some i.
Proof of Lemma ▷We know that
V = Ui1
M
· · ·
M
Uit
for some subset {Ui1, . . . , Uit} ⊂{U1, . . . , Ur}. Thus we may assume that the
sum is direct:
V = U1
M
· · ·
M
Ur.
For each i, consider the composition
U →V →Ui,
where the second map is the projection of V onto its component Ui. Since U and
Ui are both simple, this map is either an isomorphism, or else 0.
But it cannot be 0 for all i. For suppose u ∈U, u ̸= 0. We can express u as a
sum
u = u1 ⊕· · · ⊕ur
(ui ∈Ui).
Not all the ui vanish. Now u 7→ui under the composition U →V →Ui. Thus
one (at least) of these compositions is ̸= 0. Hence U ∼= Ui for some i.
◁
Turning to the ﬁrst part of the Proposition, if U ⊂V , where V is σ-isotypic,
then each simple summand of U must be of type σ, by the Lemma. It follows that
U is also σ-isotypic.
For the second part, if U and W are both σ-isotypic, then U +W is a sum (not
necessarily direct) of simple subspaces Xi of type σ:
U + W = X1 + · · · + Xr.
But then
U + W = Xi1
M
· · ·
M
Xit,
where {Xi1, . . . , Xit} are some of the X1, . . . , Xr. In particular U + W is σ-
isotypic.
◀

424–I
7–8
Corollary 7.1 Suppose σ is a simple representation of G over k, Then each G-
space V over k possesses a maximal σ-isotypic subspace Vσ, which contains every
other σ-isotypic subspace.
Deﬁnition 7.3 This subspace Vσ is called the σ-component of V .
Proposition 7.6 Every semimsimple G-space V is the direct sum of its isotypic
components:
V = Vσ1
M
· · ·
M
Vσr.
Proof ▶If we take an expression for V as a direct sum of simple subspaces, and
combine those that are isomorphic, we will obtain an expression for V as a direct
sum of isotypic spaces of different types, each of which will be contained in the
corresponding isotypic component. It follows that
V = Vσ1 + · · · + Vσr.
We have to show that this sum is direct.
It is sufﬁcient to show that
(Vσ1 + · · · + Vσi−1)
\
Vσi = 0
for i = 2, . . . , r.
Suppose not. Then we can ﬁnd a simple subspace
U ⊂Vσi, U ⊂Vσ1 + · · · + Vσi−1.
By the Lemma to the last Proposition, U must be of type σi, as a subspace of Vσi.
On the other hand, as a subspace of Vσ1 +· · ·+Vσi−1 it must be of one of the types
σ1, . . . , σi−1, by the same Lemma.
This is a contradiction. Hence the sum is direct:
V = Vσ1
M
· · ·
M
Vσr.
◀
Corollary 7.2 If the G-space V carries a multiple-free representation
α = σ1 + · · · + σr
(where the σi are distinct) then V has a unique expression as a direct sum of
simple subspaces.

424–I
7–9
Remark: It is easy to see that multiplicity does give rise to non-uniqueness. For
suppose
V = U
M
U,
where U is simple. For each λ ∈k consider the map
u 7→u ⊕λu : U →U
M
U = V.
The image of this map is a subspace
U(λ) = {u ⊕λu : u ∈U}.
This subspace is isomorphic to U, since U is simple.
It is readily veriﬁed that
U(λ) ̸= U(µ) ⇐⇒λ = µ.
It follows that
V = U(λ)
M
U(µ)
for any λ, µ with λ ̸= µ.

Chapter 8
The Character of a Representation
Amazingly, all the information about a representation of a group G can be
encoded in a single function on G, the character of the representation.
Deﬁnition 8.1 Suppose α is a representation of G over k. The character χ = χα
of α is the function χ : G →k deﬁned by
χ(g) = tr (α(g)) .
Remarks:
1. Recall that the trace of an n × n-matrix A is the sum of the diagonal ele-
ments:
tr A =
X
1≤i≤n
Aii.
The trace has the following properties:
(a) tr(A + B) = tr A + tr B;
(b) tr(λA) = λ tr A.
(c) tr AB = tr BA;
(d) tr A′ = tr A;
(e) tr A∗= tr A.
Here A′ denotes the transpose of A, and A∗the conjugate transpose:
A′
ij = Aji,
A∗
ij = Aji.
The third property is the only one that is not immediate:
tr AB =
X
i
(AB)ii =
X
i
X
j
AijBji =
X
j
X
i
BjiAij = tr BA.
424–I
8–1

424–I
8–2
Note that
tr ABC ̸= tr BAC
in general. However the trace is invariant under cyclic permutations, eg
tr ABC = tr BCA = tr CAB.
In particular, if P is invertible (non-singular) then
tr PAP −1 = tr P −1PA = tr A :
similar matrices have the same trace.
It follows from this that we can speak without ambiguity of the trace tr t
of a linear transformation t : V →V ; for the matrix T representing t with
respect to one basis will be changed to PTP −1 with respect to another basis,
where P is the matrix of the change of basis.
Example: Consider the 2-dimensional representation α of D4 over C given by
s 7→
 
i
0
0
−i
!
t 7→
 
0
1
1
0
!
.
Writing χ for χα
χ(e)
=
dim α = 2
χ(s)
=
i −i = 0
χ(s2)
=
tr
 
−1
0
0
−1
!
= −1 −1 = −2
χ(s3)
=
tr
 
−i
0
0
i
!
= −i + i = 0
χ(t)
=
i −i = 0
χ(st)
=
tr
 
0
i
−i
0
!
= 0
χ(s2t)
=
tr
 
0
−1
−1
0
!
= 0
χ(s3t)
=
tr
 
0
−i
i
0
!
= 0
In summary
χ(e) = 2, χ(s2) = −2, χ(g) = 0 if g ̸= e, s2.

424–I
8–3
Proposition 8.1
1. χα+β(g) = χα(g) + χβ(g)
2. χαβ(g) = χα(g)χβ(g)
3. χα∗(g) = χα(g−1)
4. χ1(g) = 1
5. χα(e) = dim α
Proof ▶(1) follows from the matrix form
g 7→
 
A(g)
0
0
B(g)
!
for α + β.
(2) follows from the fact that if A is an m×m-matrix and B is an n×n-matrix
then the diagonal elements of the tensor product A ⊗B are just the products
AiiBjj
(1 ≤i ≤m, 1 ≤j ≤n)
Thus
tr(A ⊗B) = tr A tr B.
(3) If α takes the matrix form
g 7→A(g)
then its dual is given (with respect to the dual basis) by
g 7→A(g)′−1 = A(g−1)′.
Hence
χα∗(g) = tr A(g−1)′ = tr A(g−1) = χα(g−1).
(4) and (5) are immediate.
◀
Remark: In effect the character deﬁnes a ring-homomorphism
χ : R(G, k) →C(G, k)
from the representation-ring R(G) = R(G, k) to the ring C(G, k) of functions on
G (with values in k).
Theorem 8.1 Suppose α, β are representations of G over k. Then
I(α, β) =
1
∥G∥
X
g∈G
χα(g−1)χβ(g).

424–I
8–4
Proof ▶It is sufﬁcient to prove the result when α = 1. For on the left-hand side
I(α, β) = I(1, α∗β);
while on the right-hand side
X
g∈G
χα(g−1)χβ(g)
=
X
g
χα∗(g)χβ(g)
=
X
g
χα∗β(g)
=
X
g
χχ1(g)α∗β(g).
Thus the result for α, β follows from that for 1, α∗β.
We have to show therefore that
I(1, α) =
1
∥G∥
X
g∈G
χα(g).
By deﬁnition, if α is a representation in V ,
I(1, α) = dim homG(k, V ).
Now
hom(k, V ) = V,
with the vector v ∈V corresponding to the map
λ 7→λv : k →V.
Moreover, the action of G is preserved under this identiﬁcation; so we may write
homG(k, V ) = V G,
where V G denotes the space of G-invariant elements of V :
V G = {v ∈V : gv = v ∀g ∈G}
Thus we have to prove that
dim V G =
1
∥G∥
X
g∈G
χα(g).
Consider the ‘averaging map’ π : V →V deﬁned by
v 7→
1
∥G∥
X
g∈G
gv,

424–I
8–5
that is,
π =
1
∥G∥
X
g∈G
α(g).
It is evident that πv ∈V G for all v ∈V , ie πv is invariant under G. For
gπv
=
1
∥G∥
X
h∈G
ghv
=
1
∥G∥
X
h∈G
hv
=
πv,
since gh runs over G as h does.
On the other hand, if v ∈V G then gv = v for all g and so
πv =
1
∥G∥
X
g∈G
gv = v.
It follws that π is a projection onto V G.
Lemma 8.1 Suppose p : V →V is a projection onto the subspace U ⊂V . Then
tr p = dim U.
Proof of Lemma ▷We know that
V = im p ⊕ker p.
Let e1, . . . , em be a basis for imp = U, and let em+1, . . . , en be a basis for ker p.
Then
pei =
(
ei
1 ≤i ≤m,
0
m + 1 ≤ilen.
It follows that the matrix of p with respect to the basis e1, . . . , en is
P =












1
...
1
0
...
0













424–I
8–6
with m 1’s down the diagonal and 0’s elsewhere. Hence
tr p = tr P = m = dim U.
◁
Applying this to the averaging map π,
tr π = dim V G.
On the other hand, by the linearity of the trace,
tr π
=
1
∥G∥
X
g
tr α(g)
=
1
∥G∥
X
g
χα(g)
Thus
dim V G =
1
∥G∥
X
g
χα(g),
as we had to show.
◀
Proposition 8.2 If k = R,
χα∗(g) = χα(g−1) = χα(g).
If k = C,
χα∗(g) = χα(g−1) = χα(g).
Proof ▶First suppose k = C. Let λ1, . . . , λn be the eigenvalues of α(g). Then
χα(g) = tr α(g) = λ1 + · · · + λn.
In fact, we can diagonalise α(g), ie we can ﬁnd a basis with respect to which
g 7→A(g) =




λ1
0
...
0
λn




Now
A(g−1) = A(g)−1 =




λ−1
1
0
...
0
λ−1
n





424–I
8–7
and so
χα(g−1) = tr A(g−1) = λ−1
1
+ · · · + λ−1
n .
But since G is a ﬁnite group, gn = e for some n (eg for n = ∥G∥), and so
λn
i = 1 =⇒|λi| = 1 =⇒λ−1
i
= λi
for each eigenvalue λi. Hence
χα(g−1) = λ1 + · · · + λn = χα(g).
The result for k = R follows from this. For if A is a real matrix satisfying
An = I then we may regard A as a complex matrix, and so deduce by the argument
above that
tr(A−1) = tr A.
But since A is real, so is tr A, and thereforeHence
tr(A−1) = tr A.
◀
Corollary 8.1 Suppose α, β are representations of G over k. Then
I(α, β) =



1
∥G∥
P
g∈G χα(g)χβ(g)
if k = R
1
∥G∥
P
g∈G χα(g)χβ(g)
if k = C
Deﬁnition 8.2 We deﬁne the inner product
⟨u, v⟩
(u(g), v(g) ∈C(G, k))
by
⟨u, v⟩=



1
∥G∥
P
g∈G u(g)v(g)
if k = C
1
∥G∥
P
g∈G u(g)v(g)
if k = R
Proposition 8.3
1. The inner product ⟨u, v⟩is positive-deﬁnite.
2. I(α, β) = ⟨χα, χβ⟩.
Proposition 8.4 Two representations are equivalent if and only if their characters
are equal:
α = β ⇐⇒χα(g) = χβ(g) for all g ∈G.

424–I
8–8
Proof ▶If α = β then
B(g) = PA(g)P −1
for some P. Hence
χβ(g) = tr B(g) = tr A(g) = χα(g).
On the other hand, suppose χα(g) = χβ(g) for all g ∈G. Then for each
simple representation σ of G over k,
I(σ, α)
=
1
∥G∥
X
g∈G
χσ(g−1)χα(g)
=
1
∥G∥
X
g∈G
χσ(g−1)χβ(g)
=
I(σ, β).
It follows that σ occurs the same number of times in α and β. Since this is true
for all simple representations σ,
α = β.
◀
Proposition 8.5 Characters are class functions, ie
g′ ∼g =⇒χα(g′) = χα(g).
Remark: Recall that we write g′ ∼g to mean that g′, g are conjugate, ie there
exists an x ∈G such that
g′ = xgx−1.
Proof ▶If
g′ = xgx−1
then (since a representation g 7→A(g) is a homomorphism)
A(g′)
=
A(x)A(g)A(x−1)
=
A(x)A(g)A(x)−1.
It follows from the basic property of the trace that
χα(g′) = tr A(g′) = tr A(g) = χα(g).
◀

424–I
8–9
Proposition 8.6 Simple characters are orthogonal, ie if α, β are distinct simple
representations of G over k then
⟨χα, χβ⟩= 0.
Proof ▶This is just a restatement of the fact that
I(α, β) = 0.
◀
When k = C we can be a little more precise.
Proposition 8.7 The simple characters of G over C form an orthonormal set, ie
⟨χα, χβ⟩=
(
1
if α = β,
0
otherwise.
Proof ▶Again, this is simply a restatement of the result for the intertwining num-
ber.
◀
Theorem 8.2 The group G has at most s simple represenations over k, where s
is the number of classes in G.
Proof ▶The class functions on G form a vector space
X ⊂C(G, k).
Lemma 8.2 dim X = s.
Proof of Lemma ▷Suppose the conjugacy classes are C1, . . . , Cn. Let ci(g) denote
the characteristic function of Ci, ie
ci(g) =
(
1
if g ∈Ci,
0
otherwise
Then the functions
ci(g)
(1 ≤i ≤s)
form a basis for the class functions on G.
◁
Lemma 8.3 Mutually orthogonal vectors (with respect to a positive-deﬁnite form)
are necessarily linearly independent.

424–I
8–10
Proof of Lemma ▷Suppose v1, . . . , vr are mutually orthogonal:
⟨vi, vj⟩= 0 if i ̸= j.
Suppose
λ1v1 + · · · + λrvr = 0.
Taking the inner product of vi with this relation,
λ1⟨vi, v1⟩+ · · · + λr⟨vi, vr⟩= 0 =⇒λi = 0.
Since this is true for all i, the vectors v1, . . . , vr must be linearly independent.
◁
Now consider the simple characters of G over k. They are mutually orthogo-
nal, by the last Proposition; and so they are linearly independent, by the Lemma.
But they belong to the space X of class functions. Hence their number cannot
exceed the dimension of this space, which by Lemma 1 is s.
◀
Remark: We shall see that when k = C, the number of simple representations
is actually equal to the number of classes. This is equivalent, by the reasoning
above, to the statement that the characters span the space of class functions.
Our major aim now is to establish this result. We shall give 2 proofs, one
based on induced representations, and one of the representation theory of product
groups.
Example: Since characters are class functions, it is only necessary to compute
their values for 1 representative from each class. The character table of a group
G over k tabulates the values of the simple representations on the various classes.
By convention, if the scalar ﬁeld k is not speciﬁed it is understood that we are
speaking of representations over C.
As an illustration, let us take the group S3. The 6 elements divide into 3
classes, corresponding to the 3 cylic types:
13 e
21 (bc), (ac), (ab)
3 (abc), (acb)
It follows that S3 has at most 3 simple characters over C. Since we already know
3, namely the 2 1-dimensional representations 1, ϵ and the 2-dimensional repre-
sentation α, we have the full panoply.
We draw up the character table as follows:
class
[13]
[21]
[3]
size
1
3
2
1
1
1
1
ϵ
1
−1
1
α
2
0
−1

424–I
8–11
Let us verify that the simple characters form an orthonormal set:
I(1, 1)
=
1
6(1 · 1 · 1 + 3 · 1 · 1 + 2 · 1 · 1) = 1
I(1, ϵ)
=
1
6(1 · 1 · 1 + 3 · 1 · −1 + 2 · 1 · 1) = 0
I(1, α)
=
1
6(1 · 1 · 2 + 3 · 1 · 0 + 2 · 1 · −1) = 0
I(ϵ, ϵ)
=
1
6(1 · 1 · 1 + 3 · −1 · −1 + 2 · 1 · 1) = 1
I(ϵ, α)
=
1
6(1 · 1 · 2 + 3 · −1 · 0 + 2 · 1 · −1) = 0
I(α, α)
=
1
6(1 · 2 · 2 + 3 · 0 · 0 + 2 · −1 · −1) = 1
It is very easy to compute the character of a permutational representation, that
is, a representation arising from the action of the group G on the ﬁnite set X.
Recall that this is the representation in the function-space C(X, k) given by
(gf)(x) = f(g−1x).
Proposition 8.8 Suppose α is the permutational representation of G arising from
the action of G on the ﬁnite set X. Then
χalpha(g) = ∥{x : gx = x}∥,
ie χ(g) is equal to the number of elements of X left ﬁxed by g.
Proof ▶Let cx(t) denote the characteristic function of the 1-point subset {x}, ie
cx(t) =
(
1
if t = x,
0
otherwise.
The ∥X∥functions cx(t) form a basis for the vector space C(X, k); and the action
of g ∈G on this basis is given by
gcx = cgx,
since
gcx(t) = cx(g−1t) = 1 ⇐⇒g−1t = x ⇐⇒t = gx.
It follows that with respect to this basis
g 7→A(g),

424–I
8–12
where A = A(g) is the matrix with entries
Axy =
(
1
if x = gy,
0
otherwise.
In particular
Axx =
(
1
if x = gx,
0
otherwise.
Hence
χalpha(g) = tr A =
X
x
Axx = ∥{x : gx = x}∥.
◀
Example: Consider the action of the group S3 on X = {a, b, c}, Let us denote the
resulting representation by ρ. We only need to compute χρ(g) for 3 values of g,
namely 1 representative of each class.
We know that
χρ(e) = dim ρ = ∥X∥= 3.
The transposition (bc) (for example) has just 1 ﬁxed point, namely a. Hence
χρ(bc) = 1.
On the other hand, the 3-cycle (abc) has no ﬁxed points, so
χρ(abc) = 0.
Let us add this character to our table:
class
[13]
[21]
[3]
size
1
3
2
1
1
1
1
ϵ
1
−1
1
α
2
0
−1
ρ
3
1
0
We know that ρ is some integral combination of the simple characters, say
ρ = r · 1 + s · ϵ + t · α,
where r, s, t ∈N. These ‘coefﬁcients’ r, s, t are unique, since the simple charac-
ters are linearly independent.

424–I
8–13
It would be easy to determine them by observation. But let us compute them
from the character of ρ. Thus
r = I(1, ρ) = 1
6(1 · 1 · 3 + 3 · 1 · 1 + 2 · 1 · 0) = 1
s = I(ϵ, ρ) = 1
6(1 · 1 · 3 + 3 · −1 · 1 + 2 · 1 · 0) = 0
t = I(α, ρ) = 1
6(1 · 2 · 3 + 3 · 0 · 1 + 2 · −1 · 0) = 0
Thus
ρ = 1 + α.

Chapter 9
The Regular Representation
The group G acts on itself in 3 ways:
• By left multiplication: (g, x) 7→gx
• By right multiplication: (g, x) 7→xg−1
• By inner automorphism: (g, x) 7→gxg−1
The ﬁrst action leads to the regular representation deﬁned below. The second
action also leads to the regular representation, as we shall see. The third action
leads to the adjoint representation, which we shall consider later.
Deﬁnition 9.1 The regular representation reg of the group G over k is the per-
mutational representation deﬁned by the action
(g, x) 7→gx
of G on itself.
Proposition 9.1 The character of the regular representation is given by
χreg(g) =
(
1
if g = e,
0
otherwise.
Proof ▶We have to determine, for each g ∈G, the number of elements x ∈G
left ﬁxed by g, ie satisfying
gx = x.
But
gx = x =⇒g = e.
Thus no element g ̸= e leaves any element ﬁxed; while g = e leaves every element
ﬁxed.
◀
424–I
9–1

424–I
9–2
Proposition 9.2 The permutational representation deﬁned by right multiplication
(g, x) 7→xg−1
is equivalent to the regular representation.
Proof ▶No element g ̸= e leaves any element ﬁxed; while g = e leaves every
element ﬁxed:
xg−1 = x ⇐⇒g = e.
Thus this representation has the same character as the regular representation; and
so it is equal (that is, equivalent) to it.
◀
Alternative proof ▶In fact it is readily veriﬁed that the representation deﬁned by
right multiplication is the dual reg∗of the regular representation. But the regular
representation is self-dual, since its character is real.
◀
Proposition 9.3 Suppose α is a representation of G over k. Then
I(α, reg) = dim α.
Proof ▶Plugging the result for the character of reg above into the formula for
the intertwining number,
I(α, reg)
=
1
∥G∥
X
g∈G
χα(g−1)χreg(g)
=
1
∥G∥∥G∥χα(e)
=
dim α.
◀
This result shows that every simple representation occurs in the regular repre-
sentation, since I(σ, reg) > 0. When k = C we can be more precise.
Proposition 9.4 Each simple representation σ of G over C occurs just dim σ
times in the regular representation reg of G over C:
reg =
X
σ
(dim σ)sigma,
where the sum extends over all simple representations σ of G over C.

424–I
9–3
Proof ▶We know that reg, as a semisimple representation, is expressible in the
form
reg =
X
σ
eσσ
(eσ ∈N).
Taking the intertwining number of a particular simple representation σ with each
side,
I(σ, reg) = eσI(σ, σ) = eσ
=
dim σ,
by the Proposition.
◀
Theorem 9.1 The dimensions of the simple representations σ1, . . . , σr of G over
C satisfy the relation
dim2 σ1 + · · · + dim2 σr = ∥G∥.
Proof ▶This follows at once on taking the dimensions on each side of the identity
reg =
X
σ
(dim σ)sigma.
◀
Example: Consider S5. We have
∥S5∥= 120;
while S5 has 7 classes:
[15], [213], [221], [312], [32], [41], [5].
Thus S5 has at most 7 simple representations over C.
Let us review the information on these representations that we already have:
1. S5 has just 2 1-dimensional representations, 1 and ϵ;
2. The natural 5-dimensional representation ρ of S5 splits into 2 parts:
ρ = 1 + α,
where α is a simple 4-dimensional representation of S5;
3. If σ is a simple representation of S5 of odd dimension then ϵσ ̸= σ;

424–I
9–4
4. More generally, if σ is a simple representation of S5 with σ([213]) ̸= 0 then
ϵσ ̸= σ;
We can apply this last result to α. For
χα([213])
=
χρ([213]) −1
=
3 −1
=
2.
Hence
ϵα ̸= α.
Thus we have found 4 of the 7 (or fewer) simple representations of S5: 1, ϵ, α, ϵα.
Our dimensional equation reads
120 = 12 + 12 + 42 + 42 + a2 + b2 + c2,
where a, b, c ∈N, with a, b, c ̸= 1. (We are allowing for the fact that S5 might
have < 7 simple representations.) In other words,
a2 + b2 + c2 = 86.
It follows that
a2 + b2 + c2 ≡6
(mod 8).
Now
n2 ≡0, 1, or 4
(mod 8)
according as n ≡0 (mod 4), or n is odd, or n ≡2 (mod 4). The only way to
get 6 is as 4 + 1 + 1. In other words, 2 of a, b, c must be odd, and the other must
be ≡2 (mod 4). (In particular a, b, c ̸= 0. So S5 must in fact have 7 simple
representations.)
By (3) above, the 2 odd dimensions must be equal: say a = b. Thus
2a2 + c2 = 86.
Evidently a = 3 or 5. Checking, the only solution is
a = b = 5, c = 6.
We conclude that S5 has 7 simple representations, of dimensions
1, 1, 4, 4, 5, 5, 6.

Chapter 10
Induced Representations
Each representation of a group deﬁnes a representation of a subgroup, by
restriction; that much is obvious. More subtly, each representation of the
subgroup deﬁnes a representation of the full group, by a process called in-
duction. This provides the most powerful tool we have for constructing
group representations.
Deﬁnition 10.1 Suppose H is a subgroup of G; and suppose α is a representation
of G in V . Then we denote by αH the representation of H in the same space V
deﬁned by restricting the group action from G to H. We call αH the restriction of
α to H.
Proposition 10.1
1. (α + β)H = αH + βH
2. (αβ)H = αHβH
3. (α∗)H = (αH)∗
4. 1H = 1
5. dim αH = dim α
6. χαH(h) = χα(h)
Example: We can learn much about the representations of G by considering their
restrictions to subgroups H ⊂G. But induced representations give us the same
information—and more—much more easily, as we shall see; so the following
example is of more intellectual interest than practical value.
Let us see what we can discover about the simple characters of S4 (over C)
from the character table for S3. Let’s assume we know—as we shall prove later
424–I
10–1

424–I
10–2
in this chapter—that the number of simple characters of S4 is equal to the num-
ber of classes, 5. Let’s suppose too that we know S4 has just 2 1-dimensional
representations, 1 and ϵ. Let γ be one of the 3 other simple representations of S4.
Let
γS3 = a1 + bϵ + cα
(a, b, c ∈N).
By the Proposition above, if ¯h ⊂¯g (where ¯h is a class in H and ¯g a class in G)
then
χγ(¯g) = χγH(¯h).
So we know some of the values of χγ:
Class
[14]
[212]
[22]
[31]
[4]
size
1
6
3
8
6
1
1
1
1
1
1
ϵ
1
−1
1
1
−1
γ
a + b + 2c
a −b
x
a + b −c
y
We have found nothing about χ([22]) and χ([4]), since these 2 classes don’t inter-
sect S3. However, if we call the values x and y as shown, then the 2 equations
I(1, γ) = 0,
I(ϵ, γ) = 1
give
15a + 3b −6c + 3x + 6y
=
0
3a + 15b −6c + 3x + 6y
=
0
Setting
s = a + b,
t = a −b,
for simplicity, these yield
x = −3s + 2t,
y = −t.
The table now reads
Class
[14]
[212]
[22]
[31]
[4]
size
1
6
3
8
6
1
1
1
1
1
1
ϵ
1
−1
1
1
−1
γ
s + 2c
t
−3s + 2c
s −c
−t
Since γ is—by hypothesis—simple,
I(γ, γ) = 1.

424–I
10–3
Thus
24 = (s + 2c)2 + 6t2 + 3(−3s + 2c)2 + 8(s −c)2 + 6t2.
On simpliﬁcation this becomes
2
=
3s2 −4sc + 2c2 + t2
=
s2 + 2(s −c)2 + t2.
Noting that s, t, c are all integral, and that s, c ≥0, we see that there are just 3
solutions to this diophantine equation:
(a, b, c) = (1, 0, 1), (0, 1, 1), (0, 0, 1).
These must yield the 3 missing characters.
We have determined the character table of S4 without constructing—even
implicitly—the corresponding representations. This has an interesting parallel in
recent mathematical history. One of the great achievements of the last 25 years has
been the determination of all ﬁnite simple groups, ie groups possessing no proper
normal (or self-conjugate) subgroups. The last link in the chain was the deter-
mination of the exceptional simple groups, ie those not belonging to the known
inﬁnite families (such as the family of alternating groups An for n ≥5). Finally,
all was known except for the largest exceptional group—the so-called mammoth
group. The character table of this group had been determined several years before
it was established that a group did indeed exist with this table.
As we remarked earlier, the technique above is not recommended for serious
character hunting. The method of choice must be induced representations, our
next topic.
Suppose V is a vector space. Then we denote by C(G, V ) the G-space of
maps f : G →V , with the action of G deﬁned by
(gf)(x) = f(g−1x)
(This extends our earlier deﬁnition of C(G, k).)
Deﬁnition 10.2 Suppose H is a subgroup of G; and suppose α is a representation
of H in U. Then we deﬁne the induced representation αG of G as follows. Let
V = {F ∈C(G, U) : F(gh) = h−1F(g)
for all g ∈G, h ∈H}.
Then V is a G-subspace of C(G, U); and αG is the representation of G in this
subspace.

424–I
10–4
Remark: That V is a G-subspace follows from the fact that we are acting on G
with G and H from opposite sides (G on the left, H on the right); and their actions
therefore commute:
(gx)h = g(xh).
Thus if F ∈V then
(gF)(xh)
=
F(g−1xh)
=
h−1F(g−1x)
=
h−1((gF)(x)),
ie gF ∈V .
This deﬁnition is too cumbersome to be of much practical use. The following
result offers an alternative, and usually more convenient, starting point.
Lemma 10.1 Suppose e = g1, g2, ..., gr are representatives of the cosets of H in
G, ie
G = g1H ∪g2H ∪... ∪grH.
Then there exists an H-subspace U ′ ⊂V such that
(a) U ′ is isomorphic to U as an H-space,
(b) V = g1U ′ L g2U ′ L ...
L grU ′.
Moreover the induced representation αG is uniquely characterised by the exis-
tence of such a subspace.
Remarks:
1. After the lemma, we may write
V = g1U
M
g2U
M
...
M
grU.
2. The action of G on V is implicit in this description of V . For suppose v is
in the ith summand, say
v = giu;
and suppose ggi is in the jth coset, say
ggi = gjh.
Then gv is in the jth summand:
gv = ggiu = gj(hu).

424–I
10–5
3. The difﬁculty of taking this result as the deﬁnition of αG lies in the awk-
wardness of showing that the resulting representation does not depend on
the choice of coset-representatives.
Proof ▶To each u ∈U let us associate the function u′ = u′(g) ∈C(G, U) by
u′(g) =
(
gu
if g ∈H
0
otherwise
Then it is readily veriﬁed that
(a) u′ ∈V , ie u′(gh) = h−1u′(g)
for all h ∈H.
(b) If u 7→u′ then hu 7→hu′.
Thus the map u 7→u′ sets up an H-isomorphism between U and an H-
subspace U ′ ⊂V .
Suppose F ∈V . From the deﬁnition of V ,
F(gh) = h−1F(g).
It follows that the values of F on any coset giH are completely determined by its
value at one point gi. Thus F is completely determined by its r values
u1 = F(e), u2 = F(g2), ..., ur = F(gr).
Let us write
F ←→(u1, u2, ..., ur).
Then it is readily veriﬁed that
u′ ←→(u, 0, ..., 0);
and more generally
giu′ ←→(0, .., u, .., 0),
ie the function giu′ vanishes on all except the ith coset giH, and takes the value u
at gi.
It follows that
F = g1u′
1 + g2u′
2 + . . . + gru′
r
since the 2 functions take the same values at the r points gi. Moreover the argu-
ment shows that this expression for F ∈V as a sum of functions in the subspaces
U ′ = g1U ′, g2U ′, ..., grU ′, respectively, is unique: so that
V = g1U ′ M
g2U ′ M
...
M
grU ′.

424–I
10–6
Finally this uniquely characterises the representation αG, since the action of
G on V is completely determined by the action of H on U, as we saw in Remark
1 above.
◀
Example: Suppose α is the representation of S3 in U = C2 given by
(abc) 7→
 
ω
0
0
ω−1
!
(ab) 7→
 
0
1
1
0
!
Let us consider the representation of S4 induced by α (where we identify S3 with
the subgroup of S4 leaving d ﬁxed).
First we must choose representatives of the S3-cosets in S4. The nicest way to
choose coset representatives of H in G is to ﬁnd—if we can—a subgroup T ⊂G
transverse to H, ie such that
1. T ∩H = {e}
2. ∥T∥∥H∥= ∥G∥.
It is readily veriﬁed that these 2 conditions imply that each element g ∈G is
uniquely expressible in the form
g = th
(t ∈T, h ∈H)
It follows that the elements of T represent the cosets gH of H in G.
In the present case we could take T to be the subgroup generated by a 4-cycle:
say
{e, (abcd), (ac)(bd), (adcb)}.
Or we could take
T = V4 = {e, (ab)(cd), (ac)(bd), (ad)(bc)}
(the Viergruppe). Let’s make the latter choice; the fact that T is normal (self-
conjugate) in G should simplify the calculations. We have
S4 = S3 ∪(ab)(cd)S3 ∪(ac)(bd)S3 ∪(ad)(bc)S3;
and so αG is the represention in the 8-dimensional vector space
V = U
M
(ab)(cd)U
M
(ac)(bd)U
M
(ad)(bc)U.
As basis for this space we may take
e1 = e,
e2 = f,
e3 = (ab)(cd)e,
e4 = (ab)(cd)f,
e5 = (ac)(bd)e,
e6 = (ac)(bd)f,
e7 = (ad)(cb)e,
e8 = (ad)(bc)f,

424–I
10–7
where e = (1, 0), f = (0, 1).
To simplify our calculations, recall that if g, x ∈Sn, and
x = (a1a2 . . . ar)(b1b2 . . . bs) . . .
in cyclic notation, then
gxg−1 = (ga1, ga2, . . . , gar)(gb1, gb2, . . . , gbs) . . . ,
since, for example,
(gxg−1)(ga1) = gxa1 = ga2.
(This is how we show that 2 elements of Sn are conjugate if and only if they are
of the same type.) In our case, suppose h ∈S3, t ∈V4. Then
hth−1 ∈V4
since V4 is normal. In other words,
ht = sh,
where s ∈V4.
Now let’s determine the matrix representing (ab). By the result above, we
have
(ab) · (ab)(cd)
=
(ab)(cd) · (ab)
(ab) · (ac)(bd)
=
(bc)(ad) · (ab)
(ab) · (ad)(bc)
=
(bd)(ac) · (ab).
Thus
(ab)e6
=
(ab) · (ac)(bd)f
=
(ad)(bc) · (ab)f
=
(ad)(bc)e
=
e7.
In fact
(ab)e1 = e2,
(ab)e2 = e1,
(ab)e3 = e4,
(ab)e4 = e3,
(ab)e5 = e8,
(ab)e6 = e7,
(ab)e7 = e6,
(ab)e8 = e5.

424–I
10–8
Hence
(ab) 7→















0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0















It is not hard to see that (ab) and (abcd) generate S4. So the representation
αS4 will be completely determined—in principle, at least—if we establish the
matrix representing (abcd). We see now that it was easier to detemine the matrix
representing (ab), because (ab) ∈S3. But the general case is not difﬁcult. Notice
that
(abcd) = (ad)(bc) · (ac)
It follows that (for example)
(abcd) · (ac)(bd)
=
(ad)(bc) · (ac) · (ac)(bd)
=
(ad)(bc) · (ac)(bd) · (ac)
=
(ab)(cd) · (ac).
Now (ac) = (abc)(ab); so under α,
(ac) 7→
 
0
ω
ω−1
0
!
We see that, for example,
(abcd)e5
=
(abcd) · (ac)(bd)e
=
(ab)(cd) · (ac)e
=
(ab)(cd)ω−1f
=
ω−1e2.
We leave it to the reader to complete this calculation of the matrix representing
(abcd).
Clearly this is too time-consuming a hobby to pursue.
It is evident that
h ∼h′ in H =⇒h ∼h′ in G
In other words, each class ¯h in H lies in a unique class ¯g in G:
¯h ⊂¯g.

424–I
10–9
Or, to put it the other way round, each class ¯g in G splits into classes h1, . . . , hr in
H:
¯g ∩H = h1 ∪· · · ∪hr.
Theorem 10.1 Suppose H is a subgroup of G; and suppose β is a representation
of H. Then
χβG(¯g) =
∥G∥
∥H∥∥¯g∥
X
¯h⊂¯g
∥¯h∥χβ(¯h),
where the sum runs over those H-classes ¯h contained in ¯g.
Proof ▶Let g1, . . . , gr be representatives of the cosets of H in G, so that βG is the
representation in
V = g1U
M
· · ·
M
grU.
Lemma 10.2 With the notation above
χβG(g) =
X
i:g−1
i
ggi=h∈H
χβ(h),
where the sum extends over those coset-representatives gi for which g−1
i ggi ∈H.
Proof ▶Let us extend the function χβ (which is of course deﬁned on H) to G by
setting
χβ(g) = 0 if g /∈H,
then our formula can be written:
χβG(g) =
X
i
χβ(g−1
i ggi),
with the sum now extending over all coset-representatives.
Suppose e1, ..., em is a basis for U. Then giej (1 ≤i ≤r, 1 ≤j ≤m) is a
basis for V .
Suppose v belongs to the ith summand of V , say
v = giu;
and suppose ggi belongs to the jth coset, say
ggi = gjh.
Then
gv = ggiu = gj(hu).

424–I
10–10
So
g(giU) ⊂gjU.
Thus the basis elements in giU cannot contribute to χβG(g) unless i = j, that is,
unless ggi = gih, ie
g−1
i ggi = h ∈H.
Moreover if this is so then
g(giej) = gi(hej),
ie the m × m matrix deﬁning the action of g on giU with respect to the basis
gie1, ..., giem is just B(h); so that its contribution to χβG(g) is
χβ(h).
The result follows on adding the contributions from all those summands sent into
themselves by g.
◀
Lemma 10.3 For each g ∈G,
χβG(g) =
1
∥H∥
X
g′∈G:g′−1gg′=h∈H
χβ(h)
Proof ▶Suppose we take a different representative of the ith coset, say
g′
i = gih.
This will make the same contribution to the sum, since
g′
i
−1gg′
i = h−1(g−1
i ggi)h;
and
χβ(h−1h′h) = χβ(h′).
Thus if we sum over all the elements of G, we shall get each coset-contribution
just ∥H∥times.
◀
To return to the proof of the Proposition, we compute how many times each
element h ∈H occurs in the sum above.
Two elements g′, g′′ deﬁne the same conjugate of g in G, ie
g′−1gg′ = g′′−1gg′′,
if and only if g′′g′−1 and g commute, ie if and only
g′′N(g) = g′N(g),

424–I
10–11
where
N(g) = {x ∈G : gx = xg}.
It follows that each G-conjugate h of g in H will occur just ∥N(g)∥times in the
sum of Corollary 1. Thus if we sum over these elements h we must multiply by
∥N(g)∥.
The result follows, since
|N(g)∥= ∥G∥
∥¯g∥
by the same argument, each conjugate x−1gx of g arising from ∥N(g)∥elements
x.
◀
Examples:
1. Let us look again at αS3→S4. The classes of S4 and S3 are related as follows:
[14] ∩S3
=
[13]
[212] ∩S3
=
[21]
[22] ∩S3
=
∅
[31] ∩S3
=
[3]
[4] ∩S3
=
∅
Hence
χαS4([14])
=
24
6 · 1χα(13) = 8
χαS4([212])
=
24
6 · 63χα(21) = 0
χαS4([22])
=
0
χαS4([31])
=
24
6 · 82χα(3) = −1
χαS4(4)
=
0
Class
[14]
[212]
[22]
[31]
[4]
size
1
6
3
8
6
αS4
8
0
0
−1
0
Since
I(αS4, αS4) = 1
24

82 + 8 · 12
= 3,

424–I
10–12
αS4 has just 3 distinct simple parts. whose determination is left to the reader.
The relation between S4 and S3 is unusual, in that classes never split. If ¯g
is a class in S4 then ¯g ∩S3 is either a whole class ¯h in H, or else is empty.
This is true more generally for Sn and Sm (m < n), where Sm is identiﬁed
with the subgroup of Sn leaving the last n−m elements ﬁxed. If ¯g is a class
in Sn, then
¯g ∩Sm = ¯h or ∅.
2. Now let’s look at the cyclic subgroup
C4 = ⟨(abcd)⟩= {e, (abcd), (ac)(bd), (adcb)}
of S4. Since C4 is abelian, each element is in a class by itself. Let θ be the
1-dimensional representation of C4 deﬁned by
(abcd) 7→i
We have
[14] ∩C4
=
{e}
[212] ∩C4
=
∅
[22] ∩C4
=
{(ac)(bd)}
[31] ∩C4
=
∅
[4] ∩C4
=
{(abcd), (adcb)}
Hence
χθS4([14])
=
24
4 · 1χθ(e) = 6
χθS4([212])
=
0
χθS4([22])
=
24
4 · 3χθ((ac)(bd)) = −2
χθS4([31])
=
0
χθS4([4])
=
24
4 · 6 (χθ((abcd)) + χθ((adcb)))
=
i + (−i) = 0
Class
[14]
[212]
[22]
[31]
[4]
size
1
6
3
8
6
θS4
6
−2
0
0
0

424–I
10–13
Since
I(θS4, θS4) = 1
24

62 + 6 · 22
= 3,
θS4 has just 3 distinct simple parts. whose elucidation we again leave to the
reader.
Proposition 10.2
1. (α + α′)G = αG + α′G;
2. (α∗)G = (αG)
∗;
3. dim αG = [G : H] dim α.
It is worth noting that permutation representations are a particular case of in-
duced representations.
Lemma 10.4 Suppose G acts transitively on the ﬁnite set X. Let α be the corre-
sponding representation of G. Take x ∈X; and let
Sx = {g ∈G : gx = x}
be the corresponding stabilizer subgroup. Then
α = 1Sx→G,
ie α is the representation of G obtained by induction from the trivial representa-
tion of Sx.
Remark: The result is easily extended to non-transitive actions. For in that case the
set splits into a number of orbits, on each of which G acts transitively. On applying
the Proposition to each orbit, we conclude that any permutation representation can
be expressed as a sum of representations, each of which arises by induction from
the trivial representation of some subgroup of G.
Proof ▶By Deﬁnition 1,
α′ = 1Sx→G
is the representation in the subspace
V ⊂C(G)
consisting of those functions F : G →k satisfying
F(gh) = h−1F(g)
∀h ∈Sx.

424–I
10–14
But since Sx acts trivially on k this condition reduces to
F(gh) = F(g),
ie F is constant on each coset gSx. Thus V can be identiﬁed with the space
C(G/Sx, k) of functions on the set G/Sx of Sx-cosets in G.
On the other hand, the G-sets X and G/Sx can be identiﬁed, with the element
gx ∈X corresponding to the coset gSx. Thus
C(G/Sx, k) = C(X, k).
Since α is by deﬁnition the representation of G in CX the result follows.
◀
Proof ▶(Alternative) By Proposition 2,
χα′(g)
=
∥{i : g−1
i ggi ∈Sx}∥
=
∥{i : ggix = gix}∥
=
∥{y ∈X : gy = y}∥,
since each y ∈X is uniquely expressible in the form y = gix. But by Proposition
???,
χα(g) = ∥{y ∈X : gy = y}∥.
Thus
χα = χα′,
and so
α = α′ = 1Sx→G.
◀
Induced representations are of great practical value. But we end with an ex-
tremely important theoretical application.
Proposition 10.3 The number of simple representations of a ﬁnite group G is
equal to the number of conjugacy classes in G.
Proof ▶Let s denote the number of classes in G. We already know that
• The characters of G are class functions.
• The simple characters are linearly independent.

424–I
10–15
Thus G has at most s simple characters; and the result will follow if we can show
that every class function is a linear combination of characters.
It sufﬁces for the latter to show that we can ﬁnd a linear combination of char-
acters taking the value 1 on a given class ¯g, and vanishing on all other classes.
We can extend the formula in the Theorem above to deﬁne a map
f(¯h) 7→f G(¯g) : X(H, k) →X(G, k)
from the space X(H, k) of class functions on the subgroup H ⊂G to the space
X(G, k) of class functions on G, by
f G(¯g) =
∥G∥
∥H∥∥¯g∥
X
¯h⊂¯g
∥¯h∥f(¯h).
Evidently this map is linear:
F(h) = af(h) + bg(h) =⇒F G(g) = af G(g) + bf G(g).
Choose any g ∈¯g. Let H be the subgroup generated by g. Thus if g is of order
d,
H = Cd = ⟨g⟩= {e, g, g2, . . . , gd−1}.
Let θ denote the 1-dimensional character on H deﬁned by
θ(g) = ω = e2πi/d.
Since H is abelian, each element is in a class by itself, so all functions on H are
class functions. The d characters on H are
1, θ, θ2, . . . , θd−1.
Let f(h) denote the linear combination
f = 1 + ω−1θ + ω−2θ2 + · · · + ω−(d−1)θd−1.
Then
f(hi) =
(
d
if i = 1,
0
if i = 0. ,
ie f vanishes off the H-class {g}, but is not identically 0.
It follows that the induced function f G(g) has the required property; it van-
ishes off ¯g, while
f G(¯g) =
∥G∥d
∥H∥∥¯g∦= 0.

424–I
10–16
This proves the result, since f G is a linear combination of characters:
f G = 1G + ωθG + ω2(θ2)G + · · · + ω−(d−1)(θd−1)G.
◀
Examples:
1. S3 has 3 classes: 13, 21 and 3. So it has 3 simple representations over C, as
of course we already knew: namely 1, ϵ and α.
2. D(4) has 5 classes: {e}, {s2}, {s, s3}, {c, d} and {h, v}. So it has 5 simple
representations over C. We already know of 4 1-dimensional representa-
tions. In addition the matrices deﬁning the natural 2-dimensional represen-
tation in R2 also deﬁne a 2-dimensional complex representation. (We shall
consider this process of complexiﬁcation more carefully in Chapter ???.)
This representation must be simple, since the matrices do not commute, as
they would if it were the sum of 2 1-dimensional representations. Thus all
the representations of D4 are accounted for.
Proposition 10.4 (Frobenius’ Reciprocity Theorem) Suppose α is a representa-
tion of G, and β a representation of H ⊂G. Then
IG(α, βG) = IH(αH, β).
Proof ▶We have
IG(α, βG)
=
1
∥G∥
X
¯g
∥¯g∥χα(¯g)
∥G∥
∥H∥∥¯g∥
X
¯h⊂¯g
∥¯h∥χβ(¯h)
=
1
∥H∥
X
¯h
∥¯h∥χα(¯h)χβ(¯h)
=
IH(αH, β).
◀
This short proof does not explain why Frobenius’ Reciprocity Theorem holds.
For that we must take a brief excursion into category theory.
Let CG denote the category of G-spaces and G-maps. Then restriction and
induction deﬁne functors
S : CG →CH,
, I : CH →CG.
Now 2 functors
E : C1 →C2,
F : C2 →C1

424–I
10–17
are said to be adjoint if for any 2 objects X ∈C1, Y ∈C2 there are bijections
MC1(X, FY ) = MC2(EX, Y )
which are natural in the sense that given any morphism
f : X →X′
in C1 the diagram
M(X, FY )
←−
M(X′, FY )


M(EX, Y )
←−
M(EX′, Y )
is commutative, and similarly given any morphism
e : Y →Y ′
in C2 the diagram
M(X, FY )
−→
M(X, FY ′)


M(EX, Y )
−→
M(EX, Y ′)
is commutative.
It’s not difﬁcult to establish—but would take us too far out of our way—that
the induction and restriction functors are adjoint in this sense: if V is a G-space,
and U a H-space, then
homH(VH, U) = homG(V, U G).
On taking dimensions, this gives Frobenius’ Theorem:
IH(αH, β) = IG(α, βG).

Chapter 11
Representations of Product Groups
The representations of a product group G × H can be expressed—in as neat
a way as one could wish—in terms of the representations of G and H.
Deﬁnition 11.1 Suppose α is a representation of G in the vector space U over k,
and β a representation of H in the vector space V over k. Then we denote by
α × β the representation of the product group G × H in the tensor product U ⊗V
deﬁned by
(g, h)
X
u ⊗v =
X
gu ⊗hv.
Lemma 11.1
1. χα×β(g, h) = χα(g)χβ(h)
2. dim(α × β) = dim α dim β
3. if α and β are both representations of G then
(α × β)G = αβ,
where the restriction is to the diagonal subgroup
G = {(g, g) : g ∈G} ⊂G × G.
Proposition 11.1 The representation α×β of G×H over C is simple if and only
if α and β are both simple. Moreover, every simple representation of G × H is of
this form.
Proof ▶
Lemma 11.2 If α1, α2 are representations of G, and β1, β2 are representations of
H, all over k, then
I(α1 × β1, α2 × β2) = I(α1, β1)I(α2, β2)
424–I
11–1

424–I
11–2
Proof ▶We have
I(α1 × β1, α2 × β2)
=
1
|G||H|
X
(g,h)∈G×H
χα1×β1(g, h)χα2×β2(g, h)
=
1
|G||H|
X
(g,h)∈G×H
χα1(g)χβ1(h)χα2(g)χβ2(h)
=
1
|G|
X
g∈G
χα1(g)χα2(g) 1
|G|
X
h∈H
χβ1(h)χβ2(h)
=
I(α1, β1)I(α2, β2)
◀
Recall that a representation α over C is simple if and only if
I(α, α) = 1.
Thus if α is a simple representation of G and β is a simple representation of
H (both over C) then
I(α × β, α × β) = I(α, α)I(β, β) = 1;
and therefore α × β is simple.
Now suppose G has r classes and H has s classes. Then G×H has rs classes,
since
(g, h) ∼(g′, h′) ⇐⇒g ∼g′ and h ∼h′.
But we have just produced rs simple representations α × β of G × G; so these are
in fact the full complement.
(The lemma shows that these representations are distinct; for
I(α1 × β1, α2 × β1) = I(α1, α2)I(β1, β2) = 0
unless α1 = α2 and β1 = β2.)
◀
It is useful to give a proof of the last part of the Proposition not using the fun-
damental result that the number of simple representations is equal to the number
of classes; for we can give an alternative proof of this result using product groups.
Proof ▶(of last part of Proposition). Suppose γ is a representation of G × H in
W over C.
Consider the restriction γH of γ to the subgroup H = e × H ⊂G × H. Let V
be a simple part of WH:
WH = V ⊕· · ·
Let
X = homH(V, W)

424–I
11–3
be the vector space formed by the H-maps t : V →W. This is non-trivial since
V is a subspace of W.
Now X is a G-space, under the action
(gt)(v) = g(tv)
Let U be a simple G-subspace of X. Then
homG(U, X)
=
homG(U, homH(V, W))
=
homG×H(U ⊗V, W).
Since this space is non-trivial, there exists a G × H map
θ : U ⊗V →W.
But since both U ⊗V and W are simple, we must have
ker θ = 0,
im θ = W.
Hence θ is an isomorphism, ie
W = U ⊗V.
Thus
γ = α × β,
where α is the representation of G in U, and β is the representation of H in V .
◀
Theorem 11.1 Suppose G has n elements and s classes. Then
1. G has s simple representations over C;
2. if these are σ1, . . . , σs then
dim2 σ1 + · · · + dim2 σs = n.
Proof ▶Let τ be the permutation representation of G × G in C(G, k) induced by
the action
(g, h)x = gxh−1
of G × G on G.
Lemma 11.3 The character of τ is given by
χτ(g, h) =
(
|G|/|¯g|
if g ∼h
0
otherwise

424–I
11–4
Proof ▶Since τ is a permutational representation,
χτ(g, h)
=
|{x : (g, h)x = x}|
=
|{x : gxh−1 = x}|
=
|{x : x−1gx = h}|.
If g ̸∼h then clearly no such x exists.
Suppose g ∼h. Then there exists at least one x, say x0, such that
h = x−1
0 gx0.
Now
x−1gx = h
⇐⇒
x−1gx = x−1
0 gx0
⇐⇒
(xx−1
0 )g = g(xx−1
0 )
⇐⇒
xx−1
0
∈Z(g)
⇐⇒
x ∈Z(g)x0.
Thus
χτ(g, h)
=
|{x : gxh−1 = x|
=
|Z(g)|
=
|G|/|¯g|.
◀
Lemma 11.4 Suppose G has simple representations σ1, . . . σs. Then
τ = σ∗
1 × σ1 + · · · + σ∗
s × σs.
Proof ▶We know that the simple representations of G × G are σi × σj. Thus
τ =
X
i,j
e(i, j)σi × σj,
where e(i.j) ∈N.

424–I
11–5
To determine e(i, j) we must compute the intertwining number
I(τ, σi × σj)
=
1
|G|2
X
g,h
χτ(g, h)χσi×σj(g, h)
=
1
|G|2
X
g,h
χτ(g, h)χσi(g)χσj(h)
=
1
|G|2
X
h∼g
|G|
|¯g| χσi(g)χσj(h)
=
1
|G|χσi(g)χσj(g)
=
1
|G|χσ∗
i (g)χσj(g)
=
I(σ∗
i , σj).
Thus
I(τ, σi × σj) =
(
1
if σ∗
i = σj
0
otherwise
In other words, σi × σj occurs in τ if and only if σ∗
i = σj, and then occurs just
once.
◀
It follows from this result in particular that the number of simple representa-
tions is equal to I(τ, τ).
Lemma 11.5 I(τ, τ) is equal to the number of classes in G.
Proof ▶We have
I(τ, τ)
=
1
|G|2
X
g,h
|χτ(g, h)|2
=
1
|G|2
X
g
X
h∼g
 |G|
|¯g|
!2
=
1
|G|2
X
g
|¯g||G|2
|¯g|2
=
X
g
1
|¯g|.
Since each class contributes |¯g| terms to this sum, each equal to 1/|¯g|, the sum is
equal to the number of classes.
◀
That proves the ﬁrst part of the Theorem; the number of simple representations
is equal to I(τ, τ), which in turn is equal to the number of classes.

424–I
11–6
The second part follows at once on taking dimensions in
τ = σ∗
1 × σ1 + · · · + σ∗
s × σs.
◀
Example: We can think of product groups in 2 ways—as a method of constructing
new groups, or as a way of splitting up a given group into factors.
We say that G = H × K, where H, K are subgroups of G, if the map
H × K →G : (h, k) 7→hk
is an isomorphism.
A necessary and sufﬁcient condition for this— supposing G ﬁnite—is that
1. elements of H and K commute, ie
hk = kh
for all h ∈H, k ∈K; and
2. |G| = |H||K|.
Now consider the symmetry group G of a cube. This has 48 elements; for
there are 8 vertices, and 6 symmetries leaving a given vertex ﬁxed.
Of these 48 symmetries, half are proper and half improper. The proper sym-
metries form a subgroup P ⊂G.
Let Z = {I, J}, where J denotes reﬂection in the centre of the cube. In fact
Z is the centre of G:
Z = ZG = {z ∈G : zg = gz for all g ∈G}.
By the criterion above,
G = Z × P.
Moreover,
P = S4,
as we can see by considering the action of symmetries on the 4 diagonals of the
cube. This deﬁnes a homomorphism
Θ : P →S4.
Since no symmetry send every diagonal into itself,
ker Θ = {I}.

424–I
11–7
Thus Θ is injective; and so it is bijective, since
|P| = 24 = |S4|.
Hence Θ is an isomorphism.
Thus
G = C2 × S4.
In theory this allows us to dtermine the character table of G from that of S4.
However, to make use of this table we must know how the classes of C2 × S4 are
to be interpreted geometrically. This is described in the following table.
class in C2 × P
size
order
geometricaldescription
{I} × 14
1
1
identity
{J} × 14
1
1
reﬂection in centre
{I} × 212
6
2
half-turn about axis joining centres of op-
posite edges
{J} × 212
6
2
relﬂection in plane through opposite
edges
{I} × 22
3
2
rotation about axis parallel to edge
through π
{J} × 22
3
2
relﬂection in central plane parallel to face
{I} × 31
8
3
rotation about diagonal through ±π
3
{J} × 31
8
6
screw reﬂection about diagonal
{I} × 4
6
4
rotation about axis parallel to edge
through ±π
2
{J} × 4
6
4
screw reﬂection about axis parallel to
edge
The character table of C2 × S4 id readily derived from that of S4. We denote
the non-trivial character of C2 (J 7→−1) by η.
Class I × 14 J × 14 I × 212 J × 212 I × 22 J × 22 I × 31 J × 31 I × 4 J × 4
1 × 1
1
1
1
1
1
1
1
1
1
1
η × 1
1
−1
1
−1
1
−1
1
−1
1
−1
1 × ϵ
1
1
−1
−1
1
1
1
1
−1
−1
η × ϵ
1
−1
−1
1
1
−1
1
−1
−1
1
1 × α
2
2
0
0
2
2
−1
−1
0
0
η × α
2
−2
0
0
2
−2
−1
1
0
0
1 × β
3
3
1
1
−1
−1
0
0
−1
−1
η × β
3
−3
1
−1
−1
1
0
0
−1
1
1 × ϵβ
3
3
−1
−1
−1
−1
0
0
1
1
η × ϵβ
3
−3
−1
1
−1
1
0
0
1
−1

424–I
11–8
Now suppose π is the 6-dimensional permutational representation of G in-
duced by its action on the 6 faces of the cube. Its character is readily determined:
Class I × 14 J × 14 I × 212 J × 212 I × 22 J × 22 I × 31 J × 31 I × 4 J × 4
π
6
0
0
2
2
4
0
0
2
0
For example, to determine χπ({J} × 4) we note that an element of this class
is a rotation about an axis parallel to an edge followed by reﬂection in the centre.
This will send each of the 4 faces parallel to the edge into an adjacent face, and
will swap the other 2 faces. Thus it will leave no face ﬁxed; and so
χπ({J} × 4) = 0.
We have
I(π, 1 × 1) = 1
48(1 · 1 · 6 + 6 · 2 · 1 + 3 · 2 · 1 + 3 · 4 · 1 + 6 · 2 · 1) = 1
(as we knew it would be, since the action is transitive). Similarly,
I(π, η × 1)
=
1
48(1 · 1 · 6 −6 · 2 · 1 + 3 · 2 · 1 −3 · 4 · 1 + 6 · 2 · 1) = 0,
I(π, 1 × ϵ)
=
1
48(1 · 1 · 6 −6 · 2 · 1 + 3 · 2 · 1 + 3 · 4 · 1 −6 · 2 · 1) = 0,
I(π, η × ϵ)
=
1
48(1 · 1 · 6 + 6 · 2 · 1 + 3 · 2 · 1 −3 · 4 · 1 −6 · 2 · 1) = 0.
It is clear at this point that the remaining simple parts of π must be of dimensions
2 and 3. Thus π contains either 1 × α or η × α. In fact
I(π, 1 × α) = 1
48(1 · 6 · 2 + 3 · 2 · 2 + 3 · 4 · 2) = 1.
The remaining part drops out by subtraction; and we ﬁnd that
π = 1 × 1 + 1 × α + η × ϵβ.

Chapter 12
Exterior Products
12.1
The exterior products of a vector space
Suppose V is a vector space. Recall that its rth exterior product ∧rV is a vector
space, spanned by elements of the form
v1 ∧· · · ∧vr
(v1, . . . , vr ∈V ),
where
vπ1 ∧· · · ∧vπr = ϵ(π)v1 ∧· · · ∧vr
for any permutation π ∈Sr.
This implies in particular that any product containing a repeated element van-
ishes:
· · · ∧v ∧· · · ∧v ∧· · · = 0.
(We are assuming here that the characteristic of the scalar ﬁeld k is not 2. In fact
we shall only be concerned with the cases k = R or C.)
The exterior product ∧rV could be deﬁned rigorously as the quotient-space
∧rV = V ⊗r/X,
where X is the subspace of V ⊗r spanned by all elements of the form
vπ1 ∧. . . vπr −ϵ(π)v1 ∧· · · ∧vr,
where v1, . . . , vr ∈V, π ∈Sr, and ϵ denotes the parity representation of Sr.
Suppose e1, . . . , en is a basis for V . Then
ei1 ∧ei2 ∧· · · ∧eir
(i1 < i2 < · · · < ir)
424–I
12–1

12.1. THE EXTERIOR PRODUCTS OF A VECTOR SPACE
424–I
12–2
is a basis for ∧rV . (Note that there is one basis element corresponding to each
subset of {e1, . . . , en} containing r elements.) It follows that if dim V = n then
∧rV = 0 if r > n;
while if r ≤n then
dim ∧rV =
 n
r
!
.
Now suppose T : V →V is a linear map. Then we can deﬁne a linear map
∧rT : ∧rV →∧rV
by
(∧rT)(v1 ∧· · · ∧vr) = (Tv1) ∧· · · ∧(Tvr).
(To see that this action is properly deﬁned, it is sufﬁcient to see that it sends the
subspace X ⊂V ⊗n described above into itself; and that follows at once since
(∧rT) (vπ1 ∧· · · ∧vπr)−ϵ(π)v1∧. . . vr = (Tvπ1)∧· · ·∧(Tvπr)−ϵ(π)(Tv1)∧· · ·∧(Tvr)
is again one of the spanning elements of X.)
In the case r = n, ∧nV is 1-dimensional, with the basis element
e1 ∧· · · ∧en;
and
∧nT = (det T)I.
This is in fact the “true” deﬁnition of the determinant.
Although we shall not make use of this, the spaces ∧rV can be combined to
form the exterior algebra ∧V of V
∧V =
M
∧rV,
with the “wedge multiplication”
∧: ∧rV × ∧sV →∧r+sV
deﬁned by
(u1 ∧· · · ∧ur) ∧(v1 ∧· · · ∧vs) = u1 ∧· · · ∧ur ∧v1 ∧· · · ∧vs,
extended to ∧V by linearity.
Observe that if a ∈∧rV, b ∈∧sV then
b ∧a = (−1)rsa ∧b.
In particular the elements of even order form a commutative subalgebra of ∧V .

12.2. THE EXTERIOR PRODUCTS OF A GROUP REPRESENTATION
424–I
12–3
12.2
The exterior products of a group representa-
tion
Deﬁnition 12.1 Suppose α is a representation of G in V . Then we denote by ∧rα
the representation of G in ∧rV deﬁned by
g(v1 ∧· · · ∧vr) = (gv1) ∧· · · ∧(gvr).
In other words, g acts through the linear map ∧r (α(g)).
Proposition 12.1 Suppose g ∈G has eigenvalues λ1, . . . , λn in the representa-
tion α. Then the character of ∧rα is the rth symmetric sum of the λ’s, ie
χ∧rα(g) =
X
i1<i2<···<ir
λi1λi2 . . . λir.
Proof ▶Let us suppose that k = C. We know that α(g) can be diagonalised, ie
we can ﬁnd a basis e1, . . . , en of the representation-space V such that
gei = λiei
(i = 1, . . . , n).
But now
gei1 ∧ei2 ∧· · · ∧eir = λi1λi2 . . . λi1ei1 ∧ei2 ∧· · · ∧eir,
from which the result follows, since these products form a basis for ∧rV .
◀
12.3
Symmetric polynomials
We usually denote the symmetric product in the Proposition above by
X
λ1 . . . λr.
It is an example of a symmetric polynomial in λ1, . . . , λn.
More generally, suppose A is a commutative ring, with 1. (In fact we shall
only be interested in the rings Z and Q.) As usual, A[x1, . . . , xn] denotes the ring
of polynomials in x1, . . . , xn with coefﬁcients in A.
The symmetric group Sn acts on this ring, by permutation of the variables:
(πP)(x1, . . . , xn) = P

xπ−1(1), . . . , xπ−1(n)

(π ∈Sn).
The polynomial P(x1, . . . , xn) is said to be symmetric if it is left invariant by this
action of Sn. The symmetric polynomials evidently form a sub-ring of A[x1, . . . , xn],
which we shall denote by Σn(A).

12.3. SYMMETRIC POLYNOMIALS
424–I
12–4
The n polynomials
a1 =
X
xi, a2 =
X
i1<i2
xi1xi2, . . . , an = x1 · · · xn
are symmetric; as are
s1 =
X
xi, s2 =
X
x2
i , ; s3 =
X
x3
i , . . . .
Proposition 12.2 The ring ΣZ(n) is freely generated over Z by a1, . . . , an, ie the
map
p(x1, . . . , xn) 7→p(a1, . . . , an) : Z[x1, . . . , xn] →Σn(Z)
is a ring-isomorphism.
Proof ▶We have to show that
1. Every symmetric polynomial P(x1, . . . , xn) over Z (ie with integer coefﬁ-
cients) is expressible as a polynomial in a1, . . . , an over Z:
P(x1, . . . , xn) = p(a1, . . . , an).
This will show that the map is surjective.
2. The map is injective, ie
p(a1, . . . , an) ≡0 =⇒p ≡0.
1. Any polynomial is a linear combination of monomials xe1
1 · · · xen
n . We order
the monomials ﬁrst by degree, with higher degree ﬁrst, and then within each
degree lexicographically, eg if n = 2 then
1 < x2 < x1 < x2
2 < x1x2 < x2
1 < x3
2 < · · · .
The leading term in p(x1, . . . , xn) is the non-zero term cxe1
1 · · · xen
n contain-
ing the greatest monomial in this ordering.
Now suppose the polynomial P(x1, . . . , xn) is symmetric. Evidently e1 ≥
e2 ≥· · · ≥en in the leading term. For if say e1 < e2 then the term
cxe2
1 xe1
2 · · · xen
n — which must also appear in P(x1, . . . , xn).
◀
Corollary 12.1 The ring ΣQ(n) is freely generated over Q by a1, . . . , an,
Proposition 12.3 The ring ΣQ(n) is freely generated over Q by s1, . . . , sn,
Proof ▶
◀

12.4. NEWTON’S FORMULA
424–I
12–5
12.4
Newton’s formula
It follows from the Propositions above that the power-sums sn are expressible in
terms of the symmetric products an, and vice versa. More precisely, there exist
polynomials Sn(x1, . . . , xn) and An(x1, . . . , xn) such that
sn = Sn(a1, . . . , n),
an = An(a1, . . . , n),
with the coefﬁcients of Sn integral and those of An rational. Newton’s formula
allows these polynomials to be determined recursively.
Let
f(t) = (1 −x1t) · · · (1 −xnt)
= 1 −a1t + a2t2 −· · · + (−1)nantn.
Then
f ′(t)
f(t) = frac−x11 −x1t + · · · + frac−xn1 −xnt
= −s1 −s2t −s3t2 −· · · .
Thus
−a1+2a2t−3a3t2+· · ·+(−1)nnantn−1 = (1−a1t+a2t2−· · ·+(−1)nantn)(−s1−s2t−s3t2−· · · ).
Equating coefﬁcients,
a1
=
s1
2a2
=
s1a1 −s2
3a3
=
s1a2 −s2a1 + s3
. . .
rar
=
s1ar −s2ar−1 + · · · + (−1)r−1sr
. . .
Evidently these equations allow us to express s1, s2, s3, . . . successively in

12.5. PLETHYSM
424–I
12–6
terms of a1, a2, a3, . . . , or vice versa:
s1 = a1
s2 = a2
1 −2a2
s3 = a3
1 −3a1a2 + 3a3
. . .
a1 = s1
2a2 = s2
1 −s2
6a3 = s3
1 −3s1s2 + 2s3
. . .
12.5
Plethysm
There is another way of looking at the exterior product — as a particular case of
the plethysm operator on the representation-ring R(G).
Suppose V is a vector space over a ﬁeld k of characteristic 0. (We shall only
be interested in the cases k = R or C.) Then the symmetric group Sn acts on the
tensor product V ⊗n by permutation of the factors:
π(v1 ⊗· · · ⊗vn) = vπ−11 ⊗· · · ⊗vπ−1n.
Thus V ⊗n carries a representation of Sn. As we know this splits into components
V Σ corresponding to the simple representations Σ of Sn:
V ⊗n = V Σ1 ⊕· · · ⊕V Σs,
where Σ1, . . . , Σs are the simple representations of Sn. (We shall ﬁnd it convenient
to use superﬁxes rather than sufﬁxes for objects corresponding to representations
of Sn.)
We are particularly interested in the components corresponding to the 2 1-
dimensional representations of Sn: the trivial representation 1n and the parity
representation ϵn, and we shall write
V P = V 1n,
V N = V ϵn.
We also use P and N to denote the operations of symmetrisation and skew-
symmetrisation on V ⊗n; that is, the linear maps
P, N : V ⊗n →V ⊗n

12.5. PLETHYSM
424–I
12–7
deﬁned by
P(v1 ⊗· · · ⊗vn) = 1
n!
X
π∈Sn
π(v1 ⊗· · · ⊗vn),
N(v1 ⊗· · · ⊗vn) = 1
n!
X
π∈Sn
ϵ(π)π(v1 ⊗· · · ⊗vn).
Suppose π ∈Sn. Regarding π as a map V ⊗n →V ⊗n, we have
πP = P = Pπ,
πN = ϵ(π)N = Nπ.
It follows that
{P 2 = P,
N 2 = N,
ie P and N are both projections onto subspaces of V ⊗n.
We say that x ∈V ⊗n is symmetric if
πx = x
for all π ∈Sn; and we say that x is skew-symmetric if
πx = ϵ(π)x
for all π. It follows at once from the relations πP = P, πN = ϵN that x is
symmetric if and only if
Px = x;
while x is skew-symmetric if and only if
Nx = x.
Thus P is a projection onto the symmetric elements in V ⊗n, and N is a projection
onto the skew-symmetric elements.
To see the connection with the exterior product ∧nV , recall that we could
deﬁne the latter by
∧nV = V ⊗n/X,
where X ⊂V ⊗n is the subspace spanned by elements of the form
πx −ϵ(π)x.
It is easy to see that Nx = 0 for such an element x; while conversely, for any
x ∈V ⊗n
x −Nx = 1
n!
X
π∈Sn
ϵ(π) (ϵ(π)x −πxs)
∈X

12.5. PLETHYSM
424–I
12–8
It follows that
X = ker N;
and so (since N is a projection)
∧nV = V/X ∼= im N = V N;
that is, the nth exterior product of V can be identiﬁed with the ϵ-component of
V ⊗n.
Now suppose that V carries a representation α of some group G. Then G acts
on V ⊗n through the representation αn.
Proposition 12.4 Suppose α is a represenation of G in V . Then the actions of G
and Sn on V ⊗n commute.
For each simple representation Σ of Sn, the component V Σ of V ⊗n is stable
under G, and so carries a representation αΣ of G. Thus
αn = αΣ1 + · · · + αΣs,
where Σ1, . . . , Σs are the simple representations of Sn.
Proof ▶We have
πg(v1 ⊗· · · ⊗vn) = π(gv1 ⊗· · · ⊗gvn)
= (gvπ−11 ⊗· · · ⊗gvπ−1n)
= gπ(v1 ⊗· · · ⊗vn).
◀
Since the actions of G and Sn on V ⊗n commute, they combine to deﬁne a
representation of the product group G × Sn on this space.
Corollary 12.2 The representation of G × Sn on V ⊗n is given by
αΣ1 × Σ1 + · · · + αΣs × Σs.
Suppose g ∈G (or more accurately, α(g)) has eigenvalues λ1, . . . , λd. We
know that the character of
∧nα = αϵn
is the nth symmetric product of the λi:
χ∧nα(g) = an(λ1, . . . , λd).

12.5. PLETHYSM
424–I
12–9
Proposition 12.5 To each simple representation Σ of Sn there corresponds a
unique symmetric function SΣ of degree n such that for any representation α of
G, and for any g ∈G with eigenvalues λ1, . . . , λd,
χ∧nα(g) = SΣ(λ1, . . . , λd).
Proof ▶We begin by establishing an important result which should perhaps have
been proved when we discussed the splitting of a G-space V into components
V = Vσ1 ⊕· · · ⊕Vσs
corresponding to the simple representations σ1, . . . , σs of G.
Lemma 12.1 The projection Pσ onto the σ-component of V is given by
Pσ = dim σ
∥G∥
X
g∈G
χσ(g−1)g.
Proof of Lemma ▷Suppose α is a representation of G in V . Then the formula
above deﬁnes a linear map
P : V →V.
Suppose h ∈G. Then (writing d for dim σ)
hPh−1 =
d
∥G∥
X
g
χσ(g−1)hgh−1
=
d
∥G∥
X
g′
χσ(h−1g′−1h)g′
=
d
∥G∥
X
g′
χσ(g′−1)g′
= P.
Now suppose α is simple. By Schur’s Lemma, the only linear transformations
commuting with all α(g) are multiples of the identity. Thus
P = ρI
for some ρ ∈C. Taking traces,
d
∥G∥
X
g
χσ(g−1)χα(g) = ρd.

12.5. PLETHYSM
424–I
12–10
It follows that
ρ =



1
α = σ
0
α ̸= σ
◁In other words,
P =



I
α = σ
0
α ̸= σ
It follows that P acts as the identity on all simple G-subspaces carrying the repre-
sentation σ, and as 0 on all simple subspaces carrying a representation σ′ ̸= σ. In
particular, P = I on Vσ and P = 0 on Vσ′ for all σ′ ̸= σ. In other words, P is the
projection onto the component Vσ.
◀

Chapter 13
Real Representations
Representation theory over C is much simpler than representation theory
over R. For that reason, we usually complexify real representations—extend
the scalars from R to C—just as we do with polynomial equations. But at the
end of the day we must determine if the representations—or solutions—that
we have obtained are in fact real.
Suppose U is a vector space over R. Then we can deﬁne a vector space V =
CU over C by “extension of scalars”. More precisely,
V = C ⊗R U.
In practical terms,
V = U ⊕iU,
ie each element v ∈V is uniquely expressible in the form
v = u1 + iu2
(u1, u2 ∈U).
If e1, . . . , en is a basis for U over R, then it is also a basis for V over C. In
particular,
dimC V = dimR U.
On the other hand, suppose V is a vector space over C. Then we can deﬁne a
vector space U = RV over R by “forgetting” scalar multiplication by non-reals.
Thus the elements of U are precisely the same as those of V . If e1, . . . , en is a
basis for V over V , then e1, ie1, e2, ie2, . . . , en, ien is a basis for U over R. In
particular,
dimR U = 2 dimC V.
Now suppose G acts on the vector space U over R. Then G also acts on CU,
by
g(u1 + iu2) = (gu1) + i(gu2).
424–I
13–1

424–I
13–2
On the other hand, suppose G acts on the vector space V over C. Then G also
acts on RV by the same rule
(g, v) 7→gv.
Deﬁnition 13.1
1. Suppose β is a real representation of G in U. Then we
denote by Cβ the complex representation of G in the vector space
CU = U ⊕iU
derived from U by extending the scalars from R to C.
2. Suppose α is a complex representation of G in V . Then we denote by Rα
the real representation of G in the vector space RV derived from V by
“forgetting” scalar multiplication by non-reals.
Remarks:
1. Suppose β is described in matrix terms, by choosing a basis for U and giving
the matrices B(g) representing β(g) with respect to this basis. Then we can
take the same basis for CU, and the same matrices to represent Cβ(g). Thus
from the matrix point of view, β and Cβ appear the same. The essential
difference is that Cβ may split even if β is simple, ie we may be able to ﬁnd
a complex matrix P such that
PB(g)P −1 =
 
C(g)
0
0
D(g)
!
for all g ∈G, although no real matrix P has this property.
2. Suppose α is described in matrix form, by choosing a basis e1, e2, . . . , en
for V , and giving the matrices A(g) representing α(g) with respect to this
basis. Then we can take the 2n elements e1, ie1, e2, ie2, . . . , en, ien as a
basis for RV ; and the matrix representing Rα(g) with respect to this basis
can be derived from the matrix A = A(g) representing α(g) as follows. By
deﬁnition,
ger =
X
s
Asres.
Let
Ar,s = Xr,s + iYr,s,
where Xr,s, Yr,s ∈R. Then
ger
=
Xsres + Ysries
g(ier)
=
−Ysres + Xsries

424–I
13–3
Thus the entry Ars is replaced in Rα(g) by the 2 × 2-matrix
 
Xr,s
−Yr,s
Yr,s
Xr,s
!
Proposition 13.1
1. C(β + β′) = Cβ + Cβ′
2. C(ββ′) = (Cβ)(Cβ′)
3. C(β∗) = (Cβ)∗
4. C1 = 1
5. dim Cβ = dim β
6. χCβ(g) = χβ(g)
7. I(Cβ, Cβ′) = I(β, β′)
8. R(α + α′) = Rα + Rα′
9. R(α∗) = (Rα)∗
10. dim Rα = 2 dim α
11. χRα(g) = 2ℜχα(g) = χα(g) + χα(g−1)
12. RCβ = 2β
13. CRα = α + α∗
Proof ▶All is immediate except (perhaps) parts (11) and (13).
11. Suppose α(g) is represented by the n × n matrix
A = X + iY,
where X, Y are real. Then—as we saw above—the entry Ars is replaced in Rα(g)
by the 2 × 2 matrix
 
Xr,s
−Yr,s
Yr,s
Xr,s
!

424–I
13–4
Thus
tr Rα(g)
=
2
X
r
Xrr
=
2ℜ(
X
r
Arr)
=
2ℜtr α(g)
=
2ℜχα(g)
=
χα(g) + χα(g−1)
since
χ(g) = χ(g−1).
13. This now follows on taking characters, since
χCRα(g)
=
χRα(g)
=
χα(g) + χα(g−1)
=
χα(g) + χα∗(g)
Since this holds for all g,
CRα = α + α∗.
◀
Lemma 13.1 Given a representation α of G over C there exists at most one real
representation β of G over R such that
α = Cβ.
Proof ▶By Proposition 1,
Cβ = Cβ′
=⇒
χCβ(g) = χCβ′(g)
=⇒
χβ(g) = χβ′(g)
=⇒
β = β′.
◀
Deﬁnition 13.2 A representation α of G over C is said to be real if α = Cβ for
some representation β over R.
Remarks:

424–I
13–5
1. In matrix terms α is real if we can ﬁnd a complex matrix P such that the
matrices
PB(g)P −1
are real for all g ∈G.
2. Since β is uniquely determined by α in this case, one can to some extent
confuse the two (as indeed in speaking of α as real), although eg if dis-
cussing simplicity it must be made clear whether the reference is to α or to
β.
Lemma 13.2 Consider the following 3 properties of the representation α over C:
1. α is real
2. χα is real, ie χα(g) ∈R for all g ∈G
3. α = α∗
We have
(1) =⇒(2) ⇐⇒(3).
Proof ▶(1) =⇒(2): If α = Cβ then
χα(g) = χβ(g).
But the trace of a real matrix is necessarily real.
(2) ⇐⇒(3): If χα is real then
χα(g) = χα(g) = χα∗(g)
for all g ∈G. Hence
α = α∗.
◀
Problems involving representations over R often arise in classical physics,
since the spaces there are normally real, eg those given by the electric and mag-
netic ﬁelds, or the vibrations of a system. The best way of tackling such a problem
is usually to complexify, ie to extend the scalars from R to C. This allows the pow-
erful techniques developed in the earlier chapters to be applied. But at the end of
the day it may be necessary to determine whether or not the representations that
arise are real. The Lemma above gives a necessary condition: if α is real then
its character must be real. But this condition is not sufﬁcient; and our aim in the
rest of the Chapter is to ﬁnd necessary and sufﬁcient conditions for reality, of as
practical a nature as possible.

424–I
13–6
Deﬁnition 13.3 Suppose α is a simple representation over C. Then we say that α
is strictly complex if χα is not real; and we say that α is quaternionic if χα is real,
but α itself is not real;
Thus the simple representations of G over C fall into 3 mutually exclusive
classes:
R real: α = Cβ
C strictly complex: χα not real
H quaternionic: χα real but α not real
Lemma 13.3 Suppose α is a simple representation over C. Then
1. If α is real, Rα = 2β, where β is a simple representation over R;
2. if α is strictly complex or quaternionic, Rα = β is a simple representation
over R.
In particular, if χα is not real then Rα must be simple.
Proof ▶If α is real, say α = Cβ, then by Proposition 1
Rα = RCβ = 2β.
Conversely, suppose Rα splits, say
Rα = β + β′.
Then by Proposition 1,
α + α∗= CRα = Cβ + Cβ′.
But since α and α∗are simple, this implies (by the unique factorisation theorem)
that
α = Cβ or α = Cβ′.
In either case α is real.
◀
This gives a (not very practical) way of distinguishing between the 3 classes:
R: α real ⇐⇒χα real and Rα splits
C: α quaternionic ⇐⇒χα real and Rα simple
H: α strictly complex ⇐⇒χα not real (=⇒Rα simple)

424–I
13–7
The next Proposition shows that the classiﬁcation of simple representations
over C into 3 classes leads to a similar classiﬁcation of simple representations
over R.
Proposition 13.2 Suppose β is a simple representation over R. Then there are 3
(mutually exclusive) possibilities:
R: Cβ = α is simple
C: Cβ = α + α∗,
H: Cβ = 2α, with α simple with α (and α∗) simple, and α ̸= α∗
In case (R), α is real and
I(β, β) = 1.
In case (C), α is strictly complex and
I(β, β) = 2.
In case (H), α is quaternionic and
I(β, β) = 4.
Proof ▶Since
RCβ = 2β,
Cβ cannot split into more than 2 parts. Thus there are 3 possibilities:
1. Cβ = α is simple
2. Cβ = 2α, with α simple
3. Cβ = α + α′, with α, α′ simple and α ̸= α′
Since
I(β, β) = I(Cβ, Cβ)
by Proposition 1, the values of I(β, β) in the 3 cases follow at once. Thus it only
remains to show that α is in the class speciﬁed in each case, and that α′ = α∗in
case (3).
In case (1), α is real by deﬁnition.
In case (2),
2χα(g) = χ2α(g) = χβ(g)

424–I
13–8
is real for all g ∈G. Hence χα(g) is real, and so α is either real or quaternionic.
If α were real, say α = Cβ′, we should have
Cβ = 2Cβ′
which would imply that
β = 2β′
by Proposition 2. Hence α is quaternionic.
In case (3)
2β = RCβ = Rα + Rα′.
Hence
Rα = β = Rα′.
But then
α + α′ = Cβ = α + α∗.
Hence
α′ = α∗.
Finally, since α∗= α′ ̸= α, α is strictly complex.
◀
Proposition 5 gives a practical criterion for determining which of the 3 classes
a simple representation β over R belongs to, namely by computing I(β, β) from
χβ. Unfortunately, the question that more often arises is: which class does a given
simple representation α over C belong to? and this is more difﬁcult to determine.
Lemma 13.4 Suppose α is a simple representation of G over C in V . Then
R: if α is real,there exists an invariant symmetric (quadratic) form on V , unique
up to a scalar multiple—but there is no invariant skew-symmetric form on
V ;
C: if α is complex, there is no invariant bilinear form on V .
H: if α is quaternionic, there exists an invariant skew-symmetric form on V ,
unique up to a scalar multiple—but there is no invariant symmetric form
on V ;
Proof ▶A bilinear form on V is a linear map
V ⊗V →C,
ie an element of
(V ⊗V )∗= V ∗⊗V ∗.

424–I
13–9
Thus the space of bilinear maps carries the representation (α∗)2 of G. Hence the
invariant bilinear maps form a space of dimension
I(1, (α∗)2) = I(1, α∗α∗) = I(α, α∗)
Since α and α∗are simple, this is 0 or 1 according as α = α∗or not, ie according
as α is either real or quaternionic, or strictly complex. In other words, if α is
complex there is no invariant bilinear form; while if α is real or quaternionic there
is an invariant bilinear form on V , say F(u, v), unique up to a scalar multiple.
Now any bilinear form can be split into a symmetric (or quadratic) part and a
skew-symmetric part; say
F(u, v) = Q(u, v) + S(u, v),
where
Q(u, v) = 1
2 (F(u, v) + F(v, u)) , S(u, v) = 1
2 (F(u, v) −F(v, u))
But it is easy to see that if F is invariant then so are Q and S. Since F is the only
invariant bilinear form on V , it follows that either
F = Q or F = S,
ie F is either symmetric or skew-symmetric. It remains to show that the former
occurs in the real case, the latter in the quaternionic case.
Suppose α is real, say α = Cβ, where β is a representation in the real vector
space U. We know that U carries an invariant symmetric form (in fact a positive-
deﬁnite one), say Q(u, u′). But this deﬁnes an invariant symmetric form CQ on
V = CU by extension of scalars. So if α is real, V carries an invariant symmetric
form.
Finally, suppose α is quaternionic. Then V carries either a symmetric or a
skew-symmetric invariant form (but not both). Suppose the former; say Q(v, v′)
is invariant. By Proposition 3, β = Rα is simple. Hence there exists a real
invariant positive-deﬁnite symmetric form on RV ; and this is the only invariant
symmetric form on RV , up to a scalar multiple. But the real part of Q(v, v′) is
also an invariant form on RV ; and it is certainly not positive-deﬁnite, since
ℜQ(iv, iv) = −ℜQ(v, v).
This contradiction shows that V cannot carry an invariant symmetric form. We
conclude that it must carry an invariant skew-symmetric form.
◀
We deduce from this Proposition the following more practical criterion for
reality.

424–I
13–10
Proposition 13.3 Suppose α is a simple representation over C. Then
1
|G|
X
g∈G
χα(g2) =





1
if α is real
0
if α is strictly complex
−1
if α is quaternionic
Proof ▶Every bilinear form has a unique expression as the sum of its symmetric
and skew-symmetric parts. In other words, the space of bilinear forms is the direct
sum of the spaces of symmetric and of skew-symmetric forms; say
V ∗⊗V ∗= V Q ⊕V S.
Moreover, each of these subspaces is stable under G; so the representation (α∗)2
in the space of bilinear forms splits in the same way; say
(α∗)2 = αQ + αS,
where αQ is the representation of G in the space V Q of symmetric forms on V,
and αS is the representation in the space V S of skew-symmetric forms.
Now the dimensions of the spaces of invariant symmetric and skew-symmetric
space are
I(1, αQ) and I(1, αS),
respectively. Thus Proposition 6 can be reworded as follows:
R: If α is real then
I(1, αQ) = 1 and I(1, αS) = 0.
C: If α is complex then
I(1, αQ) = 0 and I(1, αS) = 0.
H: If α is quaternionic then
I(1, αQ) = 0 and I(1, αS) = 1.
Thus all (!) we have to do is to compute these 2 intertwining numbers. In fact
it sufﬁces to ﬁnd one of them, since
I(1, αQ) + I(1, αS) = I(1, (α∗)2) = I(α, α∗)
which we already know to be 1 if α is real or quaternionic, and 0 if α is complex.

424–I
13–11
To compute I(1, αQ), choose a basis e1, ..., en for V ; and let the corresponding
coordinates be x1, ..., xn. Then the n(n + 1)/2 quadratic forms
x2
i
(1 ≤i ≤n),
2xixj
(1 ≤i < j ≤n)
form a basis for V Q. Let gij denote the matrix deﬁned by α(g). Thus if v =
(x1, ..., xn) ∈V , then the coordinates of gv are
(gv)i =
X
j
gijxj.
Hence
g(x2
i ) =
X
j,k
gijxjgikxk.
In particular, the coefﬁcient of x2
i in this (which is all we need to know for the
trace) is g2
ii. Similarly, the coefﬁcient of 2xixj in g(2xixj) is
giigjj + gijgji.
We conclude that
χαQ(g) =
X
i
g2
ii +
X
i,j:i<j
(giigjj + gijgji).
But
χα(g) =
X
i
gii, χα(g2) =
X
i,j
gijgji.
Thus
χαQ(g) = 1
2

χα(g)2 + χα(g2)

.
Since
I(1, αQ) = 1
|G|
X
g∈G
χα[2](g),
it follows that
2I(1, αQ) = 1
|G|
X
g∈G

χα(g))2 + χα(g2)

.
But
1
|G|
X
g
χα(g)2 = I(α, α∗).
Thus
2I(1, αQ) = I(α, α∗) + 1
|G|
X
g
χα(g2).
The result follows, since I(α, α∗) = 1 in the real and quaternionic cases, and 0 in
the complex case.
◀

Appendix A
Linear algebra over the quaternions
The basic ideas of linear algebra carry over with the quaternions H (or in-
deed any skew-ﬁeld) in place of R or C.
A vector space W over H is an abelian group (written additively) together
with an operation
H × W →W : (q, w) 7→qw,
which we cann scalar multiplication, such that
1. q(w1 + w2) = qw1 + qw2,
2. (q1 + q2)w = q1w + q2w,
3. (q1q2)w = q1(q2w),
4. 1w = w.
The notions of basis and dimension (together with linear independence and
spanning) carry over without change. Thus e1, . . . , en are said to be linearly inde-
pendent if
q1e1 + · · · + qnen = 0 =⇒q1 = · · · = qn = 0.
424–I
1–0


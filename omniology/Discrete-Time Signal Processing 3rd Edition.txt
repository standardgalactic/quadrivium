
r  
1 t 
THIRD EDITION 
DISCRETE-TIME  
SIGNAL  
PROCESSING  
ALAN V. OPPENHEIM 
MASSACIWSETTS INSTITUTE OF TECHNOLOGY 
RONALD W. SCHAFER 
HEWLETT-PACKARD LABORATORIES 
PEARSON 
Upper Saddle River . Boston . Columbus . San Francisco . New York 
Indianapolis . London . Toronto . Sydney . Singapore . Tokyo . Montreal 
Dubai . Madrid· Hong Kong· Mexico City· Munich· Paris· Amsterdam· Cape Town 

Vice President and Editorial Director, ECS: Marcia 1. Horton 
Acquisition Editor: Andrew Gilfillan 
Editorial Assistant: William Opa/uch 
Director of Team-Based Project Management: Vince O'Brien 
Senior Marketing Manager: Tim Galligan 
Marketing Assistant: Mack Patterson 
Senior Managing Editor: Scott Disanno 
Production Project Manager: Clare Romeo 
Senior Operations Specialist: Alan Fischer 
Operations Specialist: Lisa McDowell 
Art Director: Kristine Carney 
Cover Designer: Kristine Carney 
Cover Photo: Librado Romero/New York Times-Maps and Graphics 
Manager, Cover Photo Permissions: Karen Sanatar 
Composition: PreTeX Inc.: Paul Mailhot 
Printer/Binder: Courier Westford 
Typeface: 10/12 TimesTen 
Credits and acknowledgments borrowed from other sources and reproduced, with permission, in this textbook appear on the 
appropriate page within the text. 
LabVIEW is a registered trademark of National Instruments, 11500 N Mopac Expwy, Austin, TX 78759-3504. 
Mathematica is a registered trademark of Wolfram Research, Inc., 100 Trade Center Drive, Champaign, IL 61820-7237. 
MATLAB and Simulink are registered trademarks of The Math Works, 3 Apple Hill Drive, Natick, MA 01760-2098. 
© 2010,1999,1989 by Pearson Higher Education, Inc., Upper Saddle River, NJ 07458. All rights reserved. Manufactured in the 
United States of America. This publication is protected by Copyright and permissions should be obtained from the publisher prior 
to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, 
photocopying, recording, or likewise. To obtain permission{s) to use materials from this work, please submit a written request to 
Pearson Higher Education, Permissions Department, One Lake Street. Upper Saddle River, NJ 07458. 
Many of the designations by manufacturers and seller to distinguish their products are claimed as trademarks. Where those 
designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed in initial 
caps or all caps. 
'Ine author and publisher of this book have used their best efforts in preparing this book. These efforts include the development, 
research, and testing of theories and programs to determine their effectiveness. The author and publisher make no warranty of any 
kind, expressed or implied, with regard to these programs or the documentation contained in this book. 'The author and publisher 
shall not be liable in any event for incidental or consequential damages with, or arising out of, the furnishing, performance, or use of 
these programs. 
Pearson Education Ltd., London 
Pearson Education Singapore. Pte. Ltd. 
Pearson Education Canada, Inc., Toronto 
Pearson Education-Japan, Tokyo 
Pearson Education Australia Pty. Ltd., Sydney 
Pearson Education North Asia Ltd., Hong Kong 
Pearson Education de Mexico, S.A. de c.v. 
Pearson Education Malaysia, Pte. Ltd. 
Pearson Education, Inc .. Upper Saddle River, New Jersey 
Prentice Hall 
is an imprint of 
10987654321 
PEARSON 
ISBN-13: 978-0-13-198842-2 
www.pearsonhighered.com 
ISBN-lO: 
0-13-198842-5 

To Phyllis, Justine, and Jason 
To Dorothy, Bill, Tricia. Ken. and Kate 
and in memory ofJohn 


CONTENTS  
Preface 
xv  
The Companion Website 
xxii 
The Cover 
xxv 
Acknowledgments 
xxvi 
1 Introduction 
1  
2 Discrete-Time Signals and Systems 
9  
2.0 
Introduction...... 
9  
2.1 
Discrete-Time Signals . . . . 
10  
2.2 
Discre.te-Time Systems ... 
17  
2.2.1 
Memoryless Systems 
18  
2.2.2 
Linear Systems . . . . 
19  
2.2.3 
Time-Invariant Systems. 
20  
2.2.4 
Causality 
22  
2.2.5 Stability.......... 
22  
2.3 
LTI Systems. . . . . . . . . . . . 
23  
2.4 
Properties of Linear Time-Invariant Systems 
30  
2.5 
Linear Constant-Coefficient Difference Equations. 
35  
2.6 
Frequency-Domain Representation ofDiscrete-Time Signals and Systems 40  
2.6.1 
Eigenfunctions for Linear Time-Invariant Systems 
40  
2.6.2 
Suddenly Applied Complex Exponential Inputs 
46  
2.7 
Representation of Sequences by Fourier Transforms. 
48  
2.8 
Symmetry Properties of the Fourier Transform 
54  
2.9 
Fourier Transform Theorems . . . . . . . . . . . . . . 
58  
2.9.1 
Linearity of the Fourier Transform ...... 
59  
2.9.2 
Time Shifting and Frequency Shifting Theorem 
59  
2.9.3 
Time Reversal Theorem ............. 
59  
v 

vi 
Contents 
2.9.4 
Differentiation in Frequency Theorem  
59  
2.9.5 
Parseval's Theorem .......... .  
60  
2.9.6 
The Convolution Theorem ...... .  
60  
2.9.7 
The Modulation or Windowing Theorem  
61  
2.10  Discrete-Time Random Signals. 
64  
2.11  Summary 
70  
Problems ............ . 
70  
3 The z-Transform  
99  
3.0  Introduction.................. 
99  
3.1  
z-Transform . . . . . . . . . . . . . . . . . . . 
99  
3.2  
Properties of the ROC for the z-Transform 
110  
3.3  
The Inverse z-Transform. . . . . . 
115  
3.3.1 
Inspection Method ....  
116  
3.3.2 
Partial Fraction Expansion  
116  
3.3.3 
Power Series Expansion.  
122  
3.4  
z-Transform Properties 
124  
3.4.1 Linearity.........  
124  
3.4.2 
Time Shifting . . . . . . .  
125  
3.4.3 
Multiplication by an Exponential Sequence  
126  
3.4.4 
Differentiation of X (z) 
. . . . . . . .  
127  
3.4.5 
Conjugation of a Complex Sequence  
129  
3.4.6 
Time Reversal . . . . . . . . . . . . .  
129  
3.4.7 
Convolution of Sequences ......  
130  
3.4.8 
Summary of Some z-Transform Properties .  
131  
3.5  
z-Transforms and LTI Systems 
131  
3.6  
The Unilateral z-Transform . 
135  
3.7  
Summary 
137  
Problems . . . . . . . . . . . 
138  
4 Sampling of Continuous-TIme Signals  
153  
4.0  
Introduction.................... 
153  
4.1  
Periodic Sampling . . . . . . . . . . . . . . . . . 
153  
4.2  
Frequency-Domain Representation of Sampling 
156  
4.3  
Reconstruction of a Bandlimited Signal from Its Samples 
163  
4.4  
Discrete-Time Processing of Continuous-Time Signals. . 
167  
4.4.1 
Discrete-Time LTI Processing of Continuous-Time Signals. 
168  
4.4.2 
Impulse Invariance .. . . . . . . . . . . . . . . . . . . 
173  
4.5  
Continuous-Time Processing of Discrete-Time Signals. . . . . 
175  
4.6  
Changing the Sampling Rate Using Discrete-Time Processing 
179  
4.6.1 
Sampling Rate Reduction by an Integer Factor .. 
180  
4.6.2 
Increasing the Sampling Rate by an Integer Factor . 
184  
4.6.3 
Simple and Practical Interpolation Filters. . . . . . . 
187  
4.6.4 
Changing the Sampling Rate by a Noninteger Factor 
190  
4.7  
Multirate Signal Processing . . . . . . . . . . . . . . . . . . . 
194  
4.7.1 
Interchange of Filtering with Compressor/Expander 
194  
4.7.2 
Multistage Decimation and Interpolation. . . . . . . 
195  

Contents  
vii  
4.7.3  Polyphase Decompositions . . . . . . . . . . . . . . . 
197  
4.7.4  Polyphase Implementation of Decimation Filters .. 
199  
4.7.5  Polyphase Implementation of Interpolation Filters . 
200  
4.7.6  Multirate Filter Banks. . . . . 
201  
4.8  
Digital Processing of Analog Signals . 
205  
4.8.1  Prefiltering to Avoid Aliasing 
206  
4.8.2  AID Conversion . . . . . . . . 
209  
4.8.3  Analysis of Ql;lantization Errors 
214  
4.8.4  DIA Conversion . . . . . . . . . 
221  
4.9  
Oversampling and Noise Shaping in AID and D/A Conversion. 
224  
4.9.1  
Overs amp led AID Conversion with Direct Quantization 
225  
4.9.2  Oversampled AID Conversion with Noise Shaping . . 
229  
4.9.3  Oversampling and Noise Shaping in DIA Conversion . 
234  
4.10  Summary 
236  
Problems ... . . . . . . . . . . . . . . . . . . . 
237  
5 Transform Analysis of Linear Time-Invariant Systems  
274  
5.0  Introduction.................... 
274  
5.1  
The Frequency Response of LTI Systems . . . . 
275  
5.1.1  Frequency Response Phase and Group Delay 
275  
5.1.2  Illustration of Effects of Group Delay and Attenuation. 
278  
5.2  
System Functions-Linear Constant-Coefficient Difference Equations 283  
5.2.1  
Stability and Causality ............... 
285  
5.2.2  Inverse Systems . . . . . . . . . . . . . . . . . . . 
286  
5.2.3  Impulse Response for Rational System Functions 
288  
5.3  
Frequency Response for Rational System Functions . 
290  
5.3.1  Frequency Response of 1 st-Order Systems 
292  
5.3.2  Examples with Multiple Poles and Zeros 
296  
5.4  
Relationship between Magnitude and Phase 
301  
5.5  
All-Pass Systems . . . . . . . . . . . . . . . . . . 
305  
5.6  
Minimum-Phase Systems. . . . . . . . . . . . . . 
311  
5.6.1  
Minimum-Phase and All-Pass Decomposition 
311  
5.6.2  Frequency-Response Compensation of Non-Minimum-Phase  
Systems. . . . . . . . . . . . . . . . . . . 
313  
5.6.3  Properties of Minimum-Phase Systems . 
318  
5.7  
Linear Systems with Generalized Linear Phase. 
322  
5.7.1  
Systems with Linear Phase . . . . . . . . 
322  
5.7.2  Generalized Linear Phase ........ 
326  
5.7.3  Causal Generalized Linear-Phase Systems 
328  
5.7.4  Relation of FIR Linear-Phase Systems to Minimum-Phase  
Systems 
338  
5.8  
Summary 
340  
Problems . . . 
341  

viii 
Contents  
6 Structures for Discrete-TIme Systems 
374  
6.0  
Introduction ......... . 
374  
6.1  
Block Diagram Representation of Linear Constant-Coefficient  
Difference Equations .... . . . . 
375  
6.2  
Signal Flow Graph Representation. 
382  
6.3  
Basic Structures for IIR Systems 
388  
6.3.1 
Direct Forms.  
388  
6.3.2 
Cascade Form . . . . . .  
390  
6.3.3 
Parallel Form. . . . . . .  
393  
6.3.4 
Feedback in IIR Systems  
395  
6.4 
Transposed Forms . . . . . . . .  
397  
6.5 
Basic Network Structures for FIR Systems  
401  
6.5.1 
Direct Form ... . . . . . . . . . .  
401  
6.5.2 
Cascade Form . . . . . . . . . . . .  
402  
6.5.3 
Structures for Linear-Phase FIR Systems  
403  
6.6 
Lattice Filters . . . . . . . . . . . . .  
405  
6.6.1 
FIR Lattice Filters . . . . . . . . .  
406  
6.6.2 
All-Pole Lattice Structure. . . . .  
412  
6.6.3 
Generalization of Lattice Systems  
415  
6.7 
Overview of Finite-Precision Numerical Effects  
415  
6.7.1 
Number Representations . . . . . . . .  
415  
6.7.2 
Quantization in Implementing Systems .  
419  
6.8 
The Effects of Coefficient Quantization . . . . .  
421  
6.8.1 
Effects of Coefficient Quantization in IIR Systems 
422  
6.8.2 
Example of Coefficient Quantization in an Elliptic Filter 
423  
6.8.3 
Poles of Quantized 2nd -Order Sections . . . . . . . . 
427  
6.8.4 
Effects of Coefficient Quantization in FIR Systems . 
429  
6.8.5 
Example of Quantization of an Optimum FIR Filter 
431  
6.8.6 
Maintaining Linear Phase. . . . . . . . . . .  
434  
6.9 
Effects of Round-off Noise in Digital Filters ......... 
436  
6.9.1 
Analysis of the Direct Form IIR Structures. . . . . . 
436  
6.9.2 
Scaling in Fixed-Point Implementations of IIR Systems. 
445  
6.9.3 
Example of Analysis of a Cascade IIR Structure. . . . 
448  
6.9.4 
Analysis of Direct-Form FIR Systems. . . . . . . . . . 
453  
6.9.5 
Floating-Point Realizations of Discrete-Time Systems. 
458  
6.10  Zero-Input Limit Cycles in Fixed-Point Realizations of IIR  
Digital Filters . . . . . . . . . . . . . . . . . . . . . . . . . . 
459  
6.10.1 Limit Cycles Owing to Round-off and Truncation . 
459  
6.10.2 Limit Cycles Owing to Overflow .  
462  
6.10.3 Avoiding Limit Cycles. .  
463  
6.11  Summary 
463  
Problems .. . . . . . . . . . . . 
464  
7 Filter Design Techniques  
493  
7.0 
Introduction '"  
493  
7.1 
Filter Specifications  
494  

ix 
Contents 
7.2 
Design of Discrete-Time IIR Filters from Continuous-Time Filters. 
496  
7.2.1  Filter Design by Impulse Invariance . . . . . . . . . 
497  
7.2.2  Bilinear Transformation. . . . . . . . . . . . . . . . 
504  
7.3 
Discrete-Time Butterworth, Chebyshev and Elliptic Filters 
508  
7.3.1  
Examples of IIR Filter Design . . . . . . . . . 
509  
7.4 
Frequency Transformations of Lowpass IIR Filters.  
526  
7.5 
Design of FIR Filters by Windowing. . . . . . . . .  
533  
7.5.1  
Properties of Commonly Used Windows . . 
535  
7.5.2  Incorporation of Generalized Linear Phase. 
538  
7.5.3  The Kaiser Window Filter Design Method . 
541  
7.6 
Examples of FIR Filter Design by the Kaiser Window Method 
545  
7.6.1  Lowpass Filter . . . . . . . . . 
545  
7.6.2  Highpass Filter. . . . . . . . . . . 
547  
7.6.3  Discrete-Time Differentiators . . 
550  
7.7 
Optimum Approximations of FIR Filters  
554  
7.7.1  
Optimal Type I Lowpass Filters .. 
559  
7.7.2  Optimal Type II Lowpass Filters. . 
565  
7.7.3  The Parks-McClellan Algorithm. . 
566  
7.7.4  Characteristics of Optimum FIR Filters. 
568  
7.8 
Examples of FIR Equiripple Approximation  
570  
7.8.1  
Lowpass Filter . . . . . . . . . . . . . 
570  
7.8.2  Compensation for Zero-Order Hold 
571  
7.8.3  Bandpass Filter ............ 
576  
7.9  
Comments on IIR and FIR Discrete-Time Filters 
578  
7.10  Design of an Upsampling Filter. 
579  
7.11  Summary 
582  
Problems .... . . . . . 
582  
8 The Discrete Fourier Transform  
623  
8.0 
Introduction ...................... ........ 
623  
8.1 
Representation of Periodic Sequences: The Discrete Fourier Series 
624  
8.2 
Properties of the DFS . . .  
628  
8.2.1  Linearity...... 
629  
8.2.2  Shift of a Sequence 
629  
8.2.3  Duality....... 
629  
8.2.4  Symmetry Properties 
630  
8.2.5  Periodic Convolution 
630  
8.2.6  Summary of Properties of the DFS Representation of Periodic  
Sequences .............. 
633  
8.3 
The Fourier Transform of Periodic Signals .......  
633  
8.4 
Sampling the Fourier Transform . . . . . . . . . . . . .  
638  
8.5 
Fourier Representation of Finite-Duration Sequences .  
642  
8.6 
Properties of the DFT . . . . . . . .  
647  
8.6.1  Linearity........... 
647  
8.6.2  Circular Shift of a Sequence 
648  
8.6.3  Duality........ 
650  
8.6.4  Symmetry Properties . . . . 
653  

x 
Contents 
8.6.5 
Circular Convolution . . . . . . . . .  
654 
8.6.6 
Summary of Properties of the DFf .  
659 
8.7  
Linear Convolution Using the DFf . . . . . 
660 
8.7.1 
Linear Convolution of Two Finite-Length Sequences 
661 
8.7.2 
Circular Convolution as Linear Convolution with Aliasing . 
661 
8.7.3 
Implementing Linear Time-Invariant Systems Using the DFf 
667 
8.8  
The Discrete Cosine Transform (DCT) ... 
673 
8.8.1 
Definitions of the DCT . . . . . . . . . . . . . . . 
673 
8.8.2 
Definition of the DCT-1 and DCT-2 . . . . . . . . 
675 
8.8.3 
Relationship between the DFf and the DCT-1 . . 
676 
8.8.4 
Relationship between the DFf and the DCT-2. . 
678 
8.8.5 
Energy Compaction Property of the DCT-2 .  
679 
8.8.6 
Applications of the DCT .  
682 
8.9  
Summary 
683 
Problems . . . . . . . . . . . . . . 
684 
9 
Computation of the Discrete Fourier Transform  
716 
9.0  
Introduction ....................... . 
716  
9.1  
Direct Computation of the Discrete Fourier Transform 
718 
9.1.1 
Direct Evaluation of the Definition of the DFf  
718 
9.1.2 
The Goertzel Algorithm ......... .  
719  
9.1.3 
Exploiting both Symmetry and Periodicity  
722 
9.2  
Decimation-in-Time FFf Algorithms ...... . 
723 
9.2.1 
Generalization and Programming the FFT  
731 
9.2.2 
In-Place Computations ...... .  
731 
9.2.3 
Alternative Forms ......... .  
734  
9.3  
Decimation-in-Frequency FFf Algorithms 
737 
9.3.1 
In-Place Computation.  
741 
9.3.2 
Alternative Forms .  
741 
9.4  
Practical Considerations. 
743 
9.4.1 
Indexing .....  
743 
9.4.2 
Coefficients....  
745 
9.5  
More General FFf Algorithms. 
745 
9.5.1 
Algorithms for Composite Values of N  
746 
9.5.2 
Optimized FFf Algorithms. . . . . . .  
748 
9.6  
Implementation of the DFf Using Convolution . 
748 
9.6.1 
Overview of the Winograd Fourier Transform Algorithm. 
749 
9.6.2 
The Chirp Transform Algorithm .  
749 
9.7  
Effects of Finite Register Length. 
754 
9.8  
Summary 
762 
Problems . . . . . . . . . . . . . . 
763 
10 Fourier Analysis of Signals Using the Discrete Fourier Transform 
792 
10.0 
Introduction ................ . 
792 
10.1 
Fourier Analysis of Signals Using the DFf ......... . 
793 

Contents  
xi  
10.2  OFT Analysis of Sinusoidal Signals 
797  
10.2.1  The Effect of Windowing . . 
797  
10.2.2  Properties of the Windows . 
800  
10.2.3  The Effect of Spectral Sampling 
801  
10.3  The Time-Dependent Fourier Transform 
811  
10.3.1  Invertibility of XIn,) . . . . . . . . 
815  
10.3.2  Filter Bank Interpretation of XIn,) 
816  
10.3.3  The Effect of the Window ..... 
817  
10.3.4  Sampling in Time and Frequency . 
819  
10.3.5  The Overlap-Add Method of Reconstruction 
822  
10.3.6  Signal Processing Based on the Time-Dependent Fourier  
Transform ........................... 
825  
10.3.7  Filter Bank Interpretation of the Time-Dependent Fourier  
Transform ........................ 
826  
10.4  Examples of Fourier Analysis of Nonstationary Signals . . . 
829  
10.4.1  Time-Dependent Fourier Analysis of Speech Signals 
830  
10.4.2  Time-Dependent Fourier Analysis of Radar Signals. 
834  
10.5  Fourier Analysis of Stationary Random Signals: the Periodogram 
836  
10.5.1  The Periodogram ....... 
837  
10.5.2  Properties of the Periodogram . . . . . . . . . . . . . . . 
839  
10.5.3  Periodogram Averaging . . . . . . . . . . . . . . . . . . . 
843  
10.5.4  Computation of Average Periodograms Using the OFT . 
845  
10.5.5  An Example of Periodogram Analysis .......... 
845  
10.6  Spectrum Analysis of Random Signals. . . . . . . . . . . . . . . 
849  
10.6.1  Computing Correlation and Power Spectrum Estimates Using  
the OFT .......................... 
853  
10.6.2  Estimating the Power Spectrum of Quantization Noise 
855  
10.6.3  Estimating the Power Spectrum of Speech 
860  
10.7  Summary 
862  
Problems . . . . . . . . . . . . . . . . . . . . . . . 
864  
11 Parametric Signal Modeling  
890  
11.0 Introduction .............. 
890  
ILl All-Pole Modeling of Signals . . . . . 
891  
11.1.1  Least-Squares Approximation 
892  
11.1.2  Least-Squares Inverse Model. 
892  
11.1.3  Linear Prediction Formulation of All-Pole Modeling 
895  
11.2  Deterministic and Random Signal Models .......... 
896  
11.2.1  All-Pole Modeling of Finite-Energy Deterministic Signals 
896  
11.2.2  Modeling of Random Signals. . . . 
897  
11.2.3  Minimum Mean-Squared Error . . . . . 
898  
11.2.4  Autocorrelation Matching Property . . . 
898  
11.2.5  Determination of the Gain Parameter G 
899  
11.3  Estimation of the Correlation Functions. 
900  
11.3.1  The Autocorrelation Method. 
900  
11.3.2  The Covariance Method 
903  
11.3.3  Comparison of Methods ... 
904  

xii  
Contents 
11.4 Model Order . . . . . . . . . . . . . . . . .  
905  
11.5 
All-Pole Spectrum Analysis . . . . . . . . .  
907  
11.5.1  All-Pole Analysis of Speech Signals 
908  
11.5.2  Pole Locations . . . . . . . . . . . . 
911  
11.5.3  All-Pole Modeling of Sinusoidal Signals. . 
913  
11.6 
Solution of the Autocorrelation Normal Equations  
915  
11.6.1  The Levinson-Durbin Recursion ...... 
916  
11.6.2  Derivation of the Levinson-Durbin Algorithm. 
917  
11.7 
Lattice Filters. . . . . . . . . . . . . . . . .  
920  
11.7.1  Prediction Error Lattice Network . . . . 
921  
11.7.2  All-Pole Model Lattice Network. . . . . 
923  
11.7.3  Direct Computation of the k-Parameters 
925  
11.8  Summary 
926  
Problems .... . . . . . . . . . . . . . . . . . . 
927  
12 Discrete Hilbert Transforms  
942  
12.0 Introduction .............................. 
942  
12.1 
Real- and Imaginary-Part Sufficiency of the Fourier Transform. . 
944  
12.2 
Sufficiency Theorems for Finite-Length Sequences. .  
949  
12.3 
Relationships Between Magnitude and Phase. . . . .  
955  
12.4 
Hilbert 1tansform Relations for Complex Sequences  
956  
12.4.1  Design of Hilbert Transformers ... 
960  
12.4.2  Representation of Bandpass Signals . 
963  
12.4.3  Bandpass Sampling 
966  
12.5  Summary 
969  
Problems .... . . . . . . 
969  
13 Cepstrum Analysis and Homomorphic Deconvolution  
980  
13.0 Introduction ..............  
980  
13.1 
Definition of the Cepstrum . . . . . .  
981  
13.2 Definition of the Complex Cepstrum.  
982  
13.3 
Properties of the Complex Logarithm  
984  
13.4 
Alternative Expressions for the Complex Cepstrum  
985  
13.5 Properties of the Complex Cepstrum ........  
986  
13.5.1  Exponential Sequences . . . . . . . . . . . . 
986  
13.5.2  Minimum-Phase and Maximum-Phase Sequences 
989  
13.5.3  Relationship Between the Real Cepstrum and the Complex  
Cepstrum . . . . . . . . . . . . . . 
990  
13.6 Computation of the Complex Cepstrum . . . . . . . . . . . . . . . . . . 992  
13.6.1  Phase Unwrapping ......................... 993  
13.6.2  Computation of the Complex Cepstrum Using the Logarithmic  
Derivative '" 
. . . . . . . . . . . . . . . . . . . . . . . . . . . 996  
13.6.3  Minimum-Phase Realizations for Minimum-Phase Sequences . 998  
13.6.4  Recursive Computation ofthe Complex Cepstrum for Minimum- 
and Maximum-Phase Sequences . . . . . . . . . . . . . . . 
998  
13.6.5  The Use of Exponential Weighting ............. 
1000  
13.7 
Computation of the Complex Cepstrum Using Polynomial Roots 
100l 

Contents  
xiii  
13.8  Deconvolution Using the Complex Cepstrum. . . . . . . . . . . 
1002  
13.8.1  Minimum-Phase/Allpass Homomorphic Deconvolution 
1003  
13.8.2  Minimum-Phase/Maximum-Phase Homomorphic  
Deconvolution . . . . . . . . . . . . . . . . . . . . . . . . 
1004  
13.9  The Complex Cepstrum for a Simple Multipath Model ..... 
1006  
13.9.1  Computation of the Complex Cepstrum by z-Transform  
Analysis ........................... . 
1009  
13.9.2  Computation of the Cepstrum Using the DFf ..... . 
1013  
13.9.3  Homomorphic Deconvolution for the Multipath Model. 
1016  
13.9.4  Minimum-Phase Decomposition. 
1017  
13.9.5  Generalizations ..... . 
1024  
13.10 Applications to Speech Processing ... .  
1024  
13.10.1 The Speech Model ........ .  
1024  
13.10.2 Example of Homomorphic Deconvolution of Speech 
1028  
13.10.3 Estimating the Parameters of the Speech Model. 
1030  
13.10.4 Applications  
1032  
13.11  Summary 
1032  
Problems ..... . 
1034  
A Random Signals  
1043  
B Continuous-Time Filters  
1056  
C Answers to Selected Basic Problems  
1061  
Bibliography  
1082  
Index  
1091 


PREFACE  
This third edition of Discrete-Time Signal Processing is a descendent of our original 
textbook Digital Signal Processing published in 1975. That very successful text appeared 
at a time when the field was young and just beginning to develop rapidly. At that time 
the topic was taught only at the graduate level and at only a very few schools. Our 1975 
text was designed for such courses. It is still in print and is still used successfully at a 
number of schools in the United States and internationally. 
By the 1980's, the pace of signal processing research, applications and implemen­
tation technology made it clear that digital signal processing (DSP) would realize and 
exceed the potential that had been evident in the 1970's. The burgeoning importance 
of DSP clearly justified a revision and updating of the original text. In organizing that 
revision, it was clear that so many changes had occurred both in the field and in the 
level and style with which the topic was taught, that it was most appropriate to develop 
a new textbook, strongly based on our original text, while keeping the original text in 
print. We titled the new book, published in 1989, Discrete-Time Signal Processing to 
emphasize that most of the DSP theory and design techniques discussed in the text 
apply to discrete-time systems in general, whether analog or digital. 
In developing Discrete-Time Signal Processing we recognized that the basic prin­
ciples of DSP were being commonly taught at the undergraduate level, sometimes even 
as part of a first course on discrete-time linear systems, but more often, at a more ad­
vanced level in third-year, fourth-year, or beginning graduate subjects. Therefore, it 
was appropriate to expand considerably the treatment of such topics as linear systems, 
sampling, multirate signal processing, applications, and spectral analysis. In addition, 
more examples were included to emphasize and illustrate important concepts. Consis­
tent with the importance that we placed on well constructed examples and homework 
problems, that new text contained more than 400 problems. 
xv 

xvi 
Preface 
While the field continued to advance in both theory and applications, the under­
lying fundamentals remained largely the same, albeit with a refinement of emphasis, 
understanding and pedagogy. Consequently, the Second Edition of Discrete-Time Sig­
nal Processing was published in 1999. That new edition was a major revision, with the 
intent of making the subject of discrete-time signal processing even more accessible to 
students and practicing engineers, without compromising on the coverage of what we 
considered to be the essential concepts that define the field. 
This third edition of Discrete-Time Signal Processing is a major revision of our 
Second Edition. The new edition is in response to changes in the way the subject is 
taught and to changes in scope of typical courses at the undergraduate and first-year 
graduate level. It continues the tradition of emphasizing the accessibility of the topics 
to students and practicing engineers and focusing on the fundamental principles with 
broad applicability. A major feature of the new edition is the incorporation and expan­
sion of some of the more advanced topics, the understanding of which are now essential 
in order to work effectively in the field. Every chapter of the second edition has under­
gone significant review and changes, one entirely new chapter has been added, and one 
chapter has been restored and significantly up-dated from the first edition. With this 
third edition, a closely integrated and highly interactive companion web site has been 
developed by Professors Mark Yoder and Wayne Padgett of Rose-Hulman Institute of 
Technology. A more complete discussion of the website is given in the website overview 
section following this Preface. 
As we have continued to teach the subject over the ten years since the second 
edition, we have routinely created new problems for homework assignments and ex­
ams. Consistent with the importance that we have always placed on well constructed 
examples and homework problems, we have selected over 130 of the best of these to be 
included in the third edition, which now contains a total of more than 700 homework 
problems overall. The homework problems from the second edition that do not appear 
in this new edition are available on the companion web site. 
As in the earlier generations of this text, it is assumed that the reader has a 
background of advanced calculus, along with a good understanding of the elements 
of complex numbers and complex variables. A background in linear system theory for 
continuous-time signals, including Laplace and Fourier transforms, as taught in most 
undergraduate electrical and mechanical engineering curricula, remains a basic pre­
requisite. It is also now common in most undergraduate curricula to include an early 
exposure to discrete-time signals and systems, discrete-time Fourier transforms and 
discrete-time processing of continuous-time signals. 
Our experience in teaching discrete-time signal processing at the advanced under­
graduate level and the graduate level confirms that it is essential to begin with a careful 
review of these topics so that students move on to the more advanced topics from a solid 
base of understanding and a familiarity with a consistent notational framework that is 
used throughout the course and the accompanying textbook. Most typically in a first 
exposure to discrete-time signal processing in early undergraduate courses, students 
learn to carry out many of the mathematical manipulations, but it is in revisiting the 
topics that they learn to reason more deeply with the underlying concepts. Therefore in 
this edition we retain the coverage of these fundamentals in the first five chapters, en­
hanced with new examples and expanded discussion. In later sections of some chapters. 

Preface 
xvii 
some topics such as quantization noise are included that assume a basic background in 
random signals. A brief review of the essential background for these sections is included 
in Chapter 2 and in Appendix A. 
An important major change in DSP education that has occurred in the past decade 
or so is the widespread use of sophisticated software packages such as MATLAB, Lab­
VIEW, and Mathematica to provide an interactive, "hands-on" experience for students. 
The accessibility and ease of use of these software packages provide the opportunity 
to connect the concepts and mathematics that are the basis for discrete-time signal 
processing, to applications involving real signals and real-time systems. These software 
packages are well documented, have excellent technical support, and have excellent 
user interfaces. These make them easily accessible to students without becoming a dis­
traction from the goal of developing insight into and intuition about the fundamentals. 
It is now common in many signal processing courses to include projects and exercises to 
be done using one or several of the available software packages. Of course, this needs to 
be done carefully in order to maximize the benefit to student learning by emphasizing 
experimentation with the concepts, parameters, and so on, rather than simple cookbook 
exercises. It is particularly exciting that with one of these powerful software packages 
installed, every student's laptop computer becomes a state-of-the-art laboratory for 
experimenting with discrete-time signal processing concepts and systems. 
As teachers, we have consistently looked for the best way to use computer re­
sources to improve the learning environment for our students. We continue to believe 
in textbooks as the best way to encapsulate knowledge in the most convenient and stable 
form. Textbooks necessarily evolve on a relatively slow time scale. This ensures a certain 
stability and provides the time to sort through developments in the field and to test ways 
of presenting new ideas to students. On the other hand, changes in computer software 
and hardware technology are on a much faster time scale. Software revisions often occur 
semi-annually, and hardware speeds continue to increase yearly. This, together with the 
availability of the world-wide-web, provides the opportunity to more frequently update 
the interactive and experimental components of the learning environment. For these 
reasons, providing separate environments for the basic mathematics and basic concepts 
in the form of the textbook and the hands-on interactive experience primarily through 
the world-wide-web seems to be a natural path. 
With these thoughts in mind, we have created this third edition of Discrete-Time 
Signal Processing, incorporating what we believe to be the fundamental mathematics 
and concepts of discrete-time signal processing and with tight coupling to a companion 
website created by our colleagues Mark Yoder and Wayne Padgett of Rose-Hulman 
Institute of Technology. The website contains a variety of interactive and software re­
sources for learning that both reinforce and expand the impact of the text. This website 
is described in more detail in the introductory section following this Preface. It is de­
signed to be dynamic and continually changing to rapidly incorporate new resources 
developed by the authors of the text and by the website authors. The website will be 
sensitive to the continually changing hardware and software environments that serve 
as the platform for visualization of abstract concepts and experimentation with real 
signal processing problems. We are excited by the virtually limitless potential for this 
companion website environment to significantly improve our ability to teach and our 
students' ability to learn the subject of discrete-time signal processing. 

xviii 
Preface 
The material in this book is organized in a way that provides considerable flexi­
bility in its use at both the undergraduate and graduate leveL A typical one-semester 
undergraduate elective might cover in depth Chapter 2, Sections 2.0-2.9; Chapter 3; 
Chapter 4, Sections 4.0-4.6; Chapter 5, Sections 5.0-5.3; Chapter 6, Sections 6.0-6.5; 
and Chapter 7, Sections 7.0-7.3 and a brief overview of Sections 7.4-7.6. If students 
have studied discrete-time signals and systems in a previous signals and systems course, 
it would be possible to move more quickly through the material of Chapters 2, 3, and 4, 
thus freeing time for covering Chapter 8. A first-year graduate course or senior elective 
could augment the above topics with the remaining topics in Chapter 5, a discussion of 
multirate signal processing (Section 4.7), an exposure to some of the quantization issues 
introduced in Section 4.8, and perhaps an introduction to noise shaping in AID and 01A 
converters as discussed in Section 4.9. A first-year graduate course should also include 
exposure to some of the quantization issues addressed in Sections 6.6-6.9, a discussion 
of optimal FIR filters as incorporated in Sections 7.7-7.9, and a thorough treatment of 
the discrete Fourier transform (Chapter 8) and its computation using the FFf (Chapter 
9). The discussion of the DFf can be effectively augmented with many of the examples 
in Chapter 10. In a two-semester graduate course, the entire text including the new chap­
ters on parametric signal modeling (Chapter 11) and the cepstrum (Chapter 13) can be 
covered along with a number of additional advanced topics. In all cases, the homework 
problems at the end of each chapter can be worked with or without the aid of a com­
puter, and problems and projects from the website can be assigned to strengthen the 
connection between theory and computer implementation of signal processing systems. 
We conclude this Preface with a summary of chapter contents highlighting the 
significant changes in the third edition. 
. 
In Chapter 2, we introduce the basic class of discrete-time signals and systems and 
define basic system properties such as linearity, time invariance, stability, and causality. 
The primary focus of the book is on linear time-invariant systems because of the rich 
set of tools available for designing and analyzing this class of systems. In particular, in 
Chapter 2 we develop the time-domain representation of linear time-invariant systems 
through the convolution sum and discuss the class oflinear time-invariant systems repre­
sented by linear constant-coefficient difference equations. In Chapter 6, we develop this 
class of systems in considerably more detail. Also in Chapter 2 we discuss the frequency­
domain representation of discrete-time signals and systems through the discrete-time 
Fourier transform. The primary focus in Chapter 2 is on the representation of sequences 
in terms of the discrete-time Fourier transform, i.e., as a linear combination of complex 
exponentials, and the development of the basic properties of the discrete-time Fourier 
transform. 
In Chapter 3, we develop the z-transform as a generalization of the Fourier trans­
form. This chapter focuses on developing the basic theorems and properties of the 
z-transform and the development of the partial fraction expansion method for the in­
verse transform operation. A new section on the unilateral z-transform has been added 
in this edition. In Chapter 5, the results developed in Chapters 2 and 3 are used exten­
sively in a detailed discussion of the representation and analysis of linear time-invariant 
systems. While the material in Chapters 2 and 3 might be review for many students, most 
introductory signals and systems courses will not contain either the depth or breadth 
of coverage of these chapters. Furthermore, these chapters establish notation that will 

ill 
Preface 
xix 
be used throughout the text. Thus, we recommend that Chapters 2 and 3 be studied as 
carefully as is necessary for students to feel confident of their grasp of the fundamentals 
of discrete-time signals and systems. 
Chapter 4 is a detailed discussion of the relationship between continuous-time and 
discrete-time signals when the discrete-time signals are obtained through periodic sam­
pling of continuous-time signals. This includes a development of the Nyquist sampling 
theorem. In addition, we discuss upsampling and downsampling of discrete-time sig­
nals, as used, for example, in multirate signal processing systems and for sampling rate 
conversion. The chapter concludes with a discussion of some of the practical issues en­
countered in conversion from continuous time to discrete time including prefiltering to 
avoid aliasing, modeling the effects of amplitude quantization when the discrete-time 
signals are represented digitally, and the use of oversampling in simplifying the AID 
and DIA conversion processes. This third edition includes new examples of quantiza­
tion noise simulations, a new discussion of interpolation filters derived from splines, and 
new discussions of multi-stage interpolation and two-channel multi-rate filter banks. 
In Chapter 5 we apply the concepts developed in the previous chapters to a de­
tailed study of the properties of linear time-invariant systems. We define the class of 
ideal, frequency-selective filters and develop the system function and pole-zero rep­
resentation for systems described by linear constant-coefficient difference equations, 
a class of systems whose implementation is considered in detail in Chapter 6. Also in 
Chapter 5, we define and discuss group delay, phase response and phase distortion, 
and the relationships between the magnitude response and the phase response of sys­
tems, including a discussion of minimum-phase, allpass, and generalized linear phase 
systems. Third edition changes include a new example of the effects of group delay and 
attenuation, which is also available on the website for interactive experimentation. 
In Chapter 6, we focus specifically on systems described by linear constant­
coefficient difference equations and develop their representation in terms of block dia­
grams and linear signal flow graphs. Much of this chapter is concerned with developing 
a variety of the important system structures and comparing some of their properties. 
The importance of this discussion and the variety of filter structures relate to the fact 
that in a practical implementation of a discrete-time system, the effects of coefficient 
inaccuracies and arithmetic error can be very dependent on the specific structure used. 
While these basic issues are similar for digital and discrete-time analog implementations, 
we illustrate them in this chapter in the context of a digital implementation through a 
discussion of the effects of coefficient quantization and arithmetic roundoff noise for 
digital filters. A new section provides a detailed discussion of FIR and IIR lattice fil­
ters for implementing linear constant-coefficient difference equations. As discussed in 
Chapter 6 and later in Chapter 11, this class of filter structures has become extremely 
important in many applications because of their desirable properties. It is common in 
discussions oflattice filters in many texts and papers to tie their importance intimately to 
linear prediction analysis and signal modeling. However the importance of using lattice 
implementations ofFIR and IIR filters is independent of how the difference equation to 
be implemented is obtained. For example the difference equation might have resulted 
from the use of filter design techniques as discussed in Chapter 7, the use of parametric 
signal modeling as discussed in Chapter 11 or any of a variety of other ways in which a 
difference equation to be implemented arises. 

xx 
Preface 
While Chapter 6 is concerned with the representation and implementation of 
linear constant-coefficient difference equations, Chapter 7 is a discussion of procedures 
for obtaining the coefficients of this class of difference equations to approximate a 
desired system response. The design techniques separate into those used for infinite 
impulse response (IIR) filters and those used for finite impulse response (FIR) filters. 
New examples of IIR filter design provide added insight into the properties of the 
different approximation methods. A new example on filter design for interpolation 
provides a framework for comparing IIR and FIR filters in a practical setting. 
In continuous-time linear system theory, the Fourier transform is primarily an 
analytical tool for representing signals and systems. In contrast, in the discrete-time 
case, many signal processing systems and algorithms involve the explicit computation 
of the Fourier transform. While the Fourier transform itself cannot be computed, a 
sampled version of it, the discrete Fourier transform (DFT), can be computed, and 
for finite-length signals the DFT is a complete Fourier representation of the signal. In 
Chapter 8, the DFT is introduced and its properties and relationship to the discrete-time 
Fourier transform (DTFT) are developed in detail. In this chapter we also provide an 
introduction to the discrete cosine transform (DCT) which plays a very important role 
in applications such as audio and video compression. 
In Chapter 9, the rich and important variety of algorithms for computing or gen­
erating the DFT is introduced and discussed, including the Goertzel algorithm, the fast 
Fourier transform (FFT) algorithms, and the chirp transform. In this third edition, the 
basic upsampling and downsampling operations discussed in Chapter 4 are used to pro­
vide additional insight into the derivation of FFT algonthms. As also discussed in this 
chapter, the evolution of technology has considerably altered the important metrics in 
evaluating the efficiency of signal processing algorithms. At the time of our first book in 
the 1970's both memory and arithmetic computation (multiplications and also floating 
point additions) were costly and the efficiency of algorithms was typically judged by how 
much of these resources were required. Currently it is commonplace to use additional 
memory to increase speed and to reduce the power requirements in the implementation 
of signal processing algorithms. In a similar sense, multi-core platforms have in some 
contexts resulted in favoring parallel implementation of algorithms even at the cost of 
increased computation. Often the number of cycles of data exchange, communication 
on a chip, and power requirements are now key metrics in choosing the structure for 
implementing an algorithm. As discussed in chapter 9, while the FFT is more efficient in 
terms of the required multiplications than the Goertzel algorithm or the direct compu­
tation of the DFT, it is less efficient than either if the dominant metric is communication 
cycles since direct computation or the Goertzel algorithm can be much more highly 
parallelized than the FFT. 
With the background developed in the earlier chapters and particularly Chapters 
2, 3, 5, and 8, we focus in Chapter 10 on Fourier analysis of signals using the DFT. 
Without a careful understanding of the issues involved and the relationship between 
the continuous-time Fourier transform, the DTFT. and the DFT, using the DFT for 
practical signal analysis can often lead to confusion and misinterpretation. We address 
a number of these issues in Chapter 10. We also consider insome detail the Fourier anal­
ysis ofsignals with time-varying characteristics by means of the time-dependent Fourier 
transform. New in the third edition is a more detailed discussion of filter bank analysis 

Preface 
xxi 
including an illustration of the MPEG filter bank, new examples of time-dependent 
Fourier analysis of chirp signals illustrating the effect of window length, and more de­
tailed simulations of quantization noise analysis. 
Chapter 11 is an entirely new chapter on the subject ofparametric signal modeling. 
Starting with the basic concept of representing a signal as the output of an LTI system, 
Chapter 11 shows how the parameters of the signal model can be found by solution of 
a set of linear equations. Details of the computations involved in setting up and solving 
the equations are discussed and illustrated by examples. Particular emphasis is on the 
Levinson-Durbin solution algorithm and the many properties of the solution that are 
easily derived from the details of the algorithm such as the lattice filter interpretation. 
Chapter 12 is concerned with the discrete Hilbert transform. This transform arises 
in a variety of practical applications, including inverse filtering, complex representations 
for real bandpass signals, single-sideband modulation techniques, and many others. 
With the advent of increasingly sophisticated communications systems and the growing 
richness ofmethods for efficiently sampling wide-band and multi-band continuous-time 
signals, a basic understanding of Hi! bert transforms is becoming increasingly important. 
The Hilbert transform also plays an important role in the discussion of the cepstrum in 
Chapter 13. 
Our first book in 1975 and the first edition of this book in 1989 included a detailed 
treatment of the class of nonlinear techniques referred to as cepstral analysis and homo­
morphic deconvolution. These techniques have become increasingly important and now 
have widespread use in applications such as speech coding, speech and speaker recogni­
tion, analysis of geophysical and medical imaging data, and in many other applications 
in which deconvolution is an important theme. Consequently with this edition we re­
introduce those topics with expanded discussion and examples. The chapter contains a 
detailed discussion of the definition and properties of the cepstrum and the variety of 
ways of computing it including new results on the use of polynomial rooting as a ba­
sis for computing the cepstrum. An exposure to the material in Chapter 13 also offers 
the reader the opportunity to develop new insights into the fundamentals presented in 
the early chapters, in the context of a set of nonlinear signal analysis techniques with 
growing importance and that lend themselves to the same type of rich analysis enjoyed 
by linear techniques. The chapter also includes new examples illustrating the use of 
homomorphic filtering in deconvolution. 
We look forward to using this new edition in our teaching and hope that our 
colleagues and students will benefit from the many enhancements from the previous 
editions. Signal processing in general and discrete-time signal processing in particular 
have a richness in all their dimensions that promises even more exciting developments 
ahead. 
Alan V. Oppenheim 
Ronald W. Schafer 

THE COMPANION WEBSITE  
A companion website has been developed for this text by Mark A. Yoder and Wayne 
T. Padgett of Rose-Hulman Institute of Technology and is accessible at 
www.pearsonhighered.com/oppenheim.This web companion, which will continuously 
evolve, is designed to reinforce and enhance the material presented in the textbook by 
providing visualizations of important concepts and a framework for obtaining "hands­
on" experience with using the concepts. It contains six primary elements: Live Figures, 
Build-a-Figures, MATLAB-based homework problems, MATLAB-based projects, De­
mos, and additional Traditional Homework Problems, each tying into specific sections 
and pages in the book. 
Live Figures 
The Live Figures element reinforces concepts in the text by presenting "live" versions of 
select figures. With these, the reader is able to interactively investigate how parameters 
and concepts interoperate using graphics and audio. Live Figures were created with NI 
LabVIEW signal processing tools. The following three examples provide a glimpse of 
what is available with this element of the website: 
Figure 2.1O(a)-(c) in Section 2.3 on page 28 shows the graphical method for com­
puting a discrete-convolution with the result shown in Figure 2.10( d). The corresponding 
Live Figure allows the user to choose the input signals and manually slide the flipped 
input signal past the impulse response and see the result being ealculated and plotted. 
Users can quickly explore many different configurations and quickly understand how 
graphical convolution is applied. 
Figure 4.73 on page 231 of Section 4.9.2 shows the power spectral density of the 
quantization noise and signal after noise shaping. The Live Figure shows the spectrum 
of the noise and signal as a live audio file plays. The reader can see and hear the noise 
xxii 

xxiii 
The Companion Website 
as the noise shaping is enabled or disabled and as a lowpass filter is applied to remove 
the noise. 
Figure S.S(a) on page 282 of Section 5.1.2 shows three pulses, each of a different 
frequency, which enter an LTI system. Figure 5.6 on page 283 shows the output of the 
LTl system. The associated Live Figure allows students to experiment with the location 
of the poles and zeros in the system as well as the amplitude, frequency, and position 
of the pulses to see the effect on the output. These are just three examples of the many 
web-based Live Figures accessible on the companion website. 
Build-a-Figure 
The Build-a-Figure element extends the concept of the Live Figure element. It guides 
the student in recreating selected figures from the text using MATLAB to reinforce the 
understanding of the basic concepts. Build-a-Figures are not simply step-by-step recipes 
for constructing a figure. Rather, they assume a basic understanding of MATLAB and 
introduce new MATLAB commands and techniques as they are needed to create the 
figures. This not only further reinforces signal processing concepts, but also develops 
MATLAB skills in the context of signal processing. As an example, Figures 2.3 and 
2.5 on pages 12 and 16 in Section 2.1 of the text are plots of several sequences. The 
corresponding Build-a-Figures introduce the MATLAB plot command techniques for 
labeling plots, incorporating Greek characters, and including a legend. Later Build-a­
Figures use this knowledge as needed in creating plots. The Noise Shaping and Group 
Delay Build-a-Figures (Figure 4.73, page 231 and Figure 5.5, page 282) have instructions 
for recreating the Live Figures discussed above. Rather than giving step-by-step instruc­
tions, they introduce new MATLAB commands and suggest approaches for recreating 
the figures with considerable latitude for experimentation. 
MATLAB Homework Problems 
Through the MATLAB Homework Problems element, the companion website provides 
a primary mechanism for combining MATLAB with homework exercises. One aspect of 
this is the use of homework to practice using MATLAB somewhat in the style of Build­
a-Figures. These exercises are much like non-MATLAB exercises but with MATLAB 
used to facilitate certain parts, such as in plotting results. The second avenue is the 
use of MATLAB to explore and solve problems that cannot be done by mathematical 
analysis. The MATLAB problems are all classroom tested and tend to be short exercises, 
comparable to the Basic Problems in the textbook, in which the user is asked to complete 
straightforward signal processing tasks using MATLAB. These problems are modest in 
scope as would be typical of one of several problems in a weekly homework assignment. 
Some problems are directly linked to analytic problems in the text, while others will 
stand on their own. Many of the problems blend analytic solutions with MATLAB, 
emphasizing the complementary value of each approach. 

xxiv 
The Companion Website 
MATLAB-Based Projects 
The MATLAB-based Projects element contains longer and more sophisticated projects 
or exercises than the homework problems. The projects explore important concepts 
from the textbook in greater depth and are relatively extensive. Projects are linked to 
sections of the text and can be used once that section is understood. For example, the 
first project is somewhat tutorial in nature and can be used at any stage. It introduces 
MATLAB and shows how it is used to create and manipulate discrete-time signals 
and systems. It assumes that the students have some programming experience, but 
not necessarily in MATLAB. Many of the other projects require some filter design 
techniques and therefore tie in with Chapter 7 (Filter Design Techniques) or later. 
Illey explore topics such as FIR and IIR filter design, filter design for sample rate 
conversion, testing a "Folk Theorem" about humans not being able to hear phase in 
a signal, enhancing speech by removing noise, hardware considerations for removing 
noise, spectral estimation and more. All have been classroom tested and some have led 
to student publications. 
Demos 
The Demos are interactive demonstrations that relate to specific chapters. Unlike the 
Live Figures, they do not tie directly to a given figure. Rather, they illustrate a bigger 
idea that the student can understand after completing the chapter. For example, one 
demo shows the importance of using a linear-phase filter when it is essential to preserve 
the shape of a bandlimited pulse. 
Additional Traditional Homework Problems 
A sixth important component of the website is a collection of problems that were re­
moved from the second edition to make room for new problems. These problems can be 
used to supplement the problems in the text. Each of these problems is given in .pdf 
and . tex form along with any figures needed to create the problem. 
In summary, the companion web site is a rich set of resources which are closely 
tied to the textbook. The resources range from the Live Figures which reinforce new 
concepts to the MATLAB-based projects which challenge the students to go beyond the 
textbook to explore new ideas. This website will continuously evolve as new teaching 
resources are developed by the authors of the text and by the website authors, Mark 
Yoder and Wayne Padgett. 

p  
THE COVER  
In this third edition of Discrete-Time Signal Processing we continue the cover theme 
of "waves" as a symbol of our book and of signal processing. The cover of the first 
edition was a colorful rendering of a time-varying spectral waterfall plot. For the second 
edition, the artist Vivian Berman carried the theme forward resulting in a combination 
of spectral plots and artistic wave patterns. In considering possibilities for the cover, 
we were particularly drawn to a striking photograph by Librado Romero in a New 
York Times article (May 7, 2(09). This article by Holland Cottcr entitled "Storm Kiny 
Wavefield" was about the artist Maya Lin's new work at the Storm King Art Center. 
With this suggestion, Kristine Carney at Pearson/Prentice-Hall produced the beautiful 
cover for this edition. 
To us, the grass-covered earthen waves in Ms. Lin's sculpture symbolize much 
about the field of Signal Processing and suggest the perfect evolution of our covers. As 
the New York Times article states, 
"Like any landscape, it is a work in progress. Vegetation is still coming in, drainage issues 
are in testing mode, and there are unruly variables: woodchucks have begun converting 
one wave into an apartment complex:' 
Change a few words here and there, and it provides an intriguing description of 
the field of Discrete-Time Signal Processing. It has a beautiful solid framework. Fur­
thermore, new ideas, constraints, and opportunities keep the field fluid and dynamically 
changing, and there will always be a few "unruly variables." As Mr. Cotter also notes, 
Ms. Lin's work 
"sharpens your eye to existing harmonies and asymmetries otherwise overlooked." 
Even after more than 40 years of living and working in the field of signal processing, we 
are consistently surprised and delighted by the harmonies, symmetries, and asymmetries 
that continue to be revealed. 
Ilnfonnation about the Stonn King Art Center can be found at www.stonnking.org and about Ms. Lin 
and her beautiful art at www.mayalin.com. 
xxv 

ACKNOWLEDGMENTS  
This third edition of Discrete-Time Signal Processing has evolved from the first two 
editions (1989, 1999) which originated from our first book Digital Signal Processing 
(1975). The influence and impact of the many colleagues, students and friends who 
have assisted, supported and contributed to those earlier works remain evident in this 
new edition and we would like to express again our deep appreciation to all whom we 
have acknowledged more explicitly in those previous editions. 
Throughout our careers we both have had the good fortune of having extraordi­
nary mentors. We would each like to acknowledge several who have had such a major 
impact on our lives and careers. 
AI Oppenheim was profoundly guided and influenced as a graduate student and 
throughout his career by Professor Amar Bose, Professor Thomas Stockham, and Dr. 
Ben Gold. As a teaching assistant for several years with and as a doctoral student super­
vised by Professor Bose, Al was significantly influenced by the inspirational teaching, 
creative research style and extraordinary standards which are characteristic of Professor 
Bose in everything that he does. Early in his career Al Oppenheim was also extremely 
fortunate to develop a close collaboration and friendship with both Dr. Ben Gold and 
Professor Thomas Stockham. The incredible encouragement and role model provided 
by Ben was significant in shaping AI's style of mentoring and research. Tom Stockham 
also provided significant mentoring, support and encouragement as well as ongoing 
friendship and another wonderful role model. The influence of these extraordinary 
mentors flows throughout this book. 
Most notable among the many teachers and mentors who have influenced Ron 
Schafer are Professor Levi T. Wilson, Professor Thomas Stockham, and Dr. James L. 
Flanagan. Professor Wilson introduced a naive small town boy to the wonders of math­
ematics and science in a way that was memorable and life changing. His dedication to 
teaching was an inspiration too strong to resist. Professor Stockham was a great teacher, 
xxvi 

Acknowledgments 
xxvii 
a friend at a crucial time, a valued colleague, and a wonderfully creative engineer. Jim 
Flanagan is a giant in the area of speech science and engineering and an inspiration to 
all who are so lucky as to have worked with him. Not all great teachers carry the title 
"Professor". He taught Ron and many others the value of careful thought, the value of 
dedication to a field of learning, and the value of clear and lucid writing and expression. 
Ron Schafer freely admits appropriating many habits of thought and expression from 
these great mentors, and does so with confidence that they don't mind at all. 
Throughout our academic careers, MIT and Georgia Tech have provided us with a 
stimulating environment for research and teaching and have provided both encourage­
ment and support for this evolving project. Since 1977 Al Oppenheim has spent several 
sabbaticals and almost every summer at the Woods Hole Oceanographic Institution 
(WHOI) and he is deeply appreciative of this special opportunity and association. It 
was during those periods and in the wonderful WHOI environment that much of the 
writing of the various editions of this book were carried out. 
AT MIT and at Georgia Tech we have both received generous financial support 
from a number of sources. Al Oppenheim is extremely grateful for the support from 
Mr. Ray Stata and Analog Devices, Inc., the Bose Foundation, and the Ford Foundation 
for the funding of research and teaching at MIT in various forms. Both of us have also 
enjoyed the support of Texas Instruments, Inc. for our teaching and research activities. 
In particular, Gene Frantz at TI has been a dedicated supporter of our work and DSP 
education in general at both academic institutions. Ron Schafer is also grateful for the 
generous support from the John and Mary Franklin Foundation, which funded the John 
and Marilu McCarty Chair at Georgia Tech. Demetrius Paris, long time director of 
the School of ECE at Georgia Tech, and W. Kelly Mosley and Marilu McCarty of the 
Franklin Foundation, deserve special thanks for their friendship and support for over 
30 years. Ron Schafer is appreciative of the opportunity to be a part of the research 
team at Hewlett-Packard Laboratories, first through research support at Georgia Tech 
over many years, and since 2004, as an HP Fellow. The third edition could not have been 
completed without the encouragement and support of HP Labs managers Fred Kitson, 
Susie Wee, and John Apostolopoulos. 
Our association with Prentice Hall Inc. began several decades ago with our first 
book published in 1975 and has continued through all three editions of this book as well 
as with other books. We feel extremely fortunate to have worked with Prentice Hall. 
The encouragement and support provided by Marcia Horton and Tom Robbins through 
this and many other writing projects and by Michael McDonald, Andrew Gilfillan, Scott 
Disanno, and Clare Romeo with this edition have significantly enhanced the enjoyment 
of writing and completing this project. 
As with the previous editions, in producing this third edition, we were fortunate to 
receive the help of many colleagues, students, and friends. We greatly appreciate their 
generosity in devoting their time to help us with this project. In particular we express 
our thanks and appreciation to: 
Professor John Buck for his significant role in the preparation of the second edition 
and his continued time and effort during the life of that edition, 
Professors Vivek Goyal, Jae Lim, Gregory Wornell, Victor Zue and Drs. Babak Ayazi­
far, Soosan Beheshti, and Charles Rohrs who have taught at MIT from various 
editions and have made many helpful comments and suggestions, 

xxviii 
Acknowledgments 
Professors Tom Barnwell, Russ Mersereau, and Jim McClellan, long-time friends and 
colleagues of Ron Schafer, who have taught frequently from various editions 
and have influenced many aspects of the book, 
Professor Bruce Black of Rose-Rulman Institute ofTechnology for carefully organiz­
ing ten years worth of new problems, selecting the best of these, and updating 
and integrating them into the chapters, 
Professor Mark Yoder and Professor Wayne Padgett for the development of an out­
standing companion web site for this edition, 
Ballard Blair for his assistance in updating the bibliography, 
Eric Strattman, Darla Secor, Diane Wheeler, Stacy Schultz, Kay Gilstrap, and Char­
lotte Doughty for their administrative assistance in the preparation of this re­
vision and continued support of our teaching activities, 
Tom Baran for his help with many of the computer issues associated with managing the 
files for this edition and for his significant help with the examples in a number 
of the chapters, 
Shay Maymon who meticulously read through most of the chapters, reworked many of 
the problems in the more advanced chapters, and made important corrections 
and suggestions, 
To all who helped in careful reviewing of the manuscript and page proofs: Berkin 
Bilgic, Albert Chang, Myung Jin Choi, Majid Fozunbal, Reeve Ingle, Jeremy 
Leow, Ying Liu, Paul Ryu, Sanquan Song, Dennis Wei, and Zahi Karam. 
And to the many teaching assistants who have influenced this edition directly or in­
directly while working with us in teaching the subject at MIT and at Georgia 
Tech. 

The rich history and future promise of signal processing derive from a strong synergy 
between increasingly sophisticated applications, new theoretical developments and con­
stantly emerging new hardware architectures and platforms. Signal processing applica­
tions span an immense set of disciplines that include entertainment, communications, 
space exploration, medicine, archaeology, geophysics, just to name a few. Signal process­
ing algorithms and hardware are prevalent in a wide range of systems. from highly spe­
cialized military systems and industrial applications to low-cost, high-volume consumer 
electronics. Although we routinely take for granted the extraordinary performance of 
multimedia systems, such as high definition video, high fidelity audio, and interactive 
games, these systems have always relied heavily on state-of-the-art signal processing. 
Sophisticated digital signal processors are at the core of all modern cell phones. MPEG 
audio and video and JPEG1 image data compression standards rely heavily on many 
of the signal processing principles and techniques discussed in this text. High-density 
data storage devices and new solid-state memories rely increasingly on the use of signal 
processing to provide consistency and robustness to otherwise fragile technologies. As 
we look to the future, it is clear that the role of signal processing is expanding, driven in 
part by the convergence of communications, computers, and signal processing in both 
the consumer arena and in advanced industrial and government applications. 
The growing number of applications and demand for incrcasingly sophisticated 
algorithms go hand-in-hand with the rapid development of device technology for imple­
menting signal processing systems. By some estimates, even with impending limitations 
1The acronyms MPEG and JPEG are the terms used in even casual conversation for referring to the 
standards developed by the "Moving Picture Expert Group (MPEG)" and the "Joint Photographic Expert 
Group (JPEG)" of the "International Organization for Standardization (ISO)." 
1 

2 
Chapter 1 
Introduction 
on Moore's Law, the processing capability of both special-purpose signal processing 
microprocessors and personal computers is likely to increase by several orders of mag­
nitude over the next 10 years. Clearly, the importance and role of signal processing will 
continue to expand at an accelerating rate well into the future. 
Signal processing deals with the representation, transformation, and manipulation 
of signals and the information the signals contain. For example, we may wish to sepa­
rate two or more signals that have been combined by some operation, such as addition, 
multiplication, or convolution, or we may want to enhance some signal component or 
estimate some parameter of a signal model. In communications systems, it is generally 
necessary to do preprocessing such as modulation, signal conditioning, and compression 
prior to transmission over a communications channel, and then to carry out postpro­
cessing at the receiver to recover a facsimile of the original signal. Prior to the 1960s, 
the technology for such signal processing was almost exclusively continuous-time ana­
log technology.2 A continual and major shift to digital technologies has resulted from 
the rapid evolution of digital computers and microprocessors and low-cost chips for 
analog to digital (AiD) and digital to analog (D/A) conversion. These developments 
in technology have been reinforced by many important theoretical developments, such 
as the fast Fourier transform (FFf) algorithm, parametric signal modeling, multirate 
techniques, polyphase filter implementation, and new ways of representing signals, such 
as with wavelet expansions. As just one example of this shift analog radio communica­
tion systems are evolving into reconfigurable "software radios" that are implemented 
almost exclusively with digital computation. 
Discrete-time signal processing is based on processing of numeric sequences in­
dexed on integer variables rather than functions of a continuous independent vari­
able. In digital signal processing (DSP), signals are represented by sequences of finite­
precision numbers, and processing is implemented using digital computation. The more 
general term discrete-time signal processing includes digital signal processing as a spe­
cial case but also includes the possibility that sequences of samples (sampled data) 
could be processed with other discrete-time technologies. Often the distinction be­
tween the terms discrete-time signal processing and digital signal processing is of minor 
importance, since both are concerned with discrete-time signals. This is particularly true 
when high-precision computation is employed. Although there are many examples in 
which signals to be processed are inherently discrete-time sequences, most applica­
tions involve the use of discrete-time technology for processing signals that originate 
as continuous-time signals. In this case, a continuous-time signal is typically converted 
into a sequence of samples, i.e., a discrete-time signal. Indeed, one of the most impor­
tant spurs to widespread application of digital signal processing was the development 
of low-cost AID, DIA conversion chips based on differential quantization with noise 
shaping. After discrete-time processing, the output sequence is converted back to a 
continuous-time signal. Real-time operation is often required or desirable for such sys­
tems. As computer speeds have increased, discrete-time processing of continuous-time 
signals in real time has become commonplace in communication systems, radar and 
sonar, speech and video coding and enhancement, biomedical engineering, and many 
2 In a general context, we shall refer to the independent variable as "time," even though in specific 
contexts, the independent variable may take on any of a broad range of possible dimensions. Consequently, 
continuous time and discrete time should be thought ofas generic terms referring to a continuous independent 
variable and a discrete independent variable, respectively. 

3 
Chapter 1 
Introduction 
other areas of application. Non-real-time applications are also common. The compact 
disc player and MP3 player are examples of asymmetric systems in which an input signal 
is processed only once. The initial processing may occur in real time, slower than real 
time, or even faster than real time. The processed form of the input is stored (on the 
compact disc or in a solid state memory), and final processing for reconstructing the 
audio signal is carried out in real time when the output is played back for listening. 
The compact disc and MP3 recording and playback systems rely on many of the signal 
processing concepts that we discuss in this book. 
Financial Engineering represents another rapidly emerging field which incorpo­
rates many signal processing concepts and techniques. Effective modeling, prediction 
and filtering of economic data can result in significant gains in economic performance 
and stability. Portfolio investment managers, for example, are relying increasingly on 
using sophisticated signal processing since even a very small increase in signal pre­
dictability or signal-to-noise ratio (SNR) can result in significant gain in performance. 
Another important area of signal processing is signal interpretation. In such con­
texts, the objective of the processing is to obtain a characterization of the input signal. 
For example, in a speech recognition or understanding system, the objective is to in­
terpret the input signal or extract information from it. Typically, such a system will 
apply digital pre-processing (filtering, parameter estimation, and so on) followed by a 
pattern recognition system to produce a symbolic representation, such as a phonemic 
transcription of the speech. This symbolic output can, in turn, be the input to a sym­
bolic processing system, such as a rules-based expert system, to provide the final signal 
interpretation. 
Still another relatively new category of signal processing involves the symbolic 
manipulation of signal processing expressions. This type of processing is potentially 
useful in signal processing workstations and for the computer-aided design of signal 
processing systems. In this class of processing, signals and systems are represented and 
manipulated as abstract data objects. Object-oriented programming languages provide 
a convenient environment for manipulating signals, systems, and signal processing ex­
pressions without explicitly evaluating the data sequences. The sophistication of systems 
designed to do signal expression processing is directly influenced by the incorporation 
of fundamental signal processing concepts, theorems, and properties, such as those that 
form the basis for this book. For example, a signal processing environment that incor­
porates the property that convolution in the time domain corresponds to multIplication 
in the frequency domain can explore a variety of rearrangements of filtering structures, 
including those involving the direct use of the discrete Fourier transform (DFT) and the 
FFT algorithm. Similarly, environments that incorporate the relationship between sam­
pling rate and aliasing can make effective use ofdecimation and interpolation strategies 
for filter implementation. Similar ideas are currently being explored for implementing 
signal processing in network environments. In this type of environment, data can po­
tentially be tagged with a high-level description of the processing to be done, and the 
details of the implementation can be based dynamically on the resources available on 
the network. 
Many of the concepts and design techniques discussed in this text are now incorpo­
rated into the structure of sophisticated software systems such as MATLAB, Simulink, 
Mathematica, and LabVIEW. In many cases where discrete-time signals are acquired 
and stored in computers, these tools allow extremely sophisticated signal processing 

4 
Chapter 1 
Introduction 
operations to be fonned from basic functions. In such cases, it is not generally necessary 
to know the details of the underlying algorithm that implements the computation of an 
operation like the FFT, but nevertheless it is essential to understand what is computed 
and how it should be interpreted. In other words, a good understanding of the concepts 
considered in this text is essential for intelligent use of the signal processing software 
tools that are now widely available. 
Signal processing problems are not confined, ofcourse, to one-dimensional signals. 
Although there are some fundamental differences in the theories for one-dimensional 
and multidimensional signal processing, much of the material that we discuss in this text 
has a direct counterpart in multidimensional systems. The theory of multidimensional 
digital signal processing is presented in detail in a variety of references including Dud­
geon and Mersereau (1984), Lim (1989), and Bracewell (1994).3 Many image processing 
applications require the use of two-dimensional signal processing techniques. This is the 
case in such areas as video coding, medical imaging, enhancement and analysis of aerial 
photographs, analysis of satellite weather photos, and enhancement of video transmis­
sions from lunar and deep-space probes. Applications of multidimensional digital signal 
processing to image processing are discussed, for example, in Macovski (1983), Castle­
man (1996), Jain (1989), Bovic (ed.) (2005), Woods (2006), Gonzalez and Woods (2007), 
and Pratt (2007). Seismic data analysis as required in oil exploration, earthquake mea­
surement, and nuclear test monitoring also uses multidimensional signal processing 
techniques. Seismic applications are discussed in, for example, Robinson and Treitel 
(1980) and Robinson and Durrani (1985). 
Multidimensional signal processing is only one of many advanced and specialized 
topics that build on the fundamentals covered in this text. Spectral analysis based on the 
use of the DFT and the use of signal modeling is another particularly rich and important 
aspect of signal processing. We discuss many facets of this topic in Chapters 10 and 11, 
which focus on the basic concepts and techniques relating to the use of the DFT and 
parametric signal modeling. In Chapter 11, we also discuss in some detail high resolu­
tion spectrum analysis methods, based on representing the signal to be analyzed as the 
response of a discrete-time linear time-invariant (LTI) filter to either an impulse or to 
white noise. Spectral analysis is achieved by estimating the parameters (e.g., the differ­
ence equation coefficients) of the system and then evaluating the magnitude squared 
of the frequency response of the model filter. Detailed discussions of spectrum analysis 
can be found in the texts by Kay (1988), Marple (1987), Therrien (1992), Hayes (1996) 
and Stoica and Moses (2005). 
Signal modeling also plays an important role in data compression and coding, 
and here again, the fundamentals of difference equations provide the basis for under­
standing many of these techniques. For example, one class of signal coding techniques, 
referred to as linear predictive coding (LPC), exploits the notion that if a signal is the 
response of a certain class of discrete-time filters, the signal value at any time index is a 
linear function of (and thus linearly predictable from) previous values. Consequently, 
efficient signal representations can be obtained by estimating these prediction param­
eters and using them along with the prediction error to represent the signal. The signal 
can then be regenerated when needed from the model parameters. This class of signal 
3 Authors names and dates are used throughout the text to refer to books and papers listed in the 
Bibliography at the end of the book. 

5 
•  
Chapter 1 
Introduction 
coding techniques has been particularly effective in speech coding and is described in 
considerable detail in Jayant and Noll (1984), Markel and Gray (1976), Rabiner and 
Schafer (1978) and Quatieri (2002), and is also discussed in some detail in Chapter II. 
Another advanced topic of considerable importance is adaptive signal processing. 
Adaptive systems represent a particular class of time-varying and, in some sense, non­
linear systems with broad application and with established and effective techniques for 
their design and analysis. Again, many of these techniques build from the fundamen­
tals of discrete-time signal processing covered in this text. Details of adaptive signal 
processing are given by Widrow and Stearns (1985), Haykin (2002) and Sayed (2008). 
These represent only a few of the many advanced topics that extend from the 
content covered in this text. Others include advanced and specialized filter design pro­
cedures, a variety ofspecialized algorithms for evaluation of the Fourier transform, spe­
cialized filter structures, and various advanced multirate signal processing techniques, 
including wavelet transforms. (See Burrus, Gopinath, and Guo (1997), Vaidyanathan 
(1993) and Vetterli and KovaCevic (1995) for introductions to these topics.) 
It has often been said that the purpose of a fundamental textbook should be to 
uncover, rather than cover, a subject. In choosing the topics and depth of coverage in 
this book, we have been guided by this philosophy. The preceding brief discussion and 
the Bibliography at the end of the book make it abundantly clear that there is a rich 
variety of both challenging theory and compelling applications to be uncovered by those 
who diligently prepare themselves with a study of the fundamentals of DSP. 
HISTORIC PERSPECTIVE 
Discrete-time signal processing has advanced in uneven steps over time. Looking back 
at the development of the field of discrete-time signal processing provides a valuable 
perspective on fundamentals that will remain central to the field for a long time to 
come. Since the invention of calculus in the 17th century, scientists and engineers have 
developed models to represent physical phenomena in terms of functions of continuous 
variables and differential equations. However, numeric techniques have been used to 
solve these equations when analytical solutions are not possible. Indeed, Newton used 
finite-difference methods that are special cases of some ofthe discrete-time systems that 
we present in this text. Mathematicians of the 18th century, such as Euler, Bernoulli, and 
Lagrange, developed methods for numeric integration and interpolation of functions of 
a continuous variable. Interesting historic research by Heideman, Johnson, and Burrus 
(1984) showed that Gauss discovered the fundamental principle of the FFT (discussed 
in Chapter 9) as early as 1805-even before the publication of Fourier's treatise on 
harmonic series representation of functions. 
Until the early 1950s, signal processing as we have defined it was typically carried 
out with analog systems implemented with electronic circuits or even with mechanical 
devices. Even though digital computers were becoming available in business environ­
ments and in scientific laboratories, they were expensive and had relatively limited 
capabilities. About that time, the need for more sophisticated signal processing in some 
application areas created considerable interest in discrete-time signal processing. One 
of the first uses of digital computers in DSP was in geophysical exploration, where rel­
atively low frequency seismic signals could be digitized and recorded on magnetic tape 

6 
Chapter 1 
Introduction 
for later processing. This type of signal processing could not generally be done in real 
time; minutes or even hours of computer time were often required to process only sec­
onds of data. Even so, the flexibility of the digital computer and the potential payoffs 
made this alternative extremely inviting. 
Also in the 1950s, the use of digital computers in signal processing arose in a 
different way. Because of the flexibility of digital computers, it was often useful to sim­
ulate a signal processing system on a digital computer before implementing it in analog 
hardware. In this way, a new signal processing algorithm or system could be studied 
in a flexible experimental environment before committing economic and engineering 
resources to constructing it. Typical examples ofsuch simulations were the vocoder sim­
ulations carried out at Massachusetts Institute ofTechnology (MIT) Lincoln Laboratory 
and Bell Telephone Laboratories. In the implementation of an analog channel vocoder, 
for example, the filter characteristics affected the perceived quality of the coded speech 
signal in ways that were difficult to quantify objectively. Through computer simulations, 
these filter characteristics could be adjusted and the perceived quality of a speech coding 
system evaluated prior to construction of the analog equipment. 
In all of these examples of signal processing using digital computers, the computer 
offered tremendous advantages in flexibility. However, the processing could not be 
done in real time. Consequently, the prevalent attitude up to the late 1%Os was that the 
digital computer was being used to approximate, orsimulate, an analog signal processing 
system. In keeping with that style, early work on digital filtering concentrated on ways in 
which a filter could be programmed on a digital computer so that with AID conversion 
of the signal, followed by digital filtering, followed by DIA conversion. the overall 
system approximated a good analog filter. The notion that digital systems might, in 
fact, be practical for the actual real-time implementation of signal processing in speech 
communication, radar processing, or any of a variety of other applications seemed, 
even at the most optimistic times, to be highly speCUlative. Speed. cost, and size were, 
of course, three of the important factors in favor of the use of analog components. 
As signals were being processed on digital computers, researchers had a natural 
tendency to experiment with increasingly sophisticated signal processing algorithms. 
Some of these algorithms grew out of the flexibility of the digital computer and had no 
apparent practical implementation in analog equipment. Thus, many of these algorithms 
were treated as interesting, but somewhat impractical, ideas. However, the development 
of such signal processing algorithms made the notion of all-digital implementation of 
signal processing systems even more tempting. Active work began on the investigation 
of digital vocoders, digital spectrum analyzers, and other all-digital systems, with the 
hope that eventually, such systems would become practical. 
The evolution of a new point of view toward discrete-time signal processing was 
further accelerated by the disclosure by Cooley and Thkey (1965) of an efficient class 
of algorithms for computation of Fourier transforms known collectively a<; the FFr. 
The FFT was significant for several reasons. Many signal processing algorithms that 
had been developed on digital computers required processing times several orders of 
magnitude greater than real time. Often, this was because spectrum analysis was an 
important component of the signal processing and no efficient means were available for 
implementing it. The FFT reduced the computation time of the Fourier transform by 
orders of magnitUde, permitting the implcmentation of increasingly sophisticated signal 

7 
Chapter 1 
Introduction 
processing algorithms with processing times that allowed interaetive experimentation 
with the system. Furthermore, with the realization that the FFf algorithms might, in 
fact, be implementable with special-purpose digital hardware, many signal processing 
algorithms that previously had appeared to be impractical began to appear feasible. 
Another important implication of the FFf was that it was an inherently discrete­
time concept. It was directed toward the computation of the Fourier transform of a 
discrete-time signal or sequence and involved a set of properties and mathematics 
that was exact in the discrete-time domain-it was not simply an approximation to 
a continuous-time Fourier transform. This had the effect of stimulating a reformulation 
of many signal processing concepts and algorithms in terms of discrete-time mathemat­
ics, and these techniques then formed an exact set of relationships in the discrete-time 
domain. Following this shift away from the notion that signal processing on a digital 
computer was merely an approximation to analog signal processing techniques, there 
emerged the current view that discrete-time signal processing is an important field of 
investigation in its own right. 
Another major development in the history of discrete-time signal processing oc­
curred in the field of microelectronics. The invention and subsequent proliferation of 
the microprocessor paved the way for low-cost implementations of discrete-time signal 
processing systems. Although the first microprocessors were too slow to implement most 
discrete-time systems in real time except at very low sampling rates, by the mid-1980s, 
integrateddrcuit technology had advanced to a level that permitted the implementation 
of very fast fixed-point and floating-point microcomputers with architectures specially 
designed for implementing discrete-time signal processing algorithms. With this tech­
nology came, for the first time, the possibility ofwidespread application of discrete-time 
signal processing techniques. The rapid pace of development in microelectronics also 
significantly impacted the development of signal processing algorithms in other ways. 
For example, in the early days of real-time digital signal processing devices, memory 
was relatively costly and one of the important metrics in developing signal processing 
algorithms was the efficient use of memory. Digital memory is now so inexpensive that 
many algorithms purposely incorporate more memory than is absolutely required so 
that the power requirements of the processor are reduced. Another area in which tech­
nology limitations posed a significant barrier to widespread deployment of DSP was in 
conversion of signals from analog to discrete-time (digital) form. The first widely avail­
able AID and DIA converters were stand-alone devices costing thousands of dollars. 
By combining digital signal processing theory with microelectronic technology, over­
sampled ND and DIA converters costing a few dollars or less have enabled a myriad 
of real-time applications. 
In a similar way, minimizing the number of arithmetic operations, such as multi­
plies or floating point additions, is now less essential, since multicore processors often 
have several multipliers available and it becomes increasingly important to reduce com­
munication between cores, even if it then requires more multiplications. In a multicore 
environment, for example, direct computation ofthe DFf (or the use of the Goertzel al­
gorithm) is more "efficient" than the use of anFFf algorithm since, although many more 
multiplications are required, communication requirements are significantly reduced be­
cause the processing can be more efficiently distributed among multiple processors or 
cores. More broadly, the restructuring of algorithms and the development of new ones 

8 
Chapter 1 
Introduction 
to exploit the opportunity for more parallel and distributed processing is becoming a 
significant new direction in the development of signal processing algorithms. 
FUTURE PROMISE 
Microelectronics engineers continue to strive for increased circuit densities and produc­
tion yields, and as a result, the complexity and sophistication of microelectronic systems 
continually increase. The complexity, speed, and capability of DSP chips have grown 
exponentially since the early 1980s and show no sign of slowing down. As wafer-scale 
integration techniques become highly developed, very complex discrete-time signal 
processing systems will be implemented with low cost, miniature size, and low power 
consumption. Furthermore, technologies such as microelectronic mechanical systems 
(MEMS) promise to produce many types of tiny sensors whose outputs will need to 
be processed using DSP techniques that operate on distributed arrays of sensor inputs. 
Consequently, the importance of discrete-time signal processing will continue to in­
crease, and the future development of the field promises to be even more dramatic than 
the course of development that we have just described. 
Discrete-time signal processing techniques have already promoted revolutionary 
advances insome fields of application. A notable example is in the area oftelecommuni­
cations, where discrete-time signal processing techniques, microelectronic technology, 
and fiber optic transmission have combined to change the nature of communication 
systems in truly revolutionary ways. A similar impact can be expected in many other 
areas. Indeed, signal processing has always been, and will always be, a field that thrives 
on new applications. The needs of a new field of application can sometimes be filled 
by knowledge adapted from other applications, but frequently, new application needs 
stimulate new algorithms and new hardware systems to implement those algorithms. 
Early on, applications to seismology, radar, and communication provided the context 
for developing many of the core signal processing techniques that we discuss in this 
book. Certainly, signal processing will remain at the heart of applications in national 
defense, entertainment, communication, and medical care and diagnosis. Recently, we 
have seen applications of signal processing techniques in new areas as disparate as 
finance and DNA sequence analysis. 
Although it is difficult to predict where other new applications will arise, there is 
no doubt that they will be obvious to those who are prepared to recognize them. The 
key to being ready to solve new signal processing problems is, and has always been, a 
thorough grounding in the fundamental mathematics of signals and systems and in the 
associated design and processing algorithms. While discrete-time signal processing is a 
dynamic, steadily growing field, its fundamentals are well formulated, and it is extremely 
valuable to learn them welL Our goal in this book is to uncover the fundamentals of the 
field by providing a coherent treatment of the theory of discrete-time linear systems, 
filtering, sampling, discrete-time Fourier analysis, and signal modeling. This text should 
provide the reader with the knowledge necessary for an appreciation of the wide scope 
of applications for discrete-time signal processing and a foundation for contributing to 
future developments in this exciting field. 

2.0 INTRODUCTION 
The term signal is generally applied to something that conveys information. Signals 
may, for example. convey information about the state or behavior of a physical system. 
As another class of examples, signals are synthesized for the purpose of communicating 
information between humans or between humans and machines. Although signals can 
be represented in many ways, in all cases, the information is contained in some pattern 
of variations. Signals are represented mathematically as functions of one or more in­
dependent variables. For example, a speech signal is represented mathematically as a 
function of time, and a photographic image is represented as a brightness function of 
two spatial variables. A common convention-and one that usually will be followed in 
this book-is to refer to the independent variable of the mathematical representation 
of a signal as time: although in specific examples, the independent variable may not in 
fact correspond to time. 
The independent variable in the mathematical representation of a signal may be 
either continuous or discrete. Continuous-time signals are defined along a continuum of 
time and are thus represented by a continuous independent variable. Continuous-time 
signals are often referred to as analog signals. Discrete-time signals are defined at discrete 
times, and thus, the independent variable has discrete values; that is, discrete-time signals 
are represented as sequences of numbers. Signals such as speech or images may have 
either a continuous- or a discrete-variable representation, and if certain conditions hold, 
these representations are entirely equivalent. Besides the independent variables being 
either continuous or discrete. the signal amplitude may be either continuous or discrete. 
Digital signals are those for which both time and amplitude are discrete. 
9 

10 
Chapter 2 
Discrete-Time Signals and Systems 
Signal-processing systems may be classified along the same lines as signals. That 
is, continuous-time systems are systems for which both the input and the output are 
continuous-time signals, and discrete-time systems are those for which both the input 
and the output are discrete-time signals. Similarly, a digital system is a system for which 
both the input and the output are digital signals. Digital signal processing, then, deals 
with the transformation of signals that are discrete in both amplitude and time. The 
principal focus ofthis book is on discrete-time-rather than digital-signals and systems. 
However, the theory of discrete-time signals and systems is also exceedingly useful for 
digital signals and systems, particularly if the signal amplitudes are finely quantized. The 
effects of signal amplitude quantization are considered in Sections 4.8,6.8-6.10, and 9.7. 
In this chapter, we present the basic definitions, establish notation, and develop 
and review the basic concepts associated with discrete-time signals and systems. The pre­
sentation of this material assumes that the reader has had previous exposure to some of 
this material, perhaps with a different emphasis and notation. Thus, this chapter is pri­
marily intended to provide a common foundation for material covered in later chapters. 
In Section 2.1, we discuss the representation of discrete-time signals as sequences 
and describe the basic sequences such as the unit impulse, the unit step, and complex 
exponential, which playa central role in characterizing discrete-time systems and form 
building blocks for more general sequences. In Section 2.2, the representation, basic 
properties, and simple examples of discrete-time systems are presented. Sections 2.3 and 
2.4 focus on the important class of linear time-invariant (LTI) systems and their time­
domain representation through the convolution sum, with Section 2.5 considering the 
specific class of LTI systems represented by linear, constant-coefficient difference equa­
tions. Section 2.6 develops the frequency domain representation of discrete-time sys­
tems through the concept of complex exponentials as eigcnfunctions, and Sections 2.7, 
2.8, and 2.9 develop and explore the Fourier transform representation of discrete-time 
signals as a linear combination of complex exponentials. Section 2.10 provides a brief 
introduction to discrete-time random signals. 
2.1 DISCRETE-TIME SIGNALS 
Discrete-time signals are represented mathematically as sequences of numbers. A se­
quence of numbers x, in which the nth number in the sequence is denoted x[n],l is 
formally written as_ 
x = {x[n]}, 
-00 < n < 00, 
(2.l) 
where n is an integer. In a practical setting, such sequences can often arise from periodic 
sampling of an analog (i.e., continuous-time) signal Xa (t). In that case, the numeric value 
of the nth number in the sequence is equal to the value of the analog signal, xa(r), at 
time nT: i.e., 
x[n] = xa(nT), 
-00 < n < 00. 
(2.2) 
The quantity T is the sampling period, and its reciprocal is the sampling frequency. Al­
though sequences do not always arise from sampling a~log waveforms, it is convenient 
'i 
1 Note that we use [1 to enclose the independent varia hIe of disFrete-variable functions, and we use () 
to enclose the independent variable of continuous-variable functions! 
/ 
I 

r 
F
e, 
to: 
t 
Section 2.1 
( 
( 
Discrete-Time Signals 
'1 
to refer to x[n] as the "nth sample" of the sequence. Also, although, strictly speaking, 
x[n] denotes the nth number in the sequence, the notation of Eq. (2.1) is often unnec­
essarily cumbersome, and it is convenient and unambiguous to refer to "the sequence 
x[n]" when we mean the entire sequence,just as we referred to "the analog signalxa(t):' 
We depict discrete-time signals (i.e., sequences) graphically, as shown in Figure 2.1. Al­
though the abscissa is drawn as a continuous line, it is important to recognize that x[n] 
is defined only for integer values of n. It is not correct to think of x[n] as being zero 
when n is not an integer; x [n Jis simply undefined for noninteger values of n. 
x[-J] x [0] 
x[-2] 
x[1] 
x[n] 
x[2] 
Figure 2.' 
Graphic representation of a 
n 
discrete-time signal. 
As an example of a sequence obtained by sampling, Figure 2.2(a) shows a segment 
of a speech signal corresponding to acoustic pressure variation as a function of time, 
and Figure 2.2(b) presents a sequence of samples of the speech signaL Although the 
original speech signal is defined at all values of time t, the sequence contains information 
about the signal only at discrete instants. The sampling theorem, discussed in Chapter 4, 
~---------------------------------32ms----------------------------~ 
(a) 
1--<-------------------------------- 256 samples ---------------------------+1 
(b) 
Figure 2.2 
(a) Segment of acontinuous-time speech signal xa(t). (b) Sequence of samples 
x[n] 
Xa(nT) obtained from the signal in part (a) with T = 125 j.LS. 

12 
Chapter 2 
Discrete-Time Signals and Systems 
guarantees that the original signal can be reconstructed as accurately as desired from a 
corresponding sequence of samples if the samples are taken frequently enough. 
In discussing the theory of discrete-time signals and systems, several basic se­
quences are of particular importance. These sequences are shown in Figure 2.3 and will 
be discussed next. 
The unit sample sequence (Figure 2.3a) is defined as the sequence 
8[n] = {~: ~ I=~: 
(2.3) 
The unit sample sequence plays the same role ,for discrete-time signals and systems that 
the unit impulse function (Dirac delta function) does for continuous-time signals and 
systems. For convenience, we often refer to the unit sample sequence as a discrete-time 
impulse or simply as an impulse. It is important to note that a discrete-time impulse 
does not suffer from the mathematic complications of the continuous-time impulse; its 
definition in Eq. (2.3) is simple and precise. 
'1 
Unit sample 
• • • • • • • • • o • • • • • • • • • • • 
n 
(a) 
Unit step 
1 
o 
n 
(b) 
Real exponential 
o 
n 
(c) 
Sinusoidal 
• J11TTl I 
n 
Figure 2.3 
Some basic sequences.
III 
0 
• III 
The sequences shown play important 
roles in the analysis and representation 
(d) 
of discrete-time signals and systems. 

13 
Section 2.1 
Discrete-Time Signals 
p[n] 
8 
n  
Figure 2.4 
Example of asequence to 
be represented as asum of scaled, 
delayed impulses. 
One of the important aspects of the impulse sequence is that an arbitrary sequence 
can be represented as a sum of scaled, delayed impulses. For example, the sequence p[n] 
in Figure 2.4 can be expressed as 
p[n] = a_30[n + 3] + a18[n - 1] + a28[n - 2] + a78[n - 7]. 
(2.4) 
More generally, any sequence can be expressed as 
x[n] = L
00 
x[k]o[n - k].  
(2.5) 
k=-oo 
We will make specific use of Eq. (2.5) in discussing the representation of discrete-time 
linear systems. 
The unit step sequence (Figure 2.3b) is defined as 
1 
n 2: 0, 
(2.6)
u[n] = 
0', 
°
{ 
n < 
. 
The unit step is related to the unit impulse by 
n 
u[n] = L 8[k];  
(2.7) 
k=-oo 
that is, the value of the unit step sequence at (time) index n is equal to the accumulated 
sum of the value at index n and all previous values of the impulse sequence. An alterna­
tive representation of the unit step in terms of the impulse is obtained by interpreting 
the unit step in Figure 2.3(b) in terms of a sum of delayed impulses, as in Eq. (2.5). In 
this case, the nonzero values are all unity, so 
u[n] = 8[n] + 8[n -
1] + 8[n - 2] + ...  
(2.8a) 
or 
u[n] = L
00 
8[n - k].  
(2.8b) 
k=O 
As yet another alternative, the impulse sequence can be expressed as the first backward 
difference of the unit step sequence, i.e., 
8[n] = u[n] - u[n -1].  
(2.9) 
Exponential sequences are another important class of basic signals. The general 
form of an exponential sequence is 
(2.10) 
If A and a are real numbers, then the sequence is real. If 0 < a < 1 and A is positive, 
then the sequence values are positive and decrease with increasing n, as in Figure 2.3( c). 

14 
Chapter 2 
Discrete-Time Signals and Systems 
For -1 < a < 0, the sequence values alternate in sign but again decrease in magnitude 
with increasing n. If lal > 1, then the sequence grows in magnitude as n increases. 
The exponential sequence A an with a complex has real and imaginary parts that 
are exponentially weighted sinusoids. Specifically, if a 
lale}wQ and A = IA lej.p, the 
sequence A an can be expressed in any of the following ways: 
x[n] = Aan = IA lej.plalneJWQfl 
IA Ilalflei(wonH ) 
(2.11) 
= IA Iiaincos(won + </» + jlA Iiainsin(won + </» • 
• 
The sequence oscillates with an exponentially growing envelope if lal > lor with an 
exponentially decaying envelope if lal < 1. (As a simple example, consider the case 
Wo 
rr.) 
When lal = 1, the sequence has the form 
x[n] = IA le j (wollH ) 
IA Icos(won + </» + jlA Isin(won + </»; 
(2.12) 
that is, the real and imaginary parts ofeJwoll vary sinusoidally with n. By analogy with the 
continuous-time case, the quantity Wo is called the frequency of the complex sinusoid 
or complex exponential, and </> is called the phase. However, since n is a dimensionless 
integer, the dimension of Wo is radians. If we wish to maintain a closer analogy with the 
continuous-time case, we can specify the units of wo to be radians per sample and the 
units of n to be samples. 
The fact that n is always an integer in Eq. (2.12) leads to some important differ­
ences between the properties ofdiscrete-time and continuous-time complex exponential 
sequences and sinusoidal sequences. Consider, for example, a frequency (wo + 2rr). In 
this case, 
x[nl = Ae}(wo+2rr)1l 
(2.13) 
= AeilVOllei2rrn 
AeJWOfl . 
Generally, complex exponential sequences with frequencies (wo + 2rrr), where r is 
an integer, are indistinguishable from one another. An identical statement holds for 
sinusoidal sequences. Specifically, it is easily verified that 
x[n] 
A cos[(wo + 2rrr)n + </>] 
(2.14)
A cos(won + </». 
The implications of this property for sequences obtained by sampling sinusoids and 
other signals will be discussed in Chapter 4. For now, we conclude that, when discussing 
complex exponential signals of the form x[n] = A eiwoll or real sinusoidal signals of the 
form x[n] 
A cos(won + </», we need only consider frequencies in an interval of length 
2rr. Typically, we will choose either -rr < wo ::: rr or 0 ::: wo < 2rr. 
Another important difference between continuous-time and discrete-time com­
plex exponentials and sinusoids concerns their periodicity in n. In the continuous-time 
case, a sinusoidal signal and a complex exponential signal are both periodic in time with 
the period equal to 2rr divided by the frequency. In the discrete-time case, a periodic 
sequence is a sequence for which 
x[n] 
x[n + N], 
for all n, 
(2.15) 

15 
p  
Section 2.1 
Discrete-Time Signals 
where the period N is necessarily an integer. If this condition for periodicity is tested 
for the discrete-time sinusoid, then 
A cos(won + cp) 
A cos(won + woN + cp), 
(2.16) 
which requires that 
wON = 2rck, 
(2.17) 
where k is an integer. A similar statement holds for the complex exponential sequence 
Cejwofl ; that is, periodicity with period N requires that 
(2.18) 
which is true only for woN 
2rc k, as in Eq. (2.17). Consequently, complex exponential 
and sinusoidal sequences are not necessarily periodic in n with period (2rc / wo) and, 
depending on the value of wo, may not be periodic at all. 
Example 2.1 
Periodic and Aperiodic Discrete-Time Sinusoids 
Considerthe signal Xl [n] = cos(7in /4). 111is signal has a period of N = 8. To show this, 
note that x[n +8] = cos(rc(n +8)/4) 
cos(rcn/4+ 2rc) = cos(7in/4) 
x[nj, satisfying 
the definition of a discrete-time periodic signaL Contrary to continuous-time sinusoids, 
increasing the value of (vo for a discrete-time sinusoid does not necessarily decrease 
the period of the signaL Consider the discrete-time sinusoid x2[n1= cos(3nn/8), which 
has a higher frequency than xl In]. However, X2 [n] is not periodic with period 8, since 
x2[n + 8] 
cos(37i(n + 8)/8) 
cos(37in/8 + 3n) 
-x2[nj. Using an argument 
analogous to the one for xl [n], we can show that x2[n] has a period of N = 16. Thus, 
increasing the value of(Vo = 27i/8 to (vo 
3n/8 also increases the period of the signal. 
This occurs because discrete-time signals are defined only for integer indices n. 
The integer restriction on n results in some sinusoidal signals not being periodic 
at all. For example, there is no integer N such that the signal x3[n] = cos(n) satisfies 
the condition X3 [n + N] = X3 [n] for all n. These and other properties of discrete-time 
sinusoids that run counter to their continuous-time counterparts are caused by the 
limitation of the time index n to integers for discrete-time signals and systems. 
When we combine the condition of Eq. (2.17) with our previous observation that 
Wo and (wo + 2n r) ar~ indistinguishable frequencies, it becomes clear that there are 
N distinguishable frequencies for which the corresponding sequences are periodic with 
period N. One set of frequencies is Wk 
2nk/N, k = 0,1, ... , N - 1. These properties 
of complex exponential and sinusoidal sequences are basic to both the theory and the 
design of computational algorithms for discrete-time Fourier analysis, and they will be 
discussed in more detail in Chapters 8 and 9. 
Related to the preceding discussion is the fact that the interpretation of high 
and low frequencies is somewhat different for continuous-time and discrete-time sinu­
soidal and complex exponential signals. For a continuous-time sinusoidal signal xU) = 
ii cos(Qot + cp), as Qo increases, x(t) oscillates progressively more rapidly. For the 
discrete-time sinusoidal signal x[n] = A cos(won + cp), as Wo increases from Wo = 0 to­
ward Wo = rc, x[n] oscillates progressively more rapidly. However, as wo increases from 
Wo 
:rr to Wo = 2n, the oscillations become slower. This is illustrated in Figure 2.5. In 

... lllllllllllllllllllllllllllllll~" ~ D"' "" ~ 2. 
o 
n 
(a) 
Wo = n/8 or Wo = 15n/8 
n 
(b) 
= nl4 or Wo = 7nl4
Wo 
o 
n 
(c) 
Wo 
o 
n 
(d) 
= n 
Figure 2.5 
cos wOn for several different values of w O. As w 0 increases from 
zero toward n (parts a-d), the sequence oscillates more rapidly. As wo increases 
from n to 2n (parts d-a), the oscillations become slower. 
16 

11 
Section 2,2 
Discrete-Time Systems 
fact, because of the periodicity in Wo of sinusoidal and complex exponential sequences, 
Wo = 2rr is indistinguishable from Wa = 0, and, more generally, frequencies around 
Wa = 2rr are indistinguishable from frequencies around wa = O. As a consequence, for 
sinusoidal and complex exponential signals, values of Wo in the vicinity of wo = 2rrk 
for any integer value of k are typically referred to as low frequencies (relatively slow 
oscillations), whereas values of wo in the vicinity of Wo = (rr + 2rrk) for any integer 
value of k are typically referred to as high frequencies (relatively rapid oscillations). 
2.2 DISCRETE-TIME SYSTEMS 
A discrete-time system is defined mathematically as a transformation or operator that 
maps an input sequence with values x [n] into an output sequence with values y[n]. This 
can be denoted as 
y[n] = T{x[n]J 
(2.19) 
and is indicated pictorially in Figure 2.6. Equation (2.19) represents a rule or formula 
for computing the output sequence values from the input sequence values. It should 
be emphasized that the value of the output sequence at each value of the index n may 
depend on input samples x[n] for all values of n, i.e., y at time n can depend on all or 
part of the entire sequence x, The following examples illustrate some simple and useful 
systems. 
Figure 2.6 
Representation of a 
discrete-time system, i.e" a 
transformation that maps an input
~I T{.\ L--­
sequence x[n) into a unique output
~L..__...J~ 
sequence YIn). 
Example Z.Z The Ideal Delay System 
The ideal delay system is defined by the equation 
yEn] = x[n 
nd], 
-00 < n < 00, 
(2.20) 
where nd is a fixed positive integer representing the delay of the system. In other words, 
the ideal delay system shifts the input sequence to the right by nd samples to form the 
output. If, in Eq. (2.20), nd is a fixed negative integer, then the system would shift the 
input to the left by Ind I samples, corresponding to a time advance. 
In the system of Example 2.2, only one sample of the input sequence is involved 
in determining a certain output sample. In the following example, this is not the case. 

18 
Chapter 2 
Discrete-Time Signals and Systems 
Example 2.3 
Moving Average 
The general moving-average system is defined by the equation 
1 
1>12  
y[n] =: 
L x[n - k]  
M 1 + M 2 + 1 k=- 1>1 1  
1 
! 
.
x[n + M d + x[n + M 1 - 1] + ... + x[n] 
(2.21)
Ml+M2+ 1 
+x[n -1] + ... +x[n - MzJ}. 
This system computes the nth sample of th\: output sequence as the average of (M 1 + 
M 2+1) samples of the inputsequence around the nth sample. Figure 2.7 shows an input 
sequence plotted as a function ofa dummy index k and the samples (solid dots) involved 
in the computationofthe output sample y[n]for n = 7. M 1 = 0, and M 2 = 5. The out­
put sample y[7] is equal to one-sixth of the sum of all the samples between the vertical 
dotted lines. To compute y[8J, both dotted lines would move one sample to the right. 
x[kJ 
o 
k 
Figure 2.7 
Sequence values involved in computing amoving average with M1 = 0 
and M2 
5. 
Classes of systems are defined by placing constraints on the properties of the 
transformation T {.}. Doing so often leads to very general mathematical representations, 
as we will see. Of particular importance are the system constraints and properties, 
discussed in Sections 2.2.1-2.2.5. 
2.2.1 Memoryless Systems 
A system is referred to as memoryless if the output y[nl at every value of n depends 
only on the input x[n] at the same value of n. 
Example 2.4 A Memoryless System 
An example of a memoryless system is a system for which x[n] and y[ll] are related by 
y[n] =: (x[n])z, 
for each value of n. 
(2.22) 

Section 2.2 
Discrete-Ti me Systems 
The system in Example 2.2 is not memoryless unless lid 
0; in particular, that 
system is referred to as having "memory" whether nd is positive (a time delay) or 
negative (a time advance). The moving average system in Example 2.3 is not memory less 
unless M 1 = M2 = O. 
2.2.2 Linear Systems 
The class of linear systems is defined by the principle ofsuperposition. IfYl[n] and Y2[n] 
are the responses of a system when XI [n] and xz[n] are the respective inputs, then the 
system is linear if and only if 
(2.23a) 
and 
T{ax[n]} = a T{x [n]} = ay[n], 
(2.23b) 
where a is an arbitrary constant. The first property is the additivity property, and the 
second the homogeneity or scaling property. These two properties together comprise 
the principle of superposition, stated as 
for arbitrary constants a and b. This equation can be generalized to the superposition 
of many inputs. Specifically, if 
x[n] = L akxdnJ, 
k 
(2.25a) 
then the output of a linear system will be 
y[nl = L akYk[n], 
k 
(2.25b) 
where Ydn] is the system response to the input xk[n]. 
By using the definition of the principle of superposition, it is easily shown that the 
systems of Examples 2.2 and 2.3 are linear systems. (See Problem 2.39.) An example of 
a nonlinear system is tpe system in Example 2.4. 
Example 2.5 
The Accumulator System 
The system defined by the input-output equation 
n 
y[n] = L x[k] 
(2.26) 
k=-oo 
is called the accumulator system. since the output at time n is the accumulation or 
sum of the present and all previous input samples. The accumulator system is a linear 
system. Since this may not be intuitively obvious, it is a useful exercise to go through 
the steps of more formally showing this. We begin by defining two arbitrary inputs 
xdn] and x2[n] and their corresponding outputs 
(2.24) 

20 
Chapter 2 
Discrete-Time Signals and Systems 
n 
Yl[n] = L xl[k], 
(2.27) 
k=-oo 
n 
Y2[n] = L x2[kJ. 
(2.28) 
k=-oo 
When the input is x3[n] = aXl[n] + bX2[n], the superposition principle requires the 
output Y3[n] 
aYdnJ + byz[n] for all possible choices of a and b. We can show this 
by starting from Eq. (2.26): 
n 
Y3[n] = L x3[k], 
(2.29) 
k=-oo 
nL (axl[k] + bX2[kl). 
(2.30) 
k=-x; 
n 
n 
a L xl[k] + b L x2lk], 
(2.31) 
k=-oo 
k=-oo 
aYl[n] +bY2In]. 
(2.32) 
Thus, the accumulator system of Eq. (2.26) satisfies the superposition principle for all 
inputs and is therefore linear. 
Example 2.6 A Nonlinear System 
Consider the system defined by 
wIn] 
loglO (lxln]I)· 
(2.33) 
This system is not linear. To prove this, we only need to find one counterexample­
that is, one set of inputs and outputs which demonstrates that the system violates 
the superposition principle, Eq. (2.24). The inputs xl[n] 
1 and x2[n] 
10 are a 
counterexample. However, the output for Xl [n] + x2[n] 
11 is 
loglO{l + 10) 
loglO(11) =1= 10glO(1) + loglO{lO) = 1. 
Also, the output for the first signal is wl[n] = 0, whereas for the second, w2[n] = 1. The 
scaling property of linear systems requires that, since x2[n] = lOxl[n], if the system is 
linear, it must be true that w2[n] 
lOwlln]. Since this is not so for Eq. (2.33) for this 
set of inputs and outputs, the system is not linear. 
2.2.3 Time-Invariant Systems 
A time-invariant system (often referred to equivalently as a shift-invariant system) is 
a system for which a time shift or delay of the input sequence causes a corresponding 
shift in the output sequence. Specifically, suppose that a system transforms the input 
sequence with values x[nJ into the output sequence with values yen]. Then, the system 
is said to be time invariant if, for all no. the input sequence with values xl[n] = x[n -no] 
produces the output sequence with values Yl[n] = yen - noJ. 
As in the case of linearity, proving that a system is time invariant requires a general 
proofmaking no specific assumptions about the input signals. On the other hand, proving 
non-time invariance only requires a counter example to time invariance. All of the 
systems in Examples 2.2-2.6 are time invariant. The style of proof for time invariance 
is illustrated in Examples 2.7 and 2.8. 

21 
Section 2.2 
Discrete-Time Systems 
Example 2.7 The Accumulator as a Time-Invariant System 
Consider the accumulator from Example 2.5. We define xI[n] = x[n -
no]. To show 
time invariance, we solve for both yIn-no] and y JIn] and compare them to see whether 
they are equal. First, 
1I~1I0 
yIn 
nO] = L x[k]. 
(2.34 ) 
k=~oo 
Next, we find 
11 
ydn ] = 
xl[k] 
(2.35)
L 
k=~oo 
11 
L x[k 
no]. 
(2.36) 
k=~oo 
Substituting the change of variables k 1 = k -
nO into the summation gives 
11-110 
Yl In] = L 
X[kl]. 
(2.37) 
kl=~OO 
Since the index k in Eq. (2.34) and the index k I in Eq. (2.37) are dummy indices of 
summation, and can have any label, Eqs. (2.34) and (2.37) are equal and therefore 
Yt[n] = yIn 
no]. The aceumulator is a time-invariant system. 
The following example illustrates a system that is not time invariant. 
Example 2.8 The Compressor System 
The system defined by the relation 
yIn] = xlMn], 
-00 < n < 00, 
(2.38) 
with M a positive integer, is ealled a compressor. Specifically, it discards (M -
1) 
samples out of M; i.e., it creates the output sequence by selecting every Mth sample. 
This system is not time invariant. We can show that it is not by considering the response 
Ytln] to the inputxl [n1= xln - no]. For the system to be time invariant, the output of 
the system when the input is xI[n] must be equal to yIn 
no]. The output y tin] that 
results from the input xI[n] can be directly computed from Eq. (2.38) to be 
YI[n] 
xl[Mn] 
x[Mn - nO]' 
(2.39) 
Delaying the output yIn] by no samples yields 
yIn ­ no] 
x[M(n 
no)]. 
(2.40) 
Comparing these two outputs, we see that yIn 
no] is not equal to y 1[n] for all M and 
nO, and therefore, the system is not time invariant. 
It is also possible to prove that a system is not time invariant by finding a single 
counterexample that violates the time-invarianee property. For instanee, a counterex­
ample for the compressor is the case when M = 2, x[n] = o[n], and xtln] = &[n -I). 
For this choice of inputs and M, yIn] 
8[n], but Y1[n] = 0; thus, it is clear that 
Yl[n] f-o yIn 
1] for this system. 

22 
Chapter 2 
Discrete-Time Signals and Systems 
2.2.4 Causality 
A system is causal if, for every choice ofno, the outputsequence value at the index n = no 
depends only on the input sequence values for n :::: no. This implies that if Xl [n] = x2[n] 
for n :::: no, then Yl [n] = Y2[n] for n :::: no. That is, the system is nonanticipative. The 
system of Example 2.2 is causal for nd :::: 0 and is noncausal for nd < O. The system of 
Example 2.3 is causal if - M 1 :::: 0 and M 2 ::: 0; otherwise it is noncausal. The system of 
Example 2.4 is causal, as is the accumulator of Example 2.5 and the nonlinear system 
in Example 2.6. However, the system of Example 2.8 is noncausal if M > 1, since 
y[l] = x[M]. Another noncausal system is given in the following example. 
1 
Example 2.9 The Forward and Backward Difference Systems 
The system defined by the relationship 
y[n] = x[n + 1] - x[n] 
(2.41) 
is referred to as the forward difference system. This system is not causal, since the 
current value of the output depends on a future value of the input. The violation of 
causality can be demonstrated by considering the two inputs Xl [n] 
o[n 
1] and 
x2[n] 
0 and their corresponding outputs Yl[n] = o[n] 
o[n 
1] and Y2[n] = 0 
for all n. Note that xl[n] = x2[n] for n S 0, so the definition of causality requires 
that Yl[nJ 
Y2[n] for n 
0, which is clearly not the case for n = O. Thus, by this 
counterexample, we have shown that the system is not causal. 
The backward difference system, defined as 
y[n] = x[n]- x[n 
IJ. 
(2.42) 
has an output that depends only on the present and past values of the input. Because 
y[noJ depends only on x[nO] and x[nO 
1], the system is causal by definition. 
2.2.5 Stability 
A number ofsomewhat different definitions are commonly used for stability of a system. 
Throughout this text, we specifically use bounded-input bounded-output stability. 
A system is stable in the bounded-input, bounded-output (BlBO) sense if and 
only if every bounded input sequence produces a bounded output sequence. The input 
x[n] is bounded if there exists a fixed positive finite value B x such that 
Ix[nJi :::: Bx < 00, 
foraHn. 
(2.43) 
Stability requires that, for every bounded input, there exists a fixed positive finite value 
By such that 
\y[n]1 :::: By < 00, 
for all n. 
(2.44) 
It is important to emphasize that the properties we have defined in this section are 
properties of systems, not of the inputs to a system. That is, we may be able to find 
inputs for which the properties hold, but the existence of the property for some inputs 
does not mean that the system has the property. For the system to have the property, it 
must hold for allinputs. For example, an unstable system may have some bounded inputs 
for which the output is bounded, but for the system to have the property of stability, it 

23 
Section 2.3 
LTI Systems 
must be true that for all bounded inputs, the output is bounded. If we can find just one 
input for which the system property does not hold, then we have shown that the system 
does not have that property. The following example illustrates the testing of stability 
for several of the systems that we have defined. 
Example 2.10 Testing for Stability or Instability 
The system of Example 2.4 is stable. To see this, assume that the input x[n1is bounded 
such that Ix[nll ::: Bx for all n. Then Iy[nll = Ix[n]12 ::: B;:. Thus, we can choose 
By = B;: and prove that y[n] is bounded. 
Likewise, we can see that the system defined in Example 2.6 is unstable, since 
y[n] 
loglO(lx[nJl) = -00 for any values of the time index n at which x[n] 
0, even 
though the output will be bounded for any input samples that are not equal to zero. 
The accumulator, as defined in Example 2.5 by Eq. (2.26), is also not stable. For 
example, consider the case when x [n] = u[n], which is clearly bounded by B x = 1. For 
this input, the output of the accumulator is 
n 
y[n] 
L u[k] 
(2.45) 
k=-oo 
n < O.
{ 0, 
(2.46)
(n + 1), 
n 2: O. 
There is no finite choice for By such that (n + 1) ::: Bv < 00 for all n; thus, the system 
is unstable. 
Using similar arguments, it can be shown that the systems in Examples 2.2, 2.3, 
2.8, and 2.9 are all stable. 
2.3 LTI SYSTEMS 
As in continuous time, a particularly important class of discrete-time systems consists of 
those that are both linear and time invariant. These two properties in combination lead 
to especially convenient representations for such systems. Most important, this class 
of systems has significant signal-processing applications. The class of linear systems is 
defined by the principle o:(superposition in Eq. (2.24). If the linearity property is com­
bined with the representation of a general sequence as a linear combination of delayed 
impulses as in Eq. (2.5), it follows that a linear system can be completely characterized 
by its impulse response. Specifically, let hk[n] be the response of the system to the input 
8[n - k], an impulse occurring at n = k. Then, using Eq. (2.5) to represent the input, it 
follows that 
y[n] 
T LE, x[k]8[n - k]I ' 
(2.47) 
and the principle of superposition in Eq. (2.24), we can write 
00 
00 
y[n] = L x[k]T{8[n - k]} = L x[k]hk[n]. 
(2.48) 
k=-oo 
k=-oo 

24 
Chapter 2 
Discrete-Time Signals and Systems 
According to Eq. (Z.48), the system response to any input can be expressed in terms 
of the responses of the system to the sequences 8[n 
kJ. If only linearity is imposed, 
then hk[n] will depend on both nand k, in which case the computational usefulness 
of Eq. (Z.48) is somewhat limited. We obtain a more useful result if we impose the 
additional constraint of time invariance. 
The property of time invariance implies that if h[n] is the response to 8[n], then 
the response to 8[n 
k] is h[n - k]. With this additional constraint, Eq. (Z.48) becomes 
yIn] = L
00 
x[k]h[n - k], 
for all n. 
(Z.49) 
k=-oo 
As a consequence of Eq. (Z.49), an LTI system is completely characterized by its impulse 
response h[n1in the sense that, given the sequences x[n] and h[n] for all n, it is possible 
to use Eq. (Z.49) to compute each sample of the output sequence y[n]. 
Equation (Z.49) is referred to as the convolution sum, and we represent this by 
the operator notation 
yIn] 
x[nJ * h[n]. 
(Z.50) 
1be operation of discrete-time convolution takes two sequences x[nl and h[n] and pro­
duces a third sequence yIn]. Equation (Z.49) expresses each sample of the output se­
quence in terms of all of the samples of the input and impulse response sequences. 
The notation of Eq. (Z.50) for the operation of convolution as shorthand for 
Eq. (Z.49) is convenient and compact but needs to be used with caution. The basic 
definition of the convolution of two sequences is embodied in Eq. (Z.49) and any use 
of the shorthand form in Eq. (Z.50) should always be referred back to Eq. (Z,49). For 
example, consider yIn - no]. From Eq. (Z.49) we see that 
yIn 
no] 
L
00 
x[k]h[n - no - k] 
(Z.51 ) 
k=-oo 
or in short hand notation 
yIn - no] = x[n] * h[n 
no] 
(Z.52) 
Substituting (n - no) for n in Eq. (Z.49) leads to the correct result and conclusion, but 
blindly trying the same substitution in Eq. (Z.50) does not. In fact, x[n - no] * h[n 
no] 
results in y[n 
Zno]. 
The derivation of Eq. (Z.49) suggests the interpretation that the input sample at 
n 
k, represented as x[k]8[n - k], is transformed by the system into an output sequence 
x[k]h[n 
kJ, for -00 < n < 00, and that, for each k, these sequences are superimposed 
(summed) to form the overall output sequence. This interpretation is illustrated in Fig­
ure 2.8, which shows an impulse response, a simple input sequence having three nonzero 
samples, the individual outputs due to each sample, and the composite output due to all 

-2 
0 
x[nJ 
3 
n 
• • • • ,II I T 
0 
2 
h[nJ 
• • • n 
• • • I• •
-2 
0 
0 
x_2[n] =x[-2]B[n + 2] 
• • • • • n 
xoln] =x[O]B[n] 
n 
I 
Y_2[nl =x[-2Jh[n + 2] 
• • • II • • • • •
-2 
0 
n 
II 
yo[n] =x[O]h[n] 
• • • • 
I • • •
0 
2 
n 
• • • • • • •
o 
• 
• • • • 
Y3(n] = x[3]h[n 
3J 
-2 
0 
n 
• • • • 1 
-2 
yIn] = Y_2[n] + yo[n] + Y3[n] 
n 
3 
Figure 2.8 
Representation of the output of an LTI system as the superposition 
of responses to individual samples of the input. 
25 

26 
Chapter 2 
Discrete-Time Signals and Systems 
the samples in theinput sequence. Specifically, x(n] can be decomposed as the sum of the 
three sequences x(-2]8(n +2], x (O]8[n], andx[3]8[n - 3] representing the three nonzero 
values in the sequence x[n]. The sequences x[-2]h[n + 2], x [O]h [n], and x(3]h(n 
3] 
are the system responses to x[-218[n + 2], x[O]8[n], and x[3]8[n - 3], respectively. The 
response to x[n] is then the sum of these three individual responses. 
Although the convolution-sum expression is analogous to the convolution integral 
of continuous-time linear system theory, the convolution sum should not be thought of 
as an approximation to the convolution integral. The convolution integral is mainly a 
tool of mathematical analysis in continuous-time linear system theory; we will see that 
the convolution sum, in addition to its analytical importance, often serves as an explicit 
realization of a discrete-time linear system. Thus, it is important to gain some insight 
into the properties of the convolution sum in actual calculations. 
The preceding interpretation of Eq. (2.49) emphasizes that the convolution sum 
is a direct result of linearity and time invariance. However, a slightly different way of 
looking at Eq. (2.49) leads to a particularly useful computational interpretation. When 
viewed as a formula for computing a single value of the output sequence, Eq. (2.49) 
dictates that y[n] (i.e., the nth value of the output) is obtained by multiplying the input 
sequence (expressed as a function of k) by the sequence whose values are h(n - k], 
-00 < k < 00 for any fixed value of n, and then summing all the values of the products 
x(k]h[n - k], with k a counting index in the summation process. Therefore, the operation 
of convolving two sequences involves doing the computation specified by Eq. (2.49) for 
each value of n, thus generating the complete output sequence y[nl, -00 < n < 00. The 
key to carrying out the computations of Eq. (2.49) to obtain y[n] is understanding how 
to form the sequence h[n 
k], -00 < k < 00, for all values of n that are of interest. To 
this end, it is useful to note that 
h[n 
k] = h(-(k 
n)]. 
(2.53) 
To illustrate the interpretation of Eq. (2.53), suppose h[k] is the sequence shown in 
Figure 2.9(a) and we wish to find h[n 
k] = h[-(k 
n)]. Define hr[k] to be h[-k], 
which is shown in Figure 2.9(b). Next, define h2[k] to be hl[k], delayed, by n samples 
on the k axis, i.e., h2[k] = hI [k -
n]. Figure 2.9(c) shows the sequence that results from 
delaying the sequence in Figure 2.9(b) by n samples. Using the relationship between 
hI [k] and h[k], we can show that h2[k] = hI [k - n] = h[-(k - n)] = h[n 
k], and thus, 
the bottom figure is the desired signal. To summarize, to compute h[n -
k] from h[k], 
we first reverse h[k] in time about k = 0 and then delay the time-reversed signal by n 
samples. 
To implement discrete-time convolution, the two sequences x[k] and h[n - k] are 
multiplied together sample by sample for -00 < k < 00, and the products are summed 
to compute the output sample y[n]. To obtain another output sample, the origin of the 
sequence h[-k] is shifted to the new sample position, and the process is repeated. This 
computational procedure applies whether the computations are carried out numerically 
on sampled data or analytically with sequences for which the sample values have simple 
formulas. The following example illustrates discrete-time convolution for the latter case. 

• 
• 
• 
• 
• • 
• 
• • • • • 
Section 2.3 
LTI Systems 
27 
h[k] 
I I I 1 II I I I t 
-3 
0 
6 
k 
(a) 
hl[k 1=h[-k] =h[O-k] 
t I I I I 1 1 I I I 
-6 
0 
3 
k 
(b) 
h2[kj=h t [k n] 
h[n-k]=h[-(k n)] 
II1 I I I • 
n 6 
0 
n 
n+3 
k 
(c) 
Figure 2.9 
Forming the sequence h[n - k]. (a) The sequence h[k] as afunction  
of k. (b) The sequence h[- k] as a function of k. (c) The sequence h[n 
k] =  
h[ 
(k - m] as afunction of k for n 
4.  
Example 2.11 
Analytical Evaluation of the Convolution Sum 
Consider a system with impulse response 
h[n] = urn] 
urn - N)  
I, 
0 
n < N -1,  
= { 0, 
othe~se.  
The input is  
an, 
n 2: 0, 
x[n] 
{ 0, 
n < 0,  
or equivalently,  
x[n) = anu[n]. 
Ib find the output at a particular index n, we must form the sums over all k of the 
product x[k]h[n -
k]. In this case, we can find formulas for y[n] for different sets 
of values of n. To do this, it is helpful to sketch the sequences x[k] and h[n -
k] as 
functions of k for different representative values of n. For example, Figure 2.10(a) 
shows the sequences x[k] and h[n 
k], plotted for n a negative integer. Clearly, all 

28 
Chapter 2 
Discrete-Time Signals and Systems 
o h[n-k] 
n 
x 
x 
o 
x x[k] 
x x 
k 
n 
(N -1) 
(a) 
! 1 
o 
x 
x 
n 
k 
n (N-1) 
(b) 
, 
< , 
< I!I III 
x x x x x x x 
11 
k 
n-(N-l) 
(c) 
ylnl 
tl I [ I
]1 
IIII1I J
• • • • • • • • • • • o 
k 
N-l 
(d) 
Figure 2,10 
Sequence involved in computing adiscrete convolution. (a}-(c) The 
sequences x[k] and h[n-k] as afunction of kfor different values of n. (Only nonzero 
samples are shown.) (d) Corresponding output sequence as afunction of n. 
negative values of n give a similar picture; i.e., the nonzero portions of the sequences 
x[k] and h[n - k] do not overlap, so 
y[n] = 0, 
n < o. 
Figure 2.10(b) illustrates the two sequences when 0 S nand n 
N + 1 SO. These two 
conditions can be combined into the single condition 0 S n S N - 1. By considering 
Figure 2.1O(b), we see that since 
x[k]h[n 
kl=ak , 
forO 
kSn 
when 0 S n S N 
1. 

29 
Section 2.3 
LTI Systems 
it follows that 
11 
y[n] = I,>k, 
for 0 :S: n :S: N - 1. 
(2.54) 
k=O 
The limits on the sum can be seen direetly from Figure 2.10(b). Equation (2.54) shows 
that y[n] is the sum of n + 1 terms of a geometric series in which the ratio of terms is 
a. This sum can be expressed in closed form using the general formula 
N2 
~Nl 
~N7+1
"I\' 
k 
~ 
-~­
(2.55)
L 
a = 
1-a 
' 
k=Nl 
Applying this formula to Eq. (2.54), we obtain 
y[nJ 
O:S:n:s:N-1. 
(2.56) 
Finally, Figure 2.1O(c) shows the two sequences when 0 < n - N + 1 or N -1 < n. As 
before, 
x[kJh[n - k] = ak , 
n -N + 1:s: k:s: n, 
but now the lower limit on the sum is n - N + 1, as seen in Figure 2.1O(c). Thus, 
n 
k
y[n] 
L 
a , 
for N 
1 < n. 
(2.57) 
k=n-N+l 
Using Eq. (2.55), we obtain 
y[n] 
or 
N+1 
(
1 aN)
n
y[n] = a -
~
. 
(2.58) 
Thus, because of the piecewise-exponential nature of both the input and the unit 
sample response, we have been able to obtain the following closed-form expression 
for y[nl as a funetion of the index n: 
0, 
n < 0, 
an+1
1 
1.
y[n] 
I-a . 
(2.59) 
n-N+l(~) N -1 < n.
a 
1 
a 
. 
This sequence is shown in Figure 2.1O(d). 
Example 2.11 illustrates how the convolution sum can be computed analytically 
when the input and the impulse response are given by simple formulas. In such cases, 
the sums may have a compact form that may be derived using the formula for the sum of 
a geometric series or other "closed-form" formulas.2 When no simple form is available, 
2Such results are discussed, for example. in Grossman (1992) and Jolley (2004). 

30 
Chapter 2 
Discrete-Time Signals and Systems 
the convolution sum can still be evaluated numerically using the technique illustrated 
in Example 2.11 whenever the sums are finite, which will be the case if either the input 
sequence or the impulse response is of finite length, i.e., has a finite number of nonzero 
samples. 
2.4 PROPERTIES OF LINEAR TIME-INVARIANT SYSTEMS 
Since all LTI systems are described by the convolution sum of Eq. (2.49), the proper­
ties of this class of systems are defined by the properties of discrete-time convolution. 
Therefore, the impulse response is a complete characterization of the properties of a 
specific LTI system. 
Some general properties of the class of LTI systems can be found by considering 
properties of the convolution operation.3 For example, the convolution operation is 
commutative: 
x[n] * h[n] 
h[n] *x[n]. 
(2.60) 
This can be shown by applying a substitution of variables to the summation index in 
Eq. (2.49). Specifically, with m 
n - k, 
-00 
00 
y[n] = L x[n 
m]h[m] = L h[m]x[n 
m] = h[n] *x[n], 
(2.61) 
m=oo 
m=-\X) 
so the roles of x[n] and h[n] in the summation are interchanged. That is, the order of 
the sequences in a convolution operator is unimportant; hence, the system output is 
the same if the roles of the input and impulse response are reversed. Accordingly, an 
LTI system with input x[n] and impulse response h[n] will have the same output as an 
LTI system with input h[n] and impulse response x[n]. The convolution operation also 
distributes over addition; i.e., 
x[n] * (hl[n] + h2[n]) 
x[n] * hl[n] + x[n] * h2[n]. 
(2.62) 
This follows in a straightforward way from Eq. (2.49) and is a direct result of the lin­
earity and commutativity of convolution. Equation (2.62) is represented pictorially in 
Figure 2.11, where Figure 2.1l(a) represents the right-hand side of Eq. (2.62) and Fig­
ure 2.11(b) the left-hand side. 
The convolution operation also satisfies the associative property, i.e., 
y[n] = (x[n] * hl [nD * h2[n] = x [n] * (hl [n] * h2[nD. 
(2.63) 
Also since the convolution operation is commutative, Eq. (2.63) is equivalent to 
y[n] = x[n] * (h2[n] * hl[n]) = (x[n] * h2[n]) * hl[n]. 
(2.64) 
These equivalences are represented pictorially in Figure 2.12. Also, Eqs. (2.63) and 
(2.64) clearly imply that if two LTI systems with impulse responses hl[n] and h2[n] are 
cascaded in either order, the equivalent overall impulse response h[n1is 
h[n] = hl[n] * h2[n] = h2[n] * hlln]. 
(2.65) 
3In our discussion below and throughout the text, we use the shorthand notation of Eq. (2.50) for 
the operation of convolution. but again emphasize that the properties of convolution are derived from the 
definition ofEq. (2.49). 

31 
Section 2.4 
Properties of Linear Time-Invariant Systems 
x[n] 
(a) 
Figure 2.11 
(a) Parallel combination of  
LTI systems. (b) An equivalent system.  
~ 
~] 
(a) 
~ 
~~] 
(b) 
Figure 2.12 
(a) Cascade combination 
of two LTI systems. (b) Equivalent 
cascade. (c) Single equivalent system. 
In a parallel combination, the systems have the same input, and their outputs 
are summed to produce an overall output. It follows from the distributive property of 
convolution that the connection of two LTI systems in parallel is equivalent to a single 
system whose impulse response is the sum of the individual impulse responses; i.e., 
h[n] = hdn] + h2[n]. 
(2.66) 
The constraints of linearity and time invariance define a class of systems with very 
special properties. Stability ana causality represent additional properties, and it is often 
important to know whether an LTI system is stable and whether it is causal. Recall from 
Section 2.2.5 that a stable system is a system for which every bounded input produces a 
bounded output. LTI systems are stable if and only if the impulse response is absolutely 
summable, i.e., if 
Bh = 
00L Ih[k]1 < 00. 
(2.67) 
k=-oo 
This can be shown as follows. From Eq. (2.61), 
ly[n]1 = Ik~OO h[k]x[n ­ k]1 :::: k~CJO Ih[k]llx[n ­ k]l· 
(2.68) 
If x[n] is bounded, so that 

32 
Chapter 2 
Discrete-Time Signals and Systems 
then substituting B x for I.l[n - k] Ican only strengthen the inequality. Hence, 
ly[n]1 :::: BxBh. 
(2.69) 
Thus, yIn] is bounded if Eq. (2.67) holds; in other words. Eq. (2.67) is a sufficient con­
dition for stability. To show that it is also a necessary condition, we must show that if 
B h 
00, then a bounded input can be found that will cause an unbounded output. Such 
an input is the sequence with values 
h*[-n] 
---, h[n] 1': 0, 
.l[n] = 
Ih[-nll 
(2.70) 
0, 
h[n] 
0,
1 
where h*[n] is the complex conjugate,of h[n]. The sequence x[n] is clearly bounded by 
unity. However, the value of the output at n = 0 is 
ex; 
oc 
Ih[k]e 
. 
y[O] 
L 
.l[-k]h[k] 
L1h[k]1 = Bh. 
(2.71) 
k=····oo 
k=-oo 
Therefore, if B h = 00, it is possible for a bounded input sequence to produce an un­
bounded output sequence. 
The class of causal systems was defined in Section 2.2.4 as comprising those systems 
for which the output y[no] depends only on the input samples x [n], for n 
no. It follows 
from Eq. (2.49) or Eq. (2.61) that this definition implies the condition 
h[n] 
0, 
n < 0, 
(2.72) 
for causality of LTI systems. (See Problem 2.69.) For this reason, it is sometimes conve­
nient to refer to a sequence that is zero for n < 0 as a causal sequence, meaning that it 
could be the impulse response of a causal system. 
To illustrate how the properties of LTI systems are reflected in the impulse re­
sponse, let us consider again some of the systems defined in Examples 2.2-2.9. First, 
note that only the systems of Examples 2.2, 2.3, 2.5, and 2.9 are linear and time in­
variant. Although the impulse response of nonlinear or time-varying systems can be 
found by simply using an impulse input, it is generally of limited interest, since the 
convolution-sum formula and Eqs. (2.67) and (2.72), expressing stability and causality, 
do not apply to such systems. 
First, let us determine the impulse responses of the systems in Examples 2.2, 2.3, 
2.5, and 2.9. We can do this by simply computing the response of each system to 8[n], 
using the defining relationship for the system. The resulting impulse responses are as 
follows: 
Ideal Delay (Example 2.2) 
h[n] 
8[n - nd], 
nd a positive fixed integer. 
(2.73) 
Moving Average (Example 23j 
1 
M2 
h[n] = 
'\' 8[n 
k]
Ml +M2+ 1 ~ 
k=-Mj 
(2.74)
1 
_____ , -Ml < n < M2. 
1 
Ml+M2+ 1 
-­
0, 
otherwise.  

33 
Section 2.4 
Properties of Linear Time-Invariant Systems 
Accumulator (Example 2.5) 
n 2: O.
h[nJ t 8[k)={~: 
u[n). 
(2.75)
n < 0,
k=-oo 
Forward Difference (Example 2.9) 
h[nl = 8[n + lJ 
8[n]. 
(2.76) 
Backward Difference (Example 2.9) 
h[n] = 8[n] - 8[n -1]. 
(2.77) 
Given the impulse responses of these basic systems [Eqs. (2.73)-(2.77)], we can 
test the stability of each one by computing the sum 
00 
Bh = L Ih[nll· 
n=-oo 
For the ideal delay, moving-average, forward difference, and backward difference ex­
amples, it is clear that B II < 00, since the impulse response has only a finite number of 
nonzero samples. In general, a system with a finite-duration impulse response (hence­
forth referred to as an FIR system) will always be stable, as long as each of the impulse 
response values is finite in magnitUde. The accumulator, however, is unstable because 
00 
Bh = Lu[nl = 00. 
n=O 
In Section 2.2.5, we also demonstrated the instability of the accumulator by giving an 
example of a bounded input (the unit step) for which the output is unbounded. 
The impulse response of the accumulator has infinite duration. This is an example 
of the class of systems referred to as infinite-duration impulse response (IIR) systems. 
An example of an IIR system that is stable is a system whose impulse response is 
h[n] = anu[nl with lal < 1;. In this case, 
00 
(2.78)  
If lal < 1, the formula for the sum of the terms of an infinite geometric series gives 
1 
< 00. 
(2.79)
1 -Ial 
If, on the other hand, lal 2: 1, then the sum is infinite and the system is unstable. 
To test causality of the LTI systems in Examples 
2.3,2.5, and 2.9, we can check 
to see whether h[n] = 0 for n < O. As discussed in Section 2.2.4, the ideal delay [nd 2: 0 
in Eq. (2.20)] is causal. If nd < 0, then the system is noncausal. For the moving average, 
causality requires that - M 1 2: 0 and M 2 2: O. The accumulator and backward difference 
systems are causal, and the forward difference system is noncausal. 

34 
Chapter 2 
Discrete-Time Signals and Systems 
(a) 
(b) 
Figure 2.13 
Equivalent systems found 
by using the commutative property of 
(c) 
convolution. 
The concept of convolution as an operation between two sequences leads to the 
simplification of many problems involving systems. A particularly useful result can be 
stated for the ideal delay system. Since the output ofthe delay system is y[n] = x[n -nd], 
and since the delay system has impulse response h[n] = o[n 
nd], it follows that 
x[n] *o[n - nd] 
o[n - nd] * x[n] 
x[n - nd]. 
(2.80) 
That is, the convolution of a shifted impulse sequence with any signal x[n] is easily 
evaluated by simply shifting x[n] by the displacement of the impulse. 
Since delay is a fundamental operation in the implementation of linear systems, 
the preceding result is often useful in the analysis and simplification ofinterconnections 
of LTI systems. As an example, consider the system of Figure 2.13(a), which consists 
of a forward difference system cascaded with an ideal delay of one sample. According 
to the commutative property of convolution, the order in which systems are cascaded 
does not matter, as long as they are linear and time invariant. Therefore, we obtain 
the same result when we compute the forward difference of a sequence and delay the 
result (Figure 2.13a) as when we delay the sequence first and then compute the forward 
difference (Figure 2.13b). Also, as indicated in Eq. (2.65) and in Figure 2.12, the overall 
impulse response of each cascade system is the convolution of the individual impulse 
responses. Consequently, 
h[n] 
(o[n + 1] - o[nD *o[n - 1] 
o[n - 1] * (S[n + 1] - S[nD 
(2.81) 
= S[n] 
S[n 
1]. 
Thus, h[n] is identical to the impulse response of the backward difference system; that 
is, the cascaded systems of Figures 2.13(a) and 2.13(b) can be replaced by a backward 
difference system, as shown in Figure 2.13(c). 
Note that the non causal forward difference systems in Figures 2.13(a) and (b) 
have been converted to causal systems by cascading them with a delay. In general, any 
noncausal FIR system can be made causal by cascading it with a sufficiently long delay. 

35 
Section 2.5 
Linear Constant-Coefficient Difference Equations 
Figure 2.14 
An accumulator in 
cascade with abackward difference. 
Since the backward difference is the
Backward­
Accumulator 
inverse system for the accumulator, the
difference 
cascade combination is equivalent to
system 
yIn] 
x[n]
xln] 
system 
the identity system. 
Another example of cascaded systems introduces the concept of an inverse system. 
Consider the cascade of systems in Figure 2.14. The impulse response of the cascade 
system is 
h[n] = urn] * (o[n] 
o[n 
1]) 
u[n] - /l[n 
1]  
(2.82) 
= ,sen]. 
That is, the cascade combination of an accumulator followed by a backward differ­
ence (or vice versa) yields a system whose overall impulse response is the impulse. 
Thus, the output of the cascade combination will always be equal to the input, since 
x[n] *o[n] = x[n]. In this case, the backward difference system compensates exactly for 
(or inverts) the effect of the accumulator; that is, the backward difference system is the 
inverse system for the accumulator. From the commutative property of convolution, the 
accumulator is likewise the inverse system for the backward difference system. Note 
that this example provides a system interpretation of Eqs. (2.7) and (2.9). In general, if 
an LTI system has impulse response h [n1, then its inverse system, if it exists, has impulse 
response hj[n] defined by the relation 
h[n] *hj[n] = hj[n] *hen] = ,sIn].  
(2.83) 
Inverse systems are useful in many situations where it is necessary to compensate 
for the effects of a system. In general, it is difficult to solve Eq. (2.83) directly for 
hj[n], given h[n]. However, in Chapter 3, we will see that the z-transform provides a 
straightforward method of finding the inverse of an LTI system. 
2.5  LINEAR CONSTANT-COEFFICIENT DIFFERENCE 
EQUATIONS 
An important class of LTI systems consists of those systems for which the input x[n] 
and the output YIn] satisfy an Nth-order linear constant-coefficient difference equation 
of the form 
N 
M
L akY[n - k] = L bmx[n - m].  
(2.84) 
m=O 
The properties discussed in Section 2.4 and some of the analysis techniques introduced 
there can be used to [md difference equation representations for some of the LTI systems 
that we have defined. 

Chapter 2 
Discrete-Time Signals and Systems
36 
Example 2.12 
Difference Equation Representation of the 
Accumulator 
The accumulator system is defined by 
n 
y[n] = L x[k]. 
(2.85) 
k=-oo 
Toshowthat the input and output satisfy a difference equation ofthe form ofEq. (2.84), 
we rewrite Eq. (2.85) as 
n-l 
y[n] ,,;, x[n] + L x[k] 
(2.86) 
k=-oo 
Also, from Eq. (2.85) 
n-l 
y[n 
1] = L x[k]. 
(2.87) 
k=-oo 
Substituting Eq. (2.87) into Eq. (2.86) yields 
y[n] = x[n] + y[n - 1], 
(2.88) 
and equivalently, 
y[n] - y[n -I] = x[n]. 
(2.89) 
Thus, in addition to satisfying the defining relationship of 
(2.85), the input 
and output of an accumulator satisfy a linear constant-coefficient difference equation 
of the form Eq. (2.84), with N 
1, ao 
1, al = -1, M 
0, and bo = 1. 
The difference equation in the form ofEq. (2.88) suggests a simple implementation 
of the accumulator system. According to Eq. (2.88), for each value of n, we add the 
current input value x[n] to the previously accumulated sum y[n -1]. This interpretation 
of the accumulator is represented in block diagram form in Figure 2.15. 
Equation (2.88) and the block diagram in Figure 2.15 are referred to as a recursive 
representation of the system, since each value is computed using previously computed 
values. This general notion will be explored in more detail later in this section. 
y[n] 
Figure 2.15 
Block diagram of a 
recursive difference equation 
representing an accumLllator. 
"­

37 
Section 2.5 
Linear Constant-Coefficient Difference Equations 
Example 2.13 Difference Equation Representation of the 
Moving-Average System 
Consider the moving-average system of Example 2.3, with M 1 
0 so that the system 
is causal. In this case, from Eq. (2.74), the impulse response is 
1 
h[nl = 
(u[n] 
urn 
M2 -11). 
(2.90)
(M2 + 
from which it follows that 
1 
M2 
y[n] = (M..,+l) Lx[n-k], 
(2.91) 
~ 
k=O 
which isa special case ofEq. (2.84), with N = O,ao = I,M 
M2, and bk = 1/(M2+1) 
forO:S: k:s: M2. 
Also, the impulse response can be expressed as 
h[n] = 
1 
(8[n] - 8[n 
M2 -1]) * urn], 
(2.92)
(M2 + 
which suggests that the causal moving-average system can be represented as the cas­
cade system of Figure 2.16. We can obtain a difference equation for this block diagram 
by noting first that 
1 
xl[n] = 
(xLn] - x[n 
M2 -11). 
(2.93)
(M2 + 1) 
From Eq. (2.89) of Example 2.12, the output of the accumulator satisfies the difference 
equation 
y[n1 y[n 
1] = xl[n], 
so that 
1 
y(nj-y[n 
1]= (M2+ )(x[n] 
x[n-M2-l]). 
(2.94)
1
Again, we have a difference equation in the form of Eq. (2.84), but this time N 
1, 
ao = 1, al = -1, M = M2 + 1 and bo 
-bMz+l 
1/(M2 + 1), and bk ootherwise. 
x[n] 
Attenuator 
1 
(M2 + 1) 
(M2 + 1) 
sample 
+ 
delay 
Figure 2.16 Block diagram of the recursive form of a moving-average system. 
In Example 2.13, we showed two different difference-equation representations 
of the moving-average system. In Chapter 6, we will see that many distinct difference 
equations can be used to represent a given LTI input-output relation. 

38 
Chapter 2 
Discrete-Time Signals and Systems 
Just as in the case of linear constant-coefficient differential equations for contin­
uous-time systems, without additional constraints or other information, a linear constant­
coefficient difference equation for discrete-time systems does not provide a unique 
specification of the output for a given input. Specifically, suppose that, for a given input 
xp[n], we have determined by some means one output sequence yp[n], so that an equa­
tion of the form of Eq. (2.84) is satisfied. Then, the same equation with the same input 
is satisfied by any output of the form 
y[n] 
yp[n] + Yh[n], 
(2.95) 
where Yh [n] is any solution to Eq. (2.84) with x[n] = 0, i.e., a solution to the equation 
NL akYh[n 
k] = O. 
(2.96) 
k=O 
Equation (2.96) is called the homogeneous difference equation and Yh[n] the homoge­
neous solution. The sequence Yh [n] is in fact a member of a family of solutions of the 
form 
N 
Yh[n] = LA mZ;:', 
(2.97) 
m=l 
where the coefficients A m can be chosen to satisfy a set of auxiliary conditions on y[n]. 
Substituting Eq. (2.97) into Eq. (2.96) shows that the complex numbers Zm must be roots 
of the polynomial 
N 
A(z) = Lakz~k. 
(2.98) 
k=O 
Le., A(Zm) = 0 for m = 1,2, .... N. Equation (2.97) assumes that all N roots of the 
polynomial in Eq. (2.98) are distinct. The form of terms associated with multiple roots 
is slightly different, but there are always N undetermined coefficients. An example of 
the homogeneous solution with multiple roots is considered in Problem 2.50. 
Since Yh [n] has N undetermined coefficients, a set of N auxiliary conditions is 
required for the unique specification of y[n] for a given x [n]. These auxiliary conditions 
might consist of specifying fixed values of y[n] at specific values of n, such as y[-1], 
y[-2], ... , y[-N], and then solving a set of N linear equations for the N undetermined 
coefficients. 
Alternatively, if the auxiliary conditions are a set of auxiliary values of y[n], the 
other values of y[n] can be generated by rewriting Eq. (2.84) as a recurrence formula, 
i.e., in the form 
N 
!vi 
'" ak 
'" bk
y[n] = - L.. 
y[n - k] + L.. -x[n - k]. 
(2.99) 
k=l ao 
k=() ao 
If the input x [n] for all n, together with a sct of auxiliary values, say, y[-1], y [ - 2], ... , 
y[-N ], is specified, then y[O] can be determined from Eq. (2.99). With y[O]. y[-1], ... , 
y[- N +1] now available, y[l] can then be calculated, and so on. When this procedure is 
used, y[n] is said to be computed recursively; i.e., the output computation involves not 
only the input sequence, but also previous values of the output sequence. 

39 
Section 2.5 
Linear Constant-Coefficient Difference Equations 
To generate values of yen] for n < -N (again assuming that the values y[-l], 
y[-2], ... , y[-N] are given as auxiliary conditions), we can rearrange Eq. (2.84) in the 
form 
N-l 
M 
y[n - N] = - L ak y[n - k] + L bk x[n - k], 
(2.100) 
k=O aN 
k=O aN 
from which y[-N - 1], y[-N 
2], ... can be computed recursively in the backward 
direction. 
Our principal interest in this text is in systems that are linear and time invariant, 
in which case the auxiliary conditions must be consistent with these additional require­
ments. In Chapter 3, when we discuss the solution of difference equations using the 
z-transform, we implicitly incorporate conditions of linearity and time invariance. As 
we will see in that discussion, even with the additional constraints of linearity and time 
invariance, the solution to the difference equation, and therefore the system, is not 
uniquely specified. In particular, there are, in general, both causal and noncausal LTI 
systems consistent with a given difference equation. 
Ifa system is characterized by a linear constant-coefficient difference equation and 
is further specified to be linear, time invariant, and causal, then the solution is unique. 
In this case, the auxiliary conditions are often stated as initial-rest conditions. In other 
words, the auxiliary information is that ifthe input x[n] is zero for n less than some time 
no, then the output yen] is constrained to be zero for n less than no. l11is then provides 
sufficient initial conditions to obtain yen] for n ?: no recursively using Eq. (2.99). 
To summarize, for a system for which the input and output satisfy a linear constant­
coefficient difference equation: 
• The output for a given input is not uniquely specified. Auxiliary information or 
conditions are required. 
•  If the auxiliary information is in the form of N sequential values of the output, 
later values can be obtained by rearranging the difference equation as a recursive 
relation running forward in n, and prior values can be obtained by rearranging the 
difference equation as a recursive relation running backward in n. 
•  Linearity, time invariance, and causality of the system will depend on the auxiliary 
conditions. If an additional condition is that the system is initially at rest, then the 
system will be linear, time invariant, and causal. 
The preceding discussion assumed that N ?: 1 in Eq. (2.84). If, instead, N = 0, 
no recursion is required to use the difference equation to compute the output, and 
therefore, no auxiliary conditions are required. 1nat is, 
y[n] 
) x[n - 
(2.101 ) 
t (bk 
k], 
k=O ao 
Equation (2.101) is in the form of a convolution, and by setting x[n] = 8[n], we see that 
the corresponding impulse response is 
L
M (!k) 8[n
hen] 
k], 
k=O 
0 
is 

40  
Chapter 2 
Discrete-Time Signals and Systems 
or 
O::Sn::SM,
h[n] = {(::}  
(2.102) 
0, 
otherwise. 
The impulse response is obviously finite in duration. Indeed, the output of any FIR 
system can be computed nonrecursively where the coefficients are the values of the 
impulse response sequence. The moving-average system of Example 2.13 with M 1 = 0 
is an example of a causal FIR system. An interesting feature of that system was that we 
also found a recursive equation for the output. In Chapter 6 we will show that there are 
many possible ways ofimplementing a desired signal transformation. Advantages of one 
method over another depend on practical" considerations, such as numerical accuracy, 
data storage, and the number of multiplications and additions required to compute each 
sample of the output. 
2.6  FREQUENCY-DOMAIN REPRESENTATION OF 
DISCRETE-TIME SIGNALS AND SYSTEMS 
In the previous sections, we summarized some of the fundamental concepts of the theory 
of discrete-time signals and systems. For LTI systems, we saw that a representation of the 
input sequence as a weighted sum of delayed impulses leads to a representation of the 
output as a weighted sum ofdelayed impulse responses. As with continuous-time signals, 
discrete-time signals may be represented in a number of different ways. For example, 
sinusoidal and complex exponential sequences playa particularly important role in 
representing discrete-time signals. This is because complex exponential sequences are 
eigenfunctions of LTI systems, and the response to a sinusoidal input is sinusoidal with 
the same frequency as the input and with amplitude and phase determined by the system. 
These fundamental properties of LTI systems make representations of signals in terms 
of sinusoids or complex exponentials (i.e., Fourier representations) very useful in linear 
system theory. 
2.6.1 Eigenfunctions for Linear Time-Invariant Systems 
The eigenfunction property of complex exponentials for discrete-time systems follows 
directly from substitution into Eq. (2.61). Specifically, with input x[n] = eiwn for -00 < 
n < 00, the corresponding output of an LTI system with impulse response h[n] is easily 
shown to be 
y[n] = H(eiw)eiwn, 
(2.103) 
where 
H(eiw) = 
00L h[k]e-iwk . 
(2.104) 
k=-oo 
Consequently, eiwn is an eigenfunction of the system, and the associated eigen­
value is H(eiw ). From Eq. (2.103), we see that H(eiw) describes the change in complex 
amplitude of a complex exponential input signal as a function of the frequency w. The 

41 
Section 2.6 
Frequency-Domain Representation of Discrete-Time Signals and Systems 
eigenvalue H (el(V) is the frequency response of the system. In general, H (elw ) is complex 
and can be expressed in terms of its real and imaginary parts as 
H(e jW ) = HR(elw ) + jH/(e}W) 
(2.105) 
or in terms of magnitude and phase as 
H(elw) 
IH(eiw)leiL H(e
1W ). 
(2.106) 
Example 2.14 Frequency Response of the Ideal Delay 
System 
As a simple and important example, consider the ideal delay system defined by 
yin] 
x[n-nd], 
(2.107) 
where nd is a fixed integer. With input x[n] 
eiwn from Eq. (2.107), we have 
yin) = e}w(n-nd) 
e-iWlldeiam. 
The frequency response of the ideal delay is therefore 
iwnd
H(ejW ) 
e-
. 
(2.108) 
As an alternative method of obtaining the frequency response. recall that the 
impulse response for the ideal delay system is h[nl = S[n - ndl Using Eq. (2.104), we 
obtain 
H(eiw )= L
(X) 
S[n 
ndle-jwn=e-jwnd. 
n=-OO 
The real and imaginary parts of the frequency response are 
HR(eiw) = eoS(wnd) , 
(2.109a) 
H/(ejW ) = 
sin(wnd)' 
(2.109b) 
The magnitude and phase are 
(2.1lOa) 
(2.1l0b) 
In Section 2.7, we will show that a broad class of signals can be represented as a 
linear combination of complex exponentials in the form 
x [n] 
2:::Cxkeiwkn. 
(2.111) 
k 
From the principle of superposition and Eq. (2.103), the corresponding output of an 
LTI system is 
y[n] = L (XkH(ejwk)eiwkn. 
(2.112) 
k 
Thus, if we can find a representation of x[n] as a superposition of complex exponential 
sequences, as in Eq. (2.111), we can then find the output using Eq. (2.112) if we know the 

42 
Chapter 2 
Discrete-Time Signals and Systems 
frequency response of the system at all frequencies Wk. The following simple example 
illustrates this fundamental property of LTI systems. 
Example 2.15 
Sinusoidal Response of LT. Systems 
Let us consider a sinusoidal input 
A '''' ' 
A 
,'" 
' 
x[n] 
A cos(won + ¢) = ZeN eJ(J)on + "2e- jop e- )(J)on. 
(2.113) 
From 
(2.103), the response to xl [nJ 
(A /2)ei <Pe j (J)on is 
YI[n] = H(e)(J)O)~e)</>ej(J)on, 
(2.114a) 
The response to x2ln] = (A /2)e-)</>e- j(J)on is 
, 
A 
•
'A. 
Y2[n] = H(e-](J)O)"2e-N e- j(J)on. 
(2.114b) 
Thus, the total response is 
y[n] = ~[H(ej(J)O)ej</>ej(J)on + H(e-J(J)O)e-J<Pe-jl1)on). 
(2.115) 
If hln] is real, it can be shown (see Problem 2.78) that H(e-J(J)O) = H*(ej(J)O). Conse­
quently, 
y[n] = A IH(ej(J)O)1 cos(won + ¢ + e), 
(2.116) 
where e 
LH(ej I1)O) is the phase of the system function at frequency woo 
For the simple example of the ideal delay, IH (ej(J)O)1 = 1 and e 
-wOnd, as we 
determined in Example 2.14. Therefore, 
y[n] = A cos(wOn + ¢ 
wOnd) 
(2.117) 
=A cos[wo(n 
nd) + ¢], 
which is identical to what we would obtain directly using the definition of the ideal 
delay system. 
The concept of the frequency response of LTI systems is essentially the same for 
continuous-time and discrete-time systems. However, an important distinction arises 
because the frequency response of discrete-time LTI systems is always a periodic func­
tion of the frequency variable W with period 2:;r. To show this, we substitute w +2:;r into 
Eq. (2.104) to obtain 
H (eJ«(J)+21l'») 
L
OQ 
h[n]e-j «(J)+21l')n 
(2.118) 
n=-oo 
Using the fact that e±j21l'n = 1 for n an integer, we have 
e-J«(J)+2:r)n 
e-)(J)ne-J21l'n = e-J(J)n. 

43 
Section 2.6 
Frequency-Domain Representation of Discrete-Time Signals and Systems 
Therefore, 
for all w, 
(2.119) 
and, more generally, 
for r an integer. 
(2.120) 
That is, H (elw ) is periodic with period 2Jr. Note that this is obviously true for the ideal 
delay system, since e-} (w+21!'lnd = 
when nd is an integer. 
The reason for this periodicity is related directly to our earlier observation that 
the sequence 
-00 < n < 00, 
is indistinguishable from the sequence 
-00 < n < 00. 
Because these two sequences have identical values for all n, the system must respond 
identically to both input sequences. This condition requires that 
(2.119) hold. 
Since H (el{J)) is periodic with period 2Jr , and since the frequencies wand w+2Jr are 
indistinguishable, it follows that we need only specify H (el{J) over an interval of length 
2Jr, e.g., 0 :s w :s 2Jr or -Jr < w :s Jr. The inherent periodicity defines the frequency 
response everywhere outside the chosen interval. For simplicity and for consistency with 
the continuous-time case, it is generally convenient to specify H(ej{J)) over the interval 
-Jr < w :s Jr. With respect to this interval, the "low frequencies" are frequencies 
close to zero, whereas the "high frequencies" are frequencies close to ±Jr. Recalling 
that frequencies differing by an integer multiple of 2Jr are indistinguishable, we might 
generalize the preceding statement as follows: The "low frequencies" are those that are 
close to an even multiple of Jr, while the "high frequencies" are those that are close to 
an odd multiple of Jr, consistent with our earlier discussion in Section 2.t. 
An important class of LTI systems includes those systems for which the frequency 
response is unity over a certain range of frequencies and is zero at the remaining fre­
quencies, corresponding to ideal frequency-selective filters. The frequency response of 
an ideallowpass filter is shown in Figure 2.17( a). Because of the inherent periodicity of 
the discrete-time frequency response, it has the appearance of a multiband filter, since 
frequencies around w = 2Jr are indistinguishable from frequencies around w 
O. In 
effect, however, the frequency response passes only low frequencies and rejects high 
frequencies. Since the frequency response is completely specified by its behavior over 
the interval -Jr < W :s Jr, the ideallowpass filter frequency response is more typically 
shown only in the interval -Jr < w :s Jr, as in Figure 2.17(b). It is understood that 
the frequency response repeats periodically with period 2Jr outside the plotted intervaL 
With this implicit assumption, the frequency responses for ideal highpass, bandstop, and 
bandpass filters are as shown in Figures 2.18(a), (b), and (c), respectively. 

15)  
-27T -27T + We 
-7T 
-We 
We 
7T 
27T -
We 
27T 
W 
(a) 
15")  
-7T 
-We 
We 
7T 
W 
(b) 
Figure 2.17 
Ideallowpass filter showing (a) periodicity ofthe frequency response 
and (b) one period of the periodic frequency response. 
1[,,('1-) 
-7T 
-we 
0 
We 
7T 
W 
(a) 
Hb,(e jW) 
1 
-7T 
-Wb 
-Wa 
0 
Wa 
Wb 
7T 
W 
(b) 
n f'('n 
Figure 2.18 
Ideal frequency-selective 
filters. (a) Highpass filter. (b) Bandstop 
filter. (c) Bandpass filter. In each case,
-7T 
-Wb 
-Wa 
0 
Wa 
Wb 
7T 
W 
the frequency response is periodic with 
(c) 
period 21T. Only one period is shown. 
44 

45 
Section 2.6 
Frequency-Domain Representation of Discrete-Time Signals and Systems 
Example 2.16 Frequency Response of the Moving-Average 
System 
The impulse response of the moving-average system of Example 2.3 is  
1 
-M} ~ n ~ M 2,  
h[nJ 
M 1 + M2 + l'  
1
0, 
otherwise.  
Therefore, the frequency response is 
(2.121) 
For the causal moving average system, Ml = 0 and Eq. (2.121) can be expressed as 
(2.122) 
Using Eq. (2.55), Eq. (2.122) becomes 
. 
1 (1 
e-iUJ (M2+1»)
H(e JUJ ) = 
. 
. 
M2 + 1 
1 - e-JUJ 
1 
(ejUJ (M2+1)/2 _ e- jUJ(M2+1)/2)e-jUJ(M2+1)/2 
= M2 + 1 
(e jUJ / 2 - r jUJ/2)rjUJ/2 
1 
sin[w(M2 + 1)/2] -jUJM2/2 
(2.123)
M2+1 
sinw/2 
e 
. 
Themagnitude and phase of H (e jUJ ) for this case, with M2 = 4, are shown in Figure 2.19. 
If the moving-average filter is symmetric, i.e., if M} = M2, then Eq. (2.123) is 
replaced by 
H(e j (}» 
_ 
1 
sin[w(2M2 + 1)/2] 
(2.124) 
-
2M2 + 1 
sin(w/2) 
-71' 
-2'IT 
'IT 
w 
5 
w 
Figure 2.19 
(a) Magnitude and (b) phase of the frequency response of the 
moving-average system for the case M1 = 0 and M2 
4. 

46 
Chapter 2 
Discrete-Time Signals and Systems 
Note that in both cases H(e jW) is periodic, as is required of the frequency re­
sponse of a discrete-time system. Note also that IH (e jW ) Ifalls off at "high frequencies" 
and LH(eiw ), i.e., the phase of H (eilJ), varies linearly with w. This attenuation of the 
high frequencies suggests that the system will smooth out rapid variations in the in­
put sequence; in other words, the system is a rough approximation to a lowpass filter. 
This is consistent with what we would intuitively expect about the behavior of the 
moving-average system. 
2.6.2 Suddenly Applied Complex Exponential Inputs 
We have seen that complex exponential inputs of the form eiwn for -00 < n < 00 
produce outputs of the form H(ejw)eiwn for LTI systems. Models of this kind are im­
portant in the mathematical representation of a wide range of signals, even those that 
exist only over a finite domain. We can also gain additional insight into LTI systems by 
considering inputs of the form 
x[n] = eiwnu[n],  
(2.125) 
i.e., complex exponentials that are suddenly applied at an arbitrary time, which for 
convenience here we choose as n = 0. Using the convolution sum in Eq. (2.61), the 
corresponding output of a causal LTI system with impulse response h[n] is 
°,  
n < 0, 
y[n] = 
(Eh[k]e-JWk) eiwn, 
n:::: O.
1 
If we consider the output for n :::: 0, we can write 
y[n] 
f:h[kJe-iWk) eJwn (f: h[kJe-iWk) eiwn 
(2.126)
( 
k=O  
k=n+l 
= H(ejW)eJwn -
(  f: h[kJe-JWk) eiwn . 
(2.127) 
k=n+l 
From Eq. (2.127), we see that the output consists of the sum of two terms, i.e., y[n] = 
yss[n] + Yt[n]. The first term, 
yss[n] = H(ejW)ejwn, 
is the steady-state response. It is identical to the response of the system when the input 
is e)wn for all n. In a sense, the second term, 
Yt[nJ = - L
00 
h[k]e-jwkejwn, 
k=n+l 
is the amount by which the output differs from the eigenfunction result. This part corre­
sponds to the transient response, because it is clear that in some cases it may approach 
zero. To see the conditions for which this is true, let us consider the size of the second 
term. Its magnitude is bounded as follows: 
L 
00 
h[k]e-iwkejwn
IYt[n]1 = 
< L
00 
Ih[kll· 
(2.128) 
k=n+l 

Section 2.6 
Frequency-Domain Representation of Discrete-Time Signals and Systems 
47 
From Eq. (2.128), it should be clear that if the impulse response has finite length, so that 
h[n] = 0 except for 0 < n 
M, then the term Yt[n] = 0 for n + 1 > M, or n > M - 1. 
In this case, 
forn > M 
1. 
When the impulse response has infinite duration, the transient response does not disap­
pear abruptly, but if the samples of the impulse response approach zero with increasing 
n, then Yt[n] will approach zero. Note that Eq. (2.128) can be written 
00 
00 
IYt[n]1 = L 
00 
h[k]e-jwkejwfl :s L Ih[k]l:s L Ih[k]l. 
(2.129) 
k=n+1 
k=O 
lilat is, the transient response is bounded by the sum of the absolute values of all of the 
impulse response samples. If the right-hand side of Eq. (2.129) is bounded, i.e., if 
00L Ih[k]1 < 00, 
k=O 
then the system is stable. From Eq. (2.129), it follows that, for stable systems, the tran­
sient response must become increasingly smaller as n -+ 00. Thus, a sufficient condition 
for the transient response to decay asymptotically is that the system be stable. 
Figure 2.20 shows the real part of a complex exponential signal with frequency 
(V = 2rr/10. The solid dots indicate the samples x[k] of the suddenly applied complex 
(a) 
k 
(b) 
Figure 2.20 
Illustration of a real part of suddenly applied complex exponential 
input with (a) FIR and (b) IIR. 

48  
Chapter 2 
Discrete-Time Signals and Systems 
exponential, while the open circles indicate the samples of the complex exponential that 
are "missing," i.e., that would be nonzero if the input were of the form ejwn for all n. The 
shaded dots indicate the samples of the impulse response h[n 
k] as a function of k for 
n = 8. In the finite-length case shown in Figure 2.20(a), it is clear that the output would 
consist only of the steady-state component for n 2: 8, whereas in the infinite-length case, 
it is clear that the "missing" samples have less and less effect as n increases, owing to 
the decaying nature of the impulse response. 
The condition for stability is also a sufficient condition for the existence of the 
frequency response function. To see this, note that, in general, 
oc 
jwk I 
00 
00
IH(ejW)1 
L hfkle-
S k~OO Ih[k]e-j,,,k l s k~OO Ih[k]l, 
so the general condition 
L
00 
Ih[k]1 < 00 
k=-oo 
ensures that H(e jW ) exists. It is no surprise that the condition for existence of the fre­
quency response is the same as the condition for dominance of the steady-state solution. 
Indeed, a complex exponential that exists for a11n can be thought of as one that is ap­
plied at n = -00. The eigenfunction property of complex exponentials depends on 
stability of the system, since at finite n, the transient response must have become zero, 
so that we only see the steady-state response H(ejlll)ejwn for all finite n. 
2.7  REPRESENTATION OF SEQUENCES BY FOURIER 
TRANSFORMS 
One of the advantages of the frequency-response representation of an LTI system is 
that interpretations of system behavior such as the one we made in Example 2.16 often 
follow easily. We will elaborate on this point in considerably more detail in Chapter 5. 
At this point, however, let us return to the question of how we may find representations 
of the form of 
(2.111) for an arbitrary input sequence. 
Many sequences can be represented by a Fourier integral of the form 
1 I]!: 
. . 
x[n] = -
X (eJW)eJW/1dw.  
(2.130)
2Ji  _]!: 
where 
jwn
X (e jW ) 
L
00 
x[n]e-
(2.131) 
n=-oo 
. 
Equations (2.130) and (2.131) together form a Fourier representation for the sequence. 
Equation (2.130), the inverse Fourier transform, is a synthesis formula. That is, it repre­
sents x[n] as a superposition of infinitesimally small complex sinusoids of the form 
1 
.. 
2Ji X (eFll)eJwndw, 
...... 

49 
Section 2.7 
Representation of Sequences by Fourier Transforms 
with (j) ranging over an interval of length 2n and with X (el "') determining the relative 
amount of each complex sinusoidal component. Although, in writing Eq. (2.130), we 
have chosen the range of values for (j) between -n and +n, any interval of length 2n 
can be used. Equation (2.131), the Fourier transjorm,4 is an expression for computing 
X (el"') from the sequence x[n], i.e., for analyzing the sequence x[n] to determine how 
much of each frequency component is required to synthesize x[n] using Eq. (2.130). 
In general, the Fourier transform is a complex-valued function of (j). As with the 
frequency response, we may either express X (el "') in rectangular form as 
(2.132a) 
or in polar form as 
(2.132b) 
With IX (el"') I representing the magnitude and LX (el"') the phase. 
The phase LX (e l "') is not uniquely specified by Eq. (2.132b), since any integer 
multiple of 2n may be added to LX (el "') at any value of (j) without affecting the result 
of the complex exponentiation. When we specifically want to refer to the principal value, 
i.e., LX (el"') restricted to the range of values between -n and +n, we denote this as 
ARG[X (el"')]. If we want to refer to a phase function that is a continuous function of 
w for 0 < (j) < n, i.e., not evaluated modulo 2n, we use the notation arg[X (el "')]. 
As is clear from comparing Eqs. (2.104) and (2.131), the frequency response of 
an LTI system is the Fourier transform of the impulse response. The impulse response 
can be obtained from the frequency response by applying the inverse Fourier transform 
integral; i.e., 
1 1]( . . 
h[n] =  -
H(eJ"')eJ",ndw. 
(2.133)
2n _]( 
As discussed previously, the frequency response is a periodic function of w. Like­
wise, the Fourier transform is periodic in (j) with period 2n. A Fourier series is commonly 
used to represent periodic signals, and it is worth noting that indeed, Eq. (2.131) is of the 
form of a Fourier series for the periodic function X (el "'). Eq. (2.130), which expresses 
the sequence values x[n] in terms of the periodic function X (el "'), is of the form of the 
integral that would be used to obtain the coefficients in the Fourier series. Our use of 
Eqs. (2.130) and (2.131) focuses on the representation of the sequence x[n]. Neverthe­
less, it is useful to be aware of the equivalence between the Fourier series representation 
of continuous-variable periodic functions and the Fourier transform representation of 
discrete-time signals, since all the familiar properties of Fourier series can be applied, 
with appropriate interpretation of variables, to the Fourier transform representation of 
a sequence. (Oppenheim and Willsky (1997), McClellan, Schafer and Yoder (2003).) 
Determining the class of signals that can be represented by Eq. (2.130) is equiv­
alent to considering the convergence of the infinite sum in Eq. (2.131). That is, we 
are concerned with the conditions that must be satisfied by the terms in the sum in 
Eq. (2.131) such that 
for all (j). 
4Eq. (2.131) is sometimes more explicitly referred to as the discrete-time Fourier transform, or DTFf. 
particularly when it is important to distinguish it from the continuous-time Fourier transform. 

Chapter 2 
Discrete-Time Signals and Systems
50 
where X (elw ) is the limit as M ---+ 00 of the finite sum 
M 
XM(elw ) = L x[n]e-lwn . 
(2.134) 
n=-M 
A sufficient condition for convergence can be found as follows: 
IX (elW)1 = In~oo x[n]e-lwnl 
:s L
00 
Ix[n]lle-lwnl 
n=-oo 
:s L
00 
Ix[n]1 < 00. 
n=-oo 
Thus, if x[n] is absolutely summable, then X (elw ) exists. Furthermore, in this case, the 
series can be shown to converge uniformly to a continuous function of w (Korner (1988), 
Kammler (2000)). Since a stable sequence is, by definition, absolutely summable, all sta­
ble sequences have Fourier transforms. It also follows, then, that any stable system, i.e., 
one having an absolutely summable impulse response, will have a finite and continuous 
frequency response. 
Absolute summability is a sufficient condition for the existence of a Fourier trans­
form representation. In Examples 2.14 and 2.16, we computed the Fourier transforms 
of the impulse response of the delay system and the moving average system. The im­
pulse responses are absolutely summable, since they are finite in length. Clearly, any 
finite-length sequence is absolutely summable and thus will have a Fourier transform 
representation. In the context of LTI systems, any FIR system will be stable and there­
fore will have a finite, continuous frequency response. However, when a sequence has 
infinite length, we must be concerned about convergence of the infinite sum. The fol­
lowing example illustrates this case. 
Example 2.17 Absolute Summability for a Suddenly-Applied 
Exponential 
Consider x[nJ = anu[nJ. The Fourier transform of this sequence is 
00 
00 
X (elw ) = L 
ane-lwn = L(ae-lw)n 
n=O 
n=O 
1 
if lae-lwi < 1 or lal < 1. 
1- ae-lw 
Clearly, the condition lal < 1is the condition for the absolute summability of x[nJ; i.e., 
00 
1 
L 
lain = --- < 00 
if lal < 1. 
(2.135)
1-lal
n=O 
Absolute summability is a sufficient condition for the existence of a Fourier trans­
form representation, and it also guarantees uniform convergence. Some sequences are 
..  

51 
Section 2.7 
Representation of Sequences by Fourier Transforms 
not absolutely summable, but are square summable, i.e., 
coL Ix[nll2 < 00. 
(2.136) 
n=-oo 
Such sequences can be represented by a Fourier transform if we are willing to relax the 
condition of uniform convergence of the infinite sum defining X (e jW ). Specifically, in 
this case, we have mean-square convergence; that is, with 
coL x[nle- jwn
X (e jW ) = 
(2.137a) 
n=-oo 
and 
XM(e jW ) = 
ML x[nle­ jwn , 
(2.137b) 
n=-M 
it follows that 
(2.138)  
In other words, the error IX (ejW)-XM(ejW)1 may not approach zero at each value of 
(j) as M --* 00, but the total "energy" in the error does. Example 2.18 illustrates this case. 
Example 2.18 
Square-Summability for the Ideal Lowpass 
Filter 
In this example we determine the impulse response of the ideallowpass filter discussed 
in Section 2.6. The frequency response is 
H 
(e jw ) = {I, 
Iwi < We, 
(2.139)
lp 
0, 
We < Iwi :s n, 
with periodicity 2n also understood. The impulse response hlp[n1can be found using 
the Fourier transform synthesis equation (2.130): 
hlp[n] = ~ jwe ejwndw 
2n -We 
= 
1 
__. 
2nJn 
[. Jwe
eJwn 
-We = 1· 
. 
__. (eJwcn _e-JWen ) 
2nJn 
(2.140) 
sin Wen 
-()C < n < 00. 
nn 
We note that, since hlp[n] is nonzero for n < 0, the ideallowpass filter is noncausal. 
Also, hlp[n] is not absolutely summable. The sequence values approach zero as n ---+ 00, 
but only as lin. This is because Hlp(e jW ) is discontinuous at w = We. Since hlp[n] is 
not absolutely summable, the infinite sum 
co 
. 
""' smwen -jwn
L 
---e 
nn 
n=-oo 
does not converge uniformly for all values of w. To obtain an intuitive feeling for this, 
let us consider HM (e jW ) as the sum of a finite number of terms: 
M 
. 
jw 
""' sm Wen - jwn
HM(e 
) = L 
---e 
. 
(2.141) 
nn
n=-M 

52 
Chapter 2 
Discrete-Time Signals and Systems 
The function HM(e jW ) is evaluated in Figure 2.21 for several values of M. Note that 
as M increases, the oscillatory behavior at W = We (often referred to as the Gibbs 
phenomenon) is more rapid, but the size of the ripples does not decrease. In fact, 
it can be shown that as M ~ 00, the maximum amplitude of the oscillations does 
not approach zero, but the oscillations converge in location toward the points W = 
±wc. Thus, the infinite sum does not converge uniformly to the discontinuous function 
Hlp(ejW ) ofEq. (2.139). However, hlp[n], as given in Eq. (2.140), is square summable, 
and correspondingly, HM(e jW ) converges in the mean-square sense to H1p(ejW ); i.e., 
lim in IHlp(ejW) 
HM(ejW )12dw = o. 
M-+oo -n 
Although the error between HM(ejW ) and Hlp(ejW ) as M ~ 00 might seem unimpor­
tant because the two functions differ only at W = We, we will see in Chapter 7 that the 
behavior of finite sums such as Eq. (2.141) has important implications in the design of 
discrete-time systems for filtering. 
HM(e j",), M = 1 
HM(ej"'),M 3 
(a) 
HM(e jW), M = 7 
IIA 
All 
"VV
IV 
71' W 
-71' 
'''-We 
o 
Well 
11'W 
(b) 
HM(ejW),M 
19 
(c) 
(d) 
Figure 2.21 
Convergence of the Fourier transform. The oscillatory behavior at 
=
W 
We is often called the Gibbs phenomenon. 
It is sometimes useful to have a Fourier transform representation for certain se­
quences that are neither absolutely summable nor square summable. We illustrate sev­
eral of these in the following examples. 
Example 2.19 Fourier Transform of a Constant 
Consider the sequence x[n] 
1 for all n. This sequence is neither absolutely summable 
nor square summable, and Eq. (2.131) does not converge in either the uniform or 

53 
Section 2.7 
Representation of Sequences by Fourier Transforms 
mean-square sense for this case. However, it is possible and useful to define the Fourier 
transform of the sequence x[n] to be the periodic impulse train 
L
00 
2.rro«(v + 2nr). 
(2.142) 
r=-oo 
The impulses in this case are functions of a continuous variable and therefore are of 
"infinite height. zero width, and unit area," consistent with the fact that Eq. (2.131) does 
not converge in any regular sense. (See Oppenheim and Willsky (1997) for a discussion 
of the definition and properties of the impUlse function.) The use of Eq. (2.142) as a 
Fourier representation of the sequence x[n] = 1 is justified principally because formal 
substitution of Eq. (2.142) into Eq. (2.130) leads to the correct result. Example 2.20 
represents a generalization of this example. 
Example 2.20 Fourier Transform of Complex Exponential 
Sequences 
Consider a sequence x[n] whose Fourier transform is the periodic impulse train 
00L 2.rro(w - wa + 2.rrr). 
(2.143) 
r=-oo 
We show in this example that x[n] is the complex exponential sequence ej(£}On, with 
-.rr<wa::: n . 
We can determine x[n] by substituting X (e jm ) into the inverse Fourier trans­
form integral of Eq. (2.130). Because the integration of X (e j (£}) extends only over one 
period, from -n < w < .rr, we need include only the r = 0 term from 
(2.143). 
Consequently, we can write 
1 jre 
(2.144)
x[n] 
-2 
2.rro(w 
]'[ 
-Jr 
From the definition of the impulse function, it follows that  
x[n] = ejU)On 
for any n.  
For wa 
0, this reduces to the sequence considered in Example 2.19. 
Clearly, x [n] in Example 2.20 is not absolutely summable, nor is it square summable, 
and IX (e j (£}) I is not finite for all w. Thus, the mathematical statement 
n=-oo 
00 
L 
2Jro(w -
WO + 2Jrr) 
(2.145) 
r=-CX)  
must be interpreted in the context of generalized functions (Ughthill, 1958). Using that  
theory, the concept of a Fourier transform representation can be extended to the class  
of sequences that can be expressed as a sum of discrete frequency components, such as  
x[n] 
Lakej(£}kn, 
-oo<n<oo. 
(2.146) 
k 
From the result of Example 2.20, it follows that 
00 
X (e j (£}) = 
L 
L 
2Jrako(w -
Wk + 2Jrr) 
(2.147) 
r=-oo k 
is a consistent Fourier transform representation of x[n] in Eq. (2.146). 

54  
Chapter 2 
Discrete-Time Signals and Systems 
Another sequence that is neither absolutely sum mabie nor square summable is 
the unit step sequence u[n]. Although it is not eompletely straightforward to show, this 
sequence can be represented by the following Fourier transform: 
1 
00 
U(e jW) = 
. + ~ 11"0(0) + 211"r). 
(2.148)
1 
e-}W 
~ 
r=-oo 
2.8  SYMMETRY PROPERTIES OF THE FOURIER 
TRANSFORM 
In using Fourier transforms, it is useful to have a detailed knowledge of the way that 
properties of the sequence manifest themselves in the Fourier transform and vice versa. 
In this section and Section 2.9, we discuss and summarize a number of such properties. 
Symmetry properties of the Fourier transform are often very useful for simplifying 
the solution of problems. The following discussion presents these properties. The proofs 
are considered in Problems 2.79 and 2.80. Before presenting the properties, however, 
we begin with some definitions. 
A conjugate-symmetric sequence xe[n] is defined as a sequence for which 
xe[n] 
x;[-n], and a conjugate-antisymmetric sequence xo[n] is defined as a sequence 
for which xo[n] = -x;[-n], where * denotes complex conjugation. Any sequence x[n] 
can be expressed as a sum of a conjugate-symmetric and conjugate-antisymmetric se­
quence. Specifically, 
x[n] = xAn] + xo[n],  
(2. 149a) 
where 
xe[n] = ~(x[n] +x*[-n]) = x;[-n]  
(2. 149b) 
and 
xo[nJ 
~(x[n] 
x*[-nD = -xZ[-n]. 
(2.149c) 
Adding Eqs. (2.149b) and (2.149c) confirms that Eq. (2. 149a) holds. A real sequence that 
is eonjugate symmetric such that xe[n] 
xe[-n] is referred to as an even sequence, and 
a real sequence that is conjugate antisymmetric such that xo[n] = -xo[-n] is referred 
to as an odd sequence. 
A Fourier transform X (e jW) can be decomposed into a sum ofconjugate-symmetric 
and conjugate-antisymmetric functions as 
) + Xo(e jUl
X (e jW ) = Xe(ejW
) ,  
(2.150a) 
where 
Xe(e jW ) = ~[X (ejW) + X*(e- jW)]  
(2.150b) 
and 
Xo(e jW) = ~[X(ejW) - X*(e- jW)].  
(2.15Oc) 
By substituting -0) for 0) in Eqs. (2.150b) and (2.150c), it follows that X e(ejW) is conju­
gate symmetric and X o(ejW) is conjugate antisymmetric; i.e., 
Xe(e jW) = X;(e- jW )  
(2.151a) 

Section 2.8 
Symmetry Properties of the Fourier Transform 
55 
and 
Xo(ej(V) 
-X;(e- jW ). 
(2.151b) 
If a real function of a continuous variable is conjugate symmetric, it is referred to as an 
even function, and a real conjugate-antisymmetric function of a continuous variable is 
referred to as an odd function. 
The symmetry properties of the Fourier transform are summarized in Table 2.1. 
The first six properties apply for a general complex sequence x[n] with Fourier trans­
form X (eiw). Properties 1 and 2 are considered in Problem 2.79. Property 3 follows from 
properties 1 and 2, together with the fact that the Fourier transform of the sum of two 
sequences is the sum of their Fourier transforms. Specifically, the Fourier transform of 
'Re(x[n]} = t(x[n] +x*[nD is the conjugate-symmetric part of X (ejW), or Xe(ejW). Sim­
ilarly, jIm (x-[n]) = ~(x[n] 
x*[n]), or equivalently, jIm {x[n]} has a Fourier transform 
that is the conjugate-antisymmetric component X o(ejW ) corresponding to property 4. 
By considering the Fourier transform of xdn] and xo[n], the conjugate-symmetric and 
conjugate-antisymmetric components, respectively, of x[n1, it can be shown that prop­
erties 5 and 6 follow. 
Ifx[n] is a real sequence, these symmetry properties become particularly straight­
forward and usefuL Specifically, for a real sequence, the Fourier transform is conjugate 
symmetric; i.e., X (ejW) = X*(e- jb») (property 7). Expressing X (ejW) in terms of its real 
and imaginary parts as 
(2.152) 
TABLE 2.1 
SYMMETRY PROPERTIES OF THE FOURIER TRANSFORM 
Sequence  
Fourier Transform 
x[nl  
X (ejW ) 
1. x*fn]  
X*(e- jW) 
2. x*[-nl  
X*(ejW ) 
3. ne(x[n]}  
X e<ejW ) 
(conjugate-symmetric part of X (ejW» 
4. jIm(x[nlJ  
X o(ejWj (conjugate-antisymmetric part of X (e jW» 
5. xe[n] 
(conjugate-symmetric part of x[n]) 
XR(ejW) 
nelX (e jW )} 
6.  xo[nl (conjugate-antisymmetric part of x[n]) 
j X l(ejWj = jIm{X (e jW ))  
The following properties apply only when x [n Jis real:  
7. Any real x[nl  
X (ejW) = X*(e- jW) 
(Fourier transform is conjugate symmetric) 
8. Any real x[nI  
XR(ejWj = XR(e-jW) 
(real part is even) 
9. Any real xln]  
XI (ejW ) = - XI (e- jW) (imaginary part is odd) 
10. Any real x[n]  
X (ejW)1 = IX (e-jW)1 
(magnitude is even) 
11. Any real x[nI  
Lx (ejW ) = -L X (e- jW) 
(phase is odd) 
12. xe[n] 
(even part ofx[n])  
XR(e jW) 
13. xo[n] (oddpartofx[n])  
j X [(e jW) 

56 
Chapter 2 
Discrete-Time Signals and Systems 
we can derive properties 8 and 9-specifically, 
XR(e j ())) 
XR(e- jW) 
(2.153a) 
and 
X[(e j ())) = -X[(e-j(V). 
(2.153b) 
In other words, the real part of the Fourier transform is an even function, and the imag­
inary part is an odd function, if the sequence is real. In a similar manner, by expressing 
X (e j ())) in polar form as 
X (ejW ) = IX (eiW)leiLx(e)W), 
(2.154) 
we can show that, for a real sequence x[n], the magnitude of the Fourier transform, 
IX (eiw ) I, is an even function of (J) and the phase, LX (eiw), can be chosen to be an odd 
function of (J) (properties 10 and 11). Also, for a real sequence, the even part of x[n] 
transforms to X R(eiw), and the odd part of x[n] transforms to j X[ (eiw ) (properties 12 
and 13). 
Example 2.21 
Illustration of Symmetry Properties 
Let us return to the sequence of Example 2.17, where we showed that the Fourier 
transform of the real sequence x[nl = anu[nJ is 
x (eiw) = 
1 
if lal < 1. 
(2.155)
1 
Then, from the properties of complex numbers, it follows that 
x (eiw ) 
1 
X*(e- jw) 
(property 7). 
XR(e jw ) = 
1 
acos(J) 
XR(e-jW) 
(property 8), 
1 + ­
jw
(e- jw ) 
(property 9),
X[(e
) = 1 + 
" 
1 
" 
IX (eJw)1 = (1 + " 
"" = IX (e-Jw)1 
(property 10), 
. 
1 (" -a sin w ) 
.
LX(eJW ) = tan-
. 
=-LX(e- JW ) 
(property 11).
l-acosw 
These functions are plotted in Figure 2.22 for a > 0, specifically, a 
0.75 (solid curve) 
and a = 0.5 (dashed curve). In Problem 2.32, we consider the corresponding plots for 
a < O. 

57 
Section 2.8 
Symmetry Properties of the Fourier Transform 
5 
4 
cl) 
"0 B 3 
P.. 2
E 
~ 1 
0 
2 
cl) 
"0 1
B 
~ -1 
-2 
r---- ..... ---------~ 
f°i-"""=-------\,-------~ 
-7T 
7T 
0 
7T 
7T 
2 
2 
Radian frequency (w) 
(b) 
5 
cl) 
4 
"0
.B 3 
P.. 2
E 
~ 
0 
-7T 
0 
7T 
2 
7T 
Radian frequency (w) 
(c) 
1.0 
"""'
en = 0.5 
:e '" 
0
~'" 
cl) 
~ -0.5 
..c:: 
~ 
-1.0 
-7T 
7T 
7T 
7T
0
-2 
2 
Radian frequeney (w) 
(d) 
Figure 2.22 
Frequency response for a system with impulse response 
h[n) = anu[n]. (a) Real part. a > 0; a = 0.75 (solid curve) and a = 0.5 (dashed 
curve). (b) Imaginary part. (c) Magnitude. a > 0; a = 0.75 (solid curve) and 
a 0.5 (dashed curve). (d) Phase. 
-7T 
Radian frequency (w) 
(a) 

58 
Chapter 2 
Discrete-Time Signals and Systems 
2.9 FOURIER TRANSFORM THEOREMS 
In addition to the symmetry properties, a variety of theorems (presented in Sections 
2.9.1-2.9.7) relate operations on the sequence to operations on the Fourier transform. 
We will see that these theorems are quite similar in most cases to corresponding theo­
rems for continuous-time signals and their Fourier transforms. To facilitate the statement 
of the theorems, we introduce the following operator notation: 
X (elUJ ) 
F{x[n]} , 
x[n] 
F-1{X (elW )}, 
:F 
. 
x [n] <,--7 X (elW ). 
That is, F denotes the operation of"takingthe Fourier transform ofx[n]," and F-1 is the 
inverse of that operation. Most of the theorems will be stated without proof. The proofs, 
which are left as exercises (Problem 2.81), generally involve only simple manipulations 
of variables of summation or integration. The theorems in this section are summarized 
in Table 2.2. 
TABLE 2.2 
FOURIER TRANSFORM THEOREMS 
Sequence 
Fourier Transform 
x[n] 
X (ejW ) 
yIn] 
Y (e jW ) 
1. ax[n] +by[n] 
2.  x[n 
nd] 
(nd an integer) 
ejwonx[n1
3. 
4. x(-n] 
5. nxfn] 
6. x[nl * yIn] 
7. x[n]y[n] 
Parseval's theorem: 
00 
8.  L Ix[n]12 = 
j "')12dw
L: IX (e
n=-O() 
00 
1  j1r 
. 
. 
9.  L x[n]y*[nJ 
-
X (eJW)Y*(eJw)dw 
2n -1r
n=-oo 
aX (ejW) +bY(e jW ) 
e- jwnd X (ejW) 
X (ej(w-",o» 
X (e-j'U) 
x* (ejW ) if x[n] real. 
.dX (e jW ) 
J 
dw 
X (ejW )Y(e jW ) 
~ jJr X (ejo)Y(ej(w-O»dfj 
2n 
-Jr 

Section 2.9 
Fourier Transform Theorems 
59 
2.9.1 Linearity of the Fourier Transform 
If 
and 
x2[n] 
X2(elw ), 
then it follows by substitution into the definition of the DTFT that 
.F 
. 
" 
aXl[n] +bx2[n] _ 
aXl(ejW) + bX2(elW). 
(2.156) 
2.9.2 Time Shifting and Frequency Shifting Theorem 
If 
.F 
" 
x[n] _ 
X (elW), 
then, for the time-shifted sequence x[n - nd], a simple transformation of the index of 
summation in the DTFT yields 
(2.157) 
Direct substitution proves the following result for the frequency-shifted Fourier trans­
form: 
" 
.F 
"( 
)
ejWOnx[n] _ 
X (el w-wo ). 
(2.158) 
2.9.3 Time Reversal Theorem 
If 
.F 
" 
x[nl _ 
X (elW ), 
then if the sequence is time reversed, 
x[-n] 
(2.159) 
If x[n] is real, this theorem becomes 
(2.160) 
2.9.4 Differentiation in Frequency Theorem 
If 
x[n] 
X (elw ), 
then, by differentiating the DTFT, it is seen that 
.dX (elw)
nx[n] 
(2.161)
J 
dw 

60 
Chapter 2 
Discrete-Time Signals and Systems 
2.9.5 Parsevals Theorem 
If 
x[n] 
x (ejW ), 
then 
00 
2
E = L Ix[n]1
1 in IX (eiW)12dw. 
(2.162)
2n 
-T(
n=-oo 
The function IX (eiW)12 is called the energy density spectrum, since it determines how the 
energy is distributed in the frequency domain. Necessarily, the energy density spectrum 
is defined only for finite-energy signals. A more general form of Parseval's theorem is 
shown in Problem 2.84. 
2.9.6 The Convolution Theorem 
If 
:F 
. 
x[n] ~ X (el (") 
and 
h[n] ....,::......,. H (ej(O), 
and if 
00 
y[n] = L x[k]h[n 
k] = x[n] *h[n], 
(2.163) 
k=-oo 
then 
Y (ejW ) 
X (ejW)H(eiw). 
(2.164) 
Thus, convolution of sequences implies multiplication of the corresponding Fourier 
transforms. Note that the time-shifting property is a special case of the convolution 
property, since 
:F 
. 
o[n - nd] ~ e- jWnd 
(2.165) 
and if h[n] = S[n 
nd], then y[n] = xfn] *S[n - nd] = x[n 
nd]. Therefore, 
H (eiw ) = e-jwnd 
and 
Y (ejW ) = e- )wnd X (eJW ). 
A formal derivation of the convolution theorem is easily achieved by applying the 
definition of the Fourier transform to y[n] as expressed inEq. (2.163). This theorem can 
also be interpreted as a direct consequence of the eigenfunction property of complex 
exponentials for LTI systems. Recall that H (eJW) is the frequency response of the LTI 
system whose impulse response is h[n]. Also, if 
x[n] = ej{vn, 
then 
y[n] = H(eJW)ejwn. 

61 
Section 2.9 
Fourier Transform Theorems 
That is, complex exponentials are eigenfunctions of LTI systems, where H(e jW), the 
Fourier transform of h[n], is the eigenvalue. From the definition of integration, the 
Fourier transform synthesis equation corresponds to the representation of a sequence 
x[n] as a superposition of complex exponentials of infinitesimal size; that is, 
1 jJT 
. . 
1 '" 
·k!:. 
"k!:.
x[n] = 
X (eJW)eJwndw = lim -
L.,; X (el 
W)eJ 
wn Dow. 
2TC 
-JT 
!:.W->O 2n k 
By the eigenfunction property of linear systems and by the principle of superposition, 
the corresponding output will be 
y[n] 
lim ~ L H(ejk!:.W)X (ejkt:,.W)ejkt:.wn Dow = 1 jJT H(ejOJ)X (ejW)ejwndw. 
!:.w->o 2n k 
2n 
-JT 
Thus, we conclude that 
as in Eq. (2.164). 
2.9.7 The Modulation or Windowing Theorem 
If 
:F 
" 
x[n] *-" X (eJW ) 
and 
and if 
y[n] = x[n]w[n], 
(2.166) 
then 
(2.167)  
Equation (2.167) is a periodic convolution, i.e., a convolution of two periodic functions 
with the limits of integration extending over only one period. The duality inherent in 
most Fourier transform theorems is evident when we compare the convolution and 
modulation theorems. However, in contrast to the continuous-time case, where this du­
ality is complete, in the discrete-time case fundamental differences arise because the 
Fourier transform is a sum, whereas the inverse transform is an integral with a pe­
riodic integrand. Although for continuous time, we can state that convolution in the 
time domain is represented by multiplication in the frequency domain and vice versa; 
in discrete time, this statement must be modified somewhat. Specifically, discrete-time 
convolution of sequences (the convolution sum) is equivalent to multiplication of cor­
responding periodic Fourier transforms, and multiplication of sequences is equivalent 
to periodic convolution of corresponding Fourier transforms. 
The theorems of this section and a number of fundamental Fourier transform pairs 
are summarized in Tables 2.2 and 2.3, respectively. One of the ways that knowledge of 
Fourier transform theorems and properties is useful is in determining Fourier transforms 

Chapter 2 
Discrete-Time Signals and Systems
62 
TABLE 2.3 
FOURIER TRANSFORM PAIRS 
Sequence 
Fourier Transform 
1. 8[nJ 
1 
e- jwno
2. 8[n 
noJ 
3. 1 
(-oo<n < (0) 
L
00 
2rr8(w + 2rrk)  
k=-oo  
4. anu[nJ 
(Ial < 1) 
1 
1 
00 
5. urn] 
--~ + L rr8(w+ 2rrk) 
k=-oo 
1 
6. (n + l)anu[nJ 
(Ial < 1) 
(1 - ae- jw)2 
rn sinwp (n + 1) urn] (Irl < 1) 
1 
7. 
sinwp 
1 
2r 
8. 
. {I,
JW 
Iwi < We,  
rrn 
X (e 
) = 
0, We < Iwi :': rr  
L O:,:n:,:M 
sin[w(M + 1)/2] e-jwM/2 
9. x[n] = { 0: otherwise 
sin(w/2) 
00 
jwon
10. e
L 2rr8(w 
Wa + 2rrk) 
k=-oo 
00 
11. cos(wOn + cjJ) 
L [rreN'8(w - Wa + 2rrk) + rre- j<I'8(w + Wa + 2Jrk)] 
k=-oc 
or inverse transforms. Often, by using the theorems and known transform pairs, it is 
possible to represent a sequence in terms of operations on other sequences for which 
the transform is known, thereby simplifying an otherwise difficult or tedious problem. 
Examples 2.22-2.25 illustrate this approach. 
Example 2.22 
Determining a Fourier Transform Using 
Tables 2.2 and 2.3 
Suppose we wish to find the Fourier transform of the sequence x[nJ 
anu[n - 5]. This 
transform can be computed by exploiting Theorems 1 and 2 ofTable 2.2 and transform 
pair 4 of Table 2.3. Let x1[n] 
anu[n]. We start with this signal because it is the most 
similar signal to x[n] in Table 2.3. The table states that 
. 
1 
X 1 (eJW ) = ----;--
(2.168)
1 
To obtain x[n] from x1[n], we first delay 
X1[1I] 
by five samples, i.e., 
x2[n] 
x1[n 
5]. Theorem 2 of Table 2.2 gives the corresponding frequency-domain 
relationship, X2 (ejW ) = e- j50JXl (e jOJ ), so 
j5w 
X2(e jW ) = 
e-
(2.169)
1 - ae- jw ' 
....  

63 
Section 2.9 
Fourier Transform Theorems 
To get from x2[n] to the desired x[n], we need only multiply by the constant as, i.e., 
x [n 1= asX2 [n].1be linearity property ofthe Fouriertransform, Theorem 1 of Table 2.2, 
then yields the desired Fourier transform, 
, 
aSe- jSw 
X (eJw ) = 
, 
(2.170)
1 
ae-Jw 
Example 2.23 
Determining an Inverse Fourier Transform 
Using Tables 2.2 and 2.3 
Suppose that 
1 
(2.171) 
Direct substitution of X (ejW) into Eq. (2.130) leads to an integral that is difficult to 
evaluate by ordinary real integration techniques. However, using the technique of 
partial fraction expansion, which we discuss in detail in Chapter 3, we can expand 
X (eiw) into the form 
b/(a - b) 
(2.172)
1- be- jw ' 
From Theorem 1 of Table 2.2 and transform pair 4 of Table 2.3, it follows that 
(2.173) 
Example 2.24 Determining the Impulse Response from the 
Frequency Response 
The frequency response of a highpass filter with linear phase is  
, 
{-iwnd 
I I  
H(eJw ) = 
e 
,We < 
W < Jr. 
(2.174)
0, 
Iwi < We,  
where a period of 2Jr is understood. This frequency response can be expressed as  
H(eiw) = e-iwnd(l_ Hlp(eiW» 
e-jwnd 
e-iWndHlp(eiW), 
where Hlp(e jW) is periodic with period 2Jr and 
H 
(e jw 
Iwi < (1)e,
= {I,
Ip 
) 
0, 
We < Iwi < Jr. 
Using the result of Example 2.18 to obtain the inverse transform of Hlp (ei {'», together 
with properties 1 and 2 of Table 2.2, we have 
h[n] = 8[n - nd] - h Ip[n - nd] 

64 
Chapter 2 
Discrete-Time Signals and Systems 
Example 2.25 
Determining the Impulse Response for a 
Difference Equation 
In this example, we determine the impulse response for a stable LTI system for which 
the inputx[n] and output yIn] satisfythe linear constant-coefficient difference equation 
yIn] -
~y[n - 1] = x[n] -
~x[n - 1]. 
(2.175) 
In Chapter 3, we will see that the z-transform is more useful than the Fourier transform 
for dealing with difference equations. However, this example offers a hint of the utility 
of transform methods in the analysis of linear systems. To find the impulse response, 
we set x[n] = 8[n]; with h[n] denoting the impulse response, Eq. (2.175) becomes 
h[n] -
~h[n - 1] = 8[n] -
~8[n - 1]. 
(2.176) 
Applying the Fourier transform to both sides of Eq. (2.176) and using properties 1 and 
2 of Table 2.2, we obtain 
H(e jw ) -
~e-jWH(ejW) = 1- ~e-jw,  
(2.177) 
or 
. 
1 - le- jw 
H(eJw) = 
4 
. 
(2.178)
1- 1e- Jw 
2 
To obtain h[n], we want to determine the inverse Fourier transform of H(e jW ). Toward 
this end, we rewrite Eq. (2.178) as 
le- jw
1
jW  
4 
H(e 
) = 
--1 _ JW 
1 
.. 
(2.179) 
1 -
2:e 
1-2:e- Jw 
From transform 4 of Table 2.3, 
(l)n 
F 
1
2:  
urn] +--+ --1 .. 
1- 2:e-Jw 
Combining this transform with property 2 of Table 2.2, we obtain 
1 - jw
4e
(1)(1)n-1 
F
-
4 
2: 
urn - 1] +--+  
(2.180)
1 - le- jw· 
2 
Based on property 1 of Table 2.2, then, 
(l)n 
(1)(1)n-1
h[n] = 
2: 
urn] -
4 
2: 
urn - 1].  
(2.181 ) 
2.10 DISCRETE-TIME RANDOM SIGNALS 
The preceding sections have focused on mathematical representations of discrete-time 
signals and systems and the insights that derive from such mathematical representations. 
Discrete-time signals and systems have both a time-domain and a frequency-domain 
representation, each with an important place in the theory and design of discrete-time 
signal-processing systems. Until now, we have assumed that the signals are deterministic, 

65 
Section 2.10 
Discrete-Time Random Signals 
i.e., that each value of a sequence is uniquely determined by a mathematical expression, 
a table of data, or a rule of some type. 
In many situations, the processes that generate signals are so complex as to make 
precise description of a signal extremely difficult or undesirable, if not impossible. 
In such cases, modeling the signal as a random process is analytically useful.5 As an 
example, we will see in Chapter 6 that many of the effects encountered in implementing 
digital signal-processing algorithms with finite register length can be represented by 
additive noise, i.e., a random sequence. Many mechanical systems generate acoustic or 
vibratory signals that can be processed to diagnose potential failure; again, signals of 
this type are often best modeled in terms of random signals. Speech signals to be pro­
cessed for automatic recognition or bandwidth compression and music to be processed 
for quality enhancement are two more of many examples. 
A random signal is considered to be a member of an ensemble of discrete-time 
signals that is characterized by a set of probability density functions. More specifically, 
for a particular signal at a particular time, the amplitude of the signal sample at that 
time is assumed to have been determined by an underlying scheme of probabilities. 
That is, each individual sample x[n] of a particular signal is assumed to be an outcome 
ofsome underlying random variable xn . The entire signal is represented by a collection 
of such random variables, one for each sample time, -00 < n < 00. This collection of 
random variables is referred to as a random process, and we assume that a particular 
sequence of samples x[n] for -00 < n < 00 has been generated by the random process 
that underlies the signal. To completely describe the random process, we need to specify 
the individual and joint probability distributions of all the random variables. 
The key to obtaining useful results from such models of signals lies in their de­
scription in terms of averages that can be computed from assumed probability laws or 
estimated from specific signals. While random signals are not absolutely summable or 
square summable and, consequently, do not directly have Fourier transforms, many (but 
not all) of the properties of such signals can be summarized in terms of averages such as 
the autocorrelation or autocovariance sequence, for which the Fourier transform often 
exists. As we, will discuss in this section, the Fourier transform of the autocorrelation 
sequence has a useful interpretation in terms of the frequency distribution of the power 
in the signal. The use of the autocorrelation sequence and its transform has another 
important advantage: The effect of processing random signals with a discrete-time lin­
ear system can be conveniently described in terms of the effect of the system on the 
autocorrelation sequence. 
In the following discussion, we assume that the reader is familiar with the basic 
concepts of random processes, such as averages, correlation and covariance functions, 
and the power spectrum. A brief review and summary of notation and concepts is 
provided in Appendix A. A more detailed presentation of the theory of random signals 
can be found in a variety of excellent texts, such as Davenport (1970), and Papoulis 
(2002), Gray and Davidson (2004), Kay (2006) and Bertsekas and Tsitsiklis (2008). 
Our primary objective in this section is to present a specific set of results that 
will be useful in subsequent chapters. Therefore, we focus on wide-sense stationary 
random signals and their representation in the context of processing with LTI systems. 
5It is common in the signal processing literature to use the terms "random" and "stochastic" inter­
changeably. In this text, we primarily refer to this class of signals as random signals or random processes. 

66 
Chapter 2 
Discrete-Time Signals and Systems 
Although, for simplicity, we assume that x[n] and h[n] are real valued, the results can 
be generalized to the complex case. 
Consider a stable LTI system with real impulse response h[n]. Let x[n] be a real­
valued sequence that is a sample sequence of a wide-sense stationary discrete-time 
random process. Then, the output of the linear system is also a sample sequence of a 
discrete-time random process related to the input process by the linear transformation 
00 
DC 
y[n] = L h[n - k]x[k] 
L h[k]x[n - k]. 
k=-oo 
k=-oo 
As we have shown, since the system is stable, y[n] will be bounded if x[n] is bounded. 
We will see shortly that if the input is stationary,6 then so is the output. The input signal 
may be characterized by its mean mx and its autocorrelation function Q1xx[m], or we may 
also have additional information about 1 st_ or even 2nd -order probability distributions. 
In characterizing the output random process y[n] we desire similar information. For 
many applications, it is sufficient to characterize both the input and output in terms of 
simple averages, such as the mean, variance, and autocorrelation. Therefore, we will 
derive input-output relationships for these quantities. 
The means of the input and output processes are, respectively, 
mXn = E{xn }, 
my" = E(Yn}, 
(2.182) 
where E{.} denotes the expected value of a random variable. In most of our discussion, 
it will not be necessary to carefully distinguish between the random variables Xn and 
Yn and their specific values x[n] and y[n]. This will simplify the mathematical notation 
significantly. For example, Eqs. (2.182) will alternatively be written 
mx[n] = E{x[n]j, 
my[n] = E(y[nll. 
(2.183) 
Ifx[n] is stationary, thenmdn] is independent of n and will be written asmx , with similar 
notation for my[n] if y[n] is stationary. 
The mean of the output process is 
my[n] 
E{y[n]} = L
00 
h[k]E{x[n 
k]}. 
k=-oo 
where we have used the fact that the expected value of a sum is the sum of the expected 
values. Since the input is stationary, mx[n - k] = m T , and consequently, 
my[n] = mx L
00 
h[k). 
(2.184) 
k=-oo 
From Eq. (2.184), we see that the mean of the output is also constant. An equivalent 
expression to Eq. (2.184) in terms of the frequency response is 
my = H(el'0 )mx. 
(2.185) 
6In the remainder of the text, we will use the term stationary to mean "wide-sense stationary." i.e., that 
Elx[nl]x[n2]} for all nl, n2 depends only on the difference (ni - n2)' Equivalently, the autocorrelation is 
only a function of the time difference (ni 
nz). 
.....  

67 
Section 2.10 
d 
~t 
5)
! 
Discrete-Time Random Signals 
Assuming temporarily that the output is nonstationary, the autocorrelation func­
tion of the output process for a real input is 
tPyy[n, n + m] 
t'{y[n]y[n + m]} 
t' L~(XF~OO h[k]h[r)x[n - k)x[n + m - r)} 
00 
00
L h[k) L h[r]t'{x[n - k)x[n + m - r]}. 
k=-oo 
r=-OO 
Sincex[n] is assumed to hestationary,t'{x[n -k)xfn +m - r]} depends only on the time 
difference In + k 
r. Therefore, 
00 
00 
tPyy[n,n+m]= L h[k] L h[r]tPxx[m +k-r] =tPyy[m]. 
(2.186) 
1<=-00 
r=-OO 
That is, the output autocorrelation sequence also depends only on the time difference 
m. Thus, for an LTI system having a wide-sense stationary input, the output is also 
wide-sense stationary. 
By making the substitution i = r - k, we can express Eq. (2.186) as 
00 
00 
tPvy[m] = L tPxx[m - i] L h[k]h[i + k] 
£=-00 
k=-oo 
(2.187) 
= L
00 
tPxx[m - i]chh[iJ, 
£=-oc  
where we have defined  
oc 
chh[i) = L hlk]h[i + k]. 
(2.188) 
k=-oo 
The sequence Chh [iJ is referred to as the deterministic autocorrelation sequence or, 
simply, the autocorrelation sequence of hen]. It should be emphasized that chh[il is the 
autocorrelation of an aperiodic-i.e., finite-energy-sequence and should not he con­
fused with the autocorrelation of an infinite-energy random sequence. Indeed, it can he 
seen that Chh li] is simply the discrete convolution of hen] with h[-n]. Equation (2.187), 
then, can be interpreted to mean that the autocorrelation of the output ofa linear system 
is the convolution of the autocorrelation of the input with the aperiodic autocorrelation 
of the system impulse response. 
Equation (2.187) suggests that Fourier transforms may be useful in characteriz­
ing the response of an LTI system to a random input. Assume, for convenience, that 
mx =0; i.e., the autocorrelation and autocovariance sequences are identical. Then, with 
cbxxCe j (1)), cbyy(eiw ). and Chh(eiw ) denoting the Fourier transforms of tPxx[m], tPyy[m], 
and Chh[i), respectively, from Eq. (2.187), 
(2.189) 
Also, from Eq. (2.188), 
Chh(eiw) = H(eiw)H*(ej(V) 
= IH(eiw)12, 

68 
Chapter 2 
Discrete-Time Signals and Systems 
so 
<pyy(e1W) = IH(elW)12<Pxx(ejW). 
(2.190) 
Equation (2.190) provides the motivation for the term power density spectrum. 
Specifically, 
2 
. 
1 r 
jw
£{y [n]} = <pyy[O] = 27r Lrc <Pyy(e 
) dw 
(2.191) 
total average power in output. 
Substituting Eq. (2.190) into Eq. (2.191), we have 
2 
1 fJr 
. 
2 
.
£{y [n]} = <Pyy[O] = 2" 
IH(elW)1 <pxAelW)dw. 
(2.192) 
7r 
-rc 
Suppose that H(ejW ) is an ideal bandpass filter, as shown in Figure 2.18(c). Since <pxx[ml 
is a real, even sequence, its Fourier transform is also real and even, i.e., 
<pxAe}w) = <pxx(e-j(U). 
Likewise, IH(ejW )12 is an even function of w. Therefore, we can write 
<Pyy[O] = average power in output 
(2.193)
1 fWb 
. 
1 f -Wo 
.
-2 
<Pxx(elW)dw + 2 
<pxxCelW)dw. 
7r 
Wo 
7r 
-Wb 
Thus, the area under <Pxx(ejW) for Wa :s Iwl :s Wb can be taken to represent the mean­
square value of the input in that frequency band. We observe that the output power 
must remain nonnegative, so 
lim 
<pyy[O] :::: o. 
(Wb-Wa)-+O .. 
This result, together with Eq. (2.193) and the fact that the band Wa :s w :s Wb can be 
arbitrarily small, implies that 
<pxx(e jW) :::: 0 
for all w. 
(2.194) 
Hence, we note that the power density function of a real signal is real, even, and non­
negative. 

69 
Section 2.10 
Discrete-Time Random Signals 
Example 2.26 White Noise 
The concept of white noise is exceedingly useful in a wide variety of contexts in the 
design and analysis of signal processing and communications systems. A white-noise 
signal is a signalforwhich ¢xx[m] = a';8[m]. We assume in this example that the signal 
has zero mean. The power spectrum of a white-noise signal is a constant, Le., 
for all w. 
The average power of a white-noise signal is therefore 
¢xx[OJ 
~ f1r <l>xx(ej{J)) dw = ~ f1r a; dw =a';. 
2rr: J~1r 
2rr: J-1r 
The concept of white noise is also useful in the representation of random signals 
whose power spectra are not constant with frequency. For example, a random signal 
yEn] with power spectrum <l>yy(ej{J)) can be assumed to be the output of an LTI system 
with a white-noise input. That is, we use Eq. (2.190) to define a system with frequency 
response H (ej{J)) to satisfy the equation 
"" (jW) _-
IH(ej{J))1 2ax'2
'Vyye 
where a; is the average power of the assumed white-noise input signal. We adjust 
the average power of this input signal to give the correct average power for y[nJ. For 
example, suppose that hEn] = anu[nJ. Then, 
. 
1 
H(eJ{J)) = ----;­
l­
and we can represent all random signals whose power spectra are of the form 
1 + a2 - 2a cos w . 
Another important result concerns the cross-correlation between the input and 
output of an LTI system: 
tPyx[m] = £{x[n]y[n + m]} 
= £ Ix[n] k%;oo h[k]x[n +m 
k]} 
(2.195) 
00L h[k]tPxx[m - k]. 
k=-oo 
In this case, we note that the cross-correlation between input and output is the convo­
lution of the impulse response with the input autocorrelation sequence. 
The Fourier transform of Eq. (2.195) is 
<Ilyx(ej{J)) = H(ej{J))<Ilxx(ejW). 
(2.196) 
This result has a useful application when the input is white noise, i.e., when 
tPxx[m] = a;o[m]. Substituting into Eq. (2.195), we note that 
tPyxlm] 
a;'h[m]. 
(2.197) 

70 
Chapter 2 
Discrete-Time Signals and Systems 
That is, for a zero-mean white-noise input, the cross-correlation between input and 
output ofa linear system is proportional to the impulse response of the system. Similarly, 
the power spectrum of a white-noise input is 
,... 
(jW) 
2 
::: :re. 
(2.198)
"'xx e 
= o-x' 
-Jr ::: W 
Thus, from Eq. (2.196), 
<pyx(ejW) = 0-: H(ejW). 
(2.199) 
In other words, the cross power spectrum is in this case proportional to the frequency re­
sponse of the system. Equations (2.197) and (2.199) may serve as the basis for estimating 
the impulse response or frequency response of an LTI system if it is possible to observe 
the output of the system in response to a white-noise input. An example application is 
in the measurement of the acoustic impulse response of a room or concert halL 
2.11 SUMMARY 
In this chapter, we have reviewed and discussed a number of basic definitions relating 
to discrete-time signals and systems. We considered the definition of a set of basic 
sequences, the definition and representation of LTI systems in terms of the convolution 
sum, and some implications of stability and causality. The class of systems for which 
the input and output satisfy a linear constant-coefficient difference equation with initial 
rest conditions was shown to be an important subclass of LTI systems. The recursive 
solution of such difference equations was discussed and the classes of FIR and IIR 
systems defined. 
An important means for the analysis and representation of LTI systems lies in their 
frequency-domain representation. The response of a system to a complex exponential 
input was considered, leading to the definition of the frequency response. The relation 
between impulse response and frequency response was then interpreted as a Fourier 
transform pair. 
We called attention to many properties of Fourier transform representations and 
discussed a variety of useful Fourier transform pairs. Tables 2.1 and 2.2 summarize the 
properties and theorems, and Table 2.3 contains some useful Fourier transform pairs. 
The chapter concluded with an introduction to discrete-time random signals. These 
basic ideas and results will be developed further and used in later chapters. 
Problems 
Basic Problems with Answers 
2.1. For each of the following systems, determine whether the system is (1) stable, (2) causal, 
(3) linear, (4) time invariant, and (5) memoryless: 
(a) T(x[n]) = g[n]x[ll] with g[lI] given 
(b) T(x[ll]) = Lk=nQ x[k] 
n f.: 0 
(c) T(x[nJ) = L~!:'=-no x[k] 
(d) T(x[n]) = x[n - no] 

71 
ns  
Chapter 2 
Problems 
[ld 
(e) T(x[n]) =: ex[n] 
ly, 
(f) T(x[n]) =: ax[n] + b 
(g) T(x[n]) =: x[-n] 
(h) T(x[nD =: x[n] + 3u[n + 1].
18) 
2.2. (a) The impulse response h[n] of an LTI system is known to be zero, except in the interval 
Non ~ N 1. The input x[n] is known to be zero, except in the interval N2 ~ n ~ N 3. 
re­
As a result, the output is constrained to be zero, except in some interval N 4 ~ n ~ N 5. 
fig 
Determine N 4 and N 5 in terms of No. N 1> N 2, and N 3. 
(b) Ifx[n] is zero, exceptfor N consecutive points, and h[n] is zero, exceptfor M consecutive
rYe 
points, what is the maximum number of consecutive points for which y[n] can be
[lis 
nonzero? 
2.3.  By direct evaluation of the convolution sum, determine the unit step response (x[n] = urn]) 
of an LTI system whose impulse response is 
O<a<1. 
2.4. Consider the linear constant-coefficient difference equation 
y[n] 
~y[n - 1] + Ay[n - 2] = 2x[n 
1]. 
Determine y[n] for n ::: 0 when x[n] = 8[n] and y[n] = 0, n < O. 
2.5. A causal LTI system is described by the difference equation 
y[n] - 5y[n 
1] + 6y[n 
2] 
2x[n - 1]. 
(a)  Determine the homogeneous response of the system, 
the possible outputs if 
x[n] 
0 for all n. 
(b) Determine the impulse response of the system. 
(c) Determine the step response of the system. 
2.6.  (a) Determine the frequency response H(eJUJ ) of the LTI system whose input and output 
satisfy the difference equation 
y[n] 
!y[n - 1] = x[n] + 2x[n 
1] + x[n - 2]. 
(b) Write a difference equation that characterizes a system whose frequency response is 
2.7. Determine whether each of the following signals is periodic. If the signal is periodic, state 
its period. 
(a)  x[n] = eJ(7rnj6) 
(b)  x[n] =: eJ(37rn/4) 
(c)  x[n] =: [sin(nn/5)]/(nn) 
(d)  x[nl =: eJ7rn /..fi. 
2.S.  An LTI system has impulse response h[n] 
5(-1/2)nu[n]. Use the Fourier transform to 
find the output of this system when the input is x[n] 
(1/3tu[n]. 

72 
Chapter 2 
Discrete-Time Signals and Systems 
2.9.  Consider the difference equation 
5 
1 
1 
y[n]--y[n 
1] + -yIn - 2] = -x[n 
I],
6 
6" 
3 
(0)  What are the impulse response, frequency response, and step response for the causal 
LTl system satisfying this difference equation? 
(b)  What is the general form of the homogeneous solution of the difference equation? 
(c)  Consider a different system satisfying the difference equation that is neither causal nor 
LTl, but that has y[O] = y[1] 
1. Find the response of this system to x[n] = 8[n]. 
2.10.  Determine the output of an LTI system if the impulse response hln] and the input x[n] are 
as follows: 
(a)  x[n] = urn] and h[nJ = anu[-n - 1], with a > 1. 
(b)  x[n] = urn - 4] and h[nJ = 2nu[-n -1]. 
(c)  x[n] = urn] andh[n] 
(0.5)2Ilu[-n]. 
(d)  h[n] = 2nu[-n - 1] and x[n] = urn] - urn -10]. 
Use your knowledge of linearity and time invariance to minimize the work in parts (b )-(d). 
2.11.  Consider an LTl system with frequency response 
" 
l_e- j2w 
H(eJW ) = 
-7f < W S 7f. 
1+ 
Determine the output y[n] for all n if the input x[n] for all n is 
. (7fn)
x[n] = sm 4'" . 
2.12.  Consider a system with input x[n] and output yIn] that satisfy the difference equation 
yIn] = ny[n 
1] + x[n]. 
The system is causal and satisfies initial-rest conditions; i.e., if x[n] = 0 for n < no, then 
yIn] = 0 for n < no. 
(a)  If x[n] 
8[n], determine y[n] for all n. 
(b)  Is the system linear? Justify your answer. 
(c)  Is the system time invariant? Justify your answer. 
2.13.  Indicate which of the following discrete-time signals are eigenfunctions of stable, LTl 
discrete-time systems: 
(0)  ej2rrn/3 
(b)  3n 
(c)  2nu[-n 
1] 
(d)  cos(WOn) 
(e)  (1/4)n 
(f)  (l/4)llu[n] +4nu[-n -1]. 
2.14.  A single input-output relationship is given for each of the following three systems: 
(a)  SystemA:x[n] 
(1/3)n, 
y[n]=2(1/3)n. 
(b) System B: x[n] = (1/2)n, 
yIn] 
(1/4)n. 
(c)  System C: x[n] = (2/3)nu[nJ, 
yIn] 
4(2/3)nu[n] - 3(1/2lu[n], 
Based on this information, pick the strongest possible conclusion that you can make about 
each system from the following list of statements: 
(i) The system cannot possibly be LTI. 
(ii)  The system must be LTI. 

73 
Chapter 2 
Problems 
(iii) The system can be LTI, and there is only one LTI system that satisfies this input-output 
constraint. 
(iv)  1be system can be LTI, but cannot be uniquely determined from the information in 
this input-output constraint. 
If you chose option (iii) from this list, specify either the impulse response h[n] or the 
frequency response H(e jW ) for the LTI system. 
2.15.  Consider the system illustrated in Figure P2.1S.1be output of an LTI system with an impulse 
responseh[n] "" (~r u[n+10] is multiplied by a unit step function urn] to yield the output of 
the overall system. Answer each of the following questions, and briefly justify your answers: 
urn] 
h[n] 
(114)"U[11 + 10]  
x[nl  
y[n] 
Figure P2.15 
(a) Is the overall system LTI? 
(b) Is the overall system causal? 
(c) Is the overall system stable in the BIBO sense? 
2.16. Consider the following difference equation: 
1 
1 
y[n]- 4 y [n -1]- gy[n 
2] = 3x[n]. 
(8) Determine the general form of the homogeneous solution to this difference equation. 
(b)  Both a causal and an anticausal LTI system are characterized by this difference equa­
tion. Find the impulse responses of the two systems. 
(c) Show that the causal LTI system is stable and the anticausal LTI system is unstable. 
(d) Find a particular solution to the difference equation when x[n] 
(1/2)nu[nJ. 
2.17. (a) Determine the Fourier transform of the sequence 
1,  0:::: n :::: M,
r[nJ 
{ 0,  otherwise. 
(b) Consider the sequence 
~[1-Cos(2j{n)J, O::::n ::::M, 
w[n] =  
2 
M 
0, 
otherwise.
1 
Sketch w[n] and express W(e jW ), the Fourier transform of w[n], in terms of R (ejW ), 
the Fourier transform of r[nl. (Hint: First express w[n] in terms of r[nJ and the complex 
exponentials ej (2rrn/M) and e- j(2rrn/M).) 
(c)  Sketch the magnitude of R (e jW ) and W (eJW ) for the case when M 
4. 

74  
Chapter 2 
Discrete-Time Signals and Systems 
2.18.  For each of the following impulse responses of LTI systems, indicate whether or not the 
system is causal: 
(a) hEn] 
(1/2)n urn] 
(b) hEn] 
(1/2)nu[n -1] 
(c) hEn] 
(1/2)lnl 
(d) hEn] 
urn + 2] - uln ~ 2] 
(e) h[n] 
(1/3)nu[n] +3nu[~n ~ 1]. 
2.19.  For each of the following impulse responses of LTI systems, indicate whether or not the 
system is stable: 
(a) hEn] 
4n u[n] 
(b) hEn] 
urn] 
urn ~ to] 
(c) hEn] = 3nu[~n ~ 1] 
(d) h[n] 
sin(rrn/3)u[n] 
(e) hEn] 
(3/4)lnl cos(rrn/4 + rr/4) 
(f) hEn] 
2u[n + 5] - u[nl- urn ~ 5]. 
2.20. Consider the difference equation representing a causal LTI system 
yEn] + (l/a)y[n 
1] = x[n 
1]. 
(8) Find the impulse response of the system, hEn], as a function of the constant a. 
(b) For what range of values of a will the system be stable? 
Basic Problems 
2.21. A discrete-time signal x[n] is shown in Figure P2.2l. 
1 
1 P 
x[nl 
-2-1 0 1 2 3 4 
FiOllre P2.21 
Sketch and label carefully each of the following signals: 
(a) x[n - 2] 
(b) x[4 
nl 
(c) x[2n] 
(d) x[n]u[2 - n] 
(e) x[n - 1]8[n - 3]. 
2.22.  Consider a discrete-time LTI system with impulse response hEn]. If the input x[n] is a 
periodic sequence with period N (i.e., if x[nl 
x[n + N]), show that the output yIn] is also 
a periodic sequence with period N. 

75 
Chapter 2 
Problems 
2.23. For each of the following systems, determine whether the system is (1) stable, (2) causal, 
(3) linear, and (4) time invariant. 
(a) T(x[n)) = (cosJrn)x[n] 
(b) T(x[n]) = x[n 2] 
00 
(c) T(x[n]) 
x[n] L 8[n 
kJ 
k=O 
00 
(d) T(x[n]) 
L x[k]. 
k=n-l 
2.24. Consider an arbitrary linear system with input x[n] and output y[n]. Show that if x[n) = 0 
for all n, then y[n1must also be zero for all n. 
2.25.  Consider a system for which the input x [n) and the output y [n] satisfy the following relation. 
8y[n) + 2y[n 
1) 
3y[n 
2J 
x[nJ 
(P2.25-1) 
(a)  For x[n] = 8[n], show that a particular sequence satisfying the difference equation is 
yp[n] = 10 (-~r urn] + do (ir urn]. 
(b)  Determine the homogeneous solution(s) to the difference equation specified in 
Eq. (P2.25-1). 
(c)  Determine yen) for -2 S n S 2 when x[n] is equal to o[n] in Eq. (P2.25-1) and the 
initial rest condition is assumed in solving the difference equation. Note that the initial 
rest condition implies the system described by Eq. (P2.25-1) is causal. 
2.26.  For each of the systems in Figure P2.26, pick the strongest valid conclusion that you can 
make about each system from the following list of statements: 
(i) The system must be LTI and is uniquely specified by the information given. 
(ii) The system must be LTI, but cannot be uniquely determined from the information 
given. 
(iii) The system could be LTI and if it is, the information given uniquely specifies the system. 
(iv) The system could be LTI, but cannot be uniquely determined from the information 
given. 
(v) The system could not possibly be LTI.  
For each system for which you choose option (i) or (iii), give the impulse response hen] for  
the uniquely specified LTI system. One example of an input and its corresponding output 
are shown for each system. 
System A: 
(~f--1 System A ~ (~f 
SystemB: 
System C: 
~ (~fu[n] --1 System C ~
- 60fu[-n -1] - 60fu[n] 
Figure P2.26 

76 
Chapter 2 
Discrete-Time Signals and Systems 
2.27.  For each of the systems in Figure P2.27, pick the strongest valid conclusion that you can 
make about each system from the following list of statements: 
(i) The system must be LT! and is uniquely specified by the information given. 
(ii) The system must be LT!, but cannot be uniquely determined from the information 
given. 
(iii) The system could be LT!, and if it is, the information given uniquely specifies the 
system. 
(iv) The system could be LT!, but cannot be uniquely determined from the information 
given. 
(v) The system could not possibly be LTI. 
8[n]  
2ej £urn]
1 System A 
8[n]
or urn] 1 System B 
x[n] + ay[n]  
T(x[nD + aT(y[n])
1 System C 
For all choices of x[n], y[n], and the constant a 
cos an) 
3 cos (Jn) + ! sin (~n + '3-)
1 SystemD 
x[n] 
y[n] = O.2y[n + 1] + x[nJ
1 System E 
Figure P2.27 
2.28. Four input-output pairs of a particular system S are specified in Figure P2.28-1: 
2 
(1)  
1 
1 1 
~ --[TI--
I . . I I 
012  
o 1 2 3 4 
(2) 
1 
1 
1 
2 I 1I
I . I 
--[TI--
I. . 
012 
~12345 
1 1 
1 111
(3) 
. I I 
--[TI--
I I I I 
012  
~~2345 
(4) 
1  
1 1 
I . . 
--[TI--
I I . . . . 
0  
012345 
Figure P2.28-1 

• • • 
• • • • 
• •• 
77 
Chapter 2 
Problems 
(a)  Can system S be time-invariant? Explain. 
(b)  Can system S be linear? Explain. 
(c)  Suppose (2) and (3) are input-output pairs of a particular system S2, and the system 
is known to be LTI. What is h[n], the impulse response of the system? 
(d) Suppose (1) is the input-output pair of an LTI system S3' What is the output of this 
system for the input in Figure P2.28-2: 
222 
o 1 2 3 4 
Figure P2.28-2 
2.29. An LTI system has impulse response defined by 
n<O 
n 
0.1, 2.3 
h[nl 
n 
4,5 
n > 5 
Determine and plot the output y[n] when the input x[n] is: 
(a) urn] 
(b) urn - 4] 
(c) u[n]- urn - 4]. 
2.30. Consider the cascade connection of two LTI systems in Figure P2.30: 
LTI 
System 1  
LTI 
System 2 
x[n] 
hj[n] 
win] 
h2[n] Ih,I'1 
y[nl 
,I Ihtll 
1 1 1 1"
...... 
0 
3 
n  
-3 
0 
n 
Figure P2.30 
(a)  Determine and sketch w[nJ if x[nJ = (-1) n u[n]. Also, determine the overall output 
y[nJ. 
(b)  Determine and sketch the overall impulse response of the cascade system; i.e., plot the 
output y[nJ = h[nJ when x[n] = 8[n]. 
(c) Now consider the input x[nJ 
28[n] + 48[n 
4] 
28[n 
12]. Sketch wIn). 
(d) For the input of part (c), write an expression for the output y[n] in terms of the overall 
impulse response h[n] as defined in part (b). Make a carefully labeled sketch of your 
answer. 

78 
Chapter 2 
Discrete-Time Signals and Systems 
2.31.  If the input and output of a causal LTI system satisfy the difference equation 
y[n]=ay[n 
l]+x[n], 
then the impulse response of the system must be h[n] 
anu[n]. 
(Il)  For what values of a is this system stable? 
(b) Consider a causal LTI system for which the input and output are related by the differ­
ence equation 
yIn] = ay[n 
1] + x[n] 
aNx[n 
N], 
where N is a positive integer. Determine and sketch the impulse response of this system. 
Hint: Use linearity and time-invariance to simplify the solution. 
(c)  Is the system in part (b) an FIR or an IIR system? Explain. 
(d)  For what values of a is the system in part (b) stable? Explain. 
2.32.  For X (eiw) = 1/(1 - ae-iw), with -1 < a < 0, determine and sketch the following as a 
function of w: 
(a)  Re{X (eiw)} 
(b)  Im{X (eiw)} 
(c)  IX (eiW)1 
(d)  Lx (e jW ). 
2.33.  Consider an LTI system defined by the difference equation 
YIn] = -2x[n] + 4x[n 
1] 
2x[n 
2]. 
(a)  Determine the impulse response of this system. 
(b) Determine the frequency response of this system. Express your answer in the form 
H (eJw ) = A(ejw)e-jwn d , 
where A(ejW) is a real function of w. Explicitly specify A(eJW ) and the delay nd of this 
system. 
(c)  Sketch a plot of the magnitude IH(ejW)1 and a plot of the phase LH(ejW). 
(d) Suppose that the input to the system is 
xlln] = 1 + eiO.5rrn 
00 < n < 00. 
Use the frequency response function to determine the corresponding output Yl[n]. 
(e)  Now suppose that the input to the system is 
xz[n] = (1 + ejO.5rrn)u[n] 
-
00 < n < 00. 
Use the defining difference equation or discrete convolution to determine the corre­
sponding output yz[n] for -00 < n < 00. Compare yt£n] and yz[n]. They should be 
equal for certain values of n. Over what range of values of n are they equal? 
2.34.  An LTI system has the frequency response 
jw 
jw
H ejw = 1-1.25e-
= 1-
0.45e­
( 
) 
1 - O.8e-JW 
1 
O.8e-)W 
(Il)  Specify the difference equation that is satisfied by the input x[n) and the output yIn]. 
(b)  Use one of the above forms of the frequency response to determine the impulse re­
sponse h[n). 
(c)  Show that IH(ejW)lz 
G2, where G is a constant. Determine the constant G. (This is 
an example of an all pass filter to be discussed in detail in Chapter 5.) 
(d)  If the input to the above system is x[n] = cos(O.2:rrn), the output should be ofthe form 
yIn) 
A cos(O.2:rrn +8). What are A and ()? 

19 
Chapter 2 
Problems 
2.35. An LTI system has impulse response given by the following plot: 
h[nJ 
1 
o 
n 
-1 
Figure P2.35-1 
The input to the system, x[nl, is plotted below as a function of n. 
x[n] 
1 
o 
2 
3 
n 
-1 
Figure P2.35-2 
(a)  Use discrete convolution to determine the output of the system y[nl = x[n] *h[n] for 
the above input. Give your answer as a carefully labeled sketch of yIn] over a range 
sufficient to define it completely. 
(b) The deterministic autocorrelation of a signal x[n] is defined in Eq. (2.188) as Cxx [n] = 
x[n] *x[-nJ. The system defined by Figure P2.35-1 is a matched filter for the input in 
Figure P2.35-2. Noting that h[nJ = x[-(n 
4)), express the output in part (a) in terms 
of cxx[n]. 
(c)  Determine the output of the system whose impulse response is h[n] when the input is 
x[n] = urn + 2]. Sketch your answer. 
2.36. An LTI discrete-time system has frequency response given by 
(1 - je-jW)(l + je-jW)  
1 - O.8e-jw  
(a)  Use one of the above forms of the frequency response to obtain an equation for the 
impulse response h[n] of the system. 
(b)  From the frequency response, determine the difference equation that is satisfied by 
the input x[n] and the output y[n1 of the system. 
(c)  If the input to this system is 
x[n]=4+2cos(won) for 
oo<n<oo, 
for what value of wo will the output be of the form 
y[n] = A 
constant 
for -00 < n < oo? What is the constant A? 

80 
Chapter 2 
Discrete-TIme Signals and Systems 
2.37. Consider the cascade of LTI discrete-time systems shown in Figure P2.37. 
x[n] 
Figure P2.37 
The first system is described by the frequency response 
Iwl :::: 0.25rr
HI (ej(O) = e- jw { ~ 
O.25rr < Iwl :::: rr 
and the second system is described by 
sin(0.5rrn)
h2[n] = 2---­
rrn 
(a)  Determine an equation that defines the frequency response, H(ejllJ ), of the overall 
system over the range -rr 
w :::: rr. 
(b)  Sketch the magnitude, IH(ejW)I, and the phase, LH(ejW), of the overall frequency 
response over the range -rr :::: w :::: rr. 
(c)  Use any convenient means to determine the impulse response h[n] of the overall cas­
cade system. 
2.38.  Consider the cascade of two LTI systems shown in Figure P2.38. 
LTI 
LTI
x[n] 
w[n] 
Y[Il]
System 1 
System 2 
h2[n]
hl[n] 
Figure P2.38 
The impulse responses of the two systems are: 
10<n s 4 
hl[n] 
urn  
5] 
and 
h2[n] = { 0 otherwise. 
(a)  Make a sketch showing bothh2[k] and hI [n -k] (for some arbitrary n < 0) as functions 
ofk. 
(b)  Determine h[nJ = hl[n] *h2[n], the impulse response of the overall system. Give your 
answer as an equation (or set of equations) that define h[n] for -00 < n < 00 or as a 
carefully labelled plot of h[n] over a range sufficient to define it completely. 
2.39.  Using the definition of linearity (Eqs. (2.23a)-(2.23b », show that the ideal delay system 
(Example 2.2) and the moving-average system (Example 2.3) are both linear systems. 
2.40.  Determine which of the following signals is periodic. If a signal is periodic, determine its 
period. 
(a)  x[n] = ej (2nn/S) 
(b)  x[n] = sin(rrn/19) 
jnn 
(c)  x[n] = ne
(d)  x[n] = ejn • 

81 
Chapter 2 
Problems 
2.41.  Consider an LTI system with IH(ejW)1 = 1, and let arg[H(ejW )] be as shown in Figure P2.41. 
If the input is 
x[n] 
cos C; n +~),  
determine the output y[n].  
!  arg[H(eiw)] 
57r16 
Slope =-1/3 
--
17/2
Slope 
113 
-7r 
w 
-S7r/6 
Figure P2.41 
2.42.  The sequences s[n], x[n], and w[n] are sample sequences of wide-sense stationary random 
processes where 
s[n] = x[n]w[n]. 
The sequences x[n] and w[n] are zero-mean and statistically independent. The autocorre­
lation function of w[n] is 
E {w[n]w[n + m]} 
and the variance of x[n] is a}. 
Show that s[n] is white, with variance a}a~. 
Advanced Problems 
2.43.  The operator T represents an LTI system. As shown in the following figures, if the input 
to the system T is (~)n urn], the output of the system is g[n]. If the input is x[n], the output 
is y[n]. 
x[n] -IL-_T--I~ y[n] Figure P2.43 
Express y[n] in terms of g[n] and x[n]. 

• • 
• 
Chapter 2 
Discrete-Time Signals and Systems
82 
2.44.  X(ejW ) denotes the Fourier transform of the complex-valued signal x[n], where the real 
and imaginary parts of x[n] are given in Figure P2.44. (Nole: The sequence is zero outside 
the interval shown.) 
3 3 
. I II I II . • 
... 
-5 -4 -3 -2 -1 0 I 2 
... 
n 
2 
3• 
... 
-5 
1 2 
... 
n 
-2 
-3 
Figure P2.44 
Perform the following calculations without explicitly evaluating X (ejW ). 
(8) Evaluate X (ejW)w=o, 
(b) Evaluate X(e}W)iw=Jr' 
(c) Evaluate r:"Jr X(ejW)dw. 
(d) Determine and sketch the signal (in the time domain) whose Fourier transform is 
X(e-}W). 
(e) Determine and sketch the signal (in the time domain) whose Fourier transform is 
jlm(X(ejW )}. 
2.45. Consider the cascade of LTI discrete-time systems shown in Figure P2.45. 
LTI 
System 1 
hj[n],Ht(ej",) 
LTI 
System 2 
h2[nJ, Hz(eFv )
x[n] 
w[nJ 
y[nJ 
Figure P2.45 
System 1 is described by the difference equation 
w[n] = x[n] 
x[n - 1], 
and System 2 is described by 
sin(O.5;rrn) 
. g Iwl < O.5;rr
h2[n] = 
<==> H2(eJW )
;rrn 
O.5;rr < Iwl < If. 
The input x[n] is 
x[n] = cos(O.4;rrn) + sin(.6;rrn) + 58[n 
2] + 2u[n]. 
Determine the overall output y[n]. 
(With careful thought, you will be able to use the properties ofLTI systems to write down the 
answer by inspection.) 
....  

• 
• 
• • • • 
• 
• 
• • 
• 
• • • 
• • 
• • • • • 
• • 
• • • • 
Chapter 2 
Problems  
83 
2.46. The DTFT pair 
lal < 1  
(P2.46-1) 
is given. 
-
(a) Using Eq. (P2.46-1), determine the DTFT, X(ei(U), of the sequence  
bnn <-1  
x[n] = -bnu[-n -1] = 
0 
n ~ O.
t 
What restriction on b is necessary for the DTFT of x[n] to exist? 
(b) Determine the sequence y[n] whose DTFT is 
2.47. Consider a "windowed cosine signal" 
x[n] = w[n] cos(won). 
(a) Determine an expression for X(ei "') in terms of W(ei"'). 
(b)  Suppose that the sequence w[n] is the finite-length sequence 
n -L:::: n:::: L 
w[n] = 
otherwise. 
Determine the DTFT W(ei"'). Hint: Use Tables 2.2 and 2.3 to obtain a "closed form" 
solution. You should find that W(ei"') is a real function of w. 
is 
(c)  Sketch theDTFT X (eiw)for the window in (b). For a given wo, how should L be chosen 
so that your sketch shows two distinct peaks? 
is 
2.48. The system T in Figure P2.48 is known to be time invariant. When the inputs to the system 
arexl [n], x2[n], andx3[n], the responses ofthe system are y 1 [n], Y2[n], and Y3[n], as shown. 
3 
Yt[n] 
2 
. II 1_ . 
1r ... 
o 
n  
o 
2 
n 
x2[n]  
4 
Y2[n]
-1l...--T----I~
r.  
r1 .. 
o 
n  
o 
2 3 
n 
-1'---T------'~ rr. 
o I 
2 
3 
4 n  
-2 -1 0 
n 
Figure P2.48 

84  
Chapter 2 
Discrete-Time Signals and Systems 
(a)  Detennine whether the system T could be linear. 
(b)  lithe input x[n] to the system T is B[n], what is the system response y[n]? 
(c)  What are all possible inputs x[n] for which the response of the system T can be deter­
mined from the given information alone? 
2.49.  The system L in Figure P2,49 is known to be linear. Shown are three output signals Yl[n], 
Y2[n], and Y3[n] in response to the input signals xl[n], x2[n], and x3[n], respectively. 
Xj[n]  
T3 T3 
ydn] 
II  1
• • 
• 
L 
Il  
0 
2 
3 
n
-J 
-1 
o_J 
--1 
~ 
x2[n]  
Yz[n] 
-1  
-1 
2 
3
11 •  
II 1
• • 
• 
L 
• 
• 
• 
° 
Il  
-11 
n
--1 
~  
-J
-J  
OJ 
X3[n]  
Y3[n] 
T2 
f2 
• r II • 
• •  
L
--1 
~ 
-2 
2 
n
0 
n  
-1 
-3 
Figure P2.49 
(a)  Determine whether the system L could be time invariant. 
(b)  If the input x[n] to the system L is 8[n], what is the system response y[n]? 
2.50. In Section 2.5, we stated that the solution to the homogeneous difference equation 
N 
LakYh[n-k] =0 
k=O 
is of the fonn 
N 
Yh[n] L Amz ;:'. 
(P2.50-1) 
m=l 
with the Am's arbitrary and the Zm's the N roots of the polynomial 
N 
A(z) 
L akz-k; 
(P2.50-2) 
k=O 

85 
Chapter 2 
Problems 
i.e., 
N 
N 
A(z) = I>kZ-k = n(1 - ZmZ- I ). 
k=O 
m=l 
(a)  Determine the general form of the homogeneous solution to the difference equation 
y[n] -
~y[n - 1] + ~y[n - 2] = 2x[n - 1]. 
(b)  Determine the coefficients A m in the homogeneous solution if y [-1] = 1 and y[0] = o. 
(c)  Now consider the difference equation 
y[n]- y[n - 1] + ty[n - 2] = 2x[n - 1]. 
(P2.50-3) 
If the homogeneous solution contains only terms of the form of Eq. (P2.50-1), show 
that the initial conditions y[-1] = 1 and y[O] = 0 cannot be satisfied. 
(d)  If Eq. (P2.50-2) has two roots that are identical, then, in place of Eq. (P2.50-1), Yh [n] 
will take the form 
N-I 
yhln] = L Amz~ +nBlZl'  
(P2.50-4) 
m=l 
where we have assumed that the double root is Z I. Using Eq. (P2.50-4), determine 
the general form of Yh [n] for Eq. (P2.50-3). Verify explicitly that your answer satisfies 
Eq. (P2.50-3) with x[n] = O. 
(e)  Determine the coefficients A 1 and B 1 in the homogeneous solution obtained in part (d) 
if y[-l] = 1 and y[O] = o. 
2.51. Consider a system with input x[n] and output y[n]. The input-output relation for the system 
is defined by the following two properties: 
1. y[n] - ay[n - 1] = x[n], 
2. y[O] = 1. 
(a)  Determine whether the system is time invariant. 
(b)  Determine whether the system is linear. 
(c)  Assume that the difference equation (property 1) remains the same, but the value y[O] 
is specified to be zero. Does this change your answer to either part (a) or part (b)? 
2.52. Consider the LTI system with impulse response 
h[n] = (~r urn], 
where j = 0. 
Determine the steady-state response, i.e., the response for large n, to the excitation 
x[n] = cos(Jrn)u[n]. 
2.53. An LTI system has frequency response 
l
-iW3
e 
, 
2Jr (3)
Iwl < 16 
2: ' 
H(eiw) = 
2Jr  (3)
0, 
-
-
< Iwl < Jr.
16 
2 
-
­

Chapter 2 
Discrete-Time Signals and Systems
86 
The input to the system is a periodic unit-impulse train with period N 
16; i.e., 
00 
x[n] = L 8[n + 16k]. 
k=-oo 
Determine the output of the system. 
2.54. Consider the system in Figure PZ.54. 
h2[n] =anu[n] I j 
•  
x[n]  
I 
yIn]
I 
I 
I
hj[n] porn-I] 
I 
I 
I 
__ J 
h[n]  
Figure P2.54 
(a) Determine the impulse response h[n] of the overall system. 
(b) Determine the frequency response of the overall system. 
(e) Specify a difference equation that relates the output y[n] to the input x[n]. 
(d) Is this system causal? Under what condition would the system be stable? 
2.55.  Let X (eiw ) denote the Fourier transform of the signal x[n] shown in Figure PZ.5S. Perform 
the following calculations without explicitly evaluating X (eiw ): 
• 
• 
• 
• -3 •  
• 
• 
• 
• 
• 
• 
• x:nl
rl' r . r 
l' r . 
7
I -2 -1 0 
1 
2 
3 
4 
5 
6 I 8  
n 
-1  
-1 
Figure P2.55 
(a) Evaluate X (ejW)lw=o. 
(b) Evaluate X (ejW)lw=.rr. 
(e) Find LX(e jW ). 
(d) Evaluate J':..rr X (ejW)dw. 
(e) Determine and sketch the signal whose Fourier transform is X (e- jW ). 
(f) Determine and sketch the signal whose Fourier transform is Re{X (e jW )}. 
2.56. For the system in Figure P2.S6, determine the output y[nJ when the input x[n] is 8[n] and 
H (ejW ) is an ideallowpass filter as indicated, i.e., 
H (ejW ) = {I, Iwl < ]'(/2, 
0, 
:rr /2 < Iwl :s Jr• 
.....  

87 
Chapter 2 
Problems 
(-Ww[n] 
wIn] 
(_l)n 
x[n] 
.-----,
1-----' 
___IL.-_. I 
1  
1 
I 
I 
-271" 
-71" 
71"  
271" 
w 
2 
2  
Figure P2.56 
2.57.  A sequence has the DTFT 
lal < 1. 
(a)  Find the sequence x[n]. 
(b)  Calculate 1/2Jr J!:.rr X (eiw )cos(w)dw. 
2.58.  An LTI system is described by the input-output relation 
y[n] = xfn] + 2xfn 
1] + x[n - 2J. 
(a)  Determine h[n], the impulse response of the system. 
(b)  Is this a stable system? 
(c)  Determine H (eiw ), the frequency response of the system. Use trigonometric identities 
to obtain a simple expression for H(eiw ). 
(d)  Plot the magnitude and phase of the frequency response. 
(e)  Now consider a new system whose frequency response is Hl(eiw) = H(ei(w+rr). 
Determine hI [n], the impulse response of the new system. 
2.59.  Let the real discrete-time signal x[n] with Fourier transform X (eiw ) be the input to a system 
with the output defined by 
v[n] = 1x[n], 
if n is e~en, 
. 
0, 
otherwIse. 
(a)  Sketch the discrete-time signal s[n] = 1 + cos(Jrn) and its (generalized) Fourier trans­
form S(eiw ). 
(b)  Express Y (ejW ), the Fourier transform of the output, as a function of X (eiw ) and 
S(eiw ). 
(c)  Suppose that it is of interest to approximate x[n] by the interpolated signal w[n] 
y[n]+ (1/2)(y[n +1] +y[n -1]). Determine the Fourier transform W(e jW ) as a function 
of Y (eiw). 
(d)  Sketch X (eiw ), Y (eiw ), and W(ejW ) for the case when x[n] = sin(Jrn/a)/(Jrn/a) and 
a > 1. Under what conditions is the proposed interpolated signal w[n] a good approx­
imation for the original x[n]? 

88 
Chapter 2 
Discrete-Time Signals and Systems 
2.60.  Consider a discrete-time LTI system with frequency response H(eJW ) and corresponding 
impulse response h[n]. 
(a)  We are first given the following three clues about the system: 
(i) The system is causal. 
(ii)  H(eJW ) = H*(e- jW). 
(iii) The DTFT ofthe sequence h[n + Ij is real.  
U sing these three clues, show that the system has an impulse response of finite duration.  
(b)  In addition to the preceding three clues, we are now given two more clues: 
1 jT( 
. 
(iv)  2 
H(eJW)dw = 2.
]"( -T( 
(v)  H(eJT() = O. 
Is there enough information to identify the system uniquely? If so, determine the 
impulse response h[n]. If not, specify as much as you can about the sequence h[n). 
2.61.  Consider the three sequences 
urn] = urn] - u[n - 6], 
wIn] = 8[n] + 28[n 
2] + 8[n - 4]. 
qln] 
vln] * wIn). 
(a)  Find and sketch the sequence q[n]. 
n-l 
(b) Find and sketch the sequence r[n] such that r[nj * v[n] = L q[k].  
k=-oo  
(c)  Isq[-n] 
v[-n]*w[-n]?Justifyyouranswer. 
2.62.  Consider an LTI system with frequency response 
H(eJW) = e- J[(w/2)+ (n/4)], 
-]"{ < w ::: ]"{. 
Determine y[n], the output of this system, if the input is  
lSJl"n 
Jl") 
x[n] 
cos -4- - '3
( 
for all n. 
2.63.  Consider a system S with input x[n] and output yIn] related according to the block diagram 
in Figure P2.63-1. 
xlnl -cp-1 LTI ,,,10m r-­
h[nJ 
y[nJ 
e-j(J)on 
Figure P2.63-1 
The input x[n] is multiplied by e-jwon, and the product is passed through a stable LTI 
system with impulse response h[n]. 
(a)  Is the system S linear? Justify your answer. 
(b)  Is the system S time invariant? Justify your answer. 
(c)  Is the system S stable? Justify your answer. 
(d)  Specify a system C such that the block diagram in Figure P2.63-2 represents an al­
ternative way of expressing the input-Dutput relationship of the system S. (Note: The 
system C does not have to be an LTI system.) 

89 
Chapter 2 
Problems 
iwotl
x[n] -I 
h[n]e
HL-__c_......L------- yIn] 
. 
.' 
! 
Figure P2.63-2 
2.64. Consider an ideallowpass filter with impulse response hlp[n] and frequency response 
H 
(eJW) = 
Iwl < 0.2n,
{I, 
lp 
0, 
0.2n:5 Iwl :5 n. 
(a) A new filter is defined by the equation hdnJ 
(-l)nhlp[n] = eJJTnhlp[n]. Determine 
an equation for the frequency response of HI (ejW ), and plot the equation for Iwl < n. 
What kind of filter is this? 
(b) A second filter is defined by the equation h2[n] = 2hlp[n] cos(0.5nn). Determine the 
equation for the frequency response H2(eiw ), and plot the equation for Iwl < n. What 
kind of filter is this? 
(c) A third filter is defined by the equation  
sin(0.1nn) 
h1[n] = ----h
• 
nn 
Determine the equation for the frequency response H3(eiw ), and plot the equation 
for Iwl < n. What kind of filter is this? 
2.65. The LTI system 
H(eiw) = { . 
O<w<n, 
J, 
-n < w < 0, 
is referred to as a 90° phase shifter and is used to generate what is referred to as an analytic 
signal wEn] as shown in Figure P2.65-1. Specifically, the analytic signal wEn] is a complex­
valued signal for which 
Re{w[n]) = x[n], 
Tm{w[n]) 
yEn]. 
R.e {w[n]! 
x[n] 
Im{w[n]) 
y[n] 
Figure P2.65-1 
If R.e{X (eiw») is as shown in Figure P2.65-2 and Tm{X (ej ,,,») 
0, determine and 
sketch W(e jW ), the Fourier transform of the analytic signal wEn] = x[n] + jy[n]. 
1 
'IT 
W 
Figure P2.65-2 

90 
Chapter 2 
Discrete-Time Signals and Systems 
2.66. The autocorrelation sequence of a signal x[nJ is defined as 
00 
Rx[n] 
L x*[kJx[n+k]. 
k=-oo 
(a)  Show that for an appropriate choice of the signal g[n], R x [nJ = x[n] *g[n], and identify 
the proper choice for g[n]. 
(b)  Show that the Fourier transform of Rx[n] is equal to IX (e jUJ )12. 
2.67. The signals x[n] and y[n] shown in Figure P2.67-1 are the input and corresponding output 
for an LTI system. 
x[nJ  
y[n] 
.'] I
==> 
•  
•
2 
3 
n  
0 
n 
1 -J 
2 I 
3 • 
-u  
Figure P2.67-1 
(a)  Find the response of the system to the sequence x2[n] in Figure P2.67-2. 
X2[n] 
5 
o 
n 
-1 
Figure P2.67-2 
(b)  Find the impulse response h[n] for this LTI system. 
2.68. Consider a system for which the input x[n] and output y[n] satisfy the difference equation 
y[n] 
21 
y[n - 1] 
x[n] 
and for which y[-1] is constrained to be zero for every input. Determine whether or not 
the system is stable. If you conclude that the system is stable, show your reasoning. If you 
conclude that the system is not stable, give an example of a bounded input that results in 
an unbounded output. 
Extension Problems 
2.69. The eausality of a system was defined in Seetion 2.2.4. From this definition, show that, for an 
LTI system. causality implies that the impulse response h[nl is zero for n < O. One approach 
is to show that if h[n] is not zero for n < 0, then the system cannot be causal. Show also 
that if the impulse response is zero for n < 0, then the system will necessarily be causal. 
2.70. Consider a discrete-time system with input x[nl and output y[nl. When the input is 
x[n] = 
urn],
Gr  

91 
Chapter 2 
Problems 
the output is 
for all n.
y[n] Gr 
Determine which of the following statements is correct: 
• The system must be LTI. 
• The system could be LTI. 
• The system cannot be LTI. 
Ifyour answer is that the system must or could be LTI, give a possible impulse response. If 
your answer is that the system could not be LTI, explain clearly why not. 
2.71. Consider an LTI system whose frequency response is 
Iwl < Jr. 
Determine whether or not the system is causal. Show your reasoning. 
2.72.  In Figure P2.72, two sequences xl[n] and x2[n] are shown. Both sequences are zero for all 
n outside the regions shown. The Fourier transforms of these sequences are Xl (ejW) and 
X2(ejW), which, in general, can be expected to be complex and can be written in the form 
X 1(ejw) = Al(w)ej1h (w), 
X2(ejW ) = A 2(w)ejli:!(w), 
where Al(w),Bl(w),A2(w), and B2(w) are all real functions chosen so that both Al(w) 
and A 2(w) are nonnegative at w 
0, but otherwise can take on both positive and negative 
values. Determine appropriate choices for B1(w) and 82(w), and sketch these two phase 
functions in the range 0 < w < 2Jr. 
2 
-1 
-3 
-2 
-1 
-4  
2 
4 
5  
6 
n 
-4 
4 
4 
-4 
10 n 
-4 
-4 
Figure P2.72 
2.73.  Consider the cascade of discrete-time systems in Figure P2.73. The time-reversal systems 
are defined by the equations f[n] = e[-n] and y[n] = g[-n]. Assume throughout the 
problem that x[n] and hI [n] are real sequences. 

-
-
___ 
92 
Chapter 2 
Discrete-Time Signals and Systems 
I 
I 
I 
I 
I 
I 
x[n] 
LTI 
TIme-
reversal 
system 
f[n] 
LTI 
system 
ht[n] 
gIn] 
Hj(e i"') 
Time-
system 
l-
reversal
ht[n] 
ern] 
system
Hj(e j",) 
t1 ____ _ 
I 
I : 
I 
yIn
I 
I 
II 
Figure P2.73 
(a) Express E (ejUl ), F(ejUl ), G (e jO», and Y Cejli» in terms of X (ejUl ) and HI (e jW ). 
(b)  The result from part (a) should convince you that the overall system is LTI. Find the 
frequency response H Celw) of the overall system. 
(c)  Determine an expression for the impulse response h[n] of the overall system in terms 
of hI [nl. 
2.74.  The overall system in the dotted box in Figure P2.74 can be shown to be linear and time 
invariant. 
(a) Determine an expression for H(ejW ), the frequency response of the overall system 
from the input x[nl to the output y[n]. in terms of HI (ejW ), the frequency response of 
jrrn
the internal LTI system. Remember that (_1)n = e
. 
(b) Plot H(eJW ) for the case when the frequency response of the internal LTI system is 
H (ejm ) 
{ 1, 
lcul < CUe, 
I 
0, 
cue < lcul .:5 :rr. 
r---­
1  
I  
1  
I  
Causal LTI 
system 
1
x[n]  I 
u[n] 
wIn] 
yIn]
hIln] 
I
I 
I 
I 
I 
1 
1 
(_1)-n 
(_l)n  
1 
I
1l __ _  
_____1 
Figure P2.74 
2.75.  Figure P2.75-1 shows the input-output relationships of Systems A and B, while Figure 
P2.75-2 contains two possible cascade combinations of these systems. 
XA[n] -I SystemA ~ YArn] 
XA[-nJ 
xB[nJ-I System B ~ YEln] =XB[n + 2] 
Figure P2.75-1 
Xj[nJ-I System A H 
System B ~wj[n] 
X2[nJ -I SystemB H 
System A ~w2[nl 
Figllre P2.75-2 

93 
Chapter 2 
Problems 
Ifxdn1= x2[n], will wl[n] and w2[n] necessarily be equal? Ifyour answer is yes, clearly and 
concisely explain why and demonstrate with an example. If your answer is not necessarily, 
demonstrate with a counterexample. 
2.76. Consider the system in Figure P2.76, where the subsystems Sl and S2 are LTI. 
--------1 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
x[n]  
y[n] 
I  
yz[n] 
I 
~ _______________ J
I  
I 
Figure P2.76 
(a)  Is the overall system enclosed by the dashed box, with inputx[n] and output yEn] equal 
to the product of Y1[n] and Y2[n], guaranteed to be an LTI system? If so, explain your 
reasoning. If not, provide a counterexample. 
(b)  Suppose Sl and S2 have frequency responses HI (ejW ) and H2(e jW ) that are known to 
be zero over certain regions. Let 
_ {oJ 
Iwl::s O.2:n:,
H ( jW)
1 e 
-
unspecified, 
O.2:n: < Iwl ::s :n:, 
H2(ejW) = {unspecified, Iwl::s OA:n:. 
0, 
OA:n: < Iwl 
:n:. 
Suppose also that the input x [n] is known to be bandlimited to 0.3:n:, i.e., 
Iwl < 0.3:n:,
X (e j "') = { ~~specified, 
O.3:n:::s Iwl::s:n:. 
Over what region of -:n: ::s w < :n: is Y(e j "'), the DTFT of yIn], guaranteed to be zero? 
2.77. A commonly used numerical operation called the first backward difference is defined as 
y[n] = 'V(x[n]) = x[n] - x[n 
1J. 
where x[n] is the input and yIn] is the output of the first-backward-difference system. 
(a)  Show that this system is linear and time invariant 
(b)  Find the impulse response of the system. 
(c)  Find and sketch the frequency response (magnitude and phase). 
(d)  Show that if 
x[n] 
fIn] * gIn], 
then 
'V(x[n]) = 'V(j[n]) *g[n] = f[nJ * 'V(g[n]). 
(e)  Find the impulse response of a system that could be cascaded with the first-difference 
system to recover the input; i.e., find hi [n), where 
hi[n] * 'V(x[nD = x[n]. 

94  
Chapter 2 
Discrete-Time Signals and Systems 
2.78.  Let H(ejW) denote the frequency response of an LTI system with impulse response h[n], 
where hEn] is, in general, complex. 
(a)  Using Eq. (2.104), show that H *(e-jW) is the frequency response of a system with 
impulse response h*[nJ. 
(b)  Show that if hEn] is real, the frequency response is conjugate symmetric, i.e., 
H(e- jW) = H *(ejW ). 
2.79.  Let X (eJW ) denote the Fourier transform of x[n]. Using the Fourier transform synthesis or 
analysis equations (Eqs. (2.130) and (2.131», show that 
(a)  the Fourier transform ofx*[n] is X*(e- jW), 
(b)  the Fourier transform of x*[-n] is X*(ejW ). 
2.80.  Show that for x[n] real, property 7 in Table 2.1 follows from property 1 and that properties 
8-11 follow from property 7. 
2.81.  In Section 2.9, we stated a number of Fourier transform theorems without proof. Using the 
Fourier synthesis or analysis equations (Eqs. (2.130) and (2.131», demonstrate the validity 
of Theorems l-S in Table 2.2. 
2.82.  In Section 2.9.6, it was argued intuitively that 
Y (ejW) 
H (ejw)X (ejw ),  
(P2.82-t) 
when Y (ejW ), H (ejW ), and X (ejW) are, respectively, the Fourier transforms of the output 
YEn], impulse response hEn], and input xln] of an LTI system; Le., 
yEn] = L
00 
x[k]h[n - k].  
(P2.82-2) 
k=-oo 
Verify Eq. (P2.82-1) by applying the Fourier transform to the convolution sum given in 
Eq. (P2.82-2). 
2.83.  By applying the Fourier synthesis equation (Eq. (2.130» to Eq. (2.167) and using Theorem 3 
in Table 2.2, demonstrate the validity of the modulation theorem (Theorem 7, Table 2.2). 
2.84.  Let x[n] and yEn] denote complex sequences and X (ejW) and Y (e jaJ) their respective 
Fourier transforms. 
(a)  By using the convolution theorem (Theorem 6 in Table 2.2) and appropriate properties 
from Table 2.2, determine, in terms of x[n] and YEn], the sequence whose Fourier 
transform is X (ejW)Y*(ejW ). 
(b)  Using the result in part (a), show that 
00 
1 j1f
L  x[n]y*[n] = 
X (ejaJ)Y*(ejW)dw. 
(P2.84-1) 
n=-oo 
2n 
-1f 
Equation (P2.84-1) is a more general form of Parseval's theorem, as given in Sec­
tion 2.9.S. 
(c) Using Eq. (P2.84-1), determine the numerical value of the sum 
f sin(nn/4) sin(nn/6). 
2nn 
Snn
n=-OQ 
2.85.  Let x[n] and X (ejW ) represent a sequence and its Fourier transform, respectively. Deter­
mine, in terms of X (e jaJ ), the transforms of ys[n], Yd[n], and Ye[n] as defined below. In each 
case, sketch the corresponding output Fourier transform Ys (e jaJ ), Yd (e jW ), and Ye (ejcV), 
respectively for X (e jW) as shown in Figure P2.8S. 

95 
Chapter 2 
Problems 
w 
Figure P2.85 
(a) Sampler: 
[ 1 -
{x[n], 
n even, 
Ys n -
0, 
n odd. 
Note that ys[nl 
2{x[n] + (_l)nx[n]) and -1 = ej'n:. 
(b) Compressor: 
Yd[n] = x[2nl 
(c) Expander: 
x[nj2], 
n even,
Ye[n] 
{ 0, 
n odd. 
2.86.  The two-frequency correlation function ¢Jx(N, w) is often used in radar and sonar to evalu­
ate the frequency and travel-time resolution of a signaL For discrete-time signals, we define 
00 
¢Jx(N, w) 
L x[n +N]x*[n - N]e- jwn . 
n=-oo 
(a) Show that 
(b) If 
0< a < 1, 
find ¢Jx(N, w). (Assume that N 2:: 0.) 
(c)  The function ¢Jx (N, w) has a frequency domain duaL Show that  
¢Jx(N, w) 
1 r: X (eJ[v+(w/2)1)X*(eJ[v-(w/2)1)eJ2vNdv.  
2.87.  Let x[nl and y[n] be stationary, uncorrelated random signals. Show that if 
w[n] 
x[nJ + y[n], 
then  
2 
2 
2  
mw = mx + my and 
aw = ax + ay . 
2.88.  Let ern] denote a white-noise sequence, and let s[n] denote a sequence that is uncorrelated 
with e[n]. Show that the sequence 
y[n] 
s[n]e[n] 
is white, i.e., that  
E{y[n]y[n + mll = A 8[m],  
where A is a constant.  

96  
Chapter 2 
Discrete-Time Signals and Systems 
2.89. Consider a random signal x[n] = s[n] + ern], where both s[n] and ern] are independent 
zero-mean stationary random signals with autoeorrelation functions tPss[m] and tPee[m], 
respectively. 
(a)  Determine expressions for tPxx[ml and ¢>u(e]a). 
(b)  Determine expressions for tPxe[m] and ¢>xe(e]W). 
(c)  Determine expressions for tPxs [mI and ¢>xs (e j ")). 
2.90. Consider an LTI system with impulse response h[nl = an u[n1 with lal < 1. 
(a)  Compute the deterministic autocorrelation function tPhh[m1 for this impulse response. 
(b)  Determine the magnitude-squared function IH(ejW )12 for the system. 
(c)  Use Parseval's theorem to evaluate the integral 
:2 1]1;
1 
jW 2
IH(e
)1 d
If 
-Jf 
(J) 
for the system. 
2.91. The input to the first-backward-difference system (Example 2.9) is a zero-mean white-noise 
signal whose autocorrelation function is tPxx[mj = a}8[ml. 
(a)  Determine and plot the autocorrelation function and the power spectrum of the eor­
responding output of the system. 
(b)  What is the average power of the output of the system? 
(c)  What does this problem tell you about the first backward difference of a noisy signal? 
2.92.  Letx[n] be a 
stationary, white-noise process, with zero mean and variance 
. Let y[nl 
be the corresponding output when x[nI is the input to an LTI system with impulse response 
h[nl. Show that 
(a)  E (x[nly[nl) 
h[O]a;, 
(b)  a; aJ 
h2[nl. 
2.93.  Let x [n I be a real stationary white-noise sequence, with zero mean and variance 
. Let x [71] 
be the input to the cascade of two causal LTI discrete-time systems, as shown in Figure P2.93. 
--......Q 
.Q . 
x[n) 
L..:J YIn] L.:.J 
wIn) 
Figure P2.93 
(a)  Isa; 
a} I:~ohr[k]? 
(b)  Is a~ = a§ I:~oh~[k]? 
(c)  Let hI [n] = anu[n] and h2[1l] 
bnu[n]. Determine the impulse response ofthe overall 
system in Figure P2.93, and, from this, determine a~. Are your answers to parts (b) 
and (c) consistent? 
2.94. Sometimes we are interested in the statistical behavior of an LTI system when the input is 
a suddenly applied random signal. Such a situation is depicted in Figure P2.94. 
--?(o 
•B 
y[nl
x[n) 
wIn) 
(switch closed at 71 
0) 
Figure P2.94 

97 
Chapter 2 
Problems 
Let x[nJ be a stationary white-noise process. The input to the system, w[n], given by 
n:::: 0,
w[nJ = {x[n J,
0, 
n < 0, 
is a nonstationary process, as is the output y[nJ. 
(a)  Derive an expression for the mean of the output in terms of the mean of the input. 
(b)  Derive an expression for the autocorrelation sequence ¢yy[nl, n2J of the output. 
(c)  Show that, for large n, the formulas derived in parts (a) and (b) approach the results 
for stationary inputs. 
(d)  Assume that h[nJ = anu[nJ. Find the mean and mean-square values of the output in 
terms of the mean and mean-square values of the input. Sketch these parameters as a 
function of n. 
2.95.  Let x[nJ and y[nJ respectively denote the input and output of a system. The input-output 
relation of a system sometimes used for the purpose of noise reduction in images is given 
by 
a 2 [nJ
y[nJ = --T-(x[nJ- mx[n]) + mx[nJ, 
ax [nJ 
where 
1 n+l 
a;[nJ ="3 L (x[kJ- mx [n])2, 
k=n-l 
1 n+1 
mx[nJ ="3 L x[k], 
k=n-l 
2 
{a;[nJ- a~, a;[nJ :::: a~, 
as [nJ = 0, 
otherwise, 
and a.~ is a known constant proportional to the noise power. 
(a)  Is the system linear? 
(b)  Is the system shift invariant? 
(c)  Is the system stable? 
(d)  Is the system causal? 
(e)  For a fixed x[n], determine y[nJ when a~ is very large (large noise power) and when 
a~ is very small (small noise power). Does y[nJ make sense for these extreme cases? 
2.96.  Consider a random process x[nJ that is the response of the LTI system shown in Fig­
ure P2.96. In the fifure, w[nJ represents a real zero-mean stationary white-noise process 
with E{w2[n]} = aw . 
H(eiw) = 
1. 
wIn] 
1-0.5 e-]W 
x[n]  Figure P2.96 
(a)  Express [{x2[n]} in terms of ¢xx[n] or <l>xx(eiw). 
(b)  Determine <I> x x (eiw ), the power density spectrum of x[n]. 
(c)  Determine ¢xx [n], the correlation function of x [n]. 

98  
Chapter 2 
Discrete-Time Signals and Systems 
2.97.  Consider an LTI system whose impulse response is real and is given by h[n]. Suppose the 
responses of the system to the two inputs x[n] and v[n] are, respectively, yln) and z[n), as 
shown in Figure P2.97. 
~ 
~] 
~ 
~]
Figure P2.97 
The inputs x[nj and v[n j in the figure represent real zero-mean stationary random processes 
with autocorrelation functions tPxx[n] and tPvv[n], cross-correlation function tPxv[n], power 
spectra ¢xx(ej(ll) and ¢vv(ejW ). and cross power spectrum ¢xv(ej(ll). 
(a)  Given tPxx[nJ, tPvvln], tPxvlnl, ¢xx(e}W), ¢vv(e}W), and ¢xv(e}W), determine ¢yz(eJW ), 
the cross power spectrum of y[n] and z[n], where ¢yz(eJW) is defined by 
:F 
. 
tPyzlnj +---+ ¢yz(e}W), 
with tPyZ£nl 
E{y[k)z[k 
n]}. 
(b)  Is the cross power spectrum ¢xv (eJW ) always nonnegative; i.e., is ¢xv(eJW) 2:: 0 for all 
w? Justify your answer. 
2.98.  Consider the LTI system shown in Figure P2.98. The input to this system, ern], is a station­
ary zero-mean white-noise signal with average power a;. Ine first system is a backward­
difference system as defined by I[nj = ern] 
ern 1]. The second system is an ideallowpass 
filter with frequency response 
Hz(ejW ) = {  1, 
Iwi < We, 
0, 
We < Iwl :5 Tf. 
ern] 
g[n] Figure P2.98 
(a)  Determine an expression for ¢ jj(e}W), the power spectrum of I[nj, and plot this 
expression for - 2rr < w < 2rr. 
(b)  Determine an expression for rPjj[m], the autocorrelation function of f[n]. 
(c)  Determine an expression for ¢gg(eiw), the power spectrum of g[n), and plot this 
expression for -2rr < w < 2rr. 
(d)  Determine an expression for o}, the average power of the output. 

3.0 INTRODUCTION 
In this chapter, we develop the z-transform representation of a sequence and study 
how the properties of a sequence are related to the properties of its z-transform. The 
z-transform for discrete-time signals is the counterpart of the Laplace transform for 
continuous-time signals, and they each have a similar relationship to the corresponding 
Fourier transform. One motivation for introducing this generalization is that the Fourier 
transform does not converge for all sequences, and it is useful to have a generalization of 
the Fourier transform that encompasses a broader class of signals. A second advantage 
is that in analytical problems, the z-transform notation is often more convenient than 
the Fourier transform notation. 
3.1 z-TRANSFORM 
The Fourier transform of a sequence x[n] was defined in Chapter 2 as 
X(e jW) = 
00L x[n]e-jf'}n. 
(3.1) 
n=-oo 
The z-transform of a sequence x[n] is defined as 
X(z) = 
00L x[n]z-n. 
(3.2) 
n=-oo 
99 

100 
Chapter 3 
The z-Transform 
This equation is, in general, an infinite sum or infinite power series, with z considered to 
be a complex variable. Sometimes it is useful to consider Eq. (3.2) as an operator that 
transforms a sequence into a function. That is, the z-transform operator Z{·}, defined as 
00 
Z(x[n]} = L 
x[n]z-n 
X(z), 
(3.3) 
n=-(XJ 
transforms the sequence x[n] into the function X(z), where z is a continuous complex 
variable. The unique correspondence between a sequence and its z-transform will be 
indicated by thc notation 
x[n] 
X(z). 
(3.4) 
The z-transform, as we have defined it in Eq. (3.2), is often referred to as the 
two-sided or bilateral z-transform, in contrast to the one-sided or unilateral z-transform, 
which is defined as 
00 
X(z) = Lx[n]z-n. 
(3.5) 
n=O 
Clearly, the bilateral and unilateral transforms are identical if x[n] 
0 for n < 0, but 
they differ otherwise. We shall give a briefintroduction to the properties of the unilateral 
z-transform in Section 3.6. 
It is evident from a comparison of Eqs. (3.1) and (3.2) that there is a close rela­
tionship between the Fourier transform and the z-transform. In particular, if we replace 
the complex variable z in Eq. (3.2) with the complex quantity ejw , then the z-transform 
reduces to the Fourier transform. This is the motivation for the notation X (eiw) for the 
jw
Fourier transform. When it exists, the Fourier transform is simply X (z) with z = e
.This 
corresponds to restricting zto have unity magnitude; i.e., for Iz I = 1, the z-transform cor­
responds to the Fourier transform. More generally, we can express the complex variable 
z in polar form as 
jw
Z 
re
. 
With z expressed in this form, Eq. (3.2) becomes 
00 
X(re jW ) = 
L 
x[n](rejW)-n, 
n=-oo  
or  
00 
X(rejW ) = 
L 
(x[n]r-n)e- jwn . 
(3.6) 
n=-oo 
Equation (3.6) can be interpreted as the Fourier transform of the product of the original 
n
sequence x[n] and the exponential sequence r- . For r = 1, Eq. (3.6) reduces to the 
Fourier transform of x[n]. 
Since the z-transform is a function of a complex variable, it is convenient to de­
scribe and interpret it using the complex z-plane. In the z-plane, the contour corre­
sponding to Izl 
1 is a circle of unit radius, as illustrated in Figure 3.1. This contour, 
referred to as the unit circle, is the set of points z 
ejw for 0 :::.: w < 2T(. The z-transform 
evaluated on the unit circle corresponds to the Fourier transform. Note that w is the 
angle between the vector from the origin to a point z on the unit circle and the real axis 

101 
Section 3.1 
z-Transform 
Tm 
z-plane 
1 
Re 
Figure 3.1 
The unit circle in the 
complex z-plane. 
of the complex ::-plane. If we evaluate X (::) at points on the unit circle in the ::-plane 
beginning at:: = 1 (i.e., w = 0) through:: 
j (Le., W 
Jr/2) to z = -1 (i.e., W 
Jr), we 
obtain the Fourier transform for 0 ~ W ~ Jr. Continuing around the unit circle would 
correspond to examining the Fourier transform from W = Jr to W = 2Jr or, equivalently, 
from W = -Jr to W = O. In Chapter 2, the Fourier transform was displayed on a linear 
frequency axis. Interpreting the Fourier transform as the ::-transform on the unit circle 
in the ::-plane corresponds conceptually to wrapping the linear frequency axis around 
the unit circle with w = 0 at :: = 1 and W 
Jr at :: 
-1. With this interpretation, the 
inherent periodicity in frequency of the Fourier transform is captured naturally, since 
a change of angle of 2Jr radians in the z-plane corresponds to traversing the unit circle 
once and returning to exactly the same point. 
As we discussed in Chapter 2, the power series representing the Fourier transform 
does not converge for all sequenees; i.e., the infinite sum may not always be finite. 
Similarly, the ::-transform does not converge for all sequences or for all values of z. 
For any given sequence, the set of values of:: for which the ::-transform power series 
converges is called the region ofconvergence (ROC), of the ::-transform. As we stated 
in Section 2.7, if the sequence is absolutely summable, the Fourier transform converges 
to a continuous function of w. Applying this criterion to Eq. (3.6) leads to the condition 
IX(rejW)1 ~ L
00 
Ix[n]r-nl < 00 
(3.7) 
ll=~OO 
for convergence of the z-transform. From Eq. (3.7) it follows that, because of the mul­
n
tiplication of the sequence by the real exponential r-
, it is possible for the ::-transform 
to converge even if the Fourier transform (r = 1) does not. For example, the sequence 
x[n] 
urn] is not absolutely summable, and therefore, the Fourier transform power 
series does not converge absolutely. However, r-nu[n] is absolutely summable if r > 1. 
This means that the ::-transform for the unit step exists with an ROC r 
1::1 > 1. 
Convergence of the power series of Eq. (3.2) for a given sequence depends only 
on 1::1, since IX(::)I < 00 if 
L 
00 
Ix[n]II::I-n < 00, 
(3.8) 
n=-oo 
i.e., the ROC of the power series in Eq. (3.2) consists of all values of z such that the 
inequality in Eq. (3.8) holds. Thus, if some value of z, say, :: = ::1, is in the ROC, 

102 
Chapter 3 
The z-Transform 
1Im 
z-plane 
Figure 3.2 
The ROC as aring in the 
z-plane. For specific cases, the inner 
boundary can extend inward to the 
origin, and the ROC becomes adisc. For 
other cases, the outer boundary can 
extend outward to infinity, 
then all values of z on the circle defined by Izi = iZ1! will also be in the ROC. As 
one consequence of this, the ROC will consist of a ring in the z-plane centered about 
the origin. Its outer boundary will be a circle (or the ROC may extend outward to 
infinity), and its inner boundary will be a circle (or it may extend inward to include the 
origin). This is illustrated in Figure 3.2. If the ROC includes the unit circle, then this of 
course implies convergence of the z-transform for Izl 
1, or equivalently, the Fourier 
transform of the sequence converges. Conversely, if the ROC does not include the unit 
circle, the Fourier transform does not converge absolutely. 
A power series of the form of Eq. (3.2) is a Laurent series. Therefore, a number 
of elegant and powerful theorems from the theory of functions of a complex variable 
can be employed in the study of the z-transform. (See Brown and Churchill (2007).) 
For example, a Laurent series, and therefore the z-transform, represents an analytic 
function at every point inside the ROC; hence, the z-transform and all its derivatives 
must be continuous functions of z within the ROC. This implies that if the ROC includes 
the unit circle, then the Fourier transform and all its derivatives with respect to W must 
be continuous functions of w. Also, from the discussion in Section 2.7, the sequence 
must be absolutely summable, i.e., a stable sequence. 
Uniform convergence of the z-transform requires absolute summability of the 
exponentially weighted sequence, as stated in Eq. (3.7). ~either of the sequences 
xdn] = sin Wen 
-00 < n < 00, 
(3.9) 
nn 
and 
x2[n] 
cos {VOn, 
-00 < n < 00, 
(3.10) 
is absolutely summable. Furthermore, neither of these sequences multiplied by r-n 
would be absolutely summable for any value of r. Thus, neither of these sequences has a 
z-transform that converges absolutely for any z. However, we showed in Section 2.7 that 
even though a sequence such asx1 [n] inEq. (3.9) isnot absolutely summable, it does have 
finite energy (Le., it is square-summable), and the Fourier transform converges in the 
mean-square sense to a discontinuous periodic function. Similarly, the sequence x2[n] 
in Eq. (3.10) is neither absolutely nor square summable, but a useful Fourier transform 
for x2[n] can be defined using impulse functions (i.e., generalized functions or Dirac 
delta functions). In both cases, the Fourier transforms are not continuous, infinitely 

Section 3.1 
s 
It 
o 
e 
,f 
:r 
it 
~r 
Ie 
.) 
ic 
~s 
~s 
st 
:e 
9) 
I-Transform 
103 
differentiable functions, so they cannot result from evaluating a z-transform on the unit 
circle. Thus, in such cases it is not strictly correct to think of the Fourier transform as 
being the z-transform evaluated on the unit circle, although we nevertheless continue 
to use the notation X (e jW ) always to denote the discrete-time Fourier transform. 
The z-transform is most useful when the infinite sum can be expressed in closed 
form, i.e., when it can be "summed" and expressed as a simple mathematical formula. 
Among the most important and useful z-transforms are those for which X (z) is equal 
to a rational function inside the ROC, i.e., 
P(z) 
(3.11)
X(z) = Q(z)' 
where P(z) and Q(z) are polynomials in z. In general, the values of z for which X (z) = 0 
are the zeros of X (z), and the values of z for which X (z) is infinite are the poles of X (z). 
In the case of a rational function as in Eq. (3.11), the zeros are the roots of the numer­
ator polynomial and the poles (for finite values of z) are the roots of the denominator 
polynomial. For rational z-transforms, a number of important relationships exist be­
tween the locations of poles of X(z) and the ROC of the z-transform. We discuss these 
more specifically in Section 3.2. However, we first illustrate the z-transform with several 
examples. 
Example 3.1 
Right-Sided Exponential Sequence 
Consider the signal x[n] = anu [n], where a denotes a real or complex number. Because 
it is nonzero only for n :::: 0, this is an example of the class of right-sided sequences, 
which are sequences that begin at some time Nl and have nonzero values only for 
Nl ::: n < 00; i.e., they occupy the right side of a plot of the sequence. From Eq. (3.2), 
00 
00 
X(z) = L 
anu[n]z-n = L(az-l)n. 
n=-oo 
n=O 
For convergence of X(z), we require that 
00 
L 
laz-lln < 00. 
n=O 
Thus, the ROC is the range ofvalues ofz for which laz-ll < lor, equivalently, Izl > lal. 
Inside the ROC, the infinite series converges to 
00 
In 
1 
z 
X(z) = "(az- ) = 
= --, 
Izl> lal. 
(3.12)
~ 
1- az-l 
z - a 
n=O 
To obtain this closed-form expression, we have used the familiar formula for the sum 
of terms of a geometric series (see Jolley, 1961). The z-transform of the sequence 
x[n] = anu[n] has an ROC for any finite value of lal. For a = 1, x[n] is the unit step 
sequence with z-transform 
1 
X(z) = ---1' 
Izi > 1. 
(3.13)
1-C 
If lal < 1, the Fourier transform of x[n] = anu[n] converges to 
. 
1 
X(eJW ) = 
.. 
(3.14)
1- ae-Jw 

104 
Chapter 3 
The z-Transform 
However, if a 2: 1, the Fourier transform of the right-sided exponential sequence does 
not converge. 
Figure 3.3 
Pole-zero plot and ROC for Example 3.1. 
In Example 3.1, the infinite sum is equal to a rational function of zinside the ROC. 
For most purposes, this rational function is a much more convenient representation 
than the infinite sum. We will see that any sequence that can be represented as a sum 
of exponentials can equivalently be represented by a rational z-transform. Such a z­
transform is determined to within a constant multiplier by its zeros and its poles. For 
this example, there is one zero, at z 
0, and one pole, at z 
a. The pole-zero plot 
and the ROC for Example 3.1 are shown in Figure 3.3 where the symbol "0" denotes 
the zero and the symbol" x" the pole. For la I 2: 1, the ROC does not include the unit 
circle, consistent with the fact that, for these values of a, the Fourier transform of the 
exponentially growing sequence anuln] does not converge. 
Example 3.2 
Left-Sided Exponential Sequence 
Now let 
x[nJ 
-anu[-n -1] 
{ ~an 
n 
-1 
n> -1. 
Since the sequence is nonzero only for n 
this is a left-sided sequence. The 
z-transform in this case is 
00 
-1 
X(z) = -
L 
anu[-n -1Jz-n 
L 
n=-oo 
11=-00 
(3.15)
00 
00 
- La-nzn 
1 
L(a-1z)/l. 
n=1 
12=0 
If la-l;;:1 < lor, equivalently, Izl < lal. the last sum in Eq. (3.15) converges. and using 
again the formula for the sum of terms in a geometric series, 
1 
1 
z 
X(z)=1-1_a- 1z 
1 
z 
a 
Izl < 14 
(3.16) 
The pole-zero plot and ROC for this example are shown in Figure 3.4. 
~ 

Section 3.1 
z-Transform 
105 
Note that for lal < 1, thesequencc -aflu[-n 1] grows exponentially as n -+ -00, and 
thus, the Fourier transform does not exist. However, if la I > 1 the Fourier transform 
is 
(3.17) 
which is identical in form to Eq. (3.14). At first glance, this would appear to violate the 
uniqueness of the Fourier transform. However, this ambiguity is resolved if we recall 
that Eq. (3.14) is the Fourier transform of aflu[n] if lal < 1, while Eq. (3.17) is the 
Fourier transform of _an u[-n 
1] when lal > 1. 
Im 
z-plane 
1 
Re 
Figure 3.4 
Pole-zero plot and ROC for Example 3.2. 
Comparing Eqs. (3.12) and (3.16) and Figures 3.3 and 3.4, we see thatthe sequences 
and, therefore, the infinite sums are different; however, the algebraic expressions for 
X(z) and the corresponding pole-zero plots are identical in Examples 3.1 and 3.2. The 
z-transforms differ only in the ROC. This emphasizes the need for specifying both the 
algebraic expression and the ROC for the bilateral z-transform of a given sequence. 
Also, in both examples, the sequences were exponentials and the resulting z-transforms 
were rational. In fact, as is further suggested by the next example, X (z) will be rational 
whenever x[n] is a linear combination of real or complex exponentials. 
Example 3.3 
Sum of Two Exponential Sequences 
Consider a signal that is the sum of two real exponentials: 
x[n] = (~) n urn] + ( _ ~) n urn]. 
(3.18) 
The z-transform is 
(3.19) 

Chapter 3 
The z-Transform
106 
DC 
DC 
1 
= L (1zz-l )n + L (1- "3 z- )n 
n=O 
n=O 
1 
1 
2(1- izz-1)
---:-- + ----:;----:­
1- 1c1 
1 + ~Cl (1- 1C1) (1 + ~Cl)  
2z (z - iz) 
(3.20)
(z- D(z+~)" 
For convergence of X(z), both sums in Eq. (3.19) must converge, which requires that 
both Iiell < 1 and 1(- ~) ell < lor, equivalently, Izl > 1and Izl > ~. Thus, the 
ROC is the region of overlap, Iz I > 1. The pole-zero plot and ROC for the z-transform 
of each of the individual terms and for the combined signal are shown in Figure 3.5. 
(a) 
(b) 
(c) 
Figure 3.5 
Pole-zero plot and ROC for the individual terms and the sum of terms 
in Examples 3.3 and 3.4. (a) 1/(1- ~r1), Izl >~. (b)1/(1 + 1r1 ), Izl > 1. 
(e) 1/(1 -
~r1) + 1/(1 + 1Z-1), Izl > ~ . 
... 

Section 3.1 
l-Transform 
107 
In each of the preceding examples, we started with the definition of the sequence 
and manipulated each of the infinite sums into a form whose sum could be recognized. 
When the sequence is recognized as a sum of exponential sequences of the form of 
Examples 3.1 and 3.2, the z-transform can be obtained much more simply using the 
fact that the z-transform operator is linear. Speciflcally, from the definition of the z­
transform in Eq. (3.2), if x[n] is the sum of two terms, then X (z) will be the sum of the 
corresponding z-transforms of the individual terms. The ROC will be the intersection of 
the individual ROCs, i.e., the values of z for which both individual sums converge. We 
have already demonstrated the linearity property in obtaining Eq. (3.19) in Example 
3.3. Example 3.4 shows how the z-transform in Example 3.3 can be obtained in a much 
more straightforward manner by expressing x[n] as the sum of two sequences. 
Example 3.4 Sum of Two Exponentials (Again' 
Again, let x[n] be given by Eq. (3.18). Then using the general result of Example 3.1 
with a = ~ and a 
, the z-transforms of the two individual terms are easily seen 
to be 
1 
1
(~r urn] 
1 
1 -1' 
Izl> 2' 
(3.21) 
-
2: Z
(Ir 
z 
(3.22)
- 3 
urn] ~ 
1 
Izl> 1 
1+ 
3' 
and, consequently, 
1 
1 
1
(~rurn] + ( - D
n urn] 
+ 
Izl> 
(3.23)
I-leI 
1+ 
2' 
2 
as determined in Example 3.3. The pole-zero plot and ROC for the z-transform of 
each of the individual terms and for the combined signal are shown in Figure 3.5. 
All the major points of Examples 3.1-3.4 are summarized in Example 3.5. 
Example 3.5 Two-Sided Exponential Sequence 
Consider the sequence 
x[n] = (- ~r urn] 
(~r u[-n -1]. 
(3.24) 
Note that this sequence grows exponentially as n ..... -00. Using the general result of 
Example 3.1 with a = -j, we obtain 
( l)n 
Z 
1 
1 
- -
urn] ~ ---;--
Izl> 3'
3 
1 +  
and using the result of Example 3.2 with a = i yields  
Z 
1 
1 
-(~ru[-n l]~ 
1 -1' 
zl < 2' 
1- 2:z 

108 
Chapter 3 
The z-Transform 
Thus, by the linearity of the z-transform, 
1 
1 
1 
1
3' < Izi and Izi < 2'
X (z) = 1 + 
(3.25)
2(1 -be!) 
2Z(z--b) 
(1 + ~Cl) (1- ~Cl) (z+§)(z D' 
In this case, the ROC is the annular region ~ < III < ~. Note that the rational function 
in this example is identical to the rational function in Example 3.4, but the ROC is 
different in this case. The pole-zero plot and the ROC for this example are shown in 
Figure 3.6. 
Since the ROC does not contain the unit circle, the sequence in Eq. (3.24) does 
not have a Fourier transform, 
Figure 3.6 
Pole-zero plot and ROC for Example 3.5. 
In each of the preceding examples, we expressed the z-transform both as a ratio 
of polynomials in z and as a ratio of polynomials in 
. From the form of the definition 
of the z-transform as given in Eq. (3.2), we see that, for sequences that are zero for 
n < 0, X (z) involves only negative powers of z. Thus, for this class of signals, it is 
particularly convenient for X (z) to be expressed in terms of polynomials in 
rather 
than z; however, even when x[n] is nonzero for n < 0, X(z) can still be expressed in 
terms of factors of the form (1 
az-1). It should be remembered that such a factor 
introduces both a pole and a zero, as illustrated by the algebraic expressions in the 
preceding examples. 
These examples show that infinitely long exponential sequences have z-transforms 
that can be expressed as rational functions of either z or z-l. The case where the se­
quence has finite length also has a rather simple form. If the sequence is nonzero only 
in the interval Nl ::: n ::: N2, the z-transform 
N2 
X(z) = L xfn]z-n 
(3.26) 
n=NI 
has no problems of convergence, as long as each of the terms Ix[n]z-nl is finite. In 
general, it may not be possible to express the sum of a finite set of terms in a closed 

109 
Section 3.1 
z-Transform 
form, but in such cases it may be unnecessary. For example, ifx[n] = 8[n]+8[n -5], then 
X(z) 
1 + z-5, which is finite for Izl > O. An example of a case where a finite number 
of terms can be summed to produce a more compact representation of the z-transform 
is given in Example 3.6. 
Example 3.6 
Finite-Length Truncated Exponential Sequence 
Consider the signal 
all , 
O:::;n 
N 
1. 
x[n] = 
0,
{ 
otherwise. 
Then 
N-l 
N-l 
1 
(3.27)
X{z) L a"z-" L (az- 1)" 
z 
a 
11=0 
,,=0 
where we have used the general formula in Eq. (2.55) to obtain a closed-form expres­
sion for the sum of the finite series. The ROC is determined by the set of values of z 
for which 
N-lL 
In < 00. 
11=0 
Since there are only a finite number of nonzero terms, the sum will be finite as long 
as az-1 is finite, which in turn requires only that lal < 00 and z :j.: O. Thus, assuming 
that lal is finite, the ROC includes the entire z-plane, with the exception of the origin 
(z 
0). The pole-zero plot for this example, with N = 16 and a real and between zero 
and unity, is shown in Figure 3.7. Specifically, the N roots of the numerator polynomial 
are at z-plane locations 
Zk = aej (2:n:k/N) , 
k = 0, 1, ...• N 
1. 
(3.28) 
(Note that these values satisfy the equation zN = aN, and when a 
1, these complex 
values are the Nth roots of unity.) The zero corresponding to k = 0 cancels the pole at 
z 
a.'Consequently, there are no poles other than the N 
1 poles at the origin. The 
remaining zeros are at z-plane locations 
Zk = aej (2:n:k/ N) • 
k = L .... N 
1. 
(3.29) 
z-plane 
15th-order pole 
Unit circle 
7r 
8 
Re 
a 
Figure 3.7 
Pole-zero plot for Example 3.6 with N = 16 and a real such that 
o< a< 1. The ROC in this example consists of all values of z except z = O. 

110 
Chapter 3 
The z-Transform 
TABLE 3.1 
SOME COMMON I-TRANSFORM PAIRS 
Sequence 
Transform  
ROC 
1. 8[n] 
2. urn] 
3. -u[-n 
1] 
4. 8[n - m] 
5. anu[nJ 
6. -anu[-n -1) 
7. nanu[n) 
8. -nanu[-n - 1] 
9. cos(won)u[n] 
10. sin(won)u[n] 
11. rn cos(won)u[nJ 
12.  rn sin(won)u[n] 
an 
0 < n < N-1
13 
,-
-
, 
. { 0, 
otherwise 
1 
1 
I­
I 
1­
z-m 
1 
l-ac 1 
1 
1 
az-1 
(l  ac 1)2 
az-1 
(1 - ac1)2 
1 - cos(wO)Z-l 
1- 2cos(wo)c 1 + c 2 
sin(WO)C1 
1 
2r 
r sin(wo)z-l 
1 - 2r cos(WO)C1 + r2c 2 
1 
aN Z-N 
1 
Allz 
Izl > 1 
Izl < 1 
All z except 0 (if m > 
Izl> lal 
Izi < lal 
Izl> lal 
Izi <  lal 
Izl> 1 
Izl> 1 
Izi > r 
Izi > r 
Izi > 0 
0) or 00 (if m < 0) 
The transform pairs corresponding to some of the preceding examples, as well as a 
number ofother commonly encountered z-transform pairs, are summarized in Table 3.1. 
We will see that these basic transform pairs are very useful in finding z-transforms given 
a sequence or, conversely, in finding the sequence corresponding to a given z-transform. 
3.2  PROPERTIES OF THE ROC FOR THE z-TRANSFORM 
The examples of the previous section suggest that the properties of the ROC depend 
on the nature of the signal. These properties are summarized in this section with some 
discussion and intuitive justification. We assume specifically that the algebraic expres­
sion for the z-transform is a rational function and that x[n) has finite amplitude, except 
possibly at n = 00 or n = -00. 

111 
Section 3.2 
Properties of the ROC for the z-Transform 
PROPERTY I: The ROC will either be of the form 0:::: rR < Izl, or Izl < rL :::: 00, or, 
in general the annulus, i.e., 0:::: rR < Izl < rL :::: 00. 
PROPERTY 2: The Fourier transform of x [n] converges absolutely if and only if the 
ROC of the z-transform of x[n] includes the unit circle. 
PROPERTY 3: The ROC cannot contain any poles. 
PROPERTY 4: If x[n] is afinite-duration sequence, i.e., a sequence that is zero except 
in a finite interval-oo < Nl :::: n :::: N2 < 00, then the ROC is the entire z-plane, 
except possibly z 
0 or z = 00. 
PROPERTY 5: If x[n] is a right-sided sequence, i.e., a sequence that is zero for 
n < Nl < 00, the ROC extends outward from the outermost (i.e., largest mag­
nitude) finite pole in X(z) to (and possibly including) z = 00. 
PROPERTY 6: If x[n] is a left-sided sequence, i.e., a sequence that is zero for 
n > N2 > -00, the ROC extends inward from the innermost (smallest magni­
tude) nonzero pole in X (z) to (and possibly including) z 
O. 
PROPERTY T A two-sided sequence is an infinite-duration sequence that is neither 
right sided nor left sided. If x[n] is a two-sided sequence, the ROC will consist 
of a ring in the z-plane, bounded on the interior and exterior by a pole and, 
consistent with Property 3, not containing any poles. 
PROPERTY 8: The ROC must be a connected region. 
Property 1 summarizes the general shape of the ROC. As discussed in Section 3.1, 
it results from the fact that the condition for convergence ofEq. (3.2) is given by Eq. (3.7) 
repeated here as 
L 
00 
Ix[nllr-n < 00 
(3.30) 
n=-oo 
where r = Izl. Equation (3.30) shows that for a given x[n], convergence is dependent 
only on r = Izl (Le., not on the angle of z). Note that if the z-transform converges for 
Izl 
ro, then we may decrease r until the z-transform does not converge. This is the 
value Izi = rR such that Ix[n]lr- n grows too fast (or decays too slowly) as n --to 00, 
so that the series is not absolutely summable. This defines rR. The z-transform cannot 
converge for r :::: rR since r-n will grow even faster. Similarly, the outer boundary rL 
can be found by increasing r from ro and considering what happens when n --to -00. 
Property 2 is a consequence of the fact that Eq. (3.2) reduces to the Fourier 
transform when Izl = 1. Property 3 follows from the recognition that X(z) is infinite at 
a pole and therefore, by definition, does not converge. 
Property 4 follows from the fact that the z-transform of a finite-length sequence 
is a finite sum of finite powers of z, i.e., 
Nz 
X(z) = L x[n]z-n. 
n=Nl 
Therefore, IX (z)1 < 00 for all z except z = 0 when N2 > 0 and/or z = 00 when Nl < O. 

112 
Chapter 3 
The z-Transform 
Properties 5 and 6 are special cases of Property 1. To interpret Property 5 for 
rational z-transforms, note that a sequence of the form 
N 
x[n] L Ak(dk)nU[n] 
(3.31) 
k=l 
is an example of a right-sided sequence composed of exponential sequences with am­
plitudes Ak and exponential factors dk. While this is not the most general right-sided 
sequence, it will suffice to illustrate Property 5. More general right-sided sequences 
can be formed by adding finite-length sequences or shifting the exponential sequences 
by finite amounts; however, such modifications to Eq. (3.31) would not change our 
conclusions. Invoking the linearity property, the z-transform of x[n] in Eq. (3.31) is 
N 
Ak 
X(z) = L 1 
(3.32) 
k=l~ 
Izi > Idkl 
Note that for values of z that lie in all of the individual ROes, Izi > Idk I, the terms can 
be combined into one rational function with common denominator 
NTI (1- dkZ-1); 
k=l 
i.e., the poles of X (z) are located at z 
db ... , dN. Assume for convenience that the 
poles are ordered so that dl has the smallest magnitude, corresponding to the innermost 
pole, and dN has the largest magnitude, corresponding to the outermost pole. The least 
rapidly increasing of these exponentials, as n increases, is the one corresponding to the 
innermost pole, i.e., dl, and the most slowly decaying (or most rapidly growing) is the 
one corresponding to the outermost pole, i.e., dN. Not surprisingly, dN determines the 
inner boundary of the ROC which is the intersection of the regions Izi > Idkl. That is, 
the ROC of the z-transform of a right-sided sum of exponential sequences is 
Izi > IdNI 
max Idkl = rR. 
(3.33)
k 
i.e., the ROC is outside the outermost pole, extending to infinity. If a right-sided se­
quence begins at II 
Nt < 0, then the ROC will not include Iz = 00. 
Another way of arriving at Property 5 is to apply Eq. (3.30) to Eq. (3.31) obtaining 
n
~It, Ak(dk)nIr-
::: t, IAkl (~ Ith/r l") < 00, 
(3.34) 
which shows that convergence is guaranteed if all the sequences Idk/rln are absolutely 
sum mabie. Again, since IdNI is the largest pole magnitude, we choose IdN/rl < 1, or 
r> IdNI. 
For Property 6, which is concerned with left-sided sequences, an exactly parallel 
argument can be carried out for a sum of left-sided exponential sequences to show 
that the ROC will be defined by the pole with the smallest magnitude. With the same 
assumption on the ordering of the poles, the ROC will be 
Izl < Idll = min Idkl 
rL, 
(3.35)
k 

113 
orm 
Section 3.2 
for 
31) 
1m­
ded 
lees 
lees 
our 
.32) 
can 
the 
lost 
east 
the 
the 
the 
It is, 
.33) 
I se­
!ling 
•.34) 
ttely 
l, or 
allel 
how 
arne 
1.35) 
Properties of the ROC for the 2-Transform 
i.e., the ROC is inside the innermost pole. If the left -sided sequence has nonzero values 
for positive values of n, then the ROC will not include the origin, z 
O. Since x[n] now 
extends to -00 along the negative n-axis, r must be restricted so that for each dk, the 
exponential sequence (dkr-1)n decays to zero as n decreases toward -00. 
n
For right-sided sequences, the ROC is dictated by the exponential weighting r-
required to have all exponential terms decay to zero for increasing n; for left-sided 
sequences, the exponential weighting must be such that all exponential terms decay to 
zero for decreasing n. Property 7 follows from the fact that for two-sided sequences, the 
exponential weighting needs to be balanced, since if it decays too fast for increasing n, 
it may grow too quickly for decreasing n and vice versa. More specifically, for two-sided 
sequences, some of the poles contribute only for n > 0 and the rest only for n < O. The 
ROC is bounded on the inside by the pole with the largest magnitude that contributes 
for 11 > 0 and on the outside by the pole with the smallest magnitude that contributes 
for n < O. 
Property 8 is intuitively suggested by ollr discussion of Properties 4 through 7. 
Any infinite two-sided sequence can be represented as a sum of a right-sided part (say, 
for n :::: 0) and a left-sided part that includes everything not included in the right-sided 
part. The right-sided part will have an ROC given by 
(3.33), while the ROC of the 
left-sided part will be given by Eq. (3.35). The ROC of the entire two-sided sequence 
must be the intersection of these two regions. Thus, if such an intersection exists, it will 
always be a simply connected annular region of the form 
rR < Izl < rL· 
'There is a possibility of no overlap between the ROes of the right- and left -sided 
parts; i.e., rL < rR. In such cases, the z-transform of the sequence simply does not exist. 
Example 3.7 
Non-Overlapping Regions of Convergence 
An example is the sequence 
(1)" 
(1)" 
x[nl 
2" 
urn] -
-3 
u[-n - 1] . 
Applying the corresponding entries from Table 3.1 separately to each part leads to 
1 
1 
X(z) = 
+ 
1 
. 
1 
1 + 3z-1 
~ 
Izl > ~ 
Izl < ~ 
Since there is no overlap between Izi > ~ and Izl < ~, we conclude that x[n] has no 
z-transform (nor Fourier transform) representation. 
As we indicated in comparing Examples 3.1 and 3.2, the algebraic expression or 
pole-zero pattern does not completely specify the z-transform of a sequence; i.e., the 
ROC must also be specified. The properties considered in this section limit the possible 
ROes that can be associated with a given pole-zero pattern. To illustrate, consider the 
poLe-zero pattern shown in Figure 3.8(a). From Properties 1, 3, and 8, there are only 
four possible choices for the ROC. These are indicated in Figures 3.8(b), (c), (d), and (e), 
each being associated with a different sequence. Specifically, Figure 3.8(b) corresponds 

Unit circle 
Tm 
z-pJane 
a 
b 
c 
Re 
(a) 
Tm 
z-pJane 
c Re 
(b) 
(c) 
Tm 
z-p[ane 
(d) 
(e) 
Figure 3.8 
Examples of four z-transforms with the same pole-zero locations, 
illustrating the different possibilities for the ROG, each of which corresponds to 
a different sequence: (b) to a right-sided sequence, (c) to a left-sided sequence, 
(d) to atwo-sided sequence, and (e) to atwo-sided sequence. 
114 
..  

115 
Section 3.3 
The Inverse l-Transform 
to a right-sided sequence, Figure 3.8( c) to a left-sided sequence, and Figures 3.8( d) and 
3.8(e) to two different two-sided sequences. If we assume, as indicated in Figure 3.8(a), 
that the unit circle falls between the pole at z = b and the pole at z 
c, then the 
only one of the four cases for which the Fourier transform would converge is that in 
Figure 3.8(e). 
In representing a sequence through its z-transform, it is sometimes convenient 
to specify the ROC implicitly through an appropriate time-domain property of the 
sequence. This is illustrated in Example 3.8. 
Example 3.8 
Stability, Causality, and the ROC 
Consider an LTI system with impulse response h[n]. As we will discuss in more detail 
in Section 
the z-transform of h[n] is called the system function of the LTI system. 
Suppose that H (z) has the pole-zero plot shown in Figure 3.9. There are three possible 
ROCs consistent with Properties 1-8 that can be associated with this pole-zero plot; 
i.e., Izl < ~, ~ < Izl < 2, and Izl > 2. However, if we state in addition that the system 
is stable (or equivalently, that h[n] is absolutely summable and therefore has a Fourier 
transform), then the ROC must include the unit circle. Thus, stability of the system 
and Properties 1-8 imply that the ROC is the region ~ < Izl < 2. Note that as a 
consequence, h[n] is two sided; therefore, the system is not causal. 
Im 
z-plane 
Unit circle 
Figure 3.9 
Pole-zero plot for the system function in Example 3.8. 
Ifwe state instead that the system is causal, and therefore thathln] is right sided, 
Property 5 would require that the ROC be the region Izi > 2. Under this condition, 
the system would not be stable; i.e., for this specific pole-zero plot, there is no ROC 
that would imply that the system is both stable and causal. 
3.3 THE INVERSE z-TRANSFORM 
In using the z-transform for analysis of discrete-time signals and systems, we must be 
ahle to move back and forth between time-domain and z -domain representations. Often, 
this analysis involves finding the z-transform ofsequences and, after some manipulation 

116 
Chapter 3 
The z-Transform 
of the algebraic expressions, finding the inverse z-transform. The inverse z-transform is 
the following complex contour integral: 
x[n] = 21 . 1. X(z)zn-1dz, 
(3.36)
rrJ fc 
where C represents a closed contour within the ROC of the z-transform. This integral 
expression can be derived using the Cauchy integral theorem from the theory of complex 
variables. (See Brown and Churchill, 2007 for a discussion of the topics of Laurent series 
and complex integration theorems, all of which are relevant to an in-depth study of fun­
damental mathematical foundations ofthe z-transform.) However, for the typical kinds 
of sequences and z-transforms that we will encounter in the analysis of discrete LTI sys­
tems, less formal procedures are sufficient and preferable to techniques based on evalu­
ation ofEq. (3.36). In Sections 3.3.1-3.3.3, we consider some of these procedures, specif­
ically the inspection method, partial fraction expansion, and power series expansion. 
3.3.1 Inspection Method 
The inspection method consists simply of becoming familiar with, or recognizing "by 
inspection," certain transform pairs. For example, in Section 3.1, we evaluated the z­
transform for sequences of the form x[n] = anu[n], where a can be either real or com­
plex. Sequences of this form arise quite frequently, and consequently, it is particularly 
useful to make direct use of the transform pair 
z 
1 
anu[n] +-----+ 
Izi > lal. 
(3.37)
1­
If we need to find the inverse z-transform of  
(3.38)
Izl> 2'
X(z) 
(1-~Cl)' 
1 
and we recall the z-transform pair of Eq. (3.37), we would recognize "by inspection " the 
associated sequence as x[n] = 0f urn]. If the ROC associated with X(z) in Eq. (3.38) 
had been Izl < !, we can recall transform pair 6 in Table 3.1 to find by inspection that 
x[n] = Gf u[-n 
IJ. 
Tables of z-transforms, such as Table 3.1, are invaluable in applying the inspection 
method. If the table is extensive, it may be possible to express a given z-transform as 
a sum of terms, each of whose inverse is given in the table. If so, the inverse transform 
(Le., the corresponding sequence) can be written from the table. 
3.3.2 Partial Fraction Expansion 
As already described, inverse z-transforms can be found by inspection if the z-transform 
expression is recognized or tabulated. Sometimes, X(z) may not be given explicitly 
in an available table, but it may be possible to obtain an alternative expression for 
X(z) as a sum of simpler terms, each of which is tabulated. This is the case for any 
rational function, since we can obtain a partial fraction expansion and easily identify 
the sequences corresponding to the individual terms . 
..  

117 
Iorrn 
Section 3.3 
The Inverse z-Transform  
mis 
To see how to obtain a partial fraction expansion, let us assume that X(z) is ex­ 
pressed as a ratio of polynomials in Z-l; Le., 
.36) 
gral 
(3.39)
X(z)
,lex 
:ries 
fun­
inds 
sys­
Such z-transforms arise frequently in the study of LTI systems. An equivalent expression 
alu­
is 
~cif-
M 
m. 
k
ZN LbkZM -
X (z) = _--,k=--,D___ 
(3.40)
N 
ZM LakZN - k 
"by 
k=O
Ie z­
:om­
Equation (3.40) explicitly shows that for such functions, there will be M zeros and N
larly 
poles at nonzero locations in the finite z-plane assuming ao, bo, aN, and bM are nonzero. 
In addition, there will be either M - N poles at z = 0 if M > N or N - M zeros at z = 0 
if N > M. In other words, z-transforms of the form of Eq. (3.39) always have the same
1.37) 
number of poles and zeros in the finite z-plane, and there are no poles or zeros at z = 00. 
To obtain the partial fraction expansion of X (z) in Eq. (3.39), it is most convenient to 
note that X (z) could be expressed in the form 
1.38) 
M 
1
n(1 
CkZ- )
'the 
X (z) = bo _k=--,l'--___ 
(3.41)
1.38) 
ao 
N 
1
n(1- dkZ- )
that 
k=l 
:tion 
where the qs are the nonzero zeros of X (z) and the dkS are the nonzero poles of X (z). 
mas 
If M < N and the poles are alIIst-order, then X (z) can be expressed as 
:arm 
X(z) = L
N 
1 
A 
k 
(3.42) 
k=1 
Obviously, the common denominator of the fractions in Eq. (3.42) is the same as the 
:orrn 
denominator in Eq. (3.41). Multiplying both sides of Eq. (3.42) by (1 
dkC 1) and
citIy 
evaluating for z =:; dk shows that the coefficients, Ak, can be found from 
dor 
any 
(3.43)
ntify 

118 
Chapter 3 
The z-Transform 
Example 3.9 
2 nd·Order Z'-Transform 
Consider a sequence x[nJ with z-transform 
1 
1 
X(z) 
(3.44)
Izl> 2'
(1 !Z-l)(l-iz-l)' 
The pole-zero plot for X (z) is shown in Figure 3.10. From the ROC and Property 
5, Section 3.2, we see that x[nJ is a right-sided sequence. Since the poles are both 
1 st-order, X (z) can be expressed in the form of Eq. (3.42); i.e., 
Al 
A2 
X(z) = 
) +--
).
(1- ~e1 
(1 !e1 
From Eq. (3.43), 
Al 
( 1- !z-l) X(Z)j 
(1 
41 z-1) . I 
= -1, 
4 
2=1/4 
(1 
le1)(1 !z 1) z=1/4
4 
1 -1)
2z 
I 
A2 
(1 - !z-l) X(Z)i 
(1 
= 2. 
2 
z=1/2 
(l le1)(1 !e1) 2=1/2 
(Observe that the common factors between the numerator and denominator must be 
canceled before evaluating the above expressions for Al and A2') Therefore, 
-1 
2 
X(z) = 
+ 
.
(1-lel ) 
(l-~el) 
Since x[nl is right sided, the ROC for each term extends outward from the outermost 
pole. From Table 3.1 and the linearity of the z-transform, it then follows that 
x[n] = 2 Gr u[n]- (~r urn]. 
Figure 3.10 
Pole-zero plot and ROC for Example 3.9. 

119 
Section 3.3 
The Inverse z-Transform 
Clearly, the numerator that would result from adding the terms in Eq. (3.42) would 
be at most of degree (N 
1) in the variable Z-l. If M :::: N, then a polynomial must 
be added to the right-hand side of Eq. (3.42), the order of which is (M 
N). Thus, for 
M :::: N, the complete partial fraction expansion would have the form 
M-N 
N 
X(z) = L Brz-r + L -1_A_k--:-
(3.45) 
r=O 
k=l 
If we are given a rational function of the form of Eq. (3.39), with M :::: N, the Brs can 
be obtained by long division of the numerator by the denominator, with the division 
process terminating when the remainder is of lower degree than the denominator. The 
AkS can still be obtained with Eq. (3.43). 
If X (z) has multiple-order poles and M :::: N, Eq. (3.45) must be further modified. 
In particular, if X (z) has a pole of order s at z 
di and all the other poles are 15t-order, 
then Eq. (3.45) becomes 
M-N 
N 
A 
S 
C 
X(z) = L Brz-r + L. 1 _ d:z- 1 + L (1-
m 
(3.46) 
r=O 
k=l,k#1 
m=l 
The coefficients Ak and By are obtained as before. The coefficients Cm are obtained 
from the equation 
Cm = -(S-_)_,1-_--.)-s--m- {d_d_S~-~_m [(1- diW)S X(W-1)]} 
_. 
(3.47) 
m. C dI 
W 
w=d; 1 
Equation (3.46) gives the most general form for the partial fraction expansion of a 
rational z-transform expressed as a function of z-l for the case M :::: N and for di a pole 
of order s. If there are several multiple-order poles, then there will be a term like the 
third sum in Eq. (3.46) for each multiple-order pole. Ifthere are no multiple-order poles, 
Eq. (3.46) reduces to Eq. (3.45). If the order of the numerator is less than the order of 
the denominator (M < N), then the polynomial term disappears from Eqs. (3.45) and 
(3.46) leading to Eq. (3.42). 
It should be noted that we could have achieved the same results by assuming that 
the rational z-transform was expressed as a function of z instead of Z-l. That is, instead 
of factors of the form (1 - az-1), we could have considered factors of the form (z - a). 
This would lead to a set of equations similar in form to Eqs. (3.41)-(3.47) that would be 
convenient for use with a table of z-transforms expressed in terms of z. Since we find 
it most convenient to express Table 3.1 in terms of 
, the development we pursued is 
more usefuL 
To see how to find the sequence corresponding to a given rational z-transform, 
let us suppose that X(z) has only 1 st-order poles, so that Eq. (3.45) is the most general 
form of the partial fraction expansion. To find x [n], we first note that the z-transform 
operation is linear, so that the inverse transform of individual terms can be found and 
then added together to form x[n]. 
The terms Brz-r correspond to shifted and scaled impulse sequences, i.e., terms 
of the form Bro[n 
rJ. The fractional terms correspond to exponential sequences. To 
decide whether a term 
1­

120 
Chapter 3 
The z-Transform 
corresponds to (dk)nU[n] or -(ddtlu[-n - 1], we must use the properties of the ROC 
that were discussed in Section 3.2. From that discussion, it follows that if X (z) has only 
simple poles and the ROC is of the form rR < Izi < rL, then a given pole dk will 
correspond to a right-sided exponential (dk)n u[n] if Idkl ::: rR, and it will correspond to 
a left-sided exponential if Idk I 2: rL. Thus, the ROC can be used to sort the poles, with all 
poles inside the inner boundary rR corresponding to right-sided sequences and all the 
poles outside the outer boundary corresponding to left-sided sequences. Multiple-order 
poles also are divided into left-sided and right-sided contributions in the same way. The 
use of the ROC in finding inverse z-transforms from the partial fraction expansion is 
illustrated by the following examples. 
Example 3.10 
Inverse by Partial Fractions 
To illustrate the case in which the partial fraction expansion has the form of Eq. (3.45), 
consider a sequence x[n] with z-transform 
1+ 
(1+C1)2 
X(z) = 
= 
, 
Izl > 1. 
(3.48) 
1 
(l-~Cl)(1-Cl) 
The pole-zero plot for X (z) is shown in Figure 3.11. From the ROC and Property 5, 
Section 3.2, it is clear that x[n] is a right-sided sequence. Since M = N 
2 and the 
poles are all I st-order, X(z) can be represented as 
Al 
+
X(z) 
EO + 
1 -1 
1 _
1 - 2z 
The constant Eo can be found by long division: 
2 
+ 1 ~-2 + 2z-1 + 1 
z-2 - 3z-1 +2 
5;:-1 -1 
Since the remainder after one step of long division is of degree 1 in the variable C 1 , 
it is not necessary to continue to divide. Thus, X (z) can be expressed as 
-1 +5c1 
X(z) 
(3.49)
2+ (1 
1 7 - 1) 
.
2~ 
(1-c 1) 

121 
lrm 
Section 3.3 
)C 
nly 
viII 
to 
all 
:he 
ler 
he 
Lis 
15), 
~) 
r5, 
the 
t9) 
The Inverse z-Transform 
Figure 3.11 
Pole-zero plot for the z-transform in Example 3.10. 
Now the coefficients A1 and Az can be found by applying Eq. (3.43) to Eq. (3.48) 
or, equivalently, Eq. (3.49). Using Eq. (3.49), we obtain 
Therefore, 
9 
8 
X(z) = 2 - -1---:;---:- + -1---z---1 . 
(3.50) 
From Table 3.1, we see that since the ROC is Iz I > 1, 
z
2 +---+ 28[nJ, 
1­
1 
Z 
---1 +---+ u [n J.
1 c 
. 
Thus, from the linearity of the z-transform, 
( 1)11
x[n] = 28[n] 
9 
:2 
urn] + 8urn]. 
In Section 3.4, we will discuss and illustrate a number of properties of the z­
transform that, in combination with the partial fraction expansion, provide a means 
for determining the inverse z-transform from a given rational algebraic expression and 
associated ROC, even when X (z) is not exactly in the form of Eq. (3.41). The examples 
of this section were simple enough so that the computation of the partial fraction ex­
-1 

122 
Chapter 3 
The I-Transform 
pansion was not difficult. However, when X (z) is a rational function with high-degree 
polynomials in numerator and denominator, the computations to factor the denomina­
tor and compute the coefficients become much more difficult. In such cases, software 
tools such as MATLAB can implement the computations with ease. 
3.3.3 Power Series Expansion 
The defining expression for the z-transform is a Laurent series where the sequence 
values x[n] are the coefficients of en. Thus, if the z-transform is given as a power series 
in the form 
X(z) = L
00 
x[n]z-n 
(3.51)
n=-:X) 
= ... + x[-2]Z2 + x[-1]z +x[O] + x[1]c 1 + x[2]e2 + ... , 
we can determine any particular value of the sequence by finding the coefficient of the 
appropriate power of 
. We have already used this approach in finding the inverse 
transform of the polynomial part of the partial fraction expansion when M ?:. N. This 
approach is also very useful for finite-length sequences where X (z) may have no simpler 
form than a polynomial in z-l. 
Example 3.II Finite-Length Sequence 
Suppose X(z) is given in the form 
X(Z)=z2(1 
~z-1)(1+z-1)(1-z-1). 
(3.52) 
Although X(z) is obviously a rational function of z, it is really not a rational function 
in the form of Eq. (3.39). Its only poles are at z 
0, so a partial fraction expansion 
according to the technique ofSection 3.3.2 is not appropriate. However, by multiplying 
the factors of Eq. (3.52), we can express X (z) as 
1 
1 
X(z) 
z2_:;;-z 
1+ 2 
Therefore, by inspection, x[n] is seen to be 
1, 
n 
-2, 
n = -1, 
x[n] 1-1, 
n 
0, 
1 
n = 1,
2' 
0, 
otherwise. 
Equivalently, 
1 
1 
x(n] = o[n + 2] -
Zoln + 1] 
o[n] + zoln 
1]. 

123 
Section 3.3 
The Inverse z-Transform 
In finding z-transforms of a sequence, we generally seek to sum the power series 
of Eq. (3.51) to obtain a simpler mathematical expression, e.g., a rational function. If 
we wish to use the power series to find the sequence corresponding to a given X (z) 
expressed in closed form, we must expand X (z) back into a power series. Many power 
series have been tabulated for transcendental functions such as log, sin, sinh, etc. In 
some cases, such power series can have a useful interpretation as z-transforms, as we 
illustrate in Example 3.12. For rational z-transforms, a power series expansion can be 
obtained by long division, as illustrated in Example 3.13. 
Example 3.12 
Inverse Transform by Power Series Expansion 
Consider the z-transform 
X(z) 
log(1 +az-1), 
Izl > lal·  
(3.53) 
Using the Taylor series expansion for 10g(1 + x) with Ixl < 1, we obtain 
00 (-l)n+1 an -n
z
X(z) = L ~'---­
n 
n=l 
Therefore, 
I 
n+l an 
x[n] =  
(-1) 
-;;' n 2:: 1, 
(3.54) 
0, 
n:S; 0. 
When X (z) is the ratio of polynomials, it is sometimes useful to obtain a power 
series by long division of the polynomials. 
Example 3.13 
Power Series Expansion by Long Division 
Consider the z-transform 
X(z) 
Izi > lal·  
(3.55)
l-az-1 ' 
Since the ROC is the exterior ofa circle, the sequence is a right-sided one. Furthermore, 
since X (z) approaches a finite constant as z approaches infinity, the sequence is causal. 
Thus, we divide, so as to obtain a series in powers of z-l, Carrying out the long division, 
we obtain 
1 
or 
1­
Hence, x[n] 
anu[n]. 

124 
Chapter 3 
The I-Transform 
By dividing the highest power of C 1 in the denominator into the highest power of 
the numerator in Example 3.13, we obtained a series in 
. An alternative is to express 
the rational function as a ratio of polynomials in z and then divide. This leads to a power 
series in z from which the corresponding left-sided sequence can be determined. 
3.4 z-TRANSFORM PROPERTIES 
Many of the mathematical properties of the z-transform are particularly useful in study­
ing discrete-time signals and systems. For example, these properties are often used in 
conjunction with the inverse z-transform techniques discussed in Section 3.3 to obtain 
the inverse z-transform of more complicated expressions. In Section 3.5 and Chapter 5 
we will see that the properties also form the basis for transforming linear constant­
coefficient difference equations to algebraic equations interms of the transform vari­
able z, the solution to which can then be obtained using the inverse z-transform. In 
this section, we consider some of the most frequently used properties. In the following 
discussion, X (z) denotes the z-transform of x[n], and the ROC of X(z) is indicated by 
Rx; Le., 
x[n] 
X(z), 
ROC = Rx. 
As we have seen, Rx represents a set of values of z such that rR < Izi < rL. For 
properties that involve two sequences and associated z-transforms, the transform pairs 
will be denoted as 
xl[n] ~ Xl(Z), 
ROC = Rxj , 
Z 
x2[n] ~ X2(Z), 
ROC = RX2 ' 
3.4.1 Linearity 
The linearity property states that 
aXl[n] + bX2[n] ~ aXl (z) + bX2(Z), 
ROC contains RXI n R X2 , 
and follows directly from the z-transform definition, Eq. (3.2); i.e., 
00 
00 
00
L (ax}[n] + bX2[n])Z-n = a L XI [n]z-n +b L x2[n]Z-n. 
n=-oo 
n=-oo 
n=-oo 
--,-
~ 
Izi E 
Izi E
RXI 
RX2 
As indicated, to split the z-transform of a sum into the sum ofcorrespondingz-transforms, 
z must be in both ROCs. Therefore, the ROC is at least the intersection of the individ­
ual ROes. For sequences with rational z-transforms, if the poles of aX 1(z) + bX2 (z) 
consist of all the poles of Xl(Z) and X2(Z) (i.e., if there is no pole-zero cancellation), 
then the ROC will be exactly equal to the overlap of the individual ROCs. If the linear 
combination is such that some zeros are introduced that cancel poles, then the ROC 
may be larger. A simple example of this occurs when Xl [n] and x2[n] are of infinite 
duration, but the linear combination is of finite duration. In this case the ROC of the 

125 
Section 3.4 
z-Transform Properties 
linear combination is the entire z-plane, with the possible exception of z 
0 or z = 00. 
An example was given in Example 3.6, where x [n] can be expressed as 
x[n] = an (u[n) 
urn - N}) = anu[n} - anu[n - N]. 
Both anu[n} and anu[n -
N} are infinite-extent right-sided sequences, and their z­
transforms have a pole at z = a. Therefore, their individual ROes would both be 
Izl > lal. However, as shown in Example 3.6, the pole at z = a is canceled by a zero 
at z = a, and therefore, the ROC extends to the entire z-plane, with the exception of 
z 
O. 
We have already exploited the linearity property in our previous discussion of the 
use of the partial fraction expansion for evaluating the inverse z-transform. With that 
procedure, X(z) is expanded into a sum of simpler terms, and through linearity, the 
inverse z-transform is the sum of the inverse transforms of each of these terms. 
3.4.2 Time Shifting 
The time-shifting property is, 
x[n 
ROC = Rx(except for the 
possible addition or 
deletion of z = 0 or z = (0). 
The quantity no is an integer. If no is positive, the original sequence x[n} is shifted right, 
and if no is negative, x[n] is shifted left. As in the case of linearity, the ROC can be 
changed, since the factor z-no can alter the number of poles at z 
0 or z = 00. 
The derivation of this property follows directly from the z-transform expression 
in Eq. (3.2). Specifically, if yIn} = x[n 
no], the corresponding z-transform is 
00 
Y(z) = L x[n 
no]z-n. 
n=-oo 
With the substitution of variables m = n -
no, 
00 
Y(z) 
L x[m]z-(m+no) 
m=-co 
oc 
z-no L x[m]z-m, 
m=-oo 
or 
Y(z) 
z-noX(z). 
The time-shifting property is often useful, in conjunction with other properties 
and procedures, for obtaining the inverse z-transform. We illustrate with an example. 

126 
Chapter 3 
The I-Transform 
Example 3.1 4 
Shifted Exponential Sequence 
Consider the z-transform 
1 
1 
X(z) = 
Izl> 4' 
z-
From the ROC, we identify this as corresponding to a right-sided sequence. We can 
first rewrite X(z) in the form 
1
X(z) = 
z-l 
(3.56)
Izl> 4'
1 
This z-transform is of the form of Eq. (3.41) with M = N = 1, and its expansion in the 
form of 
(3.45) is 
4 
X(z) = -4+ 
. 
1­
(3.57) 
From Eq. (3.57), it follows that x[n] can be expressed as 
x[n] 
-48[n] + 4 ( ~rurn]. 
(3.58) 
An expression for x[n] can be obtained more directly by applying the time-shifting 
property. First, X (z) can be written as 
1 
X(z) 
Z-l (1 ~). 
Izl> 4 
(3.59) 
4z 
From the time-shifting property, we recognize the factor 
in Eq. (3.59) as being 
associated with a time shift of one sample to the right of the sequence (i)n u[n]; i.e., 
(1)n-l
x[n] 
4 
urn - 1]. 
(3.60) 
It is easily verified that Eqs. (3.58) and (3.60) are the same for all values of n; i.e., they 
represent the same sequence. 
3.4.3 Multiplication by an Exponential Sequence 
The exponential multiplication property is 
z
zox[n] +-+ X(z/zo), 
ROC = IzolRx. 
The notation ROC 
IzolRx signifies that the ROC is Rx scaled by the number Izol; Le., 
if Rx is the set of values of z such that rR < Izi < rL, then IzolRx is the set of values of z 
such that IzolrR < Izi < IzolrL. 
This property is easily shown simply by substituting Zox[n] into Eq. (3.2). As a 
consequence of the exponential multiplication property, all the pole-zero locations are 
scaled by a factor Zo, since, if X(z) has a pole (or zero) at Z 
Zl, then X(z/zo) will 
have a pole (or zero) at Z = ZOZI. If zo is a positive real number, the scaling can be 
interpreted as a shrinking or expanding of the z-plane; i.e., the pole and zero locations 

127 
Section 3.4
form 
1.56) 
1 the 
t57) 
3.58) 
fting 
3.59) 
leing 
;i.e., 
3.60) 
,they 
I; i.e., 
ofz 
z-Transform Properties 
change along radial lines in the z-plane. If Zo is complex with unity magnitude, so that 
jwo
zo = e
, the scaling corresponds to a rotation in the z-plane by an angle of (Vo; i.e., the 
pole and zero locations change in position along circles centered at the origin. This in 
turn can be interpreted as a frequency shift or translation of the discrete-time Fourier 
transform, which is associated with the modulation in the time domain by the complex 
exponential sequence eju>(jn. That is, if the Fourier transform exists, this property has 
the form 
Example 3.1 5 
Exponential Multiplication 
Starting with the transform pair  
1  
urn] 
Izl > 1, 
(3.61)
1-c1 ' 
we can use the exponential multiplication property to determine the z-transform of 
x[n] 
rn cos(wOn)u[n], 
r > O. 
(3.62) 
First, x[n] is expressed as 
1· 
1·
x[nj = -(reJWO)nu[n] + -(re-Jwo)nu[nj.
2 
2 
Then, using Eq. (3.61) and the exponential multiplication property, we see that 
1
1 
. 
2
_(reJwo)nu[n] 
Izl > r,
2 
1 
re jwoz-1 ' 
1
1 
. 
2
-ere - Jwo)nu[n] 
Izl > r.
2 
re-jwoz- 1 ' 
From the linearity property, it follows that 
1 
x (z) = -1-_---'7----:::- + 1 
2 
Izl > r 
(3.63) 
1 
Izl > r. 
1- 2r 
3.4.4 Differentiation of X(z) 
The differentiation property states that 
nx[n] 
ROC = Rx. 
This property is verified by differentiating the z-transform expression of Eq. (3.2); i.e., 
for 
00 
X(z) = L x[n]z-n, 
n=-oo 

128 
Chapter 3 
The l-Transform 
we obtain 
oc
dX(z) 
-z L (-n)x[n]z-n-l
-z~ 
n=-oo 
L 
00 
nx[n]z-n = Z{nx[n]). 
n=-oo 
We illustrate the use of the differentiation property with two examples. 
Example 3.16 
Inverse of Non-Rational Z'-Transform 
In this example, we use the differentiation property together with the time-shifting 
property to determine the inverse z-transform considered in Example 3.12. With 
X(z) 
log(l+az-1). 
Izl > lal, 
we first differentiate to obtain a rational expression: 
dX(z) 
dz 
From the differentiation property, 
az-1 
nx[n] 
Izl> lal· 
(3.64)
1 + az- l ' 
The inverse transform of Eq. (3.64) can be obtained by the combined use of the z­
transform pair of Example 3.1, the linearity property, and the time-shifting property. 
Specifically, we can express nx[n] as 
nx[n] 
a(-a)n-l u[n-1]. 
Therefore, 
an 
Z 
x[n] 
-urn - 1] +-+ log(1 + 
Izl> lal. 
n 
The result of Example 3.16 will be useful in our discussion of the cepstrum in 
Chapter 13. 
Example 3.17 
2 nd-order Pole 
As another example of the use of the differentiation property, let us determine the 
z-transform of the sequence 
x[n] 
na"u[n] 
n(anu[n]). 
From the z-transform pair of Example 3.1 and the differentiation property, it follows 
X(z) 
that 
C_~Cl)' 
Izl > lal
d 
az-1 
Izl > lal·
(1 ­

129 
Section 3.4 
z-Transform Properties 
Therefore, 
Izi > lal· 
3.4.5 Conjugation of a Complex Sequence 
The conjugation property is expressed as 
x*[nJ 
X*(z*), 
ROC 
Rx. 
Thisproperty follows in a straightforward manner from the definition of the z-transform, 
the details of which are left as an exercise (Problem 3.54). 
3.4.6 Time Reversal 
The time-reversal property is given by 
z 
1 
x*[-n] ~ X*(1/z*), 
ROC 
Rx 
The notation ROC=llRx implies that Rx is inverted; i.e., if Rx is the set of values of z 
such that rR < Izl < rL, then the ROC for X*(llz*) is the set of values of z such tbat 
llrL < Izl < llrR. Thus, if Zo is in the ROC for x[n], then liz;) is in the ROC for the 
z-transform of x*[-n]. If the sequence x[n] is real or we do not conjugate a complex 
sequence, the result becomes 
x[-n] 
X (liz), 
ROC= 1 
Rx 
As with the conjugation property, the time-reversal property follows easily from the 
definition of the z-transform, and the details are left as an exercise (Problem 3.54). 
Note that if zo is a pole (or zero) of X (z), then 11zo will be a pole (or zero) of X (liz). 
The magnitude of llzo is simply the reciprocal of the magnitude of zoo However, the 
angle of 11Zo is the negative of the angle of zoo When the poles and zeros of X (z) are 
all real or in complex conjugate pairs, as they must be when x[n] is real, this complex 
conjugate pairing is maintained. 
Example 3.18 Time-Reversed Exponential Sequence 
As an example of the use of the property of time reversal, eonsider the sequence 
x[n] =a-lIu[-n], 
which is a time-reversed version of allu[n]. From the time-reversal property, it follows 
that 
X(z) = 
Note that the z-transform of allu[n1 has a pole at z = a, while X (z) has a pole at l/a. 

130 
Chapter 3 
The z-Transform 
3.4.7 Convolution of Sequences 
According to the convolution property, 
. 
Z 
Xl [n] * X2[n] +-----+ X I (Z)X2(Z), 
ROC contains RXl n R X2 • 
To derive this property formally, we consider 
00 
y[nl = L xdk]x2[n 
kJ, 
k=-oo 
so that 
Y(z) = L
00 
y[n]z-n 
n=-cc 
n'foo L~oo xdk]x2[n 
k]I 
If we interchange the order of summation (which is allowed for z in the ROC), 
00 
00 
Y(z) = L xtlk] L x2[n 
k]z-n.
k=-oo 
n=-oo 
Changing the index of summation in the second sum from n to m 
n - k, we obtain 
Y(z) = k~oo xI[k] {m~x x2[m]Z-m I 
= f xl[k] ~ Z-k = ( f XI[k]Z-k) X2(Z) 
k=-oo 
IzlERx2 
k=-oo 
Thus, for values of z inside the ROCs of both Xl (z) and X2(Z), we can write 
Y(z) = XI(Z)X2(Z), 
where the ROC includes the intersectionofthe ROCs of Xl (z) and X2(Z). Ifa pole that 
borders on the ROC of one of the z-transforms is canceled by a zero of the other, then 
the ROC of Y (z) may be larger. 
The use of the z-transform for evaluating convolutions is illustrated by the follow­
ing example . 
...  

131 
form  
Section 3.5 
l-Transforms and LTI Systems 
Example 3.19 Convolution of Finite-Length Sequences 
Suppose that 
xl[n] 
S[n] + 2S[n - t] + S[n 
2] 
is a finite-length sequence to be convolved with the sequence x2[n] = SEn] 
S[n 
1]. 
The corresponding z-transforms are 
Xl (z) 
1 + 2z-l + z-2 
and X2(Z) = 1 
z-l. The convolution yEn] 
Xl En] *x2[n] has z-transform 
Y(z) = Xl (Z)X2 (z) = (1 + 
= 1 + 
_z-2 
Since the sequences are both of finite length, the ROCs are both Izi > 0 and therefore 
so is the ROC of Y (z). From Y (z), we conclude by inspection of the coefficients of the 
polynomial that 
yEn] = SEn] + S[n - 1] 
S[n - 2] 
S[n 
3]. 
The important point of this example is that convolution of finite-length sequences is 
equivalent to polynomial multiplication. Conversely, the coefficients of the product of 
two polynomials are obtained by discrete convolution of the polynomial coefficients. 
lin 
The convolution property plays a particularly important role in the analysis of 
LTI systems as we will discuss in more detail in Section 3.5 and Chapter 5. An example 
of the use of the z-transform for computing the convolution of two infinite-duration 
sequences is given in Section 3.5. 
3.4.8 Summary of Some z-Transform Properties 
We have presented and discussed a number of the theorems and properties of z­
transforms, many of which are useful in manipulating z-transforms in the analysis of 
discrete-time systems. These properties and a number of others are summarized for 
convenient reference in Table 3.2. 
3.5 z-TRANSFORMS AND LTI SYSTEMS 
The properties discussed in Section 3.4 make the z-transform a very useful tool for 
discrete-time system analysis. Since we shall rely on the z-transform extensively in 
Chapter 5 and later chapters, it is worthwhile now to illustrate how the z-transform 
can be used in the representation and analysis of LTI systems. 
Recall from Section 2.3 that an LTI system can be represented as the convolution 
that  
y[n] = x[n] * h[n] of the input x[n] with h[n], where h[n] is the response of the system 
to the unit impulse sequence 8[n]. From the convolution property of Section 3.4.7, it 
follows that the z-transform of y[n] is 
Y(z) 
H(z)X(z)  
(3.65) 

132 
Chapter 3 
The l-Transform 
TABLE 3.2 
SOME l-TRANSFORM PROPERTIES 
Property 
Section 
Number 
Reference 
Sequence 
Transform 
ROC 
x[nJ 
X(z) 
Rx 
Xl [n] 
XI(Z) 
RXI 
1 
2 
3.4.1 
3.4.2 
x2[n] 
aXl [nJ + bX2[n] 
x[n 
no] 
X2(Z) 
aX} (z) + bX2(Z) 
CnoX(z) 
RX2 
Contains R.q n RX2 
Rx, except for the possible 
addition or deletion of 
the origin or 00 
3 
4 
5 
3.4.3 
3.4.4 
3.4.5 
Zox[n] 
nx[n] 
x*[nJ 
X(z/zo) 
dX(z) 
x*(/,>z 
IzolRx 
'Rx 
Rx 
6 
Re{x[n]} 
~ [X(z) + X*(z*)] 
Contains Rx 
7 
8 
9 
3.4.6 
3.4.7 
Im{x[nU 
x*[-nJ 
xl[n] * x2[n] 
12/X (z) 
X*(z*)J 
X*(I/z*) 
Xl(Z)X2(Z) 
Contains Rx 
1/ Rx 
Contains RXJ n RX2 
where H(z) and X(z) are the z-transforms of h[n] and xln] respectively. In this context, 
the z-transform H (z) is called the system function of the LTl system whose impulse 
response is h[n]. 
The computation of the output of an LTl system using the z-transform is illustrated 
by the following example. 
Example 3.20 Convolution of Infinite-Length Sequences 
Leth[n] = anu[n] and x[n] = Au(n]. To use thez-transform to evaluate the convolution 
yEn] = x[n] *h[n], we begin by finding the corresponding z-transforms as 
00 
1 
"'"' n -n _ _ 
" 
H (z) = L..... a z 
-
1 _ az 
Izl> lal, 
n=O 
and 
00 
A 
X(z) = L Az-n = ----,-
Izl> L 
n=O 
1 
The z-transform of the convolution yEn] = x[n] *hEn] is therefore 
Izl > I,
Y(z) = (1 

133 
Section 3.5 
I-Transforms and LTI Systems 
where we assume that lal < 1 so that the overlap of the ROCs is Izl > 1. 
The poles and zeros of Y(z} are plotted in Figure 3.12, and the ROC is seen to 
be the overlap region. The sequence y[n) can be obtained by determining the inverse 
z-transform. The partial fraction expansion of Y(z) is 
A 
Y(z) =-­
Izl > 1.
I-a 
Therefore, taking the inverse z-transform of each term yields 
y[n] = 
A 
(1 
an+1)u[n].
I-a 
:Im 
Region of 
convergence 
Figure 3.12 
Pole-zero plot for the I-transform of the convolution of the se­
quences uln] and anu[n] (assuming lal < 1). 
The z-transform is particularly useful in the analysis of LTI systems described by 
difference equations. Recall that in Section 2.5, we showed that difference equations of 
the form 
N 
M
yIn] =-L:(:k)y[n-k1+ L:(~k)x[n k], 
(3.66) 
k=l 
0 
k=O 
0 
behave as causal LTI systems when the input is zero prior to n = 0 and initial rest 
conditions are imposed prior to the time when the input becomes nonzero; i.e., 
y[-N], y[-N + 1], ... , y[-l] 
are all assumed to be zero. The difference equation with assumed initial rest conditions 
defines the LTI system, but it is also of interest to know the system function. If we 
apply the linearity property (Section 3.4.1) and the time-shift property (Section 3.4.2) 
to Eq. (3.66), we obtain 
(3.67) 

Chapter 3 
The z-Transform
134 
Solving for Y (z) in terms of X (z) and the parameters of the difference equation yields 
M 
LbkZ-k 
k=O
Y(z) 
X(z), 
(3.68)
N 
LakZ-k 
k=O 
and from a comparison of Eqs. (3.65) and (3.68) it follows that for the LTI system 
described by Eq. (3.66), the system function is 
M 
LbkZ-k: 
H(z) = _k=_O__ 
(3.69)
N 
L  
k=O 
Since the system defined by the difference equation of Eq. (3.66) is a causal system, 
our discussion in Section 3.2 leads to the conclusion that H(z) in Eq. (3.69) must have 
an ROC of the form Izl > rR, and since the ROC can contain no poles, rR must be 
equal to the magnitude of pole of H(z) that is farthest from the origin. Furthermore, 
the discussion in Section 3.2 also confirms that if rR < 1, Le., all poles are inside the unit 
circle, then the system is stable and the frequency response of the system is obtained by 
setting z 
e jw in Eq. (3.69). 
Note that if Eq. (3.66) is expressed in the equivalent form 
N 
M
L ak}'[n - k] = L bkx[n - k] 
(3.70) 
k=O 
k=O 
then Eq. (3.69), which gives the system function (and frequency response for stable 
systems) as a ratio of polynomials in the variable 
, can be written down directly by 
observing that the numerator is the z-transform representation of the coefficient and 
delay terms involving the input, whereas the denominator represents the coefficients 
and delays of the terms involving the output. Similarly, given the system function as 
a ratio of polynomials in Z-1 as in Eq. (3.69), it is straightforward to write down the 
difference equation in the form of Eq. (3.70) and then write it in the form of Eq. (3.66) 
for recursive implementation. 
Example 3.21 
1 st-order System 
Suppose that a causal LTI system is described by the difference equation 
yen] 
ay[n 
1] + x[n]. 
(3.71) 
By inspection, it follows that the system function for this system is 
1 
(3.72)
H(z) 
1 
ar1 ' 
with ROC Izl > 14 from which it follows from entry 5 of Table 3.1 that the impulse 
response of the system is 
hen] 
anu[n]. 
(3.73) 
..  

135 
Ilorm 
Section 3.6 
The Unilateralz-Transform 
Finally, if x[n] is a sequence with a rational z-transform such as x[n] = Au[n], we 
can find the output of the system in three distinct ways. (1) We can iterate the difference 
equation in Eq. (3.71). In general, this approach could be used with any input and would 
generally be used to implement the system, but it would not lead directly to a closed­
form solution valid for all n even if such expression exists. (2) We could evaluate the 
elds 
3.68) 
convolution of x[n] and h[nJ explicitly using the techniques illustrated in Section 2.3. 
(3) Since the z-transforms of both x(nJ and h[n1are rational functions of z, we can use 
the partial fraction method of Section 3.3.2 to find a closed-form expression for the 
output valid for all n. In fact, this was done in Example 3.20. 
stem 
We shall have much more use for the z-transform in Chapter 5 and subsequent 
chapters. For example, in Section 5.2.3, we shall obtain general expressions for the 
impulse response of an LTI system with rational system function, and we shall show 
how the frequency response of the system is related to the locations of the poles and 
3.69) 
zeros of H(z). 
3.6 THE UNIlATERAL z-TRANSFORM 
stem, 
have 
The z-transform, as defined by Eq. (3.2), and as considered so far in this chapter, is 
1St be 
more explicitly referred to as the bilateral z-transform or the two-sided z-transform.In 
nore, 
contrast, the unilateral or one-sided z-transform is defined as 
eunit 
edby 
X(z) L
00 
x[n]z-n. 
(3.74) 
n=O 
The unilateral z-transform differs from the bilateral z-transform in that the lower limit 
of the sum is always fixed at zero, regardless of the values of x[n] for n < O. If x[n] =0 
for n < 0, the unilateral and bilateral z-transforms are identical, whereas, if x[n] is not 
zero for all n < 0, they will be different. A simple example illustrates this. 
itable 
Example 3.22 
Unilateral Transform of an Impulse
tlyby 
t and 
Suppose that xl[n] = SIn]. Then it is clear from Eq. (3.74) that Xl (z) = 1, which 
;ients 
is identical to the bilateral z-transform of the impulse. However, consider x2[n]  
on as  
S[n + IJ = xlln + 1]. This time using Eq. (3.74) we find that X2(Z) 
0, whereas the 
11 the 
bilateral z-transform would be X2(Z) 
zX1(z) 
z· 
:3.66) 
Because the unilateral transform in effect ignores any left-sided part, the proper­
ties of the ROC of the unilateral z-transform will be the same as those of the bilateral 
transform of a right-sided sequence obtained by assuming that the sequence values are 
zero for n < O. That is, the ROC for all unilateral z-transforms will be of the form 
Izi > rR, and for rational unilateral z-transforms, the boundary of the ROC will be 
3.71) 
defined by the pole that is farthest from the origin of the z-plane. 
In digital signal processing applications, difference equations of the form of 
3.72) 
Eq. (3.66) are generally employed with initial rest conditions. However, in some situa­
pulse 
tions, non initial rest conditions may occur. In such cases, the linearity and time-shifting 
properties of the unilateral z-transform are particularly useful tools. The linearity prop­
3.73) 
erty is identical to that of the bilateral z-transform (Property 1 in Table 3.2). The time­

136 
Chapter 3 
The z-Transform 
shifting property is different in the unilateral case because the lower limit in the uni­
lateral transform defmition is fixed at zero. To illustrate how to develop this property, 
consider a sequence x[n] with unilateral z-transform X(z) and let y[nJ = x[n -11. Then, 
by definition 
Y(z) = L
00 
x[n 
1]z-n. 
n=O 
With the substitution of summation index m = n - 1, we can write Y(z) as 
00 
00 
Y(z) L x[m]z-(m+1) = x[-1] + z-1 L x [m]z-m , 
171=-1 
171=0 
so that 
Y(z) = x[-I] +z-lX(z). 
(3.75) 
Thus, to determine the unilateral z-transform of a delayed sequence, we must provide 
sequence values that are ignored in computing X(z). By a similar analysis, it can be 
shown that if y[n] 
x[n - k], where k > 0, then 
Y(z) = x[-k] +x[-k + l]z-1 + ... +x[-1]Z-k+l +z-kX(z) 
k 
= L x[m - k - 1]z-m+1 + z-kX(z). 
(3.76) 
m=1 
The use of the unilateral z-transform to solve for the output of a difference equa­
tion with nonzero initial conditions is illustrated by the following example. 
Example 3.23 Effect of Nonzero Initial Conditions 
Consider a system described by the linear constant-coefficient difference equation 
y[n] 
ay[n -
1] 
x[n), 
(3.77) 
which is the same as the system in Examples 3.20 and 3.21. Assume that x[n] = 0 for 
n < 0 and the initial condition at n 
-1 is denoted y[-1]. Applying the unilateral 
z-transform to Eq. (3.77) and using the linearity property as well as the time-shift 
property in Eq. (3.75). we have 
Y(z) - ay[-l] - az-1y(z) = X(z). 
Solving for Y(z) we obtain 
av[-l) 
1 
y(z) 
. 
1 + 
1 X(z). 
(3.78)
1 - az-
1- az-
Note that if y[-1] 
0 the firstterm disappears, and we are left with Y(z) = H(z)X(z), 
where 
1 
H(z) 
Izl> lal 
is the system function of the LTI system corresponding to the difference equation 
in Eq. (3.77) when iterated with initial rest conditions. This confirms that initial rest 

131 
n 
Section 3.7 
Summary 
conditions are necessary for the iterated difference equation to behave as an LTI 
system. Furthermore, note that if xln] = 0 for all n, the output will be equal to 
" 
t, 
YIn] = y[_1]an+1 
n 2: -1. 
This shows that if y[-1] f::. 0, the system does not behave linearly because the scaling 
property for linear systems [Eq. (2.23b)] requires that when the input is zero for all n, 
the output must likewise be zero for all n. 
To be more specific, suppose that x[n] = Au[nJ as in Example 3.20. We can 
determine an equation for yIn] for n 2: -1 by noting that the unilateral z-transform 
of x[n] = Au[nJ is 
A 
X(z) = -1---:;-
Izl > 1 
so that Eq. (3.78) becomes 
(3.79)
Y(z) 
Applying the partial fraction expansion technique to Eq. (3.79) gives 
e 
e 
A 
aA 
ayE-I] 
--
I-a 
Y(z) = 1 
+ 1 -
+ 1 - aC1 ' 
from which it follows that the complete solution is 
n 
-1 
(3.80)
yIn] 
Equation (3.80) shows that the system response is composed of two parts. The zero 
input response (ZIR) is the response when the input is zero (in this case when A 
0). 
The zero initial conditions response (ZICR) is the part that is directly proportional 
to the input (as required for linearity). This part remains when y[-lJ 
O. In Prob­
lem 3.49, this decomposition into ZIR and ZICR components is shown to hold for any 
difference equation of the form of Eq. (3.66). 
3.'1 SUMMARY 
In this chapter, we have defined the z-transform of a sequence and shown that it is a 
generalization of the Fourier transform. The discussion focused on the properties of the 
z-transform and techniques for obtaining the z-transform of a sequence and vice versa. 
Specifically, we showed that the defining power series of the z-transform may converge 
when the Fourier transform does not. We explored in detail the dependence of the shape 
of the ROC on the properties of the sequence. A full understanding of the properties 
of the ROC is essential for successful use of the z-transform. This is particularly true in 
developing techniques for finding the sequence that corresponds to a given z-transform, 
i.e., finding inverse z-transforms. Much of the discussion focused on z-transforms that 
are rational functions in their region of convergence. For such functions, we described a 

138  
Chapter 3 
The I-Transform 
technique of inverse transformation based on the partial fraction expansion of X (z). We 
also discussed other techniques for inverse transformation, such as the use of tabulated 
power series expansions and long division. 
An important part of the chapter was a discussion of some of the many properties 
of the z-transform that make it useful in analyzing discrete-time signals and systems. A 
variety of examples demonstrated how these properties can be used to find direct and 
inverse z-transforms. 
Problems 
Basic Problems with Answers 
3.1. Determine the z-transform, including the ROC, for each of the following sequences: 
(a)  (if urn] 
(b)  -(ifu[-n -1] 
(c)  (if u[-n] 
(d)  8[n] 
(e)  8[n - 1] 
(f)  8[n + 1] 
(g)  (!f (u[n]- urn -10]). 
3.2. Determine the z-transform of the sequence 
nO n 
N 
1, 
x[n] = { N, 
N < n. 
3.3. Determine the z-transform of each of the following sequences. Include with your answer 
the ROC in the z-plane and a sketch of the pole-zero plot. Express all sums in closed form; 
a can be complex. 
lnl
(a)  xa[n] = a 
, 
0 < lal < 1.  
b 
{I, 0::: n ::: N -1,  
( ) xb[n1=  0, 
otherwise. 
n+l, 
O:::n:::N 
1, 
(c)  xdn] = 2N-l-n, N:::n:::2(N 
1), 
0, 
otherwise.
I 
Hint: Note that xb[n] is a rectangular sequence and xdn] is a triangular sequence. First, 
express xcln] in terms of xb[n]. 
3.4. Consider the z-transform X(z) whose pole-zero plot is as shown in Figure P3.4. 
(a)  Determine the ROC of X(z) if it is known that the Fourier transform exists. For this 
case, determine whether the corresponding sequence x[n] is right sided, left sided, or 
two sided. 
(b)  Howmany possible two-sided sequences have the pole-zero plot shown in Figure P3.4? 
(c)  Is it possible for the pole-zero plot in Figure P3.4 to be associated with a sequence that 
is both stable and causal? If so, give the appropriate ROC. 

139 
Chapter 3 
Problems 
-1 
Im 
1 
3 
Unit circle 
2 
z-plane 
3 
Re 
Figure P3.4 
3.5. Determine the sequence x[n] with z-transform 
X(z) = (1 + 2z)(1 + 3z-1)(1- z-I). 
3.6. Following are several z-transforms. For each, determine the inverse z-transform using both 
methods-partial fraction expansion and power series expansion--discussed in Section 3.3. 
In addition, indicate in each case whether the Fourier transform exists. 
1 
1 
(a)  X (z) = 
1 
' 
Izl > 2 
1+ -C1 
2  
1 
1  
(b)  X(z) = 
1 
' 
Izl < 
1+ 'lc 1 
2  
1-1 
1 - ZZ 
1 
(c)  X(z) = 
3 -1 
1 -2' 
Izl> 
1 + LIz 
+ gZ 
2  
I-leI  
1 
(d)  X(z) = 
21 -2' 
Izl > 2
1- LIz  
1 - az-1  
(e)  X(z) = -1 
' 
Izl > 11/al
z  -a 
3.7. The input to a causal LTI system is 
x[n] = u[-n - t] + (~r urn]. 
The z-transform of the output of this system is 
_leI 
Y~)= 
2 
.
(1- ~Cl) (1 + c 1) 
(a)  Determine H(z), the z-transform of the system impulse response. Be sure to specify 
the ROC. 
(b) What is the ROC for Y(z)? 
(c)  Determine y[n]. 
3.8. The system function of a causal LTI system is 
1- z-1 
H(z) = 
3 
. 
1 + -c1 
4 

140  
Chapter 3 
The z-Transform 
The input to this system is 
x[n] = (~r urn] + u[-n 
I]. 
(a)  Find the impulse response of the system, h[n]. 
(b)  Find the output y[n]. 
(c)  Is the system stable? That is, is h[n] absolutely summable? 
3.9.  A causal LTI system has impulse response h[n], for which the z-transform is 
1  
-1 
H(z) = 
+z 
. 
(1 
~z-1)(1 + !e1) 
(a)  What is the ROC of H(z)? 
(b)  Is the system stable? Explain. 
(c)  Find the z-transform X(z) of an input x[n] that will produce the output 
4 
y[n] = - 1 ( 
3 -41)"urn] 
3 (2)" u[-n 
I]. 
(d)  Find the impulse response h[nJof the system. 
3.10.  Without explicitly solving for X (z), find the ROC of the z-transform of each of the following 
sequences, and determine whether the Fourier transform converges: 
(a)  x[n] [(i)" +(~)"]u[n-101 
(b)  x[n] 
{1, -10 
~ ~ 10,  
0, 
otherwIse,  
(c)  x[nl = 2"u[-nj 
(d)  x[n] = [0)"+4 - (e jrr/ 3),,] urn -1] 
(e)  x[nl = urn + 10]- urn + 5]  
,,-1  
( 
)
(f) x[n]  1 
ufn] + (2 + 3j)Il-2u[-n -11. 
3.11.  Following are four z-transforms. Determine which ones could be the z-transform of a causal 
sequence. Do not evaluate the inverse transform. You should be able to give the answer by 
inspection. Clearly state your reasons in each case. 
(1 - z-I)2
(a)  
(1 leI) 
(b)  (z - 1)2 
(z D  
(z- ~f  
(c) 
6 
(z -l) 
(d)  (z _1)6  
(z if  

141 
Chapter 3 
Problems 
3.U.  Sketch the pole-zero plot for each of the following z-transforms and shade the ROC: 
ROC: Izl < 2 
x2[n] causal
(b)  X 2 (z) = ( 
1 
2 _ )'
-1) (
1 + ZZ 
I - 3z 1 
1 + 
- 2z-2 
(c) X3(Z) = --:;-;:;--,----=-
X3[n] absolutely summable, 
-
1 
3.13. A causal sequence g[n] has the z-transform 
G(z) 
sin(z-l)(1 + 3z-2 + 2z-4), 
Find g[ll]. 
3.14.  If H(z) 
--::---::- and hEn] 
I 
= A lO'1u[n] + A2O'2 urn], determine the values of AI, A2, 0'1, 
and 0'2. 
3.15.  If H (z) 
_-!.O""-'-__ for Izi > 0, is the corresponding LTI system causal? Justify your 
answer. 
3.16. When the input to an LTI system is 
x[n] = Gr urn] + (2)n u[-n 
1], 
the corresponding output is 
yEn] = 5 Grurn] 
5 (~r urn]. 
(a)  Find the system function H (z) of the system. Plot the pole(s) and zero(s) of H (z) and 
indicate the ROC. 
(b) Find the impulse response h[n] of the system. 
(c)  Write a difference equation that is satisfied by the given input and output. 
(d)  Is the system stable? Is it causal? 
3.17. Consider an LTI system with input x[n] and output yEn] that satisfies the difference equation 
5 
yEn] 
zy[n - 1] + yEn 
2] = x[nl - x[n n 
Determine all possible values for the system's impulse response hEn] at n = O. 
3.1S. A causal LTI system has the system function 
1+2z-1 + 
H(z) = ( 
) 
.
1+ic1 (l-e l ) 
(a)  Find the impulse response of the system, hEn]. 
(b)  Find the output of this system, yEn], for the input 
x[n] = 2n. 

142 
Chapter 3 
The z-Transform 
Chapre 
3.19. For each of the following pairs of input z-transform X (z) and system function H (z), deter­
mine the ROC for the output z-transform Y(z): 
: 
(a) 
1 
1 
X(z) = 
Izl> 2 
1+ 
H(z) = 
1 
1 -1' 
Izl> 4
1 
1- 4z 
(b) 
1 
X(z) = 
Izi < 2 
I­
I 
1 
H(z) 
Izl> "3
1- jel ' 
(c) 
1 
1
"5 < Izl < 3 
X(z) =(1- ~e1) (1 +3e1)' 
1
H(z) = 1 + 3z-
1 
Izi > :3 
1+ 
3.20. For each of the following pairs of input and output z-transforms X (z) and Y (z), determine 
the ROC for the system function H(z): 
(a) 
1 
3 
X(z) = 
Izl > 4
3 -I'
1 
4z 
1 
2 
Y(z) = 
Izi >"3 
1+ 
(b) 
1 
1  
X(z) = 1 + 
Izl < 3  
1 
1 
1 
Y(z) 
6 < Izl < 3
(1 - gel) (1 + ~e1)' 
Basic Problems 
3.21. A causal LTI system has the following system function: 
H(z) 
4 + O.25z-1 - 0.5z-2 
(1 - O.25e1)(1 + O.5cl ) 
(a) What is the ROC for H(z)? 

143 
Chapter 3 
Problems 
(b)  Determine if the system is stable or not. 
(c)  Determine the difference equation that is satisfied by the input x[n] and the output 
YLn]. 
(d)  Use a partial fraction expansion to determine the impulse response h[n]. 
(e)  Find fez), the z-transform of the output, when the input is x[n] 
u[-n 
1]. Be sure 
to specify the ROC for Y(z). 
(f)  Find the output sequence yIn] when the input is x[n] = u[-n - 1]. 
3.22. A causal LTI system has system function 
1 - 4z-2 
H(z) 
1 +O.5c l · 
. The input to this system is 
x[n] 
urn] + 2 cos (In) 
00 < n < 00, 
Determine the output y[nl for large positive n; i.e., find an expression for y[n] that is 
asymptotically correct as n gets large. (Of course, one approach is to find an expression for 
y[n] that is valid for all n, but you should see an easier way.) 
3.23. Consider an LTI system with impulse response 
n::: 0,
h[n] = {an,
0, 
n < 0, 
and input 
[ ] _ {I, 0 
n:::: (N - 1), 
x n -
0, 
otherwise. 
(a)  Determine the output y[n] by explicitly evaluating the discrete convolution ofx [n] and 
h[n]. 
(b)  Determine the output y[n] by computing the inverse z-transform of the product of the 
z-transforms of x[nl and h[n]. 
3.24. Consider an LTI system that is stable and for which H(z), the z-transform of the impulse 
response, is given by 
3 
H(z) = 
1 
l' 
1+3C 
Suppose x[nl. the input to the system. is a unit step sequence. 
(a)  Determine the output yIn] by evaluating the discrete convolution of xLn] and h[n]. 
(b)  Determine the output yIn] by computing the inverse z-transform of Y(z). 
3.25. Skctch each of the following sequences and determine thcir z-transforms, including the 
ROC: 
(a)  L
00 
o[n - 4kl  
k=-oo  
(b)  ~ [ei 7<n + cos(In) +sin (I +2rrn)]u[n] 

144 
Chapter 3 
The l-Transform 
3.26. Consider a right-sided sequence x[n] with z-transform 
1 
Z2 
X(z) = (1 
ac1)(I-bz 1) 
(z 
a)(z - b) 
In Section 3.3, we considered the determination of x[n] by carrying out a partial fraction 
expansion, with X (z) considered as a ratio ofpolynomials in 
. Carry out a partial fraction 
expansion of X(z), considered as a ratio of polynomials in z, and determine x[n] from this 
expansion. 
3.27. Determine the unilateral z-transform, including the ROC, for each of the following se­
quences: 
(a) ,sEn] 
(b) ,s[n 
1] 
(c) ,s[n + 1] 
(d) Gf urn] 
(e) - (~f u[-n 
1] 
(f) (if u[-n] 
(g) {Of + (!r1u[n]  
(l)n-l 
(b)  '2 
urn - 1] 
3.28.  If X(z) denotes the unilateral z-transform ofx [n], detennine, in terms of X(z), the unilateral 
z-transform of the following: 
(a) x[n 
2] 
(b) x[n + 1] 
n 
(c)  L x[m] 
m=-oo 
3.29. For each of the following difference equations and associated input and initial conditions, 
determine the response y[n] for n 2: 0 by using the unilateral z-transfonn. 
(a) yIn] + 3y[n 
1] == x[n] 
x[n] Of urn] 
y[-l] = 1 
(b) yIn] 2y [n -1] == x[n]- ~x[1l -1] 
x[n] 
urn]  
y[-I] 
0  
(c) yEn] -
hEn 
1] 
x[n] - ix[n 
1] 
x[n] = (!f urn]  
y[-l] = 1  

145 
Chapter 3  
Problems 
Advanced Problems 
3.30. A causal LTI system has system function 
l-z-l 
l-z-l 
H(z) = 1 _ O.25e2 = (1 - O.5el )(1 + O.5el ) . 
(a)  Determine the output of the system when the input is x[n] = urn]. 
(b)  Determine the input x [n] so that the corresponding output of the above system is 
y[n] = 8[n] - 8[n - 1]. 
(c)  Determine the output y[n] when the input is x[n] = cOs(O.57Tn) for -00 < n < 00. You 
may leave your answer in any convenient form. 
3.31.  Determine the inverse z-transform ofeach of the following. In parts (a)-(c), use the methods 
specified. (In part (d), use any method you prefer.) 
(a)  Long division: 
1 - lz-l
3 
x[nl a right-sided sequence
X(z) =  
1 
l' 
1+3e 
(b)  Partial fraction: 
3 
x[nl stable
X(z)= 
1 
1 -1' 
z -
4" -
gZ 
(c)  Power series: 
1 
X(z) = In(1 - 4z), 
Izl < 4 
1 
Izl >  (3)-1/3
(d)  X(z) = 
1 -3'  
1- 3z  
3.32.  Using any method, determine the inverse z-transform for each of the following: 
1 
(a)  X(z) = 
2 
'  
(1 + ~el) (1 - 2e1)(1 - 3e1)  
(x[nl is a stable sequence) 
-1 
(b)  X(z)=ez  
3 
2  
(c)  X() = ~ (x [n1is a left-sided sequence)
z  
z - 2 ' 
3.33.  Determine the inverse z-transform of each of the following. You should find the z-transform 
properties in Section 3.4 helpful. 
3z-3 
(a)  X(z) = 
2' 
x[nlleft sided 
(I-leI) 
(b)  x (z) = sin(z), 
ROC includes Izl = 1 
z7 - 2 
(c)  X(z) = l-e7 ' 
Izl > 1 
3.34.  Determine a sequence x[nl whose z-transform is X (z) = eZ + e1/ z, z f= O. 

146 
Chapter 3 
The z-Transform 
3.35.  Determine the inverse z-transform of 
1 
X(z) = log (1 
2z), 
Izl < 
by 
(a) using the power series 
00 
m 
log(1 
x) = - L ~, 
Ixl < 1; 
m 
m=l 
(b) first differentiating X (z) and then using the derivative to recover x [nJ. 
3.36.  For each of the following sequences, determine the z-transform and ROC, and sketch the 
pole-zero diagram: 
(a) x[nJ = anu[nJ + bnu[nJ + cnu[-n -1], 
lal < Ibl < Icl 
(b) x[n] = n2anu[nJ 
4 
4 
(c) x[n] 
en [cos(;2n)]u[nJ-en [cos(;2n)]u[n 
1] 
3.37.  The pole-zero diagram in Figure P3.37 corresponds to the z-transform X(z) of a causal 
sequence x[n]. Sketch the pole-zero diagram of Y(z), where y[n) = x[-n +3). Also, specify 
the ROC for Y(z). 
Tm 
z-plane 
Re 
Figure P3.37 
3.38.  Let x[n] be the sequence with the pole-zero plot shown in Figure P3.38. Sketch the pole­
zero plot for: 
(a) y[n] = (~r x[n] 
(b) w[n] = cos(7rn)x[n]
2
Tm 
z-plane 
Re 
Figure P3.38 

147 
Chapter 3 
Problems 
3.39.  Determine the unit step response of the causal system for which the z-transform of the 
impulse response is 
1 - z3 
H(z) = 1­
3.40.  If the input x[n] to an LTI system is x[n] 
urn], the output is 
(l)n-I
y[n] = 
2: 
urn + 1]. 
(a)  Find H(z), the z-transform of the system impulse response, and plot its pole-zero 
diagram. 
(b)  Find the impulse response h[n]. 
(e)  Is the system stable? 
(d)  Is the system causal? 
3.41.  Consider a sequence x[n] for which the z-transform is 
I 
X (z) 
3 
+ --'---:;­
1  
~z-I l­
and for which the ROC includes the unit circle. Determine x[O] using the initial-value 
theorem (see Problem 3.57). 
3.42.  In Figure P3.42, H(z) is the system function of a causal LTI system. 
(a)  Using z-transforms of the signals shown in the figure, obtain an expression for W(z) in 
the form 
W(z) 
HI (z)X(z) + H2(Z)E(z), 
where both HI (z) and H2 (z) are expressed in terms of H (z). 
(b)  For the special case H (z) = z-I/(1 
determine HI (z) and H2 (z). 
(c)  Is the system H (z) stable? Are the systems HI (z) and H2 (z) stable? 
eln] 
wIn] 
FIgure P3.42 
3.43.  In Figure P3.43, h[n] is the impulse response of the LTI system within the inner box. The 
input to system h[n] is v[n], and the output is wIn]. The z-transform of h[n], H(z), exists in 
the following ROC: 
0< rmin < Izi < rmax < 00. 
(a)  Can the LTI system with impulse response h[nj be bounded input, bounded output 
stable? If so, determine inequality constraints on rmin and rmax such that it is stable. 
If not, briefly explain why. 
(b)  Is the overall system (in the large box, with input x[nj and output YIn]) LTI? If so, find 
its impulse response gIn]. If not, briefly explain why. 

I 
I 
I 
---t 
x[nJ 
I 
I 
I 
148  
Chapter 3 
The l-Transform 
(c)  Can the overall system be BIBO stable? Ifso, determine inequality constraints relating 
a, rmin, and rmax such that it is stable. If not, briefly explain why. 
wIn] Y  
I 
y[n] 
I 
I 
I 
a~  
an 
I 
1  
________ J I 
Figure P3.43
1­
3.44. A causal and stable LTI system S has its input x[n] and output y[n] related by the linear 
constant-coefficient difference equation 
10 
y[n] + L akY[n - kJ = x[n] + px[n' 1]. 
k=l 
Let the impulse response of S be the sequence h[n]. 
(a)  Show that h[O] must be nonzero. 
(b)  Show that al can be determined from knowledge of p, h[O], and h[1]. 
(c)  If h[n] = (O.9)ncos(nn/4) for 0 ::: n ::: 10, sketch the pole-zero plot for the system 
function of S, and indicate the ROC. 
3.45. When the input to an LTI system is 
x[n] Gr urn] + Znu[-n -1], 
the output is 
y[n] 
6GrU[n]-6(~rU[n]. 
(a)  Find the system function H(z) of the system. Plot the poles and zeros of H(z), and 
indicate the ROC. 
(b)  Find the impulse response hrn] of the system. 
(c)  Write the difference equation that characterizes the system. 
(d)  Is the system stable? Is it causal? 
3.46. The following information is known about an LTI system: 
(i) The system is causal. 
(ii) When the input is 
1 (l)n
x[n]  
2: 
urn] - 34 (Z)n u[-n 
1], 
then the z-transform of the output is 
1- z-2 
Y(z) 
(1  
~Z-1 )(1 - ZZ-I)' 

149 
Chapter 3 
Problems 
(a)  Find the z-transform of x[n1. 
(b)  What are the possible choices for the ROC of Y(z)? 
(c)  What are the possible choices for a linear constant -coefficient difference equation used 
to describe the system? 
(d)  What are the possible choices for the impulse response of the system? 
3.47.  Let x [n 1be a discrete-time signal with x[n) 
0 for n S 0 and z-transform X (z). Furthermore, 
given x[n], let the discrete-time signal y[n] be defined by 
y[n1 = {~.X[1l1' 
n > 0, . 
O. 
otherwIse. 
(a)  Compute Y(z) in terms of X(z). 
(b)  Using the result of part (a), find the z-transform of 
1 
w[ll] 
n + 8[n1U[1l ~ 1}. 
3.48.  The signal yen] is the output of an LTI system with impulse response hen) for a given input 
x[n). Throughout the problem, assume that Y[Il] is stable and has a z-transform Y (z) with the 
pole-zero diagram shown in Figure P3.48-1. The signal x[n] is stable and has the pole-zero 
diagram shown in Figure P3.48-2. 
1.5 
0.5 
'"
N 
~ 
0 
-0.5 
-1 
. -1.5 . 
I 
-1 
o 
1 1 
4 2 
z-plane 
2 
2 
Re(z) 
Figure P3.48-1 
0.5 
'"
N 
~ 
0 
-0.5 
-1 
1 
4 
z-plane 
1 
2 
-1 
0 
Re(z) 
Figure P3.48-2 

150  
Chapter 3 
The z-Transform 
(a)  What is the ROC, Y(z)? 
(b)  Is y[n] left sided, right sided, or two sided? 
(c)  What is the ROC of X(z)? 
(d)  Is x[n] a causal sequence? That is, does x[n] = 0 for n < O? 
(e)  What is x[O]? 
(f)  Draw the pole-zero plot of H(z), and specify its ROC. 
(g)  Is h[n] anticausal? That is, does h[n] = 0 for n > O? 
3.49.  Consider the difference equation of Eq. (3.66). 
(a)  Show that with nonzero initial conditions the unilateral z-transform of the output of 
the difference equation is 
.IV  
(k 
) 
M
~ ak 
~ y[m 
k - l]z-m+l 
L bkZ-k 
y(z) = _ k-l 
m-l. 
.. + k=O 
X(z).
N 
N 
LakZ-k 
LakZ-k 
k~  
k~ 
(b)  Use the result of (a) to show that the output has the form 
y[n] = YZIR[n] + YZICR[n] 
where YZIR [n] is the output when the input is zero for all nand YZICR[n] is the output 
when the initial conditions are all zero. 
(c)  Show that when the initial conditions are all zero, the result reduces to the result that 
is obtained with the bilateral z-transform. 
Extension Problems 
3.50.  Let x[nJ denote a causal sequence; i.e., x[n] == 0, n < O. Furthermore, assume that x[O] f. 0 
and that the z-transform is a rational function. 
(a)  Show that there are no poles or zeros of X (2) at z = 00, i.e., that lim X (z) is nonzero 
. 
z->oo 
and finite. 
(b)  Show that the number of poles in the finite z-plane equals the number of zeros in the 
finite z-plane. (The finite z-plane excludes z =:; 00.) 
3.51.  Consider a sequence with z-transform X(z) 
P(z)/Q(z), where P('l) and Q('l) are poly­
nomials in z. If the sequence is absolutely summable and if all the roots of Q(z) are inside 
the unit circle, is the sequence necessarily causal? If your answer is yes, clearly explain. If 
your answer is no, give a counterexample. 
3.52.  Let x[n] be a causal stable sequence with z-transform X(z). The complex cepstrum x[n] is 
defined as the inverse transform of the logarithm of X (z); i.e., 
-
Z
X(z) =:; 10gX(z) ~ x[n], 
where the ROC of X(z) includes the unit circle. (Strictly speaking, taking the logarithm of 
a complex number requires some careful considerations. Furthermore, the logarithm of a 
valid 'l-transform may not be a valid z-transform. For now, we assume that this operation 
is valid.) 
Determine the complex cepstrum for the sequence 
x[n] = o[n] + ao[n - N], 
where lal < L 

151 
Chapter 3 
Problems 
3.53. Assume that x[n] is real and even; i.e., x[n] = x[-n]. Further, assume that Zo is a zero of 
X (z); i.e., X (zo) 
O. 
(a)  Show that 11zo is also a zero of X(z). 
(b)  Are there other zeros of X(z) implied by the information given? 
3.54. Using the definition of the z-transform in Eq. (3.2), show that if X (z) is the z-transform of 
x[n] 
xR[n] + jx/[n], then 
Z
(a)  x*[n] ~ X*(z*) 
(b)  x[-n] 
X(llz) 
(c)  xR(n] L 
~[X(z) + X*(z*)] 
1 
(d)  x/In] 
2j[X(z)-X*(z*)]. 
3.55. Consider a real sequence x[n] that has all the poles and zeros of its z-transform inside the 
unit circle. Determine, in terms of x[n], a real sequence xl[n] not equal to x[n], but for which 
xl [0) 
x[O], IXl[n)1 
Ix[n]l, and the z-transform of xl[n) has all its poles and zeros inside 
the unit circle. 
3.56. A real finite-duration sequence whose z-transform has no zeros at conjugate reciprocal pair 
locations and no zeros on the unit circle is uniquely speeified to within a positive seale factor 
by its Fourier transform phase (Hayes et aI., 1980). 
An example of zeros at conjugate reciprocal pair locations is z 
a and (a*)-l. Even 
though we can generate sequences that do not satisfy the preeeding set of conditions, almost 
any sequence of practical interest satisfies the conditions and therefore is uniquely specified 
to within a positive scale factor by the phase of its Fourier transform. 
Consider a sequence x[nl that is real, that is zero outside 0 :s n :s N - 1, and whose 
z-transform has no zeros at conjugate reciprocal pair locations and no zeros on the unit 
circle. We wish to develop an algorithm that reconstructs cx[n] from LX(ejW), the Fourier 
transform phase of x[n], where c is a positive scale factor. 
(a)  Specify a setof (N -1) linear equations, the solution to which will provide the recovery 
ofx[n] to within a positive or negative scale factor from tan{LX (ejW )}. You do not have 
to prove that the set of (N 
1) linear equations has a unique solution. Further, show 
that if we know Lx (eJW ) rather than just tan{LX (ej{j»}, the sign of the scale factor can 
also be determined. 
(b)  Suppose 
I 
~: ::~: 
x[n]  
2, 
n=1, 
3, 
n =2, 
0, 
n:::: 3. 
Using the approach developed in part (a), demonstrate that cx[n] can be determined 
from LX (eJW ), where c is a positive scale factor. 
3.57. For a sequence xlII] that is zero for n < 0, use Eq. (3.2) to show that 
lim  X(z) = x[O]. 
z->-oo 
This result is called the initial value theorem. What is the corresponding theorem if the 
sequence is zero for n > O? 

152  
Chapter 3 
The l-Transform 
3.58.  The aperiodic autocorrelation function for a real-valued stable sequence x['l] is defined as 
00 
cxxln] = I: x[k]x[n + k]. 
k=-oo 
(a)  Show that the z-transform of Cxx [n] is 
Cxx(z) = X(z)X(z-l). 
Determine the ROC for Cxx (z). 
(b)  Suppose that x[nl 
anu[n]. Sketch the pole-zero plot for Cxx(z), including the ROC. 
Also, find cxx[n1by evaluating the inverse z-transform of Cxx (z). 
(c)  Specify another sequence, Xl In], that is not equal to x[n] in part (b), but that has the 
same autocorrelation function, cxx[nl, as x[n] in part (b). 
(d)  Specify a third sequence, x2[n], that is not equal to x[n] or Xl [n], butthat has the same 
autocorrelation function as x[n] in part (b). 
3.59.  Determine whether or not the function X (z) = z* can correspond to the z-transform of a 
sequence. Clearly explain your reasoning. 
3.60.  Let X (z) denote a ratio of polynomials in z; i.e., 
X(z) =  B(z). 
A(z) 
Show that if X (z) has a 15t-order pole at z 
ZO, then the residue of X (z) at z = Zo is equal 
to 
B(zO) 
A'(ZO)' 
where A'(zo) denotes the derivative of A(z) evaluated at z = Z00 

4.0 INTRODUCTION 
Discrete-time signals can arise in many ways, but they occur most commonly as repre­
sentations ofsampled continuous-time signals. While sampling will no doubt be familiar 
to many readers, we shall review many of the basic issues such as the phenomenon of 
aliasing and the important fact that continuous-time signal processing can be imple­
mented through a process of sampling, discrete-time processing, and reconstruction 
of a continuous-time signal. After a thorough discussion of these basic issues, we dis­
cuss multirate signal processing, ND conversion, and the use of oversampling in AID 
conversion. 
4.1 PERIODIC SAMPLING 
Discrete representations of signals can take many forms including basis expansions of 
various types, parametric models for signal modeling (Chapter 11), and nonuniform 
sampling (see for example Yen (1956), Yao and Thomas (1967) and Eldar and Oppen­
heim (2000)). Such representations are often based on prior knowledge of properties 
of the signal that can be exploited to obtain more efficient representations. However, 
even these alternative representations generally begin with a discrete-time representa­
tion of a continuous-time signal obtained through periodic sampling; i.e., a sequence of 
samples, x[n], is obtained from a continuous-time signal xc(t) according to the relation 
x[n] = xc{nT), 
-00 < n < 00. 
(4.1) 
153 

154 
Chapter 4 
Sampling of Continuous-Time Signals 
Figure 4.1 
Block diagram
xc<t) 
L..----.,..-----II X[nl =Xc (n T) 
representation of an ideal 
continuous-to-discrete-time (C/D) 
T 
converter. 
In Eq. (4.1), T is the sampling period, and its reciprocal, is = liT, is the sampling 
frequency, in samples per second. We also express the sampling frequency as Qs = 21rIT 
when we want to use frequencies in radians per second. Since sampling representations 
rely only on the assumption of a bandlimited Fourier transform, they are applicable to 
a wide class of signals that arise in many practical applications. 
We refer to a system that implements the operation of Eq. (4.1) as an ideal 
continuous-to-discrete-time (CID) converter, and we depict it in block diagram form 
as indicated in Figure 4.1. As an example of the relationship between xc(t) and x[nl, 
in Figure 2.2 we illustrated a continuous-time speech waveform and the corresponding 
sequence of samples. 
In a practical setting, the operation of sampling is implemented by an analog-to­
digital (AID) converter. Such systems can be viewed as approximations to the ideal C/D 
converter. In addition to sampling rate, which is sufficient to define the ideal C/D con­
verter, important considerations in the implementation or choice of an AID converter 
include quantization of the output samples, linearity of quantization steps, the need for 
sample-and-hold circuits, and limitations on the sampling rate. The effects of quantiza­
tion are discussed in Sections 4.8.2 and 4.8.3. Other practical issues of AID conversion 
are electronic circuit concerns that are outside the scope of this text. 
The sampling operation is generally not invertible; i.e., given the output xlnl, 
it is not possible in general to reconstruct xc(t), the input to the sampler, since many 
continuous-time signals can produce the same output sequence ofsamples. The inherent 
ambiguity in sampling is a fundamental issue in signal processing. However, it is possible 
to remove the ambiguity by restricting the frequency content of input signals that go 
into the sampler. 
It is convenient to represent the sampling process mathematically in the two stages 
depicted in Figure 4.2( a). The stages consist of an impulse train modulator, followed by 
conversion of the impulse train to a sequence. The periodic impulse train is 
s(t) = L
00 
o(t - nT), 
(4.2) 
n=-oo 
where o(t) is the unit impulse function, or Dirac delta function. The product of set) and 
xc(t) is therefore 
xs(t) = Xc (t)s(t) 
DO 
00 
= xc(t) L oCt - nT) = L xc(t)o(t - nT). 
(4.3) 
n=-oo 
n=-oo 
Using the property of the continuous-time impulse function, x(t)o(t) = x(O)o(t), some­
times called the "sifting property" of the impulse function, (see e.g., Oppenheim and 

________ _ _____ 
155 
Section 4.1 
Periodic Sampling 
Willsky, 1997), Xs (t) can be expressed as 
oc 
Xs (t) = L xc(nT )8(t - nT), 
(4.4) 
n=-OQ 
i.e., the size (area) of the impulse at sample time nT is equal to the value of the 
continuous-time signal at that time. In this sense, the impulse train modulation of 
Eq. (4.3) is a mathematical representation of sampling. 
Figure 4.2(b) shows a continuous-time signal xc(t) and the results of impulse train 
sampling for two different sampling rates. Note that the impulses xc(nT)o(t 
nT) are 
represented by arrows with length proportional to their area. Figure 4.2( c) depicts the 
corresponding output sequences. The essential difference between Xs (t) and x [n Jis that 
xs(t) is, in a sense, a continuous-time signal (specifically, an impulse train) that is zero, 
CID converter 
--, 
1_­
(a) 
-4T -2T 
0 
2T 
4T 
-2T 
-T 
0 
T 
2T 
(b) 
x[n] 
-4 -3 -2 -1 0 
4 
-4 -3 -2 -1 0 1 2 3 4 
n 
n 
(c) 
s (t) 
I 
Conversion from  
impulse train  
to discrete-time  
xcCt) I 
sequence 
I 
I 
J 
I 
: 
I 
1 
1 
1J 
Figure 4.2 Sampling with a periodic impulse train, followed by conversion to a 
discrete-time sequence. (a) Overall system. (b) Xs(~ for two sampling rates. (c) 
The output sequence for the two different sampling rates. 

156  
Chapter 4 
Sampling of Continuous-Time Signals 
except at integer multiples of T. The sequence x[n], on the other hand, is indexed 
on the integer variable n, which, in effect, introduces a time normalization; i.e., the 
sequence of numbers x[n] contains no explicit information about the sampling period 
T. Furthermore, the samples of xc(t) are represented by finite numbers in x[n] rather 
than as the areas of impulses, as withxs(t). 
It is important to emphasize that Figure 4.2(a) is strictly a mathematical repre­
sentation convenient for gaining insight into sampling in both the time domain and 
frequency domain. It is not a close representation of any physical circuits or systems 
designed to implement the sampling operation. Whether a piece of hardware can be 
construed to be an approximation to the block diagram of Figure 4.2(a) is a secondary 
issue at this point. We have introduced this representation of the sampling operation 
because it leads to a simple derivation of a key result and because the approach leads to 
a number ofimportant insights that are difficult to obtain from a more formal derivation 
based on manipulation of Fourier transform formulas. 
4.2  FREQUENCY-DOMAIN REPRESENTATION OF 
SAMPLING 
To derive the frequency-domain relation between the input and output of an ideal C/D 
converter, consider the Fourier transform of Xs (f). Since, from Eq. (4.3), Xs (t) is the 
product of xc(t) and set), the Fourier transform of xs(t) is the convolution of the Fourier 
transforms X cUQ) and SUQ) scaled by ~. The Fourier transform of the periodic 
impulse train set) is the periodic impulse train 
2:rr  
00 
S(jQ) = T L 8(0. 
k Qs),  
(4.5) 
k=-oo 
where Qs = 2:rrI T is the sampling frequency in radians/s (see Oppenheim and Willsky, 
1997 or McClellan, Schafer and Yoder, 2003). Since 
1 
X s(jQ) = -Xc(jQ) * S(jQ),
2:rr 
where *denotes the operation of continuous-variable convolution, it follows that 
1 
00 
X s(jQ) 
T L X cU(Q 
k Qs»·  
(4.6) 
k=-oo 
Equation (4.6) is the desired relationship between the Fourier transforms of the 
input and the output of the impulse train modulator in Figure 4.2(a). Equation (4.6) 
states that the Fourier transform of xs(t) consists of periodically repeated copies of 
X cUn), the Fourier transform of xc(t). These copies are shifted by integer multiples 
of the sampling frequency, and then superimposed to produce the periodic Fourier 
transform of the impulse train of samples. Figure 4.3 depicts the frequency-domain 
representation of impulse train sampling. Figure 4.3( a) represents a bandlimited Fourier 
transform having the property that X c(jQ) 
0 for 10.1 > QN. Figure 4.3(b) shows the 
periodic impulse train S(jQ), and Fifure 4.3(c) shows X s UQ), the result of convolving 
X c(jr?) with SUQ) and scaling by 2rr' It is evident that when 
Qs - QN 2:: QN, 
or 
Qs 2:: 2QN, 
(4.7) 

157 
Section 4.2 
Frequency-Domain Representation of Sampling 
fl 
(a) 
SUfl) 
27T 
T 
o 
(b) 
Xs(jfl) 
(c) 
fl 
Figure 4.3 Frequency-domain representation of sampling in the time domain. 
(a) Spectrum of the original signal. (b) Fourier transform of the sampling function. 
(c) Fourier transform of the sampled signal with Qs > 2QN' (d) Fourier transform 
of the sampled signal with Qs < 2QN' 
fl 
as in Figure 4.3(c), the replicas of X c(jrJ) do not overlap, and therefore, when they are 
added together in Eq. (4.6), there remains (to within a scale factor of lIT) a replica 
of Xc(jrJ) at each integer multiple of rJs. Consequently, xc(t) can be recovered from 
xs(t) with an ideallowpass filter. This is depicted in Figure 4.4(a), which shows the 
impulse train modulator followed by an LTI system with frequency response Hr(jrJ). 
For X djrJ) as in Figure 4.4(b), X sUrJ) would be as shown in Figure 4.4(c), where it is 
assumed that Q s > 2rJN. Since 
Xr(jQ) = Hr(jrJ)Xs(jQ), 
(4.8) 

158 
Chapter 4 
Sampling of Continuous-Time Signals 
5(t - nT) 
,.
) 
,. I HrUfl) t-I--
XXr-l(t)
xs(t)
xc(t) 
£ 
(a) 
-flN 
flN 
fl 
(b) 
Xs(jfl) 
fls> 2flN 
.. 
fl 
(c) 
" 
HrUfl) 
T I fiN < fi,< (fi,-fiN) 
I 
-flc 
flc 
fl 
(d) 
£ 
Figure 4.4 
Exact recovery of a 
-flN 
flN 
fl 
continuous-time signal from its samples 
(e) 
using an ideallowpass filter. 
it follows that if H r (j Q) is an ideallowpass filter with gain T and cutoff frequency Q c 
such that 
QN S Q c S (Qs -
QN), 
(4.9) 
then 
Xr(jQ) = Xc(jQ), 
(4.10) 
as depicted in Figure 4.4(e) and therefore xr(t) = xc(t). 
If the inequality of Eq. (4.7) does not hold, i.e., if Qs < 2QN, the copies of X c(jQ) 
overlap, so that when they are added together, X c(jQ) is no longer recoverable by 

Section 4.2 
Frequency-Domain Representation of Sampling 
159 
lowpass filtering. This is illustrated in Figure 4.3( d). In this case, the reconstructed output 
xr(t) in Figure 4.4( a) is related to the original continuous-time input through a distortion 
referred to as aliasing distortion, or, more simply, aliasing. Figure 4.5 illustrates aliasing 
in the frequency domain for the simple case of a cosine signal of the form 
xc(t) 
cos Qot, 
(4.11a) 
whose Fourier transform is 
XdjQ) 
n8(Q - Qo) + n8(Q + Qo) 
(4.11b) 
as depicted in Figure 4.5(a). Note that the impulse at -Qo is dashed. It will be helpful 
to observe its effect in subsequent plots. Figure 4.5(b) shows the Fourier transform 
of xs(t) with Qo < Q,d2, and Figure 4.5(c) shows the Fourier transform of xs(t) with 
< Qo < Q s• Figures 4.5(d) and (e) correspond to the Fourier transform of the 
fis2 <fio<fi, 
i 
_.'--'-'--"---'l......L.J.--'---'-._ 
(c) 
No aliasing 
fi <!!.. 
o T 
(d) 
XrUfi)
Aliasing 
-(fis fio) (fis - fio) 
Figure 4.5 
The effect of aliasing in the 
(c) 
sampling of acosine signal. 

160 
Chapter 4 
Sampling of Continuous-Time Signals 
lowpass filter output for Qo < Qs/2 = n/T and Qs/2 < Qo < Qs, respectively, with 
Qc = Qs/2. Figures 4.5( c) and (e) correspond to the case of aliasing. With no aliasing 
[Figures 4.5(b) and (d)], the reconstructed output is 
Xr(t) = cos Qat. 
(4.12) 
With aliasing, the reconstructed output is 
xr(t) = cos(Qs 
Qo)t; 
(4.13) 
i.e., the higher frequency signal cos Qat has taken on the identity (alias) of the lower 
frequency signal cos(Qs - Qo)t as a consequence of the sampling and reconstruction. 
This discussion is the basis for the Nyquist sampling theorem (Nyquist 1928; Shannon, 
1949), stated as follows. 
Nyquist-Shannon Sampling Theorem: Let xc(t) be a bandlimited signal with 
x c(jn) 
0 
for Inl:::: nN. 
(4.14a) 
Thenxc(t) is uniquely determined by its samples x[n] ::= xc(nT), n 
0, ±1, ±2, ...• if 
ns = T
21f 
:::: 2nN· 
(4.14b) 
The frequency QN is commonly referred to as the Nyquist frequency, and the frequency 
2QN as the Nyquist rate. 
Thus far, we have considered only the impulse train modulator in Figure 4.2(a). 
Our eventual objective is to express X (e jW ), the discrete-time Fourier transform (DTFT) 
of the sequence x[n], in terms of X s(jQ) and X c(jQ). Toward this end, let us consider 
an alternative expression for X sUQ). Applying the continuous-time Fourier transform 
to Eq. (4.4), we obtain 
XsUQ) = L
00 
xc(nT)e-jr?Tn. 
(4.15) 
n=-oo 
Since 
x[n] = xc(nT) 
(4.16) 
and 
X (e jW) 
L
00 
x[n]e- jwn 
(4.17) 
n=-oo 
' 
it follows that 
= X (e jQT
X sUn) = X (ejW)lw=r?T 
). 
(4.18) 
Consequently, from Eqs. (4.6) and (4.18), 
1 
X (e jQT ) 
T L
00 
XcU(Q 
kQs», 
(4.19) 
k=-oc 

161 
Section 4.2 
Frequency-Domain Representation of Sampling 
or equivalently, 
00
1 L Xc [(W
j 
-
2;k)]. 
(4.20) 
T k=-oo 
T 
From Eqs. (4.18)-(4.20), we see that X (eiw) is a frequency-scaled version of XsUQ) 
with the frequency scaling specified by w = QT. This scaling can alternatively be thought 
of as a normalization of the frequency axis so that the frequency Q 
Qs in XsUQ) 
is normalized to w = 2n for X (eJW ). The frequency scaling or normalization in the 
transformation from X sUQ) to X (eJW ) is directly a result of the time normalization in 
the transformation from xs(t) to x[n]. Specifically, as we see in Figure 4.2, xs(t) retains 
a spacing between samples equal to the sampling period T. In contrast, the "spacing" 
of sequence values x[n] is always unity; i.e., the time axis is normalized by a factor 
of T. Correspondingly, in the frequency domain the frequency axis is normalized by 
Is = liT. 
For a sinusoid of the form xc(t) = cos(Qot), the highest (and only) frequency is 
Qo. Since the signal is described by a simple equation, it is easy to compute the samples 
of the signal. The next two examples use sinusoidal signals to illustrate some important 
points about sampling. 
Example 4.1 
Sampling and Reconstruction of a Sinusoidal 
Signal 
If we sample the continuous-time signal xc(t) 
cos(4ooont) with sampling period 
T == 1/6000, we obtain x[n] 
xc(nT) == cos(4000nTn) = cos(won). where wo = 
4000n T = 2n/3. In this case, ns = 2n/ T 
12ooon, and the highest frequency of the 
signal is no == 4000n, so the conditions of the Nyquist sampling theorem are satisfied 
and there is no aliasing. The Fourier transform of xc(t) is 
xc(jn) == no(n 
4000n) + no(n + 4000n). 
Figure 4.6( a) shows 
Xs(jn) = T 
1 L
00 
x,[j(n kns )] 
(4.21) 
k=-oo 
for ns == 12000n. Note that X cCjQ) is a pair of impulses at n = ±4ooon, and we 
see shifted copies of this Fourier transform centered on ±ns , ±2Qs, etc. Plotting 
X (eiw) = Xs(jw/ T) as a function of the normalized frequency w 
QT results 
in Figure 4.6(b), where we have used the fact that scaling the independent variable of 
an impulse also scales its area, i.e., o(w/T) 
To(w) (Oppenheim and Winsky, 1997). 
Note that the original frequency Qo = 4000n corresponds to the normalized frequency 
wo 
4000n T = 2n/3, which satisfies the inequality wo < n, corresponding to the fact 
that no 
4000n < n / T == 6000n. Figure 4.6( a) also shows the frequency response of 
an ideal reconstruction filter H r(j~'i) for the given sampling rate of ns 
12000n. This 
figure shows that the reconstructed signal would have frequcncy Qo = 4000n. which 
is the frequency of the original signal xc(t). 

162 
Chapter 4 
Sampling of ContinlJolJs-Time Signals 
X,(jn) 
/lAjn) 
--------(
7r 
7T 
7T 
T 
+T 
T 
-160007T -120007T -80007T - 6000"-4000". 
0 
40007T 6OO0r. 8000". 
120007T 
160007T n 
(a) 
lx(,'.)ot~
r r r 
r r  
-2". 
47T 
-7T 
27T 
0 
7T 
47T 
27T 
87T 
W 
3 
3 
3 
(b) 
Figure 4.6 
(a) Continuous-time and (b) discrete-time FOlJriertransforms for sam­
pled cosine signal with frequency nO = 4000rr and sampling period T 
1/6000. 
Example 4.2 Aliasing in Sampling a Sinusoidal Signal 
Now suppose that the continuous-time signal is xC<t) = cos(16000rrt), but the sampling 
period is T = 1/6000, as it was in Example 4.1. This sampling period fails to satisfy 
the Nyquist criterion, since ns 
2rr/ T = 12000rr < 2no = 32000rr. Consequently, 
we expect to see aliasing. The Fourier transform X s un) for this case is identical to 
that of Figure 4.6(a). However, now the impulse located at n 
-4000rr is from 
Xc[j(n 
n s )] in Eq. (4.21) rather than from X C<jn,) and the impulse at n = 4000rr 
is from X c[j (n + ns)]. That is, the frequencies ±4000rr are alias frequencies. Plotting 
X (e jUJ ) = Xs(jw/T) as a function ofwyields the same graph asshowninFigure4.6(b), 
since we are normalizing by the same sampling period. The fundamental reason for 
this is that the sequence of samples is the same in both cases; i.e., 
cos(16000rrn/6000) 
cos(2rrn + 4000rrn/6000) 
cos(2rrn/3). 
(Recall that we can add any integer multiple of 2rr to the argument of the cosine 
without changing its value.) Thus, we have obtained the same sequence of samples, 
x[n] = cos(2rrn/3), by sampling two different continuous-time signals with the same 
sampling frequency. In one case, the sampling frequency satisfied the Nyquist criterion, 
and in the other case it did not. As before, Figure 4.6(a) shows the frequency response 
of an ideal reconstruction filter H rUn) for the given sampling rate of ns 
12000rr. 
It is clear from this figure that the signal that would be reconstructed would have the 
frequency no = 4000rr, which is the alias frequency of the original frequency 16000rr 
with respect to the sampling frequency ns 
12000rr. 
Examples 4.1 and 4.2 use sinusoidal signals to illustrate some of the ambiguities 
that are inherent in the sampling operation. Example 4.1 verifies that if the conditions 
of the sampling theorem hold, the original signal can be reconstructed from the samples. 
Example 4.2 illustrates that if the sampling frequency violates the sampling theorem, we 
cannot reconstruct the original signal using an ideallowpass reconstruction filter with 
cutoff frequency at one-half the sampling frequency. The signal that is reconstructed 

163 
Section 4.3
Is 
Reconstruction of aBandlimited Signal from Its Samples 
is one of the alias frequencies of the original signal with respect to the sampling rate 
used in sampling the original continuous-time signal. In both examples, the sequence of 
samples was x[n] = cos(21rnI3), but the original continuous-time signal was different. 
As suggested by these two examples, there are unlimited ways of obtaining this same 
set of samples by periodic sampling of a continuous-time sinusoid. All ambiguity is 
removed, however, if we choose Qs > 2Qo. 
4.3  RECONSTRUCTION OF A BANDLIMITED SIGNAL 
FROM ITS SAMPLES 
According to the sampling theorem, samples of a continuous-time band limited signal 
taken frequently enough are sufficient to represent the signal exactly, in the sense that 
the signal can be recovered from the samples and with knowledge of the sampling 
period. Impulse train modulation provides a convenient means for understanding the 
process of reconstructing the continuous-time band limited signal from its samples. 
In Section 4.2, we saw that if the conditions of the sampling theorem are met 
and if the modulated impulse train is filtered by an appropriate lowpass filter, then the 
Fourier transform of the filter output will be identical to the Fourier transform of the 
original continuous-time signal xc(t), and thus, the output of the filter will be xc(t). If 
rtg  
we are given a sequence of samples, x[nJ, we can form an impulse train xs(t) in which 
ify  
successive impulses are assigned an area equal to successive sequence values, i.e., 
ly, 
00 
to 
xAt) = L x[n]8(t 
nT).  
(4.22)
1m 
n=-oo 
)1f 
The nth sample is associated with the impulse at t = nT, where T is the sampling period 
ng 
associated with the sequence x[nJ. If this impulse train is the input to an ideallowpass
b), 
[or 
continuous-time filter with frequency response H r (j Q) and impulse response hr (t), then 
the output of the filter will be 
L 
00 
xln]hr(t - nT).  
(4.23) 
n=-oo 
A block diagram representation of this signal reconstruction process is shown in Fig­
ure 4.7(a). Recall that the ideal reconstruction filter has a gain of T [to compensate 
for the factor of liT in Eq. (4.19) or (4.20)] and a cutoff frequency Qc between QN 
and Qs -
QN. A convenient and commonly used choice of the cutoff frequency is 
Qc = Qsl2 = 1rIT. This choice is appropriate for any relationship between Qs and 
QN that avoids aliasing (i.e., so long as Qs ~ 2QN). Figure 4.7(b) shows the frequency 
response of the ideal reconstruction filter. The corresponding impulse response, hr(t), 
is the inverse Fourier transform of H r (j Q), and for cutoff frequency 1rI T it is given by 
hr(t) = sin(mIT) 
(4.24) 
1rtIT 
This impulse response is shown in Figure 4.7( c). Substituting Eq. (4.24) into Eq. (4.23) 
leads to 
~ . sin[1r(t-nT)IT] 
~ x[nJ 
.  
(4.25) 
n=-oo 
. 
;r(t 
nT )IT 

164 
Chapter 4 
Ideal reconstruction system 
r---------------------­
Ideal
Convert from 
reconstruction
sequence to 
filter
impulse train I xs(t) 
x,( t)
x[n] 
H,(jO) 
Sampling 
period T 
______ ...1
L.. ______________ _ 
(a)
f(t)  
7r 
7r 
o 
T 
T 
(b) 
h,( t) 
1 
(c) 
Sampling of Continuous-Time Signals 
Figure 4.7 
(a) Block diagram of an 
ideal bandlimited signal reconstruction 
system. (b) Frequency response of an 
ideal reconstruction filter. (c) Impulse 
response of an ideal reconstruction filter. 
Equations (4.23) and (4.25) express the continuous-time signal in terms of a linear 
combination of basis functions hr(t - nT) with the samples x[n] playing the role of 
coefficients. Other choices of the basis functions and corresponding coefficients could 
be used to represent other classes ofcontinuous-time functions [see, for example Unser 
(2000)]. However, the functions in Eq. (4.24) and the samples x[n] are the natural basis 
functions and coefficients for representing bandlimited continuous-time signals. 
From the frequency-domain argument of Section 4.2, we saw that ifx [n] = xc(nT), 
where Xc(j0.) = 0 for 10.1 2: niT, then xr(t) is equal to xc(t). It is not immediately 
obvious that this is true by considering Eq. (4.25) alone. However, useful insight is 
gained by looking at that equation more closely. First, let us consider the function hr(t) 
given by Eq. (4.24). We note that 
hr(O) = 1. 
(4.26a) 
'"

165 
Section 4.3 
Reconstruction of aBandlimited Signal from Its Samples 
This follows from I'Hopital's rule orthe small angle approximation for the sine function. 
In addition, 
for n = ±1, ±2, .... 
(4.26b) 
It follows from Eqs. (4.26a) and (4.26b) and Eq. (4.23) that ifx[n] = xc(nT), then 
(4.27)  
for all integer values of m. That is, the signal that is reconstructed by Eq. (4.25) has the 
same values at the sampling times as the original continuous-time signal, independently 
of the sampling period T. 
In Figure 4.8, we show a con tin uous-time signal Xc (t) and the corresponding mod­
ulated impulse train. Figure 4.8(c) shows several of the terms 
sin[;r(t - nT 
T] 
;r(t - nT)jT 
and the resulting reconstructed signal X, (t). As suggested by this figure, the ideallowpass 
filter interpolates between the impulses of xs(t) to construct a continuous-time signal 
xr(t). From Eq. (4.27), the resulting signal is an exact reconstruction of xc(t) at the 
sampling times. The fact that, if there is no aliasing, the lowpass filter interpolates the 
(a) 
/ 
/' 
/ 
I 
I 
I 
I 
I 
(b) 
(c) 
Figure 4.8 
Ideal bandlimlted 
interpolation. 

166 
Chapter 4 
Sampling of Continuous-Time Signals 
Ideal reconstruction system 
r------------ --- --­
I  
I  
I
I 
Ideal 
I
I I Convert from 
reconstruction 
I r­
filter 
I xr ( I)
Xs(t) 
Hrun) 
I 
I 
I 
T
I 
I 
I 
_J 
(a) 
(b) 
Figure 4.9 
(a) Ideal bandlimited signal reconstruction. (b) Equivalent represen­
tation as an ideal DIC converter. 
correct reconstruction between the samples follows from our frequency-domain analysis 
of the sampling and reconstruction process. 
It is useful to formalize the preceding discussion by defining an ideal system for 
reconstructing a bandlimited signal from a sequence ofsamples. We will call this system 
the ideal discrete-to-continuous-time (Die) converter. The desired system is depicted in 
Figure 4.9. As we have seen, the ideal reconstruction process can be represented as the 
conversion of the sequence to an impulse train, as in Eq. (4.22), followed by filtering with 
an ideallowpass filter, resulting in the output given by Eq. (4.25). The intermediate step 
of conversion to an impulse train is a mathematical convenience in deriving Eq. (4.25) 
and in understanding the signal reconstruction process. However, once we are familiar 
with this process, it is useful to define a more compact representation, as depicted in 
Figure 4.9(b), where the input is the sequence x[n] and the output is the continuous-time 
signal xr(t) given by Eq. (4.25). 
The properties of the ideal D/C converter are most easily seen in the frequency do­
main. To derive an input/output relation in this domain, consider the Fourier transform 
of Eq. (4.23) or Eq. (4.25), which is 
ex: 
X rUQ) = L x[n]Hr(jQ)e-jQTn. 
n=-OQ 
Since HrUQ) is common to all the terms in the sum, we can write 
X rUQ) 
Hr(jQ)X (elf.lT). 
(4.28) 
Equation (4.28) provides a frequency-domain description of the ideal D/C converter. 
According to Eq. (4.28), X (eJrV) is frequency scaled (in effect, going from the sequence 
to the impulse train causes w to be replaced by QT). Then the ideal lowpass filter 
Hr(jQ) selects the base period of the resulting periodic Fourier transform X (elm) 
and compensates for the liT scaling inherent in sampling. Thus, if the sequence x[n] 
has been obtained by sampling a bandlimited signal at the Nyquist rate or higher, the 
reconstructed signal Xr (t) will be equal to the original bandlimited signal. In any case, 
it is also clear from Eq. (4.28) that the output of the ideal Die converter is always 
bandlimited to at most the cutoff frequency of the lowpass filter, which is typically 
taken to be one-half the sampling frequency. 

167 
Section 4.4 
Discrete-Time Processing of Continuous-Time Signals 
4.4  DISCRETE-TIME PROCESSING OF CONTINUOUS-TIME 
SIGNALS 
A major application of discrete-time systems is in the processing of continuous-time 
signals. This is accomplished by a system of the general form depicted in Figure 4.10. 
The system is a cascade of a C/D converter, followed by a discrete-time system, followed 
by a D/C converter. Note that the overall system is equivalent to a continuous-time 
system, since it transforms the continuous-time input signal xc(t) into the continuous­
time output signal Yr(t). The properties of the overall system are dependent on the 
choice of the discrete-time system and the sampling rate. We assume in Figure 4.10 that 
the C/D and D/C converters have the same sampling rate. This is not essential, and later 
sections of this chapter and some of the problems at the end of the chapter consider 
systems in which the input and output sampling rates are not the same. 
The previous sections of the chapter have been devoted to understanding the 
C/O and O/C conversion operations in Figure 4.10. For convenience, and as a first step 
in understanding the overall system of Figure 4.10, we summarize the mathematical 
representations of these operations. 
The C/D converter produces a discrete-time signal 
x[n] 
xc(nT),  
(4.29) 
i.e., a sequence of samples of the continuous-time input signal xc(t). The DTFT of this 
sequence is related to the continuous-time Fourier transform of the continuous-time 
input signal by 
'w 
1 ~ [(tv 2nk)]
X(e l 
) = T L,; Xc j T - T 
' 
(4.30) 
k=-oo 
The D/C converter creates a continuous-time output signal of the form 
~ 
sin[n(t-nT)IT] 
(4.31)
Yr(t) = L,; y[n] 
n(t _ nT )IT 
' 
n=-oo 
where the sequence y[n) is the output of the discrete-time system when the input to the 
system is x[n). From Eq. (4.28), Yr(jn), the continuous-time Fourier transform of Yr(t), 
and Y(e jW ), the OTFT of y[n], are related by 
'rlT 
Y ('n) = H ('n)Y(e jrlT ) = 
TY(e} 
), Inl < ",-IT, 
(4.32)
r J 
r J 
{ D, 
otherWIse. 
Next, let us relate the output sequence y[n] to the input sequence x[n], or equiva­
lently, Y(e jW ) to X (e jW ). A simple example is the identity system, i.e., y[nJ = x[n). This 
--- ---------- --- -----------1 
xc(t) I 
I 
I 
1 
I 
: Yr(t) 
1 
I 
T  
T 
I
_____________ -1 
1_-___ --------­
Figure 4.10 
Discrete-time processing of continuous-time signals, 

168  
Chapter 4 
Sampling of Continuous-Time Signals 
is in effect the case that we have studied in detail so far. We know that if xc(t) has a band­
limited Fourier transform such that X c(j Q) 
0 for IQI :::: IT I T and if the discrete-time 
system in Figure 4.10 is the identity system such that y[n] 
x[n] = xc(nT), then the 
output will be Yr(t) = xc(t). Recall that, in proving this result, we utilized the frequency­
domain representations of the continuous-time and discrete-time signals, since the key 
concept of aliasing is most easily understood in the frequency domain. Likewise, when 
we deal with systems more complicated than the identity system, we generally carry 
out the analysis in the frequency domain. If the discrete-time system is nonlinear or 
time varying, it is usually difficult to obtain a general relationship between the Fourier 
transforms of the input and the output of the system. (In Problem 4.51, we consider an 
example of the system of Figure 4.10 in which the discrete-time system is nonlinear.) 
However, the LTI case leads to a rather simple and generally useful result. 
4.4. 1 Discrete-Time LT. Processing of Continuous-Time 
Signals 
If the discrete-time system in Figure 4.10 is linear and time invariant, we have 
Y(eiw) = H(eio»X (e JW ),  
(4.33) 
where H(eJW) is the frequency response of the system or, equivalently, the Fourier 
transform ofthe unit sample response, and X (eJW) and Y(eJW ) are the Fourier transforms 
of the input and output, respectively. Combining Eqs. (4.32) and (4.33), we obtain 
Yr(jQ) 
H r(jQ)H(eJQT)X (eJQT ).  
(4.34) 
Next, using Eq. (4.30) with w = QT, we have 
Yr(jQ) 
Hr(jQ)H(eJQT)~ f Xc [j (Q -2;k)J. 
(4.35) 
k=-OG 
If X c(jQ) 
0 for IQI :::: nIT, then the ideal lowpass reconstruction filter Hr(jQ) 
cancels the factor 11 T and selects only the term in Eq. (4.35) for k = 0; i.e., 
YCQ)={H(eJQT)Xc(jQ), 
IQI<rrIT, 
(436) 
r } 
0, 
IQI :::: nIT. 
. 
Thus, if X c(jQ) is bandlimited and the sampling rate is at or above the Nyquist rate, 
the output is related to the input through an equation of the form 
Yr(jQ) 
Heff(jQ)X c(jQ),  
(4.37) 
where 
Heff(jQ) = {~(ejQT),  10.1 <rrIT, 
(4.38)
10.1:::: rrlT. 
That is, the overall continuous-time system is equivalent to an LTI system whose effective 
frequency response is given by Eq. (4.38). 
It is important to emphasize that the linear and time-invariant behavior of the sys­
tem of Figure 4.10 depends on two factors. First, the discrete-time system must be linear 
and time invariant. Second, the input signal must be bandlimited, and the sampling rate 
must be high enough so that any aliased components are removed by the discrete-time 
~ 

169 
0 
Section 4.4 
Discrete-Time Processing of Continuous-Time Signals 
system. As a simple illustration ofthis second condition being violated, consider the case 
when Xc (t) is a single finite-duration unit-amplitude pulse whose duration is less than the 
sampling period. Ifthe pulse is unity at! 
0, thenx[n] = 8[n]. However,itisclearlypos­
sible to shift the pulse so that itis not aligned with any of the sampling times, i.e.,x[n] 
for all n. Such a pulse, being limited in time, is not bandlimited, and the conditions of the 
sampling theorem cannot hold. Even if the discrete-time system is the identity system, 
such that y[n] = x[n], the overall system will not be time invariant if aliasing occurs in 
sampling the input. In general, if the discrete-time system in Figure 4.10 is linear and 
time invariant, and if the sampling frequency is at or above the Nyquist rate associated 
with the bandwidth of the input Xc (t), then the overall system will be equivalent to an LTI 
continuous-time system with an effective frequency response given by Eq. (4.38). Fur­
thermore, Eq. (4.38) is valid even if some aliasing occurs in the aD converter, as long as 
H (eiw) does not pass the aliased components. Example 4.3 is a simple illustration of this. 
Example 4.3 
Ideal Continuous-Time Lowpass Filtering 
Using a Discrete-Time Lowpass Filter 
Consider Figure 4.10, with the LTI discrete-time system having frequency response 
H(e jW ) {1, 
Iwi < We, 
(4.39)
0, 
We < Iwl ::::: iT. 
This frequency response is periodic with period 2iT, as shown in Figure 4.11(a). 
For bandlimited inputs sampled at or above the Nyquist rate, it follows from Eq. (4.38) 
that the overall system of Figure 4.10 will behave as an LTI continuous-time system 
with frequency response 
1. 
1$11'1 < We or 1$11 < well',
HeffU$1) = 
(4.40)
{ 0, 
IQTI:::: We or 1$11:::: welT. 
As shown in Figure 4.11(b), this effective frequency response is that of an ideal 
lowpass filter with cutoff frequency $1c 
wei l' 
The graphical illustration given in Figure 4.12 provides an interpretation of how 
this effective response is achieved. Figure 4.12(a) represents the Fourier transform 
11 H(,J") 
I 
I 
-211" 
211" 
OJ 
(a) 
T 
T
(b) 
Figure 4.11 
(a) Frequency response of discrete-time system in Figure 4.10. 
(b) Corresponding effective continuous-time frequency response for bandlimited 
inputs. 

Chapter 4 
Sampling of Continuous-Time Signals
170 
K  
,n
-0" 
" 
"' 
(a) 
"N 
~ 
27T 
7T 
ON 7T 
27T 
o 
T 
T 
T 
T 
(b) 
W 
X(eiw) 
',n"n 
(e) 
?K~ 
(l 
(l  
-27T 
-We 
we 
27T 
W 
(d) 
27T 
7T 
We 
We 
7T 
27T 
0 
T 
T 
T 
T 
T 
T 
(e)d;ifil 
We 
we 
o 
T (f) T 
Figure 4.12 
(a) Fouriertransform of abandlimited input signal. (b) Fouriertrans­
form of sampled input plotted as a function of continuous-time frequency n. 
(c) Fourier transform X (eju» of sequence of samples and frequency response 
H(efrv ) of discrete-time system plotted versus w. (d) Fourier transform of output 
of discrete-time system. (e) Fourier transform of output of discrete-time system 
and frequency response of ideal reconstruction filter plotted versus n. (f) Fourier 
transform of output. 
.... 
... 

171 
Section 4.4 
Discrete-Time Processing of Continuous-Time Signals 
of a bandlimited signal. Figure 4.12(b) shows the Fourier transform of the intermediate 
modulated impulse train, which is identical to X (ejD.T), the DTFT of the sequence of 
samples evaluated forw = QT. In Figure 4.12(c), the DTFT of the sequence ofsamples 
and the frequency response of the discrete-time system are both plotted as a function 
of the normalized discrete-time frequency variable w. Figure 4.12( d) shows Y (e jUJ ) = 
H (ejUJ)X (e jUJ), the Fourier transform of the output of the discrete-time system. Figure 
4.12(e) illustrates the Fourier transform of the output of the discrete-time system as 
a function of the continuous-time frequency Q, together with the frequency response 
of the ideal reconstruction filter H r(jQ) of the D/C converter. Finally, Figure 4.12(f) 
shows the resulting Fourier transform of the output ofthe D/Cconverter. By comparing 
Figures 4.12(a) and 4.12(f), we see that the system behaves as an LTI system with 
frequency response given by Eq. (4.40) and plotted in Figure 4.11(b). 
Several important points are illustrated in Example 4.3. First, note that the ideal 
lowpass discrete-time filter with discrete-time cutoff frequency We has the effect of an 
ideallowpass filter with cutoff frequency Qe = wei T when used in the configuration 
of Figure 4.10. This cutoff frequency depends on both We and T. In particular, by using 
a fixed discrete-time lowpass filter, but varying the sampling period T, an equivalent 
continuous-time lowpass filter with a variable cutoff frequency can be implemented. 
For example, if T were chosen so that QNT < We, then the output of the system of 
Figure 4.10 would be Yr(t) = xe(t). Also, as illustrated in Problem 4.31, Eq. (4.40) will 
be valid even if some aliasing is present in Figures 4.12(b) and (c), as long as these 
distorted (aliased) components are eliminated by the filter H(e jUJ ). In particular, from 
Figure 4.12(c), we see that for no aliasing to be present in the output, we require that 
(2rr -
r2NT) :::: We, 
(4.41) 
compared with the Nyquist requirement that 
(2rr - QNT ) :::: r2NT. 
(4.42) 
As another example of continuous-time processing using a discrete-time system, let us 
consider the implementation of an ideal differentiator for bandlimited signals. 
Example 4.4 
Discrete-Time Implementation of an Ideal 
Continuous-Time Bandlimited Differentiator 
The ideal continuous-time differentiator system is defined by 
d 
Ye(t) = -[xe(t)], 
( 4.43) 
dt 
with corresponding frequency response 
He(jQ) = jQ. 
( 4.44) 
Since we are considering a realization in the form of Figure 4.10, the inputs are re­
stricted to be bandlimited. For processing bandlimited signals, it is sufficient that 
H 
('Q) = {jQ, 
IQI < rr/T, 
(4.45)
eft } 
0, 
IQI :::: rr/T, 
as depicted in Figure 4.13(a). The corresponding discrete-time system has frequency 
response 
Iwl < rr, 
( 4.46) 

172 
Chapter 4 
Sampling of Continuous-Time Signals 
and is periodic with period 2n. This frequency response is plotted in Figure 4.13(b). 
The corresponding impulse response can be shown to be 
1 J1C (jW) J'wn 
nncosnn - sinnn
h[n] 
-
-
e 
dw= 
~ 
...-
-00 < n < 00, 
2n -1C 
T 
or equivalently, 
0, 
n =0, 
h[n] 
cosnn 
(4.47) 
n :f O.
nT '
1 
Thus, if a discrete-time system with this impulse response was used in the con­
figuration of Figure 4.10, the output for every appropriately bandlimited input would 
be the derivative of the input. Problem 4.22 concerns the verification of this for a 
sinusoidal input signal. 
Illefl(j!1)1 
7T 
7T 
'iT 
!1 
T 
T 
7T 
T 
-tlleff U!1) 
:1!.1-1__..., 
2 
rr 
7T 
2 
T 
!1 
(a) 
~ 
-27T 
-7T 
7T 
2rr 
w 
-tH(e!"') 
7T 
2 
.....­
w 
7T
-
2 
(b) 
Figure 4.13 
(a) Frequency response of a continuous-time ideal bandlimited dif­
ferentiator Hc(jQ) 
jQ, Inl < niT. (b) Frequency response of a discrete-time 
filter to implement a continuous-time bandlimited differentiator. 

173 
Section 4.4 
Discrete-Time Processing of Continuous-Time Signals 
4.4.2 Impulse Invariance 
We have shown that the cascade system of Figure 4.10 can be equivalent to an LTI 
system for bandlimited input signals. Let us now assume that, as depicted in Figure 4.14, 
we are given a desired continuous-time system that we wish to implement in the form 
of Figure 4.10. With HcUO) bandlimited, Eq. (4.38) specifies how to choose H(ej(j)) so 
that He[fUO) 
Hc(jO). Specifically, 
Iwl < n, 
(4.48) 
with the further requirement that T be chosen such that 
101 0": njT. 
(4.49) 
Under the constraints of Eqs. (4.48) and (4.49), there is also a straightforward and useful 
relationship between the continuous-time impulse response hc(t) and the discrete-time 
impulse response h[nJ. In particular, as we shall verify shortly, 
h[n] = Thc(nT); 
(4.50) 
i.e., the impulse response of the discrete-time system is a scaled, sampled version of 
hc(t). When h[n] and hc(t) are related through Eq. (4.50), the discrete-time system is 
said to be an impulse-invariant version of the continuous-time system. 
Equation (4.50) is a direct consequence ofthe discussion in Section 4.2. Specifically, 
with x[n] and xc(t) respectively replaced by h[n] and hc(t) in Eq. (4.16), i.e., 
h[n] = hc(nT), 
(4.51) 
Eq. (4.20) becomes 
(4.52) 
Continuous-time 
LTI system 
xc(t) 
heel), HcUD.) 
yc(t) 
(a) 
r---------
--------1 
1  
1  
1  
T 
Discrete-time 
Lfl system 
h[n],H(eiw) 
1 
1 
1 
Figure 4.14 
(a) Continuous-time LTI system. (b) Equivalent system for bandlim­
ited inputs. 

174 
Chapter 4 
Sampling of Continuous-Time Signals 
or, if Eq. (4.49) is satisfied, 
. 
1 
(W)
H(eJ(j)) = THe jT ' 
Iwl < Jr. 
(4.53) 
Modifying Eqs. (4.51) and (4.53) to account for the scale factor of T in Eq. (4.50), we 
have 
h[n] = Thc{nT), 
(4.54) 
H(e jW ) 
Hc (/¥). 
Iwi < Jr. 
(4.55) 
Example 4.5 
A Discrete-Time Lowpass Filter Obtained 
by Impulse Invariance 
Suppose that we wish to obtain an ideallowpass discrete-time filter with cutoff fre­
quency Wc < rr. We can do this by sampling a continuous-time ideallowpass filter with 
cutoff frequency Qc = welT < rrlT defined by 
1, 
IQI < Qc,
HcCjQ) = { 0, 
IQI ~ Qc· 
The impulse response of this continuous-time system is 
sin(Qet)
hc(t) 
rrt  
so we define the impulse response of the discrete-time system to be  
· ] 
h 
Tsin(QcnT) 
sin(wen)
h[n=Tc(nT) 
=. 
rrn T 
rrn 
where We = QeT. We have already shown that this sequence corresponds to the DTFT 
H(ejW ) = { 1,. 
Iwi < We. 
0, 
we::: Iwl ::: rr, 
which is identical to H e(jwl T), as predicted by Eq. (4.55). 
Example 4.6 
Impulse Invariance Applied to 
Continuous-Time Systems with Rational System Functions 
Many continuous-time systems have impulse responses composed of a sum of expo­
nential sequences of the form 
he(t) 
AeSQiu(t). 
Such time functions have Laplace transforms 
A 
H c(s) = 
Re(s) > Re(so). 
s - So 
Ifwe apply the impulse invariance concept to such a continuous-time system, we obtain 
the impulse response 
h[n] = T hcCnT) 
AT esoTnu[n], 
... 

175 
Section 4.5 
Continuous-Time Processing of Discrete-Time Signals 
which has z-transform system function 
H(z) 
and, assuming Re(so) < 0, the frequency response 
. 
AT 
H(eJW) = 
..
1- esoTe- JW 
In this case, Eq. (4.55) does not hold exactly, because the original continuous-time 
system did not have a strictly bandlimited frequency response, and therefore, the re­
sulting discrete-time frequency response is an aliased version of H ('UQ). Even though 
aliasing occurs in such a case as this, the effect may be small. Higher-order systems 
whose impulse responses are sums of complex exponentials may in fact have fre­
quency responses that fall off rapidly at high frequencies, so that aliasing is minimal if 
the sampling rate is high enough. Thus, one approach to the discrete-time simulation 
of continuous-time systems and also to the design of digital filters is through sampling 
of the impulse response of a corresponding analog filter. 
4.5  CONTINUOUS~nME PROCESSING OF DISCRETE-TIME 
SIGNALS 
In Section 4.4, we discussed and analyzed the use of discrete-time systems for processing 
continuous-time signals in the configuration of Figure 4.10. In this section, we consider 
the complementary situation depicted in Figure 4.15, which is appropriately referred 
to as continuous-time processing of discrete-time signals. Although the system of Fig­
ure 4.15 is not typically used to implement discrete-time systems, it provides a useful 
interpretation of certain discrete-time systems that have no simple interpretation in the 
discrete domain. 
From the definition of the ideal Die converter, X c(j0.) and therefore also Yc(j0.), 
will necessarily be zero for 10.1 ::: rriT. Thus, the aD converter samples yc(t) without 
aliasing, and we can express xc(t) and yc(t) respectively as 
ao 
sin[rr(t 
nT )IT] 
(4.56)
rr(t 
nT )IT
n=-(X) 
and 
~ 
sinr"(t - nT )IT] 
(4.57)
yc(t) = L 
yen] 
rr(t 
nT )IT 
n=-oo 
h[n],H(eiw) 
1----­
1
1 
T 
T 
1 
Figure 4.15 Continuous-time 
l ___ _ 
_I
1  
proceSSing of discrete-time signals. 
1 
1 
x[n] : 
1 
: y[n] 
I 

176 
Chapter 4 
Sampling 01 Continuous-Time Signals 
where x[n] 
xc(nT) and y[n] 
yc(nT). The frequency-domain relationships for 
Figure 4.15 are 
X c(jQ) = T X (eJQT ), 
IQI < niT, 
(4.58a) 
Yc(JQ) = H c(JQ)Xc(JQ), 
(4.58b) 
.JJ 
1 (.W)
Y(eJl
) = rYe 'r ' 
Iwl < n. 
(4.58c) 
Therefore, by substituting Eqs. (4.58a) and (4.58b) into Eq. (4.58c), it follows that the 
overall system behaves as a discrete-time system whose frequency response is 
jUJ
H(e
) = He (j;), 
Iwl < Jr, 
(4.59) 
or equivalently, the overall frequency response of the system in Figure 4.15 will be equal 
to a given H(e jUJ ) if the frequency response of the continuous-time system is 
Hc(jQ) = H(e jQT ), 
IQI<JrIT. 
(4.60) 
Since Xc(JQ) 
0 for IQI 2: niT, Hc(JQ) may be chosen arbitrarily above niT. A 
convenient-but arbitrary-choice is He (j Q) 
0 for IQ I > niT. 
With this representation of a discrete-time system, we can focus on the equivalent 
effect of the continuous-time system on the band limited continuous-time signal xc(t). 
This is illustrated in Examples 4.7 and 4,8. 
Example 4.7 
Noninteger Delay 
Let us consider a discrete-time system with frequency response 
jW
H(e
) 
Iwl < JL 
( 4,61) 
When !'J. is an integer, this system has a straightforward interpretation as a delay of !'J., 
i.e., 
y[n] 
x[n -
!'J.]. 
(4.62) 
When !'J. is not an integer, Eq. (4.62) has no formal meaning, because we cannot shift 
the sequence x[n] by a noninteger amount. However, with the use of the system of 
Figure 4.15, a useful time-domain interpretation can be applied to the system specified 
by Eq. (4.61). Let HcUQ.) in Figure 4.15 be chosen to be 
Hc(jQ.) 
H(ejQT) 
e-]QTIJ. 
(4.63) 
Then, from Eq. (4.59), the overall discrete-time system in Figure 4.15 will have the 
frequency response given by Eq. (4.6 t), whether or not !'J. is an integer. To interpret 
the system of Eq. (4.61), we note that Eq. (4.63) represents a time delay of T!'J. seconds. 
Therefore, 
yc(t) 
xc(t 
T!'J.). 
(4.64) 
Furthermore, xc(t) is the bandlimited interpolation of x[n], and y[n] is obtained by 
sampling ycCt). For example, if !'J. 
1, y[n] would be the values of the bandlim­
ited interpolation halfway between the input sequence values. This is illustrated in 

177 
Section 4.5 
Continuous-Time Processing of Discrete-Time Signals 
Figure 4.16. We can also obtain a direct convolution representation for the system 
defined by Eq. (4.61). From Eqs. (4.64) and (4.56), we obtain 
yIn] = ydnT) = xc(nT - T 6.) 
f: x[k] sin[n(t - T 6. - kT 
T] I 
(4.65) 
k=-oo 
n(t - T6. -kT)jT 
t=nT 
00 
. sinn(n 
k - 6.)
L x[k] n(n - k - 6.) , 
k=-oo 
which is, by definition, the convolution of x[n] with 
sin n(n - 6.) 
-00 < n < 00.
hln] = 
n(n 
6.) . 
When 6. is not an integer, hEnJhas infinite extent. However, when 6. = no is an integer, 
itis easily shown that h [n] 
8[n -nO], which is the impulse response ofthe ideal integer 
delay system. 
o 
T 
2T 
(a) 
~_ 
ycCr) 
Xc(t-~) 
'(1 I'llTf,t 
o 
l' 
21' 
(b) 
Figure 4.16 
(a) Continuous-time processing of the discrete-time sequence (b) can 
produce a new sequence with a "half-sample" delay. 
The noninteger delay represented by Eq. (4.65) has considerable practical sig­
nificance, since such a factor often arises in the frequency-domain representation of 
systems. When this kind of term is found in the frequency response of a causal discrete­
time system, it can be interpreted in the light of this example. This interpretation is 
illustrated in Example 4.8. 
Example 4.8 
Moving-Average System with Noninteger 
Delay 
In Example 2.16, we considered the general moving-average system and obtained its 
frequency response. For the case of the causal (M + I)-point moving-average system, 

178 
Chapter 4 
Sampling of Continuous-Time Signals 
M 1 = 0 and M 2 = M, and the frequency response is 
jw 
1 
sin[w(M + 1)/2] -jwMj2
H(e 
) = 
. 
e 
, 
Iwl < lL 
(4.66)
(M + 1) 
sm(wI2) 
This representation of the frequency response suggests the interpretation of the 
(M + l)-point moving-average system as the cascade of two systems, as indicated 
in Figure 4.17. The first system imposes a frequency-domain amplitude weighting. The . 
second system represents the linear-phase term in Eq. (4.66). If M is an even integer 
(meaning the moving average of an odd number of samples), then the linear-phase 
term corresponds to an integer delay, i.e., 
y[n] = w[n - M12]. 
(4.67) 
However, if M is odd, the linear-phase term corresponds to a noninteger delay, specif­
ically, an integer-plus-one-half sample interval. This noninteger delay can be inter­
preted in terms of the discussion in Example 4.7; i.e., y[n] is equivalent to bandlim­
ited interpolation of w[n], followed by a continuous-time delay of MT12 (where T 
is the assumed, but arbitrary, sampling period associated with the DIC interpola­
tion of w[nJ), followed by c/D conversion again with sampling period T. This frac­
tional delay is illustrated in Figure 4.18. Figure 4.18(a) shows a discrete-time sequence 
x[n] = coS(0.2Slfn). This sequence is the input to a six-point (M = S) moving-average 
filter. In this example, the input is "turned on" far enough in the past so that the output 
consists only of the steady-state response for the time interval shown. Figure 4.18(b) 
shows the corresponding output sequence, which is given by 
y[n] = H(ejO.25rr)~ejO.25rrn + H(e-jO.25rr)~e-jO.25rrn
2 
2 
= ~ sin[3(0.2Slf)] e- j(O.25rr)5j2 jO.25rrll + ~ sin[3(-0.2Slf)] ej (O.25rr)5j2 - jO.25rrn
e
e
26 sin(0.12Slf) 
26 sine-0.12Slf) 
= 0.308 coS[0.2Slf(n - 2.S)]. 
Thus, the six-point moving-average filter reduces the amplitude of the cosine signal 
and introduces a phase shift that corresponds to 2.S samples of delay. This is apparent 
in Figure 4.18, where we have plotted the continuous-time cosines that would be inter­
polated by the ideal DIC converter for both the input and the output sequence. Note 
in Figure 4.18(b) that the six-point moving-average filtering gives a sampled cosine 
signal such that the sample points have been shifted by 2.5 samples with respect to 
the sample points of the input. This can be seen from Figure 4.18 by comparing the 
positive peak at 8 in the interpolated cosine for the input to the positive peak at 10.S 
in the interpolated cosine for the output. Thus, the six-point moving-average filter is 
seen to have a delay of S/2 = 2.S samples. 
H(eiw) 
r--------------------------, 
1 
1 
1 
1 
1 
1 
sin(w(M + 1)/2) 
1 
--~ 
x[n] 1 
M + 1 
sin (wI2) 
: y[n] 
1 
1__________________________ -.J 
Figure 4.17 
The moving-average system represented as a cascade of two 
systems. 

179 
Section 4.6 
Changing the Sampling Rate Using Discrete-Time Processing 
n 
(a) 
n 
(b) 
Figure 4.18 Illustration of moving-average filtering. 
(a) Input signal 
x[n] = coS(O.25nn). (b) Corresponding output of six-point moving-average 
filter. 
4.6  CHANGING THE SAMPLING RATE USING 
DISCRETE-TIME PROCESSING 
We have seen that a continuous-time signal xc(t) can be represented by a discrete-time 
signal consisting of a sequence of samples 
x[n] 
xc(nT).  
(4.68) 
Alternatively, our previous discussion has shown that, even if x[n] was not obtained 
originally by sampling, we can always use the bandlimited interpolation formula of 
Eq. (4.25) to reconstruct a continuous-time bandlimited signal xr(t) whose samples are 
x[n1= xr(nT) = xc(nT), Le., the samples ofxc(t) andxr(t) are identical at the sampling 
times even when xr(t) "I xc(t). 
It is often necessary to change the sampling rate of a discrete-time signal, i.e., to 
obtain a new discrete-time representation of the underlying continuous-time signal of 
the form 
(4.69) 
where Tl "I T. This operation is often called resampling. Conceptually, x}[n] can be ob­
tained from x[n] by reconstructingxc(t) from x[n] using Eq. (4.25) and then resampling 
xc(t) with period Tl to obtain xl[n]. However, this is not usually a practical approach, 

180 
Chapter 4 
Sampling of Continuous-Time Signals 
x-[-n]---ll>""'lB xAn] = x[n~ 
Sampling 
Sampling 
Figure 4.19 
Representation of a 
period T 
period Td =MT 
compressor or discrete-time sampler. 
because of the nonideal analog reconstruction filter, D/A converter, and AID converter 
that would be used in a practical implementation. Thus, it is of interest to consider 
methods of changing the sampling rate that involve only discrete-time operations. 
4.6.1 Sampling Rate Reduction by an Integer Factor 
The sampling rate of a sequence can be reduced by "sampling" it, Le., by defining a new 
sequence 
xd[n] = x[nM] = xc(nMT). 
(4.70) 
Equation (4.70) defines the system depicted in Figure 4.19, which is called a sampling 
rate compressor (see Crochiere and Rabiner, 1983 and Vaidyanathan, 1993) or simply a 
compressor. From Eq. (4.70), it follows thatxd[n] is identical to the sequence that would 
be obtained from xc(t) by sampling with period Td = MY. Furthermore, if X c(jQ) = 0 
for IQI ::: QN, then xdn] is an exact representation of xc(t) ifTC/Td 
TC/(MT)::: QN. 
That is, the sampling rate can be reduced to TC / M without aliasing if the original sampling 
rate is at least M times the Nyquist rate or if the bandwidth of the sequence is first 
reduced by a factor of M by discrete-time filtering. In general, the operation of reducing 
the sampling rate (including any prefiltering) is called downsampling. 
As in the case of sampling a continuous-time signal, it is useful to obtain a frequency­
domain relation between the input and output of the compressor. This time, however, 
it will be a relationship between DTFfs. Although several methods can be used to de­
rive the desired result. we will base our derivation on the results already obtained for 
sampling continuous-time signals. First, recall that the DTFf of x[n] 
xc(nT) is 
JW. 
1 ~ [( W 
2TCk )]
X (e
) 
T L.- Xc j T - T 
. 
(4.71) 
k=-oo 
Similarly, the DTFf of xd[n] 
x[nM] = Xc(nTd) with Td = MT is 
1 ~ [(w
Xd(e JW ) 
L.-
X 
j ---
2TCr)] 
(4.72)
Td 
C 
Td 
Td 
. 
r=-oo 
Now, since Td = MT, we can write Eq. (4.72) as 
. 
1 
00 
[( W 
2TCr)]
Xd(eJi'1l) 
"X j 
-
- -
. 
(4.73)
MT L.-
c 
MT 
MT 
r=-oo 
To see the relationship between Eqs. (4.73) and (4.71), note that the summation index r 
in Eq. (4.73) can be expressed as 
r = i +kM, 
( 4.74) 

181 
Section 4.6 
Changing the Sampling Rate Using Discrete-Time Processing 
where k and i are integers such that -XJ < k < 00 and 0 ::: i ::: M 
1. Clearly, r is still 
an integer ranging from -00 to 00, but now Eq. (4.73) can be expressed as 
'w 
1 M-l/ 1 
00 
[ 
(1) 
2rck 
X dee} ) = - '\' 
'\' X 
j (- -
(4.75)
M L....t 
T 
L....t 
MT 
T
C 
::)JI·
i=O 
k=·-oo 
The term inside the square brackets in Eq. (4.75) is recognized from 
(4.71) as 
oo 
X ( j(w-2rri)/M. -.!. L
X [. 
(4.76)
e 
) -
c 
]
T k=-oo 
Thus, we can express Eq. (4.75) as 
M-l 
Xd(e jW ) = ~ L X (ej(w/M~2rri/M». 
(4.77) 
;=0 
There is a strong analogy between Eqs. (4.71) and (4.77): Equation (4.71) expresses 
the Fourier transform of the sequence of samples, x[n] (with period T), in terms of the 
Fourier transform of the continuous-time signal xc(t); Equation (4.77) expresses the 
Fourier transform ofthe discrete-time sampled sequence Xd [n 1(with sampling period M) 
in terms of the Fourier transform of the sequence x[n]. If we compare Eqs. (4.72) and 
(4.77), we see that X d(ej(l» can be thought of as being composed of the superposition 
of either an infinite set of amplitude-scaled copies of Xc (j Q), frequency scaled through 
OJ 
QTd and shifted by integer multiples of 2rc [Eq. (4.72)], or M amplitude-scaled 
copies of the periodic Fourier transform X (e jW ), frequency scaled by M and shifted by 
integer multiples of2;r [Eq. (4.77)]. Either interpretation makes it clear that Xd(ejW) is 
periodic with period 2rc (as are all DTFTs) and that aliasing can be avoided by ensuring 
that X (e jW ) is bandlimited, i.e., 
(4.78) 
and 2;r I M 2:. 2OJN· 
Downsampling is illustrated in Figure 4.20 for M 
2. Figure 4.20(a) shows the 
Fourier transform of a bandlimited continuous-time signal, and Figure 4.20(b) shows 
the Fourier transform of the impulse train of samples when the sampling period is 
T. Figure 4.20(c) shows X (e jW ) and is related to Figure 4.20(b) through Eq. (4.18). 
As we have already seen, Figures 4.20(b) and (c) differ only in a scaling of the fre­
quency variable. Figure 4.20( d) shows the DTFT of the downsampled sequence when 
M 
2. We have plotted this Fourier transform as a function of the normalized frequency 
OJ = QTd. Finally, Figure 4.20(e) shows the DTFT of the downsampled sequence plotted 
as a function of the continuous-time frequency variable Q. Figure 4.20( e) is identical 
to Figure 4.20(d), except for the scaling of the frequency axis through the relation 
Q = OJITd. 
In this example, 2;r IT 
4QN; i.e., the original sampling rate is exactly twice the 
minimum rate to avoid aliasing. Thus, when the original sampled sequence is downsam­
pled by a factor of M = 2, no aliasing results. If the downsampling factor is more than 
2 in this case, aliasing will result, as illustrated in Figure 4.21. 
Figure 4.21(a) shows the continuous-time Fourier transform of xc(t), and Fig­
ure 4.21(b) shows the DTFT of the sequence x[n] 
xc(nT), when 2rciT 
4QN. Thus, 

J;:) 
-fiN 
fiN 
fi 
(a) 
;r:=X(';flT)  
~ 
~ 
21T 
-fiN 
fiN 
21T 
fi 
T 
T 
(b) 
~ ~ ~ 
-21T 
-1T 
-WN 
wN=fiNT 1T 
21T 
W fiT 
(c) 
Xd(eiw) = ! [X(eiw12) + X(ej(w-2'11")I2)]
2
(M 2) 
1 
MT 
-21T 
-1T 
1T 
21T 
w=fiTd 
(d) 
(M=2) ~ 
~~Td 
41T 
21T 
21T 
41T 
fi=~ 
Td 
Td 
Td 
Td 
T 
Td 
(e) 
Figure 4.20 
Frequency-domain illustration of downsampling. 
182 

-ow 
fiN 
fi 
(a) 
i 
~---'----........L.--A:--,---1
x(e "'--",---)---L....-~~_ 
-21T 
-11' 
11' 
w=fiT 
(b) 
-211' 
31T 
-11' 
11' 
211' 
W =nTd 
2 
2 
(c) 
-1T 
11' 
11' 
1T 
211' 
w=fiT 
we= M 
M 
(d) 
1 X(eiw) 
Hd(eiw)X(e i"') 
I' 
(\ 
I 
-211' 
-11' 
1T 
11' 
11' 
11' 
21T 
w fiT 
3 
M 
3 
(e) 
Figure 4.21 
(a)-(c) Downsampling with aliasing. (d)-(f) Downsampling with 
prefiltering to avoid aliasing. 
183 

184 
Chapter 4 
Sampling of Continuous-Time Signals 
Lowpass filter 
I  
-----1....1 
Gain 1 
tM 
•  
x[n] 
Cutoff 'TrIM 
i[nJ 
xd[n] =x[nMJ 
Sampling 
Sampling 
Sampling 
Figure 4.22 
General system for
period T 
period T 
period Td 
MT 
sampling rate reduction by M. 
WN 
r?NT = n12. Now, if we downsample byafactorofM = 3, we obtain the sequence 
xd[nJ = x[3n] = xc(n3T) whose DTFf is plotted in Figure 4.21(c) with normalized 
frequency W = r?Td. Note that because MWN = 3rr:12, which is greater than rr:, aliasing 
occurs. In general, to avoid aliasing in downsampling by a factor of M requires that 
wNM :s rr: 
or 
WN S rr:IM. 
(4.79) 
If this condition does not hold, aliasing occurs, but it may be tolerable for some appli­
cations. In other cases, downsampling can be done without aliasing if we are willing to 
reduce the bandwidth of the signal x[n] before downsampling. Thus, if x[n] is filtered 
by an ideallowpass filter with cutoff frequency rr:1M, then the output x [n] can be down­
sampled without aliasing, as illustrated in Figures 4.21(d), (e), and (f). Note that the 
sequence xd[n] = x[nM] no longer represents the original underlying continuous-time 
signal xc(t). Rather, xd[n] = i:c(nTd), where Td = MT, and i:c(t) is obtained from xc(t) 
by lowpass filtering with cutoff frequency r?c = n 1Td = rr:1(MT). 
From the preceding discussion, we see that a general system for downsampling by 
a factor of M is the one shown in Figure 4.22. Such a system is called a decimator, and 
downsampling by lowpass filtering followed by compression has beentermed decimation 
(Crochiere and Rabiner, 1983 and Vaidyanathan, 1993). 
4.6.2 Increasing the Sampling Rate by an Integer Factor 
We have seen that the reduction of the sampling rate of a discrete-time signal by an 
integer factor involves sampling the sequence in a manner analogous to sampling a 
continuous-time signal. Not surprisingly, increasing the sampling rate involves opera­
tions analogous to D/C conversion. To see this, consider a signal x[n] whose sampling 
rate we wish to increase by a factor of L. Ifwe consider the underlying continuous-time 
signal xc(t), the objective is to obtain samples 
xi[n] = Xc(nTi), 
(4.80) 
where Ti = T / L, from the sequence of samples 
x[n] = xc(nT). 
(4.81) 
We will refer to the operation of increasing the sampling rate as upsampling. 
From Eqs. (4.80) and (4.81), it follows that 
xi[n] = x[nlL] = xc(nT1L), 
n=0,±L,±2L,.... 
(4.82) 
Figure 4.23 shows a system for obtaining xi[n] from x[n] using only discrete-time pro­
cessing. The system on the left is called a sampling rate expander (see Crochiere and 
Rabiner, 1983 and Vaidyanathan, 1993) or simply an expander. Its output is 
{X[n/L], n 
0,±L,±2L,... ,
[] _ 
(4.83)
Xe n -
0 
h' 
, 
ot erWlse, 

I 
Section 4.6 
Changing the Sampling Rate Using Discrete-Time Processing  
185 
Lowpass filter 
I--~ Gain=L 
x[n] 
xe[n] 
Cutoff = 7TIL 
Xi[n] 
Sampling 
Sampling 
Sampling 
Figure 4.23 
General system for
period T 
period T; = TIL 
period T; 
TIL 
sampling rate increase by L. 
e  
or equivalently, 
d  
xe[n] = L
00 
x[k]8[n - kL]. 
(4.84)
g 
k=-oo 
The system on the right is a lowpass discrete-time filter with cutoff frequency n / Land 
I) 
gain L. This system plays a role similar to the ideal D/C converter in Figure 4.9(b). 
i-
First, we create a discrete-time impulse train xe[n], and we then use a lowpass filter to 
o  
reconstruct the sequence. 
d  
The operation of the system in Figure 4.23 is most easily understood in the fre­
quency domain. The Fourier transform of xe[n] can be expressed as 
~­
e 
jUJ  
j1vn 
e  
X e(e
) = n'%:;oo C~oo x[k]8[n 
kL]) e­
(4.85) 
= X (e jUJL ).
L 
00 
x[k]e-jlvLk
y 
k=-oo
d 
Thus, the Fourier transform of the output of the expander is a frequency-scaled 
n 
version of the Fourier transform of the input; i.e., w is replaced by wL so that (J) is now 
normalized by 
(J) = rlT;. 
(4.86) 
This effect is illustrated in Figure 4.24. Figure 4.24(a) shows a bandlimited continuous­
n 
time Fourier transform, and Figure 4.24(b) shows the DTFT of the sequence x[n] 
:a 
xAnT), where n/T = rlN. Figure 4.24(c) shows X e(ejUJ) according to Eq. (4.85), with 
ra-
L 
2, and Figure 4.24( e) shows the Fourier transform ofthe desired signal Xi [n]. We see 
that Xi(ejUJ ) can be obtained from Xe(ejUJ ) by correcting the amplitude scale from l/T
b! 
to 1/ Tj and by removing all the frequency-scaled images of X c(jrl) except at integer 
multiples of 2n. For the case depicted in Figure 4.24, this requires a lowpass filter with a 
gain of 2 and cutoff frequency n /2, as shown in Figure 4.24( d). In general, the required 
:0) 
gain would be 
since L (1/ T) = [1/(T/ L)] = 1/Ti, and the cutoff frequency would be 
n/L. 
This example shows that the system of Figure 4.23 does indeed give an output 
satisfying Eq. (4.80) if the input sequence x[n] 
xc(nT) was obtained by sampling 
without aliasing. Therefore, that system is called an interpolator, since it fills in the 
missing samples, and the operation of upsampling is consequently considered to be 
synonymous with interpolation. 
As in the case ofthe D/Cconverter, it is possible to obtain an interpolation formula 
for xi(n] in terms of x[n]. First, note that the impulse response of the lowpass filter in 
Figure 4.23 is 
sin(nn/L) 
~1) 
hi[n] = 
.  
(4.87)
53)  
nn/L 

£ 
-0.111 
0.111 
0. 
(a) 
-21T 
-1T 
1T 
21T 
w=O,T 
(b) 
T 
~=~ 
41T 
21T 
1T 
1T 
21T 
W 
o'Ti
41T 
21T 
L 
L 
L 
L 
L 
L 
(c) 
f('~; 
-21T 
-1T 
1T 
L 
(d) 
1T 
L 
1T 
21T 
w 
o'T; 
1 
L 
T 
X;(e jW ) 
~ 
~ 
-21T 
-1T 
1T 
L 
(e) 
1T 
L 
1T 
21T 
w 
o'T; 
Figure 4.24 
Frequency-domain illustration 01 interpolation. 
186 
.. 

Section 4.6 
Changing the Sampling Rate Using Discrete-Time Processing 
187 
Using Eq. (4.84), we obtain 
00 
.sill[n(n 
kL)/L]
xi[n] = 
x[k]------
(4.88)
L 
n(n - kL)/L
k=-oo 
The impulse response hi [n] has the properties 
hi[O] = 1, 
(4.89) 
n = ±L, ±2L,.... 
Thus, for the ideallowpass interpolation filter, we have 
Xi[n] = x[n/L] = xc(nT/ L) = Xc (nTi ), 
n 
0, ±L, ±2L, ... , 
(4.90) 
as desired. The fact that Xi en] 
Xc (n1i) for all n follows from our frequency-domain 
argument. 
4.6.3 Simple and Practical Interpolation Filters 
Although ideal lowpass filters for interpolation cannot be implemented exactly, very 
good approximations can be designed using techniques to be discussed in Chapter 7. 
However, in some cases, very simple interpolation procedures are adequate or are forced 
on us by computational limitations. Since linear interpolation is often used (even though 
it is often not very accurate), it is worthwhile to examine this process within the general 
framework that we have just developed. 
Linear interpolation corresponds to interpolation so that the samples between 
two original samples lie on a straight line connecting the two original sample values. 
Linear interpolation can be accomplished with the system of Figure 4.23 with the filter 
having the triangularly shaped impulse response 
h. [n] _ 
Inl/L, Inl::s L,
{1 
(4.91)
lin 
-
0, 
otherwise, 
as shown in Figure 4.25 for L 
5. With this filter, the interpolated output will be 
n+L-l 
Xlin[n] = L 
xe[k]hlin[n 
k]. 
(4.92) 
k=n-L+l 
Figure 4.26( a) depicts Xe [k] (with the envelope of hlin [n -k] shown dashed for a particular 
value n 
18) and the corresponding outputxlin[n] for the case L = 5. In thiscase,xlin[n] 
for n = 18 depends only on original samples x[3] and x[4]. From this figure, we see that 
Xlin[n] is identical to the sequence obtained by connecting the two original samples on 
either side of n by a straight line and then resampling at the L - 1 desired points in 
Figure 4.25 
Impulse response for 
o 
n 
linear interpolation. 

Chapter 4 
Sampling of Continuous-Time Signals
188 
1h..[. - 'I 
',[kl
-~, 
L=5 
T •••• I .... J•••<.<-:-~ •~'I';,.,.. T • 
o 
L 
2L 
3L 
n 
4L 
5L 
k 
~'l 
n 
(a) 
(b) 
Figure 4.26 
(a) Illustration of linear interpolation by filtering. (b) Frequency re­
sponse of linear interpolator compared with ideallowpass interpolation filter. 
between. Also, note that the original sample values are preserved because hlin[O] = 1 
and hlin[n] = 0 for Inl > L. 
The nature of the distortion in the intervening samples can be better understood 
by comparing the frequency response of the linear interpolator with that of the ideal 
lowpass interpolator for a factor of L interpolation. It can be shown (see Problem 4.56) 
that 
Hlin(ejW ) = ~ [Si~(WL/2)J2 
(4.93)
L 
sm(w/2) 
This function is plotted in Figure 4.26(b) for L = 5 together with the ideal lowpass 
interpolation filter. From the figure, we see that if the original signal is sampled at just 
the Nyquist rate, 
not oversampled, linear interpolation will not be very accurate, 
since the output of the filter will contain considerable energy in the band n / L < Iwl ::: n 
due to the frequency-scaled images of Xc(jQ) at multiples of2n/ L that are not removed 
by the linear interpolation filter. However, if the original sampling rate is much higher 
than the Nyquist rate, then the linear interpolator will be more successful in removing 
these images because Hlin(eiw) is small in a narrow region around these normalized 
frequencies, and at higher sampling rates, the increased frequency scaling causes the 
shifted copies of XcOQ) to be more localized at multiples of 2IT / L. This is intuitively 
....  

189 
Section 4.6 
Changing the Sampling Rate Using Discrete-Time Processing 
reasonable from a time domain perspective too, since, if the original sampling rate 
greatly exceeds the Nyquist rate, the signal will not vary significantly between samples, 
and thus, linear interpolation should be more accurate for oversampled signals. 
Because ofits double-sided infinite-length impulse response, the ideal bandlimited 
interpolator involves all of the original samples in the computation of each interpolated 
sample. In contrast, linear interpolation involves only two of the original samples in the 
computation of each interpolated sample. To better approximate ideal bandlimited in­
terpolation, it is necessary to use filters with longer impulse responses. For this purpose 
FIR filters have many advantages. The impulse response hi [n] of an FIR filter for inter­
polation by a factor L usually is designed to have the following properties: 
Inl2:KL 
(4.94a)  
Inl s KL 
(4.94b)  
hi[O] = 1 
n=O 
(4.94c)  
hi[n] = 0 
n = ±L, ±2L, ... , ±KL. 
(4.94d)  
The interpolated output will therefore be 
n+KL-l 
xi[n] = 
L 
xe[k]hi[n - k]. 
(4.95) 
k=n-KL+l 
Note that the impulse response for linear interpolation satisfies Eqs. (4.94a)-( 4.94d) 
with K = 1. 
It is important to understand the motivation for the constraints of Eqs. (4.94a)­
(4.94d). Equation (4.94a) states that the length of the FIR filter is 2KL - 1 samples. 
Furthermore, this constraint ensures that only 2K original samples are involved in the 
computation of each sample of xi[n]. This is because, even though hi[n] has 2KL-1 
nonzero samples, the input xe[k] has only 2K nonzero samples within the region of 
support of hi[n - k] for any n between two of the original samples. Equation (4.94b) 
ensures that the filter will not introduce any phase shift into the interpolated samples 
since the corresponding frequency response is a real function of w. The system could 
be made causal by introducing a delay of at least K L - 1 samples. In fact, the impulse 
response hi [n - K L] would yield an interpolated output delayed by K L samples, which 
would correspond to a delay of K samples at the original sampling rate. We might want 
to insert other amounts of delay so as to equalize delay among parts of a larger system 
that involves subsystems operating at different sampling rates. Finally, Eqs. (4.94c) and 
(4.94d) guarantee that the original signal samples will be preserved in the output, i.e., 
Xi[n] = x[n/L] at 
n = 0, ±L, ±2L, .... 
(4.96) 
Thus, if the sampling rate of Xi [n] is subsequently reduced back to the original rate (with 
no intervening delay or a delay by a multiple of L samples) then Xi [nL] = x [n]; i.e., the 
original signal is recovered exactly. If this consistency is not required, the conditions of 
Eqs. (4.94c) and (4.94d) could be relaxed in the design of hi[n]. 

190  
Chapter 4 
Sampling of Continuous-Time Signals 
Figure 4.27 shows xe[k] and hdn 
k] with K = 2. The figure shows that each 
interpolated value depends on 2K = 4 samples of the original input signal. Also note 
that computation of each interpolated sample requires only 2K multiplications and 
2K 
1 additions since there are always L 
1 zero samples in xe[k] between each of the 
original samples. 
h;[n - kJ 
;'  
xe[k]
/ 
" 
I 
" 
I I 
I " " 
5L
- • •• •• - -.., - - ,/ -
- • - • • - .... - I 
o 
L 
-
.... 
3L 
n 
4L 
-
.... 
6L 
k 
Figure 4.27 
Illustration of interpolation involving 2K =4 samples when L= 5. 
Interpolation is a much-studied problem in numerical analysis. Much of the de­
velopment in this field is based on interpolation formulas that exactly interpolate poly­
nomials of a certain degree. For example, the linear interpolator gives exact results for 
a constant signal and one whose samples vary along a straight line. Just as in the case 
of linear interpolation, higher-order Lagrange interpolation formulas (Schafer and Ra­
biner, 1973) and cubic spline interpolation formulas (Keys, 1981 and Unser, 2(00) can 
be cast into our linear filtering framework to provide longer filters for interpolation. 
For example, the equation 
I 
(a+2)lnILI3 
(a+3)lnILI2+1 
O~n~L 
hj[n] = 
~Inl LI3 - SinILI2 + SainiLI 
4a 
L ~ n ~ 2L 
(4.97) 
otherwise 
defines a convenient family of interpolation filter impulse responses that involve four 
(K = 2) original samples in the computation of each interpolated sample. Figure 4.28( a) 
shows the impulse response of a cubic filter for a = -0.5 and L = 5 along with the 
filter (dashed triangle) for linear (K 
1) interpolation. The corresponding frequency 
responses are shown in Figure 4.28(b) on a logarithmic amplitude (dB) scale. Note that 
the cubic filter has much wider regions around the frequencies 2rcILand 4rcIL (OArc 
and 0.8rc in this case) but lower sidelobes than the linear interpolator, which is shown 
as the dashed line. 
4.6.4  Changing the Sampling Rate by a Noninteger 
Factor 
We have shown how to increase or decrease the sampling rate of a sequence by an 
integer factor. By combining decimation and interpolation, it is possible to change 
the sampling rate by a noninteger factor. Specifically, consider Figure 4.29(a), which 

• • • 
• • • 
I 
-
-
linear pulse 
a(L _55) 
~ 
1-
cubic pulse 
I 
'" , 
0.5 
'" 
" 
'" 
o~r-~,-~~J~/~'T_/~~~~~~~-~~",~~~~~~  
-10 
-5  
o 
5 
10 
n 
20 
o
a:l 
"0 
.5 
-20 
-051T 
o 
0.51T 
1T 
Figure 4.28 Impulse responses and frequency responses for linear and cubic 
interpolation. 
Interpolator  
Decimator 
r 
I 
I 
x[n) 
l.'tdln] 
_.J
Sampling 
period: 
T  
T 
T 
T 
TM 
L 
L 
L 
L 
(a) 
Lowpass filter  
Gain =L  
Cutoff = 
x[n] 
min (1TIL, 1TIM)
Sampling  
period: 
T 
T 
T 
TM  
L 
L 
L  
(b) 
Figure 4.29 
(a) System for changing the sampling rate by a noninteger fac­
tor. (b) Simplified system in which the decimation and interpolation filters are 
combined. 
191 

Chapter 4 
Sampling of Continuous-Time Signals
192 
shows an interpolator that decreases the sampling period from T to T / L, followed by a 
decimator that increases the sampling period by M, producing an output sequence xd[n] 
that has an effective sampling period of (T M / L). By choosing L and M appropriately, 
we can approach arbitrarily close to any desired ratio of sampling periods. For example, 
if L = 100 and M 
101, then the effective sampling period is 1.01T. 
If M > L, there is a net increase in the sampling period (a decrease in the sam­
pling rate), and if M < L, the opposite is true. Since the interpolation and decima­
tion filters in Figure 4.29(a) are in cascade, they can be combined as shown in Fig­
ure 4.29(b) into one lowpass filter with gain L and cutoff equal to the minimum of 
If/ L and If/ M. If M > L, then If/ M is the dominant cutoff frequency, and there is a 
net reduction in sampling rate. As pointed out in Section 4.6.1. if x[n] was obtained by 
sampling at the Nyquist rate, the sequence xd[n] will correspond to a lowpass-filtered 
version of the original underlying bandlimited signal if we are to avoid aliasing. On 
the other hand, if M < L, then If/ L is the dominant cutoff frequency, and there will 
be no need to further limit the bandwidth of the signal below the original Nyquist 
frequency. 
Example 4.9 Sampling Rate Conversion by a Noninteger 
Rational Factor 
..... 
Figure 4.30 illustrates sampling rate conversion by a rational factor. Suppose that a 
bandlimited signal with X cU0.) as given in Figure 4.30(a) is sampled at the Nyquist 
rate; i.e., 2rr / T = 20.N. The resulting DTFf 
X (e jW
2rrk))
) = ~ i: Xc (j (~ _ 
k=-oo 
T 
T 
is plotted in Figure 4.30(b). An effective approach to changing the sampling period 
to (3/2)T, is to first interpolate by a factor L = 2 and then decimate by a factor of 
M = 3. Since this implies a net decrease in sampling rate, and the original signal was 
sampled at the Nyquist rate, we must incorporate additiona:I bandlimiting to avoid 
aliasing. 
Figure 4.30( c) shows the DTFf of the output of the L 
2 ups ampler. Ifwe were 
interested only in interpolating by a factor of 2, we could choose the lowpass filter to 
have a cutoff frequency of We 
rr /2 and a gain of L = 2. However, since the output 
of the filter will be decimated by M = 3, we must use a cutoff frequency of We = rr/3, 
but the gain of the filter should still be 2 as in Figure 4.30(d). The Fourier transform 
Xi(e jW ) of the output of the lowpass filter is shown in Figure 4.30(e). The shaded 
regions indicate the part of the signal spectrum that is removed owing to the lower 
cutoff frequency for the interpolation filter. Finally, Figure 4.30(f) shows the DTFf 
of the output of the downsampler by M 
3. Note that the shaded regions show the 
aliasing that would have occurred if the cutoff frequency of the interpolation lowpass 
filter had been rr /2 instead of rr /3. 

193 
Section 4.6 
Changing the Sampling Rate Using Discrete-Time Processing 
(M=3) 
w=f!TIL 
(f) 
Figure 4.30 
Illustration of changing the sampling rate by a non integer factor. 

194  
Chapter 4 
Sampling of Continuous-Time Signals 
4.7 MULTIRATE SIGNAL PROCESSING 
As we have seen, it is possible to change the sampling rate of a discrete-time signal by a 
combination of interpolation and decimation. For example, if we want a new sampling 
period of 1.01T, we can first interpolate by L = 100 using a lowpass filter that cuts off 
at We = j(/101 and then decimate by M = 101. These large intermediate changes in' 
sampling rate would require large amounts of computation for each output sample if we 
implement the filtering in a straightforward manner at the high intermediate sampling 
rate that is required. Fortunately, it is possible to greatly reduce the amount of com­
putation required by taking advantage of some basic techniques broadly characterized 
as multirate signal processing. These multirate techniques refer in general to utilizing 
upsampling, downsampling, compressors, and expanders in a variety of ways to increase 
the efficiency of signal-processing systems. Besides their use in sampling rate conver­
sion, they are exceedingly useful in AID and DIA systems that exploit oversampling 
and noise shaping. Another important class of signal-processing algorithms that relies 
increasingly on multirate techniques is filter banks for the analysis andlor processing of 
signals. 
Because of their widespread applicability, there is a large body of results on mul­
tirate signal processing techniques. In this section, we will focus on two basic results 
and show how a combination of these results can greatly improve the efficiency of sam­
pling rate conversion. The first result is concerned with the interchange of filtering and 
downsampling or upsampling operations. The second is the polyphase decomposition. 
We shall also give two examples of how multirate techniques are used. 
4.7.1  Interchange of Filtering with  
CompressorlExpander  
First, we will derive two identities that aid in manipulating and understanding the opera­
tion of multirate systems. Itis straightforward to show that the two systems in Figure 4.31 
are equivalent. To see the equivalence, note that in Figure 4.31(b), 
X b(ejW ) = H(e}wM)X (e jW ),  
(4.98) 
and from Eq. (4.77), 
. 
1 M-l 
Y(e1W ) = M L Xb(e}(w/M-2rri/M).  
(4.99) 
i=O 
bI  
H(z) ~ 
~ tM 
xa[n] 
y[n] 
(a) 
~H(ZM)b1 tM ~ 
Figure 4.31 
Two equivalent systems 
(b)  
based on downsampling identities. 

195 
Section 4.7 
Multirate Signal Processing
s 
H(z) 
tL b
~ ~ 
a 
(a) 
19 
ff 
in 
tL ~H(ZL)b  
Ie 
~ 
Figure 4.32 Two equivalent systems  
Ig 
(b) 
based on upsampling identities.  
n­ 
:d 
Substituting Eq. (4.98) into Eq. (4.99) gives 
19 
M-l
se 
Y(e}W) = ~ LX (e}(w/M-2Jri/M)H(e j (w 
21ri). 
(4.100)
~r.. 
1=0
ng 
.es 
Since H(ei(w 
21f1» 
H(ejW), Eq. (4.100) reduces to 
of 
M-l 
Y(eiw) = H(eJW) ~ LX (eJ(w/M-21fi/M»
ul­
i=O
llts 
m­
nd 
(4.101) 
which corresponds to Figure4.31(a). Therefore, the systems in Figure4.31(a) and4.31(b) 
are completely equivalent. 
A similar identity applies to upsampling. Specifically, using Eq. (4.85) in Sec­
tion 4.6.2, it is also straightforward to show the equivalence of the two systems in Fig­
ure 4.32. We have, from Eq. (4.85) and Figure 4.32(a), 
Y(e)W) 
X a(eiwL ) 
(4.102) 
Since, from Eq. (4.85), 
X b(ejW) 
X (e)wL), 
it follows that Eq. (4.102) is, equivalently,  
99)  
Y(eJW) = H(e)wL)X b(eiw), 
which corresponds to Figure 4.32(b). 
In summary, we have shown that the operations of linear filtering and downsam­
pIing or upsampling can be interchanged if we modify the linear filter. 
4.7.2 Multistage Decimation and Interpolation 
When decimation or interpolation ratios are large, it is necessary to use filters with very 
long impulse responses to achieve adequate approximations to the required lowpass 
filters. In such cases, there can be significant reduction in computation through the use 
of multistage decimation or interpolation. Figure 4.33(a) shows a two-stage decimation 

196 
Chapter 4 
Sampling of Continuous-Time Signals 
H1(z) 
t Ml 
H2(z) 
t M2
~ ~ 
~Anl 
(a) 
(b) 
Figure 4.33 Multistage decimation: 
(a) Two-stage decimation system.
~ H1(z)H2(zMl) Ht (M1M2) ~d[nl 
(b) Modification of (al using 
downsampling identity of Figure 4.31. 
(c) 
(c) Equivalent one-stage decimation. 
system where the overall decimation ratio is M 
MI M2. In this case, two lowpass 
filters are required; HI (z) corresponds to a lowpass filter with nominal cutoff frequency 
:rr/ MI and likewise, H2(Z) has nominal cutoff frequency :rr/ M2. Note that for single­
stage decimation, the required nominal cutoff frequency would be :rr/ M = :rr/(MI M2), 
which would be much smaller than that of either of the two filters. In Chapter 7 we will 
see that narrowband filters generally require high-order system functions to achieve 
sharp cutoff approximations to frequency-selective filter characteristics. Because of this 
effect, the two-stage implementation is often much more efficient than a single-stage 
implementation. 
The single-stage system that is equivalent to Figure 4.33( a) can be derived using the 
downsampling identity of Figure 4.31. Figure 4.33(b) shows the result of replacing the 
system H2(Z) and its preceding downsampler (by Ml) by the system H2(ZMl) followed 
by a downsampler by MI. Figure 4.33(c) shows the result of combining the cascaded 
linear systems and cascaded downsamplers into corresponding single-stage systems. 
From this, we see that the system function of the equivalent single-stage lowpass filter 
is the product 
H(z) 
HI (z)H2(zMl). 
(4.103) 
This equation, which can be generalized to any number of stages if M has many factors, 
is a useful representation of the overall effective frequency response of the two-stage 
decimator. Since it explicitly shows the effects of the two filters, it can be used as an aid in 
designing effective multistage decimators that minimize computation. (See Crochiere 
and Rabiner, 1983, Vaidyanathan, 1993, and Bellanger, 2000.) The factorization in 
Eq. (4.103) has also been used directly to design lowpass filters (Neuvo et aI., 1984). 
In this context, the filter with system function represented by Eq. (4.103) is called an 
interpolated FIR filter. This is because the corresponding impulse response can be seen 
to be the convolution of h 1[n] with the second impulse response expanded by Ml; i.e., 
00 . 
h[n] 
hl[n] * L h2[k]8[n - kMd· 
(4.104) 
k=-oo 
The same multistage principles can be applied to interpolation, where, in this case, 
the upsampling identity of Figure 4.32 is used to relate the two-stage interpolator to an 
equivalent one-stage system. This is depicted in Figure 4.34. 

197 
lals 
Section 4.7 
Multirate Signal Processing 
u:-:-u 
UlnJ
rm~~~1 H 2(z) ! 
(a) 
(b) 
Figure 4.34 
Multistage interpolation: 
x(t7L1 
LxJnJ 
(a) Two-stage interpolation system. 
1. 
! 
t (Ll~) 
H1( ZL2)H2(z) ! 
(b) Modification of (a) using upsampling
H 
identity of Figure 4.32. (c) Equivalent 
(c) 
one-stage interpolation. 
>ass 
ncy 
gle­
k(2), 
4.7.3 Polyphase Decompositions 
will 
The polyphase decomposition of a sequence is obtained by representing it as a super­
leve 
position of M subsequences, each consisting of every Mth value of successively delayed 
this 
versions of the sequence. When this decomposition is applied to a filter impulse re­
sponse, it can lead to efficient implementation structures for linear filters in several 
contexts. Specifically, consider an impulse response h[n] that we decompose into M 
subsequences hdn] with k = 0,1, ... , MIas follows: 
h[] _{h[n+k]' 
n = integer multiple of M. 
(4.105)
k n -
0, 
otherwise. 
By successively delaying these subsequences, we can reconstruct the original impulse 
response h[n]; i.e., 
M-l 
h[n] = L hdn 
k]. 
(4.106) 
k=O 
This decomposition can be represented with the block diagram in Figure 4.35. If we 
create a chain of advance elements at the input and a chain of delay elements at the 
output, the block diagram in Figure 4.36 is equivalent to that of Figure 4.35. In the 
decomposition in Figures 4.35 and 4.36, the sequences ekln] are 
(4.107)
i.e., 
and are referred to in general as the polyphase components of h[n). There are several 
104) 
other ways to derive the polyphase components, and there are other ways to index them 
for notational eonvenience (Bellanger, 2000 and Vaidyanathan, 1993), but the definition 
in Eq. (4.107) is adequate for our purpose in this section. 
an 
Figures 4.35 and 4.36 are not realizations of the filter, but they show how the filter 
can be decomposed into M parallel filters. We see this by noting that Figures 4.35 and 

Chapter 4 
Sampling of Continuous-Time Signals
198 
h[n] 
h[n + 1] 
h[n] 
h[n + 2] 
h[n+M-1] L..'_---I 
Figure 4.35 
Polyphase decomposition of filter h[n] using components edn]. 
h[n] 
Figure 4.36 
Polyphase decomposition of filter h[n] using components ekln] with 
chained delays. 
4.36 show that, in the frequency or z-transform domain, the polyphase representation 
corresponds to expressing H (z) as 
M-1 
H(z) = L EkCzM)z-k. 
(4.108) 
k=O 
Equation (4.108) expresses the system function H (z) as a sum of delayed polyphase 
component filters. For example, from Eq. (4.108), we obtain the filter structure shown 
in Figure 4.37. 
lit. 

199 
Section 4.7 
Multirate Signal Processing 
x[n] 
Figure 4.37 
Realization structure 
based on polyphase decomposition 
of h[n). 
4.7.4 Polyphase Implementation of Decimation Filters 
One of the important applications of the polyphase decomposition is in the implemen­
tation of filters whose output is then downsampled as indicated in Figure 4.38. 
In the most straightforward implementation of Figure 4.38, the filter computes 
an output sample at each value of n, but then only one of every M output samples is 
retained. Intuitively, we might expect that it should be possible to obtain a more efficient 
implementation, which does not compute the samples that are thrown away. 
To obtain a more efficient implementation, we can exploit a polyphase decomposi­
tion of the filter. Specifically, suppose we express h[n) in polyphase form with polyphase 
components 
ek[n] 
h[nM +k). 
(4.109) 
From Eq. (4.108), 
H(z) 
M-l 
= L Ek(ZM)Z-k. 
(4.110) 
k=O 
With this decomposition and the fact that downsampling commutes with addition, 
ure 4.38 can be redrawn as shown in Figure 4.39. Applying the identity in Figure 4.31 
to the system in Figure 4.39, we see that the latter then becomes the system shown in 
Figure 4.40. 
To illustrate the advantage of Figure 4.40 compared with Figure 4.38, suppose 
that the input x[n) is clocked at a rate of one sample per unit time and that H (z) is an 
N-point FIR filter. In the straightforward implementation of Figure 4.38, we require N 
multiplications and (N - 1) additions per unit time. In the system of Figure 4.40, each 
of the filters E k(Z) is of length N / M, and their inputs are clocked at a rate of 1 per M 
units oftime. Consequently, each filter requires 1(Z) multiplications per unit time and 
it (~ 1) additions per unit time. Since there are M polyphase components, the entire 
system therefore requires (N/ M) multiplications and (Z 
1) + (M 
1) additions per 
unit time. Thus, we can achieve a significant savings for some values of M and N. 
x[n] 
w [n] = y [nM] 
Figure 4.38 
Decimation system. 

200 
Chapter 4 
Sampling of Continuous-Time Signals 
x[n] 
Figure 4.39 
Implementation of 
decimation filter using polyphase 
decomposition. 
x[n] 
Figure 4.40 
Implementation of 
decimation filter after applying the 
downsampling identity to the polyphase 
decomposition. 
4.7.5 Polyphase Implementation of Interpolation Filters 
A savings similar to that just discussed for decimation can be achieved by applying the 
polyphase decomposition to systems in which a filter is preceded by an upsampler as 
shown in Figure 4.41. Since only every Lth sample of w[nl is nonzero, the most straight­
forward implementation of Figure 4.41 would involve multiplying filter coefficients by 
sequence values that are known to be zero. Intuitively, here again we would expect that 
a more efficient implementation was possible. 
To implement the system in Figure 4.41 more efficiently, we again utilize the 
polyphase decomposition of H (z). For example, we can express H(z) as in the form 
of Eq. (4.110) and represent Figure 4.41 as shown in Figure 4.42. Applying the identity 
in Figure 4.32, we can rearrange Figure 4.42 as shown in Figure 4.43. 
To illustrate the advantage of Figure 4.43 compared with Figure 4.41, we note 
that in Figure 4.41 if x[n] is clocked at a rate of one sample per unit time, then w[n] is 
clocked at a rate of L samples per unit time. If H (z) is an FIR filter of length N, we then 
~ tL b.rl H(z) h:i 
Figure 4.41 
Interpolation system. 

201 
s  
Section 4.7 
Multirate Signal Processing 
y[n] 
x[n] 
Z-I 
Figure 4.42 
Implementation of 
interpolation filter using polyphase 
decomposition. 
y[n] 
x[n] 
Figure 4.43 
Implementation of 
<. _-I  
interpolation filter after applying the 
upsampling identity to the polyphase 
decomposition. 
require N L multiplications and (N L - 1) additions per unit time. Figure 4.43, on the 
other hand, requires L eN/L) multiplications and L (tt - 1) additions per unit time for 
the set of polyphase filters, plus (L - 1) additions, to obtain y[n]. Thus, we again have 
the possibility of significant savings in computation for some values of Land N. 
For both decimation and interpolation, gains in computational efficiency result 
from rearranging the operations so that the filtering is done at the low sampling rate. 
Combinations of interpolation and decimation systems for noninteger rate changes lead 
to significant savings when high intermediate rates are required. 
4.7.6 Multirate Filter Banks 
Polyphase structures for decimation and interpolation are widely used in filter banks 
for analysis and synthesis of audio and speech signals. For example, Figure 4.44 shows 
the block diagram of a two-channel analysis and synthesis filter bank commonly used 
in speech coding applications. The purpose of the analysis part of the system is to 
split the frequency spectrum of the input x[n] into a lowpass band represented by the 
downsampled signal vo[n] and ahighpass band represented by VI [n]. In speech and audio 
coding applications, the channel signals are quantized for transmission and/or storage. 
Since the original band is nominally split into two equal parts of width Jf/2 radians, the 

202 
Chapter 4 
Sampling of Continuous-Time Signals 
x[n] 
v1[n] 
analysis 
synthesis 
Figure 4.44 Two-channel analysis and synthesis filter bank. 
sampling rates of the filter outputs can be 1/2 that of the input so that the total number 
of samples per second remains the same.I Note that downsampling the output of the 
lowpass filter expands the low-frequency band to the entire range Iwl < Jr. On the other 
hand, downsampling the output of the highpass filter down~shifts the high-frequency 
band and expands it to the full range Iwl < 7C. 
The decomposition requires that ho[n] and hI [n] be impulse responses of lowpass 
and highpass filters respectively. A common approach is to derive the highpass filter from 
the lowpass filter by hI [n] 
ej1fll h o[n]. This implies that HI (ejW) = Ho(ei (w-1f) so that 
if Ho(ejW) is a lowpass filter with nominal passband 0 :s Iwl :s 7C /2, then HI (eiu,) will be 
a highpass filter with nominal passband Jr/2 < I(vl 
7C. The purpose of the righthand 
(synthesis) part of Figure 4.44 is to reconstitute an approximation to x[n] from the two 
channel signals vo[n] and vI[n]. This is achieved by upsampling both signals and passing 
them through a lowpass filter go[n] and highpass filter g1 [n] respectively. The resulting 
interpolated signals are added to produce the full-band output signal y[n] sampled at 
the input sampling rate. 
Applying the frequency-domain results for downsampling and upsampling to the 
system in Figure 4.44 leads to the following result: 
Y(e jW) 
~ [Go(eiW) Ho(e jW ) + GI (ej(ll) Hl (e jW)] X(e jW ) 
(4.111a) 
+~ [Go(ejW) Ho(ei (w-1f) 
+Gl (eiw)HI (ej (W-1f)] X (ei (w-1f). 
(4.111b) 
Ifthe analysis and synthesis filters are ideal so that they exactly split the band 0 
Iwl:s 7C 
into two equal segments without overlapping, then it is straightforward to verify iliat 
Y(eiw ) 
X (eiw ); i.e., the synthesis filter bank reconstructs the input signal exactly. 
However, perfect or nearly perfect reconstruction also can be achieved with nonideal 
filters for which aliasing will occur in the downsampling operations of the analysis filter 
bank. To see this, note that the second term in the expression for Y(eiw) (line labeled 
Eq. (4.111b)), which represents potential aliasing distortion from the downsampling 
operation, can be eliminated by choosing the filters such that 
Go(e]W)Ho(ei (w-1f) + GI(eiw)Hl(ei (w-1f) = O. 
(4.112) 
1 Filter banks that conserve the total number of samples per second are termed maximally decimated 
filter banks. 

203 
Section 4.7
lis 
lSS 
)m 
lat 
be 
ind 
Multirate Signal Processing 
This condition is called the alias cancellation condition. One set of conditions that satisfy 
Eq. (4.112) is 
hl[n] = ej1fnho[n] ¢=> Hl(ejW) = Ho(ej (w-1f) 
(4.113a) 
go[n] 
2ho[n] ¢=> Go(e jW) = 2Ho(ejW) 
(4.113b) 
glLn] = -2hl[n] ¢=> GI (e jW) = - 2Ho(e)(w-1f). 
(4.113c) 
The filters ho[n] and hl[n] are termed quadrature mirrorfilterssince Eq. (4.113a) imposes 
mirror symmetry about w = 1r/2. Substituting these relations into Eq. (4.111a) leads to 
the relation 
(4.114) 
from which it follows that perfect reconstruction (with possible delay of M samples) 
requires 
jwM
H6(e j (}» 
-
H6(ej (w-1t) = e-
. 
(4.115) 
It can be shown (Vaidyanathan, 1993) that the only computationally realizable filters 
satisfying Eq. (4.115) exactly are systems with impulse responses of the form horn] = 
co8[n -
2no] + c18[n - 2nl 
1] where no and nl are arbitrarily chosen integers and 
COCI = !. Such systems cannot provide the sharp frequency selective properties needed 
in speech and audio coding applications, but to illustrate that such systems can achieve 
exact reconstruction, consider the simple two-point moving average lowpass filter 
1 
horn] = 2(8[n] + 8[n -1]), 
(4.116a) 
which has frequency response 
Ho(ejW ) = cos(w/2)e- jw/2. 
(4.116b) 
For this filter, Y(e jW ) 
e-jroX(ejro) as can be verified by substituting Eq. (4.116b) into 
Eq. (4.114). 
Either FIR or IIR filters can be used in the analysis/synthesis system of Figure 4.44 
with the filters related as in Eq. (4.113a)-( 4.113c) to provide nearly perfect reconstruc­
tion. The design of such filters is based on finding a design for Ho(e jW) that is an accept­
able lowpass filter approximation while satisfying Eq. (4.115) to within an acceptable 
approximation error. A set of such filters and an algorithm for their design was given 
by Johnston (1980). Smith and Barnwell (1984) and Mintzer (1985) showed that perfect 
reconstruction is possible with the two-channel filter bank of Figure 4.44 if the filters 
have a different relationship to one another than is specified by Eq. (4.113a)-(4.113c). 
The different relationship leads to filters called conjugate quadrature filters (CQF). 
Polyphase techniques can be employed to save computation in the implementation 
of the analysis/synthesis system of Figure 4.44. Applying the polyphase downsampling 
result depicted in Figure 4.40 to the two channels leads to the block diagram in Fig­
ure 4.45(a), where 
eoo[n1 
ho[2n] 
(4.117a) 
e01[n] = ho[2n + 1] 
(4.117b) 
elO[n] = hI [2n] = ej21fnho[2n] 
eoo[nJ 
(4.117c) 
j21fn j1t
ell[n] = hI [2n + 1] = e
e
ho[2n + 1] 
-e01 [nl. 
(4.117d) 

204 
Chapter 4 
Sampling of Continuous-Time Signals 
x[nJ 
vo[n] 
Z-l 
Vj[n] 
(a) 
x[n) 
Figure 4.45 
Polyphase representation 
of the two-channel analysis filter bank of 
Figure 4.44. 
Equations (4.117c) and (4.1l7d) show that the polyphase filters for hl[n] are the same 
(except for sign) as those for horn]. Therefore, only one set, eoo[n] and eodn] need be 
implemented. Figure 4.45(b) shows how both vo[n] and vl[n] can be formed from the 
outputs of the two polyphase filters. This equivalent structure, which requires only half 
the computation of Figure 4.45(a), is, of course, owing entirely to the simple relation 
between the two filters. 
The polyphase technique can likewise be applied to the synthesis filter bank, by 
recognizing that the two interpolators can be replaced by their polyphase implementa­
tions and then the polyphase structures can be combined because gl [n] = _ejJfn go[n] = 
-ejJfn2ho[n]. The resulting polyphase synthesis system can be represented in terms of 
the polyphase filters iOo[n] 
2eoo[n] and Jot en] = 2eodn] as in Figure 4.46. As in the 
case of the analysis filter bank, the synthesis polyphase filters can be shared between 
the two channels thereby halving the computation. 
x[n] 
y[n] 
(b) 
Vj[n] 
Figure 4.46 
Polyphase representation of the two-channel analysis and synthesis filter bank 
of Figure 4.44. 

205 
Digital Processing of Analog Signals 
This two-band analysis/synthesis system can be generalized to N equal width chan­
nels to obtain a finer decomposition of the spectrum. Such systems are used in audio 
coding, where they facilitate exploitation of the characteristics of human auditory per­
ception in compression of the digital information rate. (See MPEG audio coding stan­
dard and Spanias, Painter, and Atti, 2007.) Also, the two-band system can be incorpo­
rated into a tree structure to obtain an analysis/synthesis system with either uniformly 
or nonuniformly spaced channels. When the CQF filters of Smith and Barnwell, and 
Mintzer are used, exact reconstruction is possible, and the resulting analysis synthe­
sis system is essentially the discrete wavelet transform. (See Vaidyanathan, 1993 and 
Burrus, Gopinath and Guo, 1997.) 
DIGITAL PROCESSING OF ANALOG SIGNALS 
So far, our discussions of the representation of continuous-time signals by discrete-time 
signals have focused on idealized models of periodic sampling and band limited interpo­
lation. We have formalized those discussions in terms of an idealized sampling system 
that we have called the ideal continuous-to-discrete (c/D) converter and an idealized 
bandlimited interpolator system called the ideal discrete-to-continuous (D/C) converter. 
These idealized conversion systems allow us to concentrate on the essential mathemati­
cal details ofthe relationship between a bandlimited signal and its samples. For example, 
in Section 4.4 we used the idealized c/D and D/C conversion systems to show that LTI 
discrete-time systems can be used in the configuration of Figure 4.47(a) to implement 
LTI continuous-time systems if the input is bandlimited and the sampling rate is at or 
above the Nyquist rate. In a practical setting, continuous-time signals are not precisely 
bandlimited, ideal filters cannot be realized, and the ideal c/D and D/C converters can 
only be approximated by devices that are called analog-to-digital (A/D) and digital­
to-analog (D/A) converters, respectively. The block diagram of Figure 4.47(b) shows a 
ClD 
I---­
x[n]
xcCt) 
Discrete-time 
system 
Die 
yIn] 
Yr(t) 
t 
t 
T 
T 
(a) 
Sample 
Compensated
AID 
Discrete-time 
DIA
and 
reconstruction
converter 
system
hold 
filter
x[n] 
yIn] 
DA (t) 
Yr(t) 
HrUD.) 
xalt) 
xoCt) 
T 
T 
T 
(b) 
Figure 4.47 
(a) Discrete-time filtering of continuous-time signals. (b) Digital processing of 
analog Signals. 

206 
Chapter 4 
Sampling of Continuous-Time Signals 
more realistic model for digital processing of continuous-time (analog) signals. In this 
section, we will examine some of the considerations introduced by each of the compo­
nents of the system in Figure 4.47(b). 
4.8.1 Prefiltering to Avoid Aliasing 
In processing analog signals using discrete-time systems, it is generally desirable to 
minimize the sampling rate. This is because the amount of arithmetic processing required 
to implement the system is proportional to the number of samples to be processed. 
If the input is not bandlimited or if the Nyquist frequency of the input is too high, 
prefiltering may be necessary. An example of such a situation occurs in processing speech 
signals, where often only the low-frequency band up to about 3 to 4 kHz is required for 
intelligibility, even though the speech signal may have significant frequency content in 
the 4 kHz to 20 kHz range. Also, even if the signal is naturally bandlimited. wide band 
additive noise may fill in the higher frequency range, and as a result of sampling, these 
noise components would be aliased into the low-frequency band. If we wish to avoid 
aliasing, the input signal must be forced to be bandlimited to frequencies below one-half 
the desired sampling rate. This can be accomplished by lowpass filtering the continuous­
time signal prior to C/D conversion, as shown in Figure 4.48. In this context, the lowpass 
filter that precedes the C/D converter is called anantialiasingfilter. Ideally, the frequency 
response of the antialiasing filter would be 
H ('Q)={I, IQI<Qc~TCIT. 
(4.118)
aa ) 
0, IQI 2: Qc. 
From the discussion of Section 4.4.1, it follows that the overall system, from the output 
of the antialiasing filter x a(t) to the output Yr (t), will always behave as an LTI system, 
since the input to the C/D converter, xa(t), is forced by the antialiasing filter to be 
bandlimited to frequencies below TC IT radians/so Thus, the overall effective frequency 
response of Figure 4.48 will be the product of H aa(jQ) and the effective frequency 
response from xa(t) to Yr (t). Combining Eqs. (4.118) and (4.38) gives 
'fiT 
H 
('Q) = H(el 
), IQI < Qc," 
(4.119)
eff } 
{ o. 
IQI 2: Qc . 
Thus, for an ideallowpass antialiasing filter, the system of Figure 4.48 behaves as anLTI 
system with frequency response given by Eq. (4.119), even when X cUQ) is not ban­
dlimited. In practice, the frequency response H aa Un) cannot be ideally bandlimited, 
but H aa(jQ) can be made small for IQI > TC IT so that aliasing is minimized. In this case, 
the overall frequency response of the system in Figure 4.48 should be approximately 
HeffUQ) "'" HaaUQ)H(ejflT). 
(4.120) 
To achieve a negligibly small frequency response above TC IT, it would be necessary 
for H aa (j Q) to begin to "roll off," i.e., begin to introduce attenuation, at frequencies 
below TCIT, Eq. (4.120) suggests that the roll-off of the antialiasing filter (and other 
LTI distortions to be discussed later) could be at least partially compensated for by 
taking them into account in the design of the discrete-time system. This is illustrated in 
Problem 4.62. 
~ 

207 
Digital Processing of Analog Signals 
e 
d 
f 
s 
y 
cy 
ICY 
L9) 
JTI 
an­
ed, 
lse, 
Y 
20) 
,ary 
des 
:her 
. by 
din 
Section 4.8 
Anti­
Discrete- 
aliasing  
ClD 
time 
Die 
xc(t) 
x[n] 
system 
yen]
filter 
xa(t) I-.....,.._..J 
HaaUH) 
T 
T 
Figure 4.48 
Use of prefiltering to avoid aliasing. 
Sampling rate reduction by !vi 
r-
I ,..-----, 
I 
Sharp
Simple 
1----11..,.... antialiasing
antialiasing I----I~ 
filter
xit) 
x[n]
filter 
cutoff =1TIM 
Figure 4.49 
Using oversampled AID conversion to simplify a continuous-time 
antialiasing filter. 
The preceding discussion requires sharp-cutoff antialiasing filters. Such sharp­
cutoff analog filters can be realized using active networks and integrated circuits. How­
ever, in applications involving powerful, but inexpensive, digital processors, these contin­
uous-time filters may account for a major part of the cost of a system for discrete-time 
processing of analog signals. Sharp-cutoff filters are difficult and expensive to imple­
ment, and if the system is to operate with a variable sampling rate, adjustable filters 
would be required. Furthermore, sharp-cutoff analog filters generally have a highly 
nonlinear phase response, particularly at the passband edge. Thus, it is desirable for 
several reasons to eliminate the continuous-time filters or simplify the requirements on 
them. 
One approach is depicted in Figure 4.49. With QN denoting the highest frequency 
component to eventually be retained after the antialiasing filtering is completed, we first 
apply a very simple antialiasing filter that has a gradual cutoff with significant atten­
uation at MQN. Next implement the CID conversion at a sampling rate much higher 
than 2QN, e.g., at 2MQN. After that, sampling rate reduction by a factor of M that 
includes sharp antialiasing filtering is implemented in the discrete-time domain. Subse­
quent discrete-time processing can then be done at the low sampling rate to minimize 
computation. 
This use of oversampling followed by sampling rate conversion is illustrated in 
Figure 4.50. Figure 4.50(a) shows the Fourier transform of a signal that occupies the 
band IQI < QN, plus the Fourier transform ofwhat might correspond to high-frequency 
"noise" or unwanted components that we eventually want to eliminate with the an­
tialiasing filter. Also shown (dotted line) is the frequency response of an antialiasing 
filter that does not cut off sharply but gradually falls to zero at frequencies above the 
frequency QN. Figure 4.50(b) shows the Fourier transform of the output of this filter. If 
the signal xa(t) is sampled with period T such that (2:rr/T - Qc) ::0: QN, then the DTFT 

208  
Chapter 4 
Sampling of Continuous-Time Signals 
Section 
Xe(11)  
Simple anti­
aliaSing filter 
_ _ 
High-frequency 
--_ 
/ 
noise 
/ 
-.............. /. 
~--­
-11<  
He 
11 
(a) 
t.iXa(j~~j 
Filtered
LLs;g"" Lei  
-11" 
-HN 
11N 
He 
11 
(b) 
X(eiw) 
T= 7T/(MHl\l)
Sharp-cutoff 
T 
"I decimation filter ~ - ~ - - • 
Aliased noise 
I 
I 
-27T  
-WN 
WN=11l\lT=; 
27T 
W = 11T 
(c) 
~I~L'
j 
-27T 
-7T  
7T 
27T 
w=11Td 
(d) 
Figure 4.50 
Use of oversampling followed by decimation in C/O conversion. 
of the sequence x[n] will be as shown in Figure 4.50(c). Note that the "noise" will be 
aliased, but aliasing will not affect the signal band leul < euN = QNT. Now, if T and Td 
are chosen so that Td = MT and 1rlTd = QN, then x[n] can be filtered by a sharp-cutoff 
discrete-time filter (shown idealized in Figure 4.50(c)) with unity gain and cutoff fre­
quency 1r1M. The output of the discrete-time filter can be downsampled by M to obtain 
the sampled sequence xd[n] whose Fourier transform is shown in Figure 4.S0(d). Thus, 
all the sharp-cutoff filtering has been done by a discrete-time system, and only nominal 
continuous-time filtering is required. Since discrete-time FIR filters can have an exactly 
linear phase, it is possible using this oversampling approach to implement antialiasing 
filtering with virtually no phase distortion. This can be a significant advantage in situa­
tions where it is critical to preserve not only the frequency spectrum, but the waveshape 
as well. 

209 
lis 
Section 4.8 
Digital Processing of Analog Signals 
Sample 
AID 
and 
xa(t)  
hold 
xo(t) converter  XB[Il]  
Figure 4.51 
Physical configuration for 
T 
T 
AID conversion. 
4.8.2 NO Conversion 
An ideal ClD converter converts a continuous-time signal into a discrete-time signal. 
where each sample is known with infinite precision. As an approximation to this for 
digital signal processing. the system of Figure 4.51 converts a continuous-time (analog) 
signal into a digital signal, i.e., a sequence of finite-precision or quantized samples. 
The two systems in Figure 4.51 are available as physical devices. The AID converter is a 
physical device that converts a voltage orcurrent amplitude at its input into a binary code 
representing a quantized amplitude value closest to the amplitude of the input. Under 
the control of an external clock, the AID converter can be caused to start and complete 
an AID conversion every T seconds. However, the conversion is not instantaneous, and 
for this reason, a high-performance AID system typically includes a sample-and-hold, 
as in Figure 4.51. The ideal sample-and-hold system is the system whose output is 
xoCt) = L
00 
x[n]ho(t - nT), 
(4.121) 
n=-oo 
where x[n] = X a (n T) are the ideal samples of x a (t) and hoU) is the impulse response 
of the zero-order-hold system, i.e., 
h ( ) 
T,
{I, 0< t < 
(4.122)
o t = 0, otherwise. 
If we note that Eq. (4.121) has the equivalent form 
xo(t) = hoU) * L
00 
xa(nT )8(t 
nT), 
(4.123) 
n=-oo 
we see that the ideal sample-and-hold is equivalent to impulse train modulation followed 
by linear filtering with the zero-order-hold system, as depicted in Figure 4.52(a). The 
relationship between the Fourier transform of xoCt) and the Fourier transform of xaU) 
can be worked out following the style of analysis of Section 4.2, and we will do a similar 
analysis when we discuss the DIA converter. However, the analysis is unnecessary at 
this point, since everything we need to know about the behavior of the system can 
be seen from the time-domain expression. Specifically, the output of the zero-order 
hold is a staircase waveform where the sample values are held constant during the 
sampling period of T seconds. This is illustrated in Figure 4.52(b). Physical sample-and­
hold circuits are designed to sample xaCt) as nearly instantaneously as possible and to 
hold the sample value as nearly constant as possible until the next sample is taken. 
The purpose of this is to provide the constant input voltage (or current) required by 
the AID converter. The details of the wide variety of AID conversion processes and 
the details of sample-and-hold and AID circuit implementations are outside the scope 
of this book. Many practical issues arise in obtaining a sample-and-hold that samples 

210 
Chapter 4 
Sampling of Continuous-Time Signals 
Sample and hold 
00 
s(t)=2: 8(t-nT) 
n=-oo 
Zero-order 
hold 
Xa(t)  
Xo(t) 
ho(t) 
IL _________________ ~ 
(a) 
(b) 
Figure 4.52 
(a) Representation 
of an ideal sample-and-hold. 
(b) Representative input and output 
signals for the sample-and-hold. 
quickly and holds the sample value constant with no decay or "glitches." Likewise, 
many practical concerns dictate the speed and accuracy of conversion of ND converter 
circuits. Such questions are considered in Hnatek (1988) and Schmid (1976), and details 
of the performance of specific products are available in manufacturers' specification 
and data sheets. Our concern in this section is the analysis of the quantization effects in 
AID conversion. 
Since the purpose of the sample-and-hold in Figure 4.51 is to implement ideal 
sampling and to hold the sample value for quantization by the AID converter, we can 
represent the system of Figure 4.51 by the system of Figure 4.53, where the ideal ClD 
converter represents the sampling performed by the sample-and-hold and, as we will 
describe later, the quantizer and coder together represent the operation of the AID 
converter. 
The quantizer is a nonlinear system whose purpose is to transform the input sample 
x[n] into one of a finite set of prescribed values. We represent this operation as 
x[n] = Q(x[nD 
(4.124) 
Figure 4.53 
Conceptual representation 
T 
of the system in Figure 4.51. 

211 
Section 4.8 
Digital Processing of Analog Signals 
2 
2 
2 
2 
2 
2 
2 
-8 
.r = Q(x) 
Two's-complement Offset binary 
code 
code 
011 
111 
010 
110 
001 
101 
000 
100 
x 
111 
011 
110 
010 
101 
001 
100 
000 
Figure 4.54 
Typical quantizer for AID conversion. 
and refer to x[n] as the quantized sample. Quantizers can be defined with either uni­
formly or nonuniformly spaced quantization levels; however, when numerical calcula­
tions are to be done on the samples, the quantization steps usually are uniform. Figure 
4.54 shows a typical uniform quantizer characteristic,2 in which the sample values are 
rounded to the nearest quantization level. 
Several features of Figure 4.54 should be emphasized. First, note that this quan­
tizer would be appropriate for a signal whose samples are both positive and negative 
(bipolar). If it is known that the input samples are always positive (or negative), then 
a different distribution of the quantization levels would be appropriate. Next, observe 
that the quantizer of Figure 4.54 has an even number of quantization levels. With an 
even number of levels, it is not possible to have a quantization level at zero amplitude 
and also have an equal number of positive and negative quantization levels. Generally, 
the number of quantization levels will be a power of two, but the number will be much 
greater than eight, so this difference is usually inconsequential. 
Figure 4.54 also depicts coding of the quantization levels. Since there are eight 
quantization levels, we can label them by a binary code of 3 bits. (In general, 2B+1levels 
can be coded with a (B + 1)-bit binary code.) In principle, any assignment of symbols 
2Such quantizers are also called linear quantizers because of the linear progression of quantization 
steps. 

212 
Chapter 4 
Sampling of Continuous-Time Signals 
can be used, and many binary coding schemes exist, each with its own advantages and 
disadvantages, depending on the application. For example, the right-hand column of 
binary numbers in Figure 4.54 illustrates the offset binary coding scheme, in which the 
binary symbols are assigned in numeric order, starting with the most negative quantiza­
tion level. However, in digital signal processing, we generally wish to use a binary code 
that permits us to do arithmetic directly with the code words as scaled representations 
of the quantized samples. 
The left-hand column in Figure 4.54 shows an assignment according to the two's 
complement binary number system. This system for representing signed numbers is 
used in most computers and microprocessors; thus, it is perhaps the most convenient 
labeling of the quantization levels. Note, incidentally, that the offset binary code can be 
converted to two's-complement code simply by complementing the most significant bit. 
In the two's-complement system, the leftmost, or most significant, bit is considered 
as the sign bit, and we take the remaining bits as representing either binary integers 
or fractions. We will assume the latter; i.e., we assume a binary fraction point between 
the two most significant bits. Then, for the two's-complement interpretation, the binary 
symbols have the following meaning for B = 2: 
Binary symbol 
Numeric value, xB 
00 11 
3/4 
0",10 
1/2 
00 01 
1/4 
00 00 
o 
101 1 
-1/4 
1",10 
-1/2 
100 1 
-3/4 
1000 
-1 
In general, if we have a (B + I)-bit binary two's-complement fraction of the form 
aOoalaz ... aB, 
then its value is 
-ao2° +alrl +az2-z + ... +aB2-B. 
Note that the symbol 0 denotes the "binary point" of the number. The relationship 
between the code words and the quantized signal levels depends on the parameter X m 
in Figure 4.54. This parameter determines the full-scale level of the AID converter. 
From Figure 4.54, we see that the step size of the quantizer would in general be 
2X m 
Xm 
Ll = 2B+l 
(4.125) 
The smallest quantization levels (±Ll) correspond to the least significant bit of the 
binary code word. Furthermore, the numeric relationship between the code words and 
the quantized samples is 
x[n] = XmXB[n], 
(4.126) 
since we have assumed thatXB[n] isa binary number such that -1 :S: xB[n] < 1 (for two's 
complement). In this scheme, the binary coded samples xB [n] are directly proportional 

213 
Section 4.8 
Digital Processing of Analog Signals 
o 
Quantized samples 
• 
Unquantized samples 
<ll 
"0 
.€ 
Ci 
E « 
3A 
2A 
A 
0 
-A 
-2A 
-3A 
-4A 
-­ Output of ideal sample and hold 
- Output of DIA converter 
Original 
signal 
T 
2T 
3T 
4T 
5T 
000 
100 
110 
011 
011 
Figure 4.55 
Sampling, quantization, coding, and D/A conversion with a3-bit quantizer. 
to the quantized samples (in two's-complement binary); therefore, they can be used 
as a numeric representation of the amplitude of the samples. Indeed, it is generally 
appropriate to assume that the input signal is normalized, so that the numeric values of 
x[n] and xB[n] are identical and there is no need to distinguish between the quantized 
samples and the binary coded samples. 
Figure 4.55 shows a simple example of quantization and coding of the samples of 
a sine wave using a 3-bit quantizer. The unquantized samples x[n] are illustrated with 
solid dots, and the quantized samples x[n] are illustrated with open circles. Also shown 
is the output of an ideal sample-and-hold. The dotted lines labeled "output of D/A 
converter" will be discussed later. Figure 4.55 shows, in addition, the 3-bit code words 
that represent each sample. Note that, since the analog input Xa (t) exceeds the full-scale 
value of the quantizer, some of the positive samples are "clipped." 
Although much of the preceding discussion pertains to two's-complement cod­
ing of the quantization levels, the basic principles of quantization and coding in AID 
conversion are the same regardless of the binary code used to represent the samples. 
A more detailed discussion of the binary arithmetic systems used in digital computing 
can be found in texts on computer arithmetic. (See, for example, Knuth, 1998.) We now 
turn to an analysis of the effects of quantization. Since this analysis does not depend on 
the assignment of binary code words, it will lead to rather general conclusions. 

Chapter 4 
Sampling of Continuous-Time Signals
214 
x[n] 
\l~'J 
Ix[n] 
Q(x[nD 
x[nJ 
Y x[n] =x[n] + ern] 
Figura 4.56 
Additive noise model for 
ern] 
quantizer. 
4.8.3 Analysis of Quantization Errors 
From Figures 4.54 and 4.55, we see that the quantized sample x[n] will generally be dif­
ferent from the true sample value x[n]. The difference between them is the quantization 
error, defined as 
ern] = x[n] 
x[n).  
(4.127) 
For example, for the 3-bit quantizer of Figure 4.54, if 6./2 < x[n) :::: 36./2, thenx[n) 
6., 
and it follows that 
-6./2:::: ern) < 6./2.  
(4.128) 
In the case of Figure 4.54, Eq. (4.128) holds whenever 
-96./2 < x[n) :::: 76./2.  
(4.129) 
In the general case of a (B + I)-bit quantizer with 6. given by Eq. (4.125), the quanti­
zation error satisfies Eq. (4.128) whenever 
(-Xm 
6./2) < x[n] S (Xm - 6./2).  
(4.130) 
If x[n] is outside this range, as it is for the sample at t = 0 in Figure 4.55, then the 
quantization error may be larger in magnitude than 6./2, and such samples are said to 
be clipped, and the quantizer is said to be overloaded. 
A simplified, but useful, model of the quantizer is depicted in Figure 4.56. In this 
model, the quantization error samples are thought of as an additive noise signal. The 
model is exactly equivalent to the quantizer if we know ern]. In most cases, however, 
ern] is not known, and a statistical model based on Figure 4.56 is then often useful in 
representing the effects ofquantization. We will also use such a model in Chapters 6 and 
9 to describe the effects of quantization in signal-processing algorithms. The statistical 
representation of quantization errors is based on the following assumptions: 
1.  The error sequence e[n) is a sample sequence of a stationary random process. 
2.  The error sequence is uncorrelated with the sequence x[n).3 
3.  The random variables of the error process are uncorrelated; i.e., the error is a 
white-noise process. 
4.  The probability distribution of the error process is uniform over the range of 
quantization error. 
3This does not, of course, imply statistical independence, since the error is directly determined by the 
input signal. 
.  

215 
Section 4.8
Is 
a 
Digital Processing of Analog Signals 
As we will see, the preceding assumptions lead to a rather simple, but effective, 
analysis of quantization effects that can yield useful predictions of system performancc. 
It is easy to find situations where these assumptions arc not valid. For example, if Xa (t) 
is a step function, the assumptions would not be justified. However, when the signal 
is a complicated signal, such as speech or music, where the signal fluctuates rapidly in 
a somewhat unpredictable manner, the assumptions are more realistic. Experimental 
measurements and theoretical analyses for random signal inputs have shown that, when 
the quantization step size (and therefore the error) is small and when the signal varies 
in a complicated manner, the measured correlation between the signal and the quanti­
zation error decreases, and the error samples also become uncorrelated. (See Bennett, 
1948; Wid row, 1956, 1961; Sripad and Snyder, 1977; and Widrow and Kolhir, 2008.) In 
a heuristic sense, the assumptions of the statistical model appear to be valid when the 
quantizer is not overloaded and when the signal is sufficiently complex, and the quanti­
zation steps are sufficiently small, so that the amplitude of the signal is likely to traverse 
many quantization steps from sample to sample. 
Example 4.10 Quantization Error for a Sinusoidal Signal 
As an illustration, Figure 4.57(a) shows the sequence of unquantized samples of the 
cosine signal x[n] 
O.99cos(n/1O). Figure 4.57(b) shows the quantized sample se­
quence x[nl = Q{x[n]) for a 3-bit quantizer (8 + 1 = 3), assuming that X m 
1. The 
dashed lines in this figure show the eight possible quantization levels. Figures 4.57(c) 
and 4.57(d) show the quantization error ern] = x[n] - x[n] for 3- and 8-bit quantiza­
tion, respectively. In eaeh case, the scale of the quantization error is adjusted so that 
the range ±1:>./2 is indicated by the dashed lines. 
Notice that in the 3-bit case, the error signal is highly correlated with the un­
quantized signal. For example, around the positive and negative peaks of the cosine, 
the quantized signal remains constant over many consecutive samples, so that the error 
has the shape of the input sequence during these intervals. Also, note that during the 
intervals around the positive peaks, the error is greater than 1:>./2 in magnitude because 
the signal level is too large for this setting of the quantizer parameters. On the other 
. hand, the quantization error for 8-bit quantization has no apparent patterns.4 Visual 
inspection of these figures supports the preceding assertions about the quantization­
noise properties in the finely quantized (8-bit) case; i.e., the error samples appear to 
vary randomly, with no apparent correlation with the unquantized signal, and they 
range between -1:>./2 and +1:>./2. 
(a) 
Figure 4.57 
Example of quantization noise. (a) Unquantized samples of the signal 
x[n] 
0.99 cos(n/1O). 
4For periodic cosine signals, the quantization error WOUld, of course, be periodic, too; and therefore, 
its power spectrum would be concentrated at multiples of the frequency of the input signaL We used the 
frequency iVO 
1/10 to avoid this case in the example. 

216 
Chapter 4 
Sampling of Continuous-Time Signals 
o 
-I, 
--, 
o 
50 
100 
150n 
(b) 
_::~i~:&rt5aS~~~~  
o 
50 
100 
150n 
(c) 
o 
50 
100 
150n 
(d) 
Figure 4.57 
(continued) (b) Quantized samples of the cosine waveform in part 
(a) with a 3-bit quantizer. (c) Quantization error sequence for 3-bit Quantization of 
the signal in (a). (d) Quantization error sequence for 8-bit quantization of the signal 
in (a). 
For quantizers that round the sample value to the nearest quantization level, as 
shown in Figure 4.54, the amplitude of the quantization noise is in the range 
~1)./2 S ern] < 1)./2. 
(4.131) 
For small 1)., it is reasonable to assume that ern] is a random variable uniformly dis­
tributed from - I). /2 to I). /2. Therefore, the 1 st-order proba bility density assumed for 
the quantization noise is as shown in Figure 4.58. (If truncation rather than rounding 
is used in implementing quantization, then the error would always be negative, and we 
would assume a uniform probability density from -I). to 0.) To complete the statistical 
model for the quantization noise, we assume that successive noise samples are uncor­
related with each other and that ern] is uncorrelated with x[n]. 'Thus, ern] is assumed to 
be a uniformly distributed white-noise sequence. The mean value of ern] is zero, and its 
Pe,,(e) 
1 
,:l 
,:l 
2-BX m 
Figure 4.58 
Probability density 
function of quantization error for a 
,:l 
,:l 
e 
rounding quantizer such as that of 
2 
2 
Figure 4.54. 

i 
,Ials 
Section 4.8 
Digital Processing of Analog Signals 
217 
variance is 
1 
t,.2 
-de= 
. 
(4.132)
t,. 
12 
For a (B + I)-bit quantizer with full-scale value X "" the noise variance, or power, is 
On 
2-2BX 2
2 
m
=-U",-. 
(4.133)
ae 
Equation (4.133) completes the white-noise model of quantization noise since the au­
tocorrelation function would be <Pee[m] = a;8[m] and the corresponding power density 
spectrum would be 
On 
(4.134) 
Example 4.11 
Measurements of Quantization Noise 
To confirm and illustrate the validity ofthe model for quantization noise, consider again 
;On 
quantization of the signal X[II] = .99cos(njlO) which can be computed with 64-bit 
floating-point precisions (for all practical purposes unquantized) and then quantized 
to B + 1 bits. The quantization noise sequence can also be computed since we know 
both the input and the output of the quantizer. An amplitude histogram, which gives a 
count of the number of samples lying in each of a set ofcontiguous amplitude intervals 
or "bins," is often used as an estimate of the probability distribution of a random signal. 
Figure 4.59 shows histograms of the quantization noise for 16- and 8-bit quantization 
Histograms for Quantization Noise Samples 
.1, as 
. -r~------,----.---~------.------­
1500 
' _-._-mT--~I=-=;;:;==~~ 
.... 
1000
131) 
.J::J'" e 
j
::l
dis-
Z 
500 
Hor 
ding 
0 
dwe 
-2 
-1.5 
-1 
-0.5 
0 
0.5 
1 
1.5 
2 
x 10-5 
,tical 
e 
lcor­
1500 
~d to 
Id its 
I­
1000
<>
..c. 
S 
::l 
Z 
500 
x 10-3 
e 
Figure 4.59 
Histograms of quantization noise for (a) B + 1 
16 and 
(b) B+ 1 = 8. 
-3 
-2 
-1 
0 
1 
2 
3 
4 

218 
Chapter 4 
Sampling of Continuous-Time Signals 
with Xm 
1. Since the total number ofsamples was 101000, and the number ofbins was 
101, we should expect approximately 1000 samples in each bin if the noise is uniformly 
distributed. Furthermore the total range of samples should be ±1/216 = 1.53 x 10-5 
for 16-bit quantization and ±1/28 
3.9 x 10-3 for 8-bit quantization. The histograms 
of Figure 4.59 are consistent with these values, although the 8-bit case shows some 
obvious deviation from the uniform distribution. 
In Chapter 10, we show how to calculate estimates of the power density spectrum. 
Figure 4.60 shows such spectrum estimates for quantization noise signals where B +1 = 
16, 12,8, and 4 bits. Observe that in this example, when the number of bits is 8 or 
greater, the spectrum is quite flat over the entire frequency range a ::: W ::: ]f, and the 
spectrum level (in dB) is quite close to the value 
1010glO(Pee(ejW»= 1010glO (~2B) -(10.79 + 6.02B), 
. 
12(2 
) 
which is predicted by the white-noise uniform-distribution model. Note that the curves 
for B = 7, 11, and 15 differ at all frequencies by about 24 dB. Observe, however, that 
when B + 1 = 4, the model fails to predict the shape of the power spectrum of the 
noise. 
-10 
-20 
-30 
-40 
-50 
ra -60 
-70 
-80 
-90 
-100 
-110 
0 
Figure 4.60 
Power Spectra for Uniform Quantizers 
8 
11 
8 = 15 
0.2 
0.4 
0.6 
0.8 
wl7r 
Spectra of quantization noise for several values of B. 
This example demonstrates that the assumed model for quantization noise is use­
ful in predicting the performance of uniform quantizers. A common measure of the 
amount of degradation of a signal by additive noise in general and quantization noise 
in particular is the signal-to-noise ratio (SNR), defined as the ratio of signal variance 
(power) to noise variance. Expressed in dB, the signal-to-quantization-noise ratio of a 

219 
Section 4.8 
Digital Processing of Analog Signals 
(B + I)-bit uniform quantizer is 
SNRQ 
1010glO (:~) = 1010glO C2 ':;0';) 
(4.135) 
6.02B + 10.8 
2010glO ( :~ ) . 
From Eq. (4.135), we see that the SNR increases approximately 6 dB for each bit added 
to the word length of the quantized samples, i.e., for each doubling of the number of 
quantization levels. It is important to consider the term 
-2010g10 (:~ ) 
(4.136) 
in Eq. (4.135). First, recall that X In is a parameter ofthe quantizer, and it would usually 
be fixed in a practical system. The quantity ax is the rms value of the signal amplitude, 
and it would necessarily be less than the peak amplitude of the signaL For example, if 
xa(t) is a sine wave of peak amplitude X p, then ax 
Xp/.fi. Ifax is too large, the peak 
signal amplitude will exceed the full-scale amplitude X m of the AID converter. In this 
case Eq. (4.135) is no longer valid, and severe distortion results. On the other hand, if 
ax is too small, then the term in Eq. (4.136) will become large and negative, thereby 
decreasing the SNR in Eq. (4.135). In fact, it is easily seen that when ax is halved, the 
SNR decreases by 6 dB. Thus, it is very important that the signal amplitude be carefully 
matched to the full-scale amplitude of the ND converter. 
Example 4.12 
SNR for Sinusoidal Signal 
Using the signal x[n] 
A cos(n/lO), we can compute the quantization error for differ­
ent values of B + 1 with Xm = 1 and A varying. Figure 4.61 shows estimates of SNR 
as a function of Xm /O'x obtained by computing the average power over many samples 
of the signal and dividing by the corresponding estimate of the average power of the 
noise; i.e., 
1 N-l 
N L (x[n])2 
1 N-l 
N L (e[n])2 
n=O 
where in the case of Figure 4.61, N = 101000. 
Observe that the curves in Figure 4.61 closely follow Eq. (4.135) over a wide 
range of values of B. In particular, the curves are straight lines as a function of 
log(Xm/o-x ), and they are offset from one another by 12 dB because the values of 
B differ by 2. SNR increases as Xm/O'x decreases since increasing O'x with Xm fixed 

220 
Chapter 4 
Sampling of Continuous-Time Signals 
SNR for Uniform Quantizers 
100 
90 
80 
70 
(Xl 
60 
'0 
.5 50
0:: 
Z 
(/) 
40 
30 
20 
10 
0 
to-I 
100 
103
101 
10" 
Xm/(Tx 
Figure 4.61 
Signal-to-quantization-noise ratio as afunction of Xm /ax for several 
values of B. 
means that the signal uses more of the available quantization levels. However. note 
the precipitous fall of the curves as Xm/ax -7 1. Since ax 
.707A for a sine wave. this 
means that the amplitude A becomes greater than Xm = 1 and severe clipping occurs. 
Thus, the SNR decreases rapidly after the amplitude exceeds Xm . 
For analog signals such as speech or music, the distribution of amplitudes tends 
to be concentrated about zero and falls off rapidly with increasing amplitude. In such 
cases, the probability that the magnitude of a sample will exceed three or four times the 
rms value is very low. For example, if the signal amplitude has a Gaussian distribution, 
only 0.064 percent of the samples would have an amplitude greater than 4<rx• Thus, to 
avoid clipping the peaks of the signal (as is assumed in our statistical model), we might 
set the gain of filters and amplifiers preceding the AID converter so that ax = X m/4. 
Using this value of ax in Eq. (4.135) gives 
SNRQ ~ 6B - 1.25 dB. 
(4.137) 
For example, obtaining a SNR of about 90-96 dB for use in high-quality music record­
ing and playback requires 16-bit quantization, but it should be remembered that such 
performance is obtained only if the input signal is carefully matched to the full-scale 
range of the AID converter. 
This trade-off between peak signal amplitude and absolute size ofthe quantization 
noise is fundamental to any quantization process. We will see its importance again in 
Chapter6 when we discuss round-off noise in implementing discrete-time linear systems. 

221 
Section 4.8 
Digital Processing of Analog Signals 
4.8.4 DIA Conversion 
In Section 4.3, we discussed how a bandlimited signal can be reconstructed from a 
sequence of samples using ideallowpass filtering. In terms of Fourier transforms, the 
reconstruction is represented as 
X r(j0.) = X (eJQT )Hr(j0.), 
(4.138) 
where X (eJW ) is the DTFf of the sequence of samples and X r(j0.) is the Fourier 
transform of the reconstructed continuous-time signal. The ideal reconstruction filter is 
H (J'0.) = {T' 10.1 < niT, 
(4.139) 
r 
0, 10.12: niT. 
For this choice of Hr(j0.), the corresponding relation between xr(t) and x[nJ is 
sin[n(t - nT 
T] 
Xr(t) = L 
(4.140)
n(t - nT)IT
n=-oo 
The system that takes the sequence x[n] as input and produces xr(t) as output is called 
the ideal DIC converter. A physically realizable counterpart to the ideal DIe converter 
is a digital-to-analog converter (DIA converter) followed by an analog lowpass filter. As 
depicted in Figure 4.62, a D/A converter takes a sequence of binary code words xB[n] 
as its input and produces a continuous-time output of the form 
XDA(t)= L
00 
XmxB[n]ho(t-nT) 
n=-OO 
00 
(4.141) 
= L x[n]ho(t 
nT), 
n=-oo 
where ho(t) is the impulse response of the zero-order hold given by Eq. (4.122). The 
dotted lines in Figure 4.55 show the output of a DIA converter for the quantized ex­
amples of the sine wave. Note that the D/A converter holds the quantized sample for 
one sample period in the same way that the sample-and-hold holds the unquantized in­
put sample. If we use the additive-noise model to represent the effects of quantization, 
Eq. (4.141) becomes 
00 
00 
XDA(t) = L x[n]ho(t - nT) + L e[n]ho(t - nT), 
(4.142) 
n=-oo 
n=-oo 
To simplify our discussion, we define 
00 
xo(t) 
L x[n]ho(t - nT), 
(4.143) 
n=-oo 
eo(t) 
L
00 
e[n]ho(t - nT), 
(4.144) 
n=-oo 
Figure 4.62 
Block diagram of D/A 
converter. 

222 
Chapter 4 
Sampling of Continuous-Time Signals 
so that Eq. (4.142) can be written as 
XDA (t) = xo(t) + eo{t). 
(4.145) 
The signal component xo(t) is related to the input signal xa(t), since x[n] ::::; xaCnT). The 
noise signal eo(t) depends on the quantization-noise samples ern] in the same way that 
Xo (t) depends on the unquantized signal samples. The Fourier transform of Eq. (4.143) 
is 
X o(jQ) = L
00 
x[n]Ho(jQ)e- jQnT 
n=-oo 
(4.146)
jQTn
C~oo x[n]e-
) HoUQ) 
= X (eJQT)HoUQ). 
Now, since 
jQT
X (e
) = ~ k~OO Xa (j (Q _2;k)), 
(4.147) 
it follows that 
(4.148)
X o(jQ) 
[~ k~OO Xa (j (Q -2;k))]Ho(jQ). 
If Xa(jQ) is bandlimited to frequencies below niT, the shifted copies of Xa(jQ) do 
not overlap in Eq. (4.148), and if we define a compensated reconstruction filter as 
H ( 'Q) = H,(jQ) 
(4.149) 
, } 
Ho(jQ)' 
then the output of the filter will be Xa (t) if the input is xo(t). The frequency response of 
the zero-order-hold filter is easily shown to be 
Ho(jQ) = 2sin(QTI2) e-jQT/2• 
(4.150) 
Q 
Therefore, the compensated reconstruction filter is 
QTI2 
IQI < niT,
Hr(jQ) = 
sin(QTI2) 
(4.151) 
{ 0, 
IQI 2: niT. 
Figure 4.63(a) shows IHoUQ)1 as given by Eq. (4.150), compared with the magnitude 
of the ideal interpolation filter IH,(jQ)1 as given by Eq. (4.139). Both filters have a 
gain of T at Q = 0, but the zero-order-hold, although lowpass in nature, does not 
cut off sharply at Q 
niT. Figure 4.63(b) shows the magnitude of the frequency 
response of the ideal compensated reconstruction filter to be used following a zero­
order-hold reconstruction system such as a D/A converter. The phase response would 
ideally correspond to an advance time shift of T 12 seconds to compensate for the delay 
of that amount introduced by the zero-order hold. Since such a time advance cannot be 
realized in practical real-time approximations to the ideal compensated reconstruction 

223 
Section 4.8 
Digital Processing of Analog Signals 
T 
Figure 4.63 
(a) Frequency response of 
zero-order hold compared with ideal 
'TT. 
interpolating filter. (b) Ideal
T 
compensated reconstruction filter for 
use with a zero-order-hold output. 
filter, only the magnitude response would normally be compensated, and often even 
this compensation is neglected, since the gain of the zero-order hold drops only to 2/n 
(or -4 dB) at n 
niT. 
Figure 4.64 shows a D/A converter followed by an ideal compensated recon­
struction filter. As can be seen from the preceding discussion, with the ideal compen­
sated reconstruction filter following the D/A converter, the reconstructed output sig­
nal would be 
~ 
A 
sin[n(t - nT )/T]
xr(t) = L... x[n]------'-­
n(t-nT)/T
n=-oo 
(4.152) 
~ 
sin[n(t-nT)/T] 
~ .sin[n(t-nT)/T]
L... x[n] 
+ L... e[n] 
.
n(t-nT)/T 
n(t-nT)/T
n=-oo 
n=-oo 
In other words, the output would be 
xr(t) = xa(t) + ea(t), 
(4.153) 
where ea(t) would be a bandlimited white-noise signal. 
Compensated 
reconstruction 
I-----I~ 
filter 
DIA 
converter 
x,(t)
x[n] 
XDA (t) 
iir(jO) 
Figure 4.64 
Physical configuration for 
T 
D/A conversion. 
Zero-order 
hold 
1Houn)1 
Ideal interpolating 
filter Hr(jO) 
T 
1T 
o 
T 
'TT. 
T 
(a) 
IHrUO)1 
1T 
T 
(b) 

224  
Chapter 4 
Sampling of Continuous-Time Signals 
Returning to a consideration of Figure 4.47(b), we are now in a position to un­
derstand the behavior of systems for digital processing of analog signals. If we assume 
that the output of the antialiasing filter is bandlimited to frequencies below T{ / T, that 
Hr(jQ) is similarly bandlimited, and that the discrete-time system is linear and time 
invariant, then the output ofthe overall system will be of the form 
Yr(t) 
Ya(t) + ea(t), 
(4.154) 
where 
TYa(jQ) 
Hr(jQ)HO(jQ)H(ejQT)Haa(JQ)X c(JQ), 
(4.155) 
in which H aa(JQ), Ho(jQ), and H r(jQ) are the frequency responses of the antialiasing 
filter, the zero-order hold of the DIA converter, and the reconstruction lowpass filter, 
respectively. H (eiQT ) is the frequency response of the discrete-time system. Similarly, 
assuming that the quantization noise introduced by the AID converter is white noise 
with variance a} = ~2/12, it can be shown thatthe power spectrum ofthe output noise is 
Pen (jQ) = IHr (jQ)Ho(jQ)H(eiQT )12a}, 
(4.156) 
i.e., the input quantization noise is changed by the successive stages of discrete- and 
continuous-time filtering. From Eq. (4.155), it follows that, under the model for the 
quantization error and the assumption of negligible aliasing, the overall effective fre­
quency response from xc(t) to Yr(t) is 
T H eff(jQ) = ifr(jQ)Ho(jQ)H (eJQT)Haa(jQ). 
(4.157) 
If the antialiasing filter is ideal, as in Eq. (4.118), and if the compensation of the re­
construction filter is ideal, as in Eq. (4.151), then the effective frequency response is 
as given in Eq. (4.119). Otherwise Eq. (4.157) provides a reasonable model for the ef­
fective response. Note that Eq. (4.157) suggests that compensation for imperfections in 
any of the four terms can, in principle, be included in any of the other terms; e.g., the 
discrete-time system can include appropriate compensation for the anti aliasing filter or 
the zero-order hold or the reconstruction filter or all of these. 
In addition to the filtering supplied by Eq. (4.157), Eq. (4.154) reminds us that 
the output will also be contaminated by the filtered quantization noise. In Chapter 6 we 
will see that noise can be introduced as well in the implementation of the discrete-time 
linear system. This internal noise will, in general, be filtered by parts of the discrete­
time system implementation, by the zero-order hold of the DIA converter, and by the 
reconstruction filter. 
4.9  OVERSAMPLING AND NOISE SHAPING IN NO AND 
D/A CONVERSION 
In Section 4.8.1, we showed that oversampling can make it possible to implement sharp­
cutoff antialiasing filtering by incorporating digital filtering and decimation. As we dis­
cuss in Section 4.9.1, oversampling and subsequent discrete-time filtering and down­
sampling also permit an increase in the step size ~ of the quantizer or, equivalently, 
a reduction in the number of bits required in the AID conversion. In Section 4.9.2 we 
show how the step size can be reduced even further by using oversampling together 
with quantization-noise feedback, and in Section 4.9.3 we show how the oversampling 
principle can be applied in DIA conversion. 

225 
Section 4.9 
Oversampling and Noise Shaping in AID and D/A Conversion 
AID conversion  
rate conversion 
-----
--I 
r-----~  
1 
I 
I xAn]
I 
I 
I 
T  
I 
________ J 
Figure 4.65 
Oversampled AID conversion with simple quantization and down­
sampling. 
4.9.1  Oversampled NO Conversion with Direct 
Quantization 
To explore the relation between oversampling and the quantization step size, we con­
sider the system in Figure 4.65. To analyze the effect of oversampling in this system, we 
consider xa(t) to be a zero-mean, wide-sense-stationary, random process with power­
spectral density denoted by <:PXaXa un) and autocorrelation function denoted by 4>x a x a (1'). 
To simplify our discussion, we assume initially that xa(t) is already bandlimited to nN, 
Le., 
(4.158)
<:PXaXa un) = 0, 
and we assume that 2n/ T = 2MnN. The constant M, which is assumed to be an integer, 
is called the oversampling ratio. Using the additive noise model discussed in detail 
in Section 4.8.3, we can replace Figure 4.65 by Figure 4.66. The decimation filter in 
Figure 4.66 is an ideal lowpass filter with unity gain and cutoff frequency We = n / M. 
Because the entire system of Figure 4.66 is linear, its output xd[n] has two components, 
one due to the signal input xa(t) and one due to the quantization noise input ern]. We 
denote these components by xddn] and Xde[n], respectively. 
Our goal is to determine the ratio of signal power £' {x;L [n]} to quantization-noise 
power £'{xJe(n]} in the output xd[n] as a function of the quantizer step size ~ and the 
oversampling ratio M. Since the system of Figure 4.66 is linear, and since the noise is 
assumed to be uncorrelated with the signal, we can treat the two sources separately in 
computing the respective powers of the signal and noise components at the output. 
First, we will consider the signal component of the output. We begin by relating 
the power spectral density, autocorrelation function, and signal power of the sampled 
signal x[n] to the corresponding functions for the continuous-time analog signal xa{t). 
ern] 
~ 
l <[nl=1 LPF H
'.<,) y ~W'='IM. 
1M
• 
Figure 4.66 System of Figure 4.65 with quantizer replaced by linear noise model. 

226 
Chapter 4 
Sampling of Continuous-Time Signals 
Let ¢xAm] and <l>xx (ejW) respectively denote the autocorrelation and power spectral 
density of x[nl. Then, by definition, ¢xx[m] 
£{x[n +m)x[n]}, and since x[n] = xaCnT) 
and x[n + ml = xa(nT + mT), 
£{x[n + m]x[n]} = £{xa«n + m)T )xaCnT)}. 
( 4.159) 
Therefore, 
¢xx[ml 
¢xaxa(mT); 
(4.160) 
the autocorrelation function of the sequence of samples is a sampled version of the 
autocorrelation function of the corresponding continuous-time signal. In particular the 
wide-sense-stationarity assumption implies that £ (x~ (t)} is a constant independent oft. 
It then follows that 
£{x2 [n]} = £(x;(nT)} = £(x~(t)} 
for all n or t. 
(4.161) 
Since the power spectral densities are the Fourier transforms of the autocorrelation 
functions, as a consequence of Eq. (4.160), 
<l>xAejnT) = 1 ~ 
<I> 
[ . (n _2TCk)]
T ~ ~~ i 
•• 
. 
(4.162) 
k=-oo 
T 
Assuming that the input is bandlimited as in Eq. (4.158), and assuming oversampling 
by a factor of M so that 2TCIT = 2MQN, we obtain, by substituting Q = wi T into 
Eq. (4.162) 
. 
-<I> 
X 
.
I 
( W) 
Iwl < TCIM,
<l>xx(e1W) = 
T 
Xa
" iT ' 
(4.163)
{ 0, 
TCIM < (V:S: TC. 
For example, if <l>xaxa (jQ) is as depicted in Figure 4.67(a), and if we choose the sampling 
rate to be 2TC IT 
2MQN, then 4>xx(e jCtJ ) will be as depicted in Figure 4.67(b). 
It is instructive to demonstrate that Eq. (4.161) is true by utilizing the power 
spectrum. The total power of the original analog signal is given by 
1 InN
£{x;(t)} 
-
<l>xaxa(jQ)dQ.
2TC_nN 
<Pu(e1W)
£0) 
fiNM 
-fiN 
!IN 
fi 
-'Tr 
-'TrIM 
'TrIM 
'Tr 
III 
(a) 
(b) 
Figure 4.67 
Illustration of frequency and amplitude scaling between <Pxaxa(jQ) 
and <pxx(eJW ). 

227 
Section 4.9 
Oversampling and Noise Shaping in AID and DIA Conversion 
From Eq. (4.163), the total power of the sampled signal is 
<.' 
2 
1 fJr 
jw
c.{x [n]} 
2Jr  -Jr <l>xAe 
)dw 
(4.164) 
1 fJrIM 1 
( w 
-<I>xaxa j -) dw. 
(4.165)
2Jr -JrjM T 
T 
Using the fact that QN T = JrI M and making the substitution Q 
wiT in Eq. (4.165) 
gives 
£{x2[nH = 21 fnN <l>xaxa(jQ)dQ = £{X;(t)}. 
Jr -nN 
Thus, the total power of the sampled signal and the total power of the original analog 
signal are exactly the same as was also shown in Eq. (4.161). Since the decimation filteris 
an ideallowpass filter with cutoff We 
Jr1M, the signal x [n] passes unaltered through the 
filter. Therefore, the downsampled signal component at the output, Xda[n] = x[nM] = 
Xa (nMT), also has the same total power. This can be seen from the power spectrum by 
noting that, since <l>xxCe jW ) is bandlimited to Iwl < Jrl M, 
M-l 
<I> 
(ejW ) = 1 "\' <I> 
(e j (w-27rk)/M)
XdaXda 
M  L.... 
xx 
k=O 
Iwl <Jr. 
(4.166) 
Using Eq. (4.166), we obtain 
1 fJrIM 
= -2 
<l>xx(ejW)dw = £{x2[n]}.
Jr 
-Jr/M 
which shows that the power of the signal component stays the same as it traverses the 
entire system from the input xa(t) to the corresponding output component Xda[n]. In 
terms of the power spectrum, this occurs because, for each scaling of the frequency axis 
that results from sampling, we have a counterbalancing inverse scaling of the amplitude, 
so that the area under the power spectrum remains the same as we go from <l>xaxa(jQ) 
to <l>xx(e jW ) to <l>xdaxda (e jW ) by sampling. 
Now let us consider the noise component that is generated by quantization. Ac­
cording to the model in Section 4.8.3, we assume that ern] is a wide-sense-stationary 
white-noise process with zero mean and varianceS 
A2 
0'2=_ 
e 
12 
5Since the random process has zero mean, the average power and the variance are the same. 

228 
Chapter 4 
Sampling of Continuous-Time Signals 
ONM 
<f>xx( eiUJ)
-=-­
Figure 4.68 
Power spectral density of 
signal and quantization noise with an 
-7T 
-7TIM 
7TIM 
7T 
w 
oversampling factor of M. 
Consequently, the autocorrelation function and power density spectrum for ern] are, 
respectively, 
tPee[m] 
o-;8[m] 
(4.167) 
and 
<l>ee(eiw ) = 0'2e 
Iwl <no 
(4.168) 
In Figure 4.68, we show the power density spectrum of ern] and of x[n]. The power 
density spectrum of the quantized signal x[n] is the sum of these, since the signal and 
quantization-noise samples are assumed to be uncorrelated in our model. 
Although we have shown that the power in either x[n] or ern] does not depend 
on M, we note that as the oversampling ratio M increases, less of the quantization­
noise spectrum overlaps with the signal spectrum. It is this effect of the oversampling 
that lets us improve the signal-to-quantization-noise ratio by sampling-rate reduction. 
Specifically, the ideallowpass filter removes the quantization noise in the band n / M < 
Iwl :::: n, while it leaves the signal component unaltered. The noise power at the output 
of the ideallowpass filter is 
1 fIT/M
e{e2[n]} = -
O';dw = 
. 
2n -IT/M 
M 
Next, the lowpass filtered signal is downsampled, and, as we have seen, the signal power 
in the downsampled output remains the same. In Figure 4.69, we show the resulting 
power density spectrum of both Xda [n] and xdelnJ. Comparing Figures 4.68 and 4.69, we 
can see that the area under the power density spectrum for the signal has not changed, 
since the frequency axis and amplitude axis scaling have been inverses of each other. 
On the other hand, the noise power in the decimated output is the same as at the output 
of the lowpass filter; i.e., 
1 fIT 0'2 
t,.2 
(4.169)
e{x~e[n]} = 2n 
-IT ~dw = M = 12M' 
Thus, the quantization-noise power e{X~e [n]} has been reduced by a factor of M through 
the filtering and downsampling, while the signal power has remained the same. 
Figure 4.69 Power spectral density of 
signal and quantization noise after 
-7T 
7T 
w 
downsampling. 
ON 
<f>XdaXda ( eiw) 

229 
Section 4.9 
Oversampling and Noise Shaping in AID and D/A Conversion 
From Eq. (4.169), we see that for a given quantization noise power, there is a 
clear trade-off between the oversampling factor M and the quantizer step size t.. Equa­
tion (4.125) states that for a quantizer with (B + 1) bits and maximum input signal level 
between plus and minus X m, the step size is 
t. 
Xm/2B, 
and therefore, 
(4.170)  
Equation (4.170) shows that for a fixed quantizer, the noise power can be decreased 
by increasing the oversampling ratio M. Since the signal power is independent of M, 
increasing M will increase the signal-to-quantization-noise ratio. Alternatively, for a 
fixed quantization noise power Pde 
£{xJe[n]}, the required value for B is 
1 
1 
1 
B = 
2 log2 M 
2:log212 
2 log2 Pde + log2 X m. 
(4.171) 
From Eq. (4.171), we see that for every doubling of the oversampling ratio M, we need 
1/2 bit less to achieve a given signal-to-quantization-noise ratio, or, in other words, if 
we oversample by a factor M 
4, we need one less bit to achieve a desired accuracy in 
representing the signal. 
4.9.2 Oversampled.AJD Conversion with Noise Shaping 
In the previous section, we showed that oversampling and decimation can improve the 
signal-to-quantization-noise ratio. This seems to be a somewhat remarkable result. It 
implies that we can, in principle, use very crude quantization in our initial sampling of 
the signal, and if the oversampling ratio is high enough, we can still obtain an accurate 
representation of the original samples by doing digital computation on the noisy sam­
ples. The problem with what we have seen so far is that, to make a significant reduction 
in the required number of bits, we need very large oversampling ratios. For example, to 
reduce the number of bits from 16 to 12 would require M = 44 
256. This seems to be 
a rather high cost. However, the basic oversampling principle can lead to much higher 
gains if we combine it with the concept of noise spectrum shaping by feedback. 
As was indicated in Figure 4.68, with direct quantization the power density spec­
trum of the quantization noise is constant over the entire frequency band. The basic 
concept in noise shaping is to modify the AID conversion procedure so that the power 
density spectrum of the quantization noise is no longer uniform, but rather, is shaped 
such that most of the noise power is outside the band Iwl < "!i/M. In that way, the 
subsequent filtering and downsampling removes more of the quantization-noise power. 
The noise-shaping quantizer, generally referred to as a sampled-data Delta-Sigma 
modulator, is shown in Figure 4.70. (See Candy and Temes, 1992 and Schreier and Temes, 
2005.) Figure 4.70( a) shows a block diagram of how the system is implemented with inte­
grated circuits. The integrator is a switched-capacitor discrete-time integrator. The AID 
converter can be implemented in many ways, but generally, it is a simple I-bit quantizer 
or comparator. The DIA converter converts the digital output back to an analog pulse 
that is subtracted from the input signal at the input to the integrator. This system can 

230 
Chapter 4 
Sampling of Continuous-Time Signals 
~sa~ 
AJD
+ 
data 
converter 
xa(t) 
_ 
integrator 
I 
y[n] 
r-
DIA 
converter 
(a) 
tM
~
We:O 7TIM 
xdcfn] + Xde[n) 
(b) 
Figure 4.70 
Oversampled quantizer with noise shaping. 
be represented by the discrete-time equivalent system shown in Figure 4.70(b). The 
switched-capacitor integrator is represented by an accumulator system, and the delay 
in the feedback path represents the delay introduced by the D/A converter. 
As before, we model the quantization error as an additive noise source so that the 
system in Figure 4.70 can be replaced by the linear model in Figure 4.71. In this system, 
the output y[n] is the sum of two components: YAn] due to the input x[n] alone and ern] 
due to the noise ern] alone. 
We denote the transferfunction from x[n] to y[n] as HAz) and from e[n] to y[n] 
as H e(Z). These transfer functions can both be calculated in a straightforward manner 
and are 
HAz) 
1, 
(4.172a) 
He(z) 
(1- z-l). 
(4.172b) 
ern) 
-
xa(t) 
xd[n) 
y[n) 
LPF 
We:O 'TrIM 
tM 
Figure 4.71 
System of Figure 4.70 from xa(f) to xd[n] with quantizer replaced by 
alinear noise model. 

4.9 
Oversampling and Noise Shaping in AID and D/A Conversion 
231 
e[n] 
Figure 4.72 
Equivalent representation of Figure 4.71. 
Consequently, 
yx[n] 
x[n], 
( 4.173a) 
and 
ern] 
ern] ­ ern -1]. 
(4.173b) 
Therefore, the output y[n] can be represented equivalently as y[n] = x[n] + ern], where 
x[n] appears unmodified atthe output and the quantization noise ern] is modified by the 
first-difference operator He (z). This is depicted in the block diagram in Figure 4.72. With 
the power density spectrum for ern] given by Eq. (4.168), the power density spectrum 
of the quantization noise ern] that is present in y[n] is 
4>eeCeJW ) = o';IHe (e JW )12 
(4.174) 
= o';[2sin(w/2)p. 
In Figure 4.73, we show the power density spectrum of ern], the power spectrum ofe[n], 
and the same signal power spectrum that was shown in Figure 4.67(b) and Figure 4.68. 
It is interesting to observe that the total noise power is increased from £{e2[nlJ = 0; 
o 
.E. 
M 
-'IT 
1r 
'IT 
W 
M 
Figure 4.73 The power spectral density of the quantization noise and the signal. 

232 
Chapter 4 
Sampling of Continuous-Time Signals 
l=~~ 
'IT 
<t> 
('W 
4u 
2 
/ 
xdexde eJ ) = _~ . 2
----_L 
M SIn (wl(2M» 
--­
-11" 
o 
11" 
W 
Figure 4.74 
Power spectral density of the signal and Quantization noise after downsampling. 
at the quantizer to £{e2 [n]} = 2o} at the output of the noise-shaping system. However, 
note that in comparison with Figure 4.68, the quantization noise has been shaped in 
such a way that more of the noise power is outside the signal band Iwl < JrIM than in 
the direct oversampled case, where the noise spectrum is flat. 
In the system of Figure 4.70, this out-of-band noise power is removed by the low­
pass filter. Specifically, in Figure 4.74 we show the power density spectrum of <f>XdaXda (e jW) 
superimposed on the power density spectrum of <f>xdeXde(ejW ). Since the downsampler 
does not remove any of the signal power, the signal power in Xda£n] is 
Pda = £{xJa[n]} 
£{x2[n]} 
£ {x; (t)}. 
The quantization-noise power in the final output is 
1 jTr 
. 
1 
/':,.2 jTr ( ( w ))2
Pde 
-2 
<f>XdeXde(eJW)dw = 2-12 
2sin -2 
dw. 
(4.175) 
Jr 
-Tr 
Jr 
M 
-Tr 
M 
To compare this approximately with the results in Section 4.9.1, assume that M is suffi­
ciently large so that 
.(W) 
(j) 
sm 2M 
R:! 2M' 
With this approximation, Eq. (4.175) is easily evaluated to obtain 
1 /':,.2Jr2 
Pde 
(4.176)
36M3' 
From Eq. (4.176), we see again a trade-off between the oversampling ratio M and 
the quantizer step size t:... For a (B + I)-bit quantizer and maximum input signal level 
between plus and minus X m, t:.. = Xm12B. Therefore, to achieve a given quantization­
noise power Pde, we must have 
B = 
3 2 10g2 M + 10g2(Jr/6) - 21 log2 Pde + log2 Xm· 
(4.177) 
Comparing Eq. (4.177) with Eq. (4.171), we see that, whereas with direct quantization 
a doubling of the oversampling ratio M gained 1/2 bit in quantization, the use of noise 
shaping results in a gain of 1.5 bits. 
Table 4.1 gives the equivalent savings in quantizer bits over direct quantization 
with no oversampling (M = 1) for (a) direct quantization with oversampling, as dis­
cussed in Section 4.9.1, and (b) oversampling with noise shaping, as examined in this 
section. 

233 
dis­
this 
Section 4.9 
Oversampling and Noise Shaping in AID and D/A Conversion 
TABLE 4.1 
EQUIVALENT SAVINGS IN 
QUANTIZER BITS RELATIVE TO M = 1 FOR 
DIRECT QUANTIZATION AND 1st-ORDER 
NOISE SHAPING 
Direct 
Noise 
M 
quantization 
shaping 
4 
1 
2.2 
8 
1.5 
3.7 
16 
2 
5.1 
32 
2.5 
6.6 
64 
3 
8.1 
The noise-shaping strategy in Figure 4.70 can be extended by incorporating a 
second stage of accumulation as shown in Figure 4.75. In this case, with the quantizer 
again modeled as an additive noise source ern], it can be shown that 
y[n] = x[n] + ern] 
where, in the two-stage case, ern] is the result of processing the quantization noise ern] 
through the transfer function 
(4.178) 
The corresponding power density spectrum of the quantization noise now present in 
y[n] is 
(4.179) 
with the result that, although the total noise power at the output of the two-stage noise­
shaping system is greater than for the one-stage case, even more of the noise lies outside 
the signal band. More generally, p stages of accumulation and feedback can be used, 
with corresponding noise shaping given by 
¢ee(ejUJ ) = a;[2sin(w/2)]2P . 
(4.180) 
In Table 4.2, we show the equi valent reduction in quantizer bits as a function of the order 
p ofthe noise shaping and the oversampling ratio M. Note that with p = 2 and M = 64, 
we obtain almost 13 bits of increase in accuracy, suggesting that a I-bit quantizer could 
achieve about 14-bit accuracy at the output of the decimator. 
Although multiple feedback loops such as the one shown in Figure 4.75 promise 
greatly increased noise reduction, they are not without problems. Specifically, for large 
values of p, there is an increased potential for instability and oscillations to occur. 
An alternative structure known as multistage noise shaping (MASH) is considered in 
Problem 4.68. 
y[n] 
Figure 4.75 
Oversampled quantizer with 2nd-order noise shaping. 

234  
Chapter 4 
Sampling of Continuous-Time Sigllals 
TABLE 4.2 
REDUCTION IN QUANTIZER 
BITS AS ORDER POF NOISE SHAPING 
Oversampling factor M 
Quantizer 
order p 
4 
8 
16 
32 
64 
0 
1.0 
1.5 
2.0 
2.5 
3.0 
1 
2.2 
3.7 
5.1 
6.6 
8.1 
2 
2.9 
5.4 
7.9 
10.4 
12.9 
3 
3.5 
7.0 
10.5 
14.0 
17.5 
4 
4.1 
8.5 
13.0 
17.5 
22.0 
5 
4.6 
10.0 
15.5 
21.0 
26.5 
4.9.3  Oversampfing and Noise Shaping in D/A 
Conversion 
In Sections 4.9.1 and 4.9.2, we discussed the use of oversampling to simplify the process 
of AID conversion. As we mentioned, the signal is initially oversampled to simplify 
antialias filtering and improve accuracy, but the final output xd[n1 of the AID converter 
is sampled at the Nyquist rate for xa(t). The minimum sampling rate is, of course, highly 
desirable for digital processing or for simply representing the analog signal in digital 
form, as in the CD audio recording system. It is natural to apply the same principles in 
reverse to achieve improvements in the DIA conversion process. 
The basic system, which is the counterpart to Figure 4.65, is shown in Figure 4.76. 
The sequence Yd[n], which is to be converted to a continuous-time signal, is first up­
sampled to produce the sequence y[n], which is then requantized before sending it to 
a D/A converter that accepts binary samples with the number of bits produced by the 
requantization process. We can use a simple D/A converter with few bits if we can be 
assured that the quantization noise does not occupy the signal band. Then the noise can 
be removed by inexpensive analog filtering. 
In Figure 4.77, we show a structure for the quantizer that shapes the quantization 
noise in a similar manner to the 1 st-order noise shaping provided by the system in 
Figure 4.70. In our analysis we assume that Yd[n1 is effectively unquantized or so finely 
quantized relative to y[n] that the primary source of quantizer error is the quantizer in 
Figure 4.76. To analyze the system in Figures 4.76 and 4.77, we replace the quantizer 
in Figure 4.77 by an additive white-noise source ern], so that Figure 4.77 is replaced by 
Figure 4.78. The transfer function from J[n] to y[n] is unity. i.e., the upsampled signal 
Sampling rate increase by M -, 
I 
I 
LPF 
I 
.. 
Gain=M 
I
I
Yd[n] I 
cutoff = 7TIM 
: J[n] '--__--I 
I 
I
I  
____ J 
T 
Figure 4.76 
Oversampled D/A conversion. 

235 
jignals 
Section 4.9 
Irocess 
mplify 
Iverter 
highly 
digital 
pIes in 
Oversampling and Noise Shaping in AID and D/A Conversion 
y[nJ 
Figure 4.71 1st.order noise-shaping 
system for oversampled D/A 
quantization. 
ern] 
yIn] 
Figure 4.78 System of Figure 4.77 
with quantizer replaced by linear noise 
model. 
Y[n] appears at the output unaltered. The transfer function He(z) from ern] to y[n] is 
He(z) =1 
Therefore, the quantization noise component ern] that appears at the output of the 
noise-shaping system in Figure 4.78 has the power density spectrum 
<l>ee(eiw) = o';(2sinwI2)2, 
(4.181) 
where, again, u; 
6 2/12. 
An illustration of this approach to D/A conversion is given in Figure 4.79. fig­
ure 4.79(a) shows the power spectrum <l>YdYd (eiw) of the input Yd[n] in Figure 4.76. Note 
that we assume that the signal Yd[n] is sampled at the Nyquist rate. Figure 4.79(b) shows 
the corresponding power spectrum at the output of the upsampler (by M), and Fig­
ure 4.79(c) shows the quantization noise spectrum at the output of the quantizerfnoise­
shaper system. Finally, Figure 4.79(d) shows the power spectrum ofthe signal component 
superimposed on the power spectrum of the noise component at the analog output of 
the Die converter of Figure 4.76. In this case, we assume that the Die converter has an 
ideallowpass reconstruction filter with cutoff frequency rrI (MT ), which will remove as 
much of the quantization noise as possible. 
In a practical setting, we would like to avoid sharp-cutoff analog reconstruction 
filters. From Figure 4.79( d), it is clear that if we can tolerate somewhat more quantization 
noise, then the Die reconstruction filter need not roll off so sharply. Furthermore, if we 
use multistage techniques in the noise shaping, we can obtain an output noise spectrum 
of the form 
<l>ee(eiw) = u;(2 sin wI2)2p, 
which would push more of the noise to higher frequencies. In this case, the analog 
reconstruction filter specifications could be relaxed even further. 

Chapter 4 
Sampling of Continuous-Time Signals
236 
<pYdYieiw) 
-"TT 
o 
1T 
w 
(a)E 
i 
") 
-1T 
7T 
o 
7T 
7T 
w 
M 
M 
(b) 
<pee(e iw ) 4o} sin 2 (wl2)
- ~~--­
-7T 
o 
7T 
w 
(c) 
<pYaY.Ufi) 
~'(eiw1) 40-2 Tsin2 (fiT!2)
/ 
ee 
e 
o 
7T 
fi 
MT 
MT 
(d) 
Figure 4.79 
(a) Power spectral density of signal Yd[n]. (b) Power spectral density 
of signal Y[n]. (c) Power spectral density of quantization noise. (d) Power spectral 
density of the continuous-time signal and the quantization noise. 
4.10 SUMMARY 
In this chapter, we developed and explored the relationship between continuous-time 
signals and the discrete-time sequences obtained by periodic sampling. The fundamen­
tal theorem that allows the continuous-time signal to be represented by a sequence of 
samples is the Nyquist-Shannon theorem, which states that, for a bandlimited signal, 
periodic samples are a sufficient representation, as long as the sampling rate is suffi­
ciently high relative to the highest frequency in the continuous-time signal. Under this 
condition, the continuous-time signal can be reconstructed by lowpass filtering from 
knowledge of only the original bandwidth, the sampling rate and the samples. This cor­
responds to bandlimited interpolation. If the sampling rate is too low relative to the 
..  

237 
Chapter 4 
Problems 
bandwidth of the signal, then aliasing distortion occurs and the original signal cannot 
be reconstructed by bandlimited interpolation. 
The ability to represent signals by sampling permits the discrete-time processing of 
continuous-time signals. This is accomplished by first sampling, then applying discrete­
time processing, and, finally, reconstructing a continuous-time signal from the result. 
Examples given were lowpass filtering and differentiation. 
Sampling rate changes are an important class of digital signal processing oper­
ations. Downsampling a discrete-time signal corresponds in the frequency domain to 
an amplitude-scaled replication of the discrete-time spectrum and rescaling of the fre­
quency axis, which may require additional bandlimiting to avoid aliasing. Upsampling 
corresponds to effectively increasing the sampling rate and is also represented in the fre­
quency domain by a rescaling of the frequency axis. By combining upsampling and down­
sampling by integer amounts, noninteger sampling rate conversion can be achieved. We 
also showed how this can be efficiently done using multirate techniques. 
In the final sections of the chapter, we explored a number of practical considera­
tions associated with the discrete-time processing of continuous-time signals, including 
the use of prefiltering to avoid aliasing, quantization error in AID conversion, and some 
issues associated with the filtering used in sampling and reconstructing the continuous­
time signals. Finally, we showed how discrete-time decimation and interpolation and 
noise shaping can be used to simplify the analog side of AID and DIA conversion. 
The focus of this chapter has been on periodic sampling as a process for obtaining 
a discrete representation of a continuous-time signal. While such representations are by 
far the most common and are the basis for almost all of the topics to be discussed in the 
remainder of this text, there are other approaches to obtaining discrete representations 
that may lead to more compact representations for signals where other information 
(besides bandwidth) is known about the signal. Some examples can be found in Unser 
(2000). 
Problems 
Basic Problems with Answers 
4.1. The signal 
xc(t) = sin (2n(100)t) 
was sampled with sampling period T = 1/400 second to obtain a discrete-time signal x[n]. 
What is the resulting sequence x[n]? 
4.2. The sequence 
x[n] = cos (~n), 
-00 < n < 00, 
was obtained by sampling the continuous-time signal 
xc(t) = cos (Qat), 
-00 < t < 00, 
at a sampling rate of 1000 samples/s. What are two possible positive values of QO that could 
have resulted in the sequence x[n]? 

238  
Chapter 4 
Sampling of Continuous-Time Signals 
4.3.  The continuous-time signal 
Xe(t) = cos (4000rrt) 
is sampled with a sampling period T to obtain the discrete-time signal 
x[n] = cos ( lT3n) . 
(a) Determine a choice for T consistent with this information. 
(b)  Is your choice for T in part (a) unique? Ifso, explain why. Ifnot, specify another choice 
of T consistent with the information given. 
4A. The continuous-time signal 
Xe(t) = sin (20lTt) + cos (40lTt) 
is sampled with a sampling period T to obtain the discrete-time signal 
. (lTn) 
(2lTn)
x[n] = sm ""5 + cos -5-
. 
(a)  Determine a choice for T consistent with this infoimation. 
(b)  Is your choice for T in part (a) unique? Ifso, explain why. Ifnot, specify another choice 
of T consistent with the information given. 
4.5.  Consider the system of Figure 4.10, with the discrete-time system an ideallowpass filter 
with cutoff frequency IT/8 radians/so 
(a)  Ifxe(t) is bandlimited to 5kHz, what is the maximum value of T that will avoid aliasing 
in the CID converter? 
(b)  If liT = 10 kHz, what will the cutoff frequency of the effective continuous-time filter 
be? 
(c)  Repeat part (b) for liT 
20 kHz. 
4.6.  Let he (t) denote the impulse response ofan LTI continuous-time filter and hd [n] the impulse 
response of an LTI discrete-time filter. 
(a)  If 
-at 
0
he(t) = 
e 
, t 2: ,
{ 0, 
t < 0, 
where a is a positive real constant, determine the continuous-time filter frequency 
response and sketch its magnitude. 
(b)  If hd[n] = The(nT) with he(t) as in part (a), determine the discrete-time filter fre­
quency response and sketch its magnitude. 
(c)  For a given value of a, determine, as a function of T, the minimum magnitude of the 
discrete-time filter frequency response. 
4.7. A simple model ofa multi path communication channel is indicated in Figure P4.7-1. Assume 
that seCt) is bandlimited such that ScUQ) = 0 for IQI 2: IT/ T and that xe(t) is sampled with 
a sampling period T to obtain the sequence 
x[n] = xc(nT). 
.. 
seCt) 
a 
Figure P4.7-1 

239 
Chapter 4 
Problems 
(a)  Determine the Fourier transform of xcCt) and the Fourier transform of x[n] in terms 
of ScCjQ). 
(b)  We want to simulate the multipath system with a discrete-time system by choosing 
H(e jW ) in Figure P4.7-2 so that the output r[n] = xcCnT) when the input is s[n] = 
scCnT). Determine H(ejW ) in terms of T and Td. 
(c) Determine the impulse response h[n] in Figure P4.7 when (i) Td = T and (ii) Td = T /2. 
s[n] =sc(nT) 
Figure P4.7-2 
4.8. Consider the system in Figure P4.8 with the following relations: 
X cUQ) = 0, 
x[n] = xc(nT), 
n 
y[n] = T L x[k]. 
k=-oo 
y[n] 
T  
Figure P4.B 
(a)  For this system, what is the maximum allowable value of T if aliasing is to be avoided, 
i.e., so that xcCt) can be recovered from x[n]. 
(b)  Determine h[n]. 
(c)  In terms of X (e jW ), what is the value of y[n] for n --+ oo? 
(d)  Determine whether there is any value of T for which 
(P4.8-1)
y[n][n=oo = i: xcCt)dt. 
If there is such a value for T, determine the maximum value. Ifthere is not, explain and 
specify how T would be chosen so that the equality in Eq. (P4.8-1) is best approximated. 
4.9.  Consider a stable discrete-time signal x[n] whose discrete-time Fourier transform X (e jW ) 
satisfies the equation 
X (e jW ) = X (ej(w-Jr») 
and has even symmetry, i.e., x[n] = x[-n]. 
(a)  Show that X (e jW ) is periodic with a period Jr. 
(b)  Find the value of x[3]. (Hint: Find values for all odd-indexed points.) 
(c)  Let y[n] be the decimated version of x[n], i.e., y[n] = x[2n]. Can you reconstruct x[n] 
from y[n] for all n. If yes, how? If no, justify your answer. 

240 
Chapter 4 
Sampling of Continuous-Time Signals 
4.10. Each of the following continuous-time signals is used as the input xc(t) for an ideal CID 
converter as shown in Figure 4.1 with the sampling period T specified. In each case, find 
the resulting discrete-time signal x[nj. 
(a)  xc(t) = cos (2n(1000)t) , 
T = (1/3000) sec 
(b)  xc(t) = sin (2n(1000)t) . 
T = (1/1500) sec 
(e)  xcCt) = sin (2n(1000)t) / (m), 
T = (1/5000) sec 
4.11.  The following continuous-time input signals xc(t) and corresponding discrete-time output 
signals x[n] are those of an ideal CID as shown in Figure 4.1. Specify a choice for the 
sampling period T that is consistent with each pair of xcCt) and x[n]. In addition, indicate 
whether your choice of T is unique. If not, specify a second possible choice of T consistent 
with the information given. 
(a)  xc(t) = sin(lOm). 
x[n] 
sin(nn/4) 
(b)  xc(t) = sin(lOm)/(lOm), 
x[nJ = sin(nn/2)/Cnn/2). 
4.U.  In the system of Figure 4.10, assume that 
H(eJW ) = jOJ/T. 
~n::S OJ < n, 
and T = 1/10 sec. 
(a)  For each of the following inputs xe(t), find the corresponding output yc(t). 
(i) xe(t) = cos(6nt). 
(ii)  xc(t) = cos(14nt). 
(b)  Are the outputs Ye (t) those you would expect from a differentia tor? 
4.13.  In the system shown in Figure 4.15, he(t) = .5(t ~ T /2). 
(a)  Suppose the input x[n] = sin(nn/2) and T 
10. Find y[n]. 
(b)  Suppose you use the same xfn] as in part (a), but halve T to be 5. Find the resulting 
y[n]. 
(e)  In general, how does the continuous-time LTI system he(t) limit the range of the 
sampling period T that can be used without changing y[n]? 
4.14. Which of the following signals can be downsampled by a factor of 2 using the system in 
Figure 4.19 without any loss of information? 
(a)  x[n] = .5[n - no], for no some unknown integer 
(b)  x[n] = cos(nn/4) 
(e)  x[nj = cos(nn/4) + cos(3nn/4) 
(d)  x[n] 
sin (nn/3) /(nn/3) 
(e)  x[n] = (-Wsin(nn/3)/(nn/3). 
4.15. Consider the system shown in Figure P4.15. For each of the following input signals x[nJ. 
indicate whether the output xr[n] 
x[nJ. 
(a)  x[nJ 
cos(nn/4) 
(b)  x[nJ 
cos(nn/2) 
(e) 
x[n] = [Sin~:/8)r 
Hint: Use the modulation property of the Fourier transform to find X (eJW ). 

241 
Chapter 4 
Problems 
x[n] 
xr[nJ 
Figure P4.15 
4.16.  Consider the system in Figure 4.29. The input x [n] and corresponding output Xd[n] are given 
for a specific choice of M / L in each of the following parts. Determine a choice for M / L 
based on the information given, and specify whether your choice is unique. 
(a) x[n] 
sin (:rrn/3) /(:rrn/3). xd[n] 
sin (5:rrn/6) /(5:rrn/6) 
(b) x[n] 
cos (3:rrn/4), xd[n] 
cos(:rrn/2). 
4.17.  Each of the following parts lists an input signal x [71] and the upsampling and downsampling 
rates Land M for the system in Figure 4.29. Determine the corresponding output xd[n]. 
(a) x[n] = sin(2:rrn/3)/:rrn, L = 4, M = 3 
(b) x[n] = sin(3:rrn/4), L = 6, M = 7, 
4.18.  For the system shown in Figure 4.29, X (e jW ), the Fourier transform of the input signal 
x[n], is shown in Figure P4.18. For each of the following choices of Land M, specify the 
maximum possible value of wo such that Xd(ejW ) = aX (e jMw/ L ) for some constant a. 
-11' 
-Wo 
tuo 
11' 
Figure P4.18 
(a) M=3, L 
") 
"­
(b) M=5, L 
3 
(c) M=2, L 
3. 
4.19.  The continuous-time signal Xc (t) with the Fourier transform XC UQ) shown in Figure P4.19­
1 is passed through the system shown in Figure P4.19-2. Determine the range of values for 
T for which xr(t) = xAt). 
[71], 
/1
1 
o 
~ flo 
flo 
n 
3 
Figure P4.19-1 
T 
T 
Figure P4.19-2 

242 
Chapter 4 
Sampling of Continuous-Time Signals 
4.20.  Consider the system in Figure 4.10. The input signal Xc (t) has the Fourier transform shown 
in Figure P4.20 with no = 2rr(1000) radians/second. The discrete-time system is an ideal 
lowpass filter with frequency response 
H(eJW) = {1, Iwl < W.C, 
0, otherwise. 
J: 
-no 
Do 
n  Figure P4.20 
(a)  What is the minimum sampling rate Fs = 1( T such that no aliasing occurs in sampling 
the input? 
(b)  If Wc = rr(2, what is the minimum sampling rate such that Yr(t) = xc(t)? 
Basic Problems 
4.21.  Consider a continuous-time signal xc(t) with Fourier transform Xc (jn) shown in Fig­
ure P4.21-1. 
Xijn) 
1
1 
, 
, 
~ n 
-~ 
2
-3 no 
o 
2 
3~ 
no 
Figure P4.21-1 
Fourier transform Xc(}n) 
(a)  A continuous-time signal Xr (t) is obtained through the process shown in Figure P4.21­
2. First, xc(t) is multiplied by an impulse train of period Tl to produce the waveform 
Xs (t), i.e., 
+00 
xs(t) = L x[nl8(t - nTl)' 
n=-oo 
Next, xs(t) is passed through a low pass filter with frequency response Hr(jn). Hr(jn) 
is shown in Figure P4.21-3. 

243 
Ignals  
Chapter 4 
Problems 
~own 
ideal 
Fjgure P4.21-2 
Conversion system for part (a) 
HrOQ) 
npling 
o 
Figure P4.21-3 
Frequency response HrUQ) 
Fig-
Determine the range of values for Tl for which xr(t) 
xc(t). 
(b) Consider the system in Figure P4.21-4. The system in this case is the same as the 
one in part (a), except that the sampling period is now T2. The system H.~UQ) is 
some continuous-time ideal LTI filter. We want xo(t) to be equal to xc(t) for all t, i.e., 
xo(t) = xc(t) for some choice of Hsun). Find all values of T2 for which xo(t) = xc(t) 
is possible. For the largest T2 you determined that would still allow recovery of xcCt), 
choose Hs(jn) so that xo(t) = xc(t). Sketch Hsun). 
n=-!X! 
Fjgure P4.21-4 
Conversion system for part (b) 
4.22.  Suppose that the bandlimited differentiator of Example 4.4 has input xc(t) = cos(Qot) with 
no < 1rIT. In this problem, we wish to verify that the continuous-time signal reconstructed 
from the output of the bandlimited differentiator is indeed the derivative of xc(t). 
(a)  The sampled input will be x[nl = cos(won), where wo 
QoT < 1r. Determine an 
P4.21­
expression for X (eiw) that is valid for Iwl ::: 1r. 
(b)  Now use Eq. (4.46) to determine the DTFI' of Y(eiw ), the output of the discrete-time 
system. 
(c)  From Eq. (4.32) determine Yr un), the continuous-time Fourier transform of the out­
put of the DIC converter. 
(d)  Use the result of (c) to show that 
Yr(t) = -nosin(not) = :t [xcCt)]. 

Chapter 4 
Sampling of Continuous-Time Signals
244 
4.23. Figure P4.23-1 shows a continuous-time filter that is implemented using an LTI discrete-time 
filter ideallowpass filter with frequency response over -Jr :::: W :::: Jr as 
H(ejill) 
!wl <
= {I 
We 
o  We < Iwi :::: Jr. 
(8)  Ifthe continuous-time Fourier transform of xcCt), namely XdjQ), is as shown in Figure 
P4.23-2 and We = 
~, sketch and label X(e jW ), Y(e jW ) and Ye(jQ) for each of the 
following cases: 
(i) 11Tl = IlT2 
2 x 104 
(ii)  IITl = 4 x 104, IlT2 = 104 
(iii)  11Tl = 104, IlT2 = 3 x 104. 
(b)  For 11 Till Tz = 6 x 11)3, and for input signals xc(t) whose spectra are bandlimited 
to IQI < 2Jr x 5 x 103 (but otherwise unconstrained), what is the maximum choice of 
the cutoff frequency We of the filter H(ejW) for which the overall system is LTI? For 
this maximum choice of We, specify Hc(jQ). 
HcUQ) 
1--------­
xc(t) : 
1 Ye(t) 
----t 
t--+­
1 
I 
1 
T2  ____ 
1I 
Figure P4.23-1 
Xc(jQ)
1 . 
-27T X 5 x Ie}! 
27T X 5 X 103 
Q 
Figure P4.23-2 
4.24. Consider the system shown in Figure P4.24-1. 
H(jQ) 
-I 
I 
Anti-Aliasing  
1 
I 
Ye(t)
vet) 
~ 
! 
1 
I 
I 
I 
I 
I 
I
T 
T 
-l
1­
Figure P4.24-1 
The anti-aliasing filter is a continuous-time filter with the frequency response L(jQ) shown 
in Figure P4.24-2 . 
....  

245 
Chapter 4 
Problems 
t L(jn) 
e 
Tr 
o 
.!!.
e 
T 
T  
Figure P4.24·2 
The frequency response of the LTI discrete-time system between the converters is given 
by:
d 
f 
r 
(3) What is the effective continuous-time frequency response ofthe overall system, H (jn)? 
(b) Choose the most accurate statement: 
(i) Yc(t) = !bxc(3t). 
(ii) yc(t) = xe(t 
~). 
(iii) yc(t) 
!bxc(t - 3T). 
(iv) Ye(t) = xe(t -
~). 
(3) Express Ydln] in terms of Yc(t). 
(b) Determine the impulse response h[n1of the discrete-time LTI system. 
4.25.  Tho bandlimited signals, Xl (t) and xz(t), are multiplied, producing the product signal 
wet) 
Xl (t)xz(t). This signal is sampled by a periodic impulse train yielding the signal 
co 
00 
wp(t) = wet) L o(t - nT) = L w(nT)o(t - nT). 
n=-OQ 
n=-OQ 
Assume that Xl (t) is bandlimited to n l • and xz(t) is bandlimited to nz; that is. 
xz(jn) = 0, 
Inl ::: nz. 
Determine the maximum sampling interval T such that wet) is recoverable from wp(t) 
through the use of an ideallowpass filter. 
4.26.  The system of Figure P4.26 is to be used to filter continuous time music signals using a 
sampling rate of 16kHz. 
xc(t) 
Yc(t)
H(ejw) 
10
'it)  
.\ 
CID 
D/C
...  
f H 
H f 
T  
T 
Figure P4.26 
H (e j l1» is an ideallowpass filter with a cutoff of n /2. If the input has been bandlimited 
such that xc(jn) 
0 for Inl > n c • how should nc be chosen so that the overall system in 
own 
Figure P4.26 is LTI? 

246  
Chapter 4 
Sampling of Continuous-Time Signals 
4.27.  The system shown in Figure P4.27 is intended to approximate a differentia tor for bandlim­
ited continuous-time input waveforms. 
HeUrI.) 
------1
I 
I 
T  
I 
1____ - 
T 
~ 
Figure P4.27 
• The continuous-time input signal xc(t) is bandlimited to 10.1 < nM. 
• The C/D converter has sampling rate T =  ~, and produces the signal 
nM 
xd[n] = xc(nT). 
• The discrete-time filter has frequency response 
ejw/2 _ e- jw/2 
Hd(ejW) 
Iwl ~ rr.
T 
• The ideal DIC converter is such that ya[n] = yc(nT). 
(a)  Find the continuous-time frequency response Hcun) of the end-to-end system. 
(b)  Find xd[n], yc(t), and Yd[n], when the input signal is 
Sin(nMt)
xc(t) = 
. 
nMt 
4.28.  Consider the representation of the process of sampling followed by reconstruction shown 
in Figure P4.28. 
00 
set) 
L8(t-nT) 
n=-a:; 
xc(t) : 
----., 
I 
I 
I 
: yc(t) 
t--+­
1 
I 
I 
xit) 
x,.(t) 
Figure P4.28 
Assume that the input signal is 
xc(t) == 2cos(100rrt 
n/4) + cos(300nt +n/3) 
- 00 < t < 00 
The frequency response of the reconstruction filter is 
10.1 ~ n/T
Hrun) 
[~ Inl>n/T 
(a) Determine the continuous-time Fourier transform Xcun) and plot it as a function of 
n. 
(b)  Assume that Is 
l/T == 500 samples/sec and plot the Fourier transform Xsun) as 
a function of n for -2n/T ~ n ~ 2n/T. What is the output xr(t) in this case? (You 
should be able to give an exact equation for xr(t).) 
(c) Now, assume that Is = 1/T = 250 samples/sec. Repeat part (b) for this condition . 
..  

247 
Chapter 4 
Problems 
(d) Is it possible to choose the sampling rate so that  
Xr(t) = A + 2 cos(100rrt - rr/4)  
where A is a constant? If so, what is the sampling rate Is 
l/T, and what is the 
numerical value of A? 
4.29. In Figure P4.29, assume that Xc{jQ) = 0, IQI ::: rrlTl. For the general case in which 
T 1 1= T2 in the system, express Yc (t) in terms of Xc (t). Is the basic relationship different for 
T1 > T2 and T1 < T 2? 
Figure P4.29 
4.30.  In the system of Figure P4.30, X c(jQ) and H(eiw) are as shown. Sketch and label the 
Fourier transform of Yc (t) for each of the following cases: 
(0) 1/T1 = 1/T2 = 104 
(b) 1/T1 = 1/T2 
2 x 104 
(c) 1/T1 
2 x 104, 
1/T2 
104 
(d) 1/T1 = 104 , 
1/T2 = 2 x 104. 
yc(t) 
Tl  
T2 
b
-27T X 5 x UP 
27T x 5 X 103 
r'~) 
-7r  
7T 
W
~  
~ 
2 
2  
Figure P4.30 

248  
Chapter 4 
Sampling of Continuous-Time Signals 
4.31.  Figure P4.31-1 shows the overall system for filtering a continuous-time signal using a 
discrete-time filter. The frequency responses of the reconstruction filter Hr(jQ) and the 
discrete-time filter H(ejcj) are shown in Figure P4.31-2. 
CID  
DIC 
r---­
s(t) 
oCt nT)  
I 
I 
n  
I  
I 
Convert from 
Ideal 
impulse train 
I 
Convert to 
impulse 
train 
~ reconstruction I
I
H(eiw) 
filter 
1--;"-'" 
xc(t) : 
xs(t) 
to discrete-time 
I 
y,(t) 
I y,(t)
y[nll
sequence 
H,(jO) 
I
I 
I 
I
I 
1___________ _ 
__ J 
Figure P4.31-1 
H,(jO) 
5 x 1O~5  
II H(,i") I 
c 
I 
21T X 104 
0  
1T 
1T 
W 
4 
4 
Figure P4.31-2 
(a) For X djQ) as shown in Figure P4.31-3 and liT 
20kHz, sketch X s(jQ) and X (e}W). 
XcUO) 
1 
-21T X 104 
21T X 104  0 
Figure P4.31-3 
For a certain range of values of T, the overall system, with input xc(t) and output Ye(t), 
is equivalent to a continuous-time lowpass filter with frequency response H eff(jQ) 
sketched in Figure P4.31-4. 
Heff(jO) 
1 
-Oe 
Oe 
o 
Figure P4.31-4 
(b)  Determine the range of values of T for which the information presented in (a) is true 
when X c(jQ) is bandlimited to IQI ~ 21l' x 104 as shown in Figure P4.31-3. 
(c)  For the range of values determined in (b), sketch Qc as a function of liT. 
Note: This is one way of implementing a variable-cutoff continuous-time filter using fixed 
continuous-time and discrete-time filters and a variable sampling rate. 

249 
Chapter 4 
Problems 
4.32. Consider the discrete-time system shown in Figure P4.32-1 
Fjgure P4.32-1 
where 
(i) Land M are positive integers . 
... 
v,(t)  
'1') 
{x[nILl n=kL, 
kisanyinteger
( 1
xe[n] =  
. 
o 
otherwise. 
(iii)  y[nl = Ye[nM]. 
. 
{M 
Iwl::::: "-4
(iv) H(eJf.JJ) = 0 
%< Iwl ::::: IT • 
(a)  Assume that L = 2 and M = 4, and that X(ejf.JJ), the DTFf of x[n], is real and is as 
shown in Figure P4.32-2. Make an appropriately labeled sketch of Xe(ejf.JJ), ye(ejf.JJ), 
and Y(ejf.JJ), the DTFfs of xe[n], Ye[n], and y[n], respectively. Be sure to clearly label 
salient amplitudes and frequencies. 
1 
----+----r--~--~r---+---~f.JJ 
-7T 
-7T/2 
7T/2 
7T 
Fjgure P4.32-2 
(b)  Now assume L = 2 and M = 8. Determine y[n] in this case. 
Hint: See which diagrams in your answer to part (a) change. 
4.33. For the system shown in Figure P4.33, find an expression for y[n] in terms of x[n]. Simplify 
the expression as much as possible.  
),  
Q) 
Advanced Problems 
4.34. In the system shown in Figure P4.34, the individual blocks are defined as indicated. 
s(t) 
true 
Figure P4.33 
fixed  
Figure P4.34 

250  
Chapter 4 
Sampling of Continuous-Time Signals 
1,  
Inl < n . 10-3 rad/sec
H(jn): 
H(jn) = { 0, 
Inl > n . 10-3 rad/sec 
00 
System A: 
Yc(t) = L xd[k]hl (t 
kTl) 
k=-oo 
Second CID: 
Yd[n] 
Yc(nT) 
(a)  Specify a choice for T, Tl, and hI (t) so that yc(t) and Xc (t) are guaranteed to be equal 
for any choice of s(t). 
(b)  State whether your choice in (a) is unique or whether there are other choices for T, 
T}, and hI (t) that will guarantee that Yc(t) and xcU) are equal. As usual, clearly show 
your reasoning. 
(c)  For this part, we are interested in what is often referred to as consistent resampling. 
Specifically, the system A constructs a continuous-time signal Yc(t) from xd[n] the 
sequence of samples of xc(t) and is then resampled to obtain Yd[nj. The resampling is 
referred to as consistent if Yd[n] = xd[n]. Determine the most general conditions you 
can on T, Tl, and hI (t) so that Yd[n) = xd[n]. 
4.35.  Consider the system shown in Figure P4.35-1. 
For parts (a) and (b)oruy, Xe(jn) ofor Inl > 2n x 103 and H(e jW ) is as shown in Figure 
P4.35-2 (and of course periodically repeats). 
(a)  Determine the most general condition on T, if any, so that the overall continuous-time 
system from xe(t) to yc(t) is LTI. 
(b)  Sketch and clearly label the overall equivalent continuous-timc frequency response 
Heff(jn) that results when the condition determined in (a) holds. 
(c)  For this part only assume that xc(jn) in Figure P4.35-1 is bandlimited to avoid alias­
ing, i.e., xc(jn) 
0 for Inl ;:: T' For a general sampling period T, we would like to 
choose the system H (ejW) in Figure P4.35-1 so that the overall continuous-time system 
from xe(t) to Yc(t) is LTI for any input xc(t) bandlimited as above. Determine the most 
general conditions on H(ejW), if any, so that the overall cr system is LTI. Assum­
ing that these conditions hold, also specify in terms of H(ejW ) the overall equivalent 
continuous-time frequency response Heff(jn). 
Yc(t) 
T  
T 
Figure P4.35-1 
H(ejhJ) 
A 
------~--------r_-------L------~~ W 
11' 
'IT 
4  
4 
Figure P4.35-2 
III. 

251 
Chapter 4 
Problems 
4.36. We have a discrete-time signal, x[n], arriving from a source at a rate of *samples per 
second. We want to digitally res ample it to create a signal y[n] that has *samples per 
second, where T2 = ~ Tl. 
(a)  Draw a block diagram of a discrete-time system to perform the resampling. Specify 
the input/output relationship for all the boxes in the Fourier domain. 
. 
. 
I 
• 
n = 0 
d 
.
{I,  
[]
(b)  For an mput signa x[n] = o[n] = 
. 
etermme y n . 
0,  otherwise, 
4.37. Consider the decimation filter structure shown in Figure P4.37-l: 
Figure P4.37-1 
e 
where Yo[n] and Yl [n] are generated according to the following difference equations: 
e 
1 
1 
I 
Yo[n] = 4yo[n -1] -
3xo[n] + sxo[n 
1] 
e 
1 
1 
Yl[n] = 4Y1[n 
1] + 12x1[n]
s­
to 
m 
(a) How many multiplies per output sample does the implementation of the filter structure 
8t 
require? Consider a divide to be equivalent to a multiply. 
nt  
The decimation filter can also be implemented as shown in Figure P4.37-2, 
~
H(z) ~+2 ~ 
~ II r---
Figure P4.37-2 
where v[n] = av[n - 1] + bx[n] + cx[n - 1]. 
(b)  Determine a, b, and c. 
(c)  How many multiplies per output sample does this second implementation require? 
4.38. Consider the two systems of Figure P4.38. 
(a)  For M 
2, L = 3, and any arbitrary x[nJ, will YArn] = YB[n]? If your answer is yes, 
justify your answer. Ifyour answer is no, clearly explain or give a counterexample. 
(b)  How must M and L be related to guarantee YArn] 
YB[n] for arbitrary x[n]? 
System A: 
SystemB: ~I tL ~I +M ~ 
~ !l r---
Figure P4.38 

252  
Chapter 4 
Sampling of Continuous-Time Signals 
4.39. In system A, a continuous-time signal xc(t) is processed as indicated in Figure P4.39-1. 
System A 
1-----­
~ 
1
1 
1 
xc(t) 
yC<t) 1
11 
-7r/c 
7r/c 
2T
T= 1 
Ie 
Figure P4.39-1 
(a)  If M = 2 and xc(t) has the Fourier transform shown in Figure P4.39-2, determine y[nl. 
Clearly show your work on this part. 
Xe(iil) 
1 
, 
" ) 0 il 
-27r/c  
27r/e 
Figure P4.39-2 
We would now like to modify system A by appropriately placing additional processing 
modules in the cascade chain of system A (i.e., block~can be added at any point in the 
cascade chain-at the beginning, at the end, or evenin between existing blocks). All of the 
current blocks in system A must be kept. We would like the modified system to be an ideal 
LTI lowpass filter, as indicated in Figure P4.39-3. 
~ 
Hun) ~
Figure P4.39-3 
2n:jc
1 10.1 < ---r 
H(jn) = { 0 otherwise 
We have available an unlimited number of the six modules specified in the table given in 
Figure P4.39-4. The per unit cost for each module is indicated, and we would like the final 
cost to be as low as possible. Note that the ole converter is runlling at a rate of "2T". 
(b)  Design the lowest-cost modified system if M 
2 in System A. Specify the parameters 
for all the modules used. 
(c)  Design the lowest-cost modified system if M = 4 in System A. Specify the parameters 
for all the modules used. 

253 
Chapter 4 
Problems 
I x(t)~x[n] 
. 
+ 
T 
x[n]  ~X(t) 
T 
x[n1--.j ~ 
~Ylnl 
,..orr/T 
-rrIT 
X«)1 DJ, ~Y('l 
-DD­
-[0-­
Continuous to Discrete­
llme Converter 
Parameters: T 
Cost: 10 
Discrete to Continuous llme-
Converter 
Parameters: T 
Cost: 10 
Discrete-llme Lowpass Filter 
Parameters: A, T 
Cost: 10 
Continuous-llme Lowpass 
Filter 
Parameters: A, R 
Cost :20 
Expander 
Parameters: L 
Cost :5 
Compressor 
Parameters: M 
Cost :5 
Figure P4.39-4 
4.40. Consider the discrete-time system shown in Figure P4.40-1. 
Figure P4.40-1 
where 
(i) M is an integer.  
") 
[1 _ {x[nl M] 
n 
kM, 
k is any integer  
(II  Xe n 
­
o 
otherwise, 
(iii)  yIn] = Ye[nMl 
. 
{M 
Iwl ~!!.4
(iv)  H(eJIJ) = 
0 
~ <  Iwl ~ 7r, 
(0)  Assume that M = 2 and that X(ejlJ), the DTFf of x[n], is real and is as shown 
in Figure P4.40-2, Make an appropriately labeled sketch of Xe(ejlJ), Ye(ejlJ). and 
Y (ejlJ), the DTFTs ofxe[n], Ye[n], and yIn], respectively, Be sure to clearly label salient 
amplitudes and frequencies. 
X(ei"') 
lin 
nal 
:ers 
:ers 
-7r 
-7r/2 
7r/2 
Figure P4.40-2 

Chapter 4 
Sampling of Continuous-Time Signals
254 
(b)  For M = 2 and X (eiw) as given in Figure P4.40-2, find the value of 
00 
2
[; 
L  Ix[n] 
y[n]1
. 
n=-oo 
(c)  For M = 2, the overall system is LTI. Determine and sketch the magnitude of the 
frequency response ofthe overall system IHeff(eiw)l. 
(d)  For M = 6, the overall system is still LTI. Determine and sketch the magnitude of the 
overall system's frequency response IHeff(ejW)I. 
4.41.  (a) Consider the system in Figure P4.41-1 where a filter H (z) is followed by a compressor. 
Suppose that H(z) has an impulse response given by: 
(i)n , 
O:s n :s 11
h[n]  
(P4.41-1)
{ 0, 
otherwise. 
yIn] 
Figure P4.41-1 
The efficiency of this system can be improved by implementing the filter H (z) and the 
compressor using a polyphase decomposition. Draw an efficient polyphase structure 
for this system with two polyphase components. Please specify the filters you use. 
(b)  Now consider the system in Figure P4.41-2 where a filter H(z) is preceded by an ex­
pander. Suppose that H(z) has the impulse response as given in Eq. (P4.41-1). 
y[n] 
Figure P4.41-2 
The efficiency of this system can be improved by implementing the expander and filter 
H(z) using a polyphase decomposition. Draw an efficient polyphase structure for this 
system with three polyphase components. Please specify the filters you use. 
4.42.  For the systems shown in Figure P4.42-1 and Figure P4.42-2, determine whether or not it is 
possible to specify a choice for H2 (z) in System 2 so that Y2[n] = YI[n] when x2[n] = Xl [n] 
and HI (z) is as specified. If it is possible, specify H2(Z). If it is not possible, clearly explain. 
System 1: 
H1(z) 
Yl[n] 
, nl2 integer
wl[n] 
{ ~1[nI2] 
, otherwise 
Figure P4.42-1 
....  

255 
Chapter 4 
Problems 
System 2: 
nl2 integer 
otherwise 
Figure P4.42-2 
4.43.  The block diagram in Figure P4.43 represents a system that we would like to implement. 
Determine a block diagram of an equivalent system consisting of a cascade of LTI systems, 
compressor blocks, and expander blocks which results in the minimum number of multipli­
1) 
cations per output sample. 
Note: By "equivalent system," we mean that it produces the same output sequence for any 
given input sequence. 
the 
Z-6 
H (z) = ---;c--~ 
ex- 
7+c6 -2c12 
4.44.  Consider the two systems shown in Figure P4.44. 
System A: 
Figure P4.43 
System B: 
YB[n] 
Figure P4.44 
where QO represents a quantizer which is the same in both systems. For any given G(z), 
can H(z) always be specified so that the two systems are equivalent (i.e., YArn] = YB[n] 
when xA[n] = xB[n]) for any arbitrary quantizer QO? If so, specify H(z). If not, clearly 
explain your reasoning. 

256  
Chapter 4 
Sampling of Continuous-Time Signals 
4.45.  The quantizer QO in the system S1 (Figure P4.45-1) can be modeled with an additive noise. 
Figure P4.45-2 shows system S2, which is a model for system S1 
LPF 
71"
~ 
wc=i.j
H ~
Figure P4.45-1 
System S1 
ern] 
LPF 
71" 
wc=i.j 
Figure P4.45-2 
System S2 
The input x[n] is a zero-mean, wide-sense stationary random process with power spectral 
density ct>xx<e}W) which is bandlimited to JrIM and we have E [x2[n]] = 1. The additive 
noise ern] is wide-sense stationary white noise with zero mean and variance err Input and 
additive noise are uncorrelated. The frequency response of the low-pass filter in all the 
diagrams has a unit gain. 
(a)  For system S2 find the signal to noise ratio: SN R = 10 log ~~~~:ll. Note that Yx [n] is 
the output due to x[n] alone and Ye[n] is the output due to ern] alone. 
(b) To improve the SNR owing to quantization, the system of Figure P4.45-3 is proposed: 
1 
l-z-N 
LPF 
71" 
wc=i.j 
Figure P4.45-3 
where N > 0 is an integer such that Jr N < < M. Replace the quantizer with the additive 
model, as in Figure P4.45-4. Express Y1x[n] in terms of x[n] and Y1e[n] in terms of ern]. 
ern] 
4QO~] 
Figure P4.45-4 
(c)  Assume that ern] is a zero mean wide-sense stationary white noise that is uncorrelated 
with input x[n]. Is Y1e[n] a wide-sense stationary signal? How about Y1[n]? Explain. 
(d)  Is the proposed method in part (b) improving the SNR? For which value of N is the 
SNR of the system in part (b) maximized? 

257 
Chapter 4 
Problems 
4.46.  The following are three proposed identities involving compressors and expanders. For each, 
state whether or not the proposed identity is valid. Ifyour answer is that it is valid, explicitly 
show why. If your answer is no, explicitly give a simple counterexample. 
(a) Proposed identity (a): 
Half-sample delay 
D 
z-l  
z-2 
t2 
...
-I 
Figure P4.46-1 
(b) Proposed identity (b): 
t2 H 
hEn] 
t 2 
~  
H 
~ 
D 
z 
Figure P4.46-2 
(c) Proposed identity (c): 
A
-1 tL H 
~  
D  
tL
-1 
A H 
~ Figure P4.46-3 
where L is a positive integer, and A is defined in terms of X(e jW ) and Y(e jW ) (the 
respective DTFTs of A's input and output) as: 
4A~ 
Y(eiw) = (X(dW))L 
Figure P4.46-4 

258 
Chapter 4 
Sampling of Continuous-Time Signals  
Ch 
4.47. Consider the system shown in Figure P4.47-1 for discrete-time processing ofthe continuous­
time input signal gc<t). 
C-T  
LTI  
System  
H,,ijn) 
Ideal
xc(t)  
x[nJ 
C/O  
Conv.  
D-T 
LTI 
y[nJ 
[deal 
System 
DIC 
ycCt) 
Ht(ejW } 
Cony. 
T  
T 
Figure P4.47·1 
The continuous-time input signal to the overall system is of the fonn gC<t) = fc<t) + ec(t) 
where feCt) is considered to be the "signal" component and ee(t) is considered to be an 
"additive noise" component. The Fourier transforms of fdt) and ec(t) are as shown in 
Figure P4,47-2. 
Fc(jn) 
inl 
A 
--~~----~----+---~~ n 
~ r
_ 
r--I­ .. n 
-4007T 
4007T 
-4007T 
4007T 
Figure P4.47-2 
Since the total input signal gdt) does not have a bandlimited Fourier transfonn. a zero­
phase continuous-time anti aliasing filter is used to combat aliasing distortion. Its frequency 
response is given in Figure P4,47-3. 
Inl < 8007T
~'nll(&x>') 1.01> 8007T 
~ .0 
-8007T 
-4007T 
4007T 
8007T 
Figure P4.47-3 
(a)  If in Figure P4.47-1 the sampling rate is 2nlT = 1600n, and the discrete-time system 
has frequency response 
jW 
Iwl < nl2 
Hl(e
) = {~ 
nl2 < Iwl ~ n 
sketch the Fourier transform of the continuous-time output signal for the input whose 
Fourier transform is defined in Figure P4,47-2. 
(b)  If the sampling rate is 2nlT = 1600rr, determine the magnitude and phase of HI (eJW ) 
(the frequency response of the discrete-time system) so that the output of the system 
in Figure P4,47-1 is yc(t) :::: fc<t - 0.1). You may use any combination of equations or 
carefully labeled plots to express your answer. 

Chapter 4 
Problems  
259 
(c)  It turns out that since we are only interested in obtaining fc<t) at the output, we 
can use a lower sampling rate than 2:n: / T 
1600:n: while still using the antialiasing 
filter in Figure P4.47-3. Determine the minimum sampling rate that will avoid aliasing 
distortion of FcUO) and determine the frequency response of the filter HI (ejW ) that 
can be used so that yc(t) = fc<t) at the output of the system in Figure P4.47-1. 
(d)  Now consider the system shown in Figure P4.47-4, where 2:n:/T = 1600:n:, and the 
input signal is defined in Figure P4.47-2 and the antialiasing filter is as shown in Figure 
P4.47-3. 
~ 
C-T 
LTI 
System 
Haa(ja) 
Ideal
xc(t) 
x[n] 
C/D ~ 
Cony. 
t 
D-T 
v[n] 
LTl 
y[n]
t 3 
System 
H2(ei"') 
T  
Figure P4.47-4 Another System Block Diagram 
where 
_ {X[n/3] 
n = 0, ±3, ±6, ...
[ ]
v n  -
0 
otherwise  
What should Hz(ejW ) be if it is desired thaty[n] = fc(nT/3)?  
4.48. (a) A finite sequence b[n] is such that: 
B(z) + B(-z) = 2c, 
c -# O. 
Explain the structure of b[n]. Is there any constraint on the length of b[n]? 
(b)  Is it possible to have B(z) = H(z)H(z-l)? Explain. 
(c)  A length-N filter H(z) is such that, 
H(z)H(z-l) + H(-z)H(-z-l) = c. 
(P4.48-1) 
Find Go(z) and Gl (z) such that the filter shown in Figure P4.48 is LTI: 
Figure P4.48 
(d)  For GO(z) and Gl (z) given in part (c), does the overall system perfectly reconstruct the 
input? Explain. 
4.49. Consider the multirate system shown in Figure P4.49-1 with input x[nl and output y[n]: 
Figure P4.49-1 

260 
Chapter 4 
Sampling of Continuous-Time Signals 
where Qo(eJW ) and Ql (e JW ) are the frequency responses of two LTI systems. Ho(eJW ) and 
Hl (eJW ) are ideallowpass and highpass filters, respectively, with cutoff frequency at if/2 as 
shown in Figure P4.49-2: 
11 HoC"') I 
-rr/2 
rr/2 
rr 
HI (eiw) 
1 
""'w
-2 
-1T12 
1T12 
1T 
Figure P4.49-2 
The overall system is LTI if Qo(e jW ) and Ql (eJW ) are as shown in Figure P4.49-3: 
I~
• w 
-2 
-1T/2 
1T12 
rr 
rQ,"") 
•
C 
~ 
w 
-2 
-rr/2 
rrl2 
rr 
Figure P4.49-3 
For these choices of Qo(eiw ) and Ql (e jW ), sketch the frequency response 
Y( jW)
G(ejW ) 
e . 
X(eJW ) 
of the overall system. 
4.50. Consider the QMF filterbank shown in Figure P4.50: 
Figure P4.50 

261 
Chapter 4 
Problems 
The input-output relationship is Y (z) 
T (z)X (z), where  
1  
T(z) = Z(HJ(z) 
HG(-z» = 2z-1EO(z2)El(z2) 
and Eo(z2), El (z2) are the polyphase components of Ho(z). 
Parts (a) and (b) are independent. 
(a) Explain whether the following two statements are correct: 
(at) If Ho(z) is linear phase, then T(z) is linear phase. 
(a2) If EO(z) and El (z) are linear phase, then T(z) is linear phase. 
(b)  The prototype filter is known, hOrn] = 8[n] + 8[n 
1] + !8[n - 2]:  
(bI) What are hI [n], gO[n] and gl [nJ?  
(b2) What are eo[nJ and el[n]?  
(b3) What are T (z) and t[n]?  
4.51.  Consider the system in Figure 4.10 with X c(jn) = 0 for Inl 
27T(1000) and the discrete­
time system a squarer, i.e., y[nJ = x2[nJ. What is the largest value of T such that 
ycCt) = x~(t)? 
4.52. In the system of Figure P4.52, 
xc(jn) = 0, 
Inl "'- 7T/T,  
and  
, Iwl <7T/L, 
7T/ L < Iwl 
7T. 
How is y[n] related to the input signal xc(t)? 
T  
Figure P4.52 
Extension Problems 
4.53.  In many applications, discrete-time random signals arise through periodic sampling of 
continuous-time random signals. We are concerned in this problem with a derivation of 
the sampling theorem for random signals. Consider a continuous-time, stationary, random 
process defined by the random variables {xa (t)}, where t is a continuous variable. The 
autocorrelation function is defined as 
tPxcxc(r) = £{x(t)x*(t + r)}, 
and the power density spectrum is 
j0 r
pxcxc(n) £: ¢xcxc(r)e-
: dr. 
A discrete-time random process obtained by periodic sampling is defined by the set of 
random variables {x[n]}, where x[n] = xa(nT) and T is the sampling period. 

262 
Chapter 4 
Sampling of Continuous-Time Signals 
(a)  What is the relationship between ¢xx[nJ and ¢xcxc(r)? 
(b) Express the power density spectrum of the discrete-time process in terms of the power 
density spectrum of the continuous-time process. 
(c)  Under what condition is the discrete-time power density spectrum a faithful represen­
tation of the continuous-time power density spectrum? 
4.54. Consider a continuous-time random process xc(t) with a bandlimited power density spec­
trum PxcxcCQ) as depicted in Figure P4.54-1. Suppose that we sample xcCt) to obtain the 
discrete-time random process x{n] = xcCnT). 
pxcxc(n) 
1 
-no 
no 
n  Figure P4.54-1 
(a)  What is the autocorrelation sequence of the discrete-time random process? 
(b)  For the continuous-time power density spectrum in Figure P4.54-1, how should T be 
chosen so that the discrete-time process is white, i.e., so that the power spectrum is 
constant for all w? 
(c)  If the continuous-time power density spectrum is as shown in Figure P4.54-2, how 
should l' be chosen so that the discrete-time process is white? 
£0) 
-no 
no 
n 
Figure P4.54-2 
(d) What is the general requirement on the continuous-time process and the sampling 
period such that the discrete-time process is white? 
4.55. This problem explores the effect of interchanging the order of two operations on a signal, 
namely, sampling and performing a memoryless nonlinear operation. 
(a)  Consider the two signal-processing systems in Figure P4.55-1, where the CID and DIC 
converters are ideal. The mapping g[x] 
x 2 represents a memoryless nonlinear device. 
For the two systems in the figure, sketch the signal spectra at points 1, 2, and 3 when 
the sampling rate is selected to be 1/1' 
2.fm Hz and xc(t) has the Fourier transform 
shown in Figure P4.55-2. Is Yl (t) 
Y2(t)? If not, why not? Is Yl (1) 
x2Ct)? Explain 
your answer. 

263 
Chapter 4 
Problems 
System 1: 
xc(t)  
Yl(t)
CD 
T  
T 
System 2: 
xc(t) 
CD Y2(t) 
T 
T 
Figure P4.55-1 
Figure P4.55-2 
be 
(b)  Consider System 1, and let x (t) 
A cos (307f t). Let the sampling rate be 1IT = 40 Hz. 
Is Yl (t) = x~(t)? Explain why or why not. 
(c)  Consider the signal-processing system shown in Figure P4.55-3, where g[x] = x3 
and g-l[v] is the (unique) inverse, i.e., g-l[g(x)] = x. Let xU) 
A cos (307ft) and 
liT = 40 Hz. Express v[n] in terms of x[n). Is there spectral aliasing? Express y[n] in 
terms of x[n]. What conclusion can you reach from this example? You may find the 
following identity helpful: 
3 
cos  Qot i cos Qat + *cos 3Qot. 
T  
Figure P4.55-3 
(d)  One practical problem is that of digitizing a signal having a large dynamic range. Sup­
pose we compress the dynamic range by passing the signal through a memory less 
nonlinear device prior to AID conversion and then expand it back after AID conver­
sion. What is the impact of the nonlinear operation prior to the ND converter in our 
choice of the sampling rate? 
4.56.  Figure 4.23 depicts a system for interpolating a signal by a factor of L, where 
[ j_ {x[nIL]' 
n = 0, ±L, ±2L , etc ... , 
Xe n 
-
0, 
otherwise, 
and the lowpass filter interpolates between the nonzero values of xe[nJ to generate the 
upsampled or interpolated signal Xi [n]. When the lowpass filter is ideal, the interpolation is 

264  
Chapter 4 
Sampling of Continuous-Time Signals 
referred to as bandlimited interpolation. As indicated in Section 4.6.3, simple interpolation 
procedures are adequate in many applications. Two simple procedures often used are zero­
order-hold and linear interpolation. For zero-order-hold interpolation, each value of x[n] 
is simply repeated L times; i.e., 
xe£O], 
n=O.1, .... L-1, 
xe[L]. 
n = L, L + 1, ... 2L - 1, 
xi[n] 
xe[2L], 
n=2L,2L+l, ... ,
I
Linear interpolation is described in Section 4.6.2. 
(a)  Determine an appropriate choice for the impulse response of the lowpass filter in 
Figure 4.23 to implement zero-order-hold interpolation. Also, determine the corre­
sponding frequency response. 
(b)  Equation (4.91) specifies the impulse response for linear interpolation. Determine the 
corresponding frequency response. (You may find it helpful to use the fact that hlin[n] 
is triangular and consequently corresponds to the convolution of two rectangular se­
quences.) 
(c)  Sketch the magnitude of the filter frequency response for zero-order-hold and linear 
interpolation. Which is a better approximation to ideal bandlimited interpolation? 
4.57.  We wish to compute the autocorrelation function of an upsampled signal, as indicated in 
Figure P4.57-1. It is suggested that this can equivalently be accomplished with the system 
of Figure P4.57-2. Can Hz(e jW) be chosen so that ¢3[mj 
(P1[mj? If not, why not? If so, 
specify Hz(ej (1)). 
Ideallowpass 
1--1 filter cutoff ,---1'00-11 
'" 
x[n] 
I xu[n] I 
7TIL 
xi[n] 
4>dm] L xj[n+m]xiln] 
n=-OO 
Figure P4.57-1 
x[n] 
4>3[m] 
Figure P4.57-2 
4.58.  We are interested in upsampling a sequence by a factor of 2, using a system of the form of 
Figure 4.23. However, the lowpass filter in that figure is to be approximated by a five-point 
filter with impulse response hEn] indicated in Figure P4.58-1. In this system, the output)'l [n] 
is obtained by direct convolution of h[n] with w[nj. 
b  
d 
a 
e
LtlL
h1n] 
o 1 234 
n 
Figure P4.58-1 

265 
Chapter 4 
Problems 
(a)  A proposed implementation of the system with the preceding choice of h[n] is shown 
in Figure P4.S8-2. The three impulse responses h!ln], h2[n], and h3[n] are all restricted 
to be zero outside the range 0 :::: n :::: 2. Determine and clearly justify a choice for 
hl [n], h2[n], and h3[n] so that Yl In] = Y2[n] for any x[n], i.e., so that the two systems 
are identical. 
in 
x[n] 
Figure P4.58-2 
(b)  Determine the number of multiplications per output point required in the system of 
Figure P4.S8-1 and in the system of Figure P4.S8-2. You should find that the system of 
so, 
Figure P4.S8-2 is more efficient. 
4.59.  Consider the analysis-synthesis system shown in Figure P4.S9-1. The lowpass filter horn] 
is identical in the analyzer and synthesizer, and the highpass filter hl [nJ is identical in the 
analyzer and synthesizer. The Fourier transforms of hOrn] and hl[n] are related by 
x[nl 
_ 
HPF  
HPF 
Analyzer 
Synthesizer 
y[n]=yo[n]-Ydn] 
Figure P4.59-1 
(a)  If X (eiw ) and H o(eiw ) are as shown in Figure P4.S9-2, sketch (to within a scale factor) 
Xo(e]W), Go(ejW ), and Yo(e jlV ). 
(b)  Write a general expression for Go(ejW ) in terms of X (eilV) and Ho(ei(V). Do notassume 
that X (ejlV ) and Ho(eJW ) are as shown in Figure P4.S9-2. 

266 
Chapter 4 
Sampllng of Continuous-Time Signals 
X(e jW ) 
A 
-17' 
0 
1T 
w 
f'"k;'j  
~ 
I 
C  
-217' 
-17'  
17' 
0 
'1I 
17' 
W 
2 
2 
Figure P4.59·2 
(c)  Determine a set of conditions on Ho(ejW) that is as general as possible and that will 
guarantee that IY(ejW)1 is proportional to IX(ejW)1 for any stable inputx[n]. 
Note: Analyzer-synthesizer filter banks of the form developed in this problem are very 
similar to quadrature mirror filter banks. (For further reading, see Crochiere and Rabiner 
(1983), pp. 378-392.) 
4.60. Consider a real-valued sequence x[n] for which 
1l'
X (ejW ) 
O. 
Iwl <1l'.
3 
One value of x[n] may have been corrupted, and we would like to approximately or exactly 
recover it. With x[n] denoting the corrupted signal, 
x[n] = x[n] for n =1= no. 
and x[no] is real but not related to x[nO]' In each of the following three cases, specify a 
practical algorithm for exactly or approximately recovering x[nl from x[n]: 
(a)  The value of nO is known. 
(b) The exact value of no is not known, but we know that no is an even number. 
(c)  Nothing about no is known. 
4.61. Communication systems often require conversion from time-division multiplexing (TDM) 
to frequency-division multiplexing (FDM). In this problem, we examine a simple example 
of such a system. The block diagram of the system to be studied is shown in Figure P4.61-1. 
The TDM input is assumed to be the sequence of interleaved samples 
I 
xl [n/2] 
for n an even integer, 
w[n] 
x2[(n 
1)/2] 
for n an odd integer. 
Assume that the sequences xI[n] 
XcI (nT) and x2[n] = xc2(nT) have been obtained by 
sampling the continuouHime signals XcI (t) and X c2(t), respectively, without aliasing. 

267 
Chapter 4 
Problems 
a 
by 
TDM 
w[n] 
demultiplex 
1DM 
signal 
Figure P4.61-1 
Assume also that these two signals have the same highest frequency, QN, and that the 
sampling period is T = :rr/ Q N . 
(a)  Draw a block diagram of a system that produces Xl [n] and x2[n] as outputs; i.e., obtain 
a system for demultiplexing a TDM signal using simple operations. State whether or 
not your system is linear, time invariant, causal, and stable. 
The kth modulator system (k 
1 or 2) is defined by the block diagram in Figure 
P4.61-2. The lowpass filter Hj(ejW), which is the same for both channels, has gain Land 
cutoff frequency:rr / L, and the highpass filters Hk (ejW ) have unity gain and cutoff frequency 
Wk. "The modulator frequencies are such that 
and 
(assumewl > :rr/2), 
Figure P4.61-2 
(b)  Assume that QN == 2:rr x 5 x 103, Find w1 and L so that, after ideal Die conversion 
with sampling period T/ L, the Fourier transform of yc(t) is zero, except in the band of 
frequencies 
2:rr x 105 :s Iwl :s 2:rr x 105 + 2QN. 
(c)  Assume that the continuous-time Fourier transforms of the two original input signals 
are as sketched in Figure P4.61-3. Sketch the Fourier transforms at each point in the 
system. 
A  
B 
Figure P4.61-3 
(d)  Based on your solution to parts (a)-(c), discuss how the system could be generalized 
to handle M equal-bandwidth channels. 

-
-
-
-
Chapter 4 
Sampling of Continuous-Time Signals
268 
4.62. In Section 4.8.1, we considered the use of prefiitering to avoid aliasing. In practice, the 
antialiasing filter cannot be ideal. However, the nonideal characteristics can be at least 
partially compensated for with a discrete-time system applied to the sequence x[n] that is 
the output of the C/D converter. 
Consider the two systems in Figure P4.62-1. The antialiasing filters H ideal (j~) and 
H aa (jQ) are shown in Figure P4.62-2. H (eiw ) in Figure P4.62-1 is to be specified to com­
pensate for the nonideal characteristics of H aa (j Q). 
Sketch H(eiw ) so that the two sequences x[n] and w[n] are identical. 
System 1: 
T 
System 2: 
T  
Figure P4.62-1 
1 Hideal(jll) 
7r  
7r 
II 
T  
T 
Haa(jll)
1 
1 
10 
7r 
-llp 
IIp 
7r 
II 
T 
T 
Figure P4.62-2 
4.63.  As discussed in Section 4.8.2, to process sequences on a digital computer, we must quantize 
the amplitude of the sequence to a set of discrete levels. This quantization can be expressed 
in terms of passing the input sequence x[n] through a quantizer Q(x) that has an input­
output relation as shown in Figure 4.54. 
As discussed in Section 4.8.3, if the quantization interval Ll is small compared with 
changes in the level of the input sequence, we can assume that the output of the quantizer 
is of the form 
y[n] = x[n] + ern], 
where ern] = Q (x[n]) -
x[n] and ern] is a stationary random process with a 1st-order 
probability density uniform between -Ll/2 and Ll/2, un correlated from sample to sample 
and uncorrelated with x[n], so that [{e[n]x[m]} = 0 for all m and n. 
.......  

269 
Chapter 4 
Problems 
Let x[n] be a stationary white-noise process with zero mean and variance 
(a)  Find the mean, variance, and autocorrelation sequence of ern]. 
(b)  What is the signal-to-quantizing-noise ratio a;;o}? 
(c)  The quantized signal yIn] is to be filtered by a digital filter with impulse response 
h[n1 
~ [an + (_a)n lu[n]. Determine the variance of the noise produced at the output 
due to the input quantization noise, and determine the SNR at the output. 
In some cases we may want to use nonlinear quantization steps, for example, logarithmically 
spaced quantization steps. This can be accomplished by applying uniform quantization to 
the logarithm of the input as depicted in Figure P4.63, where Q[.] is a uniform quantizer as 
specified in Figure 4.54. In this case, if we assume that 8 is small compared with changes in 
the sequence In(x[n]), then we can assume that the output of the quantizer is 
In(y[n]) 
In(x[nJ) + ern]. 
Thus. 
yIn] = x[n] . exp(e[nJ). 
For small e, we can approximate exp(e[nJ) by (1 + ern]), so that 
yIn] ~ x[nj(1 + ern]) = x[n] + fIn]. 
(P4.63-1) 
This equation will be used to describe the effect of logarithmic quantization. We assume 
ern] to be a stationary random process, uncorrelated from sample to sample, independent 
of the signal x[n], and with 1st-order probability density uniform between 
(d)  Determine the mean, variance, and autocorrelation sequence of the additive noise f[n] 
defined in Eq. (4.57). 
(e)  What is the signal-to-quantizing-noise ratio a;/o}? Note that in this case a;/aJ is inde­
pendent of 
. Within the limits ofour assumption, therefore, the signal-to-quantizing­
noise ratio is independent of the input signal level, whereas, for linear quantization, 
the ratio a;/a; depends directly on a;. 
(f)  The quantized signal yIn] is to be filtered by means of a digital filter with impulse 
response h[n] = ! [an + (-a)"]u[n]. Determine the variance of the noise produced at 
the output due to the input quantization noise, and determine the SNR at the output. 
4.64.  Figure P4.64-1 shows a system in which two continuous-time signals are multiplied and 
a discrete-time signal is then obtained from the product by sampling the product at the 
Nyquist rate; i.e., YI[n] is samples of yc(t) taken at the Nyquist rate. The signal Xl (t) is 
band limited to 25 kHz (X 1(jQ) = 0 for IQI ~ 5rr x 104), and X2(t) is limited to 2.5 kHz 
(X2(jQ) 
ofor IQI ~ (rr/2) x 104). 
X2(t) 
T Nyquist rate 
Figure P4.64-1 

270  
Chapter 4 
Sampling of Continuous-Time Signals 
In some situations (digital transmission, for example), the continuous-time signals have 
already been sampled at their individual Nyquist rates, and the multiplication is to be 
carried out in the discrete-time domain, perhaps with some additional processing before 
and after multiplication, as indicated in Figure P4.64-2. Each of the systems A, B. and C 
either is an identity or can be implemented using one or more of the modules shown in 
Figure P4.64-3. 
' 
Xl(t) 
TI =2 x lO-ssec  
Y2[n] 
X2(t) 
= 2 X 1O-4sec
Tz  
Figure P4.64-2 
n 0, L,2L, ... 
Module I 
~g[n]={S[~L] 
otherwise 
~ 
Module II ~~ gIn] 
s[nM] 
1 
H(e1U» 
s[n].1 
i I f--- g[n]
Module III 
I 
-We 
We 
Figure P4.64-3 
For each of the three systems A, B, and C, either specify that the system is an identity 
system or specify an appropriate interconnection of one or more of the modules shown in 
Figure P4.64-3. Also. specify all relevant parameters L, M, and We. The systems A. B, and 
C should be constructed such that Y2[n] is proportional to Yl In], i.e., 
Y2[n] 
kYl[n] =kYe(nT) 
kxl(nT) x x2(nT), 
and these samples are at the Nyquist rate, i.e., Y2[n] does not represent oversampling or 
undersampling of Ye(t). 
4.65.  Suppose sC<t) is a speech signal with the continuous-time Fourier transform Sc(jQ) shown 
in Figure P4.65-1. We obtain a discrete-time sequence sr[n] from the system shown in 
Figure P4.65-2, where H (eJW ) is an ideal discrete-time lowpass filter with cutoff frequency 
We and a gain of L throughout the passband, as shown in Figure 4.29(b). The signal sr[nl 
will be used as an input to a speech coder, which operates correctly only on discrete-time 
samples representing speech sampled at an 8-kHz rate. Choose values of L , M, and We that 
produce the correct input signal Sr [n] for the speech coder. 

271 
Chapter 4 
Problems 
-27T·4000 
o 
27T' 4000 !1(rad/s) 
Figure P4.65-1 
fM ~sr[n]  
T = (1144.1) IDS 
Figure P4.65-2 
4.66.  In many audio applications, it is necessary to sample a continuous-time signal xc(t) at 
a sampling rate liT 
44 kHz. Figure P4.66-1 shows a straightforward system, includ­
ing a continuous-time antialias filter HaO(jn). to acquire the desired samples. In many 
applications, the "4x oversampling" system shown in Figure P4.66-2 is used instead of the 
conventional system shown in Figure P4.66-L In the system in Figure P4.66-2, 
I(VI ::: 1!14, 
otherwise, 
x[n] 
(liT) = 44 kHz 
Figure P4.66-1 
is an ideallowpass filter, and 
for some 0 ::: np::: ns ::: 00. 
x[n] 
(liT) = 4 44 kHz 176 kHz 
Figure P4.66-2 
Assuming that H(ejW ) is ideal, find the minimal set of specifications on the antialias filter 
Hal(jn), i.e., the smallest np and the largest n s , such that the overall system of Fig­
ure P4.66-2 is equivalent to the system in Figure P4.66-L 

272 
Chapter 4 
Sampling of Continuous-Time Signals 
4.67.  In this problem, we will consider the "double integration" system for quantization with 
noise shaping shown in Figure P4.67. In this system, 
1 
HI (z) = 1 
and 
H2(Z) 
and the frequency response of the decimation filter is 
Iwl < IT/M,
H3(e jW ) = {I,0,  
IT/M ~ Iwl ~ IT. 
The noise source ern], which represents a quantizer, is assumed to be a zero-mean white­
noise (constant power spectrum) signal that is uniformly distributed in amplitude and has 
noise power o} = ~2/12. 
ern] 
H1(z) 
H2(z) 
H3(z) 
~M  
L--_...J' w[n] I 
Iu[n] 
w[Mn]  
Figure P4.67 
(a)  Determine an equation for Y (z) in terms of X (z) and E(z). Assume for this part that 
E(z) exists. From the z-transform relation, show that yIn] can be expressed in the form 
yIn] = x[n - 1] + f[n], where fIn] is the output owing to the noise source ern]. What 
is the time-domain relation between f[n] and e[n]? 
(b)  Now assume that eln] is a white-noise signal as described prior to part (a). Use the 
result from part (a) to show that the power spectrum of the noise f[n] is 
Pff(ejW ) 
160;sin4 (w/2). 
What is the total noise power (o}) in the noise component of the signal yIn]? On the 
same set of axes, sketch the power spectra Pee(ejW ) and Pff(e jW ) for 0 ~ w ~ IT. 
(c)  Now assume that X (ejW ) = 0 for IT/ M < w ~ IT. Argue that the output of H3(z) is 
wIn] = x[n -1] + gIn]. State in words what gIn] is. 
(d)  Determine an expression for the noise power oJ at the output of the decimation filter. 
Assume that IT/ M « IT, i.e., M is large, so that you can use a small-angle approximation 
to simplify the evaluation of the integraL 
(e)  After the decimator, the output is v[n] = w[Mn] = x[Mn - 1] + q[n], where q[n] = 
g[Mnl. Now suppose that x[n] = xc(nT) (i.e., xln] was obtained by sampling a 
continuous-time signal). Wnat condition must be satisfied by X cUn) so that x[n 
1] 
will pass through the filter unchanged? Express the "signal component" of the output 
v[n] in terms of xdt). What is the total power aJ of the noise at the output? Give an 
expression for the power spectrum of the noise at the output, and, on the same set of 
axes, sketch the power spectra Pee(ejW ) and Pqq (e jW ) for 0 ~ w ~ IT. 
4.68.  For sigma-delta oversampled AID converters with high-order feedback loops, stability be­
comes a significant consideration. An alternative approach referred to as multi-stage noise 
shaping (MASH) achieves high-order noise shaping with only 1st-order feedback. The 

273 
Chapter 4 
Problems 
structure for 2nd -order MASH noise shaping is shown in Figure P4.68-2 and analyzed in 
this problem. 
Figure P4.68-1 is a 1st-order sigma-delta (1: -
,').) noise shaping system, where the 
effect of the quantizer is represented by the additive noise signal ern]. The noise ern] is 
explicitly shown in the diagram as a second output of the system. Assume that the input 
x[n] is a zero-mean wide-sense stationary random process. Assume also that ern] is zero­
mean, white, wide-sense stationary, and has variance ul. ern] is uncorrelated with x[n]. 
(a)  For the system in Figure P4.68-1, the output y[n] has a component yx[n] due only to 
x[n] and a component yeln] due only to ern], i.e., yIn) 
Yx [n] + Ye[n]. 
(i) Determine yx[n] in terms of x[n]. 
(ii) Determine PYe (w), the power spectral density of Ye [n]. 
1  
Quantizer
I 
1 
1 
1  
1  
1+ 
y[n]
x[n] 
Figure P4.68-1
ern] 
x[n] 
Zl 
r[n] 
Figure P4.68-2 
(a)  The system ofFigure P4.68 is now connected in the configuration shown in Figure P4.68, 
which shows the structure of the MASH system. Notice that eIln] and eZ[n] are the 
noise signals resulting from the quantizers in the sigma-delta noise shaping systems. The 
output of the system r[n] has a component r.[n] owing only to x[n], and a component 
reIn] due only to the quantization noise, i.e., r[n] 
rx [n ]+re[n]. Assume that el[n1and 
eZ[n] are zero-mean, white, wide-sense stationary, each with variance al. Also assume 
that el[n] is uncorrelated with ez[n]. 
(i)  Determinerx[n] in terms ofx[n]. 
(ii)  Determine Pre (w), the power spectral density of reIn]. 

5 
Transform Ana,lysisof 
linear Time-Invariant 
Systems 
5.0 INTRODUCTION 
In Chapter 2, we developed the Fourier transform representation of discrete-time signals 
and systems, and in Chapter 3 we extended that representation to the z-transform. In 
both chapters, the emphasis was on the transforms and their properties, with only a 
brief preview of the details of their use in the analysis of linear time-invariant (LTI) 
systems. In this chapter, we develop in more detail the representation and analysis of 
LTI systems using the Fourier and z-transforms. The material is essential background 
for our discussion in Chapter 6 of the implementation of LTI systems and in Chapter 7 
of the design of such systems. 
As discussed in Chapter 2, an LTI system can be completely characterized in the 
time domain by its impulse response h[n], with the output y[n] due to a given input x[n] 
specified through the convolution sum 
y[n] = L
00 
x[k]h[n - k]. 
(5.1) 
k= - oo 
Alternatively, since the frequency response and impulse response are directly related 
through the Fourier transform, the frequency response, assuming it exists (i.e., H (z) has 
an ROC that includes z = e jW ), provides an equally complete characterization of LTI 
systems. In Chapter 3, we developed the z-transform as a generalization of the Fourier 
transform. The z-transform of the output of an LTI system is related to the z-transform 
of the input and the z-transform of the system impulse response by 
Y(z) = H(z)X(z), 
(5.2) 
where Y(z), X(z), and H(z) denote the z-transforms of y[n] , x[n] and h[n] respectively 
and have appropriate regions of convergence. H (z) is typically referred to as the system 
274 

275 
Section 5.1 
The Frequency Response of LTI Systems 
function. Since a sequence and its z-transform form a unique pair, it follows that any LTI 
system is completely characterized by its system function, again assuming convergence. 
Both the frequency response, which corresponds to the system function evaluated 
on the unit circle, and the system function more generally as a function of the com­
plex variable z, are extremely useful in the analysis and representation of LTI systems, 
because we can readily infer many properties of the system response from them. 
5.1 THE FREOUENCY RESPONSE OF LT. SYSTEMS 
The frequency response H (ejW ) of an LTI system was defined in Section 2.6 as the com­
plex gain (eigenvalue) that the system applies to a complex exponential input (eigen­
function) ej(Vn . Furthermore, as discussed in Section 2.9.6, since the Fourier transform of 
a sequence represents a decomposition as a linear combination ofcomplex exponentials, 
the Fourier transforms of the system input and output are related by 
(S.3) 
where X(eiw) and Y(eiw) are the Fourier transforms of the system input and output, 
respectively. 
5.1.1 Frequency Response Phase and Group Delay 
The frequency response is in general a complex number at each frequency. With the 
frequency response expressed in polar form, the magnitude and phase of the Fourier 
transforms of the system input and output are related by 
IY(eiw)1 
IH(ejW)1 . IX(eiw)l, 
(S.4a) 
LY(ejW) = LH(eiw ) + LX(e]W), 
(S.4b) 
where IH (eiw )Irepresents the magnitude response or the gain of the system, and L H (ejW) 
the phase response or phase shift of the system. 
The magnitude and phase effects represented by Eqs. (S.4a) and (S.4b) can be 
either desirable, if the input signal is modified in a useful way, or undesirable, ifthe input 
signal is changed in a deleterious manner. In the latter, we often refer to the effects of 
an LTI system on a signal, as represented by Eqs. (S.4a) and (S.4b), as magnitude and 
phase distortions, respectively. 
The phase angle of any complex number is not uniquely defined, since any integer 
multiple of 2rr can be added without affecting the complex number. When the phase 
is numerically computed with the use of an arctangent subroutine, the principal value 
is typically obtained. We will denote the principal value of the phase of H(e jW ) as 
ARG[H(ejw )), where 
(S.S) 

\ 
-71" I 
276 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
Any other angle that gives the correct complex value of the function H(eiw) can be 
represented in terms of the principal value as 
iH(eiw ) = ARG[H(eiw)] + 27rr(w), 
(5.6) 
where r(w) is a positive or negative integer that can be different at each value of w. We 
will in general use the angle notation on the left side of Eq. (5.6) to denote ambiguous 
phase, since r(w) is somewhat arbitrary. 
In many cases, the principal value will exhibit discontinuities of 27r radians when 
viewed as a function of w. This is illustrated in Figure 5.1, which shows a continuous­
phase function arg[H(eiw )] and its principal value ARG[H(eiw )] plotted over the range 
arg[H(e iw )] 
71" 
-471" 
(a) 
w 
ARG[H(eiw )] 
'l 
~'--/ 
"-"/ I 
7/71" 
W 
~ 
(b) 
r(w) 
I  
2  
71" 
w 
Figure 5.1 
(a) Continuous-phase
-1 
curve for asystem function evaluated on 
the unit circle. (b) Principal value of the 
phase curve in part (a). (c) Integer 
multiples of 2n to be added to 
-2 
(e) 
ARG[H(ejW )] to obtain arg[H(ejW )]. 

277 
Section 5.1 
The Frequency Response of LTI Systems 
o:::: w :::: n. The phase function plotted in Figure 5.1(a) exceeds the range -n to +n. The 
principal value, shown in Figure 5.1(b), has jumps of 2n, owing to the integer multiples 
of 2n that must be subtracted in certain regions to bring the phase curve within the 
range of the principal value. Figure 5.1(c) shows the corresponding value of r(w) in 
Eq. (5.6). 
Throughoutthis text, in our discussion of phase, we refer to ARGlH(ejW)] as the 
"wrapped" phase, since the evaluation modulo 2n can be thought of as wrapping the 
phase on a circle. In an amplitude and phase representation (in which the amplitude is 
real-valued but can be positive or negative), ARG[H(ejW )] can be "unwrapped" to a 
phase curve that is continuous in w. The continuous (unwrapped) phase curve is denoted 
as arg[H (e jW)]. Another particularly useful representation ofphase is through the group 
delay L(W) defined as 
d 
. 
L(W) = grd[H(ejW )] 
--(arg[H(e1W)]). 
(5.7)
dw 
It is worth noting that since the derivative of arglH(e1W)] and ARG[H(ejW)] will be 
identical except for the presence of impulses in the derivative of ARG[H(ejcV)] at the 
discontinuities, the group delay can be obtained from the principal value by differenti­
ating, except at the discontinuities. Similarly, we can express the group delay in terms 
of the ambiguous phase f.H(e jW ) as 
(5.8) 
with the interpretation that impulses caused by discontinuities of size 2n in f.H(e jW) 
are ignored. 
To understand the effect of the phase and specifically the group delay of a linear 
system, let us first consider the ideal delay system. The impulse response is 
(5.9)  
and the frequency response is 
(5.10)  
or 
IHid(ejW)1 = 1, 
(5.11a) 
f.Hid(e1W) = -wnd, 
Iwl < n, 
(5.11b) 
with periodicity 2n in w assumed. From Eq. (5.11 b) we note that time delay (or advance 
if nd < 0) is associated with phase that is linear with frequency. 

278 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
In many applications, delay distortion would be considered a rather mild form of 
phase distortion, since its effect is only to shift the sequence in time. Often this would 
be inconsequential, or it could easily be compensated for by introducing delay in other 
parts of a larger system. Thus, in designing approximations to ideal filters and other 
LTI systems, we frequently are willing to accept a linear-phase response rather than a 
zero-phase response as our ideal. For example, an ideallowpass filter with linear phase 
would have frequency response 
{e- Jwnd
. 
Iwi < We,
H)p(eJW ) = 
0, 
'  
(5.12) 
We < Iwi :s Jr. 
The corresponding impulse response is 
sinwe(n 
nd)
hlp[n]  
-00 < n < 00. 
(5.13)
n(n - nd) 
The group delay represents a convenient measure of the linearity of the phase. 
Specifically, consider the output of a system with frequency response H(eiw) for a nar­
rowband input of the form x[nJ 
s[n] cos(won). Since it is assumed that X(e jW ) is 
nonzero only around w = wo, the effect of the phase of the system can be approximated 
in a narrow band around w = wo with the linear approximation 
arg[H(eJW )] ::::: -¢o - wnd,  
(5.14) 
where nd now represents the group delay. With this approximation, it can be shown 
(see Problem 5.63) that the response y[n] to x[n] = sfn] cos(won) is approximately 
y[n] 
IH(ej<iJQ)ls[n - nd] cos(won 
¢o - WOnd). Consequently, the time delay of the 
envelope sfn] of the narrowband signal x[nJ with Fourier transform centered at WO is 
given by the negative of the slope of the phase at WO. In general, we can think of a broad­
band signal as a superposition of narrowband signals with different center frequencies. 
If the group delay is constant with frequency then each narrowband component will 
undergo identical delay. Ifthe group delay is not constant, there will be different delays 
applied to different frequency packets resulting in a dispersion in time of the output 
signal energy. Thus, nonlinearity of the phase or equivalently nonconstant group delay 
results in time dispersion. 
5.1.2  Illustration of Effects of Group Delay and 
Attenuation 
As an illustration of the effects of phase, group delay, and attenuation, consider the 
specific system having system function 
C1-.98eJ.8rrz-l)(1-.98e-J.8rrz-l)) 4.( (c'k-z- 1)(Ck- Z- 1) )2  
H(z) 
( (l-.8ej4rrz-l)(1-.8e-JArrcl)!J (l-CkZ-1)(1- c'kc1 ) 
(5.15)  
, 
"'------.,...-----­
Hj(Z) 
H2(Z) 
with Ck = 0.95eJ(·15rr+.02rrk) for k 
1,2,3,4 and H1 (z) and H2(Z) defined as indi­
cated. The pole-zero plot for the overall system function H(z) is shown in Figure 5.2, 
where the factor HI (z) in Eq. (5.15) contributes the complex conjugate pair of poles 
at z = 0.8e±j.4rr as well as the pair of zeros close to the unit circle at z = .98e±j·8rr . 
...  

279 
Section 5.1 
The Frequency Response of LTI Systems 
----~---~----~----.----~--~ 
-1 
-0.5 
0 
0.5 
Real Part 
0.8 
0.6 
~ 0.4 
i::' 
0.2 
~ 
O· 
.Of;E-O.Z 
.... -0.4 
-0.6 
-0.8 
Figure 5.2 
Pole-zero plot for the filter
-1 
in the example of Section 5.1.2. (The 
number 2 indicates double-order poles 
and zeroes.) 
The factor Hz(z) in Eq. (5.15) contributes the groups of double-order poles at z = 
Ck = O.95e±j(·l5rr+.OZrrk) and double-order zeros at z = liCk = 1/O.95e'fj(·15rr+.OZrrk) 
for k = 1, 2, 3, 4. By itself, H2(Z) represents an allpass system (see Section 5.5), i.e., 
IH2(ejW)1 = 1 for all w. As we will see, H2(Z) introduces a large amount of group delay 
over a narrow band of frequencies. 
The frequency response functions for the overall system are shown in Figures 
5.3 and 5.4. These figures illustrate several important points. First observe in Figure 
5.3(a) that the principal value phase response exhibits multiple discontinuities of size 
2rr. These are due to the modulo 2rr computation of the phase. Figure 5.3(b) shows the 
unwrapped (continuous) phase curve obtained by appropriately removing the jumps of 
size 2rr. 
Figure 5.4 shows the group delay and magnitude response of the overall system. 
Observe that, since the unwrapped phase is monotonically decreasing except around 
w = ±.8rr, the group delay is positive everywhere except in that region. Also, the group 
delay has a large positive peak in the bands of frequencies .17rr < Iwl < .23rr where 
the continuous phase has maximum negative slope. This frequency band corresponds 
to the angular location of the clusters of poles and reciprocal zeros in Figure 5.2. Also 
note the negative dip around (J) = ±.8rr, where the phase has positive slope. Since 
H2 (z) represents an allpass filter, the magnitude response of the overall filter is entirely 
controlled by the poles and zeros of Hl (z). Thus, since the frequency response is H (z) 
evaluated for z = eF", the zeros at z = .98e±j·8rr cause the overall frequency response 
to be very small in a band around frequencies w = ±.8rr. 
In Figure 5.5( a) we show an input signal x [n] consisting ofthree narrowband pulses 
separated in time. Figure 5.5(b) shows the corresponding DTFT magnitude IX(ejW)I. 
The pulses are given by 
xl[n] = w[nJcos(O.2rrn), 
(5.16a) 
xz[n] = w[n]cos(O.4rrn - rr/2), 
(5.16b) 
x3[n) = w[n)cos(O.8rrn + rr/5). 
(5.16c) 

Chapter 5 
Transform Analysis of Linear Time-Invariant Systems
280 
4r----r----.---~-----r----._--_.----,_--_.----_.--_, 
2 
~ 
'" 
~. O~ 
0' 
0:: 
.. - -.. -. -. - ......
~ 
~
-< -2 
_4LI____~____~______L-____~____~____ 
_____L____~_____i____~
~ 
-71" 
-0.871" 
-0.671" 
-0.471" 
-0.271" 
o 
0.271" 
0.471" 
0.617 
0.817 
11' 
w 
(a) Principle Value of Phase Response 
60ri--~--~---'---''---~--'---:---:---:---I 
40 ~ .... "".. 
20 ~ .... .. 
0,.. .. ·· .. · 
-20 
-40  
-60 i 
~-i--~-..--..,  
. 
. 
" ... -" ........ 
-.~. - - --
. ... _"-._ .......... .
. 
. 
.... --- ..,........ _-,. 
'.",".-
..,...........-
. 
. 
-17 
-0.817 
-0.617 
-0.1T4 
-0.217 
o 
0.21T 
0.41T 
0.61T 
0.811' 
1T 
w 
(b) Unwrapped Phase Response 
Figure 5.3 
Phase response functions for system in the example of Section 5.1.2; 
(a) Principal value phase, ARG[H(eiw)]. (b) Continuous phase arg [H(eiw)]. 
where each sinusoid is shaped into a finite-duration pulse by the 61-point envelope 
sequence 
0,46cos(2nn/M), 
0 s n S M,
w[n] = 
0.54 
(5.17)
! 
0, 
otherwise 
with M 
60.1 The complete input sequence shown in Figure 5.5(a) is 
x[n] 
x3[n] + xdn - M 
1] + x2[n - 2M 
2], 
(5.18) 
i.e., the highest frequency pulse comes first, then the lowest, followed by the mid­
frequency pulse. From the windowing or modulation theorem for discrete-time Fourier 
transforms (Section 2.9.7), the DTFT of a windowed (truncated-in-time) sinusoid is the 
convolution of the DTFT of the infinite-duration sinusoid (comprised of impulses at 
± the frequency of the sinusoid) with the DTFT of the window. The three sinusoidal 
frequencies are WI = 0.2n, W2 = OAn, and W3 
0.8n. Correspondingly, in the Fourier 
transform magnitude in Figure 5.5(b) we see significant energy centered and concen­
1 In Chapters 7 and 10, we will see that this envelope sequence is called a Hamming window when used 
in filter design and spectrum analysis respectively. 

281 
Section 5.1 
The Frequency Response of LTI Systems 
200  
150  
~ 
.~ 100 
'" 
~ 
"0' 
50 
.... 
OJ) 
-0.6'IT 
-0.4'IT 
-0.2'IT 
o 
w 
(a) Group delay of H(z) 
0 
-50 
-'IT 
-0.8'IT 
2 
1.5 
0.5 
......., .........:....... 
O~~~~--~--~----~--~~---L----L---~--~~~~ 
-'IT 
-0.8'IT 
-0.6'IT 
-0,4'IT 
-0.21T 
o 
w 
(b) Magnitude of Frequency Response 
Figure 5.4 
Frequency response of system in the example of Section 5.1.2; 
(a) Group delay function, grd[H(ejW)], (b) Magnitude of frequency response, 
IH(ei:u)j. 
trated around each of the three frequencies. Each pulse contributes (in the frequency 
domain) a band of frequencies centered at the frequency of the sinusoid and with a 
shape and width corresponding to the Fourier transform of the time window applied to 
the sinusoid.2 
When used as input to the system with system function H (z), each of the frequency 
packets or groups associated with each of the narrowband pulses will be affected by the 
filter response magnitude and group delay over the frequency band of that group. From 
the filter frequency response magnitude, we see that the frequency group centered and 
concentrated around W = WI = O.2ir will experience a slight amplitude gain, and the 
one around W 
a>z = O.4ir will experience a gain of about 2. Since the magnitude of 
the frequency response is very small around frequency W 
W3 = O.8ir, the highest­
frequency pulse will be significantly attenuated. It will not be totally eliminated, of 
course, since the frequency content of that group extends below and above frequency 
W = lV3 = O.8ir because of the windowing applied to the sinusoid. Examining the plot of 
2As we will see later in Chapters 7 and 10, the width of the frequency bands is approximately inversely 
proportional to the length of the window M + 1. 

Chapter 5 
Transform Analysis of Linear Time-Invariant Systems
282 
0.5 
o 
-0.5 
~1 I 
I 1-, 
• I 
.. -
I 
I 
o 
50 
100 
150 
200 
250 
300 
Sample number (n) 
(a) Waveform of signal x[n] 
20 
· 
.. 
.. 
. 
.................................................................. _...... -. 
15 
· 
. .
· 
. .
· 
. .
·.. 
~.. .f'
~ 
· 
. .
· 
. .
~ 
· 
. .
~ 10 
~ 
~ 
.'::::: I.':::::: ::::::: n.......
:::::1[.'::::::: ::::::: ..':::::: .':::::: :::::::
5 
0
1 
L\ 
LUi' 
/iUi\ 
Ji\ 
-7r 
-0.87r 
-0.67r 
-0.47r 
-0.27r 
o 
0.27r 
0.47r 
0.67r 
0.87r 
7r 
w 
(b) Magnitude of DTFf of x[n] 
Figure 5.5 
Input signal for example of Section 5.1.2; (a) Input signal x[nJ, (b) Cor­
responding DTFT magnitude IX(ejw)l. 
the system group delay in Figure 5.4(a), we see that the group delay around frequency 
(j) = (VI = 0.2lT is significantly larger than for either (V = W2 = O.4lT or (j) = W3 = 0.8)1, 
and consequently the lowest-frequency pulse will experience the most delay through 
the system. 
The system output is shown in Figure 5.6. The pulse at frequency (V = (V3 = 0.8lT 
has been essentially eliminated, which is consistent with the low values of the frequency 
response magnitude around that frequency. The two other pulses have been increased 
in amplitude and delayed; the pulse at (V = O.2lT is slightly larger and delayed by about 
150 samples, and the pulse at (j) = O.4lT has about twice the amplitude and is delayed 
by about 10 samples. This is consistent with the magnitude response and group delay 
at those frequencies. In fact, since the low-frequency pulse is delayed by 140 samples 
more than the mid-frequency pulse and the pulses are each only 61 samples long, these 
two pulses are interchanged in time order in the output. 
The example that we have presented in this subsection was designed to illustrate 
how LTI systems can modify signals through the combined effects of amplitude scaling 
and phase shift. For the specific signal that we chose, consisting of a sum of narrowband 
components, it was possible to trace the effects on the individual pulses. This is because 
the frequency response functions were smooth and varied only slightly across the narrow 
IIa... 

283 
Section 5.2 
System Functions-Linear Constant-Coefficient Difference Equations 
Waveform of signal y[n] 
2,-------,--------.--------07------,--------.-------, 
O~----------------~~N 
-1 
-2~O------~5~O-------1~OO--~········-
-~------~------~------~ 
Sample number (n) 
Figure 5.6 
Output signal for the example of Section 5.1.2. 
frequency bands occupied by the individual components. Therefore, all the frequencies 
corresponding to a given pulse were subject to approximately the same gain and were 
delayed by approximately the same amount, resulting in the pulse shape being replicated 
with only scaling and delay at the output. For wideband signals, this would generally not 
be the case, because different parts ofthe spectrum would be modified differently by the 
system. In such cases, recognizable features of the input such as pulse shape generally 
would not be obvious in the output signal, and individual pulses separated in time in 
the input might cause overlapping contributions to the output. 
This example has illustrated a number of important concepts that will be further 
elaborated on in this and subsequent chapters. After completing a thorough study of this 
chapter, it would be worthwhile to study the example of this subsection again carefully 
to gain a greater appreciation of its nuances. To fully appreciate this example, it also 
would be useful to duplicate it with variable parameters in a convenient programming 
system such as MATLAB. Before testing the computer program, the reader should 
attempt to predict what would happen, for example, when the window length is either 
increased or decreased or when the frequencies of the sinusoids are changed. 
5.2  SYSTEMS CHARACTERIZED BY LINEAR 
CONSTANT-COEFFICIENT DIFFERENCE EQUATIONS 
While ideal filters are useful conceptually, discrete-time filters are most typically realized 
through the implementation of a linear constant-coefficient difference equation of the 
form of Eq. (5.19). 
N 
M 
LakY[n 
k] = Lbkx[n 
k]. 
(5.19) 
k=O 
k=O 
In Chapter 6, we discuss various computational structures for realizing such systems, 
and in Chapter 7, we discuss various procedures for obtaining the parameters of the 
difference equation to approximate a desired frequency response. In this section, with 
the aid of the z-transform, we examine the properties and characteristics of LTI systems 
represented by Eq. (5.19). The results and insights will play an important role in many 
of the later chapters. 

284 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
As we saw in Section 3.5, applying the z-transform to both sides of Eq. (5.19) and 
using the linearity property (Section 3.4.1) and the time-shifting property (Section 3.4.2), 
it follows that, for a system whose input and output satisfy a difference equation of the 
form of Eq. (5.19), the system function has the algebraic form 
M 
'" b .-k 
H(z) = Y(z) _ ~ 
k< 
(5.20)
X(z) -
N 
LakZ-k 
k=O 
In Eq. (5.20) H(z) takes the form of a ratio of polynomials in C 1, because Eq. (5.19) 
consists of two linear combinations of delay terms. Although Eq. (5.20) can, of course, 
be rewritten so that the polynomials are expressed as powers of z rather than of 2-1, 
it is common practice not to do so. Also, it is often convenient to express Eq. (5.20) in 
factored form as 
M 
1
n(1 
CkZ- ) 
H(z) = (bo) _k=_1____ 
(5.21)
ao 
Nn(1- dkZ-1) 
k=l 
Each of the factors (1 
CkZ- 1) in the numerator contributes a zero at z = Ck and a pole 
at z 
O. Similarly, each of the factors (1- dkC1) in the denominator contributes a zero 
at z = 0 and a pole at z = dk. 
There is a straightforward relationship between the difference equation and the 
corresponding algebraic expression for the system function. Specifically, the numera­
tor polynomial in Eq. (5.20) has the same coefficients and algebraic structure as the 
right-hand side of Eq. (5.19) (the terms of the form bkZ-k correspond to bkx[n 
kJ), 
whereas the denominator polynomial in Eq. (5.20) has the same coefficients and alge­
braicstructure as the left-hand side ofEq. (5.19) (the terms ofthe form akZ-k correspond 
to aky[n - k]). Thus, given either the system function in the form of Eq. (5.20) or the 
difference equation in the form of Eq. (5.19), it is straightforward to obtain the other. 
This is illustrated in the following example. 
Example 5.1 
2"d-Order System 
Suppose that the system function of an LTI system is 
(1 
z-1)2 
(5.22) 
H(z) = (1 
!Cl)(1+~C1)' 
To find the difference equation that is satisfied by the input and outputofthis system, we 
express H(z) in the form of Eq. (5.20) by multiplying the numerator and denominator 
factors to obtain the ratio of polynomials 
1 + 2z-1 
Y(z)
H(z) 
(5.23)
X(z)
1+ 

285 
Section 5.2 
System Functions-linear Constant-Coefficient Difference Equations 
Thus, 
(1+1z-1 
~C2)Y(Z) (1+2z- 1 +z-2)X(z), 
and the difference equation is 
y[n] + ly[n - 1] 
~ y[n 
2] 
x[n] + 2xLn 
1] + x[n 
2]. 
(5.24) 
5.2.1 Stability and Causality 
To obtain Eq. (5.20) from Eq. (5.19), we assumed that the system was linear and time 
invariant, so that Eq. (5.2) applied, but we made no further assumption about stability 
or causality. Correspondingly, from the difference equation, we can obtain the algebraic 
expression for the system function, but not the region of convergence (ROC). Specif­
ically, the ROC of H (z) is not determined from the derivation leading to Eq. (5.20), 
since all that is required for Eq. (5.20) to hold is that X(z) and Y(z) have overlapping 
ROCs. This is consistent with the fact that, as we saw in Chapter 2, the difference equa­
tion does not uniquely specify the impulse response of an LTI system. For the system 
function ofEq. (5.20) or (5.21), there are a number of choices for the ROC. For a given 
ratio of polynomials, each possible choice for the ROC will lead to a different impulse 
response, but they will all correspond to the same difference equation. However, if we 
assume that the system is causal, itfollows that h[n] must be a right-sided sequence, and 
therefore, the ROC of H (z) must be outside the outermost pole. Alternatively, if we 
assume that the system is stable, then, from the discussion in Section 2.4, the impulse 
response must be absolutely summable, i.e., 
ocL Ih[nll < 00. 
(5.25) 
n=-oo 
Since Eq. (5.25) is identical to the condition that 
L 
00 
Ih[n]z-nl < 00 
(5.26) 
n=-oo 
for Iz 1 
1, the condition for stability is equivalent to the condition that the ROC of 
H (z) includes the unit circle. Determining the ROC to associate with the system function 
obtained from the difference equation is illustrated in the following example. 
Example 5.2 
Determining the ROC 
Consider the LTI system with input and output related through the difference equation 
y[n] 
~y[n 
1] + y[n - 2] = x[n]. 
(5.27) 
From the previous discussions, the algebraic expression for H (z) is given by 
1 
1 
H(z) = 
-
(5.28) 
1 -
-
(1 - ic 1) (1 - 2C1) . 
The corresponding pole-zero plot for H(z) is indicated in Figure 5.7. There are three 
possible choices for the ROC. If the system is assumed to be causal, then the ROC 

286 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
is outside the outermost pole, i.e., Izl > 2. In this case, the system will not be stable, 
since the ROC does not include the unit circle. If we assume that the system is stable, 
then the ROC will be i < Iz I < 2, and h[n] will be a two-sided sequence. For the third 
possible choice of ROC, Izl < i, the system will be neither stable nor causal. 
Im 
z-plane 
Unit circle 
---+------{(o) 
)( 
)( 
1 
2 
Re 
"2 
Figure 5.7 
Pole-zero plot for Example 5.2. 
As Example 5.2 suggests, causality and stability are not necessarily compatible 
requirements. For an LTI system whose input and output satisfy a difference equation 
of the form of Eq. (5.19) to be both causal and stable, the ROC of the corresponding 
system function must be outside the outermost pole and include the unit circle. Clearly, 
this requires that all the poles of the system function be inside the unit circle. 
5.2.2 Inverse Systems 
For a given LTI system with system function H(z), the corresponding inverse system 
is defined to be the system with system function Hi (z) such that if it is cascaded with 
H(z), the overall effective system function is unity; i.e., 
G(z) 
H(z)Hj(z) = 1. 
(5.29) 
This implies that 
1 
H·(z) -­
I 
. 
-
H(z)' 
(5.30) 
The time-domain condition equivalent to Eq. (5.29) is 
g[n] 
h[n] * hdn] = 8[n]. 
(5.31) 
From Eq. (5.30), the frequency response of the inverse system, if it exists, is 
Hi(e
jW 
) = 
1 
. 
H(eJW )' 
(5.32) 

287 
Section 5.2 
System Functions-Linear Constant-Coefficient Difference Equations 
i.e., Hi(e jW ) is the reciprocal of H(e jW ). Equivalently, the log magnitude, phase, and 
group delay of the inverse system are negatives of the corresponding functions for 
the original system. Not all systems have an inverse. For example, the ideallowpass 
filter does not. There is no way to recover the frequency components above the cutoff 
frequency that are set to zero by such a filter. 
Many systems do have inverses, and the class of systems ,",ith rational system 
functions provides a very useful and interesting example. Specifically, consider 
M 
1
Tl (1 - CkZ- ) 
H(z) = (hO) k=l 
, 
(5.33)
ao 
NTl(l - dkZ-1) 
k=l 
with zeros at z 
Ck and poles at z = db in addition to possible zeros and/or poles at 
Z = 0 and z 
co. Then 
NTl (1 - dkZ-1) 
Hi(Z) = (:~) k=:/ 
1 
(5.34) 
Tl (1 - Ckz- ) 
k=l 
i.e., the poles of Hi (z) are the zeros of H (z) and vice versa. The question arises as to 
what ROC to associate with Hi (z). The answer is provided by the convolution theorem, 
expressed in this case by Eq. (5.31). For Eq. (5.31) to hold, the ROC of H(z) and Hi(Z) 
must overlap. If H (z) is causal, its ROC is 
Izl > max Idkl. 
(5.35)
k 
Thus, any appropriate ROC for Hi (z) that overlaps with the region specified by Eq. (5.35) 
is a valid ROC for Hi (z). Examples 5.3 and 5.4 will illustrate some of the possibilities. 
Example 5.3 
Inverse System for 1st-Order System 
Let H(z) be 
1- 0.5z-1 
H(z) = 1 -0.gel 
with ROC Izl > 0.9. Then Hi(Z) is 
Hi(Z) 
Since Hi (z) has only one pole, there are only two possibilities for its ROC, and the 
only choice for the ROC of Hi(Z) that overlaps with Izi > 0.9 is Izi > 0.5. Therefore, 
the impulse response of the inverse system is 
hi[n] 
(O.5)llu[nJ - O.9(O.5),,-lu[n 
1]. 
In this case, the inverse system is both causal and stable. 

288 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
Example 5.4 
Inverse for System with a Zero in the ROC 
Suppose that H(z) is 
z-1 - 0.5 
H (z) - ------,;­
Izl > 0.9. 
-
1 - 0.ge 1 ' 
The inverse system function is 
1 - 0.9z- 1 
-2 + 1.8z-1 
H'(z) - --:---­
I 
-
e1-O.5 
1-2e1 ' 
As before, there are two possible ROes that could be associated with this al­
gebraic expression for Hi (z): Izl < 2 and Izl > 2. In this case, however, both regions 
overlap with Izl > 0.9, so both are valid inverse systems. The corresponding impulse 
response for an ROC Izl < 2 is 
hn[n] = 2(2)n u[-n -
1]-1.8(2)n-lu[~n] 
and, for an ROC Izl > 2, is 
hdn] = -2(2)nu[n] + 1.8(2)n-lu [n - 1]. 
We see that hn[n] is stable and noncausal, while hi2[n] is unstable and causal. Theo­
retically, either system cascaded with H(z) will result in the identity system. 
A generalization from Examples 5.3 and 5.4 is that if H (z) is a causal system with 
zeros at Ck, k = 1, ..., M, then its inverse system will be causal if and only if we associate 
the ROC, 
Izl > max ICkI, 
k 
with Hi (z). If we also require that the inverse system be stable, then the ROC of Hi (z) 
must include the unit circle, in which case 
max ICkI < 1; 
k 
i.e., all the zeros of H (z) must be inside the unit circle. Thus, an LTI system is stable 
and causal and also has a stable and causal inverse if and only if both the poles and the 
zeros of H (z) are inside the unit circle. Such systems are referred to as minimum-phase 
systems and will be discussed in more detail in Section 5.6. 
5.2.3 Impulse Response for Rational System Functions 
The discussion of the partial fraction expansion technique for finding inverse z-transforms 
(Section 3.3.2) can be applied to the system function H (z) to obtain a general expression 
for the impulse response of a system that has a rational system function as in Eq. (5.21). 
Recall that any rational function of C 1 with only pt-order poles can be expressed in 
the form 
M-N 
N 
Ak
H(z) = " 
~ Brz-r"
+ ~-1
d -1' 
(5.36) 
-
kZ
r=O 
k=1 
where the terms in the first summation would be obtained by long division ofthe denom­
inator into the numerator and would be present only if M ::: N. The coefficients Ak in 
the second set of terms are obtained using Eq. (3.43). If H (z) has a multiple-order pole, 

Section 5.2 
System Functions-Linear Constant-Coefficient Difference Equations 
289 
its partial fraction expansion would have the form of Eq. (3.46).1£ the system is assumed 
to be causal, then the ROC is outside all of the poles in Eq. (5.36), and it follows that 
M-N 
N 
h[n] 
L 
Bro[n - r] + LAkdku[n], 
(5.37) 
r=O 
k=1 
where the first summation is included only if M 2: N. 
In discussing LTI systems, it is useful to identify two classes. In the first class, at 
least one nonzero pole of H(z) is not canceled by a zero. In this case, h[n] will have at 
least one term of the form A k(dk)" u[n], and h[n] will not be of finite length, i.e., will not 
be zero outside a finite interval. Consequently, systems of this class are infinite impulse 
response (IIR) systems. 
For the second class of systems, H (z) has no poles except at z 
0; Le., N = 0 
in Eqs. (5.19) and (5.20). Thus, a partial fraction expansion is not possible, and H(z) is 
simply a polynomial in 
of the form 
M 
H(z) = L bkZ-k . 
(5.38) 
k=O 
(We assume, without loss of generality, that ao = 1.) In this case, H (z) is determined to 
within a constant multiplier by its zeros. From Eq. (5.38), h[n] is seen by inspection to 
be 
M 
0::: n 
M,
h[n] = Lbk8[n - k] = {~~' 
(5.39)
otherwise. 
k=O 
In this case, the impulse response is finite in length; i.e., it is zero outside a finite intervaL 
Consequently, these systems are finite impulse response (FIR) systems. Note that for 
FIR systems, the difference equation ofEq. (5.19) is identical to the convolution sum, i.e., 
M 
y[n] = L bkx[n - kJ. 
(5.40) 
k=O 
Example 5.5 gives a simple example of an FIR system. 
Example 5.5 
A Simple FIR System 
Consider an impulse response that is a truncation of the impulse response of an IIR 
system with system function 
1 
G(z) = 
-1' 
1.::1 > lal,
I-az 
i.e.,  
a" 
0 < n < M 
h[ ] 
,-
-
, 
n = { 0 
otherwise. 
Then, the system function is 
M 
1- aM+1z-M-l 
H(z) = L a"z-II = ----,---
(5.41) 
11=0 
1­

290 
Chapter 5 
Transform Analysis of UnearTime-lnvariant Systems 
Since the zeros of the numerator are at z-plane locations 
Zk = aei2:rk/(M+1) 
k=O,l, ... ,M,  
(5,42) 
where a is assumed real and positive, the pole at Z = a is canceled by the zero denoted 
Z00 The pole-zero plot for the case M = 7 is shown in Figure 5.8. 
The difference equation satisfied by the input and output of the Lfl system is 
the discrete convolution 
M 
y[nJ = L akx[n - kJ.  
(5.43) 
k=O 
However, Eq. (5.41) suggests that the input and output also satisfy the difference 
equation 
y[n] - ay[n - 1] = x[nJ - a M +1x[n - M -I). 
(5.44) 
These two equivalent difference equations result from the two equivalent forms of 
H(z) in Eq. (5.41). 
Im 
z-plane 
-''0 
\ 
\ 
\ 
\ 
la  
Re 
I 
/ 
I 
/0 
~/ 
Figure 5.8 Pole-zero plot for Example 5.5. 
5.3  FREQUENCY RESPONSE FOR RATIONAL SYSTEM 
FUNCTIONS 
If a stable LTI system has a rational system function, i.e., if its input and output satisfy 
a difference equation ofthe form ofEq. (5.19), then its frequency response (the system 
function of Eq. (5.20) evaluated on the unit circle) has the form 
M 
iwk
l::>ke­
H(eiw ) = k=O  
(5.45)
N 
Lake-iwk  
k=O  

291 
Section 5.3 
Frequency Response for Rational System Functions 
That is, H (e jUJ) is a ratio of polynomials in the variable e- jUJ. To determine the magni­
tude, phase, and group delay associated with the frequency response of such systems, it 
is useful to express H (e jW) in terms of the poles and zeros of H (z). Such an expression 
results from substituting z = ejUJ into Eq. (S.21) to obtain 
M 
jW
n(1-cke-
) 
H (e jUJ) 
(:~) _k~_l_____ 
(S.46) 
jUJ )
n(1 
dke­
k=l 
From Eq. (S.46), it follows that 
M nil - qe-jUJI 
IH(ejW)1 = 1~I_k~_l___ _ 
(S.47) 
nil dke-jwl 
k=l 
Correspondingly, the magnitude-squared function is 
M 
2 n(1 -
Cke- jU») (1 - c'keJW )
(ho)
IH(ej('»)12 = H(ejW)H*(ejW) = 
_k=_l________ 
(S.48)
ao 
Nn(1 - dke- jUJ) (1 
d;eJW ) 
From Eq. (S.47), we note that IH(ejW)1 is the product of the magnitudes of all the zero 
factors of H (z) evaluated on the unit circle, divided by the product of the magnitudes 
of all the pole factors evaluated on the unit circle. Expressed in decibels (dB), the gain 
is defined as 
Gain in dB 
2010glO IH(ejUJ)1 
(S.49) 
M 
Gain in dB = 2010glO I:0 I+ L 2010g10 11 - qe- jWI 
o 
k=l 
N 
(S.SO) 
L20loglO 11 
dke-jwl. 
k=l 
The phase response for a rational system function has the form 
M 
N 
jw
jUJ
arg[H(e}W)] =arg[:~J + Larg[l- q e-
] - Larg[l- dke-
], 
(S.51) 
k=l 
k=l 
where arg[] represents the continuous (unwrapped) phase. 
The corresponding group delay for a rational system function is 
. 
N 
d 
. 
M 
d 
grd[H(eJUJ)] = '" -(arg[1 - dke-JW]) - '" -(arg[1 
qe-J(U]). 
(S.S2)
~dw 
~dw 
k=l 
k=l 

292 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
An equivalent expression is 
2 
jW  
jW
grd[H(ejW)] =  ~ Idkl -
Re{dke-
} 
_ ~ led - Re{qe-
} 
. 
(5.53) 
~ 1 + Idkl2 -
2Re{dke-JW} 
~ 1 + 1ck1 2 - 2Re{qe-JW }
k=l  
k=l 
In Eq. (5.51), as written, the phase of each of the terms is ambiguous; i.e., any integer 
multiple of 2n can be added to each term at each value of w without changing the 
value of the complex number. The expression for the group delay, on the other hand, is 
defined in terms of the derivative of the unwrapped phase. 
Equations (5.50), (5.51), and (5.53) represent the magnitude in dB, the phase, and 
the group delay, respectively, as a sum of contributions from each of the poles and zeros 
of the system function. Con seq uently, to gain an understanding of how the pole and zero 
locations of higher-order stable systems impact the frequency response, it is useful to 
consider in detail the frequency response of 1 st-order and 2nd -order systems in relation 
to their pole and zero locations. 
5.3.1 Frequency Response of 1 st-Order Systems 
jO
In this section, we examine the properties of a single factor of the form (1 - re
e-jW), 
where r is the radius and e is the angle of the pole or zero in the z-plane. This factor is 
typical of either a pole or a zero at a radius r and angle e in the z-plane. 
The square of the magnitude of such a factor is 
jO  
jO
11 - re
e-jWI2 = (1 - re
e-jW)(1 - re- jO ejW) = 1 + r2 - 2r cos(w - e). 
(5.54) 
The gain in dB associated with this factor is 
jO
(+/- )20 10glO 11 - re
e-jWI = (+/-)10 10glO[I + r2 - 2r cos(w - e)], 
(5.55) 
with a positive sign if the factor represents a zero and a negative sign if it represents a 
pole. 
The contribution to the principal value of the phase for such a factor is 
'0 
"  
[ 
r sinew - e) 
]
(+/-)ARG[l- reJ e-JW] = (+/-) arctan  
. 
(5.56)
1 - r cos(w - e) 
Differentiating the right-hand side of Eq. (5.56) (except at discontinuities) gives the 
contribution to the group delay of the factor as 
"0 
" 
r2 -
r cos(w - e) 
r2 -
r cos(w - e)
(+/-)grd[l- reJ e-JW] = (+/-)  
= (+/-) 
" 
.
1 + r2 - 2r cos(w - e) 
11 - reJIi e-JWI2 
(5.57) 
again, with the positive sign for a zero and a negative sign for a pole. The functions in 
Eqs. (5.54)-(5.57) are, of course, periodic in w with period 2n. Figure 5.9(a) shows a 
plot of Eq. (5.55) as a function of w over one period (0 :s w < 2n) for several values of 
e with r = 0.9. 
Figure 5.9(b) shows the phase function in Eq. (5.56) as a function of w for r = 0.9 
and several values of e. Note that the phase is zero at w = e and that, for fixed r, the 
function simply shifts with e. Figure 5.9( c) shows the group delay function in Eq. (5.57) 
for the same conditions on rand e. Note that the high positive slope of the phase around 
w = ecorresponds to a large negative peak in the group delay function at w = e. 
In inferring frequency response characteristics from pole-zero plots of either 
continuous-time or discrete-time systems, the associated vector diagrams in the 

Q 
d '" 
:.a 
d 
~ 
--00 
_._. 0  !!. 
2 
--- 0  
1T 
10,--------------------------------------, 
5 
o 
-5 
-10 
-15 
-20 
-25L-----____L-________~________L_______~ 
o 
1.5 
1.0 
0.5 
0 
-0.5 
-1.0 
-1.5 0 
E: 
2 
2.---------------------------------------, 
O~-~~·,------.-~~~,------/~,~---------.~ 
• 
I 
\ 
I 
\. 
\ 
I 
-2 
\. ,! 
\, ' 
,. 
,'  
• , 
I ' 
,.,. 
I ' 
I. 
" 
" 
"
-6 
I.'/ 
" 
.. 
.' 
-8 
~ 
~ 
-10L-------~1---------~1--------L-1--____~ 
o 
E: 
1T 
37T 
2 
Radian frequency (Il'» 
2 
(c) 
1T 
Radian frequency (w) 
(a) 
1T 
Radian frequency (w) 
(b) 
Figure 5.9 
Frequency response for a single zero, with r 
0.9 and the three 
values of (;! shown. (a) Log magnitude. (b) Phase. (c) Group delay. 
293 

294 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
Im 
z-plane 
</13 
'Re 
Fipure 5_10 
z-plane vectors for a 
t 5 -order system function evaluated on 
the unit circle, with r < 1. 
complex plane are typically helpful. In this construction, each pole and zero factor can 
be represented by a vector in the z-plane from the pole or zero to a point on the unit 
circle. For a 1st-order system function of the form 
j8
(z 
re
)
j8
1
H(z) = (1 - re
z- )  
r < 1, 
(5.58) 
z 
the pole-zero pattern is illustrated in Figure 5.10. Also indicated in this figure are the 
jw
j8
vectors VI, V2, and V3 = VI -
V2, representing the complex numbers e
, re
, and 
(ejw -
reF'), respectively. In terms of these vectors, the magnitude of the complex 
number 
ejw 
re j8 
is the ratio of the magnitudes of the vectors V3 and VI, i.e., 
jw 
j8 I
8' 
Ie 
-
re 
= IV31
11- reJ e-Jwl  
-, 
(5.59)
IVII 
or, since IVII = 1, Eq. (5.59) is just equal to IV31. The corresponding phase is 
L(1 
re j8e-j (t)) = L(ejw 
rej8 ) 
L(ejW) = L(V3) 
L(v}) 
(5.60) 
= ¢3 
¢I = ¢3 -
OJ. 
Thus, the contribution of a single factor (1 
rej8z-1) to the magnitude function at 
frequency OJ is the length of the vector V3 from the zero to the point z =ejw on the unit 
circle. The vector has minimum length when OJ = (). This accounts for the sharp dip in 
the magnitude function at OJ = ein Figure 5.9(a). The vector VI from the pole at z = Oto 
z = ejw always has unit length. Thus, it does not have any effect on the magnitude re­
sponse. Equation (5.60) states that the phase function is equal to the difference between 
the angle of the vector from the zero at rej9 to the point z 
ej(U and the angle of the 
ejw
vector from the pole at z = 0 to the point z 
. 
The dependence of the frequency-response contributions of a single factor 
j8
(1 - re e-jW) on the radius r is shown in Figure 5.11 for e= 1f and several values of r. 

10 
0 
t.Il 
"0  -10 
-20 
-30 
0 
1T 
1T 
31T 
21T 
2 
Radian frequency (w) 
2 
(a) 
'" 
~ 
:a
0:1 
0:1 
~ 
Radian frequency (w) 
2 
1 
0 
-1 
(b) 
2,--------------------------------------,  
_._. r  
-6 r­
0.5 
---r 0.7 
-8 I­
......... r  0.9 
_10~________L_ 
L_
I 
________L_______~
I  
________ 
I
--r=l 
o 
1T 
1T 
31T 
2  
2
Radian frequency ({j) 
(c) 
Figure 5.11 
Frequency response for a single zero, with () 
Jr, r 
1, 0.9, 0.7, 
and 0.5. (a) Log magnitude. (b) Phase. (c) Group delay for r =0.9, 0.7, and 0.5. 
295 

296 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
Note that the log magnitude function plotted in Figure 5.ll(a) dips more sharply as 
r becomes closer to 1; indeed, the magnitude in dB approaches -00 at w = e as r 
approaches 1. The phase function plotted in Figure S.ll(b) has positive slope around 
w = e, which becomes infinite as r approaches 1. Thus, for r = 1, the phase function is 
discontinuous, with a jump of 1T radians at w = e. Away from w = e, the slope of the 
phase function is negative. Since the group delay is the negative of the slope of the phase 
curve, the group delay is negative around w = e, and it dips sharply as r approaches 1 
becoming an impulse (not shown) when r = 1. Figure S.ll(c) shows that as we move 
away from w 
e, the group delay becomes positive and relatively flat. 
5.3.2 Examples with Multiple Poles and Zeros 
In this section, we utilize and expand the discussion of Section 5.3.1 to determine the 
frequency response of systems with rational system functions. 
Example 5.6 
2 nd·Order IIR System 
Consider the 2nd -order system 
1 
1 
H(z) = 
= 
1 
2 
2' 
(5.61)
(1 
1 
2rcosec +r C 
The difference equation satisfied by the input and output of the system is 
y[n]-2rcosey[n 
1]+r2y[n 
2J=x[n]. 
Using the partial fraction expansion technique, we can show that the impulse response 
of a causal system with this system function is 
rn sin[e(n + 1)] 
(562)
h[n] = 
. 
urn]. 
. 
sme 
The system function in Eq. (5.61) has a pole at z = rejf) and at the conjugate 
location, z = re- jf), and two zeros at z = O. The pole-zero plot is shown in Figure 5.12. 
Im 
z-plane 
Unit circle 
Figure 5.12 
Pole-zero plot for Example 5.6. 

297 
Section 5.3 
Frequency Response for Rational System Functions 
From our discussion in Section 5.3.1, 
20 loglO IH (eiw)I = - 10 loglO[1 + r2 - 2r cos(w - 8)] 
(5.63a) 
-
10 loglO[1 + r2 - 2r cos(w + 8)], 
r sinew - 8) 
r sin(w + 8) 
]
arctan 
- arctan 
, 
(5.63b)
rcos(w 
8) 
[ 1 - rcos(w + 8) 
and 
jw 
r2 - r cos(w - 8) 
r2 - r cos(w + 8) 
grd[H(e 
)] = -
(5.63c)
1 + 
2r cos(w - 8) 
1 + r2 
2r cos(w + 8) 
Thesc functions are plotted in Figure 5.13 for r 
0.9 and 8 
rrj4. 
Figure 5.12 shows the pole and zero vectors VI, v2, and V3. The magnitude re­
sponse is the product of the lengths of the zero vectors (which in this case are always 
unity), divided by the product of the lengths of the pole vectors. That is, 
1 
(5.64) 
When w "'" e, the length of the vector VI = eiw 
rei£) becomes small and changes 
jw
significantly as w varies about 8, whereas the length of the vector v2 = e
- re- j£) 
changes only slightly as w varies around w = 8. Thus, the pole at angle 8 dominates 
the frequency response around w 
8, as is evident from Figure 5.13. By symmetry, 
the pole at angle -8 dominates the frequency response around w 
-8. 
20,---------------------------------------~ 
15 
10 
5 
o 
-6 
1T 
1T 
31T 
21T 
2 
2 
Radian frequency (w) 
(a) 
Figure 5.13 
Frequency response for a complex-conjugate pair of poles as in 
Example 5.6, with r = 0.9, 8 = rrj4. (a) Log magnitude. 

298 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
19 
] '" 
~ 
71" 
71" 
371" 
271" 
2 
2
Radian frequency (w) 
(b) 
10  
8'­
6  
'" 
<!) -a 4
e 
v:l '" 
-2L!----------~----------~--------~----------~ 
71" 
371" 
271" 
2
Radian frequency (w) 
(c) 
Figure 5.13 
(continued) Frequency response for a complex-conjugate pair of 
poles as in Example 5.6, with r 
0.9,0 
1rj4. (b) Phase. (c) Group delay. 
2 
o 
71" 
2 
Example 5.7 
2 nd·Order FIR System 
In this example we consider an FIR system whose impulse response is 
h[n] = 8[n] 
2r cos08[n 
1] + r 28[n - 2]. 
(5.65) 
The corresponding system function is 
H(z) 
1 - 2r cos 
+ r2.-2, 
(5.66)
~ 
which is the reciprocal ofthe system function in Example 5.6. Therefore, the frequency­
response plots for this FIR system are simply the negative of the plots in Figure 5.13. 
Note that the pole and zero locations are interchanged in the reciprocal. 

299 
Section 5.3 
Frequency Response for Rational System Functions 
Example 5.8 
3 rd-order IIR System 
In this example, we consider a lowpass filter designed using one of the approximation 
methods to be described in Chapter 7. The system function to be considered is 
0.05634(1 + c 1)(1 -1.0166z-1 + z-2) 
(5.67)
H(z) = -(1--0.-6-83-z---=-1)-(1---1-.4-46-1-z--::-1-+-0-.7-9-57-z---=-2)' 
and the system is specified to be stable. The zeros of this system function are at the 
following locations: 
Radius 
Angle 
1 
n rad 
1 
±1.0376 rad (59.45°) 
The poles are at the following locations: 
Radius 
Angle 
0.683 
o 
0.892 
±0.6257 rad (35.85°) 
The pole-zero plot for this system is shown in Figure 5.14. Figure 5.15 shows the log 
magnitude, phase, and group delay of the system. The effect of the zeros that are on the 
unit circle at w = ±1.0376 and rr is clearly evident. However, the poles are placed so 
that, rather than peaking for frequencies close to their angles, the total log magnitude 
remains close to 0 dB over a band from w 
0 to w = O.2rr (and, by symmetry, 
from w 
1.8rr to w 
2rr), and then it drops abruptly and remains below -25 dB 
from about w 
O.3rr to 1.7n. As suggested by this example, useful approximations 
to frequency-selective filter responses can be achieved using poles to build up the 
magnitude response and zeros to suppress it. 
Im 
z-plane 
x 
Re 
x 
.13. 
Figure 5.14 
Pole-zero plot for the lowpass filter of Example 5.B. 

20r,-------------------------------------------------, 
-100~'----------~------------~-----------L----------~ 
1T 
31T 
2 
21T 
300 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
o 
-20 
~ -40 
-60 
-so 
Radian frequency (w) 
(a) 
~ 
:0 '" 
~ 
4rl----------------------------------------------------, 
o 
1T 
2 
-4 0 
10 
1T 
2 
1T 
Radian frequency (w) 
(b) 
31T 
2 
271" 
sL 
'" .a 
6 
~ 
CI) 
4 
2 
0 
0 
1T 
271"
!!. 
2 
2 
Radian frequency (w) 
(c) 
Figure 5.15 
Frequency response for the lowpass filter of Example 5.B. (a) Log 
magnitude. (b) Phase. (c) Group delay. 

301 
Section 5.4 
Relationship between Magnitude and Phase 
In this example, we see both types of discontinuities in the plotted phase curve. At 
(J) "'" 0.22;rr, there is a discontinuity of 2;rr owing to the use of the principal value in plotting. 
At (J) = ± 1.0376 and (J) 
;rr, the discontinuities of ;rr are due to the zeros on the unit circle. 
5.4 RELATIONSHIP BETWEEN MAGNITUDE AND PHASE 
In general, knowledge about the magnitude of the frequency response of an LTI system 
provides no information about the phase, and vice versa. However, for systems described 
by linear constant-coefficient difference equations, Le., rational system functions, there 
is some constraint between magnitude and phase. In particular, as we discuss in this 
section, if the magnitude of the frequency response and the number of poles and zeros 
are known, then there are only a finite number of choices for the associated phase. 
Similarly, if the number of poles and zeros and the phase are known, then, to within a 
scale factor, there are only a finite number of choices for the magnitude. Furthermore, 
under a constraint referred to as minimum phase, the frequency-response magnitude 
specifies the phase uniquely, and the frequency-response phase specifies the magnitude 
to within a scale factor. 
To explore the possible choices of system function, given the square of the mag­
nitude of the system frequency response, we consider IH (ejW )12 expressed as 
IH(ejW )12 = H(eJW)H*(eJW) 
(5.68) 
= H(z)H*(1/z*)l z=ejw , 
Restricting the system function H(z) to be rational in the form of Eq. (5.21), i.e., 
(5.69)  
we see that H*(1/z*) in Eq. (5.68) is 
M n(1- ckZ ) 
ho) k=l 
, 
(5.70)
H* (:*) 
( ao 
Nn(1 - dkz) 
k=l 
wherein we have assumed that ao and ho are reaL Therefore, Eq. (5.68) states that the 
square of the magnitude of the frequency response is the evaluation on the unit circle 

Chapter 5 
Transform Analysis of Linear Time-Invariant Systems
302 
of the z-transform 
C(z) = H(z)H*(1/z*) 
(5.71) 
Mn(1 
CkZ-l )(1 
ckZ) 
= (:~r _k;....;;l______ 
(5.72) 
n(1 - dkZ-1)(1- dkz) 
k=l 
If we know IH(ejW )12 expressed as a function of ejw, then by replacing eJw by z, we 
can construct C(z). From C(z), we would like to infer as much as possible about H(z). 
We first note that for each pole dk of H(z), there is a pole of C(z) at dk and (dk)-l. 
Similarly, for each zero Ck of H (z), there is a zero of C (z) at Ck and (cD -1. Consequently, 
the poles and zeros of C(z) occur in conjugate reciprocal pairs, with one element of 
each pair associated with H(z) and one element of each pair associated with H*(l/z*). 
Furthermore, if one element of each pair is inside the unit circle, then the other (i.e., 
the conjugate reciprocal) will be outside the unit circle. The only other alternative is for 
both to be on the unit circle in the same location. 
If H(z) is assumed to correspond to a causal, stable system, then all its poles must 
lie inside the unit circle. With this constraint, the poles of H (z) can be identified from the 
poles of C(z). However, with this constraint alone, the zeros of H (z) cannot be uniquely 
identified from the zeros of C(z). This can be seen from the following example. 
Example 5.9 Different Systems with the Same ClzJ 
Consider two different stable systems with system functions 
H1(Z) = (1 
(5.73) 
and 
1
1
H 
-
(1-C )(1+2z- ) 
574 
2(") 
(1- 0.8ejrr/4c1)(1 
0.8e-jrr/4c1)· 
(. ) 
The pole-zero plots for these systems are shown in Figures 5.16(a) and 5.l6(b), re­
spectively. The two systems have identical pole locations and both have a zero at z 
1 
but differ in the location of the second zero. 
Now, 
Cl(Z) = H 1(z)H1'(1/z*) 
(5.75)
2(1 - z-l )(1 +0.5e1)2(1 - z)(1 + O.5z)  
(1 
0.8ejrr/4c1)(1 
0.8e-Jrr/4C l )(1- 0.8e- jrr/4z)(1- 0.8ejrr/4z)  
and 
C2(Z) = H2(Z)H2(1/z*) 
(1 - z-l)(1 + 2z-1)(1- z)(1 + 2z) 
(5.76) 
(l - 0.8eJrr/4c1)(1 - 0.8e-jrr/4c1)(1 - 0.8e-jrr/4z)(1 
0.8ejrr/4z) . 
lit.. 

1 
303 
Section 5.4 
Relationship between Magnitude and Phase 
4(1 + O.Sz-l)(1 + O.Sz) = (1 + 2z-1)(1 + 2z). 
(S.77) 
= C2(Z). The pole-zero plot forCI (z) and C2(Z), which are identical, 
Using the fact that 
we see that Cl (z) 
Unit circle 
" 
is shown in Figure S.16(c). 
Im 
X 
X 
Im
z-plane 
z-plane 
Unit circle 
X 
Re 
Re 
X 
(a) 
(b) 
Figure 5.16 
Pole-zero plots fortwo system functions and their common magnitude­
squared function. (a) H1 (2). (b) H2(Z). (c) C1 (2), G2(Z), 
The system functions HI (z) and H 2(z) in Example 5.9 differ only in the location 
of one ofthe zeros. In the example, the factor 2(1 + 0.5z-1) 
(Z-l + 2) contributes the 
same to the square of the magnitude of the frequency response as the factor (1 +2z-I ), 
and consequently, IHI (eiW)1 and IH2(eiw)1 are equal. However, the phase functions for 
these two frequency responses are different. 
Im 
z-plane 
Unit circle 
X 
X 
(c) 

304 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
Example 5.10 
Determination of HfzJ from CfzJ 
Suppose we are given the pole-zero plot for e(z) in Figure 5.17 and want to determine 
the poles and zeros to associate with H (z). The conjugate reciprocal pairs of poles and 
zeros for which one element of each is associated with H(z) and one with H*(l/z*) 
are as follows: 
. 
Pole pair 1 : 
(Pb P4) 
Pole pair 2 : 
(P2, P5) 
Pole pair 3 : 
(P3, P6) 
Zero pair 1 : 
(z 1, Z4) 
Zero pair 2 : 
(Z2, 
Zero pair 3 : 
(Z3, Z6) 
Knowing that H(z) corresponds to a stable, causal system, we must choose the poles 
from each pair that are inside the unit circle, i.e., PI, P 2. and P3' No such constraint 
is imposed on the zeros. However, if we assume that the coefficients ak and bk are 
real in Eqs. (5.19) and (5.20), the zeros (and poles) either are real or occur in complex 
conjugate pairs. Consequently, the zeros to associate with H (z) are 
Z3 
or 
Z6 
and 
(Zl,ZZ) or 
(Z4,ZS)' 
Therefore, there are a total of four different stable, causal systems with three poles 
and three zeros for which the pole-zero plot of e(z) is that shown in Figure 5.17 and, 
equivalently, for which the frequency-response magnitude is the same. If we had not 
assumed that the coefficients ak and bk were real, the number of choices would be 
greater. Furthermore, if the number of poles and zeros of H(z) were not restricted, 
the number of choices for H(z) would be unlimited. To see this, assume that H(z) has 
a factor of the form 
Z-l - a* 
1-az-1' 
i.e., 
-a* 
H(z) 
HI (z) -
1 . 
(5.78)
l-az-
Factors of this form represent all-pass factors, since they have unity magnitude on the 
unit circle; they are discussed in more detail in Section 5.5. It is easily verified that for 
H(z) in Eq. (5.78), 
CCz) = H(z)H*(l/z*) 
H1(Z)Hi(1/z*); 
(5.79) 
i.e., all-pass factors cancel in e (z) and therefore would not be identifiable from the 
pole-zero plot of CCz). Consequently, if the number of poles and zeros of H(z) is 
unspecified, then, given e (z), any choice for H (z) can be cascaded with an arbitrary 
number of all-pass factors with poles inside the unit circle (i.e., lal < 1). 

Section 5.5 
All-Pass Systems 
305 
z-plane 
Unit circle 
XP4 
XPs 
o zs 
Figure 5.17 
Pole-zero plot for the magnitude-squared function in Example 5.10. 
5.5 ALL-PASS SYSTEMS 
As indicated in the discussion of Example 5.10, a stable system function of the form 
Z-l - a* 
Hap(z) = -1------,-
(5.80) 
has a frequency-response magnitude that is independent of w. This can be seen by 
writing Hap(eiw) in the form 
(5.81)  
In Eq. (5.81), the term e- jw has unity magnitude, and the remaining numerator and 
denominator factors are complex conjugates of each other and therefore have the same 
magnitude. Consequently, IHap(eiw)1 = 1. A system for which the frequency-response 
magnitude is a constant, referred to as an all-pass system, passes all of the frequency 
components of its input with constant gain or attenuation.3 
The most general form for the system function of an all-pass system with a real­
valued impulse response is a product of factors like Eq. (5.80), with complex poles being 
3In some discussions, an all-pass system is defined to have gain of unity. In this text, we use the term 
all-pass system to refer to a system that passes all frequencies with a constant gain A that is not restricted to 
be unity. 

306 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
4
-3 
Unit 
circle 
3 
4 
Im 
0.8 
z-plane 
2 
Re 
x 
o 
Figure 5.18 
Typical pole-zero plot for 
an all-pass system. 
paired with their conjugates; i.e., 
Mr Z-l _ dk Me 
(Z-l 
ek)(z-l 
ek) 
(5.82)
H ap(Z) 
A :n 1 _ dkZ-1 :n (1 _ ekZ-1)(1 
eZz 1)' 
k=l 
k=l 
where A is a positive constant and the dkS are the real poles, and the ekS the complex 
poles, of Hap(z). For causal and stable all-pass systems, Idkl < 1 and lekl < 1. In terms 
of our general notation for system functions, all-pass systems have M 
N = 2M c +Mr 
poles and zeros. Figure 5.18 shows a typical pole-zero plot for an all-pass system. In 
this case Mr = 2 and MeL Note that each pole of Hap(Z) is paired with a conjugate 
reciprocal zero. 
The frequency response for a general all-pass system can be expressed in terms 
of the frequency responses of 1st-order all-pass systems like that specified in Eq. (5.80). 
For a causal all-pass system, each of these terms consists of a single pole inside the unit 
circle and a zero at the conjugate reciprocal location. The magnitude response for such a 
term is unity, as we have shown. Thus, the log magnitude in dB is zero. With a expressed 
in polar form as a = rejfi, the phase function for Eq. (5.80) is 
iW
L [e-
re-ieJ 
-w _ 2 arctan I 
r sinew 
0) 
(5.83)
1 - rej()e-jw 
1 - r cos(w - 0) 
Likewise, the phase of a 2nd -order all-pass system with poles at Z 
reje and 
Z 
re- je is 
L[ (e- jU) ~ re~je)(e-jw . reie ) ] 
r sinew - 0)  
-2w - 2 arctan I 1 _ r cos(w 
0) 
(1 - reJer JW )(l - re-JerJW ) 
(5.84) 
r sinew + f) 
] 
-2 arctan [ 1 _ r cos(w + 0) . 

----------
307 
Section 5.5 
All-Pass Systems 
Example 5.11 
1st. and 2 nd·Order All-Pass Systems 
Figure 5.19 shows plots of the log magnitude, phase, and group delay for two 1 st-order 
all-pass systems, one with a pole at z 
0.9 (0 = 0, r = 0.9) and another with a 
2~--------------------------------~ 
1 ­
~ o----------------------------------~ 
-1 ­
_2L-_______L-1_______~1______~1______~ 
o 
v 
v 
h 
~ 
2 
Radian frequency (w) 
2 
(a) 
4,-----------------------------------, 
2 
\ 
\ 
~ 
".....­
~ 0 -------___ ~" 
\ 
\ 
-4 
0 
2v 
Radian frequency (wl 
(b) 
20 
15 
~ }1O 
en '" 
5 
--z=0.9 
---- z =-0.9 
0 
0 
v 
v 
3v 
2v 
" 
I 
,l 
" 
" 
",I 
,I, , 
, I 
" " 
"I I 
I 
\ 
I 
\ 
/ 
, 
... 
I 
......... 
,) 
2 
Radian frequency (w) 
(e) 
Figure 5.19 
Frequency response for all-pass filters with real poles at z = 0.9 
(solid line) and z 
-0.9 (dashed line). (a) Log magnitude. (b) Phase (principal 
value). (c) Group delay. 

308 
Chapter 5  
Transform Analysis of Linear Time-Invariant Systems 
pole at z 
-0.9 (0 
n, r = 0.9). For both systems, the radii of the poles are r = 0.9. 
Likewise. Figure 5.20 shows the same functions for a 2nd -order all-pass system with 
0.geinj4 and z =
poles at z  
0.ge-in/ 4. 
2r-------------------------------------~ 
11­
~ 01 
-1 c­
-2~1________~__________~________~________~ 
o  
v 
w 
~ 
2 
Radian frequency (w) 
(a) 
.:'"  
'"  
:e 
IX '" 
-2 
-4 
0  
w 
w 
2v 
2 
Radian frequency (w) 
(b) 
20 
15 
<I) '" 
~1O 
en '" 
5 
oLJ ~ 
v 
v 
3v 
2v 
2 
Radian frequency (w) 
2 
(c) 
Figure 5.20 
Frequency response of 2nd-order all-pass system with poles at z = 
O.ge±jnj4. (a) Log magnitude. (b) Phase (principal value). (c) Group delay. 

Section 5.5 
All-Pass Systems 
309 
Example 5.11 illustrates a general property of causal all-pass systems. In Fig­
ure 5.19(b), we see that the phase is nonpositive for 0 < W < 7(. Similarly, in Fig­
ure 5.20(b), if the discontinuity of 27( resulting from the computation of the principal 
value is removed, the resulting continuous-phase curve is nonpositive for 0 < W < 7(. 
Since the more general all-pass system given by Eq. (5.82) is a product of only such l st _ 
and 2nd-order factors, we can conclude that the (unwrapped) phase, arg[H ap(ejW)], of 
a causal all-pass system is always nonpositive for 0 < W < 7(. This may not appear to 
be true if the principal value is plotted, as is illustrated in Figure 5.21, which shows the 
log magnitude, phase, and group delay for an all-pass system with poles and zeros as in 
Figure 5.18. However, we can establish this result by first considering the group delay. 
The group delay ofthe simple one-pole all-pass system ofEq. (5.80) is the negative 
derivative of the phase given by Eq. (5.83). With a small amount of algebra, it can be 
shown that 
= 
1- r2 
grd 
(5.85)
----~----------­
- 2r cos(w - 0) 
11 
Since r < 1 for a stable and causal all-pass system, from Eq. (5.85) the group delay 
contributed by a single causal all-pass factor is always positive. Since the group delay of 
a higher-order all-pass system will be a sum of positive terms, as in Eq. (5.85), it is true 
in general that the group delay of a causal rational all-pass system is always positive. 
This is confirmed by Figures 5.19(c), 5.20(c), and 5.21(c), which show the group delay 
for 1st-order, 2nd-order, and 3fd-order all-pass systems, respectively. 
The positivity of the group delay of a causal all-pass system is the basis for a simple 
proof of the negativity of the phase of such a system. First, note that 
lo
aJ 
arg[Hap(eJaJ)] = -
grd[Hap(eJ¢)]d4> +arg[Hap(eJO)] 
(5.86) 
for 0 ~ W ~ 7(. From Eq. (5.82), it follows that 
Mr 1 _ d 
Me 11 ed 
Hap(eJO) = A TIl _d: TI-----;;; = A. 
(5.87)
11
k=l 
k=l 
ek 
Therefore, arg[H ap(eJo)] = 0, and since 
grd[Hap(eJaJ )] ~ 0, 
(5.88) 
it follows from Eq. (5.86) that 
arg[Hap(ejaJ )] < 0 
forO ~ ill < 7(. 
(5.89) 
The positivity of the group delay and the nonpositivity of the unwrapped phase are 
important properties of causal all-pass systems. 
All-pass systems have importance in many contexts. They can be used as compen­
sators for phase (or group delay) distortion, as we will see in Chapter 7, and they are use­
ful in the theory of minimum-phase systems, as we will see in Section 5.6. They are also 
useful in transforming frequency-selective lowpass filters into other frequency-selective 
forms and in obtaining variable-cutoff frequency-selective filters. These applications are 
discussed in Chapter 7 and applied in the problems in that chapter. 

2 
lL  
p:) 
"0 
0 
-1 
-2 
0  
'If 
2'1f 
2 
2
Radian frequency (w) 
(a)  
4  
a 
en 
'6 
<Il
p:: 
2 
0 
_4LI________J-________L-________L-______~ 
o 
• 
-
'If 
~ 
-
~ 
2 
Radian frequency (w) 
2 
(b) 
121r---------------------------------------------~ 
9 
'" 
2 
~ 
<Zl 
I 
Figure 5.21 
Frequency response for 
00 
• 
'If 
3. 
2'1f 
an all-pass system with the pole-:zero
"2. 
2 
plot in Figure 5.18. (a) Log magmtude. 
Radian frequency (w) 
(b) Phase (principal value). (c) Group 
(c)  
delay. 
310 

Section 5.6 
Minimum-Phase Systems 
311 
5.6 MINIMUM-PHASE SYSTEMS 
In Section 5.4, we showed that the frequency-response magnitude for an LTI system 
with rational system function does not uniquely characterize the system. Ifthe system is 
stable and causal, the poles must be inside the unit circle, but stability and causality place 
no such restriction on the zeros. For certain classes of problems, it is useful to impose 
the additional restriction that the inverse system (one with system function IIH(z» 
also be stable and causal. As discussed in Section 5.2.2, this then restricts the zeros, as 
well as the poles, to be inside the unit circle, since the poles of IIH(z) are the zeros of 
H(z). Such systems are commonly referred to as minimum-phase systems. The name 
minimum-phase comes from a property of the phase response, which is not obvious 
from the preceding definition. This and other fundamental properties that we discuss 
are unique to this class of systems, and therefore, anyone of them could be taken as the 
definition of the class. These properties are developed in Section 5.6.3. 
If we are given a magnitude-squared function in the form of Eq. (5.72), and we 
know that both the system and its inverse are causal and stable (i.e., is a minimum-phase 
system), then H (z) is uniquely determined and will consist of all the poles and zeros of 
C(z) = H{z)H*(llz*) that lie inside the unit circle.4 This approach is often followed 
in filter design when only the magnitude response is determined by the design method 
used. (See Chapter 7.) 
5.6.1 Minimum-Phase and All-Pass Decomposition 
In Section 5.4, we saw that, from the square of the magnitude of the frequency response 
alone, we could not uniquely determine the system function H (z), since any choice 
that had the given frequency-response magnitude could be cascaded with arbitrary all­
pass factors without affecting the magnitude. A related observation is that any rational 
system functionS can be expressed as 
H(z) 
Hmin(z)Hap(z), 
(5.90) 
where Hmin(Z) is a minimum-phase system and H ap(Z) is an all-pass system. 
To show this, suppose that H (z) has one zero outside the unit circle at z = 1Ie*, 
where lei < 1, and the remaining poles and zeros are inside the unit circle. Then H(z) 
can be expressed as 
H(z) = HI(Z)(z-l 
e*), 
(5.91) 
where, by definition, HI (z) is minimum phase. An equivalent expression for H (z) is 
7-1 ­ e* 
H (z) = HI (z){1 
-1) ~ 
(5.92)
ez 
1 _ ez-1' 
Since lei < 1, the factor HI (z)(1- ez-1) also is minimum phase, and it differs from H(z) 
only in that the zero of H (z) that was outside the unit circle at z = 1Ie* is reflected inside 
the unit circle to the conjugate reciprocal location z = e. The term (z-1 
e*)/(i-ez-1) 
4We have assumed that C(z) has no poles or zeros on the unit circle. Strictly speaking, systems with 
poles on the unit circle are unstable and are generally to be avoided in practice. Zeros on the unit circle, 
however, often occur in practical filter designs. By our definition, such systems are nonminimum phase, but 
many of the properties of minimum-phase systems hold even in this case. 
5Somewhat for convenience, we will restrict the discussion to stable, causal systems, although the 
observation applies more generally. 

312 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
is all-pass. This example can be generalized in a straightforward way to include more 
zeros outside the unit circle, thereby showing that, in general, any system function can 
be expressed as 
H(z) = Hmin(Z)Hap(z), 
(5.93) 
where H min (z) contains all the poles and zeros of H (z) that lie inside the unit circle, to­
gether with zeros that are the conjugate reciprocals of the zeros of H (z) that lie outside 
the unit circle. The system function H ap(Z) is comprised of all the zeros of H (z) that lie 
outside the unit circle, together with poles to cancel the reflected conjugate reciprocal 
zeros in H min (Z). 
Using Eq. (5.93), we can form a nonminimum-phase system from a minimum­
phase system by reflecting one or more zeros lying inside the unit circle to their conjugate 
reciprocal locations outside the unit circle, or, conversely, we can form a minimum­
phase system from a nonminimum-phase system by reflecting all the zeros lying outside 
the unit circle to their conjugate reciprocal locations inside. In either case, both the 
minimum-phase and the nonminimum-phase systems will have the same frequency­
response magnitude. 
Example 5.12 
Minimum-Phase/AII·Pass Decomposition 
To illustrate the decomposition ofa stable, causal system into the cascade of a minimum­
phase and an all-pass system, consider the two stable, causal systems specified by the 
system functions 
1 
Hl(Z) = (1+3c ) 
1+' 
and 
(1 + ~e+j1!"/4c1) (1 + ~e-j1!"/4cl) 
H2(Z) = 
()
1 -
~Z-1 
The first system function, HI (z), has a pole inside the unit circle at z 
- i, and 
a zero outside at z = -3. We will need to choose the appropriate all-pass system to 
reflect this zero inside the unit circle. From Eq. (5.91), we have c 
-~. Therefore, 
from Eqs. (5.92) and (5.93), the all-pass component will be 
z-1 + 
H ap(Z) = --,_.'"­
1+ 
and the minimum-phase component will be 
i.e., 
( 
1 + 1z-1) (C 1 + 1 ) 
HI (z) = 
3 
3 
j 
. 
1 + ~Z-1 
1 + ~Z-1 
The second system function, H 2(z), has two complex zeros outside the unit circle 
and a real pole inside. We can express H2(Z) in the form of Eq. (5.91) by factoring 

313 
Section 5.6 
Minimum-Phase Systems 
~ejJl'/4 and 
out of the numerator terms to obtain 
Factoring as in Eq. (5.92) yields 
~ (1 + ~e-j~~~z-l) (1 + ~ejJl'/4z-1)] 
H2(Z) 
[ 4 
1 jc1 
(z-1+~e-j1l'/4)(z-1+~ejJl'/4) ] 
x [ (1+~ej1l'/4~-1)(1+~e-jJl'/4cl) . 
The first term in square brackets is a minimum-phase system, while the second term 
is an all-pass system. 
5.6.2  Frequency-Response Compensation of 
Non-Minimum-Phase Systems 
In many signal-processing contexts, a signal has been distorted by an LTI system with an 
undesirable frequency response. It may then be ofinterest to process the distorted signal 
with a compensating system, as indicated in Figure 5.22. This situation may arise, for 
example, in transmitting signals over a communication channel. Ifperfect compensation 
is achieved, then sc[n] = s[n], i.e., H c(z) is the inverse of Hd(Z). However, if we assume 
that the distorting system is stable and causal and require the compensating system to 
be stable and causal, then perfect compensation is possible only if H d(Z) is a minimum­
phase system, so that it has a stable, causal inverse. 
Based on the previous discussions, assuming that H d (z) is known or approximated 
as a rational system function, we can form a minimum-phase system Hdmin(Z) by re­
flecting all the zeros of H d (z) that are outside the unit circle to their conjugate reciprocal 
locations inside the unit circle. H d (z) and H d min (z) have the same frequency-response 
magnitude and are related through an all-pass system H ap(Z), i.e., 
Hd(Z) = Hdmin(z)Hap(z). 
(5.94) 
Choosing the compensating filter to be 
1 
(5.95)
HC(Z)=H .()'
dmm Z 
we find that the overall system function relating s[n] and sc[n] is 
G(z) = H d(z)Hc(z) = H ap(Z);  
(5.96) 
1----------
--------1 
1
1 
1
Distorting 
Compensating
I 
I
system 
system 
1
I
sIn) 
sd[n) 
1 sc[n]
Hd(z) --
HJz) 
Figure 5.22 Illustration of distortion
I  
I 
1L_______ 
____________ J 
compensation by linear filtering. 

314 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
i.e., G(z) corresponds to an all-pass system. Consequently, the frequency-response mag­
nitude is exactly compensated for, whereas the phase response is modified to LHap (e jW ). 
The following example illustrates compensation of the frequency response mag­
nitude when the system to be compensated for is a nonminimum-phase FIR system. 
Example 5.13 
Compensation of an FIR System 
Consider the distorting system function to be 
6Jt C 1)
Hd(Z) = (1 - O.gejO.6Jt C 1)(1- 0.ge- jO. 
(5.97) 
x (l-1.25ejO.&r z-1)(l-1.25e-jO.8rr z-1). 
The pole-zero plot is shown in Figure 5.23. Since H d(Z) has only zeros (all poles are 
at z 
0), it follows that the system has a finite-duration impulse response. Therefore, 
the system is stable; and since H d (z) is a polynomial with only negative powers of z, 
the system is causal. However, since two of the zeros are outside the unit circle, the 
system is nonminimum phase. Figure 5.24 shows the log magnitude, phase, and group 
delay for Hd(ejW ). 
The corresponding minimum-phase system is obtained by reflecting the zeros 
that occur at Z 
1.25e±jO.8rr to their conjugate reciprocal locations inside the unit 
circle. If we express H d(Z) as 
Hd(Z) = (1 
O.gejO.6rrz-l)(1 
O.ge-jO.6rrC1)(1.25)2 
(5.98) 
x (z-1 - O.8e-jO.8rr)(z-1 _ O.8ejO.8rr ), 
then 
Hmin(Z) = (1.25)2(1 
0.gejO.6rrz-l)(1 
0.ge-jO.6rrC1) 
(5.99) 
x (1 
O.8e-jO.8rr z-1)(1 
O.8ejO.8rr z-l), 
and the all-pass system that relates H min (z) and H d(Z) is 
(z-1 _ O.8e-jO.8rr )(z-1 _ O.8ejO.8rr ) 
(5.100)
H ap(Z) = 
'08 
1 
'08 
l'
(1- O.8el . rrC )(1 - O.8e-] . rrC ) 
The log magnitude, phase, and group delay of H min(e jW ) are shown in Figure 5.25. 
Figures 5.24(a) and 5.25(a) are, of course, identical. The log magnitude, phase, and 
group delay for H ap(ejW) are plotted in Figure 5.26. 
Im 
Unit 
circle ~
z-plane 
r 
° 
L-O'd"~ 
R.e 
pole
0"'­
° 
Figure 5.23 
Pole-zero plot of FIR system in Example 5.13. 

315 
Section 5.6 
Minimum-Phase Systems 
30 
15 
p:) 
""0 
0 
-15 
-30 
4 
2 
V) 
Q 
:a'" 
0 
IX'" 
-2 
-4 
15.0 
7.5 
V) 
.!! 
0.. 
0
5 
(Z) '" 
-7.5 
-15.0 
0 
'!!. 
2 
0 
Tr 
2 
0 
'!!. 
2 
7T 
Radian frequency (w) 
(a) 
71" 
Radian frequency (w) 
(b) 
71" 
Radian frequency (w) 
(c) 
37T 
2Tr 
2 
371" 
2Tr 
2 
371" 
211" 
2 
Figure 5.24 
Frequency response for FIR system with pole-zero plot in Fig­
ure 5.23. (a) Log magnitude. (b) Phase (principal value). (c) Group delay. 

316 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
30 
15 
p:i 
0
"" 
-15 
-30 
0 
7r 
1T 
2rr 
2 
2
Radian frequency (w) 
(a) 
4 
2 
00 c 
.::l 
co "" 
0:: 
-4 
0 
7r 
1T 
3rr 
2rr 
2 
2 
Radian frequency (w) 
(b) 
15.0 
7.5 
O.l "' 
"E.. 
0
E 
co 
<;I') 
-7.5 
-15.0 
0 
7r 
7r 
31T 
27r 
2 
2
Radian frequency (w) 
(c) 
Figure 5.25 
Frequency response for minimum-phase system in Example 5.13. 
(a) Log magnitude. (b) Phase. (c) Group delay. 

317 
Section 5.6 
Minimum-Phase Systems 
30 
15 
Q:I 
'0 
0 
-15 
-30 
4 
2 
'" 
c:: 
.?3 
'0 
0 
o:J 
~ 
-2 
-4 
15.0 
7.5 
'" 
-a" 
0
8 
r:rJ'" 
-7.5 
-15.0 
Figure 5.26 
r 
r 
I 
I 
I 
0 
7T 
7T 
27T 
2 
Radian frequency (co) 
2 
(a) 
0 
7T 
7T 
37T 
2'lT 
2 
Radian frequency (co) 
2 
(b) 
0 
~ 
'IT 
3'lT 
27T 
2 
Radian frequency (w) 
2 
(c) 
Frequency response of all-pass system of Example 5.13. (The sum of 
corresponding curves in Figures 5.25 and 5.26 equals the corresponding curve in 
Figure 5.24 with the sum of the phase curves taken modulo 2Jl".) (a) Log magnitude. 
(b) Phase (principal value). (c) Group delay. 

318 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
Note that the inverse system for Hd(Z) would have poles at Z 
1.25e±jO.Sl< and 
at Z = O.ge±jO.6n: , and thus, the causal inverse would be unstable. The minimum-phase 
inverse would be the reciprocal of H min (z), as given by Eq. (5.99), and if this inverse 
were used in the cascade system of Figure 5.22, the overall effective system function 
would be Hap(z), as given in Eq. (5.100). 
5.6.3 Properties of Mlnlmum*Phase Systems 
We have been using the term "minimum phase" to refer to systems that are causal 
and stable and that have a causal and stable inverse, This choice of name is motivated 
by a property of the phase function that, while not obvious, follows from our chosen 
definition. In this section, we develop a number of interesting and important properties 
of minimum-phase systems relative to all other systems that have the same frequency­
response magnitude. 
The Minimum Phase-Lag Property 
The use of the terminology "minimum phase" as a descriptive name for a system having 
all its poles and zeros inside the unit circle is suggested by Example 5.13. Recall that, as a 
consequence ofEq. (5.90), the unwrapped phase,i.e., arg[H(ejW»), of any nonminimum­
phase system can be expressed as 
arglH(ejW)] 
arglHmin(e}W)] + arg[Hap(ejW)]. 
(5.101) 
Therefore, the continuous phase that would correspond to the principal-value phase of 
Figure 5.24(b) is the sum of the unwrapped phase associated with the minimum-phase 
function of Figure 5.25(b) and the unwrapped phase of the all-pass system associated 
with the principal-value phase shown in Figure 5.26(b). As was shown in Section 5.5 
and as indicated by the principal-value phase curves of Figures 5.19(b), 5.20(b), 5.21(b), 
and 5.26(b), the unwrapped-phase curve of an all-pass system is negative for 0 .:::; w ~ 
1T. Thus, the reflection of zeros of H min (z) from inside the unit circle to conjugate 
reciprocal locations outside always decreases the (unwrapped) phase or increases the 
negative of the phase, which is called the phase-lag function. Hence, the causal, stable 
system that has IH min (e jW)I as its magnitude response and also has all its zeros (and, 
of course, poles) inside the unit circle has the minimum phase-lag function (for 0 ~ 
W < 1T) of all the systems having that same magnitude response. Therefore, a more 
precise terminology is minimum phase-lag system, but minimum phase is historically 
the established terminology. 
To make the interpretation of minimum phase-lag systems more precise, it is 
necessary to impose the additional constraint that H (ei (!) be positive at w = 0, Le., 
H(ej o) = L
00 
h[n] > O. 
(5.102) 
n=-oo 
Note that H (ejo ) will be real if we restrict h[n] to be real. The condition ofEq. (5.102) is 
necessary because a system with impulse response -h[n] has the same poles and zeros 
for its system function as a system with impulse response h[n]. However, mUltiplying by 
-1 would alter the phase by 1r radians. Thus, to remove this ambiguity, we impose the 
condition of Eq. (5.102) to ensure that a system with all its poles and zeros inside the 

319 
Section 5.6 
Minimum-Phase Systems 
unit circle also has the minimum phase-lag property. However, this constraint is often 
of little significance, and our definition at the beginning of Section 5.6, which does not 
include it, is the generally accepted definition of the class of minimum-phase systems. 
The Minimum Group-Delay Property 
Example 5.13 illustrates another property of systems whose poles and zeros are all 
inside the unit circle. First, note that the group delay for the systems that have the same 
magnitude response is 
(5.103)  
The group delay for the minimum-phase system shown in Figure 5.25( c) is always less 
than the group delay for the nonminimum-phase system shown in Figure 5.24( c). This is 
because, as Figure 5.26(c) shows, the all-pass system that converts the minimum-phase 
system into the nonminimum-phase system has a positive group delay. In Section 5.5, we 
showed this to be a general property of all-pass systems; they always have positive group 
delay for all w. Thus, if we again consider all the systems that have a given magnitude 
response IHmin(ejW)I, the one that has all its poles and zeros inside the unit circle has the 
minimum group delay. An equally appropriate name for such systems would therefore 
be minimum group-delay systems, but this terminology is not generally used. 
The Minimum Energy-Delay Property 
In Example 5.13, there are a total offourcausal FIR systems with real impulse responses 
that have the same frequency-response magnitude as the system in Eq. (5.97). The as­
sociated pole-zero plots are shown in Figure 5.27, where Figure 5.27(d) corresponds 
to Eq. (5.97) and Figure 5.27(a) to the minimum-phase system of Eq. (5.99). The im­
pulse responses for these four cases are plotted in Figure 5.28. If we compare the four 
sequences in this figure, we observe that the minimum-phase sequence appears to have 
larger samples at its left-hand end than do all the other sequences. Indeed, it is true for 
this example and, in general, that 
Ih[O]I.:s Ihmin[O]1 
(5.104) 
for any causal, stable sequence h[n] for which 
IH(ejW)1 = IHmin(ejW)I. 
(5.105) 
A proof of this property is suggested in Problem 5.71. 
All the impulse responses whose frequency-response magnitude is equal to 
IHmin(eiw) I have the same total energy as hmin[n], since, by Parseval's theorem, 
f: Ih[n]12 = ~ fIT IH(eiW )1 2dw = ~ fIT IHmin(eiW )12dw 
n=O 
2][ 
2][
-IT 
-IT 
(5.106) 
= L
00 
Ihmin[n]12. 
n=O 
If we define the partial energy of the impulse response as 
n 
E [n] = L Ih[mJI2, 
(5.107) 
m=O 

320 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
Im
Unit 
Unit 
lIm 
circle 
z-plane 
circle 
0 
z-plane
"... ../ 
0 
0 
0 
4ttl-order 
Re 
( 
tth_nrOPT ~ Re 
pole 
/ 
0 
0 
0 
(a) 
(b) 
Unit 
circle 'x o 
o 
Im 
4th-order 
pole 
z-plane 
Re 
(c) 
Unit 
circle 
o 
o 
o 
o 
Im 
4th-order 
pole 
z-plane 
Re 
(d) 
Figure 5.27 
Four systems, all having the same frequency-response magnitude. 
Zeros are at all combinations of the complex conjugate zero pairs O.ge±jO.6rr and 
O.8e±jO.Brr and their reciprocals. 
then it can be shown that (see Problem 5.72) 
n 
n
L Ih[m]1 2 :'S L Ihmin[mll2 
(5.108) 
m=O 
m=O 
for all impulse responses h[n] belonging to the family of systems that have magni­
tude response given by Eq. (S.105). According to Eq. (5.108), the partial energy of 
the minimum-phase system is most concentrated around n = 0; i.e., the energy of the 
minimum-phase system is delayed the least of all systems having the same magnitude 
response function. For this reason, minimum-phase (lag) systems are also called min­
imum energy-delay systems, or simply, minimum-delay systems. This delay property is 
illustrated by Figure 5.29, which shows plots of the partial energy for the four sequences 
in Figure 5.28. We note for this example-and it is true in general-that the minimum 
energy delay occurs for the system that has all its zeros inside the unit circle (i.e., the 
minimum-phase system) and the maximum energy delay occurs for the system that has 
all its zeros outside the unit circle. Maximum energy-delay systems are also often called 
maximum-phase systems. 

ha[n]
3.39 
2.89 
2.19 
1.56 
1°·81 
-2 
-1 
0 
1 
2 
3 
4 
5 
6 
n 
(a) 
hb[n]
3.39 
2.89 
2.19 
1.56 
t 
81 
-2 
-1 
° 
2 
3 
4 
5 
6 
n 
(b) 
hcln]
3.50 
2.51 
2.58 
1.26 
1.00 
-2 
-1 
0 
1 
2 
3 
4 
5 
6 
n 
(c) 
hd[n]
3.50 
2.58 
2.51 
1.00 
1.26 
-2 
-1 
° 
1 
2 
3 
4 
5 
6 
n 
(d) 
Figure 5.28 Sequences corresponding to the pole-zero plots of Figure 5.27. 
321 

322  
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
30 
-­
Ea[n] (minimum phase) 
........ Eb[n] (maximum phase) 
_._. Ecln] 
--­Ed[n] 
>. 
elll:l 20 
<t) 
3 
~ 
10 
o  
2 
3 
4 
5 
n 
Figure 5.29 
Partial energies for the four sequences of Figure 5.28. (Note that 
Ealn] is for the minimum-phase sequence haln] and ElJlnJ is for the maximum­
phase sequence hb[nj.) 
5.7  LINEAR SYSTEMS WITH GENERALIZED LINEAR 
PHASE 
In designing filters and other signal-processing systems that pass some portion of the 
frequency band undistorted, it is desirable to have approximately constant frequency­
response magnitude and zero phase in that band. For causal systems, zero phase is 
not attainable, consequently, some phase distortion must be allowed. As we saw in 
Section 5.1, the effect of linear phase with integer slope is a simple time shift. A nonlinear 
phase, on the other hand, can have a major effect on the shape of a signal, even when 
the frequency-response magnitude is constant. Thus, in many situations it is particularly 
desirable to design systems to have exactly orapproximately linear phase. In this section, 
we consider a formalization and generalization of the notions of linear phase and ideal 
time delay by considering the class of systems that have constant group delay. We begin 
by reconsidering the concept of delay in a discrete-time system. 
5.7.1 Systems with Linear Phase 
Consider an LTI system whose frequency response over one period is 
Hid(e jW ) 
e- jUJa , 
Iwl < Jr,  
(5.109) 
where a is a real number, not necessarily an integer. Such a system is an "ideal delay" 
system, where a is the delay introduced by the system. Note that this system has constant 
magnitude response, linear phase, and constant group delay; i.e., 

323 
Section 5.7 
Linear Systems with Generalized Linear Phase 
r--
-------
-
---------------, 
I 
I 
I 
I 
I 
I 
-""'I"""'~ 
x[n] I 
I y[n] 
I 
I 
I 
I 
1 
T 
T 
~ : 
Figure 5.30 
Interpretation of
L _____________________________ 
non integer delay in discrete-time 
H(e iiU) 
systems. 
jW
IHid(e
)I = 1, 
(5.110a) 
LHid(ejW) = -ma, 
(5.110b) 
jW
grd[Hid(e
)] 
a. 
(5.110c) 
The inverse Fourier transform of Hid(ejUJ) is the impulse response  
sinJr(n 
a) 
hid[n] = 
-00 < n < 00. 
(5.111)
:rr;(n 
a) 
The output of this system for an input x[n] is 
sinJr(n - a) 
~ [k sinJr(n - k 
a) 
y[n] = x [n] * 
= ~ 
X 
]--.-:..---~ 
(5.112)
:rr;(n - a) 
k=-oo 
Jr(n - k - a) 
If a 
nd, where nd is an integer, then, as mentioned in Section 5.1, 
hid[n] = o[n -nd] 
(5.113) 
and 
y[n] = x[n] *o[n - nd] = x[n - nd]. 
(5.114) 
That is, if a = nd is an integer, the system with linear phase and unity gain in Eq. (5.109) 
simply shifts the input sequence by nd samples. Ifa is not an integer, the most straight­
forward interpretation is the one developed in Example 4.7 in Chapter 4. 
Specifically, a representation of the system of Eq. (5.109) is that shown in Fig­
jQaT
ure 5.30, with hc(t) 
o(t 
aT) and Hc(jQ) = e-
, so that 
H(eJW) 
e-jwa , 
Iwl < Jr. 
(5.115) 
In this representation, the choice of T is irrelevant and could simply be normalized 
to unity. It is important to stress again that the representation is valid whether or not 
x[n) was originally obtained by sampling a continuous-time signal. According to the 
representation in Figure 5.30, y[n] is the sequence of samples of the time-shifted, band­
limited interpolation of the input sequence x[n]; i.e., y[n] = xc(nT 
aT). The system 
of Eq. (5.109) is said to have a time shift of a samples, even if a is not an integer. If the 
group delay a is positive, the time shift is a time delay. If a is negative, the time shift is 
a time advance. 
This discussion also provides a useful interpretation of linear phase when it is as­
sociated with a nonconstant magnitude response. For example, consider a more general 
frequency response with linear phase, i.e., 
H(eJW) 
IH(ejW)le-jwa. 
Iwl < Jr. 
(5.116) 

324 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
Figure 5.31 
Representation of a 
~
IH(ejW)I 
jwa ~ linear-phase LTI system as a cascade of
bI e­
x[n] 
w[n] 
y[n] 
amagnitude filter and atime shift. 
Equation (5.116) suggests the interpretation of Figure 5.31. The signal x[n] is filtered 
by the zero-phase frequency response IH (ejW)I, and the filtered output is then "time 
shifted" by the (integer or noninteger) amount a. Suppose, for example, that H(eiw) is 
the linear-phase ideallowpass filter 
Hlp(ejW) = 
jwa 
{e­
0, 
, 
Iwi < We, 
We < Iwi :s Tf. 
(5.117) 
The corresponding impulse response is 
h [ . 
sinwc(n-a)
Ip n] = 
.
Tf(n -
(5.118) 
Note that Eq. (5.111) is obtained if (Ue 
Tf. 
Example 5.14 Ideal Lowpass with Linear Phase 
The impulse response of the ideallowpass filter illustrates some interesting properties 
of linear-phase systems. Figure 5.32(a) shows h1p[n] for We 
OArr and a 
nd 
5. 
Note that when a is an integer, the impulse response is symmetric about n = nd; i.e., 
sin we(2nd 
n - nd)
h1p[2nd - n] 
rr(2nd - n - nd) 
sinwe(nd - n) 
(5.119)
rr(nd 
n) 
= nlp[nj. 
In this case, we could define a zero-phase system 
if Ip(ej(J) = Hlp(ej(J)eiwnd = IHlp(eiw)l, 
(5.120) 
wherein the impulse response is shifted to the left by nd samples. yielding an even 
sequence 
~ 
sin Wen 
A 
nlp[n] = 
rrn 
= nlp[-nj. 
(5.121) 
Figure 5.32(b) shows nlp[n] for We 
DArr and a = 4.5. This is typical of the case when 
the linear phase corresponds to an integer plus one-half. As in the case of the integer 
delay, it is easily shown that if a is an integer plus one-half (or 2a is an integer), then 
nlp [2a - n] = nlp[nj. 
(5.122) 
In this case, the point of symmetry is a, which is not an integer. Therefore, since the 
symmetry is not about a point of the sequence, it is not possible to shift the sequence to 
obtain an even sequence that has zero phase. This is similar to the case of Example 4.8 
with M odd. 
Figure 5.32(c) represents a third case, in which there is no symmetry at all. In 
this case, We = DArr and a 
4.3. 

____ ____ ____ ____ 
325 
Section 5.7 
Linear Systems with Generalized Linear Phase 
0.6 ,---------------------, 
0.4 
/' \
I 
Q) 
"0 
.E 
]-
0.2 
-< 
-0.2 '-------'-------'-------'-------' 
-5 
o 
5 
10 
15 
Sample number (n) 
(a) 
0.6 ,----------------------, 
0.4 
\
I 
\ 
I
i
~ 
\ 
-< 
10 
15 
0.2 
-0.2 '-­
..L­
-'­
-'­
--' 
-5 
o 
5 
Sample number (n) 
(b) 
0.6 ,---------------------, 
i 
]­
-< 
0.4 
0.2 
-0.2 L-___--l_____L-____L-___---' 
-5 
o 
5 
Sample number (n) 
10 
15 
(c) 
Figure 5.32 
Ideallowpass filter impulse responses. with We 
= ex = 5. (b) Delay = IX = 4.5. (c) Delay 
ex = 4.3. 
O.4n. (a) Delay 

326 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
In general a linear-phase system has frequency response 
H(ejW) = IH(ejW)le-jwa. 
(5.123) 
As illustrated in Example 5.14, if 2a is an integer (Le., if a is an integer or an 
integer plus one-half), the corresponding impulse response has even symmetry about 
a; i.e., 
h[2a 
n] = h[n]. 
(5.124) 
If 2a is not an integer, then the impulse response will not have symmetry. This is illus­
trated in Figure 5.32(c), which shows an impulse response that is not symmetric, but 
that has linear phase, or equivalently, constant group delay. 
5.7.2 Generalized Linear Phase 
In the discussion in Section 5.7.1, we considered a class of systems whose frequency 
response is of the form of Eq. (5.116), i.e., a real-valued nonnegative function of w 
multiplied by a linear-phase term e-Jwa. For a frequency response ofthis form, the phase 
jwa
of H(eJW ) is entirely associated with the linear-phase factor e-
, Le., arg[H(eJW)] = 
-wa, and consequently, systems in this class are referred to as linear-phase systems. In 
the moving average of Example 4.8, the frequency response in Eq. (4.66) is a real-valued 
function of w multiplied by a linear-phase term, but the system is not, strictly speaking, 
a linear-phase system, since, at frequencies for which the factor 
1 
sin[w(M + 1)/2] 
M + 1 
sin(w/2) 
is negative, this term contributes an additional phase of TC radians to the total phase. 
Many of the advantages of linear-phase systems apply to systems with frequency 
response having the form of Eq. (4.66) as well, and consequently, it is useful to generalize 
somewhat the definition and concept of linear phase. Specifically, a system is referred 
to as a generalized linear-phase system if its frequency response can be expressed in the 
form 
H(eJW ) = A (ejW)e-jaw+jf3, 
(5.125) 
where a and fJ are constants and A (eiw) is a real (possibly bipolar) function of w. For 
the linear-phase system of Eq. (5.117) and the moving-average filter of Example 4.8, 
a = -M/2 and fJ 
O. We see, however, that the bandlimited differentiator of Exam­
ple 4.4 has the form of Eq. (5.125) with a = 0, fJ = TC/2, and A (ejW ) 
wiT. 
A system whose frequency response has the form of Eq. (5.125) is called a gen­
eralized linear-phase system because the phase of such a system consists of constant 
terms added to the linear function -wa; i.e., -wa + fJ is the equation of a straight line. 

327 
Section 5.7 
Linear Systems with Generalized Linear Phase 
However, if we ignore any discontinuities that result from the addition of constant phase 
over all or part of the band Iwl < Jr, then such a system can be characterized by constant 
group delay. That is, the class of systems such that 
. 
d 
. 
L(W) = grd[H(eJW )] 
{arg[H(e JW )]} = a 
(5.126)
dw 
have linear phase of the more general form 
arg[H (e jW )] 
fJ - wa, 
0< w < Jr, 
(5.127) 
where fJ and a are both real constants. 
Recall that we showed in Section 5.7.1 that the impulse responses of linear-phase 
systems may have symmetry about a if 2a is an integer. To see the implication of this for 
generalized linear-phase systems, it is useful to derive an equation that must be satisfied 
by h[n], a, and fJ for constant group-delay systems. This equation is derived by noting 
that, for such systems, the frequency response can be expressed as 
H (e jW ) = A (ejUJ)ej(fJ-aw) 
(5.128) 
A (e JUJ ) cos(fJ - wa) + jA (eiw ) sin(fJ 
or equivalently, as 
H(e jW ) = L
00 
h[n]e- jwn 
n=-oo 
(5.129)
00 
00 
= L h[n]coswn - j L h[n] sin wn, 
n=-oo 
n=-oo 
where we have assumed that h[n] is reaL The tangent ofthe phase angle of H(eiw ) can 
be expressed as 
L 
00 
h[n] sinwn 
sin(fJ - wa) 
n=-oo
tan(fJ 
wa) = 
= ------­
L 
cos(fJ 
wa)  
00 
h[n]coswn  
n=-oo 
Cross multiplying and combining terms with a trigonometric identity leads to the 
equation 
L 
00 
h[n] sin[w(n 
a) + fJ] = 0 for all w. 
(5.130) 
n=-oo 
This equation is a necessary condition on h[nl, a, and fJ for the system to have constant 
group delay. It is not a sufficient condition, however, and, owing to its implicit nature, 
it does not tell us how to find a linear-phase system. 
A class of examples of generalized linear-phase systems are those for which 
fJ = 0 or 
Jr, 
(5.131a) 
2a 
M = an integer, 
(5.131b) 
h[2a 
n] = h[n]. 
(5.131c) 

328 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
With f3 = 0 or Jr, Eq. (5.130) becomes 
L 
00 
h[n] sin[w(n - a)] = 0, 
(5.132) 
n=-oo 
from which it can be shown that if 2a is an integer, terms in Eq. (5.132) can be paired so 
that each pair of terms is identically zero for all w. These conditions in turn imply that 
the corresponding frequency response has the form of Eq. (5.125) with f3 = 0 or Jr and 
A (e JW ) an even (and, of course, real) function of w. 
Another class of examples of generalized linear-phase systems are those for which 
f3 = Jr/2 or 3Jr/2, 
(5.133a) 
2a = M = an integer, 
(5.133b) 
and 
h[2a 
n] = -h[n] 
(5.133c) 
Equations (5.133) imply that the frequency response has the form of Eq. (5.125) 
with f3 
Jr/2 and A (eJW ) an odd function of w. For these cases Eq. (5.130) becomes 
L 
00 
h[n] cos[w(n - a)] = 0, 
(5.134) 
n=-oo 
and is satisfied for all w. 
Note that Eqs. (5.131) and (5.133) give two sets of sufficient conditions that guar­
antee generalized linear phase or constant group delay, but as we have already seen in 
Figure 5.32(c), there are other systems that satisfy Eq. (5.125) without these symmetry 
conditions. 
5.7.3 Causal Generalized Linear-Phase Systems 
If the system is causal, then Eq. (5.130) becomes 
00 
Lh[n] sin[w(n 
a) + f3] = 0 
for all w. 
(5.135) 
n=O 
Causality and the conditions in Eqs. (5.131) and (5.133) imply that 
h[n] = 0, 
n < 0 and n > M; 
i.e., causal FIR systems have generalized linear phase if they have impulse response 
length (M + 1) and satisfy either Eq. (S.13le) or (5.133c). Specifically, it can be shown 
that if 
h[n] = {h[M
0, 
n], O:s n :s M, 
otherwise, 
(5.136a) 
then 
H(eJW) = Ae(eJW)e-jwM/2, 
(5. 136b) 

329 
Section 5.7 
Linear Systems with Generalized Linear Phase 
where Ae(eiW ) is a real, even, periodic function of w. Similarly, if 
h[n) = { ~h[M - n), o::s 1l ::s M, 
(S.137a)
otherwise, 
then it follows that 
(S.137b) 
where Ao(eiw ) is a real, odd, periodic function of w. Note that in both cases the length 
of the impulse response is (M + 1) samples. 
The conditions in Eqs. (5.136a) and (S.137a) are sufficient to guarantee a causal 
system with generalized linear phase. However, they are not necessary conditions. 
Clements and Pease (1989) have shown that causal infinite-duration impulse responses 
can also have Fourier transforms with generalized linear phase. The corresponding sys­
tem functions, however, are not rational, and thus, the systems cannot be implemented 
with difference equations. 
Expressions for the frequency response of FIR linear-phase systems are useful in 
filter design and in understanding some of the properties of such systems. In deriving 
these expressions, it turns out that significantly different expressions result, depending 
on the type of symmetry and whether M is an even or odd integer. For this reason, it is 
generally useful to define four types of FIR generalized linear-phase systems. 
Type I FIR Linear-Phase Systems 
A type I system is defined as a system that has a symmetric impulse response 
h[n) 
h[M - n], 
o::s n ::s M, 
(5.138) 
with M an even integer. The delay M /2 is an integer. The frequency response is 
M 
H(ei{J) = Lh[n)e-Jwn . 
(5.139) 
n=O 
By applying the symmetry condition, Eq. (5.138), the sum in Eq. (5.139) can be rewritten 
in the form 
M12 
)
H(eiw ) = e- j (vM/2 
La[k]coswk , 
(5.140a)
( k=O 
where 
a[O] = h[M/2], 
(5.l40b) 
ark] 
2h[(M/2) 
k], 
k=1,2, ... ,M/2. 
(S.14OC) 
Thus, from Eq. (5.140a), we see that H(e jW ) has the form of Eq. (S.136b), and in partic­
ular, f3 in Eq. (5.125) is either 0 or j[. 

330 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
Type II FIR Linear-Phase Systems 
A type II system has a symmetric impulse response as in Eg. (5.138), with M an odd 
integer. H (elw ) for this case can be expressed as 
I
CM+1)/2  
I 
H(eiw)=e-lwM/2 
{; b[k]COS[W(k-nJ ' 
(S.141a) 
where 
b[k] = 2h[(M + 1)/2 - k], 
k 
1,2, ... , (M + 1)/2. 
(S.141b) 
Again, H(elw) has the form ofEg. (S.136b) with a time delay of M/2, which in this case 
is an integer plus one-half, and f3 in Eg. (5.125) is either 0 or 1i. 
Type III FIR Linear-Phase Systems 
If the system has an antisymmetric impulse response 
h[n] = -h[M - n], 
Os n ::: M, 
(5.142) 
with M an even integer, then H(eiu,) has the form 
M/2 
]
H(ejW)=je-jwM/2 
Lc[k]sinwk , 
(S.143a)
[ k=l 
where 
c[k] = 2h[(M/2) - k], 
k 
1,2, ... ,M/2. 
(S.143b) 
In this case, H(elw ) has theform ofEg. (S.137b) with a delay of M/2, which is an integer, 
and f3 in Eg. (5.125) is rr /2 or 3rr/2. 
Type IV FIR Linear-Phase Systems 
If the impulse response is antisymmetric as in Eg. (5.142) and M is odd, then 
CM+1)/2 
H(elw)=je-jwM/2 
{; d[k]sin[w(k 
(5. 144a) 
nl] ,
[ 
where 
d[k] = 2h[(M + 1)/2 - k], 
k = 1,2, ... , (M + 1)/2. 
(S.I44b) 
As in the case of type III systems, H (e jW ) has the form of Eq. (S.137b) with delay M /2, 
which is an integer plus one-half, and f3 in Eq. (5.125) is rr/2 or 3rr /2. 
Examples ofFIR Linear-Phase Systems 
Figure 5.33 shows an example of each of the four types of FIR linear-phase impulse 
responses. The associated frequency responses are given in Examples 5.15-5.18. 

• 
• • 
• 
• 
• 
• • • • 
• • 
331 
IS 
Section 5.7 
Linear Systems with Generalized Linear Phase 
I 
Center of 
r-symmetry
.d 
I 
11 I I I I •
0 
M 
M=4 
n 
2 
(a) 
I 
Center of
b) 
r-symmetry 
se 
11 I Ii 
I 
I I I •
0 
M 
M 
5 
n 
2 
(b) 
Center of 
1 
r-- symmetry 
I 
I 
n 
(c) 
Center of 
~symmetry 
I
II 
Figure 5.33 
Examples of FIR 
linear-phase systems. (a) Type I, M 
0 
n 
even, h[n] = hIM - n]. (b) Type II, M 
odd, h[n] = hIM - n]. (c) Type III, M
~r 
even, h[n] 
-hIM - n]. (d) Type IV, M 
(d) 
odd, h[n] 
-hIM - n]. 
Example 5.1 5 
TYpe I Linea,...Phase System 
If the impulse response is 
I, 0::: n ::: 4, 
(S 14S)
h[]
n = { 0, 
otherwise, 
. 
as shown in Figure S.33(a), the system satisfies the condition of Eq. (S.138). The fre­
quency response is 
. 
4 
. 
1 
e-j (})5
/2, 
H(e}(}) = "'" e-}(})n = ----:-­
L 
1­
n=O 
(S.146) 
_ j(})2 sin(Sw/2) 
=e 
.
sin(w/2) 
The magnitude, phase, and group delay of the system are shown in Figure S.34. Since 
M = 4 is even, the group delay is an integer, i.e., a = 2. 

332 
Chapter 5  
Transform Analysis of Linear Time-Invariant Systems 
~ 
B 
~ 
3.75 
:.a 2.50 
1.25 
0 1 
Y 
V 
V 
Y 
o  
7T 
7T 
37T 
27T 
2 
Radian frequency (w) 
2 
(a) 
4,----------------------------------------, 
on 
0: 
'i3" 
~" 
-4 
0 
7T 
7T 
37T 
27T 
2 
Radian frequency (w) 
2 
(b) 
4 
3 
'" " 
} 
2 
r/J " 
:l 
0  
7T 
7T 
37T 
27T 
2 
Radian frequency (w) 
2 
(c) 
Figure 5.34  Frequency response of type Isystem of Example 5.15. (a) Magnitude. 
(b) Phase. (c) Group delay. 
Example 5.16 
"tYpe II Linear-Phase System 
If the length of the impulse response of the previous example is extended by one sam­
ple, we obtain the impulse response of Figure 5.33(b), which has frequency response 
. 
. 5/2 sin(3w)
H(eJW ) = e-Jw 
. 
.  
(5.147)
sm(w/2) 

333 
Section 5.7 
Linear Systems with Generalized Linear Phase 
The frequency-response functions for this system are shown in Figure 5.35. Note that 
the group delay in this case is constant with ex = 5/2. 
4.5 
1.5 
O~----L-~--~----~____-L__~~____~ 
o 
1T 
1T 
31T 
21T 
2 
Radian freq uency (OJ) 
2 
(a) 
4~~··············~~··············~---------------------------' 
-4~-----········L--------~--------~------~ 
o 
1T 
1T 
k 
~ 
2 
Radian frequency (OJ) 
2 
(b) 
4,------------------------------------, 
It-
OL ..................~~~~.~....~ -~I~~~~~I~~~I  
o 
1T 
1T 
31T 
2 
Radian frequency (w) 
(el 
Figure 5.35 
Frequency response of type II system of Example 5.16. (a) Magni­
tude. (b) Phase. (c) Group delay. 
Example 5.1 7 
1)pe III Linear-Phase System 
If the impulse response is 
h[n] 
o[n] 
o[n - 2], 
(5.148) 

334 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
as in Figure 5.33( c), then 
j2w
H(ejW ) = 1- e-
= j[2sin(w)]e- jw • 
(5.149) 
The frequency-response plots for this example are given in Figure 5.36. Note 
that the group delay in this case is constant with a 
1. 
3.2 '.---------------------, 
2.4 
~ " 
'[ 1.6 
~ 
71" 
Radian frequency ("') 
(a) 
3.0,.---------------------, 
'" 
~ 
~ 
-3.0 
0 
71" 
71" 
371" 
271" 
2 
Radian frequency ("') 
2 
'"[
1.5 
(b) 
'"
0) 
]- 1.0 
'" 
</) 
0.5 
71" 
71" 
371" 
271" 
2 
Radian frequency ("') 
(e) 
Figure 5.36 
Frequency response of type 111 system of Example 5.17. (a) Magni­
tude. (b) Phase. (c) Group delay. 

335 
Section 5.7 
Linear Systems with Generalized Linear Phase 
Example 5.18 TYpe IV Linea....Phase System 
In this case (Figure 5.33(d)), the impulse response is 
h[n] 
D[!!] - S[!! -1]. 
(5.150) 
for which the frequency response is 
H(e j (1) = 1 - e- j(1) 
(5.151)
j [2 sin(w/2)je-j(1)/2. 
The frequency response for this system is shown in Figure 5.37. Note that the group 
delay is equal to ~ for all w. 
Locations ofZeros for FIR Linear-Phase Systems 
The preceding examples illustrate the properties of the impulse response and the fre­
quency response for all four types of FIR linear-phase systems. It is also instructive to 
consider the locations of the zeros of the system function for FIR linear-phase systems. 
The system function is 
M 
H(z) = Lh[n]z-n. 
(5.152) 
n=O 
In the symmetric cases (types I and II), we can use Eq. (5.138) to express H(z) as 
M 
o 
H(z) = Lh[M n1z-n = L h[k]zkZ-M 
(5.153)
n=O 
k=M 
Z-M H(Z-l). 
From Eq. (5.153), we conclude that if zo is a zero of H(z), then 
H(zo) 
ZOM H(z01) = O. 
(5.154) 
This implies that if Zo 
re jB is a zero of H(z), then ZOl 
r- l e- jO is also a zero of 
H(z). When h[n] is real and Zo is a zero of H(z), z'O 
re- jO will also be a zero of H(z), 
and by the preceding argument, so will (z'O)-1 = r-1eiB . Therefore, when h[n] is real, 
each complex zero not on the unit circle will be part of a set offour conjugate reciprocal 
zeros of the form 
(1 - reiilz-1)(I- re- jB z-1)(1- r- 1ejO z-1)(1 _ r- l e- jO z-1). 
If a zero of H (z) is on the unit circle, i.e., zo = ejO , then 
e- jO = zo, so zeros on 
the unit circle come in pairs of the form 
(1 
ejBz-l)(1 - e- jB Z-l). 
If a zero of H (z) is real and not on the unit circle, the reciprocal will also be a zero of 
H (z), and H (z) will have factors of the form 
(1 ± rz-1)(1 ± r-1z- 1). 

336  
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
3.2 rl- - - - - - - - - - - - - - - - - - - - ,  
2.4 
..g 
::I11.6 
rr 
rr 
k 
h 
-2 
Radian frequency (w) 
2 
(a) 
3.0 1,------------------__, 
i9 
.;!! 
'"'" 
Il: 
-3.0 IL-____-'-____-'-____---l'--___---l 
o  
rr 
rr 
3rr 
2rr 
2 
Radian frequency (w) 
2 
(b) 
2.0 1,------------------__, 
1.5 
[l 
-a ol­
E 1. 
(/) '" 
0.5 rool-----------------; 
I 
I 
Figure 5.37 
Frequency response of 
rr 
rr 
3rr 
2rr 
type IV system of Example 5.18. 
2 
Radian frequency (w) 
2 
(a) Magnitude. (b) Phase. (c) Group 
(c)  
delay. 
Finally, a zero of H(z) at z 
±1 can appear by itself, since ±I is its own reciprocal and 
its own conjugate. Thus, we may also have factors of H (z) of the form 
(I±z-l). 
The case of a zero at z = -1 is particularly important. From Eq. (5.153), 
H(-1) = (_I)M H(-I). 

337 
ns 
Section 5.7 
Linear Systems with Generalized Linear Phase 
o
Unit 
Unit
Tm 
z-plane 
Tm 
z-plane
circle 
circle 
0 
ne 
0 
Unit 
circle 
0 
(a) 
Im 
0 
z-plane 
Unit 
circle 
0 
(b) 
Im 
z-plane 
ne 
ne 
0 
0 
(c) 
(d) 
Figure 5.38 
Typical plots of zeros for linear-phase systems. (a) Type I. (b) Type 
II. (c) Type III. (d) Type IV. 
If M is even, we have a simple identity, but if M is odd, H(-I) = -H(-I), so H(-I) 
must be zero. Thus, for symmetric impulse responses with M odd, the system function 
must have a zero at z 
-1. Figures 5 .38( a) and 5 .38(b) show typical locations of zeros 
for type I (M even) and type II (M odd) systems, respectively. 
If the impulse response is antisymmetric (types III and IV), then, following the 
approach used to obtain Eq. (5.153), we can show that 
(5.155) 
This equation can be used to show that the zeros of H(z) for the antisymmetric case are 
constrained in the same way as the zeros for the symmetric case. In the antisymmetric 
case, however, both z = 1 and z = -1 are of special interest. If z 
1, Eq. (5.155) 
becomes 
H(1) 
-H(1). 
(5.156) 
Thus, H (z) must have a zero at z 
1 for both M even and M odd. If z = -1, Eq. (5.155) 
gives 
H(-I) = (-l)-M+I H (-l). 
(5.157) 
In this case, if (M 
1) is odd (i.e., if M is even), H(-1) = -H(-l), so z = -1 must be 
a zero of H(z) if M is even. Figures S.38(c) and 5.38(d) show typical zero locations for 
type III and IV systems, respectively. 
These constraints on the zeros are important in designing FIR linear-phase sys­
tems, since they impose limitations on the types of frequency responses that can be 
achieved. For example, we note that. in approximating a highpass filter using a symmetric 
impulse response, M should not be odd, since the frequency response is constrained to 
be zero at w = 1C(Z = -1). 

338 
Chapter 5 
Transform Analysis of Unear Time-Invariant Systems 
5.7.4  Relation of FIR Linear-Phase Systems to 
Minimum-Phase Systems 
The previous discussion shows that all FIR linear-phase systems with real impulse re­
sponses have zeros either on the unit circle or at conjugate reciprocal locations. Thus, 
it is easily shown that the system function of any FIR linear-phase system can be fac­
tored into a minimum-phase term Hmm(z), a maximum-phase term Hmax(z), and a term 
H uc(z) containing only zeros on the unit circle; Le., 
H(z) = Hmin(z)Huc(z)Hmax(z), 
(5.158a) 
where 
Hmax(z) = Hmin(z-l)z-M; 
(5.158b) 
and M; is the number of zeros of Hmm(z). In Eq. (5.158a), Hmin(Z) has all Mi of its 
zeros inside the unit circle, and H liC (z) has all M 0 of its zeros on the unit circle. Hmax(Z) 
has all Mj of its zeros outside the unit circle, and, from Eq. (5.158b), its zeros are the 
reciprocals of the Mi zeros of Hmin(Z). The order ofthe system function H(z) is therefore 
M =2Mi +Mo. 
Example 5.19 
Decomposition of a Linear-Phase System 
As a simple example of the use of Eqs. (5.158), consider the minimum-phase system 
function of Eq. (5.99), for which the frequency response is plotted in Figure 5.25. The 
system obtained by applying Eq. (5.158b) to H min(Z) in Eq. (5.99) is 
Hmax(z) = (O.9)2(1-1.1111ejO.6Jr z-1)(1 
1.1111e-jO.6Jr z-l) 
x (1-1.25e-jO.8Jrz-l)(1 
1.25ejO.8"'z-1). 
Hmax(z) has the frequency response shown in Figure 5.39. Now, if these two systems 
are cascaded, it follows from Eq. (5.158b) that the overall system 
H(z) = Hmin(z)Hmax(z) 
has linear phase. The frequency response of the composite system would be obtained 
by adding the respective log magnitude, phase, and group-delay functions. Therefore, 
20loglO IH(eja»1 = 2010glO IHmin(eja» I + 2010g10 IHmax(eja» I 
(5.159) 
= 4010glO IHmin(eiw)l. 
Similarly, 
LH(eia» = LHmin(eiw ) + LHmax(ejW). 
(5.160) 
From Eq. (5.158b), it follows that 
LHmax(eja» 
-wMi 
LHmin(ejW ). 
(5.161) 
and therefore 
LH(ejW ) 
-wMj, 

terns 
Section 5.7 
Linear Systems with Generalized Linear Phase 
339 
where Mi 
4 is the number of zeros of Hmin(Z). In like manner, the group-delay 
functions of Hmin(e jW ) and Hmax(e jW) combine to give 
grQ[H(ejw)1 
M; = 4. 
: re­
The frequency-response plots for the composite system are given in Figure 5.40. Note 
that the curves are sums of the corresponding functions in Figures 5.25 and 5.39. 
hus, 
fac­
enn 
30 .. 
15
58a) 
:Xl 
0
"0 
58b) 
-15 
~f its 
IX(Z) 
-30 
0 
1T 
1T 
21T
: the 
2 
Radian frequency (w)
:fore 
(a) 
4 
2 
'Stem 
§
The 
:a 
0 
~ 
-2 
-4 
0 
1T 
1T 
2 
Radian frequency (w) 
(b) 
15.0 
7,5 
'" 
~ 
0 
~ 
V) 
-7,5 
1T 
1T 
31T 
2 
2
Radian frequency (w) 
(c) 
Figure 5.39 
Frequency response of maximum-phase system having the same 
magnitude as the system in Figure 5.25. (a) Log magnitude. (b) Phase (principal 
value). (c) Group delay. 

-
-
340 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
60 
Cl 
"" 
30 
0 
-30 
-60 
0 
TI' 
TI' 
2T1' 
1 
Radian frequency (w) 
(a) 
4 
-4 I 
! !  
2 
o 
~ 
TI' 
~ 
k 
2 
Radian frequency (w) 
2 
(b) 
8,----------------------------------, 
6 
.£ e- 41 
ell'" 
2 
TI' 
TI' 
311' 
2T1' 
2 
Radian frequency (w) 
2 
(c) 
Figure 5.40 
Frequency response of cascade of maximum-phase and minimum­
phase systems, yielding alinear-phase system. (a) Log magnitude. (b) Phase (prin­
cipal value). (c) Group delay. 
5.8 SUMMARY 
In this chapter, we developed and explored the representation and analysis of LTI 
systems using the Fourier and z-transforms. The importance of transform analysis for 
LTI systems stems directly from the fact that complex exponentials are eigenfunctions 

• • • 
• • 
341 
Chapter 5 
Problems 
of such systems and the associated eigenvalues correspond to the system function or 
frequency response. 
A particularly important class of LTI systems is that characterized by linear constant­
coefficient difference equations. Systems characterized by difference equations may 
have an impulse response that is infinite in duration (IIR) or finite in duration (FIR). 
Transform analysis is particularly useful for analyzing these systems, since the Fourier 
transform or z-transform converts a difference equation to an algebraic equation. In par­
ticular, the system function is a ratio of polynomials, the coefficients of which correspond 
directly to the coefficients in the difference equation. The roots of these polynomials 
provide a useful system representation in terms of the pole-zero plot. 
The frequency response of LTI systems is often characterized in terms of mag­
nitude and phase or group delay, which is the negative of the derivative of the phase. 
Linear phase is often a desirable characteristic of a system frequency response, since it 
is a relatively mild form of phase distortion, corresponding to a time shift. The impor­
tance of FIR systems lies in part in the fact that such systems can be easily designed 
to have exactly linear phase (or generalized linear phase), whereas, for a given set of 
frequency response magnitude specifications, IIR systems are more efficient. These and 
other trade-offs will be discussed in detail in Chapter 7. 
While, in general, for LTI systems, the frequency-response magnitude and phase 
are independent, for minimum-phase systems the magnitude uniquely specifies the 
phase and the phase uniquely specifies the magnitude to within a scale factor. Nonmini­
mum-phase systems can be represented as the cascade combination of a minimum­
phase system and an all-pass system. Relations between Fourier transform magnitude 
and phase will be discussed in considerably more detail in Chapter 12. 
Problems 
Basic Problems with Answers 
5.1.  In the system shown in Figure PS.l-1, H(eiw) is an ideallowpass filter. Determine whether 
for some choice of input x[nl and cutoff frequency We, the output can be the pulse 
t,O :s 11 :s 10, 
y[n] = {.0, otherwise, 
shown in Figure PS.1-2. 
1 IH(,'"J 
I 
-'IT  -We 
'IT
We 
Figure P5.1-1 
y[n] 
o 
10 
n FigureP5.1-2 

342  
Chapter 5 
Transform Analysis of linear Time-Invariant Systems 
5.2. Consider a stable LTI system with input x[n] and output y[n). The input and output satisfy 
the difference equation 
y[n - 1] 
¥y[n] + y[n + 1] = x[n]. 
(8)  Plot the poles and zeros of the system function in the z-plane. 
(b)  Determine the impulse response h[n]. 
5.3. Consider an LTI discrete-time system for which the input x[n] and output y[n] are related 
by the 2nd-order difference equation 
y[n -1] + ~y[n 
2] = x[n]. 
From the following list, choose two possible impulse responses for the system: 
(8) (_nn+1 urn + 1) 
(b)  3n+1u[n + 1] 
(c)  3(-3)n+2u[-n-2]  
1 ( 
l)n 
(d)  3 -3 
u[-n 
2] 
1)n+l
(e)  ( -3 
u[-n 
2] 
(f)  (~r+l urn + 1] 
(g)  (-3)n+l u[n] 
(h)  n1/3u[n]. 
5.4.  When the input to an LTI system is 
x[n] = Gr urn] + (2)nu[-n -1], 
the output is 
y[n] = 6 (~r urn] - 6 (~r urn]. 
(a)  Determine the system function H(z) of the system. Plot the poles and zeros of H(z), 
and indicate the ROC. 
(b)  Determine the impulse response h[n] of the system for all values of n. 
(c)  Write the difference equation that characterizes the system. 
(d)  Is the system stable? Is it causal? 
5.5. Consider a system described by a linear constant -coefficient difference equation with initial­
rest conditions. The step response of the system is given by 
y[n] = 0r urn] + (~r urn] + urn]. 
(8)  Determine the difference equation. 
(b)  Determine the impUlse response of the system. 
(c)  Determine whether or not the system is stable. 
5.6.  The following information is known about an LTI system: 
(1)  The system is causal. 
(2) When the input is 
1 (l)n 
j(2)nu[-n -1],
x[n] = -3 2" 
urn] 

-z 
Problems  
343 
the z-transform of the output is 
-2
1 
(a)  Determine the z-transform of x[n]. 
(b)  What are the possible choices for the ROC of Y(z)? 
(c)  What are the possible choices for the impulse response of the system? 
5.7.  When the input to an LTI system is 
x[n] = Su[n], 
the output is 
(a)  Determine the system function H(z) of the system. Plot the poles and zeros of H(z), 
and indicate the ROC. 
(b)  Determine the impulse response of the system for all values of n. 
(c)  Write the difference equation that characterizes the system. 
S.S.  A causal LTI system is described by the difference equation 
y[n] = ~y[n -1] + y[n - 2] + x[n - 1]. 
(a)  Determine the system function H (z) = Y (z) / X(z) for this system. Plot the poles and 
zeros of H(z), and indicate the ROC. 
(b)  Determine the impulse response of the system. 
(c)  You should have found the system to be unstable. Determine a stable (noncausal) 
impulse response that satisfies the difference equation. 
5.9.  Consider an LTI system with input x[n] and output y[n] for which 
y[n - 1]- ~y[n] + y[n + 1] = x[n]. 
The system mayor may not be stable or causal. By considering the pole-zero pattern 
associated with this difference equation, determine three possible choices for the impulse 
response of the system. Show that each choice satisfies the difference equation. Indicate 
which choice corresponds to a stable system and which choice corresponds to a causal 
system. 
5.10.  If the system function H (z) of an LTI system has a pole-zero diagram as shown in Fig­
ure PS.lO and the system is causal, can the inverse system Hi (z), where H (z)H i (z) = 1, be 
both causal and stable? Clearly justify your answer. 
Unit 
Im 
circle 
z-plane 
x 
1 
Re 
x 
Figure P5.1D 

344 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
5.11. The system function of an LTI system has the pole-zero plot shown in Figure PS.ll Specify 
whether each of the following statements is true, is false, or cannot be determined from the 
information given. 
(a)  The system is stable. 
(b) The system is causal. 
(c)  If the system is causal, then it must be stable. 
(d)  If the system is stable, then it must have a two-sided impulse response. 
Im 
z-plane 
x 
II 
XI 
R.e 
X 
Unit circle 
Figure P5.11 
5.12. A discrete-time causal LTI system has the system function 
(1 + O.2z-1)(1- 9z-2) 
H (z) = ~-(1-+-0."-81-'-z--"""2)--'­
(a)  Is the system stable? 
(b)  Detennine expressions for a minimum-phase system HI (z) and an all-pass system 
H ap(Z) such that 
H(z) = Hl(Z)H ap (z). 
5.13. Figure PS.13 shows the pole-zero plots for four different LTI systems. Based on these plots, 
state whether or not each system is an all-pass system. 
Im 
Im
Hj(z) 
H2(z) 
X 
4  
Re 
1 
R.e 
3 
In! 
X 
H 3(z) 
o 
1 Re 
In! 
H 4(z) 
o 
X 
1 R.e 
Figure P5.13 

• 
• 
• 
• 
• 
345 
Chapter 5 
Problems 
5.14. Determine the group delay for 0 < W < If for each of the following sequences: 
(a) 
1:::: n :::: 5, 
5 < n:::: 9, 
otherwise. 
(b) 
('l)ln-ll (l)lnl
x2[n] = 
-
+­
,2 
,2 
5.15. Consider the class of discrete-time filters whose frequency response has the form 
H(eiw ) 
IH(eiw)le-jfXw, 
where I H (eJW)Iis a real and nonnegative function ofwand a is a real constant. As discussed 
in Section 5.7.1, this class of filters is referred to as linear-phase filters. 
Consider also the class of discrete-time filters whose frequency response has the form 
H (eJW ) 
A (eJW)e- jfXW+JfJ, 
where A (e jW) is a real function of w, a is a real constant, and {3 is a real constant. As 
discussed in Section 5.7.2, filters in this class are referred to as generalized linear-phase 
filters. 
For each of the filters in Figure P5.15, determine whether it is a generalized linear­
phase filter. If it is, then find A (ejW ), a, and (3. In addition, for each filter you determine 
to be a generalized linear-phase filter, indicate whether it also meets the more stringent 
criterion for being a linear-phase filter. 
h[n]  
h[n] 
h[n] 
1'1' r . 
• 
rrr.  
rrr •
0 
n 
0 
n 
0 
n 
(a) 
(b)  
(c) 
h[n]  
h[n] 
• rr • • 
• r • 
• •
0 
n 
0 
n 
•  
1-1 
(d)  
(e) 
Figure P5.15 
5.16.  Figure P5.16 plots the continuous-phase arg[H (e jW )1for the frequency response of a specific 
LTI system, where 
arg[H(e jW)] 
-aw 
for Iwl < If and a is a positive integer. 

346  
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
arg[H(eiw)] 
UJ 
Figure P5.16 
Is the impulse response h[n] of this system a causal sequence? If the system is definitely 
causal, or if it is definitely not causal, give a proof. If the causality of the system cannot 
be determined from Figure PS.16, give examples of a noncausal sequence and a causal 
sequence that both have the foregoing phase response arg[H (eiw)]. 
5.17.  For each of the following system functions, state whether or riot it is a minimum-phase 
system. Justify your answers: 
(1 - 2z-1) (1 + ie 1) 
HI (z) 
(1- jel) (1 + jel)' 
(1+1z-1)(1 lz-l), 
H2(Z) = (1 
§z-l)(l + §z 1) 
1 - jz-l 
H3 (z) = ( 
i -1)(1 + L 1)'
1- 2z 
2 z 
H4(Z)::::: (1 
S.lS.  For each ofthe following system functions H k (z), specify a minimum-phase system function 
H min (z) such that the frequency-response magnitudes of the two systems are equal. i.e., 
IHk(ejUJ)1 
IHmin(eiUJ )I. 
(a) 
,., -1
1 
...z 
H1(Z)=1+jc 
(b) 
(1 + 
(1 
2z1-1) 
H2(Z) = 
-(1 + jz 1) 
(c) 
(1- 3e1) (1-lz-1) 
H3(Z) = ( 
3 -1)(1 
:1:7-1)'
1 - liz 
3~ 

347 
Chapter 5 
Problems 
5.19.  Figure PS.19 shows the impulse responses for several different LTI systems. Determine the 
group delay associated with each system. 
h1[n]  
h2[n) 
r 
r  
rr r r 4
• 
• • • 
•  
-1 
-1 
0 
1 
2 
3 
4 
5  
0 
2
!2 I 
3 -J
-1 
h3[n] 
• • r 
• •
-1 
0 
1 
4 
5 
-2  
8
i -J 
3 
3 
3 
h4[n] 
2 
1 
2 
1 
4 
0 
2 
3 
5 
-1 
-2 
hs[n] 
h6[n] 
6 
•
-1 rI I I I I I I 
0 
1 
2 
3 
4 
5 
6 
7 
•8 
Figure P5.19 
5.20.  Figure PS.20 shows just the zero locations for several different system functions. For each 
plot, state whether the system function could be a generalized linear-phase system imple­
mented by a linear constant-coefficient difference equation with real coefficients. 
Im 
o 
o 
o 
o 
Im 
'R.e 
Figure P5.20 

348  
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
Basic Problems 
5.21.  Let hlp[n] denote the impulse response of an ideallowpass filter with unity passband gain 
and cutofffrequency We 
if / 4. Figure PS.21 shows five systems, each of which is equivalent 
to an ideal LTI frequency-selective filter. For each system shown, sketch the equivalent fre­
quency response, indicating explici tly the band-edge frequencies in terms ofWe. In each case, 
specify whether the system is a low pass, highpass, bandpass, bandstop, or multiband filter. 
x[n]  
+ 
(a)  
(_l)n 
(_l)n  
(b) 
(e) 
x[n] {' hlp [nI2], 
n even 
0, 
n odd 
(d) 
(e)  
Figure PS.21 
5.22.  Many properties of a discrete-time sequence hEn) or an LTI system with impulse response 
hEn) can be discerned from a pole-zero plot of H(z). In this problem, we are concerned only 
with causal systems. Clearly describe the z-plane characteristic that corresponds to each of 
the following properties: 
(a) Real-valued impulse response 
(b) Finite impulse response 
(c) hEn) = h[2a - n] where 2a is an integer 
(d) Minimum phase 
(e) All-pass. 

349 
Chapter 5 
Problems 
5.23. For all parts of this problem, H(e jW) is the frequency response of a DT filter and can be 
expressed in polar coordinates as 
where A(w) is even and real-valued and O(w) is a continuous, odd function of w for -TC < 
W < TC, Le., 0«(0) is what we have referred to as the unwrapped phase. Recall: 
•  The group delay ,(w) associated with the filter is defined as 
,«(0) = _ d9«(O) 
forlwl < If. 
dw 
•  An LTI filter is called minimum phase if it is stable and causal and has a stable and 
causal inverse. 
For each of the following statements, state whether it is TRUE or FALSE. If you state 
that it is TRUE, give a clear, brief justification. If you state that it is FALSE, give a simple 
counterexample with a clear, brief explanation of why it is a counterexample. 
(a)  "If the filter is causal, its group delay must be nonnegative at all frequencies in the 
range Iwl < TC." 
(b)  "If the group delay ofthe filter is a positive constant integer for Iwl < TC the filter must 
be a simple integer delay." 
(c)  "If the filter is minimum phase and all the poles and zeros are on the real axis then 
It r(w)dw = 0:' 
5.24. A stable system with system function  H (z) has the pole-zero diagram shown in Figure 
P5.24. It can be represented as the cascade of a stable minimum-phase system Hmin(Z) and 
a stable all-pass system Hap(z). 
Tm 
I  
3 
4 
Re
2' 
Figure P5.24 
Pole-zero diagram for H(1). 
Determine a choice for Hmin (z) and Hap (z) (up to a scale factor) and draw their correspond­
ing pole-zero plots. Indicate whether your decomposition is unique up to a scale factor. 
5.25.  (a) An ideallowpass filter with impulse response h[n] is designed with zero phase, a cutoff 
frequency of We = If/4, a passband gain of 1, and a stopband gain of O. (H(eiw) is 
shown in Figure P5.21.) Sketch the discrete-time Fourier transform of (_l)n h[n]. 

Chapter 5 
Transform Analysis of Linear Time-Invariant Systems
350 
(b)  A complex-valued filter with impulse response g[n] has the pole-zero diagram shown 
in Figure PS.2S. Sketch the pole-zero diagram for (_l)ng[n). If there is not sufficient 
information provided, explain why. 
Im 
Re 
1 
x 
x 
Figure P5.25 
5.26.  Consider a discrete-time LTl system for which the frequency response H(ejW ) is described 
by: 
H(ejw)=-j, 
O<w<rr 
H(ejw)=j, 
-rr <w<O 
(a)  Is the impulse response of the system hEn] real-valued? (i.e., is hEn] 
h*[n] for all n) 
(b)  Calculate the following: 
00 
2
L 
Ih[n]1
n=-oo 
(c)  Determine the response of the system to the input x[n) 
sEn] cos(wcn), where 
o< We < rr/2 and S(eiw ) = 0 for wel3 S Iwl s rr. 
5.27.  We process the signal x[n) = cos(O.3rrn) with a unity-gain all-pass LTl system, with fre­
quency response w = H (ejW ) and a group delay of 4 samples at frequency w = O.3rr, to get 
31r )
the output yEn). We also know that LH(ejO.31r) 
9 and LH(e- jO. 
= -9. Choose the 
most accurate statement: 
(a)  yEn] 
cos(O.3rrn + 8) 
(b)  yEn] 
cos(O.3rr(n - 4) + e) 
(c)  yEn] 
cos(O.3rr(n - 4 - 8» 
(d)  yEn] 
cos(O.3rr(n 
4» 
(e)  yEn] 
cos(O.3rr(n - 4 + 8». 

351 
Chapter 5 
Problems
:ems 
5.28. A causal LTI system has the system function 
:ient 
own 
jrr/ 3z-
jrr/ 3z­
(1- e
l )(1 - e-
l )(1 + L176SC1)
H(z)  
(1 - O.gei1r/3C l )(1 
O.ge-irr/3c l)(1 + OBSel )' 
(a)  Write the difference equation that is satisfied by the input x[n] and output y[nl of this 
system. 
(b) Plot the pole-zero diagram and indicate the ROC for the system function. 
(e)  Make a carefully labeled sketch of IH(eiw)1 . Use the pole-zero locations to explain 
why the frequency response looks as it does. 
(d) State whether the following are true or false about the system: 
(i) The system is stable. 
(ii) The impulse response approaches a nonzero constant for large n. 
(iii)  Because the system function has a pole at angle li/3, the magnitude of the fre­
quency response has a peak at approximately w = li/3. 
(iv) The system is a minimum-phase system. 
(v) The system has a causal and stable inverse. 
5.29. Consider the cascade of an LTI system with its inverse system shown in Figure PS.29. 
LTI 
LTI
x[n] 
wIn] 
yIn]
Inverse System 
System 
h;(n)
h[n] 
FIgure P5.29 
The impulse response of the first system is h[n] 
o[n] + 20[n - 1]. 
(8)  Determine the impulse response hi [n] of a stable inverse system for h[nJ. Is the inverse 
system causal? 
n) 
(b)  Now consider the more general case where h[n] = .5[n] + ao[n 
1]. Under what 
conditions on a will there exist an inverse system that is both stable and causal? 
5.30.  In each of the following parts, state whether the statement is always TRUE or FALSE. 
Justify each of your answers. 
(a)  "An LTI discrete-time system consisting of the cascade connection of two minimum­
phase systems is also minimum-phase." 
(b)  "An LTI discrete-time system consisting of the parallel connection of two minimum­
phase systems is also minimum-phase." 
5.31. Consider the system function 
rz- 1 
H (z) =  ------:;---;::---;:: 
Izi > r. 
1- (2rcos 
Assume first that wo f. O. 
(a) Draw a labeled pole-zero diagram and determine h[n]. 
(b) Repeat part (a) when Wo = O. This is known as a critically damped system. 

352 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
Advanced Problems 
5.32. Suppose that a causal LTI system has an impulse response of length 6 as shown in Fig­
ure P5.32, where c is a real-valued constant (positive or negative). 
0.75 
0.5 
c 
5 
n 
-2  -1 
4 
6 
7 
-0.3 
-1 
Figure P5.32 
Which of the following statements is true: 
(3) This system must be minimum phase. 
(b) This system cannot be minimum phase. 
(c) This system mayor may not be minimum phase, depending on the value of c. 
Justify your answer. 
5.33, H (z) is the system function for a stable LTI system and is given by: 
1
H(z) = (1- 2c )(1 - O.75z-1) 
z l(1-0.5z-1) 
(3)  H (z) can be represented as a cascade of a minimum-phase system Hrninl (z) and a 
unity-gain all-pass system Hap(z), 
H(z) = Hminl (z)Hap(z). 
Determine a choice for Hminl (z) and Hap (z) and specify whether or not they are unique 
up to a scale factor. 
(b)  H (z) can be expressed as a cascade of a minimum-phase system Hmin2 (z) and a gen­
eralized linear-phase FIR system Hlp(Z): 
H(z) = Hmin2(Z)Hlp (Z). 
Determine a choice for Hmin2(Z) and Hlp(Z) and specify whether or not these are 
unique up to a scale factor. 
5.34. A discrete-time LTI system with inputx[n] and output y[n] has the frequency response mag­
nitude and group delay functions shown in Figure P5.34-1. The signal x[n], also shown in 
Figure P5.34-1, is the sum of three narrowband pulses. In particular, Figure P5.34-1 contains 
the following plots: 
• x[n] 
• IX(ejW)I, the Fourier transform magnitude of a particular input x[n] 
• Frequency response magnitude plot for the system 
• Group delay plot for the system 

353 
Chapter 5 
Problems 
Input signal x[n] 
5,-----,-----,------.-----.-----.------,-----,-----.-----~ 
-50~--~100~--~200~--~3~00~--~4~OO~--~5~OO~---6~OO~--~7:~0~0----8~0~0----~~ 
n (samples)  
Fourier Transform of Input x[n1  
l~),_--_.-r--~----_.----_.----,_----.-----r---_.----_.----~ 
80 
60 
40 
20 
°OL---~~---L--~~~--L---~~---L----~--~L----L----~ 
~ 
~ 
M 
~ 
M 
ro 
M 
M 
1 
Normalized frequency (w) 
Frequency response magnitude of filter A 
2,----,-----.----,-----,----,-----.----,----,,----,----, 
<> 1.5 
E 
.~ 
0:1 
:::E 0.5 
0.1  
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
Normalized frequency (w) 
Group delay of filter A 
80,----,----,,----~--~,_--_,----_.----._--~,_--_,----, 
.--. 60 
'"
<!) 
0..8 40: 
0:1 
~ 
20 
°Ob=~~~~~~~~==~~======~=-~--~--~· 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
Normalized frequency (w) 
Figure P5.34-1 
The input signal and the filter frequency response 
In Figure P5.34-2 you are given four possible output signals, Yi [n] i = 1,2, ... ,4. Deter­
mine which one of the possible output signals is the output of the system when the input is 
x[n]. Provide a justification for your choice. 
1 

354 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
5 
:E:: 
:>:: 
E 
~ 0
0 
:B 
'Vi 
'"0 
p,.. 
-5 0 
100 
200 
300 
400 
500 
600 
700 
800 
900 
0 
100 
200 
300 
400 
500 
600 
700 
800 
900 
0 
100 
200 
300 
400 
500 
600 
700 
800 
900 
5 
E: 
~ 
E 
B­
~ 0
0., 
:0
'v.;
'"
0 
p,.. 
-5 
5 
:E:: 
~ 
..., 
~ 
0.. 
E 
0
0 
.9:l 
:9 
'" '" 
&. 
-5 
5 
E: 
~ 
E 
8­
::I 
0
0., 
;§ 
on 
'"0 
p,.. 
-5 0 
100 
200 
300 
400 
500 
600 
700 
800 
900 
n (samples) 
Figure P5.34-2 
Possible output signals 
5.35. Suppose that a discrete-time filter has group delay T(O). Does the condition T(O) > 0 for 
-If < 0) ::: If imply that the filter is necessarily causal? Clearly explain your reasoning. 

IS 
Chapter 5 
Problems  
355 
5.36. Consider the stable LTI system with system function 
1+4C2 
H(z) = 
1 
3' 
1- 4C1 - 8C2 
The system function H (z) can be factored such that 
H(z) 
Hmin(z)Hap(z), 
where Hmin(Z) is a minimum-phase system, and Hap(z) is an all-pass system, i.e., 
o 
IHap(ej"')1 
1. 
Sketch the pole-zero diagrams for Hmin (z) and Hap (z). Be sure to label the positions of all 
the poles and zeros. Also, indicate the ROC for Hmin (z) and Hap(z). 
5.37.  An LTI system has generalized linear phase and system function H(z) 
a +bz-I +cz-2. 
The impulse response has unit energy, a:::: 0, and H(eilr ) = H(elO ) 
o. 
(a)  Determine the impulse response h[n]. 
(b) Plot IH(el"')I. 
Xl 
5.38.  H (z) is the system function for a stable LTI system and is given by: 
(a)  H (z) can be represented as a cascade of a minimum-phase system Hmin (z) and a unity­
gain all-pass system Hap(z). Determine a choice for Hmin(Z) and Hap(z) and specify 
whether or not they are unique up to a scale factor. 
(b)  Is the minimum-phase system, Hmin(Z), an FIR system? Explain. 
(c)  Is the minimum-phase system, Hmin (z), a generalized linear-phase system? If not, can 
H(z) be represented as a cascade of a generalized linear-phase system HJin (z) and an 
all-pass system HapZ(z)? If your answer is yes, determine Hlin (z) and H ap2(Z). Ifyour 
answer is no, explain why such representation does not exist. 
5.39.  H (z) is the transfer function of a stable LTI system and is given by: 
z-2 
H(z) = z(z -1/3) 
(a)  Is the system causal? Clearly justify your answer. 
(b)  H(z) can also be expressed as H(z) = Hmin (z)Hlin(Z) where Hmin(Z) is a minimum­
phase system and Hlin(Z) is a generalized linear-phase system. Determine a choice for 
Hmin(Z) and Hlin(Z), 
5.40.  System Sl has a real impulse response hl[n] anda real-valued frequency response HI (el "'). 
(0)  Does the impulse response hl[n] have any symmetry? Explain. 
(b)  System S2 is a linear-phase system with the same magnitude response as system SI' 
What is the relationship between h2[n], the impulse response of system Sz, and hl[n]? 
(c)  Can a causal IIR filter have a linear phase? Explain. If your answer is yes, provide an 
example. 

356  
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
5.41. Consider a discrete-time LTI filter whose impulse response h[lI] is nonzero only over five 
consecutive time samples; the filter's frequency response is H(e1W ). Let signals xln] and 
yIn] denote the filter's input and output, respectively. 
Moreover, you are given the following information about the filter: 
(i) L: H(ejW)dw = 4JT. 
(ii) There exists a signal a[n] that has a real and even DTFf A(ejW ) given by 
A(e1W) = H (e1W ) e12w. 
(iii) A(ejO ) 
8. and A(el7f ) = 12 . 
Completely specify the impulse response h[n], i.e., specify the impulse response at each 
time instant where it takes a nonzero value. Plot h[n],carefully and accurately labeling its 
salient features. 
5.42. A bounded-input bounded-output stable discrete-time LTI system has impulse response 
hln] corresponding to a rational system function H(z) with the pole-zero diagram shown 
in Figure P5.42. 
Im 
1 
'Re 
112 
2
-3 
Figure P5.42 
00 
In addition, we know that 
(-l)nhln] = -1 . 
n=-oo 
(a) Determine H(z) and its ROC. 
(b)  Consider a new system having an impulse response g[nl = hln + no], where nO is an 
integer. Given that G(z)I-=o 
0, and lim G(z) < 00, determine the values of 110 
... 
z~oo 
and g[O]. 
(c)  A new system has an impulse response, fIn] = h[n] *hI-n]. 
Determine F(z) and its ROC. 
(d)  Is there a right-sided signal ern] such that ern] *h[n] = urn], where urn] is the unit-step 
sequence? If so, is ern] causal? 
~ 

357 
~h 
ts 
se 
m 
Chapter 5 
Problems 
5.43.  Consider an LTI system with system function: 
(a) Is H (z) an all-pass system? Explain. 
(b)  The system is to be implemented as the cascade of three systems Hmin (z), Hmax(z), 
and Hd(Z), denoting minimum-phase, maximum-phase, and integer time shift, respec­
tively. Determine the impulse responses hmin[n], hmax[n], and hd[n], corresponding 
to each of the three systems. 
5.44.  The impulse responses offour linear-phase FIRfiltershl[n],h2[n], h3[n], and h4[n] are given 
below. Moreover, four magnitude response plots, A, E, C. and D, that potentially corre­
spond to these impulse responses are shown in Figure PSA4. For each impulse response 
hi en], i = 1, ... ,4, specify which of the four magnitude response plots, if any, corresponds 
to it. If none of the magnitude response plots matches a given hi[n], then specify "none" 
as the answer for that hi [n]. 
h1[n] = O.5o[n] +O.70[n -11 +O.So[n - 2] 
h2[n] = 1.5o[n] + o[n - 1] + o[n - 2] + 1.5.5[n - 3] 
h3[n] = -0.58[n] -
.5[n -l] + 8[n - 3] + O.S8[n 
4J 
h4[n] = -8[n] + O.S8[n 
1] - O.S8[n 
2] + 8[n - 3J. 
4.5 
2.5 
4 
3.5
2 
~ 
:t 2.5
.~  
1.5 
~ 
" 
~ 
~ 2 
1.5 
0.5  
05  
0 
0  
-1 -0.8 -0.6 -0.4 -0.2 
0.2 
0.4 
0.6 
0.8 
1 
° 
-1 -0.8 -0.6 -0.4 -0.2 
0 
0.2 
0.4 
1.8 
1.6 
1.4 
~ 1.2 
.~ 
" 
~  0.8 
0.6 
0.4 
0.2 
0 
Frequency (WITT)  
Frequency (whr) 
(a)  
(b) 
2.5 
~ 
.~ 
~ 
1.5 
0.5 
0 
-I -0.8 -0.6 -0.4 -0.2 
0 
0.2 
0.4 
Frequency (WI7T) 
-I -0.8 -0.6 -0.4 -0.2 
0 
0.2 
0.4 
0.6 
0.8 
1 
Frequency (wITI') 
(c)  
(d) 
Figure P5.44 

358 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
5.45. The pole-zero plots in Figure P5,45 describe six different causal LTI systems, 
... - 
<Ii 
0.5
0.. 
;;.-,... 
"" 
0 
<: 
';:;0 
0.... 
~ -~: t 
t 
0.5
0.. '" 
>.... 
'" 
0
<:
.;:;0E-0.5-
-1 
... - 
ro 
o.s
0.. 
C 
Cd 
0
<: 
';:;0 
~ -os 
-1 
~/ 
Q 
-1 
0 
1 
Real part 
0 
Real part 
0. 
(E) 
-2 
·,1 
0 
2 
Real part 
;: 
OJ 
0.5
0.. 
>.... 
OJ 
0 
S -0.5-
-1 
-2 
-1 
0 
Real part 
t ro 
0.5
0.. 
~ 
ro 
0
<: 
'o:b8 -os-
-1 
·-2 
-1 
0 
Real part 
1 t 
X 
~ 
... 
ro 
0.5
0­..>. 
ro 
0
,::i 
'o:b 
.§ 
ro 
-0.5 
-I  
-2 
-I 
0  
Real part  
Figure P5.4S 
Answer the following questions about the systems having the above pole-zero plots. In 
each case, an acceptable answer could be none or all. 
(a) Which systems are IIR systems? 
(b) Which systems are FIR systems? 
(c) Which systems are stable systems? 
(d) Which systems are minimum-phase systems? 
(e) Which systems are generalized linear-phase systems? 
(f) Which systems have IH(eJW)I=constant for all w? 
(g) Which systems have corresponding stable and causal inverse systems? 
(h) Which system has the shortest (least number of nonzero samples) impulse response? 
(i) Which systems have lowpass frequency responses? 
(j) Which systems have minimum group delay? 
5.46. Assume that the two linear systems in the cascade shown in Figure P5.46  are linear­
phase FIR filters. Suppose that HI (z) has order MI (impulse response length Ml + 1) 
and H2(Z) has order M2' Suppose that the frequency responses are of the form HI (eJW ) 
Ai(eJW)e- jwMt/2 and H2(ejW ) 
j A2(e jW )e-JwM2/2 , where Ml is an even integer and M2 
is an odd integer. 

359 
2 
2 
In 
M2 
Chapter 5 
Problems 
(a)  Determine the overall frequency response H(ejW ). 
(b)  Determine the length of the impulse response of the overall system. 
(c)  Determine the group delay of the overall system. 
(d)  Is the overall system a Type I, Type II, Type III, or Type-IV generalized linear-phase 
system? 
x[n] 
YEn] 
Figure P5.46 
5.47. A linear-phase FIR system has a real impulse response hEn] whose z-transform is known 
to have the form 
where a, b, and c are zeros of H(z) that you are to find. It is also known that H(ejW ) 
0 for 
w 
O. This information and knowledge of the properties of linear-phase systems are suf­
ficient to completely determine the system function (and therefore the impulse response) 
and to answer the following questions: 
(a) Determine the length of the impulse response (i.e., the number of nonzero samples). 
(b) Is this a Type I, Type II, Type III, or Type IV system? 
(c) Determine the group delay of the system in samples. 
(d)  Determine the unknown zeros a, b, and c. (The labels are arbitrary, but there are three 
more zeros to find.) 
(e) Determine the values of the impulse response and sketch it as a stem plot. 
5.48. The system function H (z) of a causal LTI system has the pole-zero configuration shown in 
Figure P5.48. It is also known that H(z) 
6 when z = 1. 
Im 
z-plane
Double zero 
1 
1 
3 
2 
Figure P5.48 
(a) Determine H(z). 
(b) Determine the impulse response h[n] of the system. 
(c) Determine the response of the system to the following input signals: 
(i) x[n] 
urn] 
~u[n - 1] 
(ii) The sequence x[n] obtained from sampling the continuous-time signal 
x(t) = 50 + lOcos 20rrt + 30 cos 40rrt 
at a sampling frequency Q s = 2Jr(40) rad/s. 

360  
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
5.49. The system function of an LTI system is given by 
21
H(z) 
(1 - 2z-1)(1 - 4z-1) 
It is known that the system is not stable and that the impulse response is two sided. 
(a) Determine the impulse response h[n] of the system. 
(b)  The impulse response found in part (a) can be expressed as the sum of a causal impulse 
response hdn1and an anticausal impulse response h2[n]. Determine the corresponding 
system functions HI (z) and H2(Z). 
5.50.  The Fourier transform of a stable LTI system is purely real and is shown in Figure P5.50. 
Determine whether this system has a stable inverse system. 
IH(eiw) 
/T\ 
-~ 
w  Figure P5.50 
5.51. A causal LTI system has the system function 
(a)  Write the difference equation that is satisfied by the input and the output of the system. 
(b)  Plot the pole-zero diagram and indicate the ROC for the system function. 
(c)  Sketch IH(ejW)I. 
(d)  State whether the following are true or false about the system: 
(i) The system is stable. 
(ii) The impulse response approaches a constant for large n. 
(iii) The magnitude of the frequency response has a peak at approximately (V = ± 7T /4. 
(iv)  1be system has a stable and causal inverse. 
5.52.  Consider a causal sequence x [n1with the z-transform 
(1  
~Z-l) (1- ~z-l) (1 17)
5~ 
X(z) 
(1 - iz) 
For what values of a is anx[n] a real, minimum-phase sequence? 
...  

361 
Chapter 5 
Problems
ims 
5.53. Consider the LTI system whose system function is 
H(z) 
(1 
O.gejO.6n 
)(1-0.ge-jO.6nz-I)(1 
1.2SejO.8nz-I)(1 
1.2Se-jO.8nz-I). 
(8)  Determine all causal system functions that result in the same frequency-response mag­
nitude as H(z) and for which the impulse responses are real valued and of the same 
length as the impulse response associated with H(::.). (There are four different such 
system functions.) Identify which system function is minimum phase and which, to 
within a time shift, is maximum phase. 
ulse 
(b) Determine the impulse responses for the system functions in part (a). 
iing 
(c) For each of the sequences in part (b), compute and plot the quantity 
E [nJ = L
II 
(h£m])2
;'50. 
m=O 
forO S n S S. Indicate explicitly which plot corresponds to the minimum-phase system. 
5.54. Shown in Figure PS.S4 are eight different finite-duration sequences. Each sequence is four 
points long. The magnitude of the Fourier transform is the same for all sequences. Whieh 
of the sequences has all the zeros of its z-transform inside the unit circle? 
20.33 
17.67 
13.33
5.33  
9.67
2.67 
2 
1 
3  
2
I 
T 
• 
T 
I 
• 
, T 
I •
1 
3 
n 
2 
1 
n 
1 
3 
n
1 
I 
I  
I
-1.33 
-6.67 
-3.33 
-15.33 
-18.67  
-20.67 
(a)  
(b) 
(c) 
21.33 
17.67 
13.33 
9.67
1.67  
5.33 
1 
3 
2  
1 
3
, I 
1 I I  
I I 
T 1
• 
t 
•  
•
1 2 
I 
n  
3 
n 
2 
n 
-1.33  
-1.33
-10.67
-11.33 
-18.67 
-20.67 
(d)  
(e) 
(f) 
21.33 
20.33 
1.67
I 2 , •
1 
3 
n 
n
I 
1 
-10.67 
-11.33
rr/4. 
-15.33 
(g)  
(h) 
Figure P5.54 
5.55. Each of the pole-zero plots in Figure P5.55, together with the specification of the ROC, 
describes an LTI system with system function H(z). In each case, determine whether any 
of the following statements are true. Justify your answer with a brief statement or a coun­
terexample. 
(8) The system is a zero-phase or a generalized linear-phase system. 
(b) The system has a stable inverse Hi(z). 

2 
(c)  
(d) 
Figure P5.55 
5.56.  Assuming ideal D/C amd C/D converters, the overall system of Figure P5.56 is a discrete­
time LTI system with frequency response H (ejU) and impulse response h[n]. 
r-----------­
I 
I 
I 
xln] I 
I 
I 
I yIn] 
I 
I 
I 
T 
T 
I  
I 
I 
1_________ _ 
J 
Figure P5.56 
(a)  H(ejU) can be expressed in the form 
H(ejU) 
A (ejw)eN(W) , 
with A (ejU) real. Determine and sketch A (e jUJ ) and (jJ(w) for Iwl < n. 
(b) Sketch h[n] for the following: 
(i) IX = 3 
(ii) IX = 3!  
(... ) 
31  
III 
IX = 
4' 

363 
ns  
Chapter 5 
Problems 
(c) Consider a discrete-time LTI system for which 
H(ejW ) = A (ejW)ejrxw, 
Iwl < Jr, 
with A (e jW ) real. What can be said about the symmetry of h[n] for the following? 
(i) a = integer 
(ii) a = M /2, where M is an odd integer 
(iii) General a. 
5.57.  Consider the class of FIR filters that have h[n] real, h[n] = ofor n < 0 andn > M, and one 
of the following symmetry properties: 
Symmetric: h[n] = h[M - n] 
Antisymmetric: h[n] = -h[M -
n] 
All filters in this class have generalized linear phase, i.e., have frequency response ofthe form 
H(ejW ) = A (ejW)e-jrxw+jf3, 
where A (e jW ) is a real function of w, a is a real constant, and {3 is a real constant. 
For the following table, show that A (e jW ) has the indicated form, and find the values 
of a and {3. 
Type 
Symmetry 
(M + 1) 
Form of A (e jW ) 
a 
{3 
I 
Symmetric 
Odd 
M/2L a[n]coswn 
n=O 
(M+1)/2 
II 
Symmetric 
Even 
L 
b[n]cosw(n -1/2) 
n=l 
III 
Antisymmetric 
Odd 
M/2L c[n]sinwn 
n=l 
(M+l)/2 
IV 
Antisymmetric 
Even 
L 
d[n]sinw(n -1/2) 
n=l 
Here are several helpful suggestions. 
•  
For type I filters, first show that H (e jW ) can be written in the form 
(M-2)/2 
(M-2)/2 
H(ejW)= L 
h[n]e-jwn + L 
h[M_n]e-jw[M-nl+h[M/2]e-jw(M/2). 
•  
The analysis for type III filters is very similar to that for type I, with the exception 
of a sign change and removal of one of the preceding terms. 
•  
For type II filters, first write H (e jW ) in the form 
(M-l)j2 
(M-l)j2 
H(ejW ) = L 
h[n]e-jwn + L 
h[M - n]e-jw[M-n1• 
n=O  
and then pull out a common factor of e- jw(M/2) from both sums.  
•  
The analysis for type IV filters is very similar to that for type II filters. 

364  
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
5.58.  Let hlp[n] denote the impulse response of an FIR generalized linear-phase lowpass fil­
ter. The impulse response hhp[nJ of an FIR generalized linear-phase highpass filter can be 
obtained by the transformation 
hhp[n] = (-l)"hlp[n]. 
If we decide to design a highpass filter using this transformation and we wish the resulting 
highpass filter to be symmetric, which of the four types of generalized linear-phase FIR 
filters can we use for the design of the lowpass filter? Your answer should consider all the 
possible types. 
5.59.  (a) A spedfic minimum-phase system has system function Hmin(zl such that 
H min (z)H ap(zl 
Hlin(Z), 
where H ap(Z) is an all-pass system function and H linCo) is a causal generalized linear­
phase system. Whatdoes this information tell you aboutthe poles and zeros of H min (zl? 
(b)  A generalized linear-phase FIR system has an impulse response with real values and 
h[n] 
0 for n < 0 and for n :::. 8, and h[n] = -h[7- n]. The system function of this 
system has a zero at z = O.8ejJfj4 and another zero at Z 
-2. What is H(zl? 
5.60.  This problem concerns a discrete-time filter with a real-valued impulse response h[n]. De­
termine whether the following statement is true or false: 
Statement: If the group delay of the filter is a constant for 0 < (j) < Jr, then the 
impulse response must have the property that either 
h[n] = hIM - n] 
or 
h[n] = -h[M - n]. 
where M is an integer. 
If the statement is true, show why it is true. If it is false, provide a counterexample. 
5.61.  The system function Hn(z) represents a type II FIR generalized linear-phase system with 
impulse response hn[n). This system is cascaded with an LTI system whose system function 
is (1- z-l) to produce a third system with system function H (z) and impulse response h[nJ. 
Prove that the overall system is a generalized linear-phase system, and determine what type 
of linear-phase system it is. 
5.62.  Let S 1 be a causal and stable LTI system with impulse response hI [II) and frequency re­
sponse H 1(ejW ). The inputx[n] and output Y[II] for S 1 are related by the difference equation 
y[n] 
y[n 
1] + ~yrn - 2) = x[n]. 
(a)  If an LTI system S2 has a frequency response given by H2(ejW ) 
H 1 
would 
you characterize S 2 as being a lowpass filter, a bandpass filter, or a highpass filter? 
Justify your answer. 
(b)  Let S3 be a causal LTI system whose frequency response H 3(ejW ) has the property that 
jW 
jW
H3(e
)Hl(e
) = 1. 
Is S 3 a minimum-phase filter? Could S 3 be classified as one of the four types of FIR 
filters with generalized linear phase? Justify your answers. 
(c)  Let S4 be a stable and noncausal LTI system whose frequency response is H4(ejW) 
and whose input x[n] and output Y[Il] are related by the difference equation: 
y[n]+a1Y[1l 
1]+a2y[n-2]=f3ox[n], 
where al, a2, and 130 are all real and nonzero constants. Specify a value for al, a value 
for a2, and a value for 130 such that IH4(ejW)1 = IHI (ejW)I. 

365 
Chapter 5 
Problems 
(d)  Let S j be an FIR filter whose impulse response is hj[n] and whose frequency response, 
Hj(ej{J), has the property that Hj(e]{JJ) 
IA (e]{J) 12 for some DTFT A (ej{J) (i.e., Sj 
is a zero-phase filter). Determine hj[n] such that hj [llj *hdllJ is the impulse response 
of a noncausal FIR filter. 
Extension Problems 
5.63. In the system shown in Figure PS.63-1, assume that the input can be expressed in the form 
x[ll] = sLnl cos(WOn). 
Assume also that s[n] is lowpass and relatively narrowband; i.e., S(ej{J) = 0 for Iwi > Ll, 
with Ll very small and Ll «wo. so that X(e jW ) is narrowband around w = ±WO. 
----~.~~~--~.~ 
x[n] 
L.:J 
y[n] 
Figure P5.63-1 
(a)  If IH(ej{J)1 = 1 and LH(ejW ) is as illustrated in Figure PS.63-2, show that y[nl 
s[n]cos(won - <Po). 
------14>0 
-11' 
11'  
W 
-4>01------­
Figure P5.63-2 
(b)  If IH(ejW)1 = 1 and LH(ejW ) is as illustrated in Figure PS.63-3, show that y[n] can be 
. expressed in the form 
y[n] 
s[n-nd]cos(WOn 
<PO-COOnd).  
Show also that YEn] can be equivalently expressed as  
Y[Il] = s[n 
fld1COS(WOIl 
<PI),  
where -</>1 is the phase of H(ej{J) at W 
UJO'  
Figure P5.63-3 

• • • 
366 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
(c)  The group delay associated with H (ejW ) is defined as 
d 
Tgr(W) = 
dwarg[H(ejW)], 
and the phase delay is defined as Tph(w) 
-(l/w)i.H(ejW ). Assume that IH(ejW)1 is 
unity over the bandwidth of x[n]. Based on your results in parts (a) and (b) and on the 
assumption that x[n] is narrowband, show that if Tgr (wo) and Tph (wo) are both integers, 
then 
y[nJ = s[n - Tgr(WO)]Cos{wo[n - Tph(WO)J}. 
This equation shows that, for a narrowband signal x[nJ, i.H(e jU» effectively applies a 
delayofTgr(WO) to the envelopes[n] ofx[n] and a delay ofTph (WO) to the carrier cos won. 
(d)  Referring to the discussion in Section 45 associated with noninteger delays of a se­
quence, how would you interpret the effect of group delay and phase delay if Tgr(WO) 
or Tph (WO) (or both) is not an in teger? 
5.64.  The signal y[n] is the output of an LTI system with input x[nJ, which is zero-mean white 
noise. The system is described by the difference equation 
N 
M 
y[n] L aky[n - k] + L bkx[n 
k], 
bo 
L 
k=l 
k=Q 
(a)  What is the z-transform cI>yy(z) of the autocorrelation function ¢yy[n]? 
Sometimes it is of interest to process y[n] with a linear filter such that the power spec­
trum ofthe linear filter's output will be flat when the input to the linear filter is y[n]. This pro­
cedure is known as "whitening" y[n], and the linear filter that accomplishes the task is said 
to be the "whitening filter" for the signal y[n]. Suppose that we know the autocorrelation 
function ¢yy[n] and its z-transform cI>yy(z), but notthe values ofthe coefficients ak and bk. 
(b)  Describe a procedure for finding a system function H 11) (z) of the whitening filter. 
(c)  Is the whitening filter unique? 
5.65.  In many practical situations, we are faced with the problem of recovering a signal that has 
been "blurred" by a convolution process. We can model this blurring process as a linear 
filtering operation, as depicted in Figure P5.65-1, where the blurring impulse response is as 
shown in Figure P5.65-2. This problem will consider ways to recover x[n] from y[n]. 
----.....~ . 
x[n] 
L:J 
y[n] 
Desired signal 
Blurred signal Figure P5.65-1 
On M-l 
otherwise
I]]] ... (1 {~: 
o 
M 
1 
n 
Figure P5.65-2 

• • 
• • • 
367 
Chapter 5 
Problems 
(a)  One approach to recovering x[n] from y[n] is to use an inverse filter; i.e., y[n] is filtered 
by a system whose frequency response is 
where H(eiw ) is the Fourier transform of h[n]. For the impulse response h[n] shown 
in Figure PS.6S-2, discuss the practical problems involved in implementing the inverse 
filtering approach. Be complete, but also be brief and to the point. 
(b)  Because of the difficulties involved in inverse filtering, the following approach is sug­
gested for recoveringx[n] from y[n]: The blurred signal y[n] is processed by the system 
shown in Figure PS.6S-3, which produces an output w[n] from which we can extract 
an improved replica of x[n]. The impulse responses hI [n] and h2[n] are shown in Fig­
ure PS.6S-4. Explain in detail the working of this system. In particular, state precisely 
the conditions under which we can recover x[n] exactly from w[n]. Hint: Consider the 
impulse response of the overall system from x[n] to w[n]. 
Figure P5.65-3 
q 
h1[n]=L 8[n-kM] 
'] 
k~O  
.J. • • .] .... J. .]. . . . ]. .  
0 
M 
2M 
3M 
(q -1)M 
qM 
n 
h2 [n] = 8[n]-8[n -1) 
1 
1 
o 
n 
-1 
Figure P5.65-4 
(c)  Let us now attempt to generalize this approach to arbitrary finite-length blurring im­
pulse responses h[n]; i.e., assume only that h[n] = 0 for n < 0 or n 2: M. Further, 
assume that hl[n] is the same as in Figure PS.6S-4. How must H2(Z) and H(z) be re­
lated for the system to work as in part (b)? What condition must H(z) satisfy in order 
that it be possible to implement H2(Z) as a causal system? 

368  
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
5.66. In this problem, we demonstrate that, for a rational z-transform, a factor of the form (z - zo) 
and a factor of the form z/(z - l/zo) contribute the same phase. 
(a)  Let H(z) = z -lja, where a is real and 0 < a < 1. Sketch the poles and zeros of the 
system, including an indication of those at z '= 00. Determine LH (e jW ), the phase of 
the system. 
(b)  Let G(z) be specified such that it has poles at the conjugate-reciprocal locations of ze­
ros of H (z) and zeros at the conjugate-reciprocal locations of poles of H (z), including 
those at zero and 00. Sketch the pole-zero diagram of G(z). Determine LG(ej{J), the 
phase of the system, and show that it is identical to LH (e jW ). 
5.67. Prove the validity of the following two statements: 
(a)  The convolution of two minimum-phase sequences is also a minimum-phase sequence. 
(b)  The sum of two minimum-phase sequences is not necessarily a minimum-phase se­
quence. Specifically, give an example of both a minimum-phase and a nonminimurn­
phase sequence that can be formed as the sum of two minimum-phase sequences. 
5.68. A sequence is defined by the relationship 
00 
r[n] = L h[m]h[n + m] = h[n] *h[-n], 
m=-oo 
where h[n] is a minimum-phase sequence and 
r[n] = j (~r urn] + ~2nu[-n -1]. 
(a)  Determine R(z) and sketch the pole-zero diagram. 
(b) Determine the minimum-phase sequence h[n] to within a scale factor of ±l. Also, 
determine the z-transform H (z) of It [nj. 
5.69. A maximum-phase sequence is a stable sequence whose z-transform has all its poles and 
zeros outside the unit circle. 
(a)  Show that maximum-phase sequences are necessarily anti-causal, i.e., that they are 
zero for n > O. 
FIR maximum-phase sequences can be made causal by including a finite amount 
of delay. A finite-duration causal maximum-phase sequence having a Fourier transform 
of a given magnitUde can be obtained by reflecting all the zeros of the z-transform of a 
minimum-phase sequence to conjugate-reciprocal positions outside the unit circle. That is, 
we can express the z-transform of a maximum-phase causal finite-duration sequence as 
H max(Z) 
H mill (z)Hap(Z). 
Obviously, this process ensures that IH max(ej t1!)I = IHmin (ejW)I. Now, the z-transform of 
a finite-duration minimum-phase sequence can be expressed as 
M 
1
Hmin(Z) 
Itmin[O] n(1- CkZ- ), 
ICkI < l. 
k=l 
(b)  Obtain an expression for the all-pass system function required to reflect all the zeros 
of H min (Z) to positions outside the unit circle. 
(c) Show that Hmax(z) can be expressed as 
Hmax(z) = 
Hmin(z-l). 
(d) Using the result of part (c), express the maximum-phase sequence hmax[n] in terms of 
hmin[nJ. 
10. 

Chapter 5 
Problems 
369 
5.70.  It is not possible to obtain a causal and stable inverse system (a perfect compensator) for 
a nonminimum-phase system. In this problem, we study an approach to compensating for 
only the magnitude of the frequency response of a nonminimum-phase system. 
Suppose that a stable nonminimum-phase LTI discrete-time system with a rational 
system function H (z) is cascaded with a compensating system H C<z) as shown in Fig­
ure P5.70. 
x[n]  I 
y[n]
I 1_______________ --1I 
Figure P5.70 
(a)  How should H c<z) be chosen so that it is stable and causal and so that the magnitude 
of the overall effective frequency response is unity? (Recall that H (z) can always be 
represented as H (z) = H ap(z)Hmin (z).) 
(b)  What are the corresponding system functions H C<z) and G(z)? 
(c)  Assume that 
H(z) = (1- O.8e j0.3n: z-l)(l- O.8e-j0.3nz-I)(1_1.2e jO. 7n z-I)(1-1.2e- jO. 7n z-I). 
Determine H min (z), H ap(z), H C<z), and G(z) for this case, and construct the pole-zero 
plots for each system function. 
5.7L  Lethmin[n] denote a minimum-phase sequence with z-transform H min(Z). Ifh[nJ is a causal 
nonminimum-phase sequence whose Fourier transform magnitude is equal to I H min(e jUJ )I, 
show that 
Ih[Oll < Ihmin[O]I. 
(Use the initial-value theorem together with Eq. (5.93).) 
5.72.  One ofthe interesting and important properties ofminimum-phase sequences is the minimum­
energy delay property; i.e., of all the causal sequences having the same Fourier transform 
magnitude function I H (e jUJ ) I, the quantity 
n 
2
E [nl L Ih[m]1
m 
0 
is maximum for all n 2: 0 when h[n] is the minimum-phase sequence. 'This result is proved 
as follows: Let hmin[n] be a minimum-phase sequence with z-transform H min (z). Further­
more, let Zk be a zero of Hmin(z) so that we can express Hmin(Z) as 
where Q (z) is again minimum phase. Now consider another sequence h[n] with z-transform 
H (z) such that 
IH(ejW)1 = IHmin(ejUJ)1 
and such that H(z) has a zero at z 
1/zt instead of at Zk. 
(a)  Express H (z) in terms of Q (z). 
(b)  Express h[n] and hmin[n) in terms of the minimum-phase sequence q[n) that has z­
transform Q (z). 
• 

370 
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
(c)  To compare the distribution of energy of the two sequences, show that 
n 
n 
F: =  L Ihmin[m]12 - L Ih[m]12 = (1 -lzd)lq[n]12. 
m=O 
m=O 
(d)  Using the result of part (c), argue that 
n 
n
L  Ih[m]12::5 L Ih min[m]12 
for all n. 
m=O 
m=O 
5.73. A causal all-pass system H ap(Z) has input x[n] and output y[nJ. 
(a)  If x[n] is a real minimum-phase sequence (which also implies that x[n] = 0 for n < 0), 
using Eq. (S.108), show that 
n 
n 
2
L  Ix[kJ1
2: L ly[k]12. 
(PS.73-1) 
k=O 
'k=O 
(b)  Show that Eq. (PS.73-1) holds even if x[n] is not minimum phase, but is zero for n < O. 
5.74. In the design ofeither continuous-time or discrete-time filters, we often approximate a spec­
ified magnitude characteristic without particular regard to the phase. For example, standard 
design techniques for lowpass and bandpass filters are derived from a consideration of the 
magnitude characteristics only. 
In many filtering problems, we would prefer that the phase characteristics be zero 
or linear. For causal filters, it is impossible to have zero phase. However, for many filtering 
applications, it is not necessary that the impulse response of the filter be zero for n < 0 if 
the processing is not to be carried out in real time. 
One technique commonly used in discrete-time filtering when the data to be filtered 
are of finite duration and are stored, for example, in computer memory is to process the 
data forward and then backward through the same filter. 
Let h [n] be the impulse response of a causal filter with an arbitrary phase character­
istic. Assume that h[n] is real, and denote its Fourier transform by H(ejW ). Let x[n] be the 
data that we want to filter. 
(a)  Method A: The filtering operation is performed as shown in Figure PS.74-1. 
x[n] 
g[n] 
~ 
g[-n] 
r[n] 
~ 
s[n] =r[-n] 
Figure P5.74-1 
1.  Determine the overall impulse response hdn] that relates x[n] to s[n], and 
show that it has a zero-phase characteristic. 
2.  Determine IHI (ejW)I, and express it in terms of IH(ejW)1 and LH(ejW ). 
(b)  Method B: As depicted in Figure PS.74-2, processx[n] through the filter h[n] to get g[n]. 
Also, process x[n] backward through h[n] to get r[n]. The output y[n] is then taken as 
the sum of g[n] and r[-n]. This composite set of operations can be represented by a 
filter with input x[n], output y[n], and impulse response h2[n]. 

Chapter 5 
Problems  
371 
--.IJIoI~I---. 
x[n] 
L:J 
g[nl 
----~.~~~----.~ 
xl-n] L:J 
r[n] 
y[n] 
g[n] + r[-nJ 
Figure P5.74-2 
1. Show that the composite filter h2[n) has a zero-phase characteristic. 
2. Determine IH2(ejW)I, and express it in terms of IH(ejW)1 and LH(eiw ). 
(c)  Suppose that we are given a sequence of finite duration on which we would like to 
perform a bandpass zero-phase filtering operation. Furthermore. assume that we are 
given the bandpass filter h[n], with frequency response as specified in Figure P5.74-3, 
which has the magnitude characteristic that we desire, but has linear phase. To achieve 
zero phase, we could use either method A or B. Determine and sketch IH1(ejW )I 
and IH2(eiw)l. From these results, which method would you use to achieve the desired 
bandpass filtering operation? Explain why. More generally, if h[n] has the desired mag­
nitude, but a nonlinear phase characteristic, which method is preferable to achieve a 
zero-phase characteristic? 
7T 
37T 
7T 
W 
4 
4 
7T 
-7T~ 
W 
Figure P5.74-3 
5.75. Determine whether the following statement is true or false. If it is true, concisely state your 
reasoning. If it is false, give a counterexample. 
Statement: If the system function H(z) has poles anywhere other than at tbe origin or 
infinity, then the system cannot be a zero-phase or a generalized linear-phase system. 
5.76. Figure P5.76 shows the zeros of the system function H (z) for a real causal linear-phase FIR 
filter. All of the indicated zeros represent factors of the form (1-aC1). The corresponding 
poles at z 
0 for these factors are not shown in the figure. The filter has approximately 
unity gain in its passband. 
(8)  One of the zeros has magnitude 0.5 and angle 153 degrees. Determine tbe exact location 
of as many other zeros as you can from this information. 
(b)  The system function H (z) is used in the system for discrete-time processing ofcontinuo us­
time signals shown in Figure 4.10, with the sampling period T = 0.5 msec. Assume that 
the continuous-time input X cUrl) is band limited and tbat the sampling rate is high 
enough to avoid aliasing. What is the time delay (in msec) through the entire system, 
assuming that both CID and DIC conversion require negligible amounts of time? 

312  
Chapter 5 
Transform Analysis of Linear Time-Invariant Systems 
(c)  For the system in part (b), sketch the overall effective continuous-time frequency re­
sponse 20 10glO IHeffU!J) I for 0 :::::!J 
niT as accurately as possible using the given 
information. From the information in Figure PS.76 estimate the frequencies at which 
H eff(j!J) = 0, and mark them on your plot. 
0
II 
2 
0 
0.5
1:: 
Q., '" 
>. 
.... 
'" 
0 
·a 
.§'" 
-0.5 
1  
0 
-1 
-1.5 
o 
-2~1____~____~____ 
o 
o 
01 
0 
o 
o 
o 
o 
01 
0 
o 
o 
~__~~__~_____L____~__~ 
-2 
-1.5 
-1 
-0.5 
0 
0.5 
1 
1.5 
2 
Real Part 
Figure P5.76 
5.77. A signal x[n] is processed through an LTI system H(z) and then downsampled by a factor 
of 2 to yield yIn1as indicated in Figure PS.77. Also, as shown in the same figure, x[n] is first 
downsampled and then processed through an LTI system G(z) to obtain TlnJ. 
(a)  Specify a choice for H(z) (other than a constant) and G(z) so that T[n] = yIn] for an 
arbitrary x [n J. 
(b)  Specify a choice for H (z) so that there is no choice for G(z) that will result in r[nJ = yIn] 
for an arbitrary x[nJ. 
B 
~ 1 t 2 
•
x[n] 
wIn] 
YIn] == wl2n] 
•
1t2 
sIn) == w[2n] -I G(z) 1
x[n]  
r[n] 
Figure P5.77 
(c)  Determine as general a set of conditions as you can on H(z) such that G(z) can be 
chosen so that TIn] = yIn] for an arbitrary x[n]. The conditions should not depend on 
x [n]. If you first develop the conditions in terms of h[n], restate them in terms of H (z). 
(d)  For the conditions determined in part (c), what is g[n] in terms of h[n] so that 
T[n] = yIn]. 

373 
Chapter 5 
Problems 
5.78.  Consider a discrete-time LTI system with a real-valued impulse response h[n]. We want to 
find h[n], or equivalently, the system function H(z) from the autocorrelation Chh[C] of the 
impulse response. The definition of the autocorrelation is 
ex; 
chh[C] = L h[k]h[k + C]. 
k=-oo 
(a)  If the system h[n] is causal and stable, can you uniquely recover h[n] from Chh[C]? 
Justify your answer. 
(b)  Assume that h[n] is causal and stable and that, in addition, you know that the system 
function has the form 
1 
H(z) = --N--­
1- LakZ-k 
k=l 
for some finite ak. Can you uniquely recover h[n] from Chh[C]? Clearly justify your 
answer. 
5.79.  Let h[n] and H(z) denote the impulse response and system function of a stable all-pass LTI 
system. Let hi [n] denote the impulse response of the (stable) LTI inverse system. Assume 
that h[n] is real. Show that hi[n] = h[-n]. 
5.80.  Consider a real-valued sequence x[n] for which X(e jW ) = 0 for ~ :::: Iwl :::: 11:. One sequence 
value of x[n] may have been corrupted, and we would like to recover it approximately or 
exactly. With g[n] denoting the corrupted signal, 
g[n] = x[n] 
for n =1= no, 
and g[no] is real but not related to x[no]. In each of the following two cases, specify a 
practical algorithm for recovering x[n] from g[n] exactly or approximately. 
(a)  The exact value of no is not known, but we know that no is an odd number. 
(b)  Nothing about no is known. 
5.81.  Show that if h[n] is an (M + I)-point FIR filter such that h[n] = h[M -
n] and H(zo) = 0, 
then H(I/zo) = O. This shows that even symmetric linear-phase FIR filters have zeros that 
are reciprocal images. (If h[n] is real, the zeros also will be real or will occur in complex 
conjugates. ) 
be 
on 

6  
Structures for 
Discrete-Time 
Systems 
6.0 INTRODUCTION 
As we saw in Chapter 5, an LTI system with a rational system function has the property 
that the input and output sequences satisfy a linear constant-coefficient difference equa­
tion. Since the system function is the z-transform of the impulse response, and since the 
difference equation satisfied by the input and output can be determined by inspection 
of the system function, it follows that the difference equation, the impulse response, and 
the system function are equivalent characterizations of the input-output relation of an 
LTI discrete-time system. When such systems are implemented with discrete-time ana­
log or digital hardware, the difference equation or the system function representation 
must be converted to an algorithm or structure that can be realized in the desired tech­
nology. As we will see in this chapter, systems described by linear constant-coefficient 
difference equations can be represented by structures consisting of an interconnection 
of the basic operations of addition, multiplication by a constant, and delay, the exact 
implementation of which is dictated by the technology to be used. 
As an illustration of the computation associated with a difference equation, con­
sider the system described by the system function 
H(-) = bo + blZ - 1 
Izl > lal. 
(6.1)
" 
1 - az- 1 ' 
The impulse response of this system is 
h[n] = boallu[n] + bjall- 1u[n -1], 
(6.2) 
and the 1st-order difference equation that is satisfied by the input and output 
seq uences is 
y[n] - ay[n -1] = box[n] + blx[n -1] . 
(6.3) 
374 

375 
ion 
ch­
ent 
ion 
:act 
on­
5.1) 
5.2)  
5.3)  
Block Diagram Representation of Linear Constant-Coefficient Difference Equations 
Equation (6.2) gives a fonnula for the impulse response for this system. However, 
since the system impulse response has infinite duration, even if we only wanted to 
compute the output over a finite interval, it would not be efficient to do so by discrete 
convolution since the amount of computation required to compute y[n] would grow 
with n. However, rewriting Eq. (6.3) in the form 
y[n] = ay[n - 1] + box[n] + blx[n - IJ 
(6.4) 
provides the basis for an algorithm for recursive computation of the output at any 
time n in tenns of the previous output y[n 
1], the current input sample x[n], and 
the previous input sample x[n 
1]. As discussed in Section 2.5, if we further assume 
initial-rest conditions (i.e., if x[n] = 0 for n < 0, then y[n] = 0 for n < 0), and if 
we use Eq. (6.4) as a recurrence formula for computing the output from past values 
of the output and present and past values of the input, the system will be linear and 
time invariant. A similar procedure can be applied to the more general case of an 
Nth-order difference equation. However, the algorithm suggested by Eq. (6.4), and 
its generalization for higher-order difference equations is not the only computational 
algorithm for implementing a particular system, and often, it is not the best choice. As 
we will see, an unlimited variety of computational structures result in the same relation 
between the input sequence x[n] and the output sequence y[nJ. 
In the remainder ofthis chapter, we consider the important issues in the implemen­
tation of LTI discrete-time systems. We first present the block diagram and signal flow 
graph descriptions of computational structures for linear constant-coefficient difference 
equations representing LTI causal systems.1 Using a combination of algebraic manipu­
lations and manipulations of block diagram representations, we derive a number of basic 
equivalent structures for implementing a causal LTI system including lattice structures. 
Although two structures may be equivalent with regard to their input-output character­
istics for infinite-precision representations of coefficients and variables, they may have 
vastly different behavior when the numerical precision is limited. This is the major rea­
son that it is of interest to study different implementationstructures. The effects offinite­
precision representation ofthesystem coefficients and the effects oftruncation orround­
ing of intermediate computations are considered in the latter sections of the chapter. 
6.1 BLOCK DIAGRAM REPRESENTA"rlON OF LINEAR 
CONSTANT-COEFFICIENT DIFFERENCE EOUATIONS 
The implementation of an LTI discrete-time system by iteratively evaluating a recur­
rence formula obtained from a difference equation requires that delayed values of the 
output, input, and intennediate sequences be available. The delay of sequence values 
implies the need for storage of past sequence values. Also, we must provide means for 
multiplication of the delayed sequence values by the coefficients, as well as means for 
adding the resulting products. Therefore, the basic elements required for the implemen­
tation of an LTI discrete-time system are adders, multipliers, and memory for storing 
1 Such flow graphs are also called "networks" in analogy to electrical circuit diagrams. We shall use the 
terms flow graph, structure, and network interchangeably with respect to graphic representations ofdifference 
equations. 

Chapter 6 
Structures for Discrete-Time Systems
376 
£
2[n] 
----.{, + 
)I 
xdn] 
xl[n] +x2[n] 
(a) 
a 
ax[n]
x[n] 
jIo 
'" 
(b) 
--+l,1:l 
, 
Figure 6.1 
Block diagram symbols. 
x[n] 
L-J 
x[n-I] 
(a) Addition of two sequences. 
(b) Multiplication of a sequence by a 
(c) 
constant. (c) Unit delay. 
delayed sequence values and coefficients. lbe interconnection of these basic elements 
is conveniently depicted by block diagrams composed of the basic pictorial symbols 
shown in Figure 6.1. Figure 6.1(a) represents the addition of two sequences. In general 
block diagram notation, an adder may have any number of inputs. However, in almost 
all practical implementations, adders have only two inputs. In all the diagrams of this 
chapter, we indicate this explicitly by limiting the number of inputs as in Figure 6.1(a). 
Figure 6.1(b) depicts multiplication of a sequence by a constant, and Figure 6.1 (c) de­
picts delaying a sequence by one sample. In digital implementations, the delay operation 
can be implemented by providing a storage register for each unit delay that is required. 
For this reason, we sometimes refer to the operator of Figure 6.1(c) as a delay register. 
In analog discrete-time implementations such as switched-capacitor filters, the delays 
are implemented by charge storage devices. The unit delay system is represented in Fig­
ure 6.1( c) by its system function, z-l. Delays of more than one sample can be denoted 
as in Figure 6.1(c), with a system function of Z-M, where M is the number of samples 
of delay; however, the actual implementation of M samples of delay would generally 
be done by cascading M unit delays. In an integrated-circuit implementation, these unit 
delays might form a shift register that is clocked at the sampling rate of the input signal. 
In a software implementation, M cascaded unit delays would be implemented as M 
consecutive memory registers. 
Example 6.1 
Block Diagram Representation of a Difference 
Equation 
As an example of the representation of a difference equation in terms of the elements 
in Figure 6.1, consider the 2nd -order difference equation 
yIn] 
aly[n - 1] + a2y[n - 21 + box[nl. 
(6.5) 
The corresponding system function is 
bO 
H(z) = 
1 
. 
(6.6)
1 
alZ-
a2z-2 
The block diagram representation of the system realization based on Eq. (6.5) is shown 
in Figure 6.2, Such diagrams give a pictorial representation of a computational al­
gorithm for implementing the system. When the system is implemented on either a 

377 
IS 
Section 6.1 
Block Diagram Representation of Linear Constant-Coefficient Difference Equations 
general-purpose computer or a digital signal processing (DSP) chip. network structures 
such as the one shown in Figure 6.2 serve as the basis for a program that implements the 
system. Ifthe system is implemented with discrete components or as a complete system 
with very large-scale integration (VLSI) technology, the block diagram is the basis for 
determining a hardware architecture for the system. In both cases, diagrams such as 
Figure 6.2 show explicitly that we must provide storage for the delayed variables (in 
this case, yEn 
1] and yEn 
2]) and also the coefficients of the difference equation (in 
this case, al, az, and bo). Furthermore, we see from Figure 6.2 that an output sequence 
value y[n] is computed by first forming the products aly[n - 1] and aZy[n - 2], then 
adding them, and, finally, adding the result to bOx[n]. Thus, Figure 6.2 conveniently 
depicts the complexity of the associated computational algorithm, the steps of the 
algorithm, and the amount of hardware required to realize the system. 
x[n] 
Figure 6.2 
Example of a block diagram representation of a difference equation. 
Example 6.1 can be generalized to higher-order difference equations of the form2 
N 
M 
y[n] - I:>kY[n - k] = I>kx[n - k], 
(6.7) 
k=l 
k=O 
with the corresponding system function 
(6.8)
H(z) 
N 
1 
LakZ-k 
k=l 
Rewriting Eq. (6.7) as a recurrence formula for y[n] in terms of a linear combination of 
past values of the output sequence and current and past values of the input sequence 
2The form used in previous chapters for a general Nth-order difference equation was 
In the remainder of the book, it will be more convenient to use the form in Eq. (6.7), where the coefficient 
of y[n] is normalized to unity and the coefficients associated with the delayed output appear with a positive 
sign after they have been moved to the right-hand side of the equation. (See Eq. (6.9).) 

378 
Chapter 6 
Structures for Discrete-Time Systems 
leads to the relation 
N 
M 
y[n] = L akY[n - k] + L 
bkx[n - k]. 
(6.9) 
k=l 
k=O 
The block diagram of Figure 6.3 is an explicit pictorial representation of Eq. (6.9). 
More precisely, it represents the pair of difference equations 
M 
urn] = Lbkx[n 
k], 
(6.lOa) 
k=O 
N 
y[nJ 
L 
akY[n - kJ + v[n]. 
(6.10b) 
k=l 
The assumption of a two-input adder implies that the additions are done in a specified 
order. That is, Figure 6.3 shows that the products aNy[n - N] and aN-ly[n -
N + 1] 
must be computed, then added, and the resulting sum added to aN-2y[n - N + 2], and 
so on. After y[nJ has been computed, the delay variables must be updated by moving 
y[n - N + 1] into the register holding y[n - N], and so on, with the newly computed 
y[n] becoming y[n 
1] for the next iteration. 
A block diagram can be rearranged or modified in a variety of ways without chang­
ing the overall system function. Each appropriate rearrangement represents a different 
computational algorithm for implementing the same system. For example, the block 
diagram of Figure 6.3 can be viewed as a cascade of two systems, the first represent­
ing the computation of urn] from x[n1and the second representing the computation of 
y[n] from urn]. Since each of the two systems is an LTI system (assuming initial-rest 
conditions for the delay registers), the order in which the two systems are cascaded can 
be reversed, as shown in Figure 6.4, without affecting the overall system function. In 
Figure 6.4, for convenience, we have assumed that M 
N. Clearly, there is no loss of 
generality, since if M i= N, some of the coefficients ak or bk in the figure would be zero, 
and the diagram could be simplified accordingly. 
x[n] 
yIn] 
x[n 
Figure 6.3 
Block diagram 
representation for ageneral Nth-order 
N] 
difference equation. 

379 
Section 6.1 
Block Diagram Representation of Linear Constant-Coefficient Difference Equations 
In terms of the system function H (z) in Eq. (6.8), Figure 6.3 can be viewed as an 
implementation of H (z) through the decomposition 
(6.11)
H(z) 
or, equivalently, through the pair of equations 
(6.12a) 
1 
V(z). 
(6.12b)
y (z) = H2(Z)V(Z) 
Figure 6.4, on the other hand, represents H (z) as 
1 
(6.13) 
1 
or, equivalently, through the equations 
x (z), 
(6.14a)
W(z) = H2(Z)X (z) 
Y(z) 
Hl(Z)W(Z) = (thkZ-k) W(z). 
(6.14b) 
k=O 

380 
Chapter 6 
Structures for Discrete-Time Systems 
wln1 
wIn-I] 
w[n-N] 
x[n] 
y[n1 
Figure 6.4 
Rearrangement of block 
diagram of Figure 6.3. We assume for 
convenience that N = M. If N =1= M, 
some of the coefficients will be zero. 
In the time domain, Figure 6.4 and, equivalently, Eqs. (6.14a) and (6.14b) can be repre­
sented by the pair of difference equations 
N 
w[nJ = L::>kw[n 
k] + x[n], 
(6.15a) 
k=l 
M 
y[n] = :l:)kw[n - k]. 
(6.15b) 
k=O 
The block diagrams of Figures 6.3 and 6.4 have several important differences. In 
Figure 6.3, the zeros of H(z). represented by HI (z), are implemented first, followed by 
the poles, represented by Hz(z). In Figure 6.4, the poles are implemented first, followed 
by the zeros. Theoretically, the order of implementation does not affect the overall sys­
tem function. However, as we will see, when a difference equation is implemented with 
finite-precision arithmetic, there can be a significant difference between two systems 
that are equivalent with the assumption of infinite precision arithmetic in the real num­
ber system. Another important point concerns the number of delay elements in the two 
systems. As drawn, the systems in Figures 6.3 and 6.4 each have a total of (N + M) 
delay elements. However, the block diagram of Figure 6.4 can be redrawn by noting 
that exactly the same signal, w[n], is stored in the two chains of delay elements in the 
figure. Consequently, the two can be collapsed into one chain, as indicated in Figure 6.5. 
The total number of delay elements in Figure 6.5 is less than or equal to the number 
required in either Figure 6.3 or Figure 6.4, and in fact it is the minimum number required 
to implement a system with system function given by Eq. (6.8). Specifically, the minimum 
number of delay elements required is, in general, max(N, M ). An implementation with 
the minimum number of delay elements is commonly referred to as a canonic form 

381 
Section 6.1 
Block Diagram Representation of Linear Constant-Coefficient Difference Equations 
x[n] 
y[nl 
Figure 6.5 Combination of delays in 
Figure 6.4. 
implementation. The noncanonic block diagram in Figure 6.3 is referred to as the direct 
form J implementation of the general Nth-order system because it is a direct realization 
of the difference equation satisfied by the input x[n] and the output y[n], which in 
turn can be written directly from the system function by inspeetion. Figure 6.5 is often 
referred to as the direct form II or canonic direct form implementation. Knowing that 
Figure 6.5 is an appropriate realization structure for H (z) given by Eq. (6.8), we can go 
directly back and forth in a straightforward manner between the system function and 
the block diagram (or the equivalent difference equation). 
Example 6.2 
Direct Form I and Direct Form II  
Implementation of an LTI System  
Consider the LTI system with system function 
1+ 
H(z) = 
. 
(6.16)
1- 1.5Cl + 0.9C2 
Comparing this system function with Eq. (6.8), we find bO 
1, bl 
2, al 
+1.5, 
and aZ = -0.9, so it follows from Figure 6.3 that we can implement the system in a 
direct form I block diagram as shown in Figure 6.6. Referring to Figure 6.5, we can also 
implement the system function in direct fonn II, as shown in Figure 6.7. In both cases, 
note that the coefficients in the feedback branches in the block diagram have opposite 
signs from the corresponding coefficients of z-1 and z-2 in Eq. (6.16). Although this 
change of sign is sometimes confusing, it is essential to remember that the feedback 
coefficients {ak} always have the opposite sign in the difference equation from their 
sign in the system function. Note also that the direct form II requires only two delay 
elements to implement H (z), one less than the direct form I implementation. 

382  
Chapter 6 
Structures for Discrete-Time Systems 
x[n] 
y[n] 
Figl1re 6.6 
Direct form I implementation of Eq. (6.16). 
x[n] 
y[n] 
Figure 6.7 
Direct form II implementation of Eq. (6.16). 
In the preceding discussion, we developed two equivalent block diagrams for im­
plementing an LTI system with system function given by Eq. (6.8). These block diagrams, 
which represent different computational algorithms for implementing the system, were 
obtained by manipulations based on the linearity of the system and the algebraic prop­
erties of the system function. Indeed, since the basic difference equations that represent 
an LTI system are linear, equivalent sets of difference equations can be obtained sim­
ply by linear transformations of the variables of the difference equations. Thus, there 
are an unlimited number of equivalent realizations of any given system. In Section 6.3, 
using an approach similar to that employed in this section, we will develop a number of 
other important and useful equivalent structures for implementing a system with system 
function as in Eq. (6.8). Before discussing these other forms, however, it is convenient 
to introduce signal flow graphs as an alternative to block diagrams for representing 
difference equations. 
6.2  SIGNAL FLOW GRAPH REPRESENTATION OF LINEAR 
CONSTANT-COEFFICIENT DIFFERENCE EQUATIONS 
A signal flow graph representation of a difference equation is essentially the same as 
a block diagram representation, except for a few notational differences. Formally, a 

383 
Section 6.2 
Signal Flow Graph Representation 
Nodej 
Figure 6.8 
Example of nodes and 
Nodek 
branches in asignal flow graph. 
signal flow graph is a network of directed branches that connect at nodes. Associated 
with each node is a variable or node value. The value associated with node k might 
be denoted Wk, or, since node variables for digital filters are generally sequences, we 
often indicate this explicitly with the notation wdn1. Branch (j, k) denotes a branch 
originating at node j and terminating at node k, with the direction from j to k being 
indicated by an arrowhead on the branch. This is shown in Figure 6.8. Each branch 
has an input signal and an output signal. The input signal from node j to branch (j, k) 
is the node value wj[n]. In a linear signal flow graph, which is the only class we will 
consider, the output of a branch is a linear transformation of the input to the branch. 
The simplest example is a constant gain, i.e., when the output of the branch is simply a 
constant multiple of the input to the branch. The linear operation represented by the 
branch is typically indicated next to the arrowhead showing the direction of the branch. 
For the case ofa constant multiplier, the constant is simply shown next to the arrowhead. 
When an explicit indication of the branch operation is omitted, this indicates a branch 
transmittance of unity, or the identity transformation. By definition, the value at each 
node in a graph is the sum of the outputs of all the branches entering the node. 
To complete the definition of signal flow graph notation, we define two special 
types of nodes. Source nodes are nodes that have no entering branches. Source nodes 
are used to represent the injection of external inputs or signal sources into a graph. 
Sink nodes are nodes that have only entering branches. Sink nodes are used to extract 
outputs from a graph. Source nodes, sink nodes, and simple branch gains are illustrated 
in the signal flow graph of Figure 6.9. The linear equations represented by the figure 
are as follows: 
(6.17) 
y[n] = dx[n] + eW2[n]. 
d 
a 
Figure 6.9 
Example of a signal flow 
graph showing source and sink nodes.
a 

384 
Chapter 6 
Structures for Discrete-Time Systems 
(a) b
Source 
Sink 
node 0 
1 
2 
3 
node 5 
o  
x[n] 
y[n] 
w~ 
branch 
Figure 6,10 
(a) Block diagram
a', Vb! 
representation of a 1 sCorder digital filter. 
(b) Structu re of the signal flow graph
4 
corresponding to the block diagram in 
(b) 
(a). 
Addition, multiplication by a constant, and delay are the basic operations required 
to implement a linear constant-coefficient difference equation. Since these are all linear 
operations, it is possible to use signal flow graph notation to depict algorithms for 
implementing LTI discrete-time systems. As an example of how the flow graph concepts 
just discussed can be applied to the representation of a difference equation, consider 
the block diagram in Figure 6.10(a), which is the direct form II realization of the system 
whose system function is given by Eq. (6.1). A signal flow graph corresponding to this 
system is shown in Figure 6.10(b). In the signal flow graph representation of difference 
equations, the node variables are sequences. In Figure 6.1O(b), node 0 is a source node 
whose value is determined by the input sequence x[n], and node 5 is a sink node whose 
value is denoted y[n]. Notice that the source and sink nodes are connected to the 
rest of the graph by unity-gain branches to clearly denote the input and output of the 
system. Obviously, nodes 3 and 5 have identical values. The extra branch with unity 
gain is simply used to highlight the fact that node 3 is the output of the system. In 
Figure 6.10(b), all branches except one (the delay branch (2,4)) can be represented 
by a simple branch gain; i.e., the output signal is a constant multiple of the branch 
input. A delay cannot be represented in the time domain by a branch gain. However, 
the z-transform representation of a unit delay is multiplication by the factor 
. If we 
represented the difference equations by their corresponding z-transform equations, all 
the branches would be characterized by their system functions. In this case, each branch 
gain would be a function of z; e.g., a unit delay branch would have a gain of z-l. By 
convention, we represent the variables in a signal flow graph as sequences rather than 
as z-transforms of sequences. However, to simplify the notation, we normally indicate 
a delay branch by showing its branch gain as z-l, but it is understood that the output 
of such a branch is the branch input delayed by one sequence value. That is, the use of 
in a signal flow graph is in the sense of an operator that produces a delay of one 
sample. The graph of Figure 6.1O(b) is shown in Figure 6.11 with this convention. The 

385 
Section 6.2 
Signal Flow Graph Representation 
Figure 6.11 
Signal flow graph of 
Figure 6.1 O(b) with the delay branch 
indicated by r 1. 
equations represented by Figure 6.11 are as follows: 
wdn] = aW4[n] +x[n], 
(6.18a) 
w2[n] 
wj[n], 
(6.18b) 
w3[n] = bow2[n] + bj w4[n], 
(6.1Sc) 
w4[n] = w2[n -
1], 
(6.18d) 
y[n] = w3[n]. 
(6.18e) 
A comparison of Figure 6.1O(a) and Figure 6.11 shows that there is a direct corre­
spondence between branches in the block diagram and branches in the flow graph. In 
fact, the important difference between the two is that nodes in the flow graph represent 
both branching points and adders, whereas in the block diagram a special symbol is used 
for adders. A brancbing point in the block diagram is represented in the flow graph by a 
node that has only one incoming branch and one or more outgoing branches. An adder 
in the block diagram is represented in the signal flow graph by a node that has two (or 
more) incoming branches. In general, we will draw flow graphs with at most two inputs 
to each node, since most hardware implementations of addition have only two inputs. 
Signal flow graphs are therefore totally equivalent to block diagrams as pictorial repre­
sentations of difference equations, but they are simpler to draw. Like block diagrams, 
they can be manipulated graphically to gain insight into the properties of a given system. 
A large body of signal flow graph theory exists that can be directly applied to discrete­
time systems when they are represented in this form. (See Mason and Zimmermann, 
1960; Chow and Cassignol, 1962; and Phillips and Nagle, 1995.) Although we will use 
flow graphs primarily for their pictorial value, we will use certain theorems relating to 
signal flow graphs in examining alternative structures for implementing linear systems. 
Equations (6.18a)-(6.18e) define a multistep algorithm for computing the output 
of the LTI system from the input sequence x[n]. This example illustrates the kind of 
data precedence relations that generally arise in the implementation of IIR systems. 
Equations (6.lSa)-(6.18e) cannot be computed in arbitrary order. Equations (6.18a) 
and (6.1Sc) require multiplications and additions, but 
(6.18b) and (6.18e) sim­
ply rename variables. Equation (6.18d) represents the "updating" of the memory of 
the system. It would be implemented simply by replacing the contents of the mem­
ory register representing w4[n] by the value of w2[n], but this would have to be done 
consistently either before or after the evaluation of all the other equations. Initial-rest 
conditions would be imposed in this case by defining W2[ -1] 
0 or W4[O] = O. Clearly, 
Eqs. (6.18a)-(6.18e) must be computed in the order given, except that the last two could 
be interchanged or Eq. (6.18d) could be consistently evaluated first. 

386 
Chapter 6 
Structures for Discrete-Time Systems 
The flow graph represents a set of difference equations, with one equation being 
written at each node of the network. In the case of the flow graph of Figure 6.11, we can 
eliminate some of the variables rather easily to obtain the pair of equations 
W2[n] = aW2[n 
1] + x[n], 
(6.19a) 
y[n] = bow2[n] + hI w2[n - 1], 
(6.19b) 
which are in the form of Eqs. (6.15a) and (6.15b); 
in direct form II. Often, the 
manipulation of the difference equations of a flow graph is difficult when dealing with 
the time-domain variables, owing to feedback of delayed variables. In such cases, it is 
always possible to work with the z-transform representation, wherein all branches are 
simple gains since delay is represented in the z-transform by multiplication by C I . Prob­
lems 6.1-6.28 illustrate the utility of z-transform analysis of flow graphs for obtaining 
equivalent sets of difference equations. 
Example 6.3 
Determination of the System Function from a 
Flow Graph 
To illustrate the use of the ;:-transform in determining the system function from a 
flow graph, consider Figure 6.12. The flow graph in this figure is not in direct form. 
Therefore, the system function cannot be written down by inspection of the graph. 
However, the set of difference equations represented by the graph can be written 
down by writing an equation for the value of each node variable in terms of the other 
node variables. The five equations are 
WI In] = w4[n] 
x[n]. 
(6.20a) 
w2[n] = O'wI[n]. 
(6.20b) 
w3[n] 
w2[n] + x[n], 
(6.20c) 
w4[n] 
w3[n 
1], 
(6.20d) 
yIn] = w2[n] + w411l]. 
(6.20e) 
These are the equations that would be used to implement the system in the form 
described by the flow graph. Equations (6.20a)-(6.20e) can be represented by the 
z-transform equations 
WI (z) = W4(Z) 
X (z), 
(6.21a)  
W2 (z) = 0' WI (z). 
(6.21b)  
W3(Z) 
W2(Z) + X (Z), 
(6.21c)  
W4(Z) 
W3(Z), 
(6.21d)  
Y (z) = W2(Z) + W4(Z). 
(6.21e)  

stems 
Section 6.2 
Signal Flow Graph Representation 
387 
being 
ecan 
x[nJ 
y[nJ
iJ9a) 
•.19b) 
1, the 
Figure 6.12 
Flow graph not in standard direct form. 
~with 
S, it is 
We can eliminate WI (z) and W3(Z) from this set of equations by substituting 
esare 
Eq. (6.21a) into Eq. (6.21b) and Eq. (6.21c) into Eq. (6.21d). obtaining 
Prob­
W2(Z) = a(W4(Z) 
X (z)). 
(6.22a)
:tining 
W4(Z) = z-1 (W2 (z) + X (z». 
(6.22b) 
y (z) = W2(Z) + W4(Z). 
(6.22c) 
Equations (6.22a) and (6.22b) can be solved for W2(Z) and W4(Z). yielding 
a(e1 -1)
W2(Z) = 
1 X (z), 
(6.23a)
1-ae 
(1 
a) 
W4(Z) = 
I X (z), 
(6.23b)
1 az­
and substituting Eqs. (6.23a) and (6.23b) into Eq. (6.22c) leads to 
Y (z) = (a(z-l 
(6.24)
1) + z-l (1- a») X (z) = (z-l a) X (z). 
1 ae1 
1 ael 
Therefore, the system function of the flow graph of Figure 6.12 is 
Z-l - a 
H(z) = 1­
(6.25) 
from which it follows that the impulse response of the system is 
h[n] = an-lu[n 
1]- a n+1u[nJ 
and the direct form I flow graph is as shown in Figure 6.13. 
y[n] 
(6.21a) 
(6.21b) 
Figure 6.13 
Directform I equivalent of Figure 6.12. 
(6.21c) 
(6.21d) 
Example 6.3 shows how the z-transform converts the time-domain expressions. 
which involve feedback and thus are difficult to solve, into linear equations that can be 
(6.21e) 
solved by algebraic techniques. The example also illustrates that different flow graph 

388 
Chapter 6 
Structures for Discrete-Time Systems 
representations define computational algorithms that require different amounts ofcom­
putational resources. By comparing Figures 6.12 and 6.13, we see that the original imple­
mentation requires only one multiplication and one delay (memory) element, whereas 
the direct form I implementation would require two multiplications and two delay ele­
ments. The direct form II implementation would require one less delay, but it still would 
require two multiplications. 
6.3 BASIC STRUCTURES FOR IIR SYSTEMS 
In Section 6.1, we introduced two alternative structures for implementing an LTI system 
with system function as in Eq. (6.8). In this section we present the signal flow graph 
representations of those systems, and we also develop several other commonly used 
equivalent flow graph network structures. Our discussion will make it clear that, for any 
given rational system function, a wide variety of equivalent sets of difference equations 
or network structures exists. One consideration in the choice among these different 
structures is computational complexity. For example, in some digital implementations, 
structures with the fewest constant multipliers and the fewest delay branches are often 
most desirable. This is because multiplication is generally a time-consuming and costly 
operation in digital hardware and because each delay element corresponds to a memory 
register. Consequently, a reduction in the number of constant multipliers means an 
increase in speed, and a reduction in the number of delay elements means a reduction 
in memory requirements. 
Other, more subtle, trade-offs arise in VLSI implementations, in which the area 
of a chip is often an important measure of efficiency. Modularity and simplicity of data 
transfer on the chip are also frequently very desirable in such implementations. In 
multiprocessor implementations, the most important considerations are often related 
to partitioning of the algorithm and communication requirements between processors. 
Other major considerations are the effects of a finite register length and finite-precision 
arithmetic. These effects depend on the way in which the computations are organized, 
i.e., on the structure of the signal flow graph. Sometimes it is desirable to use a structure 
that does not have the minimum number of multipliers and delay elements if that 
structure is less sensitive to finite register length effects. 
In this section, we develop several of the most commonly used forms for imple­
menting an LTI IIR system and obtain their flow graph representations. 
6.3.1 Direct Forms 
In Section 6.1, we obtained block diagram representations of the direct form I (Fig­
ure 6.3) and direct form II, or canonic direct form (Figure 6.5), structures for an LTI 
system whose input and output satisfy a difference equation of the form 
N 
M 
y[n] 
2=:akY[n - k] = LbkX[n 
k], 
(6.26) 
k=l 
k=O 

Section 6.3 
Basic Structures for IIR Systems 
urn] 
x [nJ 
Z-1 
x [n 1J 
. Z-1 
2
xln 
11 
1 
1 
1 
b1 
b2 
x [n ­ N + 1] <>-----+-'"-­
"---¢ 
al 
az 
389 
y[n] 
Z-l 
y[n 1] 
Z-1 
v n 2J 
N + 1] 
1 
":, 
l:~:
N] 0-----1:-__
x[n 
N 
--' 
'-----+-----. y[n N] 
Figure 6.14 Signal flow graph of direct form I structure for an Nth-order system. 
with the corresponding rational system function 
M 
LbkZ-
1c 
H (z) = --------'k_=O"--__ 
(6.27)
N 
1 
Lakz-k 
k=l 
In Figure 6.14, the direct form I structure of Figure 6.3 is shown using signal flow graph 
conventions, and Figure 6.15 shows the signal flow graph representation of the direct 
form II structure of Figure 6.5. Again, we have assumed for convenience that N = M. 
Note that we have drawn the flow graph so that each node has no more than two inputs. 
A node in a signal flow graph may have any number of inputs, but, as indicated earlier, 
this two-input convention results in a graph that is more closely related to programs and 
architectures for implementing the computationof the difference equations represented 
by the graph. 
w[n] 
bo 
x [n] 
al 
'Z-1 
b1 
y[n] 
a2 
z-l 
b2 
I 
I 
I 
I 
1 
1 
1 
I 
Figure 6.15 
Signal flow graph of direct 
":N<!------<; 
,
,
j'L--- !,-' 
b: 
j 
form II structure for an Nth-order 
N 
system. 

390 
Chapter 6 
Structures for Discrete-Time Systems 
Example 6.4 
Illustration of Direct Form I and Direct Form II 
Structures 
Consider the system function 
1+2z-1 +z-2 
H(z) 
(6.28)
1 - O.75c1 +O.12sC2 · 
Since the coefficients in the direct form structures correspond directly to the coeffi­
cients of the numerator and denominator polynomials (taking into account the minus 
sign in the denominator of Eq. (6.27», we can draw these structures by inspection with 
reference to Figures 6.14 and 6.15. The direct form I and direct form II structures for 
this example are shown in Figures 6.16 and 6.17, respectively. 
xln] 
yIn 
Z-l 
r·c
1 
2 
0.75 
Z-l 
Z-l 
-0.125 
'-­
Figure 6.16 Direct form I structure for Example 6.4. 
x [n] 
y[n: 
Z-l 
0.75 
2 
Z-l 
-0.125 
Figure 6.17 
Direct form II structure for Example 6.4. 
6.3.2 Cascade Form 
The direct form structures were obtained dircctlyfrom the system function H (z), written 
as a ratio of polynomials in the variable 
as in Eq. (6.27). Ifwe factor the numerator 
and denominator polynomials, we can express H (z) in the form 
Ml 
M2 
1
TI (1 -
fkZ- 1) TI(1 - gkZ-1)(1- gkZ-
) 
H( ) = A k=l 
k=l 
(6.29)
Z 
/1/1 
N2 
TI (1 
Ckz-1) TI (1 
dkZ- 1)(1 ­
-
d/;Z-l) 
k=I 
k=] 
where M = M 1 + 2M2 and N = NI + 2N2- In this expression, the 1st-order factors 
represent real zeros at fk and real poles at q, and the 2nd-order factors represent 
complex conjugate pairs of zeros at gk and gk and complex conjugate pairs of poles 
~ 

391 
Section 6.3 
Basic Structures for IIR Systems 
ylln] 
wz[n] 
x[n] 
"
bOl I 
"
b02 I 
\ 
b03 I 
'C,I 
Zl 
Z-I 
y[n] 
all 
bll 
al2 
biz 
al3 
b13 
[1 
k~ 
Z-l 
azl 
b21 
aZ2 
b22 
a23 
b23 
Figure 6.18 
Cascade structure for a 6th-order system with a direct form II real­
ization of each 2nd-order subsystem. 
at dk and d;. This represents the most general distribution of poles and zeros when 
all the coefficients in Eq. (6.27) are real. Equation (6.29) suggests a class of structures 
consisting of a cascade of 1 st - and 2nd-order systems. There is considerable freedom in 
the choice of composition of the subsystems and in the order in which the subsystems are 
cascaded. In practice, however, it is often desirable to implement the cascade realization 
using a minimum of storage and computation. A modular structure that is advantageous 
for many types of implementations is obtained by combining pairs of real factors and 
complex conjugate pairs into 2nd -order factors so that Eq. (6.29) can be expressed as 
N> bOk + blkZ-1 + b2kZ-2 
(6.30) 
k=l 1 
H(z) = n
where Ns = L(N + 1)/2J is the largest integer contained in (N + 1)/2. In writing H(z) 
in this form, we have assumed that M ::::: N and that the real poles and zeros have been 
combined in pairs. If there are an odd number of real zeros, one of the coefficients b2k 
will be zero. Likewise, if there are an odd number of real poles, one of the coefficients a2k 
will be zero. The individual 2nd -order sections can be implemented using either of the 
direct form structures; however, the previous discussion shows that we can implement 
a cascade structure with a minimum number of multiplications and a minimum number 
of delay elements if we use the direct form II structure for each 2nd -order section. A 
cascade structure for a 6th-order system using three direct form II 2nd -order sections 
is shown in Figure 6.18. The difference equations represented by a general cascade of 
direct form II 2nd-order sections are of the form 
Yo[n] = x[n], 
(6.31a) 
wk[n] = alkwdn - 1] + a2kwk[n - 2] + Yk-l[n], 
k 
1,2, ...• Ns , 
(631b) 
Ydn] = bOkwdn] + blkwdn - 1] + b2kwdn 
2], 
k 
1,2, ... , Ns , 
(631c) 
y[n] = YNs[n]. 
(6.31d) 
It is easy to see that a variety of theoretically equivalent systems can be obtained 
by simply pairing the poles and zeros in different ways and by ordering the 2nd -order 
sections in different ways. Indeed, if there are Ns 2nd-order sections, there are Ns! (Ns 
factorial) pairings of the poles with zeros and Ns ! orderings of the resulting 2nd-order 
sections, or a total of (Ns !)2 different pairings and orderings. Although these all have the 
same overall system function and corresponding input-output relation when infinite­

392 
Chapter 6 
Structures for Discrete-Time Systems 
precision arithmetic is used, their behavior with finite-precision arithmetic can be quite 
different, as we will see in Sections 6.8-6.10. 
Example 6.5 
Illustration of Cascade Structures 
Let us again consider the system function ofEq. (6.28). Since this isa 2nd-order system, 
a cascade structure with direct form II 2nd -order sections reduces to the structure of 
Figure 6.17. Alternatively, to illustrate the cascade structure, we can use 1st-order 
systems by expressing H(z) as a product of 1st-order factors, as in 
(1+C1)(1+z-1)
H(z) 
(6.32)
(1 
O.5C1)(1 - O.25z-1) . 
Since all of the poles and zeros are real, a cascade structure with 1st-order sections 
has real coefficients. If the poles andlor zeros were complex, only a 2nd -order section 
would have real coefficients. Figure 6.19 shows two equivalent cascade structures, each 
of which has the system function in Eq. (6.32). The difference equations represented 
by the flow graphs in the figure can be written down easily. Problem 6.22 is concerned 
with finding other, equivalent system configurations. 
~lnl' "I : I · 1 '. I,: "I : I · I '. I~" Yl~ 
0.5 
0.25 
(a) 
o----l> 
x[n] 1'. 1< I · I '. I" : I · y[~ 
0.5 
0.25 
(b) 
Figure 6.19 
Cascade structures for Example 6.5. (a) Direct form I subsections. 
(b) Direct form II subsections. 
A final comment should be made about our definition of the system function 
for the cascade form. As defined in Eq. (6.30), each 2nd -order section has five constant 
multipliers. For comparison, let us assume that M = N in H (z) as given by Eq. (6.27), and 
furthermore, assume that N is an even integer, so that Ns = N /2. Then, the direct form 
I and II structures have 2N + 1 constant multipliers, while the cascade form structure 
suggested by Eq. (6.30) has 5N/2 constant multipliers. For the 6th-order system in 
Figure 6.18, we require a total of 15 multipliers, while the equivalent direct forms would 
require a total of 13 multipliers. Another definition of the cascade form is 
l 
2 
H(z) = bo ft 1 +bIke + bzkZ-
• 
(6.33) 
1 
alkz-1 -
a2kz-2 
k=1 

393 
Section 6.3 
Basic Structures for IIR Systems 
where bo is the leading coefficient in the numerator polynomial of Eq. (6.27) and bik 
bikibok for i 
1,2 and k = 1,2, ... , N s • This form for H(?) suggests a cascade of 
four-multiplier 2nd-order sections, with a single overall gain constant boo This cascade 
form has the same number of constant multipliers as the direct form structures. As 
discussed in Section 6.9, the five-multiplier 2nd-order sections are commonly used when 
implemented with fixed-point arithmetic, because they make it possible to distribute 
the overall gain of the system and thereby control the size of signals at various critical 
points in the system. When floating-point arithmetic is used and dynamic range is not 
a problem, the four-multiplier 2nd-order sections can be used to decrease the amount 
of computation. Further simplification results for zeros on the unit circle. In this case, 
bk 
1, and we require only three multipliers per 2nd-order section. 
6.3.3 Parallel Form 
As an alternative to factoring the numerator and denominator polynomials of H(z), 
we can express a rational system function as given by Eq. (6.27) or (6.29) as a partial 
fraction expansion in the form 
(6.34) 
where N = Nl + 2N2. If M :::: N, then Np = M - N; otherwise, the first summation in 
Eq. (6.34) is not included. If the coefficients ak and bk are real in Eq. (6.27), then the 
quantities A b B k, Cb Ck, and ek are all real. In this form, the system function can be 
interpreted as representing a parallel combination of l st _ and 2nd-order IIR systems, 
with possibly Np simple scaled delay paths. Alternatively, we may group the real poles 
in pairs, so that H (z) can be expressed as 
(6.35) 
where, as in the cascade form, Ns = L(N + 1)/2J is the largest integer contained in 
(N +1)/2, and if N p = M 
N is negative, the first sum is not present. A typical example 
for N 
M = 6 is shown in Figure 6.20. The general difference equations for the parallel 
form with 2nd-order direct form II sections are 
(6.36a) 
k 
1, 2, ... , Ns , 
(6.36b) 
Np 
Ns 
y[n] = L Ckx[n - k] + LYdn]. 
k=O 
k=l 
(6.36c) 
If M < N, then the first summation in Eq. (6.36c) is not included. 

394 
Chapter 6 
Structures for Discrete-Time Systems 
Co 
wI[n] 
eO! 
Ydnl 
Z·l 
all 
I 
ell 
Z·l 
a21 
W2[n] 
e02 
Y2[n] 
x[n] 
y[n] 
a12 
en 
Z·l 
aZ2 
W3[n) 
em 
Y3[n] 
Z·l 
a13 
en 
a23 
Figure 6.20 
Parallel form structure for 6th -order system (M 
N = 6) with the  
real and complex poles grouped in pairs.  
Example 6.6 
Illustration of Parallel Form Structures 
Consider again the system function used in Examples 6.4 and 6.5. For the parallel form, 
we must express H(z) in the form ofeither Eq. (6.34) or Eq. (6.35). Ifwe use 2nd-order 
sections, 
-7+8c1 
-----,---- _ ... - 8 + -----;-----:;::-
(6.37) 
. -
1 
0.75c1 + 0.125;:-2' 
The parallel form realization for this example with a 2nd-order section is shown in 
Figure 6.21. 
Since all the poles are real, we can obtain an alternative parallel form realization 
by expanding H(z) as 
H(z) = 8 + 
18 
25 
(6.38)
1-
1-0.25C1 · 
The resulting parallel form with 1st-order sections is shown in Figure 6.22. As in the 
general case, the difference equations represented by both Figures 6.21 and 6.22 can 
be written down by inspection. 

395 
Section 6.3 
Basic Structures for IIR Systems 
8 
x[n] 
-7 
y[n] 
0.75 
Zl 
8 
-0.125 
Z-I 
Figure 6.21 
system. 
Parallel form structure for 
8 
Example 6.6 using a 2nd ·order 
x[n] 
18 
y[n] 
0.5 
-25 
0.25 
Figure 6.22 
Parallel form structure for Example 6.6 using 1st·order systems. 
6.3.4 Feedback in IIR Systems 
All the flow graphs of this section have feedback loops; i.e., they have closed paths that 
begin at a node and return to that node by traversing branches only in the direction of 
their arrowheads. Such a structure in the flow graph implies that a node variable in a loop 
depends directly or indirectly on itself. A simple example is shown in Figure 6.23(a), 
which represents the difference equation 
y[n] = ay[n - 1] + x[n]. 
(6.39) 

Chapter 6 
Structures for Discrete-Time Systems
396 
~[nJ' ~
• y[~ 
a 
Z-l 
(a) 
y[n]
x[n] 
rl 
a 
(b) 
a 
V 
a 
x[n] 
y[n] 
Figure 6.23 
(a) System with feedback 
loop. (b) FIR system with feedback loop. 
a 
(c) 
(c) Noncomputable system. 
Such loops are necessary (but not sufficient) to generate infinitely long impulse 
responses. This can be seen if we consider a network with no feedback loops. In such a 
case, any path from the input to the output can pass through each delay element only 
once. Therefore, the longest delay between the input and output would occur for a path 
that passes through all of the delay elements in the network. Thus, for a network with 
no loops, the impulse response is no longer than the total number of delay elements 
in the network. From this, we conclude that if a network has no loops, then the system 
function has only zeros (except for poles at z = 0), and the number of zeros can be no 
more than the number of delay elements in the network. 
Returning to the simple example of Figure 6.23(a), we see that when the input 
is the impulse sequence 8[n], the single-input sample continually recirculates in the 
feedback loop with either increasing (if lal > 1) or decreasing (if lal < 1) amplitude 
owing to multiplication by the constant a, so that the impulse response is h[n] = an urn]. 
This illustrates how feedback can create an infinitely long impulse response. 
If a system function has poles, a corresponding block diagram or signal flow graph 
will have feedback loops. On the other hand, neither poles in the system function nor 
loops in the network are sufficient for the impulse response to be infinitely long. Fig­
ure 6.23(b) shows a network with a feedback loop, but with an impulse response of 
finite length. This is because the pole of the system function cancels with a zero; i.e., for 
Figure 6.23(b), 
(l-ac
1)(1+ac
1
) = 1+az-1. 
(6.40)
H(z) = 1"1­
The impulse response of this system is h[n] = 8[n] + a8[n 
1]. The system is a simple 
example of a general class of FIR systems called frequency-sampling systems. This class 
of systems is considered in more detail in Problems 6.39 and 6.51. 
L 

397 
Section 6.4 
Transposed Forms 
Loops in a network pose special problems in implementing the computations im­
plied by the network. As we have discussed, it must be possible to compute the node 
variables in a network in sequence such that all necessary values are available when 
needed. In some cases, there is no way to order the computations so that the node 
variables of a flow graph can be computed in sequence. Such a network is called non­
computable (see Crochiere and Oppenheim, 1975). A simple noncomputable network 
is shown in Figure 6.23(c). The difference equation for this network is 
y[n] = ay[nJ + x[nJ. 
(6.41) 
In this form, we cannot compute y[nJ because the right-hand side of the equation in­
volves the quantity we wish to compute. The fact that a flow graph is noncomputable 
does not mean the equations represented by the flow graph cannot be solved; indeed, 
the solution to Eq. (6.41) is y[n] 
x[n]/(l-a). It simply means that the flow graph does 
not represent a set of difference equations that can be solved successively for the node 
variables. The key to the computability of a flow graph is that all loops must contain at 
least one unit delay element. Thus, in manipulating flow graphs representing implemen­
tations of LTI systems, we must be careful not to create delay-free loops. Problem 6.37 
deals with a system having a delay-free loop. Problem 7.51 shows how a delay-free loop 
can be introduced. 
6.4 TRANSPOSED FORMS 
The theory oflinear signal flow graphs provides a variety of procedures for transforming 
such graphs into different forms while leaving the overall system function between 
input and output unchanged. One of these procedures, called flow graph reversal or 
transposition, leads to a set of transposed system structures that provide some useful 
alternatives to the structures discussed in the previous section. 
Transposition of a flow graph is accomplished by reversing the directions of all 
branches in the network while keeping the branch transmittances as they were and 
reversing the roles of the input and output so that source nodes become sink nodes and 
vice versa. For single-input, single-output systems, the resulting flow graph has the same 
system function as the original graph if the input and output nodes are interchanged. 
Although we will not formally prove this result here,3 we will demonstrate that it is 
valid with two examples. 
Example 6.7 
Transposed Form for a 1 st-Order System with 
No Zeros 
The 1st-order system corresponding to the flow graph in Figure 6.24(a) has system 
function 
1 
(6.42) 
3The theorem follows directly from Mason's gain formula of signal How graph theory. (See Mason and 
Zimmermann, 1960; Chow and Cassignol, 1962; or Phillips and Nagle. 1995.) 
H(z) 

398 
Chapter 6 
Structures for Discrete-Time Systems 
To obtain the transposed form for this system, we reverse the directions of all the 
branch arrows, taking the output where the input was and injecting the input where 
the output was. The result is shown in Figure 6.24(b). It is usually convenient to draw 
the transposed network with the input on the left and the output on the right, as shown 
in Figure 6.24(c). Comparing Figures 6.24(a) and 6.24(c) we note that the only differ­
ence is that in Figure 6.24(a), we multiply the delayed output sequence y[n 
1] by the 
coefficient a, whereas in Figure 6.24(c) we multiply the output y[n] by the coefficient 
a and then delay the resulting product. Since the two operations can be interchanged, 
we can conclude by inspection that the original system in Figure 6.24(a) and the cor­
responding transposed system in Figure 6.24(c) have the same system function. 
• 
x[n] 
a 
)0 
1 
a• 
I" · y[nl 
(a) 
y[n] 
a 
• 
1,_' · x[~
1 ~ 
(b) 
)0 
0
• I 
y[n]
~[nJ' ,-'I 
a 
a• 
(c) 
Figure 6.24 
(a) Flow graph of simple 1sCorder system. (b) Transposed form of 
(a). (c) Structure of (b) redrawn with input on left. 
In Example 6.7, it is straightforward to see that the original system and its transpose 
have the same system function. However, for more complicated graphs, the result is 
often not so obvious. This is illustrated by the next example. 
Example 6.8 Transposed Form for a Basic 2 nd-order Section 
Consider the basic 2nd-order section depicted in Figure 6.25. The corresponding dif­
ference equations for this system are 
w[n] = al w[n 
1] + azw[n 
2] + x[nl. 
(6.43a) 
y[n] = bow[n] + hI w[n 
1] + hzw[n 
2]. 
(6.43b) 

399 
Section 6.4 
Transposed Forms 
The transposed flow graph is shown in Figure 6.26; its corresponding difference 
equations are 
vo[n] = bOx[n] + vl[n - 1], 
(6,44a) 
y[n] = VO[n], 
(6,44b) 
vl[n] 
aIY[n] + blx[n] + v2[n -1], 
(6,44c) 
v2[n] = a2y[n] +b2x[n]. 
(6.44d) 
Equations (6,43a)-(6,43b) and Eqs. (6,44a)-(6.44d) are different ways to orga­
nize the computation of the output samples y[n] from the input samples x[n], and it 
is not immediately clear that the two sets of difference equations are equivalent. One 
way to show this equivalence is to use the z-transforrn representations of both sets 
of equations, solve for the ratio Y(z)/X (z) = H(z) in both cases, and compare the 
results. Another way is to substitute Eq. (6.44d) into 
(6.44c), substitute the result 
into 
(6.44a), and finally, substitute that result into Eq. (6.44b). The final result is 
y[n] = aly[n 
1] + a2y[n 
2] + box[n] + blx[n 
1] + b2x[n 
2]. 
(6.45) 
Since the network of Figure 6.25 is a direct form II structure, it is easily seen that the 
input and output of the system in Figure 6.25 also satisfies the difference Eq. (6.45). 
Therefore, for initial-rest conditions, the systems in Figures 6.25 and 6.26 are 
equivalent. 
w[n] bo 
x[n] 
al 
z-I 
bl 
y[n] 
z-I 
°2 
b2 
Figure 6.25 
Direct form II structure for Example 6.S. 
x[n] 
yIn] 
Figure 6.26 
Transposed direct form II structure for Example 6.8. 
The transposition theorem can be applied to any of the structures that we have 
discussed so far. For example, the result of applying the theorem to the direct form I 
structure of Figure 6.14 is shown in Figure 6.27, and similarly, the structure obtained by 

400 
Chapter 6 
Structures for Discrete-Time Systems 
bo 
x[n] 
Z-l 
Z-l 
Y [n l 
aj 
bl 
_-1 
~ 
a3 
bz 
I 
I 
I 
I 
I 
I 
"1 
a:~, 
j 
j 
b:,' 
1 
Figure 6.27 
General flow graph resulting from applying the transposition theorem 
to the direct form I structure of Figure 6.14. 
transposing the direct form II structure of Figure 6.15 is shown in Figure 6.28. If a signal 
flow graph configuration is transposed, the number of delay branches and the number 
of coefficients remain the same. Thus, the transposed direct form II structure is also a 
canonic structure. Transposed structures derived from direct forms are also "direct" in 
the sense that they can be constructed by inspection of the numerator and denominator 
of the system function. 
An important point becomes evident through a comparison of Figures 6.15 and 
6.28. Whereas the direct form II structure implements the poles first and then the 
zeros, the transposed direct form II structure implements the zeros first and then the 
poles. These differences can become important in the presence of quantization in finite­
precision digital implementations or in the presence of noise in discrete-time analog 
implementations. 
bo 
x[n] 
.-1 
yIn' 
bj 
~ 
al 
Z-1 
bz 
a2 
~ 
Figure 6.28 
General flow graph resulting from applying the transposition theorem 
to the direct form II structure of Figure 6.15. 

401 
Section 6.5 
Basic Network Structures for FIR Systems 
When the transposition theorem is applied to cascade or parallel structures, the 
individual 2nd-order systems are replaced by transposed structures. For example, ap­
plying the transposition theorem to Figure 6.18 results in a cascade of three transposed 
direct form II sections (each like the one in Example 6.8) with the same coefficients as 
in Figure 6.18, but with the order of the cascade reversed. A similar statement can be 
made about the transposition of Figure 6.20. 
The transposition theorem further emphasizes that an infinite variety of imple­
mentation structures exists for any given rational system function. The transposition 
theorem provides a simple procedure for generating new structures. The problems of 
implementing systems with finite-precision arithmetic have motivated the development 
of many more classes of equivalent structures than we can discuss here. Thus, we con­
centrate only on the most commonly used structures. 
6.5 BASIC NETWORK STRUCTURES FOR FIR SYSTEMS 
The direct, cascade, and parallel form structures discussed in Sections 6.3 and 6.4 are 
the most common basic structures for IIR systems. These structures were developed 
under the assumption that the system function had both poles and zeros. Although the 
direct and cascade forms for IIR systems include FIR systems as a special case, there 
are additional specific forms for FIR systems. 
6.5.1 Direct Form 
For causal FIR systems, the system function has only zeros (except for poles at z = 0), 
and since the coefficients ak are all zero, the difference equation ofEq. (6.9) reduces to 
M 
y[n] L bkx[n - k]. 
(6.46) 
k=O 
This can be recognized as the discrete convolution of x[n] with the impulse response 
bn 
n=O,1, ... ,M,
h[n] 
(6.47)
{ o 
otherwise. 
In this case, the direct form I and direct form II structures in Figures 6.14 and 6.15 both 
reduce to the direct form FIR structure as redrawn in Figure 6.29. Because of the chain 
of delay elements across the top of tbe diagram, this structure is also referred to as a 
tapped delay line structure or a transversal filter structure. As seen from Figure 6.29, the 
signal at each tap along this chain is weighted by the appropriate coefficient (impulse­
response value), and the resulting products are summed to form the output y[n]. 
The transposed direct form for the FIR case is obtained by applying the transpo­
sition theorem to Figure 6.29 or, equivalently, by setting the coefficients ak to zero in 
Figure 6.27 or Figure 6.28. The result is shown in Figure 6.30. 

402 
Chapter 6 
Structures for Discrete-Time Systems 
Z-I 
x[nJ 
h[O] 
h[2] 
hIM-I] th[M]
hIll 
yIn] 
Figure 6.29 
Direct form realization of an FIR system. 
Z-I 
Z-I 
Z-l 
yIn] 
h[2] 
h[l] 
h[O]
o 
• 
jhl~' jhl< 11 E-:l­
x[n] 
Figure 6.30 
Transposition of the network of Figure 6.29. 
6.5.2 Cascade Form 
The cascade form for FIR systems is obtained by factoring the polynomial system func­
tion. That is, we represent H (z) as 
M 
Ms 
2
H (z) = L h[n]z-n = n(bOk + blkZ-1 + b2kZ- ), 
(6.48) 
n=O 
k=l 
where Ms = L(M 
1)/2J is the largest integer contained in (M 
1)/2. If M is odd, one 
of the coefficients b2k will be zero, since H (z) in that case would have an odd number 
of real zeros. The flow graph representing Eq. (6.48) is shown in Figure 6.31, which is 
identical in form to Figure 6.18 with the coefficients alk and a2k all zero. Each of the 
2nd-order sections in Figure 6.31 uses the direct form shown in Figure 6.29. Another 
alternative is to use transposed direct form 2nd-order sections or, equivalently, to apply 
the transposition theorem to Figure 6.31. 
bOI 
b02 
bOMs ~ 
z-It 
yIn]
x[n] 
Z-l 
Z-I 
bll 
b12 
C 1 
Z-l 
b21 
b22 
b21l<ls 
Figure 6.31 
Cascade form realization of an FIR system. 

Section 6.5 
Basic Network Structures for FI RSystems 
403 
6.5.3 Structures for Linear-Phase FIR Systems 
In Chapter 5, we showed that causal FIR systems have generalized linear phase if the 
impulse response satisfies the symmetry condition 
0 
h[M - n] = h[n] 
n = 0,1, ... , M 
(6.~9a) 
I 
or 
h[M -
n] = -h[n] 
n=O,l, ... ,M. 
(6.49b) 
With either of these conditions, the number of coefficient multipliers can be es­
sentially halved. To see this, consider the following manipulations of the discrete con­
volution equation, assuming that M is an even integer corresponding to type I or type 
III systems: 
M 
y[n] = L h[k]x[n - k]  
k=O  
Mj2-1 
M 
= L h[k]x[n - k] + h[MI2]x[n -
M12] + L 
h[k]x[n -
k] 
k=O 
k=Mj2+1 
Mj2-1 
Mj2-1 
= L h[k]x[n - k] + h[MI2]x[n -
M12] + L h[M - k]x[n -
M + k]. 
k=O 
k=O 
For type I systems, we use Eq. (6.49a) to obtain 
Mj2-1 
y[n] = L h[k](x[n -
k] + x[n -
M + kD + h[M12]x[n -
M 12]. 
(6.50) 
k=O 
For type III systems, we use Eq. (6.49b) to obtain 
Mj2-1 
y[n] = L h[k](x[n - k] - x[n -
M + k]). 
(6.51) 
k=O 
For the case of M an odd integer, the corresponding equations are, for type II systems, 
(M-l)j2 
y[n] = L 
h[k](x[n - k] + x[n -
M + k]) 
(6.52) 
and, for type IV systems, 
(M-l)j2 
y[n] = L 
h[k](x[n -
k] - x[n -
M + k]). 
(6.53) 
k=O 
Equations (6.50)-(6.53) imply structures with either M 12 + 1, M12, or (M + 1)/2 
coefficient multipliers, rather than the M coefficient multipliers of the general direct 
form structure of Figure 6.29. Figure 6.32 shows the structure implied by Eq. (6.50), and 
Figure 6.33 shows the structure implied by Eq. (6.52). 
In our discussion of linear-phase systems in Section 5.7.3, we showed that the 
symmetry conditions of Eqs. (6.49a) and (6.49b) cause the zeros of H (z) to occur in 
mirror-image pairs. That is, if Zo is a zero of H(z), then 1/zo is also a zero of H(z). 
Furthermore, if h[n] is real, then the zeros of H(z) occur in complex-conjugate pairs. 

404 
Chapter 6 
Structures for Discrete-Time Systems 
z-J 
Z-l 
o 
0 
o. 
o--~----~ 
x[n] 
Z-l 
h[O] 
h [1] 
h[2] 
h[MI2 1] th[M/2] 
0--_---<:>---_--0---_--¢-......... - -- - --O-----J  
y[n] 
Figure 6.32 
Direct form structure for an FIR linear-phase system when Mis an 
even integer. 
z-t 
x[n] 
Z-l 
rl 
h [0] 
Z-l th[1] 
Z-l t h[2] 
h [(M -3)12] th[(M -1)/2] 
y[n] 
Figure 6.33 
Direct form structure for an FIR linear-phase system when Mis an 
odd integer. 
As a consequence, real zeros not on the unit circle occur in reciprocal pairs. Complex 
zeros not on the unit circle occur in groups of four, corresponding to the complex 
conjugates and reciprocals. Ifa zero is on the unit circle, its reciprocal is also its conjugate. 
Consequently, complex zeros on the unit circle are conveniently grouped into pairs. 
Zeros at z = ±l are their own reciprocal and complex conjugate. The four cases are 
summarized in Figure 6.34, where the zeros at Zh z!, l/zl, and liz! are considered 
as a group of four. The zeros at Z2 and l/z2 are considered as a group of two, as are 
the zeros at Z3 and z3' The zero at Z4 is considered singly. If H (z) has the zeros shown 
in Figure 6.34, it can be factored into a product of Ist_, 2nd_, and 4th-order factors. 
Each of these factors is a polynomial whose coefficients have the same symmetry as 
the coefficients of H (z); i.e., each factoris a linear-phase jolynomial in Cl. Therefore, 
the system can be implemented as a cascade of l5t_, 2n -, and 4th-order systems. For 
example, the system function corresponding to the zeros of Figure 6.34 can be expressed 
as 
H(z) 
h[O](l + z-I)(1 + az-l + z-2)(1 + bz-1 + z-2) 
(6.54) 
x (1 + cz-1 + dz-2 + cC3 + z-4), 
where 
a = (Z2 + llz2), 
b = 2Re{Z3}, 
c = -2Re(zl + l/zl}, 
d = 2 + IZI + llzll2. 

405 
Section 6.6 
Lattice Filters 
Tm 
1
0­zi 
Z3 
z-plane
Z10 
Z4 
Zz 
'Re 
zio 
Zz 
z:l' 
1 
Figure 6.34 Symmetry of zeros for a 
o~ 
q 
linear-phase FIR filter. 
This representation suggests a cascade structure consisting of linear-phase elements. It 
can be seen that the order of the system function polynomial is M = 9 and the number 
of different coefficient multipliers is five. This is the same number «(M + 1)/2 = 5) of 
constant multipliers required for implementing the system in the linear-phase direct 
form of Figure 6.32. Thus, with no additional multiplications, we obtain a modular 
structure in terms of a cascade of short linear-phase FIR systems. 
6.6 LATrICE FILTERS 
In Sections 6.3.2 and 6.5.2, we discussed cascade forms for both IIR and FIR systems 
obtained by factoring their system functions into 15t_ and 2nd-order sections. Another 
interesting and useful cascade structure is based on a cascade (output to input) con­
nection of the basic structure shown in Figure 6.35( a). In the case of Figure 6.35( a) the 
basic building block system has two inputs and two outputs, and is called a two-port 
flow graph. Figure 6.35(b) shows the equivalent flow graph representation. Figure 6.36 
shows a cascade of M of these basic elements with a "termination" at each end of the 
cascade so that the overall system is a single-input single-output system with input x[n] 
feeding both inputs of two-port building block (1) and output yen] defined to be a(M)[n], 
the upper output of the last two-port building block M. (The lower output of the Mth 
stage is generally ignored.) Although such a structure could take many different forms 
- In] 
a'O[n] 
Two-Port  
Flow  
Graph  
(i)  
1) n 
b(i)[n] 
(a) 
(b) 
Figure 6.35 
One section of the lattice structure for FIR lattice filters. (a) Block 
diagram representation of atwo-port building block (b) Equivalent flow graph. 

406 
Chapter 6 
Structures for Discrete-Time Systems 
(0) 
(2) 
a(M-I)[n] 
a(M)[n] 
Two-Port 
Two-Port 
Flow 
Flow 
Graph 
Graph 
(1) 
(2) 
a\1J[n] 
a
r--J 
---+---< 
0 
y[n]
1
Two-PortI 
Flow 
Graph
x[n] 
(M) 
>-----'J>o 
--:::--< 
b(~~~M)[n]
b\l)[n]
b\UJ[n] 
[n] 
Figure 6.36 
Cascade connection of M basic building block sections. 
depending on the definition of the basic building block, we will limit our attention to 
the particular choice in Figure 6.35(b), which leads to a widely used class of FIR and 
IIR filter structures known as lattice filters. 
6.6.1 FIR Lattice Filters 
If the basic butterfly-shaped two-port building block in Figure 6.35(b) is used in the 
cascade of Figure 6.36, we obtain a flow graph like the one shown in Figure 6.37, whose 
lattice shape motivates the name lattice filter. The coefficients kh k2, ... , kM, are referred 
to generally as the k-parameters of the lattice structure. In Chapter 11, we will see that 
the k-parameters have a special meaning in the context of all-pole modeling of signals, 
and the lattice filter ofFigure 6.37 is an implementation structure for a linear prediction 
of signal samples. In the current chapter, our focus is only on the use of lattice filters to 
implement FIR and all-pole IIR transfer functions. 
The node variables a(i)[n] and b(il[n] in Figure 6.37 are intermediate sequences 
that depend upon the input x[n] through the set of difference equations 
a(O)[n] = b(O)[n] = x[n] 
(6.55a) 
a(i)[n] = aU-Urn] -
kib(i-1)[n 
1] 
i = 1,2, ... , M 
(6.55b) 
b(i)[n] = bU-1)[n - 1] 
kia(i-l)[n] i = 1,2, ... , M 
(6.55c) 
y[n] 
a(Ml[n]. 
(6.55d) 
As we can see, the k-parameters are coefficients in this set of M coupled difference 
equations represented by Figure 6.37 and Eqs. (6.55a)-(6.55d). It should be clear that 
x[n] 
a(O)[n] 
a(l)[n] 
a(2)[n] 
-kl 
-k2 
4 
~ 
~ 
z 
Z 
__ U_h_~ 
~ 
b(O)[n] 
b(J)[n] 
b(2)[nl 
b(M)[n] 
Figure 6.37 Lattice flow graph for an FIR system based on acascade of Mtwo­
port building blocks of Figure 6.35{b). 

ms 
Section 6.6 
Lattice Filters 
407 
these equations must be computed in the order shown (i 
0, 1, ... , M) since the output 
of stage (i 
1) is needed as input to stage (I), etc. 
The lattice structure in Figure 6.37 is clearly an LTI system, since it is a linear signal 
flow graph with only delays and constant branch coefficients. Furthermore, note that 
there are no feedback loops, which implies that the system has a finite-duration impulse 
response. In fact, a straightforward argument is sufficient to show that the impulse 
response from the input to any internal node has finite length. Specifically, consider the 
impulse response from x[n] to the node variable a{i)[n], i.e., from the input to the jth 
upper node. It is clear that if x[n] = 8[n], then a(i)[O] = 1 for every i, since the impulse 
propagates with no delay through the top branch of all the stages. All other paths to any 
node variable a(i)[n] or bU)[n] pass through at least one delay, with the greatest delay 
to 
being along the bottom path and then up to node variable aU) [nl through the coefficient 
nd 
-kj • This will be the last impulse that arrives at a(i)[n], so the impulse response will have 
length i + 1 samples. All other paths to an internal node zigzag between the top and 
bottom of the graph, thereby passing through at least one, but not all, of the delays 
occurring before the outputs of section (i). 
Note that in our introduction to lattice filters, a(i)[n] and b(i)[n] were used in 
he 
Figure 6.37 and Eqs. (6.55a)--(6.55d) to denote the node variables of building block (i) 
for any input x[n]. However, for the remainder of our discussion, it is convenient to 
assume specifically that x[n] = 8[n] so that a(i)[n] and b(i)[n] are the resulting impulse 
responses at the associated nodes, and that the corresponding z-transforms A (i) (z) and 
B(I)(z) are the transfer functions between the input and the ith nodes. Consequently, 
the transfer function between the input and the upper ith node is 
i 
i 
A (i) (z) 
La(i)[n]z-n = 1 '" a(i)z-m 
(6.56)
L-,;m· 
; 
n=O 
m=l 
where in the second form, the coefficients a~) for m ::: i are composed of sums of 
products of the coefficients kj for j ::: m. As wc have shown, the coefficient for the 
longest delay from the input to the upper node i is aii) 
ki. In this notation, the 
impulse response from x[n] to node variable a(i)[n] is 
a(i) [n] 
{ ~a~i) 
~::: ~ ::: i 
(6.57) 
o 
otherwise 
Similarly, the transfer function from the input to the lower node i is denoted 
B(i) (z). Therefore, from Figure 6.35(b) or Eqs. (6.55b) and (6.55c), we see that 
(6.58a) 
(6.58b) 
Also, we note that at the input end (i = 0) 
Ao(z) = Bo(z) = 1. 
(6.59) 
Using Eqs. (6.58a) and (6.58b) and starting with Eq. (6.59), we can calculate A(i)(Z) 
and B(i) (z) recursively up to any value of i. If we continue, the pattern that emerges in 
the relationship between B(I)(z) and A(i)(z) is 
B(i)(z) 
A(i)(l/z) 
(6.60a) 

408 
Chapter 6 
Structures for Discrete-Time Systems 
or by replacing z by l/z in Eq. (6.60a) we have the equivalent relation 
A(i)(Z) = Z~i B(i)(1/z). 
(6.60b) 
We can verify these equivalent relationships formally by induction, Le., by verifying 
that if they are true for some value i - 1 then they will be true for i. Specifically, it is 
straightforward to see from Eq. (6.59) that Eqs. (6.60a) and (6.60b) are true for i 
O. 
Now note that for i = 1, 
A(1)(z) 
A (0) (z) 
k1Z~l B(O) (z) = 1 -
k1Z~l 
B(l) (z) 
-kl A (0) (z) + Z-I B(O) (z) = -kl + 
= z-l(l 
kIZ) 
= z~l A(l\l/z) 
and for i 
2, 
A(2)(z) 
A(l)(z) 
k2Z~1 B(l)(Z) = 1 
kg-1 
k2Z-2(1 
kg) 
= 1- kl(1- k2)Z-1 
k2Z~2 
B(2)(z) = -k2A(1)(z) + 
B(I)(z) = -k2(1 
klZ~I) + z~2(1 
kIZ) 
= z~2(1 - ki (1 
k2)Z 
k2Z2) 
= z-2A(2)(1/z). 
We can prove the general result by assuming that Eq. (6.60a) and Eq. (6.60b) are true 
for i 
1, and then substitute into Eq. (6.58b) to obtain 
B(i)(z) 
-kiZ~(i-l) B(i~l)(l/z) + z-Iz-(i-l) A(i-I)(l/z) 
= Z-i [A(i-I)(l/z) 
ki ZB(i-I)(l/ Z)]. 
From Eq. (6.58a) it follows that the term in brackets is A (i) (1 / z), so that in general, 
B(i)(z) = z-i A(i)(l/z), 
as in Eq. (6.60a). Thus, we have shown that Eqs. (6.60a) and (6.60b) hold for any i 2: O. 
As indicated earlier, the transfer functions A (i) (z) and B(i) (z) can be computed 
recursively using Eq. (6.58a) and (6.58b). These transfer functions are ith-order polyno­
mials, and it is particularly useful to obtain a direct relationship among the coefficients 
of the polynomials. Toward this end, the right side of Eq. (6.57) defines the coefficients 
of A(i)(z) to be -aZ), for m = 1,2, ... , i with the leading coefficient equal to one; i.e., 
as in Eq. (6.56), 
i 
A (i)(z) = 1 
"" ali)Z-m 
(6.61)
~ m 
, 
m=1 
and similarly, 
i~l 
A(i-l)(z) = 1 L aZ-1)z-m. 
(6.62) 
m=1 

409 
Section 6.6 
Lattice Filters 
To obtain a direct recursive relationship for the coefficients (X~) in terms of (X~-1) and 
k" we combine Eqs. (6.60a) and (6.62) from which it follows that 
B(i-l)(z) = z-U-1) A (I-1)(1/z) = Z-(I-1) [1 
(6.63)
f a~-l)z+m] . 
m=l 
Substituting Eqs. (6.62) and (6.63) into Eq. (6.58a), A(i)(z) can also be expressed as 
AU'(,) (1 -E
a~-l'z-m) 
k;Z-1 (,-U-1) [1 E
a~-1)z+m]) . 
(6.64) 
Re-indexing the second summation by reversing the ordering of the terms (i.e., replacing 
m by i - m and re-summing) and combining terms in Eq. (6.64) leads to 
1-1 
A(i)(z) = 1- L [a~-l) -
klai~~l)J 
(6.65) 
m=l 
where we see that, as indicated earlier, the coefficient of Z-i is -ki. Comparing Eqs. (6.65) 
and (6.61) shows that 
·
k (I-1)J 
m = 1, ...,i-I 
(6.66a)
l(Xi_m 
(i) 
kj. 
(6.66b)
ai 
Equations (6.66) are the desired direct recursion between the coefficients of A (i) (z) and 
the coefficients of A(i-1)(z). These equations, together with Eq. (6.60a) also determine 
the transfer function B(i)(z). 
The recursion of Eqs. (6.66) can also be expressed compactly in matrix form. We 
denote by «1-1 the vector of transfer function coefficients for A(i-l)(z) and by &i-1 
these coefficients in reverse order, i.e., 
(i-I) (i-I) 
(i-1)J T 
«1-1 = [ (Xl 
(X2 
... (Xi_1 
and 
v 
[ 
(/-1) (i-l) 
U-1lJT
«i-1 
(X1_l 
(Xi_2 
... a 1 
Then 
(6.66) can be expressed as the matrix equation 
«j-1 ] 
[&i-l ] 
«t = [ ..O· . - ki 
... '1' 
. 
(6.67) 
The recursion in Eqs. (6.66) or Eqs. (6.67) is the basis for an algorithm for analyzing 
an FIR lattice structure to obtain its transfer function. We begin with the flow graph 
specified as in Figure 6.37 by the set of k-parameters {kb k2, ... , kM}. Then we can use 
Eqs. (6.66) recursively to compute the transfer functions of successively higher-order 
FIR filters until we come to end of the cascade giving us 
M 
-m 
Y(z)
A(z)=l-
(XmZ 
=--, 
(6.68a)
L 
X(z)
m=l 

410 
Chapter 6 
Structures for Discrete-Time Systems 
k-Parameters-to-Coefficients Algorithm 
Given kI, k2, ... , kM  
for i 
1, 2, ... , M  
aU) 
ki  
I 
Eq. (6.66b)  
if i > 1 then for j = 1, 2, . , . ,i - 1  
a~i) = a(i-I) _ 
IT'a~i-:l)  
Eq. (6.66a)
] 
] 
"t 1- ] 
end  
end  
Figure 6.38 Algorithm for converting 
aj = ajM) j = 1, 2, ... , M 
Eq. (6.68b) 
from k-parameters to FIR filter 
coefficients. 
where 
Cim = Ci~~) 
m = 1,2, ... , M. 
(6.68b) 
The steps of this algorithm are represented in Figure 6.38. 
It is also of interest to obtain the k-parameters in the FIR lattice structure that 
realize a given desired transfer function from inputx[n] to the output y[n] = a(M}[n];i.e., 
we wish to go from A(z) specified as a polynomial by Eqs. (6.68a) and (6.68b) to the set 
ofk-parameters for the lattice structure in Figure 6.37. This can be done by reversing the 
recursion of Eqs. (6.66) or (6.67) to obtain successively the transfer function AU-l)(z) in 
terms of A (I) (z) for i 
M, M 
1, ... , 2. The k-parameters are obtained as a by-product 
of this recursion. 
Specifically, we assume that the coefficients Ci~M) = Cim for m 
1, ... , M are spec­
ified and we want to obtain the k-parameters kl' .. ., kM to realize this transfer function 
in lattice form. We start with the last stage of the FIR lattice, i.e., with i = M. From 
Eq. (6.66b), 
kM=CiM 
(M) 
CiM 
(6.69) 
with A (M) (z) defined in terms of the specified coefficients as 
M 
M 
m
A(M)(Z) 
1- LCi~M)z-m 
1 
LCimZ-
. 
(6.70) 
m=l 
m=l 
Inverting Eqs. (6.66) or equivalently Eq. (6.67), with i 
M and kM 
Ci~) then 
determines a.M-1, the vector of transform coefficients at the next to last stage i = M -1. 
This process is repeated until we reach A (1) (z). 
To obtain a general recursion formula for Ci~:-l) in terms of Ci~) from Eq. (6.66a) 
note that Ci;~~) must be eliminated. To do this, replace m by i 
min Eq. (6.66a) and 
multiply both sides of the resulting equation by ki thereby obtaining 
k . (i) 
k. (i-l) 
k 2 (i-I)
,Cii-
,Cii -
jCim
· 
m 
m 
Adding this equation to Eq. (6.66a) results in 
Ci(i) + k'Ci(i) = Ci(i-I) _ eaCi- I ) 
m 
l 
l-m 
m 
I 
m 
from which it follows that 
(i-I) 
Ci~) + 
Cim 
= _-:-_-'-='m 
m 
1, 2, ... , i 
1. 
(6.71a)
1 

411 
Section 6.6 
Lattice Filters 
With a~-l) calculated for m = 1,2, ... , i 
1 we note from Eq. (6.66b) that 
a(i-l) 
(6.71b)
,-1 
Thus, starting with af:l = am, m 
1,2, ... M we can use Eqs. (6.71a) and (6.71b) 
to compute a~M-l), for m 
1,2, ... , M 
1 and kM-l, and then repeat this process 
recursively to obtain all of the transfer functions A(i)(Z) and, as a by-product, all of the k­
parameters needed for the lattice structure. The algorithm is represented in Figure 6.39. 
Coefficients-to-k-Parameters Algorithm 
Given a(M) 
aj j = I, 2, ... , M 
kM 
a~) 
Eq. (6.69) 
for i = M, M 
1, ... , 2 
for j = 1, 2, ... , i 
1 
(i) 
U)
(i-I) _ aj + Iqai_ j 
aj 
-
1- JJ 
Eq. (6.71a) 
I 
end 
lr. 
(i-I)
"i-I = ai-
Eq. (6.71b) 
1 
end 
Figure 6.39 Algorithm for converting from FIR filter coefficients to k-parameters. 
Example 6.9 k-Parameters for a 3rd-Order FIR System 
Consider the FIR system shown in Figure 6.40a whose system function is 
A(z) 
1 - 0.9:::-1 + 0.64:::-2 - 0.576z-3. 
Consequently, M = 3 and the coefficients oP) in Eq. (6.70), are 
of) 
0.9 a?) = 0.64 
a~3) = 0.576. 
We begin by observing that k3 = a~3) 
0.576. 
Next we want to calculate the coefficients for transfer function A(2) (z) using 
Eq. (6.71a). Specifically, applying Eq. (6.71a), we obtain (rounded to three decimal 
places): 
(3) + k 
(3)
(2) 
3(X2_
a l 
2 
-
0.795
a l 
1 -k3 
(3) + k 
(3) 
(2) 
(X2 
3(X, 
= -0.182 
(X2 
1- k~ 
From Eq. (6.71b) we then identify k2 = af) = -0.182  
To obtain A (1) (z) we again apply Eq. (6.71a) obtaining  
(2) 
(2)
(1) 
+ k2al
a 1 
0.673.
at = 
2
1- k2 

0 
412 
Chapter 6 
Structures for Discrete-Time Systems 
We then identify k1 
ail) = 0.673. The resulting lattice structure is shown in Fig­
ure6.40b. 
Z-l 
Z-l 
z-l 
;>----+---<rn]'! : 
~91 :O~I ~5761 . 
y[n] 
(a) 
x[nl 
Z-l 
(b) 
Figure 6.40 
Flow graphs for example. (a) Direct form. (b) Lattice form (coeffi­
cients rounded). 
6.6.2 AlI·Pole Lattice Structure 
A lattice structure for implementing the all-pole system function H (z) = 1/A(2) can 
be developed from the FIR lattice of the previous section by recognizing that H (z) 
is the inverse filter for the FIR system function A(z). To derive the all-pole lattice 
structure, assume that we are given y[n] = a(M)[n], and we wish to compute the input 
a(O)[n] 
x[n]. This can be done by working from right to left to invert the computations 
in Figure 6.37. More specifically, if we solve Eq. (6.58a) for A (i-1)(Z) in terms of A (i) (z) 
and B(i-1) (z) and leave Eq. (6.58b) as it is, we obtain the pair of equations 
AU-l)(Z)=A(i)(z)+ki 
B(i-l)(z) 
(6.72a) 
B(i)(Z) 
-ki A (i-1)(Z) + Z-l B(;-l)(Z), 
(6.72b) 
which have the flow graph representation shown in Figure 6.41. Note that in this case, 
the signal flow is from i to i 
1 along the top of the diagram and from i-I to i along 
the bottom. Successive connection of M stages of Figure 6.41 with the appropriate ki 
in each section takes the input a(M)[n] to the output a(0) [n] as shown in the flow graph 
of Figure 6.42. Finally, the condition x[n] = a(O)[n] 
b(O)[n] at the terminals of the last 
stage in Figure 6.42 causes a feedback connection that provides the sequences b(i) [n] 
that propagate in the reverse direction. Such feedback is, of course, necessary for an 
IIR system. 

413 
Section 6.6 
Lattice Filters 
Figure 6.41 
One stage of computation 
for an all-pole lattice system. 
The set of difference equations represented by Figure 6.42 is4 
a{M)[n] 
yIn] 
(6.73a) 
a(i-l)[n] = a(i)[n] + kib(i-l)[n -1] i = M, M 
1, ... ,1 
(6.73b) 
b(i)[n] = bU-1)[n -1] - kiaU-1)[n] 
i = M, M -1, ... ,1 
(6.73c) 
x[n] = a(O)[n] = b(Ol[n]. 
(6.73d) 
Because of the feedback inherent in Figure 6.42 and these corresponding equations, 
initial conditions must be specified for all of the node variables associated with delays. 
Typically, we would specify b(il[-1] 
0 for initial rest conditions. Then, if Eq. (6.73b) 
is evaluated first, a(i-l)[n] will be available at times n :::: 0 for evaluation of Eq. (6.73c) 
with the values of b(i-l)[n - 1] having been provided by the previous iteration. 
Now we can state that all the analysis of Section 6.6.1 applies to the all-pole lattice 
system of Figure 6.42. Ifwe wish to obtain a lattice implementation of an all-pole system 
with system function H(z) = 1/A(z), we can simply use the algorithms in Figures 6.39 
and 6.38 to obtain k-parameters from denominator polynomial coefficients orvice-versa. 
y[n] = a(Ml[n] 
x[n] 
Figure 6.42 All-pole lattice system. 
4Note that by basing our derivation of the all-pole lattice on the FIR lattice in Figure 6.37, we have 
ended up with the input denoted y[n] and the output x[n] in opposition to our normal convention. This 
labeling is, of course, arbitrary once the derivation has been completed. 

414 
Chapter 6 
Structures for Discrete-Time Systems 
Example 6.10 Lattice Implementation of an IIR System 
As an example of an IIR system, consider the system function 
B(z) = 
1 
(6.74a)
1 
1 
(6.74b)
(1 - O.8jC1)(1 +O.8jc1)(1- O.9c1 ) 
which is the inverse system for the system in Example 6.9. Figure 6.43(a) shows the 
direct form realization of this system, whereas Figure 6.43(b) shows the equivalent 
IIR lattice system using the k-parameters computed as in Example 6.9. Note that the 
lattice structure has the same number of delays (memory registers) as the direct form 
structure. However, the number of multipliers is twice the number of the direct form. 
This is obviously true for any order M. 
x[n] 
yIn] 
0.9 
-0.64 
0.576 
-
[1 
Z-l 
Z-I 
(a) 
x[n] 
yIn] 
Z-l 
(b) 
Figure 6.43 
Signal flow graph of IIR filter; (a) direct form, (b) lattice form. 
Since the lattice structure of Figure 6.42 is an IIR system, we must be concerned 
about its stability. We will see in Chapter 13 that a necessary and sufficient condi­
tion for all the zeros of a polynomial A(z) to be inside the unit circle is Ikj I < 1, 
i 
1, 2, ... , M. (See Markel and Gray, 1976.) Example 6.10 confirms this fact since, as 
shown in Eq. (6.74b) the poles of H(z) (zeros of A{z» are located inside the unit circle 
of the z-plane and all the k-parameters have magnitude less than one. For IIR systems, 
the guarantee of stability inherent in the condition Ikj I < 1 is particularly important. 
Even though the lattice structure requires twice the number of multiplications per out­
put sample as the direct form, it is insensitive to quantization of the k-parameters. This 
property accounts for the popularity of lattice filters in speech synthesis applications. 
(See Quatieri, 2002 and Rabiner and Schafer, 1978.) 

415 
Section 6.7 
Overview of Finite-Precision Numerical Effects 
6.6.3 Generalization of Lattice Systems 
We have shown that FIR systems and all-pole IIR systems have a lattice structure 
representation. When the system function has both poles and zeros, it is still possible to 
find a lattice structure based upon a modification of the all-pole structure of Figure 6.42. 
The derivation will not be provided here (See Gray and Markel, 1973, 1976.), but it is 
outlined in Problem 11.27. 
6.7  OVERVIEW OF FINITE-PRECISION NUMERICAL 
EFFECTS 
We have seen that a particular LTI discrete-time system can be implemented by a 
variety of computational structures. One motivation for considering alternatives to the 
simple direct form structures is that different structures that are equivalent for infinite­
precision arithmetic may behave differently when implemented with finite numerical 
precision. In this section, we give a brief introduction to the major numerical problems 
that arise in implementing discrete-time systems. A more detailed analysis ofthese finite 
word-length effects is given in Sections 6.8-6.10. 
6.7.1 Number Representations 
In theoretical analyses of discrete-time systems, we generally assume that signal values 
and system coefficients are represented in the real-number system. However, with ana­
log discrete-time systems, the limited precision of the components of a circuit makes 
it difficult to realize coefficients exactly. Similarly, when implementing digital signal­
processing systems, we must represent signals and coefficients in some digital number 
system that must always have finite precision. 
The problem of finite numerical precision has already been discussed in Sec­
tion 4.8.2 in the context of AID conversion. We showed there that the output samples 
from an AID converter are quantized and thus can be represented by fixed-point binary 
numbers. For compactness and simplicity in implementing arithmetic, one of the bits 
of the binary number is assumed to indicate the algebraic sign of the number. Formats 
such as sign and magnitude, one's complement, and two's complement are possible, but 
two's complement is most common.5 A real number can be represented with infinite 
precision in two's-complement form as 
x =  Xm (-bO+ I:biTi) , 
(6.75) 
1=1 
where Xm is an arbitrary scale factor and the bjs are either 0 or 1. The quantity bo is 
referred to as the sign bit. If bo = 0, then °:::: x :::: Xm, and if bo = 1, then - Xm :::: x < 0. 
Thus, any real number whose magnitude is less than or equal to Xm can be represented 
by Eq. (6.75). An arbitrary real number x would require an infinite number of bits for 
its exact binary representation. As we saw in the case of AID conversion, if we use only 
5 A detailed description of binary number systems and corresponding arithmetic is given by Knuth 
(1997). 

416 
Chapter 6 
Structures for Discrete-Time Systems 
a finite number of bits (B + 1 ), then the representation of Eq. (6.75) must be modified 
to 
x = QB[X] = Xm (-bO + t biri) = XmXB. 
(6.76) 
1=1 
The resulting binary representation is quantized, so that the smallest difference between 
numbers is 
t.. = Xm2-B. 
(6.77) 
In this case, the quantized numbers are in the range -Xm :s x < Xm. The fractional 
part of x can be represented with the positional notation 
XB = boob1b2b3'" bB, 
(6.78) 
where <> represents the binary point. 
The operation of quantizing a number to (B + 1) bits can be implemented by 
rounding or by truncation, but in either case, quantization is a nonlinear memory­
less operation. Figures 6.44(a) and 6.44(b) show the input-output relation for two's­
complement rounding and truncation, respectively, for the case B = 2. In considering 
the effects of quantization, we often define the quantization error as 
e = QB[X] - x. 
(6.79) 
For the case oftwo's-complement rounding, - t.. /2 < e :s t.. /2, and for two's-complement 
truncation, -t.. < e :s 0.6 
If a number is larger than Xm (a situation called an overflow), we must implement 
some method of determining the quantized result. In the two's-complement arithmetic 
system, this need arises when we add two numbers whose sum is greater than Xm • For 
example, consider the 4-bit two's-complement number 0111, which in decimal form 
is 7. If we add the number 0001, the carry propagates all the way to the sign bit, so 
that the result is 1000, which in decimal form is -8. Thus, the resulting error can be 
very large when overflow occurs. Figure 6.45(a) shows the two's-complement rounding 
quantizer, including the effect of regular two's-complement arithmetic overflow. An 
alternative, which is called saturation overflow or clipping, is shown in Figure 6.45(b). 
This method of handling overflow is generally implemented for AID conversion, and 
it sometimes is implemented in specialized DSP microprocessors for the addition of 
two's-complement numbers. With saturation overflow, the size of the error does not 
increase abruptly when overflow occurs; however, a disadvantage of the method is that 
it voids the following interesting and useful property of two's-complement arithmetic: 
If several two's-complement numbers whose sum would not overflow are added, then 
the result of two's-complement accumulation of these numbers is correct, even though 
intermediate sums might overflow. 
Both quantization and overflow introduce errorsin digital representations ofnum­
bers. Unfortunately, to minimize overflow while keeping the number of bits the same, 
we must increase Xm and thus increase the size of quantization errors proportionately. 
Hence, to simultaneously achieve wider dynamic range and lower quantization error, 
we must increase the number of bits in the binary representation. 
6Note that Eq. (6.76) also represents the result of rounding or truncating any (Bl + 1)-bit binary 
representation, where Bl > B. In this case Ll would be replaced by (Ll - X m 2- Bj ) in the bounds on the size 
of the quantization error. 

Section 6.7 
Overview of Finite-Precision I~umerical Effects 
417 
(a) 
Q[x] 
3 
1 
1 
! 
! 
l 
x 
4 
2 
4 
4 
2 
4 
x 
1 
4 
Figure 6.44 
Nonlinear relationships 
representing two's-complement 
(a) rounding and (b) truncation for 
(b) 
B =2. 
So far, we have simply stated that the quantity Xm is an arbitrary scale factor; 
however, this factor has several useful interpretations. In AID conversion, we considered 
Xm to be the full-scale amplitude of the AID converter. In this case, Xm would probably 
represent a voltage or current in the analog part of the system. Thus, Xm serves as a 
calibration constant for relating binary numbers in the range -1 :s xB < 1 to analog 
signal amplitudes. 
In digital signal-processing implementations, it is common to assume that all signal 
variables and all coefficients are binary fractions. Thus, if we multiply a (B +1 )-bit signal 
variable by a (B + l)-bit coefficient, the result is a (2B + l)-bit fraction that can be 
conveniently reduced to (B + 1) bits by rounding or truncating the least significant bits. 
With this convention, the quantity Xm can be thought of as a scale factor that allows the 

418 
Chapter 6 
Structures for Discrete-Time Systems 
x=Q[x] 
011 
O1l 
x 
111 
(a) 
x Q[x] 
x 
(b) 
Figure 6.45 Two's-complement rounding. (a) Natural overflow. (b) Saturation. 
representation of numbers that are greater than unity in magnitude. For example, in 
fixed-point computations, it is common to assume that each binary number has a scale 
factor of the form Xm = 2c . Accordingly, a value c = 2 implies that the binary point is 
actually located between b2 and b3 of the binary word in Eq. (6.78). Often, this scale 
factor is not explicitly represented; instead, it is implicit in the implementation program 
or hardware architecture. 

419 
Section 6.7 
Overview of Finite-Precision Numerical Effects 
Still another way of thinking about the scale factor Xm leads to the floating-point 
representations, in which the exponent c of the scale factor is called the characteristic 
and the fractional part XB is called the mantissa. The characteristic and the mantissa 
are each represented explicitly as binary numbers in floating-point arithmetic systems. 
Floating-point representations provide a convenient means for maintaining both a wide 
dynamic range and a small quantization noise; however, quantization error manifests 
itself in a somewhat different way. 
6.7.2 Quantization in Implementing Systems 
Numerical quantization affects the implementation of LTI discrete-time systems in sev­
eral ways. As a simple illustration, consider Figure 6.46(a), which shows a block diagram 
(a) 
(b) 
(e) 
Figure 6.46 Implementation of discrete-time filtering of an analog signal. (a) Ideal 
system. (b) Nonlinear model. (c) Linearized model. 

420 
Chapter 6 
Structures for Discrete-Time Systems 
for a system in which a bandlimited continuous-time signal xc(t) is sampled to obtain 
the sequence x[n], which is the input to an LTI system whose system function is 
1 
H(z) 
(6.80)
1 az-1 ' 
The output of this system, y[n], is converted by ideal band limited interpolation to the 
bandlimited signal yc(t). 
A more realistic model is shown in Figure 6.46(b). In a practical setting, sampling 
would be done with an AID converter with finite precision of (Bi + 1) bits. The sys­
tem would be implemented with binary arithmetic of (B + 1) bits. The coefficient a in 
Figure 6,46(a) would be represented with (B + 1) bits of precision. Also, the delayed 
variable v[n 
1] would be stored in a (B + I)-bit register, and when the (B + I)-bit 
number v[n - 1] is multiplied by the (B + 1)-bit number a, the resulting product would 
be (2B + 1) bits in length. If we assume that a (B + I)-bit adder is used, the product 
iH)[n 
1] must be quantized (i.e., rounded or truncated) to (B + 1) bits before it can 
be added to the (Bt + I)-bit input sample x[n]. When Bi < B, the (Bt + 1) bits of the 
input samples can be placed anywhere in the (B + I)-bit binary word with appropriate 
extension of the sign. Different choices correspond to different scalings of the input. 
The coefficient a has been quantized, so leaving aside the other quantization errors, 
the system response cannot in general be the same as in Figure 6,46(a). Finally, the 
(B + I)-bit samples v[n], computed by iterating the difference equation represented 
by the block diagram, would be converted to an analog signal by a (Bo + I)-bit D/A 
converter. When Bo < B, the output samples must be quantized further before D/A 
conversion. 
Although the model of Figure 6.46(b) could be an accurate representation of 
a real system, it would be difficult to analyze. The system is nonlinear owing to the 
quantizers and the possibility of overflow at the adder. Also, quantization errors are 
introduced at many points in the system. The effects of these errors are impossible to 
analyze precisely, since they depend on the input signal, which we generally consider to 
be unknown. Thus, we are forced to adopt several different approximation approaches 
to simplify the analysis of such systems. 
The effect of quantizing the system parameters, such as the coefficient a in Fig­
ure 6,46(a), is generally determined separately from the effect of quantization in data 
conversion or in implementing difference equations. That is, the ideal coefficients of a 
system function are replaced by their quantized values, and the resulting response func­
tions are tested to see whether, in the absence of other quantization in the arithmetic, 
quantization of the filter coefficients has degraded the performance of the system to 
unacceptable levels. For the example of Figure 6.46, if the real number a is quantized 
to (B + 1) bits, we must consider whether the resulting system with system function 
~ 
1 
H(z)=-
(6.81)
1­
is close enough to the desired system function H (z) given by Eq. (6.80). Since there are 
only 2B+1 different (B + 1 )-bit binary numbers, the pole of H (z) can occur only at 2B+1 
locations on the real axis of the z-plane, and, while it is possible that a a, in most cases 
some deviation from the ideal response would result. This type of analysis is discussed 
in more general terms in Section 6.8. 

421 
Section 6.8 
The Effects of Coefficient Quantization 
The nonlinearity of the system of Figure 6.46(b) causes behavior that cannot occur 
in a linear system. Specifically, systems such as this can exhibit zero-input limit cycles, 
where the output oscillates periodically when the input becomes zero after having been 
nonzero. Limit cycles are caused both by quantization and by overflow. Although the 
analysis of such phenomena is difficult, some useful approximate results have been 
developed. Limit cycles are discussed briefly in Section 6.10. 
If care is taken in the design of a digital implementation, we can ensure that 
overflow occurs only rarely and quantization errors are small. Under these conditions, 
the system of Figure 6.46(b) behaves very much like a linear system (with quantized 
coefficients) in which quantization errors are injected at the input and output and at 
internal points in the structure where rounding or truncation occurs. Therefore, we can 
replace the model of Figure 6.46(b) by the linearized model of Figure 6.46( c), where the 
quantizers are replaced by additive noise sources (see Gold and Rader, 1969; Jackson, 
1970a, 1970b). Figure 6.46(c) is equivalent to Figure 6.46(b) if we know each of the noise 
sources exactly. However, as discussed in Section 4.8.3, useful results are obtained if 
we assume a random noise model for the quantization noise in AID conversion. This 
same approach can be used in analyzing the effects of arithmetic quantization in digital 
implementations of linear systems. As seen in Figure 6.46( c), each noise source injects 
a random signal that is processed by a different part of the system, but since we assume 
that all parts of the system are linear, we can compute the overall effect by superposition. 
In Section 6.9, we illustrate this style of analysis for several important systems. 
In the simple example of Figure 6.46, there is little flexibility in the choice of 
structure. However, for higher-order systems, we have seen that there is a wide variety 
of choices. Some of the structures are less sensitive to coefficient quantization than 
others. Similarly, because different structures have different quantization noise sources 
and because these noise sources are filtered in different ways by the system, we will find 
that structures that are theoretically equivalent sometimes perform quite differently 
when finite-precision arithmetic is used to implement them. 
6.8 THE EFFECTS OF COEFFICIENT QUANTIZATION 
LTI discrete-time systems are generally used to perform a filtering operation. Methods 
for designing FIR and IIR filters, which are discussed in Chapter 7, typically assume 
a particular form for the system function. The result of the filter design process is a 
system function for which we must choose an implementation structure (a set of dif­
ference equations) from an unlimited number of theoretically equivalent implementa­
tions. Although we are almost always interested in implementations that require the 
least hardware or software complexity, it is not always possible to base the choice of 
implementation structure on this criterion alone. As we will see in Section 6.9, the im­
plementation structure determines the quantization noise generated internally in the 
system. Also, some structures are more sensitive than others to perturbations of the 
coefficients. As we pointed out in Section 6.7, the standard approach to the study of co­
efficient quantization and round-off noise is to treat them independently. In this section, 
we consider the effects of quantizing the system parameters. 

Chapter 6 
Structures for Discrete-Time Systems
422 
6.8.1 Effects of Coefficient Quantization in IIR Systems 
When the parameters ofa rational system function or corresponding difference equation 
are quantized, the poles and zeros of the system function move to new positions in the 
z-plane. Equivalently, the frequency response is perturbed from its original value. Ifthe 
system implementation structure is highly sensitive to perturbations of the coefficients, 
the resulting system may no longer meet the original design specifications, or an IIR 
system might even become unstable. 
A detailed sensitivity analysis for the general case is complicated and usually of 
limited value in specific cases of digital filter implementation. Using powerful simulation 
tools, it is usually easy to simply quantize the coefficients of the difference equations 
employed in implementing the system and then compute the corresponding frequency 
response and compare it with the desired frequency-response function. Even though 
simulation of the system is generally necessary in specific cases, it is still worthwhile 
to consider, in general, how the system function is affected by quantization of the co­
efficients of the difference equations. For example, the system function representation 
corresponding to both direct forms (and their corresponding transposed versions) is the 
ratio of polynomials 
M 
I>kZ-k 
H(z) 
k=O 
(6.82)
N 
1 
L:akZ-k 
k=l 
The sets of coefficients {ad and {bkl are the ideal infinite-precision coefficients in both 
direct form implementation structures (and corresponding transposed structures). Ifwe 
quantize these coefficients, we obtain the system function 
M 
L:htz-k 
H(z) = --'C.k_=o-'--__ 
(6.83)
N 
1- L:iikZ-k 
k=l 
where ak = ak + !:::.Uk and h = bk + !:::.bk are the quantized coefficients that differ from 
the original coefficients by the quantization errors !:::.ak and !:::.bk' 
Now consider how the roots of the denominator and numerator polynomials (the 
poles and zeros of H(z) are affected by the errors in the coefficients. Each polynomial 
root is affected by all of the errors in the coefficients of the polynomial since each root 
is a function of all the coefficients of the polynomial. Thus, each pole and zero will be 
affected by all of the quantization errors in the denominator and numerator polynomi­
als, respectively. More specifically, Kaiser (1966) showed that if the poles (or zeros) are 
tightly clustered, it is possible that small errors in the denominator (numerator) coeffi­
cients can cause large shifts ofthe poles (zeros) for the direct form structures. Thus, if 
the poles (zeros) are tightly clustered, corresponding to a narrow-bandpass filter or a 
narrow-bandwidth lowpass filter, then we can expect the poles of the direct form struc­
ture to be quite sensitive to quantization errors in the coefficients. Furthermore, Kaiser's 

423 
Section 6.8 
The Effects of Coefficient Quantization 
analysis showed that the larger the number of clustered poles (zeros), the greater is the 
sensitivity. 
The cascade and parallel form system functions, which are given by Eqs. (6.30) and 
(6.35), respectively, consist of combinations of 2nd-order direct form systems. However, 
in both cases, each pair of eomplex-conjugate poles is realized independently of all the 
other poles. Thus, the error in a particular pole pair is independent of its distance from 
the other poles of the system function. For the cascade form, the same argument holds 
for the zeros, since they are realized as independent 2nd -order factors. Thus, the cascade 
form is generally much less sensitive to coefficient quantization than the equivalent 
direct form realization. 
As seen in Eq. (6.35), the zeros of the parallel form system function are realized 
implicitly, through combining the quantized 2nd -order sections to obtain a common 
denominator. Thus, a particular zero is affected by quantization errors in the numerator 
and denominator coefficients of all the 2nd-order sections. However, for most practical 
filter designs, the parallel form is also found to be much less sensitive to coefficient 
quantization than the equivalent direct forms because the 2nd -order subsystems are 
not extremely sensitive to quantization. In many practical filters, the zeros are often 
widely distributed around the unit circle, or in some cases they may all be located at 
z 
±1. In the latter situation, the zeros mainly provide much higher attenuation around 
frequencies w 
0 and w = rr than is specified, and thus, movements of zeros away from 
z = ±1 do not significantly degrade the performance of the system. 
6.8.2  Example of Coefficient Quantization in an Elliptic 
Filter 
As an illustration of the effect of coefficient quantization, consider the example of an 
IIR bandpass elliptic filter designed using approximation techniques to be discussed in 
Chapter 7. The filter was designed to meet the following specifications: 
0.99:::: IH(ejW)1 :::: 1.01, 
0.3rr < Iwl :::: OArr, 
IH(ejW)1 
0.01 (i.e., - 40 dB), 
Iwl :::: 0.29rr, 
IH(ejW)1 :::: O.01(i.e., 
40dB), 
0.4l7r < Iwl :::: rr. 
That is, the filter should approximate one in the passband, 0.3rr :::: Iwl :::: OA:rr, and zero 
elsewhere in the base interval 0 :::: Iwl :::: :rr. As a concession to computational realizabil­
ity, a transition (do not care) region of O.01rr is allowed on either side of the passband. In 
Chapter 7, we will see that specifications for frequency-selective filter design algorithms 
are often represented in this form. The MATLAB function for elliptic filter design pro­
duces the coefficients of a 12th -order direct form representation of the system function 
of the form of Eq. (6.82), where the coefficients ak and bk were computed with 64-bit 
floating-point arithmetic and are shown in Table 6.1 with full 15-decimal-digit precision. 
We shall refer to this representation of the filter as "unquantized." 
The frequency response 2010glO IH(ejW)1 of the unquantized filter is shown in 
Figure 6A7(a), which shows that the filter meets the specifications in the stopbands (at 
least 40 dB attenuation). Also, the solid line in Figure 6.47(b), which is a blow-up of the 
passband region 0.3rr :::: Iwl :::: OA:rr for the unquantized filter, shows that the filter also 
meets the specifications in the passband. 

TABLE 6.1 
UNQUANTIZED DIRECT-FORM 
COEFFICIENTS FOR A 12TH-ORDER ELLIPTIC FILTER 
k 
bk  
ak 
o 
-20  
-40  
Q) -g 
.::: -60 
c:: 
01) 
E 
~ -80 
~ 
-
-100 
o  
O.l1r 
O.21T 
O.31T 
O.41T 
0.51T 
0.61T 
O.71T 
0.81T 
0.91T 
1T 
Radian frequency (w) 
! 
I 
I 
! 
I 
I -
I 
I 
I 
I 
i ! 
I 
~( 
'\/1 I 
i 
I 
I 
i
I 
I 
I 
(a) 
1.015 
1.01 
1.005 
0.995 f---~ II' 
\ 
II 
0.99 f----
I ¥ 
'11--
", 
V 
V I 
0.985 
0.381T 
0.417' 
0.4217'
O.31T 
0.321T 
0.341T 
0.361T 
Radian frequency (w) 
(b) 
o 
0.01075998066934 
1 
-0.05308642937079 
2 
0.16220359377307 
3 
-0.34568964826145 
4 
0.57751602647909 
5 
-0.77113336470234 
6 
0.85093484466974 
7 
-0.77113336470234 
8 
0.57751602647909 
9 
-0.34568964826145 
10 
0.16220359377307 
11 
-0.05308642937079 
12 
0.01075998066934 
1.00000000000000 
-5.22581881365349 
16.78472670299535 
-36.88325765883139 
62.39704677556246 
-82.65403268814103 
88.67462886449437 
-76.47294840588104 
53.41004513122380 
-29.20227549870331 
12.29074563512827 
-3.53766014466313 
0.62628586102551 
Figure 6.47 
IIR coefficient quantization example. (a) Log magnitude for unquantized elliptiC 
bandpass filter. (b) Magnitude in passband for unquantized (solid line) and 16-bit Quantized 
cascade form (dashed line). 
424 
.. 

425 
Section 6.8 
The Effects of Coefficient Quantization 
TABLE 6.2 
ZEROS AND POLES OF UNQUANTIZED 12TH-ORDER 
ELLIPTIC FILTER. 
k 
ICkI 
1 
1.0 
2 
1.0 
3 
1.0 
4 
1.0 
5 
1.0 
6 
1.0 
LCk 
Idkl 
Ldlk 
1.65799617112574 
0.65411612347125 
± 1.33272553462313 
± 0.87998582176421 
±1.28973944928129 
± 0.91475122405407 
0.92299356261936 
0.92795010695052 
0.96600955362927 
0.97053510266510 
0.99214245914242 
0.99333628602629 
± 1.15956955465354 
± 1.02603244134180 
±1.23886921536789 
±0.95722682653782 
± 1.26048962626170 
±O.93918174153968 
TIn 
(a) 
TIn 
Figure 6.48 
IIR coefficient 
quantization example. (a) Poles and 
zeros of H(Z) for unquantized 
-1--------+- Re 
coefficients. (b) Poles and zeros for 
16-bit quantization of the direct form 
(b) 
coefficients. 
Factoring the numerator and denominator polynomials corresponding to the co­
efficients in Table 6.1 in Eq. (6.82) yields a representation 
1 
H (z) = TI bon - Ckz- ) . 
(6.84) 
k=l (1­
in terms of the zeros and poles, which are given in Table 6.2. 
The poles and zeros ofthe unquantized filter that lie in the upper half ofthe z-plane 
are plotted in Figure 6.48(a). Note that the zeros are all on the unit circle, with their 
angular locations corresponding to the deep nulls in Figure 6.47. The zeros are strate­
gically placed by the filter design method on either side of the passband to provide the 
desired stopband attenuatiQn and sharp cutoff. Also note that the poles are clustered in 
the narrow passband, with two of the complex conjugate pole pairs having radii greater 
than 0.99. This finely tuned arrangement of zeros and poles is required to produce the 
narrowband sharp-cutoff bandpass filter frequency response shown in Figure 6.47(a). 
A glance at the coefficients in Table 6.1 suggests that quantization of the direct form 
may present significant problems. Recall that with a fixed quantizer, the quantization 
error size is the same, regardless of the size of the number being quantized; i.e., the 
quantization error for coefficient a12 = 0.62628586102551 can be as large as the error 
for coefficient a6 = 88.67462886449437, if we use the same number of bits and the 
same scale factor for both. For this reason, when the direct form coefficients in Table 
6.1 were quantized with 16-bit precision, each coefficient was quantized independently 
of the other coefficients so as to maximize the accuracy for each coefficient; i.e., each 
16-bit coefficient requires its own scale factor'? With this conservative approach, the 
7To simplify implementation, it would be desirable, but far less accurate, if each coefficient had the 
same scale factor. 

426 
Chapter 6 
Structures for Discrete-Time Systems 
resulting poles and zeros are as depicted in Figure 6.48(b). Note that the zeros have 
shifted noticeably, but not dramatically. In particular, the closely-spaced pair of zeros 
toward the top of the circle has remained at about the same angle, but they have moved 
off of the unit circle into a group of four complex conjugate reciprocal zeros, whereas 
the other zeros are shifted angularly but remain on the unit circle. This constrained 
movement is a result of the symmetry of the coefficients of the numerator polynomial, 
which is preserved under quantization. However, the tightly clustered poles, having no 
symmetry constraints, have moved to much different positions, and, as is easily observed, 
some of the poles have moved outside the unit circle. Therefore, the direct form system 
cannot be implemented with 16-bit coefficients because it would be unstable. 
On the other hand, the cascade form is much less sensitive to coefficient quan­
tization. The cascade form of the present example can be obtained by grouping the 
complex conjugate pairs of poles and zeros in Eq. (6.84) and Table 6.2, to form six 
2nd-order factors as in 
6 
6 
bOk(1 - CkC1)(1 - ekC1) n bOk + b1.kC1 + b2kC2 
H(z) = 
= 
. 
(6.85)
n (1 - dkC1)(1 - dk*z-l) 
1 - alkz-1 - a2kz-2 
k=1 
k=l 
The zeros q and poles dk and coefficients bik and aik of the cascade form can be com­
puted with 64-bit floating-point accuracy so these coefficients can still considered to be 
un quantized. Table 6.3 gives the coefficients of the six 2nd-order sections (as defined in 
Eq. (6.85). The pairing and ordering of the poles and zeros follows a procedure to be 
discussed in Section 6.9.3. 
TABLE 6.3 
UNQUANTIZED CASCADE-FORM 
COEFFICIENTS FOR A 12TH-ORDER ELLIPTIC FILTER 
k 
alk 
a2k 
bOk 
blk 
b2k 
1 
0.737904 
-0.851917 
0.137493 
0.023948 
0.137493 
2 
0.961757 
-0.861091 
0.281558 
-0.446881 
0.281558 
3 
0.629578 
-0.933174 
0.545323 
-0.257205 
0.545323 
4 
1.117648 
-0.941938 
0.706400 
-0.900183 
0.706400 
5 
0.605903 
-0.984347 
0.769509 
-0.426879 
0.769509 
6 
1.173028 
-0.986717 
0.937657 
-1.143918 
0.937657 
To illustrate how coefficients are quantized and represented as fixed-point num­
bers, the coefficients in Table 6.3 were quantized to 16-bit accuracy. The resulting coef­
ficients are presented in Table 6.4. The fixed-point coefficients are shown as a decimal 
integer times a power-of-2 scale factor. The binary representation would be obtained 
by converting the decimal integer to a binary number. In a fixed-point implementation, 
the scale factor would be represented only implicitly in the data shifts that would be 
necessary to line up the binary points of products prior to their addition to other prod­
ucts. Notice that binary points of the coefficients are not all in the same location. For 
example, all the coefficients with scale factor 2-15 have their binary points between the 
sign bit, bo, and the highest fractional bit, bl, as shown in Eq. (6.78). However, numbers 
whose magnitudes do not exceed 0.5, such as the coefficient b02, can be shifted left by 
one or more bit positions.8 Thus, the binary point for b02 is actually to the left of the 
8The use of different binary point locations retains greater accuracy in the coefficients, but it compli­
cates the programming or system architecture. 

427 
Section 6.8 
The Effects of Coefficient Quantization 
sign bit as if the word length is extended to 17 bits. On the other hand, numbers whose 
magnitudes exceed 1 but are less than 2, such as G16, must have their binary points 
moved one position to the right, i.e., between hI and b2 in Eq. (6.78). 
TABLE 6.4 
SIXTEEN-BIT QUANTIZED CASCADE-FORM COEFFICIENTS 
FOR A 12TH-ORDER ELLI PTIC FILTER 
k 
alk 
a2k 
bOk 
blk 
~k 
24196 x 2- 15 
-27880 x 2-15 
17805 x 2-17 
3443 x 2- 17 
17805 x 2- 17 
2 
31470 x 2-15 
-28180 x 2- 15 
18278 x 2-16 
-29131 x 2--16 
18278 x 2-16 
3 
20626 x 2-15 
-30522 x 2-15 
17556 x 2-15 
-8167 x 2--15 
17556 x 2-15 
4 
18292 x 2-14 
-30816 x 2-15 
22854 x 2-15 
-29214 x 2-15 
22854 x 
5 
19831 x 2-15 
-32234 x 
25333 x 2-15 
-13957 x 2-15 
25333 x 2-.15 
6 
19220 x 2-14 
-32315 x 2-15 
15039 x Z-14 
-18387 x 2-14 
15039 x 2-14 
The dashed line in Figure 6,47(b) shows the magnitude response in the passband 
for the quantized cascade form implementation. The frequency response is only slightly 
degraded in the passband region and negligibly in the stopband. 
To obtain other equivalent structures, the cascade form system function must be 
rearranged into a different form. For example, if a parallel form structure is determined 
(by partial fraction expansion of the unquantized system function), and the resulting 
coefficients are quantized to 16 bits as before, the frequency response in the passband 
is so close to the unquantized frequency response that the difference is not observable 
in Figure 6.47(a) and barely observahle in Figure 6,47(b). 
The example just discussed illustrates the robustness of the cascade and paral­
lel forms to the effects of coefficient quantization, and it also illustrates the extreme 
sensitivity of the direct forms for high-order filters. Because of this sensitivity, the di­
rect forms are rarely used for implementing anything other than 2nd-order systems.9 
Since the cascade and parallel forms can be configured to require the same amount of 
memory and the same or only slightly more computation as the canonic direct form, 
these modular structures are the most commonly used. More complex structures such 
as lattice structures may be more robust for very short word lengths, but they require 
significantly more computation for systems of the same order. 
6.8.3 Poles of Quantized 2 nd-Order Sections 
Even for the 2nd -order systems that are used to implement the cascade and parallel 
forms, there remains some flexibility to improve the robustness to coefficient quanti­
zation. Consider a complex-conjugate pole pair implemented using the direct form, as 
j8
in Figure 6,49. With infinite-precision coefficients, this flow graph has poles at z = re
and z 
re- j8. However, if the coefficients 2r cos e and _r2 are quantized, only a finite 
number of different pole locations is possible. The poles must lie on a grid in the z-plane 
defined by the intersection of concentric circles (corresponding to the quantization of 
r2) and vertical lines (corresponding to the quantization of 2r cos e). Such a grid is 
9An exception is in speech synthesis, where systems of 10th-order and higher are routinely imple­
mented using the direct form. luis is possible because in speech synthesis the poles of the system function 
are widely separated (see Rabiner and Schafer, 1978). 

428 
Chapter 6 
Structures for Discrete-Time Systems 
x[n] 
y[n] 
Z-1 
2r cos f) 
Z- 1 
Figure 6.49 
Direct form 
implementation of a complex-conjugate
_ r 2 
pole pair. 
illustrated in Figure 6.50(a) for 4-bit quantization (3 bits plus 1 bit for the sign); i.e., 
r2 is restricted to seven positive values and zero, whereas 2r cos eis restricted to seven 
positive values, eight negative values, and zero. Figure 6.50(b) shows a denser grid 
obtained with 7-bit quantization (6 bits plus 1 bit for the sign). The plots of Figure 6.50 
are, of course, symmetrically mirrored into each of the other quadrants of the z-plane. 
Ym 
z-plane 
1.00 
o Realizable pole positions 
'­ '­
"­ "­ "­ "­ "­
0.75 
""Unit circle 
"­
"­\ 
\ 
0.50 
0.25 
0 
0.25 
0.75 
1.00 
Re
0.50 
Cal 
Ym 
z-plane 
0.5 
0.5 
Cb) 
Figure 6.50 
Pole-locations for the 
2nd -order IIR direct form system of 
o 
1.0 
Re 
Figure 6.49. (a) Four-bit quantization of 
coefficients. (b) Seven-bit quantization. 

429 
Section 6.8 
The Effects of Coefficient Quantization 
x[n] 
Teas () 
z-l 
Tsin (} 
-T sin (} 
y[n] 
Teas () 
Figure 6.51 
Coupled form 
implementation of a complex-conjugate 
pole pair. 
Notice that for the direct fonn, the grid is rather sparse around the real axis. Thus, poles 
located around () = 0 or e = Jr may be shifted more than those around () = Jr/2. Of 
course, it is always possible that the infi,nite-precision pole location is very close to one 
of the allowed quantized poles. In this case, quantization causes no problem whatsoever, 
but in general, quantization can be expected to degrade performance. 
An alternative 2nd-order structure for realizing poles at z = rej£} and z 
re- jlJ is 
shown in Figure 6.51. This structure is referred to as the coupled form for the 2nd-order 
system (see Rader and Gold, 1967). It is easily verified that the systems of Figures 6.49 
and 6.51 have the same poles for infinite-precision coefficients. To implement the system 
of Figure 6.51, we must quantize r cos eand r sin e. Since these quantities are the real and 
imaginary parts, respectively, of the pole locations, the quantized pole locations are at 
intersections of evenly spaced horizontal and vertical lines in the z-plane. Figures 6.52( a) 
and 6.52(b) show the possible pole locations for4-bit and 7-bit quantization, respectively. 
In this case, the density of pole locations is uniform throughout the interior of the unit 
circle. Twice as many constant multipliers are required to achieve this more uniform 
density. In some situations, the extra computation might be justified to achieve more 
accurate pole location with reduced word length. 
6.8.4 Effects of Coefficient Quantization in FIR Systems 
For FIR systems, we need only be concerned with the locations of the zeros of the system 
function, since, for causal FIR systems, all the poles are at z = O. Although we have just 
seen that the direct form structure should be avoided for high-order IIR systems, it turns 
out that the direct form structure is commonly used for FIR systems. To understand why 
this is so, we express the system function for a direct form FIR system in the form 
!vi 
H (z) 
L 
h[n]z-n. 
(6.86) 
n=O 
Now, suppose that the coefficients {h[n]} are quantized, resulting in a new set of coeffi­
cients {Jl[n] 
h[n] + L'lh[n]}. The system function for the quantized system is then 
M 
H(z) = Lh[n]z-n = H(z) + L'lH(z), 
(6.87) 
n=O 

430 
Chapter 6 
Structures for Discrete-Time Systems 
Im 
z-plane 
o Realizable pole positions 
1.00 r­ --
, 
'r--, 
"­ "­ "­
Unit circle
0.75 
i 
"­
I 
\ 
0.50 
\ 
\ 
\ 
\ 
0.25 
\ 
\ 
\ 
I 
o 
0.25 
0.50 
0.75 
1.00 
Re 
(a) 
Im 
z-plane 
0.5 
0.5 
(b) 
Figure 6.52 
Pole locations for coupled 
form 2nd -order IIR system of 
o 
1.0 
R e 
Figure 6.51. (a) Four-bit quantization of 
coefficients. (b) Seven-bit quantization. 
where 
M 
!:!.H(z) = L !:!.h[n]z - n. 
(6.88) 
n=O 
Thus, the system function (and therefore, also the frequency response) of the quantized 
system is linearly related to the quantization errors in the impulse-response coefficients. 
For this reason, the quantized system can be represented as in Figure 6.53, which shows 
the unquantized system in parallel with an error system whose impulse response is 
the sequence of quantization error samples {!:!.h[n]} and whose system function is the 
corresponding z-transform, !:!.H(z). 

431 
Section 6.8 
The Effects of Coefficient Quantization 
x[n] 
Figure 6.53 
Representation of 
coefficient quantization in FI R systems. 
TABLE 6.5 
UNQUANTIZED AND QUAI\ITIZED COEFFICIENTS FOR AN OPTIMUM 
FIR LOWPASS FILTER (M = 27) 
Coefficient 
Unquantized 
16 bits 
14 bits 
13 bits 
8 bits 
h[O] = h[27] 
1.359657 x 10-3 
45 x 2- 15 
11 x 2-13 
6 x 2- 12 
Ox 2-7 
h[l] = h[26] 
-1.616993 x 10-3 
-53 x 2- 15 
-13 x 2-13 
-7x2- 12 
Ox 2-7 
h[2] = h[25] 
-7.738032 x 10-3 
-254 x 2-15 
-63 x 2-13 
-32 x 2-12 
-1 x 2-7 
h[3] = h[24] 
-2.686841 x 10-3 
-88 x 2-15 
-22 x 2-13 
-11 x 2-12 
Ox 2-7 
h[4] = h[23] 
1.255246 x 10-2 
411 x 2-15 
103 x 2-13 
51 x 2-12 
2 x 2-7 
h[5] = h[22] 
6.591530 x 10-3 
216 x 2-15 
54 x 2-13 
27 x 2- 12 
1 x 2-7 
h[6] = h[21] 
-2.217952 x 10-2 
-727 x 2-15 
-182 x 2-13 
-91 x 2- 12 
-3 x 2-7 
h[7] = h[20] 
-1.524663 x 10-2 
-500 x 2-15 
-125 x 2-13 
-62 x 2- 12 
-2 x 2-7 
h[8] = h[19] 
3.720668 x 10-2 
1219 x 2- 15 
305 x 2-13 
152 x 2- 12 
5 x 2-7 
h[9] = h[18] 
3.233332 x 10-2 
1059 x 2-15 
265 x 2-13 
132 x 2-12 
4 x 2-7 
h[lO] = h[17] 
-6.537057 x 10-2 
-2142 x 2-15 
-536 x 2-13 
-268 x 2-12 
-8 x 2-7 
h[l1] = h[16] 
-7.528754 x 10-2 
-2467 x 2-15 
-617 x 2-13 
-308 x 2-12 
-10 x 2-7 
h[12] = h[15] 
1.560970 x 10-1 
5115 x 2-15 
1279 x 2-13 
639 x 2-12 
20 x 2-7 
h[13] = h[14] 
4.394094 x 10-1 
14399 x 2- 15 
3600 x 2- 13 
1800 x 2- 12 
56 x 2-7 
Another approach to studying the sensitivity of the direct form FIR structure 
would be to examine the sensitivity of the zeros to quantization errors in the impulse­
response coefficients, which are, of course the coefficients of the polynomial H (z). If 
the zeros of H (z) are tightly clustered, then their locations will be highly sensitive to 
quantization errors in the impulse-response coefficients. The reason that the direct form 
FIR system is widely used is that, for most linear phase FIR filters, the zeros are more 
or less uniformly spread in the z-plane. We demonstrate this by the following example. 
6.8.5 Example of Quantization of an Optimum FIR Filter 
As an example of the effect of coefficient quantization in the FIR case, consider a 
linear-phase lowpass filter designed to meet the following specifications: 
0.99::: IH(ejUJ)1 ::: 1.01, 
0::: Iwl ::: O.4n, 
IH(ejUJ)1 ::: O.OOl(i.e., -60dB), 
0.6n ::: Iwl ::: n. 
This filter was designed using the Parks-McClellan design technique, which will be 
discussed in Section 7.7.3. The details of the design for this example are considered in 
Section 7.8.1. 
Table 6.5 shows the unquantized impulse-response coefficients for the system, 
along with quantized coefficients for 16-, 14-, 13-, and 8-bit quantization. Figure 6.54 

20  
0  
-20  
~ -40  
-60  
-80  
-100 
"0 
0 
Radian frequency (w) 
(a) 
0.27T 
0.47T 
0.67T 
0.87T 
7T 
0.010 
0.005 
(!.) 
"0 
..§ 
0
"a 
S 
<C 
-0.005 
-0.010 
0 
0.27T 
0.47T 
0.67T 
0.87T 
7T 
Radian frequency (w) 
(b) 
0.010 
0.005 
(!.) 
"0 
.a 
~ 
0 
<t: 
-0.005 
-0.010 
0 
0.27T 
0.47T 
0.67T 
0.87T 
7T 
Radian frequency (w) 
(c) 
Figure 6.54 
FIR Quantization example. (a) Log magnitude for unquantized case. 
(b) Approximation error for unQuantized case. (Error not defined in transition band.) 
(c) Approximation error for 16-bit quantization. 
432 

0.010 
0.005 
0 
-0.005 
-0.010 
0 
Radian frequency (w) 
(d) 
0.010 • 
0.005 
0 
-0.005 
-0.010 
0 
O.27T 
0.47T 
0.67T 
0.87T 
Radian frequency (w) 
(e) 
0.Q3 
0.02 
<:> 
-0 
0.01
E 
~ 
E 
<t: 
-0.02 
0 
0 
O.27T 
0.47T 
0.67T 
0.87T 
7T 
Radian frequency (w) 
(f) 
Figure 6.54 (continued) 
(d) Approximation error for 14-bit quantization. 
(e) Approximation error for 13-bit quantization. (f) Approximation error for 8-bit 
quantization. 
-0.01 
433 

434 
Chapter 6 
Structures for Discrete-Time Systems 
gives a comparison of the frequency responses of the various systems. Figure 6.54(a) 
shows the log magnitude in dB of the frequency response for unquantized coefficients. 
Figures 6.54(b), (c), (d), (e), and (f) show the passband and stopband approximation 
errors (errors in approximating unity in the passband and zero in the stopband) for 
the unquantized, 16-, 14-, 13-, and 8-bit quantized cases, respectively. From Figure 6.54, 
we see that the system meets the specifications for the unquantized case and both the 
16-bit and 14-bit quantized cases. However. with 13-bit quantization the stopband ap­
proximation error becomes greater than 0.001, and with 8-bit quantization the stopband 
approximation error is over 10 times as large as specified. Thus, we see that at least 14-bit 
coefficients are required for a direct form implementation of the system. However, this 
is not a serious limitation, since 16-bit or 14-bit coefficients are well matched to many 
of the technologies that might be used to implement such a filter. 
The effect of quantization of the filter coefficients on the locations of the zeros 
of the filter is shown in Figure 6.55. Note that in the unquantized case, shown in Fig­
ure 6.55(a), the zeros are spread around the z-plane, although there is some clustering 
on the unit circle. The zeros on the unit circle are primarily responsible for developing 
the stopband attenuation, whereas those at conjugate reciprocal locations off the unit 
circle are primarily responsible for forming the passband. Note that little difference 
is observed in Figure 6.55(b) for 16-bit quantization, but in Figure 6.55(c), showing 
13-bit quantization, the zeros on the unit circle have moved significantly. Finally, in Fig­
ure 6.55( d), we see that 8-bit quantization causes several of the zeros on the unit circle 
to pair up and move off the circle to conjugate reciprocal locations. This behavior of 
the zeros explains the behavior of the frequency response shown in Figure 6.54. 
A final point about this example is worth mentioning. All of the unquantized 
coefficients have magnitudes less than 0.5. Consequently, if all of the coefficients (and 
therefore, the impulse response) are doubled prior to quantization, more efficient use 
of the available bits will result, corresponding in effect to increasing B by 1. In Table 6.5 
and Figure 6.54, we did not take this potential for increased accuracy into account. 
6.8.6 Maintaining Linear Phase 
So far, we have not made any assumptions about the phase response of the FIR sys­
tem. However, the possibility of generalized linear phase is one of the major advan­
tages of an FIR system. Recall that a linear-phase FIR system has either a symmetric 
(h[M 
n] = h[n]) or an antisymmetric (h[M - n] = -h[n]) impulse response. These 
linear-phase conditions are easily preserved for the direct form quantized system. Thus, 
all the systems discussed in the example of the previous subsection have a precisely 
linear phase, regardless of the coarseness of the quantization. This can be seen in the 
way in which the conjugate reciprocal locations are preserved in Figure 6.55. 
Figure 6.55(d) suggests that, in situations where quantization is very coarse or for 
high-order systems with closely spaced zeros, it may be worthwhile to realize smaller 
sets of zeros independently with a cascade form FIR system. To maintain linear phase, 
each of the sections in the cascade must also have linear phase. Recall that the zeros 
of a linear-phase system must occur as illustrated in Figure 6.34. For example, if we use 
2nd -order sections of the form (1 
+C 2) for each complex-conjugate pair ofzeros 
on the unit circle, the zero can move only on the unit circle when the coefficient a is 

435 
Section 6.8 
The Effects of Coefficient Quantization 
FIR Lowpass Filter: 
FIR Lowpass Filter: 
Unquantized Coefficients 
16-bit Coefficien Is 
1.5 
-1.5 
-1 
o 
2 
3 
1.5 
: 
0
rt 
I 
t: 
0
co 
I 0 
;:: 
0.5 
I 
0 
.... 
0 
I 
\
.S 
0. 
O!J 
~ -O~ 
..s 
-\ 
-1.5 
Real part 
(a) 
FIR Lowpass Filter: 
FIR Lowpass Filter: 
13-bit Coefficients 
8-bit Coefficients 
~---I-o-o-r--o--
I 
0 
I 
I 0 
/ 
I 
// 
0 
-,~ 
I 
0 
I 
2 
3
-1 
o 
1 
Real part 
(b) 
1.5 
1.5 
t: 
co 
P­
0.5 
t: 
'" 
P­
0.5 
>, 
.... 
co 
I': 
'5lJ 
0 
co 
.5 
-0.5 
-0.5 
-1 
-1 
-1.5 
-1.5 
-1 
Real part 
-1 
Real part 
(c) 
(d) 
Figure 6.55 
Effect of impulse response quantization on zeros of H(i). (a) Un­
Quantized. (b) 16-bit quantization. (c) 13-bit quantization. (d) 8-bit Quantization. 
quantized. This prevents zeros from moving away from the unit circle, thereby lessening 
their attenuating effect. Similarly, real zeros inside the unit circle and at the reciprocal 
location outside the unit circle would remain real. Also, zeros at z = ±1 can be realized 
exactly by 1 st-order systems. If a pair of complex-conjugate zeros inside the unit circle is 
realized by a 2nd-order system rather than a 4th-order system, then we must ensure that, 
for each complex zero inside the unit circle, there is a conjugate reciprocal zero outside 
the unit circle. This can be done by expressing the 4th-order factor corresponding to 
zeros at z = re jO and z 
r-1e- jO as 
o 
o 
-----0: 
o 
2 
3 
O 
I 
I 
I 
~t-
36 
I 0 
v 
I 
0 
o 
o 
-~--o-~--o----
I 
0 
0(( 
I 0 
0...0 Iv-, 
I 
I 
o 
o 
o 
2 
3 
(6.89) 

436 
Chapter 6 
Structures for Discrete-Time Systems 
Z-l 
x[n] 
-2r cos 8 
r2 
1 
r2 
r2 
s8 
y[nJ 
Figure 6.56 
Subsystem to implement 4th-order factors in a linear-phase FIR 
system such that linearity of the phase is maintained independently of parameter 
quantization. 
This condition corresponds to the subsystem shown in Figure 6.56. This system uses the 
same coefficients, - 2r cos e and r2, to realize both the zeros inside the unit circle and 
the conjugate reciprocal zeros outside the unit circle. Thus, the linear-phase condition is 
preserved under quantization. Notice that the factor (1-2r cos eel +r2e 2) is identical 
to the denominator ofthe 2nd-order direct form IIR system of Figure 6.49. Therefore, the 
set of quantized zeros is as depicted in Figure 6.50. More details on cascade realizations 
of FIR systems are given by Herrmann and Schussler (1970b). 
6.9 EFFECTS OF ROUND-OFF NOISE IN DIGITAL FILTERS 
Difference equations realized with finite-precision arithmetic are nonlinear systems. 
Although it is important in general to understand how this nonlinearity affects the per­
formance of discrete-time systems, a precise analysis of arithmetic quantization effects 
is generally not required in practical applications, where we are typically concerned with 
the performance of a specific system. Indeed, just as with coefficient quantization, the 
most effective approach is often to simulate the system and measure its performance. 
For example, a common objective in quantization error analysis is to choose the digital 
word length such that the digital system is a sufficiently accurate realization of the de­
sired linear system and at the same time requires a minimum of hardware or software 
complexity. The digital word length can, of course, be changed only in steps of 1 bit, 
and as we have already seen in Section 4.8.2, the addition of 1 bit to the word length 
reduces the size ofquantization errors by a factor of 2. Thus, the choice of word length is 
insensitive to inaccuracies in the quantization error analysis; an analysis that is correct 
to within 30 to 40 percent is often adequate. For this reason, many of the important 
effects of quantization can be studied using linear additive noise approximations. We 
develop such approximations in this section and illustrate their use with several ex­
amples. An exception is the phenomenon of zero-input limit cycles, which are strictly 
nonlinear phenomena. We restrict our study of nonlinear models for digital filters to a 
brief introduction to zero-input limit cycles in Section 6.10. 
6.9.1 Analysis of the Direct Form IIR Structures 
To introduce the basic ideas, let us consider the direct form structure for an LTI discrete­
time system. The flow graph of a direct form I 2nd -order system is shown in 

437 
Section 6.9 
Effects of Round-off Noise in Digital Filters 
x[nJ 
yln] 
_-1 
< 
al
bl 
,,-I 
Z-l 
a2
b2 
(a) 
x[nJ 
Q 
Hn] 
(b) 
bo 
x[n] 
Hn] 
eo[n] 
Z-1
Z-1 
al
b1 
e3[n]
etln] 
a2
b2 
Figure 6.57 Models for direct form I 
system. (a) Infinite-precision model.
e2[n] 
e4[n] 
(b) Nonlinear quantized model. 
(c) 
(c) Linear-noise model. 
Figure 6.57(a). The general Nth·order difference equation for the direct form I 
structure is 
N 
M 
y[n] 
I>ky[n - k] + I>kx[n - k], 
(6.90) 
k=l 
k=O 

438  
Chapter 6 
Structures for Discrete-Time Systems 
and the system function is 
MLbkZ-k 
8(z)
H(z) = _k_=O-N---
(6.91)
A(z) 
1- LakZ-k 
k=l 
Let us assume that all signal values and coefficients are represented by (8 + I)-bit 
fixed-point binary numbers. Then, in implementing Eq. (6.90) with a (8 + 1)-bit adder, 
it would be necessary to reduce the length of the (2B + I)-bit products resulting from 
multiplying two (B + 1 )-bit numbers back to (B + 1) bits. Since all numbers are treated as 
fractions, we would discard the least significant 8 bits by either rounding or truncation. 
This is represented by replacing each constant multiplier branch in Figure 6.57(a) by a 
constant multiplier followed by a quantizer, as in the nonlinear model of Figure 6.57(b). 
The difference equation corresponding to Figure 6.57(b) is the nonlinear equation 
N  
M 
)i[n) = L Q[ad)[n - k]] + L Q[bkx[n 
k]]. 
(6.92) 
k=l  
k=O 
Figure 6.57(c) shows an alternative representation in which the quantizers are 
replaced by noise sources that are equal to the quantization error at the output of each 
quantizer. For example, rounding or truncation of a product bx[n] is represented by a 
noise source of the form 
ern] 
Q[bx[n)) - bx[n].  
(6.93) 
If the noise sources are known exactly, then Figure 6.57(c) is exactly equivalent to 
Figure 6.57(b). However, Figure 6.57(c) is most useful when we assume that each quan­
tization noise source has the following properties: 
1.  Each quantization noise source e[n] is a wide-sense-stationary white-noise process. 
2.  Each quantization noise source has a uniform distribution of amplitudes over one 
quantization interval. 
3.  Each quantization noise source is uncorrelated with the input to the corresponding 
quantizer, all other quantization noise sources, and the input to the system. 
These assumptions are identical to those made in the analysis of AID conversion 
in Section 4.8. Strictly speaking, our assumptions here cannot be valid, since the quanti­
zation error depends directly on the input to the quantizer. This is readily apparent for 
constant and sinusoidal signals. However, experimental and theoretical analyses have 
shown (see Bennett, 1948; Widrow, 1956, 1961; Widrow and Kollar, 2008) that in many 
situations the approximation just described leads to accurate predictions of measured 
statistical averages such as the mean, variance, and correlation function. This is true 
when the input signal is a complicated wide band signal such as speech, in which the 
signal fluctuates rapidly among all the quantization levels and traverses many of those 
levels in going from sample to sample (see Gold and Rader, 1969). The simple linear­
noise approximation presented here allows us to characterize the noise generated in the 
system by averages such as the mean and variance and to determine how these averages 
are modified by the system. 

439 
Section 6.9 
Effects of Round-off Noise in Digital Filters 
.l 
2 
8. 
e 
2 
(a) 
....-__-1 1  
Ll  
Figure 6.58 
Probability density 
function for Quantization errors. 
e 
(b) 
(a) Rounding. (b) Truncation. 
For (B + I)-bit quantization, we showed in Section 6.7.1 that, for rounding, 
1 
. 
1
22-B < e[n] :::: 22-B, 
(6.94a) 
and for two's-complement truncation, 
< ern] :::: O. 
(6.94b) 
Thus, according to our second assumption, the probability density functions for the 
random variables representing quantization error are the uniform densities shown in 
Figure 6.58(a) for rounding and in Figure 6.58(b) for truncation. The mean and variance 
for rounding are, respectively, 
me = 0, 
(6.95a) 
2-2B 
a 2 =__ 
(6.95b)
e 
12' 
For two's-complement truncation, the mean and variance are 
2- B 
(6.96a)
me=-T' 
(6.96b) 
In general, the autocorrelation sequence of a quantization noise source is, according to 
the first assumption, 
(6.97)  
In the case of rounding, which we will assume for convenience henceforth, me 
0, so the 
autocorrelation function is cPee[n] = a; o[n], and the power spectrum is <pee(ejUJ ) = a; 
for Iwl :::: 17:. In this case, the variance and the average power are identical. In the 

440 
Chapter 6 
Structures for Discrete-Time Systems 
ern] 
bo 
x[n] 
Y[n] = y[n] +f[n 
Z-l 
Z-I 
bl 
aj 
Z-l 
Z-I 
~2 
a2 
Figure 6.59 
Linear-noise model for direct form I with noise sources combined. 
case of truncation, the mean is not zero, so average-power results derived for rounding 
must be corrected by computing the mean of the signal and adding its square to the 
average-power results for rounding. 
With this model for each of the noise sources in Figure 6.57( c), we can now proceed 
to determine the effect of the quantization noise on the output of the system. To aid 
us in doing this, it is helpful to observe that all of the noise sources in that figure are 
effectively injected between the part of the system that implements the zeros and the 
part that implements the poles. Thus, Figure 6.59 is equivalent to Figure 6.57(c) if ern] 
in Figure 6.59 is 
ern] = earn] + el[n] + e2[n] + e3[n] + e4[n]. 
(6.98) 
Since we assume that all the noise sources are independent of the input and independent 
of each other, the variance of the combined noise sources for the 2nd-order direct form 
I case is 
2222225 
ae == aeo +ae1 +ae2 +ae3 +ae4 = 
2-2B 
'12' 
(6.99) 
and for the general direct form I case, it is 
(6.100)
a; = (M + 1 + 
To obtain an expression for the output noise, we note from Figure 6.59 that the 
system has two inputs, x[n] and ern], and since the system is now assumed to be linear, 
the output can be represented as Hn] 
y[n] + f[nL where y[n] is the response of the 
ideal un quantized system to the input sequence x[n] and f[n] is the response of the 
system to the input ern]. The output y[n] is given by the difference equation (6.90), but 
since ern] is injected after the zeros and before the poles, the output noise satisfies the 
difference equation 
N 
f[n] = I:>kf[n - k] + ern]; 
(6.101) 
k=l 
i.e., the properties of the output noise in the direct form I implementation depend only 
on the poles of the system. 
To determine the mean and variance of the output noise sequence, we can use 
some results from Section 2.10. Consider a linear system with system function H ef(Z) 

441 
Section 6.9 
Effects of Round-off Noise in Digital Filters 
with a white-noise input e[n] and corresponding output f[n). Then, from Eqs. (2.184) 
and (2.185), the mean of the output is 
(6.102)  
Since me = 0 for rounding, the mean of the output will be zero, so we need not be 
concerned with the mean value of the noise if we assume rounding. From Eqs. (6.97) and 
(2.190), it follows that, because, for rounding, e[n] is a zero-mean white-noise sequence, 
the power density spectrum of the output noise is simply 
Pff(w) 
<'Pff(ejW ) = a;IHe/(e jW )12• 
(6.103) 
From Eq. (2.192), the variance of the output noise can be shown to be 
1 iJr 
1 iJr
a} = -2 
Pff(w)dw = a;-2 
IHej(e jW)1 2dw. 
(6.104) 
1! 
-Jr 
1! 
-Jr 
Using Parseval's theorem in the form of Eq. (2.162), we can also express a} as 
2
aJ = a; L
00 
Ihej[nJI
. 
(6.105) 
n=-oo 
When the system function corresponding to hej [n] is a rational function, as it will always 
be for difference equations of the type considered in this chapter, we can use Eq. (A,66) 
in Appendix A for evaluating infinite sums of squares of the form of Eq. (6.105). 
We will use the results summarized in Eqs. (6.102)-(6.105) often in our analysis 
of quantization noise in linear systems. For example, for the direct form I system of 
Figure 6.59, Hej(z) = 1IA(z); i.e., the system function from the point where all the 
noise sources are injected to the output consists only of the poles of the system function 
H(z) in Eq. (6.91). Thus, we conclude that, in general, the total output variance owing 
to internal round-off or truncation is 
(6.106)  
where hej[n) is the impulse response corresponding to H ej (z) = 11A(z). The use of the 
preceding results is illustrated by the following examples. 
Example 6.11 
Round-off Noise in a 1st-Order System 
Suppose that we wish to implement a stable system having the system function 
b 
H(z) = 1-
lal < 1. 
(6.107) 
Figure 6.60 shows the flow graph of the linear-noise model for the implementation 
in which products are quantized before addition. Each noise source is filtered by the 
system from ern] to the output, for which the impulse response is hej[n] 
anu[nJ. 

442 
Chapter 6 
Structures for Discrete-Time Systems 
Since M 
0 and N = 1 for this example, from Eq. (6.103), the power spectrum of the 
output noise is 
2­ 28 (
P (w)=2-­
II 
12 
1 +a2 
1 
) • 
2acosw 
(6.108) 
and the total noise variance at the output is 
00 
Lla l
2n 
(6.109)
1Tl = 
n=O 
From Eq. (6.109), we see that the output noise variance increases as the pole at 
z = a approaches the unit circle. Thus, to maintain the noise variance below a specified 
level as la 1approaches unity, we must use longer word lengths. The following example 
also illustrates this point. 
ern) 
earn) + eb[n) 
b 
x[nJ 
Yrn) = y[n) + fIn) 
Z--1 
a 
Figure 6.60 1s'-order linear noise model. 
Example 6.12 
Round-off Noise in a 2 nd-order System 
Consider a stable 2nd -order direct form I system with system function 
H(z) 
(6.110) 
The linear-noise model for this system is shown in Figure 6.57( c), or equivalently, 
Figure 6.59, with al = 2r cos f) and a2 = _r2. In this case, the total output noise power 
can be expressed in the form 
2-28 1 1]1: 
dw 
(6.111)
= 512 2:n: -]1: 1(1 
rejB e-jw)(1 - re-jB e-jW)1 2 '  
Using Eq. (A.66) in Appendix A, the output noise power is found to be  
28  
2 
2-
(1 + r2) 
1 
(6.112)
1Tf = 512 1 _ r2 
+ 1 _ 2rL cos 2f) 
As in Example 6.11, we see that as the complex conjugate poles approach the unit 
circle (r ...... 1), the total output noise variance increases, thus requiring longer word 
lengths to maintain the variance below a prescribed level. 
The techniques of analysis developed so far for the direct form I structure can also 
be applied to the direct form II structure. The nonlinear difference equations for the 

443 
Sectiorl6.9 
Effects of Rourld-off Noise in Digital Filters 
direct form II structure are of the form 
N 
w[n] = L Q[akw[n 
k]] + x[n], 
(6.113a) 
k=l 
M 
Yln] = L Q[bkw[n - k]]. 
(6.113b) 
k=O 
Figure 6.61(a) shows the linear-noise model for a 2nd -order direct form II system. A 
noise source has been introduced after each multiplication, indicating that the products 
are quantized to (B + 1) bits before addition. Figure 6.61(b) shows an equivalent linear 
model, wherein we have moved the noise sources resulting from implementation of the 
poles and combined them into a single noise source earn] = e3[n] + e4[n] at the input. 
Likewise, the noise sources due to implementation of the zeros are combined into the 
single noise source eb[n] 
eo[n]+el[n] +e2[n] that is added directly to the output. From 
this equivalent model, it follows that for M zeros and N poles and rounding (me = 0), 
the power spectrum of the output noise is 
2~2B 
PU(w) = N 12 1H(ej(tl) 12 + (M + 
(6.114) 
and the output noise variance is 
2-28
1 fn 
a}=N 12 2n _rrIH(ej(tl)12dw+(M+l)12 
(6.115)
2-2B 
00 
2-2B 
= NuL Ih[n1l2 + (M + 1) 12 
n=-oo 
That is, the white noise produced in implementing the poles is filtered by the entire 
system, whereas the white noise produced in implementing the zeros is added directly 
to the output of the system. In writing Eq. (6.115), we have assumed that the N noise 
sources at the input are independent, so that their sum has N times the variance of a 
single quantization noise source. The same assumption was made about the (M + 1) 
noise sources at the output. These results are easily modified for two's-complement 
truncation. Recall from Eqs. (6.95a )-(6.95b) and Eqs. (6.96a)-( 6.96b) that the variance 
of a truncation noise source is the same as that ofa rounding noise source, but the mean 
of a truncation noise source is not zero. Consequently, the formulas in Eqs. (6.106) and 
(6.115) for the total output noise variance also hold for truncation. However, the output 
noise will have a nonzero average value that can be computed using Eq. (6.102). 
A comparison ofEq. (6.106) with Eq. (6.115) shows that the direct form I and direct 
form II structures are affected differently by the quantization of products in implement­
ing the corresponding difference equations. In general, other equivalent structures such 
as cascade, parallel, and transposed forms will have a total output noise variance differ­
ent from that of either of the direct form structures. However, even though Eqs. (6.106) 
and (6.115) are different, we cannot say which system will have the smaller output noise 
variance unless we know specific values for the coefficients ofthe system. In other words, 
it is not possible to state that a particular structural form will always produce the least 
output noise. 

444 
Chapter 6 
Structures for Discrete-Time Systems 
bo 
y[n]
xrn] 
f 
eorn]
Z-l 
al 
bi 
f
1 
eI[n]
e3[n] 
Z-I 
a2 
b2 
e4[n] 
e2[n) 
(a) 
ea[n] 
eb [n] 
bo 
x[n] 
al 
az 
rz-I 
Z-I 
bi 
~2 
Yrnl 
Figure 6.61 
Linear-noise models for 
direct form II. (a) Showing quantization 
of individual products. (b) With noise 
(b) 
sources combined. 
It is possible to improve the noise performance of the direct form systems (and 
therefore cascade and parallel forms as well) by using a (2B +1 )-bit adder to accumulate 
the sum of products required in both direct form systems. For example, for the direct 
form I implementation, we could use a difference equation of the form 
N 
M 
]
yin] 
Q {; ad)[n 
k] + Ebkx[n 
k] ; 
(6.116)
[ 
i.e., the sums of products are accumulated with (2B + 1)- or (2B + 2)-bit accuracy, and 
the result is quantized to (B +1) bits for output and storage in the delay memory. In the 
direct form I case, this means that the quantization noise is still filtered by the poles, but 
the factor (M +1+N) in Eq. (6.106) is replaced by unity. Similarly, for the direct form II 
realization, the difference equations (6.113a)-(6.113b) can respectively be replaced by 
wIn] = Q [t.. akw[n - k] + Xln]] 
(6.117a) 
k=l 
and 
.Y[nJ = Q [tbkW[n - k]] . 
(6.117b) 
k=O 

445 
Section 6.9 
Effects of Round-off Noise in Digital Filters 
This implies a single noise source at both the input and output, so the factors Nand 
(M + 1) in Eq. (6.115) are each replaced by unity. Thus, the double-length accumulator 
provided in most DSP chips can be used to significantly reduce quantization noise in 
direct form systems. 
6.9.2  Scaling in Fixed-Point Implementations of IIR 
Systems 
The possibility of overflow is another important consideration in the implementation 
of IIR systems using fixed-point arithmetic. If we follow the convention that each fixed­
point number represents a fraction (possibly times a known scale factor), each node in 
the structure must be constrained to have a magnitude less than unity to avoid overflow. 
If wkln] denotes the value of the kth node variable, and hk[n] denotes the impulse 
response from the input x[n] to the node variable wk[n], then 
IWk[n]1 
m]hk[m]l·  
(6.118)
= Im~oo x[n 
The bound 
Iwklnll ::: Xmax L
00 
Ihk[m]1  
(6.119) 
m=-oo 
is obtained by replacing x[n - m1by its maximum value Xmax and using the fact that the 
magnitude of a sum is less than or equal to the sum of the magnitudes of the summands. 
Therefore, a sufficient condition for Iwk[n] I < 1 is 
1 
Xmax <  -00----
(6.120) 
Ihklmll
L 
m=-oo 
for all nodes in the flow graph. If Xmax does not satisfy Eq. (6.120), then we can multiply 
x [n] by a scaling multiplier s at the input to the system so that SXmax satisfies Eq. (6.120) 
for all nodes in the flow graph; i.e., 
1 
SXmax <  ----,=------=-
(6.121) 
m:x [m~oo Ihk[m]I]' 
Scaling the input in this way guarantees that overflow never occurs at any of the nodes in 
the flow graph. Equation (6.120) is necessary as well as sufficient, since an input always 
exists such that Eq. (6.119) is satisfied with equality. (See Eq. (2.70) in the discussion 
of stability in Section 2.4.) However, Eq. (6.120) leads to a very conservative scaling of 
the input for most signals. 
Another approach to scaling is to assume that the input is a narrowband signal, 
modeled as x[n] 
Xmax cos won. In this case, the node variables will have the form 
wklnl = IHk(ejwO)lxmax cos(won + LHk(eiwo )). 
(6.122) 
Therefore, overflow is avoided for all sinusoidal signals if 
max IHk(ejW)lxmax < 1  
(6.123)
k,lwl.:o:1l' 

Chapter 6 
Structures for Discrete-Time Systems
446 
or if the input is scaled by the scale factor s such that 
1 
SXmax < 
(6.124) 
max 
k, IcvL:::rr 
Still another scaling approach is based on the energy E 
En lx[n]12 of the input 
signal. We can derive the scale factor in this case by applying the Schwarz inequality 
(see Bartle, 2(00) to obtain the following inequality relating the square of the node 
signal to the energies of the input signal and the node impulse response: 
2 
1 frr 
, 
" 
12
IWk[n]1 = -
Hk(eJW)X (eJW)eJ(vndw 
12n 
~rr  
jW 
(6.125)
~ (2~ r: IHk(ejW)12dW) (2~ r: IX(e
) 12dW). 
Therefore, if we scale the input sequence values by s and apply Parseval's theorem, we 
see that IWk[n]12 < 1 for all nodes k if 
1 
(6.126)
( 
oc 
2) 
2E < 
[00 
2] . 
" 
"~OO Ix[n11 
~, 
"'i'X ,~oo IMn1! 
Since it can be shown that for the kth node, 
00 
2 }1/2 
00 
{ n~oo Ihk[n]1 
~ m~x IHk(ejW)1 ~ n~oo Ihk[n]1. 
(6.127) 
it follows that (for most input signals) Eqs. (6.121), (6.124), and (6.126) give three de­
creasingly conservative ways of scaling the input to a digital filter (equivalently decreas­
ing the gain of the filter). Of the three, Eq. (6.126) is generally the easiest to evaluate 
analytically because the partial fraction method of Appendix A can be used; however 
use of Eq. (6.126) requires an assumption about the mean-squared value of the signal, 
E. On the other hand, Eq. (6.121) is difficult to evaluate analytically, except for the sim­
plest systems. Of course, if the filter coefficients are fixed numbers, the scale factors can 
be estimated by computing the impUlse response or frequency response numerically. 
If the input must be scaled down (s < 1), the signal-to-noise ratio (SNR) at the 
output of the system will be reduced because the signal power is reduced, but the noise 
power is dependent only on the rounding operation. Figure 6.62 shows 2nd-order direct 
form I and direct form II systems with scaling multipliers at the input. In determining 
the scaling multiplier for these systems, it is not necessary to examine each node in 
the flow graph. Some nodes do not represent addition and thus cannot overflow. Other 
nodes represent partial sums. If we use nonsaturation two's-complement arithmetic, 
such nodes are permitted to overflow if certain key nodes do not. For example, in 
Figure 6.62( a), we can focus on the node enclosed by the dashed circle. In the figure, the 
scaling multiplier is shown combined with the bkS, so that the noise source is the same 

447 
Section 6.9 
Effects of Round-off Noise in Digital Filters 
e'[n) 
"­
sbo 
; " 
\ 
\ 
f- ) 
Y[n)
x [n) 
"­
Z-1 
Z-1 
sb1 
a1 
Z-1 
Z-1 
sbz 
az 
(a) 
"­
"-
w'[n] bo ;"
s ; " 
\ 
\ 
-
,./
x [n) 
\ 
\ "- _/ 
I 
Y[n) 
~ Z-1 
"­
al 
b1 
k1 
az 
b2 
Figure 6.62 
Scaling of direct form 
systems. (a) Direct form I. (b) Direct 
(b) 
form II. 
as in Figure 6.59; i.e., it has five times the power of a single quantization noise source.lO 
Since the noise source is again filtered only by the poles, the output noise power is the 
same in Figures 6.59 and 6.62(a). However, the overall system function of the system 
in Figure 6.62(a) is sH(z) instead of H(z), so the unquantized component of the output 
Y[n] is sy[n] instead of y[n]. Since the noise is injected after the scaling, the ratio of 
signal power to noise power in the scaled system is s2 times the SNR for Figure 6.59. 
Because s < 1 if scaling is required to avoid overflow, the SNR is reduced by scaling. 
The same is true for the direct form II system of Figure 6.62(b). In this case, we 
must determine the scaling multiplier to avoid overflow at both of the circled nodes. 
Again, the overall gain of the system is s times the gain of the system in Figure 6.61(b), 
but it may be necessary to implement the scaling multiplier explicitly in this case to 
avoid overflow at the node on the left. This scaling multiplier adds an additional noise 
component to ea[n], so the noise power at the input is, in general, (N + 1)2-28 /12. 
Otherwise, the noise sources are filtered by the system in exactly the same way in both 
Figure 6.61 (b) and Figure 6.62(b). Therefore, the signal power is multiplied by s2, and 
the noise power at the output is again given by Eq. (6.115), with N replaced by (N + 1). 
The SNR is again reduced if scaling is required to avoid overflow. 
lOThis eliminates a separate scaling multiplication and quantization noise source. However, scaling 
(and quantizing) the bkS can change the frequency response of the system. Ifa separate input scaling multiplier 
precedes the implementation ofthe zeros in Figure 6.62(a), then an additional quantization noise source would 
contribute to the output noise after going through the entire system H(z). 

Chapter 6 
Structures for Discrete-Time Systems
448 
Example 6.13 Interaction Between Scaling and Round-off 
Noise 
To illustrate the interaction of scaling and round-off noise, consider the system of 
Example 6.11 with system function given by Eq. (6.107). If the scaling multiplier is 
combined with the coefficient b, we obtain the flow graph of Figure 6.63 for the scaled 
system. Suppose that the input is white noise with amplitudes uniformly distributed 
between -1 and +1. Then the total signal variance is a; = 1/3. To guarantee no 
overflow in computing y[n], we use Eq. (6.121) to compute the scale factor 
1 
l-Ial 
s 
(6.128)
00 
= -Ib-I-
Llbllaln 
n=O 
The output noise variance was determined in Example 6.11 to be 
2-2B 
1 
af2 2----
(6.129) 
a2
12 1 ­
and since we again have two (B + I)-bit rounding operations, the noise power at the 
output is the same, i.e., 
aJ. The variance of the output y'[n] due to the scaled 
input sx[n] is 
2 
a;1 
= s2a;. 
(6.130)
= G) /2b 
Therefore, the SNR at the output is 
.. 
2 
2 
2 y
s2ay = (~) a
. 
(6.131)
2 
2
a 
Ihl 
a
f 
f 
As the pole of the system approaches the unit circle, the SNR decreases because the 
quantization noise is amplified by the system and because the high gain of the system 
forces the input to be scaled down to avoid overflow. Again, we see that overflow and 
quantization noise work in opposition to decrease the performance of the system. 
e'[n] 
sb 
Yen] 
sy[n] + f'[n]
x[nl 
Z-l 
a 
Figure 6.63 
Scaled 1sl-order system. 
6.9.3 Example of Analysis of a Cascade IIR Structure 
The previous results of this section can be applied directly to the analysis ofeither paral­
lel or cascade structures composed of2nd-order direct form subsystems. The interaction 
of scaling and quantization is particularly interesting in the cascade form. Our general 

449 
Section 6.9 
Effects of Round-off Noise in Digital Filters 
comments on cascade systems will be interwoven with a specific example. 
An elliptic lowpass filter was designed to meet the following specifications: 
0.99::: IH(ejclJ)1 .:::s 1.01, 
Iwl ::: 0.5Jr, 
IH(ejW)1 ::: 0.01, 
0.56Jr < Iwl ::: Jr. 
The system function of the resulting system is 
3 ( 
1 + b 
-1 + -2 ) 
3 
H(z) = 0.079459 n 
1k~1 
z -2 
= 0.079459 nHk(Z), 
(6.132) 
k=1 
1 - alkZ 
-
a2kZ 
k=l 
where the coefficients are given in Table 6.6. Notice that all the zeros of H (z) are on the 
unit circle in this example; however, that need not be the case in general. 
Figure 6.64( a) shows a flow graph of a possible implementation of this system as a 
cascade of 2nd-order transposed direct form II subsystems. The gain constant, 0.079459, 
is such that the overall gain of the system is approximately unity in the passband, and it 
is assumed that this guarantees no overflow at the output of the system. Figure 6.64(a) 
shows the gain constant placed at the input to the system. This approach reduces the 
amplitude of the signal immediately, with the result that the subsequent filter sections 
must have high gain to produce an overall gain of unity. Since the quantization noise 
sources are introduced after the gain of 0.079459 but are likewise amplified by the rest 
of the system, this is not a good approach. Ideally, the overall gain constant, being less 
than unity, should be placed at the very end of the cascade, so that the signal and noise 
will be attenuated by the same amount. However, this creates the possibility ofoverflow 
along the cascade. Therefore, a better approach is to distribute the gain among the three 
stages of the system, so that overflow is just avoided at each stage of the cascade. This 
distribution is represented by 
(6.133) 
where S1S2S3 
0.079459. The scaling multipliers can be incorporated into the coeffi­
cients ofthe numerators of the individual system functions H£ (z) 
Sk H k(Z), as in 
3(bl +b' -l+b' Z-2) 3 
H(z) =n 
Ok 
HZ 1 
2k 
2 
nH~(z), 
(6.134)
1 -a1kZ-
-a2kZ­
k=l 
k=l 
where bOk 
Sk and bik = Skblk. The resulting scaled system is depicted in
bu 
Figure 6.64(b ). 
Also shown in Figure 6.64(b) are quantization noise sources representing the quan­
tization ofthe products before addition. Figure 6.64( c) shows an equivalent noise model, 
TABLE 6.6 
COEFFICIENTS FOR 
ELLIPTIC LOWPASS FILTER IN 
CASCADE FORM 
k 
1 
0.478882 
-0.172150 
1.719454 
2 
0.137787 
-0.610077 
0.781109 
3 
-0.054779 
-0.902374 
0.411452 

450 
Chapter 6 
Structures for Discrete-Time Systems 
0.079459 
t[nl 
bll 
kl 
al1 
b l2 
z-l 
a12 
-
Z-I 
a21 
. 
Z-1 
a22 
(a) 
x[n] 
bill 
bil 
wJlnl 
all 
biz 
wz[n] 
eoz 
Z~l 
al2 
bh 
ell 
e31 
Z-1 
aZI 
b22 
el2 
en 
e21 
e41 
e22 
e42 
(b) 
t[nl 
bin 
bi) 
eJ[n] 
wHn] 
Z-l 
all 
b02 
biz 
e2[n] 
wHn] 
Z-I 
a12 
bil 
Z-l 
a21 
bi2 
,;::-1 
a22 
(c) 
b13 
'Z~I 
a13 
y[n] 
. 
,;::-1 
w3ln] 
Hnl 
a13 
~  
23 
aZ3 
,
~ 
e23 
e43 
e3[n] 
bCJ3 
bb 
w:i[n] 
c 1 
au 
J'[n 
bi3 
C 1 
a23 
Figure 6.64 Models for 6th-order cascade system with transposed direct form II subsys­
tems. (a) Infinite-precision model. (b) Linear-noise model for scaled system, showing Quan­
tization of individual multiplications. (c) Linear-noise model with noise sources combined. 
for which it is recognized that all the noise sources in a particular section are filtered 
only by the poles of that section (and the subsequent subsystems). Figure 6.64(c) also 
uses the fact that delayed white-noise sources are still white noise and are independent 
of all the other noise sources, so that all five sources in a subsection can be combined 
into a single noise source having five times the variance of a single quantization noise 
source.!1 Since the noise sources are assumed independent, the variance of the output 
llThis discussion can be generalized to show that the transposed direct form II has the same noise 
behavior as the direct form I system. 

451 
Section 6.9 
Effects of Round-off Noise in Digital Filters 
noise is the sum of the variances owing to the three noise sources in Figure 6.64(c). 
Therefore, for rounding, the power spectrum of the output noise is 
s2IH2(eJW)12s2IH3(eJW)12 
s2IH3(eJW)12 
1. ]
p 
(w) 
2 
3 
+ 3 
+ __.,---;::-
(6.135)
1'1' 
= 
12 
[ 
IA 1(ejW )12 
IA 2(ejW )12 
IA3(ejW)12' 
(6.136) 
1 I
Jr 
+ -
1. 
2 dw] .
2n 
-Jr IA 3(eJW )1 
If a double-length accumulator is available. it would be necessary to quantize only 
the sums that are the inputs to the delay elements in Figure 6.64(b). In this case the 
factor of 5 in Eqs. (6.135) and (6.136) would be changed to 3. Furthermore, if a double­
length register were used to implement the delay elements, only the variables UJk[n] 
would have to be quantized, and there would be only one quantization noise source per 
subsystem. In that case, the factor of 5 in Eqs. (6.135) and (6.136) would be changed to 
unity. 
The scale factors Sk are chosen to avoid overflow atpoints along the cascade system. 
We will use the scaling convention of Eq. (6.124). Therefore, the scaling constants are 
chosen to satisfy 
Sl max IH1(ejW)1 < 1, 
(6.l37a)
Iwl:SJr 
SlS2 max IH1(ejW)H2(ejW)1 < 1, 
(6.137b)
Iwl:SJr 
(6.137c) 
The last condition ensures that there will be no overflow at the output of the system 
for unit-amplitude sinusoidal inputs, because the maximum overall gain of the filter is 
unity. For the coefficients of Table 6.6, the resulting scale factors are Sl 
0.186447, 
S2 = 0.529236, and S3 = 0.805267. 
Equations (6.135) and (6.136) show that the shape of the output noise power spec­
trum and the total output noise variance depends on the way that zeros and poles are 
paired to form the 2nd -order sections and on the order of the 2nd-order sections in the 
cascade form realization. Indeed, it is easily seen that, for N sections, there are (N!) 
ways to pair the poles and zeros, and there are likewise (N!) ways to order the result­
ing 2nd-order sections, a total of (N!)2 different systems. In addition, we can choose 
either direct form I or direct form II (or their transposes) for the implementation of 
the 2nd-order sections. In our example, this implies that there are 144 different cascade 
systems to consider, if we wish to find the system with the lowest output noise variance. 
For five cascaded sections, there would be 57,600 different systems. Clearly, the complete 
analysis of even low-order systems is a tedious task, since an expression like Eq. (6.136) 
must be evaluated for each pairing and ordering. Hwang (1974) used dynamic pro­
gramming and Liu and Peled (1975) used a heuristic approach to reduce the amount of 
computation. 

------
452 
Chapter 6 
Structures for Discrete-Time Systems 
Im 
z-plane 
Unit circle 
"­
---
X 
--_/
I'I 
1 Re 
X 
X 
X 
Figure 6.65 
Pole-zero plot for 
6th -order system of Figure 6.64, 
showing pairing of poles and zeros. 
Even though finding the best pairing and ordering may require computer optimiza­
tion, Jackson (1970a, 1970b, 1996) found that good results are almost always obtained 
by applying simple rules of the following form: 
1. The pole that is closest to the unit circle should be paired with the zero that is 
closest to it in the z-plane. 
2. Rule 1 should be repeatedly applied until all the poles and zeros have been paired. 
3. The resulting 2nd -order sections should be ordered according to either increasing 
closeness to the unit circle or decreasing closeness to the unit circle. 
The pairing rules are based on the observation that subsystems with high peak 
gain are undesirable because they can cause overflow and because they can amplify 
quantization noise. Pairing a pole that is close to the unit circle with an adjacent zero 
tends to reduce the peak gain of that section. These heuristic rules are implemented in 
design and analysis tools such as the MATLAB function zp2sos. 
One motivation for rule 3 is suggested by Eq. (6.135). We see that the frequency re­
sponses of some of the subsystems appear more than once in the equation for the power 
spectrum of the output noise. If we do not want the output noise variance spectrum to 
have a high peak around a pole that is close to the unit circle, then it is advantageous 
to have the frequency-response component owing to that pole not appear frequently in 
Eq. (6.135). This suggests moving such "high Q" poles to the beginning of the cascade. 
On the other hand, the frequency response from the input to a particular node in the 
flow graph will involve a product of the frequency responses of the subsystems that pre­
cede the node. Thus, to avoid excessive reduction of the signal level in the early stages 
of the cascade, we should place the poles that are close to the unit circle last in order. 

453 
Section 6.9 
Effects of Round-off Noise in Digital Filters 
Clearly then, the question of ordering hinges on a variety of considerations, including 
total output noise variance and the shape of the output noise spectrum. Jackson (1970a, 
1970b) used L p norms to quantify the analysis of the pairing-and-ordering problem and 
gave a much more detailed set of "rules of thumb" for obtaining good results without 
having to evaluate all possibilities. 
The pole-zero plot for the system in our example is shown in Figure 6.65. The 
paired poles and zeros are circled. In this case, we have chosen to order the sections 
from least peaked to most peaked frequency response. Figure 6.66 illustrates how the 
frequency responses of the individual sections combine to form the overall frequency 
response. Figures 6.66(a)-(c) show the frequency responses of the individual unscaled 
subsystems. Figures 6.66( d)-(f) show how the overall frequency response is built up. 
Notice that Figures 6.66(d)-(f) demonstrate that the scaling Eqs. (6.137a)-(6.137c) en­
sure that the maximum gain from the input to the output of any subsystem is less than 
unity. The solid curve in Figure 6.67 shows the power spectrum of the output noise for 
the ordering 123 (least peaked to most peaked). We assume that B + 1 
16 for the 
plot. Note that the spectrum peaks in the vicinity of the pole that is closest to the unit 
circle. The dotted curve shows the power spectrum of the output noise when the section 
order is reversed (i.e., 321). Since section 1 has high gain at low frequencies, the noise 
spectrum is appreciably larger at low frequencies and slightly lower around the peak. 
The high Q pole still filters the noise sources of the first section in the cascade, so it still 
tends to dominate the spectrum. The total noise power for the two orderings turns out 
to be almost the same in this case. 
The example we have just presented shows the complexity of the issues that arise 
in fixed-point implementations of cascade IIR systems. The parallel form is somewhat 
simpler because the issue of pairing and ordering does not arise. However, scaling is still 
required to avoid overflow in individual 2nd-order subsystems and when the outputs of 
the subsystems are summed to produce the overall output. The techniques that we have 
developed must therefore be applied for the parallel form as well. Jackson (1996) dis­
cusses the analysis of the parallel form in detail and concludes that its total output noise 
power is typically comparable to that of the best pairings and orderings of the cascade 
form. Even so, the cascade form is more common, because, for widely used IIR filters 
such that the zeros of the system function are on the unit circle, the cascade form can 
be implemented with fewer multipliers and with more control over the locations of the 
zeros. 
6.9.4 Analysis of Direct-Form FIR Systems 
Since the direct form I and direct form II IIR systems include the direct form FIR system 
as a special case (i.e., the case where all coefficients ak in Figures 6.14 and 6.15 are zero), 
the results and analysis techniques ofSections 6.9.1 and 6.9.2 apply to FIR systems if we 
eliminate all reference to the poles of the system function and eliminate the feedback 
paths in all the signal flow graphs. 
The direct form FIR system is simply the discrete convolution 
M 
yen] = .L:>[k]x[n 
k]. 
(6.138) 
k=O 

20 
0 
0:1 
"0 
-20 
-40 
-60 
0  
Radian frequency (w)  
(a) 
20 
0 
0:1 
"0 
-20 
-40 
-60 
0 
0.21T 
0.41T 
0.61T 
O.S1T 
1T 
Radian frequency (w) 
(b) 
20 
0 
0.21T 
0.41T 
0.61T 
O.S1T 
1T 
0:1 
"0 
-20 
-40 
I 
Figure 6.66 
Frequency-response 
functions for example system.
-60 I  
0 
0.21T 
0.41T 
0.61T 
O.S1T 
1T 
(a) 20 IOQ10 IH1 (eiw)l.  
Radian frequency (w) 
(b) 2010Q10 IH2(eiw)l. 
(c) 
(c) 2010Q10 IH3(eiw )l. 
454 

20r----------------------------------, 
ot----___ 
-20 
-40 
-60 
-80~----~------~----~L-----~------~ 
o 
Radian frequency (w) 
(d) 
20,-----------------------------------~ 
0r-----__ 
-20 
!g -40 
-60 
-80 
-100 I....--____-:--l-______....L.-______L_____~_____~ 
o 
Radian frequency (w) 
(e) 
20,-----------------------------------~ 
o~--------------_ 
-20 
!g -40 
-60 
-80 
-100L-----~L-----~------~------~------~ 
o  
027T 
O.47T 
O.67T 
7T 
Radian frequency (w) 
(f) 
Figure 6.66 
(continued) 
(d) 20 log10 IH1 (eiw)l. 
(e) 20 10910 IH1 (ejW )H2(eiW)I. 
(f) 20 10910 IH1 (eiw )H2(eiw)H3(eiw )I 
= 2010g10 1H'(eiw )I. 
455 

456 
Chapter 6 
Structures for Discrete-Time Systems 
-70 ~f--r---.,----,----,-~-~-~ 
-75 
//J
-80 
,,"
........  
--------.",...,.
Il:l 
'0 
-95 
_l00LI__~~__~__~____~__-L____L-__~__~____L-__~ 
o  
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
Radian frequency (w) 
Figure 6.67 
Output noise power spectrum for 123 ordering (solid line) and 321 
ordering (dashed line) of 2nd-order sections. 
Figure 6.68(a) shows the ideal unquantized direct form FIR system, and Figure 6.68(b) 
shows the linear-noise model for the system, assuming that all products are quantized 
before additions are performed. The effect is to inject (M + 1) white-noise sources 
directly at the output of the system, so that the total output noise variance is 
2-2B 
2 
Uf 
(M + 1) 12  
(6.139) 
This is exactly the result we would obtain by setting N = 0 and hef[n] 
S[n] in 
Eqs. (6.106) and (6.115). When a double-length accumulator is available, we would 
need to quantize only the output. Therefore, the factor (M +1) in Eq. (6.139) would be 
replaced by unity. This makes the double-length accumulator a very attractive hardware 
feature for implementing FIR systems. 
Overflow is also a problem for fixed-point realizations of FIR systems in direct 
form. For two's-complement arithmetic, we need to be concerned only about the size 
of the output, since all the other sums in Figure 6.68(b) are partial sums. Thus, the 
impulse-response coefficients can be scaled to reduce the possibility of overflow. Scaling 
multipliers can be determined using any of the alternatives discussed in Section 6.9.2. 
Of course, scaling the impulse response reduces the gain of the system, and therefore 
the SNR at the output is reduced as discussed in that section. 

457 
Section 6.9 
Effects of Round-off Noise in Digital Filters 
x[n] 
h[O] 
h[2]
h[l] 
h[3] 
yIn] 
(a) 
Z-1 
ZI 
o 
x[n] 
h[O] 
h[l] 
h[2] 
hIM -1]
h[31 
hIM] 
e3[n] 
eM[n]
eo In] 
ellnj 
e2 In] 
eM~tln] 
yIn] = YIn] + f[1I] 
(b) 
Figure 6.68 
Direct form realization of an FI Rsystem. (a) Infinite-precision model. 
(b) Linear-noise model. 
Example 6.14 
Scaling Considerations for the FIR System in 
Section 6.8.5 
The impulse-response coefficients for the system in Section 6.8.5 are given in Table 6.5. 
Simple calculations show. and from Figure 6.54(b) we see, that 
27L Ih[nll 
1.751352, 
n=O 
27 
) 1/2 
( :; Ih[nll2 
= 0.679442, 
max IH(ejlV)1 ~ 1.009. 
IlVl:::n
) 
These numbers satisfy the ordering relationship in Eq. (6.127). Thus, the system, as 
given, is scaled so that overflow is theoretically possible for a sinusoidal signal whose 
amplitude is greater than 1/1.009 
0.9911, but even so, overflow is unlikely for most 
signals. Indeed, since the filter has a linear phase, we can argue that, for wideband 
signals, since the gain in the passband is approximately unity, and the gain elsewhere 
is less than unity, the output signal should be smaller than the input signal. 
In Section 6.5.3, we showed that linear-phase systems like the one in Example 6.14 
can be implemented with about half the number of multiplications of the general FIR 
system. This is evident from the signal flow graphs of Figures 6.32 and 6.33. In these 
cases, it should be clear that the output noise variance would be halved if products 
were quantized before addition. However, the utilization of such structures involves a 
more complicated indexing algorithm than the direct form. The architecture of most 
DSP chips combines a double-length accumulator with an efficient pipelined multiply­

458  
Chapter 6 
Structures for Discrete-Time Systems 
accumulate operation and simple looping control to optimize for the case of the direct 
form FIR system. For this reason, direct form FIR implementations are often most 
attractive, even compared with IIR filters that meet frequency-response specifications 
with fewer multiplications, since cascade or parallel structures do not permit long se­
quences of multiply-accumulate operations. 
In Section 6.5.3, we discussed cascade realizations of FIR systems. The results and 
analysis techniques of Section 6.9.3 apply to these realizations; but for FIR systems 
with no poles, the pairing and ordering problem reduces to just an ordering problem. 
As in the case of IIR cascade systems, the analysis of all possible orderings can be very 
difficult if the system is composed of many subsystems. Chan and Rabiner (1973a, 1973b) 
studied this problem and found experimentally that the noise performance is relatively 
insensitive to the ordering. Their results suggest that a good ordering is an ordering for 
which the frequency response from each noise source to the output is relatively flat and 
for which the peak gain is small. 
6.9.5  Floating-Point Realizations of Discrete-Time 
Systems 
From the preceding discussion, it is clear that the limited dynamic range of fixed-point 
arithmetic makes it necessary to carefully scale the input and intermediate signal levels 
in fixed-point digital realizations of discrete-time systems. The need for such scaling can 
be essentially eliminated by using floating-point numeric representations and floating­
point arithmetic. 
In floating-point representations, a real number x is represented by the binary 
number 2CxM, where the exponent c of the scale factor is called the characteristic and 
XM is a fractional part called the mantissa. Both the characteristic and the mantissa are 
represented explicitly as fixed-point binary numbers in floating-point arithmetic sys­
tems. Floating-point representations provide a convenient means for maintaining both 
a wide dynamic range and low quantization noise; however, quantization error mani­
fests itself in a somewhat different way. Floating-point arithmetic generally maintains its 
high accuracy and wide dynamic range by adjusting the characteristic and normalizing 
the mantissa so that 0.5 < XM < 1. When floating-point numbers are multiplied, their 
characteristics are added and their mantissas are multiplied. Thus, the mantissa must 
be quantized. When two floating-point numbers are added, their characteristics must 
be adjusted to be the same by moving the binary point of the mantissa of the smaller 
number. Hence, addition results in quantization, too. Ifwe assume that the range of the 
characteristic is sufficient so that no numbers become larger than 2c, then quantization 
affects only the mantissa, but the error in the mantissa is also scaled by 2c• Thus, a 
quantized floating-point number is conveniently represented as 
X=x(1+e)=x+eX.  
(6.140) 
By representing the quantization error as a fraction e of x, we automatically represent 
the fact that the quantization error is scaled up and down with the signal level. 
The aforementioned properties of floating-point arithmetic complicate the quan­
tization error analysis of floating-point implementations of discrete-time systems. First, 
noise sources must be inserted both after each multiplication and after each addition. 
An important consequence is that, in contrast to fixed-point arithmetic, the order in 

459 
Section 6.10 
Zero-Input Limit Cycles in Fixed-Point Realizations of IIR Digital Filters 
which multiplications and additions are performed can sometimes make a big differ­
ence. More important for analysis, we can no longer justify the assumption that the 
quantization noise sources are white noise and are independent of the signaL In fact, 
in Eq. (6.140), the noise is expressed explicitly in terms of the signaL Therefore, we can 
no longer analyze the noise without making assumptions about the nature of the input 
signal. If the input is assumed to be known (e.g., white noise), a reasonable assumption 
is that the relative error £ is independent of x and is uniformly distributed white noise. 
With these types of assumptions, useful results have been obtained by Sandberg 
(1967), Liu and Kaneko (1969), Weinstein and Oppenheim (1969), and Kan and Ag­
garwal (1971). In particular, Weinstein and Oppenheim, comparing floating-point and 
fixed-point realizations of 1st_ and 2nd-order IIR systems, showed that if the number 
of bits representing the floating-point mantissa is equal to the length of the fixed-point 
word, then floating-point arithmetic leads to higher SNR at the output. Not surpris­
ingly, the difference was found to be greater for poles close to the unit circle. However, 
additional bits are required to represent the characteristic, and the greater the desired 
dynamic range, the more bits are required for the characteristic. Also, the hardware 
to implement floating-point arithmetic is much more complex than that for fixed-point 
arithmetic. Therefore, the use of floating-point arithmetic entails an increased word 
length and increased complexity in the arithmetic unit. Its major advantage is that it 
essentially eliminates the problem of overflow, and if a sufficiently long mantissa is used, 
quantization also becomes much less ofa problem. This translates into greater simplicity 
in system design and implementation. 
Nowadays, digital filtering ofmulti-media signals is often implemented on personal 
computers or workstations that have very accurate floating point numerical represen­
tions and high speed arithmetic units. In such cases, the quantization issues discussed in 
Sections 6.7-6.9 are generally of little or no concern. However, in high volume systems, 
fixed point arithmetic is generally required to achieve low cost. 
6.10  ZERO-INPUT LIMIT CYCLES IN FIXED-POINT 
REALIZATIONS OF IIR DIGITAL FILTERS 
For stable IIR discrete-time systems implemented with infinite-precision arithmetic, 
if the excitation becomes zero and remains zero for n greater than some value no, 
the output for n > no will decay asymptotically toward zero. For the same system, 
implemented with finite-register-length arithmetic, the output may continue to oscillate 
indefinitely with a periodic pattern while the input remains equal to zero. This effect is 
often referred to as zero-input limit cycle behavior and is a consequence either of the 
nonlinear quantizers in the feedback loop of the system or of overflow of additions. The 
limit cycle behavior of a digital filter is complex and difficult to analyze, and we will not 
attempt to treat the topic in any general sense. To illustrate the point, however, we will 
give two simple examples that will show how such limit cycles can arise. 
6.10.1 Limit Cycles Owing to Round-off and Truncation 
Successive round-off or truncation of products in an iterated difference equation can 
create repeating patterns. This is illustrated in the following example. 

460 
Chapter 6 
Structures for Discrete-Time Systems 
Example 6.15 
Limit Cycle Behavior in a 1 st"()rder System 
Consider the 1st-order system characterized by the difference equation 
y[n] 
ay[n -
1] + x[n], 
JaJ < l. 
(6.141) 
The signal flow graph of this system is shown in Figure 6.69(a). Let us assume 
that the register length for storing the coefficient a, the input x[n], and the filter node 
variable y[n - 1] is 4 bits (i.e., a sign bit to the left of the binary point and 3 bits to the 
right of the binary point). Because of the finite-length registers, the product ay[n -1] 
must be rounded or truncated to 4 bits before being added to x[n]. The flow graph 
representing the actual realization based on Eq. (6.141) is shown in Figure 6.69(b). 
Assuming rounding of the product, the actual output S£n] satisfies the nonlinear dif­
ference equation 
.Y[n] = may[n - 1]] + x[n], 
(6.142) 
where QLJ represents the rounding operation. Let us assume that a 
1/2 = 00100 
and thatthe input is x[n] 
(7/8)8[n] :: (00111)8[nJ. Using Eq. (6.142), we see that for 
n = 0, y[O] 
7/8 = 00111. To obtain 9[1], we multiply nO] by a, obtaining the result 
ay[O] = 00011100, a 7-bit number that must be rounded to 4 bits. This number, 7/16, 
is exactly halfway between the two 4-bit quantization levels 4/8 and 3/8. If we choose 
always to round upward in such cases, then 00011100 rounded to 4 bits i800 100 
1/2. 
Sincex[1] = 0, it follows that 5'[1] 
00100 
1/2. Continuing to iterate the difference 
equation gives H2] = Q[a.Y[lll = 00010 
1/4 and H3] = 00001 
1/8. In both 
these cases, no rounding is necessary. However, to obtain .Y[4], we must round the 
7-bit number aH31 = 00000100 to 00001. The same result is obtained for all values of 
n .::: 3. The output sequence for this example is shown in Figure 6.70(a). If a = 
we can carry out the preeeding computation again to demonstrate that the output is as 
shown in Figure 6.70(b). Thus, because of rounding ofthe produet aHn -1], the output 
reaches a constant value of 1/8 when a = 1/2 and a periodic steady-state oscillation 
between +1/8 and -1/8 when a 
-1/2. These are periodic outputs similar to those 
that would be obtained from a 1 st-order pole at z = ±1 instead of at z 
±1/2. 
0 
0 
x[n) 
y[n]
~,,' 
(a)  
0 
.. 
0  
x[n] 
j![n) 
Z·I
~. 
a 
(b) 
Figure 6.69 1st-order IIR system. (a) Infinite-precision linear system. (b) Non­
linear system due to Quantization. 

461 
Section 6.10 
Zero-Input Limit Cycles in Fixed-Point Realizations of IIR Digital Filters 
7 
8 
yin] (a=~) 
1 
2 
1 
4 
1 
8 
-2 
-] 
0 
2 
3 
4 
5 
6 
7 
n 
(a) 
7 
8 
.Y[n] (a = 1)
2 
1 
4 
1 
-2 
-1 
2 
n 
1 
8 
2 
(b) 
Figure 6.70 
Response of the 1st-order system of Figure 6.69 to an impulse. 
(a)a=~.(b)a= 
~. 
When a = +1/2, the period of the oscillation is 1, and when a = -1/2, the 
period of oscillation is 2. Such steady-state periodic outputs are called limit cycles, and 
their existence was first noted by Blackman (1965), who referred to the amplitude 
intervals to which such limit cycles are confined as dead bands. In this case, the dead 
band is _Z-B ::.: ,Hnl ::.: +2-B , where B 
3. 
The foregoing example has illustrated that a zero-input limit cycle can result from 
rounding in a 1st-order IIR system. Similar results can be demonstrated for truncation. 
2nd-order systems can also exhibit limit cycle behavior. In the case ofparallel realizations 

462 
Chapter 6 
Structures for Discrete-Time Systems 
of higher-order systems, the outputs of the individual 2nd-order systems are indepen­
dent when the input is zero. In this case, one or more of the 2nd-order sections could 
contribute a limit cycle to the output sum. In the case of cascade realizations, only the 
first section has zero input; succeeding sections may exhibit their own characteristic 
limit cycle behavior, or they may appear to be simply filtering the limit cycle output of 
a previous section. For higher-order systems realized by other filter structures, the limit 
cycle behavior becomes more complex, as does its analysis. 
In addition to giving an understanding of limit cycle effects in digital filters, the 
preceding results are useful when the zero-input limit cycle response of a system is the 
desired output. This is the case, for example, when one is concerned with digital sine 
wave oscillators for signal generation or for the generation ofcoefficients for calculation 
of the discrete Fourier transform. 
6.10.2 Limit Cycles Owing to Overflow 
In addition to the classes of limit cycles discussed in the preceding section, a more severe 
type oflimit cycle can occur owing to overflow. The effect of overflow is to insert a gross 
error in the output, and in some cases the filter output thereafter oscillates between 
large-amplitude limits. Such limit cycles have been referred to as overflow oscillation. 
The problem of oscillations caused by overflow is discussed in detail by Ebert et al. 
(1969). Overflow oscillations are illustrated by the following example. 
Example 6.16 Overflow Oscillations in a 2 nd-Order System 
Consider a 2nd-order system realized by the difference equation 
y[n] 
x[n] + Q[aly[n 
1]] + Q[a2y[n 
2]], 
(6.143) 
where Q[.] represents two's-complement rounding with a word length of3 bits plus 1 
bit for the sign. Overflow can occur with two's-complement addition of the rounded 
products. Suppose that al 
3/4 
00110 and a2 
-3/4 
10010, and assume that 
x[n] remains equal to zero for n ::: o. Furthermore, assume that y[ -1] = 3/4 
00 110 
and.H-2] 
-3/4 = 10 010. Now the output at sample n 
0 is 
.HO] 
00 110 x 00 110 + 10010 x 10 010. 
If we evaluate the products using two's-complement arithmetic, we obtain 
HO] = 00 100100 + 00 100100, 
and if we choose to round upward when a number is halfway between two quantization 
levels, the result of two's-complement addition is 
y[O] 
00 101 + 00 101 
10 010 = -l 
In this case the binary carry overflows into the sign bit, thus changing the positive sum 
into a negative number. Repeating the process gives 
HI] = 10011 + 10 011 = 00 110 
~. 
The binary carry resulting from the sum of the sign bits is lost and the negative sum is 
mapped into a positive number. Clearly. yIn] will continue to oscillate between +3/4 
and -3/4 until an input is applied. Thus, yIn] has entered a periodic limit cycle with a 
period of 2 and an amplitude of almost the full-scale amplitude of the implementation. 

463 
Section 6.11 
Summary 
The preceding example illustrates how overflow oscillations occur. Much more 
complex behavior can be exhibited by higher-order systems, and other frequencies can 
occur. Some results are available for predicting when overflow oscillations can bc sup­
ported by a difference equation (see Ebert et aI., 1969). Overflow oscillations can be 
avoided by using the saturation overflow characteristic of Figure 6.45(b) (see Ebert et 
aI., 1969). 
6.10.3 Avoiding Limit Cycles 
The possible existence of a zero-input limit cycle is important in applications where a 
digital filter is to be in continuous operation, since it is generally desired that the output 
approach zero when the input is zero. For example, suppose that a speech signal is 
sampled, filtered by a digital filter, and then converted back to an acoustic signal using 
a D/A converter. In such a situation it would be very undesirable for tbe filter to enter 
a periodic limit cycle whenever tbe input is zero, since the limit cycle would produce an 
audible tone. 
One approach to the general problem of limit cycles is to seek struetures that do 
not support limit cycle oscillations. Such structures have been derived by using state­
space representations (see Barnes and Fam, 1977; Mills, Mullis and Roberts, 1978) 
and concepts analogous to passivity in analog systems (see Rao and Kailath, 1984; 
Fettweis, 1986). However, these structures generally require more computation than 
an equivalent cascade or parallel form implementation. By adding more bits to the 
computational wordlength, we can generally avoid overflow. Similarly, since round-off 
limit cycles usually are limited to the least significant bits of the binary word, additional 
bits can be used to reduce the effective amplitude of the limit cycle. Also, Claasen 
et a!. (1973) showed that if a double-length accumulator is used so that quantization 
occurs after the accumulation of products, then limit cycles owing to round-off are much 
less likely to occur in 2nd-order systems. Thus, the trade-off between word length and 
computational algorithm complexity arises for limit cycles just as it does for coefficient 
quantization and round-off noise. 
Finally, it is important to point out that zero-input limit cycles due to both overflow 
and round-off are a phenomenon unique to IIR systems: FIR systems cannot support 
zero-input limit cycles, because they have no feedback paths. The output of an FIR 
system will be zero no later than (M + 1) samples after the input goes to zero and 
remains there. This is a major advantage of FIR systems in applications wherein limit 
cycle oscillations cannot be tolerated. 
6.11 SUMMARY 
In this chapter, we have considered many aspects of the problem of implementing an 
LTI discrete-time system. The first half of the chapter was devoted to basic implemen­
tation structures. After introducing block diagram and signal flow graphs as pictorial 
representations of difference equations, we discussed a number of basic structures for 

464 
Chapter 6 
Structures for Discrete-Time Systems 
IIR and FIR discrete-time systems. These included the direct form I, direct form II, cas­
cade form, parallel form, lattice form, and transposed version of all the basic forms. We 
showed that these forms are all equivalent when implemented with infinite-precision 
arithmetic. However, the different structures are most significan t in the context of finite­
precision implementations. Therefore, the remainder ofthe chapter addressed problems 
associated with finite precision or quantization in fixed-point digital implementations 
of the basic structures. 
We began the discussion of finite precision effects with a brief review of digital 
number representation and an overview showing that the quantization effects that are 
important in sampling (discussed in Chapter 4) are also important in representing the 
coefficients of a discrete-time system and in implementing systems using finite-precision 
arithmetic. We illustrated the effect of quantization of the coefficients of a difference 
equation through several examples. This issue was treated independently of the effects 
of finite-precision arithmetic, which we showed introduces nonlinearity into the system. 
We demonstrated that in some cases this nonlinearity was responsible for limit cycles 
that may persist after the input to a system has become zero. We also showed that quan­
tization effects can be modeled in terms of independent random white-noise sources 
that are injected internally into the flow graph. Such linear-noise models were devel­
oped for the direct form structures and for the cascade structure. In all of our discussion 
of quantization effects, the underlying theme was the conflict between the desire for fine 
quantization and the need to maintain a wide range of signal amplitudes. We saw that 
in fixed-point implementations, one can be improved at the expense of the other, but to 
improve one while leaving the other unaffected requires that we increase the number 
of bits used to represent coefficients and signal amplitudes. 1bis can be done either by 
increasing the fixed-point word length or by adopting a floating-point representation. 
Our discussion of quantization effects serves two purposes. First, we developed 
several results that can be useful in guiding the design of practical implementations. We 
found that quantization effects depend greatly on the structure used and on the specific 
parameters of the system to be implemented, and even though simulation of the system 
is generally necessary to evaluate its performance, many of the results discussed are 
useful in makingintelligent decisions in the design process. A second, equally important 
purpose of this part of the chapter was to illustrate a style of analysis that can be applied 
in studying quantization effects in a variety of digital signal-processing algorithms. The 
examples of the chapter indicate the types of assumptions and approximations that 
are commonly made in studying quantization effects. In Chapter 9, we will apply the 
analysis techniques developed here to the study of quantization in the computation of 
the discrete Fourier transform. 
Problems 
Basic Problems with Answers 
6.1. Determine the system function of the two flow graphs in Figure P6.1, and show that they 
have the same poles. 

6 
Problems  
465 
x[n] 
yin] 
2r cos!} 
Z~l 
Network 1 
(a) 
x[n] 
rcos e 
y [n] 
rcos !} 
Network 2 
(b)  
Figure P6.1 
6.2.  The signal flow graph of Figure P6.2 represents a linear difference equation with constant 
coefficientS. Determine the difference equation that relates the output y[n) to the input 
x[n). 
Q---<---<r--- ---~~--------l--- ---~-----_Q_----O 
x[n]  
yin] 
Figure P6.2 
6.3.  Figure P6.3 shows six systems. Determine which one of the last five, (b}-(f), has the same 
system function as (a). You should be able to eliminate some of the possibilities by inspec­
tion. 

x[nJ 
(a) 
.
~ 
!z_l · 
• 
~ 
1 
-<0 
1 
3 
2 
4 
(b) 
x[nJ 
(c) 
2 
x[nJ 
1 
Z-1 
1 
4 
4 
;::-1 
3 
8 
(d) 
y[nJ 
! . 
Z-1 
y[n]
0 
Z-1 
. 3 
SID "4 
y[nJ 
Z-l 
y[nJ 
Figure P6.3 
466 

--
467 
Chapter 6 
Problems 
2 
x[n) 
y[n) 
1 
1 
4 
4 
3 
8 
(e) 
2 
x [n) 
yIn) 
1 
4 
Z-l 
1 
4 
3 
8 
(0  
Figure P6.3 (continued) 
6.4.  Consider the system in Figure P6.3( d). 
(a)  Determine the system function relating the z-transforms of the input and output. 
(b)  Write the difference equation that is satisfied by the input sequence x[n1and the output 
sequence y[n). 
6.S.  An LTI system is realized by the flow graph shown in Figure P6.5. 
x [n) 
yIn) 
Z-l 
3 
1 
Z-l 
Z-l 
1 
2 
Figure P6.5 
(a)  Write the difference equation relating x[n) and y[n) for this flow graph. 
(b)  What is the system function of the system? 
(c)  In the realization of Figure P6.S, how many real multiplications and real additions are 
required to compute each sample of the output? (Assume that x[n) is real, and assume 
that multiplication by 1 does not count in the total.) 
(d)  The realization of Figure P6.5 requires four storage registers (delay elements). Is it 
possible to reduce the number of storage registers by using a different structure? If 
so, draw the flow graph; if not, explain why the number of storage registers cannot be 
reduced. 

0 
Chapter 6 
Structures for Discrete-Time Systems
468 
6.6. Determine the impulse response of each of the systems in Figure P6.6. 
~ 
~  
~ 
~-lnl'11' 1_2:  
1_1: 
11.
I,: I,: 
y[n] 
(a) 
... 
2 
-1 
3 
y[n] 
(d) 
Figure P6.6 
6.7.  Let x[n] and y[n] be sequences related by the following difference equation: 
1 
1 
y[n] 4y [n 
2] 
x[n - 2] -
4x[n]. 
Draw a direct form II signal flow graph for the causal LTI system corresponding to this 
difference equation . 

469 
Chapter 6 
Problems 
6.8.  The signal flow graph in Figure P6.8 represents an LTI system. Determine a difference 
equation that gives a relationship between the input x[n] and the output y[n] of this system. 
As usual, all branches of the signal flow graph have unity gain unless specifically indicated 
otherwise. 
x[n] 
yin] 
2  
Figure P6.8 
6.9.  Figure P6.9 shows the signal flow graph for a causal discrete-time LTI system. Branches 
without gains explicitly indicated have a gain of unity. 
(3)  By tracing the path of an impulse through the flowgraph, determine h[1], the impulse 
response at n 
1. 
(b)  Determine the difference equation relating x[n] and y[nl 
1 
x[n] 
yin] 
-1 
2 
.:;-1 
4 
Figure P6.9 
6.10. Consider the signal flow graph shown in Figure P6.1O. 
(a)  Using the node variables indicated, write the set of difference equations represented 
by this flow graph. 
(b)  Draw the flow graph of an equivalent system that is the cascade of two 1st-order 
systems. 
(c)  Is the system stable? Explain. 
x~ 
y~ 
Z-1  1 
vrn] 
2 
2 
,Z  1 
2
I  
I 
wrn1  
Figure P6.10 

470  
Chapter 6 
Structures for Discrete-Time Systems 
6.11.  Consider a causal LTI system with impulse response h[n] and system function 
(1 
2z-1)(1-4z-1)
H (z) = --.,-----:--­
z 
(a)  Draw a direct form II flow graph for the system. 
(b)  Draw the transposed form of the flow graph in part (a). 
6.12.  For the LTI system described by the flow graph in Figure P6.12, determine the difference 
equation relating the input x[n] to the output y[n]. 
1 
x[n] 
y[n1 
Figure P6.12 
6.13.  Draw the signal flow graph for the direct form I implementation of the LTI system with 
system function 
1-2
1 - ZZ 
H(z) 
1 
l' 
1- fic l 
scl 
6.14.  Draw the signal flow graph for the direct form II implementation of the LTI system with 
system function 
H(z) 
6.15.  Draw the signal flow graph for the transposed direct form II implementation of the LTI sys­
tem with system function 
1 - Iz-l + l z- 2 
H(z) = 
6 
6 
1 +C1 + ~C2 
6.16.  Consider the signal flow graph shown in Figure P6.16. 
(a)  Draw the signal flow graph that results from applying the transposition theorem to this 
signal flow graph. 
(b)  Confirm that the transposed signal flow graph that you found in (a) has the same system 
function H (z) as the original system in the figure. 
x[n] 
y[n 
Z-l 
-2 
1 
Z-l 
2 
-
~ 
3 
1 
4 
Figure P6.16 

Chapter 6 
Problems  
471 
6.17.  Consider the causal LTI system with system function 
1 -1 
1_2
H(z) = 1 
3Z 
+ 6Z 
+ 
(a)  Draw the signal flow graph for the direct form implementation of this system. 
(b)  Draw the signal flow graph for the transposed direct form implementation of the 
system. 
6.18.  For some nonzero choices of the parameter a, the signal flow graph in Figure P6.18 can 
be replaced by a 2nd-order direct form II signal flow graph implementing the same system 
function. Give one such choice for a and the system function H (z) that results. 
x [n] 
4 
-
3 
Z-l 
1 
-­
4 
:-1 
a 
y[n] 
4 
3 
3 
8 
Figure P6.18 
6.19.  Consider the causal LTI system with the system function 
2 -
~z-l - 2c2 
H(z) = ( 
. 
) ( 
).
1  
~Cl 1 + ~Cl 
Draw a signal flow graph that implements this system as a parallel combination of 1sl-order 
transposed direct form II sections. 
6.20.  Draw a signal flow graph implementing the system function 
(1 + (1 
j/2)z-1)(1 + (1 + jf2)z-1) 
as a cascade of 2nd -order transposed direct form II sections with real coefficients. 
Basic Problems 
6.21.  For many applications, it is useful to have a system that will generate a sinusoidal sequence. 
One possible way to do this is with a system whose impulse response is h[nl 
eiW()nu[nj. 
The real and imaginary parts of h[n] are therefore h,[n] 
(coswon)u[nJ and hj[n] = 
(sin won )uLn], respectively. 
In implementing a system with a complex impulse response, the real and imaginary 
parts are distinguished as separate outputs. By first writing the complex difference equation 
required to produce the desired impulse response and then separating it into its real and 
imaginary parts, draw a flow graph that will implement this system. The flow graph that 
you draw should have only real coefficients. This implementation is sometimes called the 
coupled form oscillator, since, when the input is excited by an impulse, the outputs are 
sinusoidal. 
H(z) 

Chapter 6 
Structures for Discrete-Time Systems
412 
6.22.  For the system function 
1 +2rl + 
H(z) = 1 _ 0.75z I + O.125C2' 
draw the flow graphs of all possible realizations for this system as cascades of 1 sl-order 
systems. 
6.23.  We want to implement a causal system H(z) with the pole-zero diagram shown in Fig­
ure P6.23. For all parts of this problem, Zl, Z2, PI, and P2 are real, and a gain constant that 
is independent of frequency can be absorbed into a gain coefficient in the output branch of 
each flow graph. 
Im 
Zl Z2 PI P2i 
Re 
Figure P6.23 
(a)  Draw the flow graph of the direct form II implementation. Determine an expression 
for each of the branch gains in terms of the variables ZI, Z2, PI, and P2. 
(b)  Draw the flow graph of an implementation as a cascade of 2nd-order direct form II 
sections. Determine an expression for each of the branch gains in terms of the variables 
zf, Z2, PI, and P2· 
(c)  Draw the flow graph of a parallel form implementation with 15t-order direct form II 
sections. Specify a system of linear equations that can be solved to express the branch 
gains in terms of the variables Zl, z2, PI, and P2. 
6.24.  Consider a causal LTI system whose system function is 
1  
3 .-1 + I z-2 
H (z) = 
10"~ 
= -~--"'----=--­
(1-~Cl+!C2)(1+!Cl) 
1­
(a)  Draw the signal flow graphs for implementations of the system in each of the following 
forms: 
(i) Direct form I 
(ii)  Direct form II 
(iii)  Cascade form using 1st_and 2nd-order direct form II sections 
(iv)  Parallel form using 1 st_ and 2nd-order direct form I sections 
(v) Transposed direct form II. 
(b) Write the difference equations for the flow graph of part (v) in (a) and show that this 
system has the correct system function. 
l 

413 
Chapter 6 
Problems 
6.25. A causal LTI system is defined by the signal flow graph shown in Figure P6.25, which 
represents the system as a cascade of a 2nd-order system with a lSI-order system. 
x[n]  
yIn] 
0.3 
Z-l 
0.4 
0.81 
l : !<8 ]>------+--0 
Figure P6.25 
(a)  What is the system function of the overall cascade system? 
(b)  Is the overall system stable? Explain briefly. 
(c)  Is the overall system a minimum-phase system? Explain briefly. 
(d)  Draw the signal flow graph ofa transposed direct form II implementation ofthis system. 
6.26. A causal LTI system has system function given by the following expression: 
H(z) 
(a)  Is this system stable? Explain briefly. 
(b)  Draw the signal flow graph of a parallel form implementation of this system. 
(c)  Draw the signal flow graph of a cascade form implementation of this system as a 
cascade of a lSI-order system and a 2nd-order system. Use a transposed direct form II 
implementation for the 2nd-order system. 
6.27. An LTI system with system function 
H(z) 
is to be implemented using a flow graph of the form shown in Figure P6.27. 
x [n] 
, Z-l 
Z-l  
'Z-l y[n]
Z-l 
Z-l 
Figure P6.27 

474  
Chapter 6 
Structures for Discrete-Time Systems 
(a)  Fill in all the coefficients in the diagram of Figure P6.27. Is your solution unique? 
(b)  Define appropriate node variables in Figure P6.27, and write the set of difference 
equations that is represented by the flow graph. 
6.28.  (a) Determine the system function, H(z), from x[n] to y[n] for the flow graph shown in 
Figure P6.28-1 (note that the location where the diagonal lines criss-cross is not a single 
node). 
r-------------~----~O~------_.------------­
Z-l 
-1/4 
y[n]
x[n]  
-1 
-4/5 
Figure P6.28-1 
(b)  Draw the direct form (I and II) flow graph of systems having the system function H(z). 
(c)  Design HI (z) such that H2 (z) in Figure P6.28-2 has a causal stable inverse and 
IH2(eJW)1 = IH(e}W)!. Note: Zero-pole cancellation is permitted. 
~  
~ 
H2(z) 
Figure P6.28-2 
(d) Draw the transposed direct form II flow graph for H2 (z). 
6.29.  (a) Determine the system function H (z) relating the input x[n] to the output y[n] for the 
FIR lattice filter depicted in Figure P6.29. 
x[n]  
y[n] 
Z-l 
Z-l 
Figure P6.29 
(b) Draw the lattice filter structure for the all-pole filter 1/H(z). 

475 
Chapter 6 
Problems 
6.30. Determine and draw the lattice filter implementation of the following causal all-pole system 
function: 
1
H(z) 
Is the system stable? 
6.31. An TIR lattice filter is shown in Figure P6.31. 
Figure P6.31 
(a)  By tracing the path of an impulse through the flowgraph, determine y[l] for input 
x[n] 
8[n]. 
(b) Determine a flow graph for the corresponding inverse filter. 
(c)  Determine the transfer function for the IIR filter in Figure P6.31. 
6.32. The flow graph shown in Figure P6.32 is an implementation of a causal, LTI system. 
x[n] 
yfn] 
2 
2 
Figure P6.32 
(a) Draw the transpose of the signal flow graph. 
(b)  For either the original system or its transpose, determine the difference equation relat­
ing the input x[n] to the output y[n]. (Note: The difference equations will be the same 
for both structures.) 
(c) Is the system BIBO stable? 
(d) Determine y[2] if x[n] = (1/2)nu[n]. 

476  
Chapter 6 
Structures for Discrete-Time Systems 
Advanced Problems 
6.33. Consider the LTI system represented by the FIR lattice structure in Figure P6.33-1. 
~[n] 
Z-l 
Figure P6.33-1 
(a)  Determine the system function from the input x[nJ to the output u[n] (NOT y[nJ). 
(b)  Let H(z) be the system function from the input x[nJ to the output y[nJ, and let g[n] 
be the result of expanding the associated impulse response h[nJ by 2 as shown in 
Figure P6.33-2. 
4t2~ 
Figure P6.33-2 
The impulse response g[nJ defines a new system with system function G(z). We would 
like to implement G(z) using an FIR lattice structure. Determine the k-parameters 
necessary for an FIR lattice implementation of G(z). Note: You should think carefully 
before diving into a long calculation. 
6.34. Figure P6.34-1 shows an impulse response h[n], specified as 
{ nr
/4 
h[n] 
urn], for n an integer multiple of 4 
constant in between as indicated 
1!~ j;rn; T 
1/4 Ullll11t ill 
o 
4 
8 
n 
Figure P6.34-1 
(a)  Determine a choice for hl[n] and h2[n] such that 
h[n] = hl[n] * h2[n], 
where hl[n] is an FIR filter and where h2[n] = 0 for nj4 not an integer. Is h2[n] an FIR 
or IIR filter? 

477 
Chapter 6 
Problems 
(b) The impulse response h[n] is to be used in a downsampling system as indicated in 
Figure P6.34-2. 
~I
~h[nl 
Figure P6.34-2 
Draw a flow graph implementation of the system in Figure P6.34-2 that requires the 
minimum number of nonzero and nonunity coefficient multipliers. You may use unit 
delay elements, coefficient multipliers, adders and compressors. (Multiplication by a 
zero or a one does not require a multiplier.) 
(c)  For your system, state how many multiplications per input sample and per output 
sample are reqlrired, giving a brief explanation. 
6.35. Consider the system shown in Figure P6.35-1. 
~I 
h[n] H ~4 ~ 
~ . 
_ 
~
Figure P6.35-1 
We want to implement this system using the polyphase structure shown in Figure P6.35-2. 
x[n] 
1----"---1 eo[n]  
y[n] 
Figure P6.35-2 
Polyphase structure of the system. 
For parts (a) and (b) only, assume h[n] is defined in Figure P6.35-3 
112 
112 112 
112 
112 
h[nJ 
114 
114 
114 
••• 
n Figure P6.35-3 
(h[n] 
0 for all n < 0 and n ~ 12). 

478  
Chapter 6 
Structures for Discrete-Time Systems 
(a)  Give the sequences eo[n], e1[n], e2[n], and e3 [n] that result in a correct implementation. 
(b)  We want to minimize the total number of multiplies per output sample for the imple­
mentation of the structure in Figure P6.35-2. Using the appropriate choice of eo[n], 
e1[n], e2[n], and e3[n] from part (a), determine the minimum number of multiplies 
per output sample for the overall system. Also, determine the minimum number of 
multiplies per input sample for the overall system. Explain. 
(c)  Instead of using the sequences eo[n], e1[n], e2[n], and e3[n] identified in part (a), now 
assume that Eo(eiw) and E2(eiw) the DTFfs of eo[n] and e2[n], respectively, are as 
given in Figure P6.35-4, and E1 (eiw) = E3(eiw) = o. 
00 
Eiejw) =L: !l(w + 27rr)
Eo(e jw) 
r=-oo 
I 
11 
w 
-We 
o 
We 
Figure P6.35-4 
Sketch and label H(eiw) from (-Jr, Jr). 
6.36.  Consider a general flow graph (denoted Network A) consisting of coefficient multipliers 
and delay elements, as shown in Figure P6.36-1. If the system is initially at rest, its behavior 
is completely specified by its impulse response h [n]. We wish to modify the system to create 
a new flow graph (denoted Network A d with impulse response h1[n] = (-l)nh[n]. 
~ 
x[n]  
y[n]
-1 
~ 
Network A  
Figure P6.36-1 
(a) If H (eiw ) is as given in Figure P6.36-2, sketch H 1(eiw). 
H(e jW) 
-7r 
7r  
7r 
7r 
w 
2 
2  
Figl1re P6.36-2 
(b)  Explain how to modify Network A by simple modifications ofits coefficient multipliers 
and/or the delay branches to form the new Network A 1 whose impulse response is 
h1[n]. 

479 
Chapter 6 
Problems 
(c)  If Network A is as given in Figure P6.36-3, show how to modify it by simple modifica­
tions to only the coefficient multipliers so that the resulting Network A 1 has impulse 
response h1[n]. 
x [n] 
1 
2 
2 
-2 
Z-1 
-1 
-1 
Z-1 
y[n] 
-2 
Figure P6.36-3 
6.37. The flow graph shown in Figure P6.37 is noncomputable; i.e., it is not possible to compute 
the output using the difference equations represented by the flow graph because it contains 
a closed loop having no delay elements. 
x [n] 
y[n] 
b 
a 
Z-1 
a 
-1 
Figure P6.37 
(a)  Write the difference equations for the system of Figure P6.37, and from them, find the 
system function of the flow graph. 
(b)  From the system function, obtain a flow graph that is computable. 
6.38. The impulse response of an LTI system is 
0::: n::: 7,
h[n] = {an,
0, 
otherwise. 
(a)  Draw the flow graph of a direct form nonrecursive implementation of the system. 
(b)  Show that the corresponding system function can be expressed as 
1 - a8z-8 
H(z) = 
, Izl > lal· 
1­
(c)  Draw the flow graph of an implementation of H(z), as expressed in part (b), corre­
sponding to a cascade ofan FIRsystem (numerator) with an IIR system (denominator). 
(d)  Is the implementation in part (c) recursive ornonrecursive? Is the overall system FIR 
or IIR? 

Chapter 6 
Structures for Discrete-Time Systems
480 
6.39. 
6.40. 
(e)  Which implementation of the system requires 
(i) the most storage (delay elements)? 
(ii)  the most arithmetic (multiplications and additions per output sample)? 
Consider an FIR system whose impulse response is 
h[n] = { ls(1 +cos[(2n/15)(n -no)]), 
0::: n ::: 14, 
0, 
otherwise. 
This system is an example of a class of filters known as frequency-sampling filters. Prob­
lem 6.51 discusses these filters in detail. In this problem, we consider just one specific case. 
(a)  Sketch the impulse response of the system for the cases no = 0 and no = 15/2. 
(b)  Show that the system function of the system can be expressed as 
-15 
~e- j 2Jrno/ls 
~ej2Jrno/ls]
1 [1 
H(z) = (1- z 
). 15 1- C1 + 1 _ ej2Jr/1sCl + -1-_=-e-----:j:::-2Jr-/c:-1-=-sz----:-1 
. 
(c)  Show that if no = 15/2, the frequency response of the system can be expressed as 
J'w 
1 _ jw7 {Sin(WI5/2) 
1 sin[(w - 2n/15)15/2]
H (e 
) = -
e  
+ - ---:-"-:-c---=-,'--c-:--=-~
15 
sin(w/2) 
2 sin[(w - 2n/15)/2] 
~ sin[(w + 2n/15)15/2] } 
2 sin[(w+2n/15)/2] 
. 
Use this expression to sketch the magnitude of the frequency response of the system 
for nO = 15/2. Obtain a similar expression for nO = O. Sketch the magnitude response 
for nO = O. For which choices of nO does the system have a generalized linear phase? 
(d)  Drawa signal flow graph of an implementation of the system as a cascade of an FIR sys­
tem whose system function is 1-z-ls and a parallel combination of a 1 st_ and 2nd-order 
IIR system. 
Consider the discrete-time system depicted in Figure P6.40-1. 
G  
l+r 
o 
• I ·  I · I · 1. 
0 
x[n[ 
1, 
';' _, 
.' ';' 
-1 
y;ln] 
l-r  
Figure P6.40-1 
(a)  Write the set of difference equations represented by the flow graph of Figure P6.40-1. 
(b)  Determine the system function HI (z) = Yl (z)/ X (z) of the system in Figure P6.40-1, 
and determine the magnitudes and angles of the poles of HI (z) as a function of r for 
-1<r<l. 
(c)  Figure P6.40-2 shows a different flow graph obtained from the flow graph of Fig­
ure P6.40-1 by moving the delay elements to the opposite top branch. How is the 
system function H 2(z) = Y2 (z)/ X (z) related to HI (z)? 
Z-l  
Z-l
G  
l+r 
0 
<[n] 
1 
_, 
• 
' 
-1 
,,[n]
o  
• I · I'! · 1. 
l-r  
Figure P6.40-2 
.....  

481 
Chapter 6 
Problems 
6.41.  The three flow graphs in Figure P6.41 are all equivalent implementations of the same two­
input, two-output LTI system. 
1 + r 
~,[nl' -,I 
~Yd~ 
• 
~ I'
Y2[n] 
1 r 
x2[n] 
Y2[n]  
x2[n] 
Network A  
Network B 
(a)  
(b) 
a 
Xj[n] 
yIfn] 
b 
c 
d 
Network C 
(c) 
Figure P6.41 
(a)  Write the difference equations for Network A. 
(b)  Determine values of a, b, c, and d for Network B in terms of r in Network A such that 
the two systems are equivalent. 
(c)  Determine values of e and f for Network C in terms of r in Network A such that the 
two systems are equivalent. 
(d)  Why might Network B or C be preferred over Network A? What possible advantage 
could Network A have over Network B or C? 
6.42.  Consider an all-pass system with system function 
H(z) = 
o 
• 
x[n] 
Figure P6.42 
A flow graph for an implementation of this system is shown in Figure P6.42. 
d 
(a)  Determine the coefficients b, c, and d such that the flow graph in Figure P6.42 is a 
direct realization of H (z). 
(b)  In a practical implementation of the network in Figure P6.42, the coefficients b, c, and 
d might be quantized by rounding the exact value to the nearest tenth (e.g., 0.54 will 
be rounded to 0.5 and 1/0.54 = 1.8518 ... will be rounded to 1.9). Would the resulting 
system still be an all-pass system? 

482  
Chapter 6 
Structures for Discrete-Time Systems 
(c)  Show that the difference equation relating the input and output of the all-pass system 
with system function H (z) can be expressed as 
yIn] = O.54(y[n -1] - x[nD + x[n 
1]. 
Draw the flow graph of a network that implements this difference equation with two 
delay elements, but only one multiplication by a constant other than ±1. 
(d)  With quantized coefficients, would the flow graph of part (c) be an all-pass system? 
The primary disadvantage of the implementation in part (c) compared with the im­
plementation in part (a) is that it requires two delay elements. However, for higher-order 
systems, it is necessary to implement a cascade of all-pass systems. For N all-pass sections 
in cascade, it is possible to use all-pass sections in the form determined in part (c) while 
requiring only (N + 1) delay elements. This is accomplished by sharing a delay element 
between sections_ 
(e)  Consider the all-pass system with system function 
z-l - a ) ( z-l - b )
H(z) = 
'.
( 1-ac1 
1-bel 
Draw the flow graph of a "cascade" realization composed of two sections of the form 
obtained in part (c) with one delay element shared between the sections. The resulting 
flow graph should have only three delay elements. 
(f)  With quantized coefficients a and b, would the flow graph in part (e) be an all-pass 
system? 
6.43.  All branches of the signal flow graphs in this problem have unity gain unless specifically 
indicated otherwise. 
2 
, L<lSJ 
yIn] 
x[n]  
Figure P6.43-1 
(a)  The signal flow graph of System A, shown in Figure P6.43-1, represents a causal LTI sys­
tem. Is it possible to implement the same input-output relationship using fewer delays? 
Ifit is possible, what is the minimum number of delays required to implement an equiv­
alent system? If it is not possible, explain why not. 
(b)  Does the System B shown in Figure P6.43-2 represent the same input-output relation­
ship as System A in Figure P6.43-1? Explain clearly. 
2 
: In/  
yIn]
<1 ','-'1 :'1> · 
-1  
Figure P6.43-2 
6.44.  Consider an all-pass system whose system function is 
..  

• 
• 
• 
483 
Chapter 6 
Problems 
(a)  Draw the direct form I signal flow graph for the system. How many delays and multi­
pliers do you need? (Do not count multiplying by ±1.) 
(b)  Draw a signal flow graph for the system that uses one multiplier. Minimize the number 
of delays. 
(c)  Now consider another all-pass system whose system function is 
Determine and draw a signal flow graph for the system with two multipliers and three 
delays. 
6.45. With infinite-precision arithmetic, the flow graphs shown in Figure P6.45 have the same 
system function, but with quantized fixed-point arithmetic they behave differently. Assume 
that a and b are real numbers and 0 < a < 1. 
x[n]  
b yIn] 
x[nJ b 
yIn] 
0 
0 
0  
0
!: Ir' • 
..! I"
a  
a 
Figure P6.4S 
(a)  Determine Xmax, the maximum amplitude of the input samples so that the maximum 
value of the output y [n ] of either of the two systems is guaranteed to be less than one. 
(b)  Assume that the above systems are implemented with two's-complement fixed-point 
arithmetic, and that in both cases all products are immediately rounded to B + 1 bits 
(before any additions are done). Insert round-off noise sources at appropriate locations 
in the above diagrams to model the rounding error. Assume that each of the noise 
sources inserted has average power equal to O"~ 
2-28 /12. 
(c)  If the products are rounded as described in part (b), the outputs of the two systems 
will differ; i.e., the output of the first system will be Yl [n] 
y[n] + h [n] and the 
output of the second system will be Y2[n] 
y[n] + !2[n], where h[n] and !2[n] are 
the outputs due to the noise sources. Determine the power density spectra «)flft (ejW ) 
and «) hh (ejW) of the output noise for both systems. 
(d) Determine the total noise powers a'll and al at the output for both systems. 
2 
6.46. An allpass system is to be implemented with fixed-point arithmetic. Its system function is 
(Z-l 
a*)(z-l - a)
H (z) =  ---........----:;­
(1-az-1)(1 
a*c1) 
where a = rejG • 
(a)  Draw the signal flow graphs for both the direct form I and direct form II implementa­
tions of this system as a 2nd-order system using only real coefficients. 

484 
Chapter 6 
Structures for Discrete-Time Systems 
(b)  Assuming that the products are each rounded before additions are performed, insert 
appropriate noise sources into the networks drawn in part (a), combining noise sources 
where possible, and indicating the power of each noise source in terms of ai, the power 
of a single rounding noise source. 
(c)  Circle the nodes in your network diagrams where overflow may occur. 
(d)  Specify whether or not the output noise power of the direct form II system is inde­
pendent of r, while the output noise power for the direct form I system increases as 
r ~ 1. Give a convincing argument to support your answer. Try to answer the question 
without computing the output noise power of either system. Of course, such a compu­
tation would answer the question, but you should be able to see the answer without 
computing the noise power. 
(e)  Now determine the output noise power for both systems. 
6.47. Assume that a in the flow graphs shown in Figure P6.47 is a real number and 0 < a < 1. 
Note that under infinite-precision arithmetic, the two systems are equivalent. 
--0 
0 
0
x~l. I : 1,-,' U " 
y[n] 
a 
Flow Graph #1 
x~  
y~ 
o  
• 
0
I : I,,' I : 1,-,' 
a 
Flow Graph #2  
Figure P6.47 
(a)  Assume that the two systems are implemented with two's-complement fixed-point 
arithmetic, and that in both cases all products are immediately rounded (before any 
additions are done). Insert round-off noise sources at appropriate locations in both 
flow graphs to model the rounding error (multiplications by unity do not introduce 
noise). Assume that each of the noise sources inserted has average power equal to 
ai = 2-2B /12. 
(b)  If the products are rounded as described in part (a), the outputs of the two systems will 
differ; i.e., the output of the first system will be Yl En] = yEn] + It [n] and the output of 
the second system will be Y2[n] = yEn] + hEn], where yEn] is the output owing to x[n] 
acting alone, and It En] and hEn] are the outputs owing to the noise sources. Determine 
the power density spectrum of the output noise <I>Illl (e jeo ). Also determine the total 
noise power of the output of flow graph #1; i.e., determine aJI' 
(c)  Without actually computing the output noise power for flow graph #2, you should be 
able to determine which system has the largest total noise power at the output. Give a 
brief explanation of your answer. 
II. 

485 
Chapter 6 
Problems 
6.48. Consider the parallel form flow graph shown in Figure P6.48 
0.5 
Zl 
x[n] 
0.8 
0.75 
-0.9 
yIn] 
Figure P6.48 
(a)  Assume that the system is implemented with two's-complement fixed-point arithmetic, 
and that all products (multiplications by 1 do not introduce noise) are immediately 
rounded (before any additions are done). Insert round-off noise sources at appropriate 
locations in the flow graph to model the rounding error. Indicate the size (average 
power) of each noise source in terms of cr~, the average power of one (B + I)-bit 
rounding operation. 
(b)  If the products are rounded as described in part (a), the output can be represented as 
.Y[n] 
y[n] + f[n] where y[n] is the output owing to x[n] acting alone, and fln] is the 
total output due to all the noise sources acting independently. Determine the power 
density spectrum of the output noise <I> fj(e jltJ ). 
(c)  Also determine the total noise power crj of the noise component of thc output. 
6.49. Consider the system shown in Figure P6.49, which consists of a 16-bit AID converter whose 
output is the input to an FIR digital filter that is implemented with I6-bit fixed-point arith­
metic. 
16-Bit AID Converter 
xc(t) 
Ideal 
CID 
Converter 
x[n] 
I----­
16·Bit 
Quantizer 
x[n] 
LTI 
System 
h[n], H(ejOJ) 
Y[n] 
t 
I 
T 
Figure P6.49 

486  
Chapter 6 
Structures for Discrete-Time Systems 
The impulse response of the digital filter is 
h(n] 
-.3758[n] + .758[n - 1] -
.375"[n - 2]. 
This system is implemented with 16-bit two's-complement arithmetic. The products are 
rounded to I6-bits before they are accumulated to produce the output. In anticipation of 
using the linear noise model to analyze this system, we define x[n] 
x[nJ + ern] and 
YlnJ = y[n] + f[nJ, where ern] is the quantization error introduced by the AID converter 
and f[n] is the total quantization noise at the output of the filter. 
(a)  Determine the maximum magnitude of x[n] such that no overflow can possibly occur 
in implementing the digital filter; i.e., determine Xmax such that Y[n] < 1 for a11-oo < 
n < 00 when x[n] < Xmax for all -00 < n < 00. 
(b)  Draw the linear noise model for the complete system (including the linear noise model 
of the AID). Include a detailed flow-graph for the digital filter including all noise 
sources due to quantization. 
(c)  Determine the total noise power at the output. Denote this a} 
(d)  Determine the power spectrum of the noise at the output of the filter; i.e., determine 
<4>ff(ej f1)). Plot your result. 
Extension Problems 
6.50. In this problem, we consider the implementation of a causal filter with system function 
1 
1 
H(z) = (1 
.63z-1)(1- .83z 1) 
1­
This system is to be implemented with (B + I)-bit two's-complement rounding arithmetic 
with products rounded before additions are performed. The input to the system is a zero­
mean, white, wide-sense stationary random process, with values uniformly distributed be­
tween -Xmax and +xmax. 
(a)  Draw the direct form flow graph implementation for the filter, with all coefficient 
multipliers rounded to the nearest tenth. 
(b)  Draw a flow graph implementation of this system as a cascade of two 1 st -order systems, 
with all coefficient multipliers rounded to the nearest tenth. 
(c)  Only one of the implementations from parts (a) and (b) above is usable. Which one? 
Explain. 
(d)  To prevent overflow at the output node, we must carefully choose the parameter Xmax. 
For the implementation selected in part (c), determine a value for Xmax that guarantees 
the output will stay between -1 and 1. (Ignore any potential overflow at nodes other 
than the output.) 
(e)  Redraw the flow graph selected in part (c), this time including linearized noise models 
representing quantization round-off error. 
(f)  Whether you chose the direct form or cascade implementation for part (c), there is 
still at least one more design alternative: 
(i)  If you chose the direct form, you could also use a transposed direct form. 
(ii)  If you chose the cascade form, you could implement the smaller pole first or the 
larger pole first. 
For the system chosen in part (c), which alternative (if any) has lower output quan­
tization noise power? Note you do not need to explicitly calculate the total output 
quantization noise power, but you must justify your answer with some analysis. 

487 
Chapter 6 
Problems 
6.51.  In this problem, we will develop some of the properties of a class of discrete-time systems 
called frequency-sampling filters. This class of filters has system functions of the form 
N-1 
­
H(z) = (1 - z-N). L H[k]jN , 
k=O 1­
where Zk = e](2rr/N)k for k = 0, 1, ... , N -1. 
(a)  System functions such as H(z) can be implemented as a cascade of an FIR system whose 
system function is (1 
with a parallel combination of1st-order IIR systems. Draw 
the signal flow graph of such an implementation. 
(b) Show that H(z) is an (N 
1)st-degree polynomial in 
. To do this, it is necessary to 
show that H(2) has no poles other than z 
0 and that it has no powers of z-l higher 
than (N -1). What do these conditions imply about the length of the impulse response 
of the system? 
(c) Show that the impulse response is given by the expression 
h[n] 
(~ ~ H[k1e}(2][IN)kn) (u[n]- urn - N]). 
k=O 
Hint: Determine the impulse responses of the FIR and the IIR parts of the system, 
and convolve them to find the overall impulse response. 
(d) Use I'Hopital's rule to show that 
m = 0, 1, ... , N 
1; 
-
i.e., show that the constants H[m] are samples of the frequency response of the system, 
H(e]W), at equally spaced frequencies Wm = (2:n:jN)m for m = 0.1, .... N - 1. It is 
this property that accounts for the name of this class of FIR systems. 
(e)  I!l general, both the poles Zk of the IlR part and the samples of the frequency response 
H[k] will be complex. However, if h[n] is real, we can find an implementation involving 
only real quantities. Specifically, show that if h[n] is real and N is an even integer, then 
H (z) can be expressed as 
z-N) {H(l)jN + H(-l)jN
H(z) = (1 
1-
1+z-1 
(N~-1 21H(eJ(2rr/N)k)1 
cos[8(2:n:kjN)] - z-l cos[8(2rrkjN) 2<kINl! ' 
+  L. 
. 
N  
1 
2cos(2rrkjN)Ci+C2 
k=l 
where H(eJW) = IH(e]W)le]O(w). Draw the signal flow graph representation of such a 
system when N = 16 and H(eJWk ) 
0 for k = 3, 4, .... 14. 
6.52. In Chapter 4, we showed that, in general, the sampling rate of a discrete-time signal can 
be redueed by a combination of linear filtering and time compression. Figure P6.52 shows 
a block diagram of an M-to-1 decimator that can be used to reduce the sampling rate by 
an integer factor M. According to the model, the linear filter operates at the high sampling 
rate. However, if M is large, most of the output samples of the filter will be discarded by 
the compressor. In some cases, more efficient implementations are possible. 
x[n] 
yIn] = w(Mn] 
Figure P6.52 

488  
Chapter 6 
Structures for Discrete-Time Systems 
(a)  Assume that the filter is an FIR system with impulse response such that h[n] = 0 for 
n < 0 and for n > 10. Draw the system in Figure P6.52, but replace the filter h[n] 
with an equivalent signal flow graph based on the given information. Note that it is not 
possible to implement the M-to-l compressor using a signal flow graph, so you must 
leave this as a box, as shown in Figure P6.52. 
(b) Note that some  of the branch operations can be commuted with the compression 
operation. Using this fact, draw the flow graph of a more efficient realization of the 
system of part (a). By what factor has the total number of computations required in 
obtaining the output y[n] been decreased? 
(c)  Now suppose that the filter in Figure P6.S2 has system function 
1 
1 
H(z) = 
1 
' 
Izl > 2· 
1- 'leI 
Draw the flow graph of the direct form realization of the complete system in the figure. 
With this system for the linear filter, can the total computation per output sample be 
reduced? If so, by what factor? 
(d) Finally, suppose that the filter in Figure P6.52 has system function 
1+ Ze I 
H(z) = 
~_1' 
Izl>~. 
1- 22 
Draw the flow graph for the complete system of the figure, using each of the following 
forms for the linear filter: 
(i) direct form I 
(ii)  direct form II 
(iii) transposed direet form I 
(iv) transposed direct form II. 
For which of the four forms can the system of Figure P6.S2 be more efficiently imple­
mented by commuting operations with the compressor? 
6.53. Speech production can be modeled by a linear system representing the vocal eavity, which is 
excited by puffs of air released through the vibrating vocal cords. One approach to synthe­
sizing speech involves representing the vocal cavity as a connection of cylindrical acoustic 
tubes of equal length, but with varying cross-sectional areas, as depicted in Figure P6.53. 
Let us assume that we want to simulate this system in terms of the volume velocity repre­
senting airflow. The input is coupled into the vocal tract through a small constriction, the 
vocal cords. We will assume that the input is represented by a change in volume velocity 
at the left end, but that the boundary condition for traveling waves at the left end is that 
the net volume velocity must be zero. This is analogous to an electrical transmission line 
driven by a current source at one end and with an open circuit at the far end. Current in the 
transmission line is then analogous to volume velocity in the acoustic tube, whereas voltage 
is analogous to acoustic pressure. The output of the acoustic tube is the volume velocity at 
the right end. We assume that each section is a lossless acoustic transmission line. 
Aj 
A2 
A3 
A4 
Figure P6.53 

489 
ms  
Chapter 6 
Problems 
for 
At each interface between sections, a forward-traveling wave f+ is transmitted to the next 
[n) 
section with one coefficient and reflected as a backward-traveling wave 
with a different 
lot 
coefficient. Similarly, a backward-traveling wave f- arriving at an interface is transmitted 
ust 
with one coefficient and reflected with a different coefficient. Specifically, if we consider a 
forward-traveling wave f+ in a tube with cross-sectional area A 1 arriving at the interface 
ion 
with a tube of cross-sectional area A 2, then the forward-traveling wave transmitted is 
the 
(1 + r) f+ and the reflected wave is rf+, where 
I in 
Consider the length of each section to be 3.4 cm, with the velocity of sound in air equal to 
34,000 cm/s. Draw a flow graph that will implement the four-section model in Figure P6.53, 
with the output sampled at 20,000 samples/so 
lTe.  
In spite of the lengthy introduction, this a reasonably straightforward problem. If
be 
you find it difficult to think in terms of acoustic tubes, think in terms of transmission-line 
sections with different characteristic impedances. Just as with transmission lines, it is difficult 
to express the impulse response in closed form. Therefore, draw the flow graph directly from 
physical considerations, in terms of forward- and backward-traveling pulses in each section. 
6.54.  In modeling the effects of round-off and truncation in digital filter implementations, quan­
tized variables are represented as 
x[n] 
Q[x[nJ] = xln] + ern], 
where Q[.J denotes either rounding or truncation to (B +1) bits and e[n] is the quantization 
error. We assume that the quantization noise sequence is a stationary white-noise sequence 
such that 
[{(eln] 
me)(e[n + m] - me)) = o} 8[m) 
and that the amplitudes of the noise sequence values are uniformly distributed over the 
quantization step II = 2-B .The 1 st-order probability densities for rounding and truncation 
are shown in Figures P6.54(a) and (b), respectively. 
1 pee) 
"A 
A 
e 
2 
pee) 
1
...-----1. "A 
-A 
e 
(a)  
(b) 
Figure P6.54 
(a) Determine the mean me and the variance o} for the noise owing to rounding. 
(b) Determine the mean me and the variance Cfl for the noise owing to truncation. 
6.55.  Consider an LTI system with two inputs, as depicted in Figure P6.55. Let hI [n) and h2[n] be 
the impulse responses from nodes 1 and 2, respectively, to the output, node 3. Show that if 
xl[n] and x2[n] are uncorrelated, then their corresponding outputs Yl [n j and Y2[n] are also 
uncorrelated. 

3 
xdn] 
1 
y[n] =Ydn] + Y2[n] 
Network 
Figure P6.55 
490  
Chapter 6 
Structures for Discrete-Time Systems 
x2[n]
2 
6.56.  The flow graphs in Figure P6.56 all have the same system function. Assume that the systems 
in the figure are implemented using (8 + I)-bit fixed-point arithmetic in all the compu­
tations. Assume also that all products are rounded to (8 + 1) bits before additions are 
performed. 
bo  
bo 
l
~[nl' ]',1":  
~[nl ' 
] 
I':, ] 'y[nl
]'Yln  
: 
a  
a
bl  
bl 
(a)  
(b) 
~[nl  ' "I 
~o I ' 1 " I,: y[~ 
a
bl 
(c) 
Figure P6.56 
(a)  Draw linear-noise models for each of the systems in Figure P6.56. 
(b)  Two of the flow graphs in Figure P6.56 have the same total output noise power owing to 
arithmetic round-off. Without explicitly computing the output noise power, determine 
which two have the same output noise power. 
(c)  Determine the output noise power for each of the flow graphs in Figure P6.56. Express 
your answer in terms of a~, the power of a single source of round-off noise. 
6.57.  The flow graph of a 1st-order system is shown in Figure P6.57. 
C>----* 
x[n] \=:J: y[~ 
Figure P6.57 
(a)  Assuming infinite-precision arithmetic, find the response of the system to the input 
! 
n > 0
x[n]= 
2' 
-,
{ 0, 
n < O.  
What is the response of the system for large n?  
II. 

491 
Chapter 6 
Problems 
Now suppose that the system is implemented with fixed-point arithmetic. The coef­
ficient and all variables in the flow graph are represented in sign-and-magnitude notation 
with 5-bit registers. That is, all numbers are to be considered signed fractions represented 
as 
bObl~b3b4' 
where bo. bl. b2. b3, and b4 are either 0 or 1 and 
IRegister value I == b1r l + b2r2 + b3r3 + b4Z-4. 
If bo 
0, the fraction is positive, and if bo = 1, the fraction is negative. The result of a 
multiplication of a sequence value by a coefficient is truncated before additions occur; Le., 
only the sign bit and the most significant four bits are retained. 
(b)  Compute the response of the quantized system to the input of part (a), and plot the 
responses of both the quantized and unquantized systems for 0 ~ n ~ 5. How do the 
responses compare for large n? 
(c)  Now consider the system depicted in Figure P6.57, where 
x[nl=={~(-l)lI, n:::O, 
0, 
n < O. 
Repeat parts (a) and (b) for this system and input. 
o 
• 
x[nJ 
Figure P6.57 
6.58. A causal LTI system has a system function  
1  
H(z) == ---.. ~-----;;­
1 
(a)  Is this system stable? 
(b)  If the coefficients are rounded to the nearest tenth, would the resulting system be 
stable? 
6.59. When implemented with infinite-precision arithmetic, the flow graphs in Figure P6.59 have 
the same system function. 
Network 1 
Z-l
Z-l 
Z-l 
Z·l 
Z~1 
Z~I 
Z-1 
o~_--< 
x[nJ I< I' : I" : I" : I" : I"~ : I"~ : I';  
y[n] 
Network 2  
Figure P6.59  
0 

492  
Chapter 6 
Structures for Discrete-Time Systems 
(a)  Show that the two systems have the same overall system function from input x[n] to 
output y[n]. 
(b)  Assume that the preceding systems are implemented with two's complement fixed­
point arithmetic and that products are rounded before additions are performed. Draw 
signal flow graphs that insert round-off noise sources at appropriate locations in the 
signal flow graphs of Figure P6.S9. 
(c)  Circle the nodes in your figure from part (b) where overflow can occur. 
(d) Determine the maximum size of the input samples such that overflow cannot occur in 
either of the two systems. 
(e)  Assume that lal < 1. Determine the total noise power at the output of each system, 
and determine the maximum value of lal such that Network 1 has lower output noise 
power than Network 2. 

7  
Filter Design 
Techniques 
7.0 INTRODUCTION 
Filters are a particularly important class of LTI systems. Strictly speaking, the term 
frequency-selective filter suggests a system that passes certain frequency components of 
an input signal and totally rejects all others, but in a broader context, any system that 
modifies certain frequencies relative to others is also called a filter. While the primary 
emphasis in this chapter is on the design of frequency-selective filters, some of the 
techniques are more broadly applicable. We concentrate on the design of causal filters, 
although in many contexts, filters need not be restricted to causal designs. Very often, 
noncausal filters are designed and implemented by modifying causal designs. 
The design of discrete-time filters corresponds to determining the parameters of a 
transfer function or difference equation that approximates a desired impulse response or 
frequency response within specified tolerances. As discussed in Chapter 2, discrete-time 
systems implemented with difference equations fall into two basic categories: infinite 
impulse response (UR) systems and finite impulse response (FIR) systems. Designing 
IIR filters implies obtaining an approximating transfer function that is a rational function 
of z, whereas designing FIR filters implies polynomial approximation. The commonly 
used design techniques for these two classes take different forms. Vlhen discrete-time 
filters first came into common use, their designs were based on mapping well-formulated 
and well-understood continuous-time filter designs to discrete-time designs through 
techniques such as impulse invariance and the bilinear transformation, as we will discuss 
in Sections 7.2.1 and 7.2.2. These always resulted in IIR filters and remain at the core 
of the design of frequency selective discrete-time IIR filters. In contrast, since there 
is not a body of FIR design techniques in continuous time that could be adapted to 
493 

494 
Chapter 7 
Filter Design Techniques 
the discrete-time case, design techniques for that class of filters emerged only after they 
became importan t in practical systems. The most prevalent approaches to designing FIR 
filters are the use of windowing, as we will discuss in Section 7.5 and the class of iterative 
algorithms discussed in Section 7.7 and collectively referred to as the Parks-McClellan 
algorithm. 
The design of filters involves the following stages: the specification of the desired 
properties of the system, the approximation of the specifications using a causal discrete­
time system, and the realization of the system. Although these three steps are certainly 
not independent, we focus our attention primarily on the second step, the first being 
highly dependent on the application and the third dependent on the technology to be 
used for the implementation. In a practical setting, the desired filter is generally im­
plemented with digital hardware and often used to filter a signal that is derived from a 
continuous-time signal by means of periodic sampling followed by AID conversion. For 
this reason, it has become common to refer to discrete-time filters as digital filters, even 
though the underlying design techniques most often relate only to the discrete-time 
nature of the signals and systems. The issues associated with quantization of filter coef­
ficients and signals inherent in digital representations is handled separately, as already 
discussed in Chapter 6. 
In this chapter, we will discuss a wide range of methods for designing both IIR 
and FIR filters. In any practical context, there are a variety of trade offs between these 
two classes of filters, and many factors that need to be considered in choosing a specific 
design procedure or class of filters. Our goal in this chapter is to discuss and illustrate 
some of the most widely used design techniques and to suggest some of the trade offs 
involved. The projects and problems on the companion website provide an opportunity 
to explore in more depth the characteristics of the various filter types and classes and 
the associated issues and trade offs. 
7.1 FILTER SPECIFICATIONS 
In our discussion of filter design techniques, we will focus primarily on frequency­
selective lowpass filters, although many of the techniques and examples generalize to 
other types of filters. Furthermore, as discussed in Section 7.4, lowpass filter designs are 
easily transformed into other types of frequency-selective filters. 
Figure 7.1 depicts the typical representation of the tolerance limits associated with 
approximating a discrete-time lowpass filter that ideally has unity gain in the passband 
and zero gain in the stopband. We refer to a plot such as Figure 7.1 as a "tolerance 
scheme." 
Since the approximation cannot have an abrupt transition from passband to stop­
band, a transition region from the passband edge frequency wp to the beginning of the 
stopband at Ws is allowed, in which the filter gain is unconstrained. 
Depending somewhat on the application, and the historical basis for the design 
technique, the passband tolerance limits may vary symmetrically around unity gain in 
which case 0Pl = 0P2' or the passband may be constrained to have maximum gain of 
unity, in which case 0Pl 
O. 

Section 7.1 
Filter Specifications 
495 
IH(eiw)1 
1-°,,2 .~~=~ \ 
I \  
I 
\ 
I  
I T \ . . 
I 
Passband I 
ransltIOn I 
Stopband 
I 
\ 
I 
I 
\ 
I  
\ 
I 
II 
\'-1_______ 
I 
" '-
Figure 7.1 
Lowpass filter tolerance 
o 
7r 
W 
scheme. 
Many of the filters used in practice are specified by a tolerance scheme similar 
to that which is presented below in Example 7.1, with no constraints on the phase 
response other than those imposed implicitly by requirements of stability and causality. 
For example, the poles of the system function for a causal and stable IIR filter must lie 
inside the unit circle. In designing FIR filters, we often impose the constraint of linear 
phase. This removes the phase of the signal from consideration in the design process. 
Example 7.1 
Determining Specifications for a Discrete-Time 
Filter 
Consider a discrete-time lowpass filter that is to be used to filter a continuous-time 
signal using the basic configuration of Figure 7.2. As shown in Section 4.4, if an LTI 
discrete-time system is used as in Figure 7.2, and if the input is bandlimited and the 
sampling frequency is high enough to avoid aliasing, then the overall system behaves 
as an LTI continuous-time system with frequency response 
H 
('Q) _ {H(eJrlT ), IQI < niT, 
(7.1a)
eff } 
-
0, 
IQI :::: niT. 
In such cases, it is straightforward to convert from specifications on the effective 
continuous-time filter to specifications on the discrete-time filter through the relation 
w = QT. That is, H (eJW) is specified over one period by the equation 
JW _ 
(. W)
H (e 
) -
Heff } T ' 
Iwl < n. 
(7.1 b) 
H(ei'o) 
DIe 
~ 
y[n) 
Ya (/) 
t 
T 
T 
Figure 7.2 
Basic system for discrete-time filtering of continuous-time signals. 

496 
Chapter 7 
Filter Design Techniques 
For this example, the overall system of Figure 7.2 is to have the following prop­
erties when the sampling rate is 104 samples/s (T =: 10-4 s): 
1.  The gain IHeffUQ) I should be within ±0.01 of unity in the frequency band 0 ::: 
Q :::: 2][(2000). 
2. The gain should be no greater than 0.001 in the frequency band 2][(3000) :::: Q. 
Since Eq. (7.1a) is a mapping between the continuous-time and discrete-time frequen­
cies, it only affects the passband and stopband edge frequencies and not the tolerance 
limits on frequency response magnitude. For this specific example, the parameters 
would be 
01'1  
01'2 = 0.01 
Os  = 0.001 
WI'  
OA][ radians 
Ws  
0.6][ radians. 
Therefore, in this case, the ideal passband magnitude is unity and is allowed to vary 
between (1 +8PI) and (1-01'2)' and the stopband magnitude is allowed to vary between 
oand lis. Expressed in units of decibels, 
ideal passband gain in decibels =: 2010glO (1) 
0 dB 
maximum passband gain in decibels 
2010g10(1.01) 
0.0864 dB 
minimum passband gain at passband edge in decibels 
20 log 10 (0.99) 
-0.873 dB 
maximum stopband gain in decibels 
2010g10CO.ool) =: -60 dB 
Example 7.1 was phrased in the context of using a discrete-time filter to process a 
continuous-time signal after periodic sampling. There are many applications in which 
a discrete-time signal to be filtered is not derived from a continuous-time signal, and 
there are a variety of means besides periodic sampling for representing continuous-time 
signals in terms of sequences. Also, in most of the design techniques that we discuss, the 
sampling period plays no role whatsoever in the approximation procedure. For these 
reasons, we take the point of view that the filter design problem begins from a set of 
desired specifications in terms of the discrete-time frequency variable w. Depending 
on the specific application or context, these specifications mayor may not have been 
obtained from a consideration of filtering in the framework of Figure 7.2. 
7.2  DESIGN OF DISCRETE-TIME IIR FILTERS FROM 
CONTINUOUS-TIME FILTERS 
Historically, as the field of digital signal processing was emerging, techniques for the 
design of discrete-time IIR filters relied on the transformation of a continuous-time 
filter into a discrete-time filter meeting prescribed specifications. This was and still is a 
reasonable approach for several reasons: 
• The art of continuous-time IlR filter design is highly advanced, and since useful 
results can be achieved, it is advantageous to use the design procedures already 
developed for continuous-time filters. 

497 
Section 7.2 
Design of Discrete-Time IIR Filters from Continuous-Time Filters 
• Many useful continuous-time IIR design methods have relatively simple closed­
form design formulas. Therefore, discrete-time IIR filter design methods based on 
such standard continuous-time design formulas are simple to carry out. 
• The standard approximation methods that work well for continuous-time IIR 
filters do not lead to simple closed-form design formulas when these methods are 
applied directly to the discrete-time IlR case, because the frequency response of 
a discrete-time filter is periodic, and that of a continuous-time filter is not. 
The fact that continuous-time filter designs can be mapped to discrete-time filter 
designs is totally unrelated to, and independent of, whether the discrete-time filter is to 
be used in the configuration of Figure 7.2 for processing continuous-time signals. We 
emphasize again that the design procedure for the discrete-time system begins from 
a set of discrete-time specifications. Henceforth, we assume that these specifications 
have been appropriately determined. We will use continuous-time filter approximation 
methods only as a convenience in determining the discrete-time filter that meets the 
desired specifications. Indeed, the continuous-time filter on which the approximation is 
based may have a frequency response that is vastly different from the effective frequency 
response when the discrete-time filter is used in the configuration of Figure 7.2. 
In designing a discrete-time filter by transforming a prototype continuous-time 
filter, the specifications for the continuous-time filter are obtained by a transformation 
of the specifications for the desired discrete-time filter. The system function H c(s) or 
impulse response hc(t) of the continuous-time filter is then obtained through one of 
the established approximation methods used for continuous-time filter design, such as 
those which are discussed in Appendix B. Next, the system function H(z) or impulse 
response h(n] for the discrete-time filter is obtained by applying to Hc(s) or hc(t) a 
transformation of the type discussed in this section. 
In such transformations, we generally require that the essential properties of the 
continuous-time frequency response be preserved in the frequency response of the 
resulting discrete-time filter. Specifically, this implies that we want the imaginary axis 
of the s-plane to map onto the unit circle of the z-plane. A second condition is that a 
stable continuous-time filter should be transformed to a stable discrete-time filter. This 
means that if the continuous-time system has poles only in the left half of the s-plane, 
then the discrete-time filter must have poles only inside the unit circle in the z-plane. 
These constraints are basic to all the techniques discussed in this section. 
7.2.1 Filter Design by Impulse Invariance 
In Section 4.4.2, we discussed the concept of impulse in variance, wherein a discrete­
time system is defined by sampling the impulse response of a continuous-time system. 
We showed that impulse invariance provides a direct means of computing samples of 
the output of a bandlimited continuous-time system for bandlimited input signals. In 
some contexts, it is particularly appropriate and convenient to design a discrete-time 
filter by sampling the impulse response of a continuous-time filter. For example, if the 
overall objective is to simulate a continuous-time system in a discrete-time setting, 
we might typically carry out the simulation in the configuration of Figure 7.2, with the 
discrete-time system design such that its impulse response corresponds to samples of the 

498 
Chapter 7 
Filter Design Techniques 
continuous-time filter to be simulated. In other contexts, it might be desirable to main­
tain, in a discrete-time setting, certain time-domain characteristics of well-developed 
continuous-time filters, such as desirable time-domain overshoot, energy compaction, 
controlled time-domain ripple, and so on. Alternatively, in the context of filter design, 
we can think of impulse invariance as a method for obtaining a discrete-time system 
whose frequency response is determined by the frequency response of a continuous­
time system. 
In the impulse invariance design procedure for transforming continuous-time fil­
ters into discrete-time filters, the impulse response of the discrete-time filter is chosen 
proportional to equally spaced samples of the impulse response of the continuous-time 
filter; i.e., 
h[n] = Tdhc(nTd), 
(7.2) 
where Td represents a sampling interval. As we will see, because we begin the design 
problem with the discrete-time filter specifications, the parameter Td in Eq. (7.2) in 
fact has no role whatsoever in the design process or the resulting discrete-time filter. 
However, since it is customary to specify this parameter in defining the procedure, we 
include it in the following discussion. Even if the filter is used in the basic configuration 
ofFigure 7.2, the design sampling period Td need not be the same as the sampling period 
T associated with the CID and DIC conversion. 
When impulse invariance is used as a means for designing a discrete-time filter 
with a specified frequency response, we are especially interested in the relationship 
between the frequency responses of the discrete-time and continuous-time filters. From 
the discussion of sampling in Chapter 4, it follows that the frequency response of the 
discrete-time filter obtained through Eq. (7.2) is related to the frequency response of 
the continuous-time filter by 
H(ejW ) f He (j!:!.... + j 2rr k) . 
(7.3)
Td 
Td
k=-oo 
If the continuous-time filter is bandlimited, so that 
HcU0.) = o. 
10.1::: rrjTd, 
(7.4) 
then 
jW
H(e
) 
He (j;), 
Iwl :s rr; 
(7.5) 
i.e., the discrete-time and continuous-time frequency responses are related by a linear 
scaling of the frequency axis, namely, w 
0.Td for Iwl < rr. Unfortunately, any practical 
continuous-time filter cannot be exactly bandlimited, and consequently, interference 
between successive terms in Eq. (7.3) occurs, causing aliasing, as illustrated in Figure 7.3. 
However, if the continuous-time filter approaches zero at high frequencies, the aliasing 
may be negligibly small, and a useful discrete-time filter can result from sampling the 
impulse response of a continuous-time filter. 
When the impulse invariance design procedure is used to utilize continuous-time 
filter design procedures for the design of a discrete-time filter with given frequency 
response specifications, the discrete-time filter specifications are first transformed to 
continuous-time filter specifications through the use of Eq. (7.5). Assuming that the 

499 
Section 7.2 
Design of Discrete-Time IIR Filters from Continuous-Time Filters 
~-~)-=-----w 
-2'1T 
21T 
w 
Figure 7.3 
Illustration of aliasing in the impulse invariance design technique. 
aliasing involved in the transformation from Hc(jD.) to H(eiw ) is negligible, we obtain 
the specifications on HcUD.) by applying the relation 
(7.6) 
to obtain the continuous-time filter specifications from the specifications on H(eiw). 
After obtaining a continuous-time filter that meets these specifications, the continuous­
time filter with system function H c(s) is transformed to the desired discrete-time filter 
v.ith system function H (z). We develop the algebraic details of the transformation from 
Hc(s) to H(z) shortly. Note, however, that in the transformation back to discrete-time 
frequency, H (eJW ) will be related to HAjD.) through Eq. (7.3), which again applies the 
transformation of Eq. (7.6) to the frequency axis. As a consequence, the "sampling" pa­
rameter Td cannot be used to control aliasing. Since the basic specifications are in terms 
of discrete-time frequency, if the sampling rate is increased (i.e., if Td is made smaller), 
then the cutoff frequency of the continuous-time filter must increase in proportion. In 
practice, to compensate for aliasing that might occur in the transformation from H cUD.) 
to H (e jW), the continuous-time filter may be somewhat overdesigned, i.e., designed to 
exceed the specifications, particularly in the stopband. 
While the impulse invariance transformation from continuous time to discrete 
time is defined in terms of time-domain sampling, it is easy to carry out as a transfor­
mation on the system function. To develop this transformation, we consider the system 
function of a causal continuous-time filter expressed in terms of a partial fraction ex­
pansion, so that1 
(7.7)  
The corresponding impulse response is 
IIIL Akeskt , 
t 2: 0, 
(7.8)
hc(t) 
k=l 
{ 0, 
t < O. 
1 For simplicity, we assume in the discussion that all poles of H(s) are single order. In Problem 7.41, 
we consider the modifications required for multiple-order poles. 

500  
Chapter 1 
Filter Design Techniques 
The impulse response of the causal discrete-time filter obtained by sampling Tdhc(t) is 
N 
h[n] 
Tdhc(nTd) 
LTdAkeSknTdu[n] 
k=l  
(7.9)
N 
= LTdAk(eSkTd)nu[n]. 
k=l 
The system function of the causal discrete-time filter is therefore given by 
N 
L 
TdAk
H(z)  
(7.10)
k=1 1 
eSkTdz-l' 
In comparing Eqs. (7.7) and (7.10), we observe that a pole at s = Sk in the s-plane 
transforms to a pole at z 
eSkTd in the z-plane and the coefficients in the partial fraction 
expansions of Hc(s) and H(z) are equal, except for the scaling multiplier Td. If the 
continuous-time causal filter is stable, corresponding to the real part of Sk being less than 
zero, then the magnitude of eSkTd will be less than unity, so that the corresponding pole in 
the discrete-time filter is inside the unit circle. Therefore, the causal discrete-time filter 
is also stable. Although the poles in the s-plane map to poles in the z-plane according 
to the relationship Zk = eSkTd , it is important to recognize that the impulse invariance 
design procedure does not correspond to a simple mapping of the s-plane to the z-plane 
by that relationship. In particular, the zeros in the discrete-time system function are a 
function ofthe poles eSk Td and the coefficients Td Ak in the partial fraction expansion, and 
they will not in general be mapped in the same way the poles are mapped, We illustrate 
the impulse invariance design procedure of a lowpass filter with the following example. 
Example 7.2 
Impulse Invariance with a Butterworth Filter 
In this example we consider the design of a lowpass discrete-time filter by applying 
impulse invariance to an appropriate continuous-time filter. The class of filters that we 
choose for this example is referred to as Butterworth filters, which we discuss in more 
detail in Section 7.3 and in Appendix B.2 The specifications for the discrete-time filter 
correspond to passband gain between 0 dB and -1 dB, and stopband attenuation of 
at least -15 dB, i.e., 
0.89125 S IH(ejU»! S 1, 
o s !wl S 0.27l', 
(7.11a) 
IH(ejU»1 s 0.17783, 
0.37l' s Iwl s 7l'. 
(7.11b) 
Since the parameter Td cancels in the impulse invariance procedure, we can just as 
well choose Id = 1, so that w = Q. In Problem 7.2, this same example is considered, 
but with the parameter Td explicitly included to illustrate how and where it cancels. 
In designing the filter using impulse in variance on a continuous-time Butter­
worth filter, we must first transform the discrete-time specifications to specifications 
on the continuous-time filter. For this example, we will assume that the effect of alias­
ing in Eq. (7.3) is negligible. After the design is complete, we can evaluate the resulting 
frequency response against the specifications in Eqs. (7.11a) and (7.11b). 
2Continuous-tirne Butterworth and Chebyshev filters are discussed in Appendix B. 

Section 7.2 
Design of Discrete-Time IIR Filters from Continuous-Time Filters 
501 
Because of the preceding considerations, we want to design a continuous-time 
Butterworth filter with magnitude function IHc(jQ)1 for which 
0.89125::: IHc(jQ)1 ::: 1, 
°::: IQI ::: O.2n, 
(7.12a) 
IHc(jQ)1 ::: 0.17783, 
0.3n ::: Inl ::: n. 
(7.12b) 
Since the magnitude response of an analog Butterworth filter is a monotonic function 
of frequency, Eqs. (7.12a) and (7.12b) will be satisfied if Hc(jO) 
1, 
IHc(j0.2n) I :.=0: 0.89125 
(7.13a) 
and 
IHc(j0.3n)1 ::: 0.17783. 
(7.13b) 
The magnitude-squared function of a Butterworth filter is of the form 
1 
(7.14)
1+ (n/Q c)2N' 
so that the filter design process consists of determining the parameters Nand Q c to 
meet the desired specifications. Using Eq. (7.14) in Eqs. (7.13) with equality leads to 
the equations 
0.21T )2N ( 
1 )2 
(7.15a)
1 + ( Q
= 0.89125
c 
and 
(7.15b)
1+ c~:rN = (0.1~783r 
The simultaneous solution of these two equations is N 
5.8858 and 
nc = 0.70474. The parameter N, however, must be an integer. In order that the 
specifications are met or exceeded, we must round N up to the nearest integer, N = 6, 
in which case the filter will not exactly satisfy both Eqs. (7.15a) and (7.15b) simulta­
neously. With N = 6, the filter parameter Q c can be chosen to exceed the specified 
requirements (i.e., have lower approximation error) in either the passband, the stop­
band, or both. Specifically, as the value of nc varies, there is a trade-off in the amount by 
which the stopband and passband specifications are exceeded. If we substitute N 
6 
into Eq. (7.15a), we obtain (nc = 0.7032). With this value, the passband specifications 
(of the continuous-time filter) will be met exactly, and the stopband specifications (of 
the continuous-time filter) will be exceeded. This allows some margin for aliasing in the 
discrete-time filter. With (Qc = 0.7032) and with N = 6, the 12 poles ofthe magnitude­
squared function H c(s)Hc(-s) = 1/[1 +(s/jQc)2N] are uniformly distributed in angle 
on a circle of radius (Qc = 0.7032), as indicated in Figure 7.4. Consequently, the poles 
of Hc(s) are the three pole pairs in the left half of the s-plane with the following 
coordinates: 
Pole pair 1: -0.182 ± j(0.679), 
Pole pair 2: -0.497 ± j (0.497), 
Pole pair 3: -0.679 ± j (0.182). 

502 
Chapter 7 
Filter Design Techniques 
7T
\j6;;:i 
s-plane 
\ 
1 
\ 
1 
_x- -x,
;' \ 
1 
"­
X 
\ 
1 
~  
I 
\ 
1 
/ 
\ I ,\13"Y \ 
X 
\ I 13Y 
>$
I 
, 
I 
'Re 
X
~ 
/
\ 
/
X 
)<
"­
;' 
.... X_ -x-
Figure 7.4 s-plane locations for poles of He Cs)Hc C-S) for 6th -order Butterworth 
filter in Example 7.2. 
Therefore, 
0.12093 
HeCs) 
(s2 + 0.3640s + 0.4945)(s2 + 0.9945s + 0.4945)(s2 + 1.3585s + 0.4945) . 
(7.16) 
Ifwe express H e(s) as a partiaifraction expansion, perform the transformation 
ofEq. (7.10), and then combine complex-conjugate terms, the resulting system function 
of the discrete-time filter is 
0.2871 - 0.4466z-1 
-2.1428 + 1.1455z-1  
H(z) = 1-1.2971z-1 +0.6949z-2 + -1-----:--+-0-.3-6-99-z-2  
(7.17) 
1.8557 - 0.6303z-1 
+ 1 - 0.9972z-1 + 0.2570C2· 
As is evident from Eq. (7.17), the system function resulting from the impulse invariance 
design procedure may be realized directly in parallel form. If either the cascade or 
direct form is desired, the separate 2nd-order terms are first combined in an appropriate 
way. 
The frequency-response functions of the discrete-time system are shown in Fig­
ure 7.5. The prototype continuous-time filter had been designed to meet the specifica­
tions exactly at the passband edge and to exceed the specifications at the stopband edge, 
and this turns out to be true for the resulting discrete-time filter. This is an indication 
that the continuous-time filter was sufficiently bandlimited so that aliasing presented 
no problem. Indeed, the difference between 2010glO IH(ejW)1 and 2010glO IHcUQ)1 
would not be visible on this plotting scale, except for a slight deviation around (V = jf. 
(Recall that Td 
1, so n = (V.) Sometimes, aliasing is much more of a problem. If the 
resulting discrete-time filter fails to meet the specifications because of aliasing, there 
is no alternative with impulse invariance but to try again with a higher-order filter or 
with different filter parameters, holding the order fixed. 

503 
Section 7.2 
Design of Discrete-Time IIR Filters from Continuous-Time Filters 
20r-----~------------~----------------_, 
Radian frequency (w) 
(a) 
1.2 
1.0 
0.8 
."" 
.; 0.6
-a 
S 
-( 0.4 
02 
0 
Radian frequency (w) 
(b) 
12  
10  
8  
~ 
'" 
0. 
6
8 
Vl '" 
4  
2  
0 
O.21T 
0.61T 
0.81T 
1T 
Radian frequency (w) 
(c) 
Figure 7.5 
Frequency response of 6th-order Butterworth filter transformed by 
impulse invariance. (a) Log magnitude in dB. (b) Magnitude. (c) Group delay. 

504 
Chapter 7 
Filter Design Techniques 
The basis for impulse invariance is to choose an impulse response for the discrete­
time filter that is similar in some sense to the impulse response of the continuous­
time filter. The use of this procedure may be motivated by a desire to maintain the 
shape of the impulse response or by the knowledge that if the continuous-time filter 
is bandlimited, consequently the discrete-time filter frequency response will closely 
approximate the continuous-time frequency response. When the primary objective is 
to control some aspect of the time response, such as the impulse response or the step 
response, a natural approach might be to design the discrete-time filter by impulse 
invariance or by step invariance. In the latter case, the response of the filter to a sampled 
unit step function is defined to be the sequence obtained by sampling the continuous­
time step response. If the continuous-time filter has good step response characteristics, 
such as a small rise time and low peak overshoot, these characteristics will be preserved 
in the discrete-time filter. Clearly, this concept of waveform invariance can be extended 
to the preservation of the output waveshape for a variety of inputs, as illustrated in 
Problem 7.1. The problem points out the fact that transforming the same continuous­
time filter by impulse invariance and also by step invariance (or some other waveform 
invariance criterion) does not lead to the same discrete-time filter in the two cases. 
In the impulse invariance design procedure, the relationship between continuous­
time and discrete-time frequency is linear; consequently, except for aliasing, the shape 
of the frequency response is preserved. This is in contrast to the procedure discussed 
next, which is based on an algebraic transformation. In concluding this subsection we 
iterate that the impulse invariance technique is appropriate only for bandlimited filters; 
highpass or bandstop continuous-time filters, for example, would require additional 
bandlimiting to avoid severe aliasing distortion if impulse invariance design is used. 
7.2.2 Bilinear Transformation 
The technique discussed in this subsection uses the bilinear transformation, an algebraic 
transformation between the variables sand z that maps the entire jS1-axis in the s-plane 
to one revolution of the unit circle in the z-plane. Since with this approach, -00 :::: S1 :::: 
00 maps onto -]'[ :::: W 
:::: ]'[, the transformation between the continuous-time and 
discrete-time frequency variables is necessarily nonlinear. Therefore, the use of this 
technique is restricted to situations in which the corresponding nonlinear warping of 
the frequency axis is acceptable. 
With He(s) denoting the continuous-time system function and H(z) the discrete­
time system function, the bilinear transformation corresponds to replacing s by
2(1- )
s 
1 
; 
(7.18)
Td 
1 +z­
that is, 
(2(1-Cl))
H(z) = He 
-
l' 
(7.19)
Td 
1 + Z-
As in impulse invariance, a "sampling" parameter Td is often included in the definition 
of the bilinear transformation. Historically, this parameter has been included, because 
the difference equation corresponding to H (z) can be obtained by applying the trape­
zoidal integration rule to the differential equation corresponding to Hc(s), with Td 

505 
Section 7.2 
Design of Discrete-Time IIR Filters from Continuous-Time Filters 
representing the step size of the numerical integration. (See Kaiser, 1966, and Prob­
lem 7.49.) However, in filter design, our use of the bilinear transformation is based 
on the properties of the algebraic transformation given in Eq. (7.18). As with impulse 
invariance, the parameter Td is of no consequence in the design procedure, since we 
assume that the design problem always begins with specifications on the discrete-time 
filter H (ejW). When these specifications are mapped to continuous-time specifications, 
and the continuous-time filter is then mapped back to a discrete-time filter, the effect of 
Td will cancel. We will retain the parameter Td in our discussion for historical reasons; 
in specific problems and examples, any convenient value can be chosen. 
To develop the properties of the algebraic transformation specified in Eq. (7.18), 
we solve for z to obtain 
1 + (Td/2)S 
(7.20)
z = 1 
(Td/2)s' 
and, substituting s = a + jQ into Eq. (7.20), we obtain 
1 + a Td/2 + jQTd/2
z  
(7.21)
1-
- jQTd/2 
If a <l, 0, then, from Eq. (7.21), it follows that Izi < 1 for any value of Q. Similarly, if 
a > 0, then Izi > 1 for all Q. That is, if a pole of Hc(s) is in the left-half s-plane, its 
image in the z-plane will be inside the unit circle. Therefore, causal stable continuous­
time filters map into causal stable discrete-time filters. 
Next, to show that the jQ-axis of the s-plane maps onto the unit circle, we substi­
tute s = jQ into Eq. (7.20), obtaining 
1 + jQTd/2 
(7.22)
z = 1 
j QTd/2' 
From Eq. (7.22), it is clear that Izl 
1 for all values of s on the jQ-axis. That is, the 
jQ-axis maps onto the unit circle, so Eq. (7.22) takes the form 
ejw =  1 + j QTd/2. 
(7.23) 
1 - jQTd/2 
To derive a relationship between the variables wand Q, it is useful to return to Eq. (7.18) 
jw
and substitute z = e
. We obtain 
2 (l-e-
jW 
) 
(7.24)
s = Td 
1 + e- jw 
' 
or, equivalently, 
.  
2 [2e- j (JJ/2(j sinw/2)] 
2j
s=a+JQ=-
-..........,..-=---­
-
tan(w/2). 
(7.25) 
Td 
2e-Jw/2 (cos w/2) 
Td 
Equating real and imaginary parts on both sides of Eq. (7.25) leads to the relations 
a 
oand 
2 
-
tan(w/2).  
(7.26)
Td 
or 
w 
2 arctan(QTd/2).  
(7.27) 

506 
Chapter 7 
Filter Design Techniques 
s-plane 
z-plane 
jfl 
Lm 
Image of 
s = jfl (unit circle) 
(J" 
R e 
Image of  
left half-plane  
Figure 7.6 
Mapping of the s-plane 
onto the z-plane using the bilinear 
transformation. 
w 
____________,J!:::a~~ (~)
L­
fl  
____----~_____1__ _ ____________ _ 
Figure 7.7 
Mapping of the  
- 7T 
continuous-time frequency axis onto the 
discrete-time frequency axis by bilinear 
transformation. 
These properties of the bilinear transformation as a mapping from the s-plane to 
the z-plane are summarized in Figures 7.6 and 7.7. From Eq. (7.27) and Figure 7.7, we 
see that the range of frequencies 0 :s [2 :s 00 maps to 0 :s w :s n, while the range 
-00 :s [2 :s 0 maps to -Tr :s w :s O. The bilinear transformation avoids the problem 
of aliasing encountered with the use of impulse invariance, because it maps the entire 
imaginary axis of the s-plane onto the unit circle in the z-plane. The price paid for this, 
however, is the nonlinear compression of the frequency axis, as depicted in Figure 7.7. 
Consequently, the design of discrete-time filters using the bilinear transformation is 
useful only when this compression can be tolerated or compensated for, as in the case 
of filters that approximate ideal piecewise-constant magnitude-response characteris­
tics. This is illustrated in Figure 7.8, wherein we show how a continuous-time frequency 
response and tolerance scheme maps to a corresponding discrete-time frequency re­
sponse and tolerance scheme through the frequency warping of Eqs. (7.26) and (7.27). 
If the critical frequencies (such as the passband and stopband edge frequencies) of the 
continuous-time filter are prewarped according to Eq. (7.26) then, when the continuous­
time filter is transformed to the discrete-time filter using Eq. (7.19), the discrete-time 
filter will meet the desired specifications. 
Although the bilinear transformation can be used effectively in mapping a piece­
wise-constant magnitude-response characteristic from the s-plane to the z-plane, the 
distortion in the frequency axis also manifests itself as a warping of the phase response 
of the filter. For example, Figure 7.9 shows the result of applying the bilinear transforma­
tion to an ideal linear-phase factor e- sa . If we substitute Eg. (7.18) for s and evaluate 
the result on the unit circle, the phase angle is -(2a/ Td ) tan(w/2). In Figure 7.9, the 

507 
Section 7.2 
Design of Discrete-Time IIR Filters frorn Continuous-Time Filters 
c: 
n 
I 
I 
I 
I 
.--.. .--.. 
I 
a""IN a"'IN
---- ----
I 
I 
,:3" 
,:3" 
I 
Nih'" NIh" 
\  
c:"'" 
cf 
\  
\ 
\ 
- - ------
/- c:'" -­
2' 
.",.; 
o 
~~--- - - c::" 
::t;" 
he 
Figure 7.8 
Frequency warping 
inherent in the bilinear transformation of 
a continuous-time lowpass filter into a 
discrete-time lowpass filter. To achieve 
to 
the desired discrete-time cutoff 
frequencies, the continuous-time cutoff 
frequencies must be prewarped as 
Ne 
ge 
IV 
indicated. 
:m 
.re 
lis, 
.7. 
solid curve shows the function -(2a/Td) tan(w/2), and the dotted curve is the periodic 
is 
linear-phase function -(waf Td), which is obtained by using the small-angle approxi­
Ise 
mation w/2 ~ tan(w/2). From this, it should be evident that if we desire a discrete-time 
is-
lowpass filter with a linear-phase characteristic, we cannot obtain such a filter by apply­
.cy 
ing the bilinear transformation to a continuous-time lowpassfilter with a linear-phase 
re-
characteristic. 
7). 
As mentioned previously, because of the frequency warping, the bilinear transfor­
he 
mation is most useful in the design of approximations to filters with piecewise-constant 
1S­
frequency magnitude characteristics, such as highpass, lowpass and bandpass filters. As 
ne 
demonstrated in Example 7.2, impulse invariance can also be used to design lowpass 
filters. However, impulse invariance cannot be used to map highpass continuous-time 
:e­
designs to highpass discrete-time designs, since highpass continuous-time filters are not 
he 
bandlimited. 
lse 
In Example 4.4, we discussed a class of filters often referred to as discrete-time 
differentiators. A significant feature of the frequency response of this class of filiers 
lte 
1a­
is that it is linear with frequency. The nonlinear warping of the frequency axis intro­
he 
duced by the bilinear transformation will not preserve that property. Consequently, the 

508 
27Ta 
Td 
Td 
-7T 
I 
I 
I 
Td 
Chapter 7 
Filter Design Techniques 
H(e iw ) 
I 
7Ta 
7T 
I 
a
I --w 
I 
Td 
--- ---< 
2a 
(w)
v------Ttan 2
27Ta L  
d 
Figure 7.9 
Illustration of the effect of the bilinear transformation on a linear­
phase characteristic. (Dashed line is linear phase and solid line is phase resulting 
from bilinear transformation.) 
bilinear transformation applied to a continuous-time differentiator will not result in a 
discrete-time differentiator. However, impulse invariance applied to an appropriately 
bandlimited continuous-time differentiator will result in a discrete-time differentia tor. 
7.3  DISCRETE-TIME BUTTERWORTH, CHEBYSHEV AND 
ELLIPTIC FILTERS 
Historically, the most widely used classes of frequency-selective continuous-time fil­
ters are those referred to as Butterworth, Chebyshev and elliptic filter designs. In Ap­
pendix B we briefly summarize the characteristics of these three classes of continuous­
time filters. The associated closed-form design formulas make the design procedure 
relatively straightforward. As discussed in Appendix B, the magnitude of the frequency 
response of a Butterworth continuous-time filter is monotonic in the passband and the 
stopband. A type I Chebyshev filter has an equiripple frequency response in the pass­
band and varies monotonically in the stopband. A type II Chebyshev filter is monotonic 
in the passband and equiripple in the stopband. An elliptic filter is equiripple in both the 
passband and the stopband. Clearly, these properties will be preserved when the filter 
is mapped to a digital filter with the bilinear transformation. This is illustrated by the 
dashed approximation shown in Figure 7.S. The filters resulting from applying the bilin­
ear transformation to these classes of continuous-time filters, referred to respectively as 
discrete-time Butterworth, Chebyshev and elliptic filters have similarly become widely 
used as discrete-time frequency selective filters. 

509 
Section 7.3 
Discrete-Time Butterworth, Chebyshev and Elliptic Filters 
As a first step in the design procedure for any of these classes of filters, the critical 
frequencies, Le., the band edge frequencies, must be prewarped to the continuous-time 
frequencies using Eq. (7.26) so that the frequency distortion inherent in the bilinear 
transformation will map the continuous-time frequencies back to the correct discrete­
time frequencies. This prewarping will be illustrated in more detail in Example 7.3. The 
allowed tolerances in the passbands and stopbands will be the same for the discrete-time 
and continuous-time filters since the bilinear mapping only distorts the frequency axis, 
not the amplitude scale. In using a discrete-time filter design package such as found in 
MATLAB and LabVIEW, the typical inputs would be the desired tolerances and the 
discrete-time critical frequencies. The design program explicitly or implicitly handles 
any necessary prewarping of the frequencies. 
In advance of illustrating these classes of filters with several examples, it is worth 
commenting on some general characteristics to expect. We have noted above that we 
expect the discrete-time Butterworth, Chebyshev and elliptic filter frequency responses 
to retain the mono tonicity and ripple characteristics of the corresponding continuous­
time filters. The Nth-order continuous-time lowpass Butterworth filter has N zeros at 
Q 
00. Since the bilinear transformation maps s = 00 to z = 
we would expect any 
Butterworth design utilizing the bilinear transformation to result in N zeros at z 
-1. 
The same is also true for the Chebyshev type I lowpass filter. 
7.3. I Examples of IIR Filter Design 
In the following discussion, we present a number of examples to illustrate IIR filter 
design. The purpose of Example 7.3 is to illustrate the steps in the design of a Butter­
worth filter using the bilinear transformation, in comparison with the use of impulse 
invariance. Example 7.4 presents a set of examples comparing the design of a Butter­
worth. Chebyshev I, Chebyshev II, and elliptic filter. Example 7.5 illustrates, with a 
different set of specifications, the design of a Butterworth, Chebyshev I, Chebyshev II 
and elliptic filter. These designs will be compared in Section 7.8.1 with FIR designs. For 
both Example 7.4 and 7.5 the filter design package in the signal processing toolbox of 
MATLAB was used. 
Example 7.3 
Bilinear Transformation of a Butterworth Filter 
Consider the discrete-time filter specifications of Example 7.2, in which we illustrated 
the impulse invariance technique for the design of a discrete-time filter. The specifica­
tions for the discrete-time filter are 
0.89125 ::; IH(ejW)1 ::; 1, 
0::; w ::; 0.211'. 
(7.28a) 
IH(ejW)1 
0.17783, 
0.311' ::; W ::; 11'. 
(7.28b) 

510 
Chapter 7 
Filter Design Techniques 
In carrying out the design using the bilinear transformation applied to a continuous­
time design, the critical frequencies of the discrete-time filter are first prewarped to 
the corresponding continuous-time frequencies using Eq. (7.26) so that the frequency 
distortion inherent in the bilinear transformation will map the continuous-time fre­
quencies back to the correct discrete-time critical frequencies. For this specific filter, 
with 1 HcUQ)1 representing the magnitude-response function of the continuous-time 
filter, we require that 
0.89125:::: IHcUQ)1 :::: 1, 
2 (O.2n) 
(7.29a) 
°:::: Q:::: 1d tan -2-
, 
2 
(o.3n)
IHcUQ)1 
0.17783, 
-tan --
< Q 
00. 
(7.29b)
Td 
2­
For convenience, we choose Td 
1. Also, as with Example 7.2, since a continuous-time 
Butterworth filter has a monotonic magnitude response, we can equivalently require 
that 
IHc U2tan(0.1rr»1 :::: 0.89125 
(7.30a) 
and 
IHdj2tan(0.15n»1 :::: 0.17783. 
(7.30b) 
The form of the magnitude-squared function for the Butterworth filter is 
H 
'Q 2 _ 
1 
(7.31)
I c(j )1 -1+(Q/Qd2N 
Solving for Nand Qc with the equality sign in Eqs. (7.30a) and (7.30b), we obtain 
2 
1 + ( 2 tan~~.ln) ) 2N 
(0.~9) 
(7.32a) 
and 
1 + (2tan(0.15n»)2N = (_1_)2 
(7.32b)
Qc 
0.178 
and solving for N in Eqs. (7.32a) and (7.32b) gives 
log [ (( 0.i78 )
2 
- 1) / ( (0.19/ 
1)] 
N = -.=....::--::--=--~c-:':-:--:-''---:-:-:--::---'-='. 
(7.33)
2Iog[tan(0.15n)I tan(0.1n)] 
5.305. 
Since N must be an integer, we choose N 
6. Substituting N = 6 into Eq. (7.32b), 
we obtain Q c 
0.766. For this value of Qc, the passband specifications are exceeded 
and the stopband specifications are met exactly. This is reasonable for the bilinear 
transformation, since we do not have to be concerned with aliasing. That is, with 
proper prewarping, we can be certain that the resulting discrete-time filter will meet 
the specifications exactly at the desired stopband edge. 

511 
Section 7.3 
Discrete-Time Butterworth, Chebyshev and Elliptic Filters 
In the s-plane, the 12 poles of the magnitude-squared function are uniformly 
distributed in angle on a circle of radius 0.766, as shown in Figure 7.10. 'The system 
function of the causal continuous-time filter obtained by selecting the left half-plane 
poles is 
0.20238 
Hc(s)=~-----------------~~------~----~--···-----------­
+ 1.48015 + 0.5871) 
(7.34) 
The system function for the discrete-time filter is then obtained by applying the bilinear 
transformation to H c(s) with Td 
1. lne result is 
0.0007378(1 + z-1)6
H(1:) 
(7.35) 
1 
x  ----------~--------=­
(1 
0.9044c1 +0.2155c2)·  
The magnitude, log magnitude, and group delay of the frequency response of the 
discrete-time filter are shown in Figure 7.11. At w = 0.211' the log magnitude is -0.56dB, 
and at w = 0.31l' the log magnitude is exactly -15 dB. 
Since the bilinear transformation maps the entire jr.!-axis of the s-plane onto 
the unit circle in the z-plane, the magnitude response of the discrete-time filter falls off 
much more rapidly than that of the continuous-time filter or the Butterworth discrete­
time filter designed by impulse invariance. In particular, the behavior of H (ejW ) at w = 
11' corresponds to the behavior of Hc(jr.!) at r.! 
00. Therefore, since the continuous­
time Butterworth filter has a 6th-order zero at s = 00, the resulting discrete-time filter 
has a 6th-order zero at z 
-1. 
7T 
~ 6 -....., 
s-plane
\ 
Im /
\ 
/ 
\ 
/ 
_x- -x 
--
\ 
/ " 
X 
\ 
/ 
'>\ 
I 
\ 
/ 
I 
\ /"1~ 
\ 
~ 
\ / ". 
~ 
X 
\ 
\ 
'x, 'x-
Figure 7.10 
s-plane locations for poles of Hc<s)Hc(-s) for 6th-order Butterworth 
filter in Example 7.3. 

512  
Chapter 7 
Filter Design Techniques 
20 
0  
-20  
-40  
;::!< 
" -60 
-80 
-1000 
0.211' 
0.41T 
0.61T 
0.81T 
1T 
Radian frequency (w) 
(a) 
1.2 
1.0 
0.8 
<l) 
" :a 
::l 
0.6 
S <  0.4 
0.20­
I 
~ 
I 
I 
0 
0.21T 
0.41T 
O.61T 
0.81T 
1T 
Radian frequency (w) 
(b) 
12 
8 
i 6 
'" 
<Zl 
o 
O.21T 
O.41T 
O.61T 
O.81T 
1T 
Radian frequency (w) 
(c) 
Figure 7.11 
Frequency response of 6th-order Butterworth filter transformed by 
bilinear transform. (a) Log magnitude in dB. (b) Magnitude. (c) Group delay. 

Section 7.3 
Discrete-Time Butterworth, Chebyshev and Elliptic Filters 
513 
Since the general form of the magnitude-squared of the Nth-order Butterworth 
continuous-time filter is as given by Eq. (7.31), and since w and Q are related by 
(7.26), it follows that the general Nth-order Butterworth discrete-time filter has 
the magnitude-squared function 
1 
IH(e jw )/2 = -------:;:-:-:-
(7.36) 
1+ 
where tan(wcf2) 
QcTd/2. The frequency-response function ofEq. (7.36) has the same 
properties as the continuous-time Butterworth response; i.e., it is maximally flat3 and 
IH(e jwC )12 = 0.5. However, the function in Eq. (7.36) is periodic with period 27r and 
falls off more sharply than the continuous-time Butterworth response. 
Discrete-time Butterworth filters are not typically designed directly by starting 
with Eq. (7.36), because it is not straightforward to determine the z-plane locations of 
the poles (all the zeros are at z = -1) associated with the magnitude-squared function of 
Eq. (7.36). It is necessary to determine the poles so as to factor the magnitude-squared 
function into H(z)H(z-l) and thereby determine H(z). It is much easier to factor the 
continuous-time system function, and then transform the left half-plane poles by the 
bilinear transformation as we did in Example 7.3. 
Equations of the form ofEq. (7.36) may also be obtained for discrete-time Cheby­
shev and elliptic filters. However, the details of the design computations for these com­
monly used classes offilters are best carried out by computer programs that incorporate 
the appropriate closed-form design equations. 
In the next example, we compare the design of a lowpass filter based on But­
terworth, Chebyshev I, Chebyshev II and elliptic filter designs. There are some specific 
characteristics of the frequency response magnitude and the pole-zero patterns for each 
of these four discrete-time lowpass filter types, and these characteristics will be evident 
in the designs in Example 7.4 and Example 7.5 that follow. 
For a Butterworth lowpass filter, the frequency response magnitude decreases 
monotonically in both the passband and stopband, and all the zeros of the transfer 
function are at z = -1. For a Chebyshev Type I lowpass filter, the frequency response 
magnitude will always be equiripple in the passband, i.e., will oscillate with equal max­
imum error on either side of the desired gain and will be monotonic in the stopband. 
All the zeros of the corresponding transfer function will be at z 
-1. For a Chebyshev 
TYpe II lowpass filter, the frequency response magnitude will be monotonic in the pass­
band and equiripple in the stopband, i.e., oscillates around zero gain. Because of this 
equiripple stopband behavior, the zeros of the transfer function will correspondingly 
be distributed on the unit circle. 
In both cases of Chebyshev approximation, the monotonic behavior in either the 
stopband or the passband suggests that perhaps a lower-order system might be obtained 
if an equiripple approximation were used in both the passband and the stopband. In­
deed, it can be shown (see Papoulis, 1957) that for fixed values of 0Pl' 0P2' OS, wP' and 
Ws in the tolerance scheme of Figure 7.1, the lowest order filter is obtained when the 
approximation error ripples equally between the extremes of the two approximation 
bands. This equiripple behavior is achieved with the class of filters referred to as elliptic 
3The first (2N 
1) derivatives of IH(ejW)12 are zero at w = O. 

514 
Chapter 7 
Filter Design Techniques 
filters. Elliptic filters, like the Chebyshev type II filter, has its zeros arrayed in the stop­
band region of the unit circle. These properties of Butterworth, Chebyshev, and elliptic 
filters are illustrated by the following example. 
Example 7.4 Design Comparisons 
For the four filter designs that follow, the signal processing toolbox in MATLAB was 
used. This and other typical design programs for IIR lowpass filter design, assume 
tolerance specifications as indicated in Figure 7.1 with 8 P1 
O. Although the resulting 
designs corrcspond to what would result from applying the bilinear transformation 
to appropriate continuous-time designs., any required frequency prewarping and in­
corporation of the bilinear transformation, are internal to these design programs and 
transparent to the user. Consequently the specifications are given to the design pro­
gram directly in terms of the discrete-time parameters. For this example, the filter has 
been designed to meet or exceed the following specifications.: 
passband edge frequency Q)p = O.5ir 
stopband edge frequency ills = O.6ir 
maximum passband gain 
0 dB 
minimum passband gain 
-0.3 dB 
maximum stopband gain 
-30 dB 
Referring to Figure 7.1, the corresponding passband and stopband tolerance limits are 
20 10gIO(1 + 0Pl) = 0 
or equivalently 0Pl 
o 
201og1O(l 
0P2) 
-0.3 or equivalently 0P2 
0.0339 
2010gIO(os) 
-30 
or equivalently Os 
0.0316. 
Note that the specifications are only on the magnitudes of the frequency re­
sponse. The phase is implicitly determined by the nature of the approximating func­
tions. 
Using the filter design program, it is determined that for a Butterworth design, 
the minimum (integer) filter order that meets or exceeds the given specifications is a 
15th-order filter. The resulting frequency response magnitUde, group delay, and pole­
zero plot are shown in Figure 7.12. As expected, all of the zeros of the Butterworth 
filter are at z = -1. 
For a Chebyshev type I design, the minimum filter order is 7. The resulting 
frequency response magnitude and group delay, and the corresponding pole-zero plot 
are shown in Figure 7.13. As expected, all of the zeros of the transfer function are 
at z = -1 and the frequency response magnitude is equiripple in the passband and 
monotonic in the stopband. 
For a Chebyshev type II design, the minimum filter order is again 7. The re­
sulting frequency response magnitude, group delay and pole-zero plot are shown in 
Figure 7.14. Again as expected, the frequency response magnitude is monotonic in 
the passband and equiripple in the stopband. The zeros of the transfer function are 
arrayed on the unit circle in the stopband. 
In comparing the Chebyshev I and Chebyshev II designs it is worth noting that 
for both, the order of the denominator polynomial in the transfer function corre­
sponding to the poles is 7, and the order of the numerator polynomial is also 7. In the 
implementation of the difference equation for both the Chebyshev I design and the 
Butterworth design, significant advantage can be taken of the fact that all the zeros 

515 
Section 7.3 
Discrete-Time Butterworth, Chebyshev and Elliptic Filters 
10r---------~--------~--~------r_--------~ 
O~----------------~ 
-10 
!g -20 
-30 
.............. .  
-40 
_~L-________~________ 
;~____~________
~~  
~ 
o  
Tr/4 
Tr/2  
Frequency, w  
(a) 
] 
Q 
jO.98 
<: 
0.96 
o 
Tr/4 
20 
., 
15 :
'" -a s 
'" 
10 
Vl 
5  
0  
0 
Trl4 
1T12 
Frequency, w 
(e) 
Tr/2 
Frequency, W 
(b) 
Im 
X 
X 
X 
X 
X 
X 
X 
X 
3."./4 
1T 
1r 
z-plane  
unit circle  
(d) 
Figure 7.12 Butterworth filter, 15th -order. 

516 
Chapter 7 
Filter Design Techniques 
10 
o 
-10 
~ -20 
-~ 
~Q 
-50~--------~----------~--~~--~----------~ 
o 
1T14 
1T12 
31T14 
1T 
Frequency, w 
(a) 
0.96 
0 
1T14 
............ --_. 
31T14 
1T
1T12 
Frequency, w 
(b) 
20  
15 
'" 
0.'" 
S 
10 
Vl '" 
5 
0 
0 
1T14 
1T12 
31T14 
1T 
Frequency, w 
(c) 
Im 
z-plane 
unit circle 
"­
IX 
X 
7th order zero "-
/ 
'Re 
X 
X 
(d) 
Figure 7.13 
Chebyshev Type I filter, 7th-order. 

517 
Section 7.3 
Discrete-Time Butterworth, Chebyshev and Elliptic Filters
iQues 
10,---------.----------.----------r---------, 
O~------------------~ 
-10 
~ -20 
-30 
--40 
-500~---------~~14----------v~12----L--L~~-U------~v 
Frequency, '" 
(a) 
] 
Q) 
~0.98 
« 
0.% 
o  
~14 
vl2 
3vl4 
7T 
Frequency, w 
(b) 
20 
~ 15 
l•• 
10 
'" 
5 
o~====~===-~------~----~
o  
7T14 
7T12 
3~ 
Frequency, '" 
(c) 
Im 
z-plane 
x 
unit circle 
x 
Re 
(d) 
Figure 7.14 
Chebyshev Type II filter, 7th-order. 

518  
Chapter 7 
Filter Design Techniques 
l:L-_____________ 
f
-10 
Eg  -20 
-30 
~ 
~OL---------~--------~~~~L---~--------~ 
o 
1T14 
Trl2 
37T/4 
Tr 
Frequency, w 
(a) 
0.96 
0 
i :f
§
en 
10 
5 
0 0 
Figure 7.15 
11"/4 
1T!4 
Trl2  
Frequency, w  
(b) 
1T12 
Frequency, w 
(c) 
3Trl4 
Tr 
3Trl4 
1T 
z-plane  
unit circle  
Im 
x 
x 
(d) 
Elliptic filter, 5th -order, exceeds design specifications. 

Section 7.3 
Discrete-Time Butterworth, Chebyshev and Elliptic Filters  
519 
occur at z = -1. This is not the case for the Chebyshev II filter. Consequently, in an 
implementation of the filter, the Chebyshev II design will require more multiplications 
than the Chebyshev I design. For the Butterworth design, while advantage can be 
taken of the clustered zeros at z 
-1, the filter order is more than twice that of the 
Chebyshev designs and consequently requires more multiplications. 
For the design of an elliptic filter to meet the given specifications, a filter of at 
least 5th-order is required. Figure 7.15 shows the resulting design. As with previous 
examples, in designing a filter with given specifications, the minimum specifications are 
likely to be exceeded, since the filter order is necessarily an integer. Depending on the 
application, the designer may choose which of the specifications to exactly meet and 
which to exceed. For example, with the elliptic filter design we may choose to exactly 
meet the passband and stopband edge frequencies and the passband variation and 
minimize the stopband gain. The resulting filter, which achieves 43 dB of attenuation 
in the stopband, is shown in Figure 7.16. Alternately, the added flexibility can be used 
to narrow the transition band or reduce the deviation from 0 dB gain in the passband. 
Again as expected, the frequency response of the elliptic filter is equiripple in both the 
passband and the stopband. 
10r---------,----------,----------,---------~ 
o~----------------~~ 
-10 
fi3 -20 
-30 
-40 
_50L-________~________~__~L___~~________~ 
o  
Trl4 
TrI2 
3Trl4 
Frequency. w 
(a) 
:::::::::::::: 
] 
~O.98 
<: 
... ~ ............. -. - . -- ------ --. ~ ..  
-"~ ". -................ .  
0.96 
o  
TrI"2  
Frequency. w  
(b) 
Figure 7.16 
Elliptic filter, 5th -order, minimizing the passband ripple. 
Example 7.5 
Design Example for Comparison with FIR 
Designs 
In this example we return to the specifications of Example 7.1 and illustrate the re­
alization of this filter specification with a Butterworth, Chebyshev I, Chebyshev II, 

520 
Chapter 7 
Filter Design Techniques 
20 
0 
-20 
-40 
r:tl 
-0 
-60 
-so 
-1000 
0.27T 
0.47T 
U.07T 
Radian frequency (If)) 
(a) 
1.010 
1.005F 
0) 
-0 
::l 
" 
:[1.0001­
E < 
09~[ 
I 
I
1
0.9900 
0.27T 
0.47T 
0.67T 
Radian frequency (If)) 
(b) 
25 
20 
U.1S7T 
7T 
I 
O.S7T 
7T 
en 
0) 
15 
-a 
E 
'" 
<;/) 
10 
5 
00 
0.27T 
0.47T 
0.67T 
O.S7T 
7T 
Radian frequency (If)) 
(c) 
Figure 7.17 
Frequency response of 14th-order Butterworth filter in Example 7.5. 
(a) Log magnitude in dB. (b) Detailed plot of magnitude in passband. (c) Group 
delay. 

521 
Section 7.3 
Discrete-Time Butterworth, Chebyshev and Elliptic Filters 
Im 
z-plane 
x 
Unit 
x 
/circle 
x 
x 
x 
x 
x 
x 
Re 
x 
x 
x 
x 
x 
zero 
x 
(d) 
Figure 7.17 
(continued) (d) Pole-zero plot of 14th-order Butterworth filter in 
Example 7.5. 
and elliptic designs. The designs are again carried out using the filter design program 
in the MATLAB signal processing toolbox. In Section 7.8.1 we will compare these 
IIR designs with FIR designs with the same specifications. Typical design programs 
for FIR filters require the passband tolerance limits in Figure 7.1 to be specified with 
.8pI = li p2, whereas for IIR filters, it is typically assumed that lipI = O. Consequently to 
carry out a comparison of IIR and FIR designs, some renormalization of the passband 
and stopband specifications may need to be carried out (see, for example, Problem 7.3), 
as will be done in Example 7.5. 
The lowpass discrete-time filter specifications as used for this example are: 
Iwl ::: O.4n, 
(7.37a) 
and 
0.6n ::: Iwl ::: n. 
(7.37b) 
In terms of the tolerance scheme of Figure 7.1, lip! = liP2 = 0.01, lis = 0.001, 
wp = O.4n, and Ws = 0.6n. Rescaling these specifications so that lipl = 0 corresponds 
to scaling the filter by 1/(1 + lipl) to obtain: lipl = 0, lip2 = 0.0198 and lis = .00099. 
The filters are first designed using the filter design program with these specifi­
cations and the filter designs returned by the filter design program are then rescaled 
by a factor of 1.01 to satisfy the specifications in Eqs. (7.37a) and (7.37b). 

522 
Chapter 7 
Filter Design Techniques 
20 
0 
-20 
CQ
"0 
-40 
-60 
-80 
-100 
0 
1.010 
0.21T 
0.41T 
0.61T 
Radian frequency (w) 
(a) 
0.81T 
1T 
1.005 
<!) 
"0 
;:l 
~ 
S 
~ 
].000 
0.995 
0.990 
0 
0.21T 
0.41T 
0.611" 
0.811" 
11" 
Radian frequency (w) 
(b)  
25  
20 
15
'"
<!) 
0.. 
S 
{/J '" 
10 
5 
0 
0 
0.21T 
0.41T 
0.61T 
0.81T 
11" 
Radian frequency (w) 
(c) 
Figure 7.18 
Frequency response of 8th -order Chebyshev type I filter in Exam­
ple 7.5. (a) Log magnitude in dB. (b) Detailed plot of magnitude in passband. 
(c) Group delay. 

Section 7.3 
Discrete-Time Butterworth, Chebyshev and Elliptic Filters 
523 
20 
0 
-20 
-40 
~ 
"0 
-60 
-so 
-100 0 
0.27T 
0.47T 
0.67T 
O.S7T 
7T 
Radian frequency (w) 
(a) 
1.010 
1.005 
O) 
"0 
B 
'3.1.000 
a 
<t: 
0.995 
0.990 
c-
-
-
I 
I 
I 
0 
0.27T 
0.47T 
0.67T 
O.S7T 
7T 
Radian frequency (w) 
(b) 
25 
20 
Vl 
15 
-a a 
en '" 
10 
5 
0 
Figure 7.19 
Frequency response of 8th -order Chebyshev type II filter in Exam­
ple 7.5. (a) Log magnitude in dB. (b) Detailed plot of magnitude in passband. 
(c) Group delay. 
0 
7T 
Radian frequency (w) 
(c) 

---
524 
Chapter 7 
Filter Design Techniques 
For the specifications in this example, the Butterworth approximation method 
requires a system of 14 th-order. The frequency response of the discrete-time filter that 
results from the bilinear transformation of the appropriate prewarped Butterworth fil­
teris shown in Figure 7.17. Figure 7.l7(a) shows the log magnitude in dB, Figure 7.17(b) 
shows the magnitude of H(ejW) in the passband only, and 
7.17(c) shows the 
group delay of the filter. From these plots, we see that as expected, the Butterworth 
frequency response decreases monotonically with frequency, and the gain of the filter 
becomes very small above about w 
O.7rr. 
Both Chebyshev designs I and II lead to the same order for a given set of specifi­
cations. For our specifications the required order is 8 rather than 14, as was required for 
the Butterworth approximation. Figure 7.18 shows the log magnitude, passband magni­
tude, and group delay for the type I approximation to the specifications of Eqs. (7.37a) 
and (7.37b). Note that as expected, the frequency response oscillates with equal max­
imum error on either side of the desired gain of unity in the passband. 
Figure 7.19 shows the frequency-response functions for the Chebyshev type II 
approximation. In this case, the equiripple approximation behavior is in the stopband. 
The pole-zero plots for the Chebyshev filters are shown in Figure 7.20. Note that the 
Chebyshev type I filter is similar to the Butterworth filter in that it has all eight of its 
zeros at z 
-1. On the other hand, the type II filter has its zeros arrayed on the unit 
circle. These zeros are naturally positioned by the design equations so as to achieve 
the equiripple behavior in the stopband. 
The specifications of Eqs. (7.37a) and (7.37b) are met by an elliptic filter of 
order six. This is the lowest order rational function approximation to the specifica­
tions. Figure 7.21 clearly shows the equiripple behavior in both approximation bands. 
Figure 7.22 shows that the elliptic filter, like the Chebyshev type II, has its zeros arrayed 
in the stopband region of the unit circle. 
Im 
z-plane 
X
8th-order / 
Unit
X 
zero 
I 
circle 
X 
X 
Re
X 
X 
I 
X 
(a) 
Figure 7.20 
Pole-zero plot of 8th -order Chebyshev filters in Example 7.5. (a) Type l. 
(b) Type II. 
Im 
z-plane 
x 
x 
(b) 

525 
Section 7.3 
Discrete-Time Butterworth, Chebyshev and Elliptic Filters 
20··· 
0  
-20  
-40  
il:i " 
-60 
-so 
-100 0 
Radian frequency (w) 
(a) 
1.010 
1.005 
'l) 
::! " 
'[1.000 
e 
~ 
0.995 
0.990 0 
0.41T
0.21T 
0.61T 
O.S1T 
1T 
Radian frequency (w) 
(b) 
25 
20 
'l) 
15
'" 
0.. a 
'" 
10
'" 
5 
00 
0.21T 
0.41T 
0.61T 
Radian frequency (w) 
(c) 
Figure 7.21 
Frequency response of 6th-order elliptic filter in Example 7.5. (a) Log 
magnitude in dB. (b) Detailed plot of magnitude in passband. (c) Group delay. 
O.S1T 
1T 

526 
Chapter 7 
Filter Design Techniques 
Im 
z-plane 
Unit 
circle 
x 
x 
ne 
x 
x 
Figure 7.22 
Pole-zero plot of 6th -order elliptic filter in Example 7.5. 
7.4  FREQUENCY TRANSFORMAnONS OF LOWPASS IIR 
FILTERS 
Our discussion and examples of IIR filter design have focused onthe design of frequency­
selective lowpass filters. Othertypes of frequency-selective filters such as highpass, band­
pass, bandstop, and multiband filters are equally important. As with lowpass filters, these 
other classes are characterized by one or several passbands and stopbands, each spec­
ified by passband and stopband edge frequencies. Generally the desired filter gain is 
unity in the passbands and zero in the stopbands, but as with lowpass filters, the filter 
design specifications include tolerance limits by which the ideal gains or attenuation in 
the pass- and stopbands can be exceeded. A typical tolerance scheme for a multiband 
filter with two passbands and one stopband is shown in Figure 7.23. 
war////11111
1+81~ 
181~ 
Wff$//Pd 
%;0"#////,,0'111
82 
W 
Wpl 
Wsl 
OJs2 
W p2 
1T 
Figure 7.23 
Tolerance scheme for amultiband filter. 

527 
Section 7.4 
Frequency Transformations of Lowpass IIR Filters 
The traditional approach to the design of many continuous-time frequency-select­
ive filters is to first design a frequency-normalized prototype lowpass filter and then, 
using an algebraic transformation, derive the desired filter from the prototype lowpass 
filter (see Guillemin, 1957 and Daniels, 1974). In the case of discrete-time frequency­
selective filters, we could design a continuous-time frequency-selective filter of the de­
sired type and then transform it to a discrete-time filter. This procedure would be accept­
able with the bilinear transformation, but impulse invariance clearly could not be used 
to transform highpass and bandstop continuous-time filters into corresponding discrete­
time filters because of the aliasing that results from sampling. An alternative procedure 
that works with either the bilinear transformation or impulse invariance is to design a 
discrete-time prototype lowpass filter and then perform an algebraic transformation on 
it to obtain the desired frequency-selective discrete-time filter. 
Frequency-selective filters of the lowpass, highpass, bandpass, and bandstop types 
can be obtained from a lowpass discrete-time filter by use of transformations very similar 
to the bilinear transformation used to transform continuous-time system functions into 
discrete-time system functions. To see how this is done, assume that we are given a 
lowpass system function Hlp(Z) that we wish to transform to a new system function 
H(z), which has either lowpass, highpass, bandpass, or bandstop characteristics when 
evaluated on the unit circle. Note that we associate the complex variable Z with the 
prototype lowpass filter and the complex variable z with the transformed filter. Then, 
we define a mapping from the Z-plane to the z-plane of the form 
(7.38)  
such that 
(7.39)  
Instead of expressing Z as a function of z, we have assumed in Eq. (7.38) that 
is 
expressed as a function of C 1• Thus, according to Eq. (7.39), in obtaining H(z) from 
Hlp(z) wc replace 
everywhere in Hlp(Z) by the function G(z-1 ). This is a convenient 
representation, because Hlp(Z) is normally expressed as a rational function of Z-1. 
If Hlp (Z) is the rational system function of a causal and stable system, we naturally 
require that the transformed system function H(z) be a rational function of C 1 and 
that the system also be causal and stable. This places the following constraints on the 
transformation Z-l 
0(Z-1): 
1.  O(Z-l) must be a rational function of C 1. 
2.  The inside of the unit circle of the Z-plane must map to the inside of the unit circle 
of the z-plane. 
3.  The unit circle of the Z-plane must map onto the unit circle of the z-plane. 

528 
Chapter 7 
Filter Design Techniques 
Let 0 and w be the frequency variables (angles) in the Z-plane and z-plane, re­
spectively, i.e., on the respective unit circles Z = eje and z = ejm• Then, for condition 3 
to hold, it must be true that 
= IG(e-jW)lejLG(e-jw), 
(7.40) 
and thus, 
IG(e-jm)1 = 1. 
(7.41) 
Therefore, the relationship between the frequency variables is 
-0 = LG(e-jm ). 
(7.42) 
Constantinides (1970) showed that the most general form of the function G(Z-l) 
that satisfies all the above requirements is 
N 
G(z-I) = ± n ---:: 
(7.43) 
k=l 1 
From our discussion of allpass systems in Chapter 5, it should be clear that G(z-I) as 
given in Eq. (7.43) satisfies Eq. (7.41), and it is easily shown that Eq. (7.43) maps the 
inside of the unit circle of the Z-plane to the inside of the unit circle of the z-plane if and 
only if lak I < 1. By choosing appropriate values for N and the constants ak, a variety of 
mappings can be obtained. The simplest is the one that transforms a lowpass filter into 
another lowpass filter with different passband and stopband edge frequencies. For this 
case, 
-1 
Z-l 
G(z- 1) = z 
- a 
(7.44)
1­
jm
If we substitute Z = eje and z = e
, we obtain 
(7.45) 
from which it follows that 
(1 
( 2) sin 0 
] 
(7.46)
w = arctan [ 2a + (1 + ( 2) cosO . 
This relationship is plotted in Figure 7.24 for different values of a. Although a warp­
ing of the frequency scale is evident in Figure 7.24 (except in the case a 
0, which 
corresponds to Z-1 = z-1), if the original system has a piecewise-constant lowpass 
frequency response with cutoff frequency 0p, then the transformed system will likewise 
have a similar lowpass response with cutoff frequency wp determined by the choice of a. 

w 
Figure 7.24 
Warping of the frequency 
8 
scale in lowpass-to-Iowpass 
transformation. 
TABLE 7.1 
TRANSFORMATIONS FROM A LOWPASS DIGITAL FILTER PROTOTYPE 
OF CUTOFF FREQUENCY Op TO HIGHPASS, BANDPASS, AND BANDSTOP FILTERS 
Filter Type 
Transformations 
Associated Design Formulas 
) as 
the 
and 
of 
into 
this 
.44) 
z-l -a 
Lowpass 
Z-l = 1-ac1 
z-l + a
Z-l =_
Highpass 
1 +acl 
Bandpass 
Bandstop 
sin 
a= 
sin 
Wp = desired cutoff frequency 
(ep+wp)
cos 
2 
0'=-
(0 -w )
cos ~ 
Wp = desired cutoff frequency 
tan (8;) 
Wpl = 
lower cutoff frequency  
Wp2 
desired upper cutoff frequency  
k 
tan (8;) 
Wpl 
lower cutoff frequency 
(.vp2 
desired upper cutoff frequency 
ofa. 
529 

530 
Chapter 7 
Filter Design Techniques 
Solving for a in terms of 8p and ('up, we obtain 
sin[(8p 
wp )/2]
a = ---'----"---
(7.47)
sin[(8p + wp )/2] 
Thus, to use these results to obtain a lowpass filter H (z) with cutoff frequency W p from an 
already available lowpass filter Hlp (Z) with cutoff frequency 8p, we would use Eq. (7.47) 
to determine a in the expression 
H(z) = Hlp(Z)1 
(7.48)
-a)/(1-ac1) . 
(Problem 7.51 explores how the lowpass-Iowpass transformation can be used to obtain 
a network structure for a variable cutoff frequency filter where the cutoff frequency is 
determined by a single parameter a.) 
Transformations from a lowpass filter to highpass, bandpass, and bandstop filters 
can be derived in a similar manner. These transformations are summarized in Table 7.1. 
In the design formulas, all of the cutoff frequencies are assumed to be between zero 
and Jf radians. The following example illustrates the use of such transformations. 
Example 7.6 
Transformation of a Lowpass Filter 
to a Highpass Filter 
Consider a Type I Chebyshev lowpass filter with system function 
Hlp(Z) 
(1 
0.001836(1 + Z-1)4 
1.5548Z-1 + 0.6493Z-2)(1- 1.4996Z-1 + 0.8482Z-2)· 
(7.49) 
This 4th-order system was designed to meet the specifications 
0.89125 ::: IHlp(ejO)1 ::: 1, 0::: 1:1 ::: 0.2:rr, 
(7.50a) 
IHlp(ejO)1 ::: 0.17783, 0.3:rr::: e ::::rr. 
(7.50b) 
The frequency response of this filter is shown in Figure 7.25. 
To transform this filter to a highpass filter with passband cutoff frequency 
wp = 0.6:rr, we obtain from Table 7.1 
a = _ cos [(0.2:rr + O.6Jf)/2] = -0.38197. 
(7.51)
cos [(0.2:rr - O.6:rr)/2] 
Thus, using thc lowpass-highpass transformation indicated in Table 7.1, we obtain 
H (2) = HIP(Z)lz-l=_[(CL O.38197)/(1-0.38197Z-1)] 
0.02426(1 - 2-1)4 
(7.52)
(1 + 1.0416C1 +0.40l9c2)(1 + O.5661C1 + O.7657C2) . 
The frequency response of this system is shown in Figure 7.26. Note that except for 
some distortion of the frequency scale, the high pass frequency response appears very 
much as if the lowpass frequency response were shifted in frequency by :rr. Also note 
that the 4th-order zero at Z = -1 for the lowpass filter now appears at 2 = 1 for 
the high pass filter. This example also verifies that the equiripple passband and stop­
band behavior is preserved by frequency transformations of this type. Also note that 
the group delay in Figure 7.26(c) is not simply a stretched and shifted version of Fig­
ure 7.25(c). This is because the phase variations are stretched and shifted, so that the 
derivative of the phase is smaller for the high pass filter. 

531 
Section 7.4 
Frequency Transformations of Lowpass IIR Filters 
20 
0 
-20 
r:t:l 
"0 -40 
-60 
-80 
-100 
0 
0.21T 
0.41T 
0.61T 
0.81T 
17  
Radian frequency (w)  
(a)  
1.2 
C) 
"0 
B 
=a e 
<t: 
1 
0.8 
0.6 
0.2 
0.217  
0.41T 
0.61T 
0.817 
1T 
Radian frequency (w) 
(b) 
16 
12 
C) '" 
-a 
8
e 
til'" 
4 
0 
0 
0.21T 
0.417 
0.61T 
0.81T 
17 
Radian frequency (w) 
(c) 
Figure 7.25 
Frequency response of 4th-order Chebyshev lowpass filter. (a) Log 
magnitude in dB. (b) Magnitude. (c) Group delay. 

532 
Chapter 7 
Filter Design Techniques 
20 
0 
-20 
!Xl 
"0 -40 
-60 
-80 
-100 
0 
0.211" 
0.411" 
O.fur 
0.811" 
71"  
Radian frequency (w)  
(a)  
1.2 
0.8 
0.6 
0.4 
0.2 
0 
0 
0.271" 
0.871" 
11"
0.411" 
O.fur 
Radian frequency (w) 
(b) 
16 
12 
'" 1 
8 
til 
4 
0 
0 
0.211" 
0.471" 
0.671" 
0.871" 
11" 
Radian frequency (w) 
(c) 
Figure 7.26 
Frequency response of 4th-order Chebyshev highpass filter obtained 
by frequency transformation. (a) Log magnitude in dB. (b) Magnitude. (c) Group 
delay. 

533 
Section 7.5 
Design of FIR Filters by Windowing 
7.5 DESIGN OF FIR FILTERS BY WINDOWING 
As discussed in Section 7.2, commonly used techniques for the design of IIR filters have 
evolved from applying transformations of continuous-time IIR systems into discrete­
time IIR systems. In contrast, the design techniques for FIR filters are based on directly 
approximating the desired frequency response or impulse response of the discrete-time 
system. 
The simplest method ofFIR filter design is called the window method. This method 
generally begins with an ideal desired frequency response that can be represented as 
Hd(e jW ) = L
00 
hd[n]e-jwn , 
(7.53) 
n=-oo 
where hd[n] is the corresponding impulse response sequence, which can be expressed 
in terms of Hd(e j (})) as 
1 111: 
".
H d(eJW)eJwndU). 
(7.54)
27l' -11: 
Many idealized systems are defined by piecewise-constant or piecewise-smooth fre­
quency responses with discontinuities at the boundaries between bands. As a result, 
these systems have impulse responses that are noncausal and infinitely long. The most 
straightforward approach to obtaining an FIR approximation to such systems is to trun­
cate the ideal impulse response through the process referred to as windowing. Equa­
tion (7.53) can be thought of as a Fourier series representation of the periodic frequency 
response Hd(eFJJ), with the sequence hd[n] playing the role of the Fourier coefficients. 
Thus, the approximation of an ideal filter by truncation of the ideal impulse response 
is identical to the issue of the convergence of Fourier series, a subject that has received 
a great deal of study. A particularly important concept from this theory is the Gibbs 
phenomenon, which was discussed in Example 2.18. In the following discussion, we will 
see how this effect of nonuniform convergence manifests itself in the design of FIR 
filters. 
A particularly simple way to obtain a causal FIR filter from hd[n] is to truncate 
hd[n], i.e., to define a new system with impulse response h[n] given by4 
0:5 n:5 M,
h[n] 
(7.55)
otherwise. 
More generally, we can represent h[n] as the product of the desired impulse response 
and a finite-duration "window" w[n]; i.e., 
h[n] = hd[n]w[n], 
(7.56) 
4The notation for FIR systems was established in Chapter 5. That is, M is the order of the system 
function polynomial. Thus, (M +1) is the length, or duration, of the impulse response. Often in the literature. 
N is used for the length of the impulse response of an FIRfilter; however, we have used N to denote the order 
of the denominator polynomial in the system function of an IIR filter. Thus, to avoid confusion and maintain 
consistency throughout this book, we will consider the length of the impulse response of an FIR filter to be 
(M + 1). 

534 
Chapter 7 
Filter Design TechniQues 
where, for simple truncation as in Eq. (7.55), the window is the rectangular window 
° 
1, 
~ n ~ M,
w[n]  
(7.57)
0, 
otherwise.
/ 
It follows from the modulation, or windowing, theorem (Section 2.9.7) that 
H(e j "')  ~ 11£ Hd (eP'I) W(e}(w-fJ»dB. 
(7.58)
21'( 
-IT 
That is, H(ejW) is the periodic convolution of the desired ideal frequency response 
with the Fourier transform of the window. Thus, the frequency response H(ejW) will 
be a "smeared" version of the desired response H d(e jW ). Figure 7.27(a) depicts typical 
functions H d(ejfJ ) and W(ej(w-fJ» as a function of B, as required in Eq. (7.58). 
If w[n] = 1 for all n (i.e., if we do not truncate at all),W(ejW ) is a periodic impulse 
train with period 21'(, and therefore, H(ejW) 
Hd(ejW). This interpretation suggests 
that if w[n] is chosen so that W(e jW) is concentrated in a narrow band of frequencies 
around w = 0, i.e., it approximates an impulse, then H(eiw ) will "look like" Hd(eiw ), 
except where Hd(ejW ) changes very abruptly. Consequently, the choice of window is 
governed by the desire to have w[n] as short as possible in duration, so as to minimize 
computation in the implementation of the filter, while having W{e jW) approximate an 
impulse; that is, we want W(ejW) to be highly concentrated in frequency so that the 
convolution of Eq. (7.58) faithfully reproduces the desired frequency response. These 
are conflicting requirements, as can be seen in the case of the rectangular window of 
Eq. (7.57), where 
. 
M 
. 
1 _ -jw(M+l) 
. 
W(e}"') ='" e-]wn = 
e 
_ 
_jwM/2 sm[w(M + 1)/2] 
(7.59)
~ 
1_·- e 
11=0 
• 
The magnitude of the function sin[w(M + 1)/2]/sin(w/2) is plotted in Figure 7.28 for 
the case M = 7. Note that W(ejW ) for the rectangular window has a generalized linear 
phase. As M increases, the width of the "main lobe" decreases. The main lobe is usually 
defined as the region between the first zero-crossings on either side of the origin. For 
the rectangular window, the width of the main lobe is !:l.wm = 41'(/(M + 1). However, 
for the rectangular window, the side lobes are large, and in fact, as M increases, the 
peak amplitudes of the main lobe and the side lobes grow in a manner such that the 
area under each lobe is a constant while the width of each lobe decreases with M. 
Consequently, as W(ei(w-fJ» "slides by" a discontinuity of Hd(eifJ ) as w varie~ the 
integral of W(ej(w-fJ»Hd(ejfJ ) will oscillate as each side lobe of W(ej(w-fJ» moves past 
the discontinuity. This result is depicted in Figure 7.27(b). Since the area under each 
lobe remains constant with increasing M, the oscillations occur more rapidly, but do not 
decrease in amplitude as M increases. 
In the theory of Fourier series, it is well known that this nonuniform convergence, 
the Gibbs phenomenon, can be moderated through the use of a less abrupt truncation 
of the Fourier series. By tapering the window smoothly to zero at each end, the height 
of the side lobes can be diminished; however, this is achieved at the expense of a wider 
main lobe and thus a wider transition at the discontinuity. 

es 
Section 7.5 
Design of FIR Filters by Windowing 
535 
7)  
(a) 
cal 
lse 
sts 
ies 
w 
W), 
IS 
ize 
Figure 7.27 
(a) Convolution process implied by truncation of the ideal impulse 
an 
response. (b) Typical approximation resulting from windowing the ideal impulse  
he 
response.  
(b) 
er, 
the 
:the 
iM. 
Ithe 
~ast 
ach 
:not 
lce, 
lion 
19ht 
der 
sin (w(M + 1)/2) I 
(M 7)
I 
sin (wI2)
8 
(M + 1) 
(M + 1) 
.1w
L 
Mainlobe
m 
I~ 
width 
w 
Figure 7.28 
Magnitude of the Fourier 
transform of a rectangular window 
(M = 7). 
7.5.1 Properties of Commonly Used Windows 
Some commonly used windows are shown in Figure 7.29. These windows are defined 
by the following equations: 
Rectangular 
1, o::s n :::: M,
w[n] = 
(7.60a)
{ 0, 
otherwise 

536 
Chapter 7 
Filter Design Techniques 
w[nl 
Rectangular 
I 
1.0 
0.8 
0.6 
0.4 
0 
.~ 
" 
~-"7/;" 
o~ 
I
~, / 
I
;f,l'~~=-'----
k 
2 
Hamming 
Hann 
Blackman 
Bartlett 
Figure 7.29 
Commonly used windows. 
Bartlett (triangular) 
2n/M, 
oS n S M /2, M even 
w[n]= 
2-2n/M, 
M/2 < n S M, 
(7.60b)
{ 0, 
otherwise 
Hann 
0.5 - 0.5cos(2nn/M), Os n S M,
w[nl = 
(7.60c)
{ 0, 
otherwise 
Hamming 
0.54 - 0.46 cos(2nn/M), oSn S M, 
w[n] = 
0, 
(7.60d)
{ 
otherwise 
Blackman 
0.42 - 0.5 cos(2nn/M) + O.08cos(4nn/M), Os n ::: M,
w[n] = 
(7.60e)
{ 0, 
otherwise 
(For convenience, Figure 7.29 shows these windows plotted as functions of a con­
tinuous variable; however, as specified in Eq. (7.60), the window sequence is defined 
only at integer values of n.) 
The Bartlett, Hann, Hamming, and Blackman windows are all named after their 
originators. The Hann window is associated with Julius von Hann, an Austrian meteor­
ologist. The term "hanning" was used by Blackman and Tukey (1958) to describe the 
operation of applying this window to a signal and has since become the most widely 
used name for the window, with varying preferences for the choice of "Hanning" or 
"hanning." There is some slight variation in the definition of the Bartlett and Hann 
windows. As we have defined them, w[O] = w[M] = 0, so that it would be reasonable 
to assert that with this definition, the window length is really only M - 1 samples. Other 

537 
Section 7.5 
Design of FIR Filters by Windowing 
definitions of the Bartlett and Hann windows are related to our definitions by a shift of 
one sample and redefinition of the window length. 
As will be discussed in Chapter 10, the windows defined in Eq. (7.60) are commonly 
used for spectrum analysis as well as for FIR filter design. They have the desirable 
property that their Fourier transforms are concentrated around w 
0, and they have a 
simple functional form that allows them to be computed easily. The Fourier transform of 
the Bartlett window can be expressed as a product of Fourier transforms of rectangular 
windows, and the Fourier transforms of the other windows can be expressed as sums of 
frequency-shifted Fourier transforms of the rectangular window, as given by 
(7.59). 
(See Problem 7.43.) 
The function 2010gIO IW(e jW ) Iis plotted in Figure 7.30 for each of these windows 
with M 
50. The rectangular window clearly has the narrowest main lobe, and thus, 
for a given length, it should yield the sharpest transitions of H(e jW ) at a discontinuity 
of Hd(e jW). However, the first side lobe is only about 13 dB below the main peak, 
resulting in oscillations of H (e jW ) ofconsiderable size around discontinuities of H d (e jW ). 
Table 7.2, which compares the windows ofEq. (7.60), shows that, by tapering the window 
smoothly to zero, as with the Bartlett, Hamming, Hann, and Blackman windows, the 
side lobes (second column) are greatly reduced in amplitude; however, the price paid 
is a much wider main lobe (third column) and thus wider transitions at discontinuities 
of Hd(ejW). The other columns of Table 7.2 will be discussed later. 
o~--·-~~~-~~~~~~~----------, 
fl -80 
_100L-~~~~~--~----~L-~~~------~ 
-20 
~ 
..s 
i 
-60 
o  
O.27T 
0.47T 
O.67T 
O.S7T 
7T 
Radian frequency (w) 
(a) 
-20 
~ 
.~ 
'" 
~  -40 
-60 
fl -SO 
Figure 7.30 
Fourier transforms (log
0.27T 
0.47T 
0.67T  
7T 
magnitude) of windows of Figure 7.29 
Radian frequency (w) 
with M = 50. (a) Rectangular. 
(b)  
(b) Bartlett. 

538 
Chapter 7 
Filter Design Techniques 
-80 
-loo! 
I 
I 
I 
, 
I 
I 
• 
\l) '" 
.f' 
'" 
~ 
~ 
o 
0.21T 
~ 
.~ 
~ 
~ 
~ 
0.21T 
0.41T 
0.61T 
O.S1T 
1T 
0.41T 
0.61T 
0.81T 
1T 
Radian frequency (w) 
(c) 
Radian frcquency (w) 
(d) 
0 
-20 
.f 
~ -40 
0 
,;0 -60 
..s 
0 
N 
-SO 
-100 0 
0.21T 
O.41T 
0.61T 
0.81T 
1T  
Radian frequency (w)  
Figure 7.30 (continued) (c) Hann. 
(e) 
(d) Hamming. (e) Blackman. 
7.5.2 Incorporation of Generalized Linear Phase 
In designing many types of FIR filters, it is desirable to obtain causal systems with a 
generalized linear-phase response. All the windows of Eq. (7.60) have been defined in 
anticipation of this need. Specifically, note that all the windows have the property that 
w[M 
n], 
O::s n ::s M,
w[n] 
(7.61)
{ 0, 
otherwise; 

539 
Section 7.5 
Design of FIR Filters by Windowing 
TABLE 7.2 
COMPARISON OF COMMONLY USED WINDOWS 
Peak 
Transition 
Peak 
Approximation 
Equivalent 
Width 
Side-Lobe 
Approximate 
Error, 
Kaiser 
of Equivalent 
Type of 
Amplitude 
Width of 
201oglO .5 
Window, 
Kaiser 
Window 
(Relative) 
Main Lobe 
(dB) 
f3 
Window 
Rectangular 
-13 
4n/(M + 1) 
-21 
0 
I.8hjM 
Bartlett 
-25 
8rr/M 
-25 
1.33 
2.37n/M 
Hann 
-3] 
8rr/M 
-44 
3.86 
5.01rrjM 
Hamming 
-41 
8n/M 
-53 
4.86 
6.27rr/M 
Blackman 
-57 
12n/M 
-74 
7.04 
9.19n/M 
i.e., they are symmetric about the point M 12. As a result, their Fourier transforms are 
of the form 
(7.62) 
where We(eiw ) is a real, even function of w. This is illustrated by Eq. (7.59). The conven­
tion of Eq. (7.61) leads to causal filters in general, and if the desired impulse response 
is also symmetric about M12, i.e., if hd[M - 111 = hd[n], then the windowed impulse 
response will also have that symmetry, and the resulting frequency response will have 
a generalized linear phase; that is, 
(7.63) 
where A e(eiw ) is real and is an even function of w. Similarly, if the desired impulse 
response is antisymmetric about M12, i.e., if hdM - n] = -hd[n], then the windowed 
impulse response will also be antisymmetric about M12, and the resulting frequency 
response will have a generalized linear phase with a constant phase shift of ninety 
degrees; i.e., 
(7.64) 
where Ao(eiw ) is real and is an odd function of w. 
Although the preceding statements are straightforward if we consider the product 
of the symmetric window with the symmetric (or antisymmetric) desired impulse re­
sponse, it is useful to consider the frequency-domain representation. Suppose 
hd[M - n] 
hd[I1]. Then, 
(7.65) 
where H e(eiw) is real and even. 
Ifthe window is symmetric, we can substitute Eqs. (7.62) and (7.65) into Eq. (7.58) 
to obtain 
a 
H(eiw ) = ~ /n He(eiB)e-)BM/2We(e)(w-B»)e-i(lt)-B)M/2dG. 
(7.66)
2n -n 
A simple manipulation of the phase factors leads to 
H(eiw) 
Ae(eiw)e-iwMj2, 
(7.67) 

540 
Chapter 7 
Filter Design Techniques 
where 
Ae(eJW ) = -2
1 iJr He(ejfJ)We(ej(w~f))dfj. 
(7.68) 
n 
-Jr 
Thus, we see that the resulting system has a generalized linear phase and. moreover, 
the real function A e(eJW ) is the result of the periodic convolution of the real functions 
He(eiw ) and We(eJW ). 
The detailed behavior of the convolution of Eq. (7.68) determines the magnitude 
response of the filter that results from windowing. The following example illustrates this 
for a linear-phase lowpass filter. 
Example 7.7 
Linea....Phase Lowpass Filter 
The desired frequency response is defined as 
. 
{-JWM j 2 
I:
H (elW) = 
e 
, 
w, < We, 
(7.69)
Ip 
0, 
We < Iwi :5 Jr. 
where the generalized linear-phase factor has been incorporated into the definition of 
the ideallowpass filter. The corresponding ideal impulse response is 
1 fl1!c 
-jwMj2 j(vnd 
_ sin[we(n 
MI2)]
}lip[]
II 
-_ 
e 
e 
,W -
(7.70)
2Jr 
-(ile 
n(f! 
M12) 
for-oo < n < oo.Itiseasilyshownthathlp[M 
n] = hlp[n], so if we use asymmetric 
window in the equation 
sin[wcCn - M /2)1 
11[/1] 
(7.71)
-n-(n---M-/2) 
w[n], 
then a linear-phase system will result. 
The upper part of Figure 7.31 depicts the character of the amplitude response 
that would result for all the windows of Eq. (7.60), except the Bartlett window, which is 
rarely used for filter design. (For M even, the Bartlett window would produce a mono­
tonic function Ae(ejW ), because We(ejW ) is a positive function.) The figure displays 
the important properties of window method approximations to desired frequency re­
sponses that have step discontinuities. It applies accurately when We is not close to 
zero or to Jr and when the width of the main lobe is smaller than 2we. At the bottom 
of the figure is a typical Fourier transform for a symmetric window (except for the 
linear phase). This function should be visualized in different positions as an aid in 
understanding the shape of the approximation A e(ej l1!) in the vicinity of We. 
When W 
We, the symmetric function We (ej(w-f)) is centered on the disconti­
nuity, and about one-half its area contributes to A e(ejW). Similarly, we can see that the 
peak overshoot occurs when We (ei (w-f))) is shifted such that the first negative side lobe 
on the right is just to the right of We. Similarly, the peak negative undershoot occurs 
when the first negative side lobe on the left is just to the left of We. This means that the 
distance between the peak ripples on either side of the discontinuity is approximately 
the main-lobe width t::,wm, as shown in Figure 7.31. The transition width t::,w as defined 
in the figure is therefore somewhat less than the main-lobe width. Finally, owing to the 
symmetry of We(ej(w-f)), the approximation tends to be symmetric around We; i.e., 
the approximation overshoots by an amount 8 in the passband and undershoots by the 
same amount in the stopband. 

Section 7.5 
Design of FIR Filters by Windowing 
541 
1 + 8 
--I 
HeCe iw ) 
1 - 8 
~ 
I 
I 
I 
0.5 
8 
w 
- 8 
Llwm~ 
e 
Figure 7.31 
Illustration of type of approximation obtained at a discontinuity of 
the ideal frequency response. 
The fourth column of Table 7.2 shows the peak approximation error at a discon­
tinuity (in dB) for the windows of Eq. (7.60). Clearly, the windows with the smaller 
side lobes yield better approximations of the ideal response at a discontinuity. Also, the 
third column, which shows the width of the main lobe, suggests that narrower transition 
regions can be achieved by increasing M. Thus, through the choice ofthe shape and du­
ration of the window, we can control the properties of the resulting FIR filter. However, 
trying different windows and adjusting lengths by trial and error is not a very satisfac­
tory way to design filters. Fortunately, a simple formalization of the window method has 
been developed by Kaiser (1974). 
7.5.3 The Kaiser Window Filter Design Method 
The trade-off between the main-lobe width and side-lobe area can be quantified by 
seeking the window function that is maximally concentrated around (V = 0 in the fre­
quency domain. The issue was considered in depth in a series of classic papers by Slepian 
et a1. (1961). The solution found in this work involves prolate spheroidal wave func­
tions, which are difficult to compute and therefore unattractive for filter design. How­
ever, Kaiser (1966, 1974) found that a near-optimal window could be formed using the 
zeroth-order modified Bessel function of the first kind, a function that is much easier to 

542 
Chapter 7 
Filter Design Techniques 
compute. The Kaiser window is defined as 
10['8(1 - [en - a)/a]2)1/2] 
O:sn:sM,
w[n] = 
10(fJ) 
, 
(7.72)
{ 0, 
otherwise, 
where a = M/2, and 10(-) represents the zeroth-order modified Bessel function of the 
first kind. In contrast to the other windows in Eqs. (7.60), the Kaiser window has two 
parameters: the length (M + 1) and a shape parameter fJ. By varying (M + 1) and fJ, 
the window length and shape can be adjusted to trade side-lobe amplitude for main­
lobe width. Figure 7.32(a) shows continuous envelopes of Kaiser windows of length 
M + 1 = 21 for fJ = 0,3, and 6. Notice from Eq. (7.72) that the case fJ 
0 reduces 
to the rectangular window. Figure 7 .32(b) shows the corresponding Fourier transforms 
of the Kaiser windows in Figure 7.32(a). Figure 7.32(c) shows Fourier transforms of 
Kaiser windows with fJ = 6 and M = 10,20, and 40. The plots in Figures 7.32(b) and (c) 
clearly show that the desired trade-off can be achieved. If the window is tapered more, 
the side lobes of the Fourier transform become smaller, but the main lobe becomes 
wider. Figure 7.32(c) shows that increasing M while holding fJ constant causes the main 
lobe to decrease in width, but it does not affect the peak amplitude of the side lobes. In 
fact, through extensive numerical experimentation, Kaiser obtained a pair of formulas 
that permit the filter designer to predict in advance the values of M and fJ needed to 
meet a given frequency-selective filter specification. The upper plot of Figure 7.31 is 
also typical of approximations obtained using the Kaiser window, and Kaiser (1974) 
found that, over a usefully wide range of conditions, the peak approximation error (8 
in Figure 7.31) is determined by the choice of fJ. Given that 8 is fixed, the passband 
cutoff frequency wp of the lowpass filter is defined to be the highest frequency such 
that IH(e jCtJ ) I 2: 1 - 8. The stopband cutoff frequency Ws is defined to be the lowest 
frequency such that IH (ejCtJ ) I:s 8. Therefore, the transition region has width 
!:::,.W 
Ws -
Wp 
(7.73) 
for the lowpass filter approximation. Defining 
A 
-2010g10 8, 
(7.74) 
Kaiser determined empirically that the value of fJ needed to achieve a specified value 
of A is given by 
0.l102(A 
8.7), 
A > 50, 
fJ = 
0.5842(A - 21)0.4 + 0.07886(A 
21), 
21:s A :s 50, 
(7.75) 
10.0, 
A < 21. 
(Recall that the case fJ = 0 is the rectangular window for which A 
21.) Furthermore, 
Kaiser found that to achieve prescribed values of A and l::!.w, M must satisfy 
A -8 
M 
(7.76)
2.285!:::,.w· 
Equation (7.76) predicts M to within ±2 over a wide range of values of!:::"w and A. Thus, 
with these formulas, the Kaiser window design method requires almost no iteration or 
trial and error. The examples in Section 7.6 outline and illustrate the procedure. 

1.2 .-----............. ----------------,  
........:,'Y'
/ 
0.9 -
•//
I 
Q) 
/ 
I
• 
I
"0 
B 
/ 
I
• 
I  
/ 
I  
~ 0.6­
• 
I
< 
/ 
I
• 
I 
0.3 -/ 
I
. 
/ 
/
/ 
...... / 
o  
5 
10 
15 
20 
Samples 
(a) 
O~.-----------------~ 
-25 
~ -50 
-75 
_100~--~-~~~~~-U-~~~~L--L­
o  
0.217 
0.417 
0.617 
0.817 
17 
Radian frequency (w) 
"',
\ .\  
(b) 
O~~=--------------------------, 
\ 
-25 
~ -50 
-75 
o 
. 
\•\r.
! ~'\\/l" "\ \ r1,r 
I , '\1V \r' 
I 1\ t
I 
I 
I
-100 '--__---'-----L__.L L_-'-L-LL-'---'-"'---'---L-'----'W 
0.217 
0.617 
0.817 
Radian frequency (w) 
(c) 
\ \I 
W' 
--f3=0 
---f3=3 
----f3=6 
--f3 0 
---f3 3 
----f3 6 
--M 10 
---M 20 
----M=40 
Figure 7.32 
(a) Kaiser windows for f3 = 0,3, and 6 and M 
20. (b) Fourier 
transforms corresponding to windows in (a). (c) Fourier transforms of Kaiser win­
dows with f3 = 6 and M = 10, 20, and 40. 
543 

544  
Chapter 7 
Filter Design Techniques 
Relationship ofthe Kaiser Window to Other Windows 
The basic principle of the window design method is to truncate the ideal impulse re­
sponse with a finite-length window such as one of those discussed in this section. The 
corresponding effect in the frequency domain is that the ideal frequency response is 
convolved with the Fourier transform of the window. Ifthe ideal filter is a lowpass filter, 
the discontinuity in its frequency response is smeared as the main lobe of the Fourier 
transform of the window moves across the discontinuity in the convolution process. 
To a first approximation, the width of the resulting transition band is determined by 
the width of the main lobe of the Fourier transform of the window, and the passband 
and stopband ripples are determined by the side lobes of the Fourier transform of the 
window. Because the passband and stopband ripples are produced by integration of the 
symmetric window side lobes, the ripples in the passband and the stopband are approx­
imately the same. Furthermore, to a very good approximation, the maximum passband 
and stopband deviations are not dependent on M and can be changed only by changing 
the shape of the window used. This is illustrated by Kaiser's formula, Eq. (7.75), for the 
window shape parameter, which is independent of M. The last two columns of Table 7.2 
compare the Kaiser window with the windows of Eqs. (7.60). The fifth column gives the 
Kaiser window shape parameter (13) that yields the same peak approximation error (8) 
as the window indicated in the first column. The sixth column shows the corresponding 
transition width [from Eq. (7.76)] for filters designed with the Kaiser window. This for­
mula would be a much better predictor of the transition width for the other windows 
than would the main-lobe width given in the third column of the table. 
In Figure 7.33 is shown a comparison of maximum approximation error versus 
transition width for the various fixed windows and the Kaiser window for different 
Approximation error vs. Transition width [* 
fixed windows, 0 = Kaiser (fJ 
integer)] 
-20 
-30 
~. -40 
~ 
l5 
::: -SO 
OJ 
§ 
.~ -60 
.§e 
E. -70 
c.. « 
-80 
-90 
-.::::  
Kaise;i·<Q.. 
* Bartlett  
K+tiser2"<l••• 
Kaiser3·~•• 
". 
Kaiser4 ~" * Hanning 
'. 
KaiserS't;jt Hamming 
Kai:;;;;*Q 
K~;:;;1'O ... 
* Blackman 
Kais;;!l'*Q .... 
Kaise~-o". 
o  
0.11/' 
0.21/' 
0.31/' 
0.417 
0.517 
Transition width (iliu) 
Figure 7.33 
Comparison offixed windows with Kaiser windows in alowpass filter 
design application (M = 32 and We = nj2). (Note that the designation "Kaiser 6" 
means Kaiser window with f3 
6, etc.) 

545 
Section 7.6 
Examples of FIR Filter Design by the Kaiser Window Method 
values of f3. The dashed line obtained from Eq. (7.76), shows that Kaiser's formula is 
an accurate representation of approximation error as a function of transition width for 
the Kaiser window. 
7.6  EXAMPLES OF FIR FILTER DESIGN BY THE KAISER 
WINDOW METHOD 
In this section, we give several examples that illustrate the use of the Kaiser window 
to obtain FIR approximations to several filter types including lowpass filters. These 
examples also serve to point out some important properties of FIR systems. 
7.6.1 Lowpass Filter 
With the use of the design formulas for the Kaiser window, it is straightforward to design 
an FIR lowpass filter to meet prescribed specifications. The procedure is as follows: 
1. First, the specifications must be established. This means selecting the desired wp 
and Ws and the maximum tolerable approximation error. For window design, the 
resulting filter will have the same peak error 0 in both the passband and the 
stopband. For this example, we use the same specifications as in Example 7.5, 
wp 
OAn, Ws 
0.6n, 01 = 0.01, and oz = 0.001. Since filters designed by the 
window method inherently have 01 = oz, we must set 0 = 0.001. 
2.  The cutoff frequency of the underlying ideallowpass filter must be found. Owing 
to the symmetry of the approximation at the discontinuity of Hd(ejW ), we would 
set 
Wp +Ws 
We = 
=0.5n.
2 
3. To determine the parameters of the Kaiser window, we first compute 
~W = Ws -
wp = 0.2n, 
A = -20 10glO 0 
60. 
We substitute these two quantities into Eqs. (7.75) and (7.76) to obtain the required 
values of f3 and M. For this example the formulas predict 
f3 = 5.653, 
M=37. 
4.  The impulse response of the filter is computed using Eqs. (7.71) and (7.72). We 
obtain 
sinwe(n - a) . Io[f3(1 
[(n 
a)la]z)1/z]
-"-'::........:---=-'-------=---=---.:;.. , 
0:::: n :::: M, 
h[n] = 
n(n - a) 
Io(f3) 
{ 0,  
otherwise, 
where a = MI2 = 3712 = 18.5. Since M = 37 is an odd integer, the resulting 
linear-phase system would be of type II. (See Section 5.7.3 for the definitions 
of the four types of FIR systems with generalized linear phase.) The response 
characteristics of the filter are shown in Figure 7.34. Figure 7.34(a), which shows 
the impulse response, displays the characteristic symmetry of a type II system. 

"'1/\/\'. 
0.6 ,,---------------------; 
0.4 
~ 
.2 
~ 
0.2 
< 
O•••••••••··z.  
.z··.........•••  
-0.2'~--------~--------~----------L---------~ 
o  
10 
20 
30 
40 
Sample number (n) 
(a) 
20,--------------------­
oI  
__ 
-20 
:g  
-40  
-60  
-80  
-100~'___~___~___~____L~~~~ 
o  
0.21T 
0.41T 
0.61T 
Radian frequency (w) 
(b) 
0.0010 r----------------------------, 
0.0005 :­
1
c..<E 
01-' 
I
,",,\'l'll 
-0.0005 
-0.0010 I  
" 
o  
0.21T 
O.41T 
0.67T 
Radian frequency (w) 
(c) 
Figure 7.34 
Response functions for the lowpass filter designed with a Kaiser 
window. (a) Impulse response (M = 37). (b) Log magnitude. (c) Approximation 
error for Ae(etv ). 
0.81T 
1T 
0.87T 
1T 
546 

Section 7.6 
Examples of FIR Filter Design by the Kaiser Window Method  
547 
Figure 7.34(b), which shows the log magnitude response in dB, indicates that 
H (ejW ) is zero at W = n or, equivalently, that H (z) has a zero at z = -1, as 
required for a type II FIR system. Figure 7.34(c) shows the approximation error 
in the passband and stop bands. This error function is defined as 
1 - Ae(ejW), 
0:::: W :::: wp , 
EA(W) = 
. 
(7.77) 
0- Ae(e]W), 
Ws:::: W :::: n.
1 
(The error is not defined in the transition region, O.4n < W < 0.6n.) Note the slight 
asymmetry of the approximation error, and note also that the peak approximation 
error is 8 = 0.00113 instead of the desired value of0.001. In this case it is necessary 
to increase M to 40 in order to meet the specifications. 
5.  Finally, observe that it is not necessary to plot either the phase or the group delay, 
since we know that the phase is precisely linear and the delay is M12 = 18.5 
samples. 
7.6.2 Highpass Filter 
The ideal highpass filter with generalized linear phase has the frequency response 
0
. 1,
Hhp(e]W) = 
-]'wMj2  
(7.78) 
e 
, 
We < Iwi :::: n. 
The corresponding impulse response can be found by evaluating the inverse transform 
of Hhp(ejW), or we can observe that 
Hhp(ejW ) = e-jwMj2 -
Hlp(ejU»,  
(7.79) 
where Hlp(e jW) is given by Eq. (7.69). Thus, hhp[n] is 
sinn(n - M12) 
sinweCn - M12) 
-00 < n < 00. 
(7.80)
hhp[n] = 
n(n _ M 12) 
n(n -
M 12) 
To design an FIR approximation to the highpass filter, we can proceed in a manner 
similar to that in Section 7.6.l. 
Suppose that we wish to design a filter to meet the highpass specifications 
IH(ejW)1 :::: 8z, 
Iwi :::: Ws 
where Ws = 0.35n, wp = 0.5n, and 81 = 82 = 8 = 0.02. Since the ideal response 
also has a discontinuity, we can apply Kaiser's formulas in Eqs. (7.75) and (7.76) with 
A = 33.98 and I:!.w = 0.15n to estimate the required values of f3 = 2.65 and M = 24. 
Figure 7.35 shows the response characteristics that result when a Kaiser window with 
these parameters is applied to hhp[n] with We = (0.35n + 0.5n)/2. Note that, since M 
is an even integer, the filter is a type I FIR system with linear phase, and the delay 
is precisely M 12 = 12 samples. In this case, the actual peak approximation error is 
8 = 0.0209 rather than 0.02, as specified. Since the error is less than 0.02 everywhere 
except at the stopband edge, it is tempting to simply increase M to 25, keeping f3 the 
same, thereby narrowing the transition region. This type II filter, which is shown in 
Figure 7.36, is highly unsatisfactory, owing to the zero of H(z) that is forced by the 

0.8 
0.6 
.g 
.B 
0.. 
8 
-< 
0.4 
0.2 
0 
-0.2 
-0.4 o 
10 
20 
Sample number (n) 
(a) 
30 
20,-----------------------------------------~ 
a:l 
"0 
-1000)'__~;:;;:_---;.~--~---.L.---­
0.21T 
0.41T 
0.61T 
0.81T 
1T 
Radian frequency (w) 
(b) 
0.04 ,------------------------------------------, 
"0" 
::> 
:0;:: 
]­
-< 
-0.02 
-0.04~'________~________L_______~_________L______~ 
o 
0.21T 
0.41T 
0.61T 
0.81T 
1T 
Radian frequency (w) 
(c) 
Figure 7.35 
Response functions for type I FIR highpass filter. (a) Impulse re­
sponse (M = 24). (b) Log magnitude. (c) Approximation error for Ae(ejCV ). 
548 

-
-
Q) 
.; 
"0 
0.. a 
~ 
0.6 
0.4 
0.2 
0 
-0.2 
-0.4 
-0.6 
0 
10 
20 
30  
Sample number (n)  
(a)  
20 
0 
-20 
~ -40
"0 
-so 
-100L-----~-------L-------L----~~-----­
o 
Radian frequency (w) 
(b) 
1.0 
O.S I­
Q) 
0.6 I­
.; 
"0 
-a 0.4 
a 
­
~ 
0.2 ­
0  
Figure 7.36 
Response functions for 
I 
I 
I 
I
-0.2 
type II FIR highpass filter. (a) Impulse
0 
0.21T 
0.41T 
0.61T 
O.S1T 
response (M = 25). (b) Log magnitude
Radian frequency (w) 
of Fourier transform. (c) Approximation 
(c) 
error for Ae(ejW ). 
549 

550 
Chapter 7 
Filter Design Techniques 
IHmb(ejW)1  
Nmb=4 
GIn 
G2f­
G4 f-
I 
Figure 7.37 
Ideal frequency response 
("'3
WI 
W2 
7T 
W 
for multiband filter. 
linear-phase constraint to be at z 
-1, i.e., W = JT. Although increasing the order by 1 
leads to a worse result, increasing M to 26 would, of course, lcad to a type I system that 
would exceed the specifications. Clearly, type II FIR linear-phase systems are generally 
not appropriate approximations for either highpass or bandstop filters. 
The previous discussion of high pass filter design can be generalized to the case 
of multiple passbands and stop bands. Figure 7.37 shows an ideal multiband frequency­
selective frequency response. This generalized multi band filter includes lowpass, high­
pass, bandpass, and bandstop filters as special cases. If such a magnitude function is 
jwM/ 2
multiplied by a linear-phase factor e-
, the corresponding ideal impulse response 
is 
Nmb  
2:)Gk - Gk+l) sinwk(n - M/2) 
hmb[n] 
(7.81) 
k=l 
JT(n - M/2) 
, 
where Nmb is the number of bands and GNmb+l = O. If hmb[n] is multiplied by a Kaiser 
window, the type of approximations that we have observed at the single discontinuity of 
the lowpass and highpass systems will occur at each of the discontinuities. The behavior 
will be the same at each discontinuity, provided that the discontinuities are far enough 
apart. Thus, Kaiser's formulas for the window parameters can be applied to this case to 
predict approximation errors and transition widths. Note that the approximation errors 
will be scaled by the size of the jump that produces them. That is. if a discontinuity of 
unity produces a peak error of 0, then a discontinuity of one-half will have a peak error 
of 0/2. 
7.6.3 Discrete-Time Differentiators 
As illustrated in Example 4.4, sometimes it is ofinterest to obtain samples of the deriva­
tive of a bandlimited signal from samples of the signal itself. Since the Fourier transform 
of the derivative of a continuous-time signal is jQ times the Fourier transform of the 
signal, it follows that, for bandlimited signals, a discrete-time system with frequency re­
sponse (jW / T ) for -JT < W < JT (and that is periodic, with period 2JT) will yield output 
samples that are equal to samples of the derivative of the continuous-time signal. A 
system with this property is referred to as a discrete-time differentia tor. 

1 
Section 7.6 
Examples of FIR Filter Design by the Kaiser Window Method 
551 
For an ideal discrete-time differentiator with linear phase, the appropriate fre­
quency response is 
-Jr < W < Jr. 
(7.82) 
(We have omitted the factor 1/T.) The corresponding ideal impulse response is 
cosJr(n 
M/2) 
sin Jr(n 
M /2) 
-00 < n < 00. 
(7.83)
hdiff[n] = 
(n 
M/2) 
Jr(n 
M/2)2' 
If hdiff [n] is multiplied by a symmetric window of length (M +1), then it is easily shown 
that hEn] 
-hEM - nJ. Thus, the resulting system is either a type III or a type IV 
generalized linear-phase system. 
Since Kaiser's formulas were developed for frequency responses with simple mag­
nitude discontinuities, it is not straightforward to apply them to differentia tors, wherein 
the discontinuity in the ideal frequency response is introduced by the phase. Neverthe­
less, as we show in the next example, the window method is very effective in designing 
such systems. 
Kaiser Window Design ofa Differentiator 
To illustrate the window design of a differentiator, suppose M = lO and f3 = 2.4. The 
resulting response characteristics are shown in Figure 7.38. Figure 7.38(a) shows the 
antisymmetric impulse response. Since M is even, the system is a type III linear-phase 
system, which implies that H(z) has zeros at both z = +1 (w 
0) and z = -1 (w 
Jr). 
This is clearly displayed in the magnitude response shown in Figure 7.38(b). The phase 
is exact, since type III systems have a Jr/2-radian constant phase shift plus a linear phase 
corresponding in this case to M /2 = 5 samples delay. Figure 7 .38( c) shows the amplitude 
approximation error 
o ~ W ~ 0.8Jr, 
(7.84) 
where Ao(ejW) is the amplitude of the approximation. (Note that the error is large 
around W 
Jr and is not plotted for frequencies above W = 0.8Jr.) Clearly, the linearly 
increasing magnitude is not achieved over the whole band, and, obviously, the relative 
error (i.e., Ediff(W)/W) is very large for low frequencies or high frequencies (around 
W 
rr). 
Type IV linear-phase systems do not constrain H (z) to have a zero at z = -1. 
This type of system leads to much better approximations to the amplitude function, as 
shown in Figure 7.39, for M = 5 and f3 
2.4. In this case, the amplitude approximation 
error is very small up to and beyond W = O.8rr. The phase for this system is again a Jr/2­
radian constant phase shift plus a linear phase corresponding to a delay of M /2 
2.5 
samples. This noninteger delay is the price paid for the exceedingly good amplitude 
approximation. Instead of obtaining samples of the derivative of the continuous-time 
signal at the original sampling times t 
nT, we obtain samples of the derivative at 
times t = (n - 2.5)T. However, in many applications, this noninteger delay may not 
cause a problem, or it could be compensated for by other noninteger delays in a more 
complex system involving other linear-phase filters. 

2 
<!) 
"0 
.8 
:.s.. 
0 
~ 
-J 
-2 o 
2 
4 
6 
8 
10 
Sample number (n) 
(a) 
4 
3 
<!) 
"0 
.B 
0.. a 
<t: 
2 
1 
0.21T 
0.41T 
0.61T 
0.81T 
1T 
Radian frequency (w) 
(b) 
0.2 
0.1 
<!) 
"0 
.B 
-s.. a 
<t: 
-0.1 
I 
Figure 7.38 
Response functions for 
-0.2' 
type III FIR discrete-time differentiator. 
0 
0.21T 
0.41T 
0.61T 
0.81T 
1T 
(a) Impulse response (M = 10).
Radian frequency (w) 
(b) Magnitude. (c) Approximation error 
(c) 
for Ao(ejW ). 
552 

2 
Q) 
,;
"0 
0.. 
0 
8 
<t: 
-1 
-2 
l-
l-
• 
I 
I 
~ 
t 
I 
o  
1 
2 
3 
4 
5 
Sample number (n) 
(a) 
4,-----------------------------------, 
3 
0.27T 
0.47T 
0.67T 
O.S7T 
7T 
Radian frequency (w) 
(b)  
O.OS  
0.06 
Q) 
0.04 
.; 
"0 
0.. 0.02 
8 
<t: 
0  
-0.02  
Figure 7.39 
Response functions for 
-0.04 
type IV FIR discrete-time differentiator. 
0 
0.27T 
0.47T 
0.67T 
O.S7T 
7T 
(a) Impulse response (M = 5).
Radian frequency (w) 
(b) Magnitude. (c) Approximation error 
(c)  
for Ao(ejCV ). 
553 

554  
Chapter 7 
Filter Design Techniques 
7.7 OPTIMUM APPROXIMATIONS OF FIR FILTERS 
The design of FIR filters by windowing is straightforward and is quite general, even 
though it has a number of limitations as discussed below. However, we often wish to 
design a filter that is the "best" that can be achieved for a given value of M. It is 
meaningless to discuss this question in the absence of an approximation criterion. For 
example, in the case of the window design method, it follows from the theory of Fourier 
series that the rectangular window provides the best mean-square approximation to a 
desired frequency response for a given value of M. That is, 
h[nJ 
{hd[nJ, 
0 ~ n ~ M,  
(7.85)
0, 
otherWIse, 
minimizes the expression 
1  In 
. 
. 2 
=  -
IHd(e]W) -
H(e]W)1 dw. 
(7.86)
2][ 
-rr 
(See Problem 7.25.) However, as we have seen, this approximation criterion leads to 
adverse behavior at discontinuities of H d(ejW). Furthermore, the window method does 
not permit individual control over the approximation errors in different bands. For 
many applications, better filters result from a minimax strategy (minimization of the 
maximum errors) or a frequency-weighted error criterion. Such designs can be achieved 
using algorithmic techniques. 
As the previous examples show, frequency-selective filters designed by windowing 
often have the property that the error is greatest on either side of a discontinuity of the 
ideal frequency response, and the error becomes smaller for frequencies away from 
the discontinuity. Furthermore, as suggested by Figure 7.31, such filters typically result 
in approximately equal errors in the passband and stopband. (See Figures 7.34(c) and 
7.35(c), for example.) We have already seen that, for IIR filters, if the approximation 
error is spread out uniformly in frequency and if the passband and stopband ripples are 
adjusted separately, a given design specification can be met with a lower-order filter than 
if the approximation just meets the specification at one frequency and far exceeds it at 
others. This intuitive notion is confirmed for FIR systems by a theorem to be discussed 
later in the section. 
In the following discussion, we consider a particularly effective and widely used 
algorithmic procedure for the design of FIR filters with a generalized linear phase. 
Although we consider only type I filters in detail, we indicate where appropriate, how 
the results apply to types II, III, and IV generalized linear-phase filters. 
In designing a causal type I linear-phase FIRfilter, it is convenient first to consider 
the design of a zero-phase filter, i.e., one for which 
he[nJ = he[-nJ,  
(7.87) 
and then to insert a delay sufficient to make it causal. Consequently, we consider he[nJ 
satisfying the condition of Eq. (7.87). The corresponding frequency response is given 
by 
L 
jW
Ae(e
) = L he[n]e-jwn , 
(7.88) 
n=-L 

555 
Section 7.7 
Optimum Approximations of FIR Filters 
1 + 01 f"'--""'~~ 
11---­
1 -
0 l l--~---. 
I 
I 
I 
I 
I 
I 
°2 f-
I 
I 
-°2f-
W p 
Ws I~"""""""...",....,...".,~I 1T 
w 
Figure 7.40 
Tolerance scheme and 
ideal response for lowpass filter. 
with L = MI2 an integer, or, because ofEq. (7.87), 
L 
A e(e j W) = hAO] + L 2he[n] cos(wn). 
(7.89) 
11=1 
Note that A e(e jW ) is a real, even, and periodic function of w. A causal system can be 
obtained from he [n] by delaying it by L = M 12 samples. The resulting system has 
impulse response 
h[n] = he[n - M12] = h[M - n] 
(7.90) 
and frequency response 
(7.91) 
Figure 7.40 shows a tolerance scheme for an approximation to a lowpass filter with a real 
function such as A e(e jW ). Unity is to be approximated in the band 0 :s Iwl :s w p with 
maximum absolute error 81 , and zero is to be approximated in the band Ws :s Iwl :s ]f 
with maximum absolute error 82. An algorithmic technique for designing a filter to meet 
these specifications must, in effect, systematically vary the (L + 1) unconstrained im­
pulse response values he[n], where 0 :s n :s L. Design algorithms have been developed 
in which some of the parameters L, 81,82, wp , and Ws are fixed and an iterative proce­
dure is used to obtain optimum adjustments of the remaining parameters. Two distinct 
approaches have been developed. Herrmann (1970), Herrmann and Schussler (1970a), 
and Hofstetter, Oppenheim and Siegel (1971) developed procedures in which L, 81, and 
82 are fixed, and wp and Ws are variable. Parks and McClellan (1972a, 1972b), McClellan 
and Parks (1973), and Rabiner (1972a, 1972b) developed procedures in which L, wp , ws , 
and the ratio 81182 are fixed and 81 (or (2) is variable. Since the time when these different 
approaches were developed, the Parks-McClellan algorithm has become the dominant 
method for optimum design of FIR filters. This is because it is the most flexible and the 
most computationally efficient. Thus, we will discuss only that algorithm here. 
The Parks- McClellan algorithm is based on reformulating the filter design prob­
lem as a problem in polynomial approximation. Specifically, the terms cos(wn) in 
Eq. (7.89) can be expressed as a sum of powers of cos w in the form 
cos(wn) = T,,(cosw) , 
(7.92) 

556 
Chapter 7 
Filter Design Techniques 
where Tn (x) is an nth-order polynomia1.S Consequently, Eq. (7.89) can be rewritten as 
an Lth-order polynomial in cos w, namely, 
L 
Ae(ejW) 
~::>k(coswl, 
(7.93) 
k=O 
where the akS are constants that are related to he[n], the values of the impulse response. 
With the substitution x = cosw, we can express Eq. (7.93) as 
A e(ei'L') 
P(x) Ix=cosw, 
(7.94) 
where pex) is the L th-order polynomial 
L 
P(x) = I::akXk. 
(7.95) 
k=O 
We will see that it is not necessary to know the relationship between the akS and he[n] 
(although a formula can be obtained); it is enough to know that A e(ejW ) can be expressed 
as the Lth-order trigonometric polynomial ofEq. (7.93). 
The key to gaining control over wp and Ws is to fix them at their desired values and 
let 01 and 02 vary. Parks and McClellan (1972a, 1972b) showed that with L, wp , and Ws 
fixed, the frequency-selective filter design problem becomes a problem in Chebyshev ap­
proximation over disjoint sets, an important problem in approximation theory and one 
for which several useful theorems and procedures have been developed. (See Cheney, 
1982.) To formalize the approximation problem in this case, let us define an approxi­
mation error function 
E(w) 
W(w)[Hd(eiw) 
Ae(elw)], 
(7.96) 
where the weighting function W(w) incorporates the approximation error parameters 
into the design process. In this design method, the error function E(w), the weighting 
function W (w), and the desired frequency response H d (eiw)are defined only over dosed 
subintervals of 0 ::: w ::: 71: • For example, to approximate a lowpass filter, these functions 
are defined for 0::: w ::: w p and Ws :s: w ::: 71:. The approximating function A e (elw) is not 
constrained in the transition region(s) (e.g., wp < w < ws), and it may take any shape 
necessary to achieve the desired response in the other subintervals. 
For example, suppose that we wish to obtain an approximation as in Figure 7.40, 
where L, wp, and w, are fixed design parameters. For this case, 
. 
\1, 
wp •
0::: w 
H d(eJW ) = 
(7.97) 
0, 
Ws 
w ::: 71:. 
The weighting function W(w) allows us to weight the approximation errors differently 
in the different approximation intervals. For the lowpass filter approximation problem, 
the weighting function is 
[ 
~, o::: w ::: wp • 
W(w) 
(7.98) 
I, 
Ws ::: w ::: 71:, 
5 More spccifically, Tn(.t) is the nIh-order Cbebyshev polynomial, defined as Tn(x) = cos(n cos' 1 x). 

557 
Section 7.7 
as 
Optimum Approximations of FIR Filters 
~_~7T w 
Figure 7.41 
Typical frequency 
response meeting the specifications 
of Figure 7.40. 
E(w) 
w 
Figure 7.42 
Weighted error for the 
approximation of Figure 7.41. 
where K = lh/82. If Ae(eiw) is as shown in Figure 7.41, the weighted approximation 
error, E(w) in Eq. (7.96), would be as indicated in Figure 7.42. Note that with this 
weighting, the maximum weighted absolute approximation error is 8 = 82 in both 
bands. 
The particular criterion used in this design procedure is the so-called minimax 
or Chebyshev criterion, where, within the frequency intervals of interest (the passband 
and stopband for a lowpass filter), we seek a frequency response A e(eiw) that minimizes 
the maximum weighted approximation error of Eq. (7.96). Stated more compactly, the 
best approximation is to be found in the sense of 
min 
(max IE(w)I),
{he[nJ:OsnsL} 
(J)EF 
where F is the closed subset of 0 ::s w ::s Jr such that 0 ::s w ::s (J)p or Ws ::s (J) ::s Jr. Thus, 
we seek the set of impulse response values that minimizes 8 in Figure 7.42. 
Parks and McClellan (1972a, 1972b) applied the following theorem of approxi­
mation theory to this filter design problem. 
Alternation Theorem: Let Fp denote the closed subset consisting of the disjoint union 
of closed subsets of the real axis x. Furthermore, 
r 
P(x) = Lakxk 
k=O 
is an rth-order polynomial, and D p (x) denotes a given desired function of x that is contin­
uous on Fp; Wp(x) is a positive function, continuous on Fp, and 
Ep(x) = Wp(x)[Dp(x) - P(x)] 
is the weighted error. The maximum error is defined as 
IIEII = max IEp(x)l.
XEFp 

558 
Chapter 7 
Filter Design Techniques 
A necessary and sufficient condition that P(x) be the unique rth-order polynomial that 
minimizes II E II is that E p (x) exhibit at least (r + 2) alternations; i.e., there must exist at 
least (r + 2) values Xi in Fp such that Xl < X2 < ... < Xr+2 and such that Ep(Xi) = 
-Ep(xi+l) 
± IIEII for i 
1,2, ... , (r + 1). 
At first glance, it may seem to be difficult to relate this formal theorem to the 
problem of filter design. However, in the discussion that follows, all of the elements of 
the theorem will be shown to be important in developing the design algorithm. To aid in 
understanding the alternation theorem, in Section 7.7.1 we will interpret it specifically 
for the design of a type I lowpass filter. Before proceeding to apply the alternation 
theorem to filter design, however, we illustrate in Example 7.8 how the theorem is 
applied to polynomials. 
Example 7.8 Alternation Theorem and Polynomials 
The alternation theorem provides a necessary and sufficient condition that a poly­
nomial must satisfy in order that it be the polynomial that minimizes the maximum 
weighted error for a given order. To illustrate how the theorem is applied, suppose we 
want to examine polynomials P(x) that approximate unity for -1 :s x :::: -0.1 and zero 
for 0.1 :s x :s 1. Consider three such polynomials, as shown in Figure 7.43. Each of 
these polynomials is of 5th-order, and we would like to determine which, if any, satisfy 
the alternation theorem. The closed subsets of the real axis x referred to in the theorem 
are the regions -1 :s x :s -0.1 and 0.1 :s x :s 1. We will weight errors equally in both 
regions, i.e .. W p(x) 
1. To begin, it will be useful for the reader to carefully construct 
sketches of the approximation error function for each polynomial in Figure 7.43. 
According to the alternation theorem, the optimal 5th -order polynomial must 
exhibit at least seven alternations of the error in the regions corresponding to the 
closed subset Fp. PI (x) has only five alternations-three in the region -1 :s x :::: -0.1 
and two in the region 0.1 :::: x :s 1. The points x at which the polynomial attains the 
maximum approximation error IIEII within the set Fp are called extremal points (or 
simply extremals). All alternations occur at extremals, but not all extremal points are 
alternations, as we will see. For example, the point with zero slope close to x = 1 that 
does not touch the dotted line is a local maximum, but is not an alternation, because the 
corresponding error function does not reach the negative extreme value.6The alterna­
tion theorem specifies that adjacent alternations must alternate sign, so the extremal 
value at x 
1 cannot be an alternation either. since the previous alternation was a 
positive extremal value at the first point with zero slope in 0.1 :::: x :s 1. The locations 
of the alterations are indicated by the symbol 0 on the polynomials in Figure 7.43. 
P2(x) also has only five alternations and thus is not optimal. Specifically, P2(X) 
has three alternations in -1 :::: x :s -0.1, but again, only two alternations in 
0.1 :s x :s 1. The difficulty occurs because x 
0.1 is not a negative extremal value. The 
previous alternation at x = -0.1 is a positive extremal value, so we need a negative ex­
tremal value for the next alternation. The first point with zero slope inside 0.1 :s x 
also cannot be counted, since it is a positive extremal value, like x 
-0.1, and does 
not alternate sign. We can count the second point with zero slope in this region and 
x = 1, giving two alternations in 0.1 :s x :s 1 and a total of five. 
6In this discussion, we refer to positive and negative extremals of the error function. Since the polyno­
mial is subtracted from a eonstant to form the error, the extremal points are easily located on the polynomial 
curves in Figure 7.43, but the sign is opposite of the variation above and below the desired constant values. 
1 

559 
Section 7.7 
Optimum Approximations of FIR Filters 
Pl (x) 
poly-
x 
Figure 7.43 5th-order polynomials for Example 7.S. Alternation points are indi­
cated byo. 
P3(x) has eight alternations; all points of zero slope, x = -1, x 
-0.1, x 
0.1 
and x 
L Since eight alternations satisfies the alternation theorem, which specifies a 
minimum of seven, P3(x) is the unique optimal 5th-order polynomial approximation 
for this region. 
7.7.1 Optimal TYpe I Lowpass Filters 
For type I filters, the polynomial P(x) is the cosine polynomial Ae(ej(V) in Eq. (7.93), 
with the transformation of variable x = cos wand r = L: 
L 
P(cosw) = I,>k(COswl. 
(7.99) 
k=O 
D p (x) is the desired lowpass filter frequency response in Eq. (7.97), with x 
COSw: 
1, 
coswp :::: cosw:::: 1,
Dp(cosw) 
(7.100)
{ (), 
-1:::: cosw:::: cosws . 
Wp(cosw) is given by Eq. (7.98), rephrased in terms of cos w: 
1 
coswp :::: cosw :::: 1,
Wp(cosw) = 
K' 
(7.101)
{ 1, 
-1 :::: cosw :::: cosws . 
And the weighted approximation error is 
Ep(cosw) 
Wp(cosw)[Dp(cosw) - P(cosw)]. 
(7.102) 
values. 

560 
Chapter 7 
Filter Design Techniques 
Ae(ei"') 
1+81"1I1I1I~~· 
1-81."_1111"~ 
82 
-82 
Wp 
w, 
17 
W 
Figure 7.44 
Typical example of a lowpass filter approximation that is optimal 
according to the alternation theorem for L 
7. 
The closed subset Fp is made up of the union of the intervals 0 :so W < wp and 
Ws :so W :so 7C, or, in terms ofcos w, of the corresponding intervals cos wp :so cos W :so 1 and 
-1 :s cos W .:s cos Ws. The alternation theorem then states that a set of coefficients ak in 
Eq. (7.99) will correspond to the filter representing the unique best approximation to 
the ideallowpass filter, with the ratio 01/82 fixed at K and with passband and stopband 
edges wp and ws , if and only if Ep(cos w) exhibits at least (L +2) alternations on Fp, i.e., 
if and only if E p (cos w) alternately equals plus and minus its maximum value at least 
(L + 2) times. We have previously seen such equiripp/e approximations in the case of 
elliptic IIR filters. 
Figure 7.44 shows a filter frequency response that is optimal according to the 
alternation theorem for L 
7. In this figure, A e(ejW ) is plotted against w. To formally 
test the alternation theorem, we should first redraw A e(ejW ) as a function of x = cos w. 
Furthermore, we want to explicitly examine the alternations of Ep(x). Consequently, in 
Figure 7.45(a), (b), and (c), we show P(x), Wp(x), and E p(x), respectively, as a function 
of x 
cosw. In this example, where L = 7, we see that there are nine alternations of 
the error. Consequently, the alternation theorem is satisfied. An important point is that, 
in counting alternations, we include the points cos wp and cos ws , since, according to the 
alternation theorem, the subsets (or subintervals) included in Fp are closed, i.e., the 
endpoints of the intervals are counted. Although this might seem to be a small issue, it 
is in fact very significant, as we will see. 
Comparing Figures 7.44 and 7.45 suggests that when the desired filter is a lowpass 
filter (or any piecewise-constant filter) we could easily count the alternations by direct 
examination of the frequency response, keeping in mind that the maximum error is 
different (in the ratio K = 81/02) in the passband and stopband. 
The alternation theorem states that the optimum filter must have a minimum 
of (L + 2) alternations, but it does not exclude the possibility of more than (L + 2) 
alternations. In fact, we will show that for a lowpass filter, the maximum possible number 
of alternations is (L + 3). First, however, we illustrate this in Figure 7.46 for L 
7. 

561 
Section 7.7 
Optimum Approximations of FIR Filters 
-°2 
-1 
x=cosw 
(a) 
Wp(x) 
Ep(x) 
1 
r---1
K 
I 
I 
°2
1 
I 
I  
I 
I  
-1 
-1 
1 x=cosw 
-02 
COS Ws coswp 1 
(b) 
(c) 
Figure 7.45 
EQuivalent polynomial approximation functions as a function of x = cos w. 
(a) Approximating polynomial. (b) Weighting function. (c) Approximation error. 
Figure 7.46(a) has L + 3 
10 alternations, whereas Figures 7.46(b), (c), and (d) each 
have L +2 
9 alternations. The case of L +3 alternations (Figure 7 .46a) is often referred 
to as the extraripple case. Note that for the extraripple filter, there are alternations 
at W 
0 and rr, as well as at w = wp and w = ws , i.e., at all the band edges. For 
Figures 7.46(b) and (c), there are again alternations at wp and ws , but not at both w = 0 
and w 
rr. In Figure 7.46(d), there are alternations at 0, rr, wp, and ws , but there is 
one less point of zero slope inside the stopband. We also observe that all of these cases 
are equiripple inside the passband and stopband; i.e., all points of zero slope inside the 
interval 0 < w < rr are frequencies at which the magnitude of the weighted error is 
maximal. Finally, because all of the filters in Figure 7.46 satisfy the alternation theorem 
for L 
7 and for the same value of K = lhf82, it follows that wp and/or Ws must be 
different for each, since the alternation theorem states that the optimum filter under 
the conditions of the theorem is unique. 
The properties referred to in the preceding paragraph for the filters in Figure 7.46 
result from the alternation theorem. Specifically, we will show that for type I lowpass 
filters: 
• The maximum possible number of alternations of the error is (L + 3). 
• Alternations will always occur at wp and Ws' 
• All points with zero slope inside the passband and all points with zero slope inside 
the stopband (for 0 < w < wp and Ws < w < rr) will correspond to alternations; 
i.e., the filter will be equiripple, except possibly at w = 0 and w 
rr . 

Ae(eiw) 
(a) 
Ae(eiw ) 
(b) 
Ae(e1W) 
w 
(e) 
Ae(eiw) 
(d) 
Figure 7.46 
Possible optimum 
lowpass filter approximations for L 
7. 
(a) L+ 3 alternations (extraripple case). 
(b) L+ 2 alternations (extremum at 
(j) = rr). (c) L+ 2 alternations 
(extremum at (j) = 0). (d) L+ 2 
alternations (extrem um at both (j) 
and (j) = rr). 
562 
...... 
0 

563 
Section 7.7 
Optimum Approximations of FIR Filters 
The maximum possible number ofalternations is (L + 3) 
Reference to Figure 7.44 or Figure 7.46 suggests that the maximum possible number of 
locations for alternations, are the four band edges (w = 0, TC, wp , ws ) and the frequencies 
at which A e(eiw) has zero slope. Since an Lth-order polynomial can have at most (L -1) 
points with zero slope in an open interval. the maximum possible number of locations 
for alternations are the (L 
1) local maxima or minima of the polynomial plus the four 
band edges, a total of (L + 3). In considering points with zero slope for trigonometric 
polynomials, it is important to observe that the trigonometric polynomial 
L 
P(cosw) = I.>k(cosw)k, 
(7.103) 
k=O 
when considered as a function ofw, will always have zero slope at w = 0 and w = TC, even 
though P (x) considered as a function of x may not have zero slope at the corresponding 
points x = 1 and x 
-1. This is because 
dP(cosw) 
dw 
(7.104) 
L-l 
) 
= 
sinw 
~(k + l)aHl (coswl 
,
( 
which is always zero at w 
0 and w = TC, as well as atthe (L - 1) roots of the (L 
1)st­
order polynomial represented by the sum. This behavior at w = 0 and w = TC is evident 
in Figure 7.46. In Figure 7.46(d), it happens that the polynomial P(x) also has zero slope 
at x = -1 = COSTC. 
Alternations always occur at (i) p and (i)s 
For all of the frequency responses in Figure 7.46, A e(eiw) is exactly equal to 1 - 01 at 
the passband edge wp and exactly equal to +02 at the stopband edge Ws. To suggest why 
this must always be the case, let us consider whether the filter in Figure 7.46(a) could 
also be optimal if we redefined wp as indicated in Figure 7.47 leaving the polynomial 
unchanged. The frequencies at which the magnitude of the maximum weighted error 
are equal are the frequencies w = 0, w}, WZ, wS , w3, w4, w5, W6, and w = TC, for a 
total of (L + 2) 
9. However, not all of the frequencies are alternations, since, to 
be counted in the alternation theorem, the error must alternate between 8 = ± II E II 
at these frequencies. Therefore, because the error is negative at both wz and ws , the 
frequencies counted in the alternation theorem are w = 0, w}, WZ, W3, W4, W5, W6, and 
TC, for a total of 8. Since (L + 2) = 9, the conditions of the alternation theorem are not 

564 
Chapter 7 
Filter Design Techniques 
Ae(ei"') 
Figure 7.47 Illustration that the 
passband edge wp must be an 
w 
alternation frequency. 
satisfied, and the frequency response of Figure 7.47 is not optimal with (Up and (Us as 
indicated. In other words, the removal of (Up as an alternation frequency removes two 
alternations. Since the maximum number is (L + 3), this leaves at most (L + 1), which 
is not a sufficient number. An identical argument would hold if (Us were removed as an 
alternation frequency. A similar argument can be constructed for highpass filters, but 
this is not necessarily the case for bandpass or multiband filters. (See Problem 7.63.) 
The filter will be equiripple except possibly at w = 0 or w = 1C 
The argument here is very similar to the one used to show that both wp and Ws must 
be alternations. Suppose, for example, that the filter in Figure 7.46(a) was modified as 
indicated in Figure 7.48, so that one point with zero slope did not achieve the maximum 
error. Although the maximum error occurs at nine frequencies, only eight of these can 
be counted as alternations. Consequently, eliminating one ripple as a point of maximum 
error reduces the number ofalternations by two, leaving (L+1) as the maximum possible 
number. 
The foregoing represent only a few of many properties that can be inferred from 
the alternation theorem. A variety of others are discussed in Rabiner and Gold (1975). 
Furthermore, we have considered only type I lowpass filters. While a much broader 
and detailed discussion of type II, III, and IV filters or filters with more general desired 
frequency responses is beyond the scope of this book, we briefly consider type II lowpass 
filters to further emphasize a number of aspects of the alternation theorem. 
Aiei"') 
Figure 7.48 Illustration that the 
frequency response must be equiripple
w 
in the approximation bands. 

565 
Section 7.7 
Optimum Approximations of FIR Filters 
7.7.2 OptimallYpe II Lowpass Filters 
A type II causal filter is a filter for which h[n] 
0 outside the range 0 :s: n :s: M, with 
the filter length (M + 1) even, i.e., M odd, and with the symmetry property 
h[n] = h[M - nJ. 
(7.105) 
Consequently, the frequency response H (e jW ) can be expressed in the form 
. 
(M-l)/2 
M 
H(elw) 
e-J{l)M/2 
L 
2h[n]Cos[w(2- n)]. 
(7.106) 
n=O 
Letting b[n] = 2h[(M + 1)/2 
n], n = 1,2, ... , (M + 1)/2, we can rewrite Eq. (7.106) 
as 
(7.107) 
To apply the alternation theorem to the design of type II filters, we must be able 
to identify the problem as one of polynomial approximation. To accomplish this, we 
express the summation in Eq. (7.107) in the form 
(M+l)/2 
1 
[(M-l)/2 
]
E b[nJCos[w(n-2:)]=cOS(W/2) ?; b[n]cos(wn) . 
(7.108) 
(See Problem 7.58.) The summation on the right-hand side of Eq. (7.108) can now be 
represented as a trigonometric polynomial P(cos w) so that 
H(el(J) 
e-jwM/2 cos(w/2)P(cosw), 
(7.109a) 
where 
L 
P(cosw) = Lak(cosw)k 
(7.109b) 
k=O 
and L 
(M c- 1)/2. The coefficients ak in Eq. (7.109b) are related to the coefficients 
b[nJ in Eq. (7.108), which in turn are related to the coefficients b[n] = 2h[(M+1)/2 nJ 
in Eq. (7.107). As in the type I case, it is not necessary to obtain an explicit relationship 
between the impulse response and the akS. We now can apply the alternation theorem 
to the weighted error between P(cosw) and the desired frequency response. For a 
type llowpass filter with a specified ratio K of passband to stopband ripple, the desired 
function is given by Eq. (7.97), and the weighting function for the error is given by 
Eq. (7.98). For type lliowpass filters, because of the presence of the factor cos(w/2) in 
Eq. (7.109a), the function to be approximated by the polynomial P(cos w) is defined as 
I 
1 
O:::w:s:wp ,  
Hd(ej(J) 
Dp(cos w) 
cos(w/2) ' 
(7.110)  
0, 
Ws :::W:::1T, 
and the weighting function to be applied to the error is 
I 
cos(w/2) 
W(w) 
Wp(cosw) 
K' 
(7.111) 
cos(w/2), 

566  
Chapter 7 
Filter Design Techniques 
Consequently, type II filter design is a different polynomial approximation prob­
lem than type I filter design. 
In this section, we have only outlined the design of type II filters, principally to 
highlight the requirement that the design problem first be formulated as a polynomial 
approximation problem. A similar set ofissues arises in the design of type III and type IV 
linear-phase FIR filters. Specifically, these classes also can be formulated as polynomial 
approximation problems, but in each class, the weighting function applied to the error 
has a trigonometric form, just as it does for type II filters. (See Problem 7.58.) A detailed 
discussion ofthe design and properties of these classes of filters can be found in Rabiner 
and Gold (1975). 
The details of the formulation of the problem for type I and type II linear-phase 
systems have been illustrated for the case of the lowpass filter. However, the discussion 
of type II systems in particular should suggest that there is great flexibility in the choice 
of both the desired response function Hd(ei "» and the weighting function W(cu). For 
example, the weighting function can be defined in terms of the desired function so as to 
yield equiripple percentage error approximation. This approach is valuable indesigning 
type III and type IV differentiator systems. 
7.7.3 The Parks-McClellan Algorithm 
The alternation theorem gives necessary and sufficient conditions on the error for op­
timality in the Chebyshev or minimax sense. Although the theorem does not state 
explicitly how to find the optimum filter, the conditions that are presented serve as the 
basis for an efficient algorithm for finding it. While our discussion is phrased in terms 
of type I lowpass filters, the algorithm easily generalizes. 
From the alternation theorem, we know that the optimum filter A e(e]W) will satisfy 
the set of equations 
W(cui)(Hd(eiWi ) 
Ae(eiwi)] = (_I)i+1 8, 
i = 1, 2, ... , (L + 2), 
(7.112) 
where 8 is the optimum error and Ae(eiw) is given by either Eq. (7.89) or Eq. (7.93). 
Using Eq. (7.93) for A e(eiw), we can write these equations as 
1
2
1 
Xl 
x 
xL 
1 
1 
W(CUl)  
jW
ao 
x 2 
xL  --
H d(e JUl2 )
W(CUZ) 
1 
x2 
2 
2 
-1 
I [ al ] [Hd(e . ,)] 
(7.113) 
I 
= 
Hd(eLL+2) 
,
1 
(_I)L+1
2 
L
1 XL+2 
xL+2 
x L+2 
W(CUL+2) 
where Xi == cos wi. This set of equations serves as the basis for an iterative algorithm 
for finding the optimum A e (eiw). The procedure begins by guessing a set of alternation 
frequencies CUi for i = 1,2, ... , (L +2). Note that wp and CUs are fixed and, based on our 
discussion in Section 7.7.1, are necessarily members of the set ofalternation frequencies. 
Specifically, if cu£ 
cup, then cu€+l 
CUs. The set of Eqs. (7.113) could be solved for the 
set of coefficients ak and 8. However, a more efficient alternative is to use polynomial 

Section 7.7 
Optimum Approximations of FIR Filters 
567 
interpolation. In particular, Parks and McClellan (1972a, 1972b) found that, for the 
given set of the extremal frequencies, 
(7.114)  
where 
L+2 
1 
bk=O--
(7.115) 
. 
(Xk - Xi)
1=1 
i# 
and, as before, Xi = cos Wi. That is, if A e(e jW ) is determined by the set of coefficients ak 
that satisfy Eq. (7.113), with 8 given by Eq. (7.114), then the errorfunction goes through 
±8 at the (L + 2) frequencies Wi, or, equivalently, A e (ejW) has values 1 ± K8 if 0 ::::: Wi ::::: 
wp and ±8 if w, :::: Wi ::::: :rr. Now, since A e (e JW ) is known to be an Lth-order trigonometric 
polynomial, we can interpolate a trigonometric polynomial through (L+ 1) of the (L+2) 
known values E(Wi) (or equivalently, A e(eJWj ». Parks and McClellan used the Lagrange 
interpolation formula to obtain 
L+1 
2:)dk/(X - Xk)]Ck 
k=l 
(7. 116a)
L+1 
I)dk/(X - Xk)] 
k=l 
where x = cos w, Xi = cos wi, 
and 
(7.1l6b) 
(7.1l6c) 
Although only the frequencies WI. W2" •.•, WL+1 are used in fitting the Lth-order 
polynomial, we can be assured that the polynomial also takes on the correct value at 
WL+2 because Eqs. (7.113) are satisfied by the resulting Ae(eJW ). 
Now A e(ej(U) is available at any desired frequency, without the need to solve 
the set of equations (7.113) for the coefficients ak. The polynomial of Eq. (7.116a) 
can be used to evaluate Ae(eJW ) and also E(w) on a dense set of frequencies in the 
passband and stopband. If IE(w)1 ::::: 8 for all W in the passband and stopband, then the 
optimum approximation has been found. Otherwise, we must find a new set of extremal 
frequencies. 

568 
Chapter 7 
Filter Design Techniques 
-t\ 
Figure 7.49 Illustration of the 
Parks-McClellan algorithm for 
equiripple approximation. 
Figure 7.49 shows a typical example for a type I lowpass filter before the optimum 
has been found. Clearly, the set of frequencies illi used to find li (as represented by open 
circles in the figure) was such that 0 was too small. Adopting the philosophy of the 
Remez exchange method (see Cheney, 2000), the extremal frequencies are exchanged 
for a completely new set defined by the (L + 2) largest peaks of the error curve. The 
points marked with x would be the new set of frequencies for the example shown in 
the figure. As before, illp and ills must be selected as extremal frequencies. Recall that 
there are at most (L - 1) local minima and maxima in the open intervals 0 < ill < illp 
and ills < ill < 1(. The remaining extremal frequency can be at either ill 
0 or ill = 1(. 
If there is a maximum of the error function at both 0 and 1(, then the frequency at 
which the greatest error occurs is taken as the new estimate of the frequency of the 
remaining extremum. The cycle-computing the value of 0, fitting a polynomial to the 
assumed error peaks, and then locating the actual error peaks-is repeated untilli does 
not change from its previous value by more than a prescribed small amount. This value 
of 0 is then the desired minimum maximum weighted approximation error. 
A flowchart for the Parks-McClellan algorithm is shown in Figure 7.50. In this 
algorithm, all the impulse response values he[n] are implicitly varied on each iteration 
to obtain the desired optimal approximation, but the values of he[n] are never explicitly 
computed. After the algorithm has converged, the impulse response can be computed 
from samples of the polynomial representation using the discrete Fourier transform, as 
will be discussed in Chapter 8. 
7.7.4 Characteristics of Optimum FIR Filters 
Optimum lowpass FIR filters have the smallest maximum weighted approximation error 
8 for prescribed passband and stopband edge frequencies illp and ills- For the weighting 
function of Eq. (7.98), the resulting maximum stopband approximation error is 82 = 0, 
and the maximum passband approximation error is 81 
K O. InFigure 7.51, we illustrate 
how 0 varies with the order of the filter and the passband cutoff frequency. For this 

Section 7.7 
Optimum Approximations of FIR Filters 
569 
Changed 
Calculate error E(w) 
and find local maxima 
where IE(w)12 0 
No 
Check whether the 
Best approximation 
Yes 
Retain (L + 2) 
largest 
extrema 
Figure 7.50 Flowchart of  
Parks-McClellan algorithm.  
example, K 
1 and the transition width is fixed at (ws -
w p ) = 0.2rr. The curves 
show that as Cup increases, the error 0 attains local minima. These minima on the curves 
correspond to the extraripple (L + 3 extrema) filters. AU points between the minima 
correspond to filters that are optimal according to the alternation theorem. The filters 
for M = 8 and M = 10 are type I filters, while M 
9 and M = 11 correspond to a type 
II filter. It is interesting to note that, for some choices of parameters, a shorter filter 
(M 
9) may be better (Le., it yields a smaller error) than a longer filter (M = 10). 
This may at first seem surprising and even contradictory. However, the cases M 
and M 
10 represent fundamentally different types of filters. Interpreted another way, 
filters for M 
9 cannot be considered to be special cases of M = 10 with one point set 
to zero, since this would violate the linear-phase symmetry requirement. On the other 
hand, M = 8 could always be thought of as a special case of M = 10 with the first and 
last samples set to zero. For that reason, an optimal filter for M 
8 cannot be better 
than one for M = 10. This restriction can be seen in Figure 7.51, where the curve for 
M = 8 is always above or equal to the one for M = 10. The points at which the two 
curves touch correspond to identical impulse responses, with the M = 10 filter having 
the first and last points equal to zero. 
Herrmann et aL (1973) did an extensive computational study of the relationships 
9 

510  
Chapter 7 
Filter Design Techniques 
0.14 rl-----r--------------,------, 
-M=8 
_._. M 
9 
0.02 
----·M=lO 
--- M=l1
0.00 ,-I_ _ _ _ _-'-_ _ _ _ _-'-_ _ _ _ _-'-_ _ _ _ _-' 
0.0  
0.2 'IT 
0.4 'IT 
0.67T 
0.87T 
Passband cutoff (w) 
Figure 1.51 
Illustration of the dependence of passband and stopband error on 
cutoff frequency for optimal approximations of a lowpass filter. For this example, 
K = 1and «(Us - (Up) = O.2n. (After Herrmann, Rabiner and Chan, 1973.) 
among the parameters M, 81,82, wp , and (Us for equiripple lowpass approximations, and 
Kaiser (1974) subsequently obtained the simplified formula 
M= -1010glO(8182}-13,  
(7.117) 
, 
2.324liw 
where liw = Ws -(Up, as a fit to their data. By comparing Eq. (7.117) with the design for­
mula of Eq. (7.76) for the Kaiser window method, we can see that, for the comparable 
case (81 = 82 = 8), the optimal approximations provide about 5 dB better approxima­
tion error for a given value of M. Another important advantage of the equiripple filters 
is that 81 and 82 need not be equal, as must be the case for the window method. 
7.8 EXAMPLES OF FIR EOUIRIPPLE APPROXIMATION 
The Parks-McClellan algorithm for optimum equiripple approximation of FIR filters 
can be used to design a wide variety of such filters. In this section, we give several 
examples that illustrate some of the properties of the optimum approximation and 
suggest the great flexibility that is afforded by the design method. 
7.8.1 Lowpass Filter 
For the lowpass filter case, we again approximate the set of specifications used in Ex­
ample 7.5 and Section 7.6.1 so that we can compare all the major design methods on the 

571 
Section 7.8 
Examples of FIR Equiripple Approximation 
same lowpass filter specifications. These specifications call for wp 
OArr, Ws 
0.6rr, 
81 
0.01, and lh = 0.001. In contrast to the window method, the Parks-McClellan 
algorithm can accommodate the different approximation error in the passband versus 
that in the stopband by fixing the weighting function parameter at K = 81/82 = to. 
Substituting the foregoing specifications into Eq. (7.117) and rounding up yields 
the estimate M 
26 for the value of M that is necessary to achieve the specifications. 
Figures 7.52(a), (b), and (c) show the impulse response, log magnitude, and approxima­
tion error, respectively, for the optimum filter with M = 26, wp = OArr, and Ws 
O.6rr. 
Figure 7.52(c) shows the un weigh ted approximation error 
E(w) 
{I - Ae(ejW ), 
0:::::: w:::::: wp , 
EA(W) 
(7.118) 
W(w) 
0 - Ae(ejW), 
ws:::::: W:::::: rr, 
rather than the weighted error used in the formulation of the design algorithm. The 
weighted error would be identical to Figure 7 .52( c), except that the error would be di­
vided by 10 in the passband.' The alternations of the approximation error are clearly in 
evidence in Figure 7 .52( c). There are seven alternations in the passband and eight in the 
stopband, for a total of fifteen alternations. Since L = M/2 for type I (M even) systems, 
and M 
26, the minimum number of alternations is (L + 2) = (26/2 + 2) = 15. Thus, 
the filter of Figure 7.52 is the optimum filter for M = 26, wp = OArr, and Ws = 0.6rr. 
However, Figure 7.S2(c) shows that the filter fails to meet the original specifications 
on passband and stopband error. (The maximum errors in the passband and stopband 
are 0.0116 and 0.00116, respectively.) To meet or exceed the specifications, we must 
increase M. 
The filter response functions for the case M = 27 are shown in Figure 7.53. Now 
the passband and stopband approximation errors are slightly less than the specified 
values. (The maximum errors in the passband and stopband are 0.0092 and 0.00092, 
respectively.) In this case, there are again seven alternations in the passband and eight 
alternations in the stopband, for a total of fifteen. Note that, since M 
27, this is a type 
II system, and for type II systems, the order of the implicit approximating polynomial is 
L = (M -1)/2 
(27 -1)/2 = 13. Thus. the minimum number of alternations is still 15. 
Note also that in the type II case, the system is constrained to have a zero of its system 
function at z = -1 or W = rr. This is clearly shown in Figures 7 .53(b) and (c). 
If we compare the results of this example with the results of Section 7.6.1, we 
find that the Kaiser window method requires a value M = 40 to meet or exceed the 
specifications, whereas the Parks-McClellan method requires M = 27. This disparity 
is accentuated because the window method produces approximately equal maximum 
errors in the passband and stopband, while the Parks-McClellan method can weight 
the errors differently. 
7.8.2 Compensation for Zero-Order Hold 
In many cases, a discrete-time filter is designed to be used in a system such as that de­
picted in Figure 7.54; i.e., the filter is used to process a sequence of samplesx[nl to obtain 
7For frequency-selective filters, the unweighted approximation error also conveniently displays the 
passband and stopband behavior, since AeCei"') 
1- E(ru) in the passband and Ae(eiw) = -ECw) in the 
stopband. 

0.6 
0.4 
<U 
"0 
::l 
0.2
~ 
e 
«( 
0 
-0.2 
0 
10 
20 
30 
Sample number (n) 
(a)  
20  
0  
-20  
I 
~ 
-40: 
"0 
-60 
-SO 
-100 
0 
0.2rr 
O.4rr 
0.6rr 
0.8rr 
rr 
Radian frequency (w) 
(b) 
0.015 
0.010 
<l) 
0.005 
"0 
::l 
~ 
0 
e 
«( 
-0.005  
-0.010  
-0.015  
0  
0.217 
OArr 
0.6rr 
O.Srr 
rr 
Radian frequency (w) 
(c) 
Figure 7.52 Optimum type I FIR lowpass filter for wp 
D.4rr. ws = 0.6rr. 
K 
10, and M 
26. (a) Impulse response. (b) Log magnitude of the frequency 
response. (c) Approximation error (unweighted). 
572 

0.5 
0.4 
0.3 
0.2 
0.1 
-0.1 L..-_____----'-______-L______....l 
30
o  
10 
20 
Sample number (n) 
(a) 
20r-------------------~ 
O~--------------__ 
-20 
~  -40 
-60 
-80 
-100~--~­ __~_____~_ 
_L___~ 
o 
O.21T 
0.61T 
O.S'1T 
1T 
Radian frequency (w) 
(b) 
0.010 ~------------------_, 
0.005 
-0.005 
-O.OlD '------'------'-------'--------'-----' 
1T
o  
0.2'1T 
0.41T 
0.61T 
O.S1T 
Radian frequency (w) 
(c) 
Figure 7.53 Optimum type II FIR lowpass filter for wp = O.4rr, Ws = 0.6rr, 
K = 10, and M = 27. (a) Impulse response. (b) Log magnitude of frequency 
response. (c) Approximation error (unweighted). 
573 

574 
Chapter 7 
Filter Design TechniqLles 
Discrete-time 
filter 
x[n]  
H(eiw) 
y[n1  
DIA 
converter 
HoUfl) 
Reconstruction 
filter 
YDA (t) I 
iirUfl) 
Yc(t) 
T 
Figure 7.54 
Precompensation of a discrete-time filter for the effects of a D/A 
converter. 
a sequence y[n], which is then the input to a DIA converter and continuous-time lowpass 
filter (as an approximation to the ideal D/C converter) used for the reconstruction of a 
continuous-time signal yc(t). Such a system arises as part of a system for discrete-time 
filtering of a continuous-time signal, as discussed in Section 4.8. If the DIA converter 
holds its output constant for the entire sampling period T, the Fourier transform of the 
output yc(t) is 
. 
YdjQ) = Hr(jQ)Ho(jQ)H(eiQT)X (eiQT ), 
(7.119) 
where Hr(jQ) is the frequency response of an appropriate lowpass reconstruction filter 
and 
Ho(jQ) = sin(QT/2) 
. 
2
1QT
(7.120)
Q/2 
e-
/ 
is the frequency response of the zero-order hold of the DIA converter. In Section 4.804, 
we suggested that compensation for Ho(jQ) could be incorporated into the continuous­
time reconstruction filter; i.e., Hr(jQ) could be chosen as 
QT/2 
Jr 
-
. 
IQI <­
Hr(jQ) = 
sm(QT/2) 
T 
(7.121) 
{ o 
otherwise 
so that the effect of the discrete-time filter H (eiQT ) would be undistorted by the zero­
order hold. Another approach is to build the compensation into the discrete-time filter 
by designing a filter H(eiQT ) such that 
-
'QT 
QT/2 
'QT
H(e1 
) = . 
H(el 
). 
(7.122)
sm(QT/2) 
A D/A-compensated lowpass filter can be readily designed by the Parks-McClellan 
algorithm if we simply define the desired response as 
())/2 
-
. 'w 
. 
. 
0 < ()) < ()) ,
Hd(e1 ) = 
sm(w/2) 
-
-
p 
(7.123)
{ 0, 
())s ::; W ::; n. 
Figure 7.55 shows the response functions for such a filter, wherein the specifications 
are again ())p 
OAn, ())s 
0.6n,81 = 0.01, and 82 = 0.001. In this case, the specifications 
are met with M = 28 rather than M 
27 as in the previous constant-gain case. Thus, 
for essentially no penalty, we have incorporated compensation for the DIA converter 
into the discrete-time filter so that the effective passband of the filter will be flat. (To 
emphasize the sloping nature of the passband, Figure 7.55(c) shows the magnitude 
response in the passband, rather than the approximation error, as in the frequency 
response plots for the other FIR examples.) 

0.6 
0.4 
v 
"0 
.E 
0.2
"E. 
E! 
<t: 
0 
t-
t-
I
-0.2 
• 
1 
!
• 
T 
T 
I 
I 
•
• 
o 
20 
0 
-20 
ill 
'U 
-40 
-60 
-80 
-100 
0 
1.10 
1.05 
v 
"0 
1
:a
B 
E! 
<t: 
0.95 
0.90 
0 
10 
20 
30 
Sample number (n) 
(a) 
0.211' 
0.411' 
Radian frequency (w) 
(b) 
Figure 7.55 
Optimum 
O/A-compensated lowpass filter for 
11' 
wp 
O.4n, ws = 0.6n, K = 10. and
0.211' 
M 
28. (a) Impulse response. (b) Log
Radian frequency (w) 
magnitude of the frequency response. 
(c) 
(c) Magnitude response in passband. 
575 

576 
Chapter 7 
Filter Design Techniques 
7.8.3 Bandpass Filter 
Section 7.7 focused entirely on the lowpass optimal FIR, for which there are only two 
approximation bands. However, bandpass and bandstop filters require three approx­
imation bands. To design such filters, it is necessary to generalize the discussion of 
Section 7.7 to the multiband case. This requires that we explore the implications of the 
alternation theorem and the properties of the approximating polynomial in the more 
general context. first, recall that, as stated, the alternation theorem does not assume 
any limit on the number of disjoint approximation intervals. Therefore, the minimum 
number of alternations for the optimum approximation is still (L + 2). However, multi­
band filters can have more than (L +3) alternations, because there are more band edges. 
(Problem 7.63 explores this issue.) This means that some of the statements proved in 
Section 7.7.1 are not true in the multiband case. For example, it is not necessary for all 
the local maxima or minima of Ae(eJW ) to lie inside the approximation intervals. Thus, 
local extrema can occur in the transition regions, and the approximation need not be 
equiripple in the approximation regions. 
To illustrate this, consider the desired response 
0, o ~ W 
0.31f, 
Hd(eJW ) = 
1, 
0.351f < W ~ 0.61f, 
(7.124)
[ 0, 
O.71f ~ W ~ If, 
and the error weighting function 
1, 
o ~ w ~ 0.31f, 
W(w) = 
1, 
0.351f ~ W ~ O.61f, 
(7.125) 
[ 0.2, 
0.71f ~ W ~ If. 
A value of M + 1 = 75 was chosen for the length of the impulse response of the filter. 
Figure 7.56 shows the response functions for the resulting filter. Note that the transi­
tion region from the second approximation band to the third is no longer monotonic. 
However, the use of two local extrema in this unconstrained region does not violate 
the alternation theorem. Since M = 74, the filter is a type I system, and the order of 
the implicit approximating polynomial is L = M /2 = 74/2 
37. Thus, the alternation 
theorem requires at least L +2 
39 alternations. It can be readily seen in Figure 7 .56( c), 
which shows the unweighted approximation error, that there are 13 alternations in each 
band, for a total of 39. 
Such approximations as shown in Figure 7.56 are optimal in the sense of the alter­
nation theorem, but they would probably be unacceptable in a filtering application. In 
general, there is no guarantee that the transition regions of a multiband filter will be 
monotonic, because the Parks-McClellan algorithm leaves these regions completely 
unconstrained. When this kind of response results for a particular choice of the filter 
parameters, acceptable transition regions can usually be obtained by systematically 
changing one or more of the band edge frequencies, the impulse-response length, or 
the error-weighting function and redesigning the filter. 

0.4 
0.2 
., 
'"0 
~ 
0
0.. 
S < 
-0.2 
-0.4 
0 
20 
40 
60 
80 
Sample number (n) 
(a) 
20 
0 
::Q 
'"0 
-20 
-40 
-60 
-80 
0 
Radian frequency (w) 
(b) 
0.060 
0.040 
., 
0.020 
'"0 B 
~ 
0 
~ 
-0.020 I 
-0.040 
-0.060 
0 
0.211" 
0.411" 
0.611" 
0.811" 
11"  
Radian frequency (w )  
(c)  
Figure 7.56 Optimum FIR bandpass filter for M = 74. (a) Impulse response. 
(b) Log magnitude of the frequency response. (c) Approximation error (un­
weighted). 
577 

578 
Chapter 7 
Filter Design Techniques 
7.9 COMMENTS ON IIR AND FIR DISCRETE-TIME FILTERS 
This chapter has been concerned with design methods for LTI discrete-time systems. 
We have discussed a wide range of methods of designing both infinite-duration and 
finite-duration impUlse-response filters. 
The choice between an FIR filter and an IIR filter depends on the importance to 
the design problem of the advantages of each type. IIR filters, for example, have the 
advantage that a variety offrequency-selective filters can be designed using closed-form 
design formulas. That is, once the problem has been specified in terms appropriate for a 
given approximation method (e.g., Butterworth, Chebyshev, or elliptic), then the order 
of the filter that will meet the specifications can be computed, and the coefficients (or 
poles and zeros) of the discrete-time filter can be obtained by straightforward substi­
tution into a set of design equations. This kind of simplicity of the design procedure 
makes it feasible to design IIR filters by manual computation if necessary, and it leads 
to straightforward noniterative computer programs for IIR filter design. These methods 
are limited to frequency-selective filters, and they permit only the magnitude response 
to be specified. If other magnitude shapes are desired, or if it is necessary to approx­
imate a prescribed phase- or group-delay response, an algorithmic procedure will be 
required. 
In contrast, FIR filters can have a precisely (generalized) linear phase. However, 
closed-form design equations do not exist for FIR filters. Although the window method 
is straightforward to apply, some iteration may be necessary to meet a prescribed spec­
ification. The Parks-McClellan algorithm leads to lower-order filters than the window 
method and filter design programs are readily available for both methods. Also, the 
window method and most of the algorithmic methods afford the possibility of approx­
imating rather arbitrary frequency-response characteristics with little more difficulty 
than is encountered in the design of low pass filters. In addition, the design problem for 
FIR filters is much more under control than the IIR design problem, because of the 
existence of an optimality theorem for FIR filters that is meaningful in a wide range of 
practical situations. Design techniques for FIR filters without linear phase have been 
given by Chen and Parks (1987), Parks and Burrus (1987), Schussler and Steffen (1988), 
and Karam and McClellan (1995). 
Questions of economics also arise in implementing a discrete-time filter. Eco­
nomic concerns are usually measured in terms of hardware complexity, chip area, or 
computational speed. These factors are more or less directly related to the order of 
the filter required to meet a given specification. In applications where the efficiencies 
of polyphase implementations cannot be exploited, it is generally true that a given 
magnitude-response specification can be met most efficiently with an IIR filter. How­
ever, in many cases, the linear phase available with an FIR filter may be well worth the 
extra cost. 
In any specific practical setting, the choice of class of filters and design method 
will be highly dependent on the context, constraints, specifications, and implementation 
platform. In this section, we conclude the chapter with one specific example to illustrate 
some ofthe trade offs and issues that can arise. However, it is only one ofmany scenarios, 
each of which can result in different choices and conclusions. 

579 
Section 7.10 
Design of an Upsampling Filter 
7.10 DESIGN OF AN UPSAMPLING FILTER 
We conclude this chapter with a comparison, in the context of upsampling, of IIR and 
FIR filter designs. As discussed in Chapter4, Sections 4.6.2 and 4.9.3, integerupsampling 
and oversampled DIA conversion employ an expander-by-L followed by a discrete-time 
lowpass filter. Because the sampling rate at the output of the expander is L times the 
rate at the input, the lowpass filter operates at a rate which is L-times the rate of the 
input to the upsampler or D/A converter. As we illustrate in this example, the order 
of the lowpass filter is very dependent on whether the filter is designed as an IIR or 
FIR filter and also within those classes, which filter design method is chosen. While the 
order of the resulting IIR filter might be significantly less than the order of the FIR filter, 
the FIR filter can exploit the efficiencies of a polyphase implementation. For the IIR 
designs, polyphase can be exploited for the implementation of the zeros of the transfer 
function but not for the poles. 
The system to be implemented is an upsampler-by-four, i.e., L = 4. As discussed 
in Chapter 4, the ideal filter for 1:4 interpolation is an ideallowpass filter with gain of 4 
and cutoff frequency rc/ 4. To approximate this filter we set the specifications as follows:8 
passband edge frequency wp 
O.22rc 
stopband edge frequency Ws 
O.29rc 
maximum passband gain 
OdB 
minimum passband gain 
-1dB 
maximum stopband gain 
-40 dB. 
Six different filters were designed to meet these specifications: the four IIR filter designs 
discussed in Section 7.3 (Butterworth, Chebyshev I, Chebyshev II, elliptic) and two FIR 
filter designs (a Kaiser window design and an optimal filter designed using the Parks­
McClellan algorithm). The designs were done using the signal processing toolbox in 
MATLAB. Since the FIR design program used requires passband tolerance limits that 
are symmetric about unity, the specifications above were first scaled appropriately for 
the FIR designs and the resulting FIR filter was then rescaled for a maximum of 0 dB 
gain in the passband. (See Problem 7.3.) 
The resulting filter orders for the six filters are shown in Table 7.3 and the corre­
sponding pole-zero plots are shown in Figure 7.57(a)-(f). For the two FIR designs only 
the zero locations are shown in Figure 7.57. If these filters are implemented as causal 
TABLE 7.3 
ORDERS 
OF DESIGNED FILTERS. 
Filter design 
Order 
Butterworth 
18 
Chebyshev I 
8 
Chebyshev II 
8 
Elliptic 
5 
Kaiser 
63 
Parks-McClellan 
44 
8The gain was normalized to unity in the passband. In all cases the filters can be scaled by 4 for use in 
interpolation. 

Chapter 7 
Filter Design Techniques
580 
iIm 
z-plane 
zero 
8th-order 
zero 
Im 
x' 
x 
x 
x I Re 
x 
(a) 
(b) 
Im 
If
xx\ 
x 
x 
\j 
J Re
x ',I n, 
x 
(c) 
(d) 
lIm 
{ 
0\00 
0
'< 
9.38 
Re 
0 
Re 
0/0
\ 
(f) 
Figure 7.57 
Pole-zero plots forthe six designs, (a) Butterworth filter. (b) Chebyshev I filter, 
(c) Chebyshev II filter. (d) Elliptic filter. (e) Kaiser filter. (f) Parks-McClellan filter. 
filters there will be a multiple-order pole at the origin to match the total number of 
zeros of the transfer function. 
Without exploiting available efficiencies, such as the use of a polyphase imple­
mentation, the two FIR designs require significantly more multiplications per output 
sample than any of the IIR designs. In the IIR designs, the number of mUltiplications 
per output sample will be dependent on specifically how the zeros are implemented. A 
discussion of how to efficiently implement each of the six designs follows below with 
a summary in Table 7.4 comparing the required number of multiplications per output 
Im, 
0 
0 
\ -
I -
0 
0 
(c) 
110.. 

Section 7.1 0 
Design of an Upsampling Filter 
581 
sample. The four IIR designs can be considered as a cascade of an FIR filter (imple­
menting the zeros of the transfer function) and an IIR filter (implementing the poles). 
We first discuss the two FIR designs since efficiencies that can be exploited for those 
can also be utilized with the FIR component of the IIR filters. 
Parks-McClellan and Kaiser window designs: Without exploiting symmetry of the 
impulse response or a polyphase implementation, the required number of multiplica­
tions per output sample is equal to the length ofthe filter. Ifa polyphase implementation 
is used as discussed in Section 4.7.5, then the number ofmultiplications per inputsample 
is equal to the length of the filter. Alternatively, since both filters are symmetric, the 
folded structure discussed in Section 6.5.3 (Figures 6.32 and 6.33) can be used to reduce 
the number of multiplications at the input rate by approximately a factor of 2.9 
Butterworth design: As is characteristic of discrete-time Butterworth filters, all 
of the zeros occur at z = -1 and the poles are, of course, in complex conjugate pairs. 
By implementing the zeros as a cascade of 181st-order terms of the form (1 +z-l) no 
multiplications are required for implementing the zeros. The 18 poles require a total of 
18 multiplications per output sample. 
Chebyshev I design: The Chebyshev I filter has order 8 with the zeros at z = -1 
and consequently the zeros can be implemented with no multiplications. The 8 poles 
require 8 multiplies per output sample. 
Chebyshev II design: In this design, the filter order is again 8. Since the zeros are 
now distributed around the unit circle, their implementation will require some multipli­
cations. However, since all the zeros are on the unit circle, the associated FIR impulse 
response will be symmetric, and folding and/or polyphase efficiencies can be exploited 
for implementing the zeros. 
Elliptic Iter design: The elliptic filter has the lowest (order 5) of the four IIR 
designs. From the pole-zero plot we note that it has all its zeros on the unit circle. 
ConsequenUy the zeros can be implemented efficiently exploiting symmetry as well as 
polyphase implementation. 
Table 7.4 summarizes the number of multiplications required per output sample 
for each of the six designs with several different implementation structures. The direct 
form implementation assumes that both the poles and zeros are implemented in direct 
form, i.e., it does not take advantage of the possibility of cascade implementation of 
multiple zeroes at z = -1. Exploiting a polyphase implementation but not also the sym­
metry of the impulse response, the FIR designs are slightly less efficient than the most 
efficient IIR designs, although they are also the only ones that have linear phase. Ex­
ploiting both symmetry and polyphase together in implementing the Parks-McClellan 
design, it and the elliptic filter are the most efficient. 
9It is possible to combine both folding and polyphase efficiencies in implementing symmetric FIR 
filters (see Baran and Oppenheim, 2007). The resulting number of multiplications is approximately half the 
filter length and at the rate of the input samples rather than at the rate of the output samples. However, the 
resulting structure is significantly more complex. 

582  
Chapter 7 
Filter Design Techniques 
TABLE 7.4 
AVERAGE NUMBER OF REQUIRED 
MULTIPLICATIONS PER OUTPUT SAMPLE FOR 
EACH OF THE DESIGNED FILTERS. 
Filter design 
Direct form 
Symmetric 
Polyphase 
Butterworth 
37 
18 
18 
Chebyshev I 
17 
8 
8 
Chebyshev II 
17 
13 
10.25 
Elliptic 
11 
8 
6.5 
Kaiser 
64 
32 
16 
Parks-McClellan 
45 
23 
11.25 
7.11 SUMMARY 
In this chapter, we have considered a variety of design techniques for both infinite­
duration and finite-duration impulse-response discrete-time filters. Our emphasis was 
on the frequency-domain specification of the desired filter characteristics, since this is 
most common in practice. Our objective was to give a general picture of the wide range 
of possibilities available for discrete-time filter design, while also giving sufficient detail 
about some of the techniques so that they may be applied directly, without further 
reference to the extensive literature on discrete-time filter design. In the FIR case, 
considerable detail was presented on both the window method and the Parks-McClellan 
algorithmic method of filter design. 
The chapter concluded with some remarks on the choice between the two classes of 
digital filters. The main point of that discussion was that the choice is not always clear cut 
and may depend on a multitude offactors that are often difficult to quantify or discuss in 
general terms. However, it should be clear from this chapter and Chapter 6 that digital 
filters are characterized by great flexibility in design and implementation. This flexibility 
makes it possible to implement rather sophisticated signal-processing schemes that in 
many cases would be difficult, if not impossible, to implement by analog means. 
Problems 
Basic Problems with Answers 
7.1. Consider a causal continuous-time system with impulse response hc(t) and system function 
s+a 
H c(s) 
(s +a)2 + b2 ' 
(a)  Use impulse invariance to determine HI (z) for a discrete-time system such that 
hl[n} = hcCnT). 
(b)  Use step invariance to determine H2(Z) for a discrete-time system such that 
s2[n] = scCnT), where 
n 
s2[n] = L h2[k] 
and 
sc(t) = J~oo hc('r)dr.
k=-oo 
(c) Determine the step response SI In] ofsystem 1 and the impulse response h2[n] of system 
2. Is it true that h2[n] = hI [n] = hcCnT)? Is it true that S1 [n] = s2[n] = sc(nT)? 

583 
Chapter 7 
Problems 
7.2. A discrete-time lowpass filter is to be designed by applying the impulse invariance method 
to a continuous-time Butterworth filter having magnitude-squared function 
1 
The specifications for the discrete-time system are those of Example 7.2, i.e., 
0.89125 
IH(ejW)1 ~ 1, 
0 ~ Iwl ~ 0.2rr, 
IH(ejW)1 
0.17783, 
O.3Jr ~ Iwl ~ Jr. 
Assume, as in that example, that aliasing will not be a problem; i.e., design the continuous­
time Butterworth filter to meet passband and stopband specifications as determined by the 
desired discrete-time filter. 
(a)  Sketch the tolerance bounds on the magnitude of the frequency response, IHe(jQ) I, 
of the continuous-time Butterworth filter suth that after application of the impulse 
invariance method (i.e., h[n] 
Tdhe(n7"d )), the resulting discrete-time filter will satisfy 
the given design specifications. Do not assume that Td = 1 as in Example 7.2. 
(b)  Determine the integer order N and the quantity TdQe such that the continuous-time 
Butterworth filter exactly meets the specifications determined in part (a) at the pass­
band edge. 
(c)  Note that if Td = 1, your answer in part (b) should give the values of Nand Qe 
obtained in Example 7.2. Use this observation to determine the system function Hc(s) 
for Td ::j.: 1 and to argue that the system function H (z) which results from impulse 
invariance design with Td ::j.: 1 is the same as the result for Td = 1 given by Eq. (7.17). 
7.3.  We wish to use impulse invariance or the bilinear transformation to design a discrete-time 
filter that meets specifications of the following form: 
1-81 ~ lH(ejW)1 
1 +81, 0 ~ Iwl ~ wp, 
(P7.3-1) 
IH(ejW)1 
Ws ~ Iwl ~ Jr. 
For historical reasons, most of the design formulas, tables, or charts for continuous-time 
filters are normally specified with a peak gain of unity in the passband; i.e., 
1- 81 ~ IHe(jQ)1 ~ 1, 
0 ~ IQI ~ Qp, 
(P7.3-2) 
IHc(Q)1 
82, 
Qs~IQI· 
Useful design charts for continuous-time filters specified in this form were given by Rabiner, 
Kaiser, Herrmann, and Dolan (1974). 
(a)  To use such tables and charts to design discrete-time systems with a peak gain of(1 +81)' 
it is necessary to convert the discrete-time specifications into specifications of the form 
of Eq. (P7.3-2). This can be done by dividing the discrete-time specifications by (1 +81), 
Use this approach to obtain an expression for 81 and 82 in terms of ,Ii] and 82. 
(b)  In Example 7.2, we designed a discrete-time filter with a maximum passband gain of 
unity. This filter can be converted to a filter satisfying a set of specifications such as 
those in Eq. (P7.3-1) by multiplying by a constant of the form (1+81), Find the required 
value of III and the corresponding value of 82 for this example, and use Eq. (7.17) to 
determine the coefficients of the system function of the new filter. 
(c)  Repeat part (b) for the filter in Example 7.3. 

584 
Chapter 7 
Filter Design Techniques 
7.4. The system function of a discrete-time system is 
2 
1 
H(z) = 
-
--~--:c 
1 - e-O.2z-1 
1 ­
(a)  Assume that this discrete-time filter was designed by the impulse invariance method 
with Td 
2; i.e., h[n] = 2hC<2n), where hc(t) is real. Find the system function H c(s) of 
a continuous-time filter that could have been the basis for the design. Is your answer 
unique? If not, find another system function H c(s). 
(b)  Assume that H(z) was obtained by the bilinear transform method with Td = 2. Find 
the system function H c(s) that could have been the basis for the design. Is your answer 
unique? If not, find another He (s). 
7.5.  We wish to use the Kaiser window method to design a discrete-time filter with generalized 
linear phase that meets specifications of the following form: 
IH(eJW)1 :::. 0.01, 
O:::.lwl 
0.25Jr, 
0.95:::. IH(eJW)1 :::. 1.05, 
0.35Jr 
Iwl:::. 0.6Jr, 
IH(ejW)1 :::. 0.01, 
0.65Jr 
Iwl <Jr. 
(a)  Determine the minimum length (M + 1) of the impulse response and the value of the 
Kaiser window parameter f3 for a filter that meets the preceding specifications. 
(b)  What is the delay of the filter? 
(c)  Determine the ideal impulse response hd[n] to which the Kaiser window should be 
applied. 
7.6. We wish to use the Kaiser window method to design a symmetric real-valued FIR filter with 
zero phase that meets the following specifications: 
0.9 < H(eJW ) < 1.1, 
o Iwi:::' O.2Jr, 
-0.06 < H(ejW ) < 0.06, 
0.3Jr :::. Iwl :::. 0.475Jr, 
1.9 < H(eJW ) < 2.1, 
0.525Jr :::. Iwl :::. Jr. 
This specification is to be met by applying the Kaiser window to the ideal real-valued 
impulse response associated with the ideal frequency response Hd(ejW ) given by 
1, 
0:::. Iwl:::. 0.25rr, 
Hd(e jW ) 
0, 
0.25rr:::. Iwl :::. 0.5rr, 
2, 
O.5rr:::' Iwl :::. rr.
1 
(a)  What is the maximum value of.5 that can be used to meet this specification? What is 
the corresponding value of f3? Clearly explain your reasoning. 
(b) What is the maximum value of !:l.w that can be used to meet the specification? What is 
the corresponding value of M + 1, the length of the impulse response? Clearly explain 
your reasoning. 
7.7. We are interested in implementing a continuous-time LTI lowpass filter H (jn) using the 
system shown in Figure 4.10 when the discrete-time system has frequency response Hd (eiw). 
The sampling time T 
10-4 second and the input signal xc(t) is appropriately bandlimited 
with X c(jn) = 0 for Inl 2:: 2rr(5000). 
Let the specifications on IH(jn)1 be 
0.99:::. IH(jn)1 
1.01, 
Inl :::. 2Jr(lOOO), 
IH(jn)1 :::. 0.01, 
Inl 2:: 2rr(llOO). 
Determine the corresponding specifications on the discrete-time frequency response Hd( ejW ) . 
...  

585 
Chapter 7 
Problems 
7.8. We wish  to design an optimal (Parks-McClellan) zero-phase Type I FIR lowpass filter 
with passband frequency wp 
O.3n and stopband frequency Ws = O.6n with equal error 
weighting in the passband and stopband. The impulse response of the desired filter has 
length 11; i.e., h[n] 
0 for n < -5 or n > 5. Figure P7.8 shows the frequency response 
H (e jW ) for two different filters. For each filter, specify how many alternations the filter has, 
and state whether it satisfies the alternation theorem as the optimal filter in the minimax 
sense meeting the preceding specifications. 
1.2 
0.8 
~ 
.~ 
0.6
'" 
~ 0.4 
0.2 
0  
-0.2  
0 
Radian frequency (w) 
(a) 
1.4 r--.,--.---,---,----.--,--......,---r----,,----, 
1.2 
0.8 
.f'  0.6
"" 
~ 0.4 
0.2 
0 
-0.2 
0 
0.31T 
0.61T 
1T 
Radian frequency (w) 
(b)  
Figure P7.B 
7.9. Suppose we design a discrete-time filter using the impulse invariance technique with an ideal 
continuous-time lowpass filter as a prototype. The prototype filter has a cutoff frequency 
of Sic = 2n(1000) radls, and the impulse invariance transformation uses T 
0.2 ms. What 
is the cutoff frequency We for the resulting discrete-time filter? 
7.10.  We wish to design a discrete-time lowpass filter using the bilinear transformation on a 
continuous-time ideallowpass filter. Assume that the continuous-time prototype filter has 
cutofffrequency Q e = 2n(2000) rad/s, and we choose the bilinear transformation parameter 
T 
0.4 ms. What is the cutoff frequency We for the resulting discrete-time filter? 

586  
Chapter 7 
Filter Design Techniques 
7.11.  Suppose that we have an ideal discrete-time lowpass filter with cutoff frequency We = :rr /4. 
In addition, we are told that this filter resulted from applying impulse invariance to a 
continuous-time prototype lowpass filter using T = 0.1 ms. What was the cutoff frequency 
Qe for the prototype continuous-time filter? 
7.12.  An ideal discrete-time highpass filter with cutoff frequency We = :rr /2 was designed using the 
bilinear transformation with T = 1 ms. What was the cutoff frequency Q e for the prototype 
continuous-time ideal highpass filter? 
7.13.  An ideal discrete-time lowpass filter with cutoff frequency We = 2:rr /5 was designed us­
ing impulse invariance from an ideal continuous-time lowpass filter with cutoff frequency 
Q c = 2:rr(4000) rad/s. What was the value of T? Is this value unique? If not, find another 
value of T consistent with the information given. 
7.14.  The bilinear transformation is used to design an ideal discrete-time lowpass filter with 
cutoff frequency We = 3:rr /5 from an ideal continuous-time lowpass filter with cutoff fre­
quency Q e = 2:rr(300) rad/s. Give a choice for the parameter T that is consistent with this 
information. Is this choice unique? If not, give another choice that is consistent with the 
information. 
7.15.  We wish to design an FIR lowpass filter satisfying the specifications 
0.95 < H(e jW ) < 1.05, 
0::: Iwi ::: 0.25:rr, 
-0.1 < H(e jW ) < 0.1, 
0.35:rr::: Iwl::::rr, 
by applying a window w[n] to the impulse response hd [n] for the ideal discrete-time lowpass 
filter with cutoff We = 0.3:rr. Which of the windows listed in Section 7.5.1 can be used to 
meet this specification? For each window that you claim will satisfy this specification, give 
the minimum length M + 1 required for the filter. 
7.16. We wish to design an FIR lowpass filter satisfying the specifications 
0.98 < H(e jW ) < 1.02, 
0::: Iwi ::: 0.63:rr, 
-0.15 < H(eJW ) < 0.15, 
0.65:rr ::: Iwl::::rr, 
by applying a Kaiser window to the impulse response hd[n] for the ideal discrete-time 
lowpass filter with cutoff We = 0.64:rr. Find the values of f3 and M required to satisfy this 
specification. 
7.17.  Suppose that we wish to design a bandpass filter satisfying the following specification: 
-0.02 < IH(ejW)1 < 0.02, 
0::: Iwl::: 0.2:rr, 
0.95 < IH(ejW)1 < 1.05, 
0.3:rr ::: Iwi ::: 0.7:rr, 
-0.001 < IH(ejW)1 < 0.001, 
0.75:rr::: Iwl::::rr. 
The filter will be designed by applying impulse invariance with T = 5 ms to a prototype 
continuous-time filter. State the specifications that should be used to design the prototype 
continuous-time filter. 
7.18.  Suppose that we wish to design a highpass filter satisfying the following specification: 
-0.04 < IH(ejW)1 < 0.04, 
0::: Iwl::: 0.2:rr, 
0.995 < IH(ejW)1 < 1.005, 
0.3:rr ::: Iwl::::rr. 
The filter will be designed using the bilinear transformation and T = 2 ms with a prototype 
continuous-time filter. State the specifications that should be used to design the prototype 
continuous-time filter to ensure that the specifications for the discrete-time filter are met. 

581 
Chapter 7 
Problems 
7.19.  We wish to design a discrete-time ideal bandpass filter that has a passband 7r14 ::::: W 
7r12 
by applying impulse invariance to an ideal continuous-time bandpass filter with passband 
27r(300) ::::: Q :s 27r(600), Specify a choice for T that will produce the desired filter. Is your 
choice of T unique? 
7.20.  Specify whether the following statement is true or false. Justify your answer. 
Statement: If the bilinear transformation is used to transform a continuous-time all­
pass system to a discrete-time system, the resulting discrete-time system will also be 
an aU-pass system. 
Basic Problems 
7.21.  An engineer is asked to evaluate the signal processing system shown in Figure P7.21-1 and 
improve it if necessary. The input x[n] is obtained by sampling a continuous-time signal at 
a sampling rate of 1IT = 100Hz. 
~[nl 
ry[n]
H(eiw)
4'--__.... 
Figure P7.21·1 
The goal is for H (ejW ) to be a linear-phase FIR tilter, and ideally it should have the following 
amplitude response (so it can function as a bandlimited differentiator): 
.  
{-WIT w<O
amplitude of Hid(eJW ) = 
(diT w:::: 0 
(a)  For one implementation of H(eJW ), referred to as HI (eJW ), the designer, motivated by 
the definition 
d (x (t»  
I' 
x(t) - x(t 
b.t)
1m 
dt 
LH-+O 
b.t 
chooses the system impulse response hl [n] so that the input-output relationship is 
y[n] = x[nl 
x[n 
1] 
T 
Plot the amplitude response of Hl (eJW ) and discuss how well it matches the ideal 
response, You may find the following expansions helpful: 
1
1
. 
13 
5 
7
sm(8) = (J - -8 + -8 - -8 + .. , 
3! 
51 
7! 
1 2 
1 4 
1 6
cos(t1) = 1 - -€I + -8 - -8 + ... 
2! 
4! 
6! 
(b)  We want to cascade Hl (e jW ) with another linear-phase FIR filter G(ejW), to ensure 
that for the combination of the two filters, the group delay is an integer number of 
samples. Should the length of the impulse response g[n] be an even or an odd integer? 
Explain. 
(c)  Another method for designing the discrete-time H filter is the method of impulse 
invariance, In this method, the ideal bandlimited continuous-time impulse response, 
as given in Eq. (P7.21-1), is sampled. 
h(t) 
(P7.21-1) 
(In a typical application, Qc might be slightly less than 7r1 T, making h(t) the impulse 
response of a differentiator which is bandlimited to IQ I < 7r IT.) Based on this impulse 

.........: ....... ,... '....;................. .. 
-5,000 ... 
-10,000 •.. 
588  
Chapter 7 
Filter Design Techniques 
response, we would have to create a new filter H2 which is also FIR and linear phase. 
Therefore, the impulse response, h2[n], should preserve the odd symmetryofh(/) about 
t 
O. Using the plot in Figure P7.21-2, indicate the location of samples that result if the 
impulse response is sampled at 100 Hz, and an impulse response of length 9 is obtained 
using a rectangular window. 
10,000 •. 
5,000 
o
Q 
~ 
-0.05 -0.04 -0.03 -0.02 -0.01 
o 
0.01 
0.02 
0.03 
0.04 
0.05 
I (seconds) 
Figure P7.21-2 
(d)  Again using the plot in Figure P7.21-2, indicate the location of samples if the impulse 
response h2[n] is designed to have length 8, again preserving the odd symmetry of h(/) 
aboutt = O. 
(e)  Since the desired magnitude response of H(e jlJJ ) is large near OJ 
rr, you do not want 
H2 to have a zero at OJ 
Tt:. Would you use an impulse response with an even or an 
odd number of samples? Explain. 
7.22.  In the system shown in Figure P7.22, the discrete-time system is a linear-phase FIR lowpass 
filter designed by the Parks-McClellan algorithm with 81 
0.01.82 
0.001, OJp = OArr, 
and OJs = 0.6rr. The length of the impulse response is 28 samples. 'Ihe sampling rate for the 
ideal CID and Die converters is liT = 10000 samples/sec. 
Ideal 
CID 
~ 
T 
Ideal 
.v,(l)
LTI System I .v[n1 
DIC
h[n], H(eiw) 
Converter 
T 
Figure P7.22 
(a) What property should the input signal have so that the overall system behaves as an 
LTI system with Yc(jQ) = Heff(jQ)Xc(jQ)? 

589 
Chapter 7 
Problems 
(b)  For the conditions found in (a), determine the approximation error specifications sat­
isfied by IHeff(jQ)I. Give your answer as either an equation or a plot as a function of 
Q. 
(c)  What is the overall delay from the continuous-time input to the continuous-time output 
(in seconds) of the system in Figure P7.22? 
7.23. Consider a continuous-time system with system function 
1 
Hds) = -. 
s 
This system is called an integrator, since the output yc(t) is related to the input xdt) by 
yc(t) = f~oo xc(r)dr. 
Suppose a discrete-time system is obtained by applying the. bilinear transformation to H c (s). 
(3)  What is the system function H(z) of the resulting discrete-time system? What is the 
impulse response h[n]? 
(b)  If x [11] is the input and y[n] is the output of the resulting discrete-time system, write the 
difference equation that is satisfied by the input and output. What problems do you 
anticipate in implementing the discrete-time system using this difference equation? 
(c)  Obtain an expression for the frequency response H (e jW ) of the system. Sketch the 
magnitude and phase of the discrete-time system for 0 ::::: Iwl ::::: Jr. Compare them with 
the magnitude and phase of the frequency response He (j Q) of the continuous-time 
integrator. Under what conditions could the discrete-time "integrator" be considered 
a good approximation to the continuous-time integrator? 
Now consider a continuous-time system with system function 
Gds) = s. 
This system is a differentiator; i.e., the output is the derivative of the input. Suppose a 
discrete-time system is obtained by applying the bilinear transformation to GcCs). 
(d)  What is the system function G(z) of the resulting discrete-time system? What is the 
impulse response g[n]? 
(e)  Obtain an expression for the frequency response G(ejW ) of the system. Sketch the 
magnitude and phase of the discrete-time system for 0 ::::: Iwl ::::: Jr. Compare them 
with the magnitude and phase of the frequency response GdjQ) of the continuous­
time differentiator. Under what conditions could the discrete-time "differentiator" be 
considered a good approximation to the continuous-time differentiator? 
(f)  The continuous-time integrator and differentiator are exact inverses of one another. 
Is the same true of the discrete-time approximations obtained by using the bilinear 
transforma tion ? 
7.24. Suppose we have an even-symmetric FIR filter h[n] of length 2L + 1, i.e., 
h[n] = 0 for 1111 > L, 
h[n] = h[-n]. 
The frequency response H(ejW ), i.e., the DTFT of h[n], is plotted over -Jr ::::: w ::::: Jr in 
Figure P7.24. 

590 
Chapter 7 
Filter Design Techniques 
1.2 1,.------,-------,-------,--------, 
0.8 
0.6 _............. . 
0.4 
0.2 
o 
~
-0.2  L-____________L-__________
~
L
-__________ 
_____________J 
-'TT 
-7[/2 
o 
'TT12 
'TT 
Normalized frequency w 
Figure P7.24 
What can be inferred from Figure P7.24 about the possible range of values of L'? Clearly 
explain the reason(s) for your answer. Do not make any assumptions about the design 
procedure that might have been used to obtain h[nj. 
7.25.  Let hd[n) denote the impulse response of an ideal desired system with corresponding fre­
quency response Hd(ejW), and let h[nj and H(e jW ) denote the impulse response and fre­
quency response, respectively, of an FIR approximation to the ideal system. Assume that 
h[n] 
0 for n < 0 and n > M. We wish to choose the (M + 1) samples of the impulse 
response so as to minimize the mean-square error of the frequency response defined as 
2 
1 1-
. ~
.
8 
= 2:rr __ IHd(e.lW) 
H(eJwWdill. 
(a)  Use Parseval's relation to express the error function in terms of the sequences hd[n] 
and h[n}. 
(b)  Using the result of part (a), determine the values of h[n] for 0::; n ::; M that minimize 
2
8 . 
(c)  The FIR filter determined in part (b) could have been obtained by a windowing oper­
ation. That is, h[n] could have been obtained by mUltiplying the desired infinite-length 
sequence hd [nj by a certain finite-length sequence w[n]. Determine the necessary win­
dow w[n] such that the optimal impulse response is h[nI 
w[njhd[nj. 
Advanced Problems 
7.26. Impulse invariance and the bilinear tram/ormation are two methods for designing discrete­
time filters. Both methods transform a continuous-time system function He (s) into a discrete­
time system function H (z). Answer the following questions by indicating which methode s) 
will yield the desired result: 

591 
Chapter 7 
Problems 
(a)  A minimum-phase continuous-time system has all its poles and zeros in the left-half s­
plane. If a minimum-phase continuous-time system is transformed into a discrete-time 
system, which method(s) will result in a minimum-phase discrete-time system? 
(b)  Ifthe continuous-time system is an all-pass system, its poles will be at locations Sk in the 
left-half s-plane. and its zeros will be at corresponding locations -Sk in the right-half 
s-plane. Which design method(s) will result in an all-pass discrete-time system? 
(c)  Which design method(s) will guarantee that 
H(e.iW)lw=o = HcCjQ)I!:l=o'! 
(d)  If the continuous-time system is a bandstop filter, which method(s) will result in a 
discrete-time bandstop filter? 
(e)  Suppose that HI (z), H 2(Z), and H (zl are transformed versions of Hel (s), Hc2(s), and 
H c(s), respectively. Which design method(s) will guarantee that H(z) = HI (z)H2(Z) 
whenever Hc(s) 
Hel(s)Hc2(S)'! 
(f)  Suppose that H1 (z), H2(Z), and H(z) are transformed versions of Hcl (s), Hc2(s), and 
H cCs), respectively. Which designmethod(s) will guarantee that H(zl 
HI (z)+H2(Z) 
whenever HcCs) = Hel(s) + Hc2(S)? 
(g)  Assume that two continuous-time system functions satisfy the condition 
Q > O. 
Q < O. 
If HI (z) and H 2(Z) are transformed versions of Hel (s) and Hc2(s), respectively, which 
design methodes) will result in discrete-time systems such that 
0< w < n, 
-n < w < O? 
(Such systems are called "90-degree phase splitters.") 
7.27. Suppose that we are given an ideallowpass discrete-time filter with frequency response 
H(eiw) = 
Iwl < n/4,
{1, 
0,  n/4 < Iwl :::: n. 
We wish to derive new filters from this prototype by manipulations of the impulse response 
h[n]. 
(a)  Plot the frequency response HI (eJW ) for the system whose impulse response is 
hl[n] 
h[2nJ. 
(b) Plot the frequency response H 2 (eiw) for the system whose impulse response is 
h [nJ = {h[n/2], 
n = 0, :::=2, ±4, ... , 
2 
0, 
otherWIse. 
(c)  Plot the frequency response H3(e jW ) for the system whose impulse response is 
h3[n] 
eJnn h[lIJ 
(-l)nh[nJ. 
7.28. Consider a continuous-time lowpass filter H c(s) with passband and stopband specifications 
1 
8,::::  IHc(jQ)1 :::: 1 +81, 
IQI:::: Qp. 
iHc(jQ)1 < li2, 
Qs :::: IQI· 
This filter is transformed to a lowpass discrete-time filter HI (z) by the transformation 
HI(Z) =  HcCs)ls=(1_C1)/(1+C1), 

592  
Chapter 7 
Filter Design Techniques 
and the same continuous-time filter is transformed to a highpass discrete-time filter by the 
transformation 
H2(Z) = Hc(s)ls=(1+c1)!(1-C1)" 
(a)  Determine a relationship between the passband cutoff frequency Q p of the continuous­
time lowpass filter and the passband cutoff frequency wpi of the discrete-time lowpass 
filter. 
(b)  Determine a relationship between the passband cutoff frequency Q p of the continuous­
time lowpass filter and the passband cu toff frequency wp2 of the discrete-time highpass 
filter. 
(c)  Determine a relationship between the passband cutoff frequency wpi of the discrete­
time lowpass filter and the passband cutoff frequency wp2 of the discrete-time highpass 
filter. 
(d)  The network in Figure P7.28 depicts an implementation of the discrete-time lowpass 
filter with system function HI (z). The coefficients A, B, C, and D are real. How should 
these coefficients be modified to obtain a network that implements the discrete-time 
highpass filter with system function H 2(z)? 
y[n]
x[nl 
B 
Z-l 
Figure P7.28 
7.29.  A discrete-time system with system function H (2) and impulse response h[n] has frequency 
response 
A.  
1191 <l9c •
H(eje ) 
{ 0,  
I9c < 1191 ::: Jl'. 
where 0 < I9c < Jl'. This filter is transformed into a new filter by the transformation 2 == 
i.e., 
H)(z) = H(2)lz=_z2 
H(-z2). 
(a)  Obtain a relationship between the frequency variable 19 for the originallowpass system 
H (2) and the frequency variable W for the new system HI (z). 
(b)  Sketch and carefully label the frequency response HI (e jW ) for the new filter. 
(c)  Obtain a relationship expressing hI [n] in terms of h[n]. 
(d)  Assume that H (2) can be realized by the set of difference equations 
g[n] = x[n] 
alg[n - 1] 
bIf[n 
2], 
fln] = a2g[n - 1] + b2fln - 1], 
yEn] = qf[n] - c2g[n - 1], 
wherex[n] is the input and yEn1is the output ofthe system. Determine a set of difference 
equations that will realize the transformed system HI (z) == H (-z2). 

593 
Chapter 7 
Problems 
7.30. Consider designing a discrete-time filter with system function H(z) from a continuous-time 
filter with rational system function H c(s) by the transformation 
H(z) = Hc(s)is=p[(1-ca)/(1+Ca)l' 
where ct. is a nonzero integer and f3 is real. 
(a)  If ct. > 0, for what values of f3 does a stable, causal continuous-time filter with rational 
H c(s) always lead to a stable, causal discrete-time filter with rational H(z)? 
(b)  If ct. < 0, for what values of f3 does a stable, causal continuous-time filter with rational 
H c(s) always lead to a stable, causal discrete-time filter with rational H(z)? 
(c)  For ct. = 2 and f3 = 1, determine to what contour in the z-plane the jQ-axis of the 
s-plane maps. 
(d) Suppose that the continuous-time filter is a stable lowpass filter with passband fre­
quency response such that 
for 
IQI::: 1. 
If the discrete-time system H(z) is obtained by the transformation set forth at the 
beginning of this problem, with ct. = 2 and f3 = 1, determine the values of w in the 
interval Iwl 
re for which 
7.31.  Suppose that we have used the Parks-McClellan algorithm to design a causal FIR linear­
phase lowpass filter. The system function of this system is denoted H (z). The length of the 
impulse response is 25 samples, i.e., hln] 
0 for n < 0 and for n > 24, and h[O] 1= O. The 
desired response and weighting function used were 
Jwl ::: 0.3re
Iwl ::: O.3re 
OAre ::: Iwl ::: re.
OAre ::: Iwl 
In each case below, determine whether the statement is true or false or that insufficient 
information is given. Justify your conclusions. 
(a)  hen + 12] = h[l2 
n] or hen + 12] = -h[12 
n] for -00 < n < 00. 
(b)  The system has a stable and causal inverse. 
(c)  WeknowthatH(-l) 
O. 
(d) The maximum weighted approximation error is the same in all approximation bands. 
(e)  If zo is a zero of H(z), then l/zo is a pole of H(z). 
(f)  The system can be implemented by a network (flow graph) that has no feedback paths. 
(g) The group delay is equal to 24 for 0 < w < re. 
(b)  If the coefficients of the system function are quantized to 10 bits each, the system is 
still optimum in the Chebyshev sense for the original desired response and weighting 
function. 
(i)  If the coefficients of the system function are quantized to 10 bits each, the system is 
still guaranteed to be a linear-phase filter. 
(j)  If the coefficients of the system function are quantized to 10 bits each, the system may 
become unstable. 

594  
Chapter 7 
Filter Design Techniques 
7.32.  You are required to design an FIR filter, h[n], with the following magnitude specifications: 
• Passband edge: wp = rr/100. 
• Stopband edge: Ws = rr/50. 
• Maximum stopband gain: 8s :s -60 dB relative to passband. 
It is suggested that you try using a Kaiser window. The Kaiser window design rules for shape 
parameter fJ and filter length M are provided in Section 7.5.3. 
(a)  What values of fJ and M are necessary to meet the required specifications? 
You show the resulting filter to your boss, and he is unsatisfied. He asks you to reduce 
the computations required for the filter. You bring in a consultant who suggests that you 
design the filter as a eascade of two stages: h'ln] 
p[n] *q[nj. To design p[n] he suggests 
first designing a filter, g[n], with passband edge w~ 
lOwp, stopband edge w; = lOws and 
stopband gain 8; 
8s. The filter p[n] is then obtainecl by expanding g[nj by a factor of 10: 
p[n] = {g[n/lOL when n(lO is an integer, 
0, 
otherwlse. 
(b)  What values of /3' and M' are necessary to meet the required specifications for g[n]? 
(c)  Sketch P(ejW ) from w = 0 to w = rr/4. You do not need to draw the exact shape of the 
frequency response; instead, you should show which regions of the frequency response 
are near 0 dB, and which regions are at or below -60 dB. Label all band edges in your 
sketch. 
(d)  What specifications should be used in designing q[n] to guarantee that h'[II] = p[n] * 
q[n] meets or exceeds the original requirements? Specify the passband edge, w;, stop­
band edge, w~, and stopband attenuation, o.~, required for q[nJ. 
(e)  What values of /3// and M// are necessary to meet the required specifications for q[n]? 
How many nonzero samples will h'[nJ 
q[n] * pen] have? 
(f)  The filter h'[n] from parts (b)-(e) is implemented by first directly convolving the input 
with q[n] and then directly convolving the results with pen]. The filter h[n] from part 
(a) is implemented by directly convolving the input with h[n]. Which of these two 
implementations requires fewer multiplications? Explain. Note: you should not count 
multiplications by 0 as an operation. 
7.33.  Consider a real, bandlimited signal Xa (t) whose Fourier transform Xa (jQ) has the following 
property: 
Xa(jQ) = 0 
for 
IQI > 2rr ·10000 . 
That is, the signal is bandlimited to 10 kHz. 
We wish to process xa(t) with a high pass analog filter whose magnitude response 
satisfies the following specifications (see Figure P7.33): 
l
o:s IHa(jQ)1 :s 0.1 
for O:s IQI :s 2rr· 4000 = Qs  
0.9 
IH,,(jQ)I:s 1 for Q p 
2rr· 8000 :s IQI, 
where Qs and Q p denote the stopband and passband frequencies, respectively. 

Chapter 7 
Problems  
595 
1 
0.9 
--+-----------~----------~~-------------Q 
o 
Figure P7.33 
(a)  Suppose the analog filter Ha (jQ) is implemented by discrete-time processing, accord­
ing to the diagram shown in Figure 7.2. 
The sampling frequency Is = .! is 24 kHz for both the ideal C ID and Die converters. 
T 
Determine the appropriate filter specification for IH(eJW )I, the magnitude response of 
the digital filter. 
(b) Using the bilinear transformation s 
1 ­
1 
we want to design a digital filter whose 
magnitude response specifications were 
in part (a). Find the specifications of 
IGH P (jQ1) I, the magnitude response of the highpass analog filter that is related to the 
digital filter through the bilinear transformation. Again, provide a fully labelled sketeh 
of the magnitude response specifications on IGH pU Q 1)1. 
(c)  Using the frequency transformation Sl = 1 (i.e., replacing the Laplace transform 
S2 
variable s by its reciprocal), design the highpass analog filter G H P (jQ1) from the 
lowest-order Butterworth filter, whose magnitude-squared frequency response is given 
below: 
In particular. find the lowest filter order N and its corresponding cutoff frequency Qc, 
such that the original filter's passband specification (IHa(jQp)1 
0.9) is met exactly. 
In a diagram, label the salient features of the Butterworth filter magnitude response 
that you have designed. 
(d) Draw the pole-zero diagram of the (lowpass) Butterworth filter G(S2), and find an 
expression for its transfer function. 

~ ....... .. 
~.. 
." ~-.» _.. -<.. 
. .. __ ..:........:..... . 
w 
•••••• _ 
._. ___ _ 
, 
,
----, ... _--- ... ------.-... 
~...... 
_.. -.­ ---_ ... ­ --_.­
596 
Chapter 7 
Filter Design Techniques 
7.34. A zero-phase FIR filter h[n] has associated DTFT H(e jW), shown in Figure P7.34. 
1.2r---,----.----,---.----.---,,---,----r---,----, 
1 
0.8 
0.6 
0,4 
0.2 
o 
-0.2 
-1T 
--O.81T -0.61T -0,41T -O.21T 
0 
0.21T 
O,41T O.w 
0.81T 
1T 
Normalized frequency w 
Figure P7.34 
The filter is known to have been designed using the Parks-McClellan (PM) algorithm. The 
input parameters to the PM algorithm are known to have been: 
• Passband edge: wp = O,4rr 
• Stopband edge: Ws = O.6rr 
• Ideal passband gain: G p 
1 
• Ideal stopband gain: Gs 
0 
• Error weighting function W(w) = 1 
The length of the impulse response h[n], is M + 1 = 2L + 1 and 
h[n] 
0 for Inl > L. 
The value of L is not known. 
It is claimed that there are two filters, each with frequency response identical to that 
shown in Figure P7.34, and each having been designed by the Parks-McClellan algorithm 
with different values for the input parameter L. 
• Filter 1: L 
Ll 
• Filter 2: L 
L2 > Ll' 
Both filters were designed using exactly the same Parks-McClellan algorithm and input 
parameters, except for the value of L. 
(a) What are possible values for Ll? 
(b) What are possible values for L2 > Ll? 
,-I- - - ' - - - - - - - ' - - - - ' - - - - ' - - - - - - ' - - - - ' - - - - - - ' ' - - - - - ' - - - - ' - - - - - - - '  

s 
Chapter 7 
Problems  
597 
(c) Are the impulse responses hiln] and hz[n] of the two filters identical? 
(d)  The alternation theorem guarantees "uniqueness of the rth-order polynomial." Ifyour 
answer to (c) is yes, explain why the alternation theorem is not violated. Ifyour answer 
is no, show how the two filters, hi In] and hz[n], are related. 
7.35. We are given an FIR bandpass filter h[n] that is zero phase, i.e., h[n] = h[-n]. Its associated 
DTFT H(e jW ) is shown in Figure P7.35. 
Lower 
Lower 
Upper 
Upper 
stopband 
passband 
passband 
stopband 
edge 
edge 
edge 
edge 
The 
~.ZL-__~_~~__~__ ···~____L-__ L_.~~__~~~__~ 
-11"  
-0.811" 
~.611" -0.417 
~.211" 
0 
0.211" 
0.411" 
0.617 
0.817 
Tr 
Normalized frequency tv 
Figure P7.35 
The filter is known to have been designed using the Parks-McClellan algorithm. The input 
parameters to the Parks-McClellan algorithm are known to have been: 
• Lower stopband edge: Wi = O.2n-
• Ideal passband gain: Gp = 1 
• Lower passband edge: wz 
O.3n-
• Ideal stopband gain: Gs 
0 
• Upper passband edge: W3 = O.7n-
• Error weighting function W(w) = 1 
Dthat 
• Upper stopband edge: w4 = O.8n­
rithm 
The value of the input parameter M +1, which represents the maximum number ofnonzero 
impulse response values (equivalently the filter length), is not known. 
It is claimed that there are two filters, each with a frequency response identical to that 
input 
shown in Figure P7.35, but having different impulse response lengths M + 1 = 2L + 1. 
• Filter 1: M = Ml = 14 
• Filter 2: M 
Mz f= Ml 

598  
Chapter 7 
Filter Design Techniques 
Both filters were designed using exactly the same Parks-McClellan algorithm and input 
parameters, except for the value of M. 
(a)  What are possible values for M2? 
(b)  The alternation theorem guarantees "uniqueness ofthe rth-order polynomial." Explain 
why the alternation theorem is not violated. 
7.36.  The graphs in Figure P7.36 depict four frequency-response magnitude plots of linear-phase 
FIR filters, labelled IA~(ejUJ)I, i = 1, 2, 3, 4. One or more of these plots may belong to 
equiripple linear-phase FIR filters designed by the Parks-McClellan algorithm. The maxi­
mum approximation errors in the passband and the stopband, as well as the desired cutoff 
frequencies of those bands, are also shown in the plots. Please note that the approximation 
error and filter length specifications may have been chosen differently to ensure that the 
cutoff frequencies are the same in each design. 
1.4 c--~~~-~~~-~~~-, 
1.4 
1.2  
1.2 
1 
f O.S 
... 
0.6  
~ 0.6 
0.4  
0.4 
0.2  
0.2 
~~ 0 
0.471' 
0.671' 
71' 
0.411" 
0.67f 
1T 
UJ  
UJ 
1.4 .--~~~-~~~-~~~--. 
1.4 
1.2  
1.2 
1  
1 
~ O.S 
.f O.S 
~ 
'-' 
'" 
~~ 0.6
~" 0.6 
0.4 
0.4 
0.2 
0.2 
0 
0.471' 
0.471' 
0.671' 
11" 
UJ 
(a)  What type(s) (I, II, III, IV) of linear-phase FIR filters can IA~(ejUJ)1 correspond to, for 
i = 1,2, 3, 4? Please note that there may be more than one linear-phase FIR filter type 
corresponding to each IA~(ejUJ)I. If you feel this is the case, list all possible choices. 
(b)  How many alternations does each IA~(ejUJ)1 exhibit, for i = 1, 2, 3, 4? 
,V,~I 
0.671' 
71' 
UJ 
Figure P7.36 

599 
Chapter 7 
Problems 
(c)  For each i, i = 1,2,3,4, can IA~(eiUJ)1 belong to an output of the Parks-McClellan 
algorithm? 
(d)  If you claimed that a given IA~(eiUJ)1 could correspond to an output of the Parks­
McClellan algorithm, and that it could be type I, what is the length of the impulse 
response of IA~(eiw)l? 
7.37.  Consider the two-stage system shown in Figure P737 for interpolating a sequence 
x[nJ = xc(nT) to a sampling rate that is 15 times as high as the input sampling rate; i.e., we 
desire y[n] = xc(n T /15). 
Figure P7.37 
Assume that the input sequence x[nJ 
xc(nT) wa<; obtained by sampling a band-
limited continuous-time signal whose Fourier transform satisfies the following condition: 
IXc(jQ)1 = 0 for IQI 2: 2JT(3600). Assume that the original sampling period was T 
1/8000. 
(a)  Make a sketch ofthe Fourier transform Xc(jQ) of a "typical" bandlimited input signal 
and the corresponding discrete-time Fourier transforms X (eiw) and Xe(e}W). 
(b)  To implement the interpolation system, we must, of course, use nonideal filters. Use 
your plot of Xe(ej(tJ) obtained in part (a) to determine the passband and stopband 
cutofffrequencies (Wpl and Ws 1) required to preserve the original band offrequencies 
essentially unmodified while significantly attenuating the images of the baseband spec­
trum. (That is, we desire that w[n] ~ xdnT/3).) Assuming that this can be achieved 
with passband approximation error 01 = 0.005 (for filter passband gain of 1) and stop­
band approximation error 82 = 0.01, plot the specifications for the design of the filter 
Hj (eiw) for -JT ::: W ::: JT. 
(c)  Assuming that w[n] 
xc(nT/3), make a sketch of We (eiUJ ) and use it to determine the 
passband and stopband cutoff frequencies wpz and wsz required for the second filter. 
(d)  Use the formula of Eq. (7.117) to determine the filter orders Ml and M2 for Parks­
McClellan filters that have the passband and stopband cutoff frequencies determined 
in parts (b) and (c) with 81 = 0.005 and 8z 
0.01 for both filters. 
(e)  Determine how many multiplications are required to compute 15 samples of the output 
for this case. 
7.38.  The system ofFigure 7.2 is used to perform filtering ofcontinuous-time signals with a digital 
filter. The sampling rate of the CID and D/C converters is Is = l/T = 10,000 samples/sec. 
A Kaiser window wK[n] of length M + 1 = 23 and fJ 
3.395 is used to design a 
linear-phase lowpass filter with frequency response Hlp (eiw ). When used in the system of 
Figure 7.1 so that H(eiw) = Hlp(eiw ), the overall effective frequency response (from input 
xa(t) to output Ya(t» meets the following specifications: 
0.99::: ,Heff(jQ)I ::: 1.01, 
o::: IQI ::: 2Jf(2000) 
IHeff(jQ)1 ::: 0.01 
2JT(3000) ::: IQI ::: 2JT(5000). 

600  
Chapter 7 
Filter Design Techniques 
(a)  The linear phase ofthe FIR filter introduces a time delay td. Find the time delay through 
the system (in milliseconds). 
(b)  Now a highpass filter is designed with the same Kaiser window by applying it to the 
ideal impulse response hd[n] whose corresponding frequency response is 
JW. 
{O 
Iwl < O.25rr 
HdCe
) 
2e- jwnd 
0.25rr < Iwl ::::: rr. 
That is, a linear-phase FIR highpass filter with impulse response hhp[n] = WK [nJhdln] 
and frequency response Hhp(eiW ) was obtained by multiplying hdln] by the same 
Kaiser window wK[n] that was used to design the first mentioned lowpass filter. The 
resulting FIR high pass discrete-time filter meets a set of specifications of the following 
form: 
IHhp(eiW )I ::::: 81 
0::::: Iwl ::::: Wi 
G 
82::::: IHhpCeiw)I ::::: G + 82 
W2:::::: Iwl ::::: Jf 
Use information from the lowpass filter specifications to determine the values of WI, 
W2, 0b 82, and G. 
7.39.  Figure P7.39 is the ideal, desired frequency response amplitude for a bandpass filter to be 
designed as a Type I FIR filter h[n], with DTFT H(eiw) that approximates Hd(ejW ) and 
meets the following constraints: 
-8t ::::: H(eiw) ::::: 01, 0:::::: Iwl ::::: Wt 
1  
82:::::: H(eiw) ::::: 1 + 82, W2::::: Iwl ::::: w3 
jw
-83::::: H(e
) ::::: 83, w4::::: Iwl ::::: Jf 
Hiei"') 
w  
W 
-7r  -w4 -w3 
-W2 -Wi 0 
wl w2 
W3 
w4 
7T 
Figure P7.39 
The resulting filter h[n] is to minimize the maximum weighted error and therefore must 
satisfy the alternation theorem. 
Determine and sketch an appropriate choice for the weighting function to use with the 
Parks-McClellan algorithm. 

601 
Chapter 7 
Problems 
7.40.  (a) Figure P7.40-1 shows the frequency response Ae(eJlV ) of a lowpass Type I Parks­
McClellan filter based on the following specifications. Consequently it satisfies the 
alternation theorem. 
Passband edge: 
UJp = 0.45]!' 
Stopband edge: 
UJs = 0.50]!' 
Desired passband magnitude: 1 
Desired stopband magnitude: 0 
The weighting function used in both the passband and the stopband is W (UJ) = 1. 
What can you conclude about the maximum possible number of nonzero values in the 
impulse response of the filter? 
1.6 
1.4 
1.2 
1 
0.8 
,-, 
.~ 
~ 0.6 
~' 
0.4 
0.2 
0 
-D.2 
-DA 
----.;.... ~. 
~."~ 
. ~ ~ ...... _- -­
,. 
. 
• r • ~ "I' •• ~ ........ - •••••
.. ~.. '" ... " 
-1 
-D.8 
Normalized frequency 
Stopband Passband 
Passband Stopband 
edge 
edge 
edge 
edge 
Figure P7.40-1 
(b) Figure P7.40-2 shows another frequency response Be (ellV ) for a Type I FIR filter. 
Be(eJW ) is obtained from AeCelw) from part (a) as follows: 
where kl and kz are constants. Observe that Be(ejW ) displays equiripple behavior, with 
different maximum error in the passband and stopband. 
Does this filter satisfy the alternation theorem with the passband and stopband edge 
frequencies indicated and with passband ripple and stopband ripple indicated by the 
dashed lines? 
the 

602 
Chapter 7 
Filter Design Techniques 
1.6 
1.4 
1.2 
0.8 
i" 
~ 0.6 
r:Q~ 
0.4 
0.2 
0 
-0.2 
-0.4 
.;~ 
.~~~.-!~-
••: 
••••• 4 
.:~ • 
_ 
• 
-.­
, ~ . ~ ... -~ 
I 
-1 
-0.8 
-0.2 
o 
0.2 
0.4 
0.8 
Normalized frequency 
Stopband Passband 
Passband Stopband 
edge 
edge 
edge 
edge 
Figure P7.40-2 
7.41. Assume that H c(s) has an rth-order pole at s 
so' so that H c(s) can be expressed as 
r 
Ak 
Hc(s) = L 
,_ + Gc(s), 
k=l (s 
so) 
where Gc(s) has only 1st-order poles. Assume H c(s) is causal. 
(a)  Give a formula for determining the constants Ak from Hc(s). 
(b)  Obtain an expression for the impulse response he(t) in terms ofSo and gc(t), the inverse 
Laplace transform of Gds). 
7.42. As discussed in Chapter 12, an ideal discrete-time Hilbert transformer is a system that in­
troduces -90 degrees (-JiI2 radians) of phase shift forO < (J) < :rr and +90 degrees (+:rr/2 
radians) of phase shift for -:rr < (J) < O. The magnitude of the frequency response is constant 
(unity) for 0 < 
< Ji and for -Ji < (J) < O. Such systems are also called ideal90-degree
(J) 
phase shifters. 
(a)  Give an equation for the ideal desired frequency response Hd(ejcil) of an ideal discrete­
time Hilbert transformer that also includes constant (nonzero) group delay. Plot the 
phase response of this system for -Ji < (J) < Ji. 
(b)  What type(s) of FIR linear-phase systems (I, II, III, or IV) can be used to approximate 
the ideal Hilbert transformer in part (a)? 
(c)  Suppose that we wish to use the window method to design a linear-phase approximation 
to the ideal Hilbert transformer. Use Hd(ejU» given in part (a) to determine the ideal 
impulse response hd[n] if the FIR system is to be such that h[n] = 0 for n < 0 and 
n>M. 
(d) What is the delay of the system if M 
21? Sketch the magnitude of the frequency 
response of the FIR approximation for this case, assuming a rectangular window. 

603 
Chapter 7 
Problems 
(e)  What is the delay of the system if M = 20? Sketch the magnitude of the frequency 
response of the FIR approximation for this case, assuming a rectangular window. 
7.43.  The commonly used windows presented in Section 7.5.1 can all be expressed in terms of 
rectangular windows. This fact can be used to obtain expressions for the Fourier transforms 
of the Bartlett window and the raised-cosine family ofwindows, which includes the Hanning, 
Hamming, and Blackman windows. 
(a)  Show that the (M + I)-point BartIettwindow, defined by Eq. (7.60b), can be expressed 
as the convolution of two smaller rectangular windows. Use this fact to show that the 
Fourier transform of the (M + l)-point Bartlett window is 
WB(eiw) = e-JwM/2(2/M) (Si~(WM/4»)2 
for M even, 
sm(w/2) 
or 
e-i(vM/2(2/M) (Sin[W(M + 1)/4]) (Sin[W('~ -1)/4]) 
for M odd. 
sin(w/2) 
sin(w/2) 
(b)  Itcan easily be seen thatthe (M+1 )-pointraised-cosine windows defined by Eqs, (7.60c)­
(7.60e) can all be expressed in the form 
w[n] = [A + B cos(2rrn/M) + C cos(4rrn/M)]wR[nj, 
where wR[n] is an (M + I)-point rectangular window. Use this relation to find the 
Fourier transform of the general raised-cosine window. 
(c)  Using appropriate choices for A, B, and C and the result determined in part (b), sketch 
the magnitude of the Fourier transform of the Hanning window. 
7.44. Consider the following ideal frequency response for a multi band filter: 
e-JwM/2, 
O::s Iwi < O.3rr, 
Hd(eiw ) = 
0, 
O.3rr < Iwl < 0.6rr, 
( 
0.5e-jwM/2, 
0.6rr < Iwl ::s rr. 
The impulse response hd[n] is multiplied by a Kaiser window with M 
48 and f3 
3.68, 
reSUlting in a linear-phase FIR system with impulse response h[n]. 
(8) What is the delay of the filter? 
(b) Determine the ideal desired impulse response hd[n]. 
(c)  Determine the set of approximation error specifications that is satisfied by the FIR 
filter; i.e., determine the parameters 81,82, 83, B. C, wpb wsl, Ws2, and wp2 in 
B - 81 ::s IH (eiW) I ::s B + 81' 
IH (eiw)1 :S. .:)2, 
C - 83 :S. IH(ejW)1 ::s C + 83, 
7.45.  The frequency response of a desired filter hd[n] is shown in Figure P7.4S. In this problem, 
we wish to design an (M + I)-point causal linear-phase FIR filter h[n] that minimizes the 
integral-squared error 

604 
Chapter 7 
Filter Design Techniques 
L£:~ 
-7/" 
7/" 
o 
7/" 
7/" 
W 
2 
2 
Figure P7.45 
where the frequency response of the filter h[n] is 
H (ejw ) = A(eiw)e-jwM/2 
and M is an even integer. 
(a)  Determine hd[n]. 
(b) What symmetry should  h[n] have in the range O. < n < M? Briefly explain your 
reasoning. 
(c)  Determine h[n) in the range 0::: n ::: M. 
(d) Determine an expression for the minimum integral-squared error E2 as a function of 
hd[n) and M. 
7.46.  Consider a type I linear-phase FIR lowpass filter with impulse response hLP[n] of length 
(M + 1) and frequency response 
HLP (ejw) 
,4e(ejW)e-jwM/2. 
The system has the amplitude function A e(ejW ) shown in Figure P7.46. 
A.(eiw) 
1 + 81 s::z:-:s 
1-81 
, 
82  
-82  
._•••_IiIIIIi...... 1T 
w 
Figure P7.46 
This amplitude function is the optimal (in the Parks-McClellan sense) approximation to 
unity in the band 0::: (U ::: wp. where (Up 
O.27n, and the optimal approximation to zero 
in the band (Us ::: (U ::: n, wherein (Us 
OAn. 
(a) What is the value of M? 
Suppose now that a highpass filter is derived from this lowpass filter by defining 
hHP[n] = (-1)n+lhLP[n] = -ejll'nhLP[n). 
(b) Show that the resulting frequency response isofthe form HH p(ejW ) 
Be(ejW)e-jwM/2. 

605 
Chapter 7 
Problems 
(c) Sketch Be(e jW ) for 0::: W ::: rr. 
(d) It is asserted that for the given value of M (as found in part (a)), the resulting highpass 
filter is the optimum approximation to zero in the band 0 ::: W ::: 0.6rr and to unity in 
the band 0.73rr ::: w 
rr. Is this assertion correct? Justify your answer. 
7.47. Design a three-point optimal (in the minimax sense) causallowpass filter with Ws = rr/2, 
wp = rr/3, and K = 1. Specify the impulse response hEn] of the filter you design. Note: 
eos(rr/2) = 0 and cos(rr /3) 
0.5. 
Extension Problems 
7.48. If an LTI continuous-time system has a rational system function, then its input and output 
satisfy an ordinary linear differential equation with constant coefficients. A standard pro­
cedure in the simulation of such systems is to use finite-difference approximations to the 
derivatives in the differential equations. In particular, since, for continuous differentiable 
functions yc(t), 
lim 
dt 
T-+O 
it seems plausible that if T is "small enough;' we should obtain a good approximation if we 
replace dYc(t)/dt by [yc(t) 
yeCt - T)]/ T. 
While this simple approach may be useful for simulating continuous-time systems, it is 
not generally a useful method for designing discrete-time systems for filtering applications. 
To understand the effect of approximating differential equations by difference equations, it 
is helpful to consider a specific example. Assume that the system function of a eontinuous­
time system is 
where A and c are constants. 
(a)  Show that the input xc(t) and the output yc(t) of the system satisfy the differential 
equation 
dvc(t)
dt + cYeCt) 
Axc(t)· 
(b)  Evaluate the differential equation at t 
nT, and substitute 
dyc(t) I 
~ 0-.-_'---'=-___  
dt 
l=nT  
i.e., replace the first derivative by theftrst backward difference.  
(c)  Define x[n] = Xc (n T) and yEn] = yc(nT ). With this notation and the result of part (b). 
obtain a difference equation relating x[n] and y[nl, and determine the system function 
H (z) 
Y (z)/ X (z) of the resulting discrete-time system. 
(d)  Show that, for this example, 
H(z) = Hc(s)L~=(l_Cl)!T; 
i.e., show that H(z) can be obtained directly from H c(s) by the mapping 
s 
(It can be demonstrated that if higher-order derivatives are approximated by repeated 
application of the first backward difference, then the result of part (d) holds for higher­
order systems as well.) 

606  
Chapter 7 
Filter Design Techniques 
(e)  For the mapping of part (d), determine the contour in the z-plane to which the jQ­
axis of the s-plane maps. Also, determine the region of the z-plane that corresponds 
to the left half of the s-plane. If the continuous-time system with system function 
He(s) is stable, will the discrete-time system obtained by first backward difference 
approximation also be stable? Will the frequency response of the discrete-time system 
be a faithful reproduction of the frequency response of the continuous-time system? 
How will the stability and frequency response be affected by the choice of T? 
(f)  Assume that the first derivative is approximated by the first forward difference; i.e., 
dt  It=nT R; . •• 
T 
Determine the corresponding mapping from the s-plane to the z-plane, and repeat 
part (e) for this mapping. 
7.49. Consider an LTI continuous-time system with rational system function  H e(s). The input 
xc(t) and the output Ye(t) satisfy an ordinary linear differential equation with constant coef­
ficients. One approach to simulating such systems is to use numerical techniques to integrate 
the differential equation. In this problem, we demonstrate that if the trapezoidal integra­
tion formula is used, this approach is equivalent to transforming the continuous-time system 
function H ds) to a discrete-time system function H(z) using the bilinear transformation. 
To demonstrate this statement, consider the continuous-time system function 
A 
He(s) 
s+c 
where A and c are constants. The corresponding differential equation is 
}'c(t) + CYc(t) 
Axc(t), 
where 
dYe(t)
Yc(t) 
dt 
(a) Show that Yc\nT) can be expressed in terms of YeU) as 
I
nT 
yc(nT) 
Ye(T)dT + yc(nT - T). 
(nT -T) 
The definite integral in this equation represents the area beneath the function Ye(t) for 
the interval from (nT - T) to nT. Figure P7.49 shows a function Ye(t) and a shaded 
trapezoid-shaped region whose area approximates the area beneath the curve. This 
approximation to the integral is known as the trapezoidal approximation. Clearly, as T 
approaches zero, the approximation improves. Use the trapezoidal approximation to 
obtain an expression for Ye(nT) in terms of Yc(nT 
T), Yc(nT), and yc(nT - T). 
nT  
Figure P7.49 
...  

607 
Chapter 7 
Problems 
(b)  Use the differential equation to obtain an expression for yc(nT), and substitute this 
expression into the expression obtained in part (a). 
(c)  Define x[n] =xC<n T ) and y[nJ 
yc(n T ). With this notation and the result of part (b), 
obtain a difference equation relating x[n] and y[n], and determine the system function 
H (z) 
Y (z)1 X (z) ofthe resulting discrete-time system. 
(d)  Show that, for this example, 
H(z) = HC<s) 
i.e., show that H (z) can be obtained directly from H C<s) by the bilinear transformation. 
(For higher-order differential equations, repeated trapezoidal integration applied to 
the highest order derivative ofthe output will result in the same conclusion for a general 
continuous-time system with rational system function.) 
7.50.  In this problem, we consider a method of filter design that might be called autocorrelation 
in variance. Consider a stable continuous-time system with impulse response hc<t) and sys­
tem function H C<s). The autocorrelation function ofthe system impulse response is defined 
as 
i(>c(r:) = L: hc(t)hcCt + r:)dr:, 
and for a real impulse response, it is easily shown that the Laplace transform of i(>dr:) is 
<pcCs) 
Hc<s)Hc(-s). Similarly, consider a discrete-time system with impulse response 
h[n] and system function H(z). The autocorrelation function of a discrete-time system 
impulse response is defined as 
00 
i(>[m] = L h[n]h[n + m), 
n=-oo 
and for a real impulse response, <p(z) 
H(z)H(z-l). 
Autocorrelation in variance implies that a discrete-time filter is defined by equating 
the autocorrelation function of the discrete-time system to the sampled autocorrelation 
function of a continuous-time system; i.e., 
-00 < m < 00. 
The following design procedure is proposed for autocorrelation invariance when HC<s) is 
a rational function having N 1st-order poles at Sko k = 1,2, ... , N, and M < N zeros: 
1. Obtain a partial fraction expansion of <pc(s) in the form 
N 
<Pe(s) = L 
k=l 
to 
2. Form the z-transform 
<p(z) 
3.  Find the poles and zeros of <p(z), and form a minimum-phase system function H(z) 
from the poles and zeros of <p(z) that are inside the unit circle. 
(a)  Justify each step in the proposed design procedure; i.e., show that the autocorrelation 
function of the resulting discrete-time system is a sampled version of the autocorrela­
tion function of the continuous-time system. To verify the procedure, it may be helpful 
to try it out on the 1st-order system with impulse response 
he(t) 
e-atu(t) 

608  
Chapter 7 
Filter Design Techniques 
and corresponding system function 
1 
He(s) = s +a' 
(b) What is the relationship between IH(ejW )12 and IHdjQ)e·? Whattypes of frequency­
response functions would be appropriate for autocorrelation invariance design? 
(c)  Is the system function obtained in Step 3 unique? If not, describe how to obtain addi­
tional autocorrelation-invariant discrete-time systems. 
7.51. Let Hlp(Z) denote the system function for a discrete-time lowpass filter. The implementa­
tions of such a system can be represented by linear signal flow graphs consisting of adders, 
gains, and unit delay elements as in Figure P7.51-1. We want to implement a lowpass filter 
for which the cutoff frequency can be varied by changing a single parameter. The proposed 
strategy is to replace each unit delay element in a flow graph representing Hlp(Z) by the 
network shown in Figure P7.51-2, where a is real and lal < 1. 
0---_+--­
•
Z-1 
0 
Figure P7.51-1 
oS
-1 
a 
0 
Z-l 
Figure P7.51-2 
(a)  Let H (z) denote the system function for the filter that results when the network of 
Figure P7 .51-2 is substituted for each unit delay branch in the network that implements 
Hlp(Z). Show that H(z) and Hlp(Z) are related by a mapping of the Z-plane into the 
z-plane. 
(b)  If H(e jW ) and Hlp(e j o) are the frequency responses of the two systems, determine the 
relationship between the frequency variables ()) and O. Sketch ()) as a function of 0 for 
a = 0.5, and -0.5, and show that H(eJW) is a lowpass filter. Also, if Op is the passband 
cutoff frequency for the originallowpass filter HI{,(Z), obtain an equation for ())p, the 
cutoff frequency of the new filter H (z), as a functIOn of a and Op. 
(c)  Assume that the originallowpass filter has the system function 
1 
Hlp(Z) = 1 
0.9Z 
Draw the flow graph of an implementation of Hlp (Z), and also draw the flow graph of 
the implementation of H (z) obtained by replacing the unit delay elements in the first 
flow graph by the network in Figure P7.51-2. Does the resulting network correspond 
to a computable difference equation? 
(d)  If HI (Z) corresponds to an FIR system implemented in direct form, would the flow 
graph manipulation lead to a computable difference equation? If the FIR system 
Hlp(Z) was a linear-phase system, would the resulting system H(z) also be a linear­
phase system? If the FIR system has an impulse response of length M + 1 samples 
what would be the length of the impulse response of the transformed system? 

609 
Chapter 7 
Problems 
(e)  To avoid the difficulties that arose in part (c), it is suggested that the network of 
Figure P7.51-2 be cascaded with a unit delay element, as depicted in Figure P7.S1-3. 
Repeat the analysis of part (a) when the network of Figure P7.S1-3 is substituted for 
each unit delay element. Detennine an equation that expresses (j as a function of w, 
and show that if Hlp (eJi! ) is a lowpass filter, then H (eJ(J) is not a lowpass filter. 
• 
~r---+---<o 
Z-1 ~~ 
Zl 
Figure P7.51-3 
7.52.  If we are given a basic filter module (a hardware or computer subroutine), it is sometimes 
possible to use it repetitively to implement a new filter with sharper frequency-response 
characteristics. One approach is to cascade the filter with itself two or more times, but it 
can easily be shown that, while stopband errors are squared (thereby reducing them ifthey 
are less than 1), this approach will increase the passband approximation error. Another 
approach, suggested by Tukey (1977), is shown in the block diagram of Figure P7.S2-1. 
Tukey called this approach "twicing." 
x[n] 
2 
Figure P7.52-1 
(a) Assume that the basic system has a symmetric finite-duration impulse response; Le., 
of  
h[-nl. 
-L:::::n 
L,  
hEn] =  { o 
otherwise. 
Detennine whether the overall impulse response g[n] is (i) FIR and (ii) symmetric. 
(b) Suppose that H{ei(J) satisfies the following approximation error specifications: 
(1 - 01) ::::: H{eJW ) ::::: (1 + (1), 
0::::: w:::::wp, 
-02::::: H{eJW ) ::::: 02, 
Ws ::::: W::::: IT:. 
It can be shown that if the basic system has these specifications, the overall frequency 
response G(eiw ) (from x[n] to YEn]) satisfies specifications of the form 
A ::::: G(eJW ) ::::: E, 
0 ::::: W ::::: wp. 
Detennine A, E, C, and D in tenns of 01 and Oz. If 01 « 1 and 02 « 1, what are the 
approximate maximum passband and stopband approximation errors for G{eJW )? 
(c)  As determined in part (b), Tukey's twicing method improves the passband approxi­
mation error, but increases the stopband error. Kaiser and Hamming (1977) general­
ized the twicing method so as to improve both the passband and the stopband. They 
called their approach "sharpening." The simplest sharpening system that improves both 
passband and stopband is shown in Figure P7.52-2. Assume again that the impulse re­
sponse of the basic system is as given in part (a). Repeat part (b) for the system of 
Figure P7.52-2. 

610  
Chapter 7 
Filter Design Techniques 
(d)  The basic system was assumed to be noncausaL If the impulse response of the basic 
system is a causal linear-phase FIR system such that 
O:s n 
M,
h[nl = { h[M - n], 
0, 
otherwise, 
how should the systems of Figures P7.52-1 and P7.52-2 be modified? What type(s) (I, 
II, III, or IV) of causal linear-phase FIR system(s) can be used? What are the lengths 
of the impulse responses g[nl for the systems in Figures P7.52-1 and P7.52-2 (in terms 
of L)? 
x[n] 
3 
Figure P7.52-2 
7.53.  Consider the design of a lowpass linear-phase FIR filter by means of the Parks-McClellan 
algorithm. Use the alternation theorem to argue that the approximation must decrease 
monotonically in the "don't care" region between the passband and the stopband approx­
imation intervals. Hint: Show that all the local maxima and minima of the trigonometric 
polynomial must be in either the passband or the stopband to satisfy the alternation theo­
rem. 
7.54.  Figure P7.54 shows the frequency response A e (e jW ) of a discrete-time FIR system for which 
the impulse response is 
-L :s n :s L,
he[nJ = { her -nJ,
0, 
otherwise. 
AAeiw) 
1.01 ~--:7""::-­
0.99 i-"'" 
..... 
0.01 
w
-0.01 
3  
Figure P7.54 
(a)  Show that A e (ejW) cannot correspond to an FIR filter generated by the Parks-McClellan 
algorithm with a passband edge frequency of][ /3, a stopband edge frequency of 2;r /3, 
and an error-weighting function of unity in the passband and stopband. Clearly explain 
your reasoning. Hint: The alternation theorem states that the best approximation is 
unique. 
(b)  Based on Figure P7.54 and the statement that A e (e jW) cannot correspond to an optimal 
filter, what can be concluded about the value of L? 

611 
Chapter 7 
Problems 
7.55. Consider the system in Figure P7.55. 
Ideal 
eID 
converter
xc(t) 
x[n] 
xc(nT) 
H(e iw) r­
~ 
y[n] 
Yc(t) 
D/A 
converter 
Hrun) 
YDA (t) 
t 
t 
Sampling 
Sampling 
period 
period 
T 
T 
Figure P7.55 
1. Assume that Xcun) 
Ofor InI :::rr/Tandthat 
Inl < rr/T,  
Inl > rr/T,  
denotes an ideallowpass reconstruction filter. 
2. The D/A converter has a built-in zero-order-hold circuit, so that 
YDA(t) = L
00 
y[n]ho(t 
nT), 
n=-OQ 
where 
h () 
0::5 t < T,
{1, 
o t = 0, 
otherwise. 
(We neglect quantization in the D/A converter.) 
3. The second system in Figure P7.55 is a linear-phase FIR discrete-time system with 
frequency response H(e jW ). 
We wish to design the FIR system using the Parks-McClellan algorithm to compensate for 
the effects of the zero-order-hold system. 
(a)  The Fourier transform of the output is Yc(jn) 
HeffUn)X cun). Determine an 
expression for Heff(jn) in terms of H(ejQT ) and T. 
(b)  If the linear-phase FIR system is such that h[n] 
Oforn < Oandn > 51,and T = 10-4 
s, what is the overall time delay (in ms) between xc(t) and yc(t)? 
(c)  Suppose that when T 
10-4 S, we want the effective frequency response to be equirip­
pIe (in both the passband and the stopband) within the following tolerances: 
0.99 ::5 IHeffUn)1 ::5 1.01, 
Inl ::5 2rr(looo), 
IHeffun)1 ::5 0.01, 
2rr(2ooo) ::5 Inl ::5 2rr(5OOO). 
We want to achieve this by designing an optimum linear-phase filter (using the Parks­
McClellan algorithm) that includes compensation for the zero-order hold. Give an 
equation for the ideal response Hd(e jW ) that should be used. Find and sketch the 
weighting function W(w) that should be used. Sketch a "typical" frequency response 
H (ejW ) that might result. 
(d)  How would you modify your results in part ( c) to include magnitude compensation for 
a reconstruction filter Hr(jn) with zero gain above n = 2rr(5OOO), but with sloping 
passband? 

612  
Chapter 7 
Filter Design Techniques 
7.56. After a discrete-time signal is lowpass filtered, it is often downsampled or decimated, as 
depicted in Figure P7.56-1. Linear-phase FIR filters are frequently desirable in such appli­
cations, but if the lowpass filter in the figure has a narrow transition band, an FIR system 
will have a long impulse response and thus will require a large number of multiplications 
and additions per output sample. 
IW
rlnl-1 LPF I •~ 
x[n] 
H(e 
) 
u[n] 
y[n] 
H(ejW) 
Wp =passband frequency 
Ws =stopband frequency 
(ws - wp ) = transition bandwidth 
Wp Ws 
7T 
W 
Figure P7.56·1 
In this problem, we will study the merits of a multistage implementation of the system in 
Figure P7.56-1. Such implementations are particularly useful when (J)s is small and the dec­
imation factor M is large. A general multistage implementation is depicted in Figure P7.56­
2. The strategy is to use a wider transition band in the lowpass filters of the earlier stages, 
thereby reducing the length of the required filter impulse responses in those stages. As dec­
imation occurs, the number of signal samples is reduced, and we can progressively decrease 
the widths of the transition bands of the filters that operate on the decimated signal. In this 
manner, the overall number of computations required to implement the decimator may be 
reduced. 
Figure P7.56·2 
(a)  If no aliasing is to occur as a result of the decimation in Figure P7.56-1, what is the 
maximum allowable decimation factor M in terms of ws? 
(b)  Let M = 100, Ws 
1!'/100, and wp 
0.91!'/100 in the system of Figure P7.56-2. If 
x[n] = 8[n], sketch V(ejW ) and Y (e jW). 
Now consider a two-stage implementation of the decimator for M = 100, as depicted 
in Figure P7.56-3,where Mj 
50,M2 
2,Wpl = O.91!'/lOO,Wp2 = O.91!'/2,andws2 
1!'/2. 
We must choose Wsl or, equivalently, the transition band of LPF1 ' (W.d -wp1 ),such that the 
two-stage implementation yields the same equivalent passband and stopband frequencies as 
the single-stage decimator. (We are not concerned about the detailed shape of the frequency 
response in the transition band, except that both systems should have a monotonically 
decreasing response in the transition band.) 

613 
in 
Chapter 7 
Problems 
x[n] 
LPFI  
LPF2 
Figure P7 .56-3 
(c)  Foran arbitrary Wsl andtheinputx[n] =o[n],sketch Vl(eiw ), WI (eiw), V2(eiw),and 
Y (eiw ) for the two-stage decimator of Figure P7.S6-3. 
(d)  Find the largest value of Ws 1 such that the two-stage decimator yields the same equiv­
alent passband and stopband cutoff frequencies as the single-stage system in part (b). 
In addition to possessing a nonzero transition bandwidth, the lowpass filters must 
differ from the ideal by passband and stopband approximation errors of op and Os, respec­
tively. Assume that linear-phase equiripple FIR approximations are used. It follows from 
Eq. (7.117) that, for optimum lowpass filters, 
N """ -1010glO(opos) 
13 + 1  
(P7.56-1) 
2.3246.w 
' 
where N is the length of the impulse response and 6.w = Ws -wp is the transition band ofthe 
lowpass filter. Equation P7.56-1 provides the basis for comparing the two implementations 
of the decimator. Equation (7.76) could he used in place of Eq. (P7.56-1) to estimate the 
impUlse-response length if the filters are designed by the Kaiser window method. 
(e)  Assume that op 
0.01 and Os 
0.001 for the lowpass filter in the single-stage im­
plementation. Compute the length N of the impulse response of the lowpass filter, 
and determine the number of multiplications required to compute each sample of the 
output. Take advantage of the symmetry of the impulse response of the linear-phase 
FIR system. (Note that in this decimation application. only every Mth sample of the 
output need be computed; i.e., the compressor commutes with the multiplications of 
the FIR system.) 
(f)  Using the value of Wsl found in part (d), compute the impulse response lengths NI 
and N2 of LPF1 and LPF2, respectively, in the two-stage decimator of Figure P7.56-3. 
Determine the total number of multiplications required to compute each sample of 
the output in the two-stage decimator. 
(g)  If the approximation error specifications op = 0.01 and Os = 0.001 are used for 
both filters in the two-stage decimator, the overall passband ripple may be greater 
than 0.01, since the passband ripples of the two stages can reinforce each other; e.g., 
(1 + op)(l + op) > (1 + op). To compensate for this, the filters in the two-stage im­
plementation can each be designed to have only one-half the passband ripple of the 
single-stage implementation. Therefore, assume that I5p = 0.005 and I5s = 0.001 for 
each filter in the two-stage decimator. Calculate the impulse response lengths Nl and 
N2 of LPF1 and LPF2, respectively, and determine the total number of multiplications 
required to compute each sample of the output. 

614  
Chapter 7 
Filter Design Techniques 
(b) Should we also reduce the specification on the stopband approximation error for the 
filters in the two-stage decimator? 
(i)  Optional. The combination of M 1 = 50 and M 2 = 2 may not yield the smallest total 
number of multiplications per output sample. Other integer choices for M 1 and M 2 are 
possible such that M 1M2 = 100. Determine the values of M1 and M 2 that minimize 
the number of multiplications per output sample. 
7.57. In this problem, we develop a technique for designing discrete-time filters with minimum 
phase. Such filters have all their poles and zeros inside (or on) the unit circle. (We will allow 
zeros on the unit circle.) Let us first consider the problem of converting a type I linear­
phase FIR equiripple lowpass filter to a minimum-phase system. If H (eJW) is the frequency 
response of a type I linear-phase filter, then 
1. The corresponding impulse response 
h[M-n), 0::: n ::: M,
h[n) 
{ 0, 
otherwise, 
is real and M is an even integer. 
2.  It follows from part 1 that H (eJW) 
A e(eJW)e-Jwno, where A e(ejW) is real and 
no = M/2 is an integer. 
3. The passband ripple is 81; i.e., in the passband, A e(eJW ) oscillates between (1 + 81) 
and (1 
81)' (See Figure P7.57-1.) 
AAeiw) 
1 
w 
Figure P7.57-1 
4.  The stopband ripple is 82; i.e., in the stopband, -82 ::: Ae(eJW ) S 82, and Ae(eJW) 
oscillates between -82 and +82, (See Figure P7.57-1.) 
The following technique was proposed by Herrmann and Schussler (1970a) for converting 
this linear-phase system into a minimum-phase system that has a system function Hmin (z) 
and unit sample response hmin [n) (in this problem, we assume that minimum-phase systems 
can have zeros on the unit circle): 
Step 1. Create a new sequence 
h [nJ = {h[n), 
n 1= no, 
1 
h[no] + 82, 
n 
nO. 
Step 2. Recognize that HI (Z) can be expressed in the form 
HI (z) = z-no H 2(z)H2(1/z) = z-no H3(Z) 
for some H2(Z), where H2(Z) has all its poles and zeros inside or on the unit 
circle and h2[n] is real. 
Step 3. Define 
Hmin(Z) 
a 

615 
Chapter 7 
Problems 
The denominator constant where a = (Jl - 81 + 82 + 
nor­
malizes the passband so that the resulting frequency response Hmin (eiw) will 
oscillate about a value of unity. 
(a)  Show that if hi En] is chosen as in Step 1, then HI (eiw) can be written as 
Hl(eJW) 
e-JwnOH3(ejW), 
where H3(ejW) is real and nonnegative for all values of w. 
(b)  If H3(eJW) ::::: 0, as was shown in part (a), show that there exists an H2(Z) such that 
H3(Z) 
H2(Z)H2(1/z), 
where H2(;:') is a minimum-phase system function and h2[n] is real (i.e., justify Step 2). 
(c)  Demonstrate that the new filter Hmin (eJW ) is an equiripple lowpass filter (i.e., that its 
magnitude characteristic is of the form shown in Figure P7.57-2) by evaluating vi and 
8Z. What is the length of the new impulse response hmin[n]? 
1 + Il; 
l-Il; 
1T  
W 
Figure P7.57·2 
(d)  In parts (a), (b), and (c), we assumed that we started with a type I FIR linear-phase 
filter. Will this technique work if we remove the linear-phase constraint? Will it work 
if we use a type II FIR linear-phase system? 
7.58. Suppose that we have a program that finds the set of coeffieients aln], n = 0,1, ... , L, that 
minimizes 
t a[n] coswn] I' 
n=O 
given L, F, W (w), and H d(eJW ). We have shown that the solution to this optimization prob­
lem implies a noncausal FIR zero-phase system with impulse response satisfying he[n] = 
hel-n]. By delaying he[n] by L samples, we obtain a eausal type I FIR linear-phase system 
with frequency response 
L 
2L 
H(eiw ) = e-iwM/2 La[n]coswn L h[n]e-Jwn , 
n=O 
n=O 
where the impulse response is related to the coefficients a[nl by 
a[n] = {2h. [M/2 - n] for 1 ::::: n ::::: L, 
h[Mj2] 
for n = 0, 
and M 
2L is the order of the system function polynomial. (The length of the impulse 
response is M + 1.) 
The other three types (II, III, and IV) of linear-phase FIR filters can be designed by 
the available program if we make suitable modifications to the weighting function W(w) 

616  
Chapter 7 
Filter Design Techniques 
and the desired frequency response Hd(ejW ). To see how to do this, it is necessary to 
manipulate the expressions for the frequency response into the standard form assumed by 
the program. 
(a)  Assume that we wish to design a causal type II FIR linear-phase system such that 
h[n] 
h[M -
n] for n = 0,1, ... , M, where M is an odd integer. Show that the 
frequency response of this type of system can be expressed as 
(M+l)/2  
H(ejW) = e-jwMj2 L 
b[n]COslO(n -!),  
n=l  
and determine the relationship between the coefficients b[n] and h[n]. 
(b) Show that the summation 
(M+l)/2
L 
b[n]COslO(n -1) 
n=l 
can be written as 
(M-l)/2 
cos(lO/2) L 
b[n]coslOn 
n=O 
by obtaining an expression for b[n] for n = 1,2, ... , (M + 1)/2 in terms of b[n] for 
n 
0,1, ... , (M 
1)/2. Hint: Note carefully that b[n] is to be expressed in terms of 
b[n]. Also, use the trigonometric identity cos a cos f3 = 1cos(a + (3) + ! cos(a - (3). 
(c)  If we wish to use the given program to design type II systems (M odd) for a given F, 
W(lo), and Hd(e jW ), show how to obtain L, F, W(lO), and Hd(ejW ) in terms of M, F, 
W(lO), and Hd(e jW ) such that if we run the program using L, F, W(lO), and Hd(ejW), 
we may use the resulting set of coefficients to determine the impulse response of the 
desired type II system. 
(d)  Parts (a)-(c) can be repeated for types III and IV causal linear-phase FIR systems 
where h[nl 
-h[M 
n]. For these cases, you must show that, for type III systems (M 
even), the frequency response can be expressed as 
Mj2 
H(ej{J) 
e-jwMj2 L e[n]sinlOn 
n=l 
(M-2)j2 
e-j {J)MI2 sinlO L 
e[n] cos lOn, 
n=O 
and for type IV systems (M odd), 
(M+l)/2 
jw
jwM/2
H(e
) 
e-
L 
d[n] sin lO(n -1) 
n=l 
(M-l)/2 
e- j{J)M/2 sin(lO/2) L 
d[n] cos lOn. 
n=O 
As in part (b), it is necessary to express e[n] in terms of ern] and d[nJ in terms of d[nJ 
using the trigonometric identity sin a cos f3 = i sin(a + (3) + !sin (a - (3). McClellan 
and Parks (1973) and Rabiner and Gold (1975) give more details on issues raised in 
this problem. 

617 
Chapter 7 
Problems 
7.59.  In this problem, we consider a method of obtaining an implementation of a variable-cutoff 
linear-phase filter. Assume that we are given a zero-phase filter designed by the Parks­
McClellan method. The frequency response of this filter can be represented as 
L  
Ae(eJe ) = Lak(COSe/,  
k=a  
and its system function can therefore be represented as 
with eJ() = Z. (We use Z for the original system and z for the system to be obtained by 
transformation of the original system.) 
(a)  Using the preceding expression for the system function, draw a block diagram or 
flow graph of an implementation of the system that utilizes multiplications by the 
coefficients ak, additions, and elemental systems having system function (Z + Z-l)/2. 
(b)  What is the length of the impulse response of the system? The overall system can be 
made causal by cascading the system with a delay of L samples. Distribute this delay 
as unit delays so that all parts of the network will be causal. 
(c)  Suppose that we obtain a new system function from A e (Z) by the substitution 
Be(z) = A e(Z)I(z+Z-1)/2=ao+a!l(z+c 1)/2]" 
Using the flow graph obtained in part (a), draw the flow graph of a system that im­
plements the system function Be(z). What is the length of the impulse response of this 
system? Modify the network as in part (b) to make the overall system and all parts of 
the network causal. 
(d)  If A e(eJe ) is the frequency response of the original filter and Be(eJW) is the frequency 
response of the transformed filter, determine the relationship between e and w. 
(e)  The frequency response of the original optimal filter is shown in Figure P7.59. For 
the case C<"1 = 1 - eta and 0 ::::: eta < 1, describe how the frequency response Be(eJW) 
changes as eta varies. Hint: Plot A e (eJe ) and Be (e JW) as functions of cos e and cos w. 
Are the resulting transformed filters also optimal in the sense of having the minimum 
maximum weighted approximation errors in the transformed passband and stopband? 
Figure P7.59
of d[n] 
(f) Optional. Repeat part (e) for the case etl = 1 + eta and -1 < eta ::::: O. 

618  
Chapter 7 
Filter Design Techniques 
7.60.  In this problem, we consider the effect of mapping continuous-time filters to discrete-time 
filters by replacing derivatives in the differential equation for a continuous-time filter by 
central differences to obtain a difference equation. The first central difference of a sequence 
x[n] is defined as 
~ (1) (x[n]) = x[n + 1] - x[n - 1], 
and the kth central difference is defined recursively as 
~(k){x[n]) = ~(1)(~(k-1)(x[n])}. 
For consistency, the zeroth central difference is defined as 
~(O)(x[n]) = x[n]. 
(a)  If X (z) is the z-transform of x [n], determine the z-transform of ~ (k) (x[n]). 
The mapping of an LTI continuous-time filter to an LTI discrete-time filter is as 
follows: Let the continuous-time filter with input x(t) and output yet) be specified by a 
differential equation of the form 
~  dky(t) _ ~ drx(t) 
~ak--k- -
~br--. 
dt 
dtr 
k=O 
r=O 
Then the corresponding discrete-time filter with input x[n] and output y[n] is specified by 
the difference equation 
N 
M 
Lak~(k)(y[n]) = Lbr~(r)(x[n]). 
k=O 
r=O 
(b)  If He(s) is a rational continuous-time system function and Hd(Z) is the discrete-time 
system function obtained by mapping the differential equation to a difference equation 
as indicated in part (a), then 
Hd(Z) = Hc<s)ls=m(z)' 
Determine m(z). 
(c)  Assume that H c(s) approximates a continuous-time lowpass filter with a cutoff fre­
quency of Q = 1; i.e., 
H( 'Q) "" { 1, IQI < I: 
} 
0, otherwise. 
This filter is mapped to a discrete-time filter using central differences as discussed in 
part (a). Sketch the approximate frequency response that you would expect for the 
discrete-time filter, assuming that it is stable. 
7.61.  Let h[n] be the optimal type I equiripple lowpass filter shown in Figure P7.61, designed 
with weighting function W(e jW ) and desired frequency response Hd(e jW ). For simplicity, 
assume that the filter is zero phase (i.e., noncausal). We will use h[n] to design five different 
FIR filters as follows: 
h1[n] = h[-n], 
h2[n] = (-l)nh[n], 
h3[n] = h[n] * h[n], 
h4[n] = h[n]- K8[n], where K is a constant, 
h [n] = {h[n/2] 
for n e~en, 
5 
0 
otherwise. 

619 
Chapter 7 
Problems 
For each filter hiln], determine whether hdn1 is optimal in the minimax sense. That is, 
determine whether 
hi[n]  min max (W(ejW)IHd(ejW) -
Hi (ejW)I) 
hi[n] OJEF 
for some choices of a piecewise-constant Hd(ejOJ) and a piecewise-constant W (ejW), where 
F is a union of disjoint closed intervals on 0 ::: w ::: 1C. If hi[n] is optimal, determine the 
corresponding Hd(ejW) and W(ejW). If hi[n] is not optimal, explain why. 
as 
by a 
by 
7.62.  Suppose that you have used the Parks-McClellan algorithm to design a causal FIR linear­
phase system. The system function of this system is denoted H (z). The length of the impulse 
response is 25 samples, h[n] 
0 for n < 0 and for n > 24, and h[O] :f:. O. For each of the 
following questions, answer "true," "false," or "insufficient information given": 
(aj  hln + 121 = h[12 - n] or h[n + 12] = -h[12 - n] for -00 < n < 00. 
(b)  The system has a stable and causal inverse. 
(c)  We know that H(-l) = O. 
(d)  The maximum weighted approximation error is the same in all approximation bands. 
(e)  The system can be implemented by a signal flow graph that has no feedback paths. 
(f) The group delay is positive for 0 < w < 1C. 
7.63.  Consider the design of a type I bandpass linear-phase FIR filter using the Parks-McClellan 
algorithm. The impulse response length is M +1 = 2L+1. Recall that for type I systems, the 
frequency response is of the form H(eJW ) 
Ae(eJW)e-JwM/2, and the Parks-McClellan 
algorithm finds the function A e(eJOJ) that minimizes the maximum value of the error func­
tion 
Figure P7.61 
WE F, 
where F is a closed subset of the interval 0 ::: W ::: 1C, W(w) is a weighting function, and 
Hd(eJW) defines the desired frequency response in the approximation intervals F. The 
tolerance scheme for a bandpass filter is shown in Figure P7.63. 
(a)  Give the equation for the desired response H d(eJW ) for the tolerance scheme in Fig­
ureP7.63. 

620 
Chapter 7 
Filter Design Techniques 
(b)  Give the equation for the weighting function W(w) for the tolerance scheme in Fig­
ure P7.63. 
(c)  What is the minimum number of alternations of the error function for the optimum 
filter? 
(d)  What is the maximum number of alternations of the error function for the optimum 
filter? 
IAe(eiW)1 
I 
I
1 + 02 
1 
1-°2 
°3 
01·--~ 
W2 
W3 
w4 
7T 
W
-°1 ___l1li 
-°3 
Figure P7.63 
(e)  Sketch a "typical" weighted error function E(w) that could be the error function for 
an optimum bandpass filter if M 
14. Assume the maximum number of alternations. 
(f)  Now suppose that M, wI, WZ, w3, the weighting function, and the desired function are 
kept the same, but W4 is increased, so that the transition band (W4 - (3) is increased. 
Will the optimum filter for these new specifications necessarily have a smaller value of 
the maximum approximation error than the optimum filter associated with the original 
specifications? Clearly show your reasoning. 
(g)  In the lowpass filter case, all local minima and maxima of Ae(eJW) must occur in the 
approximation bands w E F; they cannot occur in the "don't care" bands. Also, in 
the lowpass case, the local minima and maxima that occur in the approximation bands 
must be alternations of the error. Show that this is not necessarily true in the bandpass 
filter case. Specifically, use the alternation theorem to show (i) that local maxima and 
minima of Ae(ejW) are not restricted to the approximation bands and (ii) that local 
maxima and minima in the approximation bands need not be alternations. 
7.64.  It is often desirable to transform a prototype discrete-time lowpass filter to another kind 
of discrete-time frequency-selective filter. In particular, the impulse invariance approach 
cannot be used to convert continuous-time highpass or bandstop filters to discrete-time 
highpass or bandstop filters. Consequently, the traditional approach has been to design 
a prototype lowpass discrete-time filter using either impulse invariance or the bilinear 
transformation and then to use an algebraic transformation to convert the discrete-time 
lowpass filter into the desired frequency-selective filter. 
To see how this is done, assume that we are given a lowpass system function Hlp(Z) 
that we wish to transform to a new system function H (z), which has either lowpass, highpass, 
bandpass., or bandstop characteristics when it is evaluated on the unit circle. Note that we 
associate the complex variable Z with the prototype lowpass filter and the complex variable 
z with the transformed filter. Then, we define a mapping from the Z-plane to the z-plane 

621 
Chapter 7 
Problems 
of the form 
(P7.64-1) 
such that 
H(z) 
Hlp(Z)[Z-1=G(C1).  
(P7.64-2) 
Instead of expressing Z as a function of z, we have assumed in Eq. (P7.64-1) that Z-1 is 
expressed as a function of z-l. Thus, according to Eq. (P7.64-2), in obtaining H(z) from 
Hlp(Z), we simply replace Z-1 everywhere in Hlp(Z) by the function G(e1). This is a 
convenient representation, because Hlp(Z) is normally expressed as a rational function of 
Z-1. 
If Hlp(Z) is the rational system function of a causal and stable system, we naturally 
require that the transformed system function H (z) be a rational function of z-I and that the 
system also be causal and stable. This places the following constraints on the transformation 
= G(z-I): 
1. G(z-I) must be a rational function of 
2.  The inside of the unit circle of the Z-plane must map to the inside of the unit circle 
of the z-plane. 
3. The unit circle of the Z-plane must map onto the unit circle of the z-plane. 
In this problem, you will derive and characterize the algebraic transformations necessary 
to convert a discrete-time lowpass filter into another lowpass filter with a different cutoff 
frequency or to a discrete-time highpass filter. 
(a)  Let fJ and w be the frequency variables (angles) in the Z-plane and z-plane, respectively, 
Le., on the respective unit circles Z = ej9 and z 
ejm. Show that, for Condition 3 to 
hold, G(e l ) must be an all-pass system, i.e., 
!G(e- jW)! = 1.  
(P7.64-3) 
(b)  It is possible to show that the most general form of G(e l ) that satisfies all of the 
preceding three conditions is 
N  
-I 
k
G(z-I) =± IT  z 
-a
. 
(P7.64-4)
1 - akz-1 
k=1 
From our discussion of all-pass systems in Chapter 5, it should be clear that G(el ), 
as given in Eq. (P7.64-4), satisfies Eq. (P7.64-3), i.e., is an allpass system, and thus 
meets Condition 3. Eq. (p7.64-4) also clearly meets Condition 1. Demonstrate that 
Condition 2 is satisfied if and only if !ak! < 1. 
(c)  A simple 1st-order G(z-l) can be used to map a prototype lowpass filter Hlp(Z) with 
cutoff ep to a new filter H(z) with cutoff wp. Demonstrate that 
will produce the desired mapping for some value of a. Solve for a as a function of 
fJp and wp. Problem 7.51 uses this approach to design lowpass filters with adjustable 
cutoff frequencies. 
(d)  Consider the case of a prototype lowpass filter withfJp = rr: /2. For each of the following 
choices of a, specify the resulting cutoff frequency W p for the transformed filter: 
(i)  a 
-0.2679. 
(ii)  a 
O. 
(iii)  a = 0.4142. 

622 
Chapter 7 
Filter Design Techniques 
(e)  It is also possible to find a 1 st-order all-pass system for G(z-l) such that the prototype 
lowpass filter is transformed to a discrete-time highpass filter with cutoff wp. Note 
jOp
that such a transformation must map Z-l = e
-l> 
= ej(JJp and also map 
Z-l 
1 -l> 
= -1; i.e.,lI 
Omapstow = 11'. Find G(z-l) for this transformation, 
and also, find an expression for a in terms of lip and wp. 
(f)  Using the same prototype filter and values for a as in part (d), sketch the frequency 
responses for the highpass filters resulting from the transformation you specified in 
part (e). 
Similar, but more complicated, transformations can be used to convert the prototype 
lowpass filter Hlp(Z) into bandpass and bandstop filters. Constantinides (1970) describes 
these transformations in more detail. 

8  
The Discrete 
Fourier Transform 
8.0 INTRODUCTION 
In Chapters 2 and 3, we discussed the representation of sequences and LTI systems in 
terms of the discrete-time Fourier and z-transforms, respectively. For finite-duration se­
quences, there is an alternative discrete-time Fourier representation, referred to as the 
discrete Fourier transform (DFT). The DFT is itself a sequence rather than a function 
of a continuous variable, and it corresponds to samples, equally spaced in frequency, 
of the DTFT of the signal. In addition to its theoretical importance as a Fourier repre­
sentation of sequences, the DFT plays a central role in the implementation of a variety 
of digital signal-processing algorithms. This is because efficient algorithms exist for the 
computation of the DFT. These algorithms will be discussed in detail in Chapter 9. The 
application of the DFT to spectrum analysis will be described in Chapter 10. 
Although several points of view can be taken toward the derivation and inter­
pretation of the DFT representation of a finite-duration sequence, we have chosen to 
base our presentation on the relationship between periodic sequences and finite-length 
sequences. We begin by considering the Fourier series representation of periodic se­
quences. Although this representation is important in its own right, we are most often 
interested in the application of Fourier series results to the representation of finite­
length sequences. We accomplish this by constructing a periodic sequence for which 
each period is identical to the finite-length sequence. The Fourier series representation 
of the periodic sequence then corresponds to the DFT of the finite-length sequence. 
Thus, our approach is to define the Fourier series representation for periodic sequences 
and to study the properties of such representations. Then, we repeat essentially the same 
derivations, assuming that the sequence to be represented is a finite-length sequence. 
623 

624  
Chapter 8 
The Discrete Fourier Transform 
This approach to the DFT emphasizes the fundamental inherent periodicity of the DFT 
representation and ensures that this periodicity is not overlooked in applications of the 
DFT. 
8.1  REPRESENTATION OF PERIODIC SEQUENCES: 
THE DISCRETE FOURIER SERIES 
Consider a sequence x[nJ that is periodic1 with period N, so that i[n] 
x[n + rN] for 
any integer values of nand r. As with continuous-time periodic signals, such a sequence 
can be represented by a Fourier series corresponding to a sum of harmonically related 
complex exponential sequences, i.e., complex exponentials with frequencies that are 
integer multiples of the fundamental frequency (2rr / N) associated with the periodic 
sequence x[n]. These periodic complex exponentials are of the form 
ek[n] = eJ(2rr/ N)kn = ek[n + r N],  
(8.1) 
where k is any integer, and the Fourier series representation then has the form2 
x[n] = ~LX[k]eJ(2rr/N)kn. 
(8.2) 
k 
The Fourier series representation of a continuous-time periodic signal gener­
ally requires infinitely many harmonically related complex exponentials, whereas the 
Fourier series for any discrete-time signal with period N requires only N harmoni­
cally related complex exponentials. To see this, note that the harmonically related com­
plex exponentials ek[n] in Eq. (8.1) are identical for values of k separated by N; i.e., 
eo[n] = eNln], edn] = eN+dn], and, in general, 
ekHN[n] = eJ(2rr:/N)(k+eN )n 
eJ(2rr/N)kll ej21r:en 
ej (2rr/N)kn 
ek[n], 
(8.3) 
where f is any integer. Consequently, the set of N periodic complex exponentials eo[n], 
el[n], ... , eN-l[n] defines all the distinct periodic complex exponentials with frequen­
cies that are integer multiples of (2rr / N). Thus, the Fourier series representation of a 
periodic sequence x[n] need contain only N of these complex exponentials. For nota­
tional convenience, we choose k in the range of 0 to N 
1; hence, Eq. (8.2) has the 
form 
N-I 
x[n] = ~ LX [k]eJ(2rr j N)kn.  
(8.4) 
k=O 
However, choosing k to range over any full period of X[k] would be equally valid. 
To obtain the sequence of Fourier series coefficients X[k] from the periodic se­
quence x[n], we exploit the orthogonality of the set of complex exponential sequences. 
1Henceforth. we will use the tilde C) to denote periodic sequences whenever it is important to clearly 
distinguish between periodic and aperiodic sequences. 
2The multiplicative constant 1/N is included in Eq. (8.2) for convenience. It could also be absorbed 
into the definition of X[k J. 

Section 8.1 
Representation of Periodic Sequences: The Discrete Fourier Series 
625 
After mUltiplying both sides of Eq. (8.4) by e-J(2n/N)rn and summing from n 
0 to 
n = N -1, we obtain 
N-l 
N-l 1 N-l
L i[n]e-J(2n/N)rn = L N LX [k]eJ (2n/NHk-r)n. 
(8.5) 
n=O 
n=O 
k=O 
After interchanging the order of summation on the right-hand side, Eq. (8.5) becomes 
I: i[n]e- j (2n/Nlrn = I: X[k] [~ I: eJ(2nI N)(k-r)n] . 
(8.6) 
n=O 
k=O 
n=O 
The following identity expresses the orthogonality of the complex exponentials: 
1 I: e j (2n/N)(k-r)n = {I, 
k 
r = mN, 
m an integer, 
(8.7) 
N 
0, 
otherwise. 
n=O 
This identity can easily be proved (see Problem 8.54), and when it is applied to the 
summation in brackets in Eq. (8.6), the result is 
N-lL i[n]e- j (2n/N)rn = X[r]. 
(8.8) 
Thus, the Fourier series coefficients X[k] in Eq. (8.4) are obtained from i[n] by the 
relation 
N-l 
X [k] = L i[n]e-J(2rr/N)kn. 
(8.9) 
n=O 
Note that the sequence X[k] defined in Eq. (8.9) is also periodic with period N ifEq. (8.9) 
is evaluated outside the range 0 :'S k .:'S N 
1; i.e., X[0] = X[N ], X[1] = X[N + 1], 
and, more generally, 
N-l 
X [k + N] L i[n]e-J(2rr/N)(k+N)n 
n=O 
N-l 
)
= L i[n]e-)(2rr/N)kn 
e-j2:n:n = X[k], 
( n=O 
for any integer k. 
The Fourier series coefficients can be interpreted to be a sequence offinite length, 
given by Eq. (8.9) for k = 0, ... , (N - 1), and zero otherwise, or as a periodic sequence 
defined for all k by Eq. (8.9). Clearly, both of these interpretations are acceptable, since 
in Eq. (8.4) we use only the values of X[k] for 0 :'S k :'S (N - 1). An advantage to inter­
preting the Fourier series coefficients X[k] as a periodic sequence is that there is then a 
duality between the time and frequency domains for the Fourier series representation 
of periodic sequences. Equations (8.9) and (8.4) together are an analysis-synthesis pair 
and will be referred to as the discrete Fourier series (DFS) representation of a periodic 
sequence. 
For convenience in notation, these equations are often written in terms of the 
complex quantity 
(8.10) 

626 
Chapter 8 
The Discrete Fourier Transform 
With this notation, the DFS analysis-synthesis pair is expressed as follows: 
Analysis equation: X [k] 
N-lL i[nJWtn . 
(8.11) 
n=O 
Synthesis equation: 
N-l 
i[n] = ~ LX [k]WN
kn . 
(8.12) 
k=O 
In both of these equations, X[k] and i[n] are periodic sequences. We will sometimes 
find it convenient to use the notation 
i[n] 
X[k] 
(8.13) 
to signify the relationships of Eqs. (8.11) and (8.12). The following examples illustrate 
the use of those equations. 
Example 8.1 
DFS of a Periodic Impulse Train 
We consider the periodic impulse train 
n 
rN, 
ranyinteger,
x[n] = f 8[n - rN] = { ~: 
(8.14)
otherwise. 
r=-oo 
Since x[n] = 8[nl for 0 ::s n ::s N 
1, the DFS coefficients are found, using Eq. (8.11), 
to be 
N-l 
X [k] L 8[n]W~n W2 
1. 
(8.15) 
n=O 
In this case, X[k] 
1 for all k. Thus, substituting Eq. (8.15) into Eq. (8.12) leads to 
the representation 
00 
N-l 
1 N-l 
"" • 
1 "" 
-kn 
N L e j (21fI N)kn
x[n] 
L..J u[n -rNJ = -
L..J WN 
(8.16)
N 
r=-oo 
k=O 
k=O 
Example 8.1 produced a useful representation of a periodic impulse train in terms 
of a sum of complex exponentials, wherein all the complex exponentials have the same 
magnitude and phase and add to unity at integer multiples of N and to zero for all other 
integers. If we look closely at Eqs. (8.11) and (8.12), we see that the two equations are 
very similar, differing only in a constant multiplier and the sign of the exponents. This 
duality between the periodic sequence i[n] and its DFS coefficients X[k] is illustrated 
in the following example. 

627 
Section 8.1 
Representation of Periodic Sequences: The Discrete Fourier Series 
Example 8.2 
Duality in the DFS 
In this example, the DFS coefficients are a periodic impulse train: 
Y[k]= L
00 
N8[k-rN]. 
r=-oo 
Substituting hk] into Eq. (8.12) gives 
y[n] = 
N-l 
~ L N8[k]WN
kn = wN
O 
1. 
k=O 
In this case, y[n] = 1 for all n. Comparing this result with the results for x[n] and X[k] 
of Example 8.1, we see that hk] 
Nx[k] and y[n] = X[n]. In Section 8.2.3, we will 
show that this example is a special case of a more general duality property. 
Ifthe sequence i[n] is equal to unity over only part of one period, we can also ob­
tain a closed-form expression for the DFS coefficients. This is illustrated by the following 
example. 
Example 8.3 The DFS of a Periodic Rectangular Pulse Train 
For this example, x[nJ is the sequence shown in Figure 8.1, whose period is N = 10. 
From Eq. (8.11), 
4 
4 
X[k] = L Wfo = Le-j (21!'/lO)kn. 
(8.17) 
n=O 
n=O 
This finite sum has the closed form 
1 -
wi~ _ _ 
j(41!'k/1O) sin(d/2)
x [k] 
(8.18)
1 - wto - e 
sin(d/lO)' 
The magnitude and phase of the periodic sequence X[k] are shown in Figure 8.2. 
x[n] 
. . I I I I I . . . . . Illl!..... ll ..· 
-10 
-6 
012345678910 
n 
Figure 8.1 
Periodic sequence with period N = 10 for which the Fourier series 
representation is to be computed. 

628 
Chapter 8 
The Discrete Fourier Transform 
IX[kJI 
5 
-1 0 1 2 3 4 5 6 7 8 910 
15 
20 
k 
(a) 
i <t.X[k] 
7r 
k 
x denotes indeterminate <t. 
-7T 
(magnitude 
0) 
(b) 
Figure 8.2 
Magnitude and phase ofthe Fourier series coefficients of the sequence 
of Figure 8.1. 
We have shown that any periodic sequence can be represented as a sum ofcomplex 
exponential sequences. The key results are summarized in Eqs. (8.11) and (8.12). As 
we will see, these relationships are the basis for the DFf, which focuses on finite-length 
sequences. Before discussing the DFf, however, we will consider some of the basic 
properties of the DFS representation of periodic sequences in Section 8.2, and then, in 
Section 8.3, we will show how we can use the DFS representation to obtain a DTFT 
representation of periodic signals. 
8.2 PROPERTIES OF THE DFS 
Just as with Fourier series and Fourier and Laplace transforms for continuous-time 
signals, and with discrete-time Fourier and z-transforms for nonperiodic sequences, 
certain properties of the DFS are of fundamental importance to its successful use in 
signal-processing problems. In this section, we summarize these important properties. 
It is not surprising that many of the basic properties are analogous to properties of the 
z-transform and DTFf. However, we will be careful to point out where the periodicity 
of both i[n] and X[k] results in some important distinctions. Furthermore, an exact 
duality exists between the time and frequency domains in the DFS representation that 
does not exist in the DTFf and z-transform representation of sequences. 
liI.c 

629 
Section 8.2 
Properties of the DFS 
8.2.1 Linearity 
Consider two periodic sequences xl[n] and x2[n], both with period N, such that 
(S.19a) 
and 
(S.19b) 
Then 
(S.20) 
This linearity property follows immediately from the form of Eqs. (S.l1) and (S.12). 
8.2.2 Shift of a Sequence 
If a periodic sequence i[n] has Fourier coefficients X[k], then i[n 
m] is a shifted 
version of x[n], and 
x[n - m] 
(S.21) 
The proof of this property is considered in Problem S.55. Note that any shift that is 
greater than or equal to the period (i.e., m ::: N) cannot be distinguished in the time 
domain from a shorter shift ml such that m 
ml + m2N, where ml and m2 are integers 
and 0 ::5 ml ::5 N -
1. (Another way of stating this is that ml 
m modulo Nor, 
equivalently, m1 is the remainder when m is divided by N.) It is easily shown that with 
this representation of m, wtm 
W~nl; i.e., as it must be, the ambiguity of the shift in 
the time domain is also manifest in the frequency-domain representation. 
Because the sequence of Fourier series coefficients of a periodic sequence is a 
periodic sequence, a similar result applies to a shift in the Fourier coefficients by an 
integer e. Specifically, 
WNnfi[n] ~ X [k - n 
(S.22) 
Note the difference in the sign of the exponents in Eqs. (S.21) and (S.22). 
8.2.3 Duality 
Because of the strong similarity between the Fourier analysis and synthesis equations 
in continuous time, there is a duality between the time domain and frequency domain. 
However, for the DTFT of aperiodic signals, no similar duality exists, since aperiodic 
signals and their Fourier transforms are very different kinds of functions: Aperiodic 
discrete-time signals are, of course, aperiodic sequences, whereas their DTFTs are al­
ways periodic functions of a continuous frequency variable. 
From Eqs. (S.11) and (S.12), we see that the DFS analysis and synthesis equations 
differ only in a factor of 1/N and in the sign of the exponent of WN. Furthermore, a 
periodic sequence and its DFS coefficients are the same kinds of functions; they are both 

630 
Chapter 8 
The Discrete Fourier Transform 
periodic sequences. Specifically, taking account of the factor 1/N and the difference in 
sign in the exponent between Eqs. (8.11) and (8.12), it follows from Eq. (8.12) that 
N-l 
Ni[-n] L X[k]W~n 
(8.23) 
k=O 
or, interchanging the roles of nand k in Eq. (8.23), 
N-l 
k
Nx[-kJ = LX [n]WN. 
(8.24) 
n=O 
We see that Eq. (8.24) is similar to Eq. (8.11). In other words, the sequence of DFS 
coefficients of the periodic sequence X[n] is Nx[-k], i.e., the original periodic sequence 
in reverse order and multiplied by N. TIlls duality property is summarized as follows: 
If 
x[n] 
X[k], 
(8.25a) 
then 
X[n] ~ Nx[-k]. 
(8.25b) 
8.2.4 Symmetry Properties 
As we discussed in Section 2.8, the Fourier transform of an aperiodic sequence has 
a number of useful symmetry properties. The same basic properties also hold for the 
DFS representation of a periodic sequence. The derivation of these properties, which is 
similar in style to the derivations in Chapter 2, is left as an exercise. (See Problem 8.56.) 
The resulting properties are summarized for reference as properties 9-17 in Table 8.1 
in Section 8.2.6. 
8.2.5 Periodic Convolution 
Let Xl [n] and x2[n] be two periodic sequences, each with period N and with DFS coef­
ficients denoted by XI[k] and X2[k), respectively. If we form the product 
X3[k] = Xt[k)X2[k], 
(8.26) 
then the periodic sequence x3[n) with Fourier series coefficients X3[k] is 
N-l 
x3[n) L x}[m]x2[n - m]. 
(8.27) 
m=O 
This result is not surprising, since our previous experience with transforms suggests 
that multiplication of frequency-domain functions corresponds to convolution of time­
domain functions and Eq. (8.27) looks very much like a convolution sum. Equation (8.27) 
involves the summation of values of the product of xt[m] with x2[n -
m], which is a 
time-reversed and time-shifted version of x2[m], just as in aperiodic discrete convo­
lution. However, the sequences in Eq. (8.27) are all periodic with period N, and the 
summation is over only one period. A convolution in the form of Eq. (8.27) is referred 

Section 8.2 
Properties of the DFS  
631 
to as a periodic convolution. Just as with aperiodic convolution, periodic convolution is 
commutative; i.e., 
N-l 
x3[n] L x2[m]xl[n - m].  
(8.28) 
m=O 
To demonstrate that X3[k]. given by Eq. (8.26), is the sequence of Fourier coeffi­
cients corresponding to x3[n] given by Eq. (8.27), let us first apply Eq. (8.11), the DFS 
analysis equation, to Eq. (8.27) to obtain 
N-J (N-l 
)
X3[k] = ~ EXl [m]X2[n - m] wtn ,  
(8.29) 
which, after we interchange the order of summation, becomes 
(8.30) 
The inner sum on the index n is the DFS for the shifted sequence x2[n - mJ. Therefore, 
from the shifting property of Section 8.2.2, we obtain 
N-lL x2[n 
m]W~ 
W,t'n Xz[k], 
n=O 
which can be substituted into Eq. (8.30) to yield 
X3 Ik] =Ex, 1m ]W~mX21k] (E Xl 1m ]W~m) X21k] = X, [kj X2[kJ. 
(8.31) 
In summary, . 
N-lL xl[m]x2[n 
mJ  
(8.32) 
m=O 
The periodic convolution of periodic sequences thus corresponds to multiplication of 
the corresponding periodic sequences of Fourier series coefficients. 
Since periodic convolutions are somewhat different from aperiodic convolutions, 
it is worthwhile to consider the mechanics of evaluating 
(8.27). First, note that 
Eq. (8.27) calls forthe product ofsequencesxllmJ andx2[n 
mJ 
x2[-(m -n)] viewed 
as functions of m with n fixed. This is the same as for an aperiodic convolution, but with 
the following two major differences: 
1.  The sum is over the finite interval 0 ~ m ~ N 
1. 
2.  The values of x2ln 
m] in the interval 0 ~ m ~ N - 1 repeat periodically for m 
outside of that interval. 
These details are illustrated by the following example. 

632  
Chapter 8 
The Discrete Fourier Transform 
Example 8.4 
Periodic Convolution 
An illustration of the procedure for forming the periodic convolution of two periodic 
sequences corresponding to Eq. (8.27) is given in Figure 8.3, wherein we have illus­
trated the sequences x2[m], xl[m], X2[-m], x2[1-m] 
X2[ -em 
1)], and x2[2 - ml 
x2[-em - 2)]. To evaluate x3[n] in Eq. (8.27) for n 
2, for example, we multiply xl[m] 
by x2[2 
ml and then sum the product terms Xl [mlx2[2 
ml for 0 
m:s N 
1,ob­
taining X3 [2]. As n changes, the sequence x2[n 
ml shifts appropriately, and Eq. (8.27) 
is evaluated for each value of 0 :s n :s N 
1. Note that as the sequence x2[n 
ml 
shifts to the right or left, values that leave the interval between the dotted lines at 
one end reappear at the other end because of the periodicity. Because of the period­
icity of x3[n], there is no need to continue to evaluate Eq. (8.27) outside the interval 
O<n:sN 
1. 
:  
x2[m) 
I . 
I 
I 
I 
~J.iliu
10  
IN 
m
-N 
I 
I  
I 
I  
I  
: 
xdm] 
I 
I
I I I I ~!l I I I 
I 
I 
-N 
10 
IN 
m 
I  
I 
I 
:  
x2[-ml 
Lull Ii. ! rI [ II Iio 1 ! III [ II 
-N 
10 
IN  
m 
I 
I  
I 
I 
X2 [1- ml = iz(-(m 1)] 
: 
I  
: 
I 
I 
~ ! ! [ II IiI. ! rI [ I IiI 0 1 ! I II I 
-N 
10 
IN  
m 
I 
I  
I 
I  
:  
: ~~ ~ ~~~ ~ 
~II 01 ! I IIiI I . ! rI [ IiI I 0 1 ! III I 
-N 
10 
IN  
m 
I 
I 
Figure 8.3 
Procedure for forming the periodiC convolution of two periodiC 
sequences. 

633 
Section 8.3 
The Fourier Transform of Periodic Signals 
The duality theorem in Section 8.2.3 suggests that if the roles of time and frequency 
are interchanged, we will obtain a result almost identical to the previous result. That is, 
the periodic sequence 
(8.33) 
where 
[n] and X2 [n] are periodic sequences, each with period N, has the DFS coeffi­
cients given by 
N-l 
X3[k] = ~ L Xdl]X2[k -l],  
(8.34) 
£=0 
corresponding to lIN times the periodic convolution of Xdk] and X2[k]. This result 
can also be verified by substituting X3[k], given by Eq. (8.34), into the Fourier series 
relation of Eq. (8.12) to obtain x3[n]. 
8.2.6  Summary of Properties of the DFS Representation 
of Periodic Sequences 
The properties of the DFS representation discussed in this section are summarized in 
Table 8.1. 
8.3 THE FOURIER TRANSFORM OF PERIODIC SIGNALS 
As discussed in Section 2.7, uniform convergence ofthe Fourier transform of a sequence 
requires that the sequence be absolutely summable, and mean-square convergence re­
quires that the sequence be square summable. Periodic sequences satisfy neither con­
dition. However, as we discussed briefly in Section 2.7, sequences that can be expressed 
as a sum of complex exponentials can be considered to have a Fourier transform rep­
resentation in the form of Eq. (2.147), i.e., as a train of impulses. Similarly, it is often 
useful to incorporate the DFS representation of periodic signals within the framework 
of the discrete-time Fourier transform. This can be done by interpreting the discrete­
time Fourier transform of a periodic signal to be an impulse train in the frequency 
domain with the impulse values proportional to the DFS coefficients for the sequence. 
Specifically, if i[n] is periodic with period N and the corresponding DFS coefficients 
are X[k], then the Fourier transform ofi[n] is defined to be the impulse train 
-. 
~ 2rr -
( 
(8.35)
X(eJW ) = '--' NX [k]8 
w 
k=-oo 
Note that X(ejW ) has the necessary periodicity with period 2rr since X[k] is periodic 
with period N, and the impulses are spaced at integer multiples of 2rr IN, where N is an 

634 
Chapter 8 
The Discrete Fourier Transform 
TABLE 8.1 
SUMMARY OF PROPERTIES OF THE DFS 
Periodic Sequence (Period N)  
DFS Coefficients (Period N) 
1.  ilnl 
2.  xl[nl, i2[n] 
3.  aXl[nl + bxZ[n] 
4.  X [n] 
5.  x[n 
ml 
6.  WHeni[nl  
N-l  
7.  L il[mli2[n 
m] (periodic convolution) 
m=(} 
8.  il[n]i2[n] 
9.  i*[n] 
10. i*[-nl 
11. 
Re{i[nlJ 
12. 
jIm{i[nlJ 
13. 
xe[nl = !(.~[n] +i*[-n]) 
14. 
io[n] = !(i[n] 
x*[-n])  
Properties 15-17 apply only when x[n] is real.  
15. Symmetry properties for ifnI real. 
16. 
xe[n] = ~(i[nl +i[-n]) 
17. 
.~o[n] 
~(.~[n] - i[-nl) 
X[k] periodic with period N 
Xl [k]. X2[kl periodic with period N 
aXIlkl +bX2[k] 
Ni[-kJ 
WtmX[k] 
X[k - lJ 
Xllk]X2[kJ 
N-l 
~ L Xl[llX:z[k-l] (periodic convolution) 
e=o 
X*[-k]  
X*[k]  
Xe[k] = ~ (X [k] + X*[-k])  
Xo[k] 
1(X [k] 
X*[-k])  
Re{X[kIJ  
jIm(X [kll 
I 
X[k] 
X*[-k]  
Re{X [kll = RelX [-klJ  
Im{~ [k]} 
-::.ImIX [-k]}  
IX [kll 
IX [-kll 
LX[k] = -LXl-k] 
Re(X [k]J 
jIm{X[klJ 
integer. To show that X(e jUJ ) as defined in Eq. (8.35) is a Fourier transform represen­
tation of the periodic sequence i[n], we substitute Eq. (8.35) into the inverse Fourier 
transform Eq. (2.130); i.e., 
2JT  
2JT 
E
1 1 -€ 
X(ejw)eiUJndw = 
1 1 - L
00 
2]( 
[k]8 
(J) - N2](k).
eJUJndw, 
(8.36)
2]( 
O-E 
2]( 0-. 
N 
(
k=-oo 
where IE satisfies the inequality 0 < IE < (2](/N). Recall that in evaluating the inverse 
Fourier transform, we can integrate over any interval of length 2](, since the integrand 
X(ejW)ejwn is periodic with period 2](. In Eq. (8.36) the integration limits are denoted 
0- IE and 2]( - E, which means that the integration is from just before w = 0 to just before 
w 
2](. These limits are convenient, because they include the impulse at w = 0 and 

Section 8.3 
The Fourier Transform of Periodic Signals 
635 
exclude the impulse at w = 21i.3 Interchanging the order of integration and summation 
leads to 
21ik) jwn
-­
e 
dw 
N 
(8.37)
N-l 
= ~ LX [k]e j (2;rIN)kn. 
k=O 
The final form of Eq. (8.37) results because only the impulses corresponding to 
k = 0, 1, ... , (N -1) are included in the interval between WOE and w 
21i -
E. 
Comparing Eq. (8.37) and Eq. (8.12), we see that the final right-hand side of 
Eq. (8.37) is exactly equal to the Fourier series representation for x[n], as specified by 
Eq. (8.12). Consequently, the inverse Fourier transform of the impulse train in Eq. (8.35) 
is the periodic signali[n], as desired. 
Although the Fourier transform of a periodic sequence does not converge in the 
normal sense, the introduction of impulses permits us to include periodic sequences 
formally within the framework of Fourier transform analysis. This approach was also 
used in Chapter 2 to obtain a Fourier transform representation of other nonsummable 
sequences, such as the two-sided constant sequence (Example 2.19) or the complex 
exponential sequence (Example 2.20). Although the DFS representation is adequate 
for most purposes, the Fourier transform representation of Eq. (8.35) sometimes leads 
to simpler or more compact expressions and simplified analysis. 
Example 8.5 The Fourier Transform of a Periodic 
Discrete-Time Impulse Train 
Consider the periodic discrete-time impulse train 
p[n] = L
00 
8[n 
rN], 
(8.38) 
r=-oo 
which is the same as the periodic sequence x[n] considered in Example 8.1. From the 
results of that example, it follows that 
P[k] = 1, 
for all k. 
(8.39) 
Therefore, the DTFf of p[n1is 
,.
P(eJW ) = 002Jl'(
L !i 8 w 
(8.40) 
k=-oo 
The result of Example 8.5 is the basis for a useful interpretation of the relation 
between a periodic signal and a finite-length signal. Consider a finite-length signalx[n] 
such that x [n] = 0 except in the interval 0 :::: n :::: N 
1, and consider the convolution 
3The limits 0 to 2n would present a problem since the impulses at both 0 and 2n would require special 
handling. 

636  
Chapter 8 
The Discrete Fourier Transform 
I 
.'i'[n] 
'u III I • II 1 I I . I I I It! n
-N 
0 
N 
x[n]  
Figure 8.4 Periodic sequence x[n] 
formed by repeating afinite-length 
sequence, x[n], periodically.
II 1 I I • •  
Alternatively, x[n] 
x[n] over one
• • • • • •  
• • • • • 
n
0 
N  
period and is zero otherwise. 
of x[n] with the periodic impulse train p[n] of Example 8.5: 
00  
00 
.t[n] = x[n] * p[n] = x[n] * L 8[n - rN] = .L x[n - rN]. 
(8.41) 
r=-C(;; 
r==-oo 
Equation (8.41) states that i[nJ consists of a set of periodically repeated copies of the 
finite-length sequence x[n]. Figure 8.4 illustrates how a periodic sequence i[nl can be 
formed from a finite-length sequence x[n] through Eq. (S.41). The Fourier transform of 
x[nJ is X (ejtO) , and the Fourier transform of i[n] is 
X(eiw ) = X (eiw ) p(eiw ) 
. 
00 
2n (
= X(e JW ) L li8 UJ 
2~k)  
(8.42) 
k==-oo 
= f 2; X(e j (27r/N)k)8 (UJ 
2~k) . 
k=-oo 
Comparing Eq. (8.42) with Eq. (8.35), we conclude that 
X[k] = X(e j (2rr/N)k) = X(e jW)\. 
(8.43) 
w=(2rr/N)k 
In other words, the periodic sequence X [k] of DFS coefficients in Eq. (8.11) has an 
discrete-time interpretation as equally spaced samples of the DTFT of the finite-length 
sequence obtained by extracting one period of i[n]; i.e., 
x[n] = {i[nJ, 
0:::: n ~ N -1,  
(8.44) 
. 
0, 
otherWIse. 
This is also consistent with Figure 8.4, where it is clear that x[n] can be obtained from 
iln] using Eq. (S.44). We can verify Eq. (8.43) in yet another way. Since x[n] 
i[n] for 
0:::: n :::: N 
1 and x[n] = 0 otherwise, 
N-l 
N-l 
X(eiw ) = L x[n]e- jwn L i[n]e- jwn . 
(8.45) 
n=O 
n=O 
Comparing Eq. (8.45) and Eq. (8.11), we see again that 
X[k] = X (eiW )!w=2Jrk/N. 
(8.46) 
This corresponds to sampling the Fourier transform at N equally spaced frequencies 
between UJ = 0 and u) = 2n with a frequency spacing of 2nIN. 

637 
Section 8.3 
The Fourier Transform of Periodic Signals 
Example 8.6 
Relationship Between the Fourier Series 
Coefficients and the Fourier Transform of One Period 
We again consider the sequence x[n] of Example 8.3, which is shown in Figure 8.l. 
One period of .t[n] for the sequence in Figure 8.1 is 
I, 
0::: n ::: 4,
x[n] 
(8.47)
{ 0, 
otherwise. 
The Fourier transform of one period of x[n] is given by 
X(e}W) = t e- jwn = e- j2w sin(5wj2). 
(8.48) 
, 
71=0 
sin(wj2) 
Equation (8.46) can be shown to be satisfied for this example by substituting 
(J) 
2JrkjlO into Eq. (8.48), giving 
X[k] = - j(4nk/1O) sin(Jrkj2) 
e 
sin(JrkjlO)' 
which is identical to the result in Eq. (8.18). The magnitude and phase of X(ejW ) are 
sketched in Figure 8.5. Note that the phase is discontinuous at the frequencies where 
X(e jW) = O. That the sequences in Figures 8.2(a) and (b) correspond to samples of 
Figures 8.5(a) and (b), respectively, is demonstrated in Figure 8.6, where Figures 8.2 
and 8.5 have been superimposed. 
w 
(a) 
7T 
w 
(b) 
Figure 8.5 
Magnitude and phase of the Fourier transform of one period of the 
sequence in Figure 8.1. 

638 
Chapter 8 
The Discrete Fourier Transform 
IX(ej"')I,IX[k]1 
5 
o 
11" 
o 
<lX(e j",), <):X[k) 
11" 
211" 
311" 
411" 
w 
k
10 
20 
(a) 
w 
k 
-11" 
(b) 
Figure 8.6 
Overlay of Figures 8.2 and 8.5 illustrating the DFS coefficients of a 
periodic sequellce as samples of the Fourier transform of one period. 
8.4 SAMPLING THE FOURIER TRANSFORM 
In this section, we discuss with more generality the relationship between an aperiodic 
sequence with Fourier transform X(eiw ) and the periodic sequence for which the DFS 
coefficients correspond to samples of X(eiw) equally spaced in frequency. We will find 
this relationship to be particularly important when we discuss the discrete Fourier trans­
form and its properties later in the chapter. 
Consider an aperiodic sequence x[n] with Fourier transform X(ei(f), and assume 
that a sequence X[k] is obtained by sampling X (eiw) at frequencies Wk = 2lCk/N; i.e., 
X[k] = X(eiW )I{v=(21T/N)k = X(e J(21T/N)k). 
(8.49) 
Since the Fourier transform is periodic in W with period 2lC, the resulting sequence is 
periodic in k with period N. Also, since the Fourier transform is equal to the z-transform 
evaluated on the unit circle, it follows that X [k] can also be obtained by sampling X (z) 
at N equally spaced points on the unit circle. Thus, 
X[k] = X(z) 
= X(eJ(21T/N)k). 
(8.50) 
These sampling points are depicted in Figure 8.7 for N = 8. The figure makes it clear 
that the sequence of samples is periodic, since the N points are equally spaced starting 
with zero angle. Therefore, the same sequence repeats as k varies outside the range 
o:::: k :::: N 
1 since we simply continue around the unit circle visiting the same set of 
N points. 

639 
Section 8.4 
Sampling the Fourier Transform 
Unit 
circle 
Figure 8.7 
Points on the unit circle at 
which XCZ) is sampled to obtain the 
periodic sequence Xlk] (N = 8). 
Note that the sequence of samples X[k], being periodic with period N, could be 
the sequence of DFS coefficients of a sequence .t[n]. To obtain that sequence, we can 
simply substitute X[k] obtained by sampling into Eq. (8.12): 
1 N-l 
kn
.t[n] = N LX [k]WN
• 
(8.51) 
k=O 
Since we have made no assumption about x[n] other than that the Fourier transform 
exists, we can use infinite limits to indicate that the sum is 
oc 
X(eju» = L x[m]e-ju>m 
(8.52) 
m=-oc 
is over all nonzero values of x[m]. 
Substituting Eq. (8.52) into Eq. (8.49) and then substituting the resulting expres­
sion for X[k] into 
(8.51) gives 
IN-l[oo 
] 
kn
.t[n] = N {; m~oo x[m]e- j (2JT/N)km 
WN
, 
(8.53) 
which, after we interchange the order of summation, becomes 
m].
xlnJ ~mtxlm] [ ~ ~wN,,·-m,] ~ m~oo xlmJPIn 
(8.54) 
The term in brackets in Eq. (8.54) can be seen from either Eq. (8.7) or Eq. (8.16) 
to be the Fourier series representation of the periodic impulse train of Examples 8.1 
and 8.2. Specifically, 
N-l 
00 
-[ 
] 
1 L W-k(n-m)
pn-m=-
N 
L S[n 
m 
rN] 
(8.55)
N k=O 
r=-oc 
and therefore, 
00 
00 
.t[n] = x[n] * L S[n - rN] L x[n 
rN], 
(8.56) 
r=.-oo 
r=-oo 
where *denotes aperiodic convolution. That is,.t [n] is the periodic sequence that results 
from the aperiodic convolution of x[n] with a periodic unit-impulse train. Thus, the 

640 
Chapter 8 
The Discrete Fourier Transform 
x[n] 
• • • • • • • • • • • • • • 
• • • • • • • 
n
o 
8 
(a) 
co 
x[n] = 2: x[n - r12]
r 
-00 
n 
N= 12 
(b) 
Figure 8.8 
(a) Finite-length sequence x[n]. (b) Periodic sequence x[n] corre­
sponding to sampling the Fourier transform of x[nl with N 
12. 
periodic sequence x[n], corresponding to X[kJ obtained by sampling X (ej{J)) , is fonned 
from x[n] by adding together an infinite number of shifted replicas of x[n]. The shifts 
are all the positive and negative integer multiples of N, the period of the sequence 
X[k]. This is illustrated in Figure 8.8, where the sequence x[n] is of length 9 and the 
value of N in Eq. (8.56) is N 
12. Consequently, the delayed replications of x[n] do 
not overlap, and one period of the periodic sequence x[n] is recognizable as x[n]. This 
is consistent with the discussion in Section 8.3 and Example 8.6, wherein we showed 
that the Fourier series coefficients for a periodic sequence are samples of the Fourier 
transform of one period. In Figure 8.9 the same sequence x[n] is used, but the value 
of N is now N 
7. In this case, the replicas of x[n] overlap and one period of x[n] is 
no longer identical to x[nl. In both cases, however, Eq. (8.49) still holds; i.e., in both 
cases, the DFS coefficients of x[n] are samples of the Fourier transform of x[n] spaced 
00 
x[n] 
2: x[n r7]
r=-::x) 
JI I IlllJI I I1II !II111111 IllllIll!! 
n 
N 
7 
Figure 8.9 
Periodic sequence x[n] corresponding to sampling the Fourier trans­
form of x[n] in Figure 8.8(a) with N 
7. 
III.. 

641 
Section 8.4 
Sampling the Fourier Transform 
in frequency at integer multiples of 2n: / N. This discussion should be reminiscent of our 
discussion of sampling in Chapter 4. The difference is that here we are sampling in the 
frequency domain rather than in the time domain. However, the general outlines of the 
mathematical representations are very similar. 
For the example in Figure 8.8, the original sequence x[nl can be recovered from 
x[n] by extracting one period. Equivalently, the Fourier transform X(e jUJ ) can be re­
covered from the samples spaced in frequency by 2n/12. In contrast, in Figure 8.9, 
x[n] cannot be recovered by extracting one period of x[n], and, equivalently, X(e jUJ ) 
cannot be recovered from its samples ifthe sample spacing is only 2n:/7. In effect, for 
the case illustrated in Figure 8.8, the Fourier transform of x[nJ has been sampled at a 
sufficiently small spacing (in frequency) to be able to recover it from these samples, 
whereas Figure 8.9 represents a case for which the Fourier transform has been under­
sampled. The relationship between x[n] and one period of x[n] in the undersampled 
case can be thought of as a form of aliasing in the time domain, essentially identical 
to the frequency-domain aliasing (discussed in Chapter 4) that results from undersam­
pIing in the time domain. Obviously, time-domain aliasing can be avoided only if x[n] 
has finite length, just as frequency-domain aliasing can be avoided only for signals that 
have bandlimited Fourier transforms. 
This discussion highlights several important concepts that will playa central role 
in the remainder of the chapter. We have seen that samples of the Fourier transform of 
an aperiodic sequence x [n] can be thought of as DFS coefficients of a periodic sequence 
x[n] obtained through summing periodic replicas of x[n]. If x[n] is finite length and we 
take a sufficient number of equally spaced samples of its Fourier transform (specifically, 
a number greater than or equal to the length of x[n]), then the Fourier transform is 
recoverable from these samples, and, equivalently, x[nJ is recoverable from the corre­
sponding periodic sequence x[n]. Specifically, if x[n] 
0 outside the interval n 
0, 
n = N -1, then 
_ {x[n]' 
0:::: n:::: N 
1,
[ ] 
(8.57)
x n -
0, 
otherwise. 
If the interval of support of x[n] is different than 0, N 
1 then Eq. (8.57) would be 
appropriately modified. 
A direct relationship between X (ejUJ ) and its samples X [k], Le., an interpolation 
formula for X(e jW ), can be derived (see Problem 8.57). However, the essence of our 
previous discussion is that to represent or to recover x[n], it is not necessary to know 
X(e jW ) at all frequencies if x[n] has finite length. Given a finite-length sequence x[n], 
we can form a periodic sequence using Eq. (8.56), which in tum can be represented by 
a DFS. Alternatively, given the sequence of Fourier coefficients X[k], we can find x[n] 
and then use Eq. (8.57) to obtain x[n]. When the Fourier series is used in this way to 
represent finite-length sequences, it is called the discrete Fourier transform or DFf. In 
developing, discussing, and applying the DFf, it is always important to remember that 
the representation through samples of the Fourier transform is in effect a representation 
ofthe finite-duration sequence by a periodic sequence, one period of which is the finite­
duration sequence that we wish to represent. 

642  
Chapter 8 
The Discrete Fourier Transform 
8.5  FOURIER REPRESENTATION OF FINITE-DURATION 
SEQUENCES: THE DFT 
In this section, we formalize the point of view suggested at the end of the previous 
section. We begin by considering a finite-length sequence x[n] of length N samples such 
that x[n] = 0 outside the range 0 ::s n ::s N 
1. In many instances, we will want to 
assume that a sequence has length N, even if its length is M ::s N. In such cases, we 
simply recognize that the last (N - M) samples are zero. To each finite-length sequence 
of length N, we can always associate a periodic sequence 
i[n] L
00 
x[n -rN].  
(8.58a) 
r=-oo 
The finite-length sequence x[n] can be recovered from i[n] through Eq. (S.57), i.e., 
o::s n ::s N 
1,
x[n] = {i[n], 
(8.58b)
0, 
otherwise. 
Recall from Section S.4 that the DFS coefficients of i[n] are samples (spaced in 
frequency by 21f/N) of the Fourier transform of x[n]. Since x[n] is assumed to have 
finite length N, there is no overlap between the terms x[n 
r N ] for different values of 
r. Thus, Eq. (8.58a) can alternatively be written as 
i[n] = x[(n modulo N)].  
(S.59) 
For convenience, we will use the notation «n))N to denote (n modulo N); with this 
notation, Eq. (S.59) is expressed as 
i[n] 
x[«n))N].  
(S.60) 
Note that Eq. (8.60) is equivalent to Eq. (S.5Sa) only when x[n] has length less than or 
equal to N. The finite-duration sequence x[n] is obtained from i[n] by extracting one 
period, as in Eq. (8.5Sb). 
One informal and useful way of visualizing Eq. (8.59) is to think of wrapping a plot 
of the finite-duration sequence x [n] around a cylinder with a circumference equal to the 
length of the sequence. As we repeatedly traverse the circumference of the cylinder, we 
see the finite-length sequence periodically repeated. With this interpretation, represen­
tation of the finite-length sequence by a periodic sequence corresponds to wrapping the 
sequence around the cylinder; recovering the finite-length sequence from the periodic 
sequence using Eq. (8.58b) can be visualized as unwrapping the cylinder and laying it fiat 
so that the sequence is displayed on a linear time axis rather than a circular (modulo N) 
time axis. 

643 
Section 8.5 
Fourier Representation of Finite-Duration Sequences 
As defined in Section 8.1, the sequence of DFS coefficients X[k] of the periodic 
sequence x[n] is itself a periodic sequence with period N. To maintain a duality between 
the time and frequency domains, we will choose the Fourier coefficients that we associate 
with a finite-duration sequence to be a finite-duration sequence corresponding to one 
period of X[k]. This finite-duration sequence, X [k], will be referred to as the DFT. Thus, 
the DFT, X [k], is related to the DFS coefficients, X[k], by 
I
X[k], 
OSkSN-l, 
X [k] = 
(8.61) 
0, 
otherwise, 
and 
X[k] 
X[(k modulo N)] = X[«(k))N]. 
(8.62) 
From Section 8.1, X[kJ and x[n] are related by 
N-l 
X[k] 
Lx[n]wtn, 
(8.63) 
n=O 
1 N-l 
kn
x[n] 
N L X[k]WN
. 
(8.64) 
k=O 
where WN = e-j (2n/N). 
Since the summations in Eqs. (8.63) and (8.64) involve only the interval between 
zero and (N - 1), it follows from Eqs. (8.58b) to (8.64) that 
N-l 
Lx[n]Wtn , 
O:sk:sN-l, 
X[k] = 
n=O 
(8.65) 
I0, 
otherwise, 
1 N-l 
kn
LX [k]WN
, 
O:sn:sN-l, 
x[n] = 
N k=O 
(8.66)
10, 
otherwise. 

644 
Chapter 8 
The Discrete Fourier Transform 
Generally, the DFT analysis and synthesis equations are written as follows: 
N-l 
Analysis equation: X [k] = L x[n]W!n, 
O<k<N 
1, 
(8.67) 
n=O 
N-l 
kn
Synthesis equation: 
x[n] = ~ LX [k]WN
, 
o::s n ::s N 
1. 
(8.68) 
k=O 
That is, the fact that X fk] = 0 for k outside the interval 0 ::s k ::s N 
1 and that x[n] = 0 
for n outside the interval 0 ::s n ::s N - 1 is implied, but not always stated explicitly. The 
relationship between xfn] and X [k] implied by Eqs. (8.67) and (8.68) will sometimes be 
denoted as 
V:F:J
x[n] ~ X [k]. 
(8.69) 
In recasting Eqs, (8.11) and (8.12) in the form of Eqs. (8.67) and (8.68) for finite­
duration sequences, we have not eliminated the inherent periodicity. As with the DFS, 
the DFT X fk] is equal to samples of the periodic Fourier transform X(e jW ), and if 
Eq. (8.68) is evaluated for values of n outside the interval 0 ::s n 
N 
1, the result 
will not be zero, but rather a periodic extension of x[n]. The inherent periodicity is 
always present. Sometimes, it causes us difficulty, and sometimes we can exploit it, but 
to totally ignore it is to invite trouble. In defining the DFT representation, we are simply 
recognizing that we are interested in values of x[n] only in the interval 0 
n < N 
1, 
because x[n] is really zero outside that interval, and we are interested in values of X [k] 
only in the interval 0 ::s k ::s N 
1 because these are the only values needed in Eq. (8.68) 
to reconstruct X[n]. 
Example 8.7 
The DFT of a Rectangular Pulse 
To illustrate the DFIofa finite-duration sequence, consider x En] shown in Figure 8.1O(a). 
In determining the DFI, we can consider x[n] as a finite-duration sequence with any 
length greater than or equal to N 
5. Considered as a sequence of length N 
5, the 
periodic sequence x[n] whose DFS corresponds to the DFI of x[n] is shown in Fig­
ure 8.1O(b). Since the sequence in Figure 8.1O(b) is constant over the interval 0 
11::: 4, 
it follows that 
4 e-j(2;r:k/5)n _ 
1 - e- j2rrk
x [k) 
n=O 
1 - e- j(2;r:k/5) 
(8.70) 
5, 
k 
0, ±5, ±lO, ... , 
{ 0, 
otherwise; 
i.e., the only nonzero DFS coefficients X[k1 are at k = 0 and integer multiples of 
k = 5 (all of which represent the same complex exponential frequency). The DFS 
coefficients are shown in Figure 8.1O(c). Also shown is the magnitude of the DTFT, 
IX(eiw)l. Clearly, X[k] is a sequence of samples of X(e jW ) at frequencies (Uk 
2rrk/5. 
According to Eq. (8.61), the five-point DFI of x[n1 corresponds to the Hnite-Iength 
sequence obtained by extracting one period of X[k). Consequently, the Hve-pointDFT 
of x[n] is shown in Figure 8.10(d). 

• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
645 
Section 8.5 
Fourier Representation of Finite-Duration Sequences 
1 
x[n] 
n
o 
4 
(a) 
x[n] 
) 
e 
o 
5 
10 
15 
20 
n 
(b) 
:­
5 
X[k]
;, 
if 
It 
is 
It 
-1 
0 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
k 
o 
27T 
47T 
W 
(c) 
r. 
X[k] 
-2 
-1 
o 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
k 
(d) 
Figure 8.10 Illustration of the OFf. (a) Finite-length sequence x[n]. (b) Periodic 
sequence x[n) formed from x[n] with period N 
5. (c) Fourier series coefficients 
X[k] for x[n]. To emphasize that the Fourier series coefficients are samples of the 
Fourier transform, IX(eiw )I is also shown. (d) OFf of x[n]. 
If, instead, we consider x[n] to be oflength N = 10, then the underlying periodic 
sequence is that shown in F~gure 8.ll(b), which is the periodic sequence considered 
in Example 8.3. Therefore, X [k] is as shown in Figures 8.2 and 8.6, and the lO-point 
DFT X [k] shown in Figures 8.ll(c) and 8.U(d) is one period of X[k]. 
I 
I 
I 
I 
I 
I 
.... 
IX(ei"') I
\/
\ 
\ 
\ 
\ ,­
1/ 

646 
Chapter 8 
The Discrete Fourier Transform 
........'1 I I I I . . . . . .. '~l  
o 
4 
n 
(a) 
x[n] 
I I I I I .....'I I I I I ..... I I I  
-10 
0 
4 
10 
n 
(b) 
5\ 
IX[k]1
3.24 
3.24 
1.24 
1 
1.24 
. . . . . . . . . . -I . I . ! 
. I . . . 
• I 
-10 
0 
10 
k 
(c) 
LX[k]
0.4 71" 
0.271" 
-10 
o 
10 
k 
-0.4 71" 
(d) 
Figure 8.11 
Illustration of the OFT. (a) Finite-length sequence x[n]. (b) Periodic 
sequence x[n] formed from x[n] with period N = 10. (c) DFT magnitude. (d) DFT 
phase. (x's indicate indeterminate values.) 
The distinction between the finite-duration sequence x[n] and the periodic se­
quence x[n] related through Eqs. (8.57) and (8.60) may seem minor, since. by using 
these equations, it is straightforward to construct one from the other. However, the 
distinction becomes important in considering properties of the DIT and in considering 
the effect on x [n] of modifications to X [k]. This will become evident in the next section, 
where we discuss the properties of the DIT representation. 

647 
Section 8.6 
Properties of the OFT 
8.6 PROPERTIES OF THE OFT 
In this section, we consider a number of properties of the DFf for [mite-duration 
sequences. Our discussion parallels the discussion of Section 8.2 for periodic sequences. 
However, particular attention is paid to the interaction of the finite-length assumption 
and the implicit periodicity of the DFf representation of finite-length sequences. 
8.6.1 Linearity 
If two finite-duration sequences Xl [n] and X2 [n] are linearly combined, i.e., if 
(8.71) 
then the DFf of x3[n] is 
x3[k] 
aX 1[k] + bX2[k]. 
(8.72) 
Clearly, if xI[n] has length NI and x2[n] has length N2, then the maximum length of 
x3[n] will be N3 = max(NJ, N2). Thus, in order for Eq. (8.72) to be meaningful, both 
DFfs must be computed with the same length N 2: N 3. If, for example, N I < N 2, then 
X I[k] is the DFf of the sequence xl[n] augmented by (N2 - NI) zeros. That is, the 
N2-pointDFfofxl[n] is 
Nl-I 
(8.73)
X I[k] = L xl[n]Wt~, 
n=O 
and the Nz-point DFf of x2[n] is 
NZ-I 
X 2[k] = L x2[n]Wt~, 
O~k~N2-1. 
(8.74) 
n=O 
In summary, if 
(8.7Sa) 
and 
V:F:J
x2[n] ~ X z[k], 
(8.7Sb) 

648 
Chapter 8 
The Discrete Fourier Transform 
Sectiol 
then 
axdn] +bx2[n] 
aX I[k] + bX 2[k], 
(8.76) 
where the lengths of the sequences and their DFfs are all equal to at least the maximum 
of the lengths of xI[n] and x2[n]. Of course, DFfs of greater length can be computed 
by augmenting both sequences with zero-valued samples. 
8.6.2 Circular Shift of a Sequence 
According to Section 2.9.2 and property 2 in Table 2.2, if X(ejr.tJ) is the discrete-time 
Fourier transform of x[n], then e- jwm X(ejW ) is the Fourier transform of the time-shifted 
sequence x[n 
m]. In other words, a shift in the time domain by m points (with posi­
tive m corresponding to a time delay and negative m to a time advance) corresponds 
in the frequency domain to multiplication of the Fourier transform by the linear-phase 
factor e-jwm. In Section 8.2.2, we discussed the corresponding property for the DFS 
coefficients of a periodic sequence; specifically, if a periodic sequence x[n] has Fourier 
series coefficients X[k], then the shifted sequence x[n 
m] has Fourier series coeffi­
cients e-} (2JTkjN)m X[k]. Now we will consider the operation in the time domain that 
corresponds to multiplying the DFf coefficients of a finite-length sequence x [n] by the 
linear-phase factor e- }(2rrk/N)m. Specifically, let xI[n] denote the finite-length sequence 
for which the DFf is e-j(2rrk/N)m X [k]; i.e., if 
Dr:!
x[n] +------+ X [k], 
(8.77) 
then we are interested in Xl [n] such that 
xI[n] ~ X dk] 
e-}(21l'k/N)m X [k] = W~X[k]. 
(8.78) 
Since the N-point DFf represents a finite-duration sequence of length N, both x[n] and 
xdn] must be zero outside the interval 0 ~ n ~ N 
1, and consequently, xlln] cannot 
result from a simple time shift of x[n]. The correct result follows directly from the result 
of Section 8.2.2 and the interpretation of the DFf as the Fourier series coefficients of 
the periodic sequence Xl [«n»N]. In particular, from Eqs. (8.59) and (8.62) it follows 
that 
Drs ­
x[n] 
x [((n»N ] +------+ X [k] = X[«k»N], 
(8.79) 
and similarly, we can define a periodic sequence xt[n] such that 
xI[n] = xll«n»N] 
XI[k] = X 1[((k»N], 
(8.80) 
where, by assumption, 
X dk] = e-}(2rrk/N)m X [k]. 
(8.81) 

649 
Section 8.6 
s 
~ 
r 
l­
It 
e 
:e 
Properties of the DFT 
Therefore, the DFS coefficients of Xl [n] are 
(8.82) 
Note that 
(8.83) 
That is, since e- } (2:rrk/ N)m is periodic with period N in both k and m, we can drop the 
notation ((k))N. Hence, Eq. (8.82) becomes 
Xdk] = e-}(2rrk/N)m X [k], 
(8.84) 
so that it follows from Section 8.2.2 that 
xdn] = x[n -
m] = x[((n -
m))N]. 
(8.85) 
Thus, the finite-length sequence xdn] whose DFT is given by Eq. (8.81) is 
o:s n :s N - 1, 
(8.86)
otherwise. 
Equation (8.86) tells us how to construct x1[n] from x[n]. 
Example 8.8 Circular Shift of a Sequence 
The circular shift procedure is illustrated in Figure 8.12 for m = -2; i.e., we want 
to determine x1[n] = x[((n + 2))N] for N = 6, which we have shown will have 
OFf X l[k] = W62kX [k]. Specifically, from x[n], we construct the periodic sequence 
x[n] = x[((n))61, as indicated in Figure 8.12(b). According to Eq. (8.85), we then 
shift x [n] by 2 to the left, obtaining Xl [n] = x [n + 2] as in Figure 8.12( c). Finally, using 
Eq. (8.86), we extract one period oU1 [n] to obtain Xl [n], as indicated in Figure 8.12(d). 
A comparison of Figures 8.12(a) and (d) indicates clearly that x1[n] does not 
correspond to a linear shift of x[n], and in fact, both sequences are confined to the 
interval between 0 and (N -1). By reference to Figure 8.12, we see that x1[n] can be 
formed by shifting x[n], so that as a sequence value leaves the interval 0 to (N - 1) at 
one end, it enters at the other end. Another interesting point is that, for the example 
shown in Figure 8.12(a), if we form x2[n] = x[((n - 4))6] by shifting the sequence by 4 
to the right modulo 6, we obtain the same sequence as Xl [n]. In terms of the DFf, this 
= W6 2k
results because Wtk 
or, more generally, w;:l = WN"(N-m)k, which implies 
that an N -point circular shift in one direction by m is the same as a circular shift in the 
opposite direction by N - m. 

650  
Chapter 8 
The Discrete Fourier Transform 
I 
I 
x[nJ 
.\
'1 III11\ ' ...
• • • • • •  
• • • • • 
n
10  
IN 
I  
I 
I  
I
(a)
I 
I 
I 
I 
I I 1 I III li1 I III li1 IIII I II 
x[nJ 
n
10  
IN 
I 
(b)  
I 
I 
I  
I 
Xl n 
X n +2]
1 
l' 
1: 
1 
- [1 -[
IIII t IiI IJt IiI IJt IJt 
10  
IN
I 
I 
n 
I 
I 
I 
(c)  
I 
I  
I 
I  
I
: 
I 
I' 
I 
xILnj.{xl[nl, 
005n05N-l 
I  
: 
0, 
otherwise 
........il III Ii........  
10 
IN 
n 
I 
I 
(d) 
Figure 8.12 
Circular shift of afinite-length sequence; i.e., the effect in the time 
domain of multiplying the OFT of the sequence by a linear-phase factor. 
In Section 8.5, we suggested the interpretation of forming the periodic sequence 
x[n] from the finite-length sequence x[n] by displaying x[n] around the circumference 
of a cylinder with a circumference of exactly N points. As we repeatedly traverse the 
circumference of the cylinder, the sequence that we see is the periodic sequence x[n]. 
A linear shift of this sequence corresponds, then, to a rotation of the cylinder. In the 
context of finite-length sequences and the DFf, such a shift is called a circular shift or 
a rotation of the sequence within the interval 0 .::: n .::: N - 1. 
In summary, the circular shift property of the DFf is 
x[((n - m»N], 
0.::: n'::: N -1 ~ e- j (2rrkIN)mX [k] = WNX[k]. 
(8.87) 
8.6.3 Duality 
Since the DFf is so closely associated with the DFS, we would expect the DFT to exhibit 
a duality property similar to that of the DFS discussed in Section 8.2.3. In fact, from an 
examination of Eqs. (8.67) and (8.68), we see that the analysis and synthesis equations 
differ only in the factor 1/N and the sign of the exponent of the powers of WN. 

651 
Section 8.6 
Properties of the OFT 
The DFT duality property can be derived by exploiting the relationship between 
the DFT and the DFS as in our derivation of the circular shift property. Toward this. 
end, consider x[n] and its DFT X [k], and construct the periodic sequences 
i[n] = x[«n))N], 
(S.SSa) 
X[k] = X [«k»N], 
(S.SSb) 
so that 
i[n] 
X [k]. 
(S.S9) 
From the duality property given in Eqs. (S.25), 
Xen] ~ Ni[-k]. 
(S.90) 
If we define the periodic sequence ii£n] 
X [n], one period of which is the finite­
length sequence xi£n] 
X [n], then the DFS coefficients of ii£n] are XI[k] = Ni[-k]. 
Therefore, the DFT of Xl [n] is 
O:s k :s N 
1,
X 1[k] = {~i[-k], 
(8.91)
otherwise, 
or, equivalently, 
O:s k :s N -1, 
(8.92)
otherwise. 
Consequently, the duality property for the DFT can be expressed as follows: If 
'OF.:!
x[n] ~ X [k], 
(S.93a) 
then 
'OF.:!
X [n] ~ Nx[«-k»N], 
O:sk:SN-1. 
(S.93b) 
The sequence Nx[«-k))N] is Nx[k] index reversed, modulo N. Index-reversing 
modulo N corresponds specifically to «-k»N 
N -k for 1 :s k :s N 
1 and «-k»N 
«k))N for k 
O. As in the case of shifting modulo N, the process of index-reversing 
modulo N is usually best visualized in terms of the underlying periodic sequences. 
Example 8.9 The Duality Relationship for the DFT 
To illustrate the duality relationship in Eqs. (8.93), let us consider the sequence x[n] of 
Example 8.7. Figure 8.13( a) shows the finite-length sequence x[n), and Figures 8.13(b) 
and 8.13(c) are the real and imaginary parts, respectively, of the corresponding 10­
point DFT X [k). By simply relabeling the horizontal axis, we obtain the complex 
sequence xI[n] 
X [n], as shown in Figures 8.13(d) and 8.13(e). According to the 
duality relation in Eqs. (8.93), the lO-point DFT of the (complex-valued) sequence 
X [n] is the sequence shown in Figure 8.13(f). 

652  
Chapter 8 
The Discrete Fourier Transform 
rl II I ...... *1 
o 
2345678910 
n 
(a) 
RelX[kll 
5~  
/ 
Re{X(ei",) I 
I 
'-...,1 
J 
1
1\ .. //S,->. .,-//1"'-"0 <//[""", /1
o 
1 '--'2 
3 
4 
S 
6 
7 
8'-/ 9 
10 
k 
o  
11" 
2rr 
,,) 
(b) 
S  
Im{X[kJI 
Im{X(ei"')1  
I" 
~\ 
I 
\ 
...-....... 
,//" 
I 
' \  
O~-,--,F~~~~~--~~~~--~~ 
0, 
5'-6 
7 
8 
9 
10 
k 
0\ 
11" 
211" 
W 
\ 
\ 
(e) 
S  
Re{xdnll ~ ReIX[nll 
1
_ It 
It 
• 
It 
0 
II 
• 
II 
•
0 
o 
2 
3 
4 
5 
6 
7 
8 
9 
10 
n 
(d) 
ImlxJ[nll ~ ImlX[nll 
3.08 
• 
• 
3 
• 
• 
• 
I 0.73 • I . 
o  
2 
I 
4 
5 
6 
7 
8 
9 
10 
n 
-0.73 
-3,08 
(e) 
XJlkJ ~ IOx[«-k)hol 
r. . . . . I I I I ~  
o 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
k 
(f) 
Figure 8.13  Illustration of duality. (a) Real finite-length sequence x[n]. (b) and 
(c) Real and imaginary parts of corresponding OFT X[k]. (d) and (e) The real and 
imaginary parts of the dual sequence Xl [n] = X[n]. (f) The OFT of x1 In]. 

653 
Section 8.6 
Properties of the DFT 
8.6.4 Symmetry Properties 
Since the DFT of x[n] is identical to the DFS coefficients of the periodic sequenee 
i[n] 
x[«n))N], symmetry properties associated with the DFT can be inferred from the 
symmetry properties of the DFS summarized in Table 8.1 in Seetion 8.2.6. Specifically, 
using Eqs. (8.88) together with Properties 9 and 10 in Table 8.1, we have 
x*[nJ 
X*[«-k))N], 
o n:S N -1, 
(8.94) 
and 
X*[k], 
O:sn:sN 
1. 
(8.95) 
Properties 11-14 in Table 8.1 refer to the decomposition of a periodic sequence into the 
sum of a conjugate-symmetric and a conjugate-antisymmetric sequence. This suggests 
the decomposition of the finite-duration sequence x [n] into the two finite-duration 
sequences of duration N corresponding to one period of the conjugate-symmetric and 
one period of the conjugate-antisymmetric components of i[n]. We will denote these 
components ofx[n] asxep[n] andxop[nj. Thus, with 
x[nJ 
x[«n))N] 
(8.96) 
and the conjugate-symmetric part being 
[n] = i{x[n] + i*[-n]}, 
(8.97) 
and the conjugate-antisymmetric part being 
xo[n] = i{i[n] -i*[-n]}, 
(8.98) 
we define xep[n] and xop[n] as 
xep[n] = idn], 
O:sn:sN 
1, 
(8.99) 
xop[n] = xo[n], 
O:sn:sN 
1, 
(8.100) 
or, equivalently, 
xep[n] 
i{x[«n))N] +x*[«-n))Nl}, 
o:s n :s N 
1, 
(8.101a) 
xop[n] = i{x[«n))N] - x*[«-n))N]}, 
o:s n :s N -
1, 
(8.101b) 
with both xep[n] and xop[n] being finite-length sequences, i.e., both zero outside the 
interval 0 :s n :s N -1. Since «-n))N = (N - n) and «n))N = n for 0 :s n :s N -1, we 
can also express Eqs. (8.101) as 
xep[n] = i{x[n] + x*[N 
n]}, 
1 :s n :s N 
1, 
(8.102a) 
xep[O] 
Re{x [O]} , 
(8.102b) 
xop[n] 
[n] 
x*[N - nU, 
1 :s n :s N -1, 
(S.102c) 
xop[O] = jIm{x[O]}. 
(S.102d) 
This form of the equations is convenient, since it avoids the modulo N computation of 
indices. 

654 
Chapter 8 
The Discrete Fourier Transform 
Clearly, xep[n] and xop[n] are not equivalent to xe[n] and xo[n] as defined by 
Eqs. (2.149a) and (2.149b). However, it can be shown (see Problem 8.59) that 
xep[n] = {xdn] + xe[n - N]}, 
0 :s n :s N 
1, 
(8.103) 
and 
xop[n] = {xo[n] + xo[n - N]}, 
O:sn:sN 
1. 
(8.104) 
In other words, xep[n] and xop[n] can be generated by time-aliasing xe[n] and xo[n] 
into the interval 0 :s n :s N -1. The sequences xep[n] and xop[n] will be referred to 
as the periodic conjugate-symmetric and periodic conjugate-antisymmetric components, 
respectively, of x[n]. When xep[n] and xop[n] are real, they will be referred to as the 
periodic even and periodic oddcomponents, respectively. Note that the sequences xep[n] 
and xop[n] are not periodic sequences; they are, however, finite-length sequences that 
are equal to one period of the periodic sequences jAn] and xo[n], respectively. 
Equations (8.101) and (8.102) define xep[n] and xop[n] in terms of x[n]. The in­
verse relation, expressing x[n] in terms of xep[n] and xop[n], can be obtained by using 
Eqs. (8.97) and (8.98) to express x[n] as 
x[n] = xe[n] + xo[n]. 
(8.105) 
Thus, 
x[n] 
x[n] = ie[n] + xo[n], 
0 :s n :s N 
1. 
(8.106) 
Combining Eqs. (8.106) with Eqs. (8.99) and (8.100), we obtain 
x[n] =xep[n] + xop[n]. 
(8.107) 
Alternatively, Eqs. (8.102), when added, also lead to Eq. (8.107). The symmetry prop­
erties of the DFf associated with properties 11-14 in Table 8.1 now follow in a straight­
forward way: 
V:F:J
Re{x[n]} +--+ X ep[k], 
(8.108) 
V:FJ
jIm{x[n]} +--+ X op[k], 
(8.109) 
V:F:J
xep[n] +--+ Re{X [k]}, 
(8.110) 
xop[n] ~ jIm{X [k]}. 
(8.111) 
8.6.5 Circular Convolution 
In Section 8.2.5, we showed that multiplication of the DFS coefficients of two periodic 
sequences corresponds to a periodic convolution of the sequences. Here, we consider 
two finite-duration sequences xI[n] and x2[n], both of length N, with DFfs X dk] and 
X 2[k], respectively, and we wish to determine the sequence x3[n], for which the DFf 
is X 3[k] = X dk]X 2[k]. To determine x3[n], we can apply the results of Section 8.2.5. 
Specifically, x3[n] corresponds to one period of x3[n], which is given by Eq. (8.27). Thus, 
N-I 
x3[n] = L idm]i2[n 
m], 
o:s n :s N -1, 
(8.112) 
m=O 

655 
Section 8.6 
Properties of the DFT 
or, equivalently, 
N-l 
x3[n] = L xl[«m))N]x2[«n 
m))N], 
O=::;n 
N 
1. 
(8.113) 
m=O 
Since «m))N = m for 0 =::; m =::; N -1, Eq. (8.113) can be written 
N-l 
x3[n] = L xllm]x2[«n - m))N], 
O=::;n=::;N-1. 
(8.114) 
m=O 
Equation (8.114) differs from a linear convolution of xdn] and x2[n] as defined 
by Eq. (2.49) in some important respects. In linear convolution, the computation of the 
sequence value x3[n] involves multiplying one sequence by a time-reversed and linearly 
shifted version of the other and then summing the values of the product xl[m]x2[n - m] 
over all m. To obtain successive values of the sequence formed by the convolution 
operation, the two sequences are successively shifted relative to each other along a 
linear axis. In contrast, for the convolution defined by Eq. (8.114), the second sequence 
is circularly time reversed and circularly shifted with respect to the first. For this reason, 
the operation ofcombining two finite-length sequences according to Eq. (8.114) is called 
circular convolution. More specifically, we refer to Eq. (8.114) as an N-point circular 
convolution, explicitly identifying the fact that both sequences have length N (or less) 
and that the sequences are shifted modulo N. Sometimes, the operation of forming a 
sequence x3[nj for 0 =::; n =::; N 
1 using Eq. (8.114) will be denoted 
(8.115)  
i.e., the symbol @ denotes N -point circular convolution. 
Since the DFTofx3[n]isX 3[k] = Xl [k1X 2[k] andsince X l[k]X 2[k] 
X2[k]X l[k], 
it follows with no further analysis that 
x3[n] = x2[n] @xl[n], 
(8.116) 
or, more specifically, 
N-l 
x3[n] = L x2[m]xt[((n 
m))N]. 
(8.117) 
m=O 
That is, circular convolution, like linear convolution, is a commutative operation. 
Since circular convolution is really just periodic convolution, Example 8.4 and 
Figure 8.3 are also illustrative of circular convolution. However, if we use the notion of 
circular shifting, it is not necessary to construct the underlying periodic sequences as in 
Figure 8.3. This is illustrated in the following examples. 
Example 8.10 Circular Convolution with a Delayed Impulse 
Sequence 
An example of circular convolution is provided by the result of Section 8.6.2. Let x2[n1 
be a finite-duration sequence of length Nand 
x}ln] = 8[n 
no], 
(8.118) 

656 
Chapter 8 
The Discrete Fourier Transform 
where 0 < nO < N. Clearly, Xl [n] can be considered as the finite-duration sequence 
0, 
0< n < no, 
xdn] = 
1, 
n = no, 
(8.119) 
0, 
no < n ~ N 
1.
1 
as depicted in Figure 8.14 for n 0 = 1. 
The DFf of xl[n] is 
wkno
x ilk] 
N 
(8.120) 
If we form the product 
x3[k] = W~no X Z[k], 
(8.121) 
we see from Section 8.6.2 that the finite-duration sequence corresponding to X 3[k] 
is the sequence xZ[n] rotated to the right by nO samples in the interval 0 
n ~ 
N 
1. That is, the circular convolution of a sequence xz[n] with a single delayed unit 
impulse results in a rotation of xZ[n] in the interval 0 ~ n ~ N 
1. This example is 
illustrated in Figure 8.14 for N 
5 and no = 1. Here, we show the sequences xZ[m]
I 
<,[m[
· . . -I [ I I • . . . 
0 
N 
m 
· . . . I . . . . . . . 
<,1m[ 
0 
N 
m
I ,,[«O-m)MO~m~N-l 
• •• 
I I [ I . . . . 
0 
N 
m 
x2[((1 
m))N],O:5 m :5 N 
· . . I II I [ . . . . 
m
0 
N
I 
,,[n] ='.In] ® x,ln] 
• • • I I [ I . . . . 
0 
N 
n 
Figure 8.14 
Circular convolution of a finite-length sequence x2[n] with a single 
delayed impulse, x1 In] = o[n- 1]. 

Section 8.6 
Properties of the DFT 
657 
and xl[m] and then x2[«0 
m»N j and x2[«(1 
m»N j. It is clearfrom these two cases 
that the result of circular convolution of x2[n] with a single shifted unit impulse will 
be to circularly shift x2[n]. The last sequence shown is x3[n], the result of the circular 
convolution of Xl En] and x2[nj. 
Example 8.11 
Circular Convolution of Two Rectangular 
Pulses 
As another example of circular convolution, let 
{ 
1, 
0, 
0::;: n ::;: L -1, 
otherwise, 
(8.122) 
where, in Figure 8.15, L 
6. If we let N denote the DFr length, then, for N 
L, the 
N -point DFTs are 
x I[k] = X 2[k] 
N-lL wtn = {N,
0,
n=O 
k =0, 
otherwise. 
(8.123) 
If we explicitly multiply X I[k] and X 2[k], we obtain 
X l[k]X 2[k] = { 
N2 
' 
0, 
k 
0, 
otherwise. 
(8.124) 
from which it follows that 
X3[n] = N, 
O::;:n::;:N 
1. 
(8.125) 
This result is depicted in Figure 8.15. Clearly, as the sequence X2[ «(n - m»N] is rotated 
with respect to Xl Em], the sum of products xl[m]x2[«n 
m»N] will always be equal 
to N. 
Of course, it is possible to consider xl[n] and x2[n] as 2L-point sequences by 
augmenting them with L zeros. If we then perform a 2L-point circular convolution of 
the augmented sequences, we obtain the sequence in Figure 8.16, which can be seen to 
be identical to the linear convolution of the finite-duration sequences xl[n] and x2[n]. 
This important observation will be discussed in much more detail in Section 8.7. 
Note that for N 
2L, as in Figure 8.16, 
W Lk
1 
N
X 1[k] = X 2[k] 
1 
so the DFT of the triangular-shaped sequence X3 [n] in Figure 8.16( e) is 
1- WLk)2
X 3[k] = 
N
( 1- Wk 
N 
withN =2L 

• • • • 
658 
Chapter 8 
The Discrete Fourier Transform 
Xl[n] 
, , ,1] ] ] ] ] ] , , 
o 
N 
n 
(a) 
X2[n] 
, , ,1] I I I ] I , , , , , , 
n
o 
N 
(b) 
x3[n] =xdn] @X2[nj 
, , ~l I I I ] I , , , , , , 
o 
N 
n 
(c) 
Figure 8.15 N-point circular convolution of two constant sequences of length N. 
1 
xdn] 
n 
o 
L 
N 
(a) 
(b) 
x2[« -n»Nj, 0 S n S N-1 
, , , ,1! , , , , , , [ [ [ [ I , , , ,  
o 
L 
N 
n 
(c) 
1 
x2[n] 
o 
L 
N 
n 
Figure 8.16 
2L-point circular convolution of two constant sequences of length L. 

659 
Section 8.6 
Properties of the DFT 
....'1 I I ...... I I I... .  
o 
2 
L 
N 
n 
(d) 
x3[n] =xl[n] ® x2[n] 
. . . . , ! 1II 
LIIII! I • • • • • 
o 
L 
N 
n 
(e) 
Figure 8.16 (continued) 
The circular convolution property is represented as 
(8.126) 
In viewofthe duality ofthe DFT relations,it is notsurprisingthat the DFTofa product of 
two N -point sequences is the circular convolution of their respective DFTs. Specifically, 
if x3[n] = Xl [n]X2[n), then 
X3[k) = 
1 N-l 
N I: X di)X 2[((k 
i»N] 
(8.127) 
l=O 
or 
(8.128) 
8.6.6 Summary of Properties of the DFT 
The properties of the DFT that we discussed in Section 8.6 are summarized in Table 8.2. 
Note that for all of the properties, the expressions given specify x[n] for 0::: n ::: N - 1 
and X [k] for 0::: k ::: N -1. Both x[n] and X [k] are equal to zero outside those ranges. 

660 
Chapter 8 
The Discrete Fourier Transform 
TABLE B.2 
SUMMARY OF PROPERTIES OF THE DFT 
Finite-Length Sequence (Length N) 
N -point DFr (Length N) 
L 
x[n] 
X[k] 
2. 
Xl [n], X2[n] 
X I [k], X2[k] 
3. 
aXI[n] + bX2[n] 
aX l[k] -r bX2[k] 
4. 
X[n] 
Nx[(C-k))N] 
5. 
x[«n ­ m»N] 
Wkm X[k]
N 
6. W;/Il x [n] 
X[CCk - i»N] 
N-l 
7. L xl[m]x2[«n ­ m»N] 
X I [k]X2[k] 
m=O 
1 N-I 
8. 
Xl [n]x2[n] 
-
" 
Xl[llX2[C(k-l»N]
N L.. 
. 
£=0 
9. 
x*[n] 
X*[« -k»N] 
10. 
x*[(( -n» N] 
X*[k] 
11. 
Re{x[n]J 
X ep[k] = ~{X [«k»N J+ X*[C(-k»N lJ 
12. 
jIm{x[nlJ 
X op[k] = 1{X [«k»N] - X*[« -k»N lJ 
2 
13. xep[n] 
i{x[n] + x*[«-n»N 11 
Re(X [klJ 
14. 
xop[nJ 
i(x[n] 
x*[«-n»NlJ 
jIm{X [k]) 
Properties 15-17 apply only when x[nJ is reaL 
X[k] = X*[((-k))NJ 
RejX[k]) = Re(X[C(-k))Nll 
15. Symmetry properties 
Im{X[k]) = -Im{X[«-k»N]) 
1 
IX[kJI = IX[«-k))NJI 
L{X[k]) = -L[X[C(-k»NlJ 
16. 
xep[n] 
i(x[n] +x[«-n))N]) 
Re[X[kl) 
17. 
xop[n] 
i{x[n] 
x[«-n»Nll 
jIm{X[k]) 
8.7  COMPUTING LINEAR CONVOLUTION USING 
THE DFT 
We will show in Chapter 9 that efficient algorithms are available for computing the 
DFf of a finite-duration sequence. These are known collectively as FFf algorithms. 
Because these algorithms are available, it is computationally efficient to implement a 
convolution of two sequences by the following procedure: 
(a) Compute the N-pointDFfs X 1[k] and X 2[k] ofthetwosequencesxt[n] andx2[n], 
respectively. 
(b) Compute the product X3[k] 
X 1[k]X2[k] forO:s k:s N-1. 
(c) Compute the sequence x3[n] 
xt[n] @x2[n] as the inverse DFf of X 3[k]. 

661 
Section 8.7
form 
Linear Convolution Using the OFT 
In many DSP applications, we are interested in implementing a linear convolution of 
two sequences; i.e., we wish to implement an LTI system. This is certainly true, for 
example, in filtering a sequence such as a speech waveform or a radar signal or in 
computing the autocorrelation function of such signals. As we saw in Section 8.6.5, 
the multiplication of DFTs corresponds to a circular convolution of the sequences. To 
obtain a linear convolution, we must ensure that circular convolution has the effect of 
linear convolution. The discussion at the end of Example 8.11 hints at how this might 
be done. We now present a more detailed analysis. 
8.7.1  Linear Convolution of Two Finite-Length  
Sequences  
Consider a sequence xl[n] whose length is L points and a sequence x2[n] whose length 
is P points, and suppose that we wish to combine these two sequences by linear convo­
lution to obtain a third sequence 
x3[n] = L
00 
xdm]x2[n - m].  
(8.129) 
m=-oo 
Figure 8.17(a) shows a typical sequence xdm] and Figure 8.17(b) shows a typical se­
quence x2[n -
m] for the three cases n = -1, for 0 .:s n .:s L - 1, and n = L + P - l. 
Clearly, the product xdm]x2[n - m] is zero for all m whenever n < 0 and n > L + P - 2; 
i.e., x3[n] i= 0 for 0 .:s n .:s L + P - 2. Therefore, (L + P - 1) is the maximum length of 
the sequence x3[n] resulting from the linear convolution of a sequence of length L with 
a sequence of length P. 
8.7~2 Circular Convolution as Linear Convolution with  
Aliasing  
As Examples 8.10 and 8.11 show, whether a circular convolution corresponding to the 
product of two N -point DFTs is the same as the linear convolution ofthe corresponding 
finite-length sequences depends on the length of the DFT in relation to the length 
of the finite-length sequences. An extremely useful interpretation of the relationship 
between circular convolution and linear convolution is in terms of time aliasing. Since 
this interpretation is so important and useful in understanding circular convolution, we 
will develop it in several ways. 
In Section 8.4, we observed that if the Fourier transform X (eiw) of a sequence 
x[n] is sampled at frequencies (Uk = 2nk/N, then the resulting sequence corresponds to 
the DFS coefficients of the periodic sequence 
i[n] = L
00 
x[n - rN]. 
(8.130) 
r=-oo 

• • • • • • • • • • • • 
• • • • • • • • • • • • 
662 
Chapter 8 
The Discrete Fourier Transform 
lllllllllllll........ x,lm  
o 
L 
m 
(a) 
I 
~H-~ 
............. • ! III . . . . . . . . . . . . . . . . . . . . . .  
-P 
-1 0 
L 
m 
X2[n-m] 
o ~ 
L 
m 
n-P+1 
x2[L + P-l-mj 
• • • • • • • • • o• • • • • • • • • • • • J 
L Llll ---
m 
L+P 1 
(b) 
Figure 8.17 
Example of linear convolution of two finite-length sequences show­
ing that the result is such that x3[n] 
0 for n ::: -1 and for n ::::: L+ P 
1. 
(a) Finite-length sequence x1 [mI. (b) x2[n 
m] for several values of n. 
From our discussion of the DFT, it follows that the finite-length sequence 
X (e j (2:n:k/N) 
0 < k < N 
1,
X [k] 
, 
--
(8.131)
{ 0, 
otherwise, 
is the DFT of one period of .t[n], as given by Eq. (8.130); i.e., 
.t[n], 0::: n ::: N -1, 
(8.132)
xp[n] = { 0, 
otherwise. 
Obviously, if x[n] has length less than or equal to N, no time aliasing occurs and 
xp[n] 
xln]. However, if the length of x[n] is greater than N, xp[n] may not be equal 
to x[n] for some or all values of n. We will henceforth use the subscript p to denote 

n 
Section 8.7 
Linear Convolution Using the OFT 
663 
that a sequence is one period of a periodic sequence resulting from an inverse DFf of a 
sampled Fourier transform. The subscript can be dropped if it is clear that time aliasing 
is avoided. 
The sequence X3 [n] in Eq. (8.129) has Fourier transform 
(8.133) 
Ifwe define a DFT 
o:s k :s N - 1, 
(8.134) 
then it is clear from Eqs. (8.133) and (8.134) that, also 
X 3[k] 
Xl(eJ(ZnkIN)X 2(eJ(ZnkIN», 
O:sk 
N-l. 
(8.135) 
and therefore, 
(8.136) 
That is, the sequence resulting as the inverse DFT of X3lk] is 
rN], O:sn:sN 
1,
X3p[n] 
(8.137)
= IrI;oo x3[n 
0, 
otherwise, 
and from Eq. (8.136), it follows that 
x3p[n] 
xdn] @x2[n]. 
(8.138) 
Thus, the circular convolution oftwo finite-length sequences is equivalent to linear 
convolution of the two sequences, followed by time aliasing according to Eq. (8.137). 
Note that if N is greater than or equal to either L or P, X I[k] and X 2[k] represent 
xI[n] and xz[n] exactly, but x3p[n] 
x3[n] for all n only if N is greater than or equal 
to the length of the sequence x3[n]. As we showed in Section 8.7.1, if xl[n] has length 
Land xz[n] has length P, then x3[n] has maximum length (L + P - 1). Therefore, the 
circular convolution corresponding to X I [k]X 2[k] is identical to the linear convolution 
corresponding to Xl (ejW)X z(ejW) if N, the length of the DFfs, satisfies N ::: L + P - 1. 
Example 8.1 2 
Circular Convolution as Linear Convolution 
with Aliasing 
The results of Example 8.11 are easily understood in light of the interpretation just 
discussed. Note that xl[n] and xz[n] are identical constant sequences of length L = 
P 
6, as shown in Figure 8.18(a). The linear convolution ofx1[1I] andxZ[n] is oflength 
L + P 
1 = 11 and has the triangular shape shown in Figure 8.1S(b). In FiguresS.l8(c) 
and (d) are shown two of the shifted versions X3[1I - rN] in Eq. (8.137), x3[n - N] 
and x3[n + N] for N = 6. The N-point circular convolution of xl[n] and xZ[n] can be 
formed by using Eq. (S.137). This is shown in Figure 8.1S(e) for N = L 
6 and in 
.) 
~) 
Figure S.IS(f) for N 
2L = 12. Note that for N = L 
6, only x3[n] and x3[n + N] 
contribute to the result. For N = 2L 
12, only x3[n] contributes to the result. Since 
d 
the length of the linear convolution is (2L - 1), the result of the circular convolution 
for N = 2L is identical to the result of linear convolution for all 0 ~ 11 ~ N 
1. In 
:e 
II 

---
664 
Chapter 8 
The Discrete Fourier Transform 
fact, this would be true for N = 2L 
1 = 11 as well. 
Xt[n] 
x2[n] • 
L 
P=6 
.'1 I I I I I ..
• • • • • • • 
• •• 
• 
n
o 
L=P 
------­
W 
L 
x3[n] =xdn] * x2[n] 
T I I [ ) ] ) [ I 
• • • • • • • • 
LL. • 
••• 
n
o 
2L-l 
(b) 
X3[n-N], 
T I I InII I IN;L ,
• •• •••• •o • • • 
N 
n 
(e) 
X3[n + N], 
N=L=6 
I I [ )L] ) II I L
---t 
• • • • • • • • ........  
-N 
o 
n 
(d) 
Xl[n]@X2[n]. 
N=L 6 
· ...... .LI I I I I I . 
o 
N- 1 • • • • • • • --
n 
(e) 
Xdn]@X2[n], 
N 
2L 
! I I I ]L[ III I !
• • • • • • • • 
n
o 
------- -­
(f) 
Figure 8.18 
Illustration that circular convolution is equivalent to linear convolu­
tion followed by aliasing. (a) The sequences xl In] and x2ln] to be convolved. (b) The 
linear convolution of x1 In] and x2lnl. (c) x31n - N] for N 
6. (d) x3[n + N] for 
N 
6. (e) x1 In] CD x2[nl. which is equal to the sum of (b), (c), and (d) in the 
interval 0 ~ n ~ 5. (f) x1 In] @ x2[n]. 

• • • • • 
665 
Section 8.7 
Linear Convolution Using the DFT 
, , ,'I I I 11'1 I I , , , • • • • 
o 
L  
n 
(a) 
• • • • • . ..11 I I I .......... .  
o 
P  
n 
(b) 
7 
n
o  
P 
L+ P-l 
(c) 
Figure 8.1 9 An example of linear corlVolution of two finite-length sequences. 
As Example 8.12 points out, time aliasing in the circular convolution of two finite­
length sequences can be avoided if N 2: L + P - 1. Also, it is clear that if N = L 
P, 
all of the sequence values of the circular convolution may be different from those of 
the linear convolution. However, if P < L, some of the sequence values in an L-point 
circular convolution will be equal to the corresponding sequence values of the linear 
convolution. The time-aliasing interpretation is useful for showing this. 
Consider two finite-duration sequences xl[n] and x2[n], with xl[n] of length Land 
x2[n] oflength P, where P < L, as indicated in Figures 8.19(a) and 8.19(b), respectively. 
Let us first consider the L-point circular convolution of xl[n] and x2[n] and inquire as 
to which sequence values in the circular convolution are identical to values that would 
be obtained from a linear convolution and which are not. The linear convolution of 
xl[n] with x2[n] will be a finite-length sequence of length (L + P - 1), as indicated in 
Figure 8.19(c). To determine the L-point circular convolution, we use Eqs. (8.137) and 
(8.138) so that 
1,
xl[n] Q;) x2[n] = r'foo x3[n - rL], 
(8.139) 
{ 0,  
otherwise. 
Figure 8.20(a) shows the term in Eq. (8.139) for r = 0, and Figures 8.20(b) and 8.20(c) 
show the terms for r = -1 and r 
+1, respectiVely. From Figure 8.20, it should be 
clear that in the interval 0 ~ n ~ L -1, x3p[n] is influenced only by x3[n] and x3[n +LJ. 

X3[n] 
o 
n
L 
L+P-l 
(a) 
X3[n + L] 
-L 
o t 
n 
P-2 
(b) 
X3[n LJ 
o 
L 
n 
(c) 
X3p[n]=X3[nj + x3[n+Lj,O nsL-l 
, .. • • • • • 0 
f 
• • • • • • • • • • • 
'" 
n 
P-l 
(d) 
Figure 8.20 
Interpretation of circular convolution as linear convolution followed 
by aliasing for the circular convolution of the two sequences x1[nl and x2[nl in 
Figure 8.19. 
666 

667 
Section 8.7 
Linear Convolution Using the OFT 
In general, whenever P < L, only the term x3[n + L] will alias into the interval 
o::::: n ~ L -1. More specifically, when these terms are summed, the last (P -1) points of 
x3[n +L], which extend fromn = Oto n = P -2, will be added to the first (P -1) points 
ofx3[n], and thelast (P 
1) points ofx3[n], extending from n = L to n = L + P 
2, will 
contribute only to the next period of the underlying periodic result x3[n]. Then, x3p[n] 
is formed by extracting the portion for 0 ~ n < L - 1. Since the last (P - 1) points of 
x3[n + L] and the last (P - 1) points of x3[n] are identical, we can alternatively view 
the process of forming the circular convolution x3p[nI through linear convolution plus 
aliasing, as taking the (P - 1) values of x3[n] from n 
L to n 
L + P - 2 and adding 
them to the first (P - 1) values of x3[n]. This process is illustrated in Figure 8.21 for 
the case P = 4 and L = 8. Figure 8.21(a) shows the linear convolution x3[n], with the 
points for n ~ L denoted by open symbols. Note that only (P 
1) points for n ~ L 
are nonzero. Figure 8.21(b) shows the formation of x3p[n] by "wrapping x3[n] around 
on itself." The first (P - 1) points are corrupted by the time aliasing, and the remaining 
points from n = P 
1 to n 
L - 1 (i.e., the last L - P + 1 points) are not corrupted; 
that is, they are identical to what would be obtained with a linear convolution. 
From this discussion, it should be clear that if the circular convolution is of suffi­
cient length relative to the lengths ofthe sequences xl[n] and x2[n], then aliasing with 
nonzero values can be avoided, in which case the circular convolution and linear con­
volution will be identical. Specifically, if, for the case just considered, x3[n] is replicated 
with period N ~ L + P 
1, then no nonzero overlap will occur. Figures 8.21(c) and 
8.21 (d) illustrate this case, again for P = 4 and L = 8, with N = 11. 
8.7.3  Implementing Linear Time-Invariant Systems Using 
the OFT 
The previous discussion focused on ways of obtaining a linear convolution from a cir­
cular convolution. Since LTI systems can be implemented by convolution, this implies 
that circular convolution (implemented by the procedure suggested at the beginning of 
Section 8.7) can be used to implement these systems. To see how this can be done, let 
us first consider an L-point input sequence x[n] and a P-point impUlse response hln]. 
The linear convolution of these two sequences, which will be denoted by YIn], has finite 
duration with length (L + PI). Consequently, as discussed in Section 8.7.2, for the cir­
cular convolution and linear convolution to be identical, the circular convolution must 
have a length of at least (L + P 
1) points. The circular convolution can be achieved 
by multiplying the DFTs of x[n] and h[n]. Since we want the product to represent the 
DFT of the linear convolution of x[n] and h[n], which has length (L + P -1), the DFTs 
that we compute must also be of at least that length, i.e., both x[n] and h[n] must be 
augmented with sequence values of zero amplitude. This process is often referred to as 
zero-padding. 
This procedure permits the computation of the linear convolution of two finite­
length sequences using the DFT; i.e., the output of an FIR system whose input also has 
finite length can be computed with the DFT. In many applications, such as filtering a 
speech waveform, the input signal is ofindefinite duration. Theoretically, while we might 
be able to store the entire waveform and then implement the procedure just discussed 
using a DFT for a large number of points, such a DFT might be impractical to compute. 

668 
Chapter 8 
The Discrete Fourier Transform 
x3[n] 
n
o 
L 
X3p[n] =Xl[n] @X2[n], 
N=L 
f 
I 
n
L
o 
P-l 
(b) 
X3[n] 
n 
X2[n], 
Figure 8.21 
Illustration of how the 
result of a circular convolution "wraps 
n 
around." (a) and (b) N = L, so the
o 
N 
aliased "tail" overlaps the first (P 
1) 
paints. (c) and (d) N = (L + P 
1), so 
(d) 
no overlap occurs. 
Another consideration is that for this method of filtering, no filtered samples can be 
computed until all the input samples have been collected. Generally, we would like to 
avoid such a large delay in processing. The solution to both of these problems is to use 
block convolution, in which the signal to be filtered is segmented into sections of length 
L. Each section can then be convolved with the finite-length impulse response and the 

669 
Section 8.7 
Linear Convolution Using the OFT 
h[n] 
n 
P 1 
x[n] 
n 
Figure 8.22 
Finite-length impulse response h[n] and indefinite-length signal x[n] 
to be filtered. 
filtered sections fitted together in an appropriate way. The linear filtering of each block 
can then be implemented using the DFT. 
To illustrate the procedure and to develop the procedure for fitting the filtered 
sections together, consider the impulse response h[n] of length P and the signal x[n] 
depicted in Figure 8.22. Henceforth, we will assume thatx[n] = 0 for n < 0 and that the 
length of x[n] is much greater than P. The sequence x[n] can be represented as a sum 
-of shifted nonoverlapping finite-length segments of length L; i.e., 
00 
x[n] 
Lx,[n - rL], 
(8.140) 
,=0 
where 
x,ln] 
{ 
X[n+rL], 
0, 
0::sn::sL-1, 
otherwise. 
(8.141) 
Figure 8.23( a) illustrates this segmentation for x[n] in Figure 8.22. Note that within each 
segment x, In], the first sample is at n = 0; however, the zeroth sample of x,[n] is the r Lth 
sample of the sequence x[n]. This is shown in Figure 8.23(a) by plotting the segments 
in their shifted positions but with the redefined time origin indicated. 
Because convolution is an LTI operation, it follows from Eq. (8.140) that 
00 
yIn] = x[n] * h[n] = Ly,[n 
rL], 
(8.142) 
,=0 
where 
y,[n] = x,[n] *h[n]. 
(8.143) 
Since the sequences x, In] have only L nonzero points and h[n] is of length P, each of 
the terms y,[n] = x,[n] * h[n] has length (L + PI). Thus, the linear convolution 
x,[n] * h[n] can be obtained by the procedure described earlier using N-point DFTs, 
wherein N :::: L + P 
1. Since the beginning of each input section is separated from 
its neighbors by L points and each filtered section has length (L + P - 1), the nonzero 
points in the filtered sections will overlap by (P 
1) points, and these overlap samples 

670 
Chapter 8 
The Discrete Fourier Transform 
xo[n]
t!lln 
L 
o 
··]11111!1········· 
n 
xdn] 
o 
tTIIIIIIII.........  
n
IIIIP 
L 
11 
L 
x2[n] 
o ··'11111111l1l······· n 
(a) 
Y(J[n] 
n 
Ydn] 
n 
u[IWT!1!f 
n 
Figure 8.23 
(a) Decomposition of x[n] 
I 
I 
in Figure 8.22 into nonoverlapping 
sections of length L. (b) Result of 
(b) 
convolving each section with h[n]. 
must be added in carrying out the sum required by Eq. (8.142). This is illustrated in 
Figure 8.23(b), which illustrates the filtered sections, Yr[n] = xr[n] * h[n]. Just as the 
input waveform is reconstructed by adding the delayed waveforms in Figure 8.23(a), 
the filtered result x[n] * h[n] is constructed by adding the delayed filtered sections 
depicted in Figure 8.23(b). This procedure for constructing the filtered output from 
filtered sections is often referred to as the overlap-add method, because the filtered 
sections are overlapped and added to construct the output. The overlapping occurs 
because the linear convolution of each section with the impulse response is, in general, 
longer than the length of the section. The overlap-add method of block convolution is 
not tied to the DFf and circular convolution. Clearly, all that is required is that the 
smaller convolutions be computed and the results combined appropriately. 

671 
Section 8.7
m 
x[n] 
jin 
the 
(a), 
lons 
rom 
~red 
curs 
~ral, 
mis 
the 
Linear Convolution Using the OFT 
An alternative block convolution procedure, commonly called the overlap-save 
method, corresponds to implementing an L-point circular convolution of a P-point 
impulse response h[n] with an L-point segment xr[n] and identifying the part of the 
circular convolution that corresponds to a linear convolution. The resulting output 
segments are then "patched together" to form the output. Specifically, we showed that 
if an L-point sequence is circularly convolved with a P-point sequence (P < L), then 
the first (P - 1) points of the result are incorrect due to time aliasing, whereas the 
remaining points are identical to those that would be obtained had we implemented a 
linear convolution. Therefore, we can divide x[n] into sections of length L so that each 
input section overlaps the preceding section by (P - 1) points. That is, we define the 
sections as 
xr[n] 
x[n + r(L - P + 1) - P + 1], 
O::5n::5L 
1, 
(8.144) 
wherein, as before, we have defined the time origin for each section to be at the beginning 
of that section rather than at the origin of x[n]. This method of sectioning is depicted in 
Figure 8.24(a). The circular convolution of each section with h[n] is denoted Yrp[n], the 
extra subscript p indicating that Yrp[n] is the result of a circular convolution in which 
time aliasing has occurred. These sequences are depicted in Figure 8.24(b). The portion 
of each output section in the region 0 ::5 n ::5 P - 2 is the part that must be discarded. 
The remaining samples from successive sections are then abutted to construct the final 
filtered output. That is, 
00 
y[n] = LYr[n 
r(L - P + 1) + P - 1], 
(8.145) 
r=O 
where 
[ ] _ { Yrp[n], 
P - 1 ::5 n ::5 L - 1, 
(8.146)
Yr n -
0, 
otherwise. 
This procedure is called the overlap-save method because the input segments overlap, 
so that each succeeding input section consists of (L - P + 1) new points and (P - 1) 
points saved from the previous section. 
The utility of the overlap-add and the overlap-save methods of block convolution 
may not be immediately apparent. In Chapter 9, we consider highly efficient algorithms 
for computing the DFT. These algorithms, collectively called the FFT, are so efficient 
that, for FIR impulse responses of even modest length (on the order of 25 or 30), it 
may be more efficient to carry out block convolution using the DFT than to implement 
the linear convolution directly. The length P at which the DFT method becomes more 
efficient is, of course, dependent on the hardware and software available to implement 
the computations. (See Stockham, 1966, and Helms, 1967.) 

n 
xl[n] 
I 
n 
I 
I 
x2[n] 
:.y 
I 
I 
L 
1 
o i~ 
n 
(a) 
I
I 
I  
I 
I 
yop[n] 
I 
L-li 
n
~'X-i~!tTtt"'Il!lIUi 
I 
n 
. ~ j -IlIIlllIIlll 
(b) 
Figure 8.24 
(a) Decomposition of x[n] in Figure 8.22 into overlapping sections 
of length L. (b) Result of convolving each section with h[n]. The portions of each 
filtered section to be discarded in forming the linear convolution are indicated. 
672 

673 
Section 8.8 
The Discrete Cosine Transform (OCT) 
8.8 THE DISCRETE COSINE TRANSFORM fDCTJ 
The DFT is perhaps the most common example of a general class of finite-length trans­
form representations of the form 
N-l 
A[k] = L x[n]cPk'[n], 
(8.147) 
n=O 
1 N-l 
x[n] = N L A[k]cPk[n], 
(8.148) 
k=O 
where the sequences cPdn], referred to as the basis sequences, are orthogonal to one 
another; i.e., 
1 N-l 
1, 
m 
k, 
N L cPdn]cP;'(n] 
(8.149)
{ 0, 
m -:f k. 
n=O 
In the case ofthe DFT, the basis sequences are the complex periodic sequences ej2rrknlN, 
and the sequence Alk] is, in general, complex even if the sequence x [n] is real. Itis natural 
to inquire as to whether there exist sets of real-valued basis sequences that would yield 
a real-valued transform sequence A[k] when x[n] is real. This has led to the definition 
of a number of other orthogonal transform representations, such as Haar transforms, 
Hadamard transforms (see Elliott and Rao, 1982), and Hartley transforms (Bracewell, 
1983,1984,1989). (The definition and properties of the Hartley transform are explored 
in Problem 8.68.) Another orthogonal transform for real sequences is the discrete cosine 
transform (DCT). (See Ahmed, Natarajan and Rao, 1974 and Rao and Yip, 1990.) The 
DCT is closely related to the DFT and has become especially useful and important in a 
number of signal-processing applications, particularly speech and image compression. 
In this section, we conclude our discussion of the DFT by introducing the DCT and 
showing its relationship to the DFT. 
8.8.1 Definitions of the DCT 
The DCT is a transform in the form of Eqs. (8.147) and (8.148) with basis sequences 
cPdn] that are cosines. Since cosines are both periodic and have even symmetry, the 
extension of x(n] outside the range 0 ~ n ~ (N - 1) in the synthesis Eq. (8.148) will 
be both periodic and symmetric. In other words, just as the DFT involves an implicit 
assumption of periodicity, the DCT involves implicit assumptions of both periodicity 
and even symmetry. 
In the development of the DFT, we represented finite-length sequences by first 
forming periodic sequences from which the finite-length sequence can be uniquely re­
covered and then using an expansion in terms of periodic complex exponentials. In a 
similar style, the DCT corresponds to forming a periodic, symmetric sequence from 
a finite-length sequence in such a way that the original finite-length sequence can be 
uniquely recovered. Because there are many ways to do this, there are many definitions 
of the DCT. In Figure 8.25, we show 17 samples for each of four examples of symmet­
ric periodic extensions of a four-point sequence. The original finite-length sequence is 
shown in each subfigure as the samples with solid dots. These sequences are all periodic 

3• 2 
o 
I 1 
674 
Chapter 8 
The Discrete Fourier Transform 
Xj[n] 
x2[n] 
4 
4 
lId! l1dl\111' 
[ 1 fiiIrI1II! ! II1 1 
0 
6 
12 
n 
0 
5 
8 
16 n 
(a) 
(b)
x3[n] 
x4[n] 
4 
4 IT 2 
16 n 
o 
16 n 
-1 
-z 
-3 
-3 
-4 
-4 
(c) 
(d) 
Figure 8.25 
Four ways to extend afour-point sequence x[n] both periodically and 
symmetrically. The finite-length sequence x[n] is plotted with solid dots. (a) Type-1 
periodic extension for DCT-1. (b) Type-2 periodic extension for DCT-2. (c) Type-3 
periodic extension for DCT-3. (d) Type-4 periodic extension for DCT-4. 
(with period 16 or less) and also have even symmetry. In each case, the finite-length 
sequence is easily extracted as the first four points of one period. For convenience, 
we denote the periodic sequences obtained by replicating with period 16 each of the 
four subsequences in Figure 8.25(a), (b), (c), and (d) as il[n], xz[n], x3[n], and x4[nj, 
respectively. We note that xdn] has period (2N - 2) 
6 and has even symmetry about 
both n = 0 and n 
(N 
1) = 3. The sequence xz[n] has period 2N 
8 and has 
even symmetry about the "half sample" points n 
-~ and ~. The sequence x3[n] has 
period 4N = 16 and has even symmetry about n = 0 and n = 8. The sequence x4[n] 
also has period 4N 
16 and even symmetry about the "half sample" points n = -~ 
and n = (2N 
~) = 1.]. 
The four different cases shown in Figure 8.25 illustrate the periodicity that is 
implicit in the four common forms of the OCT, which are referred to as OCT-I, OCT-2, 
DCT-3, and OCT-4 respectively. It can be shown (see Martucci, 1994) that there are 
four more ways to create an even periodic sequence from x[n]. This implies four other 
possible OCT representations. Furthermore, it is also possible to create eight odd­
symmetric periodic real sequences from x[n], leading to eight different versions of 
the discrete sine transform (DST), wherein the basis sequences in the orthonormal 
representation are sine functions. These transforms make up a family of 16 orthonormal 
transforms for real sequences. Of these, the OCT-1 and OCT-2 representations are the 
most used, and we shall focus on them in the remainder of our discussion. 

675 
Section 8.8 
The Discrete Cosine Transform (OCT) 
8.8.2 Definition of the DCT-l and DCT-2 
All of the periodic extensions leading to different forms of the DCT can be thought of 
as a sum of shifted copies of the N-point sequences ±x[n] and ±x[~nl. The differences 
between the extensions for the DCT-l and DCT-2 depend on whether the endpoints 
overlap with shifted versions of themselves and, if so, which of the endpoints overlap. 
For the DCT-l, x [n 1is first modified at the endpoints and then extended to have period 
2N 
2. The resulting periodic sequence is 
(8.150) 
where xa[n] is the modified sequence xa[n] 
a[n]x[n], with 
!, n 
0 and N 
1,
a[n] =  
(8.151)
(1, 
1:5 n :5 N 
2. 
The weighting of the endpoints compensates for the doubling that occurs when the two 
terms in Eq. (8.150) overlap at n = 0, n = (N ~ 1), and at the corresponding points 
spaced from these by integer multiples of (2N ~ 2). With this weighting, it is easily 
verified that x[n] 
Xl [n] for n 
0,1, ... , N 
1. The resulting periodic sequence xl[n] 
has even periodic symmetry about the points n = 0 and n = N -1, 2N ~2, etc., which we 
refer to as Type-l periodic symmetry. Figure 8.25 (a) is an example of Type-l symmetry 
where N = 4 and the periodic sequence xdn] has period 2N ~ 2 = 6. The DCT-l is 
defined by the transform pair 
N-l 
( 
k 
)
Xc1[k] =2 La[n]x[n]cos : 
n 
' 
o:5 k :5 N 
1, 
(8.152)
1 
n=O 
x[n] = N~ I:a[k]XC1[k]COs(:kn ), 
O:5n:5N 
1, 
(8.153)
1
k=O 
where a[n] is defined in Eq. (8.151). 
For the DCT-2, x[n] is extended to have period 2N, and the periodic sequence is 
given by 
(8.154) 
Because the endpoints do not overlap, no modification of them is required to ensure 
that x[n] 
x2[n] for n 
0,1, ... , N ~ 1. In this case, which we call Type-2 periodic 
symmetry, the periodic sequence x2[n] has even periodic symmetry about the "half 
sample" points -1/2, N - 1/2, 2N - 1/2, etc. This is illustrated by Figure 8.25(b) for 
N 
4 and period 2N = 8. The DCT-2 is defined by the transform pair 
XC2[k] 2 I:x[n] cos Crk(~:+ 1») , 
0:5 k :5 N - 1, 
(8.155) 
n=O 
1 N-l 
(k(2 
1»)
x[n] 
N  L 
,8[k]X c2 [k] cos 
1T 
2:+ 
, 
0:5 n < N -1, 
(8.156) 
k=O 

676 
Chapter 8 
The Discrete Fourier Transform 
where the inverse OCT-2 involves the weighting function 
{i'
tJ[k] = 
k 
0 
(8.157)
1, 
1:5 k :5 N - 1. 
In many treatments, the OCT definitions include normalization factors that make the 
transforms unitary.4 For example, the OCT-2 form is often defined as 
X
c2 [k] if; 
N-l 
(
2 -
nk(2n + 1) 
NtJ[k] L x[n] cos. 
2N 
) 
n=O 
• 
O:5k:5N 
1, 
(8.158) 
x[n] 
[2N-l 
)
V~ L Mk]X
c2 [k] cos (nk(~:+ 1) 
, °:5 n :5 N - 1, (8.159) 
k=O 
where 
~[k] = { 
k 
k 
0, 
1,2, ... , N 
1. 
(8.160) 
Comparing these equations with Eqs. (8.155) and (8.156), we see that the multiplicative 
factors 2, 1/N, and tJ[k] have been redistributed between the direct and inverse trans­
forms. (A similar normalization can be applied to define a normalized version of the 
OCT-I.) While this normalization creates a unitary transform representation, the defi­
nitions in Eqs. (8.152) and (8.153) and Eqs. (8.155) and (8.156) are simpler to relate to 
the OFf as we have defined it in this chapter. Therefore, in the following discussions, we 
use our definitions rather than the normalized definitions that are found, for example, 
in Rao and Yip (1990) and many other texts. 
Although we normally evaluate the OCT only for 0 :5 k :5 N -1, nothing prevents 
our evaluating the OCT equations outside that interval, as illustrated in Figure 8.26, 
wherein the OCT values for 0 :5 k < N - 1 are shown as solid dots. These figures 
illustrate that the OCTs also are even periodic sequences. However, the symmetry of 
the transform sequence is not always the same as the symmetry of the implicit periodic 
input sequence. While xdn] and the extension of Xc1[k] both have Type-1 symmetry 
with the same period, we see from a comparison of Figures 8.25( c) and 8.26(b) that the 
extended X c2 [k] ):las the same symmetry as X3 [n] ratherthan X2 [n]. Furthermore, xc2 [n] 
extends with period 4N while x2[n] has period 2N. 
Since the OCTs are orthogonal transform representations, they have properties 
similar in form to those of the OFT. These properties are elaborated on in some detail 
in Ahmed, Natarajan and Rao (1974) and Rao and Yip (1990). 
8.8.3 Relationship between the DFT and the DCT-1 
As might be expected, there is a close relationship between the OFf and the various 
classes ofthe OCT ofa finite-length sequence. To develop this relationship, we note that, 
41be DCT would be a unitary transform if it is orthonormal and also has the property that 
N-l 
N-l 
L(x[n])2 
L(X c2 [k])2. 
n=O 
k=O 

677 
Section 8.8 
The Discrete Cosine Transform (OCT) 
15 
12 
k 
-20 
(a) 
(b) 
Figure 8.26 
DCT-1 and DCT-2 for the four-point sequence used in Figure 8.25. 
(a) DCT-1. (b) DCT-2. 
since, for the OCT-I, xI[n] is constructed from xI[n] through Eqs. (8.150) and (8.151), 
one period of the periodic sequence xI[n] defines the finite-length sequence 
xl[n] = Xa [«n)hN-2] + xa [«-n»zN-2] 
[n], 
n 
0,1. ... , 2N - 3, 
(8.161) 
where xa[n] = a[n]x[n] is the N -point real sequence with endpoints divided by 2. From 
Eq. (8.161), it follows that the (2N - 2)-point OFT of the (2N - 2)-point sequence xIln] 
is 
x 1[k] = X ark] + X~[k] 
2Re{X ark]), 
k = 0, 1, ... , 2N - 3, 
(8.162) 
where X ark] is the (2N - 2)-point OFT ofthe N-point sequence a[n]x[n1; i.e., a[n]x[n] 
is padded with (N - 2) zero samples. Using the definition of the (2N 
2)-point OFT of 
the padded sequence, we obtain for k = 0, 1 ... , N 
1, 
XI[k] = 2Re{Xa[k]) = 2I:a[n]x[n] cos (2~~n2) 
X c1 [k]. 
(8.163) 
n=O 
Therefore, the OCT-l of an N-point sequence is identical to the first N points of Xl[k], 
the (2N - 2)-point OFT of the symmetrically extended sequence Xl [n], and it is also 
identical to twice the real part of the first N points of Xa[k], the (2N 
2)-point DFT of 
the weighted sequence xa[n]. 
Since, as discussed in Chapter 9, fast computational algorithms exist for the OFT, 
they can be used to compute the OFTs X ark] or X1[k] in Eq. (8.163), thus providing 
a convenient and readily available fast computation of the DCT-l. Since the definition 
of the OCT-l involves only real-valued coefficients, there are also efficient algorithms 
for computing the OCT-l of real sequences more directly without requiring the use 
of complex multiplications and additions. (See Ahmed, Natarajan and Rao, 1974 and 
Chen and Fralick, 1977.) 
The inverse DCT-l can also be computed using the inverse OFT. It is only nec­
essary to use Eq. (8.163) to construct X I[k] from X c1[k] and then compute the inverse 
(2N 
2)-point OFT. Specifically, 
cl
X [k1_{X [k], 
k=0, ... ,N-1, 
(8.164) 
1 
-
X c1 [2N-2-k], k=N, ... ,2N 
3, 

678 
Chapter 8 
The Discrete Fourier Transform 
and, using the definition of the (2N - 2)-point inverse DFf, we can compute the sym­
metrically extended sequence 
2N-3 
x [n] = 
1 
"X [k]ej2rrkn/(2N -2) 
n = 0, 1, ... , 2N - 3, 
(8.165)
1 
2N -2 ~ 1 
, 
k=O 
from which we can obtain x[n] by extracting the first N points, i.e., x[n] = xl[n] for 
n = 0, 1, ... , N - 1. By substitution of Eq. (8.164) into Eq. (8.165), it also follows that 
the inverse DCT-1 relation can be expressed in terms of X cl [k] and cosine functions, as 
in Eq. (8.153). This is suggested as an exercise in Problem 8.71. 
8.8.4 Relationship between the DFT and the DCT-2 
It is also possible to express the DCT-2 of a finite-length sequence x[n] in terms ofthe 
DFf. To develop this relationship, observe that one period of the periodic sequence 
x2[n] defines the 2N-point sequence 
x2[n] = x[((n)hN] + x[((-n - 1»2N] = x2[n], 
n = 0, 1, ... ,2N - 1, 
(8.166) 
where x[n] is the original N-point real sequence. From Eq. (8.166), it follows that the 
2N-point DFf of the 2N-point sequence X2 [n] is 
X2[k] = X [k] + X* [k]e j2rrk/(2N), 
k = 0, 1, ... , 2N -1, 
(8.167) 
where X [k] is the 2N-point DFf of the N -point sequence x[n]; i.e., in this case, x[n] is 
padded with N zero samples. From Eq. (8.167), we obtain 
X2[k] = X [k] + X*[k]e j2rrk/(2N) 
= ejrrk/(2N) (X [k]e- jrrk/(2N) + X*[k]ejrrk/(2N») 
(8.168) 
= ejrrk/(2N)2Re {X [k]e- jrrk/(2N) }. 
From the definition of the 2N-point DFf of the padded sequence, it follows that 
N-l 
. k 2 }" 
(Jrk(2n+1»)
Re { X [k]e-Jrr /( N) 
= ~ x[n] cos 
2N 
. 
(8.169) 
n=O 
Therefore, using Eqs. (8.155), (8.167), and (8.169), we can express XC2[k] in terms of 
X [k], the 2N-point DFf ofthe N-point sequence x[n], as 
XC2[k] = 2Re{X [k]e- jrrk/(2N) }, 
k = 0, 1, ... , N - 1, 
(8.170) 
or in terms of the 2N-point DFf of the 2N-point symmetrically extended sequence 
x2[n] defined by Eq. (8.166) as 
X C2[k] = e-jrrk/(2N) X 2[k], 
k = 0, 1, ... , N - 1, 
(8.171) 
and equivalently, 
X 2[k] = ejrrk/(2N) X C2 [k], 
k = 0, 1, ... , N - 1. 
(8.172) 
As in the case ofthe DCT-1, fast algorithms can be used to compute the 2N-point 
DFfs X[k] and X2[k] in Eqs. (8.170) and (8.171), respectively. Makhoul (1980) discusses 
other ways that the DFf can be used to compute the DCT-2. (See also Problem 8.72.) In 
addition, special fast algorithms for the computation of the DCT-2 have been developed 
(Rao and Yip, 1990). 

679 
Section 8.8 
I) 
» 
:e 
l) 
The Discrete Cosine Transform (OCT) 
The inverse DCT-2 can also be computed using the inverse DFf. The procedure 
utilizes Eq. (8.172) together with a symmetry property of the DCT-2. Specifically, it is 
easily verified by direct substitution into Eq. (8.155) that 
k = 0, 1, ... , 2N 
1, 
(8.173) 
from which it follows that 
xc2[O], 
k=O, 
e jrck/(2N) X C2[k], 
k = 1, ... , N 
1, 
x2[k] 
(8.174)
0, 
k 
N, 
_eink/(2N) X C2[2N 
k], 
k 
N + 1, N + 2, ... , 2N 
1.
I 
Using the inverse DFf, we can compute the symmetrically extended sequence 
2N-J 
x2[n] 
~"X [k]ei2rrkn/(2N) 
n = 0,1•...• 2N 
1, 
(8.175)
2N ~ 2 
, 
k=O 
from which we can obtainx[n] 
x2[n] forn = 0, 1, ... , N -1. By substituting Eq. (8.174) 
into Eq. (8.175), we can easily show that the inverse DCT-2 relation is that given by 
Eq. (8.156). (See Problem 8.73.) 
8.8.5 Energy Compaction Property of the DCT-2 
The DCT-2 is used in many data compression applications in preference to the DFf 
because ofa property that is frequently referred to as "energy compaction." Specifically, 
the DCT-2 of a finite-length sequence often has its coefficients more highly concentrated 
at low indices than the DFf does. The importance ofthis flows from Parseval's theorem, 
which, for the DCT-l, is 
N-I 
1 
N-l 
2
L a[nllx[n]1 = 2N _ 2 L a[k]IX cl [k]1 2 , 
(8.176) 
n=O 
k=O 
and, for the DCT-2, is 
N-l 
1 N-l
L Ix[n]l2 = N L 13[k]IX C2 [k]12• 
(8.177) 
n=O 
k=O 
where 13[k] is defined in Eq. (8.157). The DCT can be said to be concentrated in the 
low indices of the DCT if the remaining DCT coefficients can be set to zero without 
a significant impact on the energy of the signal. We illustrate the energy compaction 
property in the following example. 

680 
Chapter 8 
The Discrete Fourier Transform 
Example 8.1 3 
Energy Compaction in the DCT-2 
Consider a test input of the form 
x[n] = an cos(wOn + 4», 
n = 0,1, ... , N-1. 
(8.178) 
Such a signal is illustrated in Figure 8.27 for a = .9, wo = O.h, 4> = 0, and N = 32. 
• • 
• 
T 
• 
_lLI__________~--------~----------~--------~----------~--------~~ 
0.5 
:s 
0 I 
>< 
-0.5 
o 
5 
10 
15 
20 
25 
30 
n 
Figure 8.27 
Test signal for comparing OFT and OCT. 
The real and imaginary parts of the 32-point DFf of the 32-point sequence in 
Figure 8.27 are shown in Figures 8.28(a) and (b), respectively, and the DCf-2 of the 
sequence is shown in Figure 8.28(c). In the case of the DFf, the real and imaginary 
parts are shown for k = 0, 1, ... ,16. Since the signal is real, X[O] and X[16] are real. 
The remaining values are complex and conjugate symmetric. Thus, the 32 real numbers 
shown in Figures 8.28(a) and (b) completely specify the 32-point DIT. In the case of 
the DCf-2, we show all 32 of the real DCf-2 values. Clearly, the DCT-2 values are 
highly concentrated at low indices, so Parseval's theorem suggests that the energy 
of the sequence is more concentrated in the DCf-2 representation than in the DFf 
representation. 
This energy concentration property can be quantified by truncating the two 
representations and comparing the mean-squared approximation error for the two 
representations when both use the same number of real coefficient values. To do this, 
we define 
N-1 
x~ft[n] = ~ L Tm[k]X[k]ej2Jrkn/N, 
n = 0, 1, ... , N - 1, 
(8.179) 
k=O 
where, in this case, X [k] is the N-point DIT of x[n] and 
1, 
0 ~ k ~ (N - 1 - m)/2, 
Tm[k] = 
0, 
(N + 1 - m)/2 ~ k ~ (N - 1 + m)/2,
11, 
(N + 1 + m)/2 ~ k ~ N - 1. 
Ifm = 1, the term X[N12] is removed. Ifm = 3, then the terms X[N12] and X[N12 -1] 
and its corresponding complex conjugate X[N12 + 1] are removed, and so forth; i.e., 
x~ft[n] for m = 1,3,5, ... , N -1 is the sequence that is synthesized by symmetrically 
omitting m DIT coefficients.5 With the exception of the DIT value, X[N12], which is 
5For simplicity, we assume that N is an even integer. 

681 
Section 8.8 
The Discrete Cosine Transform (DCT) 
real, each omitted complex OFf value and its corresponding complex conjugate actu­
ally corresponds to omitting two real numbers. For example, m = 5 would correspond 
to setting the coefficients Xf14], X[15], X[16], X[17], and X[18] to zero in synthesizing 
x~ftrn] from the 32-point OFf shown in Figures 8.28(a) and (b). 
Likewise, we can truncate the OCT-2 representation, obtaining 
N-l-m 
x~ct[n] ! L ,8[klX c2[k] cos Crk(~~+ 1)) . 
O::5n::5N-l. 
(8.180) 
k=O 
In this case, if m 
5, we omit the OCT-2 coefficients X C2 [27], ... , X C2[31] in the 
synthesis of xgctrn] from the OCT-2 shown in Figure 8.28(c). Since these coefficients 
are very small, x~ct[n] should differ only slightly from x[nl 
6 
4 
~ 
~ 
2
~ 
2 
4 
10 
14 
Transform index k 
(a) 
5 
I 
I 
I 
I 
I 
I 
I 
1 
1 
! • " • • 
.. 
~ 
0 
~ -5-
­
I 
I 
I 
I 
. 
I
-10 
0 
2 
4 
6 
8 
10 
12 
14 
16 
Transform index k 
(b) 
20 
I 
I 
I 
I 
I 
~ 10 c­
"" 
i 1 
­
«0 ' T 
I 
I 
I 
I
-10 
0 
5 
10 
15 
20 
25 
30 
Transform index k 
(c) 
Figure 8.28 
(a) Real part of 32-point DFT; (b) Imaginary part of 32-point DFT; 
(c) 32-point DCT-2 of the test signal plotted in Figure 8.27. 
16 

682  
Chapter 8 
The Discrete Fourier Transform 
To show how the approximation errors depend on m for the OFf and the o Cf-2, 
we define 
N-J 
Edft[m] = ~ L Ix[n] 
x~ft[nlI2 
n=O 
and 
N-l 
Edct[m] = ~ L Ix[n] -
x~ct[nJI2 
n=O 
to be the mean-squared approximation errors for the truncated OFf and OCT, re­
spectively. These errors are plotted in Figure 8.29, with Edft[m] indicated with 0 and 
Edct [m] shown with •. For the special cases m = 0 (no truncation) and m = N - 1 (only 
the OC value is retained), the OFf truncation function is TO[k] = 1 for 0:'::. k :'::. N 
1 
and TN-l[k] = 0 for 1 :'::. k :'::. N - 1 and T N-1[O] =: 1. In these cases, both represen­
tations give the same error. For values 1 :'::. m :'::. 30, the OFf error grows steadily as m 
increases, whereas the OCT error remains very small-up to about m = 25-implying 
that the 32 numbers ofthe sequence x[n] can be represented with slight error by only 
seven OCT-2 coefficients. 
0.5 
DCT truncation error 
0.4 
DFT truncation error  
S... 0.3  
...  
'"  
'" e 0.2 
Il 
I 
I 
I 
... .0 
..0__ """",.0" 
0.1  ~__c--~-~-~--~-~-~-~--~-~­
0  
0  
5 
10 
15 
20 
25 
30 
Number of coefficients set to zero 
Figure 8.29 Comparison of truncation errors for DFT and DCT-2. 
The signal in Example 8.13 is a low frequency exponentially decaying signal with 
zero phase. We have chosen this example very carefully to emphasize the energy com­
paction property. Not every choice of x[n] will give such dramatic results. Highpass 
signals and even some signals of the form of Eq. (8.178) with different parameters do 
not show this dramatic difference. Nevertheless. in many cases of interest in data com­
pression, the DCf-2 provides a distinct advantage over the DFf. It can be shown (Rao 
and Yip, 1990) that the DCf-2 is nearly optimum in the sense of minimum mean-squared 
truncation error for sequences with exponential correlation functions. 
8.8.6 Applications of the DCT 
The major application of the DCT-2 is in signal compression, where it is a key part of 
many standardized algorithms. (See Jayant and Noll, 1984, Pau, 1995, Rao and Hwang, 

683 
Section 8.9 
Summary 
1996, Taubman and Marcellin, 2002, Bosi and Goldberg, 2003 and Spanias, Painter and 
Atti, 2007.) In this application, the blocks of the signal are represented by their cosine 
transforms. The popularity of the DCf in signal compression is mainly as a result of 
its energy concentration property, which we demonstrated by a simple example in the 
previous section. 
The DCT representations, being orthogonal transforms like the DFf, have many 
properties similar to those of the DFf that make them very flexible for manipulating 
the signals that they represent. One of the most important properties of the DFf is 
that periodic convolution of two finite-length sequences corresponds to multiplication 
of their corresponding DFfs. We have seen in Section 8.7 that it is possible to exploit 
this property to compute linear convolutions by doing only DFf computations. In the 
case of the DCf, the corresponding result is that mUltiplication of DCfs corresponds 
to periodic convolution of the underlying symmetrically extended sequences. However, 
there are additional complications. For example, the periodic convolution of two Type-2 
symmetric periodic sequences is not a Type-2 sequence, but rather, a 1)'pe-l sequence. 
Alternatively, periodic convolution of a 1)'pe-1 sequence with a Type-2 sequence of the 
same implied period is a Type-2 sequence. Thus, a mixture of DCfs is required to ef­
fect periodic symmetric convolution by inverse transformation of the product of DCfs. 
There are many more ways to do this, because we have many different DCT definitions 
from which to choose. Each different combination would correspond to periodic convo­
lution of a pair of symmetrically extended finite sequences. Martucci (1994) provides a 
complete discussion of the use of DCf and DST transforms in implementing symmetric 
periodic convolution. 
Multiplication of DCTs corresponds to a special type of periodic convolution that 
has some features that may be useful in some applications. As we have seen for the DFf, 
periodic convolution is characterized by end effects, or "wrap around" effects. Indeed, 
even linear convolution of two finite-length sequences has end effects as the impulse 
response engages and disengages from the input. The end effects of periodic symmetric 
convolution are different from ordinary convolution and from periodic convolution as 
implemented by multiplying DFfs. The symmetric extension creates symmetry at the 
endpoints. The "smooth" boundaries that this implies often mitigate the end effects 
encountered in convolving finite-length sequences. One area in which symmetric con­
volution is particularly useful is image filtering, where objectionable edge effects are 
perceived as blocking artifacts. In such representations, the DCf may be superior to 
the DFf or even ordinary linear convolution. In doing periodic symmetric convolution 
by multiplication of DCfs, we can force the same result as ordinary convolution by 
extending the sequences with a sufficient number of zero samples placed at both the 
beginning and the end of each sequence. 
8.9 SUMMARY  
In this chapter, we have discussed discrete Fourier representations of finite-length se­
quences. Most of our discussion focused on the discrete Fourier transform (DFf), which 
is based on the DFS representation of periodic sequences. By defining a periodic se­
quence for which each period is identical to the finite-length sequence, the DFfbecomes 

684  
Chapter 8 
The Discrete Fourier Transform 
identical to one period of the DFS coefficients. Because of the importance of this un­
derlying periodicity, we first examined the properties of DFS representations and then 
interpreted those properties in terms of finite-length sequences. An important result is 
that the DFT values are equal to samples of the z-transform at equally spaced points 
on the unit circle. This leads to the notion of time aliasing in the interpretation of DFf 
properties, a concept we used extensively in the study of circular convolution and its 
relation to linear convolution. We then used the results of this study to show how the 
DFT could be employed to implement the linear convolution of a finite-length impulse 
response with an indefinitely long input signal. 
The chapter concluded with an introduction to the DCT. It was shown that the 
DCT and DFT are closely related and that they share an implicit assumption of peri­
odicity. The energy compaction property, which is the main reason for the popularity of 
the DCT in data compression, was demonstrated with an example. 
Problems 
Basic Problems with Answers 
8.1. Suppose xc(t) is a periodic continuous-time signal with period 1 ms and for which the 
Fourier series is 
9 
xc(t) = L akej (2000Jrkt). 
k=-9 
The Fourier series coefficientsak are zero for Ikl > 9. xcCt) is sampled with a sample spacing 
T = ~ x 10-3 s to form x[nl. That is, 
x[nI 
Xc (6;00) . 
(a)  Is x[n] periodic and, if so, with what period? 
(b)  Is the sampling rate above the Nyquist rate? That is, is T sufficiently small to avoid 
aliasing? 
(c)  Find the DFS coefficients of x[n] in terms of ak. 
8.2.  Sup~osex[n]isa periodic sequence with period N.lhen:i[nl is also periodic with period3N. 
Let X [k] denote the DFS coefficients of X [n] considered as a periodic sequence with period 
N. and let X3[k] denote the DFS coefficients of x[n] considered as a periodic sequence with 
period 3N. 
(a)  Express X 3[k] in terms of X [k]. 
(b) By explicitly calculating X [k] and X3[k], verify your result in part (a) when x[n] is as 
given in Figure P8.2. 
x[n],N 2 
II 21 II 21 11 21 II 21 II 21 
-2 -I 0 
1 2 
3 
4 
5 
6 
7 
n 
Figure PB.2 

685 
Chapter 8 
Problems 
8.3. Figure P8.3 shows three periodic sequences xl[n] through x3[n]. These sequences can be 
expressed in a Fourier series as 
1 N~l _ 
. 
x[n] 
N L X [k]eJ (2rr/N)kn. 
k=O 
(8) For which sequences can the time origin be chosen such that all the X [kJ are real? 
(b) For which sequences can the time origin be chosen such that all the X [k] (except for 
k an integer multiple of N) are imaginary? 
(c) For which sequences does X [k] 
0 for k 
±2. ±4, ±6? 
... 11 .... 1111 .... 1111 • • 
n 
xz[n] 
I . . . . . 111 • • • • • 111 • • 
n 
1 1 I I 
111 I 
1 I I I 
n
I 
I I I I 
I I I I 
I 
Figure P8.3 
8.4. Consider the sequence x[n] given by x[n] 
aNu[n]. Assume lal < 1. A periodic sequence 
x[n] is constructed from x[n] in the following way: 
00 
x[n] 
L x[n + rN]. 
r=-oo 
(8) Determine the Fourier transform X (eiw) of x[nJ. 
(b) Determine the DFS coefficients X [k] for the sequence x[n]. 
(c) HowisX[k]relatedtoX(eiw)? 
8.5. Compute the DFf of each of the following finite-length sequences considered to be of 
length N (where N is even): 
(8) x[n] = B[n], 
(b) x[n] = B[n -no], 
0 S no S N -1, 
In even. 
0 S n S N 
1, 
(c) x[nJ= { 0:  nodd, 
OsnsN-l, 
(d)  x[nJ = {01" 0 s n S N /2 -1,  
N/2 S n S N -1,  

• • • 
• • • • 
686 
Chapter 8 
The Discrete Fourier Transform 
an  
O<n<N 
1, 
(e)  x[n] = { 0,' other;ise. 
8.6.  Consider the complex sequence 
jwon
e
, 
O::s n 
N - 1, 
x[n] 
{ 0, 
otherwise. 
(8)  Find the Fourier transform X (ejW) of x[n]. 
(b)  Find the N-point DFf X [k] of the finite-length sequence x[n]. 
(c)  Find the DFf of x[n] for the case Wo 
2nko/N, where kO is an integer. 
8.7.  Consider the finite-length se~uence x En] in Figure PB.7. Let X (z) be the z-transform of x[n]. 
If we sample X (z) at z 
eJ( rr j4)k, k 
0, 1,2,3, we obtain 
XI [k] = X(z) I =ej (2"i4)k, 
k = 0, 1, 2,3.
z
Sketch the sequence xI[n] obtained as the inverse DFf of X Ilk]. 
11  I I I I I 
-3 -2 -1 o 1 
2 
3 
4 
5 
6 
7 
8 
9 
n 
Figure PS.7 
8.8. Let X (e jW ) denote the Fourier transform of the sequencex[n 1 
(O.5)nu[nl. Let yen] denote 
a finite-duration sequence of length 10; i.e., yen] = 0, n < 0, and yEn] = 0, n ~ lO. The 10­
point DFf of yen], denoted by Y [k], corresponds to lO equally spaced samples of X(ejW ); 
i.e., Y [k] 
X(ej2rrk/1O). Determine yen]. 
8.9.  Consider a 20-point finite-duration sequence x[n] such that x[n] 
0 outside 0 ::s n ::s 19, 
and let X (ejW) represent the discrete-time Fourier transform of x[n]. 
(8)  If it is desired to evaluate X(ejW) at UJ 
4n/5 by computing one M-point Off, 
determine the smallest possible M, and develop a method to obtain X (e jW) at w = 4n /5 
using the smallest M. 
(b)  If it is desired to evaluate X(e jW) at w = lOn/27 by computing one L~oint Off, 
determine the smallest possible L, and develop a method to obtain X(ej1 rr/27) using 
the smallest L. 
8.10.  The two eight-point sequences Xl en] and x2[n] shown in Figure PB.lO have DFfs X I[k] and 
X 2lkl, respectively. Determine the relationship between Xl [k] and X2[k]. 
c 
Xt[nJ 
b 
d 
a 
e 
o 
2 
3 
4 
5 678 
n 
c 
x2[n] 
b 
a
[ 
d 
I 
e 
• • • I I 1• 
n
0 
1 
2 
3 
4 
5 
6 
7 
8 
Figure PS.10 

• 
• 
• 
• 
• 
• 
n 
Chapter 8 
Problems  
687 
8.11.  Figure PS.l1 shows two finite-length sequences Xl [n] and x2[n]. Sketch their six-point cir­
cular convolution. 
6 
xj[n]  
xz[n]
5
4 
r •
0 
2 
3 
4 
5 
n 
•  • • 0• •1 
2 
• • 
n 
Figure PB.ll 
]. 
8.12.  Suppose we have two four-point sequences x[n] and hEn] as follows: 
n 
x[n] 
cos C2' ) , 
n = 0, 1. 2. 3. 
hEn] = 2n. 
n = 0, 1. 2,3. 
(a)  Calculate the four-point DFf X [k]. 
(b)  Calculate the four-point DFf H[k]. 
(c)  Calculate yEn] = x[n]@)h[n] by doing the circular convolution directly. 
(d)  Calculate yEn] of part (c) by multiplying the DFfs of x[n] and h[n] and performing an 
inverse DFf. 
te 
8.13.  Consider the finite-length sequence x[n] in Figure PS.13. The five-point DFf of x[n] is
0­
denoted by X [k]. Plot the sequence yEn] whose DFf is
t); 
Y [k] 
W;2k X [k]. 
,9, 
r r r 
-2  
-1 
o 
2 
3 
4 
5 
6 
7 
n 
Figure PB.13 
8.14.  Two finite-length signals, x1[n] and x2[n], are sketched in Figure PS.14. Assume that xlln] 
and x2[n] are zero outside of the region shown in the figure. Let x3[n] be the eight-point 
circular convolution of xl [n1with x2[n]; i.e., X3 [n1 x1[n]® X2 [n]. Determine x3[2]. 
2 
2 
2 
-1 
0 
2 
3 
4 
5 
6 
7 
8 
9 
10 
3 
xz[n] 
2 
-1 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
Figure PB.14 

• 
• 
• 
• 
688 
Chapter 8 
The Discrete Fourier Transform 
8.15.  Figure PSJ5-1 shows two sequences xl[n] and x2[n]. The value of x2[n] at time n 
3 is 
not known, but is shown as a variable a. Figure PB.15-2 shows YIn], the four-point circular 
convolution of xl[n] and x2[n]. Based on the graph of YIn], can you specify a uniquely? If 
so, what is a? If not, give two possible values of a that would yield the sequence yIn] as 
shown. 
xtln] 
-L IL •
-1 
0 
1 
2 
3 
4 
5 
x2[n] 
-- L l
-1 
0 
1 
2 
3 
4 •5 
Figure P8.15-1 
_rl~ •5 
y[n] 
-lor 1 3 4 
-1 
-1 
Figure P8.15-2 
8.16.  Figure PS.16-1 shows a six-point discrete-time sequence x[n]. Assume thatx[n] = ooutside 
the interval shown. The value of x[4] is not known and is represented as b. Note that the 
sample shown for b in the figure is not necessarily to scale. Let X (e jW ) be the DTIT of x[n] 
and XI [k] be samples of X(ejW ) every 11: /2; i.e., 
jW
XI[k] = X(e 
)!w=(1t/2)k, 
0< k:::: 3. 
The four-point sequence Xl [n] that results from taking the four-point inverse DIT of X Ilk] 
is shown in Figure PB.16-2. Based on this figure, can you determine b uniquely? If so, give 
the value for b. 
r • rr rr • 
x[n] 
-1 
0 
1 
2 
3 
4 
5 
6 
7 
Figure P8.16-1 
xl[n] 
rII rr •
-1 
0 
1 
2 
3 
4 
5 
Figure P8.16-2 

689 
Chapter 8 
Problems 
S.t7.  Figure PS.17 shows two finite-length sequences Xl In] and xz[n]. What is the smallest N such 
that the N -point circular convolution of xl[n] and xz[n] are equal to the linear convolution 
of these sequences, i.e., such that xl[n] @xz[n] 
xl[n] * x2[n]? 
-1 
345 
-2 
2  
xz[n] 
-1 
0 
1 
2 
3 
6 
7 
-1  
Figure PS.17 
S.tS.  Figure PS.18-1 shows a sequence x[n] for which the value of x[3] is an unknown constant c. 
The sample with amplitude c is not necessarily drawn to scale. Let 
X l[k] = X [k]ej2rr3k/5. 
where X [k] is the five-point DFT of x[n]. The sequence xl[n] plotted in Figure P8.1S-2 is 
the inverse DFT of Xl [k]. What is the value of c? 
2 
x[n] 
5 
6
-1 
0 
-1 
Figure PS.1S-1 
2 
2 
-1 
0 
4 
5 
Figure PS.1S-2 
S.19.  Two finite-length sequences x[n] and XI[n] are shown in Figure PS.19. The DFTs of these 
sequences, X [k] and X 11k]. respectively, are related by the equation 
X 11k] = X [k]e- j(2JTkm/6) , 
where m is an unknown constant. Can you determine a value of m consistent with Fig­
ure PS.19? Is your choice of m unique? Ifso, justify your answer. If not, find another choice 
of m consistent with the information given. 

• 
• • • • 
• • 
• • • 
690 
Chapter 8 
The Discrete Fourier Transform 
3 
x[n] 
2 
-1 
0 
5 
6 
7 
-1 
-11 
3 
xl[n] 
2 
~ 
-1 
5 
6 
7 
-1 
Figure PB.19 
8.20.  Two finite-length sequences x[n] and xl[n] are shown in Figure P8.20. The N-point DFfs 
of these sequences, X [k] and X i[k], respectively, are related by the equation 
X ilk] = X [k]ej2nk2/N, 
where N is an unknown constant. Can you determine a value of N consistent with Fig­
ure P8.20? Is your choice for N unique? If so, justify your answer. If not, find another 
choice of N consistent with the information given. 
x[n]
rl r . 
-1 
2 
3 
4 
5 
6 
7 
o-J 
xl[n]
r r4 
0 
1 
2 
5 
6 
7 
3 -J 
Figure PB.20 
Basic  Problems 
8.21.  (8) Figure P8.21-1 shows two periodic sequences, xl[n] and x2[nj, with period N = 7. Find 
a sequence h[n] whose DFS is equal to the product of the DFS of xl[n] and the DFS 
of x2[n], i.e., 
Yl[k] = Xl[kJX2[kj. 

• • 
• • 
• 
• 
• 
• 
691 
Chapter 8 
Problems 
6 5 
·.. IIlIY ,1 0 .!Ifl t 
2 ,1 0 IIII t , .
•
-7 
-5 
-3 
-1 o 123 4 5 6  
n 
-7  -5 
-3 
-1 0 1 2 3 4 5 6 
n 
Figura P8.21-1 
(b)  Figure P8.21-2 shows a periodic sequence x3[n] with period N 
7. Find a sequence 
h[n] whose DFS is equal to the product of the DFS ofil [n] and the DFS ofi3[n], i.e., 
y2[k] 
X 1[k]X3[k]. 
... I . I .. II... I.. I • •• I. . 
-7  -5 
-3 
-1 0 1 2 3 4 5 6 
n 
Figura P8.21-2 
8.22. Consider an N -point sequence x[n], i.e., 
x [n] = 0 for n > N 
1 and n < O. 
The discrete-time Fourier transform of x [n] is X(ej(V), and the N -point DFT of x[n] is X[k). 
If'R.e {X[k]} = 0 for k 
0,1, .... N - 1, can we conclude that 'R.e {X(e jW )} 
0 for -JI" < 
w ~ JI"? If your answer is yes, explicitly show why. If not, give a simple counterexample. 
8.23. Consider the real finite-length sequence x[n] shown in Figure PB.23. 
x[n] 
rrrr •
-2  -1 
0 
1 
2 
3 
4 
5 
6 
n 
Figura PS.23 
(a) Sketch the finite-length sequence yEn] whose six-point DFT is 
Y [k] = Wgk X [k],  
where X [k] is the six-point DFT of x[n].  
(b)  Sketch the finite-length sequence wEn] whose six-point DFT is  
W [k] 
Im{X [k]}.  
(c)  Sketch the finite-length sequence q[n] whose three-point DFT is  
Q[k] 
X [2k + 1], 
k = 0, 1,2.  

692 
Chapter 8 
The Discrete Fourier Transform 
8.24. Figure P8.24 shows a finite-length sequence x[n]. Sketch the sequences 
xl[n] = x[«n 
2»4], 
O."S n ."S 3. 
and 
x2[n] 
x[«(-n»4], 
O."S n."S 3. 
iW 
x[n] 
-.-....... 
-.-.......  
o 1 2 3 
n 
Figure P8.24 
8.25.  Consider the signal x[n] = 8[n 
4] + 28[n - 5] + 8[n 
6]. 
(a)  Find X(eiw) the discrete-time Fourier transform of x[nJ. Write expressions for the 
magnitude and phase of X (eiw), and sketeh these functions. 
(b)  Find all values of N for which the N -point DFT is a set of real numbers. 
(c)  Can you find a three-point causal signal xl[n] (i.e., xl[n] 
0 for n < 0 and n > 2) for 
which the three-point DFT of xI[n] is: 
XI[k] = IX[k]1 
k = 0, 1,2  
where X[k] is the three-point DFT of x[n]?  
8.26.  We have shown that the DFT X[k] of a finite-length sequence x[n] is identical to samples of 
the DTFT X(eiw ) ofthat sequence at frequencies Wk = (2rr:/N)k; i.e., X[k] 
X(ei (2rcjN)k) 
for k = 0, 1. ... , N -1. Now consider a sequence yEn] = e-i(njN)nx[n] whose DFT is Y[k]. 
(a)  Determine the relationship between the DFT Y[k] and the DTFT X(eiw ). 
(b)  The result of part (a) shows that Y[k] is a differently sampled version of X (eiw). What 
are the frequencies at which X (eiw ) is sampled? 
(c)  Given the modified DFT Y[k], how would you recover the original sequence x[n]? 
8.27.  The lO-point DFT of a lO-point sequence g[n] is 
G[k] 
10 8[k] .  
Find G(eiw ), the DTFT of g[nl.  
8.28.  Consider the six-point sequence 
x[n] 
M[n] + 58[n - 1] + 48[n - 2] + 38[n - 3] + 28[n 
4] + 8[n - 5] 
shown in Figure P8.28. 
x[n] 
6 
5 
4 
3 r Tl 
-3 -2 -1 
0 
1 
2 
3 
4 
5 
6 
7 
8 
n 
Figure P8.28 

693 
Chapter 8  
Problems 
(a)  Determine X[k], the six-point DFT of x[n]. Express your answer in terms of 
jbr/ 6•
W6 
e­
(b)  Plot the sequence w[n], n 
0,1, ... ,5, that is obtained by computing the inverse 
six-point DFT of W[k] 
w6- 2k X[k]. 
(c)  Use any convenient method to evaluate the six-point circular convolution of x In] with 
the sequence h[n] = ,s[n] + £l[n - 1] + ,s[n - 2]. Sketch the result. 
(d)  If we convolve the given x[n] with the given h[n] by N-point circular convolution, how 
should N be chosen so that the result of the circular convolution is identical to the 
result of linear convolution? That is, choose N so that 
N-l 
yp[n] 
x[n] @h[n] L x[m]h[«n ­
m»N] 
m=O 
x[n] * h[n] 
00L x[mlh[n ­
m] for 0:::; n :::; N - 1. 
m=-oo 
(e)  In certain applications, such as multicarrier communication systems (see Starr et ai, 
1999), the linear convolution of a finite-length signal x[n] of length L samples with a 
shorter finite-length impulse response h[n] is required to be identical (over 0 :::; n :::: 
L - 1) to what would have been obtained by L-point circular convolution of x[n] with 
h[n]. This can be achieved by augmenting the sequence x[n] appropriately. Starting 
with the graph of Figure P8.28, where L = 6, add samples to the given sequence x[n] 
to produce a new sequence xl[n] such that with the sequence h[n] given in part (c), the 
ordinary convolution Yl[n] 
xl[n] *h[n] satisfies the equation 
00 
Yl[n] = xl[n] * h[n] = L xl[m]h[n 
m] 
m=-oo 
5 
= Yp[n] = x[n] ©h[n] = L x[m]h[«n 
m»6] for 0 :::: n :::; 5. 
m=O 
(f)  Generalize the result of part (e) for the case where h [n] is nonzero for 0 :::; n :::: M 
and x[n] is nonzero for 0 :::; n :::: L - L where M < L; i.e., show how to construct a 
sequence xl[n] from x[n] such that the linear convolution xl[n] * h[n] is equal to the 
circular convolution x[n] ©h[n] for 0 :::; n :::: L - 1. 
8.29. Consider the real five-point sequence 
x[n] = ,sIn] + ,s[n - 1] + ,s[n 
2) - ,s[n - 3] + ,s[n - 4]. 
The deterministic autocorrelation of this sequence is the inverse DTFI of 
C(ejW ) = X(ejw)X*(ejw ) = IX(ejW)12 , 
where X*(e jW ) is the complex conjugate of X(e}W). For the given x[n], the autocorrelation 
can be found to be 
ern] 
x[n] *x[-n]. 
(a)  Plot the sequence e[n]. Observe that e[ -n] 
e[n] for all n. 
(b) Now assume that we compute the seven-point DFT (N = 5) ofthe sequence x[n]. Call 
this DFT Xs[k). Then, we compute the inverse DFf of Cs[k] = Xs[k]X5'[k]. Plot the 
resulting sequence eS[n]. How is es[n] related to ern] from part (a)? 

694 
Chapter 8 
The Discrete Fourier Transform 
(c)  Now assume that we compute the lO-point DFf (N = 10) of the sequence x[n]. Call 
this DFT XlO[kj. Then, we compute the inverse DFf of CIO[k] = XlO[kjXio[k]. Plot 
the resulting sequence clO[nJ. 
(d)  Now suppose that we use XlO[k] to form DlO[k] = Wf~CIO[k] = Wf~XlO[k]Xio[k], 
where WIO 
e-j (21l'/1O). Then, we compute the inverse DFT of DlO[k]. Plot the re­
sulting sequence dlO[nj. 
8.30. Consider two sequences x[n] and h[nJ, and let yIn] denote their ordinary (linear) convolu­
tion, yIn] = x[n] *h[n]. Assume that x[n] is zero outside the interval 21 ::::: n ::::: 31, and h[n] 
is zero outside the interval 18 ::::: n ::::: 31. 
(a)  The signal yIn] will be zero outside of an interval Nl ::::: n ::::: N2' Determine numerical 
values for Nl and N2' 
(b)  Now suppose that we compute the 32-point DFTs of 
n = 0, 1, ...• 20 
xI[n] = {~[n] 
n =21,22, ... ,31 
and 
n = 0.1. ... ,17
hI[n] 
{ ~[n] 
n 
18.19, .... 31 
(Le., the zero samples at the beginning of each sequence are included). Then, we form 
the product YI[k] 
Xl[k]Hl[k]. If we define Yl[n] to be the 32-point inverse DFf of 
YI[k], how is YI[n] related to the ordinary convolution yIn]? That is, give an equation 
that expresses yt£n] in terms of y[n] for 0::::: n ::::: 31. 
(c)  Suppose that you are free to choose the DFT length (N) in part (b) so that the se­
quences are also zero-padded at their ends. What is the minimum value of N so that 
Yl[n] = yIn] forO::::: n:5 N -I? 
8.31.  Consider the sequence x[n] = 28[nl + 8[n -IJ - 8[n 
2]. 
(a)  Determine the DTFT X(e jW) of x[n] and the DTFT Y(ejW ) of the sequence y[nl = 
x[-n]. 
(b)  Using your results from part (a) find an expression for 
W(ejU) 
X(ejW)Y(ejW). 
(c)  Using the result of part (b) make a plot of wIn] 
x[n] *yIn]. 
(d) Now plot the sequence yp[n] = x[«-n»4] as a function of n for 0::::: n ::::: 3. 
(e)  Now use any convenient method to evaluate the four-point circular convolution of x[n] 
with Yp[n]. Call your answer wp[n] and plot it. 
(f)  If we convolve x[n] with Yp[n] = x[« -n»N], how should N be chosen to avoid time­
domain aliasing? 
8.32.  Considerafinite-durationsequencex[njoflengthPsuchthatx[n] 
Oforn < Oandn:::: P. 
We want to compute samples of the Fourier transform at the N equally spaced frequencies 
2rck 
Wk= N 
k = 0,1, ... , N-1. 
Determine and justify procedures for computing the N samples of the Fourier transform 
using only one N-point DFT for the following two cases: 
(a)  N> P. 
(b)  N < P. 

695 
m 
Chapter 8 
Problems 
ali 
8.33. An FIR filter has a lO-point impulse response, i.e., 
jot 
h[n] 
0 
for n < 0 and for n > 9. 
~], 
Given that the lO-point DFT of h[n] is given by 
1 
1 
H[k] 
-8[k 
1] + 38[k 
7],
u­
5 
n] 
find H{e jW ), the DTFr of h[n]. 
8.34. Suppose  that xl[n) and x2[n] are two finite-length sequences of length N, i.e., xl [n] 
x2[n] = 0 outside 0 ~ n :::: N 
1. Denote the z-transform of XI [n] by XI (z), and denote the 
N-point DFT of x2[n] by X2[k]. The two transforms X I (z) and X2[k] are related by: 
:al 
k = 0,1, ... , N - 1
X2[k] = Xl (Z)i ,", ~ e-j2'!vk 
z 
2 
Determine the relationship between xI[n] and x2[n]. 
rm 
of 
Advanced Problems 
on 
8.35. Figure PS.35-1 illustrates a six-point discrete-time sequence x[n]. Assume that x[n] is zero 
,e­
outside the interval shown. 
iat 
2  
2 
b
x[nl 
1 
1 
[n) 
o 
-1  
0 
2 
3 
4 
5 
6 
7 
Figure PB.35-1
le-
P. 
The value of x[4] is not known and is represented as b. The sample in the figure is
ies 
not shown to scale. Let X(e jW ) be the DTFT of x[n] and Xl[k] be samples of X(eiw) at 
Wk = 2:rrkj4, i.e., 
0:::: k:::: 3.
rm 
The four-point sequence XI [n] that results from taking the four-point inverse DFT of X I [k] 
is shown in Figure PS.35-2. Based on the figure can you determine b uniquely? If so, give 
the value of b. 

696 
Chapter 8 
The Discrete Fourier Transform 
4 
X1[n] 
2 
2 
• 
• n 
-1 
o 
1 
2 
3 
4 
5 
Figure PB.35-2 
8.36.  (a) X(e jW ) is the DTFT of the discrete-time signal 
x[n] = (1/2)n u[n]. 
Find a length-5 sequence g[n] whose five-point DFT G[k] is identical to samples of the 
DTFT of x[n] at COk = 21fk/5, i.e., 
g[n] =°for n < 0, n > 4 
and 
X(ej21Ck /5) for k
G[k]  
= 0, 1, ... ,4. 
(b) Let w[nl be a sequence that is strictly nonzero for °~ n ~ 9 and zero elsewhere, i.e., 
w[n] =fo 0, °~ n ~ 9 
w[n] = ° otherwise 
Determine a choice for w[n] such that its DTFT W(e jW ) is equal to X(e jW) at the 
frequencies co =27fk/5, k =0,1, ...• 4, i.e., 
W (ej21Ck/5) 
X(e j2rrk/5) for k = 0, 1, ... ,4. 
8.37.  A discrete-time LTI filter S is to be implemented using the overlap-save method. In the 
overlap-save method, the input is divided into overlapping blocks, as opposed to the 
overlap-add method where the input blocks are nonoverlapping. For this implementa­
tion, the input signal x[n] will be divided into overlapping 256-point blocks Xr [n]. Adjacent 
blocks will overlap by 255 points so that they differ by only one sample. This is represented 
by Eq. (P8.37-1) which is a relation between xr[n] and x[n], 
x [n] = {x[n + rJ °~ n ~ 255 
(P8.37-1) 
r 
° 
otherwIse, 
where r ranges over all integers and we obtain a different block xr[n] for each value of r. 

697 
Chapter 8 
Problems 
Each block is processed by computing the 256-point DFf of xr[n], multiplying the result 
with H[k] given in Eq. (P8.37-2), and computing the 256-point inverse DFf of the product. 
I 
0:5 k :5 31 
H[k] 
011 
32:5 k :5 224 
(P8.37-2) 
225 :5 k :5 255 
One sample from each block computation (in this case only a single sample per block) is 
then "saved" as part of the overall output. 
(0)  Is S an ideal frequency-selective filter? Justify your answer. 
(b)  Is the impulse response of S real valued? Justify your answer. 
(c)  Determine the impulse response of S. 
8.38.  x[n] is a real-valued finite-length sequence of length 512, i.e., 
x[n] = 0 
n < 0, n 2: 512 
and has been stored in a 512-point data memory. It is known that X[k] the 512-point DFf 
of x[n] has the property 
X[k] = 0 
250 :5 k :5 262. 
In storing the data, one data point at most may have been corrupted. Specifically, if s[n] 
denotes the stored data, s[n] = x[n] except possibly at one unknown memory location no. To 
test and possibly correct the data, you are able to examine S[k], the 512-point DFf of s[n]. 
(0)  Specify whether, by examining S[k], it is possible and if so, how, to detect whether an 
error has been made in one data point, i.e., whether or not s[n) = x[n] for all n. 
In parts (b) and (c), assume that you know for sure that one data point has been corrupted, 
i.e., that s[n] = x[n] except at n 
nO. 
(b)  In this part, assume the value of no is unknown. Specify a procedure for determining 
from S[k] the value of no. 
(c)  In this part, assume that you know the value of no. Specify a procedure for determining 
x[no] from S[k). 
8.39.  In the system shown in the Figure P8.39, Xl [n] and x2[n] are both causal, 32-point sequences, 
i.e., they are both zero outside the interval 0:5 n 
31. y[n] denotes the linear convolution 
ofxl[n) and x2[n], i.e., y[n] = xI[n] *x2[n]. 
N-point 
xs[n) = x3[n) ® x4[n) 
Circular 
Convolution 
Figure PB.39 
(a)  Determine the values of N for which all the values ofy[nl can be completely recovered 
from x5[n]. 
(b)  Specify explicitly how to recover y[n] from x5[n) for the smallest value of N which you 
determined in part (a). 

698  
Chapter 8 
The Discrete Fourier Transform 
8.40.  Three real-valued seven-point sequences (xl [n],x2[n], and x3[n]) are shown in FigureP8.40. 
For each of these sequences, specify whether the seven-point DFI can be written in the form 
Xi [kJ 
A;[k]e-j(2rrk/7)kai 
k 
0,1, ... ,6 
where Ai[k] is real-valued and 2!X; is an integer. Include a brief explanation. For each 
sequence which can be written in this form, specify all corresponding values of !Xi for 
O:'::!Xi < 7. 
(a)J(b) 
~)
LL 
0 
1 
2 
(a) 
Jr 
(b) 
T 
I 
0 
1 
2 
(a)• 
---~. 
0 
1 
2 
(a) 
xj[n) 
n
3  
4 
5 
6 
xz[n] 
(b)  
(b) 
(b)
• 
T
i 
3 
4 
5 
~n 
6 
x3[n] 
• 
3 
(b) 
T 
4 
(b) 
1 
I 
5 
•6 
n Figure P8.40 
8.41.  Suppose x[n] is the eiglft-point complex-valued sequence with real part x,[nJ and imagi­
nary part Xi [nJ shown in Figure P8.41 (i.e., x[n] = xr[n] + jx; [nl). Let y[n] be thefour-point 
complex-valued sequence such that Y[k], the four-point DFI of y[n], is equal to the odd­
indexed values of X[k], the eight-point DFI of x[n] (the odd-indexed values of X[kJ are 
those for which k = 1, 3, 5, 7). 

• 
• 
• 
• 
699 
sform 
P8.40. 
:form 
. each 
11; for 
Iimagi­
If-point 
he odd­
([k] are 
Chapter 8 
Problems 
x,[n] 
(2) 
(1) 
T 
r 
0 
1 
2 
3 
i4 
5 
6 
7 
n 
~ 
1
(-1) 
(-1) 
(3) 
(3) 
xi[n] 
I  
I 
5-~.­
6 
7 
n
~·1f·3­ 4  
(-1) 
(-3) 
figure PB.41 
Determine the numerical values of y,.[n] and Yi [n], and the real and imaginary parts of y[n). 
8.42. x[n] is a finite-length sequence of length 1024, i.e., 
x[n] = 0 for n < O. n > 1023. 
The autocorrelation of x[n] is defined as 
coL  x[n]x[n + m]. 
n=-oo 
and XN[k] is defined as the N-point DFT of x[n], with N 2: 1024. 
We are interested in computing cx.dm]. A proposed procedure begins by first computing 
the N-point inverse DFT of IXN[k]12 to obtain an N-point sequence gN[n]. i.e., 
gN[n] = N-point IDFT {IXN[kI12 } . 
(II)  Determine the minimum value of N so that cxxlm] can be obtained from gN[n]. Also 
specify how you would obtain cxxlm] from gN[n]. 
(b)  Determine the minimum value of N so that cxxlm] for Iml :::: 10 can be obtained from 
gN[n]. Also specify how you would obtain these values from gN [n]. 

700  
Chapter 8 
The Discrete Fourier Transform 
8.43. In Figure P8.43, x[n] is a finite-length sequence oflength 1024. The sequence R[k] is obtained 
by taking the 1024-point DFf of x[n] and compressing the result by 2. 
r[nJ 
Figure PS.43 
(a)  Choose the most accurate statement for r[n], the 512-point inverse DFfof R[k]. Justify 
your choice in a few concise sentences. 
(i) r[n] 
x[n], 0:::; n :::; 511  
(ii)  r[n] = x[2n], 0:::; n :::; 511  
(iii)  r[n] 
x[nJ + x[n + 512], 0:::; n :::; 511  
(iv)  r[n] = x[n] + x[-n + 512], 0:::; n :::; 511  
(v) r[nJ = x[n] + x[1023 
nJ. 0:::; n :::; 511  
In all cases r[n] 
0 outside 0:::; n :::; 511.  
(b)  The sequence Y[k] is obtained by expanding R[k] by 2. Choose the most accurate state­
ment for y[nJ, the 1024-point inverse DFf of Y[k]. Justify your choice in a few concise 
sentences. 
0) y[nl = {i (x[nJ + x'.n + 5121.), 
0:::; n :::; 511 
i (x[nJ + x[n - 512]). 
512:::; n :::; 1023  
(ii)  y[nJ = {x[n]. 
0:::; n :::; 511  
x[n - 512], 
512:::; n :::; 1023  
(iii)  y[nj 
{x[n1, 
n even  
0, 
n odd  
X[2nJ, 
0 < n < 511 
(iv)  y[n] 
{ x[2(n - 512)], 
512 
n:::; 1023  
(v) y[nJ = i (x[nJ + x[1023 
n]), 0:::; n :::; 1023  
In all cases y[n] 
0 outside 0 :::; n :::; 1023.  

Chapter 8 
Problems  
701 
8.44. Figure PS.44 shows two finite-length sequences Xl In] and x2[n] oflength 7. Xi (eiw ) denotes 
the DlFf of x;[nJ, and X;[k] denotes the seven-point DFf of xi[n]. 
4 
1 
1 
1 
~ 
I 
2 
5 
6 
3 
5 
n 
0 
3 
4 
0 
2 
4 
-1 -1 
-1 -1 
-1 
-1 
-1 
1 
n 
6 
Figure PS.44 
For each of the sequences xiln] and x2[n], indicate whether each one of the following 
properties holds: 
(a)  Xi (eiw ) can be written in the form 
Xi (eiw) = Ai (w)eiO:iW , 
for WE (-Jr, Jr), 
where Ai (w) is real and (Xi is a constant. 
(b)  X;[k] can be written in the form 
Xi[k] = Bilk]eitlik , 
where Bi [k] is real and Pi is a constant. 
8.45.  1be sequence x[n] is a 12S-point sequence (i.e., x[n] = 0 for n < (I and for n > 127), and 
x[n] has at least one nonzero sample. The DTFf of x[n] is denoted X(e jW ). What is the 
largest integer M such that it is possible for X (ei2nk/ M) to be zero for all integer values of 
k? Construct an example for the maximal M that you have found. 
8.46. Each part ofthis problem may be solved independently. All parts use the signal x[n] given by 
x[n] = 30[n] - o[n -1] + 2o[n - 3] + o[n - 4] -
o[n 
6]. 
(a)  Let X (eiw) be the DlFf of x[n]. Define 
R[k] = X (eiW )!w=24k '  
Plot the signal r[n] which is the four-point inverse DFf of R[k].  
(b)  Let X[k] be the eight-point DFf of x[n], and let H[k] be the eight-point DFf of the 
impulse response h[n] given by 
h[n] = o[n] 
o[n - 4]. 
Define Y[k] 
X[k]H[k] for (I :::: k :::: 7. Plot yIn], the eight-point DFf of Y[k]. 

702  
Chapter 8 
The Discrete Fourier Transform 
8.47.  Consider a time-limited continuous-time signal xc<t) whose duration is 100 ms. Assume 
that this signal has a bandlimited Fourier transform such that xc(jn) 
0 for Inl 
2iT(1O,000) rad/s; i.e., assume that aliasing is negligible. We want to compute samples of 
Xcun) with 5 Hz spacing over the interval 0 ::s n ::: 2JT(1O, 000). This can be done with a 
4ooo-point DFf. Specifically, we want to obtain a 4000-point sequence x[n] for which the 
4ooo-point DFf is related to Xcun) by: 
X[k] 
exXC<j2JT ·5 . k), 
k = 0, 1, ... , 1999. 
(P8.47-1) 
where ex is a known scale factor. The following method is proposed to obtain a 4000-point 
sequence whose DFf gives the desired samples of XcUn). First, xc(t) is sampled with a 
sampling period of T = 501ls. Next, the resulting 2000-point sequence is used to form the 
sequence x[n] as follows: 
I 
xc(nT), 
0 ::s n ::s 1999. 
x[n] 
xc«n 
2000)T), 2000 ::sn 
3999, 
(P8.47-2) 
0, 
otherwise. 
Finally, the 4000-point DFf i[k] of this sequence is computed. For this method, determine 
how i[k] is related to xc(jn). Indicate this relationship in a sketch for a "typical" Fourier 
transform xc(jn). Explicitly state whether or not i[k] is the desired result, i.e., whether 
irk] equals X[k) as specified in Eq. (P8.47-1). 
8.48.  x[n] is a real-valued finite length sequence of length 1024, 
x[n] = 0 
n < 0, n > 1023. 
Only the following samples of the 1024-point DFf of x[n] are known 
X[k] 
k = 0, 16, 16 x 2,16 x 3, ... , 16 x (64 -1) 
Also, we observe sEn] which is a corrupted version of x[n], with first 64 points corrupted, i.e., 
sEn] 
x[n] for n 2: 64, and s[n] '" x[n], for 0 ::s n ::s 63. Describe a procedure to recover the 
first 64 samples ofx[n] using only 1024-point DFf and IDFf blocks, multipliers, and adders. 
8.49.  The deterministic crosscorrelation function between two real sequences is defined as 
00  
00 
cxy[n] 
L 
y[m]x[n + m] = L y[-m]x[n - m] = y[-n] *x[n] 
-
00 < n < 00 
m=-oo 
m=-oo 
(a)  Show that the DTFT of cxy[n] is CXy(eiw ) = X(eiw)Y*(eiw). 
(b)  Suppose that x[n] = 0 for n < 0 and n > 99 and y[n] = 0 for n < 0 and n > 49. The 
corresponding crosscorrelation function exy[n] will be nonzero only in a finite-length 
interval Nt ::: n ::s N2' What are Nl and N27 
(c)  Suppose that we wish to compute values of cx), [n) in the interval 0 ::s n ::s 20 using the 
following procedure: 
(i) Compute X[k], the N-point DFf of x[n] 
(ii)  Compute Y[k], the N-point DFfof yEn] 
(iii)  Compute C[k] = X[k]Y*[k] for 0::: k ::s N 
1 
(iv) Compute e[n], the inverse DFf of C[k] 
What is the minimum value of N such that c[n] = cxy[n], 0 ::s n ::: 207 Explain your 
reasoning. 

703 
Chapter 8 
Problems 
8.50.  The DFf of a finite-duration sequence corresponds to samples of its z-transform on the 
unit circle. For example, the DFf of a 10-point sequence x[n] corresponds to samples of 
X(z) at the 10 equally spaced points indicated in Figure PB.SO-l. We wish to find the equally 
spaced samples of X(z) on the contour shown in Figure PB.SO-2; i.e., we wish to obtain 
X (z) Iz=O.5ei [(2n:k11O)+(1r/1Ol]' 
Show how to modify x [n] to obtain a sequence xdn] such that the DFf ofx1[n] corresponds 
to the desired samples of X (z). 
9>m 
z-plane 
10 a Jans
Unit  
211'R d' 
circle 
Circle with 
radius = 1. 
2 
/ 211" 
z-plane 
/ 
10
~
/ 
_ 
-­
-211" 
20 
Figure PS.50-1 
Figure PS.50-2 
8.51. Let w[n] denote the linear convolution ofx[nJ and y[n]. Let g[n] denote the40-pointcircular 
convolution of x[n] and y[n]: 
w[n] = x[n] * y[n] 
00L x[k]y[n ­ k), 
k=-oo 
g[n] 
x[n]@)y[n] = 
39L x[k]y[«n - k»40]· 
k=O 
(a)  Determine the values of n for which w[n] can be nonzero. 
(b)  Determine the values of n for which w[n] can be obtained from g [n]. Explicitly specify 
at what index values n in g[n] these values of w[n] appear. 
8.52.  Let x[n] 
0, n < 0, n > 7, be a real eight-point sequence, and let X [k] be its eight-point 
DFf. 
(a)  Evaluate 
in terms of x[n]. 

704 
Chapter 8 
The Discrete Fourier Transform 
(b)  Let urn] 
0, n < 0, n > 7, be an eight-point sequence, and let V[k] be its eight-point 
DFf. 
If V[k] 
X(z) at z = 2 exp(J (27l'k + 7l')/8) for k = 0, ... ,7, where X(z) is the z-
transform of x[n], express urn] in terms ofx[n]. 
(c)  Let w[n] = 0, n < 0, n > 3, be a four-point sequence, and let W[k] be its four-point 
DFf. 
If W[k] = X [k] + X[k +4], express w[n] in terms of x[n]. 
(d)  Let y[n] = 0, n < 0, n > 7, be an eight-point sequence, and let Y [k] be its eight-point 
DFf. 
If 
Y [k] = { 2X [k], k = 0, 2, 4, 6, 
0, 
k 
1,3,5,7, 
express y[n] in terms of x[n]. 
8.53. Read each part of this problem carefully to note the differences among parts. 
(a)  Consider the signal 
x[nJ = { 1 + cos(7l'n/4) 
0.5 cos(37l'n/4), 
0 ~ n ~ 7, 
0, 
otherwise, 
which can be represented by the IDFf equation as 
j 
7,
x[n] = I~to X s[k]e (21rk/S)n, o ~ n ~ 
0,  
otherwise, 
where X s[k] is the eight-point DFf of x[n]. Plot XS[k] for 0 ~ k ~ 7. 
(b)  Determine V16[k], the 16-point DFf of the 16-point sequence 
u[n] = { 1 + cos(7l'n/4) 
0.Scos(37l'n/4), 
O::s n ::s 15, 
0, 
otherwise. 
Plot V16[k] for 0 ~ k ::s 15. 
(c)  Finally, consider IX 16[k]l, the magnitude of the 16-point DFf of the eight-point se­
quence 
x[n] = { 1 + cos(7l'n/4) 
0.5 cos(37l'n/4), 
0 ~ n ::s 7, 
0, 
otherwise. 
Plot IX 16[kll for O::s k::s 15 without explicitly evaluating the DFT expression. You will 
not be able to find all values of IX 16[k]1 by inspection as in parts (a) and (b), but you 
should be able to find some of the values exactly. Plot all the values you know exactly 
with a soLid circle, and pLot estimates of the other values with an open circle. 

705 
Chapter 8 
Problems 
Extension Problems 
8.54.  In deriving the DFS analysis Eq. (8.11), we used the identity of Eq. (8.7). To verify this 
identity, we will consider the two conditions k - r 
mN and k - r f- mN separately. 
(0)  For k - r = mN, show that ej(2rr/N)(k-r)n = 1 and, from this, that 
!
N-l 
L 
ej (2rr/N)(k-r)n = 1 
fork 
r 
mN. 
(P8.54-1) 
n=O 
(b)  Since k and r are both integers in Eq. (8.7), we can make the substitution k - r 
e 
and consider the summation 
N-l 
N-l
! L 
ej(2rr/N)ln=! L[ejCbr/N)ft. 
(P8.54-2) 
n=O 
71=0 
Because this is the sum of a finite number of terms in a geometric series, it can be 
expressed in closed form as 
1 N-l 
1 1 
J'(2rr/N)tN
'(2 /N)£ 
e 
_ 
[eJ:n: 
]n = ----c=---,-:~ 
(P8.54-3)
N L 
N 1
n=O 
For what values of eis the right-hand side of Eq. (P8.54-3) equation indeterminate? 
That is, are the numerator and denominator both zero? 
(c)  From the result in part (b), show that if k 
r f- mN, then 
N-l 
~ L 
ej (2:n:/N)(k-r)n 
O.  
(P8.54-4) 
n=O 
8.55.  In Section 8.2, we stated the property that if 
xl(n] 
x[n 
m], 
then 
-
WN 
km­
XI[k] = 
X [k], 
where X[k] and Xl[k] are the DFS coefficients ofX[n] and Xl en], respectively. In this prob­
lem, we consider the proof of that property. 
(0)  Using Eq. (8.11) together with an appropriate substitution ofvariables, show that Xl [k] 
can be expressed as 
N-l-m 
m
XI[k] =wt
L 
x[r]Wff· 
(P8.55-1) 
r=-m 
(b)  The summation in Eq. (P8.55-1) can be rewritten as 
N-l-m 
-1 
N-l-m 
L 
x[r]Wff = L 
x[r]Wff + 
L 
x[r]Wff· 
(P8.55-2) 
r=-m 
r=-m 
r=O 
Using the fact that x[r] and Wff are both periodic, show that 
-1 
N-l
L x[r]Wff = L x[r]Wff· 
(P8.55-3) 
r=-m 
r=N-m 

706 
Chapter 8 
The Discrete Fourier Transform 
(c)  From your results in parts (a) and (b), show that 
N-l 
Xl[k] 
W~m L i[r]W; 
W~mX[k]. 
r=O 
8.56.  (8) Table 8.1 lists a number of symmetry properties of the DFS for periodic sequences, 
several of which we repeat here. Prove that each of these properties is true. In carrying 
out your proofs, you may use the definition of the DFS and any previous property in 
the list. (For example, in proving property 3, you may use properties 1 and 2.) 
Sequence 
DFS 
1. i*[II] 
X*[-kJ 
2. i*[ -n] 
X*[k] 
3. Reli[n]} 
Xe[k] 
4. jImli[nlJ 
XoLkJ 
(b)  From the properties proved in part (a), show that for a real periodic sequence i[n], 
the following symmetry properties of the DFS hold: 
1.  Re{X Lk]) = RelX [-klJ 
2.  Im{X' [k]) = -Im{X [-klJ 
3.  IX [k]1 = IX [-kJI 
4.  Lx [k] = -Lx [-k] 
8.57.  We stated in Section 8.4 that a direct relationship between X(eiUJ ) and X[k] can be derived, 
where X' [k] is the DFS coefficients of a periodic sequence and X (e jUJ ) is the Fourier trans­
form of one period. Since X[k] corresponds to samples of X(e1CV ), the relationship then 
corresponds to an interpolation formula. 
One approach to obtaining the desired relationship is to rely on the discussion of 
Section 8.4, the relationship of Eq. (8.54), and the modulation property of Section 2.9.7. 
The procedure is as follows: 
1.  With X[k] denoting the DFS coefficients of i[n], express the Fourier transform 
X(e jUJ ) of iln] as an impulsc train; i.e., scaled and shifted impulse functions Sew). 
2.  From 
(8.57), x[n] can be expressed as x[n1 
x[n]wln]. where w[n] is an appro­
priate finite-length window. 
3.  Since X[II] = i[n)w[n], from Section 2.9.7, X (eiw)can be expressed as the (periodic) 
convolution of X(e jW ) and W(e jW ). 
By carrying out the details in this procedure, show that X (eiw ) can be expressed as 
X(e jW )  
~ LX [k] .sin[(wN - 21rk)/2] e- j [(N-1)/2](w-2rrk/N). 
N 
k 
sm[[w - (21rk/N)]/2) 
Specify explicitly the limits on the summation. 
8.58.  Let X [k] denote the N -point DFT of the N -point sequence x[n]. 
(8)  Show that if 
x[n] 
-x[N -1 - n], 
then X [0) 
O. Consider separately the cases of N even and N odd. 
(b)  Show that if N is even and if 
x[n] = x[N 
1 - n], 
then X [N/2] = O. 

707 
Chapter 8 
Problems 
8.59.  In Section 2.8, the conjugate-symmetric and conjugate-antisymmetric components of a se­
quence x[n] were defined, respectively, as 
1 
xe[n] 
2(x[n] +x*[-n]), 
1 
xoln] = 2 (x[n] 
x*[-n]). 
In Section 8.6.4, we found it convenient to define respectively the periodic conjugate­
symmetric and periodic conjugate-antisymmetric components of a sequence of finite dura­
tion N as 
Xep[n] = !(x[«n»N]+x*[((-n))N])' 
O:'Sn:'SN 
1. 
xop[n] = !(x[«n))N] - x*[«-n))N])' 
O:'Sn:'SN 
1. 
(a)  Show that xep[n] can be related to xe[n] and that xop[n] can be related to xo[n] by the 
relations 
xep[nJ = (xe[n1 +xe[n - N]), 
O:'S n:'S N -1, 
xop[n] = (xo[n1+xo[n 
N]), 
O:'Sn:'SN 
1. 
(b)  x[n] is considered to be a sequence of length N, and in general, xe[n] cannot be re­
covered from xep[n], and xo[n] cannot be recovered from xop[n]. Show that with x[n] 
considered as a sequence of length N, but with x[n] = 0, n > Nj2.xe[n] can be ob­
tained from Xep[n], and xo[n] can be obtained from xop[nj. 
8.60. Show from Eqs. (8.65) and (8.66) that with x[n] as an N-point sequence and X [kj as its 
N-point DFf, 
lI:1Ix[n]12 = ~ ~ IX [k]12. 
n=O 
k=O 
This equation is commonly referred to as Parseval's relation for the DFf. 
8.61.  x[n] is a real-valued, nonnegative, finite-length sequence of length N; i.e., x[n] is real and 
nonnegative for 0 :'S n :'S N - 1 and is zero otherwise. The N-point DFf of x[n] is X [k], 
and the Fourier transform of x[n] is X(ejW ). 
Determine whether each of the following statements is true or false. For each state­
ment, if you indicate that it is true, clearly show your reasoning. If you state that it is false, 
construct a counterexample. 
(8)  If X(ejW) is expressible in the form 
X (ejW ) = B(w)ejaw , 
where B(w) is real and a is a real constant, then X [k] can be expressed in the form 
X [k] 
A[k]ejyk , 
where A[k] is real and y is a real constant. 
(b)  If X [k] is expressible in the form 
X [k] = A[k]ejyk • 
where A[k] is real and y is a real constant, then X (e jW ) can be expressed in the form 
X(e jW ) = B(w)ejaw , 
where B(w) is real and a is a real constant. 

708  
Chapter 8 
The Discrete Fourier Transform 
8.62. x[n] and y[n1are two real-valued, positive, finite-length sequences of length 256; i.e., 
x[n] > 0, 
0:::: n 
255, 
y[n] > 0, 
0< n < 255, 
x[n] = y[nj = 0, 
otherwise. 
r[n] denotes the linear convolution of x[n] and y[nj. R(eJW ) denotes the Fourier transform 
of r[n]. Rs[kJ denotes 128 equally spaced samples of R(eJW); i.e., 
Rs[kJ= R(eiW)i 
' 
k =0, 1, ... , 127. 
w=2rrk/128 
Given x[n] and yIn], we want to obtain R,vlk] as efficiently as possible. The only modules 
available are those shown in Figure P8.62.The costs associated with each module are as 
follows: 
Modules I and II are free. 
Module III costs 10 units. 
Module IV costs 50 units. 
Module V costs 100 units. 
Module I  
Module III 
~r;L-~  
[n] 
III 
127 
~ 
~S[n+128r]  
2: sdm]S2[n m]
S2[n] 
1"=-00 
m=O 
(a)  
(c) 
Module IV 
Module II 
[n] 
IV
~~
{s[n]'o:s;n:S;127  
255 
~ 
~=
O,otherwise 
S2[n]  
2: sdm]s2[n m] 
m 
0 
(b)  
(d) 
Module V 
~127
$l~] =2: s[ll]e-j (21Tll28)nk 
n 0 
(e) 
Figure P8.62 
By appropriately connecting one or several of each module, construct a system for 
which the inputs are x[n] and Y[Il] and the output is Rs[k]. The important considerations 
are (a) whether the system works and (b) how efficient it is. The lower the total cost, the 
more efficient the system is. 

709 
Chapter 8 
Problems 
8.63.  yen] is the output of a stable LTI system with system function H(z) = l/(z 
bz-l ), where 
b is a known constant. We would like to recover the input signal x[n] by operating on YIn]. 
The following procedure is proposed for recovering part of x[n] from the data yen]: 
1. Using yIn], O:'S n ::: N -1, ealculate Y [kJ, the N-point DFT of y[n]. 
2. Form 
V[k] = (WN
k 
bW~)Y [k]. 
3. Calculate the inverse DFT of V[k] to obtain v[n]. 
For which values ofthe indexn in the range n = 0, 1, ... , N -1 are we guaranteed that 
x[n] = v[n]? 
8.64.  A modified discrete Fourier transform (MDFT) was proposed (Vernet, 1971) that computes 
samples of the z-transform on the unit circle offset from those computed by the DFT. In 
particular, with X M[k] denoting the MDFT of x(n), 
XM[kj  
X(Z)I 
' 
k = 0, 1, 2, ... , N 
1. 
z=e} [2rrkIN+rrIN J 
Assume that N is even. 
(a)  The N-point MDFT of a sequence x[n] corresponds to the N-point DFTof a sequence 
XM en], which is easily constructed from x[n]. Determine XM en] in terms of x[n]. 
(b)  Ifx[n] is real, not all the points in the DFT are independent, since the DFT is conjugate 
symmetric; i.e., X [k] = X*[«-k»Nl for 0 :'S k :'S N 
1. Similarly, if x[n] is real, not 
all the points in the MDFT are independent. Determine, for x[n] real, the relationship 
between points in X M [k]. 
(c)  (i) Let R[k] 
X M [2k]; that is, R[k] contains the even-numbered points in X M [k]. 
From your answer in part (b), show that X M [k] can be recovered from R[k]. 
(ii)  R[k] can be considered to be the N /2-point MDFT ofan N /2-point sequence r[nJ. 
Determine a simple expression relating r[n] directly to x[n]. 
According to parts (b) and (c), the N-point MDFT of a real sequence x[n] can be com­
puted by forming r[n] from x[nl and then computing the N /2-point MDFT of T[n]. 
The next two parts are directed at showing that the MDFT can be used to implement 
a linear convolution. 
(d)  Consider three sequences Xl [n], x2[n],andx3[n]. alloflength N. Let XIM [k], X2M [k], 
and X3M [k], respectively, denote the MDFTs ofthe three sequences. If 
X3M [k] = XIM [k]X2M [k], 
express X3[n] in terms of xtln] and x2[n]. Your expression must be of the form of a 
single summation over a "combination" ofXl [n] and x2[n] in the same style as (but not 
identical to) a circular convolution. 
(e)  It is convenient to refer to the result in part (d) as a modified circular convolution. Ifthe 
sequences Xl [n] and x2[n] are both zero for n ::: N /2, show that the modified circular 
convolution of Xl en] and x2[n] is identical to the linear convolution of Xl en] and x2[n]. 

710 
Chapter 8 
The Discrete Fourier Transform 
8.65.  In some applications in coding theory, it is necessary to compute a 63-point circular con­
volution of two 63-point sequences xln] and hln]. Suppose that the only computational 
devices available are multipliers, adders, and processors that compute N-point DFfs, with 
N restricted to be a power of 2. 
(a)  It is possible to compute the 63-point circular convolution ofxln] and h[n] using a num­
ber of 64-point DFfs, inverse DFfs, and the overlap-add method. How many DFfs 
are needed? Hint: Consider each of the 63-point sequences as the sum of a 32-point 
sequence and 31-point sequence. 
(b)  Specify an algorithm that computes the 63-point circular convolution of x[n] and h[n] 
using two 128-point DFfs and one 128-point inverse DFf. 
(c)  We could also compute the 63-point circular convolution ofxln] and h[n] by computing 
their linear convolution in the time domain and then aliasing the results. In terms of 
multiplications, which of these three methods is most efficient? Which is least efficient? 
(Assume that one complex mUltiplication requires four real multiplications and that 
xln] and h[n] are real.) 
8.66. We want to filter a very long sequence with an FIR filter whose impulse response is 50 sam­
ples long. We wish to implement this filter with a DFf using the overlap-save technique. 
The procedure is as follows: 
1. The input sections must be overlapped by V samples. 
2. From the output of each section, we must extract M samples such that when these 
samples from each section are abutted, the resulting sequence is the desired filtered 
output. 
Assume that the input segments are 100 samples long and that the size of the DFf is 
128 (= p) points. Assume further that the output sequence from the circular convolution 
is indexed from point 0 to point 127. 
(3)  Determine V. 
(b)  Determine M. 
(c)  Determine the index of the beginning and the end of the M points extracted; i.e., deter­
mine which of the 128 points from the circular convolution are extracted to be abutted 
with the result from the previous section. 
8.67. A problem that often arises in practice is one in which a distorted signal yIn] is the output 
that results when a desired signal x[n] has been filtered by an LTI system. We wish to re­
cover the original signal x[n] by processing yIn]. In theory, xln] can be recovered from yIn] 
by passing yIn] through an inverse filter having a system function equal to the reciprocal of 
the system function of the distorting filter. 
Suppose that the distortion is caused by an FIR filter with impulse response 
hln] 
8[n] 
O.58[n 
nO], 
where no is a positive integer, i.e., the distortion ofx[n] takes the form ofan echoat delayno. 
(3)  Determine the z-transform H (z) and the N -point DFf H[k] of the impulse response 
hln]. Assume that N 
4no. 
(b)  Let Hi(Z) denote the system function of the inverse filter, and let hi In] be the corre­
sponding impulse response. Determine hiln]. Is this an FIR or an IlR filter? What is 
the duration of hi In]? 

-
-
711 
Chapter 8 
Problems 
(c)  Suppose that we use an FIR filter of length N in an attempt to implement the inverse 
filter, and let the N -point OFT of the FIR filter be 
G[k] = 1/H[k], 
k = 0, 1, ... , N -1. 
What is the impulse response g[n] of the FIR filter? 
(d)  It might appear that the FIR filter with OFT G[k] = 1/H[k] implements the inverse 
filter perfectly. After all, one might argue that the FIR distorting filter has an N -point 
OFT H[k] and the FIR filter in cascade has an N-point OFT G[k] = 1/H[k], and since 
G[k]H[k] = 1 for all k, we have implemented an all-pass, nondistorting filter. Briefly 
explain the fallacy in this argument. 
(e)  Perform the convolution of g[n] with h[n], and thus determine how well the FIR filter 
with N-point OFT G[k] = 1/H[k] implements the inverse filter. 
8.68. A sequence x[n] of length N has a discrete Hartley transform (OHT) defined as 
N-l 
X H[k] = L x[n]HN[nk], 
k = 0, 1, ... , N - 1, 
(P8.68-1) 
n=O 
where 
with 
CN[a] = cos(2na/N), 
SN [a] = sin(2na/N). 
Originally proposed by R.Y.L. Hartley in 1942 for the continuous-time case, the Hartley 
transform has properties that make it useful and attractive in the discrete-time case as well 
(Bracewell, 1983, 1984). Specifically, from Eq. (P8.68-1), it is apparent that the OHT of a 
real sequence is also a real sequence. In addition, the OHT has a convolution property, and 
fast algorithms exist for its computation. 
In complete analogy with the OFT, the OHT has an implicit periodicity that must be 
acknowledged in its use. That is, if we consider x[n] to be a finite-length sequence such that 
x[n] = 0 for n < 0 and n > N - 1, then we can form a periodic sequence 
00 
i[n] = L x[n+rN] 
r=-oo 
such that x[n] is simply one period of i[n]. The periodic sequence i[n] can be represented 
by a discrete Hartley series (DHS), which in turn can be interpreted as the OHT by focusing 
attention on only one period of the periodic sequence. 
(a)  The OHS analysis equation is defined by 
N-l 
XH[k] = L i[n]HN[nk]. 
(P8.68-2) 
n=O 
Show that the OHS coefficients form a sequence that is also periodic with period N; i.e., 
XH[k] = XH[k + N] 
for all k. 

712  
Chapter 8 
The Discrete Fourier Transform 
(b) It can also be shown that the sequences HN[nk] are orthogonal; i.e., 
N-I  
{N'
L  
HN[nk]HN[mk] = 
«n»N 
«m»N, 
k=O  
0, 
otherwise. 
Using this property and the DHS analysis formula ofEq. (P8.68-2), show that the DHS 
synthesis formula is 
1 N-I _ 
x[nj = N  L 
XH[k]HN[nkj. 
(P8.68-3) 
k=O 
Note that the DHT is simply one period of the DHS coefficients, and likewise, the DHT 
synthesis (inverse) equation is identical to the DHS synthesis Eq. (P8.68-3), except that we 
simply extract one period of x[n]; i.e., the DHT synthesis expression is 
1 lV-I 
x[n] = N LX H[k]HN[nk], 
n=0,1, ... ,N 
1. 
(P8.68-4) 
k=O 
With 
(P8.68-1) and (P8.68-4) as definitions of the analysis and synthesis relations, 
respectively, for the DHT, we may now proceed to derive the useful properties of this 
representation of a finite-length discrete-time signal. 
(c)  Verify that HN[a] = HN[a + N], and verify the following useful property of HN[a]: 
HN[a + b] = HN[a]CN[b] + HN[-a]SN[b] 
= HN [b]CN [a] + HN[-b]SN[aj. 
(d)  Consider a circularly shifted sequence 
x[n 
no] = x[«n - nO»N], 
n = 0,1, ... , N -1,
xI[n]  
(P8.68-5)
{ 0,  
otherwise. 
In other words, xI[n] is the sequence that is obtained by extracting one period from 
the shifted periodic sequence x[n - nolo Using the identity verified in part (c), show 
that the DHS coefficients for the shifted periodic sequence are 
DHS -
­
x[n 
nO]  +--l> XH[k]CNlnokj + XH[-k]SN[nok]. 
(P8.68-6) 
From this, we conclude that the DHT of the finite-length circularly shifted sequence 
x!«n - nO»N1is 
x[«(n 
nO»)N] 
xH[kjCN[nok] + X H[«-k»N]SN[nokj. 
(P8.68-7) 
(e)  Suppose that x3[n] is the N-point circular convolution of two N -point sequences Xl In] 
and xl[n]; i.e., 
x3[n] 
xtln] ®xz[n] 
N-I 
(P8.68-8)  
L 
xl[m]x2[«n -m»N], 
n 
O,l, ... ,N-1.  
m=O  

713 
i) 
n 
'iN 
» 
:e 
7) 
n] 
Chapter 8 
Problems 
By applying the DHT to both sides of Eq. (PS.6S-8) and using Eq. (PS.6S-7), show that 
XH3[k] 
~XHl[k](X H2[k] + X H2[« -k))N]) 
(PS.68-9)
+5: X Hl[«-k))N J(XH2[k] -
XH2[((-k))N]) 
for k 
0,1, .. . , N 
1. This is the desired convolution property. 
Note that a linear convolution can be computed using the DHT in the same way 
that the DFT can be used to compute a linear convolution. While computing X H3 [k1from 
XHl[kj and XH2[kj requires the same amount of computation as computing X 3lk] from 
Xl [k j and X 2[k], the computation of the DHT requires only half the number of real mul­
tiplications required to compute the DFT. 
(f)  Suppose that we wish to compute the DHT of an N-point sequence x[n] and we have 
available the means to compute the N -point DFT. Describe a technique for obtaining 
X H[k] from X [k] for k = 0,1, ... , N - 1. 
(g)  Suppose that we wish to compute the DFT of an N -point sequence x [n Jand we have 
available the means to compute the N -point DHT. Describe a technique for obtaining 
X[kl from XH[kl for k = 0.1, ... , N-1. 
8.69.  Let x[nl be an N-point sequence such that x[n] 
0 for n < 0 and for n > N 
1. Let x[n] 
be the 2N-point sequence obtained by repeating x[nl; i.e., 
I 
x[n], 
0 :s n :s N 
1,  
x[n] = 
x[n - N], N:s n :s 2N 
1.  
0, 
otherwise. 
Consider the implementation of a discrete-time filter shown in Figure PS.69. This 
system has an impulse response h[n] that is 2N points long; i.e., h[n] = 0 for n < 0 and for 
n > 2N 
1. 
(a)  In Figure PS.69-1, what is X[k], the2N-point DFTofX [n], in terms of X [k], the N-point 
DFTofx[n]? 
(b)  The system shown in Figure PS.69-1 can be implemented using only N -point DFTs as 
indicated in Figure PS.69-2 for appropriate choices for System A and System B. Spec­
ify System A and System B so that Yln] in Figure PS.69-1 and y[n] in Figure PS.69-2 
are equal for 0 :s n :s 2N -1. Note that h[nJ and y[n] in Figure PS.69-2 are 2N-point 
sequences and w[nJ and g[n] are N -point sequences. 
2N-point 
2N-point 
DFf 
IDFf 
.Y[n]
X[k]
x[n1 
2N-point 
DFf 
t 
h[nl  
Figure P8.69-1 

714  
Chapter 8 
The Discrete Fourier Transform 
I 
w[n] 
y[n]
N-point 
N-point 
System B
DFf 
IDFf
X[k]
x[n] 
-
N-point 
D~ 
N-point sequences 
~ 
/
g[n] I  
2N-point sequences 
System A 
lI[n] 
Figure PB.69-2 
8.70.  In this problem, you will examine the use of the DFf to implement the filtering necessary 
for the discrete-time interpolation, or upsampling, of a signaL Assume that the discrete-time 
signal x[nl was obtained by sampling a continuous-time signal xc(t) with a sampling period 
T. Moreover, the continuous-time signal is appropriately bandlimited; Le., XcU Q) 
0 for 
IQI 2': 2rr/T. For this problem, we will assume that x[n] has length N; i.e., x[n] = 0 for 
n < 0 or n > N 
1, where N is even. It is not strictly possible to have a signal that is both 
perfectly bandlimited and of finite duration, but this is often assumed in practical systems 
processing finite-length signals which have very little energy outside the band IQI ~ 2rr/T. 
We wish to implement a 1:4 interpolation, i.e., increase the sampling rate by a factor 
of 4. As seen in Figure 4.23, we can perform this sampling rate conversion using a sampling 
rate expander followed by an appropriate lowpass filter. In this chapter, we have seen that 
the lowpass filter could be implemented using the DFf if the filter is an FIR impulse re­
sponse. For this problem, assume that this filter has an impulse response h[n] that is N + 1 
points long. Figure PS.70-1 depicts such a system, where H[k] is the 4N-point DFf of the 
impulse response ofthe lowpass filter. Note that both u[n] and y[n] are 4N-point sequences. 
f4 
4N-point 
DFf 
H[k] 
4N-point 
IDFf
x[n] 
u[n] 
V[k] 
Y[k] 
y[n] 
Figure P8.70-1 
(a)  Specify the DFT H[k] such that the system in Figure PS.70-1 implements the desired 
upsampling system. Think carefully about the phase of the values of H[k]. 
(b)  It is also possible to upsample x[n] using the system in Figure PS.70-2. Specify System 
A in the middle box so that the 4N-point signal Y2[n] in this figure is the same as y[n] 
in Figure P8.70-2. Note that System A may consist of more than one operation. 

Chapter 8 
Problems  
715 
(c)  Is there a reason that the implementation in Figure P8.70-2 might be preferable to 
Figure P8. 70-1? 
x[n] 
N-point 
DFT 
System 
A 
4N-point 
IDFr
X[k] 
Y2[k] 
Y2[n] 
Figura pa.70-2 
8.71. Derive 
(8.153) using Eqs. (8.164) and (8.165). 
8.72. Consider the following procedure 
(8) Form the sequence v[n] 
x2[2n] where x2[n] is given by Eq. (8.166). This yields 
v[n] 
x [2n] 
n = 0, 1, ... , N /2 -1 
v[N 
1 - n] = x[2n + 1], 
n = 0, 1, ... , N /2 
1. 
(b) Compute V[k], the N -point DFf of vLn]. 
Demonstrate that the following is true: 
x c2 [k] 
2R.e{e-j2ll'k/(4N)V[k]}, 
k = 0, 1, ... , N - 1, 
;=; 
[1l'k(4n + 1) ] 
= 2 L 
v[n]cos 
2N 
. 
k 
0,1, ... , N 
1, 
n=O 
k = 0, 1, ... , N 
1. 
Note that this algorithm uses N-point rather than 2N-point DFfs as required in 
Eq. (8.167). In addition, since v[n] is a real sequence, we can exploit even and odd
i!. 
symmetries to do the computation of V[k] in one N /4-point complex Df~r: 
8.73. Derive Eq. (8.156) using Eqs. (8.174) and (8.157). 
8.74. (a) Use Parse val's theorem for the DFT to derive a relationship between L IX cl[k]12 
k 
and L Ix[n]1 2. 
n 
(b) Use Parseval's theorem for the DFr to derive a relationship between L IX c2[k]12 
k 
and L Ix[n]12 . 
n 

9 
Computation 
ef the Discrete 
Fourier Transform 
9.0 INTRODUCTION 
The discrete Fourier transform (DFT) plays an important role in the analysis, design, 
and implementation of discrete-time signal-processing algorithms and systems because 
the basic properties of the discrete-time Fourier transform and discrete Fourier trans­
form, discussed in Chapters 2 and 8, respectively, make it particularly convenient to 
analyze and design systems in the Fourier domain. It is equally important that efficient 
algorithms exist for explicitly computing the DFT. As a result, the DFT is an important 
component in many practical applications of discrete-time systems. 
In this chapter, we discuss several methods for computing values of the DFT. 
The major focus of the chapter is a particularly efficient class of algorithms for the 
digital computation of the N-point DFT. Collectively, these efficient algorithms, which 
are discussed in Sections 9.2, 9.3, and 9.5, are called FFT algorithms. To achieve the 
highest efficiency, the FFT algorithms must compute all N values of the DFT. When 
we require values of the DFT at only a few frequencies in the range 0 :s w < 2n, 
other algorithms may be more efficient and flexible, even though they are less efficient 
than the FFT algorithms for computation of all the values of the DFT. Examples of 
such algorithms are the Goertzel algorithm, discussed in Section 9.1.2, and the chirp 
transform algorithm, discussed in Section 9.6.2. 
There are many ways to measure the complexity and efficiency of an implementa­
tion or algorithm, and a final assessment depends on both the available implementation 
technology and the intended application. We will use the number of arithmetic mul­
tiplications and additions as a measure of computational complexity. This measure is 
simple to apply, and the number of multiplications and additions is directly related to 
716 

717 
Section 9.0 
Introduction 
the computational speed when algorithms are implemented on general-purpose digi­
tal computers or special-purpose processors. However, other measures are sometimes 
more appropriate. For example. in custom VLSI implementations, the area of the chip 
and power requirements are important considerations and may not be directly related 
to the number of arithmetic operations. 
In terms of multiplications and additions. the class of FFf algorithms can be or­
ders of magnitude more efficient than competing algorithms. The efficiency of FFf 
algorithms is so high, in fact, that in many cases the most efficient procedure for imple­
menting a convolution is to compute the transform of the sequences to be convolved, 
multiply their transforms, and then compute the inverse transform of the product of 
transforms. The details of this technique were discussed in Section 8.7. In seeming con­
tradiction to this, there is a set of algorithms (mentioned briefly in Section 9.6) for 
evaluation of the DFT (or a more general set of samples of the Fourier transform) 
that derive their efficiency from a reformulation of the Fourier transform in terms of a 
convolution and thereby implement the Fourier transform computation by using effi­
cient procedures for evaluating the associated convolution. This suggests the possibility 
of implementing a convolution by multiplication of DFfs, where the DFfs have been 
implemented by first expressing them as convolutions and then taking advantage of 
efficient procedures for implementing the associated convolutions. While this seems on 
the surface to be a basic contradiction, we will see in Section 9.6 that in certain cases it 
is an entirely reasonable approach. 
In the sections that follow, we consider a number of algorithms for computing the 
discrete Fourier transform. We begin in Section 9.1 with discussions of direct computa­
tion methods, i.e., methods based on direct use of the defining relation for the DFT as a 
computational formula. We include in this discussion the Goertzel algorithm (Goertzel, 
1958), which requires computation proportional to N 2, but with a smaller constant of 
proportionality than that of the direct evaluation of the defining formula. One of the 
principal advantages of the direct evaluation method or the Goertzel algorithm is that 
they are not restricted to computation of the DFf, but can be used to compute any 
desired set of samples of the DTFf of a finite-length sequence. 
In Sections 9.2 and 9.3 we present a detailed discussion of FFf algorithms for 
which computation is proportional to N log2 N. This class of algorithms is considerably 
more efficient in terms of arithmetic operations than the Goertzel algorithm, but is 
specifically oriented toward computation ofall the values of the DFf. We do not attempt 
to be exhaustive in our coverage of that class of algorithms, but we illustrate the general 
principles common to all algorithms of this type by considering in detail only a few of 
the more commonly used schemes. 
In Section 9.4, we consider some of the practical issues that arise in implementing 
the power-of-two-Iength FFT algorithms discussed in Sections 9.2 and 9.3. Section 9.5 
provides a brief overview of algorithms for N a composite number including a brief 
reference to FFT algorithms that are optimized for a particular computer architecture. 
In Section 9.6, we discuss algorithms that rely on formulating the computation of the 
DFTin terms ofa convoLution. In Section 9.7, we consider effects of arithmetic round-off 
in FFf aLgorithms. 

718 
Chapter 9 
Computation of the Discrete Fourier Transform 
9.1 DIRECT COMPUTATION OF THE DISCRETE FOURIER TRANSFORM 
As defined in Chapter 8, the DFT of a finite-length sequence of length N is 
N-l 
X[k] = Lx[n]W~n, 
k = 0, 1, ... , N - 1, 
(9.1) 
n=O 
where WN = e- j (211'/N). The inverse discrete Fourier transform is given by 
1 N-l 
kn
x[n] = N L X[k]WN 
, 
n 
0, 1, , , , , N 
1. 
(9.2) 
k=O 
In Eqs. (9.1) and (9.2), both x[n] and X[k] may be complex,1 Since the expressions on 
the right-hand sides of those equations differ only in the sign of the exponent of WNand 
in the scale factor liN, a discussion of computational procedures for Eq, (9.1) applies 
with straightforward modifications to Eq, (9.2), (See Problem 9,1.) 
Most approaches to improving the efficiency of computation of the DFT exploit 
the symmetry and periodicity properties of w~n; specifically, 
= WN
kn
W~(N-n) 
(W~n)* (complex conjugate symmetry) 
(9.3a) 
kn _ Wk(n+N) _ W(k+N)n
W N -
N 
-
N 
(periodicity in nand k), 
(9.3b) 
(Since W~n 
cos(2rrknjN) - j sin(2rrknjN) these properties are a direct consequence 
of the symmetry and periodicity of the underlying sine and cosine functions,) Because 
the complex numbers w~n have the role of coefficients in Eqs. (9,1) and (9,2), the re­
dundancy implied by these conditions can be used to advantage in reducing the amount 
of computation required for their evaluation, 
9.1.1 Direct Evaluation of the Definition of the DFT 
To create a frame of reference, consider first the direct evaluation of the defining DFT 
expression in Eq, (9.1), Since x[n] may be complex, N complex multiplications and 
(N - 1) complex additions are required to compute each value of the DFT if we use 
Eq. (9,1) directly as a formula for computation. To compute all N values therefore re­
quires a total of N 2 complex multiplications and N (N -1) complex additions. Expressing 
1In discussing algorithms for computing the DFf of a finite-length sequence x[nJ, it is worthwhile to 
recall from Chapter 8 that the DFf values defined by Eq. (9.1) can be thought of either as samples of the 
DTFf X(e jW ) at frequencies Wk 
2:n:k/ N or as coefficients in the discrete-time Fourier series for the periodic 
sequence 
00 
.tIn] 
L x[n +rNJ. 
T=-OO 
It will be helpful to keep both interpretations in mind and to be able to switch focus from one to the other as 
is convenient. 

719 
Section 9.1 
Direct Computation of the Discrete Fourier Transform 
Eq. (9.1) in terms of operations on real numbers, we obtain 
N-l 
X[k] = L [CRe(x[n]}Re{wtn} 
Im{x[n]}Im(wtn}) 
n=O 
+ j(Re{x[n]}Im{Wtn} + Im{x[n]}Re{wtnn] • 
(9.4) 
k=0,1, ... ,N-1, 
which shows that each complex multiplication x[n] . wtn requires four real multipli­
cations and two real additions, and each complex addition requires two real additions. 
Therefore, for each value of k, the direct computation of X[k] requires 4N real multi­
plications and (4N - 2) real additions.2 Since X[k] must be computed for N different 
values of k, the direct computation of the discrete Fourier transform of a sequence 
x[n] requires 4N2 real multiplications and N(4N - 2) real additions. Besides the mul­
tiplications and additions called for by Eq. (9.4), the digital computation of the DFf 
on a general-purpose digital computer or with special-purpose hardware also requires 
provision for storing and accessing the N complex input sequence values x[n] and val­
ues of the complex coefficients wtn. Since the amount of computation, and thus the 
computation time, is approximately proportional to N 2 , it is evident that the number 
of arithmetic operations required to compute the DFf by the direct method becomes 
very large for large values of N. For this reason, we are interested in computational 
procedures that reduce the number of multiplications and additions. 
As an illustration of how the properties of wtn can be exploited, using the sym­
metry property in Eq. (9.3a), we can group terms in the summation in Eq. (9.4) for n 
and (N 
n). For example, the grouping 
Re{x[n]}Re(Wtn} + Re{x[N 
n]}Re{W~(N-n)} 
(Re{x[nD + Re{x[N - n]))Re{Wtn} 
eliminates one real multiplication, as does the grouping 
-Im{x[n]}Im{Wtn} - Im{x[N 
n]}Im{W~(N-nJ} 
= -(Im{x[n]} 
Im{x[N - n]})Im{Wtn}. 
Similar groupings can be used for the other terms in Eq. (9.4). In this way, the number 
of multiplications can be reduced by approximately a factor of 2. We can also take 
advantage ofthe fact that for certain values of the product kn, the implicit sine and cosine 
functions take on the value 1 or 0, thereby eliminating the need for multiplications. 
However, reductions of this type still leave us with an amount of computation that is 
proportional to N 2• Fortunately, the second property [Eq. (9.3b)], the periodicity of the 
complex sequence wtn , can be exploited with recursion to achieve significantly greater 
reductions of the computation. 
9.1.2 The Goertzel Algorithm 
The Goertzel algorithm (Goertze1, 1958) is an example of how the periodicity of the 
sequence wtn can be used to reduce computation. To derive the algorithm, we begin 
2Throughout the discussion, the formula for the number of computations may be only approximate. 
Multiplication by W~, for example, does not require a multiplication. ~evertheless, when N is large, the 
estimate of computational complexity obtained by including such multiplications is sufficiently accurate to 
permit comparisons between different classes of algorithms. 

720 
Chapter 9 
Computation of the Discrete Fourier Transform 
by noting that 
WN
kN = eJ(21i/N)Nk = eJ21ik = 1, 
(9.5) 
since k is an integer. This is a result of the periodicity with period N of WN
kn in either n 
or k. Because of Eq. (9.5), we may multiply the right side of Eq. (9.1) by WN
kN without 
affecting the equation. Thus, 
N-l 
N-l 
X[k] = W,V-kN L x[r]W,tr = L x[r]WNk(N-r). 
(9.6) 
r=O 
r=O 
To suggest the final result, we define the sequence 
ydn] = L
00 
x[r]WNk(n-r)u[n 
r]. 
(9.7) 
r=-oo 
From Eqs. (9.6) and (9.7) and the factthatx[n] = 0 for n <: 0 and n ~ N, it follows that 
X[k] 
ydn] 
(9.8) 
Equation (9.7) can be interpreted as a discrete convolution of the finite-duration se­
quence x[n], 0 :s n :s N 
1, with the sequence WNknu[n]. Consequently, ydn] can be 
viewed as the response of a system with impulse response WNknu[n] to a finite-length 
input x[n]. In particular, X[k] is the value of the output when n = N. 
The signal flow graph of a system with impulse response WNknu[n] is shown in 
Figure 9.1, which represents the difference equation 
ydn] = WN
kydn 
1] + x[n], 
(9.9) 
where initial rest conditions are assumed. Since the general input x [n] and the coefficient 
W,V-k are both complex, the computation of each new value of Yk[n] using the system 
of Figure 9.1 requires 4 real multiplications and 4 real additions. All the intervening 
values ydl], Yk[2], .... Yk[N 
1] must be computed in order to compute ydN] 
X[k], 
so the use of the system in Figure 9.1 as a computational algorithm requires 4N real 
multiplications and 4N real additions to compute X[k] for a particular value of k. Thus, 
this procedure is slightly less efficient than the direct method. However, it avoids the 
computation or storage of the coefficients w~n, since these quantities are implicitly 
computed by the recursion implied by Figure 9.1. 
It is possible to retain this simplification while reducing the number of multiplica­
tions by a factor of 2. To see how this may be done, note that the system function of the 
system of Figure 9.1 is 
1 
Hk(Z) = ---;;;--kC1 
(9.10)
1- YfN 
Figure 9.1 
Flow graph of 1St -order 
complex recursive computation of X[k]. 

721 
Section 9.1 
Direct Computation of the Discrete Fourier Transform 
Multiplying both the numerator and the denominatorof Hk(Z) by the factor (1- W~z-l), 
we obtain 
(9.11 ) 
1-2cos(21Tk/N)C1 +cz. 
The signal flow graph ofFigure 9.2 corresponds to the direct form II implementation of 
the system function of Eq. (9.11) for which the difference equation for the poles is 
vk[n] = 2cos(21Tk/N)vk[n -1] - vk[n - 2] + x[n]. 
(9.12a) 
After N iterations ofEq. (9.12a) starting with initial rest conditions Wk[ -2] 
Wk[-1] 
0, the desired DFf value can be obtained by implementing the zero as in 
X[k] = Yk[n]L=N = vk[N] - Wtvk[N 
1]. 
(9.12b) 
If the input is complex, only two real multiplications per sample are required to 
implement the poles of this system, since the coefficients are real and the factor -1 need 
not be counted as a multiplication. As in the case of the 1st-order system, for a complex 
input, four real additions per sample are required to implement the poles (if the input 
is complex). Since we only need to bring the system to a state from which ydN] can be 
computed, the complex multiplication by - W~ required to implement the zero of the 
system function need not be performed at every iteration of the difference equation, 
but only after the Nth iteration. Thus, the total computation is 2N real multiplications 
and 4N real additions for the poles,3 plus 4 real multiplications and 4 real additions for 
the zero. The total computation is therefore 2(N + 2) real multiplications and 4(N + 1) 
real additions, about half the number of real multiplications required with the direct 
method. In this more efficient scheme, we still have the advantage that COS(21T k/ N) 
and wt are the only coefficients that must be computed and stored. The coefficients 
Wtll are again computed implicitly in the iteration of the recursion formula implied by 
Figure 9.2. 
As an additional advantage of the use of this network, let us consider the compu­
tation of the DFT of x[n] at the two symmetric frequencies 21Tk/N and 21T(N 
k)/N, 
x[nJ 
2COse~k) 
Z-l 
Z-l 
-1 
-w~ 
Yk[n] 
Figure 9.2 
Flow graph of 2nd-order 
recursive computation of X[k1 
(Goertzel algorithm). 
3This assumes that X[II] is complex. If X[II] is real, the operation count is N real multiplications and 
2N real additions for implementing the poles. 

722 
Chapter 9 
Computation of the Discrete Fourier Transform 
that is, the computation of X[k] and X[N 
k]. It is straightforward to verify that the 
network in the form of Figure 9.2 required to compute X[N 
k] has exactly the same 
poles as that in Figure 9.2, but the coefficient for the zero is the complex conjugate of 
that in Figure 9.2. (See Problem 9.21.) Since the zero is implemented only on the final 
iteration, the 2N multiplications and 4N additions required for the poles can be used 
for the computation of two DFT values. Thus, for the computation of all N values of the 
discrete Fourier transform using the Goertzel algorithm, the number of real multiplica­
tions required is approximately N 2 and the number of real additions is approximately 
2N2• While this is more efficient than the direct computation of the discrete Fourier 
transform, the amount of computation is still proportional to N 2• 
In either the direct method or the Goertzel algorithm we do not need to evaluate 
X[k] at all N values of k. Indeed, we can evaluate X[k] for any M values of k, with 
each DFT value being computed by a recursive system of the form of Figure 9.2 with 
appropriate coefficients. In this case, the total computation is proportional to N M. The 
Goertzel method and the direct method are attractive when M is small; however, as 
indicated previously, algorithms are available for which the computation is proportional 
to N log2 N when N is a power of 2. Therefore, when M is less than log2 N, either the 
Goertzel algorithm or direct evaluation of the DFT may in fact be the most efficient 
method, but when all N values of X[k] are required, the decimation-in-time algorithms, 
to be considered next, are roughly (N/ log2 N) times more efficient than either the direct 
method or the Goertzel algorithm. 
As we have derived it, the Goertzel algorithm computes the DFT value X[k], 
which is identical to the DTFI X(e F») evaluated at frequency w = 2rrk/ N. With only a 
minor modification of the above derivation, we can show that X (ei(») can be evaluated 
at any frequency Wa by iterating the difference equation 
va[n] 
2cos(wo)vdn - 1] -
va[n 
2] + x[n], 
(9.13a) 
N times with the desired value of the DTFT obtained by 
X(ei f1)a) = e-j{l)aN (vdN] - e-j (1)a va [N 
1]). 
(9.13b) 
Note that in the case Wa = 2rrk/N Eqs. (9.13a) and (9.13b) reduce to Eqs. (9.12a) 
and (9.12b). Because Eq. (9.13b) must only be computed once, it is only slightly less 
efficient to compute the value of X (e j {») at an arbitrarily chosen frequency than at a 
DFI frequency. 
Still another advantage of the Goertzel algorithm in some real-time applications 
is that the computation can begin as soon as the first input sample is available. The 
computation then involves iterating the difference equation Eq. (9.12a) or Eq. (9.13a) 
as each new input sample becomes available. After N iterations, the desired value of 
X(ej 
can be computed with either Eq. (9.l2b) or Eq. (9.13b) as is appropriate. 
f1) 
9.1.3 Exploiting both Symmetry and Periodicity 
Computational algorithms that exploit both the symmetry and the periodicity of the 
sequence wtn were known long before the era of high-speed digital computation. At 
that time, any scheme that reduced manual computation by even a factor of 2 was 
welcomed. Heideman, Johnson and Burrus (1984) have traced the origins of the basic 
principles of the FFT back to Gauss, as early as 1805. Runge (1905) and later Danielson 

723 
Section 9.2 
Decimation-in-Time FFT Algorithms 
and Lanczos (1942) described algorithms for which computation was roughly propor­
tional to N log2 N rather than N2• However, the distinction was not of great importance 
for the small values of N that were feasible for hand computation. 11le possibility of 
greatly reduced computation was generally overlooked until about 1965, when Cooley 
and Tukey (1965) published an algorithm for the computation of the discrete Fourier 
transform that is applicable when N is a composite number, i.e., the product of two 
or more integers. The publication of their paper touched off a flurry of activity in the 
application of the discrete Fourier transform to signal processing and resulted in the 
discovery of a number of highly efficient computational algorithms. Collectively, the 
entire set of such algorithms has come to be known as the fast Fourier transform, or the 
FFT.4 
In contrast to the direct methods discussed above, FFT algorithms are based on 
the fundamental principle of decomposing the computation of the discrete Fourier 
transform of a sequence of length N into smaller-length discrete Fourier transforms 
that are combined to form the N -point transform. These smaller-length transforms may 
be evaluated by direct methods, or they may be further decomposed into even smaller 
transforms. The manner in which this principle is implemented leads to a variety of 
different algorithms, all with comparable improvements in computational speed. In this 
chapter, we are concerned with two basic classes of FFT algorithms. The first class, called 
decimation in time, derives its name from the fact that in the process of arranging the 
computation into smaller transformations, the sequence x[n] (generally thought of as 
a time sequence) is decomposed into successively smaller subsequences. In the second 
general class of algorithms, the sequence of discrete Fourier transform coefficients X[k] 
is decomposed into smaller subsequences-hence its name, decimation in frequency. 
We discuss decimation-in-time algorithms in Section 9.2. Decimation-in-frequency 
algorithms are discussed in Section 9.3. This is an arbitrary ordering. The two sections 
are essentially independent and can therefore be read in either order. 
9.2 DECIMAnON-IN-TIME FFT ALGORITHMS 
Dramatic efficiency in computing the DFT results from decomposing the computation 
into successively smaller DFT computations while exploiting both the symmetry and 
the periodicity of the complex exponential W~n = e-i (2Jr/N)kn. Algorithms in which 
the decomposition is based on decomposing the sequence x [n] into successively smaller 
subsequences are called decimation-in-time algorithms. 
The principle of decimation-in-time is conveniently illustrated by considering the 
special case of N an integer power of 2, i.e., N = 2v. Since N is divisible by two, 
we can consider computing X[k] by separating x[n] into two (N/2)-point5 sequences 
consisting of the even-numbered points g[n] = x[2n] and the odd-numbered points 
h[n] = x[2n + 1]. Figure 9.3 shows this decomposition and also the (somewhat obvious, 
but crucial) fact that the original sequence can be recovered simply by re-interleaving 
the two sequences. 
4See Cooley, Lewis and Welch (1967) and Heideman, Johnson and Burrus (1984) for historical sum­
maries of algorithmic developments related to the FFr. 
5 When discussing FFr algorithms, it is common to use the words sample and point interchangeably to 
mean sequence value, i.e., a single number. Also, we refer to a sequence of length N as an N -point sequence, 
and the DFr of a sequence of length N will be called an N-point DFr. 

724 
Chapter 9 
Computation of the Discrete Fourier Transform 
gIn] = x[2n] 
x[n] 
h[n] =x[2n+ 1] 
Figure 9.3 Illustration of the basic 
'--_--'I 0:5 n :5 NI2 
1 
principle of decimation-in-time. 
To understand the significance ofFigure 9.3 as an organizing principle for comput­
ing the Off, it is helpful to consider the frequency-domain equivalents of the operations 
depicted in the block diagram. First, note that the time-domain operation labeled "Left 
Shift 1" corresponds in the frequency domain to multiplying X (eiw) by eiw . As discussed 
in Section 4.6.1, corresponding to the compression of the time sequences by 2, the OTFfs 
G(eiw) and H(eiw ) (and therefore G[k] and H[k]) are obtained by frequency-domain 
aliasing that occurs after expanding the frequency scale by the substitution w -+ wl2 in 
X(e jW) and ejw X(eiw ). That is, the OTFfs of the compressed sequences g[n] 
x[2n] 
and h[n] = x[2n + 1J are respectively 
G(eiw) 
~ (X (eJw/2) + X(eJ(w-21r)/2») 
(9.14a) 
H(eJW) = ~ (X (ejw/2)ejw/2 + X(eJ(w-21r)/2)eJ(w-2rr)/2) . 
(9.14b) 
The sequence-expansion-by-2 shown in the right half of the block diagram in Figure 9.3 
results in the frequency-compressed OTFfs Ge(eJW) = G(eJ2w) and He(e jW ) = H(ej2w), 
which, according to Figure 9.3, combine to form X(e jW) through 
X (eJW) 
Ge(eJW) + e-Jw HeCeJW ) 
=G(eJ2w ) + e-jw H (eJ2w). 
(9.15) 
Substituting Eqs. (9.14a) and (9.14b) into Eq. (9.15) will verify that the OTFf X(elw ) of 
the N-point sequence x[nJ can be represented as inEq. (9.15) in terms of the OTFfs of 
the N 12-point sequences g[nJ = x[2n] and h[nl = x[2n + 1]. Therefore, the Off X[k] 
can likewise be represented in terms ofthe OFfs of g[n] and h[n]. 
Specifically, X[k] corresponds to evaluating X (ejW ) at frequencies Wk = 2rrkiN 
with k 
0, 1, .... N 
1. Therefore, using Eq. (9.15) we obtain 
X[k] 
X (ej2rrk/N) = G(ej (2rrk/N)2) + e-j2rrk/ N H (e(j2rrk/ N)2). 
(9.16) 

725 
Section 9.2 
Decimation-in-Time FFT Algorithms 
From the definition of g[n] and G(ej (1)), it follows that 
NjZ-l 
G(ej(ZrrkjN)Z) = L x[2n]e- j (Zrrk/N)Zn 
11=0 
N/Z-l 
= L x[2n]e- j (Zrrkj(NjZ)n 
n=O 
NjZ-l 
=  L x[2n]wtiz, 
(9.17a) 
11=0 
and by a similar manipulation, it can be shown that 
N/Z-l 
H(ej(ZrrkjN)Z) 
L x[2n + 1]wtiz' 
(9.17b) 
n=O 
Thus, from Eqs. (9.17a) and (9.17b) and Eq. (9.16), it follows that 
Nj2-1  
Nj2-1 
X[k]  L x[2n]wtiz + wt L x[2n + 1]wtiz 
k = 0, 1, ... ,N 
1, 
(9.18) 
where the N -point DFT X[k] is by definition 
N-l 
X[k] =  L x[n]W~k, 
k 
0, 1, ... , N - 1. 
(9.19) 
11=0 
Likewise, by definition, the (N/2)-point DFTs of g[n] and h[n] are 
Nj2-1 
G[k] =  L x[2n]W~~z' 
k 
0,1, ... ,N/2 
1 
(9.20a) 
n=O 
N/2-1 
H[k] =  L x[2n + l]W~~2' 
k = 0, 1, ... , N /2 - 1. 
(9020b) 
11=0 
Equation (9.18) shows that the N-point DFT X[k] can be computed by evaluating the 
(N/2)-pointDFTsG[k] andH[k] overk = 0, 1, ... , N-linsteadofk = 0, 1, ... , N /2-1 
as we normally do for (N/2)-point DFTs. This is easily achieved even when G[k] and 
H[k] are computed only for k 
0,1, ... , N /2 -1, because the (N/2)-point transforms 

726 
Chapter 9 
Computation of the Discrete Fourier Transform 
x~ 
x~ 
x~ 
x~ 
xoo 
x~ 
x~ 
xm 
~---­
G[O] 
;p X [0] 
~ 
wZ 
N 
.
:2 -poml 
DFf 
N 
.
:2 
poml 
DFf 
~------~~~--------~
1l[3] 
wZ
• '0 x[7] 
Figure 9.4 Flow graph of the 
decimation-in-time decomposition of 
an N-point DFT computation into two 
(N/2)-point DFT computations (N = 8). 
are implicitly periodic with period N/2. With this observation, Eq. (9.18) can be rewrit­
ten as 
X[k] = G[((k»NjZJ + WtH[((k»N/2] 
k = 0, 1, ... , N - 1. 
(9.21) 
The notation ((k))Nj2 conveniently makes it explicit that even though G[k] and H[k] 
are only computed for k 
0,1, ... , N /2 - 1, they are extended periodically (with no 
additional computation) by interpreting k modulo N /2. 
After the two DFTs are computed, they are combined according to Eq. (9.21) 
to yield the N-point DFT X[k]. Figure 9.4 depicts this computation for N 
8. In this 
figure, we have used the signal flow graph conventions that were introduced in Chapter 6 
for representing difference equations. That is, branches entering a node are summed to 
produce the node variable. When no coefficient is indicated, the branch transmittance 
is assumed to be unity. For other branches, the transmittance of a branch is an integer 
power of the complex number WN. 
In Figure 9.4, two 4-point DFTs are computed, with G[k] designating the 4-point 
DFT of the even-numbered points and H[k] designating the 4-point DFT of the odd­
numbered points. According to Eq. (9.21), X[O] is obtained by multiplying H[O] by w2 
and adding the product to G[O]. The DFT value X[l] is obtained by multiplying H[l] by 
W~ and adding that result to G[l]. Equation (9.21) states that, because of the implicit 
periodicity of G[k] and H[k], to compute X[4], we should mUltiply H[((4))41 by W~ 
and add the result to G[((4»4]. Thus, X[4] is obtained by multiplying H[O] by W~ and 
adding the result to G[O]. As shown in Figure 9.4, the values X[5], X[6], and X[7] are 
obtained similarly. 
With the computation restructured according to Eq. (9.21), we can compare the 
number of multiplications and additions required with those required for a direct com­
putation of the DFT. Previously we saw that, for direct computation without exploiting 

727 
Section 9.2 
Decimation-in-Time FFT Algorithms 
symmetry, N 2 complex multiplications and additions were required.6 By comparison, 
Eq. (9.21) requires the computation of two (N/2)-point DFTs, which in turn requires 
2(N/2)2 complex multiplications and approximately 2(N/2)2 complex additions if we 
do the (N/2)-point DFTs by the direct method. Then the two (N12)-point DFTs must 
be combined, requiring N complex multiplications, corresponding to mUltiplying the 
second sum by Wk, and N complex additions, corresponding to adding the product 
obtained to the first sum. Consequently, the computation of Eq. (9.21) for all values 
of k requires at most N + 2(N12)2 or N + N2/2 complex multiplications and complex 
additions. It is easy to verify that for N > 2, the total N + N 2/2 will be less than N2. 
Equation (9.21) corresponds to breaking the original N-point computation into 
two (N12)-point DFT computations. If N /2 is even, as it is when N is equal to a power 
of 2, then we can consider computing each of the (N12)-point DFTs in Eq. (9.21) by 
breaking each of the sums in that equation into two (N/4)-point DFTs, which would 
then be combined to yield the (N12)-point DFTs. Thus, G[k] in Eq. (9.21) can be 
represented as 
(N/2)-1 
(N/4)-1 
(N /4)-1 
G[k] 
I: g[r]W~k/2 = I: g[1£]W~£A + I: g[2€ 
+ I]W~ft)k, 
(9.22) 
or 
(N/4)-1  
(N/4)-1 
G[k] =  I: g[2€]Wff;4 
+ W,t/2 I: g[1£ + I]Wffw 
(9.23) 
£=0 
[=0 
Similarly, H[k] can be represented as 
(N/4)-1 
(N /4)-1 
Hlk] = I: h[2€]w~k/4 + W~!2 I: h[1£ + I]W~~4' 
(9.24) 
Conseq uentiy, the (N12)-point D FT G[k] can be obtained by combining the (N14)-point 
DFTs of the sequences g[2l] and g[2t + 1]. Similarly, the (N/2)-point DFT H[k] can 
be obtained by combining the (N14)-point DFTs of the sequences h[2€] 
and h[U + lJ. 
Thus, if the 4-point DFTs in Figure 9.4 are computed according to Eqs. (9.23) and 
(9.24), then that computation would be carried out as indicated in Figure 9.5. Inserting 
the computation of Figure 9.5 into the flow graph of Figure 9.4, we obtain the complete 
flow graph of Figure 9.6, where we have expressed the coefficients in terms of powers 
of W N rather than powers of W N /2, using the fact that W N /2 = W~. 
For the 8-point DFT that we have been using as an illustration, the computation 
has been reduced to a computation of 2-point DFTs. For example, the 2-point DFT of 
the sequence consisting of x[O] and x[4] is depicted in Figure 9.7. With the computation 
of Figure 9.7 inserted in the flow graph of Figure 9.6, we obtain the complete flow graph 
for computation of the 8-point DFT, as shown in Figure 9.9. 
6For simplicity. we assume that N is large, so that (N - 1) can be approximated accurately by N. 

128 
Chapter 9 
Computation of the Oiscrete Fourier Transform 
x[O] ­
'!!. _ point
4  
DFT 
x[4] 
x [2] 
'N 
Figure 9.5 
Flow graph of the
point
4 
decimation-in-time decomposition of an 
x[6]-
-! 
DL7r 
!---+--if 
• 3'0 G[3] 
(N/2)-point OFT computation into two 
wN!2 
(N/4)-point OFT computations (N = 8). 
x[O] 
-
'!!. 
-
, 
-
'N 
"4 
-
I 
­
_ point
4  
DFT 
x[4] 
x [2] 
,!!. - point
4  
DFT 
x [6] 
x[l]­
point 
DFT
x[5] 
x[3] ­
, !!. - point
4 
.~
x[71~ DFT 1----0'" 
• 'b X[7]
W~ 
W,~ 
Figure 9.6 
Result of substituting the structure of Figure 9.5 into Figure 9.4. 
x [0] 0:: 
;;t:> 
W~=l 
.. >0
x[4] O":..----W--= 'W"{jI2=_l 
Figure 9.7 
Flow graph of a 
2 
2-point OFT. 
For the more general case, but with N still a power of 2, we would proceed by 
decomposing the (N14)-point transforms in Eqs. (9.23) and (9.24) into (N18)-point 
transforms and continue until we were left with only 2-point transforms. This requires 
j) = log2 N stages of computation. Previously, we found that in the original decomposi­
tion of an N -point transform into two (N12)-point transforms, the number of complex 
multiplications and additions required was N + 2(N/2)2. When the (N12)-point trans­
forms are decomposed into (N/4)-point transforms, the factor of (N12)2 is replaced by 

729 
Section 9.2 
Decimation-in-Time FFT Algorithms 
roth
(rn 
1)st 
stage 
stage 
w(n NI2) 
Figure 9.8 
Flow graph of basic 
N 
butterfly computation in Figure 9.9. 
x [0] O'::--------,,>O::--I------;l.::r--;.------oX[Ol 
x [4] <Y""------4..-;----""<C<:-l-~_E_----;f.:r-~--__f_--pX[11 
x [3] =-........----:::;¢'---~:____--o....,...:.::}___r_-----'<:__-tJ 
x[7] <Y""-----c..-;----""<a<-------c~:5------~X[7] 
W,~ 
W,~ 
Figure 9.9 
Flow graph of complete decimation-tn-time decomposition of an 
8-point OFT computation. 
N /2 + 2(N/4)2, so the overall computation then requires N + N + 4(N/4)2 complex 
multiplications and additions. If N = 2v , this can be done at most v 
log2 N times, 
so that after carrying out this decomposition as many times as possible, the number of 
complex multiplications and additions is equal to Nv = N 10g2 N. 
The flow graph ofFigure 9.9 displays the operations explicitly. By counting branches 
with transmittances of the form WN' we note that each stage has N complex multiplica­
tions and N complex additions. Since there are log2 N stages, we have a total of N log2 N 
complex mUltiplications and additions. This can be a substantial computational saving. 
210 
= 220
For example, if N = 
1024, then N 2 
= 1,048,576, and N log2 N = 10,240, a 
reduction of more than two orders of magnitude! 
The computation in the flow graph ofFigure 9.9 can be reduced further by exploit­
ing the symmetry and periodicity ofthe coefficients WN' We first note that, in proceeding 
from one stage to the next in Figure 9.9, the basic computation is in the form of Fig­
ure 9.8, i.e., it involves obtaining a pair of values in one stage from a pair of values in the 
preceding stage, where the coefficients are always powers of WN and the exponents are 

730 
Chapter 9 
Computation of the Discrete Fourier Transform 
(m _ I)SI 
mth 
stage 
stage 
Figure 9.10 
Row graph of simplified
w'N 
butterfly computation requiring only one 
-1 
complex multiplication. 
separated by N /2. Because of the shape of the flow graph, this elementary computation 
is called a butterfly. Since 
W;:/2 
e-j (21r/N)N/2 = e- j1r 
~1, 
(9.25) 
the factor Wl~+N/2 can be written as 
W r+N/2 _ W N /2 W r = _ Wr 
(9.26)
N 
-
N 
N 
N' 
With this observation, the butterfly computation of Figure 9.8 can be simplified to the 
form shown in Figure 9.10, which requires one complex addition and one complex 
subtraction, but only one complex multiplication instead of two. Using the basic flow 
graph of Figure 9.10 as a replacement for butterflies of the form of Figure 9.8, we obtain 
from Figure 9.9 the flow graph of Figure 9.11. In particular, the number of complex 
multiplications has been reduced by a factor of 2 over the number in Figure 9.9. 
Figure 9.11 shows log2 N stages of computation each involving a set of N/2 2­
point DFT computations (butterflies). Between the sets of2-point transforms are com­
x [0] 
x [2] 
x [6] 
X[O] 
X[3] 
Figure 9.11 
Flow graph of 8-point DFT 
WO
X[41::: ~1~:::: 
w~ 
x[l] 
x [5] 
w~ 
X[5]
1 
X[3I~M i X [6] 
x [7] 
N 
N 
N 
x[7] 
using the butterfly computation of 
-1 
-1 
-1 
Figure 9.10. 

731 
Section 9.2 
Decimation-in-Time FFT Algorithms 
plex multipliers of the form W,V' These complex multipliers have been called "twiddle 
factors" because they serve as adjustments in the process of converting the 2-point 
transforms into longer transforms. 
9.2.' Generalization and Programming the FFT 
The flow graph of Figure 9.11, which describes an algorithm for computation of an 
8-point discrete Fourier transform, is easily generalized to any N = 211, so it serves 
both as a proof that the computation requires on the order of N log N operations and 
as a graphical representation from which an implementation program could be writ­
ten. While programs in high-level computer languages are widely available, it may be 
necessary in some cases to construct a program for a new machine architecture or to 
optimize a given program to take advantage of low-level features of a given machine 
architecture. A refined analysis of the diagram reveals many details that are important 
for programming or for designing special hardware for computing the DFT. We call 
attention to some of these details in Sections 9.2.2 and 9.2.3 for the decimation-in-time 
algorithms and in Sections 9.3.1 and 9.3.2 for the decimation-in-frequency algorithms. 
In Section 9.4 we discuss some additional practical considerations. While these sections 
are not essential for obtaining a basic understanding of FFT principles, they provide 
useful guidance for programming and system design. 
9.2.2 In-Place Computations 
The essential features of the flow graph of Figure 9.11 are the branches connecting the 
nodes and the transmittance of each of these branches. No matter how the nodes in 
the flow graph are rearranged, it will always represent the same computation, provided 
that the connections between the nodes and the transmittances of the connections are 
maintained. The particular form for the flow graph in Figure 9.11 arose out of deriving 
the algorithm by separating the original sequence into the even-numbered and odd­
numbered points and then continuing to create smaller and smaller subsequences in 
the same way. An interesting by-product of this derivation is that this flow graph, in 
addition to describing an efficient procedure for computing the discrete Fourier trans­
form, also suggests a useful way of storing the original data and storing the results of 
the computation in intermediate arrays. 
To see this, it is useful to note that according to Figure 9.11, each stage of the com­
putation takes a set of N complex numbers and transforms them into another set of N 
complex numbers through basic butterfly computations of the form of Figure 9.10. This 
process is repeated v = log2 N times, resulting in the computation of the desired dis­
crete Fourier transform. When implementing the computations depicted in Figure 9.11, 
we can imagine the use of two arrays of (complex) storage registers, one for the array 
being computed and one for the data being used in the computation. For example, in 
computing the first array in Figure 9.11, one set of storage registers would contain the 
input data and the second set would contain the computed results for the first stage. 
While the validity of Figure 9.11 is not tied to the order in which the input data are 
stored, we can order the set of complex numbers in the same order that they appear 
in the figure (from top to bottom). We denote the sequence of complex numbers re­

732 
Chapter 9 
Computation of the Discrete Fourier Transform 
Xm_1W]O---~--~--------~----~ 
w' 
Xm-dq] a 
oN 
if' 
~l 
"oXm[q] 
Figure 9.12 
Flow graph of Eqs. (9.28). 
suIting from the mth stage of computation as Xm[l], where l 
0,1, ... , N - 1, and 
m 
1,2, ... , v. Furthermore, for convenience, we define the set of input samples as 
Xo[l]. We can think of Xm-lf£] as the input array and Xm[l] as the output array for the 
mID stage of the computations. Thus, for the case of N = 8, as in Figure 9.11, 
Xo[OJ = x[O], 
Xo[l] = x[4], 
Xo[2] 
x[2], 
Xo[3] = x[6], 
(9.27)
Xo[4] = x[I], 
Xo[S] = x[S], 
Xo[6] 
x [3], 
Xo[7] 
x[7J. 
Using this notation, we can label the input and output of the butterfly computation in 
Figure 9.10 as indicated in Figure 9.12, with the associated equations 
Xm[P] = Xm-tlP] + WNXm-l[q], 
(9.28a) 
Xm[q] 
Xm-l[p] 
WNXm-tlq]. 
(9.28b) 
In Eqs. (9.28), p, q, and r vary from stage to stage in a manner that is readily 
inferred from Figure 9.11 and from Eqs. (9.21), (9.23), and (9.24) and. It is clear from 
Figures 9.11 and 9.12 that only the complex numbers in locations p and q of the (m -1)st 
array are required to compute the elements p and q of the mth array. Thus, only one 
complex array of N storage registers is physically necessary to implement the complete 
computation if Xm [p] and Xm [q] are stored in the same storage registers as Xm-l [p] and 
Xm-l [q], respectively. This kind of computation is commonly referred to as an in-place 
computation. The fact that the flow graph of Figure 9.11 (or Figure 9.9) represents an 
in-place computation is tied to the fact that we have associated nodes in the flow graph 
that are on the same horizontal line with the same storage location and the fact that the 
computation between two arrays consists of a butterfly computation in which the input 
nodes and the output nodes are horizontally adjacent. 
In order that the computation may be done in place as just discussed, the input 
sequence must be stored (or at least accessed) in a nonsequential order, as shown in 
the flow graph of Figure 9.11. In fact, the order in which the input data are stored and 
accessed is referred to as bit-reversed order. To see what is meant by this terminology, 
we note that for the 8-point flow graph that we have been discussing, three binary digits 

733 
Section 9.2 
Decimation-in-Time FFT Algorithms 
nz 
nl 
no 
0 
x [000)
0 
I 
I 
0 
I 
I 
x [n2nlnO) 
1 
x [001] 
0 
x [010) 
x [0111 
0 
x[100]
0 
x[101] 
0 
x[110] 
Figure 9.13 
Tree diagram depicting 
x[111] 
normal-order sorting. 
are required to index through the data. Writing the indices in Eqs. (9.27) in binary form, 
we obtain the following set of equations: 
Xo[OOO] = x[OOO], 
Xo[OOI] = x[IOO], 
Xo[OIO] = x [010], 
Xo[OlI] = x[lIO], 
(9.29)
Xo[100] = x[OOI], 
Xo[lOl] = x[I01], 
XollIO] 
x[OlI], 
XolIlI] 
x[l1I]. 
If (nz, nl, no) is the binary representation of the index of the sequence x[n], then the 
sequence value x[nz, nb no] is stored in the array position Xo[no, nl, nz]. That is, in 
determining the position of x[nz, nl, no] in the input array, we must reverse the order 
of the bits of the index n. 
Consider the process depicted in Figure 9.13 for sorting a data sequence in normal 
order by successive examination of the bits representing the data index. If the most 
significant bit of the data index is zero, x[n] belongs in the top half of the sorted array; 
otherwise it belongs in the bottom half. Next, the top half and bottom half subsequences 
can be sorted by examining the second most significant bit, and so on. 
To see why bit-reversed order is necessary for in-place computation, recall the 
process that resulted in Figure 9.9 and Figure 9.11. The sequence x[n] was first divided 
into the even-numbered samples, with the even-numbered samples occurring in the top 
half of Figure 9.4 and the odd-numbered samples occurring in the bottom half. Such 
a separation of the data can be carried out by examining the least significant bit lno] 
in the index n. If the least significant bit is 0, the sequence value corresponds to an 
even-numbered sample and therefore will appear in the top half of the array Xo[l], 
and if the least significant bit is 1, the sequence value corresponds to an odd-numbered 

734 
Chapter 9 
Computation of the Discrete Fourier Transform 
no 
nj 
n2 
0 
x [000] 
x[lOO]
0 -
y 
0 
x [010] 
~ 
x[110] 
x [n2nln~ 
0 
x [001]
0 
x [101]
I 
0 
x [011]
1 
Figure 9.14 
Tree diagram depicting
x[lll] 
bit-reversed sorting. 
sample and consequently will appear in the bottom half of the array. Next, the even­
and odd-indexed subsequences are sorted into their even- and odd-indexed parts, and 
this can be done by examining the second least significant bit in the index. Considering 
first the even-indexed subsequence, if the second least significant bit is 0, the sequence 
value is an even-numbered term in the subsequence, and if the second least significant 
bit is 1, then the sequence value has an odd-numbered index in this subsequence. The 
same process is carried out for the subsequence formed from the original odd-indexed 
sequence values. This process is repeated until N subsequences of length 1 are obtained. 
This sorting into even- and odd-indexed subsequences is depicted by the tree diagram 
of Figure 9.14. 
The tree diagrams of Figures 9.13 and 9.14 are identical, except that for normal 
sorting, we examine the bits representing the index from left to right, whereas for the 
sorting leading naturally to Figure 9.9 or 9.11, we examine the bits in reverse order, right 
to left, resulting in bit-reversed sorting. Thus, the necessity for bit-reversed ordering of 
the sequence x[n] results from the manner in which the DFTcomputation is decomposed 
into successively smaller DFT computations in arriving at Figures 9.9 and 9.11. 
9.2.3 Alternative Forms 
Although it is reasonable to store the results of each stage of the computation in the 
order in which the nodes appear in Figure 9.11, it is certainly not necessary to do so. 
No matter how the nodes of Figure 9.11 are rearranged, the result will always be a valid 
computation of the discrete Fourier transform of x[n], as long as the branch transmit­
tances are unchanged. Only the order in which data are accessed and stored will change. 
If we associate the nodes with indexing of an array of complex storage locations, it is 
clear from our previous discussion that a flow graph corresponding to an in-place com­
putation results only if the rearrangement of nodes is such that the input and output 
nodes for each butterfly computation are horizontally adjacent. Otherwise two complex 
storage arrays will be required. Figure 9.11, is, of course, such an arrangement. Another 

735 
Section 9.2 
Decimation-in-Time FFT Algorithms 
is depicted in Figure 9.15. In this case, the input sequence is in normal order and the 
sequence of DFT values is in bit-reversed order. Figure 9.15 can be obtained from 
ure 9.11 as follows: All the nodes that are horizontally adjacent to x[4] in Figure 9.11 
are interchanged with all the nodes horizontally adjacent to x[1]. Similarly, all the nodes 
that are horizontally adjacent to x[6] in Figure 9.11 are interchanged with those that are 
horizontally adjacent to x[3]. The nodes horizontally adjacent to x[O], x[2], x[5], and 
x[7] are not disturbed. The resulting flow graph in Figure 9.15 corresponds to the form 
of the decimation-in-time algorithm originally given by Cooley and Tukey (1965). 
The only difference between Figures 9.11 and 9.15 is in the ordering of the nodes. 
This implies that Figures 9.11 and 9.15 represent two different programs for carrying 
out the computations. The branch transmittances (powers of WN) remain the same, 
and therefore the intermediate results will be exactly the same-they will be computed 
in a different order within each stage. There are, of course, a large variety of possible 
orderings. However, most do not make much sense from a computational viewpoint. As 
one example, suppose that the nodes are ordered such that the input and output both 
appear in normal order. A flow graph of this type is shown in Figure 9.16. In this case, 
however, the computation cannot be carried out in place because the butterfly structure 
does not continue past the first stage. Thus, two complex arrays of length N would be 
required to perform the computation depicted in Figure 9.16. 
In realizing the computations depicted by Figures 9.11, 9.15, and 9.16, it is clearly 
necessary to access elements of intermediate arrays in non-sequential order. Thus, for 
greater computational speed, the complex numbers must be stored in random-access 
memory.? For example, in the computation of the first array in Figure 9.11 from the 
input array, the inputs to each butterfly computation are adjacent node variables and 
are thought of as being stored in adjacent storage locations. In the computation of 
the second intermediate array from the first, the inputs to a butterfly are separated by 
two storage locations; and in the computation of the third array from the second, the 
inputs to a butterfly computation are separated by four storage locations. If N > 8, the 
separation between butterfly inputs is 8 for the fourth stage, 16 for the fifth stage, and 
so on. The separation in the last (vth) stage is N /2. 
In Figure 9.15 the situation is similar in that, to compute the first array from the 
input data we use data separated by 4, to compute the second array from the first array we 
use input data separated by 2, and then finally, to compute the last array we use adjacent 
data. It is straightforward to imagine simple algorithms for modifying index registers 
to access the data in the flow graph of either Figure 9.11 or Figure 9.15 if the data are 
stored in random-access memory. However, in the flow graph ofFigure 9.16, the data are 
accessed non-sequentially, the computation is not in place, and a scheme for indexing 
the data is considerably more complicated than in either of the two previous cases. 
Even given the availability of large amounts of random-access memory, the overhead 
for index computations could easily nullify much of the computational advantage that 
is implied by eliminating multiplications and additions. Consequently, this structure has 
no apparent advantages. 
7When the Cooley-Thkey algorithms first appeared in 1965, digital memory was expensive and of 
limited size. The size and availability of random access memory is no longer an issue except for exceedingly 
large values of N. 

x [0] 
X[O] 
x[l] 
X[4] 
x[2] 
X[2] 
x[3] 
X[6]
-1 
-1 
W,Z 
x [4]  
X[I] 
xIS] 
W~ 
WQ 
X[61~ 
Fig~re 9.15 
Rearrangement of 
x17] 
N 
~::::
Figure 9.11 with input in normal order 
-1 
-1 
-1 
and output in bit-reversed order. 
x [0] 
X[O] 
xlI] 
X[I] 
x[2] 
X[2] 
x13] 
WO 
x [4] 
N 
wZ 
x[S]  
X[S] 
wZ 
x[6]  
X[6] 
Figure 9.16 Rearrangement of 
w~ 
x17]  
Figure 9.11 with both input and output in
-1 X[7]
-1 
-1 
normal order. 
xlO] 
X[O] 
x [4] 
X[I) 
x[2] 
x [6] 
X[3] 
x[I)  
X[4] 
x[S]  
X[S] 
x[3] 
X[6]  
Figure 9.17 
Rearrangement of 
Figure 9.11 having the same geometry 
for each stage, thereby simplifying data
x [7]  
X[7) 
access. 
736 

737 
Section 9.3 
Decimation-in-Frequency FFT Algorithms 
Some forms have advantages even if they do not allow in-place computation. 
A rearrangement of the flow graph in Figure 9.11 that is particularly useful when an 
adequate amount of random-access memory is not available is shown in Figure 9.17. This 
flow graph represents the decimation-in-time algorithm originally given by Singleton 
(1969). Note first that in this flow graph the input is in bit-reversed order and the output 
is in normal order. The important feature of the flow graph is that the geometry is 
identical for each stage; only the branch transmittances change from stage to stage. This 
makes it possible to access data sequentially. Suppose, for example that we have four 
separate mass-storage files, and suppose that the first half of the input sequence (in 
bit-reversed order) is stored in one file and the second half is stored in a second file. 
Then the sequence can be accessed sequentially in files 1 and 2 and the results written 
sequentially on files 3 and 4, with the first half of the new array being written to file 3 
and the second half to file 4. Then at the next stage of computation, files 3 and 4 are the 
input, and the output is written to files 1 and 2. This is repeated for each of the v stages. 
Such an algorithm could be useful in computing the DFf of extremely long se­
quences. This could mean values of N on the order of hundreds of millions since random­
access memories of giga-byte size are routinely available. Perhaps a more interesting 
feature of the diagram in Figure 9.17 is that the indexing is very simple and it is the same 
from stage-to-stage. With two banks of random-access memory, this algorithm would 
have very simple index calculations. 
9.3 DECIMATION-IN-FREQUENCY FFT ALGORITHMS 
The decimation-in-time FFf algorithms are based on structuring the DFf computation 
by forming smaller and smaller subsequences of the input sequence x[n]. Alternatively, 
we can consider dividing the DFf sequence X[k] into smaller and smaller subsequences 
in the same manner. FFf algorithms based on this procedure are commonly called 
decimation-in-frequency algorithms. 
To develop this class of FIT algorithms, we again restrict the discussion to N 
a power of 2 and consider computing separately the N /2 even-numbered frequency 
samples and the N/2 odd-numbered frequency samples. We have depicted this in the 
block diagram representation in Figure 9.18where Xo[k) 
X[2k) andXILk) = X[2k+1]. 
In shifting left by 1 DFf sample so that the compressor selects the odd-indexed samples, 
it is important to remember that the DFf X[k] is implicitly periodic with period N. This 
is denoted "Circular Left Shift 1" (and correspondingly "Circular Right Shift 1") in 
Figure 9.18. Observe that this diagram has a similar structure to Figure 9.3, where the 
same operations were applied to the time sequence x[n) instead of the DFT X[k). In 
this case, Figure 9.18 directly depicts the fact that the N -point transform X[k) can be 
obtained by interleaving its even-indexed and odd-indexed samples after expansion by 
a factor of 2. 
Figure 9.18 is a correct representation of X[kJ, but in order to use it as the basis 
for computing X[k], we first show that X[2k] and X[2k + 1] can be computed from 
the time-domain sequence x[n]. In Section 8.4 we saw that the DFf is related to the 
DTFf by sampling at frequencies 2JT k/ N with the result that the corresponding time­

738 
Chapter 9 
Computation ofthe Discrete Fourier Transform 
X(k] 
X[k] 
Circular 
Circular  
Left  
Right  
Shift 1  
Shift 1 
Figure 9.18 Illustration of the basic 
principle of decimation-in-frequency. 
domain operation is time-aliasing with repetition length (period) N. As discussed in 
Section 8.4, if N is greater than or equal to the length of the sequence x[n], the inverse 
DFT yields the original sequence over 0 ~ n ~ N - 1 because the N -point copies of 
x[n1do not overlap when time-aliased with repetition offset N. However, in Figure 
9.18, the DFT is compressed by 2, which is equivalent to sampling the DTFT X (e jW ) at 
frequencies 2Jfk/(N/2). Thus, the implicit periodic time-domain signal represented by 
Xo[k] = X[2k] is 
xo[n] = L
00 
x[n + mN/2] 
00 < n < 00. 
(9.30) 
m=-oo 
Since x[n] has length N, only two of the shifted copies of xln] overlap in the interval 
o ~ n ~ N /2 - 1, so the corresponding finite-length sequence xo[n] is 
xoln] = x[n] + x[n + N /2] 
o ~ n ~ N/2 -1. 
(9.31a) 
To obtain the comparable result for the odd-indexed DFT samples, recall that the cir­
cularly shifted DFT X[k + 1] corresponds to W~x[n] (see Property 6 of Table 8.2). 
Therefore the N/2-point sequence xl[n] corresponding to XIlk] 
X[2k + 1] is 
Xl[n] = x[n]W~ +xln + N/2]W~+N!2 
= (x[n] 
x[n+N/2])W~ 
0<n<N/2-1, 
(9.31b) 
. 
N/2
smce WN 
=-1.  
From Eqs. (9.31a) and (9.31b), it follows that  
N/2-1 
Xo[k] 
L (x[n] + x[n + N /2])Wm2 
(9.32a) 
n=O 
Nj2-1 
Xl[k] = L [(x[n] 
x[n + N /2]) W~]Wt/2 
(9.32b) 
11=0 
k = 0, 1, ... , N /2 
1. 
Se 

, 
Section 9.3 
Decimation-in-Frequency FFT Algorithms 
739 
x [0] Q------p---+---I 
x [1 ] O-.......----,<-;-----O----J--l 
X[O] 
X[2j
N 
point
Z 
Xo[k] 
DFT 
X[4] 
x [3] Q---*--*'---'*",,""-P---+---I 
X[6]
'---------' 
x [4] O-"""*---l~~....-.t----J--l 
X[l] 
X[3] 
x [6] d---+-----'~-b---,.:.:.--I 
X[5] 
X[7] 
Figure 9.19 Flow graph of decimation-in-frequency decomposition of an N-point 
OFT computation into two (N/2)-point OFT computations (N = 8). 
Equation (9.32a) is the (N/2)-point DFT of the sequence xo[n] obtained by adding the 
second half of the input sequence to the first half. Equation (9.32b) is the (N/2)-point 
DFT of the sequence xdn] obtained by subtracting the second half ofthe input sequence 
from the first half and multiplying the resulting sequence by WN. 
Thus, using Eqs. (9.32a) and (9.32b), the even-numbered and odd-numbered out­
put points of X[kJ can be computed since X[2kJ = Xo[k] and X[2k + 1] = Xl[k], 
respectively. The procedure suggested by Eqs. (9.32a) and (9.32b) is illustrated for the 
case of an 8-point DFT in Figure 9.19. 
Proceeding in a manner similar to that followed in deriving the decimation -in-time 
algorithm, we note that for N a power of 2, N /2 is divisible by 2 so the (N/2)-point DFTs 
can be computed by computing the even-numbered and odd numbered output points 
for those DFTs separately. As in the case of the procedure leading to Eqs. (9.32a) 
and (9.32b), this is accomplished by combining the first half and the last half of the 
input points for each of the (N/2)-point DFTs and then computing (N/4)-point DFTs. 
The flow graph resulting from taking this step for the 8-point example is shown in 
Figure 9.20. For the 8-point example, the computation has now been reduced to the 
computation of 2-point DFTs, which are implemented by adding and subtracting the 
input points, as discussed previously. Thus, the 2-point DFTs in Figure 9.20 can be 
replaced by the computation shown in Figure 9.21, so the computation of the 8-point 
DFT can be accomplished by the algorithm depicted in Figure 9.22. We again see logz N 
stages of 2-point transforms coupled together through twiddle factors that in this case 
occur at the output of the 2-point transforms. 
By counting the arithmetic operations in Figure 9.22 and generalizing to N = 2", 
we see that the computation of Figure 9.22 requires (N/2) logz N complex multiplica­
tions and N logz N complex additions. Thus, the total number of computations is the 
same for the decimation-in-frequency and the decimation-in-time algorithms. 

x [0] 
N 
. f--o X[O]
'4 pomt 
x[I] 
OFT 
f--oX[4] 
w~ 
x [2] 
N 
. f--oX[2] 
- -pomt
W 2 
4 
x [3] 
-1 
N 
OFT 
X[6] 
W~ 
x [4] 
N. 
X[I]
'4- pomt 
x[5] 
OFT 
f--oX[5] 
WO
<I'lL 
" 
_1 
t xP]
~N
. 
Fig.n9.2D Aowgrapho1
w2 '4 - pomt 
decimatio~-in-fre~uency decomposition 
x [7] 
N 
OFT 
X[7] 
of an a-pomt OFT mto four 2-pomt OFT 
-1 
-1 
. 
computations. 
Xv- 1[P] 0;;: 
:;p 
OXy[Pl 
Figure 9.21 
Flow graph of atypical 
2-point OFT as required in the last stage 
of decimation-in-freQuency
Xv_dq] 
Xy[q] 
decomposition. 
x [0] 
X[O] 
WO 
~XI4] 
X[21 
WO 
::::~ 
XI3]~ 
~XI6] 
_1 
~;::
x [4] 
N 
X[I] 
WO 
~XI5] 
X[3]
::::~ 
Figure 9.22 
Flow graph of complete
WOtV
x[7] 
x[7] 
decimation-in-freQuency decomposition 
-1 
-1 
of an 8-point OFT computation. 
740 

741 
Section 9.3 
Decimation-in-Frequency FFT Algorithms 
9.3.1 In-Place Computation 
The flow graph in Figure 9.22 depicts one FFf algorithm based on decimation in fre­
quency. We can observe a number of similarities and also a number of differences in 
comparing this graph with the flow graphs derived on the basis of decimation in time. 
As with decimation in time, of course, the flow graph of Figure 9.22 corresponds to a 
computation of the discrete Fourier transform, regardless of how the graph is drawn, 
as long as the same nodes are connected to each other with the proper branch trans­
mittances. In other words, the flow graph of Figure 9.22 is not based on any assumption 
about the order in which the input sequence values are stored. However, as was done 
with the decimation-in-time algorithms, we can interpret successive vertical nodes in 
the flow graph ofFigure 9.22 as corresponding to successive storage registers in a digital 
memory. In this case, the flow graph in Figure 9.22 begins with the input sequence in 
normal order and provides the output DFf in bit-reversed order. The basic computa­
tion again has the form of a butterfly computation, although the butterfly is different 
from that arising in the decimation-in-time algorithms. However, because of the but­
terfly nature of the computation, the flow graph of Figure 9.22 can be interpreted as an 
in-place computation of the discrete Fourier transform. 
9.3.2 Alternative Forms 
A variety of alternative forms for the decimation-in-frequency algorithm can be ob­
tained by transposing the decimation-in-time forms developed in Section 9.2.3. If we 
denote the sequence of complex numbers resulting from the mth stage of the computa­
tion as Xm[t'], where e = 0,1, ... , N 
1, and m 
1,2, ... , v, then the basic butterfly 
computation shown in Figure 9.23 has the form 
Xm[P] = Xm-l[p] + Xm-l[q], 
(9.33a) 
Xm[q] = (Xm-l[p] 
Xm-l[q])lVN· 
(9.33b) 
Comparing Figures 9.12 and 9.23 or Eqs. (9.28) and (9.33), it appears that the 
butterfly computations are different for the two classes of FFf algorithms. However, 
the two butterfly flow graphs are, in the terminology of Chapter 6, transposes of one 
another. That is, if we reverse the direction of arrows and redefine the input and output 
nodes in Figure 9.12, we obtain Figure 9.23 and vice-versa. Since the FFf flow graphs 
consist of connected sets of butterflies, it is not surprising, therefore, that we also note 
Figure 9.23 
Flow graph of atypical 
butterfly computation required in 
Rgure 9.22. 

742 
Chapter 9 
Computation of the Discrete Fourier Transform 
x [0] 
X[O] 
x [4] 
X[I] 
x [2] 
X[2] 
x[6] 
X[3] 
WO 
N
x[l] 
X[4]
-1 wZ 
x[5] 
X[5]
-1 wZ 
Figure 9.24 
Flow graph of a 
x[3] 
X[6] 
decimation-in-frequency OFT algorithm
-1 wZ 
obtained from Figure 9.22. Input in 
x [7] 
X[7] 
bit-reversed order and output in normal 
-1 
-1 
-1 
order. (Transpose of Figure 9.15.) 
a resemblance between the FFT flow graphs of Figures 9.11 and 9.22. Specifically, Fig­
ure 9.22 can be obtained from Figure 9.11 by reversing the direction of signal flow and 
interchanging the input and output. That is, Figure 9.22 is the transpose of the flow graph 
in Figure 9.11. In Chapter 6 we stated a transposition theorem that applies only to single­
input/single-output flow graphs. When viewed as flow graphs, however, FFT algorithms 
are multi-input/multi-output systems, which require a more general form of the transpo­
sition theorem. (See Claasen and Mecklenbrauker, 1978.) Nevertheless, it is intuitively 
clear that the input-output characteristics of the flow graphs in Figures 9.11 and 9.22 
are the same based simply on the above observation that the butterflies are transposes 
of each other. This can be shown more formally by noting that the butterfly equations in 
Eqs. (9.33) can be solved backward, starting with the output array. (Problem 9.31 out­
lines a proof of this result.) More generally, it is true that for each decimation-in-time 
FFT algorithm, there exists a decimation-in-frequency FFT algorithm that corresponds 
to interchanging the input and output and reversing the direction of all the arrows in 
the flow graph. 
This result implies that all the flow graphs of Section 9.2 have counterparts in the 
class ofdecimation-in-frequency algorithms. This, ofcourse, also corresponds to the fact 
that, as before, it is possible to rearrange the nodes of a decimation-in-frequency flow 
graph without altering the final result. 
Applying the transposition procedure to Figure 9.15 leads to Figure 9.24. In this 
flow graph, the output is in normal order and the input is in bit-reversed order. The 
transpose of the flow graph of Figure 9.16 would lead to a flow graph with both the 
input and output in normal order. An algorithm base on the resulting flow graph would 
suffer from the same limitations as for Figure 9.16. 
The transpose of Figure 9.17 is shown in Figure 9.25. Each stage of Figure 9.25 has 
the same geometry, a property that simplifies data access, as discussed before. 

743 
Section 9.4 
Practical Considerations 
x [0] 
X[O] 
x[I] 
X[4] 
x[2] 
X[2] 
x[3] 
X[6] 
x [4] 
X[I] 
xIS] 
X[S] 
x[6] 
X[3] 
Figure 9.25 
Rearrangement of 
Figure 9.22 having the same geometry 
x[?] 
x[7] 
for each stage, thereby simplifying data 
-1 
-1 
-1 
access. (Transpose of Figure 9.17.) 
9.4 PRACTICAL CONSIDERATIONS 
In Sections 9.2 and 9.3, we discussed the basic principles of efficient computation of the 
DFf when N is an integer power of 2. In these discussions, we favored the use of signal 
flow graph representations rather than explicitly writing out in detail the equations that 
such flow graphs represent. Of necessity, we have shown flow graphs for specific values 
of N. However, by considering a flow graph such as that in Figure 9.11, for a specific 
value of N, it is possible to see how to structure a general computational algorithm that 
would apply to any N 
2v. While the discussion in Sections 9.2 and 9.3 is completely 
adequate for a basic understanding of the FFf principles, the material of this section is 
intended to provide useful guidance for programming and system design. 
Although it is true that the flow graphs of the previous sections capture the essence 
of the FFf algorithms that they depict, a variety of details must be considered in the 
implementation of a given algorithm. In this section, we briefly suggest some of these. 
Specifically, in Section 9.4.1 we discuss issues associated with accessing and storing data 
in the intermediate arrays of the FFf. In Section 9.4.2 we discuss issues associated 
with computing or accessing the branch coefficients in the flow graph. Our emphasis 
is on algorithms for N a power of 2, but much of the discussion applies to the general 
case as well. For purposes of illustration, we focus primarily on the decimation-in-time 
algorithm of Figure 9.11. 
9.4.1 Indexing 
In the algorithm depicted in Figure 9.11, the input must be in bit-reversed order so 
that the computation can be performed in place. The resulting DFf is then in normal 
order. Generally, sequences do not originate in bit-reversed order, so the first step in the 
implementation of Figure 9.11 is to sort the input sequence into bit-reversed order. As 
can be seen from that figure and Eqs. (9.27) and (9.29), bit-reversed sorting can be done 

744 
Chapter 9 
Computation of the Discrete Fourier Transform 
in place, since samples are only pairwise interchanged; Le., a sample at a given index is 
interchanged with the sample in the location specified by the bit-reversed index. This 
is conveniently done in place by using two counters, one in normal order and the other 
in bit-reversed order. The data in the two positions specified by the two counters are 
simply interchanged. Once the input is in bit-reversed order, we can proceed with the 
first stage ofcomputation. In this case, the inputs to the butterflies are adjacent elements 
of the array Xo['J. In the second stage, the inputs to the butterflies are separated by 2. In 
the mth stage, the butterfly inputs are separated by 2m-i. The coefficients are powers of 
w~N/2m) in the mth stage and are required in normal order if computation of butterflies 
begins at the top of the flow graph of Figure 9.11. The preceding statements define the 
manner in which data must be accessed at a given stage, which, of course, depends on 
the flow graph that is implemented. For example, in the mth stage of Figure 9.15, the 
m
butterfly spacing is 2v-
, and in this case the coefficients are required in bit-reversed 
order. The input is in normal order; however, the output is in bit-reversed order, so it 
generally would be necessary to sort the output into normal order by using a normal­
order counter and a bit-reversed counter, as discussed previously. 
In general, if we consider all the flow graphs in Sections 9.2 and 9.3, we see that each 
algorithm has its own characteristic indexing issues. The choice of a particular algorithm 
depends on a number of factors. The algorithms utilizing an in-place computation have 
the advantage of making efficient use of memory. Two disadvantages, however, are 
that the kind of memory required is random-access rather than sequential memory 
and that either the input sequence or the output DFT sequence is in bit-reversed order. 
Furthermore, depending on whether adecimation-in-time ora decimation-in-frequency 
algorithm is chosen and whether the inputs or the outputs are in bit-reversed order, the 
coefficients are required to be accessed in either normal order or bit-reversed order. If 
non-random-access sequential memory is used, some fast Fourier transform algorithms 
utilize sequential memory, as we have shown, but either the inputs or the outputs must 
be in bit-reversed order. While the flow graph for the algorithm can be arranged so that 
the inputs, the outputs, and the coefficients are in normal order, the indexing structure 
required to implement these algorithms is complicated, and twice as much random 
access memory is required. Consequently, the use of such algorithms does not appear 
to be advantageous. 
The in-place FFT algorithms of Figures 9.11, 9.15, 9.22, and 9.24 are among the 
most commonly used. If a sequence is to be transformed only once, then bit-reversed 
sorting must be implemented on either the input or the output. However, in some 
situations a sequence is transformed, the result is modified in some way, and then the 
inverse DFT is computed. For example, in implementing FIR digital filters by block 
convolution using the discrete Fourier transform, the DFT of a section of the input 
sequence is multiplied by the DFT of the filter impulse response, and the result is inverse 
transformed to obtain a segment of the output of the filter. Similarly, in computing 
an autocorrelation function or cross-correlation function using the discrete Fourier 
transform, a sequence will be transformed, the DFTs will be multiplied, and then the 
resulting product will be inverse transformed. When two transforms are cascaded in 
this way, it is possible, by appropriate choice of the FFT algorithms, to avoid the need 
for bit reversal. For example, in implementing an FIR digital filter using the DFT, 
we can choose an algorithm for the direct transform that utilizes the data in normal 

745 
Section 9.5 
More General FFT Algorithms 
order and provides a DFT in bit-reversed order. Either the flow graph corresponding 
to Figure 9.15, based on decimation in time, or that of Figure 9.22, based on decimation 
in frequency, could be used in this way. The difference between these two forms is that 
the decimation-in-time form requires the coefficients in bit-reversed order, whereas the 
decimation-in-frequency form requires the coefficients in normal order. 
Note that Figure 9.11 utilizes coefficients in normal order, whereas Figure 9.24 
requires the coefficients in bit-reversed order. If the decimation-in-time form of the 
algorithm is chosen for the direct transform, then the decimation-in-frequency form 
of the algorithm should be chosen for the inverse transform, requiring coefficients in 
bit-reversed order. Likewise, the decimation-in -freq uency algorithm for the direct trans­
form should be paired with the decimation-in-time algorithm for the inverse transform, 
which would then utilize normally ordered coefficients. 
9.4.2 Coefficients 
We have observed that the coefficients WN(twiddle factors) may be required in either 
bit-reversed order or in normal order. In either case we must store a table sufficient 
to look up all required values, or we must compute the values as needed. The first 
alternative has the advantage of speed, but of course requires extra storage. We observe 
from the flow graphs that we require WNfor r = 0, 1, ... , (N/2) 
1. Thus, we require 
N /2 complex storage registers for a complete table of values of WN.8 In the case of 
algorithms in which the coefficients are required in bit-reversed order, we can simply 
store the table in bit-reversed order. 
The computation of the coefficients as they are needed saves storage, but is less 
efficient than storing a lookup table. Ifthe coefficients are to be computed, it is generally 
most efficient to use a recursion formula. At any given stage, the required coefficients 
are all powers of a complex number of the form W~, where q depends on the algorithm 
and the stage. Thus, if the coefficients are required in normal order, we can use the 
recursion formula 
W~e = W~ . W~(e-l) 
(9.34) 
to obtain the eth coefficient from the (£ _1)st coefficient. Clearly, algorithms that require 
coefficients in bit-reversed order are not well suited to this approach. It should be 
noted that Eq. (9.34) is essentially the coupled-form oscillator of Problem 6.21. When 
using finite-precision arithmetic, errors can build up in the iteration of this difference 
equation. ]berefore, it is generally necessary to reset the value at prescribed points 
(e.g., W~/4 
- j) so that errors do not become unacceptable. 
9.5 MORE GENERAL FFT ALGORITHMS 
The power-of-two algorithms discussed in detail in Sections 9.2 and 9.3 are straight­
forward, highly efficient and easy to program. However, there are many applications 
where efficient algorithms for other values of N are very useful. 
8This number can be reduced using symmetry at the cost of greater complexity in accessing desired 
values. 

746 
Chapter 9 
Computation of the Discrete Fourier Transform 
9.5.1 Algorithms for Composite Values of N 
Although the special case of N a power of 2 leads to algorithms that have a partic­
ularly simple structure, this is not the only restriction on N that can lead to reduced 
computation for the DFT. The same principles that were applied in the power-of-two 
decimation-in-time and decimation-in-frequency algorithms can be employed when N 
is a composite integer, i.e., the product of two or more integer factors. For example, if 
N = NtNz, it is possible to express the N-point DFT as either a combination of Nt Nz­
point DFTs or as a combination of Nz Nt-point DFTs, and thereby obtain reductions in 
the number of computations. To see this, the indices nand k are represented as follows: 
nt :: 0, 1, ... , Nt 
1 
n 
Nznt + nz 
(9.35a)
{ nz -
0, 1, ... , Nz 
1 
kt = 0, 1, ... , Nt 
1
k 
kt + NtkZ 
(9.35b)
{ kz = 0, 1, ... , Nz 
1. 
Since N = NlNz, these index decompositions ensure that nand k range over all the 
values 0, 1, ... , N 
1. Substituting these representations of nand k into the definition 
of the DFT leads after a few manipulations to 
X[k] = X[kt + Ntkz] 
Nz-t [(Nl-t 
)]
L 
L X[NZnl + nz]W~nl 
W~ln2 
w~~n2, 
(9.36) 
n2=O 
nl =0 
where kl = 0,1, ... , Nt 
1 and kz 
0,1, ... , Nz -
1. The part of Eq. (9.36) inside 
the parentheses represents Nz Nt-point DFTs, while the outer sum corresponds to Nt 
Nz-point DFTs of the outputs of the first set of transforms occurring after modification 
by the twiddle factors WN"1n,-. 
If N1 = 2 and Nz 
N12, Eq. (9.36) reduces to the first stage decomposition of 
the decimation-in-frequency power-of-two algorithm depicted in Figure 9.19 of Section 
9.3, which consists of N 12 2-point transforms followed by two N 12-point transforms. 
Conversely, if Nl = N 12 and Nz 
2, Eq. (9.36) reduces to the first stage decomposition 
of the decimation-in-time power-of-two algorithm depicted in Figure 9.4 Section 9.2, 
which consists of two N12-point transforms followed by N12 2-point transforms.9 
Cooley-Tukey algorithms for general composite N are obtained by first doing the 
Nl-point transforms and then again applying Eq. (9.36) to another remaining factor Nz 
of NI Nt until all the factors of N have been used. The repeated application ofEq. (9.36) 
9For Figure 9.4 to be an exact representation of Eq. (9.36), the two-point butterflies of the last stage 
must be replaced by the butterflies of Figure 9.10. 

747 
Section 9.5 
More General FFT Algorithms 
Number of FLOPS for MATLAB FFT Function 
6 ~~------.---------.---------~--------.---------,-, 
X 105 
5 
... 
~ 4 
.............. . 
C 3 
... 
<U 
~ 
e 
::lZ 2 
......... 
1 
50 
.... -... -. -.. -. -.............. 
~ -. ---­
. 
.
. 
. 
100 
150 
200 
250 
Transform length N 
Figure 9.26 
Number of floating-point operations as afunction of N for MATLAB 
fft ( ) function (revision 5.2). 
leads to decompositions similar to the power-of-two algorithms. These algorithms re­
quire only slightly more complicated indexing than the power of 2 case. If the factors 
of N are relatively prime, the number of multiplications can be further reduced at the 
cost of more complicated indexing. The "prime factor" algorithms use different index 
decompositions from those of Eqs. (9.35a) and (9.35b) so as to eliminate the twiddle 
factors in Eq. (9.36), and thus save a significant amount of computation. The details of 
the more general Cooley-Thkey and prime factor algorithms are discussed in Burrus 
and Parks (1985), Burrus (1988), and Blahut (1985). 
As an illustration of what can be achieved using such prime factor algorithms, 
consider the measurements plotted in Figure 9.26. These measurements of the number 
of floating-point operations (FLOPS) as a function of N are for MATLAB's fft ( ) 
function in Rev. 5.2 of MATLAB.lO As we have discussed, the total number of floating 
point operations should be proportional to N log2 N for N a power of two and propor­
tional to N 2 for direct computation. For other values of N the total operation count will 
be dependent on the number (and cardinality) of the factors. 
When N is a prime number, direct evaluation is required so the number ofFLOPS 
will be proportional to N 2• The upper (solid) curve in Figure 9.26 shows the function 
FLOPS(N) = 6N2 + 2N(N - 1). 
(9.37) 
All the points falling on this curve are for values N a prime number. The lower dashed 
curve shows the function 
FLOPS(N) = 6N log2 N. 
(9.38) 
lOThis graph was created with a modified version of a program written by C. S. Burrus. Since it is 
no longer possible to measure the number of floating-point operations in recent revisions of MATLAB, the 
reader may not be able to repeat this experiment. 

748 
Chapter 9 
Computation of the Discrete Fourier Transform 
The points falling on this curve are all for N a power of two. For other composite 
numbers the number of operations falls between the two curves. To see how efficiency 
varies from integer to integer, consider values of N from 199 to 202. The number 199 is 
a prime, so the number of operations (318004) falls on the maximum curve. The value 
N = 200 has the factorization N = 2 ·2·2·5 . 5, and the number of operations (27134) 
is near the minimum curve. For N 
201 = 3 . 67, the number of FLOPS is 113788, and 
for N = 202 = 2 . 101 the number is 167676. This wide difference between N = 201 
and N = 202 is because a WI-point transform requires much more computation than 
a 67-point transform. Also note that when N has many small factors (such as N 
200) 
the efficiency is much greater. 
9.5.2 Optimized FFT Algorithms 
An FFT algorithm is based on the mathematical decomposition of the DFT into a com­
bination of smaller transforms as we showed in detail in Sections 9.2 and 9.3. The FFT 
algorithm can be expressed in a high-level programming language that can be translated 
into machine-level instructions by compilers running on the target machine. In general, 
this will lead to implementations whose efficiency will vary with machine architecture. 
To address the issue of maximizing efficiency over a range of machines, Frigo and John­
son (1998 and 2005), developed a free-software library called FFTW ("Fastest Fourier 
Transform in the West"). FFTW uses a "planner" to adapt its generalized Coley-Tukey­
type FFT algorithms to a given hardware platform, thereby maximizing efficiency. The 
system operates in two stages, the first being a planning stage in which the computations 
are organized so as to optimize performance on the given machine, and the second being 
a computation stage where the reSUlting plan (program) is executed. Once the plan is 
determined for a given machine, it can be executed on that machine as many times as 
needed. The details of FFTW are beyond our scope here. However, Frigo and Johnson, 
2005 have shown that over a wide range of host machines, the FFTW algorithm is sig­
nificantly faster than other implementations for values of N ranging from about 16 up 
to 8192. Above 8192, the performance ofFFTW drops drastically due to memory cache 
issues. 
9.6  IMPLEMENTATION OF THE DFT USING 
CONVOLUTION 
Because of the dramatic efficiency of the FFT, convolution is often implemented by 
explicitly computing the inverse D FT ofthe product of the DFTs of each sequence to be 
convolved, where an FFT algorithm is used to compute both the forward and the inverse 
DFTs. In contrast, and even in apparent (but, of course, not actual) contradiction, it is 
sometimes preferable to compute the DFT by first reformulating it as a convolution. 
We have already seen an example of this in the Goertzel algorithm. A number of other, 
more sophisticated, procedures are based on this approach as discussed in the following 
sections. 

749 
Section 9.6 
Implementation of the OFT Using Convolution 
9.6.1  Overview of the Winograd Fourier Transform 
Algorithm 
One procedure proposed and developed by S. Winograd (1978), often referred to as 
the Winograd Fourier transform algorithm (WFfA), achieves its efficiency by express­
ing the DFf in terms of polynomial multiplication or, equivalently, convolution. The 
WFfA uses an indexing scheme corresponding to the decomposition of the DFT into a 
multiplicity of short-length DFfs where the lengths are relatively prime. Then the short 
DFfs are converted into periodic convolutions. A scheme for converting a DFf into a 
convolution when the number of input samples is prime was proposed by Rader (1968), 
but its application awaited the development of efficient methods for computing periodic 
convolutions. Winograd combined all of the foregoing procedures together with highly 
efficient algorithms for computing cyclic convolutions into a new approach to com­
puting the DFf. The techniques for deriving efficient algorithms for computing short 
convolutions are based on relatively advanced number-theoretic concepts, such as the 
Chinese remainder theorem for polynomials, and consequently, we do not explore the 
details here. However, excellent discussions of the details of the WFfA are available 
in McClellan and Rader (1979), Blahut (1985), and Burrus (1988). 
With the WFTA approach, the number of multiplications required for an N -point 
DFf is proportional to N rather than N log N. Although this approach leads to algo­
rithms that are optimal in terms of minimizing multiplications, the number of additions 
is significantly increased in comparison with the FFT. Therefore, the WFfA is most 
advantageous when multiplication is significantly slower than addition, as is often the 
case with fixed-point digital arithmetic. However, in processors where multiplication 
and accumulation are tied together, the Cooley-Tukey or prime factor algorithms are 
generally preferable. Additional difficulties with the WFfA are that indexing is more 
complicated, in-place computation is not possible, and there are major structural dif­
ferences in algorithms for different values of N. 
Thus, although the WFfA is extremely important as a benchmark for determining 
how efficient the DFfcomputation can be (in terms of number ofmultiplications), other 
factors often dominate in determining the speed and efficiency of a hardware or software 
implementation of the DFf computation. 
9.6.2  The Chirp Transform Algorithm 
Another algorithm based on expressing the DFf as a convolution is referred to as 
the chirp transform algorithm (CTA). This algorithm is not optimal in minimizing any 
measure of computational complexity, but it has been useful in a variety of applications, 
particularly when implemented in technologies that are well suited to doing convolu­
tion with a fixed, prespecified impulse response. The CTA is also more flexible than the 
FFT, since it can be used to compute any set of equally spaced samples of the Fourier 
transform on the unit circle. 
To derive the CTA, we letx[n] denote an N-pointsequence and X(e jW ) its Fourier 
transform. We consider the evaluation of M samples of X(e jW ) that are equally spaced 
in angle on the unit circle, as indicated in Figure 9.27, i.e., at frequencies 
W/c = wo + kLlw, 
k=O,l, ... ,M 
1,  
(9.39) 

750 
Chapter 9 
Computation of the Discrete Fourier Transform 
1m 
z-plane 
Unit  
circle  
Re 
Figure 9.27 
Frequency samples for 
chirp transform algorithm. 
where the starting frequency wo and the frequency increment ~w can be chosen arbi­
trarily. (For the specific case of the DFf, Wo =0, M = N, and ~W = 2JrIN.) The Fourier 
transform corresponding to this more general set of frequency samples is given by 
N-l 
X(ejwk ) 
Lx[n]e-jlt.lkn, 
k=O,l, ... ,M 
1, 
(9.40) 
n=O 
or, with W defined as 
W 
e-jAlt.l 
(9.41) 
and using Eq. (9.39), 
N-l 
X (ejlt.lk) 
Lx[n]e-jW()nwnk . 
(9.42) 
n=O 
To express X (ej(J)k) as a convolution, we use the identity 
nk 
i[n2 + k2 -
(k - n)2] 
(9.43) 
to express Eq. (9.42) as 
N-l 
X(ej(J)k) = L 
x [n]e-jW()n W n2 /2W k2/2W-(k-n)2/2. 
(9.44) 
n=O 
Letting 
g[n] 
x[n]e-jW()nWn2/2, 
(9.45) 
we can then write 
W k2 /2 
N-l 
)
X(e jwk ) = 
L 
g[n]W-(k-n)2/2 
, 
k = 0, 1, ... , M 
1. 
(9.46)
( n=O 

751 
Section 9.6 
Implementation of the DFT Using Convolution 
Figure 9.28 
Block diagram of chirp 
transform algorithm. 
In preparation for interpreting Eq. (9.46) as the output of a linear time-invariant system, 
we obtain more familiar notation by replacing k by nand n by k in Eq. (9.46): 
n2
X(e jwn ) 
W 
/ 2 (£ g[k1W-(Il-k)2/2) , 
n 
0,1, ... , M 
1. 
(9.47) 
k=O 
In the form of Eq. (9.47), X (ejwn ) corresponds to the convolution of the sequence 
/ 2 
2
g[n] with the sequence W-n2 
, followed by multiplication by the sequence Wn2/ . The 
output sequence, indexed on the independent variable n, is the sequence of frequency 
samples X (e jwn ). With this interpretation, the computation ofEq. (9.47) is as depicted in 
Figure 9.28. The sequence W- n2 /2 can be thought of as a complex exponential sequence 
with linearly increasing frequency n6w. In radar systems, such signals are called chirp 
signals-hence the name chirp transform. A system similar to Figure 9.28 is commonly 
used in radar and sonar signal processing for pulse compression (Skolnik, 2002). 
For the evaluation of the Fourier transform samples specified in Eq. (9.47), we 
need only compute the output of the system in Figure 9.28 over a finite interval. In 
2
2
Figure 9.29, we depict illustrations of the sequences g[n], W-n1/ , and g[n] * W- n2 / . 
Since g [nJ is of finite duration, only a finite portion of the sequence W -n
2/2 is used in 
nl
obtaining g [n] * W-
l2 over the interval n 
0, 1, ... , M - 1, specifically, that portion 
from n = -(N -1) to n 
M - 1. Let us define 
W- n2 2
/ 
-(N 
1) 
nsM-1,
h[n] == 
' 
(9.48)
{ 0, 
otherwise, 
as illustrated in Figure 9.30. It is easily verified by considering the graphical represen­
tation of the process of convolution that 
2
g[n] * W-
n1 
/ 
g[n] * h[n], 
n 
O,l, ... ,M 
1. 
(9.49) 
Consequently, the infinite-duration impulse response W-n1 / 2 in the system ofFigure 9.28 
can be replaced by the finite-duration impulse response of Figure 9.30. The system is 
now as indicated in Figure 9.31, where h[n] is specified by Eq. (9.48) and the frequency 
samples are given by 
n 
0,1, ... , M - 1. 
(9.50) 
Evaluation of frequency samples using the procedure indicated in Figure 9.31 has 
a number of potential advantages. In general, we do not require N = M as in the FFf 
algorithms, and neither N nor M need be composite numbers. In fact, they may be 
prime numbers if desired. Furthermore, the parameter (.vo is arbitrary. This increased 
flexibility over the FFf does not preclude efficient computation, since the convolution 
in Figure 9.31 can be implemented efficiently using an FFf algorithm with the technique 

• • • • • • • • • • • • • 
• • • • • 
• • • • 
II IIIIII1I. 
gIn] 
n 
(N-l)
(a) 
w-n'i2 
o 
n 
(b) 
gIn] * W-n'i2 
II I II1 I I1I!I1III 1I I 1III I I 
n 
(M-1)
(c) 
Figure 9.29 
An illustration of the sequences used in the chirp transform  
algorithm. Note that the actual sequences involved are complex valued.  
. 
22 
22 
22
(a) gIn] 
x[n]e-;WOn wn / . (b) W-n / . (c) gIn] * W- n / . 
h [n]
I [ III [ IIIIIII [ I . . . . . . . . . . . " 
-(N -1) 
(M 1) 
Figure 9.30 
An illustration of the region of support for the FIR chirp filter. Note  
that the actual values of h[n] as given by Eq. (9.48) are complex.  
gIn] 
e-iwonwn2i2 
h[n] 
Figure 9.31 
Block diagram of chirp 
transform system for finite-length 
wn2J2 
impulse response. 
752 

Section 9.6 
Implementation of the OFT Using Convolution 
753 
of Section 8.7 to compute the convolution. As discussed in that section, the FFT size 
must be greater than or equal to (M + N 
1) in order that the circular convolution will 
be equal to g [n] *h[n] for 0 :S n :S M 
1. The FFT size is otherwise arbitrary and can, for 
example, be chosen to be a power of 2. It is interesting to note that the FIT algorithms 
used to compute the convolution implied by the CIA could be of the Winograd type. 
These algorithms themselves use convolution to implement the DIT computation. 
In the system of Figure 9.31 h[ll] is noncausal, and for certain real-time implemen­
tations it must be modified to obtain a causal system. Since h[n] is of finite duration, 
this modification is easily accomplished by delaying h[n] by (N 
1) to obtain a causal 
impulse response: 
w-(n-N+lh2, 
n = 0, 1, ... , M + N - 2,
hdn] 
(9.51)
{ 0, 
otherwise. 
Since both the chirp demodulation factor at the output and the output signal are also 
delayed by (N - 1) samples, the Fourier transform values are 
X(ejwll ) = Yl[n + N 
1], 
n 
0,1, ... , M 
1. 
(9.52) 
Modifying the system of Figure 9.31 to obtain a causal system results in the system 
ofFigure 9.32. An advantage of this system stems from the fact that it involves the convo­
lution of the input signal (modulated with a chirp) with a fixed, causal impulse response. 
Certain technologies, such as charge-coupled devices (CCD) and surface acoustic wave 
(SAW) devices, are particularly useful for implementing convolution with a fixed, pre­
specified impulse response. These devices can be used to implement FIR filters, with the 
filter impulse response being specified at the time of fabrication by a geometric pattern 
of electrodes. A similar approach was followed by Hewes, Broderson and Buss (1979) 
in implementing the CfA with CCDs. 
Further simplification of the CfA results when the frequency samples to be com­
j2rr/ N
putedcorrespond to the DIT, i.e., when wo = 0 and W = e-
, so that Wn = 27m/N. 
In this case, it is convenient to modify the system of Figure 9.32. Specifically, with wo = 0 
and W 
e-j2;r/N = WN, consider applying an additional unit of delay to the impulse 
response ·in Figure 9.32. With N even, w!1 = ej2;r = 1, so 
W-(n-N)2/2 
W-n2 / 2 
N 
N 
(9.53) 
Therefore, the system now is as shown in Figure 9.33, where 
Jl2ln]=IWNn2/2, 
n 
1,~, ... ,M+N-1, 
(9.54)
0, 
otherWise. 
In this case, the chirp signal modulating x[n] and the chirp signal modulating the output 
of the FIR filter are identical, and 
X(ej2;rnI N) = Y2[n + N], 
11 = 0,1, ... , M - 1. 
(9.55) 
Figure 9.32 
Block diagram of chirp 
transform system for causal finite-length 
impulse response. 

754 
Chapter 9 
Computation of the Discrete Fourier Transform 
h2[nJ
g[n] 
Figure 9.33 
Block diagram of chirp 
transform system for obtaining OFT
W n2i2 
w(n 
N)lJ2 = Wn2i2  
N 
N 
N 
samples.  
Example 9.' 
Chirp Transform Parameters 
Suppose we have a finite-length sequence x[n] that is nonzero only on the interval 
n 
0, ... ,25, and we wish to compute 16 samples of the DTFf X(eJW ) at the fre­
quencies Wk 
2n/27 + 2nk/1024 for k = 0, ... , 15. We can compute the desired 
frequency samples through convolution with a causal impulse response using the sys­
tem in Figure 9.32 with an appropriate choice of parameters. We set M = 16, the 
number of samples desired, and N = 26, the length of the sequence. The frequency of 
the initial sample, WO, is 2n/27, while the interval between adjacent frequency samples, 
~w, is 2n/1024. With these choices for the parameters, we know from Eq. (9.41) that 
W 
e- jD.w, and so the causal impulse response we desire is from Eq. (9.51) 
j2n"/1024]-(n-25)2/2 
n - 0 
40
[e­
hdn] = 
' 
-,.:., 
, 
{ O. 
otherWise. 
For this causal impulse response, the output Yl [n] will be the desired frequency samples 
beginning at Yl [25], i.e., 
Yl[n + 25] = X(eJwn)lwn=2rr/27+2rrll/l024, 
n = 0, ... ,15. 
An algorithm similar to the CTA was first proposed by Bluestein (1970), who 
showed that a recursive realization of Figure 9.32 can be obtained for the case 
llw = 2nI N, N a perfect square. (See Problem 9.48.) Rabiner, Schafer and Rader 
(1969) generalized this algorithm to obtain samples of the z-transform equally spaced 
in angle on a spiral contour in the z-plane. This more general form of the CTA was 
called the chirp z-transform (CZT) algorithm. The algorithm that we have called the 
CTA is a special case of the CZT algorithm. 
9.7 EFFECTS OF FINITE REGISTER LENGTH 
Since the fast Fourier transform algorithm is widely used for digital filtering and spec­
trum analysis, it is important to understand the effects of finite register length in the 
computation. As in the case of digital filters, a precise analysis of the effects is difficult. 
However, a simplified analysis is often sufficient for the purpose of choosing the required 
register length. The analysis that we will present is similar in style to that carried out 
in Section 6.9. Specifically, we analyze arithmetic round-off by means of a linear-noise 
model obtained by inserting an additive noise source at each point in the computation 
algorithm where round-off occurs. Furthermore, we will make a number of assumptions 
to simplify the analysis. The results that we obtain lead to several simplified, but useful, 
estimates of the effect of arithmetic round-off. Although the analysis is for rounding, it 
is generally easy to modify the results for truncation. 

755 
Section 9.7 
Effects of Finite Register Length 
x[0] 
x[4] 
x[2] 
x[6] 
xlI] 
x[5] 
x[3] 
x[7] o---<:J"'----"O--+--cf----+-c,o---o----+_-b X[7] 
Figure 9.34 
Flow graph for 
-1 
i 
decimation-in-time FFT algorithm. 
We have seen several different algorithmic structures for the FFf. However, the 
effects of round-off noise are very similar among the different classes of algorithms. 
Therefore, even though we consider only the radix-2 decimation-in-time algorithm, our 
results are representative of other forms as well. 
The flow graph depicting a decimation-in-time algorithm for N = 8 was shown 
in Figure 9.11 and is reproduced in Figure 9.34. Some key aspects of this diagram are 
common to all standard radix-2 algorithms. The DFf is computed in v = log2 N stages. 
At each stage a new array of N numbers is formed from the previous array by linear 
combinations of the elements, taken two at a time. The vth array contains the desired 
DFf. For radix-2 decimation-in-time algorithms, the basic 2-point DFf computation is 
of the form 
Xm[P] = Xm-l[P] + W,vXm-l[q], 
(9.56a) 
Xm[q] = X m -l[P] -
WNX m-l[q). 
(9.56b) 
Here the subscripts m and (m 
1) refer to the mth array and the (m - 1)5t array, 
respectively, and P and q denote the location of the numbers in each array. (Note that 
m = 0 refers to the input array and m = v refers to the output array.) A flow graph 
representing the butterfly computation is shown in Figure 9.35. 
At each stage, N /2 separate butterfly computations are carried out to produce the 
next array. The integer r varies with P, q, and m in a manner that depends on the specific 
form of the FFf algorithm used. However, our analysis is not tied to the specific way 
Figure 9,35 Butterfly computation for 
decimation-in-time. 

756 
Chapter 9 
Computation of the Discrete Fourier Transform 
X m- 1lP) 0 
0::::  
:;PXmlP) 
Figure 9.36 
Linear-noise model for 
Wk 
fixed-point round-off noise in a
X m_1[q]  
-1 
Xm[q] 
decimation-in-time butterfly 
€[m,q) 
computation. 
in which r varies. Also, the specific relationship among p, q, and m, which determines 
how we index through the mth array, is not important for the analysis. The details of 
the analysis for decimation in time and decimation in frequency differ somewhat due 
to the different butterfly forms, but the basic results do not change significantly. In our 
analysis we assume a butterfly of the form of Eqs. (9.56a) and (9.56b), corresponding 
to decimation in time. 
. 
We model the round-off noise by associating an additive noise generator with each 
fixed-point mUltiplication. With this model, the butterfly of Figure 9.35 is replaced by 
that of Figure 9.36 for analyzing the round-off noise effects. The notation s[m, q] repre­
sents the complex-valued error introduced in computing the mth array from the (m 1)st 
array; specifically, it indicates the error resulting from quantization of multiplication of 
the qth element of the (m - 1)8t array by a complex coefficient. 
Since we assume that, in general, the input to the FFf is a complex sequence, 
each of the multiplications is complex and thus consists of four real multiplications. We 
assume that the errors due to each real multiplication have the following properties: 
1.  The errors are uniformly distributed random variables over the range -(1/2). 2-B 
to (1/2) . 2-B , where, as defined in Section 6.7.1, numbers are represented as 
(B + I)-bit signed fractions. Therefore, each error source has variance 2-2B /12. 
2.  The errors are uncorrelated with one another. 
3.  All the errors are uncorrelated with the input and, consequently, also with the 
output. 
Since each of the four noise sequences is uncorrelated zero-mean white noise and all 
have the same variance, 
2-2B 
£{le[m,q]\2} =4· 12 
=~.2-2B=a~.  
(9.57) 
To determine the mean-square value of the output noise at any output node, we must 
account for the contribution from each of the noise sources that propagate to that node. 
We can make the following observations from the flow graph of Figure 9.34: 
1.  The transmission function from any node in the flow graph to any other node to 
which it is connected is multiplication by a complex constant of unity magnitude 
(because each branch transmittance is either unity or an integer power of WN). 
2.  Each output node connects to seven butterflies in the flow graph. (In general, each 
output node would connect to (N 
1) butterflies.) For example, Figure 9.37(a) 

o-----+-----<~-_iJ_--__<l_--_p_-_<r_-+-_QX[O] 
-1 
(a) 
o-------Q~___p_--o----""""O­____<l_-__o X[2] 
-1 
Figure 9.37 
(a) Butterflies that affect 
(b) 
X[O]; (b) butterflies that affect X[2]. 
757 

758 
Chapter 9 
Computation of the Discrete Fourier Transform 
shows the flow graph with all the butterflies removed that do not connect to X[O), 
and Figure 9.37(b) shows the flow graph with all the butterflies removed that do 
not connect to X[2]. 
These observations can be generalized to the case of N an arbitrary power of 2. 
As a consequence of the first observation, the mean-square value of the magnitude 
of the component of the output noise due to each elemental noise source is the same and 
equal to oJ The total output noise at each output node is equal to the sum of the noise 
propagated to that node. Since we assume that all the noise sources are uncorrelated, 
the mean-square value of the magnitude of the output noise is equal to a~ times the 
number of noise sources that propagate to that node. At most one complex noise source 
is introduced at each butterfly; consequently, from observation 2, at most (N -1) noise 
sources propagate to each output node. In fact, not all the butterflies generate round-off 
noise, since some (for example, all those in the first and second stages for N 
8) involve 
only multiplication by unity. However, if for simplicity we assume that round-off occurs 
for each butterfly, we can consider the result as an upper bound on the output noise. 
With this assumption, then, the mean square value of the output noise in the kth DIT 
value, F[k], is given by 
[{IF[k]1 2 } = (N -
1)a~, 
(9.58) 
which, for large N, we approximate as 
[{jF[k)12} ~ Na~. 
(9.59) 
According to this result, the mean-square value of the output noise is proportional to 
N, the number of points transformed. The effect of doubling N, or adding another stage 
in the FIT, is to double the mean-square value of the output noise. In Problem 9.52, we 
consider the modification of this result when we do not insert noise sources for those 
butterflies that involve only multiplication by unity or j. Note that for FIT algorithms, 
a double-length accumulator does not help us reduce round-off noise, since the outputs 
of the butterfly computation must be stored in (B +1)-bit registers at the output of each 
stage. 
In implementing an FIT algorithm with fixed-point arithmetic, we must ensure 
against overflow. From Eqs. (9.56a) and (9.56b), it follows that 
max(IXm_r[p]l, IXm-dq]l) :::: max(IXm[p]l, IXm[q]l) 
(9.60) 
and also 
max(IXm[p]l, IXm[q]l) :::: 2max(lXm-l[P]I.IXm_dq]l)· 
(9.61) 
(See Problem 9.51.) Equation (9.60) implies that the maximum magnitude is non­
decreasing from stage to stage. If the magnitude of the output of the FFT is less than 
unity, then the magnitude of the points in each array must be less than unity, i.e., there 
will be no overflow in any of the arrays.ll 
To express this constraint as a bound on the input sequence, we note that the 
condition 
1 
Ix[n]1 < N' 
0:::: n:::: N -1, 
(9.62) 
11 Actually, one should discuss overflow in terms of the real and imaginary parts of the data rather 
than the magnitude. However, Ixl < 1 implies that 1R.e{xll < 1 and IIm1x11 < 1, and only a slight increase in 
allowable signal level is achieved by scaling on the basis of real and imaginary parts. 

Section 9.7
n 
], 
o 
e 
d 
e 
i, 
e 
e 
e 
ff 
T 
/) 
:0 
~e 
'e 
.e 
s, 
ts 
:h 
») 
l) 
1­
~e 
Ie 
~) 
er 
in 
Effects of Finite Register Length 
759 
is both necessary and sufficient to guarantee that 
IX[kJI < 1, 
O:sk:sN 
L 
(9.63) 
This follows from the definition of the DFT, since 
N-l 
N-lL Ix[n]1 
k =0, 1, ... N 
L 
(9.64)
IX[k]1 = L x [n]wtn 
n=O 
n=O 
Thus, Eq. (9.62) is sufficient to guarantee that there will be no overflow for all stages of 
the algorithm. 
To obtain an explicit expression for the noise-to-signal ratio at the output of the 
FIT algorithm, consider an input in which successive sequence values are uncorrelated, 
i.e., a white-noise input signal. Also, assume that the real and imaginary parts of the 
input sequence are uncorrelated and that each has an amplitude density that is uniform 
between -l/(.JiN) and +l/(.JiN). (Note that this signal satisfies Eq. (9.62).) Then 
the average squared magnitude of the complex input sequence is 
(9.65) 
The DIT of the input sequence is 
N-l 
X[k] = L x[n]Wkn , 
(9.66) 
n=O 
from which it can be shown that, under the foregoing assumptions on the input, 
N-l 
£{IX[k]12} = L £{lx[n]12}IWkn 12 
n=O 
(9.67) 
2 
1 
= NO'x = 3N' 
Combining Eqs. (9.59) and (9.67). we obtain 
(9.68) 
According to Eq. (9.68), the noise-to-signal ratio increases as N2, or 1 bit per stage. 
That is, if N is doubled, corresponding to adding one additional stage to the FFf, then 
to maintain the same noise-to-signal ratio, 1 bit must be added to the register length. 
The assumption of a white-noise input signal is, in fact, not critical here. For a variety of 
other inputs, the noise-to-signal ratio is still proportional to N 2, with only the constant 
of proportionality changing. 

760 
Chapter 9 
Computation of the Discrete Fourier Transform 
1 
Xm_1[P] 0 
• 
2 
6:: 
pXm[p1 
Xm Ifq] 
Xm [q] 
Figure 9.38 Butterfly showing scaling 
multipliers and associated fixed-point 
E[m, q] 
round-off noise. 
Equation (9.61) suggests an alternative scaling procedure. Since the maximum 
magnitude increases by no more than a factor of 2 from stage to stage, we can frevent 
overflow by requiring that Ix[nll < 1 and incorporating an attenuation of '1 at the 
input to each stage. In this case, the output will consist of the DFr scaled by 1/N. 
Although the mean-square output signal will be 1/N times what it would be if no 
scaling were introduced, the input amplitude can be N times larger without causing 
overflow. For the white-noise input signal, this means that we can assume that the real 
and imaginary parts are uniformly distributed from -1/v2 to 1/v2, so that Ix[n]1 < 1. 
Thus, with the v divisions by 2, the maximum expected value of the magnitude squared 
of the DFr that can be attained (for the white input signal) is the same as that given in 
Eq. (9.67). However, the output noise level will be much less than in Eq. (9.59), since 
the noise introduced at early stages of the FFr will be attenuated by the scaling that 
takes place in the later arrays. Specifically, with scaling by 1/2 introduced at the input 
to each butterfly, we modify the butterfly of Figure 9.36 to that of Figure 9.38, where, 
in particular, two noise sources are now associated with each butterfly. As before, we 
assume that the real and imaginary parts of these noise sources are uncorrelated and 
are also uncorrelated with the other noise sources and that the real and imaginary parts 
are uniformly distributed between ±(1/2) . 2- B • Thus, as before, 
t'{te[m, q]12} = aj = ~ .2-2B = t'{I€[m, p]e}. 
(9.69) 
Because the noise sources are all uncorrelated, the mean-squared magnitUde of the 
noise at each output node is again the sum of the mean-squared contributions of each 
noise source in the flow graph. However, unlike the previous case, the attenuation that 
each noise source experiences through the flow graph depends on the array at which it 
originates. A noise source originating at the mth array will propagate to the output with 
multiplication by a complex constant with magnitude (1/2y-m-l. By examination of 
Figure 9.34, we see that for the case N = 8, each output node connects to: 
1 butterfly originating at the (v - l)st array, 
2 butterflies originating at the (v - 2)nd array, 
4 butterflies originating at the (v - 3)fd array, etc. 

761 
Section 9.7 
Effects of Finite Register Length 
For the general case with N 
2v , each output node connects to 2,,-m-l butterflies 
and therefore to 2,,-m noise sources that originate at the mth array. Thus, at each output 
node, the mean-square magnitude of the noise is 
v-I 
2v- m
E{IF[k]12} = o"~ L 
. (0.5)2v-2m-2 
m=O 
v-I 
= o"~ L(0.5)v-m-2 
m=O 
(9.70) 
v-I 
= O"~.2 LO.5k 
k=O 
v 
= 2~21 0.5 
4 2 1 
05")
vH1_0.5 
O"B(-·· 
For large N, we assume that OS (i.e., 1/N) is negligible compared with unity, so 
(9.71)  
which is much less than the noise variance resulting when all the scaling is carried out 
on the input data. 
Now we can combine Eq. (9.71) with Eq. (9.67) to obtain the output noise-to-signal 
ratio for the case of step-by-step scaling and white input. We obtain 
(9.72)  
a result proportional to N rather than to N 2 • An interpretation of Eq. (9.72) is that the 
output noise-to-signal ratio increases as N, corresponding to half a bit per stage, a result 
first obtained by Welch (1969). It is important to note again that the assumption of a 
white-noise signal is not essential in the analysis. The basic result of an increase of half 
a bit per stage holds for a broad class of signals, with only the constant multiplier in 
Eq. (9.72) being dependent on the signal. 
We should also note that the dominant factor that causes the increase of the noise­
to-signal ratio with N is the decrease in signal level (required by the overflow constraint) 
as we pass from stage to stage. According to Eq. (9.71), very little noise (only a bit or 
two) is present in the final array_ Most of the noise has been shifted out of the binary 
word by the scalings. 
We have assumed straight fixed-point computation in the preceding discussion; 
i.e., only preset attenuations were allowed, and we were not permitted to rescale on 
the basis of an overflow test. Clearly, if the hardware or programming facility is such 

762 
Chapter 9 
Computation of the Discrete Fourier Transform 
that straight fixed-point computation must be used, we should, if possible, incorporate 
attenuators of 1/2 at each array rather than use a large attenuation of the input array. 
A third approach to avoiding overflow is the use of block floating point. In this 
procedure the original array is normalized to the far left of the computer word, with the 
restriction that Ix [n JI < 1; the computation proceeds in a fixed-point manner, except 
that after every addition there is an overflow test. If overflow is detected, the entire 
array is divided by 2 and the computation continues. The number of necessary divisions 
by 2 are counted to determine a scale factor for the entire final array. The output noise­
to-signal ratio depends strongly on how many overflows occur and at what stages of the 
computation they occur. The positions and timing of overflows are determined by the 
signal being transformed; thus, to analyze the noise-to-signal ratio in a block floating­
point implementation of the FFT, we would need to know the input signal. 
The preceding analysis shows that scaling to avoid overflow is the dominant factor 
in determining the noise-to-signal ratio of fixed-point implementations of FFT algo­
rithms. Therefore, floating-point arithmetic should improve the performance of these 
algorithms. The effect offloating-point round-off on the FFT was analyzed both theoret­
ically and experimentally by Gentleman and Sande (1966), Weinstein and Oppenheim 
(1969), and Kaneko and Liu (1970). These investigations show that, since scaling is no 
longer necessary, the decrease of noise-to-signal ratio with increasing N is much less 
dramatic than for fixed-point arithmetic. 
For example, Weinstein (1969) showed theoretically that the noise-to-signal ratio 
is proportional to v for N = 2v , rather than proportional to N as in the fixed-point case. 
Therefore, quadrupling \! (raising N to the fourth power) increases the noise-to-signal 
ratio by only 1 bit. 
9.8 SUMMARY 
In this chapter we have considered techniques for computation of the discrete Fourier 
transform, and we have seen how the periodicity and symmetry of the complex factor 
e-i (2rrIN)kn can be exploited to increase the efficiency of OFT computations. 
We considered the Goertzel algorithm and the direct evaluation of the OFT ex­
pression because of the importance of these techniques when not all N of the OFT 
values are required. However, our major emphasis was on fast Fourier transform (FFT) 
algorithms. We described the decimation-in-time and decimation-in-frequency classes 
of FFT algorithms in some detail and some of the implementation considerations, such 
as indexing and coefficient quantization. Much of the detailed discussion concerned al­
gorithms that require N to be a power of2, since these algorithms are easy to understand, 
simple to program, and most often used. 
The use of convolution as the basis for computing the OFT was briefly discussed. 
We presented a brief overview of the Winograd Fourier transform algorithm, and in 
somewhat more detail we discussed an algorithm called the chirp transform algorithm. 
The final section of the chapter discussed effects of finite word length in OFT 
computations. We used linear-noise models to show that the noise-to-signal ratio of a 
OFT computation varies differently with the length of the sequence, depending on how 
scaling is done. We also commented briefly on the use offloating-point representations. 

763 
Chapter 9 
Problems 
Problems 
Basic Problems with Answers 
9.1.  Suppose that a computer program is available for computing the DFf 
N-l 
X[k] L x[n]e-J(2rr j N)kn, 
k = 0, 1, ... , N -1: 
n=O 
i.e., the input to the program is the sequence x[n] and the output is the DFf X[kJ. Show 
how thc input and/or output sequences may be rearranged such that the program can also 
be used to compute the inverse DFf 
N-l 
x[n] 
~L X[k]e j (2rr/N)kn, 
n 
0,1, ... , N 
1; 
k=O 
i.e., the input to the program should be X[k] or a sequence simply related to X[k], and the 
output should be either x[n] or a sequence simply related tox[n). Thcre are scveral possible 
approaches. 
9.2.  Computing the DFf generally requires complex multiplications. Consider the product 
X + jY = (A + jB)(C + jD) = (AC - BD) + j(BC + AD). In this form, a complex 
mUltiplication requires four real multiplications and two real additions. Verify that a com­
plex multiplication can be performed with three real multiplications and five additions using 
the algorithm 
X 
(A - B)D + (C - D)A, 
Y = (A 
B)D + (C + D)B. 
9.3.  Suppose that you time-reverse and delay a real-valued 32-point sequence x[n] to obtain 
xl[n] = x[32-n). Ifxl[n) is used as the input for the system in Figure P9.4, find an expression 
for y[32] in terms of X(eju», the DTFT of the original sequence x[n). 
9.4.  Consider the system shown in Figure P9.4. If the input to the system, x[n], is a 32-point 
sequence in the interval 0:::: n :::: 31, the output y[n] at n 
32 is equal to X (eJu» evaluated 
at a specific frequency Wk. What is Wk for the coefficients shown in Figure P9.4? 
x[nl  
y[n] 
(141T)
Zeos 3z 
-1  
Figure P9.4 

764  
Chapter 9 
Computation of the Discrete Fourier Transform 
9.5.  Consider the signal flow graph in Figure P9.S. Suppose that the input to the system x[n] is 
an 8-point sequence. Choose the values of a and b such that y[8] = X(ej6Jrj8). 
yIn]
x[n] 
Z-l 
a 
b 
z-J 
-1 
Figure P9.5 
9.6.  Figure P9.6 shows the graph representation of a decimation-in-time FFf algorithm for 
N 
8. The heavy line shows a path from sample x[7] to DFf sample X[2]. 
x[O] a 
, 
::;A 
0;;: 
• 
P 
q 
'pX[O] 
x[4] a 
r:r= 
="0 
0;;: 
)(.:p 
q 
\ 
/ • pX[l] 
x [2] a 
00:;;:: 
::;A 
• 
if 
)(', '0 
q 
\ 
X /. p X[21
)0 
w2 
x [6] a 
0""" 
"0 
/,1 
cf 
" "0 
q X X X' pX[3] 
x[I] a 
CI;:: 
:;;0 
0: 
P 
0 
'i 
'I Y. b 
x[5] a 
Cf'" 
="0 
0: 
)( 
pol Y \. b 
x[7] c 
r:r 
="0 
r:f 
• '0 
d 
• b 
(a)  What is the "gain" along the path that is emphasized in Figure P9.6? 
(b)  How many other paths in the flow graph begin at x[7] and end at X[2]? Is this true 
in general? That is, how many paths are there between each input sample and each 
output sample? 
(c)  Now consider the DFf sample X[2]. By tracing paths in the flow graph of Figure P9.6, 
show that each input sample contributes the proper amount to the output DFfsample; 
i.e., verify that 
N-l 
X[2] L x[n]e- j (2:rr j N)2n. 
n=O 

765 
Chapter 9 
Problems 
9.7. Figure P9.7 shows the flow graph for an 8-point decimation-in-time FFT algorithm. Let x[n] 
be the sequence whose DFT is X[k]. In the flow graph, A[·], B[·], C[.], and D[·] represent 
separate arrays that are indexed consecutively in the same order as the indicated nodes. 
(a)  Specify how the elements of the sequence x[n] should be placed in the array A[r], 
r = 0, 1, ... , 7. Also, specify how the elements of the DFT sequence should be ex­
tracted from the array D[r], r 
0,1, ... ,7. 
(b)  Without determining the values in the intermediate arrays, B[·] and C[.], determine and 
sketch the array sequence D[r]. r 
0, 1. ... ,7, if the input sequence is 
x[n] = (-WN)n,n 
0,1, ... ,7. 
(c)  Determine and sketch the sequence C[r], r == 0, 1, ... ,7, if the output Fourier trans­
form is X[k] = 1, k == 0, 1, .. ,7. 
B[O]  
C[O] 
A [0] o--__-Gc--_----:;o--_+_--Q:---_-P---l_-Q-----+-Q D [0] 
w,~ 
B[l] 
A [1] o-----<::F---_l---=:;,Q-----Q:--*---P---l--Q---\----,~ D [1] 
A [2] 
wZ 
A[3] o-----<::F---_l---=:;,Q-----(j'------4>----l--Q-4----*--l~D [3] 
A [4] 
A[5] 
A [6] 
A [7] 
wZ 
wZ 
Figure P9.7 
9.S. In implementing an FFf algorithm, it is sometimes useful to generate the powers of WN 
with a recursive difference equation, or oscillator. In this problem we consider a radix-2 
decimation-in-time algorithm for N 
211. Figure 9.11 depicts this type of algorithm for 
N = 8. To generate the coefficients efficiently, the frequency of the oscillator would change 
from stage to stage. 
Assume that the arrays are numbered °through v = log2 N, so the array holding the 
initial input sequence is the zeroth array and the DFT is in the vth array. In computing the 
butterflies in a given stage, all butterflies requiring the same coefficients WAr are evaluated 
before obtaining new coefficients. In indexing through the array, we assume that the data in 
the array are stored in consecutive complex registers numbered °through (N 
1). All the 
following questions are concerned with the computation of the mth array from the (m _1)st 
array, where 1 :=; m 
v. Answers should be expressed in terms of m. 

766  
Chapter 9 
Computation of the Discrete Fourier Transform 
(a)  How many butterflies must be computed in the mth stage? How many different coef­
ficients are required in the mth stage? 
(b)  Write a difference equation whose impulse response h[n] contains the coefficients WN 
required by the butterflies in the mth stage. 
(c)  The difference equation from part (b) should have the form of an oscillator, i.e., h[n] 
should be periodic for n ;::: O. What is the period of h[n]? Based on this, write an 
expression for the frequency of this oscillator as a function of m. 
9.9. Consider the butterfly in Figure P9.9. This butterfly was extracted from a signal flow graph 
implementing an FFf algorithm. Choose the most accurate statement from the following 
list: 
L  The butterfly was extracted from a decimation-in-time FFf algorithm. 
2.  The butterfly was extracted from a decimation-in-frequency FFf algorithm. 
3.  It is not possible to say from the figure which kind of FFf algorithm the butterfly 
came from. 
W~ 
-1 
Figure P9.9 
9.10. A finite-length signal x[n) is nonzero in the interval 0:::: n :::: 19. This signal is the input to 
the system shown in Figure P9.10, where 
h[n] = {ei (27f/21)(n-19)2/2 
n 
0,1, ... ,28, 
0, 
' 
otherwise. 
W = e-i (27f/21) 
The output of the system, yIn], for the interval n = 19, ... ,28 can be expressed in terms of 
the DTFf X(ei(V) for appropriate values of w. Write an expression for yIn] in this interval 
in terms of X(eiw). 
x[n) 
yIn] 
e-j (27TI7)nwn212 
w(n-19)212 
Figure P9.10 
9.11. The butterfly flow graph in Figure 9.10 can be used to compute the DFf of a sequence of 
length N = 2v "in-place," i.e., using a single array of complex-valued registers. Assume this 
array of registers A[l] is indexed on 0 ::; l :::: N - 1. The input sequence is initially stored 
in A[l] in bit-reversed order. The array is then processed by v stages of butterflies. Each 
butterfly takes two array elements A[tol and A[ld as inputs, then stores its outputs into 

767 
Chapter 9 
Problems 
those same array locations. The values of eo and el depend on the stage number and the 
location of the butterfly in the signal flow graph. The stages of the computation are indexed 
bym 
1, ... , l!. 
(8)  What is lel 
eol as a function of the stage number m? 
(b)  Many stages contain butterflies with the same "twiddle" factor WN.For these stages, 
how far apart are the values of eo for the butterflies with the same WN? 
9.U.  Consider the system shown in Figure P9.12, with 
ej (2rr/1O)(n-ll)2/2 
n  
0,1, ... ,15,
h~]=  
' 
{ 0,  
otherwise. 
It is desired that the output of the system, y[n + 11] = X(e jwn ), where Wn = (2n'/19) + 
n(2.n:JlO) for n = 0, ... ,4. Give the correct value for the sequence r[n] in Figure P9.12 such 
that the output y(n] provides the desired samples of the DTFT. 
x[n] 
y[n] 
r[n] 
Figure P9.12 
9.13.  Assume that you wish to sort a sequence x[n] of length N = 16 into bit-reversed order for 
input to an FFT algorithm. Give the new sample order for the bit-reversed sequence. 
9.14.  For the following statement, assume that the sequence x[n] has length N = 2v and that 
X[k] is the N -point DFT of x[n]. Indicate whether the statement is true or false, and justify 
your answer. 
Statement: It is impossible to construct a signal flow graph to compute X[k] from x[n1 
such that both x[n] and X[k] are in normal sequential (not bit-reversed) order. 
9.15.  The butterfly in Figure P9.15 was taken from a decimation-in-frequency FFT with N = 16. 
where the input sequence was arranged in normal order. Note that a 16-point FFT will have 
four stages, indexed m 
1..... 4. Which of the four stages have butterflies of this form? 
Justify your answer. 
Figure P9.15 

768  
Chapter 9 
Computation of the Discrete Fourier Transform 
9.16.  The butterfly in Figure P9.16 was taken from a decimation-in-time FFT with N = 16. 
Assume that the four stages of the signal flow graph are indexed by m = 1, .... 4. What are 
the possible values of r for each of the four stages? 
-1
W16  
Figure P9.16 
2v
9.17.  Suppose you have two programs for computing the DFT of a sequencex[n] that has N = 
nonzero samples. Program A computes the DFT by directly implementing the definition 
of the DFT sum from Eq. (8.67) and takes N 2 seconds to run. Program B implements 
the decimation-in-time FFT algorithm and takes ION log2 N seconds to run. What is the 
shortest sequence N such that Program B runs faster than Program A? 
9.18.  The butterfly in Figure P9.18 was taken from a decimation-in-time FFT with N = 16. 
Assume that the four stages of the signal flow graph are indexed by m = 1, ... ,4. Which 
of the four stages have butterflies of this form? 
Wf6 
-1 
Figure P9.18 
9.19. Suppose you are told that an N = 32 FFT algorithm has a "twiddle" factor of Wl2 for one 
of the butterflies in its fifth (last) stage. Is the FFT a decimation-in-time or decimation-in­
frequency algorithm? 
9.20.  Suppose you have a signal x [n] with 1021 nonzero samples whose DTFT you wish to estimate 
by computing the DFT. You find that it takes your computer 100 seconds to compute the 
1021-point DFT of x[n]. You then add three zero-valued samples at the end of the sequence 
to form a l024-point sequence Xl [n]. The same program on your computer requires only 
1 second to compute X Ilk]. Reflecting, you realize that by using Xl [n], you are able to 
compute more samples of X (eiw) in a much shorter time by adding some zeros to the 
end of x[n] and pretending that the sequence is longer. How do you explain this apparent 
paradox? 
Basic Problems 
9.21.  In Section 9.1.2, we used the fact that WN'kN 
I to derive a recurrence algorithm for 
computing a specific DFT value X[k] for a finite-length sequence x[n]. n = 0, 1, ... ,N 
1. 
(a)  Using the fact that W'f! = wfJn 
1, show that X[N -k] can be obtained as the output 
after N iterations of the difference equation depicted in Figure P9.21-1. That is, show 
that 
X[N - k] 
Yk[N]. 

769 
Chapter 9 
Problems 
W~ 
Figure PQ.21-1 
(b)  Show that X[N - k] is also equal to the output after N iterations of the difference 
equation depicted in Figure P9.21-2. Note that the system of Figure P9.21-2 has the 
same poles as the system in Figure 9.2, but the coefficient required to implement 
the complex zero in Figure P9.21-2 is the complex conjugate of the corresponding 
coefficient in Figure 9.2; i.e., WN
k 
(W~)*. 
x[n] 
271'k\ 
2eos (NJ 
-1  
Figure pg,21-2 
9.22.  Consider the system shown in Figure P9.22. The subsystem from x[n] to y[nl is a causal, 
LTI system implementing the difference equation 
y[n] = x[n] + ay[n 
1].  
x[nl is a finite length sequence of length 90, i.e.,  
x[nJ = 0 for n < 0 and n > 89.  
~Y[M] 
a  
Figure pg.22 
Determine a choice for the complex constant a and a choice for the sampling instant 
M so that 
y[MJ = X(ejW)1 w=2n/60. 
9.23.  Construct a flow graph for a 16-point radix-2 decimation-in-time FFf algorithm. Label all 
multipliers in terms of powers of W16, and also label any branch transmittances that are 
equal to -1. Label the input and output nodes with the appropriate values of the input 
and DFf sequences, respectively. Determine the number of real multiplications and the 
number of real additions required to implement the flow graph. 

770 
Chapter 9 
Computation of the Discrete Fourier Transform 
9.24.  It is suggested that if you have an FFT subroutine for computing a length-N DFT, the 
inverse DFT of an N -point sequence X[k] can be implemented using this subroutine as 
follows: 
1. Swap the real and imaginary parts of each DFT coefficient X[kJ. 
2. Apply the FFT routine to this input sequence. 
3. Swap the real and imaginary parts of the output sequence. 
4. Scale the resulting sequence by tv to obtain the sequence x[n], corresponding to the 
inverse DFT of X[kJ. 
Determine whether this procedure works as claimed. If it doesn't, propose a simple modi­
fication that will make it work. 
9.25.  The DFT is a sampled version of the DTFT of a finite-length sequence; i.e., 
X[k] = X(ei (2Jf/N)k) 
= X(eiWk)i
wk=(2JrIN)k 
N-IL x[nJe-i(2Jf/N)kl1 
k = 0, 1, ...• N 
1. 
(P9.25-1) 
11=0 
Furthermore, an FFT algorithm is an efficient way to compute the values X[k]. 
Now consider a finite-length sequence x[n] whose length is N samples. We want to evaluate 
X(z), the z-transform of the finite-length sequence, at the following points in the z-plane 
Zk = rei (2Jf/N)k 
k = 0,1, ... , N - 1, 
where r is a positive number. We have available an FFT algorithm. 
(a)  Plot the points Zk in the z-plane for the case N 
8 and r 
0.9. 
(b)  Write an equation [similar to Eq. (P9.25-1) above] for X(Zk) that shows that X(Zk) is 
the DFT of a modified sequence i[nl. What is i[n]? 
(c)  Describe an algorithm for computing X(Zk) using the given FFT function. (Direct 
evaluation is not an option.) You may describe your algorithm using any combination 
of English text and equations, but you must give a step-by-step procedure that starts 
with the sequence x[n] and ends with X(Zk). 
9.26. We are given a finite-length sequence x[nJ of length 627 (Le., x[n] = °for n < 0 and 
n > 626), and we have available a program that will compute the DFT of a sequence of any 
length N = 2". 
For the given sequence, we want to compute samples of the DTFT at frequencies 
2JT  
2JTk 
k = 0, 1, ... ,255.
Wk = 627 + 256 
Specify how to obtain a new sequence y[n] from x[n] such that the desired frequency samples 
can be obtained by applying the available FFT program to y[n] with vas small as possible. 
9.27. A finite-length signal  of length L = 500 (x[n] = 0 for n < 0 and n > L - 1) is ob­
tained by sampling a continuous-time signal with sampling rate 10,000 samples per second. 
We wish to compute samples of the z-transform of x[ll] at the N equally spaced points 
Zk = (0.8)ei2JfkjN, for 0:5 k :5 N -1, with an effective frequency spacing of 50 Hz or less. 
(a)  Determine the minimum value for N if N 
2". 
(b)  Determine a sequence y[n] of length N, where N is as determined in part (a), such that 
its DFT Y[k] is equal to the desired samples of the z-transform of x[nJ. 

771 
Chapter 9 
Problems 
9.28. You are asked to build a system that computes the DFT of a 4-point sequence 
x[O], xli], x[2], x[3].  
You can purchase any number ofcomputational units at the per-unit cost shown in Table 9.1.  
TABLE 9.1 
Design a system of the lowest possible cost. Draw the associated block diagram and 
indicate the system cost. 
Advanced Problems 
9.29. Consider an N-point sequence x[nl with DFT X[k], k 
0.1, ... , N 
1. The following 
algorithm computes the even-indexed DFT values X[k), k 
0.2, ... , N 
2, for N even, 
using only a single N 12-point DFT: 
1. Form the sequence y[n] by time aliasing, i.e., 
y[nJ = {xo,[n] +x[n + NI2],  O::'C n ::'C NI2 -1, 
otherwise. 
2. Compute Y[r], r = 0, 1, ... , (N12) - 1, the N 12-point DFT of y[nJ. 
3. Then the even-indexed values of X[k] are X[k] = y[kI2], for k = 0, 2, ... , N 
2. 
(a)  Show that the preceding algorithm produces the desired results. 
(b)  Now suppose that we form a finite-length sequence y[n] from a sequence x[n] by 
x[n + rM]. 
O::'C n::'C M 
1,
y[n) = 
T=-OO!
f 
0, 
otherwise. 
Determine the relationship between the M-point DFT Y[k] and X(e jW ), the Fourier 
transform of x [n]. Show that the result of part (a) is a special case of the result of part 
(b). 
(c)  Develop an algorithm similar to the one in part (a) to compute the odd-indexed DFT 
values X[k], k = 1, 3, ... , N - 1, for N even, using only a single N 12-point DFT. 

772 
Chapter 9 
Computation of the Discrete Fourier Transform 
9.30.  The system in Figure P9.30 computes an N-point (where N is an even number) DFT X[k] 
of an N-point sequence x[n] by decomposing x[n] into two N /2-point sequences g1 [n] and 
82[n], computing the N /2-point DFT's Gl[k] and G2[k], and then combining these to form 
X[k]. 
Ifgl[n] is the even-indexed values ofx[n] and g2[n] is the odd-indexed values ofx[n1 
81[n1 
x[2n] and g2[n] = x[2n + 1] then X[k] will be the DFT of x[n]. 
In using the system in Figure P9.30 an error is made in forming gl[n] and g2[n], such that 
gl[n] is incorrectly chosen as the odd-indexed values and g2[n] as the even indexed values 
but G1 [k] and G2[k] are still combined as in Figure P9.30 and the incorrect sequence X[k] 
results. Express X[k] in terms of X[k]. 
9.31.  In Section 9.3.2, it was asserted that the transpose of the flow graph of an FFT algorithm 
is also the flow graph of an FFT algorithm. The purpose of this problem is to develop that 
result for radix-2 FFT algorithms. 
(8)  The basic butterfly for the decimation-in-frequency radix-2 FFT algorithm is depicted 
in Figure P9.31-1. This flow graph represents the equations 
Xm[p] = Xm-l[P] + Xm-l[q], 
Xm[q] = (Xm-1[P] - X m -1[q])WN· 
Starting with these equations, show that X m -l[P] and X m -l[q] can be computed from 
Xm [pI and Xm[q], respectively, using the butterfly shown in Figure P9.31-2. 
Xm  dp] 0;: 
;D" 
oXm[P] 
WN
Xm_l[q] d' 
"0 
.. 
o Xm[q] 
Figure P9.31-1 
1 
Xm[P] 0 
•
2 VXm,IP]  
•-1 
lW-r
"2 
N 
Xm[q] 0 
.. 
if 
.. 
'bXm_M]
-1  
Figure P9.31-2 

773 
Chapter 9 
Problems 
(b)  In the decimation-in-frequency algorithm of Figure 9.22, Xv[r], r = 0, 1, .... N 
1 is 
the DFf X[k] arranged in bit-reversed order, and XO[r] 
x[r], r = 0,1, ... , N 
1; 
i.e., the zeroth array is the input sequence arranged in normal order. If each butterfly 
in Figure 9.22 is replaced by the appropriate butterfly of the form of Figure P9.31, the 
result would be a flow graph for computing the sequence x[n] (in normal order) from 
the DFf X[k] (in bit-reversed order). Draw the resulting flow graph for N 
8. 
(c)  The flow graph obtained in part (b) represents an inverse DFf algorithm, i.e., an 
algorithm for computing 
1 N-l 
x[n] = N L X[k]WN'kn, 
n = 0, 1, ... , N 
1. 
n=O 
Modify the flow graph obtained in part (b) so that it computes the DFf 
N-l 
X[k] L x[n]wtn , 
k = 0, 1, ... , N - 1, 
n=O 
rather than the inverse DFf. 
(d)  Observe that the result in part (c) is the transpose of the decimation-in-frequency 
algorithm of Figure 9.22 and that it is identical to the decimation-in-time algorithm 
depicted in Figure 9.11. Does it follow that, to each decimation-in-time algorithm (e.g., 
Figures 9.15-9.17), there corresponds a decimation-in-frequency algorithm that is the 
transpose of the decimation-in-time algorithm and vice versa? Explain. 
9.32. We want to implement a 6-point decimation-in-time FFT using a mixed radix approach. 
One option is to first take three 2-point DFfs, and then use the results to compute the 
6-point DFf. For this option: 
(a)  Draw a flowgraph to show what a 2-point DFf calculates. Also, fill in the parts of the 
flowgraph in Figure P9.32-1 involved in calculating the DFf values Xo, XI, and X4' 
2-point 
DFT 
2-point 
DFT 
Figure pg.32-1 
(b) How many complex multiplications does this option require? (Multiplying a number 
by -1 does not count as a complex multiplication.) 

774  
Chapter 9 
Computation of the Discrete Fourier Transform 
A second option is to start with two 3-point DFTs, and then use the results to compute the 
6-point OFT. 
(c) Draw a flowgraph to show what a 3-point OFT calculates. Also, fill in all of the flowgraph 
in Figure P932-2 and briefly explain how you derived your implementation; 
(d)  How many complex mUltiplications does this option require? 
Xo
Xo 
3-point h~ · L7 
• Xl
Xl 
DFf 
"" 7 
W6 
X4 
Xl 
W~ 
Xl 
X3 
X3 
: X,
~~ICZ~~  
Xs 
Xs 
W~ 
Figure P9.32-2 
9.33. The decimation-in-frequency FFT algorithm was developed in Section 9.3 for radix 2, i.e., 
N 
2v. A similar approach leads to a radix-3 algorithm when N = 3v. 
(a)  Draw a flow graph for a 9-point decimation-in-frequency algorithm using a 3 x 3 
decomposition of the DFT. 
(b) For  N 
3v, how many complex multiplications by powers of WN are needed to 
compute the DFT of an N-point complex sequence using a radix-3 decimation-in­
frequency FFT algorithm? 
(c)  For N 
3v , is it possible to use in-place computation for the radix-3 decimation-in­
frequency algorithm? 
9.34. We have seen that an FFT algorithm can be viewed as an interconnection of butterfly com­
putational elements. For example, the butterfly for a radix-2 decimation-in-frequency FFT 
algorithm is shown in Figure P934-1. The butterfly takes two complex numbers as input 
and produces two complex numbers as output. Its implementation requires a complex mul­
tiplication by WN' where r is an integer that depends on the location of the butterfly in 
jO
the flow graph of the algorithm. Since the complex multiplier is of the form WN= e
, 
the CORDIC (coordinate rotation digital computer) rotator algorithm (see Problem 9.46) 
can be used to implement the complex multiplication efficiently. Unfortunately, while the 
CORDIC rotator algorithm accomplishes the desired change of angle, it also introduces a 
fixed magnification that is independent of the angle e. Thus, if the CORDIC rotator algo­
rithm were used to implement the multiplications by WN' the butterfly of Figure P934-1 
would be replaced by the butterfly of Figure P9.34-2, where G represents the fixed magni­
fication factor of the CORDIC rotator. (We assume no error in approximating the angle of 
rotation.) Ifeach butterfly in the flow graph of the decimation-in-frequency FFT algorithm 
is replaced by the butterfly ofFigure P9.34-2, we obtain a modified FFT algorithm for which 
the flow graph would be as shown in Figure P9.34-3 for N =S. The output of this modified 
algorithm would not be the desired DFT. 

775 
Chapter 9 
Problems 
W' 
Xm - 1[q] d--_~l--'Q---Joo.':...-N--oXm [q] 
Figure P9.34.1 
GWN
Xm_1[q] 
Xm[q]
-1  
Figure P9.34-2 
x[O]  
Y[O] 
GW~ 
x[l] 
Y[4] 
x[2] 
Y[2] 
GW,Z
x[3]  
Y[6] 
x[4]  
Y[l] 
GW~ 
x[S]  
Y[5] 
x[6] 
Y[3] 
x[7] 
-1 
-1 
-1 
GW,Z 
Y[7] 
Figure P9.34-3 
(a) Show that the output for the modified FFf algorithm is Y[k] = W[k]X[k], where X[k] 
is the correct DFr of the input sequence x[n] and W[k] is a function of G, N, and k. 
(b)  The sequence W[k] can be described by a particularly simple rule. Find this rule and 
indicate its dependence on G, N. and k. 
(c)  Suppose that we wish to preprocess the input sequence x[n] to compensate for the effect 
of the modified FFf algorithm. Determine a procedure for obtaining a sequence .t[n] 
from x[n] such that if ifni is the input to the modified FFf algorithm, then the output 
will be X[k], the correct DFf of the original sequence x[n]. 

776  
Chapter 9 
Computation of the Discrete Fourier Transform 
9.35.  lbis problem deals with the efficient computation of samples of the z-transform of a finite­
length sequence. Using the chirp transform algorithm, develop a procedure for computing 
values of X (z) at 25 points spaced uniformly on an arc of a circle of radius 0.5, beginning at 
an angle of -rr/6 and ending at an angle of2iT/3. The length ofthe sequence is 100 samples. 
9.36. Consider a 1024-point sequence x[n] constructed by interleaving two 512-point sequences 
xe[n] and xo[n]. Specifically, 
I 
xe[n/2], if n = 0, 2, 4, ... , 1022; 
x[n] 
xo[(n - 1)/2], if n 
1, 3, 5, ... , 1023; 
0, for n outside of the range 0 :":0 n :":0 1023. 
Let X[k] denote the 1024-point DFT of x[n] and Xe[k] and Xolk] denote the 512-point 
DFfs ohern] and xoln], respectively. Given X[k] we would like to obtain Xe[k] from X[k] 
in a computationally efficient way where computational efficiency is measured in terms of 
the total number of complex multiplies and adds required. One not-very-efficient approach 
is as shown in Figure P9.36: 
1024-point IDFT 
512-point DFT 
Figure PD.36 
Specify the most efficient algorithm that you can (certainly more efficient than the 
block diagram of Figure P9.36) to obtain Xe[k] from X[k]. 
9.37. Suppose that a program is available that computes the DFT of a complex sequence. If we 
wish to compute the DFT of a real sequence, we may simply specify the imaginary part to 
be zero and use the program directly. However, the symmetry of the DFT of a real sequence 
can be used to reduce the amount of computation. 
(8)  Let x[n] be a real-valued sequence of length N, and let X[k] be its DFT with real and 
imaginary parts denoted XR[kj and X/[k], respectively; i.e., 
X[kJ 
XR[k] + jX/[k]. 
Show that if x[n] is real, then XR[k] = XR[N 
k] and X/[k] 
-X/[N -
kj for 
k 
1, ... , N - 1. 
(b) Now consider two real-valued sequences xl[n] and x2[n] with DFTs X I[k] and X2lk], 
respectively. Let g[n] be the complex sequence g[n] 
xl[n] + jX2[n], with corre­
sponding DFT G[k] = G R[k]+ jGIlk]. Also, let GOR[k], GER[k], GOllk] and GEI[k] 
denote, respectively, the odd part of the real part, the even part of the real part, the odd 
part ofthe imaginary part, and the even part of the imaginary part of G[k]. Specifically, 
for 1 :":0 k :":0 N - 1, 
GORlk] = i(GR[k]- GR[N - k]j, 
GER[k] 
i{GR[k] + GR[N 
kll, 
GOllk] = i{GIlk] - GIlN -
kj), 
GEI[k] = i{Gdk]+GI[N-k]}. 
and GOR[O] = GOl[O] = 0, GER[O] 
G R[O], GEI[O] 
GdO]. Determine expres­
sions for Xl [k] and X2[k] in terms of GOR[k], GER(k], GOl[k], and GEI[kj. 

777 
Chapter 9 
Problems 
2v
(c)  Assume that N = 
and that a radix-2 FFT program is available to compute the 
DFT. Determine the number of real multiplications and the number of real additions 
required to compute both XI[k] and X2[k] by (i) using the program twice (with the 
imaginary part of the input set to zero) to compute the two complex N-point DFTs 
XI[k] and X2[kj separately and (ii) using the scheme suggested in part (b), which 
requires only one N-point DFT to be computed. 
(d)  Assume that we have only one real N-point sequence x[n], where N is a power 
of 2. Let xI[n] and x2[n] be the two real Nl2-point sequences xI[n] = x[2n] and 
x2[n] = x[2n + 1], where n 
0, I, ... , (N12) - 1. Determine X[k] in terms of the 
(N12)-point DFTs XI[k] and X2[kj. 
(e)  Using the results of parts (b), (c), and (d), describe a procedure for computing the 
DFT ofthe real N -point sequence x [n] utilizing only one N 12-pointFFT computation. 
Determine the numbers ofreal multiplications and real additions required by this pro­
cedure, and compare these numbers with the numbers required if the X[k] is computed 
using one N -point FFT computation with the imaginary part set to zero. 
9.38.  Let x[n] and h[n] be two real finite-length sequences such that 
x[n] = 0 
for n outside the interval 0 :.: n :.: L 
1. 
h[n] = 0 
for n outside the interval 0 :.: n :.: P 
1. 
We wish to compute the sequence y[n] 
x[n] *h[n], where *denotes ordinary convolution. 
(a)  What is the length ofthe sequence y[n]? 
(b)  For direct evaluation of the convolution sum, how many real multiplications are re­
quired to compute all of the nonzero samples of y[n]? The following identity may be 
useful: 
N 
N(N + 1)
k 
2 
k=l 
(c)  State a procedure for using the DFT to compute all of the nonzero samples of y[nj. 
Determine the minimum size of the DFTs and inverse DFTs in terms of Land P. 
2v
(d)  Assume that L = P = N 12, where N 
is the size of the DFT. Determine a 
formula for the number of real multiplications required to compute all the nonzero 
values of y[nj using the method of part (c) if the DFTs are computed using a radix­
2 FFT algorithm. Use this formula to determine the minimum value of N for which 
the FFT method requires fewer real multiplications than the direct evaluation of the 
convolution sum. 
9.39.  In Section 8.7.3, we showed that linear time-invariant filtering can be implemented by sec­
tioning the input signal into finite-length segments and using the DFT to implement circular 
convolutions on these segments. The two methods discussed were called the overlap-add 
and the overlap-save methods. If the DFTs are computed using an FFT algorithm, these 
sectioning methods can require fewer complex multiplications per output sample than the 
direct evaluation of the convolution sum. 
(a)  Assume that the complex input sequence x[n] is of infinite duration and that the 
complex impulse response h[n] is of length P samples, so that h[n] of: 0 only for 
o :.: n :.: P 
1. Also, assume that the output is computed using the overlap-save 
method, with the DFTs of length L 
2v , and suppose that these DFTs are computed 
using a radix-2 FFT algorithm. Determine an expression for the number of complex 
multiplications required per output sample as a function of l! and P. 

778  
Chapter 9 
Computation of the Discrete Fourier Transform 
(b)  Suppose that the length of the impulse response is P = 500. By evaluating the formula 
obtained in part (a), plot the number of multiplications per output sample as a function 
of v for the values of v ~ 20 such that the overlap-save method applies. For what 
value of v is the number of multiplications minimal? Compare the number of complex 
multiplications per output sample for the overlap-save method using the FFf with the 
number of complex multiplications per output sample required for direct evaluation 
of the convolution sum. 
(c)  Show that for large FFf lengths, the number of complex multiplications per output 
sample is approximately v. Thus, beyond a certain FFf length, the overlap-save method 
is less efficient than the direct method. If P = 500, for what value of v will the direct 
method be more efficient? 
(d)  Assume that the FFf length is twice the length of the impuLse response (i.e., L = 2P), 
and assume that L 
2v. Using the formula obtained in part (a), determine the smallest 
value of P such that the overlap-save method using the FFf requires fewer complex 
multiplications than the direct convolution method. 
9.40.  x[n] is a 1024-point sequence that is nonzero only for 0 
n ~ 1023. Let X[k] be the 1024­
point DFf of x[n]. Given X[k], we want to compute x[n] in the ranges 0 ~ n ~ 3 and 
1020 ~ n ~ 1023 using the system in Figure P9AO. Note that the input to the system is the 
sequence of DFf coefficients. By selecting m1[n], mz [n], and h [n], show how the system can 
be used to compute the desired samples of x[n]. Note that the samples y[n] for 0 ~ n :s 7 
must contain the desired samples of x[n]. 
'[nl =>lkll,,1 
mj[n]  
m2[n] 
Figure P9.40 
9.41.  A system has been built for computing the 8-point DFf y[0], Y[l], ... , Y[7] of a sequence 
y[O], y[I] ...., y[7]. However, the system is not working properly: only the even DFf samples 
Y[O), Y[2], Y[4], Y[6] are being computed correctly. To help you solve the problem, the data 
you can access are: 
• the (correct) even DFf samples. y[0], Y[2l, Y[4], Y[6]; 
• the first 4 input values y[O], y[l], y[2l, y[3] (the other inputs are unavailable). 
(a)  If y[O] = 1, and y[I] 
y[2] 
y[3] = 0, and y[0] = Y[2] 
Y[4] 
Y[6] = 2, what are 
the missing values Y[l], Y[3J, Y[5], Y[7]? Explain. 
(b)  You need to build an efficient system that computes the odd samples 
Y[I], Y[3], Y[5], Y[7] for any set of inputs. The computational modules you have avail­
able are one 4-point DFf and one 4-point IDFf. Both are free. You can purchase 
adders, subtracters, or multipliers for $10 each. Design a system of the lowest possible 
cost that takes as input 
y[O], y[IJ, y[2), y[3], Y[O], Y[2], Y[4], Y[6] 
and produces as output 
y[Il, Y[3], Y[5], Y[7J. 
Draw the associated block diagram and indicate the total cost. 

179 
Chapter 9 
Problems 
9.42. Consider a class ofOFT-based algorithms for implementing a causal FIR filter with impulse 
response h[n] that is zero outside the interval 0 ~ n ~ 63. The input signal (for the FIR 
filter) x[n] is segmented into an infinite number of possibly overlapping 128-point blocks 
xi[n], for i an integer and -00 ~ i ~ 00, such that 
.[ ]_ {x[n], 
iL ~ n ~ iL + 127, 
Xl n -
0, 
otherwise, 
where L is a positive integer.  
Specify a method for computing  
ydn] = xj[n] * h[n] 
for any i. Your answer should be in the form of a block diagram utilizing only the types of 
modules shown in Figures PP9.42-1 and PP9,42-2. A module may be used more than once 
or not at all. 
The four modules in Figure P9.42-2 either use radix-2 FFTs to compute X[k], the 
N-point OFT of x[n], or use radix-2 inverse FFTs to compute x[nJ from X[kJ. 
Your specification must include the lengths of the FITs and [FFTs used. For each 
"shift by no" module, you should also specify a value for no, the amount by which the input 
sequence is to be shifted. 
xl[n] 
Multiply 
x[n] 
x[n-nol 
xl[n]x2[n] 
x2[n] 
Figure P9.42-1 
FFr-l 
where P[kl is X[k] in 
(N-point) 
bit-reversed order. 
x[n] 
P[k] 
q[n] 
FFf-2 
(N-point) 
X[kl 
where q [n] isx[n] in 
bit-reversed order. 
X[k] 
IFFr-l 
(N-point) 
r[n] 
where r[n] isx[n] in 
bit-reversed order. 
IFFT-2 
(N-point) 
x[n]
S[k] 
where S[k] is X[k] in 
bit-reversed order. 
Figure P9.42-2 
Extension Problems 
9.43. In many applications (such as evaluating frequency responses and interpolation), it is of 
interest to compute the OIT of a short sequence that is "zero-padded." In such cases, a 
specialized "pruned" FIT algorithm can be used to increase the efficiency of computation 
(Markel, 1971). In this problem, we will consider pruning of the radix-2 decimation-in­
frequency algorithm when the length of the input sequence is M ~ 2JJ- and the length of the 
OFTisN 
2V , where fl, <v. 
(a)  Oraw the complete flow graph of a decimation-in-frequency radix-2 FFT algorithm for 
N = 16. Label all branches appropriately. 

780 
." 
Chapter 9 
Computation of the Discrete Fourier Transform 
(b)  Assume that the input sequence is of length M = 2; i.e., x[n] f= 0 only for N 
0 and 
N 
1. Draw a new flow graph for N 
16 that shows how the nonzero input samples 
propagate to the output DIT; i.e., eliminate or prune all branches in the flow graph of 
part (a) that represent operations on zero-inputs. 
(c)  In part (b), all of the butterflies in the first three stages of computation should have been 
effectively replaced by a half-butterfly of the form shown in Figure P9.43, and in the 
last stage, all the butterflies should have been of the regular form. For the general case 
where the length of the input sequence is M :": 2/L and the length of the DIT is N =2v, 
where /1- < v, determine the number of stages in which the pruned butterflies can be 
used. Also, determine the number of complex multiplications required to compute the 
N -point DIT of an M -point sequence using the pruned FFf algorithm. Express your 
answers in terms of v and /1-. 
Xm - 1[P]Q;: 
0 
oXm[Pl 
WN
'0 
.. 
oXm [q1 
Figure P9.43 
9.44. In Section 9.2, we showed that if N is divisible by 2, an N-point DIT may be expressed as 
X[k] = G[«k»N/2] + WtH[«k»N/2L 
0:,,: k :": N -1. 
(P9.44-I) 
where G[k] is the N /2-point DIT of the sequence of even-indexed samples, 
g[n] 
x [2n], 
0:,,: n :": (N/2) - 1, 
and H[k] is the N/2-point DIT of the odd-indexed samples, 
h[n] = x[2n + 1], 
0:,,: n:,,: (N/2) 
1. 
Note that G[k] and H[k] must be repeated periodically for N /2 :": k :": N - 1 for 
Eq. (P9.44-I) to make sense. When N = 2v, repeated application of this decomposition 
leads to the decimation-in-time FFf algorithm depicted for N 
8 in Figure 9.11. As we 
have seen, such algorithms require complex multiplications by the "twiddle" factors wt. 
Rader and Brenner (1976) derived a new algorithm in which the multipliers are purely imag­
inary, thus requiring only two real multiplications and no real additions. In this algorithm, 
Eq. (P9.44-I) is replaced by the equations 
X[O] = G[O] + F[O],  
(1'9.44-2) 
X[N/2J 
G[O] - F[O],  
(P9.44-3) 
1 
F[k]
X[k] = G[k]  
k f= 0, N/2. 
(P9.44-4)
sin(27T:k/ N) , 
Here, F[k] is the N /2-point DFT of the sequence 
f[n] 
x[2n + I] 
x[2n -1] + Q, 
where 
2 (N/2)-1 
Q =  N L 
x[2n + I] 
n=O 
is a quantity that need be computed only once. 

781 
Chapter 9 
Problems 
(a)  Show that F[O] 
H[O] and therefore that Eqs. (P9.44-2) and (P9.44-3) give the same 
result as Eq. (P9.44-1) for k 
0, N /2. 
(b)  Show that 
for k = 1,2, ... , (N/2) 
1. Use this result to obtain Eq. (P9.44-4). Why must we 
compute X[O] and X[N/2] using separate equations? 
(c)  When N 
2v , we can apply Eqs. (P9.44-2)-(P9.44-4) repeatedly to obtain a com­
plete decimation-in-time FFf algorithm. Determine formulas for the number of real 
multiplications and for the number of real additions as a function of N. In counting 
operations due to 
(P9.44-4), take advantage of any symmetries and periodicities, 
but do not exclude "trivial" multiplications by ±j/2. 
' 
(d)  Rader and Brenner (1976) state that FFf algorithms based on Eqs. (P9 .44-2 )-(P9 .44-4) 
have "poor noise properties." Explain why this might be true. 
9.45. A modified FFf algorithm called the split-radix FFf, or SRFFf, was proposed by Duhamel 
and Hollman (1984) and Duhamel (1986). The flow graph for the split-radix algorithm is 
similar to the radix-2 flow graph, but it requires fewer real multiplications. Tn this problem, 
we illustrate the principles of the SRFFf for computing the DFf X[k] of a sequence x[n] 
of length N. 
(a)  Show that the even-indexed terms of X[k] can be expressed as the N /2-point DFf 
(N/2)-1 
X[2k] = L (x[n] +x(n + N/2])W~kn 
n=O 
fork 
0,1, ... ,(N/2)-1. 
(b)  Show that the odd-indexed terms of the DFf X[k] can be expressed as the N /4-point 
DfTs 
X(4k + 1J 
(N/4)-1
L 
{(x[n] - x[n + N /2]) - j(x(n + N /4] - x[n + 3N/4])) WNwffn 
n=O 
for k 
0,1, ... , (N/4) - 1, and 
X[4k +3] 
(N/4)-1
L {(x[n] - x[n + N /2)) + j (x[n + N /4J - x[n + 3N/4])} W~n W~kn 
,,=0 
for k = 0, 1, ... , (N/4) - l. 
(c)  The flow graph in Figure P9.45 represents the preceding decomposition ofthe DfTfor a 
16-point transform. Redraw this flow graph, labeling each branch with the appropriate 
multiplier coefficient. 

782 
Chapter 9 
Computation of the Discrete Fourier Transform 
x[O] Q---------~o-----r---I_------{lX[O] 
1----------<0 X[2] 
x [2] 
xli] 
\----------<0 X[4] 
x[3] 
Eight­
\----------<0 X[6] 
point 
DFf 
1-------0 X[8] 
xIS] 
x[4] 
\----------<0 X[lO] 
x[6] 
,.----------<0 X[12] 
x[7] VYV't0}Y7 
x[8] 
x[9] 
x[lO] 
x[ll] 
x [12] 
x[13] 
x[14] 
x[lS] 
Figure P9.45 
oX[14] 
X[l] 
Fo~r-
[X[S] 
pomt 
DFf 
X[9) 
X[13] 
X[3] 
Fo~r- [X[7] 
pomt 
DFf 
Xlll] 
X[1S) 
(d)  Determine the number of real multiplications required to implement the 16-point 
transform when the SRFFT principle is applied to compute the other DFTs in Fig­
ure P9.45. Compare this number with the number of real multiplications required to 
implement a 16-point radix-2 decimation-in-frequency algorithm. In both cases, as­
sume that multiplications by w2 are not done. 
9.46.  In computing the DFT, it is necessary to multiply a complex number by another complex 
number whose magnitude is unity, i.e., (X + j y)ejo . Clearly, such a complex multiplication 
changes only the angle of the complex number, leaving the magnitude unchanged. For 
this reason, multiplications by a complex number ejIJ are sometimes called rotations. In 
DFT or FFT algorithms, many different angles e may be needed. However, it may be 
undesirable to store a table of all required values of sin e and cos e, and computing these 
functions by a power series requires many multiplications and additions. With the CORDIC 
algorithm given by Voider (1959), the product (X + jY)e jO can be evaluated efficiently by 
a combination of additions, binary shifts, and table lookups from a small table. 
(a)  Define e, = arctan(2-i). Show that any angle 0 < e < If/2 can be represented as 
M-l 
e  L aiel + E 
9 + E, 
i=O 
where ai 
±1 and the error E is bounded by  
lEI ~ arctan(2-M ).  

783 
Chapter 9 
Problems 
(b) The angles OJ may be eomputed in advance and stored in a small table of length M. 
State an algorithm for obtaining the sequence {al} for i 
0,1, ... , M 
1, such that 
ai = ±l. Use your algorithm to determine the sequence {ai} for representing the angle 
e = l00rr/512 when M 
11. 
(c)  Using the result of part (a), show that the recursion 
Xo  
X, 
YO  
Y, 
2-1+1
X  
X 
Y 
1,2, ... ,M,
i  
i-I 
O:i-l i-I 
' 
Yi  
Yj-l +ai_lXi_I2-i+I, 
i = 1,2, ... , M, 
will produce the eomplex number 
where e = L:i'!:OI aiOI and GM is real, is positive, and does not depend on e. That 
is, the original complex number is rotated in the complex plane by an angle eand 
magnified by the constant GM. 
(d) Determine the magnification constant GM as a function of M. 
9.47.  In Section 9.3, we developed the decimation-in-frequency FFT algorithm for radix 2, i.e., 
N = 2". It is possible to formulate a similar a1gorithm for the general case of N 
m \J , where 
m is an integer. Such an algorithm is known as a radix-m FFf algorithm. In this problem. 
we will examine the radix-3 decimation-in-frequency FFf for the case when N 
9, i.e., the 
input sequence x[n] = 0 for n < 0 and n > 8. 
(a)  Formulate a method of computing the DFf samples X[3k] for k = 0,1,2. Consider 
defining Xdk] = X(ejwk)!wk=21rk/3.How can you define a time sequence xl [n] in terms 
of x[n] such that the 3-point DFf of Xl [n] is Xl [k] = X[3k]? 
(b)  Now define a sequence x2[n] in terms of x[n] such that the 3-point DFT of x2[n] is 
X2[k] = X[3k + 1] for k 
0,1,2. Similarly, define x3[n] such that its 3-point DFf 
X3[k] = X[3k + 2] for k = 0, 1,2. Note that we have now defined the 9-point DFT as 
three 3-point DFfs from appropriately constructed 3-point sequences. 
(c)  Draw the signal flow graph for the N 
3 DFf, i.e., the radix-3 butterfly. 
(d)  Using the results for parts (a) and (b), sketch the signal flow graph for the system that 
constructs the sequences xl[n], x2[n], and x3[n], and then use 3-point DFf boxes on 
these sequences to produce X[k] for k 
0, ... ,8. Note that in the interest of clarity, 
you should not draw the signal flow graph for the N = 3 DFfs, but simply use boxes 
labeled"N = 3 DFI:" The interior of these boxes is the system you drew for part (c). 
(e)  Appropriate factoring of the powers of W9 in the system you drew in part (d) allows 
these systems to be drawn as N = 3 DFfs, followed by "twiddle" factors analogous 
to those in the radix-2 algorithm. Redraw the system in part Cd) such that it consists 
entirely of N = 3 DFfs with "twiddle" factors. This is the complete formulation of the 
radix-3 decimation-in-frequency FFT for N 
9. 
(f)  How many complex multiplications are required to compute a 9-point DFf using a 
direct implementation of the DFf equation? Contrast this with the number of com­
plex multiplications required by the system you drew in part (e). In general, how 
many complex multiplications are required for the radix-3 FFf of a sequence of length 
N 
3"? 

784  
Chapter 9 
Computation of the Discrete Fourier Transform 
9.48.  Bluestein (1970) showed that if N = M2, then the chirp transform algorithm has a recursive 
implementation. 
(a)  Show that the DFT can be expressed as the convolution 
N-J 
X[k] = h*[k] L (x[nlh*[n])h[k 
n], 
n=O 
where * denotes complex conjugation and 
h[n] = ej(n/N)n
1
• 
-00 < n < 00. 
(b) Show that the desired values of X[k] (i.e., for k = 0, 1, ... , N 
1) can also be obtained 
by evaluating the convolution of part (a) for k 
N, N + 1, ... , 2N 
1. 
(c)  Use the result of part (b) to show that X[k] is also equal to the output of the system 
shown in Figure P9A8 for k = N, N + 1, ... , 2N 
1, where h[kI is the finite-duration 
sequence 
2 
A 
{e j (n/N)k , 
O::::k 
2N-L 
h[kJ = 
0, 
otherwise. 
(d)  Using the fact that N 
M2, show that the system function corresponding to the 
impulse response hLkJ is 
2N-l 
2
H(z) 
L ei (n/N)k
k=O 
M-l  
-2M2 
'\' j(n/N)rl~-r 
1-z 
L.., e 
<. 
-1-+-e-j7:·("""2n-/:-:M7:")-rz--""'M:-:;"' 
r=O 
Hint: Express k as k 
r + eM. 
(e)  The expression for H(z) obtained in part (d) suggests a recursive realization of the FIR 
system. Draw the flow graph of such an implementation. 
(f)  Use the result of part (e) to determine the total numbers of eomplex multiplications 
and additions required to compute all of the N desired values of X[k). Compare those 
numbers with the numbers required for direct computation of X[kJ. 
h[kJ 
h*[k]  
;; *[ k] 
Figure P9.48 
9.49.  In the Goertzel algorithm for computation of the discrete Fourier transform, X[kl is com­
puted as 
X[k] 
Yk[N], 
where ydnJ is the output of the network shown in Figure P9,49. C.Dnsider the implemen­
tation of the Goertzel algorithm using fixed-point arithmetic with rounding. Assume that 
the register length is B bits plus the sign, and assume that the products are rounded before 
additions. Also, assume that round-off noise sources are independent. 

785 
Chapter 9 
Problems 
x[r] 
-1 
Figure P9.49 
(a)  Assuming that x[n] is real, draw a flow graph of the linear-noise model for the finite­
precision computation of the real and imaginary parts of X[k]. Assume that multipli­
cation by ±1 produces no round-off noise. 
(b)  Compute the variance of the round-off noise in both the real part and the imaginary 
part of X[k]. 
9.50. Consider direct computation of the DFf using fixed-point arithmetic with rounding. As­
sume that the register length is B bits plus the sign (i.e., a total of B + 1 bits) and that 
the round-off noise introduced by any real multiplication is independent of that produced 
by any other real multiplication. Assuming that x[n] is real, determine the variance of the 
round-off noise in both the real part and the imaginary part of each DFf value X[k]. 
9.51.  In implementing a decimation-in-time FFT algorithm, the basic butterfly computation is 
Xm[p] = X m-l[P] + W~Xm-l[q], 
Xm[q] = X m-l[P] -
W~Xm_llq]. 
In using fixed-point arithmetic to implement the computations, it is commonly assumed that 
all numbers are scaled to be less than unity. Therefore, to avoid overflow, it is necessary 
to ensure that the real numbers that result from the butterfly computations do not exceed 
unity. 
(a)  Show that if we require 
IXm - 1[p]1 < ~ 
and 
IXm-l[q]1 < i. 
then overflow cannot occur in the butterfly computation; i.e., 
IRe{Xm[p]JI < 1. 
IIm{Xm[p]]1 < 1. 
and 
(b) In practice, it is easier and most convenient to require 
and 
1
IIm{Xm-l[q]}1 < 2' 
Are these conditions sufficient to guarantee that overflow eannot oeeur in the decima­
tion-in-time butterfly computation? Explain. 
9.52.  In deriving formulas for the noise-to-signal ratio for the fixed-point radix-2 decimation-in­
time FFf algorithm, we assumed that each output node was connected to (N 
1) butterfly 
computations, each of which contributed an amount ali 
! .2-28 to the output noise 
variance. However, when W~ = ±1 or ±j, the multiplications can in fact be done without 

786  
Chapter 9 
Computation of the Discrete Fourier Transform 
error. Thus, if the results derived in Section 9.7 are modified to account for this fact, we 
obtain a less pessimistic estimate of quantization noise effects. 
(a)  For the decimation-in-time algorithm discussed in Section 9.7, determine, for each 
stage, the number of butterflies that involve multiplication by either ±1 or ±j. 
(b)  Use the result of part (a) to find improved estimates of the output noise variance, 
Eq. (9.58), and noise-to-signal ratio, Eq. (9.68), for odd values of k. Discuss how these 
estimates are different for even values of k. Do not attempt to find a closed form 
expression of these quantities for even values of k. 
(c)  Repeat parts (a) and (b) for the case where the output of each stage is attenuated 
by a factor of ~; i.e., derive modified expressions corresponding to Eq. (9.71) for the 
output noise variance and Eq. (9.72) for the output noise-to-signal ratio, assuming that 
multiplications by ±1 and 
do not introduce error. 
9.53. In Section 9.7  we considered a noise analysis of the decimation-in-time FFf algorithm 
of Figure 9.11. Carry out a similar analysis for the decimation-in-frequency algorithm of 
Figure 9.22, obtaining equations for the output noise variance and noise-to-signal ratio for 
scaling at the input and also for scaling by ~ at each stage of computation. 
9.54.  In this problem, we consider a procedure for computing the DFf of four real symmetric 
or antisymmetric N-point sequences using only one N-point DFf computation. Since we 
are considering only finite-length sequences, by symmetric and antisymmetric, we explicitly 
mean periodic symmetric and periodic antisymmetric, as defined in Section 8.6.4. Let Xl En], 
x2[n], x3[n], and x4fn] denote the four real sequences of length N, and let Xt[k], X2[k], 
X3[k], and X4[k] denote the corresponding DFfs. We assume first that Xl [n] and x2[n] are 
symmetric and x3[n] and x4[n] are antisymmetric; i.e., 
Xt[n] = Xl[N - n], 
X2[n] 
X2[N 
n], 
x3[n] 
-X3[N 
n], 
x4[n] = -X4[N - n], 
for n = 1,2, ... , N -1 and x3[O] 
X4[O] = O. 
(a)  Define YdnJ 
Xl [n] + x3[nJ and let Yl[k] denote the DFf of Yt[nj. Determine how 
XI[k] and X2[k] can be recovered from YI[k]. 
(b)  YI [n] as defined in part (a) is a real sequence with s)'l11metric part Xl En] and antisym­
metric part x3[n]. Similarly, we define the real sequence Y2[n] = x2[n] + x4[n], and we 
let Y3 [n] be the complex sequence 
Y3[n] = ydn] + jyz[n]. 
First, determine how YI[k] and Yz[k] can be determined from Y3[k], and then, using 
the results of part (a), show how to obtain XI[k], Xz[k], X3[k], and X4[kj from Y3[k]. 
The result of part (b) shows that we can compute the DFf of four real sequences simulta­
neously with only one N-point DFf computation if two sequences are symmetric and the 
other two are antisymmetric. Now consider the case when an four are symmetric; i.e., 
xdnJ 
xjLN-n], 
1,2,3,4, 
for n = 0,1, ... , N 
1. For parts (c)-(f), assume x3[n] and x4[n] are real and symmetric, 
not antisymmetric. 
(c)  Consider a real symmetric sequence x3[n]. Show that the sequence 
u3[n] = x3[(n + 1))N] - x3[ «n 
1»N] 
is an anti symmetric sequence; i.e., u3[n] 
-u3[N 
n] for n = 1,2, ... , N - 1 and 
u3[O] 
O. 

181 
Chapter 9 
Problems 
(d)  Let U3[k] denote the N-point DFf of u3[n]. Determine an expression for U3[k] in 
terms of X3[k]. 
(e)  By using the procedure of part (c), we can form the real sequence ytfn] 
xdn] + 
u3[n], where xl[n] is the symmetric part and u3[n] is the antisymmetric part of Yl[n]. 
Determine how Xl[k] and X3[k] can be recovered from Yl[k]. 
(f)  Now let Y3[n] 
ytfn] + jY2[n], where 
with 
u3[n] 
x3[((n + I))N] - x3[((n - I»N], 
u4[n] = x4[«n + 1))N]- x4[«n -1»N], 
for n = 0,1, ... , N 
1. Determine how to obtain Xl[k], X2[k], X3[k], and X4[k] from 
Y3[k]. (Note that X3[O] and X4[O] cannot be recovered from Y3[k], and if N is even, 
X3[N/2] and X4[N/2] also cannot be recovered from Y3[k].) 
9.55.  The input and output of a linear time-invariant system satisfy a difference equation of the 
form 
N 
y[n] = L aky[n 
kJ. 
k=l 
Assume that an FFT program is available for computing the DFf of any finite-length 
sequence of length L = 2v. Describe a procedure that utilizes the available FFT program 
to compute 
for k = 0, 1, ... , Sl1 , 
where H (z) is the system function of the system. 
9.56.  Suppose that we wish to multiply two very large numbers (possibly thousands of bits long) 
on a 16-bit computer. In this problem, we will investigate a technique for doing this using 
FFfs. 
(a)  Let p(x) and q(x) be the two polynomials 
L-l 
M-l 
i
p(x) = L ajx , 
q(x) = L b;xi. 
;=0  
;=0 
Show that the coefficients of the polynomial rex) 
p(x)q(x) can be computed using 
circular convolution. 
(b)  Show how to compute the coefficients of rex) using a radix-2 FFT program. For what 
orders of magnitude of (L + M) is this procedure more efficient than direct computa­
tion? Assume that L + M = 2" for some integer v. 
(c)  Now suppose that we wish to compute the product of two very long positive binary 
integers u and v. Show that their product can be computed using polynomial multipli­
cation, and describe an algorithm for computing the product using an FFT algorithm. 
If u is an 8000-bit number and v is a lOOO-bit number, approximately how many real 
multiplications and additions are required to compute the product u . v using this 
method? 
(d)  Give a qualitative discussion of the effect of finite-precision arithmetic in implementing 
the algorithm of part ( c). 

788  
Chapter 9 
Computation of the Discrete Fourier Transform 
9.57. The discrete Hartley transform (DHT) of a sequence x[n] oflength N is defined as 
N-l 
XH[k] L x[n]HN[nkJ, 
k 
0,1. ... , N 
1, 
n=O 
where 
HN[a] = CN[a] + SN[a), 
with 
CN[a] = cos(21ra/N), 
SN[a] = sin(2rra/N). 
Problem 8.68 explores the properties ofthe discrete Hartley transform in detail, particularly 
its circular convolution property. 
(a) Verify that HN[a) 
HN[a + N], and verify the following useful property of HN[a): 
HN[a +b] = HN[a]CN[b] + HN[-a]SN[b] 
HN[b]CN[a] + HN[-b]SN[a]. 
(b)  By decomposing x[n] into its even-numbered points and odd-numbered points, and 
by using the identity derived in part (a), derive a fast DHT algorithm based on the 
decimation-in-time principle. 
9.58.  In this problem, we will write the FFT as a sequence of matrix operations. Consider the 
8-point decimation-in-time FFT algorithm shown in Figure P9.S8. Let a and f denote the 
input and output vectors, respectively. Assume that the input is in bit-reversed order and 
that the output is in normal order (compare with Figure 9.ll). Let b, c, d, and e denote the 
intermediate vectors shown on the flow graph. 
b[O] 
c[O] 
d[O] 
e[O] 
a[0]  
flO] 
b[l] 
c[I] 
dell 
e[l] 
l[lJ
all] 
-1 
b[2] 
w~ 
c[2] 
d[2] 
e[2] 
a[2] 
1[2] 
b[3] 
w~ 
c [3] 
d[3] 
e[3]
a[3] 
1[3]
-1
-1 
b[4] 
c[4] 
d[4] 
w~ 
e4] 
a[4] 
1[4] 
b[5] 
c[5) 
d[5) 
w§ 
e[5] 
a[5]  
1[5]
-1 
WO 
a[6] 
1[6J
b[6] 
8 
c[6] 
d[6] 
W~ 
e[6) 
b[7] 
W§ 
c[7] 
d[7] 
W~ 
e[7]
a[7)  
1[7]
-1  
-1 
-1 
Figure P9.58 

789 
Chapter 9 
Problems 
(a)  Determine the matrices Fl , Tl, F2, T2, and F3 such that  
b = Fla,  
c = Tlb,  
d = F2c,  
e = T2d, 
f = F3e. 
(b)  The overall FFT, taking input a and yielding output f can be described in matrix notation 
as f = Qa, where 
Q = F3T2F2Tl Fl' 
Let QH be the complex (Hermitian) transpose of the matrix Q. Draw the flow graph 
for the sequence of operations described by QH. What does this structure compute? 
(c)  Determine (1/N)QH Q. 
9.59.  In many applications, there is a need to convolve long sequences x[n] and h[n] whose sam­
ples are integers. Since the sequences have integer coefficients, the result ofthe convolution 
y[n] = x[n] * h[n] will naturally also have integer coefficients as well. 
A major drawback of computing the convolution of integer sequences with FFTs is 
that floating-point arithmetic chips are more expensive than integer arithmetic chips. Also, 
rounding noise introduced during a floating-point computation may corrupt the result. In 
this problem, we consider a class of FFT algorithms known as number-theoretic transforms 
(NTIs), which overcome these drawbacks. 
(a)  Let x[n] and h[n] be N-point sequences and denote their DFTs by X[k] and H[k], 
respectively. Derive the circular convolution property of the DFT. Specifically, show 
that Y[k] = X[k]H[k], where y[n] is the N-point circular convolution of x[n] and h[n]. 
Show that the circular convolution property holds as long as WN in the DFT satisfies 
N-lL  W7/ = {N' k= 0, 
(P9.59-1)
0, k:;6 0. 
n=O 
The key to defining NTTs is to find an integer-valued WN that satisfies Eq. (P9.59­
1). This will enforce the orthogonality of the basis vectors required for the DFT to  
function properly. Unfortunately, no integer-valued WN exists that has this property  
for standard integer arithmetic.  
In order to overcome this problem, NTIs use integer arithmetic defined modulo some  
integer P. Throughout the current problem, we will assume that P = 17. That is,  
addition and multiplication are defined as standard integer addition and multiplication,  
followed by modulo P = 17 reduction. For example, «23+18)h7 = 7. «10+7))17 = 0,  
«23 x 18)h7 = 6, and «10 x 7)h7 = 2. (Just compute the sum or product in the normal  
way, and then take the remainder modulo 17.)  
(b)  Let P = 17, N = 4, and WN = 4. Verify that 
(('I:l W7/)) = {~' ~;~: 
n=O 
p 
(c)  Let x[n] and h[n] be the sequences 
x[n] = 8[n] + 28[n -
1] + 38[n - 2], 
h[n] = 38[n] + 8[n -
1]. 

790  
Chapter 9 
Computation of the Discrete Fourier Transform 
Compute the 4-point NIT X[k] of x[n] as follows: 
XI'] ~ ( (% Xln]WN') ) p 
Compute H[k] in a similar fashion. Also,compute Y[k] = H[k]X[k]. Assume the values 
of P, N, and WN given in part (a). Besure to use modulo 17arithmetic for each operation 
throughout the computation, not just for the final result! 
(d)  The inverse NTf of Y[kJ is defined by the equation 
yIn] ~ ( (N)-l EYI']W';;"') ) p 
(P9.59-2) 
In order to compute this quantity properly, we must determine the integers (1/N)-l 
and WN1 such that 
(((N)-lN))p = 1, 
((WNWNl))p = 1. 
Use the values of P, N, and W N given in part (a), and determine the aforesaid integers. 
(e)  Compute the inverse NTf shown in Eq. (P9.59-2) using the values of (N)-l and WN
1 
determined in part (d). Check your result by manually computing the convolution 
y[n] = x[n] * h[n]. 
9.60. Sections 9.2 and 9.3 focus on the fast Fourier transform for sequences where N is a power 
of 2. However, it is also possible to find efficient algorithms to compute the DFT when 
the length N has more than one prime factor, i.e., cannot be expressed as N = mV for 
some integer m. In this problem, you will examine the case where N = 6. The techniques 
described extend easily to other composite numbers. Burrus and Parks (1985) discuss such 
algorithms in more detaiL 
(0)  The key to decomposing the FFT for N 
6 is to use the concept of an index map, 
proposed by Cooley and Tukey (1965) in their original paper on the FFT. Specifically, 
for the case of N = 6, we will represent the indices nand k as 
n  
3nl +nz fornl = 0, 1; n2 
0,1,2; 
(P9.60-1) 
k =  kl + 2k2 for kl = 0, 1; k2 = 0, 1,2; 
(P9.60-2) 
Verify that using each possible value of nl and nz produces each value of n = 0, ... , 5 
once and only once. Demonstrate that the same holds for k with each choice of kl 
and k2. 
(b) Substitute Eqs. (P9.60-1) and (P9.60-2) into the definition of the DFT to get a new 
expression for the DFT in terms of n], n2, k}, and k2' The resulting equation should 
have a double summation over nl and n2 instead of a single summation over n. 
(c)  Examine the W6 terms in your equation carefully. You can rewrite some of these as 
equivalent expressions in W2 and W3' 
(d)  Based on part (c), group the terms in your DFT such that the nz summation is outside 
and the nl summation is inside. You should be able to write this expression so that 
it can be interpreted as three DFTs with N 
2, followed by some "twiddle" factors 
(powers of W6), followed by two N 
3 DFTs. 

791 
Chapter 9 
Problems 
(e)  Draw the signal flow graph implementing your expression from part (d). How many 
complex multiplications does this require? How does this compare with the number 
of complex multiplications required by a direct implementation of the DFT equation 
for N = 6? 
(f)  Find an alternative indexing similar to Eqs. (P9.60-1) and (P9.60-2) that results in a 
signal flow graph that is two N 
3 DFTs followed by three N 
2 DFfs. 

10  
Fou rier Analysis 
I~ 
of Signals Using the 
Discrete Fourier Transform 
10.0 INTRODUCTION 
In Chapter 8, we developed the discrete Fourier transform (DFT) as a Fourier repre­
sentation of finite-length signals. Because the DFT can be computed efficiently, it plays 
a central role in a wide variety of signal-processing applications, including filtering and 
spectrum analysis. In this chapter, we take an introductory look at Fourier analysis of 
signals using the DFT. 
In applications and algorithms based on explicit evaluation of the Fourier trans­
form, it is ideally the discrete-time Fourier transform (DTFT) that is desired, although 
it is the DFT that can actually be computed. For finite-length signals, the DFT provides 
frequency-domain samples of the DTFT, and the implications of this sampling must 
be clearly understood and accounted for. For example, as considered in Section 8.7, in 
linear filtering or convolution implemented by mUltiplying DFTs rather than DTFTs, 
a circular convolution is implemented, and special care must be taken to ensure that 
the results will be equivalent to a linear convolution. In addition, in many filtering and 
spectrum analysis applications, the signals do not inherently have finite length. As we 
will discuss, this inconsistency between the finite-length requirement of the DFT and 
the reality of indefinitely long signals can be accommodated exactly or approximately 
through the concepts of windowing, block processing, and the time-dependent Fourier 
transform. 
792 

793 
Section 10.1 
Fourier Analysis of Signals Using the OFT 
Continuous-to­
Antialiasing 
discrete-time
lowpass filter xAt) 
conversion 
'-------' 
HaaUQ'1 
w[n] 
Figure 10.1 
Processing steps in the discrete-time Fourier analysis of a 
continuous-time signal. 
10.1 FOURIER ANALYSIS OF SIGNALS USING THE DFT 
One of the major applications of the DFf is in analyzing the frequency content of 
continuous-time signals. For example, as we describe in Section 1004.1, in speech analysis 
and processing, frequency analysis is particularly useful in identifying and modeling 
the resonances of the vocal cavity. Another example, introduced in Section 1004.2, is 
Doppler radar, in which the velocity of a target is represented by the frequency shift 
between the transmitted and received signals. 
The basic steps in applying the DFT to continuous-time signals are indicated in 
Figure 10.1. The antialiasing filter is incorporated to eliminate or minimize the effect 
of aliasing when the continuous-time signal is converted to a sequence by sampling. 
The need for multiplication of x[n] by wEn], i.e., windowing, is a consequence of the 
finite-length requirement of the DFf. In many cases of practical interest, sc(t) and, con­
sequently, x[n] are very long or even indefinitely long signals (such as with speech or 
music). Therefore, a finite-duration window w[n] is applied to x[n] prior to computation 
of the DFf. Figure 10.2 illustrates the Fourier transforms of the signals in Figure 10.1. 
Figure 10.2( a) shows a continuous-time spectrum that tapers off at high frequencies but 
is not bandlimited. It also indicates the presence of some narrowband signal energy, 
represented by the narrow peaks. The frequency response of an antialiasing filter is il­
lustrated in Figure 10.2(b). As indicated in Figure 10.2( c), the reSUlting continuous-time 
Fourier transform Xc(jQ) contains little useful information about Sc(jQ) for frequen­
cies above the cutoff frequency of the filter. Since Haa(jQ) cannot be ideal, the Fourier 
components of the input in the passband and the transition band also will be modified 
by the frequency response of the filter. 
The conversion of xc(t) to the sequence of samples x[n] is represented in the fre­
quency domain by periodic replication, frequency normalization, and amplitude scaling 
i.e., 
. 
1 ~ (()) 
2:rrr)
X (e1W ) = T ~ Xc hi + jT . 
(10.1) 
r=-oo 
This is illustrated in Figure 1O.2(d). In a practical implementation, the anti aliasing filter 
cannot have infinite attenuation in the stopband. Therefore, some nonzero overlap of 
the terms in Eq. (10.1), i.e., aliasing, can be expected; however, this source of error can 
be made negligibly small either with a high-quality continuous-time filter or through the 
use of initial oversampling followed by more effective discrete-time lowpass filtering 
and decimation, as discussed in Section 4.8.1. If x[n] is a digital signal, so that AID 
conversion is incorporated in the second system in Figure 10.1, then quantization error 
is also introduced. As we have seen in Section 4.8.2, this error can be modeled as a 

Sc(jQ) 
0 
Q
-no 
no 
(a) 
Haa(jQ) 
I~ 'I "'-I
71' 
0 
11' 
Q 
T 
T
(b)
Lt\:,<:Q) 
71' 
-Q 
0 
71' 
Q
Qo 
T 
T
(c) 
~, ~X(~j~J 
-271' 
-11' 
-Wo 
0 
wo=QoT 
11' 
271' 
w 
(d) 
W(e IW ) 
~ 
A 
f 
-271' 
-71' 
0 
71' 
211' 
w 
(e) -+i I- 211' 
~j~:'::jJr
-211' 
-71' 
0 
11' 
211' 
W 
(f) 
Figure 10.2 Illustration of the Fourier transforms of the system of Figure 10.1. 
(a) Fourier transform of continuous-time input signal. (b) Frequency response of 
antialiasing filter. (c) Fourier transform of output of antialiasing filter. (d) Fourier 
transform of sampled signal. (e) Fouriertransform of window sequence. (f) Fourier 
transform of windowed signal segment and frequency samples obtained using DFT 
samples. 
794 

795 
Section 10.1 
Fourier Analysis of Signals Using the OFT 
noise sequence added to x[n]. The noise can be made negligible through the use of 
fine-grained quantization. 
The sequence x[n] is typically multiplied by a finite-duration window w[n], since 
the input to the DFf must be offinite duration. This produces the finite-length sequence 
urn] 
w[n]x[n]. The effect in the frequency domain is a periodic convolution, i.e., 
i
1C 
V(ejW) = ~ 
X (e j9 )W(ej (w-9»dfl. 
(10.2)
2rr 
-1C 
Figure 10.2( e) illustrates the Fourier transform of a typical window sequence. Note that 
the main lobe is assumed to be concentrated around w 
0, and, in this illustration, the 
side lobes are very small, suggesting that the window tapers at its edges. The properties 
of windows such as the Bartlett, Hamming, Hanning, Blackman, and Kaiser windows 
are discussed in Chapter 7 and in Section 10.2. At this point, it is sufficient to observe that 
convolution of W(e jW ) with X (e jW) will tend to smooth sharp peaks and discontinuities 
in X (ejW ). This is depicted by the continuous curve plotted in Figure 1O.2(f). 
The final operation in Figure 10.1 is the computation of the DFf. The DFf of the 
windowed sequence urn] = w[n]x[n] is 
N-l 
V[k] = L u[n]e-j (21C/N)kn, 
k 
0,1, ... , N 
1, 
(10.3) 
n=O 
where we assume that the window length L is less than or equal to the DFf length N. 
V [k], the DFf of the finite-length sequence u[n], corresponds to equally spaced samples 
of the DTFf of urn]; Le., 
(lOA) 
Figure 10.2(f) also shows V[k] as the samples of V(e jCV). Since the spacing between 
DFf frequencies is 2rr / N, and the relationship between the normalized discrete-time 
frequency variable and the continuous-time frequency variable is w = nT, the DFT 
frequencies correspond to the continuous-time frequencies 
2rrk 
nk=~. 
(10.5)
NT 
The use of this relationship between continuous-time frequencies and DFT frequencies 
is illustrated by Examples 10.1 and 10.2. 
Example 10.1 
Fourier Analysis Using the DFT 
Consider a bandlimited continuous-time signal xc(t) such that Xc(jfJ) 
0 for IfJl ::: 
211"(2500). We wish to use the system of Figure 10.1 to estimate the continuous-time 
spectrum Xc (jfJ). Assume that the antialiasing filter Haa (jfJ) is ideal, andthe sampling 
rate for the CfD converter is liT 
5000 samples/so If we want the DFf samples V[k] 
to be equivalent to samples of XdjfJ) that are at most 211"(10) radls or 10 Hz apart, 
what is the minimum value that we should use for the DFf size N? 
From Eq. (10.5), we see that adjacent samples in the DFf correspond to 
continuous-time frequencies separated by 211"I (NT). Therefore, we require that 
2rr 
-
< 20rr,
NT ­

796 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
which implies that 
N:::: SOO 
satisfies the condition. If we wish to use a radix-2 FFf algorithm to compute the DFf 
in Figure 10.1, we would choose N = S12 for an equivalent continuous-time frequency 
spacing of L'1Q 
2n(SOOO/S12) = 2n(9.77) rad/s. 
Example 10.2 Relationship Between DFT Values 
Considerthe problem posed in Example 10.1, in which liT = SOOO, N = S12,andxc(t) 
is real-valued and is sufficiently bandlimited to avoid aliasing with the given sampling 
rate. If it is determined that V[I1] 
2000(1 
j), what can be said about other values 
of V[k] or about Xc(jQ)? 
Referring to the symmetry properties of the DFf given in Table 8.2, V[k] = 
V*[« -k»N], k = 0, 1, ... , N 
1, and consequently, V[N 
k] = V*[k], so it follows 
in this case that 
V[S12 -11] 
V[S01] 
V*[ll] 
2000(1 
j). 
We also know that the DFf sample k = 11 corresponds to the continuous-time fre­
quency Qn 
2n(1l) (SOOO)/S12 
2n(107.4), and similarly, k 
SOl corresponds to 
the frequency -2n(1l)(SOOO)/512 = -27!'(107.4). Although windowing smooths the 
spectrum, we can say that 
XdjQ1l) = Xc(j27!'(107.4» ~ T· V[ll] = 0.4(1 + j). 
Note that the factor T is required to compensate for the factor liT introduced by 
sampling, as in Eq. (10.1). We can again exploit symmetry to conclude that 
Xc(- j Q ll) 
Xc( - j27!'(107.4» ~ T . V*[I1] = 0.4(1 
j). 
Many commercial real-time spectrum analyzers are based on the principles em­
bodied in Figures 10.1 and 10.2. It should be clear from the preceding discussion, how­
ever, that numerous factors affect the interpretation the DFT of a windowed segment 
of the sampled signal in terms of the continuous-time Fourier transform of the original 
input seCt). To accommodate and mitigate the effects of these factors, care must be taken 
in filtering and sampling the input signal. Furthermore, to interpret the results correctly, 
the effects of the time-domain windowing and of the frequency-domain sampling in­
herent in the DFT must be clearly understood. For the remainder of the discussion, 
we will assume that the issues of antialiasing filtering and continuous-to-discrete-time 
conversion have been satisfactorily handled and are negligible. In the next section, we 
concentrate specifically on the effects of windowing and of the frequency-domain sam­
pling imposed by the DFT. We choose sinusoidal signals as the specific class of examples 
to discuss, because sinusoids are perfectly bandlimited and they are easily computed. 
However, most of the issues raised by the examples apply more generally. 

--- --- -
191 
Section 10.2 
OFT Analysis of Sinusoidal Signals 
10.2 DFT ANALYSIS OF SINUSOIDAL SIGNALS 
The DTFf of a sinusoidal signal A cos(won + ¢) (existing for all n) is a pair of impulses 
at +wo and -wo (repeating periodically with period 2:rr). In analyzing sinusoidal signals 
using the DFf, windowing and spectral (frequency-domain) sampling have important 
effects. As we will see in Section 10.2.1, windowing smears or broadens the impulses of 
the Fourier representation, thus, the exact frequency is less sharply defined. Windowing 
also reduces the ability to resolve sinusoidal signals that are close together in frequency. 
The spectral sampling inherent in the DFf has the effect of potentially giving a mis­
leading or inaccurate picture of the true spectrum of the sinusoidal signal. 1his effect is 
discussed in Section 10.2.3. 
10.2.1 The Effect of Windowing 
Consider a continuous-time signal consisting of the sum of two sinusoidal components; 
i.e., 
-00 < t < 00. 
(10.6) 
Assuming ideal sampling with no aliasing and no quantization error, we obtain the 
discrete-time signal 
-00 < n < 00, 
(10.7) 
where wo = QoT and WI 
Ql T. The windowed sequence v[n] in Figure 10.1 is then 
(10.8) 
To obtain the DTFf of v[n], we can expand Eq. (10.8) in terms of comp1ex exponentials 
and use the frequency-shifting property of Eq. (2.158) in Section 2.9.2. Specifically, we 
rewrite v[n] as 
Ao 
e
· 
Ao 
'0'
v[n] = -w[n]eJ 0eJwon + -w[n]e-J °e-JaJon 
2 
2 
(10.9) 
Al 
~. 
Al 
~.
+ Tw[n]eJ leJWJ.n + Tw[n]e-J le-Jw1n , 
from which, with Eq. (2.158), it follows that the Fourier transform of the windowed 
sequence is 
(10.10) 
+ Al ejo] W{e}(w-Wj) + Al 
2 
2 
According to Eq. (10.10), the Fourier transform of the windowed signal consists of the 
Fourier transform of the window, shifted to the frequencies ±wo and ±Wl and sca1ed 
by the complex amplitudes of the individual complex exponentials that make up the 
signaL 

798 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
Example 10.3 Effect of Windowing on Fourier Analysis of 
Sinusoidal Signals 
In this example, we consider the system of Figure 10.1 and, in particular, W(ejW ) 
and V(e jW ) for sc<t) of the form of Eq. (10.6), a sampling rate l/T 
10 kHz and a 
rectangular window w[n] oflength 64. The signal amplitude and phase parameters are 
Ao = 1, A1 = 0.75, and 80 = 81 = 0, respectively. To illustrate the essential features, 
we specifically display only the magnitudes of the Fourier transforms. 
IW(eiW)1 
64 
32 
-'11' 
o 
'11' 
W 
(a) 
Figure 10.3 Illustration of Fourier analysis of windowed cosines with arectangu­
larwindow. (a) Fourier transform of window. (b)-(e) Fouriertransform of windowed 
cosines as 01 - 00 becomes progressively smaller. (b) 00 
(21T/6) x 104, 
01 = (21T/3) x 104. 
lV(eiW)1 
32 
16 

799 
Section 10.2 
OFT Analysis of Sinusoidal Signals 
32 
16 
30 
21r
"'0= 14 
15 
21r 
"'1 = 12 
-1r 
o 
1T 
'" 
(d) 
40 
20 
-1r 
0 
1r", 
(e) 
Figure 10.3 (continued) (c) QO = (2rr/14) x 104, Q1 = (4rr/15) x 104. 
(d) QO = (2rr/14) x 104, Q1 = (2rr/12) x 104. (e) Qo = (2rr/14) x 104, 
Q1 = (471"/25) x 104. 

Chapter 10 
Fourier AnalYSis of Signals Using the Discrete Fourier Transform
800 
In Figure 10.3(a), we show !W(ej(v)" and in Figures 10.3(b), (c), (d), and (e), we 
show W(ejW)1 for several choices of Qo and QJ in Eq. (10.6) or, equivalently, wo and 
WI in Eq. (10.7). In Figure 10.3(b), QO 
(2n/6) x 104 and Q I 
(2n/3) x 104 , or, 
equivalently,wo = 2n/6 and WI = 2n/3. In Figure 10.3(c)-(e), the frequencies become 
progressively closer. For the parameters in Figure lO.3(b), the frequency and amplitude 
of the individual components are evident. Specifically, Eq. (10.10) suggests that, with 
no overlap between the replicas of W (e JW ) at wo and WI, there will be a peak of height 
32Ao at wo and 32A1 at WI, since W (eJW ) has a peak height of 64. In Figure lO.3(b), the 
two peaks are at approximately wo = 2n/6 and WI 
2n/3, with peak amplitudes in 
the correct ratio. In Figure 10.3(c), there is more overlap between the window replicas 
at wo and WI. and while two distinct peaks are present, the amplitude of the spectrum 
at W = W() is affected by the amplitude of the sinusoidal signal at frequency WI and vice 
versa. This interaction is called leakage: The component at one frequency leaks into 
the vicinity of another component owing to the spectral smearing introduced by the 
window. Figure 1O.3(d) shows the case where the leakage is even greater. Notice how 
side lobes adding out of phase can reduce the heights of the peaks. In Figure 1 O.3(e), 
the overlap between the spectrum windows at wo and WI is so significant that the two 
peaks visible in (b)-(d) have merged into one. In other words, with this window, the 
two frequencies corresponding to Figure lO.3(e) will not be resolved in the spectrum. 
10.2.2 Properties of the Windows 
Reduced resolution and leakage are the two primary effects on the spectrum as a result 
of applying a window to the sinusoidal signal. The resolution is influenced primarily by 
the width of the main lobe of W(e JW ), whereas the degree of leakage depends on the 
relative amplitude of the main lobe to the side lobes of W(eJW ). In Chapter 7, in a filter 
design context, we showed that the width of the main lobe and the relative side-lobe 
amplitude depend primarily on the window length L and the shape (amount of tapering) 
of the window. The rectangular window, which has Fourier transform 
L-l 
. 
2 
jW 
'" - Jwn _ 
- jwCL-lJ/2 sm(wLI )
Wr(e
) 
~e 
-e 
, 
(10.11)
sin(w/2)
n=O 
has the narrowest main lobe for a given length (t:.ml = 411: 1L), but it has the largest side 
lobes of all the commonly used windows. Other windows discussed in Chapter 7 include 
the Bartlett, Hann, and Hamming windows. The DTFTs of all these windows have 
main-lobe width t:.ml = 811:1 (L -1), which is approximately twice that of the rectangular 
window, but they have significantly smaller side-lobe amplitudes. The problem with all 
these windows is that there is no possibility of trade-off between main-lobe width and 
side-lobe amplitude, since the window length is the only variable parameter. 
As we saw in Chapter 7, the Kaiser window is defined by 
0:::: n:::: L 
1, 
(10.12)
WK[n] = I 
0, 
otherwise, 
where a 
(L - 1)/2 and 100 is the zeroth-order modified Bessel function of the first 
kind. (Note that the notation of Eq. (10.12) differs slightly from that of Eq. (7.72) in 

Section 10.2 
DFT Analysis of Sinusoidal Signals 
801 
that L denotes the length of the window in Eq. (10.12), whereas the length of the filter 
design window in Eq. (7.72) is denoted M + 1.) We have already seen in the context of 
the filter design problem that this window has two parameters, 13 and L, which can be 
used to trade between main-lobe width and relative side-lobe amplitude. (Recall that 
the Kaiser window reduces to the rectangular window when 13 = 0.) The main-lobe 
width 8 m! is defined as the symmetric distance between the central zero-crossings. The 
relative side-lobe level As) is defined as the ratio in dB of the amplitude of the main lobe 
to the amplitude of the largest side lobe. Figure 10.4, which is a duplicate of Figure 7.32, 
shows Fourier transforms of Kaiser windows for different lengths and different values 
of {3. In designing a Kaiser window for spectrum analysis, we want to specify a desired 
value of As! and determine the required value of {3. Figure 1O.4(c) shows that the relative 
side-lobe amplitude is essentially independent of the window length and thus depends 
only on 13. This was confirmed by Kaiser and Schafer (1980), who obtained the following 
least squares approximation to 13 as a function of As!: 
0, 
As! :::: 13.26, 
{3 
0.76609(Asl 
13.26)oA + 0.09834(As! - 13.26), 
13.26 < As] :::: 60, 
(10.13)
10.12438(Asl + 6.3), 
60 < As] :::: 120. 
Using values of 13 from Eq. (10.13) gives windows with actual side-lobe values that 
differ by less than 0.36 from the value of ASl used in Eq. (10.13) for the entire range of 
13.26 < As! < 120. (Note that the value 13.26 is the relative side-lobe amplitude of the 
rectangular window, to which the Kaiser window reduces for 13 = 0.) 
Figure 1O.4(c) also shows that the main-lobe width is inversely proportional to 
the length of the window. The trade-off between main-lobe width, relative side-lobe 
amplitude, and window length is displayed by the approximate relationship 
+ 12)
L :::: --lS-S-=8--]- + 1, 
(10.14) 
m 
which was also given by Kaiser and Schafer (1980). 
Equations (10.12), (10.13), and (10.14) are the necessary equations for determin­
ing a Kaiser window with desired values of main-lobe width and relative side-lobe 
amplitude. To design a window for prescribed values of ASl and 8 m[ requires simply 
the computation of 13 from Eq. (10.13), the computation of L from Eq. (10.14), and the 
computation of the window using Eq. (10.12). Many of the remaining examples of this 
chapter use the Kaiser window. Other spectrum analysis windows are considered by 
Harris (1978). 
10.2.3 The Effect of Spectral Sampling 
As mentioned previously, the DFT of the windowed sequence v[nl provides samples of 
V(eiw) at the N equally spaced discrete-time frequencies Wk. 
2rrk/N, k = 0,1, ... , 
N - 1. These are equivalent to the continuous-time frequencies Qk 
(2rrk)/(NT), 
for k 
0,1, ... , N /2 (assuming that N is even). The indices k = N /2 + 1, ... , N 
1 
correspond to the negative continuous-time frequencies -2rr(N 
k)/(NT). Spectral 
sampling, as imposed by the DFT, can sometimes produce misleading results. This effect 
is best illustrated by example. 

lo2[ 
L.________________~-;.;?~>~-~~",~.-,--------------~ 
09 L 
./ " 
'\ " 
. 
. " 
'\ . 
. / 
I 
'\ '. 
~ 
./ 
I 
'\ 
'. 
--~=O
E 
" 
I 
\ 
,
• 
I 
\ 
• 
----- ~ 3
lO.6 
/ 
I 
\ 
~ 
----~=6
." 
I 
\ 
'.
" 
/ 
'\ 
,
. 
/ 
'\ 
" 
f-
/ 
/ 
, 
•
0.3.,. 
,. 
, 
' 
. 
"'~ ,/ 
// 
,," 
0 
5 
10 
15 
20  
Samples  
(a) 
0 
-25 
I 
I ~: II ,'''V 1".\1 _.\1 . \I \I \I \1 \11 
--~ 0 
I::Q 
-----~=3 
"0 
I W-, ~ X Ii V 1f f 
----~ 6
-'"I 
I 
, 
.' "'" 
I
'tl 
l \ 
I
I 
~ 
"• I I,I I \"I• \,/'!!.I \ . 
• 
, 
f 
I 
-75 
-100 
0 
0.27T 
0.47T 
0.67T 
0.87T 
7T 
Radian frequency (Ctl) 
(b) 
-50 
-75 
-1001 
II I i I 
I" 
-, 
--L 11 
---.- L =21 
ra 
---- L 
41 
o 
0.27T 
0.47T 
Radian frequency (ll)) 
(c) 
0.67T 
O.87T 
7T 
Figure 10.4 (a) Kaiser windows for fJ 
0,3, and 6and L 
21. (b) Fourier trans­
form corresponding to windows in (a). (c) Fourier transforms of Kaiser windows 
with fJ 
6 and L= 11,21, and 41. 
802 

803 
Section 10.2 
OFT Analysis of Sinusoidal Signals 
Example 10.4 Illustration of the Effect of Spectral Sampling 
Consider the same parameters as in Figure lO.3(c) in Example 10.3, i.e., Ao = 1, 
Al 
0.75, WO = 21l'/14, (VI 
4n/15, and 81 = 82 = 0 in Eq. (10.8). wIn] is a 
rectangular window of length 64. Then 
u[n] Icos G~ n) + 0.75 cos C;n), 0::: n ::: 63, 
(10.15) 
0, 
otherwise. 
Figure 1O.5(a) shows the windowed sequence urn]. Figures 1O.5(b), (c), (d), and (e) 
show the corresponding real part, imaginary part, magnitude, and phase, respectively, 
of the DFT of length N 
64. Observe that since x[n] is real, X[N 
kj = X*[kl and 
2 
v[nJ 
., 
"0::; 
.j:; 
0
0.. 
n
Ei « 
-1 
-2 0 
32 
64 
(a) 
40 
Re(V[kJl. Re(V(d"'») 
30 
~
., 
"0 
20 
I 
:::I 
\ 
~ 10
Ei « 
",A It",. Ii, 
..........., 
.~d rl",~ IAi\.;
0 
U 
Vv 
' V 
k,wNI27r 
-10 0 
32 
64 
(b) 
Im(lV[kJI), Im(V(dW»)  
20  
., 
"0 
~ 
0 
k.wNI27r
8 « 
-20 
64
0 
32 
(c) 
Figure 10.5 Cosine sequence and OFT with a rectangular window for N = 64. 
(a) Windowed signal. (b) Real part of OFT. (c) Imaginary part of OFT. Note that the 
OTFT is superimposed as the light continuous line. 

804 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
40 
IVlk]I.IV(eiW)1 
30 
~ 
.g 
20
.E 
0.. 
10
E « 
o 
.wNI27T 
-10L-----------------~----------------~-­
o 
32 
64 
(d) 
4 
ARG{Vlk]]. ARG(Vl(ejW ») 
2 
'1) -g 
o
0.. 
E « 
32 
64 
(e) 
Figure 10.5 (continued) (dl Magnitude of Off. (e) Phase of Off. 
X(ej (2rr-w» 
X*(ejW); i.e., the real part and the magnitude are even functions and 
the imaginary part and phase are odd functions of k and w. 
In Figures lO.5(b )-(e), the horizontal (frequency) axis is labeled in terms ofthe 
OFf index or frequency sample number k. The value k 
32 corresponds to W = 77: 
or, equivalently, Q 
77:IT. As is the usual convention in displaying the OFf of a 
time sequence, we display the OFf values in the range from k = 0 to k 
N - 1, 
corresponding to displaying samples of the OTFf in the frequency range 0 to 277:. 
Because of the inherent periodicity of the OTFf, the first half of this range corresponds 
to the positive continuous-time frequencies, i.e., Q between zero and 77:IT, and the 
second half of the range to the negative frequencies, Le., Q between -77:/ T and zero. 
Note the even periodic symmetry of the real part and the magnitude and the odd 
periodic symmetry of the imaginary part and the phase. 
Recall that the OFf V[k] is a sampled version of the OTFf V (el "'). Superim­
posed on each OFf with a light gray line in Figures lO.5(b)-(e) is the corresponding 
OTFf, i.e., Re(V(ejW )}, Im(V(e jW )}, W(eiW)I, and ARG{V(ejW )} respectively. The 
frequency scale for these functions is the specially defined normalized scale denoted 
wN 1(277:); i.e., N on the OFf index scale corresponds to w 
277: on the conventional 
frequency scale of the OTFf. We also follow this convention of superimposing the 
OTFT in Figures lO.6, 10.7.10.8, and lO.9. 
The magnitude of the OFT in Figure 10.5( d) corresponds to samples of IV (eiw)I 
(the light continuous line), which shows the expected concentration around WI 
277:/7.5 and wo 
277: /14, the frequencies of the two sinusoidal components of the 
input. Specifically, the frequency WI 
477:115 
277:(8.533 .. .)/64 lies between the OFf 
samples corresponding to k = 8 and k = 9. Likewise, the frequency W() = 277:/14 = 
277:(4.5714 ...)/641ies between the OFT samples corresponding to k = 4 and k 
5. 
Note that the frequency locations of the peaks of the gray curve in Figure 10.5(d) 

805 
Section 10.2 
OFT Analysis of Sinusoidal Signals 
are between spectrum samples obtained from the DFT. In general, the locations of 
peaks in the D FT values do not necessarily coincide with the exact frequency locations 
of the peaks in the DTFT, since the true spectrum peaks can lie between spectrum 
samples. Correspondingly, as evidenced in Figure lO.5(d), the relative amplitudes of 
peaks in the DFT will not necessarily reflect the relative amplitudes of the spectrum 
peaks of W(ejW)I. 
Example 10.5 Signal Frequencies Matching DFT 
Frequencies Exactly 
Consider the sequence 
o::: n ::: 63,
v[nl = Icos (~~ n) + 0.75 cos C; n), 
(10.16) 
0, 
otherwise, 
as shown in Figure 1O.6(a). Again, a rectangular window is used with N 
L = 64. This 
is very similar to the previous example, except that in this case, the frequencies of the 
cosines coincide exactly with two of the DFT frequencies, Specifically, the frequency 
WI = 2rr/8 
2rr8/64 corresponds exactly to the DFT sample k = 8 and the frequency 
WO = 2rr/16 =2rr4/64 to the DFT sample k 
4. 
The magnitude of the 64-point DFT of v[n] for this example is shown in Fig­
ure 1O.6(b) and corresponds to samples of W(ejW)1 (which again is superimposed 
v[n]
2 
:!) 
"0 
::I 
.';::: 
Q, 
0 
8 
<: 
-1 
-2 
0 
32 
64 
(a) 
40 
lV(k]I,IV(dW)1 
30 
;.;  
"0  20
.E 
I
Q, 
8 
10
<:  
0  
k,wN!2'1T 
-10  
0 
32  
64 
(b) 
Figure 10.6 Oiscrete Fourier analysis of the sum of two sinusoids for a case 
in which the Fourier transform is zero at all OFT frequencies except those cor­
responding to the frequencies of the two sinusoidal components. (a) Windowed 
Signal. (b) Magnitude of OFT. Note that (I V(efrv)l) is superimposed as the light 
continuous line. 
n 

806 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
with a light line) at a frequency spacing of 2n /64. Although the signal parameters in 
Example 10.4 are very similar, the appearance of the DFT is for this example and strik­
ingly different. In particular, for this example, the DFT has two strong spectral lines at 
the samples corresponding to the frequencies of the two sinusoidal components in the 
signal and no frequency content at the other DFT values. In fact, this clean appear­
ance of the D FT in Figure 1O.6(b) is largely an illusion resulting from the sampling of 
the spectrum. Comparing Figures 10.6(b) and (c), we can see that the reason for the 
clean appearance of Figure 1O.6(b) is that for this choice of parameters, the Fourier 
transform is exactly zero at the frequencies that are sampled by the DFT, except those 
corresponding to k 
4,8,64 - 8, and 64 - 4. Although the signal of Figure 1O.6(a) 
has significant content at almost all frequencies, as evidenced by the gray curve in 
Figure 1O.6(b), we do not see this in the DFT, because of the sampling of the spectrum. 
Another way of viewing this is to note that the 64-point rectangular window selects 
exactly an integer number of periods of the two sinusoidal components in Eq. (10.16). 
The 64-point DFT then corresponds to the DFS of this signal replicated with period 
64. This replicated signal will have only four nonzero DFS coefficients corresponding 
to the two sinusoidal components on Eq. (10.16). This is an example of how the inher­
ent assumption of periodicity gives a correct answer to a different problem. We are 
interested in the finite-length case and the results are quite misleading from that point 
of view. 
To illustrate this point further, we can extend v[n] in Eq. (10.16) by zero-padding 
to obtain a 128-point sequence. The corresponding 128-point DFT is shown in Fig­
ure 10.7. With this finer sampling of the spectrum, the presenee of significant content 
at other frequencies becomes apparent. In this case, the windowed signal is not natu­
rally periodic with period 128. 
\lI{k]l, lV(eiw)1
40 r . 
0) 
30 
"0 E 
~ 20 
e 
-< 
10 
0~111»J11~11~1~ 
64 
128 
Figure 10.7 
DFT of the Signal as in Figure 10.6(a), but with twice the number of 
frequency samples used in Figure 10.6(b). 
In Figures 10.5, 10.6, and 10.7, the windows were rectangular. In the next set of 
examples, we illustrate the effect of different choices for the window. 
Example 10.6 DFT Analysis of Sinusoidal Signals Using 
a Kaiser Window 
In this example we return to the frequency, amplitude, and phase parameters of Ex­
ample lOA, but now with a Kaiser window applied, so that 
v[n] 
WK[n] cos G: n) + O.75wKln] cos C~n), 
(10.17) 

807 
Section 10.2 
OFT Analysis of Sinusoidal Signals 
where wK[n] is the Kaiser window as given by 
(10.12). We will select the Kaiser 
window parameter f3 to be equal to 5.48, which, according to Eq. (10.13), results in a 
window for which the relative side-lobe amplitude is Asl = 40 dB. Figure lO.8(a) shows 
2 
v[n] 
_2L------------------L------------------~ 
o 
32 
64 
(a) 
20 
lV[kJI, lV(eiw)1 
J 
k,wNI2.
~ \ 
32 
64 
(b) 
v[nJ
2 
n 
-1 
-2 L-____________________________________L-­
o 
32 
(c) 
10 
I 
III 
'\ 
\ 
lV[k]1 IV( . )1
eJW 
) 
f' 
(,/ \ 
~ 
k ,wN!2rr 
o o 
16 
32 
(d) 
Figure 10.8 Discrete Fourier analysis with Kaiser window. (a) Windowed se­
Quence for L= 64. (b) Magnitude of OFT for L 
64. (c) Windowed sequence for 
L = 32. (d) Magnitude of OFT for L 
32. 

808 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
the windowed sequence v[n] for a window length of L 
64, and Figure 10.8(b) shows 
the magnitude ofthe corresponding DFf. From Eq. (10.17), we see that the difference 
between the two frequencies is Wt -wO = 2]'[/7.5 -2]'[/14 
0.389. From Eq. (1O.14),it 
follows that the width of the main lobe of the Fourier transform of the Kaiser window 
with L = 64 and f3 
5.48 is L},ml = 0.401. Thus, the main lobes of the two replicas 
of W K (e jW ) centered at Wo and WI will just slightly overlap in the frequency interval 
between the two frequencies. This is evident in Figure 10.8(b), where we see that the 
two frequency components are clearly resolved. 
Figure lO.8( c) shows the same signal, multiplied by a Kaiser window with L = 32 
and f3 = 5.48. Since the window is half as long, we expect the width of the main 
lobe of the Fourier transform of the window to double. and Figure 1O.8(d) confirms 
this. Specifically, 
(10.13) and (10.14) confirm that for L 
32 and f3 = 5,48, 
the main-lobe width is L},ml 
0.815. Now, the main lobes of the two copies of the 
Fourier transform of the window overlap throughout the region between the two 
cosine frequencies, and we do not see two distinct peaks. 
In all the previous examples except in Figure 10.7. the DFT length N was equal 
to the window length L. In Figure 10.7, zero-padding was applied to the windowed 
sequence before computing the DFT to obtain the Fourier transform on a more finely 
divided set of frequencies. However, we must realize that this zero-padding will not 
improve the ability to resolve close frequencies, which depends on the length and shape 
of the window. This is illustrated by the next example. 
Example 10.7 DFT Analysis with 32-point Kaiser Window 
and Zero-Padding 
In this example, we repeat Example 10.6 using the Kaiser window with L 
32 and 
f3 = 5.48, and with the DFf length varying. Figure 1O.9(a) shows the DFf magnitude 
for N = L = 32 as in Figure 1O.8(d), and Figures lO.9(b) and (c) show the DFT 
magnitude again with window length L = 32, but with DFf lengths N 
64 and N = 
128, respectively. As with Example 10.5, this zero-padding of the 32-point sequence 
results in finer spectral sampling of the DTFT. As shown by the light continuous 
curve, the underlying envelope of each DFT magnitude in Figure 10.9 is the same. 
Consequently, increasing the DFf size by zero-padding does not change the ability to 
resolve the two sinusoidal frequency components, but it does change the spacing of the 
frequency samples. If N were increased beyond 128, the dots denoting the DFf sample 
10 
IV[kJLIV(dw)1 
.g 
E 
:g, 
5  
E  
<:: 
lk wN127T 
~ 
o o 
16 
32 
(a) 
Figure 10,9 
IIIustrati0nof effect of OFT length for Kaiser window of length L 
32. 
(a) Magnitude of OFT for N 
32. 

809 
Section 10.2 
OFT Analysis of Sinusoidal Signals 
10 
5 
10 
Q) 
"0::: 
'R 5 
8 
'< 
0 0 
lV[k]l, !V(ei"')I 
32 
(b) 
!V[k]l, !V(eiw)1 
64 
k,wNI21T 
k,wNI21T 
128 
(c) 
Figure 10.9 (continued) (b) Magnitude of OFT for N = 64. (c) Magnitude of OFT 
for N 
128, 
values would tend to merge together and become indistinct. Consequently, DIT values 
are often plotted by connecting consecutive points by straight-line segments without 
indicating each individual point. For example, in Figures 10.5 through 10.8, we have 
shown a light continuous line as the DTFf W(e.i''')1 of the finite-length sequence 
v[n]. In fact, this curve is a plot of the DFT of the sequence after zero-padding to 
N = 2048. In these examples, this sampling of the DTFT is sufficiently dense so as to 
be indistinguishable from the function of the continuous variable w. 
For a complete representation of a sequence of length L, the L-point DFI is suf­
ficient, since the original sequence can be recovered exactly from it. However, as we 
saw in the preceding examples, simple examination of the L-point DFI can result in 
misleading interpretations. For this reason, it is common to apply zero-padding, so that 
the spectrum is sufficiently oversampled and important features are therefore read­
ily apparent. With a high degree of time-domain zero-padding or frequency-domain 
oversampling, simple interpolation (e.g., linear interpolation) between the DFT val­
ues provides a reasonably accurate picture of the Fourier spectrum, which can then be 
used, for example, to estimate the locations and amplitudes of spectrum peaks, This is 
illustrated in the following example. 
Example 10.8 Oversampling and Linear Interpolation for 
Frequency Estimation 
Figure 10.10 shows how a 2048-point DFT can be used to obtain a finely spaced eval­
uation of the Fourier transform of a windowed signal and how increasing the window 
width improves the ability to resolve closely spaced sinusoidal components. The signal 

810 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
of Example 10.6 having frequencies 21T /14 and 4rr/15 was windowed with Kaiser win­
dows of lengths L = 32, 42, 54, and 64 with f3 = 5.48. First, note that in all cases, the 
2048-point DFT gives a smooth result when the points are connected by straight lines. 
In Figure to.l0(a), where L = 32, the two sinusoidal components are not resolved, 
and, of eourse, increasing the DFT length will only result in a smoother curve. As the 
window length increases from L 
32 to L = 42, however, we see improvement in our 
ability to distinguish the two frequencies and the approximate relative amplitudes of 
each sinusoidal component. Tne dashed lines in all the figures indicate the DFT indices 
kO = 146 "" 2048/14 and kl = 273 "" 4096/15, which correspond to the nearest DFT 
frequencies (N = 2048) for the cosine components. Note that the 2048-point DFT in 
Figure 1O.1O(c) would be much more effective for precisely locating the peak of the 
windowed Fourier transform than the coarsely sampled DFT in Figure 1 0.8(b), which 
is also computed with a 64-point Kaiser window. Note also that the amplitudes of the 
two peaks in Figure 10.10 are very close to being in the correct ratio of 0.75 to 1. 
1024 
2048 
(a) 
1024 
2048 
(b) 
1\ 
A 
1024 
2048 
(c) 
Figure 10.10 
Illustration of the computation of the OFT for N » L with linear 
interpolation to create a smooth curve: (a) N 
1024, L = 32. (b) N = 1024, 
L = 42. (c) N = 1024, L 
64. (The values ko 
146 "" 2048/14 and 
k1 = 273 "" 4096/15 are the closest OFT frequencies to cuo = 21T/14 and 
CU1 = 4rr/15 when the OFT length is N = 2048.) 

811 
Section 10.3 
The Time-Dependent Fourier Transform 
10.3 THE TIME-DEPENDENT FOURIER TRANSFORM 
In Section 10.2, we illustrated the use of the DFT for obtaining a frequency-domain 
representation of a signal composed of sinusoidal components. In that discussion, we 
assumed that the frequencies of the cosines did not change with time, so that no mat­
ter how long the window, the signal properties (amplitudes, frequencies, and phases) 
would be the same from the beginning to the end of the window. Long windows give 
better frequency resolution, but in practical applications of sinusoidal signal models, 
the signal properties (e.g., amplitude, frequency) often change with time. For example, 
nonstationary signal models of this type are required to describe radar, sonar, speech, 
and data communication signals. This conflicts with the use of long analysis windows. 
A single DFT estimate is not sufficient to describe such signals, and as a result, we 
are led to the concept of the time-dependent Fourier transform, also referred to as the 
short-time Fourier transform.1 
We define the time-dependent Fourier transform of a signal x[n] as 
X[n, A) = I:
00 
x[n + m]w[m]e-j>.m, 
(10.18) 
m=-oc 
where w[n] is a window sequence. In the time-dependent Fourier representation, the 
one-dimensional sequence x [n], a function of a single discrete variable, is converted into 
a two-dimensional function of the time variable n, which is discrete, and the frequency 
variable A, which is continuous.2 Note that the time-dependent Fourier transform is 
periodic in Awith period 2rc; therefore, we need consider only values ofAfor 0 ::: A < 2rc 
or any other interval of length 2rc. 
Equation (10.18) can be interpreted as the DTFT of the shifted signalx[n +m], as 
viewed through the window w[m]. The window has a stationary origin, and as n changes, 
the signal slides past the window, so that at each value of n, a different portion of the 
signal is extracted by the window for Fourier analysis. As an illustration, consider the 
following example. 
Example 10.9 Time-Dependent Fourier Transform of a 
Linear Chirp Signal 
A continuous-time linear chirp signal is defined as  
xc(t) = cos(B(t» 
cos(Ao/2), 
(10.19)  
1 Further discussion of the time-dependent Fourier transform can be found in a variety of references, 
including Allen and Rabiner (1977), Rabiner and Schafer (1978), Crochiere and Rabiner (1983) and Quatieri 
(2002). 
2We denote the frequency variable of the time-dependent Fourier transform by A to maintain a dis­
tinction from the frequency variable of the conventional DTFI: which we always denote by w. We use the 
mixed bracket-parenthesis notation X[n, A) as a reminder that n is a discrete variable, and A is a continuous 
variable. 

812 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
where Ao has units of radians/s2 . (Such signals are called chirps because, in the auditory 
frequency range, short pulses sound like bird chirps.) The signal xC<t) in Eq. (10.19) is 
a member of the more general class of frequency modulation (FM) signals for which 
the instantaneous frequency is defined as the time derivative of the cosine argument 
B(t). Therefore, in this case, the instantaneous frequency is 
dB(t) 
d ( 
2)
Qi(t) = --
Aot 
= 2Aot, 
(10.20)
dt 
dt 
which varies in proportion to time; hence, the designation as a linear chirp signal. If 
we sample xc(t), we obtain the discrete-time linear chirp signal3 
x[n] 
xc(nT) = cos(AoT2n2) 
cos(aon2), 
(10.21) 
where ao 
AoT2 has units of radians. The instantaneous frequency of the sampled 
chirp signal is a frequency-normalized, sampled version ofthe instantaneous frequency 
of the continuous-time signal; i.e., 
UJi[n] = Qi(nT)· T = 2AoT2n = 2aOn, 
(10.22) 
which displays the same proportional increase with sample index n, with ao controlling 
the rate of increase. Figure 10.11 shows two 1201-sample segments of the sampled chirp 
signal in Eq. (10.21) with ao = 15rr x 10-6. (The samples are connected by straight 
lines for plotting.) Observe that over a short interval, the signal looks sinusoidal, but 
the spacing between peaks becomes smaller and smaller as time progresses, indicating 
increasing frequency with time. 
x[320 +m] 
m 
1200 
(a) 
x[720+m
. A 
~~~lfl" 
fI 
fI 
A 
,.... 
m 
o 
1200 
V 
v v V V V 
~ 
(b) 
Figure 10.11 
Two segments of the linear chirp signal x[n] 
cos(aon2) for 
aO 
15rr x 10-6with a400-sample Hamming window superimposed. (a) X[n, ),,) 
at n 
320would bethe DTFT ofthe top trace multiplied by the window. (b) X[720, ),,) 
would be the DTFT of the bottom trace multiplied by the window. 
3We have seen discrete-time linear complex exponential chirp signals in Chapter 9 in the context of 
the chirp transform algorithm. 

____ 
813 
Section 10.3 
The Time-Dependent Fourier Transform 
200,----.----,----,----,----,-----.----,----,----,----, 
~ 
I~ 
~I 
~ 
111..--------------------------..,
'a 100 U""" 
~'I 
r 
0'-----'-­
.\ 
o 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
(a) 
v 
]100 
.~ 
:::t 
50 
O'--__-'--L~L___~____~__~____-L 
L___~____~__~ 
o 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
(b) 
150~---,----.-----.----.-----,----,----.-----.----.----, 
v 
] 100 
'8 
~ 50
:::t 
o 0 
0.05 
0.1 
0.15 
0.2 
0.25 
0,3 
0.35 
0.4 
0.45 
0.5 
A / (27r) 
(c) 
Figure 10.12 
DTFTs of segments of a linear chirp signal: (a) DTFT of 20,000 
samples of the signal x[n] = cos(aon2). (b) DTFT of x[5000 + m]w[m] where 
w[m] is a Hamming window of length L 
401; Le., X[5000, A). (c) DTFT of 
x[15,000 + m]w[m] where w[m] is a Hamming window of length L = 401; Le., 
X[15,OOO, A). 
The relationship of the shifted signal to the window in time-dependent Fourier 
analysis is also illustrated in Figure 10.11. Typically, w[m] in Eq. (10.18) has finite 
length around m 
0, so that X[n, A) displays the frequency characteristics of the 
signal around time n. Figure 1O.l1(a) shows x [320 + m] as a function of m for °::s 
m ::s 1200 together with a Hamming window w[m] of length L = 401 samples. The 
time-dependent transform at time n = 320 is the DTFT of w[m]x[320 + m]. Similarly, 
Figure 1O.11(b) shows the window and a later segment of the chirp signal beginning at 
sample n 
720. 
Figure 10.12 illustrates the importance of the window in discrete-time Fourier 
analysis of time-varying signals. Figure 1O.12(a) shows the DTFf of 20,000 samples 
(with a rectangular window) of the discrete-time chirp. Over this interval, the normal­
ized instantaneous frequency of the chirp, 
fi[n] = Wi [n]/(2Jr) = 2aon/(2rr), 
goes from °to 0.00003rr(20,000)/(2rr) = 0.3. This variation of the instantaneous fre­
quency forces the DTFT representation, which involves only fixed frequencies acting 
over all n, to include all frequencies in that range and beyond as is evident in Fig­
ure 1O.12(a). Thus, the DTFT of a long segment of the signal shows only that the 

814  
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
signal has a wide bandwidth in the conventional D1FI sense. On the other hand, Fig­
ures 1O.12(b) and (c) show D1FIs using a 401 sample Hamming window for segments 
of the chirp waveform at n = 5000 and 15,000, respectively. Thus, Figures lO.l2(b) 
(c) are plots [as functions of A/(2x)] of the time-dependent Fourier transform values 
IX[5000, ,1.)1 and IX[15,000, ,1.)1, respectively. Since the window length L =401 is such 
that the signal does not change frequency very much across the window interval, the 
time-dependent Fourier transform tracks the frequency variation very well. Note that 
at samples 5000 and 15,000, we would expect a peak in the time-dependent transform 
at A/(2x) = 0.00003x(5000)/(2x) = 0.075 and A/(2x) 
0.OOOO3x(15,000)/(2x) 
0.225, respectively. This is confirmed by examination of Figures 1O.12(b) and (c). 
Example 10.10 Plotting X[n,l.): The Spectrogram 
In Figure 10.13, we show a display as a function of both time index n and frequency 
A/(2x) of the magnitude of the time-dependent Fourier transform, Inn, ,1.)1, for the 
signal 
0  
n<O 
[n] = 
cos(aon2)  
0::5 n ::5 20,000 
(10.23)
y 
cos(0.2xn) 
20,000 < n ::5 25,000 
1cos(0.2xn) + cos(0.23xn) 
25,000 < n. 
Spectrogram for Window Length L 
401 
0.5r-------~------_,-------,r-------~------_r------_, 
0'...... 
0.4 
Ii 0.3 
~ 
=< 0.2 
0.1 
o  
5000 
10000 
15000 
20000 
25000 
30000 
sample index n 
Spectrogram for Window Length L = 101 
0.5ir-------~--------r-------_r--------r_------_r------~ 
0.4 
Ii 
~ 0.3  
-<  
0.2 
0.1 
O~~~~~~~~~
o  
5000 
10000 
15000 
20000 
25000 
30000 
sample index n 
Figure 10.13 The magnitude of the time-dependent Fourier transform of y[n] in 
Eq. (10.23): (a) Using aHamming window of length L 
401. (b) Using aHamming 
window of length L= 101. 

815 
Section 10.3 
The Time-Dependent Fourier Transform 
Note that the signal yEn] is equal to x[n] in Eq. (lO.21) in Example lO.9 for 
°:s: n :s: 20,000, and then it abruptly changes to cosine components with fixed fre­
quencies for n > 20,000. This signal was designed to make several important points 
about time-dependent Fourier analysis. First, consider Figure 1O.13(a), which shows 
the time-dependent Fourier transform of yEn] over the interval 0 :s: n :s: 30,000 with 
a Hamming window oflength L 
401. This display, which shows 2010g10 lY[n. A)I as 
a function of A/2rr in the vertical dimension, and the time index n in the horizontal 
dimension is called a spectrogram. The value 20 10glO Iy[n, A) lover a restricted range 
of 50 dB is represented by the darkness of the marking at En, A). The plots in Fig­
ures 10.12(b) and (c) are vertical slices (shown in Figure lO.12 as magnitude) through 
the image at n = 5000 and n = 15,000 respectively at the locations of the dashed 
lines in Figure lO.13(a). Note the linear progression during the chirp interval. Also, 
note that during the constant-frequency intervals, the dark line remains horizontal. 
The width of the dark features in Figure 1O.13(a) is dependent on the width of the 
main lobe Llml of the DTFT of the window. Table 7.2 indicates that for the Hamming 
window, this width is approximately Llml 
8rr/ M wherein M +1 is the window length. 
For a 401-point window, Llml/(2rr) = 0.01. Thus, the two close-in-frequency cosines 
are clearly resolved in the interval 25,000 < n :s: 30,000, because their normalized fre­
quency difference is (0.23rr 
O.2rr)/(2Jr) = 0.015, which is significantly greater than 
the main-lobe width 0.01. Note that the vertical width of the dark sloping bar for the 
chirp interval is wider than the horizontal bars representing the constant-frequency 
intervals. This extra broadening is caused by the frequency variation across the window 
and is a small-scale version of the effect seen in Figure 1O.12(a), wherein the variation 
across the 20,000-sample window is much greater. 
The image in Figure 1O.13(a) illustrates another important aspect of time-de­
pendent Fourier analysis. The 401-sample window provides good frequency resolu­
tion at almost all points in time. However, note that at n = 20,000 and 25,000 the 
signal properties change abruptly, so that for an interval of about 401 samples around 
these times, the window contains samples from both sides of the change. This leads to 
the fuzzy area wherein the signal properties are much less clearly represented by the 
spectrogram. We can improve the ability to resolve events in the time dimension by 
shortening the window. This is illustrated in Figure 1 O.13(b) wherein the window length 
is L 
101. The points of change are much better resolved with this window. How­
ever, the normalized main-lobe frequency width of a W1-sample Hamming window is 
Llml/(2rr) 
0.04, and the two constant-frequency cosines after n = 25.000 are only 
separated by 0.015 in normalized frequency. Thus, as is dearfrom Figure 1O.13(b), the 
two frequencies are not resolved with the 1OI-sample window, although the location 
of the abrupt changes in the signal are much more accurately resolved in time. 
Examples 10.9 and 10.10 illustrate how the principles of discrete-time Fourier 
analysis that were discussed in Sections 10.1 and 10.2 can be applied to signals whose 
properties vary with time. Time-dependent Fourier analysis is widely used both as an 
analysis tool for displaying signal properties and as a representation for signals. In the 
latter use, it is important to develop a deeper understanding of the two-dimensional 
representation in Eq. (10.18). 
10.3.1 Invertibility of X[n,A' 
Since X[n, A) is the DTFT of x[n + m]w[m], the time-dependent Fourier transform is 
invertible if the window has at least one nonzero sample. Specifically, from the Fourier 

Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform
816 
transform synthesis equation (2.130), 
1 i
21r 
x[n +m]w[m) 
X[n,)")ej"md)", 
-00 < m < 00, 
(10.24)
27r 
0 
or equivalently, 
i
21r 
x[n + m] = 
1 
X[n, )")d)" 
(10.25)
27rw[m) 0 
if w[m) :f. 0.4 Thus with m chosen as anyone value for which w[m] :f. 0, x[n] for all 
values of II can be recovered from X[n, ).,) using Eq. (10.25). 
While the above discussion shows that the time-dependent Fourier transform is an 
invertible transformation, Eq. (10.24) and (10.25) do not provide a computable inverse, 
since evaluating them requires knowing X[n,).,) at all )., and also requires evaluating 
an integraL However, the inverse transform becomes a DFf when X[n, ).,) is sampled 
in both the time and frequency dimensions. We will discuss this matter more fully in 
Section 10.3.4. 
10.3.2 Filter Bank Interpretation of X[n,l) 
A rearrangement of the sum in Eq. (10.18) leads to another useful interpretation of 
the time-dependent Fourier transform. If we make the substitution m' = n + m in 
Eq. (10.18), then X[n, ).,) can be written as 
X[n,).,) = L
00 
x[m']w[-(n - m')]ejA(n-m'l. 
(10.26) 
m'=-DO 
Equation (10.26) ean be interpreted as the convolution 
X[n,).,) = x[n] * h,Jn), 
(1O.27a) 
where 
h)Jn) = w[_n]e jAn • 
(1O.27b) 
From Eq. (1 0.27a), we see that the time-dependent Fourier transform as a function 
of n with)., fixed can be interpreted as the output of an LTI filter with impulse response 
hAl'l] or, equivalently, with frequency response 
HA (ejW ) = W(ej(,l.-w). 
(10.28) 
In general, a window that is nonzero for positive time will be called a noncausal 
window, since the computation of X['l,).,) using Eq. (10.18) requires samples that follow 
sample n in the sequence. Equivalently, in the linear-filtering interpretation, the impulse 
response hAln] 
wI-n]ej).n is noncausal if wIn] 
0 for n < 0. That is, a window that 
is nonzero for n 2: 0 gives a noncausal impulse response h,l.[n] in Eq. (lO.27b), whereas 
if the window is nonzero for n :::: 0, the linear filter is causal. 
4Since X[n, A) is periodic in A with period 21r, the integration in Eqs. (10.24) and (10.25) can be over 
any interval of length 2;r. 
It. 

817 
Section 10.3 
The Time-Dependent Fourier Transform 
In the definition of Eq. (10.18), the time origin of the window is held fixed, and 
the signal is considered to be shifted past the interval of support of the window. This 
effectively redefines the time origin for Fourier analysis to be at sample n of the signaL 
Another possibility is to shift the window as n changes, keeping the time origin for 
Fourier analysis fixed at the original time origin of the signal. This leads to a definition 
for the time-dependent Fourier transform of the form 
x 
X[n, J,..) 
L x[m]w[m - nje- jAm . 
(10.29) 
m=-(X) 
The relationship between the definitions of Eqs. (10.18) and (10.29) is easily shown to 
be 
X[n, J,..) 
e- jAn X[n, J,..). 
(10.30) 
The definition of Eq. (10.18) is particularly convenient when we consider using 
the DFf to obtain samples in Aof the time-dependent Fourier transform, since, if w[m] 
is of finite length in the range 0 S m S (L 
1), then so is x[n + m]w[m]. On the other 
hand, the definition of Eq. (10.29) has some advantages for the interpretation of Fourier 
analysis in terms of filter banks. Since our primary interest is in applications of the DFf, 
we will base most of our discussions on Eq. (10.18). 
10.3.3 The Effect of the Window 
The primary purpose of the window in the time-dependent Fourier transform is to limit 
the extent of the sequence to be transformed, so that the spectral characteristics are 
approximately constant over the duration of the window. The more rapidly the signal 
characteristics change, the shorter the window should be. We saw in Section 10.2 that as 
the window becomes shorter, frequency resolution decreases. The same effect is true, 
of course, for X[n, A). On the other hand, as the window length decreases, the ability 
to resolve changes with time increases. Consequently, the choice of window length 
becomes a trade-off between frequency resolution and time resolution. This trade-off 
was illustrated in Example 10.10. 
The effect of the window on the properties of the time-dependent Fourier trans­
form can be seen by assuming that the signal x[n] has a conventional DTFf X (e jW). 
First, let us assume that the window is unity for all m; i.e., assume that there is no window 
at alL Then, from Eq. (10.18), 
(10.31) 
Of course, a typical window for time-dependent spectrum analysis tapers to zero, so 
as to select only a portion of the signal for analysis. On the other hand, as discussed 
in Section 10.2, the length and shape of the window are chosen so that the Fourier 
transform of the window is narrow in A compared with variations in A of the Fourier 
transform of the signal. Thus, the need for good time resolution and good frequency 
resolution often requires compromise. The Fourier transform of a typical window is 
illustrated in Figure 1O.14(a). 

818 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
W(ei"') 
o 
1T 
21T 
(jJ 
(a) 
HA(ejw) 
W(eJ(A - wJ) 
(jJ
o 
A 
1T 
2'iT 
21T+A 
(b) 
Figure 10.14 (a) Illustration of the Fourier transform of a Bartlett window for 
time-dependent Fourier analysis. (b) Equivalent bandpass filter for time-dependent 
Fourier analysis. 
If we consider the time-dependent Fourier transform for fixed n, then it follows 
from the properties of DTFTs that 
X[n, A) = 2~ fo27r ej8n X (e j8 )W(ej (}'-8)de; 
(10.32) 
i.e., the Fourier transform of the shifted signal is convolved with the Fourier transform 
of the window. This is similar to Eq. (10.2), except that in Eq. (10.2), we assumed that the 
signal was not successively shifted relative to the window. Here, we compute a Fourier 
transform for each value of n. In Section 10.2, we saw that the ability to resolve two 
narrowband signal components depends on the width of the main lobe of the Fourier 
transform of the window, whereas the degree of leakage of one component into the 
vicinity of the other depends on the relative side-lobe amplitude. The case ofno window 
at all corresponds to w[n] = 1 for all n. In this case, W(e jUJ ) = 2Jr8(ill) for -Jr :s ill:S Jr, 
which gives precise frequency resolution but no time resolution. 
In the linear-filtering interpretation of Eqs. (1O.27a), (1O.27b), and (10.28), W(e jUJ ) 
typically has the lowpass characteristics depicted in Figure 1O.14(a), and consequently, 
H}.(e jW ) is a bandpass filter whose passband is centered at ill = A, as depicted in Fig­
ure 1O.14(b). Clearly, the width of the passband ofthis filter is approximately equal to the 
width of the main lobe of the Fourier transform of the window. The degree of rejection 
of adjacent frequency components depends on the relative side-lobe amplitude. 
The preceding discussion suggests that if we are using the time-dependent Fourier 
transform to obtain a time-dependent estimate of the frequency spectrum ofa signal, it is 
desirable to taper the window to lower the side lobes and to use as long a window as fea­
sible to improve the frequency resolution. This has already been illustrated in Examples 
10.9 and 10.10, and we will consider other examples in Section lOA. However, before 
doing so, we discuss the use of the DFf in explicitly evaluating the time-dependent 
Fourier transform. 

819 
Section 10.3 
The Time-Dependent Fourier Transform 
10.3.4 Sampling in Time and Frequency 
Explicit computation of X[n, A) can be done only on a finite set of values of A, cor­
responding to sampling the time-dependent Fourier transform in the domain of its 
frequency variable A. Just as finite-length signals can be exactly represented through 
samples of the DTFI: signals of indeterminate length can be represented through sam­
ples of the time-dependent Fourier transform, if the window in Eq. (10.18) has finite 
length. As an example, suppose that the window has length L with samples beginning 
at m = 0; i.e., 
w[ml = 0 
outside the interval 0 < m :;;: L 
1. 
(10.33) 
If we sample Xln, A) at N equally spaced frequencies Ak = 2Tlk/N, with N 2: L, then we 
can recover the original windowed sequence from the sampled time-dependent Fourier 
transform. Specifically, if we define X[n, k] to be 
L-l 
X[n, k] = X[n, 2rck/N) L x[n + m]w[m]e-j(2n/N)km, 
0:;;: k :;;: N - 1, (10.34) 
m=O 
then X[n, kJ with n fixed is the DFT of the windowed sequence x[n + m]w[m]. Using 
the inverse DFT, we obtain 
1 N-l 
x[n + m]w[m] = N L X[n, k]ej(2n/N)km, 
0:;;: m :;;: L -1. 
(10.35) 
k=O 
Since we assume that the window w[m] =1= 0 for 0:;;: m :;;: L -1, the sequence values can 
be recovered in the interval from n through (n + L 
1) using the equation 
N-l 
xln + m] 
1 
" 
X[n k]ej (2n/N)km 
05m:;;:L-1. 
(10.36)
Nw[m] ~, 
, 
k=O 
The important point is that the window has finite length and that we can take at least 
as many samples in the A dimension as there are nonzero samples in the window; i.e., 
N > L. While Eq. (10.33) corresponds to a noncausal window, we could have used a 
causal window with w[m] =1= 0 for -(L - 1) :;;: m :;;: 0 or a symmetric window such that 
w[m] = w[-m] for Iml :;;: (L - 1)/2, with L an odd integer. The use of a noncausal 
window in Eq. (10.34) is simply more convenient for our analysis, since it leads very 
naturally to the interpretation of the sampled time-dependent Fourier transform as the 
DFT of the windowed block of samples beginning with sample n. 
Since 
(10.34) corresponds to sampling Eq. (10.18) in A, it also corresponds 
to sampling Eqs. (10.26), (1O.27a), and (10.27b) in A. Specifically, Eq. (10.34) can be 
rewritten as 
X[n, k] = x[n] * hk[n], 
0:;;: k :;;: N -1, 
(10.37a) 
where 
(10.37b) 
Equations (1O.37a) and (1O.37b) can be viewed as a bank of N filters, as depicted in 
Figure 10.15, with the kth filter having frequency response 
Hk(ejW ) = W(ej [(2Jrk/N)-wl). 
(10.38) 

820 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
X[n,N-1] 
x[n] 
X[n,l] 
Figure 10,15 Filter bank 
representation of the time-dependent 
X[n, 0] 
Fourier transform. 
Our discussion suggests thatx[n] for -00 < n < 00 can be reconstructed if X[n, )..) 
or X[n, k] is sampled in the time dimension, as well. Specifically, using Eq. (10.36), we 
can reconstruct the signal in the interval no ::: n ::: no + L - 1 from X[no, k], and we 
can reconstruct the signal in the interval no + L ::: n ::: no + 2L 
1 from X[no + L, k], 
and so on. Thus, x[nl can be reconstructed exactly from the time-dependent Fourier 
transform sampled in both the frequency and the time dimension. In general, for the 
region of support of the window as specified in Eq. (10.33), we define this sampled 
• time-dependent Fourier transform as 
L-1 
X[rR. k] = X[rR, 2nkJN) = L x[rR +m]w[m]e-j (2rr/N)km, 
(10.39) 
m=O 
where rand k are integers such that -00 < r < 00 and 0 ::: k < N 
1. To further 
simplify our notation. we define 
Xr[k] = X[rR. k] = X[rR, AAJ, 
-00 < r < 00, 
0::: k ::: N 
1, 
(10.40) 
where Ak = 2nkJN. This notation denotes explicitly that the sampled time-dependent 
Fourier transform is simply a sequence of N-point DFTs of the windowed signal seg­
ments 
xr[m] = x[rR + m]w[m], 
-00 < r < 00. 
0::: m ::: L 
1, 
(10.41) 
with the window position moving in jumps of R samples in time. Figure 10.16 shows 
lines in the [n, A)-plane corresponding to the region of support of X[n, A) and the grid 
ofsampling points in the [n, A)-plane for the case N = 10 and R = 3. As we have shown, 
it is possible to uniquely reconstruct the original signal from such a two-dimensional 
discrete representation for appropriate choice of L. 
Equation (1039) involves the following integer parameters: the window length 
L; the number of samples in the frequency dimension, or the DFT length N; and the 
sampling interval in the time dimension, R. Although not all choices of these parame­
ters will permit exact reconstruction of the signal, numerous combinations of N, R, and 
w[n] and L can be used. The choice L ::: N guarantees that it is possible to reconstruct 
the windowed segments xr[m] from the block transforms Xr[k]. If R < L, the segments 
overlap, but if R > L, some of the samples of the signal are not used and therefore 
cannot be reconstructed from Xr[k]. Thus, as one possibility, if the three sampling pa­
rameters satisfy the relation R ::: L ::: N, then we can (in principle) recover R samples 
ofx[n] block-by-block for all n from Xr [k]. Notice that each block of R samples of the 

• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
821 
Section 10.3 
The Time-Dependent Fourier Transform 
0 
X[3,A) 
X[6,A) 
X[9,A) 
0 
2 
3 
4 
5 
6 
7 
8 
9 
10 
n 
k 
A 
N-l 
27r 
21T 
N 
t 
0 
0 
(a) 
Xl[k] 
X[R,k] 
X2[kl =X[2R, k] 
X3[kj =X[3R, k) 
0 
n 
0 
2 
3 
r 
(b) 
Figure 10.16 
(a) Region of support for X[n, A). (b) Grid of sampling points in 
the In, ).)-plane for the sampled time-dependent Fourier transform with N = 10 
and R 
3. 
signal is represented by N complex numbers in the sampled time-dependent Fourier 
representation; or, if the signal is real, only N real numbers are required, due to the 
symmetry of the DFT. 
As a specific example, the signal can be reconstructed exactly from the sampled 
time-dependent Fourier transform for the special case R = L = N. In this case, N 
samples of a real signal are represented by N real numbers, and this is the minimum 
that we could expect to achieve for an arbitrarily chosen signal. For R = L = N we can 
recover xr[m] = x[rR + mJw[m] for 0 :s m :s N - 1 by computing the inverse DFT 
of Xr[k]. Therefore, we can express x[nJ for r R :s n :s [(r + 1)R 
1J in terms of the 
windowed segments Xr [m] as 
xr[n 
rR]
x[nJ= ---­
r R :s n :s [(r + l)R - 1], 
(10.42)
w[n - rR] 

822 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
i.e., we recover the N -point windowed segments, remove the effect of the window, and 
then abut the segments together to reconstruct the original sequence. 
10.3.5 The Overlap-Add Method of Reconstruction 
While the previous discussion verifies the possibility of theoretically exact reconstruc­
tion of the signal from its time- and frequency-sampled time-dependent Fourier trans­
form, the demonstration proof is not a viable reconstruction algorithm when modifica­
tions are made to the time-dependent Fourier transform as is common, for example, in 
applications such as audio coding and noise reduction. In these applications, division by 
a tapering window as required in Eq. (10.42) can greatly enhance errors at the edges; 
therefore, the signal blocks may not fit together smoothly. In such applications, it is 
helpful to make R smaller than Land N so that the blocks of samples overlap. Then, 
if the window is properly chosen, it will not be necessary to undo the windowing as in 
Eq. (10.42). 
Suppose that R .:s L .:s N. Then we can write 
1 N-l 
, 
xr[m] 
x[rR+m]w[m] = N LXr [k]eJ (2rrk/N)m 
O.:sm.:sL-l. 
(10.43) 
k=O 
The recovered segments are shaped by the window, and their time origin is at the 
beginning of the window. A different approach to putting the signal back together that 
is more robust to changes in Xr[kj is to shift the windowed segments to their original 
time locations r R and then simply add them together; i.e., 
00 
x[n] = L 
xr[n 
rR]. 
(10.44) 
r=-oo 
If we can show thatx[n] = x[n] for all n, then Eqs. (10.43) and (10.44) together comprise 
a method for time-dependent Fourier synthesis having the capability of perfect recon­
struction. Substituting Eq. (10.43) into Eq. (10.44) leads to the following representation 
of x[n]: 
00 
x[n] 
Lx[rR+n rR]w[n-rR] 
r=-oo 
00 
= x[n] L 
w[n - rR] 
(10.45) 
r=-oo 
If we define 
00 
w[n] = L 
w[n - rR], 
(1O.46a) 
r=-X) 
then the reconstructed signal in Eq. (10.45) can be expressed as 
x[nJ 
x[n]w[n]. 
(10.46b) 
It follows from Eq. (10.46b) that the condition for perfect reconstruction is 
w[n] L
00 
w[n - rR] = C 
00 < n < 00, 
(10.47) 
r=-oo 

823 
Section 10.3 
The TIme-Dependent Fourier Transform 
i.e., the shifted-by-R copies of the window must add to a constant reconstruction gain 
C for all n. 
Note that the sequence w[n] is a periodic sequence (with period R) comprised of 
time-aliased window sequences. As a simple example, consider a rectangular window 
wrecdn] of length L samples. If R 
the windowed segments simply fit together 
block-by-block with no overlap. In this case, the condition of Eq. (10.47) is satisfied 
with C = 1, because the shifted windows fit together with no overlap and no gaps. (A 
simple sketch will confirm this.) If L for the rectangular window is even, and R = L/2 
a simple analysis or sketch will again verify that the condition of Eq. (10.47) is satisfied 
with C = 2. In fact, if L = 2 v , the signal x[n] can be perfectly reconstructed from Xr[k] 
by the overlap-add method of Eq. (10.44) when L :::: Nand R = L, L/2, ... , 1. The 
corresponding reconstruction gains would be C = 1. 2, ... , L. While this demonstrates 
that the overlap-add method can perfectly reconstruct the original signal for some rect­
angular windows, and some window spacings R, the rectangular window is rarely used in 
time-dependent Fourier analysis/synthesis because ofits poor leakage properties. Other 
tapered windows such as the Bartlett, Hann, Hamming, and Kaiser windows are more 
commonly used. Fortunately, these windows with their superior spectral isolation prop­
erties, can also produce perfect or near-perfect reconstruction from the time-dependent 
Fourier transform. 
Two windows with which perfect reconstruction can be achieved are the Bartlett 
and Hann windows, which were introduced in Chapter 7 in the context of FIR filter 
design. They are defined again here in Eqs (10.48) and (10.49), respectively: 
Bartlett (triangular) 
2n/M, 
0.::: n'::: M12, 
WBart[n] = 
2 - 2n/M,
{ 0, 
MI2 < n :::: M, 
otherwise 
(10.48) 
Hann 
0.5 
0.5 cos(2nn/M), 
0:::: n :::: M, 
(10.49)
{ 0, 
otherwise 
As these windows are defined, the window length is L = M + 1 with the two end 
samples equal to zero.s With M even and R = M /2, then it is easily shown for the 
Bartlett window that the condition of 
(10.47) is satisfied with C = 1. Figure 1O.17(a) 
shows overlapping Bartlett windows of length M + 1 (first and last samples zero) when 
R = M /2. It is clear that these shifted windows add up to the reconstruction gain 
constant C 
1. Figure 1O.17(b) shows the same choice of L = M +1 and R = M/2 for 
the Hann window. Although it is less obvious from this plot, it is also true that these 
shifted windows add up for all n to the constant C 
1. A similar statement is also true 
for the Hamming window and many other windows. 
5With these definitions, the actual number of nonzero samples is M 
1 for both the Bartlett and Hann 
windows, but the inclusion of the zero samples leads to convenient mathematical simplifications. 

Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform
824 
o 
R 
M 
n 
(a) 
(a) Shifted M + 1-point
])(X5(iX 
Figure 10.17 
Bartlett windows with R 
M/2. 
(b) Shifted M + 1-point Hann windows 
o 
R 
M 
n 
with R= M/2. The dashed line is the 
(b) 
periodic sequence w[a]. 
Figure 7.30 gives a comparison of the DTFT of the rectangular, Bartlett and Hann 
windows. Note that the main-lobe width of the Bartlett and Hann windows is twice that 
of the rectangular window of the same length L, but the side lobes are significantly lower 
in amplitude for both the Bartlett and Hann windows. Thus, they and the other windows 
in Figure 7.30 are much preferred over the rectangular window for time-dependent 
Fourier analysis/synthesis. 
While Figure 10.17 is intuitively plausible, it is less obvious that the Bartlett and 
Hann windows for M 
2" can provide perfect reconstruction for values of 
R = M/2, M/4, ... ,1 with corresponding reconstruction gains of M/(2R). To see this, 
it is helpful to recall that the envelope sequence w[n] is inherently periodic with period 
R, so it can be represented by an inverse DFT as 
00 
R-l 
!v[n] = L w[n 
r R] 
~ L W(eJ(2rrkIR»eJ(2rrkIR)n, 
(10.50) 
r=-oo 
k=O 
where W(e j (2rrkIR}) is the DTFT of w[nl sampled at frequencies (211:k/ R), k 
0,1, 
... , R - 1. From Eq. (10.50) it is clear that a condition for perfect reconstruction is 
W(eJ(2rrk/R) 
0 
k=1,2, .... R 
1, 
(1O.SIa) 
and if Eq. (1O.5Ia) holds, then it follows from Eq. (10.50) that the reconstruction gain is 
'0 
C 
W~J) 
(10.5Ib) 
Problem 7.43 of Chapter 7 explores the notion that the commonly used Bartlett, 
Hann, Hamming, and Blackman windows can be represented in terms of rectangular 
windows for which it is relatively easy to obtain a closed-form expression for the DTFT 
of the window. In particular, Problem 7.43 gives the result that for M even, the Bartlett 
window defined as in Eq. (10.48) has DTFT 
jm _ 
( 2 ) (Sin(WM/4»)2 -jwM/2 
(10.52)
Wsart(e 
) -
M 
sin(w/2) 
e 
. 
From Eq. (10.52) it follows that the Bartlett window Fourier transform has equally 
spaced zeros at frequencies 411:k / M, for k = 1, 2, ... , M 
1. Therefore, if we choose R 

825 
Section 10.3 
The Time-Dependent Fourier Transform 
so that 2Jrk/ R = 4Jrk/ M or R = M/2, the condition Eq. (10.51a) is satisfied. Substituting 
w = 0 into Eq. (10.52) gives WBart(eO) = M/2, so it follows that perfect reconstruction 
results with C = M/(2R) = 1 if R = M/2. Choosing R = M/2 aligns the frequencies 
2Jrk/ R with all the zeros of WBart(e jW ). If M is divisible by 4, we can use R = M/4 and 
the frequencies 2Jrk/R will still align with zeros of WBart (eja;), and the reconstruction 
gain will be C 
M/ (2R) 
2. If M is a power of two, R can be smaller with concomitant 
increase in C. 
The DTFT WHann(eja;) also has zeros equally spaced at integer multiples of 4Jr/ M, 
so exact reconstruction is also possible with the Hann window defined as in Eq. (10.49). 
The equally spaced zeros of WBart(e iw ) and WHann (e jW ) are evident in the plots in 
Figure 7.30(b) and (c), respectively. Figure 7.30(d) shows the Hamming window, which 
is a version of the Hann window that is optimized to minimize the side-lobe levels. As 
a result of the adjustment of the coefficients from 0.5 and 0.5 to 0.54 and 0.46, the zeros 
of WHamm(eia;) are slightly displaced, so it is no longer possible to choose R so that 
the frequencies 2Jrk/ R fall precisely on zeros of WHamm(eia;). However, as shown in 
Table 7.2, the maximum side-lobe level for frequencies above 4Jr/ Mis -41 dB. Thus, the 
condition of Eq. (1O.51a) is satisfied approximately at each of the frequencies 2Jrk/ R. 
Equation 10.50 shows thatifEq. (10.51a) is not satisfied exactly, w(n] will tend to oscillate 
around C with period R imparting a slight amplitude modulation to the reconstructed 
signal. 
10.3.6  Signa. Processing Based on the Time-Dependent 
Fourier Transform 
A general framework for signal processing based on the time-dependent Fourier trans­
form is depicted in Figure 10.18. This system is based on the fact that a signal x(n] can 
be reconstructed exactly from its time- and frequency-sampled time-dependent Fourier 
transform X,(k] if the window and sampling parameters are appropriately chosen, as 
discussed above. Ifthe processing shown in Figure 10.18 is done so that Y,[k] maintains 
its integrity as a time-dependent Fourier transform, then a processed signal y[n] can be 
reeonstructed by a technique of time-dependent Fourier synthesis, such as the overlap­
add method or a technique involving a bank of bandpass filters. For example, if x[n] 
is an audio signal, Xr[k] can be quantized for signal compression. The time-dependent 
Fourier representation provides a natural and convenient framework, wherein auditory 
masking phenomena can be exploited to "hide" the quantization noise. (See, for exam­
ple, Bosi and Goldberg, 2003 and Spanias, Painter and Atti, 2007.) Time-dependent 
Fourier synthesis is then used to reconstruct a signal y[n] for listening. This is the basis 
for MP3 audio coding, for example. Another application is audio noise suppression, 
Time 
yin]
x[nJ 
Dependent
--...:.....:.., 
Fourier 
Analysis 
X[r,k] 
F'requency 
Domain 
Processing 
Y[r,k] 
Time 
Dependent 
Fourier 
Synthesis 
Figure 10.18 Signal processing based on time-dependent Fourier analysis! 
synthesis. 

826  
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
wherein the acoustic noise spectrum is estimated and then either subtracted from the 
time-dependent Fourier spectrum of the input signal or used as the basis for Wiener 
filtering applied to the Xr[k]. (See Quatieri, 2002.) These and many other applications 
are greatly facilitated by the FFT algorithms that are available for efficient computation 
of the discrete-time-dependent Fourier transform. 
A discussion ofapplications of this type would take us too far afield; however, these 
kinds of block-processing techniques for discrete-time signals were also introduced in 
Chapter 8, when we discussed the use of the DFT for implementing the convolution of 
a finite-length impulse response with an input signal of indefinite length. This method 
of implementation of LTI systems has a useful interpretation in terms of the definitions 
and concepts of time-dependent Fourier analysis and synthesis, as discussed so far. 
Specifically, assume that x[n] = 0 for n < 0, and suppose that we compute the 
time-dependent Fourier transform for R == L and a rectangular window. In other words, 
the sampled time-dependent Fourier transform X,[k] consists of a set of N-point DFTs 
of segments of the input sequence 
xr[m]=x[rL+m], 
O:::;:m:::;:L-L 
(10.53) 
Since each sample of the signal x [n] is included, and the blocks do not overlap, it follows 
that 
ClO 
x[n] = Lx,[n 
rL]. 
(10.54) 
,=0 
Now, suppose that we define a new time-dependent Fourier transform 
Y,[k] = H[k]Xr[k], 
0:::;: k :::;: N - 1, 
(10.55) 
where H[k] is the N -point DFT of a finite-length unit sample sequence hen] such that 
h[n] = 0 for n < 0 and for n > P -1. Ifwe compute the inverse DFT of Y, [k], we obtain 
1 N-l 
N-l 
Yr[m] = N L 
Yr [k]e j (2JrjN)km = L 
xr[i]h[«m - i))N]. 
(10.56) 
k=O  
(=0 
That is, Yr [m] is the N -point circular convolution ofhem] and xr[m]. Since h[m] has length 
P samples and xr[m] has length L samples, it follows from the discussion of Section 8.7 
that if N ::: L + P -1, then Yr[m] will be identical to the linear convolution of h[m] with 
xr[m] in the interval 0:::;: m :::;: L + P - 2, and it will be zero, otherwise. Thus, it follows 
that if we construct an output signal 
ClO 
yen] = LYr[n - rL],  
(10.57) 
r=O 
then y[n] is the output of an LTI system with impulse response hen]. The procedure just 
described corresponds exactly to the overlap-add method of block convolution. The 
overlap-save method discussed in Section 8.7 can also be applied within the framework 
of the time-dependent Fourier transform. 
10.3.7  Filter Bank Interpretation of the nme-Dependent 
Fourier Transform 
Another way to see that the time-dependent Fourier transform can be sampled in the 
time dimension is to recall that for fixed A(or for fixed k if the analysis frequencies are 
Ak = 2rr:k/N) the time-dependent Fourier transform is a one-dimensional sequence in 
time that is the output of a bandpass filter with frequency response as in Eq. (10.28). 

827 
Section 10.3 
The Time-Dependent Fourier Transform 
10 
5 
101T 121T 
1T 
21T 
ill 
16 
16 
(a) 
10­
(b) 
Figure 10.19 Filterbank frequency response. (a) Rectangular window. (b) Kaiser 
window. 
This is illustrated in Figure 10.19. Figure 1O.19(a) shows the equivalent set of bandpass 
filters corresponding to a rectangular window with L 
N 
16. Figure 10.19 illustrates 
the filter bank interpretation, even for the case where Land N are much larger. When 
N increases, the filter bands become narrower, and the side lobes overlap with adjacent 
channels in the same way. Note that the pass bands of the filters corresponding to the 
rectangular window overlap significantly, and their frequency selectivity is not good by 
any standard. In fact, the side lobes of anyone of the bandpass filters overlap completely 
with several of the passbands on either side. This suggests that, in general, we might 
encounter a problem with aliasing in the time dimension, since the Fourier transform 
of any other finite-length tapering window will not be an ideal filter response either. 
Our discussion in Section 10.3.5, however, shows that even the rectangular window can 
provide perfect reconstruction with overlapping windows, in spite of the poor frequency 
selectivity. Although aliasing occurs in the individual bandpass filter outputs, it can be 
argued that the aliasing distortion cancels out when all channels are recombined in the 
overlap-add synthesis. This notion of alias cancellation is one of the important concepts 
to result from a detailed investigation of the filter bank interpretation. 
If a tapering window is used, the side lobes are greatly reduced. Figure 1O.19(b) 
shows the case for a Kaiser window of the same length as the rectangular window used 

828 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
in Figure 1O.19(a), i.e., L 
N = 16. The side lobes are much smaller. but the main 
lobe is much wider, so the filters overlap even more. Again, the previous argument 
based on block processing ideas shows conclusively that we can reconstruct the original 
signal almost exactly from the time- and frequency-sampled time-dependent Fourier 
transform if R is small enough. Thus, for a Kaiser window such as in Figure 1O.19(b), 
the sampling rate of the sequences representing each of the bandpass analysis channels 
could be 2:rr/ R = !lml' where !lml is the width of the main lobe of the Fourier transform 
of the window.6 In the example of Figure 10.19(b), the main lobe width is approxi­
mately !lml = 0.4:rr, which implies that the time sampling interval could be R = 5 for 
nearly perfect reconstruction of the signal from X[r R, Ak) by the overlap-add method. 
More generally, in the case of the Hamming window of length L 
M + 1 samples, for 
example, !lm! = 8:rr/ M so nominally, the time sampling interval should be R = M /4. 
With this sampling rate in time, our discussion above shows that the signal x[n] could 
be reconstructed nearly perfectly from X[rR, Ak) using a Hamming window and the 
overlap-add method of synthesis with R 
L /4 and L·:::: N. 
When using the overlap-add method of analysis/synthesis, the parameters gener­
ally satisfy the relation R :::: L :::: N. This implies that (taking account of symmetries) the 
effective total number of samples (numbers) per second of the time-dependent Fourier 
representation X[r R, Ak) is a factor of N / R greater than the sample rate of x[n1itself. 
This may not be an issue in some applications, but it presents a significant problem in 
data compression applications, such as audio coding. Fortunately, the filter bank point 
of view is the basis for showing that it is possible to choose these parameters to sat­
isfy R = N < L and still achieve nearly perfect reconstruction of the signal from its 
time-dependent Fourier transform. An example of such an analysis/synthesis system 
was discussed in Section 4.7.6, where R 
N 
2, and the lowpass and highpass filters 
have impulse responses of length L, which can be as large as desired to achieve sharp 
cutoff filters. The two-channel filter bank can be generalized to a higher number of 
channels with R 
N, and, as in the example of Section 4.7.6, polyphase techniques can 
be employed to increase computational efficiency. The advantage of requiring R = N 
is that the total number of samples/s remains the same as for the input x[n]. As an 
example, Figure 10.20 shows the first few bandpass channels of the basic analysis filter 
bank specified by the MPEG-II audio coding standard. This filter bank performs time­
dependent Fourier analysis with offset center frequencies Ak 
(2k + l)n/64 using 32 
real bandpass filters. Since the real bandpass filters have a pair of pass bands centered at 
frequencies ±Ak,this is equivalent to 64 complex bandpass filters. In this case, the length 
of the impulse responses (equivalent to the window length) is L = 513 with the first and 
last samples being equal to zero. The downsampling factor is R = 32. Observe that the 
filters overlap significantly at their band edges, and downsampling by R = 32 causes 
significant aliasing distortion. However, a more detailed analysis of the complete anal­
ysis/synthesis system shows that the aliasing distortion due to the nonideal frequency 
responses cancels in the reconstruction process. 
6Since, for ourdefinition. the time-dependent Fourier transform channel signals, X[n, Ak), are bandpass 
signals centered on frequency Ak, they can be frequency-downshifted by Ak. so that the result is a lowpass 
signa! in the band ±D.ml' The resulting lowpass signals have highest frequency D.ml/2, so the lowest sampling 
rate would be 27£/ R = D.ml' If R 
N. the frequency-downshifting occurs automatically as a result of the 
downsampling operation. 

, 
Section 10.4 
Examples of Fourier Analysis of Nonstationary Signals 
829 
First Four Channels of MPEG Analysis Filter Bank 
20r------.------.-------,------.----~~------~----_,r_----~ 
!Xi 
"0 
o 
-20 
-40 
.5 -60 
t= 
.~ 
01) 
-80 
-100 
-140 
k 
0 
I 
I 
I 
I 
I ,, 
I 
k=l 
, -
k 2 
k=3 
_ 
...." 
~
I 
I 
I 
I 
I 
J I 
J I 
J I 
I 
I 
I I It,: ., II _ 
I 
_ 
I 
• 
_ 
I 
_ 
< 
,,' , 
" 
" 
\ 
I 
\ 
I 
\ 
, 
I 
\ , 
I 
, ,
, , 
o 
17164 
17132 
17116 
1718 
normalized frequency w 
Figure 10.20 
Several bandpass channels for the MPEG-II analysis filter bank. 
A full-scale discussion of analysis and synthesis filter banks is beyond our scope 
in this chapter. An outline of such a discussion is given as the basis for Problem 10.46, 
and detailed discussions can be found in Rabiner and Schafer (1978), Crochiere and 
Rabiner (1983) and Vaidyanathan (1993). 
10.4  EXAMPLES OF FOURIER ANALYSIS OF 
NONSTATIONARY SIGNALS 
In Section 10.3.6, we considered a simple example of how the time-dependent Fourier 
transform can be used to implement linear filtering. In such applications, we are not 
so much interested in spectral resolution as in whether it is possible to reconstruct 
a modified signal from the modified time-dependent Fourier transform. On the other 
hand, the concept of the time-dependent Fourier transform is often used as a framework 
for a variety of techniques for obtaining spectrum estimates for nonstationary discrete­
time signals, and in these applications spectral resolution, time variation, and other 
issues are the most important. 
A nonstationary signal is a signal whose properties vary with time, for example, a 
sum of sinusoidal components with time-varying amplitudes, frequencies, or phases. As 
we will illustrate in Section 10.4.1 for speech signals and in Section 10.4.2 for Doppler 

830 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
radar signals, the time-dependent Fourier transform often provides a useful description 
of how the signal properties change with time. 
When we apply time-dependent Fourier analysis to a sampled signal, the entire 
discussion of Section 10.1 holds for each DFT that is computed. In other words, for 
each segment xr[n] of the signal, the sampled time-dependent Fourier transform Xr[k] 
would be related to the Fourier transform of the original continuous-time signal by 
the processes described in Section 10.1. Furthermore, if we were to apply the time­
dependent Fourier transform to sinusoidal signals with constant (i.e., nontime-varying) 
parameters, the discussion of Section 10.2 should also apply to each of the DFTs that we 
compute. When the signal frequencies do not change with time, it is tempting to assume 
that the time-dependent Fourier transform would vary only in the frequency dimension 
in the manner described in Section 10.2, but this would be true only in very special 
cases. For example, the time-dependent Fourier transform will be constant in the time 
dimension if the signal is periodic with period N p and L = toNp and R = roNp' where 
to and ro are integers; i.e., the window includes exactly to periods, and the window is 
moved by exactly ro periods between computations of the DFT. In general, even if the 
signal is exactly periodic, the varying phase relationships that would result as different 
segments of the waveform are shifted into the analysis window would cause the time­
dependent Fourier transform to vary in the time dimension. However, for stationary 
signals, if we use a window that tapers to zero at its ends, the magnitude IXr[k]1 will 
vary only slightly from segment to segment, with most of the variation of the complex 
time-dependent Fourier transform occurring in the phase. 
10.4.1  Time-Dependent Fourier Analysis of Speech 
Signals 
Speech is produced by excitation of an acoustic tube, the vocal tract, which is terminated 
on one end by the lips and on the other end by the glottis. There are three basic classes 
of speech sounds: 
•  Voiced sounds are produced by exciting the vocal tract with quasi-periodic pulses 
of airflow caused by the opening and closing of the glottis. 
•  Fricative sounds are produced by forming a constriction somewhere in the vocal 
tract and forcing air through the constriction so that turbulence is created, thereby 
producing a noise-like excitation. 
•  Plosive sounds are produced by completely closing off the vocal tract, building up 
pressure behind the closure, and then abruptly releasing the pressure. 
Detailed discussions of models for the speech signal and applications of the time­
dependent Fourier transform are found in texts such as Flanagan (1972), Rabiner and 
Schafer (1978), O'Shaughnessy (1999), Parsons (1986) and Quatieri (2002). 
With a constant vocal tract shape, speech can be modeled as the response of an LTI 
system (the vocal tract) to a quasiperiodic pulse train for voiced sounds or wide band 
noise for unvoiced sounds. The vocal tract is an acoustic transmission system character­
ized by its natural frequencies, called formants, which correspond to resonances in its 
frequency response. In normal speech, the vocal tract changes shape relatively slowly 
with time as the tongue and lips perform the gestures of speech, and thus it can be 

831 
Section 10.4 
Examples of Fourier Analysis of Nonstationary Signals 
modeled as a slowly time-varying filter that imposes its frequency-response properties 
on the spectrum of the excitation. A typical speech waveform is shown in Figure 1O.2l. 
From this brief description of the process of speech production and from Fig­
ure 10.21, we see that speech is definitely a nonstationary signal. However, as illustrated 
0.17
o 
u 
0.17  
pO  
y' ('" "rr "1' l'0.34 
p 
!I.rcl~"1"A1''''' 0......... 
_ 
............ ,It 
•• ,. flit 
I 
• 
".  ;, 
;'V'Y"vnlf v~ 
0.34  
0.51  
1\ 
s  
v 
0.51 
,nr :1' 1'" 
• 
r ff" 
, 
(" " 
" 
0.68 
v 
n 
0.85 
........ 
z 
1).. , 
1.02 
................ 
I  
1.19 
V 
& V 
J" 
0
V 
v'" 
1.19  
l.36 
n 
l.36 
to 
p "7 7' m't )1 
f ,.p' I' 
P 
pi
!?''\7' 
1" 
1.53  
~ 
" " 1.70 
Figure 10.21 
Waveform of the speech 
utterance "Two plus seven is less than 
ten." Each line is 0.17 s in duration. The 
time-aligned phonemic transcript is 
f' 
v" 
v  
indicated below the waveform. The 
1.70 
1.87  
sampling rate is 16,000 samples/s, so 
n 
each line represents 2720 samples. 

832 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
in the figure, the characteristics of the signal can be assumed to remain essentially con­
stant over time intervals on the order of 30 or 40 ms. The frequency content of the speech 
signal may range up to 15 kHz or higher, but speech is highly intelligible even when 
bandlimited to frequencies below about 3 kHz. Commercial telephone systems, for ex­
ample, typically limit the highest transmitted frequency to about 3 kHz. A standard 
sampling rate for digital telephone communication systems is 8000 samples/so 
Figure 10.21 shows that the waveform consists of a sequence of quasiperiodic 
voiced segments interspersed with noise-like unvoiced segments. This figure suggests 
that if the window length L is not too long, the properties of the signal will not change 
appreciably from the beginning of the segment to the end. Thus, the DFT of a windowcd 
speech scgment should display the frequency-domain properties of thc signal at the 
time corresponding to the window location. For example, if the window length is long 
enough so that the fundamental frequency and its harmonics are resolved, the OFT of a 
windowed segment of voiced speech should show a series of peaks at integer multiples 
of the fundamental frequency of the signal in that inte.rval. This would normally require 
that the window span several periods of the waveform. If the window is too short, then 
the harmonics will not be resolved, but the general spectrum shape will still be evident. 
This is typical of the trade-off between frequency resolution and time resolution that is 
required in the analysis of nonstationary signals. We saw this before in Example 10.9. If 
the window is too long, the signal properties may change too much across the window; 
if the window is too short, resolution of narrowband components will be sacrificed. This 
trade-off is illustrated in the following example. 
Example 10.11 
Spectrogram Display of the Time-Dependent 
Fourier Transform of Speech 
Figure 1O.22(a) shows a spectrogram display of the time-dependent Fourier transform 
of the speech signal in Figure 10.21. The time waveform is also shown on the same 
time scale, below the spectrogram. More specifically, Figure 1O.22(a) is a wideband 
spectrogram. A wideband spectrogram representation results from a window that is 
relatively short in time and is characterized by poor resolution in the frequency dimen­
sion and good resolution in the time dimension. The frequency axis is labeled in terms 
of continuous-time frequency. Since the sampling rate of the signal was 16,000 sam­
ples/s, it follows that the frequency A = JT corresponds to 8 kHz. The specific window 
used in Figure 10.22(a) was a Hamming window of duration 6.7 ms, corresponding to 
L = 108. The value of R was 16, representing I-ms time increments.7 The broad, dark 
bars that move horizontally across the spectrogram correspond to the resonance fre­
quencies of the vocal tract, which, as we see, change with time. The vertically striated 
appearance of the spectrogram is due to the quasiperiodic nature of voiced portions of 
the waveform, as is evident by comparing the variations in the waveform display and 
the spectrogram. Since the length of the analysis window is on the order of the length 
of a period of the waveform, as the window slides along in time, it alternately covers 
high-energy segments of the waveform and then lower energy segments in between, 
thereby producing the vertical striations in the plot during voiced intervals. 
In a narrowband time-dependent Fourier analysis, a longer window is used to 
provide higher frequency resolution, with a corresponding decrease in time resolution. 
7In plotting spectrograms, it is common to use relatively small values of R so that a smoothly varying 
display is obtained. 

833 
Section 10.4 
Examples of Fourier Analysis of Nonstationary Signals 
Such a narrowband analysis of speech is illustrated by the display in Figure 10.22(b). 
In this case, the window was a Hamming window of duration 45 ms. This corresponds 
to L = 720. The value of R was again 16. 
~ 
N 
::t:: 
~ 
;.., 
u 
'" 
::> '" 
r:;r' 
.l::: '" 
8 
7 
6 
:'. 
:" 
5 
iii 
~ 
4 
"\
h 
UWI~\
::. ~' 
3 , 
;': 
2 
Time (5) 
(a) 
"':~.,
2 
.-:;" 
•.
4r
...........~'..­
:-:';"~ I
;~ 
, 
,.­
...., 
Time (5) 
(b) 
Figure 10.22 
(a) Wideband spectrogram of waveform of Figure 10.21. 
(b) Narrowband spectrogram. 

834  
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
This example only hints at the many reasons that the time-dependent Fourier 
transform is so important in speech analysis and processing. Indeed, the concept is 
used directly and indirectly as the basis for acoustic-phonetic analysis and for many 
fundamental speech-processing applications, such as digital coding, noise and reverber­
ation removal, speech recognition, speaker verification, and speaker identification. For 
present purposes, our discussion simply serves as an introductory illustration. 
10.4.2 Time-Dependent Fourier Analysis of Radar 
Signals 
Another application area in which the time-dependent Fourier transform plays an im­
portant role is radar signal analysis. The following are elements of a typical radar system 
based on the Doppler principle: 
•  Antennas for transmitting and receiving (often the same). 
•  A transmitter that generates an appropriate signal at microwave frequencies. In 
our discussion, we will assume that the signal consists of sinusoidal pulses. While 
this is often the case, other signals may be used, depending on the specific radar 
objectives and design. 
•  A receiver that amplifies and detects echoes of the transmitted pulses that have 
been reflected from objects illuminated by the antenna. 
In such a radar system, the transmitted sinusoidal signal propagates at the speed of 
light, reflects off the object, and returns at the speed of light to the antenna, thereby 
undergoing a time delay of the round-trip travel time from the antenna to the object. 
If we assume that the transmitted signal is a sinusoidal pulse of the form cos(Qot) and 
the distance from the antenna to the object is p(t), then the received signal is a pulse of 
the form 
s(t) = cos[QoU - 2p(t)/c)], 
(10.58) 
where c is the velocity of light. If the object is not moving relative to the antenna, then 
pet) = Po, where Po is the range. Since the time delay between the transmitted and 
received pulses is 2Po/c, a measurement of the time delay may be used to estimate 
the range. If, however, p(t) is not constant, the received signal is an angle-modulated 
sinusoid and the phase difference contains information about both the range and the 
relative motion of the object with respect to the antenna. Specifically, let us represent 
the time-varying range in a Taylor's series expansion as 
1 
pet) 
Po + Pot + 2! pot2 + ... ,  
(10.59) 
where PO is the nominal range, Po is the velocity, Po is the acceleration, and so on. 
Assuming that the object moves with constant velocity (i.e., PO = 0), and substituting 
Eq. (10.59) into Eq. (10.58), we obtain 
set) = cos[(Qo 
2QoPo/c)t - 2Qopo/c]. 
(10.60) 
In this case, the frequency of the received signal differs from the frequency of the 
transmitted signal by the Doppler frequency, defined as 
Qd = -2Qopo/c, 
(10.61) 

835 
Section 10.4 
Examples of Fourier Analysis of Nonstationary Signals 
Thus, the time delay can still be used to estimate the range, and we can determine the 
speed of the object relative to the antenna if we can determine the Doppler frequency. 
In a practical setting, the received signal is generally very weak, and thus a noise 
term should be added to Eq. (10.60). We will neglect the effects of noise in the simple 
analysis of this section. Also, in most radar systems, the signal of Eq. (10.60) would be 
frequency shifted to a lower nominal frequency in the detection process. However, the 
Doppler shift will still satisfy Eq. (10.61), even if s(t) is demodulated to a lower center 
frequency. 
To apply time-dependent Fourier analysis to such signals, we first bandlimit the 
signal to a frequency band that includes the expected Doppler frequency shifts and then 
sample the resulting signal with an appropriate sampling period T, thereby obtaining a 
discrete-time signal of the form 
x[n] = cos[(wo 
2WOPo/c)n - 2woPo/c], 
(10.62) 
where WO = 120T. In many cases, the object's motion would be more complicated than we 
have assumed, requiring the incorporation of higher order terms from Eq. (10.59) and 
thereby producing a more complicated angle modulation in the received signal. Another 
way to represent this more complicated variation of the frequency of the echoes is to 
use the time-dependent Fourier transform with a window that is short enough, so that 
the assumption of constant Doppler-shifted frequency is valid across the entire window 
interval, but not so short as to sacrifice adequate resolution when two or more moving 
objects create Doppler-shifted return signals that are superimposed at the receiver. 
Example 10.12 Time-Dependent Fourier Analysis 
of Doppler Radar Signals 
An example of time-dependent Fourier analysis of Doppler radar signals is shown in 
Figure 10.23. (See Schaefer, Schafer and Mersereau, 1979.) The radar data had been 
preprocessed to remove low-velocity Doppler shifts, leaving the variations displayed in 
the figure. The window for the time-dependent Fourier transform was a Kaiser window 
with N 
L 
64 and fJ 
4. In the figure, IXr [k]1 is plotted with time as the vertical di­
mension (increasing upward) and frequency as the horizontal dimension.SIn this case, 
the successive DFTs are plotted close together. A hidden-line elimination algorithm 
is used to create a two-dimensional view of the time-dependent Fourier transform. To 
the left of the center line is a strong peak that moves in a smooth path through the 
time-frequency plane. This corresponds to a moving object whose velocity varies in a 
regular manner. The other broad peaks in the time-dependent Fourier transform are 
due to noise and spurious returns called clutter in radar terminology. An example of 
motion that might create such a variation of the Doppler frequency is a rocket moving 
at constant velocity but rotating about its longitudinal axis. A peak moving through the 
time-dependent Fourier transform might correspond to reflections from a fin on the 
rocket that is alternately moving toward and then away from the antenna because of 
the spinning of the rocket. Figure 1 0.23(b) shows an estimate of the Doppler frequency 
as a function of time. This estimate was obtained simply by locating the highest peak 
ineachDFT. 
8The plot shows the negative frequencies on the left of the line through the center of the plot and 
positive frequencies on the right. This can be achieved by computing the DFT of (_l)n Xr [n] and noting that 
the computation effectively shifts the origin of the DFf index to k = N /2. Alternatively. the DFT of xr[n] 
can be computed and then reindexed. 

__ 
836 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
Short-time Fourier analysis 
'17" •..------------------..., 
0.5'17" 
<=I 
0) 
::I
t  
~ 
,....1I 
0 I
0) 
E 
.: 
t<I
E= 
;a 
t<I 
~ -{).5'17" 
-rr LI__--'1.-.__-'-__--''--__-'-
--' 
o  
60 
Time 
(b)  
-rr  
o  
rr 
Radian frequency 
(a) 
Figure  10.23 
Illustration of time-dependent Fourier analysis of Doppler radar signal. 
(a) Sequence of Fourier transforms of Doppler radar signal. (b) Doppler frequency estimated 
by picking the largest peak in the time-dependent Fourier transform. 
10.5  FOURIER ANALYSIS OF STATIONARY RANDOM 
SIGNALS: THE PERIODOGRAM 
In previous sections, we discussed and illustrated Fourier analysis for sinusoidal signals 
with stationary (nontime-varying) parameters and for nonstationary signals such as 
speech and radar. In cases where the signal can be modeled by a sum of sinusoids or a 
linear system excited by a periodic pulse train, the Fourier transforms of finite-length 
segments of the signal have a convenient and natural interpretation in terms of Fourier 
transforms, windowing, and linear system theory. However, more noise-like signals, 
such as the example of unvoiced speech in Section 10.4.1, are best modeled as random 
signals. 
As we discussed in Section 2.10 and as shown in Appendix A, random processes are 
often used to model signals when the process that generates the signal is too complex 
for a reasonable deterministic model. Typically, when the input to an LTI system is 
modeled as a stationary random process, many of the essential characteristics of the 
input and output are adequately represented by averages, such as the mean value (dc 
level), variance (average power), autocorrelation function, or power density spectrum. 

837 
Section 10.5 
Fourier Analysis of Stationary Random Signals: the Periodogram 
Consequently, it is of particular interest to estimate these for a given signal. As discussed 
in Appendix A, an estimate of the mean value of a stationary random process from a 
finite-length segment of data is the sample mean, defined as 
1 L-l 
mx 
L L 
x[n]. 
(10.63) 
n=O 
Similarly, an estimate of the variance is the sample variance, defined as 
1 L-l 
a-; = L L(x[n] mx )2. 
(10.64) 
n=O 
The sample mean and the sample variance, which are themselves random variables, are 
unbiased and asymptotically unbiased estimators, respectively; i.e., the expected value 
of mx is the true mean mx and the expected value of crt approaches the true variance ut 
as L approaches 00. Furthermore, they are both consistentestimators; i.e., they improve 
with increasing L, since their variances approach zero as L approaches 00. 
In the remainder of this chapter, we study the estimation of the power spectrum9 
of a random signal using the DFT. We will see that there are two basic approaches 
to estimating the power spectrum. One approach, which we develop in this section, is 
referred to as periodogram analysis and is based on direct Fourier transformation of 
finite-length segments of the signal. The second approach, developed in Section 10.6, is 
to first estimate the autocovariance sequence and then compute the Fourier transform of 
this estimate. In either case, we are typically interested in obtaining unbiased consistent 
estimators. Unfortunately, the analysis of such estimators is very difficult, and generally, 
only approximate analyses can be accomplished. Even approximate analyses are beyond 
the scope of this text, and we refer to the results of such analyses only in a qualitative 
way. Detailed discussions are given in Blackman and Tukey (1958), Hannan (1960), 
Jenkins and Watts (1968), Koopmans (1995), Kay and Marple (1981), Marple (1987), 
Kay (1988) and Stoica and Moses (2005). 
10.5.1 The Periodogram 
Let us consider the problem of estimating the power density spectrum pss(n) of a 
continuous-time signal sc(t). An intuitive approach to the estimation of the power spec­
trum is suggested by Figure 10.1 and the associated discussion in Section 10.1. Based 
on that approach, we now assume that the input signal sc(t) is a stationary random sig­
nal. The antialiasing lowpass filter creates a new stationary random signal whose power 
spectrum is bandlimited, so that the signal can be sampled without aliasing. Then, x[n] 
is a stationary discrete-time random signal whose power density spectrum Pxx(w) is 
proportional to pss(n) over the bandwidth of the antialiasing filter; i.e., 
Pxx{w) = ~Pss (*), 
Iwl < 1<, 
(10.65) 
where we have assumed that the cutoff frequency of the antialiasing filter is 1< / T and that 
T is the sampling period. (See Problem 10.39 for a further consideration of sampling of 
9The term power spectrum is commonly used interchangeably with the more precise term powerdensity 
spectrum. 

838 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
random signals.) Consequently, a good estimate of Pxx (w) will provide a useful estimate 
of PssCQ). The window w[n] in Figure 10.1 selects a finite-length segment (L samples) 
of x[n], which we denote urn], the Fourier transform of which is 
L-l 
V(e jW )  L w[n]x[n]e- jwn. 
(10.66) 
n=O 
Consider as an estimate of the power spectrum the quantity 
ICw) = L1U W(ej{J) 12 ,  
(10.67) 
where the constant U anticipates a need for normalization to remove bias in the spec­
trum estimate. When the window w [n] is the rectangular window sequence, this estima­
tor for the power spectrum is called the periodogram. If the window is not rectangular, 
I (w) is called the modified periodogram. Clearly, the periodogram has some of the basic 
properties of the power spectrum. It is nonnegative, and for real signals, it is a real and 
even function of frequency. Furthermore, it can be shown (Problem 10.33) that 
1 
L-l 
I (co) 
LU L 
cvv[m]e-jwm  
(10.68) 
m=-(L-l) 
, 
where 
L-l 
cvv[m] = L x[n]w[n]x[n + m]w[n + m]. 
(10.69) 
n=O 
We note that the sequence cvv[m] is the aperiodic correlation sequence for the finite­
length sequence urn] 
w[n]x[n]. Consequently, the periodogram is in fact the Fourier 
transform of the aperiodic correlation of the windowed data sequence. 
Explicit computation of the periodogram can be carried out only at discrete fre­
quencies. From Eqs. (10.66) and (10.67), we see that if the DTFf of w[n]x[n] is re­
placed by its DFf, we will obtain samples at the DFf frequencies Wk = 2lfk/N for 
k 
0,1, ... , N - 1. Specifically, samples of the periodogram are given by 
I[k] = I(Wk) = L~ W[k]1 2,  
(10.70) 
where V[k] is the N-point DFfof w[n]x[n]. Ifwewanttochoose Nto be greater than the 
window length L, appropriate zero-padding would be applied to the sequence w[n]x[n]. 
If a random signal has a nonzero mean, its power spectrum has an impulse at 
zero frequency. If the mean is relatively large, this component will dominate the spec­
trum estimate, causing low-amplitude, low-frequency components to be obscured by 
leakage. Therefore, in practice the mean is often estimated using Eq. (10.63), and the 
resulting estimate is subtracted from the random signal before computing the power 
spectrum estimate. Although the sample mean is only an approximate estimate of the 
zero-frequency component, subtracting it from the signal often leads to better estimates 
at neighboring frequencies. 

839 
Section 10.5 
Fourier Analysis of Stationary Random Signals: the Periodogram 
10.5.2 Properties of the Periodogram 
The nature of the periodogram estimate of the power spectrum can be determined by 
recognizing that, for each value of w, I (w) is a random variable. By computing the mean 
and variance of I (w), we can determine whether the estimate is biased and whether it 
is consistent. 
From Eq. (10.68), the expected value of I (w) is 
L-l 
£ 
} _ 
1 
"" 
£ 
_jwm
{/(w) -
LU 
L....,; 
{cvv[m]Je 
. 
(10.71) 
m=-(L-l) 
The expected value of cvv[m] can be expressed as 
L-l 
£{cvv[m]J L £{x[n]w[n]x[n + m]w[n + m]} 
n=O 
(10.72)
L-l 
= L w[n]w[n + m]£{x[n]x[n + m]}. 
n=O 
Since we are assuming that x[n] is stationary, 
£{x[n]x[n + m]} 
<pxx[m], 
(10.73) 
and Eq. (10.72) can then be rewritten as 
£{cvv[m]} 
cww[m]<pxx[m], 
(10.74) 
where Cww [m] is the aperiodic autocorrelation of the window, i.e., 
L-l 
cww[m] L w[n]w[n + m]. 
(10.75) 
That is, the mean of the aperiodic autocorrelation of the windowed signal is equal to the 
aperiodic autocorrelation of the window multiplied by the true autocorrelation function; 
i.e., in an average sense, the autocorrelation function of the data window appears as a 
window on the true autocorrelation function. 
From Eq. (10.71 ), Eq. (10.74), and the modulation-windowing property ofFourier 
transforms (Section 2.9.7), it follows that 
£{l(w)} 
(10.76) 
where Cww (e jW ) is the Fourier transform of the aperiodic autocorrelation of the window, 
i.e., 
(10.77) 
According to Eq. (10.76), both the periodogram and the modified periodogram 
are biased estimates of the power spectrum, since £ {/ (w)} is not equal to PxAw). Indeed, 
we see that the bias arises as a result of convolution of the true power spectrum with the 
Fourier transform of the aperiodic autocorrelation of the data window. If we increase 
the window length, we expect that W(e jW ) should become more concentrated around 
w 
0, and thus Cww(ejW) should look increasingly like a periodic impulse train. If 
the scale factor 1/(LU) is correctly chosen, then £{l(w)} should approach PxAw) as 

840 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
Cww(e}W) approaches a periodic impulse train. The scale can be adjusted by choosing 
the normalizing constant U so that 
1 
1 L-l
Jlf 
-2--
IW(e jW )12daJ = -
L(w[n])2 
1, 
(10.78) 
1r LU 
-If 
LU n=O 
or 
1 L-l 
U = L L(w[n])2. 
(10.79) 
n=O 
For the rectangular window, we should choose U = 1, while other data windows would 
require a value of 0 < U < 1 if w[n] is normalized to a maximum value of 1. Alter­
natively, the normalization can be absorbed into the amplitude of w[n]. Therefore, if 
properly normalized, the periodogram and modified periodogram are both asymptoti­
cally unbiased; i.e., the bias approaches zero as the window length increases. 
To examine whether the periodogram is a consistent estimate or becomes a consis­
tent estimate as the window length increases, it is necessary to consider the behavior of 
the variance of the periodogram. An expression for the variance of the periodogram is 
very difficult to obtain even in the simplest cases. However, it has been shown (see Jenk­
ins and Watts, 1968) that over a wide range ofconditions, as the window length increases, 
var[/(w)]::::::: P;x(w). 
(10.80) 
That is, the variance of the periodogram estimate is approximately the same size as the 
square of the power spectrum that we are estimating. Therefore, since the variance does 
not asymptotically approach zero with increasing window length, the periodogram is 
not a consistent estimate. 
The properties of the periodogram estimate of the power spectrum just discussed 
are illustrated in Figure 10.24, which shows periodogram estimates of white noise us­
ing rectangular windows of lengths L 
16,64,256, and 1024. The sequence x[nJ 
5.---------------------------------------, 
4 
3 
2 
128 
256 
384 
512 
Sample number (k) 
(a) 
Figure 10.24 
Periodograms of pseudorandom white-noise sequence. (a) Win­
dow length L= 16 and DFT length N = 1024. 

5r-----------------------------------~ 
4 
1 
Sample number (k) 
(b) 
8r-------------------------------------~ 
6 
2 
128 
256 
Sample number (k) 
(c) 
12r-------------------------------------~ 
10 
128 
256 
384 
512 
Sample number (k) 
(d) 
Figure10.24 
(continued) (b) L 
64 and N = 1024. (c) L= 256 and N = 1024. 
(d)L 
1024andN 
1024. 
841 

842 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
was obtained from a pseudorandom-number generator whose output was scaled so 
that Ix [n]1 :::; .J3. A good random-number generator produces a uniform distribution of 
amplitudes, and the measured sample-to-sample correlation is small. Thus, the power 
spectrum of the output of the random-number generator could be modeled in this case 
by Pxx (w) = a} = 1 for all w. For each of the four rectangular windows, the periodogram 
was computed with normalizing constant Uland at frequencies Wk = 21rk/N for 
N = 1024 using the DFf. That is, 
2
1 
1 
1 I
L 
­
I[k] 
I(Wk) 
"L 1V [k]1 2 ="L L w[n]x[nJe-i (2n"jN)kn, 
(10.81) 
n=O 
In Figure 10.24, the DFf values are connected by straight lines for purposes of display.  
Recall that I (w) is real and an even function of w, so we only need to plot I[k] for  
o :::; k :::; N /2 corresponding to 0 :::; w :::; 1r. Note that the spectrum estimate fluctuates 
more rapidly as the window length L increases. This behavior can be understood by 
recalling that, although we view the periodogram method as a direct computation of the 
spectrum estimate, we have seen that the underlying correlation estimate of Eq. (10.69) 
is, in effect, Fourier transformed to obtain the periodogram. Figure 10.25 illustrates a 
windowed sequence, x[n]w[n], and a shifted version, x[n + m]w[n + m], as required in 
Eq. (10.69). From this figure, we see that (L 
m) signal values are involved in computing 
a particular correlation lag value cvv[m]. Thus, when m is close to L, only a few values of 
x[nJ are involved in the computation, and we expect that the estimate of the correlation 
sequence will be considerably more inaccurate for these values of m and consequently 
will also show considerable variation between adjacent values of m. On the other hand, 
when m is small, many more samples are involved, and the variability of cvv[m] with m 
should not be as great. The variability at large values of m manifests itself in the Fourier 
transform as fluctuations at all frequencies, and thus, for large L, the periodogram 
estimate tends to vary rapidly with frequency. Indeed, it can be shown (see Jenkins 
x[n] w[n] 
n
o 
-m 
o 
L-l 
(a) 
x[n+m]w[n+mJ 
n 
Figure 10.25 Illustration of sequences 
L- m 
1 
L 
1 
involved in EQ. (10.69). (a) A 
finite-length sequence. (b) Shifted 
(b) 
sequence for m> O. 

843 
Section 10.5 
Fourier Analysis of Stationary Random Signals: the Periodogram 
and Watts, 1968) that if N = L, the periodogram estimates at the DFf frequencies 
2rrkj N become uncorrelated. Since, as N increases, the DFf frequencies get closer 
together, this behavior is inconsistent with our goal of obtaining a good estimate of 
the power spectrum. We would prefer to obtain a smooth spectrum estimate without 
random variations resulting from the estimation process. This can be accomplished by 
averaging multiple independent periodogram estimates to reduce the fluctuations. 
10.5.3 Periodogram Averaging 
The averaging of periodograms in spectrum estimation was first studied extensively by 
Bartlett (1953); later, after fast algorithms for computing the DFf were developed, 
Welch (1970) combined these computational algorithms with the use of a data window 
w[n] to develop the method of averaging modified periodograms. In periodogram aver­
aging, a data sequencex[n], 0 S n S Q 1, is divided into segments oflength-L samples, 
with a window of length L applied to each; Le., we form the segments 
xr[n] = x[rR + n]w[n], 
OsnsL-l. 
(10.82) 
If R < L the segments overlap, and for R = L the segments are contiguous. Note 
that Q denotes the length of the available data. The total number of segments depends 
on the values of, and relationship among, R, L, and Q. Specifically, there will be K full­
length segments, where K is the largest integer for which (K -l)R + (L 
1) s Q-1. 
The periodogram of the rth segment is 
(10.83) 
where Xr(e jW) is the DTFf ofxr[n]. Each Ir(w) has the properties of a periodogram, as 
described previously. Periodogram averaging consists of averaging the K periodogram 
estimates Ir(w); i.e., we form the time-averaged periodogram defined as 
_ 
1 K-l  
I(w) = K L Ir(w). 
(10.84)  
r=O 
To examine the bias and variance of I(w), let us take L = R, so that the segments do 
not overlap, and assume that <pxx[m] is small for m > L; i.e., signal samples more than 
L apart are approximately uncorrelated. If we assume that the periodograms Ir (w) are 
identically distributed independent random variables, then the expected value of I (w) is 
1 K-l  
K L £{lr(w)}. 
(10.85)  
r=O 
or, since we assume that the periodograms are independent and identically distributed, 
£{l(w)} = £{lr(w)} 
for any r. 
(10.86) 
From Eq. (10.76), it follows that 
(10.87)  

Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform
844 
where L is the window length. When the window w[n] is the rectangular window, the 
method of averaging periodograms is called Bartlett's procedure, and in this case it can 
be shown that 
cww[m] 
{~ -Iml, Iml s (L 
otherwise, 
1), 
(10.88) 
and, therefore, 
Cww(ejW) = (Si~(WL/2»)2
sm«(v/2) 
(10.89) 
That is, the expected value of the average periodogram spectrum estimate is the convo­
lution of the true power spectrum with the Fourier transform of the triangular sequence 
cww[n] that results as the autocorrelation of the rectangular window. Thus, the average 
periodogram is also a biased estimate of the power spectrum. 
To examine the variance, we use the fact that, in general, the variance of the aver­
age of K independent identically distributed random variables is 1/K times the variance 
of each individual random variable. (See Bertsekas and Tsitsiklis, 2008.) Therefore, the 
variance of the average periodogram is 
1
var[I(w)] 
-var[Ir(w)], 
(10.90)
K 
or, with Eq. (10.80), it follows that 
-
1 
2 
var[I(w)]:::::: K Pxx(w). 
(10.91) 
Consequently, the variance of I (w) is inversely proportional to the number of peri­
odograms averaged, and as K increases, the variance approaches zero. 
From Eq. (10.89), we see that as L, the length of the segment xr[n], increases, 
the main lobe of Cww(ejW ) decreases in width, and consequently, from Eq. (10.87), 
e{I(w)} more closely approximates Pxx(w). However, for fixed total data length Q, 
the total number of segments (assuming that L = R) is Q/L; therefore, as L increases, 
K decreases. Correspondingly, from Eq. (10.91), the variance of I (w) will increase. Thus, 
as is typical in statistical cstimation problems, for a fixed data length there is a trade 
off between bias and variance. However, as the data length Q increases, both Land K 
can be allowed to increase, so that as Q approaches 00, the bias and variance of l(w) 
can approach zero. Consequently, periodogram averaging provides an asymptotically 
unbiased, consistent estimate of p.>:x(w). 
The preceding discussion assumed tha t nonoverlapping rectangular windows were 
used in computing the time-dependent periodograms. Welch (1970) showed that if a 
different window shape is used, the variance of the average periodogram still behaves, 
as in Eq. (10.91). Welch also considered the case of overlapping windows and showed 
that if the overlap is one-half the window length, the variance is further reduced by 
almost a factor of 2, due to the doubling of the number of sections. Greater overlap 
does not continue to reduce the variance, because the segments become decreasingly 
independent as the overlap increases. 

845 
Section 10.5 
Fourier Analysis of Stationary Random Signals: the Periodogram 
10.5.4  Computation of Average Periodograms Using 
the DFT 
As with the periadagram, the average periadagram can be explicitly evaluated anly 
at a discrete set af frequencies. Because af the availability af the FFT algarithms for 
camputing the DFf, a particularly canvenient and widely used chaice is the set af 
frequencies Wk = 2Jrk/N far an appropriate chaice af N. Fram Eq. (10.84), we see that 
if the DFf af xr[n] is substituted far the Faurier transfarm af xr[n] in Eq. (10.83), we 
abtain samples of 1(w) at the DFf frequencies Wk 
2Jrk/N far k 
0,1, ... , N - l. 
Specifically, with Xr[k] denating the DFf of xr[n], 
1 
2 
LV IXr[k]1 , 
(1O.92a) 
_ 
I[k] = 
_ 
I(Wk) = 
1 K-l 
K L Ir[k]. 
(lO.92b) 
r=O 
It is worthwhile to' note the relatianship between periodogram averaging and 
the time-dependent Faurier transfarm as discussed in detail in Section 10.3. Equation 
(10.92a) shaws that, except far the introduction of the narmalizing constant l/(LV), 
each individual periadogram is simply the magnitude-squared af the time-dependent 
Faurier transfarm at time r R and frequency 2Jrk/ N. Thus, far each frequency index k, 
the average pawer spectrum estimate at frequency carresponding to' k is the time aver­
age of the time-sampled time-dependent Faurier transfarm. This can be visualized by 
considering the spectrograms in Figure 10.22. The value l[k] is simply the average along 
a harizantalline at frequency 2Jrk/N (or 2Jrk/(NT) in analag frequency).l0 Averaging 
the wide band spectrogram implies that the resulting pawer spectrum estimate will be 
smaoth when considered as a functian af frequency, while the narrowband conditian 
correspands to' longer time windaws and thus, less smaathness in frequency. 
We denate Ir(2Jrk/N) as the sequence Ir[k] and I (2Jrk/N) as the sequence I[k]. 
According toEqs. (1O.92a) and (lO.92b), the average periadogram estimate af the pawer 
spectrum is camputed at N equally spaced frequencies by averaging the magnitude af 
the DFTs afthe windawed data segments with the normalizing factar LV. This methad 
af power spectrum estimation provides a very canvenient framewark within which to 
trade aff between resalution and variance af the spectrum estimate. It is particularly 
simple and efficient to' implement using the FFf algarithms discussed in Chapter 9. An 
important advantage of the methad over thase to be discussed in Section 10.6 is that 
the spectrum estimate is always nonnegative. 
10.5.5  An Example of Periodogram Analysis 
Pawer spectrum analysis is a valuable taal far madeling signals, and it alsO' can be used 
to detect signals, particularly when it comes to' finding hidden periadicities in sampled 
lONote that the spectrogram is normally computed such that the windowcd segments overlap con­
siderably as r varies, while in periodogram averaging R is normally equal to the window length or half the 
window length. 

846 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
3 
2 
1 
0) 
"0 
.S 
0 
~ 
< -1 
-2 
-3 o  
20 
40 
60 
80 
100 
Sample number (n) 
Figure 10.26 
Cosine sequence with white noise. as in Eq. (10.93). 
signals. As an example of this type of application of the average periodogram method, 
consider the sequence 
x[n] = A cos(won + 8) + ern],  
(10.93) 
where (J is a random variable uniformly distributed between 0 and 2n ,is independent of 
ern], and ern] is a zero-mean white-noise sequence that has a constant power spectrum; 
i.e., Pee(w) = a; for all w. In signal models of this form, the cosine is generally the 
desired component and ern] is an undesired noise component. Often, in practical signal 
detection problems, we are interested in the case for which the power in the cosine 
signal is small compared with the noise power. It can be shown (see Problem 10040) that 
over the base period of frequency Iwl s n, the power spectrum for this signal is 
A2n 
Pxx(w) = T[tS(w - wo) + tS(w + wo)] + a; 
for Iwl s n. 
(10.94) 
From Eqs. (10.87) and (10.94), it follows that the expected value of the average peri­
odogramis 
[{l(w)} = 4~~[Cww(ej(W-WO» + Cww(ej(w+wo»] + a;. 
(10.95) 
Figures 10.26 and 10.27 show the use of the averaging method for a signal of the form 
of Eq. (10.93), with A = 0.5, wo = 2n/21, and random phase 0 S 8 < 2n. The noise 
was uniformly distributed in amplitude such that -./3 < ern] S ./3. Therefore, it is 
easily shown that a; = 1. The mean ofthe noise component is zero. Figure 10.26 shows 
101 samples of the sequence x[n). Since the noise component ern] has a maximum 
amplitude ./3, the cosine component in the sequence x[n] (having period 21) is not 
visually apparent. 
Figure 10.27 shows average periodogram estimates of the power spectrum for 
rectangular windows with amplitude 1, so that U = 1, and of lengths L = 1024,256,64, 
and 16, with the total record length Q = 1024 in all cases. Except for Figure 10.27(a), 
the windows overlap by one-half the window length. Figure 1O.27(a) is the periodogram 
of the entire record, and Figures 1O.27(b), (c), and (d) show the average periodogram 

60 
50 
40
.., 
"0 
:a
E 30 
e 
-< 20 
128 
256 
384 
512 
Sample number (k) 
(a) 
20 
15 
.., 
"0 
:;j 
.1:: 
10
0.. 
8 
-< 
5 
128 
256 
384 
512 
Sample number (k) 
(b) 
6r-----------------------------------~ 
5 
] 
4 
:a 3 
8 
-< 
2 
1 
OL-------~--------~---------L--____~ 
o  
128 
256 
384 
512 
Sample number (k) 
(c) 
Figure 10.27 
Example of average periodogram for signal of length Q = 1024. 
(a) Periodogram for window length L = Q = 1024 (only one segment). (b) K 
7 
and L= 256 (overlap by L/2). (c) K = 31 and L= 64. 
847 

!""""", 
Chapter 1 0 
Fourier Analysis of Signals Using the Discrete Fourier Transform
848 
(!) 
"0 
E 
'a 
8 
<: 
0.600 
00 
128 
256 
384 
512 
Sample number (k) 
(d) 
Figure 10.27 (continued) (d) K = 127 and L = 16. 
for K 
7, 31, and 127 segments, respectively. In all cases, the average periodogram was 
evaluated using 1024-point DFfs at frequencies Wk = 2rrk/1024. (For window lengths 
L < 1024, the windowed sequence was augmented with zero-samples before comput­
ing the DFf.) Therefore, the frequency Wo = 2rr/21 lies between DFf frequencies 
W48 = 2rr48/1024 and W49 = 2rr49j1024. 
In using such estimates of the power spectrum to detect the presence and/or the 
frequency of the cosine component, we might search for the highest peaks in the spec­
trum estimate and compare their size with that of the surrounding spectrum values. 
From Eqs. (10.89) and (10.95), the expected value of the average periodogram at the 
frequency wo is 
A2L 
£(/(wo)} 
(10.96)
4+ 
Thus, if the peak due to the cosine component is to stand out against the variability of the 
average periodogram, then in this special case, we must choose L so that A2L/4» (f;. 
This is illustrated by Figure 10.27(a), where L is as large as it can be for the record length 
Q. We see that L 
1024 gives a very narrow main lobe of the Fourier transform of 
the autocorrelation of the rectangular window, so it would be possible to resolve very 
closely spaced sinusoidal signals. Note that for the parameters of this example (A 
0.5,
(f; = 1) and with L = 1024, the peak amplitude in the periodogram at frequency 2rr/21 
is close, but not equal, to the expected value of 65. We also observe additional peaks 
in the periodogram with amplitudes greater than 10. Clearly, if the cosine amplitude 
A had been smaller by only a factor of 2, it is possible that its peak would have been 
confused with the inherent variability of the periodogram. 
We have seen that the only sure way to reduce the variance of the spectrum esti­
mate is to increase the record length of the signal. This is not always possible, and even 
if it is possible, longer records require more processing. We can reduce the variability 

849 
Section 10.6 
Spectrum Analysis of Random Signals 
of the estimate while keeping the record length constant if we use shorter windows and 
average over more sections. The cost of doing this is illustrated by parts (b), (c), and 
(d) of Figure 10.27. Note that as more sections are used, the variance of the spectrum 
estimate decreases, but in accordance with 
(10.96), so does the amplitude of the 
peak as a result of the cosine. Thus, we again face a trade-off. That the shorter windows 
reduce variability is clear, especially if we compare the high-frequency regions away 
from the peak in parts (a), (b) and (c) of Figure 10.27. Recall that the idealized power 
spectrum of the model for the pseudorandom-noise generator is a constant (o} = 1) 
for all frequencies. In Figure 10.27(a) there are peaks as high as about 10 when the true 
spectrum is 1. In Figure 1O.27(b), the variation away from 1 is less than about 3, and in 
Figure 10.27(c), the variation around 1 is less than 0.5. However, shorter windows also 
reduce the peak amplitude of any narrowband component, and they also degrade our 
ability to resolve closely spaced sinusoids. This reduction in peak amplitude is also clear 
from Figure 10.27. Again, if we were to reduce A by a factor of 2 in Figure 10.27(b), 
the peak height would be approximately 4, which is not much different from many of 
the other peaks in the high-frequency region. In Figure 1O.27(c) a reduction of A by a 
factor of 2 would make the peak approximately 1.25, which would be indistinguishable 
from the other ripples in the estimate. In Figure 10.27 (d), the window is very short, and 
thus the fluctuations of the spectrum estimate are greatly reduced, but the spectrum 
peak due to the cosine is very broad and barely above the noise even for A 
0.5. If 
the length were any smaller, spectral leakage from the negative-frequency component 
would cause there to be no distinct peak in the low-frequency region. 
This example confirms that the average periodogram provides a straightforward 
method of trading off between spectral resolution and reduction of the variance of the 
spectrum estimate. Although the theme of the example was the detection of a sinusoid 
in noise, the average periodogram could also be used in signal modeling. The spectrum 
estimates of Figure 10.27 clearly suggest a signal model of the form of Eq. (10.93), and 
most of the parameters of the model could be estimated from the average periodogram 
power spectrum estimate. 
10.6  SPECTRUM ANALYSIS OF RANDOM SIGNALS USING ESTIMATES 
OF THE AUTOCORRELATION SE~UENCE 
In the previous section, we considered the periodogram as a direct estimate of the 
power spectrum of a random signal. The periodogram or the average periodogram is 
a direct estimate in the sense that it is obtained directly by Fourier transformation 
of the samples of the random signal. Another approach, based on the fact that the 
power density spectrum is the Fourier transform of the autocorrelation function, is to 
first obtain an estimate of the autocorrelation function ~xxrm] for a finite set of lag 
values -M :s m :s M, and then apply a window wdm] before computing the DTFT of 
this estimate. This approach to power spectrum estimation is often referred to as the 
Blackman-Tukey method. (See Blackman and Thkey, 1958.) In this section, we explore 
some of the important facets of this approach and show how the DFT can be used to 
implement it. 

850 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
Let us assume, as before, that we are given a finite record of a random signal x[n]. 
This sequence is denoted 
[ ] _ {x[n] for 0:5 n :5 Q 
1, 
(10.97)
v n -
0 
otherwise. 
Consider an estimate of the autocorrelation sequence as 
A 
! 
1 
¢xx[m] = Qcvv[m], 
(1O.98a) 
where, since cvv[-m] = cvv[m], 
Q-l 
Q-Im[-l 
c,,[ml ~ ?; vlnlvln + ml ~ o?; x[nlx[n + 1m II. 
Iml:5 Q -1, (1O.98b) 
otherwise, 
corresponding to the aperiodic correlation of a rectangUlarly windowed segment ofx[n) 
of length Q. 
To determine the properties of this estimate of the autocorrelation sequence, we 
consider the mean and variance of the random variable <pxx[m}. From Eqs. (1O.98a) and 
(10.98b), it follows that 
Q-Im[-l 
Q-[m[-l 
A 
1" 
1 
" 
£{¢xx[m)} = Q t'o 
£{x[n]x[n + 1m!)} = Q t'o 
¢xx[m), 
(10.99) 
and since ¢xx[m) does not depend on n for a stationary random process, 
(Q -Iml) 
iml :5 Q 
1,
£{¢xx[m]} = 
0 
Q 
¢xx[m], 
otherwise. 
A 
{ 
(10.100) 
FromEq. (10.100), we see that <Pxx [m] isa biasedestimateof¢xx[m], since£{<pxxlm]} 
is not equal to ¢x.•;[m], but the bias is small if Im\ « Q. We see also that an unbiased 
estimator of the autocorrelation sequence for Iml :5 Q - 1 is 
¢xx[m] = ( 
1 
) cvv[m]; 
(1O.10n
Q-Iml 
. 
i.e., the estimator is unbiased if we divide by the number of nonzero terms in the sum 
of lagged products involved in computing each value of cvv[m], rather than by the total 
number of samples in the data record. 
The variance of the autocorrelation function estimates is difficult to compute, 
even with simplifying assumptions. However, approximate formulas for the variance of 
both <pxx[m] and ¢xx[m] can be found in Jenkins and Watts (1968). For our purposes 
here, it is sufficient to observe from Eq. (1O.98b) that as Iml approaches Q, fewer and 
fewer samples of x[n] are involved in the computation of the aut0correlation estimate; 
therefore, the variance of the autocorrelation estimate can be expected to increase 
with increasing Iml. In the case of the periodogram, this increased variance affects 
the spectrum estimate at all frequencies, because all the autocorrelation lag values 
are implicitly involved in the computation of the periodogram. However, by explicitly 
computing the autocorrelation estimate, we are free to choose which correlation lag 

851 
Section 10.6 
Spectrum Analysis of Random Signals 
values to include when estimating the power spectrum. Thus, we define the power 
spectrum estimate 
M-l 
Sew) 
L 
¢xx[m]wdm]e-jwm , 
(10.102) 
m=-{M-l) 
where wdm] is a symmetric window of length (2M 
1) applied to the estimated auto­
correlation function. We require that the product of the autocorrelation sequence and 
the window be an even sequence when x [n] is real, so that the power spectrum estimate 
will be a real, even function of w. Therefore, the correlation window must be an even 
sequence. By limiting the length of the correlation window so that M « Q, we include 
only autocorrelation estimates for which the variance is low. 
The mechanism by which windowing the autocorrelation sequence reduces the 
variance of the power spectrum estimate is best understood in the frequency domain. 
From Eqs. (10.68), (10.69), and (10.98b), it follows that, with w[n] = 1 for 0 :::; n :::; 
(Q - 1), i.e., a rectangular window, the periodogram is the Fourier transform of the 
autocorrelation estimate ¢xx[m]; i.e., 
A 
1 
Fl. 2 
tPxx[m] = Qcvv[m] +---+ QIV(eJa»1 
lew). 
(10.103) 
Therefore, from Eq. (10.102), the spectrum estimate obtained by windowing of ¢xx[m] 
is the convolution 
Sew) = -1 llr 1(6)W (ej (a>-lI»)d6. 
(10.104)
c 
2:rr -lr 
From Eq. (10.104), we see that the effect of applying the window wc[m] to the auto­
correlation estimate is to convolve the periodogram with the Fourier transform of the 
autocorrelation window. This will smooth the rapid fluctuations of the periodogram 
spectrum estimate. The shorter the correlation window, the smoother the spectrum 
estimate will be, and vice versa. 
The power spectrum Pxx (w) is a nonnegative function of frequency, and the peri­
odogram and the average periodogram automatically have this property by definition. 
In contrast, from Eq. (10.104), it is evident that nonnegativity is not guaranteed for 
Sew), unless we impose the further condition that 
for -:rr < w:::;:rr. 
(10.105) 
This condition is satisfied by the Fourier transform of the triangular (Bartlett) win­
dow, but it is not satisfied by the rectangular, Hanning, Hamming, or Kaiser windows. 
Therefore, although these latter windows have smaller side lobes than the triangular 
window, spectral leakage may cause negative spectrum estimates in low-level regions 
of the spectrum. 
The expected value of the smoothed periodogram is 
M-l 
E{S(w)} = L 
E{¢xx [m]}wclm]e-ja>m 
m=-(M-l) 
(10.106) 
M-l 
( Q 
Iml ) 
-ja>m
L 
tPxxlm] 
Q 
wc[m]e 
. 
m=-(M-l) 

852 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
If Q » M, the term (Q -Iml}/Q in Eq. (10.106) can be neglected,11 so we obtain 
M-l 
1 I1/:
£{S(w)};;;::: L 
q)xx[m]wc[m]e-jwm = 2 
Pxx (B) WdeJ(w-IJ»dB. (10.107) 
n -1/:
m=-(M-1) 
Thus, the windowed autocorrelation estimate leads to a biased estimate of the power 
spectrum. Just as with the average periodogram, it is possible to trade spectral resolution 
for reduced variance of the spectrum estimate. Ifthe length of the data record is fixed, we 
can have lower variance if we are willing to accept poorer resolution of closely spaced 
narrowband spectral components, or we can have better resolution if we can accept 
higher variance. If we are free to observe the signal for a longer time (Le., increase 
the length Q of the data record), then both the resolution and the variance can be 
improved. The spectrum estimate Sew) is asymptotically unbiased if the correlation 
window is normalized so that 
1
2n I1/:-1/: Wc(eJW)dw = 1 
we [0]. 
(10.108) 
With this normalization, as we increase Q together with the length of the correlation 
window, the Fourier transform of the correlation window approaches a periodic impulse 
train and the convolution of Eq. (10.107) duplicates Pxx (w). 
The variance of Sew) has been shown (see Jenkins and Watts, 1968) to be of the 
form 
1 
M-l 
)
var[S(w)] ~ ( Q L 
w~[m] P;x(w). 
(10.109) 
m=-(M-l) 
Comparing Eq. (10.109) with the corresponding result in Eq. (10.80) for the peri­
odogram leads to the conclusion that, to reduce the variance of the spectrum esti­
mate, we should choose M and the window shape, possibly subject to the condition of 
Eq. (10.105), so that the factor 
1 
M-l 
) 
(10.110)
( Q m=E-l) w~[m] 
is as small as possible. Problem 10.37 deals with the computation of this variance reduc­
tion factor for several commonly used windows. 
Estimation of the power spectrum based on the Fourier transform of an estimate 
of the autocorrelation function is a clear alternative to the method of averaging peri­
odograms. Itis not necessarily better inany general sense; it simply has different features, 
and its implementation would be different. In some situations, it may be desirable to 
compute estimates of both the autocorrelation sequence and the power spectrum, in 
which case it would be natural to use the method of this section. Problem 10.43 explores 
the issue of determining an autocorrelation estimate from the average periodogram. 
11 More precisely, we could define an effective correlation window we[m] = wdm](Q - imi)/Q. 

Section 10.6 
Spectrum Analysis of Random Signals  
853 
10.6.1  Computing Correlation and Power Spectrum 
Estimates Using the DFT 
The autocorrelation estimate 
Q-Iml-l 
~  
1 
"" 
¢xx[m] =  Q 
~ x[n]x[n + 1m!] 
(10.111) 
is required for 1m I :s M - 1 in the method of power spectrum estimation that we are 
considering. Since ¢xx[-m] 
¢xx[m], it is necessary to compute Eq. (10.111) only for 
nonnegative values of m, i.e., for 0 :s m :s M -
1. The DFf and its associated fast 
computational algorithms can be used to advantage in the computation of ¢xx [m], if we 
observe that ¢xxlm] is the aperiodic discrete convolution of the finite-length sequence 
x[n] with x[-n]. If we compute X[k], the N-point DFf of x[n], and multiply by X*[k], 
we obtain IX[k]12 , which corresponds to the circular convolution of the finite-length 
sequence x[n] with x[«-n))Nl, i.e., a circular autocorrelation. As our discussion in Sec­
tion 8.7 suggests, and as developed in Problem 10.34, it should be possible to augment 
the sequence x[n] with zero-valued samples and force the circular autocorrelation to 
be equal to the desired aperiodic autocorrelation over the interval 0 :s m :s M 
1. 
To see how to choose N for the DFf, consider Figure 10.28. Figure 1O.28(a) shows 
the two sequences x[n] and x[n + m] as functions of n for a particular positive value of 
m. Figure l0.28(b) shows the sequences x(n] and x[((n + m»N] that are involved in the 
circular autocorrelation corresponding to IX[k]12. Clearly, the circular autocorrelation 
will be equal to Q¢xx [m] for 0 :s m :s M - 1 if x [((n +m)hv1does not wrap around and 
overlap x [n] when 0 :s m :s M - 1. From Figure 1 0.28(b), it follows that this will be the 
case whenever N - (M -1) 2: Q or N 2: Q + M 
1. 
x[n +m]  
I 
x[n]  
--:!JIII[!I!!IfII~IJ!!!~ 
n 
Q I-m 
Q 
(a) 
(b) 
figure 10.28 Computation of the 
circular autocorrelation. (a) x[n] and 
x[n + m] for afinite-length sequence of 
length Q. (b) x[n] and x[«n + m)N] as 
in circular correlation. 

854 
Chapter 10  
Fourier Analysis of Signals Using the Discrete Fourier Transform 
In summary, we can compute ¢xx[m] forO::: m ::: M-l by the following procedure: 
1. Form an N-point sequence by augmenting x[n] with (M -1) zero-samples. 
2. Compute the N -point DFf, 
N-l 
X[k]  L x[nle- j(2rr/N)kn 
for k = 0, L ... , N - 1. 
n=O 
3.  Compute  
IX[kJI2 
X[k]X*[k] 
for k 
0, 1, ... , N - 1.  
4.  Compute the inverse DFT of IX[k1l2 to obtain  
1 N-l  
j
cvv[m] = N  L IX[k]1 2e (2rrI N)km 
for m 
0,1, ... , N-1. 
k=O 
5. Divide the resulting sequence by Q to obtain the autocorrelation estimate 
A 
1 
¢xx[m] = Qcvv[m] 
form=O,I, ... ,M-1. 
This is the desired set of autocorrelation values, which can be extended symmet­
rically for negative values of m. 
If M is small, it may be more efficient simply to evaluate Eq. (10.111) directly. In 
this case, the amount of computation is proportional to Q. M. In contrast, if the DFTs 
in this procedure are computed using one of the FFT algorithms discussed in Chapter 9 
with N ~ Q+M -1, the amount of computation will be approximately proportional to 
N log2 N for N a power of 2. Consequently, for sufficiently large values of M, use of the 
FFT is more efficient than direct evaluation ofEq. (10.111). The exact break-even value 
of M will depend on the particular implementation ofthe DFT computations; however, 
as shown by Stockham (1966), this value would probably be less than M = 100. 
To reduce the variance of the estimate of the autocorrelation sequence or the 
power spectrum estimated from it, we must use large values of the record length Q. This 
is not generally a problem with computers having large memories and fast processors. 
However, since M is generally much less than Q, it is possible to section the sequence 
x[n] in a manner similar to the procedures that were discussed in Section 8.7.3 for 
convolution ofa finite-length impulse response with an indefinitely long input sequence. 
Rader (1970) presented a particularly efficient and flexible procedure that uses many 
of the properties of the DFT of real sequences to reduce the amount of computation 
required. The development of this technique is the basis for Problem 10.44. 
Once the autocorrelation estimate has been computed, samples of the power 
spectrum estimate Sew) can be computed at frequencies Wk = 2rrk/N by forming the 
finite-length sequence 
I 
¢xx[m]wdm], 
o ::: m ::: M -
1, 
s[m] =  
0, 
M:::m::: N 
M, 
(10.112) 
¢xAN 
m]wc[N 
m], 
N-M+1:::m:::N 
1. 
where wc[m] is the symmetric correlation window. Then the DFT of s[m] is 
S[k] 
S(w)Iw=2rrkIN' 
k 
0,1, ... , N -1, 
(10.113) 

855 
Section 10.6 
Spectrum Analysis of Random Signals 
x[n] 
Q[x[nlJ 
+ 
Figure 10.29 Procedure for obtaining 
quantization noise sequence. 
where S(w) is the Fourier transform of the windowed autocorrelation sequence as de­
fined by Eq. (10.102). Note that N can be chosen as large as is convenient and practical, 
thereby providing samples of Sew) at closely spaced frequencies. However, as our dis­
cussions in this chapter have consistently shown, the frequency resolution is always 
determined by the length and shape of the window wclm]. 
10.6.2  Estimating the Power Spectrum of Quantization 
Noise 
In Chapter 4, we assumed that the error introduced by quantization has the properties of 
a white-noise random process. The techniques discussed so far in this chapter were used 
to compute the power spectrum estimates of Figure 4.60 that were used to suggest the 
validity of this approximation. In this section, we provide additional examples of the use 
of estimates of the autocorrelation sequence and power spectrum estimation in studying 
the properties of quantization noise. The discussion will reinforce our confidence in the 
white-noise model, and it will also offer an opportunity to point out some practical 
aspects of power spectrum estimation. 
Consider the experiment depicted in Figure 10.29. A lowpass-filtered speech sig­
nal xc(t) was sampled at a 16-KHz rate, yielding the sequence of samples x[n] that were 
plotted in Figure 10.21.12 These samples were quantized with a 1O-bit linear quantizer 
(B = 9), and the corresponding error sequence ern] 
Q[x[n)) - x[n] was computed. 
Figure 10.30 shows 2000 consecutive samples of the speech signal plotted on the first 
and third lines of the graph. The second and fourth lines show the corresponding quan­
tization error sequence. Visual inspection and comparison of these two plots tends to 
strengthen our belief in the previously assumed model; i.e., that the noise appears to 
vary randomly throughout the range 
< ern] :s 2-(B+l). However, such qual­
itative observations can be misleading. The flatness of the quantization noise spectrum 
can be verified only by estimating the autocorrelation sequence and power spectrum of 
the quantization noise ern]. 
Figure 10.31 shows estimates of the autocorrelation and power spectrum of the 
quantization noise for a record length of Q 
3000 samples. The autocorrelation se­
quence estimate was calculated over the range of lags Iml :s 100 using Eqs. (10.98a) 
and (10.98b). The resulting estimate is shown in Figure 10.31(a). Over this range, 
-1.45 x 10-8 < $[m] :s 1.39 x 10-8 except for $[0] = 3.17 x 10-7. The autocorre­
lation estimate suggests that the sample-to-sample correlation of the noise sequence is 
quite low. The resulting autocorrelation estimate was multiplied by Bartlett windows 
12Although the samples were originally quantized to 12 bits by the AID converter, for purposes of this 
experiment, they were scaled to a maximum value of 1, and a small amount of random noise was added to the 
samples. We assume that these samples are "unquantized," i.e., we consider the 12-bit samples to effectively 
be unquantized relative to the subsequent quantization that we are applying in this discussion. 

856 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
100 
200 
300 
400 
500 
600 
700 
800 
900 
Sample index 
Figure 10.30 
Speech waveform (first and third lines) and the corresponding 
quantization error (second and fourth lines) for 1O-bit quantization (magnified 29 
times). Each line corresponds to 1000 consecutive samples connected by straight 
lines for convenience in plotting. 
o 
1000 
with M = 100 and M = 50. The windows are shown in Figure 10.31 superimposed 
on ¢[m] (with scaling so that they can be plotted on the same axes) and the corre­
sponding spectrum estimates, computed as discussed in Section 10.6.1, are shown in 
Figure 10.31(b). 
As seen in Figure 10.31(b), the Blackman-Tukey spectrum estimate for M 
100 
(the thin continuous line) shows somewhat erratic fluctuations about the dashed line 
plotted at the spectrum level 10 logl0(2-18/12) = -64.98 dB (the value of the white 
power spectrum with u; 
2-2B /12 for B 
9). The heavy line shows the power 
spectrum estimate for M = 50. We see from Figure 10.31(b) that the spectrum estimate 
is within ±2 dB of the spectrum of the white-noise approximation for B + 1 = 10 for 
all frequencies. As discussed in Section 10.6, the shorter window gives smaller variance 
and a smoother spectrum estimate resulting from the lower frequency resolution of the 
shorter window. In either case, the spectrum estimate seems to support the validity of 
the white-noise model for quantization noise. 
Although we have computed quantitative estimates of the autocorrelation and the 
power spectrum, our interpretation of these measurements has been only qualitative. It 

851 
1000 
2000 
3000 
4000 
5000 
6000 
7000 
-1 
'---__-'--___~___'__~....__"_I____..J'_____L_____'____...l.____'___....J 
-100 
-80 
-60 
-40 
-20 
o 
20 
40 
60 
80 
100 
Time index 
-63,----.----.----.~---_r----,_-----,_-----,_--_, 
8000 
Figure 10.31 
(a) Autocorrelation estimate for 10-bit quantization noise for Iml :"0 100 
with record length 0 = 3,000. (b) Power spectrum estimates by the Blackman-Tukey 
method using Bartlett windows with M 
100 and M 
50. (Dashed line shows level 
of 10 log10(2-18/12).) 
e­ 
in 
is reasonable now to wonder how small the autocorrelation would be if e[n] were really a  
white-noise process. To give quantitative answers to such questions, confidence intervals 
)Q 
for our estimates could be computed and statistical decision theory applied. (See Jenkins 
Ie 
and Watts (1968), for some tests for white noise.) In many cases, however, this additional 
te 
statistical treatment is not necessary. In a practical setting, we are often comfortable and 
er 
content simply with the observation that the normalized autocorrelation is very small 
te 
everywhere, except at m = O. 
Dr 
Among the many important insights of this chapter is that the estimate of the au­
::e 
tocorrelation and power spectrum ofa stationary random process should improve if the 
Ie 
record length is increased. This is illustrated by Figure 10.32, which corresponds to Fig­
of 
ure 10.31, except that Q was increased to 30,000 samples. Recall that the variance of the 
autocorrelation estimate is proportional to 1/Q. Thus, increasing Q from 3000 to 30,000 
Ie 
should bring about a tenfold reduction in the variance of the estimate. A comparison 
It 
of Figures 1O.31( a) and 10.32(a) seems to verify this result. For Q = 3000, the estimate 
Frequency in Hz 
m 
Section 10.6 
4 
3 
to: 
.2 
~ 2
<3 
.... 
.... 
0 u 
0 :; « 
0 
Spectrum Analysis of Random Signals 
xl0-7 

858 
-65.5 I~-~~--=~--:::=---~----L---.l---....l---.-J 
5000 
6000 
7000 
8000 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
X1Q-7  
4,-­ ~--------=-~---= 
3 
§E 2 
~ 
~ 
g 1 
~ 
o ~6 
....... 
4' 
uCi"..  
'I '...... 
~ 
'1I"Ia4 
t " 
4~  
-1 I~----~------~----~------~----~------~----~~----~------~----~ 
-100 
-80 
-60 
-40 
-20 
0 
20 
40 
60 
80 
UIO 
Time index 
(a) 
~ 
*...'. 
~.5r'------,-------~------,_------._------,_----_.------~------~ 
c:;' 
::3­
§ 
'[ 
65 
'" lil 
~ 
o  
1000 
2000 
3000 
4000 
Frequency in Hz 
(b) 
Figure 10.32 
(a) Autocorrelation estimate for 10-bit quantization noise; record length 
Q = 30,000. (b) Power spectrum estimates by the Blackman-Tukey method using Bartlett 
windows with M = 100 and M 
50. 
falls between the limits -1.45 x 10-8 ::s $[m] ::s 1.39 x 10-8 , while for Q = 30,000, the 
limits are -4.5 x 10-9 ::s $[m] ::s 4.15 x 10-9• Comparing the range of variation for 
Q = 3000 with the range for Q = 30,000 indicates that the reduction is consistent with 
the tenfold reduction in variance that we expected.I3 We note from Eq. (10.110) that 
a similar reduction in variance of the spectrum estimate is also expected. This is again 
evident in comparing Figure 10.31(b) with Figure 1O.32(b). (Be sure to note that the 
scales are different between the two sets of plots.) The variation about the white-noise 
approximate spectrum level is only ±0.5 dB in the case of the longer record length. 
Note that the spectrum estimates in Figure 1O.32(b) display the same trade off between 
variance and resolution. 
In Chapter 4 we argued that the white-noise model was reasonable, as long as the 
quantization step size was small. When the number of bits is small, this condition does 
13RecaJl that a reduction in variance by a factor of 10 corresponds to a reduction in amplitude by a 
factor of v'iO "" 3.16. 

859 
Section 10.6 
Spectrum Analysis of Random Signals 
o 
100 
200 
300 
400 
500 
Sample index 
600 
700 
800 
900 
1000 
Figure 10.33 
Speech waveform (first and third lines) and the corresponding 
quantization error (second and fourth lines) for 4-bit quantization (magnified 23 
times). Each line corresponds to 1000 consecutive samples connected by straight 
lines for convenience in plotting. 
not hold. To see the effect on the quantization noise spectrum, the previous experiment 
was repeated using only 16 quantization levels, or 4 bits. Figure 10.33 shows the speech 
waveform and quantization error for 4-bit quantization. Note that some portions of the 
error waveform tend to look very much like the original speech waveform. We would 
expect this to be reflected in the estimate of the power spectrum. 
Figure 10.34 shows the autocorrelation and power spectrum estimates of the error 
sequence for 4-bit quantization for a record length of 30,000 samples. In this case, the 
autocorrelation shown in Figures 1O.34(a) is much less like the ideal autocorrelation for 
white noise. This is not surprising in view of the obvious correlation between the signal 
and noise displayed in Figure 10.33. Figure 1O.34(b) shows the power spectrum estimates 
for Bartlett windows with M = 100 and M 
50, respectively. Clearly, the spectrum is 
not flat, although the general level reflects the average noise power. In fact, as we shall 
see, the quantization noise tends to have the general shape of the speech spectrum. 
Thus, the white-noise model for quantization noise can be viewed only as a rather crude 
approximation in this case, and it would be less valid for coarser quantization. 

Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform
860 
xlfr-4  
10  
8 
Q 
.~ 
6
1;;i 
-.;...... 
4
0 
<J B 
:::I 
2  
<r.:  
0 
-2 
-100 
-80 
-60 
-40 
-20 
o 
20 
40 
60 
80 
100 
Time index 
(a) 
-22 
-24 
~ 
2. -26  
5  
2 -28  
~  
~ -30  
\) 
~ -32 
-34 
-36 
i 
~----------------------
0 
1000 
2000 
3000 
4000 
5000 
6000 
7000 
8000 
Frequency in Hz 
(b) 
Figure 10.34 
(a) Autocorrelation estimate for 4-bit Quantization noise; record length 
Q = 30,000. (b) Power spectrum estimates by the Blackman-Tukey method using Bartlett 
windows with M 
100 and M = 50. (Dashed line shows level of 10 log10(2-6/12).) 
The example of this section illustrates how autocorrelation and power spectrum 
estimates can be used to support theoretical models. Specifically, we have demonstrated 
the validity of some of our basic assumptions in Chapter 4, and we have given an 
indication of how these assumptions break down for very crude quantization. This is 
only a rather simple, but useful, example that shows how the techniques of the current 
chapter can be applied in practice. 
10.6.3 Estimating the Power Spectrum of Speech 
We have seen that the time-dependent Fourier transform is particularly well-suited to 
the representation of speeeh signals, since it can track the time-varying nature of the 
speech signal. However, in some cases, it is useful to take a different point of view. In 
particular, even though the waveform of speech as in Figure 10.21 shows significant 
variability in time, as does its time-dependent Fourier transform in Figure 10.22, it is 
nevertheless possible to assume that it is a stationary random signal and apply our 

0 
~ -10 
il: 
""'8 -20 
::0.. 
~ -30 
0.. 
'" il --40 
~ 
.:e -50 
-60 0 
1000 
2000 
3000 
4000 
5000 
6000 
7000 
8000 
Section 10.6 
Spectrum Analysis of Random Signals 
861 
X1Q-3 
15r-----r_----,-----~----or----_r----~----_,----~r_----r_--__. 
I" 
10 
.S 
Qi '" .. c 
5 
E 
~ 
0 
-40 
-30 
-20 
-10 
0 
10 
20 
30 
40 
50 
Time index 
(a) 
Frequency in Hz 
(b) 
Figure 10.35 (a) Autocorrelation estimate for speech signal of Figure 10.21; record length 
Q = 30,000. (b) Power spectrum estimates by the Blackman-Tukey method using Bartlett 
window (heavy line) and Hamming window (light line) with M = 50. 
long-term spectrum analysis techniques. These methods average over a time interval 
that is much longer than the changing events of speech. This gives a general spectrum 
shape that can be useful in designing speech coders and in determining the bandwidth 
requirements for speech transmission. 
Figure 10.35 shows an example of estimating the power spectrum of speech us­
ing the Blackman-Thkey method. The autocorrelation sequence estimated from Q = 
30,000 samples of the speech signal in Figure 10.21 is shown in Figure 1O.35(a), together 
with Bartlett and Hamming windows of length 2M + 1 = 101. Figure 1O.35(b) shows 
the corresponding power spectrum estimates. The two estimates are grossly similar but 
dramatically different in detail. This is because of the nature of the DTFTs of the win­
dows. Both have the same main-lobe width AWm = 8rr1M, however their side lobes 
are very different. The side lobes of the Bartlett window are strictly nonnegative, while 
those of the symmetric Hamming window (which are smaller than those of the Bartlett 
window) are negative for some frequencies. When convolved with the periodogram 

862 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
corresponding to the autocorrelation estimate, this yields the dramatically different 
results shown. 
The Bartlett window guarantees a positive spectrum estimate for all frequencies. 
However, this is not true for the Hamming window. The effect of this is particularly 
pronounced in regions of rapid variability of the periodogram, where side lobes due to 
adjacent frequencies can cancel orinterfere to produce negative spectrum estimates. The 
dots in Figure 1O.35(b) show the frequencies where the spectrum estimate was negative. 
When plotting in dB, it is necessary to take the absolute value of the negative estimates. 
Thus, while the Bartlett window and the Hamming window have the same main-lobe 
width, the positive side lobes ofthe Bartlett window tend to fill in gaps between relatively 
strong frequencies, while the lower side lobes of the Hamming window lead to less 
leakage between frequencies, but the danger of negative spectrum estimates as positive 
and negative side lobes interact. 
The Hamming window (or other windows such as the Kaiser window) can be 
used in spectrum estimation without danger of negative estimates if they are used 
in the method of averaging periodograms that are discussed in Section 10.5.3. This 
method guarantees positive estimates, because positive periodograms are averaged. 
Figure 10.36 shows a comparison of the Blackman-Tukey estimates of Figure 10.35(b) 
with an estimate obtained by the Welch method of averaging modified periodograms. 
The heavy dashed line is the Welch estimate. Note that it follows the general shape of 
the other two estimates, but it differs significantly in the high frequency region, where 
the speech spectrum is naturally small, and where the frequency response of the analog 
antialiasing filter causes the spectrum to be very small. Because of its superior ability 
to deliver consistent resolution for spectra with wide dynamic range, and because it is 
easily implemented using the DFT, the method of averaging periodograms is widely 
used in many practical applications of spectrum estimation. 
All the spectrum estimates in Figure 10.36 show that the speech signal is character­
ized by a peak below 500 Hz and a fall-off with increasing frequency by 30 to 40 dB at 6 
KHz. Several prominent peaks between 3 KHz and 5 KHz could be due to higher vocal 
tract resonances that do not vary with time. A different speaker or different speech 
material would certainly produce a different spectrum estimate, but the general nature 
of the spectrum estimates would be similar to those of Figure 10.36. 
10.7 SUMMARY 
One of the important applications of signal processing is spectrum analysis of signals. 
Because of the computational efficiency of the FFT, many of the techniques for spec­
trum analysis of continuous-time or discrete-time signals use the DFT either directly or 
indirectly. In this chapter, we explored and illustrated some of these techniques. 
Many of the issues associated with spectrum analysis are best understood in the 
context of the analysis of sinusoidal signals. Since the use of the DFT requires finite­
length signals, windowing must be applied in advance of the analysis. For sinusoidal 
signals, the width of the spectral peak observed in the DFT is dependent on the win­
dow length, with an increasing window length resulting in the sharpening of the peak. 
Consequently, the ability to resolve closely spaced sinusoids in the spectrum estimate 

863 
-50 
\ 
Section 10.7 
Summary 
O.------,------~------~------_.------_r------~------~----~ 
-60 
-10 
-20 
\ " ....... 
\ 
-w~----~------~------~------~------~------~------~----~ 
o  
1000 
2000 
3000 
4000 
5000 
6000 
7000 
8000 
Frequency in Hz 
Figure 10.36 Power spectrum estimates by the Blackman-Tukey method using Bartlett  
window (heavy line) and Hamming window (light line) with M = 50. The dashed line shows  
the power spectrum obtained by averaging overlapping periodograms using a Hamming  
window with M = 50.  
decreases as the window becomes shorter. A second, independent effect inherent in 
spectrum analysis using the DFr is the associated spectral sampling. Specifically, since 
the spectrum can be computed only at a set of sample frequencies, the observed spec­
trum can be misleading if we are not careful in its interpretation. For example, important 
features in the spectrum may not be directly evident in the sampled spectrum. To avoid 
this, the spectral sample spacing can be reduced by increasing the D Frsize in one of two 
ways. One method is to increase the DFr size while keeping the window length fixed 
(requiring zero-padding of the windowed sequence). This does not increase resolution. 
The second method is to increase both the window length and the DFr size. In this case, 
spectral sample spacing is decreased, and the ability to resolve closely spaced sinusoidal 
components is increased. 
While increased window length and resolution are typically beneficial in the spec­
trum analysis of stationary data, for time-varying data, it is generally preferable to keep 
the window length sufficiently short, so that over the window duration, the signal char­
acteristics are approximately stationary. This leads to the concept of the time-dependent 

864 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
Fourier transform, which, in effect, is a sequence of Fourier transforms obtained as the 
signal slides past a finite-duration window. A common and useful interpretation of the 
time-dependent Fourier transform is as a bank of filters, with the frequency response 
of each filter corresponding to the transform of the window, frequency shifted to one 
of the DFT frequencies. The time-dependent Fourier transform has important applica­
tions both as an intermediate step in filtering signals and for analyzing and interpreting 
time-varying signals, such as speech and radar signals. Spectral analysis ofnonstationary 
signals typically involves a trade-off between time and frequency resolution. Specifically, 
our ability to track spectral characteristics in time increases as the length of the analysis 
window decreases. However, a shorter analysis window results in decreased frequency 
resolution. 
The DFT also plays an important role in the analysis of stationary random sig­
nals. An intuitive approach to estimating the power spectrum of random signals is to 
compute the squared magnitude of the DFT of a segment of the signal. The reSUlting 
estimate, called the periodogram, is asymptotically unbiased. The variance of the pe­
riodogram estimate, however, does not decrease to zero as the length of the segment 
increases; consequently, the periodogram is not a good estimate. However, by dividing 
the available signal sequence into shorter segments and averaging the associated peri­
odograms, we can obtain a well-behaved estimate. An alternative approach is to first 
estimate the autocorrelation function. This can be done either directly or with the DFT. 
If a window is then applied to the autocorrelation estimates followed by the DFT, the 
result, referred to as the smoothed periodogram, is a good spectrum estimate. 
Problems 
Basic Problems with Answers 
10.1. A  real continuous-time signal xc(t) is bandlimited to frequencies below 5 kHz; i.e., 
Xc(jo.) = 0 for 10.1 :::: 27r(5000). The signal xc(t) is sampled with a sampling rate of 
10,000 samples per second (10 kHz) to produce a sequence x[nJ = xc<nT) with T = 10-4. 
Let X[kJ be the 1OO0-point DFT of x[n]. 
(a) To what continuous-time frequency does the index k = 150 in X[kJ correspond? 
(b) To what continuous-time frequency does the index k = 800 in X[kJ correspond? 
10.2. A continuous-time signal xc(t) isbandlimited t05kHz;i.e., Xdjo.) = ofor 10.1 :::: 27r(5000). 
xdt) is sampled with period T, producing the sequence x[n] 
xc(nT). To examine the 
spectral properties of the signal, we compute the N -point DFT of a segment of N samples 
of x[n] using a computer program that requires N = 2v, where v is an integer. 
Determine the minimum value for N and the range of sampling rates 
1 
Fmin < T < Fmax 
such that aliasing is avoided, and the effective spacing between DFT values is less than 
5 Hz; i.e., the equivalent continuous-time frequencies at which the Fourier transform is 
evaluated are separated by less than 5 Hz. 

865 
Chapter 10 
Problems 
10.3.  A continuous-time signal xc(t) = cos(not) is sampled with period T to produce the se­
quencex[n] = xcCnT). An N-pointrectangular window is applied toxIn] forO, 1, ... , N -1, 
and X[k], for k 
0,1, ... , N - 1, is the N-point DFr of the resulting sequence. 
(a)  Assuming that nO, N, and ko are fixed, how should T be chosen so that X[kO] and 
X[N 
ko] are nonzero, and X[kJ °for all other values of k? 
(b)  Is your answer unique? If not, give another value of T that satisfies the conditions of 
part (a). 
10.4.  Let xc(t) be a real-valued, bandlimited signal whose Fourier transform Xcun) is zero for 
Inl ::: 2Jr(5000). The sequence x[n] is obtained by sampling xcCt) at 10 kHz. Assume that 
the sequence x[nJ is zero for n < °and n > 999. 
Let X[k] denote the l000-point DFr of x[n]. It is known that X[900] = 1 and 
X[420] 
5. Determine xc(jn) for as many values of n as you can in the region Inl < 
2Jr(5000). 
10.5.  Consider estimating the spectrum of a discrete-time signal x[n] using the DFr with a Ham­
ming window applied to x[n]. A conservative rule of thumb for the frequency resolution 
of windowed DFf analysis is that the frequency resolution is equal to the width of the 
main lobe of W(e jW). You wish to be able to resolve sinusoidal signals that are separated 
by as little as Jr/1 00 in w. In addition, your window length L is constrained to be a power 
of 2. What is the minimum length L 
2v that will meet your resolution requirement? 
10.6.  The following are three different signals Xi [n] that are the sum of two sinusoids: 
xl[n] = cos (Jl'n/4) + cos (17Jrn/64) , 
x2[n] = cos (Jrn/4) + 0.8 cos (21Jrn/64) , 
x3[n] = cos (Jrn/4) + 0.001 cos (21Jrn/64). 
We wish to estimate the spectrum of each of these signals using a 64-point DFr with a 
64-point rectangular window wInJ. Indicate which of the signals' 64-point DFfs you would 
expect to have two distinct spectral peaks after windowing. 
10.7.  Let x[n] be a 5000-point sequence obtained by sampling a continuous-time signal xc(t) at 
T = 50 {lS. Suppose X[k] is the 8192-point DFr of x[nJ. What is the equivalent frequency 
spacing in continuous time of adjacent DFf samples? 
10.8.  Assume that x[nI is a 1000-point sequence obtained by sampling a continuous-time signal 
xc(t) at 8 kHz and that xc(jn) is sufficiently bandlimited to avoid aliasing. What is the 
minimum DFf length N such that adjacent samples of X[k] correspond to a frequency 
spacing of 5 Hz or less in the original continuous-time signal? 
10.9.  Xr[k] denotes the time-dependent Fourier transform (TDFf) defined in Eq. (lOAD). For 
this problem, consider the TDFr when both the DFf length N 
36 and the sampling 
interval R 
36. The window w[n] is a rectangular window of length L = 36. Compute the 
TDFf Xr[k] for -00 < r < 00 and 0:::: k :::: N 
1 for the signal 
I 
cos(Jl'n/6), 0:::: n :::: 35, 
x[nl = 
cos(Jrn/2), 
36:::: n 
71, 
0, 
otherwise. 
10.10. Figure PlO.I0 shows the spectrogram of a chirp signal of the form 
x[n] 
sin ( won + ~An2) . 
Note that the spectrogram is a representation of the magnitude of X[n, kl, as defined in 
Eq. (10.34), where the dark regions indicate large values of IX[n, kll. Based on the figure, 
estimate wo and A. 

866 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
0.9 
IX[n,kJI 
0.8 I­
0.7 I­
0.6 I- 
­
0.5 l- 
.~~ 
t:: a 
0.4  
­
:::~ 
O.~ [ 
I  
I 
I 
I 
I 
I 
I 
I 
I 
0 
2000 
4000 
6000 
8000 
10000 12000 14000 16000 18000 
n (samples) 
Figure P10.10 
10.11.  A continuous-time signal is sampled at a sampling rate of 10 kHz, and the Off of 1024 
samples is computed. Oetermine the continuous-time frequency spacing between spectral 
samples. Justify your answer. 
10.12.  Let x[n] be a signal with a single sinusoidal component. The signal x[n] is windowed with 
an L-point Hamming window wen] to obtain vI[n] before computing VI (eiw). The signal 
x[n] is also windowed with an L-point rectangular window to obtain v2[n], which is used 
to compute V2(e jW). Will the peaks in 1V2(eFo )Iand IVl (eil")1 have the same height? Ifso, 
justify your answer. If not, which should have a larger peak? 
10.13.  It is desired to estimate the spectrum of x[n] by applying a 512-point Kaiser window to 
the signal before computing X Cei("). 
(a)  The requirements for the frequency resolution of the system specify that the largest 
allowable main lobe for the Kaiser window is n /100. What is the best side-lobe at­
tenuation expected under these constraints? 
(b)  Suppose that you know that x[n] contains two sinusoidal components at least n/50 
apart, and that the amplitude of the stronger component is 1. Based on your answer 
to part (a), give a threshold on the smallest value of the weaker component you would 
expect to see over the side lobe of the stronger sinusoid. 
10.14.  A speech signal is sampled with a sampling rate of 16,000 samples/s (16 kHz). A window 
of 20-ms duration is used in time-dependent Fourier analysis of the signal, as described 
in Section 10.3, with the window being advanced by 40 samples between computations of 
the Off. Assume that the length of each Off is N = 2v. 
(a)  How many samples are there in each segment of speech selected by the window? 
(b)  What is the "frame rate" of the time-dependent Fourier analysis; i.e., how many Off 
computations are done per second of input signal? 

Chapter 10 
Problems  
867 
(c)  What is the minimum size N of the DFT such that the original input signal can be 
reconstructed from the time-dependent Fourier transform? 
(d)  What is the spacing (in Hz) between the DFT samples for the minimum N from 
part (c)? 
10.15.  A real-valued continuous-time segment of a signal xc(t) is sampled at a rate of 20,000 
samples/s, yielding a lOoo-point finite-length discrete-time sequence x[n] that is nonzero 
in the interval 0 :::; n :::; 999. It is known that xc(t) is also bandlimited such that XcUQ) 
0 
for IQI 2: 2n(1O,OOO); i.e., assume that the sampling operation does not introduce any 
distortion due to aliasing. 
X[k] denotes the 1000-point DFT of x[n]. X[800] is known to have the value 
X[800] = 1 + j. 
(a)  From the information given, can you determine X[k] at any other values of k? If so, 
state which value(s) of k and what the corresponding value of X[k] is. If not, explain 
why not. 
(b)  From the information given, state the value(s) of Q for which Xc(jQ) is known and 
the corresponding value(s) of Xc(jQ). 
10.16.  Let x[n] be a discrete-time signal whose spectrum you wish to estimate using a windowed 
DFT. You are required to obtain a frequency resolution of at least nj25 and are also 
required to use a window length N = 256. A safe estimate of the frequency resolution of 
a spectral estimate is the main-lobe width of the window used. Which of the windows in 
Table 7.2 will satisfy the criteria given for frequency resolution? 
10.17.  Letx[n] be a discrete-time signal obtaincd by sampling a continuous-time signal xc(t) with 
some sampling period T so that x[n] = xc(nT). Assume xc(t) is bandlimited to 100 Hz, i.e, 
XcUQ) = ofor IQI 2: 2n(100). We wish to estimate the continuous-time spectrum XdjQ) 
by computing a 1024-point DFT of x[n], X[k]. What is the smallest value of T such that 
the equivalent frequency spacing between consecutive DFT samples X[k] corresponds to 
1 Hz or less in continuous-time frequency? 
10.18,  Figure PIO.18 shows the magnitude IV[k]1 of the 128-point DFT V[k] for a signal v[n]. 
The signal v[n] was obtained by multiplying x[n] by a 128-point rectangular window w[n]; 
i.e., v[n] 
x[nlw[n]. Note that Figure PlO.18 shows IV [k]I only for the interval 0 :s k :::; 
64. Which of the following signals could be x[nl? That is, which are consistent with the 
information shown in the figure? 
xt[n] = cos(nnj4) + cos(O.26nn), 
x2[n] = cos(nnj4) + (lj3)sin(nnj8), 
x3[n] = cos(nnj4) + (lj3) cos(nnj8) , 
x4[n] 
cos(nnj8) + (lj3)cos(nnjI6), 
xs[n] 
(lj3) cos(ltnj4) + cos(nnj8), 
x6[n] = cos(nnj4) + (lj3)cos(ltnj8 + ltj3). 

868 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
70 
60 
50 
40 
~ 
;::. 
-
30 
20 
10 
0 
0 
10 
20 
30 
40 
50 
60 
70 
DFrindexk  
Figure P10.18 
10.19. A signal x [n] is analyzed using the time-dependent Fourier transform Xrlk], as defined in 
Eq. (10.40). Initially, the analysis is performed with an N 
128 DFT using an L = 128­
point Hamming window wen]. The time-domain sampling of adjacent blocks is R 
128; 
i.e., the windowed segments are offset by 128 samples in time. The frequency resolution 
obtained with this analysis is not sufficient, and it is desired to improve the resolution. 
Several methods of modifying the analysis are suggested to accomplish this goal. Which 
of the following methods will improve the frequency resolution of the time-dependent 
Fourier transform Xr[k]? 
METHOD 1: 
Increase N to 256 while maintaining Land R at the same values. 
METHOD 2: 
Increase both Nand L to 256, while maintaining R the same. 
METHOD 3: 
Decrease R to 64 while maintaining the same Nand L. 
METHOD 4: 
Decrease L to 64 while maintaining the same Nand R. 
METHOD 5: 
Maintain N, Rand L the same, but change wen] to be a rectangular 
window. 
10.20. Assume that you wish to estimate the spectrum ofx[n] by applying a Kaiser window to the 
signal before computing the DTFT. You require that the side lobe of the window be 30 dB 
below the main lobe and that the frequency resolution be ;rr/40. The width of the main 
lobe of the window is a safe estimate of the frequency resolution. Estimate the minimum 
window length L that will meet these requirements. 
Basic Problems 
10.2L  Let x [n] = cos(2;rrn/ 5) and v[n] be the sequence obtained by applying a 32-point rectan­
gular window tox[n] before computing V(eiw). Sketch IV(eiw)1 for -;rr ::: w ::: ;rr, labeling 
the frequencies of all peaks and the first nulls on either side of the peak. In addition, label 
the amplitudes of the peaks and the strongest side lobe of each peak. 
10.22.  In this problem we are interested in estimating the spectra of three very long real-valued 
data sequences Xl en], x2[n]. and x3[n], each consisting of the sum oftwo sinusoidal compo­
nents. However, we only have a 256-point segment of each sequence available for analysis. 

869 
Chapter 10 
Problems 
Let xdn], x2[n], and x3[n] denote the 256-point segments of xl[n], x2[n], and x3[nJ, re­
spectively, We have some information about the nature of the spectra of the infinitely 
long sequences, as indicated in Eqs, (PlO.22-l) through (PlO,22-3). Two different spec­
tral analysis procedures are being considered for use, one using a 256-point rectangular 
window and the other a 256-point Hamming window. These procedures are described 
below. In the descriptions, the signal RN [n] denotes the N -point rectangular window and 
tlN[n] denotes the N-point Hamming window. The operator DFT2048{'} indicates taking 
the 2048-point DFf ofits argument after zero-padding the end of the input sequence. This 
will give a good interpolation of the DTFT from the frequency samples of the DFf. 
· 
17JT 
JT 
Xl (e1W ) "'" o(w+ -) +o(w+-)
64 
4 
JT 
H«(v 
'4) + o(w  
(PlO.22-1) 
· 
llJT 
X2(eJW ) "'" O.0170(w+ 
(PIO.22-2) 
· 
257JT 
JT
Jw
X3(e 
) "'" O.01O«(v + 1024) + o(w + '4) 
JT 
257JT 
H(w '4) + O.01o(w -
1024) 
(PlO.22-3) 
Based on Eqs. (PlO.22-1) through (PlO.22-3), indicate which of the spectral analysis proce­
dures described below would allow you to conclude responsibly whether the anticipated 
frequency components were present. A good justification at a minimum will include a 
quantitative consideration of both resolution and side-lobe behavior of the estimators. 
Note that it is possible that both or neither of the algorithms will work for any given 
data sequence. Table 7.2 may be useful in deciding which algorithm(s) to use with which 
sequence. 
Spectral Analysis Algorithms  
Algorithml: Use the entire data segment with a rectangular window.  
v[n]  
R256[n]x[n] 
= iDFf2048{V[nU i· 
Algorithm 2: Use the entire data segment with a Hamming window. 
v[n]  
tl256[n]x[n] 
IV(eiW)1  
2rrk 
IDFf2048(v[n]) I· 
(j)= 2048 
10.23.  Sketch the spectrogram obtained by using a 256-point rectangular window and 256-point 
DFfs with no overlap (R = 256) on the signal 
x[n1 cos [:n + 1000 sin (~~~O)] 
for the interval 0 
n "S 16,000. 
10.24.  (a) Consider the system of Figure PIO.24-1 with input x (t) 
ei (3rr/8)10
4
( , sampling period 
T = 10-4 , and 
I. 0 < n < N 
1, 
w[nl = { 0, 
other~se. 
What is the smallest nonzero value of N such that Xw[k] is nonzero at exactly one 
value of k? 

870  
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
(b)  Suppose now that N = 32, the input signal is x(t) 
eiQor , and the sampling period 
T is chosen such that no aliasing occurs during the sampling process. Figures PlO.24-2 
and PlO.24-3 show the magnitude of the sequence Xw[k] for k = 0, ... ,31 for the 
following two different choices of w[n]: 
1, 0 S n S 31. 
wl[n] = { 0, 
otherwise, 
1,  Os n S 7, 
w2[n] = { 0, 
otherwise. 
Indicate which figure corresponds to which choice of w[n]. State your reasoning 
clearly. 
x[t] 
xw[k] 
T 
w[n] 
Figure P10.24-1 
25 
20 
15 
10 
5 
ot I I I I I III II Itt t t t ! ! 
5 
10 
15 
k 
Figure P10.24-2 
! ! ! 
20 
! ! ! ! ! 
25 
! ! ! ! t 
30 
35 
8 
6 
4 
2 
JIllIIIIIIIIIIttIIttI!!tIlttIIt 
5 
10 
15 
20 
25 
30 
k 
Figure P10.24-3 
35 

871 
Chapter 10 
Problems 
(c)  For the input signal and system parameters of part (b), we would like to estimate the 
value of Qo from Figure PI0.24-3 when the sampling period is T = 10-4 . Assuming 
that the sequence 
I, 0::: n ::: 31, 
w[n] = { 0, 
otherwise, 
and that the sampling period is sufficient to ensure that no aliasing occurs during 
sampling, estimate the value of Qo. Is your estimate exact? If it is not, what is the 
maximum possible error of your frequency estimate? 
(d)  Suppose you were provided with the exact values of the 32-point DFT Xw[k] for the 
window choices wl[n] and w2[n]. Briefly describe a procedure to obtain a precise 
estimate of Qo. 
Advanced Problems 
10.25. In Figure PlO.25, a filter bank is shown for which 
horn] = 38[n + 1] + 28[n] + 8[n - 1], 
and 
.~ 
hq [n] = eJ 
M horn], 
for q = 1, ... , N -1. 
The filter bank consists of N filters, modulated by a fraction 1/M of the total frequency 
band. Assume M and N are both greater than the length of horn]. 
vQ[n] 
x[n] 
VN_l[n] 
Figure P10.25 
Filter bank 
(a)  Express Yq [n] in terms of the time-dependent Fourier transform X[n,).) of x[n], and 
sketch and label explicitly the values for the associated window in the time-dependent 
Fourier transform. 
For parts (b) and (c), assume thatM = N. Since vq[n] depends on the two integer variables 
q and n, we alternatively write it as the two-dimensional sequence v[q, n]. 
(b)  For R = 2, describe a procedure to recover x[n] for all values of n if v[q, n] is available 
for all integer values of q and n. 
(c) Will your procedure in (b) work if R = 5? Clearly explain. 

872  
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
10.26.  The system in Figure PlO.26-1 uses a modulated filter bank for spectral analysis. (For 
further illustration, Figure PlO.26-2 shows how the frequency responses Hk(e jW ) relate.) 
The impulse response of the prototype filter horn] is sketched in Figure PlO.26-3. 
In] 
voln]
hoLnJ 
• 
I 
I ",[nl • 
2'TTk 
hkln] = eiwknholn],  
where k = 0,1, ... , N-1
Wk= N 
holn] = lowpass prototype filter 
Hk(z) = Ho(e-/27TklNz) 
Figure P10.26-1 
m
fl:~:") 0 fl,(",") = fl~';("-~» 
Wk 
W 
Figure P10.26-2
° 
0.9 
0:5n:5M-1 
otherwise 
n 
Figure P10.26-3 
An alternative system for spectral analysis is shown in Figure P10.26-4. Determine w[n] 
so that G[k] = Vk[O], for k = 0, 1, ... , N - 1. 
x 
hkln] 
{ 
0.9n 
holn] = ° 
° 
M-l 

Chapter 10 
Problems  
873 
~ 
G[k] = L g[n]e-j(27TnldN)
L g[n]e-j (27TnklNl 1-----;.-
no~.~ 
x[n] 
n"'~ 
w[n]  
Figure P10.26-4 
10.27.  We are interested in obtaining 256 equally spaced samples of the z-transform of xw[n]. 
xw[n] is a windowed version of an arbitrary sequence x[n] where xw[n] 
x[n]w[n] and 
w[n] = 1,0::: n ::: 255 and w[n] = °otherwise. The z-transform of xw[n] is defined as 
255 
Xw(z) = L x[n]z-n. 
Il=O 
The samples Xw[k] that we would like to compute are 
Xw(z)1 
j 2". k 
k = 0.1, ... ,255. 
z=O~ge ~ 
We would like to process the signal x[n] with a modulated filter bank, as indicated in 
Figure PI0.27. 
Each filter in the filter bank has an impulse response that is related to the prototype 
causallowpass filter horn] as follows: 
k = 1, 2, ... ,255. 
Each output of the filter bank is sampled once, at time n = Nk, to obtain Xw[k], i.e., 
Determine holn], aJk and Nk so that 
k = 0, 1, ... ,255. 
h [ ] 
on 
• 
x [n] 
hiln] 
vi [n] 
~ 
h255[n] 
v255[n] 
Figure P10.27 

874  
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
10.28. (a) In Figure PlO.28-1, we show a system for spectral analysis of a signal xc(t), where 
N-l  
GdnJ L gl[n]e-j  
1=0  
N = 512, 
and LR = 256. 
For the most general choice of the multiplier coefficient at, determine the choice for 
Land R which will result in the smallest number of multiplies per second. 
xc(t) 
ao 
go[n] 
Go[n] 
Xo[n] 
GJ[n] 
XJ[n]
a1 
81[n] 
N·pt 
Off 
X 2[n]
Gz[n]
a2 
8z[n] 
GN_1[n] 
XN_1[n]
aN-J 
8N-I[n] 
Figure P10.28-1 
(b)  In Figure PlO.28-2, we show another system for spectral analysis of a signal xc(t), 
where 
h[nJ = { (0.93l 0 ~ n ~ .z55 
o 
otherwise' 
hk[n]=h[n]e- jwkn , k=0,1,···,N-1, and N=512. 
Listed below are two possible choices for M, four possible choices for Wb and six 
possible choices for the coefficients al. From this set identify all combinations for 
which Yk[n] = Xdn], i.e., for which both systems will provide the same spectral 
analysis. There may be more than one. 
M: 
(a) 256 
(b) 512 
Wk: 
(a)' 2rrk 
256 
(b) 2rrk 
512 
(c) 
(d) 
ar 
(a) 
(0.93)' 
1=0,1, .. ·,255, 
zero otherwise 
(b) 
(0.93)-1 
1=0,1, .. ·,511 
(c) 
(0.93)' 
[=0, 1, .. ·,511 
(d) 
(0.93)-1 
1=0, 1, .. ·,255, 
zero otherwise 
(e) 
(0.93)1 
[=256,257, .. ·,511, 
zero otherwise 
(f) 
(0.93)-1 
1=256,257, .. ·,511, 
zero otherwise 

Chapter 10 
Problems  
875 
xc(t) 
x[nJ  
Yo[n]
H 
--~_"T'""_;--r-........_h_o[_nJ_--' 
t M  
T = 1()-4 
Y2[n] 
Figure P10.28-2 
10.29.  The system shown in Figure PlO.29 is proposed as a spectrum analyzer. The basic operation 
is as follows: The spectrum of the sampled input is frequency-shifted; the lowpass filter 
selects the lowpass band offrequencies; the downsampler "spreads" the selected frequency 
band back over the entire range -n < W < n; and the OFT samples that frequency band 
uniformly at N frequencies. 
Assume that the input is bandlimited so that Xc(jQ) 
0 for IQI ::: niT. The LTI 
system with frequency response H(ejW) is an ideal lowpass filter with gain of one and 
cutoff frequency n1M. Furthermore, assume that 0 < WI < n and the data window w[n] 
is a rectangular window of length N. 
(a)  Plot the OTFTs, X(ej(J), Y(ejW), R(ejW ), and Rd(ejW) for the given Xc(jQ) and for 
WI 
nl2 and M = 4. Give the relationship between the input and output Fourier 
transforms for each stage of the process; e.g., in the fourth plot, you would indicate 
R(ejW) 
H(ejW)Y(ejW). 
(b)  Using your result in part (a), generalize to determine the band of continuous-time 
frequencies in XdjQ) that falls within the passband of the lowpass discrete-time 
filter. Your answer will depend on M, Wi and T. For the specific case of Wi 
n12 and 
M 
4, indicate this band offrequencies on the plot of Xc(jQ) given for part (a). 
(t:) (i) What continuous-timefrequencies in XcUQ) are associated with the OfT values 
V[k] for 0 ~ k ~ N12? 
(ii) What continuous-time frequencies in XcUQ) do the values for N12 < k ~ N -1 
correspond to? In each case, give a formula for the frequencies Qk. 
e-jw1n 
w[n] 
xc(t) 
CID 
(T) 
=xc(nT) 
H(eiw) 
r[nJ 
t M 
=r[Mn] 
N-point 
DFT 
VIk] 
Figure P10.29 

876 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
10..30.  Consider a real time-limited continuous-time signal xcCt) whose duration is 100 ms. As­
sume that this signal has a bandlimited Fourier transform such that Xc(jQ) 
0 for 
IQI 
2Jr(1O, 000) rad/s; i.e., assume that aliasing is negligible. We want to compute samples 
of Xc(jQ) with 5-Hz spacing over the interval 0 S Q :::: 2Jr(10,000). This can be done with 
a 4000-point DFT. Specifically, we want to obtain a 4ooo-point sequence x[n] for which 
the 4000-point DFT is related to Xc(jQ) by 
X[k] = aXdj2Jr ·5· k), 
k = 0, 1. ... ,1999, 
where a is a known scale factor. 'Ibree methods are proposed to obtain a 4000-point 
sequence whose DFT gives the desired samples of XdjQ). 
METHOD 1: 
xc(t) is sampled with a sampling period T = 25 JLS; i.e., we compute 
XI [k], the DFT of the sequence 
[ j _ {xc(nT), 
n 
0.1. ... ,3999, 
Xl n -
0, 
otherwise. 
Since xc(t) is time limited to 100 ms, Xl [nJ is.a finite-length sequence oflength 4000  
(100 ms/25 JLs).  
METHOD 2: 
xc(t) is sampled with a sampling period of T = 50 JLS. Since xc(t) is  
time limited to 100 ms, the resulting sequence will have only 2000 (100 ms/50 JLs)  
nonzero samples; i.e.,  
X [nJ = {xc(nT), 
n = 0, ~, ... , 1999. 
2 
0, 
otherWise. 
In other words, the sequence is padded with zero-samples to create a 4000-point  
sequence for which the 40oo-point DFT X2[kj is computed.  
METHOD 3: 
Xc (t) is sampled with a sampling period of T = 50 JLS, as in Method 2.  
The resulting 2000-point sequence is used to form the sequence x3[n] as follows:  
I 
xc(nT), 
0:::: n :::: 1999, 
x3[n] = 
xcCCn 
2000)T), 
2000:::: n :::: 3999, 
0, 
otherwise. 
]be 4oo0-point DFT X3[k] ofthis sequence is computed. 
For each of the three methods, determine how each 4ooo-point DFT is related to Xc(jQ). 
Indicate this relationship in a sketch for a "typical" Fourier transform XcCjQ). State ex­
plicitly which method(s) provide the desired samples of Xc(jQ). 
10.31.  A continuous-time finite-duration signal xc(t) is sampled at a rate of 20,000 samples/s, 
yielding a 1000-point finite-length sequencex[nJ that is nonzero in the interval °S n :::: 999. 
Assume for this problem that the continuous-time signal is also bandlimitcd such that 
Xc(jQ) °for IQI :::: 2Jr(1O,000): i.e., assume that negligible aliasing distortion occurs 
in sampling. Assume also that a device or program is available for computing looO-point 
DFTs and inverse DFTs. 
(0)  If X[kj denotes the 1000-point DFT of the sequence x[nj, how is X[k] related to 
Xc(j Q)? What is the effective continuous-time frequency spacing between DFT sam­
ples? 
The following procedure is proposed for obtaining an expanded view of the Fourier trans­
form Xc(jQ) in the intervallQI 
2Jr(50oo), starting with the looo-point DFT X[k]. 
Step 1. Form the new looo-point DFT 
I 
X[k], 
0:::: k :::: 250, 
W[k] = 
0, 
251:::: k S 749, 
X[kj, 
750 S k:::: 999. 

Chapter 10 
Problems  
877 
Step2. Compute the inverse 1000-point DFT of W[k], obtaining w[nlforn 
0,1, .... 999. 
Step 3. Decimate the sequence w[n] by a factor of 2 and augment the result with 500 
consecutive zero samples, obtaining the sequence 
w[2n], 
0:::: n :::: 499,
y[n] 
{ 0, 
500.:::: n :::: 999. 
Step 4. Compute the lOoo-point DFT of y[n], obtaining Y[k]. 
(b)  
The designer of this procedure asserts that 
Y[k]  
aXcU2rr· 10· k), 
k 
0,1, ... ,500, 
where a is a constant of proportionality. Is this assertion correct? If not, explain 
why not. 
10.32.  An analog signal consisting of a sum of sinusoids was sampled with a sampling rate of 
is = 10000 samples/s to obtain x[n] 
xcCnT). Four spectrograms showing the time­
dependent Fourier transform IX[n, )..)1 were computed using either a rectangular or a 
Hamming window. They are plotted in Figure Pl0.32. (A log amplitude scale is used. and 
only the top 35 dB is shown.) 
a 
(b) 
-5
0.8 
o:k- 
j
• 
-10 
0.6 ............. 
0.6 
-15 
~  
i 
:<  
~ 
i 
-20
04
0.4 
0:"  
-25 
0.2 
1 
··1 
-30 
0 
1000 
2000 
3000 
n (samples)  
n (samples) 
0.8  
0.8 
0.6  
0.6 
~  
~ 
.<  
:< 
0.4  
0.4 
0.2  
0.2  
0  
1000 
2000 
3000 
0 
1000 
2000 
3000 
n (samples) 
n (samples) 
Figure P10.32 
1000 
2000 
3000 
.. 

878  
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
(a)  Which spectrograms were computed with a rectangular window? 
(a) 
(b) 
(c) 
(d) 
(b) Which pair (or pairs) of spectrograms have approximately the same frequency reso­
lution? 
(a&b) 
(b&d) 
(c&d) 
(a&d) 
(b&c) 
(c)  Which spectrogram has the shortest time window? (a) 
(b) 
(c) 
(d) 
(d) To the nearest 100 samples, estimate the window length L (in samples) ofthe window 
in spectrogram (b). 
(e)  Use the spectrographic data in Figure PlO.32 to assist you in writing an equation (or 
equations) for an analog sum of sinusoids xc(t), which when sampled at a sampling 
rate of is 
10000, would produce the above spectrograms. Be as complete as you 
can in your description of the signal. Indicate any parameters that cannot be obtained 
from the spectrogram. 
10.33.  The periodogram I (w) of a discrete-time random signal x[n] was defined in Eq. (10.67) as 
1  
. 
2 
I (w) = LU IVCeJw)I , 
where V(ejW ) is the DTFf of the finite-length sequence v[n] 
w[n]x[n], with wIn] a 
finite-length window sequence of length L, and U is a normalizing constant. Assume that 
xln] and wIn] are real. 
Show that the periodogram is also equal to 1/LU times the Fourier transform of the 
aperiodic autocorrelation sequence of v[n]; i.e., 
L-l
1 
jwm
ICw) 
LU L 
cvv[m]e-
m=-(L-I) 
, 
where 
L-l 
cvv[m] = L v[n]v[n + m]. 
n=O 
10.34.  Consider a finite-length sequence x[n] such that x[n] = 0 for n < 0 and n :::: L. Let X[k] be 
the N-point DIT of the sequence x[n], where N > L. Define cxx[m] to be the aperiodic 
autocorrelation function of x[n]; i.e., 
00 
cxx[m] 
L x[n]x[n + m]. 
n=-oo 
Define 
1 N-l 
cxx[m] = -
" 
IX[k]12ej (2rr/N)km 
m 
0,1, ... ,N - L
N~ . 
, 
m=O 
(a)  Determine the minimum value of N that can be used for the DIT if we require that 
culm] = cxx[m], 
O~m~L-l. 
(b)  Determine the minimum value of N that can be used for the DIT if we require that 
culm] = culm], 
o ~ m ~ M -1, 
where M < L. 

879 
Chapter 10 
Problems 
10.35. The symmetric Bartlett window, which arises in many aspects of power spectrum estima­
tion, is defined as 
I -lmIIM, 
Iml.::: M 
1, 
WB [m] = { 0,  
(PlO.35-1)
otherwise. 
The Bartlett window is particularly attractive for obtaining estimates of the power spec­
trum by windowing an estimated autocorrelation function, as discussed in Section 10.6. 
This is because its Fourier transform is nonnegative, which guarantees that the smoothed 
spectrum estimate will be nonnegative at all frequencies. 
(a)  Show that the Bartlett window as defined in Eq. (PlO.35-l) is (11M) times the aperi­
odic autocorrelation of the sequence (u[n] - urn 
MD. 
(b) From the result of part (a), show that the Fourier transform of the Bartlett window is 
WB(e jW ) = ~ [Si~(WMI2)J2, 
(PI0.35-2)
M 
sm«(/)12) 
which is clearly nonnegative. 
(c)  Describe a procedure for generating other finite-length window sequences that have 
nonnegative Fourier transforms. 
10.36. Consider a signal 
x[n] = [sin (:rr2n)y u[n] 
whose time-dependent discrete Fourier transform is computed using the analysis window 
1,  0.::: n 
13,
[
W n] = { 0, 
otherwise. 
Let X[n, k] 
X[n, 2:rrkj7) for 0.::: k .::: 6, where X[n,'\) is defined as in Section 10.3. 
(a) Determine X[O, kJ for 0 .::: k .::: 6. 
(b) Evaluate L~=O X[n, kJ for 0.::: n < 00. 
Extension Problems 
10.37. In Section 10.6,  we showed that a smoothed estimate of the power spectrum can be 
obtained by windowing an estimate of the autocorrelation sequence. It was stated (see 
Eq. (10.109» that the variance of the smoothed spectrum estimate is 
var[S(w)] 
F~x(w), 
where F, the variance ratio or variance reduction factor, is 
1 
M-l 
F = Q L 
(wclm])2 
m=-(M-l) 
As discussed in Section 10.6, Q is the length of the sequence x[n] and (2M 
1) is the length 
of the symmetric window Wc[m] that is applied to the autocorrelation estimate. Thus, if Q 
is fixed, the variance of the smoothed spectrum estimate can be reduced by adjusting the 
shape and duration of the window applied to the correlation function. 
In this problem we will show that F decreases as the window length decreases, but 
we also know from the previous discussion of windows in Chapter 7 that the width of 
the main lobe of Wc(ejW) increases with decreasing window length, so that the ability to 

880  
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
resolve two adjacent frequency components is reduced as the window width decreases. 
Thus, there is a trade-off between variance reduction and resolution. We will study this 
trade-off for the following commonly used windows: 
Rectangular 
I,  Iml:;;M 
1,
wR[m] = { 0, 
otherwise. 
Bartlett (triangular) 
WB[m] = {1.- lmI/M, 
Iml ::sM-l, 
0, 
otherwise. 
Hanning/Hamming 
a + f:3 cos[nm/(M 
1)]. 
Iml:;; M  
I,
WH[m] 
{ 0,  
otherwise. 
(a = f:3 
0.5 for the Hanning window, and a = 0.54 and f:3 = 0.46 for the Hamming 
window.) 
(a)  Find the Fourier transform of each of the foregoing windows; i.e., compute WR(ejW), 
WB (ejW ), and WH (ejW). Sketch each of these Fourier transforms as functions of w. 
(b)  For each of the windows, show that the entries in the following table are approximately 
true when M » 1: 
Approximate 
Approximate 
Window Name 
Main-lobe Width 
Variance Ratio (F) 
Rectangular 
2rr / M 
2M/Q 
Bartlett 
4rr / M 
2M/(3Q) 
HanningIHamming 
3rr / M 
2M(a2 + /32/2)/Q 
10.38.  Show that the time-dependent Fourier transform, as defined by Eq. (10.18), has the fol­
lowing properties: 
(a)  Linearity: 
If x[n] 
axdn] + bX2[n], 
then 
X[n,)..) 
aXl[n,)..) + bX2[n, )..). 
(b)  Shifting: If yEn] = x[n - nO], then Y[n,)..) 
X[n - no, )..). 
(c)  Modulation: If yEn] = ejwonx[n], then Y[n,)..) = ejwon X[n,).. 
wo). 
(d)  Conjugate Symmetry: If x[n] is real, then X[n,)..) = X*[n. -)..). 
10.39.  Suppose that Xc (t) is a real, continuous-time stationary random signal with autocorrelation 
function 
ePeC'r) = [(xcCt)xcCt + ,)}  
and power density spectrum  
00 
PdQ) = 1 ePd,)e-jrl'd,. 
-00 
Consider a discrete-time stationary random signal x[n] that is obtained by sampling xc(t) 
with sampling period T; I.e., x[n] 
xdnT). 
(a)  Show that eP[m], the autocorrelation sequence for x[n], is  
cf>[m] 
ePc(mT).  

881 
Chapter 10 
Problems 
(b)  What is the relationship between the power density spectrum Pc (1.1) for the continuous­
time random signal and the power density spectrum P(w) for the discrete-time random 
signal? 
(c)  What condition is necessary such that 
pew) 
.!:.. Pc (:.':.) 
Iwl < rr?
T 
T' 
10.40. In Section 10.5.5, we considered the estimation of the power spectrum of a sinusoid plus 
white noise, In this problem, we will determine the true power spectrum of such a signal. 
Suppose that 
x[n] 
A cos(won + 9) + e[n], 
where 9 is a random variable that is uniformly distributed on the interval from 0 to 2rr 
and ern] is a sequence of zero-mean random variables that are independent of each other 
and also independent of 9. In other words, the cosine component has a randomly selected 
phase, and ern] represents white noise. 
(a)  Show that for the preceding assumptions, the autocorrelation function for x[n] is 
A2 
<pxx[m] = S(x[n]x[m +n]) = 2 cos(WOm ) + a;8[m] , 
where a; = S{(e[n])2}. 
(b)  From theresult ofpart (a), show that over one period in frequency, the power spectrum 
of x[n] is 
Iwl ::: rr. 
10.41. Consider a discrete-time signal x[n] of length N samples that was obtained by sampling a 
stationary, white, zero-mean continuous-time signal. It follows that 
£{x[n]x[m]} 
a18[n - m], 
£(x[nll = 0, 
Suppose that we compute the DFT of the finite-length sequence x[n], thereby obtaining 
X[k] for k = 0, 1, .. " N 
1. 
(a)  Determine the approximate variance of IX[k]12 using Eqs, (10.80) and (10.81). 
(b)  Determine the cross-correlation between values of the DFT; i.e., determine 
£{X[k]X*[r]} as a function of k and r. 
10.42. A bandlimited continuous-time signal has a bandlimited power spectrum that is zero for 
11.11 2: 2rr(104) radls. The signal is sampled at a rate of 20,000 samples/s over a time 
interval of 10 s. The power spectrum of the signal is estimated by the method of averaging 
periodograms as described in Section 10.5.3. 
(a)  What is the length Q (number of samples) of the data record? 
(b)  If a radix-2 FFT program is used to compute the periodograms, what is the minimum 
length N if we wish to obtain estimates of the power spectrum at equally spaced 
frequencies no more than 10 Hz apart? 
(c)  If the segment length L is equal to the FFT length N in part (b), how many segments 
K are available if the segments do not overlap? 
(d)  Suppose that we wish to reduce the variance of the spectrum estimates by a factor of 
10 while maintaining the frequency spacing of part (b). Give two methods of doing 
this. Do these two methods give the same results? If not, explain how they differ. 

882  
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
10.43.  Suppose that an estimate of the power spectrum of a signal is obtained by the method of 
averaging periodograms, as discussed in Section 10.5.3. That is, the spectrum estimate is 
1 K-l 
I(w) = K L Ir(w), 
r=O 
where the K periodograms 1r (w) are computed from L-point segments of the signal using 
Eqs. (10.82) and (10.83). We_define an estimate of the autoeorrelation function as the 
inverse Fourier transform of I (w); i.e., 
1 jJr  -
.
¢>[m) 
-2 
l(w)eJwmdw. 
rr 
-Jr 
(a)  Show that 
-
1 
£{4>[m]} = LUcww[ml4>xx[m), 
where L is the length of the segments, U is a normalizing factor given by Eq. (10.79), 
and cwwlm], given by Eq. (10.75), is the aperiodic autocorrelation function of the 
window that is applied to the signal segments. 
(b)  In the application of periodogram averaging, we normally use an FFT algorithm to 
eompute I(w) at N equally spaced frequencies; i.e., 
I[k]  
I (2rrkIN), 
k = 0, 1, ... , N 
1, 
where N 2: L. Suppose that we compute an estimate of the autocorrelation function 
by computing the inverse DFT of I[k], as in 
N-l 
¢>p[mj = ~ L I [k]ej (2Jf/N)km , 
m = 0, 1, ... , N 
1. 
k=O 
Obtain an expression for £{¢>p[mH. 
(c)  How should N be chosen so that 
£{¢>p[mH = £{¢)[m)}, 
m = 0, 1, ... , L -1? 
10.44.  Consider the computation of the autocorrelation estimate 
Q-Iml-l 
_ 
1" 
4>xx [m] = Q 
L....-
x[n]x[n + Iml], 
(PlO,44-1) 
n=O 
where x[n] is a real sequence. Since ~xx[-m] = $xx[m), it is necessary only to evaluate 
(PI0.44-1) for 0 ~ m ~ M 
1 to obtain $xx[m] for -(M - 1) ~ m :5 M 
1, as is 
required to estimate the power density spectrum using Eq. (10.102). 
(a)  When Q » M, it may not be feasible to compute $xx [ml using a single FFT computa­
tion. In such cases, it is convenient to express $xx[m] as a sum of correlation estimates 
based on shorter sequences. Show that if Q = K M, 
A 
1 K-l 
4>xx[m] 
- L q[m], 
Q i=O 
where 
M-l 
q[m] = L x[n + iM]x[n + iM + m], 
n=O 
for 0 :5 m ~ M 
1. 

883 
Chapter 10 
Problems 
(b)  Show that the correlations cj[m] can be obtained by computing the N-point circular 
correlations 
N-l 
ci[m] L xi [n]Yi[«n + m»N], 
n=O 
where the sequences 
X[n+iM]. O:::n:::M-l, 
{ 0, 
M ::: n::: N -1, 
and 
y;[n] = x[n + iM], 
O:::n:::N-1. 
(PlO.44-2) 
What is the minimum value of N (in terms of M) such that e;[m] 
c;[m] for 0 ::: m ::: 
M-1? 
(c)  State a procedure for computing <pxx[m] for 0::: m ::: M 
1 that involves the com­
putation of 2K N-point DFfs of real sequences and one N-point inverse Off. How 
many complex multiplications are required to compute <Pxx [m] for 0 ::: m ::: M - 1 if 
a radix-2 FFf is used? 
(d)  What modifications to the procedure developed in part (c) would be necessary to 
compute the cross-correlation estimate 
Q-Iml-l 
~ 
1 
'\' 
¢xy[m] = Q 
L..., 
x[n]y[n + m], 
-(M 
1) 
m::: M - I, 
n=O 
where x [n] and y[n] are real sequences known for 0 ::: n ::: Q 
1 ? 
(e)  Rader (1970) showed that, for computing the autocorrelation estimate ¢xx[m] for 
0::: m ::: M 
I, significant savings of computation can be achieved if N = 2M. Show 
that the N -point Off ofa segment Yi [n] as defined in Eq. (PI0.44-2) can be expressed 
as 
k 
0.1, .. , N 
1. 
State a procedure for computing <Pxx [m] for 0 ::: m ::: M 
1 that involves the compu­
tation of K N -point OFfs and one N -point inverse Off. Determine the total number 
of complex multiplications in this case if a radix-2 FFf is used. 
10.45.  In Section 10.3 we defined the time-dependent Fourier transform ofthe signal x[m] so that, 
for fixed n, it is equivalent to the regular OTFf of the sequence x[n +m]w[m], where w[m] 
is a window sequence. It is also useful to define a time-dependent autocorrelation function 
for the sequence x[n] such that, for fixed n, its regular Fourier transform is the magnitude 
squared of the time-dependent Fourier transform. Specifically, the time-dependent auto­
correlation function is defined as 
ern, m] = 21 Ilr IX[n, A)12ejAmdA, 
l( 
-lr 
where X[n, A) is defined by Eq. (10.18). 
(a)  Show that if x[n] is real 
00 
e[n,m]= L x[n+r]w[r]x[m+n+rlw[m+r]; 
T=-OO 
i.e., for fixed n, c[n, m] is the aperiodic autocorrelation of the sequence x[n + r ]w[r], 
-00 < r < 00. 

884  
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
(b)  Show that the time-dependent autocorrelation function is an even function of m for 
n fixed, and use this fact to obtain the equivalent expression 
oc 
ern, m] 
L x[r]x[r 
mJhm[n 
r], 
r=-oo 
where 
hmlr] 
w[-rJw[-(m +r)J. 
(PI0.45-I) 
(c)  What condition must the window w[r] satisfy so that Eq. (PlO.45-I) can be used to 
compute ern, m] for fixed m and -00 < n < 00 by causal operations? 
(d)  Suppose that 
ar, 
r 2: 0, 
w[-r] 
(PIO.45-2)
{ 0, 
r < O. 
Find the impulse response hmlr] for computing the mth autocorrelation lag value, 
and find the corresponding system function Hm (z). From the system function, draw 
the block diagram of a causal system for computing the m th autocorrelation lag value 
ern, m] for -00 < n < 00 for the window of Eq. (PlO.45-2). 
(c)  Repeat part (d) for 
rar,  r 2: O. 
w[-r] 
{ 0,  
r < O. 
10.46.  Time-dependent Fourier analysis is sometimes implemented as a bank of filters, and even 
when FFT methods are used, the filter bank interpretation may provide useful insight. 
This problem examines that interpretation, the basis of which is the fact that when A is 
fixed, the time-dependent Fourier transform X[n, ),), defined by Eq. (10.18), is simply a 
sequence that can be viewed as the result of a combination of filtering and modulation 
operations. 
(a)  Show that X[n, A) is the output of the system ofFigurePlO.46-1 if the impulse response 
of the LTI system is horn] = w[-n]. Show also that if A is fixed, the overall system in 
Figure PlO.46-1 behaves as an LTI system, and determine the impulse response and 
frequency response of the equivalent LTI system. 
X(n.A) 
e-iAn  
e jAn 
Figure P10.46-1 
(b)  Assuming A fixed in Figure PI0.46-1, show that, for typical window sequences and for 
fixed )" the sequence sIn] 
X[n, A) has a lowpass DTFT. Show also that, for typical 
window sequences, the frequency response of the overall system in Figure PlO.46 is 
a bandpass filter centered at w = A. 
(c)  Figure PIO.46-2 shows a bank of N bandpass filter channels, where each channel 
is implemented as in Figure PlO.46-1. The center frequencies of the channels are 
Ak 
2rrk/N, and horn] = w[-nl is the impulse response ofa lowpassfilter. Show that 
the individual outputs n[n] are samples (in the A-dimension) of the time-dependent 
Fourier transform. Show also that the overall output is yIn] = Nw[O]x[n]: i.e., show 
that the system of Figure PI0.46-2 reconstructs the input exactly (within a scale factor) 
from the sampled time-dependent Fourier transform. 

885 
Chapter 10 
Problems 
x[n] 
Figure P10.46-2 
The system of Figure PlO.46-2 converts the single input sequence x[n] into N se­
quences, thereby increasing the total number of samples per second by the factor N. As 
shown in part (b), for typical window sequences, the channel signals h[n] have lowpass 
Fourier transforms. Thus, it should be possible to reduce the sampling rate of these sig­
nals, as shown in Figure PlO.46-3. In particular, if the sampling rate is reduced by a factor 
R = N, the total number of samples per second is the same as for x [n]. In this case, the filter 
bank is said to be critically sampled. (See Crochiere and Rabiner, 1983.) Reconstruction 
of the original signal (rom the decimated channel signals requires interpolation as shown. 
Clearly, it is of interest to determine how well the original input x [n] can be reconstructed 
by the system. 
Figure P10.46-3 

886  
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
(d)  For the system ofFigure P10.46-3, show that the regular DTFf of the output is given 
by the relation 
R-IN-l 
Y(eiw) = ! L L Go(e}(W-Ak»Ho(e}(W-Ak-2rri/R)}X (e}(w-2rri/R», 
£=0 k=O 
where Ak = 2rrklN. This expression clearly shows the aliasing resulting from the 
decimation ofthe channel signals Hnl. From the expression for Y(e jW ), determine a 
relation or set of relations that must be satisfied jointly by Ho(e}W) and Go(ejW ) such 
that the aliasing cancels and y[n] = x[nl 
(e)  Assume that R 
N and the frequency response of the lowpass filter is an ideal 
lowpass filter with frequency response 
H(ejW ) 
Iwl<rrIN,
{t
o 
0, rrlN < Iwl ::::: rr. 
For this frequency response Ho(e}W), determine whether it is possible to find a fre­
quency response of the interpolation filter Go(e}W) such that the condition derived 
in part (d) is satisfied. If so, determine Go(ejW ). 
(f)  Optional: Explore the possibility of exact reconstruction when the frequency response 
ofthe lowpass filter Ho(eJW )(the Fourier transform of w[-n]) is nonideal and nonzero 
in the intervallwl < 21fI N. 
(g)  Show that the output of the system of Figure PlO.46-3 is 
00 
00 
y[n] =  N L x[n - rN] L go[n - £R]ho[lR + rN 
n]. 
r=-oo 
£=-00 
From this expression, determine a relation or set of relations that must be satisfied 
jointly by horn] and go[n] such that y[n] = x[n]. 
(h)  Assume that R = N and the impulse response of the lowpass filter is 
h [n] = {I, -(N - .1) ::::: n ::::: 0, 
o 
0, 
otherwIse. 
For this impulse response horn], determine whether it is possible to find an impulse 
response of the interpolation filter go[n] such that the condition derived in part (g) is 
satisfied. If so, determine go[n]. 
(i)  Optional: Explore the possibility of exact reconstruction when the impulse response 
of the lowpass filter hO[n] 
w [-n] is a tapered window with length greater than N. 
10.47.  Consider a stable LTI system with a real input x[n], a real impulse response h[n], and 
output y[nl. Assume that the input x[nl is white noise with zero mean and variance a;. 
The system function is 
M 
LbkZ-k 
H(z) = _k_=_O___ 
N 
1- LakZ-k 
k=l 
where we assume the aks and bkS are real for this problem. The input and output satisfy 
the following difference equation with constant coefficients: 
N 
M 
y[n] = L akY[n 
kJ + L bkx[n 
k]. 
k=l 
k=O 

887 
Chapter 10 
Problems 
If all the aks are zero, y[nJ is called a moving-average (MA) linear random process. If all 
the bkS are zero, except for bo, then yen] is called an autoregressive (AR) linear random 
process. If both Nand M are nonzero, then yen] is an autoregressive moving-average 
(ARMA) linear random process. 
(a)  Express the autocorrelation of yen] in terms of the impulse response h[nl of the linear 
system. 
(b)  Use the result of part (a) to express the power density spectrum of y[n] in terms of 
the frequency response of the system. 
(c)  Show that the autocorrelation sequence t/Jyy[m] of an MA process is nonzero only in 
the intervallml :s M. 
(d)  Find a general expression for the autocorrelation sequence for an AR process. 
(e)  Show that if bo 
1, the autocorrelation function of an AR process satisfies the 
difference equation 
N 
t/Jyy[O) 
I:>kt/Jyy[k] + a;, 
k=1 
N 
t/Jyy[m] = I:>kt/Jyy[m - k), 
m~1. 
k=l 
(f)  Use the result of part (e) and the symmetry of t/Jyy[m) to show that 
NL Qkt/Jyy[jm 
kll = t/Jyy[m], 
m 
1,2, ... , N. 
k=1 
It can be shown that, given t/Jyy[m) for m 
0,1, ... , N, we can always solve uniquely for 
the values of the aks and a} for the random-process model. These values may be used in 
the result in part (b) to obtain an expression for the power density spectrum of y[n). This 
approach is the basis for a number of parametric spectrum estimation techniques. (For 
further discussion of these methods, see Gardner, 1988; Kay, 1988; and Marple, 1987.) 
10.48. This problem illustrates the basis for an FFT-based procedure for interpolating the samples 
(obtained at a rate satisfying the Nyquist theorem) of a periodic continuous-time signal. 
Let 
1  
4 (l)lkl jkt
xdt) = 16 L 2 
e 
k=-4 
be a periodic signal that is processed by the system in Figure Pl0.48. 
(a)  Sketch the 16-point sequence G[k]. 
(b)  Specify how you would change G[k] into a 32-point sequence Q[k] so that the 32-point 
inverse DFT of Q[k] is a sequence 
(n27i)
q[n] = aXe 
32 
' 
o:s n :s 31, 
for some nonzero constant ct. You need not specify the value of ct. 
G[kJ 
T = ~_1! 
urn] u [n -16]
16  
Figure P10.48 

888 
Chapter 10 
Fourier Analysis of Signals Using the Discrete Fourier Transform 
10.49. In many real applications, practical constraints do not allow long time sequences to be 
processed, However, significant information can be gained from a windowed section of 
the sequence, In this problem, you will look at computing the Fourier transform of an 
infinite-duration signal x[n], given only a block of 256 samples in the range 0 :::E: n :::E: 255, 
You decide to use a 256-point OFf to estimate the transform by defining the signal 
A[ ] _  {x[n], 
O:::E: n :::E: 255, 
x n  -
0, 
otherwise, 
and computing the 256-point OFT of x[n]. 
(a)  Suppose the signal x[n] came from sampling a continuous-time signal xC<t) with sam­
pling frequency Is 
20 kHz; i,e., 
x[n]  
xe(nTs), 
1/ Ts = 20 kHz. 
Assume that xe(t) is bandlimited to 10 kHz. If the OFf of x[n] is written X[k], k = 
0,1, ... ,255, what are the continuous-time frequencies corresponding to the OFT 
indices k = 32 and k = 231? Be sure to express your answers in Hertz. 
(b)  Express the OTFTofx[n] in terms of the OTFf of x[n] and the OTFf of a 256-point 
rectangular window wR[n]. Use the notation X (eFt» and WR(eJW ) to represent the 
OTFfs ofx[n] and wR[n], respectively. 
(c)  Suppose you try an averaging technique to estimate the transform for k = 32: 
Xavg[32j = aX [31] + X[32] + aX [33]. 
Averaging in this manner is equivalent to mUltiplying the signal x[n] by a new window 
Wavg[n] before computing the DFf. Show that Wavg(ejW) must satisfy 
1, 
W 
0, 
Wavg(eJW ) = 
a, W 
±2n/L, 
0, 
W = 2nk/L, 
for k = 2,3, ... , L - 2,
1  
where L = 256. 
(d)  Show that the DTFT of this new window can be written in terms of WR(e jW ) and two 
shifted versions of WR(eJW ). 
(e)  Derive a simple formula for Wavg[n], and sketch the window for a = -0.5 and 
O:::E: n :::E: 255. 
10.50.  It is often of interest to zoom in on a region of a OFf of a signal to examine it in more 
detaiL In this problem, you will explore two algorithms for implementing this process of 
obtaining additional samples of X (eJW ) in a frequency region of interest. 
Suppose X N [kJis the N -point OFf of a finite-length signal x[nJ. Recall that X N[k] 
consists of samples of X (eJW ) every2n/N in w. Given XN[k], we would like to compute N 
samples of X (eJW ) between W = We -
fj,w and W 
We + fj,w with spacing 2fj,w/N, where 
2nke 
we=~ 
and 
fj,w = 
N 
This is equivalent to zooming in on X (eJW ) in the region We -
fj,w < W < We + fj,w, One 
system used to implement the zoom is shown in Figure P10.50-1. Assume that xzlnj is 
zero-padded as necessary before the N-point OFf and h[n] is an ideallowpass filter with 
a cutoff frequency fj,w. 

889 
Chapter 10 
Problems 
Figure P10.50-1 
Figure P10.50-2 
(a)  In terms of ka and the transform length N, what is the largest (possibly noninteger) 
value of M that can be used if aliasing is to be avoided in the downsampler? 
(b)  Consider x[n] with the Fourier transform shown in Figure PlO.50-2. Using the max­
imum value of M from part (a), sketch the Fourier transforms of the intermediate 
signals xe£n] and xdn] when We = rr/2 and ~W rr/6. Demonstrate that the system 
provides the desired frequency samples. 
Another procedure for obtaining the desired samples can be developed by viewing the 
finite-length sequence X N [k] indexed on k as a discrete-time data sequence to be processed 
as shown in Figure PlO.50-3. The impulse response of the first system is 
00 
p[n] = L o[n + rN]. 
r=-oo 
and the filter h[n] has the frequency response 
H(eiw) 
Iwl::srr/M,
{I. 
o. 
otherwise. 
The zoomed output signal is defined as 
Xz[n] = XNM[Mkc 
Mka +n), 
O::s n::S N-l, 
for appropriate values of kc and ka . Assume that ka is chosen so that M is an integer in 
the following parts. 
(c)  Suppose that the ideallowpass filter h[nJ is approximated by a causal Type I linear­
phase filter of length 513 (nonzero for 0 ::s n ::s 512). Indicate which samples of 
XN M [n] provide the desired frequency samples. 
(d)  Using sketches of a typical spectrum for XN[k] and X (eJW), demonstrate that the 
system in Figure PlO.50-3 produces the desired samples. 
Extract correct 
p[n] 
h[n] 1--_0-1 
portion of 
XN[n] 
XNM[nll.....-_se_q_u_e_nc_e_...l 
Figure P10.50-3 

11  
Parametric Signal  
ModeHng  
11.0 INTRODUCTION 
Throughout this text, we have found it convenient to use several different representa­
tions of signals and systems. For example, the representation of a discrete-time signal 
as a sequence of scaled impulses was used in Eq. (2.5) of Chapter 2 to develop the 
convolution sum for LTI systems. The representation as a linear combination of sinu­
soidal and complex exponential signals led to the Fourier series, the Fourier transform, 
and the frequency domain characterization of signals and LTI systems. Although these 
representations are particularly useful because of their generality, they are not always 
the most efficient representation for signals with a known structure. 
This chapter introduces another powerful approach to signal representation called 
parametric signal modeling. In this approach. a signal is represented by a mathematical 
model that has a predefined structure involving a limited number of parameters. A 
given signal s[n] is represented by choosing the specific set of parameters that results in 
the model output s[n] being as close as possible in some prescribed sense to the given 
signal. A common example is to model the signal as the output of a discrete-time linear 
system as shown in Figure 11.1. Such models, which are comprised of the input signal 
v[n] and the system function H (z) of the linear system, become useful with the addition 
of constraints that make it possible to solve for the parameters of H (z) given the signal 
LTI
v[n] 
s[n]
System 
Figure 11.1 
Linear system model for a
H(z ) 
signal s[n]. 
890 

891 
Section 11.1 
All-Pole Modeling of Signals 
to be represented. For example, if the input v[n] is specified, and the system function is 
assumed to be a rational function of the form 
q 
L:>kZ-k 
H(z) = _k_=_O 
p ___ 
(11.1) 
1- LakZ-k 
k=l 
then the signal is modeled by the values of the akS and bkS or equivalently, by the poles 
and zeros of H (z), along with knowledge of the input. The input signal v[n] is generally 
assumed to be a unit impulse 8[n] for deterministic signals, or white noise if the signal 
s[n] is viewed as a random signaL When the model is appropriately chosen, it is possible 
to represent a large number of signal samples by a relatively small set of parameters. 
Parametric signal modeling has a wide range of applications, including data com­
pression, spectrum analysis, signal prediction, deconvolution, filter design, system iden­
tification, signal detection, and signal classification. In data compression, for example, 
it is the set of model parameters that is transmitted or stored, and the receiver then uses 
the model with those parameters to regenerate the signaL In filter design, the model pa­
rameters are chosen to best approximate, in some sense, the desired frequency response, 
or equivalently, the desired impulse response, and the model with these parameters then 
corresponds to the designed filter. The two key elements for success in all of the appli­
cations are an appropriate choice of model and an accurate estimate of the parameters 
for the modeL 
11.1 ALL-POLE MODELING OF SIGNALS 
The model represented by Eq. (11.1) in general has both poles and zeros. While there 
are a variety of techniques for determining the full set of numerator and denominator 
coefficients in Eq. (11.1), the most successful and most widely used have concentrated 
on restricting q to be zero, in which case, H(z) in Figure 11.1 has the form 
G 
G 
H(z) = 
(11.2)
p 
= A(z) , 
1 
Lakz- k 
k=l 
where we have replaced the parameter bo by the parameter G to emphasize its role 
as an overall gain factor. Such models are aptly termed "all-pole" models.1 By its very 
nature, it would appear that an all-pole model would be appropriate only for modeling 
signals of infinite duration. While this may be true in a theoretical sense, this choice 
for the system function of the model works well for signals found in many applications, 
and as we will show, the parameters can be computed in a straightforward manner from 
finite-duration segments of the given signaL 
1 Detailed discussion of this case and the general pole/zero case are given in Kay (1988), Thierrien 
(1992). Hayes (1996) and Stoica and Moses (2005). 

892  
Chapter 11 
Parametric Signal Modeling 
The input and output of the all-pole system in Eq. (11.2) satisfy the linear constant­
coefficient difference equation 
p 
s[n] = L ak.~[n 
k] + Gv[n],  
(11.3) 
k=l 
which indicates that the model output at time n is comprised of a linear combination 
of past samples plus a scaled input sample. As we will see, this structure suggests that 
the all-pole model is equivalent to the assumption that the signal can be approximated 
as a linear combination of (or equivalently, is linearly predictable from) its previous 
values. Consequently, this method for modeling a signal is often also referred to as 
linear predictive analysis or linear prediction.2 
II . 1.1 Least-Squares Approximation 
The goal in all-pole modeling is to choose the input vln] and the parameters G, and 
ab ... ,ap in Eq. (11.3) such that s[n] is a close approximation in some sense to s[n], 
the signal to be modeled. If, as is usually the case, v[n] is specified in advance (e.g., 
v[n] = 8[nI), a direct approach to determining the best values for the parameters might 
be to minimize the total energy in the errorsignalese[n] 
(s[n]-s[n]), thereby obtaining 
a least-squares approximation to sln]. Specifically, for deterministic signals, the model 
parameters might be chosen to minimize the total squared error 
p 
)2
00  
00 
( 
s[n])2 = 
L ak.Hn - k] - Gv[n] 
(11.4)
n~oc(s[n] -
n~oo s[n] 
k=l 
In principle, the akS minimizing this squared error can be found by differentiating the ex­
pression in Eq. (11.4) with respect to each parameter, setting that derivative to zero, and 
solving the resulting equations. However, this results in a nonlinear system ofequations, 
the solution of which is computationally difficult, in generaL Although this least-squares 
problem is too difficult for most practical applications, the basic least-squares principle 
can be applied to slightly different formulations with considerable success. 
1 I .1.2 Least-Squares Inverse Model 
A formulation based on inverse filtering provides a relatively straightforward and 
tractable solution for the parameter values in the all-pole modeL In any approach to 
approximation, it is recognized at the outset that the model output will in most cases 
not be exactly equal to the signal to be modeled. The inverse filtering approach is based 
on the recognition that if the given signal s[n] is in fact the output of the filter H(z) in 
the model of Figure 11.1 then with s[n] as the input to the inverse of H(z), the output 
will be v[n]. Consequently, as depicted in Figure 11.2 and with H(z) assumed to be an 
all-pole system as specified in Eq.(11.2), the inverse filter, whose system function 
p 
A(z) = 1  LakZ-k, 
(11.5) 
k=l 
2When used in the context of speech processing. linear predictive analysis is often referred to as linear 
predictive coding (LPC). (See Rabner and Schafer, 1978 and Quatieri, 2002.) 

893 
Section 11.1 
All-Pole Modeling of Signals 
s[n] 
Inverse 
g[n]
System 
A(z) 
Figure 11.2 
Inverse filter formulation 
for all-pole signal modeling. 
is sought so that its output g[n] would be equal to the scaled input Gv[n]. In this for­
mulation, then, we choose the parameters of the inverse filter (and therefore implicitly 
the parameters of the model system) to minimize the mean-squared error between g[n] 
and Gv[n]. As we will see, this leads to a set of well-behaved linear equations. 
From Figure 11.2 and Eq. (11.5) it follows that g[n] and s[n] satisfy the difference 
equation 
g[n] = s[n] - L
p 
aks[n - k]. 
(11.6) 
k=l 
The modeling error e[n] is now defined as 
p 
ern] = g[n] - Gv[n] = s[n] - L aks[n - k] - Gv[n]. 
(11.7) 
k=l 
If v[n] is an impulse, then, for n > 0, the error ern] corresponds to the error between 
s[n] and the linear prediction of s[n] using the model parameters. Thus, it is convenient 
to also express Eq. (11.7) as 
ern] = ern] - Gv[n], 
(11.8) 
where ern] is the prediction error given by 
p 
ern] = s[n] - L aks[n - k]. 
(11.9) 
k=l 
For a signal that exactly fits the all-pole model of Eq. (11.3), the modeling error ern] 
will be zero, and the prediction error ern] will be the scaled input, i.e., 
ern] = Gv[n]. 
(11.10) 
This formulation in terms of inverse filtering leads to considerable simplification, 
since v[n] is assumed known and ern] can be computed from s[n] using Eq. (11.9). The 
parameter values ak are then chosen to minimize 
(11.11)  
where the notation ( . ) denotes a summing operation for finite energy determinis­
tic signals and an ensemble averaging operation for random signals. Minimizing [ in 
Eq. (11.11) results in an inverse filter that minimizes the total energy in the modeling 
error in the case of deterministic signals or the mean-squared value of the modeling 
error in the case of random signals. For convenience, we will often refer to (.) as the 
averaging operator where its interpretation as a sum or as an ensemble average should 
be clear from the context. Again, note that in solving for the parameters ak specifying 
the inverse system of Figure 11.2, the all-pole system is implicitly specified, as well. 

894 
Chapter 11 
Parametric Signal Modeling 
To find the optimal parameter values, we substitute Eq. (11.8) into Eq. (11.11) to 
obtain 
E 
((ern) - Gv[n])2), 
(11.12) 
or equivalently, 
E = (e2[n]) + G2 (v2[n]) - 2G (v[n)e[n]) . 
(11.13) 
To find the parameters that minimize E, we differentiate Eq. (11.12) with respect 
to the ith filter coefficient ai and set the derivative equal to zero, leading to the set of 
equations 
at: 
a [( 2 
) 
. ] 
.
-
e [n] 
2G (v[n]s[n 
I)) = 0, 
I = 1,2, ... , p, 
(11.14)
aai 
aaj 
where we have assumed that G is independent of ai and, of course, so is v[n], and 
consequently that 
a~i [G2(v2[n))] =0. 
(11.15) 
For models that will be of interest to us, v[n) will be an impulse if s[n] is a causal 
finite-energy signal and white noise if s[n) is a wide-sense stationary random pro­
cess. With v[n] an impulse and s[nl zero for n < 0, the product uln]s[n - i] °for 
i = 1,2, ... p. With urn] as white noise, 
(v[n]s[n 
i]) = 0, 
i = 1,2, ... p, 
(11.16) 
since for any value of n, the input of a causal system with white-noise input is uncorre­
lated with the output values prior to time n. Thus, for both cases, Eq. (11.14) reduces 
to 
at: 
a (2) 
.
-
e [n] =0 
I =1,2" ... ,p 
(11.17)
aaj 
aai 
In other words, choosing the coefficients to minimize the average squared modeling 
error (e2 [n]) is equivalent to minimizing the average squared prediction error (e2 [n)}. 
Expanding Eq. (11.17) and invoking the linearity of the averaging operator, we obtain 
from Eq. (11.17) the equations 
p 
(s[n]s[n - i]) - Lak (s[n 
k]s[n 
i]) = 0, 
i = 1, ... , p. 
(11.18) 
k=l 
Defining 
<Pss[i, k] = (s[n 
i]s[n 
k]), 
(11.19) 
Eqs. (11.18) can be rewritten more compactly as 
pL akifJss[i, k] = ifJss[i, 0], 
i = 1,2, ... , p. 
(11.20) 
k=l 
Equations (11.20) comprise a system of p linear equations in p unknowns. Com­
putation of the parameters of the model can be achieved by solving the set of linear 
equations for the parameters ak for k = 1,2, ... , p, using known values for ifJss[i, k] for 
i = 1,2, ... , p and k 
0,1, ... , p or first computing them from s[nJ. 

Section 11.1 
All-Pole Modeling of Signals 
895 
11.1.3  Linear Prediction Formulation of All-Pole 
Modeling 
As suggested earlier, an alternative and useful interpretation ofall-pole signal modeling 
stems from the interpretation ofEq. (11.3) as a linear prediction of the output in terms 
of past values, with the prediction error ern] being the scaled input Gv[n], i.e., 
p 
ern] = s[n] L aks[n - k] 
Gv[n].  
(11.21) 
k=l 
As indicated by Eq. (11.17), minimizing the inverse modeling error£ in Eq. (11.11) 
is equivalent to minimizing the averaged prediction error (e2 [n]). If the signal s[n] were 
produced by the model system, and if v[n] is an impulse, and if s[n] truly fits the all-pole 
model, then the signal at any n > 0 is linearly predictable from past values, i.e., the 
prediction error is zero. If v[n] is white noise, then the prediction error is white. 
The interpretation in terms of prediction is depicted in Figure 11.3, where the 
transfer function of the prediction filter P(z) is 
p 
P(z) = LakZ-k,  
(11.22) 
k=l 
This system is referred to as the pth-order linear predictor for the signal s[n]. Its output 
is 
p 
s[n] = I:>ks[n 
k], 
(11.23) 
k=l 
and as Figure 11.3 shows, the prediction error signal is ern] = s[n] 
srn]. The sequence 
ern] represents the amount by which the linear predictor fails to exactly predict the 
signal s[n]. For this reason, e[n] is also sometimes called the prediction error residual or 
simply the residual. With this point of view, the coefficients ak are called the prediction 
coefficients. As is also shown in Figure 11.3, the prediction error filter is related to the 
linear predictor by 
p 
A(z) = 1 - P(z) = 1 L akz-k .  
(11.24) 
k=l 
s".;;[n"';]'--i-'r------------------....-I100{-+ Ji-,_e...;.[n...;.l~ 
1 
1 
1 
1 
1 
Linear 
, 
1 
Predictor 
1 
: 
1 __
P(z) 
_________
: 
__ J 
Figure 11.3 Linear prediction 
A(z)  
formulation for all-pole signal modeling. 

896 
Chapter 11 
Parametric Signal Modeling 
11.2 DETERMINISTIC AND RANDOM SIGNAL MODELS 
To use the optimum inverse filter or equivalently the optimum linear predictor as a basis 
for parametric signal modeling, it is necessary to be more specific about the assumed 
input v[n] and about the method of computing the averaging operator (.). To this end, 
we consider separately the case of deterministic signals and the case of random signals. 
In both cases, we will use averaging operations that assume knowledge of the signal 
to be modeled over all time -00 < n < 00. In Section 11.3, we discuss some of the 
practical considerations when only a finite-length segment of the signal s[n] is available. 
11.2.1  Ail-Pole Modeling of Finite-Energy Deterministic 
Signals 
In this section, we assume an all-pole model that is causal and stable and also that both 
the input v[n] and the signal s[n] to be modeled are zero for n < O. We further assume 
that s[n] has finite energy and is known for all n :::: O. We choose the operator (.) in 
Eq. (11.11) as the total energy in the modeling error sequence e[n], i.e., 
e 
(le[n]12) = L
co 
le[n]12 .  
(11.25) 
n=-::"X) 
With this definition of the averaging operator, <Pss[i, k] in Eq. (11.19) is given by 
<Pss[i, k] 
L
00 
s[n 
i]s[n - k], 
(11.26) 
n=-oo 
and equivalently, 
<Pss[i, k] = L
00 
s[n]s[n - (i - k)]. 
(11.27) 
n=-oo 
The coefficients <Pss[i, k] in Eq. (11.20) are now 
<Pss[i, k] 
rss[i - k],  
(11.28) 
where for real signals s[n], rss[m] is the deterministic autocorrelation function 
00  
00 
rss[m] 
L s[n + m]s[n] = L s[n]s[n 
m]. 
(11.29) 
n=-co 
n=-oo 
Therefore, Eq. (11.20) takes the form 
L
p 
akrss[i - k] = rss[i] 
i = 1,2, ... , p. 
(11.30) 
k=l 
These equations are called the autocorrelation normal equations and also the Yule­
Walker equations. They provide a basis for computing the parameters ab ... , ap from 
the autocorrelation function of the signal. In Section 11.2.5, we discuss an approach to 
choosing the gain factor G. 

Section 11.2 
Deterministic and Random Signal Models 
897 
w[n] 
white 
noise 
LTI 
System 
H(z) 
s[n] 
Figure 11.4 Linear system model for a 
random signal s[n]. 
11.2.2 Modeling of Random Signals 
For all-pole modeling of zero-mean, wide-sense stationary, random signals, we assume 
that the input to the all-pole model is zero-mean, unit-variance, white noise as indicated 
in Figure 11.4. The difference equation for this system is 
p 
s[n] = LQks[n - k] + Gw[n], 
(11.31) 
k=l 
where the input has autocorrelation function E{w[n + m]w[n]} 
o[m], zero mean 
(E{w[n]) = 0), and unit average power (E{(w[n])2} = 0[0] = 1), with E{·} representing 
the expectation or probability average operator.3 
The resulting model for analysis is the same as that depicted in Figure 11.2, but 
the desired output g[n] changes. In the case ofrandom signals, we want to make g[n] as 
much like a white-noise signal as possible, rather than the unit sample sequence that was 
desired in the deterministic case. For this reason, the optimal inverse filter for random 
signals is often referred to as a whitening filter. 
We also choose the operator (.) in Eq. (11.11) as an appropriate one for random 
signals, specifically the mean-squared value or equivalently the average power. Then 
Eq. (11.11) becomes 
(11.32) 
If s[n] is assumed to be a sample function of a stationary random process, then ¢ss[i, k] 
in Eq. (11.19) would be the autocorrelation function 
¢ss[i, k] = E{s[n - i]s[n - k]} = rss[i - k]. 
(11.33) 
The system coefficients can be found as before from Eq. (11.20). Thus, the system 
coefficients satisfy a set of equations of the same form as Eq. (11.30), i.e., 
p 
LQkrss[i - k] = rss[i], 
i = 1,2, ... , p. 
(11.34) 
k=l 
Therefore, modeling random signals again results in the Yule-Walker equations, with 
the autocorrelation function in this case being defined by the probabilistic average 
rss[m] = E {s[n + m]s[n]} = E {s[n]s[n - m]}. 
(11.35) 
3 Computation of E{.} requires knowledge ofthe probability densities. In the case ofstationary random 
signals, only one density is required. In the case of ergodic random processes, a single infinite time average 
could be used. In practical applications, however, such averages must be approximated by estimates obtained 
from finite time averages. 

898 
Chapter 11 
Parametric Signal Modeling 
11.2.3 Minimum Mean-Squared Error 
For modeling of either deterministic signals (Section 11.2.1) or random signals (Section 
11.2.2) the minimum value of the prediction error ern] in Figure 11.3 can be expressed 
in terms of the corresponding correlation values in Eq. (11.20) to find the optimum 
predictor coefficients. To see this, we write [, as 
(11.36)
£ ~((*1-Ea"rn -kin 
As outlined in more detail in Problem 11.2, if Eq. (11.36) is expanded, and Eq. (11.20) 
is substituted into the result, it follows that in general, 
p 
[, = 4>ss[O, 0] - I>k4>ss[O, k]. 
(11.37) 
k=l 
Equation (11.37) is true for any appropriate choice of the averaging operator. In par­
ticular, for averaging definitions for which 4>ss[i, k] 
rss[i 
k], Eq. (11.37) becomes 
p 
[, = rss[O] - LakrSs[k]. 
(11.38) 
k=l 
11 .2.4 Autocorrelation Matching Property 
An important and useful property of the all-pole model resulting from the solution 
of Eq. (11.30) for deterministic signals and Eq. (11.34) for random signals is referred 
to as the autocorrelation matching property (Makhoul, 1973). Equations (11.30) and 
(11.34) represent a set of p equations to be solved for the model parameters ak for 
k = 1, ... , p. The coefficients in these equations on both the left- and right-hand sides 
of the equations are comprised of the (p + 1) correlation values rss[m], m 
0,1, ... , p, 
where the correlation function is appropriately defined, depending on whether the signal 
to be modeled is deterministic or random. 
The basis for verifying the autocorrelation matching property is to observe that 
the signal S[n] obviously fits the model when the model system H(z) in Figure 11.1 is 
specified as the all-pole system in Eq. (11.2). If we were to consider again applying 
all-pole modeling to s[n], we would of course again obtain Eqs. (11.30) or (11.34), but 
this time, with rss [m] in place of rss [m]. The solution must again be the same parameter 
values ak, k = 1,2, ... , p, since s[n] fits the model, and this solution will result if 
rss[m] = crss[m] 
0 ~ m ~ p, 
(11.39) 
where c is any constant. The fact that the equality in Eq. (11.39) is required follows 
from the form of the recursive solution of the Yule-Walker equations as developed in 
Section 11.6. In words, the autocorrelation normal equations require that for the lags 
1m I = 0, 1, ... , p the autocorrelation functions of the model output and the signal being 
modeled are proportional. 

899 
Section 11.2 
Deterministic and Random Signal Models 
11.2.5 Determination of the Gain Parameter G 
With the approach that we have taken, determination of the optimal choice for the 
coefficients ak of the model does not depend on the system gain G. From the perspective 
of the inverse filtering formulation in Figure 11.2, one possibility is to choose G so that 
(s[n])2) 
((s[n])2); For finite-energy deterministic signals, this corresponds to matching 
the total energy in the model output to the total energy in the signal that is being 
modeled. For random signals, it is the average power that is matched. In both cases, this 
corresponds to choosing G, so that rss[O] 
rss[O]. With this choice, the proportionality 
factor c in Eq. (11.39) is unity. 
Example I I .1 
I st·Order System 
Figure 11.5 shows two signals, both of which are outputs of a 1st-order system with 
system function 
1
H(z) = .__..-. 
(11.40)
1 - az-1 
The signal sd[n] = h[n] = anu[n] is the output when the input is a unit impulse 8[nJ, 
while the signal sr(n] is the output when the input to the system is a zero mean, unit 
variance white-noise sequence. Both signals extend over the range -00 < n < 00, as 
suggested by Figure 11.5. 
~..
_____________~M!~mrii~1mii,'ttlimiii"!!iMi______... 
I 
n 
n 
Figure 11.5 Examples of deterministic and random outputs of a 1 sl-order all-pole 
system. 
The autocorrelation function for the signal sd[n] is 
00 
Iml
'"' n+m n 
a­
rSdSd(m] = rhh [m 1= L 
a 
a 
= --2' 
(11.41)
I-a
n=O 
the autocorrelation function of sr[n] is also given by Eq. (11.41) since sr[n] is the 
response of the system to white noise, for which the autocorrelation function is a unit 
impulse. 
Since both signals were generated with a 1st-order all-pole system, a 15t-order 
all-pole model will be an exact fit. In the deterministic case, the output of the optimum 

900 
Chapter 11 
Parametric Signal Modeling 
inverse filter will be a unit impulse, and in the random signal case, the output of the 
optimum inverse filter will be a zero-mean white-noise sequence with unit average 
power. To show that the optimum inverse filter will be exact, note that for a 1st-order 
model, Eqs. (11.30) or (11.34) reduce to 
rSdSd [0]a1 = r SdSd [1], 
(11.42) 
so from Eq. (11.41), it follows that the optimum predictor coefficient for both the 
deterministic and the random signal is 
Cl 
al = rSdSd [1] 
ix. 
(11.43) 
rSdSd [0] 
1­
From Eq. (11.38), the minimum mean-squared error is 
2
1 
a
[; 
1, 
(11.44) 
which is the size of the unit impulse in the deterministic case and the average power 
of the white-noise sequence in the random case. 
As mentioned earlier, and as is clear in this example, when the signal is generated 
by an all-pole system excited by either an impulse or white noise, all-pole modeling can 
determine the parameters of the all-pole system exactly. This requires prior knowledge 
of the model order p and the autocorrelation function. This was possible to obtain 
for this example, because a closed-form expression was available for the infinite sum 
required to compute the autocorrelation function. In a practical setting, it is generally 
necessary to estimate the autocorrelation function from a finite-length segment of the 
given signal. Problem 11.14 considers the effect of finite autocorrelation estimates (to 
be discussed next) for the deterministic signal Sd [n] of this section. 
11.3 ESTIMATION OF THE CORRELATION FUNCTIONS 
To use the results of Sections 11.1 and 11.2 for modeling of either deterministic or ran­
dom signals, we require apriori knowledge of the correlation functions ¢ss[i, k] that are 
needed to form the system equations satisfied by the coefficients ak, or we must estimate 
these from the given signal. Furthermore, we may want to apply block processing or 
short-time analysis techniques to represent the time-varying properties of a nonstation­
ary signal, such as speech. In this section, we will discuss two distinct approaches to 
the computation of the correlation estimates for practical application of the concepts 
of parametric signal modeling. These two approaches have come to be known as the 
autocorrelation method and the covariance method. 
11.3.1 The Autocorrelation Method 
Suppose that we have available a set of M + 1 signal samples s[n] forO :s n :s M, and we 
wish to compute the coefficients for an all-pole model. In the autocorrelation method, 
it is assumed that the signal ranges over -00 < n < 00, with the signal samples taken to 

901 
Section 11.3 
Estimation of the Correlation Functions 
be zero for all n outside the interval 0 ~ n ~ M, even if they have been extracted from 
a longer sequence. This, of course, imposes a limit to the exactness that can be expected 
of the model, since the IIR impulse response of an all-pole model will be used to model 
the finite-length segment of sfn]. 
Although the prediction error sequence need not be computed explicitly to solve 
for the filter coefficients, it is nevertheless informative to consider its computation in 
some detail. The impulse response of the prediction error filter is, by the definition of 
A(z), in Eq. (11.24), 
p 
hA[n] = 8[n] 
I>kO[n - k]. 
(11.45) 
k=l 
It can be seen that since the signal sIn] has finite length M + 1 and hA[n], the impulse 
response of the prediction filter A[z], has length p + 1, the prediction error sequence 
ern] = hA[n] * sIn] will always be identically zero outside the interval 0 ~ n ~ M + p. 
Figure 11.6 shows an example of the prediction error signal for a linear predictor with 
p = 5. In the upper plot, hA[n 
m] the (time-reversed and shifted) impulse response 
of the prediction error filter, is shown as a function of m for three different values of n. 
The dark lines with square dots depict hA[n 
m], and the lighter lines with round dots 
show the sequence s[m] for 0 ~ m ~ 30. On the left side is hA[O - m], which shows that 
the first nonzero prediction error sample is e[O] = s[0]. This, of course, is consistent with 
Eq. (11.9). On the extreme right is hA[M + p - m], which shows that the last nonzero 
error sample is elM + p] = -aps[MJ. The second plot in Figure 11.6 shows the error 
signal e[n] for 0 < n ~ M +p. From the point of view of linear prediction, it follows that 
the first p samples (dark lines and dots) are predicted from samples that are assumed 
to be zero. Similarly, the samples of the input for n ?: M + 1 are assumed to be zero to 
obtain a finite-length signal. The linear predictor attempts to predict the zero samples 
p 
ern] 
hA[n]*s[n] s[ll] LaJcS[n-k] 
k~l 
--------i.--- T,I'. . . .! ..• T .., ••.. __.. _____
-
II J. ~ ••• ~ • • • • ...~ •• · r 
i 
/I 
! 
M 
M+p 
Figure 11.6 Illustration (for p = 5) of computation of prediction error for the 
autocorrelation method. (Square dots denote samples of hAln - m] and light round 
dots denote samples of s[m] for the upper plot and ern] for the lower plot.) 

Chapter 11 
Parametric Signal Modeling
902 
sIn], sIn + m] 
•
r 
~·II·
'h'TH:!~h 
-------..
~11·' •• ~ 
t 
Mt 
n
t 
-m 
• 
M 
m 
Figure 11.7 Illustration of computation of the autocorrelation function for afinite­
length sequence. (Square dots denote samples of s[n + m], and light round dots 
denote samples of s[n].) 
in the interval M +1 ::; n ::; M + p from prior samples that are nonzero and part of the 
original signal.Indeed, if s[O] :/= 0 and s[M] :/= 0, then it will be true that both e[O] 
s[O] 
and e[M + p] = -ups[MJ will be nonzero. That is, the prediction error (total-squared 
error E) can never be exactly zero if the signal is defined to be zero outside the interval 
o ::; n ::; M. Furthermore, the total-squared prediction error for a pth-order predictor 
would be 
00 
M+p 
e(P) 
(e[n]2) = L e[nf L e[n]2, 
(11.46) 
n=-oo 
n=O 
i.e., the limits of summation can be infinite for convenience, but practically speaking, 
they are finite. 
When the signal is assumed to be identically zero outside the interval 0 ::; n ::; M, 
the correlation function ¢ss[i, k] reduces to the autocorrelation function rss[m] where 
the values needed in Eq. (11.30) are for m = Ii - kl. Figure 11.7 shows the shifted 
sequences used in computing rss[m] with s[n] denoted by round dots and s{n + m] by 
square dots. Note that for a finite-length signal, the product s[n]s[n + m] is nonzero 
only over the interval 0 ::; n ::; M - m when m ~ O. Since rss is an even function, i.e., 
rss[-m] = rss[m] 
rss[lmi] it follows that the autocorrelation values needed for the 
Yule-Walker equations can be computed as, 
00 
M-Iml 
rdlml] 
L s[n]s[n + Iml] = L s[n]s[n + Iml]· 
(11.47) 
n=-oo 
n=O 
For the finite-length sequence sin], Eq. (11.47) has all the necessary properties of an 
autocorrelation function and Tss [m] 
0 for m > M. But of course rss [m] is not the same 
as the autocorrelation function of the infinite length signal from which the segment was 
extracted. 
Equation (11.47) can be used to compute estimates of the autocorrelation func­
tion for either deterministic or random signals.4 Often, the finite-length input signal 
is extracted from a longer sequence of samples. This is the case, for example, in ap­
plications to speech processing, where voiced segments (e.g., vowel sounds) of speech 
are treated as deterministic and unvoiced segments (fricative sounds) are treated as 
4In the context of random signals, it was shown in Section 10.6 that Eq. (11.47) is abiased estimate of 
the autocorrelation function. When p « M as is often the case, this statistical bias is generally negligible. 

903 
Section 11.3 
Estimation of the Correlation Functions 
random signals.s According to the previous discussion, the first p and last p samples of 
the prediction error can be large due to the attempt to predict nonzero samples from 
zero samples and to predict zero samples from nonzero samples. Since this can bias the 
estimation of the predictor coefficients, a signal-tapering window, such as a Hamming 
window is generally applied to the signal before computation of the autocorrelation 
function. 
11.3.2 The Covariance Method 
An alternative choice for the averaging operator for the prediction error for a pth-order 
predictor is 
M 
£~j = (e[n])2) = L(e[n])2. 
(11.48) 
n=p 
As in the autocorrelation method, the averaging is over a finite interval (p s: n s: M), 
but the difference is that the signal to be modeled is known over the larger interval 
Os: n s: M. The total-squared prediction error only includes values of ern] that can be 
computed from samples within the interval 0 < n s: M. Consequently, the averaging 
takes place over a shorter interval p s: n s: M. This is significant, since it relieves 
the inconsistency between the all-pole model and the finite-length signa1.6 In this case, 
we only seek to match the signal over a finite interval rather than over all n as in the 
autocorrelation method. The upper plot in Figure 11.8 shows the same signal s[m] as 
[ 
p 
ern] 
hA[n] *s[n] =s[n]- L,a!(S[n k] 
-I 
kZ"l 
..~-.--..~
.'. T••• ' 
"".' .' •• 
n
p 
• 
• •I 1-'· 
• 
M 
Figure 11.8 Illustration (for p = 5) of computation of prediction error for the 
covariance method. (In upper plot, square dots denote samples of hA[n - m], and 
light round dots denote samples of s[m].) 
SIn both cases, the deterministic autocorrelation function in Eq. (11.47) is used as an estimate. 
6The definitions of total-squared prediction error in Eqs. (11.48) and (11.46) are distinctly different, 
so we use the subscript COy to distinguish them. 

904 
Chapter 11 
Parametric Signal Modeling 
s[n - iJ, s[n 
k] 
..  
n 
p 
!.J 
Figure 11.9 Illustration of computation of covariance function for afinite-length 
sequence. (Square dots denote samples of s[n - k] and light round dots denote 
samples of s[n - i].) 
in the upper part of Figure 11.6, but in this case, the prediction error is only computed 
over the interval p :s n :s M as needed in Eq. (11.48). As shown by the prediction error 
filter impulse responses hA[n 
m] in the upper plot, there are no end effects when the 
prediction error is computed in this way, since all the signal samples needed to compute 
the prediction error are available. Because of this, it is possible for the prediction error 
to be precisely zero over the entire interval p :s n :s M, if the signal from which the 
finite length segment was extracted was generated as the output of an all-pole system. 
Seen another way, if sin] is the output of an all-pole system with an input that is zero 
for n > 0, then as seen from Eqs. (11.9) and (11.10) the prediction error will be zero for 
n > O. 
The covariance function inherits the same definition of the averaging operator, 
i.e., 
M 
<Pss[i, k] L s[n - i]s[n - kJ. 
(11.49) 
n=p 
The shifted sequences sin - i] (light lines and round dots) and sin - k] (dark lines and 
square dots) are shown in Figure 11.9. This figure shows that since we need <Pss[i, k] only 
for i = 0, 1, ... , p and k = 1. 2, ... , p, the segment sin] for 0 :s n :s M contains all the 
samples that are needed to compute <Pss [i, k] in Eq. (11.49). 
11.3.3 Comparison of Methods 
The autocorrelation and covariance methods have many similarities, but there are many 
important differences in the methods and the resulting all-pole models. In this section, 
we summarize some of the differences that we have already demonstrated and call 
attention to some others. 
Prediction Error 
Both the averaged prediction error (e2[n]) and averaged modeling error (e2[nl) are 
nonnegative and nonincreasing with increasing model order p. In the autocorrelation 
method based on estimates obtained from finite-length signals, the averaged modeling 
or prediction error will never be zero, because the autocorrelation values will not be ex­
act. Furthermore, the minimum value of the prediction error even with an exact model 
is Gv[n] as indicated by Eq. (11.10). In the covariance method, the prediction error for 
n > 0 can be exactly zero if the original signal was generated by an all-pole model. This 
will be demonstrated in Example 11.2. 

905 
Section 11.4 
Model Order 
Equations for Predictor Coefficients 
In both methods, the predictor coefficients that minimize the averaged prediction error 
satisfy a general set of linear equations expressed in matrix form as .a = -,;. The co­
efficients ofthe all-pole model are obtained by inverting the matrix.; i.e., a = .-1-,;. 
In the covariance method, the elements <Pss [i, k] of the matrix • are computed using 
Eq. (11.49). In the autocorrelation method, the covariance values become autocorrela­
tion values, i.e., <Pss[i, k] = rss [Ii - kl] and are computed using Eq. (11.47). In both cases, 
the matrix. is symmetric and positive-definite, but in the autocorrelation method, the 
matrix. is also a Toeplitz matrix. This implies numerous special properties of the solu­
tion, and it implies that the solution of the equations can be done more efficiently than 
would be true in generaL In Section 11.6, we will explore some of these implications for 
the autocorrelation method. 
Stability ofthe Model System 
The prediction error filter has a system function A(z) that is a polynomial in 
. There­
fore, it can be represented in terms of its zeros as 
p 
p 
A(z)=1 
LakZ-k=fl(1 
ZkZ-1). 
(11.50) 
k=l 
k=l 
In the autocorrelation method, the zeros of the prediction error filter A(z) are 
guaranteed to lie strictly within the unit circle of the z plane; i.e., IZk I < 1. This means 
that the poles of the causal system function H(z) = GI A(z) of the model lie inside the 
unit circle, which implies that the model system is stable. A simple proof of this assertion 
is given by Lang and McClellan (1979) and McClellan (1988). Problem 11.10 develops 
a proof that depends on the lattice filter interpretation of the prediction error system 
to be discussed in Section 11.7.1. In the covariance method as we have formulated it, 
no such guarantee can be given. 
11.4 MODEL ORDER 
An important issue in parametric signal modeling is the model order p, the choice of 
which has a major impact on the accuracy of the model. A common approach to choosing 
p is to examine the averaged prediction error (often referred to as the residual) from 
the optimum pth-order model. Let aiP) be the parameters for the optimal pth-order 
predictor found using Eq. (11.30). The prediction error energy for the lh-order model 
using the autocorrelation method is7 
E(" 
,j::oo (,[nl - ~a:P'* -kl)
2 
(11.51) 
For the zeroth-order predictor, (p = 0), there are no delay terms in Eq. (11.51), i.e., the 
"predictor" is just the identity system so e[nl = s[nl. Consequently, for p = 0, 
£(0) = L
00 
s2[n] = r.dO]. 
(11.52) 
n=-oo 
7RecalJ that £~g~ denotes the total-squared prediction error for the covariance method, while we use 
£(p) with no subscript to denote the total-squared prediction error for the autocorrelation method. 

906 
Chapter 11 
Parametric Signal Modeling 
Plotting the normalized mean-squared prediction error V(p) = E(p) /S(O) as a func­
tion of p shows how increasing p changes this error energy. In the autocorrelation 
method, we showed that the averaged prediction error can never be precisely zero, 
even if the signal s[n] was generated by an all-pole system, and the model order is the 
same as the order of the generating system. In the covariance method, however, if the 
aU-pole model is a perfect model for the signal s[n], sJt:J will become identically zero 
at the correct choice of p, since the averaged prediction error only considers values 
for p 
n:S M. Even if s[n] is not perfectly modeled by an all-pole system, there is 
often a value of p above which increasing p has little or no effect on either V(p) or 
vJgJ 
SigJ/sJg~. This threshold is an efficient choice of model order for representing 
the signal as an all-pole model. 
Example 11.2 Model Order Selection 
To demonstrate the effect of model order, consider a signal s[n] generated by exciting 
a 10th-order system 
0.6
H(z) 
(11.53)
(1 
1.03z-1 + 0.79z-2 
1.34z-3 + 0.78c4 - O.92c5 
+1.22z-6 - 0.43C7 +0.6z-8 
0.29z-9 - 0.23ClO) 
with an impulse v[n] = 8[nJ. The samples of s[n] for 0 :s n :s 30 are shown as the 
sequence in the upper plots in Figures 11.6 and 11.8. This signal was used as the signal 
to be modeled by an all-pole model with both the autocorrelation method and the 
covariance method. Using the 31 samples of s[nl, the appropriate autocorrelation and 
covariance values were computed and the predictor coefficients computed by solv­
ing Eqs. (11.30) and (11.34) respectively. The normalized mean-squared prediction 
errors are plotted in Figure 11.10. Note that in both the autocorrelation and covari­
ance methods the normalized error decreases abruptly at p = 1 in both plots, then 
decreasing more slowly as p increases. At p 
10, the covariance method gives zero 
error, while the autocorrelation method gives a nonzero averaged error for p 2: 10. 
This is consistent with our discussion of the prediction error in Section 11.3. 
1( 
-g~B 0.8 
~-g
cr'"  
~~ 0.6  
o E 0 
"0 t:: 
.~ ~ 
0.4 
-S'B
0 
.....­
0"0 
0.2
Z ~ 
,
0.. 
I 
, 
.... .....,
0' 0 
2 
4 
6 
8 
10 
12 
14 
16 
18 
20 
Model order p 
Figure 11.10 
Normalized mean-squared prediction error V(P) as a function of 
model order p in Example 11.2. 
, I 
-
-
Covariance Method 

Section 11.5 
All-Pole Spectrum Analysis 
907 
While Example 11.2 is an ideal simulation, the general nature of the dependence 
of averaged prediction error as a function of p is typical of what happens when all-pole 
modeling is applied to sampled signals. The graph of V(p) as a function of p tends to 
flatten out at some point, and that value of p is often selected as the value to be used 
in the model. In applications such as speech analysis, it is possible to choose the model 
order based on physical models for the production of the signal to be modeled. (See 
Rabiner and Schafer, 1978.) 
11.5 ALL-POLE SPECfRUM ANALYSIS 
All-pole signal modeling provides a method of obtaining high-resolution estimates of a 
signal's spectrum from truncated or windowed data. The use of parametric signal mod­
eling in spectrum analysis is based on the fact that if the data fits the model, then a finite 
segment of the data can be used to determine the model parameters and, consequently, 
also its spectrum. Specifically, in the deterministic case 
IS(ejW )12 
IH(ejW)121V(ejl1l)12 = IH(ejW )12 
(11.54) 
since IV(ejW )12 = 1 for a unit impulse excitation to the model system. Likewise, for 
random signals the power spectrum of the output of the model is 
P',s(ejW) = IH(ejW)12Pww(ejW) 
IH(ejW)12, 
(11.55) 
since Pww(e jW ) = 1 for the white-noise input. Thus, we can obtain an estimate of the 
spectrum of a signal sIn] by computing an all-pole model for the signal and then com­
puting the magnitude-squared of the frequency response of the model system. For both 
the deterministic and random cases, the spectrum estimate takes the form 
2
G
Spectrum estimate = IH(e jW )12 = 
(11.56)
P 
1 L 
k=1 
To obtain an understanding of the nature of the spectrum estimate in Eq. (11.56) 
for the deterministic case, it is useful to recall that the DTFT of the finite-length signal 
sIn] is 
M 
S(e jW ) =L s [n]e- jl1ln . 
(11.57) 
n=O 
Furthermore, note that 
M-Iml  
rsslm] = L sIn + m]s[n]  
(11.58) 
n=O 
where, due to the finite length of sIn], rsslmJ 
0 for Iml > M. The values of rss[ml 
for m = 0, 1. 2, ... ,p are used in the computation of the all-pole model using the 
autocorrelation method. Thus, it is reasonable to suppose that there is a relationship 

908 
Chapter 11 
Parametric Signal Modeling 
between the Fourier spectrum of the signal, IS(ejW)12, and the all-pole model spectrum, 
IS(e j U»1 2 = IH(ejW )12• 
One approach to illuminating this relationship is to obtain an expression for the 
averaged prediction error in terms of the DTFT of the signal s[n]. Recall that the 
prediction error is e[n] = h.4[n] * s[n], where hA[n) is the impulse response of the 
prediction error filter. From Parseval's Theorem, the averaged prediction error is 
M+p 
1 In 
E = L (e[n])2 = 
IS(ei"')1 2IA(ej U»12dw, 
(11.59)
2Jr -n
n=O 
. 
where S(eju» is the DTFf of s[n] as given by Eq. (11.57). Since H(z) 
G/A(z), 
Eq. (11.59) can be expressed in terms of H(eju» as 
_ 
IS(eju»e
G2 In 
E--
. 
2dw. 
(11.60)
2Jr -n IH(eJW)1 
. 
Since the integrand in Eq. (11.60) is positive, and IH(ejW )12 > 0 for -Jr < w :s Jr, 
it therefore follows from Eq. (11.60) that minimizing E is equivalent to minimizing 
the ratio of the energy speetrum of the signal s[n] to the magnitude-squared of the 
frequency response of the linear system in the all-pole model. The implication of this is 
that the all-pole model spectrum will attempt to match the energy spectrum of the signal 
more closely at frequencies where the signal spectrum is large, since frequencies where 
IS(ej U»1 2 > IH(ej U»1 2 contribute more to the mean-squared error than frequencies 
where the opposite is true. Thus, the all-pole model spectrum estimate favors a good 
fit around the peaks of the signal spectrum. This will be illustrated by the discussion in 
Section 11.5.1. Similar analysis and reasoning also applies to the case in which s[n] is 
random. 
11.5.1 All-Pole Analysis of Speech Signals 
All-pole modeling is widely used in speech processing both for speech coding, where the 
term linear predictive coding (LPC) is often used, and for spectrum analysis. (See Atal 
and Hanauer, 1971, Makhoul, 1975, Rabiner and Schafer, 1978, and Quatieri, 2002.) To 
illustrate many of the ideas discussed in this chapter, we discuss in some detail the use 
of all-pole modeling for spectrum analysis of speech signals. This method is typically 
applied in a time-dependent manner by periodically selecting short segments of the 
speech signal for analysis in much the same way as is done in time-dependent Fourier 
analysis as discussed in Section 10.3. Since the time-dependent Fourier transform is 
essentially a sequence of DTFfs of finite-length segments, the above discussion of the 
relationship between the DTFT and the all-pole spectrum characterizes the relationship 
between time-dependent Fourier analysis and time-dependent all-pole model spectrum 
analysis, as well. 
Figure 11.11 shows a 201-point Hamming-windowed segment of a speech signal 
s [11] in the top panel and the corresponding autocorrelation function rss [m] below. Dur­
ing this time interval, the speech signal is voiced (vocal cords vibrating), as evidenced 
by the periodic nature of the signal. This periodicity is reflected in the autocorrelation 
function as the peak at about 27 samples (27/8 =3.375 ms for 8 kHz sampling rate) and 
integer multiples thereof. 

Section 11.5 
All-Pole Spectrum Analysis 
909 
O.S 
o 
-D.S 
Samples number (8 kHz sampling rate) 
(a) 
Sr--r---,--.-,,-----------,------------,-----------~ 
'---'-- '----L__'--__________-'-___ 
o 
-S 
o  12 
27 
40 SO 
100 
150 
200 
Lag number (8 kHz sampling rate) 
(b) 
Figure 11.11 
(a) Windowed voiced speech waveform. (b) Corresponding auto­
correlation function (samples connected by straight lines). 
When applying all-pole modeling to voiced speech, it is useful to think of the 
signal as being deterministic, but with an excitation function that is a periodic train of 
impulses. This accounts for the periodic nature of the autocorrelation function when 
several periods of the signal are included in the window as in Figure 11.11( a). 
Figure 11.12 shows a comparison of the DTFT ofthe signal in Figure 11.11(a) with 
spectra computed from all-pole modeling with two different model orders and using the 
autocorrelation function in Figure 11.11(b). Note that the DTFT of s[n] shows peaks at 
multiples of the fundamental frequency Fo = 8 kHz/27 = 296 Hz, as well as many other 
less prominent peaks and dips that can be attributed to the windowing effects discussed 
in Section 10.2.1. If the first 13 samples of rss[m] in Figure l1.11(b) are used to compute 
an all-pole model spectrum (p = 12), the result is the smooth curve shown with the 
heavy line in Figure 1l.12(a). With the filter order as 12 and the fundamental period of 
27 samples, this spectrum estimate in effect ignores the spectral structure owing to the 
periodicity of the signal and produces a much smoother spectrum estimate. If 41 values 
of rss[m] are used, however, we obtain the spectrum plotted with the thin line. Since 
the period of the signal is 27, a value of p = 40 includes the periodicity peak in the 
autocorrelation function and thus, the all-pole spectrum tends to represent much of the 
fine detail in the DTFT spectrum. Note that both cases support our assertion above that 
the all-pole model spectrum estimate tends to favor good representation at the peaks 
of the DTFT spectrum. 
o 
SO 
100 
ISO 
200 

Chapter 11 
Parametric Signal Modeling
910 
40 
iii' 
2.­
20 
~ 
0 
s 
.~ 
co 
E 
bll -20 
..9 
-40 
-60 o 
500 
1000 
1500 
2000 
2500 
Frequency in kHz 
(a) 
3000 
3500 
4000 
~I~ 0.5 
II 
~ 
5 
10 
15 
20 
25 
30 
35 
40 
P 
(b) 
Figure 11.12 (a) Comparison of DTFT and all-pole model spectra for voiced 
speech segment in Figure 11.11 (a). (b) Normalized prediction error as a function 
of p. 
This example illustrates that the choice of the model order p controls the degree 
of smoothing of the DTFf spectrum. Figure 11.12(b) shows that as p increases, the 
mean-squared prediction error decreases quickly and then levels off, as in our previous 
example. Recall that in Sections 11.2.4 and 11.2.5, we argued that the all-pole model 
with appropriately chosen gain results in a match between the autocorrelation func­
tions of the signal and the all-pole model up to p correlation lags as in Eq. (11.39). 
This implies that as p increases, the aU-pole model spectrum will approach the DTFf 
spectrum, and when p -+ co, it follows that rhh[m] = rss[m] for all m, and therefore, 
IH(ej<J)12 = IS(e jW)1 2• However, this does not mean that H(e jW) 
S(e jW ) because 
H (z) is an IIR system, and S(z) is the z-transform of a finite-length sequence. Also note 
that as p -+ 00, the averaged prediction error does not approach zero, even though 
IH(e jW)1 2 -+ IS(e jW)1 2. As we have discussed, this occurs because the total error in 
Eq. (11.11) is the prediction error e[n] minus Gv[nJ. Said differently, the linear pre­
dictor must always predict the first nonzero sample from the zero-valued samples that 
precede it. 

911 
Section 11.5 
All-Pole Spectrum Analysis 
0.1 ':--------,------,--------,-----------, 
-0.1 o 
50 
100 
150 
200 
Sample number (8 kHz sampling rate) 
(a) 
0.1 ,---,----,--....,---------r-------,----­
o 
-0.1 
150 
200
o  12 
40 50 
100 
Lag number (8 kHz sampling rate) 
(b) 
Figure 11.13 
(a) Windowed unvoiced speech waveform. (b) Corresponding au­
tocorrelation function (samples connected by straight lines). 
The other main class of speech sounds is comprised ofthe unvoiced sounds such as 
fricatives. These sounds are produced by creating random turbulent air flow in the vocal 
tract; therefore, they are best modeled in terms of an all-pole system excited by white 
noise. Figure 11.13 shows an example of a 201-point Hamming-windowed segment of 
unvoiced speech and its corresponding autocorrelation function. Note that the autocor­
relation function shows no indication of periodicity in either the signal waveform or the 
autocorrelation function. A comparison of the DTFT of the signal in Figure l1.13(a) 
with two all-pole model spectra computed from the autocorrelation function in Fig­
ure l1.13(b) is shown in Figure 11.14(a). From the point of view of spectrum analysis of 
random signals, the magnitude-squared ofthe DTFT is a periodogram. Thus, it contains 
a component that is randomly varying with frequency. Again, by choice of the model 
order, the periodogram can be smoothed to any desired degree. 
11.5.2 Pole Locations 
In speech processing, the poles of the all-pole model have a close relationship to the res­
onance frequencies of the vocal tract, thus, it is often useful to factor the polynomial A (z) 
to obtain its zeros for representation as in Eq. (11.50). As discussed in Section 11.3.3, 
the zeros Zk of the prediction error filter are the poles of the all-pole model system 
function. It is the poles of the system function that are responsible for the peaks in the 
spectrum estimates discussed in Section 11.5.1. The closer a pole is to the unit circle, 
the more peaked is the spectrum for frequencies close to the angle of the pole. 

912 
Chapter 11 
Parametric Signal Modeling 
20 
10 
~ 
0 
~ 
<;) 
-10 
"0 
:::I 
.~ -20 
til e 
OIl -30
..9 
-40 
-50 
-60 
0 
500 
1000 
1500 
2000 
2500 
3000 
3500 
4000 
Frequency in kHz 
(a) 
0 
5 
10 
15 
20 
25 
30 
35 
40 
p 
(b) 
Figure 11.14 (a) Comparison of DTFT and ali-pole model spectra for unvoiced 
speech segment in Figure 11.13(a). (b) Normalized prediction error as afunction 
of p. 
1 
<;;)
~c 
II 
0.5 
:§;
;::,. 
0 
Figure 11.15 shows the zeros of the prediction error system function A(z) (poles 
of the model system) for the two spectrum estimates in Figure 11.12(a). For p == 12, the 
zeros of A (z) are denoted by the open circles. Five complex conjugate pairs of zeros are 
close to the unit circle, and their manifestations as poles are clearly evident in heavy 
line curve of Figure 11.12(a). For the case p = 40, the zeros of A(z) are denoted by 
the large filled dots. Observe that most of the zeros are close to the unit circle, and 
they are more or less evenly distributed around the unit circle. This produces the peaks 
in the model spectrum that are spaced approximately at multiples of the normalized 
radian frequency corresponding to the fundamental frequency of the speech signal; i.e., 
at angles 2](296 Hz)/8 kHz. 

913 
Section 11.5 
All-Pole Spectrum Analysis 
o p = 12 
• p =40 
1 
.... 
0.5 
k 
OJ 
~ 
0 
-0.5 
-1 
o • 
o • 
Figure 11.15 Zeros of prediction error 
filters (poles of model systems) used to 
-1 
-0.5 
0 
0.5 
1 
obtain the spectrum estimates in 
Real Part 
Figure 11.12. 
11.5.3 All-Pole Modeling of Sinusoidal Signals 
As another important example, we consider the use of the poles of an all-pole model to 
estimate frequencies of sinusoidal signals. To see why this is possible, consider the sum 
of two sinusoids 
(11.61) 
The z-transform of s[n] has the form 
bo + blZ-1 + b2Z-2 + b3Z-3 
(11.62)
S(z) 
That is, the sum of two sinusoids can be represented as the impulse response of an LTI 
system whose system function has both poles and zeros. The numerator polynomial 
would be a somewhat complicated function of the amplitudes, frequencies, and phase 
shifts. What is important for our discussion is that the numerator is a 3rd-order poly­
nomial and the denominator is a 4th -order polynomial, the roots of which are all on 
the unit circle at angles equal to 
and ±W2.The difference equation describing this 
system with impulse excitation has the form 
3 
s[n] 
k] =  Lbk8[n - k] 
(11.63) 
k=l 
where the coefficients ak would result from multiplying the denominator factors. Note 
that 
4 
s[nJ  
Laks[n 
k]=O forn2:4, 
(11.64) 
k=l 
which suggests that the signal s [n] can be predicted with no error by a 4 th-order predictor 
except at the very beginning (0 S n S 3). The coefficients of the denominator can be 

Chapter 11 
Parametric Signal Modeling
914 
50 
I 
.....~..... 
I 
I  
~
,---.,----r---.~=1
... 
"  
Q)  
. , " "  n·
-0 
I 
~. 
T! • 
r 
T"
.E' 
.! i.... " 
'" 
"", ,1 !
1 o~·';ltir·;t~··ll~l~f:n.;)' 'lJI"rr • 
-50 L 
! 
__ .M~__ .M~_____~ __ • __ ~_t-____~__.__ ~__.__~~____~~__.___~-, 
o  
10 
20 
30 
40 
50 
60 
70 
80 
90 
100 
Time indexn 
(a) Sinusoidal signal  
120  
100
ill 
-0 
.5 
80 
Q) 
-0 Z 
.~ 60 
<Il 6 
40 
t>/j 
0 
...:l 
20 
0 
0.15  
0.2 
0.22  
Frequency wlr.  
(b) Spectrum estimates 
Figure 11.16 Spectrum estimation for asinusoidal signal. 
estimated from the signal by applying the covariance method to a short segment of the 
signal selected so as not to include the first four samples. In the ideal case for which 
Eq. (11.61) accurately represents the signal (e.g., high SNR), the roots of the resulting 
polynomial provide good estimates of the frequencies of the component sinusoids. 
Figure 11.16(a) shows a plot of 101 samples of the signal8 
s[n] 
20cos(O.2:rrn - O.b) + 22 cos (O.22:rrn + O.9.rr). 
(11.65) 
Because the two frequencies are close together, it is necessary to use a large number of 
samples to resolve the two frequencies by Fourier analysis. However, since the signal fits 
the aU-pole model perfectly, the covariance method can be used to obtain very accurate 
estimates of the frequencies from very short segments of the signal. This is illustrated 
in Figure 1l.16(b). 
The DTFf of the 101 samples (with rectangular window) shows no indication that 
there are two distinct sinusoid frequencies around OJ = 0.21:rr. Recall that the main lobe 
width for an (M + I)-point rectangular window is ~w = 4.rr/(M + 1). Consequently, a 
101-point rectangular window can clearly resolve two frequencies only if they are no 
closer than about .04:rr rad/s. Correspondingly, the DTFT does not show two spectral 
peaks. 
Similarly, use of the autocorrelation method results in the spectrum estimate 
shown by the heavy line. This estimate also contains only one spectral peak. The predic­
8The tapering of the segment of the signal in Figure ll.l6(a) is not a result of windowing. It is caused 
by the "beating" of the two cosines ofnearly the same frequency. The period ofthe beat frequency (difference 
between 0.22lT and 0.2lT) is 100 samples. 
DTFf 
Autocorrelation method 
0.25  
0.3 

915 
Section 11.6 
Solution of the Autocorrelation Normal Equations 
tion error polynomial (in factored form) obtained with the autocorrelation method is 
Aa(z) 
(1 - 0.998ej0.21rrz-1)(1- 0.998e-jO.21rr Z-1) 
. (1 - 0.426z-1)(1 
0.1165z-1)  
(11.66) 
The two real poles contribute no peaks, and the complex poles are close to the unit circle, 
but at ±0.211r, which is halfway between the two frequencies. Thus, the windowing 
inherent in the autocorrelation method causes the resulting model to lock onto the 
average frequency 0.2hr. 
On the other hand, the factored prediction error polynomial obtained with the 
covariance method is (with rounding of the magnitudes and angles) given by 
Ac(z) 
(1 
ejO.21fz-l)(1_e-jo·2ll"z-1) 
. (1 
ejO.22ll" z-1)(1- e-jO.221fz-l). 
(11.67) 
In this case, the angles of the zeros are almost exactly equal to the frequencies of the 
two sinusoids. Also shown in Figure l1.16(b) is the frequency response of the model, 
i.e., 
(11.68)  
plotted in dB. In this case, the prediction error is very close to zero, which, if used 
to estimate the gain of the all-pole model, would lead to an indeterminant estimate. 
Therefore, the gain is arbitrarily set to one, which leads to a plot of 
(11.68) on a 
similar scale to the other estimates. Since the poles are almost exactly on the unit circle, 
the magnitude spectrum becomes exceedingly large at the pole frequencies. Note that 
the roots ofthe prediction error polynomial give an accurate estimate of the frequencies. 
This method, of course, does not provide accurate information about the amplitudes 
and phases of the sinusoidal components. 
11.6  SOLUTION OF THE AUTOCORRELATION NORMAL 
EOUATIONS 
In both the autocorrelation and covariance methods of computing the correlation val­
ues, the predictor coefficients that minimize the mean-squared inverse filter error and 
equivalently the prediction error satisfy a set of linear equations of the general form: 
4>ss[1,1] 4>ss [1. 2] 4>ss [1,3] ... 4>ss[l, p] 
4>ss[2, 1] 4>ss [2.2] 4>ss [2,3] ... 4>ss [2, p] 
4>ssl3, 1] 4>ss [3,2] 4>ss [3, 3] ... 4>ss [3, p] 
4>ss[p, 1] 4>ss[p, 2] 4>ss[p, 3] ... 4>ss[P. p] 
a1 
a2 
a3 
a p 
4>ss [1, 0] 
4>ss [2,0] 
4>ss [3,0] 
(11.69) 
4>ss[p, O] 

916 
Chapter 11 
Parametric Signal Modeling 
In matrix notation, these linear equations have the representation 
.a 
1/1. 
(11.70) 
Since ¢[i, k] 
¢[k, i], in both the autocorrelation and covariance methods, the matrix 
• is symmetric, and, because it arises in a least-squares problem, it is also positive­
definite, which guarantees tbat it is invertible. In general, this leads to efficient solu­
tion methods, such as the Cbolesky decomposition (see Press, et aL, 2007), that are 
based on matrix factorization and applicable when. is symmetric and positive definite. 
However, in the specific case of the autocorrelation method or any method for which 
¢ss[i, k] 
rss[li - kl]' Eqs. (11.69) become the autocorrelation normal equations (also 
referred to as the Yule-Walker equations). 
rss[O] 
rss [1] 
rss [2) 
rss[p 
1] 
rss[l] 
rss[l] 
rss [0] 
r~s[1] 
rsslp - 2] 
Gl 
rssl2] 
rss[2] 
rss[l] 
r" [0] 
rss [p 
3] 
G2 
r~s [3]
G3 
(11.71) 
rsslp 
l]rss[p 
2]rss[p-3]··· 
rss[O] JLapJ 
Lrss[p] 
In this case, in addition to the matrix. being symmetric and positive-definite, it is also a 
Toeplitz matrix, i.e., all the elements on each subdiagonal are equal. This property leads 
to an efficient algorithm, referred to as the Levinson-Durbin recursion, for solving the 
equations. 
11 .6.1 The Levinson-Durbin Recursion 
The Levinson-Durbin algorithm for computing the predictor coefficients that minimize 
the total-squared prediction error results from the high degree of symmetry in the matrix 
• and furthermore, as Eg. (11.71) confirms, the elements of the right-hand side vector 
1/1 are primarily the same values that populate the matrix •. Equations (L-D.l) to 
(L-D.6) in Figure 11.17 define the computations. A derivation of these equations is 
given in Section 11.6.2, but before developing the details of the derivation, it is helpful 
to simply examine the steps of the algorithm. 
(I..-D.1) 1bis step initializes the mean-squared prediction error to be the energy of 
the signal. That is, a zeroth-order predictor (no predictor) yields no reduction in 
prediction error energy, since the prediction error ern] is identical to the signal 
s[n]. 
The next line in Figure 11.17 states that steps (L-D.2) through (L-D.5) are repeated p 
times, with each repetition of those steps increasing the order of the predictor by one. In 
other words, the algorithm computes a predictor of order i from the predictor of order 
i-I starting with i-I = O. 
(I..-D.2) This step computes a quantity ki. The sequence ofparameters ki, iI, 2, ... , P 
which we refer to as the k-parameters, plays a key role in generating the next set 
of predictor coefficients.9 
9For reasons to be discussed in Section 11.7, the k-parameters are also called PARCOR (for PARtial 
CORrelation) coeffiCients or also, reflection coefficients. 

917 
Section 11.6 
Solution of the Autocorrelation Normal Equations 
Levinson-Dnrbin Algorithm 
£(0) 
rss [0] 
(L-D.l) 
for i = 1,2, ... , p 
k; = (rdi] 
i:aY-1)rss[i - Jl) I£(i-l) (L-D.2) 
;=1 
a/C) 
ki 
(L-D.3) 
if i > 1 then for j = 1,2, ... , i-I 
(i) 
. (i-l) 
k 
(i-I)
= 
-
;ai _ 
(L-DA)
a j 
aj 
j 
end 
£(i) = (1 - kt)£U-1) 
(L-D.5) 
end  
aj = ajP) 
j = 1, 2, ... , p 
(L-D.6)  
Figure 11.17 Equations defining the Levinson-Durbin algorithm. 
(L-D.3) This equation states that aii), the ith coefficient of the ith-order predictor, is 
equal to k;. 
(L-DA) In this equation, k; is used to compute the remaining coefficients of the ith_ 
order predictor as a combination of the coefficients of the predictor of order 
(i 
1) with those same coefficients in reverse order. 
(L-D.S) This equation updates the prediction error for the ith-order predictor. 
(L-D.6) This is the final step where the pth-order predictor is defined to be the result 
after p iterations of the algorithm. 
The Levinson-Durbin algorithm is valuable because it is an efficient method of 
solution of the autocorrelation normal equations and also for the insight that it provides 
about the properties of linear prediction and all-pole models. For example, from Eq. 
(L-D.5), it can be shown that the averaged prediction error for a pth-order predictor is 
the product of the prediction errors for all lower-order predictors, from which it follows 
that 0 < £(i) :::: £(i-I) < £(p) and 
p 
p 
£(p) = £(0) n(1 - kh = rss [0] n(1 
(11.72) 
;=1 
;=1 
Since £(i) > 0, it must be true that -1 < ki < 1 for i = 1, 2, ... , p. That is, the 
k-parameters are strictly less than one in magnitude. 
11.6.2 Derivation of the Levinson-Durbin Algorithm 
From Eq. (11.30), the optimum predictor coefficients satisfy the set of equations 
p 
rss[i] - Lakrss[i 
kl 
0 
1,2, ... , p, 
(11.73a) 
k=1 

918 
Chapter 11 
Parametric Signal Modeling 
and the minimum mean-squared prediction error is given by 
p 
r,s[O] - I:>krss[k] 
£(p). 
(l1.73b) 
k=l 
Since Eq. (l1.73b) contains the same correlation values as in Eq. (l1.73a), it is possible 
to take them together and write a new set of p + 1 equations that are satisfied by 
the p unknown predictor coefficients and the corresponding unknown mean-squared 
prediction error £(p). These equations have the matrix form 
rss[O] 
rss [1] 
rss[2] 
rss [p] 
r,s[l] 
rss[O] 
rss[l] 
... rss[p -1] 
rss[2] 
rss [1] 
r,,[O] 
... rss[p 
2] 
rdp] rss[p - 1] rss[p - 2] .. , 
rss[O] 
1 
a(p) 
-
tp) 
-a2 
-a(p)
-
p 
£(p) 
o 
o 
(11.74) 
o 
It is this set of equations that can be solved recursively by the Levinson-Durbin al­
gorithm. This is done by successively incorporating a new correlation value at each 
iteration and solving for the next higher-order predictor in terms of the new correlation 
value and the previously found predictor. 
For any order i, the set of equations in Eq. (11.74) can be represented in matrix 
notation as 
R(i)a(i) 
e(i), 
(11.75) 
We wish to show how the i th solution can be derived from the (i _l)st solution. In other 
words, given a(i-l), the solution to R(i-l)a(i-l) = e(i-l), we wish to derive the solution 
to R (i) a(i) = e(i), 
First, write the equations RU-l)a(i-l) 
e(i-l) in expanded form as 
1
rss[O] 
rssll] 
rss[2] 
... r,,[i 
1] 
_a(i-l)
rss[l] 
r,s[O] 
rss[l] 
... r,,[i 
2] 
rss[2] 
rss 11] 
r,s[O] 
,., rdi -3] 
_ah-1)
2 
= 
rss[i 
1] rss[i - 2] rss[i - 3] . , . 
rss[O] 
I I -ai~-;.l) I 
£0-1) 
o 
o 
(11.76) 
0 
Then append a 0 to the vector a(i-l) and multiply by the matrix R(i) to obtain 
r,s[O] 
rss[l] 
rss [2] 
rss[i] 
rss[l] 
r,s [0] 
rss[l] 
rss [i 
1] 
rss[2] 
rss[l] 
rss[O] 
. " rssCi - 2] 
rssCi 
1] rss[i - 2] rss[i - 3] , , , 
rss[1] 
rss[i] 
rssCi -1] r5s [i 
2]", 
rss[O] 
where, to satisfy Eq, (11.77), 
i-I 
1 
_a(i-l) 
I 
_aU-I) 
2 
(i-I)
-a _
i 1 
o 
£(i-l) 
o 
o 
(11.77) 
o 
yei-I ) 
(i-I) 
[']" (i-I) 
[-. 
']
Y 
= r58 I 
- L-- a j 
rS8 I -
J . 
(11.78) 
j=1 

919 
Section 11.6 
Solution of the Autocorrelation Normal Equations 
It is in Eq. (11.78) that the new autocorrelation value rss[i] is introduced. However, 
Eq. (11.77) is not yet in the desired form R(i)a(i) 
e(i). The key step in the derivation is 
to recognize that due to the special symmetry of the Toeplitz matrix R (i), the equations 
can be written in reverse order (first equation last and last equation first, and so on) and 
the matrix for the resulting set of equations is still R (i); i.e., 
rsslO] 
rss[1] 
rss[2] 
rss[i] 
rss [1] 
rss[O] 
rss[1] 
, .. rss[i-1] 
rss[2] 
rss[1] 
rss[O] 
. " rss[i - 2] 
rss[i - 1] rssli - 2] rss[i - 3] , .. 
rss[i] 
rss[i 
1] rss[i - 2] ... 
o 
(i-1)
-a _
i
1 
(i-1) 
-ai­ 2 
(i-I)
-aj 
1 
Now Eq. (11.77) is combined with Eq. (11.79) according to 
1 
(i-I)
-a1 (i-1)
-a2 
(i-I)
-ai ­ 1 
o 
-ki 
0 
(i-I)
-a _
i I 
(1-1)
-a _
i 2 
(i-I)
-a1 
1 
£(i-l) 
o o 
o 
yO-I) 
y(i-l) 
o o 
(11.79) 
o 
£(i-1) 
y(i-l) 
o o 
(11.80) 
o 
£(i-l) 
Equation (11.80) is now approaching the desired form R(i)aU) = e(i). All that remains 
is to choose y (i -1) , so that the right hand vector has only a single nonzero entry. This 
requires that 
i-I 
[ '] 
,,(i-I) 
[.
rss I 
-
~aj 
rssl 
j] 
j=1 
(11.81) 
which ensures cancelation of the last element of the right hand side vector, and causes 
the first element to be 
(11.82) 
With this choice of y(i -1), it follows that the vector of ith-order prediction coefficients 
is 
1 
-atil 
(i) 
-a2 
(i)
-ai_1
0)
-ai 
1 
(i-I)
-at 
(i-I)
-a2 
(i-I)
-ai _ 1 
0 
0 
- ki 
(11.83) 
(I -1)
-a1 
1 
From Eq. (11.83), we can write the set of equations for updating the coefficients as 
j = 1,2, ... , i-I, 
(11.84a) 

920 
Chapter 11 
Parametric Signal Modeling 
and 
a(i) = ki. 
(11.84b)
t 
Equations (11.81), (11.84b). (11.84a), and (11.82) are the key equations ofthe Levinson­
Durbin algorithm. They correspond to Eqs. (L-D.2), (L-D.3), (L-D.4), and (L-D.5) in 
Figure 11.17, which shows how they are used order-recursively to compute the optimum 
prediction coefficients as well as the corresponding mean-squared prediction errors and 
coefficients ki for all linear predictors up to order p. 
1 1.7 !AlTICE FILTER$ 
Among the many interesting and useful concepts that emerge from the Levinson­
Durbin algorithm is its interpretation in terms of the lattice structures introduced in 
Section 6.6. There, we showed that any FIR filter with system function of the form 
M 
A(z) = 1 -
L>~kZ-k 
(11.85) 
k=l 
can be implemented by a lattice structure as depicted in Figure 6.37. Furthermore, we 
showed that the coefficients of the FIR system function are related to the k-parameters 
of a corresponding lattice filter by a recursion given in Figure 6.38, which is repeated 
for convenience in the bottom half of Figure 11.18. By reversing the steps in the k­
to-a algorithm, we obtained an algorithm given in Figure 6.39 for computing the k­
parameters from thecoefficientsaj ,j 
1,2, ... , M. Thus, there isa unique relationship 
between the coefficients of the direct form representation and the lattice representation 
of an FIR filter. 
In this chapter, we have shown that a pth-order prediction error fIlter is an FIR 
filter with system function 
p 
A(p)(z) = 1 
"a(P),,-k 
~ 
k 
..... 
, 
k=l 
whose coefficients can be computed from the autocorrelation function of a signal 
through a process that we have called the Levinson-Durbin algorithm. A by-product 
of the Levinson-Durbin computation is a set of parameters that we have also denoted 
ki and called the k-parameters. A comparison of the two algorithms in Figure 11.18 
shows that their steps are identical except for one important detaiL In the algorithm 
derived in Chapter 6, we started with the lattice filter with known coefficients k; and 
derived the recursion for obtaining the coefficients of the corresponding direct form 
FIR fIlter. In the Levinson-Durbin algorithm, we begin with the autocorrelation func­
tion of a signal and compute the k-parameters recursively as an intermediate result in 
computing the coefficients of the FIR prediction error filter. Since both algorithms give 
a unique result after p iterations, and since there is a unique relationship between the 
k-parameters and the coefficients of an FIR filter, it follows that if M = p and aj = aj 
for j = 1,2, ... , p, the k-parameters produced by the Levinson-Durbin algorithm must 
be the k-parameters of a lattice fIlter implementation of the FIR prediction error filter 
A(p) (z). 

921 
Section 11.7 
Lattice Filters 
Levinson-Durbin Algorithm 
£(0) 
rss[O] 
for i = 1, 2, ... , P 
ki = (rss[i] - I:aY-l)rss [i - n) /£U-l) Eq. (11.81) 
J=1 
(i)
ai 
k; 
Eq. (11.84b) 
if i > 1 then for j = 1,2, ... , i 
1 
a(i) = aU-I) - kja(i-l) 
Eq. (11.84a) 
J} 
I-} 
end 
£(i) 
(1 - kt)£U-1) 
Eq. (11.82) 
end 
1,2, ... , p 
Lattice k-to-a Algorithm 
Given kl, k2, ... , kM 
for i = 1,2, ... , M 
a}il = ki 
Eq. (6.66b) 
if i > 1 then for j 
1,2, ... , i - 1 
(i) 
(i-I) 
k 
(i-I) 
E (666)
end 
a j 
_ j 
. a
jai
q. 
end 
1,2, ... ,M 
Eq. (6.68b) 
Figure 11.18 Comparison of the Levinson-Durbin algorithm and the algorithm 
for converting from k-parameters of alattice structure to the FIR impulse response 
coefficients in EQ. (11.85). 
11 .7.1 Prediction Error Lattice Network 
To explore the lattice filter interpretation further, suppose that we have an jth-order 
prediction error system function 
i 
A(i)(Z) = 1 -
I>~i)Z-k. 
(11.86) 
k=1 
The z-transforrn representation of the prediction errorlO would be 
E(i)(z) = A(i)(z)S(z), 
(11.87) 
lOThe z-transform equations are used assuming that the z-transforms of ern] and s{n] exist. Although 
this would not be true for random signals, the relationships between the variables remain in effect for the 
system. The z-transform notation facilitates the development of these relationships. 

922 
Chapter 11 
Parametric Signal Modeling 
and the time-domain difference equation for this FIR filter is 
i 
e(i)[n] 
s[n] - I>ki)s[n - k]. 
(11.88) 
k=l 
The sequence e(i)[n] is given the more specific name forward prediction error 
because it is the error in predicting s[n1from i previous samples. 
The source of the lattice filter interpretation is Eqs. (l1.84a) and (l1.84b), which, 
if substituted into Eq. (11.86), yield the following relation between A(i) (z) and A(i-ll(z): 
A(i)(Z) = A(i-l)(Z) - kiZ- i A(i-l)(z-l). 
(11.89) 
This is not a surprising result if we consider the matrix representation ofthe polynomial 
A(i)(z) in Eq. (11.83).11 Now, ifEq. (11.89) is substituted for A(i)(Z) in Eq. (11.87), the 
result is 
E(i)(z) = AU-1)(z)S(z) - kiZ-i A(i-l)(z-l)S(z). 
(11.90) 
The first term in Eq. (11.90) is E(i-l)(z), i.e., the prediction error for an (i 
l)st-order 
filter. The second term has a similar interpretation, if we define 
E(i)(z) = 
A(i)(z-l)S(z) = B(i)(z)S(z), 
(11.91) 
where we have defined B(i) (z) as 
B(i)(z) 
C i A(i)(z-l) 
(11.92) 
The time-domain interpretation of Eq. (11.91) is 
i 
e(i)[n] 
s[n 
iJ - I>ki)s[n - i +k]. 
(11.93) 
k=l 
The sequence e(i)[n] is called the backward prediction error, since Eq. (11.93) suggests 
i
that s[n - i] is "predicted" (using coefficients ak » from the i samples that follow sample 
n - i. 
With these definitions, it follows from Eq. (11.90) that 
E(i)(z) = E(i-l)(Z) - kiZ-1E(i-1)(z). 
(11.94) 
and hence, 
e(i)[n] = e(i-l)[n] 
kie(i-l)[n 
1]. 
(11.95) 
By substituting Eq. (11.89) into Eq. (11.91), we obtain 
E(i)(z) = z-1 E(i-l)(z) 
ki E(i-l)(z), 
(11.96) 
which, in the time domain, corresponds to 
e(i)[n] = e(i-l)[n -1] 
kieU-1)[n]. 
(11.97) 
1!'The algebraic manipulations to derive this result are suggested as an exercise in Problem 11.21. 

923 
Section 11.7 
Lattice Filters 
e(i-1)[n1 
e(i)[ n ] 
Figure 11.19 Signal flow graph of 
e(i)[n] 
prediction error computation. 
The difference equations in Eq. (11.95) and Eq. (11.97) express the jth-order 
forward and backward prediction errors in terms ofki and the (j _1)st -order forward and 
backward prediction errors. This pair of difference equations is represented by the flow 
graph of Figure 11.19. Therefore, Figure 11.19 represents a pair of difference equations 
that embody one iteration of the Levinson-Durbin recursion. As in the Levinson­
Durbin recursion, we start with a zeroth-order predictor for which 
(11.98) 
With e(O)[n] 
s[n] and e(O)[n) = sIn] as inputs to a first stage as depicted in Figure 11.19 
with k[ as coefficient, we obtain e(l)[n] and e(1)[n] as outputs. These are the required 
inputs for stage 2. We can use p successive stages of the structure in Figure 11.19 to 
build up a system whose output will be the desired pth-order prediction error signal 
ern] = e(p)[n]. Such a system, as depicted in Figure 11.20, is identical to the lattice 
network in Figure 6.37 of Section 6.6.12 In summary, Figure 11.20 is a signal flow graph 
representation of the equations 
e(O)[n] = e(O)[nl = sIn] 
(11.99a) 
e{i)[n] = e(i-l)[n] 
kieU-1)[n 
1] 
i = 1,2, ...• p 
(11.99b) 
eU)[n] = e(i-l)[n 
1] - kje{i-l)[n] i = 1. 2, ... , p 
(11.99c) 
efn] = e(p)[n], 
(11.99d) 
where, if the coefficients kj are determined by the Levinson-Durbin recursion, the 
variables eU)[n] and e(i)[n] are the forward and backward prediction errors for the ith_ 
order optimum predictor. 
11.7.2 All-Pole Model Lattice Network 
In Section 6.6.2, we showed that the lattice network of Figure 6.42 is an implementation 
ofthe all-pole system function H(z) = 1/A(z), where A(z) is the system function of an 
FIR system; i.e., H(z) is the exact inverse of A(z), and in the present context, it is the 
system function of the all-pole model with G = 1. In this section, we review the all-pole 
lattice structure in terms of the notation of forward and backward prediction error. 
12Note that in Figure 6.37 the node variables were denoted a(i)[n] and b(i)[n] instead of e<i)[n] and 
e(i)[n], respectively. 

--
924 
Chapter 11 
Parametric Signal Modeling 
c 1 
C(OJ[nJ 
e{l)[n] 
e(2)[n] 
C(pl[nJ 
sIn] 
e(O)[n] 
e(2)fn] 
Figure 11.20 Signal flow graph of lattice network implementation of pth-order 
prediction error computation. 
ern] = e(P)[n] 
e(p-l)[n] 
e(P-2)[n] 
s[n] 
Z-l 
e(PJ[n] 
e(P-ll[n] 
e(1)[n] 
e(O)[n] 
Figure 11.21 
All-pole lattice system. 
If we replace the node variable labels at;) [n] and b(i) [n] in Figure 6.42 with the cor­
responding eli) In] and e(i)[n] we obtain the flow graph of Figure 11.21, which represents 
the set of equations 
e(p)[n] = ern] 
(11.1ooa) 
e(i-l)[n] 
e(i)[n] + kie(i-l)[n 
1] 
i 
p, p 
1, ... ,1 
(11.100b) 
e(iJrn] = e(i-l)[n -1] - kjeU-1)[n] 
i = p, p -1, ... ,1 
(11.10Oc) 
s[n] = e(O)[n] = e(O) [n]. 
(11.100d) 
As we discussed in Section 6.6.2, any stable all-pole system can be implemented 
by a lattice structure such as Figure 11.21. For such systems, the guarantee of stability 
inherent in the condition Ikj I < 1 is particularly important. Even though the lattice 
structure requires twice the number of multiplications per output sample as the direct 
form, it may be the preferred implementation when coefficients must be coarsely quan­
tized. The frequency response ofthe direct form is exceedingly sensitive to quantization 
of the coefficients. Furthermore, we have seen that high-order direct form IIR systems 
can become unstable owing to quantization of their coefficients. This is not the case for 
the lattice network, as long as the condition Ikd < 1 is maintained for the quantized 
k-parameters. Furthermore, the frequency response of the lattice network is relatively 
insensitive to quantization of the k-parameters. 

925 
Section 11.7 
Lattice Filters 
11.7.3 Direct Computation of the k-Parameters 
The structure of the flow graph in Figure 11.20 is a direct consequence of the Levinson­
Durbin recursion, and the parameters ki' i = 1, 2, ... , p can be obtained from the 
autocorrelation values r,~s[m], m = 0,1, ... , p through iterations of the algorithm of 
Figure 11.17. From our discussion so far, the ki parameters have been an ancillary 
consequence ofcomputing the predictor parameters. However, Itakura and Saito (1968, 
1970), showed that the ki parameters can be computed directly from the forward and 
backward prediction errors in Figure 11.20. And because of the iterative structure as a 
cascade of the stages in Figure 11.19, the ki parameters can be computed sequentially 
from signals available from previous stages of the lattice. The direct computation of the 
parameter ki is achieved with the following equation: 
L 
00 
e(i-l)[n]e(i-l)[n 
1] 
11=-00
eI 
(11.101) 
Observe that Eq. (11.101) is in the form of the energy-normalized cross-correlation 
between the forward and backward prediction errors at the output of the ith stage. For 
this reason kr computed using Eq. (11.101) is called a PARCOR coefficient, or more 
precisely PARtial CORrelation coefficient. Figure 11.20 has the interpretation that the 
correlation in s[n] represented by the autocorrelation function rss[m] is removed step­
by-step by the lattice filter. For a more detailed discussion of the concept of partial 
correlation, see Stoica and Moses (2005) or Markel and Gray (1976). 
Equation (11.101) for computing kr is the geometric mean between a value k{ 
that minimizes the mean-squared forward prediction error and a value k? that minimizes 
the mean-squared backward prediction error. Ine derivation ofthis result is considered 
in Problem 11.28. Note that we have shown the limits on the sums as infinite simply to 
emphasize that all error samples are involved in the sum. To be more specific, all the 
sums in Eq. (11.1Ot) could start at n = 0 and end at n = M + i, since this is the 
range over which the error signal output of both the forward and backward jth-order 
predictors would be nonzero. This is the same assumption that was made in setting up the 
autocorrelation method for finite-length sequences. Indeed, Problem 11.29 outlines a 
proof that kr computed by Eq. (11.101) gives identically the same result as ki computed 
by Eq. (11.81) or Eq. (L-D.2) in Figure 11.17. Therefore, Eq. (11.101) can be substituted 
for Eq. (L-D.2) in Figure 11.17, and the resulting set of prediction coefficients will be 
identical to those computed from the autocorrelation function. 
To use Eq. (11.1Ot), it is necessary to actually compute the forward and backward 
prediction errors by employing the computations of Figure 11.19. In summary, the fol­
lowing steps result in computation of the PARCOR coefficients kr for i = 1,2, ... , p: 
PARCOR.O Initialize with e(O)[n] = e{O)[n] 
s[n] for 0 S n sM. 
For i = 1,2, ... , p repeat the following steps. 

926 
Chapter 11 
Parametric Signal Modeling 
PARCOR.l Compute e(i)[n] and e(i-l)[n] using Eq. (11.99b) and Eq. (11.99c) respec­
tively for 0 S n S M + i. Save these two sequences as input for the next stage. 
PARCOR.2 Compute k; using Eq. (11.101). 
Another approach to computing the coefficients in Figure 11.20 was introduced 
by Burg, 1975, who formulated the all-pole modeling problem in terms of the maximum 
entropy principle. He proposed to use the structure of Figure 11.20, which embodies the 
Levinson-Durbin algorithm, with coefficients kf that minimize the sum of the mean­
squared forward and backward prediction errors at the output of each stage. The result 
is given by the equation 
N 
2 I>(i-1)[n]e(i-1)[n 
1] 
kB 
I 
N 
fI=i 
N 
(11.102) 
L(e(i-l) [n])2 + L(e(i-l)[n - 1])2 
n=i 
n=i 
The procedure for using this equation to obtain the sequence kf, i 
1, 2, .... p is the 
B
same as the PARCOR method. In statement PARCOR.2, k; is simply replaced by kj 
from Eq. (11.102). In this case, the averaging operator is the same as in the covariance 
.method, which means that very short segments of s[n] can be used, while maintaining 
high spectral resolution. 
Even though the Burg method uses a covariance-type analysis, the condition 
Ikf I < 1 holds, implying that the all-pole model implemented by the lattice filter will be 
stable. (See Problem 11.30.) Just as in the case of the PARCOR method, Eq. (11.102) 
can be substituted for Eq. (L-D.2) in Figure 11.17 to compute the prediction coefficien ts. 
While the resulting coefficients will differ from those obtained from the autocorrela­
tion function or from Eq. (11.101), the resulting all-pole model will still be stable. The 
derivation of Eq. (11.102) is the subject of Problem 11.30. 
11.8 SUMMARY 
This chapter provides an introduction to parametric signal modeling. We have em­
phasized all-pole models, but many of the concepts discussed apply to more general 
techniques involving rational system functions. We have shown that the parameters of 
an all-pole model can be computed by a two-step process. The first step is the compu­
tation of correlation values from a finite-length signal. The second step is solving a set 
of linear equations, where the correlation values comprise the coefficients. We showed 
that the solutions obtained depend on how the correlation values are computed, and 
we showed that if the correlation values are true autocorrelation values, a particularly 
useful algorithm, called the Levinson-Durbin algorithm, can be derived for the solu­
tion of the equations. Furthermore, the structure of the Levinson-Durbin algorithm 
was shown to illuminate many useful properties of the all-pole model. The subject of 
parametric signal modeling has a rich history, a voluminous literature, and abundant 
applications, all of which make it a subject worthy of further advanced study. 

927 
Chapter 11 
Problems 
Problems 
Basic Problems 
11.1. sIn] is a finite-energy signal known for all n. ¢ss[i. kJ is defined as 
¢di, k] = L
00 
sIn 
i]s[n - k]. 
n=-oo 
Show that ¢ss[i, k] can be expressed as a function of Ii 
kl. 
11.2. In general, the mean-squared prediction error is defined in Eq. (11.36) as 
(P11.2-1) 
(a)  Expand Eq. (Pll.2-1) and use the fact that (s[n - i]s[n 
k]) = ¢ss[i. k] = ¢ss[k, i] 
to show that 
p 
p 
p 
t:  
¢ss[O, 0] 
2 Lak¢ss[O,k] + Lai Lak¢ss[i,k] 
(Pl1.2-2) 
k=l 
i=l 
k=l 
(b)  Show that for the optimum predictor coefficients, which satisfy Eqs. (11.20), Eq. (PI 1.2­
2) becomes 
p 
t: = ¢ss[O. OJ L ak¢ss[O, k], 
(Pl1.2-3) 
k=l 
lL3.  The impulse response of a causal all-pole model of the form of Figure 11.1 and Eq. (11.3) 
with system parameters G and {ad satisfies the difference equation 
p 
h[n] L akh[n - k] + Go[n] 
(Pl1.3-1) 
k=l 
(a)  The autocorrelation function of the impulse response of the system is 
rhh[m] = L
00 
h[n]h[n + m] 
n=-(X) 
By substituting Eq. (P11.3-1) into the equation for rhh[-m], and using the fact that 
rhh[-m] = rhh[m] show that 
pL akrhh[lm 
klJ = rhh[m], 
m = 1,2, ... , p 
(PI1.3-2) 
k=l 
(b)  Using the same approach as in (a), now show that 
p 
rhh[O] 
Lakrhh[k] 
G2. 
(PI1.3-3) 
k=l 

928  
Chapter 11 
Parametric Signal Modeling 
11.4.  Consider a signal x[n] = s[n] + w[n], where s[n] satisfies the difference equation 
s[n] = O.8s[n - 1] + v[n]. 
v[n] is a zero-mean white-noise sequence with variance o} = 0.49 and w[n] is a zero-mean 
white-noise sequence with variance O'~ = 1. The processes v[n] and w[n] are uncorrelated. 
Determine the autocorrelation sequences cPss[m] and cPxx[m]. 
11.5.  The inverse filter approach to all-pole modeling of a deterministic signal s[n] is discussed 
in Section 11.1.2 and depicted in Fig. 11.2. The system function of the inverse filter is given 
in Eq. (11.5). 
(a)  Based on this approach, determine the coefficients a1 and a2 of the best all-pole model 
for s[n] 
8[n] + 8[n 
2] with p 
2. 
(b)  Again, based on this approach, determine the coefficients al, a2 and a3 of the best 
all-pole model for s[n] = 8[n] + 8[n - 2] with p = 3. 
lL6.  Suppose that you have computed the parameters G and ab k 
1,2, ... , p of the aU-pole 
model 
G 
H(z) 
p 
1- L 
b=l 
Explain how you might use the OFf to evaluate the all-pole spectrum estimate IH(ejwk)1 
at N frequencies (Uk = 21fk/N for k = 0, 1, ... , N - 1. 
11.7.  Consider a desired causal impulse response hd[n] that we wish to approximate by a system 
having impulse response h[n] and system function 
H(z) = 
b 
1­
Our optimality criterion is to minimize the error function given by 
00 
E =  L(hd[n] - hln])2. 
n=O 
(a)  Suppose a is given, and we wish to determine the unknown parameter b which mini­
mizes E. Assume that lal < 1. Does this result in a nonlinear set of equations? If so, 
show why. If not, determine b. 
(b)  Suppose b is given, and we wish to determine the unknown parameter a which mini­
mizes E. Is this a nonlinear problem? If so, show why. If not, determine a. 
lL8.  Assume that s[n] is a finite-length (windowed) sequence that is zero outside the interval 
o ::5 n ::5 M 
1. The pth-order backward linear prediction error sequence for this signal is 
defined as 
p 
ern] = s[n] - L fhs[n + k] 
k=1 
That is, s[n] is "predicted" from the p samples that follow sample n. The mean-squared 
backward prediction error is defined as 
2 
00 
00 
p 
!!  
m~oo(e[m])2 = m~oo s[m]-EPks[m + k]
(  
) 

929 
Chapter 11 
Problems 
where the infinite limits indicate that the sum is over all nonzero values of (e[m])Z as in 
the autocorrelation method used in "forward prediction." 
(a)  The prediction error sequence ern] is zero outside a finite interval N1 :s n :s Nz. 
Determine Nl and NZ. 
(b)  Following the approach used in this chapter to derive the forward linear predictor, 
derive the set of normal equations that are satisfied by the f3ks that minimize the 
mean-squared prediction error t. Give your final answer in a concise, well-defined 
form in terms of autocorrelation values. 
(c)  Based on the result in part (b), describe how the backward predictor coefficients lf3k} 
related to the forward predictor coefficients {O'k}? 
Advanced Problems 
11.9. Consider a signal sIn] that we model as the impulse response of an all-pole system of 
order p. Denote the system function of the pth-order all-pole model as H(p) (z) and the 
corresponding impulse response as h(p)[n]. Denote the inverse of H(p)(z) as Hi<:V)(z) = 
1/H(p) (z). The corresponding impulse response is hi:; [n]. The inverse filter, characterized 
by hi~:[n], is chosen to minimize the total squared error &(p) given by 
00 
&(p) 
L 
[,s[n] 
g(P)[n]t, 
n=-oo 
where g(p)[n] is the output of the filter Hi<:V) (z) when the input is sIn). 
(a)  Figure PlL9 depicts a signal flow graph of the lattice filter implementation of Hi~~ (z). 
Determine h~~[n the impulse response at n = 1. 
(b)  Suppose we now wish to model the signal sIn] as the impulse response of a 2nd-order 
all-pole filter. Draw a signal flow graph of the lattice filter implementation of Hi~J (z). 
(c)  Determine the system function H(2) (z) of the 2nd-order all-pole filter. 
sIn] 
Figure P11.9 
11.10. Consider an ith-order predictor with prediction error system function 
i 
i 
A(i)(z) 1- LaY)z-) I1 (1­
(PI LlO-l) 
)=1 
)=1  
From the Levinson-Durbin recursion, it follows that aji) = kj. Use this fact with  
Eq. (PlLlO-I) to show that if Ikil ~ 1, it must be true that Izj)1 ~ 1 for some j. That  

930  
Chapter 11 
Parametric Signal Modeling 
is, show that the condition Ikj I < 1 is a necessary condition for A (p) (z) to have all its zeros 
strictly inside the unit circle. 
U11.  Consider an LTI system with system function H(z) = hO + hIZ-1• The signal yEn] is the 
output of this system due to an input that is white noise with zero mean and unit variance. 
(8)  What is the autocorrelation function ryy[m] of the output signal yEn]? 
(b)  The 2nd-order forward prediction error is defined as 
ern) 
yEn) - aIy[n 
1) - a2y[n 
2). 
Without using the Yule-Walker equations directly, find al and a2, such that the vari­
ance of ern] is minimized. 
(c)  The backward prediction filter for yEn] is defined as 
ern] = y[n] - burn + 1] 
b2y[n + 2]. 
Find bi and b2 such that the variance of ern] is minimized. Compare these coefficients 
to those determined in part (b). 
11.12.  (8) The autocorrelation function, ryy[m] of a zero-mean wide-sense stationary random 
process yEn] is given. In terms of ryy[m], write the Yule-Walker equations that result 
from modeling the random process as the response to a white noise sequence of a 
3fd -order all-pole model with system function 
A 
H(z) = 
. 
1-ael -be3 
(b)  A random process v[n] is the output of the system shown in Figure PI1.12-l, where 
x[n] and zEn] are independent, unit variance, zero mean, white noise signals, and 
hEn] = SEn -1] + is[n 
2). Find rvv[m], the autocorrelation of v[n]. 
Figure P11.12-1 
(c)  Random process YI[n] is the output of the system shown in Figure Pl1.12-2, where 
x[n] and zEn] are independent, unit variance, zero-mean, white noise signals, and 
1 
Hl(Z) = 
I 
3'
I-ae 
be 
The same a and b as found in part (a) are used for all-pole modeling of Yl[n]. The 
inverse modeling error, WI En], is the output of the system shown in Figure PI1.I2-3. 
Is wI[n] white? Is wI[n] zero mean? Explain. 
Figure P11.12-2 
1 - az-1 - bz-3
4 
~] 
Figure P11.12-3 
(d) What is the variance of wl[n]? 

931 
Chapter 11 
Problems 
11.13.  We have observed the first six samples of a causal signal s[n] given by s[O] = 4, s[l] = 8, 
s[2] 
4, s[3] 
2, s[4J 
1, and s[5] 
0.5. For the first parts of this problem, we will 
model the signal using a stable, causal, minimum-phase, two-pole system having impulse 
response s[n] and system function 
G
H (z) = -----:;----",.
1 
The approach is to minimize the modeling error £ given by 
5 
£ = 
min L (g[n] -
G~[n])2 , 
a],az,A n=O 
where g[n] is the response ofthe inverse system to s[n], and the inverse system has system 
function 
(a)  Write g[n] -
G~[n] for 0:::: n 
5, 
(b)  Based on your work in part (a), write the linear equations for the desired parameters 
al, a2, and G. 
(c)  WhatisG? 
(d)  For this s[n], without solving the linear equations in part (b), discuss whether you 
expect that the modeling error £ will be zero, 
For the rest of this problem, we will model the signal using a different stable, causal, 
minimum-phase system having impulse response 82[n] and system function 
bo +blZ-1 
H2(Z) = 
-1 ' 
1-az 
The modeling error to be minimized in this case is 
given by 
5 
£2 = min L (g[n] - r[n])2 , 
a,bo,b! n=O 
where g[n] is the response of the inverse system to s[n], and the inverse system now has 
system function 
A(z) 
1­
Furthermore, r[n] is the impulse response of a system with system function 
B(z) = bo + blZ-l , 
(e) For this model, write g[n] 
r[n] for 0:::: n :::: 5, 
(f) Calculate the parameter values a, bo, and bi that minimize the modeling error, 
(g) Calculate the modeling error £2 in part (f), 
11.14.  In Example 11.1, we considered the sequence sd[n] = anu[n], which is the impulse re­
sponse of a 1st-order all-pole system having system function 
1 
H(z) = -1---:­
In this problem we consider the estimation of the parameters of an all-pole model for the 
signal sd[n] known only over the interval 0 :::: n :::: M, 

932  
Chapter 11 
Parametric Signal Modeling 
(a)  First, consider the estimation of a 1st-order model by the autocorrelation method. 
To begin, show that the autocorrelation function of the finite-length sequence s[n] = 
sd[nj(u[n] 
urn 
M -1]) = an(u[nJ -
urn 
M -
1]) is 
11 - a 2(M-lml+l)
rss[m] = aim, 
.  
(PI1.14-1)
1­
(b)  Use the autocorrelation function determined in (a) in Eq. (11.34), and solve for the 
coefficient a1 of the 1st-order predictor. 
(c)  You should find that the result obtained in (b) is not the exact value (i.e., a1 -I ex) 
as obtained in Example 11.1, when the autocorrelation function was computed using 
the infinite sequence. Show, however, that al -+ a for M -+ 00. 
(d)  Use the results of (a) and (b) in Eq. (11.38) to determine the minimum mean-squared 
prediction error for this example. Show that for M -+ 00 the error approaches the 
minimum mean-squared error found in Example 11.1 for the exact autocorrelation 
function. 
(e)  Now, consider the covariance method for estimating the correlation function. Show 
that for p = 1, tP5s(i, kJ in Eq. (11.49) is given by 
1_a2M 
o:s (i. k) :s 1. 
(Pl1.14-2)
tPss[i,kJ 
(f)  Use the result of (e) in Eq. (11.20) to solve for the coefficient of the optimum 1 st-order 
predictor. Compare your result to the result in (b) and to the result in Example 11.1. 
(g)  Use the results of (e) and (f) in Eq. (11.37) to find the minimum mean-squared predic­
tion error. Compare your result to the result in (d) and to the result in Example 11.1. 
11.15. Consider the signal 
n 
S[nJ=30r U[n]+4(-D urn]. 
(a) We want to use a causal, 2nd-order all-pole model, i.e., a model of the form 
A
If(z) =  -----,,------:;c 
1 - al 
-
a2 
to optimally represent the signal s[n], in the least-square error sense. Find a1, a2, and 
A. 
(b)  Now, suppose we want to use a causal, 3rd -order all-pole model, i.e., a model of the 
form 
B 
H(z) 
to optimally represent the signal s[n], in the least-square error sense. Find, bl, b2, b3, 
and B. 
11.16. Consider the signal 
s[n]=20ru[n]+3(-Dn u[n1. 
(Pl1.l6-1) 
We wish to model this signal using a 2nd -order (p = 2) all-pole model or, equivalently, 
using 2nd-order linear prediction. 
For this problem, since we are given an analytical expression for s[n] ands[nJ is the impulse 
response of an all-pole filter, we can obtain the linear prediction coefficients directly from 

933 
Chapter 11 
Problems 
the z-transform of sen]. (You are asked to do this in part (a).) In practical situations, we 
are typically given data, i.e., a set of signal values, and not an analytical expression. In this 
case, even when the signal to be modeled is the impulse response of an all-pole filter, we 
need to perform some computation on the data, using methods such as those discussed in 
Section 11.3, to determine the linear prediction coefficients. 
There are also situations in which an analytical expression is available for the signal, but 
the signal is not the impulse response of an all-pole filter, and we would like to model it 
as such. In this case, we again need to carry out computations such as those discussed in 
Section 11.3. 
(a)  For sen] as given in Eq. (Pl1.16-1), determine the linear prediction coefficients ai. a2 
directly from the z-transform of sen]. 
(b)  Write the normal equations for p 
2 to obtain equations for aI, a2 in terms of rss em]. 
(c)  Determine the values of rss[O] , rss[1], and rssl21 for the signal sen] given in 
Eq. (P11.l6-1). 
(d)  Solve the system of equations from part (a) using the values you found in part (b) to 
obtain values for the aks. 
(e)  Are the values of ak from part (c) what you would expect for this signal? Justify your 
answer clearly. 
(f)  Suppose you wish to model the signal now with p = 3. Write the normal equations 
for this case. 
(g)  Find the value of rssf3]. 
(h)  Solve for the values of ak when p = 3. 
(i)  Are the values of ak found in part (h) what you would expect given s[n]? Justify your 
answer clearly. 
(j)  Would the values ofal, a2 you found in (h) change if we model the signal with p 
4? 
11.17.  x[n] and yen] are sample sequences of jointly wide-sense stationary, zero-mean random 
processes. The following information is known about the autocorrelation function rPxx [mI 
and cross correlation rPyx[m]: 
rPxx[m] = { 0 
modd 
m even 
rPyx[-l] 
2 
rPyx[O] = 3 
rPyx[l] = 8 
rPyx[2] =-3 
rPyx[3] = 2 
rPyx[4] 
-0.75 
(a)  The linear estimate of y given x is denoted Yx. It is designed to minimize 
e = E (I yen] 
Yx[n lI2 ). 
(Pll.17-1) 
where the ydn] is formed by processing x [n] with an FIRfilterwhose impulse response 
hen] is of length 3 and is given by 
h[n] = has[n] + hlo[n - 1] + h2o[n 
2]. 
Determine ha, hI, and 112 to minimize e. 
(b)  In this pan, Yx, the linear estimate of y given x, is again designed to minimize e in 
Eq. (Pll.17-1), but with different assumptions on the structure of the linear filter. 
Here the estimate is formed by processing x[nl with an FIR filter whose impulse 
response g[n] is of length 2 and is given by 
g[n] 
glo[n-l]+g2o[n 
2]. 
Determine the gl and g2 to minimize e. 

934  
Chapter 11 
Parametric Signal Modeling 
(c)  The signal,x[n] can be modeled as the output from a two-pole filter H(z)whose input 
is w[n), a wide-sense stationary, zero-mean, unit-variance white-noise signal. 
H(z) 
1 
Determine al and a2 based on the least-squares inverse model in Section 11.1.2. 
(d)  We want to implement the system shown in Figure P11.17 where the coefficients ai 
are from all-pole modeling in part (c) and the coefficients hi are the values of the 
impulse response of the linear estimator in part (a). Draw an implementation that 
minimizes the total cost of delays, where the cost of each individual delay is weighted 
linearly by its clock rate. 
h 
hl  h
0
w[n] I 
2 n 
12 
Iyx[njl 
IYx[2n]
P
1- Lakz-k 
I 
I 
) n
k-l 
0 
1 
2 
Figure P11.17 
(e)  Let fa be the cost in part (a) and let Eb be the cost in part (b), where each f is defined 
as in Eq. (Pl1.17-1). Is fa larger than, equal to, or smaller than Eb, or is there not 
enough information to compare them? 
(f)  Calculate fa and fb when <Pyy [0] = 88. (Hint: The optimum FIR filters calculated in 
parts (a) and (b) are such that E [h[n](y[n] - y.>;[nD] 
0.) 
11.18.  A discrete-time communication channel with impulse response h[n] is to be compensated 
for with an LTI system with impulse response hc[n] as indicated in Figure P1l.18. The 
channel h[nJ is known to be a one-sample delay. i.e., 
h[n] =  8[n - 1]. 
The compensator heln] is an N -point causal FIR filter, i.e., 
N-I 
k
Hc(z) = L ag- . 
k=O 
The compensator hc[n] is designed to invert (or compensate for) the channel. Specifically, 
hcfn] is designed so that with s[n] = 8[n], S[n] is as "close" as possible to an impulse; i.e., 
hcfn] is designed so that the error 
00 
E 
L Is[n]- 8[n]12 
11=-00 
is minimized. Find the optimal compensator of length N, i.e., determine aO, aI, ... , aN-I 
to minimize E. 
channel 
compensator 
s[n]  -l 
h[n] H 
hAn] r- :5[n] 
Figure P11.18 

935 
Chapter 11 
Problems 
11.19.  A speech signal was sampled with a sampling rate of 8 kHz. A 300-sample segment was se­
lected from a vowel sound and multiplied by a Hamming window as shown in Figure Pl1.19. 
From this signal a set of linear predictors 
i 
p(i)(z) = I>ki)z-k, 
k==l 
with orders ranging from i 
1 to i = 11 was computed using the autocorrelation method. 
This set of predictors is shown in Table 11.1 below in a form suggestive of the Levinson­
Durbin recursion. 
Windowed Segment of Vowel Sound 
0.3 
------~---·------.·--·---T---·---- ,..----,.----. 
0.2 
0.1 
o~~~~~~~~~~~~~~~~~~~~~~~~~-~~ 
--D.1 
--D.2 
-0.3 
f 
--D.4L---..----.~~-------~~.--.----~~-.~..----~.-.-.----~~------~ 
o  
50 
100 
150 
200 
250 
300 
Sample index n 
Figure P11.19 
(a)  Determine the z-transform A (4) (z) of the 4th-order prediction error filter. Draw and 
label the flow graph of the direct form implementation of this system. 
(b)  Determine the set of k-parameters {k1' k2. k3. k4} for the 4th-order prediction error 
lattice filter. Draw and label the flow graph of the lattice implementation of this 
system. 
(c)  The minimum mean-squared prediction error for the 2nd-order predictor is E(2) = 
0.5803. What is the minimum mean-squared prediction error for the 3rd-order predic­
tor? What is the total energy ofthe signal s[n]? What is the value of the autocorrelation 
function rss[l]? 
TABLE 11.1 
PREDICTION COEFFICIENTS FOR ASET OF LINEAR PREDICTORS 
I i 
(i)
a1 
(i)
a2 
(i)
a3 
(i)
a4 
(i) 
as 
(i)
a6 
(i)
a7 
(i)
a8 
(i)
a9 
(i)
a lO 
Ji)l
all 
1 0.8328 
2 0.7459 
0.1044 
3 0.7273 -0.0289 0.1786 
4 0.8047 -0.0414 0.4940 -0.4337 
5 0.7623 
0.0069 0.4899 -0.3550 -0.0978 
6 0.6889 -0.2595 0.8576 -0.3498 
0.4743 -0.7505 
7 0.6839 -0.2563 0.8553 -0.3440 ' 0.4726 -0.7459 -0.0067 
8 0.6834 -0.3095 0.8890 -0.3685 
0.5336 -0.7642 
0.0421 -0.0713 
9 0.7234 -0.3331 1.3173 -0.6676 
0.7402 -1.2624 
0.2155 -0.4544 0.5605 
10 0.6493 -0.2730 1.2888 -0.5007 
0.6423 -1.1741 
0.0413 -0.4103 0.4648 0.1323 
11 0.6444 -0.2902 1.3040 i -0.5022 
0.6859 -1.1980 
0.0599 -0.4582 0.4749 0.1081 0.0371 

I I 
i 
936 
Chapter 11 
Parametric Signal Modeling 
(d)  The minimum mean-squared prediction errors for these predictors form a sequence 
(E(O), E(1) , E(2) , ... , E(11)}. It can be shown that this sequence decreases abruptly 
in going from i 
0 to i = 1 and then decreases slowly for several orders and then 
makes a sharp decrease. At what order i would you expect this to occur? 
(e)  Sketch carefully the prediction error sequence e(ll)[n] for the given input s[n] in 
Figure Pl1.19. Show as much detail as possible. 
(f) The system function of the 11th-order all-pole model is 
G 
G 
G 
H(z) = 
11 
11 
1 _ '" a(ll) -k n(1- ZiZ-1)
L....-
k 
z 
k=1 
;=1 
The following are five of the roots of the 11th-order prediction error filter A(ll) (z). 
i I IZil I LZi (rad) I 
1 
0.2567 
2.0677 
2 
0.9681 
. 1.4402 
3 
0.9850 
0.2750 
4 
0.8647 
2.0036 
5 
0.9590 
2.4162 
State briefly in words where the other six zeros of A(11) (z) are located. Be as precise 
as possible. 
(g)  Use information given in the table and in part (c) of this problem to determine the 
gain parameter G for the 11th-order all-pole model. 
(h)  Carefully sketch and label a plot of the frequency response of the 11th-order all-pole 
model filter for analog frequencies 0 ::: F ::: 4 kHz. 
11.20. Spectrum analysis is often applied to signals comprised of sinusoids. Sinusoidal signals 
are particularly interesting, because they share properties with both deterministic and 
random signals. On the one hand, we can describe them in terms of a simple equation. On 
the other hand, they have infinite energy, so we often characterize them in terms of their 
average power, just as with random signals. This problem explores some theoretical issues 
in modeling sinusoidal signals from the point of view of random signals. 
We can consider sinusoidal signals as stationary random signals by assuming that 
the signal model is s[n] = A cos(won + 9) for -00 < n < 00, where both A and 9 can 
be considered to be random variables. In this model, the signal is considered to be an 
ensemble of sinusoids described by underlying probability laws for A and 9. For simplicity, 
assume that A is a constant, and 9 is a random variable that is uniformly distributed over 
0:::9 <2Jr. 
(a)  Show that the autocorrelation function for such a signal is 
A2 
rss[m] 
E{s[n +m]s[n]} = 2 cos(wOm}. 
(Pl1.20-1) 
(b) Using Eq. (11.34), write the set of equations that is satisfied by the coefficients of a 
2nd-order linear predictor for this signal. 
(c)  Solve the equations in (b) for the optimum predictor coefficients. Your answer should 
be a function of wo. 
(d) Factor the polynomial A(z) = 1 
-azz-Z describing the prediction error filter. 
(e)  Use Eq. (11.37) to determine an expression for the minimum mean-squared pre­
diction error. Your answer should confirm why random sinusoidal signals are called 
"predictable" and/or "deterministic." 

937 
Chapter 11 
Problems 
11.21. Using Eqs. (l1.84a) and (1l.84b) from the Levinson-Durbin recursion, derive the relation 
between the ith and i-Ist prediction error filters given in Eq. (11.89). 
11.22. In this problem, we consider the construction of lattice filters to implement the inverse 
filter for the signal 
(a)  Find the values of the k-parameters kl and k2 for the 2nd-order case (Le., p 
2). 
(b) Draw the signal flow graph of a lattice filter implementation of the inverse filter, i.e., 
the filter that outputs y[nl 
A8[n] (a scaled impulse) when the input x[nl = s[n]. 
(c)  Verify that the signal flow graph you drew in part (b) has the correct impulse response 
by showing that the z-transform of this inverse filter is indeed proportional to the 
inverse of S(z). 
(d) Draw the signal flow graph for a lattice filter that implements an aU-pole system such 
that when the input is X[II] 
8[n], the output is the signal s[n] given above. 
(e)  Derive the system function of the signal flow graph you drew in part (d) and demon­
strate that its impulse response h[n] satisfies h[n] = s[n]. 
11.23. Consider the signal 
S[II]=aGr uln]+fl(~r urn] 
where a and fl are constants. We wish to linearly predict s[nl from its past p values using 
thc relationship 
p 
s[nl 
I>ks[n - k] 
k=l 
where the coefficients ak are constants. The coefficients ak are chosen to minimize the 
prediction error 
oc 
£, 
L (s[n]-8[n])2. 
n=-oc 
(a)  With rsslm] denoting the autocorrelation function of s[n], write the equations for the 
case p = 2 the solution to which will result in at> a2. 
(b) Determine a pair of values for a and fl such that when p = 2, the solution to the 
normal equations is al = Hand a2 = - g. Is your answer unique? Explain. 
(c)  If a = 8 and fl = 
determine k-parameter k3, resulting from using the Levinson 
recursion to solve the normal equations for p = 3. Is that different from k3 when 
solving for p = 4? 
11.24. Consider the following Yule-Walker equations: f pap = y P' where: 
[¢~ll ]
Yp
ap{~]  
4>[p] 
and 
[ ¢[Ol 
... 4>[p ll]
fp 
(a Toeplitz matrix) 
<P[p:- 11 
4>[0] 

r 
Ii 
938 
Chapter 11 
Parametric Signal Modeling 
The Levinson-Durbin algorithm provides the following recursive solution for the normal 
equation r p+l up+l = r pH: 
aP+1 
p+l 
<1>[p+I]- (rtf Up 
T 
<1>[0] 
(rp) Up 
aP+1 
m 
p 
am -
pH 
p 
a p+1 . a p _m+1 
m 
1, ... , p 
where rt is the backward version of rp: rt = [<1>[p] . .. <1> [l]]T ,and aI 
:~bl. Note that 
for vectors, the model order is shown in the subscript; but for scalars, the model order is 
shown in the superscript. 
Now consider the following normal equation: r p b p = cp , where 
b 
p l:{] 
_[c[.,1]]
cp -
: 
c[p] 
Show that the recursive solution for r p+1 bp+1 = Cp+1 is: 
b P+1 
pH 
c[p+I] 
<1>[0] 
{rtf bp 
(rpf Up 
bP+1 -bP -bPH·aP 
1
m 
-
m 
p+l 
p-m+ 
m = 1" .. ,p 
11.25. 
h 
bl 
£l1l 
were 1 
4>[01' 
(Note: You may find it helpful to note that u~ = rplrt.) 
Consider a colored wide-sense stationary random signal s[n] that we desire to whiten using 
the system in Figure PIl.l5-1: In designing the optimal whitening filter for a given order p, 
we pick the coefficient aiP), k = 1, ... , p that satisfy the autocorrelation normal equations 
given by Eq. (11.34), where rss[m] is the autocorrelation of s[n]. 
sIn] 
r----­
.II-~p a 7­k l 
, 
~k=1 k~ 
, 
gIn] 
• 
Figure P11.25-1 
It is known that the optimal lnd-order whitening filter for 
s[nJ 
is 
H () 
1 
1 -1 
(. 
(2) 
1 
(2) 
1 ) 
h' h 
. 
I 
. 
h lnd
2 Z 
+ 4 Z 
-
, I.e., at 
- 4' a2 
8 ' w Ie we Imp ement In t e 
-
order lattice structure in Figure Pl1.l5-l. We would also like to use a 4th-order system, 
with transfer function 
H4(Z) 
1 
4I>i4)z-k. 
k=l 
We implement this system with the lattice structure in Figure PIl.l5-3. Determine which, 
if any of H4(Z), kl' k2' k3' k4 can be exactly determined from the information given above. 
Explain why you cannot determine the remaining, if any, parameters. 

939 
Chapter 11 
Problems 
s[nJ  
g[nJ 
Figure P11.25-2 
Lattice structure for 2nd -order system 
s[nJ  
g[nJ 
Figure P11.25-3 
Lattice structure for 4th-order system 
Extension Problems 
11.26. Consider a stable all-pole model with system function 
G 
G 
H (z) = ----,p,----­
m
1- L amz-
A(z) 
m=l 
Assume that g is positive. 
In this problem, we will show that a set of (p +1) samples of the magnitude-squared 
of H (z) on the unit circle; i.e., 
k = 0, 1, ... , p, 
is sufficient to represent the system. Specifically, given elk], k = 0, 1, ... , p, show that the 
parameters G and am, m = 0, 1, .... p can be determined. 
(a) Consider the z-transform 
1 
A(z)A(z-l) 
Q(z) = H(z)H(c1) = 
G2 
which corresponds to a sequence q[nJ. Determine the relationship between q[nJ and 
hA[n], the impulse response of the prediction error filter whose system function is 
A(z). Over what range of n will q[n] be nonzero? 
(b)  Design a procedure based on the DFT for determining q [n] from the given magnitude­
squared samples C[k]. 
(c)  Assuming that the sequence q[n] as determined in (b) is known, state a procedure 
for determining A(z) and G. 

940 
Chapter 11 
Parametric Signal Modeling 
11.27.  The general IIR lattice system in Figure 1l.21 is restricted to all-pole systems. However, 
both poles and zeros can be implemented by the system of Figure PIl.27-1 (Gray and 
Markel, 1973, 1975). Each of the sections in Figure Pll.27-1 is described by the flow graph 
of Figure Pl1.27-2. In other words, Figure 11.21 is embedded in Figure Pll.27-I with the 
output formed as a linear combination of the backward prediction error sequences. 
x[n] 
e(Pl[n] 
e(P-l)[n] 
e(P-2)[n] 
e(ll[n] 
e(Ol[n] 
~ 
Section 
Section 
Section 
e(P)[n] 
P 
e(P-l)[n] 
P-l 
e(P-2)[n] 
e(1l[n] 
1 
e(O)[n] 
Cp 
Cp_l 
-I-­
-
Cp_ 2 
-
cl 
-
Co 
-
y[n) 
Figure P11.27-1 
e(il[n] 
e(i-l)[n] 
-41 
Z-l 
e(i)[n]  
e(i-ll[n] 
Figure P11.27-2 
(a)  Show that the system function between the input X(z) = E(p)(z) and e(i)(z) is 
e(i)(,) 
-lA(i)( -1)
fl(i)(z) 
__~_ zz 
(P1U7-1) 
X(z) 
A(p)(z) 
(b)  Show that fl(p) (z) is an all-pass system. (This result is not needed for the rest of the 
problem.) 
(c)  The overall system function from X(z) to Y (z) is 
Y( ) 
P 
-lA(i)( -1) 
Q( )
H(z) =  _z_ 
~ ci Z 
z 
= __2_. 
(PI1.27-2) 
X(z) 
A(P)(z) 
A(p)(z)
to 
Show that the numerator Q(z) in Eq. (Pll.27-2) is a pth-orderpolynomial of the form 
P 
Q(z) = L qmz-m 
(Pll.27-3) 
m=O 
where the coefficients Cm in Figure P11.27 are given by the equation 
P 
~  
(i)
cm=qm+ L 
Ci ai_ 
m 
p,p-l, ... ,I,O. 
(Pl1.27-4)
m 
i=m+1 
(d) Give a procedure for computing all the parameters needed to implement a system 
function such as Ea. (P11.27-2) using the lattice structure of Figure PIl.27. 

941 
Chapter 11 
Problems 
(e)  Using the procedure described in (c), draw the complete flow graph of the lattice 
implementation of the system 
1 + 3z-1 + 3z-2 + z-3 
H (z) = ------:;----~----;;: 
(PIL27-5)
1 
+ 
11.28. In Section 11.7.3, the k-parameters are computed by Eqs. (11.101). Using the relations 
e(i)[n] = e(i-l)[n]- ki e(i-1)[n -1] and e(i)[n] =e(i-1)[n -1] 
kie(i-1)[n], show that 
kP = V!kf k~,
1 
I 
I 
where k{ is the value of k; that minimizes the mean-squared forward prediction error 
00 
t:(i) 
L (e(i) [n])2, 
11=-00 
and kf is the value of ki that minimizes the mean-squared backward prediction error 
00 
to) 
L (e(i) [n])2. 
11=-00 
11.29. Substitute Eq, (11.88) and Eq, (11.93) into 
(11.101) to show that 
00 
L  
e(i-1)[n]e(i-1)[n 
1] 
11=-00 
i-I 
[ '] 
,,(i-I) 
['
I"ss I 
-
~ a j 
rss I 
j] 
----''----,,--,----- = ki' 
11.30.  As discussed in Section 11.7.3, Burg (1975) proposed computing the k parameters so as 
to minimize the sum of the forward and backward prediction errors at the ith stage of the 
lattice filter; Le" 
M 
B(i) = L 
[(e(i) [n])2 + (e(i)[n])2] 
(PIUO-l) 
n=i 
where the sum is over the fixed interval i :s n :s M, 
(a)  Substitute the lattice filter signals e(i)[n] 
e(i-l)[n] 
kie(i-l)[n -1] and eU)[n] 
e(i-1)[n -1] - kie(i-1)[n] into (Pl1.30-1) and show thatthe value of ki that minimizes 
B(i) is 
M 
2 L 
e(i-l)[n]eU- 1)[n 
1] 
(Pl1.30-2) 
M 
Hint: Consider the expression L(x[n] ± y[n])2 > 0 where x[n] and )'[n] are two 
distinct sequences, 
(c)  Given a set of Burg coefficients kf, iI, 2, . , . , p, how would you obtain the coeffi­
cients of the corresponding prediction error filter A(p) (z)? 

12 
Discrete Hilbert 
Transforms 
12.0 INTRODUCTION 
In general, the specification of the Fourier transform of a sequence requires complete 
knowledge of both the real and imaginary parts or of the magnitude and phase at 
all frequencies in the range -If < W 
:::: If. However, we have seen that under cer­
tain conditions, there are constraints on the Fourier transform. For example, in Sec­
tion 2.8, we saw that if x[n] is real, then its Fourier transform is conjugate symmetric, 
i.e., X(eJW ) = X *(e - JW ). From this, it follows that for real sequences, specification of 
X (eJW ) for 0 :::: W :::: If also specifies it for -If :::: W :::: O. Similarly, we saw in Section 5.4 
that under the constraint of minimum phase, the Fourier transform magnitude and 
phase are not independent; i.e., specification of magnitude determines the phase and 
specification of phase determines the magnitude to within a scale factor. In Section 8.5, 
we saw that for sequences of finite length N, specification of X (elw) at N equally spaced 
frequencies determines X (eJW) at all frequencies. 
In this chapter, we will see that the constraint of causality of a sequence implies 
unique relationships between the real and imaginary parts of the Fourier transform. 
Relationships of this type between the real and imaginary parts of complex functions 
arise in many fields besides signal processing, and they are commonly known as Hilbert 
transform relationships. In addition to developing these relationships for the Fourier 
transform of causal sequences, we will develop related results for the DFT and for 
sequences with one-sided Fourier transforms. Also, in Section 12.3 we will indicate how 
the relationship between magnitude and phase for minimum-phase sequences can be 
interpreted in terms of the Hilbert transform. 
942 

943 
Section 12.0 
Introduction 
Although we will take an intuitive approach in this chapter (see Gold, Oppenheim 
and Rader, 1970) it is important to be aware that the Hilbert transform relationships fol­
low formally from the properties ofanalytic functions. (See Problem 12.21.) Specifically, 
the complex functions that arise in the mathematical representation of discrete-time sig­
nals and systems are generally very well-behaved functions. With few exceptions, the 
z-transforms that have concerned us have had well-defined regions in which the power 
series is absolutely convergent. Since a power series represents an analytic function 
within its ROC, it follows that z-transforms are analytic functions inside their ROCs. 
By the definition of an analytic function, this means that the z-transform has a well­
defined derivative at every point inside the ROC. Furthermore, for analytic functions 
the z-transform and all its derivatives are continuous functions within the ROC. 
The properties of analytic functions imply some rather powerful constraints on 
the behavior of the z-transform within its ROC. Since the Fourier transform is the z­
transform evaluated on the unit circle, these constraints also restrict the behavior of 
the Fourier transform. One such constraint is that the real and imaginary parts sat­
isfy the Cauchy-Riemann conditions, which relate the partial derivatives of the real 
and imaginary parts of an analytic function. (See, for example, Churchill and Brown, 
1990.) Another constraint is the Cauchy integral theorem, through which the value of 
a complex function is specified everywhere inside a region of analyticity in terms of the 
values of the function on the boundary of the region. On the basis of these relations 
for analytic functions, it is possible, under certain conditions, to derive explicit integral 
relationships between the real and imaginary parts of a z-transform on a closed contour 
within the ROC. In the mathematics literature, these relations are often referred to 
as Poisson sformulas. In the context of system theory, they are known as the Hilbert 
transform relations. 
Rather than following the mathematical approach just discussed, we will develop 
the Hilbert transform relations by exploiting the fact that the real and imaginary parts 
of the Fourier transform of a causal sequence are the transforms of the even and odd 
components, respectively, of the sequence (properties 5 and 6, Table 2.1). As we will 
show, a causal sequence is completely specified by its even part, implying that the Fourier 
transform of the original sequence is completely specified by its real part. In addition 
to applying this argument to specifying the Fourier transform of a particular causal 
sequence in terms of its real part, we can also apply it, under certain conditions, to 
specify the Fourier transform of a sequence in terms of its magnitude. 
The notion of an analytic signal is an important concept in continuous-time signal 
processing. An analytic signal is a complex time function (which is analytic) having a 
Fourier transform that vanishes for negative frequencies. A complex sequence cannot 
be considered in any formal sense to be analytic, since it is a function of an integer 
variable. However, in a style similar to that described in the previous paragraph, it is 
possible to relate the real and imaginary parts of a complex sequence whose spectrum is 
zero on the unit circle for -J( < W < O. A similar approach can also be taken in relating 
the real and imaginary parts of the DFT for a periodic or, equivalently, a finite-length 
sequence. In this case, the "causality" condition is that the periodic sequence be zero in 
the second half of each period. 

944  
Chapter 12 
Discrete Hilbert Transforms 
Thus, in this chapter, a notion of causality will be applied to relate the even and odd 
components of a function or, equivalently, the real and imaginary parts of its transform. 
We will apply this approach in four situations. First, we relate the real and imaginary 
parts of the Fourier transform X(e jW ) of a sequence x[nJ that is zero for n < O. In the 
second situation, we obtain a relationship between the real and imaginary parts of the 
DFf for periodic sequences or, equivalently, for a finite-length sequence considered to 
be of length N, but with the last (N/2) - 1 points restricted to zero. In the third case, 
we relate the real and imaginary parts of the logarithm of the Fourier transform under 
the condition that the inverse transform of the logarithm of the transform is zero for 
n < O. Relating the real and imaginary parts of the logarithm of the Fourier transform 
corresponds to relating the log magnitude and phase of X(e jW). Finally, we relate the 
real and imaginary parts of a complex sequence whose Fourier transform, considered 
as a periodic function of w, is zero in the second half of each period. 
12.1  REAL- AND IMAGINARY-PART SUFFICIENCY OF THE 
FOURIER TRANSFORM FOR CAUSAL SEQUENCES 
Any sequence can be expressed as the sum of an even sequence and an odd sequence. 
Specifically, with xe[nJ and xo[nJ denoting the even and odd parts, respectively, of x[n],l 
we have 
x[nJ = xe[n] + xo[n],  
(12.1) 
where 
x[n] + x[-nJ
Xe[n]  
(12.2)
2 
and 
xo[n] = x[nJ 
xl-n] 
(12.3)
2 
Equations (12.1) to (12.3) apply to an arbitrary sequence, whether or not it is causal 
and whether or not it is real. However, if x[n] is causal, i.e., if x[n] = 0, n < 0, then it 
is possible to recover x[nJ from xe[nJ or to recover x[n] for n i= 0 from xo[n]. Consider, 
for example, the causal sequence x[n] and its even and odd components, as shown in 
Figure 12.1. Because x[n] is causal, x[n] = 0 for n < 0 and x[-n] 
0 for n > O. 
Therefore, the nonzero portions of x[n] and x[-n] do not overlap except at n 
O. For 
this reason, it follows from Eqs. (12.2) and (12.3) that 
x[n]  
2xe[n]u[n] -
xe[0]8[n] 
(12.4) 
and 
x[n] =  2xo[n]u[n] +x[O]8[n]. 
(12.5) 
The validity of these relationships is easily seen in Figure 12.1. Note that x[n] is com­
pletely determined by xe[n]. On the other hand, xo[O] = 0, so we can recover x[n] from 
xo[n] only for n i= O. 
In x[nl is real, then Xe lnl and xolnl in Eqs, (12.2) and (12.3) are the even and odd parts, respectively, 
of x[nl as considered in Chapter 2, If x[nl is complex, for the purposes of this discussion we still define xe[n] 
and xolnl as in Eqs, (12.2) and (12,3), which do not correspond to the conjugate-symmetric and conjugate­
antisymmetric parts of a complex sequence as considered in Chapter 2, 

945 
Section 12.1 
Real- and Imaginary-Part Sufficiency of the Fourier Transform 
x[n] 
• 1II[ I I ....I I I r t
• • • • • • • • 
0 
n 
X [-n] 
T I I I I I [ III•
0 
• • • • • • • • • n 
xe[k] 
, ! t t r I I I J 1I I I I ! t t ! 
t 
0 
n 
xa[k] 
• J I I I !
, , 1 1 1 1 I I I 
0 
! t ! t 
n 
Figure 12.1 
Even and odd parts of a real causal sequence. 
Now, if x[n] is also stable, i.e., absolutely summable, then its Fourier transform 
exists. We denote the Fourier transform of x[n] as 
(12.6)  
where XR(e jW ) is the real part and X/(e jW ) is the imaginary part of X(e JW ). Recall that 
if x[n] is a real sequence, then XR(e jW) is the Fourier transform of xe[n] and j X/(eJW ) 
is the Fourier transform of xo[n]. Therefore, for a causal, stable, real sequence, XR(e jUJ ) 
completely determines X (e jUJ ), since, if we are given X R(ejW ), we can find X (eJW ) by 
the following process: 
L Find xe[n] as the inverse Fourier transform of XR(ejW ). 
2. Find x[n] using Eq. (12.4). 
3. Find X (ejW ) as the Fourier transform of x[n]. 

946 
Chapter 12 
Discrete Hilbert Transforms 
This also implies, of course, that X/ (elm) can be determined from X R(ejW ). In 
Example 12.1, we illustrate how this procedure can be applied to obtain X(elw) and 
X/(e jW) from XR(e jW ). 
Example 12.1 
Finite-Length Sequence 
Consider a real, causal sequence x[n] for which XR(e jW ), the real part of the DTFf, is 
jW
XR(e
) = 1 + cos2w. 
(12.7) 
We would like to determine the original sequence x[n], its Fourier transform X (ejW), 
and the imaginary part of the Fourier transform, X/(ejW). As a first step, we rewrite 
Eq. (12.7) expressing the cosine as a sum of complex exponentials: 
. 
1'2 
1 
"'L. 
XRCeJw) 
1 + Ze-J m + ZeJL.W. 
(12.8) 
We know that XR(e jm) is the Fourier transform of xefn], the even part of x[n] as 
defined in Eq. (12.2). Comparing Eq. (12.8) with the definition of the Fourier transform, 
Eq. (2.131), we can match terms to obtain 
1 
1 
xe[n] = 8[n] + Z8[n 
2] + 28[n + 2]. 
Now that we have obtained the even part, we can use the relation in Eq. (12.4) to 
obtain 
x[nJ 
8[nJ + 8[n - 2]. 
(12.9) 
From x[n]. we get 
jZw
X(ejw) = 1 + e­
= 1 + cos 2w - j sin 2w. 
(12.10) 
From Eq. (12.10), we can both confirm that XR(ejW ) is as specified in Eq. (12.7) and 
also obtain 
X/(e jW ) = sin2w. 
(12.11) 
As an alternative path to obtaining X I (e jW), we can first use Eq. (12.3) to get xo[n] 
from x[n]. Substituting Eq. (12.9) into Eq. (12.3) then yields 
1 
1 
xo[n] = Z8[n 
2]- 28[n + 2]. 
The Fourier transform of xo[n] is j X / (elm), so we find 
. 
1 
'Zw 
1 '2
jX/(eJW) = -e-J 
_ -eJ W 
2 
2 
-jsin2w, 
so that 
X/(elw ) = -sin2w, 
which is consistent with Eq. (12,11). 

947 
Section 12.1 
Real- and Imaginary-Part Sufficiency of the Fourier Transform 
Example 12.2 
Exponential Sequence 
Let 
J'w 
1 
a cos(u
XR(e 
) = ------;::; 
lal < 1, 
(12.12)
1- mcos(U + 
or equivalently, 
lal < 1, 
(12.13) 
with a real. We first determine xelnl and then x[n] using Eq. (12.4). 
To obtain xe[nI, the inverse Fourier transform of XR (eFv), it is convenient to first 
obtain XR(Z), the z-transform of xdnl. This follows directly from Eq. (12.13), given 
that 
XR(ejW ) 
XR(Z)!z=e}w' 
Consequently, by replacing ejw by Z in Eq. (12.13), we obtain 
1- (a/2Hz +z-l)
X R (z) = ----'---.----::­
1- a(z + C 1) +a2 
(12.14) 
1- 1(z+z-1) 
(1-ac1)(1 
az) 
(12.15) 
Since we began with the Fourier transform XR(ejW ) and obtained XR(Z) by 
extending XR(ejW ) into the z-plane, the ROC of XR(Z) must, of course, include the 
unit circle and is then bounded on the inside by the pole at z 
a and on the outside 
by the pole at z = l/a. 
From Eq. (12.15), we now want to obtain Xe [n I, the inverse z-transform of XR(Z), 
We do this by expanding Eq. (12.15) in partial fractions, yielding 
XR(Z) = -1 [1 
+ --
1] , 
(12.16)
2 l-az-1 
1-az 
with the ROC specified to include the unit circle. The inverse z-transform ofEq. (12.16) 
can then be applied separately to each term to obtain 
1 n 
1 -n
xe[nl = la u[nl + 2a 
u[-nl· 
(12.17) 
Consequently, from Eq. (12.4), 
x[n] 
a ll ulnl +a-nu[-n]u[nl ~ o[nl 
allu[nl. 
X(elW) is then given by 
(12.18) 
and X (z) is given by 
1 
X(z) = -1---:;-
Izl> lal· 
(12.19) 

Chapter 12 
Discrete Hilbert Transforms
948 
The constructive procedure illustrated in Example 12.1 can be interpreted analyti­
cally to obtain a general relationship that expresses Xl (eiill ) directly in terms of XR (eiw). 
From Eq. (12.4), the complex convolution theorem, and the fact that xe[O] 
x[O], it 
follows that 
X (eiw ) =  1 flr XR(eJo)U (e j (w-0)d8 
x[O], 
(12.20)
1'( 
-lr 
where U (ejU) is the Fourier transform of the unit step sequence. As stated in Section 2.7, 
although the unit step is neither absolutely summable nor square summable, it can be 
1'( 
represented by the Fourier transform 
U (eiw ) 
L
00 
1'($(w 
1 
21'(k) + -.--;­
(12.21) 
k=-oo 
or, since the term 1/(1 ­ e- jW) can be rewritten as 
1 
1­
= 1 
j 
(W)
2 - z-cot "2 
I 
(12.22) 
Eq. (12.21) becomes 
U(eiw) 
L
00 
1'($(w ­
1 
21'(k) + z­
. 
~ cot (~) . 
(12.23) 
k=-oc 
Using Eq. (12.23), we can express Eq. (12.20) as 
X(e jW ) 
XR(eiw ) + jX[(eiw ) 
, 
1
= XR(e1W ) + -2 flr XR(e1'0 )d8 
(12.24) 
-lr 
j 
j8
- flr XR(e
) cot (w-=-8) de 
x[O].
2][ 
-lr  
2 
Equating real and imaginary parts in Eq. (12.24) and noting that 
1 flr
x[O] = 
XR(e j8 )d8,  
(12.25)
21'( 
-lr 
we obtain the relationship 
1W. 
1 flr 
'0 
(W-8)
XI(e
) = 
21'( 
-lr XR(e1 ) cot -2-
de. 
(12.26) 
A similar procedure can be followed to obtain x[n] and X(e JW ) from XI(eiw ) and x[O] 
using Eq. (12.5). This process results in the following equation for XR(e jW ) in terms of 
Xl(e jW ): 
iw ) 
1 fJl' 
ill 
(we) 
(12.27)
XR(e
x[O] + 21'( 
-lr X/(e
) cot -2-
de. 
Equations (12.26) and (12.27), which are called discrete Hilbert transform rela­
tionships, hold for the real and imaginary parts of the Fourier transform of a causal, 
stable, real sequence. They are improper integrals, since the integrand is singular at 

949 
Section 12.2 
Sufficiency Theorems for Finite-Length Sequences 
-cot(~)
2 
I 
(J 
Figure 12.2 Interpretation of the 
Hilbert transform as a periodic 
convolution. 
w - 0 = O. Such integrals must be evaluated carefully to obtain a consistent finite result. 
This can be done formally by interpreting the integrals as Cauchy principal values. That 
is, Eq. (12.26) becomes 
X[(e jUJ ) = --P 
XR(eFi ) cot --
dO,
1 J7t 
(W 0)
2rc 
-7t 
2 
(12.28a) 
and Eq. (12.27) becomes 
(12.28b) 
where P denotes the Cauchy principal value of the integral that follows. The meaning 
of the Cauchy principal value in Eq. (12.28a), for example, is 
. 
1 
[J7t1J 
(w 0)
X/(e JUJ ) = -- lim 
XR(eJ )cot --
dO 
2rc 8-+0 
UJ+8 
2 
(12.29) 
+ £:-8 XR(ejlJ ) cot (w 2 0) dO] . 
Equation (12.29) shows that X[(e jUJ ) is obtained by the periodic convolution of 
cot(w/2) with X R(e jUJ ), with special care being taken in the vicinity of the singularity 
at 0 
w. In a similar manner, Eq. (12.28b) involves the periodic convolution of cot(w/2) 
with X[Ce jUJ ). 
The two functions involved in the convolution integral of Eq. (12.28a) or, equiva­
lently, Eq. (12.29) are illustrated in Figure 12.2. The limit in Eq. (12.29) exists because 
the function cot[(w 
0)/2] is antisymmetric at the singular point (J = wand the limit is 
taken symmetrically about the singularity. 
12.2  SUFFICIENCY THEOREMS FOR FINITE-LENGTH 
SEQUENCES 
In Section 12.1, we showed that causality or one-sidedness of a real sequence implies 
some strong constraints on the Fourier transform of the sequence. The results of the 

950 
Chapter 12 
Discrete Hilbert Transforms 
previous section apply, of course, to finite-length causal sequences, but since the finite­
length property is more restrictive, it is perhaps reasonable to expect the Fourier trans­
form of a finite-length sequence to be more constrained. We will see that this is indeed 
the case. 
One way to take advantage of the finite-length property is to recall that finite­
length sequences can be represented by the DFf. Since the DFf involves sums rather 
than integrals, the problems associated with improper integrals disappear. 
Since the DFf is, in reality, a representation of a periodic sequence, any results 
we obtain must be based on corresponding results for periodic sequences. Indeed, it 
is important to keep the inherent periodicity of the DFf firmly in mind in deriving 
the desired Hilbert transform relation for finite-length sequences. Therefore, we will 
consider the periodic case first and then discuss the application to the finite-length case. 
Consider a periodic sequence x[n] with period N that is related to a finite-length 
sequence x[n] of length N by 
x[n] = x[((n))N]. 
(12.30) 
As in Section 12.1, x[n] can be represented as the sum of an even and odd periodic 
sequence, 
x[n] = xe[n] +xo[n], 
n = 0, 1, ... , (N - 1), 
(12.31) 
where 
x[n] +x[-n]
xe[n] 
n = 0, 1, ... , (N - 1), 
(12.32a)
2  
and  
x[-n] 
n 
0,1, ... , (N - 1). 
(12.32b)
xo[n] 
A periodic sequence cannot, of course, be causal in the sense used in Section 12.1. 
We can, however, define a "periodically causal" sequence to be a periodic sequence for 
which x[n] 
0 for N/2 < n < N. That is, x[n] is identically zero over the last half of 
the period. We assume henceforth that N is even; the case of N odd is considered in 
Problem 12.25. Note that because of the periodicity of i[n], it is also true that x[n] = 0 
for -N/2 < n < O. For finite-length sequences, this restriction means that although the 
sequence is considered to be of length N, the last (N/2) - 1 points are in fact zero. In 
Figure 12.3, we show an example of a periodically causal sequence and its even and odd 
parts with N 
8. Because i[n] is zero in the second half of each period, x[-n] is zero in 
the first half of each period, and, consequently, except for n = 0 and n = N /2, there is 
no overlap between the nonzero portions of i[n] and i[-n]. Therefore, for periodically 
causal periodic sequences, 
2ie[n], 
n = 1,2, ... , (N/2) - 1, 
x[n] = 
xe[n], 
n 
0, N/2, 
(12.33)
{ 0, 
n=(N/2)+1, ... ,N 
1, 
and 
2xo[n], n:: 1,2, ... , (N/2) 
1,
i[n] 
(12.34)
{ 0, 
n - (N/2) + 1, ... , N 
1, 

951 
Section 12.2 
Sufficiency Theorems for Finite-Length Sequences 
i[n] 
... I . .lrIII • • • II III • • . I 
I III • • •
-N 
0 
N 
n 
x[-n] 
···1111. • • III I I••• III I I• • • III I 
-N 
0 
N 
n 
·r I ! I! I rIrI ! I! I rIrI ! It I I II I r 
iAn] 
-N 
0 
N 
n 
io[n] 
TIl 
TIl 
TIl 
... 
11 '-N 
• 11 ' 0 
• 11 ' N 
• 11 ' 
n 
Figure 12.3 
Even and odd parts of aperiodically causal, real, periodic sequence 
of period N 
8. 
where x[n] cannot be recovered from xo[n] because xo[O] = xo[N/2] = O. If we define 
the periodic sequence 
1, 
n=0,N/2, 
uN[n] = 
2, 
n = 1,2, ... , (N/2) -1, 
(12.35)
{ 0, 
n 
(N/2) + 1, ... , N - 1. 
then it follows that, for N even, we can express x[n] as 
x[n] = xe[n]uN[n] 
(12.36) 
and 
x[n] = xo[n]uN[n] + x[0]8[n] + x[N/2]8[n - (N/2)], 
(12.37) 
where 8[n] is a periodic unit-impulse sequence with period N. Thus, the sequence x[n] 
can be completely recovered from xe[n]. On the other hand, .to[n] will always be zero at 
n = 0 and n = N /2, and consequently, x[n] can be recovered from xo[n] only for n :j:: 0 
andn :j:: N/2. 
Ifx[n] is areal periodic sequence of period N withDFS X[k], then XR[k], the real 
part of X[k], is the DFS ofi'e£n] and jXIlk) is the DFS ofxoln]. Hence, Eqs. (12.36) and 
(12.37) imply that, for a periodic sequence of period N, which is periodically causal in 
the sense defined earlier, X[k] can be recovered from its real part or (almost) from its 
imaginary part. Equivalently, XI[k] can be obtained from XR[k], and XR[k] can (almost) 
be obtained from XI[k). 

952  
Chapter 12 
Discrete Hilbert Transforms 
Specifically, suppose that we are given XR[k]. Then, we can obtain X[k] and XI[k] 
by the following procedure: 
1. Compute xe[n] using the DFS synthesis equation 
1 N-l 
xe[n] 
N L 
XR[k]ej (21f/N)kn. 
(12.38) 
k=O 
2. Compute x[n] using Eq. (12.36). 
3.  Compute X[k] using the DFS analysis equation 
N-l 
X[k]  
L 
x[n]e-j (21f/N)kn = XR[k] + jXI[k]. 
(12.39) 
n=O 
In contrast to the general causal case discussed in Section 12.1, the procedure just 
outlined can be implemented on a computer, since Eqs. (12.38) and (12.39) can be 
evaluated accurately and efficiently using an FFf algorithm. 
To obtain an explicit relation between XR[k], and XI[k], we can carry out the 
procedure analytically. From Eq. (12.36) and Eq. (8.34), it follows that 
X[k] = XR[k] + jX/[k] 
1 N-l 
(12.40) 
N L 
XR[m]UN[k - m]; 
m=O 
i.e., X[k] is the periodic convolution of XR[k], the DFS of xe[n], with U N[k] the DFS of 
uN[n]. The DFS of uN[n] can be shown to be (see Problem 12.24) 
N, 
k 
0, 
UN[k] = 
-j2cot(nk/N), 
kodd, 
(12.41) 
0, 
k even.
I 
If we define 
VN[k] = {-j2cot(nk/N), 
k odd, 
(12.42)
0, 
k even, 
then Eq. (12.40) can be expressed as 
N-l 
X[k]  
XR[k] + ~ L 
XR[m]VN[k - ml 
(12.43) 
m=O 
Therefore, 
N-l 
jX/[k] = ~ L 
XR[m]VN[k 
m),  
(12.44) 
m=O 
which is the desired relation between the real and imaginary parts of the DFS of a 
periodically causal, real, and periodic sequence. Similarly, beginning with Eq. (12.37) 
we can show that 
N-l 
XR[k) = ~ LjX/[m]VN[k 
m]+x[OJ+(-1)kx [N/2]. 
(12.45) 
m=O 

Section 12.2 
Sufficiency Theorems for Finite-Length Sequences  
953 
Equations (12.44) and (12.45) relate the real and imaginary parts of the DFS rep­
resentation of the periodic sequence i[nJ.1t i [nJis thought of as the periodic repetition 
of a finite-length sequence x[nJ as in Eq. (12.30), then 
- {i[n],  O:s n :s N -1, 
(12.46)
x n 
[ J-
0 
h' 
, 
ot erwlse. 
If x[n] has the "periodic causality" property with respect to a period N (i.e., x[nJ = 0 
for n < 0 and for n > N /2), then all of the preceding discussion applies to the DFf of 
x[n]. In other words, we can remove the tildes from Eqs. (12.44) and (12.45), thereby 
obtaining the DFf relations 
1, 
(12.47) 
otherwise, 
and 
XR[k] =  
~ EjXI[mJVNlk-mJ+xIOJ+(-1)kXIN/2J, O:sk:sN 
1, (12.48)
I 
0,  
otherwise. 
Note that the sequence VN[k 
m) given by Eq. (12.42) is periodic with period N, so we 
do not need to worry about computing «k - m))N in Eqs. (12.47) and (12.48), which 
are the desired relations between the real and imaginary parts of the N -point DFf of a 
real sequence whose actual length is less than or equal to (N/2) + 1 (for N even). These 
equations are circular convolutions, and, for example, Eq. (12.47) can be evaluated 
efficiently by the following procedure: 
L Compute the inverse DFT of XR[k) to obtain the sequence 
x[n] +x[«-n))N] 
xep[n] = 
2 
' 
O:Sn:sN 
1. 
(12.49) 
2. Compute the periodic odd part of x[n] by 
Xep[n),  
0 < n < N /2, 
xop[n) =  
-xep[n], 
N /2 < n :s N 
1, 
(12.50) 
0, 
otherwise.
1  
3. Compute the DFT of xop[n] to obtain jXI[k). 
Note that if, instead of computing the odd part of x[n] in step 2, we compute 
I 
Xep[O], 
n = 0,  
0< n < N/2, 
x[n] = 
2xep[n], 
(12.51)
xep[N/2],  n 
N/2, 
0,  
otherwise, 
then the DFT of the resulting sequence would be X[k], the complete DFT of x[n). 

954 
Chapter 12  
Discrete Hilbert Transforms 
Example  12.3 Periodic Sequence 
Consider a sequence that is periodically causal with period N = 4 and that has 
2, 
k 
0, 
3, 
k = 1,
XR[k) 1
4, 
k = 2,  
3, 
k 
3.  
We can find the imaginary part of the DFf in one of two ways. The first way is to use 
Eq. (12.47). For N 
4, 
I 
2j, k=-1+4m, 
V4[k] 
-2j, k 
1 + 4m. 
0, 
otherwise, 
where m is an integer. Implementing the convolution in Eq. (12.47) yields 
1 3 
jXj[k) =  4 L XR[m]V4[k - m], 
0~k~3 
m=O 
j, k 
1,  
= 
- j, k = 3, 
( 
0, 
otherwise. 
Alternatively, we can follow the three-step procedure that includes Eqs. (12.49) 
and (12.50). Computing the inverse DFf XR[k] yields 
1 3 
1  
xe[n] = 4 L XR[klWikn 
4[2 + 3(j)1l + 4(-I)n + 3(-j)ll]  
k=O 
3, 
n = 0,  
= 
-~'. n 
1,3, 
( 
0. 
n =2. 
Note that although this sequence is not itself even symmetric, a periodic replication of 
xe[n] is even symmetric. Thus, the DFf XR[k] of xe[n] is purely real. Equation (12.50) 
allows us to find the periodically odd part xop[nj; specifically, 
! 
~, n=l, 
xop[nl =  i, n = 3, 
0, 
otherwise. 
Finally, we obtain jXj[kj from the DFf of xop[nj: 
3 
" 
nk 
lk 13k
jX/[kj  
L.,.. xop[n]W4 
-ZW4 + ZW4 
n=O 
I 
j, k 
1, 
= 
- j, k 
3, 
0, 
otherwise, 
which is, of course, the same as was obtained from Eq. (12.47). 

955 
Section 12.3 
Relationships Between Magnitude and Phase 
12.3  RELATIONSHIPS BETWEEN MAGNITUDE 
AND PHASE 
So far, we have focused on the relationships between the real and imaginary parts of the 
Fourier transform of a sequence. Often, we are interested in relationships between the 
magnitude and phase ofthe Fourier transform. In this section, we consider the conditions 
under which these functions might be uniquely related. Although it might appear on 
the surface that a relationship between real and imaginary parts implies a relationship 
between magnitude and phase, that is not the case. This is clearly demonstrated by 
Example 5.9 in Section 5.4. The two system functions H 1(z) and H2(Z) in that example 
were assumed to correspond to causal, stable systems. Therefore, the real and imaginary 
parts of HI (ejW ) are related through the Hilbert transform relations of Eqs. (12.28a) 
and (12.28b), as are the real and imaginary parts of H2(ejW ). However, LH l(ejW) could 
not be obtained from IH1(ejW)I, since Hl(e jW ) and H2(ejW) have the same magnitude 
but a different phase. 
The Hilbert transform relationship between the real and imaginary parts of the 
Fourier transform of a sequence x[n] was based on the causality of x[n]. We can obtain 
a Hilbert transform relationship between magnitude and phase by imposing causality 
on a sequence i[n] derived from x[n] for which the Fourier transform X(e jW ) is the 
logarithm ofthe Fourier transform of x[n]. Specifically, we define i[n] so that 
(12.52a) 
i[n] 
(12.52b) 
where 
(12.53) 
and, as defined in Section 5.1, arg(X (e jCV )] denotes the continuous phase of X (ejW). The 
sequence i[n] is commonly referred to as the complex cepstrum of x[n], the properties 
and applications of which are discussed in detail in Chapter 13.2 
Ifwe now require thati[n] be causal, then the real and imaginary parts of X(ejW), 
corresponding to 10gIX(ejW)1 and arg[X(ejW)], respectively, will be related through 
Eqs. (12.28a) and (12.28b); i.e., 
rrl 
(W-O)
2n1 P -rr log IX(eJ'8 )1 cot -2-
dO 
(12.54) 
and 
10gIX(ejW)1 =x[O]+ 2~PL: arg[X(eje)]cot(W 2 O)dO. 
(12.55a) 
where, in Eq. (12.55a), i[O] is 
rr 
1 l
.
x[O] = 2 
logIX(e1W)ldw. 
(12.55b) 
n 
-rr 
2 Although x[nl is referred to as the complex cepstrum it is real valued since x(ei"') is defined in 
Eq. (12.53) is conjugate symmetric. 

956 
Chapter 12 
Discrete Hilbert Transforms 
Although it is not at all obvious at this point, in Problem 12.35 and in Chapter 13 
we develop the fact that the minimum-phase condition defined in Section 5.6, namely, 
that X (z) have all its poles and zeros inside the unit circle, guarantees causality of the 
complex cepstrum. Thus, the minimum-phase condition in Section 5.6 and the condition 
of causality of the complex cepstrum turn out to be the same constraint developed 
from different perspectives. Note that when i[n] is causal, arg[X (eJW )] is completely 
determined through Eq. (12.54) by log IX(ejW)I; however, the complete determination 
of log IX (ejW)1 by Eq. (12.55a) requires both arg[X (elm)) and the quantity i[O). If i[O] 
is not known, then log IX (eJW)1 is determined only to within an additive constant, or 
equivalently, IX(ejW)1 is determined only to within a multiplicative (gain) constant. 
Minimum phase and causality of the complex cepstrum are not the only constraints 
that provide a unique relationship between the magnitude and phase of the DTFT. 
As one example of another type of constraint, it has been shown (Hayes, Lim and 
Oppenheim, 1980) that if a sequence is of finite length and if its z-transform has no 
zeros in conjugate reciprocal pairs, then, to within a scale factor, the sequence (and 
consequently, also the magnitude of the DTFT) is uniquely determined by the phase of 
the Fourier transform. 
12.4 HILBERT TRANSFORM RELATIONS FOR COMPLEX 
SE~UENCES 
Thus far, we have considered Hilbert transform relations for the Fourier transform 
of causal sequences and the DFT of periodic sequences that are "periodically causal" 
in the sense that they are zero in the second half of each period. In this section, we 
consider complex sequences for which the real and imaginary components can be related 
through a discrete convolution similar to the Hilbert transform relations derived in 
the previous sections. These relations are particularly useful in representing bandpass 
signals as. complex signals in a manner completely analogous to the analytic signals of 
continuous-time signal theory (Papoulis, 1977). 
As mentioned previously, it is possible to base the derivation of the Hilbert trans­
form relations on a notion of causality or one-sidedness. Since we are interested in 
relating the real and imaginary parts of a complex sequence, one-sidedness will be ap­
plied to the DTFT of the sequence. We cannot, of course, require that the DTFT be 
zero for (J) < 0, since it must be periodic. Instead, we consider sequences for which 
the Fourier transform is zero in the second half of each period; i.e., the z-transform is 
zero on the bottom half (-Jr S 
< 0) of the unit circle. Thus, with x[n] denoting the 
(J) 
sequence and X(eJW ) its Fourier transform, we require that 
X(eJW) = 0, 
-Jr :s (J) < O. 
(12.56) 
(We could just as well assume that X (e jm) is zero for 0 < 
S Jr.) The sequence x[n1
(J) 
corresponding to X (eJW ) must be complex, since, if x [n] were real, X (e jm) would be 
conjugate symmetric, i.e., X (elw ) = X*(e- jW). Therefore, we express x[n] as 
x[n] = xr[n] + jXi[n], 
(12.57) 
where xr[n] and xj[n1 are real sequences. In continuous-time signal theory, the com­
parable signal is an analytic function and thus is called an analytic signal. Although 
analyticity has no formal meaning for sequences, we will nevertheless apply the same 
terminology to complex sequences whose Fourier transforms are one-sided. 

957 
Section 12.4 
Hilbert Transform Relations for Complex Sequences 
If Xr(eiO) and Xi (eiw ) denote the Fourier transforms of the real sequences xr[n] 
and Xi [n], respectively, then 
X(eiw ) 
Xr(eiO) + jXi(eiw), 
(12.58a) 
and it follows that 
and 
(12.58b) 
(12.58c) 
Note that Eq. (12.58c) gives an expression for jXj(e jW), which is the Fourier 
transform ofthe imaginary signal j Xi [n]. Note also that Xr (eiw) and Xi (eiw ), the Fourier 
transforms of the real and imaginary parts, respectively, of x[n] are both complex­
valued functions. In general, the complex transforms Xr (eiw ) and j Xi (eiw ) playa role 
similar to that played in the previous sections by the even and odd parts, respectively, of 
causal sequences. However, Xr(eiw) is conjugate symmetric, i.e., Xr(eiw) = X:(e-iw). 
Similarly, j Xj(e jW) is conjugate antisymmetric, i.e., j Xi (eiw) =- j X7(e- jO). 
Figure 12.4 depicts an example of a complex one-sided Fourier transform of a 
complex sequence x[n] 
xr[n] + jXi[n], and the corresponding two-sided transforms 
of the real sequences xr[n] and Xi [n]. This figure shows pictorially the cancellation 
implied by Eqs. (12.58). 
If X(eiw) is zero for -Jr .:s w < 0, then there is no overlap between the nonzero 
portions of X (eiw ) and X*(e- jW) except at w = O. Thus, X (eiw ) can be recovered except 
at w 
0 from either Xr(eiw) or Xi(eiw). Since X(eiw) is assumed to be zero at w 
±Jr, 
X (eiw) is totally recoverable except at w = 0 from j Xi (eiw). This is in contrast to the 
situation in Section 12.2, in which the causal sequence could be recovered from its odd 
part, except at the endpoints. 
In particular, 
0< w < Jr, 
(12.59)
-Jr .:s w < 0, 
and 
(12.60)  
Alternatively, we can relate Xr (eiw) and Xi (eiw) directly by 
jW
Xj(eiw) = {-jxr(e
), 
0 < W < Jr, 
(12.61)
jXr(el'V), 
-j( .:s W < 0, 
or 
(12.62a) 
where 
0< W < Jr, 
(12.62b)
-Jr < W < O. 

-217',_ -­
-17 
'­
17 
217"_ 
Chapter 12 
Discrete Hilbert Transforms
958 
X(e jlll) 
-317 
-211'\ 
//~17' 
/,/'1T 
217\ 
//,//31T 
w 
// 
\
\ .... // 
(a) 
X*(e-1W) 
-317 
-217 
-17 
17 
217 
317 
w 
(b) 
X,(e}W) 
-317 
--- 3rr 
W 
(c) 
Xi(e jW) 
_/ 
w 
(d) 
Figure 12.4 Illustration of decomposition of aone-sided Fouriertransform. (Solid 
curves are real parts and dashed curves are imaginary parts.) 
Equations (12.62) are illustrated by comparing Figures 12.4( c) and 12.4( d). Xi (e jW ) 
is the Fourier transform of xi[n], the imaginary part of x[n], and Xr(e jW ) is the Fourier 
transform of xr[n], the real part of x[n]. Thus, according to Eqs. (12.62), xi[n] can be 
obtained by processing Xr [n] with an LTI discrete-time system with frequency response 
H (e jW ), as given by Eq. (12.62b). This frequency response has unity magnitude, a phase 
angle of -7i/Uor 0 < W < 7i, and a phase angle of +7i/2 for -7i < W < O. Such a system 
is called an ideal 90-degree phase shifter or a Hilbert transformer. From Eq& (12.62), it 

959
Section 12.4 
Hilbert Transform Relations for Complex Sequences 
2 
I
3 4 5 6 7 8 
n 
Figure 12.5 Impulse response of an 
ideal Hilbert transformer or gO-degree 
phase shifter. 
follows that 
1T 
-1 
o 1 2 
2 
(12.63)  
Thus, - Xr [n] can also be obtained from Xi [n] with a 90-degree phase shifter. 
The impulse response h[n] of a 90-degree phase shifter, corresponding to the 
frequency response H(eJW ) given in Eq. (12.62b), is 
0
1 1
. 
in
1 
.
h[n] =  -
jeJcvndw - -
jeJwndw, 
21l' 
-n 
21l' 
0 
or 
2 sin2(1l'n/2) 
0 
h[n] = 
1l' 
n 
,n =1= 
, 
(12.64) 
\ 0, 
n =0. 
The impulse response is plotted in Figure 12.5. Using Eqs. (12.62) and (12.63), we obtain 
the expressions 
xj[n] = L
00 
h[n - m]xr[m] 
(12.65a) 
m=-oo 
and 
xr[n) = - L
00 
h[n - m]xj[m).  
(12.65b) 
m=-oo 
Equations (12.65) are the desired Hilbert transform relations between the real 
and imaginary parts of a discrete-time analytic signal. Figure 12.6 shows how a discrete­
time Hilbert transformer system can be used to form a complex analytic signal, which 
is simply a pair of real signals. 

960  
Chapter 12 
Discrete Hilbert Transforms 
xr[n] 
xr[n] } Complex 
Figure 12.6 Block diagram
signal 
x[n]  
representation of the creation of a 
complex sequence whose Fourier 
transform is one-sided. 
12.4.1 Design of Hilbert Transformers 
The impulse response ofthe Hilbert transformer, as given in Eq. (12.64), is not absolutely 
summable. Consequently, 
00 
H(ejW ) = L: h[n]e-jwn 
(12.66) 
n=-oo 
converges to Eq. (12.62b) only in the mean-square sense. Thus, the ideal Hilbert trans­
former or 90-degree phase shifter takes its place alongside the ideallowpass filter and 
ideal bandlimited differentiator as a valuable theoretical concept that corresponds to a 
noncausal system and for which the system function exists only in a restricted sense. 
Approximations to the ideal Hilbert transformer can, of course, be obtained. FIR 
approximations with constant group delay can be designed using either the window 
method or the equiripple approximation method. In such approximations, the 90-degree 
phase shift is realized exactly, with an additional linear phase component required for a 
causal FIR system. The properties of these approximations are illustrated by examples 
of Hilbert transformers designed with Kaiser windows. 
Example 12.4 
Kaiser Window Design of Hilbert 
Transformers 
The Kaiser window approximation for an FIR discrete Hilbert transformer of order 
M (length M + 1) would be of the form 
lo{tl(l- [en 
nd )/nd]2)1 / 21) (~sin2[]f(n­
O:::n :::M,
h[n] = 
lo(fJ) 
]f 
n - nd 
0,  
otherwise,
!(
(12.67) 
where nd = M12. If M is even, the system is a type III FIR generalized linear-phase 
system, as discussed in Section 5.7.3. 
Figure 12.7(a) shows the impulse response, and Figure 12.7(b) shows the magni­
tude of the frequency response, for M = 18 and fJ 
2.629. Because h[n] satisfies the 
symmetry condition h[nJ = -h[M - n] for 0::: n ::: M, the phase is exactly 90 degrees 
plus a linear-phase component corresponding to a delay of nd 
18/2 
9 samples; 
i.e., 
. 
-]f 
LH(e1W) = 
- 9w, 
o < W < ]f.  
(12.68)
2 
From Figure 12.7(b), we see that, as required for a type III system, the frequency 
response is zero at z = 1 and z 
-1 (w = 0 and W 
]f). Thus, the magnitude response 
cannot approximate unity very well, except in some middle band WL < Iwi < WH. 

961 
Section 12.4 
Hilbert Transform Relations for Complex Sequences 
1.0 
0.5 
!!) 
"0 
B 
0
~ 
e 
-< 
-0.5 
-1.0 o  
5 
10  
Sample number (n)  
(a) 
1.2,-----------------------, 
-
-
• , 1 
I 
T 
I 
T , 
• 
I 
15 
20 
O~_______L________L_______~________~______~ 
o 
0.21T 
Radian frequency (w) 
(b) 
Figure 12.7 (a) Impulse response and (b) magnitude response of an FIR Hilbert 
transformer designed using the Kaiser window. (M = 18 and f3 
2.629.) 
If M is an odd integer, we obtain a type IV system, as shown in Figure 12.8, 
where M = 17 and f3 = 2.44. For type IV systems, the frequency response is forced 
to be zero only at z = 1 (w = 0). Therefore, a better approximation to a constant­
magnitude response is obtained for frequencies around w = Jr. The phase response is 
exactly 90 degrees at all frequencies, plus a linear-phase component corresponding to 
nd = 17/2 = 8.5 samples delay; i.e., 
. 
-Jr 
iH (e1W ) = 2 - 8.5w, 
0< w < Jr.  
(12.69) 
From a comparison of Figures 12.7(a) and 12.8(a), we see that type III FIR Hilbert 
transformers have a significant computational advantage over type IV systems when 
it is not necessary to approximate constant magnitude at W 
Jr. This is because, for 
type III systems, the even-indexed samples of the impulse response are all exactly zero. 

962  
Chapter 12 
Discrete Hilbert Transforms 
Thus, taking advantage of the antisymmetry in both cases, the system with M 
17 
would require eight multiplications to compute each output sample, while the system 
with M = 18 would require only five multiplications per output sample. 
1.0 
0.5 
.., 
"0 B 
:.::l 
0.. 
0 
9 « 
-0.5 
-1.0 
0 
5 
10 
Sample number (n) 
(a) 
15 
20 
1.2 
.g 
J 
OLI________~__________L_________~_________L________~ 
o  
0.271" 
0.471" 
0.671" 
0.871" 
71" 
Radian frequency (w) 
(b) 
Figure 12.8 (a) Impulse response and (b) magnitude response of an FIR Hilbert 
transformer designed using the Kaiser window. (M 
17 and f3 
2.44.) 
Type III and IV FIRlinear-phase Hilbert transformer approximations with equirip­
pIe magnitude approximation and exactly 9O-degree phase can be designed using the 
Parks-McClellan algorithm as described in Sections 7.7 and 7.8, with the expected im­
provements in magnitude approximation error over window-designed filters of the same 
length (see Rabiner and Schafer, 1974). 
The exactness of the phase of type III and IV FIR systems is a compelling motiva­
tion for their use in approximating Hilbert transformers. IIR systems must have some 

Section 12.4 
Hilbert Transform Relations for Complex Sequences 
963 
Complex 
Figure 12.9 Block diagram 
signal 
representation of the allpass phase 
y[n] 
splitter method for the creation of a 
complex sequence whose Fourier 
transform is one-sided. 
phase response error as well as magnitude response error in approximating a Hilbert 
transformer. The most successful approach to designing IIR Hilbert transformers is to 
design a "phase splitter," which consists of two allpass systems whose phase responses 
differ by approximately 90 degrees over some portion of the band 0 < Iwl < 11:. Such 
systems can be designed by using the bilinear transformation to transform a continuous­
time phase-splitting system to a discrete-time system. (For an example of such a system, 
see Gold, Oppenheim and Rader, 1970.) 
Figure 12.9 depicts a 90-degree phase-splitting system. IfXr [n] denotes a real input 
signal and xi[n] its Hilbert transform, then the complex sequence x[n] 
xr[n] + jXi[n] 
has a Fourier transform that is identically zero for -11: :::: w < 0; i.e., X (z) is zero on 
the bottom half of the unit circle of the z-plane. In the system of Figure 12.6, a Hilbert 
transformer was used to form the signal Xi [n] from Xr [n]. In Figure 12.9, we process 
xr[n] through two systems: Hl(eJW) and H2(eJW ). Now, if Hl(eJW) and H2(eJW) are 
allpass systems whose phase responses differ by 90 degrees, then the complex signal 
y[n] = Yr[n] + jYi[n] has a Fourier transform that also vanishes for -11: :::: w < O. 
Furthermore, ly(ejW)I = IX (eJW)I, since the phase-splitting systems are allpass systems. 
The phases ofy(eJW) and X (eJW) will differ by the phase component common to HI (eJW) 
and H2(eJW). 
12.4.2 Representation of Bandpass Signals 
Many of the applications of analytic signals concern narrowband communication. In 
such applications, it is sometimes convenient to represent a bandpass signal in terms of 
a lowpass signal. To see how this may be done, consider the complex lowpass signal 
x[n] = xr[n] + jXi[n], 
where Xi [n] is the Hilbert transform of Xr [n] and 
-11: :::: w < O. 
The Fourier transforms Xr(eJW) and jXi(eJW) are depicted in Figures 12.10(a) and 
12.10(b), respectively, and the reSUlting transform X(e jW) 
Xr(eJW) + jXi(ejW) is 
shown in Figure 12.1O(c). (Solid curves are real parts and dashed curves are imaginary 
parts.) Now, consider the sequence 
s[n] = X[n]eJiocn = sr[n] + jSj[n], 
(12.70) 
where sr[n] and si[n] are real sequences. The corresponding Fourier transform is 
(12.71) 

964 
Chapter 12 
Discrete Hilbert Transforms
$,1.1 
(S) 
-1T 
-aw 
-~w 
1T 
~ 
W 
(a) 
_b'IO)  
-1T 
-aw~-aw 
1T 
w 
~ 
~  
(b) 
J'J 
~~) 
J'J 
-1T 
-~w 
1T 
w 
(c) 
IS(,>-I 
I
f\ 
~I 
f\
-21T 
,-/ 
-1T 
we ,/'\ 7T 
21T 
'- / 
W 
wc+ aw 
(d) 
IS,(,>-I 
A 
1 
A 
(+1 A 
I 
(:)
D
-21T 
-1T 
W 
-
\ 
1T 
21T 
-
W 
c 
w + aw
c
(e) 
jS;(e i"') 
wc+ aw 
(f) 
Figure 12.10 Fourier transforms for representation of bandpass signals. (Solid 
curves are real parts and dashed curves are imaginary parts.) (Note that in parts (b) 
and (f) the functions jX;(eiw)and jS;(eiw)are plotted, where X; (ejW) and Si(eiw) 
are the Fourier transforms of x;[n] and s;[n], respectively.) 
which is depicted in Figure 12.1O(d). Applying Eqs. (12.58) to S(ejW ) leads to the 
equations 
Sr(ejW) = ~rS(ejW) + S*(e-jw)], 
(12.72a) 
jSj(e jW) = irS(ejW) - S*(e- jW )]. 
(12.72b) 

Section 12.4 
Hilbert Transform Relations for Complex Sequences 
965 
For the example of Figure 12.10, S, (eiw) and j S i (eiw) are illustrated in Figures 12.10(e) 
and 12.10(f), respectively. It is straightforward to show that if X, (eiw ) = 0 for 
t:,.w < Iwi s 7r, and if We + t:,.w < Jr, then S(eiw) will be a one-sided bandpass signal such 
that S(eiw ) = 0 except in the interval We < W S We +t:,.w. As the example of Figure 12.10 
illustrates, and ascan be shown using Eqs. (12.57) and (12.58), Si (eiw) = H(eiw)S,(eiw), 
i.e., si[n] is the Hilbert transform of s,[n]. 
An alternative representation of a complex signal is in terms of magnitude and 
phase; i.e., x[n] can be expressed as 
x[n] = A[n]eicfJ[n1, 
(12.73a) 
where 
(12.73b) 
and 
I/>[n] = arctan (xi[n]). 
(12.73c)
x,[n] 
Therefore, from Eqs. (12.70) and (12.73), we can express s[n] as 
s[n] = (x,[n] + jXi[n])eiwcn 
(12.74a) 
= A [n]ei(wcn+cfJ[n]) , 
(12.74b) 
from which we obtain the expressions 
s, [n] = x, [n] cos Wen -
Xi [n] sin Wen, 
(12.75a) 
or 
s,[n] = A[n] cos(wen + I/>[n]), 
(12.75b) 
and 
(12.76a) 
or 
si[n] = A[n] sin(wen + I/>[n]). 
(12.76b) 
Equations (12.75a) and (12.76a) are depicted in Figures 12.11(a) and 12.11(b), 
respectively. These diagrams illustrate how a complex bandpass (single-sideband) signal 
can be formed from a reallowpass signal. 
Taken together, Eqs. (12.75) and (12.76) are the desired time-domain representa­
tions of a general complex bandpass signal s [n] in terms of the real and imaginary parts 
of a complex lowpass signal x [n]. Generally, this complex representation is a convenient 
mechanism for representing a real bandpass signal. For example, Eq. (12.75a) provides 
a time-domain representation of the real bandpass signal in terms of an "in-phase" com­
ponent x,[n] and a "quadrature" (90-degree phase-shifted) component Xi [n]. Indeed, as 
illustrated in Figure 12.1O(e), Eq. (12.75a) permits the representation ofreal bandpass 
signals (or filter impulse responses) whose Fourier transforms are not conjugate sym­
metric about the center of the passband (as would be the case for signals of the form 
x,[n] cos wen). 

966 
Chapter 12 
Discrete Hilbert Transforms 
cos Wen 
sin wen 
sin Wen 
cos wen 
(a) 
(b) 
Figure 12.11 
Block diagram representation of EQs. (12.75a) and (12.76a) for 
obtaining a single-sideband signal. 
It is clear from the form of Eqs. (12.75) and (12.76) and from Figure 12.11 that 
a general bandpass signal has the form of a sinusoid that is both amplitude and phase 
modulated. The sequence A[n] is called the envelope and rJ![n] the phase. This narrow­
band signal representation can be used to represent a variety of amplitude and phase 
modulation systems. The example of Figure 12.10 is an illustration of single-sideband 
modulation. If we consider the real signal sr[n] as resulting from single-sideband mod­
ulation with the lowpass real signal xr[n] as the input, then Figure 12.11(a) represents a 
scheme for implementing the single-sideband modulation system. Single-sideband mod­
ulation systems are useful in frequency-division multiplexing, since they can represent 
a real bandpass signal with minimum bandwidth. 
12.4.3 Bandpass Sampling 
Another important use of analytic signals is in the sampling of bandpass signals. In 
Chapter 4, we saw that, in general, if a continuous-time signal has a bandlimited Fourier 
transform such that Sc(jQ) 
0 for IQ I ::::: Q N, then the signal is exactly represented 
by its samples if the sampling rate satisfies the inequality 2rr/ T ::::: 2QN. The key to 
the proof of this result is to avoid overlapping the replicas of Sc(jQ) that form the 
DTFT of the sequence of samples. A bandpass continuous-time signal has a Fourier 
transform such that Sc(jQ) 
0 for 0 ::: IQI ::: Qc and for IQI ::::: Qc + ~Q. Thus, its 
bandwidth, or region of support, is really only 2~Q rather than 2(Qc + ~Q), and with 
a proper sampling strategy, the region -Qc ::: Q ::: Q c can be filled with images of 
the nonzero part of Sc(jQ) without overlapping. This is greatly facilitated by using a 
complex representation of the bandpass signal. 
As an illustration, consider the system of Figure 12.12 and the signal shown in 
Figure 12.13(a). The highest frequency of the input signal is Qc + ~Q. If this signal 
is sampled at exactly the Nyquist rate, 2rr/ T = 2(Qc + ~Q), then the resulting se­
quence of samples, .sAn] 
sc(nT), has the Fourier transform Sr(ejcll) plotted in Fig­
ure 12.13(b). Using a discrete-time Hilbert transformer, we can form the complex 
sequence s[n] = sr[n] + jSj[n] whose Fourier transform is S(eJW ) in Figure 12.13(c). The 

Section 12.4 
Hilbert Transform Relations for Complex Sequences  
967 
srlnj = sc(nT) 
Complex decimated 
T 
signalsdlnj 
Figure 12.12 System for reduced-rate sampling of a real bandpass signal by 
decimation of the equivalent complex bandpass signal. 
width of the nonzero region of S(eJW )is l'1w = (l'1Q)T. Defining M as the largest integer 
less than or equal to 2Jr/ l'1w, we see that M copies of S(eJW ) would fit into the interval 
-Jr < W < Jr. (In the example of Figure 12.13( c), 2Jr/ l'1w = 5.) Thus, the sampling rate 
of s[n] can be reduced by decimation as shown in Figure 12.12, yielding the reduced-rate 
complex sequence sd[n] = srd[n] + jSid[n] = s[Mn] whose Fourier transform is 
1  M-l 
Sd(eJW ) = M L S(eJ[(w-2rrk)/Ml).  
(12.77) 
k=O 
Figure 12.13(d) showsSd(eJW) withM = 5inEq. (12.77). S(eJW ) and two ofthe frequency­
scaled and translated copies of S(eJW ) are indicated explicitly in Figure 12.13( d). It is 
clear that aliasing has been avoided and that all the information necessary to reconstruct 
the original sampled real bandpass signal now resides in the discrete-time frequency in­
terval -Jr < W .:::: Jr. A complex filter applied to sd[n] can transform this information 
in useful ways, such as by further bandlimiting, amplitude or phase compensation, etc., 
or the complex signal can be coded for transmission or digital storage. This processing 
takes place at the low sampling rate, and this is, of course, the motivation for reducing 
the sampling rate. 
The original real bandpass signal Sr [n] can be reconstructed ideally by the following 
procedure: 
1.  Expand the complex sequence by a factor M; i.e., obtain 
_  {srd[n/M] + jSid[n/M], 
n = 0, ±M, ±2M, ... ,
[ ]  
(12.78)
Se n -
0,  
otherwise. 
2.  Filter the signal se[n] using an ideal complex bandpass filter with impulse response 
hi [n] and frequency response 
-Jr < W < We,  
We < W < We + l'1w, 
(12.79)  
We + l'1w < W < Jr.  
(In our example, We + l'1w = Jr.) 
3. Obtain sr[n] = Re{se[n] * hi[n]). 
A useful exercise is to plot the Fourier transform Se(eJW ) for the example of 
Figure 12.13 and verify that the filter of Eq. (12.79) does indeed recover s[n]. 

, 
Sc(jll) 
1 
II
-lle 
-lle-all 
(a) 
Sr(ej<.,) 
1 
T 
-317 
-217 
,../ -'TT' 
-We 
We '/17 217 
We 
217 
,/ 317 
W 
(b) 
S(ejr,,) 
2 
T 
-317 
-217 
'../-17 
We '/17 
217 
,../ 317 
W 
(c) 
~ S(ei[(w - 611")15]) 
51S(ell(w - 811")15])
Sd(eiUJ) 
w 
(d) 
Figure 12.13 Example of reduced-rate sampling of abandpass signal using the 
system of Figure 12.12. (a) Fourier transform of continuous-time bandpass signal. 
(b) Fourier transform of sampled signal. (c) Fourier transform of complex bandpass 
discrete-time signal derived from the signal of part (a). (d) Fourier transform of 
decimated complex bandpass of part (c). (Solid curves are real parts and dashed 
curves are imaginary parts.) 
968  

969 
Section 12.5 
Summary 
Another useful exercise is to consider a complex continuous-time signal with a 
one-sided Fourier transform equal to Sc(jQ) for Q ~ O. It can be shown that such a 
signal can be sampled with sampling rate 211:/ T = 6. Q, directly yielding the complex 
sequence sd[n]. 
12.5 SUMMARY 
In this chapter, we have discussed a variety of relations between the real and imaginary 
parts of Fourier transforms and the real and imaginary parts of complex sequences. 
These relationships are collectively referred to as Hilbert transform relationships. Our 
approach to deriving all the Hilbert transform relations was to apply a basic causality 
principle that allows a sequence or function to be recovered from its even part. We 
showed that, for a causal sequence, the real and imaginary parts of the Fourier transform 
are related through a convolution-type integral. Also, for the special case when the 
complex cepstrum of a sequence is causal or, equivalently, both the poles and zeros of 
its z-transform lie inside the unit circle (the minimum-phase condition), the logarithm 
of the magnitude and the phase of the Fourier transform are a Hilbert transform pair 
of each other. 
Hilbert transform relations were derived for periodic sequences that satisfy a 
modified causality constraint and for complex sequences whose Fourier transforms 
vanish on the bottom half of the unit circle. Applications of complex analytic signals to 
the representation and efficient sampling of bandpass signals were also discussed. 
Problems 
Basic Problems 
12.1. Consider a sequence x[n] with DTFT X (ejUJ ). The sequence x[n] is real valued and causal, 
and 
Re{X (e jUJ )} = 2 
2a cos w. 
Determine Im{X (e jUJ )}. 
12.2. Consider a sequence x[n] and its DTFT X (ejUJ ). The following is known: 
x[n] is real and causal, 
Re{X(e jUJ )} i - cosw. 
Determine a sequence x[n] consistent with the given information. 
12.3. Consider a sequence x[nJ and its DTFT X(e jUJ ). The following is known: 
x[n] is real, 
x[O] 
0, 
x[l] > 0, 
5
IX(e jUJ )12 
4 
cosw. 
Determine two distinct sequences Xl [n] and x2[n] consistent with the given information. 

970  
Chapter 12 
Discrete Hilbert Transforms 
U.4.  Consider a complex sequence x[n] 
x,[nl + jXi[n], where x,[n] and xdn] are the real 
part and imaginary part, respectively. The z-transform X(z) of the sequence x[n] is zero 
on the bottom half of the unit circle; i.e., X (eiw ) = °for rr 
w < 2rr. The real part of x [n] 
is 
1/2, 
n = 0, 
x,[n] = 
-1/4, 
n 
±2, 
0, 
otherwise.
\ 
Determine the real and imaginary parts of X(eiw). 
U.S. Find the Hilbert transforms Xi [n] 
1{(x, [n]} of the following sequences: 
(a)  x,[n] = cosW()n 
(b)  x,[n] 
sinW()n 
(c)  xrln] 
sin(W()n)  
rrn  
U.6. The imaginary part of X (eiw) for a causal, real sequence x[n] is 
Xj(ejW) = 2sinw 
3sin4w. 
Additionally, it is known that X(ejW)lw=o 
6. Find x[n]. 
U.7.  (a) x [n] is a real, causal sequence with the imaginary part of its DTFT X (eiw ) given by 
Im{X (eiw)}  
sin w + 2 sin 2w. 
Determine a choice for x[n]. 
(b)  Is your answer to part (a) unique? If so, explain why. If not, determine a second, 
distinct choice for x[n] satisfying the relationship given in part (a). 
U.S.  Consider a real, causal sequence x[n] with DTFT X(ejW ) = XR(eiw ) + jXj(elW). The 
imaginary part of the DTFT is 
Xj(elw ) 
3sin(2w). 
Which of the real parts XRm (elw) listed below are consistent with this information: 
XRl (elw ) 
:23 cos(2w), 
XR2(elw ) = -3 cos(2w) -1, 
XR3(eiw ) 
-3 cos(2w) , 
jW
XR4(e
) = 2cos(3w). 
. 
3 
XRS(eJW ) = :2 cos(2w) + 1. 

971 
Chapter 12 
Problems 
12.9.  The following information is known about a real, causal sequence x[nl and its DTFT 
X (e j "'): 
:Im{X(ej "'») 
3sin(w) + sin(3w), 
X (ej"')I",=rr 
3. 
Determine a sequence x[nl consistent with this information. Is the sequence unique? 
12.10.  Consider h[n], the real-valued impulse response of a stable, causal LTI system with fre­
quency response H(ejU». The following is known: 
(i)  The system has a stable, causal inverse.  
5  
. 12 
4 -cosw
(ii)  H (elU» 
= -'---­
I 
5 + 4cosw  
Determine h[n] in as much detail as possible. 
12.11.  Let x[n] 
xr[nl + jXi[n] be a complex-valued sequence such that X(ejU» 
0 for 
-rr .:::: w < O. The imaginary part is 
4 
n = 3, 
xi£nl = { ~4, n = -3. 
Specify the real and imaginary parts of X (e j "'). 
12.12.  h[n] is a causal, real-valued sequence with h[O] nonzero and positive. The magnitude 
squared of the frequency response of h [n I is given by 
. 12 
10 
H(el "') 
= 9  
3'2 cos(w).
1 
(8)  Determine a choice for h[n]. 
(b)  Is your answer to part (b) unique? If so, explain why. If not, determine a second, 
distinct choice for h[nl satisfying the given conditions. 
12.13.  Let x[nl denote a causal, complex-valued sequence with Fourier transform 
If XR(e j ",) = 1 + cos(w) + sinew) - sin(2w), determine X/(ej",). 
12.14.  Consider a real, anticausal sequence x[n] with DTFT X(ej "'). The real part of X(ej ",) is 
00 
XR(e j "')  L(1/2i cos(kw). 
k=O 
Find X/(el",), the imaginary part of X(eJU». (Remember that a sequence is said to be 
anticausal if x[nl = 0 for n > 0.) 

972  
Chapter 12 
Discrete Hilbert Transforms 
U.1S.  x[n] is a real, causal sequence with DTFT X(el "'). The imaginary part of X(el"') is 
Im(X(el "')} 
sinw, 
and it is also known that 
ex:L x[n] = 3. 
n=-OQ 
Determine x[n]. 
12.16.  Consider a real, causal sequence x[n] with DTFT X(e j "'), where the following two facts 
are given about X (ej{V): 
jW
XR(e 
) 
2 
4cos(3w), 
X (ej"')!w=rr 
7. 
Are these facts consistent? That is, can a sequence x[n1satisfy both? If so, give one choice 
for x[n]. If not, explain why not. 
12.17.  Consider a real, causal, finite-length signal x[n) with length N = 2 and with a 2-point 
DFf X[kJ = XR[k) + jX/[kJ for k = 0.1. If XR[kJ = 2o[k] 
4a[k 
1], is it possible to 
determine x[n] uniquely? Ifso, give x[n]. If not, give several choices for x[n] satisfying the 
stated condition on XR[k]. 
U.18.  Letx[n] be areal-valued, causal, finite-length sequence with length N 
3. Find two choices 
for x[n] such that the real part of the DFT XR[k) matches that shown in Figure P12.18. 
Note that only one of your sequences is "periodically causal" according to the definition 
in Section 10.2, where x[n] =°for N/2 < n s: N 
1. 
9 
XR[k] 
6 
6 
o 
1 
2 
k 
Figure P12.18 
U.19.  Let x [n) be a real, causal, finite-length sequence with length N = 4 that is also periodically 
causal. The real part of the 4-point DFT XR[k] for this sequence is shown in Figure P12.19. 
Determine the imaginary part of the DFT jX/[k). 
4 
XRik] 
o 
2 
3 
k 
FigureP12.19 
12.20.  Consider a sequence x [n] that is real, causal, and of finite length with N = 6. The imaginary 
part of the 6-point D Ff of this sequence is - 
j2/vIJ, k= 2, 
jX/[k] = 
j2/vIJ, 
k 
4, 
0, 
otherwise.
1 

• • 
• • • 
• • • 
Chapter 12 
Problems 
913 
Additionally, it is known that 
1 5 
6 EX[k] 
l. 
k=O 
Which of the sequences shown in Figure P12.20 are consistent with the information given? 
xJn] 
x2[n]
2/3 
.2/31 
5
r 
0 
3 
4 
5 
n 
0 
1 
2 
3 
4 
n 
1-2/3 
X3[n] 
6 
x4[n] 
112/31 
2 
0 
3 
4 
5 
n 
0 
3 
4 
5 
n
~21) 
xs[n] 
X6[n]
2/3 
• 
12/3 I 
0 
4 
5 
n 
2 
3 
4 
5 
n
~2/31  
2 
x,[n] 
xsln] 
1 12/3.
0 
• •
0 
4 
5 
n 
3 
4 
5 
n
-J 
L2/~  
Figure P12.20 
12.21. Let x[nl be a real causal sequence for which Ix[nll < 00. The z-transform of x[n] is 
00 
X(z) = E x[n]z-n, 
n=O 
which is a Taylor's series in the variable z-1 and therefore converges to an analytic function 
everywhere outside some circular disc centered at z = O. (The ROC includes the point 
z 
00, and, in fact, X (oc) = x[OJ.) The statement that X (z) is analytic (in its ROC) implies 
strong constraints on the function X(z). (See Churchill and Brown, 1990.) Specifically, its 
real and imaginary parts each satisfy Laplace's equation, and the real and imaginary parts 
are related by the Cauchy-Riemann equations. We will use these properties to determine 
X(z) from its real part when x[n] is a real, finite-valued, causal sequence. 
Let the z-transform of such a sequence be 

Chapter 12 
Discrete Hilbert Transforms
974 
where XR(Z) and Xj(z) are real-valued functions of z. Suppose that X R(Z) is 
Jw 
p+acosw
XR(pe 
) = 
, 
a real, 
p 
Jw
for z = pe
. Then find X(z) (as an explicit function of z), assuming that X(z) is analytic 
everywhere except at z = O. Do this using both of the following methods. 
(8)  Method 1, Frequency Domain. Use the fact that the real and imaginary parts of X(z) 
must satisfy the Cauchy-Riemann equations everywhere that X(z) is analytic. The 
Cauchy-Riemann equations are the following: 
1. In Cartesian coordinates, 
au 
ilV 
av 
au 
ax = ay' 
ax 
ay' 
where z 
x + jy and X(x + jy) = U(x, y) + jV(x, y). 
2. In polar coordinates, 
au  
lav 
av 
lau 
ap  
p aw' 
ap 
-p aw' 
where z = peJw and X(peJW) = U(p. w) + jV(p, w). 
Since we know that U = X R, we can integrate these equations to find V 
X / and 
hence X. (Be careful to treat the constant of integration properly.) 
(b) Method2, Time Domain. The sequence x [n] can be represented as x [n ] 
Xe [n] + Xo [n], 
where Xe [n] is real and even with Fourier transform X R(eJW ) and the sequence xo[n] 
is real and odd with Fourier transform j X/(eJW ). Find xe[n] and, using causality, find 
xo[n] and hence x[n] and X(z). 
12.22.  x[n] is a causal, real-valued sequence with Fourier transform X(eJW ). It is known that 
Re(X(eiw)} 
1 +3cosw+cos3w. 
Determine a choice for x[n] consistent with this information, and specify whether or not 
your choice is unique. 
12.23.  x[n] is a real-valued, causal sequence with DTFT X(e jW ). Determine a choice for x[n] if 
the imaginary part of X(eiw ) is given by: 
Im{X(eiw)} 
3sin(2w) - 2sin(3w). 
12.24.  Show that the sequence of DFS coefficients for the sequence 
1, 
n 
0, 
NI2, 
uN[n] = 
2, 
n = 1,2, ... , N 12 -1, 
0, 
n 
N 12 + 1, ... , N - 1,
1 
is 
N. 
k = 0, 
UN[k] = 
-j2cot(rrkIN), 
k odd, 
0, 
keven, k:;6 O.
I 
Hint: Find the z-transform of the sequence 
UN[n] = 2u[n] 
2u[n - N12] 
8[n] + 8[n - NI2]. 
and sample it to obtain [; [k]. 

Chapter 12 
Problems  
975 
Advanced Problems 
12.25. Consider a real-valued finite-duration sequence x[n] of length M. Specifically, x[n] = 0 
for n < 0 and n > M 
1. Let X[k] denote the N-point DFT of x[n] with N :?: M and N 
odd. The real part of X[k] is denoted XR[k]. 
(a)  Determine, in terms of M, the smallest value of N that will permit X[k) to be uniquely 
determined from XR[kj. 
(b)  With N satisfying the condition determined in part (a), X[kj can be expressed as the 
circular convolution of XR[kj with a sequence UN [kj. Determine UN[k]. 
12.26.  Yr [n] is a real-valued sequence withDTFT Yr(e jW ). The sequences Yr[n] and Yi [n] in Figure 
P12.26 are interpreted as the real and imaginary parts of a complex sequence y[nj, i.e., 
y[n] 
Yr[n] + jYt[n]. Determine a choice for H(ejW ) in Figure P12.26 so that Y(ejW) is 
Yr (ejW) for negative frequencies and zero for positive frequencies between -1( and 1(, i.e., 
J'W) 
{Yr(ejw), 
-1( < W < 0 
Y(e  
= 0, 
0 < W < 1( 
Yr[n) -----,r---------~ Yr[n) 1 
',-1_---1 
YIn] = Yr[n] + jYi[n] 
~H(eiw ) I---~ Yi[n] 
Figure P12.26 System for obtaining YIn] from Yrln]. 
12.27. Consider a complex sequence h[n] = hr[nj + jhi[nj, where hr[n] and hi[n] are both real 
sequences, and let H(ejW ) = HR(ejW ) + j H[(ejW ) denote the Fourier transform of h[n], 
where HR(eja» and H/(eJW) are the real and imaginary parts, respectively, of H(eJW). 
Let HER(eJW) and HOR (eja» denote the even and odd parts, respectively, of HR (ejW ), 
and let HEI(ejw ), and HOl(eJW ) denote the even and odd parts, respectively, of H[(eja». 
Furthermore, let HA (ejW) and HB (eJW ) denote the real and imaginary parts of the Fourier 
transform of hrln], and let Hc(ejW ) and HD(eja» denote the real and imaginary parts 
of the Fourjer transform of hi[n]. Express HA(eJW), HB(ejW ), Hc(ejW ), and HD(ejW ) in 
terms of HER (ej(U), HOR (ejW ), HEI(ejW ), and HOl(ejW). 
12.28. The ideal Hilbert transformer (90-degree phase shifter) has frequency response (over one 
period) 
H(ejW ) = {-:-j, w> 0, 
J, 
W <0. 
Figure P12.28-1 shows H(ejW ), and Figure P12.28-2 shows the frequency response of an 
ideallowpass filter Hlp(ejW ) with cutoff frequency We 
1(/2. These frequency responses 
are clearly similar, each having discontinuities separated by 1(. 
ll(eiw) 
j 
-'1T 
7T 
W 
-j 
Figure P12.28-1 

976  
Chapter 12 
Discrete Hilbert Transforms 
11 H,,("i") 
I~  
I 
II 
-271' 
-71' 
71' 
71' 
71' 
271' 
w 
2 
2  
Figure P12.28-2 
(a)  Obtain a relationship that expresses H(e jW ) in terms of Hlp(e jU». Solve this equation 
for Hlp(ejW) in terms of H(e jW ). 
(b)  Use the relationships in part (a) to obtain expressions for h[n] in terms of h1p[n] and 
for hlp[n] in terms of h[n]. 
The relationships obtained in parts (a) and (b) were based on definitions of ideal 
systems with zero phase. However, similar relationships hold for nonideal systems 
with generalized linear phase. 
(c)  Use the results of part (b) to obtain a relationship between the impulse response 
of a causal FIR approximation to the Hilbert transformer and the impulse response 
of a causal FIR approximation to the lowpass filter, both of which are designed by 
(1) incorporating an appropriate linear phase, (2) determining the corresponding 
ideal impulse response, and (3) multiplying by the same window of length (M + 1) 
samples, i.e., by the window method discussed in Chapter 7. (If necessary, consider 
the cases of M even and M odd separately.) 
(d)  For the Hilbert transformer approximations of Example 12.4, sketch the magnitude 
of the frequency responses of the corresponding lowpass filters. 
12.29.  In Section 12.4.3, we discussed an efficient scheme for sampling a bandpass continuous­
time signal with Fourier transform such that 
Sc(jQ) = 0 
for IQI 
Qc 
and IQI::: a c + t.a. 
In that discussion, it was assumed that the signal was initially sampled with sampling fre­
quency 2Jr / T =2(ac + t.a). The bandpass sampling scheme is depicted in Figure 12.12. 
After we form a complex bandpass discrete-time signal s[n] with one-sided Fourier trans­
form S(e jW), the complex signal is decimated by a factor M, which is assumed to be the 
largest integer less than or equal to 2iT / (t.aT). 
(a)  By carrying through an example such as the one depicted in Figure 12.13, show that if 
the quantity 2Jr /(t.aT) is not an integer for the initial sampling rate chosen, then the 
resulting decimated signal sd[n] will have regions of nonzero length where its Fourier 
transform Sd(e jW ) is identically zero. 
(b)  How should the initial sampling frequency 2Jr/ T be chosen so that a decimation factor 
M can be found such that the decimated sequence sd[n] in the system of Figure 12.12 
will have a Fourier transform Sd(e jW ) that is not aliased yet has no regions where it 
is zero over an interval of nonzero length? 
12.30. Consider an LTI system with frequency response, 
H(e jW ) = {1, 0 ~ w ~ Jr, 
0, 
-Jr < W < O. 
The input x [n] to the system is restricted to be real valued and to have a Fourier transform 
(i.e., x[n] is absolutely summable). Determine whether or not it is possible to always 
uniquely recover the system input from the system output. If it is possible, describe how. 
If it is not possible, explain why not. 

977 
Chapter 12 
Problems 
Extension Problems 
12.31. Derive an integral expression for H (z) outside the unit circle in terms of ReiH (e jW») when 
hEn] is a real, stable, and causal sequence, i.e., h[n] = 0 for n > O. 
12.32. Let 1£{.J denote the (ideal) operation of Hilbert transformation; that is, 
00 
1£{x[n]) = L x[k]h[n 
k], 
k=-oo 
where h[n] is 
l
2Sin2(ifn/2) , n #0,
h{n] 
ifn 
0, 
n =0. 
Prove the following properties of the ideal Hilbert transform operator. 
(a)  1£{1l{x[nlll 
-x[n] 
(b) L
00 
x[n]1l{x[n]) = 0 
[Hint: Use Parseval's theorem.] 
11=-00 
(c)  1£{x[n]*y[n]} 
1£{x[n])*y[n] = x[n]*1£{y[nll, wherex{n] and yEn] are any sequences. 
12.33. An ideal Hilbert transformer with impulse response 
l
2 sin2(ifn/2) 
hEn] = 
ifn' n # 0, 
0, 
n =0, 
has input x,[n] and output xi[n] 
x,[n] * h[n], where x,[n] is a discrete-time random 
signal. 
(a)  Find an expression for the autocorrelation sequence tPXjXi[m] in terms of h[nJ and 
tPXrXr Em]. 
(b)  Find an expression for the cross-correlation sequence tPXrXi [m J. Show that in this case, 
tPxrx; [ml is an odd function of m. 
(c)  Find an expression for the autocorrelation function of the complex analytic signal 
x[n] = xr[nJ + jXj[nj. 
(d) Determine the power spectrum Pxx(w) for the complex signal in part (c). 

978  
Chapter 12 
Discrete Hilbert Transforms 
12.34.  In Section 12.4.3, we discussed an efficient scheme for sampling a bandpass continuous­
time signal with Fourier transform such that 
sc(jn) 
0 
for Inl::: nc and Inl::: nc + Ll.n. 
The bandpass sampling scheme is depicted in Figure 12.12. At the end of the section, 
a scheme for reconstructing the original sampled signal sr[n] was given. The original 
continuous-time signal sc(t) in Figure 12.12 can, of course, be reconstructed from sr[n] 
by ideal bandlimited interpolation (ideal Die conversion). Figure P12.34-1 shows a block 
diagram of the system for reconstructing a real continuous-time bandpass signal from a 
decimated complex signal. The complex bandpass filter Hj(ejW) in the figure has a fre­
quency response given by Eq. (12.79). 
Ideal 
1  -I 
Die
Yrd[n] L-J Yre[n] IComplex 
Yr[n] 
converter I Ye(t)
bandpass 
filter 
H;(e iw) 
T
•
Yid[n] 
L-J Yie[n] 
y;[nJ 
Figure P12.34-1 
(a)  Using the example depicted in Figure 12.13, show that the system of Figure P12.34-1 
will reconstruct the original real bandpass signal (i.e., Yc(t) = sc{t» if the inputs to 
the reconstruction system are Yrd[n] = srd[n] and Yid[n] = sid[n]. 
(b) Determine the impulse response hj[n] 
hri[n] + jhji[n] of the complex bandpass 
filter in Figure P12.34-1. 
(c)  Draw a more detailed block diagram of the system of Figure P12.34-1 in which only 
real operations are shown. Eliminate any parts of the diagram that are not necessary 
to compute the final output 
(d)  Now consider placing a complex LTI system between the system of Figure 12.12 and 
the system ofFigure P12.34-1. This is depicted in Figure P12.34-2, where the frequency 
response of the system is denoted H (ejW). Determine how H (ejW ) should be chosen 
if it is desired that 
Yc(jn) = Heff(jn)Sc(jn), 
where 
1,  nc < Inl < nc + Ll.nj2,
Heff(jn) 
{ 0,  otherwise. 
Complex
5rd[n] 
Yrd[n
LTI 
system 
H(eiw) 
sid[n] 
Yid[n] Figure P12.34-2 

Chapter 12 
Problems 
979 
U.3S.  In Section 12.3, we defined a sequence x[n] referred to as the complex cepstrum of a 
sequence x[nJ, and indicated that a causal complex cepstrum x[n] is equivalent to the 
minimum-phase condition of Section 5.4 on x[nJ. The sequencex[n] is the inverse Fourier 
transform of X(ejW ) as defined in Eq. (12.53). Note that because X(ejW) and X(ejW ) are 
defined, the ROC of both X (z) and X(z) must include the unit circle. 
(a)  Justify the statement that the singularities (poles) of X(z) will occur wherever X{z) 
has either poles or zeros. Use this fact to prove that if x[n] is causal, x[n] is minimum 
phase. 
(b)  Justify the statement that ifx[n] is minimum phase the constraints of the ROC require 
.xfn] to be causal. 
We can examine this property forthe case whenx [n] can be written as a superposition 
of complex exponentials. Specifically, consider a sequence x[n] whose z-transform is 
Mi 
Mo 
1
n(1 
akz- ) n(1 
bk Z) 
A k=l 
k=l
X(z) 
Ni 
No
n(1- qz-l) n(1- dk Z) 
k=l 
k=l 
where A > 0 and ak, bb Ck and dk all have magnitude less than one. 
(c)  Write an expression for X(z) = log X (z). 
(d)  Solve for x[n] by taking the inverse z-transform of your answer in part (c). 
(e)  Based on part (d) and the expression for X(z), argue that for sequences x[n] of this 
form, a causal complex cepstrum is equivalent to having minimum phase. 

13  
Cepstrum Analysis 
and ,Homomorphic: 
Deconvolution 
13.0 INTRODUCTION 
Throughout this text, we have focused primarily on linear signal processing methods. 
In this chapter, we introduce a class of nonlinear techniques referred to as cepstrum 
analysis and homomorphic deconvolution. These methods have proven to be effective 
and useful in a variety of applications. In addition, they further illustrate the considerable 
flexibility and sophistication offered by discrete-time signal processing technologies. 
In 1963, Bogert, Healy, and Tukey published a paper with the unusual title "The 
Quefrency Analysis ofTime Series for Echoes: Cepstrum, Pseudoautocovariance, Cross­
Cepstrum, and Saphe Cracking."(See Bogert, Healy and Tukey, 1963.) They observed 
that the logarithm of the power spectrum of a signal containing an echo has an additive 
periodic component due to the echo, and thus, the power spectrum of the logarithm of 
the power spectrum should exhibit a peak at the echo delay. They called this function 
the cepstrum, interchanging letters in the word spectrum because "in general, we find 
ourselves operating on the frequency side in ways customary on the time side and vice 
versa." Bogert et al. went on to define an extensive vocabulary to describe this new sig­
nal processing technique; however, only the terms cepstrum and quefrency have been 
widely used. 
At about the same time, Oppenheim (1964,1967, 1969a) proposed a new class 
of systems called homomorphic systems. Although nonlinear in the classic sense, these 
systems satisfy a generalization of the principle of superposition; i.e., input signals and 
their corrcsponding responses are superimposed (combined) by an operation having 
the same algebraic properties as addition. The concept of homomorphic systems is very 
general, but it has been studied most extensively for the combining operations of mul­
980 

981 
Section 13.1 
Definition of the Cepstrum 
tiplication and convolution, because many signal models involve these operations. The 
transformation of a signal into its cepstrum is a homomorphic transformation that maps 
convolution into addition, and a refined version of the cepstrum is a fundamental part 
of the theory of homomorphic systems for processing signals that have been combined 
by convolution. 
Since the introduction of the cepstrum, the concepts of the cepstrum and homo­
morphic systems have proved useful in signal analysis and have been applied with suc­
cess in processing speech signals (Oppenheim, 1969b, Oppenheim and Schafer, 1968 and 
Schafer and Rabiner, 1970), seismic signals (Ulrych, 1971 and Tribolet, 1979), biomed­
ical signals (Senmoto and Childers, 1972), old acoustic recordings (Stockham, Cannon 
and Ingebretsen, 1975), and sonar signals (Reut, Pace and Heator, 1985). The cepstrum 
has also been proposed as the basis for spectrum analysis (Stoica and Moses, Z005). This 
chapter provides a detailed treatment ofthe properties and computational issues associ­
ated with the cepstrum and with deconvolution based on homomorphic systems. A num­
ber of these concepts are illustrated in Section 13.10 in the context of speech processing. 
13.1 DEFINITION OF THE CEPSTRUM 
The original motivation for the cepstrum as defined by Bogert et al. is illustrated by the 
following simple example. Consider a sampled signal x[n] that consists of the sum of a 
signal v[n] and a shifted and scaled copy (echo) of that signal; i.e., 
x[n] 
v[n] + av[n - no] 
v[n] * (o[n] + ao[n 
noD. 
(13.1) 
Noting that x[n] can be represented as a convolution, it follows that the discrete-time 
Fourier transform of such a signal has the form of a product 
X(ej(J)) = V(ej (J))[l +ae-jwno ]. 
(13.2) 
The magnitUde of X (e jW) is 
!X(ejW)1 = IV(ejW )I(l + a 2 + 2a cos(wnO»1/2, 
(13.3) 
a real even function of w. The basic observation motivating the cepstrum was that the 
logarithm of the product such as in Eq. (13.3) would be a sum of two corresponding 
terms, specifically 
log /X(eJW)1 = log IV(eJW )I+ i logO + a 2 +Za cos(wno)). 
(13.4) 
For convenience, we define Cx(eJW ) = log IX (eJW ) I. Also, in anticipation of a discussion 
in which we will want to stress the duality between the time- and frequency- domains, 
we substitute w 
ZJrj to obtain 
CAeJ2nf 
log IX(e J27rf) I 
log IV(e j2Jrf
) = 
= 
) I+ ~ logO + a 2 + Za cos(ZJrjno». (13.5) 
There are two components to this real function of normalized frequency j. The 
term log IV (eJ2nf ) 1is due solely to the signal v[n], and the second term, log(l + a 2 + 
Za cos(ZJrjno)) is due to the combination (echoing) of the signal with itself. We can 
think of CAeJ2Jrf ) as a waveform with continuous independent variable j. The part 
due to the echo will be periodic in j with period l/no.1 We are used to the notion 
1Because logO + a 2 + 2ev cos(2nf no» is the log-magnitude of a DTFT, it is also periodic in f with 
period one (2n in w), as well as 1/no. 

982 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
that a periodic time waveform has a line spectrum, i.e., its spectrum is concentrated at 
integer multiples of a common fundamental frequency, which is the reciprocal of the 
fundamental period. In this case, we have a "waveform" that is a real, even function 
of f (Le., frequency). Fourier analysis appropriate for a continuous-variable periodic 
function such as Cx(ej2Jrf) would naturally be the inverse DTFf; i.e., 
Jr
1 l
11/2
cx[n] = -
Cx(ejUJ)ejUJndw = 
CAei2Jrf)ei2Jrfndf. 
(13.6)
2:rr 
-71: 
-1/2 
In the terminology of Bogert et aI., cAn] is referred to as the cepstrum of Cx(ej2Jrf) 
(or equivalently, of x[n] since CAei2Jrf ) is derived directly from x[n]). Although the 
cepstrum defined as in Eq. (13.6) is clearly a function of a discrete-time index n, Bogert 
et ai. introduced the term "quefrency" to draw a distinction between the cepstrum time 
domain and that of the original signaL Because the term 10g(1 + a2 + 2acos(2:rrfno» 
in Cx(ei2Jrf) is periodic in f with period 1/no, the corresponding component in cx[n] 
will be nonzero only at integer multiples of no, the fundamental quefrency of the term 
10g(1 + a2 + 2a cos(2:rrfno)). Later in this chapter, we will show that for this example 
of a simple echo with lal < 1, the cepstrum has the form 
00 
k 
cx[n] 
cv[n] + 2)-ll+1 ~k (8[n + kno] + 8[n 
kno]), 
(13.7) 
k=l 
where cv[n] is the inverse DTFf of log IV (ejW)I, (i.e., the cepstrum of v[n]), and the 
discrete impulses involve only the echo parameters a and no. It was this result that 
led Bogert et ai. to observe that the cepstrum of a signal with an echo had a "peak" 
at the echo delay time no that stands out clearly from cv[n]. Thus the cepstrum could 
be used as the basis for detecting echoes. As mentioned above, the strange-sounding 
terms "cepstrum" and "quefrency" and other terms were created to call attention to a 
new way of thinking about Fourier analysis of signals wherein the time and frequency 
domains were interchanged. In the remainder of this chapter, we will generalize the 
concept ofcepstrum by using the complex logarithm, and we will show many interesting 
properties of the resulting mathematical definition. Furthermore, we will see that the 
complex cepstrum can also serve as the basis for separating signals that are combined 
by convolution. 
13.2 DEFINITION OF THE COMPLEX CEPSTRUM 
As the basis for generalizing the concept of the cepstrum, consider a stable sequence 
x[n] whose z-transform expressed in polar form is 
X(z) = IX(z)leiLx(z), 
(13.8) 
where IX(z)1 and LX(z) are the magnitude and angle, respectively, of the complex 
function X ez). Since x[n] is stable, the ROC for X ez) includes the unit circle, and the 
DTFf of x[n] exists and is equal to X(eiw). The complex cepstrum associated with x[n] 

983 
Section 13.2 
Definition of the Complex Cepstrum 
is defined to be the stable sequence x[n],2 whose z-transform is 
X(z) 
10g[X(z)]. 
(13.9) 
Although any base can be used for the logarithm, the natural logarithm (Le., base e) 
is typically used and will be assumed throughout the remainder of the discussion. The 
logarithm of a complex quantity X (z) expressed as in Eq. (13.8) is defined as 
10g[X(z)] = 10g[IX(z)leiLx(zl] 
log IX(z)1 + jLX(z). 
(13.10) 
Since in the polar representation of a complex number the angle is unique only to within 
integer multiples of 211', the imaginary part of Eq. (13.10) is not well defined. We will 
address that issue shortly; for now we assume that an appropriate definition is possible 
and has been used. 
The complex cepstrum exists if 10g[X(z)] has a convergent power series represen­
tation of the form 
X(z) =10g[X(z)] = L
00 
x[n)z-n, 
Izi = 1, 
(13.11) 
n=-oo 
i.e., X(z) = 10g[X(z)] must have all the properties of the z-transform of a stable se­
quence. Specifically, the ROC for the power series representation of10g[X(z)] must be 
of the form 
(13.12) 
where 0 < TR < 1 < TL. If this is the case, x[n], the sequence of coefficients of the power 
series, is what we call the complex cepstrum of x[n]. 
Since we require x[n] to be stable, the ROC of X(z) includes the unit circle, and 
the complex cepstrum can be represented using the inverse DTFT as 
1 fn: 
..
x[n] = 
10g[X(elW)]elWndw 
211' -n: 
(13.13) 
~ fn: [log IX(eiw)1 + j LX(eiw)]eiwndw. 
211' -n: 
The term complex cepstrum distinguishes our more general definition from the 
original definition of the cepstrum by Bogert et al. (1963), which was originally stated in 
terms of the power spectrum of continuous-time signals. The use of the word complex 
in this context implies that the complex logarithm is used in the definition. It does not 
imply that the complex cepstrum is necessarily a complex-valued sequence. Indeed, as 
we will see shortly, the definition we choose for the complex logarithm ensures that the 
complex cepstrum of a real sequence will also be a real sequence. 
The operation of mapping a sequence x[n] into its complex cepstrum x[n] is de­
noted as a discrete-time system operator D* [.]; i.e., x = D* [x]. This operation is depicted 
as the block diagram on the left in Figure 13.1. Similarly, since Eq. (13.9) is invertible 
with the complex exponential function, we can also define the inverse system D;l [.] 
2In a somewhat more general definition of the complex: cepstrum, x[n1and x[n1 need not be restricted 
to be stable. However, with the restriction of stability the important concepts can be illustrated with simpler 
notation than in the general case. 

984 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
Figure 13.1 
System notation for the
x[n] I 
~] 
Y[n] I 
. ~]
--+1)10. 
D.[] ! 
-~
.. : 
D;l[J !  
mapping and inverse mapping between 
a signal and its complex cepstrum. 
which recovers x[n] from x[n). The block diagram representation of D;l[.] is shown on 
the right in Figure 13.1. Specifically, D*[.) and D;l[.) in Figure 13.1 are defined so that if 
y[n] 
x[n] in Figure 13.1, then yen] = x[n]. In the context of homomorphic filtering of 
convolved signals to be discussed in Section 13.8, D. [.] is called the characteristic system 
for convolution. 
As introduced in Section 13.1, the cepstrum cx[n] of a signal3 is defined as the 
inverse Fourier transform of the logarithm of the magnitude of the Fourier transform; 
i.e., 
1
cx[n] 
2 jJr log IX(eja»leja>nd  
(13.14)
:rr 
-Jr  
w. 
Since the Fourier transform magnitude is real and nonnegative, no special considera­
tions are involved in defining the logarithm in Eq. (13.14). By comparing Eq. (13.14) 
and Eq. (13.13), we see that cx[n] is the inverse transform of the real part of X(eja». 
Consequently cx[n] is equal to the conjugate-symmetric part of x[n]; Le., 
cx[n] 
x[n]+;*[-n]  
(13.15) 
The cepstrum is useful in many applications, and since it does not depend on the phase 
of X(ejlV ), it is much easier to compute than the complex cepstrum. However, since it 
is based on only the Fourier transform magnitude, it is not invertible, i.e., x[n] cannot 
in general be recovered from cx[n], except in special cases. The complex cepstrum is 
somewhat more difficult to compute, but it is invertible. Since the complex cepstrum 
is a more general concept than the cepstrum, and since the properties of the cepstrum 
can be derived from the properties of the complex cepstrum using Eq. (13.15), we will 
emphasize the complex cepstrum in this chapter. 
The additional difficulties encountered in defining and computing the complex 
cepstrum are worthwhile for a variety of reasons. First, we see from Eq. (13.10) that the 
complex logarithm has the effect of creating a new Fourier transform whose real and 
imaginary parts are log IX(eja» I and f. X(ejlV), respectively. Thus, we can obtain Hilbert 
transform relations between these two quantities when the complex cepstrum is causal. 
We discuss this point further in Section 13.5.2 and see in particular how it relates to 
minimum-phase sequences. A second more general motivation, developed in Section 
13.8, stems from the role that the complex cepstrum plays in defining a class of systems 
for separating and filtering signals that are combined by convolution. 
13.3 PROPERTIES OF THE COMPLEX LOGARITHM 
Since the complex logarithm plays a key role in the definition of the complex cepstrum, 
it is important to understand its definition and properties. Ambiguity in the definition 
3ex (n1is also referred to as the real cepstrum to emphasize that it corresponds to only the real part of 
the complex logarithm. 

Section 13.4 
Alternative Expressions for the Complex Cepstrum 
985 
of the complex logarithm causes serious computational issues. These will be discussed 
in detail in Section 13.6. A sequence has a complex cepstrum if the logarithm of its 
z-transform has a power series expansion, as in Eq. (13.11), where we have specified 
the ROC to include the unit circle. This means that the Fourier transform 
(13.16) 
must be a continuous, periodic function of w, and consequently, both log IX (eJW )I and 
LX (eJW ) must be continuous functions of w. Provided that X (z) does not have zeros on 
the unit circle, the continuity of log IX (eiW)1 is guaranteed, since X (eiw) is assumed to be 
analytic on the unit circle. However, as previously discussed in Section 5.1.1, LX (eJW ) is 
in general ambiguous, since at each w, any integer multiple of2rr can be added, and con­
tinuity of LX(e}W) is dependent on how the ambiguity is resolved. Since ARG[X(eJW)] 
can be discontinuous, it is generally necessary to specify LX (eJW ) explicitly in Eq. (13.16) 
as the unwrapped (i.e., continuous) phase curve arg[X(e}W)]. 
It is important to note that if X(z) = Xl (Z)X2(Z), then 
arg[X(eJW )] 
arg[Xl (eJW)] + arg[X2(eJW)]. 
(13.17) 
A similar additive property will not hold for ARG[X (eJW)], i.e., in general, 
ARG[X(eJW)] =1= ARG[Xl(eJW)] + ARG[X2(eJW )]. 
(13.18) 
Therefore, in order that X(ejW) be analytic (continuous) and have the property that if 
X(eJW ) = Xl (eJW)X2(eJW), then 
(13.19) 
we must define X(eiUJ ) as 
X(eiw) 
log IX(ejUJ) I + jarg[X(eJUJ)]. 
(13.20) 
With x[nJ real, arg[X(ejW)] can always be specified so that it is an odd periodic function 
of w. With arg[X(eJW)] an odd function of wand log IX(ejW)1 an even function of w, the 
complex cepstrum x[n] is guaranteed to be rea1.4 
13.4 ALTERNATIVE EXPRESSIONS FOR THE COMPLEX 
CEPSTRUM 
So far we have defined the complex cepstrum as the sequence ofcoefficients inthe power 
series representation of X(z) = 10g[X(z)], and we have also given an integral formula in 
Eq. (13.13) for determining x[n] from X(eiUJ ) 
log IX(eJW)1 + LX(eiw ), where LX (eiw) 
is the unwrapped phase function arg[X(eJW)]. The logarithmic derivative can be used 
to derive other relations for the complex cepstrum that do not explicitly involve the 
complex logarithm. Assuming that 10g[X(z)] is analytic, then 
X'( ) = X'(z) 
(13.21) 
z 
X(z) 
41be approach outlined above to the problems presented by the complex logarithm can be developed 
more formally through the concept of the Riemann surface (Brown and Churchill, 20(8). 

Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution
986 
where' denotes differentiation with respect to z. From property 4 in Table 3.2, zX'(z) 
is the z-transform of -nf[n], i.e., 
~[]  z 
X~'('
-nx n  +----)0- z 
Z) 
(13.22) 
Consequently, from Eq. (13.21), 
A[]  Z 
zX'(z)
- nx n 
+----)0- --. 
(13.23)
X(z) 
Beginning with Eq. (13.21) we can also derive a difference equation that is satisfied 
by x[n] and x[n]. Rearranging Eq. (13.21) and multiplying by z, we obtain 
zX'(z) = zX'(z)· X(z). 
(13.24) 
Using Eq. (13.22), the inverse z-transform of this equation is 
00 
-nx[n] = L (-k.i[k])x[n 
k]. 
(13.25) 
k=-oo 
Dividing both sides by -n, we obtain 
x[n] = f (~) x[k]x[n 
k], 
n #0. 
(13.26) 
k=-oo 
The value of X[O] can be obtained by noting that 
JT
1 l
A 
•
x[O]  
21i 
-JT X(e1W)dw. 
(13.27) 
Since the imaginary part of XCe jW ) is an odd function of w, Eq. (13.27) becomes 
JT 
1 l
.
frO]  
-
log IX(e1W)ldw. 
(13.28)
21i 
-JT 
In summary, a signal and its complex cepstrum satisfy a nonlinear difference equa­
tion (Eq. (13.26». Under certain conditions, this implicit relation between x[n] and x[n] 
can be rearranged into a recursion formula that can be used in computation. Formulas 
of this type are discussed in Section 13.6.4. 
13.5  THE COMPLEX CEPSTRUM FOR EXPONENTIAL, 
MINIMUM-PHASE AND MAXIMUM-PHASE 
SE~UENCES 
13.5.1 Exponential Sequences 
If a sequence x[n] consists of a sum of complex exponential sequences, its z-transform 
X(z) is a rational function of z. Such sequences are both useful and amenable to analysis. 
In this section, we consider the complex cepstrum for stable sequences x[n] whose z­
transforms are of the form 
Mi 
Mo 
Az
T fl (1  
akz-1) fl (1 - bkZ) 
X (z) = 
k=l 
k=l 
(13.29)
Ni 
No 
1
fl (1 - CkZ- ) fl (1 
dkZ) 
k=l 
k=l 

981 
Section 13.5 
Properties of the Complex Cepstrum 
where lad, Ibk I, ICkI, and Idkl are allless than unity, so that factors of the form (1-akZ-1) 
and (1 
CkZ- 1) correspond to the M; zeros and the N; poles inside the unit circle, and 
the factors (1 - bkZ) and (1- dkZ) correspond to the Mo zeros and the No poles outside 
the unit circle. Such z-transforms are characteristic of sequences composed of a sum 
of stable exponential sequences. In the special case where there are no poles (i.e., the 
denominator ofEq. (13.29) is unity), then the corresponding sequence x[n] is a sequence 
of finite length (M + 1 
Mo + M; + 1). 
Through the properties ofthe complex logarithm, the product ofterms in Eq. (13.29) 
is transformed to the sum of logarithmic terms: 
Mi 
Mo 
X(z) 
10g(A) + log(zr) + I)og(1 
akz-1) + I)og(1 
bkZ) 
k=l 
k=l 
(13.30) 
Ni 
No 
Llog(1- CkZ-1) L 10g(1 - dkZ). 
k=l 
k=l 
The properties of xln] depend on the composite properties of the inverse transforms of 
each term. 
For real sequences, A is real, and if A is positive, the first term 10g(A) contributes 
only to itO]. Specifically, (see Problem 13.15), 
itO] = log IAI. 
(13.31 ) 
If A is negative, it is less straightforward to determine the contribution to the complex 
cepstrum due to the term 10g(A). The term zr corresponds only to a delay or advance 
of the sequence x[n]. If r = 0, this term vanishes from Eq. (13.30). However, if r :j:. 0, 
then the unwrapped phase function arg[X(ejUJ)] will include a linear term with slope 
r. Consequently, with arg[X(eja»] defined to be odd and periodic in w and continuous 
for Iwl < JT:, this linear-phase term will force a discontinuity in arg[X (eja»] at w = 
and X(z) will no longer be analytic on the unit circle. Although the cases of A negative 
and/or r :j:. 0 can be formally accommodated, doing so seems to offer no real advantage, 
because if two transforms of the form of Eq. (13.29) are multiplied together, we would 
not expect to be able to determine how much of either A or r was contributed by each 
component. This is analogous to the situation in ordinary linear filtering where two 
signals, each with dc levels, have been added. Therefore, this question can be avoided in 
practice by first determining the algebraic sign of A and the value of r and then altering 
the input, so that its z-transform is of the form 
Mj 
Mo 
1
IAI n(1 
akz- ) n(1 
X (z) = ___k_==-l____k=-==-l___ 
(13.32)
Ni 
No
n(1- CkZ-1) n(1- dkZ) 
k=l 
k=l 

988 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
Correspondingly, Eq. (13.30) becomes 
M, 
Mo 
X(z) = log IAI + I)og(l 
akz-1) + L 10g(1 - bkz) 
k=l 
k=l 
(13.33)
N; 
No 
- Llog(l 
CkZ-1) L log(l - dkZ). 
k=l 
k=l 
With the exception of the term log IAI, which we have already considered, all the terms 
in Eq. (13.33) are of the form 10g(1- ac1) and 10g(1- f3z). Bearing in mind that these 
factors represent z-transforms with regions of convergence that include the unit circle, 
we can make the power series expansions 
fan -n
10g(1- az-1) = 
-z , 
. Izi > lal, 
(13.34) 
n
n=l 
00 fJn 
log(l 
fJz) = L-zn, 
Izl < IfJ-11. 
(13.35) 
n
n=l 
Using these expressions, we see that for signals with rational z-transforms as in 
Eq. (13.32), x[n] has the general form 
log IAI, 
n =0, 
(13.36a) 
M; 
n 
N; 
n
L ak 
_ 
+ L 
ek
, 
n > 0, 
(13.36b) 
n 
n
x[n] = 
k=l 
k=l  
Mo b-n 
No d-n  
n < O. 
(13.36c)
L--;- L --;-, 
k=l 
k=l 
Note that for the special case of a finite-length sequence, the second term would 
be missing in each of Eqs. (13.36b) and (13.36c). Equations (13.36a) to (13.36c) suggest 
the following general properties of the complex cepstrum: 
Property 1: The complex cepstrum decays at least as fast as l/lnl: Specifically, 
lnl
a 
Ix[n]1 < C-, 
-00 < n < 00,
Inl 
where C is a constant and a equals the maximum of lakl, Ibkl, hi, and Idkl.5 
Property 2: x[n] will have infinite duration, even if x[n] has finite duration. 
Property 3: Ifx [n] is real, x[n] is also real. 
5In practice, we generally deal with finite-length signals, which are represented by polynomials in z~l; 
i.e., the numerator in Eq. (13.32). In many cases, the sequence may be hundreds or thousands of samples long. 
For such sequenees, as the sequence length increases, it is increasingly likely that almost all of the zeros of 
the polynomial will cluster around the unit circle (Hughes and NikeghbaJi, 2005). This implies that for long 
finite-length sequences, the decay of the complex cepstrum is due primarily to the factor 1/n. 

989 
Section 13.5 
Properties of the Complex Cepstrum 
Properties 1 and 2 follow directly from Eqs. (13.36a) to (13.36c). We have suggested 
property 3 earlier on the basis that for x[nJ real, log IX(ejW)1 is even and arg[X(ejW)] is 
odd, so that the inverse transform of 
X(e jW ) 
log IX(ejW)1 + jarg[X(ejW)] 
is real. To see property 3 in the context of this section, we note that if x[n] is real, then 
the poles and zeros of X (z) are in complex conjugate pairs. Therefore, for every complex 
term of the form an In in Eqs. (13.36a) to (13.36c) there will be a complex conjugate 
term (a*)nI n, so that their sum will be real. 
13.5.2 Minimum-Phase and Maximum-Phase Sequences 
As discussed in Chapters 5 and 12, a minimum-phase sequence is a real, causal, and 
stable sequence with all the poles and zeros of the z-transform inside the unit circle. 
Note that 10g[X(z)] has singularities at both the poles and the zeros of X(z). Since we 
require that the ROC of 10g[X(z)] include the unit circle so that x[n] is stable, and since 
causal sequences have an ROC of the form rR < Izl, it follows that there can be no 
singularities of 10g[X(z)] on or outside the unit circle if x[n] = 0 for n < O. Conversely, 
if all the singularities of X(z) = 10g[X(z)] are inside the unit circle, then it follows that 
x[n] = 0 for n < O. Since the singularities of X(z) are the poles and the zeros of X(z), 
the complex cepstrum of x[n] will be causal (x[n] 
0 for n < 0) if and only if the poles 
and zeros of X(z) are inside the unit circle. In other words, x[n] is a minimum-phase 
sequence if and only if its complex cepstrum is causal. 
This is easily seen for the case of exponential or finite-length sequences by con­
sidering Eqs. (13.36a)-(13.36c). Clearly, all terms in Eq. (13.36c) will be zero if all the 
coefficients bk and dk are zero, i.e., if there are no poles or zeros outside or on the unit 
circle. Thus, another property of the complex cepstrum is 
Property 4: The complex cepstrum x[n] 
0 for n < 0 if and only if x[n] is minimum 
phase, i.e., X (z) has all its poles and zeros inside the unit circle. 
Therefore, causality of the complex cepstrum is equivalent to the minimum phase lag, 
minimum group delay, and minimum energy delay properties that also characterize 
minimum-phase sequences. 
Example 13.1 
Complex Cepstrum of a Minimum-Phase Echo 
System 
The concept of the cepstrum arose initially from a consideration of echoes. As we 
showed in Section 13.1, a signal with an echo is represented by a convolution 
x[n] = v[n] *p[n], where 
no
p[n] = .5[n] + a.5[n - nO] ~ P(z) = 1 + az-
. 
(13.37) 
The zeros of P(z) are at locations Zk 
al/noej2rr(k+l/2)/no, and if lal < 1, all the 
zeros will lie inside the unit circle, in which case p[n] is a minimum-phase system. To 

990 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
find the complex cepstrum p[n], we can use the power series expansion of 10g[P(z)] 
as in Section 13.5.1 to obtain 
, 
00 (_a)n 
P(z) = log[l + az-no ] = - L __z-nno, 
(13.38) 
n 
n=l 
from which it follows that 
00 
m 
p[n] = L(-1)m+l~8[n mno]. 
(13.39) 
m
m=l 
From Eq. (13.39), we see that i)[nJ = 0 for n < 0 for JaJ < 1 as it should be for a 
minimum-phase system. Furthermore, we see that the nonzero values of the complex 
cepstrum for the minimum-phase echo system occur at positive integer multiples of no. 
Maximum-phase sequences are stable sequences whose poles and zeros are all 
outside the unit circle. Thus, maximum-phase sequences are left-sided, and, by analogous 
arguments, it follows that the complex cepstrum of a maximum-phase sequence is also 
left-sided. Thus, another property of the complex cepstrum is: 
Property 5: The complex cepstrum x[n] 
0 for n > 0 if and only if x[n] is maximum 
phase; i.e., X(z) has all its poles and zeros outside the unit circle. 
This property of the complex cepstrum is easily verified for exponential or finite­
length sequences by noting that if all the qs and akS are zero (i.e., no poles or zeros 
inside the unit circle), then Eq. (13.36b) shows that x[n) = 0 for n > O. 
In Example 13.1, we determined the complex cepstrum of the impulse response 
of the echo system when lal < 1; i.e., when the echo is smaller than the direct signal. If 
laI > 1, the echo is larger than the direct signal, and the zeros of the system function 
P(z) = 1+az-no lie outside the unit circle. In this case, the echo system is a maximum­
phase system.6 The corresponding complex cepstrum is 
00 
-m 
p[n) = loglaI8[n) + L(-1)m+l~-8[n +mno]. 
(13.40) 
m 
m=l 
From Eq. (13.40) we see that p[n) = 0 for n > 0 for lal > 1 as it should be for 
a maximum-phase system. In this case, we see that the nonzero values of the complex 
cepstrum for the maximum-phase echo system occur at negative integer multiples of no. 
13.5.3  Relationship Between the Real Cepstrum and the 
Complex Cepstrum 
As discussed in Sections 13.1 and 13.2, the Fourier transform ofthe real cepstrum cx[n] 
is the real part of the Fourier transform of the complex cepstrum x[n], and equivalently, 
cAn) corresponds to the even part of x[n). i.e., 
x[n] +x[-n]
cx[n)  
(13.41)
2 
6P(z) = eno (a + znO) has no poles at z = 0, which are ignored in computing p[n/ 

Section 13.5 
Properties of the Complex Cepstrum 
991 
Fourier 
transform 
log 1·1 
Inverse 
Fourier 
transform 
Figure 13.2 Determination ofthe complex cepstrum for minimum-phase signals. 
If x[n] is causal, as it is if x[n] is minimum phase, then Eq. (13.41) is reversible, 
i.e., x[n] can be recovered from cx[n] by applying an appropriate window to cx[n]. 
Specifically, 
(13.42a) 
where 
2 n > 0  
imin[n] = 2u[n] 
o[n] = 
1 nO. 
(13.42b)  
On < 0 
1 
Equations (13.42a) and (13.42b) indicate how the complex cepstrum can be ob­
tained from the cepstrum and consequently also from the log magnitude alone if x[n1is 
known to be minim um phase. This is also illustrated in block diagram form in Figure 13.2. 
In the following example, we illustrate Eqs. (13.41) and (13.42a) for the minimum­
phase echo system of Example 13.1. 
Example I 3.2 
Real Cepstrum of a Minimum-Phase 
Echo System 
Consider the complex cepstrum ofthe minimum-phase echosystem as given in Eq. (13.39) 
in Example 13.1. From Eq. (13.41) it follows that the real cepstrum for the minimum­
phase echo system is 
1 00m 
'\' 
m+l a
-( L, (-1) 
-o[n - mnO]
2 
m 
m=l 
00 
m 
'\' 
1 m+l a
+ L, (-) 
-o[-n - mnO]. 
(13.43) 
m 
m=l 
Since o[-n] = o[n], Eq. (13.43) can be written in the more compact form 
L 
00 
(_l)m+l ~ 
m 
2m 
(o[n ­
mnO] + o[n + mnoJ) . 
(13.44) 
m=l 
Also note that ifcp[n] is given by Eq. (13.44) and fmin[n] is given by Eq. (13.42b), then 
emin[n]cp[n] is equal to forn] in Eq. (13.39). 

992 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
,-------------------------------------------.I 
X(ejw) 
X(ejW) 
Inverse 
~ 
Fourier 
Complex 
: i[n] 
transform 
logarithm 
Fourier ~ 
transform 
I 
I 
._------------------------------------ ______ 1 
D.[ ] 
Figure 13.3 Cascade of three systems implementing the computation of the 
complex cepstrum operation 0*[ ]. 
13.6 COMPUTATION OF THE COMPLEX CEPSTRUM 
The practical use of the complex cepstrum requires accurate and efficient computational 
methods to obtain it from a sampled signal. Implicit in all of the previous discussions 
has been the assumption of uniqueness and continuity of the complex logarithm of 
the Fourier transform of the input signal. If the mathematical representations obtained 
above are to serve as the basis for computation of the complex cepstrum, or equivalently, 
as the basis for realizations of the system D*[·], then we must deal with the issues 
associated with computing the Fourier transform and the complex logarithm. 
The system D*[·] is represented in terms of the Fourier transform by the equations 
X(e jW ) = L
00 
x[n]e- jwn , 
(13.4Sa) 
n=-oo 
X(e jW ) = 10g[X(eiw)],  
(13.4Sb) 
x[n]  ~ f7r X(ejw)ei())ndw. 
(13.4Sc)
2n 
-'IT 
These equations correspond to the cascade of three systems as depicted in Figure 13.3. 
In computing the complex cepstrum numerically, we are limited to finite-length 
input sequences, and we can compute the Fourier transform at only a finite number of 
frequencies. That is, instead of using the DTFI, we must use the DFI. Thus, instead of 
Eqs. (13.4Sa) to (13.4Sc), we have the computational realization 
N-l 
X[k] = X(e jW ) 
= L x[n]e-J(2'IT/N)kn, 
(13.46a)
I 
w=(2'IT / N)k 
n=0 
X[k] 
10g[X(ejW )] I 
' 
(13.46b) 
w=(2'IT/N)k 
N-l 
xp[n] = ! L X[k]e j (2'IT/N)kn. 
(13.46c) 
k=O 
These operations are depicted in Figure 13.4(a), and the corresponding operations for 
realizing the inverse system are depicted in Figure 13.4(b). 
Since in Eq. (13.46b) X[k] is a sampled version of X(e jW), it follows from the 
discussion in Section 8.4 that xp[n] will be a time-aliased version of x[n], i.e., thatXp[n] 

993 
Section 13.6 
Computation of the Complex Cepstrum 
I 
x[n] : 
~ 
I  
I  
DFT 
XIk] 
Complex 
logarithm 
I-------------------------------------------~ 
XIk] 
lOFT 
: xpln] 
I 
I 
I 
I 
._------------------------------------------, 
D.[ ] 
(a) 
.-------------------------------------------. 
y~ [n] 
I 
I 
~ 
I  
I , __________________________________________ 
DFT 
Yp[k] 
Complex 
exponential 
Yp[k] 
lOFT 
I 
: yp[n] 
I ,I 
.1
• 
D;ll] 
(b) 
Figure 13.4 Approximate realization using the OFT of (a) D*[ .J and (b) D;1[ .J. 
is related to the desired x[nJ by 
xp[nJ = L
00 
x[n + rNJ. 
(13.47) 
r=-OQ 
However, we noted in Property 1 in Section 13.5 that x[n] decays faster than an exponen­
tial sequence, so it is to be expected that the approximation would become increasingly 
better as N increases. By appending zeros to an input sequence, it is generally possible 
to increase the sampling rate of the complex logarithm of the Fourier transform so that 
severe time aliasing does not occur in the computation of the complex cepstrum. 
13.6.1 Phase Unwrapping 
Samples of X(ejW) as given by Eq. (13.46b) require samples of logl X (e jW ) I and 
arg[X (e jW)]. SamplesofloglX (ej(O)Iata suitable sampling rate can be computed by com­
puting the DFfofx[n) with zero padding. SamplesARG[X(ej(V»), i.e., the phase modulo 
2rr are likewise straightforward to compute from samples of X (ejW) by using standard 
inverse tangent routines available in most high-level computer languages. However, to 
obtain the complex cepstrum or its aliased version xp[n], we require samples of the 
unwrapped phase arg[X (ejW»). Consequently, effective procedures for unwrapping the 
phase, that is, obtaining samples ofthe unwrapped phase from samples ofthe phase mod­
ulo 2rr, become an important computational aspect of obtaining the complex cepstrum. 
To illustrate the issues, consider a finite-length causal input sequence whose Fourier 
transform is of the form 
M  
X(ej(V) = Lx[n]e-jwn  
n=O 
(13.48)
M, 
Mo 
Ae-jwMo n(1 -
ake- )W) n(1 
bkejw), 
k=l 
k=l 

994 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
arg(X[k]) 
, 
k 
ARG(X[kD 
.. 
....... .  
~
7T 
• 
II 
'/ 
k 
NI2 
-7T •.. ............. ~....... ~,....... .  
(b) 
27Tr[k] 
• 
• 
• 
• 
• 
• 
k 
NI2 
-27T I.... 
• •• ............... _ 
........................ .  
Figure 13.5 
(a) Samples of 
-47T ........ ..  
• 
• • • • • • • • HqU. 
arg[X(eJW»). (b) Principal value of 
part (a). (c) Correction sequence for 
(c) 
obtaining arg from ARG. 
where lak I and Ibk I are less than unity, M = Mo + Mi, and A is positive. A continuous­
phase curve for a sequence of this form is shown in Figure 13.5(a). The dots indicate 
samples at frequencies Wk = (2rr / N)k. Figure 13.5(b) shows the principal value and its 
samples as computed from the DFf of the input sequence. One approach to unwrapping 
the principal-value phase is based on the relation 
arg(X[kD = ARG(X[kD +2u[k], 
(13.49) 
where r[k] denotes an integer that determines the appropriate multiple of2rr to add to 
the principal value at frequency WI< = 2nk/N. Figure 13.5(c) shows 2nr[k] required to 
obtain Figure 13.5(a) from 13.5(b). This example suggests the following algorithm for 
computing r[k] from ARG(X[kD starting with r[O] = 0: 

995 
Section 13.6 
Computation of the Complex Cepstrum 
1. If ARG(X[k]) - ARG(X[k - 1]) > 2n - 81. then r[k] 
r[k - 1] - 1. 
2. If ARG(X[k]) 
ARG(X[k 
1]) < -(2n - 81), then r[k] 
r[k - 1] + 1. 
3. Otherwise, r[k] 
r[k - 1]. 
4. Repeat steps 1 
3 for 1 ::s k < N /2. 
After r[k] is determined, Eq. (13.49) can be used to compute arg(X[k]) for 0 ::s k < N /2. 
At this stage, arg(X[k]) will contain a large linear-phase component due to the factor 
e-JwMo in Eq. (13.48). This can be removed by adding 2nkMo/ N to the unwrapped 
phase over the interval 0 ::s k < N /2. The values of arg(X[k]) for N /2 < k ::s N - 1 can 
be obtained by using symmetry. Finally, arg(X[N/2]) = O. 
The above algorithm works well if the samples of ARG(X[k]) are close enough 
together so that the discontinuities can be detected reliably. The parameter 81 is a toler­
ance recognizing that the magnitude of the difference between adjacent samples of the 
principal-value phase will always be less than 2n. If81 is too large, a discontinuity will be 
indicated where there is none. If 8t is too small, the algorithm will miss a discontinuity 
falling between two adjacent samples of a rapidly varying unwrapped phase function 
arg[X(ej(.!J)]. Obviously, increasing the sampling rate of the DFT by increasing N im­
proves the chances of correctly detecting discontinuities, and thus, correctly computing 
arg(X[k]). If arg[X (eJW )] varies rapidly, then we expect i[n] to decay less rapidly than 
if arg[X(ejwn varied more slowly. Therefore, aliasing of i[n] is more of a problem for 
rapidly varying phase. Increasing the value of N reduces the aliasing of the complex 
cepstrum and also improves the chances of being able to correctly unwrap the phase of 
X[k] by the previously described algorithm. 
In some cases, the simple algorithm we just developed may fail because it is im­
possible or impractical to use a large enough value for N. Often, the aliasing for a given 
N is acceptable, but principal-value discontinuities cannot be reliably detected. Tribo­
let (1977, 1979) proposed a modification of the algorithm that uses both the principal 
value of the phase and the phase derivative to compute the unwrapped phase. As above, 
Eq. (13.49) gives the set of permissible values at frequency Wk = (2n/ N)k, and we seek 
to determine r[k]. It is assumed that we know the phase derivative, 
d 
I 
dw 
w=21rk/N 
arg'(X[k]) 
-arg[X(eJW. )] 
at all values of k. (A procedure for computing these samples of the phase derivative 
will be developed in Section 13.6.2.) To compute arg(X[k]) we further assume that 
arg(X[k 
1]) is known. Then, arg(X[k]), the estimate of arg(X[k]), is defined as 
_ 
. 
t::.w, 
I 
( 
)
arg(X[k]) = arg(X[k - 1]) + T{arg (X[kD + arg (X[k 
I])}. 
13.50 
Equation (13.50) is obtained by applying trapezoidal numerical integration to the sam­
ples of the phase derivative. This estimate is said to be consistent iffor some 82 an integer 
r[k] exists such that 
larg(X[k]) 
ARG(X[kD - 2nr[k]1 < 82 < n. 
(13.51) 
Obviously, the estimate improves with decreasing numerical integration step size t::.w. 
Initially, t::.w 
2n/ N as provided by the DFT. If Eq. (13.51) cannot be satisfied by 
an integer r[k], then t::.w is halved, and a new estimate of arg(X[k]) is computed with 

996 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
the new step size. Then, Eq. (13.51) is evaluated with the new estimate. Increasingly 
accurate estimates of arg(X[k]) are computed by numerical integration until Eq. (13.51) 
can be satisfied by an integer r[k]. That resulting r[k] is used in Eq. (13.49) to finally 
compute arg(X[k]). This unwrapped phase is then used to compute arg(X[k + 1]), and 
so on. 
Another approach to phase unwrapping for a finite-length sequence is based on 
the fact that the z-transform of a finite-length sequence is a finite-order polynomial, 
and therefore can be viewed as consisting of a product of pt-order factors. For each 
such factor, ARG[X(eju»] and arg[X(eju»] are equal, i.e., the phase for a single factor 
will never require unwrapping. Furthermore, the unwrapped phase for the product of 
the individual factors is the sum of the unwrapped phases of the individual factors. 
Consequently, by treating a finite-length sequence of length N as the coefficients in an 
Nth-order polynomial, and by first factoring that polynomial into its 15t-order factors, 
the unwrapped phase can be easily computed. For small values of N, conventional 
polynomial-rooting algorithms can be applied. For large values, an effective algorithm 
has been developed by Sitton et al. (2003) and has been successfully demonstrated with 
polynomials of order in the millions. However, there are cases in which that algorithm 
also fails, particularly in identifying roots that are not close to the unit circle. 
In the discussion above, we have briefly described several algorithms for obtaining 
the unwrapped phase. Karam and Oppenheim (2007) have also proposed combining 
these algorithms to exploit their various advantages. 
Other issues in computing the complex cepstrum from a sampled input signal x[n] 
relate to the linear-phase term in arg[X(eju»] and the sign of the overall scale factor 
A. In our definition of the complex cepstrum, arg[X (eju»] is required to be continuous, 
odd and periodic in w. Therefore, the sign of A must be positive, since if negative, a 
phase discontinuity would occur at w = O. Furthermore, arg[X (e jW )] cannot contain a 
linear term, since that would impose a discontinuity at w = Jr. Consider, for example, 
a finite-length causal sequence of length M + 1. The corresponding z-transform will be 
of the form of Eq. (13.29) with No=Ni=O, and M=Mo+Mi. Also, since x[n] = 0, n < 0, 
it follows that r 
- Mo. Consequently, the Fourier transform takes the form 
M 
X(eju» =  Lx[n]e-iu>n 
n=O 
(13.52)
Mj 
Ma 
= Ae-ju>Mo n(1 - ake-ju» n(1- bkeju», 
k=l 
k=l 
with lakl and Ibklless than unity. The sign of A is easily determined, since it will corre­
spond to the sign of X(eiu» at w = 0, which, in turn, is easily computed as the sum of 
all the terms in the input sequence. 
13.6.2  Computation of the Complex Cepstrum Using the 
Logarithmic Derivative 
As an alternative to the explicit computation of the complex logarithm, a mathematical 
representation based on the logarithmic derivative can be exploited. For real sequences, 

997 
Section 13.6 
Computation of the Complex Cepstrum 
the derivative of X(e jW) can be represented in the equivalent forms 
~. 
dX(e jW) 
d 
. 
d 
. 
X'(eJW) = 
= -log IX(eJW)1 + j-arg[X(eJW )] 
(13.53a)
dw 
dw 
dw 
and 
jW
X'(e jW ) = X'(e 
) 
(13.53b)
X (e jW ) , 
where I represents differentiation with respect to w. Since the DTFT of x[n] is 
X(e jW) = L
00 
x[n]e-jwn , 
(13.54) 
n=-oo 
its derivative with respect to w is 
X'(e jW)= L
00 
(-jnx[n])e- jwn ; 
(13.55) 
n=-oo 
i.e., X'(e jW ) is the DTFf of - jnx[n]. Likewise, X'(e jW) is the Fourier transform of 
- jnx[nl Thus, x[n] can be determined for n t= 0 from 
-1 
jW) .
/n X'(e
x[n] =  --
. 
eJwndw, 
n t= O. 
(13.56)
2nnj -n X (eJW) 
The value of x[0] can be determined from the log magnitude as 
1 
.
/n
x[O] =  -
log IX(eJW)ldw. 
(13.57)
2n -n 
Equations (13.54) to (13.57) represent the complex cepstrum in terms of the 
DTFfs of x[n] and nx[n] and thus do not explicitly involve the unwrapped phase. For 
finite-length sequences, samples of these transforms can be computed using the DFf, 
thereby leading to the corresponding equations 
N-l  
I
X[k] = L x[n]e-j (2n/N)kn = X(e jW) 
, 
(13.58a) 
n=O 
w=(2n/ N)k 
N-l 
X'[k] = - j L nx[n]e-j (2n/N)kn = X'(e jW) I 
, 
(13.58b) 
n=O 
w=(2n/N)k 
N-l 
I  
X [n] = __1_ " 
X [k] ej (2n/N)kn  
1 S n S N -1, 
(13.5Sc)
dp 
·nN ~ X[k] 
,
} 
k=O 
1 N-l 
Xdp[O] = N L log IX[k]l, 
(13.58d) 
k=O 
where the subscript d refers to the use of the logarithmic derivative and the subscript 
p is a reminder of the inherent periodicity of the DFT calculations. With the use of 

998 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
Eqs. (13.S8a) to (13.S8d), we avoid the problems of computing the complex logarithm 
at the cost, however, of more severe aliasing, since now 
1 
xdp[n) = - L
00 
(n + rN)x[n + rN], 
n f= O. 
(13.59) 
n r=-oc 
Thus, assuming that the sampled continuous phase curve is accurately computed, we 
would expect that for a given value of N, xp[n) in Eq. (13.46c) would be a better ap­
proximation to x[n) than would xdp[n] in Eq. (13.S8c). 
13.6.3  Minimum..phase Realizations for Minimum-Phase 
Sequences 
In the special case of minimum-phase sequences, the mathematical representation is 
simplified, as indicated in Figure 13.2. A computational realization based on using the 
DFT in place of the Fourier transform in Figure 13.2 is given by the equations 
N-l 
X[k) = L x [n]e-j(2rr/N)kn , 
(13.6Oa) 
n=O 
N-l 
cxp[n) 
~ L log IX[k)lei (2rr/N)kn. 
(13.6Ob) 
k=O 
In this case, it is the cepstrum that is aliased; i.e., 
cxp[n) =  L
00 
cx[n + rN). 
(13.61) 
r=-oo 
To compute the complex cepstrum from cxp[n) based on Figure 13.2, we write: 
cxp[n], 
n 
0, 
N /2, 
xcp[n) 
2cxp [n), 1:s n < N/2, 
(13.62)
{ 0,  
N/2 < n :s N - 1. 
Clearly, xcp[n] f= xp[n], since it is the even part of x[n] that is aliased, rather than x[n) 
itself. Nevertheless, for large N ,xcp[n] can be expectedto be a reasonable approximation 
to x[n] over the finite interval 0 :s n < N /2. Similarly, if x[n] is maximum phase, an 
approximation to the complex cepstrum would be obtained from 
cxp[n], 
n = 0, 
N/2, 
xcp[n] = 
0, 
1 :s n < N /2. 
(13.63)
{ 2cxp [n],  N/2 < n :s N - 1. 
13.6.4 Recursive Computation of the Complex Cepstrum 
for Minimum- and Maximum-Phase Sequences 
For minimum-phase sequences, the difference Eq. (13.26) can be rearranged to provide 
a recursion formula for x[n]. Since for minimum-phase sequences both x[n) 
0 and 

999 
Section 13.6 
Computation of the Complex Cepstrum 
x[n] = 0 for n < 0, Eq. (13.26) becomes 
x[n] t (~) x[k]x[n - k], 
n > 0, 
k=O 
(13.64)
n-l 
x[n]x[O] + L (~) x[k]x[n - k], 
k=O 
which is a recursion for D*[ ] for minimum-phase signals. Solving for x[n] yields the 
recursion formula 
0,  
n < 0, 
1
x[n]
x[n] 
~ (k) ~ 
x[n-k]  
(13.65)
L.... 
-;; 
x[k] 
x[O] 
, n > O.
x[O] 
k=O 
Assumingx[O] > 0, the value of x[O] can be shown to be (see Problem 13.15) 
x[O] = 10g(IAI) = 10g(lx[O]l).  
(13.66) 
Therefore. Eqs. (13.65) and (13.66) constitute a procedure for computing the complex 
cepstrum for minimum-phase signals. It also follows from Eq. (13.65) that this com­
putation is causal for minimum-phase inputs; i.e., the output at time no is dependent 
only on the input for n .::: no, where no is arbitrary (see Problem 13.20). Similarly, 
Eqs. (13.64) and (13.66) represent the computation of the minimum-phase sequence 
from its complex cepstrum. 
For maximum-phase signals, x[n] = 0, and x[n] = 0 for n > O. Thus, in this case 
Eq. (13.26) becomes 
x[n] =  
k], 
n < 0,
t (~) .~[k]x[n ­
k=n 
(13.67)
o 
x[n]x[O] + L (~) x[k]x[n 
k]. 
k=n+l 
Solving for x[n], we have 
0
x[n] 
k 
~ 
x[n 
k] 
-
-
-
x[k] 
n < 0, 
x[n] = 
x[O] 
kEl (n) 
x[0] 
, 
(13.68) 
{  log(x[0]) , 
n 
0, 
0, 
n > 0. 
Equation (13.68) serves as a procedure for computing the complex cepstrum for a 
maximum-phase sequence and Eq. (13.67) is a computational procedure for the inverse 
characteristic system for convolution. 
Thus we see that in the case of minimum-phase or maximum-phase sequences, 
we also have the recursion formulas of Eqs. (13.64)-(13.68) as possible realizations of 
the characteristic system and its inverse. These equations can be quite useful when the 
input sequence is very short or when only a few samples of the complex cepstrum are 
desired. With these formulas, of course, there is no aliasing error. 

1000 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
13.6.5 The Use of Exponential Weighting 
Exponential weighting of a sequence can be used to avoid or mitigate some of the 
problems encountered in computing the complex cepstrum. Exponential weighting of 
a sequence x[n] is defined by 
w[n] 
anx[n]. 
(13.69) 
The corresponding z-transform is 
W(z) = X(a-1z). 
(13.70) 
If the ROC of X(z) is rR < Izi < rL, then the ROC of W(z) is lalrR < Izl < lalrL, and 
the poles and zeros of X(z) are shifted radially by the factor lal; i.e., if zo is a pole or 
zero of X(z), then zoa is the corresponding pole or zero of W(z). 
A convenient property of exponential weighting is that it commutes with convo­
lution. That is, if x[n] 
xl[n] * x2[n] and w[n] 
anx[n], then 
W(z) 
X(a-1z) 
Xl (a-1z)X2(a-I z), 
(13.71) 
so that 
w[n] = (anXI[n]) * (anX2[n]) 
(13.72) 
= wI[n] * w2[n]. 
Thus, in computing the complex cepstrum, if X (z) 
Xl (Z)X2(Z), 
W(z) = 10g[W(z)] 
(13.73) 
= 10g[WI (z)] + log[W2(Z)]. 
Exponential weighting can be exploited with cepstrum computation in a variety 
of ways. For example, poles or zeros of X (z) on the unit circle require special care 
in computing the complex cepstrum. It can be shown (Carslaw, 1952) that a factor 
10g(1 
ejee-jW) has a Fourier series 
., 
00 
jen 
10g(1 - eJe e-JW) = - L _e_e-jwn 
(13.74) 
n=l n 
and thus, the contribution of such a term to the complex cepstrum is (e jen /n)u[n - 1]. 
However, the log magnitude is infinite, and the phase is discontinuous with a jump of j[ 
radians at w = (). This presents obvious computational difficulties that we would prefer 
to avoid. By exponential weighting with 0 < a < 1, all poles and zeros are moved 
radially inward. Therefore, a pole or zero on the unit circle will move inside the unit 
circle. 
As another example, consider a causal, stable signal x[n] that is nonminimum 
phase. The exponentially weighted signal, w[n] = anx[n], can be converted into a 
minimum-phase sequence if a is chosen, so that IZmaxal < 1, where Zmax is the location 
of the zero with the greatest magnitude. 

1001 
Section 13.7 
Computation of the Complex Cepstrum Using Polynomial Roots 
13.7  COMPUTATION OF THE COMPLEX CEPSTRUM 
USING POLYNOMIAL ROOTS 
In Section 13.6.1, we discussed the fact that for finite-length sequences, we could exploit 
the fact that the z-transform is a finite-order polynomial, and that the total unwrapped 
phase can be obtained by summing the unwrapped phases for each of the factors. If the 
polynomial is first factored into its 1st-order terms using a polynomial rooting algorithm, 
then the unwrapped phase for each factor is easily specified analytically. In a similar 
manner the complex cepstrum for the finite-length sequence can be obtained by first 
factoring the polynomial, and then summing the complex cepstra for each of the factors. 
The basic approach is suggested by Section 13.5.1. If the sequence x[n] has finite 
length, as is essentially always the case with signals obtained by sampling, then its z-
transform is a polynomial in 
of the form 
M 
X(z) =  Lx[n]z-Il. 
(13.75) 
n=O 
Such an Mth-order polynomial in C 1 can be represented as 
Mi 
Mo 
X(z) 
x[O] n(1 - amz-1) n(1 
b;;;lz-l), 
(13.76) 
m=l 
m=l 
where the quantities am are the (complex) zeros that lie inside the unit circle, and the 
quantities b~l are the zeros that are outside the unit circle; i.e., lam I < 1 and Ibm I < 1. 
We assume that no zeros lie precisely on the unit circle. If we factor a term -b~lCl out 
of each factor of the product on the right in Eq. (13.76), that equation can be expressed 
as 
Mi 
Mo 
1
X(z) = Az-Mv n(1 
amz- ) n(1- bmz), 
(l3.77a) 
m=l 
m=l 
where 
Mo 
A =x[O](-1)Mo n b;;;l. 
(13.77b) 
m=l 
This representation can be computed by using a polynomial rooting algorithm to find 
the zeros am and 1jbm that lie inside and outside the unit circle, respectively, for the 
polynomial whose coefficients are the sequence x[n]J 
Given the numeric representation ofthe z-transform polynomial as in Eqs. (13.77 a) 
and (13.77b), numeric values of the complex cepstrum sequence can be computed from 
7Perhaps not surprisingly. it is rare that a computed root of a polynomial is precisely on the unit circle. 
In cases where this occurs. such roots can be moved by exponential weighting. as described in Section 13.6.5. 

1002  
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
Eqs. (13.36a)-(13.36c) as 
10g1AI, 
n =0, 
M; 
n 
_Lam 
x[n] =  ~ 
-;;' n > 0, 
(13.78)
m=l 
Mo 
Lb~n, n < O. 
m=l n 
If A < 0, this fact can be recorded separately, along with the value of Mo, the number of 
roots that are outside the unit circle. With this information and x[n], we have all that is 
needed to reconstruct the original signal x[nJ. Indeed, in Section 13.8.2, it will be shown 
that, in principle, x[n] can be computed recursively from just M + 1 = Mo + Mj + 1 
samples of x[n]. 
This method of computation is particularly useful when M 
Mo + Mj is small, but 
it is not limited to small M. Steiglitz and Dickinson (1982) first proposed this method 
and reported successful rooting of polynomials with degree as high as M 
256, which 
was a practical limit imposed by computational resources readily available at that time. 
With the polynomial rooting algorithm of Sitton et al. (2003), the complex cepstrum 
of extremely long finite-length sequences can be accurately computed. Among the ad­
vantages of this method are the fact that there is no aliasing and there are none of the 
uncertainties associated with phase unwrapping. 
13.8  DECONVOLUTION USING THE COMPLEX 
CEPSTRUM 
The complex cepstrum operator D*[ ], plays a key role in the theory of homomorphic 
systems, which is based on a generalization of the principle of superposition (Oppen­
heim, 1964, 1967, 1969a, Schafer, 1969 and Oppenheim, Schafer and Stockham, 1968). In 
homomorphic filtering of convolved signals, the operator D* [ ] is termed the character­
istic system for convolution since it has the special property of transforming convolution 
into addition. To see this, suppose 
x[n] = xl[n] * x2[n]  
(13.79) 
so that the corresponding z-transform is 
X(z) = Xl(Z) . Xz(z).  
(13.80) 
If the complex logarithm is computed as we have prescribed in the definition of the 
complex cepstrum, then 
X(z) =  10g[X(z)] = log[Xl (Z)] +log[X2(Z)] 
(13.81)
X1(Z) + X2(Z), 
which implies that the complex cepstrum is 
x[n]  
D*[xl[n] * x2[n]] 
xl[n] + .t2[n). 
(13.82) 
A similar analysis shows that if yIn] 
= 
Yl[n] + n[n]. then it follows that 
D;l[Yl[nJ + Y2[n]] = Yl[n] * Y2[n]. If the cepstral components xl[n] and x2[n] occupy 

1003 
Section 13.8 
Deconvolution Using the Complex Cepstrum 
D.[ I 
+ lIn] + 
Linear 
system 
L[ ] 
+ YIn] 
+ 
] 
r---------------------------------------------,• * 
x[n] * • •
I 
I 
yIn]
I 
I 
I 
•
I 
I 
I 
I _______________ ------------------------ ______ 1 
Figure 13.6 Canonic form for homomorphic systems where inputs and corre­
sponding outputs are combined by convolution. 
different quefrency ranges, linear filtering can be applied to the complex cepstrum to 
remove either xl[n) or x2[n]. If this is followed by transformation through the inverse 
system D;l[], the corresponding component will be removed in the output. This proce­
dure for separating convolved signals (deconvolution) is depicted in Figure 13.6, where 
the system L[ ] is a linear (although not necessarily time invariant) system. The sym­
bols *and + at the inputs and outputs of the component systems in Figure 13.6 denote 
the operations of superposition that hold at each point in the diagram. Figure 13.6 is a 
general representation of a class of systems that obey a generalized principle of super­
position with convolution as the operation for combining signals. All members of this 
class of systems differ only in the linear part L[ ]. 
In the remainder of this section, we illustrate how cepstral analysis can be used for 
the special deconvolution problems of decomposing a signal into either a convolution 
of a minimum-phase and allpass component or minimum-phase and maximum-phase 
component. In Section 13.9, we illustrate how cepstral analysis can be applied to de­
convolution of a signal convolved with an impulse train, representing for example, an 
idealization of a multi path environment. In Section 13.10, we generalize this example 
to illustrate how cepstral analysis has been successfully applied to speech processing. 
13.8.1  Minimum-PhaselAilpass Homomorphic 
Deconvolution 
Any sequence x[n] for which the complex cepstrum exists can always be expressed as 
the convolution of a minimum-phase and an all pass sequence as in 
x[n] 
xminln] * xap[n).  
(13.83) 
In Eq. (13.83) Xmin[n] and xap[n] denote minimum-phase and allpass components re­
spectively. 
If x[nJ is not minimum phase, then the system of Figure 13.2 with input x[n] and 
imin[n] given by Eq. (13.42b) produces the complex cepstrum of the minimum-phase 
sequence that has the same Fourier transform magnitude as x[n). If imax[n] = f.min[-n) 
is used, the output will be the complex cepstrum ofthe maximum-phase sequence having 
the same Fourier transform magnitude as x[n]. 
We can obtain the complexcepstrumxmin[n] of the sequencexmin[n] inEq. (13.83) 
through the operations of Figure 13.2. The complex cepstrum xapln] can be obtained 
from x[n] by subtracting Xmin[n] from lIn], i.e., 
xapln] = x[n] - Xmin[n]. 
To obtain Xmin[n] and xapln], we apply the transformation D;l to Xmin[n] and xap[n]. 

1004 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
l'mm[nJ 
Xmin(eiw)
cx[nl 
Inverse ~ 
Xmi,,(e jw)
Fourier 
transform 
X(ejw) 
log 1·1 ~ 
Fourier 
x 
transform 
Fourier 
expO
transform 
Inverse 
Fourier 
transform 
Inverse
1 XaiejW) 
xap[n] 
Fourier
-.:...; 
transform 
Figure 13.7 Deconvolution of a sequence into minimum-phase and allpass components 
using the cepstrum. 
Although the approach outlined above to obtain Xmin [n] and xap [n] is theoretically 
correct, explicit evaluation of the complex cepstrum x[n] is required in its implementa­
tion. Ifwe are interested only in obtaining Xmin [n] and xap[n], evaluation of the complex 
cepstrum and the associated need for phase unwrapping can be avoided. The basic strat­
egy is incorporated in the block diagram of Figure 13.7. This system relies on the fact 
that 
jW
jUJ) _ 
X (e
)
Xap(e 
-
(13.84a)
Xmin 
The magnitude of Xap(eiw) is therefore 
IXap (e jW ) I = 
IX(e 
jW
) I 
(13.84b)
IXmin 
= 1 
and 
iXap(eiw) = iX(eiw) -
iXmin(eiw). 
(13.84c) 
Since xap[n] is obtained as the inverse Fourier transform of eiiXap(eJ"'), (that is, 
IXap(eiw)1 = 1), each of the phase functions in Eq. (13.84c) need only be known or 
specified to within integer multiples of 21T. Therefore, even though as a natural conse­
quence of the procedure outlined in Figure 13.7, iXmin(ejW ) = Im{Xmin(e jUJ )} will be 
an unwrapped phase fUnction, iX (eiw) in Eq. (13.84c) can be computed modulo 21T. 
13.8.2 Minimum-Phase/Maximum-Phase Homomorphic  
Deconvolution  
Another representation of a sequence is as the convolution of a minimum-phase se­
quence with a maximum-phase sequence as in 
x[n] = xmn[n] *xmx[n], 
(13.85) 
where xmn[n] and xmx[n] denote minimum-phase and maximum-phase components, 
respectively.8 In this case, the corresponding complex cepstrum is 
x[n] 
xmn[n] + xmx[n]. 
(13.86) 
8In general the minimum-phase component Xmn [n] in Eq. (13.85) will be different from Xmin[n] in 
Eq. (13.83). 

1005 
Section 13.8 
Deconvolution Using the Complex Cepstrum 
x[n] 
D.[ '1 
.t[n] 
.tmn[n] 
D;;-l[. ] 
xmn[n] 
x 
t'mn[n1 
.tmx[n] 
D;;-lr .J 
xmxfn] 
x 
Figure 13.8 The use of homomorphic deconvolution to separate asequence into 
minimum-phase and maximum-phase components. 
To extract xmn[n] and Xmx [n] from x[n], we specify xmn[n] as 
xmn[n) = emn[n)x[n], 
(13.87a) 
where 
emn[n) = urn]. 
(13.87b) 
Similarly, we specify xmx[n] as 
(13.88a) 
where 
emx[n]=u[-n 
1]. 
(13.88b) 
Xmn[n) and Xmx [n] can be obtained fromxmn[n] and xmx [n], respectively, as the output of 
the inverse characteristic system D;l [.]. The operations required for the decomposition 
of 
(13.85) are depicted in Figure 13.8. This method of factoring a sequence into its 
minimum~ and maximum-phase parts has been used by Smith and Barnwell (1986) in 
the design of filter banks. Note that we have arbitrarily assigned all of X[O] to xmn [0], 
and we have set xmx[O] = O. Obviously, other combinations are possible, since all that 
is required is that xmn [0) + xmx [0] 
X[0]. 
The recursion formulas of Section 13.6.4 can be combined with the representation 
of Eq. (13.85) to yield an interesting result for finite-length sequences. Specifically, in 
spite of the infinite extent of the complex cepstrum of a finite-length sequence, we can 
show that for an input sequence of length M + 1, we need only M + 1 samples of x[n] 
to determine x[n). To see this, consider the z-transform of Eq. (13.85), i.e., 
X(z) = Xmn(Z)Xmx(z), 
(13.89a) 
where 
M;
An (1
Xmn(Z) 
- akz-1), 
(13.89b) 
k=l 
Mo 
Xmx(z) = n(1 
bkZ), 
(13.89c) 
k=1 

1006 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
with lakl < 1 and Ibkl < 1. Note that we have neglected the delay of Mo samples 
that would be needed for a causal sequence, so that xmn[n] °outside the interval 
°~ n ~ Mi and xmAn] °outside the interval -Mo ~ n ~ O. Since the sequence x[n] 
is the convolution of xmn[n] and xmx[n], it is nonzero in the interval -Mo :::: n ~ Mi. 
Using the previous recursion formulas, we can write 
0, 
n <0, 
ex[Ol, 
n=O, 
xmn[n] = 
n-l 
k 
(13.90) 
{ x[n]xmn[O] + ~ (;;) x[k]xmn[n - k], n > 0, 
and 
x[n]+ t (~)X[k]Xmx[n-k]' n <0, 
xmx[n] = 
k=n+l 
(13.91) 
{ 1, 
n =0, 
0, 
n > 0. 
Clearly, we require Mi + 1 values of x[n] to compute xmn[n] and Mo values of x[n] 
to compute xmx[n]. Thus, only Mi + Mo + 1 values of the infinite sequence x[n] are 
required to completely recover the minimum-phase and maximum-phase components 
of the finite-length sequence xln]. 
As mentioned in Section 13.7, the result that we have just obtained could be used 
to implement the inverse characteristic system for convolution when the cepstrum has 
been computed by polynomial rooting. We simply need to compute xmnln] and xmx[n] 
by the recursions ofEqs. (13.90) and (13.91) and then reconstruct the original signal by 
the convolution x[n] = Xmn[n] * xmx[n]. 
13.9 THE COMPLEX CEPSTRUM FOR A SIMPLE 
MULTIPATH MODEL 
As discussed in Example 13.1, a highly simplified model of multipath or reverberation 
consists of representing the received signal as the convolution of the transmitted signal 
with an impulse train. Specifically, with urn] denoting a transmitted signal and p[n] the 
impulse response of a multipath channel or other system generating multiple echoes, 
x[n] = urn] * p[n], 
(13.92a) 
or, in the z-transform domain, 
X(z) 
V(z)P(z). 
(13.92b) 
In our analysis in this section, we choose p[n] to be of the form 
p[n] 
8[n] + {38[n - No] + {328[n 
2NoL 
(13.93a) 
and its z-transform is then 
3N 
P(z) = 1 + {3z-No + f32z·-2No = 1 -
f33 z-
o 
(13.93b)
1­

1007 
Section 13.9 
The Complex Cepstrum for aSimple Multipath Model 
0.5 
x 
o t-e--------t-------H 
-D.5 
-1 
-1 
-0.5 
o 
0.5  
Figure 13.9 
Pole-zero plot of the 
z-transform X(Z) = V(Z)P(Z) for the
Re(zl 
example signal of Figure 13.10. 
For example, p[n] might correspond to the impulse response of a multipath channel or 
other system that generates multiple echoes at a spacing of No and 2No. The component 
v[n] will be taken to be the response of a 2nd-order system, such that 
bo + blZ-1 
V (z) = ---:-;:---;-------;-::---,;­ 
Izl> Irl· 
(13.94a)
(1 
rejez-1)(1 
rrjecl) ' 
In the time domain, v[n] can be expressed as 
v[n] = bow[n] + blw[n -1],  
(13.94b) 
where 
rn 
(13.94c)
4sin 0 
Figure 13.9 shows the pole-zero plot of the z-transform X (z) = V(z)P(z) for the specific 
set of parameters bo 
0.98, bi 
1, fJ 
r = 0.9, (J = n /6, and No = 15. Figure 13.10 
shows the signals v[n], p[n], and x[n] for these parameters. As seen in Figure 13.10, the 
convolution of the pulse-like signal v[n] with the impulse train p[n] results in a series 
of superimposed delayed copies (echoes) of v[nJ. 
This signal model is a simplified version of models that are used in the analysis 
and processing of signals in a variety of contexts, including communications systems, 
speech processing, sonar, and seismic data analysis. In a communications context, v[n] 
in Eqs. (13.92a) and (13.92b) might represent a signal transmitted over a multipath 
channel, x[n] the received signal, and p[n] the channel impulse response. In speech 
processing, v[n] would represent the combined effects of the glottal pulse shape and the 
resonance effects of the human vocal tract, while p[n] would represent the periodicity 
of the vocal excitation during voiced speech such as a vowel sound (Flanagan, 1972; 
Rabiner and Schafer, 1978; Quatieri, 2002). Equation (13.94a) incorporates only one 
resonance, while in the general speech model, the denominator would generally include 
at least ten complex poles. In seismic data analysis, v[n] would represent the waveform 
wLnl= --2-{cos(On) 
cos[O(n + 2)]}u[n], 

1008 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
4 
3 
2
.., 
"0 
:l :[ 
1 
S 
-< 
0 
-1 
-2 
...... .. .. ··"'1.. ·.. ··  
.........: 
· 
· ............;. 
. .......  
· 
.
· 
.
· 
. 
~=-'tltl[jl·~lml,+.:.:=-=··  
.. 
, 
L'______~________~______-L______~~____~ 
-20 
0 
1.0 
1 _........... .. 
<U 
11 
:::; 0.5 
~ ••• I • ~ 
• « •••• _I 
~ 
« 
o 
..----1........-
1 
-
-0.5 !L­______~________L-______~________~______~ 
-20 
0 
20 
40 
60 
80 
Sample number [n] 
(b) 
4 .--------r-------:-------: 
2 
<l) 
"0 
:a
E 
~ -1 r........·..· :  
: 
-2 L!________~ 
20 
40 
60 
80 
Sample number [n] 
(a) 
· 
, 
,
, 
, 
. 
· 
, 
,, 
· 
, 
. 
, 
-. 
• ••• _ - -.- ••••• _ 
- ••• «. _, -.. 
~ ...• 
· 
.
,l'
· 
.
· 
, 
,
· 
, 
· 
, 
, 
, 
· 
, 
. 
, 
. 
, 
....­
_··t··················.....-.... 
0 ,---::w 11 ~l:.."::~]jllr:..:Jll~...............:.............
: 
: 
: 
. 
. 
_______L________~______~______~ 
-20 
0 
20 
40 
60 
80 
Sample number [n] 
(c) 
Figure 13.10 The sequences: (a) v[n], 
(b) p[n], and (c) x[n] corresponding to 
the pole-zero plot of Figure 13.9. 
of an acoustic pulse propagating in the earth due to a dynamite explosion or similar 
disturbance. The impulsive component p[n] would represent reflections at boundaries 
between layers having different propagation characteristics. In the practical use of such 

1009 
Section 13.9 
The Complex Cepstrum for a Simple Multipath Model 
a model, there would be more impulses in p[nJ than we assumed in Eq. (13.93a), and 
they would be unequally spaced. Also, the component V (z) would generally involve 
many more zeros, and often no poles are included in the model (Ulrych, 1971; Tribolet, 
1979; Robinson and Treitel, 1980). 
Although the model discussed above is a highly simplified representation of that 
encountered in typical applications, it is analytically convenient and useful to obtain 
exact formulas to compare with computed results obtained for sampled signals. Fur­
thermore, we will see that this simple model illustrates all the important properties of 
the cepstrum of a signal with a rational z-transform. 
In Section 13.9.1, we evaluate analytically the complex cepstrum for the received 
signal x[nJ. In Section 13.9.2, we illustrate the computation of the complex cepstrum 
using the DFT, and in Section 13.9.3 illustrate the technique of homomorphic deconvo­
lution. 
13.9.1  Computation of the Complex Cepstrum by 
z-Transform Analysis 
To determine an equation for x[n], the complex cepstrum of x[n] for the simple model 
of Eq. (13.92a), we use the relations 
x[n] = D[n] + p[n],  
(13.9Sa) 
X(z) = V (z) + P(z),  
(13.9Sb) 
X(z) = 10g[X(z)],  
(13.96a) 
V(z) = 10g[V(z)],  
(13.96b) 
and 
p(z) = 10g[P(z)].  
(13.96c) 
To determine il[n], we can directly apply the results in Section 13.5. Specifically, to 
express V (z) in the form of Eq. (13.29), we first note that for the specific signal X (z) in 
Figure 13.9, the poles of V(z) are inside the unit circle and the zero is outside (r = 0.9 
and bo/bl = 0.98), so that in accordance with Eq. (13.29), we rewrite V(z) as 
1 
V(z) = (1 _b1C (1 + (bo/bl)Z) 
Izi > Irl. 
(13.97) 
As discussed in Section 13.5, the factor 
contributes a linear component to the 
unwrapped phase that will force a discontinuity at w = ±Jl' in the Fourier transform of 
v[n], so V(z) will not be analytic on the unit circle. To avoid this problem, we can alter 
v[n] (and therefore also x[nl) with a one-sample time shift so that we evaluate instead 
the complexcepstrum of v[n +1] and, consequently, also x[n +1]. If x[n] or v[n] is to be 
resynthesized after some processing of the complex cepstrum, we can remember this 
time shift and compensate for it at the final output. 
With v[n] replaced by v[n + 1], and correspondingly V(z) replaced by zV(z), we 
now consider V (z) to have the form 
V (z) = 
b: (1 + (bo/bl)Z). 
(13.98) 
(1- re,8z-1)(1 - re-,8z-1) 

Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution
1010 
From Eqs. (13.36a) to (13.36c), we can write D[n] exactly as 
log bI, 
n =0, 
(13.99a) 
1 
'0 
'0
-[ere} )1l + (re-} )1l], 
n > 0, 
(13.99b) 
D[n] = 1n 
1 (-bo) -1l , 
n < O. 
(13.99c) 
n 
bI 
To determine p[n], we can evaluate the inverse z-transform of P(z), which, from 
Eq. (13.93b), is 
P(z) 
10g(1 
tpz-3NO) - log(1 - flz- NO ), 
(13.100) 
where for our example fl 
0.9, and consequently, Ifll < 1. One approach to determining 
the inverse z-transform of Eq. (13.100) is to use the power series expansion of P(z). 
Specifically, since IfJI < 1, 
00 
3k 
00 
k 
P(z) = - L ~z-3NOk + L ~ Z-Nok, 
(13.101) 
k=l 
k=l 
from which it follows that p[n] is 
00 fl3k 
00 flk 
fJ[n] = - L T 
8[n 
3Nok] + L T 8[n - Nok]. 
(13.102) 
k=l 
k=l 
An alternative approach to obtaining p[n] is to use the property developed in 
Problem 13.28. 
From Eq. (13.95a), the complex cepstrum of x[n] is 
x[n] = D[n] + pIn], 
(13.103) 
where iJ[n] and pIn] are given by Eqs. (13.99a) to (13.99c) and (13.102), respectively. 
The sequences iJ[n], pIn], and x[n] are shown in Figure 13.11. 
The cepstrum of x[n], cAn], is the even part of x[n], i.e., 
cx[n] = ~(x[n] + x[-n]) 
(13.104) 
and furthermore 
cAn] = cv[n] + cp[n]. 
(13.105) 
From Eqs. (13.99a) to (13.99c), 
00 (-1)k(bo/bl)-k  
cvln] = log(b})8[n] + L 
,." 
(81n - k] + 8[n + k])  
k=l 
(13.106a) 
~ rk cos(ek)
+ ~ , 
(8[n - k] + 8[n + k]). 
k=l 
and from Eq. (13.102), 
1 00 fl3k 
cp[n] -2 L T{8[n - 3Nok] + 8[n + 3Nok]) 
k=l 
(13.106b)
1 00 flk 
+ 2L T{8[n - Nok] + 8[n + Nok]}. 
k=l 
The sequences cv[n], cp[n], and cx[n] for this example are shown in Figure 13.12. 

2,--------r--------~-------,·--------, 
1.5 ................................ .  
, 
,
••••••••• - - - •• • • 
• ••••• - - - - ""..
~ 
• •••••• - _••••••••• r 
o~------~--..qm 
..:........... " ..... . 
-0.5 
-100 
-50 
o 
50 
100 
Sample number [n] 
(a) 
2.---------.---------.---------.--------, 
1.5 ........ --­
.......... ---- ...., ....... --- ······r······--_········  
E 
'" 
1 
: 
! 
~0.5 
« 
O~------~------~----~~------~ 
-0.5 
-100 
-50 
o 
50 
100  
Sample number [n]  
(b)  
2.---------r-------~.-------~.--------, 
1.5 
--{).5 
.............. r"' 
-100 
-50 
o 
50 
100 
Sample number [n] 
Figure 13.11 
The sequences: (a) v[n]. 
(c) 
(b) p[n]. and (c) x[n]. 
1011 

1.5 r'---:----:-----.----~ 
1 •.. 
]:a 0.5 
~ 
01 
n·~~A'm~' 
-0.5 ,'-------'--------'------'-------' 
-100 
-50 
0 
50 
100 
Sample number [n] 
(a) 
1.5 ,..-----....-------.,......-------.,......---­
1 ~........................... .  
-8 
;::I 
. 
.
'§.. 0.5 
····f····"j············f···.. 
~ 
r
I 
1
°IT 
: I 
1: 
-O.5~'----~'------~-------'----~ 
-100 
-50 
0 
50 
100 
Sample number [n] 
(b) 
1.5 
1""1- - - - - . , . . . - - - - - - , - - - - - - , - - - - - - - - ,  
1 t·················· ~ ................ '1~ ......... .  
<l.) 
"0 
;::I%0.5 
~ 
o1-, 
' -~M'A'~'J' T I 
-O'~100 
-50 
o 
50 
100 
Sample number [n] 
Figure 13.12 
The sequences: 
(c) 
(a) cv[n], (b) cp[n]. and (c) cx[n]. 
1012 

Section 13.9 
The Complex Cepstrum for a Simple Multipath Model 
1013 
13.9.2 Computation of the Cepstrum Using the DFT 
In Figures 13.11 and 13.12, we showed the complex cepstra and the cepstra corre­
sponding to evaluating the analytical expressions obtained in Section 13.9.1. In most 
applications, we do not have simple mathematical formulas for the signal values, and 
consequently, we cannot analytically determine .t[n] or ex [n]. However, for finite-length 
sequences, we can use either polynomial rooting or the DFT to compute the complex 
cepstrum. In this section, we illustrate the use of the DFT in the computation of the 
complex cepstrum and the cepstrum of x [n] for the example of this section. 
To compute the complex cepstrum or the cepstrum using the DFT as in Fig­
ure 13.4(a), it is necessary that the input be of finite extent. Thus, for the signal model 
discussed at the beginning of this section, x [n] must be truncated. In the examples dis­
cussed in this section, the signal x[n] in Figure 13.10(c) was truncated to N = 1024 
samples and 1024-point DFTs were used in the system of Figure 13.4(a) to compute the 
complex cepstrum and the cepstrum of the signal. Figure 13.13 shows the Fourier trans­
forms that are involved in the computation of the complex cepstrum. Figure 13.13(a) 
shows the logarithm of the magnitude of the DFT of1024 samples ofx [n] in Figure 13.10, 
with the DFT samples connected in the plot to suggest the appearance of the DTFT 
of the finite-length input sequence. Figure 13.13(b) shows the principal value of the 
phase. Note the discontinuities as the phase exceeds ±7T and wraps around modulo 27T. 
Figure 13.13(c) shows the continuous "unwrapped" phase curve obtained as discussed 
in Section 13.6.1. As discussed above, and as is evident by carefully comparing Fig­
ures 13.13(b) and 13.13(c), a linear-phase component corresponding to a delay of one 
sample has been removed so that the unwrapped phase curve is continuous at 0 and 7T . 
Thus, the unwrapped phase of Figure 13.13(c) corresponds to x[n + 1] rather than x[n]. 
Figures 13.13(a) and 13.13(c) correspond to the computation of samples of the 
real and imaginary parts, respectively, of the DTFT of the complex cepstrum. Only the 
frequency range 0 :s w :s 7T is shown, since the function of Figure 13.13(a) is even and 
periodic with period 27T, and the function of Figure 13.13( c) is odd and periodic with 
period 27T. In examining the plots in Figures 13.13(a) and 13.13(c), we note that they 
have the general appearance of a rapidly varying, periodic (in frequency) component 
added to a more slowly varying component. The periodically varying component in fact 
corresponds to PCe jW) and the more slowly varying component to VCe jW ). 
In Figure 13.14(a), we show the inverse Fourier transform of the complex log­
arithm of the DFT, i.e., the time-aliased complex cepstrum .tp[n]. Note the impUlses 
at integer multiples of No = 15. These are contributed by p[n] and correspond to the 
rapidly varying periodic component observed in the logarithm of the DFT. We see also 
that since the input signal is not minimum phase, the complex cepstrum is nonzero for 
n < 0.9 
Since a large number of points were used in computing the DFTs, the time-aliased 
complex cepstrum differs very little from the exact values that would be obtained by 
9In using the DFf to obtain the inverse Fourier transform of Figures 13.13(a) and 13.13( c), the values 
associated with n < 0 would normally appear in the interval N /2 < n :::: N - 1. Traditionally, time sequences 
are displayed with n = 0 in the center, sowe have repositioned xp[nJ accordingly and have shown only a total 
of 201 points symmetrically about n = O. 

6 
4 
2 
a:l 
0
"0 
-2 
-4 
-{iO 
0.211" 
0.411" 
0.611" 
11" 
Radian frequency w 
(a)  
4  
2 
~ 0 
~ 
"" 
-2 
~......... . 
~.. 
•• ___ •••• 0 _____ • ___ • 
__ .. ____ ."". 
0.811" 
-40 
0.211" 
0.411" 
0.611" 
0.811" 
11" 
Radian frequency w 
(b) 
4 
2 
-40 
..:..... -­
-_ ...:----­
. 
. 
;g 
:.s"" 
~ 
Figure 13.13 
Fourier transforms of 
x[n] in Figure 13.10. (a) Log magnitude. 
(b) Principal value of the phase. 
(c) Continuous "unwrapped" phase after
0.211" 
0.411" 
0.611" 
0.811" 
11" 
removing a linear-phase component 
Radian frequency w 
from part (b). The DFT samples are 
(c) 
connected by straight lines. 
1014  

Section 13.9 
The Complex Cepstrum for aSimple Multipath Model 
1015 
2r---------.-------~--------~--------_, 
1.5 . - ~.... ". --_ ... -. ~ .. -. _..... 
1 
-100 
-50 
o 
50 
100 
Sample number [n] 
(a) 
2r-------~---------,--------~--------_, 
Figure 13.14 (a) Complex cepstrum 
-100 
-50 
o 
xp[n] of sequence in Figure 13.10(c). 
Sample number [n] 
(b) Cepstrum cx[n] of sequence in 
(b) 
Figure 13.10(c). 
1.5 
1 .---- ...... ---- .. 
-0.5 
50 
100 
evaluating Eqs. (13.99a) to (13.99c), (13.102), and (13.103) for the specific values of the 
parameters used to generate the input signal of Figure 13.10. 
The time-aliased cepstrum cxp[n] for this example is shown in Figure 13.14(b). As 
with the complex cepstrum, impulses at multiples of 15 are evident, corresponding to 
the periodic component of the logarithm of the magnitude of the Fourier transform. 
As mentioned at the beginning of this section, convolution of a signal v[n] with 
an impulse train such as p[n] is a model for a signal containing multiple echoes. Since 
x [n] is a convolution of v[n] and p[n], the echo times are often not easily detected by 
examining x[n]. In the cepstral domain, however, the effect of pen] is present as an 
additive impulse train, and consequently, the presence and location of the echoes are 
often more evident. As discussed in Section 13.1, it was this observation that motivated 
the proposal by Bogert, Healy and Tukey (1963) that the cepstrum be used as a means 
for detecting echoes. This same idea was later used by Noll (1967) as a basis for detecting 
vocal pitch in speech signals. 

1016  
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
v[nJ 
~ 
x[n] 
D.[ .J 
x[n] 
Frequency-
invariant 
linear filter 
Y[n] 
D;l[ . ] 
(a) 
~  
f[n] 
(b) 
Figure 13.15 
(a) System for homomorphic deconvolution. (b) Time-domain rep­
resentation of frequency-invariant filtering. 
13.9.3  Homomorphic Deconvolution for the Multipath 
Model 
For the multipath model that is the basis for Section 13.9, the slowly varying component 
of the complex logarithm, and equivalently the "low-time" (low-quefrency) portion 
of the complex cepstrum, were mainly due to v[n]. Correspondingly, the more rapidly 
varying component of the complex logarithm and the "high-time" (high-quefrency) 
portion of the complex cepstrum were due primarily to pInJ. This suggests that the 
two convolved components of x[n] can be separated by applying linear filtering to the 
logarithm of the Fourier transform (i.e., frequency invariant filtering), or, equivalently 
the complex cepstrum components can be separated by windowing or time gating the 
complex cepstrum. 
Figure 13.15(a) depicts the operations involved in separation of the components 
of a convolution by filtering the complex logarithm of the Fourier transform of a sig­
naL The frequency-invariant linear filter can be implemented by convolution in the 
frequency domain or, as indicated in Figure 13.15(b), by multiplication in the time do­
main. Figure 13.16(a) shows the time response of a lowpass frequency-invariant linear 
system as required for recovering an approximation to v[n], and Figure 13.16(b) shows 
the time response of a highpass frequency-invariant linear system for recovering an 
approximation to p[n].l0 
Figure 13.17 shows the result of lowpass frequency-invariant filtering. The more 
rapidly varying curves in Figures 13.17(a) and 13.17(b) are the complex logarithm of 
the Fourier transform of the input signal, i.e., the Fourier transform of the complex cep­
strum. The slowly varying (dashed) curves in Figures 13.17(a) and 13.17(b) are the real 
and imaginary parts, respectively, of the Fourier transform of:Y[n], when the frequency­
invariant linear system l[n] is ofthe form of Figure 13.16(a) with Nt = 14, l'h 
14, and 
with the system of Figure 13.15 implemented using DFTs of length N = 1024. Figure 
13.17(c) shows the corresponding output YIn]. This sequence is the approximation to 
lOFigure 13.16 assumes that the systems D*[·] and D;l[.] are implemented using the DFT as in 
Figure 13.4. 

1017 
Section 13.9 
The Complex Cepstrum for a Simple Multipath Model 
I f[nJ 
I! 
~[------=[l-~[-'--n  
o 
(a) 
-----i , 
1 
,,,,,, 
e[n] 
_ 
Figure 13.16 Time response of 
frequency-invariant linear systems for 
homomorphic deconvolution. 
(a) Lowpass system. (b) Highpass 
system. (Solid line indicates envelope of 
__'L--_-L-__---'______-'-_---.J''--___ n 
the sequence ern] as it would be applied
-N2 
o 
N-N2 
N-l 
in a OFT implementation. The dashed 
(b) 
line indicates the periodic extension.) 
v[n] obtained by homomorphic deconvolution. To relate this output y[n] to v[n], re­
call that in computing the unwrapped phase, a linear-phase component was removed, 
corresponding to a one-sample time shift of v[n]. Consequently, y[n] in Figure 13.17(c) 
corresponds to an approximation to v[n + 1] obtained by homomorphic deconvolution. 
This type of filtering has been successfully used in speech processing to recover 
the vocal tract response information (Oppenheim, 1969b; Schafer and Rabiner, 1970) 
and in seismic signal analysis to recover seismic wavelets (Ulrych, 1971; Tribolet, 1979). 
Figure 13.18 shows the result of highpass frequency-invariant filtering. The rapidly 
varying curves in Figures 13.18(a) and (b) are the real and imaginary parts, respectively, 
of the Fourier transform of jl[n] when the frequency-invariant linear system l[n] is of 
the form of Figure 13.16(b) with Nl = 14 and N2 = 512 (i.e., the negative-time parts are 
completely removed). Again, the system is implemented using a 1024-point DFT. Figure 
13.18(c) shows the corresponding output y[n]. This sequence is the approximation to 
p[n] obtained by homomorphic deconvolution. In contrast to the use of the cepstrum to 
detect echoes or periodicity, this approach seeks to obtain the impulse train that specifies 
the location and size of the repeated copies of v[n). 
13.9.4 Minimum-Phase Decomposition 
In Section 13.8.1, we discussed ways that homomorphic deconvolution could be used 
to decompose a sequence into minimum-phase and allpass components or minimum­
phase and maximum-phase components. We will apply these techniques to the signal 
model of Section 13.9. Specifically, for the parameters of the example, the z-transform 
of the input is 
(0.98 + z-l)(1 + 0.9z-15 + 0.81z-30)
X (z) = V (z) p (z) = -(1------:--,-;---;------:--:",,---:-
(13.107) 

1018 
6 ,r-------,-------~--------~------._------_. 
4 
2 
ta 
0 
-2 
-6 
-4 roooo ......ooT .......... T····· 
0 
0.2n-
0.417 
0.6n-
0.8n-
17 
Radian frequency it) 
(a) 
4 
r,----------~--------~----------._----------r_--------_, 
'" § 
"6 
OJ 
~ 
0.21T 
0.47i-
O.61T 
17 
Radian frequency it) 
(b) 
0 
-2 
-40 
O.Sn­
4 
3 
~... o......... jr.:o .... .  
2 ,......  
0 ••••••• 
I 
<I.l 
. 
. 
.
1 •... 00 •• 
-. ~ ..... - --...... -, .. - ....... - - "." ....... _.... . 
~ 
s « 
o ~ .............. IJllIlllIllIlIllIl.llil.·...··' ., 
.... ~  
-1 
-2 
L'________~__________~________~________~________~ 
-20 
o 
20 
40 
60 
so 
Sample number [n] 
(c) 
Figure 13.17 Lowpass 
frequency-invariant linear filtering in the 
system of Figure 13.15. (a) Real parts of 
the Fou rier transforms of the input (solid 
line) and output (dashed line) of the 
lowpass system with N1 = 14 and 
~ = 14 in Rgure 13.16(a). 
(b) Imaginary parts of the input (solid 
line) and output (dashed line). 
(c) Output sequence y[n] for the input of 
Figure 13.10(c). 

----- ---- ---
2 r-------.-------.-------r-------r-----~ 
Radian frequency w 
(a) 
4 r-------r-------.-------r-----~.-----_, 
Radian frequency w 
(b) 
1.5 r-------,.--------,.--------,.--------,.------~ 
1 
~:a 0.5 
... -­
~ 
-0.5 
-20 
o 
20 
40 
60 
80 
Sample number [n] 
(c) 
Figure 13.18 Illustration of highpass 
frequency-invariant linear filtering in the 
system of Figure 13.15. (a) Real part of 
the Fourier transform of the output of 
the highpass frequency-invariant system 
with N1 
14 and N2 = 512 in 
Figure 13.16(b). (b) Imaginary part for 
conditions of part (a). (c) Output 
sequence y[n] for the input of 
Figure 13.10. 
1019 

1020 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
First, we can write X (z) as the product of a minimum-phase z-transform and an allpass 
z-transform; i.e., 
X(z) 
Xmin(Z)Xap(z), 
(13.108) 
where 
X . (z) = (1 + 0.98z-1)(1 + 0.9z-15 + 0.81z-30) 
(13.109)
min 
(1 -
)(1 _ 
and 
(13.110)  
The sequences Xmin(n] and xap(n] can be found using the partial fraction expan­
sion methods of Chapter 3, and the corresponding complex cepstra Xmin[n] and xap[n] 
can be found using the power series technique of Section 13.5 (see Problem 13.25). 
Alternatively, Xmin[n] and xap[n] can be obtained exactly from x[n] by the operations 
discussed in Section 13.8.1 and as depicted in Figure 13.7. If the characteristic systems 
in Figure 13.7 are implemented using the DFf, then the separation is only approximate 
since xap [n] is infinitely long, but the approximation error can be small over the interval 
where xap[n] is large if the DFf length is large enough. Figure 13.19(a) shows the com­
plex cepstrum for x[n] as computed using a 1024-point DFf, again with a one-sample 
time delay removed from v[n] so that the phase is continuous at lC. Figure 13.19(b) shows 
the complex cepstrum of the minimum-phase component Xmin[n], and Figure 13.19(c) 
shows the complex cepstrum of the allpass component xap[n] as obtained by the oper­
ations of Figure 13.7 with D*[·] implemented as in Figure 13.4(a). 
Using the DFf as in Figure 13.4(b) to implement the system D; 1[.] gives the ap­
proximations to the minimum-phase and allpass components shown in Figures 13.20(a) 
and 13.20(b), respectively. Since all the zeros of P(z) are inside the unit circle, all of P(z) 
is included in the minimum-phase z-transform or, equivalently, p[n] is entirely included 
in Xmin [n]. Thus, the minimum-phase component consists of delayed and scaled replicas 
of the minimum-phase component of v[n]. Therefore, the minimum-phase component 
of Figure 13.20(a) appears very similar to the input shown in Figure 13.10(c). From 
Eq. (13.110), the allpass component can be shown to be 
xap[n] = 0.988[n] + 0.0396(-0.98)n-1 urn - 1]. 
(13.111) 
The result of Figure 13.20(b) is very close to this ideal result for small values of n where 
the sequence values are of significant amplitude. This example illustrates a technique 
of decomposition that has been applied by Bauman, Lipshitz and Vanderkooy (1985) 
in the analysis and characterization of the response of electroacoustic transducers. A 
similar decomposition technique can be used to factor magnitude-squared functions as 
required in digital filter design (see Problem 13.27). 
As an alternative to the minimum-phase/all pass decomposition, we can express 
X (z) as the product of a minimum-phase z-transform and a maximum-phase z-transform; 
i.e., 
X (z) 
Xmn (z)XmAz), 
(13.112) 
where 
(1 + 0.9Z-15 + O.81Z-30) 
(13.113)
Xmn(Z) = (1- 0.gejrr/6z-1)(1- O.ge-jrr/6z-1) 
.......  

2,---------,-------~,-------~,-------_, 
1.5 
GJ 
1 
. ~ 
..........................  
~ 
} 0.5 
< 
· 
..
· 
. 
.............................. 
.... :..................:. __ .-_._--_ ...... . 
· 
-100 
-50 
o 
50 
100  
Sample number [n]  
(a) 
3r--------.,-------~.-------~--------_, 
2.5 .............................. ~ ..  
·· 
. 
..
••••••••••••• - - - • - r - ••••••••••••••• ~. •••• 
• ••••••••• - . (' - •••••• - •••••••••
2 
· 
. 
.
· 
.
· 
.
· 
.
<U 1.5
-0 a 
'a 
E < 0.5 
o~------~--------~~__..~~----~­
-0.5 .................., ...... .  
-100 
-50 
o 
50 
100  
Sample number [nJ  
(b) 
1.5 ,----------,..----------,..----------,_----------, 
1 .................., ....................................c......  
· 
.
· 
.
· 
. 
0.5 ., ............. ­
-0 '" 
.-8 
0 !------~---wmlHH~--~-----1
} 
.................... -.---_. __ ................... 
<....{l.5 
· 
.
· 
.
· 
.
· 
. 
· 
, 
. 
.................. --------_....................... . 
-1 
, 
-1.5 L....-__ ..........._'--____~'---______~'---_______  
Figure 13.19 (a) Complex cepstrum of 
-100 
-50 
o 
50 
100 
x[n] = xminn * Xap[nj. (b) Complex
Sample number [n] 
cepstrum of xmin[nj. (c) Complex 
(c) 
cepstrum of Xap[nj. 
1021 

1022 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
4  
3 r··············.·,···········.·,···············.······.........,.............  
2 
] 
I\) 
1
~ 
e 
<: 
0 
-1 
-2 -20 
o 
20 
40 
60 
80 
Sample number [11] 
(a) 
1.5 
1 ...............................:.............. ~............. 
.g 
:=.'.:: 05 •...
i' 
., .•...• 
• •• ~ " ....... - - - - - - - - - ... 
- - - ••••• -0" •• - •••••••••• 
<: 
o I .... •• .... II 
, 
, 
1·!\;y.Y.tn'/'r~~.·...·i-.·..,.....••·.......·............·.·.·~ 
--0.5 
L.'_ _ _--'~_ _ _...L-___--'-____'--___...J 
~ 
0 
W 
40 
W 
80 
Figure 13.20 (a) Minimum-phase 
Sample number [11] 
output. (b) Allpass output obtained as 
(b) 
depicted in Figure 13.7. 
and 
Xmx (z) = 0.98z + 1. 
(13.114) 
The sequences Xmn [n] and Xmx In] can be found using the partial fraction expansion 
methods of Chapter 3, and the corresponding complex cepstra xmn [n] and xmx [n] can be 
found using the power series technique of Section 13,5 (see Problem 13.25). Alterna­
tively, xmn [n] and xmAn] can be obtained exactly from x [n] by the operations discussed 
in Section 13.8.2 and as depicted in Figure 13.8, where 
.emn[n] = u[n] 
(13.115) 
and 
.emx[n] = u[-n 
1]. 
(13.116) 
That is, the minimum-phase sequence is now defined by the positive time part of the 
complex cepstrum and the maximum-phase part is defined by the negative time part 
of the complex cepstrum. If the characteristic systems in Figure 13.8 are implemented 
using the DFf, the negative time part of the complex cepstrum is positioned in the 
....  

1023 
Section 13.9 
The Complex Cepstrum for aSimple Multipath Model 
last half of the DFf interval. In this case, the separation of the minimum-phase and 
maximum-phase components is only approximate because of time aliasing, but the 
time-aliasing error can be made small by choosing a sufficiently large DFf length. 
Figure 13.19(a) shows the complex cepstrum of x[n] as computed using a 1024-point 
DFI: Figure 13.21 shows the two output sequences that are obtained from the complex 
cepstrum of Figure 13.19(a) using Eqs. (13.87) and (13.88) as in Fig 13.8 with the inverse 
characteristic system being implemented using the DFf as in Figure 13.4(b). As before, 
since p[n] is entirely included in xmn[n], the corresponding output xmn[n] consists of 
delayed and scaled replicas of a minimum-phase sequence, thus, it also looks very much 
like the input sequence. However, a careful comparison of Figures 13.20(a) and 13.21(a) 
shows that Xmin[n] =fo xmn[n]. From Eq. (13.114), the maximum-phase sequence is 
Xmx[n] 
O.988[n + 1] T 8[nJ.  
(13.117) 
Figure 13.21(b) is very close to this ideal result. (Note the shift due to the linear phase 
removed in the phase unwrapping.) This technique of minimum-phase/maximum-phase 
decomposition was used by Smith and Barnwell (1984) inthe design and implementation 
of exact reconstruction filter banks for speech analysis and coding. 
2 .-------,-----~,-----~,-----~,------, 
1.5 
1 
-D.5 
-1 ~------~----~~----~~----~~----~  
-20 
o 
20 
40 
60 
80  
Sample number [n]  
(a) 
1.5  ,-------.---------.----------,,-------,-----_, 
1 ...... 
.g 
. 
.
,B 
...... ., .......................... . 
Q.. 0.5 
a 
<t: 
o ~----~~------~------~------~----~ 
-D.5 '--______L-______'--______'--_____'--____---l 
-20 
o 
20 
40 
60 
80 
Figure 13.21 
(a) Minimum-phase 
Sample number [n] 
output. (b) Maximum-phase output 
(b)  
obtained as depicted in Figure 13.8. 

I 
1024  
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
13.9.5 Generalizations 
The example in Section 13.9 considered a simple exponential signal that was convolved 
with an impulse train to produce a series ofdelayed and scaled replicas ofthe exponential 
signal. This model illustrates many of the features of the complex cepstrum and of 
homomorphic filtering. 
In particular, in more general models associated with speech, communication, and 
seismic applications an appropriate signal model consists of the convolution of two com­
ponents. One component has the characteristics of v[n], specifically a Fourier transform 
that is slowly varying in frequency. The second has the characteristics of p[n], i.e., an 
echo pattern or impulse train for which the Fourier transform is more rapidly varying 
and quasiperiodic in frequency. Thus, the contributions of the two components would 
be separated in the complex cepstrum or the cepstrum, and, furthermore, the complex 
cepstrum or the cepstrum would contain impulses at multiples of the echo delays. Thus, 
homomorphic filtering can be used to separate the .convolutional components of the 
signal, or the cepstrum can be used to detect echo delays. In the next section, we will 
illustrate the use of these general properties of the cepstrum in applications to speech 
analysis. 
13.10 APPLICATIONS TO SPEECH PROCESSING 
Cepstrum techniques have been applied successfully to speech analysis in a variety of 
ways. As discussed briefly in this section, the previous theoretical discussion and the 
extended example of Section 13.9 apply in a relatively straightforward way to speech 
analysis. 
13.10.1 The Speech Model 
As we briefly described in Section 1004.1, there are three basic classes of speech sounds 
corresponding to different forms of excitation of the vocal tract. Specifically: 
•  Voiced sounds are produced by exciting the vocal tract with quasiperiodic pulses 
of airflow caused by the opening and closing of the glottis. 
•  Fricative sounds are produced by forming a constriction somewhere in the vocal 
tract and forcing air through the constriction so that turbulence is created, thereby 
producing a noise-like excitation. 
•  Plosive sounds are produced by completely closing off the vocal tract, building up 
pressure behind the closure, and then abruptly releasing the pressure. 
In each case, the speech signal is produced by exciting the vocal tract system (an acous­
tic transmission system) with a wideband excitation. The vocal tract shape changes 
relatively slowly with time, thus, it can be modeled as a slowly time-varying filter that 
imposes its frequency-response properties on the spectrum of the excitation. The vocal 
tract is characterized by its natural frequencies (called formants), which correspond to 
resonances in its frequency response. 

1025 
Section 13.10 
Applications to Speech Processing 
Pitch period 
System coefficients 
(vocal tract parameters) 
.1...1,. I .. 
Impulse 
..--_.... p[n]
train 
generator 
s[n]
Time-varying 
}-----+-l discrete-time 1---"'" 
Speech
system
Random 
samples 
number 
generator 
Amplitude 
Figure 13.22 Discrete-time model of speech production. 
Ifwe assume that the excitation sources and the vocal tract shape are independent, 
we arrive at the discrete-time model of Figure 13.22 as a representation of the sampled 
speech waveform. In this model, samples of the speech signal are assumed to be the 
output of a time-varying discrete-time system that models the resonances of the vocal 
tract system. The mode of excitation of the system switches between periodic impulses 
and random noise, depending on the type of sound being produced. 
Since the vocal tract shape changes rather slowly in continuous speech, it is rea­
sonable to assume that the discrete-time system in the model has fixed properties over a 
time interval on the order of 10 ms. Thus, the discrete-time system may be characterized 
in each such time interval by an impulse response or a frequency response or a set of 
coefficients for an IIR system. Specifically, a model for the system function of the vocal 
tract takes the form 
V(z) 
(13.118) 
or, equivalently, 
K; 
Ko 
1
ACKo D(1- akz- ) D(1- 1hz) 
k=l 
k=l
V (z) = -[P-/z-]-:.:.....::.:....----:.:.....::.:....---
(13.119) 
D (1- rkejllk z-1)(1 
rke-jllkz-l) 
k=l 
where the quantities rkejllk {with Irhl < 1 are the complex natural frequencies of the 
vocal tract, which, of course, are dependent on the vocal tract shape and consequently 
are time varying. The zeros of V (z) account for the finite-duration glottal pulse waveform 
and for the zeros of transmission caused by the constrictions of the vocal tract in the 
creation ofnasal voiced sounds and fricatives. Such zeros are often not included, because 
it is very difficult to estimate their locations from only the speech waveform. Also, it has 

Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution
1026 
been shown (Atal and Hanauer, 1971) that the spectral shape ofthe speech signal can be 
accurately modeled using no zeros, if we include extra poles beyond the number needed 
just to account for the vocal tract resonances. The zeros are included in our analysis, 
because they are necessary for an accurate representation of the complex cepstrum of 
speech. Note that we include the possibility of zeros outside the unit circle. 
The vocal tract system is excited by an excitation sequence p[n], which is a train 
of impulses when modeling voiced speech sounds and r[n], which is a pseudorandom 
noise sequence when modeling unvoiced speech sounds, such as fricatives and plosives. 
Many of the fundamental problems of speech processing reduce to the estimation 
of the parameters of the model of Figure 13.22. These parameters are as follows: 
• The coefficients of V(z) inEq. (13.118) or the pole and zero locations inEq. (13.119) 
• The mode of excitation of the vocal tract system; i.e., a periodic impulse train or 
random noise 
• The amplitude of the excitation signal 
• The pitch period of the speech excitation for voiced speech. 
Homomorphic deconvolution can be applied to the estimation of the parameters if it 
is assumed that the model is valid over a short time interval, so that a short segment of 
length L samples of the sampled speech signal can be thought of as the convolution 
s[n] = v[n] * p[n] 
for 0 ::: n ::: L - 1, 
(13.120) 
where v[n] is the impulse response of the vocal tract and p[n] is either periodic (for 
voiced speech) or random noise (for unvoiced speech). Obviously, Eq. (13.120) is not 
valid at the edges of the interval, because of pulses that occur before the beginning of 
the analysis interval and pulses that end after the end of the interval. To mitigate the 
effect of the "discontinuities" of the model at the ends of the interval, the speech signal 
s[n] can be multiplied by a window w[n] that tapers smoothly to zero at both ends. Thus, 
the input to the homomorphic deconvolution system is 
x[n] 
w[n]s[n]. 
(13.121) 
Let us first consider the case of voiced speech. If w[n] varies slowly with respect 
to the variations of v[n], then the analysis is greatly simplified if we assume that 
x[n] 
v[n] * pw[n], 
(13.122) 
where 
pw[n] = w[n]p[n]. 
(13.123) 
(See Oppenheim and Schafer, 1968.) A more detailed analysis without this assumption 
leads to essentially the same conclusions as below (Verhelst and Steenhaut, 1986). For 
voiced speech, p[n] is a train of impulses of the form 
M-l 
p[n] = L 8[n - kNol 
(13.124) 
k=O 
so that 
M-l 
pw[n] L w[kNo18[n - kNoL 
(13.125) 
k=O 

Section 13.10 
Applications to Speech Processing 
1027 
where we have assumed that the pitch period is No and that M periods are spanned by 
the window. 
The complex cepstra of x[n], v[n], and pw[n] are related by 
x[n] = v[n] + pw[n]. 
(13.126) 
To obtain pw[n], we define a sequence 
w[kNoL k 
0, 1, ... , M 
1, 
(13.127)
{ 0, 
otherwise, 
whose Fourier transform is 
M-J 
Pw(ejW) = L w[kNoJe-{J>kNo 
WNo(e)wNO). 
(13.128) 
k=O 
Thus, Pw(e]W) and Pw(e]W) are both periodic with period 2rr/No, and the complex cep­
strum of pw[n] is 
, [ ] _ {wNo[n/No], n = 0, ±No, ±2No, ... , 
(13.129)
Pw n -
0, 
otherwise. 
The periodicity of the complex logarithm resulting from the periodicity of the voiced 
speech signal is manifest in the complex cepstrum as impulses spaced at integer multiples 
of No samples (the pitch period). If the sequence wNo[n] is minimum phase, then Pwln] 
will be zero for n < O. Otherwise, Pw [n] will have impulses spaced at intervals of No 
samples for both positive and negative values of n. In either case, the contribution of 
pw[n] to x[n] will be found in the intervallnl 2: No. 
From the power series expansion of the complex logarithm of V (z), it can be shown 
that the contribution to the complex cepstrum due to v[n] is 
f3k
Tl 
~ 
, 
n < 0, 
n 
k=l 
v[n] = 
log IAI> 
n =0, 
(13.130) 
Ki 
TI 
[P/2] 
k
- L Ci + L 
COS(Okn), n > O. 
n 
n 
k=l 
k=l 
As with the simpler example in Section 13.9.1, the term Z-Ko in Eq. (13.119) 
represents a linear-phase factor that would be removed in obtaining the unwrapped 
phase and the complex cepstrum. Consequently, v[n] in Eq. (13.130) more accurately is 
the complex cepstrum of v[n + Kol. 
From Eq. (13.130), we see that the contributions of the vocal tract response to 
the complex cepstrum occupy the full range -00 < n < 00, but they are concentrated 
around n 
O. We note also that since the vocal tract resonances are represented by 
poles inside the unit circle, their contribution to the complex cepstrum is zero for n < O. 

1028  
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
13.10.2  Example of Homomorphic Deconvolution of 
Speech 
For speech sampled at 10,000 samples/s, the pitch period No will range from about 25 
samples for a high-pitched voice up to about 150 samples for a very low-pitched voice. 
Since the vocal tract component of the complex cepstrum D[n] decays rapidly, the peaks 
of pw(n] stand out from D(n]. In other words, in the complex logarithm, the vocal tract 
components are slowly varying, and the excitation components are rapidly varying. This 
is illustrated by the following example. Figure 13.23( a) shows a segment of a speech wave 
Input Speech Segment 
1 r,------~r_------,_------~--------~------_r------~r_------,_------~--------r_------~ 
0.5 
o 
-D.5 
~!  
~ 
o 
5 
10 
15 
20 
25 
30 
35 
40 
45 
SO  
Time (msec)  
(a)  
High Quefrency Component of the Input 
0.5 
o 
1  
.J.. 
I  
~ 
-0.5 o  
5 
10 
15 
20 
25 
30 
35 
40 
45 
SO 
Time (msec) 
(b) 
Low Quefrency Component of the Input 
~ 
o 
5 
10 
15 
20 
25 
SO  
Time (msec)  
(c)  
Figure 13.23 Homomorphic deconvolution of speech. (a) Segment of speech weighted by 
a Hamming window. (b) High quefrency component of the signal in (a). (c) Low quefrency 
component of the signal in (a). 
0.5 
o ! 
-D.5 
-1 
30 
35 
40 
45 

Section 13.10 
Applications to Speech Processing 
1029 
Log Magnitude 
iii' 
~ 
0 
"0 
B
·S 
Oil 
~ 
E 
Oil 
0 
.....l 
4 
2 
-2 
-4 
-6 
0'. _ •• 
~ __ ••• ____ ... 
~ ___ ••• 
· 
.
· 
.
· 
. 
. --.-.. ----.- '.. --.. -­ -.. -...'...----.. -­
.- ­ -~ .. ---.....'. -... ---.
, 
, 
. 
. 
, 
, 
. 
,
. 
, 
, 
, 
, 
, 
, 
,
.. 
.
. 
.. 
,
.z. ________ .. _____ ..... ____ ,..... _____ ......_... ___ •.. __ ... _____ .. __ ....... __ ... ____ ... __ •... __ ... _... ____ ._ 
, 
. , . . .  
_._------_._._ ... 
, 
, . .
· 
..
· 
.
· 
.
· 
.
• - - - - -
~ • - - 'l 
- - - - -
- - - • - 7 • -
-8 I 
I 
0 
500 
1000 
1500 
2000 
2500 
3000 
3500 
4000 
Frequency (Hz) 
(a) 
Unwrapped Phase Curve 
10  
5  
~ 
"0 
0 
E-
o 
en 
~ -5 
..c 
t:I.. 
-10  
-15  
• ___ ••• ___ •• Jo __ ••• ____ •• _.! •••• ____ •• __ _
· 
. 
0 
500 
1000 
1500 
2000 
2500 
3000 
3500 
4000 
Frequency (Hz) 
(b) 
Figure 13.24 Complex logarithm of the signal of Figure 13.23(a): (a) Log magnitude. 
(b) Unwrapped phase. 
multiplied by a Hamming window of length 401 samples (50 ms time duration at a sam­
pling rate of 8000 samples/s). Figure 13.24 shows the complex -logarithm (log magnitude 
and unwrapped phase) of the DFf of the signal in Figure 13.23(a).11 Note the rapidly 
varying, almost periodic component due to pw[n] and the slowly varying component 
due to v[n]. These properties are manifest in the complex cepstrum of Figure 13.25 
in the form of impUlses at multiples of approximately 13 ms (the period of the input 
speech segment) due to pw[n] and in the samples in the region InTI < 5 ms, which we 
attribute to v[n]. As in the previous section, frequency-invariant filtering can be used 
11 In all the figures of this section, the samples of all sequences were connected for ease in plotting. 

1030 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
Complex Cepstrum of Speech Segmcnt 
3 
2 
1 
o ' 
,ft 
-1\ "",. -.. 
""",,,,,,*,,,u.Iu._ , 
, " ."'""I0Il.....___---1 
-1 
-2 
-3 
-20 
-15 
-10 
-5 
0 
5 
10 
15 
20 
Quefrency (msec) 
Figure 13.25 
Complex cepstrum of the signal in Figure 13.23(a} (inverse DTFT of the  
complex logarithm in Figure 13.24).  
to separate the components of the convolutional model of the speech signal. Lowpass 
filtering of the complex logarithm can be used to recover an approximation to v[n], and 
bighpass filtering can be used to obtain pw[n]. Figure 13.23(c) shows an approximation 
to v[n] obtained by using a lowpass frequency-invariant filter as in Figure 13.16(a) with 
Nt 
30 and N2 = 30. The slowly varying dotted curves in Figure 13.24 show the com­
plex logarithm of the DTFf of the low quefrency component shown in Figure 13.23( c). 
On the other hand, Figure 13.23(b) is an approximation to pw[n] obtained by applying 
to the complex cepstrum a symmetrical high pass frequency-invariant filter as in Fig­
ure 13.16(b) with Nl = 95 and N2 
95. In both cases, the inverse characteristic system 
was implemented by using 1024-point DFfs, as in Figure 13A(b). 
13.10.3 Estimating the Parameters of the Speech Model 
Although homomorphic deconvolution can be successfully applied in separating the 
components of a speech waveform, in many speech processing applications we are 
interested only in estimating the parameters in a parametric representation of the speech 
signal. Since the properties of the speech signal change rather slowly with time, it is 
common to estimate the parameters of the model of Figure 13.22 at intervals of about 
10 ms (100 times/s). In this case, the time-dependent Fourier transform discussed in 
Chapter 10 serves as the basis for time-dependent homomorphic analysis. For example, 
it may be sufficient to examine segments of speech selected about every 10 ms (100 
samples at 10,000 Hz sampling rate) to determine the mode of excitation of the model 
(voiced or unvoiced) and, for voiced speech, the pitch period. Or we may wish to track 
the variation of the vocal tract resonances (formants). For such problems, the phase 
computation can be avoided by using the cepstrum, which requires only the logarithm 
of the magnitude of the Fourier transform. Since the cepstrum is the even part of the 
i...  

1031 
Section 13.10 
Applications to Speech Processing 
~ 
10FT 
OF}'
ST4 
OFT 
10gH M 
~
~ 
Data 
Cepstrum 
window 
window 
(a) 
Input speech segment  
(normalized and weighted  
by a Hamming window) 
Cepstrum 
Spectra  
~~ 
-60 l 
r 
j . !  
--'-
~1_-----.J 
o 
10 
20 
30 
40 
o 
4 
8 
12 
16 
20 
o 
1 
2 
3 
4 
5 
Time (ms)  
Time (ms) 
Frequency (kHz) 
Analysis for Voiced Speech 
(b) 
Input speech segment  
(normalized and weighted  
by a Hamming window)  
o  
10 
20 
30 
40 
Time (ms) 
Time (ms) 
Frequency (kHz) 
Analysis for Unvoiced Speech 
(c) 
Figure 13.26 
(a) System for cepstrum analysis of speech signals. (b) Analysis for voice 
speech. (c) Analysis for unvoiced speech. 
complex cepstrum, our previous discussion suggests that the low-time portion of cAn] 
should correspond to the slowly varying components ofthe log magnitude of the Fourier 
transform of the speech segment, and for voiced speech, the cepstrum should contain 
impulses at multiples of the pitch period. An example is shown in Figure 13.26. 
Figure 13.26(a) shows the operations involved in estimating the speech parame­
ters using the cepstrum. Figure 13.26(b) shows a typical result for voiced speech. The 
windowed speech signal is labeled A, log IX[k]1 is labeled C, and the cepstrum cx[n] is 
labeled D. The peak in the cepstrum at about 8 ms indicates that this segment of speech 
is voiced with that period. The smoothed spectrum, or spectrum envelope, obtained by 
frequency-invariant lowpass filtering with cutoff below 8 ms is labeled E and is super­
imposed on C. The situation for unvoiced speech, shown in Figure 13.26(c), is similar, 
except that the random nature of the excitation component of the input speech segment 

1032 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
causes a rapidly varying random component in log IX[k]1 instead of a periodic compo­
nent. Thus, in the cepstrum the low-time components correspond as before to the vocal 
tract system function; however, since the rapid variations in log IX[k]1 are not periodic, 
no strong peak appears in the cepstrum. Therefore, the presence or absence of a peak 
in the cepstrum in the normal pitch period range serves as a very good voicedlunvoiced 
detector and pitch period estimator. The result of lowpass frequency-invariant filtering 
in the unvoiced case is similar to that in the voiced case. A smoothed spectrum envelope 
estimate is obtained as in E. 
In speech analysis applications, the operations of Figure 13.26(a) are applied re­
peatedly to sequential segments of the speech waveform. The length of the segments 
must be carefully selected. If the segments are too long, the properties of the speech 
signal will change too much across the segment. If the segments are too short, there 
will not be enough of the signal to obtain a strong indication of periodicity. Usually 
the segment length is set at about three to four times the average pitch period of the 
speech signaL Figure 13.27 shows an example of how the cepstrum can be used for pitch 
detection and for estimation of the vocal tract resonance frequencies. Figure 13.27(a) 
shows a sequence of cepstra computed for speech waveform segments selected at 20-ms 
intervals. The existence of a prominent peak throughout the sequence of speech seg­
ments indicates that the speech was voiced throughout. The location of the cepstrum 
peak indicates the value of the pitch period in each corresponding time interval. Figure 
13.27(b) shows the log magnitude with the corresponding smoothed spectra superim­
posed. The lines connect estimates of the vocal tract resonances obtained by a heuristic 
peak-picking algorithm. (See Schafer and Rabiner, 1970.) 
13.10.4 Applications 
As indicated previously, cepstrum analysis methods have found widespread application 
in speech processing problems. One of the most successful applications is in pitch de­
tection (Noll, 1967). They also have been used successfully in speech analysis/synthesis 
systems for low bit-rate coding of the speech signal (Oppenheim, 1969b; Schafer and 
Rabiner, 1970). 
Cepstrum representations of speech have also been used with considerable suc­
cess in pattern recognition problems associated with speech processing such as speaker 
identification (Atal, 1976), speaker verification (Furui, 1981) and speech recognition 
(Davis and Mermelstein, 1980). Although the technique of linear predictive analysis of 
Chapter 11 is the most widely used method of obtaining a representation of the vocal 
tract component of the speech model, the linear predictive model representation is of­
ten transformed to a cepstrum representation for use in pattern recognition problems 
(Schroeder, 1981; luang, Rabiner and Wilpon 1987). This transformation is explored in 
Problem 13.30. 
13.11 SUMMARY 
In this chapter, we discussed the technique of cepstrum analysis and homomorphic 
deconvolution. We focused primarily on definitions and properties of the complex cep­
strum and on the practical problems in the computation of the complex cepstrum. An 

~l. 
J. 
I", 
L 
j 
'PI 
L 
j 
1,..­
~L 
j 
.... 
.. 
ft 
1 
IV' 
I!.­
1
, 
.... 
1 
I' 
.&. 
1"'­
A
.. 
... 
A 
" 
A 
I'" 
A 
w 
o 
2 
4 
6 
8 
10 12 14 16 18 20 
o 
1 
2 
3 
4 
Time (ms) 
Frequency (kHz) 
(a) 
(b) 
Figure 13.27 
(a) Cepstra and (b) log spectra for sequential segments of voiced 
speech. 
1033 

1034  
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
idealized example was discussed to illustrate the use of cepstrum analysis and homo­
morphic deconvolution for separating components of a convolution. The application 
of cepstrum analysis techniques to speech processing problems was discussed in some 
detail as an illustration of their use in a real application. 
Problems 
Basic Problems 
13.1.  (a) Consider a discrete-time system that is linear in the conventional sense. If yIn] 
T[x[n]} is the output when the input is x[n], then the zero signal O[n1is the signal that 
can be added to x[n] such that T[x[nJ + O[n]} = y[n] + T{O[nlJ = y[n]. What is the 
zero signal for conventional linear systems? 
(b)  Consider a discrete-time system y[n] = T {x[nJl that is homomorphic, with con­
volution as the operation for combining signals at both the input and the output. 
What is the zero signal for such a system; i.e., what is the signal O[n] such that 
T(x[n] *O[n]J = y[n] * T{O[n]J 
y[nJ? 
(c)  Consider a discrete-time system yIn] 
T(x[n]} that is homomorphic, with con­
volution as the operation for combining signals at both the input and the output. 
What is the zero signal for such a system; 
what is the signal O[n] such that 
T{x[n] *O[n]} 
y[n] * T{O[n]) = y[n]? 
13.2.  Let Xl In] and x2[n] denote two sequences and Xl [n] and x2[n] their corresponding complex 
cepstra. If xt£n] *x2[n] 
o[nj, determine the relationship between xl[n] and x2[n]. 
13.3.  In considering the implementation of homomorphic systems for convolution, we restricted 
our attention to input signals with rational z-transforms of the form of Eq. (13.32). If an 
input sequence x[n] has a rational z-transform but has either a negative gain constant or 
an amount of delay not represented by Eq. (13.32), then we can obtain a z-transform of 
the form of Eq. (13.32) by shifting x[n] appropriately and multiplying by -1. The complex  
cepstrum may then be computed using Eq. (13.33).  
Suppose that x[n] 
o[n] - 2o[n -1], and define y[n] 
ax[n 
rJ, where ex = ±1  
and r is an integer. Find ex and r such that Y(z) is in the form of Eq. (13.32), and then find  
yIn].  
13.4.  In Section 13.5.1, we stated that linear-phase contributions should be removed from the 
unwrapped phase curve before computation of the complex cepstrum. This problem is 
concerned with the effect of not removing the linear-phase component due to the factor 
zr in Eq. (l3.29). 
Specifically, assume that the input to the characteristic system for convolution is  
xln] = o[n + r]. Show that formal application of the Fourier transform definition  
1 iJr 
..
x[n] 
10g[X(eJ"')]eJwndw 
(P13.4-1)
2n  -Jr 
I 
leads to  
cos(nn) 
0  
r---, n i= ,
x[nl 
n 
0, 
n =0. 
The advantage of removing the linear-phase component of the phase is clear from this  
result, since for large r such a component would dominate the complex cepstrum.  
J.... 

---- - - - - ----------- - -------- - - - --- - -------- - --- - ------ - - - ---------- - -------
Chapter 13 
Problems  
1035 
13.5.  Suppose that the z-transform of s[n] is 
(1 
lz-l)(l_l z)
S(z) = __2 
4. 
(1_~Cl)(l !z) 
Determine the pole locations of the z-transform of ns[n], other than poles at Izi 
0 or 00. 
13.6.  Suppose that the complex cepstrum of yIn] is .Y[n] 
sIn] +2o[n]. Determine yIn] in terms 
of s[n]. 
13.7.  Determine the complex cepstrum of x[n] = 2o[n] - 2o[n -1] + O.50[n 
2], shifting x[n] 
or changing its sign, if necessary. 
13.8.  Suppose that the z-transform of a stable sequence x[n] is given by 
X(z) 
and that a stable sequence yIn] has complex cepstrum yIn] 
x[-n], where x[n] is the 
complex cepstrum of x[n]. Determine yIn]. 
13.9.  Equations (13.65) and (13.68) are recursive relationships that can be used to compute the 
complex cepstrum x[n] when the input sequence x[n] is minimum phase and maximum 
phase, respectively. 
(a)  Use Eq. (13.65) to compute recursively the complex cepstrum of the sequence 
x[n] = anu[n], where lal < l. 
(b)  Use Eq. (13.68) to compute recursively the complex cepstrum of the sequence 
x[n] 
,sIn] 
aa[n + 1], where lal < l. 
13.10.  ARG{X(ejwH represents the principal value of the phase of X(ejW), and arg\X(ejW )} 
represents the continuous phase of X(ejW). Suppose that ARG[X (ejW)} has been sampled 
at frequencies ())k = 2rck/N to obtain ARG{X[kH = ARG{X(ej (2rr/N)k)} as shown in 
Figure P13.lO. Assuming that larg\X[k]} - arg\X[k -1]}1 < 1< for all k,determine and plot 
the sequence r[k] as in Eq. (13.49) and arg[X[k]} for 0 ::s k ::s 10. 
ARG(X[k]) 
7T 
-----------------------------------------.----------.-------------------­
-----.----------.----------------.-------------------------------------­
'--+--+--+--+--+--+--+--~-+--~---------k 
5  
10 
-----------------------.------------- ---------------------.-------------­
-7T -----------.------- ---- - ----.- ----------------- --- - ----- --- - ---------- --­
Figure P13.1D 

1036  
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
13.11.  Let x[n] be the complex cepstrum of a real-valued sequence x[n]. Specify whether each 
of the following statements is true or false. Give brief justifications for your answers. 
Statement1: Ifxl[n]=x[-n]thenxl[n] 
.~[-n].  
Statement 2: Since x[n] is real-valued, the complex cepstrum x[n] must also be real-valued.  
Advanced Problems 
13.12.  Consider the system depicted in Figure P13.12, where S1 is an LTl system with impulse 
response hl en] and S2 is a homomorphic system with convolution as the input and output 
operations; i.e., the transformation T2{'} satisfies 
T2(w1[n] * w2[n]} 
T2{w1 en]} * Tz(wz[n]}. 
Suppose that the complex cepstrum of the input x[n] is x[n] 
8[n] + 8[n - 1]. Find a 
closed-form expression for h1 [n] such that the output is yen] 
8[n]. 
~ 
T2(· J 
yIn] •
~ 
h,ln] I 
s{ 
S2 
Figure P13.12 
13.13.  The complex cepstrum of afinite length signalx[nJ is computed as shown in Figure P13.13-l. 
Suppose we knowthatx[n] is minimum phase (all poles and zeros are inside the unit circle). 
We use the system shown in Figure P13.13-2 to find the real cepstrum of x[n]. Explain how 
to construct x[n] from cx[n]. 
x[n] 
DTFr 
log[X(eiw)]  
x[n]
Inverse 
DTFT 
Figure P13.l3-1 
x[n] 
DTFr 
!ogIX(ejW )I I----i Inverse 
cAn] 
DTFr 
I 
Figure P13.13-2 
13.14.  Consider the class of sequences that are real and stable and whose z-transforms are of the 
form 
Mi 
MQ 
1
n(1 
akZ- ) n(1 
bkZ) 
X(z) = IAI_k=_l________ 
Ni 
No
n(1 
qrl) n(1 - dkZ) 
k=l 
k=l 
where lakl, Ibkl, Iql, Idk I < 1. LeU[n] denote the complex cepstrum of x[n]. 
(a) Let yen] = x[-nJ. Determine y[nJ in terms of x[n]. 
(b) If x[n] is causal, is it also minimum phase? Explain. 

, .
Chapter 13 
Problems 
1037 
(c)  Suppose that x[n] is a finite-duration sequence such that 
Mi 
Mo 
1
X(z) = IAI Tl (1- akZ- ) Tl (1- bkZ), 
k=l 
k=l 
with lak I < 1 and Ibk I < 1. The function X (z) has zeros inside and outside the unit 
circle, Suppose that we wish to determine yIn] such that IY(ejW)1 
IX(ejwYI and 
Y(z) has no zeros outside the unit circle. One approach that achieves this objective is 
depicted in Figure P13.14. Determine the required sequence f[n). A possible applica­
tion of the system in Figure P13.14 is to stabilize an unstable system by applying the 
transformation of Figure P13.14 to the sequence of coefficients of the denominator 
of the system function. 
X~l
! 
logl·1 
£[nJ 
Figure P13.14 
13.15. It can be shown (see Problem 3.50) that if x[nJ = 0 for n < 0, then 
x[O] = lim X(z).
2-+00 
This result was called the initial value theorem for right-sided sequences. 
(a)  Prove a similar result for left-sided sequences, i.e., for sequences such that x[n] = 0 
for n > O. 
(b) Use the initial value theorems to prove that x[O] 
log(x[O]) if x[n] is a minimum-
phase sequence. 
(c)  Use the initial value theorems to prove that x[O] = 10g(x[0j) if xln] is a maximum­
phase sequence. 
(d) Use the initial value theorems to prove that X[O] = log IAI when X (z) is given by 
Eq. (13.32). Is this result consistent with the results of parts (b) and (c)? 
13.16. Consider a sequence x[n] with complex cepstrum x[nl, such that i[n] = -i[-n]. Deter­
mine the quantity 
13.17. Consider a real, stable, even, two-sided sequence h[n]. The Fourier transform of h[n] is 
positive for all w, 
-1T < w::: 1T. 
Assume that the z-transform of h[n] exists. Do not assume that B(z) is rational. 
(a)  Show that there exists a minimum-phase signal gIn], such that 
B(z) = G(z)G(z-l), 
where G(z) is the z-transform of a sequence gin], which has the property that gIn] 
0 
for n < O. State explicitly the relationship between h[n] and gIn], the complex cepstra 
of h[n] and gIn], respectively. 

1038  
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
(b)  Given a stable signal sen], with rational z-transforrn 
(1 
2z-1)(1­
S(z) = -----';''---:­
(1-4Z-1)(1 
Define h[n] 
s[n] *s[-n]. Find G(z) (as in part (a») in terms of S(z). 
(c)  Consider the system in Figure P13.17, where i[n] is defined as 
ern] 
urn 
1] + (-I)nu[n 
1]. 
Determine the most general conditions on x[n] such that y[n] = x [n] for all n. 
... Fourier 
X(ejw) 
Inverse 
transform 
log IX(ejO!)1 
Fourier 
transform 
, 
f[n] 
Fourier 
transform 
x 
Inverse 
Complex
Fourier 
exponential
transform 
Figure P13.17 
13.18. Consider a maximum-phase signal x[n]. 
(a)  Show that the complex cepstrum x[n] of a maximum-phase signal is related to its 
cepstrum cx[n] by 
x[n j 
ex [n jimax [n], 
where imax[n] = 2u[-n] 
8[n]. 
(b) Using the relationships in part (a), show that 
. 
1 j1f 
'f! 
(w -e)
arg{X(e1U>)) = -p 
loglX(e1 )Icot --
de. 
2n 
-1f 
2 
(c) Also show that 
. 
1 
.f!
j1f 
(we)
log I X (eJU» I 
x[O]- 2n P 
-Jr: arg{X(e1 )}cot -2-
de. 
13.19. Consider a sequence x[n] with Fourier transform X(eju» and complex cepstrum x[n]. A 
new signal yen] is obtained by homomorphic filtering where 
Hn] = (x[n] -x[-n])u[n 
1]. 
(a) Show that y[n] is a minimum-phase sequence. 
(b) What is the phase of Y(e1W )? 
(c) Obtain a relationship between arg[Y(eju»] and log IY (ejc,,) I. 
(d) If x[n] is minimum phase, how is y[n] related to x[lIj? 

Chapter 13 
Problems  
1039 
13.20.  Equation (13.65) represents a recursive relationship between a sequence x[n] and its com­
plex cepstrum x[n]. Show from Eq. (13.65) that the characteristic system D*[·] behaves as 
a causal system for minimum-phase inputs; i.e., show that for minimum-phase inputs, x[n] 
is dependent only on x[k] for k ::5. n. 
13.21.  Describe a procedure for computing a causal sequence x[n], for which 
X(z) = 
13.22.  The sequence 
h[n] = 8[n] + a8[n - no] 
is a simplified model for the impulse response of a system that introduces an echo. 
(a)  Determine the complex cepstrum h[n] for this sequence. Sketch the result. 
(b)  Determine and sketch the cepstrum ch[n). 
(c)  Suppose that an approximation to the complex cepstrum is computed using N -point 
DFTs as in Eqs. (13.46a) to (13.46c). Obtain a closed-form expression for the approx­
imation hp[n], 0::5. n ::5. N 
1, for the case no = N /6. Assume that phase unwrapping 
can be aceurately done. What happens if N is not divisible by no? 
(d)  Repeat part (c) for the cepstrum approximation cxpln]. 0 ::5. n ::5. N - 1, as computed 
using Eqs. (13.60a) and (13.60b). 
(e)  If the largest impulse in the cepstrum approximation cxp[n] is to be used to detect the 
value of the echo delay no, how large must N be to avoid ambiguity? Assume that 
aceurate phase unwrapping can be achieved with this value of N. 
13.23.  Let x[n] be a finite-length minimum-phase sequence with complex cepstrum x[n], and 
define y[n] as 
with complex cepstrum y[n). 
(a)  If 0 < a < 1, how is Y[nJ related to x[n]? 
(b)  How should a be chosen so that y[n] is no longer minimum phase? 
(c)  How should a be chosen so that if linear-phase terms are removed before computing 
the complex cepstrum, then yin] = 0 for n > O? 
13.24.  Consider a minimum-phase sequence x[nJ with z-transform X(z) and complex cepstrum 
x[nJ. A new complex cepstrum is defined by the relation 
ji[nJ = (an 
l)i[nJ. 
Determine the z-transform Y (z). Is the result also minimum phase? 
13.25.  Section 13.9.4 contains an example of how the complex cepstrum can be used to obtain 
two different decompositions involving convolution of a minimum-phase sequence with 
another sequence. In that example, 
(0.98 + z-l)(1 + 0.9z-15 + 0.8Ie30) 
X(z) = ~ejn/6Cl)(1 _ 0,ge-jn /6c 1)' 
(a)  In one decomposition, X (z) = Xmin(Z)Xap(z) where 
(1 +0.98z-1)(1 +0.ge15 +0.81e30) 
Xmin(Z) = 
(1 
)(1­

1040  
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
and 
(0.98+z-1) 
Xap(z) 
(1 + 0.98c1)· 
Use the power series expansion of the logarithmic terms to find the complex cepstra 
Xmin En], xap[n], and x[n]. Plot these sequences and compare your plots with those in 
Figure 13.19. 
(b) In the second decomposition, X(z) 
Xmn(Z)Xmx(z) where 
z-l(1 + 0.9z-15 + 0.81C30)
X mn (z) = ----:--::7"--;------:--;-r--o­
(1 
() 
and 
Xmx(Z) = (0.98z + 1). 
Use the power series expansion of the logarithmic terms to find the complex cepstra 
and show that xmn[n] =1= Xmin[n] but that x[n] = xmn[n] + xmx[n] is the same as in 
part (a). Note that 
(1 - (0.9)3z-45)
(1 + 0.9z- 15 + 0.8Iz-30) 
(1 
0.9z- 15)· 
13.26. Suppose that sEn] = hEn] * g[n] * p[n], where hEn] is a minimum-phase sequence, g[nl is a 
maximum-phase sequence, and p[nl is 
4 
p[n]  I>:lko[n - knO], 
k=O 
where fXk and no are not known. Develop a method to separate h[n] from sEn]. 
Extension Problems 
13.27.  Let x[n] be a sequence with z-transform X (z) and complex cepstrum x[n]. The magnitude­
squared function for X (z) is 
V(z) 
X(z)x* (l/z"'). 
Since V(e jW ) = IX(ejW)12 2: 0, the complex cepstrum D[nl corresponding to V(z) can be 
computed without phase unwrapping. 
(a)  Obtain a relationship between the complex cepstrum D[n] and the complex cepstrum 
x[n]. 
(b) Express the complex cepstrum D[nl in terms of the cepstrum cx[n]. 
(c)  Determine the sequence C[n] such that 
Xmin[n] = i[n]D[n] 
is the complex cepstrum of a minimum-phase sequence Xmin[n] for which 
jW)12 
jW
IXmin(e 
= V(e
). 
(d) Suppose that X(z) is as given by Eq. (13.32). Use the result of part (c) and Eqs. (13.36a), 
(13.36b), and (13.36c) to find the complex cepstrum of the minimum-phase sequence, 
and work backward to find Xmin(Z). 
The technique employed in part (d) may be used in general to obtain a minimum-phase 
factorization of a magnitude-squared function. 
13.28. Let x[n] be the complex cepstrum of x[n]. Define a sequence xe[n] to be 
[ ]_ {X[ll/NJ, n 
0,±N.±2N..... 
Xe n -
0, 
otherwise. 
Show that the complex cepstrum of xe[ll] is given by 
A 
[ 
)_ {xln/Nl, n=0.±N,±2N,...• 
Xe n -
0, 
otherwise. 

1041 
Chapter 13 
Problems 
13.29. In speech analysis, synthesis, and coding, the speech signal is commonly modeled over a 
short time interval as the response of an LTI system excited by an excitation that switches 
between a train of equally spaced pulses for voiced sounds and a wideband random noise 
source for unvoiced sounds. To use homomorphic deconvolution to separate the compo­
nents of the speech model, the speech signal s[n] 
v[n] * p[n] is multiplied by a window 
sequence w[nl to obtain x[nl = s[n]w[n]. To simplify the analysis, x[n] is approximated by 
x[n] = (v[n] * p[n]) . w[n] :::::: v[n] * (p[n] . w[n]) = v[n] * pw[n] 
where pw[n] = p[n]w[n] as in Eq. (13.123). 
(a)  Give an example of p[n], v[n], and w[n] for which the above assumption may be a 
poor approximation. 
(b)  One approach to estimating the excitation parameters (voiced/unvoiced decision and 
pulse spacing for voiced speech) is to compute the real cepstrum cx[n] of the win­
dowed segment of speech x[n] as depicted in Figure P13.29-1. For the model of Sec­
tion 13.10.1, express cx[n] in terms of the complex cepstrumi[nJ. How would you use 
cx[n] to estimate the excitation parameters? 
1
4 
F H 1·1 H log H F- ~ 
Figure P13.29-1 
(c) Suppose that we replace the log operation in Figure P13.29-1 with the "squaring" 
operation so that the resulting system is as depicted in Figure P13.29-2. Can the new 
"cepstrum" qx[n] be used to estimate the excitation parameters? Explain. 
Figure P13.29-2 
13.30. Consider a stable LTI system with impulse response h[nJ and all-pole system function 
G 
H(z) = --N--­
1- Lag-k 
k=1 
Such all-pole systems arise in linear-predictive analysis. It is of interest to compute the 
complex cepstrum directly from the coefficients of H(z). 
(a)  Determine h[O]. 
(b) Show that 
n-1 ( 
)
h[nJ = an + L 
~ h[kJan-ko 
n::::l. 
k=1 
With the relations in parts (a) and (b), the complex cepstrum can be computed without 
phase unwrapping and without solving for the roots of the denominator of H(z). 
13.31. A somewhat more general model for echo than the system in Problem 13.22 is the system 
depicted in Figure P13.31. The impulse response of this system is 
h[n] = 8[nJ + ag[n - nol, 
where ag[nJ is the impulse response of the echo path. 

1042 
Chapter 13 
Cepstrum Analysis and Homomorphic Deconvolution 
s[n] 
: x[n] 
I  
I  
I 
I 
I 
I 
I 
____________ _J 
Figllre P13.31 
(a) Assuming that 
max  laG(ejW)1 < 1. 
-j'f<{t)<Jr 
show that the complex cepstrum h[nJ has the form 
00 
k 
h[nJ =  L(-ll+1~gk[n knol, 
k=l 
k· 
and determine an expression for gdn] in terms of g[n]. 
(b)  For the conditions of part (a), determine and sketch the complex cepstrum h[nJ when 
g[n] = o[n]. 
(c)  For the conditions of part (a), determine and sketch the complex cepstrum h[n] when 
gIn] 
a"u[n]. What condition must be satisfied by a and a so that the result of part 
(a) applies? 
(d)  For the conditions of part (a), determine and sketch the complex cepstrum h[n] when 
g[n] = aOo[n] + al 0[11 
Ill]' What condition must be satisfied by a, ao, aJ, and III so 
that the result of part (a) applies? 
13.32.  An interesting use of exponential weighting is in computing the complex cepstrum without 
phase unwrapping. Assume that X (z) has no poles and zeros on the unit circle. Then 
it is possible to find an exponential weighting factor a in the product wIn] 
a"x[n], 
such that none of the poles or zeros of X (z) are shifted across the unit circle in forming 
W(z) 
X(a-1z). 
(a) Assuming that no poles or zeros of X (z) move across the unit circle, show that 
W[II]  
a"i[II]. 
(P13.32-1) 
(b)  Now suppose that instead of the complexcepstrum, we compute cx[n] and cw[n]. Use 
the result of part (a) to obtain expressions for both cx[n] and Cw[lI] in terms of i[n). 
(c) Now show that 
2(cx [n] 
ancwln])
i[1I ] 
211 
n #= O. 
(P13.32-2)
1 - a 
(d)  Since cx[n] and cw[n] can be computed from log IX(ej(V)1 and log IW(ejW)I, respec­
tively. Eq. (P13.32-2) is the basis for computing the complex cepstrum without com­
puting the phase of X (e jW ). Discuss some potential problems that might arise with 
this approach. 
.l.  

A 
Random Signals 
In this appendix, we collect and summarize a number of results and establish the notation 
relating to the representation of random signals. We make no attempt here to provide a 
detailed discussion of the difficult and subtle mathematical issues of the underlying the­
ory. Although our approach is not rigorous, we have summarized the important results 
and the mathematical assumptions implicit in their derivation. Detailed presentation 
of the theory of random signals are found in texts such as Davenport (1970), Papoulis 
(1984), Gray and Davidson (2004), Kay (2006), and Bertsekas and Tsitsiklis (2008). 
A.1 DISCRETE-TIME RANDOM PROCESSES 
The fundamental concept in the mathematical representation of random signals is that 
of a random process. In our discussion of random processes as models for discrete-time 
signals, we assume that the reader is familiar with the basic concepts of probability, such 
as random variables, probability distributions, and averages. 
In using the random-process model in practical signal-processing applications, we 
consider a particular sequence to be one of an ensemble of sample sequences. Given 
a discrete-time signal, the structure, i.e., the underlying probability law, of the corre­
sponding random process is generally not known and must somehow be inferred. It 
may be possible to make reasonable assumptions about the structure of the process, or 
it may be possible to estimate the properties of a random-process representation from 
a finite segment of a typical sample sequence. 
Formally, a random process is an indexed family of random variables {xu} charac­
terized by a set of probability distribution functions that, in general, may be a function 
of the index n. In using the concept of a random process as a model for discrete-time 
1043 

1044 
App.A 
Random Signals 
signals, the index n is associated with the time index. In other words, each sample value 
x [n] of a random signal is assumed to have resulted from a mechanism that is governed 
by a probability law. An individual random variable Xn is described by the probability 
distribution function 
PXn (xn, n) 
Probability [xn :::; xn], 
(A1) 
where Xn denotes the random variable and Xn is a particular value of xn. 1 If Xn takes 
on a continuous range of values, it is equivalently specified by the probability density 
function 
Pxn(xn,n) = oPXn(xn,n) 
(A2)
"'xn 
or the probability distribution function 
rn 
PXn (xn, n) = )-00 Pxn(x, n)dx. 
(A3) 
The interdependence of two random variables Xn and Xm of a random process is 
described by the joint probability distribution function 
PXn ,X (xn, n, xm, m) = Probability [xn :::; Xn and Xm :::; xm] 
(AA)
m 
and by the joint probability density 
02 P, 
(
Pxn,xm(Xn,n,xm,m) 
Xn,Xm xn,n,xm,m) 
(AS)
OXnOXm 
Two random variables are statistically independent if knowledge of the value of 
one does not affect the probability density of the other. If all the random variables of a 
collection ofrandom variables, {xn }, are statistically independent, then 
PXlI,Xm (xn, n, xm, m) 
PXn (xn, n) . PXm (xm, m) 
m fn. 
(A6) 
A complete characterization of a random process requires the specification of all 
possible joint probability distributions. As we have indicated, these probability distribu­
tions may be a function of the time indices m and n. In the case where all the probability 
distributions are independent of a shift of time origin, the random process is said to be 
stationary. For example, the 2nd-order distribution of a stationary process satisfies 
Pxn+k,Xm+k(Xn+b n + k. Xm+b m + k) = PXnXm (xn' n, xm, m) 
for all k. 
(A7) 
In many of the applications of discrete-time signal processing, random processes 
serve as models for signals in the sense that a particular signal can be considered a sample 
sequence of a random process. Although the details of such signals are unpredictable­
making a deterministic approach to signal representation inappropriate-certain av­
erage properties of the ensemble can be determined, given the probability law of the 
process. These average properties often serve as a useful, although incomplete, charac­
terization of such signals. 
1In this appendix, boldface type is used to denote the random variables and regular type denotes 
dummy variables of probability functions. 

1045 
Section A.2 
Averages 
A.2 AVERAGES 
It is often useful to characterize a random variable by averages such as the mean and 
variance. Since a random process is an indexed set of random variables, we may likewise 
characterize the process by statistical averages of the random variables making up the 
random process. Such averages are called ensemble averages. We begin the discussion 
of averages with some definitions. 
A.2.1 Definitions 
The average, or mean, of a random process is defined as 
m"n = E{xnl = i: xpxn(x, n)dx,  
(A8) 
where E denotes an operator called mathematical expectation. In general, the mean 
(expected value) may depend on n. In addition, if gO is a single-valued function, then 
g(xn) is a random variable, and the set ofrandom variables {g (xn)Idefines a new random 
process. To compute averages of this new process, we can derive probability distributions 
of the new random variables. Alternatvely, it can be shown that 
E{g(xn)l 
i:.g(X)Pxn(X, n)dx.  
(A9) 
If the random variables are discrete-i.e., if they have quantized values-the integrals 
become summations over all possible values of the random variable. In that case E(g (x) } 
has the form 
(A.I0)  
x 
In cases where we are interested in the relationship between multiple random 
processe~ we must be concerned with multiple sets of random variables. For example, 
for two sets of random variables, {xn} and {ym}, the expected value of a function of the 
two random variables is defined as 
E{g(xn, Ym)} = i: i: g(x, Y)Pxn,Ym (x, n, y, m)dx dy, 
(All) 
where P"n'Ym(xm , n, Ym, m) is the joint probability density of the random variables Xn 
andym • 
The mathematical expectation operator is a linear operator; that is, it can be shown 
that 
1.  E{xn + Ym} = E{xn} + E{ym}; i.e., the average of a sum is the sum of the averages. 
2.  E{axnl = aE{xn}; i.e., the average of a constant times Xn is equal to the constant 
times the average of Xn . 
In general, the average of a product of two random variables is not equal to the 
product of the averages. When this property holds, however, the two random variables 
are said to be linearly independent or uncorrelated. That is, Xn and Y
are linearly inde­
m 
pendent or uncorrelated if 
(Al2) 

1046  
App. A 
Random Signals 
It is easy to see from Eqs. (All) and (A12) that a sufficient condition for linear inde­
pendence is 
P"",Y (xn , n, Ym, m) 
P"" (xn , n) . PY (Ym, m). 
(A.13)
m  
m 
However, Eq. (A13) is a stronger statement of independence than Eq. (A12). As 
previously stated, random variables satisfying Eq. (A13) are said to be statistically 
independent. IfEq. (A13) holds for all values ofn andm, the random processes {xn} and 
{Ym} are said to be statistically independent. Statistically independent random processes 
are also linearly independent; but the converse is not true: Linear independence does 
not imply statistical independence. 
It can be seen from Eqs. (A9)-(All) that averages generally are functions of the 
time index. For stationary processes, the mean is the same for all the random variables 
that constitute the process; i.e., the mean ofa stationary process is a constant, which we 
denote simply mx • 
In addition to the mean of a random process, as.defined in Eq. (AS), a number 
of other averages are particularly important within the context of signal processing. 
These are defined next. For notational convenience, we assume that the probability 
distributions are continuous. Corresponding definitions for discrete random processes 
can be obtained by applying Eq. (AlO). 
The mean-square value of Xn is the average of IXnI2; i.e., 
£{lxnI 2} = mean square = 
2P"n (x, n)dx. 
(A14)
i: Ix 1 
The mean-square value is sometimes referred to as the average power. 
The variance ofxn is the mean-square value of [xn - mXn ]; i.e., 
var[xn ] = £{\(xn - mx,,)12} = a;".  
(A15) 
!  
Since the average of a sum is the sum of the averages, it follows that Eq. (A15) can be 
written as 
var[xn] = £{lxnI2} -lmxn I2.  
(A16) 
In general, the mean-square value and the variance are functions oftime; however, they 
are constant for stationary processes. 
The mean, meansquare,and variance are simple averages that provide only a small 
amount of information about a process. A more useful average is the autocorrelation 
sequence, which is defined as 
(,bxx[n,m] = £{xnx~} 
(A17) 
= [00 [00 XnX!PXn."",(Xn,n,xm,m)dxndxm,
1-00 Loo 
where * denotes complex conjugation. The autocovariance sequence of a random pro­
cess is defined as 
Yxx[n, m] = £{(Xn - mXn)(Xm - mXm )*}, 
(A18) 
which can be written as 
YxAn, m] 
(,bxx[n, m] -
mXnm~ 
m .  
(A19) 
~ 

1047 
Section A,2 
Averages 
Note that, in general, both the autocorrelation and autocovariance are two-dimensional 
sequences, i.e., functions of two discrete variables. 
1be autocorrelation sequence is a measure of the dependence between values of 
the random processes at different times. In this sense, it partially describes the time 
variation of a random signaL A measure of the dependence between two different 
random signals is obtained from the cross-correlation sequence. If {xn} and (Ym} are 
two random processes, their cross-correlation is 
¢xy[n, m] = £{xnY;;'} 
(A.20) 
= [:[: xY*Pxn.y",(x, n, y,m)dxdy, 
where PXn'Y (x, n, y, m) is the joint probability density ofxnandYm' The cross-covariance 
m 
function is defined as 
(A,21) 
= ¢xy[n, m] - mXnm;m' 
As we have pointed out, the statistical properties of a random process generally 
vary with time. However, a stationary random process is characterized by an equilibrium 
condition in which the statistical properties are invariant to a shift of time origin. This 
means that the 1st-order probability distribution is independent oftime. Similarly, all the 
joint probability functions are also invariant to a shift of time origin; i.e., the 2nd-order 
joint probability distributions depend only on the time difference (m 
n). First-order 
averages such as the mean and variance are independent of time; 2nd-order averages, 
such as the autocorrelation ¢xx [n, m], are dependent on the time difference (m 
n). 
Thus, for a stationary process, we can write 
mx = £{xnL 
(A,22) 
a; = £{I(xn m
x )12}, 
(A.23) 
both independent of n. If we now denote the time difference by m, we have 
¢xx[n + m, n] = ¢xx[m] = £{xn+mx~}. 
(A,24) 
That is, the autocorrelation sequence ofa stationary random process is a one-dimensional 
sequence, a function of the time difference m. 
In many instances, we encounter random processes that are not stationary in the 
strict sense-i.e., their probability distributions are not time invariant-but Eqs. (A,22)­
(A,24) still hold. Such random processes are said to be wide-sense stationary. 
A.2.2 Time Averages 
In a signal-processing context, the notion of an ensemble ofsignals is a convenient math­
ematical concept that allows us to use the theory of probability to represent the signals. 
However, in a practical situation, we always have available at most a finite number 
of finite-length sequences rather than an infinite ensemble of sequences. For example, 
we might wish to infer the probability law or certain averages of the random-process 
representation from measurements on a single member of the ensemble. When the 
probability distributions are independent of time, intuition suggests that the amplitude 

1048 
App.A 
Random Signals 
distribution (histogram) of a long segment of an individual sequence of samples should 
be approximately equal to the single probability density that describes each of the ran­
dom variables of the random-process model. Similarly, the arithmetic average of a large 
number of samples of a single sequence should be very close to the mean of the process. 
To formalize these intuitive notions, we define the time average of a random process as 
1 
L 
(xn ) = lim 2 
E Xn • 
(A.2S)
L-+oo L + 1 n=-L 
Similarly, the time autocorrelation sequence is defined as 
L 
* 
l' 
1 
'" 
* 
(A.26)
(xn+mxn) = un -21 L.., xn+mx". 
L-+oo L + n=-L 
It can be shown that the preceding limits exist if (xn} is a stationary process with fi­
nite mean. As defined in Eqs. (A.2S) and (A.26), these time averages are functions 
of an infinite set of random variables and thus are properly viewed as random vari­
ables themselves. However, under the condition known as ergodicity, the time averages 
in Eqs. (A.2S) and (A.26) are equal to constants in the sense that the time averages 
of almost all possible sample sequences are equal to the same constant. Furthermore, 
they are equal to the corresponding ensemble average.2 That is, for any single sample 
sequence {x[n]} for -00 < n < 00, 
1 
L 
(x[n]) 2imoo 2L + 1 E x[n] = £{xn} 
mx 
(A.27) 
n=-L 
and 
L 
(x[n + mJx*[n]) = lim 2 1 
E x[n + m]x*[n] = £{Xn+mx;} = Q>xx[m]. (A.28)
L-+oo L + 1 n=-L 
The time-average operator (.) has the same properties as the ensemble-average operator 
£ {.}. Thus, we generally do not distinguish between the random variable XI! and its value 
in a sample sequence, x[n]. For example, the expression £{x[n]} should be interpreted 
as £{xn} = (x[n]). In general, for ergodic processes, time averages equal ensemble 
averages. 
In practice, it is common to assume that a given sequence is a sample sequence of 
an ergodic random process so that averages can be computed from a single sequence. 
Of course, we generally cannot compute with the limits in Eqs. (A.27) and (A.28), but 
instead the quantities 
1 L-l 
mx = L Ex[n], 
(A.29) 
11=0 
1 L-l 
2
a} = L E Ix[n] - mx l , 
(A.30) 
n=O 
2 A more precise statement is that the random variables (xn) and (xn+mx~) have means equal to mx 
and <pxx[ml, respectively, and their variances are zero. 
.l 

1049 
Section A.3 
Properties of Correlation and Covariance Sequences of Stationary Processes 
and 
I L-1 
(x[n + m]x*[n]}L = L L x[n + m]x*[n] 
(A31) 
n=O 
or similar quantities are often computed as estimates of the mean, variance, and au­
tocorrelation. mx and a} are referred to as the sample mean and sample variance, 
respectively. The estimation of averages of a random process from a finite segment of 
data is a problem of statistics, which we touch on briefly in Chapter 10. 
A.3  PROPERTIES OF CORRELATION AND COVARIANCE 
SEQUENCES OF STATIONARY PROCESSES 
Several useful properties of correlation and covariance functions follow in a straight­
forward way from the definitions. These properties are given in this section. 
Consider two real sta tionary random processes {xn } and {Yn} with autocorrelation, 
autocovariance, cross-correlation, and cross-covariance being given, respectively, by 
¢.u[m] = f{xn+mx~}, 
(A32) 
yxx[m] = f{(xn+m 
mx)(xn 
mx)*}, 
(A33) 
¢xy[m] = f{xn+mY~}, 
(A34) 
yxy[m] = f{(xn+m 
mx)(Yn - my)*}, 
(A.35) 
where mx and my are the means of the two processes. The following properties are easily 
derived by simple manipulations of the definitions: 
Property I 
(A.36a)  
yxy[m] = ¢xy[mj 
(A36b)  
These results follow directly from Eqs. (A19) and (A21), and they indicate that the  
correlation and covariance sequences are identical for zero-mean processes. 
Property 2 
¢xx [0] 
f[lxn 12] = Mean-square value, 
(A37a) 
Yu[O] = 
= Variance. 
(A.37b) 
Property 3 
¢xx[-m] = ¢;Am], 
(A38a) 
Yxx[-m] = y;x[m], 
(A38b) 
¢xy[-m] = ¢;x[m], 
(A38c) 
Yxy[-m] = y;x[m]. 
(A38d) 

1050  
App.A 
Random Signals 
Property 4 
lr/>xy[m]1 2 :::: r/>xx[O]r/>yy[O],  
(A.39a) 
IYry[m]12 < Yu [O]Yyy [0].  
(A.39b) 
In particular, 
lr/>xx[m]1 :::: r/>xx[O],  
(A.40a) 
Iyxx[m)l :::: Yxx[O].  
(A.40b) 
Property 5. IfYn = xn- no ' then 
r/>yy[m] = r/>xx[m],  
(A.41 a) 
yyy[mJ 
yxx[m].  
(A.41 b) 
Property 6. For many random processes, the random variables become 
uncorrelated as they become more separated in time. If this is true, 
lim yxx[m] = 0,  
(A.42a)
m->OO 
lim r/>xx[m] = Imxl2,  
(A.42b)
m->oo 
lim yxy[m] = 0,  
(A.42c) 
m..... OO 
lim r/>xy[m] = mxm~.  
(A.42d)
m-+oo 
" 
The essence of these results is that the correlation and covariance are finite-energy 
sequences that tend to die out for large values ofm. Thus, it is often possible to represent 
these sequences in terms of their Fourier transforms or z-transforms. 
A.4  FOURIER TRANSFORM REPRESENTATION 
OF RANDOM SIGNALS 
Although the Fourier transform of a random signal does not exist except in a generalized 
sense, the autocovariance and autocorrelation sequences of such a signal are aperiodic 
sequences for which the transform does exist. The spectral representation of the cor­
relation functions plays an important role in describing the input-output relations for 
a linear time-invariant system when the input is a random signal. Therefore, it is of 
interest to consider the properties of correlation and covariance sequences and their 
corresponding Fourier and z-transforms. 
We define <pxx(eiw ), rxx(e jW ), <pxy(eiw ), and rxy(eiw) as the DTFTs of r/>xx[m], 
yxx[m], r/>xy[m), and yxy[m), respectively. Since these functions are all DTFfs of se­
quences, they must be periodic with period 2n. From Eqs. (A.36a) and (A.36b), it 
follows that, over one period Iwl :::: n, 
<pxxCe iw) = rxx(eiw) + 2nlmx I28(w), 
Iwl :::: n, 
(A.43a) 
and 
<pxy(eiw) = r xy(eiw) + 2nmxm~8(w), 
,wi::: n. 
(A.43b) 

Section AA 
Fourier Transform Representation of Random Signals 
1051 
In the case of zero-mean processes (m x = 0 and my = 0), the correlation and covariance 
functions are identical so that ¢>xx(ejW) = rxx(e jUJ ) and ¢>Xy(ejW ) = rxy(ejW). 
From the inverse Fourier transform equation, it follows that 
1 iJr 
..
-
r xx (eJW)eJwmdw, 
(A,44a)
27f 
-Jr 
A.. [] 
1 iJr '" (jUJ) jwmd 
(A,44b)
'f'XX m = 2 
.....xx e 
e 
w, 
7f 
-Jr 
and, consequently, 
(A.4Sa) 
a; = Yxx[O] 
-1 i1t rxx(ejw)dw. 
(A,4Sb)
27f 
-Jr 
Sometimes it is notationally convenient to define the quantity 
Pxx(w) = ¢>xx(ejW), 
(A.46) 
in which case Eqs. (A,4Sa) and (A,4Sb) are expressed as 
(A,47a) 
1 iJr
a; = -2 
Pxx(w)dw. 
(A,47b) 
7f 
-Jr 
Thus, the area under Pxx(w) for -7f ::: w ::: Jr is proportional to the average power in 
the signaL In fact, as we discussed in Section 2.10, the integral of Pxx(w) over a band 
of frequencies is proportional to the power in the signal in that band. For this reason, 
the function Pxx (w) is called the power density spectrum, or simply, the power spectrum. 
When Pxx(w) is a constant independent of w, the random process is referred to as a 
white-noise process, or simply, white noise. When Pxx(w) is constant over a band and 
zero otherwise, we refer to it as bandlimited white noise. 
From Eq. (A,38a), it can be shown that Pxx(w) = P;x(w); i.e., Pxx(w) is always 
real valued. Furthermore, for real random processes, ¢xx[m] = ¢xx [-m], so in the real 
case, Pxx(w) is both real and even; i.e., 
(A.48) 
An additional important property is that the power density spectrum is nonnegative; 
i.e., Pxx(w) 2: 0 for all w.This point is discussed in Section 2.10. 
The cross power density spectrum is defined as 
(A,49)  

1052  
App.A 
Random Signals 
This function is generally complex, and from Eq. (A.38c), it follows that 
P,Xy(w) = P;x(w). 
(A. 50) 
Finally, as shown in Section 2.10, ifx[n] is a random signal input to a linear time-invariant 
discrete-time system with frequency response H(e jW ), and if y[n] is the corresponding 
output, then 
<pyy(ejW ) = IH(eiw)12<pxx(ejW) 
(A.51) 
and 
<pxy(ejW ) = H(ejW)<PxxCe j(}». 
(A.52) 
Example A.1 
Noise Power Output of Ideal Lowpass Filter 
Suppose that x[nJ is a zero-mean white-noise sequence with 4>xx[m} = o';8[mJ and 
power spectrum cI>.u(e jW ) 
(Jt for Iwi ~ 11:, and furthermore, assume that x[n] is 
the input to an ideallowpass filter with cutoff frequency We. Then from Eq. (A.51), it 
follows that the output y[n] would be a bandlimited white noise process whose power 
spectrum would be 
4>yy(ejW) = {(JJ, 
Iwi < We, 
(A.53) 
0, 
We < Iwi 
11:. 
Using the inverse Fourier transform, we obtain the autocorrelation sequence 
4>yy[m] 
sin(wcm)  
(A.54) 
Now, using Eq. (A.45a), we get for the average power of the output, 
_ 1 j(J)c 
2
e{i[n]j 
4>yy[O] 
211: 
(Jx dw = (J2 We 
(A.55) 
-wc 
x 11: 
A.5  USE OF THE z-TRANSFORM IN AVERAGE POWER 
COMPUTATIONS 
To carry out average power calculations using Eq. (A.45a), we must evaluate an in­
tegral of the power spectrum as was done in Example A.1. While the integral in that 
example was easy to evaluate, such integrals in general are difficult to evaluate as real 
integrals. However, a result based on the z-transform makes the calculation of average 
output power straightforward in the important case ofsystems that have rational system 
functions. 
In general, the z-transform can be used to represent the covariance function but 
not a correlation function. This is because when a signal has nonzero average value, its 
correlation function will contain an additive constant component that does not have a 
z-transform representation. When the average value is zero, however, the covariance 
and correlation functions are, of course, equaL If the z-transform of Yxx [m] exists, then 
since YxA -m] = Y:Am] it follows that in general  
rxx(z) = r;x(l/z*). 
(A.56)  
1.  

1053 
Section A.5 
Use of the z-Transform in Average Power Computations 
Furthermore, since Yxx [m] is two sided and conjugate-symmetric, it follows that the 
region of convergence of r xx (z) must be of the form 
1 
ra < Izl < ­ra 
where necessarily 0 < ra < 1. In the important case when r xAz) is a rationalfunction of 
z, Eq. (A56) implies that the poles and zeros of r xx (z) must occur in complex-conjugate 
reciprocal pairs. 
The major advantage of the z-transform representation is that when rxx(z) is a 
rational function, the average power of the random signal can be computed easily using 
the relation 
Inverse z-transform I 
£{lx[n] 
mx l2} = a} = Yxx[O] = 
of r.xx(z), 
. 
(A57)
{ evaluated for m = 0 
Itis straightforward to evaluate the right -hand side of this equation using a method based 
on the observation that when rxx (z) is a rational function of z, yxx[m) can be computed 
for all m by employing a partial fraction expansion. Then to obtain the average power, 
we can simply evaluate yxx[m] for m 
O. 
The z-transform is also useful in determining the autocovariance and average 
power of the output of an LTI system when the input is a random signal. Generalizing 
Eq. (A51) leads to 
ryy(z) 
H(z)H*(1/z*)rxx(z), 
(A58) 
and from the properties of the z-transform and Eq. (AS8), it follows that the autoco­
variance of the output is the convolution 
yyy[m] = hem] *h*[-m] * yxx[m]. 
(A.59) 
This result is particularly useful in quantization noise analysis where we need to com­
pute the average. output power when the input to a linear difference equation is a 
zero-mean white noise signal with average power a;. Since the autocovariance of 
such an input is yxx[m] 
a;8[mJ, it follows that the autocovariance of the output is 
yyy[m] 
a;(h[mJ *h*[-m]), i.e., the covariance ofthe output is proportional to the de­
terministic autocorrelation of the impulse response of the LTI system. From this result 
it follows that 
oc 
£{y2[nH 
yyy[O] 
a; L Ih[n]12. 
(A60) 
n=-oo 
As an alternative to computing the sum of squares of the impulse response sequence, 
which can be rather difficult for IIR systems, we can apply the method suggested in 
Eq. (AS7) to obtain £{y2[nH from a partial fraction expansion of ryy(z). Recall that 
for a white noise input with yu[m] = a;8fm],thez-transformisrxx(z) 
o} so ryy(z) = 
a;H(z)H*(l/z*). Therefore, Eq. (A57) applied to the output of the system gives 
Inverse z-transform of I 
£{y2[nn = yyy[O] = 
ryy(z) 
H(z)H*(l/z*)a;, 
. 
(A61)
{ 
evaluated for m 
0 

1054 
App.A 
Random Signals 
Now consider the special case of a stable and causal system having a rational 
system function of the form 
M 
1
n(1 
cmz- ) 
H (z) = A ....;.m=_l=--___ 
N 
Izl > max{ldkl}, 
(A.62)
k
n(1 
dkZ-1) 
k=l 
where maxdldk n< 1 and M < N. Such a system function might describe the relation­
ship between an internal round-off noise source and the output ofa system implemented 
with fixed-point arithmetic. Substituting Eq. (A.62) for H(z) in Eq. (A.S8) gives 
M n(1 - cmz-1)(1- c~z) 
ryy(z) = a}H(z)H*(l/z*) = a}IAI2:.:.:m--,~.:..1_______ 
(A.63) 
n(1 
dkZ-1)(1 - d:z) 
k=l 
Since we have assumed that Idk I < 1 for all k, all of the original poles are inside the 
unit circle and therefore the other poles at (d:)-l are at conjugate reciprocal locations 
outside the unit circle. The region of convergence for r yy (z) is therefore maxk Idk I < 
Izl < mink l(dt)- l l. For such rational functions, it can be shown that since M < N, the 
partial fraction expansion has the form 
2 ({-. ( 
Ak 
A;))
ryy(z) = ax ~ 1- dkZ-1 -
1 _ (d:)-lZ-l' 
(A.64) 
where the coefficients are found from 
Ak = H(z)H*(l/z*)(l 
dkZ-1)1 
(A.65)
Z=dk . 
Since the poles at z = dk are inside the inner boundary of the region of convergence, 
each of them corresponds to a right-sided sequence, while the poles at z = (dk}-l each 
correspond to a left-sided sequence. Thus, the autocovariance function corresponding 
to Eq. (A.64) is 
N 
yyy[n] = a} L(Ak(ddnu[n] + AZ(dk)-nu[-n 
1]), 
k=l 
from which it follows that we can obtain the average power from 
a; 
yyy [0] a; (t Ak) , 
(A.66) 
k=l 
where the quantities Ak are given by Eq. (A.65). 
Thus, the computation of the total average power of the output of a system with 
rational system function and white noise input reduces to the straightforward problem 
of finding partial fraction expansion coefficients for the z-transform of the output auto­
correlation function. The utility of this approach is illustrated by the following example. 

1055 
Section A.5 
Use of the I-Transform in Average Power Computations 
Example A.2 
Noise Power Output of a 2 nd-Order IIR Filter 
Consider a system with impulse response 
rn sin O(n + 1)
h[n] 
. 0 
urn]  
(A.67) 
sm 
and system function 
H (z) =  ---:-;:----;-----;;:---,,-
(A.68)
(1 
When the input is white noise with total average power oJ, the z-transform of the 
autocovariance function of the output is 
r yy(z) 
o} (_····-:-.(J::--l:--l ----:-.(J:---:l::-) (- .. 
'(J 1 
"(J) (A.69)
(l-rel c)(1 
re-) c) 
(1 
re-l z)(l-re) z) 
from which we obtain, using Eq. (A.65), 
£(y2[n]J 
O'J [ Cl 
re~j(Jc1J(-(l--re---}7;;'(J-Z~-(l - rej(JZ») i z =rej8 
(A.70) 
+ Cl 
r:j(J C 1J(-o-"":'re- j6Z~(1 - rej6zJ Iz=re- j9 ] 
• 
Making the indicated substitutions, placing both terms over a common denominator, 
and doing some algebra leads to 
2 (1 +r2) ( 
(A.71)
£(y2[n]) = O'x 1--;2 
1 
Thus, using the partial fraction expansion of r yy(z), we have effectively evaluated the 
expression 
00 
00 1 n . O( 
1) 12 
£(i[n]) = 0'; E Ih[nll2 = a; E r sm 
~ + 
, 
n=-oo 
n=O 
which would be difficult to sum in closed form, and the expression 
1 1]'(  
0'2 1
]'( 
dw 
£{i[n]) = -
O':IH(ejW )12dw 
x 
2n _]'(  
2n -]'( 1(1 
which would be difficult to evaluate as an integral over the real variable w. 
The result of Example A.2 is an illustration of the power of the partial fraction 
method in evaluating average power formulas. In Chapter 6, we make use of this tech­
nique in the analysis of quantization effects in the implementation of digital filters. 

B  
Continuous-Time 
Filters 
The techniques discussed in Chapter 7 for designing IIR digital filters rely on the avail­
ability of appropriate continuous-time filter designs. In this appendix,we briefly sum­
marize the characteristics of several classes of lowpass filter approximations that we 
referred to in Chapter 7. More detailed discussions of these classes of filters appear in 
Guillemin (1957), Weinberg (1975) and Parks and Burrus (1987), and extensive design 
tables and formulas are found in Zverev (1967). Design programs for all the common 
continuous-time approximations and transformations to digital filters are available in 
MATLAB, Simulink, and LabVIEW. 
B.1 BUTTERWORTH LOWPASS FILTERS 
Butterworth lowpass filters are defined by the property that the magnitude response is 
maximally flat in the passband. For an Nth-order lowpass filter, this means that the first 
(2N - 1) derivatives of the magnitude-squared function are zero at Q = O. Another 
property is that the magnitude response is monotonic in the passband and the stopband. 
The magnitude-squared function for a continuous-time Butterworth lowpass filter has 
the form 
H 
.Q 
2 _ 
1 
(B.l)
I cCJ 
)1 - 1 + (jQ/jQc )2N· 
This function is plotted in Figure B.I. 
As the parameter N in Eq. (B.l) increases, the filter characteristics become sharper: 
that is, they remain close to unity over more of the passband and become close to zero 
more rapidly in the stopband, although the magnitude-squared function at the cutoff 
frequency Q c will always be equal to one-half because of the nature of Eq. (B.l). The 
1056 

Section B.l 
Butterworth Lowpass Filters 
1057 
IHAjil)1 2 
1 
2 
Figure B.1 
Magnitude-squared 
function for continuous-time 
il 
Butterworth filter. 
o 
1r--"""""'.... 
Figure B.2 
Dependence of Butterworth 
magnitude characteristics on the 
n 
order N. 
dependence of the Butterworth filter characteristic on the parameter N is indicated in 
Figure B.2, which shows IHc(jQ)1 for several values of N. 
From the magnitude-squared function in Eq. (B.l), we observe by substituting 
jQ = s that He(s)Hc(-s) must be ofthe form 
1 
Hc<s)Hc(-s) = 1 + (sljQ )2N' 
(B.2)
c 
The roots of the denominator polynomial (the poles of the magnitude-squared function) 
are therefore located at values of s satisfying 1 + (slj Qe)2N = 0; i.e., 
Sk = (_1)1/2N (jQ ) = Qce(jrr/2N)(2k+N-l), 
k 
0,1, ... , 2N 
1. 
(B.3)
e
Thus, there are 2N poles equally spaced in angle on a circle of radius Qc in the s-plane. 
The poles are symmetrically located with respect to the imaginary axis. A pole never 
falls on the imaginary axis, and one occurs on the real axis for N odd, but not for N 
even. The angular spacing between the poles on the circle is 1f1N radians. For example, 
for N 
3, the poles are spaced by 1f13 radians, or 60 degrees, as indicated in Figure B.3. 
To determine the system function of the analog filter to associate with the Butterworth 
magnitude-squared function, we perform the factorization Hc(s)Hc(-s). The poles of 
the magnitude-squared function always occur in pairs; i.e., if there is a pole at s = Sk, 
then a pole also occurs at s = -Sk. Consequently, to construct Hc(s) from the magnitude­
squared function, we would choose the one pole from each such pair. To obtain a stable 
and causal filter, we should choose all the poles on the left-half-plane part of the s-plane. 
With this approach, Hc(s) would be 
which can be written as 
Q3
H (s) -
_~...... ---::--"c___--,c 
C 
-
s3 + 2Qcs2 + 2Q s + Q~'
c
In general the numerator of Hc(s) would be Q~ to ensure that IHe(O) 
1. 

1058 
App. B 
Continuous-Time Filters 
Im 
s-plane 
~n__, ~ 
( 
xL 
-\" ) 
~ 
Figure B.3 s-plane pole locations for 
the magnitude-squared function of 
3rd-order Butterworth filter. 
B.2 CHEBYSHEV FILTERS 
In a Butterworth filter, the magnitude response is monotonic in both the passband 
and the stopband. Consequently, if the filter specifications are in terms of maximum 
passband and stopband approximation error, the specifications are exceeded toward the 
low-frequency end of the passband and above the stopband cutoff frequency. A more 
efficient approach, which usually leads to a lower order filter, is to distribute the accuracy 
of the approximation uniformly over the passband or the stopband (or both). This is 
accomplished by choosing an approximation that has an equiripple behavior rather 
than a monotonic behavior. The class of Chebyshev filters has the property that the 
magnitude of the frequency response is either equiripple in the passband and monotonic 
in the stopband (referred to as a type I Chebyshev filter) or monotonic in the passband 
and equiripple in the stopband (a type II Chebyshev filter). The frequency response of 
a type I Chebyshev filter is shown in Figure BA. The magnitude-squared function for 
this filter is of the form 
1 
IHc(JQ)12 = 1 + 
(BA) 
where VN(X) is the Nth-order Chebyshev polynomial defined as 
VN(X) 
cos(N cos-1 x). 
(B.5) 
For example, for N = 0, Vo(x) = 1; for N = 1, Vl (x) = cos(cos-1 x) 
x; for N 
2, 
V2(X) = cos(2cos-1 x) 
2x2 - 1; and so on. 
Hc(jfl) 
Figure B.4 
Type I Chebyshev lowpass
fle 
fl 
filter approximation. 
J...  

1059 
Section B.2 
Chebyshev Filters 
Figure B.5 
Location of poles for the 
magnitude-squared function of ard-order 
type Ilowpass Chebyshev filter. 
From Eq. (B.5), which defines the Chebyshev polynomials, it is straightforward 
to obtain a recurrence formula from which Vl'Hl(X) can be obtained from VN(X) and 
VN-l (x). By applying trigonometric identities to Eq. (B.5), it follows that 
VN+1(X) 
2XVN(X) - VN-l(X). 
(B.6) 
From Eq. (B.5), we note that Vh(x) varies between zero and unity for 0 < x < 1. For 
x > 1, cos-1 x is imaginary, so VN(X) behaves as a hyperbolic cosine and consequently 
increases monotonically. Referring to Eq. (B.4), we see that IHc(jQ)e ripples between 
1 and 1/(1 + 82) for 0:::: QI Q c :::: 1 and decreases monotonically for QI Qc > 1. Three 
parameters are required to specify the filter: e, Qc, and N. In a typical design, 8 is 
specified by the allowable passband ripple and Qc is specified by the desired passband 
cutoff frequency. The order N is then chosen so that the stopband specifications are 
met. 
The poles of the Chebyshev filter lie on an ellipse in the s-plane. As shown in 
Figure B.5, the ellipse is defined by two circles whose diameters are equal to the minor 
and major axes of the ellipse. The length of the minor axis is 2aQc, where 
a = ~(alIN 
a-liN) 
CB.7) 
with 
a= 
+ 
(B.8) 
The length of the major axis is 2bQc, where 
b 
!(alIN + a-liN). 
(B.9) 
To locate the poles of the Chebyshev filter on the ellipse, we first identify the points on 
the major and minor circles equally spaced in angle with a spacing of 1TI N in such a way 
that the points are symmetrically located with respect to the imaginary axis and such 
that a point never falls on the imaginary axis and a point occurs on the real axis for N 
odd but not for N even. This division of the major and minor circles corresponds exactly 
to the manner in which the circle is divided in locating the poles of a Butterworth filter 
as in Eq. (B.3). The poles of a Chebyshev filter fall on the ellipse, with the ordinate 

1060 
App.B 
Continuous-Time Filters 
specified by the points identified on the major circle and the abscissa specified by the 
points identified on the minor circle. In Figure B.5, the poles are shown for N = 3. 
A type II Chebyshev lowpass filter can be related to a type I filter through a 
transformation. Specifically, if in Eq. (BA) we replace the term 
V~ (Q/ Qd by its 
reciprocal and also replace the argument of V~ by its reciprocal, we obtain 
H 
'Q 2 _ 
1 
(B.10)
1 c() )1 - 1 + re2V~(QclQ)]-1' 
This is the analytic form for the type II Chebyshev lowpass filter. One approach to 
designing a type II Chebyshev filter is to first design a type I filter and then apply the 
transformation of Eq. (B.lO). 
B.3 ELLIPTIC FILTERS 
Ifwe distribute the error uniformly across the entire passband or across the entire stop­
band, as in the Chebyshev cases, we are able to meet the design specifications with 
a lower order filter than if we permit a monotonically varying error in the passband 
and stopband, as in the Butterworth case. We note that in the type I Chebyshev ap­
proximation, the stopband error decreases monotonically with frequency, raising the 
possibility of further improvements if we distribute the stopband error uniformly across 
the stopband. This suggests the lowpass filter approximation in Figure B.6. Indeed, it 
can be shown (Papoulis, 1957) that this type of approximation (Le., equiripple error in 
the passband and the stopband) is the best that can be achieved for a given filter order 
N, in the sense that for given values of Q p, 81, and 82, the transition band (Qs 
Qp) is 
as small as possible. 
This class of approximations, referred to as elliptic filters, has the form 
1 
(B.ll)
IHc(JQ)12 
1 + e2U~(Q)' 
where UN(Q) is a Jacobian elliptic function. To obtain equiripple error in both the 
passband and the stopband, elliptic filters must have both poles and zeros. As can be 
seen from Figure B.6, such a filter will have zeros on the jQ-axis of the s-plane. A 
discussion of elliptic filter design, even on a superficial level, is beyond the scope of this 
appendix. The reader is referred to the texts by Guillemin (1957), Storer (1957), Gold 
and Rader (1969) and Parks and Burrus (1987) for more detailed discussions. 
HAjO) 
Figure B.6 Equiripple approximation
o 
Op 
Os 
o 
in both passband and stopband. 
I i 
1 

c  
Answers to Selected 
Basic Problems 
This appendix contains the answers to the first 20 basic problems in Chapter 2 
through 10. 
Answers to Basic Problems in Chapter 2 
2.1. (a) Always (2), (3), (5). If g[n] is bounded, (1). 
(b) (3). 
(c) Always (1), (3), (4). If no = 0, (2) and (5). 
(d) Always (1), (3), (4). If no = 0, (5). If no 2: 0, (2). 
(e) (1), (2), (4), (5). 
(1) Always (1), (2), (4), (5). If b = 0, (3). 
(g) (1), (3). 
(h) (1), (5). 
2.2. (a) N4 = No + N 2, Ns = Nl + N3· 
(b) At most N + M - 1 nonzero points. 
2.3. 
n < 0, 
y[n] = It~na' 
n 2: 0.
I-a' 
2.4. y[n] = 8[(1/2) n - (1/4) n]u[n]. 
2.5. (a) Yh[n] = A l (2)n + A2(3)n . 
1061 

1062 
App. C 
Answers to Selected Basic Problems 
(b)  h[n] = 2(3n - 2n)u[n]. 
(c)  s[n] = [-8(2)(n-1) + 9(3)(n-1) + l]u[n]. 
2.6.  (a) 
. 
1 + 2e- jw + e- j2u)
H(e JW) = ----:;.--­
1­
(b)  y[n] + b[n - 1] + ~y[n - 2] 
x[n] - !x[n 
1] + x[n - 3]. 
2.7.  (a) Periodic, N = 12. 
(b)  Periodic, N = 8. 
(c)  Not periodic. 
(d)  Not periodic. 
2.S.  y[n] = 3(-1/2)nu[n] + 2(1/3)nu[n]. 
2.9.  (a) 
h[n]=2[(~r -(~rJu[n], 
H(e jW) 
s[n]=[-2(~r +(~r +1]U[n]. 
(b)  Yh[n] = A1(1/2)" + A2(1/3)1l. 
(c)  y[n] = 4(1/2)n - 3(1/3)n 
2(1/2)nu[-n -1] + 2(1/3)llu[-n 
1]. Other 
answers are possible. 
2.10. (a) 
y[n] = {a-
1/(1 - a-1). n:::: -1, 
a"/(l- a-I), 
n:::: -2. 
(b) 
I, 
n :::: 3, 
y[n] = { 2 (n-3), 
n < 2. 
(c) 
1, 
n :::: 0,
y[n] = 
2n,
{ 
n :::: -1. 
(d) 
O. 
n 
9, 
y[n]= 
1 
2 (n-9) , 
8:::: n:::: -1,
{ 2 (n+1) _ 2 (n-9) , 
-2:::: n. 
2.11. y[n] 
2,j'i sin(lT(n + 1)/4). 
2.12. (a) y[n] = n!u[nJ. 
(b)  The system is linear. 
(c)  The system is not time invariant. 

1063 
App. G 
Answers to Selected Basic Problems 
2.13. (a), (b), and (e) are eigenfunctions of stable LTI systems. 
2.14. (a) (iv). 
(b) (i). 
(e) (iii), h[n] 
(1j2) nu[n]. 
2.15. (a) Not LTI. Inputs O[n] and 8[n - 1] violate TL 
(b) Not causal. Consider x[n] = o[n 
1]. 
(e) Stable. 
2.16. (a) Yh[n] 
A1(lj2)n + Az(-lj4)n. 
(b)  Causal: hcln] = 2(lj2)nu[n] + (-lj4)nu[n]. 
Anticausal: hac[n] 
-2(lj2)nu[-n -1] - (-lj4)nu[-n 
1]. 
(e) heln] is absolutely summable, hacln] is not. 
(d) Yp[n] = (lj3)(-lj4)nu[n] + (2j3)(lj2)nu[nJ + 4(n + 1)(lj2)(nt-l)u[n + 1]. 
2.17. (a) 
e_jWM/Zsin (w (Mtl)) . 
sin (I) 
(b) W(ejW ) = (lj2)R (ej(V) 
(lj4)R (ej(w-ZJf/M) - (lj4)R (ej(w+ZJf/M). 
2.18. Systems (a) and (b) are causal. 
2.19. Systems (b), (c), (e), and (f) are stable. 
2.20. (a) h[n] = (-lja)n-1 u[n -1]. 
(b) The system will be stable for lal > 1. 
Answers to Basic Problems in Chapter 3 
1
3.1. (a) 
Izl > ~.
-1'
1 
1 
(b) 
-1 ' 
Izl < ~.
1­
(e) 
-1' 
Izl<~.
1­
(d) 1, 
all z.  
(e)z-I, 
z=/=O.  
(f) z, 
Izl < 00. 
1­
(g) ~_----'--'c:---_ 
-1 
Izl =/= o. 
3.2. X (z) 

1064 
App. C 
Answers to Selected Basic Problems 
-1(
Z 
a 
...  ) 
-1
3.3. (a) Xa(z) = 0 
1, d 
1,' 
ROC: lal < Izl < la 
I. 
-az 
1- Z-N 
(b) Xb(Z) = 
< 
" 
ROC:zf;O. 
(1- Z-N)2 
(c) XcCz) 
"'1 , 
ROC: z f; O. 
(1 - z 
3.4. (a) (1/3) < Izi < 2, two sided. 
(b) Two sequences. 0/3) < Iz < 2 and 2 < Izl < 3. 
(c) No. Causal sequence has Izl > 3, which does not include the unit circle. 
3.5. x[n] = 28[n + 1] + 58[n] - 48[n - 1] 
38[n 
2]. 
3.6. (a) x[n] 
( _!)n u[n], Fourier transform exists. 
(b) x[n] = -(-! )nu[-n - 1], Fourier transform does not exist. 
(c) x[n] = 4 ( -!rurn] 
3 ( -!rurn], Fourier transform exists. 
(d) x[n] = (-!r urn], Fourier transform exists. 
(e)  x[n] = -(a-(n+l)u[n]+a-(n-l)u[n 
1], Fouriertransformexistsiflal > 1. 
1 - Z-1 
3.7. (a) H (z) 
, 
Izi > 1.
l+z 
1 
(b) ROC{Y (z)} = Izi > 1. 
(c) y[n] = [-~Of + ~ (_I)n] urn]. 
3.8. (a) h[n] = (-~r urn] -
(-~r-l urn 
1]. 
(b) y[n]. = TI8 ( -43)n urn] 8(l)n
13:3 
urn]. 
(c) The system is stable. 
3.9. (a) Izi  > 0/2). 
(b) Yes. The ROC includes the unit circle. 
1-1
1 - !z
(c) X(z) = 
1 ,ROC: Izi < 2. 
1 
2z­
(d) h[n] 
2(!fu[n]-(-!ru[n]. 
3.10. (a) Izi  > ~. 
(b) 0 < Izi < 00. 
(c) Izi < 2. 
(d) Izi > 1. 
(e) Izi < 00. 
(f) ! < Izi < ,JD. 

1065 
App. C 
Answers to Selected Basic Problems 
3.11. (a) Causal. 
(b) Not causal. 
(c) Causal. 
(d) Not causal. 
3.12. (a) 
Im 
Re 
Figure P3.12 
(b) 
Im 
Re 
Figure P3.12 
(c) 
Im 
-2 
3 
Re 
2: 
Figure P3.12 

1066  
App. C 
Answers to Selected Basic Problems 
11--~ 3 
2
313
• 
• g[ 1 -
111 + 9! 
71' 
3.14.  A 1 
A 2 = 1/2, 
a1 
-1/2, 
a2 
1/2. 
3.15.  h[n] = 0r (u(n] 
urn ~ 10]). The system is causal. 
1 
2z -1 
2 
3.16.  (a) H (z) = 
, 
Izl > -3'
1 ~ 
-1 
n 
(ry)<n-l)
(b) h[n] 
( ~ ) urn] ~ 2 J 
urn ~ I). 
(c) y[n] -
~y[n ~ 1] = x[n] - 2x[n -1]. 
(d) The system is stable and causal. 
3.17.  h[O] can be 0, 1/3, or 1. To be painstakingly literal, h[O] can also be 2/3, due 
to the impulse response h[n] = (2/3)(2)nu[n] - (1/3)(1/2)nu[-n - 1], which 
satisfies the difference equation but has no ROC. This noncausal system with no 
ROC can be implemented as the parallel combination ofits causal and anticausal 
components. 
3.18.  (a) h[n] = -2o(n] + }(_~)nu[n] + ~u[n]. 
18 
(b) y[n] = S2n. 
3.19.  (a) Izl > 1/2. 
(b) 1/3 < Izi < 2. 
(c) Izl > 1/3. 
3.20.  (a) Izi > 2/3. 
(b) Izi > 1/6. 
Answers to Basic Problems in Chapter 4 
4.L x[n] = sin(rrn/2). 
4.2. Qo 
250rr,1750rr. 
4.3. (a) T 
1/12,000. (b) Not unique. T = 5/12,000. 
4.4. (a) T = 1/100. 
(b) Not unique. T 
11/100. 
4.5. (a) T 
1/10,000. 
(b) 625 Hz. 
(c) 1250 Hz. 
4.6. (a) HAiQ) 
l/(a + iQ). 
aT
(b) Hd(ejW ) = T/(l- e-
e- jW ). 
(c) IHd(ejW)1 = T/(l +e-aT ). 
4.7. (a) 
jQrd
x c(jm = Sc(jQ)(l + ae-
). 
X (ejW ) = (~)Sc(i;) (l+ae-iwrdIT) 
for Iwl :s rr. 
(b) H(eiw) = 1 +ae-jwrd/T. 
(c) 0) h[n] = o[n] + ao[n - 1]. 
(ii) h[n] = O[n] + a sin(1r(n-1/2))
1r(n-1/2} 
. 

1067
App. C  
Answers to Selected Basic Problems 
4.8. (a)  T :::: 1/20,000. 
(b)  h[n] = Tu[n]. 
(c)  TX(eJW)lw=o. 
(d)  T:::: 1/10,000. 
4.9. (a)  X (eJ(w+rrJ) = X (e)(w+rr-rr)) = X (eJW) . 
(b)  x[3] = O.  
_ {y[n/2l, 
n even, 
() []
cxn-
dd
O,  
no. 
4.10. (a)  x[n] = cos(2rrn/3). 
(b)  x[n] 
- sin(2rrn/3). 
(c)  x[n] 
sin(2rrn/5)/(rrn/50oo). 
4.11. (a)  T = 1/40, T = 9/40. 
(b)  T = 1/20, unique. 
4.12.  (a) (i) Yc(t) 
-611' sin(6rrt). 
Oi) Yc(t) = -611' sin(6rrt). 
(b) (i)  Yes. 
(ii) No. 
4.13. (a)  y[n] = sin (rr{ - %). 
(b)  Same y[n]. 
(c)  hc(t) has no effect on T. 
4.14. (a)  No. 
(b)  Yes. 
(c)  No. 
(d)  Yes. 
(e)  Yes. (No information is lost; however, the signal cannot be recovered by the 
system in Figure P3.21.) 
4.15. (a)  Yes. 
(b)  No. 
(c)  Yes. 
4.16. (a)  M/L = 5/2, unique. 
(b)  M / L = 2/3; unique. 
4.17. (a)  xd[n] = (4/3) sin (rrn/2) /(rrn). 
(b)  xd[n] 
O. 
4.18. (a)  Wo 
211' /3. 
(b)  Wo = 311'/5. 
(c)  wo=rr. 
4.19. T:::: 11'/ Qo. 
4.20. (a)  Fs ::: 2000 Hz. 
(b)  Fs ::: 4000 Hz. 

1068  
App. G 
Answers to Selected Basic Problems 
Answers to Basic Problems in Chapter 5 
5.1. x[n] = y[n], We = 1'[. 
5.2. (a) Poles: z 
3,1/3, Zeros: z = 0, 00. 
(b) h[n] = -(3/8)(1/3)nu[n] 
(3/8)3 nu[-n 
1]. 
5.3. (a), (d) are the impulse responses, 
1-2el 
5.4. (a) H(z) = 
Izl > 3/4.
1- ~-I 
(b) h[n] 
(3/4)nu[n] 
2(3/4)n-Iu[n -1]. 
(e) y[n] 
(3/4)y[n  
1] = x[n] 
2x[n 
1]. 
(d) Stable and causal. 
5.5. (a) y[n] (7/12)y[n-l]+(1/12)y[n-2] = 3x[n]-(19/6)x[n 1]+(2/3)x[n-2]. 
(b) h[n] = 38[n] - (2/3)(1/3)n-l u[n -1] - (3/4)(1/4)n-l u[n -1]. 
(e)  Stable. 
1 
1 
5.6. (a) X (z) =  
1 
' 
-2 < Izl < 2.
(1- zz-I)(1 
2z- l ) 
(b) ~ < Izl < 2. 
(e) h[n] = o[n] - o[n - 2]. 
1 - z-l  
3 
5.7. (a) H(z) =  
I 
3' Izl> -4' 
(1 
zz-I)(1 + 'leI) 
(b) h[n] = -(2/5)(1/2) nuln] + (7/5)(-3/4)nu[nl 
(e) y[n] + (1/4)y[n -1] 
(3/8)y[n 
2] = x[nJ- x[n - 1], 
Z-l 
5.8. (a) H(z) = 
~, 
Izi > 2. 
1 
-1 - z­
(b) h[n] = -(2/5)(-1/2)nu[n] + (2/5)(2)nu[n]. 
(e) h[n] = -(2/5)(-1/2)"u[n] - (2/5)(2)nu[-n -1]. 
5.9. 
1
[ 4 
1(1)"-1]
h[nJ 
-3(2)"-1+ 3 2 
u[-n], 
Izi < 2' 
4  
1
1 (l)n-l
h[n] = -3(2)"-lu[-n] - 3 2 
urn 
1], 
2 < Izi < 2, 
4  
1 (l)n-l
h[n] = 3 (2)n-l u[n -1] - 3 2 
urn 
1], 
Izi > 2. 
5.10.  Hi(Z) cannot be causal and stable. The zero of a H(z) at z 
00 is a pole of Hi (z). 
The existence of a pole at z 
00 implies that the system is not causal. 
5.U.  (a) Cannot be determined. 
(b) Cannot be determined. 
(e) False. 
(d) True. 
..&.. 

1069 
App. C 
Answers to Selected Basic Problems 
5.12. (a) Stable. 
(b) 
. 
(1 +0.2z-1)(1- ~Z-1) (1 + ~Z-l) 
Hl(Z) --9 
. -
(1 
jO.9z -1)(1 + jO.9z -1) 
, 
(c 1 
n(z-I+~) 
Hap(z) = (1- ~Z-1) (1 + ~z-l)' 
5.13. H 1(Z), H3(Z), and H 4(Z) are allpass systems. 
5.14. (a) 5. 
(b) ~. 
0, A (e jliJ
5.15.  (a) a = 1, {3 = 
) = 1 +4cos(w). The system is a generalized linear­
phase system but not a linear-phase system, because A (e jW) is not nonneg­
ative for all w. 
(b) Not a generalized linear-phase or a linear-phase system. 
(c) a = 1, {3 = 0, A (ejW ) = 3 + 2cos(w). Linear phase, since IH(ejW)1 
A (ejW) 2: 0 for all w.  
= 0, A (eiliJ ) 
(d)  a = 1(2, {3 
= 2 cos(w(2). Generalized linear phase, because 
A (e jW) is not nonnegative at all w. 
(e)  a = 1, {3 = 1'[/2, A (eiw) = 2 sin(w). Generalized linear phase, because 
{3 1= O. 
5.16.  h[n] is not necessarily causal. Both h[n] = o[n - a] and h[n] 
8[n + 1] + 
8[n -
(2a + 1)] will have this phase. 
5.17. H2(Z) and H3(Z) are minimum-phase systems. 
2(1 
~Z-l) 
5.1S. (a) Hmin(Z) = 
1 
1 
. 
. 
1 + :3z­
(b) Hmin(Z) 3(1 
~Z-l). 
9(1- ~Z-l) (1 lz-1) 
(c) Hmin(Z)  
2
4 
(1 
~Z-1) 
5.19. hl[n]: 2,h2[n]: 3/2,h3[n]: 2,h4[n]: 3,hs[n]: 3,h6[n]: 7(2. 
5.20.  Systems HI(Z) and H3(Z) have a linear phase and can be implemented by a 
real-valued difference equation. 
Answers to Basic Problems in Chapter 6 
6.L Network 1: 
1 
H(z) 
1 
2r cosez 

1070 
App.C 
Answers to Selected Basic Problems 
Network 2: 
r sin ilz-1 
H(z)=1-2rcosOz 1+ 
Both systems have the same denominators and thus the same poles. 
6.2. y[n] - 3y[n -1] 
y[n - 2] - y[n - 3] = x[n] - 2x[n -1] + x[n 
2). 
6.3. The system in Part (d) is the same as that in Part (a). 
6.4. (a) 
-2 
(b) 
1 
3 
] 
y[n] + 4 y [n -1] -
sy[n 
2] 
2x[n] + 4x[n -1]. 
6.5. (a) 
y[n] - 4y[n -1] + 7y[n - 3] + 2y[n - 4] 
x[n). 
(b) 
1 
H(z) = 1 
4z-1 + 7z --3 + 2z 4' 
(c) Two multiplications and four additions. 
(d) No. It requires at least four delays to implement a 4th-order system. 
6.6. 
4 
4 
3 
3 
.l 
7 
-1 
0 
6 
7 
-2 
(a) 
(b) 
3 
3 
2 
8 
9 
7 
8 
(c) 
(d) 
Figure P6.6 
......  

1071 
App. C 
Answers to Selected Basic Problems 
6.7. 
1 
x [n]  
1
o  
• I ~ I:::: 
4 
j>--->----Y[n 
1 
4  
Figure P6.7 
6.8.  yen] - 2y[n - 2] = 3x[n - 1] + x[n - 2]. 
6.9.  (a) h[l] = 2. 
(b)  yen] + yen - 1] - 8y[n - 2] = x[n] + 3x[n -1] + x[n - 2] - 8x[n - 3]. 
6.10. (a) 
yen] 
x[n] + v[n - 1]. 
v[n] 
2x[n] + 21 y [n] + wen 
1]. 
wen] 
x[n] + 21 y [n]. 
(b) 
1 
2  
Figure P6.10 
(c)  The poles are at z = -1/2 and z = 1. Since the second pole is on the unit 
circle, the system is not stable. 
6.11. (a) 
In]
x 
Y[n] 
Z~l 
1 
1 
2 
Z-l 
-6 
Z~l 
8 
Figure P6.11 

1072 
App. C 
Answers to Selected Basic Problems 
(b) 
x[n] 
yen] 
Z-l 
1 
z-1 2 
-6 
Z-1 
8 
Figure P6.11 
6.12. y[n] -
8y[n -
1] = -2xlnJ + 6x[n -
1] + 2x[n - 2]. 
6.13. 
([n] 
y[n 
z-1 
1 
4 
Z-1
1 
8 
L--.... 
Figure P6.13 
6.14. 
6.15. 
([n] 
y[n 
1 
fZ-l 5 
2 
6 
Z-1 1
1 
2 
-6 
Figure P6.14 
x[n] 
7 
6 
Z-l 
-1 
1 
6 
Z-1 1 
2 
yen 
Figure P6.15 
........  

1073 
App.C 
Answers to Selected Basic Problems 
6.16. (a) 
y[nJ
x [nJ 
.-1
Z-I 
f 
1  
2  
-2
1 
4 
I 
.( _-I 
! 
3 
I  
Figure P6.16 
(b)  Both systems have the system function  
- 2z -1 + 3c2)  
H (z) = -'---~---'---;----2-----
(1 
6.17. (a) 
Figure P6.17-1 
(b) 
Figure P6.17-2 
6.18.  If a = 2/3, the overall system function is 
H(z) = 
1+ 
If a = -2, the overall system function is 
H(z) = 1+ 
6.19. 
1 + 2z-1 
-~~-:::---
1­
-2 
9 
-8 
I 
y[n]
x[n] 
Z-1 
1 
3 
i 
Z-1 
2 j
3  
Figure P6.19 

----
1 
1074  
App. C 
Answers to Selected Basic Problems 
6.20. 
Z-l 
2 
5 Z-l 
1 
4 
4  
y[n
x[n] 
Z-l 
5 
2 
Z-l 
-1 
Figure P6.20 
Answers to Basic Problems in Chapter 7 
7.1. (a) 
aT
HI (z) = 
1 - e-
cos(bT)z -1 
aT 
1 
2e aT cos(bT)z -1 + r2aTz -2' ROC: Izl > e-
. 
(b) 
aT
H2(Z)=(1- z-1)S2(Z), ROC: Izl > e-
, where 
all 
1 
1 
S2(Z)=  
-. 
. 
a 2 +b 2 1-z-1 
2(a+Jb)1 
r(a+Jb)Tz-l 
2(a 
jb)l­
(c) They are not equal. 
7.2. (a) 
Hc(jn) 
0.89125 la'_ _Ii 
I 
I 
I 
I 
m
017783l -l­
0.211' 0.3 1T 
n 
Td 
Td 
Figure P7.2 
(b) N = 6, ncTd = 0.7032. 
(c)  The poles in the s-plane are on a circle ofradius R = 0.7032/Td. They map 
to poles in the z-plane at z = eSkTd • The factors of Td cancel out, leaving the 
pole locations in the z-plane for H(z) independent of Td. 
..... 

App. C 
Answers to Selected Basic Problems 
1075 
7.3. (a) 32 
82/(1 + 81), 31 
281/(1 + 81). 
(b) 
0.18806,81 = 0.05750 
H(z) 
0.3036 
0.47232-1 
(c) Use the same 81 and 82. 
H(z) 
(1 
1.2686z 
7.4. (a) 
1 
0.5 
Hc(s) = -- - -­
s+O.1 
s+0.2 
The answer is not unique. Another possibility is 
1 
0.5 
Hc(s) = 
-
. 
s+0.I+j2rr 
s+0.2+j2rr 
(b) 
2(1 + s) 
l+s 
Hc(s) = 
-­
0.1813 + 1.8187s 
0.3297 + 1.6703s . 
This answer is unique. 
7.5. (a) M + 1 = 91, fJ = 3.3953. 
(b)  MI2 
45. 
sin [0.625rr(n 
45)]
cd[n = _-"--_____-=c
( ) h 
] 
rr(n - 45) 
7.6. (a) 8  
0.03, fJ 
2.181. 
(b) t;.(J) = 0.05rr, M 
63. 
7.7. 
sin [O.3rr(n 
45)] 
rr(n - 45) 
0.99:::: IH(ejW)1 :::: 1.01, 
Iwl :::: 0.2rr, 
IH(ejW)1 :::: 0.01, 
0.22rr :::: Iwl :::: rr 
7.8. (a) Six alternations. L 
5, so this does not satisfy the alternation theorem and 
is not optimal. 
(b) Seven alternations, which satisfies the alternation theorem for L = 5. 
7.9. eVe = O.4rr. 
7.10. We 
2.3842 rad. 
7.11. Q c = 2 rr(1250) rad/sec. 
7.12. Q c 
2000 rad/sec. 
7.13. T 
50 f1S. This T is unique. 
7.14. T = 1.46 ms. This T is unique. 

1076 
App. C 
Answers to Selected Basic Problems 
7.15. Hamming and Hanning: M + 1 = 81, Blackman: M + 1 = 121. 
7.16. f3 = 2.6524, M = 181. 
7.17. 
IHc(jQ)1 < 0.02, 
IQI s 2:rr(20) rad/sec, 
0.95 < IHc(jQ)1 < 1.05, 
2:rr(30) S IQI s 2:rr(70) rad/sec, 
IHc(jQ)1 < 0.001, 
2:rr(75) rad/sec S IQI. 
7.18. 
IHc(jQ)1 < 0.04, 
IQI S 324.91 rad/sec, 
0.995 < IHc(jQ)1 < 1.005, 
IQI :::: 509.52 rad/sec. 
7.19. T 
0.41667 ms. This T is unique. 
7.20. True. 
Answers to Basic Problems in Chapter 8 
8.1. (a) x[n] is periodic with period N 
6. 
(b) T will not avoid aliasing. 
(e) 
ao + a6 +a-6, 
k=O, 
al+ a7+ a-5, 
k = 1, 
az + as + a_4, 
k =2,
X[k] = 2:rr 
a3 +a9 +a-3 +a-9, k= 3, 
a4 +a-z + a-8, 
k 
4, 
as + a_I + a_7, 
k=5. 
8.2. (a) 
for k = 3£,
X3[k] = {3X[k/3],
0, 
otherwise. 
(b) 
k =0,
X[k]={3,-1, 
k = 1. 
9, 
k =0, 
X3[k] 
0, 
k = 1,2,4,5, 
-3, k 
3.
1 
8.3. (a) x2[n]. 
(b) None of the sequences. 
(e) xI[n] andx3[n]. 

1077 
App. C 
Answers to Selected Basic Problems 
8.4. (a) 
(b) 
_ 
1 
X[k] = 
"(2 /N)k'
1 - cu-J rc 
(c) 
8.5. (a) X [k] = l. 
(b) X [k] 
w;no 
(c) 
X [k] = {  N /2, 
k = 0, ~/2,
0, 
otherwIse. 
(d) 
! 
N/2, 
e- j (rck/N)(N/2-1)(_1)(k-l)/2. 
1
X [k]  
sm(krc/ N)' 
0, 
(e) 
X [k] 
8.6. (a) 
" 
1 
ej(wo-w)N 
X (eJW) -
--c:-:---,­
-
1 - ej(wo-w) • 
(b) 
1 -
ejlJ.!oN 
X [k] = 
" 
k'
1- eJwOWN 
(c) 
X [k] _ { N, 
k 
ko 
-
0,  
otherwise. 
8.7. 
~lll'l • • 
-1 
0 
1 
2 
3 
4 
5 
n 
Figure PB.7 
k=O, 
kodd, 
otherwise. 
8.8. 
1024 (1) n o::: n ::: 9,
y[n] 
1023 
2 
' 
{ 0, 
otherwise. 

1078 
App. C 
Answers to Selected Basic Problems 
8.9.  (a) 1. Let xl[n] = Lm x[n + 5m]for n 
0,1, ... 4. 
2. Let X1[k] be the five-point FFT of xl[n]. M = 5. 
3. Xt£2] is X (eiw ) at (J) 
4rr/5. 
(b)  Define x2[n] = I:m W27(n+9m)x[n + 9mJ for n = 0, ... ,8. 
Compute X2[kJ, the 9-point DFT of x2[n]. 
lw
X2[2J 
X (e
) Iw=lOn/27' 
8.10. X2[k] 
(-I)kXl[k]. 
8.11. 
6 
5 
4 
3 
1T2f 
-1 
0 
1 
2 
3 
4 
5 
6 
7 
n 
Figure P8.11 
8.12. (a) 
2, 
k = 1,3, 
X[k]= { 0, k=0,2. 
(b) 
IS, 
k = 0, 
H [k] = 
-3 + j6,  k 
1, 
k = 2,
{ -3 - j6, k = 3. 
(c) y[n] = -38[n] - 68[n 
1] + 38[n 
2] + 68[n - 3]. 
(d) y[n] = -38[n] - 68[n 
1] + 38[n - 2] + 68[n - 3]. 
8.13. 
2j 
2j 
. II 
• II.
y[n] 
-1  0 
1 
2 
3 
4 
5 
Figure P8.13 
8.14. x3[2] = 9. 
8.15. a = -1. This is unique. 
8.16. b = 3. This is unique. 
8.17. N = 9. 
8.18. c = 2. 
8.19. m 
2. This is not unique. Any m 
2 + 6£ for integer .e works. 
8.20. N = 5. This is unique. 
J 

1079 
App. C  
Answers to Selected Basic Problems 
Answers to Basic Problems in Chapter 9 
9.1.  Ifthe input is (l/N)X [«-n))N], the output of the DFf program will be x[n], 
the IDFf of X [k]. 
9.2. 
X=AD-BD+CA-DA=AC-BD 
Y=AD 
BD+BC+BD 
BC+AD. 
9.3. 
y[32] = X(e-/2rr(7/32») == X(e/2;T(25/32». 
9.4.  Wk = 71r /16. 
9.5. 
a=-h 
b 
_e- j (6rr/8). 
9.6.  (a) The gain is - WJ. 
(b)  There is one path. In general, there is only one path from any input sample 
to any output sample. 
(c)  By tracing paths, we see 
2
X [2] = x[O] . 1 + xll]W8 - x[2] 
x[3]Wl + ... 
x[4] + x[5]Wl- x[6] 
x[7]Wr 
9.7.  (a) Store x[n] in A [.] in bit-reversed order, and D[·] will contain X [k] in se­
quential (normal) order. 
(b) 
8, 
r = 3,
D[r] t0, 
otherwise. 
(c) 
Clr] = {1, 
r = 0, ~,2, 3, 
. 
0, 
otherWIse. 
9.S.  (8) N/2 butterflies with 2(m-l) different coefficients. 
(b)  y[n] = WJ"·my[n 
l]+x[n]. 
(c)  Period: 2m , Frequency: 2n2-m . 
9.9.  Statement 1. 
9.10. 
yIn] 
X (eiW)lw=(2rr/7)+(2rr/21)(n-19). 
9.11. (8) 2m-I. 
(b)  2m. 
9.12. r[n] 
e-/(2rr/19)nw n2/ 2 where W 
e-J(2rr/lO). 

1080  
App. C 
Answers to Selected Basic Problems 
9.13.  x[O], x[8], x[4], x[12], x[2], x[lO], x[6], x[14], x[l], x[9], x[5], x [13], x[3], x[ll], 
x[?], x[15]. 
9.14.  False. 
9.15.  m = l. 
9.16. 
0. 
m 
1, 
0.4, 
m =2, 
r= 1
0,2,4,6, 
m =3, 
0,1,2,3,4,5,6,7, m 
4. 
9.17.  N 
64. 
9.1S.  m 
3 or 4. 
9.19.  Decimation-in-time. 
9.20.  1021 is prime, so the program must implement thdull OFT equations and cannot 
exploit any FFTalgorithm. The computation time goes as N 2. Contrastingly, 1024 
is a power of 2 and can exploit the N log N computation time of the FFT. 
Answers to Basic Problems in Chapter 10 
10.1.  (a) f = 1500 Hz. 
(b) f 
-2000 Hz. 
10.2.  N = 2048 and 10000 Hz < f < 10240 Hz. 
10.3.  (a) T = 2rrko/(NQo). 
(b) Not unique. T = (2rr/Qo)(1-ko/N). 
10.4. 
x c (j2rr(4200» = 5 x 10-4 
Xc(-j2rr(4200» 
5 x 10-4 
X c (j2rr (1000» = 10-4 
X c(- j2rr(looo» 
10-4 
10.5.  L 
1024. 
10.6.  x2[n] will have two distinct peaks. 
10.7.  ~Q 2rr(2.44) rad/sec. 
10.S.  N 2":. 1600. 
10.9. 
18, 
k 
3,33, 
X o[k] = { 0, 
otherwise. 
18, 
k 
9,27, 
Xl[kl = { 0, 
otherwise. 
Xr[k] = 0 for r i= 0,1. 
10.10. (uo 
0.25rr rad/sample, A 
rr/76000 rad/sample2• 
10.11. ~f = 9.77 Hz. 
..l.  

1081 
App. C 
Answers to Selected Basic Problems 
10.12.  The peaks will not have the same height. The peak from the rectangular window 
will be bigger. 
10.13.  (a) A = 21 dB. 
(b) Weak components will be visible if their amplitude exceeds 0.0891. 
10.14.  (a) 320 samples. 
(b) 400 DFf/second. 
(c) N = 256. 
(d) 62.5 Hz. 
10.15.  (a) X [200] = 1 - j. 
(b) 
X (j2rr(4oo0» = 5 x 10-5(1 
j) 
X (-j2rr(4000» = 5 x 10-5(1 + j). 
10.16.  Rectangular, Hanning, Hamming, and Bartlett windows work. 
10.17. T > 1/1024 sec.  
10.1S. x2[n], x3[n], x6[n].  
10.19.  Methods 2 and 5 will improve the resolution. 
10.20.  L = M + 1 = 262. 

BIBLIOGRAPHY  
Adams, J. w., and Wilson, 1. A. N., "A New Approach to FIR Digital Filters with Fewer Multiplies and 
Reduced Sensitivity," IEEE Trans. ofCircuits and Systems, Vol. 30, pp. 277-283, May 1983. 
Ahmed, N., Natarajan, T., and Rao, K. R, "Discrete Cosine Transform," IEEE Trans. on Computers, Vol. C-23, 
pp. 90-93, Jan. 1974. 
Allen, 1., and Rabiner, L., "A Unified Approach to Short-time Fourier Analysis and Synthesis," Proc. IEEE 
Trans. on Computers, Vol. 65, pp. 1558-1564, Nov. 1977. 
. 
Atal, B. S., and Hanauer, S. L., "Speech Analysis and Synthesis by Linear Prediction of the Speech Wave," J. 
Acoustical Society ofAmerica, VoL 50, pp. 637-655, 1971. 
Atal, B. S., "Automatic Recognition of Speakers from their Voices," IEEE Proceedings, Vol. 64, No.4, pp. 460­
475, Apr. 1976. 
Andrews, H. C, and Hunt, B. R, Digital Image Restoration, Prentice Hall, Englewood Cliffs, NJ, 1977. 
Bagchi, S., and Mitra, S., The Nonuniform Discrete Fourier Trans/ormand Its Applications in Signal Processing, 
Springer, New York, NY, 1999. 
Baran, T. A., and Oppenheim. A. V., "Design and Implementation of Discrete-time Filters for Efficient 
Rate-conversion Systems," Proceedings of the 41st Annual Asilomar Conference on Signals, Systems. 
and Computers, Asilomar, CA, Nov. 4-7, 2007. 
Baraniuk, R, "Compressive Sensing," IEEE Signal Processing Magazine, Vol. 24, No.4, pp. 118-121, July 
2007. 
Barnes, C W., and Fam, A. T.. "Minimum Norm Recursive Digital Filters that are Free of Over-flow Limit 
Cycles," IEEE Trans. Circuits and Systems, Vol. CAS-24. pp. 569-574, Oct. 1977. 
Bartels R H., Beatty, 1. C, and Barsky, B. A., An Introduction to Splines for Use in Computer Graphics and 
Geometric Modelling, Morgan Kauffman, San Francisco, CA, 1998. 
Bartle, R. 0., The Elements ofReal Analysis, 3rd ed, John Wiley and Sons, New York, NY, 2000. 
Bartlett, M. S., An Introduction to Stochastic Processes with Special Reference to Methods and Applications, 
Cambridge University Press, Cambridge, UK, 1953. 
Bauman, p', LipshilZ, S., and Vanderkooy. J., "Cepstral Analysis of Electroacoustic Transducers," Proc. Int. 
Conf Acoustics, Speech, and Signal Processing (ICASSP '85), Vol. 10, pp. 1832-1835, Apr. 1985. 
Bellanger, M., Digital Processing ofSignals, 3rd ed., Wiley, New York, NY, 2000. 
Bennett, W R., "Spectra of Quantized Signals," Bell System Technical J., Vol. 27, pp. 446-472,1948. 
Bertsekas, D. and Tsitsiklis, 1., Introduction to Probability, 2nd ed., Athena Scientific, Belmont, MA, 2008. 
Blackman, R. B., and Thkey, 1. W., The Measurement of Power Spectra, Dover Publications, New York. NY, 
1958. 
Blackman, R., Linear Data-Smoothing and Prediction in Theory and Practice, Addison-Wesley, Reading, MA, 
1965. 
Blahut, R. E., Fast Algorithms for Digital Signal Processing, Addison-Wesley, Reading, MA, 1985. 
Bluestein, L. I., "A Linear Filtering Approach to the Computation of Discrete Fourier Transform," IEEE 
Trans. Audio Electroacoustics, Vol. AU-18, pp. 451-455, 1970. 
Bogert, B. P., Healy, M. J. R, and Thkey, J. W, "The Quefrency Alanysis of Times Series for Echos: Cepstrum, 
Pseudo-autocovariance, Cross-cepstrum, and Saphe Cracking," Chapter 15, Proc, Symposium on Time 
Series Analysis, M. Rosenblatt, ed., John Wiley and Sons, New York, NY, 1963. 
Bosi, M., and Goldberg, R E.,Introduction to Digital Audio Coding and Standards, Springer Science+Business 
Media, New York, NY, 2003. 
Bovic, A., ed., Handbook ofImage and Video Processing, 2nd ed., Academic Press, Burlington, MA, 2005. 
I
1082 
~ 

..  
Bibliography 
1083 
Bracewell, R N., "The Discrctc Hartley Transform," .r. Optical Society of America, Vol. 73, pp. 1832-1835, 
1983. 
Bracewell, R N., "Thc Fast Hartlcy Transform," IEEE Proceedings, Vol. 72, No.8, pp. 1010--1018, 1984. 
Bracewell, R. N., Two-Dimensional Imaging, Prentice Hall, New York, NY, 1994. 
Bracewell, R N., The Fourier Transform and Its Applications, 3rd ed., McGraw-Hili, New York, NY, 1999. 
Brigham, E .. Fast Fourier Transform and Its Applications, Prentice Hall, Upper Saddle River, NJ, 1988. 
Brigham, E. 0., and Morrow,R. E., "The Fast Fourier Transform," IEEE Spectrum, Vo1. 4, pp. 63-70, Dec. 1967 . 
Brown, J. W., and Churchill, R v., Introduction to Complex Variables and Applications. 8th ed.. McGraw-Hill, 
New York, NY, 2008. 
Brov.TI, R c., Introduction to Random Signal Analysis and Kalman Filtering, Wiley, New York, NY, 1983. 
Burden, R L., and Faires, 1. D., Numerical Analysis, 8th ed., Brooks Cole, 2004. 
Burg, J. P., "A New Analysis Technique for Time Series Data," Proc. NATO Advanced Study Institute on 
Signal Processing, Enschede, Netherlands, 1968. 
Burrus, C. S., "Efficient Fourier Transform and Convolution Algorithms," in Advanced Topics in Signal 
Processing, J. S. Lim and A. V. Oppenheim, eds., Prentice Hall, Englewood Cliffs, NJ, 1988. 
Burrus, C. S., and Parks, T. w., DFTIFFT and Convolution Algorithms Theory and Implementation, Wiley, 
New York, NY, 1985. 
Burrus, C. S., Gopinath, R. A., and Guo, H., Introduction to Wavelets and Wavelet Transforms: A Primer, 
Prentice Hall, 1997. 
Candy, 1. c., and Temes, G. c., Oversampling Delta-Sigma Data Converters: Theory, Design, and Simulation, 
IEEE Press, New York. NY, 1992. 
Candes, E., "Compressive Sampling," Int. Congress ofMathematics, 2006, pp. 1433-1452. 
Candes, E., and Wakin, M., "An Introduction to Compressive Sampling," IEEE Signal Processing Magazine, 
Vol. 25, No.2, pp. 21-30, Mar. 2008. 
Capon, J., "Maximum-likelihood Spectral Estimation," in Nonlinear Methods of Spectral Analysis, 2nd ed., 
S. Haykin, ed., Springer-Verlag, New York, NY, 1983. 
Carslaw, H. S., Introduction to the Theory of Fourier's Series and Integrals, 3rd ed., Dover Publications, New 
York, NY, 1952. 
Castleman, K. R, Digital Image Processing, 2nd ed., Prentice Hall, Upper Saddle River, NJ, 1996. 
Chan. D. S. K., and Rabiner, L. R, "An Algorithm for Minimizing Roundoff Noise in Cascade Realizations of 
Finite Impulse Response Digital Filters," Bell System Technical 1., Vol. 52, No.3, pp. 347-385, Mar. 1973. 
Chan, D. S. K., and Rabiner, L. R, "Analysis of Quantization Errors in the Direct Form for Finite Impulse 
Response Digital Filters," IEEE Trans. Audio Electroacoustics, Vol. 21, pp. 354-366, Aug. 1973. 
Chellappa, R,.Girod, B., Munson, D. c., Tekalp, A M., and Vetterli, M., "The Past, Present, and Future 
of Image and Multidimensional Signal Processing," IEEE Signal Processing Magazine, Vol. 15, No.2, 
pp. 21-58, Mar. 1998. 
Chen, W. H., Smith, C. H., and Fralick, S. c., "A Fast Computational Algorithm for the Discrete Cosine 
Transform," IEEE Trans. Commun., Vol. 25, pp. 1004-1009, September 1977. 
Chen, X., and Parks, T. W., "Design of FIR Filters in the Complex Domain," IEEE Trans. Acoustics, Speech, 
and Signal Processing, VoL 35, pp. 144-153, 1987. 
Cheney, E. W., Introduction to Approximation Theory, 2nd ed., Amer. Math. Society. New York, NY, 2000. 
Chow, Y, and Cassignol, E., Linear Signal Flow Graphs and Applications, Wiley. New York, NY, 1962. 
Cioffi, J. M" and Kailath, T., "Fast Recursive Least-squares Transversal Filters for Adaptive Filtering," IEEE 
Trans. Acoustics, Speech, and Signal Processing, Vol. 32. pp. 607-624, June 1984. 
Claasen, T. A., and Mecklenbrauker, W. E, "On the Transposition of Linear Time-varying Discrete-time 
Networks and its Application to Multirate Digital Systems," Philips 1. Res .. Vol. 23, pp. 78-102,1978. 
Claasen, T. A C. M., Mecklenbrauker, W. F. G., and Peek, J. B. H., "Second-order Digital Filter with only 
One Magnitude-truncation Quantizer and Having Practically no Limit Cycles," Electronics Letters, 
Vol. 9, No.2, pp. 531-532, Nov. 1973. 
Clements, M. A, and Pease, J., "On Causal Linear Phase IIR Digital Filters," IEEE Trans. Acoustics, Speech, 
and Signal Processing, Vol. 3, pp. 479-484. Apr. 1989. 
Committee, DSp, ed., Programs for Digital Signal Processing, IEEE Press, New York, NY, 1979. 
Constantinides. A G., "Spectral Transformations for Digital Filters," IEEE Proceedings, VoL 117, No.8, 
pp. 1585-1590, Aug. 1970. 

1084 
Bibliography 
Cooley, J. w., Lewis, P. A w., and Welch, P. D., "Historical Notes on thc Fast Fourier Transform," IEEE 
Trans. Audio Electroacoustics, Vol. 15, pp. 76-79, June 1967. 
Cooley, J. W., and Thkey, J. W., "An Algorithm for the Machine Computation of Complex Fourier Series," 
Mathematics ofComputation, Vol. 19, pp. 297-301, Apr. 1965. 
Crochiere, R. E., and Oppenheim, A. V, "Analysis of Linear Digital Networks," IEEE Proceedings, Vol. 63, 
pp.581-595,Apr.1975. 
Crochiere, R E., and Rabiner, L. R, Multirate Digital Signal Processing, Prentice Hall, Englewood Cliffs, 
NJ,1983. 
Daniels, R w., Approximation Methods for Electronic Filter Design, McGraw-Hill, New York, NY, 1974. 
Danielson, G. C, and Lanczos, C, "Some Improvements in Practical Fourier Analysis and their Application to 
X-ray Scattering from Liquids," 1 Franklin Inst., Vol. 233, pp. 365-380 and 435-452. Apr. and May 1942. 
Davenport, W. B., Probability and Random Processes: An Introduction for Applied Scientists and Engineers, 
McGraw-Hill, New York, NY, 1970. 
Davis, S. B., and Mermelstein, P., "Comparison of Parametric Representations for Monosyllabic Word 
Recognition," IEEE Trans. Acoustic;; Speech and Signal Processing, Vol. ASSP-28, No.4, pp. 357-366, 
Aug. 1980. 
Deller, 1. R, Hansen, J. H. L., and Proakis, J. G., Discrete-Time Processing of Speech Signals, Wiley-IEEE 
Press, New York, NY, 2000. 
Donoho, D. L., "Compressed Sensing," IEEE Trans. on Information Theory, Vol. 52, No.4, pp. 1289-1306, 
Apr. 2006. 
Dudgeon, D. E., and Mersereau, R M., Two-Dimensional Digital Signal Processing, Prentice Hall. 
Englewood Cliffs, NJ, 1984. 
Duhamel, P., "Implementation of 'Split-radix' FFr Algorithms for Complex, Real, and Real-symmetric 
Data," IEEE Trans. Acoustics, Speech, and Signal Processing, VoL 34, pp. 285-295, Apr. 1986. 
Duhamel, P., and Hollmann, H., "Split Radix FFT Algorithm," Electronic Letters, Vol. 20, pp. 14-16,Jan. 1984. 
Ebert, P. M., Maw, 1. E., and Taylor, M. c., "Overflow Oscillations in Digital Filters," Bell System Technical 
1, Vol. 48, pp. 2999-3020, 1969. 
Eldar, Y. c., and Oppenheim, A. V., "Filterbank Reconstruction ofBandlimited Signals from Nonuniform and 
Generalized Samples," IEEE Trans. on Signal Processing, Vol. 48, No. 10, pp. 2864-2875, October, 2000. 
Elliott, n E, and Rao, K. R, Fast Transforms: Algorithms, Analysis, Applications, Academic Press, New 
York, NY, 1982. 
J. 
Feller, w., An Introduction to Probability Theory and Its Applications, Wiley, New York, NY, 1950, Vols. 1 
and 2. 
Fettweis, A, "Wave Digital Filters: Theory and Practice," IEEE Proceedings, Vol. 74, No.2, pp. 270-327, 
Feb. 1986. 
Flanagan, 1. L, Speech Analysis, Synthesis and Perception, 2nd ed., Springer-Verlag, New York, NY, 1972. 
Frerking, M. E., Digital Signal Processing in Lommunication Systems, Kluwer Academic, Boston, MA, 1994. 
Friedlander, B., "Lattice Filters for Adaptive Processing," iEEE Proceedings, Vol. 70, pp. 829-867, Aug. 1982. 
rriedlander, B., "Lattice Methods for Spectral Estimation," iEEE Proceedings, Vol. 70, pp. 990-1017, 
September 1982. 
Frigo, M., and Johnson, S. G., "FFTW: An Adaptive Software Architecture for the FFT," Proc. Int. Conf 
Acoustics, Speech, and Signal Processing (ICASSP '98), Vol. 3, pp. 1381-1384, May 1998. 
Frigo, M., and Johnson, S. G., "The Design and Implementation of FFTW3," Proc. of the IEEE, Vol. 93, 
No.2, pp. 216-23L Feb. 2005. 
Furui, S., "Cepstral Analysis Technique for Automatic Speaker Verification," IEEE Trans. Acoustic.~ Speech, 
and Signal Processing, Vol. ASSP-29, No.2, pp. 254-272, Apr. 1981. 
Gallager, R., Principles of Digital Communication, Cambridge University Press, Cambridge, UK, 2008. 
Gardner, w., Statistical Spectral Analysis: A Non-Probabilistic Theory, Prentice Hall, Englewood Cliffs, NJ, 
1988. 
Gentleman, W. M., and Sande, G., "Fast Fourier Transforms for Fun and Profit," 1966 Fall 10int Computer 
Conf, AFIPS Cont Proc, Vol. 29., Spartan Books, Washington, nc., pp. 563-578, 1966. 
Goertzel, G., "An Algorithm for the Evaluation of Finite Trigonometric Series," American Math. Monthly, 
Vol. 65, pp. 34-35, Jan. 1958. 
Gold, B., Oppenheim, A V, and Rader, eM., "Theory and Implementation of the Discrete Hilbert 
Transform," in Proc. Symp. Computer Processing in Communications, Vol. 19, Polytechnic Press, New 
York, NY, 1970. 
~ 

1085 
1  
I 
Bibliography 
Gold, B., and Rader, C. M., Digital Processing of Signal~, McGraw-Hill, New York, NY, 1969. 
Gonzalez, R. c., and Woods, R. E., Digital Image Processing, Wiley, 2007. 
Goyal, V., "Theoretical Foundations of Transfonn Coding," IEEE Signal Processing Magazine. Vol. 18, 
No.5, pp. 9-21, Sept. 2001. 
Gray, A. H., and Markel, J. D., "A Computer Program for Designing Digital Elliptic Filters," IEEE Trans. 
Acoustics, Speech, and Signal Processing, Vol. 24, pp. 529-538, Dec. 1976. 
Gray, R. M., and Davidson, L D., Introduction to Statistical Signal Processing, Cambridge L'niversity Press, 
2004. 
Griffiths, L. J., "An Adaptive Lattice Structure for Noise Canceling Applications," Proc. Int. Cont Acoustia, 
Speech, and Signal Processing (ICASSP '78), Tulsa, OK, Apr. 1978, pp. 87-90. 
Grossman, S., Calculus Part 2, 5th ed., Saunders College Publications, Fort Worth, TX, 1992. 
Guillemin, E. A., Synthesis ofPassive Networks, Wiley, New York, NY, 1957. 
Hannan, E. J., Time Series Analysis, Methuen, London, UK, 1960. 
Harris, F. 1., "On the Use of Windows for Harmonic Analysis with the Discrete Fourier Transform," IEEE 
Proceedings, Vol. 66, pp. 51-83, Jan. 1978. 
Hayes, M. H., Lim, J. S., and Oppenheim, A. v., "Signal Reconstruction from Phase and Magnitude," IEEE 
Trans. Acoustics, Speech, and Signal Processing, Vol. 28, No.6, pp. 672-{)80, Dec. 1980. 
Hayes, M., Statistical Digital Signal Processing and Modeling, Wiley, New York, NY, 19%. 
Haykin, S., Adaptive Filter Theory, 4th ed., Prentice Hall, 2002. 
Haykin, S., and Widrow, B., Least-Mean-Square Adaptive Filters, Wiley-Interscience, Hoboken, NJ, 2003. 
Heideman, M. T., Johnson, D. H., and Burrus, C. S., "Gauss and the History of the Fast Fourier Transform," 
IEEE ASSP Magazine, Vol. 1, No.4, pp. 14-21, Oct. 1984. 
Helms, H. D., "Fast Fourier Transform Method of Computing Difference Equations and Simulating Filters," 
IEEE Trans. Audio Electroacoustics, Vol. 15, No.2, pp. 85-90, 1967. 
Herrmann, 0., "On the Design of Nonrecursive Digital Filters with Linear Phase," Elec. Lett., Vol. 6, No. 11, 
pp.328-329,1970. 
Herrmann, 0., Rabiner, L. R., and Chan, 0. S. K., "Practical Design Rules for Optimum Finite Impulse 
Response Lowpass Digital Filters," Bell System Technicall, Vol. 52, No.6, pp. 769-799. July-Aug. 1973. 
Herrmann, 0., and SchUssler, W., "Design of Nonrecursive Digital Filters with Minimum Phase," Elec. Lett., 
Vol. 6, No.6, pp. 329-330, 1970. 
Herrmann, 0., and W. Schussler, "On the Accuracy Problem in the Design of Nonrecursive Digital Filters," 
Arch. Electronic Ubertragungstechnik, Vol. 24, pp. 525-526, 1970. 
Hewes, C. R., Broderson, R. w., and Buss, D. D., "Applications of CCD and Switched Capacitor Filter 
Technology," IEEE Proceedings, Vol. 67, No. 10, pp.1403-1415, Oct. 1979. 
Hnatek, E. R., A User's Handbook ofDIA and AID Converters, R. E. Krieger Publishing Co., Malabar, 1988. 
Hofstetter, E., Oppenheim, A. v., and Siegel, 1., "On Optimum Nonrecursive Digital Filters," Proc. 9th 
Allerton Cont Circuit System Theory, Oct. 1971. 
Hughes, C. P., and Nikeghbali, A., "The Zeros of Random Polynomials Cluster Near the Unit Circle," 
arXiv:math/0406376v3 [math.CVj, http://arxiv.orgl PS_cache/mathlpdf/0406/0406376v3.pdf. 
Hwang, S. Y., "On Optimization of Cascade Fixed Point Digital Filters.," IEEE Trans. Circuits and Systems, 
Vol. 21, No.1, pp. 163-166, Jan. 1974. 
Itakura, F. I., and Saito, S., "Analysis-synthesis Telcphony Based upon the Maximum Likelihood Method," 
Proc. 6th Int. Congress on Acoustics, pp. CI7-20, Tokyo, 1968. 
Itakura, F. I., and Saito, S., "A Statistical Method for Estimation of Speech Spectral Density and Formant 
Frequencies," Elec. and Comm. in Japan, Vol. 53-A, No.1, pp. 36-43, 1970. 
Jackson, L. B., "On the Interaction of Roundoff Noise and Dynamic Range in Digital Filters," Bell System 
Technical J., Vol. 49, pp. 159-184, Feb. 1970. 
Jackson, L. B., "Roundoff-noise Analysis for Fixed-point Digital Filters Realized in Cascade or Parallel 
Form," IEEE Trans. Audio Electroacoustics, Vol. 18, pp. 107-122, June 1970. 
Jackson, L. B., Digital Filters and Signal Processing: With MATLAB Exercises, 3rd ed., Kluwer Academic 
Publishers, Hingham, MA, 1996. 
Jacobsen, E., and Lyons, R., "The Sliding DFf," IEEE Signal Processing Magazine, Vol. 20, pp. 74-80, 
Mar. 2003. 
Jain, A. K., Fundamentals of Digital Image Processing, Prentice Hall, Englewood Cliffs, NJ, 1989. 

1086 
Bibliography 
Jayant, N. S., and Noll, P., Digital Coding ofWaveforms, Prentice Hall, Englewood Cliffs, NJ, 1984.  
Jenkins, G. M., and Watts, D. G., Spectral Analysis and Its Applications, Holden-Day, San Francisco, CA, 1968.  
Jolley, L. B. W., Summation ofSeries, Dover Publications, New York, NY, 1961.  
Johnston, 1., "A FIlter Family Designed for Use in Quadrature Mirror Filter Banks," Proc. Int. Con!  
Acoustics, Speech, and Signal Processing (ICASSP '80), Vol. 5, pp. 291-294, Apr. 1980. 
JUang, B.-H., Rabiner, L. R, and Wilpon, J. G.• "On the Use of Bandpass Liftering in Speech Recognition,"  
IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. ASSP-35, No.7, pp. 947-954, July 1987.  
Kaiser,1. E, "Digital Filters," in System Analysis by Digital Computer, Chapter 7, E E Kuo and 1. F. Kaiser,  
eds., Wiley, New York, NY, 1966. 
Kaiser, J. F., "Nonrecursive Digital Filter Design Using the Io-sinh Window Function," Proc. 1974 IEEE 
International Symp. on Circuits and Systems, San Francisco. CA. 1974. 
Kaiser, 1. E, and Hamming, R. W., "Sharpening the Response of a Symmetric Nonrecursive Filter by 
Multiple Use of the Same Filter," IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. 25, No.5, 
pp. 415-422, Oct. 1977. 
Kaiser,1. E. and Schafer, R W, "On the Use of the lo-sinh Window for Spectrum Analysis," IEEE Trans. 
Acoustics, Speech, and Signal Processing, Vol. 28, No. J, pp. 105-107, Feb. 1980. 
Kan, E. P. F., and Aggarwal, 1. K., "Error Analysis of Digital Filters Employing Floating Point Arithmetic," 
IEEE Trans. Circuit Theory, Vol. 18, pp. 678-686, Nov. 1971. 
Kaneko, T., and Liu, B., "Accumulation of Roundoff Error in Fast Fourier Transforms," J. Assoc. Comput. 
Mach., Vol. 17, pp. 637--654, Oct. 1970. 
Kanwal, R, Linear Integral Equations. 2nd ed., Springer, 1997. 
Karam, L. 1., and McClellan, 1. H., "Complex Chebychev Approximation for FIR Filter Design," IEEE 
Trans. Circuits and Systems, Vol. 42, pp. 207-216, Mar. 1995. 
Karam, Z. N., and Oppenheim, A. v., "Computation of the One-dimensional Unwrapped Phase," 15th 
International Conference on Digital Signal Processing, pp. 304-307, July 2007. 
Kay, S. M., Modern Spectral Estimation Theory and Application, Prentice Hall. Englewood Cliffs, NJ, 1988. 
Kay, S. M., Intuitive Probability and Random Processes Using MATLAB, Springer, New York, NY, 2006. 
Kay, S. M., and Marple, S. L., "Spectrum Analysis: A Modern Perspective," IEEE Proceedings, Vol. 69, 
pp. 1380-1419, Nov. 1981. 
Keys, R, "Cubic Convolution Interpolation for Digital Image Processing," IEEE Trans. Acoustics, Speech 
and Signal Processing, Vol. 29, No.6, pp. 1153-1160, Dec. 1981. 
Kleijn, W, "Principles of Speech Coding," in Springer Handbook ofSpeech Processing.J. Benesty, M. Sondhi, 
and Y. Huang, eds., Springer, 2008, pp. 283-306. 
Knuth, D. E., The Art of Computer Programming; Seminumerical Algorithms, 3rd ed .. Addison-Wesley, 
Reading, MA, 1997, Vol. 2. 
Koopmanns, L. H., Spectral Analysis of Time Series, 2nd ed., Academic Press, New York, NY, 1995. 
Komer, T. W., Fourier Analysis, Cambridge University Press, Cambridge, UK, 1989. 
Lam. H. Y. F., Analog and Digital Filters: Design and Realization, Prentice Hall, Englewood Cliffs, NJ, 1979. 
Lang, S. W, and McClellan, 1. H., "A Simple Proof of Stability for All-pole Linear Prediction Models," IEEE 
Proceedings, Vol. 67, No.5, pp. 860-861, May 1979. 
Leon-Garcia, A., Probability and Random Processes for Electrical Engineering, 2nd ed., Addison-Wesley, 
Reading, MA, 1994. 
Lighthill, M. 1., Introduction to Fourier Analysis and Generalized Functions, Cambridge University Press, 
Cambridge, UK, 1958. 
Lim, 1. S.• Two-Dimensional Digital Signal Processing, Prentice Hall, Englewood Cliffs, NJ, 1989. 
Liu, B., and Kaneko, T, "Error Analysis of Digital Filters Realized in Floating-point Arithmetic," IEEE 
Proceedings, Vol. 57, pp. 1735-1747, Oct. 1969. 
Liu, B., and Peled, A., "Heuristic Optimization of the Cascade Realization of Fixed Point Digital Filters," 
IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. 23, pp. 464-473, 1975. 
Maeovski, A., Medical Image Processing, Prentice Hall, Englewood Cliffs, Nl. 1983. 
Makhoul, 1., "Spectral Analysis of Speech by Linear Prediction," IEEE Trans. Audio and Electroacoustics, 
Vol. AU-21, No.3, pp. 140-148, June 1973. 
Makhoul, 1., "Linear Prediction; A Tutorial Review," IEEE Proceedings, Vol. 62, pp. 561-580, Apr. 1975. 
Makhoul, 1., "A Fast Cosine Transform in One and Two Dimensions," IEEE Trans. Acoustics, Speech, and 
Signal Processing, Vol. 28, No.1, pp. 27-34, Feb. 1980. 
.....  

Bibliography 
1087 
Maloberti, F.. Data Converters, Springer, New York, NY, 2007. 
Markel, J. n. "FFT Pruning," IEEE Trans. Audio and Electroacoustics. Vol. 19, pp. 305-311, Dec. 1971. 
Markel, 1. D.. and Gray, A. H., Jr., Linear Prediction ofSpeech, Springer-Verlag, New York, NY, 1976. 
Marple, S. L., Digital Spectral Analysis with Applications, Prentice Hall, Englewood Cliffs, NJ, 1987. 
Martucci, S. A., "Symmetrical Convolution and the Discrete Sine and Cosine Transforms," IEEE Trans. 
Signal Processing, Vol. 42, No.5, pp. 1038--1051, May 1994. 
Mason, S., and Zimmermann, H. 1.. Electronic Circuits, Signals and Systems, Wiley, New York, NY, 1960. 
Mathworks, Signal Processing Toolbox Users Guide, The Mathworks, Inc.. Natick, MA, 1998. 
McClellan,1. H., and Parks. T. w., "A Unified Approach to the Design of Optimum FIR Linear Phase Digital 
Filters," IEEE Trans. Circuit Theory. Vol. 20, pp. 697-701, Nov. 1973. 
McClellan, J. H., and Rader, C. M., Number Theory in Digital Signal Processing, Prentice Hall, Englewood 
Cliffs, N J, 1979. 
McClellan, 1. H., "Parametric Signal Modeling," Chapter 1, Advanced Topics in Signal Processing, 1. S. Lim 
and A. V. Oppenheim, eds., Prentice Hall, Englewood Cliffs, 1988. 
Mersereau, R. M., Schafer, R. W., Barnwell, T. P., and Smith, n L., "A Digital Filter Design Package for PCs 
and TMS320s," Proc. MIDCON, Dallas, TX, 1984. 
Mills, W. L., Mullis, C. T., and Roberts, R. A., "Digital Filter Realizations Without Overflow Oscillations," 
IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. 26, pp. 334--338, Aug. 1978. 
Mintzer, E, "Filters for Distortion-free Two-band MuItirate Filter Banks," IEEE Trans. Acoustics, Speech 
and Signal Processing, Vol. 33, No.3, pp. 626-630, June 1985. 
Mitra, S. K., Digital Signal Processing, 3rd ed., McGraw-Hili, New York, NY, 2005. 
Moon, T., and Stirling, w., Mathematical Methods and Algorithms for Signal Processing, Prentice Hall, 1999. 
Nawab, S. H., and Quatieri, T. E, "Short-time Fourier transforms," in Advanced Topics in Signal Processing, 
1. S. Lim and A. V. Oppenheim. eds., Prentice Hall, Englewood Cliffs, NJ, 1988. 
Neuvo, Y., Dong, c.-Y., and Mitra, S., "Interpolated Finite Impulse Response Filtcrs," IEEE Trans. Acoustics, 
Speech and Signal Processing, Vol. 32, No.3, pp. 563--570, June 1984. 
Noll, AM., "Cepstrum Pitch Determination," J. Acoustical Society ofAmerica, VoL 41, pp. 293-309, Feb. 1967. 
Nyquist, H., "CertainTopics in Telegraph Transmission Theory," AlEE Trans., VoL 90, No.2, pp. 2S()"'305, 1928. 
Oetken, G., Parks, T. W., and Schussler, H. w., "New Results in the Design of Digital Interpolators." IEEE 
Trans. Acoustics, Speech, and Signal Processing, VoL 23, pp. 301-309, June 1975. 
Oppenheim, A. V., "Superposition in a Class ofNonlinear Systems," RLE Technical Report No. 432, MIT, 1964. 
Oppenheim, A. V., "Generalized Superposition," Information and Control, Vol. 11, Nos. 5--6, pp. 528--536, 
Nov.-Dec., 1967. 
Oppenheim, A. V., "Generalized Linear Filtering," Chapter 8, Digital Processing of SignaL~, B. Gold and 
C M. Rader, eds., McGraw-Hili. New York, 1969a. 
Oppenheim, A. V., "A Speech Analysis-synthesis System Based on Homomorphic Filtering," 1. Acoustical 
Society ofAmerica, Vol. 45, pp. 458-465, Feb. 1969b. 
Oppenheim, A. v., and Johnson, D. H., "Discrete Representation of Signals," IEEE Proceedings, Vol. 60, 
No.6. pp. 681--691, June 1972. 
Oppenheim, A. V., and Schafer, R w., "Homomorphic Analysis of Speech," IEEE Trans. Audio 
Electroacoustics, VoL AU-16, No.2, pp. 221-226, June 1%8. 
Oppenheim, A. V., and Schafer, R W., Digital Signal Processing, Prentice Hall, Englewood Cliffs. NJ, 1975. 
Oppenheim, A. v., Schafer, R W., and Stockam, T. G., Jr., "Nonlinear Filtering of Multiplied and Convolved 
Signals," IEEE Proceedings, Vol. 56, No.8, pp. 1264-1291, Aug. 1968. 
Oppenheim. A. V., and Willsky, A S., Signals and Systems, 2nd ed., Prentice Hall, Upper Saddle River, NJ, 
1997. 
Oraintara, S., Chen. Y. J., and Nguyen, T., "Integer Fast rouricr Transform," IEEE Trans. on Signal 
Processing, Vol. 50, No.3, pp. 607--61S, Mar. 2001. 
O'Shaughnessy, D., Speech Communication, Human and Machine, 2nd ed., Addison-Wesley, Reading, MA, 
1999. 
Pan, n, "A TUtorial on MPEG/audio Compression," iEEE Multimedia, pp. 60-74, Summer 1995. 
Papoulis, A., "On the Approximation Problem in Filter Design," in IRE Nat. Convention Record, Part 2, 
1957, pp. 175-185. 
Papouiis. A., The Fourier Integral and Its Applications, McGraw-Hili. New York, NY, 1 %2. 

1088 
Bibliography 
Papoulis, A., Signal Analysis, McGraw-Hill Book Company, New York, NY, 1977. 
Papoulis, A., Probability, Random Variables and Stochastic Processes, 4th ed., McGraw-Hill, New York, NY, 
2002. 
Parks, T. W., and Burrus, C. S., Digital Filter Design, Wiley, New York, NY, 1987. 
Parks, T. W., and McClellan, J. H., "Chebyshev Approximation for Nonrecursive Digital Filters with Linear 
Phase," IEEE Trans. Circuit Theory, Vol. 19, pp. 189-194, Mar. 1972. 
Parks, T. W., and McClellan, J. H., "A Program for the Design of Linear Phase Finite Impulse Response 
Filters," IEEE Trans. Audio Electroacoustics, Vol. 20, No.3, pp, 195-199, Aug. 1972. 
Parsons, T. J., Voice and Speech Processing, Prentice Hall, New York, NY, 1986. 
Parzen, E., Modern Probability Theory and Its Applications, Wiley, New York, NY, 1960, 
Pennebaker, W. B., and Mitchell, J. L., lPEG: Still Image Data Compression Standard. Springer, New York, 
NY, 1992, 
Phillips, C. L., and Nagle, H. T., Jr., Digital Control System Analysis and Design, 3rd ed., Prentice Hall, 
Upper Saddle River, NJ, 1995. 
Pratt, w., Digital Image Processing, 4th ed., Wiley, New York, NY, 2007. 
Press, W. H. F., Teukolsky, S. A. B. P., Vetterling, W. T., and Flannery, B. P, Numerical Recipes: The Art of 
Scientific Computing, 3rd ed., Cambridge University Press, Cambridge, UK, 2007. 
Proakis, J. G., and Manolakis, D. G., Digital Signal Processing, Prentice Hall, Upper Saddle River. NJ,2006. 
Quatieri. T. E, Discrete-Time Speech Signal Processing: Principles and Practice, Prentice Hall, Englewood 
Cliffs, NJ, 2002. 
Rabiner, L. R, "The Design of Finite Impulse Response Digital Filters Using Linear Programming 
Techniques," Bell System Technical 1., Vol. 51, pp. 1117-1198, Aug. 1972. 
Rabiner, L. R., "Linear Program Design of Finite Impulse Response (FIR) Digital Filters," IEEE Trans. 
Audio and Electroacoustics, Vol. 20, No.4, pp. 280-288, Oct. 1972, 
Rabiner, L. R, and Gold, B., Theory and Application ofDigital Signal Processing, Prentice Hall, Englewood 
Cliffs, NJ, 1975. 
Rabiner, L. R, Kaiser, J. F., Herrmann, 0., and Dolan, M. T., "Some Comparisons Between FIR and IIR 
Digital Filters," Bell System Technical 1., Vol. 53, No.2, pp. 305-331, Feb. 1974. 
Rabiner, L. R, and Sehafer, R. w., "On the Behavior of Minimax FIR Digital Hilbert TIansformers," Bell 
System Technical 1., Vol. 53, No.2, pp. 361-388, Feb. 1974. 
Rabiner, L. R. and Schafer, R w., Digital Processing of Speech Signals, Prentice Hall, Englewood Cliffs, 
NJ,1978, 
Rabiner, L. R., Schafer, R. w., and Rader, C. M., "The Chirp z-transform Algorithm," IEEE Trans. Audio 
Electroacoustics, Vol. 17, pp. 86-92, June 1969. 
Rader, C. M., "Discrete I-"ourier Transforms when the Number of Data Samples is Prime," IEEE Proceedings. 
Vol. 56, pp. 1107-1108, June 1968. 
Rader, C. M., "An Improved Algorithm for High-speed Autoeorrelation with Applications to Spectral 
Estimation," [EEE Trans. Audio Electroacoustics, Vol. 18, pp. 439-441, Dec. 1970. 
Rader, C. M., and Brenner, N. M., "A New Principle for Fast FourierTransfonnation," IEEE Trans. Acoustics, 
Speech, and Signal Processing, Vol. 25, pp. 264-265, June 1976. 
Rader, C. M., and Gold, B., "Digital Filter Design Techniques in the Frequency Domain," IEEE Proceedings, 
Vol. 55, pp. 149-171, Feb. 1967. 
Ragazzini, J. R., and Franklin, G. E, Sampled Data Control Systems, McGraw-Hill, New York, NY, 1958. 
Rao, K. R, and Hwang, 1. 1., Techniques and Standards for Image, Video, and Audio Coding, Prentice Hall, 
Upper Saddle River, NJ, 1996. 
Rao, K. R, and Yip, P, Discrete Cosine Transform: Algorithms, Advantages, Applications, Academic Press, 
Boston, MA, 1990. 
Rao, S. K., and Kailath, T., "Orthogonal Digital Filters for VLSI Implementation," IE EE Trans. Circuits and 
System, Vol. 31, No. 11, pp. 933-945, Nov. 1984. 
Reut. Z., Pace, N. G., and Heaton, M. 1. p" "Computer Classification of Sea Beds by Sonar," Nature, VoL 314, 
pp. 426-428, Apr. 4, 1985. 
Robinson, E. A., and Durrani, T. S., Geophysical Signal Processing, Prentice Hall, Englewood Cliffs, NJ, 1985. 
Robinson, E. A., and Treitel, S., Geophysical Signal Analysis, Prentice Hall, Englewood Cliffs, NJ, 1980. 
Romberg, 1, "Imaging Via Compressive Sampling," IEEE Signal Processing Magazine, Vol. 25. No.2. 
pp. 14-20, Mar. 2008. 
.....  

Bibliography 
1089 
Ross, S., A First Course in Probability, 8th ed., Prentice Hall, Upper Saddle River, NJ, 2009. 
Runge, C, "Uber die Zerlegung Empirisch Gegebener Periodischer Functionen in Sinuswellen," Z. Math. 
Physik, Vol. 53, pp. 117-123, 1905. 
Sandberg, I. w., "Floating-point -roundoff Accumulation in Digital Filter Realizations," Bell System Technical 
J., Vol. 46, pp. 1775-1791, Oct. 1967. 
Sayed, A., Adaptive Filters, Wiley, Hoboken, NJ, 2008. 
Sayed, A. H., Fundamentals ofAdaptive Filtering, Wiley-IEEE Press, 2003. 
Sayood, K., Introduction to Data Compression, 3rd ed., Morgan Kaufmann, 2005. 
Schaefer, R T., Schafer, R W., and Mersereau, R M., "Digital Signal Processing for Doppler Radar Signals," 
Proc.I979IEEE Int. Con! on Acoustics. Speech, and Signal Processing, pp. 170-173, 1979. 
Schafer, R w., "Echo Removal by Generalized Linear Filtering," RLE Tech. Report No. 466, MIT, 
Cambridge, MA, 1969. 
Schafer, R. w., "Homomorphic Systems and Cepstrum Analysis ofSpeech," Chapter 9, Springer Handbook of 
Speech Processing and Communication, 1. Benesty, M. M. Sondhi, and Y. Huang, eds., Springer-Verlag, 
Heidelberg, 2007. 
Schafer, R w., and Rabiner, L. R, "System for Automatic Formant Analysis of Voiced Speech," 1 Acoustical 
Society ofAmerica, Vol. 47, No.2, pt. 2, pp. 634-648, Feb. 1970. 
Schafer, R w., and Rabiner, L. R, "A Digital Signal Processing Approach to Interpolation," IEEE 
Proceedings, Vol. 61, pp. 692-702, June 1973. 
Schmid, H., Electronic Analog/Digital Conversions, Wiley, New York, NY, 1976. 
Schreier, R, and Ternes, G. C. Undersumding Delta-Sigma Data Converters, IEEE Press and John Wiley 
and Sons, Hoboken, NJ, 2005. 
Schroeder, M. R, "Direct (Nonrecursive) Relations Between Cepstrum and Predictor Coefficients," IEEE 
Trans. Acoustics. Speech and Signal Processing, Vol. 29, No.2, pp. 297-301, Apr. 1981. 
Schussler, H. w., and Steffen, P., "Some Advanced Topics in Filter Design," in Advanced Topics in Signal 
Processing, S. Lim and A V. Oppenheim, eds., Prentice Hall. Englewood Cliffs, NJ, 1988. 
Senmoto, S., and Childers, D. G., "Adaptive Decomposition of a Composite Signal of Identical Unknown 
Wavelets in Noise," IEEE Trans. on Systems, Man, and Cybernetics, Vol. SMC-2, No.1, pp. 59, Jan. 1972. 
Shannon, C. E., "Communication in the Presence of Noise," Proceedings of the Institute of Radio Engineers 
(IRE), VoL 37, No.1, pp. 10-21, Jan. 1949. 
Singleton, R. C, "An Algorithm for Computing the Mixed Radix Fast Fourier Transforms," IEEE Trans. 
Audio Electroacoustics. Vol. 17, pp. 93-103, June 1969. 
Sitton, G. A, Burrus, C S., Fox, J. w., and Treitel, S., "Factoring Very-high-degree Polynomials," IEEE 
Signal Processing Magazine, VoL 20, No.6, pp. 27-42, Nov. 2003. 
Skolnik, M. L, Introduction to Radar Systems, 3rd ed., McGraw-Hill. New York, NY, 2002. 
Slepian, D., Landau, H. T., and Pollack, H. 0., "Prolate Spheroidal Wave Functions, Fourier Analysis, and 
Uncertainty Principle (I and II)," Bell System Technical L Vol. 40. No.1, pp. 43-80,1961. 
Smith, M., and Barnwell, T, " A Procedure for Designing Exact Reconstruction Filter Banks for Tree­
structured Subband Coders," Proc. Int. Cont Acoustics, Speech, and Signal Processing (ICASSP '84), 
VoL 9, Pt. 1, pp. 421-424. Mar. 1984. 
Spanias, A., Painter, T, and Atti, v., Audio Signal Processing and Coding, Wiley, Hoboken, NJ, 2007. 
Sripad, A, and Snyder, D., "A Necessary and Sufficient Condition for Quantization Errors to be Uniform and 
White," IEEE Trans. Acoustics, Speech and Signal Processing, VoL 25, No.5, pp. 442-448, Oct. 1977. 
Stark, H., and Woods, 1., Probability and Random Processes with Applications to Signal Processing, 3rd ed., 
Prentice Hall, Englewood Cliffs, NJ, 2001. 
Starr, T., Cioffi, 1. M., and Silverman, P.1., Understanding Digital Subscriber Line Technology, Prentice Hall, 
Upper Saddle River, NJ, 1999. 
Steiglitz, K., "The Equivalence of Analog and Digital Signal Processing," Information and Control, Vol. 8, 
No.5, pp. 455-467, Oct. 1965. 
Steiglitz, K., and Dickinson, 8., "Phase Unwrapping by Factorization," IEEE Trans. Acoustics, Speech and 
Signal Processing, Vol. 30. No.6, pp. 984-991, Dec. 1982. 
Stockham, T G., "High Speed Convolution and Correlation," in 1966 Spring Joint Computer Conference, 
AFIPS Proceedings, Vol. 28, pp. 229-233, 1966. 
Stockham, T G., Cannon, T. M., and Ingebretsen, R. 8., "Blind Deconvolution Through Digital Signal 
Processing," IEEE Proceedings, Vol. 63, pp. 678-692, Apr. 1975. 

1090 
Bibliography 
Stoica, P., and Moses, R, Spectral Analysis ofSignals, Pearson Prentice Hall, Upper Saddle River, NJ, 2005.  
Storer, 1. E., Passive Network Synthesis, McGraw-Hill, New York, NY, 1957.  
Strang, G., "The Discrete Cosine Transforms," SIAM Review, Vol. 41, No.1, pp. 135-137, 1999.  
Strang, G., and Nguyen, T.. Wavelets and Filter Banks, Wellesley-Cambridge Press, Cambridge, MA, 1996.  
Taubman D. S., and Marcellin, M. W., JPEG 2000: Image Compression Fundamentals, Standards, and  
Practice, Kluwer Academic Publishers, Norwell, MA, 2002. 
Therrien, C. W., Discrete Random Signals and Statistical Signal Processing, Prentice Hall, Englewood Cliffs, 
NJ,1992. 
Tribolet, 1. M., "A New Phase Unwrapping Algorithms," IEEE Trans. Acoustics, Speech, and Signal 
Processing, Vol. 25. No.2, pp. 170-177, Apr. 1977. 
Tribolet, 1. M., Seismic Applications of Homomorphic Signal Processing, Prentice Hall, Englewood Cliffs, 
N1.1979. 
Tukey, 1. w.. Exploratory Data Analysis, Addison-Wesley, Reading. MA, 1977. 
Ulrych, T. J., "Application of Homomorphic Deconvolution to Seismology," Geophysics, Vol. 36, No.4, 
pp. 650-660, Aug.I97L 
Unser, M., "Sampling-50 Years after Shannon," IEEE Proceedings, VoL 88, No.4, pp. 569-587, Apr. 2000.  
Vaidyanathan, P. P., Multirate Systems and Filter Banks, Prentice Hall, Englewood Cliffs, NJ, 1993.  
Van Etten, W. c., Introduction to Random Signals and Noise, John Wiley and Sons, Hoboken, NJ,2005.  
Verhelst, W., and Steenhaut, 0., "A New Model for the Short-time Complex Cepstrum of Voiced Speech,"  
IEEE Trans. on Acoustics, Speech, and Signal Processing, Vol. ASSP-34, No.1, pp. 43--51, February 1986. 
Vernet.1. L, "Real Signals Fast Fourier Transfonn: Storage Capacity and Step Number Reduction by Means 
of an Odd Discrete Fourier Transfonn," IEEE Proceedings, VoL 59, No. 10, pp. 1531-1532, Oct. 1971. 
Vetterli, M., "A Theory of Multirate Filter Banks," IEEE Trans. Acoustics, Speech, and Signal Processing, 
Vol. 35, pp. 356-372, Mar. 1987. 
Vetterli, M., and Kovacevic, 1., Wavelets and Sub band Coding, Prentice Hall, Englewood Cliffs, NJ, 1995. 
Voider, 1. E., "The Cordic Trigonometric Computing Techniques," IRE Trans. Electronic Computers, Vol. 8, 
pp. 330-334,Sept. 1959. 
Walden, R, "Analog-to-digital Converter Survey and Analysis," IEEE Journal on Selected Areas in 
Communications, VoL 17, No.4, pp. 539-550, Apr. 1999. 
Watkinson, J., MPEG Handbook, Focal Press, Boston, MA, 2001. 
Weinberg, L, Network Analysis and Synthesis, R E. Kreiger, Huntington, NY, 1975. 
Weinstein, C. J., "Roundoff Noise in Floating Point Fast Fourier Transform Computation," IEEE Trans. 
Audio Electroacoustics, Vol. 17, pp. 209-215, Sept. 1969. 
Weinstein, C. 1., and Oppenheim, A. v.. "A Comparison of Roundoff Noise in Floating Point and Fixed 
Point Digital Filter Realizations:' IEEE Proceedings, Vol. 57, pp. 1181-1183, June 1969. 
Welch, P. D., "A Fixed-point Fast Fourier Transfonn Error Analysis," IEEE Trans. Audio Electroacoustics, 
Vol. 17, pp. 153-157, June 1969. 
Welch, P. D., "The Use of the Fast Fourier Transform for the Estimation of Power Spectra," IEEE Trans. 
Audio Electroacoustics, Vol. 15, pp. 70-73, June 1970. 
Widrow, B., "A Study of Rough Amplitude Quantization by Means of Nyquist Sampling Theory," IRE Trans. 
Circuit Theory, Vol. 3, pp. 266-276, Dec. 1956. 
Widrow, B., "Statistical Analysis of Amplitude-quantized Sampled-data Systems," AlEE Trans. (Applications 
and Industry), Vol. 81, pp. 555-568, Jan. 1961. 
Widrow, B .. and Kollar, 1., Quantization Noise: Roundoff Error in Digital Computation, Signal Processing, 
Control, and Communications. Cambridge University Press, Cambridge, l...:K, 2008. 
Widrow, B., and Stearns, S. D., Adaptive Signal Processing, Prentice Hall, Englewood Cliffs, NJ, 1985. 
Winograd, S., "On Computing the Discrete Fourier Transform," Mathematics of Computation. Vol. 32, 
No. 141, pp. 175-199, Jan. 1978. 
Woods, ]. W., Multidimensional Signal, Image, and Video Processing and Coding. Academic Press, 2006. 
Yao, K, and Thomas, J. B., "On Some Stability and Interpolatory Properties of Nonuniform Sampling 
Expansions," IEEE Trans. Circuit Theory, Vol. CT-14, pp. 404-408, Dec. 1967. 
Yen,1. L., On Nonuniform Sampling of Bandwidth-limited Signals," IEEE Trans. Circuit Theory, Vol. Cf-3, 
pp. 251-257, Dec. 1956. 
Zverev, A. L, Handbook ofFilter Synthesis, Wiley, New York, NY, 1967. 
l 

INDEX  
AID conversion, See Analog-to-digital (AID)  
conversion  
Absolute summability, 50-52, 65  
defined, 50  
for suddenly-applied exponential, 50-51  
Accumulator, 19-20, 23, 33, 35  
and the backward difference system, 35  
difference equation representation of, 36  
impulse response of, 33  
in cascade with backward difference, 35  
inverse system, 35  
system, 19-20  
as time-invariant system, 21  
Additivity property, 19  
Alias cancellation condition, 203  
Aliasing, 159  
antialiasing filter, 206--207  
and bilinear transformation, 506  
distortion, 159  
downsampling with, 181-184  
prefiltering to avoid, 206--209  
in sampling a sinusoidal signal, 162  
All-pass systems, 305-310  
first- and second-order, 307-309  
All-pole lattice structure, 412--414  
All-pole model lattice network, 923-924  
All-pole modeling, 891-895  
autocorrelation matching property, 898  
determination of the gain parameter G, 899-900  
of finite-energy deterministic signals, 896--897  
least-squares approximation, 892  
least-squares inverse model, 892--894  
linear prediction, 892  
linear prediction formulation of, 895  
linear predictive analysis, 892  
minimum mean-squared error, 898  
random signal modeling, 897  
All-pole spectrum analysis, 907-915  
pole locations, 911-913  
sinusoidal signals, 913-915  
speech signals, 908--911  
Alternation theorem, 558--565  
defined. 558  
and polynomials, 558  
Analog signals, 9  
digital filtering of, 205-224  
AID conversion, 209-214  
DIA conversion, 220-224  
ideal continuous-to-discrete (CfD) converter,  
205  
ideal discrete-to-continuous (D/C) converter,  
205  
Analog-to-digital (AID) conversion, 2,209-214  
measurements of quantization noise, 217-218  
offset binary coding scheme, 212  
oversampling and noise shaping in, 224--236  
physical configuration for, 209  
quantization errors:  
analysis of, 214--215, 214--220  
for a sinusoidal signal, 215-217  
quantizer,210-213  
Analog-to-digital (AID) converters, 205  
Analytic signals, 956, 969  
and bandpass sampling, 966--969  
as a complex time function, 943  
defined, 943  
and narrowband communication, 963  
Analyzer-synthesizer filter banks, 266  
Antialiasing filter, 206--207, 793-795  
frequency response of, 793  
Aperiodic discrete-time sinusoids, 15  
Asymptotically unbiased estimators, 837  
1091 

1092 
Autocorrelation:  
circular, 853  
deterministic autocorrelation sequence, 67  
invariance, 607  
method. 900-903  
and parametric signal modeling, 900-903  
Autocorrelation matching property, 898  
all-pole modeling, 898  
Autocorrelation normal equations, 896  
Levinson-Durbin algorithm, derivation of,  
917-920  
Levinson-Durbin recursion, 916-917  
solutions of, 915-919  
Autocorrelation sequence of h[n], 67-68  
Autoregressive (AR) linear random process, 887  
Autoregressive moving-average (ARMA) linear  
random process, 887  
Backward difference, 13, 22, 33  
Backward difference system, 22, 33, 93, 96  
and the accumulator, 35  
impulse response of, 34  
Backward prediction error, 922-923, 925-926,941  
Bandlimited interpolation, 264  
Bandlimited signal, reconstruction from its samples,  
163-166  
Bandpass filter, and FIR equiripple approximation,  
576-577  
Bartlett (triangular) windows, 536-539, 823-824,  
862-863  
Bartlett's procedure, 844  
Basic sequences/sequence operations:  
complex exponential sequences, 14-15  
exponential sequences, 13-14  
sinusoidal sequences, 14  
unit sample sequence, 12  
unit step sequence, 12-13  
Basis sequences, 673  
Bilateral z-transform, 100, 135  
Bilinear transformation, 504-508  
and aliasing, 506  
of a Butterworth filter, 509-513  
frequency warping, 506-507  
Bit-reversed order, 732  
Blackman-Tukey estimates, 862  
Blackman windows, 536-539, 824  
Blackman-Tukey method, 850, 862  
Block convolution, 668  
Block floating point, 762  
Index 
Block processing, 792  
Bounded-input, bounded-output (BIBO), 22  
Butterfly computation, 730  
Butterworth filter, 581  
bilinear transformation of, 509-513  
impulse invariance, 500-504  
Canonic direct form implementation, 381  
Canonic form implementation, 380-381  
Cascade-form structures, 390-393  
illustration of, 392  
Cascade IIR structure, analysis of, 448--453  
Cascaded systems, 34-35  
Cauchy integral theorem, 943  
Cauchy principal value, 949  
Cauchy-Riemann conditions, 943  
Causal generalized linear-phase systems, 328-338  
FIR linear-phase systems:  
examples of, 330-331  
locations of zeros for, 335-338  
relation to minimum-phase systems, 338-340  
type I FIR linear-phase systems, 330  
example, 331-332  
type II FIR linear-phase systems, 330  
example, 332-333  
type III FIR linear-phase systems, 330  
example, 333-334  
type IV FIR linear-phase systems, 330  
example, 335  
Causal sequences, 32  
even and odd parts of, 945  
exponential sequence, 947  
finite-length sequence, 946  
real- and imaginary-part sufficiency of the Fourier  
transform for, 944-949  
Causal systems, 32  
Causality, 22  
Cepstrum:  
complex, defined, 982-984  
defined, 981-982  
real,984fn  
Cepstrum analysis, 980  
Characteristic system for convolution, 984  
Characteristic, use of term, 419, 458  
Chebyshev criterion, 557  
Chebyshev I design, 581  
Chebyshev II design, 581  
Chirp signals, 751  
Chirp transform algorithm (erA), 749-754  
parameters, 754  
Chirp transform, defined, 751  
~­

1093 
Index 
Chirp z-transform (CZT) algorithm, 754  
Chirps, defined, 812  
Cholesky decomposition, 916  
Circular autocorrelation, 853  
Circular convolution, discrete Fourier transform  
(DFT),654-659  
Circular shift of a sequence, discrete Fourier  
transform (DFT), 648-650  
Clipped samples, 214  
Clipping, 416  
"Closed-form" formulas, 29  
Clutter, 835  
Coefficient quantization, effects of, 421-436  
in an elliptical filter, 423--427  
in FIR systems, 429-431  
in IIR systems, 422-423  
maintaining linear phase, 434-436  
in an optimum FIR filter, 431-434  
poles of quantized second-order sections, 427-429  
Commonly used windows, 535-536  
Complex cepstrum, 955-956, 979  
alternative expressions for, 985-986  
computation of,992-lOoo  
exponential weighting, 1000  
minimum-phase realizations for minimum-phase  
sequences, 998  
phase unwrapping, 993-997  
recursive computation for minimum- and  
maximum-phase sequences, 999-1000  
using polynomial roots, 1001-1002  
using the DFT, 1013-1016  
using the logarithmic derivative, 997-998  
by z-transform analysis, 1009-1012  
deconvolution using, 1002-1006  
minimum-phase/allpass homomorphic  
deconvolution, 1003-1004  
minimum-phase/maximum-phase homomorphic  
deconvolution, 1004-1005  
defioed,982-984  
for exponential sequences, 986-989  
minimum phase and causality of, 956  
for minimum-phase and maximum-phase  
sequences, 989-990  
relationship between the real cepstrum and,  
990-992  
for a simple multipath model, 1006-1024  
generalizations, 1024  
homomorphic deconvolution, 1016-1017  
minimum-phase decomposition, 1017-1023  
speech processing applications, 1024-1032  
applications, 1032  
formants, 1024  
fricative sounds, 1024  
homomorphic deconvolution of speech,  
example of, 1028-1030  
plosive sounds, 1024  
speech model, 1024-1027  
speech model, estimating the parameters of,  
1030-1032  
vocal tract, 1025  
voiced sounds, 1024  
Complex exponential sequences, 14-15,53-54  
Complex logarithm, 982  
properties of, 984-985  
Complex sequences, Hilbert transform relations for,  
956-969  
Complex time functions, analytic signals as, 959  
Compressor, 180  
Compressor, defined, 21  
Conjugate-antisymmetric sequence, 54-55  
Conjugate quadrature filters (CQF), 203  
Conjugate-symmetric sequence, 54-55  
Conjugation property, z-transform, 129  
Consistent estimators, 837  
Consistent resampling, 250  
Constant, Fourier transform of, 52-53  
Continuous-time filters, design of discrete-time IIR  
filters from, 496-508  
bilinear transformation, 504-508  
filter design by impulse invariance, 497-504  
Continuous-time processing of discrete-time signals,  
175-178  
noninteger delay, 176-177  
moving-average system with noninteger with,  
177-179  
Continuous-time signals, 9  
aliasing in sampling a sinusoidal signal, 162  
bandlimited signal, reconstruction from its  
samples, 163-166  
digital filtering of analog signals, 205-224  
discrete-time lowpass filter, ideal continuous-time  
lowpass filtering using, 169-171  
discrete-time processing of, 167-176  
LTI, 168-169  
discrete-time signals, continuous-time processing  
of, 175-179  
frequency-domain representation of sampling,  
154,156-163  

1094 
Continuous-time signals (continued)  
ideal continuous-time bandlimited differentia tor,  
discrete-time implementation of, 171-172  
impulse invariance, 173-175  
applied to continuous-time systems with rational  
system functions, 174-175  
discrete-time lowpass filter obtained by, 174  
multirate signal processing, 194-205  
reconstruction of a bandlimited signal from  
samples, 163-166  
sampling and reconstruction of a sinusoidal  
signal,161-162  
sampling of, 153-273  
sampling rate, changing using discrete-time  
processing, 179-193  
Convolution:  
characteristic system for, 984  
circular, 654-659  
commutative property of, 34  
linear, 660--672  
with aliasing, circular convolution as, 661  
of two finite-length sequences, 660--672  
operation, 30  
Convolution property, z-transform, 130-131  
convolution of finite-length sequences, 131  
Convolution sum, 24-26  
analytical evaluation of, 27-29  
computation of, 23-27  
defined, 24  
Convolution theorem: 
Fourier transform, 60--61 
z-transform,130-131 
Cooley-Tukey algorithms, 735, 746-747, 749  
Coupled form, for second-order systems, 429  
Coupled form oscillator, 471  
Critically damped system, 351  
Cross-correlation, 69--70, 881, 883, 925  
CTA, See Chirp transform algorithm (CTA)  
D/A conversion, See Digital-to-analog (D/A)  
conversion  
DCT, See Discrete cosine transform (DCT)  
DCT-IIDCT-2, See also Discrete cosine transform  
(DCT)  
defined,675  
relationship between, 676-678  
Dead bands, defined, 461  
Index 
Decimation:  
defined, 184  
multistage, 195-197  
Decimation filters, polyphase implementation of,  
199-200  
Decimation-in-frequency FFT algorithms, 737-743  
alternative forms, 741-743  
in-place computations, 741  
Decimation-in-time FFT algorithms, 723-737  
alternative forms, 734-737  
defined, 723  
generalization and programming the FFT, 731  
in-place computations, 731-734  
Decimator, defined, 184  
Decomposition:  
Cholesky, 916  
linear time-invariant (LTI) systems, 311-313  
minimum-phase, 1017-1023  
of one-sided Fourier transform, 958  
polyphase, 197-9  
Deconvolution:  
using the complex cepstrum, 1002-1006  
minimum-phase/allpass homomorphic  
deconvolution, 1003-1004  
minimum-phase/maximum-phase homomorphic  
deconvolution, 1004-1005  
Delay register, 375  
Deterministic autocorrelation sequence, 67-(i8  
DFS, See Discrete Fourier series (DFS)  
DFT, See Discrete Fourier transform (DFT)  
Difference equation representation, of accumulator,  
36  
Difference equations;  
block diagram representation of, 376-377  
determining the impulse response from, 64  
Differentiation property, z-transform, 127-129  
inverse of non-rational z-transform, 128  
second-order pole, 128-129  
Digital filters, 494  
Digital signal processing, 10-17  
Digital signal processors, 1fn  
Digital filtering of analog signals, 205-224  
AID conversion, 209-214  
DIA conversion, 220-224  
ideal continuous-to-discrete (C/D) converter, 205  
ideal discrete-to-continuous (D/C) converter, 205  
Digital signals, 9  

1095 
Index 
Digital-to-analog (D/A) conversion, 2, 220-224  
block diagram, 221  
ideal D/C converter, 221  
oversampling and noise shaping in, 224-236  
zero-order hold, 222-223  
Digital-to-analog (D/A) converters, 205  
Dirac delta function, 12, 154  
Direct-form FIR systems, analysis of, 453-458  
Direct form I implementation, 381  
Direct form II implementation, 381  
Direct-form IIR structures, analysis of, 436-445  
Direct-form structures, 388-390  
illustrations of, 390  
Discrete cosine transform (Def), 673-683  
applications of, 682-683  
DCT-IIDCT-2: 
defined, 675  
relationship between, 676-678  
definitions of, 673-674  
energy compaction property of the DCT-2,  
679-682  
relationship between the DFT and the DCT-2,  
678-679  
Discrete Fourier series (DFS), 624-628  
duality in, 627  
Fourier representation of finite-duration  
sequences, 642-646  
Fourier transform:  
of one period, relationship between Fourier  
series coefficients and, 637-638  
of a periodic discrete-tir~e impulse train, 635  
of periodic signals, 633-638  
sampling, 638-641  
of a periodic rectangular pulse train, 627-628  
properties of, 628-633, 647-660  
circular convolution, 654-659  
circular shift of a sequence, 648-650  
duality, 629-630, 650-652  
linearity, 629, 647-648  
periodic convolution, 630-633  
shift of a sequence, 629  
summary, 634, 660-661  
symmetry, 630  
symmetry properties, 653-654  
of a rectangular pulse, 644-646  
representation of periodic sequences, 624-628  
Discrete Fourier transform (DIT), 3, 623-715  
computation of, 716-791  
coefficients, 745  
direct computation of, 718-723  
direct evaluation of the definition of, 718-719  
exploiting both symmetry and periodicity,  
722-723  
Goertzel algorithm, 717, 719-722, 749  
indexing, 743-745  
practical considerations, 743-745  
computation of average periodograms using, 845  
computation of the complex cepstrum using,  
1013-1016  
computing linear convolution using, 660-672  
decimation-in-frequency FIT algorithms, 737-743  
alternative forms, 741-743  
in-place computations, 741  
decimation-in-time FFI algorithms, 723-737  
alternative forms, 734-737  
defined, 723  
generalization and programming the FIT, 731  
in-place computations, 731-734  
defined, 623,641  
DIT analysis of sinusoidal signals, 797-810  
effect of spectral sampling, 801-810  
effect of windowing, 797-800  
window properties, 800-801  
DIT analysis of sinusoidal signals using a Kaiser  
window, 806-808  
discrete cosine transform (DCf), 673-683  
discrete Fourier series, 624-628  
properties of, 628-633  
finite-length sequences, sufficiency theorcms for,  
950  
finite register length, effects of, 754-762  
Fourier analysis of nonstationary signals:  
examples of, 829-836  
radar signals, 834-836  
speech signals, 830-834  
Fourier analysis of signals using, 792-796  
Fourier analysis of stationary random signals,  
836-849  
computation of average periodograms using the  
DIT,845  
periodogram,837-843  
periodogram analysis, 837, 845-849  
periodogram averaging, 843-845  
general FIT algorithms, 745-748  
chirp transform algorithm (CfA), 749-754  
Winograd Fourier transform algorithm  
(WFfA),749  

1096 
Discrete Fourier transfonn (DFr) (continued)  
implementing linear time-invariant systems using,  
667-672  
linear convolution, 660-672  
with aliasing, circular convolution as, 661-667  
of two finite-length sequences, 661  
properties of, 647-660  
circular convolution, 655-659  
circular shift of a sequence, 648-650  
duality, 650-652  
linearity, 647-648  
summary,659-660  
symmetry properties, 653-654  
of a rectangular pulse, 644-646  
signal frequencies matching DFr frequencies  
exactly, 805--806  
spectrum analysis of random signals using  
autocorrelation sequence estimates, 849-862  
correlation and power spectrum estimates,  
853-855  
power spectrum of quantization noise, 855--860  
power spectrum of speech, 860-862  
time-dependent Fourier transform, 811-829  
defined, 811  
effect of the window, 817-818  
filter bank interpretation of, 826-829  
filter bank interpretation of X[n, A), 816-817  
invertibility of X[n, A), 815--816  
of a linear chirp signal, 811-814  
overlap-add method of reconstruction, 822--825  
sampling in time and frequency, 819--822  
signal processing based on, 825-826  
spectrogram, 814-815  
Discrete Hilbert transform relationships, 949  
Discrete Hilbert transforms, 942-979  
Discrete sine transform (DST), 674  
Discrete-time Butterworth filter, design of, 508-526  
Discrete-time convolution, implementing, 26-27  
Discrete-time differentiators, 507, 550  
and Kaiser window filter design method, 550-553  
Discrete-time filters:  
design of, 493-494  
detennining specifications for, 495-496  
IIR filter design, from continuous-time filters,  
496-508  
bilinear transfonnation, 504-508  
filter design by impulse invariance, 497-504  
Discrete-time Fourier transform (DTFr), 49fn, 623,  
792  
Index 
Discrete-time linear time-invariant (LTI) filter, 4  
Discrete-time model of speech production, 1025,  
1025-1026  
Discrete-time processing, of continuous-time  
signals, 167-176  
Discrete-time random signals, 64-70  
Discrete-time signal processing, 2, 17-23  
backward difference system, 22  
causality, 22  
defined, 17  
discrete-time random signals, 64-70  
forward difference system, 22  
Fourier transforms, representation of sequences  
by, 48-54  
ideal delay system, 17 .  
instability, testing for, 23  
linear systems, 19-20  
accumulator system, 19-20  
nonlinear system, 20  
memoryless systems, 18-19  
moving average, 18  
stability, 22-23  
testing for, 23  
techniques, future promise of, 8  
time-invariant systems, 20-21  
accumulator as, 21  
Discrete-time signals, 10  
basic sequences/sequence operations:  
complex exponential sequences, 14-15  
exponential sequences, 13-14  
sinusoidal sequences, 14  
unit sample sequence, 12  
unit step sequence, 12-13  
continuous-time processing of, 175-178  
defined, 9  
discrete-time systems, 17-23  
graphic depiction of, 11  
graphic representation of, 11  
sampling frequency, 10  
sampling period, 10  
as sequences of numbers, 10-17  
signal-processing systems, classification of, 10  
Discrete-time sinusoids, periodic/aperiodic, 15  
Discrete-time systems, 17-23  
coefficient quantization effects, 421-436  
in an elliptical filter, 423-427  
in FIR systems, 429-431  
in IIR systems, 422-423  
maintaining linear phase, 434-436  
in an optimum FIR filter, 431-434  
..... 

1097 
Index 
poles of quantized second-order sections,  
427-429  
discrete-time random signals:  
autocorrelation/autocovariance sequence, 65-66  
deterministic autocorrelation sequence, 67-68  
power density spectrum, 68  
random process, 65  
white noise, 69-70  
finite-precision numerical effects, 415-421  
number representations, 415-419  
quantization in implementing systems, 419-421  
FIR systems:  
basic network structures for, 401-405  
cascade form structures, 402  
direct-form structures, 401-402  
floating-point realizations of, 458-459  
Fourier transform theorems, 59-64  
convolution theorem, 60-61  
differentiation in frequency, 59  
frequency shifting, 59  
linearity of Fourier transform, 59  
modulation or windowing theorem, 61-62  
Parseval's theorem, 60  
time reversal, 59  
time shifting, 59  
frequency-domain representation of, 40-48  
eigenfunctions for linear time-invariant systems,  
40-45  
frequency response of the ideal delay system, 41  
frequency response of the moving-average  
system, 45-46 
.  
ideal frequency-selective filters, 43-44  
sinusoidal response of linear time-invariant  
systems, 42-43  
suddenly-applied complex exponential inputs,  
46-48  
ideal delay system, 17  
IIR systems:  
basic structures for, 388-397  
cascade form structures, 390-393  
direct form structures, 388-390  
feedback in, 395-397  
parallel form structures, 393-395  
lattice filters, 405-415  
all-pole lattice structure, 412-414  
FIR,406-412  
generalization of lattice systems, 415  
lattice implementation of an IIR system, 414  
linear constant-coefficient difference equations,  
35-41  
block diagram representation of, 375-382  
signal flow graph representation of, 382-388  
linear-phase FIR systems, structures for, 403-405  
linear systems, 19  
accumulator system, 19-20  
nonlinear system, 20  
linear time-invariant systems, 23-35  
convolution sum, 23-29  
eigenfunctions for, 40-46  
properties of, 30-35  
memory less systems, 18-19  
moving average, 18  
representation of sequences by Fourier  
transforms, 48-54  
absolute summability for suddenly-applied  
exponential, 50  
Fourier transform of a constant, 52-53  
Fourier transform of complex exponential  
sequences, 53  
inverse Fourier transform, 48  
square-summability for the ideal lowpass filter,  
51-52  
round-off noise in digital filters, effects of, 436-459  
stability, 22-23  
testing for, 23  
structures for, 374-492  
symmetric properties of Fourier transform, 55-57  
conjugate-antisymmetric sequence, 55-56  
conjugate-symmetric sequence, 55-56  
even function, 55  
even sequence, 54  
illustration oL 56-57  
odd function, 55  
odd sequence, 54  
time-invariant systems, 20-21  
accumulator as, 21  
compressor system, 21  
transposed forms, 397-401  
Doppler frequency, 834-835  
Doppler radar:  
defined, 793  
signals, 835-836  
Downsampling,180-182  
with aliasing, 181-184  
defined, 180  
frequency-domain illustration of, 181-182  
with prefiltering to avoid aliasing, 183  

1098 
Duality:  
discrete Fourier series, 627, 629-630, 650-652  
discrete Fourier transform (DFf), 650-652  
Echo detection, and cepstrum, 982  
Eigenfunctions, 40-45  
for linear time-invariant (LTI) systems, 40-46, 61  
Eigenvalues, 40-41, See also Frequency response of  
Elliptic filter design, 508-526, 581  
Energy compaction property of the DCf-2, 679-682  
Energy density spectrum, 60  
Equiripple approximations, 560  
Even function, 55-56  
Even sequence, 54-55  
Even symmetry, 673  
Expander (sampling rate), 184  
Exponential mUltiplication property, z-transform,  
126-127  
Exponential sequences, 13-14,947  
External points, 558  
Extraripple case, 561  
Extremals, 558  
Fast Fourier transform (FFf) algorithms, 3-4, 6-7,  
660,671,716  
decimation-in-frequency FFf algorithms, 737-743  
decimation-in-time FFf algorithms, 723-737  
finite register length, effects of, 754-762  
general FFf algorithms, 745-748  
FFf, See Fast Fourier transform (FFf) algorithms  
FFfW ("Fastest Fourier Transform in the West")  
algorithm, 748  
Filter bank interpretation, of time-dependent  
Fourier transform, 826-829  
Filter banks:  
analyzer-synthesizer, 266  
multirate,201-205  
alias cancellation condition, 203  
quadrature mirror filters, 203  
Filter design:  
bilinear transformation, 504-508  
Butterworth filter, 508-526  
bilinear transformation of, 509-513  
Chebyshev filter, 508-526  
elliptic filters, 508-526  
FIR filters, 578  
design by windowing, 533-545  
FIR equiripple approximation examples,  
570-577  
optimal type I lowpass filters, 559-564  
Index 
optimal type II lowpass filters, 565-566  
optimum approximations of, 554-559  
optimum lowpass FIR filter characteristics,  
568-570  
Parks-McClellan algorithm, 566-568  
IIR filters, 578  
design comparisons, 513-519  
design example for comparison with FIR  
designs, 519-526  
design examples, 509-526  
by impUlse invariance, 497-504  
lowpass IIR filters, frequency transformations of,  
526-532  
specifications, 494-496  
stages of, 494  
techniques, 493-622  
upsampling filter, 579-582  
Filters, 493  
Financial engineering, defined, 3  
Finite-energy deterministic signals, all-pole  
modeling of, 896-897  
Finite impulse response (FIR) systems, 493  
Finite-length sequences, 946  
convolution of, 131  
sufficiency theorems for, 949-954  
Finite-length truncated exponential sequence, 109  
Finite-precision numerical effects, 415-421  
number representations, 415-419  
quantization in implementing systems, 419-421  
Finite register length, effects of, 754-762  
FIR equiripple approximation, examples of, 570-577  
bandpass filter, 576-577  
compensation for zero-order hold, 571-575  
lowpass filter, 570-571  
FIR filters:  
design by windowing, 533-545  
incorporation of generalized linear phase,  
538-541  
Kaiser window filter design method, 541-553  
properties of commonly used windows, 535-538  
optimum approximations of, 554-559  
FIR lattice filters, 406-412  
FIR linear-phase systems:  
examples of, 330-331  
locations of zeros for, 335-338  
relation to minimum-phase systems, 338-340  
type I FIR linear-phase systems, 329  
example, 331-332  
type II FIR linear-phase systems, 330  
example, 332-333  

1099 
........  
Index 
type III FIR linear-phase systems, 330  
example, 333-334  
type IV FIR linear-phase systems, 330  
example, 335  
FIR systems:  
basic network structures for, 401-405  
cascade form structures, 403  
direct-form structures, 401-402  
coefficient quantization, effects of, 429-431  
First backward difference, 93  
Fixed-point realizations of IIR digital filters,  
zero-input limit cycles in, 459-463  
Floating-point arithmetic, 458  
Floating-point operations (FLOPS), 747-748  
Floating-point realizations of discrete-time systems,  
458-459  
Floating-point representations, 419  
Flow graph:  
reversal, 397  
transposition of, 397-401  
Formants, 830, 1024  
Forward difference, 33  
Forward difference systems, 22  
noncausal, 34-35  
Forward prediction error, 922  
Fourier analysis of nonstationary signals:  
examples of, 829-836  
radar signals, 834-836  
speech signals, 830-834  
Fourier analysis of signals using DFT, 792-796  
basic steps, 793  
DFT analysis of sinusoidal signals, 797-810  
effect of spectral sampling, 801-810  
effect of windowing, 797-800  
window properties, 800-801  
relationship between DFT values, 796  
Fourier analysis of sinusoidal signals:  
effect of windowing on, 798-800  
Fourier analysis of stationary random signals,  
836-849  
computation of average periodograms using the  
DFT,845  
periodogram, 837-843  
periodogram analysis, example of, 845-849  
periodogram averaging, 843-845  
Fourier transform:  
of complex exponential sequences, 53-54  
of a constant, 52-53  
convolution theorem, 60-61  
differentiation in frequency theorem, 59  
linearity of, 59  
magnitude of, 49  
modulation or windowing theorem, 61-62  
of one period, relationship between Fourier series  
coefficients and, 637-638  
pairs, 62  
Parseval's theorem for, 60  
of a periodic discrete-time impulse train, 635  
of periodic signals, 633-638  
phase of, 49  
sampling, 638-641  
symmetry properties of, 54-58  
theorems, 58-64  
time reversal theorem, 59  
time shifting and frequency shifting theorem, 59  
of a typical window sequence, 795  
Fourier transforms:  
defined, 49  
discrete-time (DTFf), 49fn  
inverse, 48-49  
representation of sequences by, 48-54  
Frequency, 14  
Frequency-division multiplexing (FDM), 266  
Frequency-domain representation:  
of discrete-time signals/systems, 40-48  
of sampling, 154-157  
Frequency estimation:  
oversampling and linear interpolation for,  
809-810  
Frequency response, 40-43  
of antialiasing filter, 793  
defined, 40-41  
determining the impulse response from, 63  
of the ideal delay system, 41  
of linear time-invariant (LTI) systems, 275-283  
effects of group delay and attenuation, 278-283  
frequency response phase and group delay,  
275-278  
of the moving-average system, 45-46  
for rational system functions, 290-301  
examples with multiple poles and zeros, 296-301  
first-order systems, 292-296  
second-order FIR system, 298  
second-order IIR system, 296-298  
third-order IIR system, 299-301  
Frequency-response compensation of  
non-minimum-phase systems, 313-318  

1100 
Frequency-sampling filters, 480, 487  
Frequency-sampling systems, 396  
Frequency-selective filters, 493, See also Filter design  
obtaining from a lowpass discrete-time filter, 527  
Frequency shifting, 59  
Frequency warping, and bilinear transformation of,  
506-507  
Fricative sounds, 830, 1024  
Gain, 275  
Generalized linear phase, linear systems with,  
326-328  
examples of, 327-328  
Gibbs phenomenon, 52, 534  
Goertzel algorithm, 717, 719-722  
Hamming windows, 280fn, 536-539, 823-824,  
,862-863  
Hann windows, 536-539, 823-824  
Highpass filter, transformation of a lowpass filter to,  
530-532  
Hilbert transform relations, 942-979  
for complex sequences, 956-969  
defined, 943  
finite-length sequences, 946  
sufficiency theorems for, 949-954  
between magnitude and phase, 955  
Poisson's formulas, 943  
real- and imaginary-part sufficiency of the Fourier  
transform for causal sequences, 944-949  
relationships between magnitude and phase,  
955-956  
Hilbert transform relationships, 942, 969  
discrete, 948-949  
Hilbert transformer, 958  
bandpass sampling, 966-969  
bandpass signals, representation of, 963-966  
design of, 960-963  
impulse response of, 960  
Kaiser window design of, 960-963  
Homogeneity property, 19  
Homogeneous difference equation, 38  
Homogeneous solution, 38  
Homomorphic deconvolution, 980, 1026  
of speech, example of, 1028-1030  
Homomorphic systems, 980  
Ideal 9O-degree phase shifter, 602, 958-959  
Ideal continuous-to-discrete-time (C/D) converter,  
154,205  
Index 
Ideal D/C converter, 221  
Ideal delay impulse response, 32  
Ideal delay system, 17  
frequency response of, 41-43  
Ideal discrete-to-continuous (D/C) converter, 205  
Ideallowpass filter, square-summability for, 51-52  
IIR digital filters, zero-input limit cycles in  
fixed-point realizations of, 459-463  
IIR systems:  
basic structures for, 388-397  
cascade form structures, 390-393  
coefficient quantization, effects of, 422-423  
direct forms structures, 388-390  
feedback in, 395-397  
lattice implementation of, 414  
parallel form structures, 393-395  
scaling in fixed-point implementations of, 445-448  
Impulse invariance:  
basis for, 504  
with a Butterworth filter, 500-504  
design procedure, 504  
filter design by, 497-504  
Impulse response:  
of accumulator, 33  
determining for a difference equation, 64  
determining from the frequency response, 63  
for rational system functions, 288-290  
Impulse sequence, 13  
In-place computations:  
decimation-in-frequency FFT algorithms, 741  
decimation-in-time FFT algorithms, 731-734  
defined, 732  
Indexing, discrete Fourier transform (DFT), 743-745  
Infinite-duration impulse response (IIR) systems, 33  
Infinite impUlse response (IIR) systems, 493  
Initial-rest conditions, 39  
Initial value theorem, 151  
for right-sided sequences, 1037  
Inspection method, inverse z-transform, 116  
Instability, testing for, 23  
Instantaneous frequency, 812  
Integer factor:  
increasing sampling rate by, 184-187  
reducing sampling rate by, 180-184  
Integrator, 589  
Interpolated FIR filter, defined, 196  
Interpolation, 187-190  
defined, 185  

-
1101 
Index 
Interpolation filters, 187-190  
polyphase implementation of, 200-201  
Interpolator, 187-190  
defined, 185  
multistage, 195-197  
Inverse Fourier transforms, 48-49, 63  
Inverse systems, 35, 286-288  
accumulator, 35  
defined,33  
for first-order system, 287  
inverse for system with a zero in the ROC, 288  
Inverse z-transform, 115-124  
inspection method, 116  
inverse by partial fractions, 120-121  
partial fraction expansion, 116-117  
power series expansion, 122-124  
second-order z-transform, 118-120  
JPEG (Joint Photographic Expert Group), ltn 
k-parameters, 406, 920  
direct computation of, 925-926  
Kaiser window, BOO-8m, 823  
design, 581  
DFf analysis of sinusoidal signals using, 806-808  
and zero-padding, DFf analysis with, 808-809  
Kaiser window design, of Hilbert transformers,  
960-962  
Kaiser window filter design method, 541-553  
examples of FIR filter design by, 545-553  
discrete-time differentiators, 550-553  
highpass filter, 547-550  
lowpass filter, 545-547  
relationship of the Kaiser window to other  
windows, 544-545  
LabView, 3,509 
Lattice filters, 405-415,920-926  
all-pole lattice structure, 412-414  
all-pole model lattice network, 923-924  
direct computation of k-parameters, 925-926  
FIR, 406-412  
generalization of lattice systems, 415  
lattice implementation of an IIR system, 414  
prediction error lattice network, 921-923  
Lattice k-to-a algorithm, 921  
Lattice systems:  
generalization of, 415  
Laurent series, 102  
Leakage, 800  
Least-squares approximation, 892  
all-pole modeling, 892  
Least-squares inverse model, 892-894  
all-pole modeling, 892-894  
Left-sided exponential sequence, 104--105  
Levinson-Durbin algorithm, derivation of, 917-921,  
926  
Levinson-Durbin recursion, 916-917, 923  
Limit cycles:  
avoiding, 463  
defined, 667  
owing to overflow, 462-463  
owing to round-off and truncation, 460-462  
Limit cycles, defined, 461  
Linear chirp signal, of a time-dependent Fourier  
transform, 811-814  
Linear constant-coefficient difference equations,  
35-41  
difference equation representation of, 36  
of the moving-average system, 37  
homogeneous difference equation, 38  
systems characterized by, 283-290  
impulse response for rational system functions, 
288-290  
inverse systems, 286-288  
second-order system, 284--285  
stability and causality, 285-286  
Linear convolution, 660-672  
with aliasing, circular convolution as, 661-667  
of two finite-length sequences, 661  
Linear interpolation, 187-190  
Linear phase:  
causal generalized linear-phase systems, 328-338  
generalized, 326-338  
ideallowpass with, 324--326  
systems with, 322-326  
Linear-phase filters, 345  
Linear-phase FIR systems, structures for, 403-405  
Linear-phase lowpass filters, and windowing,  
540-541  
Linear prediction formulation, of all-pole modeling,  
895  
Linear predictive analysis, 892  
Linear predictive coding (LPC), 4, 892fn  
Linear predictor, 895  
Linear quantizers, 21lfn  
Linear systems, 19-20, 326  
accumulator system, 19-20  
nonlinear system, 20  

1102 
Linear time-invariant (LTI) systems, 4, 23-35  
all-pass systems, 305-310  
first- and second-order, 307-309  
cascade combination of, 31  
convolution operation, 30  
convolution sum, 24-26  
analytical evaluation of, 27-29  
eigenfunctions for, 40-46  
FIR linear-phase systems, relation to, 338-340  
frequency response for rational system functions,  
290-301  
frequency response of, 275-283  
linear constant-coefficient difference equations,  
35-40  
systems characterized by, 283-290  
linear systems with generalized linear phase,  
322-340  
causal generalized linear-phase systems, 328-338  
generalized linear phase, 326-328  
systems with linear phase, 322-326  
minimum-phase systems, 311-322  
decomposition, 311-313  
relation of FIR linear-phase systems to, 338-340  
non-minimum-phase systems, frequency response  
compensation of, 313-318  
parallel combination of, 31  
properties of, 30-35  
relationship between magnitude and phase,  
301-305  
sinusoidal response of, 42  
suddenly applied complex exponential inputs,  
46-48  
system function, 115, 132  
transform analysis of, 274-373  
z-transform and, 131-134  
Linearity:  
discrete Fourier series (DFS), 647-648  
discrete Fourier transform (DFT), 647-648  
of Fourier transform, 59  
z-transform, 124-125  
Linearity property, z-transform, 124-125  
Long division, power series expansion by, 119  
Lowpass filter:  
and FIR equiripple approximation, 570-571  
tolerance scheme, 495  
transformation to a highpass filter, 530-532  
Lowpass IIR filters, frequency transformations,  
526-532  
Index 
Magnitude:  
of Fourier transform, 49  
of frequency response, 275, 281  
relationship between phase and, 301-305  
Magnitude response, 275  
Magnitude spectrum, 915  
Magnitude-squared function, 291, 305, 311, 501,  
510-511, 513, 1020  
Mantissa, 419,458  
Mason's gain formula of signal flow graph theory,  
397fn  
Matched filter, 79  
Mathematica,3  
Mi\TLi\B, 3, 283, 509, 579  
Maximally decimated filter, banks, 202fn  
Maximum energy-delay systems, 320  
Maximum-phase systems, 320  
Memoryless systems, 18-19  
Microelectronic mechanical systems (MEMS), 8  
Microelectronics engineers, and digital signal  
processing, 8  
Minimax criterion, 557  
Minimum energy-delay systems, 320  
Minimum group-delay property, 319  
Minimum mean-squared error, 898  
Minimum-phase/allpass homomorphic  
deconvolution, 1003-1004  
Minimum-phase echo system, complex cepstrum of,  
989-990  
Minimum phase-lag system, 318  
Minimum-phase LTI filter, 349  
Minimum-phase/maximum-phase homomorphic  
deconvolution, 1004-1005  
Minimum-phase systems, 311-322  
decomposition, 311-313  
defined, 311  
frequency-response compensation of  
non-minimum-phase systems, 313-318  
properties of, 318-322  
minimum energy-delay property, 319-322  
minimum group-delay property, 319  
minimum phase-lag property, 318-319  
Model order, 905-907  
selection, 906-907  
Modified periodogram, 838  
Modulation theorem, 61-62  
Moore's Law, 2  
Moving average. 18,31-32  
Moving-average (Mi\) linear random process, 887  

Index 
1103 
Moving-average system, 40  
difference equation representation of, 37  
frequency response of, 45-46  
with noninteger delay, 177-179  
MP3 audio coding, 825  
MPEG-II audio coding standard, 828  
MPEG (Moving Picture Expert Group), lfn  
Multidimensional signal processing, 4  
Multirate filter banks, 201-205  
alias cancellation condition, 203  
quadrature mirror filters, 203  
Multirate signal processing, 194-205  
compressor/expander, interchange of filtering  
with,194-195  
defined, 194  
interpolation filters, polyphase implementation  
of, 200--201  
multirate filter banks, 201-205  
multistage decimation and interpolation, 195-197  
polyphase decompositions, 197-199  
polyphase implementation of decimation filters,  
199-200  
Multistage noise shaping (MASH), 233, 272  
N-point DFT, 723fn  
Narrowband communication, and analytic signals,  
963  
Narrowband time-dependent Fourier analysis, 832  
Networking, 480  
Networks, use of term, 375fn  
9O-degree phase shifter, ideal, 958-959  
9O-degree phase splitters, 589  
Noise shaping:  
in AID conversion, 224-234  
in analog-to-digital (AID) conversion, 224-236  
in D/A conversion, 234-236  
multistage noise shaping (MASH), 233, 272  
Noise-shaping quantizer, 220--221  
Non-minimum-phase systems, frequency-response  
compensation of, 313-318  
Non-overlapping regions of convergence (ROC),  
113  
Nonanticipative system, 22  
Noncausal window, 816  
Noncomputable network, 396-397  
Noninteger factor, changing sampling rate by,  
190--193  
Nonlinear systems, 20  
nth-order Chebyshev polynomial, 556fn  
Number-theoretic transforms (NTIs), 789  
Nyquist frequency, 160  
Nyquist rate, 160  
Nyquist-Shannon sampling theorem, 160,236  
Odd function, 55-56  
Odd sequence, 54-55  
Offset binary coding scheme, 212  
One-sided Fourier transform, decomposition of,958  
One-sided z-transform, 100, 135  
One's complement, 415  
Optimum lowpass FIR filter, characteristics of,  
568-570  
Overflow, 416  
Overflow oscillations:  
defined, 462  
in second-order system, 462-463  
Overlap-add method, 670, 777, 825, 826, 828  
Overlap-save method, 671,777  
Overloaded quantizers, 214  
Oversampling:  
in A/D conversion, 224-234  
in analog-to-digital (A/D) conversion, 224-236  
in D/A conversion, 234-236  
and linear interpolation for frequency estimation,  
809-810  
oversampled AID conversion with direct  
quantization, 225-229  
Oversampling ratio, 225  
Parallel-form structures, 393-395  
illustration of, 394-395  
Parametric signal modeling, 890--941  
all-pole modeling of signals, 891-895  
autoeorrelation matching property, 898  
determination of the gain parameter G, 899-900  
of finite-energy deterministic signals, 896-897  
least-squares approximation, 892  
least-squares inverse model, 892-894  
linear prediction, 892  
linear prediction formulation of, 895  
linear predictive analysis, 892  
minimum mean-squared error, 898  
random signal modeling, 897  
all-pole spectrum analysis, 907-915  
pole locations, 911-913  
sinusoidal signals, 913-915  
speech signals, 908-911  
applications, 891  
•

I 
I 
1104 
Parametric signal modeling, (continued) 
autocorrelation normal equations: 
Levinson-Durbin algorithm, derivation of, 
917-920  
Levinson-Durbin recursion, 916-917  
solutions of, 915-919  
defined, 890  
deterministic and random signal models, 896-900  
estimation of correlation functions, 900-905  
autocorrelation method, 900-903  
comparison of methods, 904-905  
covariance method, 903--904  
equations for predictor coefficients, 905  
prediction error, 904  
stability of the model system, 905  
lattice filters, 920-926  
all-pole model lattice network, 923-924  
direct computation of k-parameters, 925-926  
prediction error lattice network, 921-923  
model order, 905-907  
selection, 906-907  
PARCOR coefficient, 916fn, 925  
Parks-McClellan algorithm, 494, 566-568, 568, 571,  
579,963  
Parseval's theorem, 58,60,96,441,446,679,707,908  
Partial energy of an impUlse response, 319  
Partial fraction expansion, inverse z-transform,  
116-117,120-121  
Periodic conjugate-antisymmetric components, 654  
Periodic conjugate-symmetric components, 654  
Periodic convolution, 61  
discrete Fourier series, 630-633  
Periodic discrete-time sinusoids, 15  
Periodic even components, 654  
Periodic impulse train, 1026  
Periodic odd components, 654  
Periodic sampling, 153-156, 236  
Periodic sequence, 14-15,950,954  
Periodogram, 837-843, 864  
computation of average periodograms using the 
DFf,845  
defined, 838  
modified, 838  
periodogram analysis, example of, 837, 845-849  
periodogram averaging, 843-845  
properties of, 839-843  
smoothed, 851  
Periodogram analysis, 837  
Index 
Phase:  
defined, 14  
relationship between phase magnitude and,  
301-305  
Phase distortions, 275  
Phase-lag function, 318  
Phase response, 275  
Plosive sounds, 830, 1024  
Poisson's formulas, 943  
Pole locations, all-pole spectrum analysis, 911-913  
Poles of quantized second-order sections, 427-429  
Polynomials, and alternation theorem, 558  
Polyphase components ofh[n], 197  
Polyphase implementation of decimation filters,  
199-200  
Power density spectrum, 68  
Power series expansion, 122-124  
finite-length sequence, 122  
inverse transform by, 123  
by long division, 123  
Power spectral density, 225-228, 231-232, 236  
Power spectrum of quantization noise estimates,  
855-860  
Power spectrum of speech estimates, 860-862  
Predictable random sinusoidal signals, 936fn  
Prediction coefficients, 895  
Prediction error residual, 895  
Prime factor algorithms, 747,749  
Quadrature mirror filters, 203  
Quantization errors, 416  
analog-to-digital (AID) conversion:  
analysis of, 214-220  
for a sinusoidal signal, 215-217  
analysis of, 214-215  
defined, 214  
for a sinusoidal signal, 215-217  
Quantization errors, analysis, 214-220  
Quantization noise, measurements of, 217-218  
Quantizers,210-213  
linear, 21lfn  
overloaded, 214  
Quasiperiodic voiced segments, 832  
Radar signals, time-dependent Fourier analysis of, 
834-836  
Radix-m FFf algorithm, 783  
Random noise, 1026  
..  
I  
1 
t 

1105 
Index 
Random process, 65  
autoregressive (AR) linear random process, 887  
autoregressive moving-average (ARMA) linear  
random process, 887  
moving-average (MA) linear random process, 887  
Random signal modeling, 897  
Random signals, 65-70  
Random sinusoidal signals, 936  
Range, 834  
Real cepstrum, 984fn  
Rectangular pulse, discrete Fourier series of,  
644-646  
Rectangular windows, 535-537, 539  
Recursive computation, 38  
Reflection coefficients, 916fn  
Region of convergence (ROC), 101-102,285-286  
determining, 285-286  
non-overlapping, 113  
properties offor z-transform, 110-115  
stability, causality and, 115  
Resampling,179-180  
consistent, 250  
defined, 179  
Residual, 895, 905  
Right-sided exponential sequence, 103-104  
Rotation of a sequence, 650  
Rotations, 782  
Round-off noise in digital filters:  
analysis of the direct-form IIR structures, 436-445  
cascade IIR structure, analysis of, 448-453  
direct-form FIR systems, analysis of, 453-458  
effects of, 436-459  
first-order system, 441-442  
interaction between scaling and round-off noise,  
448  
scaling in fixed-point implementations of IIR  
systems, 445-448  
second-order system, 442  
Sample mean, 837  
Sample variance, 837  
Sampled-data Delta-Sigma modulator, 220-221  
Sampling:  
frequency-domain representation of, 156-163  
in time and frequency, 819-822  
Sampling frequency/Nyquist frequency, 154  
Sampling period, 154  
Sampling rate:  
changing by a noninteger factor, 190-193  
changing using discrete-time signals, 179-193  
increasing by an integer factor, 184-187  
reduction by an integer factor, 180-184  
Sampling rate compressor, 180  
Sampling rate expander, 184  
Saturation overflow, 416  
Scaling, interaction between round-off noise and,  
448  
Scaling property, 19  
Second -order z-transform, 118-120  
Seismic data analysis, 4  
and multidimensional signal processing  
techniques, 4  
Sequence value, 723fn  
Sequences:  
autocorrelation, 67-68  
basic, 12-15  
causal,32  
complex exponential, 14-15, 53-54  
conjugate-antisymmetric, 54-55  
conjugate-symmetric, 54-55  
deterministic autocorrelation, 67  
even, 54-55  
exponential, 13-14,947  
finite-length, 946, 949-954  
convolution of, 131  
impulse, 13  
left-sided exponential, 104-105  
odd,54-55  
periodic, 14-15  
right-sided exponential, 103--104  
shifted exponential, 126  
sinusoidal,14  
time-reversed exponential, 129  
unit sample, 12-13  
unit step, 12-13  
Sharpening, 609  
Shift-invariant system, See TIme-invariant systems  
Shifted exponential sequence, 126  
Short-time Fourier transform, See Time-dependent  
Fourier transform  
Sifting property of the impulse function, 154  
Sign and magnitUde, 415  
Sign bit, 415  
Signal flow graph representation of linear  
constant-coefficient difference equations,  
382-388  
Signal interpretation, 3  
Signal modeling, 4-5  

1106 
Signal predictability, 3  
Signal processing, 2  
based on time-dependent Fourier transform,  
825-826  
historical perspective, 5-8  
multidimensional, 4  
problems/solutions, 4  
Signal-processing systems, classification of, 10  
Signal-to-noise ratio (SNR), 3  
Signal-to-quantization-noise ratio, 219-220  
Signals:  
defined, 9  
mathematical representation of, 9  
Simulink,3  
Sink nodes, 383  
Sinusoidal signals:  
all-pole spectrum analysis, 913-915  
defined, 218  
quantization error for, 215-217  
signal-to-quantization-noise ratio, 219-220  
for uniform quantizers, 220  
Smoothed periodogram, 851  
Source nodes, 383  
Specifications, filter design, 494--496  
Spectral analysis, 4  
Spectral sampling, effect of, 801-810  
illustration, 803-805  
Spectrograms, 814-815  
plotting, 832  
wideband, 832  
Spectrum analysis of random signals using  
autocorrelation sequence estimates, 849-862  
correlation and power spectrum estimates,  
853-855  
power spectrum of quantization noise estimates,  
855-860  
power spectrum of speech estimates, 860-862  
Speech model, 1024-1027  
speech model, estimating the parameters of,  
1030-1032  
vocal tract, 1025  
voiced sounds, 1024  
Speech signals:  
all-pole spectrum analysis, 908-911  
time-dependent Fourier analysis of, 830-834  
Split-radix FFT (SRFFT), 781  
Square summability, 51, 65  
for the ideallowpass filter, 51-52  
Stability, 22-23  
Index 
testing for, 23  
Stationary, use of term, 66fn  
Steady-state response, 46  
Suddenly-applied exponential:  
absolute summability for, 51-52  
inputs, 46-48  
Summability:  
absolute, 50-52, 65  
square-, 51, 65  
for the ideallowpass filter, 51-52  
Superposition, principle of, 19, 20, 23, 980,  
1002-1003  
Surface acoustic wave (SAW), 753  
Switched-capacitor technologies, 2  
Symmetry properties:  
discrete Fourier series, 630, 653-654  
discrete Fourier transform (DIT), 653-654  
Fourier transform, 54-57  
illustration of,56-57 
System function, 274-275  
determination of, from a flow graph, 386-387  
linear time-invariant (LTI) systems, 115, 132  
Tapped delay line structure, 401  
Telecommunications, and discrete-time signal  
processing, 8  
Time and frequency, sampling in, 819-822  
Time-dependent Fourier analysis of radar signals,  
834-836  
clutter, 835  
Doppler radar signals, 835-836  
Time-dependent Fourier synthesis, 822, 825  
Time-dependent Fourier transform, 792,811-829  
defined, 811  
effect of the window, 817-818  
filter bank interpretation, 826-829  
of X[n, AJ, 816-817 
~ 
invertibility of X[n, Aj, 815-816  
of a linear chirp signal, 811-814  
overlap-add method of reconstruction, 822-825  
sampling in time and frequency, 819-822  
signal processing based on, 825-826  
spectrogram, 814-815  
Time-dependent Fourier transform of speech, 
spectral display of, 832-834  
Time-division multiplexing (TDM), 266  
Time invariance, 24  
Time-invariant systems, 20-21  
accumulator as, 21  
accumulators as, 21  
compressor system, 21  
1·  

1107 
Index 
Time-reversal property, z-transform, 129  
time-reversed exponential sequence, 129  
Time-shifting property, z-transform, 125-126  
shifted exponential sequence, 126  
Tolerance schemes, 494  
Transform analysis of linear time-invariant (LTI)  
systems, 274-373  
Transposed form, 397-401  
for a basic second-order section, 398-399  
for a first-order system with no zeros, 397-398  
flow graph reversal, 397  
transposition, 397-401  
Transposition, 397-401  
Transversal filter structure, 401  
Trapezoidal approximation, 606  
Twicing, 609  
"Twiddle factors," 731  
Two-port flow graph, 405  
Two-sided exponential sequence, 107-108, 135  
Two-sided z-transform, 100  
Two's complement, 415  
Type-1 periodic symmetry, 675  
Type-2 periodic symmetry, 675  
Type I FIR linear-phase systems, 329  
example, 331-332  
Type II FIR linear-phase systems, 330  
example, 332-333  
Type III FIR linear-phase systems, 330  
example, 333-334  
Type IV FIR linear-phase systems, 330  
example, 335  
Unbiased estimators, 837  
Unilateral z-transform, 100, 135-137  
of an impulse, 135  
nonzero initial conditions, effect of, 136-137  
Unit circle, 100-101  
Unit impulse function, 154  
Vnit sample sequence, 12-13  
Unit step sequence, 12-13  
Unitary transforms, 676  
Unquantized filter, 423  
Unvoiced segments, 832  
Unwrapped phase, 349  
Upsampling filter design, 579-582  
Vocal tract, 830,1025-1026  
Voiced segments, 832  
Voiced sounds, 830, 1024  
Welch estimate, 862  
White noise, 69, 443  
Whitening filter, 897  
Whitening procedure, 366  
Wide band spectrogram, 832  
Window:  
method,530  
noncausal,816  
Windowing, 792  
Bartlett (triangular) windows, 536-539  
Blackman windows, 536-539  
commonly used windows, 535-536  
design of FIR filters by, 533-545  
and FIR filters, 533-545  
incorporation of generalized linear phase, 
538--541  
Kaiser window filter design method, 541-553  
properties of commonly used windows, 535-538  
Hamming windows, 536-539  
Hann windows, 536-539  
rectangular windows, 535-537, 539  
theorem, 61-62  
Winograd Fourier transform algorithm (WFI'A), 749  
Yule-Walker equations, 896, 898, 916  
z-transform, 99-152, See also Inverse z-transform  
bilateral, 100, 135  
common pairs, 110  
defined,99-100  
finite-length truncated exponential sequence, 109  
infinite sum, 103  
inverse, 115-124  
inspection method, 116  
inverse by partial fractions, 120-121  
partial fraction expansion, 116-117  
power series expansion, 122-124  
second-order z-transform, 118-120  
left-sided exponential sequence, 104-105  
LTI systems and, 131-134  
one-sided, 100, 135  
properties, 124-131  
conjugation property, 129  
convolution property, 130-131  
differentiation property, 127-129  
exponential multiplication property, 126-127  
linearity property, 124-125  
summary of, 131  
time-reversal property, 129  
time-shifting property, 125-126  

1108 
z-transform (continued)  
region of convergence (ROC), 101-102  
properties of, 110-115  
right -sided exponential sequence, 103-104  
sum of two exponential sequences, 105-107  
two-sided, 100  
two-sided exponential sequence, 107-108  
uniform convergence of, 102  
unilateral, 100, 135-137  
of an impulse, 135  
nonzero initial conditions, effect of, 136-137  
unit circle, 100-101  
Index 
z-transform operator Z{·}, 100  
Zero-input limit cycles:  
avoiding limit cycles, 463  
in fixed-point realizations of IIR digital filters,  
459-463  
limit cycles owing to overflow, 462-463  
limit cycles owing to round-off and truncation,  
460-462  
Zero-order hold, 222-223  
compensation for, 571-575  
Zero-padding, 667  

